Epoch: 1| Step: 0
Training loss: 5.109011173248291
Validation loss: 5.258159438769023

Epoch: 6| Step: 1
Training loss: 5.1947455406188965
Validation loss: 5.256122509638469

Epoch: 6| Step: 2
Training loss: 5.292648792266846
Validation loss: 5.253888448079427

Epoch: 6| Step: 3
Training loss: 4.8596038818359375
Validation loss: 5.251748879750569

Epoch: 6| Step: 4
Training loss: 5.725878715515137
Validation loss: 5.249511162439982

Epoch: 6| Step: 5
Training loss: 5.035617828369141
Validation loss: 5.24734099706014

Epoch: 6| Step: 6
Training loss: 5.041064739227295
Validation loss: 5.2451512813568115

Epoch: 6| Step: 7
Training loss: 5.4882612228393555
Validation loss: 5.242893298467

Epoch: 6| Step: 8
Training loss: 5.772491455078125
Validation loss: 5.2406338055928545

Epoch: 6| Step: 9
Training loss: 6.107505798339844
Validation loss: 5.238356987635295

Epoch: 6| Step: 10
Training loss: 4.869660377502441
Validation loss: 5.236076831817627

Epoch: 6| Step: 11
Training loss: 5.365157127380371
Validation loss: 5.233574946721395

Epoch: 6| Step: 12
Training loss: 5.418026924133301
Validation loss: 5.231005032857259

Epoch: 6| Step: 13
Training loss: 5.102956771850586
Validation loss: 5.228415012359619

Epoch: 2| Step: 0
Training loss: 5.0012288093566895
Validation loss: 5.225576639175415

Epoch: 6| Step: 1
Training loss: 5.621845722198486
Validation loss: 5.222633600234985

Epoch: 6| Step: 2
Training loss: 5.646130561828613
Validation loss: 5.219719886779785

Epoch: 6| Step: 3
Training loss: 5.240132808685303
Validation loss: 5.216633001963298

Epoch: 6| Step: 4
Training loss: 5.983371734619141
Validation loss: 5.213440338770549

Epoch: 6| Step: 5
Training loss: 5.123964309692383
Validation loss: 5.209898789723714

Epoch: 6| Step: 6
Training loss: 4.97206974029541
Validation loss: 5.206295808156331

Epoch: 6| Step: 7
Training loss: 4.743998050689697
Validation loss: 5.202481349309285

Epoch: 6| Step: 8
Training loss: 5.742750644683838
Validation loss: 5.19854474067688

Epoch: 6| Step: 9
Training loss: 4.699685096740723
Validation loss: 5.194385051727295

Epoch: 6| Step: 10
Training loss: 5.1292338371276855
Validation loss: 5.189957936604817

Epoch: 6| Step: 11
Training loss: 5.580542087554932
Validation loss: 5.185467799504598

Epoch: 6| Step: 12
Training loss: 5.501786231994629
Validation loss: 5.180740674336751

Epoch: 6| Step: 13
Training loss: 4.831967353820801
Validation loss: 5.175888776779175

Epoch: 3| Step: 0
Training loss: 5.411655426025391
Validation loss: 5.1705992221832275

Epoch: 6| Step: 1
Training loss: 5.116485595703125
Validation loss: 5.165077606836955

Epoch: 6| Step: 2
Training loss: 5.01818323135376
Validation loss: 5.159291664759318

Epoch: 6| Step: 3
Training loss: 4.671511173248291
Validation loss: 5.153046131134033

Epoch: 6| Step: 4
Training loss: 4.5884623527526855
Validation loss: 5.14678970972697

Epoch: 6| Step: 5
Training loss: 5.036739826202393
Validation loss: 5.140083471934001

Epoch: 6| Step: 6
Training loss: 3.8555917739868164
Validation loss: 5.133451660474141

Epoch: 6| Step: 7
Training loss: 4.749094486236572
Validation loss: 5.126057068506877

Epoch: 6| Step: 8
Training loss: 6.001898765563965
Validation loss: 5.118849277496338

Epoch: 6| Step: 9
Training loss: 4.748079299926758
Validation loss: 5.110891977945964

Epoch: 6| Step: 10
Training loss: 6.730402946472168
Validation loss: 5.102892637252808

Epoch: 6| Step: 11
Training loss: 5.2635955810546875
Validation loss: 5.0946517785390215

Epoch: 6| Step: 12
Training loss: 5.884491920471191
Validation loss: 5.08585254351298

Epoch: 6| Step: 13
Training loss: 5.725118637084961
Validation loss: 5.0770041942596436

Epoch: 4| Step: 0
Training loss: 5.320619583129883
Validation loss: 5.067773421605428

Epoch: 6| Step: 1
Training loss: 4.052735805511475
Validation loss: 5.058327039082845

Epoch: 6| Step: 2
Training loss: 4.494480133056641
Validation loss: 5.048704981803894

Epoch: 6| Step: 3
Training loss: 5.532500743865967
Validation loss: 5.0390571753184

Epoch: 6| Step: 4
Training loss: 5.388760566711426
Validation loss: 5.028629541397095

Epoch: 6| Step: 5
Training loss: 5.741247177124023
Validation loss: 5.018813649813334

Epoch: 6| Step: 6
Training loss: 5.575690269470215
Validation loss: 5.008513609568278

Epoch: 6| Step: 7
Training loss: 6.065051078796387
Validation loss: 4.9981240431467695

Epoch: 6| Step: 8
Training loss: 3.6748297214508057
Validation loss: 4.987602154413859

Epoch: 6| Step: 9
Training loss: 5.705277919769287
Validation loss: 4.976906935373942

Epoch: 6| Step: 10
Training loss: 5.309271812438965
Validation loss: 4.966566642125447

Epoch: 6| Step: 11
Training loss: 4.703961372375488
Validation loss: 4.955949942270915

Epoch: 6| Step: 12
Training loss: 4.905732154846191
Validation loss: 4.945402383804321

Epoch: 6| Step: 13
Training loss: 4.6570210456848145
Validation loss: 4.9343156814575195

Epoch: 5| Step: 0
Training loss: 5.010439872741699
Validation loss: 4.923619906107585

Epoch: 6| Step: 1
Training loss: 5.129297256469727
Validation loss: 4.912861426671346

Epoch: 6| Step: 2
Training loss: 5.2295660972595215
Validation loss: 4.902409633000691

Epoch: 6| Step: 3
Training loss: 4.707984447479248
Validation loss: 4.891455451647441

Epoch: 6| Step: 4
Training loss: 4.931168556213379
Validation loss: 4.880671262741089

Epoch: 6| Step: 5
Training loss: 4.932345390319824
Validation loss: 4.8699188232421875

Epoch: 6| Step: 6
Training loss: 4.417261600494385
Validation loss: 4.859592755635579

Epoch: 6| Step: 7
Training loss: 6.2788615226745605
Validation loss: 4.848508834838867

Epoch: 6| Step: 8
Training loss: 4.33105993270874
Validation loss: 4.838074088096619

Epoch: 6| Step: 9
Training loss: 4.818150520324707
Validation loss: 4.827560265858968

Epoch: 6| Step: 10
Training loss: 5.064716339111328
Validation loss: 4.81696621576945

Epoch: 6| Step: 11
Training loss: 5.332005500793457
Validation loss: 4.806810816129048

Epoch: 6| Step: 12
Training loss: 3.9701077938079834
Validation loss: 4.797091166178386

Epoch: 6| Step: 13
Training loss: 5.023749351501465
Validation loss: 4.78705628712972

Epoch: 6| Step: 0
Training loss: 4.538228988647461
Validation loss: 4.7777308622996015

Epoch: 6| Step: 1
Training loss: 4.8156633377075195
Validation loss: 4.768011808395386

Epoch: 6| Step: 2
Training loss: 4.037759780883789
Validation loss: 4.758398056030273

Epoch: 6| Step: 3
Training loss: 5.65993595123291
Validation loss: 4.749537626902263

Epoch: 6| Step: 4
Training loss: 5.735518932342529
Validation loss: 4.739759763081868

Epoch: 6| Step: 5
Training loss: 4.47415828704834
Validation loss: 4.729960362116496

Epoch: 6| Step: 6
Training loss: 4.2702226638793945
Validation loss: 4.719938198725383

Epoch: 6| Step: 7
Training loss: 5.1966352462768555
Validation loss: 4.710054715474446

Epoch: 6| Step: 8
Training loss: 4.976828098297119
Validation loss: 4.701422135035197

Epoch: 6| Step: 9
Training loss: 4.184787750244141
Validation loss: 4.693601131439209

Epoch: 6| Step: 10
Training loss: 4.4843010902404785
Validation loss: 4.686004916826884

Epoch: 6| Step: 11
Training loss: 4.992937088012695
Validation loss: 4.678806344668071

Epoch: 6| Step: 12
Training loss: 4.500895023345947
Validation loss: 4.671722888946533

Epoch: 6| Step: 13
Training loss: 5.491562366485596
Validation loss: 4.664604266484578

Epoch: 7| Step: 0
Training loss: 5.308023929595947
Validation loss: 4.657697796821594

Epoch: 6| Step: 1
Training loss: 4.729384899139404
Validation loss: 4.650980393091838

Epoch: 6| Step: 2
Training loss: 4.314059257507324
Validation loss: 4.644467274347941

Epoch: 6| Step: 3
Training loss: 4.273622035980225
Validation loss: 4.637942512830098

Epoch: 6| Step: 4
Training loss: 5.047167778015137
Validation loss: 4.63159453868866

Epoch: 6| Step: 5
Training loss: 4.827484130859375
Validation loss: 4.6250563859939575

Epoch: 6| Step: 6
Training loss: 4.501817226409912
Validation loss: 4.619181394577026

Epoch: 6| Step: 7
Training loss: 4.572484970092773
Validation loss: 4.612964590390523

Epoch: 6| Step: 8
Training loss: 4.935609340667725
Validation loss: 4.606685717900594

Epoch: 6| Step: 9
Training loss: 5.564640045166016
Validation loss: 4.600561459859212

Epoch: 6| Step: 10
Training loss: 5.081806182861328
Validation loss: 4.594554901123047

Epoch: 6| Step: 11
Training loss: 4.375214576721191
Validation loss: 4.588363567988078

Epoch: 6| Step: 12
Training loss: 4.640454292297363
Validation loss: 4.582435051600139

Epoch: 6| Step: 13
Training loss: 3.8476171493530273
Validation loss: 4.576406796773274

Epoch: 8| Step: 0
Training loss: 5.109454154968262
Validation loss: 4.570188919703166

Epoch: 6| Step: 1
Training loss: 4.056002140045166
Validation loss: 4.5639744599660235

Epoch: 6| Step: 2
Training loss: 5.0972490310668945
Validation loss: 4.557940721511841

Epoch: 6| Step: 3
Training loss: 4.604273796081543
Validation loss: 4.5515875816345215

Epoch: 6| Step: 4
Training loss: 4.555971622467041
Validation loss: 4.54597008228302

Epoch: 6| Step: 5
Training loss: 5.43223762512207
Validation loss: 4.539470752080281

Epoch: 6| Step: 6
Training loss: 3.037799596786499
Validation loss: 4.533584674199422

Epoch: 6| Step: 7
Training loss: 5.143089294433594
Validation loss: 4.527442614237468

Epoch: 6| Step: 8
Training loss: 5.868159294128418
Validation loss: 4.521946032842

Epoch: 6| Step: 9
Training loss: 4.515033721923828
Validation loss: 4.515821218490601

Epoch: 6| Step: 10
Training loss: 4.127723693847656
Validation loss: 4.509843349456787

Epoch: 6| Step: 11
Training loss: 3.9811978340148926
Validation loss: 4.503550410270691

Epoch: 6| Step: 12
Training loss: 4.405928611755371
Validation loss: 4.4974366426467896

Epoch: 6| Step: 13
Training loss: 4.973808765411377
Validation loss: 4.491414229075114

Epoch: 9| Step: 0
Training loss: 5.6624603271484375
Validation loss: 4.485864679018657

Epoch: 6| Step: 1
Training loss: 5.255399227142334
Validation loss: 4.479814807573955

Epoch: 6| Step: 2
Training loss: 4.4013261795043945
Validation loss: 4.473735968271892

Epoch: 6| Step: 3
Training loss: 4.017073631286621
Validation loss: 4.467362642288208

Epoch: 6| Step: 4
Training loss: 4.259778022766113
Validation loss: 4.460967620213826

Epoch: 6| Step: 5
Training loss: 4.9835991859436035
Validation loss: 4.455496311187744

Epoch: 6| Step: 6
Training loss: 3.4148848056793213
Validation loss: 4.4494006633758545

Epoch: 6| Step: 7
Training loss: 4.209997653961182
Validation loss: 4.443650563557942

Epoch: 6| Step: 8
Training loss: 4.620155334472656
Validation loss: 4.4375613530476885

Epoch: 6| Step: 9
Training loss: 4.673914909362793
Validation loss: 4.4314502875010175

Epoch: 6| Step: 10
Training loss: 3.8238630294799805
Validation loss: 4.424840807914734

Epoch: 6| Step: 11
Training loss: 4.735763072967529
Validation loss: 4.419139385223389

Epoch: 6| Step: 12
Training loss: 5.0294389724731445
Validation loss: 4.413323998451233

Epoch: 6| Step: 13
Training loss: 4.758237838745117
Validation loss: 4.4076197147369385

Epoch: 10| Step: 0
Training loss: 4.440926551818848
Validation loss: 4.402543862660726

Epoch: 6| Step: 1
Training loss: 4.140126705169678
Validation loss: 4.3967617352803545

Epoch: 6| Step: 2
Training loss: 5.0803141593933105
Validation loss: 4.391305009524028

Epoch: 6| Step: 3
Training loss: 5.329316139221191
Validation loss: 4.385461211204529

Epoch: 6| Step: 4
Training loss: 4.503384590148926
Validation loss: 4.380073070526123

Epoch: 6| Step: 5
Training loss: 4.116909980773926
Validation loss: 4.374332149823506

Epoch: 6| Step: 6
Training loss: 4.8026347160339355
Validation loss: 4.368657191594441

Epoch: 6| Step: 7
Training loss: 4.414253234863281
Validation loss: 4.362744490305583

Epoch: 6| Step: 8
Training loss: 4.168919086456299
Validation loss: 4.356870452562968

Epoch: 6| Step: 9
Training loss: 3.908867835998535
Validation loss: 4.351818601290385

Epoch: 6| Step: 10
Training loss: 4.385849952697754
Validation loss: 4.345636526743571

Epoch: 6| Step: 11
Training loss: 4.4439496994018555
Validation loss: 4.340392112731934

Epoch: 6| Step: 12
Training loss: 4.461089134216309
Validation loss: 4.334848006566365

Epoch: 6| Step: 13
Training loss: 4.585681915283203
Validation loss: 4.329450805981954

Epoch: 11| Step: 0
Training loss: 4.016974449157715
Validation loss: 4.32383131980896

Epoch: 6| Step: 1
Training loss: 4.009964942932129
Validation loss: 4.3185648918151855

Epoch: 6| Step: 2
Training loss: 4.174435615539551
Validation loss: 4.313156882921855

Epoch: 6| Step: 3
Training loss: 5.078057289123535
Validation loss: 4.307734568913777

Epoch: 6| Step: 4
Training loss: 3.8069753646850586
Validation loss: 4.301798224449158

Epoch: 6| Step: 5
Training loss: 4.824848175048828
Validation loss: 4.296151200930278

Epoch: 6| Step: 6
Training loss: 4.697303771972656
Validation loss: 4.290777087211609

Epoch: 6| Step: 7
Training loss: 3.1213388442993164
Validation loss: 4.28493348757426

Epoch: 6| Step: 8
Training loss: 3.9482388496398926
Validation loss: 4.28009291489919

Epoch: 6| Step: 9
Training loss: 5.240797519683838
Validation loss: 4.2747383912404375

Epoch: 6| Step: 10
Training loss: 4.264472484588623
Validation loss: 4.268766681353251

Epoch: 6| Step: 11
Training loss: 5.070075035095215
Validation loss: 4.262558380762736

Epoch: 6| Step: 12
Training loss: 5.460973739624023
Validation loss: 4.257541696230571

Epoch: 6| Step: 13
Training loss: 4.0810065269470215
Validation loss: 4.252269824345906

Epoch: 12| Step: 0
Training loss: 3.9646785259246826
Validation loss: 4.246701955795288

Epoch: 6| Step: 1
Training loss: 3.7864747047424316
Validation loss: 4.24159316221873

Epoch: 6| Step: 2
Training loss: 4.202249526977539
Validation loss: 4.235790848731995

Epoch: 6| Step: 3
Training loss: 4.602457046508789
Validation loss: 4.230174382527669

Epoch: 6| Step: 4
Training loss: 4.602790832519531
Validation loss: 4.225482940673828

Epoch: 6| Step: 5
Training loss: 5.195411682128906
Validation loss: 4.220060745875041

Epoch: 6| Step: 6
Training loss: 4.746219635009766
Validation loss: 4.214262127876282

Epoch: 6| Step: 7
Training loss: 4.450000762939453
Validation loss: 4.209255496660869

Epoch: 6| Step: 8
Training loss: 4.94955587387085
Validation loss: 4.203681310017903

Epoch: 6| Step: 9
Training loss: 3.3838729858398438
Validation loss: 4.197599291801453

Epoch: 6| Step: 10
Training loss: 4.857428550720215
Validation loss: 4.191687345504761

Epoch: 6| Step: 11
Training loss: 3.419064998626709
Validation loss: 4.186612844467163

Epoch: 6| Step: 12
Training loss: 4.917288780212402
Validation loss: 4.1816056569417315

Epoch: 6| Step: 13
Training loss: 3.7321372032165527
Validation loss: 4.175434072812398

Epoch: 13| Step: 0
Training loss: 4.0056986808776855
Validation loss: 4.170499722162883

Epoch: 6| Step: 1
Training loss: 4.5579304695129395
Validation loss: 4.164987047513326

Epoch: 6| Step: 2
Training loss: 4.624969482421875
Validation loss: 4.159544467926025

Epoch: 6| Step: 3
Training loss: 3.1193041801452637
Validation loss: 4.154425938924153

Epoch: 6| Step: 4
Training loss: 4.168086051940918
Validation loss: 4.1497165362040205

Epoch: 6| Step: 5
Training loss: 3.9541797637939453
Validation loss: 4.144865711530049

Epoch: 6| Step: 6
Training loss: 4.630559921264648
Validation loss: 4.1395905415217085

Epoch: 6| Step: 7
Training loss: 5.0031023025512695
Validation loss: 4.13468857606252

Epoch: 6| Step: 8
Training loss: 4.796744346618652
Validation loss: 4.130088766415914

Epoch: 6| Step: 9
Training loss: 3.320875883102417
Validation loss: 4.124619126319885

Epoch: 6| Step: 10
Training loss: 3.7354190349578857
Validation loss: 4.11927064259847

Epoch: 6| Step: 11
Training loss: 4.2060866355896
Validation loss: 4.114778955777486

Epoch: 6| Step: 12
Training loss: 4.109113693237305
Validation loss: 4.108937780062358

Epoch: 6| Step: 13
Training loss: 5.6068267822265625
Validation loss: 4.104601740837097

Epoch: 14| Step: 0
Training loss: 3.5629637241363525
Validation loss: 4.098201950391133

Epoch: 6| Step: 1
Training loss: 4.661769866943359
Validation loss: 4.093255003293355

Epoch: 6| Step: 2
Training loss: 4.249731063842773
Validation loss: 4.087411006291707

Epoch: 6| Step: 3
Training loss: 2.9204578399658203
Validation loss: 4.081717371940613

Epoch: 6| Step: 4
Training loss: 4.041902542114258
Validation loss: 4.076335867245992

Epoch: 6| Step: 5
Training loss: 3.9933829307556152
Validation loss: 4.072179992993672

Epoch: 6| Step: 6
Training loss: 4.3310441970825195
Validation loss: 4.065989057223002

Epoch: 6| Step: 7
Training loss: 4.298635482788086
Validation loss: 4.06098469098409

Epoch: 6| Step: 8
Training loss: 5.11861515045166
Validation loss: 4.055178960164388

Epoch: 6| Step: 9
Training loss: 4.8192901611328125
Validation loss: 4.049722711245219

Epoch: 6| Step: 10
Training loss: 3.832655668258667
Validation loss: 4.044479688008626

Epoch: 6| Step: 11
Training loss: 4.804600715637207
Validation loss: 4.039645075798035

Epoch: 6| Step: 12
Training loss: 3.8516476154327393
Validation loss: 4.034210840861003

Epoch: 6| Step: 13
Training loss: 4.381217002868652
Validation loss: 4.029588262240092

Epoch: 15| Step: 0
Training loss: 4.830501556396484
Validation loss: 4.023710648218791

Epoch: 6| Step: 1
Training loss: 4.253410816192627
Validation loss: 4.019468625386556

Epoch: 6| Step: 2
Training loss: 4.2614641189575195
Validation loss: 4.014548261960347

Epoch: 6| Step: 3
Training loss: 4.55994987487793
Validation loss: 4.008755604426066

Epoch: 6| Step: 4
Training loss: 2.7948927879333496
Validation loss: 4.003282308578491

Epoch: 6| Step: 5
Training loss: 5.191964149475098
Validation loss: 3.998724857966105

Epoch: 6| Step: 6
Training loss: 4.231633186340332
Validation loss: 3.9934712251027427

Epoch: 6| Step: 7
Training loss: 4.099835395812988
Validation loss: 3.988449811935425

Epoch: 6| Step: 8
Training loss: 3.820713520050049
Validation loss: 3.983688751856486

Epoch: 6| Step: 9
Training loss: 4.225848197937012
Validation loss: 3.9781959454218545

Epoch: 6| Step: 10
Training loss: 4.14261531829834
Validation loss: 3.9740854501724243

Epoch: 6| Step: 11
Training loss: 3.3529725074768066
Validation loss: 3.9689420064290366

Epoch: 6| Step: 12
Training loss: 4.094249725341797
Validation loss: 3.963335076967875

Epoch: 6| Step: 13
Training loss: 4.053964138031006
Validation loss: 3.958116094271342

Epoch: 16| Step: 0
Training loss: 3.7981998920440674
Validation loss: 3.9528242349624634

Epoch: 6| Step: 1
Training loss: 4.109696388244629
Validation loss: 3.948055704434713

Epoch: 6| Step: 2
Training loss: 3.119248390197754
Validation loss: 3.9448766311009726

Epoch: 6| Step: 3
Training loss: 3.5038459300994873
Validation loss: 3.9408893982569375

Epoch: 6| Step: 4
Training loss: 4.889021873474121
Validation loss: 3.9355671405792236

Epoch: 6| Step: 5
Training loss: 5.0255327224731445
Validation loss: 3.928162097930908

Epoch: 6| Step: 6
Training loss: 4.732509136199951
Validation loss: 3.92545485496521

Epoch: 6| Step: 7
Training loss: 3.700026512145996
Validation loss: 3.920607407887777

Epoch: 6| Step: 8
Training loss: 4.753288745880127
Validation loss: 3.915258526802063

Epoch: 6| Step: 9
Training loss: 3.4511966705322266
Validation loss: 3.909868319829305

Epoch: 6| Step: 10
Training loss: 3.5793509483337402
Validation loss: 3.9049024184544883

Epoch: 6| Step: 11
Training loss: 4.297375202178955
Validation loss: 3.9002020756403604

Epoch: 6| Step: 12
Training loss: 4.109531879425049
Validation loss: 3.8943463563919067

Epoch: 6| Step: 13
Training loss: 3.8854055404663086
Validation loss: 3.8894255558649697

Epoch: 17| Step: 0
Training loss: 4.747382164001465
Validation loss: 3.8840842644373574

Epoch: 6| Step: 1
Training loss: 3.1785926818847656
Validation loss: 3.8798125982284546

Epoch: 6| Step: 2
Training loss: 4.34135103225708
Validation loss: 3.874478022257487

Epoch: 6| Step: 3
Training loss: 3.5491716861724854
Validation loss: 3.8702025016148887

Epoch: 6| Step: 4
Training loss: 4.056731224060059
Validation loss: 3.8651559750239053

Epoch: 6| Step: 5
Training loss: 3.6698575019836426
Validation loss: 3.8609653313954673

Epoch: 6| Step: 6
Training loss: 3.409188985824585
Validation loss: 3.8557252883911133

Epoch: 6| Step: 7
Training loss: 4.582046031951904
Validation loss: 3.8476433753967285

Epoch: 6| Step: 8
Training loss: 3.786241054534912
Validation loss: 3.8428092002868652

Epoch: 6| Step: 9
Training loss: 4.340479373931885
Validation loss: 3.8375752766927085

Epoch: 6| Step: 10
Training loss: 3.570773124694824
Validation loss: 3.8318528731664023

Epoch: 6| Step: 11
Training loss: 3.557267665863037
Validation loss: 3.8263969818751016

Epoch: 6| Step: 12
Training loss: 4.998727321624756
Validation loss: 3.8211851517359414

Epoch: 6| Step: 13
Training loss: 4.251152515411377
Validation loss: 3.818626562754313

Epoch: 18| Step: 0
Training loss: 4.105059623718262
Validation loss: 3.812580426534017

Epoch: 6| Step: 1
Training loss: 3.948704242706299
Validation loss: 3.805523633956909

Epoch: 6| Step: 2
Training loss: 3.1964612007141113
Validation loss: 3.802071531613668

Epoch: 6| Step: 3
Training loss: 3.2572898864746094
Validation loss: 3.798752466837565

Epoch: 6| Step: 4
Training loss: 3.996034860610962
Validation loss: 3.7919960419336953

Epoch: 6| Step: 5
Training loss: 4.869070053100586
Validation loss: 3.7874122858047485

Epoch: 6| Step: 6
Training loss: 4.3092474937438965
Validation loss: 3.782008727391561

Epoch: 6| Step: 7
Training loss: 4.206587791442871
Validation loss: 3.7769997119903564

Epoch: 6| Step: 8
Training loss: 3.295442581176758
Validation loss: 3.7711464961369834

Epoch: 6| Step: 9
Training loss: 4.2101898193359375
Validation loss: 3.76630437374115

Epoch: 6| Step: 10
Training loss: 4.284652233123779
Validation loss: 3.7618178923924765

Epoch: 6| Step: 11
Training loss: 3.022491931915283
Validation loss: 3.758116523424784

Epoch: 6| Step: 12
Training loss: 4.389960289001465
Validation loss: 3.754706859588623

Epoch: 6| Step: 13
Training loss: 4.010887145996094
Validation loss: 3.7473654747009277

Epoch: 19| Step: 0
Training loss: 4.149146556854248
Validation loss: 3.741621176401774

Epoch: 6| Step: 1
Training loss: 3.8825788497924805
Validation loss: 3.736870527267456

Epoch: 6| Step: 2
Training loss: 2.677602529525757
Validation loss: 3.732195417086283

Epoch: 6| Step: 3
Training loss: 4.400150299072266
Validation loss: 3.7280839681625366

Epoch: 6| Step: 4
Training loss: 4.4170756340026855
Validation loss: 3.7216045459111533

Epoch: 6| Step: 5
Training loss: 4.140529632568359
Validation loss: 3.7220515807469687

Epoch: 6| Step: 6
Training loss: 4.732909202575684
Validation loss: 3.7139841318130493

Epoch: 6| Step: 7
Training loss: 4.031667232513428
Validation loss: 3.707966764767965

Epoch: 6| Step: 8
Training loss: 3.3446707725524902
Validation loss: 3.7034715016682944

Epoch: 6| Step: 9
Training loss: 3.0127806663513184
Validation loss: 3.6986143589019775

Epoch: 6| Step: 10
Training loss: 3.97174072265625
Validation loss: 3.693057378133138

Epoch: 6| Step: 11
Training loss: 3.5964736938476562
Validation loss: 3.68748946984609

Epoch: 6| Step: 12
Training loss: 3.837466239929199
Validation loss: 3.68403955300649

Epoch: 6| Step: 13
Training loss: 3.9967737197875977
Validation loss: 3.6791212956110635

Epoch: 20| Step: 0
Training loss: 4.428984642028809
Validation loss: 3.6756332317988076

Epoch: 6| Step: 1
Training loss: 3.3113903999328613
Validation loss: 3.6697779099146524

Epoch: 6| Step: 2
Training loss: 3.5083954334259033
Validation loss: 3.6648061275482178

Epoch: 6| Step: 3
Training loss: 4.63177490234375
Validation loss: 3.660231590270996

Epoch: 6| Step: 4
Training loss: 3.2114832401275635
Validation loss: 3.6581935485204062

Epoch: 6| Step: 5
Training loss: 3.9606070518493652
Validation loss: 3.65390678246816

Epoch: 6| Step: 6
Training loss: 3.5476412773132324
Validation loss: 3.648248791694641

Epoch: 6| Step: 7
Training loss: 3.7817344665527344
Validation loss: 3.642545978228251

Epoch: 6| Step: 8
Training loss: 3.97696590423584
Validation loss: 3.639493147532145

Epoch: 6| Step: 9
Training loss: 4.140822410583496
Validation loss: 3.6341735124588013

Epoch: 6| Step: 10
Training loss: 3.1844944953918457
Validation loss: 3.6286431153615317

Epoch: 6| Step: 11
Training loss: 4.033446311950684
Validation loss: 3.622937321662903

Epoch: 6| Step: 12
Training loss: 4.956562519073486
Validation loss: 3.6197691361109414

Epoch: 6| Step: 13
Training loss: 2.612044334411621
Validation loss: 3.616278330485026

Epoch: 21| Step: 0
Training loss: 3.6869983673095703
Validation loss: 3.6142305533091226

Epoch: 6| Step: 1
Training loss: 3.719757318496704
Validation loss: 3.6119057734807334

Epoch: 6| Step: 2
Training loss: 4.243809223175049
Validation loss: 3.6046916246414185

Epoch: 6| Step: 3
Training loss: 2.527231216430664
Validation loss: 3.596759875615438

Epoch: 6| Step: 4
Training loss: 3.66493821144104
Validation loss: 3.5911864042282104

Epoch: 6| Step: 5
Training loss: 4.108100891113281
Validation loss: 3.5866606632868447

Epoch: 6| Step: 6
Training loss: 4.402469635009766
Validation loss: 3.5830198526382446

Epoch: 6| Step: 7
Training loss: 4.702252388000488
Validation loss: 3.5785340865453086

Epoch: 6| Step: 8
Training loss: 3.2062554359436035
Validation loss: 3.572256644566854

Epoch: 6| Step: 9
Training loss: 4.335699081420898
Validation loss: 3.5662391980489097

Epoch: 6| Step: 10
Training loss: 3.2114315032958984
Validation loss: 3.5618225733439126

Epoch: 6| Step: 11
Training loss: 3.5686047077178955
Validation loss: 3.5570009549458823

Epoch: 6| Step: 12
Training loss: 4.042508125305176
Validation loss: 3.5521419048309326

Epoch: 6| Step: 13
Training loss: 3.008315086364746
Validation loss: 3.548189083735148

Epoch: 22| Step: 0
Training loss: 2.7566895484924316
Validation loss: 3.5420859257380166

Epoch: 6| Step: 1
Training loss: 3.7117092609405518
Validation loss: 3.5373839934666953

Epoch: 6| Step: 2
Training loss: 3.7803547382354736
Validation loss: 3.533711036046346

Epoch: 6| Step: 3
Training loss: 3.7291550636291504
Validation loss: 3.5291982889175415

Epoch: 6| Step: 4
Training loss: 3.2612972259521484
Validation loss: 3.524128278096517

Epoch: 6| Step: 5
Training loss: 4.412825584411621
Validation loss: 3.517921487490336

Epoch: 6| Step: 6
Training loss: 3.2785518169403076
Validation loss: 3.512061278025309

Epoch: 6| Step: 7
Training loss: 4.088796615600586
Validation loss: 3.508752703666687

Epoch: 6| Step: 8
Training loss: 3.4888854026794434
Validation loss: 3.505248546600342

Epoch: 6| Step: 9
Training loss: 4.483707904815674
Validation loss: 3.5027909676233926

Epoch: 6| Step: 10
Training loss: 2.8348731994628906
Validation loss: 3.4956907431284585

Epoch: 6| Step: 11
Training loss: 3.909224033355713
Validation loss: 3.491264502207438

Epoch: 6| Step: 12
Training loss: 4.214664936065674
Validation loss: 3.4875094095865884

Epoch: 6| Step: 13
Training loss: 3.532242774963379
Validation loss: 3.483906070391337

Epoch: 23| Step: 0
Training loss: 3.3499460220336914
Validation loss: 3.4799399375915527

Epoch: 6| Step: 1
Training loss: 2.9819231033325195
Validation loss: 3.4748701254526773

Epoch: 6| Step: 2
Training loss: 3.8086421489715576
Validation loss: 3.47060755888621

Epoch: 6| Step: 3
Training loss: 3.512173652648926
Validation loss: 3.465003768603007

Epoch: 6| Step: 4
Training loss: 3.6109588146209717
Validation loss: 3.457884669303894

Epoch: 6| Step: 5
Training loss: 5.208596229553223
Validation loss: 3.4525883197784424

Epoch: 6| Step: 6
Training loss: 3.095461368560791
Validation loss: 3.4471967220306396

Epoch: 6| Step: 7
Training loss: 3.76599383354187
Validation loss: 3.4438889821370444

Epoch: 6| Step: 8
Training loss: 2.783951997756958
Validation loss: 3.4395968914031982

Epoch: 6| Step: 9
Training loss: 4.405648708343506
Validation loss: 3.4355327685674033

Epoch: 6| Step: 10
Training loss: 3.258378267288208
Validation loss: 3.4299063285191855

Epoch: 6| Step: 11
Training loss: 4.3934478759765625
Validation loss: 3.424743096033732

Epoch: 6| Step: 12
Training loss: 2.7735846042633057
Validation loss: 3.419448177019755

Epoch: 6| Step: 13
Training loss: 3.6026744842529297
Validation loss: 3.4164159297943115

Epoch: 24| Step: 0
Training loss: 4.799219131469727
Validation loss: 3.4100801944732666

Epoch: 6| Step: 1
Training loss: 4.047210216522217
Validation loss: 3.404783606529236

Epoch: 6| Step: 2
Training loss: 3.9481918811798096
Validation loss: 3.4017395973205566

Epoch: 6| Step: 3
Training loss: 3.456575632095337
Validation loss: 3.3978002866109214

Epoch: 6| Step: 4
Training loss: 2.9128003120422363
Validation loss: 3.3920517762502036

Epoch: 6| Step: 5
Training loss: 3.07608962059021
Validation loss: 3.3874497413635254

Epoch: 6| Step: 6
Training loss: 3.4058570861816406
Validation loss: 3.3834849198659263

Epoch: 6| Step: 7
Training loss: 3.216855049133301
Validation loss: 3.3794161876042685

Epoch: 6| Step: 8
Training loss: 3.9287819862365723
Validation loss: 3.3734633525212607

Epoch: 6| Step: 9
Training loss: 3.4148426055908203
Validation loss: 3.3682796955108643

Epoch: 6| Step: 10
Training loss: 3.7099783420562744
Validation loss: 3.3647446235020957

Epoch: 6| Step: 11
Training loss: 3.1488165855407715
Validation loss: 3.359179417292277

Epoch: 6| Step: 12
Training loss: 2.8213300704956055
Validation loss: 3.3554182052612305

Epoch: 6| Step: 13
Training loss: 3.806450366973877
Validation loss: 3.3497418562571206

Epoch: 25| Step: 0
Training loss: 3.3394923210144043
Validation loss: 3.345774253209432

Epoch: 6| Step: 1
Training loss: 3.482541084289551
Validation loss: 3.3419278065363565

Epoch: 6| Step: 2
Training loss: 3.3653719425201416
Validation loss: 3.337372899055481

Epoch: 6| Step: 3
Training loss: 3.1311748027801514
Validation loss: 3.334176222483317

Epoch: 6| Step: 4
Training loss: 3.8557779788970947
Validation loss: 3.327640493710836

Epoch: 6| Step: 5
Training loss: 3.8505172729492188
Validation loss: 3.3230686585108438

Epoch: 6| Step: 6
Training loss: 3.3123722076416016
Validation loss: 3.3197983503341675

Epoch: 6| Step: 7
Training loss: 3.8246359825134277
Validation loss: 3.314579884211222

Epoch: 6| Step: 8
Training loss: 3.123462200164795
Validation loss: 3.310274879137675

Epoch: 6| Step: 9
Training loss: 3.728367805480957
Validation loss: 3.307279944419861

Epoch: 6| Step: 10
Training loss: 3.101532459259033
Validation loss: 3.3017234802246094

Epoch: 6| Step: 11
Training loss: 3.214954376220703
Validation loss: 3.296854098637899

Epoch: 6| Step: 12
Training loss: 4.204270362854004
Validation loss: 3.2930949529012046

Epoch: 6| Step: 13
Training loss: 3.3014612197875977
Validation loss: 3.2882530291875205

Epoch: 26| Step: 0
Training loss: 3.3990225791931152
Validation loss: 3.2835872173309326

Epoch: 6| Step: 1
Training loss: 2.391974449157715
Validation loss: 3.2788290977478027

Epoch: 6| Step: 2
Training loss: 3.5708670616149902
Validation loss: 3.276457111040751

Epoch: 6| Step: 3
Training loss: 3.1144633293151855
Validation loss: 3.271752119064331

Epoch: 6| Step: 4
Training loss: 4.1871843338012695
Validation loss: 3.267195701599121

Epoch: 6| Step: 5
Training loss: 4.039016246795654
Validation loss: 3.2640844583511353

Epoch: 6| Step: 6
Training loss: 3.1657519340515137
Validation loss: 3.2588626543680825

Epoch: 6| Step: 7
Training loss: 3.3266048431396484
Validation loss: 3.254389484723409

Epoch: 6| Step: 8
Training loss: 4.034366607666016
Validation loss: 3.250349760055542

Epoch: 6| Step: 9
Training loss: 3.4388670921325684
Validation loss: 3.2460901339848838

Epoch: 6| Step: 10
Training loss: 3.857017993927002
Validation loss: 3.2413769960403442

Epoch: 6| Step: 11
Training loss: 2.7415850162506104
Validation loss: 3.2373050848642984

Epoch: 6| Step: 12
Training loss: 3.4511871337890625
Validation loss: 3.2320513327916465

Epoch: 6| Step: 13
Training loss: 3.3318300247192383
Validation loss: 3.228310704231262

Epoch: 27| Step: 0
Training loss: 2.973757028579712
Validation loss: 3.223729888598124

Epoch: 6| Step: 1
Training loss: 3.024808406829834
Validation loss: 3.21950097878774

Epoch: 6| Step: 2
Training loss: 3.3505029678344727
Validation loss: 3.2154844204584756

Epoch: 6| Step: 3
Training loss: 3.738537073135376
Validation loss: 3.212038437525431

Epoch: 6| Step: 4
Training loss: 3.831892728805542
Validation loss: 3.209232290585836

Epoch: 6| Step: 5
Training loss: 2.5068063735961914
Validation loss: 3.20237930615743

Epoch: 6| Step: 6
Training loss: 3.1040470600128174
Validation loss: 3.198688824971517

Epoch: 6| Step: 7
Training loss: 3.840101480484009
Validation loss: 3.19732395807902

Epoch: 6| Step: 8
Training loss: 3.614410400390625
Validation loss: 3.1923759380976358

Epoch: 6| Step: 9
Training loss: 4.039518356323242
Validation loss: 3.1860939661661782

Epoch: 6| Step: 10
Training loss: 3.3665826320648193
Validation loss: 3.1811901330947876

Epoch: 6| Step: 11
Training loss: 3.00656795501709
Validation loss: 3.178573489189148

Epoch: 6| Step: 12
Training loss: 3.637023448944092
Validation loss: 3.173899213473002

Epoch: 6| Step: 13
Training loss: 3.229370594024658
Validation loss: 3.169804096221924

Epoch: 28| Step: 0
Training loss: 3.6797146797180176
Validation loss: 3.1659796039263406

Epoch: 6| Step: 1
Training loss: 3.2586827278137207
Validation loss: 3.159436901410421

Epoch: 6| Step: 2
Training loss: 3.267216920852661
Validation loss: 3.1560171445210776

Epoch: 6| Step: 3
Training loss: 3.393230438232422
Validation loss: 3.152387539545695

Epoch: 6| Step: 4
Training loss: 3.1953535079956055
Validation loss: 3.146569530169169

Epoch: 6| Step: 5
Training loss: 4.366057395935059
Validation loss: 3.142895817756653

Epoch: 6| Step: 6
Training loss: 3.513308525085449
Validation loss: 3.1381448904673257

Epoch: 6| Step: 7
Training loss: 2.371011734008789
Validation loss: 3.133625864982605

Epoch: 6| Step: 8
Training loss: 3.3268799781799316
Validation loss: 3.1287430127461753

Epoch: 6| Step: 9
Training loss: 3.4496335983276367
Validation loss: 3.1244707504908242

Epoch: 6| Step: 10
Training loss: 3.581547737121582
Validation loss: 3.119751969973246

Epoch: 6| Step: 11
Training loss: 3.486098289489746
Validation loss: 3.1152336994806924

Epoch: 6| Step: 12
Training loss: 2.2632322311401367
Validation loss: 3.1096033255259194

Epoch: 6| Step: 13
Training loss: 3.3890440464019775
Validation loss: 3.1083385944366455

Epoch: 29| Step: 0
Training loss: 3.1889853477478027
Validation loss: 3.110530376434326

Epoch: 6| Step: 1
Training loss: 2.9675731658935547
Validation loss: 3.117546796798706

Epoch: 6| Step: 2
Training loss: 4.189702033996582
Validation loss: 3.1217008431752524

Epoch: 6| Step: 3
Training loss: 3.149197816848755
Validation loss: 3.092815637588501

Epoch: 6| Step: 4
Training loss: 3.333937644958496
Validation loss: 3.090086023012797

Epoch: 6| Step: 5
Training loss: 3.4468131065368652
Validation loss: 3.088712215423584

Epoch: 6| Step: 6
Training loss: 3.376276969909668
Validation loss: 3.094089229901632

Epoch: 6| Step: 7
Training loss: 3.421469211578369
Validation loss: 3.095693071683248

Epoch: 6| Step: 8
Training loss: 3.3302011489868164
Validation loss: 3.0833109617233276

Epoch: 6| Step: 9
Training loss: 3.06368350982666
Validation loss: 3.0771878957748413

Epoch: 6| Step: 10
Training loss: 3.5744423866271973
Validation loss: 3.072750727335612

Epoch: 6| Step: 11
Training loss: 2.890620708465576
Validation loss: 3.0669874350229898

Epoch: 6| Step: 12
Training loss: 2.8760342597961426
Validation loss: 3.0635852018992105

Epoch: 6| Step: 13
Training loss: 3.0382556915283203
Validation loss: 3.062183698018392

Epoch: 30| Step: 0
Training loss: 2.7261719703674316
Validation loss: 3.057899077733358

Epoch: 6| Step: 1
Training loss: 3.098745822906494
Validation loss: 3.052798946698507

Epoch: 6| Step: 2
Training loss: 2.873682975769043
Validation loss: 3.0486292441685996

Epoch: 6| Step: 3
Training loss: 2.936077117919922
Validation loss: 3.045230189959208

Epoch: 6| Step: 4
Training loss: 2.9686760902404785
Validation loss: 3.04012401898702

Epoch: 6| Step: 5
Training loss: 2.794887065887451
Validation loss: 3.035735527674357

Epoch: 6| Step: 6
Training loss: 2.9597012996673584
Validation loss: 3.0314149061838784

Epoch: 6| Step: 7
Training loss: 2.5986523628234863
Validation loss: 3.028011759122213

Epoch: 6| Step: 8
Training loss: 3.9574408531188965
Validation loss: 3.022710124651591

Epoch: 6| Step: 9
Training loss: 2.9382386207580566
Validation loss: 3.019461433092753

Epoch: 6| Step: 10
Training loss: 3.858980417251587
Validation loss: 3.0152788162231445

Epoch: 6| Step: 11
Training loss: 3.768968343734741
Validation loss: 3.01116681098938

Epoch: 6| Step: 12
Training loss: 3.819120407104492
Validation loss: 3.006996989250183

Epoch: 6| Step: 13
Training loss: 3.7842540740966797
Validation loss: 3.004020174344381

Epoch: 31| Step: 0
Training loss: 3.811488628387451
Validation loss: 2.9985790252685547

Epoch: 6| Step: 1
Training loss: 3.8735833168029785
Validation loss: 2.994040568669637

Epoch: 6| Step: 2
Training loss: 3.7718043327331543
Validation loss: 2.99026095867157

Epoch: 6| Step: 3
Training loss: 2.943850517272949
Validation loss: 2.986204504966736

Epoch: 6| Step: 4
Training loss: 2.9691519737243652
Validation loss: 2.9832178354263306

Epoch: 6| Step: 5
Training loss: 3.0132312774658203
Validation loss: 2.984063506126404

Epoch: 6| Step: 6
Training loss: 2.792067766189575
Validation loss: 2.9848742882410684

Epoch: 6| Step: 7
Training loss: 3.6097629070281982
Validation loss: 2.9720374743143716

Epoch: 6| Step: 8
Training loss: 3.4226348400115967
Validation loss: 2.9639832973480225

Epoch: 6| Step: 9
Training loss: 2.7839245796203613
Validation loss: 2.9604647159576416

Epoch: 6| Step: 10
Training loss: 2.9590072631835938
Validation loss: 2.958386182785034

Epoch: 6| Step: 11
Training loss: 2.246309995651245
Validation loss: 2.9546240170796714

Epoch: 6| Step: 12
Training loss: 2.780308246612549
Validation loss: 2.9530019760131836

Epoch: 6| Step: 13
Training loss: 3.418689727783203
Validation loss: 2.949842651685079

Epoch: 32| Step: 0
Training loss: 2.70402193069458
Validation loss: 2.9456390937169394

Epoch: 6| Step: 1
Training loss: 3.086055278778076
Validation loss: 2.94162925084432

Epoch: 6| Step: 2
Training loss: 2.2719452381134033
Validation loss: 2.9372854232788086

Epoch: 6| Step: 3
Training loss: 3.7843360900878906
Validation loss: 2.9340261220932007

Epoch: 6| Step: 4
Training loss: 3.8031532764434814
Validation loss: 2.930212616920471

Epoch: 6| Step: 5
Training loss: 3.03200101852417
Validation loss: 2.925270199775696

Epoch: 6| Step: 6
Training loss: 1.6986479759216309
Validation loss: 2.921589175860087

Epoch: 6| Step: 7
Training loss: 3.5070948600769043
Validation loss: 2.918896476427714

Epoch: 6| Step: 8
Training loss: 2.523261785507202
Validation loss: 2.914392034212748

Epoch: 6| Step: 9
Training loss: 3.9289774894714355
Validation loss: 2.91177499294281

Epoch: 6| Step: 10
Training loss: 3.3259153366088867
Validation loss: 2.9090909560521445

Epoch: 6| Step: 11
Training loss: 2.7288026809692383
Validation loss: 2.9047590096791587

Epoch: 6| Step: 12
Training loss: 3.4083144664764404
Validation loss: 2.9000072876612344

Epoch: 6| Step: 13
Training loss: 3.8846497535705566
Validation loss: 2.8963557879130044

Epoch: 33| Step: 0
Training loss: 3.3930509090423584
Validation loss: 2.8920173247655234

Epoch: 6| Step: 1
Training loss: 3.438816547393799
Validation loss: 2.8867756525675454

Epoch: 6| Step: 2
Training loss: 3.3514819145202637
Validation loss: 2.8840222358703613

Epoch: 6| Step: 3
Training loss: 2.9405839443206787
Validation loss: 2.880273938179016

Epoch: 6| Step: 4
Training loss: 3.3029026985168457
Validation loss: 2.875015695889791

Epoch: 6| Step: 5
Training loss: 2.8400070667266846
Validation loss: 2.8693217833836875

Epoch: 6| Step: 6
Training loss: 2.7406158447265625
Validation loss: 2.8667884270350137

Epoch: 6| Step: 7
Training loss: 3.0648746490478516
Validation loss: 2.8630537589391074

Epoch: 6| Step: 8
Training loss: 2.815297842025757
Validation loss: 2.8586443662643433

Epoch: 6| Step: 9
Training loss: 3.5735597610473633
Validation loss: 2.856757322947184

Epoch: 6| Step: 10
Training loss: 3.035484552383423
Validation loss: 2.853759487469991

Epoch: 6| Step: 11
Training loss: 3.247328758239746
Validation loss: 2.850514054298401

Epoch: 6| Step: 12
Training loss: 2.7067012786865234
Validation loss: 2.8497382402420044

Epoch: 6| Step: 13
Training loss: 2.5802719593048096
Validation loss: 2.8412599563598633

Epoch: 34| Step: 0
Training loss: 2.826256275177002
Validation loss: 2.8363856077194214

Epoch: 6| Step: 1
Training loss: 3.7954459190368652
Validation loss: 2.833356261253357

Epoch: 6| Step: 2
Training loss: 2.184957981109619
Validation loss: 2.831256548563639

Epoch: 6| Step: 3
Training loss: 3.1013717651367188
Validation loss: 2.827032486597697

Epoch: 6| Step: 4
Training loss: 3.424565315246582
Validation loss: 2.8232560555140176

Epoch: 6| Step: 5
Training loss: 2.971980094909668
Validation loss: 2.819278120994568

Epoch: 6| Step: 6
Training loss: 3.3367185592651367
Validation loss: 2.8170663118362427

Epoch: 6| Step: 7
Training loss: 2.7348053455352783
Validation loss: 2.813000281651815

Epoch: 6| Step: 8
Training loss: 3.6352782249450684
Validation loss: 2.8085873126983643

Epoch: 6| Step: 9
Training loss: 2.6470839977264404
Validation loss: 2.8052501678466797

Epoch: 6| Step: 10
Training loss: 2.6075477600097656
Validation loss: 2.8010536432266235

Epoch: 6| Step: 11
Training loss: 3.06217622756958
Validation loss: 2.797926902770996

Epoch: 6| Step: 12
Training loss: 3.4056460857391357
Validation loss: 2.7954790592193604

Epoch: 6| Step: 13
Training loss: 2.5928280353546143
Validation loss: 2.7934820652008057

Epoch: 35| Step: 0
Training loss: 2.9453675746917725
Validation loss: 2.791736880938212

Epoch: 6| Step: 1
Training loss: 3.0967488288879395
Validation loss: 2.7868665059407554

Epoch: 6| Step: 2
Training loss: 3.208031177520752
Validation loss: 2.7841673692067466

Epoch: 6| Step: 3
Training loss: 2.583003520965576
Validation loss: 2.780204693476359

Epoch: 6| Step: 4
Training loss: 3.3838443756103516
Validation loss: 2.777514338493347

Epoch: 6| Step: 5
Training loss: 2.542187213897705
Validation loss: 2.7722970247268677

Epoch: 6| Step: 6
Training loss: 3.5129923820495605
Validation loss: 2.769830902417501

Epoch: 6| Step: 7
Training loss: 2.7832796573638916
Validation loss: 2.7668780088424683

Epoch: 6| Step: 8
Training loss: 2.6141390800476074
Validation loss: 2.7630301316579184

Epoch: 6| Step: 9
Training loss: 2.9227144718170166
Validation loss: 2.760232170422872

Epoch: 6| Step: 10
Training loss: 2.3066277503967285
Validation loss: 2.757464607556661

Epoch: 6| Step: 11
Training loss: 3.367600202560425
Validation loss: 2.756063143412272

Epoch: 6| Step: 12
Training loss: 3.1362507343292236
Validation loss: 2.7525473833084106

Epoch: 6| Step: 13
Training loss: 3.288583993911743
Validation loss: 2.7482473452885947

Epoch: 36| Step: 0
Training loss: 2.6886000633239746
Validation loss: 2.7468011379241943

Epoch: 6| Step: 1
Training loss: 3.7160518169403076
Validation loss: 2.741752783457438

Epoch: 6| Step: 2
Training loss: 2.569908380508423
Validation loss: 2.7423153718312583

Epoch: 6| Step: 3
Training loss: 3.2019665241241455
Validation loss: 2.7348793347676597

Epoch: 6| Step: 4
Training loss: 3.2285890579223633
Validation loss: 2.7323758602142334

Epoch: 6| Step: 5
Training loss: 2.4389305114746094
Validation loss: 2.727365732192993

Epoch: 6| Step: 6
Training loss: 3.483067035675049
Validation loss: 2.7238831321398416

Epoch: 6| Step: 7
Training loss: 2.6067886352539062
Validation loss: 2.722091635068258

Epoch: 6| Step: 8
Training loss: 3.1566545963287354
Validation loss: 2.7195454438527427

Epoch: 6| Step: 9
Training loss: 2.490149974822998
Validation loss: 2.714668850104014

Epoch: 6| Step: 10
Training loss: 2.15244460105896
Validation loss: 2.711078325907389

Epoch: 6| Step: 11
Training loss: 3.7326407432556152
Validation loss: 2.707437594731649

Epoch: 6| Step: 12
Training loss: 3.240074634552002
Validation loss: 2.704587380091349

Epoch: 6| Step: 13
Training loss: 2.3226542472839355
Validation loss: 2.6997737288475037

Epoch: 37| Step: 0
Training loss: 2.921354293823242
Validation loss: 2.6965901851654053

Epoch: 6| Step: 1
Training loss: 2.3198862075805664
Validation loss: 2.7009570201238

Epoch: 6| Step: 2
Training loss: 3.1591835021972656
Validation loss: 2.697322130203247

Epoch: 6| Step: 3
Training loss: 2.619638442993164
Validation loss: 2.7125152548154197

Epoch: 6| Step: 4
Training loss: 2.8550636768341064
Validation loss: 2.689277172088623

Epoch: 6| Step: 5
Training loss: 2.7732489109039307
Validation loss: 2.681198239326477

Epoch: 6| Step: 6
Training loss: 2.897841691970825
Validation loss: 2.6774573723475137

Epoch: 6| Step: 7
Training loss: 3.187528610229492
Validation loss: 2.674679378668467

Epoch: 6| Step: 8
Training loss: 3.704162836074829
Validation loss: 2.6763437191645303

Epoch: 6| Step: 9
Training loss: 2.7494606971740723
Validation loss: 2.6751453479131064

Epoch: 6| Step: 10
Training loss: 3.4687893390655518
Validation loss: 2.6720930337905884

Epoch: 6| Step: 11
Training loss: 2.692654609680176
Validation loss: 2.6676419377326965

Epoch: 6| Step: 12
Training loss: 2.493811845779419
Validation loss: 2.66360334555308

Epoch: 6| Step: 13
Training loss: 2.546635150909424
Validation loss: 2.659603754679362

Epoch: 38| Step: 0
Training loss: 3.084690570831299
Validation loss: 2.6550097465515137

Epoch: 6| Step: 1
Training loss: 2.7348999977111816
Validation loss: 2.6478896538416543

Epoch: 6| Step: 2
Training loss: 2.1407108306884766
Validation loss: 2.643489360809326

Epoch: 6| Step: 3
Training loss: 2.7509450912475586
Validation loss: 2.6382786432902017

Epoch: 6| Step: 4
Training loss: 2.966097116470337
Validation loss: 2.635374108950297

Epoch: 6| Step: 5
Training loss: 3.242023468017578
Validation loss: 2.6336336533228555

Epoch: 6| Step: 6
Training loss: 2.7954297065734863
Validation loss: 2.629896799723307

Epoch: 6| Step: 7
Training loss: 2.926992416381836
Validation loss: 2.625847498575846

Epoch: 6| Step: 8
Training loss: 3.3200442790985107
Validation loss: 2.62630565961202

Epoch: 6| Step: 9
Training loss: 3.4198226928710938
Validation loss: 2.620883822441101

Epoch: 6| Step: 10
Training loss: 2.6204209327697754
Validation loss: 2.622396151224772

Epoch: 6| Step: 11
Training loss: 2.5104429721832275
Validation loss: 2.624369422594706

Epoch: 6| Step: 12
Training loss: 2.6602861881256104
Validation loss: 2.6160505612691245

Epoch: 6| Step: 13
Training loss: 2.4283194541931152
Validation loss: 2.6072307030359902

Epoch: 39| Step: 0
Training loss: 3.1213018894195557
Validation loss: 2.6134135921796164

Epoch: 6| Step: 1
Training loss: 2.3220272064208984
Validation loss: 2.609776337941488

Epoch: 6| Step: 2
Training loss: 2.8620262145996094
Validation loss: 2.6095129251480103

Epoch: 6| Step: 3
Training loss: 3.1524882316589355
Validation loss: 2.6061777671178183

Epoch: 6| Step: 4
Training loss: 2.343060255050659
Validation loss: 2.5921716690063477

Epoch: 6| Step: 5
Training loss: 2.705888509750366
Validation loss: 2.58793838818868

Epoch: 6| Step: 6
Training loss: 3.314426898956299
Validation loss: 2.5802971522013345

Epoch: 6| Step: 7
Training loss: 3.504359722137451
Validation loss: 2.580546498298645

Epoch: 6| Step: 8
Training loss: 3.2532215118408203
Validation loss: 2.5770218769709268

Epoch: 6| Step: 9
Training loss: 2.1248209476470947
Validation loss: 2.5770349899927774

Epoch: 6| Step: 10
Training loss: 2.5036768913269043
Validation loss: 2.5691028038660684

Epoch: 6| Step: 11
Training loss: 2.6277520656585693
Validation loss: 2.5691140492757163

Epoch: 6| Step: 12
Training loss: 2.3737361431121826
Validation loss: 2.565517465273539

Epoch: 6| Step: 13
Training loss: 2.709273338317871
Validation loss: 2.5643037954966226

Epoch: 40| Step: 0
Training loss: 1.997415542602539
Validation loss: 2.558234453201294

Epoch: 6| Step: 1
Training loss: 2.3188326358795166
Validation loss: 2.5558517376581826

Epoch: 6| Step: 2
Training loss: 3.279041290283203
Validation loss: 2.551381309827169

Epoch: 6| Step: 3
Training loss: 2.7879292964935303
Validation loss: 2.5469312270482383

Epoch: 6| Step: 4
Training loss: 3.1845755577087402
Validation loss: 2.545102834701538

Epoch: 6| Step: 5
Training loss: 2.234715461730957
Validation loss: 2.540093104044596

Epoch: 6| Step: 6
Training loss: 3.5107009410858154
Validation loss: 2.541521688302358

Epoch: 6| Step: 7
Training loss: 2.655796527862549
Validation loss: 2.5303940773010254

Epoch: 6| Step: 8
Training loss: 3.330655097961426
Validation loss: 2.532724618911743

Epoch: 6| Step: 9
Training loss: 2.822707414627075
Validation loss: 2.528948267300924

Epoch: 6| Step: 10
Training loss: 2.506333827972412
Validation loss: 2.527149518330892

Epoch: 6| Step: 11
Training loss: 2.6490237712860107
Validation loss: 2.5235186020533242

Epoch: 6| Step: 12
Training loss: 2.467165946960449
Validation loss: 2.5206027030944824

Epoch: 6| Step: 13
Training loss: 2.534440755844116
Validation loss: 2.5169332027435303

Epoch: 41| Step: 0
Training loss: 2.786867141723633
Validation loss: 2.5109560092290244

Epoch: 6| Step: 1
Training loss: 2.6705329418182373
Validation loss: 2.50832200050354

Epoch: 6| Step: 2
Training loss: 2.314067840576172
Validation loss: 2.50712917248408

Epoch: 6| Step: 3
Training loss: 2.7763888835906982
Validation loss: 2.506336053212484

Epoch: 6| Step: 4
Training loss: 2.3076653480529785
Validation loss: 2.5044139623641968

Epoch: 6| Step: 5
Training loss: 3.2865097522735596
Validation loss: 2.4994907776514688

Epoch: 6| Step: 6
Training loss: 2.612687826156616
Validation loss: 2.500328699747721

Epoch: 6| Step: 7
Training loss: 3.2631819248199463
Validation loss: 2.491220553716024

Epoch: 6| Step: 8
Training loss: 2.555562973022461
Validation loss: 2.4886961778004966

Epoch: 6| Step: 9
Training loss: 2.9561214447021484
Validation loss: 2.482411881287893

Epoch: 6| Step: 10
Training loss: 2.544682025909424
Validation loss: 2.4822402397791543

Epoch: 6| Step: 11
Training loss: 2.2684266567230225
Validation loss: 2.4826958576838174

Epoch: 6| Step: 12
Training loss: 3.1613924503326416
Validation loss: 2.4824378490448

Epoch: 6| Step: 13
Training loss: 2.1698122024536133
Validation loss: 2.479215979576111

Epoch: 42| Step: 0
Training loss: 2.8782267570495605
Validation loss: 2.474639574686686

Epoch: 6| Step: 1
Training loss: 1.8087598085403442
Validation loss: 2.4719789822896323

Epoch: 6| Step: 2
Training loss: 3.0650243759155273
Validation loss: 2.468904217084249

Epoch: 6| Step: 3
Training loss: 2.574738025665283
Validation loss: 2.46458892027537

Epoch: 6| Step: 4
Training loss: 2.659295082092285
Validation loss: 2.464734156926473

Epoch: 6| Step: 5
Training loss: 2.7398266792297363
Validation loss: 2.458848158518473

Epoch: 6| Step: 6
Training loss: 2.106473207473755
Validation loss: 2.4568373958269754

Epoch: 6| Step: 7
Training loss: 2.91780424118042
Validation loss: 2.454900105794271

Epoch: 6| Step: 8
Training loss: 2.7180118560791016
Validation loss: 2.450058400630951

Epoch: 6| Step: 9
Training loss: 3.0596766471862793
Validation loss: 2.4443409045537314

Epoch: 6| Step: 10
Training loss: 2.441746950149536
Validation loss: 2.441156586011251

Epoch: 6| Step: 11
Training loss: 2.1706595420837402
Validation loss: 2.440926432609558

Epoch: 6| Step: 12
Training loss: 3.32269287109375
Validation loss: 2.4420291582743325

Epoch: 6| Step: 13
Training loss: 2.430666923522949
Validation loss: 2.4451840122540793

Epoch: 43| Step: 0
Training loss: 2.493319511413574
Validation loss: 2.4406126737594604

Epoch: 6| Step: 1
Training loss: 1.8401204347610474
Validation loss: 2.429471174875895

Epoch: 6| Step: 2
Training loss: 2.9838314056396484
Validation loss: 2.4410359064737954

Epoch: 6| Step: 3
Training loss: 2.448458671569824
Validation loss: 2.429452379544576

Epoch: 6| Step: 4
Training loss: 2.2326323986053467
Validation loss: 2.4120216170946756

Epoch: 6| Step: 5
Training loss: 2.809767723083496
Validation loss: 2.411437153816223

Epoch: 6| Step: 6
Training loss: 2.8235151767730713
Validation loss: 2.414115031560262

Epoch: 6| Step: 7
Training loss: 2.2318503856658936
Validation loss: 2.4125515818595886

Epoch: 6| Step: 8
Training loss: 2.8055644035339355
Validation loss: 2.414519210656484

Epoch: 6| Step: 9
Training loss: 2.442568302154541
Validation loss: 2.4106903870900473

Epoch: 6| Step: 10
Training loss: 2.7745957374572754
Validation loss: 2.40798811117808

Epoch: 6| Step: 11
Training loss: 2.6415560245513916
Validation loss: 2.407266696294149

Epoch: 6| Step: 12
Training loss: 2.8202428817749023
Validation loss: 2.4024078051249185

Epoch: 6| Step: 13
Training loss: 2.9451537132263184
Validation loss: 2.4004984299341836

Epoch: 44| Step: 0
Training loss: 2.4743893146514893
Validation loss: 2.3976014057795205

Epoch: 6| Step: 1
Training loss: 2.543269395828247
Validation loss: 2.3946622808774314

Epoch: 6| Step: 2
Training loss: 2.885371685028076
Validation loss: 2.3850912849108377

Epoch: 6| Step: 3
Training loss: 2.37998628616333
Validation loss: 2.38486381371816

Epoch: 6| Step: 4
Training loss: 2.8497371673583984
Validation loss: 2.3812150955200195

Epoch: 6| Step: 5
Training loss: 1.857867956161499
Validation loss: 2.3748422265052795

Epoch: 6| Step: 6
Training loss: 2.9804439544677734
Validation loss: 2.3695340951283774

Epoch: 6| Step: 7
Training loss: 2.4356675148010254
Validation loss: 2.3697588046391806

Epoch: 6| Step: 8
Training loss: 2.9426636695861816
Validation loss: 2.3568859895070395

Epoch: 6| Step: 9
Training loss: 2.4465651512145996
Validation loss: 2.3653934796651206

Epoch: 6| Step: 10
Training loss: 2.8876566886901855
Validation loss: 2.361096819241842

Epoch: 6| Step: 11
Training loss: 2.853485345840454
Validation loss: 2.359004855155945

Epoch: 6| Step: 12
Training loss: 2.011289596557617
Validation loss: 2.3601576685905457

Epoch: 6| Step: 13
Training loss: 2.157886028289795
Validation loss: 2.358060598373413

Epoch: 45| Step: 0
Training loss: 2.3327088356018066
Validation loss: 2.3553358713785806

Epoch: 6| Step: 1
Training loss: 2.4104866981506348
Validation loss: 2.353999356428782

Epoch: 6| Step: 2
Training loss: 2.7148241996765137
Validation loss: 2.351731816927592

Epoch: 6| Step: 3
Training loss: 2.4287195205688477
Validation loss: 2.350776513417562

Epoch: 6| Step: 4
Training loss: 1.9357248544692993
Validation loss: 2.3423609336217246

Epoch: 6| Step: 5
Training loss: 2.391397714614868
Validation loss: 2.3411728938420615

Epoch: 6| Step: 6
Training loss: 2.455288887023926
Validation loss: 2.331552823384603

Epoch: 6| Step: 7
Training loss: 2.0007474422454834
Validation loss: 2.331751267115275

Epoch: 6| Step: 8
Training loss: 3.0585131645202637
Validation loss: 2.333618938922882

Epoch: 6| Step: 9
Training loss: 2.5553219318389893
Validation loss: 2.330104390780131

Epoch: 6| Step: 10
Training loss: 2.31559419631958
Validation loss: 2.33767823378245

Epoch: 6| Step: 11
Training loss: 2.386162519454956
Validation loss: 2.3301071325937905

Epoch: 6| Step: 12
Training loss: 2.9135289192199707
Validation loss: 2.3234627842903137

Epoch: 6| Step: 13
Training loss: 3.1931896209716797
Validation loss: 2.314651687939962

Epoch: 46| Step: 0
Training loss: 2.577235460281372
Validation loss: 2.3139283061027527

Epoch: 6| Step: 1
Training loss: 2.456366777420044
Validation loss: 2.317752202351888

Epoch: 6| Step: 2
Training loss: 2.555187225341797
Validation loss: 2.3149645725886026

Epoch: 6| Step: 3
Training loss: 2.1021862030029297
Validation loss: 2.31152073542277

Epoch: 6| Step: 4
Training loss: 2.778748035430908
Validation loss: 2.312696655591329

Epoch: 6| Step: 5
Training loss: 2.7642643451690674
Validation loss: 2.309562603632609

Epoch: 6| Step: 6
Training loss: 2.3748459815979004
Validation loss: 2.307846486568451

Epoch: 6| Step: 7
Training loss: 2.4055747985839844
Validation loss: 2.309771498044332

Epoch: 6| Step: 8
Training loss: 2.7302465438842773
Validation loss: 2.307456851005554

Epoch: 6| Step: 9
Training loss: 2.304248809814453
Validation loss: 2.3058088620503745

Epoch: 6| Step: 10
Training loss: 2.329885482788086
Validation loss: 2.295918564001719

Epoch: 6| Step: 11
Training loss: 2.146209478378296
Validation loss: 2.2940486669540405

Epoch: 6| Step: 12
Training loss: 2.4905452728271484
Validation loss: 2.2858843008677163

Epoch: 6| Step: 13
Training loss: 2.5279407501220703
Validation loss: 2.2876352667808533

Epoch: 47| Step: 0
Training loss: 2.622972011566162
Validation loss: 2.283185839653015

Epoch: 6| Step: 1
Training loss: 2.146641254425049
Validation loss: 2.2818455894788108

Epoch: 6| Step: 2
Training loss: 2.7995824813842773
Validation loss: 2.2838222980499268

Epoch: 6| Step: 3
Training loss: 2.024721145629883
Validation loss: 2.279691537221273

Epoch: 6| Step: 4
Training loss: 2.601247787475586
Validation loss: 2.2743590474128723

Epoch: 6| Step: 5
Training loss: 2.2967982292175293
Validation loss: 2.271687388420105

Epoch: 6| Step: 6
Training loss: 1.7207221984863281
Validation loss: 2.264875590801239

Epoch: 6| Step: 7
Training loss: 2.7461249828338623
Validation loss: 2.264669199784597

Epoch: 6| Step: 8
Training loss: 2.53222393989563
Validation loss: 2.2647206584612527

Epoch: 6| Step: 9
Training loss: 2.301442861557007
Validation loss: 2.2547353903452554

Epoch: 6| Step: 10
Training loss: 2.5659847259521484
Validation loss: 2.2537944118181863

Epoch: 6| Step: 11
Training loss: 2.41502046585083
Validation loss: 2.247572660446167

Epoch: 6| Step: 12
Training loss: 2.3108365535736084
Validation loss: 2.2449352542559304

Epoch: 6| Step: 13
Training loss: 2.919037342071533
Validation loss: 2.248799502849579

Epoch: 48| Step: 0
Training loss: 2.0149922370910645
Validation loss: 2.2520134846369424

Epoch: 6| Step: 1
Training loss: 3.157177209854126
Validation loss: 2.24752938747406

Epoch: 6| Step: 2
Training loss: 2.476969003677368
Validation loss: 2.236969470977783

Epoch: 6| Step: 3
Training loss: 1.742570161819458
Validation loss: 2.2398022214571633

Epoch: 6| Step: 4
Training loss: 2.897665500640869
Validation loss: 2.2357836763064065

Epoch: 6| Step: 5
Training loss: 2.419875144958496
Validation loss: 2.2401036620140076

Epoch: 6| Step: 6
Training loss: 2.505573034286499
Validation loss: 2.2392218708992004

Epoch: 6| Step: 7
Training loss: 2.322694778442383
Validation loss: 2.235444943110148

Epoch: 6| Step: 8
Training loss: 2.2415783405303955
Validation loss: 2.239287177721659

Epoch: 6| Step: 9
Training loss: 2.444652557373047
Validation loss: 2.234394073486328

Epoch: 6| Step: 10
Training loss: 2.241849660873413
Validation loss: 2.230509082476298

Epoch: 6| Step: 11
Training loss: 2.0012357234954834
Validation loss: 2.2202105124791465

Epoch: 6| Step: 12
Training loss: 2.3052942752838135
Validation loss: 2.2240891655286155

Epoch: 6| Step: 13
Training loss: 2.81874942779541
Validation loss: 2.2198804219563804

Epoch: 49| Step: 0
Training loss: 2.355255126953125
Validation loss: 2.215361396471659

Epoch: 6| Step: 1
Training loss: 2.0433030128479004
Validation loss: 2.2102767626444497

Epoch: 6| Step: 2
Training loss: 2.1085968017578125
Validation loss: 2.214223941167196

Epoch: 6| Step: 3
Training loss: 2.126765012741089
Validation loss: 2.2128764192263284

Epoch: 6| Step: 4
Training loss: 2.8118205070495605
Validation loss: 2.202337900797526

Epoch: 6| Step: 5
Training loss: 3.1196112632751465
Validation loss: 2.1993490854899087

Epoch: 6| Step: 6
Training loss: 1.8001465797424316
Validation loss: 2.198384960492452

Epoch: 6| Step: 7
Training loss: 2.7847392559051514
Validation loss: 2.199804703394572

Epoch: 6| Step: 8
Training loss: 1.5581679344177246
Validation loss: 2.1972970962524414

Epoch: 6| Step: 9
Training loss: 2.175053596496582
Validation loss: 2.1881465911865234

Epoch: 6| Step: 10
Training loss: 2.679076671600342
Validation loss: 2.198075830936432

Epoch: 6| Step: 11
Training loss: 2.5065388679504395
Validation loss: 2.191896994908651

Epoch: 6| Step: 12
Training loss: 1.9883522987365723
Validation loss: 2.176250716050466

Epoch: 6| Step: 13
Training loss: 2.90419340133667
Validation loss: 2.1845489144325256

Epoch: 50| Step: 0
Training loss: 2.0071492195129395
Validation loss: 2.180993636449178

Epoch: 6| Step: 1
Training loss: 2.163322925567627
Validation loss: 2.1837313771247864

Epoch: 6| Step: 2
Training loss: 3.2187719345092773
Validation loss: 2.1761422952016196

Epoch: 6| Step: 3
Training loss: 2.291708469390869
Validation loss: 2.177122871081034

Epoch: 6| Step: 4
Training loss: 2.8121275901794434
Validation loss: 2.1809217731157937

Epoch: 6| Step: 5
Training loss: 2.3968586921691895
Validation loss: 2.177634835243225

Epoch: 6| Step: 6
Training loss: 2.0869030952453613
Validation loss: 2.1699398358662925

Epoch: 6| Step: 7
Training loss: 1.724946141242981
Validation loss: 2.1716293891270957

Epoch: 6| Step: 8
Training loss: 1.9065977334976196
Validation loss: 2.170211652914683

Epoch: 6| Step: 9
Training loss: 2.4292898178100586
Validation loss: 2.177633762359619

Epoch: 6| Step: 10
Training loss: 2.2824151515960693
Validation loss: 2.166149655977885

Epoch: 6| Step: 11
Training loss: 2.791409969329834
Validation loss: 2.17019921541214

Epoch: 6| Step: 12
Training loss: 2.052281379699707
Validation loss: 2.164973417917887

Epoch: 6| Step: 13
Training loss: 2.2863011360168457
Validation loss: 2.1634666323661804

Epoch: 51| Step: 0
Training loss: 1.462207317352295
Validation loss: 2.1638052463531494

Epoch: 6| Step: 1
Training loss: 2.2913503646850586
Validation loss: 2.158659815788269

Epoch: 6| Step: 2
Training loss: 2.4749386310577393
Validation loss: 2.160329043865204

Epoch: 6| Step: 3
Training loss: 1.5183336734771729
Validation loss: 2.163025200366974

Epoch: 6| Step: 4
Training loss: 2.1319937705993652
Validation loss: 2.158983508745829

Epoch: 6| Step: 5
Training loss: 2.843069553375244
Validation loss: 2.155483841896057

Epoch: 6| Step: 6
Training loss: 1.690143346786499
Validation loss: 2.1490176717440286

Epoch: 6| Step: 7
Training loss: 2.088980197906494
Validation loss: 2.1440406243006387

Epoch: 6| Step: 8
Training loss: 2.602108955383301
Validation loss: 2.1497129003206887

Epoch: 6| Step: 9
Training loss: 3.0363950729370117
Validation loss: 2.150291999181112

Epoch: 6| Step: 10
Training loss: 2.766252040863037
Validation loss: 2.1415039698282876

Epoch: 6| Step: 11
Training loss: 2.4106695652008057
Validation loss: 2.1392310857772827

Epoch: 6| Step: 12
Training loss: 2.559723377227783
Validation loss: 2.1388930082321167

Epoch: 6| Step: 13
Training loss: 2.385105609893799
Validation loss: 2.1467169920603433

Epoch: 52| Step: 0
Training loss: 2.236147880554199
Validation loss: 2.1441931327184043

Epoch: 6| Step: 1
Training loss: 2.3229684829711914
Validation loss: 2.1396471858024597

Epoch: 6| Step: 2
Training loss: 2.1606545448303223
Validation loss: 2.1430548230806985

Epoch: 6| Step: 3
Training loss: 2.0788683891296387
Validation loss: 2.13946525255839

Epoch: 6| Step: 4
Training loss: 2.3729896545410156
Validation loss: 2.1444079875946045

Epoch: 6| Step: 5
Training loss: 2.407047748565674
Validation loss: 2.1377779642740884

Epoch: 6| Step: 6
Training loss: 1.9171292781829834
Validation loss: 2.132437288761139

Epoch: 6| Step: 7
Training loss: 1.9407658576965332
Validation loss: 2.147455314795176

Epoch: 6| Step: 8
Training loss: 2.8788771629333496
Validation loss: 2.1297433177630105

Epoch: 6| Step: 9
Training loss: 2.022674083709717
Validation loss: 2.128679653008779

Epoch: 6| Step: 10
Training loss: 2.35513973236084
Validation loss: 2.1337381402651467

Epoch: 6| Step: 11
Training loss: 2.7750463485717773
Validation loss: 2.124044120311737

Epoch: 6| Step: 12
Training loss: 2.541455030441284
Validation loss: 2.1216448148091636

Epoch: 6| Step: 13
Training loss: 1.9668889045715332
Validation loss: 2.1256101926167807

Epoch: 53| Step: 0
Training loss: 1.9365005493164062
Validation loss: 2.126624564329783

Epoch: 6| Step: 1
Training loss: 2.381484031677246
Validation loss: 2.1178052028020224

Epoch: 6| Step: 2
Training loss: 2.159609317779541
Validation loss: 2.1183655858039856

Epoch: 6| Step: 3
Training loss: 2.1521472930908203
Validation loss: 2.1170352498690286

Epoch: 6| Step: 4
Training loss: 2.39949631690979
Validation loss: 2.1168885231018066

Epoch: 6| Step: 5
Training loss: 1.5525145530700684
Validation loss: 2.121384004751841

Epoch: 6| Step: 6
Training loss: 1.8365734815597534
Validation loss: 2.125321924686432

Epoch: 6| Step: 7
Training loss: 2.3125438690185547
Validation loss: 2.1277860005696616

Epoch: 6| Step: 8
Training loss: 2.594327688217163
Validation loss: 2.122938632965088

Epoch: 6| Step: 9
Training loss: 2.820383071899414
Validation loss: 2.12830126285553

Epoch: 6| Step: 10
Training loss: 2.6066689491271973
Validation loss: 2.1188897291819253

Epoch: 6| Step: 11
Training loss: 2.3141133785247803
Validation loss: 2.1176392833391824

Epoch: 6| Step: 12
Training loss: 2.2036921977996826
Validation loss: 2.1111492911974588

Epoch: 6| Step: 13
Training loss: 2.4933085441589355
Validation loss: 2.1072155435880027

Epoch: 54| Step: 0
Training loss: 2.6283702850341797
Validation loss: 2.124508281548818

Epoch: 6| Step: 1
Training loss: 2.3353025913238525
Validation loss: 2.121200124422709

Epoch: 6| Step: 2
Training loss: 1.904795527458191
Validation loss: 2.128671149412791

Epoch: 6| Step: 3
Training loss: 2.3566277027130127
Validation loss: 2.1285590330759683

Epoch: 6| Step: 4
Training loss: 2.459700584411621
Validation loss: 2.1304470101992288

Epoch: 6| Step: 5
Training loss: 2.2525205612182617
Validation loss: 2.128233313560486

Epoch: 6| Step: 6
Training loss: 1.906229853630066
Validation loss: 2.1284183065096536

Epoch: 6| Step: 7
Training loss: 2.5207362174987793
Validation loss: 2.1248398423194885

Epoch: 6| Step: 8
Training loss: 2.400791645050049
Validation loss: 2.115501860777537

Epoch: 6| Step: 9
Training loss: 1.9304026365280151
Validation loss: 2.1017070213953652

Epoch: 6| Step: 10
Training loss: 2.381880760192871
Validation loss: 2.099494695663452

Epoch: 6| Step: 11
Training loss: 2.006575584411621
Validation loss: 2.092320740222931

Epoch: 6| Step: 12
Training loss: 2.2143430709838867
Validation loss: 2.088749567667643

Epoch: 6| Step: 13
Training loss: 2.521758556365967
Validation loss: 2.099010487397512

Epoch: 55| Step: 0
Training loss: 2.9766454696655273
Validation loss: 2.106671949227651

Epoch: 6| Step: 1
Training loss: 2.0405163764953613
Validation loss: 2.1014411052068076

Epoch: 6| Step: 2
Training loss: 2.0389723777770996
Validation loss: 2.1018200119336448

Epoch: 6| Step: 3
Training loss: 1.8897449970245361
Validation loss: 2.0908798774083457

Epoch: 6| Step: 4
Training loss: 1.639155387878418
Validation loss: 2.091627379258474

Epoch: 6| Step: 5
Training loss: 2.9385433197021484
Validation loss: 2.0934377113978067

Epoch: 6| Step: 6
Training loss: 1.768937587738037
Validation loss: 2.093312382698059

Epoch: 6| Step: 7
Training loss: 2.5002543926239014
Validation loss: 2.09483136733373

Epoch: 6| Step: 8
Training loss: 2.219430446624756
Validation loss: 2.0893062154452005

Epoch: 6| Step: 9
Training loss: 2.6228013038635254
Validation loss: 2.0938228170077005

Epoch: 6| Step: 10
Training loss: 2.4938201904296875
Validation loss: 2.0901801387468972

Epoch: 6| Step: 11
Training loss: 1.941418170928955
Validation loss: 2.091957688331604

Epoch: 6| Step: 12
Training loss: 2.24111270904541
Validation loss: 2.086049179236094

Epoch: 6| Step: 13
Training loss: 2.1837596893310547
Validation loss: 2.085013469060262

Epoch: 56| Step: 0
Training loss: 2.005930185317993
Validation loss: 2.083826204140981

Epoch: 6| Step: 1
Training loss: 2.5596165657043457
Validation loss: 2.074271539847056

Epoch: 6| Step: 2
Training loss: 2.523552417755127
Validation loss: 2.0779410004615784

Epoch: 6| Step: 3
Training loss: 2.102250099182129
Validation loss: 2.082678973674774

Epoch: 6| Step: 4
Training loss: 1.844071388244629
Validation loss: 2.073898216088613

Epoch: 6| Step: 5
Training loss: 2.2706732749938965
Validation loss: 2.072846472263336

Epoch: 6| Step: 6
Training loss: 1.7464512586593628
Validation loss: 2.0811394651730857

Epoch: 6| Step: 7
Training loss: 2.412219524383545
Validation loss: 2.0688067277272544

Epoch: 6| Step: 8
Training loss: 2.3112730979919434
Validation loss: 2.078823725382487

Epoch: 6| Step: 9
Training loss: 1.9875848293304443
Validation loss: 2.0809829433759055

Epoch: 6| Step: 10
Training loss: 2.4548866748809814
Validation loss: 2.0853430032730103

Epoch: 6| Step: 11
Training loss: 2.1855645179748535
Validation loss: 2.068968971570333

Epoch: 6| Step: 12
Training loss: 2.3783364295959473
Validation loss: 2.073746065298716

Epoch: 6| Step: 13
Training loss: 2.5938167572021484
Validation loss: 2.067075808842977

Epoch: 57| Step: 0
Training loss: 1.9794517755508423
Validation loss: 2.077995936075846

Epoch: 6| Step: 1
Training loss: 2.346161365509033
Validation loss: 2.080289383729299

Epoch: 6| Step: 2
Training loss: 2.3241817951202393
Validation loss: 2.0883448322614035

Epoch: 6| Step: 3
Training loss: 2.1412949562072754
Validation loss: 2.082655648390452

Epoch: 6| Step: 4
Training loss: 2.02453351020813
Validation loss: 2.074659744898478

Epoch: 6| Step: 5
Training loss: 1.8442013263702393
Validation loss: 2.070799708366394

Epoch: 6| Step: 6
Training loss: 2.429914951324463
Validation loss: 2.063259760538737

Epoch: 6| Step: 7
Training loss: 2.7361257076263428
Validation loss: 2.0535868604977927

Epoch: 6| Step: 8
Training loss: 1.913358211517334
Validation loss: 2.057786703109741

Epoch: 6| Step: 9
Training loss: 2.5674052238464355
Validation loss: 2.067242443561554

Epoch: 6| Step: 10
Training loss: 2.1049392223358154
Validation loss: 2.0684039195378623

Epoch: 6| Step: 11
Training loss: 2.300703525543213
Validation loss: 2.072598139444987

Epoch: 6| Step: 12
Training loss: 2.012840986251831
Validation loss: 2.0725444555282593

Epoch: 6| Step: 13
Training loss: 2.7669122219085693
Validation loss: 2.057616889476776

Epoch: 58| Step: 0
Training loss: 2.3896448612213135
Validation loss: 2.044387479623159

Epoch: 6| Step: 1
Training loss: 1.7010140419006348
Validation loss: 2.048970023790995

Epoch: 6| Step: 2
Training loss: 1.99208664894104
Validation loss: 2.060437321662903

Epoch: 6| Step: 3
Training loss: 2.0400357246398926
Validation loss: 2.0674261450767517

Epoch: 6| Step: 4
Training loss: 2.335932731628418
Validation loss: 2.071693241596222

Epoch: 6| Step: 5
Training loss: 1.85774564743042
Validation loss: 2.082751750946045

Epoch: 6| Step: 6
Training loss: 2.220973491668701
Validation loss: 2.081522047519684

Epoch: 6| Step: 7
Training loss: 2.075099468231201
Validation loss: 2.0903086264928183

Epoch: 6| Step: 8
Training loss: 2.1736531257629395
Validation loss: 2.090901811917623

Epoch: 6| Step: 9
Training loss: 2.429835319519043
Validation loss: 2.090800166130066

Epoch: 6| Step: 10
Training loss: 2.675140857696533
Validation loss: 2.083100378513336

Epoch: 6| Step: 11
Training loss: 2.2641782760620117
Validation loss: 2.079403499762217

Epoch: 6| Step: 12
Training loss: 2.5823938846588135
Validation loss: 2.069086790084839

Epoch: 6| Step: 13
Training loss: 2.718045711517334
Validation loss: 2.067231218020121

Epoch: 59| Step: 0
Training loss: 2.159759759902954
Validation loss: 2.0673940976460776

Epoch: 6| Step: 1
Training loss: 2.671353816986084
Validation loss: 2.0653233726819358

Epoch: 6| Step: 2
Training loss: 2.5702762603759766
Validation loss: 2.055249810218811

Epoch: 6| Step: 3
Training loss: 2.262242317199707
Validation loss: 2.0553876161575317

Epoch: 6| Step: 4
Training loss: 2.771738290786743
Validation loss: 2.057384749253591

Epoch: 6| Step: 5
Training loss: 1.8987687826156616
Validation loss: 2.0453545649846396

Epoch: 6| Step: 6
Training loss: 2.346808433532715
Validation loss: 2.0470074812571206

Epoch: 6| Step: 7
Training loss: 2.124746322631836
Validation loss: 2.039709289868673

Epoch: 6| Step: 8
Training loss: 2.125725030899048
Validation loss: 2.04353928565979

Epoch: 6| Step: 9
Training loss: 2.35758113861084
Validation loss: 2.0629232923189798

Epoch: 6| Step: 10
Training loss: 2.1374335289001465
Validation loss: 2.064169685045878

Epoch: 6| Step: 11
Training loss: 1.9752578735351562
Validation loss: 2.0757742524147034

Epoch: 6| Step: 12
Training loss: 2.566174030303955
Validation loss: 2.098616043726603

Epoch: 6| Step: 13
Training loss: 1.0789989233016968
Validation loss: 2.0987913608551025

Epoch: 60| Step: 0
Training loss: 2.1554222106933594
Validation loss: 2.1043872038523355

Epoch: 6| Step: 1
Training loss: 1.926557183265686
Validation loss: 2.0901678800582886

Epoch: 6| Step: 2
Training loss: 2.589611530303955
Validation loss: 2.049901088078817

Epoch: 6| Step: 3
Training loss: 1.4148138761520386
Validation loss: 2.032466729482015

Epoch: 6| Step: 4
Training loss: 2.3716201782226562
Validation loss: 2.040635883808136

Epoch: 6| Step: 5
Training loss: 2.7237753868103027
Validation loss: 2.0487066308657327

Epoch: 6| Step: 6
Training loss: 1.9600281715393066
Validation loss: 2.0546369155248008

Epoch: 6| Step: 7
Training loss: 2.02463436126709
Validation loss: 2.05588565270106

Epoch: 6| Step: 8
Training loss: 2.368452548980713
Validation loss: 2.057993551095327

Epoch: 6| Step: 9
Training loss: 2.092522144317627
Validation loss: 2.0602398117383323

Epoch: 6| Step: 10
Training loss: 2.3434653282165527
Validation loss: 2.0714840292930603

Epoch: 6| Step: 11
Training loss: 1.9597575664520264
Validation loss: 2.0751141707102456

Epoch: 6| Step: 12
Training loss: 2.3321518898010254
Validation loss: 2.0751564304033914

Epoch: 6| Step: 13
Training loss: 2.8270158767700195
Validation loss: 2.0765438278516135

Epoch: 61| Step: 0
Training loss: 2.3841819763183594
Validation loss: 2.0785682996114097

Epoch: 6| Step: 1
Training loss: 2.3855607509613037
Validation loss: 2.08257261912028

Epoch: 6| Step: 2
Training loss: 1.623313307762146
Validation loss: 2.0762043793996177

Epoch: 6| Step: 3
Training loss: 2.2569503784179688
Validation loss: 2.0801647305488586

Epoch: 6| Step: 4
Training loss: 2.0301547050476074
Validation loss: 2.075258731842041

Epoch: 6| Step: 5
Training loss: 2.1506595611572266
Validation loss: 2.0683305660883584

Epoch: 6| Step: 6
Training loss: 2.5985403060913086
Validation loss: 2.06426340341568

Epoch: 6| Step: 7
Training loss: 2.490736961364746
Validation loss: 2.0615660548210144

Epoch: 6| Step: 8
Training loss: 1.8828785419464111
Validation loss: 2.0547614296277366

Epoch: 6| Step: 9
Training loss: 2.218834161758423
Validation loss: 2.046988924344381

Epoch: 6| Step: 10
Training loss: 2.432840347290039
Validation loss: 2.0532758235931396

Epoch: 6| Step: 11
Training loss: 2.5430240631103516
Validation loss: 2.0485485593477883

Epoch: 6| Step: 12
Training loss: 2.0912389755249023
Validation loss: 2.049419363339742

Epoch: 6| Step: 13
Training loss: 2.4248945713043213
Validation loss: 2.0476481715838113

Epoch: 62| Step: 0
Training loss: 2.505523443222046
Validation loss: 2.043368637561798

Epoch: 6| Step: 1
Training loss: 1.9944562911987305
Validation loss: 2.038611570994059

Epoch: 6| Step: 2
Training loss: 1.8329789638519287
Validation loss: 2.036807378133138

Epoch: 6| Step: 3
Training loss: 1.9622645378112793
Validation loss: 2.0287542144457498

Epoch: 6| Step: 4
Training loss: 2.761314630508423
Validation loss: 2.0262122551600137

Epoch: 6| Step: 5
Training loss: 1.9799476861953735
Validation loss: 2.0356048742930093

Epoch: 6| Step: 6
Training loss: 2.3995707035064697
Validation loss: 2.029716908931732

Epoch: 6| Step: 7
Training loss: 2.879401922225952
Validation loss: 2.037051876386007

Epoch: 6| Step: 8
Training loss: 1.7855767011642456
Validation loss: 2.039403816064199

Epoch: 6| Step: 9
Training loss: 2.2405858039855957
Validation loss: 2.037717600663503

Epoch: 6| Step: 10
Training loss: 2.4493117332458496
Validation loss: 2.043562948703766

Epoch: 6| Step: 11
Training loss: 2.098417282104492
Validation loss: 2.0337078968683877

Epoch: 6| Step: 12
Training loss: 1.906014323234558
Validation loss: 2.036126653353373

Epoch: 6| Step: 13
Training loss: 2.1927976608276367
Validation loss: 2.0253141125043235

Epoch: 63| Step: 0
Training loss: 1.641223669052124
Validation loss: 2.028809587160746

Epoch: 6| Step: 1
Training loss: 2.262458562850952
Validation loss: 2.033137619495392

Epoch: 6| Step: 2
Training loss: 2.853303909301758
Validation loss: 2.0415899554888406

Epoch: 6| Step: 3
Training loss: 2.5935845375061035
Validation loss: 2.048920452594757

Epoch: 6| Step: 4
Training loss: 1.6215320825576782
Validation loss: 2.0450539588928223

Epoch: 6| Step: 5
Training loss: 2.3411269187927246
Validation loss: 2.052197019259135

Epoch: 6| Step: 6
Training loss: 2.0890936851501465
Validation loss: 2.0444787740707397

Epoch: 6| Step: 7
Training loss: 2.6771836280822754
Validation loss: 2.0443270802497864

Epoch: 6| Step: 8
Training loss: 1.7063288688659668
Validation loss: 2.042258381843567

Epoch: 6| Step: 9
Training loss: 1.938782811164856
Validation loss: 2.0466151436169944

Epoch: 6| Step: 10
Training loss: 2.3986284732818604
Validation loss: 2.0476356943448386

Epoch: 6| Step: 11
Training loss: 2.4486429691314697
Validation loss: 2.042397896448771

Epoch: 6| Step: 12
Training loss: 2.08976411819458
Validation loss: 2.0313310027122498

Epoch: 6| Step: 13
Training loss: 2.430190324783325
Validation loss: 2.017322222391764

Epoch: 64| Step: 0
Training loss: 2.108952045440674
Validation loss: 2.0221136609713235

Epoch: 6| Step: 1
Training loss: 2.4416818618774414
Validation loss: 2.0201149781545005

Epoch: 6| Step: 2
Training loss: 2.3890764713287354
Validation loss: 2.020193258921305

Epoch: 6| Step: 3
Training loss: 2.0578761100769043
Validation loss: 2.028808852036794

Epoch: 6| Step: 4
Training loss: 2.3567285537719727
Validation loss: 2.0348589221636453

Epoch: 6| Step: 5
Training loss: 2.464146614074707
Validation loss: 2.0391733249028525

Epoch: 6| Step: 6
Training loss: 1.5728614330291748
Validation loss: 2.0234214266141257

Epoch: 6| Step: 7
Training loss: 2.0264856815338135
Validation loss: 2.0218703746795654

Epoch: 6| Step: 8
Training loss: 2.6377339363098145
Validation loss: 2.0314296086629233

Epoch: 6| Step: 9
Training loss: 1.5484850406646729
Validation loss: 2.0332663655281067

Epoch: 6| Step: 10
Training loss: 2.067460536956787
Validation loss: 2.03411732117335

Epoch: 6| Step: 11
Training loss: 2.3544793128967285
Validation loss: 2.020794451236725

Epoch: 6| Step: 12
Training loss: 1.8807313442230225
Validation loss: 2.008484681447347

Epoch: 6| Step: 13
Training loss: 2.5681705474853516
Validation loss: 2.0244712233543396

Epoch: 65| Step: 0
Training loss: 1.5019257068634033
Validation loss: 2.0108665227890015

Epoch: 6| Step: 1
Training loss: 2.7231106758117676
Validation loss: 2.020126740137736

Epoch: 6| Step: 2
Training loss: 2.4409220218658447
Validation loss: 2.0135660767555237

Epoch: 6| Step: 3
Training loss: 2.154998540878296
Validation loss: 2.014494299888611

Epoch: 6| Step: 4
Training loss: 1.8593521118164062
Validation loss: 2.0177878538767495

Epoch: 6| Step: 5
Training loss: 1.9516491889953613
Validation loss: 2.0133840839068093

Epoch: 6| Step: 6
Training loss: 2.184539318084717
Validation loss: 2.007525324821472

Epoch: 6| Step: 7
Training loss: 3.053645610809326
Validation loss: 2.007532755533854

Epoch: 6| Step: 8
Training loss: 2.0085039138793945
Validation loss: 2.008353610833486

Epoch: 6| Step: 9
Training loss: 1.4803404808044434
Validation loss: 2.009358366330465

Epoch: 6| Step: 10
Training loss: 2.4287612438201904
Validation loss: 2.0190171798070273

Epoch: 6| Step: 11
Training loss: 2.4363112449645996
Validation loss: 2.0245545506477356

Epoch: 6| Step: 12
Training loss: 2.5662012100219727
Validation loss: 2.0123873949050903

Epoch: 6| Step: 13
Training loss: 1.87700617313385
Validation loss: 2.0154205362002053

Epoch: 66| Step: 0
Training loss: 2.2030975818634033
Validation loss: 2.0196406841278076

Epoch: 6| Step: 1
Training loss: 2.080566883087158
Validation loss: 2.0156284968058267

Epoch: 6| Step: 2
Training loss: 2.1608428955078125
Validation loss: 2.0130537947018943

Epoch: 6| Step: 3
Training loss: 2.025312662124634
Validation loss: 2.0038516918818154

Epoch: 6| Step: 4
Training loss: 1.4634923934936523
Validation loss: 2.019541064898173

Epoch: 6| Step: 5
Training loss: 2.0701613426208496
Validation loss: 2.0074756145477295

Epoch: 6| Step: 6
Training loss: 2.1776154041290283
Validation loss: 2.023327906926473

Epoch: 6| Step: 7
Training loss: 2.3084869384765625
Validation loss: 2.0199368794759116

Epoch: 6| Step: 8
Training loss: 2.5793204307556152
Validation loss: 2.0314401388168335

Epoch: 6| Step: 9
Training loss: 2.089397430419922
Validation loss: 2.0320344964663186

Epoch: 6| Step: 10
Training loss: 2.589010715484619
Validation loss: 2.0230886340141296

Epoch: 6| Step: 11
Training loss: 2.3577632904052734
Validation loss: 2.023179511229197

Epoch: 6| Step: 12
Training loss: 1.9071784019470215
Validation loss: 2.020123382409414

Epoch: 6| Step: 13
Training loss: 2.515979290008545
Validation loss: 2.0092405478159585

Epoch: 67| Step: 0
Training loss: 1.8997154235839844
Validation loss: 2.01425439119339

Epoch: 6| Step: 1
Training loss: 2.1710453033447266
Validation loss: 2.0217591921488443

Epoch: 6| Step: 2
Training loss: 2.408557891845703
Validation loss: 2.029357632001241

Epoch: 6| Step: 3
Training loss: 2.5667777061462402
Validation loss: 2.0345200896263123

Epoch: 6| Step: 4
Training loss: 1.7064003944396973
Validation loss: 2.0373857418696084

Epoch: 6| Step: 5
Training loss: 2.3460211753845215
Validation loss: 2.036705176035563

Epoch: 6| Step: 6
Training loss: 1.9428467750549316
Validation loss: 2.036800424257914

Epoch: 6| Step: 7
Training loss: 2.421934127807617
Validation loss: 2.028288781642914

Epoch: 6| Step: 8
Training loss: 1.7456393241882324
Validation loss: 2.0366495847702026

Epoch: 6| Step: 9
Training loss: 2.2478811740875244
Validation loss: 2.0381168723106384

Epoch: 6| Step: 10
Training loss: 2.6196084022521973
Validation loss: 2.040351708730062

Epoch: 6| Step: 11
Training loss: 2.4160118103027344
Validation loss: 2.033438722292582

Epoch: 6| Step: 12
Training loss: 2.0845699310302734
Validation loss: 2.0446498692035675

Epoch: 6| Step: 13
Training loss: 2.3398869037628174
Validation loss: 2.037038246790568

Epoch: 68| Step: 0
Training loss: 2.2592849731445312
Validation loss: 2.03579443693161

Epoch: 6| Step: 1
Training loss: 1.9730250835418701
Validation loss: 2.0403008262316384

Epoch: 6| Step: 2
Training loss: 2.083479881286621
Validation loss: 2.0425819555918374

Epoch: 6| Step: 3
Training loss: 1.9840922355651855
Validation loss: 2.0456549525260925

Epoch: 6| Step: 4
Training loss: 2.3454298973083496
Validation loss: 2.040985425313314

Epoch: 6| Step: 5
Training loss: 2.419086456298828
Validation loss: 2.0370643536249795

Epoch: 6| Step: 6
Training loss: 2.3163766860961914
Validation loss: 2.028130273024241

Epoch: 6| Step: 7
Training loss: 1.5875407457351685
Validation loss: 2.022052049636841

Epoch: 6| Step: 8
Training loss: 2.0001108646392822
Validation loss: 2.0211670796076455

Epoch: 6| Step: 9
Training loss: 2.4997642040252686
Validation loss: 2.0227887829144797

Epoch: 6| Step: 10
Training loss: 2.3631691932678223
Validation loss: 2.0234811504681907

Epoch: 6| Step: 11
Training loss: 2.63382887840271
Validation loss: 2.0194108486175537

Epoch: 6| Step: 12
Training loss: 2.063504219055176
Validation loss: 2.021649161974589

Epoch: 6| Step: 13
Training loss: 2.587207317352295
Validation loss: 2.0265210270881653

Epoch: 69| Step: 0
Training loss: 2.435652256011963
Validation loss: 2.026296397050222

Epoch: 6| Step: 1
Training loss: 1.6923894882202148
Validation loss: 2.018622795740763

Epoch: 6| Step: 2
Training loss: 1.6073546409606934
Validation loss: 2.0270896355311074

Epoch: 6| Step: 3
Training loss: 1.9107698202133179
Validation loss: 2.029508868853251

Epoch: 6| Step: 4
Training loss: 1.444502830505371
Validation loss: 2.0372559626897178

Epoch: 6| Step: 5
Training loss: 2.5410003662109375
Validation loss: 2.028272658586502

Epoch: 6| Step: 6
Training loss: 2.209326982498169
Validation loss: 2.019632577896118

Epoch: 6| Step: 7
Training loss: 2.471482276916504
Validation loss: 2.0166343053181968

Epoch: 6| Step: 8
Training loss: 2.451446294784546
Validation loss: 2.000492036342621

Epoch: 6| Step: 9
Training loss: 2.720975875854492
Validation loss: 2.004351556301117

Epoch: 6| Step: 10
Training loss: 1.9525315761566162
Validation loss: 2.010708471139272

Epoch: 6| Step: 11
Training loss: 2.4641332626342773
Validation loss: 2.014143943786621

Epoch: 6| Step: 12
Training loss: 2.471041679382324
Validation loss: 2.0117400487264

Epoch: 6| Step: 13
Training loss: 2.5128862857818604
Validation loss: 2.0164623856544495

Epoch: 70| Step: 0
Training loss: 1.8005101680755615
Validation loss: 2.0173979997634888

Epoch: 6| Step: 1
Training loss: 2.819967269897461
Validation loss: 2.012774189313253

Epoch: 6| Step: 2
Training loss: 2.069490671157837
Validation loss: 2.00984118382136

Epoch: 6| Step: 3
Training loss: 2.7455155849456787
Validation loss: 2.017880996068319

Epoch: 6| Step: 4
Training loss: 2.0760016441345215
Validation loss: 2.009286721547445

Epoch: 6| Step: 5
Training loss: 2.063215732574463
Validation loss: 2.0123313069343567

Epoch: 6| Step: 6
Training loss: 1.8491780757904053
Validation loss: 2.0080739458402

Epoch: 6| Step: 7
Training loss: 1.9038881063461304
Validation loss: 2.006120721499125

Epoch: 6| Step: 8
Training loss: 1.9930206537246704
Validation loss: 2.000505566596985

Epoch: 6| Step: 9
Training loss: 1.830714464187622
Validation loss: 2.002861201763153

Epoch: 6| Step: 10
Training loss: 3.1062803268432617
Validation loss: 2.009915033976237

Epoch: 6| Step: 11
Training loss: 1.9432319402694702
Validation loss: 2.0111155907313027

Epoch: 6| Step: 12
Training loss: 2.2024331092834473
Validation loss: 2.0176618893941245

Epoch: 6| Step: 13
Training loss: 2.130415916442871
Validation loss: 2.01284392674764

Epoch: 71| Step: 0
Training loss: 2.0535149574279785
Validation loss: 2.0196900169054666

Epoch: 6| Step: 1
Training loss: 2.3534369468688965
Validation loss: 2.0150248805681863

Epoch: 6| Step: 2
Training loss: 2.246338367462158
Validation loss: 2.0195412238438926

Epoch: 6| Step: 3
Training loss: 1.774658203125
Validation loss: 2.007631997267405

Epoch: 6| Step: 4
Training loss: 1.8884968757629395
Validation loss: 2.007854640483856

Epoch: 6| Step: 5
Training loss: 2.6053452491760254
Validation loss: 2.0101799368858337

Epoch: 6| Step: 6
Training loss: 2.1933608055114746
Validation loss: 2.011367599169413

Epoch: 6| Step: 7
Training loss: 2.348544120788574
Validation loss: 2.0017508467038474

Epoch: 6| Step: 8
Training loss: 1.8535462617874146
Validation loss: 2.0061020453770957

Epoch: 6| Step: 9
Training loss: 2.1782588958740234
Validation loss: 2.0048197706540427

Epoch: 6| Step: 10
Training loss: 2.6474645137786865
Validation loss: 2.0157612562179565

Epoch: 6| Step: 11
Training loss: 1.812722086906433
Validation loss: 2.017569442590078

Epoch: 6| Step: 12
Training loss: 2.628288507461548
Validation loss: 2.0167829593022666

Epoch: 6| Step: 13
Training loss: 1.994016170501709
Validation loss: 2.0167274475097656

Epoch: 72| Step: 0
Training loss: 1.8834717273712158
Validation loss: 2.0091031392415366

Epoch: 6| Step: 1
Training loss: 2.100597858428955
Validation loss: 2.0046018759409585

Epoch: 6| Step: 2
Training loss: 2.982973575592041
Validation loss: 2.010023832321167

Epoch: 6| Step: 3
Training loss: 1.9354557991027832
Validation loss: 2.0159804423650107

Epoch: 6| Step: 4
Training loss: 2.3558239936828613
Validation loss: 2.0168257554372153

Epoch: 6| Step: 5
Training loss: 1.4877500534057617
Validation loss: 2.0149117708206177

Epoch: 6| Step: 6
Training loss: 2.3078103065490723
Validation loss: 2.0099825064341226

Epoch: 6| Step: 7
Training loss: 1.839371919631958
Validation loss: 2.0149707595507302

Epoch: 6| Step: 8
Training loss: 2.271801233291626
Validation loss: 2.0122536023457847

Epoch: 6| Step: 9
Training loss: 2.625403642654419
Validation loss: 2.0090503295262656

Epoch: 6| Step: 10
Training loss: 1.4469568729400635
Validation loss: 2.0128244757652283

Epoch: 6| Step: 11
Training loss: 2.244377613067627
Validation loss: 2.009888549645742

Epoch: 6| Step: 12
Training loss: 2.351912021636963
Validation loss: 2.021157721678416

Epoch: 6| Step: 13
Training loss: 2.4286680221557617
Validation loss: 2.0233752727508545

Epoch: 73| Step: 0
Training loss: 2.1702158451080322
Validation loss: 2.0317944486935935

Epoch: 6| Step: 1
Training loss: 2.1134047508239746
Validation loss: 2.0298362374305725

Epoch: 6| Step: 2
Training loss: 1.9206264019012451
Validation loss: 2.0188931226730347

Epoch: 6| Step: 3
Training loss: 2.1875452995300293
Validation loss: 2.0277663270632424

Epoch: 6| Step: 4
Training loss: 2.3455371856689453
Validation loss: 2.0195852518081665

Epoch: 6| Step: 5
Training loss: 1.924908995628357
Validation loss: 2.0170525709788003

Epoch: 6| Step: 6
Training loss: 2.073963165283203
Validation loss: 2.0155809124310813

Epoch: 6| Step: 7
Training loss: 1.9800056219100952
Validation loss: 2.0161728858947754

Epoch: 6| Step: 8
Training loss: 2.4027414321899414
Validation loss: 2.0161578257878623

Epoch: 6| Step: 9
Training loss: 2.398437261581421
Validation loss: 2.027280271053314

Epoch: 6| Step: 10
Training loss: 1.9662355184555054
Validation loss: 2.0435261130332947

Epoch: 6| Step: 11
Training loss: 2.239654541015625
Validation loss: 2.0294660727183023

Epoch: 6| Step: 12
Training loss: 2.281381130218506
Validation loss: 2.0251277883847556

Epoch: 6| Step: 13
Training loss: 2.2097561359405518
Validation loss: 2.0163608392079673

Epoch: 74| Step: 0
Training loss: 2.4261879920959473
Validation loss: 2.0374554991722107

Epoch: 6| Step: 1
Training loss: 2.4037764072418213
Validation loss: 2.025234282016754

Epoch: 6| Step: 2
Training loss: 2.5302844047546387
Validation loss: 2.0098215540250144

Epoch: 6| Step: 3
Training loss: 1.8096034526824951
Validation loss: 2.008521835009257

Epoch: 6| Step: 4
Training loss: 1.911877989768982
Validation loss: 2.010740121205648

Epoch: 6| Step: 5
Training loss: 2.0578536987304688
Validation loss: 2.0131032268206277

Epoch: 6| Step: 6
Training loss: 2.123546838760376
Validation loss: 2.0095070401827493

Epoch: 6| Step: 7
Training loss: 1.845743179321289
Validation loss: 1.995763897895813

Epoch: 6| Step: 8
Training loss: 1.6023436784744263
Validation loss: 2.015689750512441

Epoch: 6| Step: 9
Training loss: 2.2259178161621094
Validation loss: 2.0066096981366477

Epoch: 6| Step: 10
Training loss: 2.1920228004455566
Validation loss: 2.01151450475057

Epoch: 6| Step: 11
Training loss: 2.4262800216674805
Validation loss: 2.0078189174334207

Epoch: 6| Step: 12
Training loss: 2.301006317138672
Validation loss: 2.021790345509847

Epoch: 6| Step: 13
Training loss: 2.428349494934082
Validation loss: 2.02032079299291

Epoch: 75| Step: 0
Training loss: 1.7481549978256226
Validation loss: 2.0258219838142395

Epoch: 6| Step: 1
Training loss: 2.271622657775879
Validation loss: 2.048103988170624

Epoch: 6| Step: 2
Training loss: 2.0566253662109375
Validation loss: 2.030117948849996

Epoch: 6| Step: 3
Training loss: 1.8714373111724854
Validation loss: 2.023926575978597

Epoch: 6| Step: 4
Training loss: 1.9528440237045288
Validation loss: 2.033954163392385

Epoch: 6| Step: 5
Training loss: 2.496461868286133
Validation loss: 2.021097640196482

Epoch: 6| Step: 6
Training loss: 2.348024606704712
Validation loss: 2.0317961970965066

Epoch: 6| Step: 7
Training loss: 2.8310494422912598
Validation loss: 2.0286307533582053

Epoch: 6| Step: 8
Training loss: 1.8391892910003662
Validation loss: 2.0332096219062805

Epoch: 6| Step: 9
Training loss: 2.1370949745178223
Validation loss: 2.0325538714726767

Epoch: 6| Step: 10
Training loss: 2.168348550796509
Validation loss: 2.0165743033091226

Epoch: 6| Step: 11
Training loss: 2.1020758152008057
Validation loss: 2.0153536796569824

Epoch: 6| Step: 12
Training loss: 1.9802618026733398
Validation loss: 2.014169971148173

Epoch: 6| Step: 13
Training loss: 2.426692247390747
Validation loss: 2.0184972882270813

Epoch: 76| Step: 0
Training loss: 1.9577988386154175
Validation loss: 2.022584001223246

Epoch: 6| Step: 1
Training loss: 1.424269199371338
Validation loss: 2.0255507628122964

Epoch: 6| Step: 2
Training loss: 2.477515697479248
Validation loss: 2.014698068300883

Epoch: 6| Step: 3
Training loss: 2.447500467300415
Validation loss: 2.016316831111908

Epoch: 6| Step: 4
Training loss: 1.8225560188293457
Validation loss: 2.0203570326169333

Epoch: 6| Step: 5
Training loss: 2.286123514175415
Validation loss: 2.0330880085627236

Epoch: 6| Step: 6
Training loss: 2.488311529159546
Validation loss: 2.0342800617218018

Epoch: 6| Step: 7
Training loss: 2.316293716430664
Validation loss: 2.0194325049718223

Epoch: 6| Step: 8
Training loss: 2.757535457611084
Validation loss: 2.020840048789978

Epoch: 6| Step: 9
Training loss: 2.1733756065368652
Validation loss: 2.0180344581604004

Epoch: 6| Step: 10
Training loss: 1.6300729513168335
Validation loss: 2.0086138049761453

Epoch: 6| Step: 11
Training loss: 2.2023305892944336
Validation loss: 2.0162818829218545

Epoch: 6| Step: 12
Training loss: 1.9632048606872559
Validation loss: 2.0150634050369263

Epoch: 6| Step: 13
Training loss: 2.307182788848877
Validation loss: 2.0201907555262246

Epoch: 77| Step: 0
Training loss: 2.1731154918670654
Validation loss: 2.0209449330965676

Epoch: 6| Step: 1
Training loss: 2.2424330711364746
Validation loss: 2.0178058544794717

Epoch: 6| Step: 2
Training loss: 1.8013490438461304
Validation loss: 2.020438075065613

Epoch: 6| Step: 3
Training loss: 2.24418306350708
Validation loss: 2.019219080607096

Epoch: 6| Step: 4
Training loss: 1.5637104511260986
Validation loss: 2.0150997241338096

Epoch: 6| Step: 5
Training loss: 2.114872455596924
Validation loss: 2.0172131657600403

Epoch: 6| Step: 6
Training loss: 2.9647979736328125
Validation loss: 2.0116193493207297

Epoch: 6| Step: 7
Training loss: 1.7909926176071167
Validation loss: 2.0130040446917215

Epoch: 6| Step: 8
Training loss: 2.5979719161987305
Validation loss: 2.030330697695414

Epoch: 6| Step: 9
Training loss: 2.4311723709106445
Validation loss: 2.0243070125579834

Epoch: 6| Step: 10
Training loss: 2.548191547393799
Validation loss: 2.0464218258857727

Epoch: 6| Step: 11
Training loss: 2.0047712326049805
Validation loss: 2.041555325190226

Epoch: 6| Step: 12
Training loss: 1.9408278465270996
Validation loss: 2.049221416314443

Epoch: 6| Step: 13
Training loss: 1.6905674934387207
Validation loss: 2.0445303916931152

Epoch: 78| Step: 0
Training loss: 2.347376823425293
Validation loss: 2.047484815120697

Epoch: 6| Step: 1
Training loss: 2.1014955043792725
Validation loss: 2.019258697827657

Epoch: 6| Step: 2
Training loss: 1.75893235206604
Validation loss: 2.0322752793629966

Epoch: 6| Step: 3
Training loss: 2.2767412662506104
Validation loss: 2.0383691787719727

Epoch: 6| Step: 4
Training loss: 2.5479729175567627
Validation loss: 2.0314299861590066

Epoch: 6| Step: 5
Training loss: 2.2631168365478516
Validation loss: 2.013002256552378

Epoch: 6| Step: 6
Training loss: 2.2736895084381104
Validation loss: 2.0106769601504006

Epoch: 6| Step: 7
Training loss: 2.1595377922058105
Validation loss: 2.007819414138794

Epoch: 6| Step: 8
Training loss: 1.7698838710784912
Validation loss: 2.0136783123016357

Epoch: 6| Step: 9
Training loss: 2.1008589267730713
Validation loss: 1.9994081656138103

Epoch: 6| Step: 10
Training loss: 2.0020456314086914
Validation loss: 2.0190426111221313

Epoch: 6| Step: 11
Training loss: 2.0873851776123047
Validation loss: 2.0170050660769143

Epoch: 6| Step: 12
Training loss: 2.3024775981903076
Validation loss: 2.01008810599645

Epoch: 6| Step: 13
Training loss: 2.312711238861084
Validation loss: 2.021510660648346

Epoch: 79| Step: 0
Training loss: 2.1771061420440674
Validation loss: 2.0223576625188193

Epoch: 6| Step: 1
Training loss: 2.5112228393554688
Validation loss: 2.015392621358236

Epoch: 6| Step: 2
Training loss: 2.1111199855804443
Validation loss: 2.0174039800961814

Epoch: 6| Step: 3
Training loss: 1.8819365501403809
Validation loss: 2.023058831691742

Epoch: 6| Step: 4
Training loss: 2.197610855102539
Validation loss: 2.031246860822042

Epoch: 6| Step: 5
Training loss: 2.465280532836914
Validation loss: 2.0212395389874778

Epoch: 6| Step: 6
Training loss: 2.0736145973205566
Validation loss: 2.0401090383529663

Epoch: 6| Step: 7
Training loss: 2.225254774093628
Validation loss: 2.0483821431795755

Epoch: 6| Step: 8
Training loss: 2.129209518432617
Validation loss: 2.0193376739819846

Epoch: 6| Step: 9
Training loss: 2.5333211421966553
Validation loss: 2.016218145688375

Epoch: 6| Step: 10
Training loss: 1.69896399974823
Validation loss: 2.0074272553126016

Epoch: 6| Step: 11
Training loss: 1.525010347366333
Validation loss: 2.005350629488627

Epoch: 6| Step: 12
Training loss: 2.6360511779785156
Validation loss: 2.0125897924105325

Epoch: 6| Step: 13
Training loss: 1.8785479068756104
Validation loss: 2.004111131032308

Epoch: 80| Step: 0
Training loss: 2.0293235778808594
Validation loss: 2.0038748383522034

Epoch: 6| Step: 1
Training loss: 2.2530624866485596
Validation loss: 2.0041021704673767

Epoch: 6| Step: 2
Training loss: 2.0762248039245605
Validation loss: 2.0059578021367392

Epoch: 6| Step: 3
Training loss: 1.8969967365264893
Validation loss: 2.006293694178263

Epoch: 6| Step: 4
Training loss: 1.5273277759552002
Validation loss: 2.008752246697744

Epoch: 6| Step: 5
Training loss: 2.7679500579833984
Validation loss: 2.007399638493856

Epoch: 6| Step: 6
Training loss: 2.321943759918213
Validation loss: 2.0071221590042114

Epoch: 6| Step: 7
Training loss: 2.46067476272583
Validation loss: 2.0121911764144897

Epoch: 6| Step: 8
Training loss: 2.057821750640869
Validation loss: 2.0101263324419656

Epoch: 6| Step: 9
Training loss: 2.022745132446289
Validation loss: 2.028334379196167

Epoch: 6| Step: 10
Training loss: 1.9612069129943848
Validation loss: 2.033569077650706

Epoch: 6| Step: 11
Training loss: 2.280470371246338
Validation loss: 2.0432449181874595

Epoch: 6| Step: 12
Training loss: 2.4124388694763184
Validation loss: 2.0535857876141868

Epoch: 6| Step: 13
Training loss: 2.1941351890563965
Validation loss: 2.042282521724701

Epoch: 81| Step: 0
Training loss: 1.9867973327636719
Validation loss: 2.0287137428919473

Epoch: 6| Step: 1
Training loss: 2.689281463623047
Validation loss: 2.022257467110952

Epoch: 6| Step: 2
Training loss: 3.240143060684204
Validation loss: 2.0198100209236145

Epoch: 6| Step: 3
Training loss: 2.6559557914733887
Validation loss: 2.0160107811292014

Epoch: 6| Step: 4
Training loss: 2.0491108894348145
Validation loss: 2.024636447429657

Epoch: 6| Step: 5
Training loss: 2.24249529838562
Validation loss: 2.027443448702494

Epoch: 6| Step: 6
Training loss: 1.7278552055358887
Validation loss: 2.0291547775268555

Epoch: 6| Step: 7
Training loss: 1.5353600978851318
Validation loss: 2.031501591205597

Epoch: 6| Step: 8
Training loss: 1.6754438877105713
Validation loss: 2.026864151159922

Epoch: 6| Step: 9
Training loss: 2.101043701171875
Validation loss: 2.0353925426801047

Epoch: 6| Step: 10
Training loss: 1.7158576250076294
Validation loss: 2.034393548965454

Epoch: 6| Step: 11
Training loss: 2.137211799621582
Validation loss: 2.02961673339208

Epoch: 6| Step: 12
Training loss: 2.482452630996704
Validation loss: 2.0274014671643577

Epoch: 6| Step: 13
Training loss: 2.082240581512451
Validation loss: 2.0237096349398294

Epoch: 82| Step: 0
Training loss: 1.7236454486846924
Validation loss: 2.0304605960845947

Epoch: 6| Step: 1
Training loss: 2.1726536750793457
Validation loss: 2.0257384975751243

Epoch: 6| Step: 2
Training loss: 2.1814539432525635
Validation loss: 2.0062898794809976

Epoch: 6| Step: 3
Training loss: 3.1555018424987793
Validation loss: 2.006856858730316

Epoch: 6| Step: 4
Training loss: 2.1138384342193604
Validation loss: 2.0137567718823752

Epoch: 6| Step: 5
Training loss: 1.55351984500885
Validation loss: 2.005805472532908

Epoch: 6| Step: 6
Training loss: 2.120129346847534
Validation loss: 2.017811596393585

Epoch: 6| Step: 7
Training loss: 2.3619658946990967
Validation loss: 2.032644271850586

Epoch: 6| Step: 8
Training loss: 1.8125479221343994
Validation loss: 2.0157993038495383

Epoch: 6| Step: 9
Training loss: 2.247332811355591
Validation loss: 2.0258759260177612

Epoch: 6| Step: 10
Training loss: 2.400556802749634
Validation loss: 2.022712548573812

Epoch: 6| Step: 11
Training loss: 2.1124918460845947
Validation loss: 2.019339601198832

Epoch: 6| Step: 12
Training loss: 2.0760884284973145
Validation loss: 2.035966078440348

Epoch: 6| Step: 13
Training loss: 1.9321918487548828
Validation loss: 2.029991010824839

Epoch: 83| Step: 0
Training loss: 2.3425493240356445
Validation loss: 2.040347695350647

Epoch: 6| Step: 1
Training loss: 1.5779943466186523
Validation loss: 2.02686736981074

Epoch: 6| Step: 2
Training loss: 3.0314860343933105
Validation loss: 2.0375110507011414

Epoch: 6| Step: 3
Training loss: 2.2281088829040527
Validation loss: 2.023985723654429

Epoch: 6| Step: 4
Training loss: 2.174651622772217
Validation loss: 2.0139718453089395

Epoch: 6| Step: 5
Training loss: 1.5873628854751587
Validation loss: 2.0079704920450845

Epoch: 6| Step: 6
Training loss: 1.9441611766815186
Validation loss: 2.0088865160942078

Epoch: 6| Step: 7
Training loss: 1.8606960773468018
Validation loss: 1.9997507135073345

Epoch: 6| Step: 8
Training loss: 2.3571577072143555
Validation loss: 2.0028029481569924

Epoch: 6| Step: 9
Training loss: 2.290235996246338
Validation loss: 2.0062646667162576

Epoch: 6| Step: 10
Training loss: 2.1433308124542236
Validation loss: 2.012405494848887

Epoch: 6| Step: 11
Training loss: 1.802603840827942
Validation loss: 2.0127075910568237

Epoch: 6| Step: 12
Training loss: 2.9435248374938965
Validation loss: 2.020597060521444

Epoch: 6| Step: 13
Training loss: 1.6322649717330933
Validation loss: 2.0207314093907676

Epoch: 84| Step: 0
Training loss: 2.492917060852051
Validation loss: 2.0218574603398642

Epoch: 6| Step: 1
Training loss: 1.745940923690796
Validation loss: 2.019474148750305

Epoch: 6| Step: 2
Training loss: 1.4503368139266968
Validation loss: 2.022199352582296

Epoch: 6| Step: 3
Training loss: 2.2183079719543457
Validation loss: 2.0190550883611045

Epoch: 6| Step: 4
Training loss: 2.47776460647583
Validation loss: 2.0189103285471597

Epoch: 6| Step: 5
Training loss: 2.4039690494537354
Validation loss: 2.006586035092672

Epoch: 6| Step: 6
Training loss: 1.888041377067566
Validation loss: 2.0112191438674927

Epoch: 6| Step: 7
Training loss: 2.340165853500366
Validation loss: 1.9975141684214275

Epoch: 6| Step: 8
Training loss: 2.232039451599121
Validation loss: 1.9977590839068096

Epoch: 6| Step: 9
Training loss: 2.5904459953308105
Validation loss: 2.0072048703829446

Epoch: 6| Step: 10
Training loss: 1.97295343875885
Validation loss: 2.0131374398867288

Epoch: 6| Step: 11
Training loss: 1.5861891508102417
Validation loss: 2.008961021900177

Epoch: 6| Step: 12
Training loss: 2.0335984230041504
Validation loss: 2.0276102423667908

Epoch: 6| Step: 13
Training loss: 2.5501151084899902
Validation loss: 2.0289806524912515

Epoch: 85| Step: 0
Training loss: 1.6915311813354492
Validation loss: 2.047210693359375

Epoch: 6| Step: 1
Training loss: 1.779596209526062
Validation loss: 2.046621640523275

Epoch: 6| Step: 2
Training loss: 1.9712144136428833
Validation loss: 2.025461812814077

Epoch: 6| Step: 3
Training loss: 1.8926490545272827
Validation loss: 2.0296022097269693

Epoch: 6| Step: 4
Training loss: 2.6483659744262695
Validation loss: 2.0249952475229898

Epoch: 6| Step: 5
Training loss: 2.3385515213012695
Validation loss: 2.0132819612820945

Epoch: 6| Step: 6
Training loss: 1.7368297576904297
Validation loss: 2.0083889961242676

Epoch: 6| Step: 7
Training loss: 2.4968514442443848
Validation loss: 2.0111140410105386

Epoch: 6| Step: 8
Training loss: 2.110330581665039
Validation loss: 2.018336911996206

Epoch: 6| Step: 9
Training loss: 2.410158157348633
Validation loss: 2.019833425680796

Epoch: 6| Step: 10
Training loss: 2.1773486137390137
Validation loss: 2.020751972993215

Epoch: 6| Step: 11
Training loss: 2.3295531272888184
Validation loss: 2.0209460258483887

Epoch: 6| Step: 12
Training loss: 2.1734018325805664
Validation loss: 2.0292362769444785

Epoch: 6| Step: 13
Training loss: 2.223694324493408
Validation loss: 2.0222402612368264

Epoch: 86| Step: 0
Training loss: 1.7766573429107666
Validation loss: 2.0225777626037598

Epoch: 6| Step: 1
Training loss: 2.0155646800994873
Validation loss: 2.017946263154348

Epoch: 6| Step: 2
Training loss: 2.1763317584991455
Validation loss: 2.010253051916758

Epoch: 6| Step: 3
Training loss: 1.8831908702850342
Validation loss: 2.0081587433815002

Epoch: 6| Step: 4
Training loss: 1.8496977090835571
Validation loss: 2.0025941729545593

Epoch: 6| Step: 5
Training loss: 2.694675922393799
Validation loss: 2.0049977898597717

Epoch: 6| Step: 6
Training loss: 2.4939417839050293
Validation loss: 2.012728671232859

Epoch: 6| Step: 7
Training loss: 1.2566649913787842
Validation loss: 2.014404296875

Epoch: 6| Step: 8
Training loss: 2.5307607650756836
Validation loss: 2.0218122800191245

Epoch: 6| Step: 9
Training loss: 2.4069485664367676
Validation loss: 2.0159276723861694

Epoch: 6| Step: 10
Training loss: 2.531723737716675
Validation loss: 2.0212430556615195

Epoch: 6| Step: 11
Training loss: 1.911099910736084
Validation loss: 2.0370919903119407

Epoch: 6| Step: 12
Training loss: 2.1733579635620117
Validation loss: 2.032499452431997

Epoch: 6| Step: 13
Training loss: 2.231783866882324
Validation loss: 2.059663474559784

Epoch: 87| Step: 0
Training loss: 1.6535365581512451
Validation loss: 2.033949693044027

Epoch: 6| Step: 1
Training loss: 1.793534278869629
Validation loss: 2.0211081902186074

Epoch: 6| Step: 2
Training loss: 2.623960494995117
Validation loss: 2.0206316908200583

Epoch: 6| Step: 3
Training loss: 2.5288665294647217
Validation loss: 2.028444210688273

Epoch: 6| Step: 4
Training loss: 2.534921407699585
Validation loss: 2.022554417451223

Epoch: 6| Step: 5
Training loss: 2.3377740383148193
Validation loss: 2.02091775337855

Epoch: 6| Step: 6
Training loss: 1.5985733270645142
Validation loss: 2.0170822143554688

Epoch: 6| Step: 7
Training loss: 2.300351142883301
Validation loss: 2.0090195735295615

Epoch: 6| Step: 8
Training loss: 2.132429838180542
Validation loss: 2.0141779581705728

Epoch: 6| Step: 9
Training loss: 2.0979857444763184
Validation loss: 2.0043814380963645

Epoch: 6| Step: 10
Training loss: 2.5220632553100586
Validation loss: 2.0136465628941855

Epoch: 6| Step: 11
Training loss: 1.8112633228302002
Validation loss: 2.0166739026705423

Epoch: 6| Step: 12
Training loss: 1.7676541805267334
Validation loss: 2.0202076037724814

Epoch: 6| Step: 13
Training loss: 2.0681886672973633
Validation loss: 2.0049222509066262

Epoch: 88| Step: 0
Training loss: 2.449235439300537
Validation loss: 2.0105124513308206

Epoch: 6| Step: 1
Training loss: 2.056424140930176
Validation loss: 2.0232169032096863

Epoch: 6| Step: 2
Training loss: 2.2482962608337402
Validation loss: 2.0028987924257913

Epoch: 6| Step: 3
Training loss: 2.3202388286590576
Validation loss: 2.0103080670038858

Epoch: 6| Step: 4
Training loss: 1.7770479917526245
Validation loss: 2.0175589323043823

Epoch: 6| Step: 5
Training loss: 1.7909526824951172
Validation loss: 2.007814804712931

Epoch: 6| Step: 6
Training loss: 1.937677264213562
Validation loss: 2.0202388763427734

Epoch: 6| Step: 7
Training loss: 2.071711778640747
Validation loss: 2.01450514793396

Epoch: 6| Step: 8
Training loss: 2.2516260147094727
Validation loss: 2.016454537709554

Epoch: 6| Step: 9
Training loss: 2.2845652103424072
Validation loss: 2.0062279303868613

Epoch: 6| Step: 10
Training loss: 2.212520122528076
Validation loss: 2.0091691613197327

Epoch: 6| Step: 11
Training loss: 1.744547963142395
Validation loss: 2.0193806886672974

Epoch: 6| Step: 12
Training loss: 2.2272579669952393
Validation loss: 2.0226022005081177

Epoch: 6| Step: 13
Training loss: 2.1787400245666504
Validation loss: 2.0187302231788635

Epoch: 89| Step: 0
Training loss: 1.5867210626602173
Validation loss: 2.0119152665138245

Epoch: 6| Step: 1
Training loss: 1.9742623567581177
Validation loss: 2.023486018180847

Epoch: 6| Step: 2
Training loss: 2.192481517791748
Validation loss: 2.0166640480359397

Epoch: 6| Step: 3
Training loss: 2.2747769355773926
Validation loss: 2.016634921232859

Epoch: 6| Step: 4
Training loss: 1.7964601516723633
Validation loss: 2.0128894050916037

Epoch: 6| Step: 5
Training loss: 2.087820529937744
Validation loss: 2.0079798301060996

Epoch: 6| Step: 6
Training loss: 2.4179434776306152
Validation loss: 2.0222297310829163

Epoch: 6| Step: 7
Training loss: 2.5377626419067383
Validation loss: 2.0235119462013245

Epoch: 6| Step: 8
Training loss: 1.9792146682739258
Validation loss: 2.0070947806040444

Epoch: 6| Step: 9
Training loss: 2.359428882598877
Validation loss: 2.0209566553433738

Epoch: 6| Step: 10
Training loss: 2.1275577545166016
Validation loss: 2.01420791943868

Epoch: 6| Step: 11
Training loss: 2.082108974456787
Validation loss: 2.0240992307662964

Epoch: 6| Step: 12
Training loss: 2.01493501663208
Validation loss: 2.0161213874816895

Epoch: 6| Step: 13
Training loss: 2.0908899307250977
Validation loss: 2.016093830267588

Epoch: 90| Step: 0
Training loss: 1.7292627096176147
Validation loss: 2.020825127760569

Epoch: 6| Step: 1
Training loss: 1.660596251487732
Validation loss: 2.015157719453176

Epoch: 6| Step: 2
Training loss: 1.9843686819076538
Validation loss: 2.0154146949450173

Epoch: 6| Step: 3
Training loss: 1.8993734121322632
Validation loss: 2.01183021068573

Epoch: 6| Step: 4
Training loss: 1.8198480606079102
Validation loss: 2.021104117234548

Epoch: 6| Step: 5
Training loss: 2.7859833240509033
Validation loss: 2.0203123887379966

Epoch: 6| Step: 6
Training loss: 2.340970754623413
Validation loss: 2.0279636780420938

Epoch: 6| Step: 7
Training loss: 2.1091525554656982
Validation loss: 2.014371951421102

Epoch: 6| Step: 8
Training loss: 2.197664260864258
Validation loss: 2.0187626481056213

Epoch: 6| Step: 9
Training loss: 1.8360872268676758
Validation loss: 2.0316163301467896

Epoch: 6| Step: 10
Training loss: 2.413943290710449
Validation loss: 2.012651483217875

Epoch: 6| Step: 11
Training loss: 1.7744930982589722
Validation loss: 2.0047643383344016

Epoch: 6| Step: 12
Training loss: 2.8202695846557617
Validation loss: 2.012089947859446

Epoch: 6| Step: 13
Training loss: 2.222313404083252
Validation loss: 2.0091275572776794

Epoch: 91| Step: 0
Training loss: 2.143261432647705
Validation loss: 2.015586018562317

Epoch: 6| Step: 1
Training loss: 1.785351037979126
Validation loss: 2.019909918308258

Epoch: 6| Step: 2
Training loss: 1.8008631467819214
Validation loss: 2.01202400525411

Epoch: 6| Step: 3
Training loss: 2.1133296489715576
Validation loss: 2.013695478439331

Epoch: 6| Step: 4
Training loss: 1.7989925146102905
Validation loss: 2.0148092110951743

Epoch: 6| Step: 5
Training loss: 1.7262969017028809
Validation loss: 2.0203514297803244

Epoch: 6| Step: 6
Training loss: 2.584324836730957
Validation loss: 2.031598707040151

Epoch: 6| Step: 7
Training loss: 2.4208908081054688
Validation loss: 2.0341648856798806

Epoch: 6| Step: 8
Training loss: 2.2100775241851807
Validation loss: 2.0376174251238504

Epoch: 6| Step: 9
Training loss: 1.7915641069412231
Validation loss: 2.027158478895823

Epoch: 6| Step: 10
Training loss: 2.791937828063965
Validation loss: 2.027307609717051

Epoch: 6| Step: 11
Training loss: 2.3210103511810303
Validation loss: 2.0151062409083047

Epoch: 6| Step: 12
Training loss: 1.9576436281204224
Validation loss: 2.0188954869906106

Epoch: 6| Step: 13
Training loss: 2.2303225994110107
Validation loss: 2.025116582711538

Epoch: 92| Step: 0
Training loss: 2.080526351928711
Validation loss: 2.015993297100067

Epoch: 6| Step: 1
Training loss: 1.9168939590454102
Validation loss: 2.0187819600105286

Epoch: 6| Step: 2
Training loss: 1.8001961708068848
Validation loss: 2.0196604331334433

Epoch: 6| Step: 3
Training loss: 3.181288957595825
Validation loss: 2.0078381101290383

Epoch: 6| Step: 4
Training loss: 2.5329785346984863
Validation loss: 2.023474097251892

Epoch: 6| Step: 5
Training loss: 1.6701126098632812
Validation loss: 2.0277241269747415

Epoch: 6| Step: 6
Training loss: 1.8705776929855347
Validation loss: 2.0234143137931824

Epoch: 6| Step: 7
Training loss: 2.2025036811828613
Validation loss: 2.0148900945981345

Epoch: 6| Step: 8
Training loss: 2.273744583129883
Validation loss: 2.0295399030049643

Epoch: 6| Step: 9
Training loss: 2.2292964458465576
Validation loss: 2.015785058339437

Epoch: 6| Step: 10
Training loss: 2.1040537357330322
Validation loss: 2.0133904814720154

Epoch: 6| Step: 11
Training loss: 1.5475735664367676
Validation loss: 2.0176348090171814

Epoch: 6| Step: 12
Training loss: 2.0114760398864746
Validation loss: 2.016540070374807

Epoch: 6| Step: 13
Training loss: 1.952035903930664
Validation loss: 2.0177737275759378

Epoch: 93| Step: 0
Training loss: 1.8356596231460571
Validation loss: 2.015448888142904

Epoch: 6| Step: 1
Training loss: 1.8460190296173096
Validation loss: 2.007279614607493

Epoch: 6| Step: 2
Training loss: 2.7483482360839844
Validation loss: 2.011873443921407

Epoch: 6| Step: 3
Training loss: 2.1424505710601807
Validation loss: 2.0166364908218384

Epoch: 6| Step: 4
Training loss: 2.0880672931671143
Validation loss: 2.015040616194407

Epoch: 6| Step: 5
Training loss: 1.7823036909103394
Validation loss: 2.013899644215902

Epoch: 6| Step: 6
Training loss: 2.4306702613830566
Validation loss: 2.0144134561220803

Epoch: 6| Step: 7
Training loss: 1.9793933629989624
Validation loss: 2.014922877152761

Epoch: 6| Step: 8
Training loss: 2.206467628479004
Validation loss: 2.0274723370869956

Epoch: 6| Step: 9
Training loss: 1.7352347373962402
Validation loss: 2.024167815844218

Epoch: 6| Step: 10
Training loss: 2.725586175918579
Validation loss: 2.0218929847081504

Epoch: 6| Step: 11
Training loss: 1.6572797298431396
Validation loss: 2.0492487947146096

Epoch: 6| Step: 12
Training loss: 2.2748961448669434
Validation loss: 2.0373610655466714

Epoch: 6| Step: 13
Training loss: 2.2230782508850098
Validation loss: 2.041402439276377

Epoch: 94| Step: 0
Training loss: 1.5640063285827637
Validation loss: 2.0434996485710144

Epoch: 6| Step: 1
Training loss: 1.770777702331543
Validation loss: 2.062904496987661

Epoch: 6| Step: 2
Training loss: 2.4023046493530273
Validation loss: 2.0663987398147583

Epoch: 6| Step: 3
Training loss: 1.7596889734268188
Validation loss: 2.0604680379231772

Epoch: 6| Step: 4
Training loss: 2.5586423873901367
Validation loss: 2.061771591504415

Epoch: 6| Step: 5
Training loss: 1.747775912284851
Validation loss: 2.042355934778849

Epoch: 6| Step: 6
Training loss: 2.306849241256714
Validation loss: 2.0411600271860757

Epoch: 6| Step: 7
Training loss: 2.267057418823242
Validation loss: 2.03370467821757

Epoch: 6| Step: 8
Training loss: 2.3111133575439453
Validation loss: 2.0234238505363464

Epoch: 6| Step: 9
Training loss: 1.87484610080719
Validation loss: 2.0235024293263755

Epoch: 6| Step: 10
Training loss: 1.9521912336349487
Validation loss: 2.018109222253164

Epoch: 6| Step: 11
Training loss: 2.444464683532715
Validation loss: 2.020187238852183

Epoch: 6| Step: 12
Training loss: 1.806223750114441
Validation loss: 2.019466499487559

Epoch: 6| Step: 13
Training loss: 2.663130283355713
Validation loss: 2.008401334285736

Epoch: 95| Step: 0
Training loss: 2.197981357574463
Validation loss: 2.0191518863042197

Epoch: 6| Step: 1
Training loss: 2.2773728370666504
Validation loss: 2.0175071557362876

Epoch: 6| Step: 2
Training loss: 2.2683284282684326
Validation loss: 2.024585485458374

Epoch: 6| Step: 3
Training loss: 3.347144603729248
Validation loss: 2.0076394081115723

Epoch: 6| Step: 4
Training loss: 2.4704346656799316
Validation loss: 2.0180462996164956

Epoch: 6| Step: 5
Training loss: 2.3126578330993652
Validation loss: 2.01837811867396

Epoch: 6| Step: 6
Training loss: 2.074796199798584
Validation loss: 2.0296470522880554

Epoch: 6| Step: 7
Training loss: 1.8167996406555176
Validation loss: 2.0146211981773376

Epoch: 6| Step: 8
Training loss: 1.6997199058532715
Validation loss: 2.02333801984787

Epoch: 6| Step: 9
Training loss: 1.6526082754135132
Validation loss: 2.023148457209269

Epoch: 6| Step: 10
Training loss: 2.0516324043273926
Validation loss: 2.0111168026924133

Epoch: 6| Step: 11
Training loss: 1.90190851688385
Validation loss: 2.0161429047584534

Epoch: 6| Step: 12
Training loss: 1.798717975616455
Validation loss: 2.007466514905294

Epoch: 6| Step: 13
Training loss: 1.618580937385559
Validation loss: 2.0130380988121033

Epoch: 96| Step: 0
Training loss: 1.4415669441223145
Validation loss: 2.0166328152020774

Epoch: 6| Step: 1
Training loss: 1.8776187896728516
Validation loss: 2.009114066759745

Epoch: 6| Step: 2
Training loss: 2.054243803024292
Validation loss: 2.0247593323389688

Epoch: 6| Step: 3
Training loss: 2.7937045097351074
Validation loss: 2.0186859567960105

Epoch: 6| Step: 4
Training loss: 2.302403688430786
Validation loss: 2.0240357518196106

Epoch: 6| Step: 5
Training loss: 2.082956314086914
Validation loss: 2.0333267052968345

Epoch: 6| Step: 6
Training loss: 1.850775957107544
Validation loss: 2.0266798933347068

Epoch: 6| Step: 7
Training loss: 2.1873950958251953
Validation loss: 2.0229838689168296

Epoch: 6| Step: 8
Training loss: 2.3313562870025635
Validation loss: 2.0282035867373147

Epoch: 6| Step: 9
Training loss: 1.5192651748657227
Validation loss: 2.0276060700416565

Epoch: 6| Step: 10
Training loss: 3.0238759517669678
Validation loss: 2.0308509469032288

Epoch: 6| Step: 11
Training loss: 1.805979609489441
Validation loss: 2.03310759862264

Epoch: 6| Step: 12
Training loss: 1.8849587440490723
Validation loss: 2.0357239842414856

Epoch: 6| Step: 13
Training loss: 2.3710999488830566
Validation loss: 2.0405839681625366

Epoch: 97| Step: 0
Training loss: 2.0342977046966553
Validation loss: 2.035477081934611

Epoch: 6| Step: 1
Training loss: 2.172724723815918
Validation loss: 2.025792181491852

Epoch: 6| Step: 2
Training loss: 2.393524408340454
Validation loss: 2.0373904506365457

Epoch: 6| Step: 3
Training loss: 2.1633341312408447
Validation loss: 2.0389690001805625

Epoch: 6| Step: 4
Training loss: 2.04131817817688
Validation loss: 2.043076515197754

Epoch: 6| Step: 5
Training loss: 2.3382890224456787
Validation loss: 2.0335084398587546

Epoch: 6| Step: 6
Training loss: 2.0682129859924316
Validation loss: 2.037139376004537

Epoch: 6| Step: 7
Training loss: 2.073376417160034
Validation loss: 2.032615303993225

Epoch: 6| Step: 8
Training loss: 2.4252171516418457
Validation loss: 2.029207090536753

Epoch: 6| Step: 9
Training loss: 2.2477142810821533
Validation loss: 2.0134138464927673

Epoch: 6| Step: 10
Training loss: 1.795945644378662
Validation loss: 2.0150907039642334

Epoch: 6| Step: 11
Training loss: 1.8353363275527954
Validation loss: 2.023906429608663

Epoch: 6| Step: 12
Training loss: 1.9145116806030273
Validation loss: 2.0283886988957724

Epoch: 6| Step: 13
Training loss: 2.106045722961426
Validation loss: 2.016245484352112

Epoch: 98| Step: 0
Training loss: 2.264674186706543
Validation loss: 2.014931639035543

Epoch: 6| Step: 1
Training loss: 2.412613868713379
Validation loss: 2.0333416064580283

Epoch: 6| Step: 2
Training loss: 1.477384328842163
Validation loss: 2.0261011521021524

Epoch: 6| Step: 3
Training loss: 1.8591625690460205
Validation loss: 2.039117077986399

Epoch: 6| Step: 4
Training loss: 1.949826955795288
Validation loss: 2.0269736647605896

Epoch: 6| Step: 5
Training loss: 2.181145429611206
Validation loss: 2.050813357035319

Epoch: 6| Step: 6
Training loss: 2.6294853687286377
Validation loss: 2.040242314338684

Epoch: 6| Step: 7
Training loss: 2.0681562423706055
Validation loss: 2.0384709437688193

Epoch: 6| Step: 8
Training loss: 2.507526159286499
Validation loss: 2.038560469945272

Epoch: 6| Step: 9
Training loss: 2.215501308441162
Validation loss: 2.0519821445147195

Epoch: 6| Step: 10
Training loss: 2.2069950103759766
Validation loss: 2.0378130276997886

Epoch: 6| Step: 11
Training loss: 2.0491113662719727
Validation loss: 2.033204754193624

Epoch: 6| Step: 12
Training loss: 2.2195839881896973
Validation loss: 2.032676180203756

Epoch: 6| Step: 13
Training loss: 1.6208443641662598
Validation loss: 2.02918549378713

Epoch: 99| Step: 0
Training loss: 2.042184591293335
Validation loss: 2.0321357448895774

Epoch: 6| Step: 1
Training loss: 2.1878111362457275
Validation loss: 2.015704850355784

Epoch: 6| Step: 2
Training loss: 2.623403310775757
Validation loss: 2.0146580934524536

Epoch: 6| Step: 3
Training loss: 2.1469757556915283
Validation loss: 2.017130196094513

Epoch: 6| Step: 4
Training loss: 2.5049853324890137
Validation loss: 2.017090837160746

Epoch: 6| Step: 5
Training loss: 2.0143420696258545
Validation loss: 2.0125855008761087

Epoch: 6| Step: 6
Training loss: 1.8423815965652466
Validation loss: 2.021073500315348

Epoch: 6| Step: 7
Training loss: 2.0846314430236816
Validation loss: 2.016190846761068

Epoch: 6| Step: 8
Training loss: 2.1330819129943848
Validation loss: 2.030928115049998

Epoch: 6| Step: 9
Training loss: 2.2223386764526367
Validation loss: 2.01933745543162

Epoch: 6| Step: 10
Training loss: 2.415377378463745
Validation loss: 2.0269724130630493

Epoch: 6| Step: 11
Training loss: 1.7254538536071777
Validation loss: 2.026624103387197

Epoch: 6| Step: 12
Training loss: 2.0925846099853516
Validation loss: 2.0269705057144165

Epoch: 6| Step: 13
Training loss: 1.439016342163086
Validation loss: 2.0214441418647766

Epoch: 100| Step: 0
Training loss: 1.8532750606536865
Validation loss: 2.0148643056551614

Epoch: 6| Step: 1
Training loss: 2.2409400939941406
Validation loss: 2.0135231415430703

Epoch: 6| Step: 2
Training loss: 2.33655047416687
Validation loss: 2.012227475643158

Epoch: 6| Step: 3
Training loss: 2.004671335220337
Validation loss: 1.9970335563023884

Epoch: 6| Step: 4
Training loss: 1.7485100030899048
Validation loss: 2.0069373647371926

Epoch: 6| Step: 5
Training loss: 1.948859691619873
Validation loss: 2.0115267435709634

Epoch: 6| Step: 6
Training loss: 1.9662245512008667
Validation loss: 2.0203118324279785

Epoch: 6| Step: 7
Training loss: 1.8175445795059204
Validation loss: 2.013962984085083

Epoch: 6| Step: 8
Training loss: 1.7744100093841553
Validation loss: 2.0109187960624695

Epoch: 6| Step: 9
Training loss: 2.9197425842285156
Validation loss: 2.015277902285258

Epoch: 6| Step: 10
Training loss: 1.945685863494873
Validation loss: 2.016959031422933

Epoch: 6| Step: 11
Training loss: 2.221552610397339
Validation loss: 2.0112655560175576

Epoch: 6| Step: 12
Training loss: 2.006992816925049
Validation loss: 2.0084033807118735

Epoch: 6| Step: 13
Training loss: 2.323765993118286
Validation loss: 2.0144936045010886

Epoch: 101| Step: 0
Training loss: 1.9856740236282349
Validation loss: 2.020066420237223

Epoch: 6| Step: 1
Training loss: 2.8228397369384766
Validation loss: 2.016950766245524

Epoch: 6| Step: 2
Training loss: 2.4697043895721436
Validation loss: 2.010605812072754

Epoch: 6| Step: 3
Training loss: 1.1943433284759521
Validation loss: 2.0131095250447593

Epoch: 6| Step: 4
Training loss: 1.68002450466156
Validation loss: 2.015475809574127

Epoch: 6| Step: 5
Training loss: 2.0471715927124023
Validation loss: 2.0217408339182534

Epoch: 6| Step: 6
Training loss: 2.491682529449463
Validation loss: 2.02553258339564

Epoch: 6| Step: 7
Training loss: 2.628709554672241
Validation loss: 2.031352460384369

Epoch: 6| Step: 8
Training loss: 1.5148742198944092
Validation loss: 2.045546809832255

Epoch: 6| Step: 9
Training loss: 2.381772041320801
Validation loss: 2.0466758211453757

Epoch: 6| Step: 10
Training loss: 1.512169361114502
Validation loss: 2.0416773160298667

Epoch: 6| Step: 11
Training loss: 2.395319938659668
Validation loss: 2.050961971282959

Epoch: 6| Step: 12
Training loss: 2.1289422512054443
Validation loss: 2.054993291695913

Epoch: 6| Step: 13
Training loss: 2.1312355995178223
Validation loss: 2.04444686571757

Epoch: 102| Step: 0
Training loss: 2.0303831100463867
Validation loss: 2.040521502494812

Epoch: 6| Step: 1
Training loss: 1.9388394355773926
Validation loss: 2.027656058470408

Epoch: 6| Step: 2
Training loss: 2.358926296234131
Validation loss: 2.0177210569381714

Epoch: 6| Step: 3
Training loss: 2.2611207962036133
Validation loss: 2.015241861343384

Epoch: 6| Step: 4
Training loss: 2.199948787689209
Validation loss: 2.0220449368158975

Epoch: 6| Step: 5
Training loss: 2.001727342605591
Validation loss: 2.028707424799601

Epoch: 6| Step: 6
Training loss: 2.0533716678619385
Validation loss: 2.0265098015467324

Epoch: 6| Step: 7
Training loss: 2.4458389282226562
Validation loss: 2.0365407466888428

Epoch: 6| Step: 8
Training loss: 2.223022937774658
Validation loss: 2.0300972064336142

Epoch: 6| Step: 9
Training loss: 1.8276249170303345
Validation loss: 2.0254449446996055

Epoch: 6| Step: 10
Training loss: 2.5842909812927246
Validation loss: 2.0302801728248596

Epoch: 6| Step: 11
Training loss: 1.8809094429016113
Validation loss: 2.0341529051462808

Epoch: 6| Step: 12
Training loss: 2.0312812328338623
Validation loss: 2.0161414543787637

Epoch: 6| Step: 13
Training loss: 1.9343761205673218
Validation loss: 2.020633121331533

Epoch: 103| Step: 0
Training loss: 2.109502077102661
Validation loss: 2.0167956153551736

Epoch: 6| Step: 1
Training loss: 1.9758540391921997
Validation loss: 2.0240920384724936

Epoch: 6| Step: 2
Training loss: 1.8175132274627686
Validation loss: 2.020628333091736

Epoch: 6| Step: 3
Training loss: 1.915487289428711
Validation loss: 2.0317092339197793

Epoch: 6| Step: 4
Training loss: 2.105447769165039
Validation loss: 2.0346233447392783

Epoch: 6| Step: 5
Training loss: 1.9735020399093628
Validation loss: 2.032469550768534

Epoch: 6| Step: 6
Training loss: 2.3772127628326416
Validation loss: 2.037637948989868

Epoch: 6| Step: 7
Training loss: 2.1500167846679688
Validation loss: 2.0364965200424194

Epoch: 6| Step: 8
Training loss: 2.185819625854492
Validation loss: 2.042473077774048

Epoch: 6| Step: 9
Training loss: 2.505768299102783
Validation loss: 2.043766458829244

Epoch: 6| Step: 10
Training loss: 2.219783067703247
Validation loss: 2.0552830894788108

Epoch: 6| Step: 11
Training loss: 1.6959819793701172
Validation loss: 2.04789932568868

Epoch: 6| Step: 12
Training loss: 1.8842220306396484
Validation loss: 2.0472180445988974

Epoch: 6| Step: 13
Training loss: 2.375903844833374
Validation loss: 2.04027392466863

Epoch: 104| Step: 0
Training loss: 1.9429171085357666
Validation loss: 2.0443180799484253

Epoch: 6| Step: 1
Training loss: 2.117159366607666
Validation loss: 2.023225208123525

Epoch: 6| Step: 2
Training loss: 2.213972568511963
Validation loss: 2.0191691716512046

Epoch: 6| Step: 3
Training loss: 2.073422431945801
Validation loss: 2.0165275931358337

Epoch: 6| Step: 4
Training loss: 2.067634105682373
Validation loss: 2.0130521655082703

Epoch: 6| Step: 5
Training loss: 1.9907041788101196
Validation loss: 2.01811683177948

Epoch: 6| Step: 6
Training loss: 2.1699070930480957
Validation loss: 2.003321886062622

Epoch: 6| Step: 7
Training loss: 2.0414271354675293
Validation loss: 2.010919710000356

Epoch: 6| Step: 8
Training loss: 2.461759090423584
Validation loss: 2.0186831951141357

Epoch: 6| Step: 9
Training loss: 1.8820465803146362
Validation loss: 2.0103572607040405

Epoch: 6| Step: 10
Training loss: 2.523559808731079
Validation loss: 2.025808334350586

Epoch: 6| Step: 11
Training loss: 2.162938117980957
Validation loss: 2.023940602938334

Epoch: 6| Step: 12
Training loss: 1.8955237865447998
Validation loss: 2.0201740066210427

Epoch: 6| Step: 13
Training loss: 1.8964173793792725
Validation loss: 2.036369522412618

Epoch: 105| Step: 0
Training loss: 2.0004117488861084
Validation loss: 2.0232418179512024

Epoch: 6| Step: 1
Training loss: 2.1147561073303223
Validation loss: 2.0296918153762817

Epoch: 6| Step: 2
Training loss: 2.2936975955963135
Validation loss: 2.025546749432882

Epoch: 6| Step: 3
Training loss: 2.028428554534912
Validation loss: 2.0275843143463135

Epoch: 6| Step: 4
Training loss: 2.321288585662842
Validation loss: 2.0323758920033774

Epoch: 6| Step: 5
Training loss: 2.134918689727783
Validation loss: 2.027377665042877

Epoch: 6| Step: 6
Training loss: 1.6725434064865112
Validation loss: 2.024604916572571

Epoch: 6| Step: 7
Training loss: 2.366243362426758
Validation loss: 2.0249165097872415

Epoch: 6| Step: 8
Training loss: 1.99281907081604
Validation loss: 2.028168519337972

Epoch: 6| Step: 9
Training loss: 2.6719589233398438
Validation loss: 2.030885954697927

Epoch: 6| Step: 10
Training loss: 2.078221321105957
Validation loss: 2.0123679637908936

Epoch: 6| Step: 11
Training loss: 1.7878305912017822
Validation loss: 2.020191808541616

Epoch: 6| Step: 12
Training loss: 2.505958080291748
Validation loss: 2.026784300804138

Epoch: 6| Step: 13
Training loss: 1.3169808387756348
Validation loss: 2.014002780119578

Epoch: 106| Step: 0
Training loss: 1.4063656330108643
Validation loss: 2.019983152548472

Epoch: 6| Step: 1
Training loss: 2.0762135982513428
Validation loss: 2.0171159903208413

Epoch: 6| Step: 2
Training loss: 2.1197333335876465
Validation loss: 2.020640512307485

Epoch: 6| Step: 3
Training loss: 2.4040474891662598
Validation loss: 2.018072565396627

Epoch: 6| Step: 4
Training loss: 2.7464442253112793
Validation loss: 2.0295153856277466

Epoch: 6| Step: 5
Training loss: 1.4746323823928833
Validation loss: 2.0239694118499756

Epoch: 6| Step: 6
Training loss: 1.6569432020187378
Validation loss: 2.019182542959849

Epoch: 6| Step: 7
Training loss: 1.936497449874878
Validation loss: 2.0194867849349976

Epoch: 6| Step: 8
Training loss: 2.3435115814208984
Validation loss: 2.0324252049128213

Epoch: 6| Step: 9
Training loss: 2.3837761878967285
Validation loss: 2.032439172267914

Epoch: 6| Step: 10
Training loss: 2.0572662353515625
Validation loss: 2.0270142356554666

Epoch: 6| Step: 11
Training loss: 2.172231435775757
Validation loss: 2.0274033347765603

Epoch: 6| Step: 12
Training loss: 2.0330564975738525
Validation loss: 2.028607885042826

Epoch: 6| Step: 13
Training loss: 2.3388051986694336
Validation loss: 2.0249427954355874

Epoch: 107| Step: 0
Training loss: 1.7861026525497437
Validation loss: 2.019655187924703

Epoch: 6| Step: 1
Training loss: 2.367417812347412
Validation loss: 2.0356816252072654

Epoch: 6| Step: 2
Training loss: 1.8389661312103271
Validation loss: 2.033182442188263

Epoch: 6| Step: 3
Training loss: 2.4050419330596924
Validation loss: 2.020766854286194

Epoch: 6| Step: 4
Training loss: 2.4017133712768555
Validation loss: 2.0264746149381003

Epoch: 6| Step: 5
Training loss: 1.9808835983276367
Validation loss: 2.038680911064148

Epoch: 6| Step: 6
Training loss: 2.2841179370880127
Validation loss: 2.032938599586487

Epoch: 6| Step: 7
Training loss: 1.5710525512695312
Validation loss: 2.0293580691019693

Epoch: 6| Step: 8
Training loss: 2.139479875564575
Validation loss: 2.0296761194864907

Epoch: 6| Step: 9
Training loss: 2.2044291496276855
Validation loss: 2.035746415456136

Epoch: 6| Step: 10
Training loss: 2.701059341430664
Validation loss: 2.0287392139434814

Epoch: 6| Step: 11
Training loss: 2.1491587162017822
Validation loss: 2.039471685886383

Epoch: 6| Step: 12
Training loss: 1.997103214263916
Validation loss: 2.0366801818211875

Epoch: 6| Step: 13
Training loss: 1.372403621673584
Validation loss: 2.035772224267324

Epoch: 108| Step: 0
Training loss: 1.9820036888122559
Validation loss: 2.0203304290771484

Epoch: 6| Step: 1
Training loss: 2.189762592315674
Validation loss: 2.031017462412516

Epoch: 6| Step: 2
Training loss: 1.6838958263397217
Validation loss: 2.018839259942373

Epoch: 6| Step: 3
Training loss: 1.6429669857025146
Validation loss: 2.0342100858688354

Epoch: 6| Step: 4
Training loss: 2.117640733718872
Validation loss: 2.027308185895284

Epoch: 6| Step: 5
Training loss: 2.6394524574279785
Validation loss: 2.0424625476201377

Epoch: 6| Step: 6
Training loss: 2.1200389862060547
Validation loss: 2.0333779056866965

Epoch: 6| Step: 7
Training loss: 1.718091607093811
Validation loss: 2.039469540119171

Epoch: 6| Step: 8
Training loss: 2.220377206802368
Validation loss: 2.0408572951952615

Epoch: 6| Step: 9
Training loss: 1.546973466873169
Validation loss: 2.0355695684750876

Epoch: 6| Step: 10
Training loss: 2.0075948238372803
Validation loss: 2.031739890575409

Epoch: 6| Step: 11
Training loss: 1.9240230321884155
Validation loss: 2.0277544260025024

Epoch: 6| Step: 12
Training loss: 2.4130520820617676
Validation loss: 2.028679291407267

Epoch: 6| Step: 13
Training loss: 2.6581008434295654
Validation loss: 2.0220946272214255

Epoch: 109| Step: 0
Training loss: 2.376309394836426
Validation loss: 2.021819313367208

Epoch: 6| Step: 1
Training loss: 2.0583009719848633
Validation loss: 2.0175334016482034

Epoch: 6| Step: 2
Training loss: 1.4395618438720703
Validation loss: 2.016639451185862

Epoch: 6| Step: 3
Training loss: 1.9171158075332642
Validation loss: 2.021909236907959

Epoch: 6| Step: 4
Training loss: 2.044307231903076
Validation loss: 2.020253380139669

Epoch: 6| Step: 5
Training loss: 1.7295708656311035
Validation loss: 2.012290974458059

Epoch: 6| Step: 6
Training loss: 2.848632335662842
Validation loss: 2.0226728320121765

Epoch: 6| Step: 7
Training loss: 2.0736753940582275
Validation loss: 2.028308888276418

Epoch: 6| Step: 8
Training loss: 2.5579490661621094
Validation loss: 2.0293423533439636

Epoch: 6| Step: 9
Training loss: 2.3961644172668457
Validation loss: 2.024812082449595

Epoch: 6| Step: 10
Training loss: 2.2634589672088623
Validation loss: 2.0374156634012857

Epoch: 6| Step: 11
Training loss: 2.073208808898926
Validation loss: 2.027616818745931

Epoch: 6| Step: 12
Training loss: 1.4385768175125122
Validation loss: 2.0390149553616843

Epoch: 6| Step: 13
Training loss: 1.8319592475891113
Validation loss: 2.054309606552124

Epoch: 110| Step: 0
Training loss: 2.2140021324157715
Validation loss: 2.0500203371047974

Epoch: 6| Step: 1
Training loss: 2.109490394592285
Validation loss: 2.0470679799715676

Epoch: 6| Step: 2
Training loss: 1.999567985534668
Validation loss: 2.047834277153015

Epoch: 6| Step: 3
Training loss: 2.368917942047119
Validation loss: 2.0394614934921265

Epoch: 6| Step: 4
Training loss: 2.8731253147125244
Validation loss: 2.0307268102963767

Epoch: 6| Step: 5
Training loss: 1.9997994899749756
Validation loss: 2.021954973538717

Epoch: 6| Step: 6
Training loss: 2.8154256343841553
Validation loss: 2.0211461186408997

Epoch: 6| Step: 7
Training loss: 2.242640733718872
Validation loss: 2.033372422059377

Epoch: 6| Step: 8
Training loss: 1.9188463687896729
Validation loss: 2.0281864404678345

Epoch: 6| Step: 9
Training loss: 1.896742582321167
Validation loss: 2.0192597111066184

Epoch: 6| Step: 10
Training loss: 1.8540198802947998
Validation loss: 2.030912379423777

Epoch: 6| Step: 11
Training loss: 1.5795825719833374
Validation loss: 2.0320054292678833

Epoch: 6| Step: 12
Training loss: 1.2148308753967285
Validation loss: 2.0227670073509216

Epoch: 6| Step: 13
Training loss: 1.961167573928833
Validation loss: 2.0466220577557883

Epoch: 111| Step: 0
Training loss: 1.794501543045044
Validation loss: 2.0345552961031594

Epoch: 6| Step: 1
Training loss: 2.1602420806884766
Validation loss: 2.0363947550455728

Epoch: 6| Step: 2
Training loss: 1.9557018280029297
Validation loss: 2.0273470481236777

Epoch: 6| Step: 3
Training loss: 1.8042417764663696
Validation loss: 2.037664453188578

Epoch: 6| Step: 4
Training loss: 2.351294994354248
Validation loss: 2.0336888432502747

Epoch: 6| Step: 5
Training loss: 2.386594772338867
Validation loss: 2.0380114714304605

Epoch: 6| Step: 6
Training loss: 1.4995551109313965
Validation loss: 2.0345239837964377

Epoch: 6| Step: 7
Training loss: 1.7474360466003418
Validation loss: 2.033496677875519

Epoch: 6| Step: 8
Training loss: 2.3721065521240234
Validation loss: 2.0346136490503945

Epoch: 6| Step: 9
Training loss: 2.128848075866699
Validation loss: 2.0331302285194397

Epoch: 6| Step: 10
Training loss: 1.814523696899414
Validation loss: 2.03397007783254

Epoch: 6| Step: 11
Training loss: 2.0559842586517334
Validation loss: 2.0311498045921326

Epoch: 6| Step: 12
Training loss: 1.8030551671981812
Validation loss: 2.0432053208351135

Epoch: 6| Step: 13
Training loss: 2.876140594482422
Validation loss: 2.0417661468187966

Epoch: 112| Step: 0
Training loss: 1.8419737815856934
Validation loss: 2.0516762336095176

Epoch: 6| Step: 1
Training loss: 2.3282124996185303
Validation loss: 2.047614792982737

Epoch: 6| Step: 2
Training loss: 1.6085028648376465
Validation loss: 2.0324998696645102

Epoch: 6| Step: 3
Training loss: 1.8404221534729004
Validation loss: 2.0308746496836343

Epoch: 6| Step: 4
Training loss: 2.4529690742492676
Validation loss: 2.0386630098025003

Epoch: 6| Step: 5
Training loss: 1.9070106744766235
Validation loss: 2.0246169567108154

Epoch: 6| Step: 6
Training loss: 2.2499918937683105
Validation loss: 2.0397733052571616

Epoch: 6| Step: 7
Training loss: 2.518648624420166
Validation loss: 2.035893718401591

Epoch: 6| Step: 8
Training loss: 1.6364973783493042
Validation loss: 2.0265000263849893

Epoch: 6| Step: 9
Training loss: 2.220181941986084
Validation loss: 2.026259183883667

Epoch: 6| Step: 10
Training loss: 2.0630745887756348
Validation loss: 2.0369415283203125

Epoch: 6| Step: 11
Training loss: 2.3122148513793945
Validation loss: 2.037929673989614

Epoch: 6| Step: 12
Training loss: 1.9300110340118408
Validation loss: 2.062982201576233

Epoch: 6| Step: 13
Training loss: 2.251534938812256
Validation loss: 2.0551059444745383

Epoch: 113| Step: 0
Training loss: 1.8147964477539062
Validation loss: 2.056604822476705

Epoch: 6| Step: 1
Training loss: 2.2469098567962646
Validation loss: 2.048457403977712

Epoch: 6| Step: 2
Training loss: 2.14449143409729
Validation loss: 2.06608784198761

Epoch: 6| Step: 3
Training loss: 1.3405821323394775
Validation loss: 2.056512157122294

Epoch: 6| Step: 4
Training loss: 1.9267315864562988
Validation loss: 2.0396066308021545

Epoch: 6| Step: 5
Training loss: 1.7346534729003906
Validation loss: 2.0553064743677774

Epoch: 6| Step: 6
Training loss: 1.9095381498336792
Validation loss: 2.039634625116984

Epoch: 6| Step: 7
Training loss: 1.9590388536453247
Validation loss: 2.0432169834772744

Epoch: 6| Step: 8
Training loss: 2.5038928985595703
Validation loss: 2.039999465147654

Epoch: 6| Step: 9
Training loss: 2.524238109588623
Validation loss: 2.0266986091931662

Epoch: 6| Step: 10
Training loss: 1.352038860321045
Validation loss: 2.0288893977801004

Epoch: 6| Step: 11
Training loss: 2.7625904083251953
Validation loss: 2.0365508000055947

Epoch: 6| Step: 12
Training loss: 2.2120718955993652
Validation loss: 2.0210058093070984

Epoch: 6| Step: 13
Training loss: 2.4779210090637207
Validation loss: 2.021650751431783

Epoch: 114| Step: 0
Training loss: 1.8271548748016357
Validation loss: 2.03259668747584

Epoch: 6| Step: 1
Training loss: 2.428109645843506
Validation loss: 2.0245561599731445

Epoch: 6| Step: 2
Training loss: 1.3360207080841064
Validation loss: 2.0270216266314187

Epoch: 6| Step: 3
Training loss: 2.386287212371826
Validation loss: 2.0375922322273254

Epoch: 6| Step: 4
Training loss: 2.378986120223999
Validation loss: 2.031723737716675

Epoch: 6| Step: 5
Training loss: 2.720860481262207
Validation loss: 2.036660313606262

Epoch: 6| Step: 6
Training loss: 1.6880519390106201
Validation loss: 2.0427364110946655

Epoch: 6| Step: 7
Training loss: 1.7841770648956299
Validation loss: 2.02995099623998

Epoch: 6| Step: 8
Training loss: 2.473165988922119
Validation loss: 2.0284560918807983

Epoch: 6| Step: 9
Training loss: 2.0985679626464844
Validation loss: 2.028186579545339

Epoch: 6| Step: 10
Training loss: 2.2628355026245117
Validation loss: 2.0392189025878906

Epoch: 6| Step: 11
Training loss: 2.4250388145446777
Validation loss: 2.030578136444092

Epoch: 6| Step: 12
Training loss: 1.128502607345581
Validation loss: 2.027128577232361

Epoch: 6| Step: 13
Training loss: 1.9436687231063843
Validation loss: 2.032534956932068

Epoch: 115| Step: 0
Training loss: 2.402223587036133
Validation loss: 2.0333006381988525

Epoch: 6| Step: 1
Training loss: 2.0381574630737305
Validation loss: 2.0344068010648093

Epoch: 6| Step: 2
Training loss: 1.9877080917358398
Validation loss: 2.0344240268071494

Epoch: 6| Step: 3
Training loss: 2.777115821838379
Validation loss: 2.037113308906555

Epoch: 6| Step: 4
Training loss: 2.114414691925049
Validation loss: 2.02744330962499

Epoch: 6| Step: 5
Training loss: 1.8144869804382324
Validation loss: 2.036621888478597

Epoch: 6| Step: 6
Training loss: 1.820534586906433
Validation loss: 2.0404404401779175

Epoch: 6| Step: 7
Training loss: 1.657568335533142
Validation loss: 2.029879887898763

Epoch: 6| Step: 8
Training loss: 2.220271110534668
Validation loss: 2.0389755765597024

Epoch: 6| Step: 9
Training loss: 1.7730505466461182
Validation loss: 2.0504562060038247

Epoch: 6| Step: 10
Training loss: 2.02768611907959
Validation loss: 2.0443946719169617

Epoch: 6| Step: 11
Training loss: 2.128084659576416
Validation loss: 2.0529996355374656

Epoch: 6| Step: 12
Training loss: 2.4198195934295654
Validation loss: 2.0529136459032693

Epoch: 6| Step: 13
Training loss: 1.8966259956359863
Validation loss: 2.0397945841153464

Epoch: 116| Step: 0
Training loss: 2.232405185699463
Validation loss: 2.0221488873163858

Epoch: 6| Step: 1
Training loss: 1.938157320022583
Validation loss: 2.023384471734365

Epoch: 6| Step: 2
Training loss: 1.9787628650665283
Validation loss: 2.0239213705062866

Epoch: 6| Step: 3
Training loss: 2.6271142959594727
Validation loss: 2.028869112332662

Epoch: 6| Step: 4
Training loss: 1.9350512027740479
Validation loss: 2.027366360028585

Epoch: 6| Step: 5
Training loss: 2.1367132663726807
Validation loss: 2.0289453864097595

Epoch: 6| Step: 6
Training loss: 2.1784684658050537
Validation loss: 2.0284265875816345

Epoch: 6| Step: 7
Training loss: 1.7286986112594604
Validation loss: 2.038168450196584

Epoch: 6| Step: 8
Training loss: 2.5519301891326904
Validation loss: 2.034796953201294

Epoch: 6| Step: 9
Training loss: 1.7878522872924805
Validation loss: 2.047115425268809

Epoch: 6| Step: 10
Training loss: 2.0134072303771973
Validation loss: 2.0407095352808633

Epoch: 6| Step: 11
Training loss: 1.6870927810668945
Validation loss: 2.0421547889709473

Epoch: 6| Step: 12
Training loss: 1.4138468503952026
Validation loss: 2.0557329853375754

Epoch: 6| Step: 13
Training loss: 2.90378999710083
Validation loss: 2.058356463909149

Epoch: 117| Step: 0
Training loss: 1.7470849752426147
Validation loss: 2.0552276372909546

Epoch: 6| Step: 1
Training loss: 2.175323009490967
Validation loss: 2.050459345181783

Epoch: 6| Step: 2
Training loss: 1.923410177230835
Validation loss: 2.0559731920560202

Epoch: 6| Step: 3
Training loss: 2.049220323562622
Validation loss: 2.060306668281555

Epoch: 6| Step: 4
Training loss: 2.5307376384735107
Validation loss: 2.059215704600016

Epoch: 6| Step: 5
Training loss: 2.1511967182159424
Validation loss: 2.0454902052879333

Epoch: 6| Step: 6
Training loss: 2.3793838024139404
Validation loss: 2.0380336244901023

Epoch: 6| Step: 7
Training loss: 1.6630135774612427
Validation loss: 2.0429999033610025

Epoch: 6| Step: 8
Training loss: 1.9576407670974731
Validation loss: 2.042606234550476

Epoch: 6| Step: 9
Training loss: 2.0860366821289062
Validation loss: 2.045077164967855

Epoch: 6| Step: 10
Training loss: 1.699608325958252
Validation loss: 2.050748328367869

Epoch: 6| Step: 11
Training loss: 2.4107601642608643
Validation loss: 2.0326854387919107

Epoch: 6| Step: 12
Training loss: 1.8978452682495117
Validation loss: 2.040085514386495

Epoch: 6| Step: 13
Training loss: 2.3329153060913086
Validation loss: 2.03405233224233

Epoch: 118| Step: 0
Training loss: 2.4064269065856934
Validation loss: 2.036739706993103

Epoch: 6| Step: 1
Training loss: 1.9779008626937866
Validation loss: 2.034021258354187

Epoch: 6| Step: 2
Training loss: 1.9447462558746338
Validation loss: 2.042432963848114

Epoch: 6| Step: 3
Training loss: 1.8203444480895996
Validation loss: 2.0457694927851358

Epoch: 6| Step: 4
Training loss: 2.168025493621826
Validation loss: 2.041318655014038

Epoch: 6| Step: 5
Training loss: 2.1071863174438477
Validation loss: 2.0423251589139304

Epoch: 6| Step: 6
Training loss: 2.1943445205688477
Validation loss: 2.035015424092611

Epoch: 6| Step: 7
Training loss: 1.5454609394073486
Validation loss: 2.0405067801475525

Epoch: 6| Step: 8
Training loss: 2.405695676803589
Validation loss: 2.0498868425687156

Epoch: 6| Step: 9
Training loss: 2.18013334274292
Validation loss: 2.046710431575775

Epoch: 6| Step: 10
Training loss: 2.4348323345184326
Validation loss: 2.0507097045580545

Epoch: 6| Step: 11
Training loss: 2.0032808780670166
Validation loss: 2.04298863808314

Epoch: 6| Step: 12
Training loss: 1.7202732563018799
Validation loss: 2.0506748954455056

Epoch: 6| Step: 13
Training loss: 1.942862629890442
Validation loss: 2.0428335666656494

Epoch: 119| Step: 0
Training loss: 2.2465457916259766
Validation loss: 2.0638577143351235

Epoch: 6| Step: 1
Training loss: 2.1999242305755615
Validation loss: 2.048367420832316

Epoch: 6| Step: 2
Training loss: 2.601384162902832
Validation loss: 2.068750262260437

Epoch: 6| Step: 3
Training loss: 2.2340235710144043
Validation loss: 2.0457855463027954

Epoch: 6| Step: 4
Training loss: 2.3357396125793457
Validation loss: 2.0568496783574424

Epoch: 6| Step: 5
Training loss: 1.693197250366211
Validation loss: 2.0511648058891296

Epoch: 6| Step: 6
Training loss: 2.6889238357543945
Validation loss: 2.035880386829376

Epoch: 6| Step: 7
Training loss: 2.0817487239837646
Validation loss: 2.051940659681956

Epoch: 6| Step: 8
Training loss: 1.9928945302963257
Validation loss: 2.0535810589790344

Epoch: 6| Step: 9
Training loss: 1.8500549793243408
Validation loss: 2.0454383889834085

Epoch: 6| Step: 10
Training loss: 1.876822590827942
Validation loss: 2.046943465868632

Epoch: 6| Step: 11
Training loss: 1.8287853002548218
Validation loss: 2.045458277066549

Epoch: 6| Step: 12
Training loss: 1.494325876235962
Validation loss: 2.052717367808024

Epoch: 6| Step: 13
Training loss: 1.922685146331787
Validation loss: 2.046002447605133

Epoch: 120| Step: 0
Training loss: 2.723247766494751
Validation loss: 2.0455994606018066

Epoch: 6| Step: 1
Training loss: 1.707690954208374
Validation loss: 2.048505981763204

Epoch: 6| Step: 2
Training loss: 1.979029893875122
Validation loss: 2.040870944658915

Epoch: 6| Step: 3
Training loss: 2.482150077819824
Validation loss: 2.0420846740404763

Epoch: 6| Step: 4
Training loss: 1.2669569253921509
Validation loss: 2.0631067554155984

Epoch: 6| Step: 5
Training loss: 1.893606185913086
Validation loss: 2.0347365935643515

Epoch: 6| Step: 6
Training loss: 2.353471279144287
Validation loss: 2.049295167128245

Epoch: 6| Step: 7
Training loss: 2.1827433109283447
Validation loss: 2.0446354746818542

Epoch: 6| Step: 8
Training loss: 1.778566837310791
Validation loss: 2.0328803062438965

Epoch: 6| Step: 9
Training loss: 2.364736557006836
Validation loss: 2.0496171911557517

Epoch: 6| Step: 10
Training loss: 2.007052183151245
Validation loss: 2.04754904905955

Epoch: 6| Step: 11
Training loss: 1.755590796470642
Validation loss: 2.048191706339518

Epoch: 6| Step: 12
Training loss: 2.2903459072113037
Validation loss: 2.0508961280186973

Epoch: 6| Step: 13
Training loss: 1.7585774660110474
Validation loss: 2.037404735883077

Epoch: 121| Step: 0
Training loss: 2.233504295349121
Validation loss: 2.0525136987368264

Epoch: 6| Step: 1
Training loss: 2.52174711227417
Validation loss: 2.051017423470815

Epoch: 6| Step: 2
Training loss: 2.2352752685546875
Validation loss: 2.035032053788503

Epoch: 6| Step: 3
Training loss: 1.9077167510986328
Validation loss: 2.0457725723584494

Epoch: 6| Step: 4
Training loss: 2.0729641914367676
Validation loss: 2.046319564183553

Epoch: 6| Step: 5
Training loss: 1.6590943336486816
Validation loss: 2.044077217578888

Epoch: 6| Step: 6
Training loss: 1.7376354932785034
Validation loss: 2.0527646938959756

Epoch: 6| Step: 7
Training loss: 1.6853511333465576
Validation loss: 2.0611742734909058

Epoch: 6| Step: 8
Training loss: 1.7908296585083008
Validation loss: 2.045581122239431

Epoch: 6| Step: 9
Training loss: 2.108610153198242
Validation loss: 2.0584235191345215

Epoch: 6| Step: 10
Training loss: 2.350351333618164
Validation loss: 2.0623050928115845

Epoch: 6| Step: 11
Training loss: 1.896805763244629
Validation loss: 2.053945998350779

Epoch: 6| Step: 12
Training loss: 2.184138059616089
Validation loss: 2.064950426419576

Epoch: 6| Step: 13
Training loss: 2.2884531021118164
Validation loss: 2.0649435917536416

Epoch: 122| Step: 0
Training loss: 2.2591896057128906
Validation loss: 2.054107427597046

Epoch: 6| Step: 1
Training loss: 2.085573673248291
Validation loss: 2.043947378794352

Epoch: 6| Step: 2
Training loss: 2.0466957092285156
Validation loss: 2.048139234383901

Epoch: 6| Step: 3
Training loss: 1.3594746589660645
Validation loss: 2.040770868460337

Epoch: 6| Step: 4
Training loss: 1.8981188535690308
Validation loss: 2.0323012669881186

Epoch: 6| Step: 5
Training loss: 2.2453413009643555
Validation loss: 2.0475486715634665

Epoch: 6| Step: 6
Training loss: 2.5706565380096436
Validation loss: 2.041512906551361

Epoch: 6| Step: 7
Training loss: 2.3180365562438965
Validation loss: 2.060793936252594

Epoch: 6| Step: 8
Training loss: 1.7892138957977295
Validation loss: 2.053674022356669

Epoch: 6| Step: 9
Training loss: 2.057158946990967
Validation loss: 2.0613852739334106

Epoch: 6| Step: 10
Training loss: 2.250422477722168
Validation loss: 2.0651225248972573

Epoch: 6| Step: 11
Training loss: 1.8244659900665283
Validation loss: 2.0531039039293923

Epoch: 6| Step: 12
Training loss: 2.295703411102295
Validation loss: 2.0597275296847024

Epoch: 6| Step: 13
Training loss: 2.012061595916748
Validation loss: 2.0705259641011557

Epoch: 123| Step: 0
Training loss: 1.9378011226654053
Validation loss: 2.0720152854919434

Epoch: 6| Step: 1
Training loss: 1.7941604852676392
Validation loss: 2.0669323404630027

Epoch: 6| Step: 2
Training loss: 2.1740288734436035
Validation loss: 2.075957715511322

Epoch: 6| Step: 3
Training loss: 1.7285525798797607
Validation loss: 2.070680300394694

Epoch: 6| Step: 4
Training loss: 1.9121332168579102
Validation loss: 2.0682653188705444

Epoch: 6| Step: 5
Training loss: 2.4491679668426514
Validation loss: 2.0714394052823386

Epoch: 6| Step: 6
Training loss: 1.8447107076644897
Validation loss: 2.072054445743561

Epoch: 6| Step: 7
Training loss: 1.827056884765625
Validation loss: 2.0747437477111816

Epoch: 6| Step: 8
Training loss: 1.8722621202468872
Validation loss: 2.073843320210775

Epoch: 6| Step: 9
Training loss: 2.1498398780822754
Validation loss: 2.074507017930349

Epoch: 6| Step: 10
Training loss: 2.225348949432373
Validation loss: 2.065098206202189

Epoch: 6| Step: 11
Training loss: 2.483309268951416
Validation loss: 2.0625017285346985

Epoch: 6| Step: 12
Training loss: 1.680403709411621
Validation loss: 2.079819639523824

Epoch: 6| Step: 13
Training loss: 2.563019275665283
Validation loss: 2.080762048562368

Epoch: 124| Step: 0
Training loss: 2.1973230838775635
Validation loss: 2.0638763109842935

Epoch: 6| Step: 1
Training loss: 1.7084273099899292
Validation loss: 2.062578638394674

Epoch: 6| Step: 2
Training loss: 2.3453152179718018
Validation loss: 2.066781679789225

Epoch: 6| Step: 3
Training loss: 2.0164284706115723
Validation loss: 2.0514938036600747

Epoch: 6| Step: 4
Training loss: 1.8276886940002441
Validation loss: 2.0571114818255105

Epoch: 6| Step: 5
Training loss: 1.9742043018341064
Validation loss: 2.048439542452494

Epoch: 6| Step: 6
Training loss: 2.9511823654174805
Validation loss: 2.0534400145212808

Epoch: 6| Step: 7
Training loss: 2.1567294597625732
Validation loss: 2.0563844045003257

Epoch: 6| Step: 8
Training loss: 2.023174524307251
Validation loss: 2.0556297500928244

Epoch: 6| Step: 9
Training loss: 1.71894109249115
Validation loss: 2.050550321737925

Epoch: 6| Step: 10
Training loss: 1.847961187362671
Validation loss: 2.066972235838572

Epoch: 6| Step: 11
Training loss: 2.1550164222717285
Validation loss: 2.0723960399627686

Epoch: 6| Step: 12
Training loss: 1.8999476432800293
Validation loss: 2.077841639518738

Epoch: 6| Step: 13
Training loss: 1.864454746246338
Validation loss: 2.0811003843943277

Epoch: 125| Step: 0
Training loss: 1.7807015180587769
Validation loss: 2.0818520387013755

Epoch: 6| Step: 1
Training loss: 1.9406940937042236
Validation loss: 2.0788782238960266

Epoch: 6| Step: 2
Training loss: 2.7215733528137207
Validation loss: 2.080194652080536

Epoch: 6| Step: 3
Training loss: 1.691152811050415
Validation loss: 2.0560877124468484

Epoch: 6| Step: 4
Training loss: 2.509587049484253
Validation loss: 2.054086963335673

Epoch: 6| Step: 5
Training loss: 2.5127553939819336
Validation loss: 2.058331847190857

Epoch: 6| Step: 6
Training loss: 2.434051036834717
Validation loss: 2.0501250425974527

Epoch: 6| Step: 7
Training loss: 2.1103737354278564
Validation loss: 2.0470521648724875

Epoch: 6| Step: 8
Training loss: 1.7370519638061523
Validation loss: 2.0563602646191916

Epoch: 6| Step: 9
Training loss: 1.6344280242919922
Validation loss: 2.0499292612075806

Epoch: 6| Step: 10
Training loss: 1.9599395990371704
Validation loss: 2.0501712958017984

Epoch: 6| Step: 11
Training loss: 2.030451536178589
Validation loss: 2.0535887082417807

Epoch: 6| Step: 12
Training loss: 1.94841730594635
Validation loss: 2.0572200218836465

Epoch: 6| Step: 13
Training loss: 1.657406210899353
Validation loss: 2.0549179315567017

Testing loss: 1.682873634125689

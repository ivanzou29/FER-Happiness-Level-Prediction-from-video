Epoch: 1| Step: 0
Training loss: 5.86735725402832
Validation loss: 5.3975480397542315

Epoch: 6| Step: 1
Training loss: 5.178128242492676
Validation loss: 5.395801385243733

Epoch: 6| Step: 2
Training loss: 5.070183753967285
Validation loss: 5.39418896039327

Epoch: 6| Step: 3
Training loss: 5.96232795715332
Validation loss: 5.3926340738932295

Epoch: 6| Step: 4
Training loss: 5.9236650466918945
Validation loss: 5.391145308812459

Epoch: 6| Step: 5
Training loss: 3.9847025871276855
Validation loss: 5.389647165934245

Epoch: 6| Step: 6
Training loss: 5.389281272888184
Validation loss: 5.388258059819539

Epoch: 6| Step: 7
Training loss: 5.925953388214111
Validation loss: 5.387018760045369

Epoch: 6| Step: 8
Training loss: 5.649548530578613
Validation loss: 5.385884841283162

Epoch: 6| Step: 9
Training loss: 5.553802490234375
Validation loss: 5.3846235275268555

Epoch: 6| Step: 10
Training loss: 5.26664400100708
Validation loss: 5.383426904678345

Epoch: 6| Step: 11
Training loss: 4.820705890655518
Validation loss: 5.382275263468425

Epoch: 6| Step: 12
Training loss: 5.481429100036621
Validation loss: 5.381030321121216

Epoch: 6| Step: 13
Training loss: 6.324878692626953
Validation loss: 5.379759788513184

Epoch: 2| Step: 0
Training loss: 4.867454528808594
Validation loss: 5.378365755081177

Epoch: 6| Step: 1
Training loss: 5.764109134674072
Validation loss: 5.377042134602864

Epoch: 6| Step: 2
Training loss: 4.689528942108154
Validation loss: 5.375538428624471

Epoch: 6| Step: 3
Training loss: 5.962090969085693
Validation loss: 5.374063173929851

Epoch: 6| Step: 4
Training loss: 4.440911293029785
Validation loss: 5.372396787007649

Epoch: 6| Step: 5
Training loss: 6.291043281555176
Validation loss: 5.370685418446858

Epoch: 6| Step: 6
Training loss: 5.9188618659973145
Validation loss: 5.36892294883728

Epoch: 6| Step: 7
Training loss: 6.4347639083862305
Validation loss: 5.367061297098796

Epoch: 6| Step: 8
Training loss: 4.5051069259643555
Validation loss: 5.365088780721028

Epoch: 6| Step: 9
Training loss: 5.800442695617676
Validation loss: 5.362872997919719

Epoch: 6| Step: 10
Training loss: 5.855077743530273
Validation loss: 5.360775073369344

Epoch: 6| Step: 11
Training loss: 4.685088157653809
Validation loss: 5.358461141586304

Epoch: 6| Step: 12
Training loss: 5.927547454833984
Validation loss: 5.356102069218953

Epoch: 6| Step: 13
Training loss: 4.96586275100708
Validation loss: 5.353712797164917

Epoch: 3| Step: 0
Training loss: 4.989813804626465
Validation loss: 5.350930213928223

Epoch: 6| Step: 1
Training loss: 5.914129257202148
Validation loss: 5.34803827603658

Epoch: 6| Step: 2
Training loss: 4.997285842895508
Validation loss: 5.345189650853475

Epoch: 6| Step: 3
Training loss: 5.274758815765381
Validation loss: 5.342074076334636

Epoch: 6| Step: 4
Training loss: 5.565439224243164
Validation loss: 5.3387877146403

Epoch: 6| Step: 5
Training loss: 5.992300510406494
Validation loss: 5.335330645243327

Epoch: 6| Step: 6
Training loss: 5.505615711212158
Validation loss: 5.33155632019043

Epoch: 6| Step: 7
Training loss: 5.077731132507324
Validation loss: 5.3281049728393555

Epoch: 6| Step: 8
Training loss: 5.857219219207764
Validation loss: 5.323976198832194

Epoch: 6| Step: 9
Training loss: 6.063898086547852
Validation loss: 5.319693644841512

Epoch: 6| Step: 10
Training loss: 4.61418342590332
Validation loss: 5.31553594271342

Epoch: 6| Step: 11
Training loss: 4.298445224761963
Validation loss: 5.310996611913045

Epoch: 6| Step: 12
Training loss: 5.228085994720459
Validation loss: 5.306270758310954

Epoch: 6| Step: 13
Training loss: 6.186568737030029
Validation loss: 5.301123301188151

Epoch: 4| Step: 0
Training loss: 4.981269836425781
Validation loss: 5.295937856038411

Epoch: 6| Step: 1
Training loss: 4.392751693725586
Validation loss: 5.290517171223958

Epoch: 6| Step: 2
Training loss: 4.892500877380371
Validation loss: 5.284724712371826

Epoch: 6| Step: 3
Training loss: 6.457548141479492
Validation loss: 5.278749465942383

Epoch: 6| Step: 4
Training loss: 5.760100841522217
Validation loss: 5.272629102071126

Epoch: 6| Step: 5
Training loss: 5.628323554992676
Validation loss: 5.266382296880086

Epoch: 6| Step: 6
Training loss: 5.3473286628723145
Validation loss: 5.259552796681722

Epoch: 6| Step: 7
Training loss: 5.690517902374268
Validation loss: 5.252449591954549

Epoch: 6| Step: 8
Training loss: 5.6054582595825195
Validation loss: 5.245039780934651

Epoch: 6| Step: 9
Training loss: 4.140855312347412
Validation loss: 5.237818797429402

Epoch: 6| Step: 10
Training loss: 5.850854873657227
Validation loss: 5.230350017547607

Epoch: 6| Step: 11
Training loss: 4.660860538482666
Validation loss: 5.221934000651042

Epoch: 6| Step: 12
Training loss: 5.822528839111328
Validation loss: 5.214133262634277

Epoch: 6| Step: 13
Training loss: 5.312585830688477
Validation loss: 5.205549399058024

Epoch: 5| Step: 0
Training loss: 5.74383544921875
Validation loss: 5.1970951954523725

Epoch: 6| Step: 1
Training loss: 5.565196990966797
Validation loss: 5.188068389892578

Epoch: 6| Step: 2
Training loss: 4.956584930419922
Validation loss: 5.178697903951009

Epoch: 6| Step: 3
Training loss: 6.131549835205078
Validation loss: 5.169373909632365

Epoch: 6| Step: 4
Training loss: 4.102372169494629
Validation loss: 5.15990948677063

Epoch: 6| Step: 5
Training loss: 4.685364723205566
Validation loss: 5.149895270665486

Epoch: 6| Step: 6
Training loss: 4.165230751037598
Validation loss: 5.1402796904246015

Epoch: 6| Step: 7
Training loss: 5.92146110534668
Validation loss: 5.1304629643758135

Epoch: 6| Step: 8
Training loss: 5.230157852172852
Validation loss: 5.120206594467163

Epoch: 6| Step: 9
Training loss: 6.209648609161377
Validation loss: 5.110595464706421

Epoch: 6| Step: 10
Training loss: 5.13594388961792
Validation loss: 5.100778420766194

Epoch: 6| Step: 11
Training loss: 5.28159236907959
Validation loss: 5.091383775075276

Epoch: 6| Step: 12
Training loss: 3.9035181999206543
Validation loss: 5.081538120905559

Epoch: 6| Step: 13
Training loss: 5.899046421051025
Validation loss: 5.0718382596969604

Epoch: 6| Step: 0
Training loss: 5.2725629806518555
Validation loss: 5.062164465586345

Epoch: 6| Step: 1
Training loss: 4.47034215927124
Validation loss: 5.052527666091919

Epoch: 6| Step: 2
Training loss: 6.349944114685059
Validation loss: 5.043403784434001

Epoch: 6| Step: 3
Training loss: 5.550142765045166
Validation loss: 5.033564488093059

Epoch: 6| Step: 4
Training loss: 4.52236270904541
Validation loss: 5.0245598157246905

Epoch: 6| Step: 5
Training loss: 4.862364292144775
Validation loss: 5.01522421836853

Epoch: 6| Step: 6
Training loss: 4.565895080566406
Validation loss: 5.006533940633138

Epoch: 6| Step: 7
Training loss: 6.0168304443359375
Validation loss: 4.997769435246785

Epoch: 6| Step: 8
Training loss: 4.689901351928711
Validation loss: 4.988556702931722

Epoch: 6| Step: 9
Training loss: 3.8881020545959473
Validation loss: 4.979487021764119

Epoch: 6| Step: 10
Training loss: 6.354734420776367
Validation loss: 4.970441738764445

Epoch: 6| Step: 11
Training loss: 5.426636219024658
Validation loss: 4.961158434549968

Epoch: 6| Step: 12
Training loss: 4.775952339172363
Validation loss: 4.951937596003215

Epoch: 6| Step: 13
Training loss: 4.413247108459473
Validation loss: 4.94244368871053

Epoch: 7| Step: 0
Training loss: 5.064990043640137
Validation loss: 4.932939608891805

Epoch: 6| Step: 1
Training loss: 5.251675605773926
Validation loss: 4.923940420150757

Epoch: 6| Step: 2
Training loss: 5.15187931060791
Validation loss: 4.91541310151418

Epoch: 6| Step: 3
Training loss: 5.107231140136719
Validation loss: 4.90666937828064

Epoch: 6| Step: 4
Training loss: 4.972286224365234
Validation loss: 4.89863657951355

Epoch: 6| Step: 5
Training loss: 4.415194988250732
Validation loss: 4.889980753262837

Epoch: 6| Step: 6
Training loss: 3.512850284576416
Validation loss: 4.882262309392293

Epoch: 6| Step: 7
Training loss: 4.373282432556152
Validation loss: 4.874879837036133

Epoch: 6| Step: 8
Training loss: 4.208903789520264
Validation loss: 4.867623964945476

Epoch: 6| Step: 9
Training loss: 5.747825622558594
Validation loss: 4.860374768575032

Epoch: 6| Step: 10
Training loss: 6.13516092300415
Validation loss: 4.853857199350993

Epoch: 6| Step: 11
Training loss: 4.202661037445068
Validation loss: 4.847190221150716

Epoch: 6| Step: 12
Training loss: 6.077064037322998
Validation loss: 4.840543270111084

Epoch: 6| Step: 13
Training loss: 5.326879024505615
Validation loss: 4.8340175946553545

Epoch: 8| Step: 0
Training loss: 5.681212425231934
Validation loss: 4.827715992927551

Epoch: 6| Step: 1
Training loss: 6.044338226318359
Validation loss: 4.82126768430074

Epoch: 6| Step: 2
Training loss: 5.609438419342041
Validation loss: 4.815334320068359

Epoch: 6| Step: 3
Training loss: 5.5263285636901855
Validation loss: 4.809118111928304

Epoch: 6| Step: 4
Training loss: 5.147645950317383
Validation loss: 4.802923123041789

Epoch: 6| Step: 5
Training loss: 4.8899617195129395
Validation loss: 4.796994964281718

Epoch: 6| Step: 6
Training loss: 4.437544822692871
Validation loss: 4.791076382001241

Epoch: 6| Step: 7
Training loss: 3.556889533996582
Validation loss: 4.7853100299835205

Epoch: 6| Step: 8
Training loss: 5.009527683258057
Validation loss: 4.779786189397176

Epoch: 6| Step: 9
Training loss: 3.7099721431732178
Validation loss: 4.7737964789072675

Epoch: 6| Step: 10
Training loss: 4.894301891326904
Validation loss: 4.768364508946736

Epoch: 6| Step: 11
Training loss: 3.8602733612060547
Validation loss: 4.762877384821574

Epoch: 6| Step: 12
Training loss: 4.91030740737915
Validation loss: 4.757002830505371

Epoch: 6| Step: 13
Training loss: 5.029184341430664
Validation loss: 4.751431306203206

Epoch: 9| Step: 0
Training loss: 5.6463212966918945
Validation loss: 4.7460784912109375

Epoch: 6| Step: 1
Training loss: 5.467599391937256
Validation loss: 4.741281350453694

Epoch: 6| Step: 2
Training loss: 4.518937110900879
Validation loss: 4.735703468322754

Epoch: 6| Step: 3
Training loss: 5.369093418121338
Validation loss: 4.730691274007161

Epoch: 6| Step: 4
Training loss: 4.716390609741211
Validation loss: 4.7249159415562945

Epoch: 6| Step: 5
Training loss: 4.665521144866943
Validation loss: 4.719619671503703

Epoch: 6| Step: 6
Training loss: 5.01831579208374
Validation loss: 4.714017311731975

Epoch: 6| Step: 7
Training loss: 6.1310601234436035
Validation loss: 4.708892265955607

Epoch: 6| Step: 8
Training loss: 4.9000749588012695
Validation loss: 4.704233010609944

Epoch: 6| Step: 9
Training loss: 4.879993915557861
Validation loss: 4.698201258977254

Epoch: 6| Step: 10
Training loss: 4.318904876708984
Validation loss: 4.69344687461853

Epoch: 6| Step: 11
Training loss: 3.314784049987793
Validation loss: 4.688481171925862

Epoch: 6| Step: 12
Training loss: 4.218742847442627
Validation loss: 4.68313725789388

Epoch: 6| Step: 13
Training loss: 4.119967460632324
Validation loss: 4.678128480911255

Epoch: 10| Step: 0
Training loss: 4.114213943481445
Validation loss: 4.6731367111206055

Epoch: 6| Step: 1
Training loss: 5.29970121383667
Validation loss: 4.668292244275411

Epoch: 6| Step: 2
Training loss: 4.813635349273682
Validation loss: 4.662998755772908

Epoch: 6| Step: 3
Training loss: 3.4027791023254395
Validation loss: 4.657940904299418

Epoch: 6| Step: 4
Training loss: 4.055512428283691
Validation loss: 4.65293534596761

Epoch: 6| Step: 5
Training loss: 4.4615654945373535
Validation loss: 4.648404836654663

Epoch: 6| Step: 6
Training loss: 5.289677619934082
Validation loss: 4.643317619959514

Epoch: 6| Step: 7
Training loss: 5.474031448364258
Validation loss: 4.638485431671143

Epoch: 6| Step: 8
Training loss: 5.054469108581543
Validation loss: 4.633945027987163

Epoch: 6| Step: 9
Training loss: 5.070870399475098
Validation loss: 4.629061460494995

Epoch: 6| Step: 10
Training loss: 4.3329668045043945
Validation loss: 4.624179283777873

Epoch: 6| Step: 11
Training loss: 5.849529266357422
Validation loss: 4.619244416554769

Epoch: 6| Step: 12
Training loss: 4.3044538497924805
Validation loss: 4.614291350046794

Epoch: 6| Step: 13
Training loss: 4.813726425170898
Validation loss: 4.609641353289287

Epoch: 11| Step: 0
Training loss: 5.2214837074279785
Validation loss: 4.605078935623169

Epoch: 6| Step: 1
Training loss: 3.442856788635254
Validation loss: 4.600608825683594

Epoch: 6| Step: 2
Training loss: 4.964015483856201
Validation loss: 4.595573623975118

Epoch: 6| Step: 3
Training loss: 4.012716293334961
Validation loss: 4.591159264246623

Epoch: 6| Step: 4
Training loss: 4.766962051391602
Validation loss: 4.586273113886516

Epoch: 6| Step: 5
Training loss: 5.653278350830078
Validation loss: 4.581833283106486

Epoch: 6| Step: 6
Training loss: 5.103190898895264
Validation loss: 4.577556252479553

Epoch: 6| Step: 7
Training loss: 4.042909622192383
Validation loss: 4.572724898656209

Epoch: 6| Step: 8
Training loss: 4.6435017585754395
Validation loss: 4.567425489425659

Epoch: 6| Step: 9
Training loss: 3.9917831420898438
Validation loss: 4.563033978144328

Epoch: 6| Step: 10
Training loss: 4.014615535736084
Validation loss: 4.557958563168843

Epoch: 6| Step: 11
Training loss: 5.177092552185059
Validation loss: 4.553797086079915

Epoch: 6| Step: 12
Training loss: 5.1225433349609375
Validation loss: 4.548603534698486

Epoch: 6| Step: 13
Training loss: 5.297286033630371
Validation loss: 4.544600605964661

Epoch: 12| Step: 0
Training loss: 4.419571876525879
Validation loss: 4.539451360702515

Epoch: 6| Step: 1
Training loss: 4.750539302825928
Validation loss: 4.53507395585378

Epoch: 6| Step: 2
Training loss: 4.471193790435791
Validation loss: 4.530379454294841

Epoch: 6| Step: 3
Training loss: 5.3863067626953125
Validation loss: 4.525662740071614

Epoch: 6| Step: 4
Training loss: 3.3822884559631348
Validation loss: 4.521136204401652

Epoch: 6| Step: 5
Training loss: 4.7259674072265625
Validation loss: 4.515942970911662

Epoch: 6| Step: 6
Training loss: 4.780638694763184
Validation loss: 4.511023243268331

Epoch: 6| Step: 7
Training loss: 4.898195266723633
Validation loss: 4.506435712178548

Epoch: 6| Step: 8
Training loss: 4.82518196105957
Validation loss: 4.50146206219991

Epoch: 6| Step: 9
Training loss: 4.678092002868652
Validation loss: 4.4976561069488525

Epoch: 6| Step: 10
Training loss: 4.849593162536621
Validation loss: 4.492301781972249

Epoch: 6| Step: 11
Training loss: 4.612199783325195
Validation loss: 4.487245162328084

Epoch: 6| Step: 12
Training loss: 4.668320178985596
Validation loss: 4.482830802599589

Epoch: 6| Step: 13
Training loss: 4.161111831665039
Validation loss: 4.47848363717397

Epoch: 13| Step: 0
Training loss: 3.347891092300415
Validation loss: 4.4741290012995405

Epoch: 6| Step: 1
Training loss: 5.078826904296875
Validation loss: 4.469287753105164

Epoch: 6| Step: 2
Training loss: 5.119887351989746
Validation loss: 4.464214563369751

Epoch: 6| Step: 3
Training loss: 3.902120590209961
Validation loss: 4.459908088048299

Epoch: 6| Step: 4
Training loss: 3.5574262142181396
Validation loss: 4.455012122790019

Epoch: 6| Step: 5
Training loss: 5.102517127990723
Validation loss: 4.4501283168792725

Epoch: 6| Step: 6
Training loss: 4.369312286376953
Validation loss: 4.445696830749512

Epoch: 6| Step: 7
Training loss: 5.998289108276367
Validation loss: 4.440416892369588

Epoch: 6| Step: 8
Training loss: 4.291008949279785
Validation loss: 4.436000108718872

Epoch: 6| Step: 9
Training loss: 4.455453872680664
Validation loss: 4.430833061536153

Epoch: 6| Step: 10
Training loss: 4.615540504455566
Validation loss: 4.426227887471517

Epoch: 6| Step: 11
Training loss: 3.6931090354919434
Validation loss: 4.421809752782186

Epoch: 6| Step: 12
Training loss: 5.070145130157471
Validation loss: 4.416803320248921

Epoch: 6| Step: 13
Training loss: 5.170646667480469
Validation loss: 4.412290652592977

Epoch: 14| Step: 0
Training loss: 4.606164932250977
Validation loss: 4.407214005788167

Epoch: 6| Step: 1
Training loss: 5.15283203125
Validation loss: 4.40233051776886

Epoch: 6| Step: 2
Training loss: 5.453572750091553
Validation loss: 4.39758574962616

Epoch: 6| Step: 3
Training loss: 3.8894193172454834
Validation loss: 4.392723321914673

Epoch: 6| Step: 4
Training loss: 4.968874454498291
Validation loss: 4.388547420501709

Epoch: 6| Step: 5
Training loss: 5.407840251922607
Validation loss: 4.384074608484904

Epoch: 6| Step: 6
Training loss: 3.5746371746063232
Validation loss: 4.378758430480957

Epoch: 6| Step: 7
Training loss: 4.221517562866211
Validation loss: 4.374154686927795

Epoch: 6| Step: 8
Training loss: 4.682089805603027
Validation loss: 4.369386871655782

Epoch: 6| Step: 9
Training loss: 5.264429569244385
Validation loss: 4.3643103043238325

Epoch: 6| Step: 10
Training loss: 3.129668951034546
Validation loss: 4.358863910039266

Epoch: 6| Step: 11
Training loss: 3.3214211463928223
Validation loss: 4.354723612467448

Epoch: 6| Step: 12
Training loss: 4.774017333984375
Validation loss: 4.349857171376546

Epoch: 6| Step: 13
Training loss: 4.469212055206299
Validation loss: 4.345043579737346

Epoch: 15| Step: 0
Training loss: 3.717782974243164
Validation loss: 4.339683890342712

Epoch: 6| Step: 1
Training loss: 4.656839847564697
Validation loss: 4.337049563725789

Epoch: 6| Step: 2
Training loss: 4.36525821685791
Validation loss: 4.332115173339844

Epoch: 6| Step: 3
Training loss: 4.620073318481445
Validation loss: 4.325770338376363

Epoch: 6| Step: 4
Training loss: 4.281381607055664
Validation loss: 4.320425669352214

Epoch: 6| Step: 5
Training loss: 3.597515106201172
Validation loss: 4.315736770629883

Epoch: 6| Step: 6
Training loss: 4.70891809463501
Validation loss: 4.311001737912496

Epoch: 6| Step: 7
Training loss: 5.273338317871094
Validation loss: 4.305153290430705

Epoch: 6| Step: 8
Training loss: 4.631250381469727
Validation loss: 4.301005681355794

Epoch: 6| Step: 9
Training loss: 3.7225120067596436
Validation loss: 4.296018560727437

Epoch: 6| Step: 10
Training loss: 5.247903347015381
Validation loss: 4.290590325991313

Epoch: 6| Step: 11
Training loss: 4.392546653747559
Validation loss: 4.2856777509053545

Epoch: 6| Step: 12
Training loss: 3.81255841255188
Validation loss: 4.278546373049418

Epoch: 6| Step: 13
Training loss: 4.9847564697265625
Validation loss: 4.272292137145996

Epoch: 16| Step: 0
Training loss: 4.892144203186035
Validation loss: 4.266709208488464

Epoch: 6| Step: 1
Training loss: 4.4664835929870605
Validation loss: 4.26320743560791

Epoch: 6| Step: 2
Training loss: 4.118068695068359
Validation loss: 4.255045255025228

Epoch: 6| Step: 3
Training loss: 4.629617214202881
Validation loss: 4.250444253285726

Epoch: 6| Step: 4
Training loss: 4.56577730178833
Validation loss: 4.247324029604594

Epoch: 6| Step: 5
Training loss: 3.99562931060791
Validation loss: 4.2406465609868365

Epoch: 6| Step: 6
Training loss: 4.515941619873047
Validation loss: 4.235554814338684

Epoch: 6| Step: 7
Training loss: 3.547726631164551
Validation loss: 4.228942513465881

Epoch: 6| Step: 8
Training loss: 3.342921018600464
Validation loss: 4.222803473472595

Epoch: 6| Step: 9
Training loss: 5.092592716217041
Validation loss: 4.217213312784831

Epoch: 6| Step: 10
Training loss: 4.030061721801758
Validation loss: 4.211064338684082

Epoch: 6| Step: 11
Training loss: 5.200070381164551
Validation loss: 4.205545743306478

Epoch: 6| Step: 12
Training loss: 4.815869331359863
Validation loss: 4.199827790260315

Epoch: 6| Step: 13
Training loss: 3.8274312019348145
Validation loss: 4.193009575208028

Epoch: 17| Step: 0
Training loss: 4.946233749389648
Validation loss: 4.187376300493876

Epoch: 6| Step: 1
Training loss: 5.512248516082764
Validation loss: 4.1819694836934405

Epoch: 6| Step: 2
Training loss: 3.7156004905700684
Validation loss: 4.1756056149800616

Epoch: 6| Step: 3
Training loss: 3.2306313514709473
Validation loss: 4.170206546783447

Epoch: 6| Step: 4
Training loss: 5.220655918121338
Validation loss: 4.1651943524678545

Epoch: 6| Step: 5
Training loss: 4.98502254486084
Validation loss: 4.159207304318746

Epoch: 6| Step: 6
Training loss: 4.087471961975098
Validation loss: 4.154221097628276

Epoch: 6| Step: 7
Training loss: 4.176197052001953
Validation loss: 4.151098728179932

Epoch: 6| Step: 8
Training loss: 5.12346887588501
Validation loss: 4.143057346343994

Epoch: 6| Step: 9
Training loss: 3.7993271350860596
Validation loss: 4.137709538141887

Epoch: 6| Step: 10
Training loss: 2.8989779949188232
Validation loss: 4.132226904233296

Epoch: 6| Step: 11
Training loss: 4.796201705932617
Validation loss: 4.127981384595235

Epoch: 6| Step: 12
Training loss: 3.7060062885284424
Validation loss: 4.123611807823181

Epoch: 6| Step: 13
Training loss: 3.8411083221435547
Validation loss: 4.118261973063151

Epoch: 18| Step: 0
Training loss: 3.6022815704345703
Validation loss: 4.111998637517293

Epoch: 6| Step: 1
Training loss: 5.041419506072998
Validation loss: 4.107072552045186

Epoch: 6| Step: 2
Training loss: 3.911262273788452
Validation loss: 4.1017599900563555

Epoch: 6| Step: 3
Training loss: 5.208148002624512
Validation loss: 4.097351272900899

Epoch: 6| Step: 4
Training loss: 3.7813920974731445
Validation loss: 4.0912736256917315

Epoch: 6| Step: 5
Training loss: 4.86730432510376
Validation loss: 4.085645874341329

Epoch: 6| Step: 6
Training loss: 3.7203032970428467
Validation loss: 4.080716053644816

Epoch: 6| Step: 7
Training loss: 2.9925084114074707
Validation loss: 4.074898322423299

Epoch: 6| Step: 8
Training loss: 4.13339900970459
Validation loss: 4.070467631022136

Epoch: 6| Step: 9
Training loss: 4.668060302734375
Validation loss: 4.065287947654724

Epoch: 6| Step: 10
Training loss: 4.236284255981445
Validation loss: 4.060646851857503

Epoch: 6| Step: 11
Training loss: 4.747607707977295
Validation loss: 4.056457797686259

Epoch: 6| Step: 12
Training loss: 4.379411697387695
Validation loss: 4.050692518552144

Epoch: 6| Step: 13
Training loss: 3.779405117034912
Validation loss: 4.045097986857097

Epoch: 19| Step: 0
Training loss: 3.9354357719421387
Validation loss: 4.040553291638692

Epoch: 6| Step: 1
Training loss: 4.800911903381348
Validation loss: 4.036129474639893

Epoch: 6| Step: 2
Training loss: 4.451794624328613
Validation loss: 4.03089718023936

Epoch: 6| Step: 3
Training loss: 4.170988082885742
Validation loss: 4.026631156603496

Epoch: 6| Step: 4
Training loss: 2.9834189414978027
Validation loss: 4.022119482358296

Epoch: 6| Step: 5
Training loss: 3.984384059906006
Validation loss: 4.018384099006653

Epoch: 6| Step: 6
Training loss: 4.0228118896484375
Validation loss: 4.014062404632568

Epoch: 6| Step: 7
Training loss: 3.6931567192077637
Validation loss: 4.009610414505005

Epoch: 6| Step: 8
Training loss: 4.839672088623047
Validation loss: 4.003554940223694

Epoch: 6| Step: 9
Training loss: 4.141029357910156
Validation loss: 3.9980599085489907

Epoch: 6| Step: 10
Training loss: 3.115591526031494
Validation loss: 3.9928476810455322

Epoch: 6| Step: 11
Training loss: 4.420401096343994
Validation loss: 3.9884516398111978

Epoch: 6| Step: 12
Training loss: 4.237315654754639
Validation loss: 3.9832704067230225

Epoch: 6| Step: 13
Training loss: 5.344743251800537
Validation loss: 3.9800051848093667

Epoch: 20| Step: 0
Training loss: 3.5654382705688477
Validation loss: 3.9740946292877197

Epoch: 6| Step: 1
Training loss: 4.619482517242432
Validation loss: 3.968078335126241

Epoch: 6| Step: 2
Training loss: 3.501314640045166
Validation loss: 3.9642587900161743

Epoch: 6| Step: 3
Training loss: 4.350152015686035
Validation loss: 3.9593919118245444

Epoch: 6| Step: 4
Training loss: 3.995316982269287
Validation loss: 3.955589850743612

Epoch: 6| Step: 5
Training loss: 4.396929740905762
Validation loss: 3.95081357161204

Epoch: 6| Step: 6
Training loss: 4.28162145614624
Validation loss: 3.9455729722976685

Epoch: 6| Step: 7
Training loss: 4.5695905685424805
Validation loss: 3.9404208262761435

Epoch: 6| Step: 8
Training loss: 3.343052864074707
Validation loss: 3.9351150592168174

Epoch: 6| Step: 9
Training loss: 3.5344326496124268
Validation loss: 3.929779291152954

Epoch: 6| Step: 10
Training loss: 4.278403282165527
Validation loss: 3.925043304761251

Epoch: 6| Step: 11
Training loss: 4.408219337463379
Validation loss: 3.918739120165507

Epoch: 6| Step: 12
Training loss: 4.493183612823486
Validation loss: 3.9142780701319375

Epoch: 6| Step: 13
Training loss: 3.891800880432129
Validation loss: 3.9080228010813394

Epoch: 21| Step: 0
Training loss: 3.3820042610168457
Validation loss: 3.9043135245641074

Epoch: 6| Step: 1
Training loss: 3.5880050659179688
Validation loss: 3.9001729091008506

Epoch: 6| Step: 2
Training loss: 3.6099166870117188
Validation loss: 3.8942192792892456

Epoch: 6| Step: 3
Training loss: 4.192879676818848
Validation loss: 3.8894745111465454

Epoch: 6| Step: 4
Training loss: 2.984184741973877
Validation loss: 3.8846941391626992

Epoch: 6| Step: 5
Training loss: 3.9476826190948486
Validation loss: 3.88045601050059

Epoch: 6| Step: 6
Training loss: 4.581396579742432
Validation loss: 3.8753390312194824

Epoch: 6| Step: 7
Training loss: 4.13533878326416
Validation loss: 3.8699695269266763

Epoch: 6| Step: 8
Training loss: 4.881397724151611
Validation loss: 3.866104086240133

Epoch: 6| Step: 9
Training loss: 4.755664825439453
Validation loss: 3.8605966567993164

Epoch: 6| Step: 10
Training loss: 3.772500991821289
Validation loss: 3.8559661308924356

Epoch: 6| Step: 11
Training loss: 4.859174728393555
Validation loss: 3.8507132530212402

Epoch: 6| Step: 12
Training loss: 3.9077491760253906
Validation loss: 3.8460108041763306

Epoch: 6| Step: 13
Training loss: 3.70621657371521
Validation loss: 3.840384840965271

Epoch: 22| Step: 0
Training loss: 3.486220359802246
Validation loss: 3.835213224093119

Epoch: 6| Step: 1
Training loss: 3.772862434387207
Validation loss: 3.830552260080973

Epoch: 6| Step: 2
Training loss: 4.520078659057617
Validation loss: 3.824647148450216

Epoch: 6| Step: 3
Training loss: 4.64640998840332
Validation loss: 3.8203824361165366

Epoch: 6| Step: 4
Training loss: 3.6558334827423096
Validation loss: 3.81592333316803

Epoch: 6| Step: 5
Training loss: 5.166258811950684
Validation loss: 3.8113956451416016

Epoch: 6| Step: 6
Training loss: 3.9923605918884277
Validation loss: 3.806527018547058

Epoch: 6| Step: 7
Training loss: 3.585179328918457
Validation loss: 3.8018720149993896

Epoch: 6| Step: 8
Training loss: 3.3395779132843018
Validation loss: 3.7966811259587607

Epoch: 6| Step: 9
Training loss: 4.277161121368408
Validation loss: 3.7925088008244834

Epoch: 6| Step: 10
Training loss: 3.6442036628723145
Validation loss: 3.7872692346572876

Epoch: 6| Step: 11
Training loss: 3.7583978176116943
Validation loss: 3.7829999526341758

Epoch: 6| Step: 12
Training loss: 3.48073673248291
Validation loss: 3.7782806555430093

Epoch: 6| Step: 13
Training loss: 4.1262993812561035
Validation loss: 3.773252089818319

Epoch: 23| Step: 0
Training loss: 5.133829116821289
Validation loss: 3.768695036570231

Epoch: 6| Step: 1
Training loss: 3.9647376537323
Validation loss: 3.7651226917902627

Epoch: 6| Step: 2
Training loss: 4.519336700439453
Validation loss: 3.759711980819702

Epoch: 6| Step: 3
Training loss: 4.107969284057617
Validation loss: 3.754940986633301

Epoch: 6| Step: 4
Training loss: 3.8010847568511963
Validation loss: 3.750885844230652

Epoch: 6| Step: 5
Training loss: 4.814851760864258
Validation loss: 3.745958924293518

Epoch: 6| Step: 6
Training loss: 4.555445671081543
Validation loss: 3.741386572519938

Epoch: 6| Step: 7
Training loss: 3.7497825622558594
Validation loss: 3.7364392280578613

Epoch: 6| Step: 8
Training loss: 3.859530210494995
Validation loss: 3.731925924619039

Epoch: 6| Step: 9
Training loss: 3.3416690826416016
Validation loss: 3.7275871435801187

Epoch: 6| Step: 10
Training loss: 3.4645955562591553
Validation loss: 3.7228766679763794

Epoch: 6| Step: 11
Training loss: 1.8064007759094238
Validation loss: 3.7186489502588906

Epoch: 6| Step: 12
Training loss: 3.552401065826416
Validation loss: 3.714658260345459

Epoch: 6| Step: 13
Training loss: 3.930952310562134
Validation loss: 3.710792819658915

Epoch: 24| Step: 0
Training loss: 4.4986090660095215
Validation loss: 3.7067678372065225

Epoch: 6| Step: 1
Training loss: 3.8968591690063477
Validation loss: 3.701280434926351

Epoch: 6| Step: 2
Training loss: 4.602254390716553
Validation loss: 3.6973088582356772

Epoch: 6| Step: 3
Training loss: 3.616529941558838
Validation loss: 3.6925313472747803

Epoch: 6| Step: 4
Training loss: 4.571822166442871
Validation loss: 3.6883891423543296

Epoch: 6| Step: 5
Training loss: 3.5564651489257812
Validation loss: 3.682807127634684

Epoch: 6| Step: 6
Training loss: 2.9031760692596436
Validation loss: 3.678972363471985

Epoch: 6| Step: 7
Training loss: 3.7457022666931152
Validation loss: 3.6744810740152993

Epoch: 6| Step: 8
Training loss: 3.4129130840301514
Validation loss: 3.669787367184957

Epoch: 6| Step: 9
Training loss: 3.4270904064178467
Validation loss: 3.665347615877787

Epoch: 6| Step: 10
Training loss: 2.8395063877105713
Validation loss: 3.6617408990859985

Epoch: 6| Step: 11
Training loss: 4.027651786804199
Validation loss: 3.6563274463017783

Epoch: 6| Step: 12
Training loss: 4.030600070953369
Validation loss: 3.6529069344202676

Epoch: 6| Step: 13
Training loss: 4.648703575134277
Validation loss: 3.6486547390619912

Epoch: 25| Step: 0
Training loss: 3.6276235580444336
Validation loss: 3.644500494003296

Epoch: 6| Step: 1
Training loss: 3.713250160217285
Validation loss: 3.640201727549235

Epoch: 6| Step: 2
Training loss: 4.25307035446167
Validation loss: 3.636638045310974

Epoch: 6| Step: 3
Training loss: 3.9849579334259033
Validation loss: 3.6320271094640098

Epoch: 6| Step: 4
Training loss: 4.203895568847656
Validation loss: 3.628334641456604

Epoch: 6| Step: 5
Training loss: 3.920084238052368
Validation loss: 3.624857783317566

Epoch: 6| Step: 6
Training loss: 3.676827907562256
Validation loss: 3.6216543515523276

Epoch: 6| Step: 7
Training loss: 3.9264016151428223
Validation loss: 3.617629130681356

Epoch: 6| Step: 8
Training loss: 4.195282459259033
Validation loss: 3.6127281983693442

Epoch: 6| Step: 9
Training loss: 2.7183334827423096
Validation loss: 3.6080976327260337

Epoch: 6| Step: 10
Training loss: 3.840620756149292
Validation loss: 3.603664676348368

Epoch: 6| Step: 11
Training loss: 4.2565765380859375
Validation loss: 3.599415977795919

Epoch: 6| Step: 12
Training loss: 3.05192494392395
Validation loss: 3.5948805809020996

Epoch: 6| Step: 13
Training loss: 3.5655834674835205
Validation loss: 3.5902557373046875

Epoch: 26| Step: 0
Training loss: 3.86613130569458
Validation loss: 3.5861777861913047

Epoch: 6| Step: 1
Training loss: 4.254907608032227
Validation loss: 3.5821901162465415

Epoch: 6| Step: 2
Training loss: 4.527300834655762
Validation loss: 3.5780324141184487

Epoch: 6| Step: 3
Training loss: 3.594294548034668
Validation loss: 3.5738170941670737

Epoch: 6| Step: 4
Training loss: 3.6680350303649902
Validation loss: 3.56928813457489

Epoch: 6| Step: 5
Training loss: 3.0210914611816406
Validation loss: 3.564328749974569

Epoch: 6| Step: 6
Training loss: 3.5731048583984375
Validation loss: 3.559572180112203

Epoch: 6| Step: 7
Training loss: 3.388043165206909
Validation loss: 3.5552210013071694

Epoch: 6| Step: 8
Training loss: 3.849029064178467
Validation loss: 3.5506684382756553

Epoch: 6| Step: 9
Training loss: 3.1673731803894043
Validation loss: 3.5461882750193277

Epoch: 6| Step: 10
Training loss: 3.4937477111816406
Validation loss: 3.5421653191248574

Epoch: 6| Step: 11
Training loss: 4.488155364990234
Validation loss: 3.5377424558003745

Epoch: 6| Step: 12
Training loss: 3.4678521156311035
Validation loss: 3.5331610838572183

Epoch: 6| Step: 13
Training loss: 3.7598278522491455
Validation loss: 3.528825839360555

Epoch: 27| Step: 0
Training loss: 4.0186567306518555
Validation loss: 3.524813413619995

Epoch: 6| Step: 1
Training loss: 3.6928675174713135
Validation loss: 3.520345608393351

Epoch: 6| Step: 2
Training loss: 3.5753867626190186
Validation loss: 3.515804568926493

Epoch: 6| Step: 3
Training loss: 3.1571836471557617
Validation loss: 3.511517643928528

Epoch: 6| Step: 4
Training loss: 3.1606359481811523
Validation loss: 3.5074915488560996

Epoch: 6| Step: 5
Training loss: 3.4438552856445312
Validation loss: 3.5032978852589927

Epoch: 6| Step: 6
Training loss: 4.319862365722656
Validation loss: 3.499234398206075

Epoch: 6| Step: 7
Training loss: 3.8247385025024414
Validation loss: 3.49455734093984

Epoch: 6| Step: 8
Training loss: 2.8104400634765625
Validation loss: 3.490370750427246

Epoch: 6| Step: 9
Training loss: 3.2874271869659424
Validation loss: 3.4857859214146933

Epoch: 6| Step: 10
Training loss: 3.770285129547119
Validation loss: 3.4814154307047525

Epoch: 6| Step: 11
Training loss: 4.492473602294922
Validation loss: 3.4772595961888633

Epoch: 6| Step: 12
Training loss: 3.889707326889038
Validation loss: 3.472636262575785

Epoch: 6| Step: 13
Training loss: 3.8396804332733154
Validation loss: 3.468645532925924

Epoch: 28| Step: 0
Training loss: 4.641338348388672
Validation loss: 3.4641263484954834

Epoch: 6| Step: 1
Training loss: 1.766394853591919
Validation loss: 3.4598014752070108

Epoch: 6| Step: 2
Training loss: 4.144251346588135
Validation loss: 3.456022302309672

Epoch: 6| Step: 3
Training loss: 2.596526622772217
Validation loss: 3.451327681541443

Epoch: 6| Step: 4
Training loss: 4.618450164794922
Validation loss: 3.446862538655599

Epoch: 6| Step: 5
Training loss: 4.450929641723633
Validation loss: 3.4427438179651895

Epoch: 6| Step: 6
Training loss: 3.5836760997772217
Validation loss: 3.438235799471537

Epoch: 6| Step: 7
Training loss: 3.92927885055542
Validation loss: 3.4337277015050254

Epoch: 6| Step: 8
Training loss: 3.2027039527893066
Validation loss: 3.4296498696009317

Epoch: 6| Step: 9
Training loss: 2.8731589317321777
Validation loss: 3.4254415035247803

Epoch: 6| Step: 10
Training loss: 3.8640220165252686
Validation loss: 3.421148180961609

Epoch: 6| Step: 11
Training loss: 3.471630573272705
Validation loss: 3.4168580770492554

Epoch: 6| Step: 12
Training loss: 4.070133686065674
Validation loss: 3.412412643432617

Epoch: 6| Step: 13
Training loss: 3.2478866577148438
Validation loss: 3.4082369804382324

Epoch: 29| Step: 0
Training loss: 3.4730372428894043
Validation loss: 3.403968175252279

Epoch: 6| Step: 1
Training loss: 3.9378979206085205
Validation loss: 3.399802803993225

Epoch: 6| Step: 2
Training loss: 4.127711296081543
Validation loss: 3.3957000573476157

Epoch: 6| Step: 3
Training loss: 2.222238540649414
Validation loss: 3.391443967819214

Epoch: 6| Step: 4
Training loss: 4.383828639984131
Validation loss: 3.3871570428212485

Epoch: 6| Step: 5
Training loss: 4.554747581481934
Validation loss: 3.3828408320744834

Epoch: 6| Step: 6
Training loss: 3.008096218109131
Validation loss: 3.3783117135365806

Epoch: 6| Step: 7
Training loss: 3.0103232860565186
Validation loss: 3.3739766677220664

Epoch: 6| Step: 8
Training loss: 4.2498602867126465
Validation loss: 3.369654138882955

Epoch: 6| Step: 9
Training loss: 3.1973228454589844
Validation loss: 3.3653295834859214

Epoch: 6| Step: 10
Training loss: 4.254877090454102
Validation loss: 3.361060460408529

Epoch: 6| Step: 11
Training loss: 2.242229700088501
Validation loss: 3.3569571574529014

Epoch: 6| Step: 12
Training loss: 3.236886978149414
Validation loss: 3.352841337521871

Epoch: 6| Step: 13
Training loss: 3.773827075958252
Validation loss: 3.3484519720077515

Epoch: 30| Step: 0
Training loss: 3.410724639892578
Validation loss: 3.3447072505950928

Epoch: 6| Step: 1
Training loss: 3.6025421619415283
Validation loss: 3.3402002652486167

Epoch: 6| Step: 2
Training loss: 2.7098331451416016
Validation loss: 3.336180647214254

Epoch: 6| Step: 3
Training loss: 3.8106112480163574
Validation loss: 3.3321726322174072

Epoch: 6| Step: 4
Training loss: 3.32861328125
Validation loss: 3.3275678157806396

Epoch: 6| Step: 5
Training loss: 4.741164207458496
Validation loss: 3.3233808676401773

Epoch: 6| Step: 6
Training loss: 3.3209807872772217
Validation loss: 3.318742354710897

Epoch: 6| Step: 7
Training loss: 3.8486461639404297
Validation loss: 3.3144665161768594

Epoch: 6| Step: 8
Training loss: 3.658477306365967
Validation loss: 3.3106257915496826

Epoch: 6| Step: 9
Training loss: 3.543182373046875
Validation loss: 3.3052390416463218

Epoch: 6| Step: 10
Training loss: 3.168733596801758
Validation loss: 3.3013847271601358

Epoch: 6| Step: 11
Training loss: 3.4416463375091553
Validation loss: 3.297100822130839

Epoch: 6| Step: 12
Training loss: 2.605203151702881
Validation loss: 3.293588876724243

Epoch: 6| Step: 13
Training loss: 3.6830861568450928
Validation loss: 3.2900472482045493

Epoch: 31| Step: 0
Training loss: 3.350884199142456
Validation loss: 3.2859761714935303

Epoch: 6| Step: 1
Training loss: 2.7930870056152344
Validation loss: 3.2818557818730674

Epoch: 6| Step: 2
Training loss: 3.227710247039795
Validation loss: 3.2772708336512246

Epoch: 6| Step: 3
Training loss: 4.318915367126465
Validation loss: 3.2729663848876953

Epoch: 6| Step: 4
Training loss: 2.7548575401306152
Validation loss: 3.2687760392824807

Epoch: 6| Step: 5
Training loss: 3.614497661590576
Validation loss: 3.2648008267084756

Epoch: 6| Step: 6
Training loss: 3.237124443054199
Validation loss: 3.260919451713562

Epoch: 6| Step: 7
Training loss: 3.0679218769073486
Validation loss: 3.2580113410949707

Epoch: 6| Step: 8
Training loss: 3.4721059799194336
Validation loss: 3.2536935011545816

Epoch: 6| Step: 9
Training loss: 4.859443187713623
Validation loss: 3.2503450314203897

Epoch: 6| Step: 10
Training loss: 3.0834898948669434
Validation loss: 3.245458443959554

Epoch: 6| Step: 11
Training loss: 3.2104411125183105
Validation loss: 3.2414053678512573

Epoch: 6| Step: 12
Training loss: 3.2790584564208984
Validation loss: 3.237568656603495

Epoch: 6| Step: 13
Training loss: 3.859351396560669
Validation loss: 3.2332107623418174

Epoch: 32| Step: 0
Training loss: 4.779980659484863
Validation loss: 3.2294169664382935

Epoch: 6| Step: 1
Training loss: 2.889057159423828
Validation loss: 3.2249879042307534

Epoch: 6| Step: 2
Training loss: 3.035862922668457
Validation loss: 3.220369259516398

Epoch: 6| Step: 3
Training loss: 2.6681947708129883
Validation loss: 3.215974450111389

Epoch: 6| Step: 4
Training loss: 3.1383752822875977
Validation loss: 3.211719592412313

Epoch: 6| Step: 5
Training loss: 3.474094867706299
Validation loss: 3.207510511080424

Epoch: 6| Step: 6
Training loss: 3.05159068107605
Validation loss: 3.2032247384389243

Epoch: 6| Step: 7
Training loss: 3.217343330383301
Validation loss: 3.198892672856649

Epoch: 6| Step: 8
Training loss: 3.9788997173309326
Validation loss: 3.1951520442962646

Epoch: 6| Step: 9
Training loss: 3.2217798233032227
Validation loss: 3.1904667615890503

Epoch: 6| Step: 10
Training loss: 3.8563575744628906
Validation loss: 3.188416361808777

Epoch: 6| Step: 11
Training loss: 2.639282703399658
Validation loss: 3.183615962664286

Epoch: 6| Step: 12
Training loss: 3.7134501934051514
Validation loss: 3.1790777842203775

Epoch: 6| Step: 13
Training loss: 3.716062307357788
Validation loss: 3.175271133581797

Epoch: 33| Step: 0
Training loss: 4.180296897888184
Validation loss: 3.171113610267639

Epoch: 6| Step: 1
Training loss: 3.2559380531311035
Validation loss: 3.1668463945388794

Epoch: 6| Step: 2
Training loss: 3.8702585697174072
Validation loss: 3.1629412174224854

Epoch: 6| Step: 3
Training loss: 2.8639440536499023
Validation loss: 3.1588332653045654

Epoch: 6| Step: 4
Training loss: 2.6939010620117188
Validation loss: 3.154948353767395

Epoch: 6| Step: 5
Training loss: 3.4293391704559326
Validation loss: 3.1511831283569336

Epoch: 6| Step: 6
Training loss: 2.1598873138427734
Validation loss: 3.1477067867914834

Epoch: 6| Step: 7
Training loss: 3.545275926589966
Validation loss: 3.143691380818685

Epoch: 6| Step: 8
Training loss: 3.1737029552459717
Validation loss: 3.139951467514038

Epoch: 6| Step: 9
Training loss: 3.187891960144043
Validation loss: 3.136340618133545

Epoch: 6| Step: 10
Training loss: 4.407764434814453
Validation loss: 3.1330498854319253

Epoch: 6| Step: 11
Training loss: 3.0017356872558594
Validation loss: 3.1295785109202066

Epoch: 6| Step: 12
Training loss: 3.3174004554748535
Validation loss: 3.125810742378235

Epoch: 6| Step: 13
Training loss: 3.5547585487365723
Validation loss: 3.121880531311035

Epoch: 34| Step: 0
Training loss: 3.538504123687744
Validation loss: 3.1178868214289346

Epoch: 6| Step: 1
Training loss: 3.837618350982666
Validation loss: 3.113980213801066

Epoch: 6| Step: 2
Training loss: 4.139455318450928
Validation loss: 3.1093618472417197

Epoch: 6| Step: 3
Training loss: 2.8124828338623047
Validation loss: 3.104780832926432

Epoch: 6| Step: 4
Training loss: 2.3738698959350586
Validation loss: 3.1004669268925986

Epoch: 6| Step: 5
Training loss: 2.677429676055908
Validation loss: 3.0969424645105996

Epoch: 6| Step: 6
Training loss: 2.7224156856536865
Validation loss: 3.0932379563649497

Epoch: 6| Step: 7
Training loss: 3.5012407302856445
Validation loss: 3.0898176431655884

Epoch: 6| Step: 8
Training loss: 3.292273998260498
Validation loss: 3.0864007472991943

Epoch: 6| Step: 9
Training loss: 3.1916301250457764
Validation loss: 3.0832013289133706

Epoch: 6| Step: 10
Training loss: 3.1551027297973633
Validation loss: 3.0796731313069663

Epoch: 6| Step: 11
Training loss: 3.2924575805664062
Validation loss: 3.0764711697896323

Epoch: 6| Step: 12
Training loss: 3.6109185218811035
Validation loss: 3.072920282681783

Epoch: 6| Step: 13
Training loss: 3.832082748413086
Validation loss: 3.0694326957066855

Epoch: 35| Step: 0
Training loss: 2.5016024112701416
Validation loss: 3.066149632136027

Epoch: 6| Step: 1
Training loss: 3.0244429111480713
Validation loss: 3.062713702519735

Epoch: 6| Step: 2
Training loss: 3.897611618041992
Validation loss: 3.059468626976013

Epoch: 6| Step: 3
Training loss: 2.4659886360168457
Validation loss: 3.0559657414754233

Epoch: 6| Step: 4
Training loss: 2.5217933654785156
Validation loss: 3.0524346828460693

Epoch: 6| Step: 5
Training loss: 2.9205679893493652
Validation loss: 3.0492541790008545

Epoch: 6| Step: 6
Training loss: 3.0182275772094727
Validation loss: 3.0462600191434226

Epoch: 6| Step: 7
Training loss: 2.9753198623657227
Validation loss: 3.043335954348246

Epoch: 6| Step: 8
Training loss: 3.552386999130249
Validation loss: 3.040627916653951

Epoch: 6| Step: 9
Training loss: 3.4390242099761963
Validation loss: 3.037656585375468

Epoch: 6| Step: 10
Training loss: 3.1899149417877197
Validation loss: 3.034590244293213

Epoch: 6| Step: 11
Training loss: 4.059007167816162
Validation loss: 3.0317301750183105

Epoch: 6| Step: 12
Training loss: 4.266690254211426
Validation loss: 3.0282443364461265

Epoch: 6| Step: 13
Training loss: 3.4507193565368652
Validation loss: 3.0249370336532593

Epoch: 36| Step: 0
Training loss: 2.9940438270568848
Validation loss: 3.0215367476145425

Epoch: 6| Step: 1
Training loss: 3.5051982402801514
Validation loss: 3.01765771706899

Epoch: 6| Step: 2
Training loss: 3.9613072872161865
Validation loss: 3.014045238494873

Epoch: 6| Step: 3
Training loss: 3.2531747817993164
Validation loss: 3.01009738445282

Epoch: 6| Step: 4
Training loss: 3.2899422645568848
Validation loss: 3.0068160692850747

Epoch: 6| Step: 5
Training loss: 2.8491504192352295
Validation loss: 3.0028954346974692

Epoch: 6| Step: 6
Training loss: 2.599971294403076
Validation loss: 2.999422470728556

Epoch: 6| Step: 7
Training loss: 2.8065624237060547
Validation loss: 2.9959909121195474

Epoch: 6| Step: 8
Training loss: 3.470646619796753
Validation loss: 2.9929341475168862

Epoch: 6| Step: 9
Training loss: 3.2514054775238037
Validation loss: 2.989561756451925

Epoch: 6| Step: 10
Training loss: 3.337007522583008
Validation loss: 2.9860184590021768

Epoch: 6| Step: 11
Training loss: 3.309389591217041
Validation loss: 2.9828722874323526

Epoch: 6| Step: 12
Training loss: 3.4760875701904297
Validation loss: 2.979496717453003

Epoch: 6| Step: 13
Training loss: 2.6197946071624756
Validation loss: 2.976059834162394

Epoch: 37| Step: 0
Training loss: 2.812995433807373
Validation loss: 2.972736636797587

Epoch: 6| Step: 1
Training loss: 2.2871437072753906
Validation loss: 2.969672679901123

Epoch: 6| Step: 2
Training loss: 3.007544994354248
Validation loss: 2.966467579205831

Epoch: 6| Step: 3
Training loss: 3.3957252502441406
Validation loss: 2.963728149731954

Epoch: 6| Step: 4
Training loss: 3.864096164703369
Validation loss: 2.960725784301758

Epoch: 6| Step: 5
Training loss: 3.2430453300476074
Validation loss: 2.9576682249704995

Epoch: 6| Step: 6
Training loss: 3.1172406673431396
Validation loss: 2.9544278581937156

Epoch: 6| Step: 7
Training loss: 3.795502185821533
Validation loss: 2.9515605370203652

Epoch: 6| Step: 8
Training loss: 3.1072025299072266
Validation loss: 2.9488083521525064

Epoch: 6| Step: 9
Training loss: 3.4354031085968018
Validation loss: 2.9456483125686646

Epoch: 6| Step: 10
Training loss: 2.9853806495666504
Validation loss: 2.9421945810317993

Epoch: 6| Step: 11
Training loss: 2.7865190505981445
Validation loss: 2.9388447602589927

Epoch: 6| Step: 12
Training loss: 3.2076311111450195
Validation loss: 2.9354113737742105

Epoch: 6| Step: 13
Training loss: 3.0861239433288574
Validation loss: 2.9323039452234902

Epoch: 38| Step: 0
Training loss: 3.0946977138519287
Validation loss: 2.9289215803146362

Epoch: 6| Step: 1
Training loss: 3.7128758430480957
Validation loss: 2.925684173901876

Epoch: 6| Step: 2
Training loss: 3.854184865951538
Validation loss: 2.9223021268844604

Epoch: 6| Step: 3
Training loss: 2.5875821113586426
Validation loss: 2.918928543726603

Epoch: 6| Step: 4
Training loss: 2.9497785568237305
Validation loss: 2.9157028992970786

Epoch: 6| Step: 5
Training loss: 3.399909734725952
Validation loss: 2.912853797276815

Epoch: 6| Step: 6
Training loss: 3.3492212295532227
Validation loss: 2.909820834795634

Epoch: 6| Step: 7
Training loss: 3.555468797683716
Validation loss: 2.9067424138387046

Epoch: 6| Step: 8
Training loss: 2.978969097137451
Validation loss: 2.9036772648493447

Epoch: 6| Step: 9
Training loss: 2.8873486518859863
Validation loss: 2.900580565134684

Epoch: 6| Step: 10
Training loss: 2.961428642272949
Validation loss: 2.897231419881185

Epoch: 6| Step: 11
Training loss: 2.0841870307922363
Validation loss: 2.8943968613942466

Epoch: 6| Step: 12
Training loss: 2.598145008087158
Validation loss: 2.8912050326665244

Epoch: 6| Step: 13
Training loss: 3.591233253479004
Validation loss: 2.888416369756063

Epoch: 39| Step: 0
Training loss: 2.50925874710083
Validation loss: 2.8855653603871665

Epoch: 6| Step: 1
Training loss: 3.400454521179199
Validation loss: 2.8821094830830893

Epoch: 6| Step: 2
Training loss: 2.942274332046509
Validation loss: 2.879491686820984

Epoch: 6| Step: 3
Training loss: 3.642645835876465
Validation loss: 2.876505891482035

Epoch: 6| Step: 4
Training loss: 3.1906914710998535
Validation loss: 2.8733694156010947

Epoch: 6| Step: 5
Training loss: 3.301178216934204
Validation loss: 2.870408614476522

Epoch: 6| Step: 6
Training loss: 3.15146541595459
Validation loss: 2.867525060971578

Epoch: 6| Step: 7
Training loss: 2.8504414558410645
Validation loss: 2.864437381426493

Epoch: 6| Step: 8
Training loss: 2.632990837097168
Validation loss: 2.8613078196843467

Epoch: 6| Step: 9
Training loss: 2.956313133239746
Validation loss: 2.858338793118795

Epoch: 6| Step: 10
Training loss: 2.7768402099609375
Validation loss: 2.855377952257792

Epoch: 6| Step: 11
Training loss: 3.0045905113220215
Validation loss: 2.8525989055633545

Epoch: 6| Step: 12
Training loss: 3.1082897186279297
Validation loss: 2.8500465552012124

Epoch: 6| Step: 13
Training loss: 3.595656156539917
Validation loss: 2.8471231857935586

Epoch: 40| Step: 0
Training loss: 3.7382609844207764
Validation loss: 2.8443752924601235

Epoch: 6| Step: 1
Training loss: 2.2246882915496826
Validation loss: 2.8410937786102295

Epoch: 6| Step: 2
Training loss: 3.337862253189087
Validation loss: 2.838454842567444

Epoch: 6| Step: 3
Training loss: 3.1117568016052246
Validation loss: 2.835517644882202

Epoch: 6| Step: 4
Training loss: 3.04475736618042
Validation loss: 2.8328656355539956

Epoch: 6| Step: 5
Training loss: 3.257660388946533
Validation loss: 2.8299120664596558

Epoch: 6| Step: 6
Training loss: 3.3829970359802246
Validation loss: 2.8270791371663413

Epoch: 6| Step: 7
Training loss: 2.842949390411377
Validation loss: 2.823976198832194

Epoch: 6| Step: 8
Training loss: 2.898831367492676
Validation loss: 2.8210196097691855

Epoch: 6| Step: 9
Training loss: 2.882086992263794
Validation loss: 2.817734638849894

Epoch: 6| Step: 10
Training loss: 2.3599259853363037
Validation loss: 2.8149068355560303

Epoch: 6| Step: 11
Training loss: 3.3966526985168457
Validation loss: 2.812108278274536

Epoch: 6| Step: 12
Training loss: 2.6060190200805664
Validation loss: 2.809611956278483

Epoch: 6| Step: 13
Training loss: 3.4661941528320312
Validation loss: 2.8069265286127725

Epoch: 41| Step: 0
Training loss: 3.9382410049438477
Validation loss: 2.8045366605122886

Epoch: 6| Step: 1
Training loss: 3.824704647064209
Validation loss: 2.8016324440638223

Epoch: 6| Step: 2
Training loss: 2.5578560829162598
Validation loss: 2.7990482648213706

Epoch: 6| Step: 3
Training loss: 3.1941771507263184
Validation loss: 2.7965089082717896

Epoch: 6| Step: 4
Training loss: 3.920365571975708
Validation loss: 2.793853759765625

Epoch: 6| Step: 5
Training loss: 2.7023444175720215
Validation loss: 2.791005770365397

Epoch: 6| Step: 6
Training loss: 2.551356792449951
Validation loss: 2.788469672203064

Epoch: 6| Step: 7
Training loss: 2.5259017944335938
Validation loss: 2.785597324371338

Epoch: 6| Step: 8
Training loss: 2.3860116004943848
Validation loss: 2.783560593922933

Epoch: 6| Step: 9
Training loss: 3.0991761684417725
Validation loss: 2.7812029918034873

Epoch: 6| Step: 10
Training loss: 2.578981876373291
Validation loss: 2.7781553268432617

Epoch: 6| Step: 11
Training loss: 2.407283306121826
Validation loss: 2.776052554448446

Epoch: 6| Step: 12
Training loss: 2.981903553009033
Validation loss: 2.7734762827555337

Epoch: 6| Step: 13
Training loss: 3.344360113143921
Validation loss: 2.77104389667511

Epoch: 42| Step: 0
Training loss: 3.0805106163024902
Validation loss: 2.7682753006617227

Epoch: 6| Step: 1
Training loss: 2.2183732986450195
Validation loss: 2.766167163848877

Epoch: 6| Step: 2
Training loss: 2.2844245433807373
Validation loss: 2.7639247377713523

Epoch: 6| Step: 3
Training loss: 2.4656693935394287
Validation loss: 2.7619179487228394

Epoch: 6| Step: 4
Training loss: 2.8078927993774414
Validation loss: 2.7599018216133118

Epoch: 6| Step: 5
Training loss: 3.1824607849121094
Validation loss: 2.7574996948242188

Epoch: 6| Step: 6
Training loss: 3.485044479370117
Validation loss: 2.7552664279937744

Epoch: 6| Step: 7
Training loss: 3.665802001953125
Validation loss: 2.7530192931493125

Epoch: 6| Step: 8
Training loss: 2.1518006324768066
Validation loss: 2.750138282775879

Epoch: 6| Step: 9
Training loss: 2.9981558322906494
Validation loss: 2.747398773829142

Epoch: 6| Step: 10
Training loss: 3.200404405593872
Validation loss: 2.744783083597819

Epoch: 6| Step: 11
Training loss: 2.8957605361938477
Validation loss: 2.741874853769938

Epoch: 6| Step: 12
Training loss: 3.161234140396118
Validation loss: 2.7396075328191123

Epoch: 6| Step: 13
Training loss: 3.8700942993164062
Validation loss: 2.7368763287862143

Epoch: 43| Step: 0
Training loss: 2.7151076793670654
Validation loss: 2.7337438265482583

Epoch: 6| Step: 1
Training loss: 3.458784580230713
Validation loss: 2.7314648628234863

Epoch: 6| Step: 2
Training loss: 2.06388258934021
Validation loss: 2.728519161542257

Epoch: 6| Step: 3
Training loss: 3.4315121173858643
Validation loss: 2.7257593870162964

Epoch: 6| Step: 4
Training loss: 2.649404525756836
Validation loss: 2.723451534907023

Epoch: 6| Step: 5
Training loss: 3.0264949798583984
Validation loss: 2.720239996910095

Epoch: 6| Step: 6
Training loss: 3.095397472381592
Validation loss: 2.717114249865214

Epoch: 6| Step: 7
Training loss: 3.1404647827148438
Validation loss: 2.714658180872599

Epoch: 6| Step: 8
Training loss: 2.968576669692993
Validation loss: 2.7123099168141684

Epoch: 6| Step: 9
Training loss: 2.820868968963623
Validation loss: 2.7091912031173706

Epoch: 6| Step: 10
Training loss: 2.8678317070007324
Validation loss: 2.7071353793144226

Epoch: 6| Step: 11
Training loss: 3.1790971755981445
Validation loss: 2.703845977783203

Epoch: 6| Step: 12
Training loss: 3.1207144260406494
Validation loss: 2.7008965412775674

Epoch: 6| Step: 13
Training loss: 2.4237818717956543
Validation loss: 2.6979164679845176

Epoch: 44| Step: 0
Training loss: 2.744380235671997
Validation loss: 2.6961814165115356

Epoch: 6| Step: 1
Training loss: 2.7541487216949463
Validation loss: 2.693401058514913

Epoch: 6| Step: 2
Training loss: 3.7688887119293213
Validation loss: 2.690850019454956

Epoch: 6| Step: 3
Training loss: 2.7023632526397705
Validation loss: 2.687939683596293

Epoch: 6| Step: 4
Training loss: 3.4214138984680176
Validation loss: 2.6853570540746055

Epoch: 6| Step: 5
Training loss: 2.777710199356079
Validation loss: 2.6825634241104126

Epoch: 6| Step: 6
Training loss: 2.8191683292388916
Validation loss: 2.6804134448369346

Epoch: 6| Step: 7
Training loss: 3.6080291271209717
Validation loss: 2.6776047945022583

Epoch: 6| Step: 8
Training loss: 2.7127864360809326
Validation loss: 2.6749407847722373

Epoch: 6| Step: 9
Training loss: 2.7210681438446045
Validation loss: 2.6721429228782654

Epoch: 6| Step: 10
Training loss: 2.5596134662628174
Validation loss: 2.6692363023757935

Epoch: 6| Step: 11
Training loss: 2.7029130458831787
Validation loss: 2.6665800412495932

Epoch: 6| Step: 12
Training loss: 2.201908588409424
Validation loss: 2.6624867717425027

Epoch: 6| Step: 13
Training loss: 2.931147813796997
Validation loss: 2.657095948855082

Epoch: 45| Step: 0
Training loss: 2.7492098808288574
Validation loss: 2.654053966204325

Epoch: 6| Step: 1
Training loss: 3.035508155822754
Validation loss: 2.652403394381205

Epoch: 6| Step: 2
Training loss: 2.624128580093384
Validation loss: 2.6480120420455933

Epoch: 6| Step: 3
Training loss: 2.647289991378784
Validation loss: 2.646993080774943

Epoch: 6| Step: 4
Training loss: 2.7327206134796143
Validation loss: 2.6441069841384888

Epoch: 6| Step: 5
Training loss: 2.9702811241149902
Validation loss: 2.642409563064575

Epoch: 6| Step: 6
Training loss: 3.106508255004883
Validation loss: 2.6397239764531455

Epoch: 6| Step: 7
Training loss: 2.956418514251709
Validation loss: 2.6364359060923257

Epoch: 6| Step: 8
Training loss: 2.159132719039917
Validation loss: 2.6341217358907065

Epoch: 6| Step: 9
Training loss: 3.0180935859680176
Validation loss: 2.630285104115804

Epoch: 6| Step: 10
Training loss: 3.1451191902160645
Validation loss: 2.62874702612559

Epoch: 6| Step: 11
Training loss: 3.270646095275879
Validation loss: 2.6283220847447715

Epoch: 6| Step: 12
Training loss: 2.6185483932495117
Validation loss: 2.62229331334432

Epoch: 6| Step: 13
Training loss: 2.8279457092285156
Validation loss: 2.6197405656178794

Epoch: 46| Step: 0
Training loss: 2.6026906967163086
Validation loss: 2.618267297744751

Epoch: 6| Step: 1
Training loss: 3.669945240020752
Validation loss: 2.615405042966207

Epoch: 6| Step: 2
Training loss: 2.6558165550231934
Validation loss: 2.613367279370626

Epoch: 6| Step: 3
Training loss: 2.0642449855804443
Validation loss: 2.616862118244171

Epoch: 6| Step: 4
Training loss: 3.268031120300293
Validation loss: 2.6279183626174927

Epoch: 6| Step: 5
Training loss: 2.8026790618896484
Validation loss: 2.6074095567067466

Epoch: 6| Step: 6
Training loss: 2.8474485874176025
Validation loss: 2.603954553604126

Epoch: 6| Step: 7
Training loss: 2.4709973335266113
Validation loss: 2.6044869422912598

Epoch: 6| Step: 8
Training loss: 3.1793668270111084
Validation loss: 2.6094658374786377

Epoch: 6| Step: 9
Training loss: 3.452115058898926
Validation loss: 2.6171129941940308

Epoch: 6| Step: 10
Training loss: 2.7425975799560547
Validation loss: 2.6056368748346963

Epoch: 6| Step: 11
Training loss: 2.1101534366607666
Validation loss: 2.5981568495432534

Epoch: 6| Step: 12
Training loss: 3.046412706375122
Validation loss: 2.592760999997457

Epoch: 6| Step: 13
Training loss: 2.4393515586853027
Validation loss: 2.587814370791117

Epoch: 47| Step: 0
Training loss: 2.6804144382476807
Validation loss: 2.5852957566579184

Epoch: 6| Step: 1
Training loss: 1.995895504951477
Validation loss: 2.5821926991144815

Epoch: 6| Step: 2
Training loss: 2.685600757598877
Validation loss: 2.580299139022827

Epoch: 6| Step: 3
Training loss: 2.9977526664733887
Validation loss: 2.5888078610102334

Epoch: 6| Step: 4
Training loss: 2.9602608680725098
Validation loss: 2.5741780201594033

Epoch: 6| Step: 5
Training loss: 2.5096435546875
Validation loss: 2.5724024772644043

Epoch: 6| Step: 6
Training loss: 2.925957441329956
Validation loss: 2.5701183875401816

Epoch: 6| Step: 7
Training loss: 2.6407508850097656
Validation loss: 2.5691327253977456

Epoch: 6| Step: 8
Training loss: 3.124711513519287
Validation loss: 2.564351717631022

Epoch: 6| Step: 9
Training loss: 2.7969107627868652
Validation loss: 2.5639304320017495

Epoch: 6| Step: 10
Training loss: 2.8041367530822754
Validation loss: 2.5585009256998696

Epoch: 6| Step: 11
Training loss: 3.0652174949645996
Validation loss: 2.5559977293014526

Epoch: 6| Step: 12
Training loss: 2.7589635848999023
Validation loss: 2.553088585535685

Epoch: 6| Step: 13
Training loss: 2.869607448577881
Validation loss: 2.5491642157236734

Epoch: 48| Step: 0
Training loss: 3.629936695098877
Validation loss: 2.5503310163815818

Epoch: 6| Step: 1
Training loss: 3.1494696140289307
Validation loss: 2.550023396809896

Epoch: 6| Step: 2
Training loss: 2.579766035079956
Validation loss: 2.5485657453536987

Epoch: 6| Step: 3
Training loss: 3.29974365234375
Validation loss: 2.5430997212727866

Epoch: 6| Step: 4
Training loss: 2.8111379146575928
Validation loss: 2.541684945424398

Epoch: 6| Step: 5
Training loss: 2.5370330810546875
Validation loss: 2.5383705695470176

Epoch: 6| Step: 6
Training loss: 2.4574270248413086
Validation loss: 2.5357943773269653

Epoch: 6| Step: 7
Training loss: 2.6346256732940674
Validation loss: 2.5336145957310996

Epoch: 6| Step: 8
Training loss: 2.3774526119232178
Validation loss: 2.5331010023752847

Epoch: 6| Step: 9
Training loss: 2.570491313934326
Validation loss: 2.528973340988159

Epoch: 6| Step: 10
Training loss: 2.8603529930114746
Validation loss: 2.5251638094584146

Epoch: 6| Step: 11
Training loss: 2.033092975616455
Validation loss: 2.5229836304982505

Epoch: 6| Step: 12
Training loss: 2.96452260017395
Validation loss: 2.5260173082351685

Epoch: 6| Step: 13
Training loss: 2.3221445083618164
Validation loss: 2.5200219551722207

Epoch: 49| Step: 0
Training loss: 2.448093891143799
Validation loss: 2.51988693078359

Epoch: 6| Step: 1
Training loss: 2.644035816192627
Validation loss: 2.544431487719218

Epoch: 6| Step: 2
Training loss: 2.456409454345703
Validation loss: 2.548797825972239

Epoch: 6| Step: 3
Training loss: 2.5881195068359375
Validation loss: 2.5159778197606406

Epoch: 6| Step: 4
Training loss: 2.5296852588653564
Validation loss: 2.503337025642395

Epoch: 6| Step: 5
Training loss: 2.28696870803833
Validation loss: 2.502451161543528

Epoch: 6| Step: 6
Training loss: 3.064361572265625
Validation loss: 2.5016226371129355

Epoch: 6| Step: 7
Training loss: 2.8836617469787598
Validation loss: 2.500310699144999

Epoch: 6| Step: 8
Training loss: 2.8006691932678223
Validation loss: 2.4959716399510703

Epoch: 6| Step: 9
Training loss: 2.726422071456909
Validation loss: 2.4985968271891275

Epoch: 6| Step: 10
Training loss: 3.219172477722168
Validation loss: 2.4960611859957376

Epoch: 6| Step: 11
Training loss: 2.4078550338745117
Validation loss: 2.4950788418451944

Epoch: 6| Step: 12
Training loss: 2.479949474334717
Validation loss: 2.4916169246037803

Epoch: 6| Step: 13
Training loss: 3.2704052925109863
Validation loss: 2.490747809410095

Epoch: 50| Step: 0
Training loss: 2.84358811378479
Validation loss: 2.4892028172810874

Epoch: 6| Step: 1
Training loss: 2.522573947906494
Validation loss: 2.4850866993268332

Epoch: 6| Step: 2
Training loss: 3.181288242340088
Validation loss: 2.4815362294514975

Epoch: 6| Step: 3
Training loss: 1.8685522079467773
Validation loss: 2.481378118197123

Epoch: 6| Step: 4
Training loss: 2.723073959350586
Validation loss: 2.4805824160575867

Epoch: 6| Step: 5
Training loss: 3.2560808658599854
Validation loss: 2.4768073558807373

Epoch: 6| Step: 6
Training loss: 2.851752758026123
Validation loss: 2.4710007905960083

Epoch: 6| Step: 7
Training loss: 2.8843650817871094
Validation loss: 2.4678381085395813

Epoch: 6| Step: 8
Training loss: 2.8134024143218994
Validation loss: 2.462838888168335

Epoch: 6| Step: 9
Training loss: 2.1898679733276367
Validation loss: 2.46245410044988

Epoch: 6| Step: 10
Training loss: 2.1987576484680176
Validation loss: 2.4581110874811807

Epoch: 6| Step: 11
Training loss: 2.715824842453003
Validation loss: 2.458181063334147

Epoch: 6| Step: 12
Training loss: 2.308149814605713
Validation loss: 2.455512305100759

Epoch: 6| Step: 13
Training loss: 2.911994695663452
Validation loss: 2.455810268719991

Epoch: 51| Step: 0
Training loss: 2.5533218383789062
Validation loss: 2.4608660141626992

Epoch: 6| Step: 1
Training loss: 2.31061053276062
Validation loss: 2.4547001918156943

Epoch: 6| Step: 2
Training loss: 2.581770658493042
Validation loss: 2.4573520024617515

Epoch: 6| Step: 3
Training loss: 3.517315149307251
Validation loss: 2.4489949146906533

Epoch: 6| Step: 4
Training loss: 3.0233147144317627
Validation loss: 2.441818674405416

Epoch: 6| Step: 5
Training loss: 2.6024246215820312
Validation loss: 2.4387302001317344

Epoch: 6| Step: 6
Training loss: 2.0522468090057373
Validation loss: 2.4356638193130493

Epoch: 6| Step: 7
Training loss: 2.4476146697998047
Validation loss: 2.4348118702570596

Epoch: 6| Step: 8
Training loss: 3.2102482318878174
Validation loss: 2.432071030139923

Epoch: 6| Step: 9
Training loss: 2.6114368438720703
Validation loss: 2.4303818146387735

Epoch: 6| Step: 10
Training loss: 2.972332715988159
Validation loss: 2.4293795029322305

Epoch: 6| Step: 11
Training loss: 1.93661367893219
Validation loss: 2.4271411101023355

Epoch: 6| Step: 12
Training loss: 2.548793315887451
Validation loss: 2.4241779247919717

Epoch: 6| Step: 13
Training loss: 2.4827470779418945
Validation loss: 2.420559306939443

Epoch: 52| Step: 0
Training loss: 2.5461394786834717
Validation loss: 2.4207383394241333

Epoch: 6| Step: 1
Training loss: 2.6569509506225586
Validation loss: 2.416739066441854

Epoch: 6| Step: 2
Training loss: 2.9203555583953857
Validation loss: 2.4167051712671914

Epoch: 6| Step: 3
Training loss: 2.090247392654419
Validation loss: 2.416778246561686

Epoch: 6| Step: 4
Training loss: 1.9647767543792725
Validation loss: 2.411974827448527

Epoch: 6| Step: 5
Training loss: 2.763047456741333
Validation loss: 2.40882408618927

Epoch: 6| Step: 6
Training loss: 2.606642484664917
Validation loss: 2.4077287316322327

Epoch: 6| Step: 7
Training loss: 2.6554431915283203
Validation loss: 2.4033495585123696

Epoch: 6| Step: 8
Training loss: 2.756890296936035
Validation loss: 2.404509743054708

Epoch: 6| Step: 9
Training loss: 2.4099950790405273
Validation loss: 2.400451342264811

Epoch: 6| Step: 10
Training loss: 3.0502283573150635
Validation loss: 2.401555895805359

Epoch: 6| Step: 11
Training loss: 2.69730281829834
Validation loss: 2.3965200980504355

Epoch: 6| Step: 12
Training loss: 2.907565116882324
Validation loss: 2.3909136851628623

Epoch: 6| Step: 13
Training loss: 2.294989824295044
Validation loss: 2.393805225690206

Epoch: 53| Step: 0
Training loss: 2.4943385124206543
Validation loss: 2.393822213013967

Epoch: 6| Step: 1
Training loss: 2.526132345199585
Validation loss: 2.3937078714370728

Epoch: 6| Step: 2
Training loss: 1.833777666091919
Validation loss: 2.389576276143392

Epoch: 6| Step: 3
Training loss: 2.3572707176208496
Validation loss: 2.3853000005086265

Epoch: 6| Step: 4
Training loss: 3.2936015129089355
Validation loss: 2.3789051373799643

Epoch: 6| Step: 5
Training loss: 2.648874521255493
Validation loss: 2.381441354751587

Epoch: 6| Step: 6
Training loss: 1.8792524337768555
Validation loss: 2.376732885837555

Epoch: 6| Step: 7
Training loss: 2.8104665279388428
Validation loss: 2.376783827940623

Epoch: 6| Step: 8
Training loss: 2.4313833713531494
Validation loss: 2.373138109842936

Epoch: 6| Step: 9
Training loss: 2.4632811546325684
Validation loss: 2.368512809276581

Epoch: 6| Step: 10
Training loss: 3.2143747806549072
Validation loss: 2.3699398835500083

Epoch: 6| Step: 11
Training loss: 2.1566262245178223
Validation loss: 2.3677755991617837

Epoch: 6| Step: 12
Training loss: 2.8142499923706055
Validation loss: 2.3639461994171143

Epoch: 6| Step: 13
Training loss: 2.935502052307129
Validation loss: 2.3623817364374795

Epoch: 54| Step: 0
Training loss: 2.455305576324463
Validation loss: 2.3608792622884116

Epoch: 6| Step: 1
Training loss: 1.9755210876464844
Validation loss: 2.3581873377164206

Epoch: 6| Step: 2
Training loss: 2.641563892364502
Validation loss: 2.361357351144155

Epoch: 6| Step: 3
Training loss: 3.0479683876037598
Validation loss: 2.3596001863479614

Epoch: 6| Step: 4
Training loss: 2.7208409309387207
Validation loss: 2.357833464940389

Epoch: 6| Step: 5
Training loss: 2.6833248138427734
Validation loss: 2.3572656909624734

Epoch: 6| Step: 6
Training loss: 2.2822461128234863
Validation loss: 2.350554863611857

Epoch: 6| Step: 7
Training loss: 2.8397765159606934
Validation loss: 2.347019632657369

Epoch: 6| Step: 8
Training loss: 2.8642220497131348
Validation loss: 2.344650944073995

Epoch: 6| Step: 9
Training loss: 2.8327927589416504
Validation loss: 2.3497929175694785

Epoch: 6| Step: 10
Training loss: 2.336695909500122
Validation loss: 2.3471946716308594

Epoch: 6| Step: 11
Training loss: 2.3417038917541504
Validation loss: 2.3462283412615457

Epoch: 6| Step: 12
Training loss: 1.7097766399383545
Validation loss: 2.3434324860572815

Epoch: 6| Step: 13
Training loss: 2.6690902709960938
Validation loss: 2.3386669556299844

Epoch: 55| Step: 0
Training loss: 2.774766206741333
Validation loss: 2.3349217573801675

Epoch: 6| Step: 1
Training loss: 2.4395627975463867
Validation loss: 2.3328880071640015

Epoch: 6| Step: 2
Training loss: 2.2598114013671875
Validation loss: 2.3317352135976157

Epoch: 6| Step: 3
Training loss: 2.0878467559814453
Validation loss: 2.3265485167503357

Epoch: 6| Step: 4
Training loss: 2.7067818641662598
Validation loss: 2.3258963028589883

Epoch: 6| Step: 5
Training loss: 2.976010799407959
Validation loss: 2.3236815333366394

Epoch: 6| Step: 6
Training loss: 3.090672492980957
Validation loss: 2.3236417174339294

Epoch: 6| Step: 7
Training loss: 2.5792312622070312
Validation loss: 2.3178718288739524

Epoch: 6| Step: 8
Training loss: 2.3446695804595947
Validation loss: 2.324748158454895

Epoch: 6| Step: 9
Training loss: 2.8155624866485596
Validation loss: 2.3199482361475625

Epoch: 6| Step: 10
Training loss: 1.5313292741775513
Validation loss: 2.3198052247365317

Epoch: 6| Step: 11
Training loss: 1.8808256387710571
Validation loss: 2.318815271059672

Epoch: 6| Step: 12
Training loss: 2.5051333904266357
Validation loss: 2.3216240604718528

Epoch: 6| Step: 13
Training loss: 2.9427132606506348
Validation loss: 2.3129084507624307

Epoch: 56| Step: 0
Training loss: 2.8440356254577637
Validation loss: 2.3076983292897544

Epoch: 6| Step: 1
Training loss: 2.645700454711914
Validation loss: 2.30796217918396

Epoch: 6| Step: 2
Training loss: 2.5850296020507812
Validation loss: 2.30444606145223

Epoch: 6| Step: 3
Training loss: 2.3705430030822754
Validation loss: 2.3058398564656577

Epoch: 6| Step: 4
Training loss: 2.6234021186828613
Validation loss: 2.306410789489746

Epoch: 6| Step: 5
Training loss: 2.374084711074829
Validation loss: 2.306347211201986

Epoch: 6| Step: 6
Training loss: 2.1375491619110107
Validation loss: 2.3082292874654136

Epoch: 6| Step: 7
Training loss: 2.6687657833099365
Validation loss: 2.301631530125936

Epoch: 6| Step: 8
Training loss: 1.9563446044921875
Validation loss: 2.2970226804415383

Epoch: 6| Step: 9
Training loss: 1.8485028743743896
Validation loss: 2.2968558073043823

Epoch: 6| Step: 10
Training loss: 2.5714268684387207
Validation loss: 2.288401246070862

Epoch: 6| Step: 11
Training loss: 2.5958657264709473
Validation loss: 2.284610390663147

Epoch: 6| Step: 12
Training loss: 2.503382444381714
Validation loss: 2.2836944460868835

Epoch: 6| Step: 13
Training loss: 2.8710429668426514
Validation loss: 2.2848455905914307

Epoch: 57| Step: 0
Training loss: 2.104187488555908
Validation loss: 2.2865997950236

Epoch: 6| Step: 1
Training loss: 1.917142391204834
Validation loss: 2.2755942344665527

Epoch: 6| Step: 2
Training loss: 2.4887208938598633
Validation loss: 2.2750083605448403

Epoch: 6| Step: 3
Training loss: 2.409702777862549
Validation loss: 2.269858181476593

Epoch: 6| Step: 4
Training loss: 2.1476638317108154
Validation loss: 2.267972151438395

Epoch: 6| Step: 5
Training loss: 2.108140707015991
Validation loss: 2.262739837169647

Epoch: 6| Step: 6
Training loss: 2.910360813140869
Validation loss: 2.2681170304616294

Epoch: 6| Step: 7
Training loss: 3.048539638519287
Validation loss: 2.2634268204371133

Epoch: 6| Step: 8
Training loss: 2.3105275630950928
Validation loss: 2.2650941809018454

Epoch: 6| Step: 9
Training loss: 2.6153526306152344
Validation loss: 2.259314934412638

Epoch: 6| Step: 10
Training loss: 3.0344152450561523
Validation loss: 2.2590836683909097

Epoch: 6| Step: 11
Training loss: 1.7361273765563965
Validation loss: 2.262140432993571

Epoch: 6| Step: 12
Training loss: 2.5947582721710205
Validation loss: 2.2570214668909707

Epoch: 6| Step: 13
Training loss: 2.6341233253479004
Validation loss: 2.2518771489461265

Epoch: 58| Step: 0
Training loss: 2.647599458694458
Validation loss: 2.253534972667694

Epoch: 6| Step: 1
Training loss: 1.9826483726501465
Validation loss: 2.2536163528760276

Epoch: 6| Step: 2
Training loss: 2.3119349479675293
Validation loss: 2.2562657594680786

Epoch: 6| Step: 3
Training loss: 2.158987045288086
Validation loss: 2.2544753154118857

Epoch: 6| Step: 4
Training loss: 2.825974464416504
Validation loss: 2.2514655192693076

Epoch: 6| Step: 5
Training loss: 3.0269572734832764
Validation loss: 2.2417144179344177

Epoch: 6| Step: 6
Training loss: 2.2560808658599854
Validation loss: 2.243789792060852

Epoch: 6| Step: 7
Training loss: 2.5783469676971436
Validation loss: 2.2458104689915976

Epoch: 6| Step: 8
Training loss: 2.0528125762939453
Validation loss: 2.255503992239634

Epoch: 6| Step: 9
Training loss: 2.8030450344085693
Validation loss: 2.26547904809316

Epoch: 6| Step: 10
Training loss: 2.858196258544922
Validation loss: 2.267616013685862

Epoch: 6| Step: 11
Training loss: 2.199214458465576
Validation loss: 2.263458013534546

Epoch: 6| Step: 12
Training loss: 1.615366816520691
Validation loss: 2.260895570119222

Epoch: 6| Step: 13
Training loss: 2.629345417022705
Validation loss: 2.246987203756968

Epoch: 59| Step: 0
Training loss: 2.5364699363708496
Validation loss: 2.238349119822184

Epoch: 6| Step: 1
Training loss: 2.7878358364105225
Validation loss: 2.2347307006518045

Epoch: 6| Step: 2
Training loss: 2.411672592163086
Validation loss: 2.226655880610148

Epoch: 6| Step: 3
Training loss: 2.2185001373291016
Validation loss: 2.2280304431915283

Epoch: 6| Step: 4
Training loss: 2.115067481994629
Validation loss: 2.2244920134544373

Epoch: 6| Step: 5
Training loss: 2.753143072128296
Validation loss: 2.2279717326164246

Epoch: 6| Step: 6
Training loss: 2.0428085327148438
Validation loss: 2.223286588986715

Epoch: 6| Step: 7
Training loss: 2.0582103729248047
Validation loss: 2.2192537983258567

Epoch: 6| Step: 8
Training loss: 2.7958199977874756
Validation loss: 2.218794345855713

Epoch: 6| Step: 9
Training loss: 2.4987740516662598
Validation loss: 2.216992278893789

Epoch: 6| Step: 10
Training loss: 2.3313956260681152
Validation loss: 2.22109325726827

Epoch: 6| Step: 11
Training loss: 2.4951624870300293
Validation loss: 2.214193860689799

Epoch: 6| Step: 12
Training loss: 2.3556342124938965
Validation loss: 2.219112674395243

Epoch: 6| Step: 13
Training loss: 1.9652347564697266
Validation loss: 2.2196106910705566

Epoch: 60| Step: 0
Training loss: 2.376223087310791
Validation loss: 2.219406465689341

Epoch: 6| Step: 1
Training loss: 1.9524526596069336
Validation loss: 2.208125034968058

Epoch: 6| Step: 2
Training loss: 2.488471508026123
Validation loss: 2.2059778968493142

Epoch: 6| Step: 3
Training loss: 2.5352942943573
Validation loss: 2.206426819165548

Epoch: 6| Step: 4
Training loss: 1.7792737483978271
Validation loss: 2.2052634954452515

Epoch: 6| Step: 5
Training loss: 2.007099151611328
Validation loss: 2.203391174475352

Epoch: 6| Step: 6
Training loss: 2.091574192047119
Validation loss: 2.208512266476949

Epoch: 6| Step: 7
Training loss: 2.212188720703125
Validation loss: 2.2108457883199057

Epoch: 6| Step: 8
Training loss: 3.186283588409424
Validation loss: 2.218448301156362

Epoch: 6| Step: 9
Training loss: 2.7609667778015137
Validation loss: 2.217643996079763

Epoch: 6| Step: 10
Training loss: 2.123777389526367
Validation loss: 2.217512309551239

Epoch: 6| Step: 11
Training loss: 2.492968797683716
Validation loss: 2.2274542252222695

Epoch: 6| Step: 12
Training loss: 2.926460027694702
Validation loss: 2.2253411213556924

Epoch: 6| Step: 13
Training loss: 2.272660732269287
Validation loss: 2.224577307701111

Epoch: 61| Step: 0
Training loss: 2.292146682739258
Validation loss: 2.2133182088534036

Epoch: 6| Step: 1
Training loss: 2.5101466178894043
Validation loss: 2.20774507522583

Epoch: 6| Step: 2
Training loss: 2.313215970993042
Validation loss: 2.1967754562695823

Epoch: 6| Step: 3
Training loss: 2.04746413230896
Validation loss: 2.1976665457089744

Epoch: 6| Step: 4
Training loss: 2.6234805583953857
Validation loss: 2.190370261669159

Epoch: 6| Step: 5
Training loss: 2.5453925132751465
Validation loss: 2.188300887743632

Epoch: 6| Step: 6
Training loss: 1.9826682806015015
Validation loss: 2.184483369191488

Epoch: 6| Step: 7
Training loss: 2.7220139503479004
Validation loss: 2.188498040040334

Epoch: 6| Step: 8
Training loss: 2.3065781593322754
Validation loss: 2.1856193939844766

Epoch: 6| Step: 9
Training loss: 2.6351051330566406
Validation loss: 2.1821930408477783

Epoch: 6| Step: 10
Training loss: 2.046359062194824
Validation loss: 2.1844971776008606

Epoch: 6| Step: 11
Training loss: 1.8708394765853882
Validation loss: 2.1779363552729287

Epoch: 6| Step: 12
Training loss: 2.510442018508911
Validation loss: 2.1839577158292136

Epoch: 6| Step: 13
Training loss: 2.41813588142395
Validation loss: 2.180323322614034

Epoch: 62| Step: 0
Training loss: 1.7495899200439453
Validation loss: 2.189194142818451

Epoch: 6| Step: 1
Training loss: 3.0058422088623047
Validation loss: 2.189899186293284

Epoch: 6| Step: 2
Training loss: 2.047168254852295
Validation loss: 2.190932353337606

Epoch: 6| Step: 3
Training loss: 2.4838528633117676
Validation loss: 2.1795583764712014

Epoch: 6| Step: 4
Training loss: 2.712658405303955
Validation loss: 2.175641496976217

Epoch: 6| Step: 5
Training loss: 2.4326539039611816
Validation loss: 2.1730807622273765

Epoch: 6| Step: 6
Training loss: 2.1317026615142822
Validation loss: 2.1702585419019065

Epoch: 6| Step: 7
Training loss: 2.3286824226379395
Validation loss: 2.169604738553365

Epoch: 6| Step: 8
Training loss: 2.656036376953125
Validation loss: 2.1665360728899636

Epoch: 6| Step: 9
Training loss: 2.4782700538635254
Validation loss: 2.160797337690989

Epoch: 6| Step: 10
Training loss: 2.2443721294403076
Validation loss: 2.165815850098928

Epoch: 6| Step: 11
Training loss: 1.8143727779388428
Validation loss: 2.1651878555615744

Epoch: 6| Step: 12
Training loss: 2.468397855758667
Validation loss: 2.1687118212381997

Epoch: 6| Step: 13
Training loss: 2.107536554336548
Validation loss: 2.167933702468872

Epoch: 63| Step: 0
Training loss: 2.161334991455078
Validation loss: 2.1679348150889077

Epoch: 6| Step: 1
Training loss: 2.504453659057617
Validation loss: 2.174491544564565

Epoch: 6| Step: 2
Training loss: 2.710927963256836
Validation loss: 2.1753379503885903

Epoch: 6| Step: 3
Training loss: 2.636240005493164
Validation loss: 2.1650243202845254

Epoch: 6| Step: 4
Training loss: 2.26400089263916
Validation loss: 2.1636454860369363

Epoch: 6| Step: 5
Training loss: 2.705143690109253
Validation loss: 2.160993754863739

Epoch: 6| Step: 6
Training loss: 2.5553109645843506
Validation loss: 2.156354228655497

Epoch: 6| Step: 7
Training loss: 1.705459713935852
Validation loss: 2.155687769254049

Epoch: 6| Step: 8
Training loss: 2.0179643630981445
Validation loss: 2.1533341805140176

Epoch: 6| Step: 9
Training loss: 2.2845141887664795
Validation loss: 2.1536089976628623

Epoch: 6| Step: 10
Training loss: 1.863060474395752
Validation loss: 2.150226573149363

Epoch: 6| Step: 11
Training loss: 2.7139158248901367
Validation loss: 2.1464540362358093

Epoch: 6| Step: 12
Training loss: 2.0707130432128906
Validation loss: 2.1443766156832376

Epoch: 6| Step: 13
Training loss: 2.3628580570220947
Validation loss: 2.143485188484192

Epoch: 64| Step: 0
Training loss: 2.1380057334899902
Validation loss: 2.14380019903183

Epoch: 6| Step: 1
Training loss: 1.980291724205017
Validation loss: 2.1465822060902915

Epoch: 6| Step: 2
Training loss: 1.5378344058990479
Validation loss: 2.1411786874135337

Epoch: 6| Step: 3
Training loss: 2.423435926437378
Validation loss: 2.141076922416687

Epoch: 6| Step: 4
Training loss: 2.607056140899658
Validation loss: 2.140452563762665

Epoch: 6| Step: 5
Training loss: 1.9839410781860352
Validation loss: 2.1402632196744285

Epoch: 6| Step: 6
Training loss: 2.514371871948242
Validation loss: 2.143149177233378

Epoch: 6| Step: 7
Training loss: 2.084108352661133
Validation loss: 2.1451067527135215

Epoch: 6| Step: 8
Training loss: 2.4112892150878906
Validation loss: 2.144373675187429

Epoch: 6| Step: 9
Training loss: 2.4239745140075684
Validation loss: 2.140138884385427

Epoch: 6| Step: 10
Training loss: 2.2923240661621094
Validation loss: 2.1416412591934204

Epoch: 6| Step: 11
Training loss: 2.1312661170959473
Validation loss: 2.1332831382751465

Epoch: 6| Step: 12
Training loss: 3.3031935691833496
Validation loss: 2.136777400970459

Epoch: 6| Step: 13
Training loss: 2.4346492290496826
Validation loss: 2.131858766078949

Epoch: 65| Step: 0
Training loss: 2.6216418743133545
Validation loss: 2.1368215084075928

Epoch: 6| Step: 1
Training loss: 1.9842872619628906
Validation loss: 2.133293867111206

Epoch: 6| Step: 2
Training loss: 2.4395296573638916
Validation loss: 2.1369271675745645

Epoch: 6| Step: 3
Training loss: 2.592489242553711
Validation loss: 2.1370221177736917

Epoch: 6| Step: 4
Training loss: 2.089118480682373
Validation loss: 2.136987785498301

Epoch: 6| Step: 5
Training loss: 2.269029140472412
Validation loss: 2.133308748404185

Epoch: 6| Step: 6
Training loss: 1.9282740354537964
Validation loss: 2.1307201385498047

Epoch: 6| Step: 7
Training loss: 2.3317790031433105
Validation loss: 2.129652718702952

Epoch: 6| Step: 8
Training loss: 2.189483165740967
Validation loss: 2.1306985219319663

Epoch: 6| Step: 9
Training loss: 2.4433388710021973
Validation loss: 2.129756450653076

Epoch: 6| Step: 10
Training loss: 2.2083983421325684
Validation loss: 2.124917487303416

Epoch: 6| Step: 11
Training loss: 1.9757928848266602
Validation loss: 2.126222252845764

Epoch: 6| Step: 12
Training loss: 2.399712324142456
Validation loss: 2.1238245964050293

Epoch: 6| Step: 13
Training loss: 2.6063337326049805
Validation loss: 2.1219951113065085

Epoch: 66| Step: 0
Training loss: 2.2700624465942383
Validation loss: 2.120861907800039

Epoch: 6| Step: 1
Training loss: 2.2258124351501465
Validation loss: 2.1151112715403237

Epoch: 6| Step: 2
Training loss: 2.344013214111328
Validation loss: 2.1254563530286155

Epoch: 6| Step: 3
Training loss: 2.832199811935425
Validation loss: 2.119643191496531

Epoch: 6| Step: 4
Training loss: 2.413820743560791
Validation loss: 2.1248207688331604

Epoch: 6| Step: 5
Training loss: 1.4828546047210693
Validation loss: 2.120758612950643

Epoch: 6| Step: 6
Training loss: 2.376816511154175
Validation loss: 2.1267409523328147

Epoch: 6| Step: 7
Training loss: 2.7620410919189453
Validation loss: 2.129118263721466

Epoch: 6| Step: 8
Training loss: 2.5251224040985107
Validation loss: 2.13166477282842

Epoch: 6| Step: 9
Training loss: 2.580939769744873
Validation loss: 2.1272853016853333

Epoch: 6| Step: 10
Training loss: 2.0991718769073486
Validation loss: 2.1211939652760825

Epoch: 6| Step: 11
Training loss: 2.0210041999816895
Validation loss: 2.1044050653775535

Epoch: 6| Step: 12
Training loss: 1.5367767810821533
Validation loss: 2.115076998869578

Epoch: 6| Step: 13
Training loss: 2.5523011684417725
Validation loss: 2.1189783414204917

Epoch: 67| Step: 0
Training loss: 2.154452323913574
Validation loss: 2.1267330845197043

Epoch: 6| Step: 1
Training loss: 1.8219364881515503
Validation loss: 2.130916873613993

Epoch: 6| Step: 2
Training loss: 1.9392480850219727
Validation loss: 2.1375557581583657

Epoch: 6| Step: 3
Training loss: 2.056428909301758
Validation loss: 2.154268185297648

Epoch: 6| Step: 4
Training loss: 2.0996532440185547
Validation loss: 2.1650464137395224

Epoch: 6| Step: 5
Training loss: 2.454775333404541
Validation loss: 2.1734211246172586

Epoch: 6| Step: 6
Training loss: 3.0371580123901367
Validation loss: 2.166483461856842

Epoch: 6| Step: 7
Training loss: 2.627122402191162
Validation loss: 2.1483342051506042

Epoch: 6| Step: 8
Training loss: 2.1438710689544678
Validation loss: 2.144048829873403

Epoch: 6| Step: 9
Training loss: 1.9388971328735352
Validation loss: 2.13330731789271

Epoch: 6| Step: 10
Training loss: 2.5794873237609863
Validation loss: 2.133098224798838

Epoch: 6| Step: 11
Training loss: 2.4182047843933105
Validation loss: 2.1238193909327188

Epoch: 6| Step: 12
Training loss: 2.805497169494629
Validation loss: 2.1154625018437705

Epoch: 6| Step: 13
Training loss: 2.2154672145843506
Validation loss: 2.1112600564956665

Epoch: 68| Step: 0
Training loss: 2.6064670085906982
Validation loss: 2.1118722558021545

Epoch: 6| Step: 1
Training loss: 2.6244752407073975
Validation loss: 2.109631677468618

Epoch: 6| Step: 2
Training loss: 2.4343552589416504
Validation loss: 2.10907252629598

Epoch: 6| Step: 3
Training loss: 2.1689062118530273
Validation loss: 2.103866696357727

Epoch: 6| Step: 4
Training loss: 2.4080920219421387
Validation loss: 2.1018226146698

Epoch: 6| Step: 5
Training loss: 2.0337274074554443
Validation loss: 2.100116749604543

Epoch: 6| Step: 6
Training loss: 2.171238660812378
Validation loss: 2.1016690333684287

Epoch: 6| Step: 7
Training loss: 2.1520800590515137
Validation loss: 2.0960704485575357

Epoch: 6| Step: 8
Training loss: 2.1690688133239746
Validation loss: 2.094594419002533

Epoch: 6| Step: 9
Training loss: 2.1774306297302246
Validation loss: 2.0941036144892373

Epoch: 6| Step: 10
Training loss: 2.3670494556427
Validation loss: 2.094332456588745

Epoch: 6| Step: 11
Training loss: 2.0972156524658203
Validation loss: 2.099697927633921

Epoch: 6| Step: 12
Training loss: 1.6379443407058716
Validation loss: 2.0954706271489463

Epoch: 6| Step: 13
Training loss: 2.6396384239196777
Validation loss: 2.0925080378850303

Epoch: 69| Step: 0
Training loss: 2.5610709190368652
Validation loss: 2.094581365585327

Epoch: 6| Step: 1
Training loss: 1.6819628477096558
Validation loss: 2.0951155622800193

Epoch: 6| Step: 2
Training loss: 2.5497403144836426
Validation loss: 2.0964024861653647

Epoch: 6| Step: 3
Training loss: 2.4616570472717285
Validation loss: 2.0896189212799072

Epoch: 6| Step: 4
Training loss: 2.203779458999634
Validation loss: 2.0900596976280212

Epoch: 6| Step: 5
Training loss: 2.0046050548553467
Validation loss: 2.086913287639618

Epoch: 6| Step: 6
Training loss: 2.1594889163970947
Validation loss: 2.0867281953493753

Epoch: 6| Step: 7
Training loss: 1.8883308172225952
Validation loss: 2.0815152128537497

Epoch: 6| Step: 8
Training loss: 3.0054244995117188
Validation loss: 2.0889543294906616

Epoch: 6| Step: 9
Training loss: 2.74204158782959
Validation loss: 2.0898001194000244

Epoch: 6| Step: 10
Training loss: 2.0938048362731934
Validation loss: 2.0851734479268393

Epoch: 6| Step: 11
Training loss: 2.219949722290039
Validation loss: 2.0895199378331504

Epoch: 6| Step: 12
Training loss: 1.769547700881958
Validation loss: 2.0847081740697226

Epoch: 6| Step: 13
Training loss: 2.320495843887329
Validation loss: 2.0850584308306375

Epoch: 70| Step: 0
Training loss: 2.0567216873168945
Validation loss: 2.085742175579071

Epoch: 6| Step: 1
Training loss: 2.3012728691101074
Validation loss: 2.0842715303103128

Epoch: 6| Step: 2
Training loss: 1.9602243900299072
Validation loss: 2.0891023874282837

Epoch: 6| Step: 3
Training loss: 2.3926761150360107
Validation loss: 2.083832105000814

Epoch: 6| Step: 4
Training loss: 2.8832192420959473
Validation loss: 2.079170366128286

Epoch: 6| Step: 5
Training loss: 2.5289382934570312
Validation loss: 2.0770458380381265

Epoch: 6| Step: 6
Training loss: 2.3093748092651367
Validation loss: 2.0778424541155496

Epoch: 6| Step: 7
Training loss: 2.271254777908325
Validation loss: 2.075587491194407

Epoch: 6| Step: 8
Training loss: 2.7373359203338623
Validation loss: 2.076670289039612

Epoch: 6| Step: 9
Training loss: 2.115935802459717
Validation loss: 2.0707679390907288

Epoch: 6| Step: 10
Training loss: 1.4639766216278076
Validation loss: 2.0750552813212075

Epoch: 6| Step: 11
Training loss: 2.437065362930298
Validation loss: 2.0703455408414206

Epoch: 6| Step: 12
Training loss: 2.085442066192627
Validation loss: 2.07801882425944

Epoch: 6| Step: 13
Training loss: 2.090569019317627
Validation loss: 2.0789292057355246

Epoch: 71| Step: 0
Training loss: 1.8706623315811157
Validation loss: 2.0756574273109436

Epoch: 6| Step: 1
Training loss: 2.2955570220947266
Validation loss: 2.0698565244674683

Epoch: 6| Step: 2
Training loss: 2.489171028137207
Validation loss: 2.0715055068333945

Epoch: 6| Step: 3
Training loss: 2.196990489959717
Validation loss: 2.068423608938853

Epoch: 6| Step: 4
Training loss: 2.3893721103668213
Validation loss: 2.0657971501350403

Epoch: 6| Step: 5
Training loss: 2.1524274349212646
Validation loss: 2.0619676311810813

Epoch: 6| Step: 6
Training loss: 2.018799304962158
Validation loss: 2.0662730733553567

Epoch: 6| Step: 7
Training loss: 2.5745277404785156
Validation loss: 2.061766048272451

Epoch: 6| Step: 8
Training loss: 2.346296787261963
Validation loss: 2.0677308241526284

Epoch: 6| Step: 9
Training loss: 1.7609577178955078
Validation loss: 2.063121577103933

Epoch: 6| Step: 10
Training loss: 2.462604522705078
Validation loss: 2.0606022675832114

Epoch: 6| Step: 11
Training loss: 2.1632747650146484
Validation loss: 2.0592006047566733

Epoch: 6| Step: 12
Training loss: 2.0828466415405273
Validation loss: 2.064456820487976

Epoch: 6| Step: 13
Training loss: 2.686796188354492
Validation loss: 2.0584480365117392

Epoch: 72| Step: 0
Training loss: 2.188107490539551
Validation loss: 2.0630702575047812

Epoch: 6| Step: 1
Training loss: 1.9756827354431152
Validation loss: 2.053575317064921

Epoch: 6| Step: 2
Training loss: 2.347702980041504
Validation loss: 2.058633863925934

Epoch: 6| Step: 3
Training loss: 3.047502040863037
Validation loss: 2.058156192302704

Epoch: 6| Step: 4
Training loss: 2.506347179412842
Validation loss: 2.0581222573916116

Epoch: 6| Step: 5
Training loss: 1.6708852052688599
Validation loss: 2.059010366598765

Epoch: 6| Step: 6
Training loss: 2.5981950759887695
Validation loss: 2.055045247077942

Epoch: 6| Step: 7
Training loss: 1.9763567447662354
Validation loss: 2.056057790915171

Epoch: 6| Step: 8
Training loss: 1.7761327028274536
Validation loss: 2.0524624784787497

Epoch: 6| Step: 9
Training loss: 1.578979730606079
Validation loss: 2.058612863222758

Epoch: 6| Step: 10
Training loss: 2.678079605102539
Validation loss: 2.0528721610705056

Epoch: 6| Step: 11
Training loss: 2.551980495452881
Validation loss: 2.0625738302866616

Epoch: 6| Step: 12
Training loss: 1.8158423900604248
Validation loss: 2.0532432993253074

Epoch: 6| Step: 13
Training loss: 2.614427089691162
Validation loss: 2.050092657407125

Epoch: 73| Step: 0
Training loss: 2.146528720855713
Validation loss: 2.0483962297439575

Epoch: 6| Step: 1
Training loss: 2.098045825958252
Validation loss: 2.051949123541514

Epoch: 6| Step: 2
Training loss: 1.8719570636749268
Validation loss: 2.0528930028279624

Epoch: 6| Step: 3
Training loss: 2.397700786590576
Validation loss: 2.0513803362846375

Epoch: 6| Step: 4
Training loss: 2.172092914581299
Validation loss: 2.05923600991567

Epoch: 6| Step: 5
Training loss: 2.1719794273376465
Validation loss: 2.0533565481503806

Epoch: 6| Step: 6
Training loss: 2.156794786453247
Validation loss: 2.0540873805681863

Epoch: 6| Step: 7
Training loss: 2.1815319061279297
Validation loss: 2.057496805985769

Epoch: 6| Step: 8
Training loss: 3.3357748985290527
Validation loss: 2.0587517420450845

Epoch: 6| Step: 9
Training loss: 1.9631084203720093
Validation loss: 2.0559495091438293

Epoch: 6| Step: 10
Training loss: 2.35451078414917
Validation loss: 2.0549685756365457

Epoch: 6| Step: 11
Training loss: 2.1475653648376465
Validation loss: 2.0470330119132996

Epoch: 6| Step: 12
Training loss: 2.301988124847412
Validation loss: 2.0495757857958474

Epoch: 6| Step: 13
Training loss: 2.2205159664154053
Validation loss: 2.054733157157898

Epoch: 74| Step: 0
Training loss: 2.6505045890808105
Validation loss: 2.0619099140167236

Epoch: 6| Step: 1
Training loss: 2.415731906890869
Validation loss: 2.063892662525177

Epoch: 6| Step: 2
Training loss: 2.4691357612609863
Validation loss: 2.0646769801775613

Epoch: 6| Step: 3
Training loss: 2.4126710891723633
Validation loss: 2.0583314299583435

Epoch: 6| Step: 4
Training loss: 2.57077693939209
Validation loss: 2.0596839586893716

Epoch: 6| Step: 5
Training loss: 1.8620879650115967
Validation loss: 2.0636669198671975

Epoch: 6| Step: 6
Training loss: 2.5285205841064453
Validation loss: 2.058874766031901

Epoch: 6| Step: 7
Training loss: 2.4226107597351074
Validation loss: 2.052924076716105

Epoch: 6| Step: 8
Training loss: 2.4579086303710938
Validation loss: 2.0482856233914695

Epoch: 6| Step: 9
Training loss: 1.83742356300354
Validation loss: 2.047532379627228

Epoch: 6| Step: 10
Training loss: 1.4563438892364502
Validation loss: 2.0400198698043823

Epoch: 6| Step: 11
Training loss: 1.8964110612869263
Validation loss: 2.044502913951874

Epoch: 6| Step: 12
Training loss: 2.612997055053711
Validation loss: 2.046136498451233

Epoch: 6| Step: 13
Training loss: 1.8872663974761963
Validation loss: 2.044119397799174

Epoch: 75| Step: 0
Training loss: 2.0384228229522705
Validation loss: 2.048082093397776

Epoch: 6| Step: 1
Training loss: 2.1993186473846436
Validation loss: 2.04406609137853

Epoch: 6| Step: 2
Training loss: 1.9951894283294678
Validation loss: 2.0389036536216736

Epoch: 6| Step: 3
Training loss: 2.145630359649658
Validation loss: 2.038163145383199

Epoch: 6| Step: 4
Training loss: 2.4595654010772705
Validation loss: 2.0411458015441895

Epoch: 6| Step: 5
Training loss: 2.33943247795105
Validation loss: 2.042858819166819

Epoch: 6| Step: 6
Training loss: 1.9689090251922607
Validation loss: 2.044883211453756

Epoch: 6| Step: 7
Training loss: 2.7088332176208496
Validation loss: 2.0460147857666016

Epoch: 6| Step: 8
Training loss: 2.0457894802093506
Validation loss: 2.047301232814789

Epoch: 6| Step: 9
Training loss: 2.4623966217041016
Validation loss: 2.042245864868164

Epoch: 6| Step: 10
Training loss: 2.0059051513671875
Validation loss: 2.0398675998051963

Epoch: 6| Step: 11
Training loss: 2.464172124862671
Validation loss: 2.04926065603892

Epoch: 6| Step: 12
Training loss: 2.4317989349365234
Validation loss: 2.044232169787089

Epoch: 6| Step: 13
Training loss: 1.9580005407333374
Validation loss: 2.0471865137418113

Epoch: 76| Step: 0
Training loss: 2.462897539138794
Validation loss: 2.0489476124445596

Epoch: 6| Step: 1
Training loss: 2.2516098022460938
Validation loss: 2.0409223635991416

Epoch: 6| Step: 2
Training loss: 2.853335380554199
Validation loss: 2.04749204715093

Epoch: 6| Step: 3
Training loss: 2.1247143745422363
Validation loss: 2.0364157954851785

Epoch: 6| Step: 4
Training loss: 2.2494540214538574
Validation loss: 2.035254160563151

Epoch: 6| Step: 5
Training loss: 2.6173574924468994
Validation loss: 2.0354698300361633

Epoch: 6| Step: 6
Training loss: 2.8533835411071777
Validation loss: 2.0425259470939636

Epoch: 6| Step: 7
Training loss: 1.7394371032714844
Validation loss: 2.0447335839271545

Epoch: 6| Step: 8
Training loss: 1.8161120414733887
Validation loss: 2.0409717758496604

Epoch: 6| Step: 9
Training loss: 1.8914613723754883
Validation loss: 2.041368762652079

Epoch: 6| Step: 10
Training loss: 1.979270100593567
Validation loss: 2.041854818662008

Epoch: 6| Step: 11
Training loss: 2.3005709648132324
Validation loss: 2.0357757608095803

Epoch: 6| Step: 12
Training loss: 2.1311535835266113
Validation loss: 2.0331543485323587

Epoch: 6| Step: 13
Training loss: 1.8045486211776733
Validation loss: 2.0306337674458823

Epoch: 77| Step: 0
Training loss: 2.4843392372131348
Validation loss: 2.038452665011088

Epoch: 6| Step: 1
Training loss: 1.4971725940704346
Validation loss: 2.0384324391682944

Epoch: 6| Step: 2
Training loss: 2.2647948265075684
Validation loss: 2.02934596935908

Epoch: 6| Step: 3
Training loss: 2.4990501403808594
Validation loss: 2.0344796180725098

Epoch: 6| Step: 4
Training loss: 1.8694795370101929
Validation loss: 2.030811329682668

Epoch: 6| Step: 5
Training loss: 1.8631805181503296
Validation loss: 2.030634800593058

Epoch: 6| Step: 6
Training loss: 2.40128231048584
Validation loss: 2.036847392717997

Epoch: 6| Step: 7
Training loss: 2.0868587493896484
Validation loss: 2.0312289198239646

Epoch: 6| Step: 8
Training loss: 2.2477595806121826
Validation loss: 2.031750480333964

Epoch: 6| Step: 9
Training loss: 2.757230520248413
Validation loss: 2.0347460309664407

Epoch: 6| Step: 10
Training loss: 1.7587178945541382
Validation loss: 2.0369494557380676

Epoch: 6| Step: 11
Training loss: 2.6876468658447266
Validation loss: 2.0331856409708657

Epoch: 6| Step: 12
Training loss: 2.4991207122802734
Validation loss: 2.030904312928518

Epoch: 6| Step: 13
Training loss: 2.088590621948242
Validation loss: 2.0274980664253235

Epoch: 78| Step: 0
Training loss: 2.104743242263794
Validation loss: 2.033064047495524

Epoch: 6| Step: 1
Training loss: 2.560765027999878
Validation loss: 2.0273613929748535

Epoch: 6| Step: 2
Training loss: 1.7790323495864868
Validation loss: 2.0267736514409385

Epoch: 6| Step: 3
Training loss: 2.884922504425049
Validation loss: 2.0270248850186667

Epoch: 6| Step: 4
Training loss: 1.9070895910263062
Validation loss: 2.0268431107203164

Epoch: 6| Step: 5
Training loss: 2.3838109970092773
Validation loss: 2.027474840482076

Epoch: 6| Step: 6
Training loss: 1.586808204650879
Validation loss: 2.028467277685801

Epoch: 6| Step: 7
Training loss: 2.1206018924713135
Validation loss: 2.0254538456598916

Epoch: 6| Step: 8
Training loss: 2.589874744415283
Validation loss: 2.030837297439575

Epoch: 6| Step: 9
Training loss: 2.169114828109741
Validation loss: 2.0315205454826355

Epoch: 6| Step: 10
Training loss: 2.6470108032226562
Validation loss: 2.0270070234934487

Epoch: 6| Step: 11
Training loss: 2.0659828186035156
Validation loss: 2.02800714969635

Epoch: 6| Step: 12
Training loss: 1.9199343919754028
Validation loss: 2.039577861626943

Epoch: 6| Step: 13
Training loss: 2.3805017471313477
Validation loss: 2.0437236229578652

Epoch: 79| Step: 0
Training loss: 2.4894204139709473
Validation loss: 2.045886238416036

Epoch: 6| Step: 1
Training loss: 2.75527286529541
Validation loss: 2.0486340324083963

Epoch: 6| Step: 2
Training loss: 1.8837993144989014
Validation loss: 2.0434772769610086

Epoch: 6| Step: 3
Training loss: 1.6187280416488647
Validation loss: 2.0407276153564453

Epoch: 6| Step: 4
Training loss: 2.5354549884796143
Validation loss: 2.0349170764287314

Epoch: 6| Step: 5
Training loss: 2.44024395942688
Validation loss: 2.0318755308787027

Epoch: 6| Step: 6
Training loss: 2.2384233474731445
Validation loss: 2.031730850537618

Epoch: 6| Step: 7
Training loss: 2.8041319847106934
Validation loss: 2.0308216214179993

Epoch: 6| Step: 8
Training loss: 2.420196056365967
Validation loss: 2.02779754002889

Epoch: 6| Step: 9
Training loss: 1.431586503982544
Validation loss: 2.0303722620010376

Epoch: 6| Step: 10
Training loss: 2.4646964073181152
Validation loss: 2.0275455911954245

Epoch: 6| Step: 11
Training loss: 2.3635034561157227
Validation loss: 2.027947465578715

Epoch: 6| Step: 12
Training loss: 1.9094088077545166
Validation loss: 2.0323656598726907

Epoch: 6| Step: 13
Training loss: 1.6275546550750732
Validation loss: 2.038496494293213

Epoch: 80| Step: 0
Training loss: 2.7198522090911865
Validation loss: 2.045380393664042

Epoch: 6| Step: 1
Training loss: 2.462946891784668
Validation loss: 2.0484251379966736

Epoch: 6| Step: 2
Training loss: 1.785531997680664
Validation loss: 2.050743897755941

Epoch: 6| Step: 3
Training loss: 2.0450947284698486
Validation loss: 2.060177723566691

Epoch: 6| Step: 4
Training loss: 2.3258566856384277
Validation loss: 2.048604428768158

Epoch: 6| Step: 5
Training loss: 1.6827515363693237
Validation loss: 2.043450395266215

Epoch: 6| Step: 6
Training loss: 1.7047839164733887
Validation loss: 2.0419973134994507

Epoch: 6| Step: 7
Training loss: 2.7856478691101074
Validation loss: 2.041127065817515

Epoch: 6| Step: 8
Training loss: 1.4920203685760498
Validation loss: 2.0446170965830484

Epoch: 6| Step: 9
Training loss: 2.1925885677337646
Validation loss: 2.0621374050776162

Epoch: 6| Step: 10
Training loss: 2.1799733638763428
Validation loss: 2.0637542406717935

Epoch: 6| Step: 11
Training loss: 2.5104267597198486
Validation loss: 2.065189997355143

Epoch: 6| Step: 12
Training loss: 1.8655973672866821
Validation loss: 2.056738336881002

Epoch: 6| Step: 13
Training loss: 3.1109890937805176
Validation loss: 2.0632731517155967

Epoch: 81| Step: 0
Training loss: 2.1765875816345215
Validation loss: 2.0606188972791037

Epoch: 6| Step: 1
Training loss: 2.2207136154174805
Validation loss: 2.0798149704933167

Epoch: 6| Step: 2
Training loss: 2.8461358547210693
Validation loss: 2.069170296192169

Epoch: 6| Step: 3
Training loss: 2.5128190517425537
Validation loss: 2.067920168240865

Epoch: 6| Step: 4
Training loss: 2.0403735637664795
Validation loss: 2.055597503980001

Epoch: 6| Step: 5
Training loss: 1.7631232738494873
Validation loss: 2.0619078477223716

Epoch: 6| Step: 6
Training loss: 2.1274499893188477
Validation loss: 2.058732887109121

Epoch: 6| Step: 7
Training loss: 2.444945812225342
Validation loss: 2.063400149345398

Epoch: 6| Step: 8
Training loss: 1.9397776126861572
Validation loss: 2.0521962642669678

Epoch: 6| Step: 9
Training loss: 2.212003707885742
Validation loss: 2.051770548025767

Epoch: 6| Step: 10
Training loss: 2.7203524112701416
Validation loss: 2.045874019463857

Epoch: 6| Step: 11
Training loss: 1.6560845375061035
Validation loss: 2.0479488571484885

Epoch: 6| Step: 12
Training loss: 2.042965888977051
Validation loss: 2.043099661668142

Epoch: 6| Step: 13
Training loss: 2.3575873374938965
Validation loss: 2.036973317464193

Epoch: 82| Step: 0
Training loss: 2.1665897369384766
Validation loss: 2.0419776837031045

Epoch: 6| Step: 1
Training loss: 2.280921459197998
Validation loss: 2.047142227490743

Epoch: 6| Step: 2
Training loss: 1.9593558311462402
Validation loss: 2.0490862131118774

Epoch: 6| Step: 3
Training loss: 1.895527958869934
Validation loss: 2.054803431034088

Epoch: 6| Step: 4
Training loss: 2.3719377517700195
Validation loss: 2.0565653244654336

Epoch: 6| Step: 5
Training loss: 2.1258299350738525
Validation loss: 2.045119345188141

Epoch: 6| Step: 6
Training loss: 2.7862260341644287
Validation loss: 2.056170165538788

Epoch: 6| Step: 7
Training loss: 2.0085318088531494
Validation loss: 2.053270181020101

Epoch: 6| Step: 8
Training loss: 2.731839179992676
Validation loss: 2.049711585044861

Epoch: 6| Step: 9
Training loss: 2.231879711151123
Validation loss: 2.0473143061002097

Epoch: 6| Step: 10
Training loss: 2.018576145172119
Validation loss: 2.0383867422739663

Epoch: 6| Step: 11
Training loss: 2.1140475273132324
Validation loss: 2.028124133745829

Epoch: 6| Step: 12
Training loss: 1.834907054901123
Validation loss: 2.037689487139384

Epoch: 6| Step: 13
Training loss: 2.28281307220459
Validation loss: 2.023833394050598

Epoch: 83| Step: 0
Training loss: 1.9120783805847168
Validation loss: 2.0227124293645224

Epoch: 6| Step: 1
Training loss: 1.8812344074249268
Validation loss: 2.0208503802617392

Epoch: 6| Step: 2
Training loss: 2.1608519554138184
Validation loss: 2.0235350131988525

Epoch: 6| Step: 3
Training loss: 1.690796136856079
Validation loss: 2.023273289203644

Epoch: 6| Step: 4
Training loss: 2.391641855239868
Validation loss: 2.0239534974098206

Epoch: 6| Step: 5
Training loss: 2.1718196868896484
Validation loss: 2.0244184931119285

Epoch: 6| Step: 6
Training loss: 2.0954723358154297
Validation loss: 2.0223772327105203

Epoch: 6| Step: 7
Training loss: 2.3468878269195557
Validation loss: 2.0198721289634705

Epoch: 6| Step: 8
Training loss: 2.6275107860565186
Validation loss: 2.0217129588127136

Epoch: 6| Step: 9
Training loss: 1.5223133563995361
Validation loss: 2.0206888715426126

Epoch: 6| Step: 10
Training loss: 2.6649186611175537
Validation loss: 2.0266211231549582

Epoch: 6| Step: 11
Training loss: 2.4031870365142822
Validation loss: 2.0184914271036782

Epoch: 6| Step: 12
Training loss: 2.629863739013672
Validation loss: 2.021058142185211

Epoch: 6| Step: 13
Training loss: 2.216779947280884
Validation loss: 2.017196853955587

Epoch: 84| Step: 0
Training loss: 2.3428821563720703
Validation loss: 2.0171284874280295

Epoch: 6| Step: 1
Training loss: 2.2890148162841797
Validation loss: 2.010986030101776

Epoch: 6| Step: 2
Training loss: 1.93101167678833
Validation loss: 2.012170930703481

Epoch: 6| Step: 3
Training loss: 2.536777973175049
Validation loss: 2.0209277272224426

Epoch: 6| Step: 4
Training loss: 2.3037538528442383
Validation loss: 2.0298937559127808

Epoch: 6| Step: 5
Training loss: 2.3657026290893555
Validation loss: 2.03439732392629

Epoch: 6| Step: 6
Training loss: 2.1764774322509766
Validation loss: 2.028364360332489

Epoch: 6| Step: 7
Training loss: 2.490314483642578
Validation loss: 2.037209451198578

Epoch: 6| Step: 8
Training loss: 1.8369839191436768
Validation loss: 2.0338849425315857

Epoch: 6| Step: 9
Training loss: 2.2616610527038574
Validation loss: 2.034466107686361

Epoch: 6| Step: 10
Training loss: 2.489534854888916
Validation loss: 2.036201298236847

Epoch: 6| Step: 11
Training loss: 2.449824810028076
Validation loss: 2.043396015961965

Epoch: 6| Step: 12
Training loss: 2.0461766719818115
Validation loss: 2.041017015775045

Epoch: 6| Step: 13
Training loss: 0.9302327036857605
Validation loss: 2.04043980439504

Epoch: 85| Step: 0
Training loss: 2.1837100982666016
Validation loss: 2.06236732006073

Epoch: 6| Step: 1
Training loss: 1.7688281536102295
Validation loss: 2.0617075165112815

Epoch: 6| Step: 2
Training loss: 2.5583159923553467
Validation loss: 2.052302579085032

Epoch: 6| Step: 3
Training loss: 2.5138063430786133
Validation loss: 2.0533547401428223

Epoch: 6| Step: 4
Training loss: 1.9713832139968872
Validation loss: 2.0407373706499734

Epoch: 6| Step: 5
Training loss: 2.250835418701172
Validation loss: 2.0439152916272483

Epoch: 6| Step: 6
Training loss: 2.3446812629699707
Validation loss: 2.0214420557022095

Epoch: 6| Step: 7
Training loss: 1.5177940130233765
Validation loss: 2.0269177754720054

Epoch: 6| Step: 8
Training loss: 2.3782966136932373
Validation loss: 2.027284324169159

Epoch: 6| Step: 9
Training loss: 2.194021701812744
Validation loss: 2.0289273460706077

Epoch: 6| Step: 10
Training loss: 2.2661325931549072
Validation loss: 2.033525745073954

Epoch: 6| Step: 11
Training loss: 2.610470771789551
Validation loss: 2.0295821030934653

Epoch: 6| Step: 12
Training loss: 2.160060405731201
Validation loss: 2.0378461678822837

Epoch: 6| Step: 13
Training loss: 2.204408645629883
Validation loss: 2.0387117862701416

Epoch: 86| Step: 0
Training loss: 1.4019463062286377
Validation loss: 2.0371262033780417

Epoch: 6| Step: 1
Training loss: 2.554694890975952
Validation loss: 2.0296003222465515

Epoch: 6| Step: 2
Training loss: 2.408384323120117
Validation loss: 2.0338407357533774

Epoch: 6| Step: 3
Training loss: 1.8298618793487549
Validation loss: 2.0294326742490134

Epoch: 6| Step: 4
Training loss: 2.670189380645752
Validation loss: 2.0289193789164224

Epoch: 6| Step: 5
Training loss: 1.9788836240768433
Validation loss: 2.023788253466288

Epoch: 6| Step: 6
Training loss: 2.0998644828796387
Validation loss: 2.0234153866767883

Epoch: 6| Step: 7
Training loss: 2.205298662185669
Validation loss: 2.0226270159085593

Epoch: 6| Step: 8
Training loss: 2.193742513656616
Validation loss: 2.0182473063468933

Epoch: 6| Step: 9
Training loss: 2.5958375930786133
Validation loss: 2.0162633061408997

Epoch: 6| Step: 10
Training loss: 2.0161614418029785
Validation loss: 2.020236591498057

Epoch: 6| Step: 11
Training loss: 1.8940935134887695
Validation loss: 2.0146419207255044

Epoch: 6| Step: 12
Training loss: 2.232638359069824
Validation loss: 2.02245702346166

Epoch: 6| Step: 13
Training loss: 2.638322114944458
Validation loss: 2.037580053011576

Epoch: 87| Step: 0
Training loss: 2.407529354095459
Validation loss: 2.0427875320116677

Epoch: 6| Step: 1
Training loss: 2.463076114654541
Validation loss: 2.0417244036992392

Epoch: 6| Step: 2
Training loss: 2.0133771896362305
Validation loss: 2.0476662516593933

Epoch: 6| Step: 3
Training loss: 2.088649272918701
Validation loss: 2.048929214477539

Epoch: 6| Step: 4
Training loss: 2.6265642642974854
Validation loss: 2.039461294809977

Epoch: 6| Step: 5
Training loss: 2.047983169555664
Validation loss: 2.039821763833364

Epoch: 6| Step: 6
Training loss: 2.0874321460723877
Validation loss: 2.0480418602625527

Epoch: 6| Step: 7
Training loss: 1.4124780893325806
Validation loss: 2.0507375995318093

Epoch: 6| Step: 8
Training loss: 2.5270333290100098
Validation loss: 2.044376810391744

Epoch: 6| Step: 9
Training loss: 1.942460060119629
Validation loss: 2.0422916213671365

Epoch: 6| Step: 10
Training loss: 2.427703619003296
Validation loss: 2.0388167897860208

Epoch: 6| Step: 11
Training loss: 1.8553270101547241
Validation loss: 2.0270238717397056

Epoch: 6| Step: 12
Training loss: 2.140329599380493
Validation loss: 2.0231854915618896

Epoch: 6| Step: 13
Training loss: 2.458249092102051
Validation loss: 2.0219258864720664

Epoch: 88| Step: 0
Training loss: 2.299982786178589
Validation loss: 2.024101734161377

Epoch: 6| Step: 1
Training loss: 2.061004638671875
Validation loss: 2.0217993458112082

Epoch: 6| Step: 2
Training loss: 1.936596155166626
Validation loss: 2.021221379439036

Epoch: 6| Step: 3
Training loss: 2.4530138969421387
Validation loss: 2.0195974111557007

Epoch: 6| Step: 4
Training loss: 2.052114963531494
Validation loss: 2.0249866048494973

Epoch: 6| Step: 5
Training loss: 2.428175449371338
Validation loss: 2.0198203325271606

Epoch: 6| Step: 6
Training loss: 2.4760429859161377
Validation loss: 2.0183800061543784

Epoch: 6| Step: 7
Training loss: 1.7533774375915527
Validation loss: 2.0226797064145408

Epoch: 6| Step: 8
Training loss: 1.8309944868087769
Validation loss: 2.0199474692344666

Epoch: 6| Step: 9
Training loss: 1.770161747932434
Validation loss: 2.0257808764775596

Epoch: 6| Step: 10
Training loss: 2.373307466506958
Validation loss: 2.035603940486908

Epoch: 6| Step: 11
Training loss: 1.9979369640350342
Validation loss: 2.0277286171913147

Epoch: 6| Step: 12
Training loss: 2.8691673278808594
Validation loss: 2.028316378593445

Epoch: 6| Step: 13
Training loss: 1.998428463935852
Validation loss: 2.0258079965909324

Epoch: 89| Step: 0
Training loss: 2.3552889823913574
Validation loss: 2.029644946257273

Epoch: 6| Step: 1
Training loss: 3.1719436645507812
Validation loss: 2.0309898058573403

Epoch: 6| Step: 2
Training loss: 2.324739694595337
Validation loss: 2.0400679310162864

Epoch: 6| Step: 3
Training loss: 2.2275161743164062
Validation loss: 2.042964975039164

Epoch: 6| Step: 4
Training loss: 1.5916533470153809
Validation loss: 2.03554634253184

Epoch: 6| Step: 5
Training loss: 2.080063819885254
Validation loss: 2.0332899490992227

Epoch: 6| Step: 6
Training loss: 1.75271737575531
Validation loss: 2.037337382634481

Epoch: 6| Step: 7
Training loss: 2.446908473968506
Validation loss: 2.036340892314911

Epoch: 6| Step: 8
Training loss: 2.1589765548706055
Validation loss: 2.0254876017570496

Epoch: 6| Step: 9
Training loss: 2.121093511581421
Validation loss: 2.0251144766807556

Epoch: 6| Step: 10
Training loss: 2.1846988201141357
Validation loss: 2.0190534591674805

Epoch: 6| Step: 11
Training loss: 2.3739585876464844
Validation loss: 2.012678941090902

Epoch: 6| Step: 12
Training loss: 1.6198666095733643
Validation loss: 2.020937442779541

Epoch: 6| Step: 13
Training loss: 1.9002056121826172
Validation loss: 2.0193132559458413

Epoch: 90| Step: 0
Training loss: 2.301485061645508
Validation loss: 2.0116093953450522

Epoch: 6| Step: 1
Training loss: 2.3217153549194336
Validation loss: 2.005376716454824

Epoch: 6| Step: 2
Training loss: 2.570539951324463
Validation loss: 2.008663276831309

Epoch: 6| Step: 3
Training loss: 2.068305492401123
Validation loss: 2.0114618937174478

Epoch: 6| Step: 4
Training loss: 2.1977100372314453
Validation loss: 2.0140390197436013

Epoch: 6| Step: 5
Training loss: 1.5636322498321533
Validation loss: 2.0125955740610757

Epoch: 6| Step: 6
Training loss: 2.395401954650879
Validation loss: 2.0084621906280518

Epoch: 6| Step: 7
Training loss: 2.748049020767212
Validation loss: 2.0172658363978067

Epoch: 6| Step: 8
Training loss: 1.4587218761444092
Validation loss: 2.0088701446851096

Epoch: 6| Step: 9
Training loss: 2.1935219764709473
Validation loss: 2.0089938640594482

Epoch: 6| Step: 10
Training loss: 2.4531869888305664
Validation loss: 2.01251749197642

Epoch: 6| Step: 11
Training loss: 2.1173057556152344
Validation loss: 2.0159700314203897

Epoch: 6| Step: 12
Training loss: 1.4988219738006592
Validation loss: 2.015208105246226

Epoch: 6| Step: 13
Training loss: 2.375495672225952
Validation loss: 2.019842008749644

Epoch: 91| Step: 0
Training loss: 2.1171822547912598
Validation loss: 2.021199961503347

Epoch: 6| Step: 1
Training loss: 2.1235504150390625
Validation loss: 2.0269593397776284

Epoch: 6| Step: 2
Training loss: 2.504364013671875
Validation loss: 2.0240456660588584

Epoch: 6| Step: 3
Training loss: 2.032862424850464
Validation loss: 2.0196126898129783

Epoch: 6| Step: 4
Training loss: 2.1029067039489746
Validation loss: 2.025408089160919

Epoch: 6| Step: 5
Training loss: 2.3989500999450684
Validation loss: 2.0335205793380737

Epoch: 6| Step: 6
Training loss: 1.545973300933838
Validation loss: 2.0296490589777627

Epoch: 6| Step: 7
Training loss: 2.157175064086914
Validation loss: 2.0389338533083596

Epoch: 6| Step: 8
Training loss: 1.744814395904541
Validation loss: 2.0362701416015625

Epoch: 6| Step: 9
Training loss: 2.2771081924438477
Validation loss: 2.0384174386660256

Epoch: 6| Step: 10
Training loss: 1.9025378227233887
Validation loss: 2.043626844882965

Epoch: 6| Step: 11
Training loss: 2.452502727508545
Validation loss: 2.039123813311259

Epoch: 6| Step: 12
Training loss: 2.334861993789673
Validation loss: 2.037186086177826

Epoch: 6| Step: 13
Training loss: 2.626457452774048
Validation loss: 2.0332128206888833

Epoch: 92| Step: 0
Training loss: 2.591043710708618
Validation loss: 2.031266232331594

Epoch: 6| Step: 1
Training loss: 2.137289524078369
Validation loss: 2.019343833128611

Epoch: 6| Step: 2
Training loss: 1.9569978713989258
Validation loss: 2.0266756614049277

Epoch: 6| Step: 3
Training loss: 1.8860799074172974
Validation loss: 2.0177785952885947

Epoch: 6| Step: 4
Training loss: 2.352806568145752
Validation loss: 2.024202068646749

Epoch: 6| Step: 5
Training loss: 1.7591723203659058
Validation loss: 2.026355524857839

Epoch: 6| Step: 6
Training loss: 2.0435664653778076
Validation loss: 2.0291665395100913

Epoch: 6| Step: 7
Training loss: 2.543417453765869
Validation loss: 2.03261669476827

Epoch: 6| Step: 8
Training loss: 1.6732726097106934
Validation loss: 2.0305751959482827

Epoch: 6| Step: 9
Training loss: 2.385836124420166
Validation loss: 2.026604930559794

Epoch: 6| Step: 10
Training loss: 2.308267593383789
Validation loss: 2.033458689848582

Epoch: 6| Step: 11
Training loss: 2.866340398788452
Validation loss: 2.026183227698008

Epoch: 6| Step: 12
Training loss: 1.7015403509140015
Validation loss: 2.026919702688853

Epoch: 6| Step: 13
Training loss: 2.2428135871887207
Validation loss: 2.025411089261373

Epoch: 93| Step: 0
Training loss: 1.698830008506775
Validation loss: 2.0247920552889505

Epoch: 6| Step: 1
Training loss: 2.1491782665252686
Validation loss: 2.0140035152435303

Epoch: 6| Step: 2
Training loss: 2.074208974838257
Validation loss: 2.015461802482605

Epoch: 6| Step: 3
Training loss: 2.9001824855804443
Validation loss: 2.014128049214681

Epoch: 6| Step: 4
Training loss: 1.7394001483917236
Validation loss: 2.020017623901367

Epoch: 6| Step: 5
Training loss: 2.2317404747009277
Validation loss: 2.0259894529978433

Epoch: 6| Step: 6
Training loss: 2.3989152908325195
Validation loss: 2.0191737612088523

Epoch: 6| Step: 7
Training loss: 2.4018898010253906
Validation loss: 2.030080715815226

Epoch: 6| Step: 8
Training loss: 1.9405041933059692
Validation loss: 2.030679444471995

Epoch: 6| Step: 9
Training loss: 2.4697654247283936
Validation loss: 2.0297515988349915

Epoch: 6| Step: 10
Training loss: 2.045379400253296
Validation loss: 2.0217854181925454

Epoch: 6| Step: 11
Training loss: 2.113213062286377
Validation loss: 2.024124185244242

Epoch: 6| Step: 12
Training loss: 1.7615567445755005
Validation loss: 2.029852012793223

Epoch: 6| Step: 13
Training loss: 2.460799217224121
Validation loss: 2.026308755079905

Epoch: 94| Step: 0
Training loss: 1.8965879678726196
Validation loss: 2.031151314576467

Epoch: 6| Step: 1
Training loss: 1.9594069719314575
Validation loss: 2.0279135505358377

Epoch: 6| Step: 2
Training loss: 2.081150770187378
Validation loss: 2.0288865764935813

Epoch: 6| Step: 3
Training loss: 1.8925446271896362
Validation loss: 2.0264191031455994

Epoch: 6| Step: 4
Training loss: 2.1652798652648926
Validation loss: 2.0247925917307534

Epoch: 6| Step: 5
Training loss: 2.1791675090789795
Validation loss: 2.0278391440709433

Epoch: 6| Step: 6
Training loss: 1.1070348024368286
Validation loss: 2.0211777488390603

Epoch: 6| Step: 7
Training loss: 2.27585506439209
Validation loss: 2.018672068913778

Epoch: 6| Step: 8
Training loss: 2.6258411407470703
Validation loss: 2.0266961654027305

Epoch: 6| Step: 9
Training loss: 2.625682830810547
Validation loss: 2.019585827986399

Epoch: 6| Step: 10
Training loss: 2.7739925384521484
Validation loss: 2.016178250312805

Epoch: 6| Step: 11
Training loss: 2.2850685119628906
Validation loss: 2.012609362602234

Epoch: 6| Step: 12
Training loss: 1.8601526021957397
Validation loss: 2.011603593826294

Epoch: 6| Step: 13
Training loss: 2.379377841949463
Validation loss: 2.008038063844045

Epoch: 95| Step: 0
Training loss: 2.8084535598754883
Validation loss: 2.015088895956675

Epoch: 6| Step: 1
Training loss: 2.300166130065918
Validation loss: 2.0128812988599143

Epoch: 6| Step: 2
Training loss: 2.411865234375
Validation loss: 2.014559249083201

Epoch: 6| Step: 3
Training loss: 2.4175124168395996
Validation loss: 2.015372633934021

Epoch: 6| Step: 4
Training loss: 2.1222574710845947
Validation loss: 2.023132860660553

Epoch: 6| Step: 5
Training loss: 1.804172158241272
Validation loss: 2.0246013005574546

Epoch: 6| Step: 6
Training loss: 1.6376838684082031
Validation loss: 2.0271369417508445

Epoch: 6| Step: 7
Training loss: 1.350902795791626
Validation loss: 2.0388312141100564

Epoch: 6| Step: 8
Training loss: 2.3022849559783936
Validation loss: 2.049530287583669

Epoch: 6| Step: 9
Training loss: 2.2759759426116943
Validation loss: 2.058158020178477

Epoch: 6| Step: 10
Training loss: 2.7503483295440674
Validation loss: 2.05632621049881

Epoch: 6| Step: 11
Training loss: 1.9042099714279175
Validation loss: 2.0559334754943848

Epoch: 6| Step: 12
Training loss: 2.391585350036621
Validation loss: 2.0414095918337503

Epoch: 6| Step: 13
Training loss: 1.68168044090271
Validation loss: 2.0483285188674927

Epoch: 96| Step: 0
Training loss: 2.18459153175354
Validation loss: 2.032456715901693

Epoch: 6| Step: 1
Training loss: 1.7832341194152832
Validation loss: 2.0266650915145874

Epoch: 6| Step: 2
Training loss: 2.402690887451172
Validation loss: 2.0194870233535767

Epoch: 6| Step: 3
Training loss: 1.7329422235488892
Validation loss: 2.0158979098002114

Epoch: 6| Step: 4
Training loss: 2.1138827800750732
Validation loss: 2.0206244786580405

Epoch: 6| Step: 5
Training loss: 1.8922061920166016
Validation loss: 2.026941259702047

Epoch: 6| Step: 6
Training loss: 2.0267887115478516
Validation loss: 2.0295830965042114

Epoch: 6| Step: 7
Training loss: 2.6442155838012695
Validation loss: 2.0294651786486306

Epoch: 6| Step: 8
Training loss: 1.9808982610702515
Validation loss: 2.024729331334432

Epoch: 6| Step: 9
Training loss: 2.0522384643554688
Validation loss: 2.0243932803471885

Epoch: 6| Step: 10
Training loss: 2.5598974227905273
Validation loss: 2.0187565088272095

Epoch: 6| Step: 11
Training loss: 2.320573329925537
Validation loss: 2.020806054274241

Epoch: 6| Step: 12
Training loss: 2.5486128330230713
Validation loss: 2.0220471819241843

Epoch: 6| Step: 13
Training loss: 2.2297427654266357
Validation loss: 2.0200467705726624

Epoch: 97| Step: 0
Training loss: 1.9181206226348877
Validation loss: 2.0215024948120117

Epoch: 6| Step: 1
Training loss: 2.3819689750671387
Validation loss: 2.020496428012848

Epoch: 6| Step: 2
Training loss: 2.4376895427703857
Validation loss: 2.019221047560374

Epoch: 6| Step: 3
Training loss: 1.7071067094802856
Validation loss: 2.0287824670473733

Epoch: 6| Step: 4
Training loss: 2.5614585876464844
Validation loss: 2.030258357524872

Epoch: 6| Step: 5
Training loss: 2.687474489212036
Validation loss: 2.0288546085357666

Epoch: 6| Step: 6
Training loss: 2.2467098236083984
Validation loss: 2.0384195844332376

Epoch: 6| Step: 7
Training loss: 1.9670225381851196
Validation loss: 2.0491908391316733

Epoch: 6| Step: 8
Training loss: 2.128349781036377
Validation loss: 2.0416020154953003

Epoch: 6| Step: 9
Training loss: 2.1729202270507812
Validation loss: 2.0498053630193076

Epoch: 6| Step: 10
Training loss: 1.1565337181091309
Validation loss: 2.0439088145891824

Epoch: 6| Step: 11
Training loss: 2.0356786251068115
Validation loss: 2.039613723754883

Epoch: 6| Step: 12
Training loss: 2.3993005752563477
Validation loss: 2.0345543225606284

Epoch: 6| Step: 13
Training loss: 2.40946102142334
Validation loss: 2.025567809740702

Epoch: 98| Step: 0
Training loss: 2.7473502159118652
Validation loss: 2.0205843448638916

Epoch: 6| Step: 1
Training loss: 2.379444122314453
Validation loss: 2.0266823172569275

Epoch: 6| Step: 2
Training loss: 2.481499433517456
Validation loss: 2.031746437152227

Epoch: 6| Step: 3
Training loss: 2.43644380569458
Validation loss: 2.036580185095469

Epoch: 6| Step: 4
Training loss: 2.530294895172119
Validation loss: 2.0399346152941384

Epoch: 6| Step: 5
Training loss: 1.988929033279419
Validation loss: 2.0450264612833657

Epoch: 6| Step: 6
Training loss: 1.9250643253326416
Validation loss: 2.036952873071035

Epoch: 6| Step: 7
Training loss: 1.7620022296905518
Validation loss: 2.0417861938476562

Epoch: 6| Step: 8
Training loss: 2.154813289642334
Validation loss: 2.0399744113286338

Epoch: 6| Step: 9
Training loss: 1.9299964904785156
Validation loss: 2.034707248210907

Epoch: 6| Step: 10
Training loss: 2.0634703636169434
Validation loss: 2.0373375614484153

Epoch: 6| Step: 11
Training loss: 1.9216680526733398
Validation loss: 2.0349441369374595

Epoch: 6| Step: 12
Training loss: 1.943809986114502
Validation loss: 2.027993381023407

Epoch: 6| Step: 13
Training loss: 2.2286243438720703
Validation loss: 2.0246394872665405

Epoch: 99| Step: 0
Training loss: 2.071164131164551
Validation loss: 2.021694084008535

Epoch: 6| Step: 1
Training loss: 2.3629562854766846
Validation loss: 2.025130252043406

Epoch: 6| Step: 2
Training loss: 2.3355813026428223
Validation loss: 2.0222257177035012

Epoch: 6| Step: 3
Training loss: 2.2899563312530518
Validation loss: 2.016664147377014

Epoch: 6| Step: 4
Training loss: 1.572617769241333
Validation loss: 2.015826721986135

Epoch: 6| Step: 5
Training loss: 3.2910449504852295
Validation loss: 2.0128457148869834

Epoch: 6| Step: 6
Training loss: 1.69840669631958
Validation loss: 2.020618955294291

Epoch: 6| Step: 7
Training loss: 1.9371684789657593
Validation loss: 2.015264650185903

Epoch: 6| Step: 8
Training loss: 2.226672649383545
Validation loss: 2.013328433036804

Epoch: 6| Step: 9
Training loss: 1.923673152923584
Validation loss: 2.0114765763282776

Epoch: 6| Step: 10
Training loss: 2.3070228099823
Validation loss: 2.016824940840403

Epoch: 6| Step: 11
Training loss: 1.8697869777679443
Validation loss: 2.0175249576568604

Epoch: 6| Step: 12
Training loss: 1.6338030099868774
Validation loss: 2.0127133329709372

Epoch: 6| Step: 13
Training loss: 2.6201529502868652
Validation loss: 2.0236380100250244

Epoch: 100| Step: 0
Training loss: 1.7701860666275024
Validation loss: 2.0290534098943076

Epoch: 6| Step: 1
Training loss: 1.5287814140319824
Validation loss: 2.0384976267814636

Epoch: 6| Step: 2
Training loss: 1.5763591527938843
Validation loss: 2.0384894808133445

Epoch: 6| Step: 3
Training loss: 2.4412083625793457
Validation loss: 2.064004818598429

Epoch: 6| Step: 4
Training loss: 2.4105677604675293
Validation loss: 2.0731535156567893

Epoch: 6| Step: 5
Training loss: 2.320168972015381
Validation loss: 2.0685396989186606

Epoch: 6| Step: 6
Training loss: 2.639726400375366
Validation loss: 2.0736767053604126

Epoch: 6| Step: 7
Training loss: 2.4029877185821533
Validation loss: 2.0820401708285012

Epoch: 6| Step: 8
Training loss: 2.511739730834961
Validation loss: 2.0573832790056863

Epoch: 6| Step: 9
Training loss: 2.374691963195801
Validation loss: 2.0439210732777915

Epoch: 6| Step: 10
Training loss: 1.7593632936477661
Validation loss: 2.027454912662506

Epoch: 6| Step: 11
Training loss: 2.47819447517395
Validation loss: 2.018383045991262

Epoch: 6| Step: 12
Training loss: 1.724947214126587
Validation loss: 2.0086103876431785

Epoch: 6| Step: 13
Training loss: 2.96944522857666
Validation loss: 2.0157527327537537

Epoch: 101| Step: 0
Training loss: 2.2769947052001953
Validation loss: 2.0192925135294595

Epoch: 6| Step: 1
Training loss: 2.462676525115967
Validation loss: 2.0251667896906533

Epoch: 6| Step: 2
Training loss: 2.4500579833984375
Validation loss: 2.0372486313184104

Epoch: 6| Step: 3
Training loss: 2.082157850265503
Validation loss: 2.0449132323265076

Epoch: 6| Step: 4
Training loss: 1.7671549320220947
Validation loss: 2.0432133078575134

Epoch: 6| Step: 5
Training loss: 1.9323562383651733
Validation loss: 2.0452234745025635

Epoch: 6| Step: 6
Training loss: 2.8657727241516113
Validation loss: 2.0516974727312722

Epoch: 6| Step: 7
Training loss: 1.4919037818908691
Validation loss: 2.055094043413798

Epoch: 6| Step: 8
Training loss: 2.1846108436584473
Validation loss: 2.045025944709778

Epoch: 6| Step: 9
Training loss: 2.345648765563965
Validation loss: 2.04974236090978

Epoch: 6| Step: 10
Training loss: 1.8490079641342163
Validation loss: 2.049294372399648

Epoch: 6| Step: 11
Training loss: 2.303293228149414
Validation loss: 2.044989049434662

Epoch: 6| Step: 12
Training loss: 2.683828830718994
Validation loss: 2.0450987815856934

Epoch: 6| Step: 13
Training loss: 2.3643133640289307
Validation loss: 2.035801668961843

Epoch: 102| Step: 0
Training loss: 1.6574511528015137
Validation loss: 2.03728578488032

Epoch: 6| Step: 1
Training loss: 1.858083724975586
Validation loss: 2.023383855819702

Epoch: 6| Step: 2
Training loss: 1.8761110305786133
Validation loss: 2.024053613344828

Epoch: 6| Step: 3
Training loss: 2.3452553749084473
Validation loss: 2.0190901358922324

Epoch: 6| Step: 4
Training loss: 2.5412678718566895
Validation loss: 2.015473743279775

Epoch: 6| Step: 5
Training loss: 2.2106900215148926
Validation loss: 2.013622999191284

Epoch: 6| Step: 6
Training loss: 2.7074007987976074
Validation loss: 2.0144598881403604

Epoch: 6| Step: 7
Training loss: 2.0933239459991455
Validation loss: 2.00973778963089

Epoch: 6| Step: 8
Training loss: 2.3164052963256836
Validation loss: 2.0204845666885376

Epoch: 6| Step: 9
Training loss: 2.4993677139282227
Validation loss: 2.0350850224494934

Epoch: 6| Step: 10
Training loss: 1.7616865634918213
Validation loss: 2.036259134610494

Epoch: 6| Step: 11
Training loss: 2.1527724266052246
Validation loss: 2.0404791235923767

Epoch: 6| Step: 12
Training loss: 2.824775218963623
Validation loss: 2.0370346903800964

Epoch: 6| Step: 13
Training loss: 1.6304795742034912
Validation loss: 2.033707618713379

Epoch: 103| Step: 0
Training loss: 1.1845850944519043
Validation loss: 2.0252989530563354

Epoch: 6| Step: 1
Training loss: 1.893310308456421
Validation loss: 2.032128393650055

Epoch: 6| Step: 2
Training loss: 2.767261505126953
Validation loss: 2.029999792575836

Epoch: 6| Step: 3
Training loss: 2.2729148864746094
Validation loss: 2.0248592694600425

Epoch: 6| Step: 4
Training loss: 2.7044708728790283
Validation loss: 2.0269765059153237

Epoch: 6| Step: 5
Training loss: 2.1100142002105713
Validation loss: 2.025375545024872

Epoch: 6| Step: 6
Training loss: 1.9836416244506836
Validation loss: 2.028899868329366

Epoch: 6| Step: 7
Training loss: 2.105384111404419
Validation loss: 2.026746988296509

Epoch: 6| Step: 8
Training loss: 2.510472297668457
Validation loss: 2.0238918463389077

Epoch: 6| Step: 9
Training loss: 2.3690710067749023
Validation loss: 2.011740485827128

Epoch: 6| Step: 10
Training loss: 2.442411422729492
Validation loss: 2.013368626435598

Epoch: 6| Step: 11
Training loss: 1.86806058883667
Validation loss: 2.0146808425585427

Epoch: 6| Step: 12
Training loss: 1.6472384929656982
Validation loss: 2.0099562406539917

Epoch: 6| Step: 13
Training loss: 2.221134901046753
Validation loss: 2.00980939467748

Epoch: 104| Step: 0
Training loss: 2.2777342796325684
Validation loss: 2.009665548801422

Epoch: 6| Step: 1
Training loss: 1.776894211769104
Validation loss: 2.0103305776913962

Epoch: 6| Step: 2
Training loss: 1.9658427238464355
Validation loss: 2.0141150752703347

Epoch: 6| Step: 3
Training loss: 2.0676541328430176
Validation loss: 2.020940899848938

Epoch: 6| Step: 4
Training loss: 1.9144693613052368
Validation loss: 2.0253164966901145

Epoch: 6| Step: 5
Training loss: 1.9450132846832275
Validation loss: 2.023285210132599

Epoch: 6| Step: 6
Training loss: 1.9407809972763062
Validation loss: 2.0333829124768577

Epoch: 6| Step: 7
Training loss: 2.4863548278808594
Validation loss: 2.0357318917910256

Epoch: 6| Step: 8
Training loss: 1.8908334970474243
Validation loss: 2.027106543382009

Epoch: 6| Step: 9
Training loss: 1.9032379388809204
Validation loss: 2.028535783290863

Epoch: 6| Step: 10
Training loss: 2.3247668743133545
Validation loss: 2.0260268648465476

Epoch: 6| Step: 11
Training loss: 2.1293764114379883
Validation loss: 2.0325605471928916

Epoch: 6| Step: 12
Training loss: 2.772432804107666
Validation loss: 2.029259820779165

Epoch: 6| Step: 13
Training loss: 2.4742932319641113
Validation loss: 2.03114519516627

Epoch: 105| Step: 0
Training loss: 2.3616929054260254
Validation loss: 2.0325990120569863

Epoch: 6| Step: 1
Training loss: 2.1076881885528564
Validation loss: 2.025735874970754

Epoch: 6| Step: 2
Training loss: 2.2968125343322754
Validation loss: 2.0313541491826377

Epoch: 6| Step: 3
Training loss: 2.44209623336792
Validation loss: 2.0250945687294006

Epoch: 6| Step: 4
Training loss: 2.0388965606689453
Validation loss: 2.025302608807882

Epoch: 6| Step: 5
Training loss: 2.1113271713256836
Validation loss: 2.0194953282674155

Epoch: 6| Step: 6
Training loss: 1.9044196605682373
Validation loss: 2.0228352546691895

Epoch: 6| Step: 7
Training loss: 1.8376959562301636
Validation loss: 2.021891494592031

Epoch: 6| Step: 8
Training loss: 1.5831003189086914
Validation loss: 2.0135239362716675

Epoch: 6| Step: 9
Training loss: 1.9013302326202393
Validation loss: 2.0264639655749

Epoch: 6| Step: 10
Training loss: 2.805527687072754
Validation loss: 2.0266785422960916

Epoch: 6| Step: 11
Training loss: 2.1115570068359375
Validation loss: 2.023788789908091

Epoch: 6| Step: 12
Training loss: 2.440535306930542
Validation loss: 2.0276317397753396

Epoch: 6| Step: 13
Training loss: 1.9526314735412598
Validation loss: 2.0254476070404053

Epoch: 106| Step: 0
Training loss: 2.478358268737793
Validation loss: 2.0259131590525308

Epoch: 6| Step: 1
Training loss: 2.110936164855957
Validation loss: 2.028779149055481

Epoch: 6| Step: 2
Training loss: 1.697178602218628
Validation loss: 2.0270360509554544

Epoch: 6| Step: 3
Training loss: 1.9021453857421875
Validation loss: 2.0375773906707764

Epoch: 6| Step: 4
Training loss: 2.681661367416382
Validation loss: 2.0247057676315308

Epoch: 6| Step: 5
Training loss: 1.8543967008590698
Validation loss: 2.0285800298055015

Epoch: 6| Step: 6
Training loss: 2.1106600761413574
Validation loss: 2.036795159180959

Epoch: 6| Step: 7
Training loss: 2.1030731201171875
Validation loss: 2.0291800101598105

Epoch: 6| Step: 8
Training loss: 2.3359336853027344
Validation loss: 2.0286380449930825

Epoch: 6| Step: 9
Training loss: 2.571625232696533
Validation loss: 2.0357371966044107

Epoch: 6| Step: 10
Training loss: 2.0503287315368652
Validation loss: 2.0344690879185996

Epoch: 6| Step: 11
Training loss: 2.146609306335449
Validation loss: 2.033188740412394

Epoch: 6| Step: 12
Training loss: 2.1927034854888916
Validation loss: 2.0334037144978843

Epoch: 6| Step: 13
Training loss: 1.6588315963745117
Validation loss: 2.0228420893351235

Epoch: 107| Step: 0
Training loss: 3.0665507316589355
Validation loss: 2.0275830825169883

Epoch: 6| Step: 1
Training loss: 2.0667567253112793
Validation loss: 2.021433095137278

Epoch: 6| Step: 2
Training loss: 1.5591590404510498
Validation loss: 2.022402207056681

Epoch: 6| Step: 3
Training loss: 2.244631290435791
Validation loss: 2.0245964924494424

Epoch: 6| Step: 4
Training loss: 1.6156463623046875
Validation loss: 2.0291231075922647

Epoch: 6| Step: 5
Training loss: 1.4929388761520386
Validation loss: 2.0338704586029053

Epoch: 6| Step: 6
Training loss: 2.3749942779541016
Validation loss: 2.0349217454592385

Epoch: 6| Step: 7
Training loss: 2.3427743911743164
Validation loss: 2.0355372230211892

Epoch: 6| Step: 8
Training loss: 2.5960826873779297
Validation loss: 2.0295347372690835

Epoch: 6| Step: 9
Training loss: 2.275280475616455
Validation loss: 2.0303290287653604

Epoch: 6| Step: 10
Training loss: 2.0547826290130615
Validation loss: 2.0275527040163674

Epoch: 6| Step: 11
Training loss: 2.1582190990448
Validation loss: 2.025053401788076

Epoch: 6| Step: 12
Training loss: 2.1878185272216797
Validation loss: 2.0230828126271567

Epoch: 6| Step: 13
Training loss: 2.2477145195007324
Validation loss: 2.022571623325348

Epoch: 108| Step: 0
Training loss: 2.2569563388824463
Validation loss: 2.0226861238479614

Epoch: 6| Step: 1
Training loss: 2.0847697257995605
Validation loss: 2.022974411646525

Epoch: 6| Step: 2
Training loss: 1.976797103881836
Validation loss: 2.017335573832194

Epoch: 6| Step: 3
Training loss: 2.5908284187316895
Validation loss: 2.025787274042765

Epoch: 6| Step: 4
Training loss: 2.18292498588562
Validation loss: 2.0180423259735107

Epoch: 6| Step: 5
Training loss: 2.231262683868408
Validation loss: 2.0254900058110556

Epoch: 6| Step: 6
Training loss: 2.4495503902435303
Validation loss: 2.026892284552256

Epoch: 6| Step: 7
Training loss: 2.6262073516845703
Validation loss: 2.0353844165802

Epoch: 6| Step: 8
Training loss: 1.8034484386444092
Validation loss: 2.0299868981043496

Epoch: 6| Step: 9
Training loss: 2.123549699783325
Validation loss: 2.0297398964564004

Epoch: 6| Step: 10
Training loss: 1.8695578575134277
Validation loss: 2.0214425722757974

Epoch: 6| Step: 11
Training loss: 2.1700353622436523
Validation loss: 2.0146164894104004

Epoch: 6| Step: 12
Training loss: 2.1114816665649414
Validation loss: 2.0096808870633445

Epoch: 6| Step: 13
Training loss: 1.6614885330200195
Validation loss: 2.0032679239908853

Epoch: 109| Step: 0
Training loss: 1.929274559020996
Validation loss: 2.0070818265279136

Epoch: 6| Step: 1
Training loss: 2.4227685928344727
Validation loss: 2.007729927698771

Epoch: 6| Step: 2
Training loss: 2.6095285415649414
Validation loss: 2.00227028131485

Epoch: 6| Step: 3
Training loss: 1.9670041799545288
Validation loss: 2.0051145354906716

Epoch: 6| Step: 4
Training loss: 2.490917921066284
Validation loss: 2.0009430050849915

Epoch: 6| Step: 5
Training loss: 2.203494071960449
Validation loss: 2.0062463084856668

Epoch: 6| Step: 6
Training loss: 2.416520833969116
Validation loss: 2.008257885773977

Epoch: 6| Step: 7
Training loss: 2.5530753135681152
Validation loss: 2.003295421600342

Epoch: 6| Step: 8
Training loss: 1.491166591644287
Validation loss: 2.002877692381541

Epoch: 6| Step: 9
Training loss: 2.6670403480529785
Validation loss: 2.001432995001475

Epoch: 6| Step: 10
Training loss: 1.9656747579574585
Validation loss: 2.0002026160558066

Epoch: 6| Step: 11
Training loss: 1.499390959739685
Validation loss: 2.0076464215914407

Epoch: 6| Step: 12
Training loss: 2.0106966495513916
Validation loss: 2.0042704939842224

Epoch: 6| Step: 13
Training loss: 1.8573482036590576
Validation loss: 2.0107575257619223

Epoch: 110| Step: 0
Training loss: 2.731778621673584
Validation loss: 2.012216548124949

Epoch: 6| Step: 1
Training loss: 1.9138085842132568
Validation loss: 2.0026511947313943

Epoch: 6| Step: 2
Training loss: 1.9620834589004517
Validation loss: 2.0183712045351663

Epoch: 6| Step: 3
Training loss: 2.3209025859832764
Validation loss: 2.0140342513720193

Epoch: 6| Step: 4
Training loss: 2.0070672035217285
Validation loss: 2.0143704215685525

Epoch: 6| Step: 5
Training loss: 2.049236297607422
Validation loss: 2.023789962132772

Epoch: 6| Step: 6
Training loss: 1.892615556716919
Validation loss: 2.034584561983744

Epoch: 6| Step: 7
Training loss: 2.224837303161621
Validation loss: 2.0225517551104226

Epoch: 6| Step: 8
Training loss: 2.4794249534606934
Validation loss: 2.0293942292531333

Epoch: 6| Step: 9
Training loss: 1.7538399696350098
Validation loss: 2.033771733442942

Epoch: 6| Step: 10
Training loss: 2.416231155395508
Validation loss: 2.031275471051534

Epoch: 6| Step: 11
Training loss: 2.1747183799743652
Validation loss: 2.030468006928762

Epoch: 6| Step: 12
Training loss: 2.118107318878174
Validation loss: 2.0389126737912497

Epoch: 6| Step: 13
Training loss: 1.8284732103347778
Validation loss: 2.034261723359426

Epoch: 111| Step: 0
Training loss: 2.1898841857910156
Validation loss: 2.038717269897461

Epoch: 6| Step: 1
Training loss: 2.1430561542510986
Validation loss: 2.0289153258005777

Epoch: 6| Step: 2
Training loss: 2.327063798904419
Validation loss: 2.0147139032681785

Epoch: 6| Step: 3
Training loss: 1.1544735431671143
Validation loss: 2.0241174896558127

Epoch: 6| Step: 4
Training loss: 1.926371693611145
Validation loss: 2.0116789738337197

Epoch: 6| Step: 5
Training loss: 2.4205069541931152
Validation loss: 2.016185760498047

Epoch: 6| Step: 6
Training loss: 2.2565395832061768
Validation loss: 2.0178149342536926

Epoch: 6| Step: 7
Training loss: 2.577559471130371
Validation loss: 2.012490967909495

Epoch: 6| Step: 8
Training loss: 1.7441370487213135
Validation loss: 2.0120795567830405

Epoch: 6| Step: 9
Training loss: 2.1848669052124023
Validation loss: 2.0254812836647034

Epoch: 6| Step: 10
Training loss: 2.1411757469177246
Validation loss: 2.0149264534314475

Epoch: 6| Step: 11
Training loss: 2.1149234771728516
Validation loss: 2.021287182966868

Epoch: 6| Step: 12
Training loss: 2.168386936187744
Validation loss: 2.0210572679837546

Epoch: 6| Step: 13
Training loss: 2.434865713119507
Validation loss: 2.0160866578420005

Epoch: 112| Step: 0
Training loss: 2.5637073516845703
Validation loss: 2.025139093399048

Epoch: 6| Step: 1
Training loss: 2.6098618507385254
Validation loss: 2.0139674743016562

Epoch: 6| Step: 2
Training loss: 1.7748980522155762
Validation loss: 2.0201202233632407

Epoch: 6| Step: 3
Training loss: 1.5959316492080688
Validation loss: 2.0202799638112388

Epoch: 6| Step: 4
Training loss: 2.051708221435547
Validation loss: 2.023280759652456

Epoch: 6| Step: 5
Training loss: 1.7883942127227783
Validation loss: 2.0258320569992065

Epoch: 6| Step: 6
Training loss: 2.0822954177856445
Validation loss: 2.040624479452769

Epoch: 6| Step: 7
Training loss: 2.057518243789673
Validation loss: 2.030908544858297

Epoch: 6| Step: 8
Training loss: 2.17809796333313
Validation loss: 2.023854831854502

Epoch: 6| Step: 9
Training loss: 2.0398786067962646
Validation loss: 2.0181105931599936

Epoch: 6| Step: 10
Training loss: 2.390395164489746
Validation loss: 2.0182815194129944

Epoch: 6| Step: 11
Training loss: 2.497438430786133
Validation loss: 2.0204389890034995

Epoch: 6| Step: 12
Training loss: 2.118497848510742
Validation loss: 2.0168755849202475

Epoch: 6| Step: 13
Training loss: 2.0051817893981934
Validation loss: 2.0157976945241294

Epoch: 113| Step: 0
Training loss: 1.7791327238082886
Validation loss: 2.014558732509613

Epoch: 6| Step: 1
Training loss: 2.2891745567321777
Validation loss: 2.0188050071398416

Epoch: 6| Step: 2
Training loss: 1.4623552560806274
Validation loss: 2.0196819504102073

Epoch: 6| Step: 3
Training loss: 2.4735703468322754
Validation loss: 2.0109897454579673

Epoch: 6| Step: 4
Training loss: 2.681553363800049
Validation loss: 2.0249107082684836

Epoch: 6| Step: 5
Training loss: 1.7326799631118774
Validation loss: 2.0194676915804544

Epoch: 6| Step: 6
Training loss: 1.6556228399276733
Validation loss: 2.019063115119934

Epoch: 6| Step: 7
Training loss: 1.9235244989395142
Validation loss: 2.029789984226227

Epoch: 6| Step: 8
Training loss: 2.5791361331939697
Validation loss: 2.026057799657186

Epoch: 6| Step: 9
Training loss: 2.158205986022949
Validation loss: 2.0262411236763

Epoch: 6| Step: 10
Training loss: 1.959213137626648
Validation loss: 2.0456012884775796

Epoch: 6| Step: 11
Training loss: 1.9201722145080566
Validation loss: 2.0279732743899026

Epoch: 6| Step: 12
Training loss: 2.4061126708984375
Validation loss: 2.029711206754049

Epoch: 6| Step: 13
Training loss: 2.731196165084839
Validation loss: 2.051747659842173

Epoch: 114| Step: 0
Training loss: 2.5314581394195557
Validation loss: 2.0430800517400107

Epoch: 6| Step: 1
Training loss: 2.091590404510498
Validation loss: 2.0415420134862265

Epoch: 6| Step: 2
Training loss: 2.321312427520752
Validation loss: 2.0319388707478843

Epoch: 6| Step: 3
Training loss: 1.8558666706085205
Validation loss: 2.0275051395098367

Epoch: 6| Step: 4
Training loss: 2.460153818130493
Validation loss: 2.0141412814458213

Epoch: 6| Step: 5
Training loss: 1.6774826049804688
Validation loss: 2.01993461449941

Epoch: 6| Step: 6
Training loss: 2.3986990451812744
Validation loss: 2.0125430623690286

Epoch: 6| Step: 7
Training loss: 2.5929205417633057
Validation loss: 2.018171409765879

Epoch: 6| Step: 8
Training loss: 1.8427367210388184
Validation loss: 2.015316049257914

Epoch: 6| Step: 9
Training loss: 2.1486454010009766
Validation loss: 2.0149716337521872

Epoch: 6| Step: 10
Training loss: 2.1556191444396973
Validation loss: 2.0189627210299173

Epoch: 6| Step: 11
Training loss: 2.1165614128112793
Validation loss: 2.0220429499944053

Epoch: 6| Step: 12
Training loss: 1.7632766962051392
Validation loss: 2.027333617210388

Epoch: 6| Step: 13
Training loss: 1.8548929691314697
Validation loss: 2.0263400872548423

Epoch: 115| Step: 0
Training loss: 2.623300075531006
Validation loss: 2.038962801297506

Epoch: 6| Step: 1
Training loss: 1.770681619644165
Validation loss: 2.043135404586792

Epoch: 6| Step: 2
Training loss: 1.9366416931152344
Validation loss: 2.050386289755503

Epoch: 6| Step: 3
Training loss: 2.3676750659942627
Validation loss: 2.0576393405596414

Epoch: 6| Step: 4
Training loss: 2.243478775024414
Validation loss: 2.0627577106157937

Epoch: 6| Step: 5
Training loss: 2.3478949069976807
Validation loss: 2.072469790776571

Epoch: 6| Step: 6
Training loss: 2.2701730728149414
Validation loss: 2.058403710524241

Epoch: 6| Step: 7
Training loss: 1.3794662952423096
Validation loss: 2.056624432404836

Epoch: 6| Step: 8
Training loss: 1.9461222887039185
Validation loss: 2.0203372836112976

Epoch: 6| Step: 9
Training loss: 1.9847670793533325
Validation loss: 2.0166550477345786

Epoch: 6| Step: 10
Training loss: 2.74603271484375
Validation loss: 2.0135300755500793

Epoch: 6| Step: 11
Training loss: 2.0310018062591553
Validation loss: 2.018259882926941

Epoch: 6| Step: 12
Training loss: 2.504427671432495
Validation loss: 2.019757548967997

Epoch: 6| Step: 13
Training loss: 1.7323408126831055
Validation loss: 2.0315359433492026

Epoch: 116| Step: 0
Training loss: 2.378957748413086
Validation loss: 2.0235836108525596

Epoch: 6| Step: 1
Training loss: 3.237532377243042
Validation loss: 2.0304108262062073

Epoch: 6| Step: 2
Training loss: 2.482614040374756
Validation loss: 2.0260729789733887

Epoch: 6| Step: 3
Training loss: 2.0587897300720215
Validation loss: 2.0311975479125977

Epoch: 6| Step: 4
Training loss: 2.1385610103607178
Validation loss: 2.027778168519338

Epoch: 6| Step: 5
Training loss: 2.25616717338562
Validation loss: 2.0303689440091452

Epoch: 6| Step: 6
Training loss: 1.945278286933899
Validation loss: 2.030492603778839

Epoch: 6| Step: 7
Training loss: 1.9980928897857666
Validation loss: 2.0273417035738626

Epoch: 6| Step: 8
Training loss: 1.9928665161132812
Validation loss: 2.0305185516675315

Epoch: 6| Step: 9
Training loss: 1.8312406539916992
Validation loss: 2.0243895451227822

Epoch: 6| Step: 10
Training loss: 2.2619996070861816
Validation loss: 2.025145729382833

Epoch: 6| Step: 11
Training loss: 2.19049334526062
Validation loss: 2.0231358806292215

Epoch: 6| Step: 12
Training loss: 1.591333031654358
Validation loss: 2.013546327749888

Epoch: 6| Step: 13
Training loss: 1.695108413696289
Validation loss: 2.0131346782048545

Epoch: 117| Step: 0
Training loss: 2.1168181896209717
Validation loss: 2.0054179430007935

Epoch: 6| Step: 1
Training loss: 1.6709320545196533
Validation loss: 2.0060463746388755

Epoch: 6| Step: 2
Training loss: 1.8061206340789795
Validation loss: 2.0124535957972207

Epoch: 6| Step: 3
Training loss: 1.835120439529419
Validation loss: 2.0248950719833374

Epoch: 6| Step: 4
Training loss: 2.5145699977874756
Validation loss: 2.0392038027445474

Epoch: 6| Step: 5
Training loss: 2.4007070064544678
Validation loss: 2.024370769659678

Epoch: 6| Step: 6
Training loss: 2.1483676433563232
Validation loss: 2.036974549293518

Epoch: 6| Step: 7
Training loss: 2.302459239959717
Validation loss: 2.04084442059199

Epoch: 6| Step: 8
Training loss: 2.2784180641174316
Validation loss: 2.042043705781301

Epoch: 6| Step: 9
Training loss: 2.0396981239318848
Validation loss: 2.056975563367208

Epoch: 6| Step: 10
Training loss: 2.303781270980835
Validation loss: 2.049203554789225

Epoch: 6| Step: 11
Training loss: 1.8145734071731567
Validation loss: 2.047415097554525

Epoch: 6| Step: 12
Training loss: 2.1334168910980225
Validation loss: 2.0467138489087424

Epoch: 6| Step: 13
Training loss: 2.3821585178375244
Validation loss: 2.0426159699757895

Epoch: 118| Step: 0
Training loss: 2.0400657653808594
Validation loss: 2.04267817735672

Epoch: 6| Step: 1
Training loss: 1.9009668827056885
Validation loss: 2.0267035961151123

Epoch: 6| Step: 2
Training loss: 2.0469486713409424
Validation loss: 2.0286073287328086

Epoch: 6| Step: 3
Training loss: 2.055202007293701
Validation loss: 2.0276757081349692

Epoch: 6| Step: 4
Training loss: 1.8289766311645508
Validation loss: 2.023129860560099

Epoch: 6| Step: 5
Training loss: 2.5177156925201416
Validation loss: 2.019508401552836

Epoch: 6| Step: 6
Training loss: 2.4700634479522705
Validation loss: 2.018389642238617

Epoch: 6| Step: 7
Training loss: 2.1708059310913086
Validation loss: 2.017062246799469

Epoch: 6| Step: 8
Training loss: 2.390316963195801
Validation loss: 2.0219101707140603

Epoch: 6| Step: 9
Training loss: 1.8618801832199097
Validation loss: 2.0209186474482217

Epoch: 6| Step: 10
Training loss: 2.2154502868652344
Validation loss: 2.0299114187558494

Epoch: 6| Step: 11
Training loss: 2.265108823776245
Validation loss: 2.0343798398971558

Epoch: 6| Step: 12
Training loss: 2.144868850708008
Validation loss: 2.0269879897435508

Epoch: 6| Step: 13
Training loss: 1.7728707790374756
Validation loss: 2.027497092882792

Epoch: 119| Step: 0
Training loss: 2.6594557762145996
Validation loss: 2.035718818505605

Epoch: 6| Step: 1
Training loss: 2.0699892044067383
Validation loss: 2.038193921248118

Epoch: 6| Step: 2
Training loss: 1.6490164995193481
Validation loss: 2.0316251516342163

Epoch: 6| Step: 3
Training loss: 2.0521249771118164
Validation loss: 2.0311970710754395

Epoch: 6| Step: 4
Training loss: 2.1643168926239014
Validation loss: 2.027759611606598

Epoch: 6| Step: 5
Training loss: 2.6228795051574707
Validation loss: 2.032538096110026

Epoch: 6| Step: 6
Training loss: 2.0065228939056396
Validation loss: 2.0319291154543557

Epoch: 6| Step: 7
Training loss: 1.8031485080718994
Validation loss: 2.036170462767283

Epoch: 6| Step: 8
Training loss: 1.9995112419128418
Validation loss: 2.0396507581075034

Epoch: 6| Step: 9
Training loss: 1.4236297607421875
Validation loss: 2.049234688282013

Epoch: 6| Step: 10
Training loss: 2.4444069862365723
Validation loss: 2.0521744092305503

Epoch: 6| Step: 11
Training loss: 2.399324655532837
Validation loss: 2.047315756479899

Epoch: 6| Step: 12
Training loss: 2.06699800491333
Validation loss: 2.0650349060694375

Epoch: 6| Step: 13
Training loss: 2.1601192951202393
Validation loss: 2.059552808602651

Epoch: 120| Step: 0
Training loss: 2.398691415786743
Validation loss: 2.052324950695038

Epoch: 6| Step: 1
Training loss: 2.1653947830200195
Validation loss: 2.0605600078900657

Epoch: 6| Step: 2
Training loss: 2.417328119277954
Validation loss: 2.072678864002228

Epoch: 6| Step: 3
Training loss: 2.440788507461548
Validation loss: 2.0628497203191123

Epoch: 6| Step: 4
Training loss: 1.8939496278762817
Validation loss: 2.068567911783854

Epoch: 6| Step: 5
Training loss: 2.421052932739258
Validation loss: 2.0611451864242554

Epoch: 6| Step: 6
Training loss: 3.028045177459717
Validation loss: 2.045285622278849

Epoch: 6| Step: 7
Training loss: 1.7378641366958618
Validation loss: 2.0445394118626914

Epoch: 6| Step: 8
Training loss: 2.1184539794921875
Validation loss: 2.051380773385366

Epoch: 6| Step: 9
Training loss: 1.754474401473999
Validation loss: 2.037606398264567

Epoch: 6| Step: 10
Training loss: 1.5281000137329102
Validation loss: 2.0365558862686157

Epoch: 6| Step: 11
Training loss: 1.9331268072128296
Validation loss: 2.037854770819346

Epoch: 6| Step: 12
Training loss: 2.0861170291900635
Validation loss: 2.026421904563904

Epoch: 6| Step: 13
Training loss: 1.963510513305664
Validation loss: 2.0321688850720725

Epoch: 121| Step: 0
Training loss: 2.075373649597168
Validation loss: 2.0315881967544556

Epoch: 6| Step: 1
Training loss: 1.9645414352416992
Validation loss: 2.021808127562205

Epoch: 6| Step: 2
Training loss: 2.1133601665496826
Validation loss: 2.025879383087158

Epoch: 6| Step: 3
Training loss: 1.8857213258743286
Validation loss: 2.0221516489982605

Epoch: 6| Step: 4
Training loss: 1.991295576095581
Validation loss: 2.019864797592163

Epoch: 6| Step: 5
Training loss: 1.7607345581054688
Validation loss: 2.0191747347513833

Epoch: 6| Step: 6
Training loss: 2.199313163757324
Validation loss: 2.027090231577555

Epoch: 6| Step: 7
Training loss: 2.761350154876709
Validation loss: 2.016110976537069

Epoch: 6| Step: 8
Training loss: 2.618485689163208
Validation loss: 2.015262762705485

Epoch: 6| Step: 9
Training loss: 2.2191381454467773
Validation loss: 2.0170167088508606

Epoch: 6| Step: 10
Training loss: 2.3511416912078857
Validation loss: 2.0181588927904763

Epoch: 6| Step: 11
Training loss: 2.0158655643463135
Validation loss: 2.02292408545812

Epoch: 6| Step: 12
Training loss: 2.320614814758301
Validation loss: 2.0215049982070923

Epoch: 6| Step: 13
Training loss: 1.4698693752288818
Validation loss: 2.033579250176748

Epoch: 122| Step: 0
Training loss: 2.2949490547180176
Validation loss: 2.0330112973848977

Epoch: 6| Step: 1
Training loss: 1.7595574855804443
Validation loss: 2.0310383240381875

Epoch: 6| Step: 2
Training loss: 3.363433361053467
Validation loss: 2.0439595778783164

Epoch: 6| Step: 3
Training loss: 2.4525179862976074
Validation loss: 2.027777910232544

Epoch: 6| Step: 4
Training loss: 1.590543270111084
Validation loss: 2.0340646306673684

Epoch: 6| Step: 5
Training loss: 2.1462888717651367
Validation loss: 2.019634942213694

Epoch: 6| Step: 6
Training loss: 2.4735922813415527
Validation loss: 2.021500905354818

Epoch: 6| Step: 7
Training loss: 1.9985355138778687
Validation loss: 2.021616518497467

Epoch: 6| Step: 8
Training loss: 1.8061270713806152
Validation loss: 2.0275231997172036

Epoch: 6| Step: 9
Training loss: 2.3210501670837402
Validation loss: 2.0232622027397156

Epoch: 6| Step: 10
Training loss: 1.2927768230438232
Validation loss: 2.02153488000234

Epoch: 6| Step: 11
Training loss: 1.6352078914642334
Validation loss: 2.0226081013679504

Epoch: 6| Step: 12
Training loss: 2.189182996749878
Validation loss: 2.0187478065490723

Epoch: 6| Step: 13
Training loss: 2.3742427825927734
Validation loss: 2.02057017882665

Epoch: 123| Step: 0
Training loss: 2.0405654907226562
Validation loss: 2.022636830806732

Epoch: 6| Step: 1
Training loss: 2.1467113494873047
Validation loss: 2.0200339357058206

Epoch: 6| Step: 2
Training loss: 1.7803633213043213
Validation loss: 2.0114667216936746

Epoch: 6| Step: 3
Training loss: 2.0252819061279297
Validation loss: 2.018338759740194

Epoch: 6| Step: 4
Training loss: 2.2350172996520996
Validation loss: 2.0146180391311646

Epoch: 6| Step: 5
Training loss: 1.930879831314087
Validation loss: 2.0123232205708823

Epoch: 6| Step: 6
Training loss: 2.5590500831604004
Validation loss: 2.012650986512502

Epoch: 6| Step: 7
Training loss: 1.932015061378479
Validation loss: 2.006990969181061

Epoch: 6| Step: 8
Training loss: 2.337756633758545
Validation loss: 2.0109347701072693

Epoch: 6| Step: 9
Training loss: 2.2616608142852783
Validation loss: 2.012500762939453

Epoch: 6| Step: 10
Training loss: 1.9533612728118896
Validation loss: 2.0098185737927756

Epoch: 6| Step: 11
Training loss: 2.4964938163757324
Validation loss: 2.0161559184392295

Epoch: 6| Step: 12
Training loss: 1.8442224264144897
Validation loss: 2.0033907890319824

Epoch: 6| Step: 13
Training loss: 2.032148838043213
Validation loss: 2.0126449267069497

Epoch: 124| Step: 0
Training loss: 3.112917423248291
Validation loss: 2.015028258164724

Epoch: 6| Step: 1
Training loss: 1.8806729316711426
Validation loss: 2.0023157795270285

Epoch: 6| Step: 2
Training loss: 2.028301239013672
Validation loss: 2.0200467308362327

Epoch: 6| Step: 3
Training loss: 2.5467166900634766
Validation loss: 2.01828803618749

Epoch: 6| Step: 4
Training loss: 1.842840552330017
Validation loss: 2.016051709651947

Epoch: 6| Step: 5
Training loss: 2.939256191253662
Validation loss: 2.0153385003407798

Epoch: 6| Step: 6
Training loss: 2.3340394496917725
Validation loss: 2.0087319215138755

Epoch: 6| Step: 7
Training loss: 2.265995740890503
Validation loss: 2.007294515768687

Epoch: 6| Step: 8
Training loss: 1.8544905185699463
Validation loss: 2.0102391640345254

Epoch: 6| Step: 9
Training loss: 1.8409559726715088
Validation loss: 2.01278946797053

Epoch: 6| Step: 10
Training loss: 2.0102672576904297
Validation loss: 2.0047917564709983

Epoch: 6| Step: 11
Training loss: 1.4799262285232544
Validation loss: 2.013726770877838

Epoch: 6| Step: 12
Training loss: 1.9358614683151245
Validation loss: 2.0128928422927856

Epoch: 6| Step: 13
Training loss: 1.6694447994232178
Validation loss: 2.0126333038012185

Epoch: 125| Step: 0
Training loss: 1.7170510292053223
Validation loss: 2.0108331441879272

Epoch: 6| Step: 1
Training loss: 2.167534828186035
Validation loss: 2.022443413734436

Epoch: 6| Step: 2
Training loss: 2.480982542037964
Validation loss: 2.035094141960144

Epoch: 6| Step: 3
Training loss: 2.578272819519043
Validation loss: 2.0331676801045737

Epoch: 6| Step: 4
Training loss: 2.137735605239868
Validation loss: 2.0395188132921853

Epoch: 6| Step: 5
Training loss: 1.886026382446289
Validation loss: 2.0438156127929688

Epoch: 6| Step: 6
Training loss: 2.200087070465088
Validation loss: 2.0413931409517923

Epoch: 6| Step: 7
Training loss: 1.6774296760559082
Validation loss: 2.0280187726020813

Epoch: 6| Step: 8
Training loss: 2.3327503204345703
Validation loss: 2.0306219458580017

Epoch: 6| Step: 9
Training loss: 2.244109869003296
Validation loss: 2.0273255507151284

Epoch: 6| Step: 10
Training loss: 2.1262593269348145
Validation loss: 2.0309876998265586

Epoch: 6| Step: 11
Training loss: 2.562084197998047
Validation loss: 2.0225380460421243

Epoch: 6| Step: 12
Training loss: 2.0008318424224854
Validation loss: 2.0287795265515647

Epoch: 6| Step: 13
Training loss: 1.7034000158309937
Validation loss: 2.0280545155207315

Epoch: 126| Step: 0
Training loss: 2.828810214996338
Validation loss: 2.0222445130348206

Epoch: 6| Step: 1
Training loss: 1.9591188430786133
Validation loss: 2.0276947816212973

Epoch: 6| Step: 2
Training loss: 1.8343870639801025
Validation loss: 2.023375312487284

Epoch: 6| Step: 3
Training loss: 2.1374807357788086
Validation loss: 2.0231005748113

Epoch: 6| Step: 4
Training loss: 1.6105306148529053
Validation loss: 2.0156607230504355

Epoch: 6| Step: 5
Training loss: 1.3876817226409912
Validation loss: 2.028987387816111

Epoch: 6| Step: 6
Training loss: 2.378021717071533
Validation loss: 2.023764491081238

Epoch: 6| Step: 7
Training loss: 2.393101453781128
Validation loss: 2.017740329106649

Epoch: 6| Step: 8
Training loss: 2.190396785736084
Validation loss: 2.016946256160736

Epoch: 6| Step: 9
Training loss: 2.7689170837402344
Validation loss: 2.0315898855527244

Epoch: 6| Step: 10
Training loss: 2.155290365219116
Validation loss: 2.027687927087148

Epoch: 6| Step: 11
Training loss: 2.5922741889953613
Validation loss: 2.0345099767049155

Epoch: 6| Step: 12
Training loss: 1.7025463581085205
Validation loss: 2.020205040772756

Epoch: 6| Step: 13
Training loss: 1.4737865924835205
Validation loss: 2.0221033891042075

Epoch: 127| Step: 0
Training loss: 1.90241277217865
Validation loss: 2.0251294374465942

Epoch: 6| Step: 1
Training loss: 2.4104247093200684
Validation loss: 2.0207031766573587

Epoch: 6| Step: 2
Training loss: 2.3747048377990723
Validation loss: 2.0176045099894204

Epoch: 6| Step: 3
Training loss: 2.003690242767334
Validation loss: 2.0213892062505088

Epoch: 6| Step: 4
Training loss: 2.2476589679718018
Validation loss: 2.0238279898961387

Epoch: 6| Step: 5
Training loss: 2.477952480316162
Validation loss: 2.0214285254478455

Epoch: 6| Step: 6
Training loss: 1.9672751426696777
Validation loss: 2.0228492617607117

Epoch: 6| Step: 7
Training loss: 2.0125198364257812
Validation loss: 2.021160344282786

Epoch: 6| Step: 8
Training loss: 2.3552322387695312
Validation loss: 2.0175564885139465

Epoch: 6| Step: 9
Training loss: 2.1435928344726562
Validation loss: 2.011660635471344

Epoch: 6| Step: 10
Training loss: 1.5343208312988281
Validation loss: 2.017157773176829

Epoch: 6| Step: 11
Training loss: 2.191704750061035
Validation loss: 2.0241493384043374

Epoch: 6| Step: 12
Training loss: 1.958986520767212
Validation loss: 2.015824794769287

Epoch: 6| Step: 13
Training loss: 1.6587114334106445
Validation loss: 2.024155934651693

Epoch: 128| Step: 0
Training loss: 2.2601993083953857
Validation loss: 2.019355356693268

Epoch: 6| Step: 1
Training loss: 2.231779098510742
Validation loss: 2.0166110595067344

Epoch: 6| Step: 2
Training loss: 1.930823564529419
Validation loss: 2.0220821301142373

Epoch: 6| Step: 3
Training loss: 2.0805907249450684
Validation loss: 2.0251817305882773

Epoch: 6| Step: 4
Training loss: 2.0095725059509277
Validation loss: 2.025790512561798

Epoch: 6| Step: 5
Training loss: 2.1723010540008545
Validation loss: 2.013856073220571

Epoch: 6| Step: 6
Training loss: 2.1078593730926514
Validation loss: 2.012632886568705

Epoch: 6| Step: 7
Training loss: 1.8597105741500854
Validation loss: 2.014323890209198

Epoch: 6| Step: 8
Training loss: 2.299299478530884
Validation loss: 2.012525121370951

Epoch: 6| Step: 9
Training loss: 2.0228374004364014
Validation loss: 2.0067052642504373

Epoch: 6| Step: 10
Training loss: 1.2958621978759766
Validation loss: 2.015784422556559

Epoch: 6| Step: 11
Training loss: 2.556194543838501
Validation loss: 2.0105392734209695

Epoch: 6| Step: 12
Training loss: 2.327000856399536
Validation loss: 2.0190580685933432

Epoch: 6| Step: 13
Training loss: 2.111640453338623
Validation loss: 2.0207086205482483

Epoch: 129| Step: 0
Training loss: 2.7777395248413086
Validation loss: 2.0204677979151406

Epoch: 6| Step: 1
Training loss: 1.3883062601089478
Validation loss: 2.0382365385691323

Epoch: 6| Step: 2
Training loss: 2.1688573360443115
Validation loss: 2.0350841283798218

Epoch: 6| Step: 3
Training loss: 2.0117011070251465
Validation loss: 2.038905700047811

Epoch: 6| Step: 4
Training loss: 2.0123496055603027
Validation loss: 2.0298343499501548

Epoch: 6| Step: 5
Training loss: 1.9613258838653564
Validation loss: 2.037991682688395

Epoch: 6| Step: 6
Training loss: 2.1533188819885254
Validation loss: 2.032078425089518

Epoch: 6| Step: 7
Training loss: 2.1984152793884277
Validation loss: 2.031489908695221

Epoch: 6| Step: 8
Training loss: 2.00038480758667
Validation loss: 2.0215753515561423

Epoch: 6| Step: 9
Training loss: 2.0068342685699463
Validation loss: 2.0225553115208945

Epoch: 6| Step: 10
Training loss: 2.3650028705596924
Validation loss: 2.028626640637716

Epoch: 6| Step: 11
Training loss: 1.7303931713104248
Validation loss: 2.0242059230804443

Epoch: 6| Step: 12
Training loss: 2.281280517578125
Validation loss: 2.030583163102468

Epoch: 6| Step: 13
Training loss: 2.1189022064208984
Validation loss: 2.0329703291257224

Epoch: 130| Step: 0
Training loss: 1.797738790512085
Validation loss: 2.018264055252075

Epoch: 6| Step: 1
Training loss: 1.7001827955245972
Validation loss: 2.0213538805643716

Epoch: 6| Step: 2
Training loss: 2.8068771362304688
Validation loss: 2.0252725879351297

Epoch: 6| Step: 3
Training loss: 2.341555595397949
Validation loss: 2.0205411513646445

Epoch: 6| Step: 4
Training loss: 1.7423373460769653
Validation loss: 2.0183517932891846

Epoch: 6| Step: 5
Training loss: 1.8908287286758423
Validation loss: 2.018857260545095

Epoch: 6| Step: 6
Training loss: 2.2859995365142822
Validation loss: 2.0269848704338074

Epoch: 6| Step: 7
Training loss: 1.8194950819015503
Validation loss: 2.0281423330307007

Epoch: 6| Step: 8
Training loss: 1.921963095664978
Validation loss: 2.0198631286621094

Epoch: 6| Step: 9
Training loss: 2.9118804931640625
Validation loss: 2.026883920033773

Epoch: 6| Step: 10
Training loss: 1.7813161611557007
Validation loss: 2.0162482062975564

Epoch: 6| Step: 11
Training loss: 2.46512508392334
Validation loss: 2.030343234539032

Epoch: 6| Step: 12
Training loss: 1.721469521522522
Validation loss: 2.026777446269989

Epoch: 6| Step: 13
Training loss: 2.05464768409729
Validation loss: 2.0259332259496055

Epoch: 131| Step: 0
Training loss: 2.0690648555755615
Validation loss: 2.0290995836257935

Epoch: 6| Step: 1
Training loss: 2.04915714263916
Validation loss: 2.0405194958051047

Epoch: 6| Step: 2
Training loss: 2.318507194519043
Validation loss: 2.0390440026919046

Epoch: 6| Step: 3
Training loss: 2.3402152061462402
Validation loss: 2.034232238928477

Epoch: 6| Step: 4
Training loss: 2.208353281021118
Validation loss: 2.0354061126708984

Epoch: 6| Step: 5
Training loss: 1.7215137481689453
Validation loss: 2.0339325666427612

Epoch: 6| Step: 6
Training loss: 2.304825782775879
Validation loss: 2.0286407470703125

Epoch: 6| Step: 7
Training loss: 1.5655748844146729
Validation loss: 2.030487060546875

Epoch: 6| Step: 8
Training loss: 1.9467592239379883
Validation loss: 2.0292697151501975

Epoch: 6| Step: 9
Training loss: 2.05275297164917
Validation loss: 2.0256508191426597

Epoch: 6| Step: 10
Training loss: 2.3437414169311523
Validation loss: 2.0270535945892334

Epoch: 6| Step: 11
Training loss: 1.9242088794708252
Validation loss: 2.028720736503601

Epoch: 6| Step: 12
Training loss: 2.4108431339263916
Validation loss: 2.0234450300534568

Epoch: 6| Step: 13
Training loss: 1.9333438873291016
Validation loss: 2.019157032171885

Epoch: 132| Step: 0
Training loss: 2.0434465408325195
Validation loss: 2.0219648480415344

Epoch: 6| Step: 1
Training loss: 1.8987061977386475
Validation loss: 2.0170805056889853

Epoch: 6| Step: 2
Training loss: 2.348104953765869
Validation loss: 2.0209341049194336

Epoch: 6| Step: 3
Training loss: 1.6088753938674927
Validation loss: 2.023159086704254

Epoch: 6| Step: 4
Training loss: 2.030686855316162
Validation loss: 2.0243989626566568

Epoch: 6| Step: 5
Training loss: 2.2146236896514893
Validation loss: 2.026005129019419

Epoch: 6| Step: 6
Training loss: 2.5931386947631836
Validation loss: 2.0199641982714334

Epoch: 6| Step: 7
Training loss: 2.3528902530670166
Validation loss: 2.022939145565033

Epoch: 6| Step: 8
Training loss: 2.204878568649292
Validation loss: 2.022867580254873

Epoch: 6| Step: 9
Training loss: 1.998326063156128
Validation loss: 2.0174339612325034

Epoch: 6| Step: 10
Training loss: 1.6438064575195312
Validation loss: 2.0211864908536277

Epoch: 6| Step: 11
Training loss: 2.559539794921875
Validation loss: 2.0322731137275696

Epoch: 6| Step: 12
Training loss: 1.665113925933838
Validation loss: 2.0218998392422995

Epoch: 6| Step: 13
Training loss: 2.0207815170288086
Validation loss: 2.025973399480184

Epoch: 133| Step: 0
Training loss: 1.922607183456421
Validation loss: 2.0346121986707053

Epoch: 6| Step: 1
Training loss: 2.4997472763061523
Validation loss: 2.0280077854792276

Epoch: 6| Step: 2
Training loss: 1.669769048690796
Validation loss: 2.0434494614601135

Epoch: 6| Step: 3
Training loss: 2.3244237899780273
Validation loss: 2.027491887410482

Epoch: 6| Step: 4
Training loss: 2.1123619079589844
Validation loss: 2.0344173510869346

Epoch: 6| Step: 5
Training loss: 2.3620095252990723
Validation loss: 2.021439174811045

Epoch: 6| Step: 6
Training loss: 2.390718698501587
Validation loss: 2.0287140210469565

Epoch: 6| Step: 7
Training loss: 2.365760087966919
Validation loss: 2.0299707651138306

Epoch: 6| Step: 8
Training loss: 1.8785731792449951
Validation loss: 2.022960583368937

Epoch: 6| Step: 9
Training loss: 2.23187255859375
Validation loss: 2.021066188812256

Epoch: 6| Step: 10
Training loss: 1.6520843505859375
Validation loss: 2.016701062520345

Epoch: 6| Step: 11
Training loss: 1.7619627714157104
Validation loss: 2.0195358196894326

Epoch: 6| Step: 12
Training loss: 2.0631723403930664
Validation loss: 2.0223195552825928

Epoch: 6| Step: 13
Training loss: 1.9595298767089844
Validation loss: 2.0225283900896707

Epoch: 134| Step: 0
Training loss: 1.8619389533996582
Validation loss: 2.0242850383122764

Epoch: 6| Step: 1
Training loss: 1.8348617553710938
Validation loss: 2.0345107913017273

Epoch: 6| Step: 2
Training loss: 1.936631441116333
Validation loss: 2.0431572596232095

Epoch: 6| Step: 3
Training loss: 2.0512795448303223
Validation loss: 2.0335658192634583

Epoch: 6| Step: 4
Training loss: 1.3358687162399292
Validation loss: 2.0411229332288108

Epoch: 6| Step: 5
Training loss: 1.9155091047286987
Validation loss: 2.0691288908322654

Epoch: 6| Step: 6
Training loss: 2.7692103385925293
Validation loss: 2.0794764359792075

Epoch: 6| Step: 7
Training loss: 2.202779769897461
Validation loss: 2.0627777576446533

Epoch: 6| Step: 8
Training loss: 2.8558831214904785
Validation loss: 2.062091509501139

Epoch: 6| Step: 9
Training loss: 2.231541633605957
Validation loss: 2.0637333194414773

Epoch: 6| Step: 10
Training loss: 2.800154447555542
Validation loss: 2.052886883417765

Epoch: 6| Step: 11
Training loss: 2.3693878650665283
Validation loss: 2.0496971011161804

Epoch: 6| Step: 12
Training loss: 1.8790016174316406
Validation loss: 2.042695681254069

Epoch: 6| Step: 13
Training loss: 1.2628892660140991
Validation loss: 2.0384653210639954

Epoch: 135| Step: 0
Training loss: 2.099170684814453
Validation loss: 2.03808601697286

Epoch: 6| Step: 1
Training loss: 1.676286220550537
Validation loss: 2.02435302734375

Epoch: 6| Step: 2
Training loss: 1.6184043884277344
Validation loss: 2.0246010224024453

Epoch: 6| Step: 3
Training loss: 1.705476999282837
Validation loss: 2.020107169946035

Epoch: 6| Step: 4
Training loss: 2.0675411224365234
Validation loss: 2.019521435101827

Epoch: 6| Step: 5
Training loss: 2.4718284606933594
Validation loss: 2.018584966659546

Epoch: 6| Step: 6
Training loss: 2.461968421936035
Validation loss: 2.0233811140060425

Epoch: 6| Step: 7
Training loss: 1.7730062007904053
Validation loss: 2.022196968396505

Epoch: 6| Step: 8
Training loss: 2.14479398727417
Validation loss: 2.020721515019735

Epoch: 6| Step: 9
Training loss: 2.3783621788024902
Validation loss: 2.0288114746411643

Epoch: 6| Step: 10
Training loss: 1.9446029663085938
Validation loss: 2.028427998224894

Epoch: 6| Step: 11
Training loss: 2.385467052459717
Validation loss: 2.0210404992103577

Epoch: 6| Step: 12
Training loss: 2.2059385776519775
Validation loss: 2.02859236796697

Epoch: 6| Step: 13
Training loss: 2.4165542125701904
Validation loss: 2.0199407736460366

Epoch: 136| Step: 0
Training loss: 2.556323289871216
Validation loss: 2.024708171685537

Epoch: 6| Step: 1
Training loss: 1.8567540645599365
Validation loss: 2.0247041980425515

Epoch: 6| Step: 2
Training loss: 1.5807313919067383
Validation loss: 2.022883574167887

Epoch: 6| Step: 3
Training loss: 2.637209177017212
Validation loss: 2.0260446270306907

Epoch: 6| Step: 4
Training loss: 1.6029446125030518
Validation loss: 2.029688080151876

Epoch: 6| Step: 5
Training loss: 1.8990272283554077
Validation loss: 2.0163620710372925

Epoch: 6| Step: 6
Training loss: 2.1583731174468994
Validation loss: 2.0198296507199607

Epoch: 6| Step: 7
Training loss: 2.0145294666290283
Validation loss: 2.0293659567832947

Epoch: 6| Step: 8
Training loss: 1.7116179466247559
Validation loss: 2.0224469105402627

Epoch: 6| Step: 9
Training loss: 1.9874234199523926
Validation loss: 2.026624083518982

Epoch: 6| Step: 10
Training loss: 2.1433773040771484
Validation loss: 2.029328942298889

Epoch: 6| Step: 11
Training loss: 2.191779613494873
Validation loss: 2.033379932244619

Epoch: 6| Step: 12
Training loss: 2.377229690551758
Validation loss: 2.029950499534607

Epoch: 6| Step: 13
Training loss: 2.4605422019958496
Validation loss: 2.0239134430885315

Epoch: 137| Step: 0
Training loss: 2.2791452407836914
Validation loss: 2.031276822090149

Epoch: 6| Step: 1
Training loss: 2.466419219970703
Validation loss: 2.0355397860209146

Epoch: 6| Step: 2
Training loss: 1.3820480108261108
Validation loss: 2.0259496370951333

Epoch: 6| Step: 3
Training loss: 2.6409430503845215
Validation loss: 2.0381900866826377

Epoch: 6| Step: 4
Training loss: 1.9430983066558838
Validation loss: 2.033822536468506

Epoch: 6| Step: 5
Training loss: 2.5601186752319336
Validation loss: 2.047000507513682

Epoch: 6| Step: 6
Training loss: 2.175811767578125
Validation loss: 2.0588497519493103

Epoch: 6| Step: 7
Training loss: 1.6216975450515747
Validation loss: 2.044447422027588

Epoch: 6| Step: 8
Training loss: 2.073854923248291
Validation loss: 2.0516815980275473

Epoch: 6| Step: 9
Training loss: 1.9384074211120605
Validation loss: 2.03373654683431

Epoch: 6| Step: 10
Training loss: 2.0769612789154053
Validation loss: 2.046228845914205

Epoch: 6| Step: 11
Training loss: 2.0253102779388428
Validation loss: 2.0520041386286416

Epoch: 6| Step: 12
Training loss: 1.7979538440704346
Validation loss: 2.064925193786621

Epoch: 6| Step: 13
Training loss: 2.0404419898986816
Validation loss: 2.059205114841461

Epoch: 138| Step: 0
Training loss: 2.3831565380096436
Validation loss: 2.056848406791687

Epoch: 6| Step: 1
Training loss: 1.7865350246429443
Validation loss: 2.045737365881602

Epoch: 6| Step: 2
Training loss: 2.1052279472351074
Validation loss: 2.025041937828064

Epoch: 6| Step: 3
Training loss: 1.9380618333816528
Validation loss: 2.0442498326301575

Epoch: 6| Step: 4
Training loss: 1.975617527961731
Validation loss: 2.034272531668345

Epoch: 6| Step: 5
Training loss: 2.3379874229431152
Validation loss: 2.023494323094686

Epoch: 6| Step: 6
Training loss: 2.2967300415039062
Validation loss: 2.0231073101361594

Epoch: 6| Step: 7
Training loss: 2.1863906383514404
Validation loss: 2.029842952887217

Epoch: 6| Step: 8
Training loss: 1.6700499057769775
Validation loss: 2.0237067341804504

Epoch: 6| Step: 9
Training loss: 1.8237988948822021
Validation loss: 2.0264224807421365

Epoch: 6| Step: 10
Training loss: 1.8044160604476929
Validation loss: 2.0252795020739236

Epoch: 6| Step: 11
Training loss: 2.558316946029663
Validation loss: 2.036518096923828

Epoch: 6| Step: 12
Training loss: 1.9410436153411865
Validation loss: 2.0354692339897156

Epoch: 6| Step: 13
Training loss: 2.3976283073425293
Validation loss: 2.0448740124702454

Epoch: 139| Step: 0
Training loss: 2.5677785873413086
Validation loss: 2.042622963587443

Epoch: 6| Step: 1
Training loss: 1.536529302597046
Validation loss: 2.058350423971812

Epoch: 6| Step: 2
Training loss: 2.2186636924743652
Validation loss: 2.0560404658317566

Epoch: 6| Step: 3
Training loss: 1.9612605571746826
Validation loss: 2.0602943897247314

Epoch: 6| Step: 4
Training loss: 1.6764073371887207
Validation loss: 2.040658871332804

Epoch: 6| Step: 5
Training loss: 2.2257320880889893
Validation loss: 2.0478068788846335

Epoch: 6| Step: 6
Training loss: 2.1203150749206543
Validation loss: 2.030135671297709

Epoch: 6| Step: 7
Training loss: 1.3807165622711182
Validation loss: 2.0305365721384683

Epoch: 6| Step: 8
Training loss: 2.149984836578369
Validation loss: 2.034421463807424

Epoch: 6| Step: 9
Training loss: 2.6555113792419434
Validation loss: 2.0270943641662598

Epoch: 6| Step: 10
Training loss: 2.209440231323242
Validation loss: 2.026469071706136

Epoch: 6| Step: 11
Training loss: 1.9232239723205566
Validation loss: 2.034433980782827

Epoch: 6| Step: 12
Training loss: 2.245070457458496
Validation loss: 2.0332733392715454

Epoch: 6| Step: 13
Training loss: 2.1198906898498535
Validation loss: 2.036637624104818

Epoch: 140| Step: 0
Training loss: 2.3160340785980225
Validation loss: 2.0341293017069497

Epoch: 6| Step: 1
Training loss: 2.5777831077575684
Validation loss: 2.049767871697744

Epoch: 6| Step: 2
Training loss: 1.9876251220703125
Validation loss: 2.0428017576535544

Epoch: 6| Step: 3
Training loss: 1.5353519916534424
Validation loss: 2.057145039240519

Epoch: 6| Step: 4
Training loss: 2.2485947608947754
Validation loss: 2.0493070880572

Epoch: 6| Step: 5
Training loss: 2.260150671005249
Validation loss: 2.0538392265637717

Epoch: 6| Step: 6
Training loss: 1.887789249420166
Validation loss: 2.050694783528646

Epoch: 6| Step: 7
Training loss: 2.2444374561309814
Validation loss: 2.0516889492670694

Epoch: 6| Step: 8
Training loss: 1.6896140575408936
Validation loss: 2.046844998995463

Epoch: 6| Step: 9
Training loss: 2.2980613708496094
Validation loss: 2.04888786872228

Epoch: 6| Step: 10
Training loss: 1.741001844406128
Validation loss: 2.052375098069509

Epoch: 6| Step: 11
Training loss: 2.0063657760620117
Validation loss: 2.0544137557347617

Epoch: 6| Step: 12
Training loss: 1.956641435623169
Validation loss: 2.0560757716496787

Epoch: 6| Step: 13
Training loss: 2.052386999130249
Validation loss: 2.059893091519674

Epoch: 141| Step: 0
Training loss: 1.8148548603057861
Validation loss: 2.0458149115244546

Epoch: 6| Step: 1
Training loss: 1.9327895641326904
Validation loss: 2.0427294174830117

Epoch: 6| Step: 2
Training loss: 2.1623780727386475
Validation loss: 2.053752859433492

Epoch: 6| Step: 3
Training loss: 2.2087230682373047
Validation loss: 2.0548561811447144

Epoch: 6| Step: 4
Training loss: 1.5801045894622803
Validation loss: 2.051590085029602

Epoch: 6| Step: 5
Training loss: 1.9919373989105225
Validation loss: 2.051942686239878

Epoch: 6| Step: 6
Training loss: 1.600437879562378
Validation loss: 2.072712699572245

Epoch: 6| Step: 7
Training loss: 1.9193443059921265
Validation loss: 2.070110102494558

Epoch: 6| Step: 8
Training loss: 1.7002657651901245
Validation loss: 2.069938282171885

Epoch: 6| Step: 9
Training loss: 2.3726062774658203
Validation loss: 2.060649355252584

Epoch: 6| Step: 10
Training loss: 2.5766849517822266
Validation loss: 2.056913514931997

Epoch: 6| Step: 11
Training loss: 2.447819232940674
Validation loss: 2.057073970635732

Epoch: 6| Step: 12
Training loss: 2.351175308227539
Validation loss: 2.0458194812138877

Epoch: 6| Step: 13
Training loss: 2.145299196243286
Validation loss: 2.044470270474752

Epoch: 142| Step: 0
Training loss: 2.2886223793029785
Validation loss: 2.0493878722190857

Epoch: 6| Step: 1
Training loss: 2.655579090118408
Validation loss: 2.0430837869644165

Epoch: 6| Step: 2
Training loss: 1.9883122444152832
Validation loss: 2.0346102118492126

Epoch: 6| Step: 3
Training loss: 1.8372001647949219
Validation loss: 2.0324499805768332

Epoch: 6| Step: 4
Training loss: 2.0830211639404297
Validation loss: 2.040231982866923

Epoch: 6| Step: 5
Training loss: 1.794126272201538
Validation loss: 2.039031227429708

Epoch: 6| Step: 6
Training loss: 2.209459066390991
Validation loss: 2.0335861444473267

Epoch: 6| Step: 7
Training loss: 1.842341423034668
Validation loss: 2.047248204549154

Epoch: 6| Step: 8
Training loss: 1.8207647800445557
Validation loss: 2.0556673606236777

Epoch: 6| Step: 9
Training loss: 1.8029688596725464
Validation loss: 2.057360132535299

Epoch: 6| Step: 10
Training loss: 1.9748622179031372
Validation loss: 2.0570805072784424

Epoch: 6| Step: 11
Training loss: 1.8659008741378784
Validation loss: 2.0513543685277305

Epoch: 6| Step: 12
Training loss: 1.8937783241271973
Validation loss: 2.0477354327837625

Epoch: 6| Step: 13
Training loss: 2.5461432933807373
Validation loss: 2.040434996287028

Epoch: 143| Step: 0
Training loss: 2.184541702270508
Validation loss: 2.028753181298574

Epoch: 6| Step: 1
Training loss: 1.9150983095169067
Validation loss: 2.0314205288887024

Epoch: 6| Step: 2
Training loss: 2.4154181480407715
Validation loss: 2.0324549078941345

Epoch: 6| Step: 3
Training loss: 1.5190308094024658
Validation loss: 2.0310062964757285

Epoch: 6| Step: 4
Training loss: 2.528738021850586
Validation loss: 2.0332060058911643

Epoch: 6| Step: 5
Training loss: 1.792095422744751
Validation loss: 2.0311026374499

Epoch: 6| Step: 6
Training loss: 2.2980661392211914
Validation loss: 2.038662016391754

Epoch: 6| Step: 7
Training loss: 1.910664677619934
Validation loss: 2.038934648036957

Epoch: 6| Step: 8
Training loss: 2.1498656272888184
Validation loss: 2.036649763584137

Epoch: 6| Step: 9
Training loss: 2.227649450302124
Validation loss: 2.0381423036257424

Epoch: 6| Step: 10
Training loss: 2.5228428840637207
Validation loss: 2.032525420188904

Epoch: 6| Step: 11
Training loss: 1.9679759740829468
Validation loss: 2.0280809799830117

Epoch: 6| Step: 12
Training loss: 2.246066093444824
Validation loss: 2.033709168434143

Epoch: 6| Step: 13
Training loss: 1.811206340789795
Validation loss: 2.0247126619021096

Epoch: 144| Step: 0
Training loss: 2.268944263458252
Validation loss: 2.0368383328119912

Epoch: 6| Step: 1
Training loss: 2.082883596420288
Validation loss: 2.043022871017456

Epoch: 6| Step: 2
Training loss: 2.080878257751465
Validation loss: 2.036262492338816

Epoch: 6| Step: 3
Training loss: 2.6927411556243896
Validation loss: 2.0376677910486856

Epoch: 6| Step: 4
Training loss: 2.1694886684417725
Validation loss: 2.0459540685017905

Epoch: 6| Step: 5
Training loss: 1.677768349647522
Validation loss: 2.0506465435028076

Epoch: 6| Step: 6
Training loss: 2.2070915699005127
Validation loss: 2.0555965105692544

Epoch: 6| Step: 7
Training loss: 2.0724105834960938
Validation loss: 2.0709940592447915

Epoch: 6| Step: 8
Training loss: 1.5786895751953125
Validation loss: 2.0740551551183066

Epoch: 6| Step: 9
Training loss: 1.6661654710769653
Validation loss: 2.0685683687527976

Epoch: 6| Step: 10
Training loss: 1.6609930992126465
Validation loss: 2.0611030062039695

Epoch: 6| Step: 11
Training loss: 2.1583213806152344
Validation loss: 2.067261974016825

Epoch: 6| Step: 12
Training loss: 2.0917320251464844
Validation loss: 2.081620196501414

Epoch: 6| Step: 13
Training loss: 2.334345817565918
Validation loss: 2.0614266792933145

Epoch: 145| Step: 0
Training loss: 1.6132793426513672
Validation loss: 2.0502739350001016

Epoch: 6| Step: 1
Training loss: 1.7552733421325684
Validation loss: 2.052202264467875

Epoch: 6| Step: 2
Training loss: 1.7892224788665771
Validation loss: 2.0544522603352866

Epoch: 6| Step: 3
Training loss: 1.6219910383224487
Validation loss: 2.049071749051412

Epoch: 6| Step: 4
Training loss: 2.9900074005126953
Validation loss: 2.0489105780919394

Epoch: 6| Step: 5
Training loss: 1.2298879623413086
Validation loss: 2.0416375398635864

Epoch: 6| Step: 6
Training loss: 2.6200177669525146
Validation loss: 2.0451348225275674

Epoch: 6| Step: 7
Training loss: 1.7205851078033447
Validation loss: 2.0362634857495627

Epoch: 6| Step: 8
Training loss: 1.948331356048584
Validation loss: 2.04507182041804

Epoch: 6| Step: 9
Training loss: 2.4402310848236084
Validation loss: 2.0506131052970886

Epoch: 6| Step: 10
Training loss: 2.886021852493286
Validation loss: 2.041649123032888

Epoch: 6| Step: 11
Training loss: 1.6687068939208984
Validation loss: 2.048405488332113

Epoch: 6| Step: 12
Training loss: 2.348461627960205
Validation loss: 2.055584808190664

Epoch: 6| Step: 13
Training loss: 2.057785987854004
Validation loss: 2.057302474975586

Epoch: 146| Step: 0
Training loss: 1.9081618785858154
Validation loss: 2.048680285612742

Epoch: 6| Step: 1
Training loss: 2.5189828872680664
Validation loss: 2.0552066365877786

Epoch: 6| Step: 2
Training loss: 1.8288190364837646
Validation loss: 2.045786201953888

Epoch: 6| Step: 3
Training loss: 1.7710020542144775
Validation loss: 2.045352796713511

Epoch: 6| Step: 4
Training loss: 2.365055561065674
Validation loss: 2.0479063789049783

Epoch: 6| Step: 5
Training loss: 2.150467872619629
Validation loss: 2.0427169601122537

Epoch: 6| Step: 6
Training loss: 2.3339648246765137
Validation loss: 2.0331695278485618

Epoch: 6| Step: 7
Training loss: 1.811439871788025
Validation loss: 2.047213315963745

Epoch: 6| Step: 8
Training loss: 1.813912034034729
Validation loss: 2.0412920117378235

Epoch: 6| Step: 9
Training loss: 2.0560832023620605
Validation loss: 2.0411014556884766

Epoch: 6| Step: 10
Training loss: 2.087169647216797
Validation loss: 2.0551328460375466

Epoch: 6| Step: 11
Training loss: 2.2296013832092285
Validation loss: 2.0493940114974976

Epoch: 6| Step: 12
Training loss: 2.2547452449798584
Validation loss: 2.0510021646817527

Epoch: 6| Step: 13
Training loss: 1.5282196998596191
Validation loss: 2.053713579972585

Epoch: 147| Step: 0
Training loss: 1.4160363674163818
Validation loss: 2.0506455302238464

Epoch: 6| Step: 1
Training loss: 2.278557300567627
Validation loss: 2.0565762321154275

Epoch: 6| Step: 2
Training loss: 2.0685181617736816
Validation loss: 2.0522345105806985

Epoch: 6| Step: 3
Training loss: 1.9062232971191406
Validation loss: 2.064252515633901

Epoch: 6| Step: 4
Training loss: 1.6469447612762451
Validation loss: 2.0564207633336387

Epoch: 6| Step: 5
Training loss: 1.967384934425354
Validation loss: 2.0576228499412537

Epoch: 6| Step: 6
Training loss: 1.7892735004425049
Validation loss: 2.0519250631332397

Epoch: 6| Step: 7
Training loss: 2.012420892715454
Validation loss: 2.04985111951828

Epoch: 6| Step: 8
Training loss: 2.0240869522094727
Validation loss: 2.058868130048116

Epoch: 6| Step: 9
Training loss: 3.1459031105041504
Validation loss: 2.0537060697873435

Epoch: 6| Step: 10
Training loss: 2.0513548851013184
Validation loss: 2.0659484465916953

Epoch: 6| Step: 11
Training loss: 1.959381341934204
Validation loss: 2.0603272914886475

Epoch: 6| Step: 12
Training loss: 2.281181812286377
Validation loss: 2.0629560550053916

Epoch: 6| Step: 13
Training loss: 2.0465469360351562
Validation loss: 2.069007615248362

Epoch: 148| Step: 0
Training loss: 2.1813066005706787
Validation loss: 2.055178225040436

Epoch: 6| Step: 1
Training loss: 2.214751958847046
Validation loss: 2.0589491526285806

Epoch: 6| Step: 2
Training loss: 2.048596143722534
Validation loss: 2.052099625269572

Epoch: 6| Step: 3
Training loss: 1.7652288675308228
Validation loss: 2.0431725780169168

Epoch: 6| Step: 4
Training loss: 1.745154857635498
Validation loss: 2.046296000480652

Epoch: 6| Step: 5
Training loss: 2.586886405944824
Validation loss: 2.027112444241842

Epoch: 6| Step: 6
Training loss: 1.7285759449005127
Validation loss: 2.039659003416697

Epoch: 6| Step: 7
Training loss: 1.6025196313858032
Validation loss: 2.035259425640106

Epoch: 6| Step: 8
Training loss: 2.6744649410247803
Validation loss: 2.0321110685666404

Epoch: 6| Step: 9
Training loss: 1.8733270168304443
Validation loss: 2.0273898243904114

Epoch: 6| Step: 10
Training loss: 1.9696245193481445
Validation loss: 2.0335766474405923

Epoch: 6| Step: 11
Training loss: 2.0818252563476562
Validation loss: 2.0335676074028015

Epoch: 6| Step: 12
Training loss: 2.5721628665924072
Validation loss: 2.0354576110839844

Epoch: 6| Step: 13
Training loss: 2.184997797012329
Validation loss: 2.0428804556528726

Epoch: 149| Step: 0
Training loss: 2.162827968597412
Validation loss: 2.0465701619784036

Epoch: 6| Step: 1
Training loss: 1.9899733066558838
Validation loss: 2.036122957865397

Epoch: 6| Step: 2
Training loss: 1.5933191776275635
Validation loss: 2.04514350493749

Epoch: 6| Step: 3
Training loss: 2.1769614219665527
Validation loss: 2.0518545111020408

Epoch: 6| Step: 4
Training loss: 2.3564329147338867
Validation loss: 2.0489274064699807

Epoch: 6| Step: 5
Training loss: 1.6201701164245605
Validation loss: 2.05270254611969

Epoch: 6| Step: 6
Training loss: 2.4145209789276123
Validation loss: 2.0587501724561057

Epoch: 6| Step: 7
Training loss: 2.5211374759674072
Validation loss: 2.05952121814092

Epoch: 6| Step: 8
Training loss: 1.6145262718200684
Validation loss: 2.060555100440979

Epoch: 6| Step: 9
Training loss: 2.3818516731262207
Validation loss: 2.045052488644918

Epoch: 6| Step: 10
Training loss: 1.8026525974273682
Validation loss: 2.0576029419898987

Epoch: 6| Step: 11
Training loss: 2.370020866394043
Validation loss: 2.044976532459259

Epoch: 6| Step: 12
Training loss: 1.5734249353408813
Validation loss: 2.05023847023646

Epoch: 6| Step: 13
Training loss: 2.4323441982269287
Validation loss: 2.0391459862391152

Epoch: 150| Step: 0
Training loss: 1.538479208946228
Validation loss: 2.0434483687082925

Epoch: 6| Step: 1
Training loss: 1.8677676916122437
Validation loss: 2.043497323989868

Epoch: 6| Step: 2
Training loss: 1.663344144821167
Validation loss: 2.045146882534027

Epoch: 6| Step: 3
Training loss: 2.497257947921753
Validation loss: 2.050232489903768

Epoch: 6| Step: 4
Training loss: 2.313709259033203
Validation loss: 2.054032544294993

Epoch: 6| Step: 5
Training loss: 2.2750611305236816
Validation loss: 2.0506986578305564

Epoch: 6| Step: 6
Training loss: 2.6769022941589355
Validation loss: 2.054183522860209

Epoch: 6| Step: 7
Training loss: 2.1664717197418213
Validation loss: 2.0495436787605286

Epoch: 6| Step: 8
Training loss: 2.489318370819092
Validation loss: 2.0557907621065774

Epoch: 6| Step: 9
Training loss: 2.0248780250549316
Validation loss: 2.0500346223513284

Epoch: 6| Step: 10
Training loss: 2.1283066272735596
Validation loss: 2.0555781523386636

Epoch: 6| Step: 11
Training loss: 1.3689557313919067
Validation loss: 2.060349464416504

Epoch: 6| Step: 12
Training loss: 1.8093516826629639
Validation loss: 2.05237485965093

Epoch: 6| Step: 13
Training loss: 1.541751742362976
Validation loss: 2.054333964983622

Epoch: 151| Step: 0
Training loss: 1.8686350584030151
Validation loss: 2.0687737266222634

Epoch: 6| Step: 1
Training loss: 2.1677675247192383
Validation loss: 2.0776043931643167

Epoch: 6| Step: 2
Training loss: 1.7372088432312012
Validation loss: 2.0750622948010764

Epoch: 6| Step: 3
Training loss: 1.5763834714889526
Validation loss: 2.074307600657145

Epoch: 6| Step: 4
Training loss: 1.759261965751648
Validation loss: 2.071106215318044

Epoch: 6| Step: 5
Training loss: 2.668053388595581
Validation loss: 2.0578096906344094

Epoch: 6| Step: 6
Training loss: 2.2297327518463135
Validation loss: 2.0545523961385093

Epoch: 6| Step: 7
Training loss: 2.0464839935302734
Validation loss: 2.0317048033078513

Epoch: 6| Step: 8
Training loss: 1.6723300218582153
Validation loss: 2.042662044366201

Epoch: 6| Step: 9
Training loss: 2.1034626960754395
Validation loss: 2.0371576944986978

Epoch: 6| Step: 10
Training loss: 1.822953462600708
Validation loss: 2.035533607006073

Epoch: 6| Step: 11
Training loss: 2.6119236946105957
Validation loss: 2.0346789956092834

Epoch: 6| Step: 12
Training loss: 2.0313615798950195
Validation loss: 2.0245431462923684

Epoch: 6| Step: 13
Training loss: 2.4699244499206543
Validation loss: 2.021917462348938

Epoch: 152| Step: 0
Training loss: 2.160055160522461
Validation loss: 2.0271981358528137

Epoch: 6| Step: 1
Training loss: 2.3380227088928223
Validation loss: 2.032451113065084

Epoch: 6| Step: 2
Training loss: 1.832515835762024
Validation loss: 2.024123509724935

Epoch: 6| Step: 3
Training loss: 2.3602967262268066
Validation loss: 2.0273439089457193

Epoch: 6| Step: 4
Training loss: 2.24442195892334
Validation loss: 2.0130465825398765

Epoch: 6| Step: 5
Training loss: 2.490715503692627
Validation loss: 2.011588374773661

Epoch: 6| Step: 6
Training loss: 1.4240225553512573
Validation loss: 2.011002163092295

Epoch: 6| Step: 7
Training loss: 1.5619312524795532
Validation loss: 2.014668126900991

Epoch: 6| Step: 8
Training loss: 2.177180051803589
Validation loss: 2.007007340590159

Epoch: 6| Step: 9
Training loss: 2.2596359252929688
Validation loss: 2.0222520232200623

Epoch: 6| Step: 10
Training loss: 1.984886646270752
Validation loss: 2.013464570045471

Epoch: 6| Step: 11
Training loss: 2.6456751823425293
Validation loss: 2.017792224884033

Epoch: 6| Step: 12
Training loss: 2.337646484375
Validation loss: 2.0224014719327292

Epoch: 6| Step: 13
Training loss: 1.2707445621490479
Validation loss: 2.023659348487854

Epoch: 153| Step: 0
Training loss: 2.167536735534668
Validation loss: 2.0309597055117288

Epoch: 6| Step: 1
Training loss: 2.472726821899414
Validation loss: 2.047425866127014

Epoch: 6| Step: 2
Training loss: 1.5497088432312012
Validation loss: 2.043904721736908

Epoch: 6| Step: 3
Training loss: 3.1982345581054688
Validation loss: 2.0521166920661926

Epoch: 6| Step: 4
Training loss: 2.0369021892547607
Validation loss: 2.048620561758677

Epoch: 6| Step: 5
Training loss: 1.9477654695510864
Validation loss: 2.030242701371511

Epoch: 6| Step: 6
Training loss: 1.6286956071853638
Validation loss: 2.0255130330721536

Epoch: 6| Step: 7
Training loss: 1.650240182876587
Validation loss: 2.0300293366114297

Epoch: 6| Step: 8
Training loss: 2.3050758838653564
Validation loss: 2.029172718524933

Epoch: 6| Step: 9
Training loss: 2.0769309997558594
Validation loss: 2.0270678798357644

Epoch: 6| Step: 10
Training loss: 2.0975770950317383
Validation loss: 2.02623842159907

Epoch: 6| Step: 11
Training loss: 1.971837043762207
Validation loss: 2.0318861603736877

Epoch: 6| Step: 12
Training loss: 1.8241968154907227
Validation loss: 2.0334488352139792

Epoch: 6| Step: 13
Training loss: 2.21195125579834
Validation loss: 2.032464543978373

Epoch: 154| Step: 0
Training loss: 1.4261335134506226
Validation loss: 2.0456611116727195

Epoch: 6| Step: 1
Training loss: 1.5841437578201294
Validation loss: 2.040192405382792

Epoch: 6| Step: 2
Training loss: 2.6010093688964844
Validation loss: 2.0460496743520102

Epoch: 6| Step: 3
Training loss: 2.426224708557129
Validation loss: 2.0519557197888694

Epoch: 6| Step: 4
Training loss: 2.0404860973358154
Validation loss: 2.0477861364682517

Epoch: 6| Step: 5
Training loss: 1.9365668296813965
Validation loss: 2.05217436949412

Epoch: 6| Step: 6
Training loss: 2.051027297973633
Validation loss: 2.0481691559155784

Epoch: 6| Step: 7
Training loss: 2.1460013389587402
Validation loss: 2.069102466106415

Epoch: 6| Step: 8
Training loss: 2.289409875869751
Validation loss: 2.067553222179413

Epoch: 6| Step: 9
Training loss: 2.0227291584014893
Validation loss: 2.074354072411855

Epoch: 6| Step: 10
Training loss: 1.8908393383026123
Validation loss: 2.074077089627584

Epoch: 6| Step: 11
Training loss: 2.0157604217529297
Validation loss: 2.0741527676582336

Epoch: 6| Step: 12
Training loss: 1.7088050842285156
Validation loss: 2.071470042069753

Epoch: 6| Step: 13
Training loss: 2.497617483139038
Validation loss: 2.0735668738683066

Epoch: 155| Step: 0
Training loss: 1.7112021446228027
Validation loss: 2.0648189584414163

Epoch: 6| Step: 1
Training loss: 1.9603501558303833
Validation loss: 2.0597270925839744

Epoch: 6| Step: 2
Training loss: 2.3316664695739746
Validation loss: 2.0579686363538108

Epoch: 6| Step: 3
Training loss: 2.52639102935791
Validation loss: 2.0621511538823447

Epoch: 6| Step: 4
Training loss: 2.8034815788269043
Validation loss: 2.052876909573873

Epoch: 6| Step: 5
Training loss: 1.990427017211914
Validation loss: 2.054679274559021

Epoch: 6| Step: 6
Training loss: 2.0025672912597656
Validation loss: 2.066664139429728

Epoch: 6| Step: 7
Training loss: 1.7516553401947021
Validation loss: 2.0597826838493347

Epoch: 6| Step: 8
Training loss: 1.5595505237579346
Validation loss: 2.075980265935262

Epoch: 6| Step: 9
Training loss: 2.071028470993042
Validation loss: 2.083298683166504

Epoch: 6| Step: 10
Training loss: 1.8224492073059082
Validation loss: 2.0763895908991494

Epoch: 6| Step: 11
Training loss: 1.6752619743347168
Validation loss: 2.084124525388082

Epoch: 6| Step: 12
Training loss: 2.1874802112579346
Validation loss: 2.0864813725153604

Epoch: 6| Step: 13
Training loss: 2.3026907444000244
Validation loss: 2.0857744216918945

Epoch: 156| Step: 0
Training loss: 1.718092441558838
Validation loss: 2.0585384567578635

Epoch: 6| Step: 1
Training loss: 1.7113113403320312
Validation loss: 2.0512158075968423

Epoch: 6| Step: 2
Training loss: 2.1244254112243652
Validation loss: 2.0451683600743613

Epoch: 6| Step: 3
Training loss: 1.9237964153289795
Validation loss: 2.0387185414632163

Epoch: 6| Step: 4
Training loss: 2.23513126373291
Validation loss: 2.032146612803141

Epoch: 6| Step: 5
Training loss: 2.423398971557617
Validation loss: 2.0367885629336038

Epoch: 6| Step: 6
Training loss: 2.0455851554870605
Validation loss: 2.0259822010993958

Epoch: 6| Step: 7
Training loss: 2.5998878479003906
Validation loss: 2.0319674809773765

Epoch: 6| Step: 8
Training loss: 2.17803692817688
Validation loss: 2.0378562013308206

Epoch: 6| Step: 9
Training loss: 1.477457880973816
Validation loss: 2.032488842805227

Epoch: 6| Step: 10
Training loss: 1.5910873413085938
Validation loss: 2.034240504105886

Epoch: 6| Step: 11
Training loss: 2.4312405586242676
Validation loss: 2.0450334548950195

Epoch: 6| Step: 12
Training loss: 2.6682615280151367
Validation loss: 2.045277496178945

Epoch: 6| Step: 13
Training loss: 1.565397024154663
Validation loss: 2.039314051469167

Epoch: 157| Step: 0
Training loss: 2.677882671356201
Validation loss: 2.0595149199167886

Epoch: 6| Step: 1
Training loss: 2.3607630729675293
Validation loss: 2.0682974259058633

Epoch: 6| Step: 2
Training loss: 2.1148500442504883
Validation loss: 2.0723160107930503

Epoch: 6| Step: 3
Training loss: 2.3455748558044434
Validation loss: 2.0873406132062278

Epoch: 6| Step: 4
Training loss: 1.5187702178955078
Validation loss: 2.073870142300924

Epoch: 6| Step: 5
Training loss: 2.0471320152282715
Validation loss: 2.0861331025759378

Epoch: 6| Step: 6
Training loss: 1.7605135440826416
Validation loss: 2.092728098233541

Epoch: 6| Step: 7
Training loss: 1.9725472927093506
Validation loss: 2.0831145445505777

Epoch: 6| Step: 8
Training loss: 1.884857416152954
Validation loss: 2.07989893356959

Epoch: 6| Step: 9
Training loss: 1.8450648784637451
Validation loss: 2.0713366270065308

Epoch: 6| Step: 10
Training loss: 2.254207134246826
Validation loss: 2.0667078296343484

Epoch: 6| Step: 11
Training loss: 2.0589945316314697
Validation loss: 2.054137865702311

Epoch: 6| Step: 12
Training loss: 2.270726203918457
Validation loss: 2.0563637415568032

Epoch: 6| Step: 13
Training loss: 1.559015154838562
Validation loss: 2.066179891427358

Epoch: 158| Step: 0
Training loss: 1.5685213804244995
Validation loss: 2.0642054875691733

Epoch: 6| Step: 1
Training loss: 2.357044219970703
Validation loss: 2.0711695750554404

Epoch: 6| Step: 2
Training loss: 1.9981249570846558
Validation loss: 2.07187553246816

Epoch: 6| Step: 3
Training loss: 1.9548367261886597
Validation loss: 2.0786373615264893

Epoch: 6| Step: 4
Training loss: 1.6298868656158447
Validation loss: 2.088006933530172

Epoch: 6| Step: 5
Training loss: 2.491147041320801
Validation loss: 2.0846523443857827

Epoch: 6| Step: 6
Training loss: 2.0500807762145996
Validation loss: 2.0808987617492676

Epoch: 6| Step: 7
Training loss: 1.9315780401229858
Validation loss: 2.0916205644607544

Epoch: 6| Step: 8
Training loss: 2.0377156734466553
Validation loss: 2.093972106774648

Epoch: 6| Step: 9
Training loss: 2.4273910522460938
Validation loss: 2.07863450050354

Epoch: 6| Step: 10
Training loss: 1.3374738693237305
Validation loss: 2.083422680695852

Epoch: 6| Step: 11
Training loss: 1.9273087978363037
Validation loss: 2.0777432719866433

Epoch: 6| Step: 12
Training loss: 2.224121570587158
Validation loss: 2.077783723672231

Epoch: 6| Step: 13
Training loss: 2.4216227531433105
Validation loss: 2.074965993563334

Epoch: 159| Step: 0
Training loss: 2.3629541397094727
Validation loss: 2.0661445458730063

Epoch: 6| Step: 1
Training loss: 1.413495421409607
Validation loss: 2.067895849545797

Epoch: 6| Step: 2
Training loss: 1.8760080337524414
Validation loss: 2.064387480417887

Epoch: 6| Step: 3
Training loss: 1.8580873012542725
Validation loss: 2.063453276952108

Epoch: 6| Step: 4
Training loss: 1.419403314590454
Validation loss: 2.059761186440786

Epoch: 6| Step: 5
Training loss: 2.0222465991973877
Validation loss: 2.0575587153434753

Epoch: 6| Step: 6
Training loss: 1.4170489311218262
Validation loss: 2.0723878145217896

Epoch: 6| Step: 7
Training loss: 2.2249186038970947
Validation loss: 2.0674215952555337

Epoch: 6| Step: 8
Training loss: 2.1703426837921143
Validation loss: 2.0809007485707602

Epoch: 6| Step: 9
Training loss: 2.2711308002471924
Validation loss: 2.0818955103556314

Epoch: 6| Step: 10
Training loss: 2.073127031326294
Validation loss: 2.0707613825798035

Epoch: 6| Step: 11
Training loss: 3.0848546028137207
Validation loss: 2.0696086486180625

Epoch: 6| Step: 12
Training loss: 2.105224370956421
Validation loss: 2.060435692469279

Epoch: 6| Step: 13
Training loss: 1.9851038455963135
Validation loss: 2.057232141494751

Epoch: 160| Step: 0
Training loss: 2.3472518920898438
Validation loss: 2.0693682432174683

Epoch: 6| Step: 1
Training loss: 1.9259179830551147
Validation loss: 2.054723540941874

Epoch: 6| Step: 2
Training loss: 1.695502758026123
Validation loss: 2.051846504211426

Epoch: 6| Step: 3
Training loss: 1.9829145669937134
Validation loss: 2.043375293413798

Epoch: 6| Step: 4
Training loss: 2.2369484901428223
Validation loss: 2.063051998615265

Epoch: 6| Step: 5
Training loss: 1.5782554149627686
Validation loss: 2.0645925601323447

Epoch: 6| Step: 6
Training loss: 1.830482006072998
Validation loss: 2.0598594148953757

Epoch: 6| Step: 7
Training loss: 2.272050142288208
Validation loss: 2.0631945530573526

Epoch: 6| Step: 8
Training loss: 2.146455764770508
Validation loss: 2.0689560572306314

Epoch: 6| Step: 9
Training loss: 1.9816029071807861
Validation loss: 2.0568522612253823

Epoch: 6| Step: 10
Training loss: 1.8893803358078003
Validation loss: 2.066321233908335

Epoch: 6| Step: 11
Training loss: 2.396911859512329
Validation loss: 2.06097404162089

Epoch: 6| Step: 12
Training loss: 1.9044508934020996
Validation loss: 2.0717835227648416

Epoch: 6| Step: 13
Training loss: 2.2288131713867188
Validation loss: 2.059449315071106

Epoch: 161| Step: 0
Training loss: 2.6953697204589844
Validation loss: 2.077730735143026

Epoch: 6| Step: 1
Training loss: 1.7758961915969849
Validation loss: 2.078686515490214

Epoch: 6| Step: 2
Training loss: 2.038408041000366
Validation loss: 2.0849706530570984

Epoch: 6| Step: 3
Training loss: 1.8956542015075684
Validation loss: 2.0978434085845947

Epoch: 6| Step: 4
Training loss: 1.7162437438964844
Validation loss: 2.0781468749046326

Epoch: 6| Step: 5
Training loss: 1.321860671043396
Validation loss: 2.067719260851542

Epoch: 6| Step: 6
Training loss: 1.8333722352981567
Validation loss: 2.069734772046407

Epoch: 6| Step: 7
Training loss: 1.7647876739501953
Validation loss: 2.0693017840385437

Epoch: 6| Step: 8
Training loss: 2.751322031021118
Validation loss: 2.068853497505188

Epoch: 6| Step: 9
Training loss: 1.3757963180541992
Validation loss: 2.0707272489865622

Epoch: 6| Step: 10
Training loss: 2.0043468475341797
Validation loss: 2.0723027189572654

Epoch: 6| Step: 11
Training loss: 2.174198627471924
Validation loss: 2.062283436457316

Epoch: 6| Step: 12
Training loss: 2.86478328704834
Validation loss: 2.0571343898773193

Epoch: 6| Step: 13
Training loss: 2.1286818981170654
Validation loss: 2.0617725253105164

Epoch: 162| Step: 0
Training loss: 2.2928807735443115
Validation loss: 2.0576159954071045

Epoch: 6| Step: 1
Training loss: 1.6476432085037231
Validation loss: 2.0505619645118713

Epoch: 6| Step: 2
Training loss: 2.5167932510375977
Validation loss: 2.06202232837677

Epoch: 6| Step: 3
Training loss: 1.8895764350891113
Validation loss: 2.0493802229563394

Epoch: 6| Step: 4
Training loss: 1.6422886848449707
Validation loss: 2.063258111476898

Epoch: 6| Step: 5
Training loss: 2.461508274078369
Validation loss: 2.056067248185476

Epoch: 6| Step: 6
Training loss: 2.0043210983276367
Validation loss: 2.064298907915751

Epoch: 6| Step: 7
Training loss: 1.5105414390563965
Validation loss: 2.0708953142166138

Epoch: 6| Step: 8
Training loss: 1.8520591259002686
Validation loss: 2.0799739956855774

Epoch: 6| Step: 9
Training loss: 1.9592045545578003
Validation loss: 2.0961191852887473

Epoch: 6| Step: 10
Training loss: 2.101555824279785
Validation loss: 2.100410262743632

Epoch: 6| Step: 11
Training loss: 1.9813129901885986
Validation loss: 2.0974048376083374

Epoch: 6| Step: 12
Training loss: 1.9823988676071167
Validation loss: 2.106834570566813

Epoch: 6| Step: 13
Training loss: 2.6180052757263184
Validation loss: 2.1064889232317605

Epoch: 163| Step: 0
Training loss: 1.7989360094070435
Validation loss: 2.105475902557373

Epoch: 6| Step: 1
Training loss: 1.950737476348877
Validation loss: 2.0784217715263367

Epoch: 6| Step: 2
Training loss: 2.4231045246124268
Validation loss: 2.071088115374247

Epoch: 6| Step: 3
Training loss: 1.7801443338394165
Validation loss: 2.077139735221863

Epoch: 6| Step: 4
Training loss: 2.1550135612487793
Validation loss: 2.0584757328033447

Epoch: 6| Step: 5
Training loss: 2.165708541870117
Validation loss: 2.0587326486905417

Epoch: 6| Step: 6
Training loss: 2.0293030738830566
Validation loss: 2.060511827468872

Epoch: 6| Step: 7
Training loss: 2.1531219482421875
Validation loss: 2.057933767636617

Epoch: 6| Step: 8
Training loss: 1.3982598781585693
Validation loss: 2.048636555671692

Epoch: 6| Step: 9
Training loss: 2.6273765563964844
Validation loss: 2.0623618960380554

Epoch: 6| Step: 10
Training loss: 1.9507369995117188
Validation loss: 2.0653184254964194

Epoch: 6| Step: 11
Training loss: 2.0576093196868896
Validation loss: 2.0620846350987754

Epoch: 6| Step: 12
Training loss: 1.6771389245986938
Validation loss: 2.0696951349576316

Epoch: 6| Step: 13
Training loss: 2.2408270835876465
Validation loss: 2.0770161350568137

Epoch: 164| Step: 0
Training loss: 2.001643419265747
Validation loss: 2.0782454013824463

Epoch: 6| Step: 1
Training loss: 1.9690932035446167
Validation loss: 2.0894512136777244

Epoch: 6| Step: 2
Training loss: 1.9118270874023438
Validation loss: 2.0841854214668274

Epoch: 6| Step: 3
Training loss: 2.442204236984253
Validation loss: 2.0907363891601562

Epoch: 6| Step: 4
Training loss: 2.428804397583008
Validation loss: 2.0849939982096353

Epoch: 6| Step: 5
Training loss: 1.957078456878662
Validation loss: 2.081961214542389

Epoch: 6| Step: 6
Training loss: 1.8924137353897095
Validation loss: 2.095588187376658

Epoch: 6| Step: 7
Training loss: 1.8541339635849
Validation loss: 2.0809293588002524

Epoch: 6| Step: 8
Training loss: 2.332141876220703
Validation loss: 2.088126540184021

Epoch: 6| Step: 9
Training loss: 2.047532558441162
Validation loss: 2.0825414458910623

Epoch: 6| Step: 10
Training loss: 2.1127755641937256
Validation loss: 2.0776765942573547

Epoch: 6| Step: 11
Training loss: 1.4169583320617676
Validation loss: 2.0915889342625937

Epoch: 6| Step: 12
Training loss: 2.169046640396118
Validation loss: 2.088102340698242

Epoch: 6| Step: 13
Training loss: 1.6919093132019043
Validation loss: 2.0961898962656655

Epoch: 165| Step: 0
Training loss: 2.205582618713379
Validation loss: 2.094571590423584

Epoch: 6| Step: 1
Training loss: 1.7176185846328735
Validation loss: 2.0824498732884726

Epoch: 6| Step: 2
Training loss: 2.346982479095459
Validation loss: 2.0826058387756348

Epoch: 6| Step: 3
Training loss: 2.1591670513153076
Validation loss: 2.0963688294092813

Epoch: 6| Step: 4
Training loss: 2.061110019683838
Validation loss: 2.081311265627543

Epoch: 6| Step: 5
Training loss: 1.7027913331985474
Validation loss: 2.0885618329048157

Epoch: 6| Step: 6
Training loss: 2.1469779014587402
Validation loss: 2.085483968257904

Epoch: 6| Step: 7
Training loss: 1.8583606481552124
Validation loss: 2.074388027191162

Epoch: 6| Step: 8
Training loss: 2.2806248664855957
Validation loss: 2.0812602043151855

Epoch: 6| Step: 9
Training loss: 2.5961921215057373
Validation loss: 2.0786871910095215

Epoch: 6| Step: 10
Training loss: 1.651521921157837
Validation loss: 2.093668580055237

Epoch: 6| Step: 11
Training loss: 1.4878878593444824
Validation loss: 2.0712748567263284

Epoch: 6| Step: 12
Training loss: 1.926396131515503
Validation loss: 2.080586771170298

Epoch: 6| Step: 13
Training loss: 1.7190845012664795
Validation loss: 2.0772473017374673

Epoch: 166| Step: 0
Training loss: 2.2029776573181152
Validation loss: 2.0735996762911477

Epoch: 6| Step: 1
Training loss: 2.1323606967926025
Validation loss: 2.0745704571406045

Epoch: 6| Step: 2
Training loss: 1.755000352859497
Validation loss: 2.0733395417531333

Epoch: 6| Step: 3
Training loss: 1.5455350875854492
Validation loss: 2.079585631688436

Epoch: 6| Step: 4
Training loss: 1.2184090614318848
Validation loss: 2.093615412712097

Epoch: 6| Step: 5
Training loss: 1.872113823890686
Validation loss: 2.0931912461916604

Epoch: 6| Step: 6
Training loss: 2.3014965057373047
Validation loss: 2.0957887172698975

Epoch: 6| Step: 7
Training loss: 2.805133819580078
Validation loss: 2.0803751746813455

Epoch: 6| Step: 8
Training loss: 2.0784478187561035
Validation loss: 2.0831406315167746

Epoch: 6| Step: 9
Training loss: 2.2900919914245605
Validation loss: 2.081643581390381

Epoch: 6| Step: 10
Training loss: 1.6613378524780273
Validation loss: 2.0757126013437905

Epoch: 6| Step: 11
Training loss: 2.181042194366455
Validation loss: 2.070286293824514

Epoch: 6| Step: 12
Training loss: 2.2643983364105225
Validation loss: 2.089704970518748

Epoch: 6| Step: 13
Training loss: 1.6020803451538086
Validation loss: 2.0760793685913086

Epoch: 167| Step: 0
Training loss: 2.09417462348938
Validation loss: 2.0857498248418174

Epoch: 6| Step: 1
Training loss: 1.6234865188598633
Validation loss: 2.089321792125702

Epoch: 6| Step: 2
Training loss: 1.925046682357788
Validation loss: 2.087290088335673

Epoch: 6| Step: 3
Training loss: 1.62618088722229
Validation loss: 2.0856759548187256

Epoch: 6| Step: 4
Training loss: 2.500454902648926
Validation loss: 2.0798144936561584

Epoch: 6| Step: 5
Training loss: 2.0653367042541504
Validation loss: 2.0779402454694114

Epoch: 6| Step: 6
Training loss: 1.8356654644012451
Validation loss: 2.0754704078038535

Epoch: 6| Step: 7
Training loss: 1.9049218893051147
Validation loss: 2.0734012921651206

Epoch: 6| Step: 8
Training loss: 2.0909292697906494
Validation loss: 2.079168379306793

Epoch: 6| Step: 9
Training loss: 1.5831962823867798
Validation loss: 2.078866799672445

Epoch: 6| Step: 10
Training loss: 1.7482386827468872
Validation loss: 2.08795827627182

Epoch: 6| Step: 11
Training loss: 2.9017186164855957
Validation loss: 2.0855958263079324

Epoch: 6| Step: 12
Training loss: 2.0639920234680176
Validation loss: 2.080049773057302

Epoch: 6| Step: 13
Training loss: 2.0014588832855225
Validation loss: 2.0909534295399985

Epoch: 168| Step: 0
Training loss: 2.159325122833252
Validation loss: 2.0804240306218467

Epoch: 6| Step: 1
Training loss: 1.8250129222869873
Validation loss: 2.091439207394918

Epoch: 6| Step: 2
Training loss: 1.8482751846313477
Validation loss: 2.070780118306478

Epoch: 6| Step: 3
Training loss: 2.0333118438720703
Validation loss: 2.0821507374445596

Epoch: 6| Step: 4
Training loss: 2.5439844131469727
Validation loss: 2.080146312713623

Epoch: 6| Step: 5
Training loss: 2.0207393169403076
Validation loss: 2.0858975648880005

Epoch: 6| Step: 6
Training loss: 1.7413749694824219
Validation loss: 2.0973058144251504

Epoch: 6| Step: 7
Training loss: 2.4912760257720947
Validation loss: 2.084840635458628

Epoch: 6| Step: 8
Training loss: 1.9089860916137695
Validation loss: 2.0852271715799966

Epoch: 6| Step: 9
Training loss: 1.997821569442749
Validation loss: 2.0831613540649414

Epoch: 6| Step: 10
Training loss: 2.0284857749938965
Validation loss: 2.084380586942037

Epoch: 6| Step: 11
Training loss: 1.7514545917510986
Validation loss: 2.0880566040674844

Epoch: 6| Step: 12
Training loss: 2.0375125408172607
Validation loss: 2.078694303830465

Epoch: 6| Step: 13
Training loss: 1.7164255380630493
Validation loss: 2.0859373013178506

Epoch: 169| Step: 0
Training loss: 2.4052209854125977
Validation loss: 2.076440711816152

Epoch: 6| Step: 1
Training loss: 1.8505878448486328
Validation loss: 2.0846388936042786

Epoch: 6| Step: 2
Training loss: 1.7682249546051025
Validation loss: 2.0825929641723633

Epoch: 6| Step: 3
Training loss: 2.19913649559021
Validation loss: 2.0848517219225564

Epoch: 6| Step: 4
Training loss: 1.4627268314361572
Validation loss: 2.0941672325134277

Epoch: 6| Step: 5
Training loss: 2.292867660522461
Validation loss: 2.093633770942688

Epoch: 6| Step: 6
Training loss: 2.095194101333618
Validation loss: 2.0818272034327188

Epoch: 6| Step: 7
Training loss: 2.164745569229126
Validation loss: 2.097180803616842

Epoch: 6| Step: 8
Training loss: 1.3161699771881104
Validation loss: 2.0997827450434365

Epoch: 6| Step: 9
Training loss: 1.7810208797454834
Validation loss: 2.0964651703834534

Epoch: 6| Step: 10
Training loss: 2.1656904220581055
Validation loss: 2.0969185630480447

Epoch: 6| Step: 11
Training loss: 1.8564566373825073
Validation loss: 2.1006760199864707

Epoch: 6| Step: 12
Training loss: 2.044955253601074
Validation loss: 2.0869469245274863

Epoch: 6| Step: 13
Training loss: 2.483975410461426
Validation loss: 2.0876904328664145

Epoch: 170| Step: 0
Training loss: 2.455256462097168
Validation loss: 2.0865917603174844

Epoch: 6| Step: 1
Training loss: 2.5727486610412598
Validation loss: 2.0940007170041404

Epoch: 6| Step: 2
Training loss: 1.628864049911499
Validation loss: 2.0872245828310647

Epoch: 6| Step: 3
Training loss: 2.1786041259765625
Validation loss: 2.082643767197927

Epoch: 6| Step: 4
Training loss: 1.8668898344039917
Validation loss: 2.0811960697174072

Epoch: 6| Step: 5
Training loss: 1.846510648727417
Validation loss: 2.0918481747309365

Epoch: 6| Step: 6
Training loss: 1.7452443838119507
Validation loss: 2.0803409417470298

Epoch: 6| Step: 7
Training loss: 1.5569067001342773
Validation loss: 2.089476148287455

Epoch: 6| Step: 8
Training loss: 1.8018075227737427
Validation loss: 2.078644851843516

Epoch: 6| Step: 9
Training loss: 1.7144370079040527
Validation loss: 2.0859577457110086

Epoch: 6| Step: 10
Training loss: 2.755185842514038
Validation loss: 2.0940824151039124

Epoch: 6| Step: 11
Training loss: 2.1954264640808105
Validation loss: 2.095097839832306

Epoch: 6| Step: 12
Training loss: 2.2697958946228027
Validation loss: 2.1014098127683005

Epoch: 6| Step: 13
Training loss: 1.4933984279632568
Validation loss: 2.086555461088816

Epoch: 171| Step: 0
Training loss: 2.3239896297454834
Validation loss: 2.0850554505983987

Epoch: 6| Step: 1
Training loss: 1.666523814201355
Validation loss: 2.0795604586601257

Epoch: 6| Step: 2
Training loss: 2.185606002807617
Validation loss: 2.0570794542630515

Epoch: 6| Step: 3
Training loss: 1.88025963306427
Validation loss: 2.0641240080197654

Epoch: 6| Step: 4
Training loss: 1.6176137924194336
Validation loss: 2.0562078952789307

Epoch: 6| Step: 5
Training loss: 2.3600409030914307
Validation loss: 2.067575752735138

Epoch: 6| Step: 6
Training loss: 1.815891981124878
Validation loss: 2.060180405775706

Epoch: 6| Step: 7
Training loss: 2.1637868881225586
Validation loss: 2.059593995412191

Epoch: 6| Step: 8
Training loss: 1.8860409259796143
Validation loss: 2.0578378637631736

Epoch: 6| Step: 9
Training loss: 1.9386789798736572
Validation loss: 2.0617308815320334

Epoch: 6| Step: 10
Training loss: 2.432483196258545
Validation loss: 2.0484866897265115

Epoch: 6| Step: 11
Training loss: 1.6687854528427124
Validation loss: 2.0523084004720054

Epoch: 6| Step: 12
Training loss: 2.5914785861968994
Validation loss: 2.0664097666740417

Epoch: 6| Step: 13
Training loss: 2.161895275115967
Validation loss: 2.0676459868748984

Epoch: 172| Step: 0
Training loss: 1.750030517578125
Validation loss: 2.0734394192695618

Epoch: 6| Step: 1
Training loss: 1.6877024173736572
Validation loss: 2.078303118546804

Epoch: 6| Step: 2
Training loss: 1.8698410987854004
Validation loss: 2.0793078740437827

Epoch: 6| Step: 3
Training loss: 1.8631401062011719
Validation loss: 2.0893444220225015

Epoch: 6| Step: 4
Training loss: 2.273426055908203
Validation loss: 2.0826200445493064

Epoch: 6| Step: 5
Training loss: 2.021312713623047
Validation loss: 2.0877670645713806

Epoch: 6| Step: 6
Training loss: 2.430248737335205
Validation loss: 2.097483476003011

Epoch: 6| Step: 7
Training loss: 1.891947865486145
Validation loss: 2.0864437023798623

Epoch: 6| Step: 8
Training loss: 1.9404735565185547
Validation loss: 2.0924508770306907

Epoch: 6| Step: 9
Training loss: 2.1395323276519775
Validation loss: 2.1029523412386575

Epoch: 6| Step: 10
Training loss: 1.316733479499817
Validation loss: 2.10217954715093

Epoch: 6| Step: 11
Training loss: 2.4062423706054688
Validation loss: 2.0970350901285806

Epoch: 6| Step: 12
Training loss: 1.990450382232666
Validation loss: 2.0995365182558694

Epoch: 6| Step: 13
Training loss: 2.3141751289367676
Validation loss: 2.109169125556946

Epoch: 173| Step: 0
Training loss: 2.101729393005371
Validation loss: 2.1032658418019614

Epoch: 6| Step: 1
Training loss: 1.6628549098968506
Validation loss: 2.11406409740448

Epoch: 6| Step: 2
Training loss: 1.5958274602890015
Validation loss: 2.112202823162079

Epoch: 6| Step: 3
Training loss: 1.8357701301574707
Validation loss: 2.1148954232533774

Epoch: 6| Step: 4
Training loss: 1.5036438703536987
Validation loss: 2.110734204451243

Epoch: 6| Step: 5
Training loss: 2.577690601348877
Validation loss: 2.1054638028144836

Epoch: 6| Step: 6
Training loss: 2.520850658416748
Validation loss: 2.1105446020762124

Epoch: 6| Step: 7
Training loss: 1.9549723863601685
Validation loss: 2.1008120576540628

Epoch: 6| Step: 8
Training loss: 2.2039594650268555
Validation loss: 2.0815805395444236

Epoch: 6| Step: 9
Training loss: 2.0011839866638184
Validation loss: 2.0854394833246865

Epoch: 6| Step: 10
Training loss: 1.949774980545044
Validation loss: 2.0764040350914

Epoch: 6| Step: 11
Training loss: 2.2049899101257324
Validation loss: 2.0830101370811462

Epoch: 6| Step: 12
Training loss: 1.6956894397735596
Validation loss: 2.082373241583506

Epoch: 6| Step: 13
Training loss: 2.053819417953491
Validation loss: 2.0751758019129434

Epoch: 174| Step: 0
Training loss: 2.016265630722046
Validation loss: 2.087705135345459

Epoch: 6| Step: 1
Training loss: 1.9210249185562134
Validation loss: 2.073282698790232

Epoch: 6| Step: 2
Training loss: 2.4513731002807617
Validation loss: 2.065275231997172

Epoch: 6| Step: 3
Training loss: 1.8606986999511719
Validation loss: 2.079769472281138

Epoch: 6| Step: 4
Training loss: 2.074904441833496
Validation loss: 2.089729150136312

Epoch: 6| Step: 5
Training loss: 1.7754803895950317
Validation loss: 2.0921327074368796

Epoch: 6| Step: 6
Training loss: 1.9323712587356567
Validation loss: 2.0917178789774575

Epoch: 6| Step: 7
Training loss: 1.6553289890289307
Validation loss: 2.096721132596334

Epoch: 6| Step: 8
Training loss: 2.2399613857269287
Validation loss: 2.100128432114919

Epoch: 6| Step: 9
Training loss: 1.881792664527893
Validation loss: 2.077922999858856

Epoch: 6| Step: 10
Training loss: 1.899609088897705
Validation loss: 2.0880151788393655

Epoch: 6| Step: 11
Training loss: 1.9697514772415161
Validation loss: 2.0932947595914206

Epoch: 6| Step: 12
Training loss: 2.499814033508301
Validation loss: 2.0991034309069314

Epoch: 6| Step: 13
Training loss: 1.5714291334152222
Validation loss: 2.0978161692619324

Epoch: 175| Step: 0
Training loss: 1.9450315237045288
Validation loss: 2.0988670190175376

Epoch: 6| Step: 1
Training loss: 1.925795078277588
Validation loss: 2.0943952997525535

Epoch: 6| Step: 2
Training loss: 2.445138454437256
Validation loss: 2.093734880288442

Epoch: 6| Step: 3
Training loss: 2.291504144668579
Validation loss: 2.1045982440312705

Epoch: 6| Step: 4
Training loss: 1.9564305543899536
Validation loss: 2.089706599712372

Epoch: 6| Step: 5
Training loss: 2.1295952796936035
Validation loss: 2.0831648310025535

Epoch: 6| Step: 6
Training loss: 1.979019284248352
Validation loss: 2.106464604536692

Epoch: 6| Step: 7
Training loss: 2.4669857025146484
Validation loss: 2.0927799940109253

Epoch: 6| Step: 8
Training loss: 1.7772847414016724
Validation loss: 2.1120041608810425

Epoch: 6| Step: 9
Training loss: 1.4555914402008057
Validation loss: 2.1095666885375977

Epoch: 6| Step: 10
Training loss: 1.917777419090271
Validation loss: 2.1070388158162436

Epoch: 6| Step: 11
Training loss: 1.7977161407470703
Validation loss: 2.109037379423777

Epoch: 6| Step: 12
Training loss: 2.0704329013824463
Validation loss: 2.0899337927500405

Epoch: 6| Step: 13
Training loss: 1.62068772315979
Validation loss: 2.1039497454961142

Epoch: 176| Step: 0
Training loss: 1.2088525295257568
Validation loss: 2.0990259846051535

Epoch: 6| Step: 1
Training loss: 1.683293342590332
Validation loss: 2.096533497174581

Epoch: 6| Step: 2
Training loss: 1.6636943817138672
Validation loss: 2.0794672767321267

Epoch: 6| Step: 3
Training loss: 1.8615832328796387
Validation loss: 2.0908432602882385

Epoch: 6| Step: 4
Training loss: 2.0976948738098145
Validation loss: 2.08867879708608

Epoch: 6| Step: 5
Training loss: 2.0893023014068604
Validation loss: 2.082564135392507

Epoch: 6| Step: 6
Training loss: 2.1966118812561035
Validation loss: 2.077263832092285

Epoch: 6| Step: 7
Training loss: 2.1272940635681152
Validation loss: 2.0898831486701965

Epoch: 6| Step: 8
Training loss: 1.8462228775024414
Validation loss: 2.094827930132548

Epoch: 6| Step: 9
Training loss: 1.6310818195343018
Validation loss: 2.0844595432281494

Epoch: 6| Step: 10
Training loss: 2.22407603263855
Validation loss: 2.0885671377182007

Epoch: 6| Step: 11
Training loss: 2.4345054626464844
Validation loss: 2.080462396144867

Epoch: 6| Step: 12
Training loss: 2.384953737258911
Validation loss: 2.081152538458506

Epoch: 6| Step: 13
Training loss: 2.3038206100463867
Validation loss: 2.0910101334253945

Epoch: 177| Step: 0
Training loss: 1.8126733303070068
Validation loss: 2.0952165524164834

Epoch: 6| Step: 1
Training loss: 2.271210193634033
Validation loss: 2.0918423334757485

Epoch: 6| Step: 2
Training loss: 1.550863265991211
Validation loss: 2.0874662597974143

Epoch: 6| Step: 3
Training loss: 1.2500985860824585
Validation loss: 2.094280481338501

Epoch: 6| Step: 4
Training loss: 1.6001098155975342
Validation loss: 2.082710862159729

Epoch: 6| Step: 5
Training loss: 2.1963696479797363
Validation loss: 2.0867886940638223

Epoch: 6| Step: 6
Training loss: 1.7045180797576904
Validation loss: 2.0701358914375305

Epoch: 6| Step: 7
Training loss: 2.696810007095337
Validation loss: 2.074073612689972

Epoch: 6| Step: 8
Training loss: 2.4615280628204346
Validation loss: 2.0746394395828247

Epoch: 6| Step: 9
Training loss: 2.0919997692108154
Validation loss: 2.0717331171035767

Epoch: 6| Step: 10
Training loss: 1.9459584951400757
Validation loss: 2.078804393609365

Epoch: 6| Step: 11
Training loss: 1.7383935451507568
Validation loss: 2.071910818417867

Epoch: 6| Step: 12
Training loss: 1.8385324478149414
Validation loss: 2.061648726463318

Epoch: 6| Step: 13
Training loss: 2.631396770477295
Validation loss: 2.0845431685447693

Epoch: 178| Step: 0
Training loss: 2.1730008125305176
Validation loss: 2.072369337081909

Epoch: 6| Step: 1
Training loss: 1.7022171020507812
Validation loss: 2.0812270243962607

Epoch: 6| Step: 2
Training loss: 2.4546008110046387
Validation loss: 2.090821365515391

Epoch: 6| Step: 3
Training loss: 1.3124611377716064
Validation loss: 2.0788974165916443

Epoch: 6| Step: 4
Training loss: 1.863316535949707
Validation loss: 2.0954399506251016

Epoch: 6| Step: 5
Training loss: 1.843526840209961
Validation loss: 2.094063699245453

Epoch: 6| Step: 6
Training loss: 2.386685371398926
Validation loss: 2.0999987721443176

Epoch: 6| Step: 7
Training loss: 2.3017468452453613
Validation loss: 2.0853240291277566

Epoch: 6| Step: 8
Training loss: 2.548424243927002
Validation loss: 2.0874480605125427

Epoch: 6| Step: 9
Training loss: 2.3653435707092285
Validation loss: 2.0814910928408303

Epoch: 6| Step: 10
Training loss: 1.5609766244888306
Validation loss: 2.0950201948483786

Epoch: 6| Step: 11
Training loss: 1.6549111604690552
Validation loss: 2.090244253476461

Epoch: 6| Step: 12
Training loss: 1.297364354133606
Validation loss: 2.0906383196512857

Epoch: 6| Step: 13
Training loss: 2.0802907943725586
Validation loss: 2.0981902281443277

Epoch: 179| Step: 0
Training loss: 1.7356157302856445
Validation loss: 2.080490469932556

Epoch: 6| Step: 1
Training loss: 1.904270887374878
Validation loss: 2.095670501391093

Epoch: 6| Step: 2
Training loss: 2.7557225227355957
Validation loss: 2.1045764485994973

Epoch: 6| Step: 3
Training loss: 1.7663533687591553
Validation loss: 2.098470151424408

Epoch: 6| Step: 4
Training loss: 1.7701926231384277
Validation loss: 2.1079885562260947

Epoch: 6| Step: 5
Training loss: 1.7009022235870361
Validation loss: 2.1113555232683816

Epoch: 6| Step: 6
Training loss: 1.8155851364135742
Validation loss: 2.1120709578196206

Epoch: 6| Step: 7
Training loss: 1.4378256797790527
Validation loss: 2.1171876788139343

Epoch: 6| Step: 8
Training loss: 2.3020143508911133
Validation loss: 2.115353008111318

Epoch: 6| Step: 9
Training loss: 1.8053312301635742
Validation loss: 2.107423702875773

Epoch: 6| Step: 10
Training loss: 2.0966053009033203
Validation loss: 2.110762119293213

Epoch: 6| Step: 11
Training loss: 1.766072392463684
Validation loss: 2.103035032749176

Epoch: 6| Step: 12
Training loss: 2.5187602043151855
Validation loss: 2.0949741204579673

Epoch: 6| Step: 13
Training loss: 2.0906028747558594
Validation loss: 2.0760889053344727

Epoch: 180| Step: 0
Training loss: 1.4857940673828125
Validation loss: 2.0912997325261435

Epoch: 6| Step: 1
Training loss: 2.5731444358825684
Validation loss: 2.0943998297055564

Epoch: 6| Step: 2
Training loss: 1.923736810684204
Validation loss: 2.1139171719551086

Epoch: 6| Step: 3
Training loss: 1.455897569656372
Validation loss: 2.0975183645884194

Epoch: 6| Step: 4
Training loss: 1.8435906171798706
Validation loss: 2.1081016858418784

Epoch: 6| Step: 5
Training loss: 1.5706746578216553
Validation loss: 2.1141538619995117

Epoch: 6| Step: 6
Training loss: 2.0192267894744873
Validation loss: 2.0914785265922546

Epoch: 6| Step: 7
Training loss: 1.8813519477844238
Validation loss: 2.112248400847117

Epoch: 6| Step: 8
Training loss: 1.771416425704956
Validation loss: 2.1018760999043784

Epoch: 6| Step: 9
Training loss: 2.6963372230529785
Validation loss: 2.095289647579193

Epoch: 6| Step: 10
Training loss: 2.250425338745117
Validation loss: 2.106659928957621

Epoch: 6| Step: 11
Training loss: 1.975399136543274
Validation loss: 2.1114541490872702

Epoch: 6| Step: 12
Training loss: 2.264342784881592
Validation loss: 2.1000808080037436

Epoch: 6| Step: 13
Training loss: 2.0591039657592773
Validation loss: 2.07993354399999

Epoch: 181| Step: 0
Training loss: 1.5337820053100586
Validation loss: 2.0828946034113565

Epoch: 6| Step: 1
Training loss: 1.4669603109359741
Validation loss: 2.0802936951319375

Epoch: 6| Step: 2
Training loss: 2.0774331092834473
Validation loss: 2.0805615186691284

Epoch: 6| Step: 3
Training loss: 1.899052619934082
Validation loss: 2.0784164468447366

Epoch: 6| Step: 4
Training loss: 2.643839120864868
Validation loss: 2.0830270846684775

Epoch: 6| Step: 5
Training loss: 1.8741912841796875
Validation loss: 2.0770511825879416

Epoch: 6| Step: 6
Training loss: 1.3413665294647217
Validation loss: 2.0774263540903726

Epoch: 6| Step: 7
Training loss: 2.298098564147949
Validation loss: 2.0831027825673423

Epoch: 6| Step: 8
Training loss: 1.8839893341064453
Validation loss: 2.0831735134124756

Epoch: 6| Step: 9
Training loss: 2.518786907196045
Validation loss: 2.0895504355430603

Epoch: 6| Step: 10
Training loss: 1.781520962715149
Validation loss: 2.1023208498954773

Epoch: 6| Step: 11
Training loss: 2.0221610069274902
Validation loss: 2.113405962785085

Epoch: 6| Step: 12
Training loss: 2.1582353115081787
Validation loss: 2.1067806084950766

Epoch: 6| Step: 13
Training loss: 2.6842808723449707
Validation loss: 2.093766371409098

Epoch: 182| Step: 0
Training loss: 2.091674327850342
Validation loss: 2.0999356706937156

Epoch: 6| Step: 1
Training loss: 1.5863112211227417
Validation loss: 2.1192245284716287

Epoch: 6| Step: 2
Training loss: 1.7909722328186035
Validation loss: 2.1191900769869485

Epoch: 6| Step: 3
Training loss: 2.0426504611968994
Validation loss: 2.0937052567799888

Epoch: 6| Step: 4
Training loss: 1.9158003330230713
Validation loss: 2.0866665840148926

Epoch: 6| Step: 5
Training loss: 2.580064535140991
Validation loss: 2.0974626541137695

Epoch: 6| Step: 6
Training loss: 1.775903344154358
Validation loss: 2.0926718513170877

Epoch: 6| Step: 7
Training loss: 2.398437738418579
Validation loss: 2.0750931104024253

Epoch: 6| Step: 8
Training loss: 1.9403269290924072
Validation loss: 2.088446875413259

Epoch: 6| Step: 9
Training loss: 1.2738717794418335
Validation loss: 2.087058405081431

Epoch: 6| Step: 10
Training loss: 2.0860166549682617
Validation loss: 2.0870922605196633

Epoch: 6| Step: 11
Training loss: 2.3311123847961426
Validation loss: 2.0948819518089294

Epoch: 6| Step: 12
Training loss: 1.2389345169067383
Validation loss: 2.078316410382589

Epoch: 6| Step: 13
Training loss: 2.768939733505249
Validation loss: 2.0921722650527954

Epoch: 183| Step: 0
Training loss: 2.260383129119873
Validation loss: 2.0945305824279785

Epoch: 6| Step: 1
Training loss: 1.9151767492294312
Validation loss: 2.090061366558075

Epoch: 6| Step: 2
Training loss: 1.5345308780670166
Validation loss: 2.0923360188802085

Epoch: 6| Step: 3
Training loss: 2.024563789367676
Validation loss: 2.102968414624532

Epoch: 6| Step: 4
Training loss: 1.684923529624939
Validation loss: 2.1016493837038674

Epoch: 6| Step: 5
Training loss: 1.7724661827087402
Validation loss: 2.118111709753672

Epoch: 6| Step: 6
Training loss: 1.9265320301055908
Validation loss: 2.1180381178855896

Epoch: 6| Step: 7
Training loss: 2.4280219078063965
Validation loss: 2.122212290763855

Epoch: 6| Step: 8
Training loss: 2.323024272918701
Validation loss: 2.1139968236287436

Epoch: 6| Step: 9
Training loss: 1.722123146057129
Validation loss: 2.1022133827209473

Epoch: 6| Step: 10
Training loss: 1.8441343307495117
Validation loss: 2.099132776260376

Epoch: 6| Step: 11
Training loss: 2.2478318214416504
Validation loss: 2.0900021394093833

Epoch: 6| Step: 12
Training loss: 2.3450961112976074
Validation loss: 2.0876216888427734

Epoch: 6| Step: 13
Training loss: 1.5846211910247803
Validation loss: 2.0894808769226074

Epoch: 184| Step: 0
Training loss: 1.9922971725463867
Validation loss: 2.073518395423889

Epoch: 6| Step: 1
Training loss: 2.4451849460601807
Validation loss: 2.0705159505208335

Epoch: 6| Step: 2
Training loss: 2.3582568168640137
Validation loss: 2.077951451142629

Epoch: 6| Step: 3
Training loss: 2.296708106994629
Validation loss: 2.0706743399302163

Epoch: 6| Step: 4
Training loss: 2.287019729614258
Validation loss: 2.0694924195607505

Epoch: 6| Step: 5
Training loss: 2.0206332206726074
Validation loss: 2.069617430369059

Epoch: 6| Step: 6
Training loss: 2.0574350357055664
Validation loss: 2.0755675435066223

Epoch: 6| Step: 7
Training loss: 1.8647568225860596
Validation loss: 2.075522025426229

Epoch: 6| Step: 8
Training loss: 1.6639575958251953
Validation loss: 2.0757938027381897

Epoch: 6| Step: 9
Training loss: 1.7002108097076416
Validation loss: 2.079047679901123

Epoch: 6| Step: 10
Training loss: 1.186800479888916
Validation loss: 2.08168754975001

Epoch: 6| Step: 11
Training loss: 2.3642470836639404
Validation loss: 2.09477170308431

Epoch: 6| Step: 12
Training loss: 1.4748263359069824
Validation loss: 2.119995931784312

Epoch: 6| Step: 13
Training loss: 1.7892512083053589
Validation loss: 2.121825377146403

Epoch: 185| Step: 0
Training loss: 1.2033030986785889
Validation loss: 2.1285956303278604

Epoch: 6| Step: 1
Training loss: 1.552551507949829
Validation loss: 2.143515149752299

Epoch: 6| Step: 2
Training loss: 2.430382251739502
Validation loss: 2.172688881556193

Epoch: 6| Step: 3
Training loss: 2.1001224517822266
Validation loss: 2.195748249689738

Epoch: 6| Step: 4
Training loss: 3.198089599609375
Validation loss: 2.1826724211374917

Epoch: 6| Step: 5
Training loss: 2.4898507595062256
Validation loss: 2.1416033705075583

Epoch: 6| Step: 6
Training loss: 2.211409330368042
Validation loss: 2.1197227239608765

Epoch: 6| Step: 7
Training loss: 1.7356128692626953
Validation loss: 2.0916910966237388

Epoch: 6| Step: 8
Training loss: 1.9457204341888428
Validation loss: 2.0896477500597634

Epoch: 6| Step: 9
Training loss: 2.242335796356201
Validation loss: 2.0878914992014566

Epoch: 6| Step: 10
Training loss: 2.055037498474121
Validation loss: 2.0813796718915305

Epoch: 6| Step: 11
Training loss: 2.5464844703674316
Validation loss: 2.0778854290644326

Epoch: 6| Step: 12
Training loss: 1.1612513065338135
Validation loss: 2.0911858081817627

Epoch: 6| Step: 13
Training loss: 1.7930951118469238
Validation loss: 2.0780664682388306

Epoch: 186| Step: 0
Training loss: 2.7269954681396484
Validation loss: 2.10434224208196

Epoch: 6| Step: 1
Training loss: 1.498278260231018
Validation loss: 2.0855314135551453

Epoch: 6| Step: 2
Training loss: 1.6936874389648438
Validation loss: 2.0735995372136435

Epoch: 6| Step: 3
Training loss: 1.4141517877578735
Validation loss: 2.078278203805288

Epoch: 6| Step: 4
Training loss: 2.485621213912964
Validation loss: 2.0847428242365518

Epoch: 6| Step: 5
Training loss: 1.7037063837051392
Validation loss: 2.076880613962809

Epoch: 6| Step: 6
Training loss: 2.3058602809906006
Validation loss: 2.088561793168386

Epoch: 6| Step: 7
Training loss: 1.5175341367721558
Validation loss: 2.087448994318644

Epoch: 6| Step: 8
Training loss: 1.7155948877334595
Validation loss: 2.098398049672445

Epoch: 6| Step: 9
Training loss: 2.0491228103637695
Validation loss: 2.0928255716959634

Epoch: 6| Step: 10
Training loss: 1.8354376554489136
Validation loss: 2.1092654863993325

Epoch: 6| Step: 11
Training loss: 2.137474298477173
Validation loss: 2.091808537642161

Epoch: 6| Step: 12
Training loss: 2.356783628463745
Validation loss: 2.0958652893702188

Epoch: 6| Step: 13
Training loss: 2.0178327560424805
Validation loss: 2.091030399004618

Epoch: 187| Step: 0
Training loss: 1.9875019788742065
Validation loss: 2.0884326497713723

Epoch: 6| Step: 1
Training loss: 2.6060450077056885
Validation loss: 2.0948383808135986

Epoch: 6| Step: 2
Training loss: 1.639107584953308
Validation loss: 2.100476245085398

Epoch: 6| Step: 3
Training loss: 1.7522614002227783
Validation loss: 2.0955509742101035

Epoch: 6| Step: 4
Training loss: 1.7947553396224976
Validation loss: 2.0829474925994873

Epoch: 6| Step: 5
Training loss: 1.7213289737701416
Validation loss: 2.09245894352595

Epoch: 6| Step: 6
Training loss: 2.0550427436828613
Validation loss: 2.0936156709988913

Epoch: 6| Step: 7
Training loss: 2.1610805988311768
Validation loss: 2.098144213358561

Epoch: 6| Step: 8
Training loss: 1.6235747337341309
Validation loss: 2.1045394937197366

Epoch: 6| Step: 9
Training loss: 2.371189594268799
Validation loss: 2.119754672050476

Epoch: 6| Step: 10
Training loss: 1.6529004573822021
Validation loss: 2.1118229826291404

Epoch: 6| Step: 11
Training loss: 1.5664262771606445
Validation loss: 2.1136699120203652

Epoch: 6| Step: 12
Training loss: 1.9873205423355103
Validation loss: 2.1248504519462585

Epoch: 6| Step: 13
Training loss: 2.4413681030273438
Validation loss: 2.1108768582344055

Epoch: 188| Step: 0
Training loss: 1.5830707550048828
Validation loss: 2.1271167397499084

Epoch: 6| Step: 1
Training loss: 1.5088666677474976
Validation loss: 2.1165729761123657

Epoch: 6| Step: 2
Training loss: 2.066488742828369
Validation loss: 2.1214128335316977

Epoch: 6| Step: 3
Training loss: 1.8991467952728271
Validation loss: 2.113379637400309

Epoch: 6| Step: 4
Training loss: 1.9637393951416016
Validation loss: 2.1098883350690207

Epoch: 6| Step: 5
Training loss: 1.5524976253509521
Validation loss: 2.108667314052582

Epoch: 6| Step: 6
Training loss: 1.770829200744629
Validation loss: 2.1087814370791116

Epoch: 6| Step: 7
Training loss: 1.9937641620635986
Validation loss: 2.113554914792379

Epoch: 6| Step: 8
Training loss: 2.050792932510376
Validation loss: 2.1107097268104553

Epoch: 6| Step: 9
Training loss: 2.099595546722412
Validation loss: 2.107226312160492

Epoch: 6| Step: 10
Training loss: 2.338409185409546
Validation loss: 2.1005032062530518

Epoch: 6| Step: 11
Training loss: 2.601513385772705
Validation loss: 2.101523836453756

Epoch: 6| Step: 12
Training loss: 2.00773549079895
Validation loss: 2.0999519427617392

Epoch: 6| Step: 13
Training loss: 1.753652811050415
Validation loss: 2.0984699726104736

Epoch: 189| Step: 0
Training loss: 1.8245009183883667
Validation loss: 2.097930689652761

Epoch: 6| Step: 1
Training loss: 2.124577522277832
Validation loss: 2.0918303728103638

Epoch: 6| Step: 2
Training loss: 1.5469733476638794
Validation loss: 2.1001075903574624

Epoch: 6| Step: 3
Training loss: 2.178624153137207
Validation loss: 2.1006975571314492

Epoch: 6| Step: 4
Training loss: 1.8876935243606567
Validation loss: 2.0997762282689414

Epoch: 6| Step: 5
Training loss: 2.2227680683135986
Validation loss: 2.0981263717015586

Epoch: 6| Step: 6
Training loss: 1.285630464553833
Validation loss: 2.10454922914505

Epoch: 6| Step: 7
Training loss: 1.7362074851989746
Validation loss: 2.097215791543325

Epoch: 6| Step: 8
Training loss: 2.962556838989258
Validation loss: 2.106604735056559

Epoch: 6| Step: 9
Training loss: 2.26202392578125
Validation loss: 2.1188204685846963

Epoch: 6| Step: 10
Training loss: 2.2674922943115234
Validation loss: 2.1341049671173096

Epoch: 6| Step: 11
Training loss: 1.6584676504135132
Validation loss: 2.1139967838923135

Epoch: 6| Step: 12
Training loss: 1.9175612926483154
Validation loss: 2.113227049509684

Epoch: 6| Step: 13
Training loss: 1.6468634605407715
Validation loss: 2.1128292083740234

Epoch: 190| Step: 0
Training loss: 1.8026673793792725
Validation loss: 2.1184619863828025

Epoch: 6| Step: 1
Training loss: 2.184553623199463
Validation loss: 2.111911952495575

Epoch: 6| Step: 2
Training loss: 1.9773361682891846
Validation loss: 2.1141294638315835

Epoch: 6| Step: 3
Training loss: 2.218759536743164
Validation loss: 2.1020222107569375

Epoch: 6| Step: 4
Training loss: 1.4599812030792236
Validation loss: 2.0959452589352927

Epoch: 6| Step: 5
Training loss: 2.048233985900879
Validation loss: 2.0959692200024924

Epoch: 6| Step: 6
Training loss: 1.6915180683135986
Validation loss: 2.1006000638008118

Epoch: 6| Step: 7
Training loss: 1.732041358947754
Validation loss: 2.093902071317037

Epoch: 6| Step: 8
Training loss: 2.175320625305176
Validation loss: 2.0964242219924927

Epoch: 6| Step: 9
Training loss: 2.1167984008789062
Validation loss: 2.100524604320526

Epoch: 6| Step: 10
Training loss: 2.231250286102295
Validation loss: 2.1007858316103616

Epoch: 6| Step: 11
Training loss: 2.384239912033081
Validation loss: 2.0963168144226074

Epoch: 6| Step: 12
Training loss: 1.7752165794372559
Validation loss: 2.1077619393666587

Epoch: 6| Step: 13
Training loss: 2.191199541091919
Validation loss: 2.116917828718821

Epoch: 191| Step: 0
Training loss: 2.0433404445648193
Validation loss: 2.1139572660128274

Epoch: 6| Step: 1
Training loss: 2.3733887672424316
Validation loss: 2.105352838834127

Epoch: 6| Step: 2
Training loss: 1.3916020393371582
Validation loss: 2.119048515955607

Epoch: 6| Step: 3
Training loss: 2.377079963684082
Validation loss: 2.10293847322464

Epoch: 6| Step: 4
Training loss: 2.0154714584350586
Validation loss: 2.11798628171285

Epoch: 6| Step: 5
Training loss: 2.124682903289795
Validation loss: 2.1042357683181763

Epoch: 6| Step: 6
Training loss: 2.2437784671783447
Validation loss: 2.0749661922454834

Epoch: 6| Step: 7
Training loss: 1.9119789600372314
Validation loss: 2.0874724984169006

Epoch: 6| Step: 8
Training loss: 1.341881275177002
Validation loss: 2.0810963908831277

Epoch: 6| Step: 9
Training loss: 1.8702210187911987
Validation loss: 2.0674698750178018

Epoch: 6| Step: 10
Training loss: 2.037335157394409
Validation loss: 2.059618651866913

Epoch: 6| Step: 11
Training loss: 2.38339900970459
Validation loss: 2.0723750392595925

Epoch: 6| Step: 12
Training loss: 2.0064916610717773
Validation loss: 2.077054023742676

Epoch: 6| Step: 13
Training loss: 2.103404998779297
Validation loss: 2.066795070966085

Epoch: 192| Step: 0
Training loss: 2.113440752029419
Validation loss: 2.0700494249661765

Epoch: 6| Step: 1
Training loss: 2.177612543106079
Validation loss: 2.077247460683187

Epoch: 6| Step: 2
Training loss: 1.7618143558502197
Validation loss: 2.073111275831858

Epoch: 6| Step: 3
Training loss: 2.4353866577148438
Validation loss: 2.08007949590683

Epoch: 6| Step: 4
Training loss: 1.9718106985092163
Validation loss: 2.0738831162452698

Epoch: 6| Step: 5
Training loss: 1.8542672395706177
Validation loss: 2.073606530825297

Epoch: 6| Step: 6
Training loss: 1.2108664512634277
Validation loss: 2.0879852374394736

Epoch: 6| Step: 7
Training loss: 2.10063099861145
Validation loss: 2.0797137022018433

Epoch: 6| Step: 8
Training loss: 2.1236391067504883
Validation loss: 2.0861780047416687

Epoch: 6| Step: 9
Training loss: 1.6233775615692139
Validation loss: 2.086800237496694

Epoch: 6| Step: 10
Training loss: 1.8061010837554932
Validation loss: 2.0796178380648294

Epoch: 6| Step: 11
Training loss: 1.9639003276824951
Validation loss: 2.0798794825871787

Epoch: 6| Step: 12
Training loss: 2.1759159564971924
Validation loss: 2.0806265473365784

Epoch: 6| Step: 13
Training loss: 2.54274320602417
Validation loss: 2.0922237634658813

Epoch: 193| Step: 0
Training loss: 1.60751211643219
Validation loss: 2.0752928256988525

Epoch: 6| Step: 1
Training loss: 1.9974703788757324
Validation loss: 2.084973851839701

Epoch: 6| Step: 2
Training loss: 1.947848916053772
Validation loss: 2.082201520601908

Epoch: 6| Step: 3
Training loss: 2.001438617706299
Validation loss: 2.090673506259918

Epoch: 6| Step: 4
Training loss: 1.5617165565490723
Validation loss: 2.0888961354891458

Epoch: 6| Step: 5
Training loss: 2.206458568572998
Validation loss: 2.0918009877204895

Epoch: 6| Step: 6
Training loss: 2.500823497772217
Validation loss: 2.0874814987182617

Epoch: 6| Step: 7
Training loss: 1.8852894306182861
Validation loss: 2.0964703957239785

Epoch: 6| Step: 8
Training loss: 1.8266136646270752
Validation loss: 2.096704085667928

Epoch: 6| Step: 9
Training loss: 2.242691993713379
Validation loss: 2.1010170380274453

Epoch: 6| Step: 10
Training loss: 1.6593437194824219
Validation loss: 2.104683061440786

Epoch: 6| Step: 11
Training loss: 2.361154556274414
Validation loss: 2.1016821463902793

Epoch: 6| Step: 12
Training loss: 1.5244946479797363
Validation loss: 2.0937999288241067

Epoch: 6| Step: 13
Training loss: 2.2728500366210938
Validation loss: 2.098581870396932

Epoch: 194| Step: 0
Training loss: 2.0580759048461914
Validation loss: 2.1006949742635093

Epoch: 6| Step: 1
Training loss: 2.1815431118011475
Validation loss: 2.1083492437998452

Epoch: 6| Step: 2
Training loss: 2.23183012008667
Validation loss: 2.1114648580551147

Epoch: 6| Step: 3
Training loss: 2.0916855335235596
Validation loss: 2.109390119711558

Epoch: 6| Step: 4
Training loss: 2.1907920837402344
Validation loss: 2.1262413462003074

Epoch: 6| Step: 5
Training loss: 1.5611872673034668
Validation loss: 2.1199355125427246

Epoch: 6| Step: 6
Training loss: 1.7848997116088867
Validation loss: 2.1048730611801147

Epoch: 6| Step: 7
Training loss: 1.8592987060546875
Validation loss: 2.118527094523112

Epoch: 6| Step: 8
Training loss: 2.1300597190856934
Validation loss: 2.1081202824910483

Epoch: 6| Step: 9
Training loss: 1.6466772556304932
Validation loss: 2.096040407816569

Epoch: 6| Step: 10
Training loss: 2.5266735553741455
Validation loss: 2.111607829729716

Epoch: 6| Step: 11
Training loss: 1.8009684085845947
Validation loss: 2.1012844244639077

Epoch: 6| Step: 12
Training loss: 1.9141497611999512
Validation loss: 2.1198823650678

Epoch: 6| Step: 13
Training loss: 1.5750281810760498
Validation loss: 2.1219282746315002

Epoch: 195| Step: 0
Training loss: 2.134361743927002
Validation loss: 2.1243551770846048

Epoch: 6| Step: 1
Training loss: 1.519629716873169
Validation loss: 2.129061301549276

Epoch: 6| Step: 2
Training loss: 2.0284390449523926
Validation loss: 2.129526694615682

Epoch: 6| Step: 3
Training loss: 1.745736002922058
Validation loss: 2.121188441912333

Epoch: 6| Step: 4
Training loss: 2.392514228820801
Validation loss: 2.1240188678105674

Epoch: 6| Step: 5
Training loss: 2.241406202316284
Validation loss: 2.1191436449686685

Epoch: 6| Step: 6
Training loss: 1.6075242757797241
Validation loss: 2.117433945337931

Epoch: 6| Step: 7
Training loss: 2.3441944122314453
Validation loss: 2.129942615826925

Epoch: 6| Step: 8
Training loss: 1.8952022790908813
Validation loss: 2.127487381299337

Epoch: 6| Step: 9
Training loss: 1.5095140933990479
Validation loss: 2.1184224486351013

Epoch: 6| Step: 10
Training loss: 1.817581295967102
Validation loss: 2.123220761617025

Epoch: 6| Step: 11
Training loss: 1.6709774732589722
Validation loss: 2.139054616292318

Epoch: 6| Step: 12
Training loss: 2.684937000274658
Validation loss: 2.1208090583483377

Epoch: 6| Step: 13
Training loss: 1.6244834661483765
Validation loss: 2.1315422455469766

Epoch: 196| Step: 0
Training loss: 2.4964523315429688
Validation loss: 2.1152833501497903

Epoch: 6| Step: 1
Training loss: 1.817876935005188
Validation loss: 2.1376842061678567

Epoch: 6| Step: 2
Training loss: 1.64852774143219
Validation loss: 2.1449787418047586

Epoch: 6| Step: 3
Training loss: 2.5339035987854004
Validation loss: 2.1296430826187134

Epoch: 6| Step: 4
Training loss: 1.5401554107666016
Validation loss: 2.140260855356852

Epoch: 6| Step: 5
Training loss: 1.587073564529419
Validation loss: 2.1408321261405945

Epoch: 6| Step: 6
Training loss: 1.6760271787643433
Validation loss: 2.142162104447683

Epoch: 6| Step: 7
Training loss: 1.9463917016983032
Validation loss: 2.1231589317321777

Epoch: 6| Step: 8
Training loss: 2.4473679065704346
Validation loss: 2.1248775919278464

Epoch: 6| Step: 9
Training loss: 1.9144591093063354
Validation loss: 2.1237759788831077

Epoch: 6| Step: 10
Training loss: 1.8453478813171387
Validation loss: 2.1219335198402405

Epoch: 6| Step: 11
Training loss: 2.2232093811035156
Validation loss: 2.1243787010510764

Epoch: 6| Step: 12
Training loss: 1.2898162603378296
Validation loss: 2.1374991734822593

Epoch: 6| Step: 13
Training loss: 2.3891730308532715
Validation loss: 2.1311031579971313

Epoch: 197| Step: 0
Training loss: 2.0689492225646973
Validation loss: 2.1272934079170227

Epoch: 6| Step: 1
Training loss: 1.4485442638397217
Validation loss: 2.1418983737627664

Epoch: 6| Step: 2
Training loss: 1.6664783954620361
Validation loss: 2.122941474119822

Epoch: 6| Step: 3
Training loss: 2.2321889400482178
Validation loss: 2.1290977001190186

Epoch: 6| Step: 4
Training loss: 1.2887061834335327
Validation loss: 2.116880456606547

Epoch: 6| Step: 5
Training loss: 2.0583062171936035
Validation loss: 2.1212135553359985

Epoch: 6| Step: 6
Training loss: 1.7856783866882324
Validation loss: 2.1356605092684426

Epoch: 6| Step: 7
Training loss: 1.8800280094146729
Validation loss: 2.114709734916687

Epoch: 6| Step: 8
Training loss: 2.58634877204895
Validation loss: 2.1320586800575256

Epoch: 6| Step: 9
Training loss: 2.26955509185791
Validation loss: 2.132006744543711

Epoch: 6| Step: 10
Training loss: 2.3299241065979004
Validation loss: 2.127027948697408

Epoch: 6| Step: 11
Training loss: 2.1757140159606934
Validation loss: 2.119492471218109

Epoch: 6| Step: 12
Training loss: 1.8632259368896484
Validation loss: 2.0956385135650635

Epoch: 6| Step: 13
Training loss: 1.55354905128479
Validation loss: 2.106413781642914

Epoch: 198| Step: 0
Training loss: 1.7626965045928955
Validation loss: 2.0908813873926797

Epoch: 6| Step: 1
Training loss: 1.8904294967651367
Validation loss: 2.107628583908081

Epoch: 6| Step: 2
Training loss: 2.0273303985595703
Validation loss: 2.116916020711263

Epoch: 6| Step: 3
Training loss: 1.8748698234558105
Validation loss: 2.1008039315541587

Epoch: 6| Step: 4
Training loss: 1.9353008270263672
Validation loss: 2.1043145656585693

Epoch: 6| Step: 5
Training loss: 1.6206791400909424
Validation loss: 2.114736020565033

Epoch: 6| Step: 6
Training loss: 1.7244634628295898
Validation loss: 2.1107459465662637

Epoch: 6| Step: 7
Training loss: 2.231212615966797
Validation loss: 2.1187026302019754

Epoch: 6| Step: 8
Training loss: 1.9407739639282227
Validation loss: 2.122873544692993

Epoch: 6| Step: 9
Training loss: 2.4649384021759033
Validation loss: 2.1018724044164023

Epoch: 6| Step: 10
Training loss: 2.1925547122955322
Validation loss: 2.1159311731656394

Epoch: 6| Step: 11
Training loss: 2.0479280948638916
Validation loss: 2.1218412717183432

Epoch: 6| Step: 12
Training loss: 1.7805243730545044
Validation loss: 2.122681498527527

Epoch: 6| Step: 13
Training loss: 1.881277322769165
Validation loss: 2.1055648922920227

Epoch: 199| Step: 0
Training loss: 1.7503068447113037
Validation loss: 2.120097279548645

Epoch: 6| Step: 1
Training loss: 1.9963303804397583
Validation loss: 2.114222009976705

Epoch: 6| Step: 2
Training loss: 1.894695520401001
Validation loss: 2.1207674741744995

Epoch: 6| Step: 3
Training loss: 2.219984531402588
Validation loss: 2.12020472685496

Epoch: 6| Step: 4
Training loss: 1.7069456577301025
Validation loss: 2.1240051786104837

Epoch: 6| Step: 5
Training loss: 2.0026402473449707
Validation loss: 2.1094897389411926

Epoch: 6| Step: 6
Training loss: 2.3227829933166504
Validation loss: 2.1209482550621033

Epoch: 6| Step: 7
Training loss: 1.794664978981018
Validation loss: 2.1074143449465432

Epoch: 6| Step: 8
Training loss: 1.7806758880615234
Validation loss: 2.121268888314565

Epoch: 6| Step: 9
Training loss: 1.623465895652771
Validation loss: 2.1291038592656455

Epoch: 6| Step: 10
Training loss: 2.2161006927490234
Validation loss: 2.1258243123690286

Epoch: 6| Step: 11
Training loss: 2.252218008041382
Validation loss: 2.1369563937187195

Epoch: 6| Step: 12
Training loss: 1.4564152956008911
Validation loss: 2.140799880027771

Epoch: 6| Step: 13
Training loss: 2.1374106407165527
Validation loss: 2.1514644424120584

Epoch: 200| Step: 0
Training loss: 2.135188102722168
Validation loss: 2.146932899951935

Epoch: 6| Step: 1
Training loss: 2.4417428970336914
Validation loss: 2.152108907699585

Epoch: 6| Step: 2
Training loss: 2.178133726119995
Validation loss: 2.1463282704353333

Epoch: 6| Step: 3
Training loss: 1.9663145542144775
Validation loss: 2.1161709427833557

Epoch: 6| Step: 4
Training loss: 1.5527455806732178
Validation loss: 2.097220778465271

Epoch: 6| Step: 5
Training loss: 2.0992493629455566
Validation loss: 2.059040069580078

Epoch: 6| Step: 6
Training loss: 1.567289113998413
Validation loss: 2.0575366417566934

Epoch: 6| Step: 7
Training loss: 2.151925563812256
Validation loss: 2.069459239641825

Epoch: 6| Step: 8
Training loss: 2.0067825317382812
Validation loss: 2.077400326728821

Epoch: 6| Step: 9
Training loss: 1.6899784803390503
Validation loss: 2.07667871316274

Epoch: 6| Step: 10
Training loss: 1.4593253135681152
Validation loss: 2.086898148059845

Epoch: 6| Step: 11
Training loss: 2.600501537322998
Validation loss: 2.0938642024993896

Epoch: 6| Step: 12
Training loss: 1.672428846359253
Validation loss: 2.0797268748283386

Epoch: 6| Step: 13
Training loss: 2.5591704845428467
Validation loss: 2.0903279383977256

Epoch: 201| Step: 0
Training loss: 1.696317195892334
Validation loss: 2.0849812428156533

Epoch: 6| Step: 1
Training loss: 2.6212151050567627
Validation loss: 2.077098290125529

Epoch: 6| Step: 2
Training loss: 1.4370548725128174
Validation loss: 2.0833283265431723

Epoch: 6| Step: 3
Training loss: 2.0065977573394775
Validation loss: 2.0901423494021096

Epoch: 6| Step: 4
Training loss: 2.3876781463623047
Validation loss: 2.090049147605896

Epoch: 6| Step: 5
Training loss: 1.8482178449630737
Validation loss: 2.098739802837372

Epoch: 6| Step: 6
Training loss: 2.214390277862549
Validation loss: 2.085566222667694

Epoch: 6| Step: 7
Training loss: 2.3189315795898438
Validation loss: 2.080562392870585

Epoch: 6| Step: 8
Training loss: 1.9907804727554321
Validation loss: 2.0939236283302307

Epoch: 6| Step: 9
Training loss: 2.1424176692962646
Validation loss: 2.089963674545288

Epoch: 6| Step: 10
Training loss: 1.5830445289611816
Validation loss: 2.115557551383972

Epoch: 6| Step: 11
Training loss: 1.998872995376587
Validation loss: 2.1361132661501565

Epoch: 6| Step: 12
Training loss: 1.879013180732727
Validation loss: 2.1241184075673423

Epoch: 6| Step: 13
Training loss: 1.416800856590271
Validation loss: 2.1032251914342246

Epoch: 202| Step: 0
Training loss: 2.3834969997406006
Validation loss: 2.1261335810025535

Epoch: 6| Step: 1
Training loss: 2.0202531814575195
Validation loss: 2.1214794516563416

Epoch: 6| Step: 2
Training loss: 1.7637407779693604
Validation loss: 2.111856003602346

Epoch: 6| Step: 3
Training loss: 2.1635825634002686
Validation loss: 2.116119106610616

Epoch: 6| Step: 4
Training loss: 2.3270246982574463
Validation loss: 2.1245421965916953

Epoch: 6| Step: 5
Training loss: 2.4807541370391846
Validation loss: 2.1220282117525735

Epoch: 6| Step: 6
Training loss: 1.9087858200073242
Validation loss: 2.134274959564209

Epoch: 6| Step: 7
Training loss: 1.8255984783172607
Validation loss: 2.1186498204867044

Epoch: 6| Step: 8
Training loss: 2.2005326747894287
Validation loss: 2.1233166058858237

Epoch: 6| Step: 9
Training loss: 1.4505469799041748
Validation loss: 2.1288299361864724

Epoch: 6| Step: 10
Training loss: 1.8209408521652222
Validation loss: 2.1301520665486655

Epoch: 6| Step: 11
Training loss: 1.7758069038391113
Validation loss: 2.1268652280171714

Epoch: 6| Step: 12
Training loss: 1.8719842433929443
Validation loss: 2.133232355117798

Epoch: 6| Step: 13
Training loss: 1.0746392011642456
Validation loss: 2.1229090690612793

Epoch: 203| Step: 0
Training loss: 2.1602272987365723
Validation loss: 2.1162105798721313

Epoch: 6| Step: 1
Training loss: 2.314594030380249
Validation loss: 2.118648370107015

Epoch: 6| Step: 2
Training loss: 2.06099796295166
Validation loss: 2.1229538520177207

Epoch: 6| Step: 3
Training loss: 1.5498342514038086
Validation loss: 2.1187625726064048

Epoch: 6| Step: 4
Training loss: 2.2270116806030273
Validation loss: 2.107200503349304

Epoch: 6| Step: 5
Training loss: 1.6125097274780273
Validation loss: 2.11184553305308

Epoch: 6| Step: 6
Training loss: 1.7228697538375854
Validation loss: 2.1014281113942466

Epoch: 6| Step: 7
Training loss: 2.2036609649658203
Validation loss: 2.1194525559743247

Epoch: 6| Step: 8
Training loss: 1.8456237316131592
Validation loss: 2.109333078066508

Epoch: 6| Step: 9
Training loss: 1.760096788406372
Validation loss: 2.114234507083893

Epoch: 6| Step: 10
Training loss: 2.1114988327026367
Validation loss: 2.1166180968284607

Epoch: 6| Step: 11
Training loss: 1.33303964138031
Validation loss: 2.1177122592926025

Epoch: 6| Step: 12
Training loss: 1.796189308166504
Validation loss: 2.1016894578933716

Epoch: 6| Step: 13
Training loss: 2.5813112258911133
Validation loss: 2.1117473244667053

Epoch: 204| Step: 0
Training loss: 2.561147689819336
Validation loss: 2.1198336482048035

Epoch: 6| Step: 1
Training loss: 2.2916743755340576
Validation loss: 2.1223220030466714

Epoch: 6| Step: 2
Training loss: 1.5205236673355103
Validation loss: 2.1340668201446533

Epoch: 6| Step: 3
Training loss: 2.1096765995025635
Validation loss: 2.1160091956456504

Epoch: 6| Step: 4
Training loss: 2.4780454635620117
Validation loss: 2.114522715409597

Epoch: 6| Step: 5
Training loss: 2.009286403656006
Validation loss: 2.135607143243154

Epoch: 6| Step: 6
Training loss: 1.4171769618988037
Validation loss: 2.1273158391316733

Epoch: 6| Step: 7
Training loss: 1.7678563594818115
Validation loss: 2.1291616559028625

Epoch: 6| Step: 8
Training loss: 1.7745879888534546
Validation loss: 2.1331573526064553

Epoch: 6| Step: 9
Training loss: 1.9697835445404053
Validation loss: 2.1196249524752298

Epoch: 6| Step: 10
Training loss: 1.8886973857879639
Validation loss: 2.1215383609135947

Epoch: 6| Step: 11
Training loss: 2.078127384185791
Validation loss: 2.125891129175822

Epoch: 6| Step: 12
Training loss: 1.5162967443466187
Validation loss: 2.125820060571035

Epoch: 6| Step: 13
Training loss: 2.0163440704345703
Validation loss: 2.1276535987854004

Epoch: 205| Step: 0
Training loss: 1.6759274005889893
Validation loss: 2.13968163728714

Epoch: 6| Step: 1
Training loss: 2.163783073425293
Validation loss: 2.1311088403066

Epoch: 6| Step: 2
Training loss: 1.5943944454193115
Validation loss: 2.1381709774335227

Epoch: 6| Step: 3
Training loss: 1.6514848470687866
Validation loss: 2.140255848566691

Epoch: 6| Step: 4
Training loss: 1.6457178592681885
Validation loss: 2.1380669474601746

Epoch: 6| Step: 5
Training loss: 2.2269628047943115
Validation loss: 2.1433066924413047

Epoch: 6| Step: 6
Training loss: 1.9733922481536865
Validation loss: 2.1228758295377097

Epoch: 6| Step: 7
Training loss: 2.4935519695281982
Validation loss: 2.137663642565409

Epoch: 6| Step: 8
Training loss: 2.4608254432678223
Validation loss: 2.1327587167421975

Epoch: 6| Step: 9
Training loss: 1.6452503204345703
Validation loss: 2.1130759914716086

Epoch: 6| Step: 10
Training loss: 2.212340831756592
Validation loss: 2.131285230318705

Epoch: 6| Step: 11
Training loss: 1.75711190700531
Validation loss: 2.1449157198270163

Epoch: 6| Step: 12
Training loss: 1.6636971235275269
Validation loss: 2.1307707031567893

Epoch: 6| Step: 13
Training loss: 2.1068978309631348
Validation loss: 2.135278026262919

Epoch: 206| Step: 0
Training loss: 1.8756030797958374
Validation loss: 2.1248276829719543

Epoch: 6| Step: 1
Training loss: 2.143446207046509
Validation loss: 2.127977967262268

Epoch: 6| Step: 2
Training loss: 1.9144587516784668
Validation loss: 2.1259101827939353

Epoch: 6| Step: 3
Training loss: 1.5840470790863037
Validation loss: 2.113989472389221

Epoch: 6| Step: 4
Training loss: 2.062988042831421
Validation loss: 2.1244837045669556

Epoch: 6| Step: 5
Training loss: 1.7909348011016846
Validation loss: 2.1182910998662314

Epoch: 6| Step: 6
Training loss: 2.328024387359619
Validation loss: 2.1446810166041055

Epoch: 6| Step: 7
Training loss: 2.0545661449432373
Validation loss: 2.154336432615916

Epoch: 6| Step: 8
Training loss: 1.5430458784103394
Validation loss: 2.140195667743683

Epoch: 6| Step: 9
Training loss: 2.2188963890075684
Validation loss: 2.169992903868357

Epoch: 6| Step: 10
Training loss: 1.8686137199401855
Validation loss: 2.13600746790568

Epoch: 6| Step: 11
Training loss: 2.0601985454559326
Validation loss: 2.1563519636789956

Epoch: 6| Step: 12
Training loss: 2.407993793487549
Validation loss: 2.163613816102346

Epoch: 6| Step: 13
Training loss: 1.2965506315231323
Validation loss: 2.146667937437693

Epoch: 207| Step: 0
Training loss: 2.3674888610839844
Validation loss: 2.1294926007588706

Epoch: 6| Step: 1
Training loss: 1.8040167093276978
Validation loss: 2.125602344671885

Epoch: 6| Step: 2
Training loss: 2.2829861640930176
Validation loss: 2.113272806008657

Epoch: 6| Step: 3
Training loss: 1.7280244827270508
Validation loss: 2.1029560963312783

Epoch: 6| Step: 4
Training loss: 2.2238121032714844
Validation loss: 2.1125758488972983

Epoch: 6| Step: 5
Training loss: 1.870361089706421
Validation loss: 2.103601415952047

Epoch: 6| Step: 6
Training loss: 2.0499465465545654
Validation loss: 2.101040263970693

Epoch: 6| Step: 7
Training loss: 1.9757020473480225
Validation loss: 2.0955705444018045

Epoch: 6| Step: 8
Training loss: 1.9704865217208862
Validation loss: 2.103453775246938

Epoch: 6| Step: 9
Training loss: 1.9054864645004272
Validation loss: 2.097790757815043

Epoch: 6| Step: 10
Training loss: 1.6017491817474365
Validation loss: 2.100292146205902

Epoch: 6| Step: 11
Training loss: 2.0321574211120605
Validation loss: 2.0975791613260903

Epoch: 6| Step: 12
Training loss: 1.9409314393997192
Validation loss: 2.1137985587120056

Epoch: 6| Step: 13
Training loss: 1.9316539764404297
Validation loss: 2.1134594877560935

Epoch: 208| Step: 0
Training loss: 1.5157479047775269
Validation loss: 2.1188246607780457

Epoch: 6| Step: 1
Training loss: 1.7048187255859375
Validation loss: 2.1387484471003213

Epoch: 6| Step: 2
Training loss: 1.9540247917175293
Validation loss: 2.1319526632626853

Epoch: 6| Step: 3
Training loss: 1.5852843523025513
Validation loss: 2.1249685287475586

Epoch: 6| Step: 4
Training loss: 1.7404201030731201
Validation loss: 2.1277908285458884

Epoch: 6| Step: 5
Training loss: 1.9140257835388184
Validation loss: 2.1367726723353067

Epoch: 6| Step: 6
Training loss: 2.0555150508880615
Validation loss: 2.133881171544393

Epoch: 6| Step: 7
Training loss: 2.3825502395629883
Validation loss: 2.1155113776524863

Epoch: 6| Step: 8
Training loss: 1.975982427597046
Validation loss: 2.122483213742574

Epoch: 6| Step: 9
Training loss: 1.9062204360961914
Validation loss: 2.132373054822286

Epoch: 6| Step: 10
Training loss: 2.859210968017578
Validation loss: 2.1342009902000427

Epoch: 6| Step: 11
Training loss: 1.284315824508667
Validation loss: 2.14553701877594

Epoch: 6| Step: 12
Training loss: 2.365997791290283
Validation loss: 2.1390857696533203

Epoch: 6| Step: 13
Training loss: 1.6318295001983643
Validation loss: 2.135628879070282

Epoch: 209| Step: 0
Training loss: 1.8572280406951904
Validation loss: 2.1626939376195273

Epoch: 6| Step: 1
Training loss: 2.0694990158081055
Validation loss: 2.1406791607538858

Epoch: 6| Step: 2
Training loss: 1.9321796894073486
Validation loss: 2.1714384953180947

Epoch: 6| Step: 3
Training loss: 1.6330009698867798
Validation loss: 2.1589207847913108

Epoch: 6| Step: 4
Training loss: 1.7300043106079102
Validation loss: 2.156227231025696

Epoch: 6| Step: 5
Training loss: 2.4188358783721924
Validation loss: 2.166391988595327

Epoch: 6| Step: 6
Training loss: 1.8029556274414062
Validation loss: 2.1561101277669272

Epoch: 6| Step: 7
Training loss: 2.0706722736358643
Validation loss: 2.1618316570917764

Epoch: 6| Step: 8
Training loss: 1.600634217262268
Validation loss: 2.156166215737661

Epoch: 6| Step: 9
Training loss: 1.3242359161376953
Validation loss: 2.1481881141662598

Epoch: 6| Step: 10
Training loss: 2.5592541694641113
Validation loss: 2.1454955339431763

Epoch: 6| Step: 11
Training loss: 2.218794822692871
Validation loss: 2.1386817495028176

Epoch: 6| Step: 12
Training loss: 1.9006290435791016
Validation loss: 2.127183218797048

Epoch: 6| Step: 13
Training loss: 1.5935771465301514
Validation loss: 2.1220395962397256

Epoch: 210| Step: 0
Training loss: 1.8097760677337646
Validation loss: 2.1288719375928244

Epoch: 6| Step: 1
Training loss: 2.2603721618652344
Validation loss: 2.129287521044413

Epoch: 6| Step: 2
Training loss: 2.3553924560546875
Validation loss: 2.130178610483805

Epoch: 6| Step: 3
Training loss: 1.2454314231872559
Validation loss: 2.135404944419861

Epoch: 6| Step: 4
Training loss: 2.3796226978302
Validation loss: 2.139824867248535

Epoch: 6| Step: 5
Training loss: 1.6082379817962646
Validation loss: 2.13483993212382

Epoch: 6| Step: 6
Training loss: 1.7882676124572754
Validation loss: 2.128971834977468

Epoch: 6| Step: 7
Training loss: 2.2435200214385986
Validation loss: 2.1414862672487893

Epoch: 6| Step: 8
Training loss: 1.9837532043457031
Validation loss: 2.153259297211965

Epoch: 6| Step: 9
Training loss: 1.9032686948776245
Validation loss: 2.173665185769399

Epoch: 6| Step: 10
Training loss: 1.9556255340576172
Validation loss: 2.153708755970001

Epoch: 6| Step: 11
Training loss: 1.7220884561538696
Validation loss: 2.1625062624613443

Epoch: 6| Step: 12
Training loss: 1.778707504272461
Validation loss: 2.1536583503087363

Epoch: 6| Step: 13
Training loss: 2.1602330207824707
Validation loss: 2.1454598903656006

Epoch: 211| Step: 0
Training loss: 2.4063425064086914
Validation loss: 2.1428070664405823

Epoch: 6| Step: 1
Training loss: 1.8640601634979248
Validation loss: 2.1167946457862854

Epoch: 6| Step: 2
Training loss: 2.393218517303467
Validation loss: 2.10973858833313

Epoch: 6| Step: 3
Training loss: 1.8793166875839233
Validation loss: 2.1033944884936013

Epoch: 6| Step: 4
Training loss: 1.8073787689208984
Validation loss: 2.1010402043660483

Epoch: 6| Step: 5
Training loss: 1.7978744506835938
Validation loss: 2.114415486653646

Epoch: 6| Step: 6
Training loss: 1.9881243705749512
Validation loss: 2.1116231878598533

Epoch: 6| Step: 7
Training loss: 2.544353485107422
Validation loss: 2.123935798803965

Epoch: 6| Step: 8
Training loss: 1.569223165512085
Validation loss: 2.112849712371826

Epoch: 6| Step: 9
Training loss: 2.304560422897339
Validation loss: 2.127446413040161

Epoch: 6| Step: 10
Training loss: 1.953597068786621
Validation loss: 2.1212721864382424

Epoch: 6| Step: 11
Training loss: 1.3992979526519775
Validation loss: 2.1214528679847717

Epoch: 6| Step: 12
Training loss: 1.6050350666046143
Validation loss: 2.1284911235173545

Epoch: 6| Step: 13
Training loss: 1.934152364730835
Validation loss: 2.1183022061983743

Epoch: 212| Step: 0
Training loss: 1.567087173461914
Validation loss: 2.1284202138582864

Epoch: 6| Step: 1
Training loss: 2.1056394577026367
Validation loss: 2.1232109864552817

Epoch: 6| Step: 2
Training loss: 1.8387269973754883
Validation loss: 2.1261634627978006

Epoch: 6| Step: 3
Training loss: 1.8444240093231201
Validation loss: 2.1368775367736816

Epoch: 6| Step: 4
Training loss: 2.9521727561950684
Validation loss: 2.123052438100179

Epoch: 6| Step: 5
Training loss: 2.195803642272949
Validation loss: 2.136270026365916

Epoch: 6| Step: 6
Training loss: 1.9416494369506836
Validation loss: 2.130965232849121

Epoch: 6| Step: 7
Training loss: 1.7526237964630127
Validation loss: 2.13281379143397

Epoch: 6| Step: 8
Training loss: 1.551823616027832
Validation loss: 2.11588720480601

Epoch: 6| Step: 9
Training loss: 1.7329498529434204
Validation loss: 2.1323291261990867

Epoch: 6| Step: 10
Training loss: 1.8537594079971313
Validation loss: 2.1236847241719565

Epoch: 6| Step: 11
Training loss: 2.189107894897461
Validation loss: 2.146563728650411

Epoch: 6| Step: 12
Training loss: 1.5475904941558838
Validation loss: 2.1404101649920144

Epoch: 6| Step: 13
Training loss: 2.118588447570801
Validation loss: 2.1421449184417725

Epoch: 213| Step: 0
Training loss: 1.5756527185440063
Validation loss: 2.137867252031962

Epoch: 6| Step: 1
Training loss: 1.9229304790496826
Validation loss: 2.143336375554403

Epoch: 6| Step: 2
Training loss: 1.1816809177398682
Validation loss: 2.1368277867635093

Epoch: 6| Step: 3
Training loss: 2.2769250869750977
Validation loss: 2.134498357772827

Epoch: 6| Step: 4
Training loss: 1.7719110250473022
Validation loss: 2.121107200781504

Epoch: 6| Step: 5
Training loss: 1.9621092081069946
Validation loss: 2.1284175912539163

Epoch: 6| Step: 6
Training loss: 2.184971809387207
Validation loss: 2.1234529415766397

Epoch: 6| Step: 7
Training loss: 2.558351516723633
Validation loss: 2.122055451075236

Epoch: 6| Step: 8
Training loss: 2.0344839096069336
Validation loss: 2.129956603050232

Epoch: 6| Step: 9
Training loss: 2.125535726547241
Validation loss: 2.125203788280487

Epoch: 6| Step: 10
Training loss: 2.0753097534179688
Validation loss: 2.129040996233622

Epoch: 6| Step: 11
Training loss: 2.2989442348480225
Validation loss: 2.1341694394747415

Epoch: 6| Step: 12
Training loss: 1.0550379753112793
Validation loss: 2.1344603498776755

Epoch: 6| Step: 13
Training loss: 1.705163598060608
Validation loss: 2.1311370531717935

Epoch: 214| Step: 0
Training loss: 2.187532901763916
Validation loss: 2.1459795037905374

Epoch: 6| Step: 1
Training loss: 1.547444224357605
Validation loss: 2.1433270970980325

Epoch: 6| Step: 2
Training loss: 2.5066933631896973
Validation loss: 2.156372845172882

Epoch: 6| Step: 3
Training loss: 1.6131362915039062
Validation loss: 2.140701651573181

Epoch: 6| Step: 4
Training loss: 1.4872760772705078
Validation loss: 2.1529467503229776

Epoch: 6| Step: 5
Training loss: 1.9546970129013062
Validation loss: 2.1512535015741983

Epoch: 6| Step: 6
Training loss: 2.100489854812622
Validation loss: 2.154260059197744

Epoch: 6| Step: 7
Training loss: 1.6214327812194824
Validation loss: 2.144443909327189

Epoch: 6| Step: 8
Training loss: 2.0726640224456787
Validation loss: 2.150295376777649

Epoch: 6| Step: 9
Training loss: 2.1450307369232178
Validation loss: 2.14469983180364

Epoch: 6| Step: 10
Training loss: 2.5418453216552734
Validation loss: 2.1353083650271096

Epoch: 6| Step: 11
Training loss: 1.705941915512085
Validation loss: 2.136162281036377

Epoch: 6| Step: 12
Training loss: 1.3021762371063232
Validation loss: 2.1364306608835855

Epoch: 6| Step: 13
Training loss: 1.8274016380310059
Validation loss: 2.1326828400293985

Epoch: 215| Step: 0
Training loss: 1.6496644020080566
Validation loss: 2.1371702949206033

Epoch: 6| Step: 1
Training loss: 1.4648065567016602
Validation loss: 2.138563573360443

Epoch: 6| Step: 2
Training loss: 1.8271539211273193
Validation loss: 2.1561644275983176

Epoch: 6| Step: 3
Training loss: 2.434807300567627
Validation loss: 2.144170105457306

Epoch: 6| Step: 4
Training loss: 2.0366084575653076
Validation loss: 2.1557441552480063

Epoch: 6| Step: 5
Training loss: 1.9043720960617065
Validation loss: 2.1444783012072244

Epoch: 6| Step: 6
Training loss: 1.3135795593261719
Validation loss: 2.140706996122996

Epoch: 6| Step: 7
Training loss: 1.9287023544311523
Validation loss: 2.1653484106063843

Epoch: 6| Step: 8
Training loss: 2.2711782455444336
Validation loss: 2.1767699917157493

Epoch: 6| Step: 9
Training loss: 1.9248979091644287
Validation loss: 2.1581630309422812

Epoch: 6| Step: 10
Training loss: 1.7686117887496948
Validation loss: 2.1834049622217813

Epoch: 6| Step: 11
Training loss: 1.8955154418945312
Validation loss: 2.1965020497639975

Epoch: 6| Step: 12
Training loss: 2.3751537799835205
Validation loss: 2.219377815723419

Epoch: 6| Step: 13
Training loss: 1.7632710933685303
Validation loss: 2.208944082260132

Epoch: 216| Step: 0
Training loss: 1.7781248092651367
Validation loss: 2.1929024855295816

Epoch: 6| Step: 1
Training loss: 1.9278230667114258
Validation loss: 2.1732781529426575

Epoch: 6| Step: 2
Training loss: 1.6284244060516357
Validation loss: 2.1538048585255942

Epoch: 6| Step: 3
Training loss: 2.3962535858154297
Validation loss: 2.1533647974332175

Epoch: 6| Step: 4
Training loss: 2.4988021850585938
Validation loss: 2.1364748080571494

Epoch: 6| Step: 5
Training loss: 1.6798338890075684
Validation loss: 2.1540043354034424

Epoch: 6| Step: 6
Training loss: 1.7487632036209106
Validation loss: 2.1342729926109314

Epoch: 6| Step: 7
Training loss: 2.041260242462158
Validation loss: 2.1573227047920227

Epoch: 6| Step: 8
Training loss: 1.8148468732833862
Validation loss: 2.1378990610440574

Epoch: 6| Step: 9
Training loss: 2.3746399879455566
Validation loss: 2.1377492944399514

Epoch: 6| Step: 10
Training loss: 1.4856016635894775
Validation loss: 2.1598386764526367

Epoch: 6| Step: 11
Training loss: 1.4155831336975098
Validation loss: 2.145862360795339

Epoch: 6| Step: 12
Training loss: 1.9405261278152466
Validation loss: 2.1473673780759177

Epoch: 6| Step: 13
Training loss: 2.3145592212677
Validation loss: 2.14263653755188

Epoch: 217| Step: 0
Training loss: 2.105964183807373
Validation loss: 2.1455564300219216

Epoch: 6| Step: 1
Training loss: 2.353394031524658
Validation loss: 2.1580807169278464

Epoch: 6| Step: 2
Training loss: 1.7928094863891602
Validation loss: 2.178966701030731

Epoch: 6| Step: 3
Training loss: 1.6549208164215088
Validation loss: 2.168384373188019

Epoch: 6| Step: 4
Training loss: 1.736584186553955
Validation loss: 2.159060835838318

Epoch: 6| Step: 5
Training loss: 1.665358066558838
Validation loss: 2.1657750606536865

Epoch: 6| Step: 6
Training loss: 2.0249485969543457
Validation loss: 2.1503081917762756

Epoch: 6| Step: 7
Training loss: 1.922419548034668
Validation loss: 2.1362406611442566

Epoch: 6| Step: 8
Training loss: 1.8485994338989258
Validation loss: 2.1417062282562256

Epoch: 6| Step: 9
Training loss: 1.9806296825408936
Validation loss: 2.1484121481577554

Epoch: 6| Step: 10
Training loss: 2.9239749908447266
Validation loss: 2.147975265979767

Epoch: 6| Step: 11
Training loss: 1.2399423122406006
Validation loss: 2.146955053011576

Epoch: 6| Step: 12
Training loss: 1.4138054847717285
Validation loss: 2.1382229924201965

Epoch: 6| Step: 13
Training loss: 2.2656912803649902
Validation loss: 2.1514934500058494

Epoch: 218| Step: 0
Training loss: 2.108829975128174
Validation loss: 2.139120101928711

Epoch: 6| Step: 1
Training loss: 1.6870954036712646
Validation loss: 2.1406665643056235

Epoch: 6| Step: 2
Training loss: 2.434429407119751
Validation loss: 2.124408781528473

Epoch: 6| Step: 3
Training loss: 1.7072455883026123
Validation loss: 2.150370180606842

Epoch: 6| Step: 4
Training loss: 1.5569026470184326
Validation loss: 2.1551141341527305

Epoch: 6| Step: 5
Training loss: 1.8969814777374268
Validation loss: 2.1519477367401123

Epoch: 6| Step: 6
Training loss: 1.6056525707244873
Validation loss: 2.152479569117228

Epoch: 6| Step: 7
Training loss: 1.8005291223526
Validation loss: 2.1634758710861206

Epoch: 6| Step: 8
Training loss: 1.971124291419983
Validation loss: 2.1582956314086914

Epoch: 6| Step: 9
Training loss: 2.0162272453308105
Validation loss: 2.1595092018445334

Epoch: 6| Step: 10
Training loss: 2.04856014251709
Validation loss: 2.1593141754468284

Epoch: 6| Step: 11
Training loss: 2.3148436546325684
Validation loss: 2.1679537296295166

Epoch: 6| Step: 12
Training loss: 1.6502758264541626
Validation loss: 2.157539407412211

Epoch: 6| Step: 13
Training loss: 1.9116742610931396
Validation loss: 2.153090496857961

Epoch: 219| Step: 0
Training loss: 1.668372392654419
Validation loss: 2.1538920402526855

Epoch: 6| Step: 1
Training loss: 2.531330108642578
Validation loss: 2.1635661919911704

Epoch: 6| Step: 2
Training loss: 2.053267478942871
Validation loss: 2.1437647938728333

Epoch: 6| Step: 3
Training loss: 2.2889223098754883
Validation loss: 2.133920987447103

Epoch: 6| Step: 4
Training loss: 1.6735382080078125
Validation loss: 2.1396710872650146

Epoch: 6| Step: 5
Training loss: 2.0156912803649902
Validation loss: 2.166405121485392

Epoch: 6| Step: 6
Training loss: 2.0965185165405273
Validation loss: 2.1349662939707437

Epoch: 6| Step: 7
Training loss: 1.8964083194732666
Validation loss: 2.1402846376101174

Epoch: 6| Step: 8
Training loss: 2.2914321422576904
Validation loss: 2.1395761370658875

Epoch: 6| Step: 9
Training loss: 1.4075267314910889
Validation loss: 2.14732418457667

Epoch: 6| Step: 10
Training loss: 1.9943305253982544
Validation loss: 2.1306320627530417

Epoch: 6| Step: 11
Training loss: 1.7514910697937012
Validation loss: 2.1364755233128867

Epoch: 6| Step: 12
Training loss: 1.5488742589950562
Validation loss: 2.116747478644053

Epoch: 6| Step: 13
Training loss: 1.7276263236999512
Validation loss: 2.130173941453298

Epoch: 220| Step: 0
Training loss: 2.1054840087890625
Validation loss: 2.117009917894999

Epoch: 6| Step: 1
Training loss: 1.9350333213806152
Validation loss: 2.1289822459220886

Epoch: 6| Step: 2
Training loss: 1.711137294769287
Validation loss: 2.1430891156196594

Epoch: 6| Step: 3
Training loss: 1.9434367418289185
Validation loss: 2.1362234354019165

Epoch: 6| Step: 4
Training loss: 1.9474828243255615
Validation loss: 2.1325515508651733

Epoch: 6| Step: 5
Training loss: 1.5639489889144897
Validation loss: 2.1263080636660256

Epoch: 6| Step: 6
Training loss: 1.4252524375915527
Validation loss: 2.159426967302958

Epoch: 6| Step: 7
Training loss: 1.867873191833496
Validation loss: 2.149343967437744

Epoch: 6| Step: 8
Training loss: 1.8048560619354248
Validation loss: 2.1562393506368003

Epoch: 6| Step: 9
Training loss: 2.2777724266052246
Validation loss: 2.182579676310221

Epoch: 6| Step: 10
Training loss: 2.392164707183838
Validation loss: 2.181053340435028

Epoch: 6| Step: 11
Training loss: 2.4117588996887207
Validation loss: 2.174755891164144

Epoch: 6| Step: 12
Training loss: 1.7212893962860107
Validation loss: 2.1804048220316568

Epoch: 6| Step: 13
Training loss: 1.371178150177002
Validation loss: 2.181122879187266

Epoch: 221| Step: 0
Training loss: 2.082310676574707
Validation loss: 2.1821822921435037

Epoch: 6| Step: 1
Training loss: 1.4639352560043335
Validation loss: 2.155621608098348

Epoch: 6| Step: 2
Training loss: 2.3546276092529297
Validation loss: 2.165959417819977

Epoch: 6| Step: 3
Training loss: 1.4203987121582031
Validation loss: 2.167506297429403

Epoch: 6| Step: 4
Training loss: 1.921144723892212
Validation loss: 2.1684266328811646

Epoch: 6| Step: 5
Training loss: 2.2425498962402344
Validation loss: 2.1536580125490823

Epoch: 6| Step: 6
Training loss: 2.094945192337036
Validation loss: 2.156796852747599

Epoch: 6| Step: 7
Training loss: 1.727825403213501
Validation loss: 2.15981125831604

Epoch: 6| Step: 8
Training loss: 1.9749151468276978
Validation loss: 2.157700796922048

Epoch: 6| Step: 9
Training loss: 1.716653823852539
Validation loss: 2.156228721141815

Epoch: 6| Step: 10
Training loss: 1.7427403926849365
Validation loss: 2.1516387859980264

Epoch: 6| Step: 11
Training loss: 1.8270609378814697
Validation loss: 2.161998152732849

Epoch: 6| Step: 12
Training loss: 1.7681150436401367
Validation loss: 2.155902326107025

Epoch: 6| Step: 13
Training loss: 2.0655102729797363
Validation loss: 2.165645122528076

Epoch: 222| Step: 0
Training loss: 1.784960389137268
Validation loss: 2.1346664826075235

Epoch: 6| Step: 1
Training loss: 1.7120169401168823
Validation loss: 2.149655838807424

Epoch: 6| Step: 2
Training loss: 2.3045084476470947
Validation loss: 2.1435303688049316

Epoch: 6| Step: 3
Training loss: 1.349553108215332
Validation loss: 2.144782543182373

Epoch: 6| Step: 4
Training loss: 1.8521510362625122
Validation loss: 2.1438414653142295

Epoch: 6| Step: 5
Training loss: 1.812569499015808
Validation loss: 2.1456390420595803

Epoch: 6| Step: 6
Training loss: 1.7406516075134277
Validation loss: 2.139879584312439

Epoch: 6| Step: 7
Training loss: 1.672684907913208
Validation loss: 2.162467300891876

Epoch: 6| Step: 8
Training loss: 1.9881901741027832
Validation loss: 2.158298075199127

Epoch: 6| Step: 9
Training loss: 1.9125343561172485
Validation loss: 2.170348286628723

Epoch: 6| Step: 10
Training loss: 1.4528422355651855
Validation loss: 2.172681530316671

Epoch: 6| Step: 11
Training loss: 2.3603692054748535
Validation loss: 2.180636008580526

Epoch: 6| Step: 12
Training loss: 2.4120683670043945
Validation loss: 2.1801220377286277

Epoch: 6| Step: 13
Training loss: 1.9590628147125244
Validation loss: 2.188039461771647

Epoch: 223| Step: 0
Training loss: 1.6121609210968018
Validation loss: 2.1961611906687417

Epoch: 6| Step: 1
Training loss: 2.1312010288238525
Validation loss: 2.1843250393867493

Epoch: 6| Step: 2
Training loss: 1.9935150146484375
Validation loss: 2.1616466840108237

Epoch: 6| Step: 3
Training loss: 1.8834738731384277
Validation loss: 2.147714674472809

Epoch: 6| Step: 4
Training loss: 1.8566378355026245
Validation loss: 2.147440016269684

Epoch: 6| Step: 5
Training loss: 2.101097583770752
Validation loss: 2.157306353251139

Epoch: 6| Step: 6
Training loss: 1.4006661176681519
Validation loss: 2.144986569881439

Epoch: 6| Step: 7
Training loss: 1.3588000535964966
Validation loss: 2.1314255396525064

Epoch: 6| Step: 8
Training loss: 2.045435905456543
Validation loss: 2.1395947535832724

Epoch: 6| Step: 9
Training loss: 1.968989610671997
Validation loss: 2.1597625811894736

Epoch: 6| Step: 10
Training loss: 2.1503632068634033
Validation loss: 2.159676730632782

Epoch: 6| Step: 11
Training loss: 1.842524766921997
Validation loss: 2.1409197052319846

Epoch: 6| Step: 12
Training loss: 1.2784240245819092
Validation loss: 2.168989340464274

Epoch: 6| Step: 13
Training loss: 2.542060613632202
Validation loss: 2.1564718882242837

Epoch: 224| Step: 0
Training loss: 1.8868441581726074
Validation loss: 2.155439813931783

Epoch: 6| Step: 1
Training loss: 2.036907434463501
Validation loss: 2.1447555820147195

Epoch: 6| Step: 2
Training loss: 2.225473403930664
Validation loss: 2.1630050341288247

Epoch: 6| Step: 3
Training loss: 1.4478625059127808
Validation loss: 2.1299099922180176

Epoch: 6| Step: 4
Training loss: 1.8608862161636353
Validation loss: 2.1525179346402488

Epoch: 6| Step: 5
Training loss: 1.6629512310028076
Validation loss: 2.1516006787618003

Epoch: 6| Step: 6
Training loss: 2.0964479446411133
Validation loss: 2.14466659228007

Epoch: 6| Step: 7
Training loss: 1.666687250137329
Validation loss: 2.1600786248842874

Epoch: 6| Step: 8
Training loss: 2.3167309761047363
Validation loss: 2.1564400792121887

Epoch: 6| Step: 9
Training loss: 1.9374947547912598
Validation loss: 2.160290459791819

Epoch: 6| Step: 10
Training loss: 1.793731689453125
Validation loss: 2.138517121473948

Epoch: 6| Step: 11
Training loss: 1.7633191347122192
Validation loss: 2.1439013679822287

Epoch: 6| Step: 12
Training loss: 1.8063297271728516
Validation loss: 2.145820220311483

Epoch: 6| Step: 13
Training loss: 1.8821258544921875
Validation loss: 2.1283363898595176

Epoch: 225| Step: 0
Training loss: 1.6799565553665161
Validation loss: 2.1405654350916543

Epoch: 6| Step: 1
Training loss: 2.4314584732055664
Validation loss: 2.144686778386434

Epoch: 6| Step: 2
Training loss: 1.9043689966201782
Validation loss: 2.138578712940216

Epoch: 6| Step: 3
Training loss: 2.4892899990081787
Validation loss: 2.140529195467631

Epoch: 6| Step: 4
Training loss: 2.3924484252929688
Validation loss: 2.146161198616028

Epoch: 6| Step: 5
Training loss: 2.3503665924072266
Validation loss: 2.1456714868545532

Epoch: 6| Step: 6
Training loss: 1.6783406734466553
Validation loss: 2.1292423804601035

Epoch: 6| Step: 7
Training loss: 1.6464943885803223
Validation loss: 2.1503307620684304

Epoch: 6| Step: 8
Training loss: 1.4603354930877686
Validation loss: 2.144519865512848

Epoch: 6| Step: 9
Training loss: 1.6387219429016113
Validation loss: 2.1531068285306296

Epoch: 6| Step: 10
Training loss: 1.9877943992614746
Validation loss: 2.15756764014562

Epoch: 6| Step: 11
Training loss: 2.0419979095458984
Validation loss: 2.1574886639912925

Epoch: 6| Step: 12
Training loss: 1.815591812133789
Validation loss: 2.148364504178365

Epoch: 6| Step: 13
Training loss: 1.455843210220337
Validation loss: 2.1578749418258667

Epoch: 226| Step: 0
Training loss: 1.1942553520202637
Validation loss: 2.1466402808825173

Epoch: 6| Step: 1
Training loss: 1.5796797275543213
Validation loss: 2.165393869082133

Epoch: 6| Step: 2
Training loss: 1.8711376190185547
Validation loss: 2.1597668528556824

Epoch: 6| Step: 3
Training loss: 1.6679637432098389
Validation loss: 2.170922577381134

Epoch: 6| Step: 4
Training loss: 1.7477811574935913
Validation loss: 2.1562769214312234

Epoch: 6| Step: 5
Training loss: 2.094956874847412
Validation loss: 2.163209637006124

Epoch: 6| Step: 6
Training loss: 1.6370735168457031
Validation loss: 2.1426824728647866

Epoch: 6| Step: 7
Training loss: 2.3561148643493652
Validation loss: 2.133912682533264

Epoch: 6| Step: 8
Training loss: 2.0651450157165527
Validation loss: 2.1395512024561563

Epoch: 6| Step: 9
Training loss: 1.5327796936035156
Validation loss: 2.1225906213124595

Epoch: 6| Step: 10
Training loss: 1.9846340417861938
Validation loss: 2.137820303440094

Epoch: 6| Step: 11
Training loss: 1.2177507877349854
Validation loss: 2.141944090525309

Epoch: 6| Step: 12
Training loss: 2.061584949493408
Validation loss: 2.13829114039739

Epoch: 6| Step: 13
Training loss: 3.2422194480895996
Validation loss: 2.134195109208425

Epoch: 227| Step: 0
Training loss: 2.1216635704040527
Validation loss: 2.125311811765035

Epoch: 6| Step: 1
Training loss: 1.9534181356430054
Validation loss: 2.127834737300873

Epoch: 6| Step: 2
Training loss: 2.558074474334717
Validation loss: 2.131764312585195

Epoch: 6| Step: 3
Training loss: 1.5059254169464111
Validation loss: 2.129980981349945

Epoch: 6| Step: 4
Training loss: 1.1976501941680908
Validation loss: 2.1312653621037803

Epoch: 6| Step: 5
Training loss: 2.2585291862487793
Validation loss: 2.1299700339635215

Epoch: 6| Step: 6
Training loss: 1.6298738718032837
Validation loss: 2.1239662170410156

Epoch: 6| Step: 7
Training loss: 1.8579907417297363
Validation loss: 2.133394718170166

Epoch: 6| Step: 8
Training loss: 2.078357696533203
Validation loss: 2.128944377104441

Epoch: 6| Step: 9
Training loss: 1.4718174934387207
Validation loss: 2.1235298911730447

Epoch: 6| Step: 10
Training loss: 1.532402753829956
Validation loss: 2.1484912236531577

Epoch: 6| Step: 11
Training loss: 2.492008924484253
Validation loss: 2.1744481325149536

Epoch: 6| Step: 12
Training loss: 2.200453042984009
Validation loss: 2.1673404773076377

Epoch: 6| Step: 13
Training loss: 2.066227436065674
Validation loss: 2.1560563445091248

Epoch: 228| Step: 0
Training loss: 2.3053126335144043
Validation loss: 2.16743540763855

Epoch: 6| Step: 1
Training loss: 2.2686328887939453
Validation loss: 2.1543558835983276

Epoch: 6| Step: 2
Training loss: 1.6793544292449951
Validation loss: 2.1725751558939614

Epoch: 6| Step: 3
Training loss: 1.4611961841583252
Validation loss: 2.149215340614319

Epoch: 6| Step: 4
Training loss: 1.6762938499450684
Validation loss: 2.1645272374153137

Epoch: 6| Step: 5
Training loss: 1.3638100624084473
Validation loss: 2.145694375038147

Epoch: 6| Step: 6
Training loss: 2.1008994579315186
Validation loss: 2.1388344168663025

Epoch: 6| Step: 7
Training loss: 1.9604225158691406
Validation loss: 2.1488132874170938

Epoch: 6| Step: 8
Training loss: 2.430023670196533
Validation loss: 2.1484014987945557

Epoch: 6| Step: 9
Training loss: 1.4732885360717773
Validation loss: 2.161803344885508

Epoch: 6| Step: 10
Training loss: 1.7311906814575195
Validation loss: 2.1604724526405334

Epoch: 6| Step: 11
Training loss: 1.7884767055511475
Validation loss: 2.1437620719273887

Epoch: 6| Step: 12
Training loss: 2.156203508377075
Validation loss: 2.1541190346082053

Epoch: 6| Step: 13
Training loss: 2.2482919692993164
Validation loss: 2.1349231004714966

Epoch: 229| Step: 0
Training loss: 2.4795615673065186
Validation loss: 2.1626415054003396

Epoch: 6| Step: 1
Training loss: 1.748924732208252
Validation loss: 2.141699731349945

Epoch: 6| Step: 2
Training loss: 1.6913468837738037
Validation loss: 2.1466795603434243

Epoch: 6| Step: 3
Training loss: 1.5960112810134888
Validation loss: 2.156584143638611

Epoch: 6| Step: 4
Training loss: 1.3453625440597534
Validation loss: 2.1577533880869546

Epoch: 6| Step: 5
Training loss: 2.2680320739746094
Validation loss: 2.1768914461135864

Epoch: 6| Step: 6
Training loss: 2.2211148738861084
Validation loss: 2.165351847807566

Epoch: 6| Step: 7
Training loss: 2.2977075576782227
Validation loss: 2.1742409666379294

Epoch: 6| Step: 8
Training loss: 1.8816733360290527
Validation loss: 2.176193594932556

Epoch: 6| Step: 9
Training loss: 1.9349759817123413
Validation loss: 2.1705459554990134

Epoch: 6| Step: 10
Training loss: 1.9528310298919678
Validation loss: 2.2268453240394592

Epoch: 6| Step: 11
Training loss: 2.286543130874634
Validation loss: 2.2024499972661338

Epoch: 6| Step: 12
Training loss: 1.770403265953064
Validation loss: 2.1839466293652854

Epoch: 6| Step: 13
Training loss: 1.2761406898498535
Validation loss: 2.1910268465677896

Epoch: 230| Step: 0
Training loss: 1.8399980068206787
Validation loss: 2.186576326688131

Epoch: 6| Step: 1
Training loss: 1.598616361618042
Validation loss: 2.173791825771332

Epoch: 6| Step: 2
Training loss: 2.201808214187622
Validation loss: 2.1775744756062827

Epoch: 6| Step: 3
Training loss: 2.3065576553344727
Validation loss: 2.17308376232783

Epoch: 6| Step: 4
Training loss: 1.5112621784210205
Validation loss: 2.163776715596517

Epoch: 6| Step: 5
Training loss: 1.839864730834961
Validation loss: 2.1562541921933494

Epoch: 6| Step: 6
Training loss: 1.3161301612854004
Validation loss: 2.1443033814430237

Epoch: 6| Step: 7
Training loss: 1.9247307777404785
Validation loss: 2.1752293507258096

Epoch: 6| Step: 8
Training loss: 1.605919599533081
Validation loss: 2.157434821128845

Epoch: 6| Step: 9
Training loss: 2.522761106491089
Validation loss: 2.1443246404329934

Epoch: 6| Step: 10
Training loss: 2.1334028244018555
Validation loss: 2.1631915966669717

Epoch: 6| Step: 11
Training loss: 1.4537771940231323
Validation loss: 2.1290703813234964

Epoch: 6| Step: 12
Training loss: 2.1409926414489746
Validation loss: 2.158033827940623

Epoch: 6| Step: 13
Training loss: 1.8769479990005493
Validation loss: 2.153216540813446

Epoch: 231| Step: 0
Training loss: 1.4514988660812378
Validation loss: 2.147107402483622

Epoch: 6| Step: 1
Training loss: 1.7739160060882568
Validation loss: 2.1692389845848083

Epoch: 6| Step: 2
Training loss: 1.37245512008667
Validation loss: 2.1636619766553244

Epoch: 6| Step: 3
Training loss: 1.6572341918945312
Validation loss: 2.159759302934011

Epoch: 6| Step: 4
Training loss: 1.6591905355453491
Validation loss: 2.1908167004585266

Epoch: 6| Step: 5
Training loss: 2.0173163414001465
Validation loss: 2.19399893283844

Epoch: 6| Step: 6
Training loss: 2.2246484756469727
Validation loss: 2.1963953971862793

Epoch: 6| Step: 7
Training loss: 2.251884937286377
Validation loss: 2.180160701274872

Epoch: 6| Step: 8
Training loss: 1.4448609352111816
Validation loss: 2.189355432987213

Epoch: 6| Step: 9
Training loss: 2.013521909713745
Validation loss: 2.151983320713043

Epoch: 6| Step: 10
Training loss: 1.9862481355667114
Validation loss: 2.173011382420858

Epoch: 6| Step: 11
Training loss: 1.9993858337402344
Validation loss: 2.150382876396179

Epoch: 6| Step: 12
Training loss: 2.023617744445801
Validation loss: 2.1651891271273294

Epoch: 6| Step: 13
Training loss: 2.505704641342163
Validation loss: 2.1519399285316467

Epoch: 232| Step: 0
Training loss: 1.9581705331802368
Validation loss: 2.168157438437144

Epoch: 6| Step: 1
Training loss: 1.8089351654052734
Validation loss: 2.1702294747034707

Epoch: 6| Step: 2
Training loss: 1.8145761489868164
Validation loss: 2.185973107814789

Epoch: 6| Step: 3
Training loss: 1.5993854999542236
Validation loss: 2.191354433695475

Epoch: 6| Step: 4
Training loss: 1.7791858911514282
Validation loss: 2.195226490497589

Epoch: 6| Step: 5
Training loss: 1.9867184162139893
Validation loss: 2.2101628184318542

Epoch: 6| Step: 6
Training loss: 1.8962153196334839
Validation loss: 2.2057636976242065

Epoch: 6| Step: 7
Training loss: 1.8974988460540771
Validation loss: 2.202924291292826

Epoch: 6| Step: 8
Training loss: 2.2648611068725586
Validation loss: 2.172161857287089

Epoch: 6| Step: 9
Training loss: 2.099294900894165
Validation loss: 2.1589496533075967

Epoch: 6| Step: 10
Training loss: 1.856053352355957
Validation loss: 2.1616998314857483

Epoch: 6| Step: 11
Training loss: 1.648833990097046
Validation loss: 2.174418330192566

Epoch: 6| Step: 12
Training loss: 1.8739094734191895
Validation loss: 2.1739285985628762

Epoch: 6| Step: 13
Training loss: 1.949339747428894
Validation loss: 2.1826611757278442

Epoch: 233| Step: 0
Training loss: 2.320643663406372
Validation loss: 2.195341646671295

Epoch: 6| Step: 1
Training loss: 2.113828182220459
Validation loss: 2.235320210456848

Epoch: 6| Step: 2
Training loss: 1.6782859563827515
Validation loss: 2.2275839845339456

Epoch: 6| Step: 3
Training loss: 2.525507926940918
Validation loss: 2.238365630308787

Epoch: 6| Step: 4
Training loss: 1.8645645380020142
Validation loss: 2.228704114754995

Epoch: 6| Step: 5
Training loss: 1.0749458074569702
Validation loss: 2.220953126748403

Epoch: 6| Step: 6
Training loss: 1.4530901908874512
Validation loss: 2.2113873958587646

Epoch: 6| Step: 7
Training loss: 2.2388968467712402
Validation loss: 2.2002596656481423

Epoch: 6| Step: 8
Training loss: 1.7435636520385742
Validation loss: 2.1774749557177224

Epoch: 6| Step: 9
Training loss: 1.3413400650024414
Validation loss: 2.18552174170812

Epoch: 6| Step: 10
Training loss: 1.6854394674301147
Validation loss: 2.1583568851153054

Epoch: 6| Step: 11
Training loss: 2.222090244293213
Validation loss: 2.1571191946665444

Epoch: 6| Step: 12
Training loss: 1.9460830688476562
Validation loss: 2.159979303677877

Epoch: 6| Step: 13
Training loss: 2.1392407417297363
Validation loss: 2.1491384903589883

Epoch: 234| Step: 0
Training loss: 1.4622321128845215
Validation loss: 2.1460519433021545

Epoch: 6| Step: 1
Training loss: 2.285229206085205
Validation loss: 2.142670194307963

Epoch: 6| Step: 2
Training loss: 1.9363220930099487
Validation loss: 2.1439006527264914

Epoch: 6| Step: 3
Training loss: 1.337110161781311
Validation loss: 2.1537587443987527

Epoch: 6| Step: 4
Training loss: 1.6620826721191406
Validation loss: 2.1496870517730713

Epoch: 6| Step: 5
Training loss: 2.154489278793335
Validation loss: 2.1480109294255576

Epoch: 6| Step: 6
Training loss: 2.1870999336242676
Validation loss: 2.1683104435602822

Epoch: 6| Step: 7
Training loss: 2.026839017868042
Validation loss: 2.149063150087992

Epoch: 6| Step: 8
Training loss: 1.562901258468628
Validation loss: 2.156803091367086

Epoch: 6| Step: 9
Training loss: 1.7221949100494385
Validation loss: 2.1717313726743064

Epoch: 6| Step: 10
Training loss: 1.815517544746399
Validation loss: 2.186499277750651

Epoch: 6| Step: 11
Training loss: 2.201890707015991
Validation loss: 2.1856146256128945

Epoch: 6| Step: 12
Training loss: 1.936678171157837
Validation loss: 2.161788980166117

Epoch: 6| Step: 13
Training loss: 2.1029646396636963
Validation loss: 2.1745787064234414

Epoch: 235| Step: 0
Training loss: 1.7901651859283447
Validation loss: 2.170136650403341

Epoch: 6| Step: 1
Training loss: 1.6136866807937622
Validation loss: 2.170180916786194

Epoch: 6| Step: 2
Training loss: 2.1165852546691895
Validation loss: 2.169182777404785

Epoch: 6| Step: 3
Training loss: 1.4395933151245117
Validation loss: 2.1479661067326865

Epoch: 6| Step: 4
Training loss: 1.3469512462615967
Validation loss: 2.172799547513326

Epoch: 6| Step: 5
Training loss: 1.8153260946273804
Validation loss: 2.1667750477790833

Epoch: 6| Step: 6
Training loss: 2.160831928253174
Validation loss: 2.153464357058207

Epoch: 6| Step: 7
Training loss: 2.251828193664551
Validation loss: 2.163584589958191

Epoch: 6| Step: 8
Training loss: 2.2292938232421875
Validation loss: 2.158380071322123

Epoch: 6| Step: 9
Training loss: 1.5333621501922607
Validation loss: 2.163730283578237

Epoch: 6| Step: 10
Training loss: 2.232490062713623
Validation loss: 2.1417526602745056

Epoch: 6| Step: 11
Training loss: 1.4100899696350098
Validation loss: 2.163038651148478

Epoch: 6| Step: 12
Training loss: 2.4365828037261963
Validation loss: 2.1643317143122354

Epoch: 6| Step: 13
Training loss: 1.937323808670044
Validation loss: 2.146602690219879

Epoch: 236| Step: 0
Training loss: 1.9266793727874756
Validation loss: 2.1700087785720825

Epoch: 6| Step: 1
Training loss: 2.5512447357177734
Validation loss: 2.165408988793691

Epoch: 6| Step: 2
Training loss: 1.448043942451477
Validation loss: 2.1661720673243203

Epoch: 6| Step: 3
Training loss: 1.2829420566558838
Validation loss: 2.1686299045880637

Epoch: 6| Step: 4
Training loss: 1.5715348720550537
Validation loss: 2.16862823565801

Epoch: 6| Step: 5
Training loss: 1.7072455883026123
Validation loss: 2.1709688305854797

Epoch: 6| Step: 6
Training loss: 1.8218879699707031
Validation loss: 2.1810439825057983

Epoch: 6| Step: 7
Training loss: 1.9864697456359863
Validation loss: 2.181574026743571

Epoch: 6| Step: 8
Training loss: 1.4434938430786133
Validation loss: 2.180057684580485

Epoch: 6| Step: 9
Training loss: 1.563002109527588
Validation loss: 2.150696337223053

Epoch: 6| Step: 10
Training loss: 2.344282627105713
Validation loss: 2.1691617568333945

Epoch: 6| Step: 11
Training loss: 2.0945241451263428
Validation loss: 2.1452683409055076

Epoch: 6| Step: 12
Training loss: 1.4824427366256714
Validation loss: 2.16014963388443

Epoch: 6| Step: 13
Training loss: 2.6465210914611816
Validation loss: 2.169385294119517

Epoch: 237| Step: 0
Training loss: 1.7726612091064453
Validation loss: 2.172961493333181

Epoch: 6| Step: 1
Training loss: 1.538773536682129
Validation loss: 2.1636733611424765

Epoch: 6| Step: 2
Training loss: 1.8349355459213257
Validation loss: 2.1758439540863037

Epoch: 6| Step: 3
Training loss: 2.187255859375
Validation loss: 2.1631676157315574

Epoch: 6| Step: 4
Training loss: 1.7849047183990479
Validation loss: 2.1671997904777527

Epoch: 6| Step: 5
Training loss: 1.8211389780044556
Validation loss: 2.1731545130411782

Epoch: 6| Step: 6
Training loss: 1.9039640426635742
Validation loss: 2.1696866353352866

Epoch: 6| Step: 7
Training loss: 1.57279634475708
Validation loss: 2.19608743985494

Epoch: 6| Step: 8
Training loss: 1.793318271636963
Validation loss: 2.20034521818161

Epoch: 6| Step: 9
Training loss: 1.6800498962402344
Validation loss: 2.1729513009389243

Epoch: 6| Step: 10
Training loss: 2.3780717849731445
Validation loss: 2.160740594069163

Epoch: 6| Step: 11
Training loss: 1.1995902061462402
Validation loss: 2.1744072834650674

Epoch: 6| Step: 12
Training loss: 2.4440248012542725
Validation loss: 2.176836093266805

Epoch: 6| Step: 13
Training loss: 1.987777590751648
Validation loss: 2.176346937815348

Epoch: 238| Step: 0
Training loss: 1.74750554561615
Validation loss: 2.1756099462509155

Epoch: 6| Step: 1
Training loss: 1.4800267219543457
Validation loss: 2.1785688400268555

Epoch: 6| Step: 2
Training loss: 1.3884896039962769
Validation loss: 2.1741746266682944

Epoch: 6| Step: 3
Training loss: 1.5427651405334473
Validation loss: 2.181324621041616

Epoch: 6| Step: 4
Training loss: 1.7617417573928833
Validation loss: 2.1625653902689614

Epoch: 6| Step: 5
Training loss: 2.266279697418213
Validation loss: 2.1754417022069297

Epoch: 6| Step: 6
Training loss: 1.9806289672851562
Validation loss: 2.190195163091024

Epoch: 6| Step: 7
Training loss: 2.411149740219116
Validation loss: 2.2051968971888223

Epoch: 6| Step: 8
Training loss: 1.9207974672317505
Validation loss: 2.2057406107584634

Epoch: 6| Step: 9
Training loss: 2.115170478820801
Validation loss: 2.2117316722869873

Epoch: 6| Step: 10
Training loss: 1.2175500392913818
Validation loss: 2.22148597240448

Epoch: 6| Step: 11
Training loss: 1.6437504291534424
Validation loss: 2.19581671555837

Epoch: 6| Step: 12
Training loss: 2.1358628273010254
Validation loss: 2.192012925942739

Epoch: 6| Step: 13
Training loss: 2.2006258964538574
Validation loss: 2.1600722471872964

Epoch: 239| Step: 0
Training loss: 1.691272258758545
Validation loss: 2.1539769967397056

Epoch: 6| Step: 1
Training loss: 1.583001732826233
Validation loss: 2.179999808470408

Epoch: 6| Step: 2
Training loss: 1.7769429683685303
Validation loss: 2.1696409384409585

Epoch: 6| Step: 3
Training loss: 1.5486958026885986
Validation loss: 2.1559143662452698

Epoch: 6| Step: 4
Training loss: 1.9035565853118896
Validation loss: 2.168034076690674

Epoch: 6| Step: 5
Training loss: 1.6290591955184937
Validation loss: 2.171585818131765

Epoch: 6| Step: 6
Training loss: 2.258469343185425
Validation loss: 2.155474901199341

Epoch: 6| Step: 7
Training loss: 1.8231221437454224
Validation loss: 2.1686774293581643

Epoch: 6| Step: 8
Training loss: 1.8527911901474
Validation loss: 2.176660676797231

Epoch: 6| Step: 9
Training loss: 1.6243267059326172
Validation loss: 2.1614639361699424

Epoch: 6| Step: 10
Training loss: 1.33282470703125
Validation loss: 2.1641441782315574

Epoch: 6| Step: 11
Training loss: 2.7291722297668457
Validation loss: 2.1677816112836203

Epoch: 6| Step: 12
Training loss: 2.4474363327026367
Validation loss: 2.176055053869883

Epoch: 6| Step: 13
Training loss: 1.6337203979492188
Validation loss: 2.1897873679796853

Epoch: 240| Step: 0
Training loss: 1.6396760940551758
Validation loss: 2.221012453238169

Epoch: 6| Step: 1
Training loss: 2.060786247253418
Validation loss: 2.2187628547350564

Epoch: 6| Step: 2
Training loss: 2.2044405937194824
Validation loss: 2.2160316904385886

Epoch: 6| Step: 3
Training loss: 1.1903603076934814
Validation loss: 2.1849882205327353

Epoch: 6| Step: 4
Training loss: 2.50789737701416
Validation loss: 2.1619967818260193

Epoch: 6| Step: 5
Training loss: 1.8023308515548706
Validation loss: 2.1613423228263855

Epoch: 6| Step: 6
Training loss: 1.6857163906097412
Validation loss: 2.1439018646876016

Epoch: 6| Step: 7
Training loss: 2.2461514472961426
Validation loss: 2.1554259856541953

Epoch: 6| Step: 8
Training loss: 1.8859820365905762
Validation loss: 2.1570091048876443

Epoch: 6| Step: 9
Training loss: 1.882312297821045
Validation loss: 2.1637901067733765

Epoch: 6| Step: 10
Training loss: 2.207768440246582
Validation loss: 2.155499597390493

Epoch: 6| Step: 11
Training loss: 2.128882646560669
Validation loss: 2.1485655307769775

Epoch: 6| Step: 12
Training loss: 1.8963663578033447
Validation loss: 2.150217910607656

Epoch: 6| Step: 13
Training loss: 1.3163752555847168
Validation loss: 2.161159654458364

Epoch: 241| Step: 0
Training loss: 1.6327290534973145
Validation loss: 2.17267374197642

Epoch: 6| Step: 1
Training loss: 1.7042207717895508
Validation loss: 2.1634374856948853

Epoch: 6| Step: 2
Training loss: 2.163907051086426
Validation loss: 2.150332788626353

Epoch: 6| Step: 3
Training loss: 1.4090757369995117
Validation loss: 2.137637456258138

Epoch: 6| Step: 4
Training loss: 1.5778210163116455
Validation loss: 2.1634870767593384

Epoch: 6| Step: 5
Training loss: 2.335482597351074
Validation loss: 2.1734227339426675

Epoch: 6| Step: 6
Training loss: 1.7941848039627075
Validation loss: 2.1869073112805686

Epoch: 6| Step: 7
Training loss: 1.8547871112823486
Validation loss: 2.1952825586001077

Epoch: 6| Step: 8
Training loss: 1.938352346420288
Validation loss: 2.206570784250895

Epoch: 6| Step: 9
Training loss: 1.4776740074157715
Validation loss: 2.1871579686800637

Epoch: 6| Step: 10
Training loss: 1.9396158456802368
Validation loss: 2.1802980303764343

Epoch: 6| Step: 11
Training loss: 1.596688151359558
Validation loss: 2.187406897544861

Epoch: 6| Step: 12
Training loss: 2.301004409790039
Validation loss: 2.1975377003351846

Epoch: 6| Step: 13
Training loss: 1.602161169052124
Validation loss: 2.2036786675453186

Epoch: 242| Step: 0
Training loss: 1.6174544095993042
Validation loss: 2.2045618891716003

Epoch: 6| Step: 1
Training loss: 1.6972200870513916
Validation loss: 2.2064298391342163

Epoch: 6| Step: 2
Training loss: 1.5958893299102783
Validation loss: 2.2018860379854837

Epoch: 6| Step: 3
Training loss: 1.7854604721069336
Validation loss: 2.227330207824707

Epoch: 6| Step: 4
Training loss: 2.312699317932129
Validation loss: 2.2117994825045266

Epoch: 6| Step: 5
Training loss: 2.1456143856048584
Validation loss: 2.2139196395874023

Epoch: 6| Step: 6
Training loss: 2.05812931060791
Validation loss: 2.1970898310343423

Epoch: 6| Step: 7
Training loss: 1.7109463214874268
Validation loss: 2.1846140027046204

Epoch: 6| Step: 8
Training loss: 2.1599202156066895
Validation loss: 2.2179802656173706

Epoch: 6| Step: 9
Training loss: 1.1302390098571777
Validation loss: 2.21870752175649

Epoch: 6| Step: 10
Training loss: 1.784853458404541
Validation loss: 2.2100183963775635

Epoch: 6| Step: 11
Training loss: 2.0819592475891113
Validation loss: 2.2114753127098083

Epoch: 6| Step: 12
Training loss: 1.6983768939971924
Validation loss: 2.2225847641626992

Epoch: 6| Step: 13
Training loss: 1.97819983959198
Validation loss: 2.2053839564323425

Epoch: 243| Step: 0
Training loss: 2.106055736541748
Validation loss: 2.2134200731913247

Epoch: 6| Step: 1
Training loss: 1.432164192199707
Validation loss: 2.1961981455485025

Epoch: 6| Step: 2
Training loss: 1.8616126775741577
Validation loss: 2.186358690261841

Epoch: 6| Step: 3
Training loss: 1.4447710514068604
Validation loss: 2.1656509041786194

Epoch: 6| Step: 4
Training loss: 2.0028610229492188
Validation loss: 2.177972654501597

Epoch: 6| Step: 5
Training loss: 1.2615623474121094
Validation loss: 2.1805665294329324

Epoch: 6| Step: 6
Training loss: 2.395254135131836
Validation loss: 2.165878931681315

Epoch: 6| Step: 7
Training loss: 2.5940053462982178
Validation loss: 2.1576613783836365

Epoch: 6| Step: 8
Training loss: 2.2404847145080566
Validation loss: 2.1711470683415732

Epoch: 6| Step: 9
Training loss: 1.8808469772338867
Validation loss: 2.1676536003748574

Epoch: 6| Step: 10
Training loss: 1.5622566938400269
Validation loss: 2.2049373984336853

Epoch: 6| Step: 11
Training loss: 1.5092713832855225
Validation loss: 2.199219564596812

Epoch: 6| Step: 12
Training loss: 1.9774811267852783
Validation loss: 2.2012843092282615

Epoch: 6| Step: 13
Training loss: 1.4815694093704224
Validation loss: 2.1997268994649253

Epoch: 244| Step: 0
Training loss: 2.182206153869629
Validation loss: 2.1902036666870117

Epoch: 6| Step: 1
Training loss: 1.9212586879730225
Validation loss: 2.190425912539164

Epoch: 6| Step: 2
Training loss: 2.178680896759033
Validation loss: 2.190099914868673

Epoch: 6| Step: 3
Training loss: 1.5882301330566406
Validation loss: 2.174695869286855

Epoch: 6| Step: 4
Training loss: 1.5118796825408936
Validation loss: 2.179819564024607

Epoch: 6| Step: 5
Training loss: 1.5619158744812012
Validation loss: 2.1587320963541665

Epoch: 6| Step: 6
Training loss: 1.7629340887069702
Validation loss: 2.1640979846318564

Epoch: 6| Step: 7
Training loss: 2.0318832397460938
Validation loss: 2.163181722164154

Epoch: 6| Step: 8
Training loss: 1.5670166015625
Validation loss: 2.1865728298823037

Epoch: 6| Step: 9
Training loss: 1.4725697040557861
Validation loss: 2.1705935994784036

Epoch: 6| Step: 10
Training loss: 1.8223832845687866
Validation loss: 2.1684001882870994

Epoch: 6| Step: 11
Training loss: 2.4193389415740967
Validation loss: 2.173412581284841

Epoch: 6| Step: 12
Training loss: 1.9156277179718018
Validation loss: 2.1787851651509604

Epoch: 6| Step: 13
Training loss: 1.5696117877960205
Validation loss: 2.161837339401245

Epoch: 245| Step: 0
Training loss: 1.5127265453338623
Validation loss: 2.1757508913675943

Epoch: 6| Step: 1
Training loss: 2.016664981842041
Validation loss: 2.205703397591909

Epoch: 6| Step: 2
Training loss: 2.163084030151367
Validation loss: 2.1957603096961975

Epoch: 6| Step: 3
Training loss: 1.0044242143630981
Validation loss: 2.1811557610829673

Epoch: 6| Step: 4
Training loss: 2.521665573120117
Validation loss: 2.178057829538981

Epoch: 6| Step: 5
Training loss: 1.7166216373443604
Validation loss: 2.158161242802938

Epoch: 6| Step: 6
Training loss: 2.1030869483947754
Validation loss: 2.1777414679527283

Epoch: 6| Step: 7
Training loss: 1.4010593891143799
Validation loss: 2.1821458538373313

Epoch: 6| Step: 8
Training loss: 2.744354248046875
Validation loss: 2.217504143714905

Epoch: 6| Step: 9
Training loss: 1.8395721912384033
Validation loss: 2.216396768887838

Epoch: 6| Step: 10
Training loss: 1.6270534992218018
Validation loss: 2.2091639041900635

Epoch: 6| Step: 11
Training loss: 1.6153960227966309
Validation loss: 2.218021551767985

Epoch: 6| Step: 12
Training loss: 1.5982894897460938
Validation loss: 2.2193915247917175

Epoch: 6| Step: 13
Training loss: 1.8161152601242065
Validation loss: 2.1968666712443032

Epoch: 246| Step: 0
Training loss: 1.7758901119232178
Validation loss: 2.185750166575114

Epoch: 6| Step: 1
Training loss: 1.8252357244491577
Validation loss: 2.181525945663452

Epoch: 6| Step: 2
Training loss: 2.0931055545806885
Validation loss: 2.162741800149282

Epoch: 6| Step: 3
Training loss: 2.1051392555236816
Validation loss: 2.178602159023285

Epoch: 6| Step: 4
Training loss: 1.6414958238601685
Validation loss: 2.163301249345144

Epoch: 6| Step: 5
Training loss: 1.5014756917953491
Validation loss: 2.1591598788897195

Epoch: 6| Step: 6
Training loss: 1.618427038192749
Validation loss: 2.158524493376414

Epoch: 6| Step: 7
Training loss: 1.515930414199829
Validation loss: 2.150590260823568

Epoch: 6| Step: 8
Training loss: 2.4240245819091797
Validation loss: 2.155056377251943

Epoch: 6| Step: 9
Training loss: 1.780605435371399
Validation loss: 2.1679109732309976

Epoch: 6| Step: 10
Training loss: 1.5008567571640015
Validation loss: 2.1647029320398965

Epoch: 6| Step: 11
Training loss: 2.3455727100372314
Validation loss: 2.1769863764444985

Epoch: 6| Step: 12
Training loss: 1.9614624977111816
Validation loss: 2.161142726739248

Epoch: 6| Step: 13
Training loss: 2.6169369220733643
Validation loss: 2.164147694905599

Epoch: 247| Step: 0
Training loss: 1.2581489086151123
Validation loss: 2.191588362058004

Epoch: 6| Step: 1
Training loss: 1.4219236373901367
Validation loss: 2.231621563434601

Epoch: 6| Step: 2
Training loss: 2.1367616653442383
Validation loss: 2.2379905581474304

Epoch: 6| Step: 3
Training loss: 1.6723670959472656
Validation loss: 2.2470083634058633

Epoch: 6| Step: 4
Training loss: 2.1143219470977783
Validation loss: 2.2384455601374307

Epoch: 6| Step: 5
Training loss: 2.2860708236694336
Validation loss: 2.2567644715309143

Epoch: 6| Step: 6
Training loss: 1.5974197387695312
Validation loss: 2.2680147290229797

Epoch: 6| Step: 7
Training loss: 1.941718578338623
Validation loss: 2.2467400431632996

Epoch: 6| Step: 8
Training loss: 2.283454656600952
Validation loss: 2.229665736357371

Epoch: 6| Step: 9
Training loss: 1.5245413780212402
Validation loss: 2.2057494521141052

Epoch: 6| Step: 10
Training loss: 2.0023255348205566
Validation loss: 2.1492861111958823

Epoch: 6| Step: 11
Training loss: 1.7358872890472412
Validation loss: 2.157866358757019

Epoch: 6| Step: 12
Training loss: 2.3369381427764893
Validation loss: 2.146528979142507

Epoch: 6| Step: 13
Training loss: 2.4551403522491455
Validation loss: 2.1398073037465415

Epoch: 248| Step: 0
Training loss: 1.325578212738037
Validation loss: 2.1540637811024985

Epoch: 6| Step: 1
Training loss: 2.3349575996398926
Validation loss: 2.1618131200472512

Epoch: 6| Step: 2
Training loss: 1.7570099830627441
Validation loss: 2.1605546871821084

Epoch: 6| Step: 3
Training loss: 2.245016574859619
Validation loss: 2.16951992114385

Epoch: 6| Step: 4
Training loss: 2.6122143268585205
Validation loss: 2.1601648132006326

Epoch: 6| Step: 5
Training loss: 1.927748680114746
Validation loss: 2.1709211667378745

Epoch: 6| Step: 6
Training loss: 1.6188147068023682
Validation loss: 2.1636612017949424

Epoch: 6| Step: 7
Training loss: 1.4658565521240234
Validation loss: 2.159236192703247

Epoch: 6| Step: 8
Training loss: 2.285283088684082
Validation loss: 2.16454686721166

Epoch: 6| Step: 9
Training loss: 1.5060715675354004
Validation loss: 2.1622345646222434

Epoch: 6| Step: 10
Training loss: 2.032402276992798
Validation loss: 2.148426612218221

Epoch: 6| Step: 11
Training loss: 1.86983060836792
Validation loss: 2.160286247730255

Epoch: 6| Step: 12
Training loss: 2.028576135635376
Validation loss: 2.1308951377868652

Epoch: 6| Step: 13
Training loss: 2.0164899826049805
Validation loss: 2.147891024748484

Epoch: 249| Step: 0
Training loss: 1.7142146825790405
Validation loss: 2.118728001912435

Epoch: 6| Step: 1
Training loss: 2.2323362827301025
Validation loss: 2.1348628600438437

Epoch: 6| Step: 2
Training loss: 1.608661413192749
Validation loss: 2.1473355293273926

Epoch: 6| Step: 3
Training loss: 2.194192409515381
Validation loss: 2.1535253326098123

Epoch: 6| Step: 4
Training loss: 1.792495846748352
Validation loss: 2.1528937816619873

Epoch: 6| Step: 5
Training loss: 1.7969343662261963
Validation loss: 2.149807254473368

Epoch: 6| Step: 6
Training loss: 2.4486701488494873
Validation loss: 2.1453827818234763

Epoch: 6| Step: 7
Training loss: 1.7878671884536743
Validation loss: 2.17111603418986

Epoch: 6| Step: 8
Training loss: 2.3184473514556885
Validation loss: 2.1587175528208413

Epoch: 6| Step: 9
Training loss: 0.9851757287979126
Validation loss: 2.1555510560671487

Epoch: 6| Step: 10
Training loss: 1.848968744277954
Validation loss: 2.1563690106074014

Epoch: 6| Step: 11
Training loss: 1.2875295877456665
Validation loss: 2.158956507841746

Epoch: 6| Step: 12
Training loss: 1.8648536205291748
Validation loss: 2.144200543562571

Epoch: 6| Step: 13
Training loss: 2.456425666809082
Validation loss: 2.167211095492045

Epoch: 250| Step: 0
Training loss: 2.192106246948242
Validation loss: 2.1625577211380005

Epoch: 6| Step: 1
Training loss: 1.9126083850860596
Validation loss: 2.1627608935038247

Epoch: 6| Step: 2
Training loss: 2.246340274810791
Validation loss: 2.148857812086741

Epoch: 6| Step: 3
Training loss: 2.1769137382507324
Validation loss: 2.161658545335134

Epoch: 6| Step: 4
Training loss: 2.664464235305786
Validation loss: 2.1422147154808044

Epoch: 6| Step: 5
Training loss: 1.7662153244018555
Validation loss: 2.1520537535349527

Epoch: 6| Step: 6
Training loss: 1.6625852584838867
Validation loss: 2.1498628854751587

Epoch: 6| Step: 7
Training loss: 1.7225522994995117
Validation loss: 2.150056223074595

Epoch: 6| Step: 8
Training loss: 1.3034027814865112
Validation loss: 2.150557518005371

Epoch: 6| Step: 9
Training loss: 1.4329043626785278
Validation loss: 2.14926415681839

Epoch: 6| Step: 10
Training loss: 1.9939587116241455
Validation loss: 2.147081653277079

Epoch: 6| Step: 11
Training loss: 2.52008056640625
Validation loss: 2.1440409620602927

Epoch: 6| Step: 12
Training loss: 1.3939317464828491
Validation loss: 2.144698441028595

Epoch: 6| Step: 13
Training loss: 1.0377862453460693
Validation loss: 2.160743514696757

Epoch: 251| Step: 0
Training loss: 1.5601978302001953
Validation loss: 2.161669751008352

Epoch: 6| Step: 1
Training loss: 1.8008426427841187
Validation loss: 2.155138591925303

Epoch: 6| Step: 2
Training loss: 1.82232666015625
Validation loss: 2.179064393043518

Epoch: 6| Step: 3
Training loss: 1.8656563758850098
Validation loss: 2.1413105726242065

Epoch: 6| Step: 4
Training loss: 1.869488000869751
Validation loss: 2.158862292766571

Epoch: 6| Step: 5
Training loss: 1.244533658027649
Validation loss: 2.162868082523346

Epoch: 6| Step: 6
Training loss: 2.295257568359375
Validation loss: 2.1564963261286416

Epoch: 6| Step: 7
Training loss: 2.371884822845459
Validation loss: 2.1544500589370728

Epoch: 6| Step: 8
Training loss: 1.3500471115112305
Validation loss: 2.1503849029541016

Epoch: 6| Step: 9
Training loss: 2.52023983001709
Validation loss: 2.1507051587104797

Epoch: 6| Step: 10
Training loss: 2.0490119457244873
Validation loss: 2.1729770501454673

Epoch: 6| Step: 11
Training loss: 1.8694493770599365
Validation loss: 2.15002034107844

Epoch: 6| Step: 12
Training loss: 1.7390342950820923
Validation loss: 2.1699607769648233

Epoch: 6| Step: 13
Training loss: 1.7266627550125122
Validation loss: 2.169514000415802

Epoch: 252| Step: 0
Training loss: 2.205240249633789
Validation loss: 2.1597334941228232

Epoch: 6| Step: 1
Training loss: 1.3001749515533447
Validation loss: 2.179901977380117

Epoch: 6| Step: 2
Training loss: 1.4121454954147339
Validation loss: 2.179737865924835

Epoch: 6| Step: 3
Training loss: 1.908272385597229
Validation loss: 2.1526967684427896

Epoch: 6| Step: 4
Training loss: 1.4450596570968628
Validation loss: 2.1608900825182595

Epoch: 6| Step: 5
Training loss: 1.7739760875701904
Validation loss: 2.1746753255526223

Epoch: 6| Step: 6
Training loss: 2.036271095275879
Validation loss: 2.17989053328832

Epoch: 6| Step: 7
Training loss: 1.8754584789276123
Validation loss: 2.1616249084472656

Epoch: 6| Step: 8
Training loss: 1.5043056011199951
Validation loss: 2.159223794937134

Epoch: 6| Step: 9
Training loss: 1.991194725036621
Validation loss: 2.1797902981440225

Epoch: 6| Step: 10
Training loss: 2.5047008991241455
Validation loss: 2.181183914343516

Epoch: 6| Step: 11
Training loss: 1.5882313251495361
Validation loss: 2.169609308242798

Epoch: 6| Step: 12
Training loss: 1.6067934036254883
Validation loss: 2.167944331963857

Epoch: 6| Step: 13
Training loss: 2.7823402881622314
Validation loss: 2.168626050154368

Epoch: 253| Step: 0
Training loss: 1.6245930194854736
Validation loss: 2.1836560368537903

Epoch: 6| Step: 1
Training loss: 1.61759614944458
Validation loss: 2.1889695525169373

Epoch: 6| Step: 2
Training loss: 1.9203518629074097
Validation loss: 2.1773454546928406

Epoch: 6| Step: 3
Training loss: 1.3780484199523926
Validation loss: 2.1849550207455954

Epoch: 6| Step: 4
Training loss: 1.7038438320159912
Validation loss: 2.18463796377182

Epoch: 6| Step: 5
Training loss: 1.8596924543380737
Validation loss: 2.19013241926829

Epoch: 6| Step: 6
Training loss: 2.5891518592834473
Validation loss: 2.195493459701538

Epoch: 6| Step: 7
Training loss: 1.657608985900879
Validation loss: 2.1905818382898965

Epoch: 6| Step: 8
Training loss: 1.7518855333328247
Validation loss: 2.185972591241201

Epoch: 6| Step: 9
Training loss: 2.3477511405944824
Validation loss: 2.18088432153066

Epoch: 6| Step: 10
Training loss: 2.28609561920166
Validation loss: 2.1544243693351746

Epoch: 6| Step: 11
Training loss: 1.4992916584014893
Validation loss: 2.180937727292379

Epoch: 6| Step: 12
Training loss: 1.7600650787353516
Validation loss: 2.1749227245648703

Epoch: 6| Step: 13
Training loss: 1.774094820022583
Validation loss: 2.1809361577033997

Epoch: 254| Step: 0
Training loss: 1.912590503692627
Validation loss: 2.1764313777287803

Epoch: 6| Step: 1
Training loss: 1.5087802410125732
Validation loss: 2.1873807311058044

Epoch: 6| Step: 2
Training loss: 1.925666332244873
Validation loss: 2.182075619697571

Epoch: 6| Step: 3
Training loss: 1.588590145111084
Validation loss: 2.1894904176394143

Epoch: 6| Step: 4
Training loss: 1.4943652153015137
Validation loss: 2.201692581176758

Epoch: 6| Step: 5
Training loss: 1.7519630193710327
Validation loss: 2.197630147139231

Epoch: 6| Step: 6
Training loss: 2.1897573471069336
Validation loss: 2.196961462497711

Epoch: 6| Step: 7
Training loss: 2.5313124656677246
Validation loss: 2.205328126748403

Epoch: 6| Step: 8
Training loss: 1.9201972484588623
Validation loss: 2.206648031870524

Epoch: 6| Step: 9
Training loss: 1.5545791387557983
Validation loss: 2.2166911363601685

Epoch: 6| Step: 10
Training loss: 1.9201197624206543
Validation loss: 2.1899558504422507

Epoch: 6| Step: 11
Training loss: 1.3056297302246094
Validation loss: 2.201276699701945

Epoch: 6| Step: 12
Training loss: 2.0044729709625244
Validation loss: 2.2075915932655334

Epoch: 6| Step: 13
Training loss: 1.7628751993179321
Validation loss: 2.1950636506080627

Epoch: 255| Step: 0
Training loss: 2.0695879459381104
Validation loss: 2.2002073923746743

Epoch: 6| Step: 1
Training loss: 1.6943880319595337
Validation loss: 2.185360630353292

Epoch: 6| Step: 2
Training loss: 1.7645506858825684
Validation loss: 2.201077421506246

Epoch: 6| Step: 3
Training loss: 1.6094284057617188
Validation loss: 2.2038273413976035

Epoch: 6| Step: 4
Training loss: 2.3070597648620605
Validation loss: 2.215630888938904

Epoch: 6| Step: 5
Training loss: 2.4653737545013428
Validation loss: 2.186992963155111

Epoch: 6| Step: 6
Training loss: 1.0812225341796875
Validation loss: 2.2245654463768005

Epoch: 6| Step: 7
Training loss: 1.9542255401611328
Validation loss: 2.222558856010437

Epoch: 6| Step: 8
Training loss: 1.5390771627426147
Validation loss: 2.218905806541443

Epoch: 6| Step: 9
Training loss: 2.532707691192627
Validation loss: 2.204949975013733

Epoch: 6| Step: 10
Training loss: 1.3735545873641968
Validation loss: 2.207326650619507

Epoch: 6| Step: 11
Training loss: 1.4427404403686523
Validation loss: 2.1967084407806396

Epoch: 6| Step: 12
Training loss: 1.7422168254852295
Validation loss: 2.1987081368764243

Epoch: 6| Step: 13
Training loss: 1.8138760328292847
Validation loss: 2.167440334955851

Epoch: 256| Step: 0
Training loss: 1.5655908584594727
Validation loss: 2.176122506459554

Epoch: 6| Step: 1
Training loss: 1.4488475322723389
Validation loss: 2.1844289700190225

Epoch: 6| Step: 2
Training loss: 1.8766953945159912
Validation loss: 2.175092339515686

Epoch: 6| Step: 3
Training loss: 2.200416088104248
Validation loss: 2.1747416257858276

Epoch: 6| Step: 4
Training loss: 1.7694897651672363
Validation loss: 2.193373521169027

Epoch: 6| Step: 5
Training loss: 1.3383883237838745
Validation loss: 2.19461061557134

Epoch: 6| Step: 6
Training loss: 2.435375452041626
Validation loss: 2.190845708052317

Epoch: 6| Step: 7
Training loss: 2.073890447616577
Validation loss: 2.1975088914235434

Epoch: 6| Step: 8
Training loss: 2.191753625869751
Validation loss: 2.210849126180013

Epoch: 6| Step: 9
Training loss: 1.6546285152435303
Validation loss: 2.2070501248041787

Epoch: 6| Step: 10
Training loss: 2.023122787475586
Validation loss: 2.2294287284215293

Epoch: 6| Step: 11
Training loss: 1.795379877090454
Validation loss: 2.218458414077759

Epoch: 6| Step: 12
Training loss: 1.5059895515441895
Validation loss: 2.2210525274276733

Epoch: 6| Step: 13
Training loss: 1.4994006156921387
Validation loss: 2.228862146536509

Epoch: 257| Step: 0
Training loss: 1.79209566116333
Validation loss: 2.2533631126085916

Epoch: 6| Step: 1
Training loss: 1.6063872575759888
Validation loss: 2.224058826764425

Epoch: 6| Step: 2
Training loss: 1.2822401523590088
Validation loss: 2.2443448106447854

Epoch: 6| Step: 3
Training loss: 1.8515362739562988
Validation loss: 2.2233575781186423

Epoch: 6| Step: 4
Training loss: 1.8659517765045166
Validation loss: 2.2384210427602134

Epoch: 6| Step: 5
Training loss: 1.2562986612319946
Validation loss: 2.2325629790623984

Epoch: 6| Step: 6
Training loss: 2.3011317253112793
Validation loss: 2.203856348991394

Epoch: 6| Step: 7
Training loss: 2.186797618865967
Validation loss: 2.223276933034261

Epoch: 6| Step: 8
Training loss: 2.1280221939086914
Validation loss: 2.258614500363668

Epoch: 6| Step: 9
Training loss: 2.0407395362854004
Validation loss: 2.240239997704824

Epoch: 6| Step: 10
Training loss: 1.954768419265747
Validation loss: 2.227608799934387

Epoch: 6| Step: 11
Training loss: 1.4654812812805176
Validation loss: 2.1946229537328086

Epoch: 6| Step: 12
Training loss: 1.3607436418533325
Validation loss: 2.202099084854126

Epoch: 6| Step: 13
Training loss: 2.003059148788452
Validation loss: 2.1606611013412476

Epoch: 258| Step: 0
Training loss: 1.350273609161377
Validation loss: 2.172650098800659

Epoch: 6| Step: 1
Training loss: 1.72212553024292
Validation loss: 2.1759008367856345

Epoch: 6| Step: 2
Training loss: 1.5892865657806396
Validation loss: 2.184208393096924

Epoch: 6| Step: 3
Training loss: 2.073744297027588
Validation loss: 2.183870017528534

Epoch: 6| Step: 4
Training loss: 1.7055549621582031
Validation loss: 2.1861175696055093

Epoch: 6| Step: 5
Training loss: 2.297708034515381
Validation loss: 2.18510631720225

Epoch: 6| Step: 6
Training loss: 2.1477057933807373
Validation loss: 2.190408170223236

Epoch: 6| Step: 7
Training loss: 1.4235055446624756
Validation loss: 2.1775406996409097

Epoch: 6| Step: 8
Training loss: 1.4432532787322998
Validation loss: 2.1647390127182007

Epoch: 6| Step: 9
Training loss: 1.914076805114746
Validation loss: 2.1892975171407065

Epoch: 6| Step: 10
Training loss: 1.799670696258545
Validation loss: 2.2016447385152182

Epoch: 6| Step: 11
Training loss: 2.0834853649139404
Validation loss: 2.1856796741485596

Epoch: 6| Step: 12
Training loss: 2.105057716369629
Validation loss: 2.218392034371694

Epoch: 6| Step: 13
Training loss: 1.8308080434799194
Validation loss: 2.2135692040125527

Epoch: 259| Step: 0
Training loss: 2.2062721252441406
Validation loss: 2.226332366466522

Epoch: 6| Step: 1
Training loss: 1.2506520748138428
Validation loss: 2.1791001359621682

Epoch: 6| Step: 2
Training loss: 2.108119010925293
Validation loss: 2.1836958726247153

Epoch: 6| Step: 3
Training loss: 1.4942882061004639
Validation loss: 2.19097238779068

Epoch: 6| Step: 4
Training loss: 1.7537708282470703
Validation loss: 2.178170303503672

Epoch: 6| Step: 5
Training loss: 0.8879677057266235
Validation loss: 2.1765485405921936

Epoch: 6| Step: 6
Training loss: 1.5754194259643555
Validation loss: 2.1707518696784973

Epoch: 6| Step: 7
Training loss: 1.9949555397033691
Validation loss: 2.1869104703267417

Epoch: 6| Step: 8
Training loss: 2.0467529296875
Validation loss: 2.1860361297925315

Epoch: 6| Step: 9
Training loss: 2.709275245666504
Validation loss: 2.183084944883982

Epoch: 6| Step: 10
Training loss: 1.6988747119903564
Validation loss: 2.1811048785845437

Epoch: 6| Step: 11
Training loss: 1.5589772462844849
Validation loss: 2.2039950092633567

Epoch: 6| Step: 12
Training loss: 2.094705581665039
Validation loss: 2.209721406300863

Epoch: 6| Step: 13
Training loss: 1.951614499092102
Validation loss: 2.1952444513638816

Epoch: 260| Step: 0
Training loss: 1.7217073440551758
Validation loss: 2.195174296696981

Epoch: 6| Step: 1
Training loss: 2.1614279747009277
Validation loss: 2.2078534960746765

Epoch: 6| Step: 2
Training loss: 1.5420432090759277
Validation loss: 2.1826783219973245

Epoch: 6| Step: 3
Training loss: 0.8696975111961365
Validation loss: 2.1791978677113852

Epoch: 6| Step: 4
Training loss: 1.8514606952667236
Validation loss: 2.1636457641919455

Epoch: 6| Step: 5
Training loss: 2.2715907096862793
Validation loss: 2.1709234913190207

Epoch: 6| Step: 6
Training loss: 2.4960832595825195
Validation loss: 2.1571857929229736

Epoch: 6| Step: 7
Training loss: 1.8703820705413818
Validation loss: 2.1801815231641135

Epoch: 6| Step: 8
Training loss: 1.392090082168579
Validation loss: 2.158578852812449

Epoch: 6| Step: 9
Training loss: 1.4678466320037842
Validation loss: 2.159295300642649

Epoch: 6| Step: 10
Training loss: 2.128035545349121
Validation loss: 2.1769198377927146

Epoch: 6| Step: 11
Training loss: 1.6431797742843628
Validation loss: 2.166619678338369

Epoch: 6| Step: 12
Training loss: 1.4553189277648926
Validation loss: 2.169060230255127

Epoch: 6| Step: 13
Training loss: 1.9919377565383911
Validation loss: 2.155762175718943

Epoch: 261| Step: 0
Training loss: 1.4729937314987183
Validation loss: 2.164565165837606

Epoch: 6| Step: 1
Training loss: 1.44144868850708
Validation loss: 2.1737123926480613

Epoch: 6| Step: 2
Training loss: 2.8611624240875244
Validation loss: 2.188804348309835

Epoch: 6| Step: 3
Training loss: 2.178788185119629
Validation loss: 2.1853375236193338

Epoch: 6| Step: 4
Training loss: 1.8450355529785156
Validation loss: 2.191916286945343

Epoch: 6| Step: 5
Training loss: 1.692450761795044
Validation loss: 2.188406785329183

Epoch: 6| Step: 6
Training loss: 1.447485327720642
Validation loss: 2.198752840360006

Epoch: 6| Step: 7
Training loss: 1.5604567527770996
Validation loss: 2.2175464630126953

Epoch: 6| Step: 8
Training loss: 1.913750410079956
Validation loss: 2.2157559196154275

Epoch: 6| Step: 9
Training loss: 1.6888766288757324
Validation loss: 2.218800644079844

Epoch: 6| Step: 10
Training loss: 1.742527723312378
Validation loss: 2.2385491132736206

Epoch: 6| Step: 11
Training loss: 1.4597406387329102
Validation loss: 2.2331284483273826

Epoch: 6| Step: 12
Training loss: 1.821445107460022
Validation loss: 2.2420254945755005

Epoch: 6| Step: 13
Training loss: 1.6888740062713623
Validation loss: 2.2254207134246826

Epoch: 262| Step: 0
Training loss: 1.7554023265838623
Validation loss: 2.264070212841034

Epoch: 6| Step: 1
Training loss: 1.6306469440460205
Validation loss: 2.2340578039487204

Epoch: 6| Step: 2
Training loss: 1.3064521551132202
Validation loss: 2.2276952664057412

Epoch: 6| Step: 3
Training loss: 1.8463115692138672
Validation loss: 2.2156399488449097

Epoch: 6| Step: 4
Training loss: 1.1410284042358398
Validation loss: 2.2074171900749207

Epoch: 6| Step: 5
Training loss: 1.3384032249450684
Validation loss: 2.217920958995819

Epoch: 6| Step: 6
Training loss: 2.2516658306121826
Validation loss: 2.181764622529348

Epoch: 6| Step: 7
Training loss: 1.5203640460968018
Validation loss: 2.1892547408739724

Epoch: 6| Step: 8
Training loss: 2.3543200492858887
Validation loss: 2.198178470134735

Epoch: 6| Step: 9
Training loss: 1.9574054479599
Validation loss: 2.186046083768209

Epoch: 6| Step: 10
Training loss: 1.8103362321853638
Validation loss: 2.1991275747617087

Epoch: 6| Step: 11
Training loss: 1.7695918083190918
Validation loss: 2.2081486582756042

Epoch: 6| Step: 12
Training loss: 2.484255313873291
Validation loss: 2.195277730623881

Epoch: 6| Step: 13
Training loss: 1.7413967847824097
Validation loss: 2.196185847123464

Epoch: 263| Step: 0
Training loss: 1.4519566297531128
Validation loss: 2.204701781272888

Epoch: 6| Step: 1
Training loss: 1.5978811979293823
Validation loss: 2.2292087078094482

Epoch: 6| Step: 2
Training loss: 1.3151075839996338
Validation loss: 2.2308250665664673

Epoch: 6| Step: 3
Training loss: 2.0505709648132324
Validation loss: 2.229426085948944

Epoch: 6| Step: 4
Training loss: 2.2246103286743164
Validation loss: 2.18546599149704

Epoch: 6| Step: 5
Training loss: 1.7931461334228516
Validation loss: 2.1994307041168213

Epoch: 6| Step: 6
Training loss: 1.9322699308395386
Validation loss: 2.205364982287089

Epoch: 6| Step: 7
Training loss: 1.674023151397705
Validation loss: 2.186051607131958

Epoch: 6| Step: 8
Training loss: 1.7607676982879639
Validation loss: 2.17455518245697

Epoch: 6| Step: 9
Training loss: 1.2052655220031738
Validation loss: 2.1589250961939492

Epoch: 6| Step: 10
Training loss: 2.054248809814453
Validation loss: 2.179851452509562

Epoch: 6| Step: 11
Training loss: 1.9519273042678833
Validation loss: 2.1833298206329346

Epoch: 6| Step: 12
Training loss: 1.9451638460159302
Validation loss: 2.189476410547892

Epoch: 6| Step: 13
Training loss: 2.070516347885132
Validation loss: 2.1675759156545005

Epoch: 264| Step: 0
Training loss: 2.0026774406433105
Validation loss: 2.1702467799186707

Epoch: 6| Step: 1
Training loss: 1.770595908164978
Validation loss: 2.190215786298116

Epoch: 6| Step: 2
Training loss: 1.90195631980896
Validation loss: 2.1677261789639792

Epoch: 6| Step: 3
Training loss: 2.1057190895080566
Validation loss: 2.1789785027503967

Epoch: 6| Step: 4
Training loss: 2.1426329612731934
Validation loss: 2.1758304238319397

Epoch: 6| Step: 5
Training loss: 2.382789373397827
Validation loss: 2.1733795404434204

Epoch: 6| Step: 6
Training loss: 1.8943883180618286
Validation loss: 2.183615724245707

Epoch: 6| Step: 7
Training loss: 1.6009498834609985
Validation loss: 2.161630551020304

Epoch: 6| Step: 8
Training loss: 1.1821503639221191
Validation loss: 2.172382632891337

Epoch: 6| Step: 9
Training loss: 1.6012600660324097
Validation loss: 2.2306460539499917

Epoch: 6| Step: 10
Training loss: 1.2052292823791504
Validation loss: 2.2650144894917807

Epoch: 6| Step: 11
Training loss: 2.7236108779907227
Validation loss: 2.2698691884676614

Epoch: 6| Step: 12
Training loss: 1.334301471710205
Validation loss: 2.2756638725598655

Epoch: 6| Step: 13
Training loss: 1.7500252723693848
Validation loss: 2.2844281593958535

Epoch: 265| Step: 0
Training loss: 2.6574172973632812
Validation loss: 2.2938082615534463

Epoch: 6| Step: 1
Training loss: 1.5220415592193604
Validation loss: 2.2933655381202698

Epoch: 6| Step: 2
Training loss: 1.576460599899292
Validation loss: 2.2434901793797812

Epoch: 6| Step: 3
Training loss: 1.7425527572631836
Validation loss: 2.2308296163876853

Epoch: 6| Step: 4
Training loss: 1.720337152481079
Validation loss: 2.185159683227539

Epoch: 6| Step: 5
Training loss: 1.9240576028823853
Validation loss: 2.172642409801483

Epoch: 6| Step: 6
Training loss: 1.3792338371276855
Validation loss: 2.1461921334266663

Epoch: 6| Step: 7
Training loss: 1.568176507949829
Validation loss: 2.138118882973989

Epoch: 6| Step: 8
Training loss: 1.7710741758346558
Validation loss: 2.133243203163147

Epoch: 6| Step: 9
Training loss: 1.8586015701293945
Validation loss: 2.1512420177459717

Epoch: 6| Step: 10
Training loss: 1.7902003526687622
Validation loss: 2.140162626902262

Epoch: 6| Step: 11
Training loss: 2.294644832611084
Validation loss: 2.1334428787231445

Epoch: 6| Step: 12
Training loss: 2.47660756111145
Validation loss: 2.144539733727773

Epoch: 6| Step: 13
Training loss: 2.382689952850342
Validation loss: 2.1622673869132996

Epoch: 266| Step: 0
Training loss: 1.710409164428711
Validation loss: 2.1702521642049155

Epoch: 6| Step: 1
Training loss: 1.6053895950317383
Validation loss: 2.187108894189199

Epoch: 6| Step: 2
Training loss: 1.1899926662445068
Validation loss: 2.2032341361045837

Epoch: 6| Step: 3
Training loss: 2.2639107704162598
Validation loss: 2.2165881395339966

Epoch: 6| Step: 4
Training loss: 2.093829393386841
Validation loss: 2.2219107151031494

Epoch: 6| Step: 5
Training loss: 2.1147282123565674
Validation loss: 2.227050483226776

Epoch: 6| Step: 6
Training loss: 1.8389496803283691
Validation loss: 2.208474596341451

Epoch: 6| Step: 7
Training loss: 1.50789475440979
Validation loss: 2.2275002201398215

Epoch: 6| Step: 8
Training loss: 1.245297908782959
Validation loss: 2.2330180406570435

Epoch: 6| Step: 9
Training loss: 1.9094089269638062
Validation loss: 2.237162152926127

Epoch: 6| Step: 10
Training loss: 1.6735577583312988
Validation loss: 2.2244774103164673

Epoch: 6| Step: 11
Training loss: 1.7969062328338623
Validation loss: 2.223175307114919

Epoch: 6| Step: 12
Training loss: 2.289581537246704
Validation loss: 2.224310557047526

Epoch: 6| Step: 13
Training loss: 1.485072374343872
Validation loss: 2.2072635690371194

Epoch: 267| Step: 0
Training loss: 1.6596400737762451
Validation loss: 2.205999235312144

Epoch: 6| Step: 1
Training loss: 1.612524390220642
Validation loss: 2.19606747229894

Epoch: 6| Step: 2
Training loss: 2.7954277992248535
Validation loss: 2.189961592356364

Epoch: 6| Step: 3
Training loss: 1.4001123905181885
Validation loss: 2.16885507106781

Epoch: 6| Step: 4
Training loss: 1.2071948051452637
Validation loss: 2.1822860638300576

Epoch: 6| Step: 5
Training loss: 1.635087013244629
Validation loss: 2.190398414929708

Epoch: 6| Step: 6
Training loss: 1.2430338859558105
Validation loss: 2.1684595942497253

Epoch: 6| Step: 7
Training loss: 1.7231236696243286
Validation loss: 2.197701911131541

Epoch: 6| Step: 8
Training loss: 0.9900538921356201
Validation loss: 2.220726648966471

Epoch: 6| Step: 9
Training loss: 1.6749625205993652
Validation loss: 2.20174648364385

Epoch: 6| Step: 10
Training loss: 2.0896036624908447
Validation loss: 2.215609868367513

Epoch: 6| Step: 11
Training loss: 2.197288990020752
Validation loss: 2.2283912102381387

Epoch: 6| Step: 12
Training loss: 2.4161651134490967
Validation loss: 2.2556719382603965

Epoch: 6| Step: 13
Training loss: 2.0535988807678223
Validation loss: 2.250821908315023

Epoch: 268| Step: 0
Training loss: 1.3149653673171997
Validation loss: 2.2586286465326944

Epoch: 6| Step: 1
Training loss: 2.0028176307678223
Validation loss: 2.2514992157618203

Epoch: 6| Step: 2
Training loss: 2.149071216583252
Validation loss: 2.224900563557943

Epoch: 6| Step: 3
Training loss: 1.4540300369262695
Validation loss: 2.254517674446106

Epoch: 6| Step: 4
Training loss: 1.9369550943374634
Validation loss: 2.227005044619242

Epoch: 6| Step: 5
Training loss: 1.7832049131393433
Validation loss: 2.2054541309674582

Epoch: 6| Step: 6
Training loss: 1.7906904220581055
Validation loss: 2.2095057169596353

Epoch: 6| Step: 7
Training loss: 1.9798115491867065
Validation loss: 2.212506572405497

Epoch: 6| Step: 8
Training loss: 2.1595067977905273
Validation loss: 2.2424160043398538

Epoch: 6| Step: 9
Training loss: 1.2099685668945312
Validation loss: 2.217450737953186

Epoch: 6| Step: 10
Training loss: 1.4645106792449951
Validation loss: 2.212661345799764

Epoch: 6| Step: 11
Training loss: 2.05582332611084
Validation loss: 2.2155255873998008

Epoch: 6| Step: 12
Training loss: 1.8217018842697144
Validation loss: 2.2317662835121155

Epoch: 6| Step: 13
Training loss: 1.6059516668319702
Validation loss: 2.230370362599691

Epoch: 269| Step: 0
Training loss: 1.1454567909240723
Validation loss: 2.234828452269236

Epoch: 6| Step: 1
Training loss: 2.1889588832855225
Validation loss: 2.2244845827420554

Epoch: 6| Step: 2
Training loss: 1.2841441631317139
Validation loss: 2.2213515440622964

Epoch: 6| Step: 3
Training loss: 1.9497510194778442
Validation loss: 2.230355223019918

Epoch: 6| Step: 4
Training loss: 1.8340766429901123
Validation loss: 2.217365245024363

Epoch: 6| Step: 5
Training loss: 1.499936819076538
Validation loss: 2.2166582345962524

Epoch: 6| Step: 6
Training loss: 2.024949073791504
Validation loss: 2.1746100982030234

Epoch: 6| Step: 7
Training loss: 1.263925313949585
Validation loss: 2.1957019170125327

Epoch: 6| Step: 8
Training loss: 1.8245123624801636
Validation loss: 2.1996306578318277

Epoch: 6| Step: 9
Training loss: 1.509934902191162
Validation loss: 2.176593085130056

Epoch: 6| Step: 10
Training loss: 2.4704108238220215
Validation loss: 2.1944576700528464

Epoch: 6| Step: 11
Training loss: 2.7844576835632324
Validation loss: 2.1758360266685486

Epoch: 6| Step: 12
Training loss: 1.2215285301208496
Validation loss: 2.19346821308136

Epoch: 6| Step: 13
Training loss: 1.870690107345581
Validation loss: 2.1938419143358865

Epoch: 270| Step: 0
Training loss: 1.7873775959014893
Validation loss: 2.207664450009664

Epoch: 6| Step: 1
Training loss: 1.9599751234054565
Validation loss: 2.2094608743985495

Epoch: 6| Step: 2
Training loss: 2.30954909324646
Validation loss: 2.2037227551142373

Epoch: 6| Step: 3
Training loss: 1.9426698684692383
Validation loss: 2.20623650153478

Epoch: 6| Step: 4
Training loss: 1.3259568214416504
Validation loss: 2.2261207898457847

Epoch: 6| Step: 5
Training loss: 1.7539855241775513
Validation loss: 2.2335225343704224

Epoch: 6| Step: 6
Training loss: 1.536084532737732
Validation loss: 2.2689555883407593

Epoch: 6| Step: 7
Training loss: 1.5094854831695557
Validation loss: 2.213182727495829

Epoch: 6| Step: 8
Training loss: 1.73054838180542
Validation loss: 2.18017307917277

Epoch: 6| Step: 9
Training loss: 1.6429016590118408
Validation loss: 2.1917558113733926

Epoch: 6| Step: 10
Training loss: 2.6882894039154053
Validation loss: 2.186759908994039

Epoch: 6| Step: 11
Training loss: 2.0254909992218018
Validation loss: 2.198335369427999

Epoch: 6| Step: 12
Training loss: 1.738999843597412
Validation loss: 2.193459471066793

Epoch: 6| Step: 13
Training loss: 1.2907769680023193
Validation loss: 2.1763368447621665

Epoch: 271| Step: 0
Training loss: 1.7243729829788208
Validation loss: 2.195421735445658

Epoch: 6| Step: 1
Training loss: 1.4742121696472168
Validation loss: 2.1981703440348306

Epoch: 6| Step: 2
Training loss: 0.9615046977996826
Validation loss: 2.1803017457326255

Epoch: 6| Step: 3
Training loss: 1.9140387773513794
Validation loss: 2.209937870502472

Epoch: 6| Step: 4
Training loss: 1.8504271507263184
Validation loss: 2.1958682338396707

Epoch: 6| Step: 5
Training loss: 1.8243982791900635
Validation loss: 2.185769339402517

Epoch: 6| Step: 6
Training loss: 1.8605302572250366
Validation loss: 2.1938610871632895

Epoch: 6| Step: 7
Training loss: 1.8447835445404053
Validation loss: 2.19210414091746

Epoch: 6| Step: 8
Training loss: 1.9995813369750977
Validation loss: 2.2087408304214478

Epoch: 6| Step: 9
Training loss: 1.3051042556762695
Validation loss: 2.1936012903849282

Epoch: 6| Step: 10
Training loss: 1.5022201538085938
Validation loss: 2.231842835744222

Epoch: 6| Step: 11
Training loss: 2.008237361907959
Validation loss: 2.23099684715271

Epoch: 6| Step: 12
Training loss: 1.7796038389205933
Validation loss: 2.2372783422470093

Epoch: 6| Step: 13
Training loss: 2.4529967308044434
Validation loss: 2.2477075854937234

Epoch: 272| Step: 0
Training loss: 1.169600248336792
Validation loss: 2.251550257205963

Epoch: 6| Step: 1
Training loss: 1.5718822479248047
Validation loss: 2.286820650100708

Epoch: 6| Step: 2
Training loss: 1.3725144863128662
Validation loss: 2.2702913681666055

Epoch: 6| Step: 3
Training loss: 1.4696303606033325
Validation loss: 2.2672133445739746

Epoch: 6| Step: 4
Training loss: 1.8491981029510498
Validation loss: 2.2456745902697244

Epoch: 6| Step: 5
Training loss: 2.109154224395752
Validation loss: 2.2466020782788596

Epoch: 6| Step: 6
Training loss: 1.4458038806915283
Validation loss: 2.223513901233673

Epoch: 6| Step: 7
Training loss: 2.7071704864501953
Validation loss: 2.2282569805781045

Epoch: 6| Step: 8
Training loss: 2.2026519775390625
Validation loss: 2.2257104913393655

Epoch: 6| Step: 9
Training loss: 1.614970088005066
Validation loss: 2.2066603700319924

Epoch: 6| Step: 10
Training loss: 1.990498661994934
Validation loss: 2.246439754962921

Epoch: 6| Step: 11
Training loss: 1.6409964561462402
Validation loss: 2.201991538206736

Epoch: 6| Step: 12
Training loss: 1.7198896408081055
Validation loss: 2.1899383465449014

Epoch: 6| Step: 13
Training loss: 1.7628227472305298
Validation loss: 2.189623713493347

Epoch: 273| Step: 0
Training loss: 1.8390183448791504
Validation loss: 2.1869008938471475

Epoch: 6| Step: 1
Training loss: 1.2079689502716064
Validation loss: 2.1870731314023337

Epoch: 6| Step: 2
Training loss: 1.1321144104003906
Validation loss: 2.2147833108901978

Epoch: 6| Step: 3
Training loss: 1.8909224271774292
Validation loss: 2.1963791648546853

Epoch: 6| Step: 4
Training loss: 1.434781789779663
Validation loss: 2.205037514368693

Epoch: 6| Step: 5
Training loss: 2.3295931816101074
Validation loss: 2.2135717471440635

Epoch: 6| Step: 6
Training loss: 1.6959450244903564
Validation loss: 2.2310915986696878

Epoch: 6| Step: 7
Training loss: 1.9260294437408447
Validation loss: 2.2421970764795938

Epoch: 6| Step: 8
Training loss: 2.132258653640747
Validation loss: 2.240509649117788

Epoch: 6| Step: 9
Training loss: 1.1452393531799316
Validation loss: 2.2186266581217446

Epoch: 6| Step: 10
Training loss: 2.75132155418396
Validation loss: 2.2249717911084494

Epoch: 6| Step: 11
Training loss: 1.753173828125
Validation loss: 2.2191210786501565

Epoch: 6| Step: 12
Training loss: 1.91438889503479
Validation loss: 2.2058767875035605

Epoch: 6| Step: 13
Training loss: 1.7032278776168823
Validation loss: 2.182050267855326

Epoch: 274| Step: 0
Training loss: 1.7112627029418945
Validation loss: 2.200711727142334

Epoch: 6| Step: 1
Training loss: 1.5590838193893433
Validation loss: 2.1665360927581787

Epoch: 6| Step: 2
Training loss: 2.119088649749756
Validation loss: 2.181653400262197

Epoch: 6| Step: 3
Training loss: 2.030219793319702
Validation loss: 2.2086477677027383

Epoch: 6| Step: 4
Training loss: 1.6868963241577148
Validation loss: 2.234021306037903

Epoch: 6| Step: 5
Training loss: 1.374822735786438
Validation loss: 2.213535189628601

Epoch: 6| Step: 6
Training loss: 1.067330002784729
Validation loss: 2.2360658645629883

Epoch: 6| Step: 7
Training loss: 2.1930930614471436
Validation loss: 2.1951164603233337

Epoch: 6| Step: 8
Training loss: 1.166666030883789
Validation loss: 2.2087388237317405

Epoch: 6| Step: 9
Training loss: 2.1844491958618164
Validation loss: 2.2282865842183432

Epoch: 6| Step: 10
Training loss: 2.2102668285369873
Validation loss: 2.229848305384318

Epoch: 6| Step: 11
Training loss: 1.8198237419128418
Validation loss: 2.238398333390554

Epoch: 6| Step: 12
Training loss: 1.5734186172485352
Validation loss: 2.2262320121129355

Epoch: 6| Step: 13
Training loss: 1.822332501411438
Validation loss: 2.2359314362208047

Epoch: 275| Step: 0
Training loss: 1.8268444538116455
Validation loss: 2.2102006872495017

Epoch: 6| Step: 1
Training loss: 2.062319278717041
Validation loss: 2.2176180680592856

Epoch: 6| Step: 2
Training loss: 2.373013973236084
Validation loss: 2.2094425559043884

Epoch: 6| Step: 3
Training loss: 1.324009656906128
Validation loss: 2.202374796072642

Epoch: 6| Step: 4
Training loss: 1.6887633800506592
Validation loss: 2.2044966220855713

Epoch: 6| Step: 5
Training loss: 2.106846570968628
Validation loss: 2.1793437798817954

Epoch: 6| Step: 6
Training loss: 2.021191358566284
Validation loss: 2.1964021722475686

Epoch: 6| Step: 7
Training loss: 1.962350606918335
Validation loss: 2.1900077859560647

Epoch: 6| Step: 8
Training loss: 1.037219762802124
Validation loss: 2.187833031018575

Epoch: 6| Step: 9
Training loss: 1.7849152088165283
Validation loss: 2.2030441761016846

Epoch: 6| Step: 10
Training loss: 1.8714666366577148
Validation loss: 2.191753168900808

Epoch: 6| Step: 11
Training loss: 1.820114254951477
Validation loss: 2.223537882169088

Epoch: 6| Step: 12
Training loss: 1.4142966270446777
Validation loss: 2.2332614262898765

Epoch: 6| Step: 13
Training loss: 1.5008535385131836
Validation loss: 2.249484916528066

Epoch: 276| Step: 0
Training loss: 2.097604990005493
Validation loss: 2.291580239931742

Epoch: 6| Step: 1
Training loss: 1.730499029159546
Validation loss: 2.2962605158487954

Epoch: 6| Step: 2
Training loss: 1.5825228691101074
Validation loss: 2.297343929608663

Epoch: 6| Step: 3
Training loss: 2.1483774185180664
Validation loss: 2.260599454243978

Epoch: 6| Step: 4
Training loss: 2.1321632862091064
Validation loss: 2.2255595525105796

Epoch: 6| Step: 5
Training loss: 1.6785264015197754
Validation loss: 2.2468767563501992

Epoch: 6| Step: 6
Training loss: 1.283806562423706
Validation loss: 2.2228755950927734

Epoch: 6| Step: 7
Training loss: 1.9558676481246948
Validation loss: 2.2234911719957986

Epoch: 6| Step: 8
Training loss: 2.3254358768463135
Validation loss: 2.201741655667623

Epoch: 6| Step: 9
Training loss: 1.7891126871109009
Validation loss: 2.2095022598902383

Epoch: 6| Step: 10
Training loss: 1.8151662349700928
Validation loss: 2.253405769666036

Epoch: 6| Step: 11
Training loss: 1.6890212297439575
Validation loss: 2.236922880013784

Epoch: 6| Step: 12
Training loss: 1.136609673500061
Validation loss: 2.2536049087842307

Epoch: 6| Step: 13
Training loss: 1.678064227104187
Validation loss: 2.242673933506012

Epoch: 277| Step: 0
Training loss: 2.0008773803710938
Validation loss: 2.2013970216115317

Epoch: 6| Step: 1
Training loss: 1.7239583730697632
Validation loss: 2.211606820424398

Epoch: 6| Step: 2
Training loss: 1.48915696144104
Validation loss: 2.207520604133606

Epoch: 6| Step: 3
Training loss: 1.5890960693359375
Validation loss: 2.215697407722473

Epoch: 6| Step: 4
Training loss: 1.7844188213348389
Validation loss: 2.184370994567871

Epoch: 6| Step: 5
Training loss: 1.797072410583496
Validation loss: 2.213747799396515

Epoch: 6| Step: 6
Training loss: 2.2888193130493164
Validation loss: 2.216303586959839

Epoch: 6| Step: 7
Training loss: 2.0326180458068848
Validation loss: 2.209098696708679

Epoch: 6| Step: 8
Training loss: 2.075929641723633
Validation loss: 2.2033472657203674

Epoch: 6| Step: 9
Training loss: 1.746949315071106
Validation loss: 2.2142442067464194

Epoch: 6| Step: 10
Training loss: 1.3258752822875977
Validation loss: 2.215470532576243

Epoch: 6| Step: 11
Training loss: 1.5170987844467163
Validation loss: 2.2135099172592163

Epoch: 6| Step: 12
Training loss: 1.2499821186065674
Validation loss: 2.208353598912557

Epoch: 6| Step: 13
Training loss: 1.6007870435714722
Validation loss: 2.231831669807434

Epoch: 278| Step: 0
Training loss: 1.5020942687988281
Validation loss: 2.2345998287200928

Epoch: 6| Step: 1
Training loss: 2.384032726287842
Validation loss: 2.251975655555725

Epoch: 6| Step: 2
Training loss: 2.0155062675476074
Validation loss: 2.213348905245463

Epoch: 6| Step: 3
Training loss: 1.7000926733016968
Validation loss: 2.216772178808848

Epoch: 6| Step: 4
Training loss: 1.62026047706604
Validation loss: 2.2100089391072593

Epoch: 6| Step: 5
Training loss: 1.8881700038909912
Validation loss: 2.2059476375579834

Epoch: 6| Step: 6
Training loss: 1.4344875812530518
Validation loss: 2.2043540676434836

Epoch: 6| Step: 7
Training loss: 1.620519995689392
Validation loss: 2.2177409331003823

Epoch: 6| Step: 8
Training loss: 1.4652996063232422
Validation loss: 2.2097678581873574

Epoch: 6| Step: 9
Training loss: 2.5531420707702637
Validation loss: 2.218353569507599

Epoch: 6| Step: 10
Training loss: 1.5460705757141113
Validation loss: 2.1880303223927817

Epoch: 6| Step: 11
Training loss: 1.684903860092163
Validation loss: 2.2110759218533835

Epoch: 6| Step: 12
Training loss: 1.4007904529571533
Validation loss: 2.1886475483576455

Epoch: 6| Step: 13
Training loss: 1.471108317375183
Validation loss: 2.176854888598124

Epoch: 279| Step: 0
Training loss: 1.8666397333145142
Validation loss: 2.208234190940857

Epoch: 6| Step: 1
Training loss: 1.8736002445220947
Validation loss: 2.191737115383148

Epoch: 6| Step: 2
Training loss: 1.8654741048812866
Validation loss: 2.1979962388674417

Epoch: 6| Step: 3
Training loss: 1.5091686248779297
Validation loss: 2.197994569937388

Epoch: 6| Step: 4
Training loss: 2.343755006790161
Validation loss: 2.1865616043408713

Epoch: 6| Step: 5
Training loss: 1.6710395812988281
Validation loss: 2.211788455645243

Epoch: 6| Step: 6
Training loss: 1.3566937446594238
Validation loss: 2.226949135462443

Epoch: 6| Step: 7
Training loss: 1.5168724060058594
Validation loss: 2.2282884319623313

Epoch: 6| Step: 8
Training loss: 1.1862623691558838
Validation loss: 2.1914661725362143

Epoch: 6| Step: 9
Training loss: 1.6200146675109863
Validation loss: 2.237798035144806

Epoch: 6| Step: 10
Training loss: 1.6290761232376099
Validation loss: 2.216358741124471

Epoch: 6| Step: 11
Training loss: 2.28348970413208
Validation loss: 2.2053863406181335

Epoch: 6| Step: 12
Training loss: 2.0780959129333496
Validation loss: 2.2043301661809287

Epoch: 6| Step: 13
Training loss: 1.7467823028564453
Validation loss: 2.1873194177945456

Epoch: 280| Step: 0
Training loss: 1.6167892217636108
Validation loss: 2.1871854662895203

Epoch: 6| Step: 1
Training loss: 2.162148952484131
Validation loss: 2.1973549922307334

Epoch: 6| Step: 2
Training loss: 1.791703224182129
Validation loss: 2.20346333583196

Epoch: 6| Step: 3
Training loss: 1.5898547172546387
Validation loss: 2.2048702041308084

Epoch: 6| Step: 4
Training loss: 2.2009263038635254
Validation loss: 2.2017888029416404

Epoch: 6| Step: 5
Training loss: 1.8453282117843628
Validation loss: 2.2182693481445312

Epoch: 6| Step: 6
Training loss: 1.5648295879364014
Validation loss: 2.212486962477366

Epoch: 6| Step: 7
Training loss: 1.8763395547866821
Validation loss: 2.225844164689382

Epoch: 6| Step: 8
Training loss: 1.4176723957061768
Validation loss: 2.2388493021329245

Epoch: 6| Step: 9
Training loss: 2.0400619506835938
Validation loss: 2.2089189688364663

Epoch: 6| Step: 10
Training loss: 1.980545997619629
Validation loss: 2.2379575769106546

Epoch: 6| Step: 11
Training loss: 0.7799454927444458
Validation loss: 2.234366714954376

Epoch: 6| Step: 12
Training loss: 1.5560576915740967
Validation loss: 2.2337458729743958

Epoch: 6| Step: 13
Training loss: 2.2931675910949707
Validation loss: 2.240929921468099

Epoch: 281| Step: 0
Training loss: 2.0848255157470703
Validation loss: 2.2297598123550415

Epoch: 6| Step: 1
Training loss: 1.9307043552398682
Validation loss: 2.247914969921112

Epoch: 6| Step: 2
Training loss: 1.5409749746322632
Validation loss: 2.2233043710390725

Epoch: 6| Step: 3
Training loss: 1.6577523946762085
Validation loss: 2.25740917523702

Epoch: 6| Step: 4
Training loss: 1.3556931018829346
Validation loss: 2.2754064400990806

Epoch: 6| Step: 5
Training loss: 1.8782343864440918
Validation loss: 2.2585941155751548

Epoch: 6| Step: 6
Training loss: 1.3006844520568848
Validation loss: 2.240033666292826

Epoch: 6| Step: 7
Training loss: 2.3899996280670166
Validation loss: 2.2172200282414756

Epoch: 6| Step: 8
Training loss: 1.9512938261032104
Validation loss: 2.192925214767456

Epoch: 6| Step: 9
Training loss: 1.8227977752685547
Validation loss: 2.2085583209991455

Epoch: 6| Step: 10
Training loss: 1.3617186546325684
Validation loss: 2.1847878098487854

Epoch: 6| Step: 11
Training loss: 1.918900489807129
Validation loss: 2.1979751586914062

Epoch: 6| Step: 12
Training loss: 1.4947861433029175
Validation loss: 2.1857044100761414

Epoch: 6| Step: 13
Training loss: 1.6755387783050537
Validation loss: 2.194606522719065

Epoch: 282| Step: 0
Training loss: 1.597259521484375
Validation loss: 2.196955064932505

Epoch: 6| Step: 1
Training loss: 1.5952703952789307
Validation loss: 2.1944709022839866

Epoch: 6| Step: 2
Training loss: 1.4758596420288086
Validation loss: 2.189331372578939

Epoch: 6| Step: 3
Training loss: 1.4364187717437744
Validation loss: 2.2172451615333557

Epoch: 6| Step: 4
Training loss: 2.4140570163726807
Validation loss: 2.219294706980387

Epoch: 6| Step: 5
Training loss: 2.838095188140869
Validation loss: 2.2250524361928306

Epoch: 6| Step: 6
Training loss: 1.219860315322876
Validation loss: 2.238901992638906

Epoch: 6| Step: 7
Training loss: 1.6602336168289185
Validation loss: 2.25068465868632

Epoch: 6| Step: 8
Training loss: 2.0150012969970703
Validation loss: 2.2408758401870728

Epoch: 6| Step: 9
Training loss: 1.9099770784378052
Validation loss: 2.26271520058314

Epoch: 6| Step: 10
Training loss: 2.1546530723571777
Validation loss: 2.247507075468699

Epoch: 6| Step: 11
Training loss: 1.170379877090454
Validation loss: 2.232917825380961

Epoch: 6| Step: 12
Training loss: 1.7554051876068115
Validation loss: 2.249804735183716

Epoch: 6| Step: 13
Training loss: 1.4159126281738281
Validation loss: 2.2355693777402244

Epoch: 283| Step: 0
Training loss: 2.069453716278076
Validation loss: 2.217010498046875

Epoch: 6| Step: 1
Training loss: 1.6330300569534302
Validation loss: 2.2157905101776123

Epoch: 6| Step: 2
Training loss: 1.856624960899353
Validation loss: 2.2348402937253318

Epoch: 6| Step: 3
Training loss: 2.042341947555542
Validation loss: 2.2121978600819907

Epoch: 6| Step: 4
Training loss: 1.79835045337677
Validation loss: 2.2291096647580466

Epoch: 6| Step: 5
Training loss: 1.3446776866912842
Validation loss: 2.2138443986574807

Epoch: 6| Step: 6
Training loss: 2.000119209289551
Validation loss: 2.213365594546

Epoch: 6| Step: 7
Training loss: 1.555260181427002
Validation loss: 2.22006622950236

Epoch: 6| Step: 8
Training loss: 1.7447141408920288
Validation loss: 2.2184852957725525

Epoch: 6| Step: 9
Training loss: 2.3536863327026367
Validation loss: 2.2223026752471924

Epoch: 6| Step: 10
Training loss: 1.1964550018310547
Validation loss: 2.2834197282791138

Epoch: 6| Step: 11
Training loss: 1.5580134391784668
Validation loss: 2.309837063153585

Epoch: 6| Step: 12
Training loss: 2.140284776687622
Validation loss: 2.2583955923716226

Epoch: 6| Step: 13
Training loss: 1.6050307750701904
Validation loss: 2.2994391719500222

Epoch: 284| Step: 0
Training loss: 1.3258888721466064
Validation loss: 2.2726399103800454

Epoch: 6| Step: 1
Training loss: 1.6422765254974365
Validation loss: 2.291617194811503

Epoch: 6| Step: 2
Training loss: 2.5344491004943848
Validation loss: 2.287664294242859

Epoch: 6| Step: 3
Training loss: 2.085704803466797
Validation loss: 2.2727962334950766

Epoch: 6| Step: 4
Training loss: 2.3283824920654297
Validation loss: 2.25751785437266

Epoch: 6| Step: 5
Training loss: 1.7123442888259888
Validation loss: 2.262681802113851

Epoch: 6| Step: 6
Training loss: 1.975220799446106
Validation loss: 2.2132451931635537

Epoch: 6| Step: 7
Training loss: 1.640542984008789
Validation loss: 2.1977705160776773

Epoch: 6| Step: 8
Training loss: 1.3432821035385132
Validation loss: 2.1805991530418396

Epoch: 6| Step: 9
Training loss: 1.4408776760101318
Validation loss: 2.1748294631640115

Epoch: 6| Step: 10
Training loss: 1.094428539276123
Validation loss: 2.17077507575353

Epoch: 6| Step: 11
Training loss: 1.8293495178222656
Validation loss: 2.1561037500699363

Epoch: 6| Step: 12
Training loss: 1.978912591934204
Validation loss: 2.160664697488149

Epoch: 6| Step: 13
Training loss: 2.521660327911377
Validation loss: 2.1832160552342734

Epoch: 285| Step: 0
Training loss: 1.6774346828460693
Validation loss: 2.2117602229118347

Epoch: 6| Step: 1
Training loss: 1.4428081512451172
Validation loss: 2.2331170439720154

Epoch: 6| Step: 2
Training loss: 1.1670119762420654
Validation loss: 2.2320474982261658

Epoch: 6| Step: 3
Training loss: 1.8202909231185913
Validation loss: 2.213896155357361

Epoch: 6| Step: 4
Training loss: 2.223571300506592
Validation loss: 2.268728017807007

Epoch: 6| Step: 5
Training loss: 2.136514902114868
Validation loss: 2.2725338339805603

Epoch: 6| Step: 6
Training loss: 2.238618850708008
Validation loss: 2.2674110333124795

Epoch: 6| Step: 7
Training loss: 1.8633105754852295
Validation loss: 2.2910183668136597

Epoch: 6| Step: 8
Training loss: 1.5575785636901855
Validation loss: 2.300700624783834

Epoch: 6| Step: 9
Training loss: 1.2459052801132202
Validation loss: 2.306321461995443

Epoch: 6| Step: 10
Training loss: 1.542806625366211
Validation loss: 2.2454712192217507

Epoch: 6| Step: 11
Training loss: 2.689551830291748
Validation loss: 2.282311797142029

Epoch: 6| Step: 12
Training loss: 1.357660174369812
Validation loss: 2.258198857307434

Epoch: 6| Step: 13
Training loss: 1.9087536334991455
Validation loss: 2.2804882327715554

Epoch: 286| Step: 0
Training loss: 1.454930067062378
Validation loss: 2.2669255336125693

Epoch: 6| Step: 1
Training loss: 1.5720527172088623
Validation loss: 2.257768392562866

Epoch: 6| Step: 2
Training loss: 1.9632080793380737
Validation loss: 2.242604990800222

Epoch: 6| Step: 3
Training loss: 1.6457788944244385
Validation loss: 2.2663604418436685

Epoch: 6| Step: 4
Training loss: 1.6451796293258667
Validation loss: 2.249481717745463

Epoch: 6| Step: 5
Training loss: 2.290066957473755
Validation loss: 2.23945681254069

Epoch: 6| Step: 6
Training loss: 1.9006050825119019
Validation loss: 2.23108446598053

Epoch: 6| Step: 7
Training loss: 2.4121956825256348
Validation loss: 2.229745785395304

Epoch: 6| Step: 8
Training loss: 1.1243932247161865
Validation loss: 2.2412749528884888

Epoch: 6| Step: 9
Training loss: 1.3007254600524902
Validation loss: 2.2253249883651733

Epoch: 6| Step: 10
Training loss: 2.217744827270508
Validation loss: 2.253532906373342

Epoch: 6| Step: 11
Training loss: 1.5348892211914062
Validation loss: 2.242887099583944

Epoch: 6| Step: 12
Training loss: 1.7453463077545166
Validation loss: 2.259275794029236

Epoch: 6| Step: 13
Training loss: 1.5073004961013794
Validation loss: 2.2207990884780884

Epoch: 287| Step: 0
Training loss: 1.7788147926330566
Validation loss: 2.2516958912213645

Epoch: 6| Step: 1
Training loss: 1.211040735244751
Validation loss: 2.2336872816085815

Epoch: 6| Step: 2
Training loss: 2.0372121334075928
Validation loss: 2.2389983336130777

Epoch: 6| Step: 3
Training loss: 1.3991279602050781
Validation loss: 2.243665595849355

Epoch: 6| Step: 4
Training loss: 1.3614907264709473
Validation loss: 2.2723728020985923

Epoch: 6| Step: 5
Training loss: 2.8416874408721924
Validation loss: 2.2689641316731772

Epoch: 6| Step: 6
Training loss: 1.614685297012329
Validation loss: 2.2790939013163247

Epoch: 6| Step: 7
Training loss: 1.552270531654358
Validation loss: 2.2906136910120645

Epoch: 6| Step: 8
Training loss: 1.5774585008621216
Validation loss: 2.276116907596588

Epoch: 6| Step: 9
Training loss: 1.4379593133926392
Validation loss: 2.292927543322245

Epoch: 6| Step: 10
Training loss: 1.547741413116455
Validation loss: 2.2811032136281333

Epoch: 6| Step: 11
Training loss: 2.2474710941314697
Validation loss: 2.275398333867391

Epoch: 6| Step: 12
Training loss: 1.6138392686843872
Validation loss: 2.2710772355397544

Epoch: 6| Step: 13
Training loss: 1.4614596366882324
Validation loss: 2.2613653341929116

Epoch: 288| Step: 0
Training loss: 2.036632537841797
Validation loss: 2.2706434528032937

Epoch: 6| Step: 1
Training loss: 0.9190250635147095
Validation loss: 2.228487511475881

Epoch: 6| Step: 2
Training loss: 2.124964952468872
Validation loss: 2.220301846663157

Epoch: 6| Step: 3
Training loss: 1.9016956090927124
Validation loss: 2.2144179344177246

Epoch: 6| Step: 4
Training loss: 2.031125783920288
Validation loss: 2.2026099363962808

Epoch: 6| Step: 5
Training loss: 1.7138835191726685
Validation loss: 2.2029970288276672

Epoch: 6| Step: 6
Training loss: 2.0807361602783203
Validation loss: 2.1850258111953735

Epoch: 6| Step: 7
Training loss: 1.4440884590148926
Validation loss: 2.2115447322527566

Epoch: 6| Step: 8
Training loss: 2.366412401199341
Validation loss: 2.2171632051467896

Epoch: 6| Step: 9
Training loss: 1.8761630058288574
Validation loss: 2.210575501124064

Epoch: 6| Step: 10
Training loss: 1.7246484756469727
Validation loss: 2.2336178024609885

Epoch: 6| Step: 11
Training loss: 1.3780596256256104
Validation loss: 2.2899863719940186

Epoch: 6| Step: 12
Training loss: 1.548661231994629
Validation loss: 2.2979090213775635

Epoch: 6| Step: 13
Training loss: 1.5569357872009277
Validation loss: 2.2947310407956443

Epoch: 289| Step: 0
Training loss: 1.6768841743469238
Validation loss: 2.3124326864878335

Epoch: 6| Step: 1
Training loss: 1.3162846565246582
Validation loss: 2.28461225827535

Epoch: 6| Step: 2
Training loss: 2.2048702239990234
Validation loss: 2.259006758530935

Epoch: 6| Step: 3
Training loss: 2.0593700408935547
Validation loss: 2.2554257114728293

Epoch: 6| Step: 4
Training loss: 1.7387051582336426
Validation loss: 2.24325168132782

Epoch: 6| Step: 5
Training loss: 1.6192607879638672
Validation loss: 2.23401270310084

Epoch: 6| Step: 6
Training loss: 1.550886631011963
Validation loss: 2.2412532369295755

Epoch: 6| Step: 7
Training loss: 1.1693822145462036
Validation loss: 2.238776743412018

Epoch: 6| Step: 8
Training loss: 2.0884878635406494
Validation loss: 2.239555319150289

Epoch: 6| Step: 9
Training loss: 1.3385369777679443
Validation loss: 2.2139034469922385

Epoch: 6| Step: 10
Training loss: 1.5282723903656006
Validation loss: 2.230184276898702

Epoch: 6| Step: 11
Training loss: 1.796528935432434
Validation loss: 2.2230477531751

Epoch: 6| Step: 12
Training loss: 1.760486125946045
Validation loss: 2.220892091592153

Epoch: 6| Step: 13
Training loss: 2.2206411361694336
Validation loss: 2.2326669096946716

Epoch: 290| Step: 0
Training loss: 1.2478079795837402
Validation loss: 2.2314563194910684

Epoch: 6| Step: 1
Training loss: 2.0251660346984863
Validation loss: 2.22231654326121

Epoch: 6| Step: 2
Training loss: 1.2745130062103271
Validation loss: 2.2409040530522666

Epoch: 6| Step: 3
Training loss: 1.6271674633026123
Validation loss: 2.2312736908594766

Epoch: 6| Step: 4
Training loss: 0.7942612171173096
Validation loss: 2.2157653172810874

Epoch: 6| Step: 5
Training loss: 1.3860045671463013
Validation loss: 2.2277983824412027

Epoch: 6| Step: 6
Training loss: 2.6124536991119385
Validation loss: 2.233339707056681

Epoch: 6| Step: 7
Training loss: 2.0609750747680664
Validation loss: 2.2777106960614524

Epoch: 6| Step: 8
Training loss: 1.6188623905181885
Validation loss: 2.2286967833836875

Epoch: 6| Step: 9
Training loss: 1.9752073287963867
Validation loss: 2.2519852916399636

Epoch: 6| Step: 10
Training loss: 1.7693965435028076
Validation loss: 2.244598150253296

Epoch: 6| Step: 11
Training loss: 1.9074008464813232
Validation loss: 2.2646928230921426

Epoch: 6| Step: 12
Training loss: 1.4840198755264282
Validation loss: 2.318149149417877

Epoch: 6| Step: 13
Training loss: 2.193744659423828
Validation loss: 2.264675517876943

Epoch: 291| Step: 0
Training loss: 1.6451315879821777
Validation loss: 2.2866176764170327

Epoch: 6| Step: 1
Training loss: 1.580999732017517
Validation loss: 2.2547062834103904

Epoch: 6| Step: 2
Training loss: 2.435751438140869
Validation loss: 2.232185204823812

Epoch: 6| Step: 3
Training loss: 1.8648837804794312
Validation loss: 2.2387130856513977

Epoch: 6| Step: 4
Training loss: 1.4048871994018555
Validation loss: 2.207854231198629

Epoch: 6| Step: 5
Training loss: 1.5112783908843994
Validation loss: 2.2117989460627236

Epoch: 6| Step: 6
Training loss: 2.1261396408081055
Validation loss: 2.1938771406809487

Epoch: 6| Step: 7
Training loss: 1.921497106552124
Validation loss: 2.2066452503204346

Epoch: 6| Step: 8
Training loss: 1.562509536743164
Validation loss: 2.2112551033496857

Epoch: 6| Step: 9
Training loss: 1.550049066543579
Validation loss: 2.2323097387949624

Epoch: 6| Step: 10
Training loss: 1.3973276615142822
Validation loss: 2.2440632581710815

Epoch: 6| Step: 11
Training loss: 1.3903284072875977
Validation loss: 2.2469447453816733

Epoch: 6| Step: 12
Training loss: 1.6924084424972534
Validation loss: 2.256290773550669

Epoch: 6| Step: 13
Training loss: 1.7944809198379517
Validation loss: 2.2907461722691855

Epoch: 292| Step: 0
Training loss: 1.7783764600753784
Validation loss: 2.2976620197296143

Epoch: 6| Step: 1
Training loss: 1.643829584121704
Validation loss: 2.3106901248296103

Epoch: 6| Step: 2
Training loss: 2.509077310562134
Validation loss: 2.32174559434255

Epoch: 6| Step: 3
Training loss: 2.3325135707855225
Validation loss: 2.2788129250208535

Epoch: 6| Step: 4
Training loss: 1.6057963371276855
Validation loss: 2.2773447831471763

Epoch: 6| Step: 5
Training loss: 1.9290308952331543
Validation loss: 2.265832702318827

Epoch: 6| Step: 6
Training loss: 2.1044974327087402
Validation loss: 2.2455384731292725

Epoch: 6| Step: 7
Training loss: 1.6664372682571411
Validation loss: 2.294982671737671

Epoch: 6| Step: 8
Training loss: 0.7435495257377625
Validation loss: 2.3043609658877053

Epoch: 6| Step: 9
Training loss: 1.860581398010254
Validation loss: 2.3015783627827964

Epoch: 6| Step: 10
Training loss: 1.4127558469772339
Validation loss: 2.2901641130447388

Epoch: 6| Step: 11
Training loss: 0.9190853834152222
Validation loss: 2.285694440205892

Epoch: 6| Step: 12
Training loss: 1.1452867984771729
Validation loss: 2.281907558441162

Epoch: 6| Step: 13
Training loss: 2.25046706199646
Validation loss: 2.229297379652659

Epoch: 293| Step: 0
Training loss: 2.021246910095215
Validation loss: 2.2345635890960693

Epoch: 6| Step: 1
Training loss: 2.1392436027526855
Validation loss: 2.238300840059916

Epoch: 6| Step: 2
Training loss: 1.260716438293457
Validation loss: 2.2195162177085876

Epoch: 6| Step: 3
Training loss: 1.7746081352233887
Validation loss: 2.2417254050572715

Epoch: 6| Step: 4
Training loss: 1.887682557106018
Validation loss: 2.2442440191904702

Epoch: 6| Step: 5
Training loss: 1.1546366214752197
Validation loss: 2.2761902809143066

Epoch: 6| Step: 6
Training loss: 1.6903533935546875
Validation loss: 2.2798197269439697

Epoch: 6| Step: 7
Training loss: 1.6460237503051758
Validation loss: 2.3003612955411277

Epoch: 6| Step: 8
Training loss: 2.03773832321167
Validation loss: 2.279638727506002

Epoch: 6| Step: 9
Training loss: 1.4951722621917725
Validation loss: 2.290566841761271

Epoch: 6| Step: 10
Training loss: 1.9107060432434082
Validation loss: 2.289965808391571

Epoch: 6| Step: 11
Training loss: 1.8630825281143188
Validation loss: 2.323918581008911

Epoch: 6| Step: 12
Training loss: 1.362217903137207
Validation loss: 2.318449079990387

Epoch: 6| Step: 13
Training loss: 1.8936835527420044
Validation loss: 2.2834513982137046

Epoch: 294| Step: 0
Training loss: 1.9023487567901611
Validation loss: 2.274339437484741

Epoch: 6| Step: 1
Training loss: 2.097975492477417
Validation loss: 2.2690161863962808

Epoch: 6| Step: 2
Training loss: 1.6892588138580322
Validation loss: 2.2582197984059653

Epoch: 6| Step: 3
Training loss: 1.835123896598816
Validation loss: 2.2730258107185364

Epoch: 6| Step: 4
Training loss: 2.3378939628601074
Validation loss: 2.2796747287114463

Epoch: 6| Step: 5
Training loss: 1.4884366989135742
Validation loss: 2.254287282625834

Epoch: 6| Step: 6
Training loss: 1.2863171100616455
Validation loss: 2.2686298489570618

Epoch: 6| Step: 7
Training loss: 1.613563060760498
Validation loss: 2.240282197793325

Epoch: 6| Step: 8
Training loss: 1.3645164966583252
Validation loss: 2.254333774248759

Epoch: 6| Step: 9
Training loss: 1.8141567707061768
Validation loss: 2.245646814505259

Epoch: 6| Step: 10
Training loss: 1.415919303894043
Validation loss: 2.254170556863149

Epoch: 6| Step: 11
Training loss: 1.2007039785385132
Validation loss: 2.273952007293701

Epoch: 6| Step: 12
Training loss: 1.8795850276947021
Validation loss: 2.244647741317749

Epoch: 6| Step: 13
Training loss: 1.781757116317749
Validation loss: 2.2439077893892923

Epoch: 295| Step: 0
Training loss: 1.4462589025497437
Validation loss: 2.282371242841085

Epoch: 6| Step: 1
Training loss: 1.4817432165145874
Validation loss: 2.2865410248438516

Epoch: 6| Step: 2
Training loss: 0.9876840114593506
Validation loss: 2.2882296244303384

Epoch: 6| Step: 3
Training loss: 1.9389660358428955
Validation loss: 2.290586769580841

Epoch: 6| Step: 4
Training loss: 2.0560712814331055
Validation loss: 2.3193947474161782

Epoch: 6| Step: 5
Training loss: 1.8417032957077026
Validation loss: 2.302690784136454

Epoch: 6| Step: 6
Training loss: 1.7736256122589111
Validation loss: 2.2924378315607705

Epoch: 6| Step: 7
Training loss: 1.5457298755645752
Validation loss: 2.2295374075571694

Epoch: 6| Step: 8
Training loss: 2.267124652862549
Validation loss: 2.2146693666776023

Epoch: 6| Step: 9
Training loss: 1.5609261989593506
Validation loss: 2.2003052035967507

Epoch: 6| Step: 10
Training loss: 1.887129783630371
Validation loss: 2.205808679262797

Epoch: 6| Step: 11
Training loss: 2.135909080505371
Validation loss: 2.210078318913778

Epoch: 6| Step: 12
Training loss: 2.076838493347168
Validation loss: 2.207775910695394

Epoch: 6| Step: 13
Training loss: 1.6820487976074219
Validation loss: 2.214772423108419

Epoch: 296| Step: 0
Training loss: 2.181246280670166
Validation loss: 2.2140249411265054

Epoch: 6| Step: 1
Training loss: 1.981353998184204
Validation loss: 2.2216827472050986

Epoch: 6| Step: 2
Training loss: 2.21170973777771
Validation loss: 2.2148294846216836

Epoch: 6| Step: 3
Training loss: 1.3706986904144287
Validation loss: 2.205891191959381

Epoch: 6| Step: 4
Training loss: 0.920594334602356
Validation loss: 2.2328128019968667

Epoch: 6| Step: 5
Training loss: 1.818751335144043
Validation loss: 2.235616127649943

Epoch: 6| Step: 6
Training loss: 1.6829311847686768
Validation loss: 2.245155135790507

Epoch: 6| Step: 7
Training loss: 1.8586783409118652
Validation loss: 2.267118453979492

Epoch: 6| Step: 8
Training loss: 1.2505853176116943
Validation loss: 2.2476514975229898

Epoch: 6| Step: 9
Training loss: 1.8514604568481445
Validation loss: 2.248634378115336

Epoch: 6| Step: 10
Training loss: 2.0610640048980713
Validation loss: 2.2144137620925903

Epoch: 6| Step: 11
Training loss: 1.628701090812683
Validation loss: 2.2615482608477273

Epoch: 6| Step: 12
Training loss: 1.257320523262024
Validation loss: 2.2341350317001343

Epoch: 6| Step: 13
Training loss: 1.4771525859832764
Validation loss: 2.2146148085594177

Epoch: 297| Step: 0
Training loss: 1.491302251815796
Validation loss: 2.2513770262400308

Epoch: 6| Step: 1
Training loss: 1.7778277397155762
Validation loss: 2.2412625749905906

Epoch: 6| Step: 2
Training loss: 1.9587966203689575
Validation loss: 2.2336938977241516

Epoch: 6| Step: 3
Training loss: 1.6744630336761475
Validation loss: 2.2189772526423135

Epoch: 6| Step: 4
Training loss: 1.3745437860488892
Validation loss: 2.222350796063741

Epoch: 6| Step: 5
Training loss: 1.2330741882324219
Validation loss: 2.233878791332245

Epoch: 6| Step: 6
Training loss: 1.8968532085418701
Validation loss: 2.2230594158172607

Epoch: 6| Step: 7
Training loss: 1.7271307706832886
Validation loss: 2.2194347580273948

Epoch: 6| Step: 8
Training loss: 1.3081982135772705
Validation loss: 2.203639348347982

Epoch: 6| Step: 9
Training loss: 1.8265676498413086
Validation loss: 2.1960290670394897

Epoch: 6| Step: 10
Training loss: 1.7301748991012573
Validation loss: 2.2113275130589805

Epoch: 6| Step: 11
Training loss: 2.168077230453491
Validation loss: 2.1974385380744934

Epoch: 6| Step: 12
Training loss: 1.8910905122756958
Validation loss: 2.2273802359898887

Epoch: 6| Step: 13
Training loss: 2.0867862701416016
Validation loss: 2.190536598364512

Epoch: 298| Step: 0
Training loss: 1.6309142112731934
Validation loss: 2.2347289323806763

Epoch: 6| Step: 1
Training loss: 1.8042032718658447
Validation loss: 2.2548272212346396

Epoch: 6| Step: 2
Training loss: 1.7965115308761597
Validation loss: 2.256243964036306

Epoch: 6| Step: 3
Training loss: 2.281111478805542
Validation loss: 2.265048066775004

Epoch: 6| Step: 4
Training loss: 2.139035701751709
Validation loss: 2.302960753440857

Epoch: 6| Step: 5
Training loss: 1.3670564889907837
Validation loss: 2.3531654675801597

Epoch: 6| Step: 6
Training loss: 1.5741701126098633
Validation loss: 2.304749290148417

Epoch: 6| Step: 7
Training loss: 2.229480028152466
Validation loss: 2.352064569791158

Epoch: 6| Step: 8
Training loss: 1.7758879661560059
Validation loss: 2.297303835550944

Epoch: 6| Step: 9
Training loss: 1.8317323923110962
Validation loss: 2.3059792121251426

Epoch: 6| Step: 10
Training loss: 1.6054434776306152
Validation loss: 2.2366782426834106

Epoch: 6| Step: 11
Training loss: 1.2991342544555664
Validation loss: 2.2394827802975974

Epoch: 6| Step: 12
Training loss: 1.7094275951385498
Validation loss: 2.2198163866996765

Epoch: 6| Step: 13
Training loss: 1.2923694849014282
Validation loss: 2.1984498500823975

Epoch: 299| Step: 0
Training loss: 1.3767930269241333
Validation loss: 2.233856519063314

Epoch: 6| Step: 1
Training loss: 1.126755952835083
Validation loss: 2.210404694080353

Epoch: 6| Step: 2
Training loss: 1.8372886180877686
Validation loss: 2.235051691532135

Epoch: 6| Step: 3
Training loss: 1.9735612869262695
Validation loss: 2.237002452214559

Epoch: 6| Step: 4
Training loss: 2.121210813522339
Validation loss: 2.2632098396619162

Epoch: 6| Step: 5
Training loss: 1.8045248985290527
Validation loss: 2.233862817287445

Epoch: 6| Step: 6
Training loss: 1.8658024072647095
Validation loss: 2.2784393628438315

Epoch: 6| Step: 7
Training loss: 2.028931140899658
Validation loss: 2.2910194198290506

Epoch: 6| Step: 8
Training loss: 1.4933040142059326
Validation loss: 2.285895526409149

Epoch: 6| Step: 9
Training loss: 2.401158094406128
Validation loss: 2.321964740753174

Epoch: 6| Step: 10
Training loss: 1.1949199438095093
Validation loss: 2.3465747038523355

Epoch: 6| Step: 11
Training loss: 1.8443833589553833
Validation loss: 2.3469610015551248

Epoch: 6| Step: 12
Training loss: 1.7515207529067993
Validation loss: 2.3223970929781594

Epoch: 6| Step: 13
Training loss: 1.2443915605545044
Validation loss: 2.2833580374717712

Epoch: 300| Step: 0
Training loss: 1.3906910419464111
Validation loss: 2.2455777128537497

Epoch: 6| Step: 1
Training loss: 1.7758504152297974
Validation loss: 2.238833010196686

Epoch: 6| Step: 2
Training loss: 1.5677905082702637
Validation loss: 2.2502140005429587

Epoch: 6| Step: 3
Training loss: 1.7386524677276611
Validation loss: 2.2351853450139365

Epoch: 6| Step: 4
Training loss: 1.1002676486968994
Validation loss: 2.254518687725067

Epoch: 6| Step: 5
Training loss: 2.7113890647888184
Validation loss: 2.2418895165125527

Epoch: 6| Step: 6
Training loss: 1.9192378520965576
Validation loss: 2.22765052318573

Epoch: 6| Step: 7
Training loss: 1.397165298461914
Validation loss: 2.2339269717534385

Epoch: 6| Step: 8
Training loss: 1.6090799570083618
Validation loss: 2.2616502046585083

Epoch: 6| Step: 9
Training loss: 2.751011371612549
Validation loss: 2.2543824315071106

Epoch: 6| Step: 10
Training loss: 1.7729485034942627
Validation loss: 2.2769851287206015

Epoch: 6| Step: 11
Training loss: 1.957633376121521
Validation loss: 2.2906471888224282

Epoch: 6| Step: 12
Training loss: 1.2258096933364868
Validation loss: 2.2816273967425027

Epoch: 6| Step: 13
Training loss: 1.3430012464523315
Validation loss: 2.300042430559794

Testing loss: 1.9904635192678988

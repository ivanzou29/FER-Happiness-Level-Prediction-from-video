Epoch: 1| Step: 0
Training loss: 4.460141181945801
Validation loss: 5.365412950515747

Epoch: 6| Step: 1
Training loss: 4.802305698394775
Validation loss: 5.363584915796916

Epoch: 6| Step: 2
Training loss: 6.312429428100586
Validation loss: 5.361812432607015

Epoch: 6| Step: 3
Training loss: 6.530414581298828
Validation loss: 5.360082228978475

Epoch: 6| Step: 4
Training loss: 4.422230243682861
Validation loss: 5.35836394627889

Epoch: 6| Step: 5
Training loss: 5.064146041870117
Validation loss: 5.35663636525472

Epoch: 6| Step: 6
Training loss: 5.011742115020752
Validation loss: 5.354854424794515

Epoch: 6| Step: 7
Training loss: 5.760097026824951
Validation loss: 5.353135188420613

Epoch: 6| Step: 8
Training loss: 6.601075172424316
Validation loss: 5.3512946764628095

Epoch: 6| Step: 9
Training loss: 5.851126670837402
Validation loss: 5.349467674891154

Epoch: 6| Step: 10
Training loss: 5.435993194580078
Validation loss: 5.347456693649292

Epoch: 6| Step: 11
Training loss: 4.733583927154541
Validation loss: 5.345416069030762

Epoch: 6| Step: 12
Training loss: 5.345126628875732
Validation loss: 5.343359470367432

Epoch: 6| Step: 13
Training loss: 5.585895538330078
Validation loss: 5.341197570164998

Epoch: 2| Step: 0
Training loss: 5.792458534240723
Validation loss: 5.338913599650065

Epoch: 6| Step: 1
Training loss: 5.700197219848633
Validation loss: 5.336554447809855

Epoch: 6| Step: 2
Training loss: 5.715965270996094
Validation loss: 5.3341028690338135

Epoch: 6| Step: 3
Training loss: 5.297025203704834
Validation loss: 5.331572691599528

Epoch: 6| Step: 4
Training loss: 5.832921028137207
Validation loss: 5.328906536102295

Epoch: 6| Step: 5
Training loss: 6.470961093902588
Validation loss: 5.326128323872884

Epoch: 6| Step: 6
Training loss: 5.391602516174316
Validation loss: 5.323109865188599

Epoch: 6| Step: 7
Training loss: 5.140437126159668
Validation loss: 5.32004181543986

Epoch: 6| Step: 8
Training loss: 5.4970011711120605
Validation loss: 5.316699822743733

Epoch: 6| Step: 9
Training loss: 4.857625484466553
Validation loss: 5.313326120376587

Epoch: 6| Step: 10
Training loss: 4.348324775695801
Validation loss: 5.309737205505371

Epoch: 6| Step: 11
Training loss: 5.479030609130859
Validation loss: 5.305927594502767

Epoch: 6| Step: 12
Training loss: 5.186531066894531
Validation loss: 5.301982482274373

Epoch: 6| Step: 13
Training loss: 4.739077568054199
Validation loss: 5.297605752944946

Epoch: 3| Step: 0
Training loss: 4.597657680511475
Validation loss: 5.2932554086049395

Epoch: 6| Step: 1
Training loss: 4.714608192443848
Validation loss: 5.288578033447266

Epoch: 6| Step: 2
Training loss: 5.27243709564209
Validation loss: 5.283587376276652

Epoch: 6| Step: 3
Training loss: 5.77828311920166
Validation loss: 5.278350114822388

Epoch: 6| Step: 4
Training loss: 5.361937522888184
Validation loss: 5.272909164428711

Epoch: 6| Step: 5
Training loss: 3.8902032375335693
Validation loss: 5.267046292622884

Epoch: 6| Step: 6
Training loss: 5.668079853057861
Validation loss: 5.260941982269287

Epoch: 6| Step: 7
Training loss: 6.444159984588623
Validation loss: 5.254500150680542

Epoch: 6| Step: 8
Training loss: 5.878422737121582
Validation loss: 5.247902154922485

Epoch: 6| Step: 9
Training loss: 5.799202919006348
Validation loss: 5.240965684254964

Epoch: 6| Step: 10
Training loss: 5.161396026611328
Validation loss: 5.2337846755981445

Epoch: 6| Step: 11
Training loss: 6.383131980895996
Validation loss: 5.226116339365642

Epoch: 6| Step: 12
Training loss: 4.456862449645996
Validation loss: 5.218336423238118

Epoch: 6| Step: 13
Training loss: 5.154847621917725
Validation loss: 5.210331360499064

Epoch: 4| Step: 0
Training loss: 6.287102222442627
Validation loss: 5.201876560846965

Epoch: 6| Step: 1
Training loss: 5.868231773376465
Validation loss: 5.193444013595581

Epoch: 6| Step: 2
Training loss: 3.678406238555908
Validation loss: 5.184218168258667

Epoch: 6| Step: 3
Training loss: 4.067802429199219
Validation loss: 5.17491602897644

Epoch: 6| Step: 4
Training loss: 5.8079071044921875
Validation loss: 5.165668249130249

Epoch: 6| Step: 5
Training loss: 5.4280104637146
Validation loss: 5.155920664469401

Epoch: 6| Step: 6
Training loss: 6.458553314208984
Validation loss: 5.146131674448649

Epoch: 6| Step: 7
Training loss: 5.1457133293151855
Validation loss: 5.1361831823984785

Epoch: 6| Step: 8
Training loss: 5.041256904602051
Validation loss: 5.125707228978475

Epoch: 6| Step: 9
Training loss: 5.82534646987915
Validation loss: 5.1154150168101

Epoch: 6| Step: 10
Training loss: 4.0077738761901855
Validation loss: 5.1046459674835205

Epoch: 6| Step: 11
Training loss: 5.284889221191406
Validation loss: 5.093170086542766

Epoch: 6| Step: 12
Training loss: 4.445127487182617
Validation loss: 5.082153717676799

Epoch: 6| Step: 13
Training loss: 5.62991189956665
Validation loss: 5.07090425491333

Epoch: 5| Step: 0
Training loss: 4.980027198791504
Validation loss: 5.059481302897136

Epoch: 6| Step: 1
Training loss: 5.218651294708252
Validation loss: 5.047801057497661

Epoch: 6| Step: 2
Training loss: 5.360940456390381
Validation loss: 5.03625734647115

Epoch: 6| Step: 3
Training loss: 4.628387451171875
Validation loss: 5.024096409479777

Epoch: 6| Step: 4
Training loss: 4.667228698730469
Validation loss: 5.012509107589722

Epoch: 6| Step: 5
Training loss: 3.655956983566284
Validation loss: 5.00118072827657

Epoch: 6| Step: 6
Training loss: 4.505359649658203
Validation loss: 4.989889621734619

Epoch: 6| Step: 7
Training loss: 5.410694122314453
Validation loss: 4.978682994842529

Epoch: 6| Step: 8
Training loss: 5.012753486633301
Validation loss: 4.967640399932861

Epoch: 6| Step: 9
Training loss: 6.222350120544434
Validation loss: 4.956915934880574

Epoch: 6| Step: 10
Training loss: 5.590152740478516
Validation loss: 4.94595464070638

Epoch: 6| Step: 11
Training loss: 4.987441062927246
Validation loss: 4.935752550760905

Epoch: 6| Step: 12
Training loss: 5.512674331665039
Validation loss: 4.926055987675984

Epoch: 6| Step: 13
Training loss: 5.1818013191223145
Validation loss: 4.916581789652507

Epoch: 6| Step: 0
Training loss: 4.9590349197387695
Validation loss: 4.9071149826049805

Epoch: 6| Step: 1
Training loss: 4.926019668579102
Validation loss: 4.897823492685954

Epoch: 6| Step: 2
Training loss: 4.47688627243042
Validation loss: 4.889212369918823

Epoch: 6| Step: 3
Training loss: 5.334017276763916
Validation loss: 4.880065361658732

Epoch: 6| Step: 4
Training loss: 4.626163482666016
Validation loss: 4.8718944390614825

Epoch: 6| Step: 5
Training loss: 5.39701509475708
Validation loss: 4.863373517990112

Epoch: 6| Step: 6
Training loss: 6.148964881896973
Validation loss: 4.855045874913533

Epoch: 6| Step: 7
Training loss: 4.454651355743408
Validation loss: 4.846509138743083

Epoch: 6| Step: 8
Training loss: 4.306955337524414
Validation loss: 4.838523546854655

Epoch: 6| Step: 9
Training loss: 5.208390235900879
Validation loss: 4.830216487248738

Epoch: 6| Step: 10
Training loss: 4.975732803344727
Validation loss: 4.822180072466533

Epoch: 6| Step: 11
Training loss: 5.219987392425537
Validation loss: 4.814424514770508

Epoch: 6| Step: 12
Training loss: 4.606867790222168
Validation loss: 4.807260036468506

Epoch: 6| Step: 13
Training loss: 4.506289958953857
Validation loss: 4.800026496251424

Epoch: 7| Step: 0
Training loss: 4.936590194702148
Validation loss: 4.793254971504211

Epoch: 6| Step: 1
Training loss: 5.336061477661133
Validation loss: 4.786624709765117

Epoch: 6| Step: 2
Training loss: 5.556455135345459
Validation loss: 4.779688199361165

Epoch: 6| Step: 3
Training loss: 5.795958995819092
Validation loss: 4.772981961568196

Epoch: 6| Step: 4
Training loss: 4.951371192932129
Validation loss: 4.766374508539836

Epoch: 6| Step: 5
Training loss: 4.997067451477051
Validation loss: 4.759386459986369

Epoch: 6| Step: 6
Training loss: 4.7669196128845215
Validation loss: 4.7525047063827515

Epoch: 6| Step: 7
Training loss: 4.349216461181641
Validation loss: 4.745639403661092

Epoch: 6| Step: 8
Training loss: 4.989192962646484
Validation loss: 4.738789240519206

Epoch: 6| Step: 9
Training loss: 4.727348327636719
Validation loss: 4.7325215339660645

Epoch: 6| Step: 10
Training loss: 3.5427374839782715
Validation loss: 4.725768645604451

Epoch: 6| Step: 11
Training loss: 4.741318225860596
Validation loss: 4.7199656168619795

Epoch: 6| Step: 12
Training loss: 4.27865743637085
Validation loss: 4.713499704996745

Epoch: 6| Step: 13
Training loss: 4.836287975311279
Validation loss: 4.707374095916748

Epoch: 8| Step: 0
Training loss: 4.814817905426025
Validation loss: 4.7009743849436445

Epoch: 6| Step: 1
Training loss: 4.921934127807617
Validation loss: 4.694737434387207

Epoch: 6| Step: 2
Training loss: 3.7278566360473633
Validation loss: 4.6884621779123945

Epoch: 6| Step: 3
Training loss: 4.015489101409912
Validation loss: 4.681589841842651

Epoch: 6| Step: 4
Training loss: 4.568160057067871
Validation loss: 4.675613800684611

Epoch: 6| Step: 5
Training loss: 5.291358947753906
Validation loss: 4.668764630953471

Epoch: 6| Step: 6
Training loss: 4.519798278808594
Validation loss: 4.6623334884643555

Epoch: 6| Step: 7
Training loss: 4.284818649291992
Validation loss: 4.655831654866536

Epoch: 6| Step: 8
Training loss: 4.906545639038086
Validation loss: 4.649143060048421

Epoch: 6| Step: 9
Training loss: 4.627270698547363
Validation loss: 4.642168839772542

Epoch: 6| Step: 10
Training loss: 5.715734481811523
Validation loss: 4.635270635286967

Epoch: 6| Step: 11
Training loss: 4.35952091217041
Validation loss: 4.628244558970134

Epoch: 6| Step: 12
Training loss: 5.690272808074951
Validation loss: 4.621134241422017

Epoch: 6| Step: 13
Training loss: 5.098809719085693
Validation loss: 4.614171862602234

Epoch: 9| Step: 0
Training loss: 5.926671504974365
Validation loss: 4.607180595397949

Epoch: 6| Step: 1
Training loss: 4.644985198974609
Validation loss: 4.6007927258809405

Epoch: 6| Step: 2
Training loss: 4.288758277893066
Validation loss: 4.594175457954407

Epoch: 6| Step: 3
Training loss: 4.753576278686523
Validation loss: 4.587278922398885

Epoch: 6| Step: 4
Training loss: 4.592239856719971
Validation loss: 4.580890655517578

Epoch: 6| Step: 5
Training loss: 3.892172336578369
Validation loss: 4.574345668156941

Epoch: 6| Step: 6
Training loss: 4.49229621887207
Validation loss: 4.567706187566121

Epoch: 6| Step: 7
Training loss: 4.395256996154785
Validation loss: 4.5619557698567705

Epoch: 6| Step: 8
Training loss: 4.91115665435791
Validation loss: 4.555260260899861

Epoch: 6| Step: 9
Training loss: 4.197749614715576
Validation loss: 4.548740069071452

Epoch: 6| Step: 10
Training loss: 4.213783264160156
Validation loss: 4.542511622111003

Epoch: 6| Step: 11
Training loss: 4.679065704345703
Validation loss: 4.536485592524211

Epoch: 6| Step: 12
Training loss: 4.480706691741943
Validation loss: 4.5306022961934405

Epoch: 6| Step: 13
Training loss: 5.867996692657471
Validation loss: 4.524892886479695

Epoch: 10| Step: 0
Training loss: 4.3936052322387695
Validation loss: 4.5187907218933105

Epoch: 6| Step: 1
Training loss: 4.857897758483887
Validation loss: 4.5129497448603315

Epoch: 6| Step: 2
Training loss: 4.411715507507324
Validation loss: 4.50669527053833

Epoch: 6| Step: 3
Training loss: 4.649599075317383
Validation loss: 4.501112659772237

Epoch: 6| Step: 4
Training loss: 5.173605918884277
Validation loss: 4.495425224304199

Epoch: 6| Step: 5
Training loss: 3.5581278800964355
Validation loss: 4.489298502604167

Epoch: 6| Step: 6
Training loss: 4.774911880493164
Validation loss: 4.483719031016032

Epoch: 6| Step: 7
Training loss: 4.784393310546875
Validation loss: 4.477652072906494

Epoch: 6| Step: 8
Training loss: 5.748726844787598
Validation loss: 4.4716906150182085

Epoch: 6| Step: 9
Training loss: 4.435001373291016
Validation loss: 4.465522567431132

Epoch: 6| Step: 10
Training loss: 3.835827589035034
Validation loss: 4.459489385286967

Epoch: 6| Step: 11
Training loss: 5.03374719619751
Validation loss: 4.45309058825175

Epoch: 6| Step: 12
Training loss: 3.9149010181427
Validation loss: 4.446403582890828

Epoch: 6| Step: 13
Training loss: 4.674065589904785
Validation loss: 4.440085689226787

Epoch: 11| Step: 0
Training loss: 4.212902545928955
Validation loss: 4.433505058288574

Epoch: 6| Step: 1
Training loss: 3.8414559364318848
Validation loss: 4.427103439966838

Epoch: 6| Step: 2
Training loss: 6.087282180786133
Validation loss: 4.420315265655518

Epoch: 6| Step: 3
Training loss: 4.6773576736450195
Validation loss: 4.413600921630859

Epoch: 6| Step: 4
Training loss: 4.274543285369873
Validation loss: 4.4067691167195635

Epoch: 6| Step: 5
Training loss: 4.888434410095215
Validation loss: 4.399243513743083

Epoch: 6| Step: 6
Training loss: 3.8542604446411133
Validation loss: 4.3915865023930865

Epoch: 6| Step: 7
Training loss: 3.8277690410614014
Validation loss: 4.383355299631755

Epoch: 6| Step: 8
Training loss: 5.093269348144531
Validation loss: 4.375696221987407

Epoch: 6| Step: 9
Training loss: 3.4168999195098877
Validation loss: 4.366451422373454

Epoch: 6| Step: 10
Training loss: 4.521145820617676
Validation loss: 4.358210444450378

Epoch: 6| Step: 11
Training loss: 3.894582986831665
Validation loss: 4.35002589225769

Epoch: 6| Step: 12
Training loss: 4.714522361755371
Validation loss: 4.34242304166158

Epoch: 6| Step: 13
Training loss: 5.746554374694824
Validation loss: 4.336149136225383

Epoch: 12| Step: 0
Training loss: 4.652464866638184
Validation loss: 4.328661203384399

Epoch: 6| Step: 1
Training loss: 4.422998905181885
Validation loss: 4.3216579755147295

Epoch: 6| Step: 2
Training loss: 4.676877975463867
Validation loss: 4.315003554026286

Epoch: 6| Step: 3
Training loss: 4.618379592895508
Validation loss: 4.307622035344441

Epoch: 6| Step: 4
Training loss: 3.1532623767852783
Validation loss: 4.300136645634969

Epoch: 6| Step: 5
Training loss: 5.267704963684082
Validation loss: 4.294065833091736

Epoch: 6| Step: 6
Training loss: 3.784252882003784
Validation loss: 4.288564999898274

Epoch: 6| Step: 7
Training loss: 4.484637260437012
Validation loss: 4.2815161148707075

Epoch: 6| Step: 8
Training loss: 4.001943588256836
Validation loss: 4.274695475896199

Epoch: 6| Step: 9
Training loss: 4.400660514831543
Validation loss: 4.267713030179341

Epoch: 6| Step: 10
Training loss: 4.894520282745361
Validation loss: 4.261236270268758

Epoch: 6| Step: 11
Training loss: 4.5404767990112305
Validation loss: 4.254486441612244

Epoch: 6| Step: 12
Training loss: 4.031091213226318
Validation loss: 4.247437119483948

Epoch: 6| Step: 13
Training loss: 4.807224273681641
Validation loss: 4.240923166275024

Epoch: 13| Step: 0
Training loss: 3.960578441619873
Validation loss: 4.234648863474528

Epoch: 6| Step: 1
Training loss: 4.156739234924316
Validation loss: 4.227888822555542

Epoch: 6| Step: 2
Training loss: 4.175356864929199
Validation loss: 4.2215070724487305

Epoch: 6| Step: 3
Training loss: 3.612194299697876
Validation loss: 4.214727282524109

Epoch: 6| Step: 4
Training loss: 5.227421760559082
Validation loss: 4.207770109176636

Epoch: 6| Step: 5
Training loss: 4.406549453735352
Validation loss: 4.2012704610824585

Epoch: 6| Step: 6
Training loss: 3.772061824798584
Validation loss: 4.193885723749797

Epoch: 6| Step: 7
Training loss: 5.176609039306641
Validation loss: 4.18757688999176

Epoch: 6| Step: 8
Training loss: 4.763487815856934
Validation loss: 4.180633823076884

Epoch: 6| Step: 9
Training loss: 3.8457510471343994
Validation loss: 4.172910253206889

Epoch: 6| Step: 10
Training loss: 3.9029111862182617
Validation loss: 4.166765133539836

Epoch: 6| Step: 11
Training loss: 4.602708339691162
Validation loss: 4.160506923993428

Epoch: 6| Step: 12
Training loss: 3.91705584526062
Validation loss: 4.154483000437419

Epoch: 6| Step: 13
Training loss: 5.018915176391602
Validation loss: 4.1483326355616255

Epoch: 14| Step: 0
Training loss: 4.294617652893066
Validation loss: 4.142400701840718

Epoch: 6| Step: 1
Training loss: 3.3168468475341797
Validation loss: 4.136458237965901

Epoch: 6| Step: 2
Training loss: 4.7882795333862305
Validation loss: 4.130626916885376

Epoch: 6| Step: 3
Training loss: 4.810358047485352
Validation loss: 4.124711672465007

Epoch: 6| Step: 4
Training loss: 4.162928581237793
Validation loss: 4.119680722554524

Epoch: 6| Step: 5
Training loss: 4.601847171783447
Validation loss: 4.113943179448445

Epoch: 6| Step: 6
Training loss: 3.844149112701416
Validation loss: 4.10865851243337

Epoch: 6| Step: 7
Training loss: 3.36252498626709
Validation loss: 4.102121313412984

Epoch: 6| Step: 8
Training loss: 3.8097190856933594
Validation loss: 4.0970762968063354

Epoch: 6| Step: 9
Training loss: 4.3864850997924805
Validation loss: 4.091683546702067

Epoch: 6| Step: 10
Training loss: 4.344273090362549
Validation loss: 4.086383144060771

Epoch: 6| Step: 11
Training loss: 4.603098392486572
Validation loss: 4.081177910168965

Epoch: 6| Step: 12
Training loss: 4.118424415588379
Validation loss: 4.07569420337677

Epoch: 6| Step: 13
Training loss: 5.005385398864746
Validation loss: 4.069897969563802

Epoch: 15| Step: 0
Training loss: 4.321257591247559
Validation loss: 4.06489376227061

Epoch: 6| Step: 1
Training loss: 3.9353525638580322
Validation loss: 4.059992631276448

Epoch: 6| Step: 2
Training loss: 4.38774299621582
Validation loss: 4.054476658503215

Epoch: 6| Step: 3
Training loss: 4.396975994110107
Validation loss: 4.04936424891154

Epoch: 6| Step: 4
Training loss: 3.2728710174560547
Validation loss: 4.044737537701924

Epoch: 6| Step: 5
Training loss: 3.393289089202881
Validation loss: 4.0399850606918335

Epoch: 6| Step: 6
Training loss: 4.6302170753479
Validation loss: 4.034463286399841

Epoch: 6| Step: 7
Training loss: 3.453305959701538
Validation loss: 4.02959942817688

Epoch: 6| Step: 8
Training loss: 4.529960632324219
Validation loss: 4.024796446164449

Epoch: 6| Step: 9
Training loss: 5.1753435134887695
Validation loss: 4.019052465756734

Epoch: 6| Step: 10
Training loss: 4.6644792556762695
Validation loss: 4.014229456583659

Epoch: 6| Step: 11
Training loss: 4.302648544311523
Validation loss: 4.008382638295491

Epoch: 6| Step: 12
Training loss: 4.4019036293029785
Validation loss: 4.004077196121216

Epoch: 6| Step: 13
Training loss: 3.599400043487549
Validation loss: 3.9990254640579224

Epoch: 16| Step: 0
Training loss: 3.3291969299316406
Validation loss: 3.994049549102783

Epoch: 6| Step: 1
Training loss: 4.5577592849731445
Validation loss: 3.9894564946492515

Epoch: 6| Step: 2
Training loss: 4.363966941833496
Validation loss: 3.984515110651652

Epoch: 6| Step: 3
Training loss: 3.9335155487060547
Validation loss: 3.9790608882904053

Epoch: 6| Step: 4
Training loss: 3.827298641204834
Validation loss: 3.9744435946146646

Epoch: 6| Step: 5
Training loss: 4.042168617248535
Validation loss: 3.970144748687744

Epoch: 6| Step: 6
Training loss: 3.5157346725463867
Validation loss: 3.9648590087890625

Epoch: 6| Step: 7
Training loss: 4.229091644287109
Validation loss: 3.9598358472188315

Epoch: 6| Step: 8
Training loss: 4.0582594871521
Validation loss: 3.9549315770467124

Epoch: 6| Step: 9
Training loss: 3.6450788974761963
Validation loss: 3.9496768712997437

Epoch: 6| Step: 10
Training loss: 4.402885437011719
Validation loss: 3.945584257443746

Epoch: 6| Step: 11
Training loss: 4.745028495788574
Validation loss: 3.9397690693537393

Epoch: 6| Step: 12
Training loss: 4.140913009643555
Validation loss: 3.9361886580785117

Epoch: 6| Step: 13
Training loss: 4.726126670837402
Validation loss: 3.930963317553202

Epoch: 17| Step: 0
Training loss: 3.0430984497070312
Validation loss: 3.9262595971425376

Epoch: 6| Step: 1
Training loss: 4.001791954040527
Validation loss: 3.921718637148539

Epoch: 6| Step: 2
Training loss: 4.216989517211914
Validation loss: 3.916682720184326

Epoch: 6| Step: 3
Training loss: 4.826164722442627
Validation loss: 3.911842147509257

Epoch: 6| Step: 4
Training loss: 3.4806394577026367
Validation loss: 3.9065003792444863

Epoch: 6| Step: 5
Training loss: 3.5639023780822754
Validation loss: 3.9019996325174966

Epoch: 6| Step: 6
Training loss: 4.101848125457764
Validation loss: 3.897830287615458

Epoch: 6| Step: 7
Training loss: 3.704651117324829
Validation loss: 3.8925543626149497

Epoch: 6| Step: 8
Training loss: 4.263123512268066
Validation loss: 3.888485590616862

Epoch: 6| Step: 9
Training loss: 4.73175573348999
Validation loss: 3.883328914642334

Epoch: 6| Step: 10
Training loss: 4.082000732421875
Validation loss: 3.878318508466085

Epoch: 6| Step: 11
Training loss: 4.590858459472656
Validation loss: 3.874232371648153

Epoch: 6| Step: 12
Training loss: 3.9347667694091797
Validation loss: 3.869873841603597

Epoch: 6| Step: 13
Training loss: 4.089153289794922
Validation loss: 3.8649996519088745

Epoch: 18| Step: 0
Training loss: 4.38957405090332
Validation loss: 3.860171834627787

Epoch: 6| Step: 1
Training loss: 3.3961117267608643
Validation loss: 3.855217218399048

Epoch: 6| Step: 2
Training loss: 3.57319712638855
Validation loss: 3.85042671362559

Epoch: 6| Step: 3
Training loss: 4.837783336639404
Validation loss: 3.845756928126017

Epoch: 6| Step: 4
Training loss: 4.694475173950195
Validation loss: 3.8411329984664917

Epoch: 6| Step: 5
Training loss: 5.369492530822754
Validation loss: 3.8360957304636636

Epoch: 6| Step: 6
Training loss: 2.671278238296509
Validation loss: 3.831860065460205

Epoch: 6| Step: 7
Training loss: 3.626157283782959
Validation loss: 3.827458620071411

Epoch: 6| Step: 8
Training loss: 4.002937316894531
Validation loss: 3.8236627181371055

Epoch: 6| Step: 9
Training loss: 4.011033058166504
Validation loss: 3.8186557292938232

Epoch: 6| Step: 10
Training loss: 4.179703235626221
Validation loss: 3.81403656800588

Epoch: 6| Step: 11
Training loss: 4.420491695404053
Validation loss: 3.8090582688649497

Epoch: 6| Step: 12
Training loss: 3.129347801208496
Validation loss: 3.8043198188145957

Epoch: 6| Step: 13
Training loss: 3.492058753967285
Validation loss: 3.8002771139144897

Epoch: 19| Step: 0
Training loss: 4.417952537536621
Validation loss: 3.7956928809483848

Epoch: 6| Step: 1
Training loss: 3.874380588531494
Validation loss: 3.7914660771687827

Epoch: 6| Step: 2
Training loss: 3.89914870262146
Validation loss: 3.7870615323384604

Epoch: 6| Step: 3
Training loss: 5.074191093444824
Validation loss: 3.7825961112976074

Epoch: 6| Step: 4
Training loss: 3.782719373703003
Validation loss: 3.778355081876119

Epoch: 6| Step: 5
Training loss: 4.191899299621582
Validation loss: 3.7746878465016684

Epoch: 6| Step: 6
Training loss: 3.693481683731079
Validation loss: 3.7698932886123657

Epoch: 6| Step: 7
Training loss: 3.828104019165039
Validation loss: 3.765326142311096

Epoch: 6| Step: 8
Training loss: 2.608976364135742
Validation loss: 3.761130770047506

Epoch: 6| Step: 9
Training loss: 4.376357078552246
Validation loss: 3.7565006017684937

Epoch: 6| Step: 10
Training loss: 4.2608723640441895
Validation loss: 3.7527145942052207

Epoch: 6| Step: 11
Training loss: 3.1941704750061035
Validation loss: 3.7488423585891724

Epoch: 6| Step: 12
Training loss: 3.490908145904541
Validation loss: 3.7443168560663858

Epoch: 6| Step: 13
Training loss: 4.276609420776367
Validation loss: 3.7402271032333374

Epoch: 20| Step: 0
Training loss: 3.593958854675293
Validation loss: 3.7367130517959595

Epoch: 6| Step: 1
Training loss: 4.638361930847168
Validation loss: 3.732090791066488

Epoch: 6| Step: 2
Training loss: 4.193559646606445
Validation loss: 3.7278186082839966

Epoch: 6| Step: 3
Training loss: 4.489440441131592
Validation loss: 3.7238358656565347

Epoch: 6| Step: 4
Training loss: 3.9767990112304688
Validation loss: 3.71991761525472

Epoch: 6| Step: 5
Training loss: 4.690596103668213
Validation loss: 3.715692122777303

Epoch: 6| Step: 6
Training loss: 4.167810440063477
Validation loss: 3.711846947669983

Epoch: 6| Step: 7
Training loss: 2.533717632293701
Validation loss: 3.7077431281407676

Epoch: 6| Step: 8
Training loss: 4.526124000549316
Validation loss: 3.7044067780176797

Epoch: 6| Step: 9
Training loss: 2.984872817993164
Validation loss: 3.700583895047506

Epoch: 6| Step: 10
Training loss: 3.2759532928466797
Validation loss: 3.6961613496144614

Epoch: 6| Step: 11
Training loss: 3.8334403038024902
Validation loss: 3.692152976989746

Epoch: 6| Step: 12
Training loss: 3.4090206623077393
Validation loss: 3.6882097721099854

Epoch: 6| Step: 13
Training loss: 3.880126714706421
Validation loss: 3.6847914854685464

Epoch: 21| Step: 0
Training loss: 4.562688827514648
Validation loss: 3.680567820866903

Epoch: 6| Step: 1
Training loss: 3.3812508583068848
Validation loss: 3.676380435625712

Epoch: 6| Step: 2
Training loss: 3.9279887676239014
Validation loss: 3.672299106915792

Epoch: 6| Step: 3
Training loss: 4.004733085632324
Validation loss: 3.6684627532958984

Epoch: 6| Step: 4
Training loss: 3.1662917137145996
Validation loss: 3.6643106937408447

Epoch: 6| Step: 5
Training loss: 3.6337475776672363
Validation loss: 3.660371502240499

Epoch: 6| Step: 6
Training loss: 4.54234504699707
Validation loss: 3.6563491821289062

Epoch: 6| Step: 7
Training loss: 3.5906126499176025
Validation loss: 3.652476708094279

Epoch: 6| Step: 8
Training loss: 3.4934263229370117
Validation loss: 3.648420969645182

Epoch: 6| Step: 9
Training loss: 3.791445255279541
Validation loss: 3.6445171435674033

Epoch: 6| Step: 10
Training loss: 3.6773767471313477
Validation loss: 3.6400526762008667

Epoch: 6| Step: 11
Training loss: 4.3466973304748535
Validation loss: 3.636367917060852

Epoch: 6| Step: 12
Training loss: 3.8782896995544434
Validation loss: 3.6318463881810508

Epoch: 6| Step: 13
Training loss: 3.438870906829834
Validation loss: 3.6276605129241943

Epoch: 22| Step: 0
Training loss: 3.616213798522949
Validation loss: 3.623615026473999

Epoch: 6| Step: 1
Training loss: 3.8062233924865723
Validation loss: 3.6200867096583047

Epoch: 6| Step: 2
Training loss: 4.2021989822387695
Validation loss: 3.6158450841903687

Epoch: 6| Step: 3
Training loss: 3.0541930198669434
Validation loss: 3.61227293809255

Epoch: 6| Step: 4
Training loss: 3.577010154724121
Validation loss: 3.607834815979004

Epoch: 6| Step: 5
Training loss: 3.9852092266082764
Validation loss: 3.6039594809214273

Epoch: 6| Step: 6
Training loss: 4.362942218780518
Validation loss: 3.599545876185099

Epoch: 6| Step: 7
Training loss: 4.360883712768555
Validation loss: 3.595591902732849

Epoch: 6| Step: 8
Training loss: 3.9079926013946533
Validation loss: 3.591138402620951

Epoch: 6| Step: 9
Training loss: 3.5539512634277344
Validation loss: 3.5871335665384927

Epoch: 6| Step: 10
Training loss: 3.611521005630493
Validation loss: 3.582726836204529

Epoch: 6| Step: 11
Training loss: 3.7519545555114746
Validation loss: 3.5782421827316284

Epoch: 6| Step: 12
Training loss: 2.9462227821350098
Validation loss: 3.5745479663213096

Epoch: 6| Step: 13
Training loss: 3.945847511291504
Validation loss: 3.5707144737243652

Epoch: 23| Step: 0
Training loss: 3.0382375717163086
Validation loss: 3.5666951338450112

Epoch: 6| Step: 1
Training loss: 4.054184913635254
Validation loss: 3.5638497273127236

Epoch: 6| Step: 2
Training loss: 3.456153154373169
Validation loss: 3.55968169371287

Epoch: 6| Step: 3
Training loss: 4.047494888305664
Validation loss: 3.555266261100769

Epoch: 6| Step: 4
Training loss: 4.096163749694824
Validation loss: 3.5516130129496255

Epoch: 6| Step: 5
Training loss: 4.339583396911621
Validation loss: 3.54750128587087

Epoch: 6| Step: 6
Training loss: 3.3443756103515625
Validation loss: 3.5434237718582153

Epoch: 6| Step: 7
Training loss: 4.06264066696167
Validation loss: 3.539201100667318

Epoch: 6| Step: 8
Training loss: 3.4662599563598633
Validation loss: 3.535223364830017

Epoch: 6| Step: 9
Training loss: 3.479881763458252
Validation loss: 3.531306505203247

Epoch: 6| Step: 10
Training loss: 4.179924011230469
Validation loss: 3.5272066593170166

Epoch: 6| Step: 11
Training loss: 3.536038875579834
Validation loss: 3.5226137240727744

Epoch: 6| Step: 12
Training loss: 3.8493523597717285
Validation loss: 3.518534302711487

Epoch: 6| Step: 13
Training loss: 2.9463274478912354
Validation loss: 3.5145790576934814

Epoch: 24| Step: 0
Training loss: 2.750150442123413
Validation loss: 3.510713815689087

Epoch: 6| Step: 1
Training loss: 4.197580337524414
Validation loss: 3.5061463514963784

Epoch: 6| Step: 2
Training loss: 4.093845367431641
Validation loss: 3.5027159055074057

Epoch: 6| Step: 3
Training loss: 3.224844455718994
Validation loss: 3.498420794804891

Epoch: 6| Step: 4
Training loss: 3.7592291831970215
Validation loss: 3.494296352068583

Epoch: 6| Step: 5
Training loss: 3.317861318588257
Validation loss: 3.490347941716512

Epoch: 6| Step: 6
Training loss: 4.264749050140381
Validation loss: 3.4857081969579062

Epoch: 6| Step: 7
Training loss: 3.8423829078674316
Validation loss: 3.4817291498184204

Epoch: 6| Step: 8
Training loss: 2.999816417694092
Validation loss: 3.477566401163737

Epoch: 6| Step: 9
Training loss: 3.2599236965179443
Validation loss: 3.473135987917582

Epoch: 6| Step: 10
Training loss: 3.5036568641662598
Validation loss: 3.468695322672526

Epoch: 6| Step: 11
Training loss: 4.39786434173584
Validation loss: 3.463310480117798

Epoch: 6| Step: 12
Training loss: 3.507966995239258
Validation loss: 3.458942413330078

Epoch: 6| Step: 13
Training loss: 3.985424518585205
Validation loss: 3.454233725865682

Epoch: 25| Step: 0
Training loss: 4.238595008850098
Validation loss: 3.449628392855326

Epoch: 6| Step: 1
Training loss: 3.670030355453491
Validation loss: 3.4461098512013755

Epoch: 6| Step: 2
Training loss: 3.857027530670166
Validation loss: 3.4411797523498535

Epoch: 6| Step: 3
Training loss: 4.1514892578125
Validation loss: 3.4369505246480307

Epoch: 6| Step: 4
Training loss: 4.717820644378662
Validation loss: 3.433348854382833

Epoch: 6| Step: 5
Training loss: 2.5744500160217285
Validation loss: 3.4287662903467813

Epoch: 6| Step: 6
Training loss: 4.3627214431762695
Validation loss: 3.4241273005803428

Epoch: 6| Step: 7
Training loss: 3.9213573932647705
Validation loss: 3.4207165241241455

Epoch: 6| Step: 8
Training loss: 3.3770456314086914
Validation loss: 3.416785995165507

Epoch: 6| Step: 9
Training loss: 3.1430673599243164
Validation loss: 3.411706884702047

Epoch: 6| Step: 10
Training loss: 2.595961093902588
Validation loss: 3.407900810241699

Epoch: 6| Step: 11
Training loss: 2.982699394226074
Validation loss: 3.4039157231648765

Epoch: 6| Step: 12
Training loss: 3.113095760345459
Validation loss: 3.400379260381063

Epoch: 6| Step: 13
Training loss: 3.580655097961426
Validation loss: 3.3963722387949624

Epoch: 26| Step: 0
Training loss: 3.926191806793213
Validation loss: 3.392845869064331

Epoch: 6| Step: 1
Training loss: 3.416649103164673
Validation loss: 3.3884201447168985

Epoch: 6| Step: 2
Training loss: 3.584108352661133
Validation loss: 3.3844451109568277

Epoch: 6| Step: 3
Training loss: 4.1334147453308105
Validation loss: 3.380487561225891

Epoch: 6| Step: 4
Training loss: 3.0510928630828857
Validation loss: 3.3761794169743857

Epoch: 6| Step: 5
Training loss: 3.400897264480591
Validation loss: 3.3726791938145957

Epoch: 6| Step: 6
Training loss: 3.155742645263672
Validation loss: 3.368443568547567

Epoch: 6| Step: 7
Training loss: 2.910824775695801
Validation loss: 3.3646146059036255

Epoch: 6| Step: 8
Training loss: 3.2131948471069336
Validation loss: 3.360606630643209

Epoch: 6| Step: 9
Training loss: 2.796003818511963
Validation loss: 3.356703519821167

Epoch: 6| Step: 10
Training loss: 4.434417724609375
Validation loss: 3.3525896867116294

Epoch: 6| Step: 11
Training loss: 3.561598062515259
Validation loss: 3.3490410645802817

Epoch: 6| Step: 12
Training loss: 4.6505327224731445
Validation loss: 3.3447032372156777

Epoch: 6| Step: 13
Training loss: 3.298553943634033
Validation loss: 3.3409027655919394

Epoch: 27| Step: 0
Training loss: 2.4466004371643066
Validation loss: 3.336919625600179

Epoch: 6| Step: 1
Training loss: 3.4164724349975586
Validation loss: 3.3334774573644004

Epoch: 6| Step: 2
Training loss: 3.0910518169403076
Validation loss: 3.3298004070917764

Epoch: 6| Step: 3
Training loss: 5.1272430419921875
Validation loss: 3.326019565264384

Epoch: 6| Step: 4
Training loss: 3.5876550674438477
Validation loss: 3.3221815824508667

Epoch: 6| Step: 5
Training loss: 3.8433291912078857
Validation loss: 3.3182995319366455

Epoch: 6| Step: 6
Training loss: 3.8776090145111084
Validation loss: 3.3142557938893638

Epoch: 6| Step: 7
Training loss: 3.7914886474609375
Validation loss: 3.3104459047317505

Epoch: 6| Step: 8
Training loss: 2.968986988067627
Validation loss: 3.306436816851298

Epoch: 6| Step: 9
Training loss: 3.2333762645721436
Validation loss: 3.302059292793274

Epoch: 6| Step: 10
Training loss: 3.484914779663086
Validation loss: 3.2988158464431763

Epoch: 6| Step: 11
Training loss: 3.1090011596679688
Validation loss: 3.2951140801111856

Epoch: 6| Step: 12
Training loss: 3.834536552429199
Validation loss: 3.2916018962860107

Epoch: 6| Step: 13
Training loss: 3.0200910568237305
Validation loss: 3.287404457728068

Epoch: 28| Step: 0
Training loss: 3.395559549331665
Validation loss: 3.2836734453837075

Epoch: 6| Step: 1
Training loss: 3.2906036376953125
Validation loss: 3.280700365702311

Epoch: 6| Step: 2
Training loss: 3.776841640472412
Validation loss: 3.2766391038894653

Epoch: 6| Step: 3
Training loss: 2.7673206329345703
Validation loss: 3.2732598384221396

Epoch: 6| Step: 4
Training loss: 3.1500797271728516
Validation loss: 3.269436836242676

Epoch: 6| Step: 5
Training loss: 3.7687301635742188
Validation loss: 3.2665192683537803

Epoch: 6| Step: 6
Training loss: 3.232349395751953
Validation loss: 3.263115485509237

Epoch: 6| Step: 7
Training loss: 3.141413450241089
Validation loss: 3.2594186464945474

Epoch: 6| Step: 8
Training loss: 2.6518166065216064
Validation loss: 3.2563119729359946

Epoch: 6| Step: 9
Training loss: 3.631621837615967
Validation loss: 3.2523873249689736

Epoch: 6| Step: 10
Training loss: 3.750609874725342
Validation loss: 3.2491491238276162

Epoch: 6| Step: 11
Training loss: 4.324494361877441
Validation loss: 3.2457446654637656

Epoch: 6| Step: 12
Training loss: 3.7836854457855225
Validation loss: 3.2426578203837075

Epoch: 6| Step: 13
Training loss: 3.4631588459014893
Validation loss: 3.2387073040008545

Epoch: 29| Step: 0
Training loss: 3.783316135406494
Validation loss: 3.2351494630177817

Epoch: 6| Step: 1
Training loss: 2.1322882175445557
Validation loss: 3.231025815010071

Epoch: 6| Step: 2
Training loss: 4.263216018676758
Validation loss: 3.22738250096639

Epoch: 6| Step: 3
Training loss: 2.6110408306121826
Validation loss: 3.2234671910603843

Epoch: 6| Step: 4
Training loss: 4.2040886878967285
Validation loss: 3.220597187678019

Epoch: 6| Step: 5
Training loss: 3.3777456283569336
Validation loss: 3.2166632811228433

Epoch: 6| Step: 6
Training loss: 4.222506523132324
Validation loss: 3.212760090827942

Epoch: 6| Step: 7
Training loss: 3.8219070434570312
Validation loss: 3.2096216678619385

Epoch: 6| Step: 8
Training loss: 2.9966788291931152
Validation loss: 3.205266992251078

Epoch: 6| Step: 9
Training loss: 3.082376480102539
Validation loss: 3.201590140660604

Epoch: 6| Step: 10
Training loss: 3.2035810947418213
Validation loss: 3.197483936945597

Epoch: 6| Step: 11
Training loss: 2.9383742809295654
Validation loss: 3.193451166152954

Epoch: 6| Step: 12
Training loss: 3.6853058338165283
Validation loss: 3.1902074019114175

Epoch: 6| Step: 13
Training loss: 3.157489776611328
Validation loss: 3.187176823616028

Epoch: 30| Step: 0
Training loss: 3.3086771965026855
Validation loss: 3.1827255487442017

Epoch: 6| Step: 1
Training loss: 4.332942008972168
Validation loss: 3.1791106462478638

Epoch: 6| Step: 2
Training loss: 3.5063774585723877
Validation loss: 3.175663789113363

Epoch: 6| Step: 3
Training loss: 3.794140100479126
Validation loss: 3.1716754039128623

Epoch: 6| Step: 4
Training loss: 3.66278076171875
Validation loss: 3.1680914958318076

Epoch: 6| Step: 5
Training loss: 3.406235933303833
Validation loss: 3.163788080215454

Epoch: 6| Step: 6
Training loss: 3.0858187675476074
Validation loss: 3.160333593686422

Epoch: 6| Step: 7
Training loss: 3.3533613681793213
Validation loss: 3.1569804747899375

Epoch: 6| Step: 8
Training loss: 3.058743953704834
Validation loss: 3.1535644133885703

Epoch: 6| Step: 9
Training loss: 3.3366317749023438
Validation loss: 3.1500575145085654

Epoch: 6| Step: 10
Training loss: 1.9079508781433105
Validation loss: 3.146678328514099

Epoch: 6| Step: 11
Training loss: 2.437333822250366
Validation loss: 3.1424108346303306

Epoch: 6| Step: 12
Training loss: 4.12512731552124
Validation loss: 3.1393269300460815

Epoch: 6| Step: 13
Training loss: 3.5152711868286133
Validation loss: 3.134908437728882

Epoch: 31| Step: 0
Training loss: 3.4284520149230957
Validation loss: 3.1325122515360513

Epoch: 6| Step: 1
Training loss: 3.6568942070007324
Validation loss: 3.1286094188690186

Epoch: 6| Step: 2
Training loss: 2.678325891494751
Validation loss: 3.124885161717733

Epoch: 6| Step: 3
Training loss: 4.098056793212891
Validation loss: 3.121149778366089

Epoch: 6| Step: 4
Training loss: 3.6368534564971924
Validation loss: 3.1174383958180747

Epoch: 6| Step: 5
Training loss: 3.0837693214416504
Validation loss: 3.1138861576716104

Epoch: 6| Step: 6
Training loss: 2.073484420776367
Validation loss: 3.10971728960673

Epoch: 6| Step: 7
Training loss: 4.080035209655762
Validation loss: 3.106202562650045

Epoch: 6| Step: 8
Training loss: 3.5688610076904297
Validation loss: 3.102670113245646

Epoch: 6| Step: 9
Training loss: 3.497519016265869
Validation loss: 3.100046078364054

Epoch: 6| Step: 10
Training loss: 2.6544759273529053
Validation loss: 3.097494204839071

Epoch: 6| Step: 11
Training loss: 3.0710301399230957
Validation loss: 3.0943212111790976

Epoch: 6| Step: 12
Training loss: 3.3705224990844727
Validation loss: 3.090427120526632

Epoch: 6| Step: 13
Training loss: 3.2688651084899902
Validation loss: 3.0871667861938477

Epoch: 32| Step: 0
Training loss: 3.963646650314331
Validation loss: 3.083996375401815

Epoch: 6| Step: 1
Training loss: 2.7802724838256836
Validation loss: 3.08041520913442

Epoch: 6| Step: 2
Training loss: 3.5426113605499268
Validation loss: 3.0770078897476196

Epoch: 6| Step: 3
Training loss: 3.450014591217041
Validation loss: 3.073340972264608

Epoch: 6| Step: 4
Training loss: 3.2700252532958984
Validation loss: 3.0696950356165567

Epoch: 6| Step: 5
Training loss: 2.7483296394348145
Validation loss: 3.0670376221338906

Epoch: 6| Step: 6
Training loss: 2.950176239013672
Validation loss: 3.063645044962565

Epoch: 6| Step: 7
Training loss: 3.8717846870422363
Validation loss: 3.0599390268325806

Epoch: 6| Step: 8
Training loss: 2.702547788619995
Validation loss: 3.0579267740249634

Epoch: 6| Step: 9
Training loss: 2.940778970718384
Validation loss: 3.054902672767639

Epoch: 6| Step: 10
Training loss: 3.097598075866699
Validation loss: 3.0528144041697183

Epoch: 6| Step: 11
Training loss: 4.008218765258789
Validation loss: 3.04925407965978

Epoch: 6| Step: 12
Training loss: 3.2788710594177246
Validation loss: 3.0456724564234414

Epoch: 6| Step: 13
Training loss: 2.944455623626709
Validation loss: 3.0418253739674888

Epoch: 33| Step: 0
Training loss: 3.5407662391662598
Validation loss: 3.0380919377009072

Epoch: 6| Step: 1
Training loss: 3.935739040374756
Validation loss: 3.03497576713562

Epoch: 6| Step: 2
Training loss: 3.774127721786499
Validation loss: 3.032475789388021

Epoch: 6| Step: 3
Training loss: 2.7353134155273438
Validation loss: 3.0291301806767783

Epoch: 6| Step: 4
Training loss: 3.5419139862060547
Validation loss: 3.0261483987172446

Epoch: 6| Step: 5
Training loss: 3.115708351135254
Validation loss: 3.0221694707870483

Epoch: 6| Step: 6
Training loss: 3.925515651702881
Validation loss: 3.0185364484786987

Epoch: 6| Step: 7
Training loss: 3.4990074634552
Validation loss: 3.0149490435918174

Epoch: 6| Step: 8
Training loss: 2.6911354064941406
Validation loss: 3.011106848716736

Epoch: 6| Step: 9
Training loss: 3.0451669692993164
Validation loss: 3.0075891415278115

Epoch: 6| Step: 10
Training loss: 2.874002695083618
Validation loss: 3.0039586623509726

Epoch: 6| Step: 11
Training loss: 2.9042158126831055
Validation loss: 3.00127112865448

Epoch: 6| Step: 12
Training loss: 2.902310609817505
Validation loss: 2.9979485273361206

Epoch: 6| Step: 13
Training loss: 2.483006000518799
Validation loss: 2.9958001375198364

Epoch: 34| Step: 0
Training loss: 3.724024534225464
Validation loss: 2.994114557902018

Epoch: 6| Step: 1
Training loss: 2.7167370319366455
Validation loss: 2.9916248321533203

Epoch: 6| Step: 2
Training loss: 3.3328657150268555
Validation loss: 2.9912471771240234

Epoch: 6| Step: 3
Training loss: 3.245849132537842
Validation loss: 2.991766651471456

Epoch: 6| Step: 4
Training loss: 3.423192024230957
Validation loss: 2.985486686229706

Epoch: 6| Step: 5
Training loss: 2.363530397415161
Validation loss: 2.9790603717168174

Epoch: 6| Step: 6
Training loss: 2.330130100250244
Validation loss: 2.976068059603373

Epoch: 6| Step: 7
Training loss: 3.394163131713867
Validation loss: 2.9734839598337808

Epoch: 6| Step: 8
Training loss: 3.5035595893859863
Validation loss: 2.9700262546539307

Epoch: 6| Step: 9
Training loss: 3.1495368480682373
Validation loss: 2.966689149538676

Epoch: 6| Step: 10
Training loss: 2.912949562072754
Validation loss: 2.9622796376546225

Epoch: 6| Step: 11
Training loss: 3.881730556488037
Validation loss: 2.960029641787211

Epoch: 6| Step: 12
Training loss: 3.3363771438598633
Validation loss: 2.9571937322616577

Epoch: 6| Step: 13
Training loss: 3.027114152908325
Validation loss: 2.954058289527893

Epoch: 35| Step: 0
Training loss: 3.7763876914978027
Validation loss: 2.9494444926579795

Epoch: 6| Step: 1
Training loss: 2.1962056159973145
Validation loss: 2.946623921394348

Epoch: 6| Step: 2
Training loss: 2.716567039489746
Validation loss: 2.943319876988729

Epoch: 6| Step: 3
Training loss: 3.1321048736572266
Validation loss: 2.941284934679667

Epoch: 6| Step: 4
Training loss: 3.155627727508545
Validation loss: 2.937902808189392

Epoch: 6| Step: 5
Training loss: 3.0860683917999268
Validation loss: 2.9345827102661133

Epoch: 6| Step: 6
Training loss: 3.4701614379882812
Validation loss: 2.9312761227289834

Epoch: 6| Step: 7
Training loss: 3.292752742767334
Validation loss: 2.9297000567118325

Epoch: 6| Step: 8
Training loss: 3.0763959884643555
Validation loss: 2.92706835269928

Epoch: 6| Step: 9
Training loss: 3.0867061614990234
Validation loss: 2.9285709460576377

Epoch: 6| Step: 10
Training loss: 2.833728313446045
Validation loss: 2.9226478338241577

Epoch: 6| Step: 11
Training loss: 3.554126501083374
Validation loss: 2.9194176197052

Epoch: 6| Step: 12
Training loss: 3.6394617557525635
Validation loss: 2.919084986050924

Epoch: 6| Step: 13
Training loss: 2.823093891143799
Validation loss: 2.914517323176066

Epoch: 36| Step: 0
Training loss: 4.176667213439941
Validation loss: 2.9086970488230386

Epoch: 6| Step: 1
Training loss: 2.882139205932617
Validation loss: 2.905662735303243

Epoch: 6| Step: 2
Training loss: 2.6731390953063965
Validation loss: 2.901310125986735

Epoch: 6| Step: 3
Training loss: 2.8394827842712402
Validation loss: 2.8976169427235923

Epoch: 6| Step: 4
Training loss: 3.1193182468414307
Validation loss: 2.896362026532491

Epoch: 6| Step: 5
Training loss: 2.8217811584472656
Validation loss: 2.8922976652781167

Epoch: 6| Step: 6
Training loss: 2.4201745986938477
Validation loss: 2.8911560773849487

Epoch: 6| Step: 7
Training loss: 2.2032079696655273
Validation loss: 2.8881258964538574

Epoch: 6| Step: 8
Training loss: 3.6298153400421143
Validation loss: 2.883503874142965

Epoch: 6| Step: 9
Training loss: 3.056684970855713
Validation loss: 2.879952390988668

Epoch: 6| Step: 10
Training loss: 2.933542490005493
Validation loss: 2.8754838705062866

Epoch: 6| Step: 11
Training loss: 3.7535738945007324
Validation loss: 2.873878995577494

Epoch: 6| Step: 12
Training loss: 3.3048670291900635
Validation loss: 2.8710238536198935

Epoch: 6| Step: 13
Training loss: 3.444790840148926
Validation loss: 2.867492059866587

Epoch: 37| Step: 0
Training loss: 3.1702306270599365
Validation loss: 2.8655624787012735

Epoch: 6| Step: 1
Training loss: 2.748117208480835
Validation loss: 2.861660122871399

Epoch: 6| Step: 2
Training loss: 2.572474718093872
Validation loss: 2.858243783315023

Epoch: 6| Step: 3
Training loss: 3.4898133277893066
Validation loss: 2.8558120727539062

Epoch: 6| Step: 4
Training loss: 2.9665894508361816
Validation loss: 2.8509912888209024

Epoch: 6| Step: 5
Training loss: 3.1324095726013184
Validation loss: 2.8467232386271157

Epoch: 6| Step: 6
Training loss: 3.923461675643921
Validation loss: 2.845369021097819

Epoch: 6| Step: 7
Training loss: 2.2711071968078613
Validation loss: 2.8397987286249795

Epoch: 6| Step: 8
Training loss: 3.1476311683654785
Validation loss: 2.835644245147705

Epoch: 6| Step: 9
Training loss: 2.758974075317383
Validation loss: 2.8336516618728638

Epoch: 6| Step: 10
Training loss: 2.7141945362091064
Validation loss: 2.8313488165537515

Epoch: 6| Step: 11
Training loss: 3.900310516357422
Validation loss: 2.8277165492375693

Epoch: 6| Step: 12
Training loss: 3.0347707271575928
Validation loss: 2.823313554128011

Epoch: 6| Step: 13
Training loss: 2.883136034011841
Validation loss: 2.8212952613830566

Epoch: 38| Step: 0
Training loss: 3.0567684173583984
Validation loss: 2.817404270172119

Epoch: 6| Step: 1
Training loss: 3.05729603767395
Validation loss: 2.8156416018803916

Epoch: 6| Step: 2
Training loss: 1.8853323459625244
Validation loss: 2.812684178352356

Epoch: 6| Step: 3
Training loss: 3.345571994781494
Validation loss: 2.8106927474339805

Epoch: 6| Step: 4
Training loss: 3.40079402923584
Validation loss: 2.8102278312047324

Epoch: 6| Step: 5
Training loss: 3.3855676651000977
Validation loss: 2.8108713030815125

Epoch: 6| Step: 6
Training loss: 2.8137388229370117
Validation loss: 2.813165068626404

Epoch: 6| Step: 7
Training loss: 2.5499813556671143
Validation loss: 2.814834713935852

Epoch: 6| Step: 8
Training loss: 2.4566292762756348
Validation loss: 2.8167871236801147

Epoch: 6| Step: 9
Training loss: 3.2357265949249268
Validation loss: 2.8140097856521606

Epoch: 6| Step: 10
Training loss: 3.1081900596618652
Validation loss: 2.7993401885032654

Epoch: 6| Step: 11
Training loss: 2.839602470397949
Validation loss: 2.789277990659078

Epoch: 6| Step: 12
Training loss: 3.5166707038879395
Validation loss: 2.787441293398539

Epoch: 6| Step: 13
Training loss: 3.4453845024108887
Validation loss: 2.7852917114893594

Epoch: 39| Step: 0
Training loss: 2.4618167877197266
Validation loss: 2.782387852668762

Epoch: 6| Step: 1
Training loss: 3.0196151733398438
Validation loss: 2.7832064628601074

Epoch: 6| Step: 2
Training loss: 3.6911582946777344
Validation loss: 2.783232092857361

Epoch: 6| Step: 3
Training loss: 3.2082085609436035
Validation loss: 2.77777898311615

Epoch: 6| Step: 4
Training loss: 2.409254550933838
Validation loss: 2.7740389108657837

Epoch: 6| Step: 5
Training loss: 2.3237814903259277
Validation loss: 2.768533190091451

Epoch: 6| Step: 6
Training loss: 3.5043070316314697
Validation loss: 2.7653085390726724

Epoch: 6| Step: 7
Training loss: 3.176395893096924
Validation loss: 2.7620627085367837

Epoch: 6| Step: 8
Training loss: 2.599722146987915
Validation loss: 2.7580970327059426

Epoch: 6| Step: 9
Training loss: 2.8046441078186035
Validation loss: 2.760167638460795

Epoch: 6| Step: 10
Training loss: 2.555784225463867
Validation loss: 2.7568377256393433

Epoch: 6| Step: 11
Training loss: 3.3626675605773926
Validation loss: 2.755675514539083

Epoch: 6| Step: 12
Training loss: 3.6319515705108643
Validation loss: 2.7475185791651406

Epoch: 6| Step: 13
Training loss: 2.894500732421875
Validation loss: 2.7431547244389853

Epoch: 40| Step: 0
Training loss: 3.5540056228637695
Validation loss: 2.7410070101420083

Epoch: 6| Step: 1
Training loss: 2.414332628250122
Validation loss: 2.738306244214376

Epoch: 6| Step: 2
Training loss: 2.4392998218536377
Validation loss: 2.7347785234451294

Epoch: 6| Step: 3
Training loss: 2.394674301147461
Validation loss: 2.7334077755610147

Epoch: 6| Step: 4
Training loss: 3.3079023361206055
Validation loss: 2.729638457298279

Epoch: 6| Step: 5
Training loss: 2.3322277069091797
Validation loss: 2.7315529187520347

Epoch: 6| Step: 6
Training loss: 3.3397676944732666
Validation loss: 2.732087731361389

Epoch: 6| Step: 7
Training loss: 2.528736114501953
Validation loss: 2.728350281715393

Epoch: 6| Step: 8
Training loss: 2.731858253479004
Validation loss: 2.721827189127604

Epoch: 6| Step: 9
Training loss: 3.198869228363037
Validation loss: 2.7183738549550376

Epoch: 6| Step: 10
Training loss: 3.3356122970581055
Validation loss: 2.7141336599985757

Epoch: 6| Step: 11
Training loss: 2.4478769302368164
Validation loss: 2.7103180487950644

Epoch: 6| Step: 12
Training loss: 3.4276657104492188
Validation loss: 2.709565758705139

Epoch: 6| Step: 13
Training loss: 3.5688252449035645
Validation loss: 2.720220446586609

Epoch: 41| Step: 0
Training loss: 2.7231457233428955
Validation loss: 2.7028539975484214

Epoch: 6| Step: 1
Training loss: 3.208207607269287
Validation loss: 2.6976042985916138

Epoch: 6| Step: 2
Training loss: 2.573495864868164
Validation loss: 2.6932870944341025

Epoch: 6| Step: 3
Training loss: 3.1147654056549072
Validation loss: 2.68974232673645

Epoch: 6| Step: 4
Training loss: 2.188999652862549
Validation loss: 2.6847389539082847

Epoch: 6| Step: 5
Training loss: 2.5612308979034424
Validation loss: 2.6858556270599365

Epoch: 6| Step: 6
Training loss: 2.9269943237304688
Validation loss: 2.682222922643026

Epoch: 6| Step: 7
Training loss: 3.5972533226013184
Validation loss: 2.6827587286631265

Epoch: 6| Step: 8
Training loss: 2.5272133350372314
Validation loss: 2.6773967345555625

Epoch: 6| Step: 9
Training loss: 2.8184115886688232
Validation loss: 2.675219019254049

Epoch: 6| Step: 10
Training loss: 3.185072183609009
Validation loss: 2.670694907506307

Epoch: 6| Step: 11
Training loss: 3.7210562229156494
Validation loss: 2.6644519170125327

Epoch: 6| Step: 12
Training loss: 2.435271739959717
Validation loss: 2.6633960405985513

Epoch: 6| Step: 13
Training loss: 2.8551385402679443
Validation loss: 2.663713018099467

Epoch: 42| Step: 0
Training loss: 2.6712229251861572
Validation loss: 2.6664381424585977

Epoch: 6| Step: 1
Training loss: 2.483450412750244
Validation loss: 2.6666558980941772

Epoch: 6| Step: 2
Training loss: 3.299171209335327
Validation loss: 2.669876297314962

Epoch: 6| Step: 3
Training loss: 3.2823383808135986
Validation loss: 2.6675702730814614

Epoch: 6| Step: 4
Training loss: 2.293158531188965
Validation loss: 2.6616867582003274

Epoch: 6| Step: 5
Training loss: 2.9280271530151367
Validation loss: 2.651202837626139

Epoch: 6| Step: 6
Training loss: 3.313413619995117
Validation loss: 2.6454800367355347

Epoch: 6| Step: 7
Training loss: 3.300046682357788
Validation loss: 2.639644145965576

Epoch: 6| Step: 8
Training loss: 1.906038761138916
Validation loss: 2.636754810810089

Epoch: 6| Step: 9
Training loss: 2.618823528289795
Validation loss: 2.6356751124064126

Epoch: 6| Step: 10
Training loss: 2.6369051933288574
Validation loss: 2.639656742413839

Epoch: 6| Step: 11
Training loss: 2.881622791290283
Validation loss: 2.636695384979248

Epoch: 6| Step: 12
Training loss: 2.9493939876556396
Validation loss: 2.6256332397460938

Epoch: 6| Step: 13
Training loss: 3.239640712738037
Validation loss: 2.622857133547465

Epoch: 43| Step: 0
Training loss: 3.046854019165039
Validation loss: 2.618394653002421

Epoch: 6| Step: 1
Training loss: 2.8282809257507324
Validation loss: 2.6127813259760537

Epoch: 6| Step: 2
Training loss: 3.5670361518859863
Validation loss: 2.610587557156881

Epoch: 6| Step: 3
Training loss: 2.659628391265869
Validation loss: 2.6088719765345254

Epoch: 6| Step: 4
Training loss: 3.215820550918579
Validation loss: 2.606385429700216

Epoch: 6| Step: 5
Training loss: 2.7052457332611084
Validation loss: 2.6009323596954346

Epoch: 6| Step: 6
Training loss: 2.7661213874816895
Validation loss: 2.600231925646464

Epoch: 6| Step: 7
Training loss: 2.7537331581115723
Validation loss: 2.5968858202298484

Epoch: 6| Step: 8
Training loss: 2.855325222015381
Validation loss: 2.5938808917999268

Epoch: 6| Step: 9
Training loss: 3.305647134780884
Validation loss: 2.5922598441441855

Epoch: 6| Step: 10
Training loss: 3.3519973754882812
Validation loss: 2.5888456106185913

Epoch: 6| Step: 11
Training loss: 2.359307289123535
Validation loss: 2.5865891774495444

Epoch: 6| Step: 12
Training loss: 1.8959791660308838
Validation loss: 2.580141226450602

Epoch: 6| Step: 13
Training loss: 1.8715465068817139
Validation loss: 2.5779475768407187

Epoch: 44| Step: 0
Training loss: 3.64835524559021
Validation loss: 2.574736177921295

Epoch: 6| Step: 1
Training loss: 3.059176445007324
Validation loss: 2.569476763407389

Epoch: 6| Step: 2
Training loss: 3.1072683334350586
Validation loss: 2.5669429302215576

Epoch: 6| Step: 3
Training loss: 2.7660489082336426
Validation loss: 2.5660336017608643

Epoch: 6| Step: 4
Training loss: 1.828418254852295
Validation loss: 2.5623780290285745

Epoch: 6| Step: 5
Training loss: 2.9557063579559326
Validation loss: 2.5612246990203857

Epoch: 6| Step: 6
Training loss: 2.3097827434539795
Validation loss: 2.5588043133417764

Epoch: 6| Step: 7
Training loss: 2.181544780731201
Validation loss: 2.555970867474874

Epoch: 6| Step: 8
Training loss: 2.544325351715088
Validation loss: 2.5522292455037436

Epoch: 6| Step: 9
Training loss: 2.765603542327881
Validation loss: 2.549720843633016

Epoch: 6| Step: 10
Training loss: 3.2553834915161133
Validation loss: 2.546945810317993

Epoch: 6| Step: 11
Training loss: 2.6739611625671387
Validation loss: 2.5440608263015747

Epoch: 6| Step: 12
Training loss: 2.5194122791290283
Validation loss: 2.5436344146728516

Epoch: 6| Step: 13
Training loss: 2.973360538482666
Validation loss: 2.540155013402303

Epoch: 45| Step: 0
Training loss: 2.045588731765747
Validation loss: 2.5371200243631997

Epoch: 6| Step: 1
Training loss: 3.1839921474456787
Validation loss: 2.5328263441721597

Epoch: 6| Step: 2
Training loss: 3.4374561309814453
Validation loss: 2.529465834299723

Epoch: 6| Step: 3
Training loss: 2.9763293266296387
Validation loss: 2.5263872941335044

Epoch: 6| Step: 4
Training loss: 2.463432788848877
Validation loss: 2.520377238591512

Epoch: 6| Step: 5
Training loss: 2.6516380310058594
Validation loss: 2.5169051686922708

Epoch: 6| Step: 6
Training loss: 3.0585789680480957
Validation loss: 2.5148726304372153

Epoch: 6| Step: 7
Training loss: 2.3231029510498047
Validation loss: 2.5134609937667847

Epoch: 6| Step: 8
Training loss: 2.4706103801727295
Validation loss: 2.512863516807556

Epoch: 6| Step: 9
Training loss: 2.646867275238037
Validation loss: 2.5098554293314614

Epoch: 6| Step: 10
Training loss: 2.7219886779785156
Validation loss: 2.5069995323816934

Epoch: 6| Step: 11
Training loss: 2.6414196491241455
Validation loss: 2.5045084158579507

Epoch: 6| Step: 12
Training loss: 2.4957969188690186
Validation loss: 2.504713296890259

Epoch: 6| Step: 13
Training loss: 2.8126838207244873
Validation loss: 2.5089250802993774

Epoch: 46| Step: 0
Training loss: 2.6665730476379395
Validation loss: 2.5038465658823648

Epoch: 6| Step: 1
Training loss: 3.0116629600524902
Validation loss: 2.5061321457227073

Epoch: 6| Step: 2
Training loss: 2.681776523590088
Validation loss: 2.500819285710653

Epoch: 6| Step: 3
Training loss: 1.9263345003128052
Validation loss: 2.491521954536438

Epoch: 6| Step: 4
Training loss: 2.7379343509674072
Validation loss: 2.491136829058329

Epoch: 6| Step: 5
Training loss: 2.9736475944519043
Validation loss: 2.4869993329048157

Epoch: 6| Step: 6
Training loss: 2.5573835372924805
Validation loss: 2.4827895164489746

Epoch: 6| Step: 7
Training loss: 3.2189321517944336
Validation loss: 2.4786285559336343

Epoch: 6| Step: 8
Training loss: 2.295217990875244
Validation loss: 2.479716420173645

Epoch: 6| Step: 9
Training loss: 2.673898696899414
Validation loss: 2.476614316304525

Epoch: 6| Step: 10
Training loss: 3.0807676315307617
Validation loss: 2.4774343172709146

Epoch: 6| Step: 11
Training loss: 2.2265477180480957
Validation loss: 2.477650463581085

Epoch: 6| Step: 12
Training loss: 2.658182382583618
Validation loss: 2.4733386437098184

Epoch: 6| Step: 13
Training loss: 2.626530647277832
Validation loss: 2.465986132621765

Epoch: 47| Step: 0
Training loss: 2.22485613822937
Validation loss: 2.459462801615397

Epoch: 6| Step: 1
Training loss: 2.812021255493164
Validation loss: 2.4534629583358765

Epoch: 6| Step: 2
Training loss: 2.7731921672821045
Validation loss: 2.454577644666036

Epoch: 6| Step: 3
Training loss: 2.979581117630005
Validation loss: 2.4535908500353494

Epoch: 6| Step: 4
Training loss: 2.564640522003174
Validation loss: 2.452003280321757

Epoch: 6| Step: 5
Training loss: 2.7010881900787354
Validation loss: 2.4477738539377847

Epoch: 6| Step: 6
Training loss: 2.9387621879577637
Validation loss: 2.446036418279012

Epoch: 6| Step: 7
Training loss: 2.7262678146362305
Validation loss: 2.44070041179657

Epoch: 6| Step: 8
Training loss: 2.177290439605713
Validation loss: 2.4367669820785522

Epoch: 6| Step: 9
Training loss: 3.2556190490722656
Validation loss: 2.436304052670797

Epoch: 6| Step: 10
Training loss: 2.182889938354492
Validation loss: 2.4356303612391152

Epoch: 6| Step: 11
Training loss: 3.012052536010742
Validation loss: 2.4326497316360474

Epoch: 6| Step: 12
Training loss: 2.3717660903930664
Validation loss: 2.429567535718282

Epoch: 6| Step: 13
Training loss: 2.0879721641540527
Validation loss: 2.4265989462534585

Epoch: 48| Step: 0
Training loss: 1.6910653114318848
Validation loss: 2.4218501647313437

Epoch: 6| Step: 1
Training loss: 2.5701935291290283
Validation loss: 2.4181841611862183

Epoch: 6| Step: 2
Training loss: 2.241036891937256
Validation loss: 2.4160203536351523

Epoch: 6| Step: 3
Training loss: 3.2272677421569824
Validation loss: 2.4216153621673584

Epoch: 6| Step: 4
Training loss: 2.9416136741638184
Validation loss: 2.4156304200490317

Epoch: 6| Step: 5
Training loss: 2.67106294631958
Validation loss: 2.4109164675076804

Epoch: 6| Step: 6
Training loss: 2.440066337585449
Validation loss: 2.4058311382929483

Epoch: 6| Step: 7
Training loss: 2.69809627532959
Validation loss: 2.4010191758473716

Epoch: 6| Step: 8
Training loss: 2.2803356647491455
Validation loss: 2.4000596404075623

Epoch: 6| Step: 9
Training loss: 2.27622127532959
Validation loss: 2.4009647170702615

Epoch: 6| Step: 10
Training loss: 3.261901378631592
Validation loss: 2.398739139238993

Epoch: 6| Step: 11
Training loss: 2.7444655895233154
Validation loss: 2.403290808200836

Epoch: 6| Step: 12
Training loss: 2.7589235305786133
Validation loss: 2.4026250640551248

Epoch: 6| Step: 13
Training loss: 2.4453165531158447
Validation loss: 2.4012429316838584

Epoch: 49| Step: 0
Training loss: 2.6428604125976562
Validation loss: 2.39909299214681

Epoch: 6| Step: 1
Training loss: 2.373292922973633
Validation loss: 2.3938010533650718

Epoch: 6| Step: 2
Training loss: 3.168231964111328
Validation loss: 2.3907920122146606

Epoch: 6| Step: 3
Training loss: 3.2975573539733887
Validation loss: 2.384826183319092

Epoch: 6| Step: 4
Training loss: 2.2984442710876465
Validation loss: 2.381574352582296

Epoch: 6| Step: 5
Training loss: 2.704535484313965
Validation loss: 2.3769986232121787

Epoch: 6| Step: 6
Training loss: 2.3045873641967773
Validation loss: 2.374359448750814

Epoch: 6| Step: 7
Training loss: 2.1935229301452637
Validation loss: 2.3703388571739197

Epoch: 6| Step: 8
Training loss: 2.2786319255828857
Validation loss: 2.3626875480016074

Epoch: 6| Step: 9
Training loss: 2.5038089752197266
Validation loss: 2.3583296140034995

Epoch: 6| Step: 10
Training loss: 2.4660396575927734
Validation loss: 2.3618483543395996

Epoch: 6| Step: 11
Training loss: 2.8307361602783203
Validation loss: 2.360684315363566

Epoch: 6| Step: 12
Training loss: 2.3917551040649414
Validation loss: 2.3568623264630637

Epoch: 6| Step: 13
Training loss: 2.167557716369629
Validation loss: 2.3598424196243286

Epoch: 50| Step: 0
Training loss: 2.127650737762451
Validation loss: 2.3628063599268594

Epoch: 6| Step: 1
Training loss: 2.0848071575164795
Validation loss: 2.3540913263956704

Epoch: 6| Step: 2
Training loss: 2.446033239364624
Validation loss: 2.3605185747146606

Epoch: 6| Step: 3
Training loss: 2.9580793380737305
Validation loss: 2.358986973762512

Epoch: 6| Step: 4
Training loss: 2.840303897857666
Validation loss: 2.344819724559784

Epoch: 6| Step: 5
Training loss: 2.2037906646728516
Validation loss: 2.3350953658421836

Epoch: 6| Step: 6
Training loss: 2.2702691555023193
Validation loss: 2.3353676398595176

Epoch: 6| Step: 7
Training loss: 2.5530972480773926
Validation loss: 2.335728923479716

Epoch: 6| Step: 8
Training loss: 2.327815055847168
Validation loss: 2.334294319152832

Epoch: 6| Step: 9
Training loss: 2.7101128101348877
Validation loss: 2.3384650150934854

Epoch: 6| Step: 10
Training loss: 2.1998965740203857
Validation loss: 2.341147263844808

Epoch: 6| Step: 11
Training loss: 2.4350028038024902
Validation loss: 2.3411423762639365

Epoch: 6| Step: 12
Training loss: 3.399416208267212
Validation loss: 2.340163826942444

Epoch: 6| Step: 13
Training loss: 2.6048455238342285
Validation loss: 2.3459896047910056

Epoch: 51| Step: 0
Training loss: 2.577913761138916
Validation loss: 2.346943974494934

Epoch: 6| Step: 1
Training loss: 2.3492965698242188
Validation loss: 2.3363466262817383

Epoch: 6| Step: 2
Training loss: 2.2260189056396484
Validation loss: 2.3274275064468384

Epoch: 6| Step: 3
Training loss: 2.3504879474639893
Validation loss: 2.320912460486094

Epoch: 6| Step: 4
Training loss: 2.3660688400268555
Validation loss: 2.3131185372670493

Epoch: 6| Step: 5
Training loss: 2.771303415298462
Validation loss: 2.3095845580101013

Epoch: 6| Step: 6
Training loss: 2.681924343109131
Validation loss: 2.304848392804464

Epoch: 6| Step: 7
Training loss: 2.2034735679626465
Validation loss: 2.3007055521011353

Epoch: 6| Step: 8
Training loss: 2.3469042778015137
Validation loss: 2.2999786933263144

Epoch: 6| Step: 9
Training loss: 2.4681622982025146
Validation loss: 2.296330769856771

Epoch: 6| Step: 10
Training loss: 2.84165096282959
Validation loss: 2.2956088383992515

Epoch: 6| Step: 11
Training loss: 2.4718337059020996
Validation loss: 2.2933775186538696

Epoch: 6| Step: 12
Training loss: 2.5042266845703125
Validation loss: 2.2919851541519165

Epoch: 6| Step: 13
Training loss: 2.5614356994628906
Validation loss: 2.288235684235891

Epoch: 52| Step: 0
Training loss: 2.3461408615112305
Validation loss: 2.2858342130978904

Epoch: 6| Step: 1
Training loss: 2.9200387001037598
Validation loss: 2.2835816542307534

Epoch: 6| Step: 2
Training loss: 2.1621358394622803
Validation loss: 2.2755412062009177

Epoch: 6| Step: 3
Training loss: 3.3735427856445312
Validation loss: 2.2776034275690713

Epoch: 6| Step: 4
Training loss: 1.8358047008514404
Validation loss: 2.2718138297398887

Epoch: 6| Step: 5
Training loss: 2.437499523162842
Validation loss: 2.2716563940048218

Epoch: 6| Step: 6
Training loss: 2.271426200866699
Validation loss: 2.2690958778063455

Epoch: 6| Step: 7
Training loss: 2.6188607215881348
Validation loss: 2.2679912646611533

Epoch: 6| Step: 8
Training loss: 2.0999817848205566
Validation loss: 2.265717367331187

Epoch: 6| Step: 9
Training loss: 2.1553902626037598
Validation loss: 2.261575182278951

Epoch: 6| Step: 10
Training loss: 2.7006053924560547
Validation loss: 2.2596248388290405

Epoch: 6| Step: 11
Training loss: 2.5394225120544434
Validation loss: 2.2591382265090942

Epoch: 6| Step: 12
Training loss: 1.777841329574585
Validation loss: 2.257530987262726

Epoch: 6| Step: 13
Training loss: 2.8682358264923096
Validation loss: 2.2508426904678345

Epoch: 53| Step: 0
Training loss: 2.703556537628174
Validation loss: 2.2516876459121704

Epoch: 6| Step: 1
Training loss: 2.5421738624572754
Validation loss: 2.2452867229779563

Epoch: 6| Step: 2
Training loss: 1.8892072439193726
Validation loss: 2.244662125905355

Epoch: 6| Step: 3
Training loss: 2.1243979930877686
Validation loss: 2.243391215801239

Epoch: 6| Step: 4
Training loss: 2.5221285820007324
Validation loss: 2.2418274879455566

Epoch: 6| Step: 5
Training loss: 2.5723536014556885
Validation loss: 2.2373023629188538

Epoch: 6| Step: 6
Training loss: 2.253044605255127
Validation loss: 2.2351058522860208

Epoch: 6| Step: 7
Training loss: 3.179948329925537
Validation loss: 2.2353552182515464

Epoch: 6| Step: 8
Training loss: 2.0817675590515137
Validation loss: 2.2296246886253357

Epoch: 6| Step: 9
Training loss: 2.7472784519195557
Validation loss: 2.231192628542582

Epoch: 6| Step: 10
Training loss: 2.136584997177124
Validation loss: 2.2279998064041138

Epoch: 6| Step: 11
Training loss: 2.3602547645568848
Validation loss: 2.228946030139923

Epoch: 6| Step: 12
Training loss: 1.9440003633499146
Validation loss: 2.22721399863561

Epoch: 6| Step: 13
Training loss: 2.5032050609588623
Validation loss: 2.2245520750681558

Epoch: 54| Step: 0
Training loss: 2.718743324279785
Validation loss: 2.219785988330841

Epoch: 6| Step: 1
Training loss: 2.0090534687042236
Validation loss: 2.220652182896932

Epoch: 6| Step: 2
Training loss: 2.5241358280181885
Validation loss: 2.2165541847546897

Epoch: 6| Step: 3
Training loss: 2.027330160140991
Validation loss: 2.214694162209829

Epoch: 6| Step: 4
Training loss: 2.6393985748291016
Validation loss: 2.2076096534729004

Epoch: 6| Step: 5
Training loss: 2.1080613136291504
Validation loss: 2.206155021985372

Epoch: 6| Step: 6
Training loss: 2.7289011478424072
Validation loss: 2.202364365259806

Epoch: 6| Step: 7
Training loss: 2.5179362297058105
Validation loss: 2.200756311416626

Epoch: 6| Step: 8
Training loss: 2.258090019226074
Validation loss: 2.201806584993998

Epoch: 6| Step: 9
Training loss: 2.484226703643799
Validation loss: 2.2049415508906045

Epoch: 6| Step: 10
Training loss: 2.7290711402893066
Validation loss: 2.2102651397387185

Epoch: 6| Step: 11
Training loss: 2.6613118648529053
Validation loss: 2.2028279105822244

Epoch: 6| Step: 12
Training loss: 2.066190719604492
Validation loss: 2.197952151298523

Epoch: 6| Step: 13
Training loss: 1.7017484903335571
Validation loss: 2.190940340360006

Epoch: 55| Step: 0
Training loss: 2.2793917655944824
Validation loss: 2.1885282595952353

Epoch: 6| Step: 1
Training loss: 2.4903173446655273
Validation loss: 2.188859979311625

Epoch: 6| Step: 2
Training loss: 2.515171527862549
Validation loss: 2.1879148284594216

Epoch: 6| Step: 3
Training loss: 2.392209768295288
Validation loss: 2.1872500578562417

Epoch: 6| Step: 4
Training loss: 2.151707410812378
Validation loss: 2.1893635789553323

Epoch: 6| Step: 5
Training loss: 2.121504783630371
Validation loss: 2.1865651607513428

Epoch: 6| Step: 6
Training loss: 2.7121381759643555
Validation loss: 2.1852289835611978

Epoch: 6| Step: 7
Training loss: 2.1162869930267334
Validation loss: 2.186297357082367

Epoch: 6| Step: 8
Training loss: 2.9765257835388184
Validation loss: 2.1836040019989014

Epoch: 6| Step: 9
Training loss: 2.1827173233032227
Validation loss: 2.1823402643203735

Epoch: 6| Step: 10
Training loss: 2.0311341285705566
Validation loss: 2.1810200413068137

Epoch: 6| Step: 11
Training loss: 2.2714061737060547
Validation loss: 2.1761775414148965

Epoch: 6| Step: 12
Training loss: 2.773601531982422
Validation loss: 2.1722141106923423

Epoch: 6| Step: 13
Training loss: 1.8400877714157104
Validation loss: 2.173300822575887

Epoch: 56| Step: 0
Training loss: 2.1809630393981934
Validation loss: 2.1707014640172324

Epoch: 6| Step: 1
Training loss: 2.4896035194396973
Validation loss: 2.1640045444170632

Epoch: 6| Step: 2
Training loss: 2.545928955078125
Validation loss: 2.168544332186381

Epoch: 6| Step: 3
Training loss: 2.199995517730713
Validation loss: 2.1692081888516745

Epoch: 6| Step: 4
Training loss: 2.327298641204834
Validation loss: 2.172635277112325

Epoch: 6| Step: 5
Training loss: 1.6649389266967773
Validation loss: 2.1599578062693277

Epoch: 6| Step: 6
Training loss: 2.386958360671997
Validation loss: 2.1595234076182046

Epoch: 6| Step: 7
Training loss: 2.9617762565612793
Validation loss: 2.1534728407859802

Epoch: 6| Step: 8
Training loss: 2.0274834632873535
Validation loss: 2.1599462032318115

Epoch: 6| Step: 9
Training loss: 2.114605665206909
Validation loss: 2.156201720237732

Epoch: 6| Step: 10
Training loss: 2.5219762325286865
Validation loss: 2.1603729724884033

Epoch: 6| Step: 11
Training loss: 3.1405792236328125
Validation loss: 2.159426132837931

Epoch: 6| Step: 12
Training loss: 1.4707553386688232
Validation loss: 2.1569561759630838

Epoch: 6| Step: 13
Training loss: 2.5883798599243164
Validation loss: 2.1641844113667807

Epoch: 57| Step: 0
Training loss: 2.5074424743652344
Validation loss: 2.167729139328003

Epoch: 6| Step: 1
Training loss: 2.830091953277588
Validation loss: 2.1674124797185264

Epoch: 6| Step: 2
Training loss: 2.3434178829193115
Validation loss: 2.1669485767682395

Epoch: 6| Step: 3
Training loss: 2.19551157951355
Validation loss: 2.1574772198994956

Epoch: 6| Step: 4
Training loss: 2.1892123222351074
Validation loss: 2.1534777084986367

Epoch: 6| Step: 5
Training loss: 2.1581459045410156
Validation loss: 2.148175080617269

Epoch: 6| Step: 6
Training loss: 2.394406318664551
Validation loss: 2.146526515483856

Epoch: 6| Step: 7
Training loss: 2.4849424362182617
Validation loss: 2.146022915840149

Epoch: 6| Step: 8
Training loss: 2.8755125999450684
Validation loss: 2.139669179916382

Epoch: 6| Step: 9
Training loss: 2.3823928833007812
Validation loss: 2.137122690677643

Epoch: 6| Step: 10
Training loss: 1.7462468147277832
Validation loss: 2.129173755645752

Epoch: 6| Step: 11
Training loss: 2.1790130138397217
Validation loss: 2.1269152561823526

Epoch: 6| Step: 12
Training loss: 2.1065239906311035
Validation loss: 2.130918820699056

Epoch: 6| Step: 13
Training loss: 1.957465648651123
Validation loss: 2.13063782453537

Epoch: 58| Step: 0
Training loss: 2.530752182006836
Validation loss: 2.1216431260108948

Epoch: 6| Step: 1
Training loss: 2.594937324523926
Validation loss: 2.1251547932624817

Epoch: 6| Step: 2
Training loss: 1.8999395370483398
Validation loss: 2.1237528324127197

Epoch: 6| Step: 3
Training loss: 2.593402862548828
Validation loss: 2.1275877952575684

Epoch: 6| Step: 4
Training loss: 2.5430517196655273
Validation loss: 2.1217539310455322

Epoch: 6| Step: 5
Training loss: 2.6934149265289307
Validation loss: 2.122720738252004

Epoch: 6| Step: 6
Training loss: 1.9791491031646729
Validation loss: 2.1210935513178506

Epoch: 6| Step: 7
Training loss: 1.7157164812088013
Validation loss: 2.1178600192070007

Epoch: 6| Step: 8
Training loss: 1.8766710758209229
Validation loss: 2.1227839986483255

Epoch: 6| Step: 9
Training loss: 2.410985231399536
Validation loss: 2.1238444248835244

Epoch: 6| Step: 10
Training loss: 2.3241090774536133
Validation loss: 2.1277759671211243

Epoch: 6| Step: 11
Training loss: 2.6095848083496094
Validation loss: 2.143197238445282

Epoch: 6| Step: 12
Training loss: 2.319810390472412
Validation loss: 2.1512944102287292

Epoch: 6| Step: 13
Training loss: 2.159900188446045
Validation loss: 2.1348077257474265

Epoch: 59| Step: 0
Training loss: 2.3782548904418945
Validation loss: 2.115532477696737

Epoch: 6| Step: 1
Training loss: 2.547919988632202
Validation loss: 2.1187854409217834

Epoch: 6| Step: 2
Training loss: 2.1755776405334473
Validation loss: 2.119648059209188

Epoch: 6| Step: 3
Training loss: 2.459758758544922
Validation loss: 2.116596221923828

Epoch: 6| Step: 4
Training loss: 2.153926134109497
Validation loss: 2.1221396923065186

Epoch: 6| Step: 5
Training loss: 2.3047611713409424
Validation loss: 2.1202973326047263

Epoch: 6| Step: 6
Training loss: 2.1204304695129395
Validation loss: 2.125702122847239

Epoch: 6| Step: 7
Training loss: 1.9480284452438354
Validation loss: 2.127088208993276

Epoch: 6| Step: 8
Training loss: 2.1121339797973633
Validation loss: 2.1299059788386026

Epoch: 6| Step: 9
Training loss: 2.5498883724212646
Validation loss: 2.125220318635305

Epoch: 6| Step: 10
Training loss: 2.8522729873657227
Validation loss: 2.120985984802246

Epoch: 6| Step: 11
Training loss: 1.657533049583435
Validation loss: 2.119955003261566

Epoch: 6| Step: 12
Training loss: 2.903965711593628
Validation loss: 2.113740086555481

Epoch: 6| Step: 13
Training loss: 1.949516773223877
Validation loss: 2.1132472157478333

Epoch: 60| Step: 0
Training loss: 2.717020273208618
Validation loss: 2.1128068963686624

Epoch: 6| Step: 1
Training loss: 2.5811092853546143
Validation loss: 2.108519275983175

Epoch: 6| Step: 2
Training loss: 1.8579380512237549
Validation loss: 2.1062552531560264

Epoch: 6| Step: 3
Training loss: 1.830338954925537
Validation loss: 2.106652001539866

Epoch: 6| Step: 4
Training loss: 1.8814436197280884
Validation loss: 2.097751279671987

Epoch: 6| Step: 5
Training loss: 1.5782972574234009
Validation loss: 2.0947203238805137

Epoch: 6| Step: 6
Training loss: 2.9062540531158447
Validation loss: 2.0959128538767495

Epoch: 6| Step: 7
Training loss: 2.4318783283233643
Validation loss: 2.0916694800059

Epoch: 6| Step: 8
Training loss: 2.2463836669921875
Validation loss: 2.0932905673980713

Epoch: 6| Step: 9
Training loss: 2.4109549522399902
Validation loss: 2.091261545817057

Epoch: 6| Step: 10
Training loss: 2.2626194953918457
Validation loss: 2.090092658996582

Epoch: 6| Step: 11
Training loss: 2.3546242713928223
Validation loss: 2.086660921573639

Epoch: 6| Step: 12
Training loss: 2.4775261878967285
Validation loss: 2.0880593260129294

Epoch: 6| Step: 13
Training loss: 2.243014097213745
Validation loss: 2.0831883351008096

Epoch: 61| Step: 0
Training loss: 2.2940287590026855
Validation loss: 2.078516105810801

Epoch: 6| Step: 1
Training loss: 2.0058367252349854
Validation loss: 2.0766064325968423

Epoch: 6| Step: 2
Training loss: 2.8033766746520996
Validation loss: 2.077063779036204

Epoch: 6| Step: 3
Training loss: 2.5198419094085693
Validation loss: 2.075258513291677

Epoch: 6| Step: 4
Training loss: 1.9366744756698608
Validation loss: 2.077755590279897

Epoch: 6| Step: 5
Training loss: 2.227893352508545
Validation loss: 2.071071982383728

Epoch: 6| Step: 6
Training loss: 1.686934232711792
Validation loss: 2.0774175922075906

Epoch: 6| Step: 7
Training loss: 1.940042495727539
Validation loss: 2.07854034503301

Epoch: 6| Step: 8
Training loss: 2.2212300300598145
Validation loss: 2.073882043361664

Epoch: 6| Step: 9
Training loss: 2.700498342514038
Validation loss: 2.0687324603398642

Epoch: 6| Step: 10
Training loss: 3.1320297718048096
Validation loss: 2.0715545217196145

Epoch: 6| Step: 11
Training loss: 2.5319008827209473
Validation loss: 2.0761308073997498

Epoch: 6| Step: 12
Training loss: 2.1344282627105713
Validation loss: 2.0759849548339844

Epoch: 6| Step: 13
Training loss: 1.5191943645477295
Validation loss: 2.0803649028142295

Epoch: 62| Step: 0
Training loss: 1.916796088218689
Validation loss: 2.077428638935089

Epoch: 6| Step: 1
Training loss: 2.5492758750915527
Validation loss: 2.0754719972610474

Epoch: 6| Step: 2
Training loss: 2.333923816680908
Validation loss: 2.0667778650919595

Epoch: 6| Step: 3
Training loss: 2.7081174850463867
Validation loss: 2.0668734510739646

Epoch: 6| Step: 4
Training loss: 2.3525290489196777
Validation loss: 2.0592145721117654

Epoch: 6| Step: 5
Training loss: 2.326803684234619
Validation loss: 2.065134286880493

Epoch: 6| Step: 6
Training loss: 2.794783592224121
Validation loss: 2.079977591832479

Epoch: 6| Step: 7
Training loss: 1.4308964014053345
Validation loss: 2.093812425931295

Epoch: 6| Step: 8
Training loss: 2.254176139831543
Validation loss: 2.0905372500419617

Epoch: 6| Step: 9
Training loss: 1.7416415214538574
Validation loss: 2.1016844511032104

Epoch: 6| Step: 10
Training loss: 2.3902859687805176
Validation loss: 2.087717851003011

Epoch: 6| Step: 11
Training loss: 2.0937039852142334
Validation loss: 2.0821064909299216

Epoch: 6| Step: 12
Training loss: 2.3372652530670166
Validation loss: 2.056696275870005

Epoch: 6| Step: 13
Training loss: 2.334965705871582
Validation loss: 2.060272753238678

Epoch: 63| Step: 0
Training loss: 2.196107864379883
Validation loss: 2.066165288289388

Epoch: 6| Step: 1
Training loss: 2.3173184394836426
Validation loss: 2.0700820287068686

Epoch: 6| Step: 2
Training loss: 2.108027458190918
Validation loss: 2.0729611118634543

Epoch: 6| Step: 3
Training loss: 2.7323451042175293
Validation loss: 2.077580233414968

Epoch: 6| Step: 4
Training loss: 1.5036983489990234
Validation loss: 2.0909795562426248

Epoch: 6| Step: 5
Training loss: 2.387880802154541
Validation loss: 2.098135689894358

Epoch: 6| Step: 6
Training loss: 2.9331979751586914
Validation loss: 2.0994595885276794

Epoch: 6| Step: 7
Training loss: 2.0701701641082764
Validation loss: 2.1215722362200418

Epoch: 6| Step: 8
Training loss: 2.2417407035827637
Validation loss: 2.1214850743611655

Epoch: 6| Step: 9
Training loss: 1.8906545639038086
Validation loss: 2.0986366470654807

Epoch: 6| Step: 10
Training loss: 2.6075477600097656
Validation loss: 2.0780622561772666

Epoch: 6| Step: 11
Training loss: 2.1976447105407715
Validation loss: 2.0704997181892395

Epoch: 6| Step: 12
Training loss: 2.277085304260254
Validation loss: 2.064556896686554

Epoch: 6| Step: 13
Training loss: 2.2679152488708496
Validation loss: 2.0620723366737366

Epoch: 64| Step: 0
Training loss: 2.3384082317352295
Validation loss: 2.0627286036809287

Epoch: 6| Step: 1
Training loss: 2.6271119117736816
Validation loss: 2.053001582622528

Epoch: 6| Step: 2
Training loss: 2.0132455825805664
Validation loss: 2.053670883178711

Epoch: 6| Step: 3
Training loss: 2.075300455093384
Validation loss: 2.0512279868125916

Epoch: 6| Step: 4
Training loss: 2.0472545623779297
Validation loss: 2.0482736428578696

Epoch: 6| Step: 5
Training loss: 1.8061283826828003
Validation loss: 2.040911773840586

Epoch: 6| Step: 6
Training loss: 2.2387495040893555
Validation loss: 2.0527817408243814

Epoch: 6| Step: 7
Training loss: 2.226698160171509
Validation loss: 2.0543213486671448

Epoch: 6| Step: 8
Training loss: 2.5123655796051025
Validation loss: 2.0626351833343506

Epoch: 6| Step: 9
Training loss: 2.2358815670013428
Validation loss: 2.0615873535474143

Epoch: 6| Step: 10
Training loss: 2.2243294715881348
Validation loss: 2.058627208073934

Epoch: 6| Step: 11
Training loss: 2.569380283355713
Validation loss: 2.0550220012664795

Epoch: 6| Step: 12
Training loss: 2.432058811187744
Validation loss: 2.047113756338755

Epoch: 6| Step: 13
Training loss: 1.9813669919967651
Validation loss: 2.0421826243400574

Epoch: 65| Step: 0
Training loss: 2.5602169036865234
Validation loss: 2.0459371209144592

Epoch: 6| Step: 1
Training loss: 1.7506417036056519
Validation loss: 2.0410012006759644

Epoch: 6| Step: 2
Training loss: 2.4110445976257324
Validation loss: 2.0356109539667764

Epoch: 6| Step: 3
Training loss: 1.7079927921295166
Validation loss: 2.047578752040863

Epoch: 6| Step: 4
Training loss: 1.634338617324829
Validation loss: 2.0421496629714966

Epoch: 6| Step: 5
Training loss: 1.9827632904052734
Validation loss: 2.0410146713256836

Epoch: 6| Step: 6
Training loss: 2.2161173820495605
Validation loss: 2.0375123023986816

Epoch: 6| Step: 7
Training loss: 2.10151743888855
Validation loss: 2.0381888151168823

Epoch: 6| Step: 8
Training loss: 2.531959295272827
Validation loss: 2.041540543238322

Epoch: 6| Step: 9
Training loss: 2.332488536834717
Validation loss: 2.04452512661616

Epoch: 6| Step: 10
Training loss: 2.7332730293273926
Validation loss: 2.042042632897695

Epoch: 6| Step: 11
Training loss: 2.689509868621826
Validation loss: 2.0440951585769653

Epoch: 6| Step: 12
Training loss: 1.8536229133605957
Validation loss: 2.0458874503771463

Epoch: 6| Step: 13
Training loss: 2.573373794555664
Validation loss: 2.041076441605886

Epoch: 66| Step: 0
Training loss: 2.065370559692383
Validation loss: 2.0488541523615518

Epoch: 6| Step: 1
Training loss: 2.0228116512298584
Validation loss: 2.0447800556818643

Epoch: 6| Step: 2
Training loss: 2.108358144760132
Validation loss: 2.0488585829734802

Epoch: 6| Step: 3
Training loss: 2.5488529205322266
Validation loss: 2.0505152146021524

Epoch: 6| Step: 4
Training loss: 1.8356267213821411
Validation loss: 2.049668828646342

Epoch: 6| Step: 5
Training loss: 2.4947986602783203
Validation loss: 2.049631575743357

Epoch: 6| Step: 6
Training loss: 1.9207704067230225
Validation loss: 2.0492417216300964

Epoch: 6| Step: 7
Training loss: 2.530726909637451
Validation loss: 2.0480031768480935

Epoch: 6| Step: 8
Training loss: 2.361328125
Validation loss: 2.048828681310018

Epoch: 6| Step: 9
Training loss: 2.224842071533203
Validation loss: 2.0511188904444375

Epoch: 6| Step: 10
Training loss: 2.408358573913574
Validation loss: 2.0447245041529336

Epoch: 6| Step: 11
Training loss: 2.6967711448669434
Validation loss: 2.0456379055976868

Epoch: 6| Step: 12
Training loss: 1.797002911567688
Validation loss: 2.0443002382914224

Epoch: 6| Step: 13
Training loss: 2.2845451831817627
Validation loss: 2.04535319407781

Epoch: 67| Step: 0
Training loss: 2.6454410552978516
Validation loss: 2.034459034601847

Epoch: 6| Step: 1
Training loss: 1.638431191444397
Validation loss: 2.026153345902761

Epoch: 6| Step: 2
Training loss: 2.165370464324951
Validation loss: 2.0317987402280173

Epoch: 6| Step: 3
Training loss: 3.0189661979675293
Validation loss: 2.0488709807395935

Epoch: 6| Step: 4
Training loss: 1.9392240047454834
Validation loss: 2.047859847545624

Epoch: 6| Step: 5
Training loss: 1.7462507486343384
Validation loss: 2.0622494419415793

Epoch: 6| Step: 6
Training loss: 2.8542332649230957
Validation loss: 2.073214371999105

Epoch: 6| Step: 7
Training loss: 2.157289505004883
Validation loss: 2.0926273663838706

Epoch: 6| Step: 8
Training loss: 2.150329113006592
Validation loss: 2.0950198769569397

Epoch: 6| Step: 9
Training loss: 1.9376872777938843
Validation loss: 2.104386309782664

Epoch: 6| Step: 10
Training loss: 2.2397897243499756
Validation loss: 2.100059966246287

Epoch: 6| Step: 11
Training loss: 1.7523853778839111
Validation loss: 2.087208946545919

Epoch: 6| Step: 12
Training loss: 2.666752576828003
Validation loss: 2.0749409993489585

Epoch: 6| Step: 13
Training loss: 2.3909716606140137
Validation loss: 2.0552092591921487

Epoch: 68| Step: 0
Training loss: 2.2985196113586426
Validation loss: 2.030262311299642

Epoch: 6| Step: 1
Training loss: 2.3530619144439697
Validation loss: 2.0249165296554565

Epoch: 6| Step: 2
Training loss: 2.3643088340759277
Validation loss: 2.0399897694587708

Epoch: 6| Step: 3
Training loss: 2.297264337539673
Validation loss: 2.0478264888127646

Epoch: 6| Step: 4
Training loss: 2.609585762023926
Validation loss: 2.0624711513519287

Epoch: 6| Step: 5
Training loss: 2.0571162700653076
Validation loss: 2.0778799851735434

Epoch: 6| Step: 6
Training loss: 2.4186887741088867
Validation loss: 2.109085818131765

Epoch: 6| Step: 7
Training loss: 2.169339179992676
Validation loss: 2.1395720640818277

Epoch: 6| Step: 8
Training loss: 2.0406394004821777
Validation loss: 2.144854108492533

Epoch: 6| Step: 9
Training loss: 2.7944910526275635
Validation loss: 2.1541884342829385

Epoch: 6| Step: 10
Training loss: 2.077540636062622
Validation loss: 2.1149075031280518

Epoch: 6| Step: 11
Training loss: 2.0350570678710938
Validation loss: 2.0938438773155212

Epoch: 6| Step: 12
Training loss: 2.3286683559417725
Validation loss: 2.074864308039347

Epoch: 6| Step: 13
Training loss: 1.8553866147994995
Validation loss: 2.058854599793752

Epoch: 69| Step: 0
Training loss: 2.209536552429199
Validation loss: 2.0559902588526406

Epoch: 6| Step: 1
Training loss: 2.0234458446502686
Validation loss: 2.0475078026453652

Epoch: 6| Step: 2
Training loss: 2.1194562911987305
Validation loss: 2.049969792366028

Epoch: 6| Step: 3
Training loss: 2.4725959300994873
Validation loss: 2.04021292924881

Epoch: 6| Step: 4
Training loss: 1.98135244846344
Validation loss: 2.0446013808250427

Epoch: 6| Step: 5
Training loss: 2.692368507385254
Validation loss: 2.0405327677726746

Epoch: 6| Step: 6
Training loss: 2.587470054626465
Validation loss: 2.037332077821096

Epoch: 6| Step: 7
Training loss: 2.380722761154175
Validation loss: 2.0347065130869546

Epoch: 6| Step: 8
Training loss: 2.5048930644989014
Validation loss: 2.0297436316808066

Epoch: 6| Step: 9
Training loss: 1.7575671672821045
Validation loss: 2.0325306057929993

Epoch: 6| Step: 10
Training loss: 2.047346591949463
Validation loss: 2.028125246365865

Epoch: 6| Step: 11
Training loss: 2.228577136993408
Validation loss: 2.0270601312319436

Epoch: 6| Step: 12
Training loss: 2.317122220993042
Validation loss: 2.0318278471628823

Epoch: 6| Step: 13
Training loss: 1.7579630613327026
Validation loss: 2.0274477998415628

Epoch: 70| Step: 0
Training loss: 1.839064598083496
Validation loss: 2.038041432698568

Epoch: 6| Step: 1
Training loss: 2.5932540893554688
Validation loss: 2.0384391943613687

Epoch: 6| Step: 2
Training loss: 1.7797267436981201
Validation loss: 2.025987188021342

Epoch: 6| Step: 3
Training loss: 2.277595043182373
Validation loss: 2.025031328201294

Epoch: 6| Step: 4
Training loss: 2.1964879035949707
Validation loss: 2.031888802846273

Epoch: 6| Step: 5
Training loss: 2.2390615940093994
Validation loss: 2.0337613821029663

Epoch: 6| Step: 6
Training loss: 2.3608481884002686
Validation loss: 2.0294811924298606

Epoch: 6| Step: 7
Training loss: 2.4404258728027344
Validation loss: 2.034092903137207

Epoch: 6| Step: 8
Training loss: 2.4496166706085205
Validation loss: 2.0394508441289267

Epoch: 6| Step: 9
Training loss: 3.1329269409179688
Validation loss: 2.0267240007718406

Epoch: 6| Step: 10
Training loss: 1.2675201892852783
Validation loss: 2.025448222955068

Epoch: 6| Step: 11
Training loss: 2.216221809387207
Validation loss: 2.0162636637687683

Epoch: 6| Step: 12
Training loss: 2.0362768173217773
Validation loss: 2.020837187767029

Epoch: 6| Step: 13
Training loss: 2.1317715644836426
Validation loss: 2.018947422504425

Epoch: 71| Step: 0
Training loss: 2.21639084815979
Validation loss: 2.0265822609265647

Epoch: 6| Step: 1
Training loss: 1.6517651081085205
Validation loss: 2.0285095969835916

Epoch: 6| Step: 2
Training loss: 2.6896979808807373
Validation loss: 2.024397552013397

Epoch: 6| Step: 3
Training loss: 2.3079488277435303
Validation loss: 2.0243865251541138

Epoch: 6| Step: 4
Training loss: 1.5997389554977417
Validation loss: 2.0289135575294495

Epoch: 6| Step: 5
Training loss: 2.4907655715942383
Validation loss: 2.022158702214559

Epoch: 6| Step: 6
Training loss: 2.171051025390625
Validation loss: 2.0299919843673706

Epoch: 6| Step: 7
Training loss: 2.2036190032958984
Validation loss: 2.024007042249044

Epoch: 6| Step: 8
Training loss: 2.000091314315796
Validation loss: 2.0196339885393777

Epoch: 6| Step: 9
Training loss: 2.4713492393493652
Validation loss: 2.0132699012756348

Epoch: 6| Step: 10
Training loss: 2.5490643978118896
Validation loss: 2.0156925121943154

Epoch: 6| Step: 11
Training loss: 1.7529621124267578
Validation loss: 2.0132723848025003

Epoch: 6| Step: 12
Training loss: 2.0918493270874023
Validation loss: 2.023086587587992

Epoch: 6| Step: 13
Training loss: 2.5552210807800293
Validation loss: 2.015592396259308

Epoch: 72| Step: 0
Training loss: 1.72003173828125
Validation loss: 2.016777773698171

Epoch: 6| Step: 1
Training loss: 2.422548770904541
Validation loss: 2.018821736176809

Epoch: 6| Step: 2
Training loss: 2.468675136566162
Validation loss: 2.0129829049110413

Epoch: 6| Step: 3
Training loss: 2.5452232360839844
Validation loss: 2.0097528100013733

Epoch: 6| Step: 4
Training loss: 1.681947112083435
Validation loss: 2.0136324564615884

Epoch: 6| Step: 5
Training loss: 2.2668654918670654
Validation loss: 2.0098766883214316

Epoch: 6| Step: 6
Training loss: 2.4913978576660156
Validation loss: 2.0129637320836387

Epoch: 6| Step: 7
Training loss: 2.671875
Validation loss: 2.0150719483693442

Epoch: 6| Step: 8
Training loss: 1.863279104232788
Validation loss: 2.0157705744107566

Epoch: 6| Step: 9
Training loss: 2.2186379432678223
Validation loss: 2.0131404995918274

Epoch: 6| Step: 10
Training loss: 1.6903138160705566
Validation loss: 2.0154950420061746

Epoch: 6| Step: 11
Training loss: 2.0424747467041016
Validation loss: 2.0143133997917175

Epoch: 6| Step: 12
Training loss: 1.9729257822036743
Validation loss: 2.020185728867849

Epoch: 6| Step: 13
Training loss: 2.655822992324829
Validation loss: 2.008613924185435

Epoch: 73| Step: 0
Training loss: 1.5621527433395386
Validation loss: 2.011243780454

Epoch: 6| Step: 1
Training loss: 2.2614543437957764
Validation loss: 2.0124427477518716

Epoch: 6| Step: 2
Training loss: 3.025954246520996
Validation loss: 2.0141128102938333

Epoch: 6| Step: 3
Training loss: 2.4330201148986816
Validation loss: 2.0106512109438577

Epoch: 6| Step: 4
Training loss: 2.015202045440674
Validation loss: 2.0148471792538962

Epoch: 6| Step: 5
Training loss: 2.3234128952026367
Validation loss: 2.0137819647789

Epoch: 6| Step: 6
Training loss: 1.7605371475219727
Validation loss: 2.016691207885742

Epoch: 6| Step: 7
Training loss: 2.3979384899139404
Validation loss: 2.0139644940694175

Epoch: 6| Step: 8
Training loss: 2.129218101501465
Validation loss: 2.0189013481140137

Epoch: 6| Step: 9
Training loss: 2.2240419387817383
Validation loss: 2.0193735162417092

Epoch: 6| Step: 10
Training loss: 1.8924064636230469
Validation loss: 2.015428602695465

Epoch: 6| Step: 11
Training loss: 1.9480843544006348
Validation loss: 2.0121384064356485

Epoch: 6| Step: 12
Training loss: 2.77482008934021
Validation loss: 2.0103824933369956

Epoch: 6| Step: 13
Training loss: 2.021555185317993
Validation loss: 2.0166245897610984

Epoch: 74| Step: 0
Training loss: 2.138134241104126
Validation loss: 2.0054858525594077

Epoch: 6| Step: 1
Training loss: 1.9415559768676758
Validation loss: 2.0201852122942605

Epoch: 6| Step: 2
Training loss: 1.3293707370758057
Validation loss: 2.0308215618133545

Epoch: 6| Step: 3
Training loss: 2.6487138271331787
Validation loss: 2.036170025666555

Epoch: 6| Step: 4
Training loss: 2.48441219329834
Validation loss: 2.034236967563629

Epoch: 6| Step: 5
Training loss: 1.9901392459869385
Validation loss: 2.034070074558258

Epoch: 6| Step: 6
Training loss: 2.6495299339294434
Validation loss: 2.0234551628430686

Epoch: 6| Step: 7
Training loss: 2.7136330604553223
Validation loss: 2.016984502474467

Epoch: 6| Step: 8
Training loss: 2.3175787925720215
Validation loss: 2.0131624937057495

Epoch: 6| Step: 9
Training loss: 1.878730058670044
Validation loss: 2.0164865056673684

Epoch: 6| Step: 10
Training loss: 2.2639119625091553
Validation loss: 2.016097923119863

Epoch: 6| Step: 11
Training loss: 2.0882222652435303
Validation loss: 2.016388754049937

Epoch: 6| Step: 12
Training loss: 2.110462188720703
Validation loss: 2.0164204438527427

Epoch: 6| Step: 13
Training loss: 1.903547763824463
Validation loss: 2.01501057545344

Epoch: 75| Step: 0
Training loss: 1.8691627979278564
Validation loss: 2.0219908356666565

Epoch: 6| Step: 1
Training loss: 2.2703778743743896
Validation loss: 2.02267191807429

Epoch: 6| Step: 2
Training loss: 2.641244649887085
Validation loss: 2.0269603530565896

Epoch: 6| Step: 3
Training loss: 2.1602394580841064
Validation loss: 2.02870645125707

Epoch: 6| Step: 4
Training loss: 2.332292318344116
Validation loss: 2.0264708201090493

Epoch: 6| Step: 5
Training loss: 2.127269983291626
Validation loss: 2.0210571885108948

Epoch: 6| Step: 6
Training loss: 2.34122896194458
Validation loss: 2.0256648659706116

Epoch: 6| Step: 7
Training loss: 2.212209701538086
Validation loss: 2.0207648078600564

Epoch: 6| Step: 8
Training loss: 2.124906063079834
Validation loss: 2.016773045063019

Epoch: 6| Step: 9
Training loss: 2.364640235900879
Validation loss: 2.011450390021006

Epoch: 6| Step: 10
Training loss: 1.996091604232788
Validation loss: 2.0200499097506204

Epoch: 6| Step: 11
Training loss: 1.8279269933700562
Validation loss: 2.013276994228363

Epoch: 6| Step: 12
Training loss: 2.2252228260040283
Validation loss: 2.0125322739283242

Epoch: 6| Step: 13
Training loss: 2.1858105659484863
Validation loss: 2.015769839286804

Epoch: 76| Step: 0
Training loss: 1.9619078636169434
Validation loss: 2.016847093900045

Epoch: 6| Step: 1
Training loss: 1.9125431776046753
Validation loss: 2.019487420717875

Epoch: 6| Step: 2
Training loss: 2.2933008670806885
Validation loss: 2.0163872639338174

Epoch: 6| Step: 3
Training loss: 1.8199456930160522
Validation loss: 2.014601469039917

Epoch: 6| Step: 4
Training loss: 2.418055772781372
Validation loss: 2.0220521092414856

Epoch: 6| Step: 5
Training loss: 2.3887779712677
Validation loss: 2.0166773796081543

Epoch: 6| Step: 6
Training loss: 2.0354156494140625
Validation loss: 2.023540218671163

Epoch: 6| Step: 7
Training loss: 2.338233709335327
Validation loss: 2.0346367359161377

Epoch: 6| Step: 8
Training loss: 2.242824077606201
Validation loss: 2.0326010982195535

Epoch: 6| Step: 9
Training loss: 2.388479471206665
Validation loss: 2.0326480666796365

Epoch: 6| Step: 10
Training loss: 2.190419912338257
Validation loss: 2.0347044467926025

Epoch: 6| Step: 11
Training loss: 2.209929943084717
Validation loss: 2.025357266267141

Epoch: 6| Step: 12
Training loss: 2.4582207202911377
Validation loss: 2.014905790487925

Epoch: 6| Step: 13
Training loss: 1.7935717105865479
Validation loss: 2.021029810110728

Epoch: 77| Step: 0
Training loss: 2.9347314834594727
Validation loss: 2.0210989912350974

Epoch: 6| Step: 1
Training loss: 1.994274377822876
Validation loss: 2.025534689426422

Epoch: 6| Step: 2
Training loss: 2.03208327293396
Validation loss: 2.02472452322642

Epoch: 6| Step: 3
Training loss: 1.8983241319656372
Validation loss: 2.029526730378469

Epoch: 6| Step: 4
Training loss: 1.9029226303100586
Validation loss: 2.0274300376574197

Epoch: 6| Step: 5
Training loss: 1.7959564924240112
Validation loss: 2.0255335569381714

Epoch: 6| Step: 6
Training loss: 1.9521644115447998
Validation loss: 2.0220157702763877

Epoch: 6| Step: 7
Training loss: 2.1365766525268555
Validation loss: 2.0201928416887918

Epoch: 6| Step: 8
Training loss: 1.8243896961212158
Validation loss: 2.017162024974823

Epoch: 6| Step: 9
Training loss: 2.27217435836792
Validation loss: 2.010992427666982

Epoch: 6| Step: 10
Training loss: 2.5981969833374023
Validation loss: 2.0200897653897605

Epoch: 6| Step: 11
Training loss: 2.966832160949707
Validation loss: 2.0197777152061462

Epoch: 6| Step: 12
Training loss: 2.3886218070983887
Validation loss: 2.0315963427225747

Epoch: 6| Step: 13
Training loss: 1.721996784210205
Validation loss: 2.037951707839966

Epoch: 78| Step: 0
Training loss: 2.093205451965332
Validation loss: 2.0371850728988647

Epoch: 6| Step: 1
Training loss: 2.008768081665039
Validation loss: 2.0378224849700928

Epoch: 6| Step: 2
Training loss: 1.8294535875320435
Validation loss: 2.0299275517463684

Epoch: 6| Step: 3
Training loss: 2.7989072799682617
Validation loss: 2.025167008241018

Epoch: 6| Step: 4
Training loss: 2.1820337772369385
Validation loss: 2.0235809087753296

Epoch: 6| Step: 5
Training loss: 2.3681960105895996
Validation loss: 2.0169122219085693

Epoch: 6| Step: 6
Training loss: 2.033627510070801
Validation loss: 2.0180546641349792

Epoch: 6| Step: 7
Training loss: 1.891448736190796
Validation loss: 2.0312461455663047

Epoch: 6| Step: 8
Training loss: 2.3365554809570312
Validation loss: 2.033688187599182

Epoch: 6| Step: 9
Training loss: 1.9800235033035278
Validation loss: 2.031272570292155

Epoch: 6| Step: 10
Training loss: 2.0764565467834473
Validation loss: 2.032220105330149

Epoch: 6| Step: 11
Training loss: 2.4059865474700928
Validation loss: 2.0347240765889487

Epoch: 6| Step: 12
Training loss: 2.6695778369903564
Validation loss: 2.0355817079544067

Epoch: 6| Step: 13
Training loss: 1.8998130559921265
Validation loss: 2.038581589857737

Epoch: 79| Step: 0
Training loss: 1.9958555698394775
Validation loss: 2.038334310054779

Epoch: 6| Step: 1
Training loss: 2.196707010269165
Validation loss: 2.035702427228292

Epoch: 6| Step: 2
Training loss: 1.9944229125976562
Validation loss: 2.0326507290204368

Epoch: 6| Step: 3
Training loss: 1.8803638219833374
Validation loss: 2.0341169834136963

Epoch: 6| Step: 4
Training loss: 2.1333370208740234
Validation loss: 2.0278333624204

Epoch: 6| Step: 5
Training loss: 2.080583095550537
Validation loss: 2.0276010036468506

Epoch: 6| Step: 6
Training loss: 2.1599249839782715
Validation loss: 2.0223839481671653

Epoch: 6| Step: 7
Training loss: 1.4094253778457642
Validation loss: 2.020983099937439

Epoch: 6| Step: 8
Training loss: 2.5518252849578857
Validation loss: 2.020167112350464

Epoch: 6| Step: 9
Training loss: 1.8465502262115479
Validation loss: 2.0139703353246055

Epoch: 6| Step: 10
Training loss: 1.798098087310791
Validation loss: 2.0179851055145264

Epoch: 6| Step: 11
Training loss: 2.835988759994507
Validation loss: 2.016061862309774

Epoch: 6| Step: 12
Training loss: 2.8142201900482178
Validation loss: 2.012893537680308

Epoch: 6| Step: 13
Training loss: 2.991067409515381
Validation loss: 2.0185745557149253

Epoch: 80| Step: 0
Training loss: 2.0115151405334473
Validation loss: 2.011371453603109

Epoch: 6| Step: 1
Training loss: 1.8917127847671509
Validation loss: 2.0217467149098716

Epoch: 6| Step: 2
Training loss: 3.0136966705322266
Validation loss: 2.014447649319967

Epoch: 6| Step: 3
Training loss: 1.9411098957061768
Validation loss: 2.0178141593933105

Epoch: 6| Step: 4
Training loss: 1.8624755144119263
Validation loss: 2.0208452939987183

Epoch: 6| Step: 5
Training loss: 2.416295051574707
Validation loss: 2.021708865960439

Epoch: 6| Step: 6
Training loss: 2.2382287979125977
Validation loss: 2.0275745590527854

Epoch: 6| Step: 7
Training loss: 2.0421245098114014
Validation loss: 2.0162565112113953

Epoch: 6| Step: 8
Training loss: 1.8153294324874878
Validation loss: 2.018937647342682

Epoch: 6| Step: 9
Training loss: 2.14216947555542
Validation loss: 2.0113946199417114

Epoch: 6| Step: 10
Training loss: 2.427488327026367
Validation loss: 2.0214073061943054

Epoch: 6| Step: 11
Training loss: 2.182687759399414
Validation loss: 2.0267295837402344

Epoch: 6| Step: 12
Training loss: 2.332613706588745
Validation loss: 2.0356903274854026

Epoch: 6| Step: 13
Training loss: 1.9346686601638794
Validation loss: 2.040899912516276

Epoch: 81| Step: 0
Training loss: 2.372020721435547
Validation loss: 2.0457316637039185

Epoch: 6| Step: 1
Training loss: 2.127410888671875
Validation loss: 2.0577890078226724

Epoch: 6| Step: 2
Training loss: 2.353187084197998
Validation loss: 2.063027282555898

Epoch: 6| Step: 3
Training loss: 2.2918286323547363
Validation loss: 2.0718844731648765

Epoch: 6| Step: 4
Training loss: 1.73894464969635
Validation loss: 2.077471435070038

Epoch: 6| Step: 5
Training loss: 2.188549518585205
Validation loss: 2.073227902253469

Epoch: 6| Step: 6
Training loss: 2.086897850036621
Validation loss: 2.0592907269795737

Epoch: 6| Step: 7
Training loss: 2.402407646179199
Validation loss: 2.035506029923757

Epoch: 6| Step: 8
Training loss: 2.44199275970459
Validation loss: 2.0242907206217446

Epoch: 6| Step: 9
Training loss: 2.3020946979522705
Validation loss: 2.0082186659177146

Epoch: 6| Step: 10
Training loss: 1.9231748580932617
Validation loss: 2.021802306175232

Epoch: 6| Step: 11
Training loss: 2.4808478355407715
Validation loss: 2.023549795150757

Epoch: 6| Step: 12
Training loss: 1.8946514129638672
Validation loss: 2.0341181556383767

Epoch: 6| Step: 13
Training loss: 2.325549840927124
Validation loss: 2.039908846219381

Epoch: 82| Step: 0
Training loss: 1.9519613981246948
Validation loss: 2.0417507688204446

Epoch: 6| Step: 1
Training loss: 1.8998682498931885
Validation loss: 2.040575623512268

Epoch: 6| Step: 2
Training loss: 1.8555389642715454
Validation loss: 2.044058541456858

Epoch: 6| Step: 3
Training loss: 1.8502616882324219
Validation loss: 2.04574845234553

Epoch: 6| Step: 4
Training loss: 2.80098819732666
Validation loss: 2.052407741546631

Epoch: 6| Step: 5
Training loss: 2.445712089538574
Validation loss: 2.0458545883496604

Epoch: 6| Step: 6
Training loss: 1.629185676574707
Validation loss: 2.0481165051460266

Epoch: 6| Step: 7
Training loss: 2.295271635055542
Validation loss: 2.0502339005470276

Epoch: 6| Step: 8
Training loss: 2.399484634399414
Validation loss: 2.0486090580622354

Epoch: 6| Step: 9
Training loss: 2.107574224472046
Validation loss: 2.049320856730143

Epoch: 6| Step: 10
Training loss: 2.965521812438965
Validation loss: 2.044913172721863

Epoch: 6| Step: 11
Training loss: 2.1841866970062256
Validation loss: 2.042596975962321

Epoch: 6| Step: 12
Training loss: 2.2627310752868652
Validation loss: 2.0388131737709045

Epoch: 6| Step: 13
Training loss: 2.3870372772216797
Validation loss: 2.0468326807022095

Epoch: 83| Step: 0
Training loss: 2.029672145843506
Validation loss: 2.0384976069132485

Epoch: 6| Step: 1
Training loss: 2.14892578125
Validation loss: 2.0294719338417053

Epoch: 6| Step: 2
Training loss: 1.8606480360031128
Validation loss: 2.0291423002878823

Epoch: 6| Step: 3
Training loss: 2.58345890045166
Validation loss: 2.018559376398722

Epoch: 6| Step: 4
Training loss: 2.1405091285705566
Validation loss: 2.015496611595154

Epoch: 6| Step: 5
Training loss: 2.7511022090911865
Validation loss: 2.0040900309880576

Epoch: 6| Step: 6
Training loss: 1.4888935089111328
Validation loss: 2.006163696448008

Epoch: 6| Step: 7
Training loss: 2.0284039974212646
Validation loss: 2.012596627076467

Epoch: 6| Step: 8
Training loss: 2.884787082672119
Validation loss: 2.0183345079421997

Epoch: 6| Step: 9
Training loss: 2.5307395458221436
Validation loss: 2.01862504084905

Epoch: 6| Step: 10
Training loss: 2.1394503116607666
Validation loss: 2.021507501602173

Epoch: 6| Step: 11
Training loss: 2.065439224243164
Validation loss: 2.0274632374445596

Epoch: 6| Step: 12
Training loss: 2.1640796661376953
Validation loss: 2.0309158762296042

Epoch: 6| Step: 13
Training loss: 1.4977234601974487
Validation loss: 2.027033885320028

Epoch: 84| Step: 0
Training loss: 2.3807172775268555
Validation loss: 2.04183300336202

Epoch: 6| Step: 1
Training loss: 1.9763025045394897
Validation loss: 2.0397658149401345

Epoch: 6| Step: 2
Training loss: 2.1424484252929688
Validation loss: 2.0370128750801086

Epoch: 6| Step: 3
Training loss: 2.0385079383850098
Validation loss: 2.0390814344088235

Epoch: 6| Step: 4
Training loss: 2.7806901931762695
Validation loss: 2.0433067282040915

Epoch: 6| Step: 5
Training loss: 2.812117099761963
Validation loss: 2.033233642578125

Epoch: 6| Step: 6
Training loss: 1.8105390071868896
Validation loss: 2.025428354740143

Epoch: 6| Step: 7
Training loss: 2.326780319213867
Validation loss: 2.020738502343496

Epoch: 6| Step: 8
Training loss: 2.2778544425964355
Validation loss: 2.0168667833010354

Epoch: 6| Step: 9
Training loss: 2.548654079437256
Validation loss: 2.018481453259786

Epoch: 6| Step: 10
Training loss: 2.1541810035705566
Validation loss: 2.0245142181714377

Epoch: 6| Step: 11
Training loss: 1.7555630207061768
Validation loss: 2.031793773174286

Epoch: 6| Step: 12
Training loss: 1.944840669631958
Validation loss: 2.0260504484176636

Epoch: 6| Step: 13
Training loss: 1.6731077432632446
Validation loss: 2.0197240114212036

Epoch: 85| Step: 0
Training loss: 2.685492753982544
Validation loss: 2.019364058971405

Epoch: 6| Step: 1
Training loss: 1.8524794578552246
Validation loss: 2.0202975471814475

Epoch: 6| Step: 2
Training loss: 2.3431079387664795
Validation loss: 2.012140154838562

Epoch: 6| Step: 3
Training loss: 2.195439577102661
Validation loss: 2.0162832339604697

Epoch: 6| Step: 4
Training loss: 2.3819143772125244
Validation loss: 2.0079463919003806

Epoch: 6| Step: 5
Training loss: 1.6035676002502441
Validation loss: 2.01734850804011

Epoch: 6| Step: 6
Training loss: 2.1252472400665283
Validation loss: 2.0231061776479087

Epoch: 6| Step: 7
Training loss: 1.9379417896270752
Validation loss: 2.013646682103475

Epoch: 6| Step: 8
Training loss: 1.3120521306991577
Validation loss: 2.018458306789398

Epoch: 6| Step: 9
Training loss: 2.6936111450195312
Validation loss: 2.0144371390342712

Epoch: 6| Step: 10
Training loss: 1.7525150775909424
Validation loss: 2.0121087431907654

Epoch: 6| Step: 11
Training loss: 2.620844841003418
Validation loss: 2.0088624556859336

Epoch: 6| Step: 12
Training loss: 2.3883237838745117
Validation loss: 2.0159698526064553

Epoch: 6| Step: 13
Training loss: 2.2727818489074707
Validation loss: 2.0111080010732016

Epoch: 86| Step: 0
Training loss: 2.392690658569336
Validation loss: 2.0192831556002298

Epoch: 6| Step: 1
Training loss: 2.2636051177978516
Validation loss: 2.014620085557302

Epoch: 6| Step: 2
Training loss: 1.9488506317138672
Validation loss: 2.0137322346369424

Epoch: 6| Step: 3
Training loss: 1.9777019023895264
Validation loss: 2.0163317918777466

Epoch: 6| Step: 4
Training loss: 1.68816339969635
Validation loss: 2.0213622053464255

Epoch: 6| Step: 5
Training loss: 1.7803211212158203
Validation loss: 2.0177732706069946

Epoch: 6| Step: 6
Training loss: 2.1738035678863525
Validation loss: 2.017589350541433

Epoch: 6| Step: 7
Training loss: 1.8593859672546387
Validation loss: 2.027180095513662

Epoch: 6| Step: 8
Training loss: 2.5380196571350098
Validation loss: 2.0177786350250244

Epoch: 6| Step: 9
Training loss: 2.7152652740478516
Validation loss: 2.013290047645569

Epoch: 6| Step: 10
Training loss: 2.687744140625
Validation loss: 2.0157747864723206

Epoch: 6| Step: 11
Training loss: 2.3049840927124023
Validation loss: 2.024982770284017

Epoch: 6| Step: 12
Training loss: 1.7976887226104736
Validation loss: 2.019837200641632

Epoch: 6| Step: 13
Training loss: 1.9941303730010986
Validation loss: 2.0195520917574563

Epoch: 87| Step: 0
Training loss: 2.2515392303466797
Validation loss: 2.021969497203827

Epoch: 6| Step: 1
Training loss: 2.2046053409576416
Validation loss: 2.0166534582773843

Epoch: 6| Step: 2
Training loss: 2.7531685829162598
Validation loss: 2.01926326751709

Epoch: 6| Step: 3
Training loss: 2.136382818222046
Validation loss: 2.0318172375361123

Epoch: 6| Step: 4
Training loss: 2.4582643508911133
Validation loss: 2.030471980571747

Epoch: 6| Step: 5
Training loss: 2.2960548400878906
Validation loss: 2.0291754603385925

Epoch: 6| Step: 6
Training loss: 2.168914318084717
Validation loss: 2.023298442363739

Epoch: 6| Step: 7
Training loss: 2.1831164360046387
Validation loss: 2.0285892486572266

Epoch: 6| Step: 8
Training loss: 2.3868966102600098
Validation loss: 2.018294334411621

Epoch: 6| Step: 9
Training loss: 1.7222399711608887
Validation loss: 2.0183645486831665

Epoch: 6| Step: 10
Training loss: 1.7254613637924194
Validation loss: 2.0276217261950173

Epoch: 6| Step: 11
Training loss: 2.1052541732788086
Validation loss: 2.0269270737965903

Epoch: 6| Step: 12
Training loss: 1.6865637302398682
Validation loss: 2.0301320950190225

Epoch: 6| Step: 13
Training loss: 1.9726829528808594
Validation loss: 2.0144241650899253

Epoch: 88| Step: 0
Training loss: 2.025381565093994
Validation loss: 2.0168906450271606

Epoch: 6| Step: 1
Training loss: 1.9403263330459595
Validation loss: 2.022566000620524

Epoch: 6| Step: 2
Training loss: 2.4482312202453613
Validation loss: 2.030391057332357

Epoch: 6| Step: 3
Training loss: 1.957511067390442
Validation loss: 2.023926059405009

Epoch: 6| Step: 4
Training loss: 1.5736818313598633
Validation loss: 2.0266621708869934

Epoch: 6| Step: 5
Training loss: 2.0003793239593506
Validation loss: 2.028639316558838

Epoch: 6| Step: 6
Training loss: 2.0716214179992676
Validation loss: 2.0185312231381736

Epoch: 6| Step: 7
Training loss: 2.4022250175476074
Validation loss: 2.033961057662964

Epoch: 6| Step: 8
Training loss: 2.495521306991577
Validation loss: 2.0237695574760437

Epoch: 6| Step: 9
Training loss: 1.8722243309020996
Validation loss: 2.032643675804138

Epoch: 6| Step: 10
Training loss: 2.311621904373169
Validation loss: 2.0236500898996987

Epoch: 6| Step: 11
Training loss: 2.6358752250671387
Validation loss: 2.019074817498525

Epoch: 6| Step: 12
Training loss: 1.830726981163025
Validation loss: 2.0162833531697593

Epoch: 6| Step: 13
Training loss: 2.3992810249328613
Validation loss: 2.0124786297480264

Epoch: 89| Step: 0
Training loss: 2.449087142944336
Validation loss: 2.0117327769597373

Epoch: 6| Step: 1
Training loss: 1.97152841091156
Validation loss: 2.023526748021444

Epoch: 6| Step: 2
Training loss: 2.0202765464782715
Validation loss: 2.024720072746277

Epoch: 6| Step: 3
Training loss: 2.6589794158935547
Validation loss: 2.0219125151634216

Epoch: 6| Step: 4
Training loss: 2.11415958404541
Validation loss: 2.0264982183774314

Epoch: 6| Step: 5
Training loss: 1.6732196807861328
Validation loss: 2.012689471244812

Epoch: 6| Step: 6
Training loss: 2.379487991333008
Validation loss: 2.01276703675588

Epoch: 6| Step: 7
Training loss: 2.1400516033172607
Validation loss: 2.014440735181173

Epoch: 6| Step: 8
Training loss: 2.251610279083252
Validation loss: 2.0247772932052612

Epoch: 6| Step: 9
Training loss: 1.8320884704589844
Validation loss: 2.020845611890157

Epoch: 6| Step: 10
Training loss: 2.391118288040161
Validation loss: 2.0242629051208496

Epoch: 6| Step: 11
Training loss: 2.4197018146514893
Validation loss: 2.0244398514429727

Epoch: 6| Step: 12
Training loss: 1.975993275642395
Validation loss: 2.030767500400543

Epoch: 6| Step: 13
Training loss: 1.9742132425308228
Validation loss: 2.023291031519572

Epoch: 90| Step: 0
Training loss: 2.07370662689209
Validation loss: 2.0260148644447327

Epoch: 6| Step: 1
Training loss: 2.586041212081909
Validation loss: 2.0202644069989524

Epoch: 6| Step: 2
Training loss: 1.7275334596633911
Validation loss: 2.0207276344299316

Epoch: 6| Step: 3
Training loss: 2.207770347595215
Validation loss: 2.021503229935964

Epoch: 6| Step: 4
Training loss: 2.7458906173706055
Validation loss: 2.0208237568537393

Epoch: 6| Step: 5
Training loss: 1.9280970096588135
Validation loss: 2.025475104649862

Epoch: 6| Step: 6
Training loss: 2.398008346557617
Validation loss: 2.022331158320109

Epoch: 6| Step: 7
Training loss: 2.334963083267212
Validation loss: 2.0243130326271057

Epoch: 6| Step: 8
Training loss: 1.9067180156707764
Validation loss: 2.0143824021021524

Epoch: 6| Step: 9
Training loss: 2.102552890777588
Validation loss: 2.0281251470247903

Epoch: 6| Step: 10
Training loss: 1.962985634803772
Validation loss: 2.0305957794189453

Epoch: 6| Step: 11
Training loss: 2.2833828926086426
Validation loss: 2.0349454283714294

Epoch: 6| Step: 12
Training loss: 2.08185076713562
Validation loss: 2.0314786632855735

Epoch: 6| Step: 13
Training loss: 1.5341212749481201
Validation loss: 2.024989147981008

Epoch: 91| Step: 0
Training loss: 2.1459994316101074
Validation loss: 2.0293824275334678

Epoch: 6| Step: 1
Training loss: 2.757772445678711
Validation loss: 2.032241622606913

Epoch: 6| Step: 2
Training loss: 2.200706720352173
Validation loss: 2.0188331405321756

Epoch: 6| Step: 3
Training loss: 2.127890110015869
Validation loss: 2.02380379041036

Epoch: 6| Step: 4
Training loss: 2.4677066802978516
Validation loss: 2.017229974269867

Epoch: 6| Step: 5
Training loss: 1.9946528673171997
Validation loss: 2.0095585783322654

Epoch: 6| Step: 6
Training loss: 2.137808322906494
Validation loss: 2.014016409715017

Epoch: 6| Step: 7
Training loss: 1.9230608940124512
Validation loss: 2.015619933605194

Epoch: 6| Step: 8
Training loss: 2.0846595764160156
Validation loss: 2.016197999318441

Epoch: 6| Step: 9
Training loss: 2.3923983573913574
Validation loss: 2.0158786376317344

Epoch: 6| Step: 10
Training loss: 1.3194539546966553
Validation loss: 2.0127884348233542

Epoch: 6| Step: 11
Training loss: 1.8147530555725098
Validation loss: 2.0135274728139243

Epoch: 6| Step: 12
Training loss: 2.1253061294555664
Validation loss: 2.0103642543156943

Epoch: 6| Step: 13
Training loss: 2.645909309387207
Validation loss: 2.0187508861223855

Epoch: 92| Step: 0
Training loss: 1.8166288137435913
Validation loss: 2.023760179678599

Epoch: 6| Step: 1
Training loss: 2.0592637062072754
Validation loss: 2.0200710892677307

Epoch: 6| Step: 2
Training loss: 2.2640016078948975
Validation loss: 2.032522439956665

Epoch: 6| Step: 3
Training loss: 1.9048994779586792
Validation loss: 2.0288782914479575

Epoch: 6| Step: 4
Training loss: 1.9059791564941406
Validation loss: 2.0360442996025085

Epoch: 6| Step: 5
Training loss: 2.3061907291412354
Validation loss: 2.0411160588264465

Epoch: 6| Step: 6
Training loss: 2.251307487487793
Validation loss: 2.0326395829518638

Epoch: 6| Step: 7
Training loss: 2.167574644088745
Validation loss: 2.029377520084381

Epoch: 6| Step: 8
Training loss: 2.2546916007995605
Validation loss: 2.0259753465652466

Epoch: 6| Step: 9
Training loss: 2.4542031288146973
Validation loss: 2.0168334245681763

Epoch: 6| Step: 10
Training loss: 1.7868379354476929
Validation loss: 2.032604912916819

Epoch: 6| Step: 11
Training loss: 1.8862688541412354
Validation loss: 2.0218507448832193

Epoch: 6| Step: 12
Training loss: 2.5428147315979004
Validation loss: 2.0153539578119912

Epoch: 6| Step: 13
Training loss: 2.3579587936401367
Validation loss: 2.0154733657836914

Epoch: 93| Step: 0
Training loss: 1.7953729629516602
Validation loss: 2.0115846991539

Epoch: 6| Step: 1
Training loss: 2.1495914459228516
Validation loss: 2.0157377123832703

Epoch: 6| Step: 2
Training loss: 2.1288459300994873
Validation loss: 2.0127106507619223

Epoch: 6| Step: 3
Training loss: 2.1614627838134766
Validation loss: 2.0073708097139993

Epoch: 6| Step: 4
Training loss: 1.9840980768203735
Validation loss: 2.0171205004056296

Epoch: 6| Step: 5
Training loss: 2.501984119415283
Validation loss: 2.011305550734202

Epoch: 6| Step: 6
Training loss: 2.2975611686706543
Validation loss: 2.0160631934801736

Epoch: 6| Step: 7
Training loss: 1.7249314785003662
Validation loss: 2.016936202843984

Epoch: 6| Step: 8
Training loss: 1.9996432065963745
Validation loss: 2.014776627222697

Epoch: 6| Step: 9
Training loss: 2.8110930919647217
Validation loss: 2.0211913188298545

Epoch: 6| Step: 10
Training loss: 2.00459623336792
Validation loss: 2.0196266770362854

Epoch: 6| Step: 11
Training loss: 2.0089101791381836
Validation loss: 2.03533136844635

Epoch: 6| Step: 12
Training loss: 2.0975289344787598
Validation loss: 2.0229078928629556

Epoch: 6| Step: 13
Training loss: 2.3664045333862305
Validation loss: 2.038928965727488

Epoch: 94| Step: 0
Training loss: 2.7707881927490234
Validation loss: 2.053439974784851

Epoch: 6| Step: 1
Training loss: 2.147646903991699
Validation loss: 2.051145315170288

Epoch: 6| Step: 2
Training loss: 2.0187220573425293
Validation loss: 2.037386735280355

Epoch: 6| Step: 3
Training loss: 1.6004955768585205
Validation loss: 2.038923362890879

Epoch: 6| Step: 4
Training loss: 1.9957842826843262
Validation loss: 2.035720467567444

Epoch: 6| Step: 5
Training loss: 2.5110421180725098
Validation loss: 2.0268592039744058

Epoch: 6| Step: 6
Training loss: 2.299078941345215
Validation loss: 2.0203481713930764

Epoch: 6| Step: 7
Training loss: 2.6001503467559814
Validation loss: 2.019586702187856

Epoch: 6| Step: 8
Training loss: 1.8361499309539795
Validation loss: 2.022114018599192

Epoch: 6| Step: 9
Training loss: 1.7637555599212646
Validation loss: 2.0202654600143433

Epoch: 6| Step: 10
Training loss: 2.191087245941162
Validation loss: 2.020336151123047

Epoch: 6| Step: 11
Training loss: 1.9677479267120361
Validation loss: 2.018332521120707

Epoch: 6| Step: 12
Training loss: 2.3391494750976562
Validation loss: 2.022307892640432

Epoch: 6| Step: 13
Training loss: 2.050565719604492
Validation loss: 2.013411740461985

Epoch: 95| Step: 0
Training loss: 2.023703098297119
Validation loss: 2.0168174107869468

Epoch: 6| Step: 1
Training loss: 2.1276164054870605
Validation loss: 2.012035151322683

Epoch: 6| Step: 2
Training loss: 1.892322063446045
Validation loss: 2.017794648806254

Epoch: 6| Step: 3
Training loss: 1.9463003873825073
Validation loss: 2.0115114053090415

Epoch: 6| Step: 4
Training loss: 2.411069869995117
Validation loss: 2.0136744379997253

Epoch: 6| Step: 5
Training loss: 2.367478132247925
Validation loss: 2.011988361676534

Epoch: 6| Step: 6
Training loss: 1.9207680225372314
Validation loss: 2.0068286259969077

Epoch: 6| Step: 7
Training loss: 2.189366340637207
Validation loss: 2.018928031126658

Epoch: 6| Step: 8
Training loss: 2.3516178131103516
Validation loss: 2.0157599449157715

Epoch: 6| Step: 9
Training loss: 2.0745224952697754
Validation loss: 2.010990937550863

Epoch: 6| Step: 10
Training loss: 1.95973801612854
Validation loss: 2.013183295726776

Epoch: 6| Step: 11
Training loss: 2.0224545001983643
Validation loss: 2.0133898655573526

Epoch: 6| Step: 12
Training loss: 2.240200996398926
Validation loss: 2.013140161832174

Epoch: 6| Step: 13
Training loss: 2.475870132446289
Validation loss: 2.022256910800934

Epoch: 96| Step: 0
Training loss: 1.6194908618927002
Validation loss: 2.027144511540731

Epoch: 6| Step: 1
Training loss: 2.0702998638153076
Validation loss: 2.030978560447693

Epoch: 6| Step: 2
Training loss: 2.744112253189087
Validation loss: 2.0325739781061807

Epoch: 6| Step: 3
Training loss: 1.493077039718628
Validation loss: 2.0361268321673074

Epoch: 6| Step: 4
Training loss: 2.14325213432312
Validation loss: 2.0424821774164834

Epoch: 6| Step: 5
Training loss: 1.6194595098495483
Validation loss: 2.0465282201766968

Epoch: 6| Step: 6
Training loss: 2.95287823677063
Validation loss: 2.053160846233368

Epoch: 6| Step: 7
Training loss: 2.671525478363037
Validation loss: 2.0422950983047485

Epoch: 6| Step: 8
Training loss: 2.395164966583252
Validation loss: 2.0306997299194336

Epoch: 6| Step: 9
Training loss: 2.0673937797546387
Validation loss: 2.031246324380239

Epoch: 6| Step: 10
Training loss: 2.16965389251709
Validation loss: 2.0284133752187095

Epoch: 6| Step: 11
Training loss: 2.0104708671569824
Validation loss: 2.0199817220369973

Epoch: 6| Step: 12
Training loss: 1.44649076461792
Validation loss: 2.0141414801279702

Epoch: 6| Step: 13
Training loss: 2.518707275390625
Validation loss: 2.0137847463289895

Epoch: 97| Step: 0
Training loss: 1.9922552108764648
Validation loss: 2.014424721399943

Epoch: 6| Step: 1
Training loss: 2.5132007598876953
Validation loss: 2.011816759904226

Epoch: 6| Step: 2
Training loss: 2.666728973388672
Validation loss: 2.015740911165873

Epoch: 6| Step: 3
Training loss: 1.941645622253418
Validation loss: 2.017553726832072

Epoch: 6| Step: 4
Training loss: 2.396402359008789
Validation loss: 2.0227693915367126

Epoch: 6| Step: 5
Training loss: 2.5715789794921875
Validation loss: 2.027565598487854

Epoch: 6| Step: 6
Training loss: 1.8895277976989746
Validation loss: 2.0250261227289834

Epoch: 6| Step: 7
Training loss: 3.002910614013672
Validation loss: 2.0203710397084556

Epoch: 6| Step: 8
Training loss: 1.6845321655273438
Validation loss: 2.0293283065160117

Epoch: 6| Step: 9
Training loss: 2.115246295928955
Validation loss: 2.0220850110054016

Epoch: 6| Step: 10
Training loss: 1.7235331535339355
Validation loss: 2.019898454348246

Epoch: 6| Step: 11
Training loss: 2.097808361053467
Validation loss: 2.016750454902649

Epoch: 6| Step: 12
Training loss: 1.9802483320236206
Validation loss: 2.0125705003738403

Epoch: 6| Step: 13
Training loss: 1.5698364973068237
Validation loss: 2.0126320719718933

Epoch: 98| Step: 0
Training loss: 1.7792460918426514
Validation loss: 2.015278975168864

Epoch: 6| Step: 1
Training loss: 1.8700370788574219
Validation loss: 2.0191624959309897

Epoch: 6| Step: 2
Training loss: 1.7542041540145874
Validation loss: 2.0262971917788186

Epoch: 6| Step: 3
Training loss: 2.456784248352051
Validation loss: 2.041944940884908

Epoch: 6| Step: 4
Training loss: 1.6975786685943604
Validation loss: 2.055412153402964

Epoch: 6| Step: 5
Training loss: 2.162241220474243
Validation loss: 2.0636159578959146

Epoch: 6| Step: 6
Training loss: 2.5232245922088623
Validation loss: 2.0584489504496255

Epoch: 6| Step: 7
Training loss: 1.9513474702835083
Validation loss: 2.0565179586410522

Epoch: 6| Step: 8
Training loss: 2.257840156555176
Validation loss: 2.050215740998586

Epoch: 6| Step: 9
Training loss: 2.224637985229492
Validation loss: 2.04230926434199

Epoch: 6| Step: 10
Training loss: 2.045973062515259
Validation loss: 2.033632278442383

Epoch: 6| Step: 11
Training loss: 2.3898258209228516
Validation loss: 2.022172530492147

Epoch: 6| Step: 12
Training loss: 2.618351459503174
Validation loss: 2.0135446588198342

Epoch: 6| Step: 13
Training loss: 2.583232879638672
Validation loss: 2.008498430252075

Epoch: 99| Step: 0
Training loss: 2.120529890060425
Validation loss: 2.012226104736328

Epoch: 6| Step: 1
Training loss: 2.7003371715545654
Validation loss: 2.009037415186564

Epoch: 6| Step: 2
Training loss: 2.5252232551574707
Validation loss: 2.011557956536611

Epoch: 6| Step: 3
Training loss: 1.8393906354904175
Validation loss: 2.0101928313573203

Epoch: 6| Step: 4
Training loss: 1.7970786094665527
Validation loss: 2.012701451778412

Epoch: 6| Step: 5
Training loss: 1.6887928247451782
Validation loss: 2.013626058896383

Epoch: 6| Step: 6
Training loss: 2.234849691390991
Validation loss: 2.017353057861328

Epoch: 6| Step: 7
Training loss: 2.288630962371826
Validation loss: 2.0182423194249473

Epoch: 6| Step: 8
Training loss: 1.9518786668777466
Validation loss: 2.0196971893310547

Epoch: 6| Step: 9
Training loss: 2.2226881980895996
Validation loss: 2.017785211404165

Epoch: 6| Step: 10
Training loss: 1.9601861238479614
Validation loss: 2.016326685746511

Epoch: 6| Step: 11
Training loss: 2.5777626037597656
Validation loss: 2.014547427495321

Epoch: 6| Step: 12
Training loss: 1.8327648639678955
Validation loss: 2.0117945472399392

Epoch: 6| Step: 13
Training loss: 2.1742210388183594
Validation loss: 2.0115848978360495

Epoch: 100| Step: 0
Training loss: 2.409015655517578
Validation loss: 2.0191877285639444

Epoch: 6| Step: 1
Training loss: 1.6873319149017334
Validation loss: 2.0093406438827515

Epoch: 6| Step: 2
Training loss: 2.466829776763916
Validation loss: 2.015121122201284

Epoch: 6| Step: 3
Training loss: 2.333902359008789
Validation loss: 2.02400940656662

Epoch: 6| Step: 4
Training loss: 2.2730963230133057
Validation loss: 2.0148402055104575

Epoch: 6| Step: 5
Training loss: 2.918923854827881
Validation loss: 2.020324488480886

Epoch: 6| Step: 6
Training loss: 2.5258302688598633
Validation loss: 2.0175106724103293

Epoch: 6| Step: 7
Training loss: 1.8216266632080078
Validation loss: 2.0197596351305642

Epoch: 6| Step: 8
Training loss: 2.337754487991333
Validation loss: 2.030031422773997

Epoch: 6| Step: 9
Training loss: 1.8617806434631348
Validation loss: 2.032547970612844

Epoch: 6| Step: 10
Training loss: 1.975637674331665
Validation loss: 2.0232193867365518

Epoch: 6| Step: 11
Training loss: 2.088561773300171
Validation loss: 2.025087575117747

Epoch: 6| Step: 12
Training loss: 1.4082860946655273
Validation loss: 2.0308684508005777

Epoch: 6| Step: 13
Training loss: 1.5456264019012451
Validation loss: 2.030503511428833

Epoch: 101| Step: 0
Training loss: 1.622572422027588
Validation loss: 2.028562545776367

Epoch: 6| Step: 1
Training loss: 2.118471622467041
Validation loss: 2.0347445408503213

Epoch: 6| Step: 2
Training loss: 1.5918564796447754
Validation loss: 2.0393534898757935

Epoch: 6| Step: 3
Training loss: 1.882481575012207
Validation loss: 2.042186439037323

Epoch: 6| Step: 4
Training loss: 2.3499755859375
Validation loss: 2.0406621297200522

Epoch: 6| Step: 5
Training loss: 1.9936200380325317
Validation loss: 2.0448424418767295

Epoch: 6| Step: 6
Training loss: 2.43046236038208
Validation loss: 2.0440974632898965

Epoch: 6| Step: 7
Training loss: 1.8019156455993652
Validation loss: 2.03376833597819

Epoch: 6| Step: 8
Training loss: 2.160742998123169
Validation loss: 2.0404235124588013

Epoch: 6| Step: 9
Training loss: 2.0911569595336914
Validation loss: 2.025004585584005

Epoch: 6| Step: 10
Training loss: 2.5240650177001953
Validation loss: 2.027940034866333

Epoch: 6| Step: 11
Training loss: 2.021761655807495
Validation loss: 2.0259419480959573

Epoch: 6| Step: 12
Training loss: 2.375627279281616
Validation loss: 2.025380770365397

Epoch: 6| Step: 13
Training loss: 2.6477208137512207
Validation loss: 2.0188114841779075

Epoch: 102| Step: 0
Training loss: 2.1301681995391846
Validation loss: 2.0224872628847756

Epoch: 6| Step: 1
Training loss: 2.3295340538024902
Validation loss: 2.015531301498413

Epoch: 6| Step: 2
Training loss: 2.3543314933776855
Validation loss: 2.01971964041392

Epoch: 6| Step: 3
Training loss: 1.6562038660049438
Validation loss: 2.0159913301467896

Epoch: 6| Step: 4
Training loss: 1.9251086711883545
Validation loss: 2.019146740436554

Epoch: 6| Step: 5
Training loss: 2.0057995319366455
Validation loss: 2.0241130789120994

Epoch: 6| Step: 6
Training loss: 1.9375004768371582
Validation loss: 2.025441328684489

Epoch: 6| Step: 7
Training loss: 1.9789124727249146
Validation loss: 2.0237427949905396

Epoch: 6| Step: 8
Training loss: 2.776554584503174
Validation loss: 2.030431012312571

Epoch: 6| Step: 9
Training loss: 1.5696768760681152
Validation loss: 2.017294387022654

Epoch: 6| Step: 10
Training loss: 1.6173207759857178
Validation loss: 2.029929220676422

Epoch: 6| Step: 11
Training loss: 2.650195360183716
Validation loss: 2.045959194501241

Epoch: 6| Step: 12
Training loss: 2.2782158851623535
Validation loss: 2.0454026659329734

Epoch: 6| Step: 13
Training loss: 2.559366464614868
Validation loss: 2.0454726219177246

Epoch: 103| Step: 0
Training loss: 2.6014599800109863
Validation loss: 2.041069487730662

Epoch: 6| Step: 1
Training loss: 1.7154401540756226
Validation loss: 2.0467836459477744

Epoch: 6| Step: 2
Training loss: 1.9102617502212524
Validation loss: 2.0458261569341025

Epoch: 6| Step: 3
Training loss: 2.3728737831115723
Validation loss: 2.037067453066508

Epoch: 6| Step: 4
Training loss: 2.5449275970458984
Validation loss: 2.0325058499972024

Epoch: 6| Step: 5
Training loss: 1.8371089696884155
Validation loss: 2.0351677735646567

Epoch: 6| Step: 6
Training loss: 2.174099922180176
Validation loss: 2.0312660932540894

Epoch: 6| Step: 7
Training loss: 2.030928134918213
Validation loss: 2.0278796354929605

Epoch: 6| Step: 8
Training loss: 1.647348403930664
Validation loss: 2.0321825742721558

Epoch: 6| Step: 9
Training loss: 2.4221439361572266
Validation loss: 2.0313581426938376

Epoch: 6| Step: 10
Training loss: 2.2749900817871094
Validation loss: 2.038383404413859

Epoch: 6| Step: 11
Training loss: 2.5114235877990723
Validation loss: 2.0382495323816934

Epoch: 6| Step: 12
Training loss: 2.1735639572143555
Validation loss: 2.0272309978803

Epoch: 6| Step: 13
Training loss: 1.4162952899932861
Validation loss: 2.0253446102142334

Epoch: 104| Step: 0
Training loss: 2.284213066101074
Validation loss: 2.0252702832221985

Epoch: 6| Step: 1
Training loss: 2.0494275093078613
Validation loss: 2.0244287848472595

Epoch: 6| Step: 2
Training loss: 2.1046016216278076
Validation loss: 2.0253612995147705

Epoch: 6| Step: 3
Training loss: 2.422454833984375
Validation loss: 2.022180994351705

Epoch: 6| Step: 4
Training loss: 1.475833773612976
Validation loss: 2.0259355703989663

Epoch: 6| Step: 5
Training loss: 2.260096549987793
Validation loss: 2.027473211288452

Epoch: 6| Step: 6
Training loss: 2.0717644691467285
Validation loss: 2.0236147046089172

Epoch: 6| Step: 7
Training loss: 1.804809331893921
Validation loss: 2.0255712270736694

Epoch: 6| Step: 8
Training loss: 2.5140624046325684
Validation loss: 2.0265154043833413

Epoch: 6| Step: 9
Training loss: 2.568739175796509
Validation loss: 2.0263803203900657

Epoch: 6| Step: 10
Training loss: 2.0634050369262695
Validation loss: 2.0238792101542153

Epoch: 6| Step: 11
Training loss: 1.9810864925384521
Validation loss: 2.0199955304463706

Epoch: 6| Step: 12
Training loss: 1.9139596223831177
Validation loss: 2.032524287700653

Epoch: 6| Step: 13
Training loss: 2.1720337867736816
Validation loss: 2.035704731941223

Epoch: 105| Step: 0
Training loss: 1.5369724035263062
Validation loss: 2.0356702407201133

Epoch: 6| Step: 1
Training loss: 1.914430856704712
Validation loss: 2.0273646910985312

Epoch: 6| Step: 2
Training loss: 1.6815564632415771
Validation loss: 2.032707154750824

Epoch: 6| Step: 3
Training loss: 1.8029600381851196
Validation loss: 2.0199883778889975

Epoch: 6| Step: 4
Training loss: 2.2698287963867188
Validation loss: 2.038714349269867

Epoch: 6| Step: 5
Training loss: 1.474029779434204
Validation loss: 2.033076504866282

Epoch: 6| Step: 6
Training loss: 1.991053819656372
Validation loss: 2.024409910043081

Epoch: 6| Step: 7
Training loss: 1.9440003633499146
Validation loss: 2.0323648850123086

Epoch: 6| Step: 8
Training loss: 2.443549156188965
Validation loss: 2.028941591580709

Epoch: 6| Step: 9
Training loss: 2.4349188804626465
Validation loss: 2.040317157904307

Epoch: 6| Step: 10
Training loss: 2.0768213272094727
Validation loss: 2.0260175267855325

Epoch: 6| Step: 11
Training loss: 2.2011873722076416
Validation loss: 2.021818995475769

Epoch: 6| Step: 12
Training loss: 2.9251856803894043
Validation loss: 2.017971316973368

Epoch: 6| Step: 13
Training loss: 2.719541549682617
Validation loss: 2.030356228351593

Epoch: 106| Step: 0
Training loss: 2.171535015106201
Validation loss: 2.019046207269033

Epoch: 6| Step: 1
Training loss: 2.549180030822754
Validation loss: 2.0236053466796875

Epoch: 6| Step: 2
Training loss: 2.3259928226470947
Validation loss: 2.024178902308146

Epoch: 6| Step: 3
Training loss: 2.642400026321411
Validation loss: 2.0209399263064065

Epoch: 6| Step: 4
Training loss: 1.6125675439834595
Validation loss: 2.0266592303911843

Epoch: 6| Step: 5
Training loss: 2.502048969268799
Validation loss: 2.0226628184318542

Epoch: 6| Step: 6
Training loss: 2.1603164672851562
Validation loss: 2.026375710964203

Epoch: 6| Step: 7
Training loss: 2.2831830978393555
Validation loss: 2.017506241798401

Epoch: 6| Step: 8
Training loss: 2.052980661392212
Validation loss: 2.027150650819143

Epoch: 6| Step: 9
Training loss: 1.6809920072555542
Validation loss: 2.034226139386495

Epoch: 6| Step: 10
Training loss: 1.617464303970337
Validation loss: 2.019853154818217

Epoch: 6| Step: 11
Training loss: 2.3059678077697754
Validation loss: 2.0314812660217285

Epoch: 6| Step: 12
Training loss: 2.1588287353515625
Validation loss: 2.039503514766693

Epoch: 6| Step: 13
Training loss: 1.5267305374145508
Validation loss: 2.0436776280403137

Epoch: 107| Step: 0
Training loss: 2.132295846939087
Validation loss: 2.0404759844144187

Epoch: 6| Step: 1
Training loss: 1.6747816801071167
Validation loss: 2.044088065624237

Epoch: 6| Step: 2
Training loss: 1.570350170135498
Validation loss: 2.0334752599398294

Epoch: 6| Step: 3
Training loss: 2.428567409515381
Validation loss: 2.017602880795797

Epoch: 6| Step: 4
Training loss: 1.9879143238067627
Validation loss: 2.024020771185557

Epoch: 6| Step: 5
Training loss: 1.466456651687622
Validation loss: 2.0185009439786277

Epoch: 6| Step: 6
Training loss: 2.2236320972442627
Validation loss: 2.030434509118398

Epoch: 6| Step: 7
Training loss: 2.2010605335235596
Validation loss: 2.0149492621421814

Epoch: 6| Step: 8
Training loss: 2.3536109924316406
Validation loss: 2.0241686503092446

Epoch: 6| Step: 9
Training loss: 1.88433837890625
Validation loss: 2.024802049001058

Epoch: 6| Step: 10
Training loss: 2.2116777896881104
Validation loss: 2.017408271630605

Epoch: 6| Step: 11
Training loss: 2.262474536895752
Validation loss: 2.0266245007514954

Epoch: 6| Step: 12
Training loss: 2.4327878952026367
Validation loss: 2.0262407263120017

Epoch: 6| Step: 13
Training loss: 2.671848773956299
Validation loss: 2.024654825528463

Epoch: 108| Step: 0
Training loss: 1.5254744291305542
Validation loss: 2.0249335765838623

Epoch: 6| Step: 1
Training loss: 1.6893928050994873
Validation loss: 2.0300377209981284

Epoch: 6| Step: 2
Training loss: 1.534769892692566
Validation loss: 2.029803474744161

Epoch: 6| Step: 3
Training loss: 2.725897789001465
Validation loss: 2.0326611399650574

Epoch: 6| Step: 4
Training loss: 2.1199803352355957
Validation loss: 2.0341305136680603

Epoch: 6| Step: 5
Training loss: 2.013270378112793
Validation loss: 2.0400527318318686

Epoch: 6| Step: 6
Training loss: 1.8188374042510986
Validation loss: 2.044296065966288

Epoch: 6| Step: 7
Training loss: 3.13649320602417
Validation loss: 2.0341140230496726

Epoch: 6| Step: 8
Training loss: 2.2500290870666504
Validation loss: 2.0323957800865173

Epoch: 6| Step: 9
Training loss: 2.047367572784424
Validation loss: 2.018880466620127

Epoch: 6| Step: 10
Training loss: 2.368556499481201
Validation loss: 2.017902374267578

Epoch: 6| Step: 11
Training loss: 1.9929497241973877
Validation loss: 2.02859894434611

Epoch: 6| Step: 12
Training loss: 2.6352791786193848
Validation loss: 2.0306243896484375

Epoch: 6| Step: 13
Training loss: 2.089615821838379
Validation loss: 2.0219080050786338

Epoch: 109| Step: 0
Training loss: 1.9897955656051636
Validation loss: 2.027220865090688

Epoch: 6| Step: 1
Training loss: 2.5215415954589844
Validation loss: 2.026434083779653

Epoch: 6| Step: 2
Training loss: 1.6498069763183594
Validation loss: 2.032061278820038

Epoch: 6| Step: 3
Training loss: 2.5742616653442383
Validation loss: 2.0340105295181274

Epoch: 6| Step: 4
Training loss: 1.6626285314559937
Validation loss: 2.035646438598633

Epoch: 6| Step: 5
Training loss: 2.82528018951416
Validation loss: 2.0338542461395264

Epoch: 6| Step: 6
Training loss: 2.111391544342041
Validation loss: 2.029317875703176

Epoch: 6| Step: 7
Training loss: 1.9091172218322754
Validation loss: 2.026533047358195

Epoch: 6| Step: 8
Training loss: 1.916456937789917
Validation loss: 2.0264161427815757

Epoch: 6| Step: 9
Training loss: 2.5375638008117676
Validation loss: 2.030343770980835

Epoch: 6| Step: 10
Training loss: 2.0161845684051514
Validation loss: 2.0254096587498984

Epoch: 6| Step: 11
Training loss: 1.6796544790267944
Validation loss: 2.032447894414266

Epoch: 6| Step: 12
Training loss: 1.9465773105621338
Validation loss: 2.030901630719503

Epoch: 6| Step: 13
Training loss: 2.2538084983825684
Validation loss: 2.032087961832682

Epoch: 110| Step: 0
Training loss: 1.8597004413604736
Validation loss: 2.044975519180298

Epoch: 6| Step: 1
Training loss: 2.4236245155334473
Validation loss: 2.036124070485433

Epoch: 6| Step: 2
Training loss: 1.8569560050964355
Validation loss: 2.035583237806956

Epoch: 6| Step: 3
Training loss: 2.033571243286133
Validation loss: 2.0414942105611167

Epoch: 6| Step: 4
Training loss: 2.4307448863983154
Validation loss: 2.032917618751526

Epoch: 6| Step: 5
Training loss: 1.9914060831069946
Validation loss: 2.0437862475713096

Epoch: 6| Step: 6
Training loss: 2.3259241580963135
Validation loss: 2.042869806289673

Epoch: 6| Step: 7
Training loss: 2.510467052459717
Validation loss: 2.04009077946345

Epoch: 6| Step: 8
Training loss: 1.5728282928466797
Validation loss: 2.037322680155436

Epoch: 6| Step: 9
Training loss: 2.169942617416382
Validation loss: 2.037363370259603

Epoch: 6| Step: 10
Training loss: 1.8413872718811035
Validation loss: 2.0294847885767617

Epoch: 6| Step: 11
Training loss: 1.9506680965423584
Validation loss: 2.034002979596456

Epoch: 6| Step: 12
Training loss: 1.8885661363601685
Validation loss: 2.0314473708470664

Epoch: 6| Step: 13
Training loss: 2.7598586082458496
Validation loss: 2.033013641834259

Epoch: 111| Step: 0
Training loss: 1.5692718029022217
Validation loss: 2.03247062365214

Epoch: 6| Step: 1
Training loss: 2.586881637573242
Validation loss: 2.024487574895223

Epoch: 6| Step: 2
Training loss: 1.7463082075119019
Validation loss: 2.02762101093928

Epoch: 6| Step: 3
Training loss: 1.7849740982055664
Validation loss: 2.0258901715278625

Epoch: 6| Step: 4
Training loss: 2.3693556785583496
Validation loss: 2.0360967914263406

Epoch: 6| Step: 5
Training loss: 2.3357629776000977
Validation loss: 2.0423065423965454

Epoch: 6| Step: 6
Training loss: 2.6674699783325195
Validation loss: 2.0418636004130044

Epoch: 6| Step: 7
Training loss: 2.1977829933166504
Validation loss: 2.0413679281870523

Epoch: 6| Step: 8
Training loss: 1.935937762260437
Validation loss: 2.0497252543767295

Epoch: 6| Step: 9
Training loss: 2.0373001098632812
Validation loss: 2.0533037185668945

Epoch: 6| Step: 10
Training loss: 2.2839317321777344
Validation loss: 2.0518517891565957

Epoch: 6| Step: 11
Training loss: 1.9153673648834229
Validation loss: 2.033608873685201

Epoch: 6| Step: 12
Training loss: 1.9977006912231445
Validation loss: 2.0362854997316995

Epoch: 6| Step: 13
Training loss: 2.273655891418457
Validation loss: 2.032879968484243

Epoch: 112| Step: 0
Training loss: 2.425813674926758
Validation loss: 2.0454511245091758

Epoch: 6| Step: 1
Training loss: 1.8681278228759766
Validation loss: 2.0308451652526855

Epoch: 6| Step: 2
Training loss: 2.336325168609619
Validation loss: 2.033523917198181

Epoch: 6| Step: 3
Training loss: 1.702453851699829
Validation loss: 2.035182694594065

Epoch: 6| Step: 4
Training loss: 2.131383180618286
Validation loss: 2.038520097732544

Epoch: 6| Step: 5
Training loss: 1.923220157623291
Validation loss: 2.0342965126037598

Epoch: 6| Step: 6
Training loss: 2.218014717102051
Validation loss: 2.0401076873143515

Epoch: 6| Step: 7
Training loss: 2.3603875637054443
Validation loss: 2.034199833869934

Epoch: 6| Step: 8
Training loss: 2.4340500831604004
Validation loss: 2.030356486638387

Epoch: 6| Step: 9
Training loss: 2.176100730895996
Validation loss: 2.027542233467102

Epoch: 6| Step: 10
Training loss: 1.8267648220062256
Validation loss: 2.026658058166504

Epoch: 6| Step: 11
Training loss: 2.1410446166992188
Validation loss: 2.0321985880533853

Epoch: 6| Step: 12
Training loss: 1.8866522312164307
Validation loss: 2.0317664543787637

Epoch: 6| Step: 13
Training loss: 2.455052375793457
Validation loss: 2.027674376964569

Epoch: 113| Step: 0
Training loss: 2.7745137214660645
Validation loss: 2.028696596622467

Epoch: 6| Step: 1
Training loss: 2.6324193477630615
Validation loss: 2.0286749402681985

Epoch: 6| Step: 2
Training loss: 2.21173357963562
Validation loss: 2.0255497694015503

Epoch: 6| Step: 3
Training loss: 2.7635507583618164
Validation loss: 2.0203458269437156

Epoch: 6| Step: 4
Training loss: 1.935686707496643
Validation loss: 2.0246596137682595

Epoch: 6| Step: 5
Training loss: 2.378025531768799
Validation loss: 2.0246858398119607

Epoch: 6| Step: 6
Training loss: 1.6114373207092285
Validation loss: 2.0263565381368003

Epoch: 6| Step: 7
Training loss: 1.8420202732086182
Validation loss: 2.0414703687032065

Epoch: 6| Step: 8
Training loss: 1.4578415155410767
Validation loss: 2.0440660317738852

Epoch: 6| Step: 9
Training loss: 2.5613784790039062
Validation loss: 2.051062901814779

Epoch: 6| Step: 10
Training loss: 2.245664119720459
Validation loss: 2.0464176138242087

Epoch: 6| Step: 11
Training loss: 1.9900732040405273
Validation loss: 2.035036047299703

Epoch: 6| Step: 12
Training loss: 1.7086591720581055
Validation loss: 2.0297244787216187

Epoch: 6| Step: 13
Training loss: 1.7964529991149902
Validation loss: 2.030958116054535

Epoch: 114| Step: 0
Training loss: 1.3945413827896118
Validation loss: 2.032696088155111

Epoch: 6| Step: 1
Training loss: 2.659106492996216
Validation loss: 2.0284900665283203

Epoch: 6| Step: 2
Training loss: 1.8225445747375488
Validation loss: 2.026697834332784

Epoch: 6| Step: 3
Training loss: 2.085418224334717
Validation loss: 2.0281145175298056

Epoch: 6| Step: 4
Training loss: 1.3915705680847168
Validation loss: 2.024120012919108

Epoch: 6| Step: 5
Training loss: 2.5730838775634766
Validation loss: 2.030269682407379

Epoch: 6| Step: 6
Training loss: 2.201188087463379
Validation loss: 2.015181561311086

Epoch: 6| Step: 7
Training loss: 2.182020425796509
Validation loss: 2.024712006251017

Epoch: 6| Step: 8
Training loss: 2.1279993057250977
Validation loss: 2.024253269036611

Epoch: 6| Step: 9
Training loss: 1.9603127241134644
Validation loss: 2.026797334353129

Epoch: 6| Step: 10
Training loss: 2.3342912197113037
Validation loss: 2.0209466417630515

Epoch: 6| Step: 11
Training loss: 2.599398612976074
Validation loss: 2.0234091679255166

Epoch: 6| Step: 12
Training loss: 2.1456499099731445
Validation loss: 2.0350856383641562

Epoch: 6| Step: 13
Training loss: 2.090329647064209
Validation loss: 2.0358624855677285

Epoch: 115| Step: 0
Training loss: 1.994107723236084
Validation loss: 2.0407868226369223

Epoch: 6| Step: 1
Training loss: 2.234323501586914
Validation loss: 2.0343032081921897

Epoch: 6| Step: 2
Training loss: 1.8264634609222412
Validation loss: 2.0441260933876038

Epoch: 6| Step: 3
Training loss: 1.6562163829803467
Validation loss: 2.0473993023236594

Epoch: 6| Step: 4
Training loss: 1.6097643375396729
Validation loss: 2.04385236899058

Epoch: 6| Step: 5
Training loss: 2.4070587158203125
Validation loss: 2.043410380681356

Epoch: 6| Step: 6
Training loss: 2.1563026905059814
Validation loss: 2.0442182620366416

Epoch: 6| Step: 7
Training loss: 3.0060875415802
Validation loss: 2.0365065534909568

Epoch: 6| Step: 8
Training loss: 2.5128142833709717
Validation loss: 2.0433648427327475

Epoch: 6| Step: 9
Training loss: 1.7731472253799438
Validation loss: 2.0391204953193665

Epoch: 6| Step: 10
Training loss: 2.5686588287353516
Validation loss: 2.0418978730837503

Epoch: 6| Step: 11
Training loss: 2.4841394424438477
Validation loss: 2.0427842338879905

Epoch: 6| Step: 12
Training loss: 1.4179896116256714
Validation loss: 2.0446135799090066

Epoch: 6| Step: 13
Training loss: 1.7446718215942383
Validation loss: 2.0411341985066733

Epoch: 116| Step: 0
Training loss: 1.32191002368927
Validation loss: 2.0439038475354514

Epoch: 6| Step: 1
Training loss: 2.928353786468506
Validation loss: 2.0377221504847207

Epoch: 6| Step: 2
Training loss: 2.652297019958496
Validation loss: 2.0412476857503257

Epoch: 6| Step: 3
Training loss: 2.180370330810547
Validation loss: 2.0379813512166343

Epoch: 6| Step: 4
Training loss: 2.0188355445861816
Validation loss: 2.0385709206263223

Epoch: 6| Step: 5
Training loss: 2.1921682357788086
Validation loss: 2.0368430813153586

Epoch: 6| Step: 6
Training loss: 1.9159002304077148
Validation loss: 2.035476724306742

Epoch: 6| Step: 7
Training loss: 1.5559781789779663
Validation loss: 2.0401323239008584

Epoch: 6| Step: 8
Training loss: 2.3292274475097656
Validation loss: 2.038592517375946

Epoch: 6| Step: 9
Training loss: 2.420215129852295
Validation loss: 2.0389296611150107

Epoch: 6| Step: 10
Training loss: 1.590352177619934
Validation loss: 2.031935135523478

Epoch: 6| Step: 11
Training loss: 2.367612361907959
Validation loss: 2.0372601747512817

Epoch: 6| Step: 12
Training loss: 2.435486078262329
Validation loss: 2.0385431249936423

Epoch: 6| Step: 13
Training loss: 1.886030912399292
Validation loss: 2.0380929509798684

Epoch: 117| Step: 0
Training loss: 1.679003357887268
Validation loss: 2.0350051522254944

Epoch: 6| Step: 1
Training loss: 1.7822099924087524
Validation loss: 2.0421972274780273

Epoch: 6| Step: 2
Training loss: 2.4661948680877686
Validation loss: 2.0514882802963257

Epoch: 6| Step: 3
Training loss: 2.4651620388031006
Validation loss: 2.0422479112943015

Epoch: 6| Step: 4
Training loss: 2.4108171463012695
Validation loss: 2.0281059940656028

Epoch: 6| Step: 5
Training loss: 1.8815131187438965
Validation loss: 2.042321821053823

Epoch: 6| Step: 6
Training loss: 1.815035104751587
Validation loss: 2.032286524772644

Epoch: 6| Step: 7
Training loss: 1.9784114360809326
Validation loss: 2.0459739168485007

Epoch: 6| Step: 8
Training loss: 2.259815216064453
Validation loss: 2.033235331376394

Epoch: 6| Step: 9
Training loss: 1.3378024101257324
Validation loss: 2.039128144582113

Epoch: 6| Step: 10
Training loss: 2.616492986679077
Validation loss: 2.0367129842440286

Epoch: 6| Step: 11
Training loss: 2.1510071754455566
Validation loss: 2.0419837633768716

Epoch: 6| Step: 12
Training loss: 2.233901262283325
Validation loss: 2.0302205085754395

Epoch: 6| Step: 13
Training loss: 2.4366250038146973
Validation loss: 2.030435582002004

Epoch: 118| Step: 0
Training loss: 1.8624147176742554
Validation loss: 2.0429555575052896

Epoch: 6| Step: 1
Training loss: 1.6381192207336426
Validation loss: 2.0337848464647927

Epoch: 6| Step: 2
Training loss: 2.373321533203125
Validation loss: 2.02435835202535

Epoch: 6| Step: 3
Training loss: 2.373778820037842
Validation loss: 2.0341285268465676

Epoch: 6| Step: 4
Training loss: 2.2349040508270264
Validation loss: 2.0307039618492126

Epoch: 6| Step: 5
Training loss: 1.855123519897461
Validation loss: 2.0311433474222818

Epoch: 6| Step: 6
Training loss: 1.7704153060913086
Validation loss: 2.034376343091329

Epoch: 6| Step: 7
Training loss: 1.8194725513458252
Validation loss: 2.0314738154411316

Epoch: 6| Step: 8
Training loss: 1.9425318241119385
Validation loss: 2.041171689828237

Epoch: 6| Step: 9
Training loss: 2.6186652183532715
Validation loss: 2.035913030306498

Epoch: 6| Step: 10
Training loss: 2.6478219032287598
Validation loss: 2.030917247136434

Epoch: 6| Step: 11
Training loss: 2.1837103366851807
Validation loss: 2.039948602517446

Epoch: 6| Step: 12
Training loss: 2.2197866439819336
Validation loss: 2.036684056123098

Epoch: 6| Step: 13
Training loss: 1.7371424436569214
Validation loss: 2.028186480204264

Epoch: 119| Step: 0
Training loss: 1.8928046226501465
Validation loss: 2.0366008281707764

Epoch: 6| Step: 1
Training loss: 1.5806039571762085
Validation loss: 2.031448562939962

Epoch: 6| Step: 2
Training loss: 2.1146903038024902
Validation loss: 2.030816912651062

Epoch: 6| Step: 3
Training loss: 2.4845972061157227
Validation loss: 2.0262529253959656

Epoch: 6| Step: 4
Training loss: 1.855432152748108
Validation loss: 2.031142473220825

Epoch: 6| Step: 5
Training loss: 2.257810592651367
Validation loss: 2.0400614937146506

Epoch: 6| Step: 6
Training loss: 1.6960638761520386
Validation loss: 2.0411259730656943

Epoch: 6| Step: 7
Training loss: 1.4828749895095825
Validation loss: 2.0377301971117654

Epoch: 6| Step: 8
Training loss: 2.12595796585083
Validation loss: 2.043039540449778

Epoch: 6| Step: 9
Training loss: 2.0690932273864746
Validation loss: 2.0302181045214334

Epoch: 6| Step: 10
Training loss: 2.204111099243164
Validation loss: 2.03754069407781

Epoch: 6| Step: 11
Training loss: 2.29702091217041
Validation loss: 2.0296585957209268

Epoch: 6| Step: 12
Training loss: 2.3207736015319824
Validation loss: 2.0369815627733865

Epoch: 6| Step: 13
Training loss: 2.9802863597869873
Validation loss: 2.0456327398618064

Epoch: 120| Step: 0
Training loss: 1.8963510990142822
Validation loss: 2.0364164113998413

Epoch: 6| Step: 1
Training loss: 2.606316566467285
Validation loss: 2.0383893847465515

Epoch: 6| Step: 2
Training loss: 1.784243106842041
Validation loss: 2.0402910113334656

Epoch: 6| Step: 3
Training loss: 2.078517436981201
Validation loss: 2.036868929862976

Epoch: 6| Step: 4
Training loss: 2.309171199798584
Validation loss: 2.0427744388580322

Epoch: 6| Step: 5
Training loss: 2.411623477935791
Validation loss: 2.052593211332957

Epoch: 6| Step: 6
Training loss: 2.327807903289795
Validation loss: 2.0495737393697104

Epoch: 6| Step: 7
Training loss: 1.5415072441101074
Validation loss: 2.0428690910339355

Epoch: 6| Step: 8
Training loss: 2.4788689613342285
Validation loss: 2.042218565940857

Epoch: 6| Step: 9
Training loss: 1.6976135969161987
Validation loss: 2.034818728764852

Epoch: 6| Step: 10
Training loss: 2.1124958992004395
Validation loss: 2.03192667166392

Epoch: 6| Step: 11
Training loss: 1.7716155052185059
Validation loss: 2.0315282940864563

Epoch: 6| Step: 12
Training loss: 2.279538154602051
Validation loss: 2.034448285897573

Epoch: 6| Step: 13
Training loss: 2.300225257873535
Validation loss: 2.0282638669013977

Epoch: 121| Step: 0
Training loss: 1.7917276620864868
Validation loss: 2.021404584248861

Epoch: 6| Step: 1
Training loss: 1.9612399339675903
Validation loss: 2.027872403462728

Epoch: 6| Step: 2
Training loss: 2.2880444526672363
Validation loss: 2.029875874519348

Epoch: 6| Step: 3
Training loss: 1.915006160736084
Validation loss: 2.022616922855377

Epoch: 6| Step: 4
Training loss: 2.447474479675293
Validation loss: 2.0269492864608765

Epoch: 6| Step: 5
Training loss: 2.266270160675049
Validation loss: 2.039503812789917

Epoch: 6| Step: 6
Training loss: 2.371856927871704
Validation loss: 2.0429755250612893

Epoch: 6| Step: 7
Training loss: 2.2593770027160645
Validation loss: 2.0432226260503135

Epoch: 6| Step: 8
Training loss: 1.4892663955688477
Validation loss: 2.0422908663749695

Epoch: 6| Step: 9
Training loss: 2.187211751937866
Validation loss: 2.039070785045624

Epoch: 6| Step: 10
Training loss: 1.9212566614151
Validation loss: 2.041348099708557

Epoch: 6| Step: 11
Training loss: 2.1509475708007812
Validation loss: 2.052132527033488

Epoch: 6| Step: 12
Training loss: 1.6584248542785645
Validation loss: 2.0443241596221924

Epoch: 6| Step: 13
Training loss: 2.493403911590576
Validation loss: 2.0501375595728555

Epoch: 122| Step: 0
Training loss: 1.8549238443374634
Validation loss: 2.0432854096094766

Epoch: 6| Step: 1
Training loss: 2.120189905166626
Validation loss: 2.050300101439158

Epoch: 6| Step: 2
Training loss: 2.643378257751465
Validation loss: 2.0445547103881836

Epoch: 6| Step: 3
Training loss: 1.4040918350219727
Validation loss: 2.046778917312622

Epoch: 6| Step: 4
Training loss: 2.076944351196289
Validation loss: 2.038614273071289

Epoch: 6| Step: 5
Training loss: 2.0344104766845703
Validation loss: 2.0343890388806662

Epoch: 6| Step: 6
Training loss: 2.2121148109436035
Validation loss: 2.0396665930747986

Epoch: 6| Step: 7
Training loss: 2.0376951694488525
Validation loss: 2.0415018796920776

Epoch: 6| Step: 8
Training loss: 1.9580399990081787
Validation loss: 2.042567869027456

Epoch: 6| Step: 9
Training loss: 2.2201356887817383
Validation loss: 2.0395772457122803

Epoch: 6| Step: 10
Training loss: 2.348785877227783
Validation loss: 2.0320364236831665

Epoch: 6| Step: 11
Training loss: 1.7640621662139893
Validation loss: 2.0462658405303955

Epoch: 6| Step: 12
Training loss: 2.110166549682617
Validation loss: 2.036203384399414

Epoch: 6| Step: 13
Training loss: 2.414686918258667
Validation loss: 2.048917293548584

Epoch: 123| Step: 0
Training loss: 3.1419637203216553
Validation loss: 2.0327160954475403

Epoch: 6| Step: 1
Training loss: 1.932466745376587
Validation loss: 2.0399136543273926

Epoch: 6| Step: 2
Training loss: 1.8742460012435913
Validation loss: 2.035490115483602

Epoch: 6| Step: 3
Training loss: 2.3429384231567383
Validation loss: 2.0433509349823

Epoch: 6| Step: 4
Training loss: 1.9087355136871338
Validation loss: 2.0340108076731362

Epoch: 6| Step: 5
Training loss: 1.5835962295532227
Validation loss: 2.036672592163086

Epoch: 6| Step: 6
Training loss: 2.176936388015747
Validation loss: 2.037505110104879

Epoch: 6| Step: 7
Training loss: 2.4750914573669434
Validation loss: 2.036855618158976

Epoch: 6| Step: 8
Training loss: 1.4987651109695435
Validation loss: 2.0335869193077087

Epoch: 6| Step: 9
Training loss: 2.4950637817382812
Validation loss: 2.024855315685272

Epoch: 6| Step: 10
Training loss: 2.0692191123962402
Validation loss: 2.0235842863718667

Epoch: 6| Step: 11
Training loss: 1.8546626567840576
Validation loss: 2.03391832113266

Epoch: 6| Step: 12
Training loss: 1.7405450344085693
Validation loss: 2.029540499051412

Epoch: 6| Step: 13
Training loss: 2.080874443054199
Validation loss: 2.0311005115509033

Epoch: 124| Step: 0
Training loss: 1.3303097486495972
Validation loss: 2.030325969060262

Epoch: 6| Step: 1
Training loss: 2.454479217529297
Validation loss: 2.0464633305867515

Epoch: 6| Step: 2
Training loss: 2.045905351638794
Validation loss: 2.0273421804110208

Epoch: 6| Step: 3
Training loss: 2.4935684204101562
Validation loss: 2.040586253007253

Epoch: 6| Step: 4
Training loss: 1.458068609237671
Validation loss: 2.0268306533495584

Epoch: 6| Step: 5
Training loss: 2.444559335708618
Validation loss: 2.036490877469381

Epoch: 6| Step: 6
Training loss: 2.028935194015503
Validation loss: 2.033481538295746

Epoch: 6| Step: 7
Training loss: 2.3009238243103027
Validation loss: 2.0256256659825644

Epoch: 6| Step: 8
Training loss: 2.028043270111084
Validation loss: 2.0351468324661255

Epoch: 6| Step: 9
Training loss: 2.086770534515381
Validation loss: 2.0434717933336892

Epoch: 6| Step: 10
Training loss: 2.187445640563965
Validation loss: 2.038366754849752

Epoch: 6| Step: 11
Training loss: 2.1703007221221924
Validation loss: 2.0429660876592

Epoch: 6| Step: 12
Training loss: 2.295701503753662
Validation loss: 2.0370694200197854

Epoch: 6| Step: 13
Training loss: 1.7001194953918457
Validation loss: 2.0338611205418906

Epoch: 125| Step: 0
Training loss: 1.8696329593658447
Validation loss: 2.0364023248354592

Epoch: 6| Step: 1
Training loss: 1.6291841268539429
Validation loss: 2.03351757923762

Epoch: 6| Step: 2
Training loss: 3.0224318504333496
Validation loss: 2.046263098716736

Epoch: 6| Step: 3
Training loss: 3.1235592365264893
Validation loss: 2.037282943725586

Epoch: 6| Step: 4
Training loss: 1.388136625289917
Validation loss: 2.036916275819143

Epoch: 6| Step: 5
Training loss: 1.6951864957809448
Validation loss: 2.029095947742462

Epoch: 6| Step: 6
Training loss: 1.990535020828247
Validation loss: 2.0387253363927207

Epoch: 6| Step: 7
Training loss: 1.660757064819336
Validation loss: 2.0365809003512063

Epoch: 6| Step: 8
Training loss: 1.73530912399292
Validation loss: 2.0355918804804483

Epoch: 6| Step: 9
Training loss: 2.554872989654541
Validation loss: 2.03280516465505

Epoch: 6| Step: 10
Training loss: 2.2307472229003906
Validation loss: 2.031751334667206

Epoch: 6| Step: 11
Training loss: 2.263890027999878
Validation loss: 2.0307244459788003

Epoch: 6| Step: 12
Training loss: 1.635628342628479
Validation loss: 2.029601534207662

Epoch: 6| Step: 13
Training loss: 2.3900842666625977
Validation loss: 2.0303183992703757

Epoch: 126| Step: 0
Training loss: 2.7012197971343994
Validation loss: 2.037621478239695

Epoch: 6| Step: 1
Training loss: 1.769108772277832
Validation loss: 2.0347184538841248

Epoch: 6| Step: 2
Training loss: 1.5279395580291748
Validation loss: 2.028892199198405

Epoch: 6| Step: 3
Training loss: 1.2828419208526611
Validation loss: 2.041839301586151

Epoch: 6| Step: 4
Training loss: 1.6867784261703491
Validation loss: 2.0437819361686707

Epoch: 6| Step: 5
Training loss: 1.8077547550201416
Validation loss: 2.036776065826416

Epoch: 6| Step: 6
Training loss: 2.287843704223633
Validation loss: 2.0377463499704995

Epoch: 6| Step: 7
Training loss: 2.4854037761688232
Validation loss: 2.0443088014920554

Epoch: 6| Step: 8
Training loss: 1.9519929885864258
Validation loss: 2.0416210293769836

Epoch: 6| Step: 9
Training loss: 2.511526584625244
Validation loss: 2.0417914589246116

Epoch: 6| Step: 10
Training loss: 2.461261749267578
Validation loss: 2.0516273975372314

Epoch: 6| Step: 11
Training loss: 2.446805715560913
Validation loss: 2.048707604408264

Epoch: 6| Step: 12
Training loss: 2.4070863723754883
Validation loss: 2.0452173352241516

Epoch: 6| Step: 13
Training loss: 2.0426571369171143
Validation loss: 2.0453699032465615

Epoch: 127| Step: 0
Training loss: 1.6172431707382202
Validation loss: 2.0391716957092285

Epoch: 6| Step: 1
Training loss: 2.092104911804199
Validation loss: 2.0465158422787986

Epoch: 6| Step: 2
Training loss: 2.3160648345947266
Validation loss: 2.0383442441622415

Epoch: 6| Step: 3
Training loss: 1.891874074935913
Validation loss: 2.038154343763987

Epoch: 6| Step: 4
Training loss: 2.070080280303955
Validation loss: 2.0499517917633057

Epoch: 6| Step: 5
Training loss: 2.6551382541656494
Validation loss: 2.0499414602915444

Epoch: 6| Step: 6
Training loss: 1.856268048286438
Validation loss: 2.0594329833984375

Epoch: 6| Step: 7
Training loss: 2.253871440887451
Validation loss: 2.064955989519755

Epoch: 6| Step: 8
Training loss: 2.386575222015381
Validation loss: 2.069865345954895

Epoch: 6| Step: 9
Training loss: 2.7455930709838867
Validation loss: 2.070300022761027

Epoch: 6| Step: 10
Training loss: 2.0954341888427734
Validation loss: 2.071928938229879

Epoch: 6| Step: 11
Training loss: 1.9040757417678833
Validation loss: 2.066317061583201

Epoch: 6| Step: 12
Training loss: 1.9758646488189697
Validation loss: 2.067069351673126

Epoch: 6| Step: 13
Training loss: 2.2561182975769043
Validation loss: 2.070150454839071

Epoch: 128| Step: 0
Training loss: 2.1278867721557617
Validation loss: 2.062801996866862

Epoch: 6| Step: 1
Training loss: 1.995271921157837
Validation loss: 2.066295862197876

Epoch: 6| Step: 2
Training loss: 2.09433913230896
Validation loss: 2.0550173123677573

Epoch: 6| Step: 3
Training loss: 2.6999170780181885
Validation loss: 2.050889790058136

Epoch: 6| Step: 4
Training loss: 2.7161154747009277
Validation loss: 2.052581787109375

Epoch: 6| Step: 5
Training loss: 1.5929627418518066
Validation loss: 2.042362848917643

Epoch: 6| Step: 6
Training loss: 1.8223174810409546
Validation loss: 2.036046346028646

Epoch: 6| Step: 7
Training loss: 2.4594554901123047
Validation loss: 2.0354342659314475

Epoch: 6| Step: 8
Training loss: 1.7700250148773193
Validation loss: 2.026278297106425

Epoch: 6| Step: 9
Training loss: 1.6252893209457397
Validation loss: 2.0303032199541726

Epoch: 6| Step: 10
Training loss: 1.6804230213165283
Validation loss: 2.0308532317479453

Epoch: 6| Step: 11
Training loss: 2.7253975868225098
Validation loss: 2.035828411579132

Epoch: 6| Step: 12
Training loss: 1.7570521831512451
Validation loss: 2.034835914770762

Epoch: 6| Step: 13
Training loss: 2.6843159198760986
Validation loss: 2.0416984955469766

Epoch: 129| Step: 0
Training loss: 1.436110496520996
Validation loss: 2.0471370816230774

Epoch: 6| Step: 1
Training loss: 2.134068489074707
Validation loss: 2.042026956876119

Epoch: 6| Step: 2
Training loss: 1.8222343921661377
Validation loss: 2.044028321901957

Epoch: 6| Step: 3
Training loss: 2.417285442352295
Validation loss: 2.0447873870531716

Epoch: 6| Step: 4
Training loss: 2.1704936027526855
Validation loss: 2.0444891850153604

Epoch: 6| Step: 5
Training loss: 1.7071744203567505
Validation loss: 2.050025463104248

Epoch: 6| Step: 6
Training loss: 1.8905162811279297
Validation loss: 2.056299606959025

Epoch: 6| Step: 7
Training loss: 1.641812801361084
Validation loss: 2.0350558956464133

Epoch: 6| Step: 8
Training loss: 1.7997238636016846
Validation loss: 2.0416579445203147

Epoch: 6| Step: 9
Training loss: 2.251445770263672
Validation loss: 2.0355138381322226

Epoch: 6| Step: 10
Training loss: 2.3108534812927246
Validation loss: 2.0454713304837546

Epoch: 6| Step: 11
Training loss: 2.518003463745117
Validation loss: 2.033024231592814

Epoch: 6| Step: 12
Training loss: 2.398251533508301
Validation loss: 2.0446823040644326

Epoch: 6| Step: 13
Training loss: 2.504249334335327
Validation loss: 2.034099519252777

Epoch: 130| Step: 0
Training loss: 2.7823143005371094
Validation loss: 2.039283037185669

Epoch: 6| Step: 1
Training loss: 2.159512996673584
Validation loss: 2.037772615750631

Epoch: 6| Step: 2
Training loss: 2.293391227722168
Validation loss: 2.0436046520868936

Epoch: 6| Step: 3
Training loss: 2.1698012351989746
Validation loss: 2.0544194181760154

Epoch: 6| Step: 4
Training loss: 2.27510404586792
Validation loss: 2.052102208137512

Epoch: 6| Step: 5
Training loss: 2.6546199321746826
Validation loss: 2.047870377699534

Epoch: 6| Step: 6
Training loss: 2.002565860748291
Validation loss: 2.053235570589701

Epoch: 6| Step: 7
Training loss: 1.6568207740783691
Validation loss: 2.046325147151947

Epoch: 6| Step: 8
Training loss: 1.7588273286819458
Validation loss: 2.0449920892715454

Epoch: 6| Step: 9
Training loss: 2.414252519607544
Validation loss: 2.041930675506592

Epoch: 6| Step: 10
Training loss: 1.8662960529327393
Validation loss: 2.0332269867261252

Epoch: 6| Step: 11
Training loss: 2.0892817974090576
Validation loss: 2.026312987009684

Epoch: 6| Step: 12
Training loss: 1.935972809791565
Validation loss: 2.03101646900177

Epoch: 6| Step: 13
Training loss: 1.5778309106826782
Validation loss: 2.033412198225657

Epoch: 131| Step: 0
Training loss: 2.9397947788238525
Validation loss: 2.040587524573008

Epoch: 6| Step: 1
Training loss: 2.044705629348755
Validation loss: 2.037615160147349

Epoch: 6| Step: 2
Training loss: 2.0480456352233887
Validation loss: 2.0333337982495627

Epoch: 6| Step: 3
Training loss: 2.043954849243164
Validation loss: 2.053028106689453

Epoch: 6| Step: 4
Training loss: 1.7981281280517578
Validation loss: 2.0395958622296653

Epoch: 6| Step: 5
Training loss: 2.298459768295288
Validation loss: 2.0550801157951355

Epoch: 6| Step: 6
Training loss: 2.2983832359313965
Validation loss: 2.0475520888964334

Epoch: 6| Step: 7
Training loss: 1.5929322242736816
Validation loss: 2.0433990359306335

Epoch: 6| Step: 8
Training loss: 2.258568286895752
Validation loss: 2.03066615263621

Epoch: 6| Step: 9
Training loss: 2.4782028198242188
Validation loss: 2.0396214922269187

Epoch: 6| Step: 10
Training loss: 1.6394437551498413
Validation loss: 2.050052603085836

Epoch: 6| Step: 11
Training loss: 2.1318395137786865
Validation loss: 2.042217751344045

Epoch: 6| Step: 12
Training loss: 1.762451410293579
Validation loss: 2.035170058409373

Epoch: 6| Step: 13
Training loss: 1.5329588651657104
Validation loss: 2.042030692100525

Epoch: 132| Step: 0
Training loss: 2.381188154220581
Validation loss: 2.041327635447184

Epoch: 6| Step: 1
Training loss: 1.805024266242981
Validation loss: 2.052059233188629

Epoch: 6| Step: 2
Training loss: 2.063692331314087
Validation loss: 2.046276251475016

Epoch: 6| Step: 3
Training loss: 2.2926318645477295
Validation loss: 2.053041378657023

Epoch: 6| Step: 4
Training loss: 1.745004653930664
Validation loss: 2.050400733947754

Epoch: 6| Step: 5
Training loss: 2.3663268089294434
Validation loss: 2.0453407168388367

Epoch: 6| Step: 6
Training loss: 2.3066234588623047
Validation loss: 2.047685444355011

Epoch: 6| Step: 7
Training loss: 1.6526527404785156
Validation loss: 2.0458009441693625

Epoch: 6| Step: 8
Training loss: 1.491734504699707
Validation loss: 2.0483153263727822

Epoch: 6| Step: 9
Training loss: 1.6640232801437378
Validation loss: 2.04627917210261

Epoch: 6| Step: 10
Training loss: 3.1054677963256836
Validation loss: 2.0476322174072266

Epoch: 6| Step: 11
Training loss: 2.5357234477996826
Validation loss: 2.0445671677589417

Epoch: 6| Step: 12
Training loss: 1.9926013946533203
Validation loss: 2.0439345439275107

Epoch: 6| Step: 13
Training loss: 1.5478366613388062
Validation loss: 2.042149066925049

Epoch: 133| Step: 0
Training loss: 1.4745440483093262
Validation loss: 2.0419368743896484

Epoch: 6| Step: 1
Training loss: 1.9277082681655884
Validation loss: 2.043707032998403

Epoch: 6| Step: 2
Training loss: 1.8879659175872803
Validation loss: 2.049188276131948

Epoch: 6| Step: 3
Training loss: 1.6947903633117676
Validation loss: 2.0483195781707764

Epoch: 6| Step: 4
Training loss: 1.9341858625411987
Validation loss: 2.0512768030166626

Epoch: 6| Step: 5
Training loss: 2.382420063018799
Validation loss: 2.052507837613424

Epoch: 6| Step: 6
Training loss: 1.3868517875671387
Validation loss: 2.049862027168274

Epoch: 6| Step: 7
Training loss: 2.3549413681030273
Validation loss: 2.0507129033406577

Epoch: 6| Step: 8
Training loss: 2.3597540855407715
Validation loss: 2.043505589167277

Epoch: 6| Step: 9
Training loss: 2.3429994583129883
Validation loss: 2.061468561490377

Epoch: 6| Step: 10
Training loss: 2.287181854248047
Validation loss: 2.0576286911964417

Epoch: 6| Step: 11
Training loss: 2.6030287742614746
Validation loss: 2.0506184101104736

Epoch: 6| Step: 12
Training loss: 2.1265552043914795
Validation loss: 2.0519206126530967

Epoch: 6| Step: 13
Training loss: 2.3779003620147705
Validation loss: 2.0459896326065063

Epoch: 134| Step: 0
Training loss: 2.0203628540039062
Validation loss: 2.0472493767738342

Epoch: 6| Step: 1
Training loss: 2.4275689125061035
Validation loss: 2.0480822324752808

Epoch: 6| Step: 2
Training loss: 2.0086443424224854
Validation loss: 2.0510181188583374

Epoch: 6| Step: 3
Training loss: 1.6828476190567017
Validation loss: 2.0543173948923745

Epoch: 6| Step: 4
Training loss: 1.8896468877792358
Validation loss: 2.0404821038246155

Epoch: 6| Step: 5
Training loss: 1.9982390403747559
Validation loss: 2.0479568243026733

Epoch: 6| Step: 6
Training loss: 2.0392005443573
Validation loss: 2.0458609064420066

Epoch: 6| Step: 7
Training loss: 2.3229575157165527
Validation loss: 2.050451656182607

Epoch: 6| Step: 8
Training loss: 1.9063724279403687
Validation loss: 2.049110690752665

Epoch: 6| Step: 9
Training loss: 1.9832415580749512
Validation loss: 2.0455111265182495

Epoch: 6| Step: 10
Training loss: 2.238487720489502
Validation loss: 2.042339046796163

Epoch: 6| Step: 11
Training loss: 2.5126049518585205
Validation loss: 2.0450514356295266

Epoch: 6| Step: 12
Training loss: 2.023649215698242
Validation loss: 2.032211403052012

Epoch: 6| Step: 13
Training loss: 2.123436450958252
Validation loss: 2.037245492140452

Epoch: 135| Step: 0
Training loss: 1.7072176933288574
Validation loss: 2.046304941177368

Epoch: 6| Step: 1
Training loss: 1.4757171869277954
Validation loss: 2.040198028087616

Epoch: 6| Step: 2
Training loss: 1.961081862449646
Validation loss: 2.0547962188720703

Epoch: 6| Step: 3
Training loss: 2.1316123008728027
Validation loss: 2.047257880369822

Epoch: 6| Step: 4
Training loss: 2.065380334854126
Validation loss: 2.0390625993410745

Epoch: 6| Step: 5
Training loss: 2.270634651184082
Validation loss: 2.04842617114385

Epoch: 6| Step: 6
Training loss: 2.377438545227051
Validation loss: 2.0523120760917664

Epoch: 6| Step: 7
Training loss: 1.8798121213912964
Validation loss: 2.0523725748062134

Epoch: 6| Step: 8
Training loss: 2.2567927837371826
Validation loss: 2.051005264123281

Epoch: 6| Step: 9
Training loss: 1.9002474546432495
Validation loss: 2.045500854651133

Epoch: 6| Step: 10
Training loss: 2.220930337905884
Validation loss: 2.0544203917185464

Epoch: 6| Step: 11
Training loss: 2.5174379348754883
Validation loss: 2.061111251513163

Epoch: 6| Step: 12
Training loss: 2.0462088584899902
Validation loss: 2.047999163468679

Epoch: 6| Step: 13
Training loss: 2.509868860244751
Validation loss: 2.0555213491121926

Epoch: 136| Step: 0
Training loss: 2.050804376602173
Validation loss: 2.044294019540151

Epoch: 6| Step: 1
Training loss: 2.433657169342041
Validation loss: 2.0526530941327414

Epoch: 6| Step: 2
Training loss: 2.190335273742676
Validation loss: 2.0465214451154075

Epoch: 6| Step: 3
Training loss: 2.3914730548858643
Validation loss: 2.0492852330207825

Epoch: 6| Step: 4
Training loss: 2.029663324356079
Validation loss: 2.0334287881851196

Epoch: 6| Step: 5
Training loss: 2.1605782508850098
Validation loss: 2.0419434706370034

Epoch: 6| Step: 6
Training loss: 1.908150553703308
Validation loss: 2.0387842456499734

Epoch: 6| Step: 7
Training loss: 1.9999399185180664
Validation loss: 2.0440507928530374

Epoch: 6| Step: 8
Training loss: 1.7745182514190674
Validation loss: 2.040830612182617

Epoch: 6| Step: 9
Training loss: 1.9880996942520142
Validation loss: 2.0419700145721436

Epoch: 6| Step: 10
Training loss: 2.193026542663574
Validation loss: 2.0461698373158774

Epoch: 6| Step: 11
Training loss: 1.8867003917694092
Validation loss: 2.043992598851522

Epoch: 6| Step: 12
Training loss: 2.512836456298828
Validation loss: 2.0322086016337075

Epoch: 6| Step: 13
Training loss: 1.592612862586975
Validation loss: 2.04950741926829

Epoch: 137| Step: 0
Training loss: 2.384340524673462
Validation loss: 2.036959091822306

Epoch: 6| Step: 1
Training loss: 1.5603115558624268
Validation loss: 2.044689138730367

Epoch: 6| Step: 2
Training loss: 2.024860143661499
Validation loss: 2.047504981358846

Epoch: 6| Step: 3
Training loss: 2.044982433319092
Validation loss: 2.0483784874280295

Epoch: 6| Step: 4
Training loss: 2.326073169708252
Validation loss: 2.0527267456054688

Epoch: 6| Step: 5
Training loss: 2.7652370929718018
Validation loss: 2.0523241758346558

Epoch: 6| Step: 6
Training loss: 2.227626323699951
Validation loss: 2.052021543184916

Epoch: 6| Step: 7
Training loss: 1.9038238525390625
Validation loss: 2.0526157220204673

Epoch: 6| Step: 8
Training loss: 1.8798024654388428
Validation loss: 2.0555574099222818

Epoch: 6| Step: 9
Training loss: 2.012518882751465
Validation loss: 2.054477552572886

Epoch: 6| Step: 10
Training loss: 2.0296196937561035
Validation loss: 2.064367651939392

Epoch: 6| Step: 11
Training loss: 1.9257694482803345
Validation loss: 2.0613257686297097

Epoch: 6| Step: 12
Training loss: 2.0137228965759277
Validation loss: 2.048324704170227

Epoch: 6| Step: 13
Training loss: 1.8334637880325317
Validation loss: 2.0464568932851157

Epoch: 138| Step: 0
Training loss: 2.7744252681732178
Validation loss: 2.0440539518992105

Epoch: 6| Step: 1
Training loss: 1.8171520233154297
Validation loss: 2.0530652602513633

Epoch: 6| Step: 2
Training loss: 2.1289947032928467
Validation loss: 2.0573389530181885

Epoch: 6| Step: 3
Training loss: 2.5506038665771484
Validation loss: 2.049157281716665

Epoch: 6| Step: 4
Training loss: 2.1591038703918457
Validation loss: 2.056515316168467

Epoch: 6| Step: 5
Training loss: 2.1498188972473145
Validation loss: 2.047624190648397

Epoch: 6| Step: 6
Training loss: 2.2229652404785156
Validation loss: 2.049461285273234

Epoch: 6| Step: 7
Training loss: 1.6745144128799438
Validation loss: 2.0410874088605246

Epoch: 6| Step: 8
Training loss: 2.406198024749756
Validation loss: 2.0502476890881858

Epoch: 6| Step: 9
Training loss: 1.7300958633422852
Validation loss: 2.042709529399872

Epoch: 6| Step: 10
Training loss: 1.2898554801940918
Validation loss: 2.0482261180877686

Epoch: 6| Step: 11
Training loss: 1.67232346534729
Validation loss: 2.052815556526184

Epoch: 6| Step: 12
Training loss: 2.2342820167541504
Validation loss: 2.045969605445862

Epoch: 6| Step: 13
Training loss: 2.254513740539551
Validation loss: 2.049319605032603

Epoch: 139| Step: 0
Training loss: 2.016935348510742
Validation loss: 2.045257886250814

Epoch: 6| Step: 1
Training loss: 2.0555896759033203
Validation loss: 2.0413506229718528

Epoch: 6| Step: 2
Training loss: 2.526207447052002
Validation loss: 2.033826768398285

Epoch: 6| Step: 3
Training loss: 1.7297905683517456
Validation loss: 2.038801670074463

Epoch: 6| Step: 4
Training loss: 2.206503391265869
Validation loss: 2.034852921962738

Epoch: 6| Step: 5
Training loss: 2.0366203784942627
Validation loss: 2.0295217633247375

Epoch: 6| Step: 6
Training loss: 1.961675763130188
Validation loss: 2.035277247428894

Epoch: 6| Step: 7
Training loss: 2.0275537967681885
Validation loss: 2.0325657526652017

Epoch: 6| Step: 8
Training loss: 2.2690162658691406
Validation loss: 2.0564067562421164

Epoch: 6| Step: 9
Training loss: 2.203333854675293
Validation loss: 2.047422150770823

Epoch: 6| Step: 10
Training loss: 1.8115248680114746
Validation loss: 2.0453383723894754

Epoch: 6| Step: 11
Training loss: 2.032925605773926
Validation loss: 2.0548389752705893

Epoch: 6| Step: 12
Training loss: 2.275290012359619
Validation loss: 2.0473724603652954

Epoch: 6| Step: 13
Training loss: 1.7061936855316162
Validation loss: 2.050617833932241

Epoch: 140| Step: 0
Training loss: 2.3524651527404785
Validation loss: 2.0506537755330405

Epoch: 6| Step: 1
Training loss: 1.8767509460449219
Validation loss: 2.05584325393041

Epoch: 6| Step: 2
Training loss: 2.2992610931396484
Validation loss: 2.0442997018496194

Epoch: 6| Step: 3
Training loss: 1.508697271347046
Validation loss: 2.0462693770726523

Epoch: 6| Step: 4
Training loss: 1.5859419107437134
Validation loss: 2.0387767354647317

Epoch: 6| Step: 5
Training loss: 2.2113912105560303
Validation loss: 2.048944413661957

Epoch: 6| Step: 6
Training loss: 1.9601882696151733
Validation loss: 2.0407939751942954

Epoch: 6| Step: 7
Training loss: 2.1873209476470947
Validation loss: 2.039643148581187

Epoch: 6| Step: 8
Training loss: 2.1284091472625732
Validation loss: 2.0416105588277182

Epoch: 6| Step: 9
Training loss: 2.401019334793091
Validation loss: 2.04505584637324

Epoch: 6| Step: 10
Training loss: 2.3873493671417236
Validation loss: 2.045931617418925

Epoch: 6| Step: 11
Training loss: 2.0334811210632324
Validation loss: 2.042072316010793

Epoch: 6| Step: 12
Training loss: 2.3827743530273438
Validation loss: 2.045785148938497

Epoch: 6| Step: 13
Training loss: 1.7172130346298218
Validation loss: 2.0477071404457092

Epoch: 141| Step: 0
Training loss: 2.0241427421569824
Validation loss: 2.0512842535972595

Epoch: 6| Step: 1
Training loss: 2.3417913913726807
Validation loss: 2.0511379639307656

Epoch: 6| Step: 2
Training loss: 1.8531934022903442
Validation loss: 2.0518064300219216

Epoch: 6| Step: 3
Training loss: 2.15403151512146
Validation loss: 2.0584619442621865

Epoch: 6| Step: 4
Training loss: 2.408313035964966
Validation loss: 2.053546905517578

Epoch: 6| Step: 5
Training loss: 1.8435066938400269
Validation loss: 2.053092300891876

Epoch: 6| Step: 6
Training loss: 1.9399943351745605
Validation loss: 2.047456920146942

Epoch: 6| Step: 7
Training loss: 2.740769863128662
Validation loss: 2.0573659539222717

Epoch: 6| Step: 8
Training loss: 1.212751030921936
Validation loss: 2.0399088660875955

Epoch: 6| Step: 9
Training loss: 2.427821159362793
Validation loss: 2.060734669367472

Epoch: 6| Step: 10
Training loss: 2.07951021194458
Validation loss: 2.054694434007009

Epoch: 6| Step: 11
Training loss: 1.728632926940918
Validation loss: 2.0549465815226235

Epoch: 6| Step: 12
Training loss: 2.300574779510498
Validation loss: 2.056047797203064

Epoch: 6| Step: 13
Training loss: 1.7508013248443604
Validation loss: 2.0594392816225686

Epoch: 142| Step: 0
Training loss: 2.2415640354156494
Validation loss: 2.0619255900382996

Epoch: 6| Step: 1
Training loss: 2.146620273590088
Validation loss: 2.052470008532206

Epoch: 6| Step: 2
Training loss: 1.938586950302124
Validation loss: 2.0572259426116943

Epoch: 6| Step: 3
Training loss: 1.9074939489364624
Validation loss: 2.0480464498202005

Epoch: 6| Step: 4
Training loss: 1.8939067125320435
Validation loss: 2.0502238670984902

Epoch: 6| Step: 5
Training loss: 1.9199315309524536
Validation loss: 2.0488524039586387

Epoch: 6| Step: 6
Training loss: 2.2114076614379883
Validation loss: 2.0464836955070496

Epoch: 6| Step: 7
Training loss: 2.056698799133301
Validation loss: 2.048366208871206

Epoch: 6| Step: 8
Training loss: 2.6542625427246094
Validation loss: 2.051501373449961

Epoch: 6| Step: 9
Training loss: 2.080876111984253
Validation loss: 2.0454839070638022

Epoch: 6| Step: 10
Training loss: 1.3880800008773804
Validation loss: 2.062739292780558

Epoch: 6| Step: 11
Training loss: 2.103264331817627
Validation loss: 2.0569677551587424

Epoch: 6| Step: 12
Training loss: 2.5097696781158447
Validation loss: 2.0701022942860923

Epoch: 6| Step: 13
Training loss: 2.0903427600860596
Validation loss: 2.0568374594052634

Epoch: 143| Step: 0
Training loss: 1.8143980503082275
Validation loss: 2.0624016523361206

Epoch: 6| Step: 1
Training loss: 1.5323371887207031
Validation loss: 2.0636069973309836

Epoch: 6| Step: 2
Training loss: 2.4151506423950195
Validation loss: 2.0606698989868164

Epoch: 6| Step: 3
Training loss: 1.943382978439331
Validation loss: 2.054707864920298

Epoch: 6| Step: 4
Training loss: 2.1896281242370605
Validation loss: 2.0452638268470764

Epoch: 6| Step: 5
Training loss: 2.090996742248535
Validation loss: 2.044940253098806

Epoch: 6| Step: 6
Training loss: 2.1051642894744873
Validation loss: 2.0411351720492044

Epoch: 6| Step: 7
Training loss: 2.123654842376709
Validation loss: 2.041766266028086

Epoch: 6| Step: 8
Training loss: 2.05831241607666
Validation loss: 2.0371366341908774

Epoch: 6| Step: 9
Training loss: 1.3620308637619019
Validation loss: 2.0358148217201233

Epoch: 6| Step: 10
Training loss: 1.871930480003357
Validation loss: 2.041517953077952

Epoch: 6| Step: 11
Training loss: 2.2216262817382812
Validation loss: 2.0404857794443765

Epoch: 6| Step: 12
Training loss: 2.381634473800659
Validation loss: 2.0443946719169617

Epoch: 6| Step: 13
Training loss: 2.6073427200317383
Validation loss: 2.042113264401754

Epoch: 144| Step: 0
Training loss: 1.7384696006774902
Validation loss: 2.0434955954551697

Epoch: 6| Step: 1
Training loss: 2.315290927886963
Validation loss: 2.042722841103872

Epoch: 6| Step: 2
Training loss: 2.340684652328491
Validation loss: 2.046157697836558

Epoch: 6| Step: 3
Training loss: 2.1944737434387207
Validation loss: 2.0437726577123008

Epoch: 6| Step: 4
Training loss: 1.5866608619689941
Validation loss: 2.0568172534306846

Epoch: 6| Step: 5
Training loss: 2.706653118133545
Validation loss: 2.0444400906562805

Epoch: 6| Step: 6
Training loss: 2.8830184936523438
Validation loss: 2.0445796648661294

Epoch: 6| Step: 7
Training loss: 1.8973723649978638
Validation loss: 2.0526928504308066

Epoch: 6| Step: 8
Training loss: 2.4295191764831543
Validation loss: 2.0472267270088196

Epoch: 6| Step: 9
Training loss: 1.5340455770492554
Validation loss: 2.0553115606307983

Epoch: 6| Step: 10
Training loss: 1.095529556274414
Validation loss: 2.0494688947995505

Epoch: 6| Step: 11
Training loss: 2.247053623199463
Validation loss: 2.0527383685112

Epoch: 6| Step: 12
Training loss: 1.4966199398040771
Validation loss: 2.0512561003367105

Epoch: 6| Step: 13
Training loss: 2.2585606575012207
Validation loss: 2.049008091290792

Epoch: 145| Step: 0
Training loss: 1.8171298503875732
Validation loss: 2.045473277568817

Epoch: 6| Step: 1
Training loss: 2.093940019607544
Validation loss: 2.0422361294428506

Epoch: 6| Step: 2
Training loss: 2.0197577476501465
Validation loss: 2.0458918611208596

Epoch: 6| Step: 3
Training loss: 1.885631799697876
Validation loss: 2.043544332186381

Epoch: 6| Step: 4
Training loss: 2.417325735092163
Validation loss: 2.046382109324137

Epoch: 6| Step: 5
Training loss: 2.2257888317108154
Validation loss: 2.0427070260047913

Epoch: 6| Step: 6
Training loss: 2.1133811473846436
Validation loss: 2.0424562295277915

Epoch: 6| Step: 7
Training loss: 1.8285589218139648
Validation loss: 2.045759697755178

Epoch: 6| Step: 8
Training loss: 2.0246710777282715
Validation loss: 2.0480722188949585

Epoch: 6| Step: 9
Training loss: 2.0352792739868164
Validation loss: 2.044234832127889

Epoch: 6| Step: 10
Training loss: 2.0696206092834473
Validation loss: 2.0427226622899375

Epoch: 6| Step: 11
Training loss: 1.7173494100570679
Validation loss: 2.046135107676188

Epoch: 6| Step: 12
Training loss: 1.8790161609649658
Validation loss: 2.0464929739634194

Epoch: 6| Step: 13
Training loss: 2.773242950439453
Validation loss: 2.0472257335980735

Epoch: 146| Step: 0
Training loss: 1.8237926959991455
Validation loss: 2.0421839356422424

Epoch: 6| Step: 1
Training loss: 2.191056489944458
Validation loss: 2.0472410519917807

Epoch: 6| Step: 2
Training loss: 2.3485612869262695
Validation loss: 2.045311987400055

Epoch: 6| Step: 3
Training loss: 2.0859432220458984
Validation loss: 2.0440681179364524

Epoch: 6| Step: 4
Training loss: 1.373964786529541
Validation loss: 2.048282484213511

Epoch: 6| Step: 5
Training loss: 2.1167378425598145
Validation loss: 2.0482937892278037

Epoch: 6| Step: 6
Training loss: 1.8549466133117676
Validation loss: 2.056537131468455

Epoch: 6| Step: 7
Training loss: 1.7067259550094604
Validation loss: 2.060621500015259

Epoch: 6| Step: 8
Training loss: 2.218594789505005
Validation loss: 2.0588582356770835

Epoch: 6| Step: 9
Training loss: 2.1647133827209473
Validation loss: 2.056981901327769

Epoch: 6| Step: 10
Training loss: 2.217618465423584
Validation loss: 2.0548752546310425

Epoch: 6| Step: 11
Training loss: 1.9775865077972412
Validation loss: 2.0580973029136658

Epoch: 6| Step: 12
Training loss: 2.416260242462158
Validation loss: 2.049400508403778

Epoch: 6| Step: 13
Training loss: 2.1154255867004395
Validation loss: 2.0622306068738303

Epoch: 147| Step: 0
Training loss: 2.0726919174194336
Validation loss: 2.0568401416142783

Epoch: 6| Step: 1
Training loss: 2.5587005615234375
Validation loss: 2.0457367102305093

Epoch: 6| Step: 2
Training loss: 1.7631969451904297
Validation loss: 2.0402217507362366

Epoch: 6| Step: 3
Training loss: 1.8163856267929077
Validation loss: 2.044895132382711

Epoch: 6| Step: 4
Training loss: 1.9145687818527222
Validation loss: 2.0487094124158225

Epoch: 6| Step: 5
Training loss: 2.2657647132873535
Validation loss: 2.058979630470276

Epoch: 6| Step: 6
Training loss: 2.1926958560943604
Validation loss: 2.0544693072636924

Epoch: 6| Step: 7
Training loss: 2.1399941444396973
Validation loss: 2.0466113090515137

Epoch: 6| Step: 8
Training loss: 1.917708396911621
Validation loss: 2.0551899671554565

Epoch: 6| Step: 9
Training loss: 1.3189079761505127
Validation loss: 2.0599406560262046

Epoch: 6| Step: 10
Training loss: 2.4098331928253174
Validation loss: 2.050754497448603

Epoch: 6| Step: 11
Training loss: 1.9944771528244019
Validation loss: 2.0539210041364035

Epoch: 6| Step: 12
Training loss: 2.6115710735321045
Validation loss: 2.052125950654348

Epoch: 6| Step: 13
Training loss: 1.687393307685852
Validation loss: 2.0499780972798667

Epoch: 148| Step: 0
Training loss: 2.384970188140869
Validation loss: 2.055752158164978

Epoch: 6| Step: 1
Training loss: 1.7019236087799072
Validation loss: 2.0525965293248496

Epoch: 6| Step: 2
Training loss: 2.387289047241211
Validation loss: 2.049234946568807

Epoch: 6| Step: 3
Training loss: 2.4433417320251465
Validation loss: 2.0520977775255838

Epoch: 6| Step: 4
Training loss: 2.0639493465423584
Validation loss: 2.0600332021713257

Epoch: 6| Step: 5
Training loss: 1.9313743114471436
Validation loss: 2.0598803957303367

Epoch: 6| Step: 6
Training loss: 1.8515294790267944
Validation loss: 2.062737206617991

Epoch: 6| Step: 7
Training loss: 2.464015007019043
Validation loss: 2.0730910301208496

Epoch: 6| Step: 8
Training loss: 2.102858066558838
Validation loss: 2.0697086652119956

Epoch: 6| Step: 9
Training loss: 2.1432766914367676
Validation loss: 2.0784186522165933

Epoch: 6| Step: 10
Training loss: 1.3775043487548828
Validation loss: 2.069230099519094

Epoch: 6| Step: 11
Training loss: 1.7468626499176025
Validation loss: 2.0630396008491516

Epoch: 6| Step: 12
Training loss: 2.516814708709717
Validation loss: 2.0742790500322976

Epoch: 6| Step: 13
Training loss: 1.8131357431411743
Validation loss: 2.07158754269282

Epoch: 149| Step: 0
Training loss: 3.1502387523651123
Validation loss: 2.0749300916989646

Epoch: 6| Step: 1
Training loss: 2.0750579833984375
Validation loss: 2.074038803577423

Epoch: 6| Step: 2
Training loss: 2.1415624618530273
Validation loss: 2.064148485660553

Epoch: 6| Step: 3
Training loss: 2.22383975982666
Validation loss: 2.0676130652427673

Epoch: 6| Step: 4
Training loss: 1.9576665163040161
Validation loss: 2.0581491788228354

Epoch: 6| Step: 5
Training loss: 1.6932839155197144
Validation loss: 2.056169410546621

Epoch: 6| Step: 6
Training loss: 2.422727108001709
Validation loss: 2.0616793235143027

Epoch: 6| Step: 7
Training loss: 1.8732048273086548
Validation loss: 2.0660606225331626

Epoch: 6| Step: 8
Training loss: 1.5556137561798096
Validation loss: 2.0593708753585815

Epoch: 6| Step: 9
Training loss: 2.400224208831787
Validation loss: 2.070238709449768

Epoch: 6| Step: 10
Training loss: 1.458681583404541
Validation loss: 2.061968187491099

Epoch: 6| Step: 11
Training loss: 1.9841840267181396
Validation loss: 2.06533145904541

Epoch: 6| Step: 12
Training loss: 1.9109110832214355
Validation loss: 2.067402958869934

Epoch: 6| Step: 13
Training loss: 1.7198971509933472
Validation loss: 2.0678102374076843

Epoch: 150| Step: 0
Training loss: 2.298563241958618
Validation loss: 2.0568004846572876

Epoch: 6| Step: 1
Training loss: 1.9997426271438599
Validation loss: 2.0717368721961975

Epoch: 6| Step: 2
Training loss: 2.0623621940612793
Validation loss: 2.0748712619145713

Epoch: 6| Step: 3
Training loss: 2.5427844524383545
Validation loss: 2.062900424003601

Epoch: 6| Step: 4
Training loss: 1.8671516180038452
Validation loss: 2.0681351820627847

Epoch: 6| Step: 5
Training loss: 2.1868999004364014
Validation loss: 2.0739063819249473

Epoch: 6| Step: 6
Training loss: 1.6027100086212158
Validation loss: 2.060951352119446

Epoch: 6| Step: 7
Training loss: 2.4796924591064453
Validation loss: 2.0507758061091104

Epoch: 6| Step: 8
Training loss: 1.6449759006500244
Validation loss: 2.067621966203054

Epoch: 6| Step: 9
Training loss: 1.411508321762085
Validation loss: 2.072881499926249

Epoch: 6| Step: 10
Training loss: 2.3241329193115234
Validation loss: 2.0580358107884726

Epoch: 6| Step: 11
Training loss: 2.001067876815796
Validation loss: 2.0734466910362244

Epoch: 6| Step: 12
Training loss: 2.025252342224121
Validation loss: 2.0701691309611

Epoch: 6| Step: 13
Training loss: 2.0093770027160645
Validation loss: 2.063711404800415

Epoch: 151| Step: 0
Training loss: 1.5812028646469116
Validation loss: 2.0636793971061707

Epoch: 6| Step: 1
Training loss: 1.7007708549499512
Validation loss: 2.075075308481852

Epoch: 6| Step: 2
Training loss: 2.543090581893921
Validation loss: 2.0643222332000732

Epoch: 6| Step: 3
Training loss: 2.515622138977051
Validation loss: 2.071626822153727

Epoch: 6| Step: 4
Training loss: 1.7601542472839355
Validation loss: 2.0652255415916443

Epoch: 6| Step: 5
Training loss: 1.871828317642212
Validation loss: 2.055802265803019

Epoch: 6| Step: 6
Training loss: 2.3724660873413086
Validation loss: 2.070198138554891

Epoch: 6| Step: 7
Training loss: 2.4997329711914062
Validation loss: 2.0611729423205056

Epoch: 6| Step: 8
Training loss: 1.7941985130310059
Validation loss: 2.0623595118522644

Epoch: 6| Step: 9
Training loss: 1.9109777212142944
Validation loss: 2.0647216041882834

Epoch: 6| Step: 10
Training loss: 2.073793888092041
Validation loss: 2.066588739554087

Epoch: 6| Step: 11
Training loss: 2.299135446548462
Validation loss: 2.0631947914759317

Epoch: 6| Step: 12
Training loss: 1.3647737503051758
Validation loss: 2.0582797129948935

Epoch: 6| Step: 13
Training loss: 2.1847589015960693
Validation loss: 2.0677190025647483

Epoch: 152| Step: 0
Training loss: 2.073540449142456
Validation loss: 2.066123644510905

Epoch: 6| Step: 1
Training loss: 2.376959800720215
Validation loss: 2.0730254650115967

Epoch: 6| Step: 2
Training loss: 1.628584384918213
Validation loss: 2.0710415045420327

Epoch: 6| Step: 3
Training loss: 2.2236480712890625
Validation loss: 2.063260813554128

Epoch: 6| Step: 4
Training loss: 1.7583296298980713
Validation loss: 2.0624054074287415

Epoch: 6| Step: 5
Training loss: 1.7653794288635254
Validation loss: 2.071796397368113

Epoch: 6| Step: 6
Training loss: 1.4415227174758911
Validation loss: 2.0628809730211892

Epoch: 6| Step: 7
Training loss: 1.9401946067810059
Validation loss: 2.0631234844525657

Epoch: 6| Step: 8
Training loss: 2.005882501602173
Validation loss: 2.072497288386027

Epoch: 6| Step: 9
Training loss: 1.5111019611358643
Validation loss: 2.062938471635183

Epoch: 6| Step: 10
Training loss: 1.8871128559112549
Validation loss: 2.0664660731951394

Epoch: 6| Step: 11
Training loss: 2.735499858856201
Validation loss: 2.068526486555735

Epoch: 6| Step: 12
Training loss: 2.7328193187713623
Validation loss: 2.0691312750180564

Epoch: 6| Step: 13
Training loss: 2.2358834743499756
Validation loss: 2.0718048016230264

Epoch: 153| Step: 0
Training loss: 2.17671275138855
Validation loss: 2.0712575117746987

Epoch: 6| Step: 1
Training loss: 2.05653715133667
Validation loss: 2.0722290873527527

Epoch: 6| Step: 2
Training loss: 2.082852363586426
Validation loss: 2.0691819985707602

Epoch: 6| Step: 3
Training loss: 1.993351936340332
Validation loss: 2.075518329938253

Epoch: 6| Step: 4
Training loss: 2.112739086151123
Validation loss: 2.0727601846059165

Epoch: 6| Step: 5
Training loss: 1.7549195289611816
Validation loss: 2.0724143187204995

Epoch: 6| Step: 6
Training loss: 2.7137651443481445
Validation loss: 2.066979229450226

Epoch: 6| Step: 7
Training loss: 1.9242322444915771
Validation loss: 2.0593063632647195

Epoch: 6| Step: 8
Training loss: 1.2613656520843506
Validation loss: 2.066459536552429

Epoch: 6| Step: 9
Training loss: 1.6886565685272217
Validation loss: 2.0717022816340127

Epoch: 6| Step: 10
Training loss: 1.8981738090515137
Validation loss: 2.0657714207967124

Epoch: 6| Step: 11
Training loss: 2.2467832565307617
Validation loss: 2.057014584541321

Epoch: 6| Step: 12
Training loss: 1.9077200889587402
Validation loss: 2.070092519124349

Epoch: 6| Step: 13
Training loss: 2.596832513809204
Validation loss: 2.0629610617955527

Epoch: 154| Step: 0
Training loss: 2.4198975563049316
Validation loss: 2.062837839126587

Epoch: 6| Step: 1
Training loss: 1.41811203956604
Validation loss: 2.0641880432764688

Epoch: 6| Step: 2
Training loss: 1.7692475318908691
Validation loss: 2.0688772002855935

Epoch: 6| Step: 3
Training loss: 1.9052520990371704
Validation loss: 2.0619833866755166

Epoch: 6| Step: 4
Training loss: 2.8147692680358887
Validation loss: 2.062673886617025

Epoch: 6| Step: 5
Training loss: 2.030573844909668
Validation loss: 2.066235661506653

Epoch: 6| Step: 6
Training loss: 1.915086269378662
Validation loss: 2.0727885166803994

Epoch: 6| Step: 7
Training loss: 1.9538066387176514
Validation loss: 2.064451108376185

Epoch: 6| Step: 8
Training loss: 1.9832510948181152
Validation loss: 2.0648900270462036

Epoch: 6| Step: 9
Training loss: 2.179380416870117
Validation loss: 2.0510162909825644

Epoch: 6| Step: 10
Training loss: 2.1395628452301025
Validation loss: 2.055600106716156

Epoch: 6| Step: 11
Training loss: 1.9413437843322754
Validation loss: 2.0410582621892295

Epoch: 6| Step: 12
Training loss: 2.247194766998291
Validation loss: 2.059212942918142

Epoch: 6| Step: 13
Training loss: 1.6961522102355957
Validation loss: 2.0571300784746804

Epoch: 155| Step: 0
Training loss: 1.9587888717651367
Validation loss: 2.0490564703941345

Epoch: 6| Step: 1
Training loss: 2.01104736328125
Validation loss: 2.0562901894251504

Epoch: 6| Step: 2
Training loss: 1.7754350900650024
Validation loss: 2.056535800298055

Epoch: 6| Step: 3
Training loss: 2.8977766036987305
Validation loss: 2.0583844979604087

Epoch: 6| Step: 4
Training loss: 1.8956162929534912
Validation loss: 2.0541454752286277

Epoch: 6| Step: 5
Training loss: 2.0442910194396973
Validation loss: 2.051734368006388

Epoch: 6| Step: 6
Training loss: 2.068925380706787
Validation loss: 2.063042243321737

Epoch: 6| Step: 7
Training loss: 2.2279233932495117
Validation loss: 2.06111216545105

Epoch: 6| Step: 8
Training loss: 2.2102108001708984
Validation loss: 2.0664716958999634

Epoch: 6| Step: 9
Training loss: 1.7536541223526
Validation loss: 2.061603009700775

Epoch: 6| Step: 10
Training loss: 2.019681930541992
Validation loss: 2.0619925061861673

Epoch: 6| Step: 11
Training loss: 1.963131308555603
Validation loss: 2.0695589184761047

Epoch: 6| Step: 12
Training loss: 1.7052037715911865
Validation loss: 2.082025388876597

Epoch: 6| Step: 13
Training loss: 2.04641056060791
Validation loss: 2.0795856515566506

Epoch: 156| Step: 0
Training loss: 2.3028783798217773
Validation loss: 2.0637097358703613

Epoch: 6| Step: 1
Training loss: 1.9673783779144287
Validation loss: 2.076715350151062

Epoch: 6| Step: 2
Training loss: 1.9332767724990845
Validation loss: 2.0676045219103494

Epoch: 6| Step: 3
Training loss: 1.9863814115524292
Validation loss: 2.0545992453893027

Epoch: 6| Step: 4
Training loss: 1.501239538192749
Validation loss: 2.0641808907190957

Epoch: 6| Step: 5
Training loss: 1.8658833503723145
Validation loss: 2.0559085607528687

Epoch: 6| Step: 6
Training loss: 2.5571279525756836
Validation loss: 2.0569223364194236

Epoch: 6| Step: 7
Training loss: 2.182283878326416
Validation loss: 2.053380072116852

Epoch: 6| Step: 8
Training loss: 2.223422050476074
Validation loss: 2.0534979701042175

Epoch: 6| Step: 9
Training loss: 1.8909724950790405
Validation loss: 2.062865972518921

Epoch: 6| Step: 10
Training loss: 1.5709705352783203
Validation loss: 2.0497817198435464

Epoch: 6| Step: 11
Training loss: 2.007115125656128
Validation loss: 2.0597553650538125

Epoch: 6| Step: 12
Training loss: 1.6982731819152832
Validation loss: 2.0734038949012756

Epoch: 6| Step: 13
Training loss: 2.5495433807373047
Validation loss: 2.067782998085022

Epoch: 157| Step: 0
Training loss: 1.7024738788604736
Validation loss: 2.072975238164266

Epoch: 6| Step: 1
Training loss: 2.1195197105407715
Validation loss: 2.057709038257599

Epoch: 6| Step: 2
Training loss: 1.686533808708191
Validation loss: 2.0756194790204368

Epoch: 6| Step: 3
Training loss: 2.433181047439575
Validation loss: 2.0662599007288613

Epoch: 6| Step: 4
Training loss: 2.353107452392578
Validation loss: 2.06471848487854

Epoch: 6| Step: 5
Training loss: 2.266465425491333
Validation loss: 2.062414526939392

Epoch: 6| Step: 6
Training loss: 1.761547565460205
Validation loss: 2.0690530141194663

Epoch: 6| Step: 7
Training loss: 1.5593687295913696
Validation loss: 2.067692518234253

Epoch: 6| Step: 8
Training loss: 1.976352334022522
Validation loss: 2.076214094956716

Epoch: 6| Step: 9
Training loss: 2.1903676986694336
Validation loss: 2.078207015991211

Epoch: 6| Step: 10
Training loss: 2.258201837539673
Validation loss: 2.076563060283661

Epoch: 6| Step: 11
Training loss: 1.5097944736480713
Validation loss: 2.0764588514963784

Epoch: 6| Step: 12
Training loss: 2.2254152297973633
Validation loss: 2.0881665349006653

Epoch: 6| Step: 13
Training loss: 2.095125913619995
Validation loss: 2.0731751124064126

Epoch: 158| Step: 0
Training loss: 1.7096543312072754
Validation loss: 2.072334865729014

Epoch: 6| Step: 1
Training loss: 2.0222651958465576
Validation loss: 2.0844473640124

Epoch: 6| Step: 2
Training loss: 1.8822237253189087
Validation loss: 2.0756545464197793

Epoch: 6| Step: 3
Training loss: 2.502397298812866
Validation loss: 2.0743056535720825

Epoch: 6| Step: 4
Training loss: 2.029834747314453
Validation loss: 2.0728926062583923

Epoch: 6| Step: 5
Training loss: 2.6536459922790527
Validation loss: 2.0795742869377136

Epoch: 6| Step: 6
Training loss: 2.1071839332580566
Validation loss: 2.0708417296409607

Epoch: 6| Step: 7
Training loss: 2.1036295890808105
Validation loss: 2.0740687052408853

Epoch: 6| Step: 8
Training loss: 1.9118881225585938
Validation loss: 2.0700814525286355

Epoch: 6| Step: 9
Training loss: 1.6393778324127197
Validation loss: 2.0720929503440857

Epoch: 6| Step: 10
Training loss: 1.7376281023025513
Validation loss: 2.062585771083832

Epoch: 6| Step: 11
Training loss: 2.220736503601074
Validation loss: 2.065452774365743

Epoch: 6| Step: 12
Training loss: 1.8196814060211182
Validation loss: 2.0705950458844504

Epoch: 6| Step: 13
Training loss: 1.7761132717132568
Validation loss: 2.0746209820111594

Epoch: 159| Step: 0
Training loss: 2.761186122894287
Validation loss: 2.0777353843053183

Epoch: 6| Step: 1
Training loss: 1.7623388767242432
Validation loss: 2.0732021729151406

Epoch: 6| Step: 2
Training loss: 1.9471951723098755
Validation loss: 2.0687665541966758

Epoch: 6| Step: 3
Training loss: 2.3139843940734863
Validation loss: 2.0712245106697083

Epoch: 6| Step: 4
Training loss: 1.7228156328201294
Validation loss: 2.07424666484197

Epoch: 6| Step: 5
Training loss: 1.68997323513031
Validation loss: 2.074788490931193

Epoch: 6| Step: 6
Training loss: 2.109078884124756
Validation loss: 2.0704967776934304

Epoch: 6| Step: 7
Training loss: 1.7441487312316895
Validation loss: 2.0711705883344016

Epoch: 6| Step: 8
Training loss: 2.339108943939209
Validation loss: 2.0730379819869995

Epoch: 6| Step: 9
Training loss: 1.585224986076355
Validation loss: 2.068229933579763

Epoch: 6| Step: 10
Training loss: 2.2660269737243652
Validation loss: 2.061816930770874

Epoch: 6| Step: 11
Training loss: 1.5510154962539673
Validation loss: 2.057641784350077

Epoch: 6| Step: 12
Training loss: 2.0646347999572754
Validation loss: 2.060026248296102

Epoch: 6| Step: 13
Training loss: 2.3541274070739746
Validation loss: 2.067969262599945

Epoch: 160| Step: 0
Training loss: 2.0471701622009277
Validation loss: 2.0659722089767456

Epoch: 6| Step: 1
Training loss: 2.1193394660949707
Validation loss: 2.06818155447642

Epoch: 6| Step: 2
Training loss: 1.8198981285095215
Validation loss: 2.0703352292378745

Epoch: 6| Step: 3
Training loss: 1.8460725545883179
Validation loss: 2.072751005490621

Epoch: 6| Step: 4
Training loss: 1.8750205039978027
Validation loss: 2.065881868203481

Epoch: 6| Step: 5
Training loss: 2.084153652191162
Validation loss: 2.0829812486966452

Epoch: 6| Step: 6
Training loss: 2.5794787406921387
Validation loss: 2.075138290723165

Epoch: 6| Step: 7
Training loss: 2.384711503982544
Validation loss: 2.0645758708318076

Epoch: 6| Step: 8
Training loss: 1.7514476776123047
Validation loss: 2.0740574995676675

Epoch: 6| Step: 9
Training loss: 1.8452603816986084
Validation loss: 2.078733762105306

Epoch: 6| Step: 10
Training loss: 1.8129332065582275
Validation loss: 2.079035917917887

Epoch: 6| Step: 11
Training loss: 2.1860804557800293
Validation loss: 2.0713094075520835

Epoch: 6| Step: 12
Training loss: 1.9464945793151855
Validation loss: 2.064823091030121

Epoch: 6| Step: 13
Training loss: 2.152663469314575
Validation loss: 2.078406810760498

Epoch: 161| Step: 0
Training loss: 2.2143869400024414
Validation loss: 2.070418059825897

Epoch: 6| Step: 1
Training loss: 2.078058958053589
Validation loss: 2.067453404267629

Epoch: 6| Step: 2
Training loss: 1.791996955871582
Validation loss: 2.0764060616493225

Epoch: 6| Step: 3
Training loss: 2.10860013961792
Validation loss: 2.071514149506887

Epoch: 6| Step: 4
Training loss: 1.9002759456634521
Validation loss: 2.0785524050394693

Epoch: 6| Step: 5
Training loss: 1.9900753498077393
Validation loss: 2.085754950841268

Epoch: 6| Step: 6
Training loss: 2.639714241027832
Validation loss: 2.0834237138430276

Epoch: 6| Step: 7
Training loss: 2.0059328079223633
Validation loss: 2.075140436490377

Epoch: 6| Step: 8
Training loss: 1.64484441280365
Validation loss: 2.070823053518931

Epoch: 6| Step: 9
Training loss: 1.2268515825271606
Validation loss: 2.068664312362671

Epoch: 6| Step: 10
Training loss: 1.5571541786193848
Validation loss: 2.061511198679606

Epoch: 6| Step: 11
Training loss: 2.6772780418395996
Validation loss: 2.061675012111664

Epoch: 6| Step: 12
Training loss: 1.9953069686889648
Validation loss: 2.060618778069814

Epoch: 6| Step: 13
Training loss: 2.6045117378234863
Validation loss: 2.057600279649099

Epoch: 162| Step: 0
Training loss: 1.9793925285339355
Validation loss: 2.06606916586558

Epoch: 6| Step: 1
Training loss: 2.1337661743164062
Validation loss: 2.064254800478617

Epoch: 6| Step: 2
Training loss: 1.7463264465332031
Validation loss: 2.068993548552195

Epoch: 6| Step: 3
Training loss: 2.028738498687744
Validation loss: 2.0643063386281333

Epoch: 6| Step: 4
Training loss: 2.135402202606201
Validation loss: 2.0722312927246094

Epoch: 6| Step: 5
Training loss: 1.8848222494125366
Validation loss: 2.08149786790212

Epoch: 6| Step: 6
Training loss: 2.421492576599121
Validation loss: 2.0785365104675293

Epoch: 6| Step: 7
Training loss: 2.1991496086120605
Validation loss: 2.084330598513285

Epoch: 6| Step: 8
Training loss: 2.8092041015625
Validation loss: 2.0746896266937256

Epoch: 6| Step: 9
Training loss: 1.752233862876892
Validation loss: 2.0885520378748574

Epoch: 6| Step: 10
Training loss: 1.316129446029663
Validation loss: 2.0838541984558105

Epoch: 6| Step: 11
Training loss: 1.97542405128479
Validation loss: 2.0671192606290183

Epoch: 6| Step: 12
Training loss: 2.0775442123413086
Validation loss: 2.0732412139574685

Epoch: 6| Step: 13
Training loss: 1.8298592567443848
Validation loss: 2.0758660435676575

Epoch: 163| Step: 0
Training loss: 1.791698932647705
Validation loss: 2.078377286593119

Epoch: 6| Step: 1
Training loss: 2.15020751953125
Validation loss: 2.0778812368710837

Epoch: 6| Step: 2
Training loss: 1.5914490222930908
Validation loss: 2.0841562350591025

Epoch: 6| Step: 3
Training loss: 2.143019676208496
Validation loss: 2.0804123481114707

Epoch: 6| Step: 4
Training loss: 1.3961524963378906
Validation loss: 2.0761592785517373

Epoch: 6| Step: 5
Training loss: 1.5349416732788086
Validation loss: 2.0822831193606057

Epoch: 6| Step: 6
Training loss: 2.7674126625061035
Validation loss: 2.0794014930725098

Epoch: 6| Step: 7
Training loss: 1.9381129741668701
Validation loss: 2.0846415956815085

Epoch: 6| Step: 8
Training loss: 1.7402570247650146
Validation loss: 2.0755860209465027

Epoch: 6| Step: 9
Training loss: 2.1096763610839844
Validation loss: 2.0898912946383157

Epoch: 6| Step: 10
Training loss: 2.0932836532592773
Validation loss: 2.076038956642151

Epoch: 6| Step: 11
Training loss: 2.4445571899414062
Validation loss: 2.0921023885409036

Epoch: 6| Step: 12
Training loss: 2.4602341651916504
Validation loss: 2.0799682339032493

Epoch: 6| Step: 13
Training loss: 1.8254797458648682
Validation loss: 2.0724817514419556

Epoch: 164| Step: 0
Training loss: 2.659663200378418
Validation loss: 2.0727333625157676

Epoch: 6| Step: 1
Training loss: 2.015876531600952
Validation loss: 2.0618887344996133

Epoch: 6| Step: 2
Training loss: 1.4384469985961914
Validation loss: 2.06340084473292

Epoch: 6| Step: 3
Training loss: 2.5717239379882812
Validation loss: 2.0682305693626404

Epoch: 6| Step: 4
Training loss: 2.5448222160339355
Validation loss: 2.070823053518931

Epoch: 6| Step: 5
Training loss: 1.3987793922424316
Validation loss: 2.078648328781128

Epoch: 6| Step: 6
Training loss: 2.1394290924072266
Validation loss: 2.0781437158584595

Epoch: 6| Step: 7
Training loss: 2.261049270629883
Validation loss: 2.085734407107035

Epoch: 6| Step: 8
Training loss: 1.559708595275879
Validation loss: 2.0726619561513266

Epoch: 6| Step: 9
Training loss: 2.2028684616088867
Validation loss: 2.074916879336039

Epoch: 6| Step: 10
Training loss: 1.93767511844635
Validation loss: 2.0834184090296426

Epoch: 6| Step: 11
Training loss: 1.6175613403320312
Validation loss: 2.0773330132166543

Epoch: 6| Step: 12
Training loss: 1.9948856830596924
Validation loss: 2.0888522068659463

Epoch: 6| Step: 13
Training loss: 2.3213212490081787
Validation loss: 2.0832037130991616

Epoch: 165| Step: 0
Training loss: 2.885742425918579
Validation loss: 2.0860442320505777

Epoch: 6| Step: 1
Training loss: 1.8072667121887207
Validation loss: 2.0799302458763123

Epoch: 6| Step: 2
Training loss: 2.2288060188293457
Validation loss: 2.0728235443433127

Epoch: 6| Step: 3
Training loss: 1.8240163326263428
Validation loss: 2.074837307135264

Epoch: 6| Step: 4
Training loss: 1.813232660293579
Validation loss: 2.0756871501604715

Epoch: 6| Step: 5
Training loss: 2.016239643096924
Validation loss: 2.067326009273529

Epoch: 6| Step: 6
Training loss: 2.160384178161621
Validation loss: 2.0734134515126548

Epoch: 6| Step: 7
Training loss: 2.167076826095581
Validation loss: 2.0675072272618613

Epoch: 6| Step: 8
Training loss: 1.9274786710739136
Validation loss: 2.0771896640459695

Epoch: 6| Step: 9
Training loss: 2.007730722427368
Validation loss: 2.0681410431861877

Epoch: 6| Step: 10
Training loss: 2.1655051708221436
Validation loss: 2.0753736893335977

Epoch: 6| Step: 11
Training loss: 1.8921058177947998
Validation loss: 2.082178235054016

Epoch: 6| Step: 12
Training loss: 1.6722850799560547
Validation loss: 2.075936218102773

Epoch: 6| Step: 13
Training loss: 1.5322929620742798
Validation loss: 2.0881776014963784

Epoch: 166| Step: 0
Training loss: 2.118439197540283
Validation loss: 2.089852968851725

Epoch: 6| Step: 1
Training loss: 2.267838478088379
Validation loss: 2.091829776763916

Epoch: 6| Step: 2
Training loss: 2.537341356277466
Validation loss: 2.100448966026306

Epoch: 6| Step: 3
Training loss: 2.2475080490112305
Validation loss: 2.0746323267618814

Epoch: 6| Step: 4
Training loss: 1.7785913944244385
Validation loss: 2.08701499303182

Epoch: 6| Step: 5
Training loss: 1.9437854290008545
Validation loss: 2.0865439573923745

Epoch: 6| Step: 6
Training loss: 1.3578938245773315
Validation loss: 2.1073076327641806

Epoch: 6| Step: 7
Training loss: 2.2562003135681152
Validation loss: 2.0817323525746665

Epoch: 6| Step: 8
Training loss: 1.8995513916015625
Validation loss: 2.0972776412963867

Epoch: 6| Step: 9
Training loss: 1.7601275444030762
Validation loss: 2.0676063100496926

Epoch: 6| Step: 10
Training loss: 1.853982925415039
Validation loss: 2.081852753957113

Epoch: 6| Step: 11
Training loss: 1.7745493650436401
Validation loss: 2.0740113655726113

Epoch: 6| Step: 12
Training loss: 2.5235557556152344
Validation loss: 2.080019950866699

Epoch: 6| Step: 13
Training loss: 1.79153573513031
Validation loss: 2.071679870287577

Epoch: 167| Step: 0
Training loss: 2.1245229244232178
Validation loss: 2.070443868637085

Epoch: 6| Step: 1
Training loss: 2.127241611480713
Validation loss: 2.081685741742452

Epoch: 6| Step: 2
Training loss: 1.5013808012008667
Validation loss: 2.067776342233022

Epoch: 6| Step: 3
Training loss: 1.8641252517700195
Validation loss: 2.072038213411967

Epoch: 6| Step: 4
Training loss: 2.236855983734131
Validation loss: 2.0948049624760947

Epoch: 6| Step: 5
Training loss: 2.0243470668792725
Validation loss: 2.092811942100525

Epoch: 6| Step: 6
Training loss: 1.9624450206756592
Validation loss: 2.080828626950582

Epoch: 6| Step: 7
Training loss: 2.247129440307617
Validation loss: 2.094968934853872

Epoch: 6| Step: 8
Training loss: 1.6629303693771362
Validation loss: 2.0919759472211203

Epoch: 6| Step: 9
Training loss: 2.2840464115142822
Validation loss: 2.088926990826925

Epoch: 6| Step: 10
Training loss: 2.212432861328125
Validation loss: 2.0710201263427734

Epoch: 6| Step: 11
Training loss: 1.7250683307647705
Validation loss: 2.0760412216186523

Epoch: 6| Step: 12
Training loss: 2.1307871341705322
Validation loss: 2.0747848749160767

Epoch: 6| Step: 13
Training loss: 1.9642261266708374
Validation loss: 2.062676946322123

Epoch: 168| Step: 0
Training loss: 1.6694629192352295
Validation loss: 2.067533791065216

Epoch: 6| Step: 1
Training loss: 1.5240542888641357
Validation loss: 2.052686313788096

Epoch: 6| Step: 2
Training loss: 2.5911364555358887
Validation loss: 2.060361603895823

Epoch: 6| Step: 3
Training loss: 2.1676104068756104
Validation loss: 2.0627490480740867

Epoch: 6| Step: 4
Training loss: 2.1321163177490234
Validation loss: 2.072429915269216

Epoch: 6| Step: 5
Training loss: 2.360220432281494
Validation loss: 2.0612269838651023

Epoch: 6| Step: 6
Training loss: 2.49586820602417
Validation loss: 2.0696922540664673

Epoch: 6| Step: 7
Training loss: 1.8356866836547852
Validation loss: 2.066356837749481

Epoch: 6| Step: 8
Training loss: 1.7201495170593262
Validation loss: 2.074499527613322

Epoch: 6| Step: 9
Training loss: 1.7195937633514404
Validation loss: 2.070469617843628

Epoch: 6| Step: 10
Training loss: 1.7529327869415283
Validation loss: 2.081524908542633

Epoch: 6| Step: 11
Training loss: 1.6527290344238281
Validation loss: 2.0797447562217712

Epoch: 6| Step: 12
Training loss: 1.9575233459472656
Validation loss: 2.0845317443211875

Epoch: 6| Step: 13
Training loss: 2.591853380203247
Validation loss: 2.074323574701945

Epoch: 169| Step: 0
Training loss: 1.8874025344848633
Validation loss: 2.079301178455353

Epoch: 6| Step: 1
Training loss: 2.0617382526397705
Validation loss: 2.078508218129476

Epoch: 6| Step: 2
Training loss: 1.8150241374969482
Validation loss: 2.07467120885849

Epoch: 6| Step: 3
Training loss: 2.457144260406494
Validation loss: 2.0933756828308105

Epoch: 6| Step: 4
Training loss: 1.572535753250122
Validation loss: 2.0727672775586448

Epoch: 6| Step: 5
Training loss: 2.091805934906006
Validation loss: 2.077242076396942

Epoch: 6| Step: 6
Training loss: 2.3387277126312256
Validation loss: 2.0693700313568115

Epoch: 6| Step: 7
Training loss: 1.9061671495437622
Validation loss: 2.085376520951589

Epoch: 6| Step: 8
Training loss: 1.955798625946045
Validation loss: 2.076635479927063

Epoch: 6| Step: 9
Training loss: 1.7244492769241333
Validation loss: 2.079037328561147

Epoch: 6| Step: 10
Training loss: 2.2571372985839844
Validation loss: 2.085757533709208

Epoch: 6| Step: 11
Training loss: 1.5334663391113281
Validation loss: 2.070016304651896

Epoch: 6| Step: 12
Training loss: 1.8193447589874268
Validation loss: 2.084191600481669

Epoch: 6| Step: 13
Training loss: 2.33681321144104
Validation loss: 2.0930874347686768

Epoch: 170| Step: 0
Training loss: 2.0035295486450195
Validation loss: 2.0851792693138123

Epoch: 6| Step: 1
Training loss: 2.2683942317962646
Validation loss: 2.0987717111905417

Epoch: 6| Step: 2
Training loss: 2.2929561138153076
Validation loss: 2.1035942435264587

Epoch: 6| Step: 3
Training loss: 2.042466163635254
Validation loss: 2.1077523628870645

Epoch: 6| Step: 4
Training loss: 2.0044898986816406
Validation loss: 2.0899598797162375

Epoch: 6| Step: 5
Training loss: 2.2663989067077637
Validation loss: 2.1054087479909263

Epoch: 6| Step: 6
Training loss: 1.5467984676361084
Validation loss: 2.0996732910474143

Epoch: 6| Step: 7
Training loss: 1.9120347499847412
Validation loss: 2.105481723944346

Epoch: 6| Step: 8
Training loss: 1.9164313077926636
Validation loss: 2.0996854106585183

Epoch: 6| Step: 9
Training loss: 1.8702054023742676
Validation loss: 2.095647911230723

Epoch: 6| Step: 10
Training loss: 1.806176781654358
Validation loss: 2.084002196788788

Epoch: 6| Step: 11
Training loss: 2.072847366333008
Validation loss: 2.0813706119855246

Epoch: 6| Step: 12
Training loss: 2.3414690494537354
Validation loss: 2.0814541776974997

Epoch: 6| Step: 13
Training loss: 1.725858449935913
Validation loss: 2.0726598699887595

Epoch: 171| Step: 0
Training loss: 1.823420763015747
Validation loss: 2.0863423148790994

Epoch: 6| Step: 1
Training loss: 1.9568190574645996
Validation loss: 2.085004210472107

Epoch: 6| Step: 2
Training loss: 2.395293951034546
Validation loss: 2.079286793867747

Epoch: 6| Step: 3
Training loss: 1.9518120288848877
Validation loss: 2.082879960536957

Epoch: 6| Step: 4
Training loss: 2.192998170852661
Validation loss: 2.073374609152476

Epoch: 6| Step: 5
Training loss: 2.5325965881347656
Validation loss: 2.082057217756907

Epoch: 6| Step: 6
Training loss: 1.7282490730285645
Validation loss: 2.077677230040232

Epoch: 6| Step: 7
Training loss: 2.1074211597442627
Validation loss: 2.0848628679911294

Epoch: 6| Step: 8
Training loss: 2.676675796508789
Validation loss: 2.080101788043976

Epoch: 6| Step: 9
Training loss: 1.6574409008026123
Validation loss: 2.0989773273468018

Epoch: 6| Step: 10
Training loss: 1.4019696712493896
Validation loss: 2.087412496407827

Epoch: 6| Step: 11
Training loss: 1.998518705368042
Validation loss: 2.089502513408661

Epoch: 6| Step: 12
Training loss: 2.1179089546203613
Validation loss: 2.0791468222935996

Epoch: 6| Step: 13
Training loss: 2.3401851654052734
Validation loss: 2.086827576160431

Epoch: 172| Step: 0
Training loss: 1.9588911533355713
Validation loss: 2.096850832303365

Epoch: 6| Step: 1
Training loss: 1.65110182762146
Validation loss: 2.0845465660095215

Epoch: 6| Step: 2
Training loss: 1.7355211973190308
Validation loss: 2.084099590778351

Epoch: 6| Step: 3
Training loss: 2.445225238800049
Validation loss: 2.0917338927586875

Epoch: 6| Step: 4
Training loss: 2.2954819202423096
Validation loss: 2.094508945941925

Epoch: 6| Step: 5
Training loss: 1.645577311515808
Validation loss: 2.098596195379893

Epoch: 6| Step: 6
Training loss: 1.5165562629699707
Validation loss: 2.0930226842562356

Epoch: 6| Step: 7
Training loss: 2.3304405212402344
Validation loss: 2.1025338172912598

Epoch: 6| Step: 8
Training loss: 2.3030028343200684
Validation loss: 2.094694435596466

Epoch: 6| Step: 9
Training loss: 1.9132988452911377
Validation loss: 2.0974098841349282

Epoch: 6| Step: 10
Training loss: 1.7539929151535034
Validation loss: 2.101285398006439

Epoch: 6| Step: 11
Training loss: 1.963912010192871
Validation loss: 2.1062302192052207

Epoch: 6| Step: 12
Training loss: 1.923079252243042
Validation loss: 2.101387619972229

Epoch: 6| Step: 13
Training loss: 2.218808174133301
Validation loss: 2.1025299429893494

Epoch: 173| Step: 0
Training loss: 1.9929499626159668
Validation loss: 2.1061209638913474

Epoch: 6| Step: 1
Training loss: 2.6641414165496826
Validation loss: 2.099807322025299

Epoch: 6| Step: 2
Training loss: 1.7038698196411133
Validation loss: 2.098959803581238

Epoch: 6| Step: 3
Training loss: 1.9945138692855835
Validation loss: 2.1135225097338357

Epoch: 6| Step: 4
Training loss: 1.9404475688934326
Validation loss: 2.0946344335873923

Epoch: 6| Step: 5
Training loss: 1.4273399114608765
Validation loss: 2.101092537244161

Epoch: 6| Step: 6
Training loss: 1.97268545627594
Validation loss: 2.1018852988878884

Epoch: 6| Step: 7
Training loss: 2.11411452293396
Validation loss: 2.107786158720652

Epoch: 6| Step: 8
Training loss: 2.1266121864318848
Validation loss: 2.097238381703695

Epoch: 6| Step: 9
Training loss: 1.7706241607666016
Validation loss: 2.0971217354138694

Epoch: 6| Step: 10
Training loss: 1.7086656093597412
Validation loss: 2.105648954709371

Epoch: 6| Step: 11
Training loss: 2.2262158393859863
Validation loss: 2.098221699396769

Epoch: 6| Step: 12
Training loss: 1.7025973796844482
Validation loss: 2.1098164916038513

Epoch: 6| Step: 13
Training loss: 2.404294013977051
Validation loss: 2.0947318871816

Epoch: 174| Step: 0
Training loss: 1.5946946144104004
Validation loss: 2.1047348578770957

Epoch: 6| Step: 1
Training loss: 2.078218936920166
Validation loss: 2.095766603946686

Epoch: 6| Step: 2
Training loss: 2.4543910026550293
Validation loss: 2.1066219210624695

Epoch: 6| Step: 3
Training loss: 1.9265702962875366
Validation loss: 2.1101405223210654

Epoch: 6| Step: 4
Training loss: 1.9546492099761963
Validation loss: 2.1094570557276406

Epoch: 6| Step: 5
Training loss: 2.1023435592651367
Validation loss: 2.1049082279205322

Epoch: 6| Step: 6
Training loss: 1.9474918842315674
Validation loss: 2.111351946989695

Epoch: 6| Step: 7
Training loss: 1.9715518951416016
Validation loss: 2.087799926598867

Epoch: 6| Step: 8
Training loss: 1.9227190017700195
Validation loss: 2.0920767188072205

Epoch: 6| Step: 9
Training loss: 1.8827943801879883
Validation loss: 2.0884971618652344

Epoch: 6| Step: 10
Training loss: 2.0861024856567383
Validation loss: 2.079136768976847

Epoch: 6| Step: 11
Training loss: 2.5665781497955322
Validation loss: 2.0805527766545615

Epoch: 6| Step: 12
Training loss: 2.029407501220703
Validation loss: 2.0789525906244912

Epoch: 6| Step: 13
Training loss: 1.2293131351470947
Validation loss: 2.0860637625058494

Epoch: 175| Step: 0
Training loss: 0.9179003238677979
Validation loss: 2.082835853099823

Epoch: 6| Step: 1
Training loss: 2.1476480960845947
Validation loss: 2.095122297604879

Epoch: 6| Step: 2
Training loss: 2.1873979568481445
Validation loss: 2.0901604096094766

Epoch: 6| Step: 3
Training loss: 2.0913915634155273
Validation loss: 2.103895982106527

Epoch: 6| Step: 4
Training loss: 2.223092555999756
Validation loss: 2.0987494587898254

Epoch: 6| Step: 5
Training loss: 2.4068727493286133
Validation loss: 2.1088524659474692

Epoch: 6| Step: 6
Training loss: 1.7379512786865234
Validation loss: 2.1205312808354697

Epoch: 6| Step: 7
Training loss: 2.04828143119812
Validation loss: 2.1055487791697183

Epoch: 6| Step: 8
Training loss: 2.576293468475342
Validation loss: 2.112507621447245

Epoch: 6| Step: 9
Training loss: 1.8098636865615845
Validation loss: 2.1127243638038635

Epoch: 6| Step: 10
Training loss: 1.7269833087921143
Validation loss: 2.0918575326601663

Epoch: 6| Step: 11
Training loss: 2.0132980346679688
Validation loss: 2.093996206919352

Epoch: 6| Step: 12
Training loss: 2.5310983657836914
Validation loss: 2.0898970564206443

Epoch: 6| Step: 13
Training loss: 1.4011330604553223
Validation loss: 2.0910832484563193

Epoch: 176| Step: 0
Training loss: 2.3292994499206543
Validation loss: 2.0959341128667197

Epoch: 6| Step: 1
Training loss: 1.583927035331726
Validation loss: 2.079639653364817

Epoch: 6| Step: 2
Training loss: 1.8274919986724854
Validation loss: 2.087266127268473

Epoch: 6| Step: 3
Training loss: 1.779217004776001
Validation loss: 2.0886847774187722

Epoch: 6| Step: 4
Training loss: 2.047096014022827
Validation loss: 2.0980195800463357

Epoch: 6| Step: 5
Training loss: 2.1987578868865967
Validation loss: 2.0964240034421286

Epoch: 6| Step: 6
Training loss: 1.904491662979126
Validation loss: 2.110288997491201

Epoch: 6| Step: 7
Training loss: 1.7242265939712524
Validation loss: 2.102535883585612

Epoch: 6| Step: 8
Training loss: 1.6006269454956055
Validation loss: 2.1060709953308105

Epoch: 6| Step: 9
Training loss: 2.4861624240875244
Validation loss: 2.1350645621617637

Epoch: 6| Step: 10
Training loss: 2.0808825492858887
Validation loss: 2.123105446497599

Epoch: 6| Step: 11
Training loss: 2.084937572479248
Validation loss: 2.1125118732452393

Epoch: 6| Step: 12
Training loss: 2.0886950492858887
Validation loss: 2.1269203623135886

Epoch: 6| Step: 13
Training loss: 2.2672290802001953
Validation loss: 2.1030874649683633

Epoch: 177| Step: 0
Training loss: 2.5395421981811523
Validation loss: 2.1248137950897217

Epoch: 6| Step: 1
Training loss: 2.0151634216308594
Validation loss: 2.1017348965009055

Epoch: 6| Step: 2
Training loss: 2.1883621215820312
Validation loss: 2.1085703571637473

Epoch: 6| Step: 3
Training loss: 1.5334272384643555
Validation loss: 2.099847435951233

Epoch: 6| Step: 4
Training loss: 2.2164087295532227
Validation loss: 2.095283548037211

Epoch: 6| Step: 5
Training loss: 1.882583498954773
Validation loss: 2.0882553259531655

Epoch: 6| Step: 6
Training loss: 1.6124799251556396
Validation loss: 2.0848687092463174

Epoch: 6| Step: 7
Training loss: 2.8908371925354004
Validation loss: 2.0846850673357644

Epoch: 6| Step: 8
Training loss: 2.3579089641571045
Validation loss: 2.085784435272217

Epoch: 6| Step: 9
Training loss: 2.097170829772949
Validation loss: 2.0924481749534607

Epoch: 6| Step: 10
Training loss: 2.0890421867370605
Validation loss: 2.0838812390963235

Epoch: 6| Step: 11
Training loss: 2.040372371673584
Validation loss: 2.0899648666381836

Epoch: 6| Step: 12
Training loss: 1.3499157428741455
Validation loss: 2.0834664503733316

Epoch: 6| Step: 13
Training loss: 1.6998448371887207
Validation loss: 2.0896932085355124

Epoch: 178| Step: 0
Training loss: 1.92673659324646
Validation loss: 2.0838168263435364

Epoch: 6| Step: 1
Training loss: 2.3697891235351562
Validation loss: 2.1006168524424234

Epoch: 6| Step: 2
Training loss: 1.325689435005188
Validation loss: 2.094613194465637

Epoch: 6| Step: 3
Training loss: 1.7655646800994873
Validation loss: 2.085386097431183

Epoch: 6| Step: 4
Training loss: 3.0565075874328613
Validation loss: 2.107334772745768

Epoch: 6| Step: 5
Training loss: 1.5528603792190552
Validation loss: 2.1035653352737427

Epoch: 6| Step: 6
Training loss: 1.7524198293685913
Validation loss: 2.1268640756607056

Epoch: 6| Step: 7
Training loss: 1.8586580753326416
Validation loss: 2.0995312531789145

Epoch: 6| Step: 8
Training loss: 2.1997146606445312
Validation loss: 2.110185901323954

Epoch: 6| Step: 9
Training loss: 2.815225124359131
Validation loss: 2.1132380962371826

Epoch: 6| Step: 10
Training loss: 1.6802570819854736
Validation loss: 2.0972392360369363

Epoch: 6| Step: 11
Training loss: 2.1826648712158203
Validation loss: 2.1019259095191956

Epoch: 6| Step: 12
Training loss: 2.106374979019165
Validation loss: 2.1054199735323587

Epoch: 6| Step: 13
Training loss: 2.1201891899108887
Validation loss: 2.094602127869924

Epoch: 179| Step: 0
Training loss: 2.103060245513916
Validation loss: 2.0803071657816568

Epoch: 6| Step: 1
Training loss: 2.15206241607666
Validation loss: 2.074123998483022

Epoch: 6| Step: 2
Training loss: 1.2643665075302124
Validation loss: 2.079751412073771

Epoch: 6| Step: 3
Training loss: 2.2274012565612793
Validation loss: 2.094076653321584

Epoch: 6| Step: 4
Training loss: 2.3177008628845215
Validation loss: 2.0862637758255005

Epoch: 6| Step: 5
Training loss: 1.6052889823913574
Validation loss: 2.088970959186554

Epoch: 6| Step: 6
Training loss: 2.4716196060180664
Validation loss: 2.0807137489318848

Epoch: 6| Step: 7
Training loss: 2.0010595321655273
Validation loss: 2.0811097423235574

Epoch: 6| Step: 8
Training loss: 1.8426365852355957
Validation loss: 2.084319273630778

Epoch: 6| Step: 9
Training loss: 2.533669948577881
Validation loss: 2.0868221124013266

Epoch: 6| Step: 10
Training loss: 2.047581195831299
Validation loss: 2.0914273063341775

Epoch: 6| Step: 11
Training loss: 1.6512620449066162
Validation loss: 2.0920837918917337

Epoch: 6| Step: 12
Training loss: 2.656630516052246
Validation loss: 2.087072014808655

Epoch: 6| Step: 13
Training loss: 1.630171537399292
Validation loss: 2.0865197579065957

Epoch: 180| Step: 0
Training loss: 1.746509075164795
Validation loss: 2.084942122300466

Epoch: 6| Step: 1
Training loss: 1.6907825469970703
Validation loss: 2.0920217434565225

Epoch: 6| Step: 2
Training loss: 1.9433557987213135
Validation loss: 2.0933450063069663

Epoch: 6| Step: 3
Training loss: 1.5982463359832764
Validation loss: 2.084120492140452

Epoch: 6| Step: 4
Training loss: 2.802957057952881
Validation loss: 2.099777360757192

Epoch: 6| Step: 5
Training loss: 1.8709267377853394
Validation loss: 2.083035429318746

Epoch: 6| Step: 6
Training loss: 1.7131924629211426
Validation loss: 2.095761696497599

Epoch: 6| Step: 7
Training loss: 2.060014486312866
Validation loss: 2.097965180873871

Epoch: 6| Step: 8
Training loss: 2.426931381225586
Validation loss: 2.1034899950027466

Epoch: 6| Step: 9
Training loss: 1.9685107469558716
Validation loss: 2.0958054264386496

Epoch: 6| Step: 10
Training loss: 1.7897107601165771
Validation loss: 2.096462925275167

Epoch: 6| Step: 11
Training loss: 2.709404468536377
Validation loss: 2.0949224630991616

Epoch: 6| Step: 12
Training loss: 1.3304839134216309
Validation loss: 2.103998839855194

Epoch: 6| Step: 13
Training loss: 2.200810432434082
Validation loss: 2.0996154149373374

Epoch: 181| Step: 0
Training loss: 2.0474321842193604
Validation loss: 2.1024991869926453

Epoch: 6| Step: 1
Training loss: 1.8542449474334717
Validation loss: 2.094841778278351

Epoch: 6| Step: 2
Training loss: 2.393101453781128
Validation loss: 2.0917892853418985

Epoch: 6| Step: 3
Training loss: 2.543905019760132
Validation loss: 2.101142247517904

Epoch: 6| Step: 4
Training loss: 2.024606227874756
Validation loss: 2.08887779712677

Epoch: 6| Step: 5
Training loss: 1.86812424659729
Validation loss: 2.094085931777954

Epoch: 6| Step: 6
Training loss: 1.4783791303634644
Validation loss: 2.0963639418284097

Epoch: 6| Step: 7
Training loss: 1.5322258472442627
Validation loss: 2.090240200360616

Epoch: 6| Step: 8
Training loss: 1.849031925201416
Validation loss: 2.090850373109182

Epoch: 6| Step: 9
Training loss: 1.3517481088638306
Validation loss: 2.103993813196818

Epoch: 6| Step: 10
Training loss: 2.077131748199463
Validation loss: 2.0970770915349326

Epoch: 6| Step: 11
Training loss: 1.6907399892807007
Validation loss: 2.103040337562561

Epoch: 6| Step: 12
Training loss: 2.978178024291992
Validation loss: 2.1068883339564004

Epoch: 6| Step: 13
Training loss: 2.0817108154296875
Validation loss: 2.1093007723490396

Epoch: 182| Step: 0
Training loss: 1.3511985540390015
Validation loss: 2.092581629753113

Epoch: 6| Step: 1
Training loss: 1.7080292701721191
Validation loss: 2.094125509262085

Epoch: 6| Step: 2
Training loss: 2.490490674972534
Validation loss: 2.101041237513224

Epoch: 6| Step: 3
Training loss: 1.9553368091583252
Validation loss: 2.107920785744985

Epoch: 6| Step: 4
Training loss: 2.189258098602295
Validation loss: 2.0930063724517822

Epoch: 6| Step: 5
Training loss: 1.821117639541626
Validation loss: 2.100724935531616

Epoch: 6| Step: 6
Training loss: 2.0160083770751953
Validation loss: 2.1100483338038125

Epoch: 6| Step: 7
Training loss: 1.98490309715271
Validation loss: 2.103607972462972

Epoch: 6| Step: 8
Training loss: 2.6931262016296387
Validation loss: 2.1189476251602173

Epoch: 6| Step: 9
Training loss: 2.2177155017852783
Validation loss: 2.114922900994619

Epoch: 6| Step: 10
Training loss: 2.006291627883911
Validation loss: 2.1137365102767944

Epoch: 6| Step: 11
Training loss: 2.422724485397339
Validation loss: 2.1060214042663574

Epoch: 6| Step: 12
Training loss: 1.770749807357788
Validation loss: 2.1062603195508323

Epoch: 6| Step: 13
Training loss: 1.2325078248977661
Validation loss: 2.0956498781840005

Epoch: 183| Step: 0
Training loss: 2.126178741455078
Validation loss: 2.1103118856747947

Epoch: 6| Step: 1
Training loss: 1.3614678382873535
Validation loss: 2.0950671831766763

Epoch: 6| Step: 2
Training loss: 2.008026599884033
Validation loss: 2.1051998337109885

Epoch: 6| Step: 3
Training loss: 1.7197239398956299
Validation loss: 2.103249470392863

Epoch: 6| Step: 4
Training loss: 2.3636507987976074
Validation loss: 2.089345137278239

Epoch: 6| Step: 5
Training loss: 1.192615032196045
Validation loss: 2.1045674085617065

Epoch: 6| Step: 6
Training loss: 1.5223751068115234
Validation loss: 2.1139043172200522

Epoch: 6| Step: 7
Training loss: 2.155426025390625
Validation loss: 2.107174495855967

Epoch: 6| Step: 8
Training loss: 2.0121824741363525
Validation loss: 2.0958449244499207

Epoch: 6| Step: 9
Training loss: 1.4652721881866455
Validation loss: 2.1006000638008118

Epoch: 6| Step: 10
Training loss: 2.320511817932129
Validation loss: 2.0933666229248047

Epoch: 6| Step: 11
Training loss: 2.366342067718506
Validation loss: 2.0909631053606668

Epoch: 6| Step: 12
Training loss: 2.755171775817871
Validation loss: 2.100714683532715

Epoch: 6| Step: 13
Training loss: 2.079406261444092
Validation loss: 2.094851632912954

Epoch: 184| Step: 0
Training loss: 1.987959384918213
Validation loss: 2.0972501834233603

Epoch: 6| Step: 1
Training loss: 1.6786010265350342
Validation loss: 2.0926924546559653

Epoch: 6| Step: 2
Training loss: 1.3290412425994873
Validation loss: 2.1064372460047402

Epoch: 6| Step: 3
Training loss: 2.5572118759155273
Validation loss: 2.106039901574453

Epoch: 6| Step: 4
Training loss: 2.330103635787964
Validation loss: 2.1043062607447305

Epoch: 6| Step: 5
Training loss: 1.6821799278259277
Validation loss: 2.116310099760691

Epoch: 6| Step: 6
Training loss: 2.3989038467407227
Validation loss: 2.1166170835494995

Epoch: 6| Step: 7
Training loss: 2.8547749519348145
Validation loss: 2.1138466199239097

Epoch: 6| Step: 8
Training loss: 1.5381488800048828
Validation loss: 2.1106635332107544

Epoch: 6| Step: 9
Training loss: 1.8173664808273315
Validation loss: 2.127989888191223

Epoch: 6| Step: 10
Training loss: 1.9188001155853271
Validation loss: 2.1145376563072205

Epoch: 6| Step: 11
Training loss: 1.8302966356277466
Validation loss: 2.1132513284683228

Epoch: 6| Step: 12
Training loss: 2.0683958530426025
Validation loss: 2.1088405648867288

Epoch: 6| Step: 13
Training loss: 1.74131441116333
Validation loss: 2.0977323055267334

Epoch: 185| Step: 0
Training loss: 2.640598773956299
Validation loss: 2.1089415351549783

Epoch: 6| Step: 1
Training loss: 1.6822075843811035
Validation loss: 2.1007224917411804

Epoch: 6| Step: 2
Training loss: 2.0483694076538086
Validation loss: 2.084592580795288

Epoch: 6| Step: 3
Training loss: 1.635522723197937
Validation loss: 2.088732957839966

Epoch: 6| Step: 4
Training loss: 2.526688814163208
Validation loss: 2.0975064436594644

Epoch: 6| Step: 5
Training loss: 1.8349272012710571
Validation loss: 2.080991188685099

Epoch: 6| Step: 6
Training loss: 1.353928565979004
Validation loss: 2.0992578665415444

Epoch: 6| Step: 7
Training loss: 2.215252161026001
Validation loss: 2.0993753472963967

Epoch: 6| Step: 8
Training loss: 1.3610504865646362
Validation loss: 2.1029832561810813

Epoch: 6| Step: 9
Training loss: 1.9078912734985352
Validation loss: 2.1054769357045493

Epoch: 6| Step: 10
Training loss: 1.7898458242416382
Validation loss: 2.10952357451121

Epoch: 6| Step: 11
Training loss: 2.6203689575195312
Validation loss: 2.116513470808665

Epoch: 6| Step: 12
Training loss: 2.1966986656188965
Validation loss: 2.1040175954500833

Epoch: 6| Step: 13
Training loss: 2.014305591583252
Validation loss: 2.1098280946413674

Epoch: 186| Step: 0
Training loss: 1.2979629039764404
Validation loss: 2.1143945852915444

Epoch: 6| Step: 1
Training loss: 1.785618782043457
Validation loss: 2.105520486831665

Epoch: 6| Step: 2
Training loss: 1.986215591430664
Validation loss: 2.1221420764923096

Epoch: 6| Step: 3
Training loss: 1.6402192115783691
Validation loss: 2.1176697413126626

Epoch: 6| Step: 4
Training loss: 1.9070963859558105
Validation loss: 2.127894182999929

Epoch: 6| Step: 5
Training loss: 2.345526933670044
Validation loss: 2.102539579073588

Epoch: 6| Step: 6
Training loss: 2.3763556480407715
Validation loss: 2.1128455797831216

Epoch: 6| Step: 7
Training loss: 2.4223389625549316
Validation loss: 2.116525888442993

Epoch: 6| Step: 8
Training loss: 1.9558311700820923
Validation loss: 2.1067134936650596

Epoch: 6| Step: 9
Training loss: 1.4879188537597656
Validation loss: 2.087404648462931

Epoch: 6| Step: 10
Training loss: 1.6886622905731201
Validation loss: 2.0960588653882346

Epoch: 6| Step: 11
Training loss: 2.237569808959961
Validation loss: 2.113574961821238

Epoch: 6| Step: 12
Training loss: 2.117338180541992
Validation loss: 2.0915226141611734

Epoch: 6| Step: 13
Training loss: 2.565176010131836
Validation loss: 2.0885736544926963

Epoch: 187| Step: 0
Training loss: 1.60841703414917
Validation loss: 2.088416655858358

Epoch: 6| Step: 1
Training loss: 2.580486536026001
Validation loss: 2.090041677157084

Epoch: 6| Step: 2
Training loss: 2.067753314971924
Validation loss: 2.0973030726114907

Epoch: 6| Step: 3
Training loss: 1.4845846891403198
Validation loss: 2.1011074980099997

Epoch: 6| Step: 4
Training loss: 1.9853883981704712
Validation loss: 2.099388579527537

Epoch: 6| Step: 5
Training loss: 2.133208751678467
Validation loss: 2.111667295296987

Epoch: 6| Step: 6
Training loss: 1.6845084428787231
Validation loss: 2.109546661376953

Epoch: 6| Step: 7
Training loss: 2.0046539306640625
Validation loss: 2.1173571149508157

Epoch: 6| Step: 8
Training loss: 1.9483975172042847
Validation loss: 2.113365411758423

Epoch: 6| Step: 9
Training loss: 1.637795090675354
Validation loss: 2.1164108316103616

Epoch: 6| Step: 10
Training loss: 2.089782953262329
Validation loss: 2.1087642709414163

Epoch: 6| Step: 11
Training loss: 1.9206624031066895
Validation loss: 2.105043192704519

Epoch: 6| Step: 12
Training loss: 2.445378541946411
Validation loss: 2.1080695192019143

Epoch: 6| Step: 13
Training loss: 2.0617880821228027
Validation loss: 2.1098687648773193

Epoch: 188| Step: 0
Training loss: 1.2713634967803955
Validation loss: 2.0965320467948914

Epoch: 6| Step: 1
Training loss: 1.5624690055847168
Validation loss: 2.1029558579126992

Epoch: 6| Step: 2
Training loss: 2.3257815837860107
Validation loss: 2.11219322681427

Epoch: 6| Step: 3
Training loss: 2.158515691757202
Validation loss: 2.104604641596476

Epoch: 6| Step: 4
Training loss: 1.8217592239379883
Validation loss: 2.1065082947413125

Epoch: 6| Step: 5
Training loss: 2.6025023460388184
Validation loss: 2.0893365343411765

Epoch: 6| Step: 6
Training loss: 1.664713740348816
Validation loss: 2.0948579907417297

Epoch: 6| Step: 7
Training loss: 2.352421283721924
Validation loss: 2.09773459037145

Epoch: 6| Step: 8
Training loss: 2.4474360942840576
Validation loss: 2.103883902231852

Epoch: 6| Step: 9
Training loss: 2.101163864135742
Validation loss: 2.0960972905158997

Epoch: 6| Step: 10
Training loss: 1.7016234397888184
Validation loss: 2.104781746864319

Epoch: 6| Step: 11
Training loss: 1.6972086429595947
Validation loss: 2.092278997103373

Epoch: 6| Step: 12
Training loss: 2.2144813537597656
Validation loss: 2.097002645333608

Epoch: 6| Step: 13
Training loss: 1.4786903858184814
Validation loss: 2.1028772989908853

Epoch: 189| Step: 0
Training loss: 1.9810603857040405
Validation loss: 2.1007772286732993

Epoch: 6| Step: 1
Training loss: 2.2900757789611816
Validation loss: 2.104344964027405

Epoch: 6| Step: 2
Training loss: 1.940882921218872
Validation loss: 2.1195682485898337

Epoch: 6| Step: 3
Training loss: 2.068113088607788
Validation loss: 2.104948341846466

Epoch: 6| Step: 4
Training loss: 2.5402560234069824
Validation loss: 2.1034533182779946

Epoch: 6| Step: 5
Training loss: 2.100681781768799
Validation loss: 2.099769135316213

Epoch: 6| Step: 6
Training loss: 1.8888970613479614
Validation loss: 2.1041707595189414

Epoch: 6| Step: 7
Training loss: 2.014949083328247
Validation loss: 2.1266435782114663

Epoch: 6| Step: 8
Training loss: 1.6185048818588257
Validation loss: 2.1172727743784585

Epoch: 6| Step: 9
Training loss: 1.686837911605835
Validation loss: 2.1067309776941934

Epoch: 6| Step: 10
Training loss: 1.6380292177200317
Validation loss: 2.1105677684148154

Epoch: 6| Step: 11
Training loss: 1.7648168802261353
Validation loss: 2.123813529809316

Epoch: 6| Step: 12
Training loss: 1.7522319555282593
Validation loss: 2.1182892123858132

Epoch: 6| Step: 13
Training loss: 1.875296711921692
Validation loss: 2.107824921607971

Epoch: 190| Step: 0
Training loss: 1.499924898147583
Validation loss: 2.115766406059265

Epoch: 6| Step: 1
Training loss: 2.0869734287261963
Validation loss: 2.103105982144674

Epoch: 6| Step: 2
Training loss: 2.167201280593872
Validation loss: 2.0929558674494424

Epoch: 6| Step: 3
Training loss: 2.3149871826171875
Validation loss: 2.0988110303878784

Epoch: 6| Step: 4
Training loss: 1.972078800201416
Validation loss: 2.088292201360067

Epoch: 6| Step: 5
Training loss: 2.1610944271087646
Validation loss: 2.0984810988108316

Epoch: 6| Step: 6
Training loss: 1.9306178092956543
Validation loss: 2.1106563011805215

Epoch: 6| Step: 7
Training loss: 1.5564923286437988
Validation loss: 2.107429345448812

Epoch: 6| Step: 8
Training loss: 1.501753807067871
Validation loss: 2.1053872108459473

Epoch: 6| Step: 9
Training loss: 1.851589322090149
Validation loss: 2.114257335662842

Epoch: 6| Step: 10
Training loss: 2.5081887245178223
Validation loss: 2.1004615823427835

Epoch: 6| Step: 11
Training loss: 1.9391775131225586
Validation loss: 2.1008365750312805

Epoch: 6| Step: 12
Training loss: 2.088369369506836
Validation loss: 2.100213328997294

Epoch: 6| Step: 13
Training loss: 1.774236798286438
Validation loss: 2.1052740017573037

Epoch: 191| Step: 0
Training loss: 1.6159112453460693
Validation loss: 2.1124175786972046

Epoch: 6| Step: 1
Training loss: 1.6721851825714111
Validation loss: 2.1054277817408242

Epoch: 6| Step: 2
Training loss: 1.8386142253875732
Validation loss: 2.105999787648519

Epoch: 6| Step: 3
Training loss: 1.949642539024353
Validation loss: 2.119217574596405

Epoch: 6| Step: 4
Training loss: 2.1385040283203125
Validation loss: 2.1199817061424255

Epoch: 6| Step: 5
Training loss: 1.7801710367202759
Validation loss: 2.1165855725606284

Epoch: 6| Step: 6
Training loss: 3.0653414726257324
Validation loss: 2.113279620806376

Epoch: 6| Step: 7
Training loss: 1.9571069478988647
Validation loss: 2.115791161855062

Epoch: 6| Step: 8
Training loss: 1.5767903327941895
Validation loss: 2.116751233736674

Epoch: 6| Step: 9
Training loss: 1.660243034362793
Validation loss: 2.1146886746088662

Epoch: 6| Step: 10
Training loss: 1.4749058485031128
Validation loss: 2.1121012767155967

Epoch: 6| Step: 11
Training loss: 1.934734582901001
Validation loss: 2.1150545279184976

Epoch: 6| Step: 12
Training loss: 1.8967161178588867
Validation loss: 2.125624120235443

Epoch: 6| Step: 13
Training loss: 2.7869925498962402
Validation loss: 2.1087077061335244

Epoch: 192| Step: 0
Training loss: 2.2792649269104004
Validation loss: 2.11527814467748

Epoch: 6| Step: 1
Training loss: 1.4241154193878174
Validation loss: 2.1155713200569153

Epoch: 6| Step: 2
Training loss: 1.8591033220291138
Validation loss: 2.114528258641561

Epoch: 6| Step: 3
Training loss: 2.6070480346679688
Validation loss: 2.126347561677297

Epoch: 6| Step: 4
Training loss: 1.2420833110809326
Validation loss: 2.1211579044659934

Epoch: 6| Step: 5
Training loss: 1.9717687368392944
Validation loss: 2.1157026489575705

Epoch: 6| Step: 6
Training loss: 1.8438001871109009
Validation loss: 2.1228712995847068

Epoch: 6| Step: 7
Training loss: 1.7534921169281006
Validation loss: 2.1106104453404746

Epoch: 6| Step: 8
Training loss: 1.4944257736206055
Validation loss: 2.0934143463770547

Epoch: 6| Step: 9
Training loss: 2.4932303428649902
Validation loss: 2.107594887415568

Epoch: 6| Step: 10
Training loss: 2.424219846725464
Validation loss: 2.1008872985839844

Epoch: 6| Step: 11
Training loss: 1.3188893795013428
Validation loss: 2.1156859397888184

Epoch: 6| Step: 12
Training loss: 1.7518978118896484
Validation loss: 2.108081658681234

Epoch: 6| Step: 13
Training loss: 2.7965567111968994
Validation loss: 2.1048267086346946

Epoch: 193| Step: 0
Training loss: 2.054229974746704
Validation loss: 2.1153258879979453

Epoch: 6| Step: 1
Training loss: 1.9114359617233276
Validation loss: 2.1085853576660156

Epoch: 6| Step: 2
Training loss: 1.6131532192230225
Validation loss: 2.122296690940857

Epoch: 6| Step: 3
Training loss: 1.5709278583526611
Validation loss: 2.1194584369659424

Epoch: 6| Step: 4
Training loss: 2.3647637367248535
Validation loss: 2.1254843870798745

Epoch: 6| Step: 5
Training loss: 1.6055617332458496
Validation loss: 2.123288889726003

Epoch: 6| Step: 6
Training loss: 2.2814319133758545
Validation loss: 2.125875095526377

Epoch: 6| Step: 7
Training loss: 1.6900439262390137
Validation loss: 2.1256858110427856

Epoch: 6| Step: 8
Training loss: 1.9999264478683472
Validation loss: 2.120485166708628

Epoch: 6| Step: 9
Training loss: 1.3732576370239258
Validation loss: 2.1213674743970237

Epoch: 6| Step: 10
Training loss: 2.263183116912842
Validation loss: 2.1310798128445945

Epoch: 6| Step: 11
Training loss: 2.081669330596924
Validation loss: 2.119119187196096

Epoch: 6| Step: 12
Training loss: 2.1348917484283447
Validation loss: 2.1229098637898765

Epoch: 6| Step: 13
Training loss: 2.335585355758667
Validation loss: 2.1283951004346213

Epoch: 194| Step: 0
Training loss: 1.7582058906555176
Validation loss: 2.113525847593943

Epoch: 6| Step: 1
Training loss: 1.8192270994186401
Validation loss: 2.1324300169944763

Epoch: 6| Step: 2
Training loss: 2.0517396926879883
Validation loss: 2.1178789734840393

Epoch: 6| Step: 3
Training loss: 2.192983627319336
Validation loss: 2.118180970350901

Epoch: 6| Step: 4
Training loss: 1.76543128490448
Validation loss: 2.1191649238268533

Epoch: 6| Step: 5
Training loss: 1.6952178478240967
Validation loss: 2.1117586294809976

Epoch: 6| Step: 6
Training loss: 1.398634672164917
Validation loss: 2.1173753142356873

Epoch: 6| Step: 7
Training loss: 1.9749436378479004
Validation loss: 2.1046916842460632

Epoch: 6| Step: 8
Training loss: 2.2657618522644043
Validation loss: 2.1088254849116006

Epoch: 6| Step: 9
Training loss: 1.7261173725128174
Validation loss: 2.1020578145980835

Epoch: 6| Step: 10
Training loss: 2.7477245330810547
Validation loss: 2.116725424925486

Epoch: 6| Step: 11
Training loss: 1.8401753902435303
Validation loss: 2.110529681046804

Epoch: 6| Step: 12
Training loss: 1.7047892808914185
Validation loss: 2.1355897188186646

Epoch: 6| Step: 13
Training loss: 2.3055667877197266
Validation loss: 2.1225799123446145

Epoch: 195| Step: 0
Training loss: 1.6417871713638306
Validation loss: 2.119798262914022

Epoch: 6| Step: 1
Training loss: 1.9168310165405273
Validation loss: 2.125515083471934

Epoch: 6| Step: 2
Training loss: 2.1413817405700684
Validation loss: 2.108683009942373

Epoch: 6| Step: 3
Training loss: 2.29227614402771
Validation loss: 2.1194652915000916

Epoch: 6| Step: 4
Training loss: 2.5749518871307373
Validation loss: 2.1190765301386514

Epoch: 6| Step: 5
Training loss: 1.8367023468017578
Validation loss: 2.115807374318441

Epoch: 6| Step: 6
Training loss: 1.9809622764587402
Validation loss: 2.1190879742304483

Epoch: 6| Step: 7
Training loss: 1.8833088874816895
Validation loss: 2.1281915307044983

Epoch: 6| Step: 8
Training loss: 1.2328176498413086
Validation loss: 2.116257826487223

Epoch: 6| Step: 9
Training loss: 2.2048587799072266
Validation loss: 2.117229779561361

Epoch: 6| Step: 10
Training loss: 2.374656915664673
Validation loss: 2.098333160082499

Epoch: 6| Step: 11
Training loss: 1.73590886592865
Validation loss: 2.1097652514775596

Epoch: 6| Step: 12
Training loss: 2.0169553756713867
Validation loss: 2.11469558874766

Epoch: 6| Step: 13
Training loss: 1.8203669786453247
Validation loss: 2.090792457262675

Epoch: 196| Step: 0
Training loss: 1.4621832370758057
Validation loss: 2.113603413105011

Epoch: 6| Step: 1
Training loss: 2.16379976272583
Validation loss: 2.11723659435908

Epoch: 6| Step: 2
Training loss: 2.449901580810547
Validation loss: 2.113027294476827

Epoch: 6| Step: 3
Training loss: 1.9226861000061035
Validation loss: 2.115228513876597

Epoch: 6| Step: 4
Training loss: 1.8957172632217407
Validation loss: 2.114596724510193

Epoch: 6| Step: 5
Training loss: 2.2876741886138916
Validation loss: 2.118392546971639

Epoch: 6| Step: 6
Training loss: 2.2277991771698
Validation loss: 2.120098869005839

Epoch: 6| Step: 7
Training loss: 1.5469515323638916
Validation loss: 2.1172713239987693

Epoch: 6| Step: 8
Training loss: 1.6570813655853271
Validation loss: 2.1330617864926658

Epoch: 6| Step: 9
Training loss: 1.9272710084915161
Validation loss: 2.13187183936437

Epoch: 6| Step: 10
Training loss: 2.1828033924102783
Validation loss: 2.120554288228353

Epoch: 6| Step: 11
Training loss: 1.5750939846038818
Validation loss: 2.1409854690233865

Epoch: 6| Step: 12
Training loss: 1.4969103336334229
Validation loss: 2.1176841457684836

Epoch: 6| Step: 13
Training loss: 2.4273970127105713
Validation loss: 2.1149579683939614

Epoch: 197| Step: 0
Training loss: 2.034255266189575
Validation loss: 2.12712029616038

Epoch: 6| Step: 1
Training loss: 1.6475410461425781
Validation loss: 2.1252973477045694

Epoch: 6| Step: 2
Training loss: 2.040987968444824
Validation loss: 2.1136308113733926

Epoch: 6| Step: 3
Training loss: 1.4688584804534912
Validation loss: 2.119154433409373

Epoch: 6| Step: 4
Training loss: 1.464066743850708
Validation loss: 2.133644918600718

Epoch: 6| Step: 5
Training loss: 1.8944547176361084
Validation loss: 2.120668133099874

Epoch: 6| Step: 6
Training loss: 2.252487897872925
Validation loss: 2.1314542293548584

Epoch: 6| Step: 7
Training loss: 2.476924419403076
Validation loss: 2.131049076716105

Epoch: 6| Step: 8
Training loss: 1.3015846014022827
Validation loss: 2.1229675014813743

Epoch: 6| Step: 9
Training loss: 1.9996609687805176
Validation loss: 2.1296159823735556

Epoch: 6| Step: 10
Training loss: 2.518378973007202
Validation loss: 2.121915320555369

Epoch: 6| Step: 11
Training loss: 2.0342860221862793
Validation loss: 2.11830602089564

Epoch: 6| Step: 12
Training loss: 2.008444309234619
Validation loss: 2.116695682207743

Epoch: 6| Step: 13
Training loss: 1.9280436038970947
Validation loss: 2.1126118501027427

Epoch: 198| Step: 0
Training loss: 2.8330044746398926
Validation loss: 2.112480123837789

Epoch: 6| Step: 1
Training loss: 2.0432660579681396
Validation loss: 2.121587594350179

Epoch: 6| Step: 2
Training loss: 2.240041971206665
Validation loss: 2.1037000815073648

Epoch: 6| Step: 3
Training loss: 1.6569645404815674
Validation loss: 2.1074206829071045

Epoch: 6| Step: 4
Training loss: 1.1163240671157837
Validation loss: 2.1217867533365884

Epoch: 6| Step: 5
Training loss: 1.6593711376190186
Validation loss: 2.118038773536682

Epoch: 6| Step: 6
Training loss: 2.9118895530700684
Validation loss: 2.1194113890329995

Epoch: 6| Step: 7
Training loss: 1.8412444591522217
Validation loss: 2.1170823176701865

Epoch: 6| Step: 8
Training loss: 1.9883426427841187
Validation loss: 2.1213550170262656

Epoch: 6| Step: 9
Training loss: 1.563550591468811
Validation loss: 2.1151453852653503

Epoch: 6| Step: 10
Training loss: 2.029348373413086
Validation loss: 2.1157179474830627

Epoch: 6| Step: 11
Training loss: 2.0290653705596924
Validation loss: 2.121802349885305

Epoch: 6| Step: 12
Training loss: 1.4603793621063232
Validation loss: 2.140070060888926

Epoch: 6| Step: 13
Training loss: 1.834979772567749
Validation loss: 2.1169626712799072

Epoch: 199| Step: 0
Training loss: 2.2269723415374756
Validation loss: 2.134184976418813

Epoch: 6| Step: 1
Training loss: 1.6879560947418213
Validation loss: 2.1335821946461997

Epoch: 6| Step: 2
Training loss: 1.4509825706481934
Validation loss: 2.1338533560434976

Epoch: 6| Step: 3
Training loss: 2.2993996143341064
Validation loss: 2.1324525674184165

Epoch: 6| Step: 4
Training loss: 1.7210949659347534
Validation loss: 2.1357685724894204

Epoch: 6| Step: 5
Training loss: 1.8980538845062256
Validation loss: 2.1156389315923056

Epoch: 6| Step: 6
Training loss: 2.050339937210083
Validation loss: 2.13019331296285

Epoch: 6| Step: 7
Training loss: 1.6469844579696655
Validation loss: 2.131998280684153

Epoch: 6| Step: 8
Training loss: 1.6129896640777588
Validation loss: 2.1351523796717324

Epoch: 6| Step: 9
Training loss: 2.4437665939331055
Validation loss: 2.147004266579946

Epoch: 6| Step: 10
Training loss: 2.665088176727295
Validation loss: 2.136384129524231

Epoch: 6| Step: 11
Training loss: 1.8378901481628418
Validation loss: 2.137804170449575

Epoch: 6| Step: 12
Training loss: 2.448923349380493
Validation loss: 2.1394272645314536

Epoch: 6| Step: 13
Training loss: 1.2357943058013916
Validation loss: 2.1235556999842324

Epoch: 200| Step: 0
Training loss: 1.968379259109497
Validation loss: 2.123423675696055

Epoch: 6| Step: 1
Training loss: 1.9258469343185425
Validation loss: 2.1201213200887046

Epoch: 6| Step: 2
Training loss: 1.8433254957199097
Validation loss: 2.1239891250928244

Epoch: 6| Step: 3
Training loss: 2.0624160766601562
Validation loss: 2.118699312210083

Epoch: 6| Step: 4
Training loss: 2.4236197471618652
Validation loss: 2.122448960940043

Epoch: 6| Step: 5
Training loss: 2.0431034564971924
Validation loss: 2.1110337177912393

Epoch: 6| Step: 6
Training loss: 1.6620776653289795
Validation loss: 2.1235803365707397

Epoch: 6| Step: 7
Training loss: 2.0946950912475586
Validation loss: 2.1097711523373923

Epoch: 6| Step: 8
Training loss: 1.8270951509475708
Validation loss: 2.1161628564198813

Epoch: 6| Step: 9
Training loss: 2.206733226776123
Validation loss: 2.129328986008962

Epoch: 6| Step: 10
Training loss: 2.264683723449707
Validation loss: 2.1295610070228577

Epoch: 6| Step: 11
Training loss: 1.5758311748504639
Validation loss: 2.1204821864763894

Epoch: 6| Step: 12
Training loss: 1.897420883178711
Validation loss: 2.1328290502230325

Epoch: 6| Step: 13
Training loss: 1.85435152053833
Validation loss: 2.1402806838353476

Testing loss: 1.7213627914730594

Epoch: 1| Step: 0
Training loss: 4.790434947166304
Validation loss: 5.881436467770278

Epoch: 5| Step: 1
Training loss: 6.350150351358878
Validation loss: 5.879928483095586

Epoch: 5| Step: 2
Training loss: 5.500833274834759
Validation loss: 5.8784816314007085

Epoch: 5| Step: 3
Training loss: 6.354753844433303
Validation loss: 5.876898864515891

Epoch: 5| Step: 4
Training loss: 6.414535300593379
Validation loss: 5.875377439995532

Epoch: 5| Step: 5
Training loss: 6.326638581696303
Validation loss: 5.873752102989161

Epoch: 5| Step: 6
Training loss: 6.331000082312141
Validation loss: 5.872063416900384

Epoch: 5| Step: 7
Training loss: 5.064972450354648
Validation loss: 5.8704813157461615

Epoch: 5| Step: 8
Training loss: 6.292727484124283
Validation loss: 5.868774064095602

Epoch: 5| Step: 9
Training loss: 6.253080295629807
Validation loss: 5.8670504063396445

Epoch: 5| Step: 10
Training loss: 5.6922440079201735
Validation loss: 5.865295159169645

Epoch: 5| Step: 11
Training loss: 7.071096943707157
Validation loss: 5.863499762088925

Epoch: 2| Step: 0
Training loss: 5.914840667203826
Validation loss: 5.861638478824253

Epoch: 5| Step: 1
Training loss: 6.468237520271836
Validation loss: 5.859725032947477

Epoch: 5| Step: 2
Training loss: 6.505779631190957
Validation loss: 5.857676213156921

Epoch: 5| Step: 3
Training loss: 5.177098888882985
Validation loss: 5.855668505416329

Epoch: 5| Step: 4
Training loss: 5.735459560349825
Validation loss: 5.853478939979652

Epoch: 5| Step: 5
Training loss: 5.3856114168450695
Validation loss: 5.851315162500669

Epoch: 5| Step: 6
Training loss: 5.924897642901303
Validation loss: 5.848976348097367

Epoch: 5| Step: 7
Training loss: 6.1337743697334375
Validation loss: 5.846642048862584

Epoch: 5| Step: 8
Training loss: 5.500137327387125
Validation loss: 5.844107251458401

Epoch: 5| Step: 9
Training loss: 6.4980691829719275
Validation loss: 5.841450209786166

Epoch: 5| Step: 10
Training loss: 6.34935584217767
Validation loss: 5.838610532719162

Epoch: 5| Step: 11
Training loss: 5.224303437304278
Validation loss: 5.835701715965317

Epoch: 3| Step: 0
Training loss: 6.0344071895584355
Validation loss: 5.8328242306847224

Epoch: 5| Step: 1
Training loss: 5.696342218191253
Validation loss: 5.829520493958191

Epoch: 5| Step: 2
Training loss: 6.3311474022643965
Validation loss: 5.826014732418166

Epoch: 5| Step: 3
Training loss: 5.456534907187836
Validation loss: 5.822584250782249

Epoch: 5| Step: 4
Training loss: 6.321789845879785
Validation loss: 5.818765094053234

Epoch: 5| Step: 5
Training loss: 5.686165789376647
Validation loss: 5.815070264221757

Epoch: 5| Step: 6
Training loss: 5.48326895230576
Validation loss: 5.810831534628255

Epoch: 5| Step: 7
Training loss: 5.462411749713956
Validation loss: 5.806429266651626

Epoch: 5| Step: 8
Training loss: 6.354839085088094
Validation loss: 5.8018352334507775

Epoch: 5| Step: 9
Training loss: 5.959584813857795
Validation loss: 5.797164484766314

Epoch: 5| Step: 10
Training loss: 6.2190812372525635
Validation loss: 5.792332160459753

Epoch: 5| Step: 11
Training loss: 6.4437282025387566
Validation loss: 5.787080845165448

Epoch: 4| Step: 0
Training loss: 6.9253160865891115
Validation loss: 5.7815035240327335

Epoch: 5| Step: 1
Training loss: 6.126951879781512
Validation loss: 5.775948205254221

Epoch: 5| Step: 2
Training loss: 6.523927809511744
Validation loss: 5.77042771255472

Epoch: 5| Step: 3
Training loss: 5.704200789482127
Validation loss: 5.764163437653485

Epoch: 5| Step: 4
Training loss: 5.471064312202147
Validation loss: 5.757860560499117

Epoch: 5| Step: 5
Training loss: 4.6741878436452104
Validation loss: 5.751417648700849

Epoch: 5| Step: 6
Training loss: 5.851511976688596
Validation loss: 5.744966591333117

Epoch: 5| Step: 7
Training loss: 6.606684288106617
Validation loss: 5.738362518836102

Epoch: 5| Step: 8
Training loss: 5.441560128275592
Validation loss: 5.731345870818785

Epoch: 5| Step: 9
Training loss: 5.2054027806819
Validation loss: 5.724439073760677

Epoch: 5| Step: 10
Training loss: 5.58699147344662
Validation loss: 5.71745142411365

Epoch: 5| Step: 11
Training loss: 6.073501201086537
Validation loss: 5.710162790809414

Epoch: 5| Step: 0
Training loss: 5.005620086701804
Validation loss: 5.702707193477391

Epoch: 5| Step: 1
Training loss: 5.440286207523957
Validation loss: 5.695267651402706

Epoch: 5| Step: 2
Training loss: 5.764636774324344
Validation loss: 5.687912880579535

Epoch: 5| Step: 3
Training loss: 5.542901305526962
Validation loss: 5.6802494168143935

Epoch: 5| Step: 4
Training loss: 5.234087489188772
Validation loss: 5.672668366514911

Epoch: 5| Step: 5
Training loss: 5.607208258513831
Validation loss: 5.664561970053042

Epoch: 5| Step: 6
Training loss: 6.795427400622341
Validation loss: 5.6566348638672705

Epoch: 5| Step: 7
Training loss: 6.176525208688495
Validation loss: 5.648249903123044

Epoch: 5| Step: 8
Training loss: 6.455341688076198
Validation loss: 5.639898131816843

Epoch: 5| Step: 9
Training loss: 5.702501905709383
Validation loss: 5.6311427095941395

Epoch: 5| Step: 10
Training loss: 5.568895756347784
Validation loss: 5.6228438165484045

Epoch: 5| Step: 11
Training loss: 5.9282935794716085
Validation loss: 5.614423840391829

Epoch: 6| Step: 0
Training loss: 5.4056479058126055
Validation loss: 5.6059426208174425

Epoch: 5| Step: 1
Training loss: 5.090914446035269
Validation loss: 5.597300619118514

Epoch: 5| Step: 2
Training loss: 6.542641957790583
Validation loss: 5.588872626590431

Epoch: 5| Step: 3
Training loss: 5.5803753243286405
Validation loss: 5.581063316785761

Epoch: 5| Step: 4
Training loss: 5.578633774066573
Validation loss: 5.572636066633566

Epoch: 5| Step: 5
Training loss: 5.001334393777704
Validation loss: 5.565079908685529

Epoch: 5| Step: 6
Training loss: 5.784455601456771
Validation loss: 5.557204222296545

Epoch: 5| Step: 7
Training loss: 6.288195665222346
Validation loss: 5.549253449905936

Epoch: 5| Step: 8
Training loss: 6.258918807288562
Validation loss: 5.541753397767211

Epoch: 5| Step: 9
Training loss: 5.16994843802205
Validation loss: 5.5338815797612995

Epoch: 5| Step: 10
Training loss: 5.639910792734225
Validation loss: 5.526736017792121

Epoch: 5| Step: 11
Training loss: 5.411470924144951
Validation loss: 5.519330934712853

Epoch: 7| Step: 0
Training loss: 5.026361400600176
Validation loss: 5.512191063722093

Epoch: 5| Step: 1
Training loss: 6.430462888961307
Validation loss: 5.505286010174941

Epoch: 5| Step: 2
Training loss: 5.75928212513711
Validation loss: 5.4982508204269065

Epoch: 5| Step: 3
Training loss: 6.206196598618442
Validation loss: 5.491193012984168

Epoch: 5| Step: 4
Training loss: 5.479748461721154
Validation loss: 5.484203085962708

Epoch: 5| Step: 5
Training loss: 4.865560734299313
Validation loss: 5.4772636628624545

Epoch: 5| Step: 6
Training loss: 5.884698532537385
Validation loss: 5.470383042424836

Epoch: 5| Step: 7
Training loss: 5.210841965836224
Validation loss: 5.463414123132542

Epoch: 5| Step: 8
Training loss: 5.8015604748094365
Validation loss: 5.456696077911089

Epoch: 5| Step: 9
Training loss: 5.7501505956417
Validation loss: 5.449786835914328

Epoch: 5| Step: 10
Training loss: 5.235342181225273
Validation loss: 5.442954108668102

Epoch: 5| Step: 11
Training loss: 3.913122981871697
Validation loss: 5.4363979596700185

Epoch: 8| Step: 0
Training loss: 5.8289437626540535
Validation loss: 5.4297789348706775

Epoch: 5| Step: 1
Training loss: 5.277506206731069
Validation loss: 5.423720493697952

Epoch: 5| Step: 2
Training loss: 5.696909739377418
Validation loss: 5.417748074299311

Epoch: 5| Step: 3
Training loss: 6.109875748739827
Validation loss: 5.41140479635632

Epoch: 5| Step: 4
Training loss: 5.525249955404927
Validation loss: 5.405851926462396

Epoch: 5| Step: 5
Training loss: 5.568414351326734
Validation loss: 5.399851303349182

Epoch: 5| Step: 6
Training loss: 5.5508458498176205
Validation loss: 5.394143781436153

Epoch: 5| Step: 7
Training loss: 5.289822995662469
Validation loss: 5.388819568172881

Epoch: 5| Step: 8
Training loss: 5.548634252816922
Validation loss: 5.383324697013203

Epoch: 5| Step: 9
Training loss: 5.201451671149573
Validation loss: 5.378251083169878

Epoch: 5| Step: 10
Training loss: 5.264447994729266
Validation loss: 5.372892313982515

Epoch: 5| Step: 11
Training loss: 4.536514486263879
Validation loss: 5.367590328760036

Epoch: 9| Step: 0
Training loss: 5.065117618441596
Validation loss: 5.362945798055264

Epoch: 5| Step: 1
Training loss: 5.470915621709785
Validation loss: 5.357931018555692

Epoch: 5| Step: 2
Training loss: 5.511313679691272
Validation loss: 5.353172791359172

Epoch: 5| Step: 3
Training loss: 6.20674792363905
Validation loss: 5.3483803674245065

Epoch: 5| Step: 4
Training loss: 4.735139187113499
Validation loss: 5.343382705647582

Epoch: 5| Step: 5
Training loss: 5.935600860440575
Validation loss: 5.33848007786898

Epoch: 5| Step: 6
Training loss: 5.657940859472385
Validation loss: 5.333686133974491

Epoch: 5| Step: 7
Training loss: 5.67555306718281
Validation loss: 5.328722438336809

Epoch: 5| Step: 8
Training loss: 5.324524857361926
Validation loss: 5.323823992097141

Epoch: 5| Step: 9
Training loss: 4.946045446092218
Validation loss: 5.318749174304006

Epoch: 5| Step: 10
Training loss: 5.141713128686302
Validation loss: 5.313617693440608

Epoch: 5| Step: 11
Training loss: 6.467345744130361
Validation loss: 5.3085689511350305

Epoch: 10| Step: 0
Training loss: 5.090250509876297
Validation loss: 5.303496918667888

Epoch: 5| Step: 1
Training loss: 5.903846857333255
Validation loss: 5.298309057027507

Epoch: 5| Step: 2
Training loss: 4.543091133548775
Validation loss: 5.293241576873646

Epoch: 5| Step: 3
Training loss: 4.649146684675574
Validation loss: 5.288205421532654

Epoch: 5| Step: 4
Training loss: 5.641548398166714
Validation loss: 5.28327062802253

Epoch: 5| Step: 5
Training loss: 4.85839794676505
Validation loss: 5.2780624265483125

Epoch: 5| Step: 6
Training loss: 5.545534127655002
Validation loss: 5.273033748287934

Epoch: 5| Step: 7
Training loss: 6.013863759016471
Validation loss: 5.2683089120297435

Epoch: 5| Step: 8
Training loss: 5.962585461401915
Validation loss: 5.262935997003778

Epoch: 5| Step: 9
Training loss: 5.97925797059353
Validation loss: 5.2578907961256425

Epoch: 5| Step: 10
Training loss: 4.806397767889568
Validation loss: 5.252960800700001

Epoch: 5| Step: 11
Training loss: 5.832666340797012
Validation loss: 5.247872383180203

Epoch: 11| Step: 0
Training loss: 4.786383173383248
Validation loss: 5.242844405424208

Epoch: 5| Step: 1
Training loss: 5.453489799648757
Validation loss: 5.237829945139984

Epoch: 5| Step: 2
Training loss: 5.606526706292767
Validation loss: 5.233194261675186

Epoch: 5| Step: 3
Training loss: 4.80961661224542
Validation loss: 5.228574098647898

Epoch: 5| Step: 4
Training loss: 5.822554557154868
Validation loss: 5.223774955880376

Epoch: 5| Step: 5
Training loss: 5.942536797640263
Validation loss: 5.218734901086585

Epoch: 5| Step: 6
Training loss: 5.714490627974381
Validation loss: 5.214090463854632

Epoch: 5| Step: 7
Training loss: 5.368992841954894
Validation loss: 5.209448293236295

Epoch: 5| Step: 8
Training loss: 5.503037567570548
Validation loss: 5.204608480463846

Epoch: 5| Step: 9
Training loss: 5.429979758321503
Validation loss: 5.200453575538579

Epoch: 5| Step: 10
Training loss: 3.7470695489639567
Validation loss: 5.195625903779852

Epoch: 5| Step: 11
Training loss: 6.293724919167845
Validation loss: 5.191336863570489

Epoch: 12| Step: 0
Training loss: 5.842218045248947
Validation loss: 5.18690717136267

Epoch: 5| Step: 1
Training loss: 5.1579398420596805
Validation loss: 5.182401234859855

Epoch: 5| Step: 2
Training loss: 5.304196139740948
Validation loss: 5.178042043521363

Epoch: 5| Step: 3
Training loss: 5.114317576955317
Validation loss: 5.173591666928249

Epoch: 5| Step: 4
Training loss: 5.6170788410909145
Validation loss: 5.1691954971576255

Epoch: 5| Step: 5
Training loss: 5.4824427491141625
Validation loss: 5.164528529943438

Epoch: 5| Step: 6
Training loss: 4.460872081804701
Validation loss: 5.160475015239801

Epoch: 5| Step: 7
Training loss: 5.342120184171617
Validation loss: 5.1562915530601545

Epoch: 5| Step: 8
Training loss: 5.627208530464897
Validation loss: 5.151535761336376

Epoch: 5| Step: 9
Training loss: 5.28705021092682
Validation loss: 5.14709309229943

Epoch: 5| Step: 10
Training loss: 4.883349970419256
Validation loss: 5.142585689675784

Epoch: 5| Step: 11
Training loss: 4.8530676038188805
Validation loss: 5.137893577220425

Epoch: 13| Step: 0
Training loss: 5.123642532327199
Validation loss: 5.133795486841892

Epoch: 5| Step: 1
Training loss: 5.317352771236306
Validation loss: 5.128919955388062

Epoch: 5| Step: 2
Training loss: 5.236137981441414
Validation loss: 5.124624843357438

Epoch: 5| Step: 3
Training loss: 4.340776042905587
Validation loss: 5.119712932468937

Epoch: 5| Step: 4
Training loss: 5.434985050577416
Validation loss: 5.115290609067388

Epoch: 5| Step: 5
Training loss: 5.12866619446071
Validation loss: 5.11107125157326

Epoch: 5| Step: 6
Training loss: 5.80436481795738
Validation loss: 5.1068697434618

Epoch: 5| Step: 7
Training loss: 4.622194006010481
Validation loss: 5.10238670401521

Epoch: 5| Step: 8
Training loss: 6.090502064121348
Validation loss: 5.0978149233466805

Epoch: 5| Step: 9
Training loss: 4.494351338565752
Validation loss: 5.093063776329601

Epoch: 5| Step: 10
Training loss: 5.414694833090601
Validation loss: 5.088979519370638

Epoch: 5| Step: 11
Training loss: 6.567887483513535
Validation loss: 5.084601598440741

Epoch: 14| Step: 0
Training loss: 5.796018470350364
Validation loss: 5.079050948014216

Epoch: 5| Step: 1
Training loss: 5.615313858133114
Validation loss: 5.073726427540855

Epoch: 5| Step: 2
Training loss: 4.67738736643477
Validation loss: 5.0685117293178825

Epoch: 5| Step: 3
Training loss: 5.059266930050917
Validation loss: 5.063610041547183

Epoch: 5| Step: 4
Training loss: 5.154586616169501
Validation loss: 5.058336893087745

Epoch: 5| Step: 5
Training loss: 5.303784210648563
Validation loss: 5.052495938261231

Epoch: 5| Step: 6
Training loss: 5.200317358456319
Validation loss: 5.047325287406177

Epoch: 5| Step: 7
Training loss: 4.909899866516501
Validation loss: 5.042039796982615

Epoch: 5| Step: 8
Training loss: 5.2569523145963055
Validation loss: 5.036490773277651

Epoch: 5| Step: 9
Training loss: 4.957624154972106
Validation loss: 5.031546803125298

Epoch: 5| Step: 10
Training loss: 5.248856737905285
Validation loss: 5.026141311632827

Epoch: 5| Step: 11
Training loss: 3.1935418406394867
Validation loss: 5.0210587884874744

Epoch: 15| Step: 0
Training loss: 4.74691391125942
Validation loss: 5.016164162436323

Epoch: 5| Step: 1
Training loss: 4.854671250123673
Validation loss: 5.0106168245662275

Epoch: 5| Step: 2
Training loss: 5.2937591300444256
Validation loss: 5.006013226634755

Epoch: 5| Step: 3
Training loss: 5.00134983915573
Validation loss: 5.000541625886715

Epoch: 5| Step: 4
Training loss: 4.178874455457778
Validation loss: 4.9956490659521835

Epoch: 5| Step: 5
Training loss: 5.975537978975917
Validation loss: 4.990660647346773

Epoch: 5| Step: 6
Training loss: 5.9851382891773
Validation loss: 4.985239055682939

Epoch: 5| Step: 7
Training loss: 5.3173034493905265
Validation loss: 4.980014683588179

Epoch: 5| Step: 8
Training loss: 4.659786852435617
Validation loss: 4.974853046361864

Epoch: 5| Step: 9
Training loss: 5.0665715175193835
Validation loss: 4.969785882225127

Epoch: 5| Step: 10
Training loss: 4.859557120783359
Validation loss: 4.964902215668922

Epoch: 5| Step: 11
Training loss: 5.335373051336132
Validation loss: 4.9592366504341125

Epoch: 16| Step: 0
Training loss: 5.258370221104839
Validation loss: 4.954124943413415

Epoch: 5| Step: 1
Training loss: 5.2644126696361475
Validation loss: 4.94909968906619

Epoch: 5| Step: 2
Training loss: 4.993333377574993
Validation loss: 4.944069146112321

Epoch: 5| Step: 3
Training loss: 4.579970264588158
Validation loss: 4.93835181924272

Epoch: 5| Step: 4
Training loss: 5.720795479260498
Validation loss: 4.933150029106253

Epoch: 5| Step: 5
Training loss: 5.608205689351733
Validation loss: 4.927874603070459

Epoch: 5| Step: 6
Training loss: 5.2932460360370985
Validation loss: 4.922667521294478

Epoch: 5| Step: 7
Training loss: 4.2246247254691065
Validation loss: 4.918187894778952

Epoch: 5| Step: 8
Training loss: 4.958612044165284
Validation loss: 4.912327106723919

Epoch: 5| Step: 9
Training loss: 5.159409913026369
Validation loss: 4.907208399699493

Epoch: 5| Step: 10
Training loss: 4.468674238936572
Validation loss: 4.901949510262185

Epoch: 5| Step: 11
Training loss: 4.256782505148998
Validation loss: 4.896533585919774

Epoch: 17| Step: 0
Training loss: 4.825284548260308
Validation loss: 4.891463064714626

Epoch: 5| Step: 1
Training loss: 5.203556993668133
Validation loss: 4.886497706011569

Epoch: 5| Step: 2
Training loss: 4.216313315401598
Validation loss: 4.8818478050934715

Epoch: 5| Step: 3
Training loss: 5.440474124330504
Validation loss: 4.877662632543085

Epoch: 5| Step: 4
Training loss: 5.061191719456373
Validation loss: 4.872191622797592

Epoch: 5| Step: 5
Training loss: 4.842034017488277
Validation loss: 4.866121570333818

Epoch: 5| Step: 6
Training loss: 5.660332149828905
Validation loss: 4.860484455528134

Epoch: 5| Step: 7
Training loss: 4.130711849094418
Validation loss: 4.8567106377874305

Epoch: 5| Step: 8
Training loss: 5.684101808263045
Validation loss: 4.8514875978512295

Epoch: 5| Step: 9
Training loss: 4.918482019669038
Validation loss: 4.846056300921613

Epoch: 5| Step: 10
Training loss: 4.998723820901339
Validation loss: 4.840900756257834

Epoch: 5| Step: 11
Training loss: 2.927211030140811
Validation loss: 4.836990940143607

Epoch: 18| Step: 0
Training loss: 5.030955430405303
Validation loss: 4.833672423909612

Epoch: 5| Step: 1
Training loss: 5.231467279185494
Validation loss: 4.825730249424534

Epoch: 5| Step: 2
Training loss: 4.786883058960812
Validation loss: 4.822822440801302

Epoch: 5| Step: 3
Training loss: 5.3685979641757635
Validation loss: 4.819189714753413

Epoch: 5| Step: 4
Training loss: 5.397291754715735
Validation loss: 4.813378670805621

Epoch: 5| Step: 5
Training loss: 4.489008300963928
Validation loss: 4.8065517871495

Epoch: 5| Step: 6
Training loss: 4.475097203397754
Validation loss: 4.801137246242454

Epoch: 5| Step: 7
Training loss: 4.54409002110137
Validation loss: 4.796764989662114

Epoch: 5| Step: 8
Training loss: 4.706538485617807
Validation loss: 4.79162346640753

Epoch: 5| Step: 9
Training loss: 5.109702003583935
Validation loss: 4.786428676287813

Epoch: 5| Step: 10
Training loss: 5.0148508775776
Validation loss: 4.78040229745773

Epoch: 5| Step: 11
Training loss: 4.946160362806157
Validation loss: 4.7765926457881465

Epoch: 19| Step: 0
Training loss: 4.934731588591261
Validation loss: 4.772455712811164

Epoch: 5| Step: 1
Training loss: 5.090811601547554
Validation loss: 4.765625758770325

Epoch: 5| Step: 2
Training loss: 5.021605161907854
Validation loss: 4.75994452786112

Epoch: 5| Step: 3
Training loss: 4.616063738034144
Validation loss: 4.754138774717569

Epoch: 5| Step: 4
Training loss: 5.504434358535114
Validation loss: 4.750059679559943

Epoch: 5| Step: 5
Training loss: 4.695860464203294
Validation loss: 4.744604065833228

Epoch: 5| Step: 6
Training loss: 4.774987002050475
Validation loss: 4.738493417474797

Epoch: 5| Step: 7
Training loss: 4.592227605675074
Validation loss: 4.7325651362565235

Epoch: 5| Step: 8
Training loss: 4.923456622116547
Validation loss: 4.727547030726862

Epoch: 5| Step: 9
Training loss: 4.5157760014085575
Validation loss: 4.722193154388921

Epoch: 5| Step: 10
Training loss: 5.050992909633605
Validation loss: 4.717345552933412

Epoch: 5| Step: 11
Training loss: 3.5789680653821017
Validation loss: 4.712352888001741

Epoch: 20| Step: 0
Training loss: 4.59386999271962
Validation loss: 4.7069832236133715

Epoch: 5| Step: 1
Training loss: 4.896453212156519
Validation loss: 4.7015658948976276

Epoch: 5| Step: 2
Training loss: 4.683945388892566
Validation loss: 4.696375510195569

Epoch: 5| Step: 3
Training loss: 5.115748545297251
Validation loss: 4.690703784908717

Epoch: 5| Step: 4
Training loss: 4.310861690970468
Validation loss: 4.68626535050388

Epoch: 5| Step: 5
Training loss: 5.2085713243151925
Validation loss: 4.6807593135585215

Epoch: 5| Step: 6
Training loss: 4.574330915709533
Validation loss: 4.674935808608041

Epoch: 5| Step: 7
Training loss: 4.506172185852943
Validation loss: 4.669875764426373

Epoch: 5| Step: 8
Training loss: 3.7178459030155264
Validation loss: 4.664309121858119

Epoch: 5| Step: 9
Training loss: 5.418134030858112
Validation loss: 4.658485763367007

Epoch: 5| Step: 10
Training loss: 5.463484316204549
Validation loss: 4.653210691299623

Epoch: 5| Step: 11
Training loss: 5.3229006215231305
Validation loss: 4.647090830063067

Epoch: 21| Step: 0
Training loss: 3.8886809490257015
Validation loss: 4.641306209550888

Epoch: 5| Step: 1
Training loss: 4.080200135053407
Validation loss: 4.6362097701042035

Epoch: 5| Step: 2
Training loss: 4.763332928888203
Validation loss: 4.631074722068874

Epoch: 5| Step: 3
Training loss: 4.543979417552808
Validation loss: 4.625543141476807

Epoch: 5| Step: 4
Training loss: 5.591522796341515
Validation loss: 4.620158832098696

Epoch: 5| Step: 5
Training loss: 4.825281781284781
Validation loss: 4.6140931287438525

Epoch: 5| Step: 6
Training loss: 5.315101076530469
Validation loss: 4.60893559489735

Epoch: 5| Step: 7
Training loss: 4.577395019660814
Validation loss: 4.603309140101525

Epoch: 5| Step: 8
Training loss: 4.246650273297905
Validation loss: 4.5977970902254635

Epoch: 5| Step: 9
Training loss: 4.931899358930704
Validation loss: 4.591738513492896

Epoch: 5| Step: 10
Training loss: 4.618418985979262
Validation loss: 4.586526541542411

Epoch: 5| Step: 11
Training loss: 6.761959078268063
Validation loss: 4.580615124810872

Epoch: 22| Step: 0
Training loss: 5.0687338042088665
Validation loss: 4.575785011766279

Epoch: 5| Step: 1
Training loss: 5.5041843449638135
Validation loss: 4.5697347248611715

Epoch: 5| Step: 2
Training loss: 5.1772409129834145
Validation loss: 4.563336382807532

Epoch: 5| Step: 3
Training loss: 4.321634762831703
Validation loss: 4.5568746045327435

Epoch: 5| Step: 4
Training loss: 4.029783233416556
Validation loss: 4.551345122280212

Epoch: 5| Step: 5
Training loss: 4.733499880444074
Validation loss: 4.5455132594073415

Epoch: 5| Step: 6
Training loss: 3.8008129906578687
Validation loss: 4.540086299185257

Epoch: 5| Step: 7
Training loss: 4.317212612039026
Validation loss: 4.5343998889691335

Epoch: 5| Step: 8
Training loss: 4.9662551854507635
Validation loss: 4.528267415933143

Epoch: 5| Step: 9
Training loss: 4.384312337371832
Validation loss: 4.522443549058853

Epoch: 5| Step: 10
Training loss: 4.455975393258166
Validation loss: 4.516512272132595

Epoch: 5| Step: 11
Training loss: 6.239197818350041
Validation loss: 4.511626130621747

Epoch: 23| Step: 0
Training loss: 4.081100839690279
Validation loss: 4.5054312399108705

Epoch: 5| Step: 1
Training loss: 4.732882928686319
Validation loss: 4.499484619897993

Epoch: 5| Step: 2
Training loss: 3.9761885971225883
Validation loss: 4.494074465515973

Epoch: 5| Step: 3
Training loss: 5.750667699110748
Validation loss: 4.4885872918312

Epoch: 5| Step: 4
Training loss: 4.40139698046412
Validation loss: 4.482461204162349

Epoch: 5| Step: 5
Training loss: 4.844407953675129
Validation loss: 4.476255778067761

Epoch: 5| Step: 6
Training loss: 4.348323472639206
Validation loss: 4.47052903764808

Epoch: 5| Step: 7
Training loss: 4.6856706228448255
Validation loss: 4.465233791043039

Epoch: 5| Step: 8
Training loss: 4.4400167715769685
Validation loss: 4.459865848871547

Epoch: 5| Step: 9
Training loss: 4.768779773544219
Validation loss: 4.45431226106403

Epoch: 5| Step: 10
Training loss: 4.728933902787561
Validation loss: 4.4485049056053665

Epoch: 5| Step: 11
Training loss: 2.2658847002289075
Validation loss: 4.442473637537178

Epoch: 24| Step: 0
Training loss: 4.219516712736484
Validation loss: 4.438062721714045

Epoch: 5| Step: 1
Training loss: 4.797458277725162
Validation loss: 4.432823560143129

Epoch: 5| Step: 2
Training loss: 4.797301829480781
Validation loss: 4.427180653418669

Epoch: 5| Step: 3
Training loss: 3.891865115173833
Validation loss: 4.422042502026616

Epoch: 5| Step: 4
Training loss: 3.7450480508204564
Validation loss: 4.416198015840127

Epoch: 5| Step: 5
Training loss: 4.23363237801567
Validation loss: 4.411049143411719

Epoch: 5| Step: 6
Training loss: 4.223650085766702
Validation loss: 4.405713411669715

Epoch: 5| Step: 7
Training loss: 4.58956282384962
Validation loss: 4.40021735001306

Epoch: 5| Step: 8
Training loss: 4.598021927829804
Validation loss: 4.395243761863366

Epoch: 5| Step: 9
Training loss: 4.748294323303147
Validation loss: 4.390688921733474

Epoch: 5| Step: 10
Training loss: 5.497797264815668
Validation loss: 4.384930086060607

Epoch: 5| Step: 11
Training loss: 5.881371148024119
Validation loss: 4.379035160595763

Epoch: 25| Step: 0
Training loss: 4.202926007066271
Validation loss: 4.373548966423202

Epoch: 5| Step: 1
Training loss: 4.6610778766993635
Validation loss: 4.368243672650162

Epoch: 5| Step: 2
Training loss: 4.534183157944773
Validation loss: 4.3630422118578

Epoch: 5| Step: 3
Training loss: 4.524948052376843
Validation loss: 4.357990341232425

Epoch: 5| Step: 4
Training loss: 4.130857528072873
Validation loss: 4.35159383017164

Epoch: 5| Step: 5
Training loss: 4.531384064894422
Validation loss: 4.345031439488562

Epoch: 5| Step: 6
Training loss: 4.793209502027036
Validation loss: 4.339833517946667

Epoch: 5| Step: 7
Training loss: 4.306387287049837
Validation loss: 4.333750597720219

Epoch: 5| Step: 8
Training loss: 4.6898421476108165
Validation loss: 4.328717977251322

Epoch: 5| Step: 9
Training loss: 4.449560833809279
Validation loss: 4.321942289301632

Epoch: 5| Step: 10
Training loss: 4.196789813508598
Validation loss: 4.317543940833167

Epoch: 5| Step: 11
Training loss: 5.162498433480014
Validation loss: 4.31110669170778

Epoch: 26| Step: 0
Training loss: 4.706599476112243
Validation loss: 4.306054592266662

Epoch: 5| Step: 1
Training loss: 4.424924843360028
Validation loss: 4.30024368941109

Epoch: 5| Step: 2
Training loss: 4.8183451243709
Validation loss: 4.294967755206641

Epoch: 5| Step: 3
Training loss: 4.102944335678914
Validation loss: 4.289580415215533

Epoch: 5| Step: 4
Training loss: 4.528879953320887
Validation loss: 4.284154343064506

Epoch: 5| Step: 5
Training loss: 4.5212580515036
Validation loss: 4.279282695654495

Epoch: 5| Step: 6
Training loss: 5.046677718425493
Validation loss: 4.274203690321227

Epoch: 5| Step: 7
Training loss: 4.734273072206207
Validation loss: 4.267450619676133

Epoch: 5| Step: 8
Training loss: 4.074492608994641
Validation loss: 4.261670656337867

Epoch: 5| Step: 9
Training loss: 4.177009537473845
Validation loss: 4.256675918487396

Epoch: 5| Step: 10
Training loss: 3.193282174860031
Validation loss: 4.250649846640082

Epoch: 5| Step: 11
Training loss: 3.8348324926366772
Validation loss: 4.245946376831781

Epoch: 27| Step: 0
Training loss: 4.769865556977536
Validation loss: 4.240618992433202

Epoch: 5| Step: 1
Training loss: 4.962272596553898
Validation loss: 4.2356297509587675

Epoch: 5| Step: 2
Training loss: 4.369735602230235
Validation loss: 4.230049687861027

Epoch: 5| Step: 3
Training loss: 4.108457281526833
Validation loss: 4.224047125632937

Epoch: 5| Step: 4
Training loss: 4.562813525989115
Validation loss: 4.219339094122694

Epoch: 5| Step: 5
Training loss: 4.7474828374872855
Validation loss: 4.21408749295863

Epoch: 5| Step: 6
Training loss: 3.3704921145763547
Validation loss: 4.209921375130947

Epoch: 5| Step: 7
Training loss: 4.235981428874717
Validation loss: 4.203593160656668

Epoch: 5| Step: 8
Training loss: 4.0416350740331906
Validation loss: 4.198753281456044

Epoch: 5| Step: 9
Training loss: 4.009722576105372
Validation loss: 4.194029684732118

Epoch: 5| Step: 10
Training loss: 4.619283907000798
Validation loss: 4.189478307261511

Epoch: 5| Step: 11
Training loss: 3.0198930175728824
Validation loss: 4.183881420285913

Epoch: 28| Step: 0
Training loss: 4.178656734392266
Validation loss: 4.179315844022524

Epoch: 5| Step: 1
Training loss: 4.8358426431402695
Validation loss: 4.174361879537842

Epoch: 5| Step: 2
Training loss: 4.228920664781224
Validation loss: 4.169336387155457

Epoch: 5| Step: 3
Training loss: 4.397882836502528
Validation loss: 4.163762767088484

Epoch: 5| Step: 4
Training loss: 3.4280132821447813
Validation loss: 4.159067331996745

Epoch: 5| Step: 5
Training loss: 4.207542622124247
Validation loss: 4.15373386620614

Epoch: 5| Step: 6
Training loss: 4.070215030022984
Validation loss: 4.1489355886214065

Epoch: 5| Step: 7
Training loss: 4.695365715042658
Validation loss: 4.1438743892927485

Epoch: 5| Step: 8
Training loss: 4.708950756179246
Validation loss: 4.138964914026186

Epoch: 5| Step: 9
Training loss: 4.416686123979043
Validation loss: 4.133811060859465

Epoch: 5| Step: 10
Training loss: 3.953258165370725
Validation loss: 4.128773302471953

Epoch: 5| Step: 11
Training loss: 3.378300783451894
Validation loss: 4.12399682456362

Epoch: 29| Step: 0
Training loss: 4.462791258125192
Validation loss: 4.11956255822692

Epoch: 5| Step: 1
Training loss: 4.354090615419054
Validation loss: 4.114924987257585

Epoch: 5| Step: 2
Training loss: 4.566872369815857
Validation loss: 4.109626220837228

Epoch: 5| Step: 3
Training loss: 3.641036112743761
Validation loss: 4.104222048790823

Epoch: 5| Step: 4
Training loss: 4.3675050022946005
Validation loss: 4.100388538744914

Epoch: 5| Step: 5
Training loss: 5.174606846419798
Validation loss: 4.095041430469074

Epoch: 5| Step: 6
Training loss: 3.939549109708518
Validation loss: 4.090675077624815

Epoch: 5| Step: 7
Training loss: 4.274585967063867
Validation loss: 4.085385739360012

Epoch: 5| Step: 8
Training loss: 4.2998374021084
Validation loss: 4.081032784500491

Epoch: 5| Step: 9
Training loss: 3.249338669615089
Validation loss: 4.076042413293094

Epoch: 5| Step: 10
Training loss: 4.06506759493512
Validation loss: 4.070428096132883

Epoch: 5| Step: 11
Training loss: 3.1291195990776752
Validation loss: 4.065923979172857

Epoch: 30| Step: 0
Training loss: 4.084112806594865
Validation loss: 4.061876513247099

Epoch: 5| Step: 1
Training loss: 4.229597373540505
Validation loss: 4.056398504797966

Epoch: 5| Step: 2
Training loss: 4.690631684991697
Validation loss: 4.050942746667403

Epoch: 5| Step: 3
Training loss: 4.218439387436842
Validation loss: 4.046504741408405

Epoch: 5| Step: 4
Training loss: 4.424581716994425
Validation loss: 4.041582753926038

Epoch: 5| Step: 5
Training loss: 3.8059583730741515
Validation loss: 4.036088868326599

Epoch: 5| Step: 6
Training loss: 4.01290575869579
Validation loss: 4.031968217886885

Epoch: 5| Step: 7
Training loss: 3.6832341234013466
Validation loss: 4.02798878548187

Epoch: 5| Step: 8
Training loss: 4.053279801400034
Validation loss: 4.024252992224009

Epoch: 5| Step: 9
Training loss: 3.582304666415527
Validation loss: 4.01842042768992

Epoch: 5| Step: 10
Training loss: 4.888656268702617
Validation loss: 4.013028043542021

Epoch: 5| Step: 11
Training loss: 4.206942841602411
Validation loss: 4.008563605228458

Epoch: 31| Step: 0
Training loss: 3.537969357251637
Validation loss: 4.004611019972476

Epoch: 5| Step: 1
Training loss: 4.581545024287147
Validation loss: 4.000082626085436

Epoch: 5| Step: 2
Training loss: 4.29345033979811
Validation loss: 3.99364609023478

Epoch: 5| Step: 3
Training loss: 3.4492878717946645
Validation loss: 3.9889788887693816

Epoch: 5| Step: 4
Training loss: 4.3339430062243745
Validation loss: 3.9858366336020596

Epoch: 5| Step: 5
Training loss: 4.2261493361048625
Validation loss: 3.9820604121324292

Epoch: 5| Step: 6
Training loss: 3.833534124892949
Validation loss: 3.9760739738998017

Epoch: 5| Step: 7
Training loss: 4.009333450209127
Validation loss: 3.969679911246678

Epoch: 5| Step: 8
Training loss: 4.111191032824189
Validation loss: 3.964297522891805

Epoch: 5| Step: 9
Training loss: 4.468344930149744
Validation loss: 3.9597833520696253

Epoch: 5| Step: 10
Training loss: 3.961543832451938
Validation loss: 3.9559690123576434

Epoch: 5| Step: 11
Training loss: 5.331339741674893
Validation loss: 3.951972310878496

Epoch: 32| Step: 0
Training loss: 4.625980788702682
Validation loss: 3.9449045606155693

Epoch: 5| Step: 1
Training loss: 4.120887787359158
Validation loss: 3.9390409370910557

Epoch: 5| Step: 2
Training loss: 4.185492418640729
Validation loss: 3.933859772259209

Epoch: 5| Step: 3
Training loss: 4.2870822358195655
Validation loss: 3.9288916771062774

Epoch: 5| Step: 4
Training loss: 4.109490570647917
Validation loss: 3.924213482276479

Epoch: 5| Step: 5
Training loss: 4.26690699277992
Validation loss: 3.9193340444272726

Epoch: 5| Step: 6
Training loss: 3.359948539566878
Validation loss: 3.9139708192891174

Epoch: 5| Step: 7
Training loss: 4.552249000496833
Validation loss: 3.9089770550845304

Epoch: 5| Step: 8
Training loss: 3.759394766612103
Validation loss: 3.9035934468478812

Epoch: 5| Step: 9
Training loss: 3.5693229789863947
Validation loss: 3.898746180115125

Epoch: 5| Step: 10
Training loss: 3.669319204855858
Validation loss: 3.8946495943699775

Epoch: 5| Step: 11
Training loss: 3.4596773010329613
Validation loss: 3.8893526900073168

Epoch: 33| Step: 0
Training loss: 4.471588726997173
Validation loss: 3.8841420538307356

Epoch: 5| Step: 1
Training loss: 3.796184461302348
Validation loss: 3.879847079193382

Epoch: 5| Step: 2
Training loss: 4.4491505873016175
Validation loss: 3.87495075727779

Epoch: 5| Step: 3
Training loss: 3.335972377049824
Validation loss: 3.870299503210651

Epoch: 5| Step: 4
Training loss: 3.9580784096529023
Validation loss: 3.8658860505222283

Epoch: 5| Step: 5
Training loss: 3.708667454329681
Validation loss: 3.860627741978142

Epoch: 5| Step: 6
Training loss: 4.003311217218104
Validation loss: 3.8557591765322847

Epoch: 5| Step: 7
Training loss: 3.0825583197033852
Validation loss: 3.8511768635482353

Epoch: 5| Step: 8
Training loss: 4.092932699605027
Validation loss: 3.8469224414022816

Epoch: 5| Step: 9
Training loss: 3.943754768670973
Validation loss: 3.841806059740546

Epoch: 5| Step: 10
Training loss: 4.858837429130524
Validation loss: 3.8381514003640604

Epoch: 5| Step: 11
Training loss: 3.6462296334091167
Validation loss: 3.832845935035511

Epoch: 34| Step: 0
Training loss: 4.245629700234545
Validation loss: 3.828378484728456

Epoch: 5| Step: 1
Training loss: 3.1669019979738096
Validation loss: 3.823973562441831

Epoch: 5| Step: 2
Training loss: 3.805332387675021
Validation loss: 3.8189992227096545

Epoch: 5| Step: 3
Training loss: 4.2378993833586565
Validation loss: 3.8147206848239454

Epoch: 5| Step: 4
Training loss: 3.736205095497971
Validation loss: 3.8103720401481738

Epoch: 5| Step: 5
Training loss: 3.88258186131214
Validation loss: 3.8052972178394957

Epoch: 5| Step: 6
Training loss: 4.135033628979693
Validation loss: 3.800678352408474

Epoch: 5| Step: 7
Training loss: 4.441097466866166
Validation loss: 3.7965353302320066

Epoch: 5| Step: 8
Training loss: 4.140379930837228
Validation loss: 3.7916326556234887

Epoch: 5| Step: 9
Training loss: 3.642857206969701
Validation loss: 3.7871498497906293

Epoch: 5| Step: 10
Training loss: 4.072295600908858
Validation loss: 3.782333365949157

Epoch: 5| Step: 11
Training loss: 1.680103835916054
Validation loss: 3.7776407350587955

Epoch: 35| Step: 0
Training loss: 4.104195118658649
Validation loss: 3.7731704564768824

Epoch: 5| Step: 1
Training loss: 3.4354580449715533
Validation loss: 3.7689323094410168

Epoch: 5| Step: 2
Training loss: 3.9410382071566565
Validation loss: 3.76443992528778

Epoch: 5| Step: 3
Training loss: 4.0773555570520275
Validation loss: 3.7600100694599323

Epoch: 5| Step: 4
Training loss: 3.5763417427058224
Validation loss: 3.7554004935710585

Epoch: 5| Step: 5
Training loss: 3.8464015096007698
Validation loss: 3.75105744393606

Epoch: 5| Step: 6
Training loss: 3.6823947285964365
Validation loss: 3.7469991147459636

Epoch: 5| Step: 7
Training loss: 3.3191740350611947
Validation loss: 3.7427723798501673

Epoch: 5| Step: 8
Training loss: 4.652245969729697
Validation loss: 3.738114032041507

Epoch: 5| Step: 9
Training loss: 4.139425312642483
Validation loss: 3.733398767448515

Epoch: 5| Step: 10
Training loss: 3.835961988569045
Validation loss: 3.7287695085384116

Epoch: 5| Step: 11
Training loss: 3.6813980305932366
Validation loss: 3.7241466563519388

Epoch: 36| Step: 0
Training loss: 3.8091548565036306
Validation loss: 3.7192998420849386

Epoch: 5| Step: 1
Training loss: 4.053285918795016
Validation loss: 3.7152403717693177

Epoch: 5| Step: 2
Training loss: 3.8536955399428217
Validation loss: 3.710307729722233

Epoch: 5| Step: 3
Training loss: 4.514485040406459
Validation loss: 3.7059572068814766

Epoch: 5| Step: 4
Training loss: 3.467492090283581
Validation loss: 3.7010391144439945

Epoch: 5| Step: 5
Training loss: 3.405813346662898
Validation loss: 3.6963388255711465

Epoch: 5| Step: 6
Training loss: 3.3527997629757658
Validation loss: 3.691532979199335

Epoch: 5| Step: 7
Training loss: 3.6972069430845598
Validation loss: 3.687256584107632

Epoch: 5| Step: 8
Training loss: 3.4968490040539812
Validation loss: 3.682907401928234

Epoch: 5| Step: 9
Training loss: 3.8363325300192357
Validation loss: 3.678302555557543

Epoch: 5| Step: 10
Training loss: 4.439710469471446
Validation loss: 3.6745392298593864

Epoch: 5| Step: 11
Training loss: 3.9915541652257036
Validation loss: 3.6695226073493235

Epoch: 37| Step: 0
Training loss: 3.499080264635874
Validation loss: 3.664912458036093

Epoch: 5| Step: 1
Training loss: 3.854612431222377
Validation loss: 3.660405756080678

Epoch: 5| Step: 2
Training loss: 3.3488757339345168
Validation loss: 3.6560307461459938

Epoch: 5| Step: 3
Training loss: 4.353390759901807
Validation loss: 3.6516679924028557

Epoch: 5| Step: 4
Training loss: 3.2095605724137943
Validation loss: 3.647187720725766

Epoch: 5| Step: 5
Training loss: 3.8461843085916643
Validation loss: 3.642777910114409

Epoch: 5| Step: 6
Training loss: 4.565730166415296
Validation loss: 3.6380247235760907

Epoch: 5| Step: 7
Training loss: 3.4036433894356106
Validation loss: 3.6335143802460275

Epoch: 5| Step: 8
Training loss: 4.214467227158027
Validation loss: 3.6288337056957496

Epoch: 5| Step: 9
Training loss: 2.8063169543598665
Validation loss: 3.6243127631829863

Epoch: 5| Step: 10
Training loss: 3.9975585639294766
Validation loss: 3.6198972819263946

Epoch: 5| Step: 11
Training loss: 4.2255781511729165
Validation loss: 3.6154734351971882

Epoch: 38| Step: 0
Training loss: 3.5226405695204344
Validation loss: 3.611134038550797

Epoch: 5| Step: 1
Training loss: 3.3400247803642515
Validation loss: 3.6070573805403088

Epoch: 5| Step: 2
Training loss: 3.313880057126392
Validation loss: 3.602638900226882

Epoch: 5| Step: 3
Training loss: 4.135987877331792
Validation loss: 3.597847385692952

Epoch: 5| Step: 4
Training loss: 4.201801985231539
Validation loss: 3.593700032646995

Epoch: 5| Step: 5
Training loss: 3.997073056322354
Validation loss: 3.5890724092312682

Epoch: 5| Step: 6
Training loss: 3.390384507330097
Validation loss: 3.584461761750439

Epoch: 5| Step: 7
Training loss: 4.358446247882233
Validation loss: 3.5805768659164836

Epoch: 5| Step: 8
Training loss: 3.547797263811711
Validation loss: 3.5768058377434366

Epoch: 5| Step: 9
Training loss: 2.9914194421086266
Validation loss: 3.570966870686855

Epoch: 5| Step: 10
Training loss: 4.100632800138567
Validation loss: 3.566653557482957

Epoch: 5| Step: 11
Training loss: 2.692607107909941
Validation loss: 3.5623813073981743

Epoch: 39| Step: 0
Training loss: 3.0550138879638267
Validation loss: 3.5584627274624876

Epoch: 5| Step: 1
Training loss: 3.434370107187356
Validation loss: 3.5542980190316764

Epoch: 5| Step: 2
Training loss: 3.9161076789367453
Validation loss: 3.550368293050944

Epoch: 5| Step: 3
Training loss: 3.8326153013598656
Validation loss: 3.547137929886879

Epoch: 5| Step: 4
Training loss: 3.912023321491771
Validation loss: 3.542685545425832

Epoch: 5| Step: 5
Training loss: 3.778475034794065
Validation loss: 3.5384399506498854

Epoch: 5| Step: 6
Training loss: 4.182641698394502
Validation loss: 3.534642376442367

Epoch: 5| Step: 7
Training loss: 3.186669783836766
Validation loss: 3.5305428598674053

Epoch: 5| Step: 8
Training loss: 3.6586565753984814
Validation loss: 3.5261798964902527

Epoch: 5| Step: 9
Training loss: 3.8906159305083796
Validation loss: 3.521885853761671

Epoch: 5| Step: 10
Training loss: 3.3341692988861937
Validation loss: 3.518604891931747

Epoch: 5| Step: 11
Training loss: 4.050563000765319
Validation loss: 3.513033384956275

Epoch: 40| Step: 0
Training loss: 4.01104784205571
Validation loss: 3.5088357034908753

Epoch: 5| Step: 1
Training loss: 3.8025806397885535
Validation loss: 3.504388884409203

Epoch: 5| Step: 2
Training loss: 3.6273968106295094
Validation loss: 3.5000668104924975

Epoch: 5| Step: 3
Training loss: 2.981687923872351
Validation loss: 3.495384488183695

Epoch: 5| Step: 4
Training loss: 3.457653959408769
Validation loss: 3.4912537671287494

Epoch: 5| Step: 5
Training loss: 3.4744931352201425
Validation loss: 3.4866068410725006

Epoch: 5| Step: 6
Training loss: 3.130456204790264
Validation loss: 3.482506597475702

Epoch: 5| Step: 7
Training loss: 4.030182689757051
Validation loss: 3.4786385213370803

Epoch: 5| Step: 8
Training loss: 4.107431394499787
Validation loss: 3.4740885415141607

Epoch: 5| Step: 9
Training loss: 3.2344529884260766
Validation loss: 3.470104520114063

Epoch: 5| Step: 10
Training loss: 3.7690143448214317
Validation loss: 3.4655667668441494

Epoch: 5| Step: 11
Training loss: 3.9002759493501356
Validation loss: 3.4612644290864405

Epoch: 41| Step: 0
Training loss: 3.6393174864626676
Validation loss: 3.456851944520924

Epoch: 5| Step: 1
Training loss: 4.002081091246539
Validation loss: 3.4524858611082307

Epoch: 5| Step: 2
Training loss: 3.432866250521527
Validation loss: 3.4488404622046125

Epoch: 5| Step: 3
Training loss: 3.143595036048916
Validation loss: 3.443923745269618

Epoch: 5| Step: 4
Training loss: 3.403289208071635
Validation loss: 3.439806366745566

Epoch: 5| Step: 5
Training loss: 3.830635960022217
Validation loss: 3.4352784520914206

Epoch: 5| Step: 6
Training loss: 3.217725720435271
Validation loss: 3.4309648526173255

Epoch: 5| Step: 7
Training loss: 3.8692268928103553
Validation loss: 3.4267195921149156

Epoch: 5| Step: 8
Training loss: 3.640461275405837
Validation loss: 3.422758629916341

Epoch: 5| Step: 9
Training loss: 3.559948861044146
Validation loss: 3.4182267852748556

Epoch: 5| Step: 10
Training loss: 3.5560032013062357
Validation loss: 3.414745061805886

Epoch: 5| Step: 11
Training loss: 3.1998309806055945
Validation loss: 3.4100759059068504

Epoch: 42| Step: 0
Training loss: 3.915878027118133
Validation loss: 3.4060846399421982

Epoch: 5| Step: 1
Training loss: 3.142467963905886
Validation loss: 3.4018833507309427

Epoch: 5| Step: 2
Training loss: 3.9200476534535493
Validation loss: 3.397807154166359

Epoch: 5| Step: 3
Training loss: 3.05104413530053
Validation loss: 3.3933677596242635

Epoch: 5| Step: 4
Training loss: 3.737126314498232
Validation loss: 3.3893247358405434

Epoch: 5| Step: 5
Training loss: 3.0955127356674996
Validation loss: 3.3849521416079966

Epoch: 5| Step: 6
Training loss: 2.7912655632381242
Validation loss: 3.38128673512358

Epoch: 5| Step: 7
Training loss: 3.640704894928729
Validation loss: 3.377186782016503

Epoch: 5| Step: 8
Training loss: 3.4095608081063467
Validation loss: 3.3731604908368427

Epoch: 5| Step: 9
Training loss: 4.045268443174538
Validation loss: 3.369329126954205

Epoch: 5| Step: 10
Training loss: 3.7393468217076546
Validation loss: 3.365162717853707

Epoch: 5| Step: 11
Training loss: 3.683511419579669
Validation loss: 3.3610972757991764

Epoch: 43| Step: 0
Training loss: 3.545577294996876
Validation loss: 3.3579925353936844

Epoch: 5| Step: 1
Training loss: 3.381990751401594
Validation loss: 3.357015132769189

Epoch: 5| Step: 2
Training loss: 3.995223053970399
Validation loss: 3.3555731795983577

Epoch: 5| Step: 3
Training loss: 3.468735978381711
Validation loss: 3.3472871224498824

Epoch: 5| Step: 4
Training loss: 2.998552927058662
Validation loss: 3.3418814051847168

Epoch: 5| Step: 5
Training loss: 3.1718103561954876
Validation loss: 3.33733844747074

Epoch: 5| Step: 6
Training loss: 3.184172744991435
Validation loss: 3.3343403169400627

Epoch: 5| Step: 7
Training loss: 3.500411418167234
Validation loss: 3.331291381931987

Epoch: 5| Step: 8
Training loss: 3.294222230012421
Validation loss: 3.327425959427559

Epoch: 5| Step: 9
Training loss: 3.9285865287985975
Validation loss: 3.323418105068386

Epoch: 5| Step: 10
Training loss: 3.8369179150607406
Validation loss: 3.3195111493245526

Epoch: 5| Step: 11
Training loss: 2.3223552510104373
Validation loss: 3.314870016494581

Epoch: 44| Step: 0
Training loss: 2.970770860811431
Validation loss: 3.311016359300714

Epoch: 5| Step: 1
Training loss: 3.874094580421882
Validation loss: 3.307097563967503

Epoch: 5| Step: 2
Training loss: 3.0842110355473396
Validation loss: 3.303449719075956

Epoch: 5| Step: 3
Training loss: 3.95178827294672
Validation loss: 3.299535614871622

Epoch: 5| Step: 4
Training loss: 2.942581975243897
Validation loss: 3.296066465262617

Epoch: 5| Step: 5
Training loss: 2.83738415183303
Validation loss: 3.2922807293741836

Epoch: 5| Step: 6
Training loss: 3.571114548092876
Validation loss: 3.2885165029406216

Epoch: 5| Step: 7
Training loss: 4.063934073056117
Validation loss: 3.2844610850833242

Epoch: 5| Step: 8
Training loss: 3.7127059417015373
Validation loss: 3.280897363757006

Epoch: 5| Step: 9
Training loss: 3.6236197376030757
Validation loss: 3.2773786322427596

Epoch: 5| Step: 10
Training loss: 2.9292608738849037
Validation loss: 3.2737186344543026

Epoch: 5| Step: 11
Training loss: 2.805082160576012
Validation loss: 3.2697330505247173

Epoch: 45| Step: 0
Training loss: 3.6681853241606666
Validation loss: 3.266089161522276

Epoch: 5| Step: 1
Training loss: 2.9847998191892744
Validation loss: 3.2625885311991225

Epoch: 5| Step: 2
Training loss: 3.115219680607977
Validation loss: 3.2594024291306196

Epoch: 5| Step: 3
Training loss: 3.574290790769887
Validation loss: 3.255636377293355

Epoch: 5| Step: 4
Training loss: 3.2709906635390253
Validation loss: 3.2523090370165115

Epoch: 5| Step: 5
Training loss: 3.663150372875358
Validation loss: 3.2485287466195722

Epoch: 5| Step: 6
Training loss: 3.7839366101364926
Validation loss: 3.2449104732277756

Epoch: 5| Step: 7
Training loss: 3.985785978720391
Validation loss: 3.241003250712085

Epoch: 5| Step: 8
Training loss: 3.605180160528481
Validation loss: 3.2374405515458498

Epoch: 5| Step: 9
Training loss: 2.6363375013724206
Validation loss: 3.233642912985032

Epoch: 5| Step: 10
Training loss: 2.8547963656675606
Validation loss: 3.230157937931288

Epoch: 5| Step: 11
Training loss: 2.6512035228031308
Validation loss: 3.226547555219201

Epoch: 46| Step: 0
Training loss: 3.5324291859822883
Validation loss: 3.223052482521506

Epoch: 5| Step: 1
Training loss: 3.394050520206766
Validation loss: 3.219293573152421

Epoch: 5| Step: 2
Training loss: 3.317965910218499
Validation loss: 3.215988350949533

Epoch: 5| Step: 3
Training loss: 2.938156703833916
Validation loss: 3.2121873884080108

Epoch: 5| Step: 4
Training loss: 3.444880659585911
Validation loss: 3.2089345707185157

Epoch: 5| Step: 5
Training loss: 3.390596029272113
Validation loss: 3.206060553041938

Epoch: 5| Step: 6
Training loss: 3.156988831716315
Validation loss: 3.2023084235431782

Epoch: 5| Step: 7
Training loss: 3.343063711880085
Validation loss: 3.1978652568913706

Epoch: 5| Step: 8
Training loss: 3.4589215126981214
Validation loss: 3.195131777103893

Epoch: 5| Step: 9
Training loss: 3.4343301202620236
Validation loss: 3.19229881726134

Epoch: 5| Step: 10
Training loss: 3.4926801159123984
Validation loss: 3.188571622014691

Epoch: 5| Step: 11
Training loss: 2.5272742236802217
Validation loss: 3.1850537250315307

Epoch: 47| Step: 0
Training loss: 3.3404079384578913
Validation loss: 3.1815682175220252

Epoch: 5| Step: 1
Training loss: 3.4345880832814357
Validation loss: 3.1789409313996004

Epoch: 5| Step: 2
Training loss: 3.2609183968467463
Validation loss: 3.175126174362068

Epoch: 5| Step: 3
Training loss: 3.1448571077765233
Validation loss: 3.1715696623212604

Epoch: 5| Step: 4
Training loss: 3.6303709295666575
Validation loss: 3.167729109587563

Epoch: 5| Step: 5
Training loss: 3.5005855070562
Validation loss: 3.1633181163360207

Epoch: 5| Step: 6
Training loss: 3.0742204561919504
Validation loss: 3.1625653370728375

Epoch: 5| Step: 7
Training loss: 2.917436625258952
Validation loss: 3.1567388539954204

Epoch: 5| Step: 8
Training loss: 3.9282006547026027
Validation loss: 3.154378399026711

Epoch: 5| Step: 9
Training loss: 3.3289682099190085
Validation loss: 3.1506967854988006

Epoch: 5| Step: 10
Training loss: 2.7414134962845504
Validation loss: 3.1492926490899618

Epoch: 5| Step: 11
Training loss: 2.823248267785739
Validation loss: 3.146894930562076

Epoch: 48| Step: 0
Training loss: 3.3250182072420613
Validation loss: 3.144312624705999

Epoch: 5| Step: 1
Training loss: 3.4906239945863593
Validation loss: 3.1426568362263403

Epoch: 5| Step: 2
Training loss: 2.963282318477101
Validation loss: 3.1373772917583507

Epoch: 5| Step: 3
Training loss: 3.2732325378199247
Validation loss: 3.133546167781331

Epoch: 5| Step: 4
Training loss: 3.006124443874223
Validation loss: 3.129899615373107

Epoch: 5| Step: 5
Training loss: 3.1184131631533254
Validation loss: 3.1260355886527167

Epoch: 5| Step: 6
Training loss: 3.1914809555592316
Validation loss: 3.1234016272638225

Epoch: 5| Step: 7
Training loss: 3.2766562139655484
Validation loss: 3.121012865274891

Epoch: 5| Step: 8
Training loss: 3.186230724661026
Validation loss: 3.1172258634445336

Epoch: 5| Step: 9
Training loss: 3.4801384183606414
Validation loss: 3.112967372820521

Epoch: 5| Step: 10
Training loss: 3.3744192683422685
Validation loss: 3.1099017861549463

Epoch: 5| Step: 11
Training loss: 4.241439725329539
Validation loss: 3.10652860713488

Epoch: 49| Step: 0
Training loss: 3.2342495364120776
Validation loss: 3.102786119850743

Epoch: 5| Step: 1
Training loss: 3.51685606701061
Validation loss: 3.100365930796705

Epoch: 5| Step: 2
Training loss: 2.8400341531553575
Validation loss: 3.0964664003625186

Epoch: 5| Step: 3
Training loss: 2.8361814619906385
Validation loss: 3.092592382436311

Epoch: 5| Step: 4
Training loss: 3.497697617876456
Validation loss: 3.088704577175931

Epoch: 5| Step: 5
Training loss: 2.933617156198236
Validation loss: 3.0854806135266384

Epoch: 5| Step: 6
Training loss: 3.350986102142247
Validation loss: 3.0821883721988397

Epoch: 5| Step: 7
Training loss: 3.5004641361390916
Validation loss: 3.078286529795177

Epoch: 5| Step: 8
Training loss: 3.4367564003961735
Validation loss: 3.075261958136915

Epoch: 5| Step: 9
Training loss: 3.2715710984307527
Validation loss: 3.0707720120663926

Epoch: 5| Step: 10
Training loss: 2.812482537109415
Validation loss: 3.0683471660019026

Epoch: 5| Step: 11
Training loss: 3.9127068218129906
Validation loss: 3.0649236670764646

Epoch: 50| Step: 0
Training loss: 3.330000800971774
Validation loss: 3.062301145961461

Epoch: 5| Step: 1
Training loss: 3.034856479525574
Validation loss: 3.0595248036633005

Epoch: 5| Step: 2
Training loss: 3.287790229083069
Validation loss: 3.056749075853699

Epoch: 5| Step: 3
Training loss: 2.9465779386369393
Validation loss: 3.0531801177782443

Epoch: 5| Step: 4
Training loss: 3.3714988350185218
Validation loss: 3.0505570684652428

Epoch: 5| Step: 5
Training loss: 3.3565618046957924
Validation loss: 3.0472085786456256

Epoch: 5| Step: 6
Training loss: 2.7789121081333654
Validation loss: 3.0437597928682742

Epoch: 5| Step: 7
Training loss: 3.3644987554322525
Validation loss: 3.0403681927264055

Epoch: 5| Step: 8
Training loss: 3.7055742236316442
Validation loss: 3.0382578944286753

Epoch: 5| Step: 9
Training loss: 2.872171752341975
Validation loss: 3.0356004210763734

Epoch: 5| Step: 10
Training loss: 3.0772479206784467
Validation loss: 3.0317591177360073

Epoch: 5| Step: 11
Training loss: 2.213415691306532
Validation loss: 3.0309489418571856

Epoch: 51| Step: 0
Training loss: 2.2953628441033214
Validation loss: 3.026911104522429

Epoch: 5| Step: 1
Training loss: 2.976587811059543
Validation loss: 3.0282282761479786

Epoch: 5| Step: 2
Training loss: 2.6543687329309256
Validation loss: 3.022688586444222

Epoch: 5| Step: 3
Training loss: 2.9207328971395357
Validation loss: 3.0195827964998756

Epoch: 5| Step: 4
Training loss: 3.404777698626298
Validation loss: 3.021294609017221

Epoch: 5| Step: 5
Training loss: 3.027316343121909
Validation loss: 3.015387000276703

Epoch: 5| Step: 6
Training loss: 3.44566817404831
Validation loss: 3.0100182244286695

Epoch: 5| Step: 7
Training loss: 2.756484968416029
Validation loss: 3.0079284018112227

Epoch: 5| Step: 8
Training loss: 4.005931747578804
Validation loss: 3.00525770802434

Epoch: 5| Step: 9
Training loss: 3.5253535641230647
Validation loss: 3.0039862864744706

Epoch: 5| Step: 10
Training loss: 3.2327168955247734
Validation loss: 2.9991938448288655

Epoch: 5| Step: 11
Training loss: 3.5126054469669676
Validation loss: 2.997182808118885

Epoch: 52| Step: 0
Training loss: 3.114519473710836
Validation loss: 2.994476100494407

Epoch: 5| Step: 1
Training loss: 3.235063415094458
Validation loss: 2.9914191498722764

Epoch: 5| Step: 2
Training loss: 2.867009710885544
Validation loss: 2.9873862029392737

Epoch: 5| Step: 3
Training loss: 2.2897679410402962
Validation loss: 2.984184438802903

Epoch: 5| Step: 4
Training loss: 3.5788590790130423
Validation loss: 2.9807340104772893

Epoch: 5| Step: 5
Training loss: 2.953880576703429
Validation loss: 2.9795818150752553

Epoch: 5| Step: 6
Training loss: 3.0566264289306924
Validation loss: 2.9782228186261532

Epoch: 5| Step: 7
Training loss: 3.327370428232301
Validation loss: 2.9755761255199116

Epoch: 5| Step: 8
Training loss: 3.5303193613022668
Validation loss: 2.9718175666572972

Epoch: 5| Step: 9
Training loss: 3.305614584886368
Validation loss: 2.9687392820198264

Epoch: 5| Step: 10
Training loss: 3.0492405242885283
Validation loss: 2.965685289826614

Epoch: 5| Step: 11
Training loss: 2.1397848220739
Validation loss: 2.9632704140810073

Epoch: 53| Step: 0
Training loss: 3.255687285728409
Validation loss: 2.958965161285297

Epoch: 5| Step: 1
Training loss: 2.7939649392551633
Validation loss: 2.9564318769664233

Epoch: 5| Step: 2
Training loss: 3.0075103211641983
Validation loss: 2.954493573873687

Epoch: 5| Step: 3
Training loss: 2.977052823429368
Validation loss: 2.9523541758113376

Epoch: 5| Step: 4
Training loss: 3.415644996912601
Validation loss: 2.9486917087959332

Epoch: 5| Step: 5
Training loss: 3.480201582496274
Validation loss: 2.947590777353074

Epoch: 5| Step: 6
Training loss: 2.443572769177415
Validation loss: 2.9441687288141214

Epoch: 5| Step: 7
Training loss: 3.022586673222714
Validation loss: 2.940498890944507

Epoch: 5| Step: 8
Training loss: 3.1016193836290293
Validation loss: 2.937821965801029

Epoch: 5| Step: 9
Training loss: 3.0300647738743987
Validation loss: 2.9360222548939325

Epoch: 5| Step: 10
Training loss: 3.382476155605137
Validation loss: 2.9350895494753533

Epoch: 5| Step: 11
Training loss: 2.5175343724985018
Validation loss: 2.9332859776929756

Epoch: 54| Step: 0
Training loss: 3.499365612847965
Validation loss: 2.931881773753877

Epoch: 5| Step: 1
Training loss: 2.1097540620777613
Validation loss: 2.9278679550673092

Epoch: 5| Step: 2
Training loss: 3.009872246499273
Validation loss: 2.9249486515233056

Epoch: 5| Step: 3
Training loss: 3.206517799897891
Validation loss: 2.9214202710609025

Epoch: 5| Step: 4
Training loss: 3.3776816557988294
Validation loss: 2.921938502799703

Epoch: 5| Step: 5
Training loss: 2.994241432836984
Validation loss: 2.9226415040147993

Epoch: 5| Step: 6
Training loss: 2.826894334300634
Validation loss: 2.926144732249796

Epoch: 5| Step: 7
Training loss: 2.7382810758626013
Validation loss: 2.92886399626726

Epoch: 5| Step: 8
Training loss: 3.2190641972046934
Validation loss: 2.9248205219534333

Epoch: 5| Step: 9
Training loss: 3.1681669763526146
Validation loss: 2.9190452731286136

Epoch: 5| Step: 10
Training loss: 3.223349091005781
Validation loss: 2.9107715545055433

Epoch: 5| Step: 11
Training loss: 3.6948195404874076
Validation loss: 2.9074457846430657

Epoch: 55| Step: 0
Training loss: 2.776438719867493
Validation loss: 2.9025846426705497

Epoch: 5| Step: 1
Training loss: 2.8682042951298072
Validation loss: 2.8994436171522113

Epoch: 5| Step: 2
Training loss: 3.6135645739849296
Validation loss: 2.896850449623676

Epoch: 5| Step: 3
Training loss: 3.167299625649606
Validation loss: 2.8941927843666417

Epoch: 5| Step: 4
Training loss: 2.7465375430424754
Validation loss: 2.891686768603926

Epoch: 5| Step: 5
Training loss: 2.8556228783963413
Validation loss: 2.8922467055839305

Epoch: 5| Step: 6
Training loss: 2.966568958802256
Validation loss: 2.8936851009454334

Epoch: 5| Step: 7
Training loss: 3.5768089039568225
Validation loss: 2.8935335458718487

Epoch: 5| Step: 8
Training loss: 2.8734435138981724
Validation loss: 2.8863236531601455

Epoch: 5| Step: 9
Training loss: 2.851825357105803
Validation loss: 2.8798905876247023

Epoch: 5| Step: 10
Training loss: 2.9529890402523655
Validation loss: 2.8769433391928607

Epoch: 5| Step: 11
Training loss: 2.840930085538102
Validation loss: 2.8734480290153446

Epoch: 56| Step: 0
Training loss: 2.972429585180275
Validation loss: 2.874437850781483

Epoch: 5| Step: 1
Training loss: 3.0877090198020274
Validation loss: 2.8771726166731164

Epoch: 5| Step: 2
Training loss: 3.256611627863963
Validation loss: 2.8786773832214307

Epoch: 5| Step: 3
Training loss: 2.966760229571245
Validation loss: 2.8788331055719945

Epoch: 5| Step: 4
Training loss: 2.6077617295193125
Validation loss: 2.8803103718034624

Epoch: 5| Step: 5
Training loss: 2.7729280245629777
Validation loss: 2.8800542191196628

Epoch: 5| Step: 6
Training loss: 3.376380920470495
Validation loss: 2.873992691627374

Epoch: 5| Step: 7
Training loss: 2.5451714370633205
Validation loss: 2.865818497209969

Epoch: 5| Step: 8
Training loss: 3.064470552452039
Validation loss: 2.860819078456775

Epoch: 5| Step: 9
Training loss: 3.347782733535292
Validation loss: 2.8573098950575857

Epoch: 5| Step: 10
Training loss: 3.194823285918498
Validation loss: 2.8540947318003504

Epoch: 5| Step: 11
Training loss: 1.9872085885471993
Validation loss: 2.85023714035426

Epoch: 57| Step: 0
Training loss: 2.888774547596406
Validation loss: 2.8472179987857995

Epoch: 5| Step: 1
Training loss: 3.3945754984439853
Validation loss: 2.844670583373023

Epoch: 5| Step: 2
Training loss: 3.3147986731520023
Validation loss: 2.8414415274971763

Epoch: 5| Step: 3
Training loss: 2.2522783400253323
Validation loss: 2.840247830795518

Epoch: 5| Step: 4
Training loss: 2.6525915645287776
Validation loss: 2.8377942294022125

Epoch: 5| Step: 5
Training loss: 3.185623813544358
Validation loss: 2.8379844900291666

Epoch: 5| Step: 6
Training loss: 3.325998549276966
Validation loss: 2.833819604733124

Epoch: 5| Step: 7
Training loss: 2.719039024687479
Validation loss: 2.832677030050689

Epoch: 5| Step: 8
Training loss: 3.08402163753019
Validation loss: 2.8290701107146243

Epoch: 5| Step: 9
Training loss: 2.9666658815818634
Validation loss: 2.827166997467593

Epoch: 5| Step: 10
Training loss: 2.890702365149237
Validation loss: 2.825674242112175

Epoch: 5| Step: 11
Training loss: 2.3148723496245216
Validation loss: 2.8239526161596573

Epoch: 58| Step: 0
Training loss: 2.7785961047760366
Validation loss: 2.8219883977298696

Epoch: 5| Step: 1
Training loss: 2.6895576406877373
Validation loss: 2.821104704530037

Epoch: 5| Step: 2
Training loss: 2.723955629735204
Validation loss: 2.8235425618870957

Epoch: 5| Step: 3
Training loss: 2.7300577140827813
Validation loss: 2.8212072550496488

Epoch: 5| Step: 4
Training loss: 2.9535681982347275
Validation loss: 2.817464265577009

Epoch: 5| Step: 5
Training loss: 3.040585016377817
Validation loss: 2.8143700952082655

Epoch: 5| Step: 6
Training loss: 3.042122131805253
Validation loss: 2.809874729269332

Epoch: 5| Step: 7
Training loss: 3.194171282012442
Validation loss: 2.806270687364026

Epoch: 5| Step: 8
Training loss: 2.7755361434295693
Validation loss: 2.8052786626450414

Epoch: 5| Step: 9
Training loss: 3.22690726371032
Validation loss: 2.8023701713599025

Epoch: 5| Step: 10
Training loss: 3.1411396572832633
Validation loss: 2.8018949130971187

Epoch: 5| Step: 11
Training loss: 3.4801769198465498
Validation loss: 2.7990423503418924

Epoch: 59| Step: 0
Training loss: 3.081496817276905
Validation loss: 2.797780559852726

Epoch: 5| Step: 1
Training loss: 3.0518731228215064
Validation loss: 2.7970323358534404

Epoch: 5| Step: 2
Training loss: 3.19675925782697
Validation loss: 2.7939886440671184

Epoch: 5| Step: 3
Training loss: 2.751722143373763
Validation loss: 2.7932986049116875

Epoch: 5| Step: 4
Training loss: 2.9619861849321656
Validation loss: 2.789946297632346

Epoch: 5| Step: 5
Training loss: 2.780547406894643
Validation loss: 2.7893798889737282

Epoch: 5| Step: 6
Training loss: 2.860538933667541
Validation loss: 2.7878952290726478

Epoch: 5| Step: 7
Training loss: 2.7334884514251576
Validation loss: 2.7849734880816603

Epoch: 5| Step: 8
Training loss: 2.705155949213318
Validation loss: 2.7835514176374487

Epoch: 5| Step: 9
Training loss: 2.8580939753400343
Validation loss: 2.7815352304337115

Epoch: 5| Step: 10
Training loss: 3.282598018766175
Validation loss: 2.779505196665708

Epoch: 5| Step: 11
Training loss: 2.247774082737491
Validation loss: 2.7758311728044673

Epoch: 60| Step: 0
Training loss: 3.064131788591817
Validation loss: 2.7750753661680982

Epoch: 5| Step: 1
Training loss: 2.803053696149707
Validation loss: 2.774245399684414

Epoch: 5| Step: 2
Training loss: 2.8000767458888394
Validation loss: 2.7727047413397554

Epoch: 5| Step: 3
Training loss: 2.6953842374681116
Validation loss: 2.771237972418678

Epoch: 5| Step: 4
Training loss: 2.7717942218557616
Validation loss: 2.7698602270272974

Epoch: 5| Step: 5
Training loss: 2.5871908141353503
Validation loss: 2.7688433389595617

Epoch: 5| Step: 6
Training loss: 2.443414018173953
Validation loss: 2.766906782487184

Epoch: 5| Step: 7
Training loss: 2.9039721021967333
Validation loss: 2.765146016455504

Epoch: 5| Step: 8
Training loss: 3.066537803209141
Validation loss: 2.763840539680819

Epoch: 5| Step: 9
Training loss: 3.4727901265585515
Validation loss: 2.7614310877801467

Epoch: 5| Step: 10
Training loss: 3.238515369156776
Validation loss: 2.7606459840255253

Epoch: 5| Step: 11
Training loss: 2.889457343081533
Validation loss: 2.758010882337713

Epoch: 61| Step: 0
Training loss: 3.228083439104283
Validation loss: 2.7551495822827325

Epoch: 5| Step: 1
Training loss: 2.6638935292821975
Validation loss: 2.75603663151351

Epoch: 5| Step: 2
Training loss: 2.7913458530694033
Validation loss: 2.75164263245938

Epoch: 5| Step: 3
Training loss: 3.2453330350179335
Validation loss: 2.747502956140031

Epoch: 5| Step: 4
Training loss: 2.4321129765244485
Validation loss: 2.7486190182222634

Epoch: 5| Step: 5
Training loss: 3.0428863239999835
Validation loss: 2.745496588179701

Epoch: 5| Step: 6
Training loss: 2.833618561094678
Validation loss: 2.7451055891311493

Epoch: 5| Step: 7
Training loss: 2.8584106153118625
Validation loss: 2.742797993721612

Epoch: 5| Step: 8
Training loss: 2.797329657910711
Validation loss: 2.7412185514820204

Epoch: 5| Step: 9
Training loss: 2.780262546765858
Validation loss: 2.7408174060831585

Epoch: 5| Step: 10
Training loss: 2.9628896593810228
Validation loss: 2.7392590781722883

Epoch: 5| Step: 11
Training loss: 3.0762642595254386
Validation loss: 2.7358928137208434

Epoch: 62| Step: 0
Training loss: 3.191617662204666
Validation loss: 2.742085200445674

Epoch: 5| Step: 1
Training loss: 3.133289633777603
Validation loss: 2.765207445911924

Epoch: 5| Step: 2
Training loss: 2.645518717245198
Validation loss: 2.7388136704804817

Epoch: 5| Step: 3
Training loss: 2.7905280699774866
Validation loss: 2.728789228203086

Epoch: 5| Step: 4
Training loss: 2.853413689037419
Validation loss: 2.7233081403903183

Epoch: 5| Step: 5
Training loss: 2.666721661318502
Validation loss: 2.7236468147327324

Epoch: 5| Step: 6
Training loss: 2.9787010060137855
Validation loss: 2.7231835831095843

Epoch: 5| Step: 7
Training loss: 2.7208199349307645
Validation loss: 2.7229267316212415

Epoch: 5| Step: 8
Training loss: 2.9253155855957815
Validation loss: 2.7225275907761866

Epoch: 5| Step: 9
Training loss: 3.014501332659671
Validation loss: 2.722761140296139

Epoch: 5| Step: 10
Training loss: 2.6058510381204707
Validation loss: 2.72139559749234

Epoch: 5| Step: 11
Training loss: 2.9739466108546515
Validation loss: 2.722563542703686

Epoch: 63| Step: 0
Training loss: 2.802522551610016
Validation loss: 2.720517333132238

Epoch: 5| Step: 1
Training loss: 3.04757932689588
Validation loss: 2.720608025189781

Epoch: 5| Step: 2
Training loss: 2.904248604697219
Validation loss: 2.717479971876508

Epoch: 5| Step: 3
Training loss: 2.7247366051723088
Validation loss: 2.7178510919118897

Epoch: 5| Step: 4
Training loss: 3.1465018027430918
Validation loss: 2.715859158761754

Epoch: 5| Step: 5
Training loss: 2.806291551855818
Validation loss: 2.71430279947762

Epoch: 5| Step: 6
Training loss: 2.6918408135766883
Validation loss: 2.712238322144922

Epoch: 5| Step: 7
Training loss: 2.5784969177168495
Validation loss: 2.708963455966525

Epoch: 5| Step: 8
Training loss: 2.442524939012446
Validation loss: 2.7087292467409907

Epoch: 5| Step: 9
Training loss: 2.823369701861354
Validation loss: 2.705802858956641

Epoch: 5| Step: 10
Training loss: 3.280024126941055
Validation loss: 2.7060933435570704

Epoch: 5| Step: 11
Training loss: 3.062048586838024
Validation loss: 2.7026240338467464

Epoch: 64| Step: 0
Training loss: 3.33325115738347
Validation loss: 2.7038093134358223

Epoch: 5| Step: 1
Training loss: 3.162388186683274
Validation loss: 2.7004516038447988

Epoch: 5| Step: 2
Training loss: 2.360816363717907
Validation loss: 2.699249605995247

Epoch: 5| Step: 3
Training loss: 2.710127036013361
Validation loss: 2.6996553530719956

Epoch: 5| Step: 4
Training loss: 2.743513346482995
Validation loss: 2.6969957264989053

Epoch: 5| Step: 5
Training loss: 3.201510138616396
Validation loss: 2.693994013479945

Epoch: 5| Step: 6
Training loss: 2.8213409493000428
Validation loss: 2.6935419468438577

Epoch: 5| Step: 7
Training loss: 2.490147057292673
Validation loss: 2.6928750483972927

Epoch: 5| Step: 8
Training loss: 2.4073223350306767
Validation loss: 2.689739898115705

Epoch: 5| Step: 9
Training loss: 2.789247821880522
Validation loss: 2.6898268119373996

Epoch: 5| Step: 10
Training loss: 2.9177321168274526
Validation loss: 2.687711910646279

Epoch: 5| Step: 11
Training loss: 3.127962462997578
Validation loss: 2.686777801471113

Epoch: 65| Step: 0
Training loss: 3.049353584203403
Validation loss: 2.6853533754863554

Epoch: 5| Step: 1
Training loss: 2.8147977979277647
Validation loss: 2.685290740774463

Epoch: 5| Step: 2
Training loss: 3.100116998248942
Validation loss: 2.6843592973752393

Epoch: 5| Step: 3
Training loss: 2.997698059665347
Validation loss: 2.6863154269889944

Epoch: 5| Step: 4
Training loss: 2.6482461055589055
Validation loss: 2.6960531313664475

Epoch: 5| Step: 5
Training loss: 2.17025454878421
Validation loss: 2.6997204032406255

Epoch: 5| Step: 6
Training loss: 2.516465609352812
Validation loss: 2.6989471144291515

Epoch: 5| Step: 7
Training loss: 2.840039358000638
Validation loss: 2.693708913951488

Epoch: 5| Step: 8
Training loss: 2.9487933325726807
Validation loss: 2.6811361632527246

Epoch: 5| Step: 9
Training loss: 2.9400944716657214
Validation loss: 2.676321029979581

Epoch: 5| Step: 10
Training loss: 2.7784159933821093
Validation loss: 2.6722684351198076

Epoch: 5| Step: 11
Training loss: 3.479447648509847
Validation loss: 2.6691038696670337

Epoch: 66| Step: 0
Training loss: 2.834774062422851
Validation loss: 2.665800305289412

Epoch: 5| Step: 1
Training loss: 2.805004643863091
Validation loss: 2.6645321250291767

Epoch: 5| Step: 2
Training loss: 2.9596240670953926
Validation loss: 2.6655980045074146

Epoch: 5| Step: 3
Training loss: 2.300930776092764
Validation loss: 2.661821192561578

Epoch: 5| Step: 4
Training loss: 2.8755018376765897
Validation loss: 2.6622597905422953

Epoch: 5| Step: 5
Training loss: 2.7700936804949867
Validation loss: 2.6612741303324206

Epoch: 5| Step: 6
Training loss: 2.9458068918411056
Validation loss: 2.661954248787283

Epoch: 5| Step: 7
Training loss: 3.024901043242759
Validation loss: 2.6579930215207686

Epoch: 5| Step: 8
Training loss: 2.7769190467571128
Validation loss: 2.6560514974286265

Epoch: 5| Step: 9
Training loss: 3.0487727588462556
Validation loss: 2.6586912343726032

Epoch: 5| Step: 10
Training loss: 2.373590854414243
Validation loss: 2.654742291719362

Epoch: 5| Step: 11
Training loss: 2.634049846452685
Validation loss: 2.6526786656419503

Epoch: 67| Step: 0
Training loss: 2.6903143939395866
Validation loss: 2.6511057275752696

Epoch: 5| Step: 1
Training loss: 3.1844788239598993
Validation loss: 2.6508508899646572

Epoch: 5| Step: 2
Training loss: 2.957351485817833
Validation loss: 2.6498068305002893

Epoch: 5| Step: 3
Training loss: 2.688852901147854
Validation loss: 2.6475568599436583

Epoch: 5| Step: 4
Training loss: 2.5750444908372474
Validation loss: 2.647459991916444

Epoch: 5| Step: 5
Training loss: 2.6476363599204022
Validation loss: 2.6479421506826424

Epoch: 5| Step: 6
Training loss: 2.9704498243218107
Validation loss: 2.6468264050247003

Epoch: 5| Step: 7
Training loss: 2.9166907354905804
Validation loss: 2.646889552106792

Epoch: 5| Step: 8
Training loss: 2.429503504419086
Validation loss: 2.6454794163926643

Epoch: 5| Step: 9
Training loss: 2.931721624568056
Validation loss: 2.6448717071893952

Epoch: 5| Step: 10
Training loss: 2.722197528216752
Validation loss: 2.6440724548217367

Epoch: 5| Step: 11
Training loss: 2.361106388860231
Validation loss: 2.6408949381077966

Epoch: 68| Step: 0
Training loss: 2.4920366295377288
Validation loss: 2.641502858948579

Epoch: 5| Step: 1
Training loss: 2.6043848073512414
Validation loss: 2.639125831044367

Epoch: 5| Step: 2
Training loss: 2.9156821814650837
Validation loss: 2.6370733441819474

Epoch: 5| Step: 3
Training loss: 2.930062964221793
Validation loss: 2.6344041663591984

Epoch: 5| Step: 4
Training loss: 2.808842548497469
Validation loss: 2.6369430408920316

Epoch: 5| Step: 5
Training loss: 2.8547153548795814
Validation loss: 2.6326977094459205

Epoch: 5| Step: 6
Training loss: 2.9358788744812006
Validation loss: 2.631870576704879

Epoch: 5| Step: 7
Training loss: 2.8321974291222807
Validation loss: 2.63253769916712

Epoch: 5| Step: 8
Training loss: 2.5690904860369472
Validation loss: 2.641538539094711

Epoch: 5| Step: 9
Training loss: 2.813474867666966
Validation loss: 2.637317090373875

Epoch: 5| Step: 10
Training loss: 2.8936006643432157
Validation loss: 2.635566014869191

Epoch: 5| Step: 11
Training loss: 1.9399501935719996
Validation loss: 2.6326096906568686

Epoch: 69| Step: 0
Training loss: 3.0026471856446872
Validation loss: 2.629758329263027

Epoch: 5| Step: 1
Training loss: 2.858169551713182
Validation loss: 2.627104324470734

Epoch: 5| Step: 2
Training loss: 1.9265676753584875
Validation loss: 2.6283483002909187

Epoch: 5| Step: 3
Training loss: 2.535730047196156
Validation loss: 2.6274761298559595

Epoch: 5| Step: 4
Training loss: 2.7974407114081963
Validation loss: 2.6269040279220124

Epoch: 5| Step: 5
Training loss: 3.056657004998044
Validation loss: 2.6274966862866638

Epoch: 5| Step: 6
Training loss: 2.8221901009034114
Validation loss: 2.6233210681905654

Epoch: 5| Step: 7
Training loss: 2.6138055158618356
Validation loss: 2.6232773184178337

Epoch: 5| Step: 8
Training loss: 3.134411639135384
Validation loss: 2.6202533206463094

Epoch: 5| Step: 9
Training loss: 2.672293290083006
Validation loss: 2.6223761524016256

Epoch: 5| Step: 10
Training loss: 2.68588820629146
Validation loss: 2.622992459374449

Epoch: 5| Step: 11
Training loss: 3.2923090987884676
Validation loss: 2.6217709238388647

Epoch: 70| Step: 0
Training loss: 2.689754339012364
Validation loss: 2.618980283330873

Epoch: 5| Step: 1
Training loss: 3.070640022338918
Validation loss: 2.616896541990577

Epoch: 5| Step: 2
Training loss: 2.6351071101791477
Validation loss: 2.6158679007739223

Epoch: 5| Step: 3
Training loss: 2.8802013615575
Validation loss: 2.6175066169793584

Epoch: 5| Step: 4
Training loss: 2.262289077453321
Validation loss: 2.612162610525017

Epoch: 5| Step: 5
Training loss: 2.831735235068477
Validation loss: 2.6091413183898884

Epoch: 5| Step: 6
Training loss: 2.6204595807718976
Validation loss: 2.61072731381972

Epoch: 5| Step: 7
Training loss: 2.885844303396291
Validation loss: 2.6117988186939

Epoch: 5| Step: 8
Training loss: 2.948688221891123
Validation loss: 2.609696168607636

Epoch: 5| Step: 9
Training loss: 3.086900469066084
Validation loss: 2.608555533742401

Epoch: 5| Step: 10
Training loss: 2.2601975248321384
Validation loss: 2.610187567704063

Epoch: 5| Step: 11
Training loss: 2.033151763355228
Validation loss: 2.6101960167837945

Epoch: 71| Step: 0
Training loss: 2.4016762403111596
Validation loss: 2.619780130278793

Epoch: 5| Step: 1
Training loss: 2.5433134674489644
Validation loss: 2.6116488328027416

Epoch: 5| Step: 2
Training loss: 2.568725188569394
Validation loss: 2.6062027156208836

Epoch: 5| Step: 3
Training loss: 2.8367263075240143
Validation loss: 2.6191091438390863

Epoch: 5| Step: 4
Training loss: 3.096554188514586
Validation loss: 2.60033914939997

Epoch: 5| Step: 5
Training loss: 2.91608855558079
Validation loss: 2.5986611967891236

Epoch: 5| Step: 6
Training loss: 2.7596457982489735
Validation loss: 2.6011274640363746

Epoch: 5| Step: 7
Training loss: 2.966210654568861
Validation loss: 2.6051860238428834

Epoch: 5| Step: 8
Training loss: 2.75053383240785
Validation loss: 2.608010390178582

Epoch: 5| Step: 9
Training loss: 2.403434890192123
Validation loss: 2.6079212372518303

Epoch: 5| Step: 10
Training loss: 2.778501116563107
Validation loss: 2.6092552949474537

Epoch: 5| Step: 11
Training loss: 3.328116546763399
Validation loss: 2.6123379314838555

Epoch: 72| Step: 0
Training loss: 2.6408016890602752
Validation loss: 2.6079097181894895

Epoch: 5| Step: 1
Training loss: 2.563623414692497
Validation loss: 2.606045142095795

Epoch: 5| Step: 2
Training loss: 2.4366749076164664
Validation loss: 2.6084888080312707

Epoch: 5| Step: 3
Training loss: 3.097427802302826
Validation loss: 2.6021239407081183

Epoch: 5| Step: 4
Training loss: 2.7414108872068663
Validation loss: 2.6008350741453823

Epoch: 5| Step: 5
Training loss: 2.521775303492548
Validation loss: 2.599524871346926

Epoch: 5| Step: 6
Training loss: 2.753448145214471
Validation loss: 2.597458275920953

Epoch: 5| Step: 7
Training loss: 2.8356015345287973
Validation loss: 2.5945909737721102

Epoch: 5| Step: 8
Training loss: 2.91622721676648
Validation loss: 2.593004648133101

Epoch: 5| Step: 9
Training loss: 2.9678722489411764
Validation loss: 2.592575952704204

Epoch: 5| Step: 10
Training loss: 2.572761366220324
Validation loss: 2.5881050836723682

Epoch: 5| Step: 11
Training loss: 2.674386077023482
Validation loss: 2.5883345850515522

Epoch: 73| Step: 0
Training loss: 2.996272473432012
Validation loss: 2.583405894880634

Epoch: 5| Step: 1
Training loss: 2.7039871164048828
Validation loss: 2.5861278052860497

Epoch: 5| Step: 2
Training loss: 2.9131265318366664
Validation loss: 2.58354780757857

Epoch: 5| Step: 3
Training loss: 2.5010177448044346
Validation loss: 2.5840636087669644

Epoch: 5| Step: 4
Training loss: 2.7439899661155955
Validation loss: 2.5862507779227935

Epoch: 5| Step: 5
Training loss: 2.6043452595824577
Validation loss: 2.5802462010742033

Epoch: 5| Step: 6
Training loss: 3.0142240284242763
Validation loss: 2.57700590255312

Epoch: 5| Step: 7
Training loss: 2.433858351918486
Validation loss: 2.57587911481435

Epoch: 5| Step: 8
Training loss: 2.3150095339222427
Validation loss: 2.5738848912068764

Epoch: 5| Step: 9
Training loss: 2.7816939267544036
Validation loss: 2.581652665000742

Epoch: 5| Step: 10
Training loss: 2.461387956355498
Validation loss: 2.583547557644346

Epoch: 5| Step: 11
Training loss: 3.8917805744194665
Validation loss: 2.5773056423688283

Epoch: 74| Step: 0
Training loss: 2.858117999834255
Validation loss: 2.5742183100652247

Epoch: 5| Step: 1
Training loss: 2.482466246345679
Validation loss: 2.5738215393269313

Epoch: 5| Step: 2
Training loss: 2.5409062191209806
Validation loss: 2.5701563284290674

Epoch: 5| Step: 3
Training loss: 2.7836439994839566
Validation loss: 2.5745580408134265

Epoch: 5| Step: 4
Training loss: 2.79605448530862
Validation loss: 2.5746334438465484

Epoch: 5| Step: 5
Training loss: 3.2166416707492442
Validation loss: 2.5740550967936655

Epoch: 5| Step: 6
Training loss: 2.4601648480146077
Validation loss: 2.573492419473865

Epoch: 5| Step: 7
Training loss: 2.857991868986572
Validation loss: 2.5764740985227528

Epoch: 5| Step: 8
Training loss: 2.4040551021436816
Validation loss: 2.572026459107177

Epoch: 5| Step: 9
Training loss: 2.601262164034763
Validation loss: 2.569702880051306

Epoch: 5| Step: 10
Training loss: 2.7276824137723277
Validation loss: 2.568422834533416

Epoch: 5| Step: 11
Training loss: 2.570475935450282
Validation loss: 2.5659248756051984

Epoch: 75| Step: 0
Training loss: 2.6032634135034143
Validation loss: 2.569336594789702

Epoch: 5| Step: 1
Training loss: 2.6835482619871143
Validation loss: 2.5670404642462823

Epoch: 5| Step: 2
Training loss: 3.0768189733573883
Validation loss: 2.5688878974501064

Epoch: 5| Step: 3
Training loss: 2.548462729735531
Validation loss: 2.566045560607491

Epoch: 5| Step: 4
Training loss: 2.840832229961682
Validation loss: 2.5644305563033116

Epoch: 5| Step: 5
Training loss: 2.6252770958833427
Validation loss: 2.5635832187323664

Epoch: 5| Step: 6
Training loss: 2.661092495394132
Validation loss: 2.560772022442733

Epoch: 5| Step: 7
Training loss: 2.561068530451631
Validation loss: 2.561503937821914

Epoch: 5| Step: 8
Training loss: 2.3781542662174426
Validation loss: 2.5705048741292758

Epoch: 5| Step: 9
Training loss: 2.3591122512533547
Validation loss: 2.56388069576067

Epoch: 5| Step: 10
Training loss: 3.089831867026376
Validation loss: 2.5637258644586054

Epoch: 5| Step: 11
Training loss: 3.4344732657511226
Validation loss: 2.5594276486463423

Epoch: 76| Step: 0
Training loss: 2.4459580614388856
Validation loss: 2.557673576619912

Epoch: 5| Step: 1
Training loss: 3.0415280105828644
Validation loss: 2.557577623486369

Epoch: 5| Step: 2
Training loss: 2.838386928062056
Validation loss: 2.556528425559635

Epoch: 5| Step: 3
Training loss: 2.451528625059741
Validation loss: 2.5556095570811133

Epoch: 5| Step: 4
Training loss: 2.4575685723207585
Validation loss: 2.5554271875381867

Epoch: 5| Step: 5
Training loss: 2.822331685807739
Validation loss: 2.553872977003369

Epoch: 5| Step: 6
Training loss: 2.747733569365668
Validation loss: 2.555451227445052

Epoch: 5| Step: 7
Training loss: 2.3566388565681557
Validation loss: 2.553995316707506

Epoch: 5| Step: 8
Training loss: 2.7600909538762366
Validation loss: 2.5527355852734694

Epoch: 5| Step: 9
Training loss: 2.1997080609216515
Validation loss: 2.552775376099333

Epoch: 5| Step: 10
Training loss: 3.2598269155693242
Validation loss: 2.548057234955177

Epoch: 5| Step: 11
Training loss: 2.6207248752229884
Validation loss: 2.5504090688494947

Epoch: 77| Step: 0
Training loss: 2.9440972125443854
Validation loss: 2.5446122733328194

Epoch: 5| Step: 1
Training loss: 2.617957158576715
Validation loss: 2.5478329953303884

Epoch: 5| Step: 2
Training loss: 2.8801993748742256
Validation loss: 2.5460517641054614

Epoch: 5| Step: 3
Training loss: 2.67754391717022
Validation loss: 2.5516195005492377

Epoch: 5| Step: 4
Training loss: 2.698079952370264
Validation loss: 2.5468126691122173

Epoch: 5| Step: 5
Training loss: 2.489494949963947
Validation loss: 2.551139506505243

Epoch: 5| Step: 6
Training loss: 2.414084030104739
Validation loss: 2.5472447998618284

Epoch: 5| Step: 7
Training loss: 3.180291195858345
Validation loss: 2.551333271793969

Epoch: 5| Step: 8
Training loss: 2.4731199965620387
Validation loss: 2.5520316229012474

Epoch: 5| Step: 9
Training loss: 2.598497572873692
Validation loss: 2.5531765380476137

Epoch: 5| Step: 10
Training loss: 2.1199235304950235
Validation loss: 2.545925493856258

Epoch: 5| Step: 11
Training loss: 3.4469925639905368
Validation loss: 2.5456559397759615

Epoch: 78| Step: 0
Training loss: 2.456075169987902
Validation loss: 2.547461689812901

Epoch: 5| Step: 1
Training loss: 2.8886074964301773
Validation loss: 2.5494819124159607

Epoch: 5| Step: 2
Training loss: 2.5268594787860694
Validation loss: 2.541671726867975

Epoch: 5| Step: 3
Training loss: 2.9609275153401904
Validation loss: 2.5383137112341436

Epoch: 5| Step: 4
Training loss: 2.7946877904680885
Validation loss: 2.5416157527159946

Epoch: 5| Step: 5
Training loss: 2.956006133200925
Validation loss: 2.5423722317067536

Epoch: 5| Step: 6
Training loss: 2.8327655971408445
Validation loss: 2.54093929472271

Epoch: 5| Step: 7
Training loss: 2.8303458414286204
Validation loss: 2.5422288020271773

Epoch: 5| Step: 8
Training loss: 2.4090908174858274
Validation loss: 2.543528767149802

Epoch: 5| Step: 9
Training loss: 2.4924548732150456
Validation loss: 2.546208066150239

Epoch: 5| Step: 10
Training loss: 2.1608567714437568
Validation loss: 2.5499959713461475

Epoch: 5| Step: 11
Training loss: 2.732532076355066
Validation loss: 2.5516556063482305

Epoch: 79| Step: 0
Training loss: 2.873465252773892
Validation loss: 2.550527461669568

Epoch: 5| Step: 1
Training loss: 2.6528070014378056
Validation loss: 2.5494093698695948

Epoch: 5| Step: 2
Training loss: 2.889930270452665
Validation loss: 2.54961861426147

Epoch: 5| Step: 3
Training loss: 2.4312759554931302
Validation loss: 2.549310646812929

Epoch: 5| Step: 4
Training loss: 3.1439969763996056
Validation loss: 2.5503334442925256

Epoch: 5| Step: 5
Training loss: 2.7116125843803305
Validation loss: 2.5479022530693625

Epoch: 5| Step: 6
Training loss: 2.398209849932646
Validation loss: 2.5422747866841178

Epoch: 5| Step: 7
Training loss: 2.4499332535662948
Validation loss: 2.5441929594764408

Epoch: 5| Step: 8
Training loss: 2.7026928683050473
Validation loss: 2.5401600739583503

Epoch: 5| Step: 9
Training loss: 2.781640014734082
Validation loss: 2.542121031020576

Epoch: 5| Step: 10
Training loss: 2.3039368522321713
Validation loss: 2.538538091252913

Epoch: 5| Step: 11
Training loss: 2.6946935855301732
Validation loss: 2.536807717001764

Epoch: 80| Step: 0
Training loss: 2.412972114499876
Validation loss: 2.534025640589278

Epoch: 5| Step: 1
Training loss: 2.85894004974008
Validation loss: 2.540267474522827

Epoch: 5| Step: 2
Training loss: 2.8583812550196748
Validation loss: 2.540576244917142

Epoch: 5| Step: 3
Training loss: 2.9108341937897033
Validation loss: 2.5430942002471455

Epoch: 5| Step: 4
Training loss: 2.282049901063963
Validation loss: 2.5463149619301366

Epoch: 5| Step: 5
Training loss: 2.8444345509668327
Validation loss: 2.555139021484825

Epoch: 5| Step: 6
Training loss: 2.723023574547735
Validation loss: 2.546691265772762

Epoch: 5| Step: 7
Training loss: 3.0176857345800587
Validation loss: 2.5454483483056394

Epoch: 5| Step: 8
Training loss: 2.7067471089490795
Validation loss: 2.535771903042874

Epoch: 5| Step: 9
Training loss: 2.149283613573301
Validation loss: 2.528516592592011

Epoch: 5| Step: 10
Training loss: 2.568988029793619
Validation loss: 2.5305363390788806

Epoch: 5| Step: 11
Training loss: 2.2886377142254513
Validation loss: 2.52857789335638

Epoch: 81| Step: 0
Training loss: 2.4954905848293976
Validation loss: 2.533623316652401

Epoch: 5| Step: 1
Training loss: 2.6667714595072507
Validation loss: 2.5285825096134644

Epoch: 5| Step: 2
Training loss: 2.4464672153193314
Validation loss: 2.5336530056043833

Epoch: 5| Step: 3
Training loss: 2.3187154803315666
Validation loss: 2.5261424775342696

Epoch: 5| Step: 4
Training loss: 3.0467363374897842
Validation loss: 2.5286950592110564

Epoch: 5| Step: 5
Training loss: 2.928366564187778
Validation loss: 2.5255962739484694

Epoch: 5| Step: 6
Training loss: 2.99911390569954
Validation loss: 2.5261659781683727

Epoch: 5| Step: 7
Training loss: 2.6177102548997238
Validation loss: 2.5280147921412612

Epoch: 5| Step: 8
Training loss: 2.5810624467817536
Validation loss: 2.5291865419259416

Epoch: 5| Step: 9
Training loss: 2.1586801959637385
Validation loss: 2.523394034122547

Epoch: 5| Step: 10
Training loss: 2.784525025304429
Validation loss: 2.5243006943334207

Epoch: 5| Step: 11
Training loss: 2.621239648160927
Validation loss: 2.523296667210127

Epoch: 82| Step: 0
Training loss: 2.5944777237661247
Validation loss: 2.5235778643579874

Epoch: 5| Step: 1
Training loss: 2.4879606270940493
Validation loss: 2.5201480081023635

Epoch: 5| Step: 2
Training loss: 2.7632298847371604
Validation loss: 2.5231523972772667

Epoch: 5| Step: 3
Training loss: 2.7733215307785417
Validation loss: 2.520043877391195

Epoch: 5| Step: 4
Training loss: 2.223161958307354
Validation loss: 2.5182415009557815

Epoch: 5| Step: 5
Training loss: 2.5751452247629425
Validation loss: 2.514758732916918

Epoch: 5| Step: 6
Training loss: 2.779820707151796
Validation loss: 2.5193389030589124

Epoch: 5| Step: 7
Training loss: 2.548179526401779
Validation loss: 2.527059426054384

Epoch: 5| Step: 8
Training loss: 2.5831895501105873
Validation loss: 2.514143153908743

Epoch: 5| Step: 9
Training loss: 2.8075039888864852
Validation loss: 2.515666384781061

Epoch: 5| Step: 10
Training loss: 2.810738499347217
Validation loss: 2.517837483433348

Epoch: 5| Step: 11
Training loss: 2.923611048690622
Validation loss: 2.5221138741369664

Epoch: 83| Step: 0
Training loss: 2.403191145642626
Validation loss: 2.5237222182168244

Epoch: 5| Step: 1
Training loss: 3.0043902063113883
Validation loss: 2.5228739681037977

Epoch: 5| Step: 2
Training loss: 2.136521241371483
Validation loss: 2.520277511123618

Epoch: 5| Step: 3
Training loss: 2.2044326472714473
Validation loss: 2.519380617197762

Epoch: 5| Step: 4
Training loss: 2.5227205657557406
Validation loss: 2.5164920149972767

Epoch: 5| Step: 5
Training loss: 2.969900450402797
Validation loss: 2.5163004266594484

Epoch: 5| Step: 6
Training loss: 2.435906892184128
Validation loss: 2.520649191275797

Epoch: 5| Step: 7
Training loss: 2.6209780444375776
Validation loss: 2.5226486437491786

Epoch: 5| Step: 8
Training loss: 2.5930951682154126
Validation loss: 2.5227974472626933

Epoch: 5| Step: 9
Training loss: 3.185984980747145
Validation loss: 2.5198697515791575

Epoch: 5| Step: 10
Training loss: 2.5225167026477187
Validation loss: 2.5227829130414436

Epoch: 5| Step: 11
Training loss: 3.728232211596266
Validation loss: 2.5122084034037697

Epoch: 84| Step: 0
Training loss: 2.6661348309258526
Validation loss: 2.5182287787536093

Epoch: 5| Step: 1
Training loss: 2.580820974390798
Validation loss: 2.514322231825359

Epoch: 5| Step: 2
Training loss: 2.873734402939864
Validation loss: 2.5135505921676597

Epoch: 5| Step: 3
Training loss: 2.1303750666971033
Validation loss: 2.5152083342096216

Epoch: 5| Step: 4
Training loss: 2.497612862542735
Validation loss: 2.5169803845951333

Epoch: 5| Step: 5
Training loss: 3.290282976683068
Validation loss: 2.5193621832533335

Epoch: 5| Step: 6
Training loss: 2.8822540321186234
Validation loss: 2.516758566614225

Epoch: 5| Step: 7
Training loss: 2.4109403796867928
Validation loss: 2.5141691335030214

Epoch: 5| Step: 8
Training loss: 2.52670898012883
Validation loss: 2.509163735104101

Epoch: 5| Step: 9
Training loss: 2.659960231196901
Validation loss: 2.5148305606012276

Epoch: 5| Step: 10
Training loss: 2.3247680015076213
Validation loss: 2.517481870860612

Epoch: 5| Step: 11
Training loss: 2.593625169071113
Validation loss: 2.5161416093941793

Epoch: 85| Step: 0
Training loss: 2.6262224166707075
Validation loss: 2.5119005516152986

Epoch: 5| Step: 1
Training loss: 3.0126524194171034
Validation loss: 2.516528064235277

Epoch: 5| Step: 2
Training loss: 2.748798454659042
Validation loss: 2.5230481976327805

Epoch: 5| Step: 3
Training loss: 2.2867293722194675
Validation loss: 2.5396699423308435

Epoch: 5| Step: 4
Training loss: 2.624733139278963
Validation loss: 2.560484523872426

Epoch: 5| Step: 5
Training loss: 2.811344672373404
Validation loss: 2.5791379999920774

Epoch: 5| Step: 6
Training loss: 2.430120497019726
Validation loss: 2.5808133260103125

Epoch: 5| Step: 7
Training loss: 2.6141005801148016
Validation loss: 2.582307513179273

Epoch: 5| Step: 8
Training loss: 2.3388406835659046
Validation loss: 2.5619815868023226

Epoch: 5| Step: 9
Training loss: 2.9520889967758857
Validation loss: 2.5500684465621797

Epoch: 5| Step: 10
Training loss: 2.885531500081721
Validation loss: 2.533729324061993

Epoch: 5| Step: 11
Training loss: 2.228706050831007
Validation loss: 2.526196674952502

Epoch: 86| Step: 0
Training loss: 2.564742247065829
Validation loss: 2.5204290993307596

Epoch: 5| Step: 1
Training loss: 2.5658727640077283
Validation loss: 2.509670983099733

Epoch: 5| Step: 2
Training loss: 2.539213017623935
Validation loss: 2.5090020746260766

Epoch: 5| Step: 3
Training loss: 1.8279109079574447
Validation loss: 2.506008216122399

Epoch: 5| Step: 4
Training loss: 2.677079157448764
Validation loss: 2.5030142375047326

Epoch: 5| Step: 5
Training loss: 2.7606841527548878
Validation loss: 2.509867791497251

Epoch: 5| Step: 6
Training loss: 2.903349711054067
Validation loss: 2.50994059822316

Epoch: 5| Step: 7
Training loss: 2.8753962036460115
Validation loss: 2.524394949097281

Epoch: 5| Step: 8
Training loss: 3.0857134387605503
Validation loss: 2.534005592162885

Epoch: 5| Step: 9
Training loss: 3.1979955773242477
Validation loss: 2.519582033261628

Epoch: 5| Step: 10
Training loss: 2.109752931999866
Validation loss: 2.509048426728764

Epoch: 5| Step: 11
Training loss: 2.2519971672839816
Validation loss: 2.5100097972873705

Epoch: 87| Step: 0
Training loss: 2.2183910670554914
Validation loss: 2.503957290644965

Epoch: 5| Step: 1
Training loss: 3.3523508847475636
Validation loss: 2.501640413442201

Epoch: 5| Step: 2
Training loss: 2.4548120225467445
Validation loss: 2.5062821176108807

Epoch: 5| Step: 3
Training loss: 3.2701224427237086
Validation loss: 2.5078648950920512

Epoch: 5| Step: 4
Training loss: 2.7783508620903907
Validation loss: 2.5055620865009542

Epoch: 5| Step: 5
Training loss: 2.3162692513569088
Validation loss: 2.510898737043439

Epoch: 5| Step: 6
Training loss: 2.2731897802512164
Validation loss: 2.514079446460864

Epoch: 5| Step: 7
Training loss: 2.719647664923838
Validation loss: 2.5129163150431846

Epoch: 5| Step: 8
Training loss: 2.634172218636303
Validation loss: 2.517468896217225

Epoch: 5| Step: 9
Training loss: 2.013549208951725
Validation loss: 2.5157611565241282

Epoch: 5| Step: 10
Training loss: 2.7138304651280323
Validation loss: 2.515034323901127

Epoch: 5| Step: 11
Training loss: 2.500399748313695
Validation loss: 2.5145325588945964

Epoch: 88| Step: 0
Training loss: 2.8264841620289833
Validation loss: 2.512398995002965

Epoch: 5| Step: 1
Training loss: 2.2756924801454774
Validation loss: 2.5149804863321243

Epoch: 5| Step: 2
Training loss: 2.0148880668579463
Validation loss: 2.5208205164959225

Epoch: 5| Step: 3
Training loss: 2.63425639146846
Validation loss: 2.519551042976801

Epoch: 5| Step: 4
Training loss: 2.827962206797062
Validation loss: 2.516297051200983

Epoch: 5| Step: 5
Training loss: 2.5960481872142003
Validation loss: 2.515221553540144

Epoch: 5| Step: 6
Training loss: 2.3854711490278473
Validation loss: 2.505455882530369

Epoch: 5| Step: 7
Training loss: 2.5738692791697217
Validation loss: 2.500089862321059

Epoch: 5| Step: 8
Training loss: 2.786554571373391
Validation loss: 2.499851937677888

Epoch: 5| Step: 9
Training loss: 2.7722388027700173
Validation loss: 2.499277558846652

Epoch: 5| Step: 10
Training loss: 3.131066347016841
Validation loss: 2.5047869470010298

Epoch: 5| Step: 11
Training loss: 2.7023237507221523
Validation loss: 2.4987167482261174

Epoch: 89| Step: 0
Training loss: 2.9655669163441485
Validation loss: 2.498285627649763

Epoch: 5| Step: 1
Training loss: 2.3029261178387013
Validation loss: 2.4999121769738113

Epoch: 5| Step: 2
Training loss: 2.6260314005058794
Validation loss: 2.5001099006020513

Epoch: 5| Step: 3
Training loss: 2.8731985668018947
Validation loss: 2.502129470363827

Epoch: 5| Step: 4
Training loss: 2.4397306750134455
Validation loss: 2.498356998491421

Epoch: 5| Step: 5
Training loss: 2.8421114717939946
Validation loss: 2.499369188037883

Epoch: 5| Step: 6
Training loss: 2.192331917560699
Validation loss: 2.493415454263931

Epoch: 5| Step: 7
Training loss: 2.4824280217272445
Validation loss: 2.4926476965507423

Epoch: 5| Step: 8
Training loss: 2.622698728815673
Validation loss: 2.4925699807485207

Epoch: 5| Step: 9
Training loss: 2.4065557508543645
Validation loss: 2.4966270183586334

Epoch: 5| Step: 10
Training loss: 2.9212242666603823
Validation loss: 2.493574647307392

Epoch: 5| Step: 11
Training loss: 2.454886417597202
Validation loss: 2.4967793939307277

Epoch: 90| Step: 0
Training loss: 2.807297960792143
Validation loss: 2.4929906294422546

Epoch: 5| Step: 1
Training loss: 3.082848038156793
Validation loss: 2.4977142931800542

Epoch: 5| Step: 2
Training loss: 2.5237992443704806
Validation loss: 2.491430986915004

Epoch: 5| Step: 3
Training loss: 2.160435249839717
Validation loss: 2.4964660024685643

Epoch: 5| Step: 4
Training loss: 2.64282333691159
Validation loss: 2.4911352385600636

Epoch: 5| Step: 5
Training loss: 2.6044533737030995
Validation loss: 2.4901831009516564

Epoch: 5| Step: 6
Training loss: 2.331937190575899
Validation loss: 2.4940959475346123

Epoch: 5| Step: 7
Training loss: 2.5427405376548933
Validation loss: 2.4980177809288855

Epoch: 5| Step: 8
Training loss: 2.598372052458625
Validation loss: 2.4931399838193427

Epoch: 5| Step: 9
Training loss: 2.461223186077563
Validation loss: 2.4911807666657806

Epoch: 5| Step: 10
Training loss: 2.7433133761984423
Validation loss: 2.484718019169062

Epoch: 5| Step: 11
Training loss: 2.8995303332720033
Validation loss: 2.487331830063714

Epoch: 91| Step: 0
Training loss: 2.4925921359772136
Validation loss: 2.490429859342368

Epoch: 5| Step: 1
Training loss: 2.327569267971329
Validation loss: 2.4921915219711965

Epoch: 5| Step: 2
Training loss: 2.4784161583050492
Validation loss: 2.4913459441178274

Epoch: 5| Step: 3
Training loss: 2.6208115722852563
Validation loss: 2.4896936684796147

Epoch: 5| Step: 4
Training loss: 2.742023343555186
Validation loss: 2.492209658628868

Epoch: 5| Step: 5
Training loss: 2.527360070012271
Validation loss: 2.496899907923258

Epoch: 5| Step: 6
Training loss: 2.7263715179800303
Validation loss: 2.493628098480973

Epoch: 5| Step: 7
Training loss: 2.8622315526629865
Validation loss: 2.5023907078515273

Epoch: 5| Step: 8
Training loss: 2.946830864067101
Validation loss: 2.5026945097167412

Epoch: 5| Step: 9
Training loss: 2.233019117294867
Validation loss: 2.4960111227274213

Epoch: 5| Step: 10
Training loss: 2.7075002024642574
Validation loss: 2.5011614485759712

Epoch: 5| Step: 11
Training loss: 1.6657186514057136
Validation loss: 2.490093264103437

Epoch: 92| Step: 0
Training loss: 2.7342858872197904
Validation loss: 2.4897905320703413

Epoch: 5| Step: 1
Training loss: 2.7912914441487438
Validation loss: 2.48932915883752

Epoch: 5| Step: 2
Training loss: 2.7313409157293074
Validation loss: 2.4904350489124716

Epoch: 5| Step: 3
Training loss: 2.191677464312316
Validation loss: 2.4904445225425342

Epoch: 5| Step: 4
Training loss: 2.6663499783184244
Validation loss: 2.483288113344487

Epoch: 5| Step: 5
Training loss: 2.7541863048813253
Validation loss: 2.4904794968781765

Epoch: 5| Step: 6
Training loss: 2.559769361805557
Validation loss: 2.488276742624685

Epoch: 5| Step: 7
Training loss: 2.417937098282083
Validation loss: 2.491095639557321

Epoch: 5| Step: 8
Training loss: 2.770568395200326
Validation loss: 2.4904143982684883

Epoch: 5| Step: 9
Training loss: 2.505178524059403
Validation loss: 2.496415843290585

Epoch: 5| Step: 10
Training loss: 2.6052330173531564
Validation loss: 2.4939268095712452

Epoch: 5| Step: 11
Training loss: 1.29096825494009
Validation loss: 2.490603718014568

Epoch: 93| Step: 0
Training loss: 2.8195062472914696
Validation loss: 2.490147899048711

Epoch: 5| Step: 1
Training loss: 2.270756034818696
Validation loss: 2.4865140123742764

Epoch: 5| Step: 2
Training loss: 2.5610349235134544
Validation loss: 2.4849324510656525

Epoch: 5| Step: 3
Training loss: 2.8025522418772013
Validation loss: 2.4864363585610287

Epoch: 5| Step: 4
Training loss: 2.8466986156639154
Validation loss: 2.4961912268448554

Epoch: 5| Step: 5
Training loss: 2.547883284675796
Validation loss: 2.4893983641880117

Epoch: 5| Step: 6
Training loss: 2.737255911200281
Validation loss: 2.4849273339564775

Epoch: 5| Step: 7
Training loss: 2.284839575580508
Validation loss: 2.487386885104697

Epoch: 5| Step: 8
Training loss: 2.8542127025446415
Validation loss: 2.4811746468940865

Epoch: 5| Step: 9
Training loss: 2.372498550173414
Validation loss: 2.489339987514746

Epoch: 5| Step: 10
Training loss: 2.6106350534659675
Validation loss: 2.488238671146598

Epoch: 5| Step: 11
Training loss: 1.5190296276736561
Validation loss: 2.487268538225454

Epoch: 94| Step: 0
Training loss: 2.6149127241052423
Validation loss: 2.48610923025272

Epoch: 5| Step: 1
Training loss: 2.436094318894097
Validation loss: 2.4851551910145497

Epoch: 5| Step: 2
Training loss: 1.7829011325050987
Validation loss: 2.489026822786136

Epoch: 5| Step: 3
Training loss: 2.5054903777057054
Validation loss: 2.4870029202004265

Epoch: 5| Step: 4
Training loss: 2.9919680522396224
Validation loss: 2.484936077010853

Epoch: 5| Step: 5
Training loss: 2.612052776989361
Validation loss: 2.483636910382604

Epoch: 5| Step: 6
Training loss: 2.5243729785402027
Validation loss: 2.481664506543229

Epoch: 5| Step: 7
Training loss: 2.7808622454255887
Validation loss: 2.480989396529145

Epoch: 5| Step: 8
Training loss: 2.8688554200061107
Validation loss: 2.4878512160121335

Epoch: 5| Step: 9
Training loss: 2.6891335246909818
Validation loss: 2.490908993364987

Epoch: 5| Step: 10
Training loss: 2.574949678836707
Validation loss: 2.4882287858814816

Epoch: 5| Step: 11
Training loss: 2.341917109644063
Validation loss: 2.4883399489349536

Epoch: 95| Step: 0
Training loss: 2.60562924830108
Validation loss: 2.488205246336008

Epoch: 5| Step: 1
Training loss: 2.3122180045082934
Validation loss: 2.476977629420783

Epoch: 5| Step: 2
Training loss: 2.3705253611110972
Validation loss: 2.491430640019018

Epoch: 5| Step: 3
Training loss: 2.713327195569094
Validation loss: 2.4941523349280956

Epoch: 5| Step: 4
Training loss: 2.1441401001576375
Validation loss: 2.494100962188814

Epoch: 5| Step: 5
Training loss: 2.93141127673417
Validation loss: 2.492140264272737

Epoch: 5| Step: 6
Training loss: 2.796789754069261
Validation loss: 2.4866584104966805

Epoch: 5| Step: 7
Training loss: 2.686792768527555
Validation loss: 2.4889806925166664

Epoch: 5| Step: 8
Training loss: 2.3768758894472066
Validation loss: 2.48521503905405

Epoch: 5| Step: 9
Training loss: 2.7969427686611272
Validation loss: 2.482642107222693

Epoch: 5| Step: 10
Training loss: 2.531167864938643
Validation loss: 2.481339441508261

Epoch: 5| Step: 11
Training loss: 2.9630995131746056
Validation loss: 2.484189816336871

Epoch: 96| Step: 0
Training loss: 2.8625160849839766
Validation loss: 2.4813115447808904

Epoch: 5| Step: 1
Training loss: 2.534985270560847
Validation loss: 2.487498293449545

Epoch: 5| Step: 2
Training loss: 2.533027966398782
Validation loss: 2.487773474160514

Epoch: 5| Step: 3
Training loss: 2.300529220067159
Validation loss: 2.4802660921867536

Epoch: 5| Step: 4
Training loss: 2.50395804841118
Validation loss: 2.485017737332344

Epoch: 5| Step: 5
Training loss: 2.850297370254015
Validation loss: 2.486071137569255

Epoch: 5| Step: 6
Training loss: 2.5909679153676057
Validation loss: 2.478799158927546

Epoch: 5| Step: 7
Training loss: 2.1111067833214734
Validation loss: 2.484622771025667

Epoch: 5| Step: 8
Training loss: 2.527565806331157
Validation loss: 2.4870646450405522

Epoch: 5| Step: 9
Training loss: 2.790286866620468
Validation loss: 2.475745899818382

Epoch: 5| Step: 10
Training loss: 2.7870477831316394
Validation loss: 2.4805280222782113

Epoch: 5| Step: 11
Training loss: 2.2587624633647874
Validation loss: 2.4819747478108334

Epoch: 97| Step: 0
Training loss: 2.9469410570331958
Validation loss: 2.494729271561755

Epoch: 5| Step: 1
Training loss: 2.5567922525343407
Validation loss: 2.5214963676784015

Epoch: 5| Step: 2
Training loss: 2.9768002232416935
Validation loss: 2.5320587200654803

Epoch: 5| Step: 3
Training loss: 2.7731470090221833
Validation loss: 2.5044414168543296

Epoch: 5| Step: 4
Training loss: 2.565773896184216
Validation loss: 2.494735674671095

Epoch: 5| Step: 5
Training loss: 2.6114535704605943
Validation loss: 2.4859925368486757

Epoch: 5| Step: 6
Training loss: 2.4461575836361633
Validation loss: 2.477655660882561

Epoch: 5| Step: 7
Training loss: 1.9398367541736643
Validation loss: 2.4815821233989594

Epoch: 5| Step: 8
Training loss: 2.4947674351247424
Validation loss: 2.48253765181371

Epoch: 5| Step: 9
Training loss: 2.804645166104166
Validation loss: 2.481901104694593

Epoch: 5| Step: 10
Training loss: 2.5318197857089135
Validation loss: 2.488563098445264

Epoch: 5| Step: 11
Training loss: 1.5112777184909552
Validation loss: 2.4993522162423245

Epoch: 98| Step: 0
Training loss: 3.0698900393199144
Validation loss: 2.5024394828975076

Epoch: 5| Step: 1
Training loss: 2.576274045547886
Validation loss: 2.502260541132898

Epoch: 5| Step: 2
Training loss: 2.4440600810680997
Validation loss: 2.5030768216402293

Epoch: 5| Step: 3
Training loss: 2.669194354690717
Validation loss: 2.5063107748740956

Epoch: 5| Step: 4
Training loss: 2.719432701247407
Validation loss: 2.5016715382391532

Epoch: 5| Step: 5
Training loss: 2.832777885145573
Validation loss: 2.499947761943556

Epoch: 5| Step: 6
Training loss: 2.6074287328737062
Validation loss: 2.494872600912374

Epoch: 5| Step: 7
Training loss: 2.4981425060492017
Validation loss: 2.4933061988798184

Epoch: 5| Step: 8
Training loss: 2.6603600512170935
Validation loss: 2.4882232603365346

Epoch: 5| Step: 9
Training loss: 2.3229411741141393
Validation loss: 2.4890263518290614

Epoch: 5| Step: 10
Training loss: 2.4710415212615477
Validation loss: 2.487329891033708

Epoch: 5| Step: 11
Training loss: 1.2490155635105356
Validation loss: 2.4814539717425963

Epoch: 99| Step: 0
Training loss: 2.7341213435639222
Validation loss: 2.479691194829209

Epoch: 5| Step: 1
Training loss: 2.7147013016248134
Validation loss: 2.4788631178578635

Epoch: 5| Step: 2
Training loss: 2.3920824183551064
Validation loss: 2.4735181771618007

Epoch: 5| Step: 3
Training loss: 2.3196669171733295
Validation loss: 2.4758115205431963

Epoch: 5| Step: 4
Training loss: 2.999391494072889
Validation loss: 2.4788464344725503

Epoch: 5| Step: 5
Training loss: 2.2853951614559675
Validation loss: 2.4708423519988467

Epoch: 5| Step: 6
Training loss: 2.6283449024275605
Validation loss: 2.4744980487706596

Epoch: 5| Step: 7
Training loss: 2.7301577062083875
Validation loss: 2.472326508803676

Epoch: 5| Step: 8
Training loss: 2.718530404879787
Validation loss: 2.475497420896038

Epoch: 5| Step: 9
Training loss: 2.6182522102608154
Validation loss: 2.48227607384919

Epoch: 5| Step: 10
Training loss: 2.3293261405747883
Validation loss: 2.4814481148471894

Epoch: 5| Step: 11
Training loss: 2.7466703544826503
Validation loss: 2.475435331379435

Epoch: 100| Step: 0
Training loss: 2.705958650688935
Validation loss: 2.4795109779554383

Epoch: 5| Step: 1
Training loss: 2.9847958253077236
Validation loss: 2.477825624871635

Epoch: 5| Step: 2
Training loss: 2.6071908912787523
Validation loss: 2.4831256084055733

Epoch: 5| Step: 3
Training loss: 2.3362164283001734
Validation loss: 2.482755429008268

Epoch: 5| Step: 4
Training loss: 3.0257212764155446
Validation loss: 2.493030409537053

Epoch: 5| Step: 5
Training loss: 2.313989314352172
Validation loss: 2.5021506674983924

Epoch: 5| Step: 6
Training loss: 2.535401789408166
Validation loss: 2.5026710347832877

Epoch: 5| Step: 7
Training loss: 2.641137248357352
Validation loss: 2.5070264976918453

Epoch: 5| Step: 8
Training loss: 2.5282065851087614
Validation loss: 2.5060324030599315

Epoch: 5| Step: 9
Training loss: 2.7579996623646927
Validation loss: 2.506194728771415

Epoch: 5| Step: 10
Training loss: 2.485286809231362
Validation loss: 2.5054136827866533

Epoch: 5| Step: 11
Training loss: 1.7511687463157628
Validation loss: 2.502509077940439

Epoch: 101| Step: 0
Training loss: 2.548829060404522
Validation loss: 2.504764717198421

Epoch: 5| Step: 1
Training loss: 2.6911728180529764
Validation loss: 2.5061486608195787

Epoch: 5| Step: 2
Training loss: 2.6212813058710958
Validation loss: 2.5048169856751175

Epoch: 5| Step: 3
Training loss: 2.6253163283349346
Validation loss: 2.5031284744994218

Epoch: 5| Step: 4
Training loss: 2.563091395915233
Validation loss: 2.50324273009181

Epoch: 5| Step: 5
Training loss: 2.447445265379058
Validation loss: 2.4957431911202526

Epoch: 5| Step: 6
Training loss: 2.893675642932612
Validation loss: 2.4936703462873986

Epoch: 5| Step: 7
Training loss: 2.516337039236147
Validation loss: 2.4895842577455047

Epoch: 5| Step: 8
Training loss: 2.6045123265063155
Validation loss: 2.488893749920842

Epoch: 5| Step: 9
Training loss: 2.5962897126600124
Validation loss: 2.485276844273007

Epoch: 5| Step: 10
Training loss: 2.7494520161588847
Validation loss: 2.4890824230328272

Epoch: 5| Step: 11
Training loss: 2.0658188906348514
Validation loss: 2.486836487570221

Epoch: 102| Step: 0
Training loss: 2.5964290157894174
Validation loss: 2.4832507494714706

Epoch: 5| Step: 1
Training loss: 2.3801030998979433
Validation loss: 2.479308390106054

Epoch: 5| Step: 2
Training loss: 2.5102745638525863
Validation loss: 2.4776076388914454

Epoch: 5| Step: 3
Training loss: 2.85399885217955
Validation loss: 2.479652843301262

Epoch: 5| Step: 4
Training loss: 2.3975861371013463
Validation loss: 2.4752242707582046

Epoch: 5| Step: 5
Training loss: 2.4656420108803045
Validation loss: 2.47701551704413

Epoch: 5| Step: 6
Training loss: 2.557580179279548
Validation loss: 2.474431981706284

Epoch: 5| Step: 7
Training loss: 2.9357037835435498
Validation loss: 2.4692905252783586

Epoch: 5| Step: 8
Training loss: 2.6871306586971913
Validation loss: 2.476071554754368

Epoch: 5| Step: 9
Training loss: 2.4547615180945326
Validation loss: 2.4728638218860364

Epoch: 5| Step: 10
Training loss: 2.4989956745783455
Validation loss: 2.4708992258284277

Epoch: 5| Step: 11
Training loss: 2.97321634116607
Validation loss: 2.4785079895815767

Epoch: 103| Step: 0
Training loss: 2.429679159168511
Validation loss: 2.473219338855037

Epoch: 5| Step: 1
Training loss: 2.1815295895535445
Validation loss: 2.4733129455924856

Epoch: 5| Step: 2
Training loss: 2.5574563796275216
Validation loss: 2.4648739530281687

Epoch: 5| Step: 3
Training loss: 2.862183072731983
Validation loss: 2.4779376034076366

Epoch: 5| Step: 4
Training loss: 2.6297410203606515
Validation loss: 2.4720845980383537

Epoch: 5| Step: 5
Training loss: 2.979127434603368
Validation loss: 2.476901908585242

Epoch: 5| Step: 6
Training loss: 2.6642608121294784
Validation loss: 2.4727282722032005

Epoch: 5| Step: 7
Training loss: 2.272390222699163
Validation loss: 2.4735516719208257

Epoch: 5| Step: 8
Training loss: 2.5570148284506016
Validation loss: 2.48004711502451

Epoch: 5| Step: 9
Training loss: 2.619326591290135
Validation loss: 2.477724763283371

Epoch: 5| Step: 10
Training loss: 2.6324673661313143
Validation loss: 2.482203984496178

Epoch: 5| Step: 11
Training loss: 2.138045601618073
Validation loss: 2.4703284340106704

Epoch: 104| Step: 0
Training loss: 2.559193222055525
Validation loss: 2.475217307469708

Epoch: 5| Step: 1
Training loss: 2.3065889435097207
Validation loss: 2.4683558475344216

Epoch: 5| Step: 2
Training loss: 2.8685625401501933
Validation loss: 2.4820201717838306

Epoch: 5| Step: 3
Training loss: 2.9999171881372244
Validation loss: 2.480325225122001

Epoch: 5| Step: 4
Training loss: 3.0732286887513363
Validation loss: 2.472376328985988

Epoch: 5| Step: 5
Training loss: 1.9133181000257082
Validation loss: 2.4766800428348463

Epoch: 5| Step: 6
Training loss: 2.909691854200607
Validation loss: 2.4708009522060066

Epoch: 5| Step: 7
Training loss: 2.5692409148382724
Validation loss: 2.4659366290667375

Epoch: 5| Step: 8
Training loss: 2.3406392116140653
Validation loss: 2.4701867972588527

Epoch: 5| Step: 9
Training loss: 1.9988797149650306
Validation loss: 2.470964396336673

Epoch: 5| Step: 10
Training loss: 2.686483235198789
Validation loss: 2.4695237110235664

Epoch: 5| Step: 11
Training loss: 1.4687821405018935
Validation loss: 2.4691654008747745

Epoch: 105| Step: 0
Training loss: 2.320353600751302
Validation loss: 2.472540127004632

Epoch: 5| Step: 1
Training loss: 2.716265101875775
Validation loss: 2.4733701441725473

Epoch: 5| Step: 2
Training loss: 2.700603919129041
Validation loss: 2.469741614092113

Epoch: 5| Step: 3
Training loss: 2.7260096298237997
Validation loss: 2.4730143375653397

Epoch: 5| Step: 4
Training loss: 2.1878328342634297
Validation loss: 2.476435283999587

Epoch: 5| Step: 5
Training loss: 2.2083736152093985
Validation loss: 2.471134334121996

Epoch: 5| Step: 6
Training loss: 2.783203810306933
Validation loss: 2.467622133142067

Epoch: 5| Step: 7
Training loss: 2.8027500271775487
Validation loss: 2.471012651960165

Epoch: 5| Step: 8
Training loss: 2.8311752814734503
Validation loss: 2.4735901140277012

Epoch: 5| Step: 9
Training loss: 2.3374664508227543
Validation loss: 2.4676463943948397

Epoch: 5| Step: 10
Training loss: 2.4640100099806146
Validation loss: 2.4664188698449885

Epoch: 5| Step: 11
Training loss: 2.779478730654393
Validation loss: 2.4691559180202667

Epoch: 106| Step: 0
Training loss: 2.9130453425976373
Validation loss: 2.460883835303291

Epoch: 5| Step: 1
Training loss: 2.5008812304897874
Validation loss: 2.464982239973249

Epoch: 5| Step: 2
Training loss: 2.2114394194455533
Validation loss: 2.462176105091866

Epoch: 5| Step: 3
Training loss: 2.429803189286279
Validation loss: 2.4654544531207843

Epoch: 5| Step: 4
Training loss: 2.700998870318338
Validation loss: 2.4644468968392785

Epoch: 5| Step: 5
Training loss: 2.684491447436528
Validation loss: 2.473835684429798

Epoch: 5| Step: 6
Training loss: 2.83159193117318
Validation loss: 2.461324312196737

Epoch: 5| Step: 7
Training loss: 2.2476100944736355
Validation loss: 2.4634384727774523

Epoch: 5| Step: 8
Training loss: 2.8515959437251155
Validation loss: 2.465968943733689

Epoch: 5| Step: 9
Training loss: 2.7708259429450655
Validation loss: 2.4693403302765384

Epoch: 5| Step: 10
Training loss: 1.75558201314543
Validation loss: 2.4602232809940077

Epoch: 5| Step: 11
Training loss: 2.903819390599673
Validation loss: 2.4706681642333095

Epoch: 107| Step: 0
Training loss: 2.3415663655838572
Validation loss: 2.463846993562041

Epoch: 5| Step: 1
Training loss: 2.888097039132565
Validation loss: 2.46350121548907

Epoch: 5| Step: 2
Training loss: 2.358517320373223
Validation loss: 2.465617463987153

Epoch: 5| Step: 3
Training loss: 2.3838626736125272
Validation loss: 2.4596055074273084

Epoch: 5| Step: 4
Training loss: 2.8763005176204337
Validation loss: 2.463062136206911

Epoch: 5| Step: 5
Training loss: 2.8588144555475368
Validation loss: 2.461980632934607

Epoch: 5| Step: 6
Training loss: 2.765336705930757
Validation loss: 2.4663843739775646

Epoch: 5| Step: 7
Training loss: 2.0558553067586933
Validation loss: 2.4719744802552213

Epoch: 5| Step: 8
Training loss: 2.3996926667209606
Validation loss: 2.463504808457987

Epoch: 5| Step: 9
Training loss: 2.5878004295897723
Validation loss: 2.4629702049747504

Epoch: 5| Step: 10
Training loss: 2.6252348658531703
Validation loss: 2.4674305307116327

Epoch: 5| Step: 11
Training loss: 2.1423127141260685
Validation loss: 2.471945666055801

Epoch: 108| Step: 0
Training loss: 2.580004013117545
Validation loss: 2.46233936728277

Epoch: 5| Step: 1
Training loss: 2.6004798629735077
Validation loss: 2.465336953305122

Epoch: 5| Step: 2
Training loss: 2.845198042535021
Validation loss: 2.4640687146186067

Epoch: 5| Step: 3
Training loss: 2.446222885363619
Validation loss: 2.4670382170462335

Epoch: 5| Step: 4
Training loss: 2.2748728412681025
Validation loss: 2.4646368045654783

Epoch: 5| Step: 5
Training loss: 2.4746505143766147
Validation loss: 2.466060879998721

Epoch: 5| Step: 6
Training loss: 2.8695602369830673
Validation loss: 2.470513446347156

Epoch: 5| Step: 7
Training loss: 2.2711192893005223
Validation loss: 2.4761517742701358

Epoch: 5| Step: 8
Training loss: 2.118753339826714
Validation loss: 2.477341295808707

Epoch: 5| Step: 9
Training loss: 2.9010080624362824
Validation loss: 2.4797823299101966

Epoch: 5| Step: 10
Training loss: 2.808257569631493
Validation loss: 2.477243396046115

Epoch: 5| Step: 11
Training loss: 2.5242777745185783
Validation loss: 2.472572887716878

Epoch: 109| Step: 0
Training loss: 2.4694476530123124
Validation loss: 2.4729427515168365

Epoch: 5| Step: 1
Training loss: 2.3668423139734274
Validation loss: 2.4769460579639415

Epoch: 5| Step: 2
Training loss: 2.131450287924872
Validation loss: 2.4787208145534354

Epoch: 5| Step: 3
Training loss: 3.2902031232284203
Validation loss: 2.462891242127791

Epoch: 5| Step: 4
Training loss: 2.865623775303037
Validation loss: 2.475923730600967

Epoch: 5| Step: 5
Training loss: 2.6379749611685006
Validation loss: 2.4668474954831265

Epoch: 5| Step: 6
Training loss: 2.228498293301907
Validation loss: 2.463809461871205

Epoch: 5| Step: 7
Training loss: 2.4810048402602964
Validation loss: 2.4636519584134997

Epoch: 5| Step: 8
Training loss: 2.6595220720121455
Validation loss: 2.467003188235683

Epoch: 5| Step: 9
Training loss: 2.403788112233103
Validation loss: 2.4654579304223763

Epoch: 5| Step: 10
Training loss: 2.5120700335428694
Validation loss: 2.4643206477074995

Epoch: 5| Step: 11
Training loss: 0.407646931914237
Validation loss: 2.4699693115286765

Epoch: 110| Step: 0
Training loss: 2.2021961824438936
Validation loss: 2.4762557969979464

Epoch: 5| Step: 1
Training loss: 2.8994510657267427
Validation loss: 2.4740829208859196

Epoch: 5| Step: 2
Training loss: 2.731794960247563
Validation loss: 2.465098102421612

Epoch: 5| Step: 3
Training loss: 2.570789420455073
Validation loss: 2.4614198161590624

Epoch: 5| Step: 4
Training loss: 2.6770761294336034
Validation loss: 2.475470590004645

Epoch: 5| Step: 5
Training loss: 2.6545313941362294
Validation loss: 2.4652164739648814

Epoch: 5| Step: 6
Training loss: 2.4527315443824245
Validation loss: 2.476266135243907

Epoch: 5| Step: 7
Training loss: 2.245059204680253
Validation loss: 2.479085627735322

Epoch: 5| Step: 8
Training loss: 2.2232823889451496
Validation loss: 2.4826577927611453

Epoch: 5| Step: 9
Training loss: 2.6528318065669194
Validation loss: 2.484238998721174

Epoch: 5| Step: 10
Training loss: 3.34159967318865
Validation loss: 2.486897928931615

Epoch: 5| Step: 11
Training loss: 2.239707825032098
Validation loss: 2.4998747953215856

Epoch: 111| Step: 0
Training loss: 2.284567315268509
Validation loss: 2.511030811885686

Epoch: 5| Step: 1
Training loss: 2.59968347823571
Validation loss: 2.514371867849972

Epoch: 5| Step: 2
Training loss: 2.548793421249616
Validation loss: 2.5122032785835575

Epoch: 5| Step: 3
Training loss: 2.774537891711469
Validation loss: 2.509298571558491

Epoch: 5| Step: 4
Training loss: 2.7303997680507486
Validation loss: 2.5145106838975586

Epoch: 5| Step: 5
Training loss: 2.3885856357081545
Validation loss: 2.511398326632093

Epoch: 5| Step: 6
Training loss: 2.799627814079826
Validation loss: 2.4971837055437005

Epoch: 5| Step: 7
Training loss: 2.366960873472625
Validation loss: 2.498111810503841

Epoch: 5| Step: 8
Training loss: 2.9439789765996367
Validation loss: 2.494603862077987

Epoch: 5| Step: 9
Training loss: 2.623038876337395
Validation loss: 2.49017912759768

Epoch: 5| Step: 10
Training loss: 2.769165359238086
Validation loss: 2.48924427570041

Epoch: 5| Step: 11
Training loss: 1.7366687625035229
Validation loss: 2.4791369249392416

Epoch: 112| Step: 0
Training loss: 2.551113697020047
Validation loss: 2.4828997976561515

Epoch: 5| Step: 1
Training loss: 2.3171123181195803
Validation loss: 2.475919144556896

Epoch: 5| Step: 2
Training loss: 2.606436896998404
Validation loss: 2.472689901065695

Epoch: 5| Step: 3
Training loss: 2.840329303186007
Validation loss: 2.466513642889235

Epoch: 5| Step: 4
Training loss: 2.594040727982388
Validation loss: 2.4693907316706554

Epoch: 5| Step: 5
Training loss: 2.597393831464293
Validation loss: 2.461333534627488

Epoch: 5| Step: 6
Training loss: 2.5773123645287987
Validation loss: 2.4741523559514484

Epoch: 5| Step: 7
Training loss: 1.9922801875075902
Validation loss: 2.469553884973511

Epoch: 5| Step: 8
Training loss: 2.6214689392717685
Validation loss: 2.464900024706694

Epoch: 5| Step: 9
Training loss: 2.579269062858818
Validation loss: 2.4627156569839896

Epoch: 5| Step: 10
Training loss: 2.8008427952344888
Validation loss: 2.466622268646635

Epoch: 5| Step: 11
Training loss: 3.1208529612217246
Validation loss: 2.4729649600643104

Epoch: 113| Step: 0
Training loss: 2.459220747612105
Validation loss: 2.467869891450598

Epoch: 5| Step: 1
Training loss: 3.0779549024028254
Validation loss: 2.4804734678048352

Epoch: 5| Step: 2
Training loss: 2.550360698994714
Validation loss: 2.479795657969805

Epoch: 5| Step: 3
Training loss: 2.6022874893066588
Validation loss: 2.485924703268174

Epoch: 5| Step: 4
Training loss: 3.059861428358218
Validation loss: 2.478975943166656

Epoch: 5| Step: 5
Training loss: 2.564222082812667
Validation loss: 2.4758028375533696

Epoch: 5| Step: 6
Training loss: 2.3640822385176987
Validation loss: 2.4726501954915037

Epoch: 5| Step: 7
Training loss: 2.8735716215427347
Validation loss: 2.465211972774709

Epoch: 5| Step: 8
Training loss: 2.424910876022629
Validation loss: 2.4712066418772873

Epoch: 5| Step: 9
Training loss: 2.13135766794971
Validation loss: 2.4654876706195368

Epoch: 5| Step: 10
Training loss: 2.1464862632569606
Validation loss: 2.4644910315166424

Epoch: 5| Step: 11
Training loss: 2.186461938058742
Validation loss: 2.4639764338951724

Epoch: 114| Step: 0
Training loss: 2.7808456126913548
Validation loss: 2.4652586566082024

Epoch: 5| Step: 1
Training loss: 2.8438925235790724
Validation loss: 2.465399363628264

Epoch: 5| Step: 2
Training loss: 2.6074750916268767
Validation loss: 2.4694096050253096

Epoch: 5| Step: 3
Training loss: 2.1461703458656074
Validation loss: 2.4699870442682585

Epoch: 5| Step: 4
Training loss: 2.7068367760231222
Validation loss: 2.468504635975264

Epoch: 5| Step: 5
Training loss: 2.0427653282523215
Validation loss: 2.4689089888872213

Epoch: 5| Step: 6
Training loss: 2.160320696512392
Validation loss: 2.4624124416205038

Epoch: 5| Step: 7
Training loss: 2.906308573470986
Validation loss: 2.4709409415843164

Epoch: 5| Step: 8
Training loss: 2.863427963765514
Validation loss: 2.4703917979430283

Epoch: 5| Step: 9
Training loss: 2.1983384967518695
Validation loss: 2.4715171307348176

Epoch: 5| Step: 10
Training loss: 2.821915358617039
Validation loss: 2.4650759700922533

Epoch: 5| Step: 11
Training loss: 2.0013161143545504
Validation loss: 2.4709125354664083

Epoch: 115| Step: 0
Training loss: 2.555233678265691
Validation loss: 2.472928072917271

Epoch: 5| Step: 1
Training loss: 2.09121858526463
Validation loss: 2.480411975441133

Epoch: 5| Step: 2
Training loss: 2.651745645900175
Validation loss: 2.474371284686989

Epoch: 5| Step: 3
Training loss: 3.0471433863665878
Validation loss: 2.484865935820125

Epoch: 5| Step: 4
Training loss: 2.7755361434295693
Validation loss: 2.4829947158082804

Epoch: 5| Step: 5
Training loss: 2.705769474633717
Validation loss: 2.4856257199388767

Epoch: 5| Step: 6
Training loss: 2.413686976190318
Validation loss: 2.4796099560316454

Epoch: 5| Step: 7
Training loss: 2.520241618002201
Validation loss: 2.468006557276849

Epoch: 5| Step: 8
Training loss: 2.400828409274859
Validation loss: 2.4686145785567915

Epoch: 5| Step: 9
Training loss: 2.7009545581530094
Validation loss: 2.4615846822379317

Epoch: 5| Step: 10
Training loss: 2.2106401762523644
Validation loss: 2.460893043237613

Epoch: 5| Step: 11
Training loss: 2.3316513084285737
Validation loss: 2.46718196324699

Epoch: 116| Step: 0
Training loss: 2.67929332269638
Validation loss: 2.4694615075153976

Epoch: 5| Step: 1
Training loss: 2.7269855160190413
Validation loss: 2.467619774032997

Epoch: 5| Step: 2
Training loss: 2.248400755602566
Validation loss: 2.4681620803176636

Epoch: 5| Step: 3
Training loss: 2.756248051995175
Validation loss: 2.4675693183497405

Epoch: 5| Step: 4
Training loss: 3.021632086725533
Validation loss: 2.46568701456178

Epoch: 5| Step: 5
Training loss: 2.9282603946026784
Validation loss: 2.461220469681078

Epoch: 5| Step: 6
Training loss: 2.3743659729169697
Validation loss: 2.465327743835811

Epoch: 5| Step: 7
Training loss: 2.3724739293554293
Validation loss: 2.463463523275963

Epoch: 5| Step: 8
Training loss: 2.4157736432729426
Validation loss: 2.464469147684169

Epoch: 5| Step: 9
Training loss: 2.2901721126727708
Validation loss: 2.4647815613878987

Epoch: 5| Step: 10
Training loss: 2.3596617669657616
Validation loss: 2.463231655290644

Epoch: 5| Step: 11
Training loss: 1.7094226558534946
Validation loss: 2.4639284315547547

Epoch: 117| Step: 0
Training loss: 2.146151793715319
Validation loss: 2.4686110493633127

Epoch: 5| Step: 1
Training loss: 1.9167791900223177
Validation loss: 2.4638038694652575

Epoch: 5| Step: 2
Training loss: 2.3831004484699254
Validation loss: 2.4661249175321047

Epoch: 5| Step: 3
Training loss: 2.6682991951405297
Validation loss: 2.4649025758392784

Epoch: 5| Step: 4
Training loss: 3.0267322341045917
Validation loss: 2.4718148184393622

Epoch: 5| Step: 5
Training loss: 2.4780556783324648
Validation loss: 2.477896342205132

Epoch: 5| Step: 6
Training loss: 2.3448209223256002
Validation loss: 2.470841391090862

Epoch: 5| Step: 7
Training loss: 3.082528000555152
Validation loss: 2.4727957085066206

Epoch: 5| Step: 8
Training loss: 2.4900155004915105
Validation loss: 2.4795717755215363

Epoch: 5| Step: 9
Training loss: 2.496786722811739
Validation loss: 2.4658140921435567

Epoch: 5| Step: 10
Training loss: 2.79136532727621
Validation loss: 2.4716527428195167

Epoch: 5| Step: 11
Training loss: 2.9400065664620283
Validation loss: 2.464162257756802

Epoch: 118| Step: 0
Training loss: 2.6645981594107684
Validation loss: 2.4752338547797312

Epoch: 5| Step: 1
Training loss: 2.09863985609858
Validation loss: 2.4713923803389153

Epoch: 5| Step: 2
Training loss: 2.2958615720734947
Validation loss: 2.469659569333087

Epoch: 5| Step: 3
Training loss: 2.8511087971148767
Validation loss: 2.467697567082294

Epoch: 5| Step: 4
Training loss: 2.5412996266815964
Validation loss: 2.478827799282934

Epoch: 5| Step: 5
Training loss: 2.5062762632444646
Validation loss: 2.477771796719166

Epoch: 5| Step: 6
Training loss: 2.7257916691658313
Validation loss: 2.4719210672245384

Epoch: 5| Step: 7
Training loss: 2.9396072296605973
Validation loss: 2.4754951214607117

Epoch: 5| Step: 8
Training loss: 2.108643581597285
Validation loss: 2.475030439202494

Epoch: 5| Step: 9
Training loss: 2.770498862670599
Validation loss: 2.475689526559428

Epoch: 5| Step: 10
Training loss: 2.6278816028790186
Validation loss: 2.470250828371481

Epoch: 5| Step: 11
Training loss: 2.7862475639322346
Validation loss: 2.474488345475551

Epoch: 119| Step: 0
Training loss: 2.1597889041180767
Validation loss: 2.4678923489284244

Epoch: 5| Step: 1
Training loss: 3.119438716090185
Validation loss: 2.47567793394923

Epoch: 5| Step: 2
Training loss: 2.3385163938067195
Validation loss: 2.4609210866552313

Epoch: 5| Step: 3
Training loss: 2.4981929924220174
Validation loss: 2.4750682081270683

Epoch: 5| Step: 4
Training loss: 1.81780440141311
Validation loss: 2.4785152522478358

Epoch: 5| Step: 5
Training loss: 2.8226236188930867
Validation loss: 2.481606730545844

Epoch: 5| Step: 6
Training loss: 2.744822135818912
Validation loss: 2.490260600032968

Epoch: 5| Step: 7
Training loss: 2.874478168398672
Validation loss: 2.4792187923005775

Epoch: 5| Step: 8
Training loss: 2.2466224114599394
Validation loss: 2.4703143430860197

Epoch: 5| Step: 9
Training loss: 2.755901333646965
Validation loss: 2.4648927581960947

Epoch: 5| Step: 10
Training loss: 2.4275227314035006
Validation loss: 2.465498126544549

Epoch: 5| Step: 11
Training loss: 3.713236977738458
Validation loss: 2.460282645315523

Epoch: 120| Step: 0
Training loss: 2.1018307879763776
Validation loss: 2.4620440180332577

Epoch: 5| Step: 1
Training loss: 2.2734864744174597
Validation loss: 2.457034221701955

Epoch: 5| Step: 2
Training loss: 3.351855854276356
Validation loss: 2.457332788672864

Epoch: 5| Step: 3
Training loss: 2.508239899814117
Validation loss: 2.4691725622766127

Epoch: 5| Step: 4
Training loss: 2.7461909877876756
Validation loss: 2.4641638542041417

Epoch: 5| Step: 5
Training loss: 2.3201047245592954
Validation loss: 2.4663440996583725

Epoch: 5| Step: 6
Training loss: 2.6550778663840537
Validation loss: 2.4633170073148443

Epoch: 5| Step: 7
Training loss: 2.729066851782903
Validation loss: 2.4656620067982846

Epoch: 5| Step: 8
Training loss: 2.3336041384179342
Validation loss: 2.4712125511850167

Epoch: 5| Step: 9
Training loss: 2.880875595822978
Validation loss: 2.465469365592796

Epoch: 5| Step: 10
Training loss: 2.1622474302412664
Validation loss: 2.467319017463917

Epoch: 5| Step: 11
Training loss: 1.955844603144268
Validation loss: 2.4647331152508922

Epoch: 121| Step: 0
Training loss: 2.7928381402481084
Validation loss: 2.4646397408979075

Epoch: 5| Step: 1
Training loss: 2.693510741474947
Validation loss: 2.463512143582526

Epoch: 5| Step: 2
Training loss: 3.2069213699678913
Validation loss: 2.4713973526284905

Epoch: 5| Step: 3
Training loss: 2.6271232918572545
Validation loss: 2.471406951490659

Epoch: 5| Step: 4
Training loss: 2.351113818380812
Validation loss: 2.467086529331578

Epoch: 5| Step: 5
Training loss: 2.870408081390334
Validation loss: 2.4711950443172954

Epoch: 5| Step: 6
Training loss: 2.171270601990708
Validation loss: 2.4726634575375677

Epoch: 5| Step: 7
Training loss: 2.6783652198980756
Validation loss: 2.4666450193997482

Epoch: 5| Step: 8
Training loss: 2.3175898032293625
Validation loss: 2.456252718172519

Epoch: 5| Step: 9
Training loss: 2.492797107340563
Validation loss: 2.47112025985303

Epoch: 5| Step: 10
Training loss: 2.061020522798827
Validation loss: 2.460433080738517

Epoch: 5| Step: 11
Training loss: 1.8949257351457152
Validation loss: 2.4651655822320646

Epoch: 122| Step: 0
Training loss: 2.698780338881881
Validation loss: 2.4683955497443293

Epoch: 5| Step: 1
Training loss: 2.510332784135058
Validation loss: 2.460995208225083

Epoch: 5| Step: 2
Training loss: 2.9287034480147653
Validation loss: 2.4617595530810417

Epoch: 5| Step: 3
Training loss: 2.525525531277988
Validation loss: 2.458903777215852

Epoch: 5| Step: 4
Training loss: 2.6160019651924706
Validation loss: 2.458490271057891

Epoch: 5| Step: 5
Training loss: 2.4838513476232515
Validation loss: 2.4714750709879088

Epoch: 5| Step: 6
Training loss: 2.1291809464763993
Validation loss: 2.473442127515039

Epoch: 5| Step: 7
Training loss: 2.453153767994136
Validation loss: 2.462593978373025

Epoch: 5| Step: 8
Training loss: 2.4169172179029634
Validation loss: 2.4597007065618266

Epoch: 5| Step: 9
Training loss: 2.63230198288408
Validation loss: 2.4604537569157974

Epoch: 5| Step: 10
Training loss: 2.7734961275163417
Validation loss: 2.458692432835724

Epoch: 5| Step: 11
Training loss: 2.1126583965440986
Validation loss: 2.4625560423320167

Epoch: 123| Step: 0
Training loss: 2.673348271604866
Validation loss: 2.46698450385238

Epoch: 5| Step: 1
Training loss: 2.674630333857471
Validation loss: 2.4683966846587317

Epoch: 5| Step: 2
Training loss: 2.4654785886355834
Validation loss: 2.4658234589308

Epoch: 5| Step: 3
Training loss: 2.4153190231481934
Validation loss: 2.4640840628226166

Epoch: 5| Step: 4
Training loss: 2.133214444582904
Validation loss: 2.4738520964181845

Epoch: 5| Step: 5
Training loss: 2.9328101847603407
Validation loss: 2.4628913954013365

Epoch: 5| Step: 6
Training loss: 2.9333479736425163
Validation loss: 2.4711691154334408

Epoch: 5| Step: 7
Training loss: 2.3713312422891786
Validation loss: 2.468137242530025

Epoch: 5| Step: 8
Training loss: 2.6417496673188117
Validation loss: 2.4720132362303016

Epoch: 5| Step: 9
Training loss: 2.324681545310965
Validation loss: 2.4679637976912012

Epoch: 5| Step: 10
Training loss: 2.293358946683074
Validation loss: 2.4654010559803283

Epoch: 5| Step: 11
Training loss: 2.958429326035766
Validation loss: 2.465433772543891

Epoch: 124| Step: 0
Training loss: 2.5388491496902317
Validation loss: 2.461903736617917

Epoch: 5| Step: 1
Training loss: 2.801048361837761
Validation loss: 2.4671979121607954

Epoch: 5| Step: 2
Training loss: 2.945129388839206
Validation loss: 2.4573984361981114

Epoch: 5| Step: 3
Training loss: 2.3949032069897123
Validation loss: 2.4739307294945245

Epoch: 5| Step: 4
Training loss: 2.2618105644077175
Validation loss: 2.4702909304187677

Epoch: 5| Step: 5
Training loss: 2.7684998306679462
Validation loss: 2.4766609661692156

Epoch: 5| Step: 6
Training loss: 1.7839684071587363
Validation loss: 2.4658925021340328

Epoch: 5| Step: 7
Training loss: 2.9854168895677873
Validation loss: 2.465346745002425

Epoch: 5| Step: 8
Training loss: 2.685979102291296
Validation loss: 2.4680365927962447

Epoch: 5| Step: 9
Training loss: 2.7171345438521315
Validation loss: 2.4639857189703323

Epoch: 5| Step: 10
Training loss: 2.13870777117421
Validation loss: 2.4649294048888835

Epoch: 5| Step: 11
Training loss: 1.7485574498762053
Validation loss: 2.468396032686692

Epoch: 125| Step: 0
Training loss: 2.3209340711152007
Validation loss: 2.472175937182404

Epoch: 5| Step: 1
Training loss: 2.3818387402607537
Validation loss: 2.4739977314806803

Epoch: 5| Step: 2
Training loss: 2.759014526876468
Validation loss: 2.4733441015310733

Epoch: 5| Step: 3
Training loss: 1.8799146456314497
Validation loss: 2.478150128609569

Epoch: 5| Step: 4
Training loss: 1.9851833709461522
Validation loss: 2.477894931005332

Epoch: 5| Step: 5
Training loss: 2.95893200113509
Validation loss: 2.4769098978843265

Epoch: 5| Step: 6
Training loss: 2.5478860919289676
Validation loss: 2.480156041255886

Epoch: 5| Step: 7
Training loss: 3.059297093460308
Validation loss: 2.4805928797463253

Epoch: 5| Step: 8
Training loss: 2.7477386887423134
Validation loss: 2.47079217923668

Epoch: 5| Step: 9
Training loss: 2.51081188673433
Validation loss: 2.4729161259030077

Epoch: 5| Step: 10
Training loss: 2.7076411805544467
Validation loss: 2.4679674324659255

Epoch: 5| Step: 11
Training loss: 3.2408223782706758
Validation loss: 2.469225720954538

Epoch: 126| Step: 0
Training loss: 2.3531069325517167
Validation loss: 2.47090430364168

Epoch: 5| Step: 1
Training loss: 2.6139271027864646
Validation loss: 2.473640306343206

Epoch: 5| Step: 2
Training loss: 2.5939978056040287
Validation loss: 2.4672870043353767

Epoch: 5| Step: 3
Training loss: 2.025937219066915
Validation loss: 2.4703103056030695

Epoch: 5| Step: 4
Training loss: 3.095327418299565
Validation loss: 2.470943132687339

Epoch: 5| Step: 5
Training loss: 2.7471416530573904
Validation loss: 2.4632190986468734

Epoch: 5| Step: 6
Training loss: 2.572058736697032
Validation loss: 2.4641138720111067

Epoch: 5| Step: 7
Training loss: 2.3684306389910232
Validation loss: 2.4691185052491744

Epoch: 5| Step: 8
Training loss: 2.4368394788142718
Validation loss: 2.4587012206958194

Epoch: 5| Step: 9
Training loss: 2.6282132781417094
Validation loss: 2.4571056991481246

Epoch: 5| Step: 10
Training loss: 2.5314670045869647
Validation loss: 2.458076634717273

Epoch: 5| Step: 11
Training loss: 2.1243675637178154
Validation loss: 2.4586789701771803

Epoch: 127| Step: 0
Training loss: 2.616383990024793
Validation loss: 2.460111533927899

Epoch: 5| Step: 1
Training loss: 2.3025602189099295
Validation loss: 2.4666002667893587

Epoch: 5| Step: 2
Training loss: 2.5763147645645477
Validation loss: 2.4636024658464444

Epoch: 5| Step: 3
Training loss: 2.658780407357313
Validation loss: 2.460474949716254

Epoch: 5| Step: 4
Training loss: 2.380625637431922
Validation loss: 2.463013131918872

Epoch: 5| Step: 5
Training loss: 2.313792099695916
Validation loss: 2.4635921309116546

Epoch: 5| Step: 6
Training loss: 2.894982265490565
Validation loss: 2.4636155326420446

Epoch: 5| Step: 7
Training loss: 2.358038517437584
Validation loss: 2.4702848520165364

Epoch: 5| Step: 8
Training loss: 2.62208640663706
Validation loss: 2.4636393857666214

Epoch: 5| Step: 9
Training loss: 2.532259798548998
Validation loss: 2.467338959528591

Epoch: 5| Step: 10
Training loss: 2.7508021831934792
Validation loss: 2.468815247361578

Epoch: 5| Step: 11
Training loss: 1.5076030526435698
Validation loss: 2.470032105526782

Epoch: 128| Step: 0
Training loss: 2.6462409438895915
Validation loss: 2.4719864579418016

Epoch: 5| Step: 1
Training loss: 2.4726236591330655
Validation loss: 2.4723763852385496

Epoch: 5| Step: 2
Training loss: 2.1290292126903214
Validation loss: 2.479403241407536

Epoch: 5| Step: 3
Training loss: 2.912795213137085
Validation loss: 2.4676053616794618

Epoch: 5| Step: 4
Training loss: 2.4913697051953343
Validation loss: 2.4645251205698

Epoch: 5| Step: 5
Training loss: 2.6950659597672812
Validation loss: 2.4659514358958856

Epoch: 5| Step: 6
Training loss: 2.7227030111098314
Validation loss: 2.4717477753664805

Epoch: 5| Step: 7
Training loss: 2.8078819508313893
Validation loss: 2.4754443326901314

Epoch: 5| Step: 8
Training loss: 2.394567094195424
Validation loss: 2.4646884951938732

Epoch: 5| Step: 9
Training loss: 2.278517458251341
Validation loss: 2.456335000435107

Epoch: 5| Step: 10
Training loss: 2.205433819366159
Validation loss: 2.4644912290305467

Epoch: 5| Step: 11
Training loss: 2.8011976541703536
Validation loss: 2.4675920925778705

Epoch: 129| Step: 0
Training loss: 2.4971715185298233
Validation loss: 2.469078665798182

Epoch: 5| Step: 1
Training loss: 2.3692007033615448
Validation loss: 2.462713099552163

Epoch: 5| Step: 2
Training loss: 3.0439066194505
Validation loss: 2.4730059159029816

Epoch: 5| Step: 3
Training loss: 2.5311507217341394
Validation loss: 2.466662014868792

Epoch: 5| Step: 4
Training loss: 2.3827545659260942
Validation loss: 2.4727648951730696

Epoch: 5| Step: 5
Training loss: 2.7081360867394637
Validation loss: 2.463840197701233

Epoch: 5| Step: 6
Training loss: 2.5529000564897353
Validation loss: 2.4698398976877045

Epoch: 5| Step: 7
Training loss: 2.278030946147937
Validation loss: 2.4702666891065936

Epoch: 5| Step: 8
Training loss: 2.757228713883573
Validation loss: 2.4757766198752655

Epoch: 5| Step: 9
Training loss: 2.5259160485773173
Validation loss: 2.4645951030274964

Epoch: 5| Step: 10
Training loss: 2.2390584877760853
Validation loss: 2.460103494113442

Epoch: 5| Step: 11
Training loss: 2.0084783140326543
Validation loss: 2.4580285050363946

Epoch: 130| Step: 0
Training loss: 1.7119676806795046
Validation loss: 2.45967220900189

Epoch: 5| Step: 1
Training loss: 2.8061438049749374
Validation loss: 2.461265994152864

Epoch: 5| Step: 2
Training loss: 2.440113622646405
Validation loss: 2.4646357868241835

Epoch: 5| Step: 3
Training loss: 2.214465450428192
Validation loss: 2.46254104162118

Epoch: 5| Step: 4
Training loss: 2.40803788384708
Validation loss: 2.460282596862029

Epoch: 5| Step: 5
Training loss: 2.472156926220277
Validation loss: 2.4515118893775862

Epoch: 5| Step: 6
Training loss: 2.8073151162229153
Validation loss: 2.4694799639742566

Epoch: 5| Step: 7
Training loss: 2.3670108338959417
Validation loss: 2.4650043650840607

Epoch: 5| Step: 8
Training loss: 2.475861748072547
Validation loss: 2.4601251341167485

Epoch: 5| Step: 9
Training loss: 2.7351961483261333
Validation loss: 2.464421995406438

Epoch: 5| Step: 10
Training loss: 2.860226197000904
Validation loss: 2.4650588145793453

Epoch: 5| Step: 11
Training loss: 3.711762603584207
Validation loss: 2.4657196729411006

Epoch: 131| Step: 0
Training loss: 2.2222603980070144
Validation loss: 2.4657312962544857

Epoch: 5| Step: 1
Training loss: 2.9863278056531875
Validation loss: 2.45838007370648

Epoch: 5| Step: 2
Training loss: 2.7453008730733885
Validation loss: 2.468242786308449

Epoch: 5| Step: 3
Training loss: 2.175878553423432
Validation loss: 2.4736299290108748

Epoch: 5| Step: 4
Training loss: 2.579809759652074
Validation loss: 2.482433668219133

Epoch: 5| Step: 5
Training loss: 2.351707884897934
Validation loss: 2.47254327693745

Epoch: 5| Step: 6
Training loss: 2.217799238208575
Validation loss: 2.457896655841321

Epoch: 5| Step: 7
Training loss: 2.8975950595116373
Validation loss: 2.4582014546877913

Epoch: 5| Step: 8
Training loss: 2.3633298758556363
Validation loss: 2.460623936128471

Epoch: 5| Step: 9
Training loss: 2.4127070007065807
Validation loss: 2.465397904981029

Epoch: 5| Step: 10
Training loss: 3.035489607759097
Validation loss: 2.4661595116906994

Epoch: 5| Step: 11
Training loss: 3.258617713271245
Validation loss: 2.4713185263912116

Epoch: 132| Step: 0
Training loss: 2.5483531757860445
Validation loss: 2.477042716206368

Epoch: 5| Step: 1
Training loss: 2.22537521027524
Validation loss: 2.4726987114947114

Epoch: 5| Step: 2
Training loss: 2.9330329206552492
Validation loss: 2.4730036603467584

Epoch: 5| Step: 3
Training loss: 2.3709932962722644
Validation loss: 2.479261798529378

Epoch: 5| Step: 4
Training loss: 2.493519298618767
Validation loss: 2.4833528627250696

Epoch: 5| Step: 5
Training loss: 2.5147561415041637
Validation loss: 2.485380209287416

Epoch: 5| Step: 6
Training loss: 2.98377449425119
Validation loss: 2.4785806032625826

Epoch: 5| Step: 7
Training loss: 2.1411109985200203
Validation loss: 2.4680180812670023

Epoch: 5| Step: 8
Training loss: 2.4317020025511815
Validation loss: 2.4721053455409434

Epoch: 5| Step: 9
Training loss: 2.7605057228165863
Validation loss: 2.4821955239751663

Epoch: 5| Step: 10
Training loss: 2.7591274681828586
Validation loss: 2.4734094124550934

Epoch: 5| Step: 11
Training loss: 1.8140879285083231
Validation loss: 2.4697454473664164

Epoch: 133| Step: 0
Training loss: 3.2266881147044866
Validation loss: 2.468597258498455

Epoch: 5| Step: 1
Training loss: 2.4879694433390394
Validation loss: 2.4578883137360705

Epoch: 5| Step: 2
Training loss: 2.6400699370686236
Validation loss: 2.464575666790279

Epoch: 5| Step: 3
Training loss: 2.424125265863121
Validation loss: 2.4689595660661863

Epoch: 5| Step: 4
Training loss: 2.5246083286288545
Validation loss: 2.4594993425771516

Epoch: 5| Step: 5
Training loss: 2.37199995392772
Validation loss: 2.4668518487156272

Epoch: 5| Step: 6
Training loss: 2.4297170361259908
Validation loss: 2.462541923069268

Epoch: 5| Step: 7
Training loss: 2.541831422896073
Validation loss: 2.4771342936140615

Epoch: 5| Step: 8
Training loss: 2.349778724966445
Validation loss: 2.4656760437310785

Epoch: 5| Step: 9
Training loss: 2.4143763464702475
Validation loss: 2.4708217828576053

Epoch: 5| Step: 10
Training loss: 2.4784380913429387
Validation loss: 2.4685176949312724

Epoch: 5| Step: 11
Training loss: 2.66141026721607
Validation loss: 2.4651935589124987

Epoch: 134| Step: 0
Training loss: 2.693093268047513
Validation loss: 2.465356550764021

Epoch: 5| Step: 1
Training loss: 2.585239927125613
Validation loss: 2.459267100835553

Epoch: 5| Step: 2
Training loss: 2.35923736372912
Validation loss: 2.462784210495661

Epoch: 5| Step: 3
Training loss: 2.6995091592598337
Validation loss: 2.4551678835601445

Epoch: 5| Step: 4
Training loss: 1.927756513257823
Validation loss: 2.4621169962324494

Epoch: 5| Step: 5
Training loss: 2.279395930761829
Validation loss: 2.4696983173887577

Epoch: 5| Step: 6
Training loss: 2.108886096951604
Validation loss: 2.4659732622608552

Epoch: 5| Step: 7
Training loss: 3.2235282319354615
Validation loss: 2.469783699337065

Epoch: 5| Step: 8
Training loss: 2.945880056079943
Validation loss: 2.4632619143815115

Epoch: 5| Step: 9
Training loss: 2.6282578188688164
Validation loss: 2.473216736051104

Epoch: 5| Step: 10
Training loss: 2.145306334240104
Validation loss: 2.46547895530018

Epoch: 5| Step: 11
Training loss: 2.2397894712696442
Validation loss: 2.461295030245553

Epoch: 135| Step: 0
Training loss: 2.7911507618504627
Validation loss: 2.4662642458906907

Epoch: 5| Step: 1
Training loss: 1.960123569524737
Validation loss: 2.4718382227010802

Epoch: 5| Step: 2
Training loss: 2.706205343886759
Validation loss: 2.469325933999628

Epoch: 5| Step: 3
Training loss: 2.553767794091012
Validation loss: 2.4782200910901175

Epoch: 5| Step: 4
Training loss: 2.851402299117249
Validation loss: 2.4844862835030064

Epoch: 5| Step: 5
Training loss: 2.358336971895716
Validation loss: 2.4893807298161916

Epoch: 5| Step: 6
Training loss: 2.262203395222229
Validation loss: 2.4822818727629574

Epoch: 5| Step: 7
Training loss: 2.048235142865675
Validation loss: 2.4866304296208837

Epoch: 5| Step: 8
Training loss: 2.809751142159298
Validation loss: 2.4883904685140106

Epoch: 5| Step: 9
Training loss: 2.796293059602857
Validation loss: 2.4883336491359946

Epoch: 5| Step: 10
Training loss: 2.9291462716215113
Validation loss: 2.477562338509572

Epoch: 5| Step: 11
Training loss: 1.252493612231262
Validation loss: 2.4707015810271855

Epoch: 136| Step: 0
Training loss: 3.1801683966716237
Validation loss: 2.45988586400069

Epoch: 5| Step: 1
Training loss: 2.571812803459618
Validation loss: 2.457621739464721

Epoch: 5| Step: 2
Training loss: 2.0105678784785472
Validation loss: 2.4595252490812065

Epoch: 5| Step: 3
Training loss: 2.6028835848708987
Validation loss: 2.4576458589324988

Epoch: 5| Step: 4
Training loss: 2.2854293790432987
Validation loss: 2.450086315249963

Epoch: 5| Step: 5
Training loss: 2.4040030354878943
Validation loss: 2.4597509238223956

Epoch: 5| Step: 6
Training loss: 2.7011709040862772
Validation loss: 2.4553143476909987

Epoch: 5| Step: 7
Training loss: 1.9324495263065506
Validation loss: 2.4572740080100433

Epoch: 5| Step: 8
Training loss: 2.4425681805343404
Validation loss: 2.4625499992974698

Epoch: 5| Step: 9
Training loss: 2.9257887469655284
Validation loss: 2.454394694282402

Epoch: 5| Step: 10
Training loss: 2.3881477036603687
Validation loss: 2.457153709444818

Epoch: 5| Step: 11
Training loss: 3.3069735797095468
Validation loss: 2.4631160962617322

Epoch: 137| Step: 0
Training loss: 2.35021680684018
Validation loss: 2.4641065185201603

Epoch: 5| Step: 1
Training loss: 2.4963698734389155
Validation loss: 2.4557659972993187

Epoch: 5| Step: 2
Training loss: 2.5608691166236524
Validation loss: 2.458708129749978

Epoch: 5| Step: 3
Training loss: 2.418640142109596
Validation loss: 2.464223683950772

Epoch: 5| Step: 4
Training loss: 2.523516202000501
Validation loss: 2.459176595101846

Epoch: 5| Step: 5
Training loss: 2.9853192977484464
Validation loss: 2.463492085856485

Epoch: 5| Step: 6
Training loss: 2.7869490622145245
Validation loss: 2.4608136423418814

Epoch: 5| Step: 7
Training loss: 2.5435745291209515
Validation loss: 2.4552153572000393

Epoch: 5| Step: 8
Training loss: 2.57995734556557
Validation loss: 2.4617269672630737

Epoch: 5| Step: 9
Training loss: 2.417909291758102
Validation loss: 2.460611343972473

Epoch: 5| Step: 10
Training loss: 2.1261609216113917
Validation loss: 2.4619818515062204

Epoch: 5| Step: 11
Training loss: 2.4207721783458114
Validation loss: 2.45443509991835

Epoch: 138| Step: 0
Training loss: 2.1185494297033034
Validation loss: 2.456006712346995

Epoch: 5| Step: 1
Training loss: 2.6016557923048356
Validation loss: 2.4698756786223037

Epoch: 5| Step: 2
Training loss: 2.193464702954367
Validation loss: 2.464282464123327

Epoch: 5| Step: 3
Training loss: 2.524697759687483
Validation loss: 2.4683245421029176

Epoch: 5| Step: 4
Training loss: 2.553583308982079
Validation loss: 2.465064174430592

Epoch: 5| Step: 5
Training loss: 2.5004350283732073
Validation loss: 2.460891398245298

Epoch: 5| Step: 6
Training loss: 2.4151526893697572
Validation loss: 2.4685753909464023

Epoch: 5| Step: 7
Training loss: 2.5897855680210435
Validation loss: 2.467053003170354

Epoch: 5| Step: 8
Training loss: 2.333049813711241
Validation loss: 2.4634541071896123

Epoch: 5| Step: 9
Training loss: 3.2359352955426157
Validation loss: 2.4616510719092894

Epoch: 5| Step: 10
Training loss: 2.518360711770866
Validation loss: 2.4635671059153235

Epoch: 5| Step: 11
Training loss: 2.6494934317613037
Validation loss: 2.46056919874049

Epoch: 139| Step: 0
Training loss: 2.650795486101665
Validation loss: 2.468273458883687

Epoch: 5| Step: 1
Training loss: 2.553903721959545
Validation loss: 2.4540650334278955

Epoch: 5| Step: 2
Training loss: 2.3770009696615038
Validation loss: 2.4617598880166214

Epoch: 5| Step: 3
Training loss: 2.4076299302324116
Validation loss: 2.4615844703660152

Epoch: 5| Step: 4
Training loss: 2.4745991623087407
Validation loss: 2.4653938594466123

Epoch: 5| Step: 5
Training loss: 2.7685568404147274
Validation loss: 2.466012074281499

Epoch: 5| Step: 6
Training loss: 2.684552195055271
Validation loss: 2.461781376258275

Epoch: 5| Step: 7
Training loss: 2.515589932973271
Validation loss: 2.4551264177224454

Epoch: 5| Step: 8
Training loss: 2.343339706111977
Validation loss: 2.4610077620875632

Epoch: 5| Step: 9
Training loss: 2.3376337227336057
Validation loss: 2.4589999650133256

Epoch: 5| Step: 10
Training loss: 2.5257958885781044
Validation loss: 2.450738839733159

Epoch: 5| Step: 11
Training loss: 2.669944467288513
Validation loss: 2.458162864873858

Epoch: 140| Step: 0
Training loss: 2.325632899290682
Validation loss: 2.459575591092658

Epoch: 5| Step: 1
Training loss: 2.409736485458899
Validation loss: 2.458914345978043

Epoch: 5| Step: 2
Training loss: 2.63706721511024
Validation loss: 2.465941102748745

Epoch: 5| Step: 3
Training loss: 2.7861967350218357
Validation loss: 2.4574717625599383

Epoch: 5| Step: 4
Training loss: 2.5119091572827466
Validation loss: 2.4638626838448383

Epoch: 5| Step: 5
Training loss: 2.3971383120498424
Validation loss: 2.465515754448748

Epoch: 5| Step: 6
Training loss: 2.433179106680301
Validation loss: 2.462635023994815

Epoch: 5| Step: 7
Training loss: 2.640082670420158
Validation loss: 2.457052197389809

Epoch: 5| Step: 8
Training loss: 2.60172965397691
Validation loss: 2.465555728024668

Epoch: 5| Step: 9
Training loss: 2.596194482686371
Validation loss: 2.4595048639893444

Epoch: 5| Step: 10
Training loss: 2.7189887807936586
Validation loss: 2.458693568188611

Epoch: 5| Step: 11
Training loss: 1.0094366662997059
Validation loss: 2.461661330247216

Epoch: 141| Step: 0
Training loss: 2.504503104596566
Validation loss: 2.4644535438971378

Epoch: 5| Step: 1
Training loss: 2.510683786787972
Validation loss: 2.4644344411155736

Epoch: 5| Step: 2
Training loss: 2.6751866926126917
Validation loss: 2.4752153629628797

Epoch: 5| Step: 3
Training loss: 2.6503536528199154
Validation loss: 2.4667973421468368

Epoch: 5| Step: 4
Training loss: 2.4239820605139806
Validation loss: 2.4596388846932165

Epoch: 5| Step: 5
Training loss: 2.2332190878195672
Validation loss: 2.464194799277033

Epoch: 5| Step: 6
Training loss: 3.0171090857155667
Validation loss: 2.4565777018879444

Epoch: 5| Step: 7
Training loss: 2.900936232088538
Validation loss: 2.467738733121822

Epoch: 5| Step: 8
Training loss: 2.547016537717846
Validation loss: 2.4594020231281903

Epoch: 5| Step: 9
Training loss: 2.0592553018564446
Validation loss: 2.4589774950015553

Epoch: 5| Step: 10
Training loss: 2.2440036768415657
Validation loss: 2.467264126631973

Epoch: 5| Step: 11
Training loss: 1.84918478711765
Validation loss: 2.470329636400348

Epoch: 142| Step: 0
Training loss: 2.113102875407263
Validation loss: 2.4603613527192065

Epoch: 5| Step: 1
Training loss: 2.624073955133368
Validation loss: 2.4640794063642053

Epoch: 5| Step: 2
Training loss: 2.6802820374700347
Validation loss: 2.4596404679211026

Epoch: 5| Step: 3
Training loss: 2.498219714474844
Validation loss: 2.457289566426163

Epoch: 5| Step: 4
Training loss: 3.0173820160284404
Validation loss: 2.462741408641631

Epoch: 5| Step: 5
Training loss: 2.4423190676345103
Validation loss: 2.465195237301741

Epoch: 5| Step: 6
Training loss: 2.614428532084486
Validation loss: 2.4605930067066817

Epoch: 5| Step: 7
Training loss: 2.5541591272870336
Validation loss: 2.4622384200285863

Epoch: 5| Step: 8
Training loss: 2.757979174540168
Validation loss: 2.4650932222036466

Epoch: 5| Step: 9
Training loss: 1.6490804855525398
Validation loss: 2.4605858808939156

Epoch: 5| Step: 10
Training loss: 2.5758761683785494
Validation loss: 2.4556567335453034

Epoch: 5| Step: 11
Training loss: 2.433269448508613
Validation loss: 2.46386426637456

Epoch: 143| Step: 0
Training loss: 2.2023260954182744
Validation loss: 2.456909823588552

Epoch: 5| Step: 1
Training loss: 3.0391877057876404
Validation loss: 2.4616065271435663

Epoch: 5| Step: 2
Training loss: 2.4507069910562467
Validation loss: 2.460953594336788

Epoch: 5| Step: 3
Training loss: 2.7542719172124803
Validation loss: 2.455135716036192

Epoch: 5| Step: 4
Training loss: 2.391300081166317
Validation loss: 2.4595305200156488

Epoch: 5| Step: 5
Training loss: 2.756906765910678
Validation loss: 2.4574685367149542

Epoch: 5| Step: 6
Training loss: 2.7003561844596016
Validation loss: 2.4624290184983155

Epoch: 5| Step: 7
Training loss: 2.3572529729908744
Validation loss: 2.467951995705795

Epoch: 5| Step: 8
Training loss: 2.273423447188634
Validation loss: 2.45893306744299

Epoch: 5| Step: 9
Training loss: 2.4993792716941567
Validation loss: 2.4588764137684302

Epoch: 5| Step: 10
Training loss: 2.3028131654313673
Validation loss: 2.4638401009341684

Epoch: 5| Step: 11
Training loss: 1.3678518806350608
Validation loss: 2.458232624340278

Epoch: 144| Step: 0
Training loss: 2.376452955220546
Validation loss: 2.4555114630717245

Epoch: 5| Step: 1
Training loss: 2.7195258075031723
Validation loss: 2.4530177285294634

Epoch: 5| Step: 2
Training loss: 2.8167741299807147
Validation loss: 2.4595647747557345

Epoch: 5| Step: 3
Training loss: 2.476113167322713
Validation loss: 2.45536910512413

Epoch: 5| Step: 4
Training loss: 2.6006266425654023
Validation loss: 2.4606155265617344

Epoch: 5| Step: 5
Training loss: 2.3437819415141155
Validation loss: 2.469337944648078

Epoch: 5| Step: 6
Training loss: 2.360643260704396
Validation loss: 2.4598216882747916

Epoch: 5| Step: 7
Training loss: 2.3314090582454416
Validation loss: 2.4654998027118333

Epoch: 5| Step: 8
Training loss: 2.869423142787289
Validation loss: 2.4680932052546596

Epoch: 5| Step: 9
Training loss: 2.431689158495325
Validation loss: 2.4601756819030998

Epoch: 5| Step: 10
Training loss: 2.4382739672296916
Validation loss: 2.4606997945313878

Epoch: 5| Step: 11
Training loss: 1.774823558795622
Validation loss: 2.4599003175437084

Epoch: 145| Step: 0
Training loss: 2.251771653233983
Validation loss: 2.469043327964911

Epoch: 5| Step: 1
Training loss: 2.016892973231949
Validation loss: 2.4706874922306823

Epoch: 5| Step: 2
Training loss: 2.7657783595092074
Validation loss: 2.4767091308571243

Epoch: 5| Step: 3
Training loss: 2.2866852690419206
Validation loss: 2.4893980090276484

Epoch: 5| Step: 4
Training loss: 2.649616890331005
Validation loss: 2.479999295562726

Epoch: 5| Step: 5
Training loss: 2.346422223378864
Validation loss: 2.4751215211338993

Epoch: 5| Step: 6
Training loss: 2.631546395251067
Validation loss: 2.4636130225161894

Epoch: 5| Step: 7
Training loss: 2.3013788816881235
Validation loss: 2.4594057957725837

Epoch: 5| Step: 8
Training loss: 2.873465750608814
Validation loss: 2.45711885507347

Epoch: 5| Step: 9
Training loss: 2.8122223611064054
Validation loss: 2.4608169082078803

Epoch: 5| Step: 10
Training loss: 2.553250810005062
Validation loss: 2.464669061708443

Epoch: 5| Step: 11
Training loss: 3.8569715174206105
Validation loss: 2.46509738912859

Epoch: 146| Step: 0
Training loss: 2.6868098060239283
Validation loss: 2.4589310676385794

Epoch: 5| Step: 1
Training loss: 1.8702860384975744
Validation loss: 2.468802403243923

Epoch: 5| Step: 2
Training loss: 2.905952028155559
Validation loss: 2.467106795405308

Epoch: 5| Step: 3
Training loss: 2.679045397286626
Validation loss: 2.4721586460935057

Epoch: 5| Step: 4
Training loss: 2.7719160177987003
Validation loss: 2.4682544339629353

Epoch: 5| Step: 5
Training loss: 2.001902986224273
Validation loss: 2.46442818904412

Epoch: 5| Step: 6
Training loss: 2.416681936369262
Validation loss: 2.472914872547796

Epoch: 5| Step: 7
Training loss: 2.5703368026372924
Validation loss: 2.469424771188935

Epoch: 5| Step: 8
Training loss: 2.37001739656526
Validation loss: 2.464062616835684

Epoch: 5| Step: 9
Training loss: 2.974962660611264
Validation loss: 2.466419282688866

Epoch: 5| Step: 10
Training loss: 2.458005872565782
Validation loss: 2.4630673874727798

Epoch: 5| Step: 11
Training loss: 2.240125605263231
Validation loss: 2.457608604421217

Epoch: 147| Step: 0
Training loss: 2.5287417947859496
Validation loss: 2.4590384083760797

Epoch: 5| Step: 1
Training loss: 2.7417309155419547
Validation loss: 2.4605725032854306

Epoch: 5| Step: 2
Training loss: 2.4422260342399382
Validation loss: 2.458467703449843

Epoch: 5| Step: 3
Training loss: 2.095373520613318
Validation loss: 2.46662807214245

Epoch: 5| Step: 4
Training loss: 2.6711224840330763
Validation loss: 2.468629504132259

Epoch: 5| Step: 5
Training loss: 2.3988040805463258
Validation loss: 2.478450043804905

Epoch: 5| Step: 6
Training loss: 2.644947644672821
Validation loss: 2.460592615090312

Epoch: 5| Step: 7
Training loss: 2.7808498994880444
Validation loss: 2.4607250465038084

Epoch: 5| Step: 8
Training loss: 2.5223592338188374
Validation loss: 2.451698001563646

Epoch: 5| Step: 9
Training loss: 2.4759961753285684
Validation loss: 2.4520524589930575

Epoch: 5| Step: 10
Training loss: 2.438288145535297
Validation loss: 2.4631825192683845

Epoch: 5| Step: 11
Training loss: 2.4873915298812137
Validation loss: 2.457223141792693

Epoch: 148| Step: 0
Training loss: 2.432532898228584
Validation loss: 2.471945662037061

Epoch: 5| Step: 1
Training loss: 2.152096050430445
Validation loss: 2.457846889752406

Epoch: 5| Step: 2
Training loss: 2.4681684557522527
Validation loss: 2.463091018030643

Epoch: 5| Step: 3
Training loss: 2.312993383613356
Validation loss: 2.4607249011694567

Epoch: 5| Step: 4
Training loss: 2.6379942119092394
Validation loss: 2.4603361010516718

Epoch: 5| Step: 5
Training loss: 2.697115972014886
Validation loss: 2.463224004759245

Epoch: 5| Step: 6
Training loss: 2.688795354186621
Validation loss: 2.468227572634112

Epoch: 5| Step: 7
Training loss: 2.33898930559327
Validation loss: 2.4706373464801947

Epoch: 5| Step: 8
Training loss: 3.0220859079224205
Validation loss: 2.466433468365288

Epoch: 5| Step: 9
Training loss: 2.686039283692407
Validation loss: 2.461727443442838

Epoch: 5| Step: 10
Training loss: 2.4339301548812635
Validation loss: 2.45618466588845

Epoch: 5| Step: 11
Training loss: 1.8927770299486726
Validation loss: 2.4597256456946393

Epoch: 149| Step: 0
Training loss: 2.600704842175856
Validation loss: 2.457360771655327

Epoch: 5| Step: 1
Training loss: 2.5308727407124594
Validation loss: 2.4665133005440003

Epoch: 5| Step: 2
Training loss: 2.5382922133857377
Validation loss: 2.46438321880944

Epoch: 5| Step: 3
Training loss: 2.307040600575256
Validation loss: 2.462782830973375

Epoch: 5| Step: 4
Training loss: 2.7905791617427522
Validation loss: 2.467322798134229

Epoch: 5| Step: 5
Training loss: 2.747262979823748
Validation loss: 2.4747332005279823

Epoch: 5| Step: 6
Training loss: 2.4872213888130252
Validation loss: 2.473360147268727

Epoch: 5| Step: 7
Training loss: 2.6186327232011317
Validation loss: 2.4855883452433924

Epoch: 5| Step: 8
Training loss: 2.5684675186654973
Validation loss: 2.473919654677683

Epoch: 5| Step: 9
Training loss: 2.4251984465895933
Validation loss: 2.4564115615139266

Epoch: 5| Step: 10
Training loss: 2.3599638551433384
Validation loss: 2.4639224523635357

Epoch: 5| Step: 11
Training loss: 3.3779357574840665
Validation loss: 2.4657901089773038

Epoch: 150| Step: 0
Training loss: 2.870505427073837
Validation loss: 2.474163301242258

Epoch: 5| Step: 1
Training loss: 1.7559317466748852
Validation loss: 2.4872527140157374

Epoch: 5| Step: 2
Training loss: 1.956785022832052
Validation loss: 2.475380857300997

Epoch: 5| Step: 3
Training loss: 2.0248908637507648
Validation loss: 2.483995496606302

Epoch: 5| Step: 4
Training loss: 2.5233815662338097
Validation loss: 2.48220634175068

Epoch: 5| Step: 5
Training loss: 2.88323789523134
Validation loss: 2.4818312099567166

Epoch: 5| Step: 6
Training loss: 2.3075801993306264
Validation loss: 2.4767329401037266

Epoch: 5| Step: 7
Training loss: 2.4558284951136367
Validation loss: 2.4824773310547967

Epoch: 5| Step: 8
Training loss: 3.218752666583392
Validation loss: 2.48245078769577

Epoch: 5| Step: 9
Training loss: 2.653445210358601
Validation loss: 2.483058776708522

Epoch: 5| Step: 10
Training loss: 3.23467146394735
Validation loss: 2.4726172992034905

Epoch: 5| Step: 11
Training loss: 0.9221434121627801
Validation loss: 2.4763485628577944

Epoch: 151| Step: 0
Training loss: 2.538563089406865
Validation loss: 2.4788500572974494

Epoch: 5| Step: 1
Training loss: 2.397446617192466
Validation loss: 2.475385508552823

Epoch: 5| Step: 2
Training loss: 2.516827124307731
Validation loss: 2.474127706786511

Epoch: 5| Step: 3
Training loss: 2.877980387447883
Validation loss: 2.4727392599546043

Epoch: 5| Step: 4
Training loss: 2.8876603053128314
Validation loss: 2.4725566480361687

Epoch: 5| Step: 5
Training loss: 2.1805879653930256
Validation loss: 2.4653137734222446

Epoch: 5| Step: 6
Training loss: 2.5805598927454456
Validation loss: 2.459868999407264

Epoch: 5| Step: 7
Training loss: 2.3702659607394048
Validation loss: 2.4620565140944612

Epoch: 5| Step: 8
Training loss: 2.5735045673870127
Validation loss: 2.4679333064299604

Epoch: 5| Step: 9
Training loss: 2.583013073748186
Validation loss: 2.470703434598599

Epoch: 5| Step: 10
Training loss: 2.379148373864749
Validation loss: 2.468801332897769

Epoch: 5| Step: 11
Training loss: 2.5434524849528626
Validation loss: 2.4655688710949315

Epoch: 152| Step: 0
Training loss: 2.2165227107100676
Validation loss: 2.460677723570443

Epoch: 5| Step: 1
Training loss: 2.480114816961114
Validation loss: 2.464361850001387

Epoch: 5| Step: 2
Training loss: 2.672102533657423
Validation loss: 2.463958484532141

Epoch: 5| Step: 3
Training loss: 2.963945697580434
Validation loss: 2.460512102285812

Epoch: 5| Step: 4
Training loss: 3.0637217050566385
Validation loss: 2.460309153271542

Epoch: 5| Step: 5
Training loss: 2.504273291003587
Validation loss: 2.4658849444590154

Epoch: 5| Step: 6
Training loss: 2.982631473633195
Validation loss: 2.4657079246768148

Epoch: 5| Step: 7
Training loss: 2.4583040224903403
Validation loss: 2.471742823877184

Epoch: 5| Step: 8
Training loss: 2.3334159382321658
Validation loss: 2.468762067773353

Epoch: 5| Step: 9
Training loss: 1.9515227803203772
Validation loss: 2.475065876183595

Epoch: 5| Step: 10
Training loss: 1.9674734790958261
Validation loss: 2.46928126216205

Epoch: 5| Step: 11
Training loss: 1.9072495013301476
Validation loss: 2.4666855586355183

Epoch: 153| Step: 0
Training loss: 2.0167261708626154
Validation loss: 2.4677892215708552

Epoch: 5| Step: 1
Training loss: 2.501207346249872
Validation loss: 2.463889483898922

Epoch: 5| Step: 2
Training loss: 2.8976702638617886
Validation loss: 2.4529773602937053

Epoch: 5| Step: 3
Training loss: 2.1454990336429978
Validation loss: 2.456730458780545

Epoch: 5| Step: 4
Training loss: 2.7398786687787666
Validation loss: 2.4673363223376854

Epoch: 5| Step: 5
Training loss: 2.5289031086767544
Validation loss: 2.46733525739491

Epoch: 5| Step: 6
Training loss: 2.773579424598193
Validation loss: 2.466405979011882

Epoch: 5| Step: 7
Training loss: 2.5137155050360787
Validation loss: 2.461775803457186

Epoch: 5| Step: 8
Training loss: 2.1304920135112004
Validation loss: 2.4602358751362177

Epoch: 5| Step: 9
Training loss: 3.1178852724864163
Validation loss: 2.465326853309936

Epoch: 5| Step: 10
Training loss: 2.2170374533896955
Validation loss: 2.4628177544313044

Epoch: 5| Step: 11
Training loss: 1.1157509697925507
Validation loss: 2.462826350092117

Epoch: 154| Step: 0
Training loss: 2.709926713607093
Validation loss: 2.461276262168076

Epoch: 5| Step: 1
Training loss: 2.408331627058681
Validation loss: 2.461263176899063

Epoch: 5| Step: 2
Training loss: 2.4113827742293625
Validation loss: 2.4629274023195604

Epoch: 5| Step: 3
Training loss: 2.156911444454206
Validation loss: 2.452405212481195

Epoch: 5| Step: 4
Training loss: 2.6829571142305904
Validation loss: 2.4622710051297587

Epoch: 5| Step: 5
Training loss: 2.614657235082682
Validation loss: 2.4622084469224355

Epoch: 5| Step: 6
Training loss: 2.34958694967348
Validation loss: 2.457893163800708

Epoch: 5| Step: 7
Training loss: 2.587248040799641
Validation loss: 2.459989100576823

Epoch: 5| Step: 8
Training loss: 3.0111800725093056
Validation loss: 2.4629284147166204

Epoch: 5| Step: 9
Training loss: 2.1938631425247705
Validation loss: 2.4616892155996513

Epoch: 5| Step: 10
Training loss: 2.4337553947243955
Validation loss: 2.4582504377020418

Epoch: 5| Step: 11
Training loss: 2.3772636467773327
Validation loss: 2.461773188554137

Epoch: 155| Step: 0
Training loss: 3.0111427003611895
Validation loss: 2.463855156230947

Epoch: 5| Step: 1
Training loss: 2.378300832496494
Validation loss: 2.473442982988672

Epoch: 5| Step: 2
Training loss: 2.229316373652571
Validation loss: 2.502727074173603

Epoch: 5| Step: 3
Training loss: 2.340371710769012
Validation loss: 2.5260328050176124

Epoch: 5| Step: 4
Training loss: 2.623261920808438
Validation loss: 2.5326666615643374

Epoch: 5| Step: 5
Training loss: 2.7596707661842443
Validation loss: 2.5225378466105624

Epoch: 5| Step: 6
Training loss: 2.835557812371281
Validation loss: 2.495205286788599

Epoch: 5| Step: 7
Training loss: 2.225415814581744
Validation loss: 2.46785686126985

Epoch: 5| Step: 8
Training loss: 2.9546417700800807
Validation loss: 2.4640196214943275

Epoch: 5| Step: 9
Training loss: 2.2294023573394384
Validation loss: 2.4547885470087354

Epoch: 5| Step: 10
Training loss: 2.4179629324256044
Validation loss: 2.4555211604439124

Epoch: 5| Step: 11
Training loss: 2.5563826696761973
Validation loss: 2.4663884380262497

Epoch: 156| Step: 0
Training loss: 2.809111440004996
Validation loss: 2.464103974626462

Epoch: 5| Step: 1
Training loss: 2.5448995780992743
Validation loss: 2.461897357065372

Epoch: 5| Step: 2
Training loss: 1.932577093247251
Validation loss: 2.4603690807920775

Epoch: 5| Step: 3
Training loss: 1.5859381013314746
Validation loss: 2.466393597626478

Epoch: 5| Step: 4
Training loss: 2.3622738038623003
Validation loss: 2.46301443871254

Epoch: 5| Step: 5
Training loss: 2.4378773323849474
Validation loss: 2.4683280334720554

Epoch: 5| Step: 6
Training loss: 2.687695607008695
Validation loss: 2.469366447326427

Epoch: 5| Step: 7
Training loss: 3.0916355498222625
Validation loss: 2.4664812850131295

Epoch: 5| Step: 8
Training loss: 2.995621505980401
Validation loss: 2.4698051700574175

Epoch: 5| Step: 9
Training loss: 2.6471225771363533
Validation loss: 2.462186776813211

Epoch: 5| Step: 10
Training loss: 2.674635236594745
Validation loss: 2.4703737162453328

Epoch: 5| Step: 11
Training loss: 2.5007682573528567
Validation loss: 2.4652587331713813

Epoch: 157| Step: 0
Training loss: 2.6798272722009133
Validation loss: 2.47158578578507

Epoch: 5| Step: 1
Training loss: 2.4436966795116573
Validation loss: 2.460148883791189

Epoch: 5| Step: 2
Training loss: 2.311367479291991
Validation loss: 2.460632955286203

Epoch: 5| Step: 3
Training loss: 2.5063010916888913
Validation loss: 2.455714217970095

Epoch: 5| Step: 4
Training loss: 3.0602930641706263
Validation loss: 2.4633865643657287

Epoch: 5| Step: 5
Training loss: 2.817079587912377
Validation loss: 2.4611994972233346

Epoch: 5| Step: 6
Training loss: 2.7131657745805406
Validation loss: 2.4609703525994413

Epoch: 5| Step: 7
Training loss: 2.690543203849551
Validation loss: 2.4591016309685827

Epoch: 5| Step: 8
Training loss: 1.4048161720349743
Validation loss: 2.4682986052008564

Epoch: 5| Step: 9
Training loss: 2.405038305600102
Validation loss: 2.4644478904727785

Epoch: 5| Step: 10
Training loss: 2.381142753528882
Validation loss: 2.4571930872952987

Epoch: 5| Step: 11
Training loss: 2.2416636423234735
Validation loss: 2.455275951215593

Epoch: 158| Step: 0
Training loss: 1.9981066563408285
Validation loss: 2.4565959922928235

Epoch: 5| Step: 1
Training loss: 2.4014013411306494
Validation loss: 2.4587520765364643

Epoch: 5| Step: 2
Training loss: 2.364310351059418
Validation loss: 2.472587845646085

Epoch: 5| Step: 3
Training loss: 2.254134088963616
Validation loss: 2.4727263518526046

Epoch: 5| Step: 4
Training loss: 2.737158617306488
Validation loss: 2.479374910205083

Epoch: 5| Step: 5
Training loss: 2.5013695780067344
Validation loss: 2.4714152921802364

Epoch: 5| Step: 6
Training loss: 2.2375182337524118
Validation loss: 2.4721781633561055

Epoch: 5| Step: 7
Training loss: 2.466068791626499
Validation loss: 2.462486643359628

Epoch: 5| Step: 8
Training loss: 2.628988777667761
Validation loss: 2.4685513621707096

Epoch: 5| Step: 9
Training loss: 2.7536956923052176
Validation loss: 2.465815252418488

Epoch: 5| Step: 10
Training loss: 3.0251384356640014
Validation loss: 2.4652819155865777

Epoch: 5| Step: 11
Training loss: 2.928734871161376
Validation loss: 2.467805919325261

Epoch: 159| Step: 0
Training loss: 2.5598421036186947
Validation loss: 2.4625077379818348

Epoch: 5| Step: 1
Training loss: 2.2389770278535917
Validation loss: 2.4683468002511764

Epoch: 5| Step: 2
Training loss: 2.6784435532336475
Validation loss: 2.4575570397534032

Epoch: 5| Step: 3
Training loss: 1.927010845848645
Validation loss: 2.4571837037806024

Epoch: 5| Step: 4
Training loss: 2.4582789032508523
Validation loss: 2.467680510308665

Epoch: 5| Step: 5
Training loss: 2.848933599296102
Validation loss: 2.4591272164675764

Epoch: 5| Step: 6
Training loss: 3.067249896385993
Validation loss: 2.4653807920027404

Epoch: 5| Step: 7
Training loss: 2.4284886778830073
Validation loss: 2.461613875995825

Epoch: 5| Step: 8
Training loss: 2.3841694954521238
Validation loss: 2.4582999369944463

Epoch: 5| Step: 9
Training loss: 2.152430925490425
Validation loss: 2.4499075801603483

Epoch: 5| Step: 10
Training loss: 2.487932453232142
Validation loss: 2.4596340784594624

Epoch: 5| Step: 11
Training loss: 3.098041370980012
Validation loss: 2.4571427247716966

Epoch: 160| Step: 0
Training loss: 2.909012005017668
Validation loss: 2.4655795200731982

Epoch: 5| Step: 1
Training loss: 2.532026948722145
Validation loss: 2.462365022016277

Epoch: 5| Step: 2
Training loss: 2.2887080311950854
Validation loss: 2.456579444799962

Epoch: 5| Step: 3
Training loss: 2.7340281239244173
Validation loss: 2.459642955848579

Epoch: 5| Step: 4
Training loss: 2.6467166137146205
Validation loss: 2.4544021213924974

Epoch: 5| Step: 5
Training loss: 2.545152889373376
Validation loss: 2.457512323763201

Epoch: 5| Step: 6
Training loss: 2.632165936958352
Validation loss: 2.459799367119838

Epoch: 5| Step: 7
Training loss: 2.365583726328868
Validation loss: 2.4661243737215925

Epoch: 5| Step: 8
Training loss: 2.4281116539233647
Validation loss: 2.4654741765683057

Epoch: 5| Step: 9
Training loss: 1.9818508885606216
Validation loss: 2.462057502640348

Epoch: 5| Step: 10
Training loss: 2.4340876633742514
Validation loss: 2.4692429239567497

Epoch: 5| Step: 11
Training loss: 2.7156478898430274
Validation loss: 2.4583820012226734

Epoch: 161| Step: 0
Training loss: 2.8485635115413293
Validation loss: 2.45748253960032

Epoch: 5| Step: 1
Training loss: 2.6920680767367595
Validation loss: 2.466364132176471

Epoch: 5| Step: 2
Training loss: 2.372756802133656
Validation loss: 2.4721771426918773

Epoch: 5| Step: 3
Training loss: 1.9697540387198038
Validation loss: 2.471337841262733

Epoch: 5| Step: 4
Training loss: 2.6840620895458267
Validation loss: 2.474665523976937

Epoch: 5| Step: 5
Training loss: 2.678694382524946
Validation loss: 2.475325904446776

Epoch: 5| Step: 6
Training loss: 2.253544452791648
Validation loss: 2.4760655045849664

Epoch: 5| Step: 7
Training loss: 1.8535751156359457
Validation loss: 2.4682225134629006

Epoch: 5| Step: 8
Training loss: 2.5525273981221166
Validation loss: 2.4692075723814577

Epoch: 5| Step: 9
Training loss: 3.359671078431492
Validation loss: 2.4602142623533596

Epoch: 5| Step: 10
Training loss: 2.1239502501758762
Validation loss: 2.4643614589839316

Epoch: 5| Step: 11
Training loss: 2.054992421082863
Validation loss: 2.4587545613225723

Epoch: 162| Step: 0
Training loss: 2.3703367731188045
Validation loss: 2.4618532465063816

Epoch: 5| Step: 1
Training loss: 2.535736064702228
Validation loss: 2.462551600824091

Epoch: 5| Step: 2
Training loss: 2.4489272824851094
Validation loss: 2.4556477952233933

Epoch: 5| Step: 3
Training loss: 2.6514134973798664
Validation loss: 2.4646661153344214

Epoch: 5| Step: 4
Training loss: 2.6557514395973394
Validation loss: 2.4617146995492547

Epoch: 5| Step: 5
Training loss: 2.5337841394097707
Validation loss: 2.460459801061339

Epoch: 5| Step: 6
Training loss: 2.287692444424267
Validation loss: 2.4634107080428844

Epoch: 5| Step: 7
Training loss: 2.4174134536604863
Validation loss: 2.464250520391681

Epoch: 5| Step: 8
Training loss: 2.8747538585615384
Validation loss: 2.463354935669631

Epoch: 5| Step: 9
Training loss: 2.4808660714634665
Validation loss: 2.4638675483565255

Epoch: 5| Step: 10
Training loss: 2.2236516831928745
Validation loss: 2.4569842602431753

Epoch: 5| Step: 11
Training loss: 2.40886536191796
Validation loss: 2.4632366964817938

Epoch: 163| Step: 0
Training loss: 2.1272386091337308
Validation loss: 2.457358523973042

Epoch: 5| Step: 1
Training loss: 2.4014611088154894
Validation loss: 2.459871466911272

Epoch: 5| Step: 2
Training loss: 2.253061860320183
Validation loss: 2.466595413708205

Epoch: 5| Step: 3
Training loss: 2.3621750946264206
Validation loss: 2.4651966195037884

Epoch: 5| Step: 4
Training loss: 2.9865283487284677
Validation loss: 2.4694592869382523

Epoch: 5| Step: 5
Training loss: 1.9186108790357723
Validation loss: 2.4659547271855122

Epoch: 5| Step: 6
Training loss: 3.013045082005699
Validation loss: 2.467521240970048

Epoch: 5| Step: 7
Training loss: 2.798730927062033
Validation loss: 2.4668529118535725

Epoch: 5| Step: 8
Training loss: 2.371867272084228
Validation loss: 2.465668964840114

Epoch: 5| Step: 9
Training loss: 2.2705052923373112
Validation loss: 2.4630983947192058

Epoch: 5| Step: 10
Training loss: 2.6282696116029713
Validation loss: 2.4646298033030045

Epoch: 5| Step: 11
Training loss: 2.4428473285965495
Validation loss: 2.464489560239101

Epoch: 164| Step: 0
Training loss: 2.5248773677242027
Validation loss: 2.468063762108273

Epoch: 5| Step: 1
Training loss: 2.4604806627375315
Validation loss: 2.464767168712345

Epoch: 5| Step: 2
Training loss: 2.343026723522874
Validation loss: 2.4686066750879965

Epoch: 5| Step: 3
Training loss: 2.091124183349523
Validation loss: 2.4663984309658473

Epoch: 5| Step: 4
Training loss: 2.6052596481579466
Validation loss: 2.4759433666376247

Epoch: 5| Step: 5
Training loss: 2.8721661076658527
Validation loss: 2.479562031985553

Epoch: 5| Step: 6
Training loss: 2.72534948435226
Validation loss: 2.481452266318566

Epoch: 5| Step: 7
Training loss: 2.341765720901507
Validation loss: 2.4757933440093103

Epoch: 5| Step: 8
Training loss: 1.9548381158873929
Validation loss: 2.470650126777128

Epoch: 5| Step: 9
Training loss: 2.7727632804499596
Validation loss: 2.461105393429877

Epoch: 5| Step: 10
Training loss: 2.4700291856593584
Validation loss: 2.467538286766866

Epoch: 5| Step: 11
Training loss: 3.4666646926825355
Validation loss: 2.4760701465266033

Epoch: 165| Step: 0
Training loss: 2.4211016004594508
Validation loss: 2.473327312637809

Epoch: 5| Step: 1
Training loss: 2.896991050066437
Validation loss: 2.471621939352387

Epoch: 5| Step: 2
Training loss: 3.128357266426162
Validation loss: 2.4719706102504735

Epoch: 5| Step: 3
Training loss: 2.662603879522917
Validation loss: 2.469746408700802

Epoch: 5| Step: 4
Training loss: 2.786604110374587
Validation loss: 2.4828223528945577

Epoch: 5| Step: 5
Training loss: 2.3480924940461145
Validation loss: 2.487823651854251

Epoch: 5| Step: 6
Training loss: 2.4761203888696084
Validation loss: 2.4876071210380717

Epoch: 5| Step: 7
Training loss: 2.599097931705104
Validation loss: 2.496246050674098

Epoch: 5| Step: 8
Training loss: 2.2111754811670656
Validation loss: 2.4983885618136217

Epoch: 5| Step: 9
Training loss: 2.7083172626507626
Validation loss: 2.4981260150445754

Epoch: 5| Step: 10
Training loss: 2.305805103426522
Validation loss: 2.4992993127864387

Epoch: 5| Step: 11
Training loss: 1.1764187526152505
Validation loss: 2.4964688556021435

Epoch: 166| Step: 0
Training loss: 2.189673842263259
Validation loss: 2.487551272165781

Epoch: 5| Step: 1
Training loss: 2.7474304679145916
Validation loss: 2.483656105457894

Epoch: 5| Step: 2
Training loss: 2.604326492491329
Validation loss: 2.4865874309823295

Epoch: 5| Step: 3
Training loss: 2.903525110765818
Validation loss: 2.473497033863668

Epoch: 5| Step: 4
Training loss: 2.262243443901451
Validation loss: 2.4654795637215328

Epoch: 5| Step: 5
Training loss: 2.657680978722214
Validation loss: 2.470982753123301

Epoch: 5| Step: 6
Training loss: 2.5090331912469894
Validation loss: 2.4610051786612894

Epoch: 5| Step: 7
Training loss: 2.968842836233485
Validation loss: 2.4628951183321464

Epoch: 5| Step: 8
Training loss: 2.0164067134982275
Validation loss: 2.4601162665437126

Epoch: 5| Step: 9
Training loss: 2.8174862212933243
Validation loss: 2.46967291984259

Epoch: 5| Step: 10
Training loss: 2.54594366913574
Validation loss: 2.4619228307902516

Epoch: 5| Step: 11
Training loss: 2.6077656608566957
Validation loss: 2.457397001099461

Epoch: 167| Step: 0
Training loss: 2.3736763327337966
Validation loss: 2.465343408576601

Epoch: 5| Step: 1
Training loss: 2.750261641106798
Validation loss: 2.460498302364041

Epoch: 5| Step: 2
Training loss: 2.2979825003200838
Validation loss: 2.46479237901423

Epoch: 5| Step: 3
Training loss: 2.5314585281944204
Validation loss: 2.459338864808684

Epoch: 5| Step: 4
Training loss: 2.7782413625645543
Validation loss: 2.4547667911640447

Epoch: 5| Step: 5
Training loss: 2.150376655055099
Validation loss: 2.4576846710934603

Epoch: 5| Step: 6
Training loss: 2.296170081096085
Validation loss: 2.4567230286003228

Epoch: 5| Step: 7
Training loss: 2.6050702981377962
Validation loss: 2.463504687482688

Epoch: 5| Step: 8
Training loss: 3.0876271704302485
Validation loss: 2.4606320509495867

Epoch: 5| Step: 9
Training loss: 1.9474693505516052
Validation loss: 2.468648880283939

Epoch: 5| Step: 10
Training loss: 2.534367372475469
Validation loss: 2.4657865757412614

Epoch: 5| Step: 11
Training loss: 3.459735325739517
Validation loss: 2.467837649980433

Epoch: 168| Step: 0
Training loss: 2.950843212072008
Validation loss: 2.4674982647888104

Epoch: 5| Step: 1
Training loss: 2.243745375604566
Validation loss: 2.4744805912427763

Epoch: 5| Step: 2
Training loss: 2.332768258290396
Validation loss: 2.470054639869591

Epoch: 5| Step: 3
Training loss: 2.0019578887138443
Validation loss: 2.4704080196848444

Epoch: 5| Step: 4
Training loss: 2.203649904744273
Validation loss: 2.4718672069454044

Epoch: 5| Step: 5
Training loss: 2.600303643175667
Validation loss: 2.4784876163263845

Epoch: 5| Step: 6
Training loss: 2.572681853870418
Validation loss: 2.466125126999826

Epoch: 5| Step: 7
Training loss: 2.75111487937687
Validation loss: 2.470368347807438

Epoch: 5| Step: 8
Training loss: 2.6032234823549962
Validation loss: 2.4727752801530234

Epoch: 5| Step: 9
Training loss: 2.566641368576154
Validation loss: 2.472098011802904

Epoch: 5| Step: 10
Training loss: 2.714406896338344
Validation loss: 2.476762829689688

Epoch: 5| Step: 11
Training loss: 2.176571604866433
Validation loss: 2.467946750808082

Epoch: 169| Step: 0
Training loss: 2.6067669106580023
Validation loss: 2.4748113480028398

Epoch: 5| Step: 1
Training loss: 2.375698990606707
Validation loss: 2.4756674066564903

Epoch: 5| Step: 2
Training loss: 1.561302489107699
Validation loss: 2.4645999721396463

Epoch: 5| Step: 3
Training loss: 2.663843140282756
Validation loss: 2.467594991174115

Epoch: 5| Step: 4
Training loss: 2.791888176020369
Validation loss: 2.459429253493166

Epoch: 5| Step: 5
Training loss: 2.8737503321225577
Validation loss: 2.473753594743977

Epoch: 5| Step: 6
Training loss: 2.522211302464281
Validation loss: 2.4686961993057084

Epoch: 5| Step: 7
Training loss: 2.894935322304232
Validation loss: 2.4630562799432902

Epoch: 5| Step: 8
Training loss: 2.2430652136606226
Validation loss: 2.4658139591953527

Epoch: 5| Step: 9
Training loss: 2.27027458059486
Validation loss: 2.4668465249649354

Epoch: 5| Step: 10
Training loss: 2.401114594406354
Validation loss: 2.4691441981756412

Epoch: 5| Step: 11
Training loss: 2.5156552330711905
Validation loss: 2.466162315294777

Epoch: 170| Step: 0
Training loss: 2.686077806182367
Validation loss: 2.4785448277533906

Epoch: 5| Step: 1
Training loss: 2.276639904834891
Validation loss: 2.4748296561848266

Epoch: 5| Step: 2
Training loss: 2.2210586786604116
Validation loss: 2.4882035814716628

Epoch: 5| Step: 3
Training loss: 1.8253542125228535
Validation loss: 2.4844049136042328

Epoch: 5| Step: 4
Training loss: 3.002253957254872
Validation loss: 2.4763502035995972

Epoch: 5| Step: 5
Training loss: 2.5830718697751767
Validation loss: 2.4861026650705016

Epoch: 5| Step: 6
Training loss: 2.729310058631793
Validation loss: 2.4870016220170825

Epoch: 5| Step: 7
Training loss: 2.477434744798314
Validation loss: 2.4793779392672155

Epoch: 5| Step: 8
Training loss: 1.9865079815212379
Validation loss: 2.4748779488341786

Epoch: 5| Step: 9
Training loss: 2.6646864711275886
Validation loss: 2.4818407204318897

Epoch: 5| Step: 10
Training loss: 2.953164075158976
Validation loss: 2.4806909136265918

Epoch: 5| Step: 11
Training loss: 2.45307397485623
Validation loss: 2.4737305158156526

Epoch: 171| Step: 0
Training loss: 2.380068840986683
Validation loss: 2.4691034076132596

Epoch: 5| Step: 1
Training loss: 1.8591874613069894
Validation loss: 2.479608501737571

Epoch: 5| Step: 2
Training loss: 2.686022862647403
Validation loss: 2.471810019804879

Epoch: 5| Step: 3
Training loss: 2.491208353493613
Validation loss: 2.4769367933975692

Epoch: 5| Step: 4
Training loss: 2.4875486244548264
Validation loss: 2.4691487565701546

Epoch: 5| Step: 5
Training loss: 2.966100052147288
Validation loss: 2.475347732429977

Epoch: 5| Step: 6
Training loss: 2.267634480619686
Validation loss: 2.464837470754453

Epoch: 5| Step: 7
Training loss: 2.7326217698225195
Validation loss: 2.470178388075351

Epoch: 5| Step: 8
Training loss: 2.192932032925666
Validation loss: 2.4606596815129307

Epoch: 5| Step: 9
Training loss: 3.00004307398073
Validation loss: 2.4628625596393805

Epoch: 5| Step: 10
Training loss: 2.1794185130635713
Validation loss: 2.4629234132290874

Epoch: 5| Step: 11
Training loss: 2.089989772935878
Validation loss: 2.469375611561582

Epoch: 172| Step: 0
Training loss: 2.1573747659098816
Validation loss: 2.47035113856777

Epoch: 5| Step: 1
Training loss: 2.2539028650321202
Validation loss: 2.472961886993299

Epoch: 5| Step: 2
Training loss: 2.46992503342011
Validation loss: 2.4619855556374204

Epoch: 5| Step: 3
Training loss: 2.5341087493081886
Validation loss: 2.471533574196985

Epoch: 5| Step: 4
Training loss: 2.611310777627963
Validation loss: 2.473544169776807

Epoch: 5| Step: 5
Training loss: 2.5463312422797935
Validation loss: 2.4743698152695117

Epoch: 5| Step: 6
Training loss: 3.089073577073747
Validation loss: 2.466795747403823

Epoch: 5| Step: 7
Training loss: 2.624979654869074
Validation loss: 2.4664679413939217

Epoch: 5| Step: 8
Training loss: 2.4834945366102184
Validation loss: 2.4665538177794954

Epoch: 5| Step: 9
Training loss: 2.3180900230864756
Validation loss: 2.4627212558948686

Epoch: 5| Step: 10
Training loss: 2.3753400860281024
Validation loss: 2.468226361170828

Epoch: 5| Step: 11
Training loss: 1.7328198308075884
Validation loss: 2.4699706347517885

Epoch: 173| Step: 0
Training loss: 2.7466908398586076
Validation loss: 2.4738167946465315

Epoch: 5| Step: 1
Training loss: 2.395551405088989
Validation loss: 2.478456208398908

Epoch: 5| Step: 2
Training loss: 2.3916509085035282
Validation loss: 2.4732211262728185

Epoch: 5| Step: 3
Training loss: 2.7130722743080207
Validation loss: 2.472783957704373

Epoch: 5| Step: 4
Training loss: 2.690843675612627
Validation loss: 2.4723095281653342

Epoch: 5| Step: 5
Training loss: 2.6774933398366065
Validation loss: 2.4679173964836885

Epoch: 5| Step: 6
Training loss: 2.1099331717733065
Validation loss: 2.4707589968670285

Epoch: 5| Step: 7
Training loss: 2.489424079259885
Validation loss: 2.468700014083161

Epoch: 5| Step: 8
Training loss: 2.193086629267175
Validation loss: 2.468738535761873

Epoch: 5| Step: 9
Training loss: 2.1816661973238274
Validation loss: 2.4736800482319072

Epoch: 5| Step: 10
Training loss: 2.77906902395255
Validation loss: 2.475164423965331

Epoch: 5| Step: 11
Training loss: 1.3151291445132098
Validation loss: 2.475418255686725

Epoch: 174| Step: 0
Training loss: 2.6458714299700743
Validation loss: 2.479291777833177

Epoch: 5| Step: 1
Training loss: 2.4350254507125126
Validation loss: 2.4779107347935585

Epoch: 5| Step: 2
Training loss: 2.0209637104987395
Validation loss: 2.466360674277808

Epoch: 5| Step: 3
Training loss: 2.760177333221117
Validation loss: 2.478600298409901

Epoch: 5| Step: 4
Training loss: 2.429534612917414
Validation loss: 2.4731564610938204

Epoch: 5| Step: 5
Training loss: 2.4543148969650077
Validation loss: 2.477296545949779

Epoch: 5| Step: 6
Training loss: 2.5258686648769038
Validation loss: 2.4722568978926946

Epoch: 5| Step: 7
Training loss: 2.2181763243145283
Validation loss: 2.471051097384782

Epoch: 5| Step: 8
Training loss: 2.2935323461227184
Validation loss: 2.476944730445952

Epoch: 5| Step: 9
Training loss: 2.693534109576243
Validation loss: 2.4719874887314424

Epoch: 5| Step: 10
Training loss: 2.7694304411583093
Validation loss: 2.4680345077907573

Epoch: 5| Step: 11
Training loss: 2.0660135801077635
Validation loss: 2.468188326556048

Epoch: 175| Step: 0
Training loss: 2.9702778950159874
Validation loss: 2.4800692459342453

Epoch: 5| Step: 1
Training loss: 2.284269972864294
Validation loss: 2.477681702309508

Epoch: 5| Step: 2
Training loss: 2.4244961244595817
Validation loss: 2.4733189864290908

Epoch: 5| Step: 3
Training loss: 2.6455859471607983
Validation loss: 2.468174517229715

Epoch: 5| Step: 4
Training loss: 2.2383074534215535
Validation loss: 2.467058109033589

Epoch: 5| Step: 5
Training loss: 2.4402414213408252
Validation loss: 2.4750404333735756

Epoch: 5| Step: 6
Training loss: 2.5493416104534816
Validation loss: 2.47626400501515

Epoch: 5| Step: 7
Training loss: 2.4546240824490653
Validation loss: 2.4839713890382917

Epoch: 5| Step: 8
Training loss: 2.3932087219051295
Validation loss: 2.46881599981878

Epoch: 5| Step: 9
Training loss: 2.485577274408189
Validation loss: 2.477350797455707

Epoch: 5| Step: 10
Training loss: 2.367690835449795
Validation loss: 2.4849451438495658

Epoch: 5| Step: 11
Training loss: 1.9059755096423256
Validation loss: 2.468873097372765

Epoch: 176| Step: 0
Training loss: 2.543970616776742
Validation loss: 2.471839436413082

Epoch: 5| Step: 1
Training loss: 2.028024548128069
Validation loss: 2.469586874361451

Epoch: 5| Step: 2
Training loss: 3.294254798498319
Validation loss: 2.4725300664780416

Epoch: 5| Step: 3
Training loss: 2.181793070959316
Validation loss: 2.4775007317885502

Epoch: 5| Step: 4
Training loss: 2.432589352761267
Validation loss: 2.4845892735811734

Epoch: 5| Step: 5
Training loss: 2.8946567770731755
Validation loss: 2.476500770128442

Epoch: 5| Step: 6
Training loss: 2.4782309703128997
Validation loss: 2.479075255164615

Epoch: 5| Step: 7
Training loss: 2.628837867969757
Validation loss: 2.4757592455952517

Epoch: 5| Step: 8
Training loss: 2.1949441947012645
Validation loss: 2.490543919691004

Epoch: 5| Step: 9
Training loss: 2.29142394658931
Validation loss: 2.481125071444967

Epoch: 5| Step: 10
Training loss: 2.4757072825289925
Validation loss: 2.4829309213195723

Epoch: 5| Step: 11
Training loss: 0.9959991709368675
Validation loss: 2.4774027300594317

Epoch: 177| Step: 0
Training loss: 2.7823393327882857
Validation loss: 2.4755537463621398

Epoch: 5| Step: 1
Training loss: 2.3226382740196367
Validation loss: 2.475879783671844

Epoch: 5| Step: 2
Training loss: 2.031066886277555
Validation loss: 2.4788028319168287

Epoch: 5| Step: 3
Training loss: 2.185551783988287
Validation loss: 2.4739993015042643

Epoch: 5| Step: 4
Training loss: 2.676421730628767
Validation loss: 2.4736896864272264

Epoch: 5| Step: 5
Training loss: 2.543635642740528
Validation loss: 2.473698103753753

Epoch: 5| Step: 6
Training loss: 2.5270412918539464
Validation loss: 2.4756916733331376

Epoch: 5| Step: 7
Training loss: 2.579135950878277
Validation loss: 2.484407992513779

Epoch: 5| Step: 8
Training loss: 2.497045869674073
Validation loss: 2.4859063409290743

Epoch: 5| Step: 9
Training loss: 2.7414694169192932
Validation loss: 2.474442462064121

Epoch: 5| Step: 10
Training loss: 2.7014721353927054
Validation loss: 2.4666527720867637

Epoch: 5| Step: 11
Training loss: 2.4328756237232088
Validation loss: 2.4716022408483744

Epoch: 178| Step: 0
Training loss: 2.68005733329026
Validation loss: 2.4734667051948276

Epoch: 5| Step: 1
Training loss: 1.533869934864524
Validation loss: 2.4748250199445754

Epoch: 5| Step: 2
Training loss: 2.5619191697745847
Validation loss: 2.477204320876229

Epoch: 5| Step: 3
Training loss: 2.6248711145640105
Validation loss: 2.4712193890806105

Epoch: 5| Step: 4
Training loss: 2.340141570381478
Validation loss: 2.478415372688249

Epoch: 5| Step: 5
Training loss: 2.29275828862712
Validation loss: 2.467457823452483

Epoch: 5| Step: 6
Training loss: 2.4161271337065253
Validation loss: 2.472273340452767

Epoch: 5| Step: 7
Training loss: 2.6162766423078607
Validation loss: 2.4674018848859416

Epoch: 5| Step: 8
Training loss: 2.41631303589559
Validation loss: 2.474980899226533

Epoch: 5| Step: 9
Training loss: 2.7515820374193245
Validation loss: 2.4669459145153843

Epoch: 5| Step: 10
Training loss: 2.4021355267357736
Validation loss: 2.4654868244733765

Epoch: 5| Step: 11
Training loss: 4.068741918224741
Validation loss: 2.472075548327445

Epoch: 179| Step: 0
Training loss: 2.374332936620494
Validation loss: 2.460840556339403

Epoch: 5| Step: 1
Training loss: 2.673020948088907
Validation loss: 2.4658773223027546

Epoch: 5| Step: 2
Training loss: 2.7987650020958865
Validation loss: 2.476002530591223

Epoch: 5| Step: 3
Training loss: 2.62426547718381
Validation loss: 2.4708598171399423

Epoch: 5| Step: 4
Training loss: 2.5786277511991043
Validation loss: 2.4781875493420484

Epoch: 5| Step: 5
Training loss: 2.3945684881255183
Validation loss: 2.468955260809673

Epoch: 5| Step: 6
Training loss: 2.469460204133402
Validation loss: 2.4753745124817828

Epoch: 5| Step: 7
Training loss: 2.6281000632827802
Validation loss: 2.4733846635089094

Epoch: 5| Step: 8
Training loss: 2.659904300000726
Validation loss: 2.474047863327404

Epoch: 5| Step: 9
Training loss: 1.7904786081706
Validation loss: 2.4811097926219703

Epoch: 5| Step: 10
Training loss: 2.461963742340129
Validation loss: 2.473232738431937

Epoch: 5| Step: 11
Training loss: 1.498919097549476
Validation loss: 2.465131663229477

Epoch: 180| Step: 0
Training loss: 1.8803326115698404
Validation loss: 2.46192759018737

Epoch: 5| Step: 1
Training loss: 2.467100340721172
Validation loss: 2.4620951477863073

Epoch: 5| Step: 2
Training loss: 3.352725949677387
Validation loss: 2.464745569465231

Epoch: 5| Step: 3
Training loss: 2.4316346440375742
Validation loss: 2.4574338808475367

Epoch: 5| Step: 4
Training loss: 2.922132184954619
Validation loss: 2.4601676463362576

Epoch: 5| Step: 5
Training loss: 2.3550327587076536
Validation loss: 2.4634849643969776

Epoch: 5| Step: 6
Training loss: 2.443907800443104
Validation loss: 2.468710963427475

Epoch: 5| Step: 7
Training loss: 2.411765972202795
Validation loss: 2.4594267956434828

Epoch: 5| Step: 8
Training loss: 2.09681659281721
Validation loss: 2.469135081361368

Epoch: 5| Step: 9
Training loss: 2.5804443099515777
Validation loss: 2.4685386796636446

Epoch: 5| Step: 10
Training loss: 2.1940735276801786
Validation loss: 2.4750546780079072

Epoch: 5| Step: 11
Training loss: 1.7131481085871023
Validation loss: 2.4770713548127357

Epoch: 181| Step: 0
Training loss: 2.4296480525209105
Validation loss: 2.4736744299498934

Epoch: 5| Step: 1
Training loss: 2.36008892247408
Validation loss: 2.4755409572710887

Epoch: 5| Step: 2
Training loss: 2.055041380602534
Validation loss: 2.4706336171220036

Epoch: 5| Step: 3
Training loss: 2.601711051296761
Validation loss: 2.470501704794903

Epoch: 5| Step: 4
Training loss: 2.5774096797062467
Validation loss: 2.4658603979698652

Epoch: 5| Step: 5
Training loss: 2.278234082031901
Validation loss: 2.4806007650493367

Epoch: 5| Step: 6
Training loss: 2.809714484998989
Validation loss: 2.4774289585979554

Epoch: 5| Step: 7
Training loss: 2.3349116187349503
Validation loss: 2.4745521287887753

Epoch: 5| Step: 8
Training loss: 2.8422213627337816
Validation loss: 2.470900166611718

Epoch: 5| Step: 9
Training loss: 2.1756404372283744
Validation loss: 2.4649516312275135

Epoch: 5| Step: 10
Training loss: 2.4823619436117283
Validation loss: 2.472500053401904

Epoch: 5| Step: 11
Training loss: 3.35423901086983
Validation loss: 2.4749711637069387

Epoch: 182| Step: 0
Training loss: 2.133307095704062
Validation loss: 2.4739071743678176

Epoch: 5| Step: 1
Training loss: 2.1558704871520105
Validation loss: 2.4739743456405985

Epoch: 5| Step: 2
Training loss: 2.8304309190690504
Validation loss: 2.4849315555723077

Epoch: 5| Step: 3
Training loss: 2.6148499939236776
Validation loss: 2.4956185053126183

Epoch: 5| Step: 4
Training loss: 2.0820385787863507
Validation loss: 2.479049709243313

Epoch: 5| Step: 5
Training loss: 2.953923516070439
Validation loss: 2.4663451670430527

Epoch: 5| Step: 6
Training loss: 2.4401311123125904
Validation loss: 2.471228952454882

Epoch: 5| Step: 7
Training loss: 2.433381832196885
Validation loss: 2.4747595055293505

Epoch: 5| Step: 8
Training loss: 2.5774869187001817
Validation loss: 2.4798170620346665

Epoch: 5| Step: 9
Training loss: 2.728705757960244
Validation loss: 2.474163646544115

Epoch: 5| Step: 10
Training loss: 2.324471083335033
Validation loss: 2.481322874861546

Epoch: 5| Step: 11
Training loss: 2.804382902728942
Validation loss: 2.474482166981924

Epoch: 183| Step: 0
Training loss: 2.4361558778371726
Validation loss: 2.4919226255538782

Epoch: 5| Step: 1
Training loss: 2.3953260686381834
Validation loss: 2.4869663910595894

Epoch: 5| Step: 2
Training loss: 2.0600027513948604
Validation loss: 2.486217211626512

Epoch: 5| Step: 3
Training loss: 2.079756361740586
Validation loss: 2.491021173383358

Epoch: 5| Step: 4
Training loss: 2.9622394044757274
Validation loss: 2.4866502647545037

Epoch: 5| Step: 5
Training loss: 2.214094193257968
Validation loss: 2.4792120005098464

Epoch: 5| Step: 6
Training loss: 2.7645795322402624
Validation loss: 2.479120664143041

Epoch: 5| Step: 7
Training loss: 2.107191750280152
Validation loss: 2.473677943887601

Epoch: 5| Step: 8
Training loss: 3.127712750779969
Validation loss: 2.4859456109938223

Epoch: 5| Step: 9
Training loss: 2.830359487717402
Validation loss: 2.4833462222509417

Epoch: 5| Step: 10
Training loss: 2.5198079275169265
Validation loss: 2.4896721617893864

Epoch: 5| Step: 11
Training loss: 2.320068449293149
Validation loss: 2.4793243892303547

Epoch: 184| Step: 0
Training loss: 2.6801183593291973
Validation loss: 2.4858172446943123

Epoch: 5| Step: 1
Training loss: 2.3779986675056595
Validation loss: 2.479433850041198

Epoch: 5| Step: 2
Training loss: 2.316976184431163
Validation loss: 2.4819858447212244

Epoch: 5| Step: 3
Training loss: 2.0074251150033446
Validation loss: 2.4776979605211893

Epoch: 5| Step: 4
Training loss: 2.431188971922937
Validation loss: 2.470905819344801

Epoch: 5| Step: 5
Training loss: 2.785119697149031
Validation loss: 2.4700166374294863

Epoch: 5| Step: 6
Training loss: 2.945401541985451
Validation loss: 2.4717421647496143

Epoch: 5| Step: 7
Training loss: 1.927249805030313
Validation loss: 2.4640560352447856

Epoch: 5| Step: 8
Training loss: 2.586126775815919
Validation loss: 2.463886774473873

Epoch: 5| Step: 9
Training loss: 2.3157669909312166
Validation loss: 2.4808437714716454

Epoch: 5| Step: 10
Training loss: 2.59279900087445
Validation loss: 2.4759638410723244

Epoch: 5| Step: 11
Training loss: 2.793419264091511
Validation loss: 2.4744496925015524

Epoch: 185| Step: 0
Training loss: 2.8944915480471365
Validation loss: 2.4758199105936245

Epoch: 5| Step: 1
Training loss: 1.8264954305821908
Validation loss: 2.46719522247491

Epoch: 5| Step: 2
Training loss: 2.1371278806989285
Validation loss: 2.473105941627046

Epoch: 5| Step: 3
Training loss: 2.4511091368651816
Validation loss: 2.468927908172837

Epoch: 5| Step: 4
Training loss: 2.709666281137413
Validation loss: 2.4768317203621293

Epoch: 5| Step: 5
Training loss: 3.135879955514552
Validation loss: 2.479323904409923

Epoch: 5| Step: 6
Training loss: 1.9281853145520087
Validation loss: 2.4778145514264955

Epoch: 5| Step: 7
Training loss: 2.560059199095747
Validation loss: 2.4859908065659155

Epoch: 5| Step: 8
Training loss: 2.535982206071588
Validation loss: 2.484497151280868

Epoch: 5| Step: 9
Training loss: 2.6209324703256986
Validation loss: 2.480280150605116

Epoch: 5| Step: 10
Training loss: 2.346151113809195
Validation loss: 2.4786958982110447

Epoch: 5| Step: 11
Training loss: 2.7747500470288
Validation loss: 2.4810180456352673

Epoch: 186| Step: 0
Training loss: 2.5634203862892373
Validation loss: 2.4779889504350785

Epoch: 5| Step: 1
Training loss: 2.574440745389137
Validation loss: 2.4803366277955896

Epoch: 5| Step: 2
Training loss: 2.6835589233093144
Validation loss: 2.4772113467527164

Epoch: 5| Step: 3
Training loss: 2.7130963526937713
Validation loss: 2.479648664779724

Epoch: 5| Step: 4
Training loss: 2.381212040956576
Validation loss: 2.4776066685786167

Epoch: 5| Step: 5
Training loss: 2.2115158564710433
Validation loss: 2.4775952332930142

Epoch: 5| Step: 6
Training loss: 2.357627878614387
Validation loss: 2.4790685671676753

Epoch: 5| Step: 7
Training loss: 2.454888262876587
Validation loss: 2.4766754140695126

Epoch: 5| Step: 8
Training loss: 2.0420558195265515
Validation loss: 2.478533685376415

Epoch: 5| Step: 9
Training loss: 2.414658654930407
Validation loss: 2.488088224021523

Epoch: 5| Step: 10
Training loss: 2.762905011759164
Validation loss: 2.476678803417594

Epoch: 5| Step: 11
Training loss: 3.238943660339135
Validation loss: 2.48583668269197

Epoch: 187| Step: 0
Training loss: 2.943107933437258
Validation loss: 2.4783657220029403

Epoch: 5| Step: 1
Training loss: 2.587226938038554
Validation loss: 2.480906212144887

Epoch: 5| Step: 2
Training loss: 2.6590238561230777
Validation loss: 2.4834997566726837

Epoch: 5| Step: 3
Training loss: 1.910834006924794
Validation loss: 2.4834147702908953

Epoch: 5| Step: 4
Training loss: 2.7152601622530135
Validation loss: 2.4720342776893602

Epoch: 5| Step: 5
Training loss: 2.34722094062404
Validation loss: 2.4832488492594798

Epoch: 5| Step: 6
Training loss: 2.1851499740843243
Validation loss: 2.4809644068709864

Epoch: 5| Step: 7
Training loss: 2.899905591283015
Validation loss: 2.4866864469864756

Epoch: 5| Step: 8
Training loss: 2.0230280045717346
Validation loss: 2.483414386273533

Epoch: 5| Step: 9
Training loss: 2.1015272172993735
Validation loss: 2.487004577879712

Epoch: 5| Step: 10
Training loss: 2.5713879964291526
Validation loss: 2.485969985135859

Epoch: 5| Step: 11
Training loss: 3.341127739983932
Validation loss: 2.4881096405945287

Epoch: 188| Step: 0
Training loss: 2.7541463977185234
Validation loss: 2.4830319795132834

Epoch: 5| Step: 1
Training loss: 1.5516477993570292
Validation loss: 2.503883817971967

Epoch: 5| Step: 2
Training loss: 2.4406541810385383
Validation loss: 2.492714936698927

Epoch: 5| Step: 3
Training loss: 3.030916018619828
Validation loss: 2.4800516213343595

Epoch: 5| Step: 4
Training loss: 1.970116156430966
Validation loss: 2.4916783872113943

Epoch: 5| Step: 5
Training loss: 2.523990157227587
Validation loss: 2.4824520762537503

Epoch: 5| Step: 6
Training loss: 2.90650889566354
Validation loss: 2.486666795909135

Epoch: 5| Step: 7
Training loss: 2.605877754102009
Validation loss: 2.4838447604751512

Epoch: 5| Step: 8
Training loss: 2.5417446583106136
Validation loss: 2.4813088143438233

Epoch: 5| Step: 9
Training loss: 2.394992802555363
Validation loss: 2.4907501564185366

Epoch: 5| Step: 10
Training loss: 2.3382233627500404
Validation loss: 2.487623937353147

Epoch: 5| Step: 11
Training loss: 1.906726902574883
Validation loss: 2.481512367796747

Epoch: 189| Step: 0
Training loss: 2.1759914111781313
Validation loss: 2.4747569485011995

Epoch: 5| Step: 1
Training loss: 2.415540915305138
Validation loss: 2.469154179962373

Epoch: 5| Step: 2
Training loss: 3.1710955784978716
Validation loss: 2.4718870178747143

Epoch: 5| Step: 3
Training loss: 2.374461363898398
Validation loss: 2.472486681997124

Epoch: 5| Step: 4
Training loss: 2.7669522823607435
Validation loss: 2.461797166465403

Epoch: 5| Step: 5
Training loss: 2.708038186839435
Validation loss: 2.471129240701152

Epoch: 5| Step: 6
Training loss: 2.4988452151669565
Validation loss: 2.474885290390399

Epoch: 5| Step: 7
Training loss: 2.623963923572634
Validation loss: 2.4737508599805245

Epoch: 5| Step: 8
Training loss: 2.1222501802294964
Validation loss: 2.4803573303029034

Epoch: 5| Step: 9
Training loss: 2.2314845242383563
Validation loss: 2.471229629808907

Epoch: 5| Step: 10
Training loss: 2.1012862353630037
Validation loss: 2.478051825842476

Epoch: 5| Step: 11
Training loss: 0.8805192853341702
Validation loss: 2.4739433783363607

Epoch: 190| Step: 0
Training loss: 2.2645528426891572
Validation loss: 2.4726234863746135

Epoch: 5| Step: 1
Training loss: 2.86039523908272
Validation loss: 2.4799587496408373

Epoch: 5| Step: 2
Training loss: 2.074090583706547
Validation loss: 2.473213783793327

Epoch: 5| Step: 3
Training loss: 2.421983482638422
Validation loss: 2.4858854488579616

Epoch: 5| Step: 4
Training loss: 2.2718523404802964
Validation loss: 2.4822420244673093

Epoch: 5| Step: 5
Training loss: 2.8819931230429905
Validation loss: 2.487279493695818

Epoch: 5| Step: 6
Training loss: 2.4029262425806404
Validation loss: 2.479211351381971

Epoch: 5| Step: 7
Training loss: 2.243323168096236
Validation loss: 2.478258769436421

Epoch: 5| Step: 8
Training loss: 2.878042021475776
Validation loss: 2.4790987491654315

Epoch: 5| Step: 9
Training loss: 2.081388735800942
Validation loss: 2.4758591560712833

Epoch: 5| Step: 10
Training loss: 2.5694621340397603
Validation loss: 2.479086595465705

Epoch: 5| Step: 11
Training loss: 2.3075493065225
Validation loss: 2.4770902799332997

Epoch: 191| Step: 0
Training loss: 2.578024659949619
Validation loss: 2.476639518783674

Epoch: 5| Step: 1
Training loss: 2.237972644348954
Validation loss: 2.4725299901400235

Epoch: 5| Step: 2
Training loss: 2.2041279559230547
Validation loss: 2.4809949481951885

Epoch: 5| Step: 3
Training loss: 2.416614751148546
Validation loss: 2.4764156719208805

Epoch: 5| Step: 4
Training loss: 2.237213572491283
Validation loss: 2.4771091527745543

Epoch: 5| Step: 5
Training loss: 2.8527924693801423
Validation loss: 2.485823718715348

Epoch: 5| Step: 6
Training loss: 2.199341250543065
Validation loss: 2.4848327414960356

Epoch: 5| Step: 7
Training loss: 3.229745841641277
Validation loss: 2.4766167956636886

Epoch: 5| Step: 8
Training loss: 2.3801983611206126
Validation loss: 2.4878615899272667

Epoch: 5| Step: 9
Training loss: 2.727368691952381
Validation loss: 2.487896584601344

Epoch: 5| Step: 10
Training loss: 2.0086960091779935
Validation loss: 2.4800504997646735

Epoch: 5| Step: 11
Training loss: 1.0679566009433217
Validation loss: 2.476477351765615

Epoch: 192| Step: 0
Training loss: 2.2664084296756797
Validation loss: 2.481769813278538

Epoch: 5| Step: 1
Training loss: 2.7237045043394903
Validation loss: 2.483444855466582

Epoch: 5| Step: 2
Training loss: 2.1557689627744936
Validation loss: 2.4846334502902696

Epoch: 5| Step: 3
Training loss: 2.2033942308344665
Validation loss: 2.479680125708893

Epoch: 5| Step: 4
Training loss: 2.537489935192574
Validation loss: 2.48054171477407

Epoch: 5| Step: 5
Training loss: 2.6647056183680506
Validation loss: 2.4782655658437673

Epoch: 5| Step: 6
Training loss: 2.7283411209128907
Validation loss: 2.4749911845978207

Epoch: 5| Step: 7
Training loss: 2.2805946140246114
Validation loss: 2.473349222522485

Epoch: 5| Step: 8
Training loss: 2.557995628740932
Validation loss: 2.476242132955883

Epoch: 5| Step: 9
Training loss: 2.5855891520352876
Validation loss: 2.487134272879786

Epoch: 5| Step: 10
Training loss: 2.3854656519850663
Validation loss: 2.4926247925607545

Epoch: 5| Step: 11
Training loss: 2.5899154630142474
Validation loss: 2.4806470871942654

Epoch: 193| Step: 0
Training loss: 2.0522114153810787
Validation loss: 2.484740848114879

Epoch: 5| Step: 1
Training loss: 2.37728159883098
Validation loss: 2.481141174966314

Epoch: 5| Step: 2
Training loss: 2.062822605680999
Validation loss: 2.4891151256629467

Epoch: 5| Step: 3
Training loss: 2.7783272634194685
Validation loss: 2.4836615131705955

Epoch: 5| Step: 4
Training loss: 2.524934212611318
Validation loss: 2.489569294196943

Epoch: 5| Step: 5
Training loss: 2.845325746039321
Validation loss: 2.4874646110828835

Epoch: 5| Step: 6
Training loss: 2.451218854598346
Validation loss: 2.478407712911399

Epoch: 5| Step: 7
Training loss: 2.44990726996119
Validation loss: 2.476403617375305

Epoch: 5| Step: 8
Training loss: 2.323699021131632
Validation loss: 2.4893279576435936

Epoch: 5| Step: 9
Training loss: 2.5726400579318867
Validation loss: 2.4911284872338153

Epoch: 5| Step: 10
Training loss: 2.392258229571106
Validation loss: 2.483347510344299

Epoch: 5| Step: 11
Training loss: 2.991907808862258
Validation loss: 2.480367939820357

Epoch: 194| Step: 0
Training loss: 2.6863007642121692
Validation loss: 2.486754934910736

Epoch: 5| Step: 1
Training loss: 2.409038859667165
Validation loss: 2.488981861948318

Epoch: 5| Step: 2
Training loss: 1.8510521293978435
Validation loss: 2.484632902534273

Epoch: 5| Step: 3
Training loss: 2.653067533591837
Validation loss: 2.4901294282228794

Epoch: 5| Step: 4
Training loss: 2.3353579003060694
Validation loss: 2.48533873189376

Epoch: 5| Step: 5
Training loss: 1.8934704514447498
Validation loss: 2.4920381602921524

Epoch: 5| Step: 6
Training loss: 2.5810557035977473
Validation loss: 2.4880931709308776

Epoch: 5| Step: 7
Training loss: 2.7187740281018846
Validation loss: 2.493861569910039

Epoch: 5| Step: 8
Training loss: 2.616461263074272
Validation loss: 2.475913338765385

Epoch: 5| Step: 9
Training loss: 2.597093931442009
Validation loss: 2.4977822977651263

Epoch: 5| Step: 10
Training loss: 2.663976584025163
Validation loss: 2.4881673174953978

Epoch: 5| Step: 11
Training loss: 1.764235228063822
Validation loss: 2.47806666250621

Epoch: 195| Step: 0
Training loss: 2.273859673068058
Validation loss: 2.482094723893761

Epoch: 5| Step: 1
Training loss: 1.7568307805476007
Validation loss: 2.4871680595736416

Epoch: 5| Step: 2
Training loss: 2.4332134996806336
Validation loss: 2.485213991765722

Epoch: 5| Step: 3
Training loss: 2.745325710935727
Validation loss: 2.483383104659777

Epoch: 5| Step: 4
Training loss: 2.643810630445797
Validation loss: 2.4810385702841358

Epoch: 5| Step: 5
Training loss: 2.366883916241231
Validation loss: 2.4836579173629434

Epoch: 5| Step: 6
Training loss: 2.1328244610685094
Validation loss: 2.4825334421333842

Epoch: 5| Step: 7
Training loss: 2.673618840678438
Validation loss: 2.49060112938955

Epoch: 5| Step: 8
Training loss: 2.7990432944048287
Validation loss: 2.4807379228617723

Epoch: 5| Step: 9
Training loss: 2.232834077894992
Validation loss: 2.4852781513516504

Epoch: 5| Step: 10
Training loss: 2.768159900754474
Validation loss: 2.4884784127317783

Epoch: 5| Step: 11
Training loss: 2.7873972989309226
Validation loss: 2.5005192296926744

Epoch: 196| Step: 0
Training loss: 2.5290469719992092
Validation loss: 2.485916323344827

Epoch: 5| Step: 1
Training loss: 2.306904802690557
Validation loss: 2.479984474466805

Epoch: 5| Step: 2
Training loss: 2.5939461507509374
Validation loss: 2.492472916286633

Epoch: 5| Step: 3
Training loss: 2.427218639438754
Validation loss: 2.494895759009037

Epoch: 5| Step: 4
Training loss: 2.859845565681108
Validation loss: 2.4949127173207173

Epoch: 5| Step: 5
Training loss: 2.278916824938772
Validation loss: 2.484713817180872

Epoch: 5| Step: 6
Training loss: 2.6668158032516796
Validation loss: 2.4840961995624125

Epoch: 5| Step: 7
Training loss: 2.3873392100889896
Validation loss: 2.479012337590711

Epoch: 5| Step: 8
Training loss: 2.4158520037940736
Validation loss: 2.4810633349729025

Epoch: 5| Step: 9
Training loss: 2.2662894491390664
Validation loss: 2.479264198650048

Epoch: 5| Step: 10
Training loss: 2.287816043554531
Validation loss: 2.4794766061380153

Epoch: 5| Step: 11
Training loss: 2.2313192319413964
Validation loss: 2.4832796365151055

Epoch: 197| Step: 0
Training loss: 3.1138112994224048
Validation loss: 2.4751875316652394

Epoch: 5| Step: 1
Training loss: 2.449582792298158
Validation loss: 2.47727704094179

Epoch: 5| Step: 2
Training loss: 2.4680611337419696
Validation loss: 2.4905724788346744

Epoch: 5| Step: 3
Training loss: 2.2418038176186275
Validation loss: 2.4839962324673395

Epoch: 5| Step: 4
Training loss: 2.1043335750842953
Validation loss: 2.4901284189059867

Epoch: 5| Step: 5
Training loss: 2.215189265997805
Validation loss: 2.4873226880531885

Epoch: 5| Step: 6
Training loss: 2.4258905407646663
Validation loss: 2.4795259701527192

Epoch: 5| Step: 7
Training loss: 2.533217619043443
Validation loss: 2.490270580951054

Epoch: 5| Step: 8
Training loss: 2.5445161907440275
Validation loss: 2.49308611574513

Epoch: 5| Step: 9
Training loss: 2.145209866907523
Validation loss: 2.49009738121446

Epoch: 5| Step: 10
Training loss: 2.5318107454823595
Validation loss: 2.48188128964426

Epoch: 5| Step: 11
Training loss: 3.2740504162168436
Validation loss: 2.486388711922804

Epoch: 198| Step: 0
Training loss: 2.9555353893780754
Validation loss: 2.4900007110873004

Epoch: 5| Step: 1
Training loss: 2.595457870573584
Validation loss: 2.4877999447570964

Epoch: 5| Step: 2
Training loss: 2.2407703772804366
Validation loss: 2.490443202219832

Epoch: 5| Step: 3
Training loss: 2.7685887894725125
Validation loss: 2.488958736634748

Epoch: 5| Step: 4
Training loss: 2.6663802907078527
Validation loss: 2.4821696019438884

Epoch: 5| Step: 5
Training loss: 1.7069956341482602
Validation loss: 2.4930210931698893

Epoch: 5| Step: 6
Training loss: 2.4653814004483188
Validation loss: 2.4803375289530165

Epoch: 5| Step: 7
Training loss: 2.336652053192342
Validation loss: 2.48659085875576

Epoch: 5| Step: 8
Training loss: 2.404383244888294
Validation loss: 2.4886365204707195

Epoch: 5| Step: 9
Training loss: 2.307366007575265
Validation loss: 2.483932851637057

Epoch: 5| Step: 10
Training loss: 2.5865407741362847
Validation loss: 2.4868402904994125

Epoch: 5| Step: 11
Training loss: 1.0158791444153406
Validation loss: 2.480764812913099

Epoch: 199| Step: 0
Training loss: 2.2744004350305573
Validation loss: 2.4761673404467337

Epoch: 5| Step: 1
Training loss: 2.426800549013784
Validation loss: 2.484335821070727

Epoch: 5| Step: 2
Training loss: 2.505100959068219
Validation loss: 2.480297585336346

Epoch: 5| Step: 3
Training loss: 2.8414475059083895
Validation loss: 2.474412793362832

Epoch: 5| Step: 4
Training loss: 1.6384133099697062
Validation loss: 2.4802166547853295

Epoch: 5| Step: 5
Training loss: 2.5097729397084554
Validation loss: 2.4747187894594993

Epoch: 5| Step: 6
Training loss: 2.369236326997128
Validation loss: 2.4724079728590245

Epoch: 5| Step: 7
Training loss: 2.5323110169885283
Validation loss: 2.4842515031041548

Epoch: 5| Step: 8
Training loss: 2.437932196227389
Validation loss: 2.4816434426584153

Epoch: 5| Step: 9
Training loss: 2.5366770645443135
Validation loss: 2.481892394977785

Epoch: 5| Step: 10
Training loss: 2.800830537370328
Validation loss: 2.4925408626039487

Epoch: 5| Step: 11
Training loss: 2.1613183646478964
Validation loss: 2.4821676408713746

Epoch: 200| Step: 0
Training loss: 2.9275495128532874
Validation loss: 2.4963619543779556

Epoch: 5| Step: 1
Training loss: 2.6629777864682134
Validation loss: 2.4997087149998074

Epoch: 5| Step: 2
Training loss: 2.6007465537848793
Validation loss: 2.49643492021214

Epoch: 5| Step: 3
Training loss: 2.387830909045972
Validation loss: 2.4899788360130346

Epoch: 5| Step: 4
Training loss: 2.255168382908961
Validation loss: 2.505282983176595

Epoch: 5| Step: 5
Training loss: 2.3858490153694563
Validation loss: 2.4963395261875756

Epoch: 5| Step: 6
Training loss: 2.2950216068754976
Validation loss: 2.492511524912247

Epoch: 5| Step: 7
Training loss: 2.679100661900911
Validation loss: 2.4961669664385515

Epoch: 5| Step: 8
Training loss: 2.373306172336934
Validation loss: 2.4926610275564234

Epoch: 5| Step: 9
Training loss: 1.697398998081581
Validation loss: 2.491901919466136

Epoch: 5| Step: 10
Training loss: 2.7261336462071006
Validation loss: 2.488392793958898

Epoch: 5| Step: 11
Training loss: 1.76385686333019
Validation loss: 2.487303247689753

Epoch: 201| Step: 0
Training loss: 2.507770858846748
Validation loss: 2.490954342154526

Epoch: 5| Step: 1
Training loss: 2.5720630006920144
Validation loss: 2.493198428905175

Epoch: 5| Step: 2
Training loss: 2.294311130500307
Validation loss: 2.4880909310481996

Epoch: 5| Step: 3
Training loss: 2.4906509110887023
Validation loss: 2.49241739167425

Epoch: 5| Step: 4
Training loss: 2.7241754876856814
Validation loss: 2.4811166072448994

Epoch: 5| Step: 5
Training loss: 2.079219099873099
Validation loss: 2.486992051389115

Epoch: 5| Step: 6
Training loss: 2.644404849211021
Validation loss: 2.4844809975394155

Epoch: 5| Step: 7
Training loss: 2.17820519801833
Validation loss: 2.4785080416868737

Epoch: 5| Step: 8
Training loss: 2.366963492391973
Validation loss: 2.481851831950996

Epoch: 5| Step: 9
Training loss: 2.542679777588004
Validation loss: 2.492923707554131

Epoch: 5| Step: 10
Training loss: 2.5751788327236254
Validation loss: 2.4738732547519664

Epoch: 5| Step: 11
Training loss: 2.8303770087818374
Validation loss: 2.484327487771139

Epoch: 202| Step: 0
Training loss: 2.1348869100810584
Validation loss: 2.5039332919512836

Epoch: 5| Step: 1
Training loss: 3.037769660439661
Validation loss: 2.492052594781767

Epoch: 5| Step: 2
Training loss: 2.390577328275197
Validation loss: 2.485165879983321

Epoch: 5| Step: 3
Training loss: 2.311596152512015
Validation loss: 2.492428497891396

Epoch: 5| Step: 4
Training loss: 2.396325986768762
Validation loss: 2.4911985079261965

Epoch: 5| Step: 5
Training loss: 2.436188662966931
Validation loss: 2.5025989176483097

Epoch: 5| Step: 6
Training loss: 2.2442179667373137
Validation loss: 2.496224540742351

Epoch: 5| Step: 7
Training loss: 2.64687947494687
Validation loss: 2.497063567274658

Epoch: 5| Step: 8
Training loss: 2.528518427354146
Validation loss: 2.484619660396346

Epoch: 5| Step: 9
Training loss: 2.285919635778518
Validation loss: 2.4756295405353224

Epoch: 5| Step: 10
Training loss: 2.6639718406666746
Validation loss: 2.4774192707921867

Epoch: 5| Step: 11
Training loss: 1.5759863383819288
Validation loss: 2.4800855645998308

Epoch: 203| Step: 0
Training loss: 2.0225125706379856
Validation loss: 2.4856505648179392

Epoch: 5| Step: 1
Training loss: 2.4726414009600544
Validation loss: 2.4828970729666975

Epoch: 5| Step: 2
Training loss: 2.915811704032272
Validation loss: 2.48943568766822

Epoch: 5| Step: 3
Training loss: 2.919834995572042
Validation loss: 2.491837667308821

Epoch: 5| Step: 4
Training loss: 2.8229672097799288
Validation loss: 2.4879461568850973

Epoch: 5| Step: 5
Training loss: 2.4027848497799953
Validation loss: 2.489025196387386

Epoch: 5| Step: 6
Training loss: 1.8920202230134835
Validation loss: 2.4978809634645933

Epoch: 5| Step: 7
Training loss: 2.417408818258154
Validation loss: 2.4859489797106917

Epoch: 5| Step: 8
Training loss: 2.1663628756462456
Validation loss: 2.4969323211761605

Epoch: 5| Step: 9
Training loss: 2.775811009847417
Validation loss: 2.493622792058084

Epoch: 5| Step: 10
Training loss: 2.1149784965414
Validation loss: 2.503529104155928

Epoch: 5| Step: 11
Training loss: 1.6932725015111358
Validation loss: 2.4895095787668526

Epoch: 204| Step: 0
Training loss: 2.2874506460400075
Validation loss: 2.4954408160782284

Epoch: 5| Step: 1
Training loss: 2.3537984464403157
Validation loss: 2.4827310573183152

Epoch: 5| Step: 2
Training loss: 2.215981704126842
Validation loss: 2.495538394061173

Epoch: 5| Step: 3
Training loss: 2.663670127813256
Validation loss: 2.5014799148489786

Epoch: 5| Step: 4
Training loss: 2.111297409582313
Validation loss: 2.493701554626944

Epoch: 5| Step: 5
Training loss: 2.683704623801304
Validation loss: 2.5060331225399524

Epoch: 5| Step: 6
Training loss: 2.064237845031278
Validation loss: 2.5080752566845574

Epoch: 5| Step: 7
Training loss: 2.666153431219766
Validation loss: 2.5126645970558266

Epoch: 5| Step: 8
Training loss: 2.273554009142737
Validation loss: 2.508020251770835

Epoch: 5| Step: 9
Training loss: 2.3342730355622976
Validation loss: 2.5132074527263866

Epoch: 5| Step: 10
Training loss: 2.925096500467692
Validation loss: 2.4991835254326555

Epoch: 5| Step: 11
Training loss: 3.3834305478365727
Validation loss: 2.4952090530753313

Epoch: 205| Step: 0
Training loss: 2.5209929255469876
Validation loss: 2.488506145284989

Epoch: 5| Step: 1
Training loss: 2.6967939203303
Validation loss: 2.4854157505102994

Epoch: 5| Step: 2
Training loss: 2.4212617159172516
Validation loss: 2.4759317832404766

Epoch: 5| Step: 3
Training loss: 3.131154522991573
Validation loss: 2.4837684010753724

Epoch: 5| Step: 4
Training loss: 1.9601038646425775
Validation loss: 2.4892663926856793

Epoch: 5| Step: 5
Training loss: 2.0726512438688127
Validation loss: 2.485237523668387

Epoch: 5| Step: 6
Training loss: 2.4740718989490653
Validation loss: 2.48380464528597

Epoch: 5| Step: 7
Training loss: 2.5998754214738518
Validation loss: 2.4867820335403024

Epoch: 5| Step: 8
Training loss: 2.078521575947892
Validation loss: 2.483349714502501

Epoch: 5| Step: 9
Training loss: 2.101517006761023
Validation loss: 2.488543494122615

Epoch: 5| Step: 10
Training loss: 2.701324833685535
Validation loss: 2.491932755288846

Epoch: 5| Step: 11
Training loss: 1.989488516207577
Validation loss: 2.4976026782846388

Epoch: 206| Step: 0
Training loss: 2.4980824746107313
Validation loss: 2.4957688447987914

Epoch: 5| Step: 1
Training loss: 2.4250590405699906
Validation loss: 2.4979804623951756

Epoch: 5| Step: 2
Training loss: 2.3005191673138623
Validation loss: 2.4965156234483072

Epoch: 5| Step: 3
Training loss: 2.254065126211016
Validation loss: 2.496382245429099

Epoch: 5| Step: 4
Training loss: 2.405096595077385
Validation loss: 2.485006208226555

Epoch: 5| Step: 5
Training loss: 2.146715840710371
Validation loss: 2.4993952297820643

Epoch: 5| Step: 6
Training loss: 2.5373724844326384
Validation loss: 2.4858879784543384

Epoch: 5| Step: 7
Training loss: 2.7663362095047908
Validation loss: 2.4880017134041115

Epoch: 5| Step: 8
Training loss: 2.4711699395339353
Validation loss: 2.4954104495665086

Epoch: 5| Step: 9
Training loss: 2.3646525794377595
Validation loss: 2.484844523273053

Epoch: 5| Step: 10
Training loss: 2.9524997271247755
Validation loss: 2.483827178663413

Epoch: 5| Step: 11
Training loss: 2.325824907051367
Validation loss: 2.4888231235397256

Epoch: 207| Step: 0
Training loss: 2.307248519138097
Validation loss: 2.494922886677976

Epoch: 5| Step: 1
Training loss: 2.522967031927995
Validation loss: 2.4946268753279437

Epoch: 5| Step: 2
Training loss: 2.7188645261999747
Validation loss: 2.4999312629786896

Epoch: 5| Step: 3
Training loss: 3.0578200724668307
Validation loss: 2.5031641724911

Epoch: 5| Step: 4
Training loss: 1.9107273238874247
Validation loss: 2.491152637197774

Epoch: 5| Step: 5
Training loss: 2.0597621200766096
Validation loss: 2.4950174151216062

Epoch: 5| Step: 6
Training loss: 2.2140106302779774
Validation loss: 2.4850717922946575

Epoch: 5| Step: 7
Training loss: 2.608671173381674
Validation loss: 2.4857618631512532

Epoch: 5| Step: 8
Training loss: 2.685725402446686
Validation loss: 2.5004354574519483

Epoch: 5| Step: 9
Training loss: 2.064825654139988
Validation loss: 2.4921963112574703

Epoch: 5| Step: 10
Training loss: 2.3377407092832883
Validation loss: 2.4842044004009916

Epoch: 5| Step: 11
Training loss: 3.275576673215612
Validation loss: 2.4864828816227402

Epoch: 208| Step: 0
Training loss: 2.0873583123097257
Validation loss: 2.4936647770277083

Epoch: 5| Step: 1
Training loss: 2.382796991016383
Validation loss: 2.4930211808347553

Epoch: 5| Step: 2
Training loss: 2.008618144922478
Validation loss: 2.4830944911831003

Epoch: 5| Step: 3
Training loss: 2.2098098952747938
Validation loss: 2.499225289790461

Epoch: 5| Step: 4
Training loss: 2.906296432288273
Validation loss: 2.492402967268828

Epoch: 5| Step: 5
Training loss: 2.2129645339461312
Validation loss: 2.4965064911928905

Epoch: 5| Step: 6
Training loss: 2.5330532856128207
Validation loss: 2.4958570404922242

Epoch: 5| Step: 7
Training loss: 2.203612469791979
Validation loss: 2.495488904921332

Epoch: 5| Step: 8
Training loss: 2.7105988832446544
Validation loss: 2.50154881184709

Epoch: 5| Step: 9
Training loss: 2.56222234943026
Validation loss: 2.5133139141261776

Epoch: 5| Step: 10
Training loss: 3.0502732326503543
Validation loss: 2.4988883924747607

Epoch: 5| Step: 11
Training loss: 1.8822270033058917
Validation loss: 2.487544886505152

Epoch: 209| Step: 0
Training loss: 2.5179314317343455
Validation loss: 2.496560464526241

Epoch: 5| Step: 1
Training loss: 1.7953422311523863
Validation loss: 2.4872926417941374

Epoch: 5| Step: 2
Training loss: 2.184349188699881
Validation loss: 2.4835472087373205

Epoch: 5| Step: 3
Training loss: 3.0391856661356886
Validation loss: 2.4796688762591192

Epoch: 5| Step: 4
Training loss: 2.305737479308156
Validation loss: 2.4882993472891917

Epoch: 5| Step: 5
Training loss: 2.610594869763527
Validation loss: 2.4927698848047175

Epoch: 5| Step: 6
Training loss: 2.07462595590219
Validation loss: 2.4924046213565827

Epoch: 5| Step: 7
Training loss: 2.2038200851088767
Validation loss: 2.495419832651072

Epoch: 5| Step: 8
Training loss: 2.5571916072457714
Validation loss: 2.495110072693756

Epoch: 5| Step: 9
Training loss: 2.2914053218825368
Validation loss: 2.494313769005159

Epoch: 5| Step: 10
Training loss: 2.880515240303648
Validation loss: 2.496052593912741

Epoch: 5| Step: 11
Training loss: 2.6057828745293943
Validation loss: 2.5035768233686126

Epoch: 210| Step: 0
Training loss: 2.2974216563219856
Validation loss: 2.494694394635668

Epoch: 5| Step: 1
Training loss: 2.6940639091676513
Validation loss: 2.5094274132188663

Epoch: 5| Step: 2
Training loss: 2.25193967922726
Validation loss: 2.514075092025717

Epoch: 5| Step: 3
Training loss: 2.2773989217628157
Validation loss: 2.5121152850550987

Epoch: 5| Step: 4
Training loss: 2.3799153710393415
Validation loss: 2.513125178620039

Epoch: 5| Step: 5
Training loss: 2.5529051930044315
Validation loss: 2.506585854059813

Epoch: 5| Step: 6
Training loss: 2.249222515138825
Validation loss: 2.507059247589772

Epoch: 5| Step: 7
Training loss: 2.3374857285007806
Validation loss: 2.512371451061077

Epoch: 5| Step: 8
Training loss: 2.8917219400390866
Validation loss: 2.497627268791998

Epoch: 5| Step: 9
Training loss: 2.529027928962512
Validation loss: 2.5084995069515337

Epoch: 5| Step: 10
Training loss: 2.471838841613897
Validation loss: 2.506671982879615

Epoch: 5| Step: 11
Training loss: 0.6553288761797755
Validation loss: 2.506082766162554

Epoch: 211| Step: 0
Training loss: 2.415406183396953
Validation loss: 2.50710191097121

Epoch: 5| Step: 1
Training loss: 2.7254919885389213
Validation loss: 2.486214198888313

Epoch: 5| Step: 2
Training loss: 2.114810524247219
Validation loss: 2.499369029052043

Epoch: 5| Step: 3
Training loss: 2.345822447982476
Validation loss: 2.494819952871456

Epoch: 5| Step: 4
Training loss: 2.893910123872839
Validation loss: 2.491149785951008

Epoch: 5| Step: 5
Training loss: 1.8930246556646708
Validation loss: 2.501661173939208

Epoch: 5| Step: 6
Training loss: 2.5689060804187074
Validation loss: 2.5019991947630684

Epoch: 5| Step: 7
Training loss: 2.648209013408634
Validation loss: 2.5011327918908997

Epoch: 5| Step: 8
Training loss: 2.507270920882529
Validation loss: 2.48831838662917

Epoch: 5| Step: 9
Training loss: 1.7705728526301727
Validation loss: 2.504411369708911

Epoch: 5| Step: 10
Training loss: 2.5420709715749332
Validation loss: 2.503101943755593

Epoch: 5| Step: 11
Training loss: 2.148935711054796
Validation loss: 2.499285099014955

Epoch: 212| Step: 0
Training loss: 2.4647963529887296
Validation loss: 2.495603498332321

Epoch: 5| Step: 1
Training loss: 1.5877561617854676
Validation loss: 2.501401377343207

Epoch: 5| Step: 2
Training loss: 2.4899338245407328
Validation loss: 2.5004410791712437

Epoch: 5| Step: 3
Training loss: 2.7830038648517514
Validation loss: 2.500272777696209

Epoch: 5| Step: 4
Training loss: 2.2272137676567105
Validation loss: 2.5103278098202852

Epoch: 5| Step: 5
Training loss: 2.230829052793348
Validation loss: 2.4995007195205554

Epoch: 5| Step: 6
Training loss: 2.4002700415323757
Validation loss: 2.50373506084803

Epoch: 5| Step: 7
Training loss: 2.614494281685945
Validation loss: 2.5020263546012536

Epoch: 5| Step: 8
Training loss: 2.285485294485842
Validation loss: 2.503679321765553

Epoch: 5| Step: 9
Training loss: 2.678450674332373
Validation loss: 2.502833727345187

Epoch: 5| Step: 10
Training loss: 3.162191709185205
Validation loss: 2.498256389231703

Epoch: 5| Step: 11
Training loss: 1.5193384043257547
Validation loss: 2.4958937499663043

Epoch: 213| Step: 0
Training loss: 1.9117881497287714
Validation loss: 2.5027846841214663

Epoch: 5| Step: 1
Training loss: 2.206309836983745
Validation loss: 2.5133603645721094

Epoch: 5| Step: 2
Training loss: 2.141291605049159
Validation loss: 2.499686984493614

Epoch: 5| Step: 3
Training loss: 2.4405924424895105
Validation loss: 2.495968086689214

Epoch: 5| Step: 4
Training loss: 2.7923092363542836
Validation loss: 2.5005249425982354

Epoch: 5| Step: 5
Training loss: 3.13728937300743
Validation loss: 2.494947322430788

Epoch: 5| Step: 6
Training loss: 2.1856654512558027
Validation loss: 2.499622938410742

Epoch: 5| Step: 7
Training loss: 1.8452166526767397
Validation loss: 2.4954962415731217

Epoch: 5| Step: 8
Training loss: 2.511038728203401
Validation loss: 2.501667003366836

Epoch: 5| Step: 9
Training loss: 2.4585593706945
Validation loss: 2.5031596760405725

Epoch: 5| Step: 10
Training loss: 2.487105877962046
Validation loss: 2.511804910450592

Epoch: 5| Step: 11
Training loss: 3.3900228923823605
Validation loss: 2.5013452804518366

Epoch: 214| Step: 0
Training loss: 2.3312595438825774
Validation loss: 2.5044048406049484

Epoch: 5| Step: 1
Training loss: 1.8869898504561404
Validation loss: 2.5071730744277603

Epoch: 5| Step: 2
Training loss: 2.507407467657043
Validation loss: 2.5037573632753998

Epoch: 5| Step: 3
Training loss: 2.368261817408945
Validation loss: 2.518090684482876

Epoch: 5| Step: 4
Training loss: 2.367600307414181
Validation loss: 2.492534948063816

Epoch: 5| Step: 5
Training loss: 2.200162695157418
Validation loss: 2.4885747108697998

Epoch: 5| Step: 6
Training loss: 3.0997456723144605
Validation loss: 2.491178262382388

Epoch: 5| Step: 7
Training loss: 2.7838575162709813
Validation loss: 2.488564116380829

Epoch: 5| Step: 8
Training loss: 2.723527591022808
Validation loss: 2.4870546572480605

Epoch: 5| Step: 9
Training loss: 2.173133320917278
Validation loss: 2.4844406287203897

Epoch: 5| Step: 10
Training loss: 2.4653796597294435
Validation loss: 2.4872338861903387

Epoch: 5| Step: 11
Training loss: 1.862606158207376
Validation loss: 2.483940578361938

Epoch: 215| Step: 0
Training loss: 1.8559035263638899
Validation loss: 2.4795778912534905

Epoch: 5| Step: 1
Training loss: 2.3001074848767575
Validation loss: 2.4768395454448284

Epoch: 5| Step: 2
Training loss: 2.3291905160227704
Validation loss: 2.480357602650465

Epoch: 5| Step: 3
Training loss: 2.0482031320773495
Validation loss: 2.4828460955162392

Epoch: 5| Step: 4
Training loss: 2.493923240514358
Validation loss: 2.477835976613756

Epoch: 5| Step: 5
Training loss: 2.7422797467401288
Validation loss: 2.4785396774163018

Epoch: 5| Step: 6
Training loss: 2.758367379968585
Validation loss: 2.478669274279348

Epoch: 5| Step: 7
Training loss: 2.96957434201707
Validation loss: 2.4777537047287557

Epoch: 5| Step: 8
Training loss: 1.9828146860097102
Validation loss: 2.484800858056169

Epoch: 5| Step: 9
Training loss: 2.6872176975754303
Validation loss: 2.484078063653744

Epoch: 5| Step: 10
Training loss: 2.3089577634772205
Validation loss: 2.4845396263231336

Epoch: 5| Step: 11
Training loss: 3.502530953966873
Validation loss: 2.4792775976115653

Epoch: 216| Step: 0
Training loss: 2.88620977599216
Validation loss: 2.4865323382645728

Epoch: 5| Step: 1
Training loss: 3.1044511771335777
Validation loss: 2.486140869153144

Epoch: 5| Step: 2
Training loss: 1.72522231341817
Validation loss: 2.489858366060841

Epoch: 5| Step: 3
Training loss: 1.9777791496812631
Validation loss: 2.4923984354613307

Epoch: 5| Step: 4
Training loss: 2.448910050353606
Validation loss: 2.5024834258730784

Epoch: 5| Step: 5
Training loss: 2.940126583981374
Validation loss: 2.4963658820761547

Epoch: 5| Step: 6
Training loss: 2.443971406316735
Validation loss: 2.5053104107548942

Epoch: 5| Step: 7
Training loss: 2.523149219970268
Validation loss: 2.5060418078169096

Epoch: 5| Step: 8
Training loss: 2.0790532842489484
Validation loss: 2.4891813357989157

Epoch: 5| Step: 9
Training loss: 2.1592498036373784
Validation loss: 2.4981411341226574

Epoch: 5| Step: 10
Training loss: 1.97043658477639
Validation loss: 2.5029243412623052

Epoch: 5| Step: 11
Training loss: 3.206737137758365
Validation loss: 2.4892160525135836

Epoch: 217| Step: 0
Training loss: 2.4426221581544953
Validation loss: 2.4887140876711884

Epoch: 5| Step: 1
Training loss: 2.5548305646402025
Validation loss: 2.482880122610446

Epoch: 5| Step: 2
Training loss: 2.4726306980253114
Validation loss: 2.488598023376992

Epoch: 5| Step: 3
Training loss: 2.6768422494435047
Validation loss: 2.484518354906338

Epoch: 5| Step: 4
Training loss: 2.300820523559915
Validation loss: 2.4976337082233133

Epoch: 5| Step: 5
Training loss: 2.0423088101910745
Validation loss: 2.5003577731349775

Epoch: 5| Step: 6
Training loss: 2.021686989770408
Validation loss: 2.4927200218805523

Epoch: 5| Step: 7
Training loss: 2.390738789960649
Validation loss: 2.500170658167722

Epoch: 5| Step: 8
Training loss: 2.6499043681325185
Validation loss: 2.503992252787157

Epoch: 5| Step: 9
Training loss: 2.288823450515171
Validation loss: 2.4975571697948244

Epoch: 5| Step: 10
Training loss: 2.9644418074567365
Validation loss: 2.502614541616115

Epoch: 5| Step: 11
Training loss: 3.175815783848984
Validation loss: 2.4996365640636684

Epoch: 218| Step: 0
Training loss: 1.9299240893916552
Validation loss: 2.504456236013375

Epoch: 5| Step: 1
Training loss: 2.1570576799839714
Validation loss: 2.4958636874824576

Epoch: 5| Step: 2
Training loss: 2.757943644660863
Validation loss: 2.5050282535875867

Epoch: 5| Step: 3
Training loss: 2.101112856499691
Validation loss: 2.5002030608043686

Epoch: 5| Step: 4
Training loss: 2.1109415524413477
Validation loss: 2.5114297339637

Epoch: 5| Step: 5
Training loss: 2.76139395825956
Validation loss: 2.5165026734985503

Epoch: 5| Step: 6
Training loss: 3.0223571266501885
Validation loss: 2.5222442254244775

Epoch: 5| Step: 7
Training loss: 2.528881330488128
Validation loss: 2.5317409808427427

Epoch: 5| Step: 8
Training loss: 2.7350201526854794
Validation loss: 2.5304100699558343

Epoch: 5| Step: 9
Training loss: 2.253689178644746
Validation loss: 2.541485269254033

Epoch: 5| Step: 10
Training loss: 2.3635620959158232
Validation loss: 2.5315983889662292

Epoch: 5| Step: 11
Training loss: 0.9535561508824439
Validation loss: 2.5121933175797952

Epoch: 219| Step: 0
Training loss: 2.0564205852583077
Validation loss: 2.5228540358157563

Epoch: 5| Step: 1
Training loss: 1.969293549601779
Validation loss: 2.531511059787683

Epoch: 5| Step: 2
Training loss: 2.7295529817692943
Validation loss: 2.5022010452083654

Epoch: 5| Step: 3
Training loss: 1.9477476634864859
Validation loss: 2.4987163228279248

Epoch: 5| Step: 4
Training loss: 2.776035864694531
Validation loss: 2.49373723419606

Epoch: 5| Step: 5
Training loss: 2.5726679528575778
Validation loss: 2.487141997647285

Epoch: 5| Step: 6
Training loss: 2.5783942804446505
Validation loss: 2.488848878482348

Epoch: 5| Step: 7
Training loss: 2.7242170591488604
Validation loss: 2.481696470306678

Epoch: 5| Step: 8
Training loss: 2.475065819992159
Validation loss: 2.474590788191613

Epoch: 5| Step: 9
Training loss: 2.093718343466957
Validation loss: 2.5010425618996397

Epoch: 5| Step: 10
Training loss: 2.5419797131308886
Validation loss: 2.489280240643902

Epoch: 5| Step: 11
Training loss: 2.282963357545305
Validation loss: 2.490641819152457

Epoch: 220| Step: 0
Training loss: 2.400714998234189
Validation loss: 2.4941378130158833

Epoch: 5| Step: 1
Training loss: 2.5045814496468344
Validation loss: 2.495585213180119

Epoch: 5| Step: 2
Training loss: 2.5049264052615134
Validation loss: 2.4969871685183183

Epoch: 5| Step: 3
Training loss: 2.3867025671377515
Validation loss: 2.4967505158611996

Epoch: 5| Step: 4
Training loss: 2.468301273561027
Validation loss: 2.4989081263351767

Epoch: 5| Step: 5
Training loss: 2.400882034346269
Validation loss: 2.4842984569602145

Epoch: 5| Step: 6
Training loss: 2.4759333921603286
Validation loss: 2.5021610297707872

Epoch: 5| Step: 7
Training loss: 3.1291298089902297
Validation loss: 2.492757530752451

Epoch: 5| Step: 8
Training loss: 1.9643771336076918
Validation loss: 2.494774280134446

Epoch: 5| Step: 9
Training loss: 2.540614290672338
Validation loss: 2.4851783437265444

Epoch: 5| Step: 10
Training loss: 2.1025468867785793
Validation loss: 2.4910299987362317

Epoch: 5| Step: 11
Training loss: 1.2932558230806017
Validation loss: 2.4898755861003936

Epoch: 221| Step: 0
Training loss: 3.0646457550793658
Validation loss: 2.481402412172819

Epoch: 5| Step: 1
Training loss: 2.3284124414562117
Validation loss: 2.482889803112509

Epoch: 5| Step: 2
Training loss: 2.7915281194971615
Validation loss: 2.495992309228346

Epoch: 5| Step: 3
Training loss: 2.188932767042051
Validation loss: 2.4880421801609516

Epoch: 5| Step: 4
Training loss: 2.027141583015136
Validation loss: 2.507041464032219

Epoch: 5| Step: 5
Training loss: 2.0192913451557932
Validation loss: 2.5074414764096136

Epoch: 5| Step: 6
Training loss: 2.728795926681337
Validation loss: 2.52768422368186

Epoch: 5| Step: 7
Training loss: 2.617007667853871
Validation loss: 2.5446250588249533

Epoch: 5| Step: 8
Training loss: 2.38135791844403
Validation loss: 2.523214525227202

Epoch: 5| Step: 9
Training loss: 2.1748283669287893
Validation loss: 2.498996473601449

Epoch: 5| Step: 10
Training loss: 2.2393455438276257
Validation loss: 2.490255826969872

Epoch: 5| Step: 11
Training loss: 2.78241748094513
Validation loss: 2.4833925531887395

Epoch: 222| Step: 0
Training loss: 2.628384996179378
Validation loss: 2.489242180522904

Epoch: 5| Step: 1
Training loss: 2.5085539390234834
Validation loss: 2.4937640298880543

Epoch: 5| Step: 2
Training loss: 2.3931008277881753
Validation loss: 2.4906078023727605

Epoch: 5| Step: 3
Training loss: 2.3294956345739357
Validation loss: 2.4861655430603795

Epoch: 5| Step: 4
Training loss: 2.5581253671354602
Validation loss: 2.4969997880902732

Epoch: 5| Step: 5
Training loss: 2.632134777647493
Validation loss: 2.4798271290520133

Epoch: 5| Step: 6
Training loss: 2.0042696915104448
Validation loss: 2.487094662099907

Epoch: 5| Step: 7
Training loss: 2.6660795658254943
Validation loss: 2.481422377170988

Epoch: 5| Step: 8
Training loss: 2.4758626147472866
Validation loss: 2.487771569416725

Epoch: 5| Step: 9
Training loss: 2.200883089904424
Validation loss: 2.484860382807995

Epoch: 5| Step: 10
Training loss: 2.531049838217102
Validation loss: 2.4816898654300052

Epoch: 5| Step: 11
Training loss: 2.887755913561358
Validation loss: 2.4867255609171175

Epoch: 223| Step: 0
Training loss: 3.33689762923046
Validation loss: 2.481772208972295

Epoch: 5| Step: 1
Training loss: 2.280212793626136
Validation loss: 2.4902545763580117

Epoch: 5| Step: 2
Training loss: 2.2109213245460633
Validation loss: 2.4947342690147933

Epoch: 5| Step: 3
Training loss: 2.6659499834524314
Validation loss: 2.4995785198959126

Epoch: 5| Step: 4
Training loss: 2.162916740878486
Validation loss: 2.505656887564235

Epoch: 5| Step: 5
Training loss: 2.811760020926964
Validation loss: 2.492852523869438

Epoch: 5| Step: 6
Training loss: 2.0447141000730635
Validation loss: 2.4902607556111347

Epoch: 5| Step: 7
Training loss: 2.345615814134077
Validation loss: 2.483967913657914

Epoch: 5| Step: 8
Training loss: 2.298302343195099
Validation loss: 2.4761104993570036

Epoch: 5| Step: 9
Training loss: 2.379873195123161
Validation loss: 2.4788614948094545

Epoch: 5| Step: 10
Training loss: 2.7335912834738534
Validation loss: 2.4923775758875855

Epoch: 5| Step: 11
Training loss: 2.460978286647376
Validation loss: 2.483373016055299

Epoch: 224| Step: 0
Training loss: 2.2880990666636767
Validation loss: 2.4922341488864

Epoch: 5| Step: 1
Training loss: 2.0169997147809093
Validation loss: 2.485479369309196

Epoch: 5| Step: 2
Training loss: 2.6269424154637644
Validation loss: 2.479347924944569

Epoch: 5| Step: 3
Training loss: 2.4092198659625956
Validation loss: 2.4794719986195703

Epoch: 5| Step: 4
Training loss: 1.8926542127055512
Validation loss: 2.4846889969191888

Epoch: 5| Step: 5
Training loss: 2.847683376606908
Validation loss: 2.481063627262822

Epoch: 5| Step: 6
Training loss: 2.2830363556168165
Validation loss: 2.4800787231104917

Epoch: 5| Step: 7
Training loss: 3.022873148132774
Validation loss: 2.494881872538603

Epoch: 5| Step: 8
Training loss: 2.818542516815721
Validation loss: 2.4868795138593973

Epoch: 5| Step: 9
Training loss: 2.287885864732221
Validation loss: 2.491094614680515

Epoch: 5| Step: 10
Training loss: 2.2626079596795647
Validation loss: 2.4905099275012414

Epoch: 5| Step: 11
Training loss: 3.5148413568892583
Validation loss: 2.481780488808347

Epoch: 225| Step: 0
Training loss: 2.277280306055052
Validation loss: 2.474872168698048

Epoch: 5| Step: 1
Training loss: 2.67141434117105
Validation loss: 2.4798067706404265

Epoch: 5| Step: 2
Training loss: 2.0602389571101423
Validation loss: 2.48715000198292

Epoch: 5| Step: 3
Training loss: 2.4039491825032626
Validation loss: 2.468053844337774

Epoch: 5| Step: 4
Training loss: 2.1171752253665748
Validation loss: 2.480621755691884

Epoch: 5| Step: 5
Training loss: 2.34945432130743
Validation loss: 2.480245753399441

Epoch: 5| Step: 6
Training loss: 2.4142009608982535
Validation loss: 2.468999725298263

Epoch: 5| Step: 7
Training loss: 3.2834810936948253
Validation loss: 2.4746288408187724

Epoch: 5| Step: 8
Training loss: 2.3584905318134233
Validation loss: 2.4793650376805925

Epoch: 5| Step: 9
Training loss: 2.3582581156495266
Validation loss: 2.4805275617220497

Epoch: 5| Step: 10
Training loss: 2.267043412912301
Validation loss: 2.4806541914218254

Epoch: 5| Step: 11
Training loss: 2.897620731237891
Validation loss: 2.4749532419020035

Epoch: 226| Step: 0
Training loss: 2.5473981448729597
Validation loss: 2.4765127319405553

Epoch: 5| Step: 1
Training loss: 2.339029262716844
Validation loss: 2.4849813429134002

Epoch: 5| Step: 2
Training loss: 1.9777945195726505
Validation loss: 2.486935122168153

Epoch: 5| Step: 3
Training loss: 2.3662965816501322
Validation loss: 2.487613929842364

Epoch: 5| Step: 4
Training loss: 2.9287581533789857
Validation loss: 2.480344305646377

Epoch: 5| Step: 5
Training loss: 2.3726475760383305
Validation loss: 2.481572467819324

Epoch: 5| Step: 6
Training loss: 2.4392363403851154
Validation loss: 2.4801395988846675

Epoch: 5| Step: 7
Training loss: 2.689507909811019
Validation loss: 2.471176528308007

Epoch: 5| Step: 8
Training loss: 2.0585169624035586
Validation loss: 2.4676904778876523

Epoch: 5| Step: 9
Training loss: 2.084568217185422
Validation loss: 2.475734632491944

Epoch: 5| Step: 10
Training loss: 2.7658972309850207
Validation loss: 2.483708686165042

Epoch: 5| Step: 11
Training loss: 2.1378849062223666
Validation loss: 2.473258485017126

Epoch: 227| Step: 0
Training loss: 2.0511925275842064
Validation loss: 2.4840247669106335

Epoch: 5| Step: 1
Training loss: 2.437215543923833
Validation loss: 2.475651735011991

Epoch: 5| Step: 2
Training loss: 1.9493302980356726
Validation loss: 2.4919900447229217

Epoch: 5| Step: 3
Training loss: 2.49124299809706
Validation loss: 2.500122091173107

Epoch: 5| Step: 4
Training loss: 2.8741146880688913
Validation loss: 2.491190233472293

Epoch: 5| Step: 5
Training loss: 2.5008374718802697
Validation loss: 2.497363826532023

Epoch: 5| Step: 6
Training loss: 2.186802562158234
Validation loss: 2.490539992783139

Epoch: 5| Step: 7
Training loss: 1.9584827027948464
Validation loss: 2.4910666238159314

Epoch: 5| Step: 8
Training loss: 2.32680438176084
Validation loss: 2.5026591463959993

Epoch: 5| Step: 9
Training loss: 3.16844706093602
Validation loss: 2.492262449502758

Epoch: 5| Step: 10
Training loss: 2.7166403115833226
Validation loss: 2.495091124993837

Epoch: 5| Step: 11
Training loss: 1.234755445352099
Validation loss: 2.4978481449297996

Epoch: 228| Step: 0
Training loss: 2.545308760704071
Validation loss: 2.509276013541922

Epoch: 5| Step: 1
Training loss: 2.7633172875994174
Validation loss: 2.508050647808768

Epoch: 5| Step: 2
Training loss: 2.458320218929505
Validation loss: 2.504119360112492

Epoch: 5| Step: 3
Training loss: 2.25053854960877
Validation loss: 2.516582887141338

Epoch: 5| Step: 4
Training loss: 2.2618704367784237
Validation loss: 2.509216367237146

Epoch: 5| Step: 5
Training loss: 2.3321898133318686
Validation loss: 2.5083596532862313

Epoch: 5| Step: 6
Training loss: 1.6967298498299113
Validation loss: 2.514745443997641

Epoch: 5| Step: 7
Training loss: 2.930057268331649
Validation loss: 2.511974663795982

Epoch: 5| Step: 8
Training loss: 2.1735241892547768
Validation loss: 2.5230598186754065

Epoch: 5| Step: 9
Training loss: 2.581872701466114
Validation loss: 2.500126080512656

Epoch: 5| Step: 10
Training loss: 2.074907378921651
Validation loss: 2.5005016101833752

Epoch: 5| Step: 11
Training loss: 3.3657940240352566
Validation loss: 2.5085544419552726

Epoch: 229| Step: 0
Training loss: 1.7623507091521116
Validation loss: 2.5057831889162054

Epoch: 5| Step: 1
Training loss: 1.7729270973601003
Validation loss: 2.5011445048292953

Epoch: 5| Step: 2
Training loss: 2.766907216917038
Validation loss: 2.5006315864031947

Epoch: 5| Step: 3
Training loss: 2.6050849414529766
Validation loss: 2.5076073773594274

Epoch: 5| Step: 4
Training loss: 2.4909536721581915
Validation loss: 2.5035499283512825

Epoch: 5| Step: 5
Training loss: 2.717225096553798
Validation loss: 2.522317905680772

Epoch: 5| Step: 6
Training loss: 1.828639422689479
Validation loss: 2.5107228631871323

Epoch: 5| Step: 7
Training loss: 2.855540889136636
Validation loss: 2.5149779741474227

Epoch: 5| Step: 8
Training loss: 2.6075328788988594
Validation loss: 2.5148000726998343

Epoch: 5| Step: 9
Training loss: 2.5300199557636893
Validation loss: 2.4994148602764716

Epoch: 5| Step: 10
Training loss: 2.2990846968783925
Validation loss: 2.49480074016113

Epoch: 5| Step: 11
Training loss: 1.7614831872679653
Validation loss: 2.50050856265396

Epoch: 230| Step: 0
Training loss: 2.9532125147209594
Validation loss: 2.5071517929621225

Epoch: 5| Step: 1
Training loss: 2.2650576177009554
Validation loss: 2.4995278389424502

Epoch: 5| Step: 2
Training loss: 2.2777923089884564
Validation loss: 2.513439176588058

Epoch: 5| Step: 3
Training loss: 2.402871274017206
Validation loss: 2.497495557102023

Epoch: 5| Step: 4
Training loss: 2.5435156636941554
Validation loss: 2.506228769835548

Epoch: 5| Step: 5
Training loss: 2.5082735487529138
Validation loss: 2.504147994521395

Epoch: 5| Step: 6
Training loss: 2.275858111512621
Validation loss: 2.505708986776082

Epoch: 5| Step: 7
Training loss: 2.198561792174605
Validation loss: 2.4950035392822767

Epoch: 5| Step: 8
Training loss: 2.130064706849299
Validation loss: 2.4947596304368953

Epoch: 5| Step: 9
Training loss: 2.6766220665141884
Validation loss: 2.4954100355475397

Epoch: 5| Step: 10
Training loss: 2.157766499950944
Validation loss: 2.504883256680435

Epoch: 5| Step: 11
Training loss: 2.3491316997103904
Validation loss: 2.514647351113489

Epoch: 231| Step: 0
Training loss: 2.2773887669087354
Validation loss: 2.496108877407793

Epoch: 5| Step: 1
Training loss: 2.813832963205922
Validation loss: 2.489367109882079

Epoch: 5| Step: 2
Training loss: 2.8596599832902756
Validation loss: 2.491532501757155

Epoch: 5| Step: 3
Training loss: 1.9121070682562376
Validation loss: 2.488605370364181

Epoch: 5| Step: 4
Training loss: 2.79197041201831
Validation loss: 2.49044932914948

Epoch: 5| Step: 5
Training loss: 2.2729418237158163
Validation loss: 2.495301221815279

Epoch: 5| Step: 6
Training loss: 2.415889900161965
Validation loss: 2.4945316630497625

Epoch: 5| Step: 7
Training loss: 2.3790856404934746
Validation loss: 2.4872213528664995

Epoch: 5| Step: 8
Training loss: 2.075134075383239
Validation loss: 2.4862103750230333

Epoch: 5| Step: 9
Training loss: 2.538942586561426
Validation loss: 2.4902362319839635

Epoch: 5| Step: 10
Training loss: 1.4168304834831167
Validation loss: 2.5071042210416374

Epoch: 5| Step: 11
Training loss: 3.627178326888722
Validation loss: 2.489445975170825

Epoch: 232| Step: 0
Training loss: 2.9692562273985907
Validation loss: 2.502709013733649

Epoch: 5| Step: 1
Training loss: 2.0472665659729996
Validation loss: 2.4983360196757194

Epoch: 5| Step: 2
Training loss: 2.3994162485363466
Validation loss: 2.5091241711979535

Epoch: 5| Step: 3
Training loss: 2.2467662036855187
Validation loss: 2.5013223449002897

Epoch: 5| Step: 4
Training loss: 2.2663411028183136
Validation loss: 2.4985327110256197

Epoch: 5| Step: 5
Training loss: 2.19668883488554
Validation loss: 2.5073970339613747

Epoch: 5| Step: 6
Training loss: 2.592327417204139
Validation loss: 2.5054420605371823

Epoch: 5| Step: 7
Training loss: 2.6840561380953845
Validation loss: 2.514311662856522

Epoch: 5| Step: 8
Training loss: 2.4141784442600716
Validation loss: 2.5316251449296114

Epoch: 5| Step: 9
Training loss: 2.387192000249902
Validation loss: 2.5123715973618816

Epoch: 5| Step: 10
Training loss: 2.211839579492359
Validation loss: 2.5136347572571953

Epoch: 5| Step: 11
Training loss: 1.2206428207760738
Validation loss: 2.512067565902231

Epoch: 233| Step: 0
Training loss: 2.2876445036725235
Validation loss: 2.5087778129043286

Epoch: 5| Step: 1
Training loss: 1.6088909421179394
Validation loss: 2.50781636222946

Epoch: 5| Step: 2
Training loss: 2.4255272675512773
Validation loss: 2.515436374708634

Epoch: 5| Step: 3
Training loss: 2.610911208362257
Validation loss: 2.5117501454148736

Epoch: 5| Step: 4
Training loss: 2.5323283406380352
Validation loss: 2.5075905405536196

Epoch: 5| Step: 5
Training loss: 2.860542434257614
Validation loss: 2.508595246415834

Epoch: 5| Step: 6
Training loss: 2.324074312738905
Validation loss: 2.504105229240303

Epoch: 5| Step: 7
Training loss: 2.462374216343047
Validation loss: 2.499619741130687

Epoch: 5| Step: 8
Training loss: 2.1649310300558495
Validation loss: 2.5152739325288667

Epoch: 5| Step: 9
Training loss: 2.452013091769592
Validation loss: 2.5185807399183777

Epoch: 5| Step: 10
Training loss: 2.461574702041949
Validation loss: 2.5179277625610887

Epoch: 5| Step: 11
Training loss: 1.5668251447806856
Validation loss: 2.5136934964648874

Epoch: 234| Step: 0
Training loss: 2.6662223763695905
Validation loss: 2.516853190661404

Epoch: 5| Step: 1
Training loss: 2.8898825851928955
Validation loss: 2.5110401168202774

Epoch: 5| Step: 2
Training loss: 2.376448942201397
Validation loss: 2.519431703163304

Epoch: 5| Step: 3
Training loss: 2.0037885783783675
Validation loss: 2.5294244996695903

Epoch: 5| Step: 4
Training loss: 2.1946168931034675
Validation loss: 2.5147187276849383

Epoch: 5| Step: 5
Training loss: 1.9505616112908426
Validation loss: 2.5134970785255164

Epoch: 5| Step: 6
Training loss: 2.220196427083695
Validation loss: 2.512654301822289

Epoch: 5| Step: 7
Training loss: 2.4009849116857396
Validation loss: 2.5112362008026587

Epoch: 5| Step: 8
Training loss: 1.5097755737766483
Validation loss: 2.503859183771205

Epoch: 5| Step: 9
Training loss: 2.877579071257701
Validation loss: 2.5013656383080787

Epoch: 5| Step: 10
Training loss: 2.9815523070283203
Validation loss: 2.4918008621996415

Epoch: 5| Step: 11
Training loss: 2.4403796669793714
Validation loss: 2.508772245503932

Epoch: 235| Step: 0
Training loss: 2.3656063023663534
Validation loss: 2.5158358222967645

Epoch: 5| Step: 1
Training loss: 2.7233779804425113
Validation loss: 2.4967695743105045

Epoch: 5| Step: 2
Training loss: 2.873255324695087
Validation loss: 2.4920777522908333

Epoch: 5| Step: 3
Training loss: 2.5063447549083393
Validation loss: 2.5004609755857814

Epoch: 5| Step: 4
Training loss: 2.3228599872146054
Validation loss: 2.5036652280880136

Epoch: 5| Step: 5
Training loss: 2.4301612122359404
Validation loss: 2.5132024959596744

Epoch: 5| Step: 6
Training loss: 2.3929630390314545
Validation loss: 2.4942478484483837

Epoch: 5| Step: 7
Training loss: 2.371395588348344
Validation loss: 2.503142388640992

Epoch: 5| Step: 8
Training loss: 1.9144847793312794
Validation loss: 2.5048462585597835

Epoch: 5| Step: 9
Training loss: 2.2707438553379764
Validation loss: 2.5072363392634647

Epoch: 5| Step: 10
Training loss: 2.409271028207416
Validation loss: 2.494367604593191

Epoch: 5| Step: 11
Training loss: 1.7901948901588833
Validation loss: 2.494816620024334

Epoch: 236| Step: 0
Training loss: 2.1844520042282585
Validation loss: 2.5022378283554736

Epoch: 5| Step: 1
Training loss: 2.2519529132561695
Validation loss: 2.494581704804748

Epoch: 5| Step: 2
Training loss: 2.3166344258468574
Validation loss: 2.502362124792135

Epoch: 5| Step: 3
Training loss: 2.407212696461079
Validation loss: 2.500877369466805

Epoch: 5| Step: 4
Training loss: 2.799857142755042
Validation loss: 2.509584428750152

Epoch: 5| Step: 5
Training loss: 2.616958653707552
Validation loss: 2.511373212350948

Epoch: 5| Step: 6
Training loss: 1.6390056792993466
Validation loss: 2.5097900864414693

Epoch: 5| Step: 7
Training loss: 1.9258038440175922
Validation loss: 2.5203590196422208

Epoch: 5| Step: 8
Training loss: 2.8898193886362495
Validation loss: 2.5347101948654513

Epoch: 5| Step: 9
Training loss: 2.607290383213005
Validation loss: 2.526013102156909

Epoch: 5| Step: 10
Training loss: 2.6878074647545596
Validation loss: 2.5370497996339774

Epoch: 5| Step: 11
Training loss: 1.9662172283489932
Validation loss: 2.5407720786764423

Epoch: 237| Step: 0
Training loss: 2.224508415523498
Validation loss: 2.495749312999536

Epoch: 5| Step: 1
Training loss: 2.2115152096243658
Validation loss: 2.5083618156636756

Epoch: 5| Step: 2
Training loss: 2.547000250047627
Validation loss: 2.4984240571519827

Epoch: 5| Step: 3
Training loss: 2.511907258976285
Validation loss: 2.497693277147591

Epoch: 5| Step: 4
Training loss: 3.0039324418254267
Validation loss: 2.5043470973570803

Epoch: 5| Step: 5
Training loss: 2.498560491019322
Validation loss: 2.4962304425607615

Epoch: 5| Step: 6
Training loss: 2.9653098000180367
Validation loss: 2.5039276879910912

Epoch: 5| Step: 7
Training loss: 1.6970732374645714
Validation loss: 2.5015433474936732

Epoch: 5| Step: 8
Training loss: 2.0687655445448376
Validation loss: 2.497847146685981

Epoch: 5| Step: 9
Training loss: 2.7899516422087025
Validation loss: 2.506743911502432

Epoch: 5| Step: 10
Training loss: 2.158194950140061
Validation loss: 2.498498064125761

Epoch: 5| Step: 11
Training loss: 2.690477983487798
Validation loss: 2.5050809488142294

Epoch: 238| Step: 0
Training loss: 2.954828003397674
Validation loss: 2.492445255770087

Epoch: 5| Step: 1
Training loss: 2.718307722225573
Validation loss: 2.496513502539679

Epoch: 5| Step: 2
Training loss: 2.1809120156837616
Validation loss: 2.4928821604072313

Epoch: 5| Step: 3
Training loss: 1.6714220235507689
Validation loss: 2.4927990919319014

Epoch: 5| Step: 4
Training loss: 2.492231697479859
Validation loss: 2.482681108810797

Epoch: 5| Step: 5
Training loss: 2.7991672265189558
Validation loss: 2.4785356934128004

Epoch: 5| Step: 6
Training loss: 2.371825053577483
Validation loss: 2.4883732262205194

Epoch: 5| Step: 7
Training loss: 2.5607895143307937
Validation loss: 2.484175676071208

Epoch: 5| Step: 8
Training loss: 2.2993814714760115
Validation loss: 2.481202128770899

Epoch: 5| Step: 9
Training loss: 1.9172651628030721
Validation loss: 2.4845799495523218

Epoch: 5| Step: 10
Training loss: 2.8624930968888513
Validation loss: 2.500295295759444

Epoch: 5| Step: 11
Training loss: 1.6038012232817536
Validation loss: 2.4867824230297666

Epoch: 239| Step: 0
Training loss: 2.559852721328909
Validation loss: 2.4835620005721077

Epoch: 5| Step: 1
Training loss: 1.8652316907294166
Validation loss: 2.4875110270544907

Epoch: 5| Step: 2
Training loss: 2.311097106167753
Validation loss: 2.4732947426631826

Epoch: 5| Step: 3
Training loss: 2.4535667847739933
Validation loss: 2.4895636519182154

Epoch: 5| Step: 4
Training loss: 2.00028763133274
Validation loss: 2.4750674455296164

Epoch: 5| Step: 5
Training loss: 2.8748611748390855
Validation loss: 2.4802357121130933

Epoch: 5| Step: 6
Training loss: 2.2744043136263263
Validation loss: 2.4836549935155094

Epoch: 5| Step: 7
Training loss: 2.5613867039313716
Validation loss: 2.484613599056044

Epoch: 5| Step: 8
Training loss: 2.0358662895109516
Validation loss: 2.4833093512953757

Epoch: 5| Step: 9
Training loss: 1.8230976850459688
Validation loss: 2.490404896601162

Epoch: 5| Step: 10
Training loss: 3.325067682939758
Validation loss: 2.495139025488715

Epoch: 5| Step: 11
Training loss: 2.816314653449413
Validation loss: 2.4874472625292148

Epoch: 240| Step: 0
Training loss: 2.446557553722777
Validation loss: 2.4950944216411868

Epoch: 5| Step: 1
Training loss: 2.294158574856881
Validation loss: 2.482584409997801

Epoch: 5| Step: 2
Training loss: 2.205091315906626
Validation loss: 2.489925677544847

Epoch: 5| Step: 3
Training loss: 2.1848471768356497
Validation loss: 2.490304183480147

Epoch: 5| Step: 4
Training loss: 2.942121076922976
Validation loss: 2.4806602704459406

Epoch: 5| Step: 5
Training loss: 3.0184807572433097
Validation loss: 2.4824425321050154

Epoch: 5| Step: 6
Training loss: 2.523678794638553
Validation loss: 2.4819657101461634

Epoch: 5| Step: 7
Training loss: 2.253785763169576
Validation loss: 2.4935172150012965

Epoch: 5| Step: 8
Training loss: 2.46698060185878
Validation loss: 2.4935118605346482

Epoch: 5| Step: 9
Training loss: 2.3311827376329366
Validation loss: 2.501380662347385

Epoch: 5| Step: 10
Training loss: 2.2656073207823155
Validation loss: 2.5129487272732782

Epoch: 5| Step: 11
Training loss: 1.6375527817467723
Validation loss: 2.5283124974418434

Epoch: 241| Step: 0
Training loss: 2.6596643382370346
Validation loss: 2.5329092576381407

Epoch: 5| Step: 1
Training loss: 2.3349176432362237
Validation loss: 2.5264480670279874

Epoch: 5| Step: 2
Training loss: 2.0308891562750744
Validation loss: 2.515098402140864

Epoch: 5| Step: 3
Training loss: 3.084676673305163
Validation loss: 2.5021405156060053

Epoch: 5| Step: 4
Training loss: 2.63298029605434
Validation loss: 2.503684690197872

Epoch: 5| Step: 5
Training loss: 2.197413297989014
Validation loss: 2.5022187242131526

Epoch: 5| Step: 6
Training loss: 2.473295059970467
Validation loss: 2.5069680302344937

Epoch: 5| Step: 7
Training loss: 2.9913795914337564
Validation loss: 2.4988201257910503

Epoch: 5| Step: 8
Training loss: 2.1828141296424324
Validation loss: 2.501571407774824

Epoch: 5| Step: 9
Training loss: 2.1711066554160565
Validation loss: 2.514048813126101

Epoch: 5| Step: 10
Training loss: 2.193912589196672
Validation loss: 2.5275399231674034

Epoch: 5| Step: 11
Training loss: 2.4601539938883947
Validation loss: 2.5050512781434096

Epoch: 242| Step: 0
Training loss: 2.9898602471495805
Validation loss: 2.49829344915687

Epoch: 5| Step: 1
Training loss: 1.9229374251755431
Validation loss: 2.4902121948052254

Epoch: 5| Step: 2
Training loss: 2.2392203339918555
Validation loss: 2.501217440368302

Epoch: 5| Step: 3
Training loss: 1.844136213811646
Validation loss: 2.488772900136077

Epoch: 5| Step: 4
Training loss: 3.0774516807004297
Validation loss: 2.494466992769326

Epoch: 5| Step: 5
Training loss: 2.104815153401023
Validation loss: 2.4919220554809898

Epoch: 5| Step: 6
Training loss: 2.3529320464237573
Validation loss: 2.495904544192293

Epoch: 5| Step: 7
Training loss: 2.2157279912042136
Validation loss: 2.5096482898877714

Epoch: 5| Step: 8
Training loss: 2.142844393101864
Validation loss: 2.5017538713478804

Epoch: 5| Step: 9
Training loss: 2.9887530583932786
Validation loss: 2.5054809490716123

Epoch: 5| Step: 10
Training loss: 2.1941018890118196
Validation loss: 2.5039995308023486

Epoch: 5| Step: 11
Training loss: 2.771025476509966
Validation loss: 2.498324941708411

Epoch: 243| Step: 0
Training loss: 2.2033445641158793
Validation loss: 2.5062907425699357

Epoch: 5| Step: 1
Training loss: 2.8995337867879893
Validation loss: 2.5008688430199353

Epoch: 5| Step: 2
Training loss: 2.157485607890443
Validation loss: 2.5103945447545906

Epoch: 5| Step: 3
Training loss: 2.188675373924954
Validation loss: 2.498294300097193

Epoch: 5| Step: 4
Training loss: 2.2724803469640054
Validation loss: 2.513619187955634

Epoch: 5| Step: 5
Training loss: 1.9010731803337433
Validation loss: 2.518618845778812

Epoch: 5| Step: 6
Training loss: 2.50825254198248
Validation loss: 2.5364454466213675

Epoch: 5| Step: 7
Training loss: 2.69394328404164
Validation loss: 2.5272285046015672

Epoch: 5| Step: 8
Training loss: 2.920451261061774
Validation loss: 2.5432706849938724

Epoch: 5| Step: 9
Training loss: 2.7328596875504565
Validation loss: 2.5256437414251653

Epoch: 5| Step: 10
Training loss: 2.1691976827230284
Validation loss: 2.533411221864938

Epoch: 5| Step: 11
Training loss: 1.9163229537384616
Validation loss: 2.517934761603082

Epoch: 244| Step: 0
Training loss: 2.797614143776968
Validation loss: 2.5088036144364665

Epoch: 5| Step: 1
Training loss: 2.2709511075990756
Validation loss: 2.4982682429006133

Epoch: 5| Step: 2
Training loss: 2.9453591259572023
Validation loss: 2.506787214312176

Epoch: 5| Step: 3
Training loss: 2.340743514397789
Validation loss: 2.5042783765159715

Epoch: 5| Step: 4
Training loss: 1.9833887007008049
Validation loss: 2.5097498397231783

Epoch: 5| Step: 5
Training loss: 2.2601793812142685
Validation loss: 2.49918379970358

Epoch: 5| Step: 6
Training loss: 2.5478104821633103
Validation loss: 2.501749332653943

Epoch: 5| Step: 7
Training loss: 3.106553220628556
Validation loss: 2.510239311274274

Epoch: 5| Step: 8
Training loss: 1.6203973584223497
Validation loss: 2.5198576801973367

Epoch: 5| Step: 9
Training loss: 2.306969912262929
Validation loss: 2.5104451369152496

Epoch: 5| Step: 10
Training loss: 1.9302920116629807
Validation loss: 2.506131417821668

Epoch: 5| Step: 11
Training loss: 2.640071020760507
Validation loss: 2.5086394478591605

Epoch: 245| Step: 0
Training loss: 2.3187905403150877
Validation loss: 2.5226950483271353

Epoch: 5| Step: 1
Training loss: 2.4567188737550554
Validation loss: 2.536909710763401

Epoch: 5| Step: 2
Training loss: 2.0651888803918967
Validation loss: 2.5241202502570816

Epoch: 5| Step: 3
Training loss: 2.519240819470826
Validation loss: 2.5346885998265383

Epoch: 5| Step: 4
Training loss: 2.2883999747815236
Validation loss: 2.5413258915589916

Epoch: 5| Step: 5
Training loss: 2.548128346227414
Validation loss: 2.5430250967319696

Epoch: 5| Step: 6
Training loss: 2.601920438319779
Validation loss: 2.541311654848603

Epoch: 5| Step: 7
Training loss: 2.595635705306244
Validation loss: 2.529640309940392

Epoch: 5| Step: 8
Training loss: 2.5449376138711326
Validation loss: 2.5168999014128515

Epoch: 5| Step: 9
Training loss: 2.184765114282023
Validation loss: 2.5134187465690445

Epoch: 5| Step: 10
Training loss: 2.0016109416022254
Validation loss: 2.5178906562665744

Epoch: 5| Step: 11
Training loss: 1.9862580263565015
Validation loss: 2.513715315341909

Epoch: 246| Step: 0
Training loss: 2.3323561347841655
Validation loss: 2.5030262432706425

Epoch: 5| Step: 1
Training loss: 2.5892954802681296
Validation loss: 2.5185559536419304

Epoch: 5| Step: 2
Training loss: 1.7575151658251682
Validation loss: 2.522655889598308

Epoch: 5| Step: 3
Training loss: 2.1784962232415586
Validation loss: 2.518831555108702

Epoch: 5| Step: 4
Training loss: 2.6736628033373444
Validation loss: 2.503144450356188

Epoch: 5| Step: 5
Training loss: 2.3248147665089105
Validation loss: 2.524089016576917

Epoch: 5| Step: 6
Training loss: 2.3179620725344607
Validation loss: 2.526142450006625

Epoch: 5| Step: 7
Training loss: 2.365388999061745
Validation loss: 2.5289256212499978

Epoch: 5| Step: 8
Training loss: 2.293014603892293
Validation loss: 2.538778701963072

Epoch: 5| Step: 9
Training loss: 2.5416605172838542
Validation loss: 2.541339397205865

Epoch: 5| Step: 10
Training loss: 2.4510127407257993
Validation loss: 2.5418120535454043

Epoch: 5| Step: 11
Training loss: 2.9655580728131996
Validation loss: 2.5261763795673193

Epoch: 247| Step: 0
Training loss: 2.5709099814325267
Validation loss: 2.536273945027476

Epoch: 5| Step: 1
Training loss: 2.4343428094226667
Validation loss: 2.5431024230107377

Epoch: 5| Step: 2
Training loss: 2.468927884030923
Validation loss: 2.544342400524649

Epoch: 5| Step: 3
Training loss: 1.8201879368677287
Validation loss: 2.511092234893855

Epoch: 5| Step: 4
Training loss: 2.8354917233688863
Validation loss: 2.5166073691622213

Epoch: 5| Step: 5
Training loss: 2.4498270791201433
Validation loss: 2.5291073604168384

Epoch: 5| Step: 6
Training loss: 2.1838930092130653
Validation loss: 2.5162692261965867

Epoch: 5| Step: 7
Training loss: 2.0918342594114994
Validation loss: 2.5182719550825423

Epoch: 5| Step: 8
Training loss: 1.8951208082154878
Validation loss: 2.526118768255997

Epoch: 5| Step: 9
Training loss: 2.492880793555702
Validation loss: 2.5263161670677734

Epoch: 5| Step: 10
Training loss: 2.6491097916358375
Validation loss: 2.5231147221118184

Epoch: 5| Step: 11
Training loss: 1.8929123086422224
Validation loss: 2.523305218263647

Epoch: 248| Step: 0
Training loss: 2.1458870470552456
Validation loss: 2.525618985131243

Epoch: 5| Step: 1
Training loss: 2.897207816715906
Validation loss: 2.540184496908502

Epoch: 5| Step: 2
Training loss: 2.5169014861174666
Validation loss: 2.5233337766826685

Epoch: 5| Step: 3
Training loss: 2.4814141822132325
Validation loss: 2.5287303491382946

Epoch: 5| Step: 4
Training loss: 2.4094605268643807
Validation loss: 2.515549715862453

Epoch: 5| Step: 5
Training loss: 2.149586318383015
Validation loss: 2.521364228380144

Epoch: 5| Step: 6
Training loss: 2.164578152583962
Validation loss: 2.5300936589037293

Epoch: 5| Step: 7
Training loss: 2.3844394820107264
Validation loss: 2.528006331680815

Epoch: 5| Step: 8
Training loss: 1.9822259514868135
Validation loss: 2.5339113734113665

Epoch: 5| Step: 9
Training loss: 1.8710936225779575
Validation loss: 2.549042226244581

Epoch: 5| Step: 10
Training loss: 2.696945005637645
Validation loss: 2.5339016780861816

Epoch: 5| Step: 11
Training loss: 2.8998523148388053
Validation loss: 2.5283285440208054

Epoch: 249| Step: 0
Training loss: 2.4298345883118047
Validation loss: 2.5520826196993913

Epoch: 5| Step: 1
Training loss: 2.1631039035300734
Validation loss: 2.5395956604501673

Epoch: 5| Step: 2
Training loss: 2.3372773375512406
Validation loss: 2.5461457969681027

Epoch: 5| Step: 3
Training loss: 2.6311908561239146
Validation loss: 2.5749598716028745

Epoch: 5| Step: 4
Training loss: 2.0228916448217027
Validation loss: 2.600654462674789

Epoch: 5| Step: 5
Training loss: 2.383456583993248
Validation loss: 2.5848515814926754

Epoch: 5| Step: 6
Training loss: 2.859468719113888
Validation loss: 2.566117071526455

Epoch: 5| Step: 7
Training loss: 1.9609347643586184
Validation loss: 2.5450327788804548

Epoch: 5| Step: 8
Training loss: 2.2271498591374925
Validation loss: 2.515095151462593

Epoch: 5| Step: 9
Training loss: 2.2404549846762367
Validation loss: 2.5164915294422547

Epoch: 5| Step: 10
Training loss: 2.7126040214598204
Validation loss: 2.4980832281939693

Epoch: 5| Step: 11
Training loss: 2.992411871483227
Validation loss: 2.5047531441383466

Epoch: 250| Step: 0
Training loss: 1.8421793567843106
Validation loss: 2.49251566991341

Epoch: 5| Step: 1
Training loss: 2.401100693071626
Validation loss: 2.499738528565409

Epoch: 5| Step: 2
Training loss: 2.2870865453399976
Validation loss: 2.5083002980698184

Epoch: 5| Step: 3
Training loss: 2.9052814797682807
Validation loss: 2.5113288849725492

Epoch: 5| Step: 4
Training loss: 1.9798363874249003
Validation loss: 2.5193466749770073

Epoch: 5| Step: 5
Training loss: 2.222012230435145
Validation loss: 2.499396620893243

Epoch: 5| Step: 6
Training loss: 1.895018650556997
Validation loss: 2.5254688727777226

Epoch: 5| Step: 7
Training loss: 1.999557565389381
Validation loss: 2.49807767871419

Epoch: 5| Step: 8
Training loss: 2.6931507231713763
Validation loss: 2.5179219905067463

Epoch: 5| Step: 9
Training loss: 2.5891437303672897
Validation loss: 2.523777654375707

Epoch: 5| Step: 10
Training loss: 2.522609043240804
Validation loss: 2.5263985444020074

Epoch: 5| Step: 11
Training loss: 3.1952305715638962
Validation loss: 2.556726351768269

Epoch: 251| Step: 0
Training loss: 2.414343265144172
Validation loss: 2.5726200748776615

Epoch: 5| Step: 1
Training loss: 2.046949428923219
Validation loss: 2.5601935435064074

Epoch: 5| Step: 2
Training loss: 1.7839480929927394
Validation loss: 2.568919704026069

Epoch: 5| Step: 3
Training loss: 2.962696046418592
Validation loss: 2.5446711251035987

Epoch: 5| Step: 4
Training loss: 2.3597317861635494
Validation loss: 2.5154777783897417

Epoch: 5| Step: 5
Training loss: 2.229579875356176
Validation loss: 2.513271972809882

Epoch: 5| Step: 6
Training loss: 2.497279403459956
Validation loss: 2.5085761986087505

Epoch: 5| Step: 7
Training loss: 2.2296167673869074
Validation loss: 2.50207072569005

Epoch: 5| Step: 8
Training loss: 2.44671083876453
Validation loss: 2.512604248357688

Epoch: 5| Step: 9
Training loss: 2.7341163730927347
Validation loss: 2.5327646645153523

Epoch: 5| Step: 10
Training loss: 2.2260283247661365
Validation loss: 2.5379584822160997

Epoch: 5| Step: 11
Training loss: 3.447775170379919
Validation loss: 2.52344181072233

Epoch: 252| Step: 0
Training loss: 2.633439258263864
Validation loss: 2.565335592812738

Epoch: 5| Step: 1
Training loss: 2.2974428266482563
Validation loss: 2.5575495639720383

Epoch: 5| Step: 2
Training loss: 2.0124787607908505
Validation loss: 2.5771981162616595

Epoch: 5| Step: 3
Training loss: 2.553371171978024
Validation loss: 2.6071485435675155

Epoch: 5| Step: 4
Training loss: 2.5778588359884433
Validation loss: 2.602098690371453

Epoch: 5| Step: 5
Training loss: 2.801295106959571
Validation loss: 2.6056228584493644

Epoch: 5| Step: 6
Training loss: 2.4400919313778813
Validation loss: 2.6041495030155413

Epoch: 5| Step: 7
Training loss: 2.1047676915083255
Validation loss: 2.5945583868555513

Epoch: 5| Step: 8
Training loss: 2.306143401041972
Validation loss: 2.558467638630259

Epoch: 5| Step: 9
Training loss: 2.3786735232914404
Validation loss: 2.554973096369957

Epoch: 5| Step: 10
Training loss: 2.3533869664651887
Validation loss: 2.543170165070815

Epoch: 5| Step: 11
Training loss: 2.2058289080110534
Validation loss: 2.5318482520100627

Epoch: 253| Step: 0
Training loss: 2.630866443650025
Validation loss: 2.5272421445473845

Epoch: 5| Step: 1
Training loss: 2.3885218525622154
Validation loss: 2.528505128245138

Epoch: 5| Step: 2
Training loss: 2.2086007538108574
Validation loss: 2.5312855094020197

Epoch: 5| Step: 3
Training loss: 1.9574572314660668
Validation loss: 2.532726249818679

Epoch: 5| Step: 4
Training loss: 2.563394529937458
Validation loss: 2.5324903792813305

Epoch: 5| Step: 5
Training loss: 2.302186184019684
Validation loss: 2.533835013390055

Epoch: 5| Step: 6
Training loss: 2.2356250174044896
Validation loss: 2.5376591369450887

Epoch: 5| Step: 7
Training loss: 2.7597181959996906
Validation loss: 2.538724291846747

Epoch: 5| Step: 8
Training loss: 3.0444534460183728
Validation loss: 2.5384805100350043

Epoch: 5| Step: 9
Training loss: 2.6413142642116405
Validation loss: 2.531799880688206

Epoch: 5| Step: 10
Training loss: 2.5132317383703304
Validation loss: 2.5263186207907586

Epoch: 5| Step: 11
Training loss: 2.5473186831951127
Validation loss: 2.509236186152012

Epoch: 254| Step: 0
Training loss: 2.352912489982651
Validation loss: 2.517882232805112

Epoch: 5| Step: 1
Training loss: 2.3794187297565164
Validation loss: 2.5113428347713502

Epoch: 5| Step: 2
Training loss: 2.1901048682200743
Validation loss: 2.501070385510784

Epoch: 5| Step: 3
Training loss: 3.0004003575528095
Validation loss: 2.5162956260060487

Epoch: 5| Step: 4
Training loss: 2.2857195820065854
Validation loss: 2.5019968442449736

Epoch: 5| Step: 5
Training loss: 2.6206924881764033
Validation loss: 2.509693490066635

Epoch: 5| Step: 6
Training loss: 2.6875830349181915
Validation loss: 2.514256308391256

Epoch: 5| Step: 7
Training loss: 1.8305919988333357
Validation loss: 2.5270841525129693

Epoch: 5| Step: 8
Training loss: 2.3407311897992837
Validation loss: 2.5359453051923504

Epoch: 5| Step: 9
Training loss: 2.404638964861579
Validation loss: 2.544764867405437

Epoch: 5| Step: 10
Training loss: 2.2443533083643747
Validation loss: 2.53500946905285

Epoch: 5| Step: 11
Training loss: 1.7233192050485058
Validation loss: 2.564022313712957

Epoch: 255| Step: 0
Training loss: 2.2817676949640906
Validation loss: 2.562600250143975

Epoch: 5| Step: 1
Training loss: 2.251933220992915
Validation loss: 2.55420121397226

Epoch: 5| Step: 2
Training loss: 2.3919716825499524
Validation loss: 2.5565886816426437

Epoch: 5| Step: 3
Training loss: 2.5679295401369386
Validation loss: 2.556812957636316

Epoch: 5| Step: 4
Training loss: 2.572161070625669
Validation loss: 2.526310901779158

Epoch: 5| Step: 5
Training loss: 2.2566829614694277
Validation loss: 2.548166450760298

Epoch: 5| Step: 6
Training loss: 2.598030419040173
Validation loss: 2.5417538390818644

Epoch: 5| Step: 7
Training loss: 2.3818691700223646
Validation loss: 2.5290878386261024

Epoch: 5| Step: 8
Training loss: 2.025095139140579
Validation loss: 2.533963173968257

Epoch: 5| Step: 9
Training loss: 2.777456632699618
Validation loss: 2.506692222177913

Epoch: 5| Step: 10
Training loss: 2.203550689910575
Validation loss: 2.5326828217613313

Epoch: 5| Step: 11
Training loss: 2.6103900145935657
Validation loss: 2.521494819349607

Epoch: 256| Step: 0
Training loss: 2.0216039649861095
Validation loss: 2.5314931242551384

Epoch: 5| Step: 1
Training loss: 2.6159427244758318
Validation loss: 2.518746376508279

Epoch: 5| Step: 2
Training loss: 2.3380899878120984
Validation loss: 2.5123678568034924

Epoch: 5| Step: 3
Training loss: 2.179285155868405
Validation loss: 2.5383595046056993

Epoch: 5| Step: 4
Training loss: 2.434317638813428
Validation loss: 2.5405312657318353

Epoch: 5| Step: 5
Training loss: 2.3685643186237355
Validation loss: 2.5681352060634617

Epoch: 5| Step: 6
Training loss: 2.309114194191445
Validation loss: 2.5673808662152764

Epoch: 5| Step: 7
Training loss: 2.6064089975867977
Validation loss: 2.5643799483973373

Epoch: 5| Step: 8
Training loss: 2.224895723449665
Validation loss: 2.5349868890259004

Epoch: 5| Step: 9
Training loss: 2.6701747548494925
Validation loss: 2.5416709764371093

Epoch: 5| Step: 10
Training loss: 2.205863603250441
Validation loss: 2.5440058236665264

Epoch: 5| Step: 11
Training loss: 2.5513327500391085
Validation loss: 2.5367181900724

Epoch: 257| Step: 0
Training loss: 2.314197844900958
Validation loss: 2.537509366896681

Epoch: 5| Step: 1
Training loss: 2.6638240763452825
Validation loss: 2.549823022136323

Epoch: 5| Step: 2
Training loss: 2.440019332621546
Validation loss: 2.549481437040467

Epoch: 5| Step: 3
Training loss: 2.206246835852237
Validation loss: 2.528826239947161

Epoch: 5| Step: 4
Training loss: 2.0902070772444867
Validation loss: 2.546972881378706

Epoch: 5| Step: 5
Training loss: 2.0390033347947716
Validation loss: 2.5340811590838346

Epoch: 5| Step: 6
Training loss: 1.4926298594250735
Validation loss: 2.5217869166134026

Epoch: 5| Step: 7
Training loss: 2.551320508237723
Validation loss: 2.515195667758728

Epoch: 5| Step: 8
Training loss: 2.922496382790212
Validation loss: 2.524986996003459

Epoch: 5| Step: 9
Training loss: 2.3285991006756572
Validation loss: 2.5197182834805525

Epoch: 5| Step: 10
Training loss: 2.6214272845433255
Validation loss: 2.52125390671727

Epoch: 5| Step: 11
Training loss: 2.5477429180733715
Validation loss: 2.5121966787803984

Epoch: 258| Step: 0
Training loss: 2.468594027070774
Validation loss: 2.5107414040848033

Epoch: 5| Step: 1
Training loss: 2.195726023108231
Validation loss: 2.5154615175346327

Epoch: 5| Step: 2
Training loss: 2.48072700657905
Validation loss: 2.5178162092510044

Epoch: 5| Step: 3
Training loss: 2.3322211975746434
Validation loss: 2.520301133453601

Epoch: 5| Step: 4
Training loss: 2.789958905973614
Validation loss: 2.527211985231443

Epoch: 5| Step: 5
Training loss: 1.6276759342901341
Validation loss: 2.5233315523343705

Epoch: 5| Step: 6
Training loss: 2.285312953694895
Validation loss: 2.5256960734022784

Epoch: 5| Step: 7
Training loss: 2.506530053528328
Validation loss: 2.511616033863448

Epoch: 5| Step: 8
Training loss: 2.403604018707649
Validation loss: 2.5144441016671726

Epoch: 5| Step: 9
Training loss: 2.1667514442094844
Validation loss: 2.5094385272860804

Epoch: 5| Step: 10
Training loss: 2.7301772675583686
Validation loss: 2.5116920983940862

Epoch: 5| Step: 11
Training loss: 2.1105852964110485
Validation loss: 2.5338749794858324

Epoch: 259| Step: 0
Training loss: 2.839958765778264
Validation loss: 2.542349209103903

Epoch: 5| Step: 1
Training loss: 2.7958500012177123
Validation loss: 2.526145446585617

Epoch: 5| Step: 2
Training loss: 2.33089185320981
Validation loss: 2.5359910042274283

Epoch: 5| Step: 3
Training loss: 2.1188073524657196
Validation loss: 2.5477354160405867

Epoch: 5| Step: 4
Training loss: 2.317211918049498
Validation loss: 2.5458030121886783

Epoch: 5| Step: 5
Training loss: 2.581980279139489
Validation loss: 2.553531042780633

Epoch: 5| Step: 6
Training loss: 1.9436044203036535
Validation loss: 2.5492530069125525

Epoch: 5| Step: 7
Training loss: 2.289359324092033
Validation loss: 2.546514064278105

Epoch: 5| Step: 8
Training loss: 2.247342765832821
Validation loss: 2.529547526975823

Epoch: 5| Step: 9
Training loss: 2.154089024495427
Validation loss: 2.5369815378185185

Epoch: 5| Step: 10
Training loss: 1.5239676311189296
Validation loss: 2.513219133106892

Epoch: 5| Step: 11
Training loss: 3.1894484903613756
Validation loss: 2.514751021875996

Epoch: 260| Step: 0
Training loss: 1.9699116261987386
Validation loss: 2.5084994752701006

Epoch: 5| Step: 1
Training loss: 2.3481732146791203
Validation loss: 2.5197958361268027

Epoch: 5| Step: 2
Training loss: 2.3902341236376827
Validation loss: 2.491875775492337

Epoch: 5| Step: 3
Training loss: 2.132374701089076
Validation loss: 2.492259914418397

Epoch: 5| Step: 4
Training loss: 2.5215811973083757
Validation loss: 2.507252429601788

Epoch: 5| Step: 5
Training loss: 2.5309473492569325
Validation loss: 2.4956962694818383

Epoch: 5| Step: 6
Training loss: 2.4429424854180555
Validation loss: 2.5036818532228775

Epoch: 5| Step: 7
Training loss: 2.2012377291613014
Validation loss: 2.5140466339072205

Epoch: 5| Step: 8
Training loss: 2.4128695506354645
Validation loss: 2.5258516233602797

Epoch: 5| Step: 9
Training loss: 2.5624041655693293
Validation loss: 2.506189571834902

Epoch: 5| Step: 10
Training loss: 2.2294819213791235
Validation loss: 2.5292898406295463

Epoch: 5| Step: 11
Training loss: 2.6228648993908443
Validation loss: 2.5093266876371203

Epoch: 261| Step: 0
Training loss: 2.3375035780609
Validation loss: 2.5315575118534306

Epoch: 5| Step: 1
Training loss: 2.8076034306728186
Validation loss: 2.5389352737243085

Epoch: 5| Step: 2
Training loss: 1.840132575234648
Validation loss: 2.52717281590908

Epoch: 5| Step: 3
Training loss: 2.3884637575104968
Validation loss: 2.5429471039912848

Epoch: 5| Step: 4
Training loss: 2.301510240384926
Validation loss: 2.5581578278891417

Epoch: 5| Step: 5
Training loss: 2.1980877368275142
Validation loss: 2.539009261918074

Epoch: 5| Step: 6
Training loss: 2.686956306286824
Validation loss: 2.5252679537870164

Epoch: 5| Step: 7
Training loss: 2.413610323435827
Validation loss: 2.510668074548789

Epoch: 5| Step: 8
Training loss: 2.6131306192866384
Validation loss: 2.537291342357345

Epoch: 5| Step: 9
Training loss: 1.7889683461296848
Validation loss: 2.513099690255873

Epoch: 5| Step: 10
Training loss: 2.054770464364418
Validation loss: 2.520129278215523

Epoch: 5| Step: 11
Training loss: 3.544932366978275
Validation loss: 2.5339678313795964

Epoch: 262| Step: 0
Training loss: 2.7715515601716123
Validation loss: 2.5336412233823236

Epoch: 5| Step: 1
Training loss: 1.9096990556949396
Validation loss: 2.5367445493779726

Epoch: 5| Step: 2
Training loss: 2.216071970717701
Validation loss: 2.5378363715219785

Epoch: 5| Step: 3
Training loss: 2.3006331650335934
Validation loss: 2.5500070858526573

Epoch: 5| Step: 4
Training loss: 1.5687446928504445
Validation loss: 2.5527935843300624

Epoch: 5| Step: 5
Training loss: 2.5125890383909355
Validation loss: 2.5636319978607847

Epoch: 5| Step: 6
Training loss: 2.3840336906561523
Validation loss: 2.548494966639822

Epoch: 5| Step: 7
Training loss: 2.5524897556678474
Validation loss: 2.5596262821276827

Epoch: 5| Step: 8
Training loss: 2.649168560662982
Validation loss: 2.5542366337696065

Epoch: 5| Step: 9
Training loss: 2.143140887593419
Validation loss: 2.557653999039425

Epoch: 5| Step: 10
Training loss: 2.6194197970332285
Validation loss: 2.556797377344383

Epoch: 5| Step: 11
Training loss: 1.8985289877365583
Validation loss: 2.5457064984615183

Epoch: 263| Step: 0
Training loss: 2.0258688440300547
Validation loss: 2.5528544247181375

Epoch: 5| Step: 1
Training loss: 2.104845170513379
Validation loss: 2.5666974317164195

Epoch: 5| Step: 2
Training loss: 2.382378410446998
Validation loss: 2.5729218005439307

Epoch: 5| Step: 3
Training loss: 2.5788580574772304
Validation loss: 2.5628258257636016

Epoch: 5| Step: 4
Training loss: 2.251051445168494
Validation loss: 2.5586005989740155

Epoch: 5| Step: 5
Training loss: 2.3464348229119487
Validation loss: 2.5559647465472595

Epoch: 5| Step: 6
Training loss: 2.40513624697565
Validation loss: 2.5364663413128303

Epoch: 5| Step: 7
Training loss: 2.421055612082142
Validation loss: 2.5362149160298317

Epoch: 5| Step: 8
Training loss: 1.911511086992804
Validation loss: 2.535559592271192

Epoch: 5| Step: 9
Training loss: 2.814609414710885
Validation loss: 2.536151065792555

Epoch: 5| Step: 10
Training loss: 2.5168553536356173
Validation loss: 2.5367626396684613

Epoch: 5| Step: 11
Training loss: 1.9026034136823837
Validation loss: 2.5351562068960756

Epoch: 264| Step: 0
Training loss: 2.220658891517467
Validation loss: 2.53631908783898

Epoch: 5| Step: 1
Training loss: 2.516079974051209
Validation loss: 2.522599048503508

Epoch: 5| Step: 2
Training loss: 2.241613972646062
Validation loss: 2.525756010965218

Epoch: 5| Step: 3
Training loss: 2.1867396531872587
Validation loss: 2.5354441208451393

Epoch: 5| Step: 4
Training loss: 2.0806635216458274
Validation loss: 2.523157249848077

Epoch: 5| Step: 5
Training loss: 2.363751930465999
Validation loss: 2.5307053973038243

Epoch: 5| Step: 6
Training loss: 2.9554472980734157
Validation loss: 2.531013375670339

Epoch: 5| Step: 7
Training loss: 2.021163664361693
Validation loss: 2.5356552267776427

Epoch: 5| Step: 8
Training loss: 2.4535438520183117
Validation loss: 2.5337629951993184

Epoch: 5| Step: 9
Training loss: 2.796972773883482
Validation loss: 2.5355176447651395

Epoch: 5| Step: 10
Training loss: 2.1180554402113363
Validation loss: 2.5325648460295413

Epoch: 5| Step: 11
Training loss: 2.4451239272462826
Validation loss: 2.5522001680891395

Epoch: 265| Step: 0
Training loss: 2.0103452864181763
Validation loss: 2.544713903533198

Epoch: 5| Step: 1
Training loss: 2.639059833098397
Validation loss: 2.539352427826769

Epoch: 5| Step: 2
Training loss: 2.409494268926267
Validation loss: 2.5285584067655154

Epoch: 5| Step: 3
Training loss: 2.729638318566987
Validation loss: 2.5391808115464296

Epoch: 5| Step: 4
Training loss: 2.708655812554
Validation loss: 2.516547683456828

Epoch: 5| Step: 5
Training loss: 2.120488370209122
Validation loss: 2.5489529439050473

Epoch: 5| Step: 6
Training loss: 1.8162254981890924
Validation loss: 2.532626260620725

Epoch: 5| Step: 7
Training loss: 1.6285311406513439
Validation loss: 2.5245659886383605

Epoch: 5| Step: 8
Training loss: 2.4993086812711516
Validation loss: 2.5281066450624463

Epoch: 5| Step: 9
Training loss: 2.7781711638898603
Validation loss: 2.528036334161209

Epoch: 5| Step: 10
Training loss: 1.9150155322289109
Validation loss: 2.520929670988856

Epoch: 5| Step: 11
Training loss: 2.709244697015776
Validation loss: 2.5263521154440944

Epoch: 266| Step: 0
Training loss: 2.3691243220662273
Validation loss: 2.5256661021207765

Epoch: 5| Step: 1
Training loss: 2.1969858762888745
Validation loss: 2.5222521774424256

Epoch: 5| Step: 2
Training loss: 2.147513450962347
Validation loss: 2.528835145494568

Epoch: 5| Step: 3
Training loss: 2.9929489879906668
Validation loss: 2.5310156442919096

Epoch: 5| Step: 4
Training loss: 2.7156714186306274
Validation loss: 2.5168902727814157

Epoch: 5| Step: 5
Training loss: 1.7474255016857372
Validation loss: 2.5286199265215945

Epoch: 5| Step: 6
Training loss: 2.086618605152638
Validation loss: 2.54720704034168

Epoch: 5| Step: 7
Training loss: 1.895969246264818
Validation loss: 2.524348351455269

Epoch: 5| Step: 8
Training loss: 2.652470042125139
Validation loss: 2.534647909726963

Epoch: 5| Step: 9
Training loss: 2.1704683208073825
Validation loss: 2.5437633993622253

Epoch: 5| Step: 10
Training loss: 2.4466638700519487
Validation loss: 2.539107270457402

Epoch: 5| Step: 11
Training loss: 2.7106919411029624
Validation loss: 2.5387310105124947

Epoch: 267| Step: 0
Training loss: 2.080332795076077
Validation loss: 2.5260641562388177

Epoch: 5| Step: 1
Training loss: 2.4247822693285066
Validation loss: 2.5393807901273715

Epoch: 5| Step: 2
Training loss: 1.669226000615732
Validation loss: 2.5417257846669186

Epoch: 5| Step: 3
Training loss: 2.666545437997728
Validation loss: 2.536560958624348

Epoch: 5| Step: 4
Training loss: 2.380926167252969
Validation loss: 2.5492683624823815

Epoch: 5| Step: 5
Training loss: 1.903454967137433
Validation loss: 2.5600141353701216

Epoch: 5| Step: 6
Training loss: 2.140519411030655
Validation loss: 2.5480961046848734

Epoch: 5| Step: 7
Training loss: 2.4545084945708897
Validation loss: 2.545534151502441

Epoch: 5| Step: 8
Training loss: 2.7913821535647876
Validation loss: 2.562159329541106

Epoch: 5| Step: 9
Training loss: 2.26902988693574
Validation loss: 2.5791148048531793

Epoch: 5| Step: 10
Training loss: 2.3851947820045654
Validation loss: 2.5573028103235367

Epoch: 5| Step: 11
Training loss: 2.27891630184244
Validation loss: 2.565207179497392

Epoch: 268| Step: 0
Training loss: 2.462791301712058
Validation loss: 2.586686555029747

Epoch: 5| Step: 1
Training loss: 2.592938583488182
Validation loss: 2.598258168173454

Epoch: 5| Step: 2
Training loss: 2.7167102572292503
Validation loss: 2.596529979868683

Epoch: 5| Step: 3
Training loss: 2.337791192179841
Validation loss: 2.622753374007566

Epoch: 5| Step: 4
Training loss: 2.3190675214747274
Validation loss: 2.5867034837922667

Epoch: 5| Step: 5
Training loss: 2.02982526844612
Validation loss: 2.574882718844301

Epoch: 5| Step: 6
Training loss: 1.7946096320963953
Validation loss: 2.5350566113625868

Epoch: 5| Step: 7
Training loss: 2.254116002310835
Validation loss: 2.5427083489088416

Epoch: 5| Step: 8
Training loss: 2.0998076305381197
Validation loss: 2.515701166425471

Epoch: 5| Step: 9
Training loss: 2.3060254367573494
Validation loss: 2.5155499409600117

Epoch: 5| Step: 10
Training loss: 2.085642132938787
Validation loss: 2.520029508623549

Epoch: 5| Step: 11
Training loss: 4.039136638735679
Validation loss: 2.505783000603775

Epoch: 269| Step: 0
Training loss: 2.3027778601969158
Validation loss: 2.512124332887051

Epoch: 5| Step: 1
Training loss: 2.338009428927137
Validation loss: 2.512415427908597

Epoch: 5| Step: 2
Training loss: 2.0015382811415936
Validation loss: 2.518494515441624

Epoch: 5| Step: 3
Training loss: 2.5359528734419254
Validation loss: 2.516480401127087

Epoch: 5| Step: 4
Training loss: 2.9488200139186698
Validation loss: 2.5090583566655273

Epoch: 5| Step: 5
Training loss: 2.948948566111123
Validation loss: 2.513823110698719

Epoch: 5| Step: 6
Training loss: 2.3591585375790514
Validation loss: 2.530230383517121

Epoch: 5| Step: 7
Training loss: 2.5214161054734534
Validation loss: 2.5402284498056162

Epoch: 5| Step: 8
Training loss: 2.4180024719351856
Validation loss: 2.5816700923725384

Epoch: 5| Step: 9
Training loss: 1.6216767882723542
Validation loss: 2.6220129270803536

Epoch: 5| Step: 10
Training loss: 1.796224194303476
Validation loss: 2.64027103331461

Epoch: 5| Step: 11
Training loss: 2.1612259215143808
Validation loss: 2.613831333413413

Epoch: 270| Step: 0
Training loss: 2.4849133178206677
Validation loss: 2.6025568805821804

Epoch: 5| Step: 1
Training loss: 2.47958228423474
Validation loss: 2.6069301184362956

Epoch: 5| Step: 2
Training loss: 2.069713693067414
Validation loss: 2.6048751757339326

Epoch: 5| Step: 3
Training loss: 2.441640125516649
Validation loss: 2.582201287946315

Epoch: 5| Step: 4
Training loss: 2.0422370139951402
Validation loss: 2.5519604121338246

Epoch: 5| Step: 5
Training loss: 2.4468374159302733
Validation loss: 2.562945625665823

Epoch: 5| Step: 6
Training loss: 2.717070488277594
Validation loss: 2.544916538907261

Epoch: 5| Step: 7
Training loss: 1.8612007345695365
Validation loss: 2.538285751863219

Epoch: 5| Step: 8
Training loss: 2.999230286083408
Validation loss: 2.5447322748482994

Epoch: 5| Step: 9
Training loss: 2.042150271484261
Validation loss: 2.5369908767917577

Epoch: 5| Step: 10
Training loss: 1.8015650700387877
Validation loss: 2.5449421301909325

Epoch: 5| Step: 11
Training loss: 2.19877161931637
Validation loss: 2.542005119041703

Epoch: 271| Step: 0
Training loss: 1.7429274283537488
Validation loss: 2.5412028989165574

Epoch: 5| Step: 1
Training loss: 1.7758741402319536
Validation loss: 2.5450562065702886

Epoch: 5| Step: 2
Training loss: 3.1609217750358054
Validation loss: 2.5336971737461558

Epoch: 5| Step: 3
Training loss: 2.633493126073137
Validation loss: 2.5440715267919516

Epoch: 5| Step: 4
Training loss: 2.245321071239403
Validation loss: 2.564445102405032

Epoch: 5| Step: 5
Training loss: 1.9449506675916575
Validation loss: 2.5656263810656257

Epoch: 5| Step: 6
Training loss: 2.8025310588820482
Validation loss: 2.5404192346546655

Epoch: 5| Step: 7
Training loss: 1.2918584383865857
Validation loss: 2.556328214554042

Epoch: 5| Step: 8
Training loss: 1.878970329476747
Validation loss: 2.5365860663386046

Epoch: 5| Step: 9
Training loss: 2.4280675657885684
Validation loss: 2.5295337620075737

Epoch: 5| Step: 10
Training loss: 2.9644530670934075
Validation loss: 2.5348507619225975

Epoch: 5| Step: 11
Training loss: 1.5066242180530047
Validation loss: 2.5335131367027324

Epoch: 272| Step: 0
Training loss: 2.9471138623741453
Validation loss: 2.530904711025676

Epoch: 5| Step: 1
Training loss: 1.6579524952763325
Validation loss: 2.553400926998378

Epoch: 5| Step: 2
Training loss: 2.0624332995177608
Validation loss: 2.579527671787298

Epoch: 5| Step: 3
Training loss: 2.7676343784255213
Validation loss: 2.579621117868195

Epoch: 5| Step: 4
Training loss: 2.389917206547204
Validation loss: 2.5521054747003777

Epoch: 5| Step: 5
Training loss: 2.627861552239394
Validation loss: 2.5623012481153826

Epoch: 5| Step: 6
Training loss: 1.9515411668990446
Validation loss: 2.5368579292553184

Epoch: 5| Step: 7
Training loss: 1.8066653028297672
Validation loss: 2.533420007382656

Epoch: 5| Step: 8
Training loss: 1.9760630583489205
Validation loss: 2.532835758015371

Epoch: 5| Step: 9
Training loss: 2.681410610839399
Validation loss: 2.518563057435786

Epoch: 5| Step: 10
Training loss: 2.5850113884326342
Validation loss: 2.518386979205986

Epoch: 5| Step: 11
Training loss: 2.05578769484221
Validation loss: 2.5189560383911465

Epoch: 273| Step: 0
Training loss: 2.888653056797224
Validation loss: 2.519559590971296

Epoch: 5| Step: 1
Training loss: 2.7103799471956695
Validation loss: 2.5286679068851554

Epoch: 5| Step: 2
Training loss: 2.5037284704645386
Validation loss: 2.5235368730723167

Epoch: 5| Step: 3
Training loss: 2.390177167446938
Validation loss: 2.5231846937656086

Epoch: 5| Step: 4
Training loss: 2.033423566640061
Validation loss: 2.524247329882277

Epoch: 5| Step: 5
Training loss: 1.9160474799849405
Validation loss: 2.516682597977274

Epoch: 5| Step: 6
Training loss: 1.6779799897747871
Validation loss: 2.5226369912775017

Epoch: 5| Step: 7
Training loss: 1.90790342322032
Validation loss: 2.523876266205351

Epoch: 5| Step: 8
Training loss: 2.343809915412549
Validation loss: 2.5357842473299868

Epoch: 5| Step: 9
Training loss: 1.827699921575316
Validation loss: 2.529915151549145

Epoch: 5| Step: 10
Training loss: 2.8501051431635647
Validation loss: 2.5537435399460957

Epoch: 5| Step: 11
Training loss: 2.659740712428306
Validation loss: 2.5663317161973973

Epoch: 274| Step: 0
Training loss: 2.624221232107294
Validation loss: 2.555715212263331

Epoch: 5| Step: 1
Training loss: 2.796455309189683
Validation loss: 2.564833404399307

Epoch: 5| Step: 2
Training loss: 2.395013707705076
Validation loss: 2.544082237654096

Epoch: 5| Step: 3
Training loss: 2.0765741053731266
Validation loss: 2.5417204340474644

Epoch: 5| Step: 4
Training loss: 1.8091350758712856
Validation loss: 2.553834152409839

Epoch: 5| Step: 5
Training loss: 2.6620336061415624
Validation loss: 2.5364221626433836

Epoch: 5| Step: 6
Training loss: 1.6468286120488582
Validation loss: 2.5203985254340413

Epoch: 5| Step: 7
Training loss: 2.4852422004426127
Validation loss: 2.5245462703948593

Epoch: 5| Step: 8
Training loss: 2.662462576303157
Validation loss: 2.5317892434249245

Epoch: 5| Step: 9
Training loss: 1.577950496279042
Validation loss: 2.522078078049317

Epoch: 5| Step: 10
Training loss: 2.427227970992619
Validation loss: 2.527619890695604

Epoch: 5| Step: 11
Training loss: 1.9744348345097915
Validation loss: 2.5198722943677585

Epoch: 275| Step: 0
Training loss: 2.8795504839088832
Validation loss: 2.5243256957416587

Epoch: 5| Step: 1
Training loss: 1.8312026632915377
Validation loss: 2.545597750777582

Epoch: 5| Step: 2
Training loss: 2.1682104577393524
Validation loss: 2.547313239027907

Epoch: 5| Step: 3
Training loss: 2.505524158727479
Validation loss: 2.566302378200932

Epoch: 5| Step: 4
Training loss: 2.501083901994184
Validation loss: 2.57467407305785

Epoch: 5| Step: 5
Training loss: 2.0291466019387125
Validation loss: 2.5782668180666515

Epoch: 5| Step: 6
Training loss: 1.7618037370368227
Validation loss: 2.56424843057646

Epoch: 5| Step: 7
Training loss: 2.5911271035493506
Validation loss: 2.5566660289896235

Epoch: 5| Step: 8
Training loss: 2.3040142563074175
Validation loss: 2.5647152497745727

Epoch: 5| Step: 9
Training loss: 2.399552772496493
Validation loss: 2.5785428604423615

Epoch: 5| Step: 10
Training loss: 2.021863759877772
Validation loss: 2.5543259025480416

Epoch: 5| Step: 11
Training loss: 2.792422367884927
Validation loss: 2.551583468212241

Epoch: 276| Step: 0
Training loss: 2.7906599839293467
Validation loss: 2.516188607746883

Epoch: 5| Step: 1
Training loss: 2.7461449739065857
Validation loss: 2.518364577546645

Epoch: 5| Step: 2
Training loss: 2.3740913259009777
Validation loss: 2.5107542789681943

Epoch: 5| Step: 3
Training loss: 2.4358741032619355
Validation loss: 2.5144743884447203

Epoch: 5| Step: 4
Training loss: 2.1787472682986095
Validation loss: 2.516951864600437

Epoch: 5| Step: 5
Training loss: 2.318648952534189
Validation loss: 2.517116250739219

Epoch: 5| Step: 6
Training loss: 2.0762772914647614
Validation loss: 2.5268488718336792

Epoch: 5| Step: 7
Training loss: 2.332823856681269
Validation loss: 2.5148045108100474

Epoch: 5| Step: 8
Training loss: 2.7940186987915414
Validation loss: 2.5236519603319225

Epoch: 5| Step: 9
Training loss: 2.014159623579475
Validation loss: 2.516209264003232

Epoch: 5| Step: 10
Training loss: 2.126042390854703
Validation loss: 2.5206388064800165

Epoch: 5| Step: 11
Training loss: 1.4478996422221893
Validation loss: 2.526218893119795

Epoch: 277| Step: 0
Training loss: 2.428411805092185
Validation loss: 2.522766431503381

Epoch: 5| Step: 1
Training loss: 2.1089214826413736
Validation loss: 2.528386710051316

Epoch: 5| Step: 2
Training loss: 2.242857510616144
Validation loss: 2.5809565013194202

Epoch: 5| Step: 3
Training loss: 2.5494232535278765
Validation loss: 2.5899301037936664

Epoch: 5| Step: 4
Training loss: 2.74047615577919
Validation loss: 2.610124548885383

Epoch: 5| Step: 5
Training loss: 2.194213374831525
Validation loss: 2.6742008154762966

Epoch: 5| Step: 6
Training loss: 2.52435191684869
Validation loss: 2.6666352610427535

Epoch: 5| Step: 7
Training loss: 2.2141575094982913
Validation loss: 2.667238950167952

Epoch: 5| Step: 8
Training loss: 2.4024927108606438
Validation loss: 2.60908895816867

Epoch: 5| Step: 9
Training loss: 2.0750461803031337
Validation loss: 2.5827235391738514

Epoch: 5| Step: 10
Training loss: 1.797864691144012
Validation loss: 2.531048892316982

Epoch: 5| Step: 11
Training loss: 2.897533347700695
Validation loss: 2.5066234625883266

Epoch: 278| Step: 0
Training loss: 1.8919083204255718
Validation loss: 2.5283692179488484

Epoch: 5| Step: 1
Training loss: 2.636415817378128
Validation loss: 2.5190476734649585

Epoch: 5| Step: 2
Training loss: 2.359735524503852
Validation loss: 2.509391275830532

Epoch: 5| Step: 3
Training loss: 2.64362205754915
Validation loss: 2.5187362638971384

Epoch: 5| Step: 4
Training loss: 2.1644019125824747
Validation loss: 2.5257941226334255

Epoch: 5| Step: 5
Training loss: 2.0332121541961494
Validation loss: 2.5163832680711637

Epoch: 5| Step: 6
Training loss: 2.708631606672281
Validation loss: 2.530449246092604

Epoch: 5| Step: 7
Training loss: 2.6435314187307113
Validation loss: 2.5224429592782163

Epoch: 5| Step: 8
Training loss: 2.033817136250768
Validation loss: 2.5148613720462407

Epoch: 5| Step: 9
Training loss: 2.090850758587001
Validation loss: 2.5241980218582314

Epoch: 5| Step: 10
Training loss: 2.229428451235259
Validation loss: 2.529502642299692

Epoch: 5| Step: 11
Training loss: 2.6879517818678833
Validation loss: 2.5391339688146206

Epoch: 279| Step: 0
Training loss: 2.159057668950226
Validation loss: 2.5449005344654476

Epoch: 5| Step: 1
Training loss: 2.3028004307611143
Validation loss: 2.562068398887529

Epoch: 5| Step: 2
Training loss: 1.848404521115884
Validation loss: 2.5784859992162446

Epoch: 5| Step: 3
Training loss: 2.501884894293099
Validation loss: 2.573484015879165

Epoch: 5| Step: 4
Training loss: 2.2285778896182236
Validation loss: 2.56512337821053

Epoch: 5| Step: 5
Training loss: 2.759298556382472
Validation loss: 2.598441236244401

Epoch: 5| Step: 6
Training loss: 1.6394036242610777
Validation loss: 2.6290345484208486

Epoch: 5| Step: 7
Training loss: 2.4571548697662737
Validation loss: 2.6311617807020076

Epoch: 5| Step: 8
Training loss: 2.6759874250518525
Validation loss: 2.605145588306962

Epoch: 5| Step: 9
Training loss: 2.207714413936322
Validation loss: 2.595197859499486

Epoch: 5| Step: 10
Training loss: 2.374467288054012
Validation loss: 2.5593738075380883

Epoch: 5| Step: 11
Training loss: 1.7898946767327921
Validation loss: 2.5448625723076628

Epoch: 280| Step: 0
Training loss: 1.9881607348898047
Validation loss: 2.522876366110454

Epoch: 5| Step: 1
Training loss: 2.0327919609328213
Validation loss: 2.52899889080513

Epoch: 5| Step: 2
Training loss: 2.3024508728021713
Validation loss: 2.528025861823108

Epoch: 5| Step: 3
Training loss: 2.361984931464908
Validation loss: 2.520711849900425

Epoch: 5| Step: 4
Training loss: 1.92388730030739
Validation loss: 2.5256948973705087

Epoch: 5| Step: 5
Training loss: 2.416601530929241
Validation loss: 2.525074316731457

Epoch: 5| Step: 6
Training loss: 2.2980588599785214
Validation loss: 2.5291102670689694

Epoch: 5| Step: 7
Training loss: 2.4702786891488557
Validation loss: 2.5253987500265813

Epoch: 5| Step: 8
Training loss: 2.6821877073616553
Validation loss: 2.520329477592371

Epoch: 5| Step: 9
Training loss: 1.9899273786281109
Validation loss: 2.5373264285482833

Epoch: 5| Step: 10
Training loss: 2.9850076534102836
Validation loss: 2.538231718645201

Epoch: 5| Step: 11
Training loss: 3.772532984524288
Validation loss: 2.5359476790867292

Epoch: 281| Step: 0
Training loss: 2.732054243244633
Validation loss: 2.530161863119892

Epoch: 5| Step: 1
Training loss: 2.0477280331647654
Validation loss: 2.533211328889753

Epoch: 5| Step: 2
Training loss: 2.5174889145486796
Validation loss: 2.5301499468517723

Epoch: 5| Step: 3
Training loss: 1.7596620450882592
Validation loss: 2.518994122799565

Epoch: 5| Step: 4
Training loss: 1.9615869548464964
Validation loss: 2.536137532516355

Epoch: 5| Step: 5
Training loss: 2.9137263781728633
Validation loss: 2.538725634016107

Epoch: 5| Step: 6
Training loss: 2.0680406305022054
Validation loss: 2.553594835801617

Epoch: 5| Step: 7
Training loss: 2.2076410852019803
Validation loss: 2.5490574564261697

Epoch: 5| Step: 8
Training loss: 2.328701895085396
Validation loss: 2.5305356520826043

Epoch: 5| Step: 9
Training loss: 2.27650658742386
Validation loss: 2.549574305025929

Epoch: 5| Step: 10
Training loss: 2.213205205740047
Validation loss: 2.5268018869809654

Epoch: 5| Step: 11
Training loss: 3.524545973087349
Validation loss: 2.540458174323604

Epoch: 282| Step: 0
Training loss: 1.8198248083033322
Validation loss: 2.5479481236368464

Epoch: 5| Step: 1
Training loss: 2.4228140702396903
Validation loss: 2.561964684683473

Epoch: 5| Step: 2
Training loss: 2.368162552351323
Validation loss: 2.5686492911008623

Epoch: 5| Step: 3
Training loss: 2.199749680930183
Validation loss: 2.579850673155265

Epoch: 5| Step: 4
Training loss: 2.472330514867228
Validation loss: 2.571405331109878

Epoch: 5| Step: 5
Training loss: 1.6813073442741022
Validation loss: 2.5704893188568767

Epoch: 5| Step: 6
Training loss: 1.9462276895036894
Validation loss: 2.5536500222093115

Epoch: 5| Step: 7
Training loss: 2.4402903699854552
Validation loss: 2.543816947993294

Epoch: 5| Step: 8
Training loss: 2.5644003171923853
Validation loss: 2.5642274755838854

Epoch: 5| Step: 9
Training loss: 2.0573404684596133
Validation loss: 2.5711594049197837

Epoch: 5| Step: 10
Training loss: 3.153889594932956
Validation loss: 2.577012505993284

Epoch: 5| Step: 11
Training loss: 1.784080130937155
Validation loss: 2.5667542483406485

Epoch: 283| Step: 0
Training loss: 1.67829588973633
Validation loss: 2.560829664919363

Epoch: 5| Step: 1
Training loss: 2.3861267054794157
Validation loss: 2.565009253177225

Epoch: 5| Step: 2
Training loss: 2.011482654531831
Validation loss: 2.55661918023045

Epoch: 5| Step: 3
Training loss: 2.950381018391781
Validation loss: 2.5527564867225085

Epoch: 5| Step: 4
Training loss: 1.6325695824508
Validation loss: 2.5729213796925077

Epoch: 5| Step: 5
Training loss: 2.3071320581162333
Validation loss: 2.547375709757344

Epoch: 5| Step: 6
Training loss: 3.048531419489193
Validation loss: 2.5463899997045116

Epoch: 5| Step: 7
Training loss: 1.7798121488222332
Validation loss: 2.535939858154028

Epoch: 5| Step: 8
Training loss: 2.0601758867007125
Validation loss: 2.526699388877387

Epoch: 5| Step: 9
Training loss: 2.422142358604605
Validation loss: 2.5231409873011783

Epoch: 5| Step: 10
Training loss: 2.6135254704467816
Validation loss: 2.533512575988243

Epoch: 5| Step: 11
Training loss: 2.2475973652593564
Validation loss: 2.5359421830869544

Epoch: 284| Step: 0
Training loss: 2.267542271162231
Validation loss: 2.5444053229112824

Epoch: 5| Step: 1
Training loss: 2.1814547248669376
Validation loss: 2.573326461734228

Epoch: 5| Step: 2
Training loss: 2.5058487188004133
Validation loss: 2.569683140902506

Epoch: 5| Step: 3
Training loss: 2.5705449425315843
Validation loss: 2.5972116075447302

Epoch: 5| Step: 4
Training loss: 2.287790302970845
Validation loss: 2.6042967509522446

Epoch: 5| Step: 5
Training loss: 2.510588823380242
Validation loss: 2.5909163189682736

Epoch: 5| Step: 6
Training loss: 2.234354512580899
Validation loss: 2.583326671704553

Epoch: 5| Step: 7
Training loss: 2.2082980411036655
Validation loss: 2.5861340128286003

Epoch: 5| Step: 8
Training loss: 2.217963065704124
Validation loss: 2.587298055499056

Epoch: 5| Step: 9
Training loss: 1.910668427307922
Validation loss: 2.548155102109502

Epoch: 5| Step: 10
Training loss: 2.4689531846272135
Validation loss: 2.5612959630468377

Epoch: 5| Step: 11
Training loss: 2.0036203280806224
Validation loss: 2.5250273263982255

Epoch: 285| Step: 0
Training loss: 2.504280145732367
Validation loss: 2.527403408724099

Epoch: 5| Step: 1
Training loss: 2.7550623854481375
Validation loss: 2.525828218126629

Epoch: 5| Step: 2
Training loss: 2.040441755447346
Validation loss: 2.5233479219171984

Epoch: 5| Step: 3
Training loss: 2.3864971751051094
Validation loss: 2.5370096133702424

Epoch: 5| Step: 4
Training loss: 2.1171102474518713
Validation loss: 2.5300160763849218

Epoch: 5| Step: 5
Training loss: 2.3235237685338825
Validation loss: 2.5266166655255278

Epoch: 5| Step: 6
Training loss: 2.392656946246297
Validation loss: 2.532802184372388

Epoch: 5| Step: 7
Training loss: 2.434692820417241
Validation loss: 2.539135518124505

Epoch: 5| Step: 8
Training loss: 2.460549653992368
Validation loss: 2.5285794569916384

Epoch: 5| Step: 9
Training loss: 2.0981249294559396
Validation loss: 2.5266580295058643

Epoch: 5| Step: 10
Training loss: 2.5637645857538813
Validation loss: 2.511193063719339

Epoch: 5| Step: 11
Training loss: 2.031774599850379
Validation loss: 2.5291463367728113

Epoch: 286| Step: 0
Training loss: 2.061872907153568
Validation loss: 2.5377919074594173

Epoch: 5| Step: 1
Training loss: 2.426868139940538
Validation loss: 2.5305359308068303

Epoch: 5| Step: 2
Training loss: 1.653735357149077
Validation loss: 2.5433314192261833

Epoch: 5| Step: 3
Training loss: 1.6831033439390417
Validation loss: 2.5746355274114423

Epoch: 5| Step: 4
Training loss: 2.3364448124061648
Validation loss: 2.577002656722855

Epoch: 5| Step: 5
Training loss: 2.863838089612544
Validation loss: 2.57444758693294

Epoch: 5| Step: 6
Training loss: 2.305586920630837
Validation loss: 2.5954680287309984

Epoch: 5| Step: 7
Training loss: 1.9710409973076732
Validation loss: 2.620584843099914

Epoch: 5| Step: 8
Training loss: 3.0258574350360736
Validation loss: 2.6175546379119146

Epoch: 5| Step: 9
Training loss: 1.871296594889469
Validation loss: 2.5975822763259617

Epoch: 5| Step: 10
Training loss: 2.418450968392234
Validation loss: 2.6073756905607586

Epoch: 5| Step: 11
Training loss: 1.829299353940605
Validation loss: 2.5959710680596477

Epoch: 287| Step: 0
Training loss: 2.1304943635727676
Validation loss: 2.626911322770429

Epoch: 5| Step: 1
Training loss: 2.2718770023216437
Validation loss: 2.618715324263322

Epoch: 5| Step: 2
Training loss: 2.089858466836324
Validation loss: 2.6264871971142214

Epoch: 5| Step: 3
Training loss: 2.468715619197902
Validation loss: 2.607821914140833

Epoch: 5| Step: 4
Training loss: 2.3856772290199806
Validation loss: 2.60131011823151

Epoch: 5| Step: 5
Training loss: 2.4385955500417347
Validation loss: 2.5916015198749

Epoch: 5| Step: 6
Training loss: 2.313558903102798
Validation loss: 2.5753602857599702

Epoch: 5| Step: 7
Training loss: 2.2377344232434075
Validation loss: 2.56980850472956

Epoch: 5| Step: 8
Training loss: 2.411906443102033
Validation loss: 2.5857274449405505

Epoch: 5| Step: 9
Training loss: 2.02194854272403
Validation loss: 2.5782290697767554

Epoch: 5| Step: 10
Training loss: 2.3883410744853277
Validation loss: 2.541903071796031

Epoch: 5| Step: 11
Training loss: 1.6865863445943752
Validation loss: 2.5629792133640126

Epoch: 288| Step: 0
Training loss: 2.4419332439036667
Validation loss: 2.563641181625278

Epoch: 5| Step: 1
Training loss: 2.2933546843015393
Validation loss: 2.5472337201019704

Epoch: 5| Step: 2
Training loss: 2.1702764103019314
Validation loss: 2.56417749903683

Epoch: 5| Step: 3
Training loss: 2.575875242795927
Validation loss: 2.5584750703654824

Epoch: 5| Step: 4
Training loss: 2.3864839878532154
Validation loss: 2.5493151417468325

Epoch: 5| Step: 5
Training loss: 2.3206100529960665
Validation loss: 2.552605557371605

Epoch: 5| Step: 6
Training loss: 2.3553864573816634
Validation loss: 2.5512431390057344

Epoch: 5| Step: 7
Training loss: 2.3866722988971483
Validation loss: 2.5536918021871613

Epoch: 5| Step: 8
Training loss: 2.1691687759504443
Validation loss: 2.571998360241278

Epoch: 5| Step: 9
Training loss: 1.8367655084094676
Validation loss: 2.5559337892951213

Epoch: 5| Step: 10
Training loss: 2.1508347332474553
Validation loss: 2.540699232705347

Epoch: 5| Step: 11
Training loss: 0.8424701166937452
Validation loss: 2.5625184027468464

Epoch: 289| Step: 0
Training loss: 2.026877524215251
Validation loss: 2.5636178423990104

Epoch: 5| Step: 1
Training loss: 1.6545861001505118
Validation loss: 2.5665860938042053

Epoch: 5| Step: 2
Training loss: 2.67779581011992
Validation loss: 2.560965314942538

Epoch: 5| Step: 3
Training loss: 2.023552261691884
Validation loss: 2.5681642291409643

Epoch: 5| Step: 4
Training loss: 2.1357446434170835
Validation loss: 2.5652528722940486

Epoch: 5| Step: 5
Training loss: 2.1539142507488216
Validation loss: 2.5529102867046327

Epoch: 5| Step: 6
Training loss: 2.7338121979400776
Validation loss: 2.5338199406684563

Epoch: 5| Step: 7
Training loss: 1.9515955925999793
Validation loss: 2.5538957129029933

Epoch: 5| Step: 8
Training loss: 2.235355615508012
Validation loss: 2.5265126303817533

Epoch: 5| Step: 9
Training loss: 2.5989915212424672
Validation loss: 2.53813444342943

Epoch: 5| Step: 10
Training loss: 2.59975271883042
Validation loss: 2.519742031299788

Epoch: 5| Step: 11
Training loss: 1.6033844795449423
Validation loss: 2.534302076301085

Epoch: 290| Step: 0
Training loss: 2.2759507173455327
Validation loss: 2.5367012899753654

Epoch: 5| Step: 1
Training loss: 1.5520845792965934
Validation loss: 2.5428114225646183

Epoch: 5| Step: 2
Training loss: 2.4564445049742254
Validation loss: 2.5487631992923143

Epoch: 5| Step: 3
Training loss: 1.953437963207082
Validation loss: 2.564384755881428

Epoch: 5| Step: 4
Training loss: 2.822999049787534
Validation loss: 2.5617700211587753

Epoch: 5| Step: 5
Training loss: 1.9312505530692978
Validation loss: 2.571311319875194

Epoch: 5| Step: 6
Training loss: 2.2524223004317867
Validation loss: 2.561191046194388

Epoch: 5| Step: 7
Training loss: 2.3182312336243056
Validation loss: 2.541914522613974

Epoch: 5| Step: 8
Training loss: 2.266735672835311
Validation loss: 2.5204910066899853

Epoch: 5| Step: 9
Training loss: 1.964853457356736
Validation loss: 2.526763299068806

Epoch: 5| Step: 10
Training loss: 2.666478736533722
Validation loss: 2.517626615678292

Epoch: 5| Step: 11
Training loss: 2.6488643516062593
Validation loss: 2.511432344631014

Epoch: 291| Step: 0
Training loss: 2.0585227534232127
Validation loss: 2.5161074971367534

Epoch: 5| Step: 1
Training loss: 2.498939575362369
Validation loss: 2.5332862724672123

Epoch: 5| Step: 2
Training loss: 1.7001220406876123
Validation loss: 2.5092662309483544

Epoch: 5| Step: 3
Training loss: 2.0707524821759224
Validation loss: 2.5205824953129685

Epoch: 5| Step: 4
Training loss: 2.199320219961532
Validation loss: 2.532431122833914

Epoch: 5| Step: 5
Training loss: 1.962623023775419
Validation loss: 2.539022056061855

Epoch: 5| Step: 6
Training loss: 2.8483656430034885
Validation loss: 2.540209117116416

Epoch: 5| Step: 7
Training loss: 2.5175553018202774
Validation loss: 2.558348428938458

Epoch: 5| Step: 8
Training loss: 2.0962602740441474
Validation loss: 2.590521683912462

Epoch: 5| Step: 9
Training loss: 2.7770356384287314
Validation loss: 2.5945310603946448

Epoch: 5| Step: 10
Training loss: 2.232972885575381
Validation loss: 2.5796716116175826

Epoch: 5| Step: 11
Training loss: 1.9746211473244217
Validation loss: 2.5728935415693335

Epoch: 292| Step: 0
Training loss: 2.5686029471005276
Validation loss: 2.5725763626121174

Epoch: 5| Step: 1
Training loss: 2.0612503658777843
Validation loss: 2.545194172631021

Epoch: 5| Step: 2
Training loss: 2.390714157536286
Validation loss: 2.519270311194784

Epoch: 5| Step: 3
Training loss: 2.3024196005171826
Validation loss: 2.524892849899106

Epoch: 5| Step: 4
Training loss: 2.329355618677964
Validation loss: 2.5329468655583582

Epoch: 5| Step: 5
Training loss: 2.1092004774446487
Validation loss: 2.526801686474743

Epoch: 5| Step: 6
Training loss: 2.766597081270638
Validation loss: 2.5372995643504295

Epoch: 5| Step: 7
Training loss: 1.7400208416052323
Validation loss: 2.541178718354633

Epoch: 5| Step: 8
Training loss: 1.8217746608840135
Validation loss: 2.546699726571911

Epoch: 5| Step: 9
Training loss: 1.8624279699542268
Validation loss: 2.5568159959764483

Epoch: 5| Step: 10
Training loss: 2.5073220792499393
Validation loss: 2.5734011210727044

Epoch: 5| Step: 11
Training loss: 3.243800191913582
Validation loss: 2.5668283170621295

Epoch: 293| Step: 0
Training loss: 2.39995574115634
Validation loss: 2.581570586616039

Epoch: 5| Step: 1
Training loss: 1.5929253632612994
Validation loss: 2.6031337037232922

Epoch: 5| Step: 2
Training loss: 1.8278058417400018
Validation loss: 2.5593858691471065

Epoch: 5| Step: 3
Training loss: 1.956656658086356
Validation loss: 2.563291218188017

Epoch: 5| Step: 4
Training loss: 2.528207245232758
Validation loss: 2.575737225063061

Epoch: 5| Step: 5
Training loss: 2.2891105152080757
Validation loss: 2.5645635599964307

Epoch: 5| Step: 6
Training loss: 2.78760915978698
Validation loss: 2.562614760116399

Epoch: 5| Step: 7
Training loss: 2.9205469387405363
Validation loss: 2.566381248224601

Epoch: 5| Step: 8
Training loss: 2.140454807616191
Validation loss: 2.5698193750535747

Epoch: 5| Step: 9
Training loss: 2.207048318172386
Validation loss: 2.5600836533920557

Epoch: 5| Step: 10
Training loss: 2.0251043222187692
Validation loss: 2.5581961053416156

Epoch: 5| Step: 11
Training loss: 1.0717216181768827
Validation loss: 2.5616007638192193

Epoch: 294| Step: 0
Training loss: 2.7254417761275023
Validation loss: 2.5620301889818013

Epoch: 5| Step: 1
Training loss: 2.172136469408826
Validation loss: 2.5494400673434283

Epoch: 5| Step: 2
Training loss: 1.7289619171378778
Validation loss: 2.5524145819631316

Epoch: 5| Step: 3
Training loss: 2.4431858748226882
Validation loss: 2.5527866458381347

Epoch: 5| Step: 4
Training loss: 2.357877546643035
Validation loss: 2.5531278432007687

Epoch: 5| Step: 5
Training loss: 1.957817726706723
Validation loss: 2.5489856227967147

Epoch: 5| Step: 6
Training loss: 2.1992936604346136
Validation loss: 2.5529222543307766

Epoch: 5| Step: 7
Training loss: 2.3453016104245235
Validation loss: 2.534838485591601

Epoch: 5| Step: 8
Training loss: 2.376659315596345
Validation loss: 2.564409436223931

Epoch: 5| Step: 9
Training loss: 1.9883139856787588
Validation loss: 2.5532489579986515

Epoch: 5| Step: 10
Training loss: 2.32948724204535
Validation loss: 2.5625956815983817

Epoch: 5| Step: 11
Training loss: 2.4101067882637404
Validation loss: 2.572581929016529

Epoch: 295| Step: 0
Training loss: 1.9476616704431569
Validation loss: 2.5690488772148448

Epoch: 5| Step: 1
Training loss: 2.4501386797834317
Validation loss: 2.5728715449918518

Epoch: 5| Step: 2
Training loss: 2.464471126870969
Validation loss: 2.5850458692032587

Epoch: 5| Step: 3
Training loss: 2.161400765874167
Validation loss: 2.570146816212462

Epoch: 5| Step: 4
Training loss: 2.01952534717162
Validation loss: 2.5614084617183073

Epoch: 5| Step: 5
Training loss: 2.4332353502516675
Validation loss: 2.566155385104851

Epoch: 5| Step: 6
Training loss: 1.9713619405038028
Validation loss: 2.572236652061445

Epoch: 5| Step: 7
Training loss: 2.0572031380321074
Validation loss: 2.5914673086645204

Epoch: 5| Step: 8
Training loss: 2.2740312043584616
Validation loss: 2.5974713061280386

Epoch: 5| Step: 9
Training loss: 2.4753066287207663
Validation loss: 2.59551283667735

Epoch: 5| Step: 10
Training loss: 2.2751531779942646
Validation loss: 2.6050162582536873

Epoch: 5| Step: 11
Training loss: 2.5439001390542963
Validation loss: 2.6219328566644045

Epoch: 296| Step: 0
Training loss: 2.593541332261909
Validation loss: 2.581993245101453

Epoch: 5| Step: 1
Training loss: 2.0258875561886014
Validation loss: 2.5738335505988146

Epoch: 5| Step: 2
Training loss: 2.1508254218839715
Validation loss: 2.5538627078566005

Epoch: 5| Step: 3
Training loss: 2.1246379095014
Validation loss: 2.554251556842309

Epoch: 5| Step: 4
Training loss: 2.3370208779587083
Validation loss: 2.565954546995862

Epoch: 5| Step: 5
Training loss: 2.213634557831857
Validation loss: 2.5804307240906517

Epoch: 5| Step: 6
Training loss: 2.406702667858211
Validation loss: 2.5640576637062833

Epoch: 5| Step: 7
Training loss: 2.3070765639730495
Validation loss: 2.5776794973452257

Epoch: 5| Step: 8
Training loss: 1.583153145559109
Validation loss: 2.600231001752356

Epoch: 5| Step: 9
Training loss: 2.3859459457797576
Validation loss: 2.609060508369968

Epoch: 5| Step: 10
Training loss: 2.408764899721965
Validation loss: 2.5635664550521087

Epoch: 5| Step: 11
Training loss: 2.4251733776918374
Validation loss: 2.5494368136936836

Epoch: 297| Step: 0
Training loss: 1.951741331166966
Validation loss: 2.547020898240234

Epoch: 5| Step: 1
Training loss: 2.2562023083832177
Validation loss: 2.5812030951153835

Epoch: 5| Step: 2
Training loss: 2.0906420742643457
Validation loss: 2.5801412693548946

Epoch: 5| Step: 3
Training loss: 1.8654799694130002
Validation loss: 2.593382797905384

Epoch: 5| Step: 4
Training loss: 2.171560031364115
Validation loss: 2.5835894655081204

Epoch: 5| Step: 5
Training loss: 2.3009194816682754
Validation loss: 2.566882948050784

Epoch: 5| Step: 6
Training loss: 2.5001109098627987
Validation loss: 2.557408815071566

Epoch: 5| Step: 7
Training loss: 2.5350521401304413
Validation loss: 2.5487399304016267

Epoch: 5| Step: 8
Training loss: 2.219561683583095
Validation loss: 2.5477553954178527

Epoch: 5| Step: 9
Training loss: 1.8633387654702445
Validation loss: 2.5662397527079777

Epoch: 5| Step: 10
Training loss: 2.6989457120689844
Validation loss: 2.548091956529345

Epoch: 5| Step: 11
Training loss: 3.431606859226666
Validation loss: 2.548488480311196

Epoch: 298| Step: 0
Training loss: 2.267178864203003
Validation loss: 2.5744064505745063

Epoch: 5| Step: 1
Training loss: 2.2795891137230817
Validation loss: 2.5726581139842057

Epoch: 5| Step: 2
Training loss: 2.3774377458927716
Validation loss: 2.5506706895376103

Epoch: 5| Step: 3
Training loss: 2.18925427530047
Validation loss: 2.5761562806786116

Epoch: 5| Step: 4
Training loss: 2.358923357246901
Validation loss: 2.59770126494983

Epoch: 5| Step: 5
Training loss: 2.7433666508885164
Validation loss: 2.586546089649088

Epoch: 5| Step: 6
Training loss: 2.1406867025351106
Validation loss: 2.579210207103782

Epoch: 5| Step: 7
Training loss: 1.8722298185544684
Validation loss: 2.601660565274025

Epoch: 5| Step: 8
Training loss: 1.9582280137439678
Validation loss: 2.5835510413407334

Epoch: 5| Step: 9
Training loss: 2.527622496431489
Validation loss: 2.579366246442682

Epoch: 5| Step: 10
Training loss: 2.279455550422899
Validation loss: 2.539240106092961

Epoch: 5| Step: 11
Training loss: 0.709021701303071
Validation loss: 2.5376554180023563

Epoch: 299| Step: 0
Training loss: 1.822909880125946
Validation loss: 2.53338693752379

Epoch: 5| Step: 1
Training loss: 1.8759595323114295
Validation loss: 2.5458852760044

Epoch: 5| Step: 2
Training loss: 1.90913017189226
Validation loss: 2.5467954127820454

Epoch: 5| Step: 3
Training loss: 2.7696744938273543
Validation loss: 2.5472517846487137

Epoch: 5| Step: 4
Training loss: 2.0762817698235025
Validation loss: 2.5463367119545444

Epoch: 5| Step: 5
Training loss: 1.8183899906584482
Validation loss: 2.5414188699363023

Epoch: 5| Step: 6
Training loss: 2.138194465478898
Validation loss: 2.550947416617204

Epoch: 5| Step: 7
Training loss: 2.579125967202786
Validation loss: 2.5509765846192662

Epoch: 5| Step: 8
Training loss: 2.7826981792496497
Validation loss: 2.545993472950718

Epoch: 5| Step: 9
Training loss: 2.756914635621761
Validation loss: 2.5666687520026947

Epoch: 5| Step: 10
Training loss: 1.6431480988235232
Validation loss: 2.595198008786824

Epoch: 5| Step: 11
Training loss: 2.802381071890228
Validation loss: 2.6238874469514135

Epoch: 300| Step: 0
Training loss: 2.3644412385927582
Validation loss: 2.7300623571752247

Epoch: 5| Step: 1
Training loss: 2.818977187785298
Validation loss: 2.841380665855962

Epoch: 5| Step: 2
Training loss: 2.4723085276460908
Validation loss: 2.8568128491733984

Epoch: 5| Step: 3
Training loss: 2.5964089977375897
Validation loss: 2.838178591591507

Epoch: 5| Step: 4
Training loss: 2.3826661111859124
Validation loss: 2.79837327035378

Epoch: 5| Step: 5
Training loss: 2.6102561148316212
Validation loss: 2.6835450006505783

Epoch: 5| Step: 6
Training loss: 2.4027449606220546
Validation loss: 2.5850328628300683

Epoch: 5| Step: 7
Training loss: 2.046628747934651
Validation loss: 2.508327399581616

Epoch: 5| Step: 8
Training loss: 2.1954552003359415
Validation loss: 2.5026504414408777

Epoch: 5| Step: 9
Training loss: 1.9993479380992663
Validation loss: 2.512313179168431

Epoch: 5| Step: 10
Training loss: 2.37355298583164
Validation loss: 2.513169102750201

Epoch: 5| Step: 11
Training loss: 2.9840266638579545
Validation loss: 2.509016459033854

Epoch: 301| Step: 0
Training loss: 2.5118592313455195
Validation loss: 2.513275980803065

Epoch: 5| Step: 1
Training loss: 2.383019810397733
Validation loss: 2.513131372791082

Epoch: 5| Step: 2
Training loss: 2.6336300992536565
Validation loss: 2.518493923772162

Epoch: 5| Step: 3
Training loss: 1.6174555754420097
Validation loss: 2.5248776077281465

Epoch: 5| Step: 4
Training loss: 2.527873672045114
Validation loss: 2.511538252369144

Epoch: 5| Step: 5
Training loss: 1.9336382619955506
Validation loss: 2.5166454456920953

Epoch: 5| Step: 6
Training loss: 2.5509777139477534
Validation loss: 2.51832860193454

Epoch: 5| Step: 7
Training loss: 2.943788979306686
Validation loss: 2.510506427843275

Epoch: 5| Step: 8
Training loss: 2.2853663682111636
Validation loss: 2.522712753039583

Epoch: 5| Step: 9
Training loss: 2.2143358189763496
Validation loss: 2.518897303587889

Epoch: 5| Step: 10
Training loss: 2.1959807441159023
Validation loss: 2.501483961592371

Epoch: 5| Step: 11
Training loss: 3.24359056378628
Validation loss: 2.5029563431071438

Epoch: 302| Step: 0
Training loss: 1.925799263334079
Validation loss: 2.5026390213512375

Epoch: 5| Step: 1
Training loss: 2.620474683989046
Validation loss: 2.486393913922927

Epoch: 5| Step: 2
Training loss: 2.780491414718668
Validation loss: 2.4779106947028993

Epoch: 5| Step: 3
Training loss: 2.3411377654539507
Validation loss: 2.4719857164963557

Epoch: 5| Step: 4
Training loss: 2.454879813428035
Validation loss: 2.4912926114564957

Epoch: 5| Step: 5
Training loss: 2.1912761248409227
Validation loss: 2.496814780847918

Epoch: 5| Step: 6
Training loss: 2.6497880311078177
Validation loss: 2.4796232729988783

Epoch: 5| Step: 7
Training loss: 2.4729472346185473
Validation loss: 2.5030855588468626

Epoch: 5| Step: 8
Training loss: 2.4428297607780247
Validation loss: 2.5115388259002924

Epoch: 5| Step: 9
Training loss: 1.8527913582853943
Validation loss: 2.5356929389264873

Epoch: 5| Step: 10
Training loss: 1.5854707980177962
Validation loss: 2.538968402390487

Epoch: 5| Step: 11
Training loss: 2.719330561245628
Validation loss: 2.5358179969545303

Epoch: 303| Step: 0
Training loss: 1.5070539552867734
Validation loss: 2.5436876807206086

Epoch: 5| Step: 1
Training loss: 2.2431654442270994
Validation loss: 2.531649034153893

Epoch: 5| Step: 2
Training loss: 1.9599384939508153
Validation loss: 2.5200786932258814

Epoch: 5| Step: 3
Training loss: 2.3311014286592124
Validation loss: 2.5431376887945785

Epoch: 5| Step: 4
Training loss: 2.8786079324640954
Validation loss: 2.532729430800474

Epoch: 5| Step: 5
Training loss: 2.1723274473225977
Validation loss: 2.5270911301264314

Epoch: 5| Step: 6
Training loss: 2.4826055979478814
Validation loss: 2.5383320819073516

Epoch: 5| Step: 7
Training loss: 2.079490614469737
Validation loss: 2.5235177648355003

Epoch: 5| Step: 8
Training loss: 2.4068240187007084
Validation loss: 2.5290922143484362

Epoch: 5| Step: 9
Training loss: 2.4529587999158227
Validation loss: 2.5215469813534144

Epoch: 5| Step: 10
Training loss: 2.490400768031643
Validation loss: 2.5292226498193293

Epoch: 5| Step: 11
Training loss: 1.1803530400924
Validation loss: 2.509113671395506

Epoch: 304| Step: 0
Training loss: 2.3847669246835626
Validation loss: 2.518966430124377

Epoch: 5| Step: 1
Training loss: 2.153415198625123
Validation loss: 2.516298255312417

Epoch: 5| Step: 2
Training loss: 2.49689519728416
Validation loss: 2.508067624116851

Epoch: 5| Step: 3
Training loss: 2.102119570502636
Validation loss: 2.5315816685528065

Epoch: 5| Step: 4
Training loss: 2.2565247978607714
Validation loss: 2.5158349338546198

Epoch: 5| Step: 5
Training loss: 2.2297877459589754
Validation loss: 2.519191121645068

Epoch: 5| Step: 6
Training loss: 1.8115326503303468
Validation loss: 2.530125878567415

Epoch: 5| Step: 7
Training loss: 2.605388405632001
Validation loss: 2.533126402442833

Epoch: 5| Step: 8
Training loss: 2.0686666604395594
Validation loss: 2.5330976563759973

Epoch: 5| Step: 9
Training loss: 2.487099551071673
Validation loss: 2.5404267700238

Epoch: 5| Step: 10
Training loss: 2.2270073697225574
Validation loss: 2.5502103523994526

Epoch: 5| Step: 11
Training loss: 2.438000652053408
Validation loss: 2.589249939350352

Epoch: 305| Step: 0
Training loss: 2.279710851370615
Validation loss: 2.580209413470978

Epoch: 5| Step: 1
Training loss: 1.9518611022864867
Validation loss: 2.553386070929636

Epoch: 5| Step: 2
Training loss: 2.6033856759403995
Validation loss: 2.548873472236641

Epoch: 5| Step: 3
Training loss: 2.642767223463336
Validation loss: 2.5500882791060846

Epoch: 5| Step: 4
Training loss: 1.9361641493321737
Validation loss: 2.543132016927033

Epoch: 5| Step: 5
Training loss: 1.3129218195688297
Validation loss: 2.5246997231374353

Epoch: 5| Step: 6
Training loss: 2.4324408627699055
Validation loss: 2.53224077575045

Epoch: 5| Step: 7
Training loss: 2.2881060480136814
Validation loss: 2.517014846153134

Epoch: 5| Step: 8
Training loss: 2.069202053316972
Validation loss: 2.5491824901268862

Epoch: 5| Step: 9
Training loss: 2.66428801625217
Validation loss: 2.517499841104785

Epoch: 5| Step: 10
Training loss: 2.6476137573758467
Validation loss: 2.5431801141188823

Epoch: 5| Step: 11
Training loss: 1.7524156927272072
Validation loss: 2.5695474330492623

Epoch: 306| Step: 0
Training loss: 2.3713853333121238
Validation loss: 2.5892647028291322

Epoch: 5| Step: 1
Training loss: 2.1745069186025354
Validation loss: 2.58603105250968

Epoch: 5| Step: 2
Training loss: 2.3781735148349874
Validation loss: 2.60590740897823

Epoch: 5| Step: 3
Training loss: 2.7864450519756248
Validation loss: 2.6367290204636973

Epoch: 5| Step: 4
Training loss: 1.7045943894731352
Validation loss: 2.642536889483097

Epoch: 5| Step: 5
Training loss: 2.4276234976628954
Validation loss: 2.6792714321442856

Epoch: 5| Step: 6
Training loss: 2.4500252157976727
Validation loss: 2.6318175381858926

Epoch: 5| Step: 7
Training loss: 2.6868238263826183
Validation loss: 2.6029159874139416

Epoch: 5| Step: 8
Training loss: 1.8779384475955978
Validation loss: 2.5979050863681405

Epoch: 5| Step: 9
Training loss: 1.902183823648544
Validation loss: 2.6100569460187306

Epoch: 5| Step: 10
Training loss: 1.8507279072357161
Validation loss: 2.553076462613081

Epoch: 5| Step: 11
Training loss: 2.1233059357453037
Validation loss: 2.568874950417412

Epoch: 307| Step: 0
Training loss: 2.5471525450587116
Validation loss: 2.5618166438790806

Epoch: 5| Step: 1
Training loss: 2.509351406722836
Validation loss: 2.541633141927114

Epoch: 5| Step: 2
Training loss: 1.7993421120534878
Validation loss: 2.546259458537099

Epoch: 5| Step: 3
Training loss: 3.2429139575818686
Validation loss: 2.5520424560594814

Epoch: 5| Step: 4
Training loss: 2.3456439694481963
Validation loss: 2.5487714856164967

Epoch: 5| Step: 5
Training loss: 2.1088496931623246
Validation loss: 2.5624708914460346

Epoch: 5| Step: 6
Training loss: 2.059385317601478
Validation loss: 2.539166277207371

Epoch: 5| Step: 7
Training loss: 1.7837326416796346
Validation loss: 2.568997480563221

Epoch: 5| Step: 8
Training loss: 1.772195254924222
Validation loss: 2.5822384434229453

Epoch: 5| Step: 9
Training loss: 2.697147794963182
Validation loss: 2.5833983425964138

Epoch: 5| Step: 10
Training loss: 1.9109373153474334
Validation loss: 2.5896475493698117

Epoch: 5| Step: 11
Training loss: 1.654700436059746
Validation loss: 2.586734116966682

Epoch: 308| Step: 0
Training loss: 2.203486527884969
Validation loss: 2.5843915976468406

Epoch: 5| Step: 1
Training loss: 1.724485779564
Validation loss: 2.5986612694219233

Epoch: 5| Step: 2
Training loss: 2.5674558957777474
Validation loss: 2.594823638053405

Epoch: 5| Step: 3
Training loss: 2.1242955105918764
Validation loss: 2.6000454023719124

Epoch: 5| Step: 4
Training loss: 2.747449385616754
Validation loss: 2.58925908980125

Epoch: 5| Step: 5
Training loss: 2.010037150822533
Validation loss: 2.6001851060276775

Epoch: 5| Step: 6
Training loss: 2.4082062930838855
Validation loss: 2.5750115408222123

Epoch: 5| Step: 7
Training loss: 2.3553684396719725
Validation loss: 2.5783913754113503

Epoch: 5| Step: 8
Training loss: 1.9765139487112338
Validation loss: 2.5745654531120277

Epoch: 5| Step: 9
Training loss: 1.6522512567844077
Validation loss: 2.5574988509791168

Epoch: 5| Step: 10
Training loss: 2.4954216997095107
Validation loss: 2.5703732059193425

Epoch: 5| Step: 11
Training loss: 1.2800227544669371
Validation loss: 2.5716332131527855

Epoch: 309| Step: 0
Training loss: 2.719975513179735
Validation loss: 2.5803645704357043

Epoch: 5| Step: 1
Training loss: 1.9900394602558122
Validation loss: 2.5899603286672717

Epoch: 5| Step: 2
Training loss: 1.543097768290675
Validation loss: 2.586849205593241

Epoch: 5| Step: 3
Training loss: 2.1850503556932694
Validation loss: 2.6109825786334206

Epoch: 5| Step: 4
Training loss: 1.6502245576813284
Validation loss: 2.605150701888749

Epoch: 5| Step: 5
Training loss: 1.6780204129438068
Validation loss: 2.6068368357795477

Epoch: 5| Step: 6
Training loss: 2.5810317790082493
Validation loss: 2.6189655318442084

Epoch: 5| Step: 7
Training loss: 2.910971631000003
Validation loss: 2.6022599576975227

Epoch: 5| Step: 8
Training loss: 2.07220786802464
Validation loss: 2.600743024366947

Epoch: 5| Step: 9
Training loss: 2.450801745430269
Validation loss: 2.5781326332364607

Epoch: 5| Step: 10
Training loss: 2.3492802796141286
Validation loss: 2.565533176440803

Epoch: 5| Step: 11
Training loss: 1.5532720822358352
Validation loss: 2.563462131035767

Epoch: 310| Step: 0
Training loss: 2.4653757914608754
Validation loss: 2.5678242598563745

Epoch: 5| Step: 1
Training loss: 1.647173283761138
Validation loss: 2.55935804682059

Epoch: 5| Step: 2
Training loss: 1.9866261366522473
Validation loss: 2.5661517771414677

Epoch: 5| Step: 3
Training loss: 3.034275236365675
Validation loss: 2.560180168362189

Epoch: 5| Step: 4
Training loss: 2.410469121425425
Validation loss: 2.5520646983394037

Epoch: 5| Step: 5
Training loss: 2.254780882278232
Validation loss: 2.561056858839914

Epoch: 5| Step: 6
Training loss: 1.592015238569162
Validation loss: 2.5557542220642406

Epoch: 5| Step: 7
Training loss: 2.149473627106578
Validation loss: 2.5677358472380396

Epoch: 5| Step: 8
Training loss: 2.1270609566473304
Validation loss: 2.5744683082491004

Epoch: 5| Step: 9
Training loss: 2.3220143868052405
Validation loss: 2.596391034159462

Epoch: 5| Step: 10
Training loss: 2.2482449786464085
Validation loss: 2.5842609906217056

Epoch: 5| Step: 11
Training loss: 3.3441763632778185
Validation loss: 2.608313365459691

Epoch: 311| Step: 0
Training loss: 1.8025148202231303
Validation loss: 2.5834740913149696

Epoch: 5| Step: 1
Training loss: 2.3743047198452527
Validation loss: 2.5852603544391783

Epoch: 5| Step: 2
Training loss: 2.201597942557039
Validation loss: 2.5699498887996364

Epoch: 5| Step: 3
Training loss: 2.113676192914239
Validation loss: 2.5592263097426464

Epoch: 5| Step: 4
Training loss: 1.4179412586920503
Validation loss: 2.5613118456486603

Epoch: 5| Step: 5
Training loss: 2.284838010359059
Validation loss: 2.560729046744404

Epoch: 5| Step: 6
Training loss: 1.9723261490902082
Validation loss: 2.547029276038685

Epoch: 5| Step: 7
Training loss: 2.0929620882948528
Validation loss: 2.5550916509557506

Epoch: 5| Step: 8
Training loss: 3.021464805810901
Validation loss: 2.5140867170072765

Epoch: 5| Step: 9
Training loss: 2.3594067299837636
Validation loss: 2.521417842965283

Epoch: 5| Step: 10
Training loss: 1.891262672013756
Validation loss: 2.520441550293165

Epoch: 5| Step: 11
Training loss: 3.810375309478644
Validation loss: 2.5250595930041886

Epoch: 312| Step: 0
Training loss: 2.5125186293181785
Validation loss: 2.53913003293728

Epoch: 5| Step: 1
Training loss: 1.7344072356106564
Validation loss: 2.5191217252306575

Epoch: 5| Step: 2
Training loss: 2.220829164120576
Validation loss: 2.558541803584921

Epoch: 5| Step: 3
Training loss: 2.2474729333930203
Validation loss: 2.587873991906797

Epoch: 5| Step: 4
Training loss: 1.7080103948888783
Validation loss: 2.609803063768778

Epoch: 5| Step: 5
Training loss: 2.3622390845195245
Validation loss: 2.600307177010334

Epoch: 5| Step: 6
Training loss: 2.4046084266436893
Validation loss: 2.5830657933247165

Epoch: 5| Step: 7
Training loss: 2.373390354996658
Validation loss: 2.5285953486569377

Epoch: 5| Step: 8
Training loss: 2.7671252992726947
Validation loss: 2.5136475679199974

Epoch: 5| Step: 9
Training loss: 2.0970200012373748
Validation loss: 2.5005365908464663

Epoch: 5| Step: 10
Training loss: 2.138493277003361
Validation loss: 2.51839118417917

Epoch: 5| Step: 11
Training loss: 2.1769003053476306
Validation loss: 2.5225545678980312

Epoch: 313| Step: 0
Training loss: 2.997994388282415
Validation loss: 2.5463078302224953

Epoch: 5| Step: 1
Training loss: 2.4154596822410395
Validation loss: 2.537910767523695

Epoch: 5| Step: 2
Training loss: 1.9977646495065717
Validation loss: 2.542716764357697

Epoch: 5| Step: 3
Training loss: 3.2232649164785943
Validation loss: 2.560490997271189

Epoch: 5| Step: 4
Training loss: 2.3163113502245762
Validation loss: 2.54432745056775

Epoch: 5| Step: 5
Training loss: 1.9634843677960223
Validation loss: 2.5534035803450177

Epoch: 5| Step: 6
Training loss: 2.079039867053739
Validation loss: 2.525863365225328

Epoch: 5| Step: 7
Training loss: 1.8953037849848582
Validation loss: 2.5205509931434147

Epoch: 5| Step: 8
Training loss: 1.8921586429024069
Validation loss: 2.5575376976257784

Epoch: 5| Step: 9
Training loss: 2.2481540949330427
Validation loss: 2.542782559393003

Epoch: 5| Step: 10
Training loss: 1.9248964083652975
Validation loss: 2.5404211859537473

Epoch: 5| Step: 11
Training loss: 2.3521925376857404
Validation loss: 2.531223379395458

Epoch: 314| Step: 0
Training loss: 2.4797125677465397
Validation loss: 2.529729512058028

Epoch: 5| Step: 1
Training loss: 2.6427511650546953
Validation loss: 2.542275372819006

Epoch: 5| Step: 2
Training loss: 2.3613959745214963
Validation loss: 2.5167942094273146

Epoch: 5| Step: 3
Training loss: 1.9811225729797544
Validation loss: 2.528700215428966

Epoch: 5| Step: 4
Training loss: 2.341126359484468
Validation loss: 2.522462142607327

Epoch: 5| Step: 5
Training loss: 2.0172460380588095
Validation loss: 2.4988718447732507

Epoch: 5| Step: 6
Training loss: 2.6143537524420286
Validation loss: 2.517623620797319

Epoch: 5| Step: 7
Training loss: 2.241457511286427
Validation loss: 2.515535009444966

Epoch: 5| Step: 8
Training loss: 2.0994404001772673
Validation loss: 2.5135215076049695

Epoch: 5| Step: 9
Training loss: 1.8941901145632802
Validation loss: 2.52225660244007

Epoch: 5| Step: 10
Training loss: 2.0043995865306226
Validation loss: 2.5151303596788948

Epoch: 5| Step: 11
Training loss: 3.0663396937442595
Validation loss: 2.529450561881572

Epoch: 315| Step: 0
Training loss: 2.3395134847194887
Validation loss: 2.5507448747815142

Epoch: 5| Step: 1
Training loss: 2.64315963177601
Validation loss: 2.6209051459459904

Epoch: 5| Step: 2
Training loss: 2.325601016061651
Validation loss: 2.6451172134890277

Epoch: 5| Step: 3
Training loss: 2.386614258721774
Validation loss: 2.7047458375736038

Epoch: 5| Step: 4
Training loss: 1.9694781092337266
Validation loss: 2.692586129893912

Epoch: 5| Step: 5
Training loss: 1.8084341024047512
Validation loss: 2.5806748775516457

Epoch: 5| Step: 6
Training loss: 2.18099017850194
Validation loss: 2.5459481329458042

Epoch: 5| Step: 7
Training loss: 2.3067357159167448
Validation loss: 2.5085272874544913

Epoch: 5| Step: 8
Training loss: 2.369669604389199
Validation loss: 2.4924946220044037

Epoch: 5| Step: 9
Training loss: 2.225149676828404
Validation loss: 2.5003334875997587

Epoch: 5| Step: 10
Training loss: 2.029380994857801
Validation loss: 2.5138059480050496

Epoch: 5| Step: 11
Training loss: 1.7027411071966305
Validation loss: 2.4995850178892582

Epoch: 316| Step: 0
Training loss: 2.5519975505087644
Validation loss: 2.521535914746564

Epoch: 5| Step: 1
Training loss: 2.5796472852450143
Validation loss: 2.5186237248334766

Epoch: 5| Step: 2
Training loss: 1.717551281536846
Validation loss: 2.525082122140832

Epoch: 5| Step: 3
Training loss: 1.4456648938273182
Validation loss: 2.5324355241614707

Epoch: 5| Step: 4
Training loss: 2.7830631475216765
Validation loss: 2.5265090444458167

Epoch: 5| Step: 5
Training loss: 2.287286688025573
Validation loss: 2.517598631750399

Epoch: 5| Step: 6
Training loss: 2.259409725613533
Validation loss: 2.492781194686626

Epoch: 5| Step: 7
Training loss: 1.936613433793503
Validation loss: 2.4896369087700667

Epoch: 5| Step: 8
Training loss: 1.8588572189920223
Validation loss: 2.5065162374585754

Epoch: 5| Step: 9
Training loss: 2.3217996782738846
Validation loss: 2.5070669822884524

Epoch: 5| Step: 10
Training loss: 2.8518116463289953
Validation loss: 2.515869034013371

Epoch: 5| Step: 11
Training loss: 1.9994540661522053
Validation loss: 2.528627045257469

Epoch: 317| Step: 0
Training loss: 1.6937919864869915
Validation loss: 2.5346552349337292

Epoch: 5| Step: 1
Training loss: 2.4102607098513484
Validation loss: 2.583136367723612

Epoch: 5| Step: 2
Training loss: 2.3858656037395876
Validation loss: 2.628578709777749

Epoch: 5| Step: 3
Training loss: 1.9980737231288193
Validation loss: 2.5922469912725616

Epoch: 5| Step: 4
Training loss: 2.0197430320682823
Validation loss: 2.6076480838790173

Epoch: 5| Step: 5
Training loss: 2.1168911997533124
Validation loss: 2.6063193972928484

Epoch: 5| Step: 6
Training loss: 2.8887732270717077
Validation loss: 2.565275158806509

Epoch: 5| Step: 7
Training loss: 2.0713314212135026
Validation loss: 2.557496163038133

Epoch: 5| Step: 8
Training loss: 1.4425138027912545
Validation loss: 2.547617868311566

Epoch: 5| Step: 9
Training loss: 2.6340030502614495
Validation loss: 2.534381283659383

Epoch: 5| Step: 10
Training loss: 2.251801617199854
Validation loss: 2.5344549697193988

Epoch: 5| Step: 11
Training loss: 1.8789558642016901
Validation loss: 2.5588052094415

Epoch: 318| Step: 0
Training loss: 2.248843637651138
Validation loss: 2.5220792203178837

Epoch: 5| Step: 1
Training loss: 2.102384385276534
Validation loss: 2.5504981522073877

Epoch: 5| Step: 2
Training loss: 1.8359611996683431
Validation loss: 2.5198113416387495

Epoch: 5| Step: 3
Training loss: 2.627123564115394
Validation loss: 2.5364162681793925

Epoch: 5| Step: 4
Training loss: 2.215544305577482
Validation loss: 2.569884377626515

Epoch: 5| Step: 5
Training loss: 2.446918581675557
Validation loss: 2.5717532671782837

Epoch: 5| Step: 6
Training loss: 1.9949314742401711
Validation loss: 2.5980142791222027

Epoch: 5| Step: 7
Training loss: 2.231400009797245
Validation loss: 2.610519744368273

Epoch: 5| Step: 8
Training loss: 1.9203404448596655
Validation loss: 2.5976635007828786

Epoch: 5| Step: 9
Training loss: 2.500238025778121
Validation loss: 2.57996521596107

Epoch: 5| Step: 10
Training loss: 2.0856572223628542
Validation loss: 2.5765768007182697

Epoch: 5| Step: 11
Training loss: 0.8761425732812099
Validation loss: 2.568188656546955

Epoch: 319| Step: 0
Training loss: 2.262207505514061
Validation loss: 2.54230084220316

Epoch: 5| Step: 1
Training loss: 2.221652858697899
Validation loss: 2.5364366461252943

Epoch: 5| Step: 2
Training loss: 2.2077467037258827
Validation loss: 2.541937678120622

Epoch: 5| Step: 3
Training loss: 2.3107751135422525
Validation loss: 2.5529096796637365

Epoch: 5| Step: 4
Training loss: 1.655121832912628
Validation loss: 2.5326799780448215

Epoch: 5| Step: 5
Training loss: 2.5566545199023203
Validation loss: 2.538668229229163

Epoch: 5| Step: 6
Training loss: 2.6368494072083766
Validation loss: 2.5427543248933757

Epoch: 5| Step: 7
Training loss: 2.0265776669678086
Validation loss: 2.55321940742053

Epoch: 5| Step: 8
Training loss: 2.084987123847557
Validation loss: 2.6027558752312463

Epoch: 5| Step: 9
Training loss: 2.168698959409579
Validation loss: 2.6104833871931072

Epoch: 5| Step: 10
Training loss: 2.301386340742351
Validation loss: 2.6130942566194606

Epoch: 5| Step: 11
Training loss: 1.4064705993557463
Validation loss: 2.616838088363287

Epoch: 320| Step: 0
Training loss: 1.9324554483625245
Validation loss: 2.608492140358575

Epoch: 5| Step: 1
Training loss: 2.5893161978594965
Validation loss: 2.569131369175574

Epoch: 5| Step: 2
Training loss: 1.6917614288597949
Validation loss: 2.5602954165866785

Epoch: 5| Step: 3
Training loss: 1.969080639882601
Validation loss: 2.5695425888310846

Epoch: 5| Step: 4
Training loss: 2.362405611604975
Validation loss: 2.534721027588817

Epoch: 5| Step: 5
Training loss: 2.651670749772255
Validation loss: 2.5609954510893784

Epoch: 5| Step: 6
Training loss: 1.9949006996203658
Validation loss: 2.566009808272092

Epoch: 5| Step: 7
Training loss: 1.9153687671345119
Validation loss: 2.512381221563682

Epoch: 5| Step: 8
Training loss: 2.6314268002948626
Validation loss: 2.516871404188196

Epoch: 5| Step: 9
Training loss: 1.8373226703012338
Validation loss: 2.5398683452715836

Epoch: 5| Step: 10
Training loss: 1.749747258055155
Validation loss: 2.5791644611462194

Epoch: 5| Step: 11
Training loss: 2.4313723496588397
Validation loss: 2.571217772696496

Epoch: 321| Step: 0
Training loss: 2.446750498365377
Validation loss: 2.5839738718262932

Epoch: 5| Step: 1
Training loss: 2.764962413318526
Validation loss: 2.5608882390378307

Epoch: 5| Step: 2
Training loss: 1.7676697765214844
Validation loss: 2.5675509922395596

Epoch: 5| Step: 3
Training loss: 1.9614563517696388
Validation loss: 2.5899178142898767

Epoch: 5| Step: 4
Training loss: 2.5710976743479814
Validation loss: 2.588836426074311

Epoch: 5| Step: 5
Training loss: 2.1160280776225218
Validation loss: 2.61389622772282

Epoch: 5| Step: 6
Training loss: 2.231281833795581
Validation loss: 2.5794260499051274

Epoch: 5| Step: 7
Training loss: 1.7425097522435329
Validation loss: 2.5641813228582144

Epoch: 5| Step: 8
Training loss: 2.5143870275230156
Validation loss: 2.564439094158371

Epoch: 5| Step: 9
Training loss: 1.9132279425625234
Validation loss: 2.583508786785711

Epoch: 5| Step: 10
Training loss: 2.075108683853616
Validation loss: 2.563835982057741

Epoch: 5| Step: 11
Training loss: 1.5298907416732477
Validation loss: 2.568464484439115

Epoch: 322| Step: 0
Training loss: 2.4366356711062234
Validation loss: 2.5749669818242715

Epoch: 5| Step: 1
Training loss: 1.8110217281367238
Validation loss: 2.5676495517607827

Epoch: 5| Step: 2
Training loss: 2.115189513966225
Validation loss: 2.587245556549769

Epoch: 5| Step: 3
Training loss: 1.9848321102833768
Validation loss: 2.569143336641059

Epoch: 5| Step: 4
Training loss: 2.150284185036019
Validation loss: 2.6106873332113345

Epoch: 5| Step: 5
Training loss: 2.19478288554309
Validation loss: 2.5796441659689973

Epoch: 5| Step: 6
Training loss: 2.136028617695754
Validation loss: 2.6053479617981834

Epoch: 5| Step: 7
Training loss: 2.345552793794628
Validation loss: 2.6007459884671777

Epoch: 5| Step: 8
Training loss: 2.217311338981202
Validation loss: 2.5836894010423648

Epoch: 5| Step: 9
Training loss: 2.4285206829507406
Validation loss: 2.591335705009784

Epoch: 5| Step: 10
Training loss: 2.0472796091185486
Validation loss: 2.5866640881479657

Epoch: 5| Step: 11
Training loss: 2.0848783740239276
Validation loss: 2.586291900659174

Epoch: 323| Step: 0
Training loss: 2.1499559619740434
Validation loss: 2.610138296084411

Epoch: 5| Step: 1
Training loss: 1.7327299822464355
Validation loss: 2.614413093759324

Epoch: 5| Step: 2
Training loss: 2.407530109696545
Validation loss: 2.585212336931201

Epoch: 5| Step: 3
Training loss: 1.8806790496168788
Validation loss: 2.608768434512715

Epoch: 5| Step: 4
Training loss: 2.4782273145145517
Validation loss: 2.573528982704725

Epoch: 5| Step: 5
Training loss: 2.3382945337246404
Validation loss: 2.5712466566638663

Epoch: 5| Step: 6
Training loss: 2.14000887949162
Validation loss: 2.5707705706988566

Epoch: 5| Step: 7
Training loss: 2.4320116120111415
Validation loss: 2.5623184038961067

Epoch: 5| Step: 8
Training loss: 2.0889742459562672
Validation loss: 2.5652792985387607

Epoch: 5| Step: 9
Training loss: 1.7405713030273253
Validation loss: 2.585531496750599

Epoch: 5| Step: 10
Training loss: 2.435660132472161
Validation loss: 2.5837286421101155

Epoch: 5| Step: 11
Training loss: 1.907242875974298
Validation loss: 2.576605640032489

Epoch: 324| Step: 0
Training loss: 2.287510368448742
Validation loss: 2.5826974837395884

Epoch: 5| Step: 1
Training loss: 2.0647960944563337
Validation loss: 2.616573757118713

Epoch: 5| Step: 2
Training loss: 2.2081607055368617
Validation loss: 2.651322738996142

Epoch: 5| Step: 3
Training loss: 2.179960391008508
Validation loss: 2.5928784940021417

Epoch: 5| Step: 4
Training loss: 1.7276656825196333
Validation loss: 2.563238646108541

Epoch: 5| Step: 5
Training loss: 2.1149781583555525
Validation loss: 2.5667648529391354

Epoch: 5| Step: 6
Training loss: 2.014284974825645
Validation loss: 2.564440860607593

Epoch: 5| Step: 7
Training loss: 2.28303562460335
Validation loss: 2.567630292055975

Epoch: 5| Step: 8
Training loss: 2.772652184426172
Validation loss: 2.5719724433519255

Epoch: 5| Step: 9
Training loss: 2.44074492996316
Validation loss: 2.56189462442506

Epoch: 5| Step: 10
Training loss: 1.9618404179028892
Validation loss: 2.5524801270187356

Epoch: 5| Step: 11
Training loss: 2.6484380401340712
Validation loss: 2.5843397892279625

Epoch: 325| Step: 0
Training loss: 1.7611219659839137
Validation loss: 2.633935079651213

Epoch: 5| Step: 1
Training loss: 2.1411960703243094
Validation loss: 2.705635643163614

Epoch: 5| Step: 2
Training loss: 2.7964090140246403
Validation loss: 2.685978310811084

Epoch: 5| Step: 3
Training loss: 2.195023704348677
Validation loss: 2.7040699242527007

Epoch: 5| Step: 4
Training loss: 2.1820713803008434
Validation loss: 2.644597389374562

Epoch: 5| Step: 5
Training loss: 2.24943037239979
Validation loss: 2.6458999382606687

Epoch: 5| Step: 6
Training loss: 2.137017098472213
Validation loss: 2.578005994111583

Epoch: 5| Step: 7
Training loss: 1.8021738621311638
Validation loss: 2.5714098588922365

Epoch: 5| Step: 8
Training loss: 2.2067969273769665
Validation loss: 2.585132004751067

Epoch: 5| Step: 9
Training loss: 2.3599027333986236
Validation loss: 2.54843629672897

Epoch: 5| Step: 10
Training loss: 2.7284588271982972
Validation loss: 2.579279215446662

Epoch: 5| Step: 11
Training loss: 2.0017799563061964
Validation loss: 2.552016698525296

Epoch: 326| Step: 0
Training loss: 1.9462311808338386
Validation loss: 2.558251965240466

Epoch: 5| Step: 1
Training loss: 2.175699941324671
Validation loss: 2.5703124536207014

Epoch: 5| Step: 2
Training loss: 1.7247296439342392
Validation loss: 2.564923931057213

Epoch: 5| Step: 3
Training loss: 2.6224272699880453
Validation loss: 2.5851165912929743

Epoch: 5| Step: 4
Training loss: 2.3566270197762216
Validation loss: 2.5649011457367283

Epoch: 5| Step: 5
Training loss: 2.1205270477347278
Validation loss: 2.605891946855797

Epoch: 5| Step: 6
Training loss: 2.538426340076974
Validation loss: 2.6117344580309276

Epoch: 5| Step: 7
Training loss: 1.7977971903273262
Validation loss: 2.640887354621945

Epoch: 5| Step: 8
Training loss: 2.145220425179525
Validation loss: 2.5885234514870965

Epoch: 5| Step: 9
Training loss: 2.0754967611935937
Validation loss: 2.6129216819678147

Epoch: 5| Step: 10
Training loss: 2.1066395294678917
Validation loss: 2.5762222165579707

Epoch: 5| Step: 11
Training loss: 3.221736374397086
Validation loss: 2.572691959087327

Epoch: 327| Step: 0
Training loss: 1.8666745043771686
Validation loss: 2.578891183693

Epoch: 5| Step: 1
Training loss: 2.53324453633268
Validation loss: 2.603551667391551

Epoch: 5| Step: 2
Training loss: 2.688761304249004
Validation loss: 2.6188829348804465

Epoch: 5| Step: 3
Training loss: 1.7771295774762714
Validation loss: 2.6260453027571176

Epoch: 5| Step: 4
Training loss: 2.001167195195704
Validation loss: 2.626603329424562

Epoch: 5| Step: 5
Training loss: 2.3013470768383697
Validation loss: 2.627351370006487

Epoch: 5| Step: 6
Training loss: 2.1775763894838116
Validation loss: 2.5778777456687756

Epoch: 5| Step: 7
Training loss: 2.1224627213186693
Validation loss: 2.5580214619942883

Epoch: 5| Step: 8
Training loss: 2.5424763895774602
Validation loss: 2.573010220287557

Epoch: 5| Step: 9
Training loss: 1.9580733214571278
Validation loss: 2.5663404296609746

Epoch: 5| Step: 10
Training loss: 2.1072329347213414
Validation loss: 2.562567221527428

Epoch: 5| Step: 11
Training loss: 1.7367910791351608
Validation loss: 2.576344372206787

Epoch: 328| Step: 0
Training loss: 2.0058584241112642
Validation loss: 2.5796361482570385

Epoch: 5| Step: 1
Training loss: 2.700574255733395
Validation loss: 2.5894775553669307

Epoch: 5| Step: 2
Training loss: 2.775270098351678
Validation loss: 2.5833109090713693

Epoch: 5| Step: 3
Training loss: 2.7379864195214494
Validation loss: 2.581439605965957

Epoch: 5| Step: 4
Training loss: 1.2319754443353372
Validation loss: 2.5825539972076963

Epoch: 5| Step: 5
Training loss: 1.571485697958833
Validation loss: 2.5791697379377956

Epoch: 5| Step: 6
Training loss: 2.258804261865204
Validation loss: 2.5723999305059646

Epoch: 5| Step: 7
Training loss: 2.4363151996435435
Validation loss: 2.5941804505703057

Epoch: 5| Step: 8
Training loss: 2.5083200291716956
Validation loss: 2.6345620949805295

Epoch: 5| Step: 9
Training loss: 1.7956641720124875
Validation loss: 2.6631482775313287

Epoch: 5| Step: 10
Training loss: 2.0436346598984927
Validation loss: 2.6280956539582503

Epoch: 5| Step: 11
Training loss: 3.3449011986799646
Validation loss: 2.6432283987160052

Epoch: 329| Step: 0
Training loss: 1.9753388246997863
Validation loss: 2.626561733429551

Epoch: 5| Step: 1
Training loss: 1.9649079996756416
Validation loss: 2.6321917291612014

Epoch: 5| Step: 2
Training loss: 1.8986081019224765
Validation loss: 2.6316566420366803

Epoch: 5| Step: 3
Training loss: 2.1164905484838075
Validation loss: 2.6166055212550683

Epoch: 5| Step: 4
Training loss: 2.1800949098767597
Validation loss: 2.587356120638114

Epoch: 5| Step: 5
Training loss: 2.4517369321932105
Validation loss: 2.5582089472073557

Epoch: 5| Step: 6
Training loss: 2.345049586798684
Validation loss: 2.530913465993129

Epoch: 5| Step: 7
Training loss: 1.7879629962684884
Validation loss: 2.571952570984909

Epoch: 5| Step: 8
Training loss: 2.2362241762481614
Validation loss: 2.550666374201873

Epoch: 5| Step: 9
Training loss: 2.6578850594404524
Validation loss: 2.5557087131633076

Epoch: 5| Step: 10
Training loss: 2.086770566399083
Validation loss: 2.544312469287512

Epoch: 5| Step: 11
Training loss: 2.6260348505408078
Validation loss: 2.5265862392426146

Epoch: 330| Step: 0
Training loss: 2.663008943010556
Validation loss: 2.5319550242725812

Epoch: 5| Step: 1
Training loss: 1.68972215219378
Validation loss: 2.523838944287203

Epoch: 5| Step: 2
Training loss: 1.9855583447251348
Validation loss: 2.514813027535435

Epoch: 5| Step: 3
Training loss: 2.3428353114224127
Validation loss: 2.5145269607782015

Epoch: 5| Step: 4
Training loss: 2.3970002580562193
Validation loss: 2.532590797474906

Epoch: 5| Step: 5
Training loss: 1.9092245813735407
Validation loss: 2.533484547947806

Epoch: 5| Step: 6
Training loss: 1.9733638891655285
Validation loss: 2.5333344624751066

Epoch: 5| Step: 7
Training loss: 1.9320929974392496
Validation loss: 2.5628557888617647

Epoch: 5| Step: 8
Training loss: 2.0609637232016658
Validation loss: 2.5373115919037477

Epoch: 5| Step: 9
Training loss: 1.9723411988322928
Validation loss: 2.54957008134773

Epoch: 5| Step: 10
Training loss: 2.6781818397214496
Validation loss: 2.5616621842052463

Epoch: 5| Step: 11
Training loss: 2.5656192681902517
Validation loss: 2.555162881373598

Epoch: 331| Step: 0
Training loss: 2.601261430794981
Validation loss: 2.6016968510086627

Epoch: 5| Step: 1
Training loss: 2.2883001626933734
Validation loss: 2.5967791192647125

Epoch: 5| Step: 2
Training loss: 2.394034752704263
Validation loss: 2.614651915928642

Epoch: 5| Step: 3
Training loss: 2.369106710746284
Validation loss: 2.6191549013921205

Epoch: 5| Step: 4
Training loss: 2.0832733399971946
Validation loss: 2.6162608655584556

Epoch: 5| Step: 5
Training loss: 1.8271446533816373
Validation loss: 2.5987956136908674

Epoch: 5| Step: 6
Training loss: 2.7222790668893007
Validation loss: 2.585242301866745

Epoch: 5| Step: 7
Training loss: 1.501123802096273
Validation loss: 2.5732937985576227

Epoch: 5| Step: 8
Training loss: 2.2432984049072635
Validation loss: 2.546209455094583

Epoch: 5| Step: 9
Training loss: 1.8754906330327288
Validation loss: 2.584374484608363

Epoch: 5| Step: 10
Training loss: 2.0955273500303786
Validation loss: 2.558499396139887

Epoch: 5| Step: 11
Training loss: 1.9435802545055183
Validation loss: 2.564001333625653

Epoch: 332| Step: 0
Training loss: 1.8998828299683754
Validation loss: 2.5847273700541455

Epoch: 5| Step: 1
Training loss: 2.4671631553951503
Validation loss: 2.586274888539758

Epoch: 5| Step: 2
Training loss: 2.578168371587123
Validation loss: 2.57946086529571

Epoch: 5| Step: 3
Training loss: 1.7584788098049635
Validation loss: 2.559506148322798

Epoch: 5| Step: 4
Training loss: 1.894826901363225
Validation loss: 2.5671764478021837

Epoch: 5| Step: 5
Training loss: 2.4418552321528972
Validation loss: 2.561992455437873

Epoch: 5| Step: 6
Training loss: 1.9008626008724645
Validation loss: 2.5687171870515813

Epoch: 5| Step: 7
Training loss: 2.27737452909893
Validation loss: 2.5722518878049545

Epoch: 5| Step: 8
Training loss: 2.1318653504827907
Validation loss: 2.5995096273031795

Epoch: 5| Step: 9
Training loss: 1.8521054916286415
Validation loss: 2.5750682125809474

Epoch: 5| Step: 10
Training loss: 1.9114790316785464
Validation loss: 2.5891442214816376

Epoch: 5| Step: 11
Training loss: 2.8102696370169284
Validation loss: 2.5909278445665316

Epoch: 333| Step: 0
Training loss: 2.389849169133587
Validation loss: 2.5967741154409434

Epoch: 5| Step: 1
Training loss: 2.295081443920567
Validation loss: 2.6044549909561856

Epoch: 5| Step: 2
Training loss: 2.3956571984161252
Validation loss: 2.5958371248620655

Epoch: 5| Step: 3
Training loss: 2.1571489754466446
Validation loss: 2.6277212501857043

Epoch: 5| Step: 4
Training loss: 2.11603157047308
Validation loss: 2.583536689327361

Epoch: 5| Step: 5
Training loss: 2.1843929568358957
Validation loss: 2.5761890039652977

Epoch: 5| Step: 6
Training loss: 2.0119624018104383
Validation loss: 2.55417002143038

Epoch: 5| Step: 7
Training loss: 2.3482644916494606
Validation loss: 2.5469554662182268

Epoch: 5| Step: 8
Training loss: 1.868566410908726
Validation loss: 2.523807061595607

Epoch: 5| Step: 9
Training loss: 1.590370184792369
Validation loss: 2.5012135739067403

Epoch: 5| Step: 10
Training loss: 1.8966193264090905
Validation loss: 2.5243195526513897

Epoch: 5| Step: 11
Training loss: 2.6782153119914116
Validation loss: 2.527012091311476

Epoch: 334| Step: 0
Training loss: 2.3377704892248357
Validation loss: 2.538719392726908

Epoch: 5| Step: 1
Training loss: 2.4981565354460957
Validation loss: 2.521989345833824

Epoch: 5| Step: 2
Training loss: 2.240858687766549
Validation loss: 2.5393478898292754

Epoch: 5| Step: 3
Training loss: 2.2236854570445295
Validation loss: 2.5440320098415605

Epoch: 5| Step: 4
Training loss: 2.297906760652367
Validation loss: 2.578864941231957

Epoch: 5| Step: 5
Training loss: 1.964914430594553
Validation loss: 2.637307067039825

Epoch: 5| Step: 6
Training loss: 2.09203325177347
Validation loss: 2.6255043960558218

Epoch: 5| Step: 7
Training loss: 1.8553942213650458
Validation loss: 2.6050910351833214

Epoch: 5| Step: 8
Training loss: 2.0555861215924986
Validation loss: 2.605622385691216

Epoch: 5| Step: 9
Training loss: 1.5062043305750767
Validation loss: 2.567798095795189

Epoch: 5| Step: 10
Training loss: 2.345313606033217
Validation loss: 2.565224614034521

Epoch: 5| Step: 11
Training loss: 1.7278349317369892
Validation loss: 2.561776458347724

Epoch: 335| Step: 0
Training loss: 1.7105281810518667
Validation loss: 2.557699665586119

Epoch: 5| Step: 1
Training loss: 2.390324592415116
Validation loss: 2.5603864334862556

Epoch: 5| Step: 2
Training loss: 1.7828265792197135
Validation loss: 2.556750239583766

Epoch: 5| Step: 3
Training loss: 2.795501456956069
Validation loss: 2.566102682011114

Epoch: 5| Step: 4
Training loss: 2.0231021323339076
Validation loss: 2.57689025672691

Epoch: 5| Step: 5
Training loss: 2.556358141115637
Validation loss: 2.5606178993522053

Epoch: 5| Step: 6
Training loss: 2.0773860219819937
Validation loss: 2.568889173587789

Epoch: 5| Step: 7
Training loss: 2.6370804150109595
Validation loss: 2.6066251911606018

Epoch: 5| Step: 8
Training loss: 1.8599163517549688
Validation loss: 2.620082044087316

Epoch: 5| Step: 9
Training loss: 2.012977458162488
Validation loss: 2.6332772539953857

Epoch: 5| Step: 10
Training loss: 1.510574341359781
Validation loss: 2.633635881755165

Epoch: 5| Step: 11
Training loss: 1.75278816183124
Validation loss: 2.6303916993132797

Epoch: 336| Step: 0
Training loss: 1.2168389032982962
Validation loss: 2.6045902797753007

Epoch: 5| Step: 1
Training loss: 2.2590669625106683
Validation loss: 2.5754848872428666

Epoch: 5| Step: 2
Training loss: 1.4288933221014244
Validation loss: 2.5829062736951562

Epoch: 5| Step: 3
Training loss: 2.353177349425219
Validation loss: 2.5691460278630993

Epoch: 5| Step: 4
Training loss: 1.65351130221149
Validation loss: 2.5927560732655754

Epoch: 5| Step: 5
Training loss: 2.794906434896861
Validation loss: 2.594393872387153

Epoch: 5| Step: 6
Training loss: 1.3208601227860464
Validation loss: 2.5513076939991137

Epoch: 5| Step: 7
Training loss: 2.6723512816198935
Validation loss: 2.5716008220578037

Epoch: 5| Step: 8
Training loss: 2.4598027595284324
Validation loss: 2.5784339796698843

Epoch: 5| Step: 9
Training loss: 2.2006188605931714
Validation loss: 2.592098418811568

Epoch: 5| Step: 10
Training loss: 2.071138382772151
Validation loss: 2.580841964017064

Epoch: 5| Step: 11
Training loss: 2.339347162552437
Validation loss: 2.610431499139691

Epoch: 337| Step: 0
Training loss: 1.706281903426266
Validation loss: 2.6032706639329315

Epoch: 5| Step: 1
Training loss: 2.23535113586715
Validation loss: 2.58354648484948

Epoch: 5| Step: 2
Training loss: 1.967757020034646
Validation loss: 2.575362935769118

Epoch: 5| Step: 3
Training loss: 1.9946795266870743
Validation loss: 2.5465281314662938

Epoch: 5| Step: 4
Training loss: 2.7206837584277968
Validation loss: 2.563000589414939

Epoch: 5| Step: 5
Training loss: 2.4996604688870843
Validation loss: 2.5389702882895056

Epoch: 5| Step: 6
Training loss: 2.372452021637256
Validation loss: 2.5205780259937254

Epoch: 5| Step: 7
Training loss: 1.9468522316962187
Validation loss: 2.510103290567893

Epoch: 5| Step: 8
Training loss: 2.162442148129366
Validation loss: 2.5098918541658177

Epoch: 5| Step: 9
Training loss: 2.156797256682488
Validation loss: 2.5302221621097116

Epoch: 5| Step: 10
Training loss: 1.865947487209759
Validation loss: 2.534058335570903

Epoch: 5| Step: 11
Training loss: 1.350390019415294
Validation loss: 2.5451409301009473

Epoch: 338| Step: 0
Training loss: 1.941453284570776
Validation loss: 2.5664790514292872

Epoch: 5| Step: 1
Training loss: 1.8689402250606768
Validation loss: 2.5396841491286586

Epoch: 5| Step: 2
Training loss: 1.9150251809200158
Validation loss: 2.529451319865334

Epoch: 5| Step: 3
Training loss: 1.8954507071350368
Validation loss: 2.5041837612865576

Epoch: 5| Step: 4
Training loss: 1.8620315935628013
Validation loss: 2.564526640396842

Epoch: 5| Step: 5
Training loss: 1.7556532419499127
Validation loss: 2.542933392022543

Epoch: 5| Step: 6
Training loss: 2.2067534955546853
Validation loss: 2.5567363840562876

Epoch: 5| Step: 7
Training loss: 1.872027074785148
Validation loss: 2.571921068430164

Epoch: 5| Step: 8
Training loss: 2.9057096573773635
Validation loss: 2.5792772126631496

Epoch: 5| Step: 9
Training loss: 2.086204368864818
Validation loss: 2.587402474290055

Epoch: 5| Step: 10
Training loss: 2.031922683836868
Validation loss: 2.6123148219782486

Epoch: 5| Step: 11
Training loss: 4.002508330661127
Validation loss: 2.61891718022017

Epoch: 339| Step: 0
Training loss: 2.25073929402984
Validation loss: 2.6689296647045944

Epoch: 5| Step: 1
Training loss: 2.16832734312637
Validation loss: 2.6872408172656836

Epoch: 5| Step: 2
Training loss: 2.1492822824209967
Validation loss: 2.6839354555263917

Epoch: 5| Step: 3
Training loss: 2.000630398581841
Validation loss: 2.644728716601565

Epoch: 5| Step: 4
Training loss: 2.064853827819594
Validation loss: 2.6437164321042914

Epoch: 5| Step: 5
Training loss: 1.655802756329401
Validation loss: 2.5976833561986443

Epoch: 5| Step: 6
Training loss: 1.991809465032252
Validation loss: 2.594648259285142

Epoch: 5| Step: 7
Training loss: 2.214252695813088
Validation loss: 2.5856251062127757

Epoch: 5| Step: 8
Training loss: 2.1193720396758597
Validation loss: 2.5722463071691997

Epoch: 5| Step: 9
Training loss: 1.9904525682154066
Validation loss: 2.571336405069905

Epoch: 5| Step: 10
Training loss: 2.368158424606691
Validation loss: 2.5737753502151746

Epoch: 5| Step: 11
Training loss: 1.658687165945235
Validation loss: 2.567475199352114

Epoch: 340| Step: 0
Training loss: 2.0166336020078894
Validation loss: 2.5590923260352696

Epoch: 5| Step: 1
Training loss: 2.4100955108457685
Validation loss: 2.5598254667890155

Epoch: 5| Step: 2
Training loss: 2.453942812526086
Validation loss: 2.579554286839055

Epoch: 5| Step: 3
Training loss: 2.242057557278342
Validation loss: 2.5640571019226073

Epoch: 5| Step: 4
Training loss: 1.7965486976569764
Validation loss: 2.5813475880560137

Epoch: 5| Step: 5
Training loss: 1.893336157069093
Validation loss: 2.5926958492760646

Epoch: 5| Step: 6
Training loss: 2.0991381874994572
Validation loss: 2.567913980869975

Epoch: 5| Step: 7
Training loss: 2.438757352050461
Validation loss: 2.6019678611473616

Epoch: 5| Step: 8
Training loss: 2.222275954497603
Validation loss: 2.5994183637116377

Epoch: 5| Step: 9
Training loss: 1.9525720042336259
Validation loss: 2.6321588038637844

Epoch: 5| Step: 10
Training loss: 1.7602986704389598
Validation loss: 2.636228663733648

Epoch: 5| Step: 11
Training loss: 2.3234000166413664
Validation loss: 2.592261438769615

Epoch: 341| Step: 0
Training loss: 2.549857423573757
Validation loss: 2.5682635000534977

Epoch: 5| Step: 1
Training loss: 1.814547598917982
Validation loss: 2.5627149204903117

Epoch: 5| Step: 2
Training loss: 2.8938527824323756
Validation loss: 2.566447652001868

Epoch: 5| Step: 3
Training loss: 2.4792982328323148
Validation loss: 2.570011697190874

Epoch: 5| Step: 4
Training loss: 1.7730076474768728
Validation loss: 2.5785506773663296

Epoch: 5| Step: 5
Training loss: 1.9313878278715517
Validation loss: 2.577033661559215

Epoch: 5| Step: 6
Training loss: 2.156478759130338
Validation loss: 2.5646002505250864

Epoch: 5| Step: 7
Training loss: 2.51811417347012
Validation loss: 2.554833193171927

Epoch: 5| Step: 8
Training loss: 1.5267140683196148
Validation loss: 2.554786874625137

Epoch: 5| Step: 9
Training loss: 1.839995489322275
Validation loss: 2.523442137470846

Epoch: 5| Step: 10
Training loss: 2.57478495320227
Validation loss: 2.533232336530296

Epoch: 5| Step: 11
Training loss: 2.4660685982672628
Validation loss: 2.5241154841547164

Epoch: 342| Step: 0
Training loss: 2.2436141416599455
Validation loss: 2.5416259013226443

Epoch: 5| Step: 1
Training loss: 2.3047533963368023
Validation loss: 2.5673249495602266

Epoch: 5| Step: 2
Training loss: 1.941901468097209
Validation loss: 2.581833897761126

Epoch: 5| Step: 3
Training loss: 2.20631632070307
Validation loss: 2.5655427367403347

Epoch: 5| Step: 4
Training loss: 1.7011885527221378
Validation loss: 2.535751288615416

Epoch: 5| Step: 5
Training loss: 2.3568350149165345
Validation loss: 2.546352477192424

Epoch: 5| Step: 6
Training loss: 2.5140732427714854
Validation loss: 2.562685277490026

Epoch: 5| Step: 7
Training loss: 2.256892350076535
Validation loss: 2.533091216898567

Epoch: 5| Step: 8
Training loss: 1.6854820902055911
Validation loss: 2.596865544644313

Epoch: 5| Step: 9
Training loss: 1.6946395030427053
Validation loss: 2.5877071905802844

Epoch: 5| Step: 10
Training loss: 2.285248270382804
Validation loss: 2.585872658937527

Epoch: 5| Step: 11
Training loss: 2.331930647170471
Validation loss: 2.566875205893331

Epoch: 343| Step: 0
Training loss: 1.8917854467846664
Validation loss: 2.597285932183345

Epoch: 5| Step: 1
Training loss: 2.1971194612843745
Validation loss: 2.5868411257273185

Epoch: 5| Step: 2
Training loss: 2.021213561268058
Validation loss: 2.561786245946191

Epoch: 5| Step: 3
Training loss: 2.495969002103281
Validation loss: 2.579448433482304

Epoch: 5| Step: 4
Training loss: 1.7657749483694645
Validation loss: 2.589188759026774

Epoch: 5| Step: 5
Training loss: 1.9668248164172455
Validation loss: 2.6255661490132445

Epoch: 5| Step: 6
Training loss: 2.3897002182975378
Validation loss: 2.5965095455670766

Epoch: 5| Step: 7
Training loss: 1.4315914129981855
Validation loss: 2.5917836859453005

Epoch: 5| Step: 8
Training loss: 2.7119326125524643
Validation loss: 2.581199415818113

Epoch: 5| Step: 9
Training loss: 2.0388340146022483
Validation loss: 2.559622350593357

Epoch: 5| Step: 10
Training loss: 1.8324146642240144
Validation loss: 2.582609926477179

Epoch: 5| Step: 11
Training loss: 1.9077651459286764
Validation loss: 2.5766602911897016

Epoch: 344| Step: 0
Training loss: 1.9201813344956724
Validation loss: 2.5780420887263373

Epoch: 5| Step: 1
Training loss: 1.6759367472746334
Validation loss: 2.5749271018317232

Epoch: 5| Step: 2
Training loss: 2.589360118604985
Validation loss: 2.5564399253844217

Epoch: 5| Step: 3
Training loss: 1.7136589249799186
Validation loss: 2.5644288092160368

Epoch: 5| Step: 4
Training loss: 2.231822656825805
Validation loss: 2.5819746868268947

Epoch: 5| Step: 5
Training loss: 2.076229062373851
Validation loss: 2.5843682959030265

Epoch: 5| Step: 6
Training loss: 2.8189536754838533
Validation loss: 2.5768206832333767

Epoch: 5| Step: 7
Training loss: 2.378225645725162
Validation loss: 2.5620977078196536

Epoch: 5| Step: 8
Training loss: 1.4755049859392189
Validation loss: 2.5766930312078196

Epoch: 5| Step: 9
Training loss: 2.41685398515868
Validation loss: 2.6082945659811485

Epoch: 5| Step: 10
Training loss: 1.8256457871478287
Validation loss: 2.624424928161608

Epoch: 5| Step: 11
Training loss: 1.6277820540810675
Validation loss: 2.617953716869748

Epoch: 345| Step: 0
Training loss: 2.491454396301505
Validation loss: 2.6306854658790004

Epoch: 5| Step: 1
Training loss: 1.794329357623045
Validation loss: 2.6147398515538334

Epoch: 5| Step: 2
Training loss: 1.6210936764636654
Validation loss: 2.6063550121396006

Epoch: 5| Step: 3
Training loss: 2.4671925327860915
Validation loss: 2.5428548378146876

Epoch: 5| Step: 4
Training loss: 1.8392848889141373
Validation loss: 2.579680037415476

Epoch: 5| Step: 5
Training loss: 2.2482623702100994
Validation loss: 2.55690637523094

Epoch: 5| Step: 6
Training loss: 2.0491220501601743
Validation loss: 2.571761336505228

Epoch: 5| Step: 7
Training loss: 2.1922813477007934
Validation loss: 2.5849340283960776

Epoch: 5| Step: 8
Training loss: 2.1892410978750707
Validation loss: 2.6086284803081856

Epoch: 5| Step: 9
Training loss: 1.9172940125914424
Validation loss: 2.6062452730948613

Epoch: 5| Step: 10
Training loss: 2.0488055494317474
Validation loss: 2.5770651054714238

Epoch: 5| Step: 11
Training loss: 1.1591903313242113
Validation loss: 2.5717585012347874

Epoch: 346| Step: 0
Training loss: 2.307070363427219
Validation loss: 2.567434177632124

Epoch: 5| Step: 1
Training loss: 2.3861829590209798
Validation loss: 2.5901749247163695

Epoch: 5| Step: 2
Training loss: 2.4231917216688816
Validation loss: 2.5731454521010675

Epoch: 5| Step: 3
Training loss: 2.181168685315574
Validation loss: 2.573411802508287

Epoch: 5| Step: 4
Training loss: 2.1968723606403064
Validation loss: 2.6055946262498924

Epoch: 5| Step: 5
Training loss: 1.891780783730347
Validation loss: 2.6214007232450713

Epoch: 5| Step: 6
Training loss: 1.4583514984452748
Validation loss: 2.6176088364061423

Epoch: 5| Step: 7
Training loss: 2.0843099466721324
Validation loss: 2.605036530491767

Epoch: 5| Step: 8
Training loss: 1.4655040015924459
Validation loss: 2.580427282379192

Epoch: 5| Step: 9
Training loss: 2.318461183724457
Validation loss: 2.6029500726219457

Epoch: 5| Step: 10
Training loss: 1.8337475641048506
Validation loss: 2.5607654896244427

Epoch: 5| Step: 11
Training loss: 3.5125569837841857
Validation loss: 2.5435499824973102

Epoch: 347| Step: 0
Training loss: 2.2369606689351498
Validation loss: 2.5646632530429985

Epoch: 5| Step: 1
Training loss: 1.8165779278748249
Validation loss: 2.571537410044889

Epoch: 5| Step: 2
Training loss: 2.25799591481031
Validation loss: 2.5622772647229497

Epoch: 5| Step: 3
Training loss: 2.1216371518120094
Validation loss: 2.6196964441263777

Epoch: 5| Step: 4
Training loss: 1.6204314935852215
Validation loss: 2.6806778185363505

Epoch: 5| Step: 5
Training loss: 2.175291927521496
Validation loss: 2.6752179000616496

Epoch: 5| Step: 6
Training loss: 2.6064658937752894
Validation loss: 2.6829354942475954

Epoch: 5| Step: 7
Training loss: 1.3900236908519799
Validation loss: 2.605137916008824

Epoch: 5| Step: 8
Training loss: 1.771828682107388
Validation loss: 2.5915008812854468

Epoch: 5| Step: 9
Training loss: 2.251074534371453
Validation loss: 2.568603933316369

Epoch: 5| Step: 10
Training loss: 2.438365978075459
Validation loss: 2.5661688916952636

Epoch: 5| Step: 11
Training loss: 3.325273465261829
Validation loss: 2.566669715739169

Epoch: 348| Step: 0
Training loss: 2.473042582964754
Validation loss: 2.551090722153023

Epoch: 5| Step: 1
Training loss: 1.5629411456107183
Validation loss: 2.5537588471197012

Epoch: 5| Step: 2
Training loss: 2.520983562774517
Validation loss: 2.58399598918447

Epoch: 5| Step: 3
Training loss: 2.1082463388500545
Validation loss: 2.605066686870072

Epoch: 5| Step: 4
Training loss: 2.523961818775054
Validation loss: 2.6577052898189706

Epoch: 5| Step: 5
Training loss: 1.9666405533280003
Validation loss: 2.707273966892962

Epoch: 5| Step: 6
Training loss: 2.294005489412639
Validation loss: 2.6974390753558746

Epoch: 5| Step: 7
Training loss: 2.312892880444296
Validation loss: 2.6463227395109388

Epoch: 5| Step: 8
Training loss: 2.283072801562913
Validation loss: 2.5843163909170603

Epoch: 5| Step: 9
Training loss: 1.7942811903818423
Validation loss: 2.53954042289434

Epoch: 5| Step: 10
Training loss: 2.1287616645641245
Validation loss: 2.5784077730099937

Epoch: 5| Step: 11
Training loss: 1.203783127753729
Validation loss: 2.540876489808465

Epoch: 349| Step: 0
Training loss: 2.357901005382559
Validation loss: 2.536930116072104

Epoch: 5| Step: 1
Training loss: 2.049578911867728
Validation loss: 2.536473046376966

Epoch: 5| Step: 2
Training loss: 2.0172236999998447
Validation loss: 2.54370422587657

Epoch: 5| Step: 3
Training loss: 2.0233670373379233
Validation loss: 2.535036649464132

Epoch: 5| Step: 4
Training loss: 2.6565515683591694
Validation loss: 2.539486087776236

Epoch: 5| Step: 5
Training loss: 2.476073881738663
Validation loss: 2.5171265553493

Epoch: 5| Step: 6
Training loss: 1.9872653966670282
Validation loss: 2.521720522561567

Epoch: 5| Step: 7
Training loss: 2.1150125403066284
Validation loss: 2.5169840669899344

Epoch: 5| Step: 8
Training loss: 2.40547276614561
Validation loss: 2.5000973761985663

Epoch: 5| Step: 9
Training loss: 2.1987128480480087
Validation loss: 2.5088590298136233

Epoch: 5| Step: 10
Training loss: 2.049270974765206
Validation loss: 2.5410400866406904

Epoch: 5| Step: 11
Training loss: 1.6409462023976313
Validation loss: 2.586376467937962

Epoch: 350| Step: 0
Training loss: 2.215378684918299
Validation loss: 2.5834214992757523

Epoch: 5| Step: 1
Training loss: 1.9693414919758894
Validation loss: 2.565667180013917

Epoch: 5| Step: 2
Training loss: 2.1471422764792125
Validation loss: 2.5335096469392084

Epoch: 5| Step: 3
Training loss: 2.148535375966564
Validation loss: 2.5280413089978055

Epoch: 5| Step: 4
Training loss: 1.7097878970394396
Validation loss: 2.530287335857116

Epoch: 5| Step: 5
Training loss: 2.542958718088171
Validation loss: 2.5375103084310977

Epoch: 5| Step: 6
Training loss: 2.269599532710428
Validation loss: 2.538463435782729

Epoch: 5| Step: 7
Training loss: 2.3338465693823416
Validation loss: 2.5279703636014172

Epoch: 5| Step: 8
Training loss: 1.9165480065586908
Validation loss: 2.545668704416907

Epoch: 5| Step: 9
Training loss: 2.063565643597458
Validation loss: 2.529196581331046

Epoch: 5| Step: 10
Training loss: 2.2766231489513826
Validation loss: 2.526603991392503

Epoch: 5| Step: 11
Training loss: 1.8785749052058036
Validation loss: 2.5491838657596633

Testing loss: 2.0859962705692

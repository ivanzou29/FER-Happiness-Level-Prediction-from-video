Epoch: 1| Step: 0
Training loss: 6.629581234970593
Validation loss: 5.838311178630176

Epoch: 6| Step: 1
Training loss: 6.024287340745558
Validation loss: 5.836738555279581

Epoch: 6| Step: 2
Training loss: 5.375424035667551
Validation loss: 5.83522823982403

Epoch: 6| Step: 3
Training loss: 5.338873350182789
Validation loss: 5.83369900601789

Epoch: 6| Step: 4
Training loss: 5.720715127804069
Validation loss: 5.832261077787926

Epoch: 6| Step: 5
Training loss: 6.766481975170886
Validation loss: 5.8307449683627945

Epoch: 6| Step: 6
Training loss: 7.033592274797135
Validation loss: 5.829214749286769

Epoch: 6| Step: 7
Training loss: 4.948377771140555
Validation loss: 5.82766389088022

Epoch: 6| Step: 8
Training loss: 6.728032854142865
Validation loss: 5.826140283133842

Epoch: 6| Step: 9
Training loss: 5.273103468876886
Validation loss: 5.82445862837042

Epoch: 6| Step: 10
Training loss: 6.014785985524353
Validation loss: 5.822866622718856

Epoch: 6| Step: 11
Training loss: 5.495674339618711
Validation loss: 5.82120021391124

Epoch: 6| Step: 12
Training loss: 5.646679278783363
Validation loss: 5.819550043127713

Epoch: 6| Step: 13
Training loss: 5.788682273908434
Validation loss: 5.817829491683594

Epoch: 2| Step: 0
Training loss: 5.29747415490392
Validation loss: 5.816015212879111

Epoch: 6| Step: 1
Training loss: 6.653174034357234
Validation loss: 5.814156436409064

Epoch: 6| Step: 2
Training loss: 6.341749054235317
Validation loss: 5.812206821499438

Epoch: 6| Step: 3
Training loss: 7.2336383222044525
Validation loss: 5.810141354896924

Epoch: 6| Step: 4
Training loss: 6.080174431557376
Validation loss: 5.808063085208167

Epoch: 6| Step: 5
Training loss: 6.0140892545561355
Validation loss: 5.806006807477242

Epoch: 6| Step: 6
Training loss: 6.311599516140773
Validation loss: 5.803524429634021

Epoch: 6| Step: 7
Training loss: 5.696910074181489
Validation loss: 5.801227755332339

Epoch: 6| Step: 8
Training loss: 5.780699997591832
Validation loss: 5.79862997492338

Epoch: 6| Step: 9
Training loss: 6.245040146245872
Validation loss: 5.795920842723825

Epoch: 6| Step: 10
Training loss: 5.422514160475438
Validation loss: 5.793078252812474

Epoch: 6| Step: 11
Training loss: 5.354527007370142
Validation loss: 5.7901680285856445

Epoch: 6| Step: 12
Training loss: 4.762568459807228
Validation loss: 5.787241042998858

Epoch: 6| Step: 13
Training loss: 5.205152145346106
Validation loss: 5.784018930844764

Epoch: 3| Step: 0
Training loss: 5.756112871390171
Validation loss: 5.78070005258369

Epoch: 6| Step: 1
Training loss: 6.178950404444588
Validation loss: 5.777279922033036

Epoch: 6| Step: 2
Training loss: 5.60736898186696
Validation loss: 5.773555604710039

Epoch: 6| Step: 3
Training loss: 5.679027912612031
Validation loss: 5.769585190996294

Epoch: 6| Step: 4
Training loss: 6.22607910117765
Validation loss: 5.765573006854689

Epoch: 6| Step: 5
Training loss: 5.348291798199814
Validation loss: 5.760940702675603

Epoch: 6| Step: 6
Training loss: 6.2259694275269295
Validation loss: 5.756551962576506

Epoch: 6| Step: 7
Training loss: 6.709769274956072
Validation loss: 5.75180185806685

Epoch: 6| Step: 8
Training loss: 5.53262984311774
Validation loss: 5.746827563417967

Epoch: 6| Step: 9
Training loss: 6.700558186523443
Validation loss: 5.741967437056551

Epoch: 6| Step: 10
Training loss: 5.536642430983722
Validation loss: 5.7359364226155005

Epoch: 6| Step: 11
Training loss: 5.255433132277197
Validation loss: 5.730263576115304

Epoch: 6| Step: 12
Training loss: 4.7001250067786176
Validation loss: 5.724238556888828

Epoch: 6| Step: 13
Training loss: 6.343378666875954
Validation loss: 5.718067871991408

Epoch: 4| Step: 0
Training loss: 5.9006666808974
Validation loss: 5.711947760958397

Epoch: 6| Step: 1
Training loss: 5.688053984438159
Validation loss: 5.7053403967123995

Epoch: 6| Step: 2
Training loss: 5.7291502056463335
Validation loss: 5.6986070269342415

Epoch: 6| Step: 3
Training loss: 5.298174583326343
Validation loss: 5.691545912810026

Epoch: 6| Step: 4
Training loss: 5.112355147643759
Validation loss: 5.6843758131309405

Epoch: 6| Step: 5
Training loss: 6.415871442919529
Validation loss: 5.676954224777842

Epoch: 6| Step: 6
Training loss: 6.246032065635062
Validation loss: 5.669048547366074

Epoch: 6| Step: 7
Training loss: 5.101133833863919
Validation loss: 5.661374463182764

Epoch: 6| Step: 8
Training loss: 6.015619926945913
Validation loss: 5.65311405369908

Epoch: 6| Step: 9
Training loss: 5.410512842208051
Validation loss: 5.645046427974786

Epoch: 6| Step: 10
Training loss: 5.080277863960674
Validation loss: 5.636803682550531

Epoch: 6| Step: 11
Training loss: 6.002562929178668
Validation loss: 5.628169600786429

Epoch: 6| Step: 12
Training loss: 5.905888622066942
Validation loss: 5.619585412323631

Epoch: 6| Step: 13
Training loss: 6.714130747667738
Validation loss: 5.61049698258555

Epoch: 5| Step: 0
Training loss: 5.483596094361676
Validation loss: 5.601559123344591

Epoch: 6| Step: 1
Training loss: 5.427657416128571
Validation loss: 5.592811292597136

Epoch: 6| Step: 2
Training loss: 6.188083813800609
Validation loss: 5.583930861350201

Epoch: 6| Step: 3
Training loss: 4.802228791632849
Validation loss: 5.575085122409433

Epoch: 6| Step: 4
Training loss: 5.286198210932745
Validation loss: 5.566434690168574

Epoch: 6| Step: 5
Training loss: 5.89695791925091
Validation loss: 5.557411295048035

Epoch: 6| Step: 6
Training loss: 6.029926688537816
Validation loss: 5.549098068128756

Epoch: 6| Step: 7
Training loss: 5.919368001865056
Validation loss: 5.540541407941944

Epoch: 6| Step: 8
Training loss: 5.483364262054008
Validation loss: 5.532089916762708

Epoch: 6| Step: 9
Training loss: 5.597932243065221
Validation loss: 5.523879836093786

Epoch: 6| Step: 10
Training loss: 5.268453953074285
Validation loss: 5.515532092067483

Epoch: 6| Step: 11
Training loss: 5.322582953556041
Validation loss: 5.507472828160076

Epoch: 6| Step: 12
Training loss: 6.429250227194982
Validation loss: 5.4991034441695055

Epoch: 6| Step: 13
Training loss: 5.937070810968413
Validation loss: 5.491144275662047

Epoch: 6| Step: 0
Training loss: 4.144368332978445
Validation loss: 5.483800468528521

Epoch: 6| Step: 1
Training loss: 5.224746822082234
Validation loss: 5.476093063514814

Epoch: 6| Step: 2
Training loss: 5.974118679281541
Validation loss: 5.468854340057459

Epoch: 6| Step: 3
Training loss: 5.7971454567551906
Validation loss: 5.461775919559064

Epoch: 6| Step: 4
Training loss: 5.527189844367895
Validation loss: 5.454346877876102

Epoch: 6| Step: 5
Training loss: 5.2425520609543215
Validation loss: 5.447449128589019

Epoch: 6| Step: 6
Training loss: 5.683388366134458
Validation loss: 5.440579328029405

Epoch: 6| Step: 7
Training loss: 4.694457523746566
Validation loss: 5.433722605344055

Epoch: 6| Step: 8
Training loss: 6.6901929905628705
Validation loss: 5.427158240486552

Epoch: 6| Step: 9
Training loss: 5.886075233364932
Validation loss: 5.420133606037002

Epoch: 6| Step: 10
Training loss: 5.744091481216162
Validation loss: 5.4135106894060865

Epoch: 6| Step: 11
Training loss: 5.764369093821516
Validation loss: 5.406733202793361

Epoch: 6| Step: 12
Training loss: 5.126954799106869
Validation loss: 5.400020472169852

Epoch: 6| Step: 13
Training loss: 5.875925843354523
Validation loss: 5.393171422168185

Epoch: 7| Step: 0
Training loss: 4.630761347022341
Validation loss: 5.386473334985057

Epoch: 6| Step: 1
Training loss: 5.648352067019021
Validation loss: 5.379756597096782

Epoch: 6| Step: 2
Training loss: 6.106167861245466
Validation loss: 5.373816012308106

Epoch: 6| Step: 3
Training loss: 5.114483720336388
Validation loss: 5.367334385120113

Epoch: 6| Step: 4
Training loss: 5.4502820143222435
Validation loss: 5.361331178609862

Epoch: 6| Step: 5
Training loss: 5.666616140402137
Validation loss: 5.35534442262955

Epoch: 6| Step: 6
Training loss: 5.69212873974612
Validation loss: 5.349623785130662

Epoch: 6| Step: 7
Training loss: 5.806680370596399
Validation loss: 5.343645596739159

Epoch: 6| Step: 8
Training loss: 4.9012159045628305
Validation loss: 5.338386357227014

Epoch: 6| Step: 9
Training loss: 5.004121226354021
Validation loss: 5.3329266850098005

Epoch: 6| Step: 10
Training loss: 5.613326781956949
Validation loss: 5.32744387452235

Epoch: 6| Step: 11
Training loss: 5.619494308038417
Validation loss: 5.321983967365676

Epoch: 6| Step: 12
Training loss: 5.0090147768222675
Validation loss: 5.316649697415208

Epoch: 6| Step: 13
Training loss: 6.085682071521217
Validation loss: 5.311338320110233

Epoch: 8| Step: 0
Training loss: 5.822025492469175
Validation loss: 5.306142615859366

Epoch: 6| Step: 1
Training loss: 5.381535659633615
Validation loss: 5.300987215287799

Epoch: 6| Step: 2
Training loss: 4.858107619279644
Validation loss: 5.295851647049967

Epoch: 6| Step: 3
Training loss: 5.537627772801063
Validation loss: 5.290952869642527

Epoch: 6| Step: 4
Training loss: 5.2102451121831415
Validation loss: 5.286143095893379

Epoch: 6| Step: 5
Training loss: 4.501138754976648
Validation loss: 5.280585884909252

Epoch: 6| Step: 6
Training loss: 5.716677764496258
Validation loss: 5.275798926334905

Epoch: 6| Step: 7
Training loss: 5.512767537862781
Validation loss: 5.270586100975741

Epoch: 6| Step: 8
Training loss: 5.655871130968427
Validation loss: 5.265516270572377

Epoch: 6| Step: 9
Training loss: 5.091946335191363
Validation loss: 5.260943526772595

Epoch: 6| Step: 10
Training loss: 6.655762273981389
Validation loss: 5.256030600282107

Epoch: 6| Step: 11
Training loss: 4.39999335028406
Validation loss: 5.251222226000499

Epoch: 6| Step: 12
Training loss: 5.846036902119033
Validation loss: 5.246454888754842

Epoch: 6| Step: 13
Training loss: 4.9454968555190915
Validation loss: 5.242126979942817

Epoch: 9| Step: 0
Training loss: 5.07778582100448
Validation loss: 5.237375505859964

Epoch: 6| Step: 1
Training loss: 5.6484979438977225
Validation loss: 5.233010519401911

Epoch: 6| Step: 2
Training loss: 4.77804837212202
Validation loss: 5.2284304441254275

Epoch: 6| Step: 3
Training loss: 7.1858765468679024
Validation loss: 5.223859467209498

Epoch: 6| Step: 4
Training loss: 5.680631073830852
Validation loss: 5.219606736570905

Epoch: 6| Step: 5
Training loss: 4.43009567398539
Validation loss: 5.214847041782669

Epoch: 6| Step: 6
Training loss: 5.315894983108258
Validation loss: 5.210590921035596

Epoch: 6| Step: 7
Training loss: 4.84439968551195
Validation loss: 5.206311598982358

Epoch: 6| Step: 8
Training loss: 5.224137683146445
Validation loss: 5.201645313297743

Epoch: 6| Step: 9
Training loss: 4.3164044824657015
Validation loss: 5.197576456906683

Epoch: 6| Step: 10
Training loss: 5.280227833080527
Validation loss: 5.193635032045444

Epoch: 6| Step: 11
Training loss: 5.340253254746361
Validation loss: 5.189593708401108

Epoch: 6| Step: 12
Training loss: 5.288879661836955
Validation loss: 5.185969421834031

Epoch: 6| Step: 13
Training loss: 5.680967834650191
Validation loss: 5.181728691563385

Epoch: 10| Step: 0
Training loss: 5.3246235459196285
Validation loss: 5.177603815064205

Epoch: 6| Step: 1
Training loss: 4.9762478284296865
Validation loss: 5.173454143467058

Epoch: 6| Step: 2
Training loss: 4.5253455267228455
Validation loss: 5.169884182329382

Epoch: 6| Step: 3
Training loss: 5.07559131503568
Validation loss: 5.166001471900903

Epoch: 6| Step: 4
Training loss: 5.526127571591159
Validation loss: 5.162295348365227

Epoch: 6| Step: 5
Training loss: 5.251525294028236
Validation loss: 5.15795016532136

Epoch: 6| Step: 6
Training loss: 6.473360197179796
Validation loss: 5.154086188775356

Epoch: 6| Step: 7
Training loss: 5.308799262486292
Validation loss: 5.150199623312979

Epoch: 6| Step: 8
Training loss: 4.614158915110801
Validation loss: 5.146034123548374

Epoch: 6| Step: 9
Training loss: 5.44487253871848
Validation loss: 5.142082216523223

Epoch: 6| Step: 10
Training loss: 5.8962834953568155
Validation loss: 5.138208379736755

Epoch: 6| Step: 11
Training loss: 4.373706299377324
Validation loss: 5.134077190198823

Epoch: 6| Step: 12
Training loss: 5.737016698224185
Validation loss: 5.130191685476901

Epoch: 6| Step: 13
Training loss: 4.957848255354344
Validation loss: 5.126225425787198

Epoch: 11| Step: 0
Training loss: 4.748261585347393
Validation loss: 5.122152079949901

Epoch: 6| Step: 1
Training loss: 5.01641896461167
Validation loss: 5.118059653779863

Epoch: 6| Step: 2
Training loss: 5.287722261415324
Validation loss: 5.1140151734478225

Epoch: 6| Step: 3
Training loss: 4.822053220944891
Validation loss: 5.109892808221646

Epoch: 6| Step: 4
Training loss: 5.142665284226258
Validation loss: 5.10561152139143

Epoch: 6| Step: 5
Training loss: 5.435074188371808
Validation loss: 5.101393567842523

Epoch: 6| Step: 6
Training loss: 4.904358711901078
Validation loss: 5.097146959063875

Epoch: 6| Step: 7
Training loss: 5.9419506859365265
Validation loss: 5.092613140154658

Epoch: 6| Step: 8
Training loss: 5.023303847366666
Validation loss: 5.0881795555857074

Epoch: 6| Step: 9
Training loss: 4.892307564874287
Validation loss: 5.083517456889183

Epoch: 6| Step: 10
Training loss: 5.2474333529081525
Validation loss: 5.079852345440871

Epoch: 6| Step: 11
Training loss: 6.302905051725312
Validation loss: 5.075137498868966

Epoch: 6| Step: 12
Training loss: 4.44343256292747
Validation loss: 5.071028162719621

Epoch: 6| Step: 13
Training loss: 5.571217595811344
Validation loss: 5.066455065340839

Epoch: 12| Step: 0
Training loss: 4.589334662591482
Validation loss: 5.061925188249018

Epoch: 6| Step: 1
Training loss: 4.863700879235459
Validation loss: 5.057739028499162

Epoch: 6| Step: 2
Training loss: 5.230261618378156
Validation loss: 5.053394433275367

Epoch: 6| Step: 3
Training loss: 5.735246056863095
Validation loss: 5.04931339216731

Epoch: 6| Step: 4
Training loss: 4.83052156590081
Validation loss: 5.044738411751928

Epoch: 6| Step: 5
Training loss: 5.601699040024354
Validation loss: 5.0404190957949995

Epoch: 6| Step: 6
Training loss: 4.576017864792637
Validation loss: 5.035837234444217

Epoch: 6| Step: 7
Training loss: 5.755310923034548
Validation loss: 5.0316917904688525

Epoch: 6| Step: 8
Training loss: 4.997863885437297
Validation loss: 5.027099192415536

Epoch: 6| Step: 9
Training loss: 5.275630331119212
Validation loss: 5.022200444454661

Epoch: 6| Step: 10
Training loss: 5.06310391061815
Validation loss: 5.017801004886695

Epoch: 6| Step: 11
Training loss: 3.8119736605171557
Validation loss: 5.013255604452514

Epoch: 6| Step: 12
Training loss: 5.8925353982126785
Validation loss: 5.008387270281019

Epoch: 6| Step: 13
Training loss: 5.589557461768168
Validation loss: 5.003536499561226

Epoch: 13| Step: 0
Training loss: 5.4839249559383365
Validation loss: 4.998790101849713

Epoch: 6| Step: 1
Training loss: 5.134992952128386
Validation loss: 4.993161484053461

Epoch: 6| Step: 2
Training loss: 4.365910488397603
Validation loss: 4.987732397288335

Epoch: 6| Step: 3
Training loss: 5.254987255193498
Validation loss: 4.982302626525237

Epoch: 6| Step: 4
Training loss: 5.525470364946884
Validation loss: 4.978006725259225

Epoch: 6| Step: 5
Training loss: 4.48260888600899
Validation loss: 4.973060641410658

Epoch: 6| Step: 6
Training loss: 4.5041349275207665
Validation loss: 4.968005142522537

Epoch: 6| Step: 7
Training loss: 4.312930762849865
Validation loss: 4.963387112314396

Epoch: 6| Step: 8
Training loss: 5.539179920410045
Validation loss: 4.958376005400497

Epoch: 6| Step: 9
Training loss: 5.6124281371003955
Validation loss: 4.953007164164183

Epoch: 6| Step: 10
Training loss: 5.3947734405490575
Validation loss: 4.947999341842866

Epoch: 6| Step: 11
Training loss: 5.643776813498302
Validation loss: 4.943433295071169

Epoch: 6| Step: 12
Training loss: 5.064434294218752
Validation loss: 4.938323970240354

Epoch: 6| Step: 13
Training loss: 4.651535822414157
Validation loss: 4.932710280037447

Epoch: 14| Step: 0
Training loss: 4.723295536109883
Validation loss: 4.927775371359307

Epoch: 6| Step: 1
Training loss: 5.601159840773128
Validation loss: 4.922650924959841

Epoch: 6| Step: 2
Training loss: 5.349258884899457
Validation loss: 4.918213563238738

Epoch: 6| Step: 3
Training loss: 4.303894302204781
Validation loss: 4.913250409793411

Epoch: 6| Step: 4
Training loss: 4.757344240093416
Validation loss: 4.908624904415713

Epoch: 6| Step: 5
Training loss: 4.138815891267878
Validation loss: 4.903293828608438

Epoch: 6| Step: 6
Training loss: 5.594786649152596
Validation loss: 4.8988704378534225

Epoch: 6| Step: 7
Training loss: 5.267355433627026
Validation loss: 4.893849570218327

Epoch: 6| Step: 8
Training loss: 4.340687282656449
Validation loss: 4.889037600817885

Epoch: 6| Step: 9
Training loss: 4.984073641764167
Validation loss: 4.884459064823523

Epoch: 6| Step: 10
Training loss: 5.654469583934002
Validation loss: 4.8800172011796645

Epoch: 6| Step: 11
Training loss: 4.8836093099856095
Validation loss: 4.8750250399997945

Epoch: 6| Step: 12
Training loss: 4.720930795541696
Validation loss: 4.869809835743457

Epoch: 6| Step: 13
Training loss: 5.653117624492062
Validation loss: 4.865159070772013

Epoch: 15| Step: 0
Training loss: 5.2224493563521595
Validation loss: 4.859947342606975

Epoch: 6| Step: 1
Training loss: 4.797156111342614
Validation loss: 4.85482843605982

Epoch: 6| Step: 2
Training loss: 4.9374499740963556
Validation loss: 4.850093862929635

Epoch: 6| Step: 3
Training loss: 4.6895670084568195
Validation loss: 4.845738484751916

Epoch: 6| Step: 4
Training loss: 5.249194855485875
Validation loss: 4.84023618505417

Epoch: 6| Step: 5
Training loss: 4.459158681369153
Validation loss: 4.8354384781612465

Epoch: 6| Step: 6
Training loss: 4.75493506001152
Validation loss: 4.830737825644685

Epoch: 6| Step: 7
Training loss: 4.984380357314911
Validation loss: 4.825609558025426

Epoch: 6| Step: 8
Training loss: 5.76313012374718
Validation loss: 4.819987990442196

Epoch: 6| Step: 9
Training loss: 4.645615896202969
Validation loss: 4.815570924898

Epoch: 6| Step: 10
Training loss: 5.282605781561197
Validation loss: 4.810226464865267

Epoch: 6| Step: 11
Training loss: 4.656113590572136
Validation loss: 4.805007981774366

Epoch: 6| Step: 12
Training loss: 4.840157043809657
Validation loss: 4.800134998648374

Epoch: 6| Step: 13
Training loss: 4.961257371577416
Validation loss: 4.794552550429896

Epoch: 16| Step: 0
Training loss: 5.253788262499521
Validation loss: 4.789893090230365

Epoch: 6| Step: 1
Training loss: 4.252647472876685
Validation loss: 4.784700997423484

Epoch: 6| Step: 2
Training loss: 5.152158651432548
Validation loss: 4.779521072714237

Epoch: 6| Step: 3
Training loss: 5.411830601693459
Validation loss: 4.774792900720546

Epoch: 6| Step: 4
Training loss: 4.608308313082237
Validation loss: 4.7689481228277

Epoch: 6| Step: 5
Training loss: 5.566937337603216
Validation loss: 4.764309927842821

Epoch: 6| Step: 6
Training loss: 4.4131787521012615
Validation loss: 4.759129701547749

Epoch: 6| Step: 7
Training loss: 5.023616900288502
Validation loss: 4.753596015447057

Epoch: 6| Step: 8
Training loss: 4.550512373025225
Validation loss: 4.748414394016583

Epoch: 6| Step: 9
Training loss: 4.4925195189801865
Validation loss: 4.743112657140286

Epoch: 6| Step: 10
Training loss: 5.545303165001212
Validation loss: 4.738267664249124

Epoch: 6| Step: 11
Training loss: 4.530925028756373
Validation loss: 4.733041573429003

Epoch: 6| Step: 12
Training loss: 5.206671528506645
Validation loss: 4.728208111429422

Epoch: 6| Step: 13
Training loss: 4.072304265778904
Validation loss: 4.723087666560728

Epoch: 17| Step: 0
Training loss: 4.90664885047774
Validation loss: 4.718917422661321

Epoch: 6| Step: 1
Training loss: 5.604394087968986
Validation loss: 4.713082655445948

Epoch: 6| Step: 2
Training loss: 6.00489416786717
Validation loss: 4.708812869247788

Epoch: 6| Step: 3
Training loss: 5.6377219551866995
Validation loss: 4.703015618726278

Epoch: 6| Step: 4
Training loss: 4.236886831386066
Validation loss: 4.697167324800866

Epoch: 6| Step: 5
Training loss: 4.78394236835481
Validation loss: 4.692508243044789

Epoch: 6| Step: 6
Training loss: 4.355569908438495
Validation loss: 4.687055812559002

Epoch: 6| Step: 7
Training loss: 4.191927718318324
Validation loss: 4.681871841613603

Epoch: 6| Step: 8
Training loss: 5.1780699538791115
Validation loss: 4.677782506303569

Epoch: 6| Step: 9
Training loss: 4.333283472996964
Validation loss: 4.672872787634659

Epoch: 6| Step: 10
Training loss: 4.120437873608055
Validation loss: 4.667348721025552

Epoch: 6| Step: 11
Training loss: 3.720889974392966
Validation loss: 4.662248268183985

Epoch: 6| Step: 12
Training loss: 4.607652038284598
Validation loss: 4.656134823733681

Epoch: 6| Step: 13
Training loss: 5.134942435858466
Validation loss: 4.651102348293461

Epoch: 18| Step: 0
Training loss: 4.820516275955102
Validation loss: 4.647770886747565

Epoch: 6| Step: 1
Training loss: 4.077529571444683
Validation loss: 4.64024726537691

Epoch: 6| Step: 2
Training loss: 4.0949924127045
Validation loss: 4.635807126634194

Epoch: 6| Step: 3
Training loss: 4.406101954793506
Validation loss: 4.630904612701683

Epoch: 6| Step: 4
Training loss: 5.014347472254966
Validation loss: 4.625924061058593

Epoch: 6| Step: 5
Training loss: 4.669320419170212
Validation loss: 4.6200771250762065

Epoch: 6| Step: 6
Training loss: 4.9864696537885695
Validation loss: 4.61501127143164

Epoch: 6| Step: 7
Training loss: 5.069250245344646
Validation loss: 4.610078264456126

Epoch: 6| Step: 8
Training loss: 5.304120624798893
Validation loss: 4.604448243703793

Epoch: 6| Step: 9
Training loss: 5.015969523068064
Validation loss: 4.59926037716364

Epoch: 6| Step: 10
Training loss: 4.0337537929917815
Validation loss: 4.593913726310977

Epoch: 6| Step: 11
Training loss: 4.410557150041171
Validation loss: 4.588203178099653

Epoch: 6| Step: 12
Training loss: 4.760070365257445
Validation loss: 4.582700003591405

Epoch: 6| Step: 13
Training loss: 5.428246527822893
Validation loss: 4.577858735935484

Epoch: 19| Step: 0
Training loss: 4.828801672469166
Validation loss: 4.57262950971736

Epoch: 6| Step: 1
Training loss: 5.167409207709592
Validation loss: 4.567724502633451

Epoch: 6| Step: 2
Training loss: 4.086994916874931
Validation loss: 4.562997834379491

Epoch: 6| Step: 3
Training loss: 4.24131515835135
Validation loss: 4.556684053151118

Epoch: 6| Step: 4
Training loss: 3.821584665245256
Validation loss: 4.55100626245955

Epoch: 6| Step: 5
Training loss: 5.1093798529458985
Validation loss: 4.544912589882947

Epoch: 6| Step: 6
Training loss: 4.412215719169931
Validation loss: 4.539225327530754

Epoch: 6| Step: 7
Training loss: 5.0742792679464666
Validation loss: 4.533690468427362

Epoch: 6| Step: 8
Training loss: 4.571267261383872
Validation loss: 4.528304692808318

Epoch: 6| Step: 9
Training loss: 4.863516560572292
Validation loss: 4.522861213408567

Epoch: 6| Step: 10
Training loss: 4.983846128142793
Validation loss: 4.517069039011493

Epoch: 6| Step: 11
Training loss: 4.452412223657531
Validation loss: 4.511604692963297

Epoch: 6| Step: 12
Training loss: 5.07370149884816
Validation loss: 4.505744375925569

Epoch: 6| Step: 13
Training loss: 4.37940844090116
Validation loss: 4.500073714888766

Epoch: 20| Step: 0
Training loss: 4.829404395996297
Validation loss: 4.494199369738201

Epoch: 6| Step: 1
Training loss: 5.354496373020661
Validation loss: 4.488729739489112

Epoch: 6| Step: 2
Training loss: 4.759137850668442
Validation loss: 4.482980012738926

Epoch: 6| Step: 3
Training loss: 3.939930362570112
Validation loss: 4.476524454945999

Epoch: 6| Step: 4
Training loss: 4.99534218319643
Validation loss: 4.470998985063458

Epoch: 6| Step: 5
Training loss: 4.579106665031013
Validation loss: 4.465544219873633

Epoch: 6| Step: 6
Training loss: 4.645491286659958
Validation loss: 4.460199387431341

Epoch: 6| Step: 7
Training loss: 4.261477511647268
Validation loss: 4.453850616766045

Epoch: 6| Step: 8
Training loss: 4.775209887620145
Validation loss: 4.448264216380004

Epoch: 6| Step: 9
Training loss: 4.371079786617554
Validation loss: 4.441427131811756

Epoch: 6| Step: 10
Training loss: 3.7585684477914456
Validation loss: 4.435549275912586

Epoch: 6| Step: 11
Training loss: 3.519041579854815
Validation loss: 4.430724473933348

Epoch: 6| Step: 12
Training loss: 5.0591683433319234
Validation loss: 4.424850559045957

Epoch: 6| Step: 13
Training loss: 4.986086560504417
Validation loss: 4.4192776190084295

Epoch: 21| Step: 0
Training loss: 3.8257686914693942
Validation loss: 4.4133775387784935

Epoch: 6| Step: 1
Training loss: 4.378828063321268
Validation loss: 4.40878417817061

Epoch: 6| Step: 2
Training loss: 3.669273721203323
Validation loss: 4.402310136307362

Epoch: 6| Step: 3
Training loss: 5.135753421948874
Validation loss: 4.396916091896832

Epoch: 6| Step: 4
Training loss: 4.74557590249722
Validation loss: 4.391606267601809

Epoch: 6| Step: 5
Training loss: 5.355587168468774
Validation loss: 4.385900668560469

Epoch: 6| Step: 6
Training loss: 4.395177686829477
Validation loss: 4.380320592553771

Epoch: 6| Step: 7
Training loss: 4.579704975470966
Validation loss: 4.374893786639701

Epoch: 6| Step: 8
Training loss: 4.26322226571804
Validation loss: 4.368613095155345

Epoch: 6| Step: 9
Training loss: 4.102900637370678
Validation loss: 4.363071364884708

Epoch: 6| Step: 10
Training loss: 4.274525505636835
Validation loss: 4.356656811991732

Epoch: 6| Step: 11
Training loss: 4.49099338748626
Validation loss: 4.352684949187845

Epoch: 6| Step: 12
Training loss: 5.121015535014836
Validation loss: 4.346931865584368

Epoch: 6| Step: 13
Training loss: 4.423371078763283
Validation loss: 4.340208242579927

Epoch: 22| Step: 0
Training loss: 4.961036693152026
Validation loss: 4.335084243536632

Epoch: 6| Step: 1
Training loss: 4.301450422651107
Validation loss: 4.328096300160796

Epoch: 6| Step: 2
Training loss: 4.375791641547204
Validation loss: 4.322428492669265

Epoch: 6| Step: 3
Training loss: 4.052826382615232
Validation loss: 4.316575432823414

Epoch: 6| Step: 4
Training loss: 4.03157172286946
Validation loss: 4.310570584710035

Epoch: 6| Step: 5
Training loss: 4.431733373795307
Validation loss: 4.304852380381495

Epoch: 6| Step: 6
Training loss: 4.7324383995532795
Validation loss: 4.299130267522596

Epoch: 6| Step: 7
Training loss: 3.2288538299300127
Validation loss: 4.293606730092513

Epoch: 6| Step: 8
Training loss: 5.183324397203076
Validation loss: 4.2882806821621084

Epoch: 6| Step: 9
Training loss: 4.241140668490244
Validation loss: 4.281972503955121

Epoch: 6| Step: 10
Training loss: 4.519612699811659
Validation loss: 4.277036909596862

Epoch: 6| Step: 11
Training loss: 4.949959209543798
Validation loss: 4.270626689594756

Epoch: 6| Step: 12
Training loss: 3.843960639950578
Validation loss: 4.265147069442816

Epoch: 6| Step: 13
Training loss: 4.744488781481881
Validation loss: 4.259556964169614

Epoch: 23| Step: 0
Training loss: 4.252072446074964
Validation loss: 4.253500656067492

Epoch: 6| Step: 1
Training loss: 5.309530168570654
Validation loss: 4.248223026759677

Epoch: 6| Step: 2
Training loss: 3.425807139386923
Validation loss: 4.241340341862206

Epoch: 6| Step: 3
Training loss: 3.931912529796126
Validation loss: 4.237460936869837

Epoch: 6| Step: 4
Training loss: 4.472086267925581
Validation loss: 4.230719796360973

Epoch: 6| Step: 5
Training loss: 3.8843469178342422
Validation loss: 4.224666581550618

Epoch: 6| Step: 6
Training loss: 5.1932771065785515
Validation loss: 4.218622427059361

Epoch: 6| Step: 7
Training loss: 5.241310012077152
Validation loss: 4.213338173509385

Epoch: 6| Step: 8
Training loss: 4.231540984342802
Validation loss: 4.207278009165677

Epoch: 6| Step: 9
Training loss: 4.488073863790116
Validation loss: 4.201072655048084

Epoch: 6| Step: 10
Training loss: 4.511216489900479
Validation loss: 4.19642424578381

Epoch: 6| Step: 11
Training loss: 3.394939718483772
Validation loss: 4.188918219906905

Epoch: 6| Step: 12
Training loss: 4.21117687783488
Validation loss: 4.182696248869721

Epoch: 6| Step: 13
Training loss: 3.7561532717419652
Validation loss: 4.177813281478807

Epoch: 24| Step: 0
Training loss: 4.335941459035355
Validation loss: 4.17179683845442

Epoch: 6| Step: 1
Training loss: 4.562391305961265
Validation loss: 4.166369077863712

Epoch: 6| Step: 2
Training loss: 4.1763245360303385
Validation loss: 4.160209089489845

Epoch: 6| Step: 3
Training loss: 3.7881716595337553
Validation loss: 4.1554327330965135

Epoch: 6| Step: 4
Training loss: 3.52158132579381
Validation loss: 4.150823908909196

Epoch: 6| Step: 5
Training loss: 4.64273999244532
Validation loss: 4.145614803567562

Epoch: 6| Step: 6
Training loss: 4.247409704370105
Validation loss: 4.139991663833801

Epoch: 6| Step: 7
Training loss: 3.5468423312527038
Validation loss: 4.135628157555797

Epoch: 6| Step: 8
Training loss: 3.724722545966826
Validation loss: 4.130551600131891

Epoch: 6| Step: 9
Training loss: 4.184345480577587
Validation loss: 4.1235471536700095

Epoch: 6| Step: 10
Training loss: 4.212584560915226
Validation loss: 4.117587201033344

Epoch: 6| Step: 11
Training loss: 4.430956246737046
Validation loss: 4.112538211726149

Epoch: 6| Step: 12
Training loss: 4.8368094648808055
Validation loss: 4.107107099926334

Epoch: 6| Step: 13
Training loss: 5.1855134032582635
Validation loss: 4.100725167195545

Epoch: 25| Step: 0
Training loss: 4.190185824259025
Validation loss: 4.094846855159777

Epoch: 6| Step: 1
Training loss: 3.546421912742294
Validation loss: 4.090539580974282

Epoch: 6| Step: 2
Training loss: 3.7837042292034133
Validation loss: 4.08519370978091

Epoch: 6| Step: 3
Training loss: 4.409699949406856
Validation loss: 4.078759194045246

Epoch: 6| Step: 4
Training loss: 4.416552896053955
Validation loss: 4.0723370516068105

Epoch: 6| Step: 5
Training loss: 4.56350487917199
Validation loss: 4.067810285253626

Epoch: 6| Step: 6
Training loss: 3.535611320416544
Validation loss: 4.061802750442993

Epoch: 6| Step: 7
Training loss: 4.543923170313288
Validation loss: 4.056434759586419

Epoch: 6| Step: 8
Training loss: 4.148005288807689
Validation loss: 4.05147666933411

Epoch: 6| Step: 9
Training loss: 3.6171631122203647
Validation loss: 4.045708938765848

Epoch: 6| Step: 10
Training loss: 4.591092748942732
Validation loss: 4.040474915870209

Epoch: 6| Step: 11
Training loss: 4.084137324894773
Validation loss: 4.03476689821518

Epoch: 6| Step: 12
Training loss: 4.730961440847427
Validation loss: 4.029432197402703

Epoch: 6| Step: 13
Training loss: 4.257252507612653
Validation loss: 4.023132273136202

Epoch: 26| Step: 0
Training loss: 4.039844429873275
Validation loss: 4.017409762951832

Epoch: 6| Step: 1
Training loss: 4.711723300056873
Validation loss: 4.012583192812573

Epoch: 6| Step: 2
Training loss: 3.6166757176252284
Validation loss: 4.0069293758844315

Epoch: 6| Step: 3
Training loss: 3.915907008312877
Validation loss: 4.001430811881797

Epoch: 6| Step: 4
Training loss: 3.8102400288103158
Validation loss: 3.9962707858180946

Epoch: 6| Step: 5
Training loss: 4.273887819766362
Validation loss: 3.9906793521454738

Epoch: 6| Step: 6
Training loss: 4.194417222182389
Validation loss: 3.9852279248822025

Epoch: 6| Step: 7
Training loss: 4.544354661170167
Validation loss: 3.980082804219455

Epoch: 6| Step: 8
Training loss: 4.391589510177758
Validation loss: 3.974349659675335

Epoch: 6| Step: 9
Training loss: 3.9523460615897448
Validation loss: 3.9686614099261943

Epoch: 6| Step: 10
Training loss: 3.8626556661403324
Validation loss: 3.9638450930827736

Epoch: 6| Step: 11
Training loss: 4.360630961968074
Validation loss: 3.9581188076847673

Epoch: 6| Step: 12
Training loss: 3.6989253571283944
Validation loss: 3.9526813444084987

Epoch: 6| Step: 13
Training loss: 4.09642581994243
Validation loss: 3.947357111239823

Epoch: 27| Step: 0
Training loss: 4.193880145672922
Validation loss: 3.9416952036711055

Epoch: 6| Step: 1
Training loss: 4.181264305375379
Validation loss: 3.937084297155566

Epoch: 6| Step: 2
Training loss: 3.867550734598369
Validation loss: 3.931597872869739

Epoch: 6| Step: 3
Training loss: 3.605167992178874
Validation loss: 3.926825024974237

Epoch: 6| Step: 4
Training loss: 4.287781571020293
Validation loss: 3.9221355863325913

Epoch: 6| Step: 5
Training loss: 4.439731520362509
Validation loss: 3.916772651591118

Epoch: 6| Step: 6
Training loss: 3.998283256247909
Validation loss: 3.9113527259166028

Epoch: 6| Step: 7
Training loss: 3.7948175829242303
Validation loss: 3.905809647217207

Epoch: 6| Step: 8
Training loss: 3.9816576498979934
Validation loss: 3.90069736782969

Epoch: 6| Step: 9
Training loss: 4.045563592113221
Validation loss: 3.8956083072945495

Epoch: 6| Step: 10
Training loss: 4.253551569953273
Validation loss: 3.891007329291574

Epoch: 6| Step: 11
Training loss: 4.11107574195654
Validation loss: 3.88545158854618

Epoch: 6| Step: 12
Training loss: 4.342872805495449
Validation loss: 3.8798582529282366

Epoch: 6| Step: 13
Training loss: 3.364888196110929
Validation loss: 3.8750913670750955

Epoch: 28| Step: 0
Training loss: 4.765766279283927
Validation loss: 3.8695553707031065

Epoch: 6| Step: 1
Training loss: 4.1033176118336225
Validation loss: 3.864729961619442

Epoch: 6| Step: 2
Training loss: 3.307151074562586
Validation loss: 3.8589770978116382

Epoch: 6| Step: 3
Training loss: 3.292286939164769
Validation loss: 3.854567113469791

Epoch: 6| Step: 4
Training loss: 4.93265382537497
Validation loss: 3.8496905066206475

Epoch: 6| Step: 5
Training loss: 3.9520006352474306
Validation loss: 3.8442873385349454

Epoch: 6| Step: 6
Training loss: 4.2453826056101915
Validation loss: 3.8388828383043974

Epoch: 6| Step: 7
Training loss: 4.465075282437244
Validation loss: 3.833975710464717

Epoch: 6| Step: 8
Training loss: 3.5951889474421312
Validation loss: 3.8282839372299353

Epoch: 6| Step: 9
Training loss: 3.915258898596872
Validation loss: 3.823887084252834

Epoch: 6| Step: 10
Training loss: 3.567550692822993
Validation loss: 3.8181059223946687

Epoch: 6| Step: 11
Training loss: 4.1384497344819815
Validation loss: 3.8132659267812454

Epoch: 6| Step: 12
Training loss: 3.316407400250095
Validation loss: 3.8086094834744464

Epoch: 6| Step: 13
Training loss: 3.570642902603821
Validation loss: 3.8033292148733837

Epoch: 29| Step: 0
Training loss: 3.632972237961545
Validation loss: 3.7985205204630215

Epoch: 6| Step: 1
Training loss: 3.369245922267592
Validation loss: 3.793989092076343

Epoch: 6| Step: 2
Training loss: 3.9277391096984764
Validation loss: 3.7893265569905044

Epoch: 6| Step: 3
Training loss: 4.348555726319628
Validation loss: 3.7844026732017033

Epoch: 6| Step: 4
Training loss: 3.635673331716035
Validation loss: 3.779478220240048

Epoch: 6| Step: 5
Training loss: 3.7167340111440215
Validation loss: 3.7743985233751927

Epoch: 6| Step: 6
Training loss: 3.9436287790279327
Validation loss: 3.7698321128517294

Epoch: 6| Step: 7
Training loss: 3.9768250983868
Validation loss: 3.764893900071606

Epoch: 6| Step: 8
Training loss: 3.4909637150229638
Validation loss: 3.7597077283488676

Epoch: 6| Step: 9
Training loss: 4.01080769535771
Validation loss: 3.755455943641865

Epoch: 6| Step: 10
Training loss: 4.219057651181038
Validation loss: 3.7508221254913705

Epoch: 6| Step: 11
Training loss: 3.518065016969278
Validation loss: 3.7464262994067283

Epoch: 6| Step: 12
Training loss: 4.34496043513187
Validation loss: 3.7413435948716725

Epoch: 6| Step: 13
Training loss: 4.307827615970827
Validation loss: 3.736300473493907

Epoch: 30| Step: 0
Training loss: 4.751531655168506
Validation loss: 3.731401225081412

Epoch: 6| Step: 1
Training loss: 3.9562371710257414
Validation loss: 3.726522022936511

Epoch: 6| Step: 2
Training loss: 4.407465124657624
Validation loss: 3.7212208251697896

Epoch: 6| Step: 3
Training loss: 4.179695714069031
Validation loss: 3.7160032370176475

Epoch: 6| Step: 4
Training loss: 3.3381106157391125
Validation loss: 3.710824679617247

Epoch: 6| Step: 5
Training loss: 3.5699426638916933
Validation loss: 3.7060621929134587

Epoch: 6| Step: 6
Training loss: 3.4341569770565825
Validation loss: 3.701582231813878

Epoch: 6| Step: 7
Training loss: 4.407102245400714
Validation loss: 3.6959467337555805

Epoch: 6| Step: 8
Training loss: 4.129356164879191
Validation loss: 3.6906237975572678

Epoch: 6| Step: 9
Training loss: 3.6595952307335327
Validation loss: 3.6858451276488844

Epoch: 6| Step: 10
Training loss: 3.362655386471969
Validation loss: 3.681040650715066

Epoch: 6| Step: 11
Training loss: 3.7651601164106516
Validation loss: 3.6761990025007556

Epoch: 6| Step: 12
Training loss: 3.0089369220302435
Validation loss: 3.6710467114533043

Epoch: 6| Step: 13
Training loss: 3.3255357296380263
Validation loss: 3.6657978820358434

Epoch: 31| Step: 0
Training loss: 4.86649108117083
Validation loss: 3.6613612082972224

Epoch: 6| Step: 1
Training loss: 3.2868144491783586
Validation loss: 3.6566907828922934

Epoch: 6| Step: 2
Training loss: 3.866221544251679
Validation loss: 3.651591732510457

Epoch: 6| Step: 3
Training loss: 4.537250623339441
Validation loss: 3.6467528718549276

Epoch: 6| Step: 4
Training loss: 3.166141533392444
Validation loss: 3.6419704172363367

Epoch: 6| Step: 5
Training loss: 2.7166418913041115
Validation loss: 3.636772551447606

Epoch: 6| Step: 6
Training loss: 3.8913876195897004
Validation loss: 3.6323617744444388

Epoch: 6| Step: 7
Training loss: 3.614851326952022
Validation loss: 3.6272027568814513

Epoch: 6| Step: 8
Training loss: 3.743853554543261
Validation loss: 3.6223144668373757

Epoch: 6| Step: 9
Training loss: 3.556483357957302
Validation loss: 3.616844957472021

Epoch: 6| Step: 10
Training loss: 3.8815650701487447
Validation loss: 3.6117958218504223

Epoch: 6| Step: 11
Training loss: 4.029716969057257
Validation loss: 3.606847255002742

Epoch: 6| Step: 12
Training loss: 3.8211616555316104
Validation loss: 3.6023783721616836

Epoch: 6| Step: 13
Training loss: 3.243810187867365
Validation loss: 3.5984555825803195

Epoch: 32| Step: 0
Training loss: 2.9868328099644574
Validation loss: 3.592799804679227

Epoch: 6| Step: 1
Training loss: 3.450949557558893
Validation loss: 3.5887631300461575

Epoch: 6| Step: 2
Training loss: 3.3484270416965116
Validation loss: 3.5841916335323623

Epoch: 6| Step: 3
Training loss: 4.234968418332357
Validation loss: 3.5790984871483773

Epoch: 6| Step: 4
Training loss: 3.3556013336455464
Validation loss: 3.573201953111997

Epoch: 6| Step: 5
Training loss: 3.1749173311299423
Validation loss: 3.5683836508963744

Epoch: 6| Step: 6
Training loss: 3.5945310324436877
Validation loss: 3.5640282280999624

Epoch: 6| Step: 7
Training loss: 3.906438960273384
Validation loss: 3.5597820733550343

Epoch: 6| Step: 8
Training loss: 3.5768305006896908
Validation loss: 3.555730735085965

Epoch: 6| Step: 9
Training loss: 4.051462997115016
Validation loss: 3.5503998437208404

Epoch: 6| Step: 10
Training loss: 4.548956430672237
Validation loss: 3.545220592793139

Epoch: 6| Step: 11
Training loss: 4.117321381391792
Validation loss: 3.5407160249812066

Epoch: 6| Step: 12
Training loss: 3.974451850187702
Validation loss: 3.5359832414586676

Epoch: 6| Step: 13
Training loss: 3.0661068909905898
Validation loss: 3.531246759193756

Epoch: 33| Step: 0
Training loss: 3.9371843968414084
Validation loss: 3.526524546853797

Epoch: 6| Step: 1
Training loss: 2.9653826438671183
Validation loss: 3.521640903169843

Epoch: 6| Step: 2
Training loss: 4.142275614396633
Validation loss: 3.516539594575162

Epoch: 6| Step: 3
Training loss: 3.509773233158142
Validation loss: 3.5120646893692102

Epoch: 6| Step: 4
Training loss: 2.9650475151796387
Validation loss: 3.5070160752504433

Epoch: 6| Step: 5
Training loss: 3.684632948629414
Validation loss: 3.5026823391989086

Epoch: 6| Step: 6
Training loss: 3.4364511450285407
Validation loss: 3.4982084730043415

Epoch: 6| Step: 7
Training loss: 2.67984319740598
Validation loss: 3.493436847316691

Epoch: 6| Step: 8
Training loss: 3.8547330422991135
Validation loss: 3.4893978164476236

Epoch: 6| Step: 9
Training loss: 4.25076780676403
Validation loss: 3.484667596092803

Epoch: 6| Step: 10
Training loss: 3.8924665267573295
Validation loss: 3.4801740653619166

Epoch: 6| Step: 11
Training loss: 4.019764707141607
Validation loss: 3.4749639257667186

Epoch: 6| Step: 12
Training loss: 3.331317928115477
Validation loss: 3.470628384465361

Epoch: 6| Step: 13
Training loss: 3.793555044478501
Validation loss: 3.4657220082168343

Epoch: 34| Step: 0
Training loss: 3.7754557959649264
Validation loss: 3.4611733774558013

Epoch: 6| Step: 1
Training loss: 3.78030005258165
Validation loss: 3.4563379544408015

Epoch: 6| Step: 2
Training loss: 3.38940632800903
Validation loss: 3.451719179133126

Epoch: 6| Step: 3
Training loss: 3.6383219625936367
Validation loss: 3.44675915161122

Epoch: 6| Step: 4
Training loss: 3.740520703384261
Validation loss: 3.4422632103742523

Epoch: 6| Step: 5
Training loss: 4.0285332566823016
Validation loss: 3.437445021680967

Epoch: 6| Step: 6
Training loss: 3.5454039358870686
Validation loss: 3.432871621452895

Epoch: 6| Step: 7
Training loss: 4.259723535440847
Validation loss: 3.4281928441426186

Epoch: 6| Step: 8
Training loss: 3.633809139582738
Validation loss: 3.4230182195370435

Epoch: 6| Step: 9
Training loss: 3.8566212578960184
Validation loss: 3.4190988877530004

Epoch: 6| Step: 10
Training loss: 2.787430486111678
Validation loss: 3.413891215505101

Epoch: 6| Step: 11
Training loss: 3.021365853212338
Validation loss: 3.4092041993823936

Epoch: 6| Step: 12
Training loss: 3.2193759939095545
Validation loss: 3.4051796867695883

Epoch: 6| Step: 13
Training loss: 3.0194968237647783
Validation loss: 3.400882867179585

Epoch: 35| Step: 0
Training loss: 3.5016141983839013
Validation loss: 3.396578564616545

Epoch: 6| Step: 1
Training loss: 3.6583634487525902
Validation loss: 3.3928068673560636

Epoch: 6| Step: 2
Training loss: 3.8251183983922314
Validation loss: 3.3882994404073536

Epoch: 6| Step: 3
Training loss: 3.1364745998731465
Validation loss: 3.383625804931608

Epoch: 6| Step: 4
Training loss: 3.3193902384128635
Validation loss: 3.3796483277878253

Epoch: 6| Step: 5
Training loss: 2.8516150064729286
Validation loss: 3.3749632185827294

Epoch: 6| Step: 6
Training loss: 3.957355271354011
Validation loss: 3.371095353085994

Epoch: 6| Step: 7
Training loss: 3.176187524462584
Validation loss: 3.366635140815908

Epoch: 6| Step: 8
Training loss: 3.4513600529660713
Validation loss: 3.3629833152977957

Epoch: 6| Step: 9
Training loss: 3.9548882332886697
Validation loss: 3.3588185877174324

Epoch: 6| Step: 10
Training loss: 3.5964660995415163
Validation loss: 3.354508653033037

Epoch: 6| Step: 11
Training loss: 3.2367082371450078
Validation loss: 3.3501808620614115

Epoch: 6| Step: 12
Training loss: 4.0703703716945165
Validation loss: 3.3460195332010403

Epoch: 6| Step: 13
Training loss: 3.1507437236735183
Validation loss: 3.341484823693728

Epoch: 36| Step: 0
Training loss: 3.2812795001929094
Validation loss: 3.3373641597234984

Epoch: 6| Step: 1
Training loss: 3.744334455158214
Validation loss: 3.3335672217011525

Epoch: 6| Step: 2
Training loss: 4.132205103640753
Validation loss: 3.329249351789192

Epoch: 6| Step: 3
Training loss: 3.115302947912994
Validation loss: 3.324960699781327

Epoch: 6| Step: 4
Training loss: 3.036085695899284
Validation loss: 3.3207661636687846

Epoch: 6| Step: 5
Training loss: 3.1894444537351507
Validation loss: 3.317289261073889

Epoch: 6| Step: 6
Training loss: 3.501202785360178
Validation loss: 3.3133412018790604

Epoch: 6| Step: 7
Training loss: 2.616956467181494
Validation loss: 3.3093457949234932

Epoch: 6| Step: 8
Training loss: 3.8362773426184016
Validation loss: 3.3056733184339273

Epoch: 6| Step: 9
Training loss: 3.560282904067404
Validation loss: 3.3018633451297617

Epoch: 6| Step: 10
Training loss: 3.6345401060562996
Validation loss: 3.2973224402557664

Epoch: 6| Step: 11
Training loss: 3.3014052664582754
Validation loss: 3.29322191911598

Epoch: 6| Step: 12
Training loss: 3.744958413151654
Validation loss: 3.289210807736703

Epoch: 6| Step: 13
Training loss: 3.3502207384405067
Validation loss: 3.285551926548461

Epoch: 37| Step: 0
Training loss: 3.670168216186749
Validation loss: 3.281289430411646

Epoch: 6| Step: 1
Training loss: 3.9386786256047723
Validation loss: 3.2780767203898016

Epoch: 6| Step: 2
Training loss: 3.6756133832717137
Validation loss: 3.2735842578556045

Epoch: 6| Step: 3
Training loss: 4.132183870846077
Validation loss: 3.269433606876232

Epoch: 6| Step: 4
Training loss: 3.4055394822765215
Validation loss: 3.2650469644078974

Epoch: 6| Step: 5
Training loss: 2.648077566169041
Validation loss: 3.2614041296614342

Epoch: 6| Step: 6
Training loss: 3.930205354639713
Validation loss: 3.257035245806771

Epoch: 6| Step: 7
Training loss: 3.9745546679281705
Validation loss: 3.252969852086174

Epoch: 6| Step: 8
Training loss: 2.7771370297101132
Validation loss: 3.248273231814144

Epoch: 6| Step: 9
Training loss: 2.734464284256607
Validation loss: 3.2442201424349375

Epoch: 6| Step: 10
Training loss: 2.954762161417056
Validation loss: 3.2400199783949413

Epoch: 6| Step: 11
Training loss: 2.8599589434257715
Validation loss: 3.236141293830171

Epoch: 6| Step: 12
Training loss: 3.300737477751554
Validation loss: 3.232445071964065

Epoch: 6| Step: 13
Training loss: 3.055995963368571
Validation loss: 3.2293860002087578

Epoch: 38| Step: 0
Training loss: 2.5401794793704147
Validation loss: 3.2256956005790145

Epoch: 6| Step: 1
Training loss: 3.6386493348402773
Validation loss: 3.222241578829566

Epoch: 6| Step: 2
Training loss: 4.008273152132895
Validation loss: 3.2183952012544896

Epoch: 6| Step: 3
Training loss: 2.9879474129294246
Validation loss: 3.214740391518613

Epoch: 6| Step: 4
Training loss: 3.719563755789949
Validation loss: 3.2111853312861705

Epoch: 6| Step: 5
Training loss: 3.6713071992838833
Validation loss: 3.2074197401439886

Epoch: 6| Step: 6
Training loss: 3.3245756179625063
Validation loss: 3.2037388655297514

Epoch: 6| Step: 7
Training loss: 3.593505452999183
Validation loss: 3.2006726933770655

Epoch: 6| Step: 8
Training loss: 3.1847306731416425
Validation loss: 3.197241638379467

Epoch: 6| Step: 9
Training loss: 3.4007685241531225
Validation loss: 3.193988255163299

Epoch: 6| Step: 10
Training loss: 3.0349439941970258
Validation loss: 3.1917101164035735

Epoch: 6| Step: 11
Training loss: 3.134152095091043
Validation loss: 3.1870455449080417

Epoch: 6| Step: 12
Training loss: 2.889286040451406
Validation loss: 3.1823748854990455

Epoch: 6| Step: 13
Training loss: 3.390143434984075
Validation loss: 3.1790546159175586

Epoch: 39| Step: 0
Training loss: 3.4046519319200392
Validation loss: 3.176154846263359

Epoch: 6| Step: 1
Training loss: 3.611639196141479
Validation loss: 3.172285707940434

Epoch: 6| Step: 2
Training loss: 3.4332523805524895
Validation loss: 3.1683669962960055

Epoch: 6| Step: 3
Training loss: 3.4238850860296735
Validation loss: 3.165231270639303

Epoch: 6| Step: 4
Training loss: 2.690308988048004
Validation loss: 3.1616228658899472

Epoch: 6| Step: 5
Training loss: 3.4867385579922034
Validation loss: 3.158636946834498

Epoch: 6| Step: 6
Training loss: 2.72403886626156
Validation loss: 3.155810826028833

Epoch: 6| Step: 7
Training loss: 3.2702471132845297
Validation loss: 3.1525644684405503

Epoch: 6| Step: 8
Training loss: 2.859687496264923
Validation loss: 3.1491718051205946

Epoch: 6| Step: 9
Training loss: 3.218641705218943
Validation loss: 3.1461022664161606

Epoch: 6| Step: 10
Training loss: 3.419951991586411
Validation loss: 3.1430434472631896

Epoch: 6| Step: 11
Training loss: 3.2531857648848783
Validation loss: 3.1392540414544223

Epoch: 6| Step: 12
Training loss: 3.4491593041486945
Validation loss: 3.1361797751730025

Epoch: 6| Step: 13
Training loss: 3.6757809905519303
Validation loss: 3.1326660604014287

Epoch: 40| Step: 0
Training loss: 3.0135490901753177
Validation loss: 3.128936894425888

Epoch: 6| Step: 1
Training loss: 3.50665957528716
Validation loss: 3.1254763558038126

Epoch: 6| Step: 2
Training loss: 2.6717150902439992
Validation loss: 3.1215948880998594

Epoch: 6| Step: 3
Training loss: 3.05052631402214
Validation loss: 3.1189026030140163

Epoch: 6| Step: 4
Training loss: 3.5216384659333233
Validation loss: 3.1150884739622384

Epoch: 6| Step: 5
Training loss: 3.247528310040071
Validation loss: 3.113283087946446

Epoch: 6| Step: 6
Training loss: 3.2880186476816764
Validation loss: 3.1094717013914686

Epoch: 6| Step: 7
Training loss: 3.2382035010268426
Validation loss: 3.1057031315099253

Epoch: 6| Step: 8
Training loss: 3.2144595689531656
Validation loss: 3.101871913847143

Epoch: 6| Step: 9
Training loss: 3.785505438130969
Validation loss: 3.098819547906044

Epoch: 6| Step: 10
Training loss: 2.9067344671910345
Validation loss: 3.095313733437793

Epoch: 6| Step: 11
Training loss: 3.11024133316254
Validation loss: 3.091928928661393

Epoch: 6| Step: 12
Training loss: 3.282400746875058
Validation loss: 3.088048285477405

Epoch: 6| Step: 13
Training loss: 3.4641341006287085
Validation loss: 3.0847886792044776

Epoch: 41| Step: 0
Training loss: 3.016907414336689
Validation loss: 3.0815872755770677

Epoch: 6| Step: 1
Training loss: 3.2647626550465536
Validation loss: 3.0777373481287795

Epoch: 6| Step: 2
Training loss: 3.439441479430881
Validation loss: 3.0751071084242994

Epoch: 6| Step: 3
Training loss: 2.651475092762801
Validation loss: 3.071270198794415

Epoch: 6| Step: 4
Training loss: 3.1134308859213
Validation loss: 3.067829321832342

Epoch: 6| Step: 5
Training loss: 2.227666641321353
Validation loss: 3.064800903217883

Epoch: 6| Step: 6
Training loss: 3.5794134360624854
Validation loss: 3.0617119430436475

Epoch: 6| Step: 7
Training loss: 3.300437412749788
Validation loss: 3.058819719836508

Epoch: 6| Step: 8
Training loss: 3.066243744449945
Validation loss: 3.055107744713344

Epoch: 6| Step: 9
Training loss: 3.44732911481017
Validation loss: 3.051430737681068

Epoch: 6| Step: 10
Training loss: 3.4970611768154196
Validation loss: 3.0494114808286845

Epoch: 6| Step: 11
Training loss: 3.1306388334675774
Validation loss: 3.046042304082745

Epoch: 6| Step: 12
Training loss: 3.0724327784014105
Validation loss: 3.0419399626419246

Epoch: 6| Step: 13
Training loss: 3.6971219493841643
Validation loss: 3.038806018613762

Epoch: 42| Step: 0
Training loss: 3.0257999150699963
Validation loss: 3.0361591451261045

Epoch: 6| Step: 1
Training loss: 3.7901598413021618
Validation loss: 3.0327103233518677

Epoch: 6| Step: 2
Training loss: 3.775330631353709
Validation loss: 3.029078174192247

Epoch: 6| Step: 3
Training loss: 3.547486643211592
Validation loss: 3.02592704849148

Epoch: 6| Step: 4
Training loss: 3.1010681834617895
Validation loss: 3.0232263272817406

Epoch: 6| Step: 5
Training loss: 2.632790585208775
Validation loss: 3.021184681596798

Epoch: 6| Step: 6
Training loss: 2.6674823904126566
Validation loss: 3.0188590778365154

Epoch: 6| Step: 7
Training loss: 3.08519169933657
Validation loss: 3.0148289342949344

Epoch: 6| Step: 8
Training loss: 2.535092110549865
Validation loss: 3.012716349935834

Epoch: 6| Step: 9
Training loss: 3.2668108817464727
Validation loss: 3.009904815736116

Epoch: 6| Step: 10
Training loss: 3.4396652338238334
Validation loss: 3.0067014813856914

Epoch: 6| Step: 11
Training loss: 2.527168940021603
Validation loss: 3.0040718631538286

Epoch: 6| Step: 12
Training loss: 3.280549183436561
Validation loss: 3.001884014564315

Epoch: 6| Step: 13
Training loss: 3.171938890837822
Validation loss: 2.9984081999072907

Epoch: 43| Step: 0
Training loss: 3.202986253664244
Validation loss: 2.995542950587946

Epoch: 6| Step: 1
Training loss: 3.0252308024481462
Validation loss: 2.993450246947647

Epoch: 6| Step: 2
Training loss: 2.8990890716522553
Validation loss: 2.9906589332071842

Epoch: 6| Step: 3
Training loss: 2.92512111576087
Validation loss: 2.9869589013651354

Epoch: 6| Step: 4
Training loss: 3.384046791907484
Validation loss: 2.986539937556434

Epoch: 6| Step: 5
Training loss: 3.382109606446049
Validation loss: 2.985190900814319

Epoch: 6| Step: 6
Training loss: 3.0405125627576526
Validation loss: 2.9842728202863187

Epoch: 6| Step: 7
Training loss: 2.8914853027223524
Validation loss: 2.979410112655681

Epoch: 6| Step: 8
Training loss: 3.1198052526484927
Validation loss: 2.9757576135384523

Epoch: 6| Step: 9
Training loss: 3.011455598608038
Validation loss: 2.971844729895454

Epoch: 6| Step: 10
Training loss: 3.099241988506272
Validation loss: 2.9688517101332064

Epoch: 6| Step: 11
Training loss: 3.32380503356064
Validation loss: 2.965202661635407

Epoch: 6| Step: 12
Training loss: 2.9568399949502346
Validation loss: 2.962939106714937

Epoch: 6| Step: 13
Training loss: 3.3498787018181018
Validation loss: 2.9598151556714467

Epoch: 44| Step: 0
Training loss: 2.5818574647808936
Validation loss: 2.9581851295277453

Epoch: 6| Step: 1
Training loss: 3.243784168923405
Validation loss: 2.956150785559614

Epoch: 6| Step: 2
Training loss: 2.447133516610836
Validation loss: 2.9528512440973476

Epoch: 6| Step: 3
Training loss: 3.6297272737896282
Validation loss: 2.953144322353977

Epoch: 6| Step: 4
Training loss: 2.6343194740751734
Validation loss: 2.9492269957976616

Epoch: 6| Step: 5
Training loss: 3.0618394411774443
Validation loss: 2.945699367867312

Epoch: 6| Step: 6
Training loss: 2.49420964108946
Validation loss: 2.9404882287741896

Epoch: 6| Step: 7
Training loss: 2.7451067652550285
Validation loss: 2.937739274580922

Epoch: 6| Step: 8
Training loss: 3.2250174825031834
Validation loss: 2.936049461699967

Epoch: 6| Step: 9
Training loss: 3.3617962295713517
Validation loss: 2.9347553687794616

Epoch: 6| Step: 10
Training loss: 3.773669502281721
Validation loss: 2.931724389574051

Epoch: 6| Step: 11
Training loss: 2.940306438902394
Validation loss: 2.9299322814406694

Epoch: 6| Step: 12
Training loss: 3.5360299226867045
Validation loss: 2.9273216494453984

Epoch: 6| Step: 13
Training loss: 3.056864477376194
Validation loss: 2.9246280368450526

Epoch: 45| Step: 0
Training loss: 3.15050898481552
Validation loss: 2.9220260200299335

Epoch: 6| Step: 1
Training loss: 3.2434967706581013
Validation loss: 2.9175029690990795

Epoch: 6| Step: 2
Training loss: 2.864379172708101
Validation loss: 2.91562042999182

Epoch: 6| Step: 3
Training loss: 3.0585210993726393
Validation loss: 2.913382715974957

Epoch: 6| Step: 4
Training loss: 3.479680341311535
Validation loss: 2.9098032897322326

Epoch: 6| Step: 5
Training loss: 3.004260375897696
Validation loss: 2.906724255331102

Epoch: 6| Step: 6
Training loss: 3.53972102558448
Validation loss: 2.903361714009416

Epoch: 6| Step: 7
Training loss: 2.683048376112889
Validation loss: 2.9011983960389043

Epoch: 6| Step: 8
Training loss: 2.5198032912424724
Validation loss: 2.8984605868809026

Epoch: 6| Step: 9
Training loss: 3.1201845833390123
Validation loss: 2.898769582419347

Epoch: 6| Step: 10
Training loss: 3.045481045062898
Validation loss: 2.893963262555479

Epoch: 6| Step: 11
Training loss: 2.167626486214872
Validation loss: 2.891705725091848

Epoch: 6| Step: 12
Training loss: 3.4098816156952783
Validation loss: 2.8896994681330685

Epoch: 6| Step: 13
Training loss: 3.075283849997694
Validation loss: 2.886959590681474

Epoch: 46| Step: 0
Training loss: 3.3298123519310012
Validation loss: 2.8861991748544913

Epoch: 6| Step: 1
Training loss: 2.543297999758615
Validation loss: 2.884045482809461

Epoch: 6| Step: 2
Training loss: 3.135282460801385
Validation loss: 2.8829834981237226

Epoch: 6| Step: 3
Training loss: 3.2256857209531944
Validation loss: 2.881930966837263

Epoch: 6| Step: 4
Training loss: 2.659777643762315
Validation loss: 2.8782952815023544

Epoch: 6| Step: 5
Training loss: 2.951868344924794
Validation loss: 2.8760797159362324

Epoch: 6| Step: 6
Training loss: 3.1626277731915744
Validation loss: 2.8733670393197825

Epoch: 6| Step: 7
Training loss: 2.675389883559541
Validation loss: 2.8714349037987157

Epoch: 6| Step: 8
Training loss: 3.157555489554649
Validation loss: 2.8683601081199908

Epoch: 6| Step: 9
Training loss: 3.2584997682593277
Validation loss: 2.8670381512320544

Epoch: 6| Step: 10
Training loss: 3.8796607260484985
Validation loss: 2.86433088172776

Epoch: 6| Step: 11
Training loss: 2.3872597139084784
Validation loss: 2.8603904463605128

Epoch: 6| Step: 12
Training loss: 2.8657728649998644
Validation loss: 2.8601678607159355

Epoch: 6| Step: 13
Training loss: 2.6346205663656845
Validation loss: 2.8557241650002485

Epoch: 47| Step: 0
Training loss: 3.648470958090697
Validation loss: 2.8534461223363543

Epoch: 6| Step: 1
Training loss: 2.90210534986774
Validation loss: 2.8519383849006155

Epoch: 6| Step: 2
Training loss: 3.267227874608556
Validation loss: 2.8501570491016923

Epoch: 6| Step: 3
Training loss: 3.2080332297794456
Validation loss: 2.847019886694608

Epoch: 6| Step: 4
Training loss: 3.120527347356236
Validation loss: 2.8442971677782882

Epoch: 6| Step: 5
Training loss: 2.8563863842746366
Validation loss: 2.8419219069269315

Epoch: 6| Step: 6
Training loss: 2.412555112851598
Validation loss: 2.839649975657727

Epoch: 6| Step: 7
Training loss: 2.688094317348251
Validation loss: 2.8388389268432306

Epoch: 6| Step: 8
Training loss: 2.8386946797324315
Validation loss: 2.8360144794759856

Epoch: 6| Step: 9
Training loss: 2.9198348322624126
Validation loss: 2.834773081198065

Epoch: 6| Step: 10
Training loss: 2.9714086573845218
Validation loss: 2.8324161382001636

Epoch: 6| Step: 11
Training loss: 2.682674067129971
Validation loss: 2.829661701772181

Epoch: 6| Step: 12
Training loss: 3.1900909963322945
Validation loss: 2.8296383063373978

Epoch: 6| Step: 13
Training loss: 2.833666520069087
Validation loss: 2.827248776532425

Epoch: 48| Step: 0
Training loss: 2.9576549199885407
Validation loss: 2.823959190924803

Epoch: 6| Step: 1
Training loss: 3.408147440895396
Validation loss: 2.821872058076899

Epoch: 6| Step: 2
Training loss: 3.168724412081183
Validation loss: 2.8167990852770144

Epoch: 6| Step: 3
Training loss: 3.1203175368480878
Validation loss: 2.8168518588639984

Epoch: 6| Step: 4
Training loss: 3.2231453819140863
Validation loss: 2.8215069975481426

Epoch: 6| Step: 5
Training loss: 2.4969263255319083
Validation loss: 2.812991049955604

Epoch: 6| Step: 6
Training loss: 2.871426725217528
Validation loss: 2.8105282123468287

Epoch: 6| Step: 7
Training loss: 2.8463511884324157
Validation loss: 2.808001205055543

Epoch: 6| Step: 8
Training loss: 3.1016453652638614
Validation loss: 2.8067514516690157

Epoch: 6| Step: 9
Training loss: 2.8425627525200143
Validation loss: 2.805279314229351

Epoch: 6| Step: 10
Training loss: 2.9595475369099886
Validation loss: 2.802599640730175

Epoch: 6| Step: 11
Training loss: 2.8808866855167348
Validation loss: 2.800172279371206

Epoch: 6| Step: 12
Training loss: 2.4592342234628304
Validation loss: 2.8010786351754935

Epoch: 6| Step: 13
Training loss: 2.847507886331437
Validation loss: 2.8049812694338976

Epoch: 49| Step: 0
Training loss: 2.838482684068005
Validation loss: 2.797402870189732

Epoch: 6| Step: 1
Training loss: 2.796662391807857
Validation loss: 2.7922233529431613

Epoch: 6| Step: 2
Training loss: 3.2149440151815947
Validation loss: 2.7912869313779716

Epoch: 6| Step: 3
Training loss: 3.239185828121252
Validation loss: 2.7914163610113505

Epoch: 6| Step: 4
Training loss: 2.4517957645694897
Validation loss: 2.7888179354826104

Epoch: 6| Step: 5
Training loss: 2.277733797201843
Validation loss: 2.788023255507079

Epoch: 6| Step: 6
Training loss: 3.3160896284923203
Validation loss: 2.788478230916324

Epoch: 6| Step: 7
Training loss: 3.0159143496830314
Validation loss: 2.786659067119232

Epoch: 6| Step: 8
Training loss: 2.9281073212471584
Validation loss: 2.7868986881056683

Epoch: 6| Step: 9
Training loss: 3.2573291593824956
Validation loss: 2.7859996144745636

Epoch: 6| Step: 10
Training loss: 2.736783955608797
Validation loss: 2.7800683643890993

Epoch: 6| Step: 11
Training loss: 2.738924786184352
Validation loss: 2.778455366037801

Epoch: 6| Step: 12
Training loss: 2.6514073827236313
Validation loss: 2.783183236748203

Epoch: 6| Step: 13
Training loss: 3.370450520950035
Validation loss: 2.7948720995551306

Epoch: 50| Step: 0
Training loss: 2.255680542830414
Validation loss: 2.7944854246268935

Epoch: 6| Step: 1
Training loss: 3.3975723743940476
Validation loss: 2.7969829886560955

Epoch: 6| Step: 2
Training loss: 2.6743357965707717
Validation loss: 2.7920415303977153

Epoch: 6| Step: 3
Training loss: 2.8220322033932437
Validation loss: 2.7885085694708796

Epoch: 6| Step: 4
Training loss: 2.2883258975418244
Validation loss: 2.798816099671225

Epoch: 6| Step: 5
Training loss: 3.1002332784187763
Validation loss: 2.7918537039279383

Epoch: 6| Step: 6
Training loss: 2.3020877981933747
Validation loss: 2.781424441921251

Epoch: 6| Step: 7
Training loss: 3.0999002686732062
Validation loss: 2.775721122934377

Epoch: 6| Step: 8
Training loss: 2.375918963479291
Validation loss: 2.7715734674042682

Epoch: 6| Step: 9
Training loss: 3.3484182124933013
Validation loss: 2.7705074826189806

Epoch: 6| Step: 10
Training loss: 3.494318847205077
Validation loss: 2.7708929002903293

Epoch: 6| Step: 11
Training loss: 3.342086806169269
Validation loss: 2.774561007031253

Epoch: 6| Step: 12
Training loss: 2.832638337409406
Validation loss: 2.7738584816800858

Epoch: 6| Step: 13
Training loss: 3.0093600168776153
Validation loss: 2.764807226394846

Epoch: 51| Step: 0
Training loss: 2.8617853730215748
Validation loss: 2.7612088032532176

Epoch: 6| Step: 1
Training loss: 2.1628652627982934
Validation loss: 2.7545395143326403

Epoch: 6| Step: 2
Training loss: 2.91404580937328
Validation loss: 2.751987562647704

Epoch: 6| Step: 3
Training loss: 2.8123983576739824
Validation loss: 2.748948864464583

Epoch: 6| Step: 4
Training loss: 2.992271003852767
Validation loss: 2.7455223360817493

Epoch: 6| Step: 5
Training loss: 2.9740152347997966
Validation loss: 2.7422068741015275

Epoch: 6| Step: 6
Training loss: 3.107363869960664
Validation loss: 2.7417448724493707

Epoch: 6| Step: 7
Training loss: 3.028473359059915
Validation loss: 2.7403136804025454

Epoch: 6| Step: 8
Training loss: 2.3959507678455534
Validation loss: 2.7356570771309063

Epoch: 6| Step: 9
Training loss: 3.0297619814912435
Validation loss: 2.7345995710887565

Epoch: 6| Step: 10
Training loss: 2.53736665873343
Validation loss: 2.733145809912897

Epoch: 6| Step: 11
Training loss: 3.2034056005619123
Validation loss: 2.732853929610205

Epoch: 6| Step: 12
Training loss: 3.098680363003811
Validation loss: 2.7299944275087675

Epoch: 6| Step: 13
Training loss: 3.009474733468736
Validation loss: 2.7272836192472263

Epoch: 52| Step: 0
Training loss: 2.925843017903982
Validation loss: 2.726392097577058

Epoch: 6| Step: 1
Training loss: 2.07305611248413
Validation loss: 2.725775327245098

Epoch: 6| Step: 2
Training loss: 3.613215793326026
Validation loss: 2.733503279016069

Epoch: 6| Step: 3
Training loss: 3.2332164513872246
Validation loss: 2.7214115605241624

Epoch: 6| Step: 4
Training loss: 2.8604965928578667
Validation loss: 2.718486363866772

Epoch: 6| Step: 5
Training loss: 2.7337419921590267
Validation loss: 2.7183964525009157

Epoch: 6| Step: 6
Training loss: 2.597692228369265
Validation loss: 2.718517732016199

Epoch: 6| Step: 7
Training loss: 3.0531045465945845
Validation loss: 2.717054854393268

Epoch: 6| Step: 8
Training loss: 2.324848711567539
Validation loss: 2.717200674529965

Epoch: 6| Step: 9
Training loss: 2.817848545236092
Validation loss: 2.7163355399301596

Epoch: 6| Step: 10
Training loss: 2.798892950144469
Validation loss: 2.716140225271481

Epoch: 6| Step: 11
Training loss: 2.7326565819608732
Validation loss: 2.715686460559661

Epoch: 6| Step: 12
Training loss: 2.792116005638095
Validation loss: 2.71506373111118

Epoch: 6| Step: 13
Training loss: 3.160989055128256
Validation loss: 2.7148711644533488

Epoch: 53| Step: 0
Training loss: 2.8266747903107325
Validation loss: 2.714396927104012

Epoch: 6| Step: 1
Training loss: 2.6564499723669703
Validation loss: 2.7122068191041278

Epoch: 6| Step: 2
Training loss: 2.3124309220177808
Validation loss: 2.710749243100349

Epoch: 6| Step: 3
Training loss: 2.8466798550064274
Validation loss: 2.706828144155027

Epoch: 6| Step: 4
Training loss: 2.6695826104916667
Validation loss: 2.7059785925579223

Epoch: 6| Step: 5
Training loss: 2.4099621563231377
Validation loss: 2.703022434883379

Epoch: 6| Step: 6
Training loss: 2.6116795214263826
Validation loss: 2.7005806857729078

Epoch: 6| Step: 7
Training loss: 3.1875170538950957
Validation loss: 2.6996354528091318

Epoch: 6| Step: 8
Training loss: 2.8985383460913896
Validation loss: 2.6990096825078185

Epoch: 6| Step: 9
Training loss: 3.508737285007967
Validation loss: 2.6981828084535846

Epoch: 6| Step: 10
Training loss: 2.78572648084158
Validation loss: 2.6939844222709906

Epoch: 6| Step: 11
Training loss: 3.0384327841626697
Validation loss: 2.6933766512723576

Epoch: 6| Step: 12
Training loss: 3.0261508486425845
Validation loss: 2.6921850737658386

Epoch: 6| Step: 13
Training loss: 2.7955169790588466
Validation loss: 2.6909195931049656

Epoch: 54| Step: 0
Training loss: 2.5126818387099883
Validation loss: 2.6879206187792226

Epoch: 6| Step: 1
Training loss: 3.0764846691169327
Validation loss: 2.68802294707343

Epoch: 6| Step: 2
Training loss: 2.9833650328019385
Validation loss: 2.688623858711291

Epoch: 6| Step: 3
Training loss: 2.514120374621541
Validation loss: 2.687392402683307

Epoch: 6| Step: 4
Training loss: 2.7262989343084567
Validation loss: 2.6869331323878125

Epoch: 6| Step: 5
Training loss: 3.5842547156215163
Validation loss: 2.6831889950886554

Epoch: 6| Step: 6
Training loss: 2.916287878962047
Validation loss: 2.683550497906808

Epoch: 6| Step: 7
Training loss: 2.5931735030037824
Validation loss: 2.681786488981045

Epoch: 6| Step: 8
Training loss: 2.7754083212731926
Validation loss: 2.679682309930667

Epoch: 6| Step: 9
Training loss: 1.9874239349204605
Validation loss: 2.6762837701502336

Epoch: 6| Step: 10
Training loss: 2.966849270747827
Validation loss: 2.6746177204102723

Epoch: 6| Step: 11
Training loss: 2.075820677831115
Validation loss: 2.671806557654326

Epoch: 6| Step: 12
Training loss: 3.019373486186244
Validation loss: 2.6696954206506724

Epoch: 6| Step: 13
Training loss: 3.3113836170626
Validation loss: 2.6694400541428442

Epoch: 55| Step: 0
Training loss: 2.6924648113306713
Validation loss: 2.6692169829108194

Epoch: 6| Step: 1
Training loss: 3.28136436626398
Validation loss: 2.6675791867916416

Epoch: 6| Step: 2
Training loss: 2.6765078706616703
Validation loss: 2.6672638333762477

Epoch: 6| Step: 3
Training loss: 3.3501881446955184
Validation loss: 2.6638140371434

Epoch: 6| Step: 4
Training loss: 2.554365598597352
Validation loss: 2.663280786283167

Epoch: 6| Step: 5
Training loss: 3.1839329825493694
Validation loss: 2.6625273038989414

Epoch: 6| Step: 6
Training loss: 3.200383795134144
Validation loss: 2.6600939145395848

Epoch: 6| Step: 7
Training loss: 1.4850714404970984
Validation loss: 2.6601608358484623

Epoch: 6| Step: 8
Training loss: 3.084828778775307
Validation loss: 2.6576045901942704

Epoch: 6| Step: 9
Training loss: 2.5373315163286967
Validation loss: 2.655827821578822

Epoch: 6| Step: 10
Training loss: 3.140165532602311
Validation loss: 2.65482603313761

Epoch: 6| Step: 11
Training loss: 2.6847791871497444
Validation loss: 2.664433189697435

Epoch: 6| Step: 12
Training loss: 2.0648134146348087
Validation loss: 2.658308450787684

Epoch: 6| Step: 13
Training loss: 2.6474630988325742
Validation loss: 2.655389549164394

Epoch: 56| Step: 0
Training loss: 2.4702588070095812
Validation loss: 2.6538569273139925

Epoch: 6| Step: 1
Training loss: 2.8713418658734757
Validation loss: 2.6526397255287355

Epoch: 6| Step: 2
Training loss: 3.0423299684555736
Validation loss: 2.6550385349156813

Epoch: 6| Step: 3
Training loss: 2.6551029196868505
Validation loss: 2.655009305330626

Epoch: 6| Step: 4
Training loss: 3.353203645123413
Validation loss: 2.655022341196588

Epoch: 6| Step: 5
Training loss: 3.2535351819858844
Validation loss: 2.654157839350691

Epoch: 6| Step: 6
Training loss: 2.7256683369956773
Validation loss: 2.652439960276021

Epoch: 6| Step: 7
Training loss: 2.2783830996224097
Validation loss: 2.6509029799322863

Epoch: 6| Step: 8
Training loss: 2.772578920525876
Validation loss: 2.6492342880304838

Epoch: 6| Step: 9
Training loss: 2.7818857387934526
Validation loss: 2.648416584723753

Epoch: 6| Step: 10
Training loss: 2.414266336964665
Validation loss: 2.645914036478671

Epoch: 6| Step: 11
Training loss: 2.538559708330933
Validation loss: 2.645240256653741

Epoch: 6| Step: 12
Training loss: 2.927316097539196
Validation loss: 2.64365526094222

Epoch: 6| Step: 13
Training loss: 2.77013783342642
Validation loss: 2.6426338665466744

Epoch: 57| Step: 0
Training loss: 2.980095156232055
Validation loss: 2.640827479678506

Epoch: 6| Step: 1
Training loss: 2.442173316997364
Validation loss: 2.639340406126853

Epoch: 6| Step: 2
Training loss: 2.4679470446514564
Validation loss: 2.6365531360748853

Epoch: 6| Step: 3
Training loss: 3.413698364671867
Validation loss: 2.6353534092806314

Epoch: 6| Step: 4
Training loss: 2.6517450165297625
Validation loss: 2.6360129233474265

Epoch: 6| Step: 5
Training loss: 2.7162559733213216
Validation loss: 2.6338445374489643

Epoch: 6| Step: 6
Training loss: 3.354747050680252
Validation loss: 2.6335596520513604

Epoch: 6| Step: 7
Training loss: 1.9373419297249228
Validation loss: 2.6307033765383743

Epoch: 6| Step: 8
Training loss: 2.8461828200596386
Validation loss: 2.630714191602339

Epoch: 6| Step: 9
Training loss: 2.85971717663168
Validation loss: 2.6325402123742103

Epoch: 6| Step: 10
Training loss: 3.022729125318918
Validation loss: 2.6283329134908313

Epoch: 6| Step: 11
Training loss: 2.6964269130994016
Validation loss: 2.6262223561481406

Epoch: 6| Step: 12
Training loss: 2.3983653303092494
Validation loss: 2.6245258902233672

Epoch: 6| Step: 13
Training loss: 2.668972290215374
Validation loss: 2.622131634876599

Epoch: 58| Step: 0
Training loss: 2.8809689465391415
Validation loss: 2.6233125136961806

Epoch: 6| Step: 1
Training loss: 3.0848419176154303
Validation loss: 2.6220925745146277

Epoch: 6| Step: 2
Training loss: 2.687373313579281
Validation loss: 2.6244952988221817

Epoch: 6| Step: 3
Training loss: 3.016165255048894
Validation loss: 2.623571234106579

Epoch: 6| Step: 4
Training loss: 2.137443684510908
Validation loss: 2.622873322782962

Epoch: 6| Step: 5
Training loss: 2.5623109329069593
Validation loss: 2.627487638754763

Epoch: 6| Step: 6
Training loss: 2.468608803876124
Validation loss: 2.6232350183246536

Epoch: 6| Step: 7
Training loss: 3.388645420465902
Validation loss: 2.6219407753466815

Epoch: 6| Step: 8
Training loss: 2.017629292255646
Validation loss: 2.6213766401118197

Epoch: 6| Step: 9
Training loss: 2.9880396527132174
Validation loss: 2.619895649863037

Epoch: 6| Step: 10
Training loss: 2.638465314187845
Validation loss: 2.628909938104923

Epoch: 6| Step: 11
Training loss: 3.1946782086481766
Validation loss: 2.624293595587055

Epoch: 6| Step: 12
Training loss: 2.435124339976803
Validation loss: 2.61520714682739

Epoch: 6| Step: 13
Training loss: 2.778941707476032
Validation loss: 2.6145189943541007

Epoch: 59| Step: 0
Training loss: 2.2647246577837516
Validation loss: 2.6144370890494675

Epoch: 6| Step: 1
Training loss: 2.864803976173486
Validation loss: 2.6146683748759374

Epoch: 6| Step: 2
Training loss: 2.6451742497756223
Validation loss: 2.6125578057131578

Epoch: 6| Step: 3
Training loss: 2.9544560092111323
Validation loss: 2.613254898615078

Epoch: 6| Step: 4
Training loss: 3.0360902505359446
Validation loss: 2.612265457381639

Epoch: 6| Step: 5
Training loss: 2.7868816493883903
Validation loss: 2.6101404616803143

Epoch: 6| Step: 6
Training loss: 2.935555808941412
Validation loss: 2.610096799266093

Epoch: 6| Step: 7
Training loss: 2.5944407818605253
Validation loss: 2.6081705111626543

Epoch: 6| Step: 8
Training loss: 2.7870502639415338
Validation loss: 2.608679322724903

Epoch: 6| Step: 9
Training loss: 2.280232241664812
Validation loss: 2.6061106076611975

Epoch: 6| Step: 10
Training loss: 3.048450238833263
Validation loss: 2.607013006454193

Epoch: 6| Step: 11
Training loss: 2.7616880164264104
Validation loss: 2.6085457312136366

Epoch: 6| Step: 12
Training loss: 2.5910788880458946
Validation loss: 2.6074512722915606

Epoch: 6| Step: 13
Training loss: 2.761815610317435
Validation loss: 2.6069660831362462

Epoch: 60| Step: 0
Training loss: 2.6428758406990265
Validation loss: 2.6040638763486355

Epoch: 6| Step: 1
Training loss: 2.7173918916452853
Validation loss: 2.6109790097857215

Epoch: 6| Step: 2
Training loss: 2.6180747279087178
Validation loss: 2.6144090774240025

Epoch: 6| Step: 3
Training loss: 2.9720185607186824
Validation loss: 2.607421234930867

Epoch: 6| Step: 4
Training loss: 3.0798985362989613
Validation loss: 2.601211432519808

Epoch: 6| Step: 5
Training loss: 3.0150863878010727
Validation loss: 2.5974644946308154

Epoch: 6| Step: 6
Training loss: 2.418738616995431
Validation loss: 2.598224197291385

Epoch: 6| Step: 7
Training loss: 3.217507872671886
Validation loss: 2.596948344590793

Epoch: 6| Step: 8
Training loss: 2.2216962138808403
Validation loss: 2.5943447794571277

Epoch: 6| Step: 9
Training loss: 3.2700502628581494
Validation loss: 2.594485611376829

Epoch: 6| Step: 10
Training loss: 2.181298757625725
Validation loss: 2.5946656874298384

Epoch: 6| Step: 11
Training loss: 2.239429651946143
Validation loss: 2.594601747925201

Epoch: 6| Step: 12
Training loss: 2.4651507444889043
Validation loss: 2.590975123521724

Epoch: 6| Step: 13
Training loss: 2.924897940395691
Validation loss: 2.5893316515400797

Epoch: 61| Step: 0
Training loss: 2.428268263430915
Validation loss: 2.590715525310551

Epoch: 6| Step: 1
Training loss: 2.578163470356915
Validation loss: 2.5869401292266443

Epoch: 6| Step: 2
Training loss: 2.3445928456834526
Validation loss: 2.587289385739426

Epoch: 6| Step: 3
Training loss: 2.183510766815412
Validation loss: 2.587729272190806

Epoch: 6| Step: 4
Training loss: 3.2156084154774587
Validation loss: 2.5887132251361744

Epoch: 6| Step: 5
Training loss: 2.256874179887477
Validation loss: 2.5868680111530375

Epoch: 6| Step: 6
Training loss: 2.741314697476261
Validation loss: 2.58507101518492

Epoch: 6| Step: 7
Training loss: 2.958811779465073
Validation loss: 2.5860077770629557

Epoch: 6| Step: 8
Training loss: 2.639559920361944
Validation loss: 2.5847579401732252

Epoch: 6| Step: 9
Training loss: 2.4756770431337394
Validation loss: 2.581953408226453

Epoch: 6| Step: 10
Training loss: 3.0339414308410646
Validation loss: 2.580599466266669

Epoch: 6| Step: 11
Training loss: 3.3579866601112136
Validation loss: 2.5818515085976808

Epoch: 6| Step: 12
Training loss: 2.745431833816327
Validation loss: 2.582398435153235

Epoch: 6| Step: 13
Training loss: 2.8278697198969334
Validation loss: 2.5824818181951628

Epoch: 62| Step: 0
Training loss: 2.495437655239362
Validation loss: 2.5823404547322037

Epoch: 6| Step: 1
Training loss: 3.330948898706778
Validation loss: 2.5782979117312994

Epoch: 6| Step: 2
Training loss: 3.2591228091290687
Validation loss: 2.5776761984085512

Epoch: 6| Step: 3
Training loss: 2.1917165172791524
Validation loss: 2.577691644795277

Epoch: 6| Step: 4
Training loss: 2.983947403717816
Validation loss: 2.577883853616176

Epoch: 6| Step: 5
Training loss: 2.8300919410678977
Validation loss: 2.5792661819199356

Epoch: 6| Step: 6
Training loss: 2.895544371607311
Validation loss: 2.5758774641936624

Epoch: 6| Step: 7
Training loss: 2.4142689045686057
Validation loss: 2.5762815261730667

Epoch: 6| Step: 8
Training loss: 2.0684286507594285
Validation loss: 2.574739641773656

Epoch: 6| Step: 9
Training loss: 2.6399732157764575
Validation loss: 2.5742300647878626

Epoch: 6| Step: 10
Training loss: 2.254088607630544
Validation loss: 2.57180856995136

Epoch: 6| Step: 11
Training loss: 2.8590608017691563
Validation loss: 2.5728935570135905

Epoch: 6| Step: 12
Training loss: 2.4327716449107464
Validation loss: 2.5708071417275504

Epoch: 6| Step: 13
Training loss: 2.964175585015792
Validation loss: 2.5736067666254248

Epoch: 63| Step: 0
Training loss: 3.192125741265039
Validation loss: 2.5708978173954735

Epoch: 6| Step: 1
Training loss: 3.167715417631775
Validation loss: 2.5695135697239517

Epoch: 6| Step: 2
Training loss: 2.3080207126421874
Validation loss: 2.577475124871797

Epoch: 6| Step: 3
Training loss: 2.851557148967582
Validation loss: 2.5676065905817467

Epoch: 6| Step: 4
Training loss: 2.860148841079331
Validation loss: 2.569581535622855

Epoch: 6| Step: 5
Training loss: 2.4982443366847824
Validation loss: 2.5666007438054366

Epoch: 6| Step: 6
Training loss: 2.49124941016696
Validation loss: 2.562270192952355

Epoch: 6| Step: 7
Training loss: 2.1668037102225095
Validation loss: 2.565781887521996

Epoch: 6| Step: 8
Training loss: 2.232706367207289
Validation loss: 2.5658838523292915

Epoch: 6| Step: 9
Training loss: 3.121617432012722
Validation loss: 2.5659079646173515

Epoch: 6| Step: 10
Training loss: 2.5704612804821645
Validation loss: 2.564761396765568

Epoch: 6| Step: 11
Training loss: 2.847289178159201
Validation loss: 2.5635557132187827

Epoch: 6| Step: 12
Training loss: 2.6713522237908363
Validation loss: 2.563900758553478

Epoch: 6| Step: 13
Training loss: 2.5982844231361515
Validation loss: 2.5648011055983035

Epoch: 64| Step: 0
Training loss: 2.7335024068070655
Validation loss: 2.5642410349441693

Epoch: 6| Step: 1
Training loss: 2.485766902610985
Validation loss: 2.563742762716126

Epoch: 6| Step: 2
Training loss: 2.8058557654008003
Validation loss: 2.5625307810105435

Epoch: 6| Step: 3
Training loss: 2.4708777806967936
Validation loss: 2.562456301184737

Epoch: 6| Step: 4
Training loss: 2.7675372048679616
Validation loss: 2.558808684117927

Epoch: 6| Step: 5
Training loss: 2.7890096707509784
Validation loss: 2.560279582000693

Epoch: 6| Step: 6
Training loss: 2.712270183759932
Validation loss: 2.556936675771386

Epoch: 6| Step: 7
Training loss: 2.282341682818089
Validation loss: 2.5628190345961492

Epoch: 6| Step: 8
Training loss: 2.2698041584382502
Validation loss: 2.557802251641077

Epoch: 6| Step: 9
Training loss: 2.2820215879952084
Validation loss: 2.558129234953543

Epoch: 6| Step: 10
Training loss: 2.8802452337972064
Validation loss: 2.561598751091916

Epoch: 6| Step: 11
Training loss: 2.970452232224619
Validation loss: 2.5591044374943306

Epoch: 6| Step: 12
Training loss: 3.1306994534757187
Validation loss: 2.559610598646261

Epoch: 6| Step: 13
Training loss: 2.8726774039071885
Validation loss: 2.559397998624194

Epoch: 65| Step: 0
Training loss: 2.9362680002491444
Validation loss: 2.559485453363669

Epoch: 6| Step: 1
Training loss: 3.5487426680341
Validation loss: 2.5553283976294807

Epoch: 6| Step: 2
Training loss: 2.646227879780904
Validation loss: 2.5512162169220076

Epoch: 6| Step: 3
Training loss: 3.0055192404210316
Validation loss: 2.5522985614146565

Epoch: 6| Step: 4
Training loss: 2.9866921581588253
Validation loss: 2.5501928424845155

Epoch: 6| Step: 5
Training loss: 2.7991339229642023
Validation loss: 2.5499724293134602

Epoch: 6| Step: 6
Training loss: 2.460969083068665
Validation loss: 2.546479130034554

Epoch: 6| Step: 7
Training loss: 2.35964307462076
Validation loss: 2.544282385423426

Epoch: 6| Step: 8
Training loss: 2.446155731770712
Validation loss: 2.5457697149174727

Epoch: 6| Step: 9
Training loss: 2.2633915909510747
Validation loss: 2.5481413518901666

Epoch: 6| Step: 10
Training loss: 2.429456693719844
Validation loss: 2.544274576445331

Epoch: 6| Step: 11
Training loss: 2.6807353025882144
Validation loss: 2.5459780215423216

Epoch: 6| Step: 12
Training loss: 2.3879635026401482
Validation loss: 2.5439907506788604

Epoch: 6| Step: 13
Training loss: 2.1915782514174986
Validation loss: 2.5471660393116724

Epoch: 66| Step: 0
Training loss: 2.782313968441005
Validation loss: 2.5459173855639183

Epoch: 6| Step: 1
Training loss: 2.765072869828907
Validation loss: 2.549560239059389

Epoch: 6| Step: 2
Training loss: 2.7131022404417133
Validation loss: 2.5509318238662178

Epoch: 6| Step: 3
Training loss: 2.6627043453117345
Validation loss: 2.5501976883995914

Epoch: 6| Step: 4
Training loss: 2.122782953136163
Validation loss: 2.5516241841316867

Epoch: 6| Step: 5
Training loss: 2.7888173227976987
Validation loss: 2.549917389273293

Epoch: 6| Step: 6
Training loss: 3.200679087108288
Validation loss: 2.548608607245615

Epoch: 6| Step: 7
Training loss: 2.673541614460435
Validation loss: 2.5463562887683207

Epoch: 6| Step: 8
Training loss: 3.1506492853846217
Validation loss: 2.5458923191494054

Epoch: 6| Step: 9
Training loss: 2.373192350222026
Validation loss: 2.545948663608019

Epoch: 6| Step: 10
Training loss: 1.8371927069278595
Validation loss: 2.545709823217941

Epoch: 6| Step: 11
Training loss: 2.7367127806402123
Validation loss: 2.5426063416083995

Epoch: 6| Step: 12
Training loss: 2.4456394939738897
Validation loss: 2.5448813407131055

Epoch: 6| Step: 13
Training loss: 2.957059308300667
Validation loss: 2.541991030731505

Epoch: 67| Step: 0
Training loss: 2.237072896418528
Validation loss: 2.5411705147765025

Epoch: 6| Step: 1
Training loss: 2.482980684762187
Validation loss: 2.540949850675184

Epoch: 6| Step: 2
Training loss: 2.246842712255244
Validation loss: 2.537197785945333

Epoch: 6| Step: 3
Training loss: 2.744289190540224
Validation loss: 2.5417800836324567

Epoch: 6| Step: 4
Training loss: 3.273897780512717
Validation loss: 2.5357173383017675

Epoch: 6| Step: 5
Training loss: 2.425799039569683
Validation loss: 2.539195787892757

Epoch: 6| Step: 6
Training loss: 2.947821805436422
Validation loss: 2.5394936454735

Epoch: 6| Step: 7
Training loss: 2.5375404843262555
Validation loss: 2.5393605688174663

Epoch: 6| Step: 8
Training loss: 2.59616555485707
Validation loss: 2.5451797272749657

Epoch: 6| Step: 9
Training loss: 2.1949470188637576
Validation loss: 2.5361284097548342

Epoch: 6| Step: 10
Training loss: 3.0028039226713177
Validation loss: 2.5305176762680377

Epoch: 6| Step: 11
Training loss: 3.002246968885574
Validation loss: 2.5310351119643557

Epoch: 6| Step: 12
Training loss: 2.4497532116903646
Validation loss: 2.53424925956704

Epoch: 6| Step: 13
Training loss: 2.8714586090978584
Validation loss: 2.5343226868372675

Epoch: 68| Step: 0
Training loss: 2.3152540149116367
Validation loss: 2.5312123079986515

Epoch: 6| Step: 1
Training loss: 2.985321214476103
Validation loss: 2.538290585287126

Epoch: 6| Step: 2
Training loss: 2.588715136195168
Validation loss: 2.5376958524040414

Epoch: 6| Step: 3
Training loss: 2.412521512424832
Validation loss: 2.5334726473080385

Epoch: 6| Step: 4
Training loss: 2.2592667380917324
Validation loss: 2.5368181844149174

Epoch: 6| Step: 5
Training loss: 2.859192722535011
Validation loss: 2.5345212261570143

Epoch: 6| Step: 6
Training loss: 2.3688223956073937
Validation loss: 2.534077983719045

Epoch: 6| Step: 7
Training loss: 2.886123368701559
Validation loss: 2.533852561853468

Epoch: 6| Step: 8
Training loss: 3.015377686790519
Validation loss: 2.5319440227746974

Epoch: 6| Step: 9
Training loss: 2.8083571545263166
Validation loss: 2.53519949462246

Epoch: 6| Step: 10
Training loss: 2.1041239617675513
Validation loss: 2.5320261012716183

Epoch: 6| Step: 11
Training loss: 2.836338655877537
Validation loss: 2.532979523479729

Epoch: 6| Step: 12
Training loss: 2.961047651109092
Validation loss: 2.5312042389175105

Epoch: 6| Step: 13
Training loss: 2.6597183920287955
Validation loss: 2.52803986684634

Epoch: 69| Step: 0
Training loss: 2.933616831113428
Validation loss: 2.532812836992358

Epoch: 6| Step: 1
Training loss: 2.2890452101122833
Validation loss: 2.5294122735876203

Epoch: 6| Step: 2
Training loss: 2.2201192150239266
Validation loss: 2.526747336907369

Epoch: 6| Step: 3
Training loss: 2.727652258185279
Validation loss: 2.5257391889448315

Epoch: 6| Step: 4
Training loss: 3.199871215613027
Validation loss: 2.5276961162031584

Epoch: 6| Step: 5
Training loss: 2.1272081234524696
Validation loss: 2.526204555542521

Epoch: 6| Step: 6
Training loss: 2.778942393833861
Validation loss: 2.5226341519884814

Epoch: 6| Step: 7
Training loss: 2.710997127349919
Validation loss: 2.524688158828052

Epoch: 6| Step: 8
Training loss: 2.5586395026622526
Validation loss: 2.524291060480557

Epoch: 6| Step: 9
Training loss: 2.4443261835602432
Validation loss: 2.5242469678187263

Epoch: 6| Step: 10
Training loss: 2.655351643955629
Validation loss: 2.523797811603532

Epoch: 6| Step: 11
Training loss: 2.6641478762552158
Validation loss: 2.524769717753591

Epoch: 6| Step: 12
Training loss: 2.370646049552894
Validation loss: 2.5301597429313163

Epoch: 6| Step: 13
Training loss: 3.1792741736503687
Validation loss: 2.5250669637304948

Epoch: 70| Step: 0
Training loss: 2.6517923088046778
Validation loss: 2.522506164074447

Epoch: 6| Step: 1
Training loss: 2.4471286452212127
Validation loss: 2.521123165356596

Epoch: 6| Step: 2
Training loss: 2.48041742226668
Validation loss: 2.521404073014188

Epoch: 6| Step: 3
Training loss: 2.2200804468992437
Validation loss: 2.5248241571062433

Epoch: 6| Step: 4
Training loss: 2.4945725654695456
Validation loss: 2.5229174035656894

Epoch: 6| Step: 5
Training loss: 2.7333134797778595
Validation loss: 2.5227400974402796

Epoch: 6| Step: 6
Training loss: 2.41572192788436
Validation loss: 2.5265395679899765

Epoch: 6| Step: 7
Training loss: 2.6850699150317108
Validation loss: 2.526213592236746

Epoch: 6| Step: 8
Training loss: 2.7964734689622683
Validation loss: 2.524635471494594

Epoch: 6| Step: 9
Training loss: 2.803472484199823
Validation loss: 2.5285187338025095

Epoch: 6| Step: 10
Training loss: 2.41231792336058
Validation loss: 2.530803311341243

Epoch: 6| Step: 11
Training loss: 3.045036191427697
Validation loss: 2.5271331840896223

Epoch: 6| Step: 12
Training loss: 2.801887921370103
Validation loss: 2.5260715967876224

Epoch: 6| Step: 13
Training loss: 3.055820420855755
Validation loss: 2.5259809401511157

Epoch: 71| Step: 0
Training loss: 2.5284302629747497
Validation loss: 2.5267049855804906

Epoch: 6| Step: 1
Training loss: 2.786511448624285
Validation loss: 2.5243276437396154

Epoch: 6| Step: 2
Training loss: 2.967551541607931
Validation loss: 2.5252434691835104

Epoch: 6| Step: 3
Training loss: 2.6688531216926226
Validation loss: 2.524193019769257

Epoch: 6| Step: 4
Training loss: 2.505294914626406
Validation loss: 2.519606068212446

Epoch: 6| Step: 5
Training loss: 2.2416670457728562
Validation loss: 2.5184856443295787

Epoch: 6| Step: 6
Training loss: 2.3503507108946273
Validation loss: 2.518676076455795

Epoch: 6| Step: 7
Training loss: 2.3337641272911567
Validation loss: 2.516318594838819

Epoch: 6| Step: 8
Training loss: 2.805005748831286
Validation loss: 2.5180221572468717

Epoch: 6| Step: 9
Training loss: 2.5942347429776595
Validation loss: 2.5181667842192947

Epoch: 6| Step: 10
Training loss: 2.515748015055631
Validation loss: 2.5185861633629543

Epoch: 6| Step: 11
Training loss: 2.870814054190942
Validation loss: 2.518321438308317

Epoch: 6| Step: 12
Training loss: 3.1065480018295064
Validation loss: 2.515420040524124

Epoch: 6| Step: 13
Training loss: 2.6421278790605656
Validation loss: 2.512755785522136

Epoch: 72| Step: 0
Training loss: 1.6326487335870277
Validation loss: 2.5130478509956062

Epoch: 6| Step: 1
Training loss: 3.0032829759153317
Validation loss: 2.5153672301475996

Epoch: 6| Step: 2
Training loss: 2.971199871919431
Validation loss: 2.5147782948367854

Epoch: 6| Step: 3
Training loss: 2.7345924508960304
Validation loss: 2.5145734085692015

Epoch: 6| Step: 4
Training loss: 2.6928186350993055
Validation loss: 2.519336158631574

Epoch: 6| Step: 5
Training loss: 2.37572287550829
Validation loss: 2.512947932686467

Epoch: 6| Step: 6
Training loss: 2.6532286570937926
Validation loss: 2.511531619175244

Epoch: 6| Step: 7
Training loss: 2.77158476504558
Validation loss: 2.513751649418155

Epoch: 6| Step: 8
Training loss: 2.987927624092727
Validation loss: 2.5109748430844028

Epoch: 6| Step: 9
Training loss: 2.886458079295501
Validation loss: 2.5122642735471112

Epoch: 6| Step: 10
Training loss: 2.8924940510443835
Validation loss: 2.5069477574991987

Epoch: 6| Step: 11
Training loss: 2.2417680833226923
Validation loss: 2.5126384437590676

Epoch: 6| Step: 12
Training loss: 2.440262720472527
Validation loss: 2.509533775693614

Epoch: 6| Step: 13
Training loss: 2.1902122032440734
Validation loss: 2.512761146423274

Epoch: 73| Step: 0
Training loss: 2.682403700380762
Validation loss: 2.5043417422449923

Epoch: 6| Step: 1
Training loss: 2.570516097054929
Validation loss: 2.5080373787992034

Epoch: 6| Step: 2
Training loss: 2.483620007074064
Validation loss: 2.5072197931688662

Epoch: 6| Step: 3
Training loss: 2.0644043886897507
Validation loss: 2.508017764301854

Epoch: 6| Step: 4
Training loss: 2.6977040171433426
Validation loss: 2.5103315019737207

Epoch: 6| Step: 5
Training loss: 2.6981561228017727
Validation loss: 2.512042841907408

Epoch: 6| Step: 6
Training loss: 3.1708664065694325
Validation loss: 2.515143844050324

Epoch: 6| Step: 7
Training loss: 2.0417717368149524
Validation loss: 2.5134734594389325

Epoch: 6| Step: 8
Training loss: 2.914777161863375
Validation loss: 2.5106471946469027

Epoch: 6| Step: 9
Training loss: 2.9486908092759565
Validation loss: 2.5072684485186287

Epoch: 6| Step: 10
Training loss: 2.500187580700733
Validation loss: 2.50814078807575

Epoch: 6| Step: 11
Training loss: 2.759660571691248
Validation loss: 2.508808385869368

Epoch: 6| Step: 12
Training loss: 2.6518098408976263
Validation loss: 2.5131188302706327

Epoch: 6| Step: 13
Training loss: 2.5631286966352542
Validation loss: 2.5095714291104394

Epoch: 74| Step: 0
Training loss: 2.885039009875079
Validation loss: 2.510075420649798

Epoch: 6| Step: 1
Training loss: 2.326934510073793
Validation loss: 2.508138443313289

Epoch: 6| Step: 2
Training loss: 2.1461612364465834
Validation loss: 2.508773278998112

Epoch: 6| Step: 3
Training loss: 2.413556586032723
Validation loss: 2.5063666972121954

Epoch: 6| Step: 4
Training loss: 2.8923413928701702
Validation loss: 2.505425256760606

Epoch: 6| Step: 5
Training loss: 2.4206348812769325
Validation loss: 2.5090188425693745

Epoch: 6| Step: 6
Training loss: 2.6826848207994547
Validation loss: 2.506501137191968

Epoch: 6| Step: 7
Training loss: 2.359786042035288
Validation loss: 2.5039151846676186

Epoch: 6| Step: 8
Training loss: 3.083016164464455
Validation loss: 2.5019535220540683

Epoch: 6| Step: 9
Training loss: 2.6238168138899414
Validation loss: 2.499430114642083

Epoch: 6| Step: 10
Training loss: 2.9138284952333398
Validation loss: 2.502664036717741

Epoch: 6| Step: 11
Training loss: 2.4797312203238535
Validation loss: 2.500197156443498

Epoch: 6| Step: 12
Training loss: 2.7726842582708966
Validation loss: 2.5006707881011123

Epoch: 6| Step: 13
Training loss: 2.3691639722159574
Validation loss: 2.50260557451847

Epoch: 75| Step: 0
Training loss: 2.474030075371513
Validation loss: 2.4971082652418644

Epoch: 6| Step: 1
Training loss: 2.7162836222141076
Validation loss: 2.4976278972239507

Epoch: 6| Step: 2
Training loss: 2.9414396448759987
Validation loss: 2.4952526157799744

Epoch: 6| Step: 3
Training loss: 2.3154737580158535
Validation loss: 2.5014527629774284

Epoch: 6| Step: 4
Training loss: 1.8668089925911906
Validation loss: 2.4926398493574102

Epoch: 6| Step: 5
Training loss: 2.7058997934174505
Validation loss: 2.4994629759971914

Epoch: 6| Step: 6
Training loss: 2.8867494738935027
Validation loss: 2.495154055169363

Epoch: 6| Step: 7
Training loss: 3.2188542913929408
Validation loss: 2.499300095813447

Epoch: 6| Step: 8
Training loss: 2.446872591331729
Validation loss: 2.5013237468543235

Epoch: 6| Step: 9
Training loss: 3.038179009292389
Validation loss: 2.5045708038756223

Epoch: 6| Step: 10
Training loss: 2.7426332290822915
Validation loss: 2.5042462368561416

Epoch: 6| Step: 11
Training loss: 2.333483168922801
Validation loss: 2.504026349580067

Epoch: 6| Step: 12
Training loss: 2.106605350390747
Validation loss: 2.5013762341770587

Epoch: 6| Step: 13
Training loss: 2.5809557546128694
Validation loss: 2.5029399751056625

Testing loss: 2.072553839105177

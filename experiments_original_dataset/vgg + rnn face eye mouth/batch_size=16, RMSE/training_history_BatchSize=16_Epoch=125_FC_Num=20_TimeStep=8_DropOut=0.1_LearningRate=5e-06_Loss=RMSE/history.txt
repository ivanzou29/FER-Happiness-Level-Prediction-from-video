Epoch: 1| Step: 0
Training loss: 5.833687508139851
Validation loss: 5.9335947679222025

Epoch: 6| Step: 1
Training loss: 6.389979099006677
Validation loss: 5.931711098716326

Epoch: 6| Step: 2
Training loss: 7.235478030785829
Validation loss: 5.9300075975683555

Epoch: 6| Step: 3
Training loss: 6.154817211123637
Validation loss: 5.92841390771441

Epoch: 6| Step: 4
Training loss: 6.469161015965539
Validation loss: 5.926857227198359

Epoch: 6| Step: 5
Training loss: 5.195044121944979
Validation loss: 5.925375488444323

Epoch: 6| Step: 6
Training loss: 5.763732434132038
Validation loss: 5.924044034859047

Epoch: 6| Step: 7
Training loss: 6.137580563391339
Validation loss: 5.922817131795706

Epoch: 6| Step: 8
Training loss: 5.467707420150885
Validation loss: 5.921507918791289

Epoch: 6| Step: 9
Training loss: 6.1520442635440675
Validation loss: 5.9203597707800135

Epoch: 6| Step: 10
Training loss: 5.769704476495633
Validation loss: 5.919130627143221

Epoch: 6| Step: 11
Training loss: 5.32516370351544
Validation loss: 5.917953548264991

Epoch: 6| Step: 12
Training loss: 6.256180415876455
Validation loss: 5.91660030309863

Epoch: 6| Step: 13
Training loss: 6.126643991743155
Validation loss: 5.91524513611609

Epoch: 2| Step: 0
Training loss: 6.345722662231389
Validation loss: 5.913875198925764

Epoch: 6| Step: 1
Training loss: 6.194098143123067
Validation loss: 5.912451823322567

Epoch: 6| Step: 2
Training loss: 4.697495040525506
Validation loss: 5.910881124091989

Epoch: 6| Step: 3
Training loss: 5.658637623009195
Validation loss: 5.909242063869882

Epoch: 6| Step: 4
Training loss: 5.548808016232043
Validation loss: 5.907576423803953

Epoch: 6| Step: 5
Training loss: 6.018123593319819
Validation loss: 5.905899817902291

Epoch: 6| Step: 6
Training loss: 5.981142929218971
Validation loss: 5.904018135149773

Epoch: 6| Step: 7
Training loss: 6.546986289887926
Validation loss: 5.902113204146162

Epoch: 6| Step: 8
Training loss: 6.162300351141267
Validation loss: 5.900003483873621

Epoch: 6| Step: 9
Training loss: 6.290942859909824
Validation loss: 5.897887888516691

Epoch: 6| Step: 10
Training loss: 6.970652230831298
Validation loss: 5.895650081235409

Epoch: 6| Step: 11
Training loss: 5.70952925951294
Validation loss: 5.893194041454393

Epoch: 6| Step: 12
Training loss: 5.6438727922061185
Validation loss: 5.8907552986338665

Epoch: 6| Step: 13
Training loss: 6.172065364943944
Validation loss: 5.888099351573394

Epoch: 3| Step: 0
Training loss: 6.410285088303802
Validation loss: 5.885174699608648

Epoch: 6| Step: 1
Training loss: 6.7441010306903175
Validation loss: 5.882407746527181

Epoch: 6| Step: 2
Training loss: 5.868669061234665
Validation loss: 5.879315732295822

Epoch: 6| Step: 3
Training loss: 6.164252487368387
Validation loss: 5.875979565059686

Epoch: 6| Step: 4
Training loss: 5.615813828797817
Validation loss: 5.872579089320041

Epoch: 6| Step: 5
Training loss: 6.805810846570515
Validation loss: 5.86892266823528

Epoch: 6| Step: 6
Training loss: 5.016585118506483
Validation loss: 5.865173698600081

Epoch: 6| Step: 7
Training loss: 6.106346530990766
Validation loss: 5.860976289440949

Epoch: 6| Step: 8
Training loss: 6.541975787527026
Validation loss: 5.856807597242699

Epoch: 6| Step: 9
Training loss: 6.336410711752183
Validation loss: 5.852346288853366

Epoch: 6| Step: 10
Training loss: 4.127801348512101
Validation loss: 5.847598897601321

Epoch: 6| Step: 11
Training loss: 6.1759841567117935
Validation loss: 5.842835081016398

Epoch: 6| Step: 12
Training loss: 5.7785532667308095
Validation loss: 5.837577619996365

Epoch: 6| Step: 13
Training loss: 5.436506509454739
Validation loss: 5.832304518664422

Epoch: 4| Step: 0
Training loss: 5.427268036709184
Validation loss: 5.826828526897322

Epoch: 6| Step: 1
Training loss: 6.441547787824913
Validation loss: 5.820948650917184

Epoch: 6| Step: 2
Training loss: 5.753997076888364
Validation loss: 5.8149212678765645

Epoch: 6| Step: 3
Training loss: 4.975355448077734
Validation loss: 5.808186342120764

Epoch: 6| Step: 4
Training loss: 5.869105852122448
Validation loss: 5.801830767939229

Epoch: 6| Step: 5
Training loss: 6.006953342948109
Validation loss: 5.794904348194551

Epoch: 6| Step: 6
Training loss: 6.273316492645221
Validation loss: 5.787775758966823

Epoch: 6| Step: 7
Training loss: 6.873129017873729
Validation loss: 5.78036492248413

Epoch: 6| Step: 8
Training loss: 6.352829922842611
Validation loss: 5.772432603735868

Epoch: 6| Step: 9
Training loss: 5.902873693945999
Validation loss: 5.764437862537254

Epoch: 6| Step: 10
Training loss: 6.1182051356612135
Validation loss: 5.756370056645472

Epoch: 6| Step: 11
Training loss: 5.289853824261655
Validation loss: 5.747767471122994

Epoch: 6| Step: 12
Training loss: 4.669479521473978
Validation loss: 5.739567954521092

Epoch: 6| Step: 13
Training loss: 6.216305046029592
Validation loss: 5.731010815293102

Epoch: 5| Step: 0
Training loss: 6.91060863370459
Validation loss: 5.722273555801072

Epoch: 6| Step: 1
Training loss: 6.471858213099909
Validation loss: 5.713507078118821

Epoch: 6| Step: 2
Training loss: 6.6098367155513005
Validation loss: 5.704492915288169

Epoch: 6| Step: 3
Training loss: 5.399850221605362
Validation loss: 5.695444225014183

Epoch: 6| Step: 4
Training loss: 4.705903610714414
Validation loss: 5.686093837751349

Epoch: 6| Step: 5
Training loss: 5.475745734721286
Validation loss: 5.676905535325641

Epoch: 6| Step: 6
Training loss: 5.983074793990003
Validation loss: 5.667614530312824

Epoch: 6| Step: 7
Training loss: 6.609362302366645
Validation loss: 5.658492625507969

Epoch: 6| Step: 8
Training loss: 5.596078057901529
Validation loss: 5.648788026210526

Epoch: 6| Step: 9
Training loss: 5.891690185730145
Validation loss: 5.639442777860881

Epoch: 6| Step: 10
Training loss: 4.80290514133174
Validation loss: 5.629778774741591

Epoch: 6| Step: 11
Training loss: 4.968853835454445
Validation loss: 5.620360884477177

Epoch: 6| Step: 12
Training loss: 5.782927728477416
Validation loss: 5.611072648213482

Epoch: 6| Step: 13
Training loss: 5.1692227019432195
Validation loss: 5.6012740298113775

Epoch: 6| Step: 0
Training loss: 4.95715899411838
Validation loss: 5.591749888604767

Epoch: 6| Step: 1
Training loss: 5.508935432403312
Validation loss: 5.582127891720133

Epoch: 6| Step: 2
Training loss: 5.938972370895963
Validation loss: 5.572357387346514

Epoch: 6| Step: 3
Training loss: 5.896335252441103
Validation loss: 5.562881999525547

Epoch: 6| Step: 4
Training loss: 6.361524813700114
Validation loss: 5.552800726810011

Epoch: 6| Step: 5
Training loss: 5.018471548828524
Validation loss: 5.542935544022718

Epoch: 6| Step: 6
Training loss: 5.517230872645733
Validation loss: 5.533244403829215

Epoch: 6| Step: 7
Training loss: 5.316427058418132
Validation loss: 5.523188979741754

Epoch: 6| Step: 8
Training loss: 6.184758976007763
Validation loss: 5.5139807248521775

Epoch: 6| Step: 9
Training loss: 4.839393280716029
Validation loss: 5.5052281586558625

Epoch: 6| Step: 10
Training loss: 5.386648818039988
Validation loss: 5.496422904866901

Epoch: 6| Step: 11
Training loss: 6.006652323162017
Validation loss: 5.488107192061245

Epoch: 6| Step: 12
Training loss: 6.122204200885801
Validation loss: 5.47988510739993

Epoch: 6| Step: 13
Training loss: 5.751203991432943
Validation loss: 5.471703856046778

Epoch: 7| Step: 0
Training loss: 6.298492299671679
Validation loss: 5.464185310127568

Epoch: 6| Step: 1
Training loss: 6.7103693303030845
Validation loss: 5.456724070389024

Epoch: 6| Step: 2
Training loss: 5.27699297819959
Validation loss: 5.449101987109623

Epoch: 6| Step: 3
Training loss: 5.280924047760749
Validation loss: 5.441717536458722

Epoch: 6| Step: 4
Training loss: 5.964737587054596
Validation loss: 5.434906322674453

Epoch: 6| Step: 5
Training loss: 5.041223343623348
Validation loss: 5.42775387810233

Epoch: 6| Step: 6
Training loss: 4.766771522560213
Validation loss: 5.421162729813532

Epoch: 6| Step: 7
Training loss: 5.2057931834704485
Validation loss: 5.414267297112633

Epoch: 6| Step: 8
Training loss: 5.693267245716852
Validation loss: 5.4077415099394255

Epoch: 6| Step: 9
Training loss: 5.273952521146972
Validation loss: 5.401278905952298

Epoch: 6| Step: 10
Training loss: 6.468043486855323
Validation loss: 5.394819903356599

Epoch: 6| Step: 11
Training loss: 4.615132708888694
Validation loss: 5.388293395070906

Epoch: 6| Step: 12
Training loss: 5.151982431417329
Validation loss: 5.382359958592198

Epoch: 6| Step: 13
Training loss: 5.356205038907981
Validation loss: 5.376419560066748

Epoch: 8| Step: 0
Training loss: 5.367704186147041
Validation loss: 5.370663187051105

Epoch: 6| Step: 1
Training loss: 6.345317177206015
Validation loss: 5.364867573834217

Epoch: 6| Step: 2
Training loss: 5.319380310220883
Validation loss: 5.359097428172887

Epoch: 6| Step: 3
Training loss: 5.887076120850277
Validation loss: 5.353495970203643

Epoch: 6| Step: 4
Training loss: 5.377818943072801
Validation loss: 5.347491138956802

Epoch: 6| Step: 5
Training loss: 6.572096429378588
Validation loss: 5.341941482846133

Epoch: 6| Step: 6
Training loss: 5.557319371715392
Validation loss: 5.336723869171241

Epoch: 6| Step: 7
Training loss: 5.52407906568378
Validation loss: 5.330574464502149

Epoch: 6| Step: 8
Training loss: 4.916958536224222
Validation loss: 5.32583392900943

Epoch: 6| Step: 9
Training loss: 5.283216663279789
Validation loss: 5.319985505230119

Epoch: 6| Step: 10
Training loss: 4.66386947360203
Validation loss: 5.313969696798892

Epoch: 6| Step: 11
Training loss: 5.385693934648983
Validation loss: 5.307638599360751

Epoch: 6| Step: 12
Training loss: 5.05977510186183
Validation loss: 5.302044681590554

Epoch: 6| Step: 13
Training loss: 4.796217882706493
Validation loss: 5.297404094966269

Epoch: 9| Step: 0
Training loss: 4.885048706686477
Validation loss: 5.2915902420092005

Epoch: 6| Step: 1
Training loss: 4.8973907374392995
Validation loss: 5.286925176543295

Epoch: 6| Step: 2
Training loss: 6.669614458051003
Validation loss: 5.281660586917677

Epoch: 6| Step: 3
Training loss: 5.416230267282544
Validation loss: 5.276384810058716

Epoch: 6| Step: 4
Training loss: 4.9936796295593275
Validation loss: 5.271249395341351

Epoch: 6| Step: 5
Training loss: 4.767937771282288
Validation loss: 5.265876318785361

Epoch: 6| Step: 6
Training loss: 5.503993751799563
Validation loss: 5.260279083970714

Epoch: 6| Step: 7
Training loss: 5.8708669855991165
Validation loss: 5.255241139375376

Epoch: 6| Step: 8
Training loss: 5.27433730749211
Validation loss: 5.250048379826533

Epoch: 6| Step: 9
Training loss: 6.322494903357259
Validation loss: 5.2446413394278775

Epoch: 6| Step: 10
Training loss: 4.725661515043237
Validation loss: 5.2394232096640785

Epoch: 6| Step: 11
Training loss: 4.529522586279832
Validation loss: 5.2353828636542525

Epoch: 6| Step: 12
Training loss: 5.755521031050724
Validation loss: 5.2294737686417

Epoch: 6| Step: 13
Training loss: 5.246012672144324
Validation loss: 5.223431799974222

Epoch: 10| Step: 0
Training loss: 5.106623010540072
Validation loss: 5.218249177029761

Epoch: 6| Step: 1
Training loss: 5.367925734743876
Validation loss: 5.21272270891704

Epoch: 6| Step: 2
Training loss: 5.022185126498646
Validation loss: 5.207248829146956

Epoch: 6| Step: 3
Training loss: 5.085953438495091
Validation loss: 5.201977821943248

Epoch: 6| Step: 4
Training loss: 5.244111164372555
Validation loss: 5.1967936839954

Epoch: 6| Step: 5
Training loss: 5.298800948200212
Validation loss: 5.191823544078702

Epoch: 6| Step: 6
Training loss: 5.251239448651177
Validation loss: 5.187556224349786

Epoch: 6| Step: 7
Training loss: 5.296820086079193
Validation loss: 5.181583722958784

Epoch: 6| Step: 8
Training loss: 6.172759405581852
Validation loss: 5.17668194305334

Epoch: 6| Step: 9
Training loss: 5.29644629619996
Validation loss: 5.170805083798876

Epoch: 6| Step: 10
Training loss: 5.208525732937195
Validation loss: 5.165032384671819

Epoch: 6| Step: 11
Training loss: 5.370217968372609
Validation loss: 5.160063995684051

Epoch: 6| Step: 12
Training loss: 5.5753546495064725
Validation loss: 5.15495280893249

Epoch: 6| Step: 13
Training loss: 4.888623105155255
Validation loss: 5.150617601960009

Epoch: 11| Step: 0
Training loss: 5.495079093273636
Validation loss: 5.1448191931725065

Epoch: 6| Step: 1
Training loss: 4.894149339150395
Validation loss: 5.138993797720627

Epoch: 6| Step: 2
Training loss: 4.632102873363128
Validation loss: 5.133521168025938

Epoch: 6| Step: 3
Training loss: 5.160597199767867
Validation loss: 5.128735367309909

Epoch: 6| Step: 4
Training loss: 4.8193874862681305
Validation loss: 5.12480132175317

Epoch: 6| Step: 5
Training loss: 5.050020825937971
Validation loss: 5.120374499572189

Epoch: 6| Step: 6
Training loss: 5.862024791465976
Validation loss: 5.115581386584538

Epoch: 6| Step: 7
Training loss: 5.969411443839321
Validation loss: 5.110586258067013

Epoch: 6| Step: 8
Training loss: 5.511573059622156
Validation loss: 5.104880811076417

Epoch: 6| Step: 9
Training loss: 5.071523653977329
Validation loss: 5.100873743894425

Epoch: 6| Step: 10
Training loss: 4.903867495391114
Validation loss: 5.095135178649189

Epoch: 6| Step: 11
Training loss: 5.942132688475676
Validation loss: 5.09235152255182

Epoch: 6| Step: 12
Training loss: 4.891574365285492
Validation loss: 5.086919374317314

Epoch: 6| Step: 13
Training loss: 4.845920396428876
Validation loss: 5.080714390151687

Epoch: 12| Step: 0
Training loss: 5.890535632196761
Validation loss: 5.076899485285024

Epoch: 6| Step: 1
Training loss: 5.0549572927178295
Validation loss: 5.071159084433797

Epoch: 6| Step: 2
Training loss: 5.823193629393596
Validation loss: 5.066114132434481

Epoch: 6| Step: 3
Training loss: 4.977147138380074
Validation loss: 5.0619967646141975

Epoch: 6| Step: 4
Training loss: 5.3446001413938085
Validation loss: 5.059091401629175

Epoch: 6| Step: 5
Training loss: 5.550361332858697
Validation loss: 5.051369979152201

Epoch: 6| Step: 6
Training loss: 3.9724593962412196
Validation loss: 5.047775298482383

Epoch: 6| Step: 7
Training loss: 4.926461156981754
Validation loss: 5.04022761633227

Epoch: 6| Step: 8
Training loss: 5.753971884172502
Validation loss: 5.036248577159063

Epoch: 6| Step: 9
Training loss: 5.397329037226097
Validation loss: 5.0316546733486245

Epoch: 6| Step: 10
Training loss: 5.412870906252421
Validation loss: 5.027035292474791

Epoch: 6| Step: 11
Training loss: 4.043000358421127
Validation loss: 5.022242536944448

Epoch: 6| Step: 12
Training loss: 5.047254802006429
Validation loss: 5.016399478239424

Epoch: 6| Step: 13
Training loss: 4.75357543496404
Validation loss: 5.01281212574539

Epoch: 13| Step: 0
Training loss: 4.174973383835795
Validation loss: 5.007293753652956

Epoch: 6| Step: 1
Training loss: 5.0788885393524845
Validation loss: 5.0023629168261845

Epoch: 6| Step: 2
Training loss: 5.60941206300865
Validation loss: 4.999714080264932

Epoch: 6| Step: 3
Training loss: 5.430607077030859
Validation loss: 4.994001796136975

Epoch: 6| Step: 4
Training loss: 5.179418680571435
Validation loss: 4.989090270632074

Epoch: 6| Step: 5
Training loss: 4.878824738798185
Validation loss: 4.983796439878163

Epoch: 6| Step: 6
Training loss: 4.086552706732454
Validation loss: 4.979247194966397

Epoch: 6| Step: 7
Training loss: 4.488105312315387
Validation loss: 4.974621069197582

Epoch: 6| Step: 8
Training loss: 4.7677539510157985
Validation loss: 4.96958292571584

Epoch: 6| Step: 9
Training loss: 6.10665075730385
Validation loss: 4.965641608909645

Epoch: 6| Step: 10
Training loss: 5.558828449454863
Validation loss: 4.961356910738393

Epoch: 6| Step: 11
Training loss: 5.2329950591825245
Validation loss: 4.9564254466716955

Epoch: 6| Step: 12
Training loss: 4.928564170373979
Validation loss: 4.9514619139139695

Epoch: 6| Step: 13
Training loss: 5.484617744138274
Validation loss: 4.946608949480482

Epoch: 14| Step: 0
Training loss: 5.355785535796251
Validation loss: 4.941915268244067

Epoch: 6| Step: 1
Training loss: 4.936965527156636
Validation loss: 4.936485604815052

Epoch: 6| Step: 2
Training loss: 4.999307393741082
Validation loss: 4.932635780369416

Epoch: 6| Step: 3
Training loss: 5.112802271779391
Validation loss: 4.929499457833945

Epoch: 6| Step: 4
Training loss: 4.854964729597401
Validation loss: 4.921984183901812

Epoch: 6| Step: 5
Training loss: 3.833522556997327
Validation loss: 4.918272413554941

Epoch: 6| Step: 6
Training loss: 5.563428136994136
Validation loss: 4.9137575411716465

Epoch: 6| Step: 7
Training loss: 5.099701382739506
Validation loss: 4.909776477193208

Epoch: 6| Step: 8
Training loss: 4.89796223989054
Validation loss: 4.9033526310820825

Epoch: 6| Step: 9
Training loss: 5.694513377364256
Validation loss: 4.898915212275034

Epoch: 6| Step: 10
Training loss: 4.2361286656306065
Validation loss: 4.894299963671966

Epoch: 6| Step: 11
Training loss: 5.453016919483188
Validation loss: 4.888452862070396

Epoch: 6| Step: 12
Training loss: 5.08619607247606
Validation loss: 4.885778200915391

Epoch: 6| Step: 13
Training loss: 5.068606049813697
Validation loss: 4.879932451406028

Epoch: 15| Step: 0
Training loss: 4.121686818625957
Validation loss: 4.875923118244934

Epoch: 6| Step: 1
Training loss: 4.396532096586849
Validation loss: 4.872660947920457

Epoch: 6| Step: 2
Training loss: 6.0168170491002755
Validation loss: 4.8734933530120905

Epoch: 6| Step: 3
Training loss: 5.431261539776201
Validation loss: 4.8715097697873695

Epoch: 6| Step: 4
Training loss: 4.214560455898786
Validation loss: 4.8652393571258195

Epoch: 6| Step: 5
Training loss: 4.607780982653846
Validation loss: 4.858329341531273

Epoch: 6| Step: 6
Training loss: 5.023826477273562
Validation loss: 4.853320472319473

Epoch: 6| Step: 7
Training loss: 5.164410416371889
Validation loss: 4.849576894912796

Epoch: 6| Step: 8
Training loss: 5.052257207126622
Validation loss: 4.842661942541309

Epoch: 6| Step: 9
Training loss: 4.824535035178423
Validation loss: 4.835309490619654

Epoch: 6| Step: 10
Training loss: 5.348815658533636
Validation loss: 4.82975387667498

Epoch: 6| Step: 11
Training loss: 4.624345475355345
Validation loss: 4.826829623339549

Epoch: 6| Step: 12
Training loss: 4.794642223896981
Validation loss: 4.824255025009114

Epoch: 6| Step: 13
Training loss: 5.643565080048616
Validation loss: 4.818691679803231

Epoch: 16| Step: 0
Training loss: 4.731605651469237
Validation loss: 4.814918075714929

Epoch: 6| Step: 1
Training loss: 4.768777173766062
Validation loss: 4.811183997217122

Epoch: 6| Step: 2
Training loss: 5.721629600741902
Validation loss: 4.806082835636743

Epoch: 6| Step: 3
Training loss: 4.916171205201021
Validation loss: 4.800816609537194

Epoch: 6| Step: 4
Training loss: 5.104905719839228
Validation loss: 4.794141257581387

Epoch: 6| Step: 5
Training loss: 4.938475307278118
Validation loss: 4.788836807317868

Epoch: 6| Step: 6
Training loss: 5.175127464604359
Validation loss: 4.782910227034181

Epoch: 6| Step: 7
Training loss: 4.407063077755239
Validation loss: 4.7777626766784325

Epoch: 6| Step: 8
Training loss: 4.809639613238035
Validation loss: 4.772938637229852

Epoch: 6| Step: 9
Training loss: 5.662980095929575
Validation loss: 4.770445240121876

Epoch: 6| Step: 10
Training loss: 4.551234302153175
Validation loss: 4.764213277871117

Epoch: 6| Step: 11
Training loss: 4.773552576354297
Validation loss: 4.759570351906067

Epoch: 6| Step: 12
Training loss: 4.910848030151629
Validation loss: 4.755626625250932

Epoch: 6| Step: 13
Training loss: 3.984140187244446
Validation loss: 4.750171323664723

Epoch: 17| Step: 0
Training loss: 4.382911286626224
Validation loss: 4.7459824453523005

Epoch: 6| Step: 1
Training loss: 4.130556006147583
Validation loss: 4.741282008457195

Epoch: 6| Step: 2
Training loss: 4.993008111838274
Validation loss: 4.735798402507856

Epoch: 6| Step: 3
Training loss: 4.315605261865955
Validation loss: 4.729971807340728

Epoch: 6| Step: 4
Training loss: 4.980292965647869
Validation loss: 4.7259877425540795

Epoch: 6| Step: 5
Training loss: 5.23844364378219
Validation loss: 4.724947062200865

Epoch: 6| Step: 6
Training loss: 5.389199596418662
Validation loss: 4.717214540848627

Epoch: 6| Step: 7
Training loss: 5.889137950564688
Validation loss: 4.711885457059111

Epoch: 6| Step: 8
Training loss: 4.863708918507534
Validation loss: 4.708814455729936

Epoch: 6| Step: 9
Training loss: 4.07067752306986
Validation loss: 4.70281193938556

Epoch: 6| Step: 10
Training loss: 4.868743794624575
Validation loss: 4.698591140181919

Epoch: 6| Step: 11
Training loss: 4.416550520803268
Validation loss: 4.693773809881389

Epoch: 6| Step: 12
Training loss: 4.513265131703195
Validation loss: 4.6881249583388875

Epoch: 6| Step: 13
Training loss: 5.3374371634364755
Validation loss: 4.683179941527524

Epoch: 18| Step: 0
Training loss: 5.066556647427983
Validation loss: 4.679006621470572

Epoch: 6| Step: 1
Training loss: 4.213040479630811
Validation loss: 4.674346338224942

Epoch: 6| Step: 2
Training loss: 4.403689025751303
Validation loss: 4.670078642439615

Epoch: 6| Step: 3
Training loss: 4.467723808638291
Validation loss: 4.666397643282268

Epoch: 6| Step: 4
Training loss: 5.119526987875916
Validation loss: 4.659271642304824

Epoch: 6| Step: 5
Training loss: 4.749269328888939
Validation loss: 4.6560797265514315

Epoch: 6| Step: 6
Training loss: 4.313889873826852
Validation loss: 4.653197839225329

Epoch: 6| Step: 7
Training loss: 5.6243789329985585
Validation loss: 4.650342774148328

Epoch: 6| Step: 8
Training loss: 5.7540335398343885
Validation loss: 4.646139992940884

Epoch: 6| Step: 9
Training loss: 4.086007987411841
Validation loss: 4.640406302662327

Epoch: 6| Step: 10
Training loss: 5.2598359747430505
Validation loss: 4.634440504294105

Epoch: 6| Step: 11
Training loss: 5.590602564673401
Validation loss: 4.628633738462218

Epoch: 6| Step: 12
Training loss: 4.447003141957523
Validation loss: 4.623326935455435

Epoch: 6| Step: 13
Training loss: 3.0560109425293907
Validation loss: 4.618561911807511

Epoch: 19| Step: 0
Training loss: 5.4087948845626865
Validation loss: 4.6147732433682505

Epoch: 6| Step: 1
Training loss: 4.967850897672343
Validation loss: 4.609475862078409

Epoch: 6| Step: 2
Training loss: 4.997561241488368
Validation loss: 4.604657016148848

Epoch: 6| Step: 3
Training loss: 4.909877723678525
Validation loss: 4.599541816836219

Epoch: 6| Step: 4
Training loss: 4.600943609564093
Validation loss: 4.594400026864467

Epoch: 6| Step: 5
Training loss: 5.379389722450934
Validation loss: 4.589726006911756

Epoch: 6| Step: 6
Training loss: 4.508959011207143
Validation loss: 4.5849578174909515

Epoch: 6| Step: 7
Training loss: 4.342179268262596
Validation loss: 4.581277432431137

Epoch: 6| Step: 8
Training loss: 4.614361047952292
Validation loss: 4.57626325749694

Epoch: 6| Step: 9
Training loss: 4.401627517019584
Validation loss: 4.571419328087022

Epoch: 6| Step: 10
Training loss: 4.9708585283909805
Validation loss: 4.56522284652879

Epoch: 6| Step: 11
Training loss: 3.7498449929308144
Validation loss: 4.560601871657149

Epoch: 6| Step: 12
Training loss: 5.223482647197765
Validation loss: 4.555877024206914

Epoch: 6| Step: 13
Training loss: 3.4508882070301024
Validation loss: 4.552523535678615

Epoch: 20| Step: 0
Training loss: 3.6853150182104195
Validation loss: 4.547440780972684

Epoch: 6| Step: 1
Training loss: 4.915472025591744
Validation loss: 4.54534097948678

Epoch: 6| Step: 2
Training loss: 4.457372306325473
Validation loss: 4.539097779794003

Epoch: 6| Step: 3
Training loss: 4.844683551039923
Validation loss: 4.533058805194089

Epoch: 6| Step: 4
Training loss: 4.074776279416445
Validation loss: 4.528612285516808

Epoch: 6| Step: 5
Training loss: 5.124309958547544
Validation loss: 4.524856072625568

Epoch: 6| Step: 6
Training loss: 5.262886761658476
Validation loss: 4.519963838607038

Epoch: 6| Step: 7
Training loss: 4.426281354481245
Validation loss: 4.5154398843909265

Epoch: 6| Step: 8
Training loss: 4.669491367128281
Validation loss: 4.510889529255137

Epoch: 6| Step: 9
Training loss: 3.6634645929907226
Validation loss: 4.506419583117671

Epoch: 6| Step: 10
Training loss: 4.99077823907661
Validation loss: 4.501967847555916

Epoch: 6| Step: 11
Training loss: 4.464529752328215
Validation loss: 4.496708672942105

Epoch: 6| Step: 12
Training loss: 5.537112475197545
Validation loss: 4.492158468401433

Epoch: 6| Step: 13
Training loss: 4.492564946778776
Validation loss: 4.4871527279765075

Epoch: 21| Step: 0
Training loss: 3.8720414957944094
Validation loss: 4.48253389106359

Epoch: 6| Step: 1
Training loss: 4.730683451787966
Validation loss: 4.477600829385855

Epoch: 6| Step: 2
Training loss: 4.28975951154259
Validation loss: 4.474718123354563

Epoch: 6| Step: 3
Training loss: 5.040164040514738
Validation loss: 4.469504623898325

Epoch: 6| Step: 4
Training loss: 4.4102777780255185
Validation loss: 4.464539916657662

Epoch: 6| Step: 5
Training loss: 4.771756389670202
Validation loss: 4.459800205573886

Epoch: 6| Step: 6
Training loss: 3.86319941635554
Validation loss: 4.454994371455584

Epoch: 6| Step: 7
Training loss: 4.431573052729661
Validation loss: 4.4525093233016975

Epoch: 6| Step: 8
Training loss: 3.773826689621661
Validation loss: 4.446212739971546

Epoch: 6| Step: 9
Training loss: 4.778341568104931
Validation loss: 4.440859924015364

Epoch: 6| Step: 10
Training loss: 4.984319895855276
Validation loss: 4.43672445621203

Epoch: 6| Step: 11
Training loss: 4.675596050269203
Validation loss: 4.43311438748839

Epoch: 6| Step: 12
Training loss: 5.694177082680673
Validation loss: 4.4286706330281955

Epoch: 6| Step: 13
Training loss: 4.442719304860183
Validation loss: 4.423357855339083

Epoch: 22| Step: 0
Training loss: 3.998580680808817
Validation loss: 4.4179473435715

Epoch: 6| Step: 1
Training loss: 4.8130358112840925
Validation loss: 4.413994982717866

Epoch: 6| Step: 2
Training loss: 4.7655921809833055
Validation loss: 4.409851550244116

Epoch: 6| Step: 3
Training loss: 4.913000043506028
Validation loss: 4.404138424342802

Epoch: 6| Step: 4
Training loss: 3.7252614691194044
Validation loss: 4.398725266469724

Epoch: 6| Step: 5
Training loss: 4.76207643562761
Validation loss: 4.39467049837254

Epoch: 6| Step: 6
Training loss: 3.9888223398714584
Validation loss: 4.390033849558169

Epoch: 6| Step: 7
Training loss: 4.057210442346604
Validation loss: 4.38525641625014

Epoch: 6| Step: 8
Training loss: 4.85637667968168
Validation loss: 4.38127822903161

Epoch: 6| Step: 9
Training loss: 4.513220546182773
Validation loss: 4.375900066978129

Epoch: 6| Step: 10
Training loss: 3.986782648021881
Validation loss: 4.371453637156021

Epoch: 6| Step: 11
Training loss: 5.269491072516404
Validation loss: 4.366893014549921

Epoch: 6| Step: 12
Training loss: 3.765664634654291
Validation loss: 4.362292007690884

Epoch: 6| Step: 13
Training loss: 5.373309912113863
Validation loss: 4.35818545429171

Epoch: 23| Step: 0
Training loss: 5.069416172639696
Validation loss: 4.353642092899134

Epoch: 6| Step: 1
Training loss: 3.5537760142348005
Validation loss: 4.348721118175757

Epoch: 6| Step: 2
Training loss: 4.236817278482176
Validation loss: 4.3431788190192515

Epoch: 6| Step: 3
Training loss: 4.237721827279416
Validation loss: 4.338359058043488

Epoch: 6| Step: 4
Training loss: 4.466465933165606
Validation loss: 4.3346632359457

Epoch: 6| Step: 5
Training loss: 4.944528719485053
Validation loss: 4.329394231303538

Epoch: 6| Step: 6
Training loss: 4.252955362831511
Validation loss: 4.325052761571916

Epoch: 6| Step: 7
Training loss: 4.142581808402417
Validation loss: 4.320520562697092

Epoch: 6| Step: 8
Training loss: 4.250663705500102
Validation loss: 4.315383610476177

Epoch: 6| Step: 9
Training loss: 5.143483396274737
Validation loss: 4.31028899121567

Epoch: 6| Step: 10
Training loss: 3.8976374781881593
Validation loss: 4.305820378325321

Epoch: 6| Step: 11
Training loss: 4.350640190633945
Validation loss: 4.301174255579225

Epoch: 6| Step: 12
Training loss: 4.685823471345699
Validation loss: 4.296393331638303

Epoch: 6| Step: 13
Training loss: 4.794708458527015
Validation loss: 4.291873976258899

Epoch: 24| Step: 0
Training loss: 4.297393434633516
Validation loss: 4.286992345326302

Epoch: 6| Step: 1
Training loss: 4.206416886577178
Validation loss: 4.281803828520145

Epoch: 6| Step: 2
Training loss: 3.7922952291671037
Validation loss: 4.277779297738954

Epoch: 6| Step: 3
Training loss: 4.5272251538203605
Validation loss: 4.272761652363601

Epoch: 6| Step: 4
Training loss: 5.146152666610893
Validation loss: 4.267672875092329

Epoch: 6| Step: 5
Training loss: 5.274891473458264
Validation loss: 4.263671297174386

Epoch: 6| Step: 6
Training loss: 4.525646665110607
Validation loss: 4.258617509341637

Epoch: 6| Step: 7
Training loss: 4.392219878260238
Validation loss: 4.25403941552931

Epoch: 6| Step: 8
Training loss: 4.123206471182823
Validation loss: 4.248788155959293

Epoch: 6| Step: 9
Training loss: 3.792295354905508
Validation loss: 4.2437449594279535

Epoch: 6| Step: 10
Training loss: 4.825705900587482
Validation loss: 4.2388061198437414

Epoch: 6| Step: 11
Training loss: 3.7769518928080004
Validation loss: 4.234222972435185

Epoch: 6| Step: 12
Training loss: 4.754183282357862
Validation loss: 4.229809071776318

Epoch: 6| Step: 13
Training loss: 3.579906836208537
Validation loss: 4.225406622646983

Epoch: 25| Step: 0
Training loss: 4.453875347967697
Validation loss: 4.220978569752977

Epoch: 6| Step: 1
Training loss: 4.370492629647791
Validation loss: 4.215849229991647

Epoch: 6| Step: 2
Training loss: 4.00580842772941
Validation loss: 4.211887136942735

Epoch: 6| Step: 3
Training loss: 4.076493561740425
Validation loss: 4.20720706006812

Epoch: 6| Step: 4
Training loss: 3.9769390055367664
Validation loss: 4.202481982880684

Epoch: 6| Step: 5
Training loss: 4.205550277574509
Validation loss: 4.197750394319103

Epoch: 6| Step: 6
Training loss: 3.8431454314865774
Validation loss: 4.192818732219946

Epoch: 6| Step: 7
Training loss: 5.167975393278043
Validation loss: 4.188951155460865

Epoch: 6| Step: 8
Training loss: 4.490478827553695
Validation loss: 4.183859499989631

Epoch: 6| Step: 9
Training loss: 3.9464234251594155
Validation loss: 4.179291209035117

Epoch: 6| Step: 10
Training loss: 4.899138137380466
Validation loss: 4.174990991670252

Epoch: 6| Step: 11
Training loss: 3.5966568841762867
Validation loss: 4.169995135602546

Epoch: 6| Step: 12
Training loss: 5.007289912767964
Validation loss: 4.166489133231579

Epoch: 6| Step: 13
Training loss: 4.146488054932423
Validation loss: 4.161470600184892

Epoch: 26| Step: 0
Training loss: 4.107458559780305
Validation loss: 4.156369487220853

Epoch: 6| Step: 1
Training loss: 4.299801471474212
Validation loss: 4.151677439147458

Epoch: 6| Step: 2
Training loss: 4.135933460194712
Validation loss: 4.147420601693183

Epoch: 6| Step: 3
Training loss: 4.921721150249224
Validation loss: 4.143049708167711

Epoch: 6| Step: 4
Training loss: 4.367633175842017
Validation loss: 4.1387786011363845

Epoch: 6| Step: 5
Training loss: 4.415187132035139
Validation loss: 4.13375528371185

Epoch: 6| Step: 6
Training loss: 4.41451554125583
Validation loss: 4.127869908134837

Epoch: 6| Step: 7
Training loss: 4.934520933325814
Validation loss: 4.123346652443996

Epoch: 6| Step: 8
Training loss: 4.1916708600508015
Validation loss: 4.1189451573724725

Epoch: 6| Step: 9
Training loss: 4.196059858362478
Validation loss: 4.114440947696653

Epoch: 6| Step: 10
Training loss: 4.501198714922039
Validation loss: 4.108813964296341

Epoch: 6| Step: 11
Training loss: 3.5426684739157657
Validation loss: 4.103398317297107

Epoch: 6| Step: 12
Training loss: 4.039386197664422
Validation loss: 4.098653908863783

Epoch: 6| Step: 13
Training loss: 3.260777137662724
Validation loss: 4.09350926902327

Epoch: 27| Step: 0
Training loss: 4.307201060118977
Validation loss: 4.089677352459339

Epoch: 6| Step: 1
Training loss: 3.9719930299619004
Validation loss: 4.084100041453473

Epoch: 6| Step: 2
Training loss: 4.761667478189921
Validation loss: 4.080327127594103

Epoch: 6| Step: 3
Training loss: 4.643434849104268
Validation loss: 4.075843065419599

Epoch: 6| Step: 4
Training loss: 3.530741427097772
Validation loss: 4.070379001613551

Epoch: 6| Step: 5
Training loss: 3.7986609257870807
Validation loss: 4.065642740502675

Epoch: 6| Step: 6
Training loss: 2.973981564370372
Validation loss: 4.060966456565543

Epoch: 6| Step: 7
Training loss: 3.9329623764027057
Validation loss: 4.056682549108312

Epoch: 6| Step: 8
Training loss: 4.878585279549183
Validation loss: 4.052438257881751

Epoch: 6| Step: 9
Training loss: 4.608151030886674
Validation loss: 4.0486460066800625

Epoch: 6| Step: 10
Training loss: 4.199370228009153
Validation loss: 4.044136131910758

Epoch: 6| Step: 11
Training loss: 3.9165988402209035
Validation loss: 4.039139806522891

Epoch: 6| Step: 12
Training loss: 4.733574022074751
Validation loss: 4.034401364170156

Epoch: 6| Step: 13
Training loss: 3.989422163268428
Validation loss: 4.030470543550616

Epoch: 28| Step: 0
Training loss: 4.31155915985343
Validation loss: 4.02561791851951

Epoch: 6| Step: 1
Training loss: 4.121482736751522
Validation loss: 4.020595477617598

Epoch: 6| Step: 2
Training loss: 4.9910677279335065
Validation loss: 4.017390791859287

Epoch: 6| Step: 3
Training loss: 3.573390977404037
Validation loss: 4.0117349546923

Epoch: 6| Step: 4
Training loss: 3.042753748221248
Validation loss: 4.006201665133439

Epoch: 6| Step: 5
Training loss: 3.80283443748359
Validation loss: 4.001939263730164

Epoch: 6| Step: 6
Training loss: 4.467458044707929
Validation loss: 3.9979556343017633

Epoch: 6| Step: 7
Training loss: 3.7389620775828334
Validation loss: 3.9919480104520217

Epoch: 6| Step: 8
Training loss: 3.4877814504066595
Validation loss: 3.9875973305936294

Epoch: 6| Step: 9
Training loss: 4.881178437513094
Validation loss: 3.9837056514017823

Epoch: 6| Step: 10
Training loss: 4.239598279672031
Validation loss: 3.9789427982221928

Epoch: 6| Step: 11
Training loss: 3.9484814296100703
Validation loss: 3.97356799180536

Epoch: 6| Step: 12
Training loss: 4.647799202828105
Validation loss: 3.9697630983417604

Epoch: 6| Step: 13
Training loss: 4.067544475516246
Validation loss: 3.9643766682426

Epoch: 29| Step: 0
Training loss: 3.9688225686955456
Validation loss: 3.9600108691108096

Epoch: 6| Step: 1
Training loss: 5.025950608556885
Validation loss: 3.954993528866367

Epoch: 6| Step: 2
Training loss: 3.6389350079306664
Validation loss: 3.951208260595411

Epoch: 6| Step: 3
Training loss: 3.128287912674823
Validation loss: 3.946323539685861

Epoch: 6| Step: 4
Training loss: 3.58145240091905
Validation loss: 3.9424300639404732

Epoch: 6| Step: 5
Training loss: 4.018872324741512
Validation loss: 3.9378242157359513

Epoch: 6| Step: 6
Training loss: 4.973964426193367
Validation loss: 3.933609124929869

Epoch: 6| Step: 7
Training loss: 4.217042704665633
Validation loss: 3.9287913256777878

Epoch: 6| Step: 8
Training loss: 3.8541033490452836
Validation loss: 3.9234841782884593

Epoch: 6| Step: 9
Training loss: 4.076380096951733
Validation loss: 3.919219172782537

Epoch: 6| Step: 10
Training loss: 4.767887366430214
Validation loss: 3.9141907813699404

Epoch: 6| Step: 11
Training loss: 3.4844424664590155
Validation loss: 3.90960585829622

Epoch: 6| Step: 12
Training loss: 3.5246477099835363
Validation loss: 3.9049037393647015

Epoch: 6| Step: 13
Training loss: 4.133861848409824
Validation loss: 3.9006755675746367

Epoch: 30| Step: 0
Training loss: 3.397876210832382
Validation loss: 3.896227233611337

Epoch: 6| Step: 1
Training loss: 4.165743204143901
Validation loss: 3.8911558552533245

Epoch: 6| Step: 2
Training loss: 3.1489913112584715
Validation loss: 3.886833150895676

Epoch: 6| Step: 3
Training loss: 3.6908315059760413
Validation loss: 3.8820375095070534

Epoch: 6| Step: 4
Training loss: 4.063893710000501
Validation loss: 3.8777009513395453

Epoch: 6| Step: 5
Training loss: 4.76479725137174
Validation loss: 3.8736801360761395

Epoch: 6| Step: 6
Training loss: 3.266740234351061
Validation loss: 3.8693705860643286

Epoch: 6| Step: 7
Training loss: 4.739222295515225
Validation loss: 3.864858112009071

Epoch: 6| Step: 8
Training loss: 4.185092518648594
Validation loss: 3.8604683299072193

Epoch: 6| Step: 9
Training loss: 4.436668640487847
Validation loss: 3.8560008693452703

Epoch: 6| Step: 10
Training loss: 3.3247653677622706
Validation loss: 3.8508654435010583

Epoch: 6| Step: 11
Training loss: 4.161018332406392
Validation loss: 3.8462964645255373

Epoch: 6| Step: 12
Training loss: 4.281292768946956
Validation loss: 3.841931189299964

Epoch: 6| Step: 13
Training loss: 3.9154026074894714
Validation loss: 3.837305139819348

Epoch: 31| Step: 0
Training loss: 4.437847070150344
Validation loss: 3.8319396028437778

Epoch: 6| Step: 1
Training loss: 4.229384743966905
Validation loss: 3.828409716340009

Epoch: 6| Step: 2
Training loss: 4.143653539395037
Validation loss: 3.8228979014976407

Epoch: 6| Step: 3
Training loss: 4.129068853231508
Validation loss: 3.8181298384492943

Epoch: 6| Step: 4
Training loss: 3.958798404610875
Validation loss: 3.8136148959251512

Epoch: 6| Step: 5
Training loss: 3.3734057864473734
Validation loss: 3.8086112362712896

Epoch: 6| Step: 6
Training loss: 4.190458249181677
Validation loss: 3.803997355798818

Epoch: 6| Step: 7
Training loss: 3.8505429665288715
Validation loss: 3.7989681633223364

Epoch: 6| Step: 8
Training loss: 3.9916116258069065
Validation loss: 3.7944174349636364

Epoch: 6| Step: 9
Training loss: 4.2775600152233855
Validation loss: 3.790370335296631

Epoch: 6| Step: 10
Training loss: 2.6540949437996986
Validation loss: 3.7857372466605193

Epoch: 6| Step: 11
Training loss: 4.1402401151454455
Validation loss: 3.7820843493306255

Epoch: 6| Step: 12
Training loss: 3.8898428624832664
Validation loss: 3.776999383250496

Epoch: 6| Step: 13
Training loss: 3.5419317408079687
Validation loss: 3.772477685395939

Epoch: 32| Step: 0
Training loss: 4.190065423641322
Validation loss: 3.767994664014965

Epoch: 6| Step: 1
Training loss: 3.979770048773331
Validation loss: 3.76374092296062

Epoch: 6| Step: 2
Training loss: 4.0736044898884165
Validation loss: 3.7595621901131153

Epoch: 6| Step: 3
Training loss: 3.0461498473510935
Validation loss: 3.754921978505234

Epoch: 6| Step: 4
Training loss: 3.022903907866383
Validation loss: 3.750700493131034

Epoch: 6| Step: 5
Training loss: 4.041763199639842
Validation loss: 3.7464017453039107

Epoch: 6| Step: 6
Training loss: 4.156905043761125
Validation loss: 3.7425847808265886

Epoch: 6| Step: 7
Training loss: 3.7267984439523563
Validation loss: 3.7376294924822773

Epoch: 6| Step: 8
Training loss: 4.011369045508365
Validation loss: 3.7339024583961935

Epoch: 6| Step: 9
Training loss: 4.696377092418096
Validation loss: 3.729240153918782

Epoch: 6| Step: 10
Training loss: 3.8981380615211267
Validation loss: 3.7243696434672566

Epoch: 6| Step: 11
Training loss: 3.5214536372497247
Validation loss: 3.719730942372115

Epoch: 6| Step: 12
Training loss: 4.175139674973999
Validation loss: 3.715907637463546

Epoch: 6| Step: 13
Training loss: 3.3475525527259045
Validation loss: 3.711600625201874

Epoch: 33| Step: 0
Training loss: 2.149739817467778
Validation loss: 3.70684215839189

Epoch: 6| Step: 1
Training loss: 3.7236675157788195
Validation loss: 3.7031810343133853

Epoch: 6| Step: 2
Training loss: 4.089677721677773
Validation loss: 3.699164546259265

Epoch: 6| Step: 3
Training loss: 2.998953477794395
Validation loss: 3.694701194605517

Epoch: 6| Step: 4
Training loss: 3.951550679096779
Validation loss: 3.6902466151763784

Epoch: 6| Step: 5
Training loss: 3.94426303917997
Validation loss: 3.6872610241022103

Epoch: 6| Step: 6
Training loss: 3.9351967023959364
Validation loss: 3.682734605334692

Epoch: 6| Step: 7
Training loss: 3.5412615076828056
Validation loss: 3.677974920632958

Epoch: 6| Step: 8
Training loss: 4.112444873196845
Validation loss: 3.674305624032491

Epoch: 6| Step: 9
Training loss: 3.5935905089439
Validation loss: 3.6699487706488507

Epoch: 6| Step: 10
Training loss: 4.99759310965273
Validation loss: 3.665837750517125

Epoch: 6| Step: 11
Training loss: 3.955461497623215
Validation loss: 3.661388058302288

Epoch: 6| Step: 12
Training loss: 3.1621534074682844
Validation loss: 3.6570505314605044

Epoch: 6| Step: 13
Training loss: 4.453745587925167
Validation loss: 3.6521512611514657

Epoch: 34| Step: 0
Training loss: 3.454309596903521
Validation loss: 3.6477323254961074

Epoch: 6| Step: 1
Training loss: 3.3500932082688917
Validation loss: 3.643679449446424

Epoch: 6| Step: 2
Training loss: 3.453445902969428
Validation loss: 3.6391234132225625

Epoch: 6| Step: 3
Training loss: 4.022185551199294
Validation loss: 3.635060698270481

Epoch: 6| Step: 4
Training loss: 4.027299231385855
Validation loss: 3.630700025640806

Epoch: 6| Step: 5
Training loss: 4.019979409312185
Validation loss: 3.6261987786728302

Epoch: 6| Step: 6
Training loss: 4.334217617090185
Validation loss: 3.622348363671807

Epoch: 6| Step: 7
Training loss: 4.353406970660711
Validation loss: 3.61735284920347

Epoch: 6| Step: 8
Training loss: 3.241140173461098
Validation loss: 3.612477498055921

Epoch: 6| Step: 9
Training loss: 4.096714955338358
Validation loss: 3.60819847230803

Epoch: 6| Step: 10
Training loss: 3.6465271353193858
Validation loss: 3.603935623848619

Epoch: 6| Step: 11
Training loss: 3.6405720850164704
Validation loss: 3.599581755078348

Epoch: 6| Step: 12
Training loss: 3.1267419156398315
Validation loss: 3.594886975870457

Epoch: 6| Step: 13
Training loss: 3.553146799736954
Validation loss: 3.590543803909113

Epoch: 35| Step: 0
Training loss: 2.701637873306918
Validation loss: 3.586355812423757

Epoch: 6| Step: 1
Training loss: 4.124886019894856
Validation loss: 3.5822162365475547

Epoch: 6| Step: 2
Training loss: 3.6738805609960763
Validation loss: 3.5778526857746873

Epoch: 6| Step: 3
Training loss: 3.6971245288878887
Validation loss: 3.573139721253635

Epoch: 6| Step: 4
Training loss: 4.2309114712365945
Validation loss: 3.568947050285233

Epoch: 6| Step: 5
Training loss: 3.7382343093970345
Validation loss: 3.5642278396271805

Epoch: 6| Step: 6
Training loss: 3.6605936967612993
Validation loss: 3.5602495101043696

Epoch: 6| Step: 7
Training loss: 3.665306677171738
Validation loss: 3.5560609281188373

Epoch: 6| Step: 8
Training loss: 3.397783308532662
Validation loss: 3.5512663422812403

Epoch: 6| Step: 9
Training loss: 3.8480194694914744
Validation loss: 3.547821770005772

Epoch: 6| Step: 10
Training loss: 3.787486458429987
Validation loss: 3.542791965704727

Epoch: 6| Step: 11
Training loss: 3.537214794725975
Validation loss: 3.538476801470411

Epoch: 6| Step: 12
Training loss: 3.4854677137675254
Validation loss: 3.5341387160716895

Epoch: 6| Step: 13
Training loss: 3.970725701638965
Validation loss: 3.5299154364783516

Epoch: 36| Step: 0
Training loss: 3.070672167030054
Validation loss: 3.5260042482510263

Epoch: 6| Step: 1
Training loss: 3.1441824583345714
Validation loss: 3.521337296271944

Epoch: 6| Step: 2
Training loss: 3.229086122226135
Validation loss: 3.5179698216956736

Epoch: 6| Step: 3
Training loss: 3.6627239067716157
Validation loss: 3.5140556619209278

Epoch: 6| Step: 4
Training loss: 2.8840815534343602
Validation loss: 3.510790562550641

Epoch: 6| Step: 5
Training loss: 3.5773558477100122
Validation loss: 3.506331099117972

Epoch: 6| Step: 6
Training loss: 4.020338802199717
Validation loss: 3.502162333514028

Epoch: 6| Step: 7
Training loss: 4.255018973212969
Validation loss: 3.498181858578694

Epoch: 6| Step: 8
Training loss: 3.5019272538956527
Validation loss: 3.4935427773153678

Epoch: 6| Step: 9
Training loss: 3.3578124204753657
Validation loss: 3.4888979717981417

Epoch: 6| Step: 10
Training loss: 3.822700736923081
Validation loss: 3.485747838100797

Epoch: 6| Step: 11
Training loss: 3.7836966677619404
Validation loss: 3.481277548808245

Epoch: 6| Step: 12
Training loss: 4.147034948580262
Validation loss: 3.4777244654852804

Epoch: 6| Step: 13
Training loss: 4.112804533747319
Validation loss: 3.4731731566460593

Epoch: 37| Step: 0
Training loss: 3.462431918690235
Validation loss: 3.4686287523678474

Epoch: 6| Step: 1
Training loss: 2.5140058151881925
Validation loss: 3.4644925908270827

Epoch: 6| Step: 2
Training loss: 3.2352189147083115
Validation loss: 3.4604077418099513

Epoch: 6| Step: 3
Training loss: 3.268757608962822
Validation loss: 3.4570073645964516

Epoch: 6| Step: 4
Training loss: 3.653557723549286
Validation loss: 3.4527330132922067

Epoch: 6| Step: 5
Training loss: 4.152477875891168
Validation loss: 3.4495843885776236

Epoch: 6| Step: 6
Training loss: 3.897484183092793
Validation loss: 3.445238869500912

Epoch: 6| Step: 7
Training loss: 2.9644242745087745
Validation loss: 3.441114963958667

Epoch: 6| Step: 8
Training loss: 2.5050672675140895
Validation loss: 3.4378474897976954

Epoch: 6| Step: 9
Training loss: 3.9175159805059545
Validation loss: 3.4341867373894446

Epoch: 6| Step: 10
Training loss: 4.226044402741802
Validation loss: 3.4293862734095377

Epoch: 6| Step: 11
Training loss: 4.182579907864158
Validation loss: 3.4257105867528077

Epoch: 6| Step: 12
Training loss: 3.9618603836947783
Validation loss: 3.4213744890493394

Epoch: 6| Step: 13
Training loss: 3.5565620593841714
Validation loss: 3.4174704807944187

Epoch: 38| Step: 0
Training loss: 3.357468174764381
Validation loss: 3.4129234526326946

Epoch: 6| Step: 1
Training loss: 3.520890289989602
Validation loss: 3.40842974618909

Epoch: 6| Step: 2
Training loss: 3.961550693336389
Validation loss: 3.405133708993086

Epoch: 6| Step: 3
Training loss: 3.2944043199033795
Validation loss: 3.400289190372738

Epoch: 6| Step: 4
Training loss: 3.332367963872397
Validation loss: 3.396808008529463

Epoch: 6| Step: 5
Training loss: 3.937986797940282
Validation loss: 3.392896345576531

Epoch: 6| Step: 6
Training loss: 2.8065913547128005
Validation loss: 3.387865411397839

Epoch: 6| Step: 7
Training loss: 4.269800175942718
Validation loss: 3.3837414905604093

Epoch: 6| Step: 8
Training loss: 2.5650397949403603
Validation loss: 3.3795228487406797

Epoch: 6| Step: 9
Training loss: 3.533600151735993
Validation loss: 3.375169349471806

Epoch: 6| Step: 10
Training loss: 3.3879065800997816
Validation loss: 3.3721711347782226

Epoch: 6| Step: 11
Training loss: 3.522826012712624
Validation loss: 3.3678555633490364

Epoch: 6| Step: 12
Training loss: 4.130177341006565
Validation loss: 3.3648712617640886

Epoch: 6| Step: 13
Training loss: 3.3365649452925794
Validation loss: 3.3596413321970657

Epoch: 39| Step: 0
Training loss: 3.1552454794586953
Validation loss: 3.3554540655298104

Epoch: 6| Step: 1
Training loss: 3.7631001852396104
Validation loss: 3.3513949929610507

Epoch: 6| Step: 2
Training loss: 2.79201438977706
Validation loss: 3.3470091934442556

Epoch: 6| Step: 3
Training loss: 3.795307353845832
Validation loss: 3.3430619051723793

Epoch: 6| Step: 4
Training loss: 3.5342099098079154
Validation loss: 3.338590949890464

Epoch: 6| Step: 5
Training loss: 3.9961938631078104
Validation loss: 3.3352774116631205

Epoch: 6| Step: 6
Training loss: 3.370364644050003
Validation loss: 3.3312644061170102

Epoch: 6| Step: 7
Training loss: 3.570743860275929
Validation loss: 3.3275338544874047

Epoch: 6| Step: 8
Training loss: 3.9252615969035713
Validation loss: 3.3229562204970513

Epoch: 6| Step: 9
Training loss: 3.3074629291235986
Validation loss: 3.3193403668405406

Epoch: 6| Step: 10
Training loss: 3.032466998066154
Validation loss: 3.31506469095744

Epoch: 6| Step: 11
Training loss: 3.046031972215137
Validation loss: 3.3114394163487564

Epoch: 6| Step: 12
Training loss: 3.597893093070927
Validation loss: 3.3073741191688533

Epoch: 6| Step: 13
Training loss: 3.427768596250114
Validation loss: 3.3039346954258946

Epoch: 40| Step: 0
Training loss: 2.9118604751948456
Validation loss: 3.301211020275528

Epoch: 6| Step: 1
Training loss: 3.2104759162982144
Validation loss: 3.2968850037722808

Epoch: 6| Step: 2
Training loss: 3.1532103754740297
Validation loss: 3.2936798329513075

Epoch: 6| Step: 3
Training loss: 3.464468710986124
Validation loss: 3.2893656321397073

Epoch: 6| Step: 4
Training loss: 3.0973786930841345
Validation loss: 3.285855926744293

Epoch: 6| Step: 5
Training loss: 3.8125442752456014
Validation loss: 3.2824029864666193

Epoch: 6| Step: 6
Training loss: 3.3354896405584817
Validation loss: 3.2794387662482523

Epoch: 6| Step: 7
Training loss: 3.920142532015064
Validation loss: 3.2759659864036266

Epoch: 6| Step: 8
Training loss: 4.005503682861616
Validation loss: 3.2724240235595827

Epoch: 6| Step: 9
Training loss: 2.8331180565524057
Validation loss: 3.2684591427726057

Epoch: 6| Step: 10
Training loss: 3.866586719431845
Validation loss: 3.2647018101139005

Epoch: 6| Step: 11
Training loss: 3.4117919914013415
Validation loss: 3.2606415757351495

Epoch: 6| Step: 12
Training loss: 3.592035166400479
Validation loss: 3.256274231413572

Epoch: 6| Step: 13
Training loss: 2.9331337152819392
Validation loss: 3.252564311133304

Epoch: 41| Step: 0
Training loss: 3.7512055048834854
Validation loss: 3.2489045326513226

Epoch: 6| Step: 1
Training loss: 3.8199170615894102
Validation loss: 3.2452890046612533

Epoch: 6| Step: 2
Training loss: 3.0893575910401476
Validation loss: 3.241865100687363

Epoch: 6| Step: 3
Training loss: 4.1621552012376295
Validation loss: 3.2368303153732363

Epoch: 6| Step: 4
Training loss: 3.3673030541367135
Validation loss: 3.2336469927448395

Epoch: 6| Step: 5
Training loss: 3.7492235015565587
Validation loss: 3.2291563874768143

Epoch: 6| Step: 6
Training loss: 3.2235903594125994
Validation loss: 3.2264375700945225

Epoch: 6| Step: 7
Training loss: 3.3164690818271367
Validation loss: 3.222140961111999

Epoch: 6| Step: 8
Training loss: 2.9354170861299176
Validation loss: 3.2184365376052084

Epoch: 6| Step: 9
Training loss: 3.022761148509339
Validation loss: 3.2150384435337727

Epoch: 6| Step: 10
Training loss: 2.8016911814848386
Validation loss: 3.211481547923924

Epoch: 6| Step: 11
Training loss: 3.036021930268584
Validation loss: 3.2076757591103098

Epoch: 6| Step: 12
Training loss: 3.328542763722834
Validation loss: 3.204427017197446

Epoch: 6| Step: 13
Training loss: 3.2631590078964017
Validation loss: 3.2002062269157983

Epoch: 42| Step: 0
Training loss: 3.786100696883071
Validation loss: 3.1966631956217526

Epoch: 6| Step: 1
Training loss: 3.7235287007229343
Validation loss: 3.192168762181912

Epoch: 6| Step: 2
Training loss: 3.1632160326075285
Validation loss: 3.1890332891162796

Epoch: 6| Step: 3
Training loss: 3.6087165000637045
Validation loss: 3.1859248389465984

Epoch: 6| Step: 4
Training loss: 3.093357562211396
Validation loss: 3.1824656728734633

Epoch: 6| Step: 5
Training loss: 3.7773153467698286
Validation loss: 3.177888078338833

Epoch: 6| Step: 6
Training loss: 2.8204464022056586
Validation loss: 3.1745464556694536

Epoch: 6| Step: 7
Training loss: 2.671322771079908
Validation loss: 3.17020343400516

Epoch: 6| Step: 8
Training loss: 3.0144980108545916
Validation loss: 3.1661019992989172

Epoch: 6| Step: 9
Training loss: 3.3046563329647314
Validation loss: 3.1631504581062337

Epoch: 6| Step: 10
Training loss: 3.461133964249498
Validation loss: 3.1616549653309485

Epoch: 6| Step: 11
Training loss: 3.3377737668799647
Validation loss: 3.1577122766293493

Epoch: 6| Step: 12
Training loss: 3.004548122051735
Validation loss: 3.1543898687836727

Epoch: 6| Step: 13
Training loss: 3.401956074085155
Validation loss: 3.1508317020966605

Epoch: 43| Step: 0
Training loss: 2.8434791855188606
Validation loss: 3.147738988992461

Epoch: 6| Step: 1
Training loss: 3.933718363995614
Validation loss: 3.144467611044056

Epoch: 6| Step: 2
Training loss: 3.4906330105135535
Validation loss: 3.1419440894912265

Epoch: 6| Step: 3
Training loss: 3.6642144991127106
Validation loss: 3.1381236374768573

Epoch: 6| Step: 4
Training loss: 3.1390694329055626
Validation loss: 3.134403348054797

Epoch: 6| Step: 5
Training loss: 3.8733664730403006
Validation loss: 3.130373708679951

Epoch: 6| Step: 6
Training loss: 2.6866069463177316
Validation loss: 3.1281131877542765

Epoch: 6| Step: 7
Training loss: 2.830573439445398
Validation loss: 3.124563949680158

Epoch: 6| Step: 8
Training loss: 3.501708567346089
Validation loss: 3.1215686015136512

Epoch: 6| Step: 9
Training loss: 3.1046672818466776
Validation loss: 3.1183058565792408

Epoch: 6| Step: 10
Training loss: 3.4402269124443587
Validation loss: 3.114534643466694

Epoch: 6| Step: 11
Training loss: 3.0662933523205753
Validation loss: 3.11232131077397

Epoch: 6| Step: 12
Training loss: 3.0446769415198482
Validation loss: 3.1087994066706086

Epoch: 6| Step: 13
Training loss: 2.845436350741841
Validation loss: 3.107106069473363

Epoch: 44| Step: 0
Training loss: 3.065517262656742
Validation loss: 3.104139549768445

Epoch: 6| Step: 1
Training loss: 3.9732851564016936
Validation loss: 3.101076294578899

Epoch: 6| Step: 2
Training loss: 2.033565433992379
Validation loss: 3.0980654202123703

Epoch: 6| Step: 3
Training loss: 2.5633934138300702
Validation loss: 3.0941665446059408

Epoch: 6| Step: 4
Training loss: 3.3200007034209094
Validation loss: 3.090746272336761

Epoch: 6| Step: 5
Training loss: 3.5608105585387815
Validation loss: 3.088416102892766

Epoch: 6| Step: 6
Training loss: 3.5769138201963737
Validation loss: 3.084009719238766

Epoch: 6| Step: 7
Training loss: 3.7173464394351585
Validation loss: 3.081466320045459

Epoch: 6| Step: 8
Training loss: 3.1557477796985403
Validation loss: 3.0779221108016928

Epoch: 6| Step: 9
Training loss: 3.217851383729677
Validation loss: 3.075241442249877

Epoch: 6| Step: 10
Training loss: 3.055850848947105
Validation loss: 3.072136257234356

Epoch: 6| Step: 11
Training loss: 3.140011551951158
Validation loss: 3.070695265966262

Epoch: 6| Step: 12
Training loss: 2.5533449337382863
Validation loss: 3.0677371625511722

Epoch: 6| Step: 13
Training loss: 3.6642524979057054
Validation loss: 3.06257424296999

Epoch: 45| Step: 0
Training loss: 2.695461987758984
Validation loss: 3.0618621265675174

Epoch: 6| Step: 1
Training loss: 3.2262032149483497
Validation loss: 3.0590056640964964

Epoch: 6| Step: 2
Training loss: 3.4456075598214646
Validation loss: 3.0549479680018927

Epoch: 6| Step: 3
Training loss: 2.7067379482949114
Validation loss: 3.051719120848474

Epoch: 6| Step: 4
Training loss: 3.0855788396697745
Validation loss: 3.0497750590189456

Epoch: 6| Step: 5
Training loss: 3.4028984169540526
Validation loss: 3.0476213502910694

Epoch: 6| Step: 6
Training loss: 2.4079915469645417
Validation loss: 3.044476756932227

Epoch: 6| Step: 7
Training loss: 2.985759633594039
Validation loss: 3.04298660083737

Epoch: 6| Step: 8
Training loss: 2.85189223799887
Validation loss: 3.0399825951429347

Epoch: 6| Step: 9
Training loss: 3.347428339674774
Validation loss: 3.0372765792498817

Epoch: 6| Step: 10
Training loss: 3.4085255992230836
Validation loss: 3.033995207863034

Epoch: 6| Step: 11
Training loss: 2.9727983678864596
Validation loss: 3.0306149764684207

Epoch: 6| Step: 12
Training loss: 3.7727087987675185
Validation loss: 3.0280556774832457

Epoch: 6| Step: 13
Training loss: 3.8867486847151267
Validation loss: 3.025782514408214

Epoch: 46| Step: 0
Training loss: 3.3505701405467745
Validation loss: 3.022699915035731

Epoch: 6| Step: 1
Training loss: 3.1362047356219893
Validation loss: 3.0196846235834585

Epoch: 6| Step: 2
Training loss: 3.5169974805764856
Validation loss: 3.018933354213575

Epoch: 6| Step: 3
Training loss: 3.275893134715802
Validation loss: 3.0173752338960282

Epoch: 6| Step: 4
Training loss: 3.0479293172855804
Validation loss: 3.0115019657999036

Epoch: 6| Step: 5
Training loss: 2.496799709440567
Validation loss: 3.0092935590066263

Epoch: 6| Step: 6
Training loss: 3.3151945275125296
Validation loss: 3.0044844806262074

Epoch: 6| Step: 7
Training loss: 2.9037724260637514
Validation loss: 3.0024532777579784

Epoch: 6| Step: 8
Training loss: 3.509325410623313
Validation loss: 3.0002484748513716

Epoch: 6| Step: 9
Training loss: 3.0789295055301955
Validation loss: 2.999348304747912

Epoch: 6| Step: 10
Training loss: 3.304086328390059
Validation loss: 2.9944133928757526

Epoch: 6| Step: 11
Training loss: 3.134610466370408
Validation loss: 2.9906118841092924

Epoch: 6| Step: 12
Training loss: 3.2079285865882974
Validation loss: 2.989280849311109

Epoch: 6| Step: 13
Training loss: 2.583047225449544
Validation loss: 2.9870251510969448

Epoch: 47| Step: 0
Training loss: 3.3187341147292257
Validation loss: 2.985914009942707

Epoch: 6| Step: 1
Training loss: 3.1455031131115936
Validation loss: 2.984174112479146

Epoch: 6| Step: 2
Training loss: 3.210570228529782
Validation loss: 2.9821079158014037

Epoch: 6| Step: 3
Training loss: 3.8399342771707334
Validation loss: 2.9796486755839626

Epoch: 6| Step: 4
Training loss: 3.3105258457112323
Validation loss: 2.9778498071132407

Epoch: 6| Step: 5
Training loss: 3.302493979414622
Validation loss: 2.973841587498672

Epoch: 6| Step: 6
Training loss: 2.715789059510933
Validation loss: 2.9708612932028187

Epoch: 6| Step: 7
Training loss: 2.916696457483647
Validation loss: 2.9672224297525704

Epoch: 6| Step: 8
Training loss: 2.963958246130037
Validation loss: 2.9632792208579386

Epoch: 6| Step: 9
Training loss: 3.527036330301767
Validation loss: 2.9602365595136564

Epoch: 6| Step: 10
Training loss: 3.0101341106156054
Validation loss: 2.957070784176429

Epoch: 6| Step: 11
Training loss: 3.420998657047886
Validation loss: 2.953930349721651

Epoch: 6| Step: 12
Training loss: 1.8766692678299128
Validation loss: 2.9504926948967625

Epoch: 6| Step: 13
Training loss: 2.549045597313598
Validation loss: 2.9493405617995547

Epoch: 48| Step: 0
Training loss: 3.349990013449288
Validation loss: 2.9474924184385145

Epoch: 6| Step: 1
Training loss: 3.1965162625960493
Validation loss: 2.945265630455367

Epoch: 6| Step: 2
Training loss: 2.53465851539667
Validation loss: 2.943217604238714

Epoch: 6| Step: 3
Training loss: 2.316722417293246
Validation loss: 2.940946315894984

Epoch: 6| Step: 4
Training loss: 2.372138457813352
Validation loss: 2.93826601848933

Epoch: 6| Step: 5
Training loss: 2.7697679770897095
Validation loss: 2.935585710374805

Epoch: 6| Step: 6
Training loss: 3.234318682046674
Validation loss: 2.9338997631749923

Epoch: 6| Step: 7
Training loss: 3.0774148034606243
Validation loss: 2.93164265814907

Epoch: 6| Step: 8
Training loss: 3.4951729867280275
Validation loss: 2.9297184921320794

Epoch: 6| Step: 9
Training loss: 3.1382915881259428
Validation loss: 2.927641809657302

Epoch: 6| Step: 10
Training loss: 2.9807983056252496
Validation loss: 2.925779919014982

Epoch: 6| Step: 11
Training loss: 3.713663677905911
Validation loss: 2.923058118997804

Epoch: 6| Step: 12
Training loss: 2.9618475730770784
Validation loss: 2.920039152684158

Epoch: 6| Step: 13
Training loss: 3.505236523058118
Validation loss: 2.9182993315613657

Epoch: 49| Step: 0
Training loss: 2.833078503836229
Validation loss: 2.9162415785054048

Epoch: 6| Step: 1
Training loss: 2.6730026631682744
Validation loss: 2.913161628567617

Epoch: 6| Step: 2
Training loss: 2.847502862595617
Validation loss: 2.9114522279915347

Epoch: 6| Step: 3
Training loss: 2.6333767883320784
Validation loss: 2.9086384000244796

Epoch: 6| Step: 4
Training loss: 3.1990920566908696
Validation loss: 2.906975494946352

Epoch: 6| Step: 5
Training loss: 3.3811633030182446
Validation loss: 2.9042623962941643

Epoch: 6| Step: 6
Training loss: 3.1083364621335017
Validation loss: 2.9030745327727043

Epoch: 6| Step: 7
Training loss: 3.24464944412471
Validation loss: 2.90114040423602

Epoch: 6| Step: 8
Training loss: 2.906840766831472
Validation loss: 2.89910399801693

Epoch: 6| Step: 9
Training loss: 2.908746283069395
Validation loss: 2.8975192635098534

Epoch: 6| Step: 10
Training loss: 2.548509880455402
Validation loss: 2.8923702847896404

Epoch: 6| Step: 11
Training loss: 3.1287538488945814
Validation loss: 2.8904465216446247

Epoch: 6| Step: 12
Training loss: 3.6794693997615657
Validation loss: 2.8888203804337405

Epoch: 6| Step: 13
Training loss: 3.25726577232876
Validation loss: 2.886865442632786

Epoch: 50| Step: 0
Training loss: 3.05539673671849
Validation loss: 2.8834097638961014

Epoch: 6| Step: 1
Training loss: 3.0224940678717345
Validation loss: 2.881621186129351

Epoch: 6| Step: 2
Training loss: 3.382444295600712
Validation loss: 2.8801921593394173

Epoch: 6| Step: 3
Training loss: 2.655469981836054
Validation loss: 2.8772126680915955

Epoch: 6| Step: 4
Training loss: 2.462230330674599
Validation loss: 2.8747754216591574

Epoch: 6| Step: 5
Training loss: 2.8711593516633735
Validation loss: 2.8721285592193095

Epoch: 6| Step: 6
Training loss: 3.42450045541545
Validation loss: 2.8705165845166736

Epoch: 6| Step: 7
Training loss: 3.239657156901436
Validation loss: 2.8664898071143328

Epoch: 6| Step: 8
Training loss: 3.1774066364489153
Validation loss: 2.8657113553468534

Epoch: 6| Step: 9
Training loss: 2.2256943465808274
Validation loss: 2.8620354621124386

Epoch: 6| Step: 10
Training loss: 3.0550199752129603
Validation loss: 2.859755735636863

Epoch: 6| Step: 11
Training loss: 3.3802799321111845
Validation loss: 2.8586121116424485

Epoch: 6| Step: 12
Training loss: 2.1602260033380873
Validation loss: 2.8581319584175433

Epoch: 6| Step: 13
Training loss: 3.6415366148840937
Validation loss: 2.856264907052325

Epoch: 51| Step: 0
Training loss: 3.1323966418580884
Validation loss: 2.8539665643942893

Epoch: 6| Step: 1
Training loss: 3.150619318797493
Validation loss: 2.8555421693672014

Epoch: 6| Step: 2
Training loss: 2.9989601558367505
Validation loss: 2.857785517928619

Epoch: 6| Step: 3
Training loss: 3.216505138182602
Validation loss: 2.8516006815544546

Epoch: 6| Step: 4
Training loss: 3.2283259784363283
Validation loss: 2.848506987047278

Epoch: 6| Step: 5
Training loss: 3.3805025244044535
Validation loss: 2.8439343431130806

Epoch: 6| Step: 6
Training loss: 3.065953390126746
Validation loss: 2.8389500922017223

Epoch: 6| Step: 7
Training loss: 2.76922855417864
Validation loss: 2.8379873078572992

Epoch: 6| Step: 8
Training loss: 2.8956364260538043
Validation loss: 2.836005372075067

Epoch: 6| Step: 9
Training loss: 2.9943884182770475
Validation loss: 2.834599441115457

Epoch: 6| Step: 10
Training loss: 3.1070802741802788
Validation loss: 2.8340085057274766

Epoch: 6| Step: 11
Training loss: 2.7689746464167526
Validation loss: 2.833673251088259

Epoch: 6| Step: 12
Training loss: 2.339180420863538
Validation loss: 2.83227616577471

Epoch: 6| Step: 13
Training loss: 2.476745021123128
Validation loss: 2.830478946092888

Epoch: 52| Step: 0
Training loss: 2.6706082202072663
Validation loss: 2.83118721146395

Epoch: 6| Step: 1
Training loss: 2.840932938909137
Validation loss: 2.8278422556793505

Epoch: 6| Step: 2
Training loss: 2.2600316951858503
Validation loss: 2.8265508412151803

Epoch: 6| Step: 3
Training loss: 3.1032203068075925
Validation loss: 2.8270088086651635

Epoch: 6| Step: 4
Training loss: 3.188818864426462
Validation loss: 2.821342216880937

Epoch: 6| Step: 5
Training loss: 3.696529647677295
Validation loss: 2.818843483625802

Epoch: 6| Step: 6
Training loss: 2.7731963576669956
Validation loss: 2.8156215863872216

Epoch: 6| Step: 7
Training loss: 3.273487609625412
Validation loss: 2.812488188542183

Epoch: 6| Step: 8
Training loss: 3.420952659567072
Validation loss: 2.809886481004671

Epoch: 6| Step: 9
Training loss: 3.1375091431017177
Validation loss: 2.807770701758707

Epoch: 6| Step: 10
Training loss: 2.5655476194879148
Validation loss: 2.809131823670397

Epoch: 6| Step: 11
Training loss: 2.8725585522506583
Validation loss: 2.8082430447705367

Epoch: 6| Step: 12
Training loss: 2.6410960843779434
Validation loss: 2.803647385951777

Epoch: 6| Step: 13
Training loss: 2.660542867520565
Validation loss: 2.8139547612282496

Epoch: 53| Step: 0
Training loss: 3.1902820189846444
Validation loss: 2.8039890932160327

Epoch: 6| Step: 1
Training loss: 3.24442796623133
Validation loss: 2.796496218244939

Epoch: 6| Step: 2
Training loss: 2.82211136445849
Validation loss: 2.792961337562097

Epoch: 6| Step: 3
Training loss: 3.08239558282056
Validation loss: 2.793429548758143

Epoch: 6| Step: 4
Training loss: 3.251866025015994
Validation loss: 2.795428720826274

Epoch: 6| Step: 5
Training loss: 2.8608419688852194
Validation loss: 2.7969861994114087

Epoch: 6| Step: 6
Training loss: 3.120052846803352
Validation loss: 2.799868744941311

Epoch: 6| Step: 7
Training loss: 2.745262399996843
Validation loss: 2.7983901681026455

Epoch: 6| Step: 8
Training loss: 2.6318731698143294
Validation loss: 2.7947265926429243

Epoch: 6| Step: 9
Training loss: 3.271629252855471
Validation loss: 2.792311086340704

Epoch: 6| Step: 10
Training loss: 3.1079616689196206
Validation loss: 2.7876809809825573

Epoch: 6| Step: 11
Training loss: 2.7953065704743394
Validation loss: 2.7837077079384396

Epoch: 6| Step: 12
Training loss: 1.9653135310658767
Validation loss: 2.7787848130672463

Epoch: 6| Step: 13
Training loss: 2.750603956312359
Validation loss: 2.777782407332907

Epoch: 54| Step: 0
Training loss: 2.4403801554659754
Validation loss: 2.7741763567982054

Epoch: 6| Step: 1
Training loss: 3.072762402471668
Validation loss: 2.7734177423051007

Epoch: 6| Step: 2
Training loss: 2.653566688103559
Validation loss: 2.77085139811275

Epoch: 6| Step: 3
Training loss: 2.979898500698118
Validation loss: 2.7682205348390134

Epoch: 6| Step: 4
Training loss: 3.0396465926791936
Validation loss: 2.769857583774536

Epoch: 6| Step: 5
Training loss: 3.0459842260081276
Validation loss: 2.766416361029999

Epoch: 6| Step: 6
Training loss: 2.422402798146354
Validation loss: 2.7674197679081645

Epoch: 6| Step: 7
Training loss: 2.5193121763102964
Validation loss: 2.765328529688796

Epoch: 6| Step: 8
Training loss: 2.8404088776377048
Validation loss: 2.761500773085164

Epoch: 6| Step: 9
Training loss: 3.2557363504051753
Validation loss: 2.7609265318420424

Epoch: 6| Step: 10
Training loss: 3.054926948235852
Validation loss: 2.756450687867125

Epoch: 6| Step: 11
Training loss: 3.191491264789095
Validation loss: 2.7577379917241482

Epoch: 6| Step: 12
Training loss: 2.902385316040259
Validation loss: 2.756509056805741

Epoch: 6| Step: 13
Training loss: 3.082688719579272
Validation loss: 2.754172872708514

Epoch: 55| Step: 0
Training loss: 2.92700235021749
Validation loss: 2.7524491010504986

Epoch: 6| Step: 1
Training loss: 3.330672600896887
Validation loss: 2.7518863999287086

Epoch: 6| Step: 2
Training loss: 2.8747482189554785
Validation loss: 2.7497919610834

Epoch: 6| Step: 3
Training loss: 3.287563970255257
Validation loss: 2.7486010952583326

Epoch: 6| Step: 4
Training loss: 3.0741979654038487
Validation loss: 2.7465287899932305

Epoch: 6| Step: 5
Training loss: 3.1420500449718207
Validation loss: 2.7447818029661972

Epoch: 6| Step: 6
Training loss: 3.263350137025031
Validation loss: 2.744223336164533

Epoch: 6| Step: 7
Training loss: 2.278378076716291
Validation loss: 2.7410545653328464

Epoch: 6| Step: 8
Training loss: 2.2312206063351336
Validation loss: 2.740876234798803

Epoch: 6| Step: 9
Training loss: 2.751439498067046
Validation loss: 2.737215887855155

Epoch: 6| Step: 10
Training loss: 2.4786477921690513
Validation loss: 2.73668532362451

Epoch: 6| Step: 11
Training loss: 2.708489535791254
Validation loss: 2.7346644657051136

Epoch: 6| Step: 12
Training loss: 3.161858740163612
Validation loss: 2.732856314213195

Epoch: 6| Step: 13
Training loss: 2.594589005780537
Validation loss: 2.729475693035843

Epoch: 56| Step: 0
Training loss: 2.2672975877551735
Validation loss: 2.731156742299142

Epoch: 6| Step: 1
Training loss: 2.8776963899543584
Validation loss: 2.7397877915433564

Epoch: 6| Step: 2
Training loss: 2.971221858474286
Validation loss: 2.742611916529522

Epoch: 6| Step: 3
Training loss: 2.945401541985451
Validation loss: 2.731186961036476

Epoch: 6| Step: 4
Training loss: 2.9138134397378086
Validation loss: 2.7270127209836588

Epoch: 6| Step: 5
Training loss: 3.134222231913914
Validation loss: 2.725036944859258

Epoch: 6| Step: 6
Training loss: 2.6286081312264953
Validation loss: 2.7222117166467825

Epoch: 6| Step: 7
Training loss: 3.1115806845861242
Validation loss: 2.7211384421573848

Epoch: 6| Step: 8
Training loss: 2.4584176862652125
Validation loss: 2.721934561765721

Epoch: 6| Step: 9
Training loss: 3.1355111920037317
Validation loss: 2.721028553344759

Epoch: 6| Step: 10
Training loss: 2.9087861183196173
Validation loss: 2.721334318000407

Epoch: 6| Step: 11
Training loss: 2.7426368801624106
Validation loss: 2.721685031082169

Epoch: 6| Step: 12
Training loss: 2.875570323600889
Validation loss: 2.7218012878971423

Epoch: 6| Step: 13
Training loss: 3.0313302255612746
Validation loss: 2.719521380206321

Epoch: 57| Step: 0
Training loss: 2.8102502938114227
Validation loss: 2.7163552373894797

Epoch: 6| Step: 1
Training loss: 2.959202562603404
Validation loss: 2.714305474871455

Epoch: 6| Step: 2
Training loss: 2.7016565821665544
Validation loss: 2.7124118954696055

Epoch: 6| Step: 3
Training loss: 3.2125203113433316
Validation loss: 2.710013709485238

Epoch: 6| Step: 4
Training loss: 3.008939933027718
Validation loss: 2.7076247290860955

Epoch: 6| Step: 5
Training loss: 2.5824271632924978
Validation loss: 2.710053665361744

Epoch: 6| Step: 6
Training loss: 2.149841293864771
Validation loss: 2.7180071287195

Epoch: 6| Step: 7
Training loss: 2.6027079858405373
Validation loss: 2.715577331522601

Epoch: 6| Step: 8
Training loss: 2.681189291384581
Validation loss: 2.710479726993641

Epoch: 6| Step: 9
Training loss: 2.8884945828580317
Validation loss: 2.7031695558848603

Epoch: 6| Step: 10
Training loss: 2.8284056297717726
Validation loss: 2.7079270327075626

Epoch: 6| Step: 11
Training loss: 3.4843845794956603
Validation loss: 2.6985998626498984

Epoch: 6| Step: 12
Training loss: 2.567858512979616
Validation loss: 2.695950418805915

Epoch: 6| Step: 13
Training loss: 3.1293652372853633
Validation loss: 2.695940440279379

Epoch: 58| Step: 0
Training loss: 2.9029617575749054
Validation loss: 2.6952654848282878

Epoch: 6| Step: 1
Training loss: 2.8543650611552387
Validation loss: 2.69554166694939

Epoch: 6| Step: 2
Training loss: 2.8960750853989534
Validation loss: 2.6975538729101225

Epoch: 6| Step: 3
Training loss: 3.027362336132462
Validation loss: 2.6961404754177085

Epoch: 6| Step: 4
Training loss: 3.016871377592281
Validation loss: 2.6937606545788557

Epoch: 6| Step: 5
Training loss: 2.7665533027250437
Validation loss: 2.691606253033572

Epoch: 6| Step: 6
Training loss: 3.094030502395841
Validation loss: 2.6888869055914943

Epoch: 6| Step: 7
Training loss: 2.782350472462326
Validation loss: 2.6872048992219644

Epoch: 6| Step: 8
Training loss: 2.68516492312254
Validation loss: 2.684450134125877

Epoch: 6| Step: 9
Training loss: 2.5861217052854144
Validation loss: 2.684460407004184

Epoch: 6| Step: 10
Training loss: 2.3988605337433775
Validation loss: 2.6808861367769867

Epoch: 6| Step: 11
Training loss: 3.271156991548173
Validation loss: 2.680072041345106

Epoch: 6| Step: 12
Training loss: 2.5213138868401335
Validation loss: 2.6770946240217937

Epoch: 6| Step: 13
Training loss: 2.6535194274524683
Validation loss: 2.6749726718920823

Epoch: 59| Step: 0
Training loss: 2.4840389960171985
Validation loss: 2.675908188233722

Epoch: 6| Step: 1
Training loss: 2.967621116833922
Validation loss: 2.674999098614217

Epoch: 6| Step: 2
Training loss: 2.8948346801358764
Validation loss: 2.6734764846881487

Epoch: 6| Step: 3
Training loss: 2.3665859351917606
Validation loss: 2.6710611557661714

Epoch: 6| Step: 4
Training loss: 2.661930428057954
Validation loss: 2.6692211810200317

Epoch: 6| Step: 5
Training loss: 2.517990800581679
Validation loss: 2.669511980778551

Epoch: 6| Step: 6
Training loss: 2.89597925783601
Validation loss: 2.669074645237173

Epoch: 6| Step: 7
Training loss: 2.715996235537273
Validation loss: 2.66972584389641

Epoch: 6| Step: 8
Training loss: 2.9075599917440775
Validation loss: 2.664793948891113

Epoch: 6| Step: 9
Training loss: 2.7042467733390203
Validation loss: 2.666171003012845

Epoch: 6| Step: 10
Training loss: 2.9871190422762686
Validation loss: 2.662715000551846

Epoch: 6| Step: 11
Training loss: 2.595340011607032
Validation loss: 2.662917054037576

Epoch: 6| Step: 12
Training loss: 3.304430651772412
Validation loss: 2.6618920338033343

Epoch: 6| Step: 13
Training loss: 3.151101322502616
Validation loss: 2.6644122210199583

Epoch: 60| Step: 0
Training loss: 2.691630006743253
Validation loss: 2.6576506717729576

Epoch: 6| Step: 1
Training loss: 2.5220002135020323
Validation loss: 2.660277406358046

Epoch: 6| Step: 2
Training loss: 3.2111208972393133
Validation loss: 2.6593419946698593

Epoch: 6| Step: 3
Training loss: 2.6849033319841533
Validation loss: 2.6560193728387707

Epoch: 6| Step: 4
Training loss: 3.0824817478106725
Validation loss: 2.6561998549571313

Epoch: 6| Step: 5
Training loss: 2.8977410232843277
Validation loss: 2.6531163299742393

Epoch: 6| Step: 6
Training loss: 2.3179593982514315
Validation loss: 2.6541285401804284

Epoch: 6| Step: 7
Training loss: 2.905907067195837
Validation loss: 2.6559486068575335

Epoch: 6| Step: 8
Training loss: 2.8578890915874346
Validation loss: 2.6515924946952913

Epoch: 6| Step: 9
Training loss: 2.789436464856905
Validation loss: 2.646863936904432

Epoch: 6| Step: 10
Training loss: 2.5958484296119915
Validation loss: 2.649503930162792

Epoch: 6| Step: 11
Training loss: 2.761141229700202
Validation loss: 2.647725957750343

Epoch: 6| Step: 12
Training loss: 3.1414452236123016
Validation loss: 2.644692277537024

Epoch: 6| Step: 13
Training loss: 2.4415868097293982
Validation loss: 2.6468783189797036

Epoch: 61| Step: 0
Training loss: 2.8865306003150524
Validation loss: 2.6453651003894567

Epoch: 6| Step: 1
Training loss: 3.07657378946832
Validation loss: 2.644704958583766

Epoch: 6| Step: 2
Training loss: 2.8277478888713468
Validation loss: 2.6451839691439707

Epoch: 6| Step: 3
Training loss: 2.523336780533983
Validation loss: 2.643019889978011

Epoch: 6| Step: 4
Training loss: 2.1064121489551537
Validation loss: 2.643609701992661

Epoch: 6| Step: 5
Training loss: 2.630131927488363
Validation loss: 2.6423216265550646

Epoch: 6| Step: 6
Training loss: 3.0660476377684396
Validation loss: 2.6428428079395534

Epoch: 6| Step: 7
Training loss: 2.889727478905883
Validation loss: 2.6423803811380857

Epoch: 6| Step: 8
Training loss: 2.718333245267073
Validation loss: 2.6403239302392105

Epoch: 6| Step: 9
Training loss: 3.165456055240491
Validation loss: 2.639681028521712

Epoch: 6| Step: 10
Training loss: 2.66498726374748
Validation loss: 2.6401016800257207

Epoch: 6| Step: 11
Training loss: 2.8108843931447547
Validation loss: 2.6388592205276034

Epoch: 6| Step: 12
Training loss: 2.7352412132440724
Validation loss: 2.6376002667789944

Epoch: 6| Step: 13
Training loss: 2.657517702863696
Validation loss: 2.6352473620487267

Epoch: 62| Step: 0
Training loss: 2.5881535464907026
Validation loss: 2.634858978329426

Epoch: 6| Step: 1
Training loss: 2.7007748304428154
Validation loss: 2.633067796896036

Epoch: 6| Step: 2
Training loss: 3.18372585279985
Validation loss: 2.6299282768390757

Epoch: 6| Step: 3
Training loss: 2.4613239287688895
Validation loss: 2.6317554317657597

Epoch: 6| Step: 4
Training loss: 3.090472402170759
Validation loss: 2.6294930470605538

Epoch: 6| Step: 5
Training loss: 2.3874314862551556
Validation loss: 2.626919861763732

Epoch: 6| Step: 6
Training loss: 2.6198710334136623
Validation loss: 2.6273479406055125

Epoch: 6| Step: 7
Training loss: 2.614432909363144
Validation loss: 2.6262825148910505

Epoch: 6| Step: 8
Training loss: 2.817615011553658
Validation loss: 2.627100664082133

Epoch: 6| Step: 9
Training loss: 2.774518471256764
Validation loss: 2.62120970816776

Epoch: 6| Step: 10
Training loss: 2.7251290212062362
Validation loss: 2.6237819661323596

Epoch: 6| Step: 11
Training loss: 2.659396473386609
Validation loss: 2.627239059523713

Epoch: 6| Step: 12
Training loss: 3.3742857283180046
Validation loss: 2.6422446285533323

Epoch: 6| Step: 13
Training loss: 2.515179991604425
Validation loss: 2.6356779629888325

Epoch: 63| Step: 0
Training loss: 1.949733627906027
Validation loss: 2.6279417690007416

Epoch: 6| Step: 1
Training loss: 2.58969046713032
Validation loss: 2.6158456275588646

Epoch: 6| Step: 2
Training loss: 3.1898637870478788
Validation loss: 2.615421758523915

Epoch: 6| Step: 3
Training loss: 3.1421086237419535
Validation loss: 2.615957132245378

Epoch: 6| Step: 4
Training loss: 2.6989586976754225
Validation loss: 2.61660965191225

Epoch: 6| Step: 5
Training loss: 2.5911236070353794
Validation loss: 2.616546005658097

Epoch: 6| Step: 6
Training loss: 2.761426421884872
Validation loss: 2.6150118766646253

Epoch: 6| Step: 7
Training loss: 2.5272703558097973
Validation loss: 2.6174674724171814

Epoch: 6| Step: 8
Training loss: 2.170252351634572
Validation loss: 2.6142487077503405

Epoch: 6| Step: 9
Training loss: 2.952650566945469
Validation loss: 2.616800049907381

Epoch: 6| Step: 10
Training loss: 2.9938934482937647
Validation loss: 2.614074229388724

Epoch: 6| Step: 11
Training loss: 2.879191121845384
Validation loss: 2.6108804346100083

Epoch: 6| Step: 12
Training loss: 3.1073749186230644
Validation loss: 2.6124299491726295

Epoch: 6| Step: 13
Training loss: 2.626510458027287
Validation loss: 2.611357523967363

Epoch: 64| Step: 0
Training loss: 2.6482569090001244
Validation loss: 2.6074423418896715

Epoch: 6| Step: 1
Training loss: 2.21561791078539
Validation loss: 2.607722735817131

Epoch: 6| Step: 2
Training loss: 2.8106153319440454
Validation loss: 2.6064956219971354

Epoch: 6| Step: 3
Training loss: 3.110431127563574
Validation loss: 2.601511988182876

Epoch: 6| Step: 4
Training loss: 3.1065010322435236
Validation loss: 2.607737729947459

Epoch: 6| Step: 5
Training loss: 2.894815901995378
Validation loss: 2.6044737113504213

Epoch: 6| Step: 6
Training loss: 2.938396560528178
Validation loss: 2.601287002410286

Epoch: 6| Step: 7
Training loss: 2.8671768562472737
Validation loss: 2.6021103382363644

Epoch: 6| Step: 8
Training loss: 3.103298057194492
Validation loss: 2.6013895154840116

Epoch: 6| Step: 9
Training loss: 2.327914030001774
Validation loss: 2.5990299732270077

Epoch: 6| Step: 10
Training loss: 2.508683193136469
Validation loss: 2.5993511053504914

Epoch: 6| Step: 11
Training loss: 2.3638495652538634
Validation loss: 2.6012884535995546

Epoch: 6| Step: 12
Training loss: 2.8125962134964486
Validation loss: 2.5987574640496827

Epoch: 6| Step: 13
Training loss: 2.4130339669003913
Validation loss: 2.5979504411506165

Epoch: 65| Step: 0
Training loss: 2.749024044599178
Validation loss: 2.5956763349135596

Epoch: 6| Step: 1
Training loss: 2.841403873705873
Validation loss: 2.600120809389285

Epoch: 6| Step: 2
Training loss: 2.3272368509943764
Validation loss: 2.5934233095876587

Epoch: 6| Step: 3
Training loss: 2.9757090068289194
Validation loss: 2.5959262067938647

Epoch: 6| Step: 4
Training loss: 3.1711482075077306
Validation loss: 2.5930218268104865

Epoch: 6| Step: 5
Training loss: 2.7154584229962873
Validation loss: 2.5915092226227796

Epoch: 6| Step: 6
Training loss: 2.3730018642850643
Validation loss: 2.589166613127486

Epoch: 6| Step: 7
Training loss: 2.2969758926342205
Validation loss: 2.5939919079340883

Epoch: 6| Step: 8
Training loss: 2.8976516686570686
Validation loss: 2.5928149242159138

Epoch: 6| Step: 9
Training loss: 2.960135399067169
Validation loss: 2.5898719045759027

Epoch: 6| Step: 10
Training loss: 2.630529483539299
Validation loss: 2.587092292533799

Epoch: 6| Step: 11
Training loss: 2.751861029248769
Validation loss: 2.5866719688610256

Epoch: 6| Step: 12
Training loss: 2.9496602719482237
Validation loss: 2.5905598244561547

Epoch: 6| Step: 13
Training loss: 2.37798944355103
Validation loss: 2.5847723449770337

Epoch: 66| Step: 0
Training loss: 2.5228413447490405
Validation loss: 2.591071910219009

Epoch: 6| Step: 1
Training loss: 2.57861175565958
Validation loss: 2.588949303156313

Epoch: 6| Step: 2
Training loss: 2.4576461823023372
Validation loss: 2.586777933700067

Epoch: 6| Step: 3
Training loss: 2.5014520243089677
Validation loss: 2.5859384373473957

Epoch: 6| Step: 4
Training loss: 2.8913222013398037
Validation loss: 2.587333525187403

Epoch: 6| Step: 5
Training loss: 2.846473312094685
Validation loss: 2.587589684988422

Epoch: 6| Step: 6
Training loss: 3.122294208229215
Validation loss: 2.5891922888476895

Epoch: 6| Step: 7
Training loss: 3.1832066961800436
Validation loss: 2.5881557419968644

Epoch: 6| Step: 8
Training loss: 3.1266636806818107
Validation loss: 2.585416190008775

Epoch: 6| Step: 9
Training loss: 2.6791394621405726
Validation loss: 2.5827823533308267

Epoch: 6| Step: 10
Training loss: 2.8434521865004276
Validation loss: 2.5825810234282125

Epoch: 6| Step: 11
Training loss: 2.612954066483421
Validation loss: 2.5777384612722622

Epoch: 6| Step: 12
Training loss: 2.0941382660200527
Validation loss: 2.5766154484101818

Epoch: 6| Step: 13
Training loss: 2.3697891291230975
Validation loss: 2.574860527063703

Epoch: 67| Step: 0
Training loss: 2.8027428816290767
Validation loss: 2.5770384068891694

Epoch: 6| Step: 1
Training loss: 2.4736277041437087
Validation loss: 2.590497532290479

Epoch: 6| Step: 2
Training loss: 2.2168205360796733
Validation loss: 2.5913159006304594

Epoch: 6| Step: 3
Training loss: 2.7184920791530227
Validation loss: 2.5869038784334975

Epoch: 6| Step: 4
Training loss: 2.967671087871939
Validation loss: 2.5929733630326384

Epoch: 6| Step: 5
Training loss: 2.4449261734938847
Validation loss: 2.584504267186161

Epoch: 6| Step: 6
Training loss: 2.577209865102518
Validation loss: 2.5736687573873747

Epoch: 6| Step: 7
Training loss: 2.7242819093647217
Validation loss: 2.576460730781179

Epoch: 6| Step: 8
Training loss: 3.0235991521001204
Validation loss: 2.5703381012450315

Epoch: 6| Step: 9
Training loss: 2.3656908598063153
Validation loss: 2.5716100816647045

Epoch: 6| Step: 10
Training loss: 2.762275866223467
Validation loss: 2.573487412833933

Epoch: 6| Step: 11
Training loss: 2.4566250269693835
Validation loss: 2.5706844968902898

Epoch: 6| Step: 12
Training loss: 3.039449711233785
Validation loss: 2.574350487787682

Epoch: 6| Step: 13
Training loss: 3.152596048802293
Validation loss: 2.573923251258898

Epoch: 68| Step: 0
Training loss: 2.6633845834332712
Validation loss: 2.5709255148396255

Epoch: 6| Step: 1
Training loss: 2.9786103980823766
Validation loss: 2.571316678461137

Epoch: 6| Step: 2
Training loss: 2.3292952291075646
Validation loss: 2.570459116239286

Epoch: 6| Step: 3
Training loss: 2.9775417849589885
Validation loss: 2.566579664807648

Epoch: 6| Step: 4
Training loss: 2.4980743621435346
Validation loss: 2.5644177572150757

Epoch: 6| Step: 5
Training loss: 2.906882268568067
Validation loss: 2.5673927567150234

Epoch: 6| Step: 6
Training loss: 2.830165232495286
Validation loss: 2.5660496139269093

Epoch: 6| Step: 7
Training loss: 2.5715074167576684
Validation loss: 2.5640273310860544

Epoch: 6| Step: 8
Training loss: 2.730577807779364
Validation loss: 2.5642307608297585

Epoch: 6| Step: 9
Training loss: 2.5565815008800503
Validation loss: 2.5636544767129714

Epoch: 6| Step: 10
Training loss: 2.598339295036845
Validation loss: 2.558486214045035

Epoch: 6| Step: 11
Training loss: 2.7919771581660893
Validation loss: 2.5602038415826005

Epoch: 6| Step: 12
Training loss: 2.28129013235247
Validation loss: 2.5759733064257655

Epoch: 6| Step: 13
Training loss: 2.8112394262974107
Validation loss: 2.596966323440296

Epoch: 69| Step: 0
Training loss: 3.261034792155615
Validation loss: 2.6032076990028172

Epoch: 6| Step: 1
Training loss: 2.411406404555422
Validation loss: 2.5624714051178623

Epoch: 6| Step: 2
Training loss: 2.9752583688051395
Validation loss: 2.5621705037233458

Epoch: 6| Step: 3
Training loss: 2.7142947293612267
Validation loss: 2.5638486058147443

Epoch: 6| Step: 4
Training loss: 2.234385670456383
Validation loss: 2.5797899361044223

Epoch: 6| Step: 5
Training loss: 2.822675987953841
Validation loss: 2.5973780738820578

Epoch: 6| Step: 6
Training loss: 2.35077813840174
Validation loss: 2.6174660909230045

Epoch: 6| Step: 7
Training loss: 2.7455082536447235
Validation loss: 2.643385172532444

Epoch: 6| Step: 8
Training loss: 2.535321575587244
Validation loss: 2.63340366265586

Epoch: 6| Step: 9
Training loss: 2.935032234079873
Validation loss: 2.6259763506511784

Epoch: 6| Step: 10
Training loss: 2.4632894267228704
Validation loss: 2.608422278653875

Epoch: 6| Step: 11
Training loss: 2.930899326195038
Validation loss: 2.6015426130938004

Epoch: 6| Step: 12
Training loss: 3.1967993823343925
Validation loss: 2.592607422690387

Epoch: 6| Step: 13
Training loss: 2.739313428997561
Validation loss: 2.583510597874181

Epoch: 70| Step: 0
Training loss: 2.6181722581814095
Validation loss: 2.5808112435837183

Epoch: 6| Step: 1
Training loss: 2.818202701054947
Validation loss: 2.58231557260773

Epoch: 6| Step: 2
Training loss: 2.8828258979299943
Validation loss: 2.57714860710634

Epoch: 6| Step: 3
Training loss: 3.1434001143944124
Validation loss: 2.5700461144274453

Epoch: 6| Step: 4
Training loss: 2.146226223528982
Validation loss: 2.5620946796203428

Epoch: 6| Step: 5
Training loss: 1.9932793710521393
Validation loss: 2.5615668574909614

Epoch: 6| Step: 6
Training loss: 2.9043211740325128
Validation loss: 2.5600046553172064

Epoch: 6| Step: 7
Training loss: 3.211081545677899
Validation loss: 2.5555999246376064

Epoch: 6| Step: 8
Training loss: 2.8090448985437937
Validation loss: 2.5528863279360947

Epoch: 6| Step: 9
Training loss: 2.356139130349161
Validation loss: 2.5566609078018736

Epoch: 6| Step: 10
Training loss: 3.005429758325182
Validation loss: 2.5516567159101964

Epoch: 6| Step: 11
Training loss: 2.4576756734527336
Validation loss: 2.5501460501041056

Epoch: 6| Step: 12
Training loss: 2.32334809224254
Validation loss: 2.552204212258404

Epoch: 6| Step: 13
Training loss: 2.845600574035844
Validation loss: 2.549441255801282

Epoch: 71| Step: 0
Training loss: 2.8342195882532595
Validation loss: 2.5489155526592513

Epoch: 6| Step: 1
Training loss: 3.127725709474617
Validation loss: 2.550520707864559

Epoch: 6| Step: 2
Training loss: 2.5377701037740543
Validation loss: 2.548536573862462

Epoch: 6| Step: 3
Training loss: 2.693600141069699
Validation loss: 2.551017247918176

Epoch: 6| Step: 4
Training loss: 2.7581703018765995
Validation loss: 2.553690148895627

Epoch: 6| Step: 5
Training loss: 2.2474692204863875
Validation loss: 2.5462709658419236

Epoch: 6| Step: 6
Training loss: 3.2489466794265547
Validation loss: 2.547301477091126

Epoch: 6| Step: 7
Training loss: 2.8032380930733516
Validation loss: 2.5512767269635366

Epoch: 6| Step: 8
Training loss: 1.8641360579347945
Validation loss: 2.5454654810771697

Epoch: 6| Step: 9
Training loss: 2.7930095536245814
Validation loss: 2.548363903736335

Epoch: 6| Step: 10
Training loss: 2.769097943971886
Validation loss: 2.548286920336474

Epoch: 6| Step: 11
Training loss: 2.415417929554648
Validation loss: 2.5455805915788368

Epoch: 6| Step: 12
Training loss: 2.036931469940121
Validation loss: 2.544108864279327

Epoch: 6| Step: 13
Training loss: 2.970025039721427
Validation loss: 2.5482327639201663

Epoch: 72| Step: 0
Training loss: 2.262623028018354
Validation loss: 2.5458151517511203

Epoch: 6| Step: 1
Training loss: 2.678516009529468
Validation loss: 2.5508628469972026

Epoch: 6| Step: 2
Training loss: 2.560746127711769
Validation loss: 2.550599166189659

Epoch: 6| Step: 3
Training loss: 2.045640063301754
Validation loss: 2.5513585105845205

Epoch: 6| Step: 4
Training loss: 2.7823442171123935
Validation loss: 2.5613643641931243

Epoch: 6| Step: 5
Training loss: 2.9764221638915873
Validation loss: 2.560851660192297

Epoch: 6| Step: 6
Training loss: 2.1520450890224274
Validation loss: 2.5426964679979287

Epoch: 6| Step: 7
Training loss: 2.708083263492025
Validation loss: 2.542569779107739

Epoch: 6| Step: 8
Training loss: 3.0444861804424854
Validation loss: 2.5421931602197745

Epoch: 6| Step: 9
Training loss: 2.4441473869485035
Validation loss: 2.538203034311752

Epoch: 6| Step: 10
Training loss: 2.857096780677924
Validation loss: 2.540923687471145

Epoch: 6| Step: 11
Training loss: 3.158672649370901
Validation loss: 2.5423521201539647

Epoch: 6| Step: 12
Training loss: 2.7541724543046566
Validation loss: 2.54023850032122

Epoch: 6| Step: 13
Training loss: 2.7258009407204735
Validation loss: 2.5445264663784064

Epoch: 73| Step: 0
Training loss: 3.0740409906318686
Validation loss: 2.5477351197020397

Epoch: 6| Step: 1
Training loss: 2.5090629336087944
Validation loss: 2.5496114840020754

Epoch: 6| Step: 2
Training loss: 2.388404063872057
Validation loss: 2.54206747011234

Epoch: 6| Step: 3
Training loss: 2.8278505813912886
Validation loss: 2.542343172083419

Epoch: 6| Step: 4
Training loss: 2.6828958861645256
Validation loss: 2.5399794737169197

Epoch: 6| Step: 5
Training loss: 2.6180799186870374
Validation loss: 2.536884833372659

Epoch: 6| Step: 6
Training loss: 2.469573064810297
Validation loss: 2.5365620042945247

Epoch: 6| Step: 7
Training loss: 2.8451968693791474
Validation loss: 2.534442442573999

Epoch: 6| Step: 8
Training loss: 2.5875736834221112
Validation loss: 2.532024696700214

Epoch: 6| Step: 9
Training loss: 2.044130305098878
Validation loss: 2.5286105644891044

Epoch: 6| Step: 10
Training loss: 2.606229062076323
Validation loss: 2.530040169272602

Epoch: 6| Step: 11
Training loss: 3.0614659646220246
Validation loss: 2.5319383258319004

Epoch: 6| Step: 12
Training loss: 2.5187700408062534
Validation loss: 2.524538443653633

Epoch: 6| Step: 13
Training loss: 3.0051585986133618
Validation loss: 2.532107408149753

Epoch: 74| Step: 0
Training loss: 2.88552224600715
Validation loss: 2.5296332136914543

Epoch: 6| Step: 1
Training loss: 2.8109095845047634
Validation loss: 2.5247345888751744

Epoch: 6| Step: 2
Training loss: 2.8021744118864813
Validation loss: 2.5267533286270796

Epoch: 6| Step: 3
Training loss: 2.8613637876403843
Validation loss: 2.533483830381983

Epoch: 6| Step: 4
Training loss: 2.3470415526109925
Validation loss: 2.529242170569534

Epoch: 6| Step: 5
Training loss: 3.1532288245960953
Validation loss: 2.52906144272577

Epoch: 6| Step: 6
Training loss: 2.2289189233697053
Validation loss: 2.5240077898819178

Epoch: 6| Step: 7
Training loss: 2.439873933193166
Validation loss: 2.5321846166653383

Epoch: 6| Step: 8
Training loss: 2.5984012309907714
Validation loss: 2.531352225543172

Epoch: 6| Step: 9
Training loss: 2.494703786386577
Validation loss: 2.533590090648335

Epoch: 6| Step: 10
Training loss: 2.7316581089459055
Validation loss: 2.5325442172649906

Epoch: 6| Step: 11
Training loss: 2.510785488614355
Validation loss: 2.527050034654015

Epoch: 6| Step: 12
Training loss: 2.7280390110862234
Validation loss: 2.528574294634373

Epoch: 6| Step: 13
Training loss: 2.421856394050254
Validation loss: 2.526783365642978

Epoch: 75| Step: 0
Training loss: 2.8475422149556606
Validation loss: 2.5254144473229494

Epoch: 6| Step: 1
Training loss: 2.5842474171375653
Validation loss: 2.526984950386242

Epoch: 6| Step: 2
Training loss: 2.7443312392093793
Validation loss: 2.5258483393252846

Epoch: 6| Step: 3
Training loss: 2.404383244888294
Validation loss: 2.529307181043997

Epoch: 6| Step: 4
Training loss: 2.3942151008732
Validation loss: 2.5319474283790084

Epoch: 6| Step: 5
Training loss: 2.9460631205833705
Validation loss: 2.5331524108814207

Epoch: 6| Step: 6
Training loss: 2.789547234307105
Validation loss: 2.5333107616857884

Epoch: 6| Step: 7
Training loss: 2.3445248149616997
Validation loss: 2.533787370031986

Epoch: 6| Step: 8
Training loss: 2.719341959032558
Validation loss: 2.530288874879537

Epoch: 6| Step: 9
Training loss: 2.976689373526375
Validation loss: 2.528559962555302

Epoch: 6| Step: 10
Training loss: 2.85206612098094
Validation loss: 2.5260042849786517

Epoch: 6| Step: 11
Training loss: 2.802251241160436
Validation loss: 2.5241824409677918

Epoch: 6| Step: 12
Training loss: 2.306008584224078
Validation loss: 2.523540860825781

Epoch: 6| Step: 13
Training loss: 2.421267820971146
Validation loss: 2.5206202792675136

Epoch: 76| Step: 0
Training loss: 2.38333794475545
Validation loss: 2.5183915904754564

Epoch: 6| Step: 1
Training loss: 2.445747702386832
Validation loss: 2.5160849646315997

Epoch: 6| Step: 2
Training loss: 2.5760121328530183
Validation loss: 2.5154637350199103

Epoch: 6| Step: 3
Training loss: 2.994563421867276
Validation loss: 2.514515815867173

Epoch: 6| Step: 4
Training loss: 2.708315502010236
Validation loss: 2.519581323564713

Epoch: 6| Step: 5
Training loss: 2.7480272673318784
Validation loss: 2.511229819986832

Epoch: 6| Step: 6
Training loss: 2.5827955998896477
Validation loss: 2.514558095944948

Epoch: 6| Step: 7
Training loss: 2.662122719410107
Validation loss: 2.515945935361341

Epoch: 6| Step: 8
Training loss: 2.378947941958141
Validation loss: 2.5224311955837893

Epoch: 6| Step: 9
Training loss: 2.7719975561596333
Validation loss: 2.5177146651651743

Epoch: 6| Step: 10
Training loss: 2.6584920957346285
Validation loss: 2.5254015606385023

Epoch: 6| Step: 11
Training loss: 2.5913039473913733
Validation loss: 2.523462742225522

Epoch: 6| Step: 12
Training loss: 2.483319328149137
Validation loss: 2.521436671626481

Epoch: 6| Step: 13
Training loss: 2.994756566823295
Validation loss: 2.520638408428169

Epoch: 77| Step: 0
Training loss: 2.7783887053587706
Validation loss: 2.5140147455744524

Epoch: 6| Step: 1
Training loss: 2.4809965758540287
Validation loss: 2.5144541446228788

Epoch: 6| Step: 2
Training loss: 2.7777053876557125
Validation loss: 2.517838390896327

Epoch: 6| Step: 3
Training loss: 2.7280194344038065
Validation loss: 2.5206747215330294

Epoch: 6| Step: 4
Training loss: 2.7900912740947277
Validation loss: 2.524354089138902

Epoch: 6| Step: 5
Training loss: 2.9174101472304312
Validation loss: 2.5252948928708254

Epoch: 6| Step: 6
Training loss: 2.292683416286351
Validation loss: 2.5272876196734972

Epoch: 6| Step: 7
Training loss: 2.854588572558067
Validation loss: 2.531749173775186

Epoch: 6| Step: 8
Training loss: 2.443200707715425
Validation loss: 2.5325862199040254

Epoch: 6| Step: 9
Training loss: 2.107033567388418
Validation loss: 2.533568540956058

Epoch: 6| Step: 10
Training loss: 2.7379682201438755
Validation loss: 2.5354642127496727

Epoch: 6| Step: 11
Training loss: 2.8221919594624825
Validation loss: 2.5342639044241326

Epoch: 6| Step: 12
Training loss: 1.9903076877137518
Validation loss: 2.5349873083375014

Epoch: 6| Step: 13
Training loss: 3.2110834761431035
Validation loss: 2.5329144346923167

Epoch: 78| Step: 0
Training loss: 2.8627579485348416
Validation loss: 2.528864548898823

Epoch: 6| Step: 1
Training loss: 3.2353988722299385
Validation loss: 2.5264292678509523

Epoch: 6| Step: 2
Training loss: 2.6958347864947956
Validation loss: 2.5268928719177195

Epoch: 6| Step: 3
Training loss: 2.4265698611509503
Validation loss: 2.522758660276117

Epoch: 6| Step: 4
Training loss: 2.5402868517400816
Validation loss: 2.522447594639225

Epoch: 6| Step: 5
Training loss: 2.4275962931492967
Validation loss: 2.522295241575627

Epoch: 6| Step: 6
Training loss: 2.6316465971432903
Validation loss: 2.5167288086231636

Epoch: 6| Step: 7
Training loss: 2.541019941087322
Validation loss: 2.5167039093731347

Epoch: 6| Step: 8
Training loss: 2.804841443780692
Validation loss: 2.5129775970903494

Epoch: 6| Step: 9
Training loss: 2.1902970007251295
Validation loss: 2.5135505605498913

Epoch: 6| Step: 10
Training loss: 2.5216607136308387
Validation loss: 2.512571578643119

Epoch: 6| Step: 11
Training loss: 2.805494272131368
Validation loss: 2.5095402360340437

Epoch: 6| Step: 12
Training loss: 2.325375463050846
Validation loss: 2.5062451401407335

Epoch: 6| Step: 13
Training loss: 2.766468242822191
Validation loss: 2.507184624430769

Epoch: 79| Step: 0
Training loss: 1.7934765730285744
Validation loss: 2.510453589287901

Epoch: 6| Step: 1
Training loss: 2.7340896021539236
Validation loss: 2.518367867396052

Epoch: 6| Step: 2
Training loss: 2.605133355327851
Validation loss: 2.544001083104385

Epoch: 6| Step: 3
Training loss: 2.359794933008903
Validation loss: 2.5193244868957074

Epoch: 6| Step: 4
Training loss: 2.747012856835606
Validation loss: 2.5114133183418943

Epoch: 6| Step: 5
Training loss: 2.4391532573151076
Validation loss: 2.5123376316220885

Epoch: 6| Step: 6
Training loss: 2.8603557301416376
Validation loss: 2.506398341988426

Epoch: 6| Step: 7
Training loss: 2.975555329623921
Validation loss: 2.5099414254252537

Epoch: 6| Step: 8
Training loss: 3.0135115891735835
Validation loss: 2.50441561798066

Epoch: 6| Step: 9
Training loss: 2.508103968714874
Validation loss: 2.5028452814966906

Epoch: 6| Step: 10
Training loss: 2.468436402309631
Validation loss: 2.5062351832027163

Epoch: 6| Step: 11
Training loss: 3.1460689599590586
Validation loss: 2.5053758677549443

Epoch: 6| Step: 12
Training loss: 2.5522997602181294
Validation loss: 2.511227936990347

Epoch: 6| Step: 13
Training loss: 2.4943902974339474
Validation loss: 2.508447948849244

Epoch: 80| Step: 0
Training loss: 2.823458705161706
Validation loss: 2.512231073489993

Epoch: 6| Step: 1
Training loss: 2.870758909046036
Validation loss: 2.5113921163197603

Epoch: 6| Step: 2
Training loss: 2.3751901249326623
Validation loss: 2.5147669653750833

Epoch: 6| Step: 3
Training loss: 2.593174330471311
Validation loss: 2.510803498872575

Epoch: 6| Step: 4
Training loss: 3.119336451126218
Validation loss: 2.511109748444239

Epoch: 6| Step: 5
Training loss: 2.874492102831936
Validation loss: 2.5124436316801613

Epoch: 6| Step: 6
Training loss: 2.4293724910722148
Validation loss: 2.51241049725739

Epoch: 6| Step: 7
Training loss: 2.856174366156187
Validation loss: 2.511093417764556

Epoch: 6| Step: 8
Training loss: 2.8007390273142363
Validation loss: 2.511378572247749

Epoch: 6| Step: 9
Training loss: 2.3168366468170922
Validation loss: 2.507254866322179

Epoch: 6| Step: 10
Training loss: 2.723164624514021
Validation loss: 2.5048880474786914

Epoch: 6| Step: 11
Training loss: 2.4174535939004307
Validation loss: 2.504803651943203

Epoch: 6| Step: 12
Training loss: 2.0341370239765095
Validation loss: 2.5017881832899596

Epoch: 6| Step: 13
Training loss: 2.421624152205684
Validation loss: 2.4992585990187743

Epoch: 81| Step: 0
Training loss: 2.96100771375358
Validation loss: 2.49838839083676

Epoch: 6| Step: 1
Training loss: 2.366405798633995
Validation loss: 2.4987337561431295

Epoch: 6| Step: 2
Training loss: 3.0186093441751782
Validation loss: 2.500067884795087

Epoch: 6| Step: 3
Training loss: 2.6604202744457743
Validation loss: 2.5057756623546794

Epoch: 6| Step: 4
Training loss: 2.725239167523217
Validation loss: 2.505862276903644

Epoch: 6| Step: 5
Training loss: 2.531035331759803
Validation loss: 2.5028449163371023

Epoch: 6| Step: 6
Training loss: 2.625785710048094
Validation loss: 2.4974134893393223

Epoch: 6| Step: 7
Training loss: 1.9953323972661383
Validation loss: 2.503692721001965

Epoch: 6| Step: 8
Training loss: 2.690755779353365
Validation loss: 2.5029005549304784

Epoch: 6| Step: 9
Training loss: 2.703371069848974
Validation loss: 2.498022966665666

Epoch: 6| Step: 10
Training loss: 2.730186873527137
Validation loss: 2.5036699061502503

Epoch: 6| Step: 11
Training loss: 2.325513565604603
Validation loss: 2.499021895123555

Epoch: 6| Step: 12
Training loss: 2.5610194697595143
Validation loss: 2.4997810983189375

Epoch: 6| Step: 13
Training loss: 2.628952411399375
Validation loss: 2.493174135392238

Epoch: 82| Step: 0
Training loss: 1.9937293574780282
Validation loss: 2.5012795829389773

Epoch: 6| Step: 1
Training loss: 2.0745706780512396
Validation loss: 2.4982377198873515

Epoch: 6| Step: 2
Training loss: 2.9435490760347283
Validation loss: 2.499624478429197

Epoch: 6| Step: 3
Training loss: 2.911348198642724
Validation loss: 2.498891489317718

Epoch: 6| Step: 4
Training loss: 2.416438344057196
Validation loss: 2.4989850050115185

Epoch: 6| Step: 5
Training loss: 3.1161108619546414
Validation loss: 2.497784029822521

Epoch: 6| Step: 6
Training loss: 2.947708571854579
Validation loss: 2.4973977850998197

Epoch: 6| Step: 7
Training loss: 2.2395377102721894
Validation loss: 2.4960131167032515

Epoch: 6| Step: 8
Training loss: 2.408180948296105
Validation loss: 2.497986061801513

Epoch: 6| Step: 9
Training loss: 2.5229601334708023
Validation loss: 2.497365739871124

Epoch: 6| Step: 10
Training loss: 2.5419270012111057
Validation loss: 2.492924341156708

Epoch: 6| Step: 11
Training loss: 2.861857019698274
Validation loss: 2.491592096937368

Epoch: 6| Step: 12
Training loss: 2.691216405365617
Validation loss: 2.4894903210831494

Epoch: 6| Step: 13
Training loss: 2.608849935184844
Validation loss: 2.4895728415636555

Epoch: 83| Step: 0
Training loss: 1.9570112094119538
Validation loss: 2.4862412135098566

Epoch: 6| Step: 1
Training loss: 2.5875816995742253
Validation loss: 2.487146171568033

Epoch: 6| Step: 2
Training loss: 2.405179665054444
Validation loss: 2.4964404514575027

Epoch: 6| Step: 3
Training loss: 2.7169674697068626
Validation loss: 2.4924000138226603

Epoch: 6| Step: 4
Training loss: 2.163807988263007
Validation loss: 2.495441747609322

Epoch: 6| Step: 5
Training loss: 3.1617779054379245
Validation loss: 2.49520826478323

Epoch: 6| Step: 6
Training loss: 2.8995296754589677
Validation loss: 2.498724723434905

Epoch: 6| Step: 7
Training loss: 2.695200688351107
Validation loss: 2.4924522426701734

Epoch: 6| Step: 8
Training loss: 2.770808131359092
Validation loss: 2.4921511305623647

Epoch: 6| Step: 9
Training loss: 3.32172534278945
Validation loss: 2.4882650968705033

Epoch: 6| Step: 10
Training loss: 2.526536862839549
Validation loss: 2.4849725160671556

Epoch: 6| Step: 11
Training loss: 1.8686340985031673
Validation loss: 2.484602511882784

Epoch: 6| Step: 12
Training loss: 2.809590678693129
Validation loss: 2.4895312305143102

Epoch: 6| Step: 13
Training loss: 2.2444286244635356
Validation loss: 2.489787461811897

Epoch: 84| Step: 0
Training loss: 2.639656747119736
Validation loss: 2.48897120932366

Epoch: 6| Step: 1
Training loss: 2.025815060322726
Validation loss: 2.4917362167279795

Epoch: 6| Step: 2
Training loss: 2.405997027686258
Validation loss: 2.486345353507588

Epoch: 6| Step: 3
Training loss: 2.4781925841612664
Validation loss: 2.493195237332022

Epoch: 6| Step: 4
Training loss: 2.5056378689609344
Validation loss: 2.487700661596111

Epoch: 6| Step: 5
Training loss: 2.6195305655615346
Validation loss: 2.486091180954744

Epoch: 6| Step: 6
Training loss: 2.9713877955458043
Validation loss: 2.4897810778965783

Epoch: 6| Step: 7
Training loss: 2.7376301599503314
Validation loss: 2.4915727197888917

Epoch: 6| Step: 8
Training loss: 2.4711071302285057
Validation loss: 2.4870907636972324

Epoch: 6| Step: 9
Training loss: 2.309317589899247
Validation loss: 2.4847949970558125

Epoch: 6| Step: 10
Training loss: 2.7483681258510644
Validation loss: 2.4914075693338527

Epoch: 6| Step: 11
Training loss: 2.4142619917825447
Validation loss: 2.4861507507327585

Epoch: 6| Step: 12
Training loss: 2.9969816918817855
Validation loss: 2.483664541004592

Epoch: 6| Step: 13
Training loss: 2.9573105311063594
Validation loss: 2.482110500941584

Epoch: 85| Step: 0
Training loss: 2.760173791721121
Validation loss: 2.491794917995946

Epoch: 6| Step: 1
Training loss: 2.7667632266126843
Validation loss: 2.4870356821292825

Epoch: 6| Step: 2
Training loss: 2.506250768647112
Validation loss: 2.493070105318655

Epoch: 6| Step: 3
Training loss: 2.769466684554611
Validation loss: 2.4948559290202277

Epoch: 6| Step: 4
Training loss: 2.1171412163774592
Validation loss: 2.4933767162040494

Epoch: 6| Step: 5
Training loss: 2.673240179006926
Validation loss: 2.495873287774828

Epoch: 6| Step: 6
Training loss: 2.405463944889038
Validation loss: 2.495907867626367

Epoch: 6| Step: 7
Training loss: 2.668596453571229
Validation loss: 2.4947007759330657

Epoch: 6| Step: 8
Training loss: 2.9679092019682694
Validation loss: 2.4945600291940715

Epoch: 6| Step: 9
Training loss: 2.4765906106695237
Validation loss: 2.4955687749388367

Epoch: 6| Step: 10
Training loss: 2.351356066390658
Validation loss: 2.4924559094895358

Epoch: 6| Step: 11
Training loss: 2.6792339686300055
Validation loss: 2.490108312270775

Epoch: 6| Step: 12
Training loss: 2.550983134717531
Validation loss: 2.4888259395328687

Epoch: 6| Step: 13
Training loss: 2.7923858247671203
Validation loss: 2.4904242708693105

Epoch: 86| Step: 0
Training loss: 2.871779254898814
Validation loss: 2.4815226240949038

Epoch: 6| Step: 1
Training loss: 2.6057154411418524
Validation loss: 2.4860246247841475

Epoch: 6| Step: 2
Training loss: 2.4274261843677554
Validation loss: 2.4852594844446676

Epoch: 6| Step: 3
Training loss: 2.0832921087636778
Validation loss: 2.4863374904187485

Epoch: 6| Step: 4
Training loss: 2.7926556178269233
Validation loss: 2.485502255211268

Epoch: 6| Step: 5
Training loss: 2.510032360582603
Validation loss: 2.49009331197686

Epoch: 6| Step: 6
Training loss: 2.621076922071307
Validation loss: 2.495545089668973

Epoch: 6| Step: 7
Training loss: 2.8817851399178545
Validation loss: 2.4846732482386282

Epoch: 6| Step: 8
Training loss: 1.8514815687044577
Validation loss: 2.4856470877887626

Epoch: 6| Step: 9
Training loss: 2.6898338475018764
Validation loss: 2.4826010042395907

Epoch: 6| Step: 10
Training loss: 2.7174653273741978
Validation loss: 2.483294989996049

Epoch: 6| Step: 11
Training loss: 3.256188003623471
Validation loss: 2.485379445857012

Epoch: 6| Step: 12
Training loss: 2.712730144810208
Validation loss: 2.4810690646498372

Epoch: 6| Step: 13
Training loss: 2.160930584436523
Validation loss: 2.4848596352093018

Epoch: 87| Step: 0
Training loss: 2.386064655185774
Validation loss: 2.485942362155791

Epoch: 6| Step: 1
Training loss: 2.7437807329059867
Validation loss: 2.4837688290337145

Epoch: 6| Step: 2
Training loss: 2.657519407443705
Validation loss: 2.4872884481494277

Epoch: 6| Step: 3
Training loss: 2.403978638152613
Validation loss: 2.4891447588077087

Epoch: 6| Step: 4
Training loss: 2.4241632295988267
Validation loss: 2.4936356119102854

Epoch: 6| Step: 5
Training loss: 2.4779661995997126
Validation loss: 2.495094668491417

Epoch: 6| Step: 6
Training loss: 2.1444569837151977
Validation loss: 2.498187974052241

Epoch: 6| Step: 7
Training loss: 3.202600054640815
Validation loss: 2.4966494041766234

Epoch: 6| Step: 8
Training loss: 2.830299510944959
Validation loss: 2.4950208830739875

Epoch: 6| Step: 9
Training loss: 2.5641109706056167
Validation loss: 2.493579009654502

Epoch: 6| Step: 10
Training loss: 2.6560322391782445
Validation loss: 2.4934244942532087

Epoch: 6| Step: 11
Training loss: 2.8442744515538085
Validation loss: 2.489665446397029

Epoch: 6| Step: 12
Training loss: 2.5555259891771525
Validation loss: 2.4892972252390604

Epoch: 6| Step: 13
Training loss: 2.511784245658994
Validation loss: 2.492427714698448

Epoch: 88| Step: 0
Training loss: 2.6860858834085843
Validation loss: 2.4832121008738315

Epoch: 6| Step: 1
Training loss: 2.568584104503958
Validation loss: 2.4749128621749676

Epoch: 6| Step: 2
Training loss: 2.9313709356091033
Validation loss: 2.4758386847832914

Epoch: 6| Step: 3
Training loss: 2.3388960356715156
Validation loss: 2.472218873049102

Epoch: 6| Step: 4
Training loss: 2.5678953730698786
Validation loss: 2.4765026995862827

Epoch: 6| Step: 5
Training loss: 2.2865749552419974
Validation loss: 2.4807134873009957

Epoch: 6| Step: 6
Training loss: 2.7822686376633805
Validation loss: 2.4778333546041558

Epoch: 6| Step: 7
Training loss: 2.3715013533895397
Validation loss: 2.483159261614828

Epoch: 6| Step: 8
Training loss: 2.857530009061243
Validation loss: 2.4865055345567906

Epoch: 6| Step: 9
Training loss: 3.16985267850384
Validation loss: 2.476636723028879

Epoch: 6| Step: 10
Training loss: 2.5131294714549446
Validation loss: 2.4776697822261076

Epoch: 6| Step: 11
Training loss: 2.1571455491733986
Validation loss: 2.4817473712998295

Epoch: 6| Step: 12
Training loss: 2.6821065499335655
Validation loss: 2.4848917537653854

Epoch: 6| Step: 13
Training loss: 2.1912904868599083
Validation loss: 2.479282333710426

Epoch: 89| Step: 0
Training loss: 1.9505946133100653
Validation loss: 2.482989102599184

Epoch: 6| Step: 1
Training loss: 2.7104845355658282
Validation loss: 2.4812936287350476

Epoch: 6| Step: 2
Training loss: 3.102154961026557
Validation loss: 2.4767037400525305

Epoch: 6| Step: 3
Training loss: 2.4605544019148846
Validation loss: 2.481428986763022

Epoch: 6| Step: 4
Training loss: 2.5351140234770564
Validation loss: 2.478016263188486

Epoch: 6| Step: 5
Training loss: 2.4768338701566615
Validation loss: 2.47749136303815

Epoch: 6| Step: 6
Training loss: 3.1086651935981577
Validation loss: 2.4813109402415643

Epoch: 6| Step: 7
Training loss: 2.399335363865055
Validation loss: 2.480875954031763

Epoch: 6| Step: 8
Training loss: 2.7014959641531835
Validation loss: 2.4767600701724426

Epoch: 6| Step: 9
Training loss: 2.268478385702929
Validation loss: 2.4833814285621894

Epoch: 6| Step: 10
Training loss: 2.8153826034477776
Validation loss: 2.4803039175619395

Epoch: 6| Step: 11
Training loss: 2.404155860316839
Validation loss: 2.471712773108025

Epoch: 6| Step: 12
Training loss: 2.211694702319538
Validation loss: 2.4731907399948514

Epoch: 6| Step: 13
Training loss: 2.823754236793126
Validation loss: 2.478755681782756

Epoch: 90| Step: 0
Training loss: 3.034409282581021
Validation loss: 2.4810707222897532

Epoch: 6| Step: 1
Training loss: 2.628178080454787
Validation loss: 2.479661865358268

Epoch: 6| Step: 2
Training loss: 2.547875798652216
Validation loss: 2.492333227538788

Epoch: 6| Step: 3
Training loss: 2.5258090092330905
Validation loss: 2.492295018729853

Epoch: 6| Step: 4
Training loss: 2.470830306447466
Validation loss: 2.48190278979712

Epoch: 6| Step: 5
Training loss: 2.571941381364911
Validation loss: 2.4675659527221

Epoch: 6| Step: 6
Training loss: 2.5062547640862216
Validation loss: 2.4725165787190195

Epoch: 6| Step: 7
Training loss: 2.3050882282045366
Validation loss: 2.476553948003562

Epoch: 6| Step: 8
Training loss: 3.1770072719453526
Validation loss: 2.4669829535254246

Epoch: 6| Step: 9
Training loss: 2.5190947871711638
Validation loss: 2.473266204923824

Epoch: 6| Step: 10
Training loss: 2.0244074672770043
Validation loss: 2.4746328832989537

Epoch: 6| Step: 11
Training loss: 2.202304119011992
Validation loss: 2.4766523663795517

Epoch: 6| Step: 12
Training loss: 2.7687480986784525
Validation loss: 2.481572343721663

Epoch: 6| Step: 13
Training loss: 2.8046971908019382
Validation loss: 2.482049713530371

Epoch: 91| Step: 0
Training loss: 2.060561916784553
Validation loss: 2.4814224452286577

Epoch: 6| Step: 1
Training loss: 2.5727201276043354
Validation loss: 2.483769868932182

Epoch: 6| Step: 2
Training loss: 2.5268729713312914
Validation loss: 2.4796026444894395

Epoch: 6| Step: 3
Training loss: 2.4597298701769517
Validation loss: 2.4841773796255775

Epoch: 6| Step: 4
Training loss: 2.7899706134137254
Validation loss: 2.4820293813539713

Epoch: 6| Step: 5
Training loss: 2.9186293583183516
Validation loss: 2.484429436813006

Epoch: 6| Step: 6
Training loss: 2.5933046073434
Validation loss: 2.476898940666367

Epoch: 6| Step: 7
Training loss: 2.6465612196235115
Validation loss: 2.478487215513074

Epoch: 6| Step: 8
Training loss: 3.0276069379043866
Validation loss: 2.480420946676837

Epoch: 6| Step: 9
Training loss: 2.761338700147295
Validation loss: 2.4753013472339327

Epoch: 6| Step: 10
Training loss: 1.9721940207218696
Validation loss: 2.469331986601334

Epoch: 6| Step: 11
Training loss: 2.2701694556353726
Validation loss: 2.4708613770958943

Epoch: 6| Step: 12
Training loss: 2.926124135031404
Validation loss: 2.479467138679967

Epoch: 6| Step: 13
Training loss: 2.464947149704468
Validation loss: 2.481850763229959

Epoch: 92| Step: 0
Training loss: 2.5228039208905035
Validation loss: 2.5008825492740496

Epoch: 6| Step: 1
Training loss: 2.3385799096144533
Validation loss: 2.4954706328624594

Epoch: 6| Step: 2
Training loss: 2.5991630600976294
Validation loss: 2.4935776949753543

Epoch: 6| Step: 3
Training loss: 2.113569483253572
Validation loss: 2.4803464764272145

Epoch: 6| Step: 4
Training loss: 2.953914799106579
Validation loss: 2.4877924895730645

Epoch: 6| Step: 5
Training loss: 2.3086014955652105
Validation loss: 2.472569870406734

Epoch: 6| Step: 6
Training loss: 2.3833876619171206
Validation loss: 2.4835330648129013

Epoch: 6| Step: 7
Training loss: 2.141123915399048
Validation loss: 2.4766663731112333

Epoch: 6| Step: 8
Training loss: 2.478759753601922
Validation loss: 2.4692106018466244

Epoch: 6| Step: 9
Training loss: 2.945546269941985
Validation loss: 2.4713934897592833

Epoch: 6| Step: 10
Training loss: 2.9567125922259243
Validation loss: 2.479065297295618

Epoch: 6| Step: 11
Training loss: 3.5220990736726336
Validation loss: 2.4823841539303766

Epoch: 6| Step: 12
Training loss: 2.501183420464724
Validation loss: 2.4845550040046405

Epoch: 6| Step: 13
Training loss: 2.1069703135379307
Validation loss: 2.490065449488999

Epoch: 93| Step: 0
Training loss: 2.2385374125680904
Validation loss: 2.4928567798844297

Epoch: 6| Step: 1
Training loss: 2.5389125368875156
Validation loss: 2.4929119878698764

Epoch: 6| Step: 2
Training loss: 2.5245174777838213
Validation loss: 2.5018412803012215

Epoch: 6| Step: 3
Training loss: 2.9790341185491616
Validation loss: 2.502424479109417

Epoch: 6| Step: 4
Training loss: 2.9934140074576963
Validation loss: 2.505545680014167

Epoch: 6| Step: 5
Training loss: 2.1432108609857354
Validation loss: 2.5042754807106533

Epoch: 6| Step: 6
Training loss: 2.7321791196779897
Validation loss: 2.50337360368314

Epoch: 6| Step: 7
Training loss: 1.8599980696801455
Validation loss: 2.4995150095669634

Epoch: 6| Step: 8
Training loss: 2.9428930894135066
Validation loss: 2.4998030346527345

Epoch: 6| Step: 9
Training loss: 3.0542172901768523
Validation loss: 2.494658007986815

Epoch: 6| Step: 10
Training loss: 2.473898046997094
Validation loss: 2.4949038857935872

Epoch: 6| Step: 11
Training loss: 2.90210748586214
Validation loss: 2.490472057693027

Epoch: 6| Step: 12
Training loss: 2.246835602692129
Validation loss: 2.490430413801181

Epoch: 6| Step: 13
Training loss: 2.7687396598261174
Validation loss: 2.488513466595535

Epoch: 94| Step: 0
Training loss: 2.8625274123832436
Validation loss: 2.4856461845596978

Epoch: 6| Step: 1
Training loss: 2.4466223575346038
Validation loss: 2.4845446962504867

Epoch: 6| Step: 2
Training loss: 2.7415733410412098
Validation loss: 2.486388488180619

Epoch: 6| Step: 3
Training loss: 2.0776052183459544
Validation loss: 2.484553156768678

Epoch: 6| Step: 4
Training loss: 2.596895355989478
Validation loss: 2.47596860357088

Epoch: 6| Step: 5
Training loss: 3.4771217839282755
Validation loss: 2.4834220066069683

Epoch: 6| Step: 6
Training loss: 2.397104097680104
Validation loss: 2.4785483147459666

Epoch: 6| Step: 7
Training loss: 2.6427956411945557
Validation loss: 2.476512021935212

Epoch: 6| Step: 8
Training loss: 2.650047420581252
Validation loss: 2.475116199116154

Epoch: 6| Step: 9
Training loss: 2.0832744844390163
Validation loss: 2.4749721872327544

Epoch: 6| Step: 10
Training loss: 2.5921889973659686
Validation loss: 2.4744748402882015

Epoch: 6| Step: 11
Training loss: 2.3360599753636664
Validation loss: 2.4741264580602658

Epoch: 6| Step: 12
Training loss: 2.7381056268072395
Validation loss: 2.4815655063312625

Epoch: 6| Step: 13
Training loss: 2.278405911848349
Validation loss: 2.4743058827199214

Epoch: 95| Step: 0
Training loss: 3.081321953790314
Validation loss: 2.4618602011898307

Epoch: 6| Step: 1
Training loss: 2.7111280742576653
Validation loss: 2.4698146021526286

Epoch: 6| Step: 2
Training loss: 2.641163607410726
Validation loss: 2.4700305048285394

Epoch: 6| Step: 3
Training loss: 2.6172940816101176
Validation loss: 2.4655445390344104

Epoch: 6| Step: 4
Training loss: 2.439745138021793
Validation loss: 2.4694712748129017

Epoch: 6| Step: 5
Training loss: 2.7105373120170153
Validation loss: 2.4712465434998117

Epoch: 6| Step: 6
Training loss: 2.1713751348036805
Validation loss: 2.4783890944644638

Epoch: 6| Step: 7
Training loss: 2.9079063371689036
Validation loss: 2.4817592517840064

Epoch: 6| Step: 8
Training loss: 2.5177036961386645
Validation loss: 2.480545787666267

Epoch: 6| Step: 9
Training loss: 2.8687018363416716
Validation loss: 2.4914215250106224

Epoch: 6| Step: 10
Training loss: 2.3794238399686556
Validation loss: 2.4910911053715252

Epoch: 6| Step: 11
Training loss: 2.0443990161013414
Validation loss: 2.494162738396325

Epoch: 6| Step: 12
Training loss: 2.4475052724592485
Validation loss: 2.492058394863823

Epoch: 6| Step: 13
Training loss: 2.799085031605892
Validation loss: 2.501530321951924

Epoch: 96| Step: 0
Training loss: 2.5527334410265126
Validation loss: 2.4895304005208274

Epoch: 6| Step: 1
Training loss: 2.668156495756189
Validation loss: 2.483792410624267

Epoch: 6| Step: 2
Training loss: 2.6582368488400823
Validation loss: 2.4847869211663696

Epoch: 6| Step: 3
Training loss: 2.7324287681683512
Validation loss: 2.4745861555182844

Epoch: 6| Step: 4
Training loss: 2.62151559541972
Validation loss: 2.468729300754221

Epoch: 6| Step: 5
Training loss: 2.6716710062957505
Validation loss: 2.474692134782989

Epoch: 6| Step: 6
Training loss: 2.3380981455190954
Validation loss: 2.4684595347598277

Epoch: 6| Step: 7
Training loss: 2.7759348626313742
Validation loss: 2.464199959438982

Epoch: 6| Step: 8
Training loss: 2.5081786842028717
Validation loss: 2.4641013984778692

Epoch: 6| Step: 9
Training loss: 2.3035569343660085
Validation loss: 2.460064510130228

Epoch: 6| Step: 10
Training loss: 2.5345413096995935
Validation loss: 2.4689919075568674

Epoch: 6| Step: 11
Training loss: 3.169335462067429
Validation loss: 2.469374605830676

Epoch: 6| Step: 12
Training loss: 2.406050240287817
Validation loss: 2.476399461451814

Epoch: 6| Step: 13
Training loss: 2.1200527495893255
Validation loss: 2.4750117712672144

Epoch: 97| Step: 0
Training loss: 2.4399317813154418
Validation loss: 2.463082887111585

Epoch: 6| Step: 1
Training loss: 2.9824049274035502
Validation loss: 2.465835645772768

Epoch: 6| Step: 2
Training loss: 2.1577527987475604
Validation loss: 2.4741087429435247

Epoch: 6| Step: 3
Training loss: 2.693872216233543
Validation loss: 2.4781669449593653

Epoch: 6| Step: 4
Training loss: 2.3822965469833
Validation loss: 2.474880991428191

Epoch: 6| Step: 5
Training loss: 2.722261463150607
Validation loss: 2.4818860908208618

Epoch: 6| Step: 6
Training loss: 2.6834197008753855
Validation loss: 2.4818563910217115

Epoch: 6| Step: 7
Training loss: 2.8296272123961512
Validation loss: 2.485723517363704

Epoch: 6| Step: 8
Training loss: 2.4001176646317046
Validation loss: 2.4841511303623682

Epoch: 6| Step: 9
Training loss: 2.511761749516508
Validation loss: 2.479886829270517

Epoch: 6| Step: 10
Training loss: 2.484890866252983
Validation loss: 2.483486600494231

Epoch: 6| Step: 11
Training loss: 2.7269799205347134
Validation loss: 2.4813099313411686

Epoch: 6| Step: 12
Training loss: 2.9263881160922347
Validation loss: 2.478364371195391

Epoch: 6| Step: 13
Training loss: 2.389888275918879
Validation loss: 2.4792170212277376

Epoch: 98| Step: 0
Training loss: 2.718625120605399
Validation loss: 2.477264561505788

Epoch: 6| Step: 1
Training loss: 2.19503554365364
Validation loss: 2.4758390378763258

Epoch: 6| Step: 2
Training loss: 2.1395009006071324
Validation loss: 2.4722506656034415

Epoch: 6| Step: 3
Training loss: 2.8794998116060126
Validation loss: 2.471264375610423

Epoch: 6| Step: 4
Training loss: 2.732612608659769
Validation loss: 2.4721031112647

Epoch: 6| Step: 5
Training loss: 2.5368584559451652
Validation loss: 2.4726429035455326

Epoch: 6| Step: 6
Training loss: 2.5871153393526485
Validation loss: 2.465164252399955

Epoch: 6| Step: 7
Training loss: 2.482445117232363
Validation loss: 2.4640352320608647

Epoch: 6| Step: 8
Training loss: 2.6917589728842786
Validation loss: 2.4637495978729422

Epoch: 6| Step: 9
Training loss: 2.5947404372217955
Validation loss: 2.458820510264378

Epoch: 6| Step: 10
Training loss: 3.093979027487772
Validation loss: 2.4610397236901753

Epoch: 6| Step: 11
Training loss: 1.812073558758497
Validation loss: 2.462912018673191

Epoch: 6| Step: 12
Training loss: 2.7545721020169287
Validation loss: 2.4660440898613833

Epoch: 6| Step: 13
Training loss: 2.640361975938745
Validation loss: 2.4645414575118587

Epoch: 99| Step: 0
Training loss: 3.0188199052950258
Validation loss: 2.4659907780005867

Epoch: 6| Step: 1
Training loss: 2.6460655190623408
Validation loss: 2.4634030701655947

Epoch: 6| Step: 2
Training loss: 2.3239824960348177
Validation loss: 2.463336626892916

Epoch: 6| Step: 3
Training loss: 2.3779094342694993
Validation loss: 2.4578155899109264

Epoch: 6| Step: 4
Training loss: 2.7296302828717764
Validation loss: 2.4644266330780313

Epoch: 6| Step: 5
Training loss: 2.2536628366923117
Validation loss: 2.459419306962255

Epoch: 6| Step: 6
Training loss: 2.489938037668207
Validation loss: 2.4641118481906776

Epoch: 6| Step: 7
Training loss: 2.5543490777731623
Validation loss: 2.467283698717637

Epoch: 6| Step: 8
Training loss: 2.575135966294511
Validation loss: 2.4726043101114077

Epoch: 6| Step: 9
Training loss: 2.72159909449657
Validation loss: 2.4594900809559324

Epoch: 6| Step: 10
Training loss: 2.6180801918856114
Validation loss: 2.4755350863860164

Epoch: 6| Step: 11
Training loss: 2.5230480165149722
Validation loss: 2.480348054447516

Epoch: 6| Step: 12
Training loss: 2.793008187822523
Validation loss: 2.4836776362441273

Epoch: 6| Step: 13
Training loss: 2.454654095554158
Validation loss: 2.4842183285621724

Epoch: 100| Step: 0
Training loss: 2.4467679405907172
Validation loss: 2.4883036709767925

Epoch: 6| Step: 1
Training loss: 2.8838024558929582
Validation loss: 2.4869400953382668

Epoch: 6| Step: 2
Training loss: 2.413891931328748
Validation loss: 2.4877831296317536

Epoch: 6| Step: 3
Training loss: 2.1110548324222096
Validation loss: 2.489374692040355

Epoch: 6| Step: 4
Training loss: 3.018551370155489
Validation loss: 2.4913649203011543

Epoch: 6| Step: 5
Training loss: 2.446853493392574
Validation loss: 2.4919947885527187

Epoch: 6| Step: 6
Training loss: 2.359702990641095
Validation loss: 2.4919318503515844

Epoch: 6| Step: 7
Training loss: 3.162797689185523
Validation loss: 2.491887711338739

Epoch: 6| Step: 8
Training loss: 2.5734403646113857
Validation loss: 2.4886593294383808

Epoch: 6| Step: 9
Training loss: 2.553786279212456
Validation loss: 2.488026077385297

Epoch: 6| Step: 10
Training loss: 2.364163421675246
Validation loss: 2.4913767549226824

Epoch: 6| Step: 11
Training loss: 2.8450725121132026
Validation loss: 2.486395424178989

Epoch: 6| Step: 12
Training loss: 2.7066671283452184
Validation loss: 2.486453867986724

Epoch: 6| Step: 13
Training loss: 2.4206812716167536
Validation loss: 2.4825553868144166

Epoch: 101| Step: 0
Training loss: 2.2730414710904143
Validation loss: 2.4777944811822006

Epoch: 6| Step: 1
Training loss: 2.800983501457917
Validation loss: 2.4802157696053446

Epoch: 6| Step: 2
Training loss: 2.236696011464002
Validation loss: 2.4854259227528104

Epoch: 6| Step: 3
Training loss: 2.2922820796579435
Validation loss: 2.477644514506855

Epoch: 6| Step: 4
Training loss: 2.625983825877831
Validation loss: 2.4697465454596377

Epoch: 6| Step: 5
Training loss: 2.880687230026253
Validation loss: 2.4733133271617236

Epoch: 6| Step: 6
Training loss: 2.6010232085791514
Validation loss: 2.4722356373236027

Epoch: 6| Step: 7
Training loss: 2.8990936770401876
Validation loss: 2.467536644191666

Epoch: 6| Step: 8
Training loss: 2.6931662154834024
Validation loss: 2.4622494545907996

Epoch: 6| Step: 9
Training loss: 2.5064628986296666
Validation loss: 2.4681910433336838

Epoch: 6| Step: 10
Training loss: 2.3852470593075985
Validation loss: 2.4618846221216706

Epoch: 6| Step: 11
Training loss: 3.114619600388939
Validation loss: 2.458245200393105

Epoch: 6| Step: 12
Training loss: 2.4882235677551816
Validation loss: 2.4613631511101657

Epoch: 6| Step: 13
Training loss: 2.20819558007871
Validation loss: 2.456444294681011

Epoch: 102| Step: 0
Training loss: 2.1618041225349223
Validation loss: 2.462989040811401

Epoch: 6| Step: 1
Training loss: 3.0700630688479515
Validation loss: 2.462345443109514

Epoch: 6| Step: 2
Training loss: 2.303122606625601
Validation loss: 2.4698899771759155

Epoch: 6| Step: 3
Training loss: 3.3112505589704666
Validation loss: 2.4793006529518067

Epoch: 6| Step: 4
Training loss: 2.8006085245456154
Validation loss: 2.4696233468128472

Epoch: 6| Step: 5
Training loss: 2.1798666603536527
Validation loss: 2.465883675444148

Epoch: 6| Step: 6
Training loss: 2.0521745871249015
Validation loss: 2.472959751909688

Epoch: 6| Step: 7
Training loss: 2.58515148375675
Validation loss: 2.4680152958715893

Epoch: 6| Step: 8
Training loss: 2.4355380647770684
Validation loss: 2.471196567882008

Epoch: 6| Step: 9
Training loss: 2.176646308901089
Validation loss: 2.480456390750562

Epoch: 6| Step: 10
Training loss: 2.4236280434013033
Validation loss: 2.4776956591186687

Epoch: 6| Step: 11
Training loss: 2.9345885112788443
Validation loss: 2.4818577999690006

Epoch: 6| Step: 12
Training loss: 2.7195263335181608
Validation loss: 2.4837038425149256

Epoch: 6| Step: 13
Training loss: 2.9482452604840725
Validation loss: 2.4854910720811167

Epoch: 103| Step: 0
Training loss: 2.3323027741845075
Validation loss: 2.4859919534278334

Epoch: 6| Step: 1
Training loss: 2.936337829549448
Validation loss: 2.4883672119495004

Epoch: 6| Step: 2
Training loss: 2.2407930404239047
Validation loss: 2.4844050215660607

Epoch: 6| Step: 3
Training loss: 3.1630327220843864
Validation loss: 2.479739352739543

Epoch: 6| Step: 4
Training loss: 2.899205355457808
Validation loss: 2.475626145737943

Epoch: 6| Step: 5
Training loss: 2.2473151824966564
Validation loss: 2.476583807660046

Epoch: 6| Step: 6
Training loss: 2.7094728762361373
Validation loss: 2.476255155118621

Epoch: 6| Step: 7
Training loss: 2.6169763280594527
Validation loss: 2.476387972453284

Epoch: 6| Step: 8
Training loss: 2.605222767623781
Validation loss: 2.4757580097295806

Epoch: 6| Step: 9
Training loss: 2.3857650724508717
Validation loss: 2.474557556396625

Epoch: 6| Step: 10
Training loss: 2.7183746429553985
Validation loss: 2.4678569900826006

Epoch: 6| Step: 11
Training loss: 1.8590999367760341
Validation loss: 2.464618638324003

Epoch: 6| Step: 12
Training loss: 2.627797316580948
Validation loss: 2.4594014374387667

Epoch: 6| Step: 13
Training loss: 2.642484653395856
Validation loss: 2.4625346334639273

Epoch: 104| Step: 0
Training loss: 2.442654563675316
Validation loss: 2.4597299347961274

Epoch: 6| Step: 1
Training loss: 2.289877996901175
Validation loss: 2.4558385512477026

Epoch: 6| Step: 2
Training loss: 2.5587121834171067
Validation loss: 2.4622408044665107

Epoch: 6| Step: 3
Training loss: 2.787329383726256
Validation loss: 2.463712727990382

Epoch: 6| Step: 4
Training loss: 2.426475536116997
Validation loss: 2.47093275608868

Epoch: 6| Step: 5
Training loss: 2.3981085436755682
Validation loss: 2.4634315406965643

Epoch: 6| Step: 6
Training loss: 2.9314883787916055
Validation loss: 2.4613735720881342

Epoch: 6| Step: 7
Training loss: 2.496596881144106
Validation loss: 2.45884324836098

Epoch: 6| Step: 8
Training loss: 2.2504511486854426
Validation loss: 2.458919913144142

Epoch: 6| Step: 9
Training loss: 2.615929873620226
Validation loss: 2.4802260072332882

Epoch: 6| Step: 10
Training loss: 2.742805085369379
Validation loss: 2.472324134092929

Epoch: 6| Step: 11
Training loss: 2.1156277136735966
Validation loss: 2.4644521814331304

Epoch: 6| Step: 12
Training loss: 2.83547288856198
Validation loss: 2.4685061370544608

Epoch: 6| Step: 13
Training loss: 2.901126406040958
Validation loss: 2.458538698788811

Epoch: 105| Step: 0
Training loss: 2.9755184715542513
Validation loss: 2.4733296502369537

Epoch: 6| Step: 1
Training loss: 2.5257006437947926
Validation loss: 2.4720210122678345

Epoch: 6| Step: 2
Training loss: 2.3747514293492467
Validation loss: 2.462389530731255

Epoch: 6| Step: 3
Training loss: 2.40155284236835
Validation loss: 2.472997206994215

Epoch: 6| Step: 4
Training loss: 2.5845127387283604
Validation loss: 2.4828324557399264

Epoch: 6| Step: 5
Training loss: 1.9726644081475797
Validation loss: 2.4768376563574193

Epoch: 6| Step: 6
Training loss: 2.9979539887891016
Validation loss: 2.4811466602294328

Epoch: 6| Step: 7
Training loss: 2.8220086320600415
Validation loss: 2.476310131479674

Epoch: 6| Step: 8
Training loss: 2.525644665749839
Validation loss: 2.4790692483904775

Epoch: 6| Step: 9
Training loss: 2.648138788982808
Validation loss: 2.475895788902326

Epoch: 6| Step: 10
Training loss: 2.4050241295442727
Validation loss: 2.4805251708334723

Epoch: 6| Step: 11
Training loss: 2.135813184690998
Validation loss: 2.4786785564171643

Epoch: 6| Step: 12
Training loss: 2.4989684837890995
Validation loss: 2.477702491150567

Epoch: 6| Step: 13
Training loss: 3.0068062820781014
Validation loss: 2.4689612801194936

Epoch: 106| Step: 0
Training loss: 2.3441924631177757
Validation loss: 2.473539318271129

Epoch: 6| Step: 1
Training loss: 2.860474088629147
Validation loss: 2.4748823080112476

Epoch: 6| Step: 2
Training loss: 3.069199377281457
Validation loss: 2.4675466606626517

Epoch: 6| Step: 3
Training loss: 2.5077334002322944
Validation loss: 2.4622518914673424

Epoch: 6| Step: 4
Training loss: 2.534361163570767
Validation loss: 2.4525158848768176

Epoch: 6| Step: 5
Training loss: 2.626143433534447
Validation loss: 2.461263176899063

Epoch: 6| Step: 6
Training loss: 2.527528829740317
Validation loss: 2.453216405246853

Epoch: 6| Step: 7
Training loss: 2.5667950990258848
Validation loss: 2.4556003886033473

Epoch: 6| Step: 8
Training loss: 2.2849195047970725
Validation loss: 2.4724380051336636

Epoch: 6| Step: 9
Training loss: 2.1164770306720326
Validation loss: 2.4681448053830954

Epoch: 6| Step: 10
Training loss: 2.197625513061859
Validation loss: 2.4617278106661526

Epoch: 6| Step: 11
Training loss: 2.9798117697236175
Validation loss: 2.454335377781911

Epoch: 6| Step: 12
Training loss: 2.389081868304624
Validation loss: 2.454654095554158

Epoch: 6| Step: 13
Training loss: 2.5871734892273066
Validation loss: 2.459919673608424

Epoch: 107| Step: 0
Training loss: 2.2442608860019986
Validation loss: 2.4697134496004614

Epoch: 6| Step: 1
Training loss: 2.5136716853025374
Validation loss: 2.4673541827083243

Epoch: 6| Step: 2
Training loss: 2.8395603043724864
Validation loss: 2.4715180793217733

Epoch: 6| Step: 3
Training loss: 2.5363493053234016
Validation loss: 2.4714738008238673

Epoch: 6| Step: 4
Training loss: 2.8378217338291645
Validation loss: 2.476451618588437

Epoch: 6| Step: 5
Training loss: 2.4159345065833424
Validation loss: 2.4780899616216137

Epoch: 6| Step: 6
Training loss: 2.382553336636245
Validation loss: 2.472302371832037

Epoch: 6| Step: 7
Training loss: 2.2268376765949585
Validation loss: 2.475090865191537

Epoch: 6| Step: 8
Training loss: 2.129571149394749
Validation loss: 2.477645717355568

Epoch: 6| Step: 9
Training loss: 3.129279753719468
Validation loss: 2.4790659544761326

Epoch: 6| Step: 10
Training loss: 2.3030167035297553
Validation loss: 2.474788692393473

Epoch: 6| Step: 11
Training loss: 2.3440300329126615
Validation loss: 2.4691551938296263

Epoch: 6| Step: 12
Training loss: 3.183624155098289
Validation loss: 2.463604921546385

Epoch: 6| Step: 13
Training loss: 2.6640195423589357
Validation loss: 2.4587290224500165

Epoch: 108| Step: 0
Training loss: 2.4718671989076686
Validation loss: 2.4590450538943602

Epoch: 6| Step: 1
Training loss: 2.024495794373391
Validation loss: 2.4536463756448668

Epoch: 6| Step: 2
Training loss: 2.6124351055353596
Validation loss: 2.464353904675755

Epoch: 6| Step: 3
Training loss: 2.5720003377934972
Validation loss: 2.4600903541491723

Epoch: 6| Step: 4
Training loss: 2.5658311359147907
Validation loss: 2.4624235802941903

Epoch: 6| Step: 5
Training loss: 2.700592265691038
Validation loss: 2.4681644228088016

Epoch: 6| Step: 6
Training loss: 2.4752345812053105
Validation loss: 2.4670684979024995

Epoch: 6| Step: 7
Training loss: 3.203500120946204
Validation loss: 2.46659861150715

Epoch: 6| Step: 8
Training loss: 2.515525484189911
Validation loss: 2.4720357284021492

Epoch: 6| Step: 9
Training loss: 1.9903862682776532
Validation loss: 2.471095214611757

Epoch: 6| Step: 10
Training loss: 2.513892293779047
Validation loss: 2.465726634849078

Epoch: 6| Step: 11
Training loss: 2.4524961017729314
Validation loss: 2.4581081373443148

Epoch: 6| Step: 12
Training loss: 2.7533853674059707
Validation loss: 2.462132142760727

Epoch: 6| Step: 13
Training loss: 2.619453565092402
Validation loss: 2.464159629260444

Epoch: 109| Step: 0
Training loss: 2.5141575484152088
Validation loss: 2.459109771011741

Epoch: 6| Step: 1
Training loss: 2.681993476563897
Validation loss: 2.466576516806629

Epoch: 6| Step: 2
Training loss: 2.68710998544022
Validation loss: 2.470316442252835

Epoch: 6| Step: 3
Training loss: 2.252253887012679
Validation loss: 2.4635488551695603

Epoch: 6| Step: 4
Training loss: 2.5390388370291586
Validation loss: 2.468066189218928

Epoch: 6| Step: 5
Training loss: 2.370894045016114
Validation loss: 2.472164062882679

Epoch: 6| Step: 6
Training loss: 2.4276987258859393
Validation loss: 2.471295167455215

Epoch: 6| Step: 7
Training loss: 3.0445654307485945
Validation loss: 2.477130379536182

Epoch: 6| Step: 8
Training loss: 2.798934604405535
Validation loss: 2.481561663293259

Epoch: 6| Step: 9
Training loss: 2.588605812142741
Validation loss: 2.4662045421816003

Epoch: 6| Step: 10
Training loss: 2.0950313461981267
Validation loss: 2.467497705177031

Epoch: 6| Step: 11
Training loss: 2.5448806536857296
Validation loss: 2.4636680511465903

Epoch: 6| Step: 12
Training loss: 2.1656028140914665
Validation loss: 2.4718965043178645

Epoch: 6| Step: 13
Training loss: 3.172139274223823
Validation loss: 2.468917406417992

Epoch: 110| Step: 0
Training loss: 2.4012281851544586
Validation loss: 2.4699104736360997

Epoch: 6| Step: 1
Training loss: 2.509577430170514
Validation loss: 2.4645328154321415

Epoch: 6| Step: 2
Training loss: 2.5317689340146545
Validation loss: 2.4620423435481675

Epoch: 6| Step: 3
Training loss: 2.706608110304954
Validation loss: 2.463771217947704

Epoch: 6| Step: 4
Training loss: 2.0491726624705042
Validation loss: 2.462576442506669

Epoch: 6| Step: 5
Training loss: 2.9437863876134824
Validation loss: 2.4603332746576623

Epoch: 6| Step: 6
Training loss: 2.623761929598036
Validation loss: 2.46126162700448

Epoch: 6| Step: 7
Training loss: 2.7122260557445643
Validation loss: 2.4603882514762008

Epoch: 6| Step: 8
Training loss: 2.5488984664623233
Validation loss: 2.4614167084943217

Epoch: 6| Step: 9
Training loss: 2.548557965806963
Validation loss: 2.452653600670216

Epoch: 6| Step: 10
Training loss: 2.470434845241817
Validation loss: 2.449957290588554

Epoch: 6| Step: 11
Training loss: 2.783426354808771
Validation loss: 2.4541053028131317

Epoch: 6| Step: 12
Training loss: 2.5330980877654956
Validation loss: 2.4451538781992257

Epoch: 6| Step: 13
Training loss: 2.319632587936628
Validation loss: 2.441902814084591

Epoch: 111| Step: 0
Training loss: 2.6161695635842883
Validation loss: 2.451691113273816

Epoch: 6| Step: 1
Training loss: 2.0230460358990103
Validation loss: 2.452276727077093

Epoch: 6| Step: 2
Training loss: 2.8578280240445975
Validation loss: 2.45466265909668

Epoch: 6| Step: 3
Training loss: 2.2817455432857097
Validation loss: 2.449923603008553

Epoch: 6| Step: 4
Training loss: 2.580087735362495
Validation loss: 2.450358116160882

Epoch: 6| Step: 5
Training loss: 3.131211782743792
Validation loss: 2.447425904014496

Epoch: 6| Step: 6
Training loss: 2.8793320567375593
Validation loss: 2.448767961777296

Epoch: 6| Step: 7
Training loss: 2.7689988414118623
Validation loss: 2.4431645035886262

Epoch: 6| Step: 8
Training loss: 2.857997875343638
Validation loss: 2.4519764181427135

Epoch: 6| Step: 9
Training loss: 2.239575822211393
Validation loss: 2.4494956963708896

Epoch: 6| Step: 10
Training loss: 2.227447655059547
Validation loss: 2.4562067327336656

Epoch: 6| Step: 11
Training loss: 2.8083603805763895
Validation loss: 2.4594653211954682

Epoch: 6| Step: 12
Training loss: 1.8356134656284622
Validation loss: 2.464408533814819

Epoch: 6| Step: 13
Training loss: 2.2735398521848382
Validation loss: 2.465612341038867

Epoch: 112| Step: 0
Training loss: 2.4797667944380097
Validation loss: 2.4632866359804657

Epoch: 6| Step: 1
Training loss: 2.1606451381531633
Validation loss: 2.4595089757571604

Epoch: 6| Step: 2
Training loss: 3.003148651823811
Validation loss: 2.4610475545827977

Epoch: 6| Step: 3
Training loss: 2.7392778310877253
Validation loss: 2.459510381350083

Epoch: 6| Step: 4
Training loss: 2.2208789765878514
Validation loss: 2.459538557664049

Epoch: 6| Step: 5
Training loss: 2.7367142616559685
Validation loss: 2.455479571109967

Epoch: 6| Step: 6
Training loss: 2.5962584901027133
Validation loss: 2.4600875274728238

Epoch: 6| Step: 7
Training loss: 2.3157481501853066
Validation loss: 2.4572468042961235

Epoch: 6| Step: 8
Training loss: 2.1987579567631355
Validation loss: 2.4507682315965846

Epoch: 6| Step: 9
Training loss: 2.894425980974186
Validation loss: 2.4510416145719196

Epoch: 6| Step: 10
Training loss: 2.6659213653780927
Validation loss: 2.4535983979388982

Epoch: 6| Step: 11
Training loss: 2.0633230012297066
Validation loss: 2.449039629115966

Epoch: 6| Step: 12
Training loss: 2.655307557706486
Validation loss: 2.4447108982063126

Epoch: 6| Step: 13
Training loss: 2.7412531203614448
Validation loss: 2.453293132487166

Epoch: 113| Step: 0
Training loss: 2.95338350002257
Validation loss: 2.447391897456664

Epoch: 6| Step: 1
Training loss: 2.069463591803512
Validation loss: 2.449052739132236

Epoch: 6| Step: 2
Training loss: 2.65359472063958
Validation loss: 2.4466231939635614

Epoch: 6| Step: 3
Training loss: 2.219874030139161
Validation loss: 2.455157108498121

Epoch: 6| Step: 4
Training loss: 2.808664376719776
Validation loss: 2.446312258185938

Epoch: 6| Step: 5
Training loss: 1.6931880173444582
Validation loss: 2.4466998437490886

Epoch: 6| Step: 6
Training loss: 2.6380337071206625
Validation loss: 2.454924237454165

Epoch: 6| Step: 7
Training loss: 2.6925754969362807
Validation loss: 2.448036829386734

Epoch: 6| Step: 8
Training loss: 2.80345658092237
Validation loss: 2.445259361786604

Epoch: 6| Step: 9
Training loss: 2.2686677691897614
Validation loss: 2.448045756947098

Epoch: 6| Step: 10
Training loss: 2.895291083267474
Validation loss: 2.4533168126496774

Epoch: 6| Step: 11
Training loss: 2.2052878726915517
Validation loss: 2.457466899536369

Epoch: 6| Step: 12
Training loss: 2.378376818845036
Validation loss: 2.465784247106301

Epoch: 6| Step: 13
Training loss: 2.8508845111171266
Validation loss: 2.4718907815113567

Epoch: 114| Step: 0
Training loss: 2.9890008197169426
Validation loss: 2.467496819460287

Epoch: 6| Step: 1
Training loss: 1.6811719855037228
Validation loss: 2.4713858041992616

Epoch: 6| Step: 2
Training loss: 1.988475917437248
Validation loss: 2.473818055576183

Epoch: 6| Step: 3
Training loss: 2.4027278934168694
Validation loss: 2.4764158765066715

Epoch: 6| Step: 4
Training loss: 2.1509446929673923
Validation loss: 2.4747637565326275

Epoch: 6| Step: 5
Training loss: 2.720261603004707
Validation loss: 2.481196131149507

Epoch: 6| Step: 6
Training loss: 2.6841445202186716
Validation loss: 2.473053854522392

Epoch: 6| Step: 7
Training loss: 2.7423466041249713
Validation loss: 2.4721655416479873

Epoch: 6| Step: 8
Training loss: 2.4315625771862135
Validation loss: 2.4783076125629333

Epoch: 6| Step: 9
Training loss: 2.3117814880913032
Validation loss: 2.473053589404378

Epoch: 6| Step: 10
Training loss: 2.9094411086490934
Validation loss: 2.4668499197634137

Epoch: 6| Step: 11
Training loss: 3.1079277619419834
Validation loss: 2.476641680778403

Epoch: 6| Step: 12
Training loss: 2.8799891098134456
Validation loss: 2.4679373920795613

Epoch: 6| Step: 13
Training loss: 2.4293920208506696
Validation loss: 2.4638955559031666

Epoch: 115| Step: 0
Training loss: 2.5335109644235305
Validation loss: 2.4594199128434613

Epoch: 6| Step: 1
Training loss: 3.2082930393083204
Validation loss: 2.4621538335244133

Epoch: 6| Step: 2
Training loss: 2.678031209329363
Validation loss: 2.458280455025981

Epoch: 6| Step: 3
Training loss: 2.784960553970273
Validation loss: 2.461812367420131

Epoch: 6| Step: 4
Training loss: 2.772251015052606
Validation loss: 2.490964806835787

Epoch: 6| Step: 5
Training loss: 2.4152313661811076
Validation loss: 2.551121220270516

Epoch: 6| Step: 6
Training loss: 2.830325119160583
Validation loss: 2.5196622592665263

Epoch: 6| Step: 7
Training loss: 2.300979061722764
Validation loss: 2.5144787579914363

Epoch: 6| Step: 8
Training loss: 1.9493186787464523
Validation loss: 2.496205963653058

Epoch: 6| Step: 9
Training loss: 2.7991311973360418
Validation loss: 2.4880341747028187

Epoch: 6| Step: 10
Training loss: 2.856310426836615
Validation loss: 2.459552936507219

Epoch: 6| Step: 11
Training loss: 2.134539900256004
Validation loss: 2.4640901907891073

Epoch: 6| Step: 12
Training loss: 2.30587737838797
Validation loss: 2.471793765064661

Epoch: 6| Step: 13
Training loss: 2.3076434301433126
Validation loss: 2.4809790539403798

Epoch: 116| Step: 0
Training loss: 2.5200633825172525
Validation loss: 2.476775568351536

Epoch: 6| Step: 1
Training loss: 2.578945884642702
Validation loss: 2.471589350924105

Epoch: 6| Step: 2
Training loss: 3.2269416937252133
Validation loss: 2.476807751631045

Epoch: 6| Step: 3
Training loss: 2.520814081123754
Validation loss: 2.4800220718093784

Epoch: 6| Step: 4
Training loss: 2.5141444617852295
Validation loss: 2.483020053108102

Epoch: 6| Step: 5
Training loss: 2.6120178179228106
Validation loss: 2.4816981355334162

Epoch: 6| Step: 6
Training loss: 2.4548324182809838
Validation loss: 2.4828358166773916

Epoch: 6| Step: 7
Training loss: 2.57547739658942
Validation loss: 2.4817908100446298

Epoch: 6| Step: 8
Training loss: 2.0911705867558004
Validation loss: 2.482797469711182

Epoch: 6| Step: 9
Training loss: 2.3363322559161355
Validation loss: 2.486683490745691

Epoch: 6| Step: 10
Training loss: 2.2721255997206145
Validation loss: 2.4856654320602485

Epoch: 6| Step: 11
Training loss: 2.5133525939054997
Validation loss: 2.4837055223946485

Epoch: 6| Step: 12
Training loss: 2.690898697918544
Validation loss: 2.4758324815724477

Epoch: 6| Step: 13
Training loss: 3.1739454605714523
Validation loss: 2.4791489501662034

Epoch: 117| Step: 0
Training loss: 2.1033062702297873
Validation loss: 2.483471096194466

Epoch: 6| Step: 1
Training loss: 2.511510101717258
Validation loss: 2.477493095247185

Epoch: 6| Step: 2
Training loss: 3.214192246788671
Validation loss: 2.48154107091324

Epoch: 6| Step: 3
Training loss: 3.1176058457431113
Validation loss: 2.478115120571819

Epoch: 6| Step: 4
Training loss: 2.833087424284273
Validation loss: 2.470951466901821

Epoch: 6| Step: 5
Training loss: 2.1992933352139286
Validation loss: 2.4671788104957013

Epoch: 6| Step: 6
Training loss: 2.223738636418814
Validation loss: 2.465616507086492

Epoch: 6| Step: 7
Training loss: 2.9689777487681956
Validation loss: 2.466784310327759

Epoch: 6| Step: 8
Training loss: 2.657557087354171
Validation loss: 2.4638497534301527

Epoch: 6| Step: 9
Training loss: 2.4763037128197745
Validation loss: 2.4641922030664665

Epoch: 6| Step: 10
Training loss: 2.0751491263242685
Validation loss: 2.4589520230840254

Epoch: 6| Step: 11
Training loss: 2.3466607901470704
Validation loss: 2.4543623021056935

Epoch: 6| Step: 12
Training loss: 2.6301099903618796
Validation loss: 2.457514082181728

Epoch: 6| Step: 13
Training loss: 2.2987795867592387
Validation loss: 2.454304688873473

Epoch: 118| Step: 0
Training loss: 2.8492129326591797
Validation loss: 2.4639984148327985

Epoch: 6| Step: 1
Training loss: 2.323429467316257
Validation loss: 2.4640883362745405

Epoch: 6| Step: 2
Training loss: 2.440777458088826
Validation loss: 2.456833743034701

Epoch: 6| Step: 3
Training loss: 2.8157125986509888
Validation loss: 2.4633716634814653

Epoch: 6| Step: 4
Training loss: 2.962362384416607
Validation loss: 2.456513625760616

Epoch: 6| Step: 5
Training loss: 2.4199408899134394
Validation loss: 2.451785918753638

Epoch: 6| Step: 6
Training loss: 2.6789823888959496
Validation loss: 2.459973343185273

Epoch: 6| Step: 7
Training loss: 2.632422172069745
Validation loss: 2.463916037719466

Epoch: 6| Step: 8
Training loss: 2.306230242057875
Validation loss: 2.461548260178687

Epoch: 6| Step: 9
Training loss: 1.9118558659115137
Validation loss: 2.466500379997331

Epoch: 6| Step: 10
Training loss: 2.634117369085408
Validation loss: 2.4653984529811965

Epoch: 6| Step: 11
Training loss: 2.7228459164904524
Validation loss: 2.4696776984949182

Epoch: 6| Step: 12
Training loss: 2.598656758538909
Validation loss: 2.461934028170272

Epoch: 6| Step: 13
Training loss: 2.26272060098622
Validation loss: 2.4551211292402133

Epoch: 119| Step: 0
Training loss: 2.454104185577339
Validation loss: 2.4613116267574147

Epoch: 6| Step: 1
Training loss: 2.5617232657413407
Validation loss: 2.457553874654475

Epoch: 6| Step: 2
Training loss: 2.505548328531147
Validation loss: 2.4561435246058685

Epoch: 6| Step: 3
Training loss: 2.753572484345417
Validation loss: 2.4570915728241074

Epoch: 6| Step: 4
Training loss: 2.8561125941374623
Validation loss: 2.450611519562831

Epoch: 6| Step: 5
Training loss: 3.0424490841168335
Validation loss: 2.452885196894959

Epoch: 6| Step: 6
Training loss: 3.0575284503352336
Validation loss: 2.4538640490375285

Epoch: 6| Step: 7
Training loss: 2.091243211150337
Validation loss: 2.462461292459636

Epoch: 6| Step: 8
Training loss: 2.230129663777589
Validation loss: 2.46071882941554

Epoch: 6| Step: 9
Training loss: 2.49383930247174
Validation loss: 2.4601451425714504

Epoch: 6| Step: 10
Training loss: 2.511955095861685
Validation loss: 2.455658076616886

Epoch: 6| Step: 11
Training loss: 2.595923650485817
Validation loss: 2.447994609701127

Epoch: 6| Step: 12
Training loss: 2.0891989595615486
Validation loss: 2.4489133442638225

Epoch: 6| Step: 13
Training loss: 1.9032526684362399
Validation loss: 2.4696931928494776

Epoch: 120| Step: 0
Training loss: 2.2808302401697187
Validation loss: 2.467906083352602

Epoch: 6| Step: 1
Training loss: 2.2795351454696093
Validation loss: 2.494198680212974

Epoch: 6| Step: 2
Training loss: 1.7370365787227748
Validation loss: 2.499246038550451

Epoch: 6| Step: 3
Training loss: 2.8789940332825905
Validation loss: 2.5364181755589406

Epoch: 6| Step: 4
Training loss: 2.983973610932608
Validation loss: 2.5003828709200087

Epoch: 6| Step: 5
Training loss: 2.7172694943534488
Validation loss: 2.4943796559590394

Epoch: 6| Step: 6
Training loss: 2.59446706396676
Validation loss: 2.484104149727159

Epoch: 6| Step: 7
Training loss: 2.3781312077725727
Validation loss: 2.480821927776607

Epoch: 6| Step: 8
Training loss: 3.230243495112981
Validation loss: 2.4749244382992655

Epoch: 6| Step: 9
Training loss: 2.482381728813356
Validation loss: 2.453160247225547

Epoch: 6| Step: 10
Training loss: 2.521402300053647
Validation loss: 2.452223658753096

Epoch: 6| Step: 11
Training loss: 2.3444686805030823
Validation loss: 2.460702325795945

Epoch: 6| Step: 12
Training loss: 2.9178355644910305
Validation loss: 2.4601150187815533

Epoch: 6| Step: 13
Training loss: 2.332996775559913
Validation loss: 2.466612151763544

Epoch: 121| Step: 0
Training loss: 3.026599896012354
Validation loss: 2.46932751302809

Epoch: 6| Step: 1
Training loss: 2.3319717476312802
Validation loss: 2.467898467438671

Epoch: 6| Step: 2
Training loss: 2.456415193161834
Validation loss: 2.475910923359516

Epoch: 6| Step: 3
Training loss: 2.359314772487705
Validation loss: 2.4809956869482264

Epoch: 6| Step: 4
Training loss: 2.6539456413698814
Validation loss: 2.4838560350064043

Epoch: 6| Step: 5
Training loss: 2.534610354482095
Validation loss: 2.482390404798638

Epoch: 6| Step: 6
Training loss: 2.685565695956776
Validation loss: 2.4823039637389215

Epoch: 6| Step: 7
Training loss: 2.764203584562682
Validation loss: 2.4742209097737686

Epoch: 6| Step: 8
Training loss: 2.7720015986109057
Validation loss: 2.480865206536653

Epoch: 6| Step: 9
Training loss: 2.7818913952479947
Validation loss: 2.4779643233981514

Epoch: 6| Step: 10
Training loss: 2.531844457829623
Validation loss: 2.475351849987137

Epoch: 6| Step: 11
Training loss: 2.383733852673847
Validation loss: 2.4754495135456023

Epoch: 6| Step: 12
Training loss: 2.6859653438170112
Validation loss: 2.477998535790574

Epoch: 6| Step: 13
Training loss: 2.0570553672063476
Validation loss: 2.47584298609501

Epoch: 122| Step: 0
Training loss: 2.997559508432117
Validation loss: 2.4734040184751067

Epoch: 6| Step: 1
Training loss: 2.7043596215512835
Validation loss: 2.467714277533772

Epoch: 6| Step: 2
Training loss: 2.7090155304737817
Validation loss: 2.4658783093169174

Epoch: 6| Step: 3
Training loss: 2.1929741076929563
Validation loss: 2.4640878444247907

Epoch: 6| Step: 4
Training loss: 1.951901350084744
Validation loss: 2.4663137011543417

Epoch: 6| Step: 5
Training loss: 2.9072178695389854
Validation loss: 2.457354776483797

Epoch: 6| Step: 6
Training loss: 2.2514812044481194
Validation loss: 2.4674271930773792

Epoch: 6| Step: 7
Training loss: 2.740885281355153
Validation loss: 2.4570350990608927

Epoch: 6| Step: 8
Training loss: 2.2311713453261697
Validation loss: 2.4574639404848995

Epoch: 6| Step: 9
Training loss: 2.5878902564858226
Validation loss: 2.4621799945260987

Epoch: 6| Step: 10
Training loss: 2.5994202554353683
Validation loss: 2.4531664834696185

Epoch: 6| Step: 11
Training loss: 2.372290018836584
Validation loss: 2.4556441442362003

Epoch: 6| Step: 12
Training loss: 2.581523989713548
Validation loss: 2.462143206042421

Epoch: 6| Step: 13
Training loss: 2.7744327962122712
Validation loss: 2.4628683114912833

Epoch: 123| Step: 0
Training loss: 2.664968207994531
Validation loss: 2.4598599774484624

Epoch: 6| Step: 1
Training loss: 2.754208725354675
Validation loss: 2.4584094590722705

Epoch: 6| Step: 2
Training loss: 2.71272065280392
Validation loss: 2.4562238489624173

Epoch: 6| Step: 3
Training loss: 2.3687030233994135
Validation loss: 2.4635444194840153

Epoch: 6| Step: 4
Training loss: 2.495995843486504
Validation loss: 2.4608295396809368

Epoch: 6| Step: 5
Training loss: 2.1184391390080464
Validation loss: 2.455245816237729

Epoch: 6| Step: 6
Training loss: 2.529439679094847
Validation loss: 2.4637163246830025

Epoch: 6| Step: 7
Training loss: 2.5546161046682188
Validation loss: 2.451277885829558

Epoch: 6| Step: 8
Training loss: 2.335141355659101
Validation loss: 2.4521172269705844

Epoch: 6| Step: 9
Training loss: 2.244138075227011
Validation loss: 2.452874399655521

Epoch: 6| Step: 10
Training loss: 2.704166983284562
Validation loss: 2.4497789455130783

Epoch: 6| Step: 11
Training loss: 2.9280473924772417
Validation loss: 2.449979218894381

Epoch: 6| Step: 12
Training loss: 2.409237184058787
Validation loss: 2.4535808099271343

Epoch: 6| Step: 13
Training loss: 2.7195967309394913
Validation loss: 2.452416433063577

Epoch: 124| Step: 0
Training loss: 2.0631271333634316
Validation loss: 2.4634203460907798

Epoch: 6| Step: 1
Training loss: 2.9082100114298925
Validation loss: 2.453772773177083

Epoch: 6| Step: 2
Training loss: 2.7171401596113247
Validation loss: 2.4585597909190837

Epoch: 6| Step: 3
Training loss: 2.8054675874253543
Validation loss: 2.453408259339534

Epoch: 6| Step: 4
Training loss: 2.47103534621026
Validation loss: 2.457163279048252

Epoch: 6| Step: 5
Training loss: 2.384568264829369
Validation loss: 2.4582837687091748

Epoch: 6| Step: 6
Training loss: 2.352821798662144
Validation loss: 2.4504229491923613

Epoch: 6| Step: 7
Training loss: 1.846646635908613
Validation loss: 2.4494921923537163

Epoch: 6| Step: 8
Training loss: 2.845333454991308
Validation loss: 2.4538034525442036

Epoch: 6| Step: 9
Training loss: 2.765676034575029
Validation loss: 2.4548867737038575

Epoch: 6| Step: 10
Training loss: 2.525090104563267
Validation loss: 2.454072449391664

Epoch: 6| Step: 11
Training loss: 2.3568760857166104
Validation loss: 2.4540832170720903

Epoch: 6| Step: 12
Training loss: 2.6557011654391287
Validation loss: 2.451089245136766

Epoch: 6| Step: 13
Training loss: 2.634494776056149
Validation loss: 2.4523491087991984

Epoch: 125| Step: 0
Training loss: 2.18320097145934
Validation loss: 2.4489016127308703

Epoch: 6| Step: 1
Training loss: 2.6880281506040706
Validation loss: 2.4465544840259525

Epoch: 6| Step: 2
Training loss: 2.2542176453002076
Validation loss: 2.44092473278875

Epoch: 6| Step: 3
Training loss: 1.9045634078298836
Validation loss: 2.450643203279886

Epoch: 6| Step: 4
Training loss: 2.3651057491088583
Validation loss: 2.4476034302201755

Epoch: 6| Step: 5
Training loss: 2.703847975875853
Validation loss: 2.445172599389885

Epoch: 6| Step: 6
Training loss: 2.4987524734645126
Validation loss: 2.434125667628544

Epoch: 6| Step: 7
Training loss: 2.9559167653961764
Validation loss: 2.4454232090273025

Epoch: 6| Step: 8
Training loss: 2.232057343071089
Validation loss: 2.4454107620272834

Epoch: 6| Step: 9
Training loss: 2.9736387456324227
Validation loss: 2.4489205973411443

Epoch: 6| Step: 10
Training loss: 2.780822035191438
Validation loss: 2.4450079877602

Epoch: 6| Step: 11
Training loss: 2.4382586154541075
Validation loss: 2.447781519845365

Epoch: 6| Step: 12
Training loss: 2.58885282082962
Validation loss: 2.4516233800943232

Epoch: 6| Step: 13
Training loss: 2.7689695663019567
Validation loss: 2.4541117795323446

Testing loss: 2.0259613541190302

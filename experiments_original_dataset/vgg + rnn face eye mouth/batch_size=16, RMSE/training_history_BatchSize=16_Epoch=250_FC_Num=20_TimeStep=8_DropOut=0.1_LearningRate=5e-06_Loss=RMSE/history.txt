Epoch: 1| Step: 0
Training loss: 6.410686761544247
Validation loss: 5.8705211367780965

Epoch: 6| Step: 1
Training loss: 6.161188458820072
Validation loss: 5.868590301085

Epoch: 6| Step: 2
Training loss: 6.241267092265175
Validation loss: 5.866738320404355

Epoch: 6| Step: 3
Training loss: 7.021663929814764
Validation loss: 5.864749271446059

Epoch: 6| Step: 4
Training loss: 5.808414130650014
Validation loss: 5.862722947599594

Epoch: 6| Step: 5
Training loss: 4.888304723653085
Validation loss: 5.860665981912485

Epoch: 6| Step: 6
Training loss: 6.152696232462243
Validation loss: 5.858613123210789

Epoch: 6| Step: 7
Training loss: 5.792220311037193
Validation loss: 5.856467296106813

Epoch: 6| Step: 8
Training loss: 6.074059860261105
Validation loss: 5.854443481344519

Epoch: 6| Step: 9
Training loss: 5.695862710798398
Validation loss: 5.852174150579991

Epoch: 6| Step: 10
Training loss: 6.388423773747972
Validation loss: 5.849990232845315

Epoch: 6| Step: 11
Training loss: 5.1559237290239
Validation loss: 5.847624366484765

Epoch: 6| Step: 12
Training loss: 5.957102650699146
Validation loss: 5.845220316228243

Epoch: 6| Step: 13
Training loss: 5.583672508451818
Validation loss: 5.842765385165631

Epoch: 2| Step: 0
Training loss: 4.465575898249558
Validation loss: 5.840206139249

Epoch: 6| Step: 1
Training loss: 5.73903240275936
Validation loss: 5.837592949355279

Epoch: 6| Step: 2
Training loss: 6.207287523225913
Validation loss: 5.834862408730538

Epoch: 6| Step: 3
Training loss: 4.90871877576331
Validation loss: 5.83191514714054

Epoch: 6| Step: 4
Training loss: 6.071390142239036
Validation loss: 5.829121576897845

Epoch: 6| Step: 5
Training loss: 6.400561379607346
Validation loss: 5.825858022122496

Epoch: 6| Step: 6
Training loss: 6.578352029466346
Validation loss: 5.822608716688724

Epoch: 6| Step: 7
Training loss: 5.769643649463706
Validation loss: 5.81916905037033

Epoch: 6| Step: 8
Training loss: 5.6058056029012935
Validation loss: 5.815476069250736

Epoch: 6| Step: 9
Training loss: 5.705959002625978
Validation loss: 5.8117682430462185

Epoch: 6| Step: 10
Training loss: 6.718761071484337
Validation loss: 5.8077056144216

Epoch: 6| Step: 11
Training loss: 5.858525980155439
Validation loss: 5.803621984128868

Epoch: 6| Step: 12
Training loss: 6.722333369461513
Validation loss: 5.799187869730148

Epoch: 6| Step: 13
Training loss: 5.877691423486484
Validation loss: 5.7944542833640735

Epoch: 3| Step: 0
Training loss: 5.9105693221477456
Validation loss: 5.78948075280374

Epoch: 6| Step: 1
Training loss: 5.446172003319955
Validation loss: 5.7845225101885545

Epoch: 6| Step: 2
Training loss: 6.5206390774426
Validation loss: 5.77927971334897

Epoch: 6| Step: 3
Training loss: 5.2748392234273656
Validation loss: 5.773784649382894

Epoch: 6| Step: 4
Training loss: 6.1187363340948
Validation loss: 5.768146545874739

Epoch: 6| Step: 5
Training loss: 5.755130303545786
Validation loss: 5.762371547809185

Epoch: 6| Step: 6
Training loss: 5.85837556580487
Validation loss: 5.756214211594631

Epoch: 6| Step: 7
Training loss: 5.676153581645045
Validation loss: 5.750010918869487

Epoch: 6| Step: 8
Training loss: 6.886790482379536
Validation loss: 5.743686748456618

Epoch: 6| Step: 9
Training loss: 5.750840830484561
Validation loss: 5.736695224721172

Epoch: 6| Step: 10
Training loss: 5.865650935819391
Validation loss: 5.729381386172242

Epoch: 6| Step: 11
Training loss: 6.044601604500408
Validation loss: 5.722326831211222

Epoch: 6| Step: 12
Training loss: 5.259404070496752
Validation loss: 5.714882131514141

Epoch: 6| Step: 13
Training loss: 5.559537977603361
Validation loss: 5.707181475553187

Epoch: 4| Step: 0
Training loss: 6.004819841437793
Validation loss: 5.69940080672206

Epoch: 6| Step: 1
Training loss: 6.413538810362145
Validation loss: 5.691258568419702

Epoch: 6| Step: 2
Training loss: 6.382463760840892
Validation loss: 5.682916408173071

Epoch: 6| Step: 3
Training loss: 5.4945270010558716
Validation loss: 5.674453891687941

Epoch: 6| Step: 4
Training loss: 6.538328578511938
Validation loss: 5.665592886596498

Epoch: 6| Step: 5
Training loss: 5.616587133250104
Validation loss: 5.656119554114913

Epoch: 6| Step: 6
Training loss: 5.453945852326315
Validation loss: 5.647445739377353

Epoch: 6| Step: 7
Training loss: 4.757946595054594
Validation loss: 5.637634499038668

Epoch: 6| Step: 8
Training loss: 6.358979383905222
Validation loss: 5.62858314831062

Epoch: 6| Step: 9
Training loss: 6.245619803000564
Validation loss: 5.61921176505157

Epoch: 6| Step: 10
Training loss: 6.195766898925452
Validation loss: 5.609161174527596

Epoch: 6| Step: 11
Training loss: 5.1760578124813685
Validation loss: 5.598937316381944

Epoch: 6| Step: 12
Training loss: 4.36294464206309
Validation loss: 5.589539660684725

Epoch: 6| Step: 13
Training loss: 5.112858602543161
Validation loss: 5.5798493897998656

Epoch: 5| Step: 0
Training loss: 5.542268113312794
Validation loss: 5.570044241318023

Epoch: 6| Step: 1
Training loss: 6.541642531170228
Validation loss: 5.560617106791062

Epoch: 6| Step: 2
Training loss: 5.828237578503846
Validation loss: 5.550699754654068

Epoch: 6| Step: 3
Training loss: 5.529297047476596
Validation loss: 5.541313886766821

Epoch: 6| Step: 4
Training loss: 4.848412220403702
Validation loss: 5.532005473985811

Epoch: 6| Step: 5
Training loss: 5.713985605533509
Validation loss: 5.522816581303661

Epoch: 6| Step: 6
Training loss: 7.011028593732782
Validation loss: 5.513532809085771

Epoch: 6| Step: 7
Training loss: 4.507257331498142
Validation loss: 5.50448440031265

Epoch: 6| Step: 8
Training loss: 5.740190846530266
Validation loss: 5.495500284456806

Epoch: 6| Step: 9
Training loss: 5.804775889405266
Validation loss: 5.486268617753848

Epoch: 6| Step: 10
Training loss: 5.035438073959374
Validation loss: 5.477487439724873

Epoch: 6| Step: 11
Training loss: 4.99120644733583
Validation loss: 5.468581249721569

Epoch: 6| Step: 12
Training loss: 5.201378331543626
Validation loss: 5.45977293448309

Epoch: 6| Step: 13
Training loss: 5.997573043804593
Validation loss: 5.451221589304013

Epoch: 6| Step: 0
Training loss: 5.446618863428672
Validation loss: 5.44296840307991

Epoch: 6| Step: 1
Training loss: 5.605653851473445
Validation loss: 5.433970332493747

Epoch: 6| Step: 2
Training loss: 4.729591092909399
Validation loss: 5.426068563157245

Epoch: 6| Step: 3
Training loss: 5.38219945597399
Validation loss: 5.418014075124714

Epoch: 6| Step: 4
Training loss: 3.792336974088223
Validation loss: 5.4098880412139065

Epoch: 6| Step: 5
Training loss: 5.367289490820435
Validation loss: 5.402508243788103

Epoch: 6| Step: 6
Training loss: 5.78014928288378
Validation loss: 5.395183990565402

Epoch: 6| Step: 7
Training loss: 6.246300174434513
Validation loss: 5.387612294312359

Epoch: 6| Step: 8
Training loss: 6.180478205774273
Validation loss: 5.379896137033856

Epoch: 6| Step: 9
Training loss: 5.5700659570264515
Validation loss: 5.372588333722009

Epoch: 6| Step: 10
Training loss: 6.204248129687942
Validation loss: 5.365690493605168

Epoch: 6| Step: 11
Training loss: 4.616479396579494
Validation loss: 5.358718520459771

Epoch: 6| Step: 12
Training loss: 5.846087799008063
Validation loss: 5.35152564117287

Epoch: 6| Step: 13
Training loss: 5.843764034805172
Validation loss: 5.345045679965968

Epoch: 7| Step: 0
Training loss: 4.986405775075966
Validation loss: 5.338081163843407

Epoch: 6| Step: 1
Training loss: 5.0800997137060495
Validation loss: 5.331816974288826

Epoch: 6| Step: 2
Training loss: 5.917576822830572
Validation loss: 5.3254889477161536

Epoch: 6| Step: 3
Training loss: 5.249423222420805
Validation loss: 5.318994180443651

Epoch: 6| Step: 4
Training loss: 5.39142989631733
Validation loss: 5.312375355174552

Epoch: 6| Step: 5
Training loss: 5.18672597235056
Validation loss: 5.306667192561317

Epoch: 6| Step: 6
Training loss: 5.921995458661071
Validation loss: 5.300382130120536

Epoch: 6| Step: 7
Training loss: 5.229784053917556
Validation loss: 5.293896192913764

Epoch: 6| Step: 8
Training loss: 4.901425850945259
Validation loss: 5.287566912192361

Epoch: 6| Step: 9
Training loss: 5.862112641703794
Validation loss: 5.281473700546526

Epoch: 6| Step: 10
Training loss: 4.927878747761223
Validation loss: 5.2757851882559095

Epoch: 6| Step: 11
Training loss: 6.211593727616359
Validation loss: 5.269587865949025

Epoch: 6| Step: 12
Training loss: 5.898027782231168
Validation loss: 5.263226635467289

Epoch: 6| Step: 13
Training loss: 4.853180595516737
Validation loss: 5.257063277108224

Epoch: 8| Step: 0
Training loss: 5.241679910340835
Validation loss: 5.250826755397067

Epoch: 6| Step: 1
Training loss: 6.163260404087576
Validation loss: 5.24502503356681

Epoch: 6| Step: 2
Training loss: 6.207957040874066
Validation loss: 5.239128269989064

Epoch: 6| Step: 3
Training loss: 5.305663613133839
Validation loss: 5.2334405928551915

Epoch: 6| Step: 4
Training loss: 5.127979621941936
Validation loss: 5.227339538076895

Epoch: 6| Step: 5
Training loss: 5.487268798406617
Validation loss: 5.221288463404279

Epoch: 6| Step: 6
Training loss: 5.725003518182598
Validation loss: 5.2155469556012575

Epoch: 6| Step: 7
Training loss: 4.75340028827124
Validation loss: 5.209612340922895

Epoch: 6| Step: 8
Training loss: 4.443889691910913
Validation loss: 5.203348515838884

Epoch: 6| Step: 9
Training loss: 5.594876990990198
Validation loss: 5.197647526053917

Epoch: 6| Step: 10
Training loss: 5.681753084676804
Validation loss: 5.191633791179058

Epoch: 6| Step: 11
Training loss: 5.575646627243305
Validation loss: 5.186208464617916

Epoch: 6| Step: 12
Training loss: 4.923731281276894
Validation loss: 5.1801773537771165

Epoch: 6| Step: 13
Training loss: 4.030677222435451
Validation loss: 5.174396372802916

Epoch: 9| Step: 0
Training loss: 4.613949952527521
Validation loss: 5.168843067319969

Epoch: 6| Step: 1
Training loss: 5.067840208537801
Validation loss: 5.163277262873107

Epoch: 6| Step: 2
Training loss: 4.725595119882843
Validation loss: 5.157845082796916

Epoch: 6| Step: 3
Training loss: 5.274247622958297
Validation loss: 5.152291398629586

Epoch: 6| Step: 4
Training loss: 4.965365810843955
Validation loss: 5.1470089494869375

Epoch: 6| Step: 5
Training loss: 5.294715823867942
Validation loss: 5.1419193141862785

Epoch: 6| Step: 6
Training loss: 6.07156355251001
Validation loss: 5.1367668043240995

Epoch: 6| Step: 7
Training loss: 5.768648507495264
Validation loss: 5.131000138834856

Epoch: 6| Step: 8
Training loss: 5.712932358287738
Validation loss: 5.125173054076176

Epoch: 6| Step: 9
Training loss: 4.488890179129269
Validation loss: 5.119389696404002

Epoch: 6| Step: 10
Training loss: 5.003521060933205
Validation loss: 5.114114847370833

Epoch: 6| Step: 11
Training loss: 4.817230871786688
Validation loss: 5.10851480473861

Epoch: 6| Step: 12
Training loss: 6.429241920503792
Validation loss: 5.103419917489418

Epoch: 6| Step: 13
Training loss: 4.9596935725155396
Validation loss: 5.0974920838328694

Epoch: 10| Step: 0
Training loss: 4.8769382633934555
Validation loss: 5.091421020043087

Epoch: 6| Step: 1
Training loss: 4.613681449618589
Validation loss: 5.086117008320859

Epoch: 6| Step: 2
Training loss: 5.262283125008017
Validation loss: 5.080634427291227

Epoch: 6| Step: 3
Training loss: 5.306796074795768
Validation loss: 5.074914099055566

Epoch: 6| Step: 4
Training loss: 5.148501776379399
Validation loss: 5.070393754164032

Epoch: 6| Step: 5
Training loss: 5.861843880903178
Validation loss: 5.064663962200073

Epoch: 6| Step: 6
Training loss: 4.906409388583357
Validation loss: 5.058939462677944

Epoch: 6| Step: 7
Training loss: 5.979316665238857
Validation loss: 5.053955685498626

Epoch: 6| Step: 8
Training loss: 5.079291482611726
Validation loss: 5.048589063010622

Epoch: 6| Step: 9
Training loss: 4.979136617189442
Validation loss: 5.043757338835629

Epoch: 6| Step: 10
Training loss: 4.060681449564699
Validation loss: 5.038500598264018

Epoch: 6| Step: 11
Training loss: 5.393116427768531
Validation loss: 5.033270041381506

Epoch: 6| Step: 12
Training loss: 5.716944008078434
Validation loss: 5.027987225820034

Epoch: 6| Step: 13
Training loss: 5.0373799198501175
Validation loss: 5.022513312978334

Epoch: 11| Step: 0
Training loss: 4.571396188961867
Validation loss: 5.017527471979388

Epoch: 6| Step: 1
Training loss: 5.002036824209421
Validation loss: 5.012228064005939

Epoch: 6| Step: 2
Training loss: 4.602792986610183
Validation loss: 5.007171732693133

Epoch: 6| Step: 3
Training loss: 5.773138841226506
Validation loss: 5.001893829268316

Epoch: 6| Step: 4
Training loss: 5.897372562702016
Validation loss: 4.99670015163963

Epoch: 6| Step: 5
Training loss: 5.237273640623295
Validation loss: 4.991640923656045

Epoch: 6| Step: 6
Training loss: 4.403389509498709
Validation loss: 4.985719919871664

Epoch: 6| Step: 7
Training loss: 5.664035897028043
Validation loss: 4.981044314169148

Epoch: 6| Step: 8
Training loss: 4.876972484130079
Validation loss: 4.975610119901571

Epoch: 6| Step: 9
Training loss: 4.3762068037719475
Validation loss: 4.970537483937906

Epoch: 6| Step: 10
Training loss: 5.279201855300089
Validation loss: 4.96532855004211

Epoch: 6| Step: 11
Training loss: 5.584651089307786
Validation loss: 4.9603071478115845

Epoch: 6| Step: 12
Training loss: 5.035073290748893
Validation loss: 4.95604304570677

Epoch: 6| Step: 13
Training loss: 4.904640273844792
Validation loss: 4.950869555150066

Epoch: 12| Step: 0
Training loss: 4.128493245959547
Validation loss: 4.945435854401346

Epoch: 6| Step: 1
Training loss: 5.248969385622863
Validation loss: 4.940380302544813

Epoch: 6| Step: 2
Training loss: 5.45579065790476
Validation loss: 4.935249008652429

Epoch: 6| Step: 3
Training loss: 4.649389756060441
Validation loss: 4.929655902195023

Epoch: 6| Step: 4
Training loss: 4.798616273432211
Validation loss: 4.92493497802361

Epoch: 6| Step: 5
Training loss: 5.471124623957939
Validation loss: 4.9192127273490875

Epoch: 6| Step: 6
Training loss: 4.98201549970803
Validation loss: 4.91351577313105

Epoch: 6| Step: 7
Training loss: 5.10594692194568
Validation loss: 4.90733267142743

Epoch: 6| Step: 8
Training loss: 5.385943605282839
Validation loss: 4.902055182202535

Epoch: 6| Step: 9
Training loss: 4.54390847875545
Validation loss: 4.896301744927945

Epoch: 6| Step: 10
Training loss: 4.688760003181186
Validation loss: 4.891027626957378

Epoch: 6| Step: 11
Training loss: 5.259504524943428
Validation loss: 4.88619837034515

Epoch: 6| Step: 12
Training loss: 5.960820388143484
Validation loss: 4.88118117280271

Epoch: 6| Step: 13
Training loss: 4.520699260264701
Validation loss: 4.875331280529526

Epoch: 13| Step: 0
Training loss: 5.22953075747331
Validation loss: 4.869772333388957

Epoch: 6| Step: 1
Training loss: 4.7804520882411055
Validation loss: 4.8648218372807746

Epoch: 6| Step: 2
Training loss: 5.009435524577828
Validation loss: 4.860351693310058

Epoch: 6| Step: 3
Training loss: 5.35003043192827
Validation loss: 4.855610721361078

Epoch: 6| Step: 4
Training loss: 5.498800320266067
Validation loss: 4.85004159181989

Epoch: 6| Step: 5
Training loss: 4.692851356192561
Validation loss: 4.844809385551749

Epoch: 6| Step: 6
Training loss: 4.508754056712442
Validation loss: 4.839907724010536

Epoch: 6| Step: 7
Training loss: 5.49443552993413
Validation loss: 4.835679975399865

Epoch: 6| Step: 8
Training loss: 4.420236883935908
Validation loss: 4.830743188829112

Epoch: 6| Step: 9
Training loss: 4.150049388545655
Validation loss: 4.8258623172866955

Epoch: 6| Step: 10
Training loss: 5.1985607576333726
Validation loss: 4.8210616804733695

Epoch: 6| Step: 11
Training loss: 5.413874371055021
Validation loss: 4.81578896169337

Epoch: 6| Step: 12
Training loss: 4.883267556920336
Validation loss: 4.810810965195225

Epoch: 6| Step: 13
Training loss: 4.637774584268022
Validation loss: 4.805757398292625

Epoch: 14| Step: 0
Training loss: 4.537876098851244
Validation loss: 4.801181479589711

Epoch: 6| Step: 1
Training loss: 4.246444561318741
Validation loss: 4.795973172152805

Epoch: 6| Step: 2
Training loss: 5.446632345707002
Validation loss: 4.791814796258962

Epoch: 6| Step: 3
Training loss: 5.343619361752929
Validation loss: 4.786419901176411

Epoch: 6| Step: 4
Training loss: 4.3011065367874695
Validation loss: 4.7820733385789564

Epoch: 6| Step: 5
Training loss: 5.022768062047755
Validation loss: 4.776791989602898

Epoch: 6| Step: 6
Training loss: 5.165336601759139
Validation loss: 4.771658924525983

Epoch: 6| Step: 7
Training loss: 5.168675473101424
Validation loss: 4.766867187034786

Epoch: 6| Step: 8
Training loss: 4.942785210877481
Validation loss: 4.762601816952963

Epoch: 6| Step: 9
Training loss: 4.9030150421241725
Validation loss: 4.757507281021371

Epoch: 6| Step: 10
Training loss: 5.352228944588691
Validation loss: 4.752941492380981

Epoch: 6| Step: 11
Training loss: 5.298099342563969
Validation loss: 4.747628305853136

Epoch: 6| Step: 12
Training loss: 3.7707219116160253
Validation loss: 4.74256988441726

Epoch: 6| Step: 13
Training loss: 4.730718932084201
Validation loss: 4.737681561348498

Epoch: 15| Step: 0
Training loss: 4.278503650880829
Validation loss: 4.732698922333555

Epoch: 6| Step: 1
Training loss: 4.440556937333007
Validation loss: 4.727369524363758

Epoch: 6| Step: 2
Training loss: 4.645971641135401
Validation loss: 4.723030321626821

Epoch: 6| Step: 3
Training loss: 5.172735136946128
Validation loss: 4.717577285389595

Epoch: 6| Step: 4
Training loss: 4.806545785203123
Validation loss: 4.712993453656582

Epoch: 6| Step: 5
Training loss: 4.9645423587586786
Validation loss: 4.707720364980387

Epoch: 6| Step: 6
Training loss: 4.8544948396002106
Validation loss: 4.7031637973811575

Epoch: 6| Step: 7
Training loss: 4.722346066116296
Validation loss: 4.69847426168059

Epoch: 6| Step: 8
Training loss: 5.127161454100701
Validation loss: 4.693484406929723

Epoch: 6| Step: 9
Training loss: 4.842564788526747
Validation loss: 4.687929735618419

Epoch: 6| Step: 10
Training loss: 5.210190383449256
Validation loss: 4.682927253648498

Epoch: 6| Step: 11
Training loss: 5.100579673884485
Validation loss: 4.676909898743873

Epoch: 6| Step: 12
Training loss: 4.461516387844373
Validation loss: 4.672301035904377

Epoch: 6| Step: 13
Training loss: 4.848040839838419
Validation loss: 4.666849024933529

Epoch: 16| Step: 0
Training loss: 4.727713049208801
Validation loss: 4.661870529732498

Epoch: 6| Step: 1
Training loss: 4.894126735329281
Validation loss: 4.655980181290262

Epoch: 6| Step: 2
Training loss: 4.237845149654618
Validation loss: 4.651157982864408

Epoch: 6| Step: 3
Training loss: 5.375281038255988
Validation loss: 4.645908332299172

Epoch: 6| Step: 4
Training loss: 4.8349576829480565
Validation loss: 4.640153443495916

Epoch: 6| Step: 5
Training loss: 4.78720380300075
Validation loss: 4.634949438429034

Epoch: 6| Step: 6
Training loss: 5.082871504686529
Validation loss: 4.629174419205449

Epoch: 6| Step: 7
Training loss: 4.971415254734789
Validation loss: 4.624499766472935

Epoch: 6| Step: 8
Training loss: 3.7841508315432066
Validation loss: 4.618004568992878

Epoch: 6| Step: 9
Training loss: 4.912689842069719
Validation loss: 4.613038791246689

Epoch: 6| Step: 10
Training loss: 4.496569279397775
Validation loss: 4.60790126538683

Epoch: 6| Step: 11
Training loss: 5.153476778495673
Validation loss: 4.602618007359234

Epoch: 6| Step: 12
Training loss: 4.307740612073596
Validation loss: 4.597250955933011

Epoch: 6| Step: 13
Training loss: 4.786318019045889
Validation loss: 4.591357933968529

Epoch: 17| Step: 0
Training loss: 4.087318551705413
Validation loss: 4.586710945338906

Epoch: 6| Step: 1
Training loss: 5.441164732952224
Validation loss: 4.581039039434921

Epoch: 6| Step: 2
Training loss: 4.459002768386286
Validation loss: 4.575472708984374

Epoch: 6| Step: 3
Training loss: 4.682556190133214
Validation loss: 4.57008049096162

Epoch: 6| Step: 4
Training loss: 4.8697336557234365
Validation loss: 4.564767857415301

Epoch: 6| Step: 5
Training loss: 4.90938511682897
Validation loss: 4.559792468607626

Epoch: 6| Step: 6
Training loss: 4.006351197105763
Validation loss: 4.55438704287568

Epoch: 6| Step: 7
Training loss: 4.360908922069677
Validation loss: 4.550126004832292

Epoch: 6| Step: 8
Training loss: 4.596569208260453
Validation loss: 4.544829127894449

Epoch: 6| Step: 9
Training loss: 4.825612818882052
Validation loss: 4.53898812264256

Epoch: 6| Step: 10
Training loss: 5.225898096024661
Validation loss: 4.532713029066547

Epoch: 6| Step: 11
Training loss: 4.212003611853809
Validation loss: 4.527255435111846

Epoch: 6| Step: 12
Training loss: 4.2933672649709145
Validation loss: 4.523200802493128

Epoch: 6| Step: 13
Training loss: 5.279772850988215
Validation loss: 4.517671660254713

Epoch: 18| Step: 0
Training loss: 4.5269822642031805
Validation loss: 4.511510926231264

Epoch: 6| Step: 1
Training loss: 5.0964947192017735
Validation loss: 4.506594523655629

Epoch: 6| Step: 2
Training loss: 4.7242849868615
Validation loss: 4.502411037500269

Epoch: 6| Step: 3
Training loss: 4.8833937154081175
Validation loss: 4.497572208635597

Epoch: 6| Step: 4
Training loss: 4.587068850346306
Validation loss: 4.491773202560993

Epoch: 6| Step: 5
Training loss: 3.9301006486734678
Validation loss: 4.486132765731801

Epoch: 6| Step: 6
Training loss: 4.219558073289465
Validation loss: 4.480747949203978

Epoch: 6| Step: 7
Training loss: 5.001993163520933
Validation loss: 4.474326719927742

Epoch: 6| Step: 8
Training loss: 4.8130362075721385
Validation loss: 4.468490450775414

Epoch: 6| Step: 9
Training loss: 4.793862456155282
Validation loss: 4.462667010803559

Epoch: 6| Step: 10
Training loss: 4.680158639159074
Validation loss: 4.457486948738382

Epoch: 6| Step: 11
Training loss: 3.1531836089328933
Validation loss: 4.451942242160031

Epoch: 6| Step: 12
Training loss: 5.027399520742519
Validation loss: 4.445686668734461

Epoch: 6| Step: 13
Training loss: 4.688014294703436
Validation loss: 4.441558808430062

Epoch: 19| Step: 0
Training loss: 4.903527154227464
Validation loss: 4.435131461722828

Epoch: 6| Step: 1
Training loss: 4.83027319662695
Validation loss: 4.429503655136719

Epoch: 6| Step: 2
Training loss: 4.343876020401047
Validation loss: 4.423230290385323

Epoch: 6| Step: 3
Training loss: 4.401809727689757
Validation loss: 4.41850385902095

Epoch: 6| Step: 4
Training loss: 4.844132931246914
Validation loss: 4.413558814194247

Epoch: 6| Step: 5
Training loss: 3.6782402545564175
Validation loss: 4.407920809206951

Epoch: 6| Step: 6
Training loss: 5.452345653030188
Validation loss: 4.402716859194827

Epoch: 6| Step: 7
Training loss: 4.683395419721754
Validation loss: 4.395435562336662

Epoch: 6| Step: 8
Training loss: 4.030596303058338
Validation loss: 4.390074744084279

Epoch: 6| Step: 9
Training loss: 4.317172407965387
Validation loss: 4.386042256560072

Epoch: 6| Step: 10
Training loss: 4.829632076363787
Validation loss: 4.381681517330022

Epoch: 6| Step: 11
Training loss: 4.221748699159795
Validation loss: 4.374263283598865

Epoch: 6| Step: 12
Training loss: 4.134542121306354
Validation loss: 4.368868282028484

Epoch: 6| Step: 13
Training loss: 4.463807473860884
Validation loss: 4.363157083309333

Epoch: 20| Step: 0
Training loss: 4.462320036692043
Validation loss: 4.35839608525942

Epoch: 6| Step: 1
Training loss: 4.905032839106172
Validation loss: 4.35278759643988

Epoch: 6| Step: 2
Training loss: 5.484929505054285
Validation loss: 4.347244540242901

Epoch: 6| Step: 3
Training loss: 4.364580662740954
Validation loss: 4.341097783595279

Epoch: 6| Step: 4
Training loss: 4.0648440127690675
Validation loss: 4.334326807481636

Epoch: 6| Step: 5
Training loss: 3.578451882968827
Validation loss: 4.329003163475854

Epoch: 6| Step: 6
Training loss: 4.129925186181588
Validation loss: 4.323578724854051

Epoch: 6| Step: 7
Training loss: 4.2406002890106524
Validation loss: 4.318666647048279

Epoch: 6| Step: 8
Training loss: 4.451897274563028
Validation loss: 4.3135538702415

Epoch: 6| Step: 9
Training loss: 4.05144440125373
Validation loss: 4.308474278803867

Epoch: 6| Step: 10
Training loss: 4.442284811686947
Validation loss: 4.302847836649691

Epoch: 6| Step: 11
Training loss: 4.575390099742243
Validation loss: 4.2962841020669265

Epoch: 6| Step: 12
Training loss: 4.284408406712392
Validation loss: 4.290870345719055

Epoch: 6| Step: 13
Training loss: 4.986259462964882
Validation loss: 4.2862036232651795

Epoch: 21| Step: 0
Training loss: 4.527876867810532
Validation loss: 4.280383354143285

Epoch: 6| Step: 1
Training loss: 3.02764867415964
Validation loss: 4.274842844225305

Epoch: 6| Step: 2
Training loss: 3.7395901197338244
Validation loss: 4.2695612183612965

Epoch: 6| Step: 3
Training loss: 4.552400881983396
Validation loss: 4.264436418803962

Epoch: 6| Step: 4
Training loss: 4.801618485973127
Validation loss: 4.259178479130431

Epoch: 6| Step: 5
Training loss: 4.372304903697641
Validation loss: 4.253774014447305

Epoch: 6| Step: 6
Training loss: 5.13351626049605
Validation loss: 4.248063880623289

Epoch: 6| Step: 7
Training loss: 5.07478105044552
Validation loss: 4.242765533613636

Epoch: 6| Step: 8
Training loss: 4.9909192595083764
Validation loss: 4.237289232921997

Epoch: 6| Step: 9
Training loss: 4.073840232521681
Validation loss: 4.231705785006868

Epoch: 6| Step: 10
Training loss: 4.45250047018658
Validation loss: 4.226433978239118

Epoch: 6| Step: 11
Training loss: 3.2553994735549545
Validation loss: 4.220615059473575

Epoch: 6| Step: 12
Training loss: 4.9637583484834025
Validation loss: 4.216361945278046

Epoch: 6| Step: 13
Training loss: 3.5968945884668337
Validation loss: 4.209985421337459

Epoch: 22| Step: 0
Training loss: 3.9204563447448058
Validation loss: 4.205015927079345

Epoch: 6| Step: 1
Training loss: 4.935153065388002
Validation loss: 4.1988809517717565

Epoch: 6| Step: 2
Training loss: 4.535467040249919
Validation loss: 4.194009191289243

Epoch: 6| Step: 3
Training loss: 4.091453548767679
Validation loss: 4.189232614874704

Epoch: 6| Step: 4
Training loss: 4.699200485079841
Validation loss: 4.183792104806481

Epoch: 6| Step: 5
Training loss: 4.576452372832124
Validation loss: 4.178071257978642

Epoch: 6| Step: 6
Training loss: 4.24865162721234
Validation loss: 4.1715820628746805

Epoch: 6| Step: 7
Training loss: 4.304447118898041
Validation loss: 4.166552020720934

Epoch: 6| Step: 8
Training loss: 4.172752259561755
Validation loss: 4.160581707279575

Epoch: 6| Step: 9
Training loss: 4.372811124102571
Validation loss: 4.15557823393125

Epoch: 6| Step: 10
Training loss: 4.665495634744745
Validation loss: 4.150752109717522

Epoch: 6| Step: 11
Training loss: 3.578785131603807
Validation loss: 4.14485115928293

Epoch: 6| Step: 12
Training loss: 4.111054632010858
Validation loss: 4.140197943011184

Epoch: 6| Step: 13
Training loss: 3.8101813347784437
Validation loss: 4.1345718571436585

Epoch: 23| Step: 0
Training loss: 3.9546691532380134
Validation loss: 4.129382801012935

Epoch: 6| Step: 1
Training loss: 4.394782110895385
Validation loss: 4.123935138161489

Epoch: 6| Step: 2
Training loss: 4.058521376580411
Validation loss: 4.11898029245069

Epoch: 6| Step: 3
Training loss: 4.261959974697995
Validation loss: 4.113168259023025

Epoch: 6| Step: 4
Training loss: 4.484452622436319
Validation loss: 4.108598333533801

Epoch: 6| Step: 5
Training loss: 4.7799760704558825
Validation loss: 4.102961264774315

Epoch: 6| Step: 6
Training loss: 4.419233955524658
Validation loss: 4.098240493419919

Epoch: 6| Step: 7
Training loss: 3.931086812475868
Validation loss: 4.093169736626926

Epoch: 6| Step: 8
Training loss: 4.244241515784831
Validation loss: 4.0870611470446345

Epoch: 6| Step: 9
Training loss: 4.329271736130545
Validation loss: 4.082127386539887

Epoch: 6| Step: 10
Training loss: 4.156827958155841
Validation loss: 4.0766815710120206

Epoch: 6| Step: 11
Training loss: 3.6676426946733804
Validation loss: 4.071463433772129

Epoch: 6| Step: 12
Training loss: 4.042149911417447
Validation loss: 4.066206664245257

Epoch: 6| Step: 13
Training loss: 4.362975899608148
Validation loss: 4.06155295459502

Epoch: 24| Step: 0
Training loss: 4.121521841657492
Validation loss: 4.056335545496437

Epoch: 6| Step: 1
Training loss: 4.035348154621099
Validation loss: 4.051801218968154

Epoch: 6| Step: 2
Training loss: 2.9346226336849908
Validation loss: 4.0469443021950235

Epoch: 6| Step: 3
Training loss: 4.61183230315054
Validation loss: 4.042151602266291

Epoch: 6| Step: 4
Training loss: 3.7250822634222045
Validation loss: 4.0368036856629566

Epoch: 6| Step: 5
Training loss: 4.772835101936617
Validation loss: 4.03213282339373

Epoch: 6| Step: 6
Training loss: 4.120351310492225
Validation loss: 4.028016126457089

Epoch: 6| Step: 7
Training loss: 4.021778186738447
Validation loss: 4.022232576454094

Epoch: 6| Step: 8
Training loss: 4.446753296892335
Validation loss: 4.017617430221918

Epoch: 6| Step: 9
Training loss: 4.145632842821524
Validation loss: 4.012188897198863

Epoch: 6| Step: 10
Training loss: 4.234883070303196
Validation loss: 4.006785458841742

Epoch: 6| Step: 11
Training loss: 3.593725320482514
Validation loss: 4.0024617289123805

Epoch: 6| Step: 12
Training loss: 4.529952922517475
Validation loss: 3.9984007260896255

Epoch: 6| Step: 13
Training loss: 4.541974445145146
Validation loss: 3.9921644475163074

Epoch: 25| Step: 0
Training loss: 3.664479051023906
Validation loss: 3.9866987047109492

Epoch: 6| Step: 1
Training loss: 4.272544753642143
Validation loss: 3.9817898809935013

Epoch: 6| Step: 2
Training loss: 5.119790942275137
Validation loss: 3.9772266165632435

Epoch: 6| Step: 3
Training loss: 3.9588038248559183
Validation loss: 3.970911813916416

Epoch: 6| Step: 4
Training loss: 4.12017725294715
Validation loss: 3.965546664362831

Epoch: 6| Step: 5
Training loss: 4.248382933905627
Validation loss: 3.9612582927668014

Epoch: 6| Step: 6
Training loss: 3.970267418759704
Validation loss: 3.9559169353884047

Epoch: 6| Step: 7
Training loss: 3.9058907305487716
Validation loss: 3.95058117063744

Epoch: 6| Step: 8
Training loss: 3.942476427858042
Validation loss: 3.944484591093207

Epoch: 6| Step: 9
Training loss: 2.7740857723317456
Validation loss: 3.939534484194625

Epoch: 6| Step: 10
Training loss: 4.235986156739983
Validation loss: 3.9339321461915215

Epoch: 6| Step: 11
Training loss: 4.011270618760184
Validation loss: 3.92952186432578

Epoch: 6| Step: 12
Training loss: 3.9511357907784452
Validation loss: 3.923487115361688

Epoch: 6| Step: 13
Training loss: 4.625318361614589
Validation loss: 3.918649955904248

Epoch: 26| Step: 0
Training loss: 4.041697603611998
Validation loss: 3.9130213933435116

Epoch: 6| Step: 1
Training loss: 3.9551042871434094
Validation loss: 3.90740101450323

Epoch: 6| Step: 2
Training loss: 3.9988728365644106
Validation loss: 3.9025853578338343

Epoch: 6| Step: 3
Training loss: 3.6880943983082264
Validation loss: 3.896977051043887

Epoch: 6| Step: 4
Training loss: 4.034459455508821
Validation loss: 3.8911041822601136

Epoch: 6| Step: 5
Training loss: 3.9758156185572378
Validation loss: 3.8865355175284337

Epoch: 6| Step: 6
Training loss: 4.676432654212478
Validation loss: 3.88166678581931

Epoch: 6| Step: 7
Training loss: 3.5077628830966843
Validation loss: 3.8753391445603484

Epoch: 6| Step: 8
Training loss: 3.962260068735383
Validation loss: 3.8707402719425823

Epoch: 6| Step: 9
Training loss: 3.999304472534954
Validation loss: 3.8650344763530406

Epoch: 6| Step: 10
Training loss: 4.689010173400139
Validation loss: 3.8595921381975176

Epoch: 6| Step: 11
Training loss: 4.193890833302087
Validation loss: 3.853959725812718

Epoch: 6| Step: 12
Training loss: 2.8182372174453207
Validation loss: 3.849469155709878

Epoch: 6| Step: 13
Training loss: 4.289661469874172
Validation loss: 3.8443345863311356

Epoch: 27| Step: 0
Training loss: 3.4382589716103955
Validation loss: 3.8394442376648974

Epoch: 6| Step: 1
Training loss: 4.479738184226702
Validation loss: 3.8349411881905424

Epoch: 6| Step: 2
Training loss: 4.324898188280189
Validation loss: 3.8294137251680516

Epoch: 6| Step: 3
Training loss: 4.063914360916249
Validation loss: 3.8237780744726

Epoch: 6| Step: 4
Training loss: 4.459269677825094
Validation loss: 3.8183181435260547

Epoch: 6| Step: 5
Training loss: 4.205035261262611
Validation loss: 3.8131154506855354

Epoch: 6| Step: 6
Training loss: 3.7149582453110153
Validation loss: 3.808570003598514

Epoch: 6| Step: 7
Training loss: 4.565561599865669
Validation loss: 3.803607053566426

Epoch: 6| Step: 8
Training loss: 3.0212040816176593
Validation loss: 3.7976913725711934

Epoch: 6| Step: 9
Training loss: 2.832960515644919
Validation loss: 3.7920689351969172

Epoch: 6| Step: 10
Training loss: 4.050190749209285
Validation loss: 3.786928332235985

Epoch: 6| Step: 11
Training loss: 3.4546846345762794
Validation loss: 3.78219327904604

Epoch: 6| Step: 12
Training loss: 4.0635534241183615
Validation loss: 3.7769927973275847

Epoch: 6| Step: 13
Training loss: 4.019130260458985
Validation loss: 3.771610510106455

Epoch: 28| Step: 0
Training loss: 4.630460042158486
Validation loss: 3.7675929539719726

Epoch: 6| Step: 1
Training loss: 3.967592325848983
Validation loss: 3.7629661089691964

Epoch: 6| Step: 2
Training loss: 3.894734613586157
Validation loss: 3.7576285017810718

Epoch: 6| Step: 3
Training loss: 3.4621924198300578
Validation loss: 3.7524217415209358

Epoch: 6| Step: 4
Training loss: 3.9906486635992677
Validation loss: 3.7470857952806487

Epoch: 6| Step: 5
Training loss: 3.4746991253691046
Validation loss: 3.742371004472645

Epoch: 6| Step: 6
Training loss: 3.6555375929381153
Validation loss: 3.7373502150580844

Epoch: 6| Step: 7
Training loss: 3.701546763253405
Validation loss: 3.7326919189155805

Epoch: 6| Step: 8
Training loss: 4.385675104036329
Validation loss: 3.728430066093699

Epoch: 6| Step: 9
Training loss: 3.727582428070054
Validation loss: 3.7230368516703223

Epoch: 6| Step: 10
Training loss: 4.128922880169915
Validation loss: 3.7177882513618377

Epoch: 6| Step: 11
Training loss: 3.7213174628157724
Validation loss: 3.7133159096227444

Epoch: 6| Step: 12
Training loss: 3.965683961735074
Validation loss: 3.708013192120088

Epoch: 6| Step: 13
Training loss: 3.2700425344144812
Validation loss: 3.7033682527335046

Epoch: 29| Step: 0
Training loss: 3.8983438056282163
Validation loss: 3.6987506123110725

Epoch: 6| Step: 1
Training loss: 3.8586022224958865
Validation loss: 3.6935578065497645

Epoch: 6| Step: 2
Training loss: 4.4902280033393245
Validation loss: 3.6890098584178586

Epoch: 6| Step: 3
Training loss: 3.4819965541412814
Validation loss: 3.683898460107157

Epoch: 6| Step: 4
Training loss: 4.209002501411084
Validation loss: 3.6796344124216964

Epoch: 6| Step: 5
Training loss: 4.062955390642094
Validation loss: 3.673946342884929

Epoch: 6| Step: 6
Training loss: 3.425079517415991
Validation loss: 3.6694583596726065

Epoch: 6| Step: 7
Training loss: 3.861205126550113
Validation loss: 3.664699236128682

Epoch: 6| Step: 8
Training loss: 3.6506876572069813
Validation loss: 3.659876098550653

Epoch: 6| Step: 9
Training loss: 3.481852897336969
Validation loss: 3.6558374242772023

Epoch: 6| Step: 10
Training loss: 3.676011243510356
Validation loss: 3.6502104432914724

Epoch: 6| Step: 11
Training loss: 3.7538274941335397
Validation loss: 3.6450715858675586

Epoch: 6| Step: 12
Training loss: 3.535470651376646
Validation loss: 3.640704567494075

Epoch: 6| Step: 13
Training loss: 3.731879641572017
Validation loss: 3.636363579287673

Epoch: 30| Step: 0
Training loss: 3.634630105404509
Validation loss: 3.6317648631512323

Epoch: 6| Step: 1
Training loss: 4.5005164379899965
Validation loss: 3.6273713303015276

Epoch: 6| Step: 2
Training loss: 3.723365291866981
Validation loss: 3.6224809093160975

Epoch: 6| Step: 3
Training loss: 2.795108684958419
Validation loss: 3.6174881262980914

Epoch: 6| Step: 4
Training loss: 3.7989798782751603
Validation loss: 3.613461315720192

Epoch: 6| Step: 5
Training loss: 3.724384175030193
Validation loss: 3.609170310641692

Epoch: 6| Step: 6
Training loss: 3.01819433938779
Validation loss: 3.60491742967635

Epoch: 6| Step: 7
Training loss: 4.737643180837446
Validation loss: 3.600154996113763

Epoch: 6| Step: 8
Training loss: 3.94460056037777
Validation loss: 3.596090820452745

Epoch: 6| Step: 9
Training loss: 3.283426634597264
Validation loss: 3.5910787080499174

Epoch: 6| Step: 10
Training loss: 3.246848706018713
Validation loss: 3.5859941465603424

Epoch: 6| Step: 11
Training loss: 3.77239141713706
Validation loss: 3.5818806002631955

Epoch: 6| Step: 12
Training loss: 4.077484665215598
Validation loss: 3.577363845292229

Epoch: 6| Step: 13
Training loss: 3.621703786810425
Validation loss: 3.5726836251792835

Epoch: 31| Step: 0
Training loss: 4.2066771518518165
Validation loss: 3.568200931744536

Epoch: 6| Step: 1
Training loss: 3.4828282502658108
Validation loss: 3.5638852688724816

Epoch: 6| Step: 2
Training loss: 3.6744065668443984
Validation loss: 3.5593427085583897

Epoch: 6| Step: 3
Training loss: 3.233338563134857
Validation loss: 3.5554153759131926

Epoch: 6| Step: 4
Training loss: 2.743288520116331
Validation loss: 3.5508928662597263

Epoch: 6| Step: 5
Training loss: 4.206523216409298
Validation loss: 3.546540030235902

Epoch: 6| Step: 6
Training loss: 4.062121799177437
Validation loss: 3.5419740580881953

Epoch: 6| Step: 7
Training loss: 4.098124489155165
Validation loss: 3.537525362478757

Epoch: 6| Step: 8
Training loss: 2.8099023478906515
Validation loss: 3.5335554850815494

Epoch: 6| Step: 9
Training loss: 3.799365698427592
Validation loss: 3.529202725805355

Epoch: 6| Step: 10
Training loss: 3.844149188890852
Validation loss: 3.5247066267502296

Epoch: 6| Step: 11
Training loss: 3.398024400180015
Validation loss: 3.52040936477439

Epoch: 6| Step: 12
Training loss: 4.152342601643186
Validation loss: 3.516002764715584

Epoch: 6| Step: 13
Training loss: 3.33918136363694
Validation loss: 3.511671632444265

Epoch: 32| Step: 0
Training loss: 4.095138197955175
Validation loss: 3.5077603455923536

Epoch: 6| Step: 1
Training loss: 2.7408066449055286
Validation loss: 3.5027400372137776

Epoch: 6| Step: 2
Training loss: 2.681125533053781
Validation loss: 3.498714937718335

Epoch: 6| Step: 3
Training loss: 2.729224886285378
Validation loss: 3.4945997857271696

Epoch: 6| Step: 4
Training loss: 3.795177943866447
Validation loss: 3.4909221225598244

Epoch: 6| Step: 5
Training loss: 3.103109363159243
Validation loss: 3.4872697173079477

Epoch: 6| Step: 6
Training loss: 4.317929595443971
Validation loss: 3.4833083408925383

Epoch: 6| Step: 7
Training loss: 3.7625458977520037
Validation loss: 3.4788732500136654

Epoch: 6| Step: 8
Training loss: 3.738497449787531
Validation loss: 3.4748045402096417

Epoch: 6| Step: 9
Training loss: 4.356779686401396
Validation loss: 3.4706026004393813

Epoch: 6| Step: 10
Training loss: 3.6721253654671204
Validation loss: 3.466494540073921

Epoch: 6| Step: 11
Training loss: 3.5196888133692363
Validation loss: 3.462247177331847

Epoch: 6| Step: 12
Training loss: 4.031936232244229
Validation loss: 3.457650212913804

Epoch: 6| Step: 13
Training loss: 3.522857821330444
Validation loss: 3.453101755009314

Epoch: 33| Step: 0
Training loss: 4.324344898510008
Validation loss: 3.448886093509116

Epoch: 6| Step: 1
Training loss: 3.2782647494919064
Validation loss: 3.4447098692964455

Epoch: 6| Step: 2
Training loss: 3.7824002872896165
Validation loss: 3.4400830360859045

Epoch: 6| Step: 3
Training loss: 2.802622765634911
Validation loss: 3.435802826808814

Epoch: 6| Step: 4
Training loss: 2.63830157208036
Validation loss: 3.4315328883261103

Epoch: 6| Step: 5
Training loss: 3.801802338271561
Validation loss: 3.4271805738820453

Epoch: 6| Step: 6
Training loss: 3.3118045004766574
Validation loss: 3.423278741305302

Epoch: 6| Step: 7
Training loss: 3.564746884299234
Validation loss: 3.41916845574651

Epoch: 6| Step: 8
Training loss: 4.207306210910749
Validation loss: 3.4150881648749296

Epoch: 6| Step: 9
Training loss: 3.374385353916122
Validation loss: 3.411336687942813

Epoch: 6| Step: 10
Training loss: 3.0853932347374893
Validation loss: 3.4067679428479454

Epoch: 6| Step: 11
Training loss: 3.9204823730446123
Validation loss: 3.4029135039026674

Epoch: 6| Step: 12
Training loss: 3.237493071677553
Validation loss: 3.398966348357848

Epoch: 6| Step: 13
Training loss: 4.048575614713975
Validation loss: 3.3950649788602263

Epoch: 34| Step: 0
Training loss: 3.171399611754489
Validation loss: 3.3910631637952724

Epoch: 6| Step: 1
Training loss: 3.0636750127486163
Validation loss: 3.3868953507727064

Epoch: 6| Step: 2
Training loss: 4.134838509041146
Validation loss: 3.383084010974044

Epoch: 6| Step: 3
Training loss: 3.866354619512189
Validation loss: 3.3792141636255404

Epoch: 6| Step: 4
Training loss: 3.629073386557446
Validation loss: 3.375202785098324

Epoch: 6| Step: 5
Training loss: 2.312687531161694
Validation loss: 3.3708906234066247

Epoch: 6| Step: 6
Training loss: 3.629831974144597
Validation loss: 3.3672628136138796

Epoch: 6| Step: 7
Training loss: 4.19681208287142
Validation loss: 3.3636424911444776

Epoch: 6| Step: 8
Training loss: 3.5731507753834078
Validation loss: 3.359499717585593

Epoch: 6| Step: 9
Training loss: 3.4746343516352156
Validation loss: 3.3553568621903858

Epoch: 6| Step: 10
Training loss: 4.118068304072534
Validation loss: 3.351403019926195

Epoch: 6| Step: 11
Training loss: 3.8064475874204295
Validation loss: 3.3470326884853865

Epoch: 6| Step: 12
Training loss: 2.1136969476558227
Validation loss: 3.342954796564862

Epoch: 6| Step: 13
Training loss: 3.23446861537037
Validation loss: 3.339474972596816

Epoch: 35| Step: 0
Training loss: 3.665104374251532
Validation loss: 3.3355900198598727

Epoch: 6| Step: 1
Training loss: 3.963998187621902
Validation loss: 3.3319348779346396

Epoch: 6| Step: 2
Training loss: 3.908534610231492
Validation loss: 3.327882760892787

Epoch: 6| Step: 3
Training loss: 2.599655964904637
Validation loss: 3.3236340951096057

Epoch: 6| Step: 4
Training loss: 3.827977267158955
Validation loss: 3.319913258204551

Epoch: 6| Step: 5
Training loss: 2.780355931281867
Validation loss: 3.3161541438948468

Epoch: 6| Step: 6
Training loss: 3.782087249120641
Validation loss: 3.3126328759563597

Epoch: 6| Step: 7
Training loss: 3.41021791382715
Validation loss: 3.3085495766817647

Epoch: 6| Step: 8
Training loss: 3.5155571825056118
Validation loss: 3.304711969283517

Epoch: 6| Step: 9
Training loss: 3.631375067390577
Validation loss: 3.3013693020462966

Epoch: 6| Step: 10
Training loss: 2.882912073250713
Validation loss: 3.297256688687084

Epoch: 6| Step: 11
Training loss: 3.279379465904561
Validation loss: 3.2935294099757795

Epoch: 6| Step: 12
Training loss: 3.483110549105503
Validation loss: 3.2898819265850827

Epoch: 6| Step: 13
Training loss: 3.244851068449968
Validation loss: 3.286206392444096

Epoch: 36| Step: 0
Training loss: 3.301147440624798
Validation loss: 3.2825743893757355

Epoch: 6| Step: 1
Training loss: 3.385309086581953
Validation loss: 3.279034814441726

Epoch: 6| Step: 2
Training loss: 3.3321842279233005
Validation loss: 3.275087679594421

Epoch: 6| Step: 3
Training loss: 3.1864067147446855
Validation loss: 3.2716628478603322

Epoch: 6| Step: 4
Training loss: 3.164956651725234
Validation loss: 3.2683817591006474

Epoch: 6| Step: 5
Training loss: 3.094626100353445
Validation loss: 3.2649947779693047

Epoch: 6| Step: 6
Training loss: 3.1013249611395777
Validation loss: 3.261327358374759

Epoch: 6| Step: 7
Training loss: 3.4208428206565844
Validation loss: 3.257767528342459

Epoch: 6| Step: 8
Training loss: 3.969508061137617
Validation loss: 3.2542364434557247

Epoch: 6| Step: 9
Training loss: 3.2386054784345513
Validation loss: 3.2506965722134105

Epoch: 6| Step: 10
Training loss: 4.047770633049546
Validation loss: 3.2473603924507746

Epoch: 6| Step: 11
Training loss: 3.400059104854489
Validation loss: 3.2432909455099095

Epoch: 6| Step: 12
Training loss: 3.03280065142102
Validation loss: 3.239807873433654

Epoch: 6| Step: 13
Training loss: 3.727992776986521
Validation loss: 3.2364759885275514

Epoch: 37| Step: 0
Training loss: 3.545849622486561
Validation loss: 3.2324768369060646

Epoch: 6| Step: 1
Training loss: 3.307010059933145
Validation loss: 3.228894121707364

Epoch: 6| Step: 2
Training loss: 3.9707182561658327
Validation loss: 3.225366760315921

Epoch: 6| Step: 3
Training loss: 3.4092152139369207
Validation loss: 3.2216343224630752

Epoch: 6| Step: 4
Training loss: 3.421366335891279
Validation loss: 3.2180558230764422

Epoch: 6| Step: 5
Training loss: 3.3449734740067942
Validation loss: 3.21413836923581

Epoch: 6| Step: 6
Training loss: 3.3279721220481093
Validation loss: 3.210385846768394

Epoch: 6| Step: 7
Training loss: 3.435401414629544
Validation loss: 3.2065386438357524

Epoch: 6| Step: 8
Training loss: 2.4653347874408986
Validation loss: 3.2028492754605478

Epoch: 6| Step: 9
Training loss: 3.636949247109424
Validation loss: 3.199364155440902

Epoch: 6| Step: 10
Training loss: 3.2949285320583312
Validation loss: 3.196035935910863

Epoch: 6| Step: 11
Training loss: 2.9106761084016237
Validation loss: 3.1924915001541216

Epoch: 6| Step: 12
Training loss: 3.6438312083072386
Validation loss: 3.189323502083152

Epoch: 6| Step: 13
Training loss: 2.9521011111510025
Validation loss: 3.185962680270006

Epoch: 38| Step: 0
Training loss: 3.0039392675363588
Validation loss: 3.182751990123735

Epoch: 6| Step: 1
Training loss: 3.3455497675375137
Validation loss: 3.1794605970849092

Epoch: 6| Step: 2
Training loss: 3.532204963042199
Validation loss: 3.176393707139038

Epoch: 6| Step: 3
Training loss: 3.4784125388290565
Validation loss: 3.1734250420119703

Epoch: 6| Step: 4
Training loss: 3.572116521244123
Validation loss: 3.1698479776001216

Epoch: 6| Step: 5
Training loss: 3.465814809184839
Validation loss: 3.1670500414490004

Epoch: 6| Step: 6
Training loss: 3.4521797923346225
Validation loss: 3.163289720645315

Epoch: 6| Step: 7
Training loss: 3.5014322620089358
Validation loss: 3.160259105264379

Epoch: 6| Step: 8
Training loss: 3.256055179559527
Validation loss: 3.156960272107155

Epoch: 6| Step: 9
Training loss: 3.3801161523758685
Validation loss: 3.1539387061630544

Epoch: 6| Step: 10
Training loss: 3.3148789410472936
Validation loss: 3.150245254798401

Epoch: 6| Step: 11
Training loss: 3.1368745643993927
Validation loss: 3.147146510887346

Epoch: 6| Step: 12
Training loss: 3.1748279675111557
Validation loss: 3.143935197212507

Epoch: 6| Step: 13
Training loss: 2.4704798179100114
Validation loss: 3.1402992977968593

Epoch: 39| Step: 0
Training loss: 2.5662275045963248
Validation loss: 3.1372557323351864

Epoch: 6| Step: 1
Training loss: 3.219677541473577
Validation loss: 3.1338263161040736

Epoch: 6| Step: 2
Training loss: 3.9998609995532433
Validation loss: 3.1308004715221824

Epoch: 6| Step: 3
Training loss: 2.7819810399465905
Validation loss: 3.1274643018938133

Epoch: 6| Step: 4
Training loss: 3.189282236934921
Validation loss: 3.1239839555114246

Epoch: 6| Step: 5
Training loss: 3.0087321671328673
Validation loss: 3.1207913859815903

Epoch: 6| Step: 6
Training loss: 3.9551569725936853
Validation loss: 3.1176195474649884

Epoch: 6| Step: 7
Training loss: 3.615913314995604
Validation loss: 3.1143220560124925

Epoch: 6| Step: 8
Training loss: 3.333100088224192
Validation loss: 3.110984832797001

Epoch: 6| Step: 9
Training loss: 3.4755789404871265
Validation loss: 3.1076439363141453

Epoch: 6| Step: 10
Training loss: 2.9760898805602043
Validation loss: 3.1046682161686774

Epoch: 6| Step: 11
Training loss: 3.6509700374497007
Validation loss: 3.1012061080071445

Epoch: 6| Step: 12
Training loss: 3.2497326667582223
Validation loss: 3.0981943850442852

Epoch: 6| Step: 13
Training loss: 2.0567985104549895
Validation loss: 3.0948269700269067

Epoch: 40| Step: 0
Training loss: 3.3288736710188536
Validation loss: 3.0920665540618173

Epoch: 6| Step: 1
Training loss: 2.553233908451372
Validation loss: 3.0892048724638124

Epoch: 6| Step: 2
Training loss: 3.60703561777202
Validation loss: 3.086583091711181

Epoch: 6| Step: 3
Training loss: 3.037959430627116
Validation loss: 3.08372531581525

Epoch: 6| Step: 4
Training loss: 3.494338224569217
Validation loss: 3.0811949652140944

Epoch: 6| Step: 5
Training loss: 2.9753572521964893
Validation loss: 3.0784096755630133

Epoch: 6| Step: 6
Training loss: 2.713468485972379
Validation loss: 3.075332821044725

Epoch: 6| Step: 7
Training loss: 3.446654735178067
Validation loss: 3.0722730749273945

Epoch: 6| Step: 8
Training loss: 3.4894462596484574
Validation loss: 3.0692490668468753

Epoch: 6| Step: 9
Training loss: 3.6598646766368215
Validation loss: 3.066269313079105

Epoch: 6| Step: 10
Training loss: 3.0131512709562127
Validation loss: 3.063134938548046

Epoch: 6| Step: 11
Training loss: 3.6975124657596816
Validation loss: 3.060229432879482

Epoch: 6| Step: 12
Training loss: 2.9531222550313796
Validation loss: 3.0570802545100877

Epoch: 6| Step: 13
Training loss: 2.760807820563126
Validation loss: 3.0538334087442673

Epoch: 41| Step: 0
Training loss: 2.831758304465561
Validation loss: 3.051085811428908

Epoch: 6| Step: 1
Training loss: 3.4378783971419287
Validation loss: 3.0486271702587957

Epoch: 6| Step: 2
Training loss: 2.6740409593605046
Validation loss: 3.0458737154543702

Epoch: 6| Step: 3
Training loss: 2.7970559684982823
Validation loss: 3.043207109458299

Epoch: 6| Step: 4
Training loss: 2.715793185628411
Validation loss: 3.0409246785132025

Epoch: 6| Step: 5
Training loss: 3.2013622364654992
Validation loss: 3.0382352747519925

Epoch: 6| Step: 6
Training loss: 3.167641807458875
Validation loss: 3.0356779758835697

Epoch: 6| Step: 7
Training loss: 3.1759504622720662
Validation loss: 3.0331948454826367

Epoch: 6| Step: 8
Training loss: 3.6384116062814598
Validation loss: 3.0307901040127647

Epoch: 6| Step: 9
Training loss: 3.4305546735246755
Validation loss: 3.0281351347164156

Epoch: 6| Step: 10
Training loss: 3.5799378712553573
Validation loss: 3.02523645049618

Epoch: 6| Step: 11
Training loss: 2.717766594886733
Validation loss: 3.022435800245496

Epoch: 6| Step: 12
Training loss: 3.4358041915256003
Validation loss: 3.0194854272258187

Epoch: 6| Step: 13
Training loss: 3.4002722687256743
Validation loss: 3.016587586358682

Epoch: 42| Step: 0
Training loss: 2.9623952211288405
Validation loss: 3.0136921540569195

Epoch: 6| Step: 1
Training loss: 3.075735646152746
Validation loss: 3.011043368954275

Epoch: 6| Step: 2
Training loss: 3.347349707012274
Validation loss: 3.0081804455048053

Epoch: 6| Step: 3
Training loss: 3.276794751348986
Validation loss: 3.0055986985004863

Epoch: 6| Step: 4
Training loss: 3.158014238756681
Validation loss: 3.0030568761174488

Epoch: 6| Step: 5
Training loss: 2.6832984195436027
Validation loss: 3.0006480841505865

Epoch: 6| Step: 6
Training loss: 3.000066438575029
Validation loss: 2.9976724231327756

Epoch: 6| Step: 7
Training loss: 3.437421346978371
Validation loss: 2.9953543137252057

Epoch: 6| Step: 8
Training loss: 3.3697841040847245
Validation loss: 2.992880572469994

Epoch: 6| Step: 9
Training loss: 2.9747674290125383
Validation loss: 2.990191040123717

Epoch: 6| Step: 10
Training loss: 2.8847110771543876
Validation loss: 2.987730739307242

Epoch: 6| Step: 11
Training loss: 3.0666726160682796
Validation loss: 2.9850067082579446

Epoch: 6| Step: 12
Training loss: 3.564121747224767
Validation loss: 2.98219741812711

Epoch: 6| Step: 13
Training loss: 3.006560463844675
Validation loss: 2.979753440923438

Epoch: 43| Step: 0
Training loss: 2.300875754107519
Validation loss: 2.9768323400541448

Epoch: 6| Step: 1
Training loss: 2.558296197959282
Validation loss: 2.974093343383677

Epoch: 6| Step: 2
Training loss: 3.6211245637341865
Validation loss: 2.9718674069644213

Epoch: 6| Step: 3
Training loss: 3.459527893271527
Validation loss: 2.9691622932980737

Epoch: 6| Step: 4
Training loss: 2.8620641184941302
Validation loss: 2.966538298076269

Epoch: 6| Step: 5
Training loss: 2.878871135937485
Validation loss: 2.9639287247781763

Epoch: 6| Step: 6
Training loss: 3.3225756303777465
Validation loss: 2.9613200723095523

Epoch: 6| Step: 7
Training loss: 3.2878581036951973
Validation loss: 2.958828405585829

Epoch: 6| Step: 8
Training loss: 3.382631786003705
Validation loss: 2.9563066408566194

Epoch: 6| Step: 9
Training loss: 2.8088939012946836
Validation loss: 2.95340804773707

Epoch: 6| Step: 10
Training loss: 2.774960983277011
Validation loss: 2.951453218444656

Epoch: 6| Step: 11
Training loss: 3.8422704811024393
Validation loss: 2.9484032048567625

Epoch: 6| Step: 12
Training loss: 2.8819242933761227
Validation loss: 2.945751167637065

Epoch: 6| Step: 13
Training loss: 3.0660956936190362
Validation loss: 2.943181664312985

Epoch: 44| Step: 0
Training loss: 3.079823291713132
Validation loss: 2.9406989920344313

Epoch: 6| Step: 1
Training loss: 3.681101533392621
Validation loss: 2.9384382521588375

Epoch: 6| Step: 2
Training loss: 3.3996281083862083
Validation loss: 2.9355990434191708

Epoch: 6| Step: 3
Training loss: 3.0768011509139854
Validation loss: 2.933157748328027

Epoch: 6| Step: 4
Training loss: 2.963629553128027
Validation loss: 2.930513745251225

Epoch: 6| Step: 5
Training loss: 2.993429936957868
Validation loss: 2.9282261844876887

Epoch: 6| Step: 6
Training loss: 2.8690959181655598
Validation loss: 2.9257909743211505

Epoch: 6| Step: 7
Training loss: 2.818147541631958
Validation loss: 2.923373377042549

Epoch: 6| Step: 8
Training loss: 3.0730241832135383
Validation loss: 2.9214195807719796

Epoch: 6| Step: 9
Training loss: 2.951472713302321
Validation loss: 2.9191934584958914

Epoch: 6| Step: 10
Training loss: 2.648501415100353
Validation loss: 2.916991152197534

Epoch: 6| Step: 11
Training loss: 3.3211266809669087
Validation loss: 2.914405850252564

Epoch: 6| Step: 12
Training loss: 3.18746110481018
Validation loss: 2.9120121373999566

Epoch: 6| Step: 13
Training loss: 2.737577383262765
Validation loss: 2.909622095570835

Epoch: 45| Step: 0
Training loss: 2.29383742662171
Validation loss: 2.9073586485711926

Epoch: 6| Step: 1
Training loss: 3.415258970480748
Validation loss: 2.905323167904207

Epoch: 6| Step: 2
Training loss: 2.914481043480574
Validation loss: 2.903140903698982

Epoch: 6| Step: 3
Training loss: 2.8258565161281015
Validation loss: 2.9008967136807557

Epoch: 6| Step: 4
Training loss: 3.1381475441492706
Validation loss: 2.898936897844646

Epoch: 6| Step: 5
Training loss: 3.1345109782669414
Validation loss: 2.896358680184235

Epoch: 6| Step: 6
Training loss: 2.608308311399259
Validation loss: 2.894334931436382

Epoch: 6| Step: 7
Training loss: 3.0045113022392185
Validation loss: 2.89206067423226

Epoch: 6| Step: 8
Training loss: 2.670990022138656
Validation loss: 2.8901194774785615

Epoch: 6| Step: 9
Training loss: 3.4074514527476443
Validation loss: 2.8881746233394785

Epoch: 6| Step: 10
Training loss: 2.9676688383899217
Validation loss: 2.8861766232139288

Epoch: 6| Step: 11
Training loss: 4.133998880844814
Validation loss: 2.884194212676068

Epoch: 6| Step: 12
Training loss: 2.641803546171157
Validation loss: 2.881932580047657

Epoch: 6| Step: 13
Training loss: 2.9080129219198345
Validation loss: 2.879589812287512

Epoch: 46| Step: 0
Training loss: 2.9190441432645633
Validation loss: 2.8772389221844348

Epoch: 6| Step: 1
Training loss: 2.733984957532934
Validation loss: 2.8755679260683378

Epoch: 6| Step: 2
Training loss: 3.346101308489449
Validation loss: 2.8732966477190374

Epoch: 6| Step: 3
Training loss: 2.9303288685977225
Validation loss: 2.8711565836909876

Epoch: 6| Step: 4
Training loss: 2.6075880133458527
Validation loss: 2.869384464421967

Epoch: 6| Step: 5
Training loss: 3.556945486059057
Validation loss: 2.867686132161955

Epoch: 6| Step: 6
Training loss: 3.189176828969436
Validation loss: 2.8654291372363887

Epoch: 6| Step: 7
Training loss: 2.862461612893792
Validation loss: 2.8631294491276775

Epoch: 6| Step: 8
Training loss: 3.4155153024827953
Validation loss: 2.8607341683684844

Epoch: 6| Step: 9
Training loss: 3.084298850436467
Validation loss: 2.858570145416317

Epoch: 6| Step: 10
Training loss: 2.7082932298087536
Validation loss: 2.8562778034707286

Epoch: 6| Step: 11
Training loss: 2.9348835757297733
Validation loss: 2.8544980759352754

Epoch: 6| Step: 12
Training loss: 2.68592619835305
Validation loss: 2.852287012317978

Epoch: 6| Step: 13
Training loss: 2.9326578366729916
Validation loss: 2.850357818449574

Epoch: 47| Step: 0
Training loss: 2.9716955730453654
Validation loss: 2.848264834132964

Epoch: 6| Step: 1
Training loss: 3.4160910020628843
Validation loss: 2.845813631256698

Epoch: 6| Step: 2
Training loss: 2.7057441855421613
Validation loss: 2.8441910873230523

Epoch: 6| Step: 3
Training loss: 2.9746706099040208
Validation loss: 2.841912357042003

Epoch: 6| Step: 4
Training loss: 2.8391750550619688
Validation loss: 2.8399184967936284

Epoch: 6| Step: 5
Training loss: 2.9533826927497873
Validation loss: 2.8380985485760433

Epoch: 6| Step: 6
Training loss: 2.880511432907758
Validation loss: 2.836170968077813

Epoch: 6| Step: 7
Training loss: 2.956920465487902
Validation loss: 2.83463032338998

Epoch: 6| Step: 8
Training loss: 3.2097054226909454
Validation loss: 2.832835242489958

Epoch: 6| Step: 9
Training loss: 3.485663889804286
Validation loss: 2.831977678866524

Epoch: 6| Step: 10
Training loss: 2.6512272638032175
Validation loss: 2.8297856269447506

Epoch: 6| Step: 11
Training loss: 3.588447971901766
Validation loss: 2.8284071189713456

Epoch: 6| Step: 12
Training loss: 2.419533662237456
Validation loss: 2.8257862769164706

Epoch: 6| Step: 13
Training loss: 2.329276600144446
Validation loss: 2.8237614839660017

Epoch: 48| Step: 0
Training loss: 2.7541446663751623
Validation loss: 2.8216989900961145

Epoch: 6| Step: 1
Training loss: 3.2902876142107265
Validation loss: 2.8199002712281955

Epoch: 6| Step: 2
Training loss: 2.9485345918470838
Validation loss: 2.8181059880509665

Epoch: 6| Step: 3
Training loss: 2.548150240614292
Validation loss: 2.8162908931674924

Epoch: 6| Step: 4
Training loss: 2.7840392452959684
Validation loss: 2.814189234055603

Epoch: 6| Step: 5
Training loss: 2.9147594937651613
Validation loss: 2.8128051168452735

Epoch: 6| Step: 6
Training loss: 3.0158031984165663
Validation loss: 2.8100574200647346

Epoch: 6| Step: 7
Training loss: 2.8508037236949013
Validation loss: 2.8089549717049214

Epoch: 6| Step: 8
Training loss: 2.778438733196832
Validation loss: 2.807486197691608

Epoch: 6| Step: 9
Training loss: 3.0697163786952504
Validation loss: 2.8049657288722636

Epoch: 6| Step: 10
Training loss: 3.272915527680262
Validation loss: 2.8036451182517164

Epoch: 6| Step: 11
Training loss: 2.6331788665600255
Validation loss: 2.8016210030445077

Epoch: 6| Step: 12
Training loss: 3.4835647530470673
Validation loss: 2.800037492773534

Epoch: 6| Step: 13
Training loss: 2.821226949196191
Validation loss: 2.7985991521621356

Epoch: 49| Step: 0
Training loss: 3.2882147124508494
Validation loss: 2.7963012589889438

Epoch: 6| Step: 1
Training loss: 2.9312601573255685
Validation loss: 2.7950915683402755

Epoch: 6| Step: 2
Training loss: 3.475816007900356
Validation loss: 2.7921196774024986

Epoch: 6| Step: 3
Training loss: 3.4823460166828952
Validation loss: 2.7908385789908676

Epoch: 6| Step: 4
Training loss: 2.7090649912527116
Validation loss: 2.789645521417077

Epoch: 6| Step: 5
Training loss: 2.651014126757637
Validation loss: 2.7871897275302526

Epoch: 6| Step: 6
Training loss: 2.7867601653054224
Validation loss: 2.7856391391818978

Epoch: 6| Step: 7
Training loss: 3.1788544934461735
Validation loss: 2.783130381564901

Epoch: 6| Step: 8
Training loss: 3.025563204961079
Validation loss: 2.781607572736572

Epoch: 6| Step: 9
Training loss: 2.4473817497887858
Validation loss: 2.7795189603216155

Epoch: 6| Step: 10
Training loss: 3.159640464609951
Validation loss: 2.777361519672189

Epoch: 6| Step: 11
Training loss: 2.9557099508369493
Validation loss: 2.776043866259841

Epoch: 6| Step: 12
Training loss: 2.2228180033683893
Validation loss: 2.7738396294856673

Epoch: 6| Step: 13
Training loss: 2.3158196000713143
Validation loss: 2.7723898758156262

Epoch: 50| Step: 0
Training loss: 2.8697033064867075
Validation loss: 2.7695622552266945

Epoch: 6| Step: 1
Training loss: 3.2948278064335703
Validation loss: 2.770436743747043

Epoch: 6| Step: 2
Training loss: 3.309331854376
Validation loss: 2.7670215306337616

Epoch: 6| Step: 3
Training loss: 2.53468974425064
Validation loss: 2.76604946923674

Epoch: 6| Step: 4
Training loss: 2.502221932067689
Validation loss: 2.765309978582677

Epoch: 6| Step: 5
Training loss: 2.9568982113298263
Validation loss: 2.7638615878965003

Epoch: 6| Step: 6
Training loss: 3.0787794317897825
Validation loss: 2.761706145822988

Epoch: 6| Step: 7
Training loss: 2.5129187581310783
Validation loss: 2.7600486989954023

Epoch: 6| Step: 8
Training loss: 2.811775792447653
Validation loss: 2.756936010566151

Epoch: 6| Step: 9
Training loss: 3.1520702522310633
Validation loss: 2.756508898235342

Epoch: 6| Step: 10
Training loss: 2.9315171695462126
Validation loss: 2.752330933684583

Epoch: 6| Step: 11
Training loss: 2.7468520266599668
Validation loss: 2.751085052860694

Epoch: 6| Step: 12
Training loss: 2.844231533320858
Validation loss: 2.7500021529911747

Epoch: 6| Step: 13
Training loss: 2.956955942784877
Validation loss: 2.7477244730486063

Epoch: 51| Step: 0
Training loss: 2.7978801093384416
Validation loss: 2.7483261389452167

Epoch: 6| Step: 1
Training loss: 2.764875321155445
Validation loss: 2.7467213795669214

Epoch: 6| Step: 2
Training loss: 3.099241680794139
Validation loss: 2.7439805677623013

Epoch: 6| Step: 3
Training loss: 3.354664041040037
Validation loss: 2.7436789779334347

Epoch: 6| Step: 4
Training loss: 3.0431080542668623
Validation loss: 2.7424215740348625

Epoch: 6| Step: 5
Training loss: 2.730537642890935
Validation loss: 2.7391169239243647

Epoch: 6| Step: 6
Training loss: 3.2898116538448576
Validation loss: 2.7374099600512927

Epoch: 6| Step: 7
Training loss: 2.983722076093577
Validation loss: 2.7359371990651753

Epoch: 6| Step: 8
Training loss: 3.103107519186979
Validation loss: 2.7354783801990425

Epoch: 6| Step: 9
Training loss: 2.3902483874098994
Validation loss: 2.7334586506315257

Epoch: 6| Step: 10
Training loss: 2.841418305970093
Validation loss: 2.7323339783076457

Epoch: 6| Step: 11
Training loss: 1.91738130542781
Validation loss: 2.730366033205584

Epoch: 6| Step: 12
Training loss: 3.0231266305480182
Validation loss: 2.728955695076285

Epoch: 6| Step: 13
Training loss: 2.6536031662814845
Validation loss: 2.7265724684743455

Epoch: 52| Step: 0
Training loss: 2.361040247682783
Validation loss: 2.725197203380412

Epoch: 6| Step: 1
Training loss: 2.4264898816189953
Validation loss: 2.7252329414733607

Epoch: 6| Step: 2
Training loss: 2.800297183205743
Validation loss: 2.733766411765625

Epoch: 6| Step: 3
Training loss: 3.0853413066293904
Validation loss: 2.7288953389028743

Epoch: 6| Step: 4
Training loss: 2.8055021755146843
Validation loss: 2.718751607726342

Epoch: 6| Step: 5
Training loss: 2.983959228952988
Validation loss: 2.7170073818227722

Epoch: 6| Step: 6
Training loss: 2.4339537622139535
Validation loss: 2.7166470546443557

Epoch: 6| Step: 7
Training loss: 2.8581438593072264
Validation loss: 2.715584486939746

Epoch: 6| Step: 8
Training loss: 3.1001342252159776
Validation loss: 2.7144877175164317

Epoch: 6| Step: 9
Training loss: 3.4119441882775425
Validation loss: 2.714813876273549

Epoch: 6| Step: 10
Training loss: 2.9277135276942894
Validation loss: 2.7127889417757602

Epoch: 6| Step: 11
Training loss: 2.5139004500411875
Validation loss: 2.7096948331483555

Epoch: 6| Step: 12
Training loss: 3.234777010936896
Validation loss: 2.7092689267535497

Epoch: 6| Step: 13
Training loss: 2.8784414508822698
Validation loss: 2.7079518905425033

Epoch: 53| Step: 0
Training loss: 2.868251343315752
Validation loss: 2.706199925688422

Epoch: 6| Step: 1
Training loss: 3.1423442936137556
Validation loss: 2.703412373285316

Epoch: 6| Step: 2
Training loss: 2.098612022385679
Validation loss: 2.7028489311821855

Epoch: 6| Step: 3
Training loss: 2.8835934457336942
Validation loss: 2.7007074000412206

Epoch: 6| Step: 4
Training loss: 2.5447494905161925
Validation loss: 2.6978186414037393

Epoch: 6| Step: 5
Training loss: 2.734408133169794
Validation loss: 2.695544969051874

Epoch: 6| Step: 6
Training loss: 2.386252399841029
Validation loss: 2.6958956912187313

Epoch: 6| Step: 7
Training loss: 2.716330141938281
Validation loss: 2.6934499451049936

Epoch: 6| Step: 8
Training loss: 3.3224438816265383
Validation loss: 2.69385950112287

Epoch: 6| Step: 9
Training loss: 2.259433784617087
Validation loss: 2.692770572929309

Epoch: 6| Step: 10
Training loss: 3.3132870296975643
Validation loss: 2.691110477519186

Epoch: 6| Step: 11
Training loss: 3.287564115297955
Validation loss: 2.6881473707840917

Epoch: 6| Step: 12
Training loss: 3.0640849469114997
Validation loss: 2.684694230175526

Epoch: 6| Step: 13
Training loss: 2.7082182248425144
Validation loss: 2.6841926776281406

Epoch: 54| Step: 0
Training loss: 3.1661834013418453
Validation loss: 2.682336519230513

Epoch: 6| Step: 1
Training loss: 2.957622675540127
Validation loss: 2.6819462871715296

Epoch: 6| Step: 2
Training loss: 3.060268601241264
Validation loss: 2.6834792733242265

Epoch: 6| Step: 3
Training loss: 2.4606240693570935
Validation loss: 2.6856149375844796

Epoch: 6| Step: 4
Training loss: 2.8891218246385075
Validation loss: 2.7042654053736297

Epoch: 6| Step: 5
Training loss: 2.900912397827305
Validation loss: 2.6961190311584473

Epoch: 6| Step: 6
Training loss: 2.2448014497359385
Validation loss: 2.6954187123476685

Epoch: 6| Step: 7
Training loss: 2.9461636312689614
Validation loss: 2.6937274049783397

Epoch: 6| Step: 8
Training loss: 2.5472164744242143
Validation loss: 2.6759359273267744

Epoch: 6| Step: 9
Training loss: 2.9374538986159666
Validation loss: 2.6743317996502536

Epoch: 6| Step: 10
Training loss: 2.7012878525467987
Validation loss: 2.673188063494761

Epoch: 6| Step: 11
Training loss: 2.6672031538447527
Validation loss: 2.682527288562949

Epoch: 6| Step: 12
Training loss: 2.7644209315058825
Validation loss: 2.6991705073578296

Epoch: 6| Step: 13
Training loss: 3.141012139443564
Validation loss: 2.716835063754597

Epoch: 55| Step: 0
Training loss: 2.123551267561467
Validation loss: 2.7168004291656525

Epoch: 6| Step: 1
Training loss: 3.207001512879793
Validation loss: 2.6934976410870504

Epoch: 6| Step: 2
Training loss: 2.730005402332592
Validation loss: 2.677364205990125

Epoch: 6| Step: 3
Training loss: 3.15263401288149
Validation loss: 2.6692162385645255

Epoch: 6| Step: 4
Training loss: 3.13133071521273
Validation loss: 2.6669306823052583

Epoch: 6| Step: 5
Training loss: 3.275668674375085
Validation loss: 2.6640988642950547

Epoch: 6| Step: 6
Training loss: 2.663729381150311
Validation loss: 2.662236084491548

Epoch: 6| Step: 7
Training loss: 2.729124597899813
Validation loss: 2.663696487655412

Epoch: 6| Step: 8
Training loss: 3.1150595684693263
Validation loss: 2.6665277196527155

Epoch: 6| Step: 9
Training loss: 2.793913824069981
Validation loss: 2.6611210310088973

Epoch: 6| Step: 10
Training loss: 3.1796512086189077
Validation loss: 2.656467010010792

Epoch: 6| Step: 11
Training loss: 2.2841336562471186
Validation loss: 2.6548000641881866

Epoch: 6| Step: 12
Training loss: 2.074630897509453
Validation loss: 2.654367370642409

Epoch: 6| Step: 13
Training loss: 2.586231042157287
Validation loss: 2.6541056336033333

Epoch: 56| Step: 0
Training loss: 2.4315787556448925
Validation loss: 2.653221251138813

Epoch: 6| Step: 1
Training loss: 2.8928843972942295
Validation loss: 2.6529194165465135

Epoch: 6| Step: 2
Training loss: 3.2076403542403065
Validation loss: 2.652864070862452

Epoch: 6| Step: 3
Training loss: 2.956133566372035
Validation loss: 2.651455235526785

Epoch: 6| Step: 4
Training loss: 2.8136211280001278
Validation loss: 2.6491600108855535

Epoch: 6| Step: 5
Training loss: 2.7898549896073646
Validation loss: 2.648331811647196

Epoch: 6| Step: 6
Training loss: 2.6067969098281045
Validation loss: 2.647336477789656

Epoch: 6| Step: 7
Training loss: 2.875903112124459
Validation loss: 2.6447887811773985

Epoch: 6| Step: 8
Training loss: 3.021743181385216
Validation loss: 2.6435305168363055

Epoch: 6| Step: 9
Training loss: 2.4291278157690708
Validation loss: 2.640348852642297

Epoch: 6| Step: 10
Training loss: 2.9751902543468622
Validation loss: 2.639717924384454

Epoch: 6| Step: 11
Training loss: 2.3911508218764044
Validation loss: 2.6386330971361454

Epoch: 6| Step: 12
Training loss: 2.8166296416638734
Validation loss: 2.6371390151423095

Epoch: 6| Step: 13
Training loss: 2.678788014692677
Validation loss: 2.6371993316856295

Epoch: 57| Step: 0
Training loss: 2.4711090598776346
Validation loss: 2.6343381179997167

Epoch: 6| Step: 1
Training loss: 2.8410542884481575
Validation loss: 2.632466083077475

Epoch: 6| Step: 2
Training loss: 3.1069089992568313
Validation loss: 2.631485163916529

Epoch: 6| Step: 3
Training loss: 2.6184852229530504
Validation loss: 2.6305230333259657

Epoch: 6| Step: 4
Training loss: 2.984080355014849
Validation loss: 2.6309824241931703

Epoch: 6| Step: 5
Training loss: 2.8667550657378444
Validation loss: 2.6308492704224338

Epoch: 6| Step: 6
Training loss: 2.702236845488606
Validation loss: 2.6286994055382067

Epoch: 6| Step: 7
Training loss: 2.7641875416110526
Validation loss: 2.627029996406639

Epoch: 6| Step: 8
Training loss: 2.3561062432509456
Validation loss: 2.6270518533714804

Epoch: 6| Step: 9
Training loss: 2.583552433279919
Validation loss: 2.6248160555135627

Epoch: 6| Step: 10
Training loss: 3.0857554707982815
Validation loss: 2.6237783465430007

Epoch: 6| Step: 11
Training loss: 2.5238163430429412
Validation loss: 2.6226946835035543

Epoch: 6| Step: 12
Training loss: 2.4840868896791983
Validation loss: 2.6222983338693107

Epoch: 6| Step: 13
Training loss: 3.177410538297158
Validation loss: 2.6241752221904933

Epoch: 58| Step: 0
Training loss: 2.7332751868816465
Validation loss: 2.623507090303109

Epoch: 6| Step: 1
Training loss: 2.26453915587719
Validation loss: 2.6170951864938483

Epoch: 6| Step: 2
Training loss: 2.5704629500397114
Validation loss: 2.61769038441024

Epoch: 6| Step: 3
Training loss: 3.1002594254753943
Validation loss: 2.61541920607766

Epoch: 6| Step: 4
Training loss: 2.4674122723074072
Validation loss: 2.617429944096462

Epoch: 6| Step: 5
Training loss: 3.0999161124415617
Validation loss: 2.6163794033844607

Epoch: 6| Step: 6
Training loss: 2.765014581072751
Validation loss: 2.615176590767094

Epoch: 6| Step: 7
Training loss: 2.831052161035006
Validation loss: 2.616873210805848

Epoch: 6| Step: 8
Training loss: 2.854541967364436
Validation loss: 2.6164761615660823

Epoch: 6| Step: 9
Training loss: 2.7308787636988727
Validation loss: 2.613481028342308

Epoch: 6| Step: 10
Training loss: 2.7065529669133617
Validation loss: 2.614857956849882

Epoch: 6| Step: 11
Training loss: 2.829870370382959
Validation loss: 2.6120188524014716

Epoch: 6| Step: 12
Training loss: 2.8068670872993184
Validation loss: 2.611471434231796

Epoch: 6| Step: 13
Training loss: 2.7012471638831874
Validation loss: 2.6104114019935007

Epoch: 59| Step: 0
Training loss: 3.0571145174778183
Validation loss: 2.608465908162891

Epoch: 6| Step: 1
Training loss: 2.5055995220697858
Validation loss: 2.608069178781229

Epoch: 6| Step: 2
Training loss: 2.456918977680078
Validation loss: 2.6080391180648355

Epoch: 6| Step: 3
Training loss: 3.027509131067368
Validation loss: 2.605437240905722

Epoch: 6| Step: 4
Training loss: 3.024575977715009
Validation loss: 2.610976285587393

Epoch: 6| Step: 5
Training loss: 3.02336873609626
Validation loss: 2.612680165529076

Epoch: 6| Step: 6
Training loss: 2.5969351090177857
Validation loss: 2.607023325355442

Epoch: 6| Step: 7
Training loss: 2.105940898547789
Validation loss: 2.6051633580558398

Epoch: 6| Step: 8
Training loss: 3.0666533352382417
Validation loss: 2.604515271060099

Epoch: 6| Step: 9
Training loss: 2.6013058830832025
Validation loss: 2.6012758053121803

Epoch: 6| Step: 10
Training loss: 2.702973466275096
Validation loss: 2.60292848270381

Epoch: 6| Step: 11
Training loss: 2.7327231513063652
Validation loss: 2.602528145619888

Epoch: 6| Step: 12
Training loss: 2.509665401847373
Validation loss: 2.6049586096445023

Epoch: 6| Step: 13
Training loss: 2.8413196280960555
Validation loss: 2.6031847183320016

Epoch: 60| Step: 0
Training loss: 3.0108072804044315
Validation loss: 2.6048784325972743

Epoch: 6| Step: 1
Training loss: 2.5240425825260044
Validation loss: 2.6022933834458017

Epoch: 6| Step: 2
Training loss: 3.200401078361575
Validation loss: 2.6018544320631323

Epoch: 6| Step: 3
Training loss: 2.789722012032551
Validation loss: 2.597129397396613

Epoch: 6| Step: 4
Training loss: 2.7992538479758697
Validation loss: 2.5963811283120974

Epoch: 6| Step: 5
Training loss: 2.61794522831845
Validation loss: 2.595060775647852

Epoch: 6| Step: 6
Training loss: 2.7261826215041087
Validation loss: 2.591293427881983

Epoch: 6| Step: 7
Training loss: 3.0014918114968134
Validation loss: 2.5923275704888993

Epoch: 6| Step: 8
Training loss: 2.3031066645354357
Validation loss: 2.5912080434618803

Epoch: 6| Step: 9
Training loss: 2.8318595044906143
Validation loss: 2.5933261509708427

Epoch: 6| Step: 10
Training loss: 2.5622154636115626
Validation loss: 2.590019791624432

Epoch: 6| Step: 11
Training loss: 2.6221467815024955
Validation loss: 2.586483624002179

Epoch: 6| Step: 12
Training loss: 2.281833417310354
Validation loss: 2.5899696223614375

Epoch: 6| Step: 13
Training loss: 2.8352815063760333
Validation loss: 2.5835944218111275

Epoch: 61| Step: 0
Training loss: 2.995283392743371
Validation loss: 2.586245945785489

Epoch: 6| Step: 1
Training loss: 2.522416608053908
Validation loss: 2.584768916733246

Epoch: 6| Step: 2
Training loss: 2.3410100561591634
Validation loss: 2.581437404747738

Epoch: 6| Step: 3
Training loss: 2.4349079532967632
Validation loss: 2.5824027436251287

Epoch: 6| Step: 4
Training loss: 2.7521346217152427
Validation loss: 2.5830993136164033

Epoch: 6| Step: 5
Training loss: 2.80469175035585
Validation loss: 2.583256563973819

Epoch: 6| Step: 6
Training loss: 2.4238893069307883
Validation loss: 2.580782589839027

Epoch: 6| Step: 7
Training loss: 3.0861686523395955
Validation loss: 2.582722489114932

Epoch: 6| Step: 8
Training loss: 2.5194140976276027
Validation loss: 2.5799768597898285

Epoch: 6| Step: 9
Training loss: 2.628386356816334
Validation loss: 2.5817953628155608

Epoch: 6| Step: 10
Training loss: 3.2304815917123086
Validation loss: 2.577939683823658

Epoch: 6| Step: 11
Training loss: 2.6217999707877238
Validation loss: 2.5792979336942996

Epoch: 6| Step: 12
Training loss: 2.5295504291949453
Validation loss: 2.5772935547710105

Epoch: 6| Step: 13
Training loss: 2.9305491896846947
Validation loss: 2.5769513165502915

Epoch: 62| Step: 0
Training loss: 2.716947374487788
Validation loss: 2.5776671802657214

Epoch: 6| Step: 1
Training loss: 3.3203843052897435
Validation loss: 2.5753156672197752

Epoch: 6| Step: 2
Training loss: 2.614469659976147
Validation loss: 2.573318702288856

Epoch: 6| Step: 3
Training loss: 2.949807862312484
Validation loss: 2.574004886524746

Epoch: 6| Step: 4
Training loss: 2.051680885263606
Validation loss: 2.5722356402040543

Epoch: 6| Step: 5
Training loss: 2.7186187186279303
Validation loss: 2.572984004716097

Epoch: 6| Step: 6
Training loss: 2.847793554860239
Validation loss: 2.570402288752753

Epoch: 6| Step: 7
Training loss: 2.651813796841033
Validation loss: 2.5701587480283252

Epoch: 6| Step: 8
Training loss: 2.9166967844543406
Validation loss: 2.5708502271726745

Epoch: 6| Step: 9
Training loss: 2.6404624753516637
Validation loss: 2.5684207188541177

Epoch: 6| Step: 10
Training loss: 2.744068685111289
Validation loss: 2.5666868384505763

Epoch: 6| Step: 11
Training loss: 1.8806455813518084
Validation loss: 2.5712390377569485

Epoch: 6| Step: 12
Training loss: 2.9786938023038387
Validation loss: 2.5663310155577705

Epoch: 6| Step: 13
Training loss: 2.584925704272068
Validation loss: 2.566163506875105

Epoch: 63| Step: 0
Training loss: 3.0479374524805127
Validation loss: 2.5664621595759876

Epoch: 6| Step: 1
Training loss: 3.123687468502396
Validation loss: 2.5664119633301845

Epoch: 6| Step: 2
Training loss: 2.7121362152156063
Validation loss: 2.5611070553142277

Epoch: 6| Step: 3
Training loss: 2.6348714050821496
Validation loss: 2.565782088853828

Epoch: 6| Step: 4
Training loss: 2.5333474970304506
Validation loss: 2.563389817480738

Epoch: 6| Step: 5
Training loss: 2.6708939742307316
Validation loss: 2.562042583175216

Epoch: 6| Step: 6
Training loss: 2.2828946390590246
Validation loss: 2.564626819067349

Epoch: 6| Step: 7
Training loss: 2.0738452639781397
Validation loss: 2.5626896423629097

Epoch: 6| Step: 8
Training loss: 3.300272849391718
Validation loss: 2.5596138587690933

Epoch: 6| Step: 9
Training loss: 2.522125375412919
Validation loss: 2.5601020929100735

Epoch: 6| Step: 10
Training loss: 2.8246532227345775
Validation loss: 2.558418201898561

Epoch: 6| Step: 11
Training loss: 2.6034821081043056
Validation loss: 2.55747006811163

Epoch: 6| Step: 12
Training loss: 2.3954823333591997
Validation loss: 2.558013151264758

Epoch: 6| Step: 13
Training loss: 2.72648266344769
Validation loss: 2.557896378776335

Epoch: 64| Step: 0
Training loss: 2.6720090241308356
Validation loss: 2.559652871250169

Epoch: 6| Step: 1
Training loss: 2.5165054960200313
Validation loss: 2.5555156644757675

Epoch: 6| Step: 2
Training loss: 2.737826800762404
Validation loss: 2.5581011348890805

Epoch: 6| Step: 3
Training loss: 2.358285614537367
Validation loss: 2.5560472546349846

Epoch: 6| Step: 4
Training loss: 2.5576104757214337
Validation loss: 2.5573592529895994

Epoch: 6| Step: 5
Training loss: 3.1623970829196315
Validation loss: 2.55687471449291

Epoch: 6| Step: 6
Training loss: 3.20324438849532
Validation loss: 2.552071712752894

Epoch: 6| Step: 7
Training loss: 3.0578902447640983
Validation loss: 2.5552329784707553

Epoch: 6| Step: 8
Training loss: 2.234730938924139
Validation loss: 2.551835765638229

Epoch: 6| Step: 9
Training loss: 2.53931714468266
Validation loss: 2.55146863631968

Epoch: 6| Step: 10
Training loss: 2.954129810040566
Validation loss: 2.551045332553317

Epoch: 6| Step: 11
Training loss: 2.334541133090935
Validation loss: 2.5508916810507616

Epoch: 6| Step: 12
Training loss: 1.9792071355061975
Validation loss: 2.551672319804216

Epoch: 6| Step: 13
Training loss: 2.978587665624687
Validation loss: 2.5484672359101515

Epoch: 65| Step: 0
Training loss: 2.4845987854939335
Validation loss: 2.5487930938534946

Epoch: 6| Step: 1
Training loss: 3.0115558268493015
Validation loss: 2.5457614734536835

Epoch: 6| Step: 2
Training loss: 2.9346593554502607
Validation loss: 2.545678448583073

Epoch: 6| Step: 3
Training loss: 2.4076040842407282
Validation loss: 2.5464116554587544

Epoch: 6| Step: 4
Training loss: 2.616743818793846
Validation loss: 2.5476215259190225

Epoch: 6| Step: 5
Training loss: 2.15571454904864
Validation loss: 2.548308439122772

Epoch: 6| Step: 6
Training loss: 2.781207652251778
Validation loss: 2.5478080179458984

Epoch: 6| Step: 7
Training loss: 2.6788371435704077
Validation loss: 2.5449525407264413

Epoch: 6| Step: 8
Training loss: 2.879903840472993
Validation loss: 2.5429681249599208

Epoch: 6| Step: 9
Training loss: 2.6710712495510527
Validation loss: 2.538931114514347

Epoch: 6| Step: 10
Training loss: 2.4520655974657166
Validation loss: 2.543703581490355

Epoch: 6| Step: 11
Training loss: 2.6371611499520937
Validation loss: 2.5448475980812524

Epoch: 6| Step: 12
Training loss: 3.3312472173453727
Validation loss: 2.541019299929886

Epoch: 6| Step: 13
Training loss: 2.2635929860808726
Validation loss: 2.543783484136224

Epoch: 66| Step: 0
Training loss: 3.0084731924527817
Validation loss: 2.543745556076879

Epoch: 6| Step: 1
Training loss: 3.1600293486475666
Validation loss: 2.543456000131774

Epoch: 6| Step: 2
Training loss: 2.2546347460586085
Validation loss: 2.538056660495903

Epoch: 6| Step: 3
Training loss: 2.5939589266911294
Validation loss: 2.5403522521788084

Epoch: 6| Step: 4
Training loss: 2.531601304530415
Validation loss: 2.5403466992292114

Epoch: 6| Step: 5
Training loss: 2.543373931154111
Validation loss: 2.5430203777801337

Epoch: 6| Step: 6
Training loss: 2.660665634163994
Validation loss: 2.5431345677064954

Epoch: 6| Step: 7
Training loss: 2.540171032050856
Validation loss: 2.542559128295366

Epoch: 6| Step: 8
Training loss: 2.494282669885769
Validation loss: 2.5439316762020563

Epoch: 6| Step: 9
Training loss: 2.7266268572275685
Validation loss: 2.5438910792763596

Epoch: 6| Step: 10
Training loss: 2.5789743584922
Validation loss: 2.5408407703848406

Epoch: 6| Step: 11
Training loss: 2.6469079386025145
Validation loss: 2.5414532795950233

Epoch: 6| Step: 12
Training loss: 2.960824768013934
Validation loss: 2.5391822434579696

Epoch: 6| Step: 13
Training loss: 2.6228856245228487
Validation loss: 2.5387243740204015

Epoch: 67| Step: 0
Training loss: 2.2827049736584746
Validation loss: 2.5354846336096197

Epoch: 6| Step: 1
Training loss: 2.5194845032606192
Validation loss: 2.537085599839285

Epoch: 6| Step: 2
Training loss: 3.001536134984335
Validation loss: 2.53484233995444

Epoch: 6| Step: 3
Training loss: 2.9304025820017525
Validation loss: 2.5335751439016256

Epoch: 6| Step: 4
Training loss: 2.53630070650223
Validation loss: 2.5309424193830936

Epoch: 6| Step: 5
Training loss: 2.468758691699988
Validation loss: 2.5309231394021108

Epoch: 6| Step: 6
Training loss: 2.195467037314014
Validation loss: 2.5314146545608445

Epoch: 6| Step: 7
Training loss: 2.8959364472470974
Validation loss: 2.5306093637566387

Epoch: 6| Step: 8
Training loss: 3.0132106148647644
Validation loss: 2.532575927203723

Epoch: 6| Step: 9
Training loss: 3.207585053479514
Validation loss: 2.532665814327864

Epoch: 6| Step: 10
Training loss: 2.3357223360615356
Validation loss: 2.5321162276155516

Epoch: 6| Step: 11
Training loss: 2.3265156143618784
Validation loss: 2.531127330178057

Epoch: 6| Step: 12
Training loss: 2.913205590957416
Validation loss: 2.5329829904422154

Epoch: 6| Step: 13
Training loss: 2.3175692284376477
Validation loss: 2.5306364187038097

Epoch: 68| Step: 0
Training loss: 2.08653839257371
Validation loss: 2.5353281739672515

Epoch: 6| Step: 1
Training loss: 2.9808462961235156
Validation loss: 2.5360297925274122

Epoch: 6| Step: 2
Training loss: 2.685128163381439
Validation loss: 2.532316681709424

Epoch: 6| Step: 3
Training loss: 2.5533382107295504
Validation loss: 2.530770228879029

Epoch: 6| Step: 4
Training loss: 2.5162612394791553
Validation loss: 2.5294777745345414

Epoch: 6| Step: 5
Training loss: 2.7295437229593658
Validation loss: 2.529611072638481

Epoch: 6| Step: 6
Training loss: 2.4057910035543824
Validation loss: 2.530086249815072

Epoch: 6| Step: 7
Training loss: 2.919025684218862
Validation loss: 2.5270664980821067

Epoch: 6| Step: 8
Training loss: 2.6670386730432503
Validation loss: 2.524488484154447

Epoch: 6| Step: 9
Training loss: 2.8163144841369494
Validation loss: 2.5284527522555753

Epoch: 6| Step: 10
Training loss: 2.8694533871608487
Validation loss: 2.5274417942688916

Epoch: 6| Step: 11
Training loss: 2.823967675831837
Validation loss: 2.5251533493208815

Epoch: 6| Step: 12
Training loss: 2.516440881196522
Validation loss: 2.521781716715154

Epoch: 6| Step: 13
Training loss: 2.5090371822553164
Validation loss: 2.5239559778655667

Epoch: 69| Step: 0
Training loss: 3.0933948659805783
Validation loss: 2.523546545233449

Epoch: 6| Step: 1
Training loss: 2.7101815788556776
Validation loss: 2.52765187857057

Epoch: 6| Step: 2
Training loss: 2.4058283646993286
Validation loss: 2.5292286435304905

Epoch: 6| Step: 3
Training loss: 2.6955484922728314
Validation loss: 2.527556452185183

Epoch: 6| Step: 4
Training loss: 2.7477048486181386
Validation loss: 2.524981055172897

Epoch: 6| Step: 5
Training loss: 3.3467326898888716
Validation loss: 2.5246281526294068

Epoch: 6| Step: 6
Training loss: 2.3146355536951044
Validation loss: 2.524832577080546

Epoch: 6| Step: 7
Training loss: 2.4816472655546287
Validation loss: 2.522590345410015

Epoch: 6| Step: 8
Training loss: 2.243420092627686
Validation loss: 2.5188941958464586

Epoch: 6| Step: 9
Training loss: 2.5468968115498636
Validation loss: 2.51898614077802

Epoch: 6| Step: 10
Training loss: 2.432673052035679
Validation loss: 2.5190156867510693

Epoch: 6| Step: 11
Training loss: 2.452578927266198
Validation loss: 2.5207980497638998

Epoch: 6| Step: 12
Training loss: 2.9978087687918995
Validation loss: 2.516328369754584

Epoch: 6| Step: 13
Training loss: 2.3908041095332617
Validation loss: 2.5186264897584

Epoch: 70| Step: 0
Training loss: 2.4934236814921817
Validation loss: 2.5156885143025667

Epoch: 6| Step: 1
Training loss: 2.5491680284014087
Validation loss: 2.5169792557999355

Epoch: 6| Step: 2
Training loss: 2.6935229566440957
Validation loss: 2.5180531032255975

Epoch: 6| Step: 3
Training loss: 2.5451138263721473
Validation loss: 2.5159515263719983

Epoch: 6| Step: 4
Training loss: 3.0510263185819437
Validation loss: 2.517710167080947

Epoch: 6| Step: 5
Training loss: 2.160226224072924
Validation loss: 2.5173299315410733

Epoch: 6| Step: 6
Training loss: 2.731153934282017
Validation loss: 2.519984556912731

Epoch: 6| Step: 7
Training loss: 2.649775254417963
Validation loss: 2.5174949283037362

Epoch: 6| Step: 8
Training loss: 2.685589221967263
Validation loss: 2.5170541656560506

Epoch: 6| Step: 9
Training loss: 1.8539868671236042
Validation loss: 2.5168802119174973

Epoch: 6| Step: 10
Training loss: 2.704600730663343
Validation loss: 2.517629342240085

Epoch: 6| Step: 11
Training loss: 3.3081070182768513
Validation loss: 2.515226075822804

Epoch: 6| Step: 12
Training loss: 2.343392917134194
Validation loss: 2.5169428183347766

Epoch: 6| Step: 13
Training loss: 2.8668551967045826
Validation loss: 2.520954449689131

Epoch: 71| Step: 0
Training loss: 2.579192524309423
Validation loss: 2.521140250655957

Epoch: 6| Step: 1
Training loss: 1.916393993165872
Validation loss: 2.517304264756788

Epoch: 6| Step: 2
Training loss: 2.1534283738475035
Validation loss: 2.5207183170614513

Epoch: 6| Step: 3
Training loss: 2.698544187485453
Validation loss: 2.5168432677960237

Epoch: 6| Step: 4
Training loss: 2.508320884631454
Validation loss: 2.513867761606291

Epoch: 6| Step: 5
Training loss: 2.2597261654533436
Validation loss: 2.511756149179672

Epoch: 6| Step: 6
Training loss: 3.2204657861023094
Validation loss: 2.5090562661557905

Epoch: 6| Step: 7
Training loss: 2.904063725300371
Validation loss: 2.511830123243579

Epoch: 6| Step: 8
Training loss: 2.933869898734884
Validation loss: 2.5077685295837377

Epoch: 6| Step: 9
Training loss: 2.614544345114146
Validation loss: 2.509259354209611

Epoch: 6| Step: 10
Training loss: 3.0108266021149293
Validation loss: 2.5089413646274217

Epoch: 6| Step: 11
Training loss: 2.290587350241884
Validation loss: 2.509041521677801

Epoch: 6| Step: 12
Training loss: 2.944125718034355
Validation loss: 2.5082406127200434

Epoch: 6| Step: 13
Training loss: 2.5445359611526177
Validation loss: 2.5117168436394195

Epoch: 72| Step: 0
Training loss: 2.8683151813163974
Validation loss: 2.509645625905771

Epoch: 6| Step: 1
Training loss: 2.447127086374485
Validation loss: 2.509220128329708

Epoch: 6| Step: 2
Training loss: 3.1758761421093755
Validation loss: 2.5113442845333194

Epoch: 6| Step: 3
Training loss: 2.5243904511105653
Validation loss: 2.5143808166906942

Epoch: 6| Step: 4
Training loss: 2.740396376570407
Validation loss: 2.514676391051143

Epoch: 6| Step: 5
Training loss: 2.8984210484287147
Validation loss: 2.5134767477855715

Epoch: 6| Step: 6
Training loss: 2.5016305374528724
Validation loss: 2.512681538237322

Epoch: 6| Step: 7
Training loss: 2.3411333863829533
Validation loss: 2.5143337371345744

Epoch: 6| Step: 8
Training loss: 2.3969551003412137
Validation loss: 2.513971705562498

Epoch: 6| Step: 9
Training loss: 2.6655301612717524
Validation loss: 2.5129366265981528

Epoch: 6| Step: 10
Training loss: 2.545729116084999
Validation loss: 2.5093712166535584

Epoch: 6| Step: 11
Training loss: 2.4110999833224747
Validation loss: 2.5061765347666793

Epoch: 6| Step: 12
Training loss: 2.6048171388141923
Validation loss: 2.4981622298953714

Epoch: 6| Step: 13
Training loss: 2.6525944407312423
Validation loss: 2.5028578000877566

Epoch: 73| Step: 0
Training loss: 2.041973155884829
Validation loss: 2.502118262291459

Epoch: 6| Step: 1
Training loss: 2.1261427555476544
Validation loss: 2.5047595017924293

Epoch: 6| Step: 2
Training loss: 3.0236106645541625
Validation loss: 2.5054383096331634

Epoch: 6| Step: 3
Training loss: 2.586393471707413
Validation loss: 2.505294692572691

Epoch: 6| Step: 4
Training loss: 2.5357717933504307
Validation loss: 2.501711053227884

Epoch: 6| Step: 5
Training loss: 2.1944489324767926
Validation loss: 2.500269120354223

Epoch: 6| Step: 6
Training loss: 2.820492133762729
Validation loss: 2.5001119429639287

Epoch: 6| Step: 7
Training loss: 2.732463408257432
Validation loss: 2.500963065297047

Epoch: 6| Step: 8
Training loss: 2.385077328817856
Validation loss: 2.5045513208865313

Epoch: 6| Step: 9
Training loss: 2.930040994298797
Validation loss: 2.503219121400863

Epoch: 6| Step: 10
Training loss: 3.1932027329223054
Validation loss: 2.5123445117973877

Epoch: 6| Step: 11
Training loss: 2.774558772845183
Validation loss: 2.5094194660863036

Epoch: 6| Step: 12
Training loss: 2.8727527626056846
Validation loss: 2.5128936629986347

Epoch: 6| Step: 13
Training loss: 2.458053691497984
Validation loss: 2.5031417973113923

Epoch: 74| Step: 0
Training loss: 2.9686239115390856
Validation loss: 2.5043331899089036

Epoch: 6| Step: 1
Training loss: 2.420072610884964
Validation loss: 2.5040385528222973

Epoch: 6| Step: 2
Training loss: 1.9722820871368387
Validation loss: 2.504628331752129

Epoch: 6| Step: 3
Training loss: 1.9421050197675709
Validation loss: 2.500033076385398

Epoch: 6| Step: 4
Training loss: 3.126446351081983
Validation loss: 2.5037420876616507

Epoch: 6| Step: 5
Training loss: 3.204854507595619
Validation loss: 2.50080450941553

Epoch: 6| Step: 6
Training loss: 2.134253605396612
Validation loss: 2.5020746523674497

Epoch: 6| Step: 7
Training loss: 2.569135494960763
Validation loss: 2.499980131706125

Epoch: 6| Step: 8
Training loss: 2.760433172980327
Validation loss: 2.4993029894339682

Epoch: 6| Step: 9
Training loss: 2.915288835875792
Validation loss: 2.4980063515906394

Epoch: 6| Step: 10
Training loss: 2.9187839089492584
Validation loss: 2.49665257143932

Epoch: 6| Step: 11
Training loss: 2.3519954853079406
Validation loss: 2.4948358127030286

Epoch: 6| Step: 12
Training loss: 2.0108668032581263
Validation loss: 2.4995517010877624

Epoch: 6| Step: 13
Training loss: 2.755176527179856
Validation loss: 2.49627465202984

Epoch: 75| Step: 0
Training loss: 3.055984728949773
Validation loss: 2.4962710306156897

Epoch: 6| Step: 1
Training loss: 2.889305184602012
Validation loss: 2.500923129355867

Epoch: 6| Step: 2
Training loss: 1.9263737446065086
Validation loss: 2.5033482858775944

Epoch: 6| Step: 3
Training loss: 2.6604296842021102
Validation loss: 2.5107204733559603

Epoch: 6| Step: 4
Training loss: 2.5719610336579333
Validation loss: 2.509742445783041

Epoch: 6| Step: 5
Training loss: 2.4601788032493737
Validation loss: 2.503577271748719

Epoch: 6| Step: 6
Training loss: 2.612756787689243
Validation loss: 2.4898106512465707

Epoch: 6| Step: 7
Training loss: 2.5762360097032357
Validation loss: 2.4978590380904047

Epoch: 6| Step: 8
Training loss: 2.8843936874098386
Validation loss: 2.5000828570304665

Epoch: 6| Step: 9
Training loss: 2.4378128829073598
Validation loss: 2.5014660828002984

Epoch: 6| Step: 10
Training loss: 2.469584167164528
Validation loss: 2.5030102091108715

Epoch: 6| Step: 11
Training loss: 2.555036608439317
Validation loss: 2.502708065060783

Epoch: 6| Step: 12
Training loss: 2.439354704435982
Validation loss: 2.5073335215920003

Epoch: 6| Step: 13
Training loss: 3.107517013196338
Validation loss: 2.516545153099018

Epoch: 76| Step: 0
Training loss: 2.3951876115683706
Validation loss: 2.5165409608313376

Epoch: 6| Step: 1
Training loss: 1.8601758498465393
Validation loss: 2.5086695869203077

Epoch: 6| Step: 2
Training loss: 2.765618361987752
Validation loss: 2.5062077697087948

Epoch: 6| Step: 3
Training loss: 3.246166168763862
Validation loss: 2.50479321335042

Epoch: 6| Step: 4
Training loss: 2.6243735882981736
Validation loss: 2.5041211492767532

Epoch: 6| Step: 5
Training loss: 2.467496916083947
Validation loss: 2.5000014623001907

Epoch: 6| Step: 6
Training loss: 2.536670485337834
Validation loss: 2.500483005277012

Epoch: 6| Step: 7
Training loss: 2.883049352990223
Validation loss: 2.4965282971330462

Epoch: 6| Step: 8
Training loss: 2.660216836337408
Validation loss: 2.498870562694506

Epoch: 6| Step: 9
Training loss: 3.1469511021008367
Validation loss: 2.4980670211333553

Epoch: 6| Step: 10
Training loss: 2.6537279609846722
Validation loss: 2.497981210043907

Epoch: 6| Step: 11
Training loss: 2.4525355706080303
Validation loss: 2.493971406418388

Epoch: 6| Step: 12
Training loss: 2.590091008829805
Validation loss: 2.489695108902508

Epoch: 6| Step: 13
Training loss: 2.1331216776676944
Validation loss: 2.4945521919987472

Epoch: 77| Step: 0
Training loss: 2.5541952516367825
Validation loss: 2.488794470478669

Epoch: 6| Step: 1
Training loss: 3.013864586454308
Validation loss: 2.4886858345214637

Epoch: 6| Step: 2
Training loss: 2.080945732168014
Validation loss: 2.49075359042841

Epoch: 6| Step: 3
Training loss: 2.9398969245539806
Validation loss: 2.495012219156376

Epoch: 6| Step: 4
Training loss: 2.6551697273386012
Validation loss: 2.4940448245711244

Epoch: 6| Step: 5
Training loss: 2.934130907722896
Validation loss: 2.4998621107857324

Epoch: 6| Step: 6
Training loss: 2.563340584191169
Validation loss: 2.5010432768571196

Epoch: 6| Step: 7
Training loss: 2.6738983880696887
Validation loss: 2.5177812200898515

Epoch: 6| Step: 8
Training loss: 2.954248608168782
Validation loss: 2.513297581958049

Epoch: 6| Step: 9
Training loss: 2.600084956321684
Validation loss: 2.493943595223417

Epoch: 6| Step: 10
Training loss: 2.3411151571389213
Validation loss: 2.489752860794801

Epoch: 6| Step: 11
Training loss: 2.426833165800675
Validation loss: 2.4912296953919437

Epoch: 6| Step: 12
Training loss: 2.73294361251464
Validation loss: 2.49351913925994

Epoch: 6| Step: 13
Training loss: 2.1434937348964955
Validation loss: 2.495407495698899

Epoch: 78| Step: 0
Training loss: 2.218355170566869
Validation loss: 2.498515069555397

Epoch: 6| Step: 1
Training loss: 2.5913687196393895
Validation loss: 2.5061082049916044

Epoch: 6| Step: 2
Training loss: 3.3011462850575284
Validation loss: 2.520188230669964

Epoch: 6| Step: 3
Training loss: 2.318341480883449
Validation loss: 2.5174554519222228

Epoch: 6| Step: 4
Training loss: 2.6741453641584076
Validation loss: 2.5195380789888477

Epoch: 6| Step: 5
Training loss: 3.0389608253396783
Validation loss: 2.5248079308709603

Epoch: 6| Step: 6
Training loss: 2.330495152721917
Validation loss: 2.521552374780018

Epoch: 6| Step: 7
Training loss: 2.3313931050494716
Validation loss: 2.5206950966840034

Epoch: 6| Step: 8
Training loss: 2.9388320824417753
Validation loss: 2.520291929718789

Epoch: 6| Step: 9
Training loss: 2.4932839782780243
Validation loss: 2.5156105309617214

Epoch: 6| Step: 10
Training loss: 2.6182339981567737
Validation loss: 2.511794797577649

Epoch: 6| Step: 11
Training loss: 2.9839629043543736
Validation loss: 2.5142257779659527

Epoch: 6| Step: 12
Training loss: 2.538866147005085
Validation loss: 2.514088811236854

Epoch: 6| Step: 13
Training loss: 2.352477038410271
Validation loss: 2.508270111005388

Epoch: 79| Step: 0
Training loss: 2.481120539057604
Validation loss: 2.5105116154824234

Epoch: 6| Step: 1
Training loss: 2.4322625647906584
Validation loss: 2.5037056162609708

Epoch: 6| Step: 2
Training loss: 3.0679052491558227
Validation loss: 2.5011080193817055

Epoch: 6| Step: 3
Training loss: 2.6506043518744513
Validation loss: 2.504484731708761

Epoch: 6| Step: 4
Training loss: 1.8327754934856273
Validation loss: 2.4971528530092253

Epoch: 6| Step: 5
Training loss: 2.5813123940230693
Validation loss: 2.496586559442504

Epoch: 6| Step: 6
Training loss: 2.272523676682816
Validation loss: 2.499169291127824

Epoch: 6| Step: 7
Training loss: 2.995986956467393
Validation loss: 2.495138798549797

Epoch: 6| Step: 8
Training loss: 3.149210718968456
Validation loss: 2.4913244955533727

Epoch: 6| Step: 9
Training loss: 2.898872939147519
Validation loss: 2.4939106531747433

Epoch: 6| Step: 10
Training loss: 2.5561493123937375
Validation loss: 2.489740236410994

Epoch: 6| Step: 11
Training loss: 2.2515879431722454
Validation loss: 2.4884797261128693

Epoch: 6| Step: 12
Training loss: 2.889225141404631
Validation loss: 2.4906787989352903

Epoch: 6| Step: 13
Training loss: 2.212979186155381
Validation loss: 2.487335632235568

Epoch: 80| Step: 0
Training loss: 2.137812193441292
Validation loss: 2.4899162378483037

Epoch: 6| Step: 1
Training loss: 2.6244181033449205
Validation loss: 2.487901276341882

Epoch: 6| Step: 2
Training loss: 2.961864960285791
Validation loss: 2.4850481029181175

Epoch: 6| Step: 3
Training loss: 2.879378095078259
Validation loss: 2.4842003654984817

Epoch: 6| Step: 4
Training loss: 2.5125742355690917
Validation loss: 2.4828936881060306

Epoch: 6| Step: 5
Training loss: 1.9114356875485707
Validation loss: 2.489086793252962

Epoch: 6| Step: 6
Training loss: 3.1883771100971425
Validation loss: 2.491964380048491

Epoch: 6| Step: 7
Training loss: 2.7689437350655908
Validation loss: 2.4883973929439334

Epoch: 6| Step: 8
Training loss: 2.8822527086065044
Validation loss: 2.4828646325283183

Epoch: 6| Step: 9
Training loss: 2.852139182109375
Validation loss: 2.4866534807078886

Epoch: 6| Step: 10
Training loss: 2.3705098723161933
Validation loss: 2.486851527482046

Epoch: 6| Step: 11
Training loss: 2.4257384955282677
Validation loss: 2.4846058224467313

Epoch: 6| Step: 12
Training loss: 2.4133740279769262
Validation loss: 2.4883133643137514

Epoch: 6| Step: 13
Training loss: 2.147217449211989
Validation loss: 2.485090220762495

Epoch: 81| Step: 0
Training loss: 2.4814142782949653
Validation loss: 2.486520700321115

Epoch: 6| Step: 1
Training loss: 2.2817162860224105
Validation loss: 2.489629487018181

Epoch: 6| Step: 2
Training loss: 2.7056511337487974
Validation loss: 2.491203376881167

Epoch: 6| Step: 3
Training loss: 3.290362103603059
Validation loss: 2.490360910030726

Epoch: 6| Step: 4
Training loss: 2.8743469491723346
Validation loss: 2.4913927044584474

Epoch: 6| Step: 5
Training loss: 2.5658554809975627
Validation loss: 2.4907975261926913

Epoch: 6| Step: 6
Training loss: 2.56517865739251
Validation loss: 2.4909819873223387

Epoch: 6| Step: 7
Training loss: 2.8611981359125096
Validation loss: 2.49459045384503

Epoch: 6| Step: 8
Training loss: 2.1511710238104365
Validation loss: 2.4927801465919046

Epoch: 6| Step: 9
Training loss: 2.1724290758502587
Validation loss: 2.4927126372050843

Epoch: 6| Step: 10
Training loss: 2.3821442214135082
Validation loss: 2.491391253054916

Epoch: 6| Step: 11
Training loss: 2.5121667441649413
Validation loss: 2.493064024676782

Epoch: 6| Step: 12
Training loss: 2.5093709791253396
Validation loss: 2.490240462551281

Epoch: 6| Step: 13
Training loss: 2.977258635890633
Validation loss: 2.4925426680503984

Epoch: 82| Step: 0
Training loss: 2.6633185189622486
Validation loss: 2.488148987676888

Epoch: 6| Step: 1
Training loss: 2.3135640557339516
Validation loss: 2.486026367031305

Epoch: 6| Step: 2
Training loss: 2.345577900487609
Validation loss: 2.488572786779827

Epoch: 6| Step: 3
Training loss: 2.446552583735512
Validation loss: 2.485186382368056

Epoch: 6| Step: 4
Training loss: 2.1594543972407028
Validation loss: 2.482993519551668

Epoch: 6| Step: 5
Training loss: 2.222840956805162
Validation loss: 2.4815572117667988

Epoch: 6| Step: 6
Training loss: 3.0147044303595
Validation loss: 2.4789379612765527

Epoch: 6| Step: 7
Training loss: 2.2326309760312557
Validation loss: 2.477862204575445

Epoch: 6| Step: 8
Training loss: 2.6053296556762437
Validation loss: 2.476927536817859

Epoch: 6| Step: 9
Training loss: 2.0739756296210334
Validation loss: 2.478643832386607

Epoch: 6| Step: 10
Training loss: 3.201932061584373
Validation loss: 2.4742426391000762

Epoch: 6| Step: 11
Training loss: 3.0637875400019166
Validation loss: 2.4785173244285117

Epoch: 6| Step: 12
Training loss: 2.5655867430869903
Validation loss: 2.4797438956635927

Epoch: 6| Step: 13
Training loss: 2.9695870273553373
Validation loss: 2.4772560600545157

Epoch: 83| Step: 0
Training loss: 2.3686718205657957
Validation loss: 2.482879476442091

Epoch: 6| Step: 1
Training loss: 2.5563081506210077
Validation loss: 2.4870489034145806

Epoch: 6| Step: 2
Training loss: 2.5465879688119846
Validation loss: 2.4813669015425144

Epoch: 6| Step: 3
Training loss: 2.4452311834397804
Validation loss: 2.4784452419843888

Epoch: 6| Step: 4
Training loss: 3.1347978726923875
Validation loss: 2.4768558974214856

Epoch: 6| Step: 5
Training loss: 2.5599503276893514
Validation loss: 2.479542765152227

Epoch: 6| Step: 6
Training loss: 2.5744089800135703
Validation loss: 2.4803969966092048

Epoch: 6| Step: 7
Training loss: 2.1290989779806693
Validation loss: 2.4845110538103956

Epoch: 6| Step: 8
Training loss: 2.7967306771150064
Validation loss: 2.482641082857551

Epoch: 6| Step: 9
Training loss: 2.906616844076887
Validation loss: 2.4879766145007824

Epoch: 6| Step: 10
Training loss: 2.5754184271829
Validation loss: 2.4845317095389112

Epoch: 6| Step: 11
Training loss: 2.2249033317568747
Validation loss: 2.4799184113586983

Epoch: 6| Step: 12
Training loss: 2.88570219938453
Validation loss: 2.4824390025614718

Epoch: 6| Step: 13
Training loss: 2.5923402930924078
Validation loss: 2.4862603764919204

Epoch: 84| Step: 0
Training loss: 2.7176328095634936
Validation loss: 2.4841812186170573

Epoch: 6| Step: 1
Training loss: 2.7597308092560198
Validation loss: 2.4856348901720873

Epoch: 6| Step: 2
Training loss: 2.500351594996651
Validation loss: 2.4835022086982383

Epoch: 6| Step: 3
Training loss: 2.541077269290936
Validation loss: 2.4847649482174803

Epoch: 6| Step: 4
Training loss: 2.3554488097401727
Validation loss: 2.4782677845430445

Epoch: 6| Step: 5
Training loss: 2.145469251936424
Validation loss: 2.4852366682574485

Epoch: 6| Step: 6
Training loss: 2.656062759083073
Validation loss: 2.4864648310418334

Epoch: 6| Step: 7
Training loss: 2.492407809956387
Validation loss: 2.48199073375004

Epoch: 6| Step: 8
Training loss: 2.456305901665375
Validation loss: 2.4828217967367276

Epoch: 6| Step: 9
Training loss: 2.6477973636990098
Validation loss: 2.4822352089319715

Epoch: 6| Step: 10
Training loss: 2.6375195000818037
Validation loss: 2.478239308078431

Epoch: 6| Step: 11
Training loss: 2.493793603807483
Validation loss: 2.4770482145557207

Epoch: 6| Step: 12
Training loss: 3.144238874218136
Validation loss: 2.4853445436469737

Epoch: 6| Step: 13
Training loss: 2.5071505329478803
Validation loss: 2.4872930731400493

Epoch: 85| Step: 0
Training loss: 2.795753211414506
Validation loss: 2.4841498026957383

Epoch: 6| Step: 1
Training loss: 2.4694790306954286
Validation loss: 2.4762983853194225

Epoch: 6| Step: 2
Training loss: 1.9312698733594245
Validation loss: 2.4791152425182332

Epoch: 6| Step: 3
Training loss: 2.003065382714503
Validation loss: 2.477320349550801

Epoch: 6| Step: 4
Training loss: 2.673270324081808
Validation loss: 2.4790131230183285

Epoch: 6| Step: 5
Training loss: 2.1138601585482095
Validation loss: 2.4796398469944814

Epoch: 6| Step: 6
Training loss: 3.207388073937411
Validation loss: 2.4827963973960183

Epoch: 6| Step: 7
Training loss: 2.9756649396203434
Validation loss: 2.478993391077676

Epoch: 6| Step: 8
Training loss: 2.63523024752848
Validation loss: 2.480258337993697

Epoch: 6| Step: 9
Training loss: 2.7055243279049437
Validation loss: 2.4869171987006635

Epoch: 6| Step: 10
Training loss: 2.724744480297218
Validation loss: 2.4811708913342496

Epoch: 6| Step: 11
Training loss: 3.1229822892887857
Validation loss: 2.4794334674105682

Epoch: 6| Step: 12
Training loss: 2.1916404775739733
Validation loss: 2.4759456455954223

Epoch: 6| Step: 13
Training loss: 2.4852623464463592
Validation loss: 2.4761604440006697

Epoch: 86| Step: 0
Training loss: 2.374588780189109
Validation loss: 2.4853506831459233

Epoch: 6| Step: 1
Training loss: 2.3594850426078744
Validation loss: 2.4790356598761996

Epoch: 6| Step: 2
Training loss: 2.776949354194145
Validation loss: 2.4891267355121323

Epoch: 6| Step: 3
Training loss: 2.5462840512170715
Validation loss: 2.5061277630836543

Epoch: 6| Step: 4
Training loss: 2.5524285738195855
Validation loss: 2.5133629653290166

Epoch: 6| Step: 5
Training loss: 2.860294548552645
Validation loss: 2.485751924075826

Epoch: 6| Step: 6
Training loss: 3.234016435438679
Validation loss: 2.4793130019353455

Epoch: 6| Step: 7
Training loss: 3.3440125264293648
Validation loss: 2.48282381330841

Epoch: 6| Step: 8
Training loss: 2.196965908378117
Validation loss: 2.4739592435065485

Epoch: 6| Step: 9
Training loss: 2.5614367814700265
Validation loss: 2.477180460016802

Epoch: 6| Step: 10
Training loss: 2.2098538064717728
Validation loss: 2.4730852748227683

Epoch: 6| Step: 11
Training loss: 2.4514035544768746
Validation loss: 2.4745139746177247

Epoch: 6| Step: 12
Training loss: 1.9784281975878892
Validation loss: 2.4746594743843358

Epoch: 6| Step: 13
Training loss: 2.642912466434864
Validation loss: 2.4755349900760977

Epoch: 87| Step: 0
Training loss: 2.817148393898019
Validation loss: 2.478451238246293

Epoch: 6| Step: 1
Training loss: 1.9292290023169196
Validation loss: 2.4834730322371863

Epoch: 6| Step: 2
Training loss: 2.2219269264902923
Validation loss: 2.4824098376150583

Epoch: 6| Step: 3
Training loss: 2.4858243541276153
Validation loss: 2.482519836653855

Epoch: 6| Step: 4
Training loss: 1.7464816283475522
Validation loss: 2.479745353884363

Epoch: 6| Step: 5
Training loss: 3.147518658086546
Validation loss: 2.4881326899553002

Epoch: 6| Step: 6
Training loss: 2.6397363194363037
Validation loss: 2.484562928711302

Epoch: 6| Step: 7
Training loss: 2.4023008761418123
Validation loss: 2.4826130406976272

Epoch: 6| Step: 8
Training loss: 2.4523729277032853
Validation loss: 2.486364467741802

Epoch: 6| Step: 9
Training loss: 2.982033974753962
Validation loss: 2.484974882688428

Epoch: 6| Step: 10
Training loss: 2.7637973069481854
Validation loss: 2.4825574196087508

Epoch: 6| Step: 11
Training loss: 2.6371015708784133
Validation loss: 2.480366221634313

Epoch: 6| Step: 12
Training loss: 2.5655886016754272
Validation loss: 2.478748163327428

Epoch: 6| Step: 13
Training loss: 3.091003586094731
Validation loss: 2.4814655693955827

Epoch: 88| Step: 0
Training loss: 2.5114089039057714
Validation loss: 2.478218672057538

Epoch: 6| Step: 1
Training loss: 3.0004688532350072
Validation loss: 2.4803939768016514

Epoch: 6| Step: 2
Training loss: 2.5304846842900983
Validation loss: 2.4764528139937045

Epoch: 6| Step: 3
Training loss: 3.0718758754787188
Validation loss: 2.476230619156678

Epoch: 6| Step: 4
Training loss: 2.8704087458771794
Validation loss: 2.4783907138139485

Epoch: 6| Step: 5
Training loss: 2.223175470885491
Validation loss: 2.4724789717101148

Epoch: 6| Step: 6
Training loss: 2.4521717722157352
Validation loss: 2.4717574974582455

Epoch: 6| Step: 7
Training loss: 3.277196651244807
Validation loss: 2.4751829362301634

Epoch: 6| Step: 8
Training loss: 2.070879012997768
Validation loss: 2.468104072766081

Epoch: 6| Step: 9
Training loss: 2.293639934508013
Validation loss: 2.4731894385781783

Epoch: 6| Step: 10
Training loss: 2.7307429141908104
Validation loss: 2.466723000252598

Epoch: 6| Step: 11
Training loss: 2.2111224309460047
Validation loss: 2.4689009093182643

Epoch: 6| Step: 12
Training loss: 1.9435225375129563
Validation loss: 2.4689758374538395

Epoch: 6| Step: 13
Training loss: 2.3550158519170865
Validation loss: 2.4699665082238744

Epoch: 89| Step: 0
Training loss: 2.708106946042045
Validation loss: 2.4651516310485815

Epoch: 6| Step: 1
Training loss: 2.4332616098711526
Validation loss: 2.4799431992507017

Epoch: 6| Step: 2
Training loss: 2.464747891020557
Validation loss: 2.487077486714411

Epoch: 6| Step: 3
Training loss: 2.190954586817208
Validation loss: 2.484329990962916

Epoch: 6| Step: 4
Training loss: 3.224788889517887
Validation loss: 2.4838682573563653

Epoch: 6| Step: 5
Training loss: 2.3524583903249914
Validation loss: 2.48221298927636

Epoch: 6| Step: 6
Training loss: 2.758188973036515
Validation loss: 2.478953846577607

Epoch: 6| Step: 7
Training loss: 2.4498567617155875
Validation loss: 2.470356076761204

Epoch: 6| Step: 8
Training loss: 2.569880023040902
Validation loss: 2.4646043535262145

Epoch: 6| Step: 9
Training loss: 2.528835416549555
Validation loss: 2.4671033526407617

Epoch: 6| Step: 10
Training loss: 2.838250512172707
Validation loss: 2.465342894814977

Epoch: 6| Step: 11
Training loss: 2.8578043308214296
Validation loss: 2.4673783237973366

Epoch: 6| Step: 12
Training loss: 2.6439595132691416
Validation loss: 2.4746489246944368

Epoch: 6| Step: 13
Training loss: 1.9642402829452656
Validation loss: 2.4765968360486146

Epoch: 90| Step: 0
Training loss: 2.668511130298477
Validation loss: 2.478004284580725

Epoch: 6| Step: 1
Training loss: 2.380597795695057
Validation loss: 2.480488870694313

Epoch: 6| Step: 2
Training loss: 2.4521993846758443
Validation loss: 2.4786279771619464

Epoch: 6| Step: 3
Training loss: 2.6559812522019692
Validation loss: 2.482033159629272

Epoch: 6| Step: 4
Training loss: 2.350515341560366
Validation loss: 2.479773380400455

Epoch: 6| Step: 5
Training loss: 2.2381772854345465
Validation loss: 2.480875353390567

Epoch: 6| Step: 6
Training loss: 2.6709348574984917
Validation loss: 2.4806069483268

Epoch: 6| Step: 7
Training loss: 3.1415924555937322
Validation loss: 2.478494975247246

Epoch: 6| Step: 8
Training loss: 2.6356967782189717
Validation loss: 2.480204634735114

Epoch: 6| Step: 9
Training loss: 2.2837345754585887
Validation loss: 2.477426693034189

Epoch: 6| Step: 10
Training loss: 2.9481658470780507
Validation loss: 2.479777114040496

Epoch: 6| Step: 11
Training loss: 2.198881458822279
Validation loss: 2.47657756620331

Epoch: 6| Step: 12
Training loss: 2.4635569361616065
Validation loss: 2.4777373005417074

Epoch: 6| Step: 13
Training loss: 2.9376873200123144
Validation loss: 2.47616798636071

Epoch: 91| Step: 0
Training loss: 3.2237101733087203
Validation loss: 2.4717510267854688

Epoch: 6| Step: 1
Training loss: 2.6116783346642984
Validation loss: 2.4698709445963

Epoch: 6| Step: 2
Training loss: 2.6817887411867396
Validation loss: 2.4674226274762883

Epoch: 6| Step: 3
Training loss: 2.9766195296784055
Validation loss: 2.4642060226154645

Epoch: 6| Step: 4
Training loss: 2.1995092711638082
Validation loss: 2.461539656019074

Epoch: 6| Step: 5
Training loss: 2.2938920977490906
Validation loss: 2.4591583602473706

Epoch: 6| Step: 6
Training loss: 2.3755362055963536
Validation loss: 2.464794095965459

Epoch: 6| Step: 7
Training loss: 2.8898891852757655
Validation loss: 2.464867335316001

Epoch: 6| Step: 8
Training loss: 2.214342171524987
Validation loss: 2.464942756836741

Epoch: 6| Step: 9
Training loss: 2.486293699591064
Validation loss: 2.466627029044625

Epoch: 6| Step: 10
Training loss: 2.5108756968708583
Validation loss: 2.4669053634502984

Epoch: 6| Step: 11
Training loss: 2.6894105729204547
Validation loss: 2.47176916874072

Epoch: 6| Step: 12
Training loss: 1.795820042619255
Validation loss: 2.472461550149727

Epoch: 6| Step: 13
Training loss: 2.826742603643533
Validation loss: 2.4708574530811065

Epoch: 92| Step: 0
Training loss: 2.3043256055247174
Validation loss: 2.464677683157476

Epoch: 6| Step: 1
Training loss: 2.779522305619533
Validation loss: 2.4711703093740676

Epoch: 6| Step: 2
Training loss: 2.6707873891448446
Validation loss: 2.4687612026298438

Epoch: 6| Step: 3
Training loss: 2.5375293974277535
Validation loss: 2.474039069748941

Epoch: 6| Step: 4
Training loss: 2.2917372143608175
Validation loss: 2.4733212798545496

Epoch: 6| Step: 5
Training loss: 2.4175540895140935
Validation loss: 2.4760167656784344

Epoch: 6| Step: 6
Training loss: 2.4330135039469436
Validation loss: 2.4787203496537313

Epoch: 6| Step: 7
Training loss: 2.353317062422542
Validation loss: 2.478935452639985

Epoch: 6| Step: 8
Training loss: 3.2099193434519946
Validation loss: 2.4765655325019633

Epoch: 6| Step: 9
Training loss: 2.3521604062999897
Validation loss: 2.479260071562984

Epoch: 6| Step: 10
Training loss: 2.5317190230252353
Validation loss: 2.4788413007923595

Epoch: 6| Step: 11
Training loss: 2.54361886476113
Validation loss: 2.480403949361691

Epoch: 6| Step: 12
Training loss: 2.357358968001291
Validation loss: 2.481378551647477

Epoch: 6| Step: 13
Training loss: 3.0558244779521075
Validation loss: 2.4754228988420897

Epoch: 93| Step: 0
Training loss: 2.7190625241306674
Validation loss: 2.478806611102521

Epoch: 6| Step: 1
Training loss: 2.8089611253557365
Validation loss: 2.477790167221129

Epoch: 6| Step: 2
Training loss: 2.200061914266329
Validation loss: 2.474060391111231

Epoch: 6| Step: 3
Training loss: 2.6491068216503932
Validation loss: 2.473155601504304

Epoch: 6| Step: 4
Training loss: 2.0904599431088546
Validation loss: 2.4728459852401943

Epoch: 6| Step: 5
Training loss: 2.55135489727098
Validation loss: 2.4720139957513245

Epoch: 6| Step: 6
Training loss: 2.757751982892136
Validation loss: 2.469753479927166

Epoch: 6| Step: 7
Training loss: 2.908045552470198
Validation loss: 2.4714360573603003

Epoch: 6| Step: 8
Training loss: 2.467970809621697
Validation loss: 2.4674201312886983

Epoch: 6| Step: 9
Training loss: 2.1804735960679307
Validation loss: 2.466547325400456

Epoch: 6| Step: 10
Training loss: 3.1908678700280038
Validation loss: 2.4669555709666793

Epoch: 6| Step: 11
Training loss: 2.228679734522173
Validation loss: 2.4649651079887294

Epoch: 6| Step: 12
Training loss: 2.193716426059915
Validation loss: 2.472473242216025

Epoch: 6| Step: 13
Training loss: 2.7845476296081553
Validation loss: 2.4760237628277166

Epoch: 94| Step: 0
Training loss: 2.5342589026034426
Validation loss: 2.4751568002834445

Epoch: 6| Step: 1
Training loss: 2.2423206249950787
Validation loss: 2.4785631764937954

Epoch: 6| Step: 2
Training loss: 2.1679997622359592
Validation loss: 2.467880788122917

Epoch: 6| Step: 3
Training loss: 2.6819006675002215
Validation loss: 2.467057899645084

Epoch: 6| Step: 4
Training loss: 2.7060675508874166
Validation loss: 2.459516746898779

Epoch: 6| Step: 5
Training loss: 2.82773094171768
Validation loss: 2.460958894505423

Epoch: 6| Step: 6
Training loss: 2.4994750425401833
Validation loss: 2.4699428589835595

Epoch: 6| Step: 7
Training loss: 2.2660032811557755
Validation loss: 2.4646476913480444

Epoch: 6| Step: 8
Training loss: 3.0204497170496145
Validation loss: 2.4737046657145836

Epoch: 6| Step: 9
Training loss: 2.259027174139022
Validation loss: 2.4721068082674624

Epoch: 6| Step: 10
Training loss: 2.619850830461698
Validation loss: 2.4751626841063317

Epoch: 6| Step: 11
Training loss: 2.4065678374198214
Validation loss: 2.471606786670927

Epoch: 6| Step: 12
Training loss: 2.889186191747762
Validation loss: 2.4753148318588227

Epoch: 6| Step: 13
Training loss: 2.621450840464526
Validation loss: 2.4751272605519765

Epoch: 95| Step: 0
Training loss: 2.717561133012046
Validation loss: 2.473596346958238

Epoch: 6| Step: 1
Training loss: 2.5889126814302394
Validation loss: 2.473740840520313

Epoch: 6| Step: 2
Training loss: 2.577531596007694
Validation loss: 2.4737919373352426

Epoch: 6| Step: 3
Training loss: 2.729445717404061
Validation loss: 2.47315833290921

Epoch: 6| Step: 4
Training loss: 2.609710489052363
Validation loss: 2.467675454041574

Epoch: 6| Step: 5
Training loss: 2.690988450086878
Validation loss: 2.476461318202442

Epoch: 6| Step: 6
Training loss: 2.699374910687151
Validation loss: 2.471973616235995

Epoch: 6| Step: 7
Training loss: 2.6831751778791424
Validation loss: 2.463865881158616

Epoch: 6| Step: 8
Training loss: 2.7724777065306094
Validation loss: 2.465941928596405

Epoch: 6| Step: 9
Training loss: 2.3902569655815733
Validation loss: 2.4656611083351696

Epoch: 6| Step: 10
Training loss: 2.3307555014260886
Validation loss: 2.4629254420595283

Epoch: 6| Step: 11
Training loss: 2.757224217420817
Validation loss: 2.465526278751123

Epoch: 6| Step: 12
Training loss: 2.1977089396539884
Validation loss: 2.4655087113606573

Epoch: 6| Step: 13
Training loss: 2.0986515575084788
Validation loss: 2.4672993127902547

Epoch: 96| Step: 0
Training loss: 3.0707218586440383
Validation loss: 2.468071397637385

Epoch: 6| Step: 1
Training loss: 2.0305435125802975
Validation loss: 2.469847728833596

Epoch: 6| Step: 2
Training loss: 2.410701053320682
Validation loss: 2.4738898873481228

Epoch: 6| Step: 3
Training loss: 2.530916733643025
Validation loss: 2.4746316950432994

Epoch: 6| Step: 4
Training loss: 3.001970915285607
Validation loss: 2.474923619462849

Epoch: 6| Step: 5
Training loss: 2.466476745854661
Validation loss: 2.4738053177477375

Epoch: 6| Step: 6
Training loss: 2.918828018093314
Validation loss: 2.474738234346612

Epoch: 6| Step: 7
Training loss: 2.8977716303131538
Validation loss: 2.477756745796103

Epoch: 6| Step: 8
Training loss: 2.3277194290718364
Validation loss: 2.4737039428556145

Epoch: 6| Step: 9
Training loss: 1.8837264126197621
Validation loss: 2.472537085547835

Epoch: 6| Step: 10
Training loss: 2.4880765773849567
Validation loss: 2.465878704122472

Epoch: 6| Step: 11
Training loss: 2.5681459519512675
Validation loss: 2.465704701549256

Epoch: 6| Step: 12
Training loss: 2.8095387446088895
Validation loss: 2.4600158580279237

Epoch: 6| Step: 13
Training loss: 2.316116392067023
Validation loss: 2.4579665724020616

Epoch: 97| Step: 0
Training loss: 2.892234890063696
Validation loss: 2.4607744677597507

Epoch: 6| Step: 1
Training loss: 3.1157174145490503
Validation loss: 2.4625142369770487

Epoch: 6| Step: 2
Training loss: 2.453562995053728
Validation loss: 2.4623786541365758

Epoch: 6| Step: 3
Training loss: 3.227163632902024
Validation loss: 2.4680196591196566

Epoch: 6| Step: 4
Training loss: 2.0834335557354717
Validation loss: 2.4641123964768545

Epoch: 6| Step: 5
Training loss: 2.7612556382169617
Validation loss: 2.4616017086090896

Epoch: 6| Step: 6
Training loss: 1.3943554850280935
Validation loss: 2.459438517363035

Epoch: 6| Step: 7
Training loss: 2.705171901578501
Validation loss: 2.466674431152839

Epoch: 6| Step: 8
Training loss: 2.5131002515718874
Validation loss: 2.458525235288434

Epoch: 6| Step: 9
Training loss: 2.6500424723550458
Validation loss: 2.461894774572608

Epoch: 6| Step: 10
Training loss: 1.948192563297359
Validation loss: 2.4631434569739716

Epoch: 6| Step: 11
Training loss: 2.6595886790119105
Validation loss: 2.4600100429712413

Epoch: 6| Step: 12
Training loss: 2.4074214709379222
Validation loss: 2.458804543372936

Epoch: 6| Step: 13
Training loss: 2.512498796756893
Validation loss: 2.4611563771918985

Epoch: 98| Step: 0
Training loss: 2.3849245810430113
Validation loss: 2.466666674506557

Epoch: 6| Step: 1
Training loss: 2.5407919290467262
Validation loss: 2.4605818557089716

Epoch: 6| Step: 2
Training loss: 3.192676753540843
Validation loss: 2.4661497111358823

Epoch: 6| Step: 3
Training loss: 2.509274730840608
Validation loss: 2.462562129726324

Epoch: 6| Step: 4
Training loss: 2.3672413835593296
Validation loss: 2.4606352766844775

Epoch: 6| Step: 5
Training loss: 2.3345230566255935
Validation loss: 2.4622963035232783

Epoch: 6| Step: 6
Training loss: 2.4289595810663167
Validation loss: 2.464290583021937

Epoch: 6| Step: 7
Training loss: 2.3367675531278405
Validation loss: 2.4581938369957808

Epoch: 6| Step: 8
Training loss: 2.362645289076083
Validation loss: 2.461122455408125

Epoch: 6| Step: 9
Training loss: 3.006156961186985
Validation loss: 2.463142722949397

Epoch: 6| Step: 10
Training loss: 2.618310852375158
Validation loss: 2.4606738075387686

Epoch: 6| Step: 11
Training loss: 2.406460443187052
Validation loss: 2.4719639552748705

Epoch: 6| Step: 12
Training loss: 2.553118582709289
Validation loss: 2.474872826992011

Epoch: 6| Step: 13
Training loss: 2.626748728688565
Validation loss: 2.4724292299372155

Epoch: 99| Step: 0
Training loss: 3.1039573206277833
Validation loss: 2.4721626644842365

Epoch: 6| Step: 1
Training loss: 3.0505378811679944
Validation loss: 2.472458142966739

Epoch: 6| Step: 2
Training loss: 1.947964801975569
Validation loss: 2.469829194679279

Epoch: 6| Step: 3
Training loss: 2.652197944779661
Validation loss: 2.469046611109523

Epoch: 6| Step: 4
Training loss: 2.7064528955250267
Validation loss: 2.4650052799067717

Epoch: 6| Step: 5
Training loss: 2.5149180680688428
Validation loss: 2.468003526333722

Epoch: 6| Step: 6
Training loss: 2.8924592667912172
Validation loss: 2.459817161058544

Epoch: 6| Step: 7
Training loss: 2.528371327851723
Validation loss: 2.4559344910066327

Epoch: 6| Step: 8
Training loss: 2.4113086189829467
Validation loss: 2.4543078784006855

Epoch: 6| Step: 9
Training loss: 2.6656208570477222
Validation loss: 2.453536126740908

Epoch: 6| Step: 10
Training loss: 2.262534513262959
Validation loss: 2.4568192916683467

Epoch: 6| Step: 11
Training loss: 2.202749017327531
Validation loss: 2.456102787100453

Epoch: 6| Step: 12
Training loss: 1.8420535542824161
Validation loss: 2.4561484751859783

Epoch: 6| Step: 13
Training loss: 2.6240281622119728
Validation loss: 2.457025913056853

Epoch: 100| Step: 0
Training loss: 2.2764478331247733
Validation loss: 2.460663504712266

Epoch: 6| Step: 1
Training loss: 2.8365674540842183
Validation loss: 2.459755717711871

Epoch: 6| Step: 2
Training loss: 2.5926250178211707
Validation loss: 2.458633361439124

Epoch: 6| Step: 3
Training loss: 2.6201101808457845
Validation loss: 2.455447626139344

Epoch: 6| Step: 4
Training loss: 2.0849755744509646
Validation loss: 2.463397682504012

Epoch: 6| Step: 5
Training loss: 2.2883510069733015
Validation loss: 2.46362184121212

Epoch: 6| Step: 6
Training loss: 2.472862391745303
Validation loss: 2.4614915980866505

Epoch: 6| Step: 7
Training loss: 2.5318708247157624
Validation loss: 2.463844157083704

Epoch: 6| Step: 8
Training loss: 2.6668591628533047
Validation loss: 2.4635049415308092

Epoch: 6| Step: 9
Training loss: 2.7160812960327485
Validation loss: 2.4619652917927057

Epoch: 6| Step: 10
Training loss: 3.085122302581732
Validation loss: 2.455432851045296

Epoch: 6| Step: 11
Training loss: 2.5692390588919203
Validation loss: 2.462402771387385

Epoch: 6| Step: 12
Training loss: 2.588595680783652
Validation loss: 2.4633813581400914

Epoch: 6| Step: 13
Training loss: 2.237042308859172
Validation loss: 2.468460806474881

Epoch: 101| Step: 0
Training loss: 2.0525717630049276
Validation loss: 2.460959298173549

Epoch: 6| Step: 1
Training loss: 2.5034528729494503
Validation loss: 2.46443707334981

Epoch: 6| Step: 2
Training loss: 2.3938108670536447
Validation loss: 2.462968825556641

Epoch: 6| Step: 3
Training loss: 2.5989238199045928
Validation loss: 2.466359473981041

Epoch: 6| Step: 4
Training loss: 2.6519949547444908
Validation loss: 2.4654252887227144

Epoch: 6| Step: 5
Training loss: 3.10247497204223
Validation loss: 2.463221883416336

Epoch: 6| Step: 6
Training loss: 3.080311574658099
Validation loss: 2.459564924197607

Epoch: 6| Step: 7
Training loss: 2.266822235528263
Validation loss: 2.464597710905813

Epoch: 6| Step: 8
Training loss: 1.89968950093599
Validation loss: 2.45811316479484

Epoch: 6| Step: 9
Training loss: 2.5719774413535954
Validation loss: 2.459366522018639

Epoch: 6| Step: 10
Training loss: 2.188816001676883
Validation loss: 2.462075445687926

Epoch: 6| Step: 11
Training loss: 2.4875596465814227
Validation loss: 2.4599965794469734

Epoch: 6| Step: 12
Training loss: 2.569606695694234
Validation loss: 2.4595856036339936

Epoch: 6| Step: 13
Training loss: 2.9735823002175326
Validation loss: 2.464926079988719

Epoch: 102| Step: 0
Training loss: 2.521347266738508
Validation loss: 2.460472773515968

Epoch: 6| Step: 1
Training loss: 2.504435514552434
Validation loss: 2.460037341312579

Epoch: 6| Step: 2
Training loss: 2.681053058353934
Validation loss: 2.466922341018049

Epoch: 6| Step: 3
Training loss: 1.9470771231519277
Validation loss: 2.4619956027626166

Epoch: 6| Step: 4
Training loss: 2.8487012748350904
Validation loss: 2.4667841170246096

Epoch: 6| Step: 5
Training loss: 2.2487617900342114
Validation loss: 2.468392986123705

Epoch: 6| Step: 6
Training loss: 2.77137185124375
Validation loss: 2.467208791668698

Epoch: 6| Step: 7
Training loss: 2.6096044199704385
Validation loss: 2.468501452718504

Epoch: 6| Step: 8
Training loss: 2.6297149094656476
Validation loss: 2.4683670036583187

Epoch: 6| Step: 9
Training loss: 2.0039635960514146
Validation loss: 2.47338589654406

Epoch: 6| Step: 10
Training loss: 2.3958525504848063
Validation loss: 2.4713757711309663

Epoch: 6| Step: 11
Training loss: 2.9586456205637495
Validation loss: 2.4696448108823112

Epoch: 6| Step: 12
Training loss: 2.9208292186089526
Validation loss: 2.4693040346816733

Epoch: 6| Step: 13
Training loss: 2.3858411208636476
Validation loss: 2.468318708386347

Epoch: 103| Step: 0
Training loss: 2.497223361162042
Validation loss: 2.46519305720889

Epoch: 6| Step: 1
Training loss: 2.6026467936192237
Validation loss: 2.4655057297269143

Epoch: 6| Step: 2
Training loss: 2.1739985561137947
Validation loss: 2.466907417194545

Epoch: 6| Step: 3
Training loss: 2.1126604278863956
Validation loss: 2.462209657312039

Epoch: 6| Step: 4
Training loss: 2.2661645707168456
Validation loss: 2.4657141291855047

Epoch: 6| Step: 5
Training loss: 2.123404633279607
Validation loss: 2.4612902353180797

Epoch: 6| Step: 6
Training loss: 3.0163163889769
Validation loss: 2.4636735188615955

Epoch: 6| Step: 7
Training loss: 2.9693419819936997
Validation loss: 2.4657488419149316

Epoch: 6| Step: 8
Training loss: 2.6801086628648445
Validation loss: 2.462427735599612

Epoch: 6| Step: 9
Training loss: 2.9075862314781733
Validation loss: 2.4588568555648465

Epoch: 6| Step: 10
Training loss: 2.2175343366542766
Validation loss: 2.4597744731593494

Epoch: 6| Step: 11
Training loss: 2.943806473176128
Validation loss: 2.461582708801661

Epoch: 6| Step: 12
Training loss: 2.7904669807693767
Validation loss: 2.4693213497826134

Epoch: 6| Step: 13
Training loss: 2.0691670253884964
Validation loss: 2.4708772499937637

Epoch: 104| Step: 0
Training loss: 2.568140660245397
Validation loss: 2.4653224409711054

Epoch: 6| Step: 1
Training loss: 2.2716128443356673
Validation loss: 2.4714568223658953

Epoch: 6| Step: 2
Training loss: 2.5790949066318887
Validation loss: 2.4652761653363564

Epoch: 6| Step: 3
Training loss: 2.873328842755531
Validation loss: 2.466819297948366

Epoch: 6| Step: 4
Training loss: 2.761870686237862
Validation loss: 2.4647414825552296

Epoch: 6| Step: 5
Training loss: 2.478664336616744
Validation loss: 2.464468966292224

Epoch: 6| Step: 6
Training loss: 2.4517630908951484
Validation loss: 2.4637446303093786

Epoch: 6| Step: 7
Training loss: 2.34055548082101
Validation loss: 2.469130683874488

Epoch: 6| Step: 8
Training loss: 2.402814915050931
Validation loss: 2.466951608524182

Epoch: 6| Step: 9
Training loss: 2.2881164678994397
Validation loss: 2.4663980926324

Epoch: 6| Step: 10
Training loss: 2.328399846795566
Validation loss: 2.463621212170383

Epoch: 6| Step: 11
Training loss: 2.857407857322076
Validation loss: 2.465276826192705

Epoch: 6| Step: 12
Training loss: 2.5167830745562
Validation loss: 2.46478975925066

Epoch: 6| Step: 13
Training loss: 2.7687981284890495
Validation loss: 2.456066870232583

Epoch: 105| Step: 0
Training loss: 2.11821471347635
Validation loss: 2.450226596162758

Epoch: 6| Step: 1
Training loss: 2.857832362219113
Validation loss: 2.4629995113769327

Epoch: 6| Step: 2
Training loss: 2.3346049612980315
Validation loss: 2.4516719718303457

Epoch: 6| Step: 3
Training loss: 2.7898146526065917
Validation loss: 2.4587403677056923

Epoch: 6| Step: 4
Training loss: 2.721356337453655
Validation loss: 2.4643150121185897

Epoch: 6| Step: 5
Training loss: 2.5269650583874395
Validation loss: 2.4597563154329034

Epoch: 6| Step: 6
Training loss: 2.2295286532346883
Validation loss: 2.456971063164804

Epoch: 6| Step: 7
Training loss: 2.946879731353489
Validation loss: 2.465210054628503

Epoch: 6| Step: 8
Training loss: 2.402271499167588
Validation loss: 2.4606165277982637

Epoch: 6| Step: 9
Training loss: 2.6933797347286137
Validation loss: 2.4713174370360003

Epoch: 6| Step: 10
Training loss: 2.716625216427235
Validation loss: 2.4712187579528857

Epoch: 6| Step: 11
Training loss: 2.4638639942198393
Validation loss: 2.470191582960259

Epoch: 6| Step: 12
Training loss: 1.8920180177899268
Validation loss: 2.4757513328342444

Epoch: 6| Step: 13
Training loss: 2.693631031923065
Validation loss: 2.4725608385429103

Epoch: 106| Step: 0
Training loss: 2.36811281758344
Validation loss: 2.471040379521457

Epoch: 6| Step: 1
Training loss: 2.980454351107912
Validation loss: 2.4712146737084253

Epoch: 6| Step: 2
Training loss: 3.106037131063616
Validation loss: 2.4671119535031982

Epoch: 6| Step: 3
Training loss: 2.5310085793737493
Validation loss: 2.468518016876927

Epoch: 6| Step: 4
Training loss: 2.4300356305810733
Validation loss: 2.4671992972673538

Epoch: 6| Step: 5
Training loss: 2.3527132684760654
Validation loss: 2.4600885127718057

Epoch: 6| Step: 6
Training loss: 2.9436406011967002
Validation loss: 2.457160675408254

Epoch: 6| Step: 7
Training loss: 2.1682479541143906
Validation loss: 2.4624935982712928

Epoch: 6| Step: 8
Training loss: 2.3592813233227075
Validation loss: 2.463827843741912

Epoch: 6| Step: 9
Training loss: 2.0742659913626276
Validation loss: 2.460090499521011

Epoch: 6| Step: 10
Training loss: 2.80930808457572
Validation loss: 2.4541375971844377

Epoch: 6| Step: 11
Training loss: 2.439163031940817
Validation loss: 2.4611641754225957

Epoch: 6| Step: 12
Training loss: 2.5208216475118688
Validation loss: 2.4583580425335336

Epoch: 6| Step: 13
Training loss: 2.235939598482375
Validation loss: 2.461889101149304

Epoch: 107| Step: 0
Training loss: 2.476524376801408
Validation loss: 2.4585709591691964

Epoch: 6| Step: 1
Training loss: 2.1420034161138855
Validation loss: 2.454030114947963

Epoch: 6| Step: 2
Training loss: 2.4965397730294367
Validation loss: 2.44965595113283

Epoch: 6| Step: 3
Training loss: 2.9145441961126184
Validation loss: 2.4565316456975013

Epoch: 6| Step: 4
Training loss: 1.9954517623588095
Validation loss: 2.458468745968798

Epoch: 6| Step: 5
Training loss: 2.413617633210717
Validation loss: 2.457036586931555

Epoch: 6| Step: 6
Training loss: 3.1281039272516984
Validation loss: 2.458445503348233

Epoch: 6| Step: 7
Training loss: 2.285674311935473
Validation loss: 2.450981758921373

Epoch: 6| Step: 8
Training loss: 2.4958132972825866
Validation loss: 2.453876647479188

Epoch: 6| Step: 9
Training loss: 2.5808634692575745
Validation loss: 2.459894449714327

Epoch: 6| Step: 10
Training loss: 2.1169255506338125
Validation loss: 2.4607241260527695

Epoch: 6| Step: 11
Training loss: 2.8556826572688148
Validation loss: 2.464792048521133

Epoch: 6| Step: 12
Training loss: 2.7811449009614804
Validation loss: 2.4735646680864867

Epoch: 6| Step: 13
Training loss: 2.7003941495506254
Validation loss: 2.4742896785030997

Epoch: 108| Step: 0
Training loss: 3.0079391650987657
Validation loss: 2.4691699350905973

Epoch: 6| Step: 1
Training loss: 2.8651816188373362
Validation loss: 2.4803323222611424

Epoch: 6| Step: 2
Training loss: 2.834477324568184
Validation loss: 2.4735103214907843

Epoch: 6| Step: 3
Training loss: 2.718779991247564
Validation loss: 2.4740267827787426

Epoch: 6| Step: 4
Training loss: 2.230567408849951
Validation loss: 2.4773111425247

Epoch: 6| Step: 5
Training loss: 2.578289512240517
Validation loss: 2.478922741107045

Epoch: 6| Step: 6
Training loss: 2.1539287512001373
Validation loss: 2.4720766052003653

Epoch: 6| Step: 7
Training loss: 2.8502194102639726
Validation loss: 2.4664574613864185

Epoch: 6| Step: 8
Training loss: 1.8901930386492682
Validation loss: 2.4706295600564028

Epoch: 6| Step: 9
Training loss: 2.0686081114454216
Validation loss: 2.463673131767127

Epoch: 6| Step: 10
Training loss: 2.673140555143195
Validation loss: 2.4617560745907596

Epoch: 6| Step: 11
Training loss: 2.3115869730125382
Validation loss: 2.461104469084645

Epoch: 6| Step: 12
Training loss: 2.468435146679827
Validation loss: 2.4533274216770735

Epoch: 6| Step: 13
Training loss: 2.7219857439709085
Validation loss: 2.4531519538062745

Epoch: 109| Step: 0
Training loss: 2.171067341537166
Validation loss: 2.4577734088638516

Epoch: 6| Step: 1
Training loss: 2.2807644889159744
Validation loss: 2.4567295570510277

Epoch: 6| Step: 2
Training loss: 3.1818111741620463
Validation loss: 2.4508846525024657

Epoch: 6| Step: 3
Training loss: 2.35448803578932
Validation loss: 2.455733368358611

Epoch: 6| Step: 4
Training loss: 2.697191550904026
Validation loss: 2.4626488845534653

Epoch: 6| Step: 5
Training loss: 2.466328942746061
Validation loss: 2.47077176648309

Epoch: 6| Step: 6
Training loss: 2.376847000699432
Validation loss: 2.4735908690484734

Epoch: 6| Step: 7
Training loss: 2.794125873447272
Validation loss: 2.477912478736617

Epoch: 6| Step: 8
Training loss: 2.4733443184203363
Validation loss: 2.480025268318287

Epoch: 6| Step: 9
Training loss: 2.346835838834335
Validation loss: 2.479324854016795

Epoch: 6| Step: 10
Training loss: 2.538155073726523
Validation loss: 2.4712466721358766

Epoch: 6| Step: 11
Training loss: 2.5121602905789118
Validation loss: 2.4768279501955712

Epoch: 6| Step: 12
Training loss: 2.950578348396608
Validation loss: 2.4689640805420554

Epoch: 6| Step: 13
Training loss: 2.4586273491681565
Validation loss: 2.4656195127605818

Epoch: 110| Step: 0
Training loss: 2.733279548286419
Validation loss: 2.4641894455975897

Epoch: 6| Step: 1
Training loss: 2.8252637520605535
Validation loss: 2.4581866516922393

Epoch: 6| Step: 2
Training loss: 2.905821409719728
Validation loss: 2.452843214563562

Epoch: 6| Step: 3
Training loss: 3.049757938472555
Validation loss: 2.4519440791005893

Epoch: 6| Step: 4
Training loss: 2.741755090077979
Validation loss: 2.453284612755823

Epoch: 6| Step: 5
Training loss: 2.3633918168866863
Validation loss: 2.4575372042577244

Epoch: 6| Step: 6
Training loss: 2.371458123001625
Validation loss: 2.4570477459328006

Epoch: 6| Step: 7
Training loss: 2.504868915958041
Validation loss: 2.4519161639492495

Epoch: 6| Step: 8
Training loss: 2.4360735705373426
Validation loss: 2.457583108210089

Epoch: 6| Step: 9
Training loss: 2.183136866613988
Validation loss: 2.4546438888732434

Epoch: 6| Step: 10
Training loss: 2.4324248860789135
Validation loss: 2.457843672483361

Epoch: 6| Step: 11
Training loss: 2.1123186834950096
Validation loss: 2.4588140459483507

Epoch: 6| Step: 12
Training loss: 2.4898075231592443
Validation loss: 2.4589820964823885

Epoch: 6| Step: 13
Training loss: 2.316057716124835
Validation loss: 2.463659518906307

Epoch: 111| Step: 0
Training loss: 2.8026107707663885
Validation loss: 2.4677883802393525

Epoch: 6| Step: 1
Training loss: 2.274461024230714
Validation loss: 2.464229501158131

Epoch: 6| Step: 2
Training loss: 2.5490331574622163
Validation loss: 2.4686563530394956

Epoch: 6| Step: 3
Training loss: 2.845860128057814
Validation loss: 2.4704068736307265

Epoch: 6| Step: 4
Training loss: 2.9184615652283044
Validation loss: 2.468282438015783

Epoch: 6| Step: 5
Training loss: 2.883071019436183
Validation loss: 2.4673413309848766

Epoch: 6| Step: 6
Training loss: 2.386228820163258
Validation loss: 2.460573587304399

Epoch: 6| Step: 7
Training loss: 2.606541357060988
Validation loss: 2.4573939570705354

Epoch: 6| Step: 8
Training loss: 2.956892244606797
Validation loss: 2.4576622537297097

Epoch: 6| Step: 9
Training loss: 1.578761661634468
Validation loss: 2.460367465731833

Epoch: 6| Step: 10
Training loss: 2.378976805193451
Validation loss: 2.453482308252903

Epoch: 6| Step: 11
Training loss: 2.288134702585338
Validation loss: 2.4545822350853497

Epoch: 6| Step: 12
Training loss: 2.4475811559296785
Validation loss: 2.4553911145655114

Epoch: 6| Step: 13
Training loss: 2.289541148895418
Validation loss: 2.4517808216038084

Epoch: 112| Step: 0
Training loss: 2.4478234997057426
Validation loss: 2.4493239287405415

Epoch: 6| Step: 1
Training loss: 2.458907195100338
Validation loss: 2.448227011845708

Epoch: 6| Step: 2
Training loss: 2.497963075522207
Validation loss: 2.449493149470016

Epoch: 6| Step: 3
Training loss: 2.7530256446117853
Validation loss: 2.445557635840483

Epoch: 6| Step: 4
Training loss: 2.5301148494842702
Validation loss: 2.451367714646062

Epoch: 6| Step: 5
Training loss: 2.706418979646332
Validation loss: 2.44898135521789

Epoch: 6| Step: 6
Training loss: 2.833230147633264
Validation loss: 2.4504197789326865

Epoch: 6| Step: 7
Training loss: 2.2436632357427406
Validation loss: 2.4499068563622513

Epoch: 6| Step: 8
Training loss: 2.418121579567863
Validation loss: 2.4585989684647642

Epoch: 6| Step: 9
Training loss: 2.4487954584627305
Validation loss: 2.453353927848075

Epoch: 6| Step: 10
Training loss: 2.8702562421124953
Validation loss: 2.458897822166994

Epoch: 6| Step: 11
Training loss: 2.796716610989583
Validation loss: 2.4646099481464985

Epoch: 6| Step: 12
Training loss: 2.1972532551735147
Validation loss: 2.461081307866762

Epoch: 6| Step: 13
Training loss: 2.295203814130867
Validation loss: 2.4649808737713714

Epoch: 113| Step: 0
Training loss: 2.384632053742942
Validation loss: 2.4633359977783464

Epoch: 6| Step: 1
Training loss: 2.9886357913657893
Validation loss: 2.4599990993203478

Epoch: 6| Step: 2
Training loss: 2.2331125387489483
Validation loss: 2.4653874123660935

Epoch: 6| Step: 3
Training loss: 2.7208983603900463
Validation loss: 2.4585972067816018

Epoch: 6| Step: 4
Training loss: 2.873336642529676
Validation loss: 2.4497270315265127

Epoch: 6| Step: 5
Training loss: 2.7072417382429426
Validation loss: 2.4526432155487896

Epoch: 6| Step: 6
Training loss: 2.5360253582658303
Validation loss: 2.4576197305089296

Epoch: 6| Step: 7
Training loss: 2.628802315866631
Validation loss: 2.4516706103672004

Epoch: 6| Step: 8
Training loss: 1.8175473146432246
Validation loss: 2.4621728854103324

Epoch: 6| Step: 9
Training loss: 1.941504309037167
Validation loss: 2.4662245536789436

Epoch: 6| Step: 10
Training loss: 2.5656147147006174
Validation loss: 2.4530722739998256

Epoch: 6| Step: 11
Training loss: 2.705125189967369
Validation loss: 2.463449663268264

Epoch: 6| Step: 12
Training loss: 2.60354776784711
Validation loss: 2.4596519462927997

Epoch: 6| Step: 13
Training loss: 2.7243275923956864
Validation loss: 2.4659150823630354

Epoch: 114| Step: 0
Training loss: 2.649070731561203
Validation loss: 2.4673631208975704

Epoch: 6| Step: 1
Training loss: 2.5746851311918264
Validation loss: 2.474947349553466

Epoch: 6| Step: 2
Training loss: 2.9671653935561606
Validation loss: 2.4718816185632346

Epoch: 6| Step: 3
Training loss: 2.6100154250701286
Validation loss: 2.4771176587403607

Epoch: 6| Step: 4
Training loss: 2.049871101048626
Validation loss: 2.474889934545663

Epoch: 6| Step: 5
Training loss: 2.7311823926017307
Validation loss: 2.4768231051220364

Epoch: 6| Step: 6
Training loss: 2.054392514248457
Validation loss: 2.486440571621785

Epoch: 6| Step: 7
Training loss: 2.5410556892678864
Validation loss: 2.4833136156652

Epoch: 6| Step: 8
Training loss: 3.249433614822539
Validation loss: 2.4849254070423226

Epoch: 6| Step: 9
Training loss: 2.79063546412643
Validation loss: 2.4821695339067045

Epoch: 6| Step: 10
Training loss: 2.525070654003317
Validation loss: 2.483515240793431

Epoch: 6| Step: 11
Training loss: 2.52430941514238
Validation loss: 2.4844505970327475

Epoch: 6| Step: 12
Training loss: 2.1778185635702756
Validation loss: 2.4796212978936345

Epoch: 6| Step: 13
Training loss: 2.4174508324310713
Validation loss: 2.4775849968232295

Epoch: 115| Step: 0
Training loss: 2.537724914354643
Validation loss: 2.481719751356029

Epoch: 6| Step: 1
Training loss: 2.7095943938484233
Validation loss: 2.4730394176000554

Epoch: 6| Step: 2
Training loss: 2.3630081397266296
Validation loss: 2.4779771200267566

Epoch: 6| Step: 3
Training loss: 3.012094753755769
Validation loss: 2.479829704887928

Epoch: 6| Step: 4
Training loss: 2.2640120453935335
Validation loss: 2.4749291024505484

Epoch: 6| Step: 5
Training loss: 2.7946601494550545
Validation loss: 2.4722527068707683

Epoch: 6| Step: 6
Training loss: 2.6739858576615316
Validation loss: 2.4667403495798004

Epoch: 6| Step: 7
Training loss: 2.162862506977865
Validation loss: 2.4661049334240603

Epoch: 6| Step: 8
Training loss: 2.403885509462241
Validation loss: 2.4658382644251637

Epoch: 6| Step: 9
Training loss: 2.2804269220293616
Validation loss: 2.468801940500342

Epoch: 6| Step: 10
Training loss: 3.1225785601516654
Validation loss: 2.4644172811367993

Epoch: 6| Step: 11
Training loss: 2.458827685635267
Validation loss: 2.461300051186532

Epoch: 6| Step: 12
Training loss: 2.3154500753662712
Validation loss: 2.4620484120381145

Epoch: 6| Step: 13
Training loss: 2.4255770045984555
Validation loss: 2.463559016891794

Epoch: 116| Step: 0
Training loss: 2.7706689044856465
Validation loss: 2.456388347958363

Epoch: 6| Step: 1
Training loss: 3.236873822430214
Validation loss: 2.4614163694761126

Epoch: 6| Step: 2
Training loss: 2.784986835907484
Validation loss: 2.4635075384656586

Epoch: 6| Step: 3
Training loss: 2.0325254690678953
Validation loss: 2.4601302664725133

Epoch: 6| Step: 4
Training loss: 2.340904543065016
Validation loss: 2.4541790716335936

Epoch: 6| Step: 5
Training loss: 3.0055934894993857
Validation loss: 2.4561542346718497

Epoch: 6| Step: 6
Training loss: 2.178535840818095
Validation loss: 2.460298808540356

Epoch: 6| Step: 7
Training loss: 2.684552727923038
Validation loss: 2.4625311883413317

Epoch: 6| Step: 8
Training loss: 2.070751445949832
Validation loss: 2.459383923240931

Epoch: 6| Step: 9
Training loss: 2.174425671922603
Validation loss: 2.4679325295519807

Epoch: 6| Step: 10
Training loss: 2.8094332611526562
Validation loss: 2.4736672534696047

Epoch: 6| Step: 11
Training loss: 2.3458276313816335
Validation loss: 2.4736545469923312

Epoch: 6| Step: 12
Training loss: 2.6716224595678346
Validation loss: 2.473134874862894

Epoch: 6| Step: 13
Training loss: 2.131988813481855
Validation loss: 2.472472350245905

Epoch: 117| Step: 0
Training loss: 2.102723662356025
Validation loss: 2.4689494989966865

Epoch: 6| Step: 1
Training loss: 2.7199713057519137
Validation loss: 2.4634114097263056

Epoch: 6| Step: 2
Training loss: 2.9602686144115733
Validation loss: 2.461178560913771

Epoch: 6| Step: 3
Training loss: 2.6715146541204255
Validation loss: 2.4678574892319456

Epoch: 6| Step: 4
Training loss: 2.3222331822008417
Validation loss: 2.46108765320641

Epoch: 6| Step: 5
Training loss: 2.3455966032745206
Validation loss: 2.4646774856585134

Epoch: 6| Step: 6
Training loss: 2.816680937134597
Validation loss: 2.4606943323200507

Epoch: 6| Step: 7
Training loss: 2.5125029720459624
Validation loss: 2.4602433613319112

Epoch: 6| Step: 8
Training loss: 2.828403016646099
Validation loss: 2.4580796374866876

Epoch: 6| Step: 9
Training loss: 2.0920592356331476
Validation loss: 2.45089793909919

Epoch: 6| Step: 10
Training loss: 2.5212150684030465
Validation loss: 2.4550336632942362

Epoch: 6| Step: 11
Training loss: 2.724502791094037
Validation loss: 2.459815028700458

Epoch: 6| Step: 12
Training loss: 2.3611069947250485
Validation loss: 2.4518596601172464

Epoch: 6| Step: 13
Training loss: 2.0108715458556485
Validation loss: 2.4521290241436664

Epoch: 118| Step: 0
Training loss: 2.001133001792494
Validation loss: 2.4506911334468846

Epoch: 6| Step: 1
Training loss: 2.168050458588339
Validation loss: 2.451439539668558

Epoch: 6| Step: 2
Training loss: 2.336650828780819
Validation loss: 2.4521236441212237

Epoch: 6| Step: 3
Training loss: 2.7949606881422318
Validation loss: 2.456382402988117

Epoch: 6| Step: 4
Training loss: 2.880908037194942
Validation loss: 2.4550920444074715

Epoch: 6| Step: 5
Training loss: 2.439009370292378
Validation loss: 2.4551530703665625

Epoch: 6| Step: 6
Training loss: 2.7414101914523976
Validation loss: 2.4574423861937413

Epoch: 6| Step: 7
Training loss: 2.825691820353175
Validation loss: 2.4650975986836237

Epoch: 6| Step: 8
Training loss: 2.6723257654987056
Validation loss: 2.4710785230034182

Epoch: 6| Step: 9
Training loss: 2.938985732930104
Validation loss: 2.4695234978214353

Epoch: 6| Step: 10
Training loss: 2.48643552154147
Validation loss: 2.469412541716785

Epoch: 6| Step: 11
Training loss: 2.268249359800117
Validation loss: 2.4653946089186287

Epoch: 6| Step: 12
Training loss: 2.7625304759834424
Validation loss: 2.4662157563740257

Epoch: 6| Step: 13
Training loss: 2.081053885536272
Validation loss: 2.4653979372163364

Epoch: 119| Step: 0
Training loss: 2.8739207356671237
Validation loss: 2.466582380822269

Epoch: 6| Step: 1
Training loss: 2.8408745281544547
Validation loss: 2.4632405802084203

Epoch: 6| Step: 2
Training loss: 2.6923489850409394
Validation loss: 2.4581430706385152

Epoch: 6| Step: 3
Training loss: 1.8718394026133827
Validation loss: 2.4589029449643003

Epoch: 6| Step: 4
Training loss: 2.4539839097407214
Validation loss: 2.4560833402849447

Epoch: 6| Step: 5
Training loss: 2.0180450814904343
Validation loss: 2.4634840127167976

Epoch: 6| Step: 6
Training loss: 2.7192948770960963
Validation loss: 2.4554892160143504

Epoch: 6| Step: 7
Training loss: 2.58443008234996
Validation loss: 2.457159866823842

Epoch: 6| Step: 8
Training loss: 2.174317667249943
Validation loss: 2.455663643074409

Epoch: 6| Step: 9
Training loss: 3.3277134887168818
Validation loss: 2.4604788055023876

Epoch: 6| Step: 10
Training loss: 2.392742341303224
Validation loss: 2.4627078999704333

Epoch: 6| Step: 11
Training loss: 2.328686537643181
Validation loss: 2.4664180139489247

Epoch: 6| Step: 12
Training loss: 2.095696412828545
Validation loss: 2.467183505395897

Epoch: 6| Step: 13
Training loss: 2.699313260092433
Validation loss: 2.4701749495917853

Epoch: 120| Step: 0
Training loss: 2.2668073002813642
Validation loss: 2.475138626968639

Epoch: 6| Step: 1
Training loss: 2.3280576145737677
Validation loss: 2.4746385837066565

Epoch: 6| Step: 2
Training loss: 2.7311275707887326
Validation loss: 2.4720597916247327

Epoch: 6| Step: 3
Training loss: 1.8418352561988742
Validation loss: 2.4754542810467113

Epoch: 6| Step: 4
Training loss: 2.2488044105380682
Validation loss: 2.4735666118828967

Epoch: 6| Step: 5
Training loss: 2.5291814947138778
Validation loss: 2.4694167013532535

Epoch: 6| Step: 6
Training loss: 3.0068326072265577
Validation loss: 2.4708755292286084

Epoch: 6| Step: 7
Training loss: 2.2681500274715383
Validation loss: 2.467290809212676

Epoch: 6| Step: 8
Training loss: 2.693301304667677
Validation loss: 2.468731183974272

Epoch: 6| Step: 9
Training loss: 3.008454489623816
Validation loss: 2.4749607397748186

Epoch: 6| Step: 10
Training loss: 1.8041239597590055
Validation loss: 2.4658893316196684

Epoch: 6| Step: 11
Training loss: 2.3361382541699958
Validation loss: 2.46806945756317

Epoch: 6| Step: 12
Training loss: 2.995311729498711
Validation loss: 2.4592592198393377

Epoch: 6| Step: 13
Training loss: 2.8656921645086157
Validation loss: 2.456407946037291

Epoch: 121| Step: 0
Training loss: 2.327492544815793
Validation loss: 2.46057710783943

Epoch: 6| Step: 1
Training loss: 2.8730598205648183
Validation loss: 2.455091744979188

Epoch: 6| Step: 2
Training loss: 2.3522050049506613
Validation loss: 2.461802545523825

Epoch: 6| Step: 3
Training loss: 2.2892699684671074
Validation loss: 2.4596557104762176

Epoch: 6| Step: 4
Training loss: 2.2851632403405366
Validation loss: 2.451931689512413

Epoch: 6| Step: 5
Training loss: 2.7583072207810075
Validation loss: 2.461359801212447

Epoch: 6| Step: 6
Training loss: 2.625041870509865
Validation loss: 2.450383932821171

Epoch: 6| Step: 7
Training loss: 2.5687995330415703
Validation loss: 2.455750228991079

Epoch: 6| Step: 8
Training loss: 2.8302971522818665
Validation loss: 2.466030538429168

Epoch: 6| Step: 9
Training loss: 2.680146647896293
Validation loss: 2.4725821806875707

Epoch: 6| Step: 10
Training loss: 2.4797537186067182
Validation loss: 2.468201218142465

Epoch: 6| Step: 11
Training loss: 2.4490305753756436
Validation loss: 2.4693789586310877

Epoch: 6| Step: 12
Training loss: 2.7202448626703664
Validation loss: 2.4734769064968583

Epoch: 6| Step: 13
Training loss: 2.088788088548842
Validation loss: 2.4728228054428865

Epoch: 122| Step: 0
Training loss: 2.4588932326486708
Validation loss: 2.467891052768385

Epoch: 6| Step: 1
Training loss: 2.811367569869696
Validation loss: 2.470540101876276

Epoch: 6| Step: 2
Training loss: 1.813647334573434
Validation loss: 2.4671496825876247

Epoch: 6| Step: 3
Training loss: 2.3197888126935275
Validation loss: 2.468196130743318

Epoch: 6| Step: 4
Training loss: 2.4591680068819572
Validation loss: 2.4704268671722605

Epoch: 6| Step: 5
Training loss: 2.384108994270967
Validation loss: 2.4658487954084123

Epoch: 6| Step: 6
Training loss: 3.1162326660163804
Validation loss: 2.4702049507022936

Epoch: 6| Step: 7
Training loss: 2.085516612134688
Validation loss: 2.4674892666991344

Epoch: 6| Step: 8
Training loss: 2.594510529942812
Validation loss: 2.463330029247513

Epoch: 6| Step: 9
Training loss: 2.5712855390096445
Validation loss: 2.4598619320743946

Epoch: 6| Step: 10
Training loss: 3.2583780140914964
Validation loss: 2.4643403439491944

Epoch: 6| Step: 11
Training loss: 2.017332315011449
Validation loss: 2.461172990784066

Epoch: 6| Step: 12
Training loss: 2.7743982504609255
Validation loss: 2.4622654394886476

Epoch: 6| Step: 13
Training loss: 2.148174699836032
Validation loss: 2.464941813779713

Epoch: 123| Step: 0
Training loss: 2.664258574935975
Validation loss: 2.462483359537201

Epoch: 6| Step: 1
Training loss: 2.5458081591204853
Validation loss: 2.4614592145503065

Epoch: 6| Step: 2
Training loss: 2.2987112373705663
Validation loss: 2.4697310514397963

Epoch: 6| Step: 3
Training loss: 2.3545231732070393
Validation loss: 2.4628874304536676

Epoch: 6| Step: 4
Training loss: 2.881299291093933
Validation loss: 2.4556832226648693

Epoch: 6| Step: 5
Training loss: 2.4460359423375766
Validation loss: 2.4689559689645315

Epoch: 6| Step: 6
Training loss: 3.4281831192194745
Validation loss: 2.4598572232001943

Epoch: 6| Step: 7
Training loss: 2.5100424291056607
Validation loss: 2.4560162459715276

Epoch: 6| Step: 8
Training loss: 1.9443387578065305
Validation loss: 2.4596542645781185

Epoch: 6| Step: 9
Training loss: 2.347250803435482
Validation loss: 2.4650462652279064

Epoch: 6| Step: 10
Training loss: 2.3459145278385205
Validation loss: 2.4638641071136527

Epoch: 6| Step: 11
Training loss: 1.9519995536725518
Validation loss: 2.4611934145313534

Epoch: 6| Step: 12
Training loss: 2.681073600461675
Validation loss: 2.4569670765334384

Epoch: 6| Step: 13
Training loss: 2.5231268251576413
Validation loss: 2.4655381971066905

Epoch: 124| Step: 0
Training loss: 2.729726796994613
Validation loss: 2.4504025816213963

Epoch: 6| Step: 1
Training loss: 2.812882630493517
Validation loss: 2.4576474838655074

Epoch: 6| Step: 2
Training loss: 2.3979502627231235
Validation loss: 2.459398658441531

Epoch: 6| Step: 3
Training loss: 2.455664905234535
Validation loss: 2.458549834809641

Epoch: 6| Step: 4
Training loss: 2.0419949896860254
Validation loss: 2.4630930023580055

Epoch: 6| Step: 5
Training loss: 3.1286215587165565
Validation loss: 2.462827410934145

Epoch: 6| Step: 6
Training loss: 2.1083454021163717
Validation loss: 2.466634399185827

Epoch: 6| Step: 7
Training loss: 1.9343487123049707
Validation loss: 2.4636736882154064

Epoch: 6| Step: 8
Training loss: 2.7457986295954644
Validation loss: 2.4679410389689598

Epoch: 6| Step: 9
Training loss: 2.899678008521047
Validation loss: 2.4655741210509943

Epoch: 6| Step: 10
Training loss: 2.101120913033451
Validation loss: 2.4644529231296066

Epoch: 6| Step: 11
Training loss: 2.0917683803352163
Validation loss: 2.4618239486526137

Epoch: 6| Step: 12
Training loss: 2.5088122505253816
Validation loss: 2.460381629774056

Epoch: 6| Step: 13
Training loss: 2.911050093434391
Validation loss: 2.4589183779326884

Epoch: 125| Step: 0
Training loss: 2.629411713978792
Validation loss: 2.45778100764944

Epoch: 6| Step: 1
Training loss: 2.8701688561294287
Validation loss: 2.4589137561324206

Epoch: 6| Step: 2
Training loss: 2.3510520609190806
Validation loss: 2.451659037899939

Epoch: 6| Step: 3
Training loss: 2.368567942364165
Validation loss: 2.4540350131185114

Epoch: 6| Step: 4
Training loss: 2.660839648415988
Validation loss: 2.454931739847441

Epoch: 6| Step: 5
Training loss: 2.432028767810882
Validation loss: 2.4529868408951185

Epoch: 6| Step: 6
Training loss: 2.1867135950418204
Validation loss: 2.4494407670806337

Epoch: 6| Step: 7
Training loss: 2.06808582258425
Validation loss: 2.4570680584194733

Epoch: 6| Step: 8
Training loss: 2.611673039873044
Validation loss: 2.4520933893775925

Epoch: 6| Step: 9
Training loss: 2.7099612894143914
Validation loss: 2.458472867551037

Epoch: 6| Step: 10
Training loss: 2.9113401731331106
Validation loss: 2.4586065808609807

Epoch: 6| Step: 11
Training loss: 2.837325668021056
Validation loss: 2.461460828894212

Epoch: 6| Step: 12
Training loss: 2.0426858447164795
Validation loss: 2.465201294000597

Epoch: 6| Step: 13
Training loss: 2.516786484887078
Validation loss: 2.4625149631207

Epoch: 126| Step: 0
Training loss: 2.197219400555449
Validation loss: 2.457509991330248

Epoch: 6| Step: 1
Training loss: 1.6068102204757237
Validation loss: 2.4613766071594014

Epoch: 6| Step: 2
Training loss: 2.7712871973331343
Validation loss: 2.4540031707870655

Epoch: 6| Step: 3
Training loss: 2.4246630955689823
Validation loss: 2.4573734289788756

Epoch: 6| Step: 4
Training loss: 2.8947999240079825
Validation loss: 2.45284535298253

Epoch: 6| Step: 5
Training loss: 2.3364534860720263
Validation loss: 2.452055566371763

Epoch: 6| Step: 6
Training loss: 2.8667164761259856
Validation loss: 2.4551989217865606

Epoch: 6| Step: 7
Training loss: 2.457077632399551
Validation loss: 2.453743259409661

Epoch: 6| Step: 8
Training loss: 2.7775244459558013
Validation loss: 2.4526127565927056

Epoch: 6| Step: 9
Training loss: 2.198288932720167
Validation loss: 2.461610925964962

Epoch: 6| Step: 10
Training loss: 2.261271325427833
Validation loss: 2.4561678082382215

Epoch: 6| Step: 11
Training loss: 2.7296456554854327
Validation loss: 2.453274133121748

Epoch: 6| Step: 12
Training loss: 2.631218945825136
Validation loss: 2.457588831996088

Epoch: 6| Step: 13
Training loss: 2.737607429521513
Validation loss: 2.4570443982377483

Epoch: 127| Step: 0
Training loss: 2.7238805308758267
Validation loss: 2.4630437486216743

Epoch: 6| Step: 1
Training loss: 2.796610217640567
Validation loss: 2.462631377315666

Epoch: 6| Step: 2
Training loss: 2.816025368256527
Validation loss: 2.462718613759116

Epoch: 6| Step: 3
Training loss: 1.9468911747390647
Validation loss: 2.4655109999635707

Epoch: 6| Step: 4
Training loss: 2.5362392281293666
Validation loss: 2.4641424713993807

Epoch: 6| Step: 5
Training loss: 2.3904880658992607
Validation loss: 2.461293690249927

Epoch: 6| Step: 6
Training loss: 2.4520080356061738
Validation loss: 2.465527858198445

Epoch: 6| Step: 7
Training loss: 2.4019812829958322
Validation loss: 2.4621743943844074

Epoch: 6| Step: 8
Training loss: 2.8831303945871722
Validation loss: 2.4550913889022628

Epoch: 6| Step: 9
Training loss: 2.700436263689945
Validation loss: 2.4574242759154803

Epoch: 6| Step: 10
Training loss: 2.7488524036492645
Validation loss: 2.456456443128704

Epoch: 6| Step: 11
Training loss: 1.870639945090637
Validation loss: 2.4589317988803807

Epoch: 6| Step: 12
Training loss: 2.7246872538712936
Validation loss: 2.446452109867022

Epoch: 6| Step: 13
Training loss: 1.9419874092817486
Validation loss: 2.4616255348383325

Epoch: 128| Step: 0
Training loss: 2.1459684854061685
Validation loss: 2.4578970236369764

Epoch: 6| Step: 1
Training loss: 2.662659306304773
Validation loss: 2.459363928789734

Epoch: 6| Step: 2
Training loss: 2.0677282939367028
Validation loss: 2.4549730713915436

Epoch: 6| Step: 3
Training loss: 2.64754324700296
Validation loss: 2.459857417047681

Epoch: 6| Step: 4
Training loss: 2.489226396795678
Validation loss: 2.4570342095725666

Epoch: 6| Step: 5
Training loss: 2.3400432520724537
Validation loss: 2.4622976026284085

Epoch: 6| Step: 6
Training loss: 3.1918517680729765
Validation loss: 2.4608505960288083

Epoch: 6| Step: 7
Training loss: 2.449925370928698
Validation loss: 2.4618517433882876

Epoch: 6| Step: 8
Training loss: 1.8822207965467614
Validation loss: 2.4573845460249886

Epoch: 6| Step: 9
Training loss: 2.7360426694654714
Validation loss: 2.4582806409156444

Epoch: 6| Step: 10
Training loss: 2.8107908353769866
Validation loss: 2.467348175582764

Epoch: 6| Step: 11
Training loss: 2.179342810062417
Validation loss: 2.4559700862199745

Epoch: 6| Step: 12
Training loss: 2.7443665109432303
Validation loss: 2.4519112939585037

Epoch: 6| Step: 13
Training loss: 2.5824642770517725
Validation loss: 2.454079136693488

Epoch: 129| Step: 0
Training loss: 2.0945337451320927
Validation loss: 2.459066214397671

Epoch: 6| Step: 1
Training loss: 2.4446893940162076
Validation loss: 2.4540018672910104

Epoch: 6| Step: 2
Training loss: 2.9756013214444788
Validation loss: 2.4618232223051395

Epoch: 6| Step: 3
Training loss: 2.357327008175599
Validation loss: 2.4635812354320894

Epoch: 6| Step: 4
Training loss: 2.6189466343439034
Validation loss: 2.464189735857617

Epoch: 6| Step: 5
Training loss: 2.3271250785650994
Validation loss: 2.4600345630336973

Epoch: 6| Step: 6
Training loss: 2.6765303182810642
Validation loss: 2.462521982498291

Epoch: 6| Step: 7
Training loss: 3.1308342450157247
Validation loss: 2.469774041876512

Epoch: 6| Step: 8
Training loss: 2.125865984219449
Validation loss: 2.4605941532944575

Epoch: 6| Step: 9
Training loss: 2.5158165331601046
Validation loss: 2.4617897293832756

Epoch: 6| Step: 10
Training loss: 2.0800035060339503
Validation loss: 2.460091727105084

Epoch: 6| Step: 11
Training loss: 2.8196773923489595
Validation loss: 2.4623258842192746

Epoch: 6| Step: 12
Training loss: 2.0143435640926945
Validation loss: 2.466071933711955

Epoch: 6| Step: 13
Training loss: 2.5840582804588332
Validation loss: 2.466120933613794

Epoch: 130| Step: 0
Training loss: 2.3001968341027808
Validation loss: 2.4544242609279334

Epoch: 6| Step: 1
Training loss: 2.460410603626952
Validation loss: 2.4631359311951577

Epoch: 6| Step: 2
Training loss: 2.3090435693594427
Validation loss: 2.457322613331385

Epoch: 6| Step: 3
Training loss: 2.7622888993607266
Validation loss: 2.46099490144179

Epoch: 6| Step: 4
Training loss: 2.6979728506318557
Validation loss: 2.4627137328596604

Epoch: 6| Step: 5
Training loss: 2.406619947654538
Validation loss: 2.4594926740518255

Epoch: 6| Step: 6
Training loss: 2.325399967369
Validation loss: 2.4579223569459345

Epoch: 6| Step: 7
Training loss: 2.9113848864049405
Validation loss: 2.4581458672204297

Epoch: 6| Step: 8
Training loss: 2.8778264622966283
Validation loss: 2.4646549948568235

Epoch: 6| Step: 9
Training loss: 2.3694496550756745
Validation loss: 2.464450375562517

Epoch: 6| Step: 10
Training loss: 2.413295784582627
Validation loss: 2.463101205840745

Epoch: 6| Step: 11
Training loss: 2.3022460419704394
Validation loss: 2.461699239721518

Epoch: 6| Step: 12
Training loss: 2.2574135186918145
Validation loss: 2.4642381120510923

Epoch: 6| Step: 13
Training loss: 2.4644119198786236
Validation loss: 2.4612993731179307

Epoch: 131| Step: 0
Training loss: 2.551632889420864
Validation loss: 2.4665734075838897

Epoch: 6| Step: 1
Training loss: 2.462180559381182
Validation loss: 2.4645542351581535

Epoch: 6| Step: 2
Training loss: 2.936658373448255
Validation loss: 2.4629497556271205

Epoch: 6| Step: 3
Training loss: 2.818339748935477
Validation loss: 2.464770880749003

Epoch: 6| Step: 4
Training loss: 2.639094072585572
Validation loss: 2.4614142385034437

Epoch: 6| Step: 5
Training loss: 2.619165748970663
Validation loss: 2.460927375893734

Epoch: 6| Step: 6
Training loss: 2.432799477563794
Validation loss: 2.4586335715451013

Epoch: 6| Step: 7
Training loss: 2.845070836102146
Validation loss: 2.4624263558776183

Epoch: 6| Step: 8
Training loss: 2.0118315729891934
Validation loss: 2.4594392605714885

Epoch: 6| Step: 9
Training loss: 2.409718181546867
Validation loss: 2.4627407430711625

Epoch: 6| Step: 10
Training loss: 2.2321803912685505
Validation loss: 2.4587222346651805

Epoch: 6| Step: 11
Training loss: 2.149415060803096
Validation loss: 2.4573949434495113

Epoch: 6| Step: 12
Training loss: 2.2589571997854776
Validation loss: 2.4581996401839845

Epoch: 6| Step: 13
Training loss: 2.4935265653704697
Validation loss: 2.464537217092202

Epoch: 132| Step: 0
Training loss: 2.1998617475591558
Validation loss: 2.4639743373905114

Epoch: 6| Step: 1
Training loss: 2.253277193344482
Validation loss: 2.458990022805828

Epoch: 6| Step: 2
Training loss: 2.595984909342913
Validation loss: 2.473129596771809

Epoch: 6| Step: 3
Training loss: 2.570632497237217
Validation loss: 2.465941509629822

Epoch: 6| Step: 4
Training loss: 2.3701199035251745
Validation loss: 2.4640266285222223

Epoch: 6| Step: 5
Training loss: 2.1132188600930752
Validation loss: 2.4650977276405586

Epoch: 6| Step: 6
Training loss: 2.3059535800069724
Validation loss: 2.466518149759135

Epoch: 6| Step: 7
Training loss: 3.280029651228698
Validation loss: 2.4650063438411274

Epoch: 6| Step: 8
Training loss: 2.5353631403927346
Validation loss: 2.4708289233763927

Epoch: 6| Step: 9
Training loss: 2.9801591584887404
Validation loss: 2.4571018744559363

Epoch: 6| Step: 10
Training loss: 2.324959978148849
Validation loss: 2.462607020264893

Epoch: 6| Step: 11
Training loss: 2.5661598679535746
Validation loss: 2.4619645170665394

Epoch: 6| Step: 12
Training loss: 2.5073232203168656
Validation loss: 2.455246868216871

Epoch: 6| Step: 13
Training loss: 2.2897147333299133
Validation loss: 2.451268151407507

Epoch: 133| Step: 0
Training loss: 2.3357088621384032
Validation loss: 2.453311046502567

Epoch: 6| Step: 1
Training loss: 2.6507864019162284
Validation loss: 2.4547696118266624

Epoch: 6| Step: 2
Training loss: 2.138480009765522
Validation loss: 2.4516473276096056

Epoch: 6| Step: 3
Training loss: 2.502468321120503
Validation loss: 2.456428393226598

Epoch: 6| Step: 4
Training loss: 2.4846500268489344
Validation loss: 2.4624581296301216

Epoch: 6| Step: 5
Training loss: 2.2534694413643246
Validation loss: 2.4550869541216827

Epoch: 6| Step: 6
Training loss: 2.6602648742277157
Validation loss: 2.4558783627789276

Epoch: 6| Step: 7
Training loss: 2.4933212714829973
Validation loss: 2.4541129777235255

Epoch: 6| Step: 8
Training loss: 2.638605372496535
Validation loss: 2.4602003497179576

Epoch: 6| Step: 9
Training loss: 2.3814863673365965
Validation loss: 2.4569480894052242

Epoch: 6| Step: 10
Training loss: 2.2209964179396997
Validation loss: 2.460809274390717

Epoch: 6| Step: 11
Training loss: 2.5248868104851194
Validation loss: 2.4709291618595333

Epoch: 6| Step: 12
Training loss: 3.033971449695656
Validation loss: 2.468014056128111

Epoch: 6| Step: 13
Training loss: 2.545902370717618
Validation loss: 2.469116924075377

Epoch: 134| Step: 0
Training loss: 2.040408687605813
Validation loss: 2.465675113041395

Epoch: 6| Step: 1
Training loss: 2.2547890241768926
Validation loss: 2.473210325429735

Epoch: 6| Step: 2
Training loss: 2.844946640935557
Validation loss: 2.4727570571960693

Epoch: 6| Step: 3
Training loss: 2.523347268395725
Validation loss: 2.471629559870077

Epoch: 6| Step: 4
Training loss: 3.0438206157286705
Validation loss: 2.470208023181514

Epoch: 6| Step: 5
Training loss: 2.322218603327039
Validation loss: 2.4599724224529687

Epoch: 6| Step: 6
Training loss: 1.9533973198827093
Validation loss: 2.4709751306108507

Epoch: 6| Step: 7
Training loss: 2.8066568500259534
Validation loss: 2.4625418948306614

Epoch: 6| Step: 8
Training loss: 2.443016656369903
Validation loss: 2.4720039491580748

Epoch: 6| Step: 9
Training loss: 2.6475502711089955
Validation loss: 2.476552375588245

Epoch: 6| Step: 10
Training loss: 2.2180548640105235
Validation loss: 2.459418563747772

Epoch: 6| Step: 11
Training loss: 3.326272368242988
Validation loss: 2.4678502113024643

Epoch: 6| Step: 12
Training loss: 2.09625640704382
Validation loss: 2.4692044222165253

Epoch: 6| Step: 13
Training loss: 2.385034444468266
Validation loss: 2.465107770141143

Epoch: 135| Step: 0
Training loss: 3.056945434582477
Validation loss: 2.4517947556790665

Epoch: 6| Step: 1
Training loss: 2.146631765121367
Validation loss: 2.4512998590581185

Epoch: 6| Step: 2
Training loss: 2.3560061624572675
Validation loss: 2.4543157793436463

Epoch: 6| Step: 3
Training loss: 3.452333247351571
Validation loss: 2.450955835069789

Epoch: 6| Step: 4
Training loss: 2.110254175144743
Validation loss: 2.4531402749397957

Epoch: 6| Step: 5
Training loss: 2.535185309715238
Validation loss: 2.456740596138486

Epoch: 6| Step: 6
Training loss: 2.999565728862693
Validation loss: 2.448162802367929

Epoch: 6| Step: 7
Training loss: 1.8996445699824465
Validation loss: 2.4528409789417394

Epoch: 6| Step: 8
Training loss: 2.6106478390597614
Validation loss: 2.4499848469116374

Epoch: 6| Step: 9
Training loss: 2.3009721194306056
Validation loss: 2.455207086907255

Epoch: 6| Step: 10
Training loss: 2.3346443807270627
Validation loss: 2.449178805879836

Epoch: 6| Step: 11
Training loss: 2.6297312288054067
Validation loss: 2.440998240646193

Epoch: 6| Step: 12
Training loss: 2.106937610892028
Validation loss: 2.4440402620363337

Epoch: 6| Step: 13
Training loss: 2.079776881795679
Validation loss: 2.448119367499353

Epoch: 136| Step: 0
Training loss: 2.3613061138733213
Validation loss: 2.447487153593333

Epoch: 6| Step: 1
Training loss: 2.477823219348296
Validation loss: 2.4431412942817423

Epoch: 6| Step: 2
Training loss: 2.6393394275231024
Validation loss: 2.446506505300965

Epoch: 6| Step: 3
Training loss: 2.530028908153527
Validation loss: 2.444344634730124

Epoch: 6| Step: 4
Training loss: 2.4950180959714205
Validation loss: 2.447099814515094

Epoch: 6| Step: 5
Training loss: 2.384586261874337
Validation loss: 2.448659424339127

Epoch: 6| Step: 6
Training loss: 2.653630659355128
Validation loss: 2.4474106015278485

Epoch: 6| Step: 7
Training loss: 2.4918834536686263
Validation loss: 2.450386138251902

Epoch: 6| Step: 8
Training loss: 2.0480387633499717
Validation loss: 2.454919276312037

Epoch: 6| Step: 9
Training loss: 2.983549153568249
Validation loss: 2.450179102897113

Epoch: 6| Step: 10
Training loss: 2.6223894489795256
Validation loss: 2.448556456590296

Epoch: 6| Step: 11
Training loss: 2.471969822585214
Validation loss: 2.444842341097911

Epoch: 6| Step: 12
Training loss: 2.122536184588832
Validation loss: 2.4452073680958875

Epoch: 6| Step: 13
Training loss: 2.721257511351088
Validation loss: 2.4482342344950436

Epoch: 137| Step: 0
Training loss: 2.4433721577091188
Validation loss: 2.453625961954172

Epoch: 6| Step: 1
Training loss: 2.268737549130242
Validation loss: 2.450102967414602

Epoch: 6| Step: 2
Training loss: 1.889421190106713
Validation loss: 2.4579247011125696

Epoch: 6| Step: 3
Training loss: 2.41651806429286
Validation loss: 2.4536638579208923

Epoch: 6| Step: 4
Training loss: 2.8497278451641495
Validation loss: 2.4528125798871825

Epoch: 6| Step: 5
Training loss: 2.7935313265088473
Validation loss: 2.4552625021922245

Epoch: 6| Step: 6
Training loss: 2.596563077663275
Validation loss: 2.4581112087783334

Epoch: 6| Step: 7
Training loss: 2.638835052030295
Validation loss: 2.4634468646421754

Epoch: 6| Step: 8
Training loss: 2.131135274086917
Validation loss: 2.464719290596228

Epoch: 6| Step: 9
Training loss: 2.9293670072094495
Validation loss: 2.4702185918374

Epoch: 6| Step: 10
Training loss: 2.7429824937407123
Validation loss: 2.4725259000259294

Epoch: 6| Step: 11
Training loss: 1.8522039018770093
Validation loss: 2.467010223030399

Epoch: 6| Step: 12
Training loss: 2.7709051902077264
Validation loss: 2.4682556494371886

Epoch: 6| Step: 13
Training loss: 2.7267398284697215
Validation loss: 2.461432416286807

Epoch: 138| Step: 0
Training loss: 3.1979630722671692
Validation loss: 2.4567545061267775

Epoch: 6| Step: 1
Training loss: 2.0260361881394506
Validation loss: 2.4618406707263154

Epoch: 6| Step: 2
Training loss: 1.9848781158155668
Validation loss: 2.454620893332727

Epoch: 6| Step: 3
Training loss: 2.5181858462710536
Validation loss: 2.4597667028319656

Epoch: 6| Step: 4
Training loss: 2.090281788205017
Validation loss: 2.450977803079322

Epoch: 6| Step: 5
Training loss: 2.6430304591963645
Validation loss: 2.4554822736289417

Epoch: 6| Step: 6
Training loss: 2.118998636156177
Validation loss: 2.4577197801707604

Epoch: 6| Step: 7
Training loss: 2.8039912897810018
Validation loss: 2.455326639291444

Epoch: 6| Step: 8
Training loss: 2.242151027475845
Validation loss: 2.448675570977141

Epoch: 6| Step: 9
Training loss: 2.002069118209797
Validation loss: 2.4503446239286903

Epoch: 6| Step: 10
Training loss: 2.6667872838716313
Validation loss: 2.459157730063895

Epoch: 6| Step: 11
Training loss: 2.984146189281602
Validation loss: 2.459801459105691

Epoch: 6| Step: 12
Training loss: 2.423844748539137
Validation loss: 2.4547900807561356

Epoch: 6| Step: 13
Training loss: 2.9240972054914804
Validation loss: 2.454269603800557

Epoch: 139| Step: 0
Training loss: 2.1790094448968564
Validation loss: 2.4557410058187545

Epoch: 6| Step: 1
Training loss: 3.251938461866341
Validation loss: 2.45683445468345

Epoch: 6| Step: 2
Training loss: 2.775533995928519
Validation loss: 2.46295624135576

Epoch: 6| Step: 3
Training loss: 2.5530488244485867
Validation loss: 2.4562021220089307

Epoch: 6| Step: 4
Training loss: 2.1429962612587645
Validation loss: 2.4631217023286487

Epoch: 6| Step: 5
Training loss: 2.7940384956935507
Validation loss: 2.4567485701297795

Epoch: 6| Step: 6
Training loss: 2.49699077695855
Validation loss: 2.457716966935986

Epoch: 6| Step: 7
Training loss: 2.6053184912046343
Validation loss: 2.4598375556779124

Epoch: 6| Step: 8
Training loss: 1.9454301545745931
Validation loss: 2.463807675689575

Epoch: 6| Step: 9
Training loss: 2.825754426128043
Validation loss: 2.4626195901144867

Epoch: 6| Step: 10
Training loss: 2.023685631579526
Validation loss: 2.460868632649786

Epoch: 6| Step: 11
Training loss: 2.6041321307117506
Validation loss: 2.4608805493420616

Epoch: 6| Step: 12
Training loss: 1.6417184727945273
Validation loss: 2.460342529067102

Epoch: 6| Step: 13
Training loss: 2.5884647063718704
Validation loss: 2.4584120533079874

Epoch: 140| Step: 0
Training loss: 2.009972267908498
Validation loss: 2.457335202126699

Epoch: 6| Step: 1
Training loss: 2.7602631916110627
Validation loss: 2.460883924112993

Epoch: 6| Step: 2
Training loss: 2.2842130882818754
Validation loss: 2.4632931208221156

Epoch: 6| Step: 3
Training loss: 2.7171343683594698
Validation loss: 2.4657613151718953

Epoch: 6| Step: 4
Training loss: 2.4064189306633077
Validation loss: 2.4657631039671393

Epoch: 6| Step: 5
Training loss: 2.8391837884065527
Validation loss: 2.471912708147571

Epoch: 6| Step: 6
Training loss: 2.4210481278096085
Validation loss: 2.4636030666665247

Epoch: 6| Step: 7
Training loss: 3.1476465182936244
Validation loss: 2.4702067523659648

Epoch: 6| Step: 8
Training loss: 2.086567872768796
Validation loss: 2.465901417421319

Epoch: 6| Step: 9
Training loss: 2.7800314301834104
Validation loss: 2.468958970573209

Epoch: 6| Step: 10
Training loss: 2.08032374118394
Validation loss: 2.4720515133882164

Epoch: 6| Step: 11
Training loss: 2.2049849214276147
Validation loss: 2.461323912624558

Epoch: 6| Step: 12
Training loss: 2.3503695785799956
Validation loss: 2.4623800742288156

Epoch: 6| Step: 13
Training loss: 2.677196802092175
Validation loss: 2.4672432176266947

Epoch: 141| Step: 0
Training loss: 2.1335627874518406
Validation loss: 2.4622079627664273

Epoch: 6| Step: 1
Training loss: 2.166252267773603
Validation loss: 2.454862088915606

Epoch: 6| Step: 2
Training loss: 2.6077723349740305
Validation loss: 2.4561489443580857

Epoch: 6| Step: 3
Training loss: 2.574161511538814
Validation loss: 2.466158395887615

Epoch: 6| Step: 4
Training loss: 2.6175438481750675
Validation loss: 2.4580814157064146

Epoch: 6| Step: 5
Training loss: 1.8710445644468214
Validation loss: 2.459479450042007

Epoch: 6| Step: 6
Training loss: 2.674540656750122
Validation loss: 2.4585439031473495

Epoch: 6| Step: 7
Training loss: 2.5964381065051145
Validation loss: 2.458861816842535

Epoch: 6| Step: 8
Training loss: 2.358708369580583
Validation loss: 2.468086201721943

Epoch: 6| Step: 9
Training loss: 2.5182591265237804
Validation loss: 2.46658806761533

Epoch: 6| Step: 10
Training loss: 2.7897117564293197
Validation loss: 2.4650030069545643

Epoch: 6| Step: 11
Training loss: 2.563773699297049
Validation loss: 2.4617300705007295

Epoch: 6| Step: 12
Training loss: 2.667910663201399
Validation loss: 2.4640546322448817

Epoch: 6| Step: 13
Training loss: 2.6025354897079134
Validation loss: 2.470854349245323

Epoch: 142| Step: 0
Training loss: 2.7225028258870347
Validation loss: 2.46391897289494

Epoch: 6| Step: 1
Training loss: 3.0881792255729352
Validation loss: 2.4627580559123468

Epoch: 6| Step: 2
Training loss: 2.5581747629268374
Validation loss: 2.4607379650785517

Epoch: 6| Step: 3
Training loss: 2.0992149656806114
Validation loss: 2.4598131144228743

Epoch: 6| Step: 4
Training loss: 2.27057312574071
Validation loss: 2.4568949602471544

Epoch: 6| Step: 5
Training loss: 2.042567722500524
Validation loss: 2.465639045523835

Epoch: 6| Step: 6
Training loss: 2.8126301205522637
Validation loss: 2.465215962191659

Epoch: 6| Step: 7
Training loss: 2.6239887287781714
Validation loss: 2.4594154616327213

Epoch: 6| Step: 8
Training loss: 2.5666005889841963
Validation loss: 2.462311263363213

Epoch: 6| Step: 9
Training loss: 1.6744392196454505
Validation loss: 2.4747672408244856

Epoch: 6| Step: 10
Training loss: 2.721779724625923
Validation loss: 2.4760380459380382

Epoch: 6| Step: 11
Training loss: 2.41346550630459
Validation loss: 2.4773171816319186

Epoch: 6| Step: 12
Training loss: 2.02136996770153
Validation loss: 2.4767778946743952

Epoch: 6| Step: 13
Training loss: 2.969304404442265
Validation loss: 2.4724619840830724

Epoch: 143| Step: 0
Training loss: 2.3929779839656664
Validation loss: 2.480658608527125

Epoch: 6| Step: 1
Training loss: 3.057650560740897
Validation loss: 2.4753547394968436

Epoch: 6| Step: 2
Training loss: 2.5959344879900725
Validation loss: 2.467127737779476

Epoch: 6| Step: 3
Training loss: 2.796841924887357
Validation loss: 2.4782230013069335

Epoch: 6| Step: 4
Training loss: 2.011977688318694
Validation loss: 2.469156175510221

Epoch: 6| Step: 5
Training loss: 2.4436762884003755
Validation loss: 2.482845835444936

Epoch: 6| Step: 6
Training loss: 2.324556008881639
Validation loss: 2.472224739754451

Epoch: 6| Step: 7
Training loss: 2.0240302083094437
Validation loss: 2.4698422667426767

Epoch: 6| Step: 8
Training loss: 2.901171769809477
Validation loss: 2.472898313737579

Epoch: 6| Step: 9
Training loss: 2.199164110389179
Validation loss: 2.4659704020431485

Epoch: 6| Step: 10
Training loss: 2.3859057750978483
Validation loss: 2.4655922278743208

Epoch: 6| Step: 11
Training loss: 2.4326291445787027
Validation loss: 2.4726943524856466

Epoch: 6| Step: 12
Training loss: 2.3898155487581425
Validation loss: 2.4674324189490355

Epoch: 6| Step: 13
Training loss: 2.583169060272946
Validation loss: 2.464215713991575

Epoch: 144| Step: 0
Training loss: 2.4981959509490803
Validation loss: 2.4670147813411645

Epoch: 6| Step: 1
Training loss: 2.918294416074081
Validation loss: 2.4712983993707236

Epoch: 6| Step: 2
Training loss: 2.7631991679711074
Validation loss: 2.468020689553499

Epoch: 6| Step: 3
Training loss: 2.2817194207471365
Validation loss: 2.4653715202406143

Epoch: 6| Step: 4
Training loss: 3.15136461520851
Validation loss: 2.4699660497203477

Epoch: 6| Step: 5
Training loss: 2.4945644415879222
Validation loss: 2.4714897823339905

Epoch: 6| Step: 6
Training loss: 2.509608496114877
Validation loss: 2.4630485240024766

Epoch: 6| Step: 7
Training loss: 2.7594740403784748
Validation loss: 2.4734494512433516

Epoch: 6| Step: 8
Training loss: 2.5727504311011593
Validation loss: 2.4703366938931763

Epoch: 6| Step: 9
Training loss: 2.8828879245722563
Validation loss: 2.476338662221593

Epoch: 6| Step: 10
Training loss: 1.9943326881867138
Validation loss: 2.468867431938049

Epoch: 6| Step: 11
Training loss: 1.7964549361452706
Validation loss: 2.4703366617223144

Epoch: 6| Step: 12
Training loss: 1.3769356368016217
Validation loss: 2.463256566740354

Epoch: 6| Step: 13
Training loss: 2.0553768727535817
Validation loss: 2.4701733409428597

Epoch: 145| Step: 0
Training loss: 2.7638637300897924
Validation loss: 2.474353772064544

Epoch: 6| Step: 1
Training loss: 2.5770523151657825
Validation loss: 2.4649753767154587

Epoch: 6| Step: 2
Training loss: 2.735068794515045
Validation loss: 2.47195984011191

Epoch: 6| Step: 3
Training loss: 2.549754194492391
Validation loss: 2.470770914105232

Epoch: 6| Step: 4
Training loss: 2.5679601786963357
Validation loss: 2.4740980624174713

Epoch: 6| Step: 5
Training loss: 2.4505162065052186
Validation loss: 2.4777330827055617

Epoch: 6| Step: 6
Training loss: 2.6531630586336457
Validation loss: 2.4767843441925517

Epoch: 6| Step: 7
Training loss: 1.5025576403029806
Validation loss: 2.4762905384682115

Epoch: 6| Step: 8
Training loss: 2.9003758285607426
Validation loss: 2.481503328448084

Epoch: 6| Step: 9
Training loss: 1.971765539936841
Validation loss: 2.4777150004559227

Epoch: 6| Step: 10
Training loss: 1.8734887708658803
Validation loss: 2.4725673312028005

Epoch: 6| Step: 11
Training loss: 2.9664792662448436
Validation loss: 2.4770698228301318

Epoch: 6| Step: 12
Training loss: 2.43943045050511
Validation loss: 2.4721061090521466

Epoch: 6| Step: 13
Training loss: 2.434704277677018
Validation loss: 2.475599452677194

Epoch: 146| Step: 0
Training loss: 3.141879462228903
Validation loss: 2.4600918240195897

Epoch: 6| Step: 1
Training loss: 2.176678511902308
Validation loss: 2.472828132396471

Epoch: 6| Step: 2
Training loss: 3.0980909314340006
Validation loss: 2.4722483671665603

Epoch: 6| Step: 3
Training loss: 2.2059846540178394
Validation loss: 2.471009230718555

Epoch: 6| Step: 4
Training loss: 2.3214155972296378
Validation loss: 2.4605513012318303

Epoch: 6| Step: 5
Training loss: 2.040237146879998
Validation loss: 2.473038292850202

Epoch: 6| Step: 6
Training loss: 2.4515045061904477
Validation loss: 2.464509791236023

Epoch: 6| Step: 7
Training loss: 1.8679507303744838
Validation loss: 2.4723465470927013

Epoch: 6| Step: 8
Training loss: 2.935190144866843
Validation loss: 2.465831181971196

Epoch: 6| Step: 9
Training loss: 2.336913858454337
Validation loss: 2.4686346872174316

Epoch: 6| Step: 10
Training loss: 2.201448275958068
Validation loss: 2.469906049375337

Epoch: 6| Step: 11
Training loss: 2.813594011942392
Validation loss: 2.4662184149035458

Epoch: 6| Step: 12
Training loss: 1.9025027856793857
Validation loss: 2.4697246639495307

Epoch: 6| Step: 13
Training loss: 2.7269521178017047
Validation loss: 2.4702174658031564

Epoch: 147| Step: 0
Training loss: 2.8872122749217772
Validation loss: 2.4717474900130645

Epoch: 6| Step: 1
Training loss: 2.5588859565965785
Validation loss: 2.465452639925657

Epoch: 6| Step: 2
Training loss: 2.785462949753937
Validation loss: 2.472165630052407

Epoch: 6| Step: 3
Training loss: 2.901927728217042
Validation loss: 2.4712866937064257

Epoch: 6| Step: 4
Training loss: 2.362696652578772
Validation loss: 2.474811323918319

Epoch: 6| Step: 5
Training loss: 2.7220742083768847
Validation loss: 2.4698035732343215

Epoch: 6| Step: 6
Training loss: 2.190384407272736
Validation loss: 2.4701180671293588

Epoch: 6| Step: 7
Training loss: 2.0248664906135345
Validation loss: 2.4773048226995225

Epoch: 6| Step: 8
Training loss: 1.831645737940054
Validation loss: 2.4669870447757503

Epoch: 6| Step: 9
Training loss: 2.8416807590109086
Validation loss: 2.4612753499955025

Epoch: 6| Step: 10
Training loss: 1.8952751036474518
Validation loss: 2.4690312897306477

Epoch: 6| Step: 11
Training loss: 2.3020354966476795
Validation loss: 2.473139775363558

Epoch: 6| Step: 12
Training loss: 2.8736738380537634
Validation loss: 2.467775562981376

Epoch: 6| Step: 13
Training loss: 2.1091631747663637
Validation loss: 2.4724571224180902

Epoch: 148| Step: 0
Training loss: 2.1674429040976357
Validation loss: 2.4732467645299887

Epoch: 6| Step: 1
Training loss: 2.3873260274883776
Validation loss: 2.4766261336277267

Epoch: 6| Step: 2
Training loss: 2.743490490990941
Validation loss: 2.476525483922772

Epoch: 6| Step: 3
Training loss: 2.3952096099553706
Validation loss: 2.4810495892998636

Epoch: 6| Step: 4
Training loss: 2.2349312896748907
Validation loss: 2.4728050408133733

Epoch: 6| Step: 5
Training loss: 2.6195454921045136
Validation loss: 2.4771103598935924

Epoch: 6| Step: 6
Training loss: 2.0742246121253674
Validation loss: 2.4750543087482044

Epoch: 6| Step: 7
Training loss: 2.120747068957815
Validation loss: 2.4666298401721187

Epoch: 6| Step: 8
Training loss: 2.4231367208626846
Validation loss: 2.4661931507011667

Epoch: 6| Step: 9
Training loss: 2.6885694106004507
Validation loss: 2.462636290649805

Epoch: 6| Step: 10
Training loss: 2.49092610643849
Validation loss: 2.4598840305471406

Epoch: 6| Step: 11
Training loss: 3.311802772703357
Validation loss: 2.4640840628226166

Epoch: 6| Step: 12
Training loss: 2.3088185673700026
Validation loss: 2.4615214144555067

Epoch: 6| Step: 13
Training loss: 2.295384552761509
Validation loss: 2.464028104107835

Epoch: 149| Step: 0
Training loss: 2.02138176256775
Validation loss: 2.461049895772661

Epoch: 6| Step: 1
Training loss: 2.343691812428739
Validation loss: 2.46516887054111

Epoch: 6| Step: 2
Training loss: 2.4347760556929527
Validation loss: 2.4684050999120113

Epoch: 6| Step: 3
Training loss: 2.2007190742958826
Validation loss: 2.4669976352839584

Epoch: 6| Step: 4
Training loss: 2.869582005277034
Validation loss: 2.469324157842839

Epoch: 6| Step: 5
Training loss: 2.2292235417701223
Validation loss: 2.4660506641322306

Epoch: 6| Step: 6
Training loss: 1.9012457402750282
Validation loss: 2.4700397148620317

Epoch: 6| Step: 7
Training loss: 2.7278079273588913
Validation loss: 2.46367916398235

Epoch: 6| Step: 8
Training loss: 2.604006240989601
Validation loss: 2.474094488847708

Epoch: 6| Step: 9
Training loss: 2.065941684472274
Validation loss: 2.4772624361457054

Epoch: 6| Step: 10
Training loss: 2.3883035396076115
Validation loss: 2.4730333439447705

Epoch: 6| Step: 11
Training loss: 3.005361217045611
Validation loss: 2.4704458632995494

Epoch: 6| Step: 12
Training loss: 2.6719337144613258
Validation loss: 2.4672167881972102

Epoch: 6| Step: 13
Training loss: 2.8789569327501034
Validation loss: 2.4741274859507256

Epoch: 150| Step: 0
Training loss: 2.342359002917222
Validation loss: 2.4710923522823056

Epoch: 6| Step: 1
Training loss: 2.1836629730334676
Validation loss: 2.4676028133411543

Epoch: 6| Step: 2
Training loss: 3.3371265445317997
Validation loss: 2.4677015243013374

Epoch: 6| Step: 3
Training loss: 2.770179661870202
Validation loss: 2.4717212775489354

Epoch: 6| Step: 4
Training loss: 2.1590692637695517
Validation loss: 2.470963069625391

Epoch: 6| Step: 5
Training loss: 2.692310552281129
Validation loss: 2.472000838724527

Epoch: 6| Step: 6
Training loss: 2.1474285184001314
Validation loss: 2.4744772651251283

Epoch: 6| Step: 7
Training loss: 1.6878856642088926
Validation loss: 2.4757780082073917

Epoch: 6| Step: 8
Training loss: 2.1160148948765354
Validation loss: 2.470782638295632

Epoch: 6| Step: 9
Training loss: 2.5446717263020613
Validation loss: 2.4747828719075446

Epoch: 6| Step: 10
Training loss: 2.550309936537107
Validation loss: 2.4701659250577657

Epoch: 6| Step: 11
Training loss: 2.9911211867758705
Validation loss: 2.4729338013575717

Epoch: 6| Step: 12
Training loss: 2.23747539826406
Validation loss: 2.475232991898241

Epoch: 6| Step: 13
Training loss: 2.4111081906587617
Validation loss: 2.4762931380413056

Epoch: 151| Step: 0
Training loss: 2.5072467676846135
Validation loss: 2.47092991769339

Epoch: 6| Step: 1
Training loss: 2.150767668360053
Validation loss: 2.4764026867057853

Epoch: 6| Step: 2
Training loss: 2.3432634993123314
Validation loss: 2.47756879801455

Epoch: 6| Step: 3
Training loss: 2.677267154859139
Validation loss: 2.4815831041667624

Epoch: 6| Step: 4
Training loss: 2.2776567561349297
Validation loss: 2.4861806709413576

Epoch: 6| Step: 5
Training loss: 2.3536741591120314
Validation loss: 2.4891961460349528

Epoch: 6| Step: 6
Training loss: 2.083752920495397
Validation loss: 2.4847175953722878

Epoch: 6| Step: 7
Training loss: 2.2926181089704194
Validation loss: 2.488352664260061

Epoch: 6| Step: 8
Training loss: 2.506474122944759
Validation loss: 2.47405119604532

Epoch: 6| Step: 9
Training loss: 3.0004234015183435
Validation loss: 2.473319094874826

Epoch: 6| Step: 10
Training loss: 2.8820035466232556
Validation loss: 2.475496064510362

Epoch: 6| Step: 11
Training loss: 2.3863005576115772
Validation loss: 2.473359838002677

Epoch: 6| Step: 12
Training loss: 2.5663175291754095
Validation loss: 2.4758430984426045

Epoch: 6| Step: 13
Training loss: 2.52070682512949
Validation loss: 2.485659365276949

Epoch: 152| Step: 0
Training loss: 2.883829242492266
Validation loss: 2.4873157746100722

Epoch: 6| Step: 1
Training loss: 2.4092505436479352
Validation loss: 2.484863729009015

Epoch: 6| Step: 2
Training loss: 2.048260983937128
Validation loss: 2.4789961000211527

Epoch: 6| Step: 3
Training loss: 2.285723962933627
Validation loss: 2.4856255061195682

Epoch: 6| Step: 4
Training loss: 2.5710214489368917
Validation loss: 2.476672951274033

Epoch: 6| Step: 5
Training loss: 2.552039964787495
Validation loss: 2.4821963884384277

Epoch: 6| Step: 6
Training loss: 2.552719057791651
Validation loss: 2.4844683903508997

Epoch: 6| Step: 7
Training loss: 1.841264115724238
Validation loss: 2.4762692924722347

Epoch: 6| Step: 8
Training loss: 2.5840557892980156
Validation loss: 2.474135468148041

Epoch: 6| Step: 9
Training loss: 2.7820417155985138
Validation loss: 2.474774996166171

Epoch: 6| Step: 10
Training loss: 2.628056336366612
Validation loss: 2.482258468920328

Epoch: 6| Step: 11
Training loss: 2.768649672505303
Validation loss: 2.476522916684069

Epoch: 6| Step: 12
Training loss: 2.463452897411455
Validation loss: 2.463166348751461

Epoch: 6| Step: 13
Training loss: 2.4475241704883146
Validation loss: 2.4685885139154022

Epoch: 153| Step: 0
Training loss: 2.70858275414307
Validation loss: 2.464872848733951

Epoch: 6| Step: 1
Training loss: 1.9273848907693911
Validation loss: 2.481036007715325

Epoch: 6| Step: 2
Training loss: 1.9307342038030153
Validation loss: 2.4825617252857417

Epoch: 6| Step: 3
Training loss: 3.0210741845940827
Validation loss: 2.50516086990748

Epoch: 6| Step: 4
Training loss: 2.3957675648383434
Validation loss: 2.508752585274949

Epoch: 6| Step: 5
Training loss: 2.547269638472532
Validation loss: 2.5052671498289407

Epoch: 6| Step: 6
Training loss: 2.8829906377570875
Validation loss: 2.493443641870175

Epoch: 6| Step: 7
Training loss: 1.8891088538402743
Validation loss: 2.465093778331368

Epoch: 6| Step: 8
Training loss: 3.238092322642085
Validation loss: 2.475249366528232

Epoch: 6| Step: 9
Training loss: 2.115562237438278
Validation loss: 2.473784500171915

Epoch: 6| Step: 10
Training loss: 2.487169816996395
Validation loss: 2.4819933193479597

Epoch: 6| Step: 11
Training loss: 2.5950922422975093
Validation loss: 2.4769463788141044

Epoch: 6| Step: 12
Training loss: 2.499601713879102
Validation loss: 2.476715131322248

Epoch: 6| Step: 13
Training loss: 2.5427726048826313
Validation loss: 2.4917365835155287

Epoch: 154| Step: 0
Training loss: 2.776118570280034
Validation loss: 2.486462162197592

Epoch: 6| Step: 1
Training loss: 3.381234803227316
Validation loss: 2.4753777591370367

Epoch: 6| Step: 2
Training loss: 2.231954690904037
Validation loss: 2.477483255321426

Epoch: 6| Step: 3
Training loss: 2.697429411691952
Validation loss: 2.483007090428983

Epoch: 6| Step: 4
Training loss: 2.7805938804018595
Validation loss: 2.4764976773768757

Epoch: 6| Step: 5
Training loss: 2.630100109530267
Validation loss: 2.479459494172015

Epoch: 6| Step: 6
Training loss: 1.8888465145755993
Validation loss: 2.479984971175238

Epoch: 6| Step: 7
Training loss: 2.1524282670778536
Validation loss: 2.4738002177791216

Epoch: 6| Step: 8
Training loss: 2.2994187989020487
Validation loss: 2.4732012637711844

Epoch: 6| Step: 9
Training loss: 2.606520501971405
Validation loss: 2.4711070659068426

Epoch: 6| Step: 10
Training loss: 2.37295524514454
Validation loss: 2.4681612189887483

Epoch: 6| Step: 11
Training loss: 2.0660281204939177
Validation loss: 2.4744435580729305

Epoch: 6| Step: 12
Training loss: 2.842050233014459
Validation loss: 2.4678549532301934

Epoch: 6| Step: 13
Training loss: 2.151153844782404
Validation loss: 2.4732552797626246

Epoch: 155| Step: 0
Training loss: 2.4490793483554216
Validation loss: 2.4759352378027613

Epoch: 6| Step: 1
Training loss: 2.1087656907163215
Validation loss: 2.4684896211087226

Epoch: 6| Step: 2
Training loss: 1.8582444801243452
Validation loss: 2.474037463612513

Epoch: 6| Step: 3
Training loss: 2.5757226096719146
Validation loss: 2.4748165341974984

Epoch: 6| Step: 4
Training loss: 2.4134324124934743
Validation loss: 2.471999432196444

Epoch: 6| Step: 5
Training loss: 2.498847886687628
Validation loss: 2.4843944172930104

Epoch: 6| Step: 6
Training loss: 3.3276802446369267
Validation loss: 2.483916722221381

Epoch: 6| Step: 7
Training loss: 2.3664853908620636
Validation loss: 2.494028541478104

Epoch: 6| Step: 8
Training loss: 2.178875515551099
Validation loss: 2.4711826105469417

Epoch: 6| Step: 9
Training loss: 2.04938289379055
Validation loss: 2.472939955599827

Epoch: 6| Step: 10
Training loss: 2.2711594956903625
Validation loss: 2.4637874026389173

Epoch: 6| Step: 11
Training loss: 2.9866560761513727
Validation loss: 2.4690781387318577

Epoch: 6| Step: 12
Training loss: 2.5745803046549525
Validation loss: 2.4630459911158393

Epoch: 6| Step: 13
Training loss: 2.6974793500702052
Validation loss: 2.471511720566661

Epoch: 156| Step: 0
Training loss: 2.305955854643461
Validation loss: 2.472466548414388

Epoch: 6| Step: 1
Training loss: 2.2123831076912865
Validation loss: 2.4743331839354017

Epoch: 6| Step: 2
Training loss: 2.0494734017962783
Validation loss: 2.476003220681893

Epoch: 6| Step: 3
Training loss: 2.7900884541808564
Validation loss: 2.4680881659331244

Epoch: 6| Step: 4
Training loss: 2.7617002753647446
Validation loss: 2.472700732304602

Epoch: 6| Step: 5
Training loss: 2.4092607364698337
Validation loss: 2.4734089425406998

Epoch: 6| Step: 6
Training loss: 2.774427812025944
Validation loss: 2.4736984491205463

Epoch: 6| Step: 7
Training loss: 2.2117870841726357
Validation loss: 2.4735160084207615

Epoch: 6| Step: 8
Training loss: 3.0721796391137923
Validation loss: 2.478197635004662

Epoch: 6| Step: 9
Training loss: 2.6534364048029295
Validation loss: 2.4718155478792156

Epoch: 6| Step: 10
Training loss: 2.2987115485256506
Validation loss: 2.4785957614057104

Epoch: 6| Step: 11
Training loss: 2.0070799444876264
Validation loss: 2.474255695876206

Epoch: 6| Step: 12
Training loss: 2.348952356120848
Validation loss: 2.477069534079378

Epoch: 6| Step: 13
Training loss: 2.7125061949206515
Validation loss: 2.4704715665483095

Epoch: 157| Step: 0
Training loss: 3.545114223001281
Validation loss: 2.4759070554953757

Epoch: 6| Step: 1
Training loss: 2.181770450619023
Validation loss: 2.4773132437827994

Epoch: 6| Step: 2
Training loss: 2.1062743958098946
Validation loss: 2.481343240851715

Epoch: 6| Step: 3
Training loss: 2.564626044365245
Validation loss: 2.483380548510497

Epoch: 6| Step: 4
Training loss: 2.3996769687696218
Validation loss: 2.47629477480815

Epoch: 6| Step: 5
Training loss: 3.0790261435716735
Validation loss: 2.476526927993373

Epoch: 6| Step: 6
Training loss: 2.074736622100845
Validation loss: 2.469026638578963

Epoch: 6| Step: 7
Training loss: 2.7396303083407796
Validation loss: 2.484131535210545

Epoch: 6| Step: 8
Training loss: 2.5625641977595723
Validation loss: 2.477088190517257

Epoch: 6| Step: 9
Training loss: 2.372848841563332
Validation loss: 2.480838489759898

Epoch: 6| Step: 10
Training loss: 2.2503272984127642
Validation loss: 2.4755879278632693

Epoch: 6| Step: 11
Training loss: 1.6032750347638947
Validation loss: 2.4732965862581033

Epoch: 6| Step: 12
Training loss: 2.7496139949039535
Validation loss: 2.474184942742243

Epoch: 6| Step: 13
Training loss: 1.9844165106809506
Validation loss: 2.4752040952281202

Epoch: 158| Step: 0
Training loss: 2.5775371459297074
Validation loss: 2.4718755259877003

Epoch: 6| Step: 1
Training loss: 2.1835276912735386
Validation loss: 2.479075667904398

Epoch: 6| Step: 2
Training loss: 2.2337940234354066
Validation loss: 2.4709615981811357

Epoch: 6| Step: 3
Training loss: 2.2626884634863433
Validation loss: 2.478064898627576

Epoch: 6| Step: 4
Training loss: 3.0244113822336365
Validation loss: 2.472076540903927

Epoch: 6| Step: 5
Training loss: 2.0248208049089644
Validation loss: 2.476220942709811

Epoch: 6| Step: 6
Training loss: 2.821535643051451
Validation loss: 2.4755977672968914

Epoch: 6| Step: 7
Training loss: 2.4548499002040303
Validation loss: 2.4864578472936567

Epoch: 6| Step: 8
Training loss: 2.3385876578132296
Validation loss: 2.4780471314891392

Epoch: 6| Step: 9
Training loss: 2.4428562100567226
Validation loss: 2.471189002308672

Epoch: 6| Step: 10
Training loss: 2.3196939485201544
Validation loss: 2.4702676140449005

Epoch: 6| Step: 11
Training loss: 2.285895334038707
Validation loss: 2.4658104662808777

Epoch: 6| Step: 12
Training loss: 2.2927484097777344
Validation loss: 2.4742165654916275

Epoch: 6| Step: 13
Training loss: 3.0459422713461817
Validation loss: 2.478933296653175

Epoch: 159| Step: 0
Training loss: 2.581084338915237
Validation loss: 2.47019698797658

Epoch: 6| Step: 1
Training loss: 2.3935430332679863
Validation loss: 2.4740016224068677

Epoch: 6| Step: 2
Training loss: 2.7614167519087314
Validation loss: 2.4700670310563395

Epoch: 6| Step: 3
Training loss: 2.857318910215535
Validation loss: 2.476455798492927

Epoch: 6| Step: 4
Training loss: 2.4698064933684702
Validation loss: 2.484561601264723

Epoch: 6| Step: 5
Training loss: 2.04336993215288
Validation loss: 2.4802532112411733

Epoch: 6| Step: 6
Training loss: 2.6820341017430405
Validation loss: 2.4778539136508875

Epoch: 6| Step: 7
Training loss: 2.2169563673007597
Validation loss: 2.481609965044486

Epoch: 6| Step: 8
Training loss: 2.0841290925106506
Validation loss: 2.4816377463319808

Epoch: 6| Step: 9
Training loss: 2.6251870951507246
Validation loss: 2.4883084617576037

Epoch: 6| Step: 10
Training loss: 2.5089940410192035
Validation loss: 2.479585345094412

Epoch: 6| Step: 11
Training loss: 2.5127554692444254
Validation loss: 2.476804647230233

Epoch: 6| Step: 12
Training loss: 2.272075861429977
Validation loss: 2.4768145058953377

Epoch: 6| Step: 13
Training loss: 2.2843354144756494
Validation loss: 2.475718999392837

Epoch: 160| Step: 0
Training loss: 1.965987795779281
Validation loss: 2.480447363573527

Epoch: 6| Step: 1
Training loss: 2.6004472237871687
Validation loss: 2.485101909377124

Epoch: 6| Step: 2
Training loss: 3.1609605442325317
Validation loss: 2.480837793005575

Epoch: 6| Step: 3
Training loss: 2.67240837974976
Validation loss: 2.4771863470535607

Epoch: 6| Step: 4
Training loss: 2.6023207467355713
Validation loss: 2.48678254686743

Epoch: 6| Step: 5
Training loss: 1.690067598469434
Validation loss: 2.4879226625807136

Epoch: 6| Step: 6
Training loss: 2.4790321334965975
Validation loss: 2.479266530651546

Epoch: 6| Step: 7
Training loss: 2.555005814905884
Validation loss: 2.4868991912179057

Epoch: 6| Step: 8
Training loss: 2.170086899876394
Validation loss: 2.485680962638299

Epoch: 6| Step: 9
Training loss: 2.781222053987496
Validation loss: 2.4803371124180686

Epoch: 6| Step: 10
Training loss: 2.44989675966062
Validation loss: 2.477600650224856

Epoch: 6| Step: 11
Training loss: 2.318419021057701
Validation loss: 2.474871237452631

Epoch: 6| Step: 12
Training loss: 2.33993015543717
Validation loss: 2.4634007070160817

Epoch: 6| Step: 13
Training loss: 2.134779473737102
Validation loss: 2.4792062665410635

Epoch: 161| Step: 0
Training loss: 2.435632038845339
Validation loss: 2.4752137274889776

Epoch: 6| Step: 1
Training loss: 2.353417156264772
Validation loss: 2.4756957421664247

Epoch: 6| Step: 2
Training loss: 2.08532728777901
Validation loss: 2.477329251782176

Epoch: 6| Step: 3
Training loss: 2.554206079507728
Validation loss: 2.4802296921281526

Epoch: 6| Step: 4
Training loss: 2.802139187129692
Validation loss: 2.473602290717574

Epoch: 6| Step: 5
Training loss: 2.047015585694196
Validation loss: 2.484314387959617

Epoch: 6| Step: 6
Training loss: 2.4925513883781925
Validation loss: 2.4863443626319084

Epoch: 6| Step: 7
Training loss: 3.0896638029340884
Validation loss: 2.4841078688598794

Epoch: 6| Step: 8
Training loss: 2.7033093339989125
Validation loss: 2.4838224392319925

Epoch: 6| Step: 9
Training loss: 2.743612848212456
Validation loss: 2.479470648414717

Epoch: 6| Step: 10
Training loss: 2.501615193257004
Validation loss: 2.4832344876102517

Epoch: 6| Step: 11
Training loss: 1.9862108524178523
Validation loss: 2.486313980913348

Epoch: 6| Step: 12
Training loss: 2.3836707398750514
Validation loss: 2.4918469841138613

Epoch: 6| Step: 13
Training loss: 2.1244701398087393
Validation loss: 2.4983754601806667

Epoch: 162| Step: 0
Training loss: 2.8428722326421796
Validation loss: 2.491555112767748

Epoch: 6| Step: 1
Training loss: 2.528155283516668
Validation loss: 2.476882047525256

Epoch: 6| Step: 2
Training loss: 3.0607850859055836
Validation loss: 2.486382990509179

Epoch: 6| Step: 3
Training loss: 1.7858664938226891
Validation loss: 2.482989454675395

Epoch: 6| Step: 4
Training loss: 2.4540249091130106
Validation loss: 2.4910979245924176

Epoch: 6| Step: 5
Training loss: 3.2922523235064016
Validation loss: 2.4774558444664203

Epoch: 6| Step: 6
Training loss: 2.0912419570613134
Validation loss: 2.4820358172262496

Epoch: 6| Step: 7
Training loss: 2.392585698329851
Validation loss: 2.482774358816129

Epoch: 6| Step: 8
Training loss: 2.016256543957094
Validation loss: 2.483192194271068

Epoch: 6| Step: 9
Training loss: 2.3741847194932815
Validation loss: 2.4864975920700747

Epoch: 6| Step: 10
Training loss: 2.601729928892368
Validation loss: 2.4886771884666494

Epoch: 6| Step: 11
Training loss: 1.8375042013522533
Validation loss: 2.4948806162820536

Epoch: 6| Step: 12
Training loss: 2.2374977751140084
Validation loss: 2.4820043261201112

Epoch: 6| Step: 13
Training loss: 2.5384937764501556
Validation loss: 2.4874961009546133

Epoch: 163| Step: 0
Training loss: 2.2675498415212307
Validation loss: 2.4863185198154594

Epoch: 6| Step: 1
Training loss: 2.24174000597628
Validation loss: 2.489764284131176

Epoch: 6| Step: 2
Training loss: 2.4805512181787943
Validation loss: 2.4811786426753897

Epoch: 6| Step: 3
Training loss: 2.5640173350743494
Validation loss: 2.4900478238309627

Epoch: 6| Step: 4
Training loss: 2.4177985555131563
Validation loss: 2.4903452730106643

Epoch: 6| Step: 5
Training loss: 2.779952871897418
Validation loss: 2.5009733927690645

Epoch: 6| Step: 6
Training loss: 2.466432086856378
Validation loss: 2.503335253854041

Epoch: 6| Step: 7
Training loss: 2.6159869272914507
Validation loss: 2.488656375541157

Epoch: 6| Step: 8
Training loss: 2.488226921410678
Validation loss: 2.4759138603643494

Epoch: 6| Step: 9
Training loss: 2.331635152382483
Validation loss: 2.4799954340749157

Epoch: 6| Step: 10
Training loss: 2.187577600465099
Validation loss: 2.476968485290229

Epoch: 6| Step: 11
Training loss: 2.7173393360641906
Validation loss: 2.4767604873089675

Epoch: 6| Step: 12
Training loss: 2.6019866872471833
Validation loss: 2.4753551247645498

Epoch: 6| Step: 13
Training loss: 2.3357501002641077
Validation loss: 2.478052423159214

Epoch: 164| Step: 0
Training loss: 2.3073464782496185
Validation loss: 2.4783938402777825

Epoch: 6| Step: 1
Training loss: 2.6549760343975044
Validation loss: 2.4797108370879233

Epoch: 6| Step: 2
Training loss: 2.405170148830269
Validation loss: 2.4730917018357985

Epoch: 6| Step: 3
Training loss: 2.223507790114574
Validation loss: 2.480572683805153

Epoch: 6| Step: 4
Training loss: 1.8707419046718508
Validation loss: 2.4824894561492443

Epoch: 6| Step: 5
Training loss: 2.798403615229885
Validation loss: 2.4731167509596794

Epoch: 6| Step: 6
Training loss: 3.082276927985384
Validation loss: 2.47261687735035

Epoch: 6| Step: 7
Training loss: 2.7635353944325987
Validation loss: 2.4786602486094176

Epoch: 6| Step: 8
Training loss: 2.5989783113638047
Validation loss: 2.4825828694126977

Epoch: 6| Step: 9
Training loss: 2.6776216512446926
Validation loss: 2.479887197810649

Epoch: 6| Step: 10
Training loss: 2.1416175935617505
Validation loss: 2.481049861571435

Epoch: 6| Step: 11
Training loss: 1.802206604844575
Validation loss: 2.484801009978232

Epoch: 6| Step: 12
Training loss: 2.6078091794513387
Validation loss: 2.4831369222077457

Epoch: 6| Step: 13
Training loss: 2.4260540745834036
Validation loss: 2.4766496067392265

Epoch: 165| Step: 0
Training loss: 2.4287068144943893
Validation loss: 2.4789599858745563

Epoch: 6| Step: 1
Training loss: 2.313948924840395
Validation loss: 2.477884302881019

Epoch: 6| Step: 2
Training loss: 2.023669019731764
Validation loss: 2.476876673139966

Epoch: 6| Step: 3
Training loss: 3.5579393876023313
Validation loss: 2.4808427023163464

Epoch: 6| Step: 4
Training loss: 2.5334079162594434
Validation loss: 2.485595914948537

Epoch: 6| Step: 5
Training loss: 2.3750772965551703
Validation loss: 2.4769797951309696

Epoch: 6| Step: 6
Training loss: 2.0939326420163944
Validation loss: 2.4814792927478777

Epoch: 6| Step: 7
Training loss: 2.5573325739823938
Validation loss: 2.4870731409236506

Epoch: 6| Step: 8
Training loss: 2.511096171200551
Validation loss: 2.4828090730919143

Epoch: 6| Step: 9
Training loss: 2.095611541823153
Validation loss: 2.475339216368972

Epoch: 6| Step: 10
Training loss: 2.239575715754366
Validation loss: 2.4768129416671227

Epoch: 6| Step: 11
Training loss: 2.4073234244584354
Validation loss: 2.4811579270019206

Epoch: 6| Step: 12
Training loss: 2.745147585488489
Validation loss: 2.4830123875319603

Epoch: 6| Step: 13
Training loss: 2.146580340780151
Validation loss: 2.4771151723224483

Epoch: 166| Step: 0
Training loss: 2.059697877524684
Validation loss: 2.4805904168354735

Epoch: 6| Step: 1
Training loss: 2.486916447725727
Validation loss: 2.4781700075671638

Epoch: 6| Step: 2
Training loss: 2.4886938019581244
Validation loss: 2.4748206847510286

Epoch: 6| Step: 3
Training loss: 2.8352335747196866
Validation loss: 2.476269132003294

Epoch: 6| Step: 4
Training loss: 2.8836509911812196
Validation loss: 2.4787587276323686

Epoch: 6| Step: 5
Training loss: 2.144489670091557
Validation loss: 2.4776544580386757

Epoch: 6| Step: 6
Training loss: 3.121634540328054
Validation loss: 2.4782170525956353

Epoch: 6| Step: 7
Training loss: 1.9583610235109823
Validation loss: 2.4942781454762266

Epoch: 6| Step: 8
Training loss: 2.817750988048048
Validation loss: 2.4906030239920995

Epoch: 6| Step: 9
Training loss: 1.2725141489257825
Validation loss: 2.494009223091408

Epoch: 6| Step: 10
Training loss: 2.5209309792851005
Validation loss: 2.483063557605012

Epoch: 6| Step: 11
Training loss: 2.7401111708315096
Validation loss: 2.4979885910746917

Epoch: 6| Step: 12
Training loss: 2.150975950651983
Validation loss: 2.4870479927059437

Epoch: 6| Step: 13
Training loss: 2.268646015064647
Validation loss: 2.493424797046465

Epoch: 167| Step: 0
Training loss: 2.008324227635594
Validation loss: 2.489477120744778

Epoch: 6| Step: 1
Training loss: 2.6807380596571964
Validation loss: 2.486676827206278

Epoch: 6| Step: 2
Training loss: 1.9486382791672467
Validation loss: 2.4853558313599144

Epoch: 6| Step: 3
Training loss: 3.1714875684624118
Validation loss: 2.4825470395351914

Epoch: 6| Step: 4
Training loss: 2.223252040610304
Validation loss: 2.4822926941641534

Epoch: 6| Step: 5
Training loss: 1.93146078200177
Validation loss: 2.4753673650094123

Epoch: 6| Step: 6
Training loss: 2.884204890069426
Validation loss: 2.4810635952310514

Epoch: 6| Step: 7
Training loss: 2.3292163108853345
Validation loss: 2.479148373148981

Epoch: 6| Step: 8
Training loss: 2.379078224618211
Validation loss: 2.4792646714615256

Epoch: 6| Step: 9
Training loss: 2.6185510527940576
Validation loss: 2.4791729443467956

Epoch: 6| Step: 10
Training loss: 2.291780492094322
Validation loss: 2.4790043470527436

Epoch: 6| Step: 11
Training loss: 2.7716929792056306
Validation loss: 2.476569254933159

Epoch: 6| Step: 12
Training loss: 2.2648895122198516
Validation loss: 2.481561126868731

Epoch: 6| Step: 13
Training loss: 2.1682302506280946
Validation loss: 2.48256354199108

Epoch: 168| Step: 0
Training loss: 2.526168054519107
Validation loss: 2.4846799651290645

Epoch: 6| Step: 1
Training loss: 2.2997462174095964
Validation loss: 2.4843599970782417

Epoch: 6| Step: 2
Training loss: 2.32114844885297
Validation loss: 2.4848033287875118

Epoch: 6| Step: 3
Training loss: 1.9393196175252951
Validation loss: 2.4842049402536546

Epoch: 6| Step: 4
Training loss: 2.5807472533751192
Validation loss: 2.483577624301932

Epoch: 6| Step: 5
Training loss: 2.5734402719655303
Validation loss: 2.4919238773218617

Epoch: 6| Step: 6
Training loss: 2.559117573704358
Validation loss: 2.4910220188319827

Epoch: 6| Step: 7
Training loss: 2.268592837436161
Validation loss: 2.490793769188684

Epoch: 6| Step: 8
Training loss: 2.3542322357858896
Validation loss: 2.4946092102185164

Epoch: 6| Step: 9
Training loss: 2.9833423365735925
Validation loss: 2.49021671064507

Epoch: 6| Step: 10
Training loss: 2.397808178773862
Validation loss: 2.491928374113762

Epoch: 6| Step: 11
Training loss: 2.631422813702712
Validation loss: 2.492626872938815

Epoch: 6| Step: 12
Training loss: 2.4810304020868226
Validation loss: 2.4820386989546965

Epoch: 6| Step: 13
Training loss: 2.0239662451662244
Validation loss: 2.4878092926558

Epoch: 169| Step: 0
Training loss: 2.362170048031138
Validation loss: 2.4853480530759646

Epoch: 6| Step: 1
Training loss: 2.2930552581636987
Validation loss: 2.477797800842298

Epoch: 6| Step: 2
Training loss: 2.6167498322247473
Validation loss: 2.4838479080702602

Epoch: 6| Step: 3
Training loss: 2.7312497835137393
Validation loss: 2.4948795650874973

Epoch: 6| Step: 4
Training loss: 1.798903804350958
Validation loss: 2.4932253917577594

Epoch: 6| Step: 5
Training loss: 1.9100682525023889
Validation loss: 2.492966947571826

Epoch: 6| Step: 6
Training loss: 2.4011831069517275
Validation loss: 2.49533948411964

Epoch: 6| Step: 7
Training loss: 2.643434463320651
Validation loss: 2.4969976198518364

Epoch: 6| Step: 8
Training loss: 2.4160651631314827
Validation loss: 2.500462084027543

Epoch: 6| Step: 9
Training loss: 2.805938526701004
Validation loss: 2.496682031564667

Epoch: 6| Step: 10
Training loss: 2.6564116484878477
Validation loss: 2.495037000428915

Epoch: 6| Step: 11
Training loss: 2.45272707293317
Validation loss: 2.5053750271500825

Epoch: 6| Step: 12
Training loss: 2.0590231518177933
Validation loss: 2.5005504399550307

Epoch: 6| Step: 13
Training loss: 2.601763559997593
Validation loss: 2.501681906467131

Epoch: 170| Step: 0
Training loss: 2.7856248030770523
Validation loss: 2.4981206187601983

Epoch: 6| Step: 1
Training loss: 2.5012810286046663
Validation loss: 2.4963045304858857

Epoch: 6| Step: 2
Training loss: 2.286024036451313
Validation loss: 2.4941032245553196

Epoch: 6| Step: 3
Training loss: 1.595769836450703
Validation loss: 2.5006822926578782

Epoch: 6| Step: 4
Training loss: 2.6721671318390543
Validation loss: 2.4959256668291117

Epoch: 6| Step: 5
Training loss: 2.234229663310271
Validation loss: 2.496050066657727

Epoch: 6| Step: 6
Training loss: 2.315947566043348
Validation loss: 2.492628459126285

Epoch: 6| Step: 7
Training loss: 2.8875759230371263
Validation loss: 2.5004023069293466

Epoch: 6| Step: 8
Training loss: 2.235822088967095
Validation loss: 2.486657020249291

Epoch: 6| Step: 9
Training loss: 2.0795080415607368
Validation loss: 2.4927897268785904

Epoch: 6| Step: 10
Training loss: 2.1143146463204143
Validation loss: 2.491128359624401

Epoch: 6| Step: 11
Training loss: 2.8515793891302073
Validation loss: 2.4901968440799265

Epoch: 6| Step: 12
Training loss: 2.0231987654018897
Validation loss: 2.4943689507169178

Epoch: 6| Step: 13
Training loss: 2.941512917741355
Validation loss: 2.4944432651778476

Epoch: 171| Step: 0
Training loss: 1.9978781411676656
Validation loss: 2.4959270837548235

Epoch: 6| Step: 1
Training loss: 2.4116502085424902
Validation loss: 2.496444621769644

Epoch: 6| Step: 2
Training loss: 2.9173256447543707
Validation loss: 2.4857046859484853

Epoch: 6| Step: 3
Training loss: 2.3749981930374497
Validation loss: 2.4888009048229742

Epoch: 6| Step: 4
Training loss: 2.750121894215679
Validation loss: 2.4907711712085083

Epoch: 6| Step: 5
Training loss: 2.543743525316401
Validation loss: 2.488486304984011

Epoch: 6| Step: 6
Training loss: 2.128407831205067
Validation loss: 2.494350041196334

Epoch: 6| Step: 7
Training loss: 2.077649743367665
Validation loss: 2.488104669762077

Epoch: 6| Step: 8
Training loss: 2.8020170882287334
Validation loss: 2.4972701347680704

Epoch: 6| Step: 9
Training loss: 2.5864185450537107
Validation loss: 2.4932677220910304

Epoch: 6| Step: 10
Training loss: 1.7848736228336461
Validation loss: 2.5002985775987505

Epoch: 6| Step: 11
Training loss: 2.415424542912418
Validation loss: 2.497360568680991

Epoch: 6| Step: 12
Training loss: 2.0184620843840966
Validation loss: 2.50350042695553

Epoch: 6| Step: 13
Training loss: 2.8167513610776966
Validation loss: 2.5022363038407436

Epoch: 172| Step: 0
Training loss: 2.033423801139719
Validation loss: 2.497312133998815

Epoch: 6| Step: 1
Training loss: 2.3317019231647063
Validation loss: 2.499918507201448

Epoch: 6| Step: 2
Training loss: 2.3165971699817662
Validation loss: 2.5045799900206713

Epoch: 6| Step: 3
Training loss: 2.0771546357012602
Validation loss: 2.503290061889934

Epoch: 6| Step: 4
Training loss: 2.887779526164669
Validation loss: 2.504031998942771

Epoch: 6| Step: 5
Training loss: 2.4046899271277957
Validation loss: 2.494977674671045

Epoch: 6| Step: 6
Training loss: 2.6805020087059734
Validation loss: 2.5029191299594853

Epoch: 6| Step: 7
Training loss: 2.70811240444428
Validation loss: 2.498581229396348

Epoch: 6| Step: 8
Training loss: 2.582867508594966
Validation loss: 2.4975018735667938

Epoch: 6| Step: 9
Training loss: 2.717589382747632
Validation loss: 2.498830044691843

Epoch: 6| Step: 10
Training loss: 2.6833314725818065
Validation loss: 2.5070841871525045

Epoch: 6| Step: 11
Training loss: 1.9399452161427477
Validation loss: 2.4907860397931905

Epoch: 6| Step: 12
Training loss: 2.6037615855193184
Validation loss: 2.4940870294709763

Epoch: 6| Step: 13
Training loss: 1.6824019389037508
Validation loss: 2.495285492338633

Epoch: 173| Step: 0
Training loss: 1.911302967323064
Validation loss: 2.4880381195402697

Epoch: 6| Step: 1
Training loss: 2.0640051002809066
Validation loss: 2.4928018974559167

Epoch: 6| Step: 2
Training loss: 2.1158040724548095
Validation loss: 2.4975577902893584

Epoch: 6| Step: 3
Training loss: 2.4364033701942094
Validation loss: 2.504099536404297

Epoch: 6| Step: 4
Training loss: 2.444146118840717
Validation loss: 2.512915057919362

Epoch: 6| Step: 5
Training loss: 2.2394084655152
Validation loss: 2.5051771758131944

Epoch: 6| Step: 6
Training loss: 2.6846712883713018
Validation loss: 2.506397026106394

Epoch: 6| Step: 7
Training loss: 2.8336954633380897
Validation loss: 2.497220926587376

Epoch: 6| Step: 8
Training loss: 2.925606858641982
Validation loss: 2.4981862402866466

Epoch: 6| Step: 9
Training loss: 2.842437105716004
Validation loss: 2.506905777040961

Epoch: 6| Step: 10
Training loss: 2.2171438071722207
Validation loss: 2.5122985487055804

Epoch: 6| Step: 11
Training loss: 2.3092374727132983
Validation loss: 2.5201010205189003

Epoch: 6| Step: 12
Training loss: 2.4398397318000455
Validation loss: 2.5140690384790254

Epoch: 6| Step: 13
Training loss: 2.1292040135441987
Validation loss: 2.5144942528327827

Epoch: 174| Step: 0
Training loss: 2.672328442026352
Validation loss: 2.505128162605438

Epoch: 6| Step: 1
Training loss: 2.2325099817546246
Validation loss: 2.502867929225314

Epoch: 6| Step: 2
Training loss: 2.500285418430164
Validation loss: 2.505059796302992

Epoch: 6| Step: 3
Training loss: 2.112039874827492
Validation loss: 2.505499124337904

Epoch: 6| Step: 4
Training loss: 2.471776724500263
Validation loss: 2.5095238476380364

Epoch: 6| Step: 5
Training loss: 2.1580400638343713
Validation loss: 2.499477776979247

Epoch: 6| Step: 6
Training loss: 2.141393815804378
Validation loss: 2.5031122703403184

Epoch: 6| Step: 7
Training loss: 2.679476804763502
Validation loss: 2.5028503540341607

Epoch: 6| Step: 8
Training loss: 2.0786283643487584
Validation loss: 2.499683153419167

Epoch: 6| Step: 9
Training loss: 2.348061626554893
Validation loss: 2.486874193036566

Epoch: 6| Step: 10
Training loss: 2.4059408410362892
Validation loss: 2.4934714347371454

Epoch: 6| Step: 11
Training loss: 2.98014619814275
Validation loss: 2.4886053763519365

Epoch: 6| Step: 12
Training loss: 2.675617118312363
Validation loss: 2.495492264736332

Epoch: 6| Step: 13
Training loss: 2.2859044081091175
Validation loss: 2.4950458872263312

Epoch: 175| Step: 0
Training loss: 2.663295064785691
Validation loss: 2.4983878818823118

Epoch: 6| Step: 1
Training loss: 1.9288230535055877
Validation loss: 2.495370536229617

Epoch: 6| Step: 2
Training loss: 2.1524001320102144
Validation loss: 2.4924195120827095

Epoch: 6| Step: 3
Training loss: 2.4025825195748634
Validation loss: 2.4967295553777826

Epoch: 6| Step: 4
Training loss: 2.765246435430906
Validation loss: 2.4965134388726002

Epoch: 6| Step: 5
Training loss: 3.1064486894826033
Validation loss: 2.5074447964378654

Epoch: 6| Step: 6
Training loss: 2.650806369094715
Validation loss: 2.513335250133657

Epoch: 6| Step: 7
Training loss: 2.03188208502218
Validation loss: 2.509650344279091

Epoch: 6| Step: 8
Training loss: 3.2331890198530218
Validation loss: 2.510874130123533

Epoch: 6| Step: 9
Training loss: 1.8125956937587027
Validation loss: 2.5114213086097665

Epoch: 6| Step: 10
Training loss: 2.450418911367304
Validation loss: 2.503389191069926

Epoch: 6| Step: 11
Training loss: 1.6838949336804123
Validation loss: 2.496727613699191

Epoch: 6| Step: 12
Training loss: 2.209167203074529
Validation loss: 2.4928389747096547

Epoch: 6| Step: 13
Training loss: 2.1734051699945374
Validation loss: 2.4802534675790513

Epoch: 176| Step: 0
Training loss: 2.6468874916403418
Validation loss: 2.4876102998135385

Epoch: 6| Step: 1
Training loss: 1.984888085542277
Validation loss: 2.483556920638779

Epoch: 6| Step: 2
Training loss: 2.3013306044469917
Validation loss: 2.4846606140383707

Epoch: 6| Step: 3
Training loss: 3.081124949674286
Validation loss: 2.4892545560124835

Epoch: 6| Step: 4
Training loss: 2.266198657843465
Validation loss: 2.4953161708969605

Epoch: 6| Step: 5
Training loss: 2.5088054081795264
Validation loss: 2.4941337105404515

Epoch: 6| Step: 6
Training loss: 2.5144486609053796
Validation loss: 2.497224363633274

Epoch: 6| Step: 7
Training loss: 2.7866460338844727
Validation loss: 2.4800171929194166

Epoch: 6| Step: 8
Training loss: 2.424207192042118
Validation loss: 2.486306221609035

Epoch: 6| Step: 9
Training loss: 1.8002735009626822
Validation loss: 2.484034628913903

Epoch: 6| Step: 10
Training loss: 2.353367008566249
Validation loss: 2.4854935341326745

Epoch: 6| Step: 11
Training loss: 2.3117722061953687
Validation loss: 2.4830510912480945

Epoch: 6| Step: 12
Training loss: 3.1442294716411703
Validation loss: 2.4836273187890847

Epoch: 6| Step: 13
Training loss: 1.9282012652333356
Validation loss: 2.47551569591342

Epoch: 177| Step: 0
Training loss: 2.4435408636883174
Validation loss: 2.483883487216134

Epoch: 6| Step: 1
Training loss: 1.76722745841778
Validation loss: 2.4884959736574874

Epoch: 6| Step: 2
Training loss: 3.0311996613088272
Validation loss: 2.495696480448084

Epoch: 6| Step: 3
Training loss: 2.067551870375768
Validation loss: 2.4963040529427833

Epoch: 6| Step: 4
Training loss: 2.5633763001226084
Validation loss: 2.492099955744164

Epoch: 6| Step: 5
Training loss: 2.3128548427211197
Validation loss: 2.4877562954751147

Epoch: 6| Step: 6
Training loss: 2.6783192870130392
Validation loss: 2.4916103336468605

Epoch: 6| Step: 7
Training loss: 2.5488021206168665
Validation loss: 2.484723912335579

Epoch: 6| Step: 8
Training loss: 2.4289676298997542
Validation loss: 2.4854629501633543

Epoch: 6| Step: 9
Training loss: 2.427782691971931
Validation loss: 2.486971507967729

Epoch: 6| Step: 10
Training loss: 1.9194126637476587
Validation loss: 2.4836100234284206

Epoch: 6| Step: 11
Training loss: 2.7988848577314465
Validation loss: 2.479213791621067

Epoch: 6| Step: 12
Training loss: 2.437046497901624
Validation loss: 2.503009970979012

Epoch: 6| Step: 13
Training loss: 2.3447834025479843
Validation loss: 2.4863954641328405

Epoch: 178| Step: 0
Training loss: 2.5680106850993005
Validation loss: 2.4850100779185116

Epoch: 6| Step: 1
Training loss: 3.4249625712110747
Validation loss: 2.505008258615612

Epoch: 6| Step: 2
Training loss: 1.975569222908006
Validation loss: 2.4936697925489733

Epoch: 6| Step: 3
Training loss: 2.4276567908585553
Validation loss: 2.5031378445273926

Epoch: 6| Step: 4
Training loss: 2.1481385179038273
Validation loss: 2.51678375346503

Epoch: 6| Step: 5
Training loss: 2.337387808488987
Validation loss: 2.507850607070491

Epoch: 6| Step: 6
Training loss: 2.2185746244509814
Validation loss: 2.5114095921785804

Epoch: 6| Step: 7
Training loss: 2.848723369907446
Validation loss: 2.5130427120751913

Epoch: 6| Step: 8
Training loss: 1.9748087102264067
Validation loss: 2.513663338600016

Epoch: 6| Step: 9
Training loss: 2.139754292295376
Validation loss: 2.5114765119639655

Epoch: 6| Step: 10
Training loss: 2.2888642833934454
Validation loss: 2.5140342658658232

Epoch: 6| Step: 11
Training loss: 2.315300354227426
Validation loss: 2.507310161408784

Epoch: 6| Step: 12
Training loss: 2.0161796812032833
Validation loss: 2.502350989220206

Epoch: 6| Step: 13
Training loss: 2.7275266500232247
Validation loss: 2.504871533463276

Epoch: 179| Step: 0
Training loss: 2.072981010729116
Validation loss: 2.509091519518834

Epoch: 6| Step: 1
Training loss: 2.2185120790817745
Validation loss: 2.499370098231626

Epoch: 6| Step: 2
Training loss: 2.8472030969819957
Validation loss: 2.4980460319288955

Epoch: 6| Step: 3
Training loss: 2.4950926299855585
Validation loss: 2.489147281100843

Epoch: 6| Step: 4
Training loss: 2.814201603172658
Validation loss: 2.4944718752567936

Epoch: 6| Step: 5
Training loss: 2.2453117269728358
Validation loss: 2.4870023689718894

Epoch: 6| Step: 6
Training loss: 2.718619858707213
Validation loss: 2.4838064890790594

Epoch: 6| Step: 7
Training loss: 2.3085122650792305
Validation loss: 2.489168081939809

Epoch: 6| Step: 8
Training loss: 2.7658464590942904
Validation loss: 2.50132042664463

Epoch: 6| Step: 9
Training loss: 2.5670182007997733
Validation loss: 2.489944708438795

Epoch: 6| Step: 10
Training loss: 2.054962487910957
Validation loss: 2.4945819477234403

Epoch: 6| Step: 11
Training loss: 2.1830180437964812
Validation loss: 2.4924141393149117

Epoch: 6| Step: 12
Training loss: 2.5808706748395167
Validation loss: 2.4996216646657285

Epoch: 6| Step: 13
Training loss: 1.9472743180519978
Validation loss: 2.5039571438525305

Epoch: 180| Step: 0
Training loss: 2.5073842666403774
Validation loss: 2.5016841143262907

Epoch: 6| Step: 1
Training loss: 1.959279792090879
Validation loss: 2.5098700871479274

Epoch: 6| Step: 2
Training loss: 3.0402428068269303
Validation loss: 2.501789453946525

Epoch: 6| Step: 3
Training loss: 2.9155387514038464
Validation loss: 2.502393189004477

Epoch: 6| Step: 4
Training loss: 2.214956667542961
Validation loss: 2.495054726213869

Epoch: 6| Step: 5
Training loss: 2.4289940338124314
Validation loss: 2.491183916955008

Epoch: 6| Step: 6
Training loss: 3.0557881199349453
Validation loss: 2.4874391992445064

Epoch: 6| Step: 7
Training loss: 2.0884249710878486
Validation loss: 2.496857822185656

Epoch: 6| Step: 8
Training loss: 2.0554690886975324
Validation loss: 2.492964022687819

Epoch: 6| Step: 9
Training loss: 2.20069849023206
Validation loss: 2.4952511506968014

Epoch: 6| Step: 10
Training loss: 2.3405613889313917
Validation loss: 2.494048027006459

Epoch: 6| Step: 11
Training loss: 2.235950901249912
Validation loss: 2.4878148111339757

Epoch: 6| Step: 12
Training loss: 1.8519130970399331
Validation loss: 2.501936305731791

Epoch: 6| Step: 13
Training loss: 2.4420619236735983
Validation loss: 2.497013040163544

Epoch: 181| Step: 0
Training loss: 2.4410499740042066
Validation loss: 2.4963422561022854

Epoch: 6| Step: 1
Training loss: 2.6949006139091023
Validation loss: 2.4981406290944355

Epoch: 6| Step: 2
Training loss: 3.12607861024271
Validation loss: 2.5169299356385535

Epoch: 6| Step: 3
Training loss: 2.6169092742155007
Validation loss: 2.5053163427172995

Epoch: 6| Step: 4
Training loss: 1.6436003816341904
Validation loss: 2.494320378289724

Epoch: 6| Step: 5
Training loss: 2.5371155771791973
Validation loss: 2.5006449582715247

Epoch: 6| Step: 6
Training loss: 2.3801673089852753
Validation loss: 2.4974176262046224

Epoch: 6| Step: 7
Training loss: 2.3198232423928435
Validation loss: 2.503294443022272

Epoch: 6| Step: 8
Training loss: 2.207422596491377
Validation loss: 2.489831398665075

Epoch: 6| Step: 9
Training loss: 2.243666742421834
Validation loss: 2.5113610407937355

Epoch: 6| Step: 10
Training loss: 2.447860803710714
Validation loss: 2.50078792871046

Epoch: 6| Step: 11
Training loss: 2.1844381429689794
Validation loss: 2.496969213765194

Epoch: 6| Step: 12
Training loss: 2.163526888781423
Validation loss: 2.499755855400969

Epoch: 6| Step: 13
Training loss: 2.7995006660996147
Validation loss: 2.4998477809976904

Epoch: 182| Step: 0
Training loss: 2.469292786237845
Validation loss: 2.515026850699244

Epoch: 6| Step: 1
Training loss: 2.88121968747824
Validation loss: 2.5084256128802784

Epoch: 6| Step: 2
Training loss: 2.6703700238544728
Validation loss: 2.513415011521496

Epoch: 6| Step: 3
Training loss: 2.508054062091813
Validation loss: 2.5079807530069704

Epoch: 6| Step: 4
Training loss: 2.4456272105541617
Validation loss: 2.508530574364637

Epoch: 6| Step: 5
Training loss: 2.590670492511358
Validation loss: 2.502843558895556

Epoch: 6| Step: 6
Training loss: 2.5462201920769787
Validation loss: 2.496091553115475

Epoch: 6| Step: 7
Training loss: 2.1933020892521893
Validation loss: 2.4971780745022447

Epoch: 6| Step: 8
Training loss: 2.2019513839219433
Validation loss: 2.4993145241352246

Epoch: 6| Step: 9
Training loss: 2.3529206976279866
Validation loss: 2.490412040805291

Epoch: 6| Step: 10
Training loss: 1.7032894790040503
Validation loss: 2.5103874099314027

Epoch: 6| Step: 11
Training loss: 2.3972795406172005
Validation loss: 2.511679103730242

Epoch: 6| Step: 12
Training loss: 2.1736745720138355
Validation loss: 2.513830235770605

Epoch: 6| Step: 13
Training loss: 2.62772373397485
Validation loss: 2.5164030937041595

Epoch: 183| Step: 0
Training loss: 2.9675232611421807
Validation loss: 2.5063873392908964

Epoch: 6| Step: 1
Training loss: 1.8587997572497905
Validation loss: 2.5039330062983947

Epoch: 6| Step: 2
Training loss: 2.3401840548389483
Validation loss: 2.497425486229814

Epoch: 6| Step: 3
Training loss: 2.3731368938687494
Validation loss: 2.4973658194278117

Epoch: 6| Step: 4
Training loss: 2.616974870385462
Validation loss: 2.4977056306506236

Epoch: 6| Step: 5
Training loss: 1.9509301632534388
Validation loss: 2.501407259004578

Epoch: 6| Step: 6
Training loss: 2.823654012817495
Validation loss: 2.497653511657582

Epoch: 6| Step: 7
Training loss: 2.1067108284067766
Validation loss: 2.503972616521261

Epoch: 6| Step: 8
Training loss: 2.3250117640043864
Validation loss: 2.505394900242371

Epoch: 6| Step: 9
Training loss: 2.9494147027203628
Validation loss: 2.5100272313194796

Epoch: 6| Step: 10
Training loss: 2.181923761667051
Validation loss: 2.5167138169989616

Epoch: 6| Step: 11
Training loss: 2.0596822506550203
Validation loss: 2.507964560393706

Epoch: 6| Step: 12
Training loss: 2.2756166272217593
Validation loss: 2.5061220946616585

Epoch: 6| Step: 13
Training loss: 2.601720123556406
Validation loss: 2.5019079636590766

Epoch: 184| Step: 0
Training loss: 2.130974057587879
Validation loss: 2.5083890590835467

Epoch: 6| Step: 1
Training loss: 2.612301310599453
Validation loss: 2.513820214033105

Epoch: 6| Step: 2
Training loss: 2.1549577019383825
Validation loss: 2.5093523726783205

Epoch: 6| Step: 3
Training loss: 2.5622484037119078
Validation loss: 2.5033396825273897

Epoch: 6| Step: 4
Training loss: 2.513727076353376
Validation loss: 2.504828272889433

Epoch: 6| Step: 5
Training loss: 2.2025333990017635
Validation loss: 2.513889100816506

Epoch: 6| Step: 6
Training loss: 1.998625879304585
Validation loss: 2.516203977565392

Epoch: 6| Step: 7
Training loss: 2.596435627222172
Validation loss: 2.5206165903614672

Epoch: 6| Step: 8
Training loss: 2.321662484425623
Validation loss: 2.51347179945462

Epoch: 6| Step: 9
Training loss: 2.014948058488298
Validation loss: 2.5151325004363043

Epoch: 6| Step: 10
Training loss: 2.2876265777420697
Validation loss: 2.516928909440553

Epoch: 6| Step: 11
Training loss: 2.959096210242196
Validation loss: 2.521001862706454

Epoch: 6| Step: 12
Training loss: 3.0216099935506238
Validation loss: 2.518406161873702

Epoch: 6| Step: 13
Training loss: 2.1767477357886995
Validation loss: 2.5241203840699002

Epoch: 185| Step: 0
Training loss: 2.2452658545547326
Validation loss: 2.531307643657882

Epoch: 6| Step: 1
Training loss: 2.6935277364778134
Validation loss: 2.5163422503855553

Epoch: 6| Step: 2
Training loss: 1.913151115331456
Validation loss: 2.509721498806522

Epoch: 6| Step: 3
Training loss: 2.226634348998372
Validation loss: 2.514436476579599

Epoch: 6| Step: 4
Training loss: 2.4727590337666765
Validation loss: 2.500637751931879

Epoch: 6| Step: 5
Training loss: 2.6178593470687375
Validation loss: 2.5091774811686367

Epoch: 6| Step: 6
Training loss: 2.862591711182655
Validation loss: 2.5016630363395658

Epoch: 6| Step: 7
Training loss: 2.3387916504960273
Validation loss: 2.4989551745054617

Epoch: 6| Step: 8
Training loss: 2.3137242968351126
Validation loss: 2.511329759185951

Epoch: 6| Step: 9
Training loss: 2.4434167503005733
Validation loss: 2.4938952933476823

Epoch: 6| Step: 10
Training loss: 2.5211339303695945
Validation loss: 2.50729442406632

Epoch: 6| Step: 11
Training loss: 1.7833342822538805
Validation loss: 2.503487157637484

Epoch: 6| Step: 12
Training loss: 2.35548312307752
Validation loss: 2.5005005017591357

Epoch: 6| Step: 13
Training loss: 2.7314365838258814
Validation loss: 2.500458389219761

Epoch: 186| Step: 0
Training loss: 2.260841423476701
Validation loss: 2.507706494333987

Epoch: 6| Step: 1
Training loss: 2.6971943795464814
Validation loss: 2.512846824406188

Epoch: 6| Step: 2
Training loss: 1.9653202639399272
Validation loss: 2.515851407495273

Epoch: 6| Step: 3
Training loss: 1.9737261297559425
Validation loss: 2.529533777716568

Epoch: 6| Step: 4
Training loss: 2.2017564784214887
Validation loss: 2.5339952973871873

Epoch: 6| Step: 5
Training loss: 2.3026478163067803
Validation loss: 2.5374690293550213

Epoch: 6| Step: 6
Training loss: 3.3081044237198536
Validation loss: 2.521891747426546

Epoch: 6| Step: 7
Training loss: 2.0895359276071876
Validation loss: 2.5238199013156497

Epoch: 6| Step: 8
Training loss: 1.935103934500663
Validation loss: 2.5124600563859714

Epoch: 6| Step: 9
Training loss: 3.109016742866123
Validation loss: 2.5141477966696266

Epoch: 6| Step: 10
Training loss: 2.1562223570890016
Validation loss: 2.506079577124009

Epoch: 6| Step: 11
Training loss: 2.578706987755439
Validation loss: 2.493374349589091

Epoch: 6| Step: 12
Training loss: 1.927112049885455
Validation loss: 2.4937301373626872

Epoch: 6| Step: 13
Training loss: 2.7088314087468017
Validation loss: 2.4890840793272058

Epoch: 187| Step: 0
Training loss: 2.1263064687901294
Validation loss: 2.4891067804267974

Epoch: 6| Step: 1
Training loss: 2.896418853039251
Validation loss: 2.4858623986415784

Epoch: 6| Step: 2
Training loss: 2.724385176427624
Validation loss: 2.4826723818235177

Epoch: 6| Step: 3
Training loss: 1.940785360544629
Validation loss: 2.490571720984533

Epoch: 6| Step: 4
Training loss: 2.2064540952345784
Validation loss: 2.4906898072098285

Epoch: 6| Step: 5
Training loss: 2.7038505330224036
Validation loss: 2.4949305315864447

Epoch: 6| Step: 6
Training loss: 2.5478575513775916
Validation loss: 2.5020696814866303

Epoch: 6| Step: 7
Training loss: 2.4180999868148816
Validation loss: 2.5006027528204937

Epoch: 6| Step: 8
Training loss: 2.878670381280653
Validation loss: 2.503323983718066

Epoch: 6| Step: 9
Training loss: 1.7640408861780699
Validation loss: 2.4984028562331164

Epoch: 6| Step: 10
Training loss: 2.696612235448464
Validation loss: 2.5122615055743593

Epoch: 6| Step: 11
Training loss: 1.95712718614265
Validation loss: 2.514752775823857

Epoch: 6| Step: 12
Training loss: 2.298124842639842
Validation loss: 2.510654459305574

Epoch: 6| Step: 13
Training loss: 2.317189590751543
Validation loss: 2.523263206800885

Epoch: 188| Step: 0
Training loss: 2.3294441530992853
Validation loss: 2.515616959900575

Epoch: 6| Step: 1
Training loss: 2.7214521813271393
Validation loss: 2.5082539677869433

Epoch: 6| Step: 2
Training loss: 1.615206255777572
Validation loss: 2.518906327059045

Epoch: 6| Step: 3
Training loss: 2.569903402034515
Validation loss: 2.5209390181989297

Epoch: 6| Step: 4
Training loss: 2.2933247434470005
Validation loss: 2.520798443849681

Epoch: 6| Step: 5
Training loss: 2.028313729978486
Validation loss: 2.5147905565118815

Epoch: 6| Step: 6
Training loss: 2.50676936145525
Validation loss: 2.5155846175907453

Epoch: 6| Step: 7
Training loss: 2.169269563293696
Validation loss: 2.5159494415898163

Epoch: 6| Step: 8
Training loss: 2.4147777300288147
Validation loss: 2.521672705470601

Epoch: 6| Step: 9
Training loss: 3.0040843499096477
Validation loss: 2.5229794113038815

Epoch: 6| Step: 10
Training loss: 2.106407961030108
Validation loss: 2.5166543785536266

Epoch: 6| Step: 11
Training loss: 2.901807774051194
Validation loss: 2.519784336045257

Epoch: 6| Step: 12
Training loss: 2.212816067118868
Validation loss: 2.518748348541602

Epoch: 6| Step: 13
Training loss: 2.3805112639018278
Validation loss: 2.5195254619048906

Epoch: 189| Step: 0
Training loss: 2.287254166030884
Validation loss: 2.516265226919104

Epoch: 6| Step: 1
Training loss: 2.23192050807324
Validation loss: 2.515044656790619

Epoch: 6| Step: 2
Training loss: 2.456186995535428
Validation loss: 2.518729077765793

Epoch: 6| Step: 3
Training loss: 2.281893495700493
Validation loss: 2.5213456670981245

Epoch: 6| Step: 4
Training loss: 2.3280021071801364
Validation loss: 2.51411095464436

Epoch: 6| Step: 5
Training loss: 3.217154542625401
Validation loss: 2.515884638748915

Epoch: 6| Step: 6
Training loss: 1.7932595409871803
Validation loss: 2.509336774827068

Epoch: 6| Step: 7
Training loss: 1.8417236054653323
Validation loss: 2.5110550750633536

Epoch: 6| Step: 8
Training loss: 2.9133925089583936
Validation loss: 2.5152419531176284

Epoch: 6| Step: 9
Training loss: 2.6718294017090907
Validation loss: 2.509242678930671

Epoch: 6| Step: 10
Training loss: 2.347180919797976
Validation loss: 2.5040451225535656

Epoch: 6| Step: 11
Training loss: 2.106616328499157
Validation loss: 2.5069182436540483

Epoch: 6| Step: 12
Training loss: 2.410918623726129
Validation loss: 2.506672807197287

Epoch: 6| Step: 13
Training loss: 2.2100405541751114
Validation loss: 2.516576038290863

Epoch: 190| Step: 0
Training loss: 2.4007555169794212
Validation loss: 2.514413767114442

Epoch: 6| Step: 1
Training loss: 2.393135896400398
Validation loss: 2.5176633391794145

Epoch: 6| Step: 2
Training loss: 2.948240570138675
Validation loss: 2.5187451932875438

Epoch: 6| Step: 3
Training loss: 3.023555152043086
Validation loss: 2.531444133959096

Epoch: 6| Step: 4
Training loss: 2.795024323468495
Validation loss: 2.523090870200949

Epoch: 6| Step: 5
Training loss: 2.3062885477623025
Validation loss: 2.5290446151955117

Epoch: 6| Step: 6
Training loss: 2.6154989232677472
Validation loss: 2.539449511646078

Epoch: 6| Step: 7
Training loss: 2.435380356395313
Validation loss: 2.5354479292292886

Epoch: 6| Step: 8
Training loss: 1.7201975188951593
Validation loss: 2.528542542369555

Epoch: 6| Step: 9
Training loss: 1.6571059983950913
Validation loss: 2.5395714039531496

Epoch: 6| Step: 10
Training loss: 1.989712542853637
Validation loss: 2.5425894474385577

Epoch: 6| Step: 11
Training loss: 2.2244864439050716
Validation loss: 2.541704776874262

Epoch: 6| Step: 12
Training loss: 2.489164043110539
Validation loss: 2.5257480306473266

Epoch: 6| Step: 13
Training loss: 2.061932109910243
Validation loss: 2.5226879916099616

Epoch: 191| Step: 0
Training loss: 2.9986696472469774
Validation loss: 2.5231235808856014

Epoch: 6| Step: 1
Training loss: 2.217743766314067
Validation loss: 2.529494936931766

Epoch: 6| Step: 2
Training loss: 2.36825084409293
Validation loss: 2.5357880121101313

Epoch: 6| Step: 3
Training loss: 2.066802536692243
Validation loss: 2.5294492462097358

Epoch: 6| Step: 4
Training loss: 2.960608310567866
Validation loss: 2.5341348182268373

Epoch: 6| Step: 5
Training loss: 2.282956673771188
Validation loss: 2.5217506274028447

Epoch: 6| Step: 6
Training loss: 2.522887556778048
Validation loss: 2.5364305049510683

Epoch: 6| Step: 7
Training loss: 2.174918697196864
Validation loss: 2.538615495507975

Epoch: 6| Step: 8
Training loss: 3.1646432602619967
Validation loss: 2.513731423487503

Epoch: 6| Step: 9
Training loss: 2.102150306605134
Validation loss: 2.508714642379305

Epoch: 6| Step: 10
Training loss: 1.537147220984214
Validation loss: 2.5055962551032116

Epoch: 6| Step: 11
Training loss: 2.8751814411793473
Validation loss: 2.507191930820291

Epoch: 6| Step: 12
Training loss: 1.6089039826226648
Validation loss: 2.500678320095917

Epoch: 6| Step: 13
Training loss: 1.9666644963829005
Validation loss: 2.4983982518087284

Epoch: 192| Step: 0
Training loss: 2.285997858508718
Validation loss: 2.5068480161071984

Epoch: 6| Step: 1
Training loss: 2.967708364754263
Validation loss: 2.511030024604349

Epoch: 6| Step: 2
Training loss: 2.4096790008137785
Validation loss: 2.5099194193819794

Epoch: 6| Step: 3
Training loss: 1.9057393875397064
Validation loss: 2.518352790856653

Epoch: 6| Step: 4
Training loss: 1.849200129928931
Validation loss: 2.505966759053536

Epoch: 6| Step: 5
Training loss: 2.649713529541496
Validation loss: 2.5134545513622704

Epoch: 6| Step: 6
Training loss: 1.7139266566204903
Validation loss: 2.5157119822815877

Epoch: 6| Step: 7
Training loss: 2.015230125368048
Validation loss: 2.5117970044575797

Epoch: 6| Step: 8
Training loss: 3.0688460636058914
Validation loss: 2.520653589529348

Epoch: 6| Step: 9
Training loss: 2.4216862758814828
Validation loss: 2.518723769007954

Epoch: 6| Step: 10
Training loss: 2.866708325667382
Validation loss: 2.529140613888873

Epoch: 6| Step: 11
Training loss: 2.3055378008034997
Validation loss: 2.520584974323488

Epoch: 6| Step: 12
Training loss: 2.289281841093714
Validation loss: 2.5220500017130805

Epoch: 6| Step: 13
Training loss: 2.2748006293161622
Validation loss: 2.5214682533463844

Epoch: 193| Step: 0
Training loss: 2.4233319238014093
Validation loss: 2.521555447727928

Epoch: 6| Step: 1
Training loss: 1.8631931500030101
Validation loss: 2.532656761431556

Epoch: 6| Step: 2
Training loss: 2.087861391867379
Validation loss: 2.5272578244978883

Epoch: 6| Step: 3
Training loss: 2.3034567440037987
Validation loss: 2.517331557410969

Epoch: 6| Step: 4
Training loss: 2.0573782472056994
Validation loss: 2.5227430901850343

Epoch: 6| Step: 5
Training loss: 2.66697305667571
Validation loss: 2.529680966703992

Epoch: 6| Step: 6
Training loss: 2.454362204964948
Validation loss: 2.535118051798569

Epoch: 6| Step: 7
Training loss: 2.6089454342979823
Validation loss: 2.533556323108784

Epoch: 6| Step: 8
Training loss: 2.268358357571991
Validation loss: 2.5513799100175754

Epoch: 6| Step: 9
Training loss: 2.8906357223724277
Validation loss: 2.5264606770647986

Epoch: 6| Step: 10
Training loss: 2.4063036033307577
Validation loss: 2.527336423255013

Epoch: 6| Step: 11
Training loss: 2.3201727519995345
Validation loss: 2.5213872652981464

Epoch: 6| Step: 12
Training loss: 2.3765504695028543
Validation loss: 2.522522090062788

Epoch: 6| Step: 13
Training loss: 2.4491822454479037
Validation loss: 2.5246558776378483

Epoch: 194| Step: 0
Training loss: 2.624063506422069
Validation loss: 2.5218039343872487

Epoch: 6| Step: 1
Training loss: 2.44539527631088
Validation loss: 2.524884874722009

Epoch: 6| Step: 2
Training loss: 2.7969418309927416
Validation loss: 2.5239166025408246

Epoch: 6| Step: 3
Training loss: 2.044103012161719
Validation loss: 2.531760639127512

Epoch: 6| Step: 4
Training loss: 2.404432328574225
Validation loss: 2.528534802645455

Epoch: 6| Step: 5
Training loss: 2.2050549866617066
Validation loss: 2.515132721621468

Epoch: 6| Step: 6
Training loss: 2.2656973991665
Validation loss: 2.5117495363364837

Epoch: 6| Step: 7
Training loss: 2.1499189228473
Validation loss: 2.5090044700534087

Epoch: 6| Step: 8
Training loss: 2.856593378908149
Validation loss: 2.5147155476228997

Epoch: 6| Step: 9
Training loss: 2.563252594382743
Validation loss: 2.50246799560315

Epoch: 6| Step: 10
Training loss: 2.3678354314504477
Validation loss: 2.507425779510071

Epoch: 6| Step: 11
Training loss: 1.9495848653968988
Validation loss: 2.5084565505204184

Epoch: 6| Step: 12
Training loss: 2.7311580371868955
Validation loss: 2.5121355358610913

Epoch: 6| Step: 13
Training loss: 1.8119674426389658
Validation loss: 2.5147527600225303

Epoch: 195| Step: 0
Training loss: 2.9115090317426366
Validation loss: 2.513665883714896

Epoch: 6| Step: 1
Training loss: 1.995692203392538
Validation loss: 2.521888407025946

Epoch: 6| Step: 2
Training loss: 2.0847536077755224
Validation loss: 2.523884193401021

Epoch: 6| Step: 3
Training loss: 1.9745113903150846
Validation loss: 2.521574625990007

Epoch: 6| Step: 4
Training loss: 2.5894460243246287
Validation loss: 2.529390821835521

Epoch: 6| Step: 5
Training loss: 1.8526543082193045
Validation loss: 2.5313278625820312

Epoch: 6| Step: 6
Training loss: 2.4839406143559417
Validation loss: 2.52361790629244

Epoch: 6| Step: 7
Training loss: 2.1552400296884078
Validation loss: 2.5381319345922613

Epoch: 6| Step: 8
Training loss: 2.8540623406790577
Validation loss: 2.5507182200629717

Epoch: 6| Step: 9
Training loss: 2.5469125382167013
Validation loss: 2.541148145867075

Epoch: 6| Step: 10
Training loss: 2.130488320552072
Validation loss: 2.553366581083652

Epoch: 6| Step: 11
Training loss: 2.8786485160666513
Validation loss: 2.547139955558252

Epoch: 6| Step: 12
Training loss: 2.641931786054346
Validation loss: 2.554138917964117

Epoch: 6| Step: 13
Training loss: 1.834978997840539
Validation loss: 2.542139757142207

Epoch: 196| Step: 0
Training loss: 2.0611035792694987
Validation loss: 2.530424022524631

Epoch: 6| Step: 1
Training loss: 2.601209828526235
Validation loss: 2.529440166091717

Epoch: 6| Step: 2
Training loss: 2.4251496848875345
Validation loss: 2.5212888279857935

Epoch: 6| Step: 3
Training loss: 2.3578027197936446
Validation loss: 2.517029703767989

Epoch: 6| Step: 4
Training loss: 1.8989540935670608
Validation loss: 2.514500052509167

Epoch: 6| Step: 5
Training loss: 2.631125161297045
Validation loss: 2.5127718207498875

Epoch: 6| Step: 6
Training loss: 1.5869341194645965
Validation loss: 2.5155393376583963

Epoch: 6| Step: 7
Training loss: 2.078943765295313
Validation loss: 2.5151966512179103

Epoch: 6| Step: 8
Training loss: 3.2307080107607153
Validation loss: 2.523493849811148

Epoch: 6| Step: 9
Training loss: 2.0418806807267047
Validation loss: 2.511692369321532

Epoch: 6| Step: 10
Training loss: 3.0453493651365036
Validation loss: 2.5222319724146094

Epoch: 6| Step: 11
Training loss: 2.6659450647427674
Validation loss: 2.523838983648302

Epoch: 6| Step: 12
Training loss: 2.0976028293506155
Validation loss: 2.5186255273597697

Epoch: 6| Step: 13
Training loss: 2.1572930950440656
Validation loss: 2.5218135147131133

Epoch: 197| Step: 0
Training loss: 1.4705491105653263
Validation loss: 2.5322651652355503

Epoch: 6| Step: 1
Training loss: 2.3555663232537247
Validation loss: 2.5274990924339287

Epoch: 6| Step: 2
Training loss: 2.8833867356604825
Validation loss: 2.531030637552893

Epoch: 6| Step: 3
Training loss: 2.458757482458151
Validation loss: 2.5321773823901563

Epoch: 6| Step: 4
Training loss: 2.849745581798741
Validation loss: 2.5285108211367

Epoch: 6| Step: 5
Training loss: 2.5519090761651197
Validation loss: 2.5484663939265717

Epoch: 6| Step: 6
Training loss: 2.2765298373694445
Validation loss: 2.538726706185874

Epoch: 6| Step: 7
Training loss: 1.989029419437362
Validation loss: 2.538793555467864

Epoch: 6| Step: 8
Training loss: 2.694693762484297
Validation loss: 2.5464300535372613

Epoch: 6| Step: 9
Training loss: 2.043368415323925
Validation loss: 2.5360458686192335

Epoch: 6| Step: 10
Training loss: 2.1652921082247607
Validation loss: 2.5356480572531086

Epoch: 6| Step: 11
Training loss: 2.277640217087873
Validation loss: 2.5374877741476856

Epoch: 6| Step: 12
Training loss: 2.429480638928155
Validation loss: 2.5179597118064367

Epoch: 6| Step: 13
Training loss: 2.4969389294973077
Validation loss: 2.5305693696397897

Epoch: 198| Step: 0
Training loss: 1.973276414737672
Validation loss: 2.509357249956073

Epoch: 6| Step: 1
Training loss: 2.501661797386817
Validation loss: 2.5231940483703803

Epoch: 6| Step: 2
Training loss: 2.4086579993850745
Validation loss: 2.5218620303110804

Epoch: 6| Step: 3
Training loss: 1.876163630216243
Validation loss: 2.513726807621201

Epoch: 6| Step: 4
Training loss: 2.431933968249906
Validation loss: 2.520561539891338

Epoch: 6| Step: 5
Training loss: 2.7773451235052864
Validation loss: 2.5182137605550237

Epoch: 6| Step: 6
Training loss: 2.236208290320961
Validation loss: 2.5202971484573435

Epoch: 6| Step: 7
Training loss: 2.0574864806313933
Validation loss: 2.5050370494211815

Epoch: 6| Step: 8
Training loss: 2.3295740316122617
Validation loss: 2.5068574713017506

Epoch: 6| Step: 9
Training loss: 2.697970376284023
Validation loss: 2.525059811352675

Epoch: 6| Step: 10
Training loss: 1.93041644826364
Validation loss: 2.5110757814606757

Epoch: 6| Step: 11
Training loss: 3.0979151575477752
Validation loss: 2.5125922171906616

Epoch: 6| Step: 12
Training loss: 2.4876533806061114
Validation loss: 2.516784874453628

Epoch: 6| Step: 13
Training loss: 2.2430838145814036
Validation loss: 2.518109928583707

Epoch: 199| Step: 0
Training loss: 2.738330269237374
Validation loss: 2.5177430423674148

Epoch: 6| Step: 1
Training loss: 2.580010481820512
Validation loss: 2.5404509518773737

Epoch: 6| Step: 2
Training loss: 2.2466994555237974
Validation loss: 2.537540155478268

Epoch: 6| Step: 3
Training loss: 2.588473640842641
Validation loss: 2.526456910181692

Epoch: 6| Step: 4
Training loss: 1.6012022031846567
Validation loss: 2.5281667887436825

Epoch: 6| Step: 5
Training loss: 3.1792363777033787
Validation loss: 2.519690078342296

Epoch: 6| Step: 6
Training loss: 2.4435340337129694
Validation loss: 2.5122817670643602

Epoch: 6| Step: 7
Training loss: 2.0879273942184464
Validation loss: 2.50939945068102

Epoch: 6| Step: 8
Training loss: 2.6056142420373942
Validation loss: 2.5071800282095307

Epoch: 6| Step: 9
Training loss: 2.0302888210020247
Validation loss: 2.510868195465708

Epoch: 6| Step: 10
Training loss: 2.527251299386158
Validation loss: 2.5161786941049162

Epoch: 6| Step: 11
Training loss: 2.1547257937983573
Validation loss: 2.5154098039598436

Epoch: 6| Step: 12
Training loss: 2.572885819429097
Validation loss: 2.524110387445698

Epoch: 6| Step: 13
Training loss: 1.7275874345130346
Validation loss: 2.5217873420591483

Epoch: 200| Step: 0
Training loss: 2.2282539234073337
Validation loss: 2.5265418327648512

Epoch: 6| Step: 1
Training loss: 2.274112561864685
Validation loss: 2.532449179211922

Epoch: 6| Step: 2
Training loss: 1.9998471678513794
Validation loss: 2.53193669364758

Epoch: 6| Step: 3
Training loss: 2.598123104047967
Validation loss: 2.5328093854856393

Epoch: 6| Step: 4
Training loss: 2.430215661639694
Validation loss: 2.5405251774683046

Epoch: 6| Step: 5
Training loss: 2.3417628701789868
Validation loss: 2.5337078108409323

Epoch: 6| Step: 6
Training loss: 2.508660003918185
Validation loss: 2.538817048557766

Epoch: 6| Step: 7
Training loss: 2.2576606461828477
Validation loss: 2.531538519094919

Epoch: 6| Step: 8
Training loss: 3.0160727687323896
Validation loss: 2.5237252747456758

Epoch: 6| Step: 9
Training loss: 2.1005026261227653
Validation loss: 2.5434425643106198

Epoch: 6| Step: 10
Training loss: 2.6734075779803814
Validation loss: 2.5274353010761916

Epoch: 6| Step: 11
Training loss: 1.9963618805740677
Validation loss: 2.5356355830156554

Epoch: 6| Step: 12
Training loss: 2.2108128381098586
Validation loss: 2.5341704048079827

Epoch: 6| Step: 13
Training loss: 2.38277267672921
Validation loss: 2.5386390058983324

Epoch: 201| Step: 0
Training loss: 1.720588117343444
Validation loss: 2.5304661388723324

Epoch: 6| Step: 1
Training loss: 2.2394952327935593
Validation loss: 2.528685066921887

Epoch: 6| Step: 2
Training loss: 2.250590246981522
Validation loss: 2.5343305735176886

Epoch: 6| Step: 3
Training loss: 2.7418286558799303
Validation loss: 2.532166201392738

Epoch: 6| Step: 4
Training loss: 2.1580031634600116
Validation loss: 2.5356952582092513

Epoch: 6| Step: 5
Training loss: 2.448228748529629
Validation loss: 2.530590725037993

Epoch: 6| Step: 6
Training loss: 2.7590930764459882
Validation loss: 2.518144400326133

Epoch: 6| Step: 7
Training loss: 2.509262283853061
Validation loss: 2.5209524005692114

Epoch: 6| Step: 8
Training loss: 2.2130416724293043
Validation loss: 2.5232924979704845

Epoch: 6| Step: 9
Training loss: 2.2967357982571373
Validation loss: 2.526447795716869

Epoch: 6| Step: 10
Training loss: 2.596949247361523
Validation loss: 2.512053566741188

Epoch: 6| Step: 11
Training loss: 1.7673442201071616
Validation loss: 2.519314968081897

Epoch: 6| Step: 12
Training loss: 2.4992641319645394
Validation loss: 2.528437712276508

Epoch: 6| Step: 13
Training loss: 2.712803794619247
Validation loss: 2.524067078778659

Epoch: 202| Step: 0
Training loss: 2.0085921499334227
Validation loss: 2.5028204425849747

Epoch: 6| Step: 1
Training loss: 2.281053403979271
Validation loss: 2.5125660275565735

Epoch: 6| Step: 2
Training loss: 2.72153146461297
Validation loss: 2.5092684915186902

Epoch: 6| Step: 3
Training loss: 2.499758899506923
Validation loss: 2.5064941934167893

Epoch: 6| Step: 4
Training loss: 2.8315634716046163
Validation loss: 2.5065105857683925

Epoch: 6| Step: 5
Training loss: 1.8935453700937603
Validation loss: 2.4991450278310072

Epoch: 6| Step: 6
Training loss: 1.63640247284435
Validation loss: 2.5058410120564028

Epoch: 6| Step: 7
Training loss: 2.1685993549014206
Validation loss: 2.5108745969826867

Epoch: 6| Step: 8
Training loss: 2.676691454594071
Validation loss: 2.5093662602267277

Epoch: 6| Step: 9
Training loss: 2.5008993438513616
Validation loss: 2.505517323261707

Epoch: 6| Step: 10
Training loss: 2.1101597209338356
Validation loss: 2.506423407031126

Epoch: 6| Step: 11
Training loss: 2.401128892838111
Validation loss: 2.508084307208275

Epoch: 6| Step: 12
Training loss: 2.9115132899320884
Validation loss: 2.513775653173916

Epoch: 6| Step: 13
Training loss: 2.3330941645344527
Validation loss: 2.5174599820302324

Epoch: 203| Step: 0
Training loss: 2.5688416699019063
Validation loss: 2.511348983892422

Epoch: 6| Step: 1
Training loss: 2.549100313232374
Validation loss: 2.512223797573222

Epoch: 6| Step: 2
Training loss: 2.4366105241826435
Validation loss: 2.5236819358516263

Epoch: 6| Step: 3
Training loss: 2.5504903583654723
Validation loss: 2.5069658983556575

Epoch: 6| Step: 4
Training loss: 2.3106084768639867
Validation loss: 2.518411897317024

Epoch: 6| Step: 5
Training loss: 2.5172794185551877
Validation loss: 2.533578986462978

Epoch: 6| Step: 6
Training loss: 2.8422287445682826
Validation loss: 2.5291218229421024

Epoch: 6| Step: 7
Training loss: 2.6133946506773924
Validation loss: 2.5231667088893017

Epoch: 6| Step: 8
Training loss: 1.6034512431407568
Validation loss: 2.529385307661136

Epoch: 6| Step: 9
Training loss: 2.086366187917804
Validation loss: 2.5349775034817306

Epoch: 6| Step: 10
Training loss: 2.117724023383379
Validation loss: 2.5266504923907553

Epoch: 6| Step: 11
Training loss: 1.9479686573726491
Validation loss: 2.5228861234933446

Epoch: 6| Step: 12
Training loss: 2.5737626593539216
Validation loss: 2.526888492386863

Epoch: 6| Step: 13
Training loss: 2.2469324818183436
Validation loss: 2.5169733986866545

Epoch: 204| Step: 0
Training loss: 2.2491506456914263
Validation loss: 2.5140111497147735

Epoch: 6| Step: 1
Training loss: 2.5212818303744364
Validation loss: 2.4884725484168686

Epoch: 6| Step: 2
Training loss: 2.4179036712516377
Validation loss: 2.5059474137723736

Epoch: 6| Step: 3
Training loss: 1.97616856686522
Validation loss: 2.491222964259892

Epoch: 6| Step: 4
Training loss: 1.6627103658106968
Validation loss: 2.504023699453283

Epoch: 6| Step: 5
Training loss: 2.5029419754685196
Validation loss: 2.494413683030675

Epoch: 6| Step: 6
Training loss: 2.878454372185515
Validation loss: 2.488361143768855

Epoch: 6| Step: 7
Training loss: 2.787518242062122
Validation loss: 2.507402142852527

Epoch: 6| Step: 8
Training loss: 1.9636540536733895
Validation loss: 2.515188364871084

Epoch: 6| Step: 9
Training loss: 2.3461118877165807
Validation loss: 2.501113452911222

Epoch: 6| Step: 10
Training loss: 2.1262360512143044
Validation loss: 2.508670363062679

Epoch: 6| Step: 11
Training loss: 2.2945164618634157
Validation loss: 2.5147076863342663

Epoch: 6| Step: 12
Training loss: 2.4971100315818404
Validation loss: 2.5102890636468103

Epoch: 6| Step: 13
Training loss: 2.862959319331651
Validation loss: 2.5125572343573443

Epoch: 205| Step: 0
Training loss: 2.375299334485185
Validation loss: 2.5298006123164694

Epoch: 6| Step: 1
Training loss: 2.510748359853605
Validation loss: 2.52209949746827

Epoch: 6| Step: 2
Training loss: 2.8593246372144323
Validation loss: 2.5191837987928194

Epoch: 6| Step: 3
Training loss: 2.624428096414903
Validation loss: 2.5191643341773773

Epoch: 6| Step: 4
Training loss: 2.1851555386140804
Validation loss: 2.5243748045081698

Epoch: 6| Step: 5
Training loss: 1.6345263525938052
Validation loss: 2.5275451485575537

Epoch: 6| Step: 6
Training loss: 1.922663891159802
Validation loss: 2.529178069673962

Epoch: 6| Step: 7
Training loss: 1.7832777962400088
Validation loss: 2.5260191506762033

Epoch: 6| Step: 8
Training loss: 2.7504050216655593
Validation loss: 2.5377841019964538

Epoch: 6| Step: 9
Training loss: 2.5016836219262153
Validation loss: 2.5288099451105035

Epoch: 6| Step: 10
Training loss: 1.935526673551238
Validation loss: 2.5344051429846868

Epoch: 6| Step: 11
Training loss: 3.260826710681996
Validation loss: 2.533243861835336

Epoch: 6| Step: 12
Training loss: 1.950980939868794
Validation loss: 2.527505436107551

Epoch: 6| Step: 13
Training loss: 2.273904968651399
Validation loss: 2.5342934447562944

Epoch: 206| Step: 0
Training loss: 1.8221642240091953
Validation loss: 2.52981900554081

Epoch: 6| Step: 1
Training loss: 3.0676369693833943
Validation loss: 2.5439483037285475

Epoch: 6| Step: 2
Training loss: 3.0453728518663796
Validation loss: 2.5415216519203154

Epoch: 6| Step: 3
Training loss: 1.9635890344493465
Validation loss: 2.5622348958406254

Epoch: 6| Step: 4
Training loss: 2.552434832182848
Validation loss: 2.5507424912908454

Epoch: 6| Step: 5
Training loss: 2.2296226486687254
Validation loss: 2.557224410053807

Epoch: 6| Step: 6
Training loss: 2.2188873248518464
Validation loss: 2.555262462997983

Epoch: 6| Step: 7
Training loss: 2.1877105611551197
Validation loss: 2.552464784932859

Epoch: 6| Step: 8
Training loss: 2.3761138311919607
Validation loss: 2.5536715385942585

Epoch: 6| Step: 9
Training loss: 2.3655147874523554
Validation loss: 2.5361910345606473

Epoch: 6| Step: 10
Training loss: 2.132538830313054
Validation loss: 2.539101217903916

Epoch: 6| Step: 11
Training loss: 1.792405515811523
Validation loss: 2.518107798247681

Epoch: 6| Step: 12
Training loss: 2.5248460175047063
Validation loss: 2.50936922141582

Epoch: 6| Step: 13
Training loss: 2.431141311059647
Validation loss: 2.518802523637226

Epoch: 207| Step: 0
Training loss: 2.3081913579620523
Validation loss: 2.507386881519793

Epoch: 6| Step: 1
Training loss: 2.657512858261911
Validation loss: 2.5100966021323843

Epoch: 6| Step: 2
Training loss: 2.2966069694404654
Validation loss: 2.5117103968076497

Epoch: 6| Step: 3
Training loss: 2.5360367337764407
Validation loss: 2.5135309890750035

Epoch: 6| Step: 4
Training loss: 2.854108118352407
Validation loss: 2.5207325203233495

Epoch: 6| Step: 5
Training loss: 2.5904694918905196
Validation loss: 2.5259395827807443

Epoch: 6| Step: 6
Training loss: 2.6419538957498987
Validation loss: 2.531977592102943

Epoch: 6| Step: 7
Training loss: 1.9767914288898867
Validation loss: 2.5297832870648294

Epoch: 6| Step: 8
Training loss: 2.605167491602365
Validation loss: 2.557115355795601

Epoch: 6| Step: 9
Training loss: 1.8653247431888342
Validation loss: 2.5564634195166858

Epoch: 6| Step: 10
Training loss: 1.9573887785863822
Validation loss: 2.541215408372473

Epoch: 6| Step: 11
Training loss: 2.1918200749808063
Validation loss: 2.534735881356374

Epoch: 6| Step: 12
Training loss: 2.100957733833079
Validation loss: 2.5466610247198327

Epoch: 6| Step: 13
Training loss: 2.415389699196967
Validation loss: 2.5403095254385186

Epoch: 208| Step: 0
Training loss: 2.4250904027074354
Validation loss: 2.5311465458077635

Epoch: 6| Step: 1
Training loss: 2.5920908573615593
Validation loss: 2.51950678850468

Epoch: 6| Step: 2
Training loss: 2.0260664309938834
Validation loss: 2.520317156149435

Epoch: 6| Step: 3
Training loss: 2.196703052981562
Validation loss: 2.5171904658799917

Epoch: 6| Step: 4
Training loss: 2.160627703412577
Validation loss: 2.518349682443273

Epoch: 6| Step: 5
Training loss: 2.67762521288934
Validation loss: 2.5160079569789318

Epoch: 6| Step: 6
Training loss: 2.550264782341726
Validation loss: 2.5139812446077725

Epoch: 6| Step: 7
Training loss: 1.6156355925753232
Validation loss: 2.5203142551227598

Epoch: 6| Step: 8
Training loss: 2.2734155817712223
Validation loss: 2.514555196175365

Epoch: 6| Step: 9
Training loss: 2.947566861986983
Validation loss: 2.5171103821048195

Epoch: 6| Step: 10
Training loss: 2.807721295624152
Validation loss: 2.526139614657613

Epoch: 6| Step: 11
Training loss: 2.5594762314000032
Validation loss: 2.5158981427688603

Epoch: 6| Step: 12
Training loss: 2.0303962674033724
Validation loss: 2.514362041867646

Epoch: 6| Step: 13
Training loss: 2.091482272385574
Validation loss: 2.520959919254915

Epoch: 209| Step: 0
Training loss: 2.4297931807616124
Validation loss: 2.52282214461487

Epoch: 6| Step: 1
Training loss: 1.5717760544078667
Validation loss: 2.524513220054004

Epoch: 6| Step: 2
Training loss: 2.2979490921716264
Validation loss: 2.5253257654627856

Epoch: 6| Step: 3
Training loss: 2.630366697674185
Validation loss: 2.528949807017526

Epoch: 6| Step: 4
Training loss: 2.080852124556192
Validation loss: 2.537024203136785

Epoch: 6| Step: 5
Training loss: 2.224161131380163
Validation loss: 2.536323133830225

Epoch: 6| Step: 6
Training loss: 2.543491292258809
Validation loss: 2.5563444155890025

Epoch: 6| Step: 7
Training loss: 2.7001039237755706
Validation loss: 2.5603876944615864

Epoch: 6| Step: 8
Training loss: 2.511347385794859
Validation loss: 2.556665143079275

Epoch: 6| Step: 9
Training loss: 1.9877007916826566
Validation loss: 2.5722906043390155

Epoch: 6| Step: 10
Training loss: 2.1002513099342113
Validation loss: 2.563096310463425

Epoch: 6| Step: 11
Training loss: 3.066936782329665
Validation loss: 2.5580640249334996

Epoch: 6| Step: 12
Training loss: 1.9058909468900307
Validation loss: 2.5694966050315053

Epoch: 6| Step: 13
Training loss: 2.6895601227751382
Validation loss: 2.5625050397373665

Epoch: 210| Step: 0
Training loss: 2.3322099524547744
Validation loss: 2.5576072285815

Epoch: 6| Step: 1
Training loss: 1.901120460305684
Validation loss: 2.5372285763502016

Epoch: 6| Step: 2
Training loss: 2.1663064168109076
Validation loss: 2.531174081676424

Epoch: 6| Step: 3
Training loss: 2.0708235199949696
Validation loss: 2.530344404694952

Epoch: 6| Step: 4
Training loss: 2.6107437290173685
Validation loss: 2.526771256522459

Epoch: 6| Step: 5
Training loss: 1.9823354016315624
Validation loss: 2.524889281335063

Epoch: 6| Step: 6
Training loss: 2.601316331552031
Validation loss: 2.5323621245520003

Epoch: 6| Step: 7
Training loss: 2.2008269749753833
Validation loss: 2.5302328412965753

Epoch: 6| Step: 8
Training loss: 2.2233791147958444
Validation loss: 2.5224509815586207

Epoch: 6| Step: 9
Training loss: 2.7028377137743087
Validation loss: 2.5310182504567407

Epoch: 6| Step: 10
Training loss: 2.484470389592167
Validation loss: 2.528647839551628

Epoch: 6| Step: 11
Training loss: 1.4874709118475324
Validation loss: 2.5339653537005553

Epoch: 6| Step: 12
Training loss: 2.7711804299231777
Validation loss: 2.5200596849119856

Epoch: 6| Step: 13
Training loss: 3.0620580860411577
Validation loss: 2.5343994829317813

Epoch: 211| Step: 0
Training loss: 2.69328448528271
Validation loss: 2.533717699025365

Epoch: 6| Step: 1
Training loss: 2.297303970657748
Validation loss: 2.532631799118299

Epoch: 6| Step: 2
Training loss: 2.694177006910992
Validation loss: 2.52689883970428

Epoch: 6| Step: 3
Training loss: 1.8096093772134236
Validation loss: 2.5277734673175694

Epoch: 6| Step: 4
Training loss: 2.625819850230449
Validation loss: 2.5339101149422185

Epoch: 6| Step: 5
Training loss: 1.9675632108933843
Validation loss: 2.529236608938546

Epoch: 6| Step: 6
Training loss: 1.797870393454477
Validation loss: 2.528932177395313

Epoch: 6| Step: 7
Training loss: 1.288978851378661
Validation loss: 2.5251235289619665

Epoch: 6| Step: 8
Training loss: 2.6407730693046028
Validation loss: 2.529478261524077

Epoch: 6| Step: 9
Training loss: 2.9122609969540645
Validation loss: 2.5211198712219116

Epoch: 6| Step: 10
Training loss: 2.2252651785227964
Validation loss: 2.518388750344702

Epoch: 6| Step: 11
Training loss: 2.6785699499216995
Validation loss: 2.540505805979121

Epoch: 6| Step: 12
Training loss: 2.392229227590376
Validation loss: 2.5355990375345865

Epoch: 6| Step: 13
Training loss: 2.3630151015552614
Validation loss: 2.5430084866076994

Epoch: 212| Step: 0
Training loss: 2.1233556780794696
Validation loss: 2.5598372449149767

Epoch: 6| Step: 1
Training loss: 2.1127872700589023
Validation loss: 2.543913884862655

Epoch: 6| Step: 2
Training loss: 2.0567827456385244
Validation loss: 2.5529132401900108

Epoch: 6| Step: 3
Training loss: 1.7004596958201523
Validation loss: 2.5521739606271243

Epoch: 6| Step: 4
Training loss: 1.4827372299722401
Validation loss: 2.5657842570417864

Epoch: 6| Step: 5
Training loss: 2.177082432894239
Validation loss: 2.5678273625412023

Epoch: 6| Step: 6
Training loss: 3.1515841602948487
Validation loss: 2.580257882135416

Epoch: 6| Step: 7
Training loss: 2.6750139681727707
Validation loss: 2.575501511632779

Epoch: 6| Step: 8
Training loss: 2.4953677176694256
Validation loss: 2.5707928982547705

Epoch: 6| Step: 9
Training loss: 2.6638079658692217
Validation loss: 2.5648343533326274

Epoch: 6| Step: 10
Training loss: 2.1996040074549366
Validation loss: 2.543847232619259

Epoch: 6| Step: 11
Training loss: 2.984626479812679
Validation loss: 2.524675740613094

Epoch: 6| Step: 12
Training loss: 2.838711813397317
Validation loss: 2.5239948487851414

Epoch: 6| Step: 13
Training loss: 2.0661256307568308
Validation loss: 2.51730694825817

Epoch: 213| Step: 0
Training loss: 2.876724720969884
Validation loss: 2.5116843403643925

Epoch: 6| Step: 1
Training loss: 3.1145099814122297
Validation loss: 2.509822448008047

Epoch: 6| Step: 2
Training loss: 1.948514210920072
Validation loss: 2.511704353382228

Epoch: 6| Step: 3
Training loss: 2.1795310781266735
Validation loss: 2.5128100581877373

Epoch: 6| Step: 4
Training loss: 2.1731959655075497
Validation loss: 2.511302970933912

Epoch: 6| Step: 5
Training loss: 2.000883741632201
Validation loss: 2.5179333728431965

Epoch: 6| Step: 6
Training loss: 2.7884378027474916
Validation loss: 2.4998672132195465

Epoch: 6| Step: 7
Training loss: 2.3880263023078108
Validation loss: 2.502155566754367

Epoch: 6| Step: 8
Training loss: 2.2127652111692155
Validation loss: 2.5036716123115705

Epoch: 6| Step: 9
Training loss: 2.352338593121631
Validation loss: 2.490386327956883

Epoch: 6| Step: 10
Training loss: 2.445745362796109
Validation loss: 2.495499175431636

Epoch: 6| Step: 11
Training loss: 1.3383924893424421
Validation loss: 2.492971243239937

Epoch: 6| Step: 12
Training loss: 2.5479914552614193
Validation loss: 2.494597637831496

Epoch: 6| Step: 13
Training loss: 2.7890627564502246
Validation loss: 2.491113628668718

Epoch: 214| Step: 0
Training loss: 2.6667811150627863
Validation loss: 2.49061029127525

Epoch: 6| Step: 1
Training loss: 2.3043217772919653
Validation loss: 2.495940679832564

Epoch: 6| Step: 2
Training loss: 2.446051732655486
Validation loss: 2.496669426302367

Epoch: 6| Step: 3
Training loss: 1.5640172077217709
Validation loss: 2.502540680195494

Epoch: 6| Step: 4
Training loss: 2.2639453844485447
Validation loss: 2.5017904069385257

Epoch: 6| Step: 5
Training loss: 2.434092952661682
Validation loss: 2.4973770209841315

Epoch: 6| Step: 6
Training loss: 3.2160391641860664
Validation loss: 2.5072175267891685

Epoch: 6| Step: 7
Training loss: 2.1880631947989055
Validation loss: 2.50931787516876

Epoch: 6| Step: 8
Training loss: 2.0647128401041788
Validation loss: 2.506724342365792

Epoch: 6| Step: 9
Training loss: 2.4327269551960815
Validation loss: 2.5082969950159195

Epoch: 6| Step: 10
Training loss: 1.9556498572823127
Validation loss: 2.5105949486284107

Epoch: 6| Step: 11
Training loss: 2.1055288783955373
Validation loss: 2.5031049202928393

Epoch: 6| Step: 12
Training loss: 2.6507798360994514
Validation loss: 2.5171266421746368

Epoch: 6| Step: 13
Training loss: 2.5078157323893695
Validation loss: 2.5065445432775553

Epoch: 215| Step: 0
Training loss: 2.0194885131327913
Validation loss: 2.5236328647758084

Epoch: 6| Step: 1
Training loss: 2.110287391290658
Validation loss: 2.5105366236305926

Epoch: 6| Step: 2
Training loss: 2.2487775872803524
Validation loss: 2.5169639262374077

Epoch: 6| Step: 3
Training loss: 2.887431261970151
Validation loss: 2.517625664734917

Epoch: 6| Step: 4
Training loss: 2.9906460847978322
Validation loss: 2.517042041303686

Epoch: 6| Step: 5
Training loss: 1.8070345722304835
Validation loss: 2.510770706812047

Epoch: 6| Step: 6
Training loss: 2.465570841340689
Validation loss: 2.5142462923032007

Epoch: 6| Step: 7
Training loss: 1.8333467425231684
Validation loss: 2.531697394624244

Epoch: 6| Step: 8
Training loss: 2.433647926670485
Validation loss: 2.5429260281569617

Epoch: 6| Step: 9
Training loss: 2.4537729756022455
Validation loss: 2.527789635217078

Epoch: 6| Step: 10
Training loss: 2.184630337309578
Validation loss: 2.528028754000368

Epoch: 6| Step: 11
Training loss: 2.496138928979093
Validation loss: 2.512234102479567

Epoch: 6| Step: 12
Training loss: 2.122284219689499
Validation loss: 2.5092549043122063

Epoch: 6| Step: 13
Training loss: 2.739491150517451
Validation loss: 2.5161644809300108

Epoch: 216| Step: 0
Training loss: 1.8911200261506687
Validation loss: 2.5181872033321504

Epoch: 6| Step: 1
Training loss: 2.3487119919085564
Validation loss: 2.514847423988859

Epoch: 6| Step: 2
Training loss: 3.0955081144216363
Validation loss: 2.526911577225288

Epoch: 6| Step: 3
Training loss: 1.7118318214834467
Validation loss: 2.497713895452088

Epoch: 6| Step: 4
Training loss: 2.7258075882308055
Validation loss: 2.514112803870853

Epoch: 6| Step: 5
Training loss: 2.124788161545831
Validation loss: 2.508869966237186

Epoch: 6| Step: 6
Training loss: 2.7711111708621248
Validation loss: 2.5148438214194115

Epoch: 6| Step: 7
Training loss: 2.4134085056371593
Validation loss: 2.51054243244961

Epoch: 6| Step: 8
Training loss: 2.0665001658567173
Validation loss: 2.5198093940917636

Epoch: 6| Step: 9
Training loss: 2.003017414316322
Validation loss: 2.5166544417111734

Epoch: 6| Step: 10
Training loss: 2.2560429485980955
Validation loss: 2.5219820153504946

Epoch: 6| Step: 11
Training loss: 2.234371585443195
Validation loss: 2.5177843449873465

Epoch: 6| Step: 12
Training loss: 2.550301522772487
Validation loss: 2.519593435697982

Epoch: 6| Step: 13
Training loss: 2.305992041740349
Validation loss: 2.5265852248302823

Epoch: 217| Step: 0
Training loss: 2.6984493853953926
Validation loss: 2.529398912411508

Epoch: 6| Step: 1
Training loss: 2.0516160410324775
Validation loss: 2.5482367403158275

Epoch: 6| Step: 2
Training loss: 2.718035110667904
Validation loss: 2.5433777120408925

Epoch: 6| Step: 3
Training loss: 2.047838290148335
Validation loss: 2.5514959217238036

Epoch: 6| Step: 4
Training loss: 1.9015264302377124
Validation loss: 2.53152097028726

Epoch: 6| Step: 5
Training loss: 2.6516007069412835
Validation loss: 2.527400712359391

Epoch: 6| Step: 6
Training loss: 2.631351507045684
Validation loss: 2.5410160785022735

Epoch: 6| Step: 7
Training loss: 2.7600113097249386
Validation loss: 2.513093926877148

Epoch: 6| Step: 8
Training loss: 3.090709078024082
Validation loss: 2.5241316400639384

Epoch: 6| Step: 9
Training loss: 2.8646446822561806
Validation loss: 2.523066498339074

Epoch: 6| Step: 10
Training loss: 0.9352078709580323
Validation loss: 2.5250611332458113

Epoch: 6| Step: 11
Training loss: 1.6181101355490612
Validation loss: 2.5308758965477143

Epoch: 6| Step: 12
Training loss: 2.015955106924146
Validation loss: 2.529481937506579

Epoch: 6| Step: 13
Training loss: 2.141977481577641
Validation loss: 2.52986684928795

Epoch: 218| Step: 0
Training loss: 1.8264888386357943
Validation loss: 2.5279572030782673

Epoch: 6| Step: 1
Training loss: 2.262259252395452
Validation loss: 2.5121943931644775

Epoch: 6| Step: 2
Training loss: 2.071275479903014
Validation loss: 2.525623284263566

Epoch: 6| Step: 3
Training loss: 2.5899949065640713
Validation loss: 2.51548518706507

Epoch: 6| Step: 4
Training loss: 3.031792189096847
Validation loss: 2.518581004188311

Epoch: 6| Step: 5
Training loss: 2.161131047361457
Validation loss: 2.5154244637180225

Epoch: 6| Step: 6
Training loss: 2.164559648058057
Validation loss: 2.5311097000234613

Epoch: 6| Step: 7
Training loss: 1.90262571904461
Validation loss: 2.524931403446375

Epoch: 6| Step: 8
Training loss: 2.02982526844612
Validation loss: 2.521910907374706

Epoch: 6| Step: 9
Training loss: 2.5990431675542256
Validation loss: 2.521760034599621

Epoch: 6| Step: 10
Training loss: 2.7898702012909613
Validation loss: 2.5323109620673496

Epoch: 6| Step: 11
Training loss: 2.5089013893815406
Validation loss: 2.519486206598333

Epoch: 6| Step: 12
Training loss: 2.4081435246451837
Validation loss: 2.520798144344493

Epoch: 6| Step: 13
Training loss: 2.082197286487512
Validation loss: 2.5022727807769716

Epoch: 219| Step: 0
Training loss: 2.6991307731456793
Validation loss: 2.5183204757931086

Epoch: 6| Step: 1
Training loss: 2.201611046022328
Validation loss: 2.5205574883058017

Epoch: 6| Step: 2
Training loss: 2.414403502443932
Validation loss: 2.50706881293598

Epoch: 6| Step: 3
Training loss: 2.7417651772314233
Validation loss: 2.4988420983892303

Epoch: 6| Step: 4
Training loss: 2.167011661450634
Validation loss: 2.495865836804853

Epoch: 6| Step: 5
Training loss: 1.9681826182629492
Validation loss: 2.4917982349510646

Epoch: 6| Step: 6
Training loss: 2.0792990214917064
Validation loss: 2.483059452835865

Epoch: 6| Step: 7
Training loss: 2.1385896013712546
Validation loss: 2.480742992549739

Epoch: 6| Step: 8
Training loss: 2.636765950098367
Validation loss: 2.4864155688295284

Epoch: 6| Step: 9
Training loss: 1.7872111640666861
Validation loss: 2.487780781654603

Epoch: 6| Step: 10
Training loss: 2.327799319843082
Validation loss: 2.491350374166414

Epoch: 6| Step: 11
Training loss: 2.552127033300655
Validation loss: 2.5045265226934124

Epoch: 6| Step: 12
Training loss: 2.910256034625262
Validation loss: 2.491443949625798

Epoch: 6| Step: 13
Training loss: 2.2422654407525466
Validation loss: 2.517609676205797

Epoch: 220| Step: 0
Training loss: 2.51137074006997
Validation loss: 2.5191768584285525

Epoch: 6| Step: 1
Training loss: 1.5469180014442068
Validation loss: 2.534073765573986

Epoch: 6| Step: 2
Training loss: 2.2589309192647424
Validation loss: 2.525892514154618

Epoch: 6| Step: 3
Training loss: 2.2920471106808793
Validation loss: 2.545852487078932

Epoch: 6| Step: 4
Training loss: 1.9217093171445623
Validation loss: 2.535313550930933

Epoch: 6| Step: 5
Training loss: 1.8531421731342894
Validation loss: 2.5541426984731723

Epoch: 6| Step: 6
Training loss: 2.0279903373036623
Validation loss: 2.5531561069468953

Epoch: 6| Step: 7
Training loss: 2.4774434060293133
Validation loss: 2.544453927008086

Epoch: 6| Step: 8
Training loss: 2.1302591207430117
Validation loss: 2.5450798057879025

Epoch: 6| Step: 9
Training loss: 2.8796236756610636
Validation loss: 2.540584354615103

Epoch: 6| Step: 10
Training loss: 2.617231408505275
Validation loss: 2.5162153874136126

Epoch: 6| Step: 11
Training loss: 2.6284182862280367
Validation loss: 2.5257931668989304

Epoch: 6| Step: 12
Training loss: 2.4019330425728813
Validation loss: 2.515029378634713

Epoch: 6| Step: 13
Training loss: 2.8239615126680673
Validation loss: 2.523656483253239

Epoch: 221| Step: 0
Training loss: 2.916736674603913
Validation loss: 2.5217490516545595

Epoch: 6| Step: 1
Training loss: 1.7197056281111538
Validation loss: 2.508862363804424

Epoch: 6| Step: 2
Training loss: 2.106482550124323
Validation loss: 2.5122209346523676

Epoch: 6| Step: 3
Training loss: 2.636090419926139
Validation loss: 2.5021360649589295

Epoch: 6| Step: 4
Training loss: 2.420658125804618
Validation loss: 2.495382288158138

Epoch: 6| Step: 5
Training loss: 2.482301434492219
Validation loss: 2.5015682546822227

Epoch: 6| Step: 6
Training loss: 2.2404430661364456
Validation loss: 2.491585287043103

Epoch: 6| Step: 7
Training loss: 2.013003278082478
Validation loss: 2.49404329504832

Epoch: 6| Step: 8
Training loss: 3.1713708937369796
Validation loss: 2.4936171509835305

Epoch: 6| Step: 9
Training loss: 2.413820124930609
Validation loss: 2.499712164521456

Epoch: 6| Step: 10
Training loss: 1.6478340526888033
Validation loss: 2.503312348439948

Epoch: 6| Step: 11
Training loss: 1.9432095110439445
Validation loss: 2.516722177268022

Epoch: 6| Step: 12
Training loss: 2.0719004173503377
Validation loss: 2.513953899774688

Epoch: 6| Step: 13
Training loss: 2.8752592218714024
Validation loss: 2.5281994809062676

Epoch: 222| Step: 0
Training loss: 2.577847737517619
Validation loss: 2.5193154018314443

Epoch: 6| Step: 1
Training loss: 2.2706342370716626
Validation loss: 2.523433933314587

Epoch: 6| Step: 2
Training loss: 2.136554160744167
Validation loss: 2.521073721360957

Epoch: 6| Step: 3
Training loss: 1.63919608241833
Validation loss: 2.5180835122895955

Epoch: 6| Step: 4
Training loss: 2.4736289571377377
Validation loss: 2.5062392262316813

Epoch: 6| Step: 5
Training loss: 2.301695663008006
Validation loss: 2.5215657696537472

Epoch: 6| Step: 6
Training loss: 2.645972350866326
Validation loss: 2.5204797659684015

Epoch: 6| Step: 7
Training loss: 2.5706276743861265
Validation loss: 2.5122616953783026

Epoch: 6| Step: 8
Training loss: 2.328085879822857
Validation loss: 2.5188041327806627

Epoch: 6| Step: 9
Training loss: 2.7117554587668584
Validation loss: 2.534376317348052

Epoch: 6| Step: 10
Training loss: 1.9368479462094694
Validation loss: 2.5384413834900252

Epoch: 6| Step: 11
Training loss: 2.576681206757981
Validation loss: 2.5358516481933036

Epoch: 6| Step: 12
Training loss: 1.7769660296823746
Validation loss: 2.547977513116753

Epoch: 6| Step: 13
Training loss: 2.50110134661001
Validation loss: 2.531900942269393

Epoch: 223| Step: 0
Training loss: 2.0534822243217663
Validation loss: 2.535229784227723

Epoch: 6| Step: 1
Training loss: 2.508506322410604
Validation loss: 2.5191035260022967

Epoch: 6| Step: 2
Training loss: 2.622805813685739
Validation loss: 2.5148704297492395

Epoch: 6| Step: 3
Training loss: 1.8941268017265045
Validation loss: 2.4976289711262885

Epoch: 6| Step: 4
Training loss: 2.3460460351848473
Validation loss: 2.496032809616518

Epoch: 6| Step: 5
Training loss: 2.8434332367256854
Validation loss: 2.498634585395804

Epoch: 6| Step: 6
Training loss: 1.8274572081976581
Validation loss: 2.498999268192684

Epoch: 6| Step: 7
Training loss: 2.284709971613121
Validation loss: 2.5108311945360327

Epoch: 6| Step: 8
Training loss: 2.4824665344687085
Validation loss: 2.4967234916060277

Epoch: 6| Step: 9
Training loss: 2.5979500281774994
Validation loss: 2.500684294826715

Epoch: 6| Step: 10
Training loss: 1.9242327114625581
Validation loss: 2.5044854774158125

Epoch: 6| Step: 11
Training loss: 2.319551901951838
Validation loss: 2.5120953583557397

Epoch: 6| Step: 12
Training loss: 1.9696477629332805
Validation loss: 2.5014399673026175

Epoch: 6| Step: 13
Training loss: 2.9987148075177426
Validation loss: 2.4980884555500675

Epoch: 224| Step: 0
Training loss: 1.973222405799773
Validation loss: 2.4917664525986094

Epoch: 6| Step: 1
Training loss: 2.284310991509829
Validation loss: 2.5005336350887513

Epoch: 6| Step: 2
Training loss: 2.499647306359232
Validation loss: 2.4997188568978066

Epoch: 6| Step: 3
Training loss: 2.331539747625393
Validation loss: 2.507610911093493

Epoch: 6| Step: 4
Training loss: 2.7046426029985615
Validation loss: 2.520483218593689

Epoch: 6| Step: 5
Training loss: 2.2574113007585117
Validation loss: 2.5238500834536475

Epoch: 6| Step: 6
Training loss: 2.744038796487028
Validation loss: 2.5208621745916107

Epoch: 6| Step: 7
Training loss: 2.2723302078184537
Validation loss: 2.5336365300826027

Epoch: 6| Step: 8
Training loss: 2.4120494757871405
Validation loss: 2.521928302464728

Epoch: 6| Step: 9
Training loss: 2.3046488095931412
Validation loss: 2.5199155056124454

Epoch: 6| Step: 10
Training loss: 2.3901454469484404
Validation loss: 2.5005408337828854

Epoch: 6| Step: 11
Training loss: 2.3600095186203203
Validation loss: 2.5432481353466945

Epoch: 6| Step: 12
Training loss: 2.0621508678518716
Validation loss: 2.514089127346827

Epoch: 6| Step: 13
Training loss: 2.192218922221957
Validation loss: 2.5263993209959956

Epoch: 225| Step: 0
Training loss: 2.0435043420635823
Validation loss: 2.5200147534498645

Epoch: 6| Step: 1
Training loss: 2.2667698565537187
Validation loss: 2.5216670325968753

Epoch: 6| Step: 2
Training loss: 2.4793219851611683
Validation loss: 2.5249548130587374

Epoch: 6| Step: 3
Training loss: 1.9755966782101133
Validation loss: 2.5357578623716

Epoch: 6| Step: 4
Training loss: 2.155699728812151
Validation loss: 2.522888753806407

Epoch: 6| Step: 5
Training loss: 2.5417585408433174
Validation loss: 2.522684116707761

Epoch: 6| Step: 6
Training loss: 2.38166846655886
Validation loss: 2.5065918483965106

Epoch: 6| Step: 7
Training loss: 2.698781487341733
Validation loss: 2.505081543651671

Epoch: 6| Step: 8
Training loss: 2.951424406747962
Validation loss: 2.4936750829369614

Epoch: 6| Step: 9
Training loss: 2.3860070998477103
Validation loss: 2.495758113672248

Epoch: 6| Step: 10
Training loss: 1.84318689055483
Validation loss: 2.4926028329086662

Epoch: 6| Step: 11
Training loss: 2.289555519312249
Validation loss: 2.498274883475524

Epoch: 6| Step: 12
Training loss: 2.04223176051585
Validation loss: 2.490952380021895

Epoch: 6| Step: 13
Training loss: 2.926835525901432
Validation loss: 2.4851248227398517

Epoch: 226| Step: 0
Training loss: 2.176894938760678
Validation loss: 2.488084387065687

Epoch: 6| Step: 1
Training loss: 2.6444877046186375
Validation loss: 2.4925035218520364

Epoch: 6| Step: 2
Training loss: 2.372998247315555
Validation loss: 2.5029415626953626

Epoch: 6| Step: 3
Training loss: 2.3657854920221943
Validation loss: 2.499122767718478

Epoch: 6| Step: 4
Training loss: 1.9999474280123546
Validation loss: 2.511823115101262

Epoch: 6| Step: 5
Training loss: 2.5428802427954054
Validation loss: 2.5010540965220356

Epoch: 6| Step: 6
Training loss: 2.2419261181897556
Validation loss: 2.51421509400754

Epoch: 6| Step: 7
Training loss: 2.5734208162620433
Validation loss: 2.5149459238223737

Epoch: 6| Step: 8
Training loss: 1.7419830339133313
Validation loss: 2.51812859656231

Epoch: 6| Step: 9
Training loss: 2.2918045233674786
Validation loss: 2.5158382270119293

Epoch: 6| Step: 10
Training loss: 2.4447455678287784
Validation loss: 2.5341727098039373

Epoch: 6| Step: 11
Training loss: 2.3229757623610907
Validation loss: 2.5324329861419685

Epoch: 6| Step: 12
Training loss: 2.6161716596354
Validation loss: 2.542664540460808

Epoch: 6| Step: 13
Training loss: 2.055638430474105
Validation loss: 2.5427949516881956

Epoch: 227| Step: 0
Training loss: 2.585162827548815
Validation loss: 2.5364228754613563

Epoch: 6| Step: 1
Training loss: 2.659549952130243
Validation loss: 2.532463897241881

Epoch: 6| Step: 2
Training loss: 2.190470830740996
Validation loss: 2.5231503223820715

Epoch: 6| Step: 3
Training loss: 1.651174144865145
Validation loss: 2.5266873560220273

Epoch: 6| Step: 4
Training loss: 2.4516886983158583
Validation loss: 2.52938965930403

Epoch: 6| Step: 5
Training loss: 2.3696803699110967
Validation loss: 2.5215669042750766

Epoch: 6| Step: 6
Training loss: 2.9883867232713888
Validation loss: 2.532229622494415

Epoch: 6| Step: 7
Training loss: 1.611572838898597
Validation loss: 2.522672035165536

Epoch: 6| Step: 8
Training loss: 3.085118129453176
Validation loss: 2.537461716186164

Epoch: 6| Step: 9
Training loss: 1.930082767174486
Validation loss: 2.5526026502312997

Epoch: 6| Step: 10
Training loss: 1.9776902430638033
Validation loss: 2.531775886945475

Epoch: 6| Step: 11
Training loss: 2.0747294973520325
Validation loss: 2.544500948997156

Epoch: 6| Step: 12
Training loss: 2.484019703920509
Validation loss: 2.547609851191992

Epoch: 6| Step: 13
Training loss: 2.289228934529061
Validation loss: 2.541722255369887

Epoch: 228| Step: 0
Training loss: 2.2893791109965136
Validation loss: 2.545905726434194

Epoch: 6| Step: 1
Training loss: 1.6038157917421667
Validation loss: 2.5354617130245125

Epoch: 6| Step: 2
Training loss: 2.3670843624396345
Validation loss: 2.5274648583318298

Epoch: 6| Step: 3
Training loss: 2.519628809733027
Validation loss: 2.5053008466255022

Epoch: 6| Step: 4
Training loss: 2.0506019731907053
Validation loss: 2.499426807815128

Epoch: 6| Step: 5
Training loss: 1.9589900951359034
Validation loss: 2.511640390268373

Epoch: 6| Step: 6
Training loss: 2.3102018948465073
Validation loss: 2.512740699031003

Epoch: 6| Step: 7
Training loss: 1.9548624474216167
Validation loss: 2.49194789201009

Epoch: 6| Step: 8
Training loss: 2.4683464782831903
Validation loss: 2.4996046707388047

Epoch: 6| Step: 9
Training loss: 2.572565546414821
Validation loss: 2.505904393771654

Epoch: 6| Step: 10
Training loss: 2.818212091585365
Validation loss: 2.5031895953418837

Epoch: 6| Step: 11
Training loss: 2.549646566244516
Validation loss: 2.5110551383617334

Epoch: 6| Step: 12
Training loss: 2.615452433263519
Validation loss: 2.5050702813721317

Epoch: 6| Step: 13
Training loss: 2.5096426967098178
Validation loss: 2.515514134398611

Epoch: 229| Step: 0
Training loss: 2.057412317013706
Validation loss: 2.521523977412384

Epoch: 6| Step: 1
Training loss: 2.032676665263463
Validation loss: 2.526523101128282

Epoch: 6| Step: 2
Training loss: 2.359216747913437
Validation loss: 2.524461079989089

Epoch: 6| Step: 3
Training loss: 2.539827108431331
Validation loss: 2.5265646534039456

Epoch: 6| Step: 4
Training loss: 1.96236073188107
Validation loss: 2.5208681487734674

Epoch: 6| Step: 5
Training loss: 1.7044901844658507
Validation loss: 2.5296469584831778

Epoch: 6| Step: 6
Training loss: 2.2582006417831444
Validation loss: 2.5250985944654

Epoch: 6| Step: 7
Training loss: 2.294376597454582
Validation loss: 2.5334534963753144

Epoch: 6| Step: 8
Training loss: 2.321935837206638
Validation loss: 2.5382829144238443

Epoch: 6| Step: 9
Training loss: 2.227587440564093
Validation loss: 2.5520589762453088

Epoch: 6| Step: 10
Training loss: 3.096564351811877
Validation loss: 2.5671940740561907

Epoch: 6| Step: 11
Training loss: 2.7995086715681126
Validation loss: 2.5750129026565

Epoch: 6| Step: 12
Training loss: 2.051652298274524
Validation loss: 2.567940921321248

Epoch: 6| Step: 13
Training loss: 2.390790447419108
Validation loss: 2.563692172232316

Epoch: 230| Step: 0
Training loss: 2.194112103347982
Validation loss: 2.570659888386529

Epoch: 6| Step: 1
Training loss: 2.4438009739703475
Validation loss: 2.5524048868654536

Epoch: 6| Step: 2
Training loss: 2.302830973144847
Validation loss: 2.562103737066267

Epoch: 6| Step: 3
Training loss: 2.245920084998182
Validation loss: 2.5654048117390107

Epoch: 6| Step: 4
Training loss: 2.203936163730712
Validation loss: 2.5590069075661743

Epoch: 6| Step: 5
Training loss: 1.9171727659472284
Validation loss: 2.5663430928549564

Epoch: 6| Step: 6
Training loss: 2.879023514614821
Validation loss: 2.539296645103342

Epoch: 6| Step: 7
Training loss: 2.891740738270893
Validation loss: 2.5240903468499045

Epoch: 6| Step: 8
Training loss: 2.6280762948003438
Validation loss: 2.517328834472929

Epoch: 6| Step: 9
Training loss: 2.516814430479912
Validation loss: 2.528903910035096

Epoch: 6| Step: 10
Training loss: 2.279658977704004
Validation loss: 2.532869629291131

Epoch: 6| Step: 11
Training loss: 2.3829028503298466
Validation loss: 2.5262762898029285

Epoch: 6| Step: 12
Training loss: 1.7875962558252163
Validation loss: 2.522101931660678

Epoch: 6| Step: 13
Training loss: 1.46054292133997
Validation loss: 2.5252994089157443

Epoch: 231| Step: 0
Training loss: 2.7406773769893475
Validation loss: 2.5223036542379527

Epoch: 6| Step: 1
Training loss: 1.8902609608643581
Validation loss: 2.5125025766590374

Epoch: 6| Step: 2
Training loss: 1.942250120176954
Validation loss: 2.5136724361882425

Epoch: 6| Step: 3
Training loss: 2.3115886232623226
Validation loss: 2.527901023496515

Epoch: 6| Step: 4
Training loss: 2.455041222002881
Validation loss: 2.5114068944645362

Epoch: 6| Step: 5
Training loss: 2.182045157142734
Validation loss: 2.5085403717074

Epoch: 6| Step: 6
Training loss: 2.3162057413483117
Validation loss: 2.5075349426370384

Epoch: 6| Step: 7
Training loss: 2.0596171953012092
Validation loss: 2.515773942538468

Epoch: 6| Step: 8
Training loss: 2.2756196655780867
Validation loss: 2.520275847737458

Epoch: 6| Step: 9
Training loss: 2.1172139634171194
Validation loss: 2.5248832851918412

Epoch: 6| Step: 10
Training loss: 2.6835935723139497
Validation loss: 2.5226082713870097

Epoch: 6| Step: 11
Training loss: 1.936157992339796
Validation loss: 2.5277348038463714

Epoch: 6| Step: 12
Training loss: 2.5343805428290285
Validation loss: 2.530420614870022

Epoch: 6| Step: 13
Training loss: 2.995886048201125
Validation loss: 2.515707705700007

Epoch: 232| Step: 0
Training loss: 1.7370955977720255
Validation loss: 2.525118776555611

Epoch: 6| Step: 1
Training loss: 2.3918699127214933
Validation loss: 2.51829714652099

Epoch: 6| Step: 2
Training loss: 1.8498399665278928
Validation loss: 2.526577612792964

Epoch: 6| Step: 3
Training loss: 2.64329763740339
Validation loss: 2.527879063762688

Epoch: 6| Step: 4
Training loss: 2.3919293205127556
Validation loss: 2.5235542057792064

Epoch: 6| Step: 5
Training loss: 2.847308604679725
Validation loss: 2.520520913386744

Epoch: 6| Step: 6
Training loss: 2.865000376543217
Validation loss: 2.521451627290653

Epoch: 6| Step: 7
Training loss: 2.467674310744076
Validation loss: 2.509551478253473

Epoch: 6| Step: 8
Training loss: 2.4385997540968365
Validation loss: 2.522918128074351

Epoch: 6| Step: 9
Training loss: 2.2885316617316107
Validation loss: 2.537804175375256

Epoch: 6| Step: 10
Training loss: 2.013735099562129
Validation loss: 2.54297656298803

Epoch: 6| Step: 11
Training loss: 2.393637261586929
Validation loss: 2.5498841028740906

Epoch: 6| Step: 12
Training loss: 1.7978735761315234
Validation loss: 2.52713604584071

Epoch: 6| Step: 13
Training loss: 2.4546119411210685
Validation loss: 2.5326850653533444

Epoch: 233| Step: 0
Training loss: 2.28471790250185
Validation loss: 2.5216351697189694

Epoch: 6| Step: 1
Training loss: 2.2532746539103017
Validation loss: 2.5183684038698884

Epoch: 6| Step: 2
Training loss: 2.413431918552558
Validation loss: 2.5050786408436188

Epoch: 6| Step: 3
Training loss: 1.7311159047550226
Validation loss: 2.499232094766744

Epoch: 6| Step: 4
Training loss: 2.18052465844858
Validation loss: 2.4996640694940426

Epoch: 6| Step: 5
Training loss: 2.568767048206049
Validation loss: 2.4857842788775346

Epoch: 6| Step: 6
Training loss: 1.8007314017258373
Validation loss: 2.488813294481284

Epoch: 6| Step: 7
Training loss: 2.910719193664211
Validation loss: 2.4753058260639187

Epoch: 6| Step: 8
Training loss: 2.457698373636677
Validation loss: 2.4875219315371955

Epoch: 6| Step: 9
Training loss: 2.1356604708303895
Validation loss: 2.4866376126341723

Epoch: 6| Step: 10
Training loss: 2.39537633324339
Validation loss: 2.491064410534264

Epoch: 6| Step: 11
Training loss: 2.1652042393012683
Validation loss: 2.4989036063249044

Epoch: 6| Step: 12
Training loss: 2.1737978539839786
Validation loss: 2.5050352569454253

Epoch: 6| Step: 13
Training loss: 2.7739615791426915
Validation loss: 2.501376281834539

Epoch: 234| Step: 0
Training loss: 3.0766855349914435
Validation loss: 2.5266558631281852

Epoch: 6| Step: 1
Training loss: 2.8269576724823517
Validation loss: 2.5371760476318226

Epoch: 6| Step: 2
Training loss: 2.347153900275734
Validation loss: 2.530847062115638

Epoch: 6| Step: 3
Training loss: 1.563132958954351
Validation loss: 2.5234696944223263

Epoch: 6| Step: 4
Training loss: 2.1447182384468735
Validation loss: 2.525449897166915

Epoch: 6| Step: 5
Training loss: 2.3311305774853346
Validation loss: 2.5312015073550316

Epoch: 6| Step: 6
Training loss: 3.0410466716262
Validation loss: 2.5267570007107465

Epoch: 6| Step: 7
Training loss: 2.017309032416523
Validation loss: 2.538175144171463

Epoch: 6| Step: 8
Training loss: 2.4277214117463637
Validation loss: 2.5484009366479365

Epoch: 6| Step: 9
Training loss: 2.1780238211513963
Validation loss: 2.544923530084282

Epoch: 6| Step: 10
Training loss: 2.0979762907624964
Validation loss: 2.5586220620799582

Epoch: 6| Step: 11
Training loss: 1.8642935572095407
Validation loss: 2.56486258103345

Epoch: 6| Step: 12
Training loss: 1.9378466142279316
Validation loss: 2.548649737082427

Epoch: 6| Step: 13
Training loss: 2.305059060355833
Validation loss: 2.553444702974888

Epoch: 235| Step: 0
Training loss: 2.691198155479162
Validation loss: 2.5494275242085664

Epoch: 6| Step: 1
Training loss: 2.1653014674864277
Validation loss: 2.5536395615473384

Epoch: 6| Step: 2
Training loss: 3.110960283104984
Validation loss: 2.5268732701164422

Epoch: 6| Step: 3
Training loss: 2.331162487361288
Validation loss: 2.5258533892647574

Epoch: 6| Step: 4
Training loss: 2.311291559319199
Validation loss: 2.523434476584876

Epoch: 6| Step: 5
Training loss: 2.100597060567366
Validation loss: 2.503811759574271

Epoch: 6| Step: 6
Training loss: 2.5417309633054086
Validation loss: 2.508344108668235

Epoch: 6| Step: 7
Training loss: 1.9873745092924853
Validation loss: 2.5178307445241197

Epoch: 6| Step: 8
Training loss: 1.8450278040747348
Validation loss: 2.521354547847432

Epoch: 6| Step: 9
Training loss: 1.6792584248249527
Validation loss: 2.5372697183883517

Epoch: 6| Step: 10
Training loss: 2.0522814685678004
Validation loss: 2.549480049878683

Epoch: 6| Step: 11
Training loss: 1.749183327987918
Validation loss: 2.558016755173216

Epoch: 6| Step: 12
Training loss: 2.644779345821866
Validation loss: 2.583355078041688

Epoch: 6| Step: 13
Training loss: 2.7715656679905027
Validation loss: 2.5580381299741908

Epoch: 236| Step: 0
Training loss: 2.1119965263684923
Validation loss: 2.5520531529309345

Epoch: 6| Step: 1
Training loss: 2.594957645048556
Validation loss: 2.54624998384481

Epoch: 6| Step: 2
Training loss: 2.7506102838449777
Validation loss: 2.531233689369429

Epoch: 6| Step: 3
Training loss: 2.2294844879137745
Validation loss: 2.5225102282828074

Epoch: 6| Step: 4
Training loss: 1.310850378331077
Validation loss: 2.5122431736092654

Epoch: 6| Step: 5
Training loss: 2.3679256484370192
Validation loss: 2.5091586515749857

Epoch: 6| Step: 6
Training loss: 2.360939164227367
Validation loss: 2.5063559956067833

Epoch: 6| Step: 7
Training loss: 2.183009306569342
Validation loss: 2.5156451080340005

Epoch: 6| Step: 8
Training loss: 1.966842817510521
Validation loss: 2.5130219665419373

Epoch: 6| Step: 9
Training loss: 1.89996891498234
Validation loss: 2.517451395337749

Epoch: 6| Step: 10
Training loss: 2.598661254130808
Validation loss: 2.521144317066237

Epoch: 6| Step: 11
Training loss: 2.9302626388585766
Validation loss: 2.531428554500929

Epoch: 6| Step: 12
Training loss: 2.031198823724311
Validation loss: 2.520679852776121

Epoch: 6| Step: 13
Training loss: 2.4423069627553957
Validation loss: 2.5230426459723145

Epoch: 237| Step: 0
Training loss: 2.6174706149341858
Validation loss: 2.531046870993964

Epoch: 6| Step: 1
Training loss: 2.7163678837706184
Validation loss: 2.5161299030987005

Epoch: 6| Step: 2
Training loss: 2.6121485240998044
Validation loss: 2.523123895863652

Epoch: 6| Step: 3
Training loss: 1.719254714380246
Validation loss: 2.5141769095483926

Epoch: 6| Step: 4
Training loss: 2.6627048825517523
Validation loss: 2.5174094558692186

Epoch: 6| Step: 5
Training loss: 2.6204924256088473
Validation loss: 2.505683272316838

Epoch: 6| Step: 6
Training loss: 1.6998270283383339
Validation loss: 2.516190348848429

Epoch: 6| Step: 7
Training loss: 1.6588891074085426
Validation loss: 2.5083763779994923

Epoch: 6| Step: 8
Training loss: 2.7484677988087958
Validation loss: 2.5188815282110113

Epoch: 6| Step: 9
Training loss: 2.2180643231048696
Validation loss: 2.523485905642623

Epoch: 6| Step: 10
Training loss: 3.040894728298925
Validation loss: 2.530632100609179

Epoch: 6| Step: 11
Training loss: 1.679285329533686
Validation loss: 2.5379408642868864

Epoch: 6| Step: 12
Training loss: 1.7401957395227203
Validation loss: 2.520290510724564

Epoch: 6| Step: 13
Training loss: 1.9366776967131225
Validation loss: 2.5458803945667063

Epoch: 238| Step: 0
Training loss: 2.5633891354139164
Validation loss: 2.55216961669386

Epoch: 6| Step: 1
Training loss: 2.187133322365101
Validation loss: 2.548102147568531

Epoch: 6| Step: 2
Training loss: 2.1978349956748096
Validation loss: 2.5373127077383235

Epoch: 6| Step: 3
Training loss: 1.9841569194957493
Validation loss: 2.539376639471167

Epoch: 6| Step: 4
Training loss: 2.50999265601311
Validation loss: 2.5237191656205673

Epoch: 6| Step: 5
Training loss: 2.284413900187662
Validation loss: 2.527085139207648

Epoch: 6| Step: 6
Training loss: 1.9612073978540518
Validation loss: 2.5260945632482428

Epoch: 6| Step: 7
Training loss: 3.1758301978661754
Validation loss: 2.510251927543513

Epoch: 6| Step: 8
Training loss: 1.5876309228211003
Validation loss: 2.519141178344975

Epoch: 6| Step: 9
Training loss: 2.6447550060550453
Validation loss: 2.524455114309988

Epoch: 6| Step: 10
Training loss: 2.356134576778326
Validation loss: 2.521278914697306

Epoch: 6| Step: 11
Training loss: 2.4961276581898257
Validation loss: 2.517538397381587

Epoch: 6| Step: 12
Training loss: 2.1167358817650954
Validation loss: 2.535266319358049

Epoch: 6| Step: 13
Training loss: 1.9493767133474693
Validation loss: 2.527594418798065

Epoch: 239| Step: 0
Training loss: 2.5052060281236685
Validation loss: 2.518542759701786

Epoch: 6| Step: 1
Training loss: 1.7887514335365335
Validation loss: 2.522646375479087

Epoch: 6| Step: 2
Training loss: 2.914100789900083
Validation loss: 2.524967922346859

Epoch: 6| Step: 3
Training loss: 1.8706430676853019
Validation loss: 2.527726126295667

Epoch: 6| Step: 4
Training loss: 2.112773728553052
Validation loss: 2.5225540520050442

Epoch: 6| Step: 5
Training loss: 1.9615227179377352
Validation loss: 2.5118205285697557

Epoch: 6| Step: 6
Training loss: 2.8779918196656675
Validation loss: 2.507924680488417

Epoch: 6| Step: 7
Training loss: 2.233528777248596
Validation loss: 2.5054183100036207

Epoch: 6| Step: 8
Training loss: 2.668154261829138
Validation loss: 2.5082627602298926

Epoch: 6| Step: 9
Training loss: 2.1497327194859066
Validation loss: 2.509156331518555

Epoch: 6| Step: 10
Training loss: 2.337108509918568
Validation loss: 2.5115477887907365

Epoch: 6| Step: 11
Training loss: 2.904025467259154
Validation loss: 2.5242453227901986

Epoch: 6| Step: 12
Training loss: 1.7979628214063337
Validation loss: 2.5183411618996137

Epoch: 6| Step: 13
Training loss: 1.5789185596648736
Validation loss: 2.5288359665161066

Epoch: 240| Step: 0
Training loss: 2.273468122243293
Validation loss: 2.5309264050769733

Epoch: 6| Step: 1
Training loss: 2.4098382922485895
Validation loss: 2.5221439191405444

Epoch: 6| Step: 2
Training loss: 2.3368759056511936
Validation loss: 2.513561120862382

Epoch: 6| Step: 3
Training loss: 2.574836344286685
Validation loss: 2.5193122551739435

Epoch: 6| Step: 4
Training loss: 2.9963607967486143
Validation loss: 2.5066336042509856

Epoch: 6| Step: 5
Training loss: 2.1101959891392426
Validation loss: 2.5263856253819297

Epoch: 6| Step: 6
Training loss: 1.843434646275729
Validation loss: 2.533835954329515

Epoch: 6| Step: 7
Training loss: 2.40671237615078
Validation loss: 2.5235587643046484

Epoch: 6| Step: 8
Training loss: 1.6937780511651717
Validation loss: 2.5434627023927705

Epoch: 6| Step: 9
Training loss: 2.988052100078445
Validation loss: 2.535477471438787

Epoch: 6| Step: 10
Training loss: 1.8970172106053134
Validation loss: 2.53536865723877

Epoch: 6| Step: 11
Training loss: 1.7052600322227511
Validation loss: 2.547652751846509

Epoch: 6| Step: 12
Training loss: 2.426217892182287
Validation loss: 2.5417061526457174

Epoch: 6| Step: 13
Training loss: 2.0241573040262635
Validation loss: 2.5368427609323896

Epoch: 241| Step: 0
Training loss: 1.517959998356382
Validation loss: 2.548488394554337

Epoch: 6| Step: 1
Training loss: 2.4693613381902177
Validation loss: 2.5449897794137315

Epoch: 6| Step: 2
Training loss: 2.312298018423897
Validation loss: 2.537334726773573

Epoch: 6| Step: 3
Training loss: 2.2260784493029826
Validation loss: 2.5418127961180663

Epoch: 6| Step: 4
Training loss: 2.8686867102329745
Validation loss: 2.547572409037822

Epoch: 6| Step: 5
Training loss: 1.7218808182689032
Validation loss: 2.5494556419422403

Epoch: 6| Step: 6
Training loss: 2.6790203009018136
Validation loss: 2.533689328223015

Epoch: 6| Step: 7
Training loss: 2.0922224251280017
Validation loss: 2.5206003370401895

Epoch: 6| Step: 8
Training loss: 2.3969696225185784
Validation loss: 2.518412875575654

Epoch: 6| Step: 9
Training loss: 1.752347325573495
Validation loss: 2.508241040463502

Epoch: 6| Step: 10
Training loss: 2.56155163963578
Validation loss: 2.5102685486295298

Epoch: 6| Step: 11
Training loss: 2.3774718421528136
Validation loss: 2.5039173429484434

Epoch: 6| Step: 12
Training loss: 1.6121181331587953
Validation loss: 2.5108097502061653

Epoch: 6| Step: 13
Training loss: 2.8287386570937794
Validation loss: 2.5159680782177176

Epoch: 242| Step: 0
Training loss: 2.124412455395526
Validation loss: 2.5293714986075777

Epoch: 6| Step: 1
Training loss: 1.9039606198517898
Validation loss: 2.531233650123319

Epoch: 6| Step: 2
Training loss: 2.1900036244819643
Validation loss: 2.5337144761558514

Epoch: 6| Step: 3
Training loss: 2.20722612178951
Validation loss: 2.5479364971928993

Epoch: 6| Step: 4
Training loss: 2.1768794960591062
Validation loss: 2.531441151500973

Epoch: 6| Step: 5
Training loss: 2.365699224680786
Validation loss: 2.539602447220867

Epoch: 6| Step: 6
Training loss: 2.824440848453866
Validation loss: 2.5345248164350895

Epoch: 6| Step: 7
Training loss: 2.7997389467342915
Validation loss: 2.553997121497125

Epoch: 6| Step: 8
Training loss: 2.511565920153571
Validation loss: 2.583007643278389

Epoch: 6| Step: 9
Training loss: 2.1605970267553194
Validation loss: 2.571127656960155

Epoch: 6| Step: 10
Training loss: 2.343698526452997
Validation loss: 2.575869604447934

Epoch: 6| Step: 11
Training loss: 1.989866812384757
Validation loss: 2.5638524030038927

Epoch: 6| Step: 12
Training loss: 2.091475090686513
Validation loss: 2.5729298816501123

Epoch: 6| Step: 13
Training loss: 2.369761461896852
Validation loss: 2.5605786804349955

Epoch: 243| Step: 0
Training loss: 2.8984315774450944
Validation loss: 2.551541412322439

Epoch: 6| Step: 1
Training loss: 2.595510597638001
Validation loss: 2.5512831439030776

Epoch: 6| Step: 2
Training loss: 2.1525838894507796
Validation loss: 2.53015612290153

Epoch: 6| Step: 3
Training loss: 2.1544846758081704
Validation loss: 2.513083159047736

Epoch: 6| Step: 4
Training loss: 1.5720304135493905
Validation loss: 2.5237022079973452

Epoch: 6| Step: 5
Training loss: 2.2678008056440033
Validation loss: 2.522471405373835

Epoch: 6| Step: 6
Training loss: 1.626957301469917
Validation loss: 2.503352016103273

Epoch: 6| Step: 7
Training loss: 2.2650965633397755
Validation loss: 2.522799392502966

Epoch: 6| Step: 8
Training loss: 1.8787240239401537
Validation loss: 2.52403171971577

Epoch: 6| Step: 9
Training loss: 2.093305056536887
Validation loss: 2.5188461200178627

Epoch: 6| Step: 10
Training loss: 3.217253994729288
Validation loss: 2.530071345191562

Epoch: 6| Step: 11
Training loss: 2.385156397939778
Validation loss: 2.5293028528248414

Epoch: 6| Step: 12
Training loss: 1.8878266613963803
Validation loss: 2.539610051511852

Epoch: 6| Step: 13
Training loss: 2.3847901189172602
Validation loss: 2.5448608937621633

Epoch: 244| Step: 0
Training loss: 2.2772270159632013
Validation loss: 2.5549236503436035

Epoch: 6| Step: 1
Training loss: 2.970697988572154
Validation loss: 2.5510030263516192

Epoch: 6| Step: 2
Training loss: 2.1456809375559582
Validation loss: 2.573198285051048

Epoch: 6| Step: 3
Training loss: 1.7478788327050474
Validation loss: 2.597020779468405

Epoch: 6| Step: 4
Training loss: 2.451912549947059
Validation loss: 2.6070500293238603

Epoch: 6| Step: 5
Training loss: 2.1843190634882452
Validation loss: 2.6014198210638333

Epoch: 6| Step: 6
Training loss: 1.8327308083724794
Validation loss: 2.597702630185887

Epoch: 6| Step: 7
Training loss: 2.6048542996771213
Validation loss: 2.5948035004196757

Epoch: 6| Step: 8
Training loss: 2.1989724273687754
Validation loss: 2.5980200950103827

Epoch: 6| Step: 9
Training loss: 1.7221701145834731
Validation loss: 2.5782723895241006

Epoch: 6| Step: 10
Training loss: 2.2223498731613422
Validation loss: 2.5880012075921552

Epoch: 6| Step: 11
Training loss: 1.8107658508946862
Validation loss: 2.5601610503599397

Epoch: 6| Step: 12
Training loss: 2.9462131570123598
Validation loss: 2.5493610706714116

Epoch: 6| Step: 13
Training loss: 2.3962331576444473
Validation loss: 2.5455384287079243

Epoch: 245| Step: 0
Training loss: 2.686996057857437
Validation loss: 2.5347724314438693

Epoch: 6| Step: 1
Training loss: 2.841548528667333
Validation loss: 2.5380938437988068

Epoch: 6| Step: 2
Training loss: 1.8765053110675172
Validation loss: 2.5390439076843325

Epoch: 6| Step: 3
Training loss: 2.2141028078188483
Validation loss: 2.5446688218094726

Epoch: 6| Step: 4
Training loss: 1.9652090169187686
Validation loss: 2.566679949127442

Epoch: 6| Step: 5
Training loss: 2.1129827094609546
Validation loss: 2.569014247473161

Epoch: 6| Step: 6
Training loss: 1.6354325733099673
Validation loss: 2.583145309086587

Epoch: 6| Step: 7
Training loss: 2.460134708313863
Validation loss: 2.551480347898745

Epoch: 6| Step: 8
Training loss: 2.402485069522253
Validation loss: 2.567224798784814

Epoch: 6| Step: 9
Training loss: 2.381520205408623
Validation loss: 2.5730388061526646

Epoch: 6| Step: 10
Training loss: 3.1241026543664554
Validation loss: 2.5426182815460243

Epoch: 6| Step: 11
Training loss: 1.5193578626073412
Validation loss: 2.532154110167335

Epoch: 6| Step: 12
Training loss: 1.8349037669047576
Validation loss: 2.5176251123195135

Epoch: 6| Step: 13
Training loss: 2.7974185521982338
Validation loss: 2.4971328029339754

Epoch: 246| Step: 0
Training loss: 1.8905103664482228
Validation loss: 2.525887228318295

Epoch: 6| Step: 1
Training loss: 2.455508877908468
Validation loss: 2.5142654314915696

Epoch: 6| Step: 2
Training loss: 2.220132101785261
Validation loss: 2.5111615406822176

Epoch: 6| Step: 3
Training loss: 1.6424162758865728
Validation loss: 2.509235932774943

Epoch: 6| Step: 4
Training loss: 2.4716427429886596
Validation loss: 2.525473718930761

Epoch: 6| Step: 5
Training loss: 2.225813569048166
Validation loss: 2.51317409120528

Epoch: 6| Step: 6
Training loss: 1.882316555695329
Validation loss: 2.506739401658243

Epoch: 6| Step: 7
Training loss: 2.5647141729757137
Validation loss: 2.5134996870455657

Epoch: 6| Step: 8
Training loss: 2.4041898751639947
Validation loss: 2.4933138049180292

Epoch: 6| Step: 9
Training loss: 2.0502053460341436
Validation loss: 2.5044273433319577

Epoch: 6| Step: 10
Training loss: 2.285977625152476
Validation loss: 2.5009392245622486

Epoch: 6| Step: 11
Training loss: 2.103734818764738
Validation loss: 2.4995056855743143

Epoch: 6| Step: 12
Training loss: 2.8207272042426212
Validation loss: 2.5105617898066996

Epoch: 6| Step: 13
Training loss: 2.912061725433858
Validation loss: 2.5241005954384104

Epoch: 247| Step: 0
Training loss: 2.3353599421213715
Validation loss: 2.5454691183614346

Epoch: 6| Step: 1
Training loss: 2.2920239141191976
Validation loss: 2.5524457920656243

Epoch: 6| Step: 2
Training loss: 2.2147789462934724
Validation loss: 2.5695383902435363

Epoch: 6| Step: 3
Training loss: 2.6583520089413257
Validation loss: 2.6066755543514195

Epoch: 6| Step: 4
Training loss: 2.4767852586751102
Validation loss: 2.6190963236916405

Epoch: 6| Step: 5
Training loss: 2.081944447724445
Validation loss: 2.6074413513141983

Epoch: 6| Step: 6
Training loss: 2.8464909014909057
Validation loss: 2.6096856775567896

Epoch: 6| Step: 7
Training loss: 2.0650658108844113
Validation loss: 2.572016853385079

Epoch: 6| Step: 8
Training loss: 2.2855503883523887
Validation loss: 2.5396318941634384

Epoch: 6| Step: 9
Training loss: 2.0805445761476493
Validation loss: 2.522400326947664

Epoch: 6| Step: 10
Training loss: 2.5194197755741694
Validation loss: 2.4974414130471176

Epoch: 6| Step: 11
Training loss: 1.7317729366739067
Validation loss: 2.5068418658525378

Epoch: 6| Step: 12
Training loss: 2.813598926747247
Validation loss: 2.488828621808279

Epoch: 6| Step: 13
Training loss: 1.9247809050090865
Validation loss: 2.502252088875365

Epoch: 248| Step: 0
Training loss: 2.1815565839271125
Validation loss: 2.502776392403446

Epoch: 6| Step: 1
Training loss: 2.446402602311491
Validation loss: 2.5072631788887185

Epoch: 6| Step: 2
Training loss: 2.3835714163495036
Validation loss: 2.5101920353174663

Epoch: 6| Step: 3
Training loss: 2.323254297007638
Validation loss: 2.5184419075942737

Epoch: 6| Step: 4
Training loss: 1.4884441616975275
Validation loss: 2.506508366307332

Epoch: 6| Step: 5
Training loss: 2.841258204423544
Validation loss: 2.5003324764422983

Epoch: 6| Step: 6
Training loss: 2.631803506003795
Validation loss: 2.5121094126467294

Epoch: 6| Step: 7
Training loss: 1.8560613388621547
Validation loss: 2.5156438996610055

Epoch: 6| Step: 8
Training loss: 2.5148979700221603
Validation loss: 2.5334304319353462

Epoch: 6| Step: 9
Training loss: 1.9830864755355235
Validation loss: 2.5466894070457164

Epoch: 6| Step: 10
Training loss: 2.2590380447716747
Validation loss: 2.5539433195485675

Epoch: 6| Step: 11
Training loss: 2.373642885062301
Validation loss: 2.5436378766717085

Epoch: 6| Step: 12
Training loss: 2.2570967546242025
Validation loss: 2.5316969237571456

Epoch: 6| Step: 13
Training loss: 2.3535360881501473
Validation loss: 2.529478937025536

Epoch: 249| Step: 0
Training loss: 2.3156415889830466
Validation loss: 2.5210216126131515

Epoch: 6| Step: 1
Training loss: 2.4762907149825204
Validation loss: 2.524793451504098

Epoch: 6| Step: 2
Training loss: 1.705920244148164
Validation loss: 2.532569839428369

Epoch: 6| Step: 3
Training loss: 1.7693776449266592
Validation loss: 2.541902512932265

Epoch: 6| Step: 4
Training loss: 2.289694428691791
Validation loss: 2.5344372843196608

Epoch: 6| Step: 5
Training loss: 2.313851554368931
Validation loss: 2.5552232590768296

Epoch: 6| Step: 6
Training loss: 1.8492983080144605
Validation loss: 2.552686601817822

Epoch: 6| Step: 7
Training loss: 2.567035847489739
Validation loss: 2.546616695231667

Epoch: 6| Step: 8
Training loss: 1.8896326029900197
Validation loss: 2.5612600009040944

Epoch: 6| Step: 9
Training loss: 2.459140860476984
Validation loss: 2.56536260293764

Epoch: 6| Step: 10
Training loss: 2.7240531326243165
Validation loss: 2.541164353768318

Epoch: 6| Step: 11
Training loss: 2.379658396912997
Validation loss: 2.549618037607353

Epoch: 6| Step: 12
Training loss: 2.138181196386958
Validation loss: 2.5209559786467555

Epoch: 6| Step: 13
Training loss: 2.6874060503484514
Validation loss: 2.5204302305223507

Epoch: 250| Step: 0
Training loss: 2.6374199733073582
Validation loss: 2.5184682727951215

Epoch: 6| Step: 1
Training loss: 2.4898021607146843
Validation loss: 2.532659460045279

Epoch: 6| Step: 2
Training loss: 1.8937126986918495
Validation loss: 2.528737693451198

Epoch: 6| Step: 3
Training loss: 1.2847075911138408
Validation loss: 2.5376011638801845

Epoch: 6| Step: 4
Training loss: 1.9741916233458916
Validation loss: 2.546911414886576

Epoch: 6| Step: 5
Training loss: 2.1868219687198995
Validation loss: 2.543972616116274

Epoch: 6| Step: 6
Training loss: 2.187156650300033
Validation loss: 2.5472836000911667

Epoch: 6| Step: 7
Training loss: 2.907138811606059
Validation loss: 2.5401502577825252

Epoch: 6| Step: 8
Training loss: 2.5397309817129563
Validation loss: 2.5283431328030583

Epoch: 6| Step: 9
Training loss: 2.6206876664751264
Validation loss: 2.5319832889573917

Epoch: 6| Step: 10
Training loss: 2.4296091931839583
Validation loss: 2.5314882582316147

Epoch: 6| Step: 11
Training loss: 2.379382056147396
Validation loss: 2.5424526177176308

Epoch: 6| Step: 12
Training loss: 1.6385156059092343
Validation loss: 2.5550908928024207

Epoch: 6| Step: 13
Training loss: 1.8753856262381958
Validation loss: 2.548255031655949

Testing loss: 2.022970810193443

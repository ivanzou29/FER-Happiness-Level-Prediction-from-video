Epoch: 1| Step: 0
Training loss: 4.302051544189453
Validation loss: 5.369755744934082

Epoch: 6| Step: 1
Training loss: 5.743605136871338
Validation loss: 5.367524782816569

Epoch: 6| Step: 2
Training loss: 4.791602611541748
Validation loss: 5.365499655405681

Epoch: 6| Step: 3
Training loss: 5.601868629455566
Validation loss: 5.363470077514648

Epoch: 6| Step: 4
Training loss: 6.162284851074219
Validation loss: 5.36154834429423

Epoch: 6| Step: 5
Training loss: 5.1739068031311035
Validation loss: 5.359626134236653

Epoch: 6| Step: 6
Training loss: 6.600034713745117
Validation loss: 5.357638756434123

Epoch: 6| Step: 7
Training loss: 3.8343887329101562
Validation loss: 5.355655749638875

Epoch: 6| Step: 8
Training loss: 4.698843479156494
Validation loss: 5.353615363438924

Epoch: 6| Step: 9
Training loss: 6.010173320770264
Validation loss: 5.351528326670329

Epoch: 6| Step: 10
Training loss: 5.336809158325195
Validation loss: 5.349309126536052

Epoch: 6| Step: 11
Training loss: 6.130395412445068
Validation loss: 5.346999009450276

Epoch: 6| Step: 12
Training loss: 5.8213067054748535
Validation loss: 5.344599564870198

Epoch: 6| Step: 13
Training loss: 5.746591091156006
Validation loss: 5.3420272668202715

Epoch: 2| Step: 0
Training loss: 4.052151679992676
Validation loss: 5.339350541432698

Epoch: 6| Step: 1
Training loss: 6.151105880737305
Validation loss: 5.3364072640736895

Epoch: 6| Step: 2
Training loss: 5.2647552490234375
Validation loss: 5.333376328150432

Epoch: 6| Step: 3
Training loss: 6.280967712402344
Validation loss: 5.330094416936238

Epoch: 6| Step: 4
Training loss: 5.686791896820068
Validation loss: 5.326701561609904

Epoch: 6| Step: 5
Training loss: 5.730165004730225
Validation loss: 5.323140859603882

Epoch: 6| Step: 6
Training loss: 5.27841854095459
Validation loss: 5.319399356842041

Epoch: 6| Step: 7
Training loss: 5.307559013366699
Validation loss: 5.315248886744182

Epoch: 6| Step: 8
Training loss: 6.133714199066162
Validation loss: 5.31123153368632

Epoch: 6| Step: 9
Training loss: 4.639841079711914
Validation loss: 5.306701580683391

Epoch: 6| Step: 10
Training loss: 5.532198905944824
Validation loss: 5.302117347717285

Epoch: 6| Step: 11
Training loss: 5.42866849899292
Validation loss: 5.29717477162679

Epoch: 6| Step: 12
Training loss: 4.7484636306762695
Validation loss: 5.292006333669026

Epoch: 6| Step: 13
Training loss: 5.152862071990967
Validation loss: 5.286699612935384

Epoch: 3| Step: 0
Training loss: 4.053370952606201
Validation loss: 5.281012097994487

Epoch: 6| Step: 1
Training loss: 6.30162239074707
Validation loss: 5.275176286697388

Epoch: 6| Step: 2
Training loss: 4.366745948791504
Validation loss: 5.268967310587565

Epoch: 6| Step: 3
Training loss: 6.010969161987305
Validation loss: 5.262547334035237

Epoch: 6| Step: 4
Training loss: 5.634565830230713
Validation loss: 5.255969047546387

Epoch: 6| Step: 5
Training loss: 5.597291946411133
Validation loss: 5.249013344446818

Epoch: 6| Step: 6
Training loss: 5.736584186553955
Validation loss: 5.241928656895955

Epoch: 6| Step: 7
Training loss: 5.103401184082031
Validation loss: 5.2347025871276855

Epoch: 6| Step: 8
Training loss: 5.614496231079102
Validation loss: 5.226891120274861

Epoch: 6| Step: 9
Training loss: 4.1222944259643555
Validation loss: 5.219034036000569

Epoch: 6| Step: 10
Training loss: 5.907598495483398
Validation loss: 5.2109174728393555

Epoch: 6| Step: 11
Training loss: 5.1324567794799805
Validation loss: 5.203007936477661

Epoch: 6| Step: 12
Training loss: 5.101638317108154
Validation loss: 5.194831848144531

Epoch: 6| Step: 13
Training loss: 5.603546142578125
Validation loss: 5.186511278152466

Epoch: 4| Step: 0
Training loss: 4.783595085144043
Validation loss: 5.177787621815999

Epoch: 6| Step: 1
Training loss: 6.446314334869385
Validation loss: 5.169175386428833

Epoch: 6| Step: 2
Training loss: 5.111189842224121
Validation loss: 5.1603866418202715

Epoch: 6| Step: 3
Training loss: 5.165645122528076
Validation loss: 5.151604652404785

Epoch: 6| Step: 4
Training loss: 5.254096984863281
Validation loss: 5.1429111162821455

Epoch: 6| Step: 5
Training loss: 5.269415378570557
Validation loss: 5.13416854540507

Epoch: 6| Step: 6
Training loss: 5.310019016265869
Validation loss: 5.125194072723389

Epoch: 6| Step: 7
Training loss: 5.977146625518799
Validation loss: 5.116654396057129

Epoch: 6| Step: 8
Training loss: 4.724827766418457
Validation loss: 5.1078832149505615

Epoch: 6| Step: 9
Training loss: 4.157138824462891
Validation loss: 5.099151452382405

Epoch: 6| Step: 10
Training loss: 5.0262131690979
Validation loss: 5.090101003646851

Epoch: 6| Step: 11
Training loss: 4.046896934509277
Validation loss: 5.081444144248962

Epoch: 6| Step: 12
Training loss: 5.749970436096191
Validation loss: 5.072461287180583

Epoch: 6| Step: 13
Training loss: 5.725101947784424
Validation loss: 5.063647905985515

Epoch: 5| Step: 0
Training loss: 4.922255516052246
Validation loss: 5.054540077845256

Epoch: 6| Step: 1
Training loss: 6.230196952819824
Validation loss: 5.045119444529216

Epoch: 6| Step: 2
Training loss: 4.248311519622803
Validation loss: 5.036006172498067

Epoch: 6| Step: 3
Training loss: 5.410300254821777
Validation loss: 5.026553471883138

Epoch: 6| Step: 4
Training loss: 5.548985481262207
Validation loss: 5.016668955485026

Epoch: 6| Step: 5
Training loss: 4.558478355407715
Validation loss: 5.006975015004476

Epoch: 6| Step: 6
Training loss: 5.925713539123535
Validation loss: 4.997455994288127

Epoch: 6| Step: 7
Training loss: 3.971116304397583
Validation loss: 4.987757523854573

Epoch: 6| Step: 8
Training loss: 5.050601005554199
Validation loss: 4.978491862614949

Epoch: 6| Step: 9
Training loss: 4.934572219848633
Validation loss: 4.969411134719849

Epoch: 6| Step: 10
Training loss: 4.8171610832214355
Validation loss: 4.960607210795085

Epoch: 6| Step: 11
Training loss: 5.225839614868164
Validation loss: 4.951268116633098

Epoch: 6| Step: 12
Training loss: 5.311692237854004
Validation loss: 4.94273575146993

Epoch: 6| Step: 13
Training loss: 4.873595714569092
Validation loss: 4.9342068036397295

Epoch: 6| Step: 0
Training loss: 4.5096540451049805
Validation loss: 4.925673166910808

Epoch: 6| Step: 1
Training loss: 5.554299831390381
Validation loss: 4.917179107666016

Epoch: 6| Step: 2
Training loss: 6.290833473205566
Validation loss: 4.9092527230580645

Epoch: 6| Step: 3
Training loss: 4.252530097961426
Validation loss: 4.900802691777547

Epoch: 6| Step: 4
Training loss: 5.59046745300293
Validation loss: 4.89262040456136

Epoch: 6| Step: 5
Training loss: 5.496713161468506
Validation loss: 4.884423494338989

Epoch: 6| Step: 6
Training loss: 4.613578796386719
Validation loss: 4.876240412394206

Epoch: 6| Step: 7
Training loss: 4.957891464233398
Validation loss: 4.868166049321492

Epoch: 6| Step: 8
Training loss: 4.925336837768555
Validation loss: 4.859681765238444

Epoch: 6| Step: 9
Training loss: 4.467676162719727
Validation loss: 4.85112460454305

Epoch: 6| Step: 10
Training loss: 4.827784538269043
Validation loss: 4.8428928057352705

Epoch: 6| Step: 11
Training loss: 3.974177598953247
Validation loss: 4.834507425626119

Epoch: 6| Step: 12
Training loss: 6.435055732727051
Validation loss: 4.826194842656453

Epoch: 6| Step: 13
Training loss: 3.5220823287963867
Validation loss: 4.818249781926473

Epoch: 7| Step: 0
Training loss: 5.460023880004883
Validation loss: 4.8103710015614825

Epoch: 6| Step: 1
Training loss: 4.844026565551758
Validation loss: 4.802260319391887

Epoch: 6| Step: 2
Training loss: 4.409369468688965
Validation loss: 4.79391614596049

Epoch: 6| Step: 3
Training loss: 4.925624847412109
Validation loss: 4.785756826400757

Epoch: 6| Step: 4
Training loss: 5.047589302062988
Validation loss: 4.777551968892415

Epoch: 6| Step: 5
Training loss: 5.22756290435791
Validation loss: 4.76957639058431

Epoch: 6| Step: 6
Training loss: 4.565879821777344
Validation loss: 4.761064052581787

Epoch: 6| Step: 7
Training loss: 4.152496337890625
Validation loss: 4.753041744232178

Epoch: 6| Step: 8
Training loss: 4.9633307456970215
Validation loss: 4.744662721951802

Epoch: 6| Step: 9
Training loss: 4.399482250213623
Validation loss: 4.736255606015523

Epoch: 6| Step: 10
Training loss: 4.5394439697265625
Validation loss: 4.727797428766887

Epoch: 6| Step: 11
Training loss: 4.669754981994629
Validation loss: 4.7190572420756025

Epoch: 6| Step: 12
Training loss: 4.5911407470703125
Validation loss: 4.711034536361694

Epoch: 6| Step: 13
Training loss: 6.089990615844727
Validation loss: 4.702936728795369

Epoch: 8| Step: 0
Training loss: 5.364835262298584
Validation loss: 4.695469339688619

Epoch: 6| Step: 1
Training loss: 4.042753219604492
Validation loss: 4.687569300333659

Epoch: 6| Step: 2
Training loss: 4.815916061401367
Validation loss: 4.679874976476033

Epoch: 6| Step: 3
Training loss: 3.488187074661255
Validation loss: 4.671730438868205

Epoch: 6| Step: 4
Training loss: 4.968766212463379
Validation loss: 4.664074937502543

Epoch: 6| Step: 5
Training loss: 4.512461185455322
Validation loss: 4.656520287195842

Epoch: 6| Step: 6
Training loss: 4.386236190795898
Validation loss: 4.649016857147217

Epoch: 6| Step: 7
Training loss: 4.948720932006836
Validation loss: 4.641504406929016

Epoch: 6| Step: 8
Training loss: 5.336356163024902
Validation loss: 4.63426395257314

Epoch: 6| Step: 9
Training loss: 5.030575275421143
Validation loss: 4.627228856086731

Epoch: 6| Step: 10
Training loss: 4.985065460205078
Validation loss: 4.619669000307719

Epoch: 6| Step: 11
Training loss: 5.154111862182617
Validation loss: 4.612848361333211

Epoch: 6| Step: 12
Training loss: 4.278494834899902
Validation loss: 4.605346202850342

Epoch: 6| Step: 13
Training loss: 5.082491874694824
Validation loss: 4.597946445147197

Epoch: 9| Step: 0
Training loss: 4.385055065155029
Validation loss: 4.5914154052734375

Epoch: 6| Step: 1
Training loss: 4.851684093475342
Validation loss: 4.583830277125041

Epoch: 6| Step: 2
Training loss: 4.838871955871582
Validation loss: 4.576843659083049

Epoch: 6| Step: 3
Training loss: 3.536818265914917
Validation loss: 4.569614251454671

Epoch: 6| Step: 4
Training loss: 5.227665901184082
Validation loss: 4.563575903574626

Epoch: 6| Step: 5
Training loss: 4.4306817054748535
Validation loss: 4.555758277575175

Epoch: 6| Step: 6
Training loss: 4.339672565460205
Validation loss: 4.549732446670532

Epoch: 6| Step: 7
Training loss: 4.69039249420166
Validation loss: 4.542391697565715

Epoch: 6| Step: 8
Training loss: 5.7516913414001465
Validation loss: 4.535996834437053

Epoch: 6| Step: 9
Training loss: 4.3838300704956055
Validation loss: 4.529383460680644

Epoch: 6| Step: 10
Training loss: 3.642634868621826
Validation loss: 4.522750814755757

Epoch: 6| Step: 11
Training loss: 5.062844753265381
Validation loss: 4.515224456787109

Epoch: 6| Step: 12
Training loss: 4.972533226013184
Validation loss: 4.508601983388265

Epoch: 6| Step: 13
Training loss: 4.963719844818115
Validation loss: 4.5018095175425215

Epoch: 10| Step: 0
Training loss: 5.9571027755737305
Validation loss: 4.494709372520447

Epoch: 6| Step: 1
Training loss: 4.003196716308594
Validation loss: 4.487909475962321

Epoch: 6| Step: 2
Training loss: 2.944462776184082
Validation loss: 4.481692552566528

Epoch: 6| Step: 3
Training loss: 4.925507545471191
Validation loss: 4.474764585494995

Epoch: 6| Step: 4
Training loss: 5.340297698974609
Validation loss: 4.467852354049683

Epoch: 6| Step: 5
Training loss: 4.7685112953186035
Validation loss: 4.46135417620341

Epoch: 6| Step: 6
Training loss: 4.451876163482666
Validation loss: 4.454746842384338

Epoch: 6| Step: 7
Training loss: 4.95110559463501
Validation loss: 4.447052915891011

Epoch: 6| Step: 8
Training loss: 4.356199264526367
Validation loss: 4.440104246139526

Epoch: 6| Step: 9
Training loss: 4.156172275543213
Validation loss: 4.434497078259786

Epoch: 6| Step: 10
Training loss: 3.948289632797241
Validation loss: 4.4268918832143145

Epoch: 6| Step: 11
Training loss: 4.343724250793457
Validation loss: 4.419842680295308

Epoch: 6| Step: 12
Training loss: 4.805848121643066
Validation loss: 4.413628578186035

Epoch: 6| Step: 13
Training loss: 4.915587425231934
Validation loss: 4.407415111859639

Epoch: 11| Step: 0
Training loss: 4.790650844573975
Validation loss: 4.401244322458903

Epoch: 6| Step: 1
Training loss: 4.9681620597839355
Validation loss: 4.394525249799092

Epoch: 6| Step: 2
Training loss: 4.214834213256836
Validation loss: 4.388752023379008

Epoch: 6| Step: 3
Training loss: 4.092807769775391
Validation loss: 4.381599982579549

Epoch: 6| Step: 4
Training loss: 4.352816581726074
Validation loss: 4.3756513595581055

Epoch: 6| Step: 5
Training loss: 4.080005168914795
Validation loss: 4.368253787358602

Epoch: 6| Step: 6
Training loss: 3.2183659076690674
Validation loss: 4.362226406733195

Epoch: 6| Step: 7
Training loss: 3.8622803688049316
Validation loss: 4.356064438819885

Epoch: 6| Step: 8
Training loss: 4.269186973571777
Validation loss: 4.349099437395732

Epoch: 6| Step: 9
Training loss: 4.572939872741699
Validation loss: 4.343480904897054

Epoch: 6| Step: 10
Training loss: 6.0091352462768555
Validation loss: 4.337361176808675

Epoch: 6| Step: 11
Training loss: 3.9763503074645996
Validation loss: 4.331080913543701

Epoch: 6| Step: 12
Training loss: 5.394172668457031
Validation loss: 4.325110713640849

Epoch: 6| Step: 13
Training loss: 4.898126602172852
Validation loss: 4.3185514609018965

Epoch: 12| Step: 0
Training loss: 5.074554920196533
Validation loss: 4.3122163613637285

Epoch: 6| Step: 1
Training loss: 4.02280330657959
Validation loss: 4.30621341864268

Epoch: 6| Step: 2
Training loss: 4.326416969299316
Validation loss: 4.300441702206929

Epoch: 6| Step: 3
Training loss: 5.128544807434082
Validation loss: 4.294878284136455

Epoch: 6| Step: 4
Training loss: 5.087972640991211
Validation loss: 4.289189100265503

Epoch: 6| Step: 5
Training loss: 3.786410331726074
Validation loss: 4.2828383048375445

Epoch: 6| Step: 6
Training loss: 5.078490257263184
Validation loss: 4.277836402257283

Epoch: 6| Step: 7
Training loss: 4.935912132263184
Validation loss: 4.271353999773662

Epoch: 6| Step: 8
Training loss: 4.626743793487549
Validation loss: 4.265486677487691

Epoch: 6| Step: 9
Training loss: 4.150481700897217
Validation loss: 4.258545120557149

Epoch: 6| Step: 10
Training loss: 3.6063687801361084
Validation loss: 4.252508958180745

Epoch: 6| Step: 11
Training loss: 4.571195125579834
Validation loss: 4.247225324312846

Epoch: 6| Step: 12
Training loss: 4.161277770996094
Validation loss: 4.240324139595032

Epoch: 6| Step: 13
Training loss: 3.0277493000030518
Validation loss: 4.2313045263290405

Epoch: 13| Step: 0
Training loss: 3.9862220287323
Validation loss: 4.223057111104329

Epoch: 6| Step: 1
Training loss: 4.552376747131348
Validation loss: 4.2161771059036255

Epoch: 6| Step: 2
Training loss: 3.6202292442321777
Validation loss: 4.209105571111043

Epoch: 6| Step: 3
Training loss: 4.258665084838867
Validation loss: 4.202457706133525

Epoch: 6| Step: 4
Training loss: 4.7138166427612305
Validation loss: 4.1964536507924395

Epoch: 6| Step: 5
Training loss: 4.967315673828125
Validation loss: 4.188815991083781

Epoch: 6| Step: 6
Training loss: 4.327395439147949
Validation loss: 4.183556278546651

Epoch: 6| Step: 7
Training loss: 4.08644962310791
Validation loss: 4.1768695910771685

Epoch: 6| Step: 8
Training loss: 4.582574367523193
Validation loss: 4.171254237492879

Epoch: 6| Step: 9
Training loss: 4.035370826721191
Validation loss: 4.164835969607036

Epoch: 6| Step: 10
Training loss: 4.188351154327393
Validation loss: 4.1598009665807085

Epoch: 6| Step: 11
Training loss: 5.193331241607666
Validation loss: 4.15442419052124

Epoch: 6| Step: 12
Training loss: 4.245306015014648
Validation loss: 4.148100852966309

Epoch: 6| Step: 13
Training loss: 3.66459584236145
Validation loss: 4.142039179801941

Epoch: 14| Step: 0
Training loss: 3.9633421897888184
Validation loss: 4.1357475121816

Epoch: 6| Step: 1
Training loss: 5.009071350097656
Validation loss: 4.130455851554871

Epoch: 6| Step: 2
Training loss: 4.953245639801025
Validation loss: 4.12494421005249

Epoch: 6| Step: 3
Training loss: 4.204362392425537
Validation loss: 4.119535485903422

Epoch: 6| Step: 4
Training loss: 3.6349339485168457
Validation loss: 4.114095012346904

Epoch: 6| Step: 5
Training loss: 5.050030708312988
Validation loss: 4.108306646347046

Epoch: 6| Step: 6
Training loss: 2.5355224609375
Validation loss: 4.101874152819316

Epoch: 6| Step: 7
Training loss: 4.550027847290039
Validation loss: 4.097518603006999

Epoch: 6| Step: 8
Training loss: 4.472748279571533
Validation loss: 4.092039028803508

Epoch: 6| Step: 9
Training loss: 4.062535762786865
Validation loss: 4.086404641469319

Epoch: 6| Step: 10
Training loss: 3.128262519836426
Validation loss: 4.080649216969808

Epoch: 6| Step: 11
Training loss: 3.4107470512390137
Validation loss: 4.075122237205505

Epoch: 6| Step: 12
Training loss: 5.343335151672363
Validation loss: 4.070676287015279

Epoch: 6| Step: 13
Training loss: 5.045375823974609
Validation loss: 4.065821727116902

Epoch: 15| Step: 0
Training loss: 4.275292873382568
Validation loss: 4.060548027356465

Epoch: 6| Step: 1
Training loss: 3.8325703144073486
Validation loss: 4.0554918845494585

Epoch: 6| Step: 2
Training loss: 5.739819526672363
Validation loss: 4.05073881149292

Epoch: 6| Step: 3
Training loss: 5.120638847351074
Validation loss: 4.046030600865682

Epoch: 6| Step: 4
Training loss: 3.1498632431030273
Validation loss: 4.040512402852376

Epoch: 6| Step: 5
Training loss: 4.680909156799316
Validation loss: 4.035054763158162

Epoch: 6| Step: 6
Training loss: 5.060213088989258
Validation loss: 4.030132015546163

Epoch: 6| Step: 7
Training loss: 3.809187650680542
Validation loss: 4.025001327196757

Epoch: 6| Step: 8
Training loss: 3.1301651000976562
Validation loss: 4.020154754320781

Epoch: 6| Step: 9
Training loss: 3.9284448623657227
Validation loss: 4.01514991124471

Epoch: 6| Step: 10
Training loss: 3.2894067764282227
Validation loss: 4.0103759765625

Epoch: 6| Step: 11
Training loss: 3.8031129837036133
Validation loss: 4.006602684656779

Epoch: 6| Step: 12
Training loss: 3.5368857383728027
Validation loss: 4.002010424931844

Epoch: 6| Step: 13
Training loss: 5.058215618133545
Validation loss: 3.996943712234497

Epoch: 16| Step: 0
Training loss: 3.852788209915161
Validation loss: 3.991910735766093

Epoch: 6| Step: 1
Training loss: 3.5406603813171387
Validation loss: 3.9876809120178223

Epoch: 6| Step: 2
Training loss: 4.096303462982178
Validation loss: 3.984364072481791

Epoch: 6| Step: 3
Training loss: 4.815952301025391
Validation loss: 3.977695425351461

Epoch: 6| Step: 4
Training loss: 3.8446600437164307
Validation loss: 3.9733484586079917

Epoch: 6| Step: 5
Training loss: 3.678398847579956
Validation loss: 3.96869949499766

Epoch: 6| Step: 6
Training loss: 4.907073974609375
Validation loss: 3.96403706073761

Epoch: 6| Step: 7
Training loss: 4.404359817504883
Validation loss: 3.959314743677775

Epoch: 6| Step: 8
Training loss: 4.774618625640869
Validation loss: 3.9547465642293296

Epoch: 6| Step: 9
Training loss: 4.214776039123535
Validation loss: 3.9493066867192588

Epoch: 6| Step: 10
Training loss: 3.640869140625
Validation loss: 3.9442673921585083

Epoch: 6| Step: 11
Training loss: 4.44972038269043
Validation loss: 3.939992586771647

Epoch: 6| Step: 12
Training loss: 3.7454822063446045
Validation loss: 3.9346303145090737

Epoch: 6| Step: 13
Training loss: 3.531442880630493
Validation loss: 3.9300110737482705

Epoch: 17| Step: 0
Training loss: 3.635040044784546
Validation loss: 3.92555300394694

Epoch: 6| Step: 1
Training loss: 3.859610080718994
Validation loss: 3.920404314994812

Epoch: 6| Step: 2
Training loss: 3.704087257385254
Validation loss: 3.9159613053003945

Epoch: 6| Step: 3
Training loss: 3.7535271644592285
Validation loss: 3.910504460334778

Epoch: 6| Step: 4
Training loss: 3.265561580657959
Validation loss: 3.9062554836273193

Epoch: 6| Step: 5
Training loss: 4.238735675811768
Validation loss: 3.901856621106466

Epoch: 6| Step: 6
Training loss: 3.9878859519958496
Validation loss: 3.8966898123423257

Epoch: 6| Step: 7
Training loss: 4.125962257385254
Validation loss: 3.8921087185541787

Epoch: 6| Step: 8
Training loss: 3.6730833053588867
Validation loss: 3.887955625851949

Epoch: 6| Step: 9
Training loss: 4.34641170501709
Validation loss: 3.883385340372721

Epoch: 6| Step: 10
Training loss: 4.171503067016602
Validation loss: 3.878640373547872

Epoch: 6| Step: 11
Training loss: 4.608215808868408
Validation loss: 3.873781204223633

Epoch: 6| Step: 12
Training loss: 4.846403121948242
Validation loss: 3.8692607482274375

Epoch: 6| Step: 13
Training loss: 4.388030052185059
Validation loss: 3.8642950852711997

Epoch: 18| Step: 0
Training loss: 4.169848918914795
Validation loss: 3.8605156342188516

Epoch: 6| Step: 1
Training loss: 4.770598888397217
Validation loss: 3.8555851380030313

Epoch: 6| Step: 2
Training loss: 4.032112121582031
Validation loss: 3.8503671487172446

Epoch: 6| Step: 3
Training loss: 4.158366680145264
Validation loss: 3.8461121320724487

Epoch: 6| Step: 4
Training loss: 3.796933650970459
Validation loss: 3.8416607777277627

Epoch: 6| Step: 5
Training loss: 4.903514862060547
Validation loss: 3.8370845715204873

Epoch: 6| Step: 6
Training loss: 3.7155303955078125
Validation loss: 3.8322611649831138

Epoch: 6| Step: 7
Training loss: 4.5325212478637695
Validation loss: 3.827772776285807

Epoch: 6| Step: 8
Training loss: 2.5464372634887695
Validation loss: 3.8228331009546914

Epoch: 6| Step: 9
Training loss: 4.075326919555664
Validation loss: 3.818513870239258

Epoch: 6| Step: 10
Training loss: 3.7348718643188477
Validation loss: 3.8139506578445435

Epoch: 6| Step: 11
Training loss: 3.3613929748535156
Validation loss: 3.8093651135762534

Epoch: 6| Step: 12
Training loss: 4.185485363006592
Validation loss: 3.8048460483551025

Epoch: 6| Step: 13
Training loss: 3.8088817596435547
Validation loss: 3.8007888793945312

Epoch: 19| Step: 0
Training loss: 3.7672133445739746
Validation loss: 3.796585281689962

Epoch: 6| Step: 1
Training loss: 3.7102088928222656
Validation loss: 3.79217259089152

Epoch: 6| Step: 2
Training loss: 4.174403667449951
Validation loss: 3.788217822710673

Epoch: 6| Step: 3
Training loss: 4.171504020690918
Validation loss: 3.7835715611775718

Epoch: 6| Step: 4
Training loss: 3.1837878227233887
Validation loss: 3.7788132429122925

Epoch: 6| Step: 5
Training loss: 3.666012763977051
Validation loss: 3.7745660543441772

Epoch: 6| Step: 6
Training loss: 4.710927963256836
Validation loss: 3.770536422729492

Epoch: 6| Step: 7
Training loss: 3.539875030517578
Validation loss: 3.7655816872914634

Epoch: 6| Step: 8
Training loss: 4.60120964050293
Validation loss: 3.761357585589091

Epoch: 6| Step: 9
Training loss: 3.8118815422058105
Validation loss: 3.7568644285202026

Epoch: 6| Step: 10
Training loss: 3.263607978820801
Validation loss: 3.753022392590841

Epoch: 6| Step: 11
Training loss: 4.789741039276123
Validation loss: 3.7496761878331504

Epoch: 6| Step: 12
Training loss: 4.365198612213135
Validation loss: 3.7436190446217856

Epoch: 6| Step: 13
Training loss: 3.2082481384277344
Validation loss: 3.7401491006215415

Epoch: 20| Step: 0
Training loss: 3.2050914764404297
Validation loss: 3.735993981361389

Epoch: 6| Step: 1
Training loss: 4.6096696853637695
Validation loss: 3.7320369879404702

Epoch: 6| Step: 2
Training loss: 4.623561859130859
Validation loss: 3.728154102961222

Epoch: 6| Step: 3
Training loss: 3.6748476028442383
Validation loss: 3.723906993865967

Epoch: 6| Step: 4
Training loss: 3.483017683029175
Validation loss: 3.7201001246770224

Epoch: 6| Step: 5
Training loss: 3.7216641902923584
Validation loss: 3.715892752011617

Epoch: 6| Step: 6
Training loss: 4.944936752319336
Validation loss: 3.7116790215174356

Epoch: 6| Step: 7
Training loss: 5.419124603271484
Validation loss: 3.707352121671041

Epoch: 6| Step: 8
Training loss: 3.098369598388672
Validation loss: 3.7027916510899863

Epoch: 6| Step: 9
Training loss: 3.6328494548797607
Validation loss: 3.698470711708069

Epoch: 6| Step: 10
Training loss: 3.451956033706665
Validation loss: 3.694444457689921

Epoch: 6| Step: 11
Training loss: 2.708787441253662
Validation loss: 3.6904537280400596

Epoch: 6| Step: 12
Training loss: 3.026517391204834
Validation loss: 3.686206301053365

Epoch: 6| Step: 13
Training loss: 4.574755668640137
Validation loss: 3.6819302241007485

Epoch: 21| Step: 0
Training loss: 4.134922027587891
Validation loss: 3.677239696184794

Epoch: 6| Step: 1
Training loss: 2.56707763671875
Validation loss: 3.6724851926167807

Epoch: 6| Step: 2
Training loss: 4.074152946472168
Validation loss: 3.6684775749842324

Epoch: 6| Step: 3
Training loss: 4.352298736572266
Validation loss: 3.6644724210103354

Epoch: 6| Step: 4
Training loss: 3.43910551071167
Validation loss: 3.6601740519205728

Epoch: 6| Step: 5
Training loss: 4.226181507110596
Validation loss: 3.656268278757731

Epoch: 6| Step: 6
Training loss: 4.518250465393066
Validation loss: 3.651670495669047

Epoch: 6| Step: 7
Training loss: 3.8607778549194336
Validation loss: 3.6472833156585693

Epoch: 6| Step: 8
Training loss: 4.4091949462890625
Validation loss: 3.6427990198135376

Epoch: 6| Step: 9
Training loss: 3.0261411666870117
Validation loss: 3.638754407564799

Epoch: 6| Step: 10
Training loss: 3.525651693344116
Validation loss: 3.634444753328959

Epoch: 6| Step: 11
Training loss: 3.020609140396118
Validation loss: 3.6297041972478232

Epoch: 6| Step: 12
Training loss: 4.075868129730225
Validation loss: 3.625554362932841

Epoch: 6| Step: 13
Training loss: 4.134654521942139
Validation loss: 3.6215627988179526

Epoch: 22| Step: 0
Training loss: 3.544435739517212
Validation loss: 3.6168631315231323

Epoch: 6| Step: 1
Training loss: 3.98622465133667
Validation loss: 3.6127919356028237

Epoch: 6| Step: 2
Training loss: 3.794739246368408
Validation loss: 3.608328660329183

Epoch: 6| Step: 3
Training loss: 4.193331718444824
Validation loss: 3.6039902766545615

Epoch: 6| Step: 4
Training loss: 3.58949613571167
Validation loss: 3.5992624759674072

Epoch: 6| Step: 5
Training loss: 4.434841156005859
Validation loss: 3.5948856274286904

Epoch: 6| Step: 6
Training loss: 3.768554925918579
Validation loss: 3.59046999613444

Epoch: 6| Step: 7
Training loss: 3.406808376312256
Validation loss: 3.5862277348836265

Epoch: 6| Step: 8
Training loss: 3.4796993732452393
Validation loss: 3.5814305941263833

Epoch: 6| Step: 9
Training loss: 4.818568229675293
Validation loss: 3.5769059658050537

Epoch: 6| Step: 10
Training loss: 2.4993958473205566
Validation loss: 3.5725111166636148

Epoch: 6| Step: 11
Training loss: 3.567131519317627
Validation loss: 3.567915439605713

Epoch: 6| Step: 12
Training loss: 3.653341770172119
Validation loss: 3.5638001759847007

Epoch: 6| Step: 13
Training loss: 3.8264007568359375
Validation loss: 3.5594969987869263

Epoch: 23| Step: 0
Training loss: 4.254054069519043
Validation loss: 3.555177370707194

Epoch: 6| Step: 1
Training loss: 3.818615436553955
Validation loss: 3.5511130491892495

Epoch: 6| Step: 2
Training loss: 3.145533561706543
Validation loss: 3.546628952026367

Epoch: 6| Step: 3
Training loss: 3.481009006500244
Validation loss: 3.542199412981669

Epoch: 6| Step: 4
Training loss: 3.507075548171997
Validation loss: 3.5383623441060386

Epoch: 6| Step: 5
Training loss: 3.8505258560180664
Validation loss: 3.53419291973114

Epoch: 6| Step: 6
Training loss: 4.291411876678467
Validation loss: 3.5301125049591064

Epoch: 6| Step: 7
Training loss: 3.411393880844116
Validation loss: 3.5256590048472085

Epoch: 6| Step: 8
Training loss: 4.106524467468262
Validation loss: 3.521221478780111

Epoch: 6| Step: 9
Training loss: 3.7647435665130615
Validation loss: 3.517247200012207

Epoch: 6| Step: 10
Training loss: 3.3249197006225586
Validation loss: 3.5127031008402505

Epoch: 6| Step: 11
Training loss: 3.3448498249053955
Validation loss: 3.508687893549601

Epoch: 6| Step: 12
Training loss: 3.1984288692474365
Validation loss: 3.504542867342631

Epoch: 6| Step: 13
Training loss: 4.223328590393066
Validation loss: 3.5013707876205444

Epoch: 24| Step: 0
Training loss: 3.662984848022461
Validation loss: 3.495993494987488

Epoch: 6| Step: 1
Training loss: 3.322403907775879
Validation loss: 3.491926113764445

Epoch: 6| Step: 2
Training loss: 4.035857200622559
Validation loss: 3.4879382054011026

Epoch: 6| Step: 3
Training loss: 4.219649791717529
Validation loss: 3.483873208363851

Epoch: 6| Step: 4
Training loss: 2.980011463165283
Validation loss: 3.4791765610376992

Epoch: 6| Step: 5
Training loss: 4.1244120597839355
Validation loss: 3.474253217379252

Epoch: 6| Step: 6
Training loss: 2.8256006240844727
Validation loss: 3.469491799672445

Epoch: 6| Step: 7
Training loss: 2.911123752593994
Validation loss: 3.464886506398519

Epoch: 6| Step: 8
Training loss: 3.964533805847168
Validation loss: 3.460225065549215

Epoch: 6| Step: 9
Training loss: 3.660097122192383
Validation loss: 3.4553513526916504

Epoch: 6| Step: 10
Training loss: 3.3027637004852295
Validation loss: 3.4502280155817666

Epoch: 6| Step: 11
Training loss: 4.830839157104492
Validation loss: 3.445263703664144

Epoch: 6| Step: 12
Training loss: 3.4008772373199463
Validation loss: 3.4405246575673423

Epoch: 6| Step: 13
Training loss: 3.6580920219421387
Validation loss: 3.4358067909876504

Epoch: 25| Step: 0
Training loss: 3.664989471435547
Validation loss: 3.4309542179107666

Epoch: 6| Step: 1
Training loss: 2.925560474395752
Validation loss: 3.426489313443502

Epoch: 6| Step: 2
Training loss: 4.449137210845947
Validation loss: 3.4222394625345864

Epoch: 6| Step: 3
Training loss: 3.326871871948242
Validation loss: 3.41866405804952

Epoch: 6| Step: 4
Training loss: 3.720388412475586
Validation loss: 3.413751244544983

Epoch: 6| Step: 5
Training loss: 3.6414780616760254
Validation loss: 3.4093879063924155

Epoch: 6| Step: 6
Training loss: 3.2164571285247803
Validation loss: 3.405006170272827

Epoch: 6| Step: 7
Training loss: 3.359930992126465
Validation loss: 3.401161472002665

Epoch: 6| Step: 8
Training loss: 3.3883843421936035
Validation loss: 3.3974878787994385

Epoch: 6| Step: 9
Training loss: 3.667418956756592
Validation loss: 3.3930064042409263

Epoch: 6| Step: 10
Training loss: 3.749934196472168
Validation loss: 3.3884532848993936

Epoch: 6| Step: 11
Training loss: 3.118093967437744
Validation loss: 3.38370672861735

Epoch: 6| Step: 12
Training loss: 4.175263404846191
Validation loss: 3.3800119956334433

Epoch: 6| Step: 13
Training loss: 3.5781500339508057
Validation loss: 3.3751989603042603

Epoch: 26| Step: 0
Training loss: 3.2121775150299072
Validation loss: 3.37011988957723

Epoch: 6| Step: 1
Training loss: 3.7034316062927246
Validation loss: 3.3651110331217446

Epoch: 6| Step: 2
Training loss: 3.4503331184387207
Validation loss: 3.3611066738764444

Epoch: 6| Step: 3
Training loss: 3.0418293476104736
Validation loss: 3.356896082560221

Epoch: 6| Step: 4
Training loss: 3.5970077514648438
Validation loss: 3.3524904251098633

Epoch: 6| Step: 5
Training loss: 3.3021843433380127
Validation loss: 3.348065733909607

Epoch: 6| Step: 6
Training loss: 2.583871364593506
Validation loss: 3.3433632850646973

Epoch: 6| Step: 7
Training loss: 4.646178245544434
Validation loss: 3.339160124460856

Epoch: 6| Step: 8
Training loss: 3.4993343353271484
Validation loss: 3.3340160051981607

Epoch: 6| Step: 9
Training loss: 3.7115063667297363
Validation loss: 3.3299274841944375

Epoch: 6| Step: 10
Training loss: 3.656097173690796
Validation loss: 3.325117826461792

Epoch: 6| Step: 11
Training loss: 3.9622039794921875
Validation loss: 3.320914069811503

Epoch: 6| Step: 12
Training loss: 3.685213088989258
Validation loss: 3.3164724906285605

Epoch: 6| Step: 13
Training loss: 3.1135425567626953
Validation loss: 3.3121547301610312

Epoch: 27| Step: 0
Training loss: 3.862868309020996
Validation loss: 3.307505170504252

Epoch: 6| Step: 1
Training loss: 3.459662437438965
Validation loss: 3.302533507347107

Epoch: 6| Step: 2
Training loss: 2.8051037788391113
Validation loss: 3.298409899075826

Epoch: 6| Step: 3
Training loss: 3.9411065578460693
Validation loss: 3.2939988374710083

Epoch: 6| Step: 4
Training loss: 3.120624542236328
Validation loss: 3.2893584171930947

Epoch: 6| Step: 5
Training loss: 2.948794364929199
Validation loss: 3.2853622436523438

Epoch: 6| Step: 6
Training loss: 3.4313669204711914
Validation loss: 3.281774560610453

Epoch: 6| Step: 7
Training loss: 3.6945700645446777
Validation loss: 3.2778321504592896

Epoch: 6| Step: 8
Training loss: 3.737621784210205
Validation loss: 3.2742128372192383

Epoch: 6| Step: 9
Training loss: 3.1185078620910645
Validation loss: 3.2703988949457803

Epoch: 6| Step: 10
Training loss: 3.6918091773986816
Validation loss: 3.265362103780111

Epoch: 6| Step: 11
Training loss: 3.5513272285461426
Validation loss: 3.2614701191584268

Epoch: 6| Step: 12
Training loss: 3.1335701942443848
Validation loss: 3.257576664288839

Epoch: 6| Step: 13
Training loss: 3.8850409984588623
Validation loss: 3.253711779912313

Epoch: 28| Step: 0
Training loss: 2.624610185623169
Validation loss: 3.249501347541809

Epoch: 6| Step: 1
Training loss: 4.442227363586426
Validation loss: 3.2454014221827188

Epoch: 6| Step: 2
Training loss: 3.4914731979370117
Validation loss: 3.2413800954818726

Epoch: 6| Step: 3
Training loss: 2.990009307861328
Validation loss: 3.236848076184591

Epoch: 6| Step: 4
Training loss: 3.308605670928955
Validation loss: 3.2330528100331626

Epoch: 6| Step: 5
Training loss: 2.9998414516448975
Validation loss: 3.2295047839482627

Epoch: 6| Step: 6
Training loss: 4.652763366699219
Validation loss: 3.2259771823883057

Epoch: 6| Step: 7
Training loss: 3.2805023193359375
Validation loss: 3.2216505209604898

Epoch: 6| Step: 8
Training loss: 3.9928574562072754
Validation loss: 3.2168734471003213

Epoch: 6| Step: 9
Training loss: 2.747748374938965
Validation loss: 3.212628642717997

Epoch: 6| Step: 10
Training loss: 4.129325866699219
Validation loss: 3.2087687253952026

Epoch: 6| Step: 11
Training loss: 3.382802724838257
Validation loss: 3.20462433497111

Epoch: 6| Step: 12
Training loss: 3.2594223022460938
Validation loss: 3.2006970643997192

Epoch: 6| Step: 13
Training loss: 2.3425378799438477
Validation loss: 3.1966649691263833

Epoch: 29| Step: 0
Training loss: 3.43825364112854
Validation loss: 3.1923750837643943

Epoch: 6| Step: 1
Training loss: 3.2350919246673584
Validation loss: 3.1887386639912925

Epoch: 6| Step: 2
Training loss: 3.6612493991851807
Validation loss: 3.1852684020996094

Epoch: 6| Step: 3
Training loss: 2.5422987937927246
Validation loss: 3.181079347928365

Epoch: 6| Step: 4
Training loss: 3.6528751850128174
Validation loss: 3.176903009414673

Epoch: 6| Step: 5
Training loss: 3.7976644039154053
Validation loss: 3.173067053159078

Epoch: 6| Step: 6
Training loss: 3.824434280395508
Validation loss: 3.1690837144851685

Epoch: 6| Step: 7
Training loss: 3.6554033756256104
Validation loss: 3.1649409532546997

Epoch: 6| Step: 8
Training loss: 2.969921112060547
Validation loss: 3.1607521375020347

Epoch: 6| Step: 9
Training loss: 3.16827392578125
Validation loss: 3.1570780277252197

Epoch: 6| Step: 10
Training loss: 2.8536767959594727
Validation loss: 3.1533029874165854

Epoch: 6| Step: 11
Training loss: 2.7815346717834473
Validation loss: 3.1493285497029624

Epoch: 6| Step: 12
Training loss: 4.06201696395874
Validation loss: 3.1466075579325357

Epoch: 6| Step: 13
Training loss: 3.2741708755493164
Validation loss: 3.141571323076884

Epoch: 30| Step: 0
Training loss: 2.9949584007263184
Validation loss: 3.1377426783243814

Epoch: 6| Step: 1
Training loss: 2.1040334701538086
Validation loss: 3.133770744005839

Epoch: 6| Step: 2
Training loss: 4.065391540527344
Validation loss: 3.130378842353821

Epoch: 6| Step: 3
Training loss: 4.159607410430908
Validation loss: 3.126603921254476

Epoch: 6| Step: 4
Training loss: 2.973278284072876
Validation loss: 3.1223851442337036

Epoch: 6| Step: 5
Training loss: 3.1635191440582275
Validation loss: 3.118624528249105

Epoch: 6| Step: 6
Training loss: 3.669754981994629
Validation loss: 3.1146277586619058

Epoch: 6| Step: 7
Training loss: 3.2220749855041504
Validation loss: 3.1111155351003013

Epoch: 6| Step: 8
Training loss: 3.1119956970214844
Validation loss: 3.1069247325261435

Epoch: 6| Step: 9
Training loss: 3.6966609954833984
Validation loss: 3.10329806804657

Epoch: 6| Step: 10
Training loss: 3.154111623764038
Validation loss: 3.0994725227355957

Epoch: 6| Step: 11
Training loss: 3.681133270263672
Validation loss: 3.095836857954661

Epoch: 6| Step: 12
Training loss: 2.679629325866699
Validation loss: 3.0921260118484497

Epoch: 6| Step: 13
Training loss: 3.547006845474243
Validation loss: 3.088536183039347

Epoch: 31| Step: 0
Training loss: 3.8967041969299316
Validation loss: 3.085106452306112

Epoch: 6| Step: 1
Training loss: 3.427046775817871
Validation loss: 3.0811049938201904

Epoch: 6| Step: 2
Training loss: 2.561708450317383
Validation loss: 3.0774319569269815

Epoch: 6| Step: 3
Training loss: 3.378466844558716
Validation loss: 3.073725779851278

Epoch: 6| Step: 4
Training loss: 2.7780425548553467
Validation loss: 3.069902221361796

Epoch: 6| Step: 5
Training loss: 3.5777392387390137
Validation loss: 3.065933664639791

Epoch: 6| Step: 6
Training loss: 3.6720879077911377
Validation loss: 3.062354882558187

Epoch: 6| Step: 7
Training loss: 3.858053207397461
Validation loss: 3.058433492978414

Epoch: 6| Step: 8
Training loss: 3.460113525390625
Validation loss: 3.0545626878738403

Epoch: 6| Step: 9
Training loss: 2.875096321105957
Validation loss: 3.051003615061442

Epoch: 6| Step: 10
Training loss: 3.1687088012695312
Validation loss: 3.048116087913513

Epoch: 6| Step: 11
Training loss: 3.661184310913086
Validation loss: 3.0528165896733603

Epoch: 6| Step: 12
Training loss: 2.426952838897705
Validation loss: 3.0544846455256143

Epoch: 6| Step: 13
Training loss: 2.8489794731140137
Validation loss: 3.059944987297058

Epoch: 32| Step: 0
Training loss: 3.850966691970825
Validation loss: 3.042816082636515

Epoch: 6| Step: 1
Training loss: 3.1792171001434326
Validation loss: 3.03098726272583

Epoch: 6| Step: 2
Training loss: 2.850416660308838
Validation loss: 3.028532346089681

Epoch: 6| Step: 3
Training loss: 3.4674692153930664
Validation loss: 3.02929159005483

Epoch: 6| Step: 4
Training loss: 2.8504114151000977
Validation loss: 3.02874755859375

Epoch: 6| Step: 5
Training loss: 3.5314831733703613
Validation loss: 3.027240792910258

Epoch: 6| Step: 6
Training loss: 3.0214600563049316
Validation loss: 3.0188995202382407

Epoch: 6| Step: 7
Training loss: 3.2051572799682617
Validation loss: 3.014761726061503

Epoch: 6| Step: 8
Training loss: 2.541396141052246
Validation loss: 3.0110194285710654

Epoch: 6| Step: 9
Training loss: 3.0452394485473633
Validation loss: 3.0085581143697104

Epoch: 6| Step: 10
Training loss: 2.689767360687256
Validation loss: 3.0060426394144693

Epoch: 6| Step: 11
Training loss: 4.4149603843688965
Validation loss: 3.0039365688959756

Epoch: 6| Step: 12
Training loss: 3.5296401977539062
Validation loss: 2.9998137950897217

Epoch: 6| Step: 13
Training loss: 2.7671947479248047
Validation loss: 2.994725505510966

Epoch: 33| Step: 0
Training loss: 3.2329883575439453
Validation loss: 2.991939981778463

Epoch: 6| Step: 1
Training loss: 2.6048200130462646
Validation loss: 2.987461527188619

Epoch: 6| Step: 2
Training loss: 2.8849291801452637
Validation loss: 2.9843103885650635

Epoch: 6| Step: 3
Training loss: 2.829090118408203
Validation loss: 2.982818285624186

Epoch: 6| Step: 4
Training loss: 3.870372772216797
Validation loss: 2.9789733489354453

Epoch: 6| Step: 5
Training loss: 3.1170425415039062
Validation loss: 2.9763253927230835

Epoch: 6| Step: 6
Training loss: 3.2418951988220215
Validation loss: 2.972066958745321

Epoch: 6| Step: 7
Training loss: 3.0507819652557373
Validation loss: 2.9683385292689004

Epoch: 6| Step: 8
Training loss: 3.217357635498047
Validation loss: 2.9645081758499146

Epoch: 6| Step: 9
Training loss: 3.216883659362793
Validation loss: 2.9609281619389853

Epoch: 6| Step: 10
Training loss: 3.738783597946167
Validation loss: 2.9578099250793457

Epoch: 6| Step: 11
Training loss: 3.127939462661743
Validation loss: 2.9541638692220054

Epoch: 6| Step: 12
Training loss: 3.129730701446533
Validation loss: 2.9505091110865274

Epoch: 6| Step: 13
Training loss: 3.0781641006469727
Validation loss: 2.947237769762675

Epoch: 34| Step: 0
Training loss: 3.0990610122680664
Validation loss: 2.9440401792526245

Epoch: 6| Step: 1
Training loss: 3.4729347229003906
Validation loss: 2.940314451853434

Epoch: 6| Step: 2
Training loss: 3.560304641723633
Validation loss: 2.9366435607274375

Epoch: 6| Step: 3
Training loss: 2.522500514984131
Validation loss: 2.9331221183141074

Epoch: 6| Step: 4
Training loss: 3.3440921306610107
Validation loss: 2.929581602414449

Epoch: 6| Step: 5
Training loss: 2.721731185913086
Validation loss: 2.926483472188314

Epoch: 6| Step: 6
Training loss: 2.741729259490967
Validation loss: 2.923559288183848

Epoch: 6| Step: 7
Training loss: 3.337386131286621
Validation loss: 2.920360247294108

Epoch: 6| Step: 8
Training loss: 3.2468109130859375
Validation loss: 2.9185306231180825

Epoch: 6| Step: 9
Training loss: 2.8076579570770264
Validation loss: 2.914649248123169

Epoch: 6| Step: 10
Training loss: 3.7226250171661377
Validation loss: 2.9124091466267905

Epoch: 6| Step: 11
Training loss: 3.101747512817383
Validation loss: 2.910914142926534

Epoch: 6| Step: 12
Training loss: 2.590395927429199
Validation loss: 2.907219489415487

Epoch: 6| Step: 13
Training loss: 3.4969000816345215
Validation loss: 2.9022560119628906

Epoch: 35| Step: 0
Training loss: 2.6088175773620605
Validation loss: 2.898549795150757

Epoch: 6| Step: 1
Training loss: 3.3672738075256348
Validation loss: 2.8965382973353067

Epoch: 6| Step: 2
Training loss: 3.477879047393799
Validation loss: 2.8941170374552407

Epoch: 6| Step: 3
Training loss: 3.0096728801727295
Validation loss: 2.8900871674219766

Epoch: 6| Step: 4
Training loss: 3.3723337650299072
Validation loss: 2.8866878350575766

Epoch: 6| Step: 5
Training loss: 3.799773931503296
Validation loss: 2.8841209411621094

Epoch: 6| Step: 6
Training loss: 3.0515127182006836
Validation loss: 2.8797075748443604

Epoch: 6| Step: 7
Training loss: 2.7730331420898438
Validation loss: 2.876656095186869

Epoch: 6| Step: 8
Training loss: 3.0204267501831055
Validation loss: 2.873290499051412

Epoch: 6| Step: 9
Training loss: 2.3435213565826416
Validation loss: 2.870645006497701

Epoch: 6| Step: 10
Training loss: 3.0693070888519287
Validation loss: 2.867805083592733

Epoch: 6| Step: 11
Training loss: 3.0358660221099854
Validation loss: 2.864896376927694

Epoch: 6| Step: 12
Training loss: 3.2354929447174072
Validation loss: 2.861294945081075

Epoch: 6| Step: 13
Training loss: 3.059995174407959
Validation loss: 2.8586683670679727

Epoch: 36| Step: 0
Training loss: 3.3436412811279297
Validation loss: 2.855270028114319

Epoch: 6| Step: 1
Training loss: 2.4279897212982178
Validation loss: 2.85183048248291

Epoch: 6| Step: 2
Training loss: 2.9035072326660156
Validation loss: 2.8492427666982016

Epoch: 6| Step: 3
Training loss: 3.751493453979492
Validation loss: 2.846116542816162

Epoch: 6| Step: 4
Training loss: 2.5810046195983887
Validation loss: 2.8432374397913613

Epoch: 6| Step: 5
Training loss: 3.6391382217407227
Validation loss: 2.839866836865743

Epoch: 6| Step: 6
Training loss: 3.1409690380096436
Validation loss: 2.837350010871887

Epoch: 6| Step: 7
Training loss: 2.381486415863037
Validation loss: 2.8344010512034097

Epoch: 6| Step: 8
Training loss: 3.1016457080841064
Validation loss: 2.8313492933909097

Epoch: 6| Step: 9
Training loss: 3.0307347774505615
Validation loss: 2.8295706510543823

Epoch: 6| Step: 10
Training loss: 3.171351432800293
Validation loss: 2.826781471570333

Epoch: 6| Step: 11
Training loss: 2.8980371952056885
Validation loss: 2.824294686317444

Epoch: 6| Step: 12
Training loss: 2.8521296977996826
Validation loss: 2.8209492762883506

Epoch: 6| Step: 13
Training loss: 3.4336087703704834
Validation loss: 2.8192686637242637

Epoch: 37| Step: 0
Training loss: 2.9286575317382812
Validation loss: 2.8163474003473916

Epoch: 6| Step: 1
Training loss: 3.1062469482421875
Validation loss: 2.8128484090169272

Epoch: 6| Step: 2
Training loss: 2.90046763420105
Validation loss: 2.8104224602381387

Epoch: 6| Step: 3
Training loss: 3.130476951599121
Validation loss: 2.8055620193481445

Epoch: 6| Step: 4
Training loss: 3.093050479888916
Validation loss: 2.80558447043101

Epoch: 6| Step: 5
Training loss: 2.749079465866089
Validation loss: 2.8043943643569946

Epoch: 6| Step: 6
Training loss: 2.783534288406372
Validation loss: 2.8009003400802612

Epoch: 6| Step: 7
Training loss: 3.452603816986084
Validation loss: 2.801208794116974

Epoch: 6| Step: 8
Training loss: 2.9325873851776123
Validation loss: 2.7945834398269653

Epoch: 6| Step: 9
Training loss: 2.8142030239105225
Validation loss: 2.7907376686731973

Epoch: 6| Step: 10
Training loss: 2.498305320739746
Validation loss: 2.787884831428528

Epoch: 6| Step: 11
Training loss: 3.5429439544677734
Validation loss: 2.785258730252584

Epoch: 6| Step: 12
Training loss: 3.1933207511901855
Validation loss: 2.7837894360224404

Epoch: 6| Step: 13
Training loss: 2.976621150970459
Validation loss: 2.7825234731038413

Epoch: 38| Step: 0
Training loss: 3.6890695095062256
Validation loss: 2.7786794900894165

Epoch: 6| Step: 1
Training loss: 2.546330690383911
Validation loss: 2.7726930379867554

Epoch: 6| Step: 2
Training loss: 3.0840039253234863
Validation loss: 2.770202159881592

Epoch: 6| Step: 3
Training loss: 2.9006237983703613
Validation loss: 2.768155852953593

Epoch: 6| Step: 4
Training loss: 2.8523964881896973
Validation loss: 2.764090577761332

Epoch: 6| Step: 5
Training loss: 2.207529306411743
Validation loss: 2.762253204981486

Epoch: 6| Step: 6
Training loss: 2.6171274185180664
Validation loss: 2.7587780952453613

Epoch: 6| Step: 7
Training loss: 3.018815040588379
Validation loss: 2.757969339688619

Epoch: 6| Step: 8
Training loss: 2.746140956878662
Validation loss: 2.755630850791931

Epoch: 6| Step: 9
Training loss: 2.4991109371185303
Validation loss: 2.7528830766677856

Epoch: 6| Step: 10
Training loss: 3.402086019515991
Validation loss: 2.748641848564148

Epoch: 6| Step: 11
Training loss: 3.04893159866333
Validation loss: 2.747231443723043

Epoch: 6| Step: 12
Training loss: 3.058213710784912
Validation loss: 2.743137001991272

Epoch: 6| Step: 13
Training loss: 3.8914265632629395
Validation loss: 2.7433662017186484

Epoch: 39| Step: 0
Training loss: 3.3804187774658203
Validation loss: 2.7415632208188376

Epoch: 6| Step: 1
Training loss: 3.8453662395477295
Validation loss: 2.7335997422536216

Epoch: 6| Step: 2
Training loss: 2.641791343688965
Validation loss: 2.7319487730662027

Epoch: 6| Step: 3
Training loss: 3.1672492027282715
Validation loss: 2.7292693456014

Epoch: 6| Step: 4
Training loss: 2.7853503227233887
Validation loss: 2.726451317469279

Epoch: 6| Step: 5
Training loss: 2.047950029373169
Validation loss: 2.72296941280365

Epoch: 6| Step: 6
Training loss: 2.494551181793213
Validation loss: 2.7209342320760093

Epoch: 6| Step: 7
Training loss: 2.8748321533203125
Validation loss: 2.724413355191549

Epoch: 6| Step: 8
Training loss: 2.5724334716796875
Validation loss: 2.7231138944625854

Epoch: 6| Step: 9
Training loss: 3.093052387237549
Validation loss: 2.724903325239817

Epoch: 6| Step: 10
Training loss: 2.7138519287109375
Validation loss: 2.7142378886540732

Epoch: 6| Step: 11
Training loss: 2.6421518325805664
Validation loss: 2.705824931462606

Epoch: 6| Step: 12
Training loss: 4.106184005737305
Validation loss: 2.706375082333883

Epoch: 6| Step: 13
Training loss: 2.620863199234009
Validation loss: 2.704187591870626

Epoch: 40| Step: 0
Training loss: 1.6875265836715698
Validation loss: 2.7056167920430503

Epoch: 6| Step: 1
Training loss: 2.8494744300842285
Validation loss: 2.707116643587748

Epoch: 6| Step: 2
Training loss: 2.6288821697235107
Validation loss: 2.7051134506861367

Epoch: 6| Step: 3
Training loss: 2.4073493480682373
Validation loss: 2.7023033698399863

Epoch: 6| Step: 4
Training loss: 2.7349963188171387
Validation loss: 2.694963256518046

Epoch: 6| Step: 5
Training loss: 3.0708258152008057
Validation loss: 2.6884905894597373

Epoch: 6| Step: 6
Training loss: 3.231203079223633
Validation loss: 2.684961279233297

Epoch: 6| Step: 7
Training loss: 3.101912498474121
Validation loss: 2.6814144055048623

Epoch: 6| Step: 8
Training loss: 2.934083938598633
Validation loss: 2.6779133081436157

Epoch: 6| Step: 9
Training loss: 3.172914505004883
Validation loss: 2.6745618184407554

Epoch: 6| Step: 10
Training loss: 2.9506733417510986
Validation loss: 2.672884782155355

Epoch: 6| Step: 11
Training loss: 3.4439806938171387
Validation loss: 2.6710708936055503

Epoch: 6| Step: 12
Training loss: 3.311375856399536
Validation loss: 2.6711979707082114

Epoch: 6| Step: 13
Training loss: 2.913665771484375
Validation loss: 2.6790688832600913

Epoch: 41| Step: 0
Training loss: 3.162808418273926
Validation loss: 2.6631995836893716

Epoch: 6| Step: 1
Training loss: 2.9907565116882324
Validation loss: 2.6581109364827475

Epoch: 6| Step: 2
Training loss: 2.6777262687683105
Validation loss: 2.654494047164917

Epoch: 6| Step: 3
Training loss: 3.1618714332580566
Validation loss: 2.650939146677653

Epoch: 6| Step: 4
Training loss: 2.3362934589385986
Validation loss: 2.651564081509908

Epoch: 6| Step: 5
Training loss: 3.190481185913086
Validation loss: 2.6500398317972818

Epoch: 6| Step: 6
Training loss: 2.6142325401306152
Validation loss: 2.6481790939966836

Epoch: 6| Step: 7
Training loss: 3.3076186180114746
Validation loss: 2.646465619405111

Epoch: 6| Step: 8
Training loss: 3.0029423236846924
Validation loss: 2.641986608505249

Epoch: 6| Step: 9
Training loss: 3.323776960372925
Validation loss: 2.640408674875895

Epoch: 6| Step: 10
Training loss: 2.819235324859619
Validation loss: 2.636401136716207

Epoch: 6| Step: 11
Training loss: 2.7640626430511475
Validation loss: 2.6323573191960654

Epoch: 6| Step: 12
Training loss: 3.055845022201538
Validation loss: 2.630666136741638

Epoch: 6| Step: 13
Training loss: 1.449326992034912
Validation loss: 2.624374429384867

Epoch: 42| Step: 0
Training loss: 2.6983845233917236
Validation loss: 2.6226771672566733

Epoch: 6| Step: 1
Training loss: 2.9050729274749756
Validation loss: 2.6207187175750732

Epoch: 6| Step: 2
Training loss: 2.8164095878601074
Validation loss: 2.6206831534703574

Epoch: 6| Step: 3
Training loss: 2.780658721923828
Validation loss: 2.6225339571634927

Epoch: 6| Step: 4
Training loss: 2.414123058319092
Validation loss: 2.641189932823181

Epoch: 6| Step: 5
Training loss: 2.959746837615967
Validation loss: 2.6335754791895547

Epoch: 6| Step: 6
Training loss: 3.0636281967163086
Validation loss: 2.6064924001693726

Epoch: 6| Step: 7
Training loss: 3.0104575157165527
Validation loss: 2.602813204129537

Epoch: 6| Step: 8
Training loss: 2.7940139770507812
Validation loss: 2.598350723584493

Epoch: 6| Step: 9
Training loss: 3.546160936355591
Validation loss: 2.5983897844950357

Epoch: 6| Step: 10
Training loss: 2.613866090774536
Validation loss: 2.5947994788487754

Epoch: 6| Step: 11
Training loss: 2.5707364082336426
Validation loss: 2.591403603553772

Epoch: 6| Step: 12
Training loss: 2.4222829341888428
Validation loss: 2.590677777926127

Epoch: 6| Step: 13
Training loss: 2.711003303527832
Validation loss: 2.5866243044535318

Epoch: 43| Step: 0
Training loss: 2.702658176422119
Validation loss: 2.58844264348348

Epoch: 6| Step: 1
Training loss: 3.347261428833008
Validation loss: 2.584372560183207

Epoch: 6| Step: 2
Training loss: 3.2166738510131836
Validation loss: 2.578450640042623

Epoch: 6| Step: 3
Training loss: 2.724515914916992
Validation loss: 2.5739989082018533

Epoch: 6| Step: 4
Training loss: 2.570369005203247
Validation loss: 2.569834351539612

Epoch: 6| Step: 5
Training loss: 2.634066343307495
Validation loss: 2.568933685620626

Epoch: 6| Step: 6
Training loss: 3.2575416564941406
Validation loss: 2.56773312886556

Epoch: 6| Step: 7
Training loss: 2.8331780433654785
Validation loss: 2.5653381745020547

Epoch: 6| Step: 8
Training loss: 2.5458807945251465
Validation loss: 2.5622569719950357

Epoch: 6| Step: 9
Training loss: 2.3192734718322754
Validation loss: 2.5662890672683716

Epoch: 6| Step: 10
Training loss: 2.730170726776123
Validation loss: 2.572845379511515

Epoch: 6| Step: 11
Training loss: 2.7932822704315186
Validation loss: 2.552550792694092

Epoch: 6| Step: 12
Training loss: 2.200385570526123
Validation loss: 2.5504448413848877

Epoch: 6| Step: 13
Training loss: 2.9128870964050293
Validation loss: 2.548333684603373

Epoch: 44| Step: 0
Training loss: 2.3318986892700195
Validation loss: 2.544346332550049

Epoch: 6| Step: 1
Training loss: 2.5140628814697266
Validation loss: 2.5442959864934287

Epoch: 6| Step: 2
Training loss: 2.97487211227417
Validation loss: 2.543857137362162

Epoch: 6| Step: 3
Training loss: 3.6851181983947754
Validation loss: 2.5439961552619934

Epoch: 6| Step: 4
Training loss: 2.979231834411621
Validation loss: 2.5379284620285034

Epoch: 6| Step: 5
Training loss: 2.9116809368133545
Validation loss: 2.5383584101994834

Epoch: 6| Step: 6
Training loss: 3.3029661178588867
Validation loss: 2.533183773358663

Epoch: 6| Step: 7
Training loss: 2.37988543510437
Validation loss: 2.5281241138776145

Epoch: 6| Step: 8
Training loss: 2.5593173503875732
Validation loss: 2.5262664953867593

Epoch: 6| Step: 9
Training loss: 2.2458062171936035
Validation loss: 2.5234738190968833

Epoch: 6| Step: 10
Training loss: 2.166020631790161
Validation loss: 2.5203930338223777

Epoch: 6| Step: 11
Training loss: 2.266364574432373
Validation loss: 2.520336469014486

Epoch: 6| Step: 12
Training loss: 2.7446186542510986
Validation loss: 2.516664505004883

Epoch: 6| Step: 13
Training loss: 3.130366802215576
Validation loss: 2.5144458611806235

Epoch: 45| Step: 0
Training loss: 2.6550025939941406
Validation loss: 2.5130929152170816

Epoch: 6| Step: 1
Training loss: 3.186091661453247
Validation loss: 2.5186840097109475

Epoch: 6| Step: 2
Training loss: 2.587958335876465
Validation loss: 2.523369828859965

Epoch: 6| Step: 3
Training loss: 3.051298141479492
Validation loss: 2.52694042523702

Epoch: 6| Step: 4
Training loss: 2.9322495460510254
Validation loss: 2.5216822226842246

Epoch: 6| Step: 5
Training loss: 2.6439902782440186
Validation loss: 2.511419693628947

Epoch: 6| Step: 6
Training loss: 2.3492813110351562
Validation loss: 2.5000957250595093

Epoch: 6| Step: 7
Training loss: 2.912050485610962
Validation loss: 2.4933574199676514

Epoch: 6| Step: 8
Training loss: 2.865756034851074
Validation loss: 2.497073849042257

Epoch: 6| Step: 9
Training loss: 2.9196078777313232
Validation loss: 2.4983083804448447

Epoch: 6| Step: 10
Training loss: 2.792414426803589
Validation loss: 2.496467332045237

Epoch: 6| Step: 11
Training loss: 1.945246934890747
Validation loss: 2.4964812994003296

Epoch: 6| Step: 12
Training loss: 2.281812906265259
Validation loss: 2.495135545730591

Epoch: 6| Step: 13
Training loss: 2.681736469268799
Validation loss: 2.494388779004415

Epoch: 46| Step: 0
Training loss: 3.1781530380249023
Validation loss: 2.4883305430412292

Epoch: 6| Step: 1
Training loss: 2.7427215576171875
Validation loss: 2.4830084840456643

Epoch: 6| Step: 2
Training loss: 2.5990586280822754
Validation loss: 2.473606983820597

Epoch: 6| Step: 3
Training loss: 2.74474835395813
Validation loss: 2.471263845761617

Epoch: 6| Step: 4
Training loss: 2.662646770477295
Validation loss: 2.4664076566696167

Epoch: 6| Step: 5
Training loss: 3.01531982421875
Validation loss: 2.4631322820981345

Epoch: 6| Step: 6
Training loss: 2.3999390602111816
Validation loss: 2.462677240371704

Epoch: 6| Step: 7
Training loss: 3.000424861907959
Validation loss: 2.4559617042541504

Epoch: 6| Step: 8
Training loss: 2.0910463333129883
Validation loss: 2.457333207130432

Epoch: 6| Step: 9
Training loss: 2.5954525470733643
Validation loss: 2.4520758390426636

Epoch: 6| Step: 10
Training loss: 1.7903392314910889
Validation loss: 2.449373960494995

Epoch: 6| Step: 11
Training loss: 3.115906238555908
Validation loss: 2.450205683708191

Epoch: 6| Step: 12
Training loss: 2.8719329833984375
Validation loss: 2.446111023426056

Epoch: 6| Step: 13
Training loss: 2.333120346069336
Validation loss: 2.442000448703766

Epoch: 47| Step: 0
Training loss: 2.9717540740966797
Validation loss: 2.4426029920578003

Epoch: 6| Step: 1
Training loss: 2.5043680667877197
Validation loss: 2.4370272556940713

Epoch: 6| Step: 2
Training loss: 2.3712220191955566
Validation loss: 2.434343775113424

Epoch: 6| Step: 3
Training loss: 2.5436081886291504
Validation loss: 2.433019439379374

Epoch: 6| Step: 4
Training loss: 2.1671533584594727
Validation loss: 2.4299917618433633

Epoch: 6| Step: 5
Training loss: 2.8393542766571045
Validation loss: 2.4301536877950034

Epoch: 6| Step: 6
Training loss: 3.1741397380828857
Validation loss: 2.4272812604904175

Epoch: 6| Step: 7
Training loss: 2.0789334774017334
Validation loss: 2.429317851861318

Epoch: 6| Step: 8
Training loss: 2.2286972999572754
Validation loss: 2.428324580192566

Epoch: 6| Step: 9
Training loss: 3.1341805458068848
Validation loss: 2.42553702990214

Epoch: 6| Step: 10
Training loss: 2.323629856109619
Validation loss: 2.4194527864456177

Epoch: 6| Step: 11
Training loss: 3.1291158199310303
Validation loss: 2.414324680964152

Epoch: 6| Step: 12
Training loss: 2.6086184978485107
Validation loss: 2.4110864400863647

Epoch: 6| Step: 13
Training loss: 2.529372215270996
Validation loss: 2.4075164794921875

Epoch: 48| Step: 0
Training loss: 2.3809847831726074
Validation loss: 2.4038469990094504

Epoch: 6| Step: 1
Training loss: 2.200857162475586
Validation loss: 2.404031674067179

Epoch: 6| Step: 2
Training loss: 2.570453643798828
Validation loss: 2.4004972775777182

Epoch: 6| Step: 3
Training loss: 2.8871545791625977
Validation loss: 2.398243546485901

Epoch: 6| Step: 4
Training loss: 3.110304117202759
Validation loss: 2.39531147480011

Epoch: 6| Step: 5
Training loss: 2.589576005935669
Validation loss: 2.397381762663523

Epoch: 6| Step: 6
Training loss: 2.5924108028411865
Validation loss: 2.3908302386601767

Epoch: 6| Step: 7
Training loss: 2.959528684616089
Validation loss: 2.3893054922421775

Epoch: 6| Step: 8
Training loss: 2.352252721786499
Validation loss: 2.386028250058492

Epoch: 6| Step: 9
Training loss: 2.537351608276367
Validation loss: 2.3850029508272805

Epoch: 6| Step: 10
Training loss: 2.8965377807617188
Validation loss: 2.380261460940043

Epoch: 6| Step: 11
Training loss: 2.3146274089813232
Validation loss: 2.380769451459249

Epoch: 6| Step: 12
Training loss: 2.1333985328674316
Validation loss: 2.376166045665741

Epoch: 6| Step: 13
Training loss: 2.4770028591156006
Validation loss: 2.3762492140134177

Epoch: 49| Step: 0
Training loss: 2.348395824432373
Validation loss: 2.370686630407969

Epoch: 6| Step: 1
Training loss: 2.8402490615844727
Validation loss: 2.3715354204177856

Epoch: 6| Step: 2
Training loss: 2.390869617462158
Validation loss: 2.3685003519058228

Epoch: 6| Step: 3
Training loss: 2.9360690116882324
Validation loss: 2.3693466583887735

Epoch: 6| Step: 4
Training loss: 2.2125113010406494
Validation loss: 2.368980606396993

Epoch: 6| Step: 5
Training loss: 3.007563591003418
Validation loss: 2.3734606901804605

Epoch: 6| Step: 6
Training loss: 2.6772961616516113
Validation loss: 2.373550375302633

Epoch: 6| Step: 7
Training loss: 2.4371862411499023
Validation loss: 2.3643714984258017

Epoch: 6| Step: 8
Training loss: 2.6951138973236084
Validation loss: 2.356062134106954

Epoch: 6| Step: 9
Training loss: 2.055837631225586
Validation loss: 2.354912499586741

Epoch: 6| Step: 10
Training loss: 1.7438783645629883
Validation loss: 2.351697087287903

Epoch: 6| Step: 11
Training loss: 2.82137131690979
Validation loss: 2.3516599337259927

Epoch: 6| Step: 12
Training loss: 2.9945690631866455
Validation loss: 2.352817972501119

Epoch: 6| Step: 13
Training loss: 2.525334119796753
Validation loss: 2.3549662033716836

Epoch: 50| Step: 0
Training loss: 2.773754596710205
Validation loss: 2.3517472545305886

Epoch: 6| Step: 1
Training loss: 2.903501272201538
Validation loss: 2.349357545375824

Epoch: 6| Step: 2
Training loss: 2.2428808212280273
Validation loss: 2.346125920613607

Epoch: 6| Step: 3
Training loss: 3.041863441467285
Validation loss: 2.3401137590408325

Epoch: 6| Step: 4
Training loss: 2.364168882369995
Validation loss: 2.3372201919555664

Epoch: 6| Step: 5
Training loss: 2.3160624504089355
Validation loss: 2.335389415423075

Epoch: 6| Step: 6
Training loss: 2.804431915283203
Validation loss: 2.3346670667330423

Epoch: 6| Step: 7
Training loss: 2.011122703552246
Validation loss: 2.331499536832174

Epoch: 6| Step: 8
Training loss: 2.5897912979125977
Validation loss: 2.326081474622091

Epoch: 6| Step: 9
Training loss: 2.399334669113159
Validation loss: 2.3232068618138633

Epoch: 6| Step: 10
Training loss: 2.326108455657959
Validation loss: 2.3216578165690103

Epoch: 6| Step: 11
Training loss: 2.7191343307495117
Validation loss: 2.3248046239217124

Epoch: 6| Step: 12
Training loss: 1.8085362911224365
Validation loss: 2.320713520050049

Epoch: 6| Step: 13
Training loss: 2.8240137100219727
Validation loss: 2.3143443663915

Epoch: 51| Step: 0
Training loss: 2.616894245147705
Validation loss: 2.3161679903666177

Epoch: 6| Step: 1
Training loss: 2.7221364974975586
Validation loss: 2.3048569758733115

Epoch: 6| Step: 2
Training loss: 2.9329802989959717
Validation loss: 2.3057315150896707

Epoch: 6| Step: 3
Training loss: 2.790632724761963
Validation loss: 2.3069783449172974

Epoch: 6| Step: 4
Training loss: 2.494032382965088
Validation loss: 2.303956170876821

Epoch: 6| Step: 5
Training loss: 2.0201447010040283
Validation loss: 2.3024598360061646

Epoch: 6| Step: 6
Training loss: 2.675607442855835
Validation loss: 2.2983856201171875

Epoch: 6| Step: 7
Training loss: 2.593280076980591
Validation loss: 2.296152194341024

Epoch: 6| Step: 8
Training loss: 2.1363444328308105
Validation loss: 2.298404018084208

Epoch: 6| Step: 9
Training loss: 1.6291496753692627
Validation loss: 2.2995110750198364

Epoch: 6| Step: 10
Training loss: 2.3031930923461914
Validation loss: 2.309536655743917

Epoch: 6| Step: 11
Training loss: 2.9721667766571045
Validation loss: 2.3097750345865884

Epoch: 6| Step: 12
Training loss: 2.450742483139038
Validation loss: 2.3025614817937217

Epoch: 6| Step: 13
Training loss: 2.3598480224609375
Validation loss: 2.2988628149032593

Epoch: 52| Step: 0
Training loss: 2.493312120437622
Validation loss: 2.3018582264582315

Epoch: 6| Step: 1
Training loss: 1.7672455310821533
Validation loss: 2.289435068766276

Epoch: 6| Step: 2
Training loss: 2.1513376235961914
Validation loss: 2.2773912946383157

Epoch: 6| Step: 3
Training loss: 2.7380173206329346
Validation loss: 2.279755473136902

Epoch: 6| Step: 4
Training loss: 2.4191970825195312
Validation loss: 2.28008379538854

Epoch: 6| Step: 5
Training loss: 1.8819068670272827
Validation loss: 2.280768613020579

Epoch: 6| Step: 6
Training loss: 2.4699604511260986
Validation loss: 2.2759236892064414

Epoch: 6| Step: 7
Training loss: 2.6209397315979004
Validation loss: 2.2747654914855957

Epoch: 6| Step: 8
Training loss: 2.758575201034546
Validation loss: 2.27610445022583

Epoch: 6| Step: 9
Training loss: 2.7751097679138184
Validation loss: 2.2716505924860635

Epoch: 6| Step: 10
Training loss: 2.511669158935547
Validation loss: 2.2666324178377786

Epoch: 6| Step: 11
Training loss: 2.652597427368164
Validation loss: 2.268447915712992

Epoch: 6| Step: 12
Training loss: 2.7092700004577637
Validation loss: 2.2654349406560264

Epoch: 6| Step: 13
Training loss: 2.2199509143829346
Validation loss: 2.2649973233540854

Epoch: 53| Step: 0
Training loss: 2.074700355529785
Validation loss: 2.262085517247518

Epoch: 6| Step: 1
Training loss: 2.343287706375122
Validation loss: 2.258946975072225

Epoch: 6| Step: 2
Training loss: 2.2597103118896484
Validation loss: 2.2511715094248452

Epoch: 6| Step: 3
Training loss: 2.294722318649292
Validation loss: 2.2477552692095437

Epoch: 6| Step: 4
Training loss: 2.2725131511688232
Validation loss: 2.247443437576294

Epoch: 6| Step: 5
Training loss: 2.582939624786377
Validation loss: 2.2396419445673623

Epoch: 6| Step: 6
Training loss: 2.4738597869873047
Validation loss: 2.2416424552599588

Epoch: 6| Step: 7
Training loss: 2.114072322845459
Validation loss: 2.237709879875183

Epoch: 6| Step: 8
Training loss: 2.21237850189209
Validation loss: 2.2374232610066733

Epoch: 6| Step: 9
Training loss: 2.749265670776367
Validation loss: 2.237731456756592

Epoch: 6| Step: 10
Training loss: 2.4859108924865723
Validation loss: 2.2353851993878684

Epoch: 6| Step: 11
Training loss: 2.7520530223846436
Validation loss: 2.2365458607673645

Epoch: 6| Step: 12
Training loss: 2.7225048542022705
Validation loss: 2.2294280926386514

Epoch: 6| Step: 13
Training loss: 2.3388848304748535
Validation loss: 2.2292901277542114

Epoch: 54| Step: 0
Training loss: 2.754295825958252
Validation loss: 2.225702921549479

Epoch: 6| Step: 1
Training loss: 2.329824447631836
Validation loss: 2.2217496633529663

Epoch: 6| Step: 2
Training loss: 3.1200435161590576
Validation loss: 2.2183815638224282

Epoch: 6| Step: 3
Training loss: 2.0589241981506348
Validation loss: 2.217943032582601

Epoch: 6| Step: 4
Training loss: 2.0046920776367188
Validation loss: 2.213474690914154

Epoch: 6| Step: 5
Training loss: 2.198489189147949
Validation loss: 2.212449371814728

Epoch: 6| Step: 6
Training loss: 1.7140296697616577
Validation loss: 2.2098922729492188

Epoch: 6| Step: 7
Training loss: 2.3898422718048096
Validation loss: 2.211040516694387

Epoch: 6| Step: 8
Training loss: 2.501835346221924
Validation loss: 2.211321254571279

Epoch: 6| Step: 9
Training loss: 3.088365316390991
Validation loss: 2.221064329147339

Epoch: 6| Step: 10
Training loss: 2.715127944946289
Validation loss: 2.2233811815579734

Epoch: 6| Step: 11
Training loss: 2.263503074645996
Validation loss: 2.2102782130241394

Epoch: 6| Step: 12
Training loss: 2.0587897300720215
Validation loss: 2.208715498447418

Epoch: 6| Step: 13
Training loss: 2.060194730758667
Validation loss: 2.2103976806004844

Epoch: 55| Step: 0
Training loss: 2.5615859031677246
Validation loss: 2.203817288080851

Epoch: 6| Step: 1
Training loss: 2.2711596488952637
Validation loss: 2.192803223927816

Epoch: 6| Step: 2
Training loss: 2.0252327919006348
Validation loss: 2.19193567832311

Epoch: 6| Step: 3
Training loss: 1.9676673412322998
Validation loss: 2.189742604891459

Epoch: 6| Step: 4
Training loss: 2.338020086288452
Validation loss: 2.1894688804944358

Epoch: 6| Step: 5
Training loss: 2.2200417518615723
Validation loss: 2.1889224847157798

Epoch: 6| Step: 6
Training loss: 2.697169542312622
Validation loss: 2.1875283122062683

Epoch: 6| Step: 7
Training loss: 2.3406426906585693
Validation loss: 2.1802322467168174

Epoch: 6| Step: 8
Training loss: 2.640247344970703
Validation loss: 2.187929014364878

Epoch: 6| Step: 9
Training loss: 2.5927531719207764
Validation loss: 2.1858889857927957

Epoch: 6| Step: 10
Training loss: 1.7195974588394165
Validation loss: 2.1818883220354715

Epoch: 6| Step: 11
Training loss: 3.004824638366699
Validation loss: 2.181515614191691

Epoch: 6| Step: 12
Training loss: 2.4046247005462646
Validation loss: 2.1783002614974976

Epoch: 6| Step: 13
Training loss: 2.1356520652770996
Validation loss: 2.1765666802724204

Epoch: 56| Step: 0
Training loss: 2.049776077270508
Validation loss: 2.1728890538215637

Epoch: 6| Step: 1
Training loss: 1.8655357360839844
Validation loss: 2.1758795579274497

Epoch: 6| Step: 2
Training loss: 2.2825145721435547
Validation loss: 2.1740445494651794

Epoch: 6| Step: 3
Training loss: 2.694965362548828
Validation loss: 2.1738823652267456

Epoch: 6| Step: 4
Training loss: 2.195560932159424
Validation loss: 2.1741814414660134

Epoch: 6| Step: 5
Training loss: 2.4607248306274414
Validation loss: 2.173102617263794

Epoch: 6| Step: 6
Training loss: 2.0461597442626953
Validation loss: 2.1727243661880493

Epoch: 6| Step: 7
Training loss: 2.7647743225097656
Validation loss: 2.1719192067782083

Epoch: 6| Step: 8
Training loss: 2.9056529998779297
Validation loss: 2.170268456141154

Epoch: 6| Step: 9
Training loss: 2.6580944061279297
Validation loss: 2.167405923207601

Epoch: 6| Step: 10
Training loss: 2.213545322418213
Validation loss: 2.168084998925527

Epoch: 6| Step: 11
Training loss: 2.4395456314086914
Validation loss: 2.1617253621419272

Epoch: 6| Step: 12
Training loss: 2.24265456199646
Validation loss: 2.1573656797409058

Epoch: 6| Step: 13
Training loss: 1.7305057048797607
Validation loss: 2.159690578778585

Epoch: 57| Step: 0
Training loss: 1.997946858406067
Validation loss: 2.1564459005991616

Epoch: 6| Step: 1
Training loss: 2.6202926635742188
Validation loss: 2.156256834665934

Epoch: 6| Step: 2
Training loss: 2.379520893096924
Validation loss: 2.1532436410586038

Epoch: 6| Step: 3
Training loss: 2.693617105484009
Validation loss: 2.1539939840634665

Epoch: 6| Step: 4
Training loss: 2.197364091873169
Validation loss: 2.155632734298706

Epoch: 6| Step: 5
Training loss: 1.715407133102417
Validation loss: 2.153372665246328

Epoch: 6| Step: 6
Training loss: 2.417663812637329
Validation loss: 2.1545599897702536

Epoch: 6| Step: 7
Training loss: 2.873228073120117
Validation loss: 2.1538343826929727

Epoch: 6| Step: 8
Training loss: 2.3880341053009033
Validation loss: 2.1481900612513223

Epoch: 6| Step: 9
Training loss: 2.140927791595459
Validation loss: 2.1467025677363076

Epoch: 6| Step: 10
Training loss: 2.177746295928955
Validation loss: 2.1364274422327676

Epoch: 6| Step: 11
Training loss: 2.347972869873047
Validation loss: 2.1438792943954468

Epoch: 6| Step: 12
Training loss: 2.3755006790161133
Validation loss: 2.156892021497091

Epoch: 6| Step: 13
Training loss: 2.0551834106445312
Validation loss: 2.1703126033147178

Epoch: 58| Step: 0
Training loss: 2.037078857421875
Validation loss: 2.1637682914733887

Epoch: 6| Step: 1
Training loss: 2.5524134635925293
Validation loss: 2.1619186798731485

Epoch: 6| Step: 2
Training loss: 2.9334521293640137
Validation loss: 2.1627465089162192

Epoch: 6| Step: 3
Training loss: 2.519085645675659
Validation loss: 2.1646443804105124

Epoch: 6| Step: 4
Training loss: 2.5878872871398926
Validation loss: 2.1547639966011047

Epoch: 6| Step: 5
Training loss: 2.1749107837677
Validation loss: 2.1543205777804055

Epoch: 6| Step: 6
Training loss: 1.810295820236206
Validation loss: 2.1446454524993896

Epoch: 6| Step: 7
Training loss: 2.501711845397949
Validation loss: 2.1458915869394937

Epoch: 6| Step: 8
Training loss: 2.462273597717285
Validation loss: 2.1412848631540933

Epoch: 6| Step: 9
Training loss: 2.4395952224731445
Validation loss: 2.136188586552938

Epoch: 6| Step: 10
Training loss: 1.5613739490509033
Validation loss: 2.1428930163383484

Epoch: 6| Step: 11
Training loss: 2.037564277648926
Validation loss: 2.14365154504776

Epoch: 6| Step: 12
Training loss: 2.596919536590576
Validation loss: 2.1356120308240256

Epoch: 6| Step: 13
Training loss: 2.0368542671203613
Validation loss: 2.131376882394155

Epoch: 59| Step: 0
Training loss: 2.784065008163452
Validation loss: 2.1224043567975364

Epoch: 6| Step: 1
Training loss: 1.958512544631958
Validation loss: 2.1196221113204956

Epoch: 6| Step: 2
Training loss: 2.941192865371704
Validation loss: 2.11544398466746

Epoch: 6| Step: 3
Training loss: 2.31959867477417
Validation loss: 2.1164285937945047

Epoch: 6| Step: 4
Training loss: 2.4118871688842773
Validation loss: 2.114257295926412

Epoch: 6| Step: 5
Training loss: 2.2072978019714355
Validation loss: 2.1151509483655295

Epoch: 6| Step: 6
Training loss: 2.11810040473938
Validation loss: 2.1141538421312966

Epoch: 6| Step: 7
Training loss: 1.8570773601531982
Validation loss: 2.1171327034632363

Epoch: 6| Step: 8
Training loss: 2.4547016620635986
Validation loss: 2.1162983775138855

Epoch: 6| Step: 9
Training loss: 1.8369728326797485
Validation loss: 2.1222835580507913

Epoch: 6| Step: 10
Training loss: 2.0834078788757324
Validation loss: 2.1324301958084106

Epoch: 6| Step: 11
Training loss: 2.1640920639038086
Validation loss: 2.1432767311731973

Epoch: 6| Step: 12
Training loss: 2.2822189331054688
Validation loss: 2.134097377459208

Epoch: 6| Step: 13
Training loss: 2.478883743286133
Validation loss: 2.1220515966415405

Epoch: 60| Step: 0
Training loss: 2.5474796295166016
Validation loss: 2.1168062885602317

Epoch: 6| Step: 1
Training loss: 2.2787601947784424
Validation loss: 2.104986051718394

Epoch: 6| Step: 2
Training loss: 2.3790364265441895
Validation loss: 2.1063625812530518

Epoch: 6| Step: 3
Training loss: 1.5013713836669922
Validation loss: 2.1119509736696878

Epoch: 6| Step: 4
Training loss: 2.0274980068206787
Validation loss: 2.107547700405121

Epoch: 6| Step: 5
Training loss: 2.1805479526519775
Validation loss: 2.111359437306722

Epoch: 6| Step: 6
Training loss: 1.794602632522583
Validation loss: 2.1118940909703574

Epoch: 6| Step: 7
Training loss: 2.238551139831543
Validation loss: 2.1063406467437744

Epoch: 6| Step: 8
Training loss: 2.411137580871582
Validation loss: 2.1000539859135947

Epoch: 6| Step: 9
Training loss: 2.630950927734375
Validation loss: 2.097313384215037

Epoch: 6| Step: 10
Training loss: 1.9919942617416382
Validation loss: 2.096059739589691

Epoch: 6| Step: 11
Training loss: 2.3352136611938477
Validation loss: 2.0958837270736694

Epoch: 6| Step: 12
Training loss: 2.833469867706299
Validation loss: 2.093018631140391

Epoch: 6| Step: 13
Training loss: 2.8260231018066406
Validation loss: 2.101219673951467

Epoch: 61| Step: 0
Training loss: 2.2771387100219727
Validation loss: 2.090722302595774

Epoch: 6| Step: 1
Training loss: 2.136618137359619
Validation loss: 2.0887008905410767

Epoch: 6| Step: 2
Training loss: 2.468010902404785
Validation loss: 2.0878137350082397

Epoch: 6| Step: 3
Training loss: 1.960234522819519
Validation loss: 2.0860888560613

Epoch: 6| Step: 4
Training loss: 2.3252830505371094
Validation loss: 2.0842148661613464

Epoch: 6| Step: 5
Training loss: 2.5451183319091797
Validation loss: 2.085789680480957

Epoch: 6| Step: 6
Training loss: 3.145686626434326
Validation loss: 2.088902731736501

Epoch: 6| Step: 7
Training loss: 2.627469062805176
Validation loss: 2.0789378881454468

Epoch: 6| Step: 8
Training loss: 2.250169515609741
Validation loss: 2.0894123713175454

Epoch: 6| Step: 9
Training loss: 1.4651963710784912
Validation loss: 2.080622752507528

Epoch: 6| Step: 10
Training loss: 1.9959217309951782
Validation loss: 2.085217456022898

Epoch: 6| Step: 11
Training loss: 2.4515726566314697
Validation loss: 2.081476112206777

Epoch: 6| Step: 12
Training loss: 1.3338786363601685
Validation loss: 2.0810407201449075

Epoch: 6| Step: 13
Training loss: 2.4956259727478027
Validation loss: 2.0850576957066855

Epoch: 62| Step: 0
Training loss: 2.4859461784362793
Validation loss: 2.0826716224352517

Epoch: 6| Step: 1
Training loss: 1.826223373413086
Validation loss: 2.0790859858194985

Epoch: 6| Step: 2
Training loss: 2.5693225860595703
Validation loss: 2.0813336968421936

Epoch: 6| Step: 3
Training loss: 2.135448694229126
Validation loss: 2.0790722767512

Epoch: 6| Step: 4
Training loss: 1.7960423231124878
Validation loss: 2.0812466144561768

Epoch: 6| Step: 5
Training loss: 2.0316553115844727
Validation loss: 2.0812360644340515

Epoch: 6| Step: 6
Training loss: 2.424724817276001
Validation loss: 2.0743557612101235

Epoch: 6| Step: 7
Training loss: 2.1816787719726562
Validation loss: 2.0816490650177

Epoch: 6| Step: 8
Training loss: 2.719266653060913
Validation loss: 2.0788018306096396

Epoch: 6| Step: 9
Training loss: 2.265793800354004
Validation loss: 2.076595942179362

Epoch: 6| Step: 10
Training loss: 2.337103843688965
Validation loss: 2.077553470929464

Epoch: 6| Step: 11
Training loss: 2.1241188049316406
Validation loss: 2.0773892203966775

Epoch: 6| Step: 12
Training loss: 2.3905506134033203
Validation loss: 2.0718746185302734

Epoch: 6| Step: 13
Training loss: 2.1873602867126465
Validation loss: 2.07321564356486

Epoch: 63| Step: 0
Training loss: 2.0429811477661133
Validation loss: 2.0676387548446655

Epoch: 6| Step: 1
Training loss: 2.704718589782715
Validation loss: 2.071108599503835

Epoch: 6| Step: 2
Training loss: 2.458998918533325
Validation loss: 2.076329529285431

Epoch: 6| Step: 3
Training loss: 2.739121913909912
Validation loss: 2.0732101996739707

Epoch: 6| Step: 4
Training loss: 2.521768093109131
Validation loss: 2.0889249444007874

Epoch: 6| Step: 5
Training loss: 1.7741937637329102
Validation loss: 2.079204181830088

Epoch: 6| Step: 6
Training loss: 2.7431774139404297
Validation loss: 2.073892831802368

Epoch: 6| Step: 7
Training loss: 1.5898082256317139
Validation loss: 2.074471096197764

Epoch: 6| Step: 8
Training loss: 2.4902117252349854
Validation loss: 2.072614769140879

Epoch: 6| Step: 9
Training loss: 1.3427863121032715
Validation loss: 2.064757764339447

Epoch: 6| Step: 10
Training loss: 2.583603858947754
Validation loss: 2.0633308490117392

Epoch: 6| Step: 11
Training loss: 2.002164840698242
Validation loss: 2.0600523352622986

Epoch: 6| Step: 12
Training loss: 2.1858201026916504
Validation loss: 2.0623846451441445

Epoch: 6| Step: 13
Training loss: 2.10638427734375
Validation loss: 2.062972048918406

Epoch: 64| Step: 0
Training loss: 2.195063352584839
Validation loss: 2.0639299949010215

Epoch: 6| Step: 1
Training loss: 2.776102304458618
Validation loss: 2.0576967000961304

Epoch: 6| Step: 2
Training loss: 2.348228693008423
Validation loss: 2.061741312344869

Epoch: 6| Step: 3
Training loss: 1.8916757106781006
Validation loss: 2.0546415050824485

Epoch: 6| Step: 4
Training loss: 2.013151168823242
Validation loss: 2.055126150449117

Epoch: 6| Step: 5
Training loss: 2.168778896331787
Validation loss: 2.058676620324453

Epoch: 6| Step: 6
Training loss: 2.1734542846679688
Validation loss: 2.059822897116343

Epoch: 6| Step: 7
Training loss: 2.395681381225586
Validation loss: 2.0732580622037253

Epoch: 6| Step: 8
Training loss: 2.8825716972351074
Validation loss: 2.085567613442739

Epoch: 6| Step: 9
Training loss: 2.0485475063323975
Validation loss: 2.0886374513308206

Epoch: 6| Step: 10
Training loss: 2.0992372035980225
Validation loss: 2.1026345690091452

Epoch: 6| Step: 11
Training loss: 1.5514404773712158
Validation loss: 2.088525374730428

Epoch: 6| Step: 12
Training loss: 2.492450714111328
Validation loss: 2.0972079634666443

Epoch: 6| Step: 13
Training loss: 2.3776941299438477
Validation loss: 2.094518462816874

Epoch: 65| Step: 0
Training loss: 2.058716058731079
Validation loss: 2.09441739320755

Epoch: 6| Step: 1
Training loss: 1.6579641103744507
Validation loss: 2.084919015566508

Epoch: 6| Step: 2
Training loss: 2.504939556121826
Validation loss: 2.0812381307284036

Epoch: 6| Step: 3
Training loss: 2.7545218467712402
Validation loss: 2.0846750934918723

Epoch: 6| Step: 4
Training loss: 2.350569725036621
Validation loss: 2.0821352203687034

Epoch: 6| Step: 5
Training loss: 2.5318515300750732
Validation loss: 2.0669804414113364

Epoch: 6| Step: 6
Training loss: 2.5791573524475098
Validation loss: 2.052759110927582

Epoch: 6| Step: 7
Training loss: 1.9110149145126343
Validation loss: 2.0473272800445557

Epoch: 6| Step: 8
Training loss: 1.9624671936035156
Validation loss: 2.0431204636891684

Epoch: 6| Step: 9
Training loss: 2.368393898010254
Validation loss: 2.0593541463216147

Epoch: 6| Step: 10
Training loss: 2.079522132873535
Validation loss: 2.0625655253728232

Epoch: 6| Step: 11
Training loss: 2.0809545516967773
Validation loss: 2.069840967655182

Epoch: 6| Step: 12
Training loss: 2.320261001586914
Validation loss: 2.0719100634256997

Epoch: 6| Step: 13
Training loss: 2.178969383239746
Validation loss: 2.0670145948727927

Epoch: 66| Step: 0
Training loss: 1.9438982009887695
Validation loss: 2.066074709097544

Epoch: 6| Step: 1
Training loss: 2.0136947631835938
Validation loss: 2.0619375109672546

Epoch: 6| Step: 2
Training loss: 1.5303384065628052
Validation loss: 2.0579849680264792

Epoch: 6| Step: 3
Training loss: 2.0605356693267822
Validation loss: 2.054236630598704

Epoch: 6| Step: 4
Training loss: 2.8808541297912598
Validation loss: 2.048602064450582

Epoch: 6| Step: 5
Training loss: 1.799804449081421
Validation loss: 2.036441226800283

Epoch: 6| Step: 6
Training loss: 1.8877259492874146
Validation loss: 2.027558426062266

Epoch: 6| Step: 7
Training loss: 1.743598461151123
Validation loss: 2.033760925134023

Epoch: 6| Step: 8
Training loss: 2.8663227558135986
Validation loss: 2.0480704307556152

Epoch: 6| Step: 9
Training loss: 2.3218979835510254
Validation loss: 2.0463150342305503

Epoch: 6| Step: 10
Training loss: 2.9515113830566406
Validation loss: 2.0528201858202615

Epoch: 6| Step: 11
Training loss: 2.3712711334228516
Validation loss: 2.059067666530609

Epoch: 6| Step: 12
Training loss: 2.325979471206665
Validation loss: 2.057349960009257

Epoch: 6| Step: 13
Training loss: 2.4225196838378906
Validation loss: 2.0624417265256247

Epoch: 67| Step: 0
Training loss: 3.0209579467773438
Validation loss: 2.0520111123720803

Epoch: 6| Step: 1
Training loss: 2.1228301525115967
Validation loss: 2.0535285671552024

Epoch: 6| Step: 2
Training loss: 1.9394034147262573
Validation loss: 2.0507010221481323

Epoch: 6| Step: 3
Training loss: 1.777747631072998
Validation loss: 2.0488802591959634

Epoch: 6| Step: 4
Training loss: 1.93160879611969
Validation loss: 2.0342534383138022

Epoch: 6| Step: 5
Training loss: 1.7311393022537231
Validation loss: 2.035269836584727

Epoch: 6| Step: 6
Training loss: 2.0059890747070312
Validation loss: 2.028448164463043

Epoch: 6| Step: 7
Training loss: 2.466369390487671
Validation loss: 2.029357135295868

Epoch: 6| Step: 8
Training loss: 2.6022801399230957
Validation loss: 2.0286256273587546

Epoch: 6| Step: 9
Training loss: 2.4869933128356934
Validation loss: 2.029070178667704

Epoch: 6| Step: 10
Training loss: 2.2992029190063477
Validation loss: 2.0312911669413247

Epoch: 6| Step: 11
Training loss: 2.639993667602539
Validation loss: 2.0262816349665322

Epoch: 6| Step: 12
Training loss: 2.4131226539611816
Validation loss: 2.0314887364705405

Epoch: 6| Step: 13
Training loss: 1.4458216428756714
Validation loss: 2.0258566538492837

Epoch: 68| Step: 0
Training loss: 2.305898666381836
Validation loss: 2.0242636998494468

Epoch: 6| Step: 1
Training loss: 2.465290069580078
Validation loss: 2.024220824241638

Epoch: 6| Step: 2
Training loss: 2.1072983741760254
Validation loss: 2.0238629380861917

Epoch: 6| Step: 3
Training loss: 2.1296558380126953
Validation loss: 2.022497753302256

Epoch: 6| Step: 4
Training loss: 2.223989486694336
Validation loss: 2.0246447722117105

Epoch: 6| Step: 5
Training loss: 2.392589569091797
Validation loss: 2.0260483423868814

Epoch: 6| Step: 6
Training loss: 2.102553606033325
Validation loss: 2.0335699915885925

Epoch: 6| Step: 7
Training loss: 2.5116586685180664
Validation loss: 2.03293110926946

Epoch: 6| Step: 8
Training loss: 2.0064537525177
Validation loss: 2.0295191605885825

Epoch: 6| Step: 9
Training loss: 2.4133713245391846
Validation loss: 2.0315120617548623

Epoch: 6| Step: 10
Training loss: 2.134676933288574
Validation loss: 2.0208582480748496

Epoch: 6| Step: 11
Training loss: 2.1905195713043213
Validation loss: 2.02366973956426

Epoch: 6| Step: 12
Training loss: 2.4114880561828613
Validation loss: 2.020685096581777

Epoch: 6| Step: 13
Training loss: 1.4083540439605713
Validation loss: 2.021811068058014

Epoch: 69| Step: 0
Training loss: 2.250326156616211
Validation loss: 2.0328580141067505

Epoch: 6| Step: 1
Training loss: 2.322122812271118
Validation loss: 2.044049322605133

Epoch: 6| Step: 2
Training loss: 2.0754525661468506
Validation loss: 2.0479682286580405

Epoch: 6| Step: 3
Training loss: 2.380308151245117
Validation loss: 2.05992990732193

Epoch: 6| Step: 4
Training loss: 2.484492778778076
Validation loss: 2.0504093964894614

Epoch: 6| Step: 5
Training loss: 1.8104479312896729
Validation loss: 2.0538890957832336

Epoch: 6| Step: 6
Training loss: 2.196528434753418
Validation loss: 2.0571548541386924

Epoch: 6| Step: 7
Training loss: 2.320788860321045
Validation loss: 2.0410922368367515

Epoch: 6| Step: 8
Training loss: 2.501704454421997
Validation loss: 2.0406605998675027

Epoch: 6| Step: 9
Training loss: 2.2070751190185547
Validation loss: 2.0432623426119485

Epoch: 6| Step: 10
Training loss: 1.6576590538024902
Validation loss: 2.042073607444763

Epoch: 6| Step: 11
Training loss: 2.471233367919922
Validation loss: 2.038358151912689

Epoch: 6| Step: 12
Training loss: 1.5027453899383545
Validation loss: 2.033660372098287

Epoch: 6| Step: 13
Training loss: 2.6022777557373047
Validation loss: 2.0358198483784995

Epoch: 70| Step: 0
Training loss: 2.237293243408203
Validation loss: 2.0359354813893638

Epoch: 6| Step: 1
Training loss: 1.9173988103866577
Validation loss: 2.033781091372172

Epoch: 6| Step: 2
Training loss: 1.7389624118804932
Validation loss: 2.035384019215902

Epoch: 6| Step: 3
Training loss: 2.544835090637207
Validation loss: 2.039938489596049

Epoch: 6| Step: 4
Training loss: 2.05595064163208
Validation loss: 2.04103030761083

Epoch: 6| Step: 5
Training loss: 2.697741985321045
Validation loss: 2.0307856599489846

Epoch: 6| Step: 6
Training loss: 2.482931613922119
Validation loss: 2.035150190194448

Epoch: 6| Step: 7
Training loss: 2.603996753692627
Validation loss: 2.0369288126627603

Epoch: 6| Step: 8
Training loss: 1.4483661651611328
Validation loss: 2.036379873752594

Epoch: 6| Step: 9
Training loss: 2.2667593955993652
Validation loss: 2.039513866106669

Epoch: 6| Step: 10
Training loss: 2.33199405670166
Validation loss: 2.0362378358840942

Epoch: 6| Step: 11
Training loss: 2.441763162612915
Validation loss: 2.02669366200765

Epoch: 6| Step: 12
Training loss: 1.8736785650253296
Validation loss: 2.026219367980957

Epoch: 6| Step: 13
Training loss: 2.1431221961975098
Validation loss: 2.0336502393086753

Epoch: 71| Step: 0
Training loss: 2.330660104751587
Validation loss: 2.0305042465527854

Epoch: 6| Step: 1
Training loss: 1.9630556106567383
Validation loss: 2.0259540677070618

Epoch: 6| Step: 2
Training loss: 2.078854560852051
Validation loss: 2.027341345945994

Epoch: 6| Step: 3
Training loss: 1.5026261806488037
Validation loss: 2.021505912144979

Epoch: 6| Step: 4
Training loss: 2.6039528846740723
Validation loss: 2.025502920150757

Epoch: 6| Step: 5
Training loss: 1.7460120916366577
Validation loss: 2.0184195240338645

Epoch: 6| Step: 6
Training loss: 2.185713291168213
Validation loss: 2.0245308876037598

Epoch: 6| Step: 7
Training loss: 2.295513153076172
Validation loss: 2.023913582166036

Epoch: 6| Step: 8
Training loss: 2.328165054321289
Validation loss: 2.029699365297953

Epoch: 6| Step: 9
Training loss: 1.8438162803649902
Validation loss: 2.036685049533844

Epoch: 6| Step: 10
Training loss: 2.933471202850342
Validation loss: 2.05210417509079

Epoch: 6| Step: 11
Training loss: 2.5393683910369873
Validation loss: 2.0613895257314048

Epoch: 6| Step: 12
Training loss: 2.3214809894561768
Validation loss: 2.0717759927113852

Epoch: 6| Step: 13
Training loss: 2.007840871810913
Validation loss: 2.0733764370282493

Epoch: 72| Step: 0
Training loss: 1.9882259368896484
Validation loss: 2.0783053636550903

Epoch: 6| Step: 1
Training loss: 1.9564040899276733
Validation loss: 2.0786073009173074

Epoch: 6| Step: 2
Training loss: 2.013312816619873
Validation loss: 2.0690743923187256

Epoch: 6| Step: 3
Training loss: 2.0136656761169434
Validation loss: 2.0622904300689697

Epoch: 6| Step: 4
Training loss: 2.64935302734375
Validation loss: 2.047258814175924

Epoch: 6| Step: 5
Training loss: 2.4896955490112305
Validation loss: 2.033209522565206

Epoch: 6| Step: 6
Training loss: 2.1169111728668213
Validation loss: 2.0261927247047424

Epoch: 6| Step: 7
Training loss: 2.148005485534668
Validation loss: 2.0181387066841125

Epoch: 6| Step: 8
Training loss: 2.323852062225342
Validation loss: 2.0142406026522317

Epoch: 6| Step: 9
Training loss: 2.7258851528167725
Validation loss: 2.0235400597254434

Epoch: 6| Step: 10
Training loss: 1.9789068698883057
Validation loss: 2.023123542467753

Epoch: 6| Step: 11
Training loss: 1.5037682056427002
Validation loss: 2.019417464733124

Epoch: 6| Step: 12
Training loss: 2.5544772148132324
Validation loss: 2.024227817853292

Epoch: 6| Step: 13
Training loss: 2.405761241912842
Validation loss: 2.022094984849294

Epoch: 73| Step: 0
Training loss: 2.305266857147217
Validation loss: 2.023808002471924

Epoch: 6| Step: 1
Training loss: 2.853971004486084
Validation loss: 2.0229912400245667

Epoch: 6| Step: 2
Training loss: 1.8850188255310059
Validation loss: 2.019087533156077

Epoch: 6| Step: 3
Training loss: 2.435879707336426
Validation loss: 2.022784650325775

Epoch: 6| Step: 4
Training loss: 1.7044034004211426
Validation loss: 2.0242658058802285

Epoch: 6| Step: 5
Training loss: 2.2722039222717285
Validation loss: 2.028222620487213

Epoch: 6| Step: 6
Training loss: 2.3449127674102783
Validation loss: 2.033949096997579

Epoch: 6| Step: 7
Training loss: 1.9581574201583862
Validation loss: 2.0362444718678794

Epoch: 6| Step: 8
Training loss: 2.1528873443603516
Validation loss: 2.044913868109385

Epoch: 6| Step: 9
Training loss: 2.2848081588745117
Validation loss: 2.040113071600596

Epoch: 6| Step: 10
Training loss: 2.0699331760406494
Validation loss: 2.043930470943451

Epoch: 6| Step: 11
Training loss: 2.7965731620788574
Validation loss: 2.056837320327759

Epoch: 6| Step: 12
Training loss: 1.6852399110794067
Validation loss: 2.043809254964193

Epoch: 6| Step: 13
Training loss: 1.8507907390594482
Validation loss: 2.0278181632359824

Epoch: 74| Step: 0
Training loss: 2.4703311920166016
Validation loss: 2.035555044809977

Epoch: 6| Step: 1
Training loss: 1.9441009759902954
Validation loss: 2.0319997668266296

Epoch: 6| Step: 2
Training loss: 2.103041648864746
Validation loss: 2.024492621421814

Epoch: 6| Step: 3
Training loss: 2.019070625305176
Validation loss: 2.0148845116297402

Epoch: 6| Step: 4
Training loss: 2.4474551677703857
Validation loss: 2.017963627974192

Epoch: 6| Step: 5
Training loss: 1.8363103866577148
Validation loss: 2.0116563638051352

Epoch: 6| Step: 6
Training loss: 2.5300564765930176
Validation loss: 2.0136115749677024

Epoch: 6| Step: 7
Training loss: 2.1717724800109863
Validation loss: 2.016478180885315

Epoch: 6| Step: 8
Training loss: 2.286895513534546
Validation loss: 2.0136019190152488

Epoch: 6| Step: 9
Training loss: 1.9520107507705688
Validation loss: 2.0146469473838806

Epoch: 6| Step: 10
Training loss: 1.9267590045928955
Validation loss: 2.013756275177002

Epoch: 6| Step: 11
Training loss: 2.079716920852661
Validation loss: 2.014577110608419

Epoch: 6| Step: 12
Training loss: 2.1233608722686768
Validation loss: 2.017709195613861

Epoch: 6| Step: 13
Training loss: 2.660095691680908
Validation loss: 2.013459265232086

Epoch: 75| Step: 0
Training loss: 2.0489389896392822
Validation loss: 2.0126380721728006

Epoch: 6| Step: 1
Training loss: 2.720712184906006
Validation loss: 2.0185458858807883

Epoch: 6| Step: 2
Training loss: 1.4623258113861084
Validation loss: 2.0195076068242392

Epoch: 6| Step: 3
Training loss: 1.8899574279785156
Validation loss: 2.0278632640838623

Epoch: 6| Step: 4
Training loss: 2.086972236633301
Validation loss: 2.030748724937439

Epoch: 6| Step: 5
Training loss: 2.4971799850463867
Validation loss: 2.036706348260244

Epoch: 6| Step: 6
Training loss: 2.298780679702759
Validation loss: 2.032389978567759

Epoch: 6| Step: 7
Training loss: 2.5647449493408203
Validation loss: 2.0354389349619546

Epoch: 6| Step: 8
Training loss: 2.8572158813476562
Validation loss: 2.03493462006251

Epoch: 6| Step: 9
Training loss: 1.5994148254394531
Validation loss: 2.0296617547671

Epoch: 6| Step: 10
Training loss: 2.0095314979553223
Validation loss: 2.024515072504679

Epoch: 6| Step: 11
Training loss: 2.571145534515381
Validation loss: 2.0252320369084678

Epoch: 6| Step: 12
Training loss: 2.0165960788726807
Validation loss: 2.0165414611498513

Epoch: 6| Step: 13
Training loss: 1.9568570852279663
Validation loss: 2.0219695568084717

Epoch: 76| Step: 0
Training loss: 2.0972673892974854
Validation loss: 2.022601226965586

Epoch: 6| Step: 1
Training loss: 1.8478708267211914
Validation loss: 2.0195236206054688

Epoch: 6| Step: 2
Training loss: 2.2894961833953857
Validation loss: 2.0172559022903442

Epoch: 6| Step: 3
Training loss: 2.0518879890441895
Validation loss: 2.0139684677124023

Epoch: 6| Step: 4
Training loss: 2.187857151031494
Validation loss: 2.0193191170692444

Epoch: 6| Step: 5
Training loss: 2.8180129528045654
Validation loss: 2.0146127740542092

Epoch: 6| Step: 6
Training loss: 2.3488478660583496
Validation loss: 2.018958806991577

Epoch: 6| Step: 7
Training loss: 2.0972390174865723
Validation loss: 2.0191337863604226

Epoch: 6| Step: 8
Training loss: 1.9155077934265137
Validation loss: 2.0218506852785745

Epoch: 6| Step: 9
Training loss: 2.197187900543213
Validation loss: 2.02008726199468

Epoch: 6| Step: 10
Training loss: 2.048224687576294
Validation loss: 2.0240790843963623

Epoch: 6| Step: 11
Training loss: 2.320256233215332
Validation loss: 2.0262731115023294

Epoch: 6| Step: 12
Training loss: 2.226365327835083
Validation loss: 2.0206902821858725

Epoch: 6| Step: 13
Training loss: 1.8533554077148438
Validation loss: 2.020841419696808

Epoch: 77| Step: 0
Training loss: 2.08750057220459
Validation loss: 2.032772401968638

Epoch: 6| Step: 1
Training loss: 2.2740962505340576
Validation loss: 2.0227120916048684

Epoch: 6| Step: 2
Training loss: 2.1543169021606445
Validation loss: 2.028913140296936

Epoch: 6| Step: 3
Training loss: 2.528294801712036
Validation loss: 2.0286474227905273

Epoch: 6| Step: 4
Training loss: 1.714085578918457
Validation loss: 2.025344888369242

Epoch: 6| Step: 5
Training loss: 2.04829478263855
Validation loss: 2.03280899922053

Epoch: 6| Step: 6
Training loss: 2.0194077491760254
Validation loss: 2.0270808140436807

Epoch: 6| Step: 7
Training loss: 2.1157803535461426
Validation loss: 2.031677722930908

Epoch: 6| Step: 8
Training loss: 3.033874750137329
Validation loss: 2.0269983410835266

Epoch: 6| Step: 9
Training loss: 2.2101447582244873
Validation loss: 2.0257855455080667

Epoch: 6| Step: 10
Training loss: 2.234707832336426
Validation loss: 2.025276780128479

Epoch: 6| Step: 11
Training loss: 1.7578256130218506
Validation loss: 2.026349504788717

Epoch: 6| Step: 12
Training loss: 1.8455373048782349
Validation loss: 2.0224132537841797

Epoch: 6| Step: 13
Training loss: 2.1820907592773438
Validation loss: 2.0200493137041726

Epoch: 78| Step: 0
Training loss: 1.8748470544815063
Validation loss: 2.021508355935415

Epoch: 6| Step: 1
Training loss: 2.40270733833313
Validation loss: 2.0198967456817627

Epoch: 6| Step: 2
Training loss: 2.0143470764160156
Validation loss: 2.0215229988098145

Epoch: 6| Step: 3
Training loss: 2.0672945976257324
Validation loss: 2.0274781584739685

Epoch: 6| Step: 4
Training loss: 1.5425803661346436
Validation loss: 2.0213043888409934

Epoch: 6| Step: 5
Training loss: 2.0332224369049072
Validation loss: 2.019537846247355

Epoch: 6| Step: 6
Training loss: 2.3295340538024902
Validation loss: 2.0199737747510276

Epoch: 6| Step: 7
Training loss: 3.5242254734039307
Validation loss: 2.0209322373072305

Epoch: 6| Step: 8
Training loss: 2.0039637088775635
Validation loss: 2.025616705417633

Epoch: 6| Step: 9
Training loss: 1.9864721298217773
Validation loss: 2.02486115694046

Epoch: 6| Step: 10
Training loss: 2.041171073913574
Validation loss: 2.0247055292129517

Epoch: 6| Step: 11
Training loss: 2.1249887943267822
Validation loss: 2.034887909889221

Epoch: 6| Step: 12
Training loss: 2.3301827907562256
Validation loss: 2.0330618818600974

Epoch: 6| Step: 13
Training loss: 1.8883341550827026
Validation loss: 2.040367901325226

Epoch: 79| Step: 0
Training loss: 2.7144851684570312
Validation loss: 2.0374227166175842

Epoch: 6| Step: 1
Training loss: 2.0410032272338867
Validation loss: 2.0409982403119407

Epoch: 6| Step: 2
Training loss: 1.9307806491851807
Validation loss: 2.0432483355204263

Epoch: 6| Step: 3
Training loss: 1.9693777561187744
Validation loss: 2.035224437713623

Epoch: 6| Step: 4
Training loss: 1.6742076873779297
Validation loss: 2.0355049769083657

Epoch: 6| Step: 5
Training loss: 2.20954966545105
Validation loss: 2.0293110807736716

Epoch: 6| Step: 6
Training loss: 2.5963053703308105
Validation loss: 2.021808683872223

Epoch: 6| Step: 7
Training loss: 2.3894762992858887
Validation loss: 2.0234967867533364

Epoch: 6| Step: 8
Training loss: 2.0844900608062744
Validation loss: 2.0170223315556846

Epoch: 6| Step: 9
Training loss: 2.0543880462646484
Validation loss: 2.017715593179067

Epoch: 6| Step: 10
Training loss: 1.7626937627792358
Validation loss: 2.018458624680837

Epoch: 6| Step: 11
Training loss: 2.0841927528381348
Validation loss: 2.023134787877401

Epoch: 6| Step: 12
Training loss: 2.3564422130584717
Validation loss: 2.0308757623036704

Epoch: 6| Step: 13
Training loss: 2.437735080718994
Validation loss: 2.0398238698641458

Epoch: 80| Step: 0
Training loss: 1.9642877578735352
Validation loss: 2.0363970001538596

Epoch: 6| Step: 1
Training loss: 2.0839319229125977
Validation loss: 2.040448168913523

Epoch: 6| Step: 2
Training loss: 1.7100811004638672
Validation loss: 2.050400753815969

Epoch: 6| Step: 3
Training loss: 1.9624359607696533
Validation loss: 2.0541531244913735

Epoch: 6| Step: 4
Training loss: 1.7265541553497314
Validation loss: 2.053942104180654

Epoch: 6| Step: 5
Training loss: 1.7240781784057617
Validation loss: 2.050677756468455

Epoch: 6| Step: 6
Training loss: 3.3765511512756348
Validation loss: 2.055704891681671

Epoch: 6| Step: 7
Training loss: 2.4710917472839355
Validation loss: 2.052206536134084

Epoch: 6| Step: 8
Training loss: 2.557082176208496
Validation loss: 2.041130781173706

Epoch: 6| Step: 9
Training loss: 2.4403059482574463
Validation loss: 2.027603010336558

Epoch: 6| Step: 10
Training loss: 1.8371145725250244
Validation loss: 2.017270545164744

Epoch: 6| Step: 11
Training loss: 2.122493267059326
Validation loss: 2.012935519218445

Epoch: 6| Step: 12
Training loss: 2.0143301486968994
Validation loss: 2.010312875111898

Epoch: 6| Step: 13
Training loss: 2.460207462310791
Validation loss: 2.0141040086746216

Epoch: 81| Step: 0
Training loss: 2.648696184158325
Validation loss: 2.0131694873174033

Epoch: 6| Step: 1
Training loss: 2.3383278846740723
Validation loss: 2.0243369738260903

Epoch: 6| Step: 2
Training loss: 1.701141595840454
Validation loss: 2.027973175048828

Epoch: 6| Step: 3
Training loss: 2.547351837158203
Validation loss: 2.029907206694285

Epoch: 6| Step: 4
Training loss: 1.6393482685089111
Validation loss: 2.035577356815338

Epoch: 6| Step: 5
Training loss: 2.3350086212158203
Validation loss: 2.0289024909337363

Epoch: 6| Step: 6
Training loss: 1.9081439971923828
Validation loss: 2.0333204865455627

Epoch: 6| Step: 7
Training loss: 2.080965757369995
Validation loss: 2.0321775873502097

Epoch: 6| Step: 8
Training loss: 2.564148187637329
Validation loss: 2.036843995253245

Epoch: 6| Step: 9
Training loss: 2.0936174392700195
Validation loss: 2.0378737449645996

Epoch: 6| Step: 10
Training loss: 2.4664697647094727
Validation loss: 2.0389696955680847

Epoch: 6| Step: 11
Training loss: 2.2075729370117188
Validation loss: 2.023514727751414

Epoch: 6| Step: 12
Training loss: 2.1788601875305176
Validation loss: 2.028079350789388

Epoch: 6| Step: 13
Training loss: 1.9630476236343384
Validation loss: 2.014594614505768

Epoch: 82| Step: 0
Training loss: 2.103139877319336
Validation loss: 2.0179280440012612

Epoch: 6| Step: 1
Training loss: 2.4959590435028076
Validation loss: 2.0232165654500327

Epoch: 6| Step: 2
Training loss: 1.8112847805023193
Validation loss: 2.024344185988108

Epoch: 6| Step: 3
Training loss: 2.1829991340637207
Validation loss: 2.0291871627171836

Epoch: 6| Step: 4
Training loss: 2.1372199058532715
Validation loss: 2.031795859336853

Epoch: 6| Step: 5
Training loss: 2.7161271572113037
Validation loss: 2.0357141693433127

Epoch: 6| Step: 6
Training loss: 2.1274213790893555
Validation loss: 2.033365567525228

Epoch: 6| Step: 7
Training loss: 2.2498083114624023
Validation loss: 2.0447572271029153

Epoch: 6| Step: 8
Training loss: 1.913560152053833
Validation loss: 2.044465402762095

Epoch: 6| Step: 9
Training loss: 2.1824240684509277
Validation loss: 2.042434732119242

Epoch: 6| Step: 10
Training loss: 2.6080427169799805
Validation loss: 2.0393129189809165

Epoch: 6| Step: 11
Training loss: 1.7745717763900757
Validation loss: 2.0532968839009604

Epoch: 6| Step: 12
Training loss: 2.5064408779144287
Validation loss: 2.0513340632120767

Epoch: 6| Step: 13
Training loss: 1.3320828676223755
Validation loss: 2.0412402947743735

Epoch: 83| Step: 0
Training loss: 2.380488395690918
Validation loss: 2.044093827406565

Epoch: 6| Step: 1
Training loss: 1.8943578004837036
Validation loss: 2.0384347438812256

Epoch: 6| Step: 2
Training loss: 2.34661865234375
Validation loss: 2.039734741051992

Epoch: 6| Step: 3
Training loss: 2.234062433242798
Validation loss: 2.0344334840774536

Epoch: 6| Step: 4
Training loss: 2.247286796569824
Validation loss: 2.026766816775004

Epoch: 6| Step: 5
Training loss: 2.497408390045166
Validation loss: 2.022269368171692

Epoch: 6| Step: 6
Training loss: 2.444880962371826
Validation loss: 2.0189988017082214

Epoch: 6| Step: 7
Training loss: 1.7244138717651367
Validation loss: 2.0125779509544373

Epoch: 6| Step: 8
Training loss: 1.502241849899292
Validation loss: 2.016818940639496

Epoch: 6| Step: 9
Training loss: 2.19473934173584
Validation loss: 2.0177423556645713

Epoch: 6| Step: 10
Training loss: 2.041102886199951
Validation loss: 2.0161866744359336

Epoch: 6| Step: 11
Training loss: 2.43471622467041
Validation loss: 2.0242857734362283

Epoch: 6| Step: 12
Training loss: 1.982480764389038
Validation loss: 2.020129402478536

Epoch: 6| Step: 13
Training loss: 2.2990620136260986
Validation loss: 2.024016559123993

Epoch: 84| Step: 0
Training loss: 2.2170467376708984
Validation loss: 2.034263789653778

Epoch: 6| Step: 1
Training loss: 2.2453255653381348
Validation loss: 2.0358142256736755

Epoch: 6| Step: 2
Training loss: 2.0063555240631104
Validation loss: 2.0388522148132324

Epoch: 6| Step: 3
Training loss: 2.193026542663574
Validation loss: 2.0454352299372354

Epoch: 6| Step: 4
Training loss: 2.7361910343170166
Validation loss: 2.039811352888743

Epoch: 6| Step: 5
Training loss: 2.1183459758758545
Validation loss: 2.045839269955953

Epoch: 6| Step: 6
Training loss: 1.232063889503479
Validation loss: 2.0419421394666037

Epoch: 6| Step: 7
Training loss: 2.46327543258667
Validation loss: 2.037619431813558

Epoch: 6| Step: 8
Training loss: 2.3005800247192383
Validation loss: 2.029145578543345

Epoch: 6| Step: 9
Training loss: 1.7873945236206055
Validation loss: 2.0329577128092446

Epoch: 6| Step: 10
Training loss: 1.9924150705337524
Validation loss: 2.0303079883257547

Epoch: 6| Step: 11
Training loss: 1.9517194032669067
Validation loss: 2.0282952586809793

Epoch: 6| Step: 12
Training loss: 2.30137300491333
Validation loss: 2.0279541015625

Epoch: 6| Step: 13
Training loss: 2.457777500152588
Validation loss: 2.033097565174103

Epoch: 85| Step: 0
Training loss: 2.7645795345306396
Validation loss: 2.0186111529668174

Epoch: 6| Step: 1
Training loss: 1.6358298063278198
Validation loss: 2.02115797996521

Epoch: 6| Step: 2
Training loss: 1.9293200969696045
Validation loss: 2.0255305965741477

Epoch: 6| Step: 3
Training loss: 2.1826131343841553
Validation loss: 2.0183075269063315

Epoch: 6| Step: 4
Training loss: 2.2151949405670166
Validation loss: 2.0142866373062134

Epoch: 6| Step: 5
Training loss: 2.166712760925293
Validation loss: 2.0180207093556723

Epoch: 6| Step: 6
Training loss: 2.645740509033203
Validation loss: 2.016310751438141

Epoch: 6| Step: 7
Training loss: 2.2342987060546875
Validation loss: 2.0111212929089866

Epoch: 6| Step: 8
Training loss: 2.14599347114563
Validation loss: 2.0124259193738303

Epoch: 6| Step: 9
Training loss: 2.041813373565674
Validation loss: 2.0053698420524597

Epoch: 6| Step: 10
Training loss: 1.7705706357955933
Validation loss: 2.020209093888601

Epoch: 6| Step: 11
Training loss: 1.9051152467727661
Validation loss: 2.0279325048128762

Epoch: 6| Step: 12
Training loss: 2.3575735092163086
Validation loss: 2.0284881591796875

Epoch: 6| Step: 13
Training loss: 2.173711061477661
Validation loss: 2.036079923311869

Epoch: 86| Step: 0
Training loss: 1.7208679914474487
Validation loss: 2.0413461724917092

Epoch: 6| Step: 1
Training loss: 2.470221996307373
Validation loss: 2.0357001026471457

Epoch: 6| Step: 2
Training loss: 2.853789806365967
Validation loss: 2.0376402338345847

Epoch: 6| Step: 3
Training loss: 2.2646570205688477
Validation loss: 2.0228395064671836

Epoch: 6| Step: 4
Training loss: 2.52422833442688
Validation loss: 2.0177239775657654

Epoch: 6| Step: 5
Training loss: 2.2143807411193848
Validation loss: 2.018351137638092

Epoch: 6| Step: 6
Training loss: 2.368788480758667
Validation loss: 2.0099159677823386

Epoch: 6| Step: 7
Training loss: 2.3934688568115234
Validation loss: 2.0088266332944236

Epoch: 6| Step: 8
Training loss: 2.7036240100860596
Validation loss: 2.0153048038482666

Epoch: 6| Step: 9
Training loss: 2.02154278755188
Validation loss: 2.011932134628296

Epoch: 6| Step: 10
Training loss: 1.6416966915130615
Validation loss: 2.0143413146336875

Epoch: 6| Step: 11
Training loss: 1.3781254291534424
Validation loss: 2.015710095564524

Epoch: 6| Step: 12
Training loss: 2.064222812652588
Validation loss: 2.0159128109614053

Epoch: 6| Step: 13
Training loss: 1.733442783355713
Validation loss: 2.0158851146698

Epoch: 87| Step: 0
Training loss: 2.1466639041900635
Validation loss: 2.0117299358050027

Epoch: 6| Step: 1
Training loss: 1.538865327835083
Validation loss: 2.015632132689158

Epoch: 6| Step: 2
Training loss: 2.405841588973999
Validation loss: 2.0263948241869607

Epoch: 6| Step: 3
Training loss: 2.5502591133117676
Validation loss: 2.0339706937472024

Epoch: 6| Step: 4
Training loss: 2.7186169624328613
Validation loss: 2.041853745778402

Epoch: 6| Step: 5
Training loss: 2.462160110473633
Validation loss: 2.041861077149709

Epoch: 6| Step: 6
Training loss: 1.277803897857666
Validation loss: 2.0484186013539634

Epoch: 6| Step: 7
Training loss: 2.550379514694214
Validation loss: 2.050036072731018

Epoch: 6| Step: 8
Training loss: 2.2835962772369385
Validation loss: 2.0428481698036194

Epoch: 6| Step: 9
Training loss: 1.898719072341919
Validation loss: 2.0411826968193054

Epoch: 6| Step: 10
Training loss: 1.9412552118301392
Validation loss: 2.0352708101272583

Epoch: 6| Step: 11
Training loss: 2.3944451808929443
Validation loss: 2.0369200706481934

Epoch: 6| Step: 12
Training loss: 2.2022581100463867
Validation loss: 2.0394229094187417

Epoch: 6| Step: 13
Training loss: 1.8585519790649414
Validation loss: 2.0302554965019226

Epoch: 88| Step: 0
Training loss: 2.6630334854125977
Validation loss: 2.03676837682724

Epoch: 6| Step: 1
Training loss: 2.342031955718994
Validation loss: 2.023429830869039

Epoch: 6| Step: 2
Training loss: 2.1715831756591797
Validation loss: 2.02544903755188

Epoch: 6| Step: 3
Training loss: 1.9637231826782227
Validation loss: 2.0315633018811545

Epoch: 6| Step: 4
Training loss: 2.172940492630005
Validation loss: 2.0226006706555686

Epoch: 6| Step: 5
Training loss: 2.2547383308410645
Validation loss: 2.0219257275263467

Epoch: 6| Step: 6
Training loss: 2.2499094009399414
Validation loss: 2.016886035601298

Epoch: 6| Step: 7
Training loss: 2.016806125640869
Validation loss: 2.013445874055227

Epoch: 6| Step: 8
Training loss: 1.933600664138794
Validation loss: 2.0182177623113

Epoch: 6| Step: 9
Training loss: 2.2863874435424805
Validation loss: 2.021429657936096

Epoch: 6| Step: 10
Training loss: 2.0442628860473633
Validation loss: 2.01696507136027

Epoch: 6| Step: 11
Training loss: 1.8883001804351807
Validation loss: 2.0184413393338523

Epoch: 6| Step: 12
Training loss: 2.3501877784729004
Validation loss: 2.0219263434410095

Epoch: 6| Step: 13
Training loss: 1.7004752159118652
Validation loss: 2.021395444869995

Epoch: 89| Step: 0
Training loss: 2.0713648796081543
Validation loss: 2.0321556329727173

Epoch: 6| Step: 1
Training loss: 1.6940317153930664
Validation loss: 2.0343070030212402

Epoch: 6| Step: 2
Training loss: 1.3188875913619995
Validation loss: 2.0411121447881064

Epoch: 6| Step: 3
Training loss: 1.96906578540802
Validation loss: 2.040141840775808

Epoch: 6| Step: 4
Training loss: 1.8483613729476929
Validation loss: 2.038540482521057

Epoch: 6| Step: 5
Training loss: 2.2120840549468994
Validation loss: 2.0501442750295005

Epoch: 6| Step: 6
Training loss: 2.3886489868164062
Validation loss: 2.05255393187205

Epoch: 6| Step: 7
Training loss: 2.844041347503662
Validation loss: 2.048685391743978

Epoch: 6| Step: 8
Training loss: 2.9142212867736816
Validation loss: 2.0610891779263816

Epoch: 6| Step: 9
Training loss: 2.2387919425964355
Validation loss: 2.056037505467733

Epoch: 6| Step: 10
Training loss: 2.5149221420288086
Validation loss: 2.044612487157186

Epoch: 6| Step: 11
Training loss: 2.0398731231689453
Validation loss: 2.041817923386892

Epoch: 6| Step: 12
Training loss: 1.9562972784042358
Validation loss: 2.0337688525517783

Epoch: 6| Step: 13
Training loss: 2.444186210632324
Validation loss: 2.039405941963196

Epoch: 90| Step: 0
Training loss: 2.5837559700012207
Validation loss: 2.0367042620976767

Epoch: 6| Step: 1
Training loss: 1.577610731124878
Validation loss: 2.029705286026001

Epoch: 6| Step: 2
Training loss: 2.0013184547424316
Validation loss: 2.0319751103719077

Epoch: 6| Step: 3
Training loss: 1.7824108600616455
Validation loss: 2.0258476932843528

Epoch: 6| Step: 4
Training loss: 2.008772373199463
Validation loss: 2.0274217327435813

Epoch: 6| Step: 5
Training loss: 2.5736422538757324
Validation loss: 2.0322811603546143

Epoch: 6| Step: 6
Training loss: 2.2164595127105713
Validation loss: 2.0382080078125

Epoch: 6| Step: 7
Training loss: 2.062161922454834
Validation loss: 2.0423506100972495

Epoch: 6| Step: 8
Training loss: 2.3521344661712646
Validation loss: 2.04571533203125

Epoch: 6| Step: 9
Training loss: 1.8037539720535278
Validation loss: 2.03674324353536

Epoch: 6| Step: 10
Training loss: 2.3433752059936523
Validation loss: 2.0378127296765647

Epoch: 6| Step: 11
Training loss: 1.9979989528656006
Validation loss: 2.0431309739748635

Epoch: 6| Step: 12
Training loss: 1.8335320949554443
Validation loss: 2.0401381850242615

Epoch: 6| Step: 13
Training loss: 2.9859418869018555
Validation loss: 2.0404736200968423

Epoch: 91| Step: 0
Training loss: 2.4325718879699707
Validation loss: 2.032638351122538

Epoch: 6| Step: 1
Training loss: 2.2367281913757324
Validation loss: 2.0332510471343994

Epoch: 6| Step: 2
Training loss: 1.7491686344146729
Validation loss: 2.025424599647522

Epoch: 6| Step: 3
Training loss: 2.325014352798462
Validation loss: 2.0200075109799704

Epoch: 6| Step: 4
Training loss: 2.256209135055542
Validation loss: 2.0161691705385842

Epoch: 6| Step: 5
Training loss: 1.334289312362671
Validation loss: 2.019243896007538

Epoch: 6| Step: 6
Training loss: 2.2155656814575195
Validation loss: 2.013094345728556

Epoch: 6| Step: 7
Training loss: 2.3530638217926025
Validation loss: 2.0125534335772195

Epoch: 6| Step: 8
Training loss: 1.978676676750183
Validation loss: 2.0136745969454446

Epoch: 6| Step: 9
Training loss: 2.128753662109375
Validation loss: 2.0109251340230307

Epoch: 6| Step: 10
Training loss: 2.269538402557373
Validation loss: 2.0059093634287515

Epoch: 6| Step: 11
Training loss: 1.900070309638977
Validation loss: 2.0084365010261536

Epoch: 6| Step: 12
Training loss: 1.8978666067123413
Validation loss: 2.010115464528402

Epoch: 6| Step: 13
Training loss: 2.9488601684570312
Validation loss: 2.020418882369995

Epoch: 92| Step: 0
Training loss: 2.4115793704986572
Validation loss: 2.020791014035543

Epoch: 6| Step: 1
Training loss: 2.1325626373291016
Validation loss: 2.026437779267629

Epoch: 6| Step: 2
Training loss: 1.519551396369934
Validation loss: 2.0346275170644126

Epoch: 6| Step: 3
Training loss: 1.9025135040283203
Validation loss: 2.03130833307902

Epoch: 6| Step: 4
Training loss: 1.9287822246551514
Validation loss: 2.037968635559082

Epoch: 6| Step: 5
Training loss: 2.349836826324463
Validation loss: 2.0467871824900308

Epoch: 6| Step: 6
Training loss: 1.765998125076294
Validation loss: 2.042496661345164

Epoch: 6| Step: 7
Training loss: 1.9346935749053955
Validation loss: 2.0531251430511475

Epoch: 6| Step: 8
Training loss: 2.0436654090881348
Validation loss: 2.0484109918276467

Epoch: 6| Step: 9
Training loss: 2.1562552452087402
Validation loss: 2.0436297257741294

Epoch: 6| Step: 10
Training loss: 2.4449243545532227
Validation loss: 2.0478062828381858

Epoch: 6| Step: 11
Training loss: 2.476259708404541
Validation loss: 2.039720078309377

Epoch: 6| Step: 12
Training loss: 2.415132999420166
Validation loss: 2.029845098654429

Epoch: 6| Step: 13
Training loss: 2.5713391304016113
Validation loss: 2.035447279612223

Epoch: 93| Step: 0
Training loss: 1.8344950675964355
Validation loss: 2.027573545773824

Epoch: 6| Step: 1
Training loss: 1.864052653312683
Validation loss: 2.0318671663602195

Epoch: 6| Step: 2
Training loss: 2.128445863723755
Validation loss: 2.0279030799865723

Epoch: 6| Step: 3
Training loss: 2.1703031063079834
Validation loss: 2.032204508781433

Epoch: 6| Step: 4
Training loss: 1.6948031187057495
Validation loss: 2.0322062969207764

Epoch: 6| Step: 5
Training loss: 1.8535327911376953
Validation loss: 2.025935709476471

Epoch: 6| Step: 6
Training loss: 1.8623417615890503
Validation loss: 2.027606705824534

Epoch: 6| Step: 7
Training loss: 2.5293710231781006
Validation loss: 2.0323229233423867

Epoch: 6| Step: 8
Training loss: 2.2566685676574707
Validation loss: 2.0315834879875183

Epoch: 6| Step: 9
Training loss: 2.1287858486175537
Validation loss: 2.03719828526179

Epoch: 6| Step: 10
Training loss: 2.389415740966797
Validation loss: 2.0285301208496094

Epoch: 6| Step: 11
Training loss: 2.255064010620117
Validation loss: 2.030142148335775

Epoch: 6| Step: 12
Training loss: 2.3200130462646484
Validation loss: 2.031118909517924

Epoch: 6| Step: 13
Training loss: 2.637354850769043
Validation loss: 2.0209731260935464

Epoch: 94| Step: 0
Training loss: 2.3775081634521484
Validation loss: 2.0183095932006836

Epoch: 6| Step: 1
Training loss: 2.4297406673431396
Validation loss: 2.018844465414683

Epoch: 6| Step: 2
Training loss: 2.0500590801239014
Validation loss: 2.0101675589879355

Epoch: 6| Step: 3
Training loss: 2.2022247314453125
Validation loss: 2.008800446987152

Epoch: 6| Step: 4
Training loss: 2.4421513080596924
Validation loss: 2.0078831911087036

Epoch: 6| Step: 5
Training loss: 2.0769267082214355
Validation loss: 2.0100720524787903

Epoch: 6| Step: 6
Training loss: 2.1734209060668945
Validation loss: 2.006684104601542

Epoch: 6| Step: 7
Training loss: 1.8510017395019531
Validation loss: 2.0064523418744407

Epoch: 6| Step: 8
Training loss: 2.5330264568328857
Validation loss: 2.0100177923838296

Epoch: 6| Step: 9
Training loss: 1.8779922723770142
Validation loss: 2.0167800982793174

Epoch: 6| Step: 10
Training loss: 1.7433303594589233
Validation loss: 2.017483373483022

Epoch: 6| Step: 11
Training loss: 2.360565662384033
Validation loss: 2.0245991349220276

Epoch: 6| Step: 12
Training loss: 2.057162046432495
Validation loss: 2.0294187664985657

Epoch: 6| Step: 13
Training loss: 1.6284959316253662
Validation loss: 2.0280415217081704

Epoch: 95| Step: 0
Training loss: 2.749825954437256
Validation loss: 2.033349335193634

Epoch: 6| Step: 1
Training loss: 1.5006449222564697
Validation loss: 2.0354163448015847

Epoch: 6| Step: 2
Training loss: 1.9970049858093262
Validation loss: 2.0331793824831643

Epoch: 6| Step: 3
Training loss: 1.6602296829223633
Validation loss: 2.0364646514256797

Epoch: 6| Step: 4
Training loss: 1.514667272567749
Validation loss: 2.027738551298777

Epoch: 6| Step: 5
Training loss: 1.1387386322021484
Validation loss: 2.028715709845225

Epoch: 6| Step: 6
Training loss: 2.943035125732422
Validation loss: 2.0303284525871277

Epoch: 6| Step: 7
Training loss: 2.5228397846221924
Validation loss: 2.035665194193522

Epoch: 6| Step: 8
Training loss: 2.3999757766723633
Validation loss: 2.025191684563955

Epoch: 6| Step: 9
Training loss: 2.6033833026885986
Validation loss: 2.0295522809028625

Epoch: 6| Step: 10
Training loss: 2.261274814605713
Validation loss: 2.0365967750549316

Epoch: 6| Step: 11
Training loss: 2.2058310508728027
Validation loss: 2.032630125681559

Epoch: 6| Step: 12
Training loss: 1.8965470790863037
Validation loss: 2.022427558898926

Epoch: 6| Step: 13
Training loss: 2.4086055755615234
Validation loss: 2.034000794092814

Epoch: 96| Step: 0
Training loss: 1.8251049518585205
Validation loss: 2.031798859437307

Epoch: 6| Step: 1
Training loss: 2.011962890625
Validation loss: 2.0255478421847024

Epoch: 6| Step: 2
Training loss: 1.8398549556732178
Validation loss: 2.0209947427113852

Epoch: 6| Step: 3
Training loss: 2.626349449157715
Validation loss: 2.0171619256337485

Epoch: 6| Step: 4
Training loss: 1.54408597946167
Validation loss: 2.0160597761472068

Epoch: 6| Step: 5
Training loss: 2.308461904525757
Validation loss: 2.017613172531128

Epoch: 6| Step: 6
Training loss: 1.6557655334472656
Validation loss: 2.0107360084851584

Epoch: 6| Step: 7
Training loss: 2.8457865715026855
Validation loss: 2.0241054693857827

Epoch: 6| Step: 8
Training loss: 1.869373083114624
Validation loss: 2.0177945295969644

Epoch: 6| Step: 9
Training loss: 2.0955827236175537
Validation loss: 2.0166147351264954

Epoch: 6| Step: 10
Training loss: 2.5110061168670654
Validation loss: 2.022302786509196

Epoch: 6| Step: 11
Training loss: 2.5223958492279053
Validation loss: 2.026922126611074

Epoch: 6| Step: 12
Training loss: 2.1434755325317383
Validation loss: 2.0181599855422974

Epoch: 6| Step: 13
Training loss: 1.975924015045166
Validation loss: 2.0111927588780723

Epoch: 97| Step: 0
Training loss: 2.1919758319854736
Validation loss: 2.0192713737487793

Epoch: 6| Step: 1
Training loss: 2.5657296180725098
Validation loss: 2.0175963044166565

Epoch: 6| Step: 2
Training loss: 1.711388349533081
Validation loss: 2.0145324071248374

Epoch: 6| Step: 3
Training loss: 1.9715604782104492
Validation loss: 2.0164188941319785

Epoch: 6| Step: 4
Training loss: 2.1699752807617188
Validation loss: 2.0184236566225686

Epoch: 6| Step: 5
Training loss: 1.7673916816711426
Validation loss: 2.015650471051534

Epoch: 6| Step: 6
Training loss: 2.64658522605896
Validation loss: 2.024905780951182

Epoch: 6| Step: 7
Training loss: 2.133190155029297
Validation loss: 2.020877242088318

Epoch: 6| Step: 8
Training loss: 2.018763542175293
Validation loss: 2.02158385515213

Epoch: 6| Step: 9
Training loss: 2.0077576637268066
Validation loss: 2.0253820021947226

Epoch: 6| Step: 10
Training loss: 1.7934967279434204
Validation loss: 2.0214073260625205

Epoch: 6| Step: 11
Training loss: 2.1876423358917236
Validation loss: 2.024064064025879

Epoch: 6| Step: 12
Training loss: 2.594935178756714
Validation loss: 2.0208000540733337

Epoch: 6| Step: 13
Training loss: 1.8796021938323975
Validation loss: 2.022177278995514

Epoch: 98| Step: 0
Training loss: 2.1650538444519043
Validation loss: 2.0243759155273438

Epoch: 6| Step: 1
Training loss: 1.8701164722442627
Validation loss: 2.020625094572703

Epoch: 6| Step: 2
Training loss: 1.6334853172302246
Validation loss: 2.0283477505048118

Epoch: 6| Step: 3
Training loss: 2.1023707389831543
Validation loss: 2.022697865962982

Epoch: 6| Step: 4
Training loss: 2.3637547492980957
Validation loss: 2.0236035188039145

Epoch: 6| Step: 5
Training loss: 2.234431266784668
Validation loss: 2.027432143688202

Epoch: 6| Step: 6
Training loss: 2.35685396194458
Validation loss: 2.0168446699778237

Epoch: 6| Step: 7
Training loss: 1.3318920135498047
Validation loss: 2.019962708155314

Epoch: 6| Step: 8
Training loss: 2.302478313446045
Validation loss: 2.0176755785942078

Epoch: 6| Step: 9
Training loss: 2.581327438354492
Validation loss: 2.021250009536743

Epoch: 6| Step: 10
Training loss: 2.2387804985046387
Validation loss: 2.0214427510897317

Epoch: 6| Step: 11
Training loss: 2.4152989387512207
Validation loss: 2.026257793108622

Epoch: 6| Step: 12
Training loss: 2.0293402671813965
Validation loss: 2.02216899394989

Epoch: 6| Step: 13
Training loss: 2.0462217330932617
Validation loss: 2.0213069319725037

Epoch: 99| Step: 0
Training loss: 2.400484085083008
Validation loss: 2.0228832562764487

Epoch: 6| Step: 1
Training loss: 2.3562138080596924
Validation loss: 2.018549084663391

Epoch: 6| Step: 2
Training loss: 2.055025577545166
Validation loss: 2.0196251471837363

Epoch: 6| Step: 3
Training loss: 2.400850296020508
Validation loss: 2.0082397858301797

Epoch: 6| Step: 4
Training loss: 1.595649003982544
Validation loss: 2.017616113026937

Epoch: 6| Step: 5
Training loss: 1.3590928316116333
Validation loss: 2.0157591303189597

Epoch: 6| Step: 6
Training loss: 1.8111698627471924
Validation loss: 2.008180320262909

Epoch: 6| Step: 7
Training loss: 1.8320858478546143
Validation loss: 2.02531506617864

Epoch: 6| Step: 8
Training loss: 2.655982732772827
Validation loss: 2.0282041231791177

Epoch: 6| Step: 9
Training loss: 1.7898083925247192
Validation loss: 2.0370346307754517

Epoch: 6| Step: 10
Training loss: 2.3203635215759277
Validation loss: 2.025001327196757

Epoch: 6| Step: 11
Training loss: 1.827811598777771
Validation loss: 2.038730502128601

Epoch: 6| Step: 12
Training loss: 2.342043876647949
Validation loss: 2.03987580537796

Epoch: 6| Step: 13
Training loss: 2.9225897789001465
Validation loss: 2.0387551188468933

Epoch: 100| Step: 0
Training loss: 1.43925940990448
Validation loss: 2.03568442662557

Epoch: 6| Step: 1
Training loss: 1.8582618236541748
Validation loss: 2.032626728216807

Epoch: 6| Step: 2
Training loss: 1.88507080078125
Validation loss: 2.0273138086001077

Epoch: 6| Step: 3
Training loss: 1.8869192600250244
Validation loss: 2.031136969725291

Epoch: 6| Step: 4
Training loss: 2.4918570518493652
Validation loss: 2.0323060552279153

Epoch: 6| Step: 5
Training loss: 2.1818342208862305
Validation loss: 2.040780504544576

Epoch: 6| Step: 6
Training loss: 2.174098253250122
Validation loss: 2.0342446168263755

Epoch: 6| Step: 7
Training loss: 1.8729513883590698
Validation loss: 2.0268357197443643

Epoch: 6| Step: 8
Training loss: 2.373356819152832
Validation loss: 2.0301289757092795

Epoch: 6| Step: 9
Training loss: 2.8610105514526367
Validation loss: 2.024171829223633

Epoch: 6| Step: 10
Training loss: 2.2192819118499756
Validation loss: 2.0246676802635193

Epoch: 6| Step: 11
Training loss: 2.0852813720703125
Validation loss: 2.0186343987782798

Epoch: 6| Step: 12
Training loss: 2.4142346382141113
Validation loss: 2.0190285444259644

Epoch: 6| Step: 13
Training loss: 1.878187656402588
Validation loss: 2.0189336140950522

Epoch: 101| Step: 0
Training loss: 2.185361862182617
Validation loss: 2.018900990486145

Epoch: 6| Step: 1
Training loss: 2.162635564804077
Validation loss: 2.0291258494059243

Epoch: 6| Step: 2
Training loss: 2.217407703399658
Validation loss: 2.0261745850245156

Epoch: 6| Step: 3
Training loss: 1.9240100383758545
Validation loss: 2.0159511168797812

Epoch: 6| Step: 4
Training loss: 1.7512030601501465
Validation loss: 2.0243204832077026

Epoch: 6| Step: 5
Training loss: 1.999328851699829
Validation loss: 2.0206124981244407

Epoch: 6| Step: 6
Training loss: 1.6160297393798828
Validation loss: 2.0240392088890076

Epoch: 6| Step: 7
Training loss: 2.6509757041931152
Validation loss: 2.022079110145569

Epoch: 6| Step: 8
Training loss: 2.395752191543579
Validation loss: 2.0164393981297812

Epoch: 6| Step: 9
Training loss: 2.1763505935668945
Validation loss: 2.0180285573005676

Epoch: 6| Step: 10
Training loss: 2.5978167057037354
Validation loss: 2.0207991003990173

Epoch: 6| Step: 11
Training loss: 2.3620619773864746
Validation loss: 2.021623154481252

Epoch: 6| Step: 12
Training loss: 1.7545559406280518
Validation loss: 2.019701679547628

Epoch: 6| Step: 13
Training loss: 1.790955662727356
Validation loss: 2.027696510155996

Epoch: 102| Step: 0
Training loss: 1.9226014614105225
Validation loss: 2.023769120375315

Epoch: 6| Step: 1
Training loss: 1.6315284967422485
Validation loss: 2.024569571018219

Epoch: 6| Step: 2
Training loss: 2.0027248859405518
Validation loss: 2.025393227736155

Epoch: 6| Step: 3
Training loss: 1.8816689252853394
Validation loss: 2.031826396783193

Epoch: 6| Step: 4
Training loss: 2.584165334701538
Validation loss: 2.0231178800264993

Epoch: 6| Step: 5
Training loss: 2.3376874923706055
Validation loss: 2.0316196282704673

Epoch: 6| Step: 6
Training loss: 1.9327194690704346
Validation loss: 2.024592638015747

Epoch: 6| Step: 7
Training loss: 2.002180337905884
Validation loss: 2.0185035467147827

Epoch: 6| Step: 8
Training loss: 2.1104865074157715
Validation loss: 2.0191723306973777

Epoch: 6| Step: 9
Training loss: 2.693291187286377
Validation loss: 2.015512526035309

Epoch: 6| Step: 10
Training loss: 2.7179243564605713
Validation loss: 2.022730310757955

Epoch: 6| Step: 11
Training loss: 1.7622140645980835
Validation loss: 2.0236910382906594

Epoch: 6| Step: 12
Training loss: 1.914806604385376
Validation loss: 2.0176011125246682

Epoch: 6| Step: 13
Training loss: 1.9586403369903564
Validation loss: 2.014481166998545

Epoch: 103| Step: 0
Training loss: 1.9260852336883545
Validation loss: 2.0205268263816833

Epoch: 6| Step: 1
Training loss: 2.2329673767089844
Validation loss: 2.0180657307306924

Epoch: 6| Step: 2
Training loss: 2.3785200119018555
Validation loss: 2.0156019727389016

Epoch: 6| Step: 3
Training loss: 2.2830886840820312
Validation loss: 2.018470843633016

Epoch: 6| Step: 4
Training loss: 2.397937297821045
Validation loss: 2.0228503545125327

Epoch: 6| Step: 5
Training loss: 2.1784281730651855
Validation loss: 2.0177905360857644

Epoch: 6| Step: 6
Training loss: 2.5141725540161133
Validation loss: 2.0232090950012207

Epoch: 6| Step: 7
Training loss: 1.370412826538086
Validation loss: 2.0376118818918862

Epoch: 6| Step: 8
Training loss: 2.5365591049194336
Validation loss: 2.03814826409022

Epoch: 6| Step: 9
Training loss: 1.6007740497589111
Validation loss: 2.0403740803400674

Epoch: 6| Step: 10
Training loss: 1.8909226655960083
Validation loss: 2.0288294156392417

Epoch: 6| Step: 11
Training loss: 1.973657250404358
Validation loss: 2.0297388434410095

Epoch: 6| Step: 12
Training loss: 2.0559134483337402
Validation loss: 2.0335691769917807

Epoch: 6| Step: 13
Training loss: 2.497664213180542
Validation loss: 2.0322418808937073

Epoch: 104| Step: 0
Training loss: 1.8483058214187622
Validation loss: 2.0362682541211448

Epoch: 6| Step: 1
Training loss: 2.773186683654785
Validation loss: 2.0325017968813577

Epoch: 6| Step: 2
Training loss: 2.1580867767333984
Validation loss: 2.0269446770350137

Epoch: 6| Step: 3
Training loss: 1.765372633934021
Validation loss: 2.0306373238563538

Epoch: 6| Step: 4
Training loss: 2.4245693683624268
Validation loss: 2.0302045742670694

Epoch: 6| Step: 5
Training loss: 2.205991744995117
Validation loss: 2.0344855388005576

Epoch: 6| Step: 6
Training loss: 1.8560504913330078
Validation loss: 2.0328401923179626

Epoch: 6| Step: 7
Training loss: 1.925351619720459
Validation loss: 2.035669982433319

Epoch: 6| Step: 8
Training loss: 1.835187315940857
Validation loss: 2.0272633035977683

Epoch: 6| Step: 9
Training loss: 1.8357433080673218
Validation loss: 2.029216766357422

Epoch: 6| Step: 10
Training loss: 1.7394285202026367
Validation loss: 2.031513253847758

Epoch: 6| Step: 11
Training loss: 2.225639581680298
Validation loss: 2.038057327270508

Epoch: 6| Step: 12
Training loss: 3.029282569885254
Validation loss: 2.038390556971232

Epoch: 6| Step: 13
Training loss: 2.1635560989379883
Validation loss: 2.0395703315734863

Epoch: 105| Step: 0
Training loss: 2.2661304473876953
Validation loss: 2.0356842478116355

Epoch: 6| Step: 1
Training loss: 2.1612613201141357
Validation loss: 2.0351810852686563

Epoch: 6| Step: 2
Training loss: 1.883924961090088
Validation loss: 2.030146837234497

Epoch: 6| Step: 3
Training loss: 1.7638404369354248
Validation loss: 2.026421626408895

Epoch: 6| Step: 4
Training loss: 2.005908727645874
Validation loss: 2.022372921307882

Epoch: 6| Step: 5
Training loss: 2.3731846809387207
Validation loss: 2.014981468518575

Epoch: 6| Step: 6
Training loss: 2.0711121559143066
Validation loss: 2.016966084639231

Epoch: 6| Step: 7
Training loss: 2.371213674545288
Validation loss: 2.0099311073621116

Epoch: 6| Step: 8
Training loss: 2.6306557655334473
Validation loss: 2.0107217828432717

Epoch: 6| Step: 9
Training loss: 2.007533550262451
Validation loss: 2.018041471640269

Epoch: 6| Step: 10
Training loss: 2.397209405899048
Validation loss: 2.018626312414805

Epoch: 6| Step: 11
Training loss: 2.0406579971313477
Validation loss: 2.023210287094116

Epoch: 6| Step: 12
Training loss: 1.9757356643676758
Validation loss: 2.01358824968338

Epoch: 6| Step: 13
Training loss: 1.858141541481018
Validation loss: 2.0140854120254517

Epoch: 106| Step: 0
Training loss: 1.80757737159729
Validation loss: 2.0152151584625244

Epoch: 6| Step: 1
Training loss: 2.2433485984802246
Validation loss: 2.010029753049215

Epoch: 6| Step: 2
Training loss: 2.2944259643554688
Validation loss: 2.0048739314079285

Epoch: 6| Step: 3
Training loss: 1.8190453052520752
Validation loss: 2.0073744654655457

Epoch: 6| Step: 4
Training loss: 1.778202772140503
Validation loss: 2.0054879983266196

Epoch: 6| Step: 5
Training loss: 2.341559410095215
Validation loss: 2.003651956717173

Epoch: 6| Step: 6
Training loss: 1.7662341594696045
Validation loss: 2.0177454551060996

Epoch: 6| Step: 7
Training loss: 2.491641044616699
Validation loss: 2.0148175756136575

Epoch: 6| Step: 8
Training loss: 2.6220879554748535
Validation loss: 2.015266021092733

Epoch: 6| Step: 9
Training loss: 2.331638813018799
Validation loss: 2.0169182419776917

Epoch: 6| Step: 10
Training loss: 1.8428031206130981
Validation loss: 2.0191505750020347

Epoch: 6| Step: 11
Training loss: 2.306572437286377
Validation loss: 2.013025085131327

Epoch: 6| Step: 12
Training loss: 2.0389671325683594
Validation loss: 2.0123884677886963

Epoch: 6| Step: 13
Training loss: 1.8374180793762207
Validation loss: 2.0140845576922097

Epoch: 107| Step: 0
Training loss: 2.306942939758301
Validation loss: 2.013389547665914

Epoch: 6| Step: 1
Training loss: 1.7644190788269043
Validation loss: 2.023035168647766

Epoch: 6| Step: 2
Training loss: 2.4781410694122314
Validation loss: 2.0223692258199057

Epoch: 6| Step: 3
Training loss: 2.2932276725769043
Validation loss: 2.023595452308655

Epoch: 6| Step: 4
Training loss: 1.8903439044952393
Validation loss: 2.021060327688853

Epoch: 6| Step: 5
Training loss: 1.6348026990890503
Validation loss: 2.0185404221216836

Epoch: 6| Step: 6
Training loss: 2.0410404205322266
Validation loss: 2.018766164779663

Epoch: 6| Step: 7
Training loss: 1.9630128145217896
Validation loss: 2.023719549179077

Epoch: 6| Step: 8
Training loss: 1.9423062801361084
Validation loss: 2.0278342167536416

Epoch: 6| Step: 9
Training loss: 2.130537986755371
Validation loss: 2.021146019299825

Epoch: 6| Step: 10
Training loss: 2.187790870666504
Validation loss: 2.0249996383984885

Epoch: 6| Step: 11
Training loss: 2.8517379760742188
Validation loss: 2.02824737628301

Epoch: 6| Step: 12
Training loss: 1.9644405841827393
Validation loss: 2.024389863014221

Epoch: 6| Step: 13
Training loss: 1.8950666189193726
Validation loss: 2.0270029306411743

Epoch: 108| Step: 0
Training loss: 1.9589476585388184
Validation loss: 2.0260533491770425

Epoch: 6| Step: 1
Training loss: 2.2349071502685547
Validation loss: 2.0284359455108643

Epoch: 6| Step: 2
Training loss: 2.1961541175842285
Validation loss: 2.0226180950800576

Epoch: 6| Step: 3
Training loss: 2.026510715484619
Validation loss: 2.024109641710917

Epoch: 6| Step: 4
Training loss: 1.9881231784820557
Validation loss: 2.0252604285875955

Epoch: 6| Step: 5
Training loss: 1.9534447193145752
Validation loss: 2.0360217889149985

Epoch: 6| Step: 6
Training loss: 1.4797747135162354
Validation loss: 2.0248307387034097

Epoch: 6| Step: 7
Training loss: 2.491407871246338
Validation loss: 2.0282186468442283

Epoch: 6| Step: 8
Training loss: 2.3596363067626953
Validation loss: 2.0313501755396524

Epoch: 6| Step: 9
Training loss: 2.4383416175842285
Validation loss: 2.0330281456311545

Epoch: 6| Step: 10
Training loss: 2.697988510131836
Validation loss: 2.0378228624661765

Epoch: 6| Step: 11
Training loss: 1.7720507383346558
Validation loss: 2.0368317365646362

Epoch: 6| Step: 12
Training loss: 1.921180009841919
Validation loss: 2.0353715221087136

Epoch: 6| Step: 13
Training loss: 1.8689857721328735
Validation loss: 2.0337959925333657

Epoch: 109| Step: 0
Training loss: 2.6053411960601807
Validation loss: 2.0369696021080017

Epoch: 6| Step: 1
Training loss: 2.980276346206665
Validation loss: 2.039223233858744

Epoch: 6| Step: 2
Training loss: 1.7979038953781128
Validation loss: 2.0255426168441772

Epoch: 6| Step: 3
Training loss: 2.1908373832702637
Validation loss: 2.030305802822113

Epoch: 6| Step: 4
Training loss: 1.6404829025268555
Validation loss: 2.0321285128593445

Epoch: 6| Step: 5
Training loss: 1.3938755989074707
Validation loss: 2.0462790926297507

Epoch: 6| Step: 6
Training loss: 1.7627891302108765
Validation loss: 2.033203363418579

Epoch: 6| Step: 7
Training loss: 1.6062302589416504
Validation loss: 2.042126218477885

Epoch: 6| Step: 8
Training loss: 2.331538677215576
Validation loss: 2.043593088785807

Epoch: 6| Step: 9
Training loss: 2.0403168201446533
Validation loss: 2.042969981829325

Epoch: 6| Step: 10
Training loss: 1.767827033996582
Validation loss: 2.0304030974706015

Epoch: 6| Step: 11
Training loss: 2.493454933166504
Validation loss: 2.031442920366923

Epoch: 6| Step: 12
Training loss: 2.2367725372314453
Validation loss: 2.016291379928589

Epoch: 6| Step: 13
Training loss: 2.360133409500122
Validation loss: 2.0192073583602905

Epoch: 110| Step: 0
Training loss: 1.5522375106811523
Validation loss: 2.0159807801246643

Epoch: 6| Step: 1
Training loss: 2.4194862842559814
Validation loss: 2.0144925316174827

Epoch: 6| Step: 2
Training loss: 2.298201084136963
Validation loss: 2.011857012907664

Epoch: 6| Step: 3
Training loss: 2.9921698570251465
Validation loss: 2.008464992046356

Epoch: 6| Step: 4
Training loss: 1.5133898258209229
Validation loss: 2.0004363457361856

Epoch: 6| Step: 5
Training loss: 2.312551975250244
Validation loss: 2.0127681692441306

Epoch: 6| Step: 6
Training loss: 1.892442226409912
Validation loss: 2.0049089988072715

Epoch: 6| Step: 7
Training loss: 1.621567726135254
Validation loss: 1.9988394975662231

Epoch: 6| Step: 8
Training loss: 2.512587070465088
Validation loss: 2.009572366873423

Epoch: 6| Step: 9
Training loss: 2.0620193481445312
Validation loss: 2.0208208759625754

Epoch: 6| Step: 10
Training loss: 1.9564357995986938
Validation loss: 2.0131629506746926

Epoch: 6| Step: 11
Training loss: 1.949706792831421
Validation loss: 2.0083741346995034

Epoch: 6| Step: 12
Training loss: 2.1712584495544434
Validation loss: 2.0131696462631226

Epoch: 6| Step: 13
Training loss: 2.470411777496338
Validation loss: 2.019791762034098

Epoch: 111| Step: 0
Training loss: 1.9060561656951904
Validation loss: 2.0208744009335837

Epoch: 6| Step: 1
Training loss: 2.183350086212158
Validation loss: 2.010908087094625

Epoch: 6| Step: 2
Training loss: 1.8645085096359253
Validation loss: 2.0144615968068442

Epoch: 6| Step: 3
Training loss: 2.4834959506988525
Validation loss: 2.0183557073275247

Epoch: 6| Step: 4
Training loss: 2.2020421028137207
Validation loss: 2.0232813159624734

Epoch: 6| Step: 5
Training loss: 1.3074610233306885
Validation loss: 2.0238777796427407

Epoch: 6| Step: 6
Training loss: 2.438636541366577
Validation loss: 2.0287726720174155

Epoch: 6| Step: 7
Training loss: 2.0071423053741455
Validation loss: 2.0307490825653076

Epoch: 6| Step: 8
Training loss: 1.7254425287246704
Validation loss: 2.0267484982808432

Epoch: 6| Step: 9
Training loss: 1.9221949577331543
Validation loss: 2.028967102368673

Epoch: 6| Step: 10
Training loss: 2.256899118423462
Validation loss: 2.023237943649292

Epoch: 6| Step: 11
Training loss: 2.6698827743530273
Validation loss: 2.032785713672638

Epoch: 6| Step: 12
Training loss: 2.1897974014282227
Validation loss: 2.0260726610819497

Epoch: 6| Step: 13
Training loss: 2.4296875
Validation loss: 2.0348953008651733

Epoch: 112| Step: 0
Training loss: 2.2400593757629395
Validation loss: 2.0205697814623513

Epoch: 6| Step: 1
Training loss: 1.9433352947235107
Validation loss: 2.0215925176938376

Epoch: 6| Step: 2
Training loss: 2.497136354446411
Validation loss: 2.021880567073822

Epoch: 6| Step: 3
Training loss: 1.8087295293807983
Validation loss: 2.0207597812016806

Epoch: 6| Step: 4
Training loss: 2.1458418369293213
Validation loss: 2.0247965455055237

Epoch: 6| Step: 5
Training loss: 1.77576744556427
Validation loss: 2.01858389377594

Epoch: 6| Step: 6
Training loss: 2.60124135017395
Validation loss: 2.0194128155708313

Epoch: 6| Step: 7
Training loss: 1.9219014644622803
Validation loss: 2.0220293005307517

Epoch: 6| Step: 8
Training loss: 1.7798333168029785
Validation loss: 2.0186636249224343

Epoch: 6| Step: 9
Training loss: 1.9284498691558838
Validation loss: 2.021781067053477

Epoch: 6| Step: 10
Training loss: 1.88847017288208
Validation loss: 2.0233638882637024

Epoch: 6| Step: 11
Training loss: 1.922794222831726
Validation loss: 2.027597427368164

Epoch: 6| Step: 12
Training loss: 2.6444573402404785
Validation loss: 2.0245523850123086

Epoch: 6| Step: 13
Training loss: 2.2290596961975098
Validation loss: 2.0144983728726706

Epoch: 113| Step: 0
Training loss: 1.8519954681396484
Validation loss: 2.0068325797716775

Epoch: 6| Step: 1
Training loss: 1.7350707054138184
Validation loss: 2.015410363674164

Epoch: 6| Step: 2
Training loss: 3.097059965133667
Validation loss: 2.0140484968821206

Epoch: 6| Step: 3
Training loss: 1.8957170248031616
Validation loss: 2.016670823097229

Epoch: 6| Step: 4
Training loss: 2.2764880657196045
Validation loss: 2.0024831891059875

Epoch: 6| Step: 5
Training loss: 1.6509714126586914
Validation loss: 2.0094743172327676

Epoch: 6| Step: 6
Training loss: 2.2117085456848145
Validation loss: 2.0079004963239035

Epoch: 6| Step: 7
Training loss: 2.2006776332855225
Validation loss: 2.018265167872111

Epoch: 6| Step: 8
Training loss: 2.5341806411743164
Validation loss: 2.0189292430877686

Epoch: 6| Step: 9
Training loss: 1.7426705360412598
Validation loss: 2.022829830646515

Epoch: 6| Step: 10
Training loss: 2.2952451705932617
Validation loss: 2.0211408336957297

Epoch: 6| Step: 11
Training loss: 2.111330270767212
Validation loss: 2.0341534415880838

Epoch: 6| Step: 12
Training loss: 1.8205604553222656
Validation loss: 2.027905364831289

Epoch: 6| Step: 13
Training loss: 2.055180549621582
Validation loss: 2.0242523352305093

Epoch: 114| Step: 0
Training loss: 1.6774238348007202
Validation loss: 2.0294440189997354

Epoch: 6| Step: 1
Training loss: 2.4633426666259766
Validation loss: 2.033364454905192

Epoch: 6| Step: 2
Training loss: 1.749283790588379
Validation loss: 2.0404415329297385

Epoch: 6| Step: 3
Training loss: 2.3055624961853027
Validation loss: 2.0361316800117493

Epoch: 6| Step: 4
Training loss: 2.9286141395568848
Validation loss: 2.039100706577301

Epoch: 6| Step: 5
Training loss: 2.100790500640869
Validation loss: 2.034834603468577

Epoch: 6| Step: 6
Training loss: 1.9952154159545898
Validation loss: 2.0385664105415344

Epoch: 6| Step: 7
Training loss: 2.3680975437164307
Validation loss: 2.0395844181378684

Epoch: 6| Step: 8
Training loss: 1.6058001518249512
Validation loss: 2.0320316553115845

Epoch: 6| Step: 9
Training loss: 2.1785576343536377
Validation loss: 2.032212416330973

Epoch: 6| Step: 10
Training loss: 2.102999210357666
Validation loss: 2.024990459283193

Epoch: 6| Step: 11
Training loss: 1.7196033000946045
Validation loss: 2.0400219162305198

Epoch: 6| Step: 12
Training loss: 2.374037742614746
Validation loss: 2.0419642329216003

Epoch: 6| Step: 13
Training loss: 1.5761222839355469
Validation loss: 2.0384088357289634

Epoch: 115| Step: 0
Training loss: 1.6170833110809326
Validation loss: 2.0230895280838013

Epoch: 6| Step: 1
Training loss: 2.3112430572509766
Validation loss: 2.0336450338363647

Epoch: 6| Step: 2
Training loss: 1.8703453540802002
Validation loss: 2.0415292183558145

Epoch: 6| Step: 3
Training loss: 2.3937690258026123
Validation loss: 2.040000061194102

Epoch: 6| Step: 4
Training loss: 1.7963179349899292
Validation loss: 2.039545238018036

Epoch: 6| Step: 5
Training loss: 2.2024850845336914
Validation loss: 2.036690831184387

Epoch: 6| Step: 6
Training loss: 1.8784794807434082
Validation loss: 2.031195104122162

Epoch: 6| Step: 7
Training loss: 2.1308279037475586
Validation loss: 2.0370684464772544

Epoch: 6| Step: 8
Training loss: 2.5759284496307373
Validation loss: 2.0352970361709595

Epoch: 6| Step: 9
Training loss: 2.2226967811584473
Validation loss: 2.037983496983846

Epoch: 6| Step: 10
Training loss: 2.242713451385498
Validation loss: 2.0454319516817727

Epoch: 6| Step: 11
Training loss: 1.7377787828445435
Validation loss: 2.043963591257731

Epoch: 6| Step: 12
Training loss: 2.2256195545196533
Validation loss: 2.0450826485951743

Epoch: 6| Step: 13
Training loss: 2.0405948162078857
Validation loss: 2.0400591492652893

Epoch: 116| Step: 0
Training loss: 2.414130210876465
Validation loss: 2.0399436553319297

Epoch: 6| Step: 1
Training loss: 1.2749803066253662
Validation loss: 2.041228989760081

Epoch: 6| Step: 2
Training loss: 1.5767868757247925
Validation loss: 2.03521728515625

Epoch: 6| Step: 3
Training loss: 1.7629588842391968
Validation loss: 2.0301992694536843

Epoch: 6| Step: 4
Training loss: 2.2418031692504883
Validation loss: 2.0337727467219033

Epoch: 6| Step: 5
Training loss: 1.601980447769165
Validation loss: 2.028018315633138

Epoch: 6| Step: 6
Training loss: 2.930522918701172
Validation loss: 2.0399775902430215

Epoch: 6| Step: 7
Training loss: 2.0859994888305664
Validation loss: 2.046600083510081

Epoch: 6| Step: 8
Training loss: 1.7751425504684448
Validation loss: 2.053330918153127

Epoch: 6| Step: 9
Training loss: 2.376128911972046
Validation loss: 2.051564653714498

Epoch: 6| Step: 10
Training loss: 2.456909656524658
Validation loss: 2.0433051784833274

Epoch: 6| Step: 11
Training loss: 2.726618766784668
Validation loss: 2.0447558561960855

Epoch: 6| Step: 12
Training loss: 2.0102782249450684
Validation loss: 2.038155754407247

Epoch: 6| Step: 13
Training loss: 2.1411921977996826
Validation loss: 2.0457814733187356

Epoch: 117| Step: 0
Training loss: 2.0855650901794434
Validation loss: 2.0444189508756003

Epoch: 6| Step: 1
Training loss: 2.2071104049682617
Validation loss: 2.033058305581411

Epoch: 6| Step: 2
Training loss: 1.9197180271148682
Validation loss: 2.02774312098821

Epoch: 6| Step: 3
Training loss: 2.722160577774048
Validation loss: 2.022981564203898

Epoch: 6| Step: 4
Training loss: 1.9019469022750854
Validation loss: 2.018963317076365

Epoch: 6| Step: 5
Training loss: 1.6610629558563232
Validation loss: 2.0087537368138633

Epoch: 6| Step: 6
Training loss: 1.7736434936523438
Validation loss: 2.007868528366089

Epoch: 6| Step: 7
Training loss: 2.1921730041503906
Validation loss: 2.022192339102427

Epoch: 6| Step: 8
Training loss: 2.5793819427490234
Validation loss: 2.021771252155304

Epoch: 6| Step: 9
Training loss: 2.299309730529785
Validation loss: 2.0350961287816367

Epoch: 6| Step: 10
Training loss: 1.7669817209243774
Validation loss: 2.0290818214416504

Epoch: 6| Step: 11
Training loss: 2.7232437133789062
Validation loss: 2.0220642685890198

Epoch: 6| Step: 12
Training loss: 1.8739268779754639
Validation loss: 2.0224139293034873

Epoch: 6| Step: 13
Training loss: 2.291792869567871
Validation loss: 2.0089792211850486

Epoch: 118| Step: 0
Training loss: 2.4038567543029785
Validation loss: 1.9987155596415203

Epoch: 6| Step: 1
Training loss: 2.0950777530670166
Validation loss: 1.9993797341982524

Epoch: 6| Step: 2
Training loss: 2.0784430503845215
Validation loss: 2.000942269961039

Epoch: 6| Step: 3
Training loss: 2.2891149520874023
Validation loss: 2.009514093399048

Epoch: 6| Step: 4
Training loss: 2.278045177459717
Validation loss: 2.0125885208447776

Epoch: 6| Step: 5
Training loss: 1.9097833633422852
Validation loss: 2.024614612261454

Epoch: 6| Step: 6
Training loss: 2.2791924476623535
Validation loss: 2.0353087186813354

Epoch: 6| Step: 7
Training loss: 1.9890891313552856
Validation loss: 2.0296735366185508

Epoch: 6| Step: 8
Training loss: 2.3239998817443848
Validation loss: 2.0390227834383645

Epoch: 6| Step: 9
Training loss: 2.889347791671753
Validation loss: 2.04543928305308

Epoch: 6| Step: 10
Training loss: 1.381462812423706
Validation loss: 2.044616440931956

Epoch: 6| Step: 11
Training loss: 2.050907850265503
Validation loss: 2.052417238553365

Epoch: 6| Step: 12
Training loss: 1.7296431064605713
Validation loss: 2.050326963265737

Epoch: 6| Step: 13
Training loss: 1.833310842514038
Validation loss: 2.0439828634262085

Epoch: 119| Step: 0
Training loss: 1.5990595817565918
Validation loss: 2.0499995152155557

Epoch: 6| Step: 1
Training loss: 1.675501823425293
Validation loss: 2.047282874584198

Epoch: 6| Step: 2
Training loss: 2.243500232696533
Validation loss: 2.0439536770184836

Epoch: 6| Step: 3
Training loss: 2.088730812072754
Validation loss: 2.0513622562090554

Epoch: 6| Step: 4
Training loss: 2.2000105381011963
Validation loss: 2.0572242736816406

Epoch: 6| Step: 5
Training loss: 1.6928730010986328
Validation loss: 2.042653282483419

Epoch: 6| Step: 6
Training loss: 2.229490280151367
Validation loss: 2.0453946391741433

Epoch: 6| Step: 7
Training loss: 1.9084349870681763
Validation loss: 2.043629745642344

Epoch: 6| Step: 8
Training loss: 2.5001235008239746
Validation loss: 2.0443000396092734

Epoch: 6| Step: 9
Training loss: 1.832038164138794
Validation loss: 2.046342968940735

Epoch: 6| Step: 10
Training loss: 3.1062991619110107
Validation loss: 2.046835422515869

Epoch: 6| Step: 11
Training loss: 2.106757879257202
Validation loss: 2.0499225854873657

Epoch: 6| Step: 12
Training loss: 2.4422316551208496
Validation loss: 2.057001054286957

Epoch: 6| Step: 13
Training loss: 1.6914763450622559
Validation loss: 2.052149514357249

Epoch: 120| Step: 0
Training loss: 2.088296413421631
Validation loss: 2.053172469139099

Epoch: 6| Step: 1
Training loss: 1.9672293663024902
Validation loss: 2.04193784793218

Epoch: 6| Step: 2
Training loss: 2.4139561653137207
Validation loss: 2.0466997623443604

Epoch: 6| Step: 3
Training loss: 2.07466721534729
Validation loss: 2.041809320449829

Epoch: 6| Step: 4
Training loss: 2.1651201248168945
Validation loss: 2.0461371143658957

Epoch: 6| Step: 5
Training loss: 1.8998267650604248
Validation loss: 2.041694382826487

Epoch: 6| Step: 6
Training loss: 2.0635006427764893
Validation loss: 2.0398990909258523

Epoch: 6| Step: 7
Training loss: 2.513439178466797
Validation loss: 2.033652047316233

Epoch: 6| Step: 8
Training loss: 2.2648353576660156
Validation loss: 2.0249561866124473

Epoch: 6| Step: 9
Training loss: 2.0545828342437744
Validation loss: 2.028034826119741

Epoch: 6| Step: 10
Training loss: 1.6963309049606323
Validation loss: 2.0265323519706726

Epoch: 6| Step: 11
Training loss: 1.5914056301116943
Validation loss: 2.0255717833836875

Epoch: 6| Step: 12
Training loss: 1.6469744443893433
Validation loss: 2.0310384035110474

Epoch: 6| Step: 13
Training loss: 2.8028388023376465
Validation loss: 2.0358876983324685

Epoch: 121| Step: 0
Training loss: 1.834079384803772
Validation loss: 2.0265990694363913

Epoch: 6| Step: 1
Training loss: 2.295772075653076
Validation loss: 2.0248743295669556

Epoch: 6| Step: 2
Training loss: 2.0206024646759033
Validation loss: 2.0247791012128196

Epoch: 6| Step: 3
Training loss: 2.253758668899536
Validation loss: 2.0386475324630737

Epoch: 6| Step: 4
Training loss: 2.4831581115722656
Validation loss: 2.034193436304728

Epoch: 6| Step: 5
Training loss: 2.2196075916290283
Validation loss: 2.0296961069107056

Epoch: 6| Step: 6
Training loss: 1.6888500452041626
Validation loss: 2.029488265514374

Epoch: 6| Step: 7
Training loss: 2.899264335632324
Validation loss: 2.027888814608256

Epoch: 6| Step: 8
Training loss: 1.6088061332702637
Validation loss: 2.035263995329539

Epoch: 6| Step: 9
Training loss: 1.9529163837432861
Validation loss: 2.0327207247416177

Epoch: 6| Step: 10
Training loss: 1.9118216037750244
Validation loss: 2.032129943370819

Epoch: 6| Step: 11
Training loss: 1.7485432624816895
Validation loss: 2.0314369599024453

Epoch: 6| Step: 12
Training loss: 1.8327727317810059
Validation loss: 2.0397440989812217

Epoch: 6| Step: 13
Training loss: 2.315944194793701
Validation loss: 2.0318881074587503

Epoch: 122| Step: 0
Training loss: 1.9447332620620728
Validation loss: 2.0346519152323403

Epoch: 6| Step: 1
Training loss: 2.4263596534729004
Validation loss: 2.0232542951901755

Epoch: 6| Step: 2
Training loss: 2.0352368354797363
Validation loss: 2.029634336630503

Epoch: 6| Step: 3
Training loss: 1.8966960906982422
Validation loss: 2.031158765157064

Epoch: 6| Step: 4
Training loss: 2.3924355506896973
Validation loss: 2.0262990991274514

Epoch: 6| Step: 5
Training loss: 1.828836441040039
Validation loss: 2.0297166109085083

Epoch: 6| Step: 6
Training loss: 2.1775875091552734
Validation loss: 2.0257599552472434

Epoch: 6| Step: 7
Training loss: 2.6434803009033203
Validation loss: 2.0224446853001914

Epoch: 6| Step: 8
Training loss: 2.1354126930236816
Validation loss: 2.0295042196909585

Epoch: 6| Step: 9
Training loss: 1.9675164222717285
Validation loss: 2.031504452228546

Epoch: 6| Step: 10
Training loss: 2.254471778869629
Validation loss: 2.0228806734085083

Epoch: 6| Step: 11
Training loss: 2.0624825954437256
Validation loss: 2.025410850842794

Epoch: 6| Step: 12
Training loss: 1.1244686841964722
Validation loss: 2.023577888806661

Epoch: 6| Step: 13
Training loss: 2.1450934410095215
Validation loss: 2.032800575097402

Epoch: 123| Step: 0
Training loss: 2.5316765308380127
Validation loss: 2.03367155790329

Epoch: 6| Step: 1
Training loss: 0.7622604370117188
Validation loss: 2.0328396956125894

Epoch: 6| Step: 2
Training loss: 2.1220483779907227
Validation loss: 2.0336089531580606

Epoch: 6| Step: 3
Training loss: 2.142310619354248
Validation loss: 2.0344813466072083

Epoch: 6| Step: 4
Training loss: 2.1589221954345703
Validation loss: 2.038058042526245

Epoch: 6| Step: 5
Training loss: 1.8973309993743896
Validation loss: 2.038045863310496

Epoch: 6| Step: 6
Training loss: 2.3636975288391113
Validation loss: 2.036718169848124

Epoch: 6| Step: 7
Training loss: 2.436664581298828
Validation loss: 2.0310187339782715

Epoch: 6| Step: 8
Training loss: 2.4271678924560547
Validation loss: 2.0252365867296853

Epoch: 6| Step: 9
Training loss: 1.8912925720214844
Validation loss: 2.0290149251619973

Epoch: 6| Step: 10
Training loss: 2.2192792892456055
Validation loss: 2.0327343543370566

Epoch: 6| Step: 11
Training loss: 2.0583457946777344
Validation loss: 2.0358928044637046

Epoch: 6| Step: 12
Training loss: 2.206282615661621
Validation loss: 2.0291211207707724

Epoch: 6| Step: 13
Training loss: 1.8624454736709595
Validation loss: 2.0299352407455444

Epoch: 124| Step: 0
Training loss: 2.4324252605438232
Validation loss: 2.0356022914250693

Epoch: 6| Step: 1
Training loss: 2.309143304824829
Validation loss: 2.026144822438558

Epoch: 6| Step: 2
Training loss: 1.8382233381271362
Validation loss: 2.0259525378545127

Epoch: 6| Step: 3
Training loss: 1.560443639755249
Validation loss: 2.0252427458763123

Epoch: 6| Step: 4
Training loss: 2.002894639968872
Validation loss: 2.0377106269200644

Epoch: 6| Step: 5
Training loss: 2.2214109897613525
Validation loss: 2.0413639545440674

Epoch: 6| Step: 6
Training loss: 1.7081156969070435
Validation loss: 2.0372502406438193

Epoch: 6| Step: 7
Training loss: 1.5997095108032227
Validation loss: 2.0368119875590005

Epoch: 6| Step: 8
Training loss: 2.06258225440979
Validation loss: 2.0330743392308555

Epoch: 6| Step: 9
Training loss: 2.311629056930542
Validation loss: 2.0261811017990112

Epoch: 6| Step: 10
Training loss: 2.4156174659729004
Validation loss: 2.0364824136098227

Epoch: 6| Step: 11
Training loss: 1.713505506515503
Validation loss: 2.033146599928538

Epoch: 6| Step: 12
Training loss: 1.8639332056045532
Validation loss: 2.03873739639918

Epoch: 6| Step: 13
Training loss: 2.6992392539978027
Validation loss: 2.038245439529419

Epoch: 125| Step: 0
Training loss: 2.3106088638305664
Validation loss: 2.0441829760869346

Epoch: 6| Step: 1
Training loss: 2.199819564819336
Validation loss: 2.0378457903862

Epoch: 6| Step: 2
Training loss: 2.7762093544006348
Validation loss: 2.055265227953593

Epoch: 6| Step: 3
Training loss: 2.15779972076416
Validation loss: 2.041486620903015

Epoch: 6| Step: 4
Training loss: 1.5173999071121216
Validation loss: 2.0493661761283875

Epoch: 6| Step: 5
Training loss: 1.5495723485946655
Validation loss: 2.052797774473826

Epoch: 6| Step: 6
Training loss: 1.9548732042312622
Validation loss: 2.056419332822164

Epoch: 6| Step: 7
Training loss: 2.277540683746338
Validation loss: 2.0533071756362915

Epoch: 6| Step: 8
Training loss: 2.165414810180664
Validation loss: 2.045596122741699

Epoch: 6| Step: 9
Training loss: 1.9148929119110107
Validation loss: 2.054746766885122

Epoch: 6| Step: 10
Training loss: 2.1534528732299805
Validation loss: 2.0530628164609275

Epoch: 6| Step: 11
Training loss: 1.4449325799942017
Validation loss: 2.0529674688975015

Epoch: 6| Step: 12
Training loss: 2.3031227588653564
Validation loss: 2.0434763034184775

Epoch: 6| Step: 13
Training loss: 2.0948216915130615
Validation loss: 2.0488326152165732

Epoch: 126| Step: 0
Training loss: 3.0549111366271973
Validation loss: 2.050480544567108

Epoch: 6| Step: 1
Training loss: 2.2474923133850098
Validation loss: 2.0508341590563455

Epoch: 6| Step: 2
Training loss: 1.6222786903381348
Validation loss: 2.0477569301923118

Epoch: 6| Step: 3
Training loss: 2.191349983215332
Validation loss: 2.052466114362081

Epoch: 6| Step: 4
Training loss: 2.1398568153381348
Validation loss: 2.0393572251001992

Epoch: 6| Step: 5
Training loss: 2.3126120567321777
Validation loss: 2.045937200387319

Epoch: 6| Step: 6
Training loss: 2.2033731937408447
Validation loss: 2.0468271374702454

Epoch: 6| Step: 7
Training loss: 2.084743022918701
Validation loss: 2.0502700408299765

Epoch: 6| Step: 8
Training loss: 1.558449625968933
Validation loss: 2.0481915871302285

Epoch: 6| Step: 9
Training loss: 1.845977783203125
Validation loss: 2.0429125825564065

Epoch: 6| Step: 10
Training loss: 2.111768960952759
Validation loss: 2.040647109349569

Epoch: 6| Step: 11
Training loss: 1.5026397705078125
Validation loss: 2.051622529824575

Epoch: 6| Step: 12
Training loss: 1.889817714691162
Validation loss: 2.048667550086975

Epoch: 6| Step: 13
Training loss: 1.9081107378005981
Validation loss: 2.0500797629356384

Epoch: 127| Step: 0
Training loss: 1.9562807083129883
Validation loss: 2.0538983941078186

Epoch: 6| Step: 1
Training loss: 1.5245081186294556
Validation loss: 2.040547251701355

Epoch: 6| Step: 2
Training loss: 2.6434221267700195
Validation loss: 2.061624983946482

Epoch: 6| Step: 3
Training loss: 1.9686028957366943
Validation loss: 2.0522421995798745

Epoch: 6| Step: 4
Training loss: 1.4106148481369019
Validation loss: 2.0551185607910156

Epoch: 6| Step: 5
Training loss: 2.0339698791503906
Validation loss: 2.0562124053637185

Epoch: 6| Step: 6
Training loss: 2.330965042114258
Validation loss: 2.0443225105603537

Epoch: 6| Step: 7
Training loss: 2.6711339950561523
Validation loss: 2.04728893438975

Epoch: 6| Step: 8
Training loss: 2.1146037578582764
Validation loss: 2.058402677377065

Epoch: 6| Step: 9
Training loss: 1.4606549739837646
Validation loss: 2.0461233456929526

Epoch: 6| Step: 10
Training loss: 1.9762548208236694
Validation loss: 2.05555792649587

Epoch: 6| Step: 11
Training loss: 1.959686279296875
Validation loss: 2.0547858277956643

Epoch: 6| Step: 12
Training loss: 2.300936222076416
Validation loss: 2.0577332576115928

Epoch: 6| Step: 13
Training loss: 2.140594959259033
Validation loss: 2.0585671265920005

Epoch: 128| Step: 0
Training loss: 2.0427863597869873
Validation loss: 2.066817303498586

Epoch: 6| Step: 1
Training loss: 2.046919345855713
Validation loss: 2.053471406300863

Epoch: 6| Step: 2
Training loss: 2.347555160522461
Validation loss: 2.0567660331726074

Epoch: 6| Step: 3
Training loss: 2.676520824432373
Validation loss: 2.0569281578063965

Epoch: 6| Step: 4
Training loss: 1.459446907043457
Validation loss: 2.054806133111318

Epoch: 6| Step: 5
Training loss: 1.719351053237915
Validation loss: 2.061398426691691

Epoch: 6| Step: 6
Training loss: 2.3380532264709473
Validation loss: 2.0653388102849326

Epoch: 6| Step: 7
Training loss: 2.6301310062408447
Validation loss: 2.066100458304087

Epoch: 6| Step: 8
Training loss: 1.504071593284607
Validation loss: 2.056646466255188

Epoch: 6| Step: 9
Training loss: 1.8972587585449219
Validation loss: 2.0545740127563477

Epoch: 6| Step: 10
Training loss: 1.9147510528564453
Validation loss: 2.0553524494171143

Epoch: 6| Step: 11
Training loss: 1.9597680568695068
Validation loss: 2.0491633216540017

Epoch: 6| Step: 12
Training loss: 1.9054412841796875
Validation loss: 2.0501460234324136

Epoch: 6| Step: 13
Training loss: 2.3388512134552
Validation loss: 2.0577419797579446

Epoch: 129| Step: 0
Training loss: 2.1438379287719727
Validation loss: 2.063332279523214

Epoch: 6| Step: 1
Training loss: 1.6496223211288452
Validation loss: 2.0595623254776

Epoch: 6| Step: 2
Training loss: 2.138505220413208
Validation loss: 2.0601399739583335

Epoch: 6| Step: 3
Training loss: 1.841886281967163
Validation loss: 2.0617265899976096

Epoch: 6| Step: 4
Training loss: 1.9941205978393555
Validation loss: 2.054762303829193

Epoch: 6| Step: 5
Training loss: 2.233867645263672
Validation loss: 2.0559174021085105

Epoch: 6| Step: 6
Training loss: 1.9402086734771729
Validation loss: 2.0634859204292297

Epoch: 6| Step: 7
Training loss: 2.0257668495178223
Validation loss: 2.0680973529815674

Epoch: 6| Step: 8
Training loss: 1.8932044506072998
Validation loss: 2.0563563108444214

Epoch: 6| Step: 9
Training loss: 1.9923081398010254
Validation loss: 2.059341390927633

Epoch: 6| Step: 10
Training loss: 2.499375820159912
Validation loss: 2.0600680708885193

Epoch: 6| Step: 11
Training loss: 2.001887798309326
Validation loss: 2.064411183198293

Epoch: 6| Step: 12
Training loss: 2.269249439239502
Validation loss: 2.05980517466863

Epoch: 6| Step: 13
Training loss: 2.31112003326416
Validation loss: 2.0593838691711426

Epoch: 130| Step: 0
Training loss: 2.002614736557007
Validation loss: 2.058149774869283

Epoch: 6| Step: 1
Training loss: 2.9518303871154785
Validation loss: 2.065298616886139

Epoch: 6| Step: 2
Training loss: 1.6980559825897217
Validation loss: 2.069959362347921

Epoch: 6| Step: 3
Training loss: 1.6561353206634521
Validation loss: 2.066016435623169

Epoch: 6| Step: 4
Training loss: 1.8992938995361328
Validation loss: 2.072897493839264

Epoch: 6| Step: 5
Training loss: 1.6206445693969727
Validation loss: 2.0660813053448996

Epoch: 6| Step: 6
Training loss: 2.760786533355713
Validation loss: 2.0585760871569314

Epoch: 6| Step: 7
Training loss: 2.571514129638672
Validation loss: 2.045221527417501

Epoch: 6| Step: 8
Training loss: 1.7389144897460938
Validation loss: 2.0490464766820273

Epoch: 6| Step: 9
Training loss: 2.2344207763671875
Validation loss: 2.0454601446787515

Epoch: 6| Step: 10
Training loss: 1.4903265237808228
Validation loss: 2.0403935313224792

Epoch: 6| Step: 11
Training loss: 2.0991058349609375
Validation loss: 2.0374689300855002

Epoch: 6| Step: 12
Training loss: 2.5345845222473145
Validation loss: 2.032878041267395

Epoch: 6| Step: 13
Training loss: 1.3255696296691895
Validation loss: 2.0344745914141336

Epoch: 131| Step: 0
Training loss: 1.758363127708435
Validation loss: 2.0262404084205627

Epoch: 6| Step: 1
Training loss: 2.3123867511749268
Validation loss: 2.0296162366867065

Epoch: 6| Step: 2
Training loss: 2.523702621459961
Validation loss: 2.026474893093109

Epoch: 6| Step: 3
Training loss: 1.150660753250122
Validation loss: 2.0382737517356873

Epoch: 6| Step: 4
Training loss: 1.8906199932098389
Validation loss: 2.0319654742876687

Epoch: 6| Step: 5
Training loss: 2.684067726135254
Validation loss: 2.0346880753835044

Epoch: 6| Step: 6
Training loss: 1.7011973857879639
Validation loss: 2.0384010473887124

Epoch: 6| Step: 7
Training loss: 2.2297089099884033
Validation loss: 2.0403414964675903

Epoch: 6| Step: 8
Training loss: 2.020528793334961
Validation loss: 2.046740492184957

Epoch: 6| Step: 9
Training loss: 1.9716150760650635
Validation loss: 2.0586714347203574

Epoch: 6| Step: 10
Training loss: 2.255474090576172
Validation loss: 2.0441232721010842

Epoch: 6| Step: 11
Training loss: 2.36163330078125
Validation loss: 2.0473983883857727

Epoch: 6| Step: 12
Training loss: 2.049816131591797
Validation loss: 2.0512115160624185

Epoch: 6| Step: 13
Training loss: 1.8717130422592163
Validation loss: 2.0499899784723916

Epoch: 132| Step: 0
Training loss: 1.9489686489105225
Validation loss: 2.057497501373291

Epoch: 6| Step: 1
Training loss: 1.1435267925262451
Validation loss: 2.04969189564387

Epoch: 6| Step: 2
Training loss: 2.9359567165374756
Validation loss: 2.0506864190101624

Epoch: 6| Step: 3
Training loss: 2.409281015396118
Validation loss: 2.0459805727005005

Epoch: 6| Step: 4
Training loss: 2.0260767936706543
Validation loss: 2.0462200045585632

Epoch: 6| Step: 5
Training loss: 1.5424814224243164
Validation loss: 2.0557055274645486

Epoch: 6| Step: 6
Training loss: 1.9149233102798462
Validation loss: 2.060860335826874

Epoch: 6| Step: 7
Training loss: 2.1390814781188965
Validation loss: 2.0539652506510415

Epoch: 6| Step: 8
Training loss: 2.0048134326934814
Validation loss: 2.0703593095143638

Epoch: 6| Step: 9
Training loss: 2.6699538230895996
Validation loss: 2.0546971956888833

Epoch: 6| Step: 10
Training loss: 2.055467128753662
Validation loss: 2.0678510467211404

Epoch: 6| Step: 11
Training loss: 2.0854880809783936
Validation loss: 2.0616912245750427

Epoch: 6| Step: 12
Training loss: 2.2687487602233887
Validation loss: 2.0654128392537436

Epoch: 6| Step: 13
Training loss: 1.4612011909484863
Validation loss: 2.0738315184911094

Epoch: 133| Step: 0
Training loss: 2.2377543449401855
Validation loss: 2.066065490245819

Epoch: 6| Step: 1
Training loss: 1.6650099754333496
Validation loss: 2.068330625693003

Epoch: 6| Step: 2
Training loss: 1.8996566534042358
Validation loss: 2.076419949531555

Epoch: 6| Step: 3
Training loss: 1.6780468225479126
Validation loss: 2.067349433898926

Epoch: 6| Step: 4
Training loss: 2.214414358139038
Validation loss: 2.0800168911616006

Epoch: 6| Step: 5
Training loss: 2.275655508041382
Validation loss: 2.0690792004267373

Epoch: 6| Step: 6
Training loss: 2.573831796646118
Validation loss: 2.0806623299916587

Epoch: 6| Step: 7
Training loss: 1.9198695421218872
Validation loss: 2.0715455611546836

Epoch: 6| Step: 8
Training loss: 2.063941240310669
Validation loss: 2.080373783906301

Epoch: 6| Step: 9
Training loss: 1.6675726175308228
Validation loss: 2.0680933594703674

Epoch: 6| Step: 10
Training loss: 2.5471303462982178
Validation loss: 2.0699996749560037

Epoch: 6| Step: 11
Training loss: 1.605337142944336
Validation loss: 2.075869878133138

Epoch: 6| Step: 12
Training loss: 2.1068973541259766
Validation loss: 2.073384245236715

Epoch: 6| Step: 13
Training loss: 2.115938663482666
Validation loss: 2.0704511602719626

Epoch: 134| Step: 0
Training loss: 2.9163401126861572
Validation loss: 2.0717175801595054

Epoch: 6| Step: 1
Training loss: 1.5907241106033325
Validation loss: 2.0686598817507424

Epoch: 6| Step: 2
Training loss: 1.8815371990203857
Validation loss: 2.06465478738149

Epoch: 6| Step: 3
Training loss: 2.8438239097595215
Validation loss: 2.0653327902158103

Epoch: 6| Step: 4
Training loss: 1.650876522064209
Validation loss: 2.064046005407969

Epoch: 6| Step: 5
Training loss: 2.016146421432495
Validation loss: 2.064860761165619

Epoch: 6| Step: 6
Training loss: 1.996609091758728
Validation loss: 2.0639540553092957

Epoch: 6| Step: 7
Training loss: 1.937646746635437
Validation loss: 2.063479502995809

Epoch: 6| Step: 8
Training loss: 2.031219959259033
Validation loss: 2.0653909047444663

Epoch: 6| Step: 9
Training loss: 2.256831169128418
Validation loss: 2.069338242212931

Epoch: 6| Step: 10
Training loss: 2.2457756996154785
Validation loss: 2.0667569041252136

Epoch: 6| Step: 11
Training loss: 1.612883448600769
Validation loss: 2.0691136916478476

Epoch: 6| Step: 12
Training loss: 1.3579260110855103
Validation loss: 2.058472474416097

Epoch: 6| Step: 13
Training loss: 2.160385847091675
Validation loss: 2.0682179927825928

Epoch: 135| Step: 0
Training loss: 1.807215690612793
Validation loss: 2.0645392338434854

Epoch: 6| Step: 1
Training loss: 2.5063390731811523
Validation loss: 2.0604662895202637

Epoch: 6| Step: 2
Training loss: 1.7149869203567505
Validation loss: 2.0559635361035666

Epoch: 6| Step: 3
Training loss: 2.564239501953125
Validation loss: 2.055211285750071

Epoch: 6| Step: 4
Training loss: 1.9033877849578857
Validation loss: 2.04985644419988

Epoch: 6| Step: 5
Training loss: 1.8485286235809326
Validation loss: 2.059219936529795

Epoch: 6| Step: 6
Training loss: 2.230259895324707
Validation loss: 2.0575873851776123

Epoch: 6| Step: 7
Training loss: 2.432084083557129
Validation loss: 2.0748090942700705

Epoch: 6| Step: 8
Training loss: 2.1475577354431152
Validation loss: 2.076340635617574

Epoch: 6| Step: 9
Training loss: 1.9005295038223267
Validation loss: 2.08335546652476

Epoch: 6| Step: 10
Training loss: 1.6542599201202393
Validation loss: 2.081390996774038

Epoch: 6| Step: 11
Training loss: 1.759508490562439
Validation loss: 2.0742253263791404

Epoch: 6| Step: 12
Training loss: 2.0499472618103027
Validation loss: 2.0857239762941995

Epoch: 6| Step: 13
Training loss: 1.8950804471969604
Validation loss: 2.0832154750823975

Epoch: 136| Step: 0
Training loss: 2.044299602508545
Validation loss: 2.086068868637085

Epoch: 6| Step: 1
Training loss: 2.4687113761901855
Validation loss: 2.069576938947042

Epoch: 6| Step: 2
Training loss: 2.2875967025756836
Validation loss: 2.0637412468592324

Epoch: 6| Step: 3
Training loss: 2.367607593536377
Validation loss: 2.0719114939371743

Epoch: 6| Step: 4
Training loss: 2.0920181274414062
Validation loss: 2.059894839922587

Epoch: 6| Step: 5
Training loss: 1.9781908988952637
Validation loss: 2.061756511529287

Epoch: 6| Step: 6
Training loss: 1.8635354042053223
Validation loss: 2.06288081407547

Epoch: 6| Step: 7
Training loss: 1.6890121698379517
Validation loss: 2.0642645359039307

Epoch: 6| Step: 8
Training loss: 1.384473204612732
Validation loss: 2.0554118355115256

Epoch: 6| Step: 9
Training loss: 2.116621494293213
Validation loss: 2.056070546309153

Epoch: 6| Step: 10
Training loss: 1.8772108554840088
Validation loss: 2.0661706924438477

Epoch: 6| Step: 11
Training loss: 1.9478871822357178
Validation loss: 2.0699378649393716

Epoch: 6| Step: 12
Training loss: 2.298412322998047
Validation loss: 2.07973176240921

Epoch: 6| Step: 13
Training loss: 2.286045551300049
Validation loss: 2.079042832056681

Epoch: 137| Step: 0
Training loss: 2.267185926437378
Validation loss: 2.098361015319824

Epoch: 6| Step: 1
Training loss: 2.188324213027954
Validation loss: 2.0962714155515036

Epoch: 6| Step: 2
Training loss: 1.9886971712112427
Validation loss: 2.0875948667526245

Epoch: 6| Step: 3
Training loss: 2.0616703033447266
Validation loss: 2.0890870690345764

Epoch: 6| Step: 4
Training loss: 1.9272410869598389
Validation loss: 2.078925132751465

Epoch: 6| Step: 5
Training loss: 1.890128493309021
Validation loss: 2.084391236305237

Epoch: 6| Step: 6
Training loss: 1.927025318145752
Validation loss: 2.0697480837504068

Epoch: 6| Step: 7
Training loss: 1.8755407333374023
Validation loss: 2.071222205956777

Epoch: 6| Step: 8
Training loss: 2.4207897186279297
Validation loss: 2.079364458719889

Epoch: 6| Step: 9
Training loss: 1.6816742420196533
Validation loss: 2.075739065806071

Epoch: 6| Step: 10
Training loss: 2.0022215843200684
Validation loss: 2.087642192840576

Epoch: 6| Step: 11
Training loss: 2.0717272758483887
Validation loss: 2.0952345530192056

Epoch: 6| Step: 12
Training loss: 2.090487003326416
Validation loss: 2.0813201665878296

Epoch: 6| Step: 13
Training loss: 2.0980377197265625
Validation loss: 2.0819344917933145

Epoch: 138| Step: 0
Training loss: 2.0942273139953613
Validation loss: 2.086876074473063

Epoch: 6| Step: 1
Training loss: 2.4826161861419678
Validation loss: 2.075529714425405

Epoch: 6| Step: 2
Training loss: 1.7778379917144775
Validation loss: 2.0614901979764304

Epoch: 6| Step: 3
Training loss: 2.3315823078155518
Validation loss: 2.0652220050493875

Epoch: 6| Step: 4
Training loss: 2.5840864181518555
Validation loss: 2.0540016889572144

Epoch: 6| Step: 5
Training loss: 2.109747886657715
Validation loss: 2.0739847819010415

Epoch: 6| Step: 6
Training loss: 2.337491035461426
Validation loss: 2.0770710905392966

Epoch: 6| Step: 7
Training loss: 1.5118670463562012
Validation loss: 2.0797064900398254

Epoch: 6| Step: 8
Training loss: 1.6524397134780884
Validation loss: 2.081059694290161

Epoch: 6| Step: 9
Training loss: 1.793737769126892
Validation loss: 2.081138769785563

Epoch: 6| Step: 10
Training loss: 1.8228306770324707
Validation loss: 2.0879995624224343

Epoch: 6| Step: 11
Training loss: 1.8076508045196533
Validation loss: 2.0805975596110025

Epoch: 6| Step: 12
Training loss: 2.3806509971618652
Validation loss: 2.083484649658203

Epoch: 6| Step: 13
Training loss: 1.6311383247375488
Validation loss: 2.076968252658844

Epoch: 139| Step: 0
Training loss: 2.2176575660705566
Validation loss: 2.075350522994995

Epoch: 6| Step: 1
Training loss: 2.1218552589416504
Validation loss: 2.087558110555013

Epoch: 6| Step: 2
Training loss: 1.8953245878219604
Validation loss: 2.0926562349001565

Epoch: 6| Step: 3
Training loss: 2.2915115356445312
Validation loss: 2.0807656049728394

Epoch: 6| Step: 4
Training loss: 1.944211483001709
Validation loss: 2.0665342609087625

Epoch: 6| Step: 5
Training loss: 1.4311197996139526
Validation loss: 2.0704330603281655

Epoch: 6| Step: 6
Training loss: 2.148571252822876
Validation loss: 2.0706229408582053

Epoch: 6| Step: 7
Training loss: 2.1392972469329834
Validation loss: 2.0660967429478965

Epoch: 6| Step: 8
Training loss: 1.867498755455017
Validation loss: 2.0765005350112915

Epoch: 6| Step: 9
Training loss: 2.2279775142669678
Validation loss: 2.079156279563904

Epoch: 6| Step: 10
Training loss: 1.4492228031158447
Validation loss: 2.0940234859784446

Epoch: 6| Step: 11
Training loss: 1.9618536233901978
Validation loss: 2.0914345383644104

Epoch: 6| Step: 12
Training loss: 1.9993751049041748
Validation loss: 2.091892123222351

Epoch: 6| Step: 13
Training loss: 2.600961685180664
Validation loss: 2.0965497692426047

Epoch: 140| Step: 0
Training loss: 2.3673832416534424
Validation loss: 2.11906627813975

Epoch: 6| Step: 1
Training loss: 1.6409209966659546
Validation loss: 2.102686087290446

Epoch: 6| Step: 2
Training loss: 1.4906673431396484
Validation loss: 2.110559364159902

Epoch: 6| Step: 3
Training loss: 1.592376947402954
Validation loss: 2.1158195535341897

Epoch: 6| Step: 4
Training loss: 2.2597014904022217
Validation loss: 2.10610294342041

Epoch: 6| Step: 5
Training loss: 2.327871322631836
Validation loss: 2.09769997994105

Epoch: 6| Step: 6
Training loss: 2.2945139408111572
Validation loss: 2.105465849240621

Epoch: 6| Step: 7
Training loss: 1.6027109622955322
Validation loss: 2.0956380367279053

Epoch: 6| Step: 8
Training loss: 2.5947680473327637
Validation loss: 2.0875331362088523

Epoch: 6| Step: 9
Training loss: 1.4559376239776611
Validation loss: 2.078933854897817

Epoch: 6| Step: 10
Training loss: 2.3950390815734863
Validation loss: 2.092478076616923

Epoch: 6| Step: 11
Training loss: 2.038355588912964
Validation loss: 2.070762038230896

Epoch: 6| Step: 12
Training loss: 2.506432294845581
Validation loss: 2.0818628668785095

Epoch: 6| Step: 13
Training loss: 1.7854704856872559
Validation loss: 2.071347196896871

Epoch: 141| Step: 0
Training loss: 2.0176498889923096
Validation loss: 2.070597072442373

Epoch: 6| Step: 1
Training loss: 2.1420645713806152
Validation loss: 2.0704854130744934

Epoch: 6| Step: 2
Training loss: 2.251295328140259
Validation loss: 2.073529322942098

Epoch: 6| Step: 3
Training loss: 2.114108085632324
Validation loss: 2.063351035118103

Epoch: 6| Step: 4
Training loss: 1.6128648519515991
Validation loss: 2.068907936414083

Epoch: 6| Step: 5
Training loss: 2.334287166595459
Validation loss: 2.073358952999115

Epoch: 6| Step: 6
Training loss: 1.8316307067871094
Validation loss: 2.0802738467852273

Epoch: 6| Step: 7
Training loss: 2.5071496963500977
Validation loss: 2.0878811279932656

Epoch: 6| Step: 8
Training loss: 1.5025306940078735
Validation loss: 2.0839749574661255

Epoch: 6| Step: 9
Training loss: 1.266649842262268
Validation loss: 2.1015023986498513

Epoch: 6| Step: 10
Training loss: 2.9600305557250977
Validation loss: 2.1020694375038147

Epoch: 6| Step: 11
Training loss: 1.5836358070373535
Validation loss: 2.101424733797709

Epoch: 6| Step: 12
Training loss: 1.5351669788360596
Validation loss: 2.110776682694753

Epoch: 6| Step: 13
Training loss: 2.597774028778076
Validation loss: 2.0969082911809287

Epoch: 142| Step: 0
Training loss: 1.7218573093414307
Validation loss: 2.103814959526062

Epoch: 6| Step: 1
Training loss: 2.4148149490356445
Validation loss: 2.115573247273763

Epoch: 6| Step: 2
Training loss: 2.114267110824585
Validation loss: 2.09340234597524

Epoch: 6| Step: 3
Training loss: 2.561713695526123
Validation loss: 2.0898029804229736

Epoch: 6| Step: 4
Training loss: 1.617653250694275
Validation loss: 2.0899035930633545

Epoch: 6| Step: 5
Training loss: 1.7974194288253784
Validation loss: 2.0826904575030007

Epoch: 6| Step: 6
Training loss: 1.338862657546997
Validation loss: 2.096588611602783

Epoch: 6| Step: 7
Training loss: 2.1350502967834473
Validation loss: 2.089039623737335

Epoch: 6| Step: 8
Training loss: 2.1876699924468994
Validation loss: 2.092008352279663

Epoch: 6| Step: 9
Training loss: 2.0699427127838135
Validation loss: 2.092558979988098

Epoch: 6| Step: 10
Training loss: 2.399026393890381
Validation loss: 2.096665322780609

Epoch: 6| Step: 11
Training loss: 2.221529006958008
Validation loss: 2.092120051383972

Epoch: 6| Step: 12
Training loss: 1.3916786909103394
Validation loss: 2.0947272777557373

Epoch: 6| Step: 13
Training loss: 2.3853607177734375
Validation loss: 2.1110644737879434

Epoch: 143| Step: 0
Training loss: 2.052825927734375
Validation loss: 2.102678815523783

Epoch: 6| Step: 1
Training loss: 2.5729730129241943
Validation loss: 2.0952531894048056

Epoch: 6| Step: 2
Training loss: 2.542816162109375
Validation loss: 2.091169794400533

Epoch: 6| Step: 3
Training loss: 1.7026734352111816
Validation loss: 2.097206155459086

Epoch: 6| Step: 4
Training loss: 2.2124714851379395
Validation loss: 2.100016494592031

Epoch: 6| Step: 5
Training loss: 2.321848154067993
Validation loss: 2.0885326663653054

Epoch: 6| Step: 6
Training loss: 1.2756853103637695
Validation loss: 2.0903491576512656

Epoch: 6| Step: 7
Training loss: 2.426257610321045
Validation loss: 2.099233607451121

Epoch: 6| Step: 8
Training loss: 1.6827685832977295
Validation loss: 2.0826080044110618

Epoch: 6| Step: 9
Training loss: 1.6173425912857056
Validation loss: 2.0855812629063926

Epoch: 6| Step: 10
Training loss: 2.156555652618408
Validation loss: 2.0783087412516275

Epoch: 6| Step: 11
Training loss: 1.8664432764053345
Validation loss: 2.0758338967959085

Epoch: 6| Step: 12
Training loss: 2.0929672718048096
Validation loss: 2.0629234313964844

Epoch: 6| Step: 13
Training loss: 1.6350888013839722
Validation loss: 2.0772918065389

Epoch: 144| Step: 0
Training loss: 1.6144402027130127
Validation loss: 2.0755733052889505

Epoch: 6| Step: 1
Training loss: 1.9403467178344727
Validation loss: 2.0818772315979004

Epoch: 6| Step: 2
Training loss: 2.0016064643859863
Validation loss: 2.0710422595342

Epoch: 6| Step: 3
Training loss: 2.483830213546753
Validation loss: 2.082640210787455

Epoch: 6| Step: 4
Training loss: 1.9141005277633667
Validation loss: 2.083993891874949

Epoch: 6| Step: 5
Training loss: 1.9290151596069336
Validation loss: 2.0876436034838357

Epoch: 6| Step: 6
Training loss: 1.6566598415374756
Validation loss: 2.089331646760305

Epoch: 6| Step: 7
Training loss: 1.1568145751953125
Validation loss: 2.091013014316559

Epoch: 6| Step: 8
Training loss: 1.953041672706604
Validation loss: 2.0841227173805237

Epoch: 6| Step: 9
Training loss: 2.805450916290283
Validation loss: 2.0969258546829224

Epoch: 6| Step: 10
Training loss: 2.1739888191223145
Validation loss: 2.10385791460673

Epoch: 6| Step: 11
Training loss: 2.394040107727051
Validation loss: 2.0951417684555054

Epoch: 6| Step: 12
Training loss: 1.9573440551757812
Validation loss: 2.090284585952759

Epoch: 6| Step: 13
Training loss: 2.287992477416992
Validation loss: 2.0912439823150635

Epoch: 145| Step: 0
Training loss: 1.9072041511535645
Validation loss: 2.0835976600646973

Epoch: 6| Step: 1
Training loss: 2.192324638366699
Validation loss: 2.0855416456858316

Epoch: 6| Step: 2
Training loss: 1.9850761890411377
Validation loss: 2.080101191997528

Epoch: 6| Step: 3
Training loss: 1.490504503250122
Validation loss: 2.070640802383423

Epoch: 6| Step: 4
Training loss: 2.1287267208099365
Validation loss: 2.0637547969818115

Epoch: 6| Step: 5
Training loss: 2.1414709091186523
Validation loss: 2.075627624988556

Epoch: 6| Step: 6
Training loss: 2.697310209274292
Validation loss: 2.0634100238482156

Epoch: 6| Step: 7
Training loss: 2.3097078800201416
Validation loss: 2.0566838582356772

Epoch: 6| Step: 8
Training loss: 1.3572927713394165
Validation loss: 2.0581721862157187

Epoch: 6| Step: 9
Training loss: 2.539527416229248
Validation loss: 2.063082536061605

Epoch: 6| Step: 10
Training loss: 1.6139328479766846
Validation loss: 2.06341145435969

Epoch: 6| Step: 11
Training loss: 1.9447349309921265
Validation loss: 2.0561808745066323

Epoch: 6| Step: 12
Training loss: 2.118450880050659
Validation loss: 2.0648512045542398

Epoch: 6| Step: 13
Training loss: 2.112017869949341
Validation loss: 2.073033173878988

Epoch: 146| Step: 0
Training loss: 1.799729347229004
Validation loss: 2.059255619843801

Epoch: 6| Step: 1
Training loss: 1.5990716218948364
Validation loss: 2.073369860649109

Epoch: 6| Step: 2
Training loss: 2.5592124462127686
Validation loss: 2.076717436313629

Epoch: 6| Step: 3
Training loss: 2.434584617614746
Validation loss: 2.0806796550750732

Epoch: 6| Step: 4
Training loss: 1.6903573274612427
Validation loss: 2.081224262714386

Epoch: 6| Step: 5
Training loss: 1.767298698425293
Validation loss: 2.06533282995224

Epoch: 6| Step: 6
Training loss: 2.4522218704223633
Validation loss: 2.0784510374069214

Epoch: 6| Step: 7
Training loss: 1.7350735664367676
Validation loss: 2.0848471323649087

Epoch: 6| Step: 8
Training loss: 2.221519947052002
Validation loss: 2.072069267431895

Epoch: 6| Step: 9
Training loss: 2.00230073928833
Validation loss: 2.0678216020266214

Epoch: 6| Step: 10
Training loss: 1.4884624481201172
Validation loss: 2.068511962890625

Epoch: 6| Step: 11
Training loss: 1.9687576293945312
Validation loss: 2.065939724445343

Epoch: 6| Step: 12
Training loss: 2.47104811668396
Validation loss: 2.0500160654385886

Epoch: 6| Step: 13
Training loss: 2.4602487087249756
Validation loss: 2.04812745253245

Epoch: 147| Step: 0
Training loss: 2.421095848083496
Validation loss: 2.05441681543986

Epoch: 6| Step: 1
Training loss: 1.8108385801315308
Validation loss: 2.051581919193268

Epoch: 6| Step: 2
Training loss: 1.8442131280899048
Validation loss: 2.0632957418759665

Epoch: 6| Step: 3
Training loss: 1.8569958209991455
Validation loss: 2.0608886082967124

Epoch: 6| Step: 4
Training loss: 1.8730709552764893
Validation loss: 2.068866272767385

Epoch: 6| Step: 5
Training loss: 2.500941753387451
Validation loss: 2.0628084341684976

Epoch: 6| Step: 6
Training loss: 1.5634868144989014
Validation loss: 2.0625198682149253

Epoch: 6| Step: 7
Training loss: 2.3379087448120117
Validation loss: 2.0749763449033103

Epoch: 6| Step: 8
Training loss: 2.3257312774658203
Validation loss: 2.0670926769574485

Epoch: 6| Step: 9
Training loss: 2.4186348915100098
Validation loss: 2.072346031665802

Epoch: 6| Step: 10
Training loss: 1.7589678764343262
Validation loss: 2.079298257827759

Epoch: 6| Step: 11
Training loss: 2.0232126712799072
Validation loss: 2.098938465118408

Epoch: 6| Step: 12
Training loss: 1.7262051105499268
Validation loss: 2.092577358086904

Epoch: 6| Step: 13
Training loss: 1.9233074188232422
Validation loss: 2.1130375067392984

Epoch: 148| Step: 0
Training loss: 2.021331787109375
Validation loss: 2.1066253582636514

Epoch: 6| Step: 1
Training loss: 1.814645528793335
Validation loss: 2.1025203466415405

Epoch: 6| Step: 2
Training loss: 1.6204465627670288
Validation loss: 2.102363626162211

Epoch: 6| Step: 3
Training loss: 2.2906274795532227
Validation loss: 2.0933297673861184

Epoch: 6| Step: 4
Training loss: 1.5980502367019653
Validation loss: 2.0998378793398538

Epoch: 6| Step: 5
Training loss: 2.3369228839874268
Validation loss: 2.084114929040273

Epoch: 6| Step: 6
Training loss: 1.82074773311615
Validation loss: 2.0774399439493814

Epoch: 6| Step: 7
Training loss: 2.308399200439453
Validation loss: 2.070503294467926

Epoch: 6| Step: 8
Training loss: 1.9787399768829346
Validation loss: 2.063783844312032

Epoch: 6| Step: 9
Training loss: 2.31168270111084
Validation loss: 2.069015840689341

Epoch: 6| Step: 10
Training loss: 1.590500831604004
Validation loss: 2.0430474082628884

Epoch: 6| Step: 11
Training loss: 2.567659854888916
Validation loss: 2.0562744140625

Epoch: 6| Step: 12
Training loss: 2.4943952560424805
Validation loss: 2.0509426991144815

Epoch: 6| Step: 13
Training loss: 1.6221970319747925
Validation loss: 2.0537532567977905

Epoch: 149| Step: 0
Training loss: 1.8343266248703003
Validation loss: 2.0465295116106668

Epoch: 6| Step: 1
Training loss: 1.8309991359710693
Validation loss: 2.0558634201685586

Epoch: 6| Step: 2
Training loss: 1.997556447982788
Validation loss: 2.0542198618253074

Epoch: 6| Step: 3
Training loss: 1.6181647777557373
Validation loss: 2.0624914169311523

Epoch: 6| Step: 4
Training loss: 2.073917865753174
Validation loss: 2.06695963939031

Epoch: 6| Step: 5
Training loss: 1.8077902793884277
Validation loss: 2.0621262788772583

Epoch: 6| Step: 6
Training loss: 2.243833065032959
Validation loss: 2.0675816337267556

Epoch: 6| Step: 7
Training loss: 1.9457097053527832
Validation loss: 2.073459188143412

Epoch: 6| Step: 8
Training loss: 2.0883209705352783
Validation loss: 2.0807469487190247

Epoch: 6| Step: 9
Training loss: 1.983891248703003
Validation loss: 2.0821744998296103

Epoch: 6| Step: 10
Training loss: 2.240710496902466
Validation loss: 2.093507011731466

Epoch: 6| Step: 11
Training loss: 2.702206611633301
Validation loss: 2.1202754577000937

Epoch: 6| Step: 12
Training loss: 2.2205607891082764
Validation loss: 2.125132660071055

Epoch: 6| Step: 13
Training loss: 1.787898302078247
Validation loss: 2.136270066102346

Epoch: 150| Step: 0
Training loss: 1.9506138563156128
Validation loss: 2.123978614807129

Epoch: 6| Step: 1
Training loss: 2.514340400695801
Validation loss: 2.1273155411084494

Epoch: 6| Step: 2
Training loss: 2.4522242546081543
Validation loss: 2.1458910504976907

Epoch: 6| Step: 3
Training loss: 2.2549281120300293
Validation loss: 2.1182256937026978

Epoch: 6| Step: 4
Training loss: 1.7169417142868042
Validation loss: 2.100222170352936

Epoch: 6| Step: 5
Training loss: 1.753938913345337
Validation loss: 2.090181569258372

Epoch: 6| Step: 6
Training loss: 2.16017484664917
Validation loss: 2.075919588406881

Epoch: 6| Step: 7
Training loss: 1.9547908306121826
Validation loss: 2.0747267405192056

Epoch: 6| Step: 8
Training loss: 2.500880479812622
Validation loss: 2.070239504178365

Epoch: 6| Step: 9
Training loss: 1.8963602781295776
Validation loss: 2.0685077905654907

Epoch: 6| Step: 10
Training loss: 2.0296273231506348
Validation loss: 2.0706475575764975

Epoch: 6| Step: 11
Training loss: 2.4882078170776367
Validation loss: 2.0663547913233438

Epoch: 6| Step: 12
Training loss: 1.9910154342651367
Validation loss: 2.0726341803868613

Epoch: 6| Step: 13
Training loss: 1.724143385887146
Validation loss: 2.070162773132324

Epoch: 151| Step: 0
Training loss: 2.544034481048584
Validation loss: 2.0627361536026

Epoch: 6| Step: 1
Training loss: 1.9947383403778076
Validation loss: 2.0683046181996665

Epoch: 6| Step: 2
Training loss: 1.978135347366333
Validation loss: 2.0706207553545632

Epoch: 6| Step: 3
Training loss: 1.6673434972763062
Validation loss: 2.080899635950724

Epoch: 6| Step: 4
Training loss: 1.7203527688980103
Validation loss: 2.0769983927408853

Epoch: 6| Step: 5
Training loss: 1.708997368812561
Validation loss: 2.0679752826690674

Epoch: 6| Step: 6
Training loss: 2.1867451667785645
Validation loss: 2.069854180018107

Epoch: 6| Step: 7
Training loss: 1.2136592864990234
Validation loss: 2.056272268295288

Epoch: 6| Step: 8
Training loss: 2.866093635559082
Validation loss: 2.063291549682617

Epoch: 6| Step: 9
Training loss: 2.0342750549316406
Validation loss: 2.059967656930288

Epoch: 6| Step: 10
Training loss: 2.0978658199310303
Validation loss: 2.0675930778185525

Epoch: 6| Step: 11
Training loss: 2.409151077270508
Validation loss: 2.073030869166056

Epoch: 6| Step: 12
Training loss: 1.9941389560699463
Validation loss: 2.073773284753164

Epoch: 6| Step: 13
Training loss: 2.139577627182007
Validation loss: 2.0743480920791626

Epoch: 152| Step: 0
Training loss: 1.8130509853363037
Validation loss: 2.0647343397140503

Epoch: 6| Step: 1
Training loss: 1.95433509349823
Validation loss: 2.0730422933896384

Epoch: 6| Step: 2
Training loss: 1.9535491466522217
Validation loss: 2.067971130212148

Epoch: 6| Step: 3
Training loss: 2.0925936698913574
Validation loss: 2.0684338410695395

Epoch: 6| Step: 4
Training loss: 2.2384629249572754
Validation loss: 2.058440307776133

Epoch: 6| Step: 5
Training loss: 2.2328529357910156
Validation loss: 2.064282993475596

Epoch: 6| Step: 6
Training loss: 1.9219598770141602
Validation loss: 2.0656365156173706

Epoch: 6| Step: 7
Training loss: 1.901080846786499
Validation loss: 2.0727113684018454

Epoch: 6| Step: 8
Training loss: 1.9972188472747803
Validation loss: 2.0656650265057883

Epoch: 6| Step: 9
Training loss: 1.9260034561157227
Validation loss: 2.076367417971293

Epoch: 6| Step: 10
Training loss: 1.9506293535232544
Validation loss: 2.079035739103953

Epoch: 6| Step: 11
Training loss: 2.2392709255218506
Validation loss: 2.077601969242096

Epoch: 6| Step: 12
Training loss: 1.884315013885498
Validation loss: 2.072556257247925

Epoch: 6| Step: 13
Training loss: 1.929137110710144
Validation loss: 2.065939803918203

Epoch: 153| Step: 0
Training loss: 2.28102445602417
Validation loss: 2.076667765776316

Epoch: 6| Step: 1
Training loss: 2.200382709503174
Validation loss: 2.0776986479759216

Epoch: 6| Step: 2
Training loss: 2.1972036361694336
Validation loss: 2.0698872407277427

Epoch: 6| Step: 3
Training loss: 1.7669720649719238
Validation loss: 2.066268742084503

Epoch: 6| Step: 4
Training loss: 1.4911367893218994
Validation loss: 2.0838571786880493

Epoch: 6| Step: 5
Training loss: 2.600454092025757
Validation loss: 2.0746650298436484

Epoch: 6| Step: 6
Training loss: 1.5558404922485352
Validation loss: 2.0791768034299216

Epoch: 6| Step: 7
Training loss: 1.9203078746795654
Validation loss: 2.0860925714174905

Epoch: 6| Step: 8
Training loss: 1.8148009777069092
Validation loss: 2.084934194882711

Epoch: 6| Step: 9
Training loss: 1.878145694732666
Validation loss: 2.092402001221975

Epoch: 6| Step: 10
Training loss: 2.515655755996704
Validation loss: 2.0840192437171936

Epoch: 6| Step: 11
Training loss: 2.2410974502563477
Validation loss: 2.081547280152639

Epoch: 6| Step: 12
Training loss: 1.4695751667022705
Validation loss: 2.080490748087565

Epoch: 6| Step: 13
Training loss: 2.099853038787842
Validation loss: 2.0908572673797607

Epoch: 154| Step: 0
Training loss: 2.168135166168213
Validation loss: 2.085930029551188

Epoch: 6| Step: 1
Training loss: 2.3675787448883057
Validation loss: 2.0821393728256226

Epoch: 6| Step: 2
Training loss: 1.5748258829116821
Validation loss: 2.0775460799535117

Epoch: 6| Step: 3
Training loss: 2.1229357719421387
Validation loss: 2.082026998202006

Epoch: 6| Step: 4
Training loss: 1.5992965698242188
Validation loss: 2.070515513420105

Epoch: 6| Step: 5
Training loss: 1.6029672622680664
Validation loss: 2.0744261344273887

Epoch: 6| Step: 6
Training loss: 2.532899856567383
Validation loss: 2.1024580200513205

Epoch: 6| Step: 7
Training loss: 2.015974521636963
Validation loss: 2.1004557609558105

Epoch: 6| Step: 8
Training loss: 2.081102132797241
Validation loss: 2.096384565035502

Epoch: 6| Step: 9
Training loss: 2.0033719539642334
Validation loss: 2.0876806378364563

Epoch: 6| Step: 10
Training loss: 1.9432423114776611
Validation loss: 2.0885506868362427

Epoch: 6| Step: 11
Training loss: 1.7560127973556519
Validation loss: 2.0843751629193625

Epoch: 6| Step: 12
Training loss: 2.036940813064575
Validation loss: 2.0882420738538108

Epoch: 6| Step: 13
Training loss: 1.9957685470581055
Validation loss: 2.086609204610189

Epoch: 155| Step: 0
Training loss: 2.7827224731445312
Validation loss: 2.076504329840342

Epoch: 6| Step: 1
Training loss: 1.4989256858825684
Validation loss: 2.0799638430277505

Epoch: 6| Step: 2
Training loss: 2.0377695560455322
Validation loss: 2.0884213050206504

Epoch: 6| Step: 3
Training loss: 2.1815948486328125
Validation loss: 2.0933247804641724

Epoch: 6| Step: 4
Training loss: 1.832331657409668
Validation loss: 2.110162913799286

Epoch: 6| Step: 5
Training loss: 1.9325029850006104
Validation loss: 2.106974502404531

Epoch: 6| Step: 6
Training loss: 2.3430256843566895
Validation loss: 2.0962527990341187

Epoch: 6| Step: 7
Training loss: 1.9278126955032349
Validation loss: 2.0843340158462524

Epoch: 6| Step: 8
Training loss: 1.3774203062057495
Validation loss: 2.090421279271444

Epoch: 6| Step: 9
Training loss: 1.8395147323608398
Validation loss: 2.088282068570455

Epoch: 6| Step: 10
Training loss: 1.5190935134887695
Validation loss: 2.098665714263916

Epoch: 6| Step: 11
Training loss: 2.2121119499206543
Validation loss: 2.0935931404431662

Epoch: 6| Step: 12
Training loss: 1.6100482940673828
Validation loss: 2.09343550602595

Epoch: 6| Step: 13
Training loss: 2.677309989929199
Validation loss: 2.091201742490133

Epoch: 156| Step: 0
Training loss: 1.950591802597046
Validation loss: 2.0888959566752114

Epoch: 6| Step: 1
Training loss: 2.992523670196533
Validation loss: 2.101986209551493

Epoch: 6| Step: 2
Training loss: 2.1592893600463867
Validation loss: 2.081979771455129

Epoch: 6| Step: 3
Training loss: 2.3800199031829834
Validation loss: 2.092455565929413

Epoch: 6| Step: 4
Training loss: 1.5986676216125488
Validation loss: 2.09951913356781

Epoch: 6| Step: 5
Training loss: 1.852623462677002
Validation loss: 2.0926109751065574

Epoch: 6| Step: 6
Training loss: 2.4691543579101562
Validation loss: 2.095244805018107

Epoch: 6| Step: 7
Training loss: 1.8287216424942017
Validation loss: 2.09182478984197

Epoch: 6| Step: 8
Training loss: 1.789113998413086
Validation loss: 2.078543484210968

Epoch: 6| Step: 9
Training loss: 1.6684949398040771
Validation loss: 2.093523144721985

Epoch: 6| Step: 10
Training loss: 2.3168869018554688
Validation loss: 2.095481554667155

Epoch: 6| Step: 11
Training loss: 1.4885437488555908
Validation loss: 2.0987935264905295

Epoch: 6| Step: 12
Training loss: 1.637087345123291
Validation loss: 2.0858067870140076

Epoch: 6| Step: 13
Training loss: 2.0708837509155273
Validation loss: 2.1047287782033286

Epoch: 157| Step: 0
Training loss: 1.7501685619354248
Validation loss: 2.090288817882538

Epoch: 6| Step: 1
Training loss: 1.6861258745193481
Validation loss: 2.10831747452418

Epoch: 6| Step: 2
Training loss: 2.3681657314300537
Validation loss: 2.095607856909434

Epoch: 6| Step: 3
Training loss: 2.206592082977295
Validation loss: 2.0894062916437783

Epoch: 6| Step: 4
Training loss: 2.0378174781799316
Validation loss: 2.0972693959871926

Epoch: 6| Step: 5
Training loss: 2.1420884132385254
Validation loss: 2.1023232539494834

Epoch: 6| Step: 6
Training loss: 1.3814204931259155
Validation loss: 2.106550912062327

Epoch: 6| Step: 7
Training loss: 1.576545238494873
Validation loss: 2.093895653883616

Epoch: 6| Step: 8
Training loss: 2.3713457584381104
Validation loss: 2.091539899508158

Epoch: 6| Step: 9
Training loss: 1.5916731357574463
Validation loss: 2.095923105875651

Epoch: 6| Step: 10
Training loss: 1.8157875537872314
Validation loss: 2.1000425219535828

Epoch: 6| Step: 11
Training loss: 1.8682861328125
Validation loss: 2.100752512613932

Epoch: 6| Step: 12
Training loss: 2.6795654296875
Validation loss: 2.0908613999684653

Epoch: 6| Step: 13
Training loss: 2.3766427040100098
Validation loss: 2.1084598104159036

Epoch: 158| Step: 0
Training loss: 1.9650118350982666
Validation loss: 2.1012142101923623

Epoch: 6| Step: 1
Training loss: 2.0186328887939453
Validation loss: 2.090260068575541

Epoch: 6| Step: 2
Training loss: 1.7316842079162598
Validation loss: 2.0915525356928506

Epoch: 6| Step: 3
Training loss: 2.3248131275177
Validation loss: 2.0858314434687295

Epoch: 6| Step: 4
Training loss: 2.3572607040405273
Validation loss: 2.100772798061371

Epoch: 6| Step: 5
Training loss: 1.7026996612548828
Validation loss: 2.0991517106691995

Epoch: 6| Step: 6
Training loss: 1.9000041484832764
Validation loss: 2.098870575428009

Epoch: 6| Step: 7
Training loss: 2.306187629699707
Validation loss: 2.1025153199831643

Epoch: 6| Step: 8
Training loss: 1.7478773593902588
Validation loss: 2.106765329837799

Epoch: 6| Step: 9
Training loss: 2.3901820182800293
Validation loss: 2.1082711219787598

Epoch: 6| Step: 10
Training loss: 1.6928232908248901
Validation loss: 2.111639459927877

Epoch: 6| Step: 11
Training loss: 2.0872039794921875
Validation loss: 2.0996960004170737

Epoch: 6| Step: 12
Training loss: 1.2985734939575195
Validation loss: 2.1062281727790833

Epoch: 6| Step: 13
Training loss: 2.324699878692627
Validation loss: 2.095577855904897

Epoch: 159| Step: 0
Training loss: 1.9960682392120361
Validation loss: 2.097865561644236

Epoch: 6| Step: 1
Training loss: 1.6777689456939697
Validation loss: 2.104078769683838

Epoch: 6| Step: 2
Training loss: 1.7192901372909546
Validation loss: 2.1104423999786377

Epoch: 6| Step: 3
Training loss: 2.51789927482605
Validation loss: 2.098238925139109

Epoch: 6| Step: 4
Training loss: 1.7790919542312622
Validation loss: 2.0955689946810403

Epoch: 6| Step: 5
Training loss: 2.0234293937683105
Validation loss: 2.10712730884552

Epoch: 6| Step: 6
Training loss: 1.9361045360565186
Validation loss: 2.098333179950714

Epoch: 6| Step: 7
Training loss: 1.6538538932800293
Validation loss: 2.1056480606396994

Epoch: 6| Step: 8
Training loss: 2.1114883422851562
Validation loss: 2.0985310276349387

Epoch: 6| Step: 9
Training loss: 2.075376510620117
Validation loss: 2.0969714919726052

Epoch: 6| Step: 10
Training loss: 1.999624490737915
Validation loss: 2.1012739737828574

Epoch: 6| Step: 11
Training loss: 2.204824447631836
Validation loss: 2.0955211321512857

Epoch: 6| Step: 12
Training loss: 1.9222791194915771
Validation loss: 2.111396928628286

Epoch: 6| Step: 13
Training loss: 2.082406997680664
Validation loss: 2.106089949607849

Epoch: 160| Step: 0
Training loss: 1.9927258491516113
Validation loss: 2.102381686369578

Epoch: 6| Step: 1
Training loss: 2.0726211071014404
Validation loss: 2.104720671971639

Epoch: 6| Step: 2
Training loss: 1.5514378547668457
Validation loss: 2.109242300192515

Epoch: 6| Step: 3
Training loss: 2.2653493881225586
Validation loss: 2.1262950698534646

Epoch: 6| Step: 4
Training loss: 2.150273323059082
Validation loss: 2.1035433411598206

Epoch: 6| Step: 5
Training loss: 1.8209173679351807
Validation loss: 2.119256913661957

Epoch: 6| Step: 6
Training loss: 2.060601234436035
Validation loss: 2.096500039100647

Epoch: 6| Step: 7
Training loss: 2.0177061557769775
Validation loss: 2.108011841773987

Epoch: 6| Step: 8
Training loss: 2.063666820526123
Validation loss: 2.108248988787333

Epoch: 6| Step: 9
Training loss: 2.1082959175109863
Validation loss: 2.1138418714205423

Epoch: 6| Step: 10
Training loss: 1.496255874633789
Validation loss: 2.1055442094802856

Epoch: 6| Step: 11
Training loss: 1.9081940650939941
Validation loss: 2.1147226095199585

Epoch: 6| Step: 12
Training loss: 1.859255075454712
Validation loss: 2.1109825571378074

Epoch: 6| Step: 13
Training loss: 2.3061842918395996
Validation loss: 2.1049994627634683

Epoch: 161| Step: 0
Training loss: 1.6540651321411133
Validation loss: 2.102659205595652

Epoch: 6| Step: 1
Training loss: 1.5554429292678833
Validation loss: 2.09782874584198

Epoch: 6| Step: 2
Training loss: 1.7232558727264404
Validation loss: 2.0997877518335977

Epoch: 6| Step: 3
Training loss: 2.14882230758667
Validation loss: 2.0826726158459983

Epoch: 6| Step: 4
Training loss: 2.1943113803863525
Validation loss: 2.0883652567863464

Epoch: 6| Step: 5
Training loss: 1.980819582939148
Validation loss: 2.0931902329126992

Epoch: 6| Step: 6
Training loss: 2.2678892612457275
Validation loss: 2.0884079138437905

Epoch: 6| Step: 7
Training loss: 1.939265251159668
Validation loss: 2.085625191529592

Epoch: 6| Step: 8
Training loss: 2.238694429397583
Validation loss: 2.0797993739446006

Epoch: 6| Step: 9
Training loss: 1.6253736019134521
Validation loss: 2.0956512093544006

Epoch: 6| Step: 10
Training loss: 2.4312028884887695
Validation loss: 2.089155117670695

Epoch: 6| Step: 11
Training loss: 1.8879237174987793
Validation loss: 2.097683846950531

Epoch: 6| Step: 12
Training loss: 2.083951711654663
Validation loss: 2.1125876108805337

Epoch: 6| Step: 13
Training loss: 1.8661203384399414
Validation loss: 2.118886490662893

Epoch: 162| Step: 0
Training loss: 2.2524168491363525
Validation loss: 2.1221898198127747

Epoch: 6| Step: 1
Training loss: 1.901637315750122
Validation loss: 2.116601904233297

Epoch: 6| Step: 2
Training loss: 2.170273780822754
Validation loss: 2.1317437092463174

Epoch: 6| Step: 3
Training loss: 2.121976375579834
Validation loss: 2.121552069981893

Epoch: 6| Step: 4
Training loss: 2.095892906188965
Validation loss: 2.1139458219210305

Epoch: 6| Step: 5
Training loss: 1.4185833930969238
Validation loss: 2.1143434246381125

Epoch: 6| Step: 6
Training loss: 2.125659942626953
Validation loss: 2.1154238184293113

Epoch: 6| Step: 7
Training loss: 1.4366786479949951
Validation loss: 2.1036754846572876

Epoch: 6| Step: 8
Training loss: 2.529244899749756
Validation loss: 2.102085510889689

Epoch: 6| Step: 9
Training loss: 1.3070775270462036
Validation loss: 2.0877345204353333

Epoch: 6| Step: 10
Training loss: 2.7836737632751465
Validation loss: 2.102258006731669

Epoch: 6| Step: 11
Training loss: 2.145153284072876
Validation loss: 2.10408882300059

Epoch: 6| Step: 12
Training loss: 1.721914291381836
Validation loss: 2.0960181752840676

Epoch: 6| Step: 13
Training loss: 1.7237449884414673
Validation loss: 2.095066467920939

Epoch: 163| Step: 0
Training loss: 2.7513861656188965
Validation loss: 2.1090336441993713

Epoch: 6| Step: 1
Training loss: 2.03128981590271
Validation loss: 2.1047651171684265

Epoch: 6| Step: 2
Training loss: 1.609813928604126
Validation loss: 2.11097780863444

Epoch: 6| Step: 3
Training loss: 2.495079517364502
Validation loss: 2.114771087964376

Epoch: 6| Step: 4
Training loss: 2.175471544265747
Validation loss: 2.103801687558492

Epoch: 6| Step: 5
Training loss: 2.329416275024414
Validation loss: 2.1055696407953897

Epoch: 6| Step: 6
Training loss: 1.4910016059875488
Validation loss: 2.1079733769098916

Epoch: 6| Step: 7
Training loss: 1.6160860061645508
Validation loss: 2.104137400786082

Epoch: 6| Step: 8
Training loss: 1.7370846271514893
Validation loss: 2.1104310353597007

Epoch: 6| Step: 9
Training loss: 2.0242321491241455
Validation loss: 2.1139300068219504

Epoch: 6| Step: 10
Training loss: 2.135861873626709
Validation loss: 2.1170608599980674

Epoch: 6| Step: 11
Training loss: 1.9058873653411865
Validation loss: 2.123631715774536

Epoch: 6| Step: 12
Training loss: 1.6048831939697266
Validation loss: 2.1114071011543274

Epoch: 6| Step: 13
Training loss: 1.4818164110183716
Validation loss: 2.121665875116984

Epoch: 164| Step: 0
Training loss: 2.5776548385620117
Validation loss: 2.101935168107351

Epoch: 6| Step: 1
Training loss: 1.8563096523284912
Validation loss: 2.1157811482747397

Epoch: 6| Step: 2
Training loss: 1.8701996803283691
Validation loss: 2.1335798104604087

Epoch: 6| Step: 3
Training loss: 1.7731268405914307
Validation loss: 2.1319605906804404

Epoch: 6| Step: 4
Training loss: 2.336888551712036
Validation loss: 2.122433086236318

Epoch: 6| Step: 5
Training loss: 1.4911377429962158
Validation loss: 2.1158894499142966

Epoch: 6| Step: 6
Training loss: 1.6451488733291626
Validation loss: 2.1107611656188965

Epoch: 6| Step: 7
Training loss: 1.0826828479766846
Validation loss: 2.111707925796509

Epoch: 6| Step: 8
Training loss: 1.66417396068573
Validation loss: 2.1234708229700723

Epoch: 6| Step: 9
Training loss: 2.027496099472046
Validation loss: 2.112739106019338

Epoch: 6| Step: 10
Training loss: 1.911413550376892
Validation loss: 2.122170110543569

Epoch: 6| Step: 11
Training loss: 2.5556797981262207
Validation loss: 2.128873864809672

Epoch: 6| Step: 12
Training loss: 1.891014575958252
Validation loss: 2.1236260135968528

Epoch: 6| Step: 13
Training loss: 2.8293557167053223
Validation loss: 2.1157913406689963

Epoch: 165| Step: 0
Training loss: 1.69158935546875
Validation loss: 2.1068095366160073

Epoch: 6| Step: 1
Training loss: 2.0020031929016113
Validation loss: 2.114229917526245

Epoch: 6| Step: 2
Training loss: 1.7656973600387573
Validation loss: 2.1065303484598794

Epoch: 6| Step: 3
Training loss: 2.2466704845428467
Validation loss: 2.1090936859448752

Epoch: 6| Step: 4
Training loss: 1.6592357158660889
Validation loss: 2.096383055051168

Epoch: 6| Step: 5
Training loss: 2.030663013458252
Validation loss: 2.0982771714528403

Epoch: 6| Step: 6
Training loss: 2.141045331954956
Validation loss: 2.1089522441228232

Epoch: 6| Step: 7
Training loss: 2.0766022205352783
Validation loss: 2.1073818604151406

Epoch: 6| Step: 8
Training loss: 1.678748607635498
Validation loss: 2.095070203145345

Epoch: 6| Step: 9
Training loss: 1.9292030334472656
Validation loss: 2.1041423281033835

Epoch: 6| Step: 10
Training loss: 1.5812678337097168
Validation loss: 2.1042983531951904

Epoch: 6| Step: 11
Training loss: 1.727961778640747
Validation loss: 2.109884182612101

Epoch: 6| Step: 12
Training loss: 2.2081010341644287
Validation loss: 2.117815931638082

Epoch: 6| Step: 13
Training loss: 2.6564700603485107
Validation loss: 2.1248849829037986

Epoch: 166| Step: 0
Training loss: 2.101228713989258
Validation loss: 2.126531183719635

Epoch: 6| Step: 1
Training loss: 1.8227026462554932
Validation loss: 2.1239442030588784

Epoch: 6| Step: 2
Training loss: 1.4179905652999878
Validation loss: 2.115780830383301

Epoch: 6| Step: 3
Training loss: 2.519380569458008
Validation loss: 2.119350790977478

Epoch: 6| Step: 4
Training loss: 2.112833023071289
Validation loss: 2.121995449066162

Epoch: 6| Step: 5
Training loss: 2.0756919384002686
Validation loss: 2.1111035346984863

Epoch: 6| Step: 6
Training loss: 1.7649835348129272
Validation loss: 2.1072358091672263

Epoch: 6| Step: 7
Training loss: 1.5494898557662964
Validation loss: 2.109286288420359

Epoch: 6| Step: 8
Training loss: 2.316243886947632
Validation loss: 2.0976975162823996

Epoch: 6| Step: 9
Training loss: 1.709092617034912
Validation loss: 2.08816929658254

Epoch: 6| Step: 10
Training loss: 2.4270970821380615
Validation loss: 2.0895188053448996

Epoch: 6| Step: 11
Training loss: 1.8817505836486816
Validation loss: 2.10243821144104

Epoch: 6| Step: 12
Training loss: 1.8516560792922974
Validation loss: 2.1046182910601297

Epoch: 6| Step: 13
Training loss: 2.503720283508301
Validation loss: 2.0875196854273477

Epoch: 167| Step: 0
Training loss: 1.780694842338562
Validation loss: 2.105510175228119

Epoch: 6| Step: 1
Training loss: 1.4687385559082031
Validation loss: 2.1063603162765503

Epoch: 6| Step: 2
Training loss: 1.9170527458190918
Validation loss: 2.1050730546315513

Epoch: 6| Step: 3
Training loss: 2.3961310386657715
Validation loss: 2.108303884665171

Epoch: 6| Step: 4
Training loss: 2.4631850719451904
Validation loss: 2.1099268396695456

Epoch: 6| Step: 5
Training loss: 2.1704111099243164
Validation loss: 2.112990915775299

Epoch: 6| Step: 6
Training loss: 1.9862658977508545
Validation loss: 2.100472112496694

Epoch: 6| Step: 7
Training loss: 2.5014030933380127
Validation loss: 2.1003881295522056

Epoch: 6| Step: 8
Training loss: 1.9954750537872314
Validation loss: 2.097538868586222

Epoch: 6| Step: 9
Training loss: 1.6324903964996338
Validation loss: 2.097811500231425

Epoch: 6| Step: 10
Training loss: 1.4243550300598145
Validation loss: 2.0972171227137246

Epoch: 6| Step: 11
Training loss: 1.9161657094955444
Validation loss: 2.0897435744603476

Epoch: 6| Step: 12
Training loss: 1.4551267623901367
Validation loss: 2.1048412720362344

Epoch: 6| Step: 13
Training loss: 2.277233600616455
Validation loss: 2.1148582696914673

Epoch: 168| Step: 0
Training loss: 2.020827293395996
Validation loss: 2.109492520491282

Epoch: 6| Step: 1
Training loss: 2.149886131286621
Validation loss: 2.125990549723307

Epoch: 6| Step: 2
Training loss: 2.2718381881713867
Validation loss: 2.110193113485972

Epoch: 6| Step: 3
Training loss: 2.029322862625122
Validation loss: 2.1145093043645224

Epoch: 6| Step: 4
Training loss: 1.6895880699157715
Validation loss: 2.1196818550427756

Epoch: 6| Step: 5
Training loss: 2.140575885772705
Validation loss: 2.111726403236389

Epoch: 6| Step: 6
Training loss: 1.7222437858581543
Validation loss: 2.1186667879422507

Epoch: 6| Step: 7
Training loss: 1.7824987173080444
Validation loss: 2.1035371820131936

Epoch: 6| Step: 8
Training loss: 1.9278583526611328
Validation loss: 2.106392204761505

Epoch: 6| Step: 9
Training loss: 2.158362627029419
Validation loss: 2.11879301071167

Epoch: 6| Step: 10
Training loss: 1.6301543712615967
Validation loss: 2.1138383746147156

Epoch: 6| Step: 11
Training loss: 2.6635942459106445
Validation loss: 2.120546340942383

Epoch: 6| Step: 12
Training loss: 1.6791713237762451
Validation loss: 2.1250792940457663

Epoch: 6| Step: 13
Training loss: 1.644595980644226
Validation loss: 2.1349571148554483

Epoch: 169| Step: 0
Training loss: 2.498884916305542
Validation loss: 2.1316298643747964

Epoch: 6| Step: 1
Training loss: 2.311060667037964
Validation loss: 2.1280901034673056

Epoch: 6| Step: 2
Training loss: 1.6796386241912842
Validation loss: 2.133306860923767

Epoch: 6| Step: 3
Training loss: 2.1942124366760254
Validation loss: 2.125280419985453

Epoch: 6| Step: 4
Training loss: 1.762342929840088
Validation loss: 2.1177542408307395

Epoch: 6| Step: 5
Training loss: 2.047940731048584
Validation loss: 2.1266194581985474

Epoch: 6| Step: 6
Training loss: 1.8225209712982178
Validation loss: 2.1120896339416504

Epoch: 6| Step: 7
Training loss: 1.7756372690200806
Validation loss: 2.121575951576233

Epoch: 6| Step: 8
Training loss: 2.081840991973877
Validation loss: 2.115540305773417

Epoch: 6| Step: 9
Training loss: 1.9267858266830444
Validation loss: 2.1142393151919046

Epoch: 6| Step: 10
Training loss: 1.757016658782959
Validation loss: 2.1157596707344055

Epoch: 6| Step: 11
Training loss: 2.082223892211914
Validation loss: 2.120746831099192

Epoch: 6| Step: 12
Training loss: 1.9254668951034546
Validation loss: 2.1312995751698813

Epoch: 6| Step: 13
Training loss: 1.6622669696807861
Validation loss: 2.125579357147217

Epoch: 170| Step: 0
Training loss: 1.781097650527954
Validation loss: 2.1236653327941895

Epoch: 6| Step: 1
Training loss: 1.4569261074066162
Validation loss: 2.122501850128174

Epoch: 6| Step: 2
Training loss: 1.8435792922973633
Validation loss: 2.1192920207977295

Epoch: 6| Step: 3
Training loss: 1.7591545581817627
Validation loss: 2.1325727105140686

Epoch: 6| Step: 4
Training loss: 2.9947872161865234
Validation loss: 2.125555157661438

Epoch: 6| Step: 5
Training loss: 2.1359939575195312
Validation loss: 2.1323046882947287

Epoch: 6| Step: 6
Training loss: 1.9181913137435913
Validation loss: 2.1249067187309265

Epoch: 6| Step: 7
Training loss: 1.430924654006958
Validation loss: 2.129640817642212

Epoch: 6| Step: 8
Training loss: 2.102543354034424
Validation loss: 2.1246720353762307

Epoch: 6| Step: 9
Training loss: 2.0629289150238037
Validation loss: 2.104675074418386

Epoch: 6| Step: 10
Training loss: 2.316525459289551
Validation loss: 2.1114073594411216

Epoch: 6| Step: 11
Training loss: 1.526974081993103
Validation loss: 2.113931576410929

Epoch: 6| Step: 12
Training loss: 2.1668920516967773
Validation loss: 2.1071297923723855

Epoch: 6| Step: 13
Training loss: 1.6466903686523438
Validation loss: 2.1145238081614175

Epoch: 171| Step: 0
Training loss: 1.666054129600525
Validation loss: 2.119509438673655

Epoch: 6| Step: 1
Training loss: 1.725860834121704
Validation loss: 2.1183716456095376

Epoch: 6| Step: 2
Training loss: 1.7741987705230713
Validation loss: 2.133854548136393

Epoch: 6| Step: 3
Training loss: 1.8970181941986084
Validation loss: 2.1448816061019897

Epoch: 6| Step: 4
Training loss: 2.6113929748535156
Validation loss: 2.1526631911595664

Epoch: 6| Step: 5
Training loss: 2.0487866401672363
Validation loss: 2.149870196978251

Epoch: 6| Step: 6
Training loss: 1.6560609340667725
Validation loss: 2.1491949955622354

Epoch: 6| Step: 7
Training loss: 2.1243796348571777
Validation loss: 2.1428701480229697

Epoch: 6| Step: 8
Training loss: 2.2954840660095215
Validation loss: 2.126254399617513

Epoch: 6| Step: 9
Training loss: 1.7434402704238892
Validation loss: 2.116166810194651

Epoch: 6| Step: 10
Training loss: 1.9706312417984009
Validation loss: 2.1098046700159707

Epoch: 6| Step: 11
Training loss: 1.9006952047348022
Validation loss: 2.105738341808319

Epoch: 6| Step: 12
Training loss: 2.258686065673828
Validation loss: 2.104176183541616

Epoch: 6| Step: 13
Training loss: 2.206500768661499
Validation loss: 2.111922800540924

Epoch: 172| Step: 0
Training loss: 1.4926966428756714
Validation loss: 2.1121612191200256

Epoch: 6| Step: 1
Training loss: 2.5305330753326416
Validation loss: 2.114793539047241

Epoch: 6| Step: 2
Training loss: 2.103454828262329
Validation loss: 2.1183274586995444

Epoch: 6| Step: 3
Training loss: 2.1105844974517822
Validation loss: 2.126227696736654

Epoch: 6| Step: 4
Training loss: 1.542245864868164
Validation loss: 2.1202288269996643

Epoch: 6| Step: 5
Training loss: 1.4370465278625488
Validation loss: 2.115912934144338

Epoch: 6| Step: 6
Training loss: 1.69364595413208
Validation loss: 2.108226716518402

Epoch: 6| Step: 7
Training loss: 2.821662187576294
Validation loss: 2.1176149050394693

Epoch: 6| Step: 8
Training loss: 1.7823739051818848
Validation loss: 2.0963664054870605

Epoch: 6| Step: 9
Training loss: 1.7030978202819824
Validation loss: 2.093721588452657

Epoch: 6| Step: 10
Training loss: 1.7568302154541016
Validation loss: 2.0930394927660623

Epoch: 6| Step: 11
Training loss: 1.8066246509552002
Validation loss: 2.1021894812583923

Epoch: 6| Step: 12
Training loss: 1.9573204517364502
Validation loss: 2.107137938340505

Epoch: 6| Step: 13
Training loss: 2.768117904663086
Validation loss: 2.1053563753763833

Epoch: 173| Step: 0
Training loss: 1.8313097953796387
Validation loss: 2.1039299766222634

Epoch: 6| Step: 1
Training loss: 2.4581799507141113
Validation loss: 2.1179168423016868

Epoch: 6| Step: 2
Training loss: 1.4911859035491943
Validation loss: 2.127089043458303

Epoch: 6| Step: 3
Training loss: 2.9089484214782715
Validation loss: 2.1342771450678506

Epoch: 6| Step: 4
Training loss: 1.9397462606430054
Validation loss: 2.128111163775126

Epoch: 6| Step: 5
Training loss: 2.188565731048584
Validation loss: 2.1193917791048684

Epoch: 6| Step: 6
Training loss: 2.0291502475738525
Validation loss: 2.1182702581087747

Epoch: 6| Step: 7
Training loss: 1.4466853141784668
Validation loss: 2.1175649563471475

Epoch: 6| Step: 8
Training loss: 1.5981364250183105
Validation loss: 2.123445709546407

Epoch: 6| Step: 9
Training loss: 2.4998483657836914
Validation loss: 2.1245652437210083

Epoch: 6| Step: 10
Training loss: 1.9038598537445068
Validation loss: 2.120405435562134

Epoch: 6| Step: 11
Training loss: 1.5012381076812744
Validation loss: 2.1287337144215903

Epoch: 6| Step: 12
Training loss: 1.9908671379089355
Validation loss: 2.117372433344523

Epoch: 6| Step: 13
Training loss: 1.7666970491409302
Validation loss: 2.129989802837372

Epoch: 174| Step: 0
Training loss: 2.00893235206604
Validation loss: 2.129916330178579

Epoch: 6| Step: 1
Training loss: 1.1104815006256104
Validation loss: 2.136346677939097

Epoch: 6| Step: 2
Training loss: 2.0808897018432617
Validation loss: 2.1542810002962747

Epoch: 6| Step: 3
Training loss: 2.0566797256469727
Validation loss: 2.1381282806396484

Epoch: 6| Step: 4
Training loss: 1.958508014678955
Validation loss: 2.1504621704419455

Epoch: 6| Step: 5
Training loss: 2.2547714710235596
Validation loss: 2.138167222340902

Epoch: 6| Step: 6
Training loss: 1.9168447256088257
Validation loss: 2.1274841825167337

Epoch: 6| Step: 7
Training loss: 2.2674384117126465
Validation loss: 2.119910955429077

Epoch: 6| Step: 8
Training loss: 1.7143070697784424
Validation loss: 2.1210955381393433

Epoch: 6| Step: 9
Training loss: 2.06901216506958
Validation loss: 2.1254446109135947

Epoch: 6| Step: 10
Training loss: 1.7052929401397705
Validation loss: 2.1131004889806113

Epoch: 6| Step: 11
Training loss: 2.219012975692749
Validation loss: 2.117503027121226

Epoch: 6| Step: 12
Training loss: 1.9018003940582275
Validation loss: 2.111478805541992

Epoch: 6| Step: 13
Training loss: 1.8669620752334595
Validation loss: 2.1305829087893167

Epoch: 175| Step: 0
Training loss: 1.4965975284576416
Validation loss: 2.1291263500849404

Epoch: 6| Step: 1
Training loss: 2.1629467010498047
Validation loss: 2.1181328296661377

Epoch: 6| Step: 2
Training loss: 2.5344126224517822
Validation loss: 2.1309907635053

Epoch: 6| Step: 3
Training loss: 1.6785123348236084
Validation loss: 2.1310762564341226

Epoch: 6| Step: 4
Training loss: 1.3579875230789185
Validation loss: 2.1277058919270835

Epoch: 6| Step: 5
Training loss: 2.253770589828491
Validation loss: 2.1262324452400208

Epoch: 6| Step: 6
Training loss: 1.7692455053329468
Validation loss: 2.1248318552970886

Epoch: 6| Step: 7
Training loss: 1.950199842453003
Validation loss: 2.131360034147898

Epoch: 6| Step: 8
Training loss: 1.8496410846710205
Validation loss: 2.1268940766652427

Epoch: 6| Step: 9
Training loss: 2.0985336303710938
Validation loss: 2.1237661441167197

Epoch: 6| Step: 10
Training loss: 2.7247958183288574
Validation loss: 2.1261298457781472

Epoch: 6| Step: 11
Training loss: 1.4051421880722046
Validation loss: 2.128089745839437

Epoch: 6| Step: 12
Training loss: 1.8478155136108398
Validation loss: 2.1199675599733987

Epoch: 6| Step: 13
Training loss: 1.7802104949951172
Validation loss: 2.1341264247894287

Testing loss: 1.7324192120874529

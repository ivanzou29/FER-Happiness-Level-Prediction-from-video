Epoch: 1| Step: 0
Training loss: 4.781280040740967
Validation loss: 5.29936424891154

Epoch: 6| Step: 1
Training loss: 3.2873036861419678
Validation loss: 5.296985228856404

Epoch: 6| Step: 2
Training loss: 5.889669418334961
Validation loss: 5.294767141342163

Epoch: 6| Step: 3
Training loss: 5.994609832763672
Validation loss: 5.292524894078572

Epoch: 6| Step: 4
Training loss: 6.628556251525879
Validation loss: 5.290260712305705

Epoch: 6| Step: 5
Training loss: 5.283832550048828
Validation loss: 5.288097222646077

Epoch: 6| Step: 6
Training loss: 5.00419282913208
Validation loss: 5.285826206207275

Epoch: 6| Step: 7
Training loss: 5.527133941650391
Validation loss: 5.2834235827128095

Epoch: 6| Step: 8
Training loss: 4.522631645202637
Validation loss: 5.2810564041137695

Epoch: 6| Step: 9
Training loss: 6.157904148101807
Validation loss: 5.27848744392395

Epoch: 6| Step: 10
Training loss: 5.395912170410156
Validation loss: 5.275911887486775

Epoch: 6| Step: 11
Training loss: 5.337093353271484
Validation loss: 5.273240725199382

Epoch: 6| Step: 12
Training loss: 5.845379829406738
Validation loss: 5.270410060882568

Epoch: 6| Step: 13
Training loss: 5.289744853973389
Validation loss: 5.2674336433410645

Epoch: 2| Step: 0
Training loss: 5.490042209625244
Validation loss: 5.2643388112386065

Epoch: 6| Step: 1
Training loss: 5.0606536865234375
Validation loss: 5.261188983917236

Epoch: 6| Step: 2
Training loss: 4.858121871948242
Validation loss: 5.257760365804036

Epoch: 6| Step: 3
Training loss: 4.989271640777588
Validation loss: 5.254347006479899

Epoch: 6| Step: 4
Training loss: 4.6099443435668945
Validation loss: 5.250771363576253

Epoch: 6| Step: 5
Training loss: 5.201691627502441
Validation loss: 5.246943473815918

Epoch: 6| Step: 6
Training loss: 6.0492658615112305
Validation loss: 5.2429672082265215

Epoch: 6| Step: 7
Training loss: 4.969351291656494
Validation loss: 5.238906065622966

Epoch: 6| Step: 8
Training loss: 5.41526460647583
Validation loss: 5.234623908996582

Epoch: 6| Step: 9
Training loss: 5.565390586853027
Validation loss: 5.230508128801982

Epoch: 6| Step: 10
Training loss: 5.85328483581543
Validation loss: 5.225724061330159

Epoch: 6| Step: 11
Training loss: 4.995279312133789
Validation loss: 5.2209681669871015

Epoch: 6| Step: 12
Training loss: 5.30338716506958
Validation loss: 5.215990781784058

Epoch: 6| Step: 13
Training loss: 5.9708356857299805
Validation loss: 5.210783243179321

Epoch: 3| Step: 0
Training loss: 4.725888252258301
Validation loss: 5.205394983291626

Epoch: 6| Step: 1
Training loss: 4.4553327560424805
Validation loss: 5.199837287267049

Epoch: 6| Step: 2
Training loss: 5.182653427124023
Validation loss: 5.193925182024638

Epoch: 6| Step: 3
Training loss: 4.888022422790527
Validation loss: 5.187888860702515

Epoch: 6| Step: 4
Training loss: 6.160771369934082
Validation loss: 5.181451638539632

Epoch: 6| Step: 5
Training loss: 4.75881814956665
Validation loss: 5.1749411424001055

Epoch: 6| Step: 6
Training loss: 4.77784538269043
Validation loss: 5.168308734893799

Epoch: 6| Step: 7
Training loss: 5.388132572174072
Validation loss: 5.161190191904704

Epoch: 6| Step: 8
Training loss: 4.744208812713623
Validation loss: 5.154120008150737

Epoch: 6| Step: 9
Training loss: 5.705816268920898
Validation loss: 5.14677619934082

Epoch: 6| Step: 10
Training loss: 5.715512752532959
Validation loss: 5.13914155960083

Epoch: 6| Step: 11
Training loss: 5.5973968505859375
Validation loss: 5.131383498509725

Epoch: 6| Step: 12
Training loss: 5.621400833129883
Validation loss: 5.123022158940633

Epoch: 6| Step: 13
Training loss: 5.570785999298096
Validation loss: 5.114775101343791

Epoch: 4| Step: 0
Training loss: 6.187496185302734
Validation loss: 5.106290102005005

Epoch: 6| Step: 1
Training loss: 4.627262115478516
Validation loss: 5.097689946492513

Epoch: 6| Step: 2
Training loss: 5.207115173339844
Validation loss: 5.088750998179118

Epoch: 6| Step: 3
Training loss: 3.4222323894500732
Validation loss: 5.079879919687907

Epoch: 6| Step: 4
Training loss: 5.879634857177734
Validation loss: 5.070797125498454

Epoch: 6| Step: 5
Training loss: 5.2577691078186035
Validation loss: 5.061474561691284

Epoch: 6| Step: 6
Training loss: 5.1336541175842285
Validation loss: 5.051864385604858

Epoch: 6| Step: 7
Training loss: 4.90025520324707
Validation loss: 5.042532920837402

Epoch: 6| Step: 8
Training loss: 4.59418249130249
Validation loss: 5.033010085423787

Epoch: 6| Step: 9
Training loss: 5.268736839294434
Validation loss: 5.023123184839885

Epoch: 6| Step: 10
Training loss: 4.487732887268066
Validation loss: 5.01380983988444

Epoch: 6| Step: 11
Training loss: 5.13111686706543
Validation loss: 5.004107395807902

Epoch: 6| Step: 12
Training loss: 6.1223320960998535
Validation loss: 4.994210441907247

Epoch: 6| Step: 13
Training loss: 5.517102241516113
Validation loss: 4.984611352284749

Epoch: 5| Step: 0
Training loss: 4.9903388023376465
Validation loss: 4.975185076395671

Epoch: 6| Step: 1
Training loss: 5.656483173370361
Validation loss: 4.965383529663086

Epoch: 6| Step: 2
Training loss: 4.804727554321289
Validation loss: 4.956128160158793

Epoch: 6| Step: 3
Training loss: 5.013928413391113
Validation loss: 4.946453968683879

Epoch: 6| Step: 4
Training loss: 4.920888900756836
Validation loss: 4.936834335327148

Epoch: 6| Step: 5
Training loss: 5.733440399169922
Validation loss: 4.927347580591838

Epoch: 6| Step: 6
Training loss: 5.694965362548828
Validation loss: 4.918010950088501

Epoch: 6| Step: 7
Training loss: 4.620098114013672
Validation loss: 4.90861709912618

Epoch: 6| Step: 8
Training loss: 4.734490394592285
Validation loss: 4.899175405502319

Epoch: 6| Step: 9
Training loss: 4.806771278381348
Validation loss: 4.889827688535054

Epoch: 6| Step: 10
Training loss: 4.4252142906188965
Validation loss: 4.880519310633342

Epoch: 6| Step: 11
Training loss: 5.466166019439697
Validation loss: 4.871366024017334

Epoch: 6| Step: 12
Training loss: 4.542050838470459
Validation loss: 4.861871719360352

Epoch: 6| Step: 13
Training loss: 4.564183235168457
Validation loss: 4.852183818817139

Epoch: 6| Step: 0
Training loss: 4.687769889831543
Validation loss: 4.842753171920776

Epoch: 6| Step: 1
Training loss: 3.5633339881896973
Validation loss: 4.8334987958272295

Epoch: 6| Step: 2
Training loss: 5.763960838317871
Validation loss: 4.824132601420085

Epoch: 6| Step: 3
Training loss: 5.155958652496338
Validation loss: 4.814441005388896

Epoch: 6| Step: 4
Training loss: 6.684906482696533
Validation loss: 4.80476729075114

Epoch: 6| Step: 5
Training loss: 5.051319599151611
Validation loss: 4.795159737269084

Epoch: 6| Step: 6
Training loss: 4.764303207397461
Validation loss: 4.785684267679851

Epoch: 6| Step: 7
Training loss: 4.891510486602783
Validation loss: 4.776187976201375

Epoch: 6| Step: 8
Training loss: 3.1824727058410645
Validation loss: 4.7666235367457075

Epoch: 6| Step: 9
Training loss: 5.020146369934082
Validation loss: 4.758346239725749

Epoch: 6| Step: 10
Training loss: 4.484922409057617
Validation loss: 4.750087896982829

Epoch: 6| Step: 11
Training loss: 4.351858139038086
Validation loss: 4.742054303487142

Epoch: 6| Step: 12
Training loss: 5.720483779907227
Validation loss: 4.733828624089559

Epoch: 6| Step: 13
Training loss: 4.899016380310059
Validation loss: 4.7259650230407715

Epoch: 7| Step: 0
Training loss: 5.103391647338867
Validation loss: 4.718763510386149

Epoch: 6| Step: 1
Training loss: 4.674046516418457
Validation loss: 4.7107376257578535

Epoch: 6| Step: 2
Training loss: 4.783806324005127
Validation loss: 4.703506708145142

Epoch: 6| Step: 3
Training loss: 5.118818759918213
Validation loss: 4.6958537101745605

Epoch: 6| Step: 4
Training loss: 6.131656646728516
Validation loss: 4.688843409220378

Epoch: 6| Step: 5
Training loss: 4.120113849639893
Validation loss: 4.68144416809082

Epoch: 6| Step: 6
Training loss: 3.9553449153900146
Validation loss: 4.673908869425456

Epoch: 6| Step: 7
Training loss: 4.983721733093262
Validation loss: 4.6669197877248125

Epoch: 6| Step: 8
Training loss: 3.905331611633301
Validation loss: 4.6591949462890625

Epoch: 6| Step: 9
Training loss: 4.424006462097168
Validation loss: 4.652639746665955

Epoch: 6| Step: 10
Training loss: 4.165340423583984
Validation loss: 4.645317713419597

Epoch: 6| Step: 11
Training loss: 5.692734241485596
Validation loss: 4.637961467107137

Epoch: 6| Step: 12
Training loss: 5.5231804847717285
Validation loss: 4.63094162940979

Epoch: 6| Step: 13
Training loss: 4.142865180969238
Validation loss: 4.623545487721761

Epoch: 8| Step: 0
Training loss: 4.032584190368652
Validation loss: 4.616418639818828

Epoch: 6| Step: 1
Training loss: 5.492683410644531
Validation loss: 4.609137614568074

Epoch: 6| Step: 2
Training loss: 5.684030532836914
Validation loss: 4.601680199305217

Epoch: 6| Step: 3
Training loss: 3.692841053009033
Validation loss: 4.594473719596863

Epoch: 6| Step: 4
Training loss: 4.987580299377441
Validation loss: 4.586977203687032

Epoch: 6| Step: 5
Training loss: 4.311262130737305
Validation loss: 4.579415400822957

Epoch: 6| Step: 6
Training loss: 5.218315124511719
Validation loss: 4.571692705154419

Epoch: 6| Step: 7
Training loss: 3.8485121726989746
Validation loss: 4.564247965812683

Epoch: 6| Step: 8
Training loss: 5.1512651443481445
Validation loss: 4.5570119222005205

Epoch: 6| Step: 9
Training loss: 4.475995063781738
Validation loss: 4.549583236376445

Epoch: 6| Step: 10
Training loss: 3.6604270935058594
Validation loss: 4.542436083157857

Epoch: 6| Step: 11
Training loss: 4.473860740661621
Validation loss: 4.5355673631032305

Epoch: 6| Step: 12
Training loss: 5.320965766906738
Validation loss: 4.528652270634969

Epoch: 6| Step: 13
Training loss: 5.0265913009643555
Validation loss: 4.521931608517964

Epoch: 9| Step: 0
Training loss: 5.079521179199219
Validation loss: 4.514879465103149

Epoch: 6| Step: 1
Training loss: 4.653229713439941
Validation loss: 4.508483489354451

Epoch: 6| Step: 2
Training loss: 6.090054512023926
Validation loss: 4.501274426778157

Epoch: 6| Step: 3
Training loss: 4.872271537780762
Validation loss: 4.494380633036296

Epoch: 6| Step: 4
Training loss: 4.039922714233398
Validation loss: 4.487414042154948

Epoch: 6| Step: 5
Training loss: 3.242598056793213
Validation loss: 4.480657974878947

Epoch: 6| Step: 6
Training loss: 4.499903202056885
Validation loss: 4.474042018254598

Epoch: 6| Step: 7
Training loss: 5.137633323669434
Validation loss: 4.466820955276489

Epoch: 6| Step: 8
Training loss: 3.5179357528686523
Validation loss: 4.459628462791443

Epoch: 6| Step: 9
Training loss: 4.282510280609131
Validation loss: 4.452502886454265

Epoch: 6| Step: 10
Training loss: 4.979848861694336
Validation loss: 4.446108102798462

Epoch: 6| Step: 11
Training loss: 4.201891899108887
Validation loss: 4.439062833786011

Epoch: 6| Step: 12
Training loss: 4.624488353729248
Validation loss: 4.431933363278707

Epoch: 6| Step: 13
Training loss: 4.923527717590332
Validation loss: 4.425151268641154

Epoch: 10| Step: 0
Training loss: 4.200738430023193
Validation loss: 4.4185859362284345

Epoch: 6| Step: 1
Training loss: 5.469751358032227
Validation loss: 4.411848068237305

Epoch: 6| Step: 2
Training loss: 3.928462505340576
Validation loss: 4.404934048652649

Epoch: 6| Step: 3
Training loss: 5.528698921203613
Validation loss: 4.398275216420491

Epoch: 6| Step: 4
Training loss: 4.74082088470459
Validation loss: 4.391039689381917

Epoch: 6| Step: 5
Training loss: 4.874551296234131
Validation loss: 4.383799473444621

Epoch: 6| Step: 6
Training loss: 3.8169751167297363
Validation loss: 4.3765872319539385

Epoch: 6| Step: 7
Training loss: 4.360746383666992
Validation loss: 4.369956731796265

Epoch: 6| Step: 8
Training loss: 4.260923862457275
Validation loss: 4.362390478452046

Epoch: 6| Step: 9
Training loss: 3.848050594329834
Validation loss: 4.35528035958608

Epoch: 6| Step: 10
Training loss: 5.014183044433594
Validation loss: 4.347641984621684

Epoch: 6| Step: 11
Training loss: 4.444903373718262
Validation loss: 4.340835293134053

Epoch: 6| Step: 12
Training loss: 3.536961078643799
Validation loss: 4.334164659182231

Epoch: 6| Step: 13
Training loss: 4.87849235534668
Validation loss: 4.327226599057515

Epoch: 11| Step: 0
Training loss: 4.281100273132324
Validation loss: 4.320702989896138

Epoch: 6| Step: 1
Training loss: 3.9745335578918457
Validation loss: 4.313126643498738

Epoch: 6| Step: 2
Training loss: 3.852017402648926
Validation loss: 4.30640443166097

Epoch: 6| Step: 3
Training loss: 4.71554708480835
Validation loss: 4.298830429712932

Epoch: 6| Step: 4
Training loss: 4.894909858703613
Validation loss: 4.291612267494202

Epoch: 6| Step: 5
Training loss: 4.629672050476074
Validation loss: 4.2847663561503095

Epoch: 6| Step: 6
Training loss: 3.3433265686035156
Validation loss: 4.277469317118327

Epoch: 6| Step: 7
Training loss: 5.098916530609131
Validation loss: 4.270589152971904

Epoch: 6| Step: 8
Training loss: 5.23940372467041
Validation loss: 4.262904405593872

Epoch: 6| Step: 9
Training loss: 5.028997421264648
Validation loss: 4.255139430363973

Epoch: 6| Step: 10
Training loss: 4.280956268310547
Validation loss: 4.24771245320638

Epoch: 6| Step: 11
Training loss: 3.342463493347168
Validation loss: 4.2402762571970625

Epoch: 6| Step: 12
Training loss: 4.120789527893066
Validation loss: 4.233884970347087

Epoch: 6| Step: 13
Training loss: 4.7932868003845215
Validation loss: 4.22610600789388

Epoch: 12| Step: 0
Training loss: 4.066999435424805
Validation loss: 4.219314098358154

Epoch: 6| Step: 1
Training loss: 4.849693775177002
Validation loss: 4.2119914293289185

Epoch: 6| Step: 2
Training loss: 4.194183349609375
Validation loss: 4.205101847648621

Epoch: 6| Step: 3
Training loss: 4.4597578048706055
Validation loss: 4.197871128718059

Epoch: 6| Step: 4
Training loss: 3.7612035274505615
Validation loss: 4.1913003126780195

Epoch: 6| Step: 5
Training loss: 3.728895664215088
Validation loss: 4.184305032094319

Epoch: 6| Step: 6
Training loss: 3.8339121341705322
Validation loss: 4.178208589553833

Epoch: 6| Step: 7
Training loss: 4.846207618713379
Validation loss: 4.171955506006877

Epoch: 6| Step: 8
Training loss: 4.887894630432129
Validation loss: 4.164904832839966

Epoch: 6| Step: 9
Training loss: 5.651218891143799
Validation loss: 4.158839225769043

Epoch: 6| Step: 10
Training loss: 3.7595760822296143
Validation loss: 4.152136524518331

Epoch: 6| Step: 11
Training loss: 4.896862030029297
Validation loss: 4.145343383153279

Epoch: 6| Step: 12
Training loss: 2.9964866638183594
Validation loss: 4.139195362726848

Epoch: 6| Step: 13
Training loss: 4.426199913024902
Validation loss: 4.1336246728897095

Epoch: 13| Step: 0
Training loss: 3.590240240097046
Validation loss: 4.127376159032186

Epoch: 6| Step: 1
Training loss: 4.6218342781066895
Validation loss: 4.12187385559082

Epoch: 6| Step: 2
Training loss: 4.193105697631836
Validation loss: 4.115793228149414

Epoch: 6| Step: 3
Training loss: 3.6170077323913574
Validation loss: 4.1103378136952715

Epoch: 6| Step: 4
Training loss: 4.1095476150512695
Validation loss: 4.1050898631413775

Epoch: 6| Step: 5
Training loss: 5.099061012268066
Validation loss: 4.099441170692444

Epoch: 6| Step: 6
Training loss: 3.2424023151397705
Validation loss: 4.093382636706035

Epoch: 6| Step: 7
Training loss: 4.7943339347839355
Validation loss: 4.087843616803487

Epoch: 6| Step: 8
Training loss: 4.553042411804199
Validation loss: 4.082207163174947

Epoch: 6| Step: 9
Training loss: 4.245955467224121
Validation loss: 4.0769104560216265

Epoch: 6| Step: 10
Training loss: 4.0745015144348145
Validation loss: 4.070823033650716

Epoch: 6| Step: 11
Training loss: 4.9232587814331055
Validation loss: 4.065715909004211

Epoch: 6| Step: 12
Training loss: 4.291662216186523
Validation loss: 4.060487866401672

Epoch: 6| Step: 13
Training loss: 3.868945360183716
Validation loss: 4.056014855702718

Epoch: 14| Step: 0
Training loss: 3.7729949951171875
Validation loss: 4.050977786382039

Epoch: 6| Step: 1
Training loss: 4.547540664672852
Validation loss: 4.045141935348511

Epoch: 6| Step: 2
Training loss: 4.169598579406738
Validation loss: 4.039167682329814

Epoch: 6| Step: 3
Training loss: 5.123055934906006
Validation loss: 4.034156719843547

Epoch: 6| Step: 4
Training loss: 3.331177234649658
Validation loss: 4.029391050338745

Epoch: 6| Step: 5
Training loss: 4.120672225952148
Validation loss: 4.026571989059448

Epoch: 6| Step: 6
Training loss: 4.412090301513672
Validation loss: 4.020081837972005

Epoch: 6| Step: 7
Training loss: 4.242215633392334
Validation loss: 4.014168659845988

Epoch: 6| Step: 8
Training loss: 4.280551433563232
Validation loss: 4.009009798367818

Epoch: 6| Step: 9
Training loss: 3.802133560180664
Validation loss: 4.002687374750773

Epoch: 6| Step: 10
Training loss: 4.311560153961182
Validation loss: 3.9983460903167725

Epoch: 6| Step: 11
Training loss: 3.8127880096435547
Validation loss: 3.992600123087565

Epoch: 6| Step: 12
Training loss: 4.187533855438232
Validation loss: 3.987351377805074

Epoch: 6| Step: 13
Training loss: 4.13386344909668
Validation loss: 3.981184482574463

Epoch: 15| Step: 0
Training loss: 4.543292045593262
Validation loss: 3.9759307702382407

Epoch: 6| Step: 1
Training loss: 4.448163986206055
Validation loss: 3.9708654483159385

Epoch: 6| Step: 2
Training loss: 3.874899387359619
Validation loss: 3.965060750643412

Epoch: 6| Step: 3
Training loss: 3.444422960281372
Validation loss: 3.9602533976236978

Epoch: 6| Step: 4
Training loss: 3.794416666030884
Validation loss: 3.954375147819519

Epoch: 6| Step: 5
Training loss: 4.22946310043335
Validation loss: 3.9494686921437583

Epoch: 6| Step: 6
Training loss: 3.8301353454589844
Validation loss: 3.9433033068974814

Epoch: 6| Step: 7
Training loss: 3.9151811599731445
Validation loss: 3.9381245374679565

Epoch: 6| Step: 8
Training loss: 4.710177898406982
Validation loss: 3.9322390953699746

Epoch: 6| Step: 9
Training loss: 4.57979679107666
Validation loss: 3.9262176354726157

Epoch: 6| Step: 10
Training loss: 4.195777893066406
Validation loss: 3.920966386795044

Epoch: 6| Step: 11
Training loss: 3.595366954803467
Validation loss: 3.916196823120117

Epoch: 6| Step: 12
Training loss: 4.420045852661133
Validation loss: 3.910893281300863

Epoch: 6| Step: 13
Training loss: 3.6788578033447266
Validation loss: 3.9053446849187217

Epoch: 16| Step: 0
Training loss: 5.078990936279297
Validation loss: 3.900475859642029

Epoch: 6| Step: 1
Training loss: 4.405569553375244
Validation loss: 3.8964561223983765

Epoch: 6| Step: 2
Training loss: 3.7426154613494873
Validation loss: 3.89124588171641

Epoch: 6| Step: 3
Training loss: 3.766110420227051
Validation loss: 3.8866453568140664

Epoch: 6| Step: 4
Training loss: 3.1972615718841553
Validation loss: 3.8813235759735107

Epoch: 6| Step: 5
Training loss: 4.007688522338867
Validation loss: 3.876552859942118

Epoch: 6| Step: 6
Training loss: 4.242603302001953
Validation loss: 3.872069994608561

Epoch: 6| Step: 7
Training loss: 3.645730495452881
Validation loss: 3.867750326792399

Epoch: 6| Step: 8
Training loss: 3.5010793209075928
Validation loss: 3.8620914618174234

Epoch: 6| Step: 9
Training loss: 4.378459453582764
Validation loss: 3.8579113483428955

Epoch: 6| Step: 10
Training loss: 3.1716837882995605
Validation loss: 3.8526708285013833

Epoch: 6| Step: 11
Training loss: 4.535561561584473
Validation loss: 3.8480040232340493

Epoch: 6| Step: 12
Training loss: 4.369582653045654
Validation loss: 3.843267281850179

Epoch: 6| Step: 13
Training loss: 4.263008117675781
Validation loss: 3.8383474747339883

Epoch: 17| Step: 0
Training loss: 3.897026538848877
Validation loss: 3.833472490310669

Epoch: 6| Step: 1
Training loss: 3.504779577255249
Validation loss: 3.828834652900696

Epoch: 6| Step: 2
Training loss: 4.004474639892578
Validation loss: 3.824803034464518

Epoch: 6| Step: 3
Training loss: 4.129268646240234
Validation loss: 3.8205557266871133

Epoch: 6| Step: 4
Training loss: 4.250324249267578
Validation loss: 3.815958102544149

Epoch: 6| Step: 5
Training loss: 4.116555213928223
Validation loss: 3.8113689025243125

Epoch: 6| Step: 6
Training loss: 3.4638638496398926
Validation loss: 3.80660072962443

Epoch: 6| Step: 7
Training loss: 3.9713845252990723
Validation loss: 3.802616278330485

Epoch: 6| Step: 8
Training loss: 4.469905376434326
Validation loss: 3.7972325483957925

Epoch: 6| Step: 9
Training loss: 5.09431791305542
Validation loss: 3.792763829231262

Epoch: 6| Step: 10
Training loss: 3.6897354125976562
Validation loss: 3.7881060441335044

Epoch: 6| Step: 11
Training loss: 3.5267722606658936
Validation loss: 3.7840751806894937

Epoch: 6| Step: 12
Training loss: 3.6775317192077637
Validation loss: 3.7800683975219727

Epoch: 6| Step: 13
Training loss: 3.6419453620910645
Validation loss: 3.7750527064005532

Epoch: 18| Step: 0
Training loss: 3.7018375396728516
Validation loss: 3.7704114516576133

Epoch: 6| Step: 1
Training loss: 3.8196661472320557
Validation loss: 3.765949090321859

Epoch: 6| Step: 2
Training loss: 5.103893280029297
Validation loss: 3.76171084245046

Epoch: 6| Step: 3
Training loss: 4.051071643829346
Validation loss: 3.757300098737081

Epoch: 6| Step: 4
Training loss: 2.823629856109619
Validation loss: 3.7531579732894897

Epoch: 6| Step: 5
Training loss: 3.9515581130981445
Validation loss: 3.7490474383036294

Epoch: 6| Step: 6
Training loss: 3.778205156326294
Validation loss: 3.744961977005005

Epoch: 6| Step: 7
Training loss: 4.0969672203063965
Validation loss: 3.7404531637827554

Epoch: 6| Step: 8
Training loss: 4.197481155395508
Validation loss: 3.7368093729019165

Epoch: 6| Step: 9
Training loss: 3.176382064819336
Validation loss: 3.7319372495015464

Epoch: 6| Step: 10
Training loss: 4.266903877258301
Validation loss: 3.7272213300069175

Epoch: 6| Step: 11
Training loss: 3.899019479751587
Validation loss: 3.722847104072571

Epoch: 6| Step: 12
Training loss: 3.3641462326049805
Validation loss: 3.7190319697062173

Epoch: 6| Step: 13
Training loss: 4.386574745178223
Validation loss: 3.7146108547846475

Epoch: 19| Step: 0
Training loss: 3.789215087890625
Validation loss: 3.7103803952534995

Epoch: 6| Step: 1
Training loss: 3.493727207183838
Validation loss: 3.706291158994039

Epoch: 6| Step: 2
Training loss: 3.568863868713379
Validation loss: 3.7025197744369507

Epoch: 6| Step: 3
Training loss: 3.2846169471740723
Validation loss: 3.6980954806009927

Epoch: 6| Step: 4
Training loss: 3.8553075790405273
Validation loss: 3.6937785148620605

Epoch: 6| Step: 5
Training loss: 4.806197166442871
Validation loss: 3.689466953277588

Epoch: 6| Step: 6
Training loss: 4.655104160308838
Validation loss: 3.685461163520813

Epoch: 6| Step: 7
Training loss: 3.334089756011963
Validation loss: 3.6812193791071572

Epoch: 6| Step: 8
Training loss: 2.970409870147705
Validation loss: 3.67715052763621

Epoch: 6| Step: 9
Training loss: 3.679720163345337
Validation loss: 3.672772447268168

Epoch: 6| Step: 10
Training loss: 4.595017433166504
Validation loss: 3.6689687172571817

Epoch: 6| Step: 11
Training loss: 3.576500415802002
Validation loss: 3.6643173694610596

Epoch: 6| Step: 12
Training loss: 3.436584949493408
Validation loss: 3.6606144110361734

Epoch: 6| Step: 13
Training loss: 4.77559757232666
Validation loss: 3.656104246775309

Epoch: 20| Step: 0
Training loss: 4.15554666519165
Validation loss: 3.6520568132400513

Epoch: 6| Step: 1
Training loss: 4.017056465148926
Validation loss: 3.6481420199076333

Epoch: 6| Step: 2
Training loss: 3.7461934089660645
Validation loss: 3.643782059351603

Epoch: 6| Step: 3
Training loss: 3.9103710651397705
Validation loss: 3.639659325281779

Epoch: 6| Step: 4
Training loss: 3.6583962440490723
Validation loss: 3.6362112363179526

Epoch: 6| Step: 5
Training loss: 4.118452072143555
Validation loss: 3.631433645884196

Epoch: 6| Step: 6
Training loss: 3.110513925552368
Validation loss: 3.627528746922811

Epoch: 6| Step: 7
Training loss: 4.310101509094238
Validation loss: 3.6233121951421103

Epoch: 6| Step: 8
Training loss: 3.400467872619629
Validation loss: 3.6191675662994385

Epoch: 6| Step: 9
Training loss: 3.6523048877716064
Validation loss: 3.6143230199813843

Epoch: 6| Step: 10
Training loss: 4.04884147644043
Validation loss: 3.6107261180877686

Epoch: 6| Step: 11
Training loss: 3.611098289489746
Validation loss: 3.6064229011535645

Epoch: 6| Step: 12
Training loss: 3.792233467102051
Validation loss: 3.601055145263672

Epoch: 6| Step: 13
Training loss: 3.5023255348205566
Validation loss: 3.5977012316385903

Epoch: 21| Step: 0
Training loss: 2.703165054321289
Validation loss: 3.593332131703695

Epoch: 6| Step: 1
Training loss: 2.3363614082336426
Validation loss: 3.589699109395345

Epoch: 6| Step: 2
Training loss: 2.966839075088501
Validation loss: 3.5860068003336587

Epoch: 6| Step: 3
Training loss: 4.041059970855713
Validation loss: 3.5817357699076333

Epoch: 6| Step: 4
Training loss: 3.75123929977417
Validation loss: 3.577895720799764

Epoch: 6| Step: 5
Training loss: 4.366665840148926
Validation loss: 3.5741543769836426

Epoch: 6| Step: 6
Training loss: 4.123321533203125
Validation loss: 3.5700920820236206

Epoch: 6| Step: 7
Training loss: 3.5715646743774414
Validation loss: 3.56621778011322

Epoch: 6| Step: 8
Training loss: 4.750461578369141
Validation loss: 3.5615954399108887

Epoch: 6| Step: 9
Training loss: 3.2256507873535156
Validation loss: 3.5572767655054727

Epoch: 6| Step: 10
Training loss: 4.249151706695557
Validation loss: 3.5532013177871704

Epoch: 6| Step: 11
Training loss: 4.684206008911133
Validation loss: 3.5489638249079385

Epoch: 6| Step: 12
Training loss: 3.5810983180999756
Validation loss: 3.544619639714559

Epoch: 6| Step: 13
Training loss: 3.8773603439331055
Validation loss: 3.539884169896444

Epoch: 22| Step: 0
Training loss: 2.3552932739257812
Validation loss: 3.5359166065851846

Epoch: 6| Step: 1
Training loss: 3.660949468612671
Validation loss: 3.5319681962331138

Epoch: 6| Step: 2
Training loss: 3.6928255558013916
Validation loss: 3.5278306007385254

Epoch: 6| Step: 3
Training loss: 3.42130184173584
Validation loss: 3.523840308189392

Epoch: 6| Step: 4
Training loss: 3.587146759033203
Validation loss: 3.5201751391092935

Epoch: 6| Step: 5
Training loss: 4.083561420440674
Validation loss: 3.5153651237487793

Epoch: 6| Step: 6
Training loss: 3.205404281616211
Validation loss: 3.5117324193318686

Epoch: 6| Step: 7
Training loss: 3.7781453132629395
Validation loss: 3.508990168571472

Epoch: 6| Step: 8
Training loss: 3.90635347366333
Validation loss: 3.5032368898391724

Epoch: 6| Step: 9
Training loss: 3.8690762519836426
Validation loss: 3.4993675152460733

Epoch: 6| Step: 10
Training loss: 4.561093330383301
Validation loss: 3.4957745472590127

Epoch: 6| Step: 11
Training loss: 3.023799180984497
Validation loss: 3.4917409420013428

Epoch: 6| Step: 12
Training loss: 3.9235899448394775
Validation loss: 3.489375869433085

Epoch: 6| Step: 13
Training loss: 4.37982177734375
Validation loss: 3.483013113339742

Epoch: 23| Step: 0
Training loss: 3.877920627593994
Validation loss: 3.478596051534017

Epoch: 6| Step: 1
Training loss: 2.8885440826416016
Validation loss: 3.474051276842753

Epoch: 6| Step: 2
Training loss: 3.847388982772827
Validation loss: 3.4699466625849404

Epoch: 6| Step: 3
Training loss: 2.925750255584717
Validation loss: 3.465548793474833

Epoch: 6| Step: 4
Training loss: 4.088063716888428
Validation loss: 3.4613954623540244

Epoch: 6| Step: 5
Training loss: 2.9898157119750977
Validation loss: 3.4563825925191245

Epoch: 6| Step: 6
Training loss: 3.4020843505859375
Validation loss: 3.4524850447972617

Epoch: 6| Step: 7
Training loss: 3.1374711990356445
Validation loss: 3.4479068915049234

Epoch: 6| Step: 8
Training loss: 3.375349283218384
Validation loss: 3.4435121615727744

Epoch: 6| Step: 9
Training loss: 4.41693115234375
Validation loss: 3.4395238558451333

Epoch: 6| Step: 10
Training loss: 4.146306037902832
Validation loss: 3.4354231357574463

Epoch: 6| Step: 11
Training loss: 4.774509429931641
Validation loss: 3.4311420917510986

Epoch: 6| Step: 12
Training loss: 3.5411128997802734
Validation loss: 3.4266698360443115

Epoch: 6| Step: 13
Training loss: 3.2655138969421387
Validation loss: 3.422723571459452

Epoch: 24| Step: 0
Training loss: 2.8399441242218018
Validation loss: 3.418406327565511

Epoch: 6| Step: 1
Training loss: 4.247633457183838
Validation loss: 3.414454619089762

Epoch: 6| Step: 2
Training loss: 4.260138988494873
Validation loss: 3.4107187191645303

Epoch: 6| Step: 3
Training loss: 2.468670606613159
Validation loss: 3.4065491358439126

Epoch: 6| Step: 4
Training loss: 4.679464340209961
Validation loss: 3.4029422998428345

Epoch: 6| Step: 5
Training loss: 3.1166694164276123
Validation loss: 3.3984573682149253

Epoch: 6| Step: 6
Training loss: 3.979945182800293
Validation loss: 3.394755760828654

Epoch: 6| Step: 7
Training loss: 3.0556905269622803
Validation loss: 3.3905853430430093

Epoch: 6| Step: 8
Training loss: 3.8941946029663086
Validation loss: 3.385917584101359

Epoch: 6| Step: 9
Training loss: 4.0477495193481445
Validation loss: 3.3827150662740073

Epoch: 6| Step: 10
Training loss: 3.0024948120117188
Validation loss: 3.378113031387329

Epoch: 6| Step: 11
Training loss: 3.866466999053955
Validation loss: 3.3747198979059854

Epoch: 6| Step: 12
Training loss: 3.528526782989502
Validation loss: 3.370965282122294

Epoch: 6| Step: 13
Training loss: 2.8768606185913086
Validation loss: 3.366488774617513

Epoch: 25| Step: 0
Training loss: 3.1696739196777344
Validation loss: 3.3627208471298218

Epoch: 6| Step: 1
Training loss: 3.6595754623413086
Validation loss: 3.3589734633763633

Epoch: 6| Step: 2
Training loss: 3.3678784370422363
Validation loss: 3.3550670544306436

Epoch: 6| Step: 3
Training loss: 3.1644837856292725
Validation loss: 3.350459893544515

Epoch: 6| Step: 4
Training loss: 3.753357410430908
Validation loss: 3.346490740776062

Epoch: 6| Step: 5
Training loss: 2.7841076850891113
Validation loss: 3.342769145965576

Epoch: 6| Step: 6
Training loss: 3.2268848419189453
Validation loss: 3.3390578031539917

Epoch: 6| Step: 7
Training loss: 3.2683749198913574
Validation loss: 3.335678140322367

Epoch: 6| Step: 8
Training loss: 3.883504629135132
Validation loss: 3.331990361213684

Epoch: 6| Step: 9
Training loss: 3.9103331565856934
Validation loss: 3.3283358812332153

Epoch: 6| Step: 10
Training loss: 3.444279909133911
Validation loss: 3.3246938784917197

Epoch: 6| Step: 11
Training loss: 3.675184965133667
Validation loss: 3.320642073949178

Epoch: 6| Step: 12
Training loss: 4.237468242645264
Validation loss: 3.3167984088261924

Epoch: 6| Step: 13
Training loss: 3.5745363235473633
Validation loss: 3.3127885659535727

Epoch: 26| Step: 0
Training loss: 3.2059240341186523
Validation loss: 3.3088154395421348

Epoch: 6| Step: 1
Training loss: 3.0898919105529785
Validation loss: 3.3047335545221963

Epoch: 6| Step: 2
Training loss: 3.545438766479492
Validation loss: 3.300988793373108

Epoch: 6| Step: 3
Training loss: 2.6237361431121826
Validation loss: 3.2974167267481485

Epoch: 6| Step: 4
Training loss: 3.7083146572113037
Validation loss: 3.293739398320516

Epoch: 6| Step: 5
Training loss: 4.160735607147217
Validation loss: 3.28989048798879

Epoch: 6| Step: 6
Training loss: 3.546090602874756
Validation loss: 3.2860371271769204

Epoch: 6| Step: 7
Training loss: 3.580203056335449
Validation loss: 3.2819437185923257

Epoch: 6| Step: 8
Training loss: 3.445842981338501
Validation loss: 3.2780100107192993

Epoch: 6| Step: 9
Training loss: 3.2760233879089355
Validation loss: 3.273606459299723

Epoch: 6| Step: 10
Training loss: 3.5863683223724365
Validation loss: 3.269754926363627

Epoch: 6| Step: 11
Training loss: 3.5295424461364746
Validation loss: 3.26559046904246

Epoch: 6| Step: 12
Training loss: 3.546389579772949
Validation loss: 3.2616207599639893

Epoch: 6| Step: 13
Training loss: 3.6092469692230225
Validation loss: 3.258052031199137

Epoch: 27| Step: 0
Training loss: 3.6202850341796875
Validation loss: 3.254555861155192

Epoch: 6| Step: 1
Training loss: 3.1706349849700928
Validation loss: 3.2512470881144204

Epoch: 6| Step: 2
Training loss: 4.11578369140625
Validation loss: 3.2475762367248535

Epoch: 6| Step: 3
Training loss: 4.046534538269043
Validation loss: 3.2443405787150064

Epoch: 6| Step: 4
Training loss: 2.2278456687927246
Validation loss: 3.240646998087565

Epoch: 6| Step: 5
Training loss: 3.943726062774658
Validation loss: 3.237573822339376

Epoch: 6| Step: 6
Training loss: 3.125810146331787
Validation loss: 3.233476479848226

Epoch: 6| Step: 7
Training loss: 3.864607095718384
Validation loss: 3.2305607000986734

Epoch: 6| Step: 8
Training loss: 2.519406795501709
Validation loss: 3.2265825271606445

Epoch: 6| Step: 9
Training loss: 3.158214807510376
Validation loss: 3.222297191619873

Epoch: 6| Step: 10
Training loss: 4.015569686889648
Validation loss: 3.2189861536026

Epoch: 6| Step: 11
Training loss: 3.293172597885132
Validation loss: 3.21514900525411

Epoch: 6| Step: 12
Training loss: 3.0263214111328125
Validation loss: 3.2117067178090415

Epoch: 6| Step: 13
Training loss: 3.6032769680023193
Validation loss: 3.2079127629597983

Epoch: 28| Step: 0
Training loss: 2.988361120223999
Validation loss: 3.2045307954152427

Epoch: 6| Step: 1
Training loss: 3.731200695037842
Validation loss: 3.2004435062408447

Epoch: 6| Step: 2
Training loss: 3.0324959754943848
Validation loss: 3.1967498064041138

Epoch: 6| Step: 3
Training loss: 4.068502426147461
Validation loss: 3.1927980184555054

Epoch: 6| Step: 4
Training loss: 3.742408514022827
Validation loss: 3.1889971097310386

Epoch: 6| Step: 5
Training loss: 3.7428348064422607
Validation loss: 3.1859158277511597

Epoch: 6| Step: 6
Training loss: 3.1020145416259766
Validation loss: 3.182168881098429

Epoch: 6| Step: 7
Training loss: 4.08980131149292
Validation loss: 3.1788341204325357

Epoch: 6| Step: 8
Training loss: 1.973002314567566
Validation loss: 3.1755081017812095

Epoch: 6| Step: 9
Training loss: 2.7165637016296387
Validation loss: 3.1722470124562583

Epoch: 6| Step: 10
Training loss: 3.2471861839294434
Validation loss: 3.1720415353775024

Epoch: 6| Step: 11
Training loss: 3.262608528137207
Validation loss: 3.166795253753662

Epoch: 6| Step: 12
Training loss: 3.6148061752319336
Validation loss: 3.160007198651632

Epoch: 6| Step: 13
Training loss: 3.7775192260742188
Validation loss: 3.156407674153646

Epoch: 29| Step: 0
Training loss: 3.1721115112304688
Validation loss: 3.1522792975107827

Epoch: 6| Step: 1
Training loss: 3.06337308883667
Validation loss: 3.149105350176493

Epoch: 6| Step: 2
Training loss: 3.6126091480255127
Validation loss: 3.1461734771728516

Epoch: 6| Step: 3
Training loss: 3.755610704421997
Validation loss: 3.143381635348002

Epoch: 6| Step: 4
Training loss: 3.032897710800171
Validation loss: 3.1391270955403647

Epoch: 6| Step: 5
Training loss: 3.9247055053710938
Validation loss: 3.134855628013611

Epoch: 6| Step: 6
Training loss: 3.571375608444214
Validation loss: 3.131123145421346

Epoch: 6| Step: 7
Training loss: 3.0070109367370605
Validation loss: 3.126959522565206

Epoch: 6| Step: 8
Training loss: 2.7386975288391113
Validation loss: 3.123133341471354

Epoch: 6| Step: 9
Training loss: 3.6731600761413574
Validation loss: 3.1191395918528237

Epoch: 6| Step: 10
Training loss: 2.955672025680542
Validation loss: 3.116109291712443

Epoch: 6| Step: 11
Training loss: 3.805572986602783
Validation loss: 3.111608862876892

Epoch: 6| Step: 12
Training loss: 3.257190704345703
Validation loss: 3.1073336203893027

Epoch: 6| Step: 13
Training loss: 2.870415449142456
Validation loss: 3.103941559791565

Epoch: 30| Step: 0
Training loss: 3.5450148582458496
Validation loss: 3.1009682019551597

Epoch: 6| Step: 1
Training loss: 2.9222612380981445
Validation loss: 3.0973035097122192

Epoch: 6| Step: 2
Training loss: 3.158229351043701
Validation loss: 3.0947510401407876

Epoch: 6| Step: 3
Training loss: 3.139111042022705
Validation loss: 3.090198834737142

Epoch: 6| Step: 4
Training loss: 2.9654126167297363
Validation loss: 3.0870748360951743

Epoch: 6| Step: 5
Training loss: 3.29130220413208
Validation loss: 3.0840595960617065

Epoch: 6| Step: 6
Training loss: 3.2631325721740723
Validation loss: 3.081045389175415

Epoch: 6| Step: 7
Training loss: 3.8790321350097656
Validation loss: 3.078069726626078

Epoch: 6| Step: 8
Training loss: 2.888258934020996
Validation loss: 3.0748722751935325

Epoch: 6| Step: 9
Training loss: 4.157316207885742
Validation loss: 3.071406602859497

Epoch: 6| Step: 10
Training loss: 2.324387550354004
Validation loss: 3.0681240955988565

Epoch: 6| Step: 11
Training loss: 3.347193717956543
Validation loss: 3.0645524660746255

Epoch: 6| Step: 12
Training loss: 3.109720230102539
Validation loss: 3.0610252618789673

Epoch: 6| Step: 13
Training loss: 3.7895922660827637
Validation loss: 3.057677745819092

Epoch: 31| Step: 0
Training loss: 3.739053726196289
Validation loss: 3.054428497950236

Epoch: 6| Step: 1
Training loss: 3.3390469551086426
Validation loss: 3.051055590311686

Epoch: 6| Step: 2
Training loss: 4.012967109680176
Validation loss: 3.0480504035949707

Epoch: 6| Step: 3
Training loss: 3.389244318008423
Validation loss: 3.0446135997772217

Epoch: 6| Step: 4
Training loss: 2.887923002243042
Validation loss: 3.041211485862732

Epoch: 6| Step: 5
Training loss: 2.6967830657958984
Validation loss: 3.0375144879023233

Epoch: 6| Step: 6
Training loss: 3.1349329948425293
Validation loss: 3.034303824106852

Epoch: 6| Step: 7
Training loss: 3.6159772872924805
Validation loss: 3.0310347080230713

Epoch: 6| Step: 8
Training loss: 3.0278773307800293
Validation loss: 3.0278356075286865

Epoch: 6| Step: 9
Training loss: 2.9840612411499023
Validation loss: 3.0244922240575156

Epoch: 6| Step: 10
Training loss: 2.8727686405181885
Validation loss: 3.021356701850891

Epoch: 6| Step: 11
Training loss: 2.675426959991455
Validation loss: 3.0179869731267295

Epoch: 6| Step: 12
Training loss: 4.2216339111328125
Validation loss: 3.0150392850240073

Epoch: 6| Step: 13
Training loss: 2.571444272994995
Validation loss: 3.011484901110331

Epoch: 32| Step: 0
Training loss: 3.1416988372802734
Validation loss: 3.0081593990325928

Epoch: 6| Step: 1
Training loss: 4.221245765686035
Validation loss: 3.004873832066854

Epoch: 6| Step: 2
Training loss: 2.897280693054199
Validation loss: 3.001625140508016

Epoch: 6| Step: 3
Training loss: 3.005739688873291
Validation loss: 2.998154362042745

Epoch: 6| Step: 4
Training loss: 3.8466591835021973
Validation loss: 2.99478010336558

Epoch: 6| Step: 5
Training loss: 4.1123270988464355
Validation loss: 2.9914348125457764

Epoch: 6| Step: 6
Training loss: 2.709496021270752
Validation loss: 2.9877466360727944

Epoch: 6| Step: 7
Training loss: 3.0608065128326416
Validation loss: 2.9843987226486206

Epoch: 6| Step: 8
Training loss: 3.2791075706481934
Validation loss: 2.981016755104065

Epoch: 6| Step: 9
Training loss: 2.909867525100708
Validation loss: 2.9780013163884482

Epoch: 6| Step: 10
Training loss: 2.845262289047241
Validation loss: 2.9748497804005942

Epoch: 6| Step: 11
Training loss: 2.6865744590759277
Validation loss: 2.971519430478414

Epoch: 6| Step: 12
Training loss: 3.0035738945007324
Validation loss: 2.9684431552886963

Epoch: 6| Step: 13
Training loss: 2.8687171936035156
Validation loss: 2.965119560559591

Epoch: 33| Step: 0
Training loss: 2.990851402282715
Validation loss: 2.962841192881266

Epoch: 6| Step: 1
Training loss: 3.6949124336242676
Validation loss: 2.95908522605896

Epoch: 6| Step: 2
Training loss: 3.367541790008545
Validation loss: 2.957151730855306

Epoch: 6| Step: 3
Training loss: 3.101567268371582
Validation loss: 2.9579040010770163

Epoch: 6| Step: 4
Training loss: 3.678373336791992
Validation loss: 2.9780638615290322

Epoch: 6| Step: 5
Training loss: 2.773693323135376
Validation loss: 2.9476791620254517

Epoch: 6| Step: 6
Training loss: 2.706221103668213
Validation loss: 2.9446529150009155

Epoch: 6| Step: 7
Training loss: 3.6465797424316406
Validation loss: 2.9410802125930786

Epoch: 6| Step: 8
Training loss: 2.479341506958008
Validation loss: 2.9381761948267617

Epoch: 6| Step: 9
Training loss: 3.780060291290283
Validation loss: 2.937053839365641

Epoch: 6| Step: 10
Training loss: 2.441251516342163
Validation loss: 2.9432514905929565

Epoch: 6| Step: 11
Training loss: 3.134486675262451
Validation loss: 2.9323561986287436

Epoch: 6| Step: 12
Training loss: 3.0238451957702637
Validation loss: 2.929016351699829

Epoch: 6| Step: 13
Training loss: 3.2305257320404053
Validation loss: 2.9263256390889487

Epoch: 34| Step: 0
Training loss: 3.025649070739746
Validation loss: 2.9236861864725747

Epoch: 6| Step: 1
Training loss: 3.197521209716797
Validation loss: 2.9202186663945517

Epoch: 6| Step: 2
Training loss: 3.7495932579040527
Validation loss: 2.918116807937622

Epoch: 6| Step: 3
Training loss: 3.2365870475769043
Validation loss: 2.914223233858744

Epoch: 6| Step: 4
Training loss: 2.5118300914764404
Validation loss: 2.9110942284266152

Epoch: 6| Step: 5
Training loss: 3.2537102699279785
Validation loss: 2.9079784949620566

Epoch: 6| Step: 6
Training loss: 3.0374813079833984
Validation loss: 2.903977394104004

Epoch: 6| Step: 7
Training loss: 3.024535655975342
Validation loss: 2.9011288483937583

Epoch: 6| Step: 8
Training loss: 2.9930450916290283
Validation loss: 2.8975656429926553

Epoch: 6| Step: 9
Training loss: 2.8342456817626953
Validation loss: 2.8929596741994223

Epoch: 6| Step: 10
Training loss: 3.153838634490967
Validation loss: 2.891088326772054

Epoch: 6| Step: 11
Training loss: 3.316159725189209
Validation loss: 2.8873622020085654

Epoch: 6| Step: 12
Training loss: 2.8338308334350586
Validation loss: 2.8849870761235556

Epoch: 6| Step: 13
Training loss: 3.360837936401367
Validation loss: 2.882664958635966

Epoch: 35| Step: 0
Training loss: 3.083070755004883
Validation loss: 2.8798147439956665

Epoch: 6| Step: 1
Training loss: 2.9040331840515137
Validation loss: 2.8766100804011026

Epoch: 6| Step: 2
Training loss: 3.2080914974212646
Validation loss: 2.873295029004415

Epoch: 6| Step: 3
Training loss: 2.5331742763519287
Validation loss: 2.8692966302235923

Epoch: 6| Step: 4
Training loss: 4.024117469787598
Validation loss: 2.865759094556173

Epoch: 6| Step: 5
Training loss: 3.0328173637390137
Validation loss: 2.8620446920394897

Epoch: 6| Step: 6
Training loss: 2.709354877471924
Validation loss: 2.859192649523417

Epoch: 6| Step: 7
Training loss: 3.713771343231201
Validation loss: 2.8571052153905234

Epoch: 6| Step: 8
Training loss: 3.3568875789642334
Validation loss: 2.8534539937973022

Epoch: 6| Step: 9
Training loss: 2.7640976905822754
Validation loss: 2.8508716424306235

Epoch: 6| Step: 10
Training loss: 2.6783270835876465
Validation loss: 2.848344624042511

Epoch: 6| Step: 11
Training loss: 3.2619755268096924
Validation loss: 2.845579981803894

Epoch: 6| Step: 12
Training loss: 2.9568002223968506
Validation loss: 2.8442513942718506

Epoch: 6| Step: 13
Training loss: 2.7593517303466797
Validation loss: 2.8397247592608132

Epoch: 36| Step: 0
Training loss: 2.7205047607421875
Validation loss: 2.8363024393717446

Epoch: 6| Step: 1
Training loss: 2.6572673320770264
Validation loss: 2.833804130554199

Epoch: 6| Step: 2
Training loss: 3.200547695159912
Validation loss: 2.8297228813171387

Epoch: 6| Step: 3
Training loss: 3.2734782695770264
Validation loss: 2.829951524734497

Epoch: 6| Step: 4
Training loss: 2.2102365493774414
Validation loss: 2.825532158215841

Epoch: 6| Step: 5
Training loss: 3.525116205215454
Validation loss: 2.8231635093688965

Epoch: 6| Step: 6
Training loss: 3.1573424339294434
Validation loss: 2.819968064626058

Epoch: 6| Step: 7
Training loss: 2.8605735301971436
Validation loss: 2.8190805912017822

Epoch: 6| Step: 8
Training loss: 2.6799659729003906
Validation loss: 2.8158204158147178

Epoch: 6| Step: 9
Training loss: 3.6222128868103027
Validation loss: 2.8156619866689048

Epoch: 6| Step: 10
Training loss: 3.7262868881225586
Validation loss: 2.8123128016789756

Epoch: 6| Step: 11
Training loss: 2.732837200164795
Validation loss: 2.8093798955281577

Epoch: 6| Step: 12
Training loss: 2.444544792175293
Validation loss: 2.807471513748169

Epoch: 6| Step: 13
Training loss: 3.58683180809021
Validation loss: 2.8036150534947715

Epoch: 37| Step: 0
Training loss: 3.259777784347534
Validation loss: 2.8001128435134888

Epoch: 6| Step: 1
Training loss: 3.2236392498016357
Validation loss: 2.7984413703282676

Epoch: 6| Step: 2
Training loss: 2.4714245796203613
Validation loss: 2.794250965118408

Epoch: 6| Step: 3
Training loss: 2.2806034088134766
Validation loss: 2.790958881378174

Epoch: 6| Step: 4
Training loss: 3.5057177543640137
Validation loss: 2.7891902128855386

Epoch: 6| Step: 5
Training loss: 3.6009674072265625
Validation loss: 2.7851818402608237

Epoch: 6| Step: 6
Training loss: 3.2639951705932617
Validation loss: 2.7832851807276406

Epoch: 6| Step: 7
Training loss: 2.4774749279022217
Validation loss: 2.781065265337626

Epoch: 6| Step: 8
Training loss: 2.890078544616699
Validation loss: 2.779929995536804

Epoch: 6| Step: 9
Training loss: 3.0846004486083984
Validation loss: 2.788697918256124

Epoch: 6| Step: 10
Training loss: 2.774841785430908
Validation loss: 2.782474398612976

Epoch: 6| Step: 11
Training loss: 3.26052188873291
Validation loss: 2.769301414489746

Epoch: 6| Step: 12
Training loss: 2.70501708984375
Validation loss: 2.7679990927378335

Epoch: 6| Step: 13
Training loss: 3.1253886222839355
Validation loss: 2.763585686683655

Epoch: 38| Step: 0
Training loss: 2.702284574508667
Validation loss: 2.76043438911438

Epoch: 6| Step: 1
Training loss: 2.425328254699707
Validation loss: 2.757675886154175

Epoch: 6| Step: 2
Training loss: 3.0109715461730957
Validation loss: 2.754331052303314

Epoch: 6| Step: 3
Training loss: 2.4814038276672363
Validation loss: 2.750918388366699

Epoch: 6| Step: 4
Training loss: 3.360783576965332
Validation loss: 2.749923348426819

Epoch: 6| Step: 5
Training loss: 3.1331377029418945
Validation loss: 2.747545003890991

Epoch: 6| Step: 6
Training loss: 2.9927923679351807
Validation loss: 2.7447898785273233

Epoch: 6| Step: 7
Training loss: 3.236396312713623
Validation loss: 2.742872397104899

Epoch: 6| Step: 8
Training loss: 3.00665283203125
Validation loss: 2.7402575810750327

Epoch: 6| Step: 9
Training loss: 3.1606812477111816
Validation loss: 2.7350629568099976

Epoch: 6| Step: 10
Training loss: 3.7656750679016113
Validation loss: 2.73387881120046

Epoch: 6| Step: 11
Training loss: 3.266228437423706
Validation loss: 2.733110706011454

Epoch: 6| Step: 12
Training loss: 2.5693702697753906
Validation loss: 2.7292210857073465

Epoch: 6| Step: 13
Training loss: 2.2164437770843506
Validation loss: 2.7247469425201416

Epoch: 39| Step: 0
Training loss: 3.663666009902954
Validation loss: 2.723313291867574

Epoch: 6| Step: 1
Training loss: 2.8724541664123535
Validation loss: 2.7214314142862954

Epoch: 6| Step: 2
Training loss: 3.614306926727295
Validation loss: 2.718433062235514

Epoch: 6| Step: 3
Training loss: 2.9448680877685547
Validation loss: 2.723377784093221

Epoch: 6| Step: 4
Training loss: 2.896404266357422
Validation loss: 2.7248535553614297

Epoch: 6| Step: 5
Training loss: 3.057647705078125
Validation loss: 2.7133891582489014

Epoch: 6| Step: 6
Training loss: 2.7794582843780518
Validation loss: 2.7068970799446106

Epoch: 6| Step: 7
Training loss: 2.7236239910125732
Validation loss: 2.7035451332728067

Epoch: 6| Step: 8
Training loss: 2.464216709136963
Validation loss: 2.702524244785309

Epoch: 6| Step: 9
Training loss: 2.246386766433716
Validation loss: 2.700715184211731

Epoch: 6| Step: 10
Training loss: 2.8159637451171875
Validation loss: 2.698872923851013

Epoch: 6| Step: 11
Training loss: 3.1484851837158203
Validation loss: 2.696319023768107

Epoch: 6| Step: 12
Training loss: 2.52567720413208
Validation loss: 2.693524638811747

Epoch: 6| Step: 13
Training loss: 3.0953426361083984
Validation loss: 2.6905145247777305

Epoch: 40| Step: 0
Training loss: 2.367640972137451
Validation loss: 2.6880733569463096

Epoch: 6| Step: 1
Training loss: 2.9397337436676025
Validation loss: 2.685518980026245

Epoch: 6| Step: 2
Training loss: 3.1332345008850098
Validation loss: 2.682586431503296

Epoch: 6| Step: 3
Training loss: 2.8885498046875
Validation loss: 2.679025332132975

Epoch: 6| Step: 4
Training loss: 3.5900747776031494
Validation loss: 2.675687630971273

Epoch: 6| Step: 5
Training loss: 3.0215396881103516
Validation loss: 2.672683676083883

Epoch: 6| Step: 6
Training loss: 2.466566562652588
Validation loss: 2.6701837380727134

Epoch: 6| Step: 7
Training loss: 2.8768396377563477
Validation loss: 2.6684234937032065

Epoch: 6| Step: 8
Training loss: 2.7596938610076904
Validation loss: 2.6693604787190757

Epoch: 6| Step: 9
Training loss: 3.1280293464660645
Validation loss: 2.663994550704956

Epoch: 6| Step: 10
Training loss: 3.1751890182495117
Validation loss: 2.6617527405420938

Epoch: 6| Step: 11
Training loss: 2.006896495819092
Validation loss: 2.6565072933832803

Epoch: 6| Step: 12
Training loss: 2.5183615684509277
Validation loss: 2.6519484321276345

Epoch: 6| Step: 13
Training loss: 3.3777034282684326
Validation loss: 2.64893368879954

Epoch: 41| Step: 0
Training loss: 2.5739054679870605
Validation loss: 2.648139238357544

Epoch: 6| Step: 1
Training loss: 3.635780096054077
Validation loss: 2.646438241004944

Epoch: 6| Step: 2
Training loss: 2.724796772003174
Validation loss: 2.641373336315155

Epoch: 6| Step: 3
Training loss: 3.219116687774658
Validation loss: 2.639108141263326

Epoch: 6| Step: 4
Training loss: 2.2384419441223145
Validation loss: 2.6400530338287354

Epoch: 6| Step: 5
Training loss: 2.60142183303833
Validation loss: 2.6440972884496055

Epoch: 6| Step: 6
Training loss: 2.406917095184326
Validation loss: 2.633715828259786

Epoch: 6| Step: 7
Training loss: 2.687032699584961
Validation loss: 2.6327444314956665

Epoch: 6| Step: 8
Training loss: 2.3552494049072266
Validation loss: 2.6256869236628213

Epoch: 6| Step: 9
Training loss: 2.6446468830108643
Validation loss: 2.6239885687828064

Epoch: 6| Step: 10
Training loss: 2.7709808349609375
Validation loss: 2.619003931681315

Epoch: 6| Step: 11
Training loss: 2.806710958480835
Validation loss: 2.618520418802897

Epoch: 6| Step: 12
Training loss: 4.227364540100098
Validation loss: 2.6154703299204507

Epoch: 6| Step: 13
Training loss: 2.716712474822998
Validation loss: 2.6133436361948648

Epoch: 42| Step: 0
Training loss: 1.9999871253967285
Validation loss: 2.609899361928304

Epoch: 6| Step: 1
Training loss: 3.4593586921691895
Validation loss: 2.6068586111068726

Epoch: 6| Step: 2
Training loss: 2.3920657634735107
Validation loss: 2.6048101584116616

Epoch: 6| Step: 3
Training loss: 3.161224126815796
Validation loss: 2.6023909648259482

Epoch: 6| Step: 4
Training loss: 2.7872517108917236
Validation loss: 2.5998507340749106

Epoch: 6| Step: 5
Training loss: 2.833732843399048
Validation loss: 2.5964812437693277

Epoch: 6| Step: 6
Training loss: 3.0961947441101074
Validation loss: 2.5949808756510415

Epoch: 6| Step: 7
Training loss: 2.9579029083251953
Validation loss: 2.5907277266184487

Epoch: 6| Step: 8
Training loss: 3.0787508487701416
Validation loss: 2.5859301487604776

Epoch: 6| Step: 9
Training loss: 2.9279212951660156
Validation loss: 2.5843791564305625

Epoch: 6| Step: 10
Training loss: 2.7154297828674316
Validation loss: 2.582213560740153

Epoch: 6| Step: 11
Training loss: 2.955587863922119
Validation loss: 2.5782851378122964

Epoch: 6| Step: 12
Training loss: 1.8270915746688843
Validation loss: 2.577713926633199

Epoch: 6| Step: 13
Training loss: 2.886439800262451
Validation loss: 2.5722983678181968

Epoch: 43| Step: 0
Training loss: 3.15183687210083
Validation loss: 2.5693113009134927

Epoch: 6| Step: 1
Training loss: 2.5630884170532227
Validation loss: 2.5663983821868896

Epoch: 6| Step: 2
Training loss: 2.320089340209961
Validation loss: 2.5650081237157187

Epoch: 6| Step: 3
Training loss: 3.4092676639556885
Validation loss: 2.564535617828369

Epoch: 6| Step: 4
Training loss: 2.374678134918213
Validation loss: 2.561089356740316

Epoch: 6| Step: 5
Training loss: 3.0319480895996094
Validation loss: 2.558416207631429

Epoch: 6| Step: 6
Training loss: 2.242159128189087
Validation loss: 2.555044094721476

Epoch: 6| Step: 7
Training loss: 2.5818097591400146
Validation loss: 2.5529611110687256

Epoch: 6| Step: 8
Training loss: 3.0887341499328613
Validation loss: 2.5508456428845725

Epoch: 6| Step: 9
Training loss: 2.358686923980713
Validation loss: 2.548373579978943

Epoch: 6| Step: 10
Training loss: 3.1559367179870605
Validation loss: 2.5513381958007812

Epoch: 6| Step: 11
Training loss: 2.2352795600891113
Validation loss: 2.551125685373942

Epoch: 6| Step: 12
Training loss: 3.5530030727386475
Validation loss: 2.562563975652059

Epoch: 6| Step: 13
Training loss: 2.4591116905212402
Validation loss: 2.558997392654419

Epoch: 44| Step: 0
Training loss: 2.8276777267456055
Validation loss: 2.5397573709487915

Epoch: 6| Step: 1
Training loss: 2.908132553100586
Validation loss: 2.5340352257092795

Epoch: 6| Step: 2
Training loss: 2.651087760925293
Validation loss: 2.5315449237823486

Epoch: 6| Step: 3
Training loss: 2.9822402000427246
Validation loss: 2.530613601207733

Epoch: 6| Step: 4
Training loss: 1.8278393745422363
Validation loss: 2.5306512117385864

Epoch: 6| Step: 5
Training loss: 2.6703438758850098
Validation loss: 2.529398759206136

Epoch: 6| Step: 6
Training loss: 2.6668343544006348
Validation loss: 2.5303743282953897

Epoch: 6| Step: 7
Training loss: 2.470205783843994
Validation loss: 2.5319198767344155

Epoch: 6| Step: 8
Training loss: 2.9396567344665527
Validation loss: 2.5293102264404297

Epoch: 6| Step: 9
Training loss: 2.9027857780456543
Validation loss: 2.5287864208221436

Epoch: 6| Step: 10
Training loss: 3.058255672454834
Validation loss: 2.5207735101381936

Epoch: 6| Step: 11
Training loss: 2.9840762615203857
Validation loss: 2.5177759329477944

Epoch: 6| Step: 12
Training loss: 3.013517379760742
Validation loss: 2.511786858240763

Epoch: 6| Step: 13
Training loss: 2.2633416652679443
Validation loss: 2.5074237982432046

Epoch: 45| Step: 0
Training loss: 3.1974682807922363
Validation loss: 2.5033135612805686

Epoch: 6| Step: 1
Training loss: 2.5125274658203125
Validation loss: 2.5008572340011597

Epoch: 6| Step: 2
Training loss: 2.9873366355895996
Validation loss: 2.4984461267789206

Epoch: 6| Step: 3
Training loss: 2.044602870941162
Validation loss: 2.4955946604410806

Epoch: 6| Step: 4
Training loss: 2.396031618118286
Validation loss: 2.494107643763224

Epoch: 6| Step: 5
Training loss: 2.5266928672790527
Validation loss: 2.4910284678141275

Epoch: 6| Step: 6
Training loss: 2.7338719367980957
Validation loss: 2.4879883925120034

Epoch: 6| Step: 7
Training loss: 2.805027484893799
Validation loss: 2.487809141476949

Epoch: 6| Step: 8
Training loss: 2.5455968379974365
Validation loss: 2.4861582120259604

Epoch: 6| Step: 9
Training loss: 2.8598036766052246
Validation loss: 2.4828094244003296

Epoch: 6| Step: 10
Training loss: 2.8479573726654053
Validation loss: 2.48281862338384

Epoch: 6| Step: 11
Training loss: 2.5639114379882812
Validation loss: 2.4769999186197915

Epoch: 6| Step: 12
Training loss: 2.591184377670288
Validation loss: 2.4757745265960693

Epoch: 6| Step: 13
Training loss: 2.8579962253570557
Validation loss: 2.4757262070973716

Epoch: 46| Step: 0
Training loss: 3.350400447845459
Validation loss: 2.4713706970214844

Epoch: 6| Step: 1
Training loss: 3.112194061279297
Validation loss: 2.467165986696879

Epoch: 6| Step: 2
Training loss: 2.3212714195251465
Validation loss: 2.4630351861317954

Epoch: 6| Step: 3
Training loss: 2.2790980339050293
Validation loss: 2.4609501361846924

Epoch: 6| Step: 4
Training loss: 2.637321710586548
Validation loss: 2.4571438431739807

Epoch: 6| Step: 5
Training loss: 2.833559513092041
Validation loss: 2.4553428490956626

Epoch: 6| Step: 6
Training loss: 3.127298355102539
Validation loss: 2.451995015144348

Epoch: 6| Step: 7
Training loss: 2.6698520183563232
Validation loss: 2.4503963390986123

Epoch: 6| Step: 8
Training loss: 2.3390674591064453
Validation loss: 2.44881808757782

Epoch: 6| Step: 9
Training loss: 2.3584866523742676
Validation loss: 2.4459308385849

Epoch: 6| Step: 10
Training loss: 2.513241767883301
Validation loss: 2.4423009554545083

Epoch: 6| Step: 11
Training loss: 2.691762924194336
Validation loss: 2.438931186993917

Epoch: 6| Step: 12
Training loss: 2.4731907844543457
Validation loss: 2.441893219947815

Epoch: 6| Step: 13
Training loss: 2.25612211227417
Validation loss: 2.434707780679067

Epoch: 47| Step: 0
Training loss: 2.3941943645477295
Validation loss: 2.4356980323791504

Epoch: 6| Step: 1
Training loss: 2.384319543838501
Validation loss: 2.434250275293986

Epoch: 6| Step: 2
Training loss: 2.3304896354675293
Validation loss: 2.439253807067871

Epoch: 6| Step: 3
Training loss: 2.8913002014160156
Validation loss: 2.449661691983541

Epoch: 6| Step: 4
Training loss: 2.4574179649353027
Validation loss: 2.4273608128229776

Epoch: 6| Step: 5
Training loss: 2.666844367980957
Validation loss: 2.425954500834147

Epoch: 6| Step: 6
Training loss: 3.6330373287200928
Validation loss: 2.4284761548042297

Epoch: 6| Step: 7
Training loss: 1.94675612449646
Validation loss: 2.4201063315073648

Epoch: 6| Step: 8
Training loss: 3.001096487045288
Validation loss: 2.4154762824376426

Epoch: 6| Step: 9
Training loss: 2.5411577224731445
Validation loss: 2.4150946140289307

Epoch: 6| Step: 10
Training loss: 2.381704330444336
Validation loss: 2.4124560753504434

Epoch: 6| Step: 11
Training loss: 2.2800869941711426
Validation loss: 2.4093875885009766

Epoch: 6| Step: 12
Training loss: 3.0199317932128906
Validation loss: 2.4116339087486267

Epoch: 6| Step: 13
Training loss: 2.5561723709106445
Validation loss: 2.4104527036348977

Epoch: 48| Step: 0
Training loss: 2.685229778289795
Validation loss: 2.410418669382731

Epoch: 6| Step: 1
Training loss: 2.79537296295166
Validation loss: 2.4087629516919455

Epoch: 6| Step: 2
Training loss: 2.9158029556274414
Validation loss: 2.406360367933909

Epoch: 6| Step: 3
Training loss: 2.271785259246826
Validation loss: 2.4033952156702676

Epoch: 6| Step: 4
Training loss: 2.0321719646453857
Validation loss: 2.399659355481466

Epoch: 6| Step: 5
Training loss: 2.3469955921173096
Validation loss: 2.3943293491999307

Epoch: 6| Step: 6
Training loss: 3.137208938598633
Validation loss: 2.3928133249282837

Epoch: 6| Step: 7
Training loss: 2.3184022903442383
Validation loss: 2.3891667326291404

Epoch: 6| Step: 8
Training loss: 2.8526928424835205
Validation loss: 2.3870052099227905

Epoch: 6| Step: 9
Training loss: 2.163482189178467
Validation loss: 2.380228598912557

Epoch: 6| Step: 10
Training loss: 2.4879610538482666
Validation loss: 2.381378730138143

Epoch: 6| Step: 11
Training loss: 2.7332024574279785
Validation loss: 2.379414757092794

Epoch: 6| Step: 12
Training loss: 2.4404571056365967
Validation loss: 2.3742255171140036

Epoch: 6| Step: 13
Training loss: 2.8415656089782715
Validation loss: 2.3712218006451926

Epoch: 49| Step: 0
Training loss: 2.5753445625305176
Validation loss: 2.3680076201756797

Epoch: 6| Step: 1
Training loss: 2.6380083560943604
Validation loss: 2.3637006680170694

Epoch: 6| Step: 2
Training loss: 2.019545078277588
Validation loss: 2.362626036008199

Epoch: 6| Step: 3
Training loss: 3.007472276687622
Validation loss: 2.3617117007573447

Epoch: 6| Step: 4
Training loss: 2.876478672027588
Validation loss: 2.358777562777201

Epoch: 6| Step: 5
Training loss: 3.0499119758605957
Validation loss: 2.360467274983724

Epoch: 6| Step: 6
Training loss: 1.7308679819107056
Validation loss: 2.358950396378835

Epoch: 6| Step: 7
Training loss: 2.7480897903442383
Validation loss: 2.3572009801864624

Epoch: 6| Step: 8
Training loss: 2.673431396484375
Validation loss: 2.3552948236465454

Epoch: 6| Step: 9
Training loss: 2.676466464996338
Validation loss: 2.3527499238650003

Epoch: 6| Step: 10
Training loss: 1.8598955869674683
Validation loss: 2.348556399345398

Epoch: 6| Step: 11
Training loss: 2.702838182449341
Validation loss: 2.347359915574392

Epoch: 6| Step: 12
Training loss: 2.2476189136505127
Validation loss: 2.346358299255371

Epoch: 6| Step: 13
Training loss: 2.6894030570983887
Validation loss: 2.3425273299217224

Epoch: 50| Step: 0
Training loss: 2.6426870822906494
Validation loss: 2.340291976928711

Epoch: 6| Step: 1
Training loss: 2.642655372619629
Validation loss: 2.3424779574076333

Epoch: 6| Step: 2
Training loss: 2.3486130237579346
Validation loss: 2.3361738125483194

Epoch: 6| Step: 3
Training loss: 2.586803913116455
Validation loss: 2.3353174924850464

Epoch: 6| Step: 4
Training loss: 2.2785120010375977
Validation loss: 2.3373780250549316

Epoch: 6| Step: 5
Training loss: 2.7080085277557373
Validation loss: 2.330747922261556

Epoch: 6| Step: 6
Training loss: 2.1500368118286133
Validation loss: 2.3258200883865356

Epoch: 6| Step: 7
Training loss: 2.2886691093444824
Validation loss: 2.3256672024726868

Epoch: 6| Step: 8
Training loss: 2.530161142349243
Validation loss: 2.325476368268331

Epoch: 6| Step: 9
Training loss: 2.063589096069336
Validation loss: 2.327250599861145

Epoch: 6| Step: 10
Training loss: 2.5093636512756348
Validation loss: 2.327310542265574

Epoch: 6| Step: 11
Training loss: 2.9031667709350586
Validation loss: 2.3319106499354043

Epoch: 6| Step: 12
Training loss: 3.1513218879699707
Validation loss: 2.3298781315485635

Epoch: 6| Step: 13
Training loss: 2.3182950019836426
Validation loss: 2.315510710080465

Epoch: 51| Step: 0
Training loss: 3.265157461166382
Validation loss: 2.3095786571502686

Epoch: 6| Step: 1
Training loss: 2.4332103729248047
Validation loss: 2.3120707273483276

Epoch: 6| Step: 2
Training loss: 1.6879280805587769
Validation loss: 2.311541755994161

Epoch: 6| Step: 3
Training loss: 2.319310426712036
Validation loss: 2.309936046600342

Epoch: 6| Step: 4
Training loss: 2.71086049079895
Validation loss: 2.3098341822624207

Epoch: 6| Step: 5
Training loss: 2.587096929550171
Validation loss: 2.3122554421424866

Epoch: 6| Step: 6
Training loss: 3.0001988410949707
Validation loss: 2.3107529083887735

Epoch: 6| Step: 7
Training loss: 2.943058967590332
Validation loss: 2.3162772059440613

Epoch: 6| Step: 8
Training loss: 2.4448187351226807
Validation loss: 2.309589684009552

Epoch: 6| Step: 9
Training loss: 2.4276962280273438
Validation loss: 2.303430120150248

Epoch: 6| Step: 10
Training loss: 2.5114288330078125
Validation loss: 2.2992231051127114

Epoch: 6| Step: 11
Training loss: 2.0079195499420166
Validation loss: 2.2943559090296426

Epoch: 6| Step: 12
Training loss: 2.4388203620910645
Validation loss: 2.2934045592943826

Epoch: 6| Step: 13
Training loss: 1.7771196365356445
Validation loss: 2.292128562927246

Epoch: 52| Step: 0
Training loss: 1.5928629636764526
Validation loss: 2.2880788246790567

Epoch: 6| Step: 1
Training loss: 2.7254080772399902
Validation loss: 2.288019061088562

Epoch: 6| Step: 2
Training loss: 2.7908449172973633
Validation loss: 2.2859528064727783

Epoch: 6| Step: 3
Training loss: 3.3137521743774414
Validation loss: 2.285176396369934

Epoch: 6| Step: 4
Training loss: 2.4883322715759277
Validation loss: 2.282449245452881

Epoch: 6| Step: 5
Training loss: 2.233112335205078
Validation loss: 2.2779279549916587

Epoch: 6| Step: 6
Training loss: 1.7100067138671875
Validation loss: 2.274134874343872

Epoch: 6| Step: 7
Training loss: 2.7145674228668213
Validation loss: 2.270765483379364

Epoch: 6| Step: 8
Training loss: 2.651202440261841
Validation loss: 2.268302838007609

Epoch: 6| Step: 9
Training loss: 2.618899345397949
Validation loss: 2.264986515045166

Epoch: 6| Step: 10
Training loss: 2.3854379653930664
Validation loss: 2.2613160610198975

Epoch: 6| Step: 11
Training loss: 1.8073877096176147
Validation loss: 2.258530020713806

Epoch: 6| Step: 12
Training loss: 2.587627649307251
Validation loss: 2.257601340611776

Epoch: 6| Step: 13
Training loss: 2.5101890563964844
Validation loss: 2.2540676991144815

Epoch: 53| Step: 0
Training loss: 3.271270275115967
Validation loss: 2.2518508036931357

Epoch: 6| Step: 1
Training loss: 2.4402353763580322
Validation loss: 2.248364528020223

Epoch: 6| Step: 2
Training loss: 2.579655647277832
Validation loss: 2.2458194295565286

Epoch: 6| Step: 3
Training loss: 2.5538158416748047
Validation loss: 2.2462493777275085

Epoch: 6| Step: 4
Training loss: 2.0888586044311523
Validation loss: 2.2449240684509277

Epoch: 6| Step: 5
Training loss: 2.8804643154144287
Validation loss: 2.2451220552126565

Epoch: 6| Step: 6
Training loss: 1.9673731327056885
Validation loss: 2.2440588672955832

Epoch: 6| Step: 7
Training loss: 2.420527458190918
Validation loss: 2.2402923107147217

Epoch: 6| Step: 8
Training loss: 1.58538818359375
Validation loss: 2.236935297648112

Epoch: 6| Step: 9
Training loss: 2.400333881378174
Validation loss: 2.2347952922185264

Epoch: 6| Step: 10
Training loss: 2.121516227722168
Validation loss: 2.2328075965245566

Epoch: 6| Step: 11
Training loss: 2.3950858116149902
Validation loss: 2.231709639231364

Epoch: 6| Step: 12
Training loss: 2.425124406814575
Validation loss: 2.2281084060668945

Epoch: 6| Step: 13
Training loss: 2.5521457195281982
Validation loss: 2.2249672015508017

Epoch: 54| Step: 0
Training loss: 1.4808080196380615
Validation loss: 2.2200939059257507

Epoch: 6| Step: 1
Training loss: 2.4992895126342773
Validation loss: 2.2225261330604553

Epoch: 6| Step: 2
Training loss: 2.7553932666778564
Validation loss: 2.217807650566101

Epoch: 6| Step: 3
Training loss: 2.5926833152770996
Validation loss: 2.2203264037768045

Epoch: 6| Step: 4
Training loss: 2.3651037216186523
Validation loss: 2.2130601604779563

Epoch: 6| Step: 5
Training loss: 3.0035417079925537
Validation loss: 2.2131511767705283

Epoch: 6| Step: 6
Training loss: 2.8852405548095703
Validation loss: 2.2182233929634094

Epoch: 6| Step: 7
Training loss: 1.7624443769454956
Validation loss: 2.2183168530464172

Epoch: 6| Step: 8
Training loss: 2.0835113525390625
Validation loss: 2.203905006249746

Epoch: 6| Step: 9
Training loss: 2.470397472381592
Validation loss: 2.2002652088801065

Epoch: 6| Step: 10
Training loss: 2.664822816848755
Validation loss: 2.205769658088684

Epoch: 6| Step: 11
Training loss: 1.9473788738250732
Validation loss: 2.206738849480947

Epoch: 6| Step: 12
Training loss: 2.2309775352478027
Validation loss: 2.205015460650126

Epoch: 6| Step: 13
Training loss: 2.4650402069091797
Validation loss: 2.2057902812957764

Epoch: 55| Step: 0
Training loss: 2.3000240325927734
Validation loss: 2.204204877217611

Epoch: 6| Step: 1
Training loss: 2.4282305240631104
Validation loss: 2.200687845547994

Epoch: 6| Step: 2
Training loss: 2.1578750610351562
Validation loss: 2.2039910356203714

Epoch: 6| Step: 3
Training loss: 2.7353873252868652
Validation loss: 2.197347124417623

Epoch: 6| Step: 4
Training loss: 2.6665358543395996
Validation loss: 2.1979589263598123

Epoch: 6| Step: 5
Training loss: 2.0079598426818848
Validation loss: 2.1926709413528442

Epoch: 6| Step: 6
Training loss: 1.8573644161224365
Validation loss: 2.1943538586298623

Epoch: 6| Step: 7
Training loss: 2.3053269386291504
Validation loss: 2.1907010475794473

Epoch: 6| Step: 8
Training loss: 2.199840784072876
Validation loss: 2.1891975005467734

Epoch: 6| Step: 9
Training loss: 2.270197868347168
Validation loss: 2.181528170903524

Epoch: 6| Step: 10
Training loss: 2.4522018432617188
Validation loss: 2.1849425236384072

Epoch: 6| Step: 11
Training loss: 2.269759178161621
Validation loss: 2.18379008769989

Epoch: 6| Step: 12
Training loss: 2.4475553035736084
Validation loss: 2.1753594279289246

Epoch: 6| Step: 13
Training loss: 2.7534050941467285
Validation loss: 2.182814657688141

Epoch: 56| Step: 0
Training loss: 2.440054178237915
Validation loss: 2.1802685459454856

Epoch: 6| Step: 1
Training loss: 1.855939507484436
Validation loss: 2.1742327014605203

Epoch: 6| Step: 2
Training loss: 2.742785930633545
Validation loss: 2.1803527077039084

Epoch: 6| Step: 3
Training loss: 2.3817861080169678
Validation loss: 2.18291183312734

Epoch: 6| Step: 4
Training loss: 2.187546730041504
Validation loss: 2.178465267022451

Epoch: 6| Step: 5
Training loss: 2.296847105026245
Validation loss: 2.1749765475591025

Epoch: 6| Step: 6
Training loss: 2.5580899715423584
Validation loss: 2.1805690924326577

Epoch: 6| Step: 7
Training loss: 2.4854884147644043
Validation loss: 2.1771143277486167

Epoch: 6| Step: 8
Training loss: 2.006568193435669
Validation loss: 2.1763874093691506

Epoch: 6| Step: 9
Training loss: 1.7112934589385986
Validation loss: 2.1782383720080056

Epoch: 6| Step: 10
Training loss: 2.357077121734619
Validation loss: 2.1805516878763833

Epoch: 6| Step: 11
Training loss: 1.789128303527832
Validation loss: 2.1755950649579368

Epoch: 6| Step: 12
Training loss: 2.7415332794189453
Validation loss: 2.178045332431793

Epoch: 6| Step: 13
Training loss: 3.158151388168335
Validation loss: 2.1735959649086

Epoch: 57| Step: 0
Training loss: 2.664078712463379
Validation loss: 2.1726299126942954

Epoch: 6| Step: 1
Training loss: 1.8206363916397095
Validation loss: 2.1688305338223777

Epoch: 6| Step: 2
Training loss: 1.9362343549728394
Validation loss: 2.1694516142209372

Epoch: 6| Step: 3
Training loss: 2.4855031967163086
Validation loss: 2.1669926842053733

Epoch: 6| Step: 4
Training loss: 2.1072278022766113
Validation loss: 2.1656185189882913

Epoch: 6| Step: 5
Training loss: 2.497743606567383
Validation loss: 2.164119283358256

Epoch: 6| Step: 6
Training loss: 2.763655185699463
Validation loss: 2.159397224585215

Epoch: 6| Step: 7
Training loss: 2.5385496616363525
Validation loss: 2.1597511370976767

Epoch: 6| Step: 8
Training loss: 1.8665436506271362
Validation loss: 2.1563759644826255

Epoch: 6| Step: 9
Training loss: 2.657564163208008
Validation loss: 2.1498637994130454

Epoch: 6| Step: 10
Training loss: 1.7273279428482056
Validation loss: 2.1536563634872437

Epoch: 6| Step: 11
Training loss: 2.8329720497131348
Validation loss: 2.1490912437438965

Epoch: 6| Step: 12
Training loss: 2.270789623260498
Validation loss: 2.1480120420455933

Epoch: 6| Step: 13
Training loss: 2.3795676231384277
Validation loss: 2.1470778385798135

Epoch: 58| Step: 0
Training loss: 2.139223575592041
Validation loss: 2.1500356992085776

Epoch: 6| Step: 1
Training loss: 2.337252140045166
Validation loss: 2.1492669781049094

Epoch: 6| Step: 2
Training loss: 2.318422555923462
Validation loss: 2.146702686945597

Epoch: 6| Step: 3
Training loss: 2.3648810386657715
Validation loss: 2.142009655634562

Epoch: 6| Step: 4
Training loss: 2.7004098892211914
Validation loss: 2.1442611813545227

Epoch: 6| Step: 5
Training loss: 2.5845086574554443
Validation loss: 2.1412084897359214

Epoch: 6| Step: 6
Training loss: 1.8044624328613281
Validation loss: 2.1373302737871804

Epoch: 6| Step: 7
Training loss: 2.234074115753174
Validation loss: 2.137890875339508

Epoch: 6| Step: 8
Training loss: 2.1404168605804443
Validation loss: 2.1381654739379883

Epoch: 6| Step: 9
Training loss: 2.7043275833129883
Validation loss: 2.137181202570597

Epoch: 6| Step: 10
Training loss: 1.722459316253662
Validation loss: 2.1389437119166055

Epoch: 6| Step: 11
Training loss: 2.5551695823669434
Validation loss: 2.133912742137909

Epoch: 6| Step: 12
Training loss: 2.129128932952881
Validation loss: 2.13591734568278

Epoch: 6| Step: 13
Training loss: 2.4844746589660645
Validation loss: 2.1336050828297934

Epoch: 59| Step: 0
Training loss: 2.4657602310180664
Validation loss: 2.1358591119448342

Epoch: 6| Step: 1
Training loss: 2.2577600479125977
Validation loss: 2.1320109764734902

Epoch: 6| Step: 2
Training loss: 2.7223215103149414
Validation loss: 2.130062719186147

Epoch: 6| Step: 3
Training loss: 2.36946702003479
Validation loss: 2.133152405420939

Epoch: 6| Step: 4
Training loss: 2.024878978729248
Validation loss: 2.128105958302816

Epoch: 6| Step: 5
Training loss: 2.443182945251465
Validation loss: 2.1241509715716043

Epoch: 6| Step: 6
Training loss: 2.0159740447998047
Validation loss: 2.1276342471440635

Epoch: 6| Step: 7
Training loss: 2.682553291320801
Validation loss: 2.1269980669021606

Epoch: 6| Step: 8
Training loss: 2.3927083015441895
Validation loss: 2.1270010670026145

Epoch: 6| Step: 9
Training loss: 2.6109554767608643
Validation loss: 2.1242803931236267

Epoch: 6| Step: 10
Training loss: 1.6899726390838623
Validation loss: 2.127196033795675

Epoch: 6| Step: 11
Training loss: 1.5018384456634521
Validation loss: 2.1200250585873923

Epoch: 6| Step: 12
Training loss: 2.5185704231262207
Validation loss: 2.121001720428467

Epoch: 6| Step: 13
Training loss: 2.313716173171997
Validation loss: 2.115675667921702

Epoch: 60| Step: 0
Training loss: 2.4311394691467285
Validation loss: 2.1185349225997925

Epoch: 6| Step: 1
Training loss: 2.1382598876953125
Validation loss: 2.1163222988446555

Epoch: 6| Step: 2
Training loss: 1.8756749629974365
Validation loss: 2.113995611667633

Epoch: 6| Step: 3
Training loss: 1.6887168884277344
Validation loss: 2.106018881003062

Epoch: 6| Step: 4
Training loss: 2.6179428100585938
Validation loss: 2.117220401763916

Epoch: 6| Step: 5
Training loss: 2.904820203781128
Validation loss: 2.107817510763804

Epoch: 6| Step: 6
Training loss: 2.498490810394287
Validation loss: 2.106385429700216

Epoch: 6| Step: 7
Training loss: 2.5018203258514404
Validation loss: 2.0998833378156028

Epoch: 6| Step: 8
Training loss: 2.4699032306671143
Validation loss: 2.0979956785837808

Epoch: 6| Step: 9
Training loss: 2.2308919429779053
Validation loss: 2.1060334841410318

Epoch: 6| Step: 10
Training loss: 2.2281441688537598
Validation loss: 2.110092123349508

Epoch: 6| Step: 11
Training loss: 1.99078369140625
Validation loss: 2.115571061770121

Epoch: 6| Step: 12
Training loss: 2.059129238128662
Validation loss: 2.113889733950297

Epoch: 6| Step: 13
Training loss: 2.328587532043457
Validation loss: 2.115041732788086

Epoch: 61| Step: 0
Training loss: 2.551737070083618
Validation loss: 2.114225149154663

Epoch: 6| Step: 1
Training loss: 2.1118993759155273
Validation loss: 2.1144675811131797

Epoch: 6| Step: 2
Training loss: 2.2033567428588867
Validation loss: 2.1164037386576333

Epoch: 6| Step: 3
Training loss: 2.2971272468566895
Validation loss: 2.12104594707489

Epoch: 6| Step: 4
Training loss: 2.385369300842285
Validation loss: 2.116020441055298

Epoch: 6| Step: 5
Training loss: 2.6244659423828125
Validation loss: 2.1155149340629578

Epoch: 6| Step: 6
Training loss: 2.3886473178863525
Validation loss: 2.1137413382530212

Epoch: 6| Step: 7
Training loss: 1.9267297983169556
Validation loss: 2.116288681825002

Epoch: 6| Step: 8
Training loss: 2.3195600509643555
Validation loss: 2.1130056381225586

Epoch: 6| Step: 9
Training loss: 2.355919361114502
Validation loss: 2.1048675775527954

Epoch: 6| Step: 10
Training loss: 2.5651025772094727
Validation loss: 2.1016069253285727

Epoch: 6| Step: 11
Training loss: 2.9142513275146484
Validation loss: 2.1068983475367227

Epoch: 6| Step: 12
Training loss: 1.2009503841400146
Validation loss: 2.099485198656718

Epoch: 6| Step: 13
Training loss: 2.0262115001678467
Validation loss: 2.0977341135342917

Epoch: 62| Step: 0
Training loss: 2.3350090980529785
Validation loss: 2.0890291929244995

Epoch: 6| Step: 1
Training loss: 2.225346088409424
Validation loss: 2.093339999516805

Epoch: 6| Step: 2
Training loss: 3.3617444038391113
Validation loss: 2.10585880279541

Epoch: 6| Step: 3
Training loss: 1.9568476676940918
Validation loss: 2.091744860013326

Epoch: 6| Step: 4
Training loss: 2.1096034049987793
Validation loss: 2.08653191725413

Epoch: 6| Step: 5
Training loss: 2.578129768371582
Validation loss: 2.091337045033773

Epoch: 6| Step: 6
Training loss: 2.2291951179504395
Validation loss: 2.0960179964701333

Epoch: 6| Step: 7
Training loss: 1.9215258359909058
Validation loss: 2.1018837690353394

Epoch: 6| Step: 8
Training loss: 2.5852537155151367
Validation loss: 2.1058473785718284

Epoch: 6| Step: 9
Training loss: 1.950936198234558
Validation loss: 2.1012505094210305

Epoch: 6| Step: 10
Training loss: 2.552029609680176
Validation loss: 2.0921766559282937

Epoch: 6| Step: 11
Training loss: 1.8063825368881226
Validation loss: 2.0885215600331626

Epoch: 6| Step: 12
Training loss: 1.8352138996124268
Validation loss: 2.0877376397450766

Epoch: 6| Step: 13
Training loss: 2.283780574798584
Validation loss: 2.088626722494761

Epoch: 63| Step: 0
Training loss: 2.136958599090576
Validation loss: 2.0798494021097818

Epoch: 6| Step: 1
Training loss: 2.716825008392334
Validation loss: 2.0736774802207947

Epoch: 6| Step: 2
Training loss: 1.8848888874053955
Validation loss: 2.0759150981903076

Epoch: 6| Step: 3
Training loss: 1.9175035953521729
Validation loss: 2.0752004782358804

Epoch: 6| Step: 4
Training loss: 2.733304500579834
Validation loss: 2.082771360874176

Epoch: 6| Step: 5
Training loss: 2.384831666946411
Validation loss: 2.084284524122874

Epoch: 6| Step: 6
Training loss: 1.7676572799682617
Validation loss: 2.086142659187317

Epoch: 6| Step: 7
Training loss: 1.9853935241699219
Validation loss: 2.0848358869552612

Epoch: 6| Step: 8
Training loss: 1.6621631383895874
Validation loss: 2.0887608329455056

Epoch: 6| Step: 9
Training loss: 2.2071692943573
Validation loss: 2.090061883131663

Epoch: 6| Step: 10
Training loss: 2.138472557067871
Validation loss: 2.085884968439738

Epoch: 6| Step: 11
Training loss: 2.5919885635375977
Validation loss: 2.0839667320251465

Epoch: 6| Step: 12
Training loss: 3.0051510334014893
Validation loss: 2.0803964336713157

Epoch: 6| Step: 13
Training loss: 2.4545738697052
Validation loss: 2.0801825722058616

Epoch: 64| Step: 0
Training loss: 2.601369857788086
Validation loss: 2.077456216017405

Epoch: 6| Step: 1
Training loss: 2.334132432937622
Validation loss: 2.0661293268203735

Epoch: 6| Step: 2
Training loss: 2.4960248470306396
Validation loss: 2.064953784147898

Epoch: 6| Step: 3
Training loss: 1.7243702411651611
Validation loss: 2.0580572883288064

Epoch: 6| Step: 4
Training loss: 2.0536398887634277
Validation loss: 2.0557179848353067

Epoch: 6| Step: 5
Training loss: 2.4160351753234863
Validation loss: 2.056703289349874

Epoch: 6| Step: 6
Training loss: 2.3087010383605957
Validation loss: 2.055596927801768

Epoch: 6| Step: 7
Training loss: 2.08693790435791
Validation loss: 2.05921079715093

Epoch: 6| Step: 8
Training loss: 2.2503867149353027
Validation loss: 2.052000641822815

Epoch: 6| Step: 9
Training loss: 2.4459915161132812
Validation loss: 2.052716056505839

Epoch: 6| Step: 10
Training loss: 2.061919689178467
Validation loss: 2.0535812775293985

Epoch: 6| Step: 11
Training loss: 2.198368549346924
Validation loss: 2.04830410083135

Epoch: 6| Step: 12
Training loss: 2.586979389190674
Validation loss: 2.0473111867904663

Epoch: 6| Step: 13
Training loss: 1.937936782836914
Validation loss: 2.044497847557068

Epoch: 65| Step: 0
Training loss: 2.284731149673462
Validation loss: 2.051307201385498

Epoch: 6| Step: 1
Training loss: 2.7659707069396973
Validation loss: 2.052184045314789

Epoch: 6| Step: 2
Training loss: 2.039893627166748
Validation loss: 2.0575347542762756

Epoch: 6| Step: 3
Training loss: 1.6258201599121094
Validation loss: 2.0509899854660034

Epoch: 6| Step: 4
Training loss: 2.825528144836426
Validation loss: 2.0488234758377075

Epoch: 6| Step: 5
Training loss: 1.910273551940918
Validation loss: 2.048313319683075

Epoch: 6| Step: 6
Training loss: 1.7215425968170166
Validation loss: 2.0408575534820557

Epoch: 6| Step: 7
Training loss: 2.4502501487731934
Validation loss: 2.0385371247927346

Epoch: 6| Step: 8
Training loss: 1.9184194803237915
Validation loss: 2.0445120334625244

Epoch: 6| Step: 9
Training loss: 2.0518527030944824
Validation loss: 2.0483169754346213

Epoch: 6| Step: 10
Training loss: 1.8930299282073975
Validation loss: 2.0535941918691

Epoch: 6| Step: 11
Training loss: 2.867438554763794
Validation loss: 2.060998777548472

Epoch: 6| Step: 12
Training loss: 2.360260009765625
Validation loss: 2.054316997528076

Epoch: 6| Step: 13
Training loss: 2.659764289855957
Validation loss: 2.0453357696533203

Epoch: 66| Step: 0
Training loss: 2.5615599155426025
Validation loss: 2.0414225260416665

Epoch: 6| Step: 1
Training loss: 2.0325746536254883
Validation loss: 2.0403515100479126

Epoch: 6| Step: 2
Training loss: 2.1675915718078613
Validation loss: 2.0371676087379456

Epoch: 6| Step: 3
Training loss: 1.9179607629776
Validation loss: 2.042230725288391

Epoch: 6| Step: 4
Training loss: 1.6906869411468506
Validation loss: 2.041080435117086

Epoch: 6| Step: 5
Training loss: 1.8659405708312988
Validation loss: 2.0402141412099204

Epoch: 6| Step: 6
Training loss: 2.2984299659729004
Validation loss: 2.042259633541107

Epoch: 6| Step: 7
Training loss: 2.22829008102417
Validation loss: 2.041400929292043

Epoch: 6| Step: 8
Training loss: 2.065227508544922
Validation loss: 2.0490837891896567

Epoch: 6| Step: 9
Training loss: 2.193695068359375
Validation loss: 2.046424627304077

Epoch: 6| Step: 10
Training loss: 2.4489853382110596
Validation loss: 2.0467830896377563

Epoch: 6| Step: 11
Training loss: 2.637317657470703
Validation loss: 2.0440257787704468

Epoch: 6| Step: 12
Training loss: 2.5254650115966797
Validation loss: 2.042414685090383

Epoch: 6| Step: 13
Training loss: 2.4900693893432617
Validation loss: 2.0409802397092185

Epoch: 67| Step: 0
Training loss: 2.597188711166382
Validation loss: 2.0380226969718933

Epoch: 6| Step: 1
Training loss: 2.1273159980773926
Validation loss: 2.045635938644409

Epoch: 6| Step: 2
Training loss: 2.394873857498169
Validation loss: 2.04036412636439

Epoch: 6| Step: 3
Training loss: 2.016629695892334
Validation loss: 2.037891685962677

Epoch: 6| Step: 4
Training loss: 2.381885528564453
Validation loss: 2.0340352058410645

Epoch: 6| Step: 5
Training loss: 2.35732102394104
Validation loss: 2.035528858502706

Epoch: 6| Step: 6
Training loss: 2.3611326217651367
Validation loss: 2.0313108762105307

Epoch: 6| Step: 7
Training loss: 2.2193655967712402
Validation loss: 2.030472457408905

Epoch: 6| Step: 8
Training loss: 1.7311245203018188
Validation loss: 2.029980560143789

Epoch: 6| Step: 9
Training loss: 2.406254291534424
Validation loss: 2.0310383836428323

Epoch: 6| Step: 10
Training loss: 2.3137621879577637
Validation loss: 2.0296789606412253

Epoch: 6| Step: 11
Training loss: 2.1820812225341797
Validation loss: 2.0245004097620645

Epoch: 6| Step: 12
Training loss: 2.0660338401794434
Validation loss: 2.029174248377482

Epoch: 6| Step: 13
Training loss: 1.7993602752685547
Validation loss: 2.033323029677073

Epoch: 68| Step: 0
Training loss: 2.7173781394958496
Validation loss: 2.033487379550934

Epoch: 6| Step: 1
Training loss: 3.048116683959961
Validation loss: 2.0289507309595742

Epoch: 6| Step: 2
Training loss: 2.0383949279785156
Validation loss: 2.0282749136288962

Epoch: 6| Step: 3
Training loss: 2.346143960952759
Validation loss: 2.0380298097928367

Epoch: 6| Step: 4
Training loss: 2.230496406555176
Validation loss: 2.0283435384432473

Epoch: 6| Step: 5
Training loss: 1.7274487018585205
Validation loss: 2.0300553242365518

Epoch: 6| Step: 6
Training loss: 1.3930244445800781
Validation loss: 2.0251088539759317

Epoch: 6| Step: 7
Training loss: 2.395407199859619
Validation loss: 2.0211050709088645

Epoch: 6| Step: 8
Training loss: 2.0139214992523193
Validation loss: 2.0293474793434143

Epoch: 6| Step: 9
Training loss: 2.148700714111328
Validation loss: 2.026147484779358

Epoch: 6| Step: 10
Training loss: 2.1922507286071777
Validation loss: 2.0259681940078735

Epoch: 6| Step: 11
Training loss: 2.4708642959594727
Validation loss: 2.027841647466024

Epoch: 6| Step: 12
Training loss: 2.1716976165771484
Validation loss: 2.0306596954663596

Epoch: 6| Step: 13
Training loss: 1.9691438674926758
Validation loss: 2.0245102047920227

Epoch: 69| Step: 0
Training loss: 3.317434310913086
Validation loss: 2.0249170660972595

Epoch: 6| Step: 1
Training loss: 2.6694843769073486
Validation loss: 2.0297715862592063

Epoch: 6| Step: 2
Training loss: 2.1187915802001953
Validation loss: 2.028654376665751

Epoch: 6| Step: 3
Training loss: 2.2775144577026367
Validation loss: 2.027269879976908

Epoch: 6| Step: 4
Training loss: 2.5050389766693115
Validation loss: 2.0291327238082886

Epoch: 6| Step: 5
Training loss: 1.8266699314117432
Validation loss: 2.0280419985453286

Epoch: 6| Step: 6
Training loss: 1.7170110940933228
Validation loss: 2.0241246223449707

Epoch: 6| Step: 7
Training loss: 1.5123577117919922
Validation loss: 2.0211418668429055

Epoch: 6| Step: 8
Training loss: 2.0807480812072754
Validation loss: 2.025960683822632

Epoch: 6| Step: 9
Training loss: 2.295851945877075
Validation loss: 2.0395812590916953

Epoch: 6| Step: 10
Training loss: 1.8548777103424072
Validation loss: 2.0508251190185547

Epoch: 6| Step: 11
Training loss: 2.2765746116638184
Validation loss: 2.063645044962565

Epoch: 6| Step: 12
Training loss: 2.698845386505127
Validation loss: 2.0592610041300454

Epoch: 6| Step: 13
Training loss: 1.5719448328018188
Validation loss: 2.0698038140932717

Epoch: 70| Step: 0
Training loss: 2.274212121963501
Validation loss: 2.0641217629114785

Epoch: 6| Step: 1
Training loss: 1.7606964111328125
Validation loss: 2.0775859554608664

Epoch: 6| Step: 2
Training loss: 1.9216312170028687
Validation loss: 2.0732524394989014

Epoch: 6| Step: 3
Training loss: 1.9202951192855835
Validation loss: 2.0686778823534646

Epoch: 6| Step: 4
Training loss: 2.67094087600708
Validation loss: 2.052139103412628

Epoch: 6| Step: 5
Training loss: 2.087798833847046
Validation loss: 2.0352214574813843

Epoch: 6| Step: 6
Training loss: 2.7342190742492676
Validation loss: 2.02687535683314

Epoch: 6| Step: 7
Training loss: 1.7238752841949463
Validation loss: 2.0219240188598633

Epoch: 6| Step: 8
Training loss: 2.8536839485168457
Validation loss: 2.0243470867474875

Epoch: 6| Step: 9
Training loss: 2.3030643463134766
Validation loss: 2.028089781602224

Epoch: 6| Step: 10
Training loss: 2.0976450443267822
Validation loss: 2.0346901416778564

Epoch: 6| Step: 11
Training loss: 2.4324567317962646
Validation loss: 2.0360074639320374

Epoch: 6| Step: 12
Training loss: 2.477485179901123
Validation loss: 2.037639300028483

Epoch: 6| Step: 13
Training loss: 1.8962345123291016
Validation loss: 2.0392362674077353

Epoch: 71| Step: 0
Training loss: 2.115029811859131
Validation loss: 2.0396565198898315

Epoch: 6| Step: 1
Training loss: 2.3028030395507812
Validation loss: 2.033526341120402

Epoch: 6| Step: 2
Training loss: 2.1180596351623535
Validation loss: 2.0341925621032715

Epoch: 6| Step: 3
Training loss: 2.3183693885803223
Validation loss: 2.0301453669865928

Epoch: 6| Step: 4
Training loss: 2.4432849884033203
Validation loss: 2.02221816778183

Epoch: 6| Step: 5
Training loss: 2.0037283897399902
Validation loss: 2.0250134468078613

Epoch: 6| Step: 6
Training loss: 2.0172579288482666
Validation loss: 2.0220864613850913

Epoch: 6| Step: 7
Training loss: 2.5094947814941406
Validation loss: 2.0167481501897178

Epoch: 6| Step: 8
Training loss: 2.7004122734069824
Validation loss: 2.021203637123108

Epoch: 6| Step: 9
Training loss: 2.5534873008728027
Validation loss: 2.020467162132263

Epoch: 6| Step: 10
Training loss: 1.96035897731781
Validation loss: 2.0201629201571145

Epoch: 6| Step: 11
Training loss: 1.9960956573486328
Validation loss: 2.0159824093182883

Epoch: 6| Step: 12
Training loss: 1.8778231143951416
Validation loss: 2.0214989384015403

Epoch: 6| Step: 13
Training loss: 1.9364265203475952
Validation loss: 2.0207148591677346

Epoch: 72| Step: 0
Training loss: 2.510192394256592
Validation loss: 2.024796962738037

Epoch: 6| Step: 1
Training loss: 2.324735164642334
Validation loss: 2.0225860675175986

Epoch: 6| Step: 2
Training loss: 1.5100977420806885
Validation loss: 2.02844774723053

Epoch: 6| Step: 3
Training loss: 2.028642416000366
Validation loss: 2.032562017440796

Epoch: 6| Step: 4
Training loss: 2.3120181560516357
Validation loss: 2.029851257801056

Epoch: 6| Step: 5
Training loss: 2.280636787414551
Validation loss: 2.0217443108558655

Epoch: 6| Step: 6
Training loss: 2.0724778175354004
Validation loss: 2.0290276209513345

Epoch: 6| Step: 7
Training loss: 2.816270351409912
Validation loss: 2.034349958101908

Epoch: 6| Step: 8
Training loss: 2.109358072280884
Validation loss: 2.0312671860059104

Epoch: 6| Step: 9
Training loss: 1.7054027318954468
Validation loss: 2.029530644416809

Epoch: 6| Step: 10
Training loss: 2.036111354827881
Validation loss: 2.037932594617208

Epoch: 6| Step: 11
Training loss: 1.612681269645691
Validation loss: 2.028453509012858

Epoch: 6| Step: 12
Training loss: 2.6644654273986816
Validation loss: 2.0332376758257547

Epoch: 6| Step: 13
Training loss: 2.683718204498291
Validation loss: 2.031039814154307

Epoch: 73| Step: 0
Training loss: 2.3313682079315186
Validation loss: 2.018419007460276

Epoch: 6| Step: 1
Training loss: 2.2991063594818115
Validation loss: 2.0165400306383767

Epoch: 6| Step: 2
Training loss: 2.6076488494873047
Validation loss: 2.015495538711548

Epoch: 6| Step: 3
Training loss: 2.2373969554901123
Validation loss: 2.011838912963867

Epoch: 6| Step: 4
Training loss: 2.010181427001953
Validation loss: 2.011728843053182

Epoch: 6| Step: 5
Training loss: 1.9244740009307861
Validation loss: 2.01316899061203

Epoch: 6| Step: 6
Training loss: 1.9592405557632446
Validation loss: 2.0110931992530823

Epoch: 6| Step: 7
Training loss: 2.3769688606262207
Validation loss: 2.0104453563690186

Epoch: 6| Step: 8
Training loss: 2.2136454582214355
Validation loss: 2.0120091438293457

Epoch: 6| Step: 9
Training loss: 2.1385092735290527
Validation loss: 2.011271039644877

Epoch: 6| Step: 10
Training loss: 2.4445793628692627
Validation loss: 2.009387493133545

Epoch: 6| Step: 11
Training loss: 2.020092010498047
Validation loss: 2.0093192060788474

Epoch: 6| Step: 12
Training loss: 1.7304866313934326
Validation loss: 2.0093602339426675

Epoch: 6| Step: 13
Training loss: 2.2725448608398438
Validation loss: 2.008547564347585

Epoch: 74| Step: 0
Training loss: 2.7388672828674316
Validation loss: 2.0121469298998513

Epoch: 6| Step: 1
Training loss: 1.9721367359161377
Validation loss: 2.012247701485952

Epoch: 6| Step: 2
Training loss: 2.3737080097198486
Validation loss: 2.0219111243883767

Epoch: 6| Step: 3
Training loss: 2.363593816757202
Validation loss: 2.016271928946177

Epoch: 6| Step: 4
Training loss: 1.7102808952331543
Validation loss: 2.025506873925527

Epoch: 6| Step: 5
Training loss: 1.9287222623825073
Validation loss: 2.029797156651815

Epoch: 6| Step: 6
Training loss: 1.9422954320907593
Validation loss: 2.0248189568519592

Epoch: 6| Step: 7
Training loss: 2.11722469329834
Validation loss: 2.0188130140304565

Epoch: 6| Step: 8
Training loss: 1.863744854927063
Validation loss: 2.020098567008972

Epoch: 6| Step: 9
Training loss: 2.1564314365386963
Validation loss: 2.021255354086558

Epoch: 6| Step: 10
Training loss: 2.256566286087036
Validation loss: 2.018990178902944

Epoch: 6| Step: 11
Training loss: 2.4382853507995605
Validation loss: 2.019108454386393

Epoch: 6| Step: 12
Training loss: 1.7063302993774414
Validation loss: 2.0119484265645347

Epoch: 6| Step: 13
Training loss: 2.795307159423828
Validation loss: 2.0149888594945273

Epoch: 75| Step: 0
Training loss: 2.4363908767700195
Validation loss: 2.0128235816955566

Epoch: 6| Step: 1
Training loss: 2.5764212608337402
Validation loss: 2.0144840677579245

Epoch: 6| Step: 2
Training loss: 2.118053674697876
Validation loss: 2.0165915886561074

Epoch: 6| Step: 3
Training loss: 1.9710034132003784
Validation loss: 2.011774996916453

Epoch: 6| Step: 4
Training loss: 2.4213712215423584
Validation loss: 2.024080137411753

Epoch: 6| Step: 5
Training loss: 2.3684113025665283
Validation loss: 2.0264957547187805

Epoch: 6| Step: 6
Training loss: 1.5395641326904297
Validation loss: 2.028047502040863

Epoch: 6| Step: 7
Training loss: 2.67922043800354
Validation loss: 2.0297449032465615

Epoch: 6| Step: 8
Training loss: 1.9560675621032715
Validation loss: 2.0384665727615356

Epoch: 6| Step: 9
Training loss: 1.8734922409057617
Validation loss: 2.0373430450757346

Epoch: 6| Step: 10
Training loss: 2.300787925720215
Validation loss: 2.036138971646627

Epoch: 6| Step: 11
Training loss: 1.8375301361083984
Validation loss: 2.019862631956736

Epoch: 6| Step: 12
Training loss: 1.8546351194381714
Validation loss: 2.0134770274162292

Epoch: 6| Step: 13
Training loss: 2.478642702102661
Validation loss: 2.0072869658470154

Epoch: 76| Step: 0
Training loss: 2.7554948329925537
Validation loss: 2.0140376885732016

Epoch: 6| Step: 1
Training loss: 2.0741984844207764
Validation loss: 2.00778458515803

Epoch: 6| Step: 2
Training loss: 2.4606387615203857
Validation loss: 2.0063589811325073

Epoch: 6| Step: 3
Training loss: 1.9770539999008179
Validation loss: 2.006597558657328

Epoch: 6| Step: 4
Training loss: 1.7904982566833496
Validation loss: 2.000757376352946

Epoch: 6| Step: 5
Training loss: 1.7618086338043213
Validation loss: 2.0163997610410056

Epoch: 6| Step: 6
Training loss: 2.753631353378296
Validation loss: 2.009954492251078

Epoch: 6| Step: 7
Training loss: 1.531606912612915
Validation loss: 2.0138166546821594

Epoch: 6| Step: 8
Training loss: 2.24509334564209
Validation loss: 2.0153134862581887

Epoch: 6| Step: 9
Training loss: 1.9856644868850708
Validation loss: 2.013115664323171

Epoch: 6| Step: 10
Training loss: 2.176088333129883
Validation loss: 2.0146278937657676

Epoch: 6| Step: 11
Training loss: 2.299619436264038
Validation loss: 2.0187264482180276

Epoch: 6| Step: 12
Training loss: 2.308764696121216
Validation loss: 2.0206134716669717

Epoch: 6| Step: 13
Training loss: 2.365460157394409
Validation loss: 2.0234846671422324

Epoch: 77| Step: 0
Training loss: 2.336315393447876
Validation loss: 2.023580094178518

Epoch: 6| Step: 1
Training loss: 2.2984824180603027
Validation loss: 2.0193066596984863

Epoch: 6| Step: 2
Training loss: 2.3570189476013184
Validation loss: 2.0234693686167398

Epoch: 6| Step: 3
Training loss: 2.352813243865967
Validation loss: 2.0203574299812317

Epoch: 6| Step: 4
Training loss: 1.9217488765716553
Validation loss: 2.0241177678108215

Epoch: 6| Step: 5
Training loss: 2.06575345993042
Validation loss: 2.036991556485494

Epoch: 6| Step: 6
Training loss: 2.2619986534118652
Validation loss: 2.0431782603263855

Epoch: 6| Step: 7
Training loss: 2.581512689590454
Validation loss: 2.0478856762250266

Epoch: 6| Step: 8
Training loss: 2.076286792755127
Validation loss: 2.043179710706075

Epoch: 6| Step: 9
Training loss: 2.202803611755371
Validation loss: 2.039171894391378

Epoch: 6| Step: 10
Training loss: 2.517674446105957
Validation loss: 2.0297560890515647

Epoch: 6| Step: 11
Training loss: 1.976935863494873
Validation loss: 2.0266504883766174

Epoch: 6| Step: 12
Training loss: 1.831796407699585
Validation loss: 2.0221843123435974

Epoch: 6| Step: 13
Training loss: 1.80672287940979
Validation loss: 2.020568072795868

Epoch: 78| Step: 0
Training loss: 2.4079113006591797
Validation loss: 2.0148439009984336

Epoch: 6| Step: 1
Training loss: 1.6533445119857788
Validation loss: 2.0162577430407205

Epoch: 6| Step: 2
Training loss: 2.193972110748291
Validation loss: 2.0150888363520303

Epoch: 6| Step: 3
Training loss: 1.7535208463668823
Validation loss: 2.0183915495872498

Epoch: 6| Step: 4
Training loss: 1.7789415121078491
Validation loss: 2.0185810923576355

Epoch: 6| Step: 5
Training loss: 2.186342477798462
Validation loss: 2.0172159473101297

Epoch: 6| Step: 6
Training loss: 2.619691848754883
Validation loss: 2.0146717031796775

Epoch: 6| Step: 7
Training loss: 1.8664859533309937
Validation loss: 2.0132449666659036

Epoch: 6| Step: 8
Training loss: 2.1250243186950684
Validation loss: 2.016214291254679

Epoch: 6| Step: 9
Training loss: 2.136430263519287
Validation loss: 2.014163315296173

Epoch: 6| Step: 10
Training loss: 2.002739906311035
Validation loss: 2.010128438472748

Epoch: 6| Step: 11
Training loss: 3.0657286643981934
Validation loss: 2.007181445757548

Epoch: 6| Step: 12
Training loss: 2.2488057613372803
Validation loss: 2.0104724168777466

Epoch: 6| Step: 13
Training loss: 2.1957290172576904
Validation loss: 2.0156080524126687

Epoch: 79| Step: 0
Training loss: 1.8435043096542358
Validation loss: 2.0096774101257324

Epoch: 6| Step: 1
Training loss: 2.455857515335083
Validation loss: 2.01999165614446

Epoch: 6| Step: 2
Training loss: 2.177572250366211
Validation loss: 2.02159720659256

Epoch: 6| Step: 3
Training loss: 2.158938407897949
Validation loss: 2.0270040035247803

Epoch: 6| Step: 4
Training loss: 2.232042074203491
Validation loss: 2.031113028526306

Epoch: 6| Step: 5
Training loss: 2.4504966735839844
Validation loss: 2.046077291170756

Epoch: 6| Step: 6
Training loss: 2.472637891769409
Validation loss: 2.0401179591814675

Epoch: 6| Step: 7
Training loss: 1.8774471282958984
Validation loss: 2.029525637626648

Epoch: 6| Step: 8
Training loss: 2.251148223876953
Validation loss: 2.019792397816976

Epoch: 6| Step: 9
Training loss: 2.2401866912841797
Validation loss: 2.0186822414398193

Epoch: 6| Step: 10
Training loss: 2.0939712524414062
Validation loss: 2.009527623653412

Epoch: 6| Step: 11
Training loss: 1.8237833976745605
Validation loss: 2.005367934703827

Epoch: 6| Step: 12
Training loss: 2.2266812324523926
Validation loss: 2.0098240971565247

Epoch: 6| Step: 13
Training loss: 2.054051399230957
Validation loss: 2.010205646355947

Epoch: 80| Step: 0
Training loss: 2.2864909172058105
Validation loss: 2.0080353816350303

Epoch: 6| Step: 1
Training loss: 2.3976550102233887
Validation loss: 2.0114530324935913

Epoch: 6| Step: 2
Training loss: 1.6552276611328125
Validation loss: 2.0079591274261475

Epoch: 6| Step: 3
Training loss: 2.492349147796631
Validation loss: 2.005895713965098

Epoch: 6| Step: 4
Training loss: 1.4401533603668213
Validation loss: 2.006050189336141

Epoch: 6| Step: 5
Training loss: 2.241580009460449
Validation loss: 2.0204303860664368

Epoch: 6| Step: 6
Training loss: 1.3696296215057373
Validation loss: 2.015318592389425

Epoch: 6| Step: 7
Training loss: 2.46171236038208
Validation loss: 2.018330236275991

Epoch: 6| Step: 8
Training loss: 1.994632601737976
Validation loss: 2.034645438194275

Epoch: 6| Step: 9
Training loss: 1.9238263368606567
Validation loss: 2.032569487889608

Epoch: 6| Step: 10
Training loss: 2.506178379058838
Validation loss: 2.034261961778005

Epoch: 6| Step: 11
Training loss: 2.4170069694519043
Validation loss: 2.046629707018534

Epoch: 6| Step: 12
Training loss: 2.418285369873047
Validation loss: 2.0418811241785684

Epoch: 6| Step: 13
Training loss: 2.517096757888794
Validation loss: 2.022164801756541

Epoch: 81| Step: 0
Training loss: 1.832578420639038
Validation loss: 2.028843323389689

Epoch: 6| Step: 1
Training loss: 1.614713191986084
Validation loss: 2.020675798257192

Epoch: 6| Step: 2
Training loss: 2.7716827392578125
Validation loss: 2.025697489579519

Epoch: 6| Step: 3
Training loss: 2.1628706455230713
Validation loss: 2.027107298374176

Epoch: 6| Step: 4
Training loss: 2.7827725410461426
Validation loss: 2.022279461224874

Epoch: 6| Step: 5
Training loss: 1.6721642017364502
Validation loss: 2.029745082060496

Epoch: 6| Step: 6
Training loss: 1.8603405952453613
Validation loss: 2.031555652618408

Epoch: 6| Step: 7
Training loss: 1.2198619842529297
Validation loss: 2.0313879450162253

Epoch: 6| Step: 8
Training loss: 2.7889437675476074
Validation loss: 2.0329495867093406

Epoch: 6| Step: 9
Training loss: 2.1184983253479004
Validation loss: 2.0340928634007773

Epoch: 6| Step: 10
Training loss: 2.43178129196167
Validation loss: 2.0326082507769265

Epoch: 6| Step: 11
Training loss: 2.4626035690307617
Validation loss: 2.0256462693214417

Epoch: 6| Step: 12
Training loss: 2.5836474895477295
Validation loss: 2.0282617807388306

Epoch: 6| Step: 13
Training loss: 2.3256571292877197
Validation loss: 2.0254340370496116

Epoch: 82| Step: 0
Training loss: 2.541017532348633
Validation loss: 2.027842660744985

Epoch: 6| Step: 1
Training loss: 2.906310558319092
Validation loss: 2.022009233633677

Epoch: 6| Step: 2
Training loss: 2.5555083751678467
Validation loss: 2.0181288520495095

Epoch: 6| Step: 3
Training loss: 2.0770010948181152
Validation loss: 2.013978600502014

Epoch: 6| Step: 4
Training loss: 2.3159382343292236
Validation loss: 2.0131484468777976

Epoch: 6| Step: 5
Training loss: 1.4828541278839111
Validation loss: 2.0070859591166177

Epoch: 6| Step: 6
Training loss: 1.5951828956604004
Validation loss: 2.011208196481069

Epoch: 6| Step: 7
Training loss: 2.4772286415100098
Validation loss: 2.013733704884847

Epoch: 6| Step: 8
Training loss: 2.0051841735839844
Validation loss: 2.0146902004877725

Epoch: 6| Step: 9
Training loss: 1.8684003353118896
Validation loss: 2.0245567560195923

Epoch: 6| Step: 10
Training loss: 1.7539290189743042
Validation loss: 2.0326473712921143

Epoch: 6| Step: 11
Training loss: 2.033677577972412
Validation loss: 2.0318275094032288

Epoch: 6| Step: 12
Training loss: 2.2264797687530518
Validation loss: 2.0393611192703247

Epoch: 6| Step: 13
Training loss: 2.441098690032959
Validation loss: 2.030715763568878

Epoch: 83| Step: 0
Training loss: 2.100757360458374
Validation loss: 2.0366740028063455

Epoch: 6| Step: 1
Training loss: 1.8612834215164185
Validation loss: 2.0316718022028604

Epoch: 6| Step: 2
Training loss: 2.0705933570861816
Validation loss: 2.0297897855440774

Epoch: 6| Step: 3
Training loss: 1.9577887058258057
Validation loss: 2.029279887676239

Epoch: 6| Step: 4
Training loss: 1.823488712310791
Validation loss: 2.0234259963035583

Epoch: 6| Step: 5
Training loss: 3.031765937805176
Validation loss: 2.018344461917877

Epoch: 6| Step: 6
Training loss: 2.8355612754821777
Validation loss: 2.005340298016866

Epoch: 6| Step: 7
Training loss: 1.9161059856414795
Validation loss: 2.0076785484949746

Epoch: 6| Step: 8
Training loss: 2.4454903602600098
Validation loss: 2.0070321758588157

Epoch: 6| Step: 9
Training loss: 1.9294748306274414
Validation loss: 2.0118515888849893

Epoch: 6| Step: 10
Training loss: 1.7954760789871216
Validation loss: 2.0117598374684653

Epoch: 6| Step: 11
Training loss: 2.1150612831115723
Validation loss: 2.0086374084154763

Epoch: 6| Step: 12
Training loss: 2.344752311706543
Validation loss: 2.012598673502604

Epoch: 6| Step: 13
Training loss: 2.194944381713867
Validation loss: 2.014963169892629

Epoch: 84| Step: 0
Training loss: 2.1119353771209717
Validation loss: 2.0084563891092935

Epoch: 6| Step: 1
Training loss: 1.9515300989151
Validation loss: 2.002203325430552

Epoch: 6| Step: 2
Training loss: 1.7911550998687744
Validation loss: 2.009027143319448

Epoch: 6| Step: 3
Training loss: 1.9697054624557495
Validation loss: 2.0147350629170737

Epoch: 6| Step: 4
Training loss: 2.1979827880859375
Validation loss: 2.011898934841156

Epoch: 6| Step: 5
Training loss: 2.050325393676758
Validation loss: 2.0192428827285767

Epoch: 6| Step: 6
Training loss: 2.7308294773101807
Validation loss: 2.017297605673472

Epoch: 6| Step: 7
Training loss: 2.278104305267334
Validation loss: 2.020737051963806

Epoch: 6| Step: 8
Training loss: 2.225954055786133
Validation loss: 2.017661968866984

Epoch: 6| Step: 9
Training loss: 1.8217647075653076
Validation loss: 2.0228057305018106

Epoch: 6| Step: 10
Training loss: 2.156559944152832
Validation loss: 2.0213890274365744

Epoch: 6| Step: 11
Training loss: 2.226675510406494
Validation loss: 2.0241084893544516

Epoch: 6| Step: 12
Training loss: 2.280343532562256
Validation loss: 2.0253796378771463

Epoch: 6| Step: 13
Training loss: 2.4387588500976562
Validation loss: 2.020924230416616

Epoch: 85| Step: 0
Training loss: 2.0394859313964844
Validation loss: 2.011831283569336

Epoch: 6| Step: 1
Training loss: 1.7311450242996216
Validation loss: 2.0171324014663696

Epoch: 6| Step: 2
Training loss: 2.442138195037842
Validation loss: 2.0052693684895835

Epoch: 6| Step: 3
Training loss: 2.169447898864746
Validation loss: 2.000435709953308

Epoch: 6| Step: 4
Training loss: 1.9804322719573975
Validation loss: 2.014739672342936

Epoch: 6| Step: 5
Training loss: 2.0682778358459473
Validation loss: 2.0166812737782798

Epoch: 6| Step: 6
Training loss: 2.2351298332214355
Validation loss: 2.0259175300598145

Epoch: 6| Step: 7
Training loss: 1.9880146980285645
Validation loss: 2.0285329023996987

Epoch: 6| Step: 8
Training loss: 2.2853293418884277
Validation loss: 2.026010036468506

Epoch: 6| Step: 9
Training loss: 2.488534688949585
Validation loss: 2.029590884844462

Epoch: 6| Step: 10
Training loss: 2.0774550437927246
Validation loss: 2.0310819149017334

Epoch: 6| Step: 11
Training loss: 2.270989418029785
Validation loss: 2.027085800965627

Epoch: 6| Step: 12
Training loss: 2.0515382289886475
Validation loss: 2.0292896827061973

Epoch: 6| Step: 13
Training loss: 2.593538999557495
Validation loss: 2.0247352917989097

Epoch: 86| Step: 0
Training loss: 1.829077124595642
Validation loss: 2.0273760159810386

Epoch: 6| Step: 1
Training loss: 2.508026123046875
Validation loss: 2.0257315635681152

Epoch: 6| Step: 2
Training loss: 1.9774384498596191
Validation loss: 2.030630052089691

Epoch: 6| Step: 3
Training loss: 2.444430351257324
Validation loss: 2.0238353610038757

Epoch: 6| Step: 4
Training loss: 2.691943407058716
Validation loss: 2.016118288040161

Epoch: 6| Step: 5
Training loss: 2.1104202270507812
Validation loss: 2.0173463026682534

Epoch: 6| Step: 6
Training loss: 2.6051435470581055
Validation loss: 2.008711596330007

Epoch: 6| Step: 7
Training loss: 2.1971020698547363
Validation loss: 2.0182738105456033

Epoch: 6| Step: 8
Training loss: 2.065891981124878
Validation loss: 2.0180184046427407

Epoch: 6| Step: 9
Training loss: 1.9320876598358154
Validation loss: 2.0169657468795776

Epoch: 6| Step: 10
Training loss: 2.047171115875244
Validation loss: 2.018164058526357

Epoch: 6| Step: 11
Training loss: 2.3030238151550293
Validation loss: 2.0247329473495483

Epoch: 6| Step: 12
Training loss: 2.1466588973999023
Validation loss: 2.0140135288238525

Epoch: 6| Step: 13
Training loss: 1.5748234987258911
Validation loss: 2.011158267656962

Epoch: 87| Step: 0
Training loss: 2.376394510269165
Validation loss: 2.007207691669464

Epoch: 6| Step: 1
Training loss: 1.917026162147522
Validation loss: 2.008216083049774

Epoch: 6| Step: 2
Training loss: 2.098578453063965
Validation loss: 2.0059860944747925

Epoch: 6| Step: 3
Training loss: 1.8090417385101318
Validation loss: 2.010630746682485

Epoch: 6| Step: 4
Training loss: 2.9133057594299316
Validation loss: 2.004081130027771

Epoch: 6| Step: 5
Training loss: 2.2971811294555664
Validation loss: 2.0138407548268638

Epoch: 6| Step: 6
Training loss: 2.012418508529663
Validation loss: 2.0086984038352966

Epoch: 6| Step: 7
Training loss: 2.909583568572998
Validation loss: 2.011220951875051

Epoch: 6| Step: 8
Training loss: 2.28092098236084
Validation loss: 2.013298531373342

Epoch: 6| Step: 9
Training loss: 2.2377681732177734
Validation loss: 2.008923590183258

Epoch: 6| Step: 10
Training loss: 2.0840506553649902
Validation loss: 2.015274087587992

Epoch: 6| Step: 11
Training loss: 1.1719756126403809
Validation loss: 2.0189708471298218

Epoch: 6| Step: 12
Training loss: 2.245117425918579
Validation loss: 2.0112096468607583

Epoch: 6| Step: 13
Training loss: 1.6738717555999756
Validation loss: 2.008147736390432

Epoch: 88| Step: 0
Training loss: 2.308656930923462
Validation loss: 2.0127527316411338

Epoch: 6| Step: 1
Training loss: 2.2996866703033447
Validation loss: 2.0133416255315146

Epoch: 6| Step: 2
Training loss: 2.1686158180236816
Validation loss: 2.014560361703237

Epoch: 6| Step: 3
Training loss: 1.8786510229110718
Validation loss: 2.023650864760081

Epoch: 6| Step: 4
Training loss: 1.742021083831787
Validation loss: 2.020696004231771

Epoch: 6| Step: 5
Training loss: 2.1888718605041504
Validation loss: 2.0288501580556235

Epoch: 6| Step: 6
Training loss: 2.064110279083252
Validation loss: 2.0344084898630777

Epoch: 6| Step: 7
Training loss: 2.2419180870056152
Validation loss: 2.042260448137919

Epoch: 6| Step: 8
Training loss: 1.795477271080017
Validation loss: 2.0389758547147117

Epoch: 6| Step: 9
Training loss: 1.7575494050979614
Validation loss: 2.0432336131731668

Epoch: 6| Step: 10
Training loss: 2.6101057529449463
Validation loss: 2.0517252882321677

Epoch: 6| Step: 11
Training loss: 2.5382394790649414
Validation loss: 2.042155663172404

Epoch: 6| Step: 12
Training loss: 2.363157272338867
Validation loss: 2.0408323407173157

Epoch: 6| Step: 13
Training loss: 2.2746331691741943
Validation loss: 2.030714750289917

Epoch: 89| Step: 0
Training loss: 1.6365396976470947
Validation loss: 2.012429694334666

Epoch: 6| Step: 1
Training loss: 2.608458995819092
Validation loss: 2.0016777912775674

Epoch: 6| Step: 2
Training loss: 2.1542129516601562
Validation loss: 2.000410536924998

Epoch: 6| Step: 3
Training loss: 2.637298107147217
Validation loss: 2.0136438409487405

Epoch: 6| Step: 4
Training loss: 2.3215150833129883
Validation loss: 2.0225786368052163

Epoch: 6| Step: 5
Training loss: 2.267279624938965
Validation loss: 2.027598758538564

Epoch: 6| Step: 6
Training loss: 1.5577175617218018
Validation loss: 2.0310696562131247

Epoch: 6| Step: 7
Training loss: 2.3441638946533203
Validation loss: 2.027861217657725

Epoch: 6| Step: 8
Training loss: 2.1488895416259766
Validation loss: 2.0242375135421753

Epoch: 6| Step: 9
Training loss: 2.152778387069702
Validation loss: 2.0305669705073037

Epoch: 6| Step: 10
Training loss: 1.7897969484329224
Validation loss: 2.033244550228119

Epoch: 6| Step: 11
Training loss: 2.3607492446899414
Validation loss: 2.0289159019788108

Epoch: 6| Step: 12
Training loss: 2.30391788482666
Validation loss: 2.0313888589541116

Epoch: 6| Step: 13
Training loss: 2.394867420196533
Validation loss: 2.0267879168192544

Epoch: 90| Step: 0
Training loss: 2.3453924655914307
Validation loss: 2.0307886600494385

Epoch: 6| Step: 1
Training loss: 1.9373019933700562
Validation loss: 2.0246665875116983

Epoch: 6| Step: 2
Training loss: 2.2585182189941406
Validation loss: 2.025628129641215

Epoch: 6| Step: 3
Training loss: 2.2991559505462646
Validation loss: 2.022905866305033

Epoch: 6| Step: 4
Training loss: 2.169250726699829
Validation loss: 2.016963839530945

Epoch: 6| Step: 5
Training loss: 2.0565783977508545
Validation loss: 2.0152662992477417

Epoch: 6| Step: 6
Training loss: 2.0120253562927246
Validation loss: 2.0131463209788003

Epoch: 6| Step: 7
Training loss: 1.9925634860992432
Validation loss: 2.0004852215449014

Epoch: 6| Step: 8
Training loss: 1.6222120523452759
Validation loss: 2.0074387590090432

Epoch: 6| Step: 9
Training loss: 2.1037778854370117
Validation loss: 2.002961575984955

Epoch: 6| Step: 10
Training loss: 2.555306911468506
Validation loss: 2.0050063133239746

Epoch: 6| Step: 11
Training loss: 1.7273693084716797
Validation loss: 2.0146286884943643

Epoch: 6| Step: 12
Training loss: 2.1236531734466553
Validation loss: 2.019676903883616

Epoch: 6| Step: 13
Training loss: 3.0554733276367188
Validation loss: 2.0362733006477356

Epoch: 91| Step: 0
Training loss: 1.7224106788635254
Validation loss: 2.0384490688641868

Epoch: 6| Step: 1
Training loss: 2.667255163192749
Validation loss: 2.033622999986013

Epoch: 6| Step: 2
Training loss: 2.017038345336914
Validation loss: 2.040288766225179

Epoch: 6| Step: 3
Training loss: 2.163601875305176
Validation loss: 2.0345605413118997

Epoch: 6| Step: 4
Training loss: 2.777700424194336
Validation loss: 2.0252416928609214

Epoch: 6| Step: 5
Training loss: 2.257598876953125
Validation loss: 2.031890312830607

Epoch: 6| Step: 6
Training loss: 1.9021990299224854
Validation loss: 2.0271833340326944

Epoch: 6| Step: 7
Training loss: 2.47578763961792
Validation loss: 2.016745626926422

Epoch: 6| Step: 8
Training loss: 2.6365609169006348
Validation loss: 2.0145135521888733

Epoch: 6| Step: 9
Training loss: 1.7246646881103516
Validation loss: 2.0073888103167215

Epoch: 6| Step: 10
Training loss: 1.7082427740097046
Validation loss: 2.0092491706212363

Epoch: 6| Step: 11
Training loss: 1.926666259765625
Validation loss: 2.0070143342018127

Epoch: 6| Step: 12
Training loss: 2.1237289905548096
Validation loss: 2.0157353480656943

Epoch: 6| Step: 13
Training loss: 1.8637073040008545
Validation loss: 2.0110384225845337

Epoch: 92| Step: 0
Training loss: 2.071333169937134
Validation loss: 2.0115483005841575

Epoch: 6| Step: 1
Training loss: 1.6648823022842407
Validation loss: 2.0166693727175393

Epoch: 6| Step: 2
Training loss: 3.175503730773926
Validation loss: 2.016031642754873

Epoch: 6| Step: 3
Training loss: 2.290151596069336
Validation loss: 2.01533571879069

Epoch: 6| Step: 4
Training loss: 2.2482571601867676
Validation loss: 2.020082434018453

Epoch: 6| Step: 5
Training loss: 1.6701103448867798
Validation loss: 2.023916761080424

Epoch: 6| Step: 6
Training loss: 2.527926206588745
Validation loss: 2.032813807328542

Epoch: 6| Step: 7
Training loss: 2.0760579109191895
Validation loss: 2.02839732170105

Epoch: 6| Step: 8
Training loss: 1.5123214721679688
Validation loss: 2.034434735774994

Epoch: 6| Step: 9
Training loss: 1.6592047214508057
Validation loss: 2.0316344499588013

Epoch: 6| Step: 10
Training loss: 2.350902795791626
Validation loss: 2.049249768257141

Epoch: 6| Step: 11
Training loss: 2.3243186473846436
Validation loss: 2.041527430216471

Epoch: 6| Step: 12
Training loss: 2.373230218887329
Validation loss: 2.042004962762197

Epoch: 6| Step: 13
Training loss: 1.8739140033721924
Validation loss: 2.036632756392161

Epoch: 93| Step: 0
Training loss: 2.7720947265625
Validation loss: 2.033980409304301

Epoch: 6| Step: 1
Training loss: 2.0906362533569336
Validation loss: 2.0321280360221863

Epoch: 6| Step: 2
Training loss: 1.4321460723876953
Validation loss: 2.035271863142649

Epoch: 6| Step: 3
Training loss: 2.6122987270355225
Validation loss: 2.0303174257278442

Epoch: 6| Step: 4
Training loss: 2.0447347164154053
Validation loss: 2.0264031489690146

Epoch: 6| Step: 5
Training loss: 1.9322372674942017
Validation loss: 2.030147910118103

Epoch: 6| Step: 6
Training loss: 1.7809951305389404
Validation loss: 2.017313241958618

Epoch: 6| Step: 7
Training loss: 2.6089138984680176
Validation loss: 2.0199992656707764

Epoch: 6| Step: 8
Training loss: 2.3255739212036133
Validation loss: 2.005260388056437

Epoch: 6| Step: 9
Training loss: 2.299870252609253
Validation loss: 2.0085476835568747

Epoch: 6| Step: 10
Training loss: 2.1461706161499023
Validation loss: 2.016024728616079

Epoch: 6| Step: 11
Training loss: 2.791325807571411
Validation loss: 2.011739353338877

Epoch: 6| Step: 12
Training loss: 1.4201481342315674
Validation loss: 2.0154772996902466

Epoch: 6| Step: 13
Training loss: 1.6313934326171875
Validation loss: 2.0142135620117188

Epoch: 94| Step: 0
Training loss: 2.0616931915283203
Validation loss: 2.0112460255622864

Epoch: 6| Step: 1
Training loss: 1.7170766592025757
Validation loss: 2.0158028602600098

Epoch: 6| Step: 2
Training loss: 2.181947708129883
Validation loss: 2.0094853043556213

Epoch: 6| Step: 3
Training loss: 2.198866367340088
Validation loss: 2.018012583255768

Epoch: 6| Step: 4
Training loss: 2.173574924468994
Validation loss: 2.0147674083709717

Epoch: 6| Step: 5
Training loss: 2.396946668624878
Validation loss: 2.0126879811286926

Epoch: 6| Step: 6
Training loss: 2.112023115158081
Validation loss: 2.012245237827301

Epoch: 6| Step: 7
Training loss: 2.2574212551116943
Validation loss: 2.0120979150136313

Epoch: 6| Step: 8
Training loss: 2.0120415687561035
Validation loss: 2.007053275903066

Epoch: 6| Step: 9
Training loss: 2.2242677211761475
Validation loss: 2.0120288729667664

Epoch: 6| Step: 10
Training loss: 2.2686469554901123
Validation loss: 2.0127190152804055

Epoch: 6| Step: 11
Training loss: 1.7454475164413452
Validation loss: 2.003241221110026

Epoch: 6| Step: 12
Training loss: 1.9561611413955688
Validation loss: 2.001749654610952

Epoch: 6| Step: 13
Training loss: 2.7280890941619873
Validation loss: 2.0134819944699607

Epoch: 95| Step: 0
Training loss: 2.3396482467651367
Validation loss: 2.016904433568319

Epoch: 6| Step: 1
Training loss: 2.0268826484680176
Validation loss: 2.029741048812866

Epoch: 6| Step: 2
Training loss: 2.2116639614105225
Validation loss: 2.031444807847341

Epoch: 6| Step: 3
Training loss: 2.0582704544067383
Validation loss: 2.0303727785746255

Epoch: 6| Step: 4
Training loss: 2.4785194396972656
Validation loss: 2.025972048441569

Epoch: 6| Step: 5
Training loss: 1.9057954549789429
Validation loss: 2.0257933338483176

Epoch: 6| Step: 6
Training loss: 1.6251113414764404
Validation loss: 2.015516479810079

Epoch: 6| Step: 7
Training loss: 2.3195722103118896
Validation loss: 2.015896479288737

Epoch: 6| Step: 8
Training loss: 2.363839626312256
Validation loss: 2.0208435654640198

Epoch: 6| Step: 9
Training loss: 2.0849335193634033
Validation loss: 2.014383872350057

Epoch: 6| Step: 10
Training loss: 1.9284276962280273
Validation loss: 2.006342669328054

Epoch: 6| Step: 11
Training loss: 2.09958553314209
Validation loss: 2.005434274673462

Epoch: 6| Step: 12
Training loss: 1.8788105249404907
Validation loss: 1.9985507130622864

Epoch: 6| Step: 13
Training loss: 2.343013286590576
Validation loss: 2.0030553539594016

Epoch: 96| Step: 0
Training loss: 2.6971569061279297
Validation loss: 2.006085375944773

Epoch: 6| Step: 1
Training loss: 1.8709602355957031
Validation loss: 2.0173657536506653

Epoch: 6| Step: 2
Training loss: 2.36067533493042
Validation loss: 2.018332620461782

Epoch: 6| Step: 3
Training loss: 2.400660276412964
Validation loss: 2.0259966452916465

Epoch: 6| Step: 4
Training loss: 2.086395740509033
Validation loss: 2.021416207154592

Epoch: 6| Step: 5
Training loss: 1.6629992723464966
Validation loss: 2.0249518156051636

Epoch: 6| Step: 6
Training loss: 2.0841917991638184
Validation loss: 2.028839866320292

Epoch: 6| Step: 7
Training loss: 1.867350697517395
Validation loss: 2.0310240189234414

Epoch: 6| Step: 8
Training loss: 2.5786855220794678
Validation loss: 2.026041090488434

Epoch: 6| Step: 9
Training loss: 2.1525328159332275
Validation loss: 2.031245172023773

Epoch: 6| Step: 10
Training loss: 2.292341947555542
Validation loss: 2.023618300755819

Epoch: 6| Step: 11
Training loss: 2.699537754058838
Validation loss: 2.025822023550669

Epoch: 6| Step: 12
Training loss: 1.6001030206680298
Validation loss: 2.01721062262853

Epoch: 6| Step: 13
Training loss: 2.1373932361602783
Validation loss: 2.01589963833491

Epoch: 97| Step: 0
Training loss: 1.9704530239105225
Validation loss: 2.000417868296305

Epoch: 6| Step: 1
Training loss: 2.471731424331665
Validation loss: 2.0019185741742453

Epoch: 6| Step: 2
Training loss: 2.0279667377471924
Validation loss: 2.004006107648214

Epoch: 6| Step: 3
Training loss: 2.3345301151275635
Validation loss: 2.0085925261179605

Epoch: 6| Step: 4
Training loss: 2.2543959617614746
Validation loss: 2.0188386042912803

Epoch: 6| Step: 5
Training loss: 2.05906343460083
Validation loss: 2.0185463428497314

Epoch: 6| Step: 6
Training loss: 2.170422077178955
Validation loss: 2.0294668674468994

Epoch: 6| Step: 7
Training loss: 2.4323625564575195
Validation loss: 2.0276132225990295

Epoch: 6| Step: 8
Training loss: 1.4529670476913452
Validation loss: 2.0254438320795694

Epoch: 6| Step: 9
Training loss: 2.221414804458618
Validation loss: 2.033568541208903

Epoch: 6| Step: 10
Training loss: 2.3562052249908447
Validation loss: 2.031019906202952

Epoch: 6| Step: 11
Training loss: 2.3145298957824707
Validation loss: 2.0274810194969177

Epoch: 6| Step: 12
Training loss: 1.62349534034729
Validation loss: 2.0139084657033286

Epoch: 6| Step: 13
Training loss: 2.2239527702331543
Validation loss: 2.0180617769559226

Epoch: 98| Step: 0
Training loss: 2.9546313285827637
Validation loss: 2.009998381137848

Epoch: 6| Step: 1
Training loss: 2.254953384399414
Validation loss: 2.0026562809944153

Epoch: 6| Step: 2
Training loss: 2.0652241706848145
Validation loss: 2.0048996210098267

Epoch: 6| Step: 3
Training loss: 1.7584145069122314
Validation loss: 2.0011398990948996

Epoch: 6| Step: 4
Training loss: 2.4316253662109375
Validation loss: 1.999981681505839

Epoch: 6| Step: 5
Training loss: 2.076030969619751
Validation loss: 1.9984392921129863

Epoch: 6| Step: 6
Training loss: 2.3955249786376953
Validation loss: 1.9976327021916707

Epoch: 6| Step: 7
Training loss: 1.7916337251663208
Validation loss: 1.9959303140640259

Epoch: 6| Step: 8
Training loss: 2.092428207397461
Validation loss: 2.003920038541158

Epoch: 6| Step: 9
Training loss: 2.140836477279663
Validation loss: 2.006325046221415

Epoch: 6| Step: 10
Training loss: 2.534435749053955
Validation loss: 2.005424996217092

Epoch: 6| Step: 11
Training loss: 1.5363285541534424
Validation loss: 2.0090346932411194

Epoch: 6| Step: 12
Training loss: 1.9729740619659424
Validation loss: 2.0165350834528604

Epoch: 6| Step: 13
Training loss: 1.8691768646240234
Validation loss: 2.0129089752833047

Epoch: 99| Step: 0
Training loss: 2.4648048877716064
Validation loss: 2.017205079396566

Epoch: 6| Step: 1
Training loss: 2.475635528564453
Validation loss: 2.022301216920217

Epoch: 6| Step: 2
Training loss: 2.298056125640869
Validation loss: 2.0302326679229736

Epoch: 6| Step: 3
Training loss: 1.7170662879943848
Validation loss: 2.0376288493474326

Epoch: 6| Step: 4
Training loss: 2.603475570678711
Validation loss: 2.045647998650869

Epoch: 6| Step: 5
Training loss: 2.130781650543213
Validation loss: 2.041629115740458

Epoch: 6| Step: 6
Training loss: 2.0511200428009033
Validation loss: 2.0332305232683816

Epoch: 6| Step: 7
Training loss: 2.0566556453704834
Validation loss: 2.007862865924835

Epoch: 6| Step: 8
Training loss: 2.483792781829834
Validation loss: 2.0092041293780007

Epoch: 6| Step: 9
Training loss: 2.3233656883239746
Validation loss: 2.0101107557614646

Epoch: 6| Step: 10
Training loss: 1.6934020519256592
Validation loss: 2.0093632141749063

Epoch: 6| Step: 11
Training loss: 2.041147232055664
Validation loss: 2.0067225893338523

Epoch: 6| Step: 12
Training loss: 1.7015457153320312
Validation loss: 2.0114128589630127

Epoch: 6| Step: 13
Training loss: 1.810934066772461
Validation loss: 2.012196878592173

Epoch: 100| Step: 0
Training loss: 1.908281683921814
Validation loss: 2.0118459463119507

Epoch: 6| Step: 1
Training loss: 2.2890853881835938
Validation loss: 2.015198071797689

Epoch: 6| Step: 2
Training loss: 1.963428258895874
Validation loss: 2.0219587087631226

Epoch: 6| Step: 3
Training loss: 2.599369764328003
Validation loss: 2.015866994857788

Epoch: 6| Step: 4
Training loss: 1.6863691806793213
Validation loss: 2.0129692554473877

Epoch: 6| Step: 5
Training loss: 1.8058199882507324
Validation loss: 2.011372705300649

Epoch: 6| Step: 6
Training loss: 2.3786702156066895
Validation loss: 2.0146148602167764

Epoch: 6| Step: 7
Training loss: 2.5802979469299316
Validation loss: 2.0099889437357583

Epoch: 6| Step: 8
Training loss: 1.8240822553634644
Validation loss: 2.0129045645395913

Epoch: 6| Step: 9
Training loss: 2.3115265369415283
Validation loss: 2.012011190255483

Epoch: 6| Step: 10
Training loss: 1.6761196851730347
Validation loss: 2.016840140024821

Epoch: 6| Step: 11
Training loss: 2.3324384689331055
Validation loss: 2.022215803464254

Epoch: 6| Step: 12
Training loss: 1.6880910396575928
Validation loss: 2.009028156598409

Epoch: 6| Step: 13
Training loss: 2.6843833923339844
Validation loss: 2.0150996446609497

Epoch: 101| Step: 0
Training loss: 2.4115614891052246
Validation loss: 2.0106831590334573

Epoch: 6| Step: 1
Training loss: 2.576648712158203
Validation loss: 2.0268688201904297

Epoch: 6| Step: 2
Training loss: 1.5430195331573486
Validation loss: 2.019846498966217

Epoch: 6| Step: 3
Training loss: 1.844853401184082
Validation loss: 2.01604296763738

Epoch: 6| Step: 4
Training loss: 1.9207786321640015
Validation loss: 2.0271944204966226

Epoch: 6| Step: 5
Training loss: 2.174593448638916
Validation loss: 2.017029086748759

Epoch: 6| Step: 6
Training loss: 1.609959602355957
Validation loss: 2.019184728463491

Epoch: 6| Step: 7
Training loss: 1.9188365936279297
Validation loss: 2.0205063819885254

Epoch: 6| Step: 8
Training loss: 1.9208897352218628
Validation loss: 2.0208167235056558

Epoch: 6| Step: 9
Training loss: 2.14316987991333
Validation loss: 2.0179767409960427

Epoch: 6| Step: 10
Training loss: 2.5159764289855957
Validation loss: 2.013956387837728

Epoch: 6| Step: 11
Training loss: 2.5343804359436035
Validation loss: 2.0087095896402993

Epoch: 6| Step: 12
Training loss: 1.9831212759017944
Validation loss: 2.011337637901306

Epoch: 6| Step: 13
Training loss: 2.5511975288391113
Validation loss: 2.005984385808309

Epoch: 102| Step: 0
Training loss: 2.1320242881774902
Validation loss: 2.008884390195211

Epoch: 6| Step: 1
Training loss: 2.1640892028808594
Validation loss: 2.0018070936203003

Epoch: 6| Step: 2
Training loss: 1.9591151475906372
Validation loss: 2.006670832633972

Epoch: 6| Step: 3
Training loss: 2.3767402172088623
Validation loss: 2.00521852572759

Epoch: 6| Step: 4
Training loss: 1.9830209016799927
Validation loss: 2.013616939385732

Epoch: 6| Step: 5
Training loss: 2.4735329151153564
Validation loss: 2.0097036957740784

Epoch: 6| Step: 6
Training loss: 1.8125046491622925
Validation loss: 2.0015514492988586

Epoch: 6| Step: 7
Training loss: 2.2784950733184814
Validation loss: 1.9991936484972637

Epoch: 6| Step: 8
Training loss: 2.080386161804199
Validation loss: 2.0017959078152976

Epoch: 6| Step: 9
Training loss: 2.093435049057007
Validation loss: 2.0118526220321655

Epoch: 6| Step: 10
Training loss: 2.1102852821350098
Validation loss: 2.016288181145986

Epoch: 6| Step: 11
Training loss: 2.27164626121521
Validation loss: 2.0181989669799805

Epoch: 6| Step: 12
Training loss: 2.0037078857421875
Validation loss: 2.029484510421753

Epoch: 6| Step: 13
Training loss: 1.859458088874817
Validation loss: 2.0366336504618325

Epoch: 103| Step: 0
Training loss: 1.8973478078842163
Validation loss: 2.0522860288619995

Epoch: 6| Step: 1
Training loss: 2.3056933879852295
Validation loss: 2.047984004020691

Epoch: 6| Step: 2
Training loss: 2.1951780319213867
Validation loss: 2.0468716621398926

Epoch: 6| Step: 3
Training loss: 2.2816193103790283
Validation loss: 2.0528388818105063

Epoch: 6| Step: 4
Training loss: 2.45202374458313
Validation loss: 2.049267510573069

Epoch: 6| Step: 5
Training loss: 2.4427731037139893
Validation loss: 2.050621430079142

Epoch: 6| Step: 6
Training loss: 2.838169574737549
Validation loss: 2.045827090740204

Epoch: 6| Step: 7
Training loss: 2.1887564659118652
Validation loss: 2.0360450744628906

Epoch: 6| Step: 8
Training loss: 1.1039695739746094
Validation loss: 2.0324758887290955

Epoch: 6| Step: 9
Training loss: 1.973996639251709
Validation loss: 2.008183538913727

Epoch: 6| Step: 10
Training loss: 1.9316505193710327
Validation loss: 2.007151106993357

Epoch: 6| Step: 11
Training loss: 1.7198238372802734
Validation loss: 2.0207953254381814

Epoch: 6| Step: 12
Training loss: 2.3545591831207275
Validation loss: 2.019005556901296

Epoch: 6| Step: 13
Training loss: 2.4143965244293213
Validation loss: 2.008334736029307

Epoch: 104| Step: 0
Training loss: 1.8325591087341309
Validation loss: 2.020862638950348

Epoch: 6| Step: 1
Training loss: 1.7413777112960815
Validation loss: 2.0090973178545632

Epoch: 6| Step: 2
Training loss: 1.67587411403656
Validation loss: 2.013533373673757

Epoch: 6| Step: 3
Training loss: 2.510643482208252
Validation loss: 2.008051613966624

Epoch: 6| Step: 4
Training loss: 1.7898879051208496
Validation loss: 2.0187873442967734

Epoch: 6| Step: 5
Training loss: 2.285017967224121
Validation loss: 2.0105759501457214

Epoch: 6| Step: 6
Training loss: 2.678190231323242
Validation loss: 2.0087817708651223

Epoch: 6| Step: 7
Training loss: 2.2000701427459717
Validation loss: 2.011287271976471

Epoch: 6| Step: 8
Training loss: 2.1526002883911133
Validation loss: 2.009704331556956

Epoch: 6| Step: 9
Training loss: 2.5869760513305664
Validation loss: 2.004864235719045

Epoch: 6| Step: 10
Training loss: 2.124904155731201
Validation loss: 2.0130558212598166

Epoch: 6| Step: 11
Training loss: 1.7582439184188843
Validation loss: 2.0066184997558594

Epoch: 6| Step: 12
Training loss: 2.6574501991271973
Validation loss: 2.010444621245066

Epoch: 6| Step: 13
Training loss: 1.9274041652679443
Validation loss: 2.0253634254137673

Epoch: 105| Step: 0
Training loss: 2.2557971477508545
Validation loss: 2.0298030177752175

Epoch: 6| Step: 1
Training loss: 2.3275418281555176
Validation loss: 2.0286036928494773

Epoch: 6| Step: 2
Training loss: 1.870336651802063
Validation loss: 2.035921275615692

Epoch: 6| Step: 3
Training loss: 1.7533994913101196
Validation loss: 2.0361750523249307

Epoch: 6| Step: 4
Training loss: 1.952175498008728
Validation loss: 2.0420427521069846

Epoch: 6| Step: 5
Training loss: 1.8169296979904175
Validation loss: 2.036331752936045

Epoch: 6| Step: 6
Training loss: 2.3526124954223633
Validation loss: 2.0434675812721252

Epoch: 6| Step: 7
Training loss: 2.5479354858398438
Validation loss: 2.037867565949758

Epoch: 6| Step: 8
Training loss: 1.6563351154327393
Validation loss: 2.0338783462842307

Epoch: 6| Step: 9
Training loss: 2.306922435760498
Validation loss: 2.0431655248006186

Epoch: 6| Step: 10
Training loss: 2.424830913543701
Validation loss: 2.0371716618537903

Epoch: 6| Step: 11
Training loss: 2.3859939575195312
Validation loss: 2.0285478035608926

Epoch: 6| Step: 12
Training loss: 2.338090419769287
Validation loss: 2.0277302463849387

Epoch: 6| Step: 13
Training loss: 1.638488531112671
Validation loss: 2.022278885046641

Epoch: 106| Step: 0
Training loss: 1.7760980129241943
Validation loss: 2.022040625413259

Epoch: 6| Step: 1
Training loss: 2.1137752532958984
Validation loss: 2.0268515149752298

Epoch: 6| Step: 2
Training loss: 1.9079242944717407
Validation loss: 2.0199331839879355

Epoch: 6| Step: 3
Training loss: 1.741336464881897
Validation loss: 2.0254089633623757

Epoch: 6| Step: 4
Training loss: 2.3846356868743896
Validation loss: 2.0250474413235984

Epoch: 6| Step: 5
Training loss: 1.882843017578125
Validation loss: 2.0297593673070273

Epoch: 6| Step: 6
Training loss: 2.317385196685791
Validation loss: 2.021828293800354

Epoch: 6| Step: 7
Training loss: 2.5521621704101562
Validation loss: 2.029999633630117

Epoch: 6| Step: 8
Training loss: 2.6808247566223145
Validation loss: 2.037696063518524

Epoch: 6| Step: 9
Training loss: 2.404736042022705
Validation loss: 2.0334816773732505

Epoch: 6| Step: 10
Training loss: 1.3808438777923584
Validation loss: 2.021898627281189

Epoch: 6| Step: 11
Training loss: 1.7411630153656006
Validation loss: 2.0181360046068826

Epoch: 6| Step: 12
Training loss: 2.0493292808532715
Validation loss: 2.0074148972829184

Epoch: 6| Step: 13
Training loss: 2.5919835567474365
Validation loss: 2.0245587825775146

Epoch: 107| Step: 0
Training loss: 2.3241000175476074
Validation loss: 2.0209019581476846

Epoch: 6| Step: 1
Training loss: 1.6900818347930908
Validation loss: 2.016374389330546

Epoch: 6| Step: 2
Training loss: 2.3085289001464844
Validation loss: 2.0157349705696106

Epoch: 6| Step: 3
Training loss: 2.2997519969940186
Validation loss: 2.0120354096094766

Epoch: 6| Step: 4
Training loss: 1.716549277305603
Validation loss: 2.0082510511080423

Epoch: 6| Step: 5
Training loss: 1.4297840595245361
Validation loss: 2.01482746998469

Epoch: 6| Step: 6
Training loss: 2.073319673538208
Validation loss: 2.008411169052124

Epoch: 6| Step: 7
Training loss: 2.008127212524414
Validation loss: 2.005354424317678

Epoch: 6| Step: 8
Training loss: 2.4526000022888184
Validation loss: 2.0041807492574057

Epoch: 6| Step: 9
Training loss: 2.427079677581787
Validation loss: 1.9957674344380696

Epoch: 6| Step: 10
Training loss: 2.460054874420166
Validation loss: 2.0125354727109275

Epoch: 6| Step: 11
Training loss: 2.3145053386688232
Validation loss: 2.0082597931226096

Epoch: 6| Step: 12
Training loss: 1.951326847076416
Validation loss: 2.014095107714335

Epoch: 6| Step: 13
Training loss: 2.0260863304138184
Validation loss: 2.016031503677368

Epoch: 108| Step: 0
Training loss: 1.985219120979309
Validation loss: 2.007386406262716

Epoch: 6| Step: 1
Training loss: 2.3527908325195312
Validation loss: 2.0154613653818765

Epoch: 6| Step: 2
Training loss: 2.3821561336517334
Validation loss: 2.0185362100601196

Epoch: 6| Step: 3
Training loss: 2.263784885406494
Validation loss: 2.0199844241142273

Epoch: 6| Step: 4
Training loss: 1.3239914178848267
Validation loss: 2.018062710762024

Epoch: 6| Step: 5
Training loss: 2.195431709289551
Validation loss: 2.0153660774230957

Epoch: 6| Step: 6
Training loss: 2.861794948577881
Validation loss: 2.0178221464157104

Epoch: 6| Step: 7
Training loss: 2.3087379932403564
Validation loss: 2.017370879650116

Epoch: 6| Step: 8
Training loss: 2.714378833770752
Validation loss: 2.0165621240933738

Epoch: 6| Step: 9
Training loss: 1.613088607788086
Validation loss: 2.012878100077311

Epoch: 6| Step: 10
Training loss: 2.125237464904785
Validation loss: 2.0179672837257385

Epoch: 6| Step: 11
Training loss: 1.8409655094146729
Validation loss: 2.0158480207125344

Epoch: 6| Step: 12
Training loss: 1.5237277746200562
Validation loss: 2.02008589108785

Epoch: 6| Step: 13
Training loss: 1.9515650272369385
Validation loss: 2.022763172785441

Epoch: 109| Step: 0
Training loss: 1.8191150426864624
Validation loss: 2.0176186760266623

Epoch: 6| Step: 1
Training loss: 1.85142183303833
Validation loss: 2.0158827702204385

Epoch: 6| Step: 2
Training loss: 1.9657180309295654
Validation loss: 2.022009531656901

Epoch: 6| Step: 3
Training loss: 1.7106449604034424
Validation loss: 2.019112785657247

Epoch: 6| Step: 4
Training loss: 1.6208635568618774
Validation loss: 2.0148302912712097

Epoch: 6| Step: 5
Training loss: 2.065558433532715
Validation loss: 2.0112448732058206

Epoch: 6| Step: 6
Training loss: 1.8848466873168945
Validation loss: 2.018522838751475

Epoch: 6| Step: 7
Training loss: 2.3119072914123535
Validation loss: 2.0068529645601907

Epoch: 6| Step: 8
Training loss: 2.755622386932373
Validation loss: 2.0067182381947837

Epoch: 6| Step: 9
Training loss: 2.1285758018493652
Validation loss: 2.0076348781585693

Epoch: 6| Step: 10
Training loss: 2.70794677734375
Validation loss: 2.010385533173879

Epoch: 6| Step: 11
Training loss: 1.6483298540115356
Validation loss: 2.015833616256714

Epoch: 6| Step: 12
Training loss: 2.205869436264038
Validation loss: 2.0138087471326194

Epoch: 6| Step: 13
Training loss: 2.6483206748962402
Validation loss: 2.015694816907247

Epoch: 110| Step: 0
Training loss: 2.5291404724121094
Validation loss: 2.017098049322764

Epoch: 6| Step: 1
Training loss: 2.0195021629333496
Validation loss: 2.021174351374308

Epoch: 6| Step: 2
Training loss: 1.9922295808792114
Validation loss: 2.014698882897695

Epoch: 6| Step: 3
Training loss: 2.6588549613952637
Validation loss: 2.009011705716451

Epoch: 6| Step: 4
Training loss: 2.400394916534424
Validation loss: 2.0046748717625937

Epoch: 6| Step: 5
Training loss: 1.9924968481063843
Validation loss: 1.9998484253883362

Epoch: 6| Step: 6
Training loss: 1.9378306865692139
Validation loss: 1.9995125532150269

Epoch: 6| Step: 7
Training loss: 2.6424560546875
Validation loss: 2.0064039826393127

Epoch: 6| Step: 8
Training loss: 1.4452000856399536
Validation loss: 2.006050785382589

Epoch: 6| Step: 9
Training loss: 2.086754322052002
Validation loss: 2.0091691414515176

Epoch: 6| Step: 10
Training loss: 1.7743332386016846
Validation loss: 2.010134736696879

Epoch: 6| Step: 11
Training loss: 2.2355237007141113
Validation loss: 2.006549914677938

Epoch: 6| Step: 12
Training loss: 1.5182690620422363
Validation loss: 2.010829051335653

Epoch: 6| Step: 13
Training loss: 2.0998687744140625
Validation loss: 2.0194925665855408

Epoch: 111| Step: 0
Training loss: 2.0177924633026123
Validation loss: 2.03684930006663

Epoch: 6| Step: 1
Training loss: 2.421506643295288
Validation loss: 2.0392391880353293

Epoch: 6| Step: 2
Training loss: 2.2114148139953613
Validation loss: 2.029497504234314

Epoch: 6| Step: 3
Training loss: 2.09714674949646
Validation loss: 2.053633371988932

Epoch: 6| Step: 4
Training loss: 2.196840524673462
Validation loss: 2.0414313673973083

Epoch: 6| Step: 5
Training loss: 1.4541535377502441
Validation loss: 2.053247392177582

Epoch: 6| Step: 6
Training loss: 2.505751848220825
Validation loss: 2.058957556883494

Epoch: 6| Step: 7
Training loss: 1.7707723379135132
Validation loss: 2.0713140964508057

Epoch: 6| Step: 8
Training loss: 2.562321662902832
Validation loss: 2.049165407816569

Epoch: 6| Step: 9
Training loss: 1.8994925022125244
Validation loss: 2.052012801170349

Epoch: 6| Step: 10
Training loss: 2.0425305366516113
Validation loss: 2.026891311009725

Epoch: 6| Step: 11
Training loss: 2.33316707611084
Validation loss: 2.0219863454500833

Epoch: 6| Step: 12
Training loss: 1.9794433116912842
Validation loss: 2.0083571871121726

Epoch: 6| Step: 13
Training loss: 1.9835184812545776
Validation loss: 2.0035335421562195

Epoch: 112| Step: 0
Training loss: 2.0279381275177
Validation loss: 2.013022541999817

Epoch: 6| Step: 1
Training loss: 2.2777678966522217
Validation loss: 2.015626768271128

Epoch: 6| Step: 2
Training loss: 2.1537351608276367
Validation loss: 2.013726452986399

Epoch: 6| Step: 3
Training loss: 2.661027431488037
Validation loss: 2.0195442835489907

Epoch: 6| Step: 4
Training loss: 1.8983677625656128
Validation loss: 2.0229835311571756

Epoch: 6| Step: 5
Training loss: 1.9042084217071533
Validation loss: 2.023105343182882

Epoch: 6| Step: 6
Training loss: 1.7340898513793945
Validation loss: 2.026692589124044

Epoch: 6| Step: 7
Training loss: 2.071692943572998
Validation loss: 2.0289941231409707

Epoch: 6| Step: 8
Training loss: 2.7723803520202637
Validation loss: 2.030506173769633

Epoch: 6| Step: 9
Training loss: 2.473773241043091
Validation loss: 2.030019541581472

Epoch: 6| Step: 10
Training loss: 1.847264289855957
Validation loss: 2.0219818552335105

Epoch: 6| Step: 11
Training loss: 2.192491292953491
Validation loss: 2.029462436834971

Epoch: 6| Step: 12
Training loss: 2.205742835998535
Validation loss: 2.0185575087865195

Epoch: 6| Step: 13
Training loss: 2.0726544857025146
Validation loss: 2.019697586695353

Epoch: 113| Step: 0
Training loss: 2.769559860229492
Validation loss: 2.019452154636383

Epoch: 6| Step: 1
Training loss: 2.3090953826904297
Validation loss: 2.015956381956736

Epoch: 6| Step: 2
Training loss: 2.0978505611419678
Validation loss: 2.0161757667859397

Epoch: 6| Step: 3
Training loss: 1.811751127243042
Validation loss: 2.009811798731486

Epoch: 6| Step: 4
Training loss: 2.476890802383423
Validation loss: 1.9931087295214336

Epoch: 6| Step: 5
Training loss: 1.8250199556350708
Validation loss: 1.9987623691558838

Epoch: 6| Step: 6
Training loss: 2.3654046058654785
Validation loss: 1.9991512298583984

Epoch: 6| Step: 7
Training loss: 1.9970028400421143
Validation loss: 1.9947518706321716

Epoch: 6| Step: 8
Training loss: 1.7843284606933594
Validation loss: 1.9994876583417256

Epoch: 6| Step: 9
Training loss: 1.7977960109710693
Validation loss: 2.0123940308888755

Epoch: 6| Step: 10
Training loss: 2.0459794998168945
Validation loss: 2.018374333779017

Epoch: 6| Step: 11
Training loss: 1.6395751237869263
Validation loss: 2.032814105351766

Epoch: 6| Step: 12
Training loss: 2.393146276473999
Validation loss: 2.0252681771914163

Epoch: 6| Step: 13
Training loss: 2.5477442741394043
Validation loss: 2.029590904712677

Epoch: 114| Step: 0
Training loss: 1.887218713760376
Validation loss: 2.0378617644309998

Epoch: 6| Step: 1
Training loss: 2.380343437194824
Validation loss: 2.020072817802429

Epoch: 6| Step: 2
Training loss: 1.9338488578796387
Validation loss: 2.0208985606829324

Epoch: 6| Step: 3
Training loss: 2.7291622161865234
Validation loss: 2.016754607359568

Epoch: 6| Step: 4
Training loss: 2.032390594482422
Validation loss: 2.0135317047437034

Epoch: 6| Step: 5
Training loss: 2.490201950073242
Validation loss: 2.0051873524983725

Epoch: 6| Step: 6
Training loss: 2.385432720184326
Validation loss: 2.002549111843109

Epoch: 6| Step: 7
Training loss: 1.7961180210113525
Validation loss: 1.9943276047706604

Epoch: 6| Step: 8
Training loss: 1.7869386672973633
Validation loss: 1.9886874953905742

Epoch: 6| Step: 9
Training loss: 1.8162118196487427
Validation loss: 1.988571325937907

Epoch: 6| Step: 10
Training loss: 2.041332483291626
Validation loss: 1.9947536389033

Epoch: 6| Step: 11
Training loss: 2.510002613067627
Validation loss: 1.9878047108650208

Epoch: 6| Step: 12
Training loss: 1.9194846153259277
Validation loss: 1.9932432969411213

Epoch: 6| Step: 13
Training loss: 1.8732755184173584
Validation loss: 1.9975444674491882

Epoch: 115| Step: 0
Training loss: 2.3234424591064453
Validation loss: 1.9907752871513367

Epoch: 6| Step: 1
Training loss: 2.0320420265197754
Validation loss: 1.9886927207310994

Epoch: 6| Step: 2
Training loss: 2.797060012817383
Validation loss: 1.9958074291547139

Epoch: 6| Step: 3
Training loss: 2.171374559402466
Validation loss: 1.9950059652328491

Epoch: 6| Step: 4
Training loss: 2.4106554985046387
Validation loss: 1.9977085590362549

Epoch: 6| Step: 5
Training loss: 2.326713800430298
Validation loss: 1.9974117080370586

Epoch: 6| Step: 6
Training loss: 1.5707378387451172
Validation loss: 2.000283102194468

Epoch: 6| Step: 7
Training loss: 1.945885181427002
Validation loss: 1.9982668956120808

Epoch: 6| Step: 8
Training loss: 2.308091878890991
Validation loss: 1.9995046655337017

Epoch: 6| Step: 9
Training loss: 2.0112674236297607
Validation loss: 2.0053303440411887

Epoch: 6| Step: 10
Training loss: 2.180647373199463
Validation loss: 2.0102058251698813

Epoch: 6| Step: 11
Training loss: 2.18459415435791
Validation loss: 2.015469570954641

Epoch: 6| Step: 12
Training loss: 1.217226266860962
Validation loss: 2.013061781724294

Epoch: 6| Step: 13
Training loss: 1.9635571241378784
Validation loss: 2.022600849469503

Epoch: 116| Step: 0
Training loss: 1.985128402709961
Validation loss: 2.0212411483128867

Epoch: 6| Step: 1
Training loss: 2.6383049488067627
Validation loss: 2.020118077596029

Epoch: 6| Step: 2
Training loss: 2.6127734184265137
Validation loss: 2.023152152697245

Epoch: 6| Step: 3
Training loss: 1.545686960220337
Validation loss: 2.0199713508288064

Epoch: 6| Step: 4
Training loss: 1.9468684196472168
Validation loss: 2.024058004220327

Epoch: 6| Step: 5
Training loss: 1.7712185382843018
Validation loss: 2.01453697681427

Epoch: 6| Step: 6
Training loss: 2.246760129928589
Validation loss: 2.0142608284950256

Epoch: 6| Step: 7
Training loss: 2.169447660446167
Validation loss: 2.0001004338264465

Epoch: 6| Step: 8
Training loss: 2.093825340270996
Validation loss: 2.0007336139678955

Epoch: 6| Step: 9
Training loss: 2.1236753463745117
Validation loss: 2.002877672513326

Epoch: 6| Step: 10
Training loss: 2.402092933654785
Validation loss: 2.005069454511007

Epoch: 6| Step: 11
Training loss: 2.2459521293640137
Validation loss: 2.003415564695994

Epoch: 6| Step: 12
Training loss: 1.520876169204712
Validation loss: 1.9965275923411052

Epoch: 6| Step: 13
Training loss: 2.152555227279663
Validation loss: 2.0027645031611123

Epoch: 117| Step: 0
Training loss: 2.1156165599823
Validation loss: 1.9995304544766743

Epoch: 6| Step: 1
Training loss: 2.220498561859131
Validation loss: 1.9979135592778523

Epoch: 6| Step: 2
Training loss: 2.0045340061187744
Validation loss: 2.004794398943583

Epoch: 6| Step: 3
Training loss: 1.9324145317077637
Validation loss: 2.002018849054972

Epoch: 6| Step: 4
Training loss: 2.255638599395752
Validation loss: 2.0079049468040466

Epoch: 6| Step: 5
Training loss: 2.1019623279571533
Validation loss: 2.022996723651886

Epoch: 6| Step: 6
Training loss: 1.3593051433563232
Validation loss: 2.0300363898277283

Epoch: 6| Step: 7
Training loss: 2.291212558746338
Validation loss: 2.0269360144933066

Epoch: 6| Step: 8
Training loss: 2.1771464347839355
Validation loss: 2.0317089756329856

Epoch: 6| Step: 9
Training loss: 1.699358582496643
Validation loss: 2.039220929145813

Epoch: 6| Step: 10
Training loss: 2.2168798446655273
Validation loss: 2.0281054576238

Epoch: 6| Step: 11
Training loss: 1.8995120525360107
Validation loss: 2.0344009598096213

Epoch: 6| Step: 12
Training loss: 2.195700168609619
Validation loss: 2.036875307559967

Epoch: 6| Step: 13
Training loss: 2.8536462783813477
Validation loss: 2.020916481812795

Epoch: 118| Step: 0
Training loss: 2.12078857421875
Validation loss: 2.012622674306234

Epoch: 6| Step: 1
Training loss: 2.124077081680298
Validation loss: 2.0122400522232056

Epoch: 6| Step: 2
Training loss: 2.090273857116699
Validation loss: 2.008602281411489

Epoch: 6| Step: 3
Training loss: 2.0965943336486816
Validation loss: 2.0088215271631875

Epoch: 6| Step: 4
Training loss: 2.5199663639068604
Validation loss: 2.006043334801992

Epoch: 6| Step: 5
Training loss: 1.3201655149459839
Validation loss: 2.0051663716634116

Epoch: 6| Step: 6
Training loss: 2.007328510284424
Validation loss: 2.0056358774503074

Epoch: 6| Step: 7
Training loss: 1.7988333702087402
Validation loss: 2.007541537284851

Epoch: 6| Step: 8
Training loss: 1.8405345678329468
Validation loss: 2.014567017555237

Epoch: 6| Step: 9
Training loss: 2.2019450664520264
Validation loss: 2.012514372666677

Epoch: 6| Step: 10
Training loss: 1.9664487838745117
Validation loss: 2.011731485525767

Epoch: 6| Step: 11
Training loss: 2.4370903968811035
Validation loss: 2.0150151451428733

Epoch: 6| Step: 12
Training loss: 2.1564743518829346
Validation loss: 2.016689956188202

Epoch: 6| Step: 13
Training loss: 2.365262985229492
Validation loss: 2.0211105147997537

Epoch: 119| Step: 0
Training loss: 1.7854523658752441
Validation loss: 2.018083115418752

Epoch: 6| Step: 1
Training loss: 1.6422609090805054
Validation loss: 2.0260581374168396

Epoch: 6| Step: 2
Training loss: 1.763535976409912
Validation loss: 2.0291704138120017

Epoch: 6| Step: 3
Training loss: 2.4050705432891846
Validation loss: 2.0228966077168784

Epoch: 6| Step: 4
Training loss: 2.415260076522827
Validation loss: 2.0223339398701987

Epoch: 6| Step: 5
Training loss: 1.7403181791305542
Validation loss: 2.0190304716428122

Epoch: 6| Step: 6
Training loss: 2.01566743850708
Validation loss: 2.0229084293047586

Epoch: 6| Step: 7
Training loss: 2.1999716758728027
Validation loss: 2.0285681883494058

Epoch: 6| Step: 8
Training loss: 1.757267713546753
Validation loss: 2.0211387872695923

Epoch: 6| Step: 9
Training loss: 2.2711076736450195
Validation loss: 2.021751801172892

Epoch: 6| Step: 10
Training loss: 2.639801502227783
Validation loss: 2.030633866786957

Epoch: 6| Step: 11
Training loss: 2.522876262664795
Validation loss: 2.0340195695559182

Epoch: 6| Step: 12
Training loss: 1.4895161390304565
Validation loss: 2.030355532964071

Epoch: 6| Step: 13
Training loss: 2.388507604598999
Validation loss: 2.0329858660697937

Epoch: 120| Step: 0
Training loss: 2.024488925933838
Validation loss: 2.0257617831230164

Epoch: 6| Step: 1
Training loss: 1.9254589080810547
Validation loss: 2.024839480717977

Epoch: 6| Step: 2
Training loss: 2.2537922859191895
Validation loss: 2.024645467599233

Epoch: 6| Step: 3
Training loss: 1.6093274354934692
Validation loss: 2.028088172276815

Epoch: 6| Step: 4
Training loss: 1.3255534172058105
Validation loss: 2.0216332276662192

Epoch: 6| Step: 5
Training loss: 2.2532763481140137
Validation loss: 2.0318354964256287

Epoch: 6| Step: 6
Training loss: 2.556882858276367
Validation loss: 2.025768200556437

Epoch: 6| Step: 7
Training loss: 1.5395374298095703
Validation loss: 2.017547070980072

Epoch: 6| Step: 8
Training loss: 2.211646556854248
Validation loss: 2.021195411682129

Epoch: 6| Step: 9
Training loss: 2.5430736541748047
Validation loss: 2.018512268861135

Epoch: 6| Step: 10
Training loss: 2.570936679840088
Validation loss: 2.0167912244796753

Epoch: 6| Step: 11
Training loss: 1.6340413093566895
Validation loss: 2.0158278147379556

Epoch: 6| Step: 12
Training loss: 2.270660877227783
Validation loss: 2.0117623607317605

Epoch: 6| Step: 13
Training loss: 2.197148561477661
Validation loss: 2.008906622727712

Epoch: 121| Step: 0
Training loss: 2.1072707176208496
Validation loss: 2.021035671234131

Epoch: 6| Step: 1
Training loss: 1.7530651092529297
Validation loss: 2.007735808690389

Epoch: 6| Step: 2
Training loss: 1.6534361839294434
Validation loss: 2.0063284834225974

Epoch: 6| Step: 3
Training loss: 2.218104839324951
Validation loss: 2.0036535263061523

Epoch: 6| Step: 4
Training loss: 2.0894758701324463
Validation loss: 2.014201521873474

Epoch: 6| Step: 5
Training loss: 2.3331665992736816
Validation loss: 2.004942158857981

Epoch: 6| Step: 6
Training loss: 2.471473455429077
Validation loss: 2.0101459622383118

Epoch: 6| Step: 7
Training loss: 1.67862069606781
Validation loss: 2.0097678899765015

Epoch: 6| Step: 8
Training loss: 1.8263463973999023
Validation loss: 2.0164392590522766

Epoch: 6| Step: 9
Training loss: 2.347078800201416
Validation loss: 2.0241995453834534

Epoch: 6| Step: 10
Training loss: 1.8826061487197876
Validation loss: 2.02053972085317

Epoch: 6| Step: 11
Training loss: 2.1750621795654297
Validation loss: 2.0269126693407693

Epoch: 6| Step: 12
Training loss: 2.036433696746826
Validation loss: 2.0228426655133567

Epoch: 6| Step: 13
Training loss: 2.330209493637085
Validation loss: 2.0207645495732627

Epoch: 122| Step: 0
Training loss: 2.199333667755127
Validation loss: 2.014719466368357

Epoch: 6| Step: 1
Training loss: 1.9267220497131348
Validation loss: 2.018473505973816

Epoch: 6| Step: 2
Training loss: 1.7802469730377197
Validation loss: 2.023713847001394

Epoch: 6| Step: 3
Training loss: 1.9710630178451538
Validation loss: 2.0202861626942954

Epoch: 6| Step: 4
Training loss: 2.050142765045166
Validation loss: 2.0217676162719727

Epoch: 6| Step: 5
Training loss: 2.5403456687927246
Validation loss: 2.020242174466451

Epoch: 6| Step: 6
Training loss: 2.5327634811401367
Validation loss: 2.0200093587239585

Epoch: 6| Step: 7
Training loss: 2.186829090118408
Validation loss: 2.024511913458506

Epoch: 6| Step: 8
Training loss: 2.343996286392212
Validation loss: 2.022560258706411

Epoch: 6| Step: 9
Training loss: 1.3241469860076904
Validation loss: 2.0183184345563254

Epoch: 6| Step: 10
Training loss: 1.5621355772018433
Validation loss: 2.027837872505188

Epoch: 6| Step: 11
Training loss: 2.4515583515167236
Validation loss: 2.0249571402867637

Epoch: 6| Step: 12
Training loss: 2.0073940753936768
Validation loss: 2.0320566495259604

Epoch: 6| Step: 13
Training loss: 2.124713659286499
Validation loss: 2.0257993936538696

Epoch: 123| Step: 0
Training loss: 1.9185460805892944
Validation loss: 2.0290011763572693

Epoch: 6| Step: 1
Training loss: 1.5900179147720337
Validation loss: 2.023779491583506

Epoch: 6| Step: 2
Training loss: 2.7904598712921143
Validation loss: 2.031559328238169

Epoch: 6| Step: 3
Training loss: 2.3386011123657227
Validation loss: 2.026496390501658

Epoch: 6| Step: 4
Training loss: 1.719595193862915
Validation loss: 2.0217844446500144

Epoch: 6| Step: 5
Training loss: 1.8240306377410889
Validation loss: 2.027298072973887

Epoch: 6| Step: 6
Training loss: 1.9708805084228516
Validation loss: 2.0248477458953857

Epoch: 6| Step: 7
Training loss: 1.9363272190093994
Validation loss: 2.0236802101135254

Epoch: 6| Step: 8
Training loss: 2.4376556873321533
Validation loss: 2.018837332725525

Epoch: 6| Step: 9
Training loss: 1.8188081979751587
Validation loss: 2.020933429400126

Epoch: 6| Step: 10
Training loss: 1.6576204299926758
Validation loss: 2.025374174118042

Epoch: 6| Step: 11
Training loss: 2.4338736534118652
Validation loss: 2.017165462176005

Epoch: 6| Step: 12
Training loss: 2.7586193084716797
Validation loss: 2.0093891620635986

Epoch: 6| Step: 13
Training loss: 1.7539293766021729
Validation loss: 2.0210214853286743

Epoch: 124| Step: 0
Training loss: 2.319836139678955
Validation loss: 2.009865125020345

Epoch: 6| Step: 1
Training loss: 1.6949539184570312
Validation loss: 2.0213720997174582

Epoch: 6| Step: 2
Training loss: 2.0889642238616943
Validation loss: 2.0181007186571756

Epoch: 6| Step: 3
Training loss: 2.1970157623291016
Validation loss: 2.0202597777048745

Epoch: 6| Step: 4
Training loss: 2.443774700164795
Validation loss: 2.028167247772217

Epoch: 6| Step: 5
Training loss: 2.5748047828674316
Validation loss: 2.027335047721863

Epoch: 6| Step: 6
Training loss: 1.6664514541625977
Validation loss: 2.0273194511731467

Epoch: 6| Step: 7
Training loss: 1.635117769241333
Validation loss: 2.033284385999044

Epoch: 6| Step: 8
Training loss: 2.321488857269287
Validation loss: 2.0313838521639505

Epoch: 6| Step: 9
Training loss: 1.9168243408203125
Validation loss: 2.0288489858309426

Epoch: 6| Step: 10
Training loss: 2.091946601867676
Validation loss: 2.032229701677958

Epoch: 6| Step: 11
Training loss: 1.9069592952728271
Validation loss: 2.0279942750930786

Epoch: 6| Step: 12
Training loss: 1.8919801712036133
Validation loss: 2.035108745098114

Epoch: 6| Step: 13
Training loss: 2.148937463760376
Validation loss: 2.039648433526357

Epoch: 125| Step: 0
Training loss: 2.3745620250701904
Validation loss: 2.0269630948702493

Epoch: 6| Step: 1
Training loss: 2.4267187118530273
Validation loss: 2.029920836289724

Epoch: 6| Step: 2
Training loss: 2.0127196311950684
Validation loss: 2.0279535055160522

Epoch: 6| Step: 3
Training loss: 1.5134968757629395
Validation loss: 2.028563380241394

Epoch: 6| Step: 4
Training loss: 2.0566329956054688
Validation loss: 2.0233885844548545

Epoch: 6| Step: 5
Training loss: 1.6583906412124634
Validation loss: 2.0218294064203897

Epoch: 6| Step: 6
Training loss: 1.6954673528671265
Validation loss: 2.030424972375234

Epoch: 6| Step: 7
Training loss: 2.003939628601074
Validation loss: 2.0226706862449646

Epoch: 6| Step: 8
Training loss: 2.9363341331481934
Validation loss: 2.0206675132115683

Epoch: 6| Step: 9
Training loss: 1.831809163093567
Validation loss: 2.006224354108175

Epoch: 6| Step: 10
Training loss: 2.540438175201416
Validation loss: 2.0151819388071694

Epoch: 6| Step: 11
Training loss: 2.088801860809326
Validation loss: 2.0077547828356423

Epoch: 6| Step: 12
Training loss: 2.1853904724121094
Validation loss: 1.9961092869440715

Epoch: 6| Step: 13
Training loss: 1.678020715713501
Validation loss: 2.0032549500465393

Epoch: 126| Step: 0
Training loss: 1.7626365423202515
Validation loss: 2.012422184149424

Epoch: 6| Step: 1
Training loss: 1.7854516506195068
Validation loss: 2.0230385263760886

Epoch: 6| Step: 2
Training loss: 1.7671904563903809
Validation loss: 2.032483081022898

Epoch: 6| Step: 3
Training loss: 2.902458667755127
Validation loss: 2.041142702102661

Epoch: 6| Step: 4
Training loss: 2.4915294647216797
Validation loss: 2.0334810415903726

Epoch: 6| Step: 5
Training loss: 1.7619850635528564
Validation loss: 2.051956534385681

Epoch: 6| Step: 6
Training loss: 2.3033668994903564
Validation loss: 2.0343772172927856

Epoch: 6| Step: 7
Training loss: 1.798375129699707
Validation loss: 2.031525174776713

Epoch: 6| Step: 8
Training loss: 2.072385787963867
Validation loss: 2.0334920287132263

Epoch: 6| Step: 9
Training loss: 2.425723075866699
Validation loss: 2.0259377559026084

Epoch: 6| Step: 10
Training loss: 1.8160371780395508
Validation loss: 2.014708638191223

Epoch: 6| Step: 11
Training loss: 1.3957880735397339
Validation loss: 2.0141888856887817

Epoch: 6| Step: 12
Training loss: 3.017033338546753
Validation loss: 2.0159282088279724

Epoch: 6| Step: 13
Training loss: 1.754868984222412
Validation loss: 2.017556925614675

Epoch: 127| Step: 0
Training loss: 2.4297690391540527
Validation loss: 2.0204366644223533

Epoch: 6| Step: 1
Training loss: 2.0869507789611816
Validation loss: 2.0125688314437866

Epoch: 6| Step: 2
Training loss: 1.9463059902191162
Validation loss: 2.0185901323954263

Epoch: 6| Step: 3
Training loss: 2.266829252243042
Validation loss: 2.010976711908976

Epoch: 6| Step: 4
Training loss: 1.853316307067871
Validation loss: 2.0173559387524924

Epoch: 6| Step: 5
Training loss: 2.3294262886047363
Validation loss: 2.016217509905497

Epoch: 6| Step: 6
Training loss: 1.9960874319076538
Validation loss: 2.013272523880005

Epoch: 6| Step: 7
Training loss: 1.7572685480117798
Validation loss: 2.018704354763031

Epoch: 6| Step: 8
Training loss: 2.030089855194092
Validation loss: 2.017424702644348

Epoch: 6| Step: 9
Training loss: 2.5283753871917725
Validation loss: 2.021444002787272

Epoch: 6| Step: 10
Training loss: 1.8763868808746338
Validation loss: 2.0323503613471985

Epoch: 6| Step: 11
Training loss: 1.889998197555542
Validation loss: 2.023361086845398

Epoch: 6| Step: 12
Training loss: 2.3852016925811768
Validation loss: 2.0372462272644043

Epoch: 6| Step: 13
Training loss: 1.3408143520355225
Validation loss: 2.0431326031684875

Epoch: 128| Step: 0
Training loss: 2.238635301589966
Validation loss: 2.0468437870343528

Epoch: 6| Step: 1
Training loss: 2.113800525665283
Validation loss: 2.0579498410224915

Epoch: 6| Step: 2
Training loss: 2.305957317352295
Validation loss: 2.046098291873932

Epoch: 6| Step: 3
Training loss: 1.6038974523544312
Validation loss: 2.0327475865681968

Epoch: 6| Step: 4
Training loss: 2.1878037452697754
Validation loss: 2.0332558353741965

Epoch: 6| Step: 5
Training loss: 2.2225027084350586
Validation loss: 2.029111862182617

Epoch: 6| Step: 6
Training loss: 2.029487371444702
Validation loss: 2.0240376194318137

Epoch: 6| Step: 7
Training loss: 1.976750373840332
Validation loss: 2.014864961306254

Epoch: 6| Step: 8
Training loss: 1.0260236263275146
Validation loss: 2.010877788066864

Epoch: 6| Step: 9
Training loss: 2.0493035316467285
Validation loss: 2.0177802642186484

Epoch: 6| Step: 10
Training loss: 2.385097026824951
Validation loss: 2.0248584349950156

Epoch: 6| Step: 11
Training loss: 2.5316038131713867
Validation loss: 2.0355018973350525

Epoch: 6| Step: 12
Training loss: 2.2344589233398438
Validation loss: 2.048233389854431

Epoch: 6| Step: 13
Training loss: 2.003941297531128
Validation loss: 2.0340843399365744

Epoch: 129| Step: 0
Training loss: 2.120405673980713
Validation loss: 2.0320767164230347

Epoch: 6| Step: 1
Training loss: 1.7835893630981445
Validation loss: 2.025656541188558

Epoch: 6| Step: 2
Training loss: 2.536656618118286
Validation loss: 2.0242340763409934

Epoch: 6| Step: 3
Training loss: 1.6625449657440186
Validation loss: 2.0121994018554688

Epoch: 6| Step: 4
Training loss: 2.172245502471924
Validation loss: 2.0124028523763022

Epoch: 6| Step: 5
Training loss: 1.9461054801940918
Validation loss: 2.0241387685139975

Epoch: 6| Step: 6
Training loss: 1.8397634029388428
Validation loss: 2.022751728693644

Epoch: 6| Step: 7
Training loss: 2.2761406898498535
Validation loss: 2.0200743277867637

Epoch: 6| Step: 8
Training loss: 1.9190723896026611
Validation loss: 2.0296937028566995

Epoch: 6| Step: 9
Training loss: 1.5841083526611328
Validation loss: 2.028972109158834

Epoch: 6| Step: 10
Training loss: 2.364893674850464
Validation loss: 2.0184597969055176

Epoch: 6| Step: 11
Training loss: 2.5710785388946533
Validation loss: 2.0217304627100625

Epoch: 6| Step: 12
Training loss: 2.109415054321289
Validation loss: 2.026275853315989

Epoch: 6| Step: 13
Training loss: 2.0307421684265137
Validation loss: 2.0417882204055786

Epoch: 130| Step: 0
Training loss: 1.9419547319412231
Validation loss: 2.0251799821853638

Epoch: 6| Step: 1
Training loss: 2.177025556564331
Validation loss: 2.0315483808517456

Epoch: 6| Step: 2
Training loss: 1.801872968673706
Validation loss: 2.04186604420344

Epoch: 6| Step: 3
Training loss: 1.9727661609649658
Validation loss: 2.035699208577474

Epoch: 6| Step: 4
Training loss: 1.87306547164917
Validation loss: 2.029647092024485

Epoch: 6| Step: 5
Training loss: 2.697110891342163
Validation loss: 2.0282764037450156

Epoch: 6| Step: 6
Training loss: 1.7020184993743896
Validation loss: 2.0308074752489724

Epoch: 6| Step: 7
Training loss: 1.9992256164550781
Validation loss: 2.0175987482070923

Epoch: 6| Step: 8
Training loss: 1.9789690971374512
Validation loss: 2.0347012678782144

Epoch: 6| Step: 9
Training loss: 2.383449077606201
Validation loss: 2.0270302295684814

Epoch: 6| Step: 10
Training loss: 1.9847396612167358
Validation loss: 2.0212196509043374

Epoch: 6| Step: 11
Training loss: 1.9784964323043823
Validation loss: 2.024335583051046

Epoch: 6| Step: 12
Training loss: 2.32020902633667
Validation loss: 2.0433842738469443

Epoch: 6| Step: 13
Training loss: 2.098773956298828
Validation loss: 2.035456597805023

Epoch: 131| Step: 0
Training loss: 2.513855457305908
Validation loss: 2.0219358007113137

Epoch: 6| Step: 1
Training loss: 1.9430689811706543
Validation loss: 2.0352731347084045

Epoch: 6| Step: 2
Training loss: 2.1521058082580566
Validation loss: 2.029626170794169

Epoch: 6| Step: 3
Training loss: 2.5678153038024902
Validation loss: 2.0379950205485025

Epoch: 6| Step: 4
Training loss: 1.8234939575195312
Validation loss: 2.033458332220713

Epoch: 6| Step: 5
Training loss: 1.738513708114624
Validation loss: 2.029329478740692

Epoch: 6| Step: 6
Training loss: 2.1764230728149414
Validation loss: 2.0342247684796653

Epoch: 6| Step: 7
Training loss: 1.5663238763809204
Validation loss: 2.034187972545624

Epoch: 6| Step: 8
Training loss: 2.2050790786743164
Validation loss: 2.0379625161488852

Epoch: 6| Step: 9
Training loss: 1.834773063659668
Validation loss: 2.03989843527476

Epoch: 6| Step: 10
Training loss: 1.9950995445251465
Validation loss: 2.035657505194346

Epoch: 6| Step: 11
Training loss: 2.463120698928833
Validation loss: 2.039937953154246

Epoch: 6| Step: 12
Training loss: 1.5675568580627441
Validation loss: 2.0246970852216086

Epoch: 6| Step: 13
Training loss: 2.3217010498046875
Validation loss: 2.028806765874227

Epoch: 132| Step: 0
Training loss: 1.5026227235794067
Validation loss: 2.026575247446696

Epoch: 6| Step: 1
Training loss: 1.8871216773986816
Validation loss: 2.022464334964752

Epoch: 6| Step: 2
Training loss: 1.5977076292037964
Validation loss: 2.0272007981936135

Epoch: 6| Step: 3
Training loss: 2.027794361114502
Validation loss: 2.021801233291626

Epoch: 6| Step: 4
Training loss: 2.1811764240264893
Validation loss: 2.023707409699758

Epoch: 6| Step: 5
Training loss: 2.3312807083129883
Validation loss: 2.0198371609052024

Epoch: 6| Step: 6
Training loss: 1.8887763023376465
Validation loss: 2.024709701538086

Epoch: 6| Step: 7
Training loss: 1.8997936248779297
Validation loss: 2.0217371384302774

Epoch: 6| Step: 8
Training loss: 2.065927028656006
Validation loss: 2.017352302869161

Epoch: 6| Step: 9
Training loss: 2.252168655395508
Validation loss: 2.0212387839953103

Epoch: 6| Step: 10
Training loss: 2.1297152042388916
Validation loss: 2.029066185156504

Epoch: 6| Step: 11
Training loss: 2.3939132690429688
Validation loss: 2.026818792025248

Epoch: 6| Step: 12
Training loss: 2.630249261856079
Validation loss: 2.031964202721914

Epoch: 6| Step: 13
Training loss: 2.190049648284912
Validation loss: 2.023558517297109

Epoch: 133| Step: 0
Training loss: 2.5248796939849854
Validation loss: 2.017858564853668

Epoch: 6| Step: 1
Training loss: 2.006009101867676
Validation loss: 2.018631855646769

Epoch: 6| Step: 2
Training loss: 1.714761734008789
Validation loss: 2.024416704972585

Epoch: 6| Step: 3
Training loss: 2.2655744552612305
Validation loss: 2.0241185824076333

Epoch: 6| Step: 4
Training loss: 1.811781883239746
Validation loss: 2.0236259500185647

Epoch: 6| Step: 5
Training loss: 2.0410289764404297
Validation loss: 2.0228788455327353

Epoch: 6| Step: 6
Training loss: 2.169816017150879
Validation loss: 2.031368613243103

Epoch: 6| Step: 7
Training loss: 2.670473575592041
Validation loss: 2.0318488279978433

Epoch: 6| Step: 8
Training loss: 1.618430495262146
Validation loss: 2.031568189462026

Epoch: 6| Step: 9
Training loss: 2.229691505432129
Validation loss: 2.0368090669314065

Epoch: 6| Step: 10
Training loss: 1.6618825197219849
Validation loss: 2.0444020430246987

Epoch: 6| Step: 11
Training loss: 1.615713119506836
Validation loss: 2.0511577129364014

Epoch: 6| Step: 12
Training loss: 2.6486759185791016
Validation loss: 2.066131134827932

Epoch: 6| Step: 13
Training loss: 1.7299965620040894
Validation loss: 2.0598219434420266

Epoch: 134| Step: 0
Training loss: 2.1945700645446777
Validation loss: 2.0687752962112427

Epoch: 6| Step: 1
Training loss: 2.212521553039551
Validation loss: 2.0692159136136374

Epoch: 6| Step: 2
Training loss: 1.8348913192749023
Validation loss: 2.0760452151298523

Epoch: 6| Step: 3
Training loss: 1.9988439083099365
Validation loss: 2.0831100145975747

Epoch: 6| Step: 4
Training loss: 2.3234214782714844
Validation loss: 2.0876317421595254

Epoch: 6| Step: 5
Training loss: 2.1483683586120605
Validation loss: 2.0755610863367715

Epoch: 6| Step: 6
Training loss: 2.0267436504364014
Validation loss: 2.0667840242385864

Epoch: 6| Step: 7
Training loss: 1.7722563743591309
Validation loss: 2.064060608545939

Epoch: 6| Step: 8
Training loss: 2.028510332107544
Validation loss: 2.0416043996810913

Epoch: 6| Step: 9
Training loss: 1.872917890548706
Validation loss: 2.0395859281222024

Epoch: 6| Step: 10
Training loss: 2.17057728767395
Validation loss: 2.032666107018789

Epoch: 6| Step: 11
Training loss: 1.995977520942688
Validation loss: 2.0333156983057656

Epoch: 6| Step: 12
Training loss: 2.222165107727051
Validation loss: 2.0384039282798767

Epoch: 6| Step: 13
Training loss: 2.0245933532714844
Validation loss: 2.0370046099027

Epoch: 135| Step: 0
Training loss: 1.6167162656784058
Validation loss: 2.0375989079475403

Epoch: 6| Step: 1
Training loss: 2.352036952972412
Validation loss: 2.0291247963905334

Epoch: 6| Step: 2
Training loss: 1.7924046516418457
Validation loss: 2.0394234657287598

Epoch: 6| Step: 3
Training loss: 1.9587159156799316
Validation loss: 2.036783238252004

Epoch: 6| Step: 4
Training loss: 2.301912307739258
Validation loss: 2.0433963338534036

Epoch: 6| Step: 5
Training loss: 1.6775214672088623
Validation loss: 2.041499972343445

Epoch: 6| Step: 6
Training loss: 2.123838424682617
Validation loss: 2.039853314558665

Epoch: 6| Step: 7
Training loss: 1.7251063585281372
Validation loss: 2.045942723751068

Epoch: 6| Step: 8
Training loss: 1.7785725593566895
Validation loss: 2.050334930419922

Epoch: 6| Step: 9
Training loss: 2.0934739112854004
Validation loss: 2.0530512928962708

Epoch: 6| Step: 10
Training loss: 2.895922899246216
Validation loss: 2.0582518577575684

Epoch: 6| Step: 11
Training loss: 1.6784085035324097
Validation loss: 2.066001852353414

Epoch: 6| Step: 12
Training loss: 2.8469510078430176
Validation loss: 2.062144378821055

Epoch: 6| Step: 13
Training loss: 1.8769700527191162
Validation loss: 2.053776482741038

Epoch: 136| Step: 0
Training loss: 1.732196569442749
Validation loss: 2.0371225078900657

Epoch: 6| Step: 1
Training loss: 2.168260097503662
Validation loss: 2.041564126809438

Epoch: 6| Step: 2
Training loss: 2.319002628326416
Validation loss: 2.0308261712392173

Epoch: 6| Step: 3
Training loss: 2.0473527908325195
Validation loss: 2.031243403752645

Epoch: 6| Step: 4
Training loss: 2.2356717586517334
Validation loss: 2.033453643321991

Epoch: 6| Step: 5
Training loss: 2.269923210144043
Validation loss: 2.039066215356191

Epoch: 6| Step: 6
Training loss: 1.9040563106536865
Validation loss: 2.0359620253245034

Epoch: 6| Step: 7
Training loss: 2.0732085704803467
Validation loss: 2.0370067159334817

Epoch: 6| Step: 8
Training loss: 1.7791985273361206
Validation loss: 2.0364133715629578

Epoch: 6| Step: 9
Training loss: 2.0215866565704346
Validation loss: 2.029443879922231

Epoch: 6| Step: 10
Training loss: 1.8767971992492676
Validation loss: 2.0406822164853415

Epoch: 6| Step: 11
Training loss: 2.227328062057495
Validation loss: 2.0509618322054544

Epoch: 6| Step: 12
Training loss: 1.59423828125
Validation loss: 2.045478641986847

Epoch: 6| Step: 13
Training loss: 2.5935418605804443
Validation loss: 2.049027065436045

Epoch: 137| Step: 0
Training loss: 1.928012728691101
Validation loss: 2.050611197948456

Epoch: 6| Step: 1
Training loss: 2.4870426654815674
Validation loss: 2.0552406907081604

Epoch: 6| Step: 2
Training loss: 2.055814027786255
Validation loss: 2.0537546277046204

Epoch: 6| Step: 3
Training loss: 2.080078601837158
Validation loss: 2.0527669390042624

Epoch: 6| Step: 4
Training loss: 2.0369725227355957
Validation loss: 2.0536967714627585

Epoch: 6| Step: 5
Training loss: 2.148582696914673
Validation loss: 2.0406391819318137

Epoch: 6| Step: 6
Training loss: 2.866273880004883
Validation loss: 2.0505139430363974

Epoch: 6| Step: 7
Training loss: 1.3431949615478516
Validation loss: 2.0434553821881614

Epoch: 6| Step: 8
Training loss: 2.132312774658203
Validation loss: 2.0522379080454507

Epoch: 6| Step: 9
Training loss: 1.6417289972305298
Validation loss: 2.0507338643074036

Epoch: 6| Step: 10
Training loss: 2.4953348636627197
Validation loss: 2.048601806163788

Epoch: 6| Step: 11
Training loss: 1.7537610530853271
Validation loss: 2.0473219752311707

Epoch: 6| Step: 12
Training loss: 1.623619556427002
Validation loss: 2.0449422796567283

Epoch: 6| Step: 13
Training loss: 1.9848843812942505
Validation loss: 2.0372463266054788

Epoch: 138| Step: 0
Training loss: 2.1848411560058594
Validation loss: 2.033665577570597

Epoch: 6| Step: 1
Training loss: 2.004354476928711
Validation loss: 2.0316112438837686

Epoch: 6| Step: 2
Training loss: 1.703565239906311
Validation loss: 2.0354719360669455

Epoch: 6| Step: 3
Training loss: 2.1461730003356934
Validation loss: 2.034105976422628

Epoch: 6| Step: 4
Training loss: 2.110886812210083
Validation loss: 2.0253148277600608

Epoch: 6| Step: 5
Training loss: 2.240290880203247
Validation loss: 2.0396328568458557

Epoch: 6| Step: 6
Training loss: 1.7065272331237793
Validation loss: 2.031981190045675

Epoch: 6| Step: 7
Training loss: 2.1567928791046143
Validation loss: 2.033901651700338

Epoch: 6| Step: 8
Training loss: 1.292792558670044
Validation loss: 2.0335145394007363

Epoch: 6| Step: 9
Training loss: 1.9158134460449219
Validation loss: 2.0402985413869223

Epoch: 6| Step: 10
Training loss: 1.8486303091049194
Validation loss: 2.0319116910298667

Epoch: 6| Step: 11
Training loss: 2.713016986846924
Validation loss: 2.0393943786621094

Epoch: 6| Step: 12
Training loss: 2.0067391395568848
Validation loss: 2.054283618927002

Epoch: 6| Step: 13
Training loss: 2.513497829437256
Validation loss: 2.067191223303477

Epoch: 139| Step: 0
Training loss: 2.996400833129883
Validation loss: 2.075354516506195

Epoch: 6| Step: 1
Training loss: 2.669300079345703
Validation loss: 2.0902807116508484

Epoch: 6| Step: 2
Training loss: 2.4784908294677734
Validation loss: 2.0880058209101358

Epoch: 6| Step: 3
Training loss: 1.6963627338409424
Validation loss: 2.0801475445429483

Epoch: 6| Step: 4
Training loss: 1.7869089841842651
Validation loss: 2.0751037001609802

Epoch: 6| Step: 5
Training loss: 1.8915629386901855
Validation loss: 2.0602792501449585

Epoch: 6| Step: 6
Training loss: 2.4393208026885986
Validation loss: 2.0586207509040833

Epoch: 6| Step: 7
Training loss: 1.735006332397461
Validation loss: 2.0361846884091697

Epoch: 6| Step: 8
Training loss: 1.519127607345581
Validation loss: 2.0465298891067505

Epoch: 6| Step: 9
Training loss: 1.4508367776870728
Validation loss: 2.034829080104828

Epoch: 6| Step: 10
Training loss: 1.922476053237915
Validation loss: 2.0363444288571677

Epoch: 6| Step: 11
Training loss: 1.9645227193832397
Validation loss: 2.035608649253845

Epoch: 6| Step: 12
Training loss: 2.3579227924346924
Validation loss: 2.0406455198923745

Epoch: 6| Step: 13
Training loss: 1.7308712005615234
Validation loss: 2.0511777997016907

Epoch: 140| Step: 0
Training loss: 1.9759918451309204
Validation loss: 2.0489975412686667

Epoch: 6| Step: 1
Training loss: 2.0682952404022217
Validation loss: 2.0460666020711265

Epoch: 6| Step: 2
Training loss: 1.6469101905822754
Validation loss: 2.0461379090944924

Epoch: 6| Step: 3
Training loss: 1.8693066835403442
Validation loss: 2.050639788309733

Epoch: 6| Step: 4
Training loss: 2.028313159942627
Validation loss: 2.0450538198153176

Epoch: 6| Step: 5
Training loss: 1.6560062170028687
Validation loss: 2.0442081292470298

Epoch: 6| Step: 6
Training loss: 2.33355712890625
Validation loss: 2.053071916103363

Epoch: 6| Step: 7
Training loss: 1.682262659072876
Validation loss: 2.0530795454978943

Epoch: 6| Step: 8
Training loss: 1.9246273040771484
Validation loss: 2.054990371068319

Epoch: 6| Step: 9
Training loss: 2.129385232925415
Validation loss: 2.0510226686795554

Epoch: 6| Step: 10
Training loss: 2.557199478149414
Validation loss: 2.04809699455897

Epoch: 6| Step: 11
Training loss: 1.9130113124847412
Validation loss: 2.0575453837712607

Epoch: 6| Step: 12
Training loss: 2.3148274421691895
Validation loss: 2.050841748714447

Epoch: 6| Step: 13
Training loss: 2.1925010681152344
Validation loss: 2.061247150103251

Epoch: 141| Step: 0
Training loss: 1.6530284881591797
Validation loss: 2.042205254236857

Epoch: 6| Step: 1
Training loss: 2.1600704193115234
Validation loss: 2.047260602315267

Epoch: 6| Step: 2
Training loss: 2.4193310737609863
Validation loss: 2.0575668811798096

Epoch: 6| Step: 3
Training loss: 2.15010404586792
Validation loss: 2.0609391729036965

Epoch: 6| Step: 4
Training loss: 1.5096640586853027
Validation loss: 2.064063290754954

Epoch: 6| Step: 5
Training loss: 1.7662780284881592
Validation loss: 2.055770536263784

Epoch: 6| Step: 6
Training loss: 2.2694544792175293
Validation loss: 2.0618642568588257

Epoch: 6| Step: 7
Training loss: 1.9947214126586914
Validation loss: 2.0478904644648233

Epoch: 6| Step: 8
Training loss: 1.9844950437545776
Validation loss: 2.0475940306981406

Epoch: 6| Step: 9
Training loss: 1.9998352527618408
Validation loss: 2.04475728670756

Epoch: 6| Step: 10
Training loss: 2.0895049571990967
Validation loss: 2.043498377005259

Epoch: 6| Step: 11
Training loss: 2.0922937393188477
Validation loss: 2.042600691318512

Epoch: 6| Step: 12
Training loss: 1.8690228462219238
Validation loss: 2.0364246368408203

Epoch: 6| Step: 13
Training loss: 2.4027562141418457
Validation loss: 2.0434266924858093

Epoch: 142| Step: 0
Training loss: 2.1702122688293457
Validation loss: 2.0477508902549744

Epoch: 6| Step: 1
Training loss: 2.173600196838379
Validation loss: 2.0368993083635965

Epoch: 6| Step: 2
Training loss: 2.523594379425049
Validation loss: 2.0500122706095376

Epoch: 6| Step: 3
Training loss: 2.3817901611328125
Validation loss: 2.0441890160242715

Epoch: 6| Step: 4
Training loss: 2.1935877799987793
Validation loss: 2.043411294619242

Epoch: 6| Step: 5
Training loss: 1.5247573852539062
Validation loss: 2.050723969936371

Epoch: 6| Step: 6
Training loss: 1.23887038230896
Validation loss: 2.0425330996513367

Epoch: 6| Step: 7
Training loss: 1.9191811084747314
Validation loss: 2.0504648288091025

Epoch: 6| Step: 8
Training loss: 1.747986078262329
Validation loss: 2.0497730374336243

Epoch: 6| Step: 9
Training loss: 1.923743724822998
Validation loss: 2.0377219518025718

Epoch: 6| Step: 10
Training loss: 1.9604592323303223
Validation loss: 2.0521688063939414

Epoch: 6| Step: 11
Training loss: 2.59005069732666
Validation loss: 2.056196928024292

Epoch: 6| Step: 12
Training loss: 2.2037854194641113
Validation loss: 2.050907870133718

Epoch: 6| Step: 13
Training loss: 1.5833792686462402
Validation loss: 2.0472359657287598

Epoch: 143| Step: 0
Training loss: 1.8019933700561523
Validation loss: 2.0492894848187766

Epoch: 6| Step: 1
Training loss: 1.9462794065475464
Validation loss: 2.050548791885376

Epoch: 6| Step: 2
Training loss: 2.5308806896209717
Validation loss: 2.0533397595087686

Epoch: 6| Step: 3
Training loss: 2.06765079498291
Validation loss: 2.05766491095225

Epoch: 6| Step: 4
Training loss: 2.2226881980895996
Validation loss: 2.0620832045873008

Epoch: 6| Step: 5
Training loss: 1.8456331491470337
Validation loss: 2.0765920281410217

Epoch: 6| Step: 6
Training loss: 2.235652208328247
Validation loss: 2.062444726626078

Epoch: 6| Step: 7
Training loss: 2.1723804473876953
Validation loss: 2.050511380036672

Epoch: 6| Step: 8
Training loss: 1.4686797857284546
Validation loss: 2.06193337837855

Epoch: 6| Step: 9
Training loss: 2.221656322479248
Validation loss: 2.0582130551338196

Epoch: 6| Step: 10
Training loss: 2.2013869285583496
Validation loss: 2.0535458525021872

Epoch: 6| Step: 11
Training loss: 1.9219996929168701
Validation loss: 2.04962291320165

Epoch: 6| Step: 12
Training loss: 1.816758394241333
Validation loss: 2.0443469484647117

Epoch: 6| Step: 13
Training loss: 1.629129409790039
Validation loss: 2.0467111070950827

Epoch: 144| Step: 0
Training loss: 2.0461955070495605
Validation loss: 2.049537420272827

Epoch: 6| Step: 1
Training loss: 1.7313926219940186
Validation loss: 2.046236256758372

Epoch: 6| Step: 2
Training loss: 1.6773700714111328
Validation loss: 2.0373521049817405

Epoch: 6| Step: 3
Training loss: 2.662677764892578
Validation loss: 2.040477474530538

Epoch: 6| Step: 4
Training loss: 1.9542968273162842
Validation loss: 2.0406391620635986

Epoch: 6| Step: 5
Training loss: 1.6513381004333496
Validation loss: 2.0547810594240823

Epoch: 6| Step: 6
Training loss: 2.837284564971924
Validation loss: 2.0478237867355347

Epoch: 6| Step: 7
Training loss: 2.1564364433288574
Validation loss: 2.0478360851605735

Epoch: 6| Step: 8
Training loss: 1.853861689567566
Validation loss: 2.061161776383718

Epoch: 6| Step: 9
Training loss: 1.781660556793213
Validation loss: 2.0646209915479026

Epoch: 6| Step: 10
Training loss: 1.9586706161499023
Validation loss: 2.0760443409283957

Epoch: 6| Step: 11
Training loss: 1.6821085214614868
Validation loss: 2.0584867795308432

Epoch: 6| Step: 12
Training loss: 1.8517446517944336
Validation loss: 2.0674565633138022

Epoch: 6| Step: 13
Training loss: 2.370453357696533
Validation loss: 2.0661357045173645

Epoch: 145| Step: 0
Training loss: 1.8390005826950073
Validation loss: 2.0657742818196616

Epoch: 6| Step: 1
Training loss: 2.1450376510620117
Validation loss: 2.0711427330970764

Epoch: 6| Step: 2
Training loss: 1.8627163171768188
Validation loss: 2.0810956160227456

Epoch: 6| Step: 3
Training loss: 2.226783275604248
Validation loss: 2.0607974330584207

Epoch: 6| Step: 4
Training loss: 1.6127573251724243
Validation loss: 2.060330947240194

Epoch: 6| Step: 5
Training loss: 2.038207530975342
Validation loss: 2.0482290387153625

Epoch: 6| Step: 6
Training loss: 1.9829113483428955
Validation loss: 2.052599847316742

Epoch: 6| Step: 7
Training loss: 1.841055989265442
Validation loss: 2.0520074168841043

Epoch: 6| Step: 8
Training loss: 1.9350496530532837
Validation loss: 2.044443507989248

Epoch: 6| Step: 9
Training loss: 1.8348991870880127
Validation loss: 2.0401247342427573

Epoch: 6| Step: 10
Training loss: 2.5981242656707764
Validation loss: 2.0433376232783

Epoch: 6| Step: 11
Training loss: 1.9941004514694214
Validation loss: 2.0457934141159058

Epoch: 6| Step: 12
Training loss: 2.2825207710266113
Validation loss: 2.0472188194592795

Epoch: 6| Step: 13
Training loss: 2.074087619781494
Validation loss: 2.0516085028648376

Epoch: 146| Step: 0
Training loss: 2.3581440448760986
Validation loss: 2.0529655814170837

Epoch: 6| Step: 1
Training loss: 2.352954864501953
Validation loss: 2.054458975791931

Epoch: 6| Step: 2
Training loss: 1.7329041957855225
Validation loss: 2.054491937160492

Epoch: 6| Step: 3
Training loss: 2.1470885276794434
Validation loss: 2.0539199908574424

Epoch: 6| Step: 4
Training loss: 1.9044969081878662
Validation loss: 2.0582675536473594

Epoch: 6| Step: 5
Training loss: 2.116628646850586
Validation loss: 2.044601082801819

Epoch: 6| Step: 6
Training loss: 1.8195689916610718
Validation loss: 2.040349781513214

Epoch: 6| Step: 7
Training loss: 1.8318355083465576
Validation loss: 2.044474482536316

Epoch: 6| Step: 8
Training loss: 1.6118204593658447
Validation loss: 2.034323036670685

Epoch: 6| Step: 9
Training loss: 1.5826455354690552
Validation loss: 2.042551577091217

Epoch: 6| Step: 10
Training loss: 2.1074328422546387
Validation loss: 2.0447251200675964

Epoch: 6| Step: 11
Training loss: 2.0702450275421143
Validation loss: 2.03802098830541

Epoch: 6| Step: 12
Training loss: 2.4034533500671387
Validation loss: 2.045695165793101

Epoch: 6| Step: 13
Training loss: 2.2171502113342285
Validation loss: 2.047830363114675

Epoch: 147| Step: 0
Training loss: 1.8864080905914307
Validation loss: 2.0532861153284707

Epoch: 6| Step: 1
Training loss: 2.3718323707580566
Validation loss: 2.051676313082377

Epoch: 6| Step: 2
Training loss: 2.4837403297424316
Validation loss: 2.0722203056017556

Epoch: 6| Step: 3
Training loss: 1.402131199836731
Validation loss: 2.074258049329122

Epoch: 6| Step: 4
Training loss: 1.9104079008102417
Validation loss: 2.072731137275696

Epoch: 6| Step: 5
Training loss: 2.0139434337615967
Validation loss: 2.0891759991645813

Epoch: 6| Step: 6
Training loss: 2.1255927085876465
Validation loss: 2.093440671761831

Epoch: 6| Step: 7
Training loss: 1.7993046045303345
Validation loss: 2.0825288891792297

Epoch: 6| Step: 8
Training loss: 2.3172965049743652
Validation loss: 2.084380785624186

Epoch: 6| Step: 9
Training loss: 1.9893075227737427
Validation loss: 2.0874675114949546

Epoch: 6| Step: 10
Training loss: 2.0687646865844727
Validation loss: 2.0782911578814187

Epoch: 6| Step: 11
Training loss: 1.5368499755859375
Validation loss: 2.067956825097402

Epoch: 6| Step: 12
Training loss: 2.108964443206787
Validation loss: 2.0577606558799744

Epoch: 6| Step: 13
Training loss: 2.482741355895996
Validation loss: 2.0464837551116943

Epoch: 148| Step: 0
Training loss: 2.059718608856201
Validation loss: 2.0408228834470115

Epoch: 6| Step: 1
Training loss: 2.1164045333862305
Validation loss: 2.0350876450538635

Epoch: 6| Step: 2
Training loss: 1.865119457244873
Validation loss: 2.038391570250193

Epoch: 6| Step: 3
Training loss: 1.7726175785064697
Validation loss: 2.0385830203692117

Epoch: 6| Step: 4
Training loss: 2.2301645278930664
Validation loss: 2.045085350672404

Epoch: 6| Step: 5
Training loss: 1.3855834007263184
Validation loss: 2.044788201649984

Epoch: 6| Step: 6
Training loss: 2.665675163269043
Validation loss: 2.0532431602478027

Epoch: 6| Step: 7
Training loss: 2.3601231575012207
Validation loss: 2.0464154481887817

Epoch: 6| Step: 8
Training loss: 1.925966501235962
Validation loss: 2.050698479016622

Epoch: 6| Step: 9
Training loss: 2.562567949295044
Validation loss: 2.0618231495221457

Epoch: 6| Step: 10
Training loss: 1.9890053272247314
Validation loss: 2.0513827204704285

Epoch: 6| Step: 11
Training loss: 1.633630633354187
Validation loss: 2.0598902106285095

Epoch: 6| Step: 12
Training loss: 1.5746254920959473
Validation loss: 2.0622634490331015

Epoch: 6| Step: 13
Training loss: 2.1878180503845215
Validation loss: 2.074771742026011

Epoch: 149| Step: 0
Training loss: 2.340114116668701
Validation loss: 2.068497816721598

Epoch: 6| Step: 1
Training loss: 2.216721773147583
Validation loss: 2.0633397301038108

Epoch: 6| Step: 2
Training loss: 1.8816485404968262
Validation loss: 2.0622820258140564

Epoch: 6| Step: 3
Training loss: 2.0247931480407715
Validation loss: 2.0592834750811257

Epoch: 6| Step: 4
Training loss: 2.527284622192383
Validation loss: 2.0523664355278015

Epoch: 6| Step: 5
Training loss: 1.3597798347473145
Validation loss: 2.0495266914367676

Epoch: 6| Step: 6
Training loss: 1.5035878419876099
Validation loss: 2.04308021068573

Epoch: 6| Step: 7
Training loss: 2.063027858734131
Validation loss: 2.04618231455485

Epoch: 6| Step: 8
Training loss: 2.145622491836548
Validation loss: 2.0447559555371604

Epoch: 6| Step: 9
Training loss: 2.4647440910339355
Validation loss: 2.0622604290644326

Epoch: 6| Step: 10
Training loss: 1.819846749305725
Validation loss: 2.0659126440684

Epoch: 6| Step: 11
Training loss: 2.2985992431640625
Validation loss: 2.089376469453176

Epoch: 6| Step: 12
Training loss: 1.715971827507019
Validation loss: 2.0880210200945535

Epoch: 6| Step: 13
Training loss: 2.0034079551696777
Validation loss: 2.088101089000702

Epoch: 150| Step: 0
Training loss: 2.6697425842285156
Validation loss: 2.0925267736117044

Epoch: 6| Step: 1
Training loss: 1.4438598155975342
Validation loss: 2.0814454555511475

Epoch: 6| Step: 2
Training loss: 1.6306918859481812
Validation loss: 2.074242949485779

Epoch: 6| Step: 3
Training loss: 1.3167240619659424
Validation loss: 2.069050967693329

Epoch: 6| Step: 4
Training loss: 1.9638152122497559
Validation loss: 2.0622368454933167

Epoch: 6| Step: 5
Training loss: 2.18379545211792
Validation loss: 2.069877545038859

Epoch: 6| Step: 6
Training loss: 2.64996075630188
Validation loss: 2.0673481027285256

Epoch: 6| Step: 7
Training loss: 1.8230812549591064
Validation loss: 2.0592982371648154

Epoch: 6| Step: 8
Training loss: 2.3627917766571045
Validation loss: 2.0579462250073752

Epoch: 6| Step: 9
Training loss: 1.7639998197555542
Validation loss: 2.067477742830912

Epoch: 6| Step: 10
Training loss: 1.9606808423995972
Validation loss: 2.0676044623057046

Epoch: 6| Step: 11
Training loss: 1.98088538646698
Validation loss: 2.055505633354187

Epoch: 6| Step: 12
Training loss: 2.082634925842285
Validation loss: 2.059163053830465

Epoch: 6| Step: 13
Training loss: 2.0125415325164795
Validation loss: 2.0563672184944153

Epoch: 151| Step: 0
Training loss: 2.349072217941284
Validation loss: 2.04259059826533

Epoch: 6| Step: 1
Training loss: 2.4422550201416016
Validation loss: 2.052581012248993

Epoch: 6| Step: 2
Training loss: 1.8579970598220825
Validation loss: 2.0559571385383606

Epoch: 6| Step: 3
Training loss: 2.0112392902374268
Validation loss: 2.0568328897158303

Epoch: 6| Step: 4
Training loss: 2.156848669052124
Validation loss: 2.060287435849508

Epoch: 6| Step: 5
Training loss: 2.5554299354553223
Validation loss: 2.0612906018892923

Epoch: 6| Step: 6
Training loss: 1.4653009176254272
Validation loss: 2.0591976046562195

Epoch: 6| Step: 7
Training loss: 2.071441650390625
Validation loss: 2.0606723626454673

Epoch: 6| Step: 8
Training loss: 1.6829184293746948
Validation loss: 2.060125211874644

Epoch: 6| Step: 9
Training loss: 1.4846396446228027
Validation loss: 2.064606467882792

Epoch: 6| Step: 10
Training loss: 1.969538688659668
Validation loss: 2.0573481917381287

Epoch: 6| Step: 11
Training loss: 1.8911360502243042
Validation loss: 2.0404356916745505

Epoch: 6| Step: 12
Training loss: 2.216508626937866
Validation loss: 2.0528560082117715

Epoch: 6| Step: 13
Training loss: 1.7309198379516602
Validation loss: 2.0499767065048218

Epoch: 152| Step: 0
Training loss: 2.733030319213867
Validation loss: 2.0505738059679666

Epoch: 6| Step: 1
Training loss: 1.7136942148208618
Validation loss: 2.062976360321045

Epoch: 6| Step: 2
Training loss: 2.101909637451172
Validation loss: 2.062456468741099

Epoch: 6| Step: 3
Training loss: 1.8953598737716675
Validation loss: 2.067172427972158

Epoch: 6| Step: 4
Training loss: 1.592543601989746
Validation loss: 2.0645529429117837

Epoch: 6| Step: 5
Training loss: 1.9959534406661987
Validation loss: 2.0850537021954856

Epoch: 6| Step: 6
Training loss: 1.9048707485198975
Validation loss: 2.0649113059043884

Epoch: 6| Step: 7
Training loss: 2.1053080558776855
Validation loss: 2.0729708671569824

Epoch: 6| Step: 8
Training loss: 2.0259363651275635
Validation loss: 2.0845702290534973

Epoch: 6| Step: 9
Training loss: 2.585084915161133
Validation loss: 2.070898175239563

Epoch: 6| Step: 10
Training loss: 1.8635367155075073
Validation loss: 2.0720253388086953

Epoch: 6| Step: 11
Training loss: 1.954725742340088
Validation loss: 2.0748870770136514

Epoch: 6| Step: 12
Training loss: 2.109865665435791
Validation loss: 2.057237525780996

Epoch: 6| Step: 13
Training loss: 1.4012120962142944
Validation loss: 2.056036114692688

Epoch: 153| Step: 0
Training loss: 1.871263861656189
Validation loss: 2.0636706352233887

Epoch: 6| Step: 1
Training loss: 1.5031932592391968
Validation loss: 2.0710200866063437

Epoch: 6| Step: 2
Training loss: 1.8566656112670898
Validation loss: 2.0527970592180886

Epoch: 6| Step: 3
Training loss: 1.9556968212127686
Validation loss: 2.0625824332237244

Epoch: 6| Step: 4
Training loss: 1.4037373065948486
Validation loss: 2.0601956049601235

Epoch: 6| Step: 5
Training loss: 2.1985924243927
Validation loss: 2.0521897077560425

Epoch: 6| Step: 6
Training loss: 1.517486333847046
Validation loss: 2.0623236099878945

Epoch: 6| Step: 7
Training loss: 2.5841329097747803
Validation loss: 2.0593692660331726

Epoch: 6| Step: 8
Training loss: 1.8696579933166504
Validation loss: 2.0633781949679055

Epoch: 6| Step: 9
Training loss: 2.0871784687042236
Validation loss: 2.0737502773602805

Epoch: 6| Step: 10
Training loss: 2.508786916732788
Validation loss: 2.0697542826334634

Epoch: 6| Step: 11
Training loss: 2.1856749057769775
Validation loss: 2.0502437353134155

Epoch: 6| Step: 12
Training loss: 2.4713196754455566
Validation loss: 2.0765963792800903

Epoch: 6| Step: 13
Training loss: 1.8993465900421143
Validation loss: 2.0742197831471763

Epoch: 154| Step: 0
Training loss: 1.7116589546203613
Validation loss: 2.0689616401990256

Epoch: 6| Step: 1
Training loss: 1.8704605102539062
Validation loss: 2.0649511218070984

Epoch: 6| Step: 2
Training loss: 2.2612593173980713
Validation loss: 2.0700371066729226

Epoch: 6| Step: 3
Training loss: 1.3567620515823364
Validation loss: 2.092019518216451

Epoch: 6| Step: 4
Training loss: 2.417768955230713
Validation loss: 2.0880138079325357

Epoch: 6| Step: 5
Training loss: 1.870401382446289
Validation loss: 2.072912017504374

Epoch: 6| Step: 6
Training loss: 2.2702383995056152
Validation loss: 2.07022492090861

Epoch: 6| Step: 7
Training loss: 1.8433527946472168
Validation loss: 2.072350641091665

Epoch: 6| Step: 8
Training loss: 2.0640974044799805
Validation loss: 2.0623355706532798

Epoch: 6| Step: 9
Training loss: 2.2781295776367188
Validation loss: 2.062975823879242

Epoch: 6| Step: 10
Training loss: 2.4708902835845947
Validation loss: 2.0647207498550415

Epoch: 6| Step: 11
Training loss: 2.136134147644043
Validation loss: 2.05534690618515

Epoch: 6| Step: 12
Training loss: 1.4766179323196411
Validation loss: 2.05525149901708

Epoch: 6| Step: 13
Training loss: 1.925180196762085
Validation loss: 2.0486655235290527

Epoch: 155| Step: 0
Training loss: 2.388732433319092
Validation loss: 2.043803075949351

Epoch: 6| Step: 1
Training loss: 2.416346549987793
Validation loss: 2.053517699241638

Epoch: 6| Step: 2
Training loss: 2.1642191410064697
Validation loss: 2.0603849291801453

Epoch: 6| Step: 3
Training loss: 1.921338438987732
Validation loss: 2.053979972998301

Epoch: 6| Step: 4
Training loss: 2.0457329750061035
Validation loss: 2.065556287765503

Epoch: 6| Step: 5
Training loss: 1.4445583820343018
Validation loss: 2.070202191670736

Epoch: 6| Step: 6
Training loss: 2.281576633453369
Validation loss: 2.0925902724266052

Epoch: 6| Step: 7
Training loss: 2.5676064491271973
Validation loss: 2.0943533778190613

Epoch: 6| Step: 8
Training loss: 1.6648945808410645
Validation loss: 2.10863467057546

Epoch: 6| Step: 9
Training loss: 2.0181350708007812
Validation loss: 2.107620278994242

Epoch: 6| Step: 10
Training loss: 1.4849050045013428
Validation loss: 2.1140641371409097

Epoch: 6| Step: 11
Training loss: 1.4113826751708984
Validation loss: 2.0985857049624124

Epoch: 6| Step: 12
Training loss: 2.2185628414154053
Validation loss: 2.0892734924952188

Epoch: 6| Step: 13
Training loss: 1.9991416931152344
Validation loss: 2.0940310756365457

Epoch: 156| Step: 0
Training loss: 1.7511143684387207
Validation loss: 2.081791122754415

Epoch: 6| Step: 1
Training loss: 2.3886160850524902
Validation loss: 2.07250181833903

Epoch: 6| Step: 2
Training loss: 1.265708327293396
Validation loss: 2.075677295525869

Epoch: 6| Step: 3
Training loss: 2.242004871368408
Validation loss: 2.0632875164349875

Epoch: 6| Step: 4
Training loss: 2.0799708366394043
Validation loss: 2.063659151395162

Epoch: 6| Step: 5
Training loss: 1.7693703174591064
Validation loss: 2.050240953763326

Epoch: 6| Step: 6
Training loss: 1.9393234252929688
Validation loss: 2.064904431502024

Epoch: 6| Step: 7
Training loss: 1.4398233890533447
Validation loss: 2.0695037643114724

Epoch: 6| Step: 8
Training loss: 2.5857369899749756
Validation loss: 2.0622900327046714

Epoch: 6| Step: 9
Training loss: 1.5434632301330566
Validation loss: 2.066721260547638

Epoch: 6| Step: 10
Training loss: 2.402191400527954
Validation loss: 2.0606440901756287

Epoch: 6| Step: 11
Training loss: 1.8355348110198975
Validation loss: 2.072215437889099

Epoch: 6| Step: 12
Training loss: 2.634514093399048
Validation loss: 2.0664827624956765

Epoch: 6| Step: 13
Training loss: 2.017233371734619
Validation loss: 2.0851668318112693

Epoch: 157| Step: 0
Training loss: 2.0800833702087402
Validation loss: 2.080493708451589

Epoch: 6| Step: 1
Training loss: 2.1442699432373047
Validation loss: 2.0942020614941916

Epoch: 6| Step: 2
Training loss: 1.897278070449829
Validation loss: 2.1091917355855307

Epoch: 6| Step: 3
Training loss: 1.6293575763702393
Validation loss: 2.127073129018148

Epoch: 6| Step: 4
Training loss: 2.082357406616211
Validation loss: 2.114667296409607

Epoch: 6| Step: 5
Training loss: 2.139575481414795
Validation loss: 2.104703366756439

Epoch: 6| Step: 6
Training loss: 1.8080869913101196
Validation loss: 2.081856290499369

Epoch: 6| Step: 7
Training loss: 1.7404299974441528
Validation loss: 2.071081002553304

Epoch: 6| Step: 8
Training loss: 2.2252142429351807
Validation loss: 2.0778825283050537

Epoch: 6| Step: 9
Training loss: 1.7388691902160645
Validation loss: 2.0642435948053994

Epoch: 6| Step: 10
Training loss: 1.6654143333435059
Validation loss: 2.0687705477078757

Epoch: 6| Step: 11
Training loss: 1.9225616455078125
Validation loss: 2.061776955922445

Epoch: 6| Step: 12
Training loss: 2.4110889434814453
Validation loss: 2.054275115331014

Epoch: 6| Step: 13
Training loss: 2.702970504760742
Validation loss: 2.0482061306635537

Epoch: 158| Step: 0
Training loss: 2.159393072128296
Validation loss: 2.0541309912999473

Epoch: 6| Step: 1
Training loss: 1.7546131610870361
Validation loss: 2.0464943051338196

Epoch: 6| Step: 2
Training loss: 2.181900978088379
Validation loss: 2.048050900300344

Epoch: 6| Step: 3
Training loss: 2.062811851501465
Validation loss: 2.0526989102363586

Epoch: 6| Step: 4
Training loss: 1.8814482688903809
Validation loss: 2.049348791440328

Epoch: 6| Step: 5
Training loss: 1.6887012720108032
Validation loss: 2.0684165954589844

Epoch: 6| Step: 6
Training loss: 1.5330450534820557
Validation loss: 2.086561938126882

Epoch: 6| Step: 7
Training loss: 1.4881389141082764
Validation loss: 2.095181107521057

Epoch: 6| Step: 8
Training loss: 2.1620514392852783
Validation loss: 2.112803280353546

Epoch: 6| Step: 9
Training loss: 1.8351281881332397
Validation loss: 2.0968835751215615

Epoch: 6| Step: 10
Training loss: 1.7316409349441528
Validation loss: 2.1186476945877075

Epoch: 6| Step: 11
Training loss: 2.5380499362945557
Validation loss: 2.0988591512044272

Epoch: 6| Step: 12
Training loss: 2.410278081893921
Validation loss: 2.087908903757731

Epoch: 6| Step: 13
Training loss: 2.59409236907959
Validation loss: 2.0907206535339355

Epoch: 159| Step: 0
Training loss: 2.264665126800537
Validation loss: 2.0710142453511557

Epoch: 6| Step: 1
Training loss: 2.014901876449585
Validation loss: 2.0743595957756042

Epoch: 6| Step: 2
Training loss: 1.6397032737731934
Validation loss: 2.0609646836916604

Epoch: 6| Step: 3
Training loss: 2.110384464263916
Validation loss: 2.0598174730936685

Epoch: 6| Step: 4
Training loss: 1.2683181762695312
Validation loss: 2.0656901001930237

Epoch: 6| Step: 5
Training loss: 2.4756431579589844
Validation loss: 2.0570939977963767

Epoch: 6| Step: 6
Training loss: 1.8084831237792969
Validation loss: 2.0545023481051126

Epoch: 6| Step: 7
Training loss: 1.8277122974395752
Validation loss: 2.059584379196167

Epoch: 6| Step: 8
Training loss: 2.147491931915283
Validation loss: 2.0587929288546243

Epoch: 6| Step: 9
Training loss: 1.7687537670135498
Validation loss: 2.057511587937673

Epoch: 6| Step: 10
Training loss: 1.6925123929977417
Validation loss: 2.058348596096039

Epoch: 6| Step: 11
Training loss: 2.258268117904663
Validation loss: 2.0740352471669516

Epoch: 6| Step: 12
Training loss: 2.539119243621826
Validation loss: 2.062139372030894

Epoch: 6| Step: 13
Training loss: 1.8401386737823486
Validation loss: 2.066181182861328

Epoch: 160| Step: 0
Training loss: 1.7941453456878662
Validation loss: 2.064385632673899

Epoch: 6| Step: 1
Training loss: 1.3932933807373047
Validation loss: 2.058014710744222

Epoch: 6| Step: 2
Training loss: 1.7330946922302246
Validation loss: 2.0589508016904197

Epoch: 6| Step: 3
Training loss: 1.779653787612915
Validation loss: 2.063755671183268

Epoch: 6| Step: 4
Training loss: 1.8032891750335693
Validation loss: 2.0591869155565896

Epoch: 6| Step: 5
Training loss: 1.9191386699676514
Validation loss: 2.0711172620455423

Epoch: 6| Step: 6
Training loss: 2.3000898361206055
Validation loss: 2.068085571130117

Epoch: 6| Step: 7
Training loss: 2.5896244049072266
Validation loss: 2.0637174050013223

Epoch: 6| Step: 8
Training loss: 2.100466728210449
Validation loss: 2.059060494105021

Epoch: 6| Step: 9
Training loss: 2.293349266052246
Validation loss: 2.0602468053499856

Epoch: 6| Step: 10
Training loss: 1.8029685020446777
Validation loss: 2.046109159787496

Epoch: 6| Step: 11
Training loss: 2.3366150856018066
Validation loss: 2.070028384526571

Epoch: 6| Step: 12
Training loss: 1.6419739723205566
Validation loss: 2.05496213833491

Epoch: 6| Step: 13
Training loss: 2.1173253059387207
Validation loss: 2.0717706282933555

Epoch: 161| Step: 0
Training loss: 2.9493234157562256
Validation loss: 2.0655548771222434

Epoch: 6| Step: 1
Training loss: 1.5587443113327026
Validation loss: 2.0725990533828735

Epoch: 6| Step: 2
Training loss: 2.6843225955963135
Validation loss: 2.067267100016276

Epoch: 6| Step: 3
Training loss: 2.089738607406616
Validation loss: 2.0783151189486184

Epoch: 6| Step: 4
Training loss: 2.2286603450775146
Validation loss: 2.0732800364494324

Epoch: 6| Step: 5
Training loss: 2.2214784622192383
Validation loss: 2.0849780639012656

Epoch: 6| Step: 6
Training loss: 1.3358020782470703
Validation loss: 2.0770984490712485

Epoch: 6| Step: 7
Training loss: 1.30869460105896
Validation loss: 2.0737255414326987

Epoch: 6| Step: 8
Training loss: 1.499211311340332
Validation loss: 2.066562990347544

Epoch: 6| Step: 9
Training loss: 2.229433536529541
Validation loss: 2.0850388606389365

Epoch: 6| Step: 10
Training loss: 1.5706762075424194
Validation loss: 2.0664365688959756

Epoch: 6| Step: 11
Training loss: 2.095175266265869
Validation loss: 2.059773802757263

Epoch: 6| Step: 12
Training loss: 2.160200357437134
Validation loss: 2.0546183983484902

Epoch: 6| Step: 13
Training loss: 2.089829921722412
Validation loss: 2.065069794654846

Epoch: 162| Step: 0
Training loss: 1.88211190700531
Validation loss: 2.0602598190307617

Epoch: 6| Step: 1
Training loss: 2.3270020484924316
Validation loss: 2.0551583568255105

Epoch: 6| Step: 2
Training loss: 1.6110845804214478
Validation loss: 2.064507861932119

Epoch: 6| Step: 3
Training loss: 1.746012568473816
Validation loss: 2.062684257825216

Epoch: 6| Step: 4
Training loss: 2.235607624053955
Validation loss: 2.0635031859079995

Epoch: 6| Step: 5
Training loss: 1.5618782043457031
Validation loss: 2.0679962436358132

Epoch: 6| Step: 6
Training loss: 2.50974178314209
Validation loss: 2.0642941196759543

Epoch: 6| Step: 7
Training loss: 1.9642986059188843
Validation loss: 2.072088917096456

Epoch: 6| Step: 8
Training loss: 1.96396803855896
Validation loss: 2.0862202843030295

Epoch: 6| Step: 9
Training loss: 2.3881640434265137
Validation loss: 2.0977379282315574

Epoch: 6| Step: 10
Training loss: 2.193972587585449
Validation loss: 2.1008418798446655

Epoch: 6| Step: 11
Training loss: 2.3819921016693115
Validation loss: 2.109911104043325

Epoch: 6| Step: 12
Training loss: 1.5498498678207397
Validation loss: 2.111476262410482

Epoch: 6| Step: 13
Training loss: 1.4972928762435913
Validation loss: 2.115135649840037

Epoch: 163| Step: 0
Training loss: 2.0964980125427246
Validation loss: 2.1086475451787314

Epoch: 6| Step: 1
Training loss: 1.761816382408142
Validation loss: 2.1158992449442544

Epoch: 6| Step: 2
Training loss: 1.9127347469329834
Validation loss: 2.089108427365621

Epoch: 6| Step: 3
Training loss: 2.5514488220214844
Validation loss: 2.0811097224553428

Epoch: 6| Step: 4
Training loss: 2.2491607666015625
Validation loss: 2.0634364088376365

Epoch: 6| Step: 5
Training loss: 1.584798812866211
Validation loss: 2.060657878716787

Epoch: 6| Step: 6
Training loss: 2.0973124504089355
Validation loss: 2.0646813114484153

Epoch: 6| Step: 7
Training loss: 2.3202567100524902
Validation loss: 2.0591925581296286

Epoch: 6| Step: 8
Training loss: 1.793449878692627
Validation loss: 2.06433375676473

Epoch: 6| Step: 9
Training loss: 1.7502145767211914
Validation loss: 2.0695399045944214

Epoch: 6| Step: 10
Training loss: 1.9912683963775635
Validation loss: 2.069529374440511

Epoch: 6| Step: 11
Training loss: 2.380349636077881
Validation loss: 2.074577589829763

Epoch: 6| Step: 12
Training loss: 1.9880549907684326
Validation loss: 2.077511111895243

Epoch: 6| Step: 13
Training loss: 1.8655885457992554
Validation loss: 2.0812268455823264

Epoch: 164| Step: 0
Training loss: 1.2741825580596924
Validation loss: 2.0793012380599976

Epoch: 6| Step: 1
Training loss: 1.7485649585723877
Validation loss: 2.0762003461519876

Epoch: 6| Step: 2
Training loss: 2.2018580436706543
Validation loss: 2.0998060504595437

Epoch: 6| Step: 3
Training loss: 1.6144928932189941
Validation loss: 2.0806102752685547

Epoch: 6| Step: 4
Training loss: 1.7357308864593506
Validation loss: 2.0848357677459717

Epoch: 6| Step: 5
Training loss: 2.7772836685180664
Validation loss: 2.0880016883214316

Epoch: 6| Step: 6
Training loss: 1.7571967840194702
Validation loss: 2.0953218936920166

Epoch: 6| Step: 7
Training loss: 1.6193104982376099
Validation loss: 2.1068787773450217

Epoch: 6| Step: 8
Training loss: 2.0470187664031982
Validation loss: 2.0921614368756614

Epoch: 6| Step: 9
Training loss: 2.1958580017089844
Validation loss: 2.107276141643524

Epoch: 6| Step: 10
Training loss: 2.7555243968963623
Validation loss: 2.0883768796920776

Epoch: 6| Step: 11
Training loss: 2.3043746948242188
Validation loss: 2.0809368888537088

Epoch: 6| Step: 12
Training loss: 2.0865273475646973
Validation loss: 2.0772194067637124

Epoch: 6| Step: 13
Training loss: 1.589861273765564
Validation loss: 2.069487432638804

Epoch: 165| Step: 0
Training loss: 1.7401902675628662
Validation loss: 2.0687698125839233

Epoch: 6| Step: 1
Training loss: 1.797109603881836
Validation loss: 2.0728716254234314

Epoch: 6| Step: 2
Training loss: 1.7465596199035645
Validation loss: 2.0732120275497437

Epoch: 6| Step: 3
Training loss: 2.094343662261963
Validation loss: 2.072334627310435

Epoch: 6| Step: 4
Training loss: 2.2385454177856445
Validation loss: 2.070915917555491

Epoch: 6| Step: 5
Training loss: 2.742356061935425
Validation loss: 2.0729565620422363

Epoch: 6| Step: 6
Training loss: 2.120957612991333
Validation loss: 2.0764986276626587

Epoch: 6| Step: 7
Training loss: 1.5767662525177002
Validation loss: 2.0871676206588745

Epoch: 6| Step: 8
Training loss: 1.7690763473510742
Validation loss: 2.078347464402517

Epoch: 6| Step: 9
Training loss: 2.118229866027832
Validation loss: 2.098164916038513

Epoch: 6| Step: 10
Training loss: 1.8247652053833008
Validation loss: 2.102803905804952

Epoch: 6| Step: 11
Training loss: 2.1734800338745117
Validation loss: 2.0909120440483093

Epoch: 6| Step: 12
Training loss: 1.590686321258545
Validation loss: 2.0908620953559875

Epoch: 6| Step: 13
Training loss: 1.9062564373016357
Validation loss: 2.0989678303400674

Epoch: 166| Step: 0
Training loss: 1.681732416152954
Validation loss: 2.094404856363932

Epoch: 6| Step: 1
Training loss: 1.8400437831878662
Validation loss: 2.096772253513336

Epoch: 6| Step: 2
Training loss: 2.66851544380188
Validation loss: 2.0936057964960733

Epoch: 6| Step: 3
Training loss: 2.0118541717529297
Validation loss: 2.0975101788838706

Epoch: 6| Step: 4
Training loss: 2.056705951690674
Validation loss: 2.088763097922007

Epoch: 6| Step: 5
Training loss: 1.6580681800842285
Validation loss: 2.0890621542930603

Epoch: 6| Step: 6
Training loss: 1.4409449100494385
Validation loss: 2.0757447481155396

Epoch: 6| Step: 7
Training loss: 1.9720048904418945
Validation loss: 2.0821313858032227

Epoch: 6| Step: 8
Training loss: 2.0976343154907227
Validation loss: 2.0775027871131897

Epoch: 6| Step: 9
Training loss: 2.31150484085083
Validation loss: 2.085371732711792

Epoch: 6| Step: 10
Training loss: 2.3178577423095703
Validation loss: 2.0826127330462136

Epoch: 6| Step: 11
Training loss: 1.8742661476135254
Validation loss: 2.0713632901509604

Epoch: 6| Step: 12
Training loss: 1.690779209136963
Validation loss: 2.091564655303955

Epoch: 6| Step: 13
Training loss: 1.6291327476501465
Validation loss: 2.0949417551358542

Epoch: 167| Step: 0
Training loss: 1.5864325761795044
Validation loss: 2.105566620826721

Epoch: 6| Step: 1
Training loss: 1.6726875305175781
Validation loss: 2.1022652983665466

Epoch: 6| Step: 2
Training loss: 2.172159194946289
Validation loss: 2.1104470094045005

Epoch: 6| Step: 3
Training loss: 2.155583381652832
Validation loss: 2.1040947635968528

Epoch: 6| Step: 4
Training loss: 1.7096645832061768
Validation loss: 2.1074017882347107

Epoch: 6| Step: 5
Training loss: 2.610844135284424
Validation loss: 2.080075820287069

Epoch: 6| Step: 6
Training loss: 2.1744539737701416
Validation loss: 2.0785460074742637

Epoch: 6| Step: 7
Training loss: 2.0989065170288086
Validation loss: 2.074975391228994

Epoch: 6| Step: 8
Training loss: 1.3210088014602661
Validation loss: 2.0720937252044678

Epoch: 6| Step: 9
Training loss: 2.4861648082733154
Validation loss: 2.0697955886522927

Epoch: 6| Step: 10
Training loss: 1.9349985122680664
Validation loss: 2.076226751009623

Epoch: 6| Step: 11
Training loss: 1.8493098020553589
Validation loss: 2.08284193277359

Epoch: 6| Step: 12
Training loss: 1.7321715354919434
Validation loss: 2.0892316500345864

Epoch: 6| Step: 13
Training loss: 2.063967227935791
Validation loss: 2.1003530820210776

Epoch: 168| Step: 0
Training loss: 1.9622085094451904
Validation loss: 2.1024869680404663

Epoch: 6| Step: 1
Training loss: 2.1598992347717285
Validation loss: 2.1022920409838357

Epoch: 6| Step: 2
Training loss: 2.13273549079895
Validation loss: 2.096347769101461

Epoch: 6| Step: 3
Training loss: 2.4432015419006348
Validation loss: 2.09328685204188

Epoch: 6| Step: 4
Training loss: 1.7146083116531372
Validation loss: 2.092850605646769

Epoch: 6| Step: 5
Training loss: 2.0598504543304443
Validation loss: 2.1131298343340554

Epoch: 6| Step: 6
Training loss: 1.5945689678192139
Validation loss: 2.1122880776723227

Epoch: 6| Step: 7
Training loss: 1.674649953842163
Validation loss: 2.101488411426544

Epoch: 6| Step: 8
Training loss: 1.664006233215332
Validation loss: 2.098958432674408

Epoch: 6| Step: 9
Training loss: 2.3691155910491943
Validation loss: 2.091127177079519

Epoch: 6| Step: 10
Training loss: 2.1243135929107666
Validation loss: 2.082755664984385

Epoch: 6| Step: 11
Training loss: 2.24898362159729
Validation loss: 2.0799715916315713

Epoch: 6| Step: 12
Training loss: 1.629821538925171
Validation loss: 2.0789198080698648

Epoch: 6| Step: 13
Training loss: 1.559759259223938
Validation loss: 2.075290242830912

Epoch: 169| Step: 0
Training loss: 2.3486392498016357
Validation loss: 2.07764861981074

Epoch: 6| Step: 1
Training loss: 1.727297067642212
Validation loss: 2.074371119340261

Epoch: 6| Step: 2
Training loss: 2.115004777908325
Validation loss: 2.0765056212743125

Epoch: 6| Step: 3
Training loss: 1.7741472721099854
Validation loss: 2.0710548162460327

Epoch: 6| Step: 4
Training loss: 1.5995548963546753
Validation loss: 2.0862426360448203

Epoch: 6| Step: 5
Training loss: 2.193103790283203
Validation loss: 2.0895862778027854

Epoch: 6| Step: 6
Training loss: 2.4047818183898926
Validation loss: 2.091494003931681

Epoch: 6| Step: 7
Training loss: 1.8788716793060303
Validation loss: 2.0951381723086038

Epoch: 6| Step: 8
Training loss: 1.6744335889816284
Validation loss: 2.095767915248871

Epoch: 6| Step: 9
Training loss: 1.76335608959198
Validation loss: 2.079854726791382

Epoch: 6| Step: 10
Training loss: 1.822731614112854
Validation loss: 2.091085116068522

Epoch: 6| Step: 11
Training loss: 2.100660562515259
Validation loss: 2.0739484230677285

Epoch: 6| Step: 12
Training loss: 1.8986587524414062
Validation loss: 2.070485273996989

Epoch: 6| Step: 13
Training loss: 1.976379156112671
Validation loss: 2.066828469435374

Epoch: 170| Step: 0
Training loss: 1.8647453784942627
Validation loss: 2.0760331948598227

Epoch: 6| Step: 1
Training loss: 1.6552151441574097
Validation loss: 2.0638391375541687

Epoch: 6| Step: 2
Training loss: 2.0792503356933594
Validation loss: 2.0683534741401672

Epoch: 6| Step: 3
Training loss: 2.2286219596862793
Validation loss: 2.081103722254435

Epoch: 6| Step: 4
Training loss: 1.6251873970031738
Validation loss: 2.08632226785024

Epoch: 6| Step: 5
Training loss: 1.5911972522735596
Validation loss: 2.110728899637858

Epoch: 6| Step: 6
Training loss: 2.409053325653076
Validation loss: 2.1049638589223227

Epoch: 6| Step: 7
Training loss: 1.9133305549621582
Validation loss: 2.0976858139038086

Epoch: 6| Step: 8
Training loss: 2.7608907222747803
Validation loss: 2.115933338801066

Epoch: 6| Step: 9
Training loss: 1.9237778186798096
Validation loss: 2.1026702721913657

Epoch: 6| Step: 10
Training loss: 1.3948824405670166
Validation loss: 2.106186807155609

Epoch: 6| Step: 11
Training loss: 1.4043281078338623
Validation loss: 2.096688667933146

Epoch: 6| Step: 12
Training loss: 1.8745503425598145
Validation loss: 2.1101486682891846

Epoch: 6| Step: 13
Training loss: 2.5911967754364014
Validation loss: 2.114821473757426

Epoch: 171| Step: 0
Training loss: 1.8309787511825562
Validation loss: 2.1016523241996765

Epoch: 6| Step: 1
Training loss: 1.7943013906478882
Validation loss: 2.112425446510315

Epoch: 6| Step: 2
Training loss: 2.0426394939422607
Validation loss: 2.113277276357015

Epoch: 6| Step: 3
Training loss: 1.8758435249328613
Validation loss: 2.119455178578695

Epoch: 6| Step: 4
Training loss: 1.6798332929611206
Validation loss: 2.1186401645342507

Epoch: 6| Step: 5
Training loss: 2.167463779449463
Validation loss: 2.1031619707743325

Epoch: 6| Step: 6
Training loss: 2.046306610107422
Validation loss: 2.112843096256256

Epoch: 6| Step: 7
Training loss: 2.430755615234375
Validation loss: 2.0937856435775757

Epoch: 6| Step: 8
Training loss: 2.1227028369903564
Validation loss: 2.100647787253062

Epoch: 6| Step: 9
Training loss: 1.6294615268707275
Validation loss: 2.1037684281667075

Epoch: 6| Step: 10
Training loss: 1.6106908321380615
Validation loss: 2.0823761423428855

Epoch: 6| Step: 11
Training loss: 2.4339888095855713
Validation loss: 2.08543727795283

Epoch: 6| Step: 12
Training loss: 1.4657403230667114
Validation loss: 2.1121253768603006

Epoch: 6| Step: 13
Training loss: 1.8872463703155518
Validation loss: 2.109987994035085

Epoch: 172| Step: 0
Training loss: 2.063065767288208
Validation loss: 2.107553402582804

Epoch: 6| Step: 1
Training loss: 1.7220079898834229
Validation loss: 2.1281246741612754

Epoch: 6| Step: 2
Training loss: 1.3583472967147827
Validation loss: 2.127254327138265

Epoch: 6| Step: 3
Training loss: 2.067762851715088
Validation loss: 2.136038144429525

Epoch: 6| Step: 4
Training loss: 1.624518871307373
Validation loss: 2.119171937306722

Epoch: 6| Step: 5
Training loss: 1.5055108070373535
Validation loss: 2.131809870402018

Epoch: 6| Step: 6
Training loss: 1.5516732931137085
Validation loss: 2.132598618666331

Epoch: 6| Step: 7
Training loss: 2.1327104568481445
Validation loss: 2.1223708987236023

Epoch: 6| Step: 8
Training loss: 2.021390199661255
Validation loss: 2.1054131587346396

Epoch: 6| Step: 9
Training loss: 2.069797992706299
Validation loss: 2.1087918281555176

Epoch: 6| Step: 10
Training loss: 2.3536834716796875
Validation loss: 2.1214311917622886

Epoch: 6| Step: 11
Training loss: 2.3800854682922363
Validation loss: 2.124567667643229

Epoch: 6| Step: 12
Training loss: 2.0329761505126953
Validation loss: 2.111999253431956

Epoch: 6| Step: 13
Training loss: 2.2266998291015625
Validation loss: 2.129997173945109

Epoch: 173| Step: 0
Training loss: 2.0222907066345215
Validation loss: 2.123712102572123

Epoch: 6| Step: 1
Training loss: 1.9403042793273926
Validation loss: 2.135265350341797

Epoch: 6| Step: 2
Training loss: 2.4090123176574707
Validation loss: 2.147440572579702

Epoch: 6| Step: 3
Training loss: 2.2738959789276123
Validation loss: 2.1265518267949424

Epoch: 6| Step: 4
Training loss: 1.795979619026184
Validation loss: 2.142458419005076

Epoch: 6| Step: 5
Training loss: 1.9484167098999023
Validation loss: 2.129095216592153

Epoch: 6| Step: 6
Training loss: 1.707465648651123
Validation loss: 2.127986709276835

Epoch: 6| Step: 7
Training loss: 1.564692497253418
Validation loss: 2.1244322061538696

Epoch: 6| Step: 8
Training loss: 2.558272123336792
Validation loss: 2.133319139480591

Epoch: 6| Step: 9
Training loss: 2.0355873107910156
Validation loss: 2.1190927823384604

Epoch: 6| Step: 10
Training loss: 1.7907893657684326
Validation loss: 2.104800542195638

Epoch: 6| Step: 11
Training loss: 1.5645194053649902
Validation loss: 2.10737552245458

Epoch: 6| Step: 12
Training loss: 1.3084032535552979
Validation loss: 2.0938929120699563

Epoch: 6| Step: 13
Training loss: 2.0230562686920166
Validation loss: 2.094065328439077

Epoch: 174| Step: 0
Training loss: 2.0651888847351074
Validation loss: 2.0980105996131897

Epoch: 6| Step: 1
Training loss: 1.6007070541381836
Validation loss: 2.0946130553881326

Epoch: 6| Step: 2
Training loss: 1.4466192722320557
Validation loss: 2.0923041105270386

Epoch: 6| Step: 3
Training loss: 2.819765567779541
Validation loss: 2.092982451121012

Epoch: 6| Step: 4
Training loss: 1.9755839109420776
Validation loss: 2.0880549550056458

Epoch: 6| Step: 5
Training loss: 2.2173256874084473
Validation loss: 2.0961451530456543

Epoch: 6| Step: 6
Training loss: 2.0892887115478516
Validation loss: 2.0910900632540383

Epoch: 6| Step: 7
Training loss: 1.2540311813354492
Validation loss: 2.1013272205988565

Epoch: 6| Step: 8
Training loss: 1.8305165767669678
Validation loss: 2.106086234251658

Epoch: 6| Step: 9
Training loss: 1.5207545757293701
Validation loss: 2.1055197715759277

Epoch: 6| Step: 10
Training loss: 2.2240052223205566
Validation loss: 2.1163488030433655

Epoch: 6| Step: 11
Training loss: 1.845589280128479
Validation loss: 2.108501136302948

Epoch: 6| Step: 12
Training loss: 2.0095882415771484
Validation loss: 2.120045840740204

Epoch: 6| Step: 13
Training loss: 2.0396523475646973
Validation loss: 2.1311374505360923

Epoch: 175| Step: 0
Training loss: 2.1453609466552734
Validation loss: 2.1401667992273965

Epoch: 6| Step: 1
Training loss: 2.1134374141693115
Validation loss: 2.137780805428823

Epoch: 6| Step: 2
Training loss: 2.0036439895629883
Validation loss: 2.1546608805656433

Epoch: 6| Step: 3
Training loss: 1.9838004112243652
Validation loss: 2.148163894812266

Epoch: 6| Step: 4
Training loss: 1.899160385131836
Validation loss: 2.1457403699556985

Epoch: 6| Step: 5
Training loss: 2.0714340209960938
Validation loss: 2.145346442858378

Epoch: 6| Step: 6
Training loss: 1.331135869026184
Validation loss: 2.1540628472963967

Epoch: 6| Step: 7
Training loss: 1.856015682220459
Validation loss: 2.1527735193570456

Epoch: 6| Step: 8
Training loss: 1.5572586059570312
Validation loss: 2.152987798055013

Epoch: 6| Step: 9
Training loss: 2.045107364654541
Validation loss: 2.155084550380707

Epoch: 6| Step: 10
Training loss: 2.048347234725952
Validation loss: 2.1578389604886374

Epoch: 6| Step: 11
Training loss: 1.6908748149871826
Validation loss: 2.1561175187428794

Epoch: 6| Step: 12
Training loss: 2.0411131381988525
Validation loss: 2.118722399075826

Epoch: 6| Step: 13
Training loss: 2.113043785095215
Validation loss: 2.110685129960378

Epoch: 176| Step: 0
Training loss: 1.7623631954193115
Validation loss: 2.0921013355255127

Epoch: 6| Step: 1
Training loss: 2.320746898651123
Validation loss: 2.090309739112854

Epoch: 6| Step: 2
Training loss: 1.430442452430725
Validation loss: 2.081587255001068

Epoch: 6| Step: 3
Training loss: 2.0403356552124023
Validation loss: 2.0790791710217795

Epoch: 6| Step: 4
Training loss: 2.3855724334716797
Validation loss: 2.0701916416486106

Epoch: 6| Step: 5
Training loss: 1.927093744277954
Validation loss: 2.0755772391955056

Epoch: 6| Step: 6
Training loss: 1.7572962045669556
Validation loss: 2.0738266507784524

Epoch: 6| Step: 7
Training loss: 2.1693437099456787
Validation loss: 2.0741995175679526

Epoch: 6| Step: 8
Training loss: 1.7986395359039307
Validation loss: 2.0929601788520813

Epoch: 6| Step: 9
Training loss: 1.5940717458724976
Validation loss: 2.091496527194977

Epoch: 6| Step: 10
Training loss: 2.3283727169036865
Validation loss: 2.1286262472470603

Epoch: 6| Step: 11
Training loss: 2.389805555343628
Validation loss: 2.139387547969818

Epoch: 6| Step: 12
Training loss: 1.7936493158340454
Validation loss: 2.1501505374908447

Epoch: 6| Step: 13
Training loss: 1.6541025638580322
Validation loss: 2.149504800637563

Epoch: 177| Step: 0
Training loss: 1.5631306171417236
Validation loss: 2.1612618962923684

Epoch: 6| Step: 1
Training loss: 2.3407273292541504
Validation loss: 2.158815344174703

Epoch: 6| Step: 2
Training loss: 1.8758585453033447
Validation loss: 2.1443211833635965

Epoch: 6| Step: 3
Training loss: 1.4650981426239014
Validation loss: 2.148161252339681

Epoch: 6| Step: 4
Training loss: 1.7736122608184814
Validation loss: 2.13124680519104

Epoch: 6| Step: 5
Training loss: 2.5130767822265625
Validation loss: 2.1175174911816916

Epoch: 6| Step: 6
Training loss: 2.6459507942199707
Validation loss: 2.1208415031433105

Epoch: 6| Step: 7
Training loss: 1.5754450559616089
Validation loss: 2.107521037260691

Epoch: 6| Step: 8
Training loss: 1.6916005611419678
Validation loss: 2.1077908476193747

Epoch: 6| Step: 9
Training loss: 1.5375410318374634
Validation loss: 2.0952746272087097

Epoch: 6| Step: 10
Training loss: 1.4627618789672852
Validation loss: 2.090213676293691

Epoch: 6| Step: 11
Training loss: 2.3422961235046387
Validation loss: 2.096221109231313

Epoch: 6| Step: 12
Training loss: 2.0223710536956787
Validation loss: 2.1034451524416604

Epoch: 6| Step: 13
Training loss: 2.2635889053344727
Validation loss: 2.1217467983563743

Epoch: 178| Step: 0
Training loss: 2.0512402057647705
Validation loss: 2.1321034034093223

Epoch: 6| Step: 1
Training loss: 1.584425687789917
Validation loss: 2.143196960290273

Epoch: 6| Step: 2
Training loss: 1.488108515739441
Validation loss: 2.12718673547109

Epoch: 6| Step: 3
Training loss: 2.0124826431274414
Validation loss: 2.140637993812561

Epoch: 6| Step: 4
Training loss: 1.7869365215301514
Validation loss: 2.1304070154825845

Epoch: 6| Step: 5
Training loss: 2.2286429405212402
Validation loss: 2.1308175722757974

Epoch: 6| Step: 6
Training loss: 1.7630172967910767
Validation loss: 2.1103846629460654

Epoch: 6| Step: 7
Training loss: 2.194633960723877
Validation loss: 2.1148725946744285

Epoch: 6| Step: 8
Training loss: 2.198277473449707
Validation loss: 2.112299462159475

Epoch: 6| Step: 9
Training loss: 2.2578320503234863
Validation loss: 2.095967392126719

Epoch: 6| Step: 10
Training loss: 1.8558104038238525
Validation loss: 2.090072453022003

Epoch: 6| Step: 11
Training loss: 1.6775211095809937
Validation loss: 2.091586927572886

Epoch: 6| Step: 12
Training loss: 1.9061359167099
Validation loss: 2.082274774710337

Epoch: 6| Step: 13
Training loss: 2.111356258392334
Validation loss: 2.074918528397878

Epoch: 179| Step: 0
Training loss: 1.4680432081222534
Validation loss: 2.0607351064682007

Epoch: 6| Step: 1
Training loss: 1.6875169277191162
Validation loss: 2.0633872747421265

Epoch: 6| Step: 2
Training loss: 1.9729869365692139
Validation loss: 2.0684190591176352

Epoch: 6| Step: 3
Training loss: 2.025099277496338
Validation loss: 2.080820163091024

Epoch: 6| Step: 4
Training loss: 2.611872673034668
Validation loss: 2.087904135386149

Epoch: 6| Step: 5
Training loss: 1.724290132522583
Validation loss: 2.0967045029004416

Epoch: 6| Step: 6
Training loss: 1.8269046545028687
Validation loss: 2.099943439165751

Epoch: 6| Step: 7
Training loss: 2.289641857147217
Validation loss: 2.1100488106409707

Epoch: 6| Step: 8
Training loss: 2.1059811115264893
Validation loss: 2.1026824315389

Epoch: 6| Step: 9
Training loss: 1.7447373867034912
Validation loss: 2.1290844480196633

Epoch: 6| Step: 10
Training loss: 2.404359817504883
Validation loss: 2.128637154897054

Epoch: 6| Step: 11
Training loss: 1.4903686046600342
Validation loss: 2.1184944907824197

Epoch: 6| Step: 12
Training loss: 1.9677671194076538
Validation loss: 2.1353400548299155

Epoch: 6| Step: 13
Training loss: 1.804009199142456
Validation loss: 2.1194841265678406

Epoch: 180| Step: 0
Training loss: 2.1992156505584717
Validation loss: 2.1233231822649636

Epoch: 6| Step: 1
Training loss: 1.7196638584136963
Validation loss: 2.1270531018575034

Epoch: 6| Step: 2
Training loss: 2.4243602752685547
Validation loss: 2.127687374750773

Epoch: 6| Step: 3
Training loss: 2.030790090560913
Validation loss: 2.127370913823446

Epoch: 6| Step: 4
Training loss: 2.5092477798461914
Validation loss: 2.1224603255589805

Epoch: 6| Step: 5
Training loss: 1.9022966623306274
Validation loss: 2.1093312899271646

Epoch: 6| Step: 6
Training loss: 1.7508866786956787
Validation loss: 2.1192474365234375

Epoch: 6| Step: 7
Training loss: 1.6466641426086426
Validation loss: 2.126534958680471

Epoch: 6| Step: 8
Training loss: 2.024091958999634
Validation loss: 2.1197630167007446

Epoch: 6| Step: 9
Training loss: 1.9433573484420776
Validation loss: 2.12776380777359

Epoch: 6| Step: 10
Training loss: 1.5272984504699707
Validation loss: 2.1394569277763367

Epoch: 6| Step: 11
Training loss: 1.4983839988708496
Validation loss: 2.1343985199928284

Epoch: 6| Step: 12
Training loss: 1.5449280738830566
Validation loss: 2.112257401148478

Epoch: 6| Step: 13
Training loss: 2.180767059326172
Validation loss: 2.1275421579678855

Epoch: 181| Step: 0
Training loss: 1.5775444507598877
Validation loss: 2.1086068550745645

Epoch: 6| Step: 1
Training loss: 2.5243821144104004
Validation loss: 2.1137407422065735

Epoch: 6| Step: 2
Training loss: 1.955689549446106
Validation loss: 2.1077064275741577

Epoch: 6| Step: 3
Training loss: 2.160459518432617
Validation loss: 2.1067620118459067

Epoch: 6| Step: 4
Training loss: 2.5056867599487305
Validation loss: 2.1039104064305625

Epoch: 6| Step: 5
Training loss: 1.58424711227417
Validation loss: 2.1142002940177917

Epoch: 6| Step: 6
Training loss: 2.537015199661255
Validation loss: 2.112431804339091

Epoch: 6| Step: 7
Training loss: 1.3821525573730469
Validation loss: 2.122125724951426

Epoch: 6| Step: 8
Training loss: 2.0004162788391113
Validation loss: 2.1182249784469604

Epoch: 6| Step: 9
Training loss: 1.8435083627700806
Validation loss: 2.1003385384877524

Epoch: 6| Step: 10
Training loss: 2.225135087966919
Validation loss: 2.1139233112335205

Epoch: 6| Step: 11
Training loss: 1.4739155769348145
Validation loss: 2.1074087222417197

Epoch: 6| Step: 12
Training loss: 1.842545747756958
Validation loss: 2.11825301249822

Epoch: 6| Step: 13
Training loss: 1.5141785144805908
Validation loss: 2.1175292134284973

Epoch: 182| Step: 0
Training loss: 1.9471734762191772
Validation loss: 2.1250391403834024

Epoch: 6| Step: 1
Training loss: 2.5225439071655273
Validation loss: 2.1156646609306335

Epoch: 6| Step: 2
Training loss: 1.626328945159912
Validation loss: 2.1014566818873086

Epoch: 6| Step: 3
Training loss: 2.2465832233428955
Validation loss: 2.1091506282488504

Epoch: 6| Step: 4
Training loss: 2.095930576324463
Validation loss: 2.114867687225342

Epoch: 6| Step: 5
Training loss: 1.692757487297058
Validation loss: 2.118472139040629

Epoch: 6| Step: 6
Training loss: 1.755965232849121
Validation loss: 2.1186700661977134

Epoch: 6| Step: 7
Training loss: 1.3324415683746338
Validation loss: 2.1420766512552896

Epoch: 6| Step: 8
Training loss: 1.4756884574890137
Validation loss: 2.132929523785909

Epoch: 6| Step: 9
Training loss: 2.4743826389312744
Validation loss: 2.1413448452949524

Epoch: 6| Step: 10
Training loss: 2.144195556640625
Validation loss: 2.1349886457125344

Epoch: 6| Step: 11
Training loss: 1.842531681060791
Validation loss: 2.1549734274546304

Epoch: 6| Step: 12
Training loss: 1.8342998027801514
Validation loss: 2.153944810231527

Epoch: 6| Step: 13
Training loss: 2.3994574546813965
Validation loss: 2.1476826270421348

Epoch: 183| Step: 0
Training loss: 1.8489588499069214
Validation loss: 2.139655570189158

Epoch: 6| Step: 1
Training loss: 1.8969422578811646
Validation loss: 2.132882297039032

Epoch: 6| Step: 2
Training loss: 1.575331211090088
Validation loss: 2.11726842323939

Epoch: 6| Step: 3
Training loss: 2.0267860889434814
Validation loss: 2.1168116529782615

Epoch: 6| Step: 4
Training loss: 3.225611925125122
Validation loss: 2.12645955880483

Epoch: 6| Step: 5
Training loss: 2.0340943336486816
Validation loss: 2.1235033869743347

Epoch: 6| Step: 6
Training loss: 2.189572811126709
Validation loss: 2.1252657572428384

Epoch: 6| Step: 7
Training loss: 1.6300715208053589
Validation loss: 2.125830908616384

Epoch: 6| Step: 8
Training loss: 1.6729815006256104
Validation loss: 2.1370327870051065

Epoch: 6| Step: 9
Training loss: 1.428134560585022
Validation loss: 2.130040764808655

Epoch: 6| Step: 10
Training loss: 1.7328083515167236
Validation loss: 2.1229270497957864

Epoch: 6| Step: 11
Training loss: 1.6681541204452515
Validation loss: 2.120575408140818

Epoch: 6| Step: 12
Training loss: 1.583885669708252
Validation loss: 2.1428014437357583

Epoch: 6| Step: 13
Training loss: 2.269507884979248
Validation loss: 2.149379332860311

Epoch: 184| Step: 0
Training loss: 2.0908710956573486
Validation loss: 2.176604231198629

Epoch: 6| Step: 1
Training loss: 1.7963738441467285
Validation loss: 2.1593686739603677

Epoch: 6| Step: 2
Training loss: 1.0403313636779785
Validation loss: 2.167541722456614

Epoch: 6| Step: 3
Training loss: 2.066143035888672
Validation loss: 2.18085370461146

Epoch: 6| Step: 4
Training loss: 1.9872103929519653
Validation loss: 2.147777179876963

Epoch: 6| Step: 5
Training loss: 1.8502600193023682
Validation loss: 2.1623401244481406

Epoch: 6| Step: 6
Training loss: 2.627922534942627
Validation loss: 2.1563095847765603

Epoch: 6| Step: 7
Training loss: 2.7975807189941406
Validation loss: 2.1442705392837524

Epoch: 6| Step: 8
Training loss: 1.4535017013549805
Validation loss: 2.1536600589752197

Epoch: 6| Step: 9
Training loss: 2.1176633834838867
Validation loss: 2.1555825074513755

Epoch: 6| Step: 10
Training loss: 1.759381890296936
Validation loss: 2.155592203140259

Epoch: 6| Step: 11
Training loss: 1.9131474494934082
Validation loss: 2.14655339717865

Epoch: 6| Step: 12
Training loss: 1.7636679410934448
Validation loss: 2.1437218387921653

Epoch: 6| Step: 13
Training loss: 1.337289571762085
Validation loss: 2.1322243014971414

Epoch: 185| Step: 0
Training loss: 1.7461131811141968
Validation loss: 2.1229482094446817

Epoch: 6| Step: 1
Training loss: 1.8797352313995361
Validation loss: 2.13868514696757

Epoch: 6| Step: 2
Training loss: 1.8987146615982056
Validation loss: 2.1337727308273315

Epoch: 6| Step: 3
Training loss: 2.390507698059082
Validation loss: 2.1159975131352744

Epoch: 6| Step: 4
Training loss: 1.7236571311950684
Validation loss: 2.112446665763855

Epoch: 6| Step: 5
Training loss: 2.2944982051849365
Validation loss: 2.1280994017918906

Epoch: 6| Step: 6
Training loss: 1.511366605758667
Validation loss: 2.139701783657074

Epoch: 6| Step: 7
Training loss: 2.2675156593322754
Validation loss: 2.1462666591008506

Epoch: 6| Step: 8
Training loss: 1.7093417644500732
Validation loss: 2.13323175907135

Epoch: 6| Step: 9
Training loss: 1.6233010292053223
Validation loss: 2.144686679045359

Epoch: 6| Step: 10
Training loss: 1.7647517919540405
Validation loss: 2.148405909538269

Epoch: 6| Step: 11
Training loss: 1.9408286809921265
Validation loss: 2.1458968122800193

Epoch: 6| Step: 12
Training loss: 1.8300219774246216
Validation loss: 2.1422165632247925

Epoch: 6| Step: 13
Training loss: 1.8377116918563843
Validation loss: 2.145936965942383

Epoch: 186| Step: 0
Training loss: 2.2769408226013184
Validation loss: 2.125824789206187

Epoch: 6| Step: 1
Training loss: 1.4973444938659668
Validation loss: 2.1318881114323935

Epoch: 6| Step: 2
Training loss: 2.013936996459961
Validation loss: 2.132671038309733

Epoch: 6| Step: 3
Training loss: 1.3237483501434326
Validation loss: 2.1133275429407754

Epoch: 6| Step: 4
Training loss: 2.5135397911071777
Validation loss: 2.1186187068621316

Epoch: 6| Step: 5
Training loss: 2.045076608657837
Validation loss: 2.119710644086202

Epoch: 6| Step: 6
Training loss: 1.699282169342041
Validation loss: 2.140843629837036

Epoch: 6| Step: 7
Training loss: 1.5286799669265747
Validation loss: 2.142459054787954

Epoch: 6| Step: 8
Training loss: 2.509354591369629
Validation loss: 2.1508339444796243

Epoch: 6| Step: 9
Training loss: 1.3577570915222168
Validation loss: 2.1492165525754294

Epoch: 6| Step: 10
Training loss: 2.140305519104004
Validation loss: 2.1796350876490274

Epoch: 6| Step: 11
Training loss: 1.5220822095870972
Validation loss: 2.1832575599352517

Epoch: 6| Step: 12
Training loss: 1.8653404712677002
Validation loss: 2.1712401310602822

Epoch: 6| Step: 13
Training loss: 2.1056857109069824
Validation loss: 2.1718095541000366

Epoch: 187| Step: 0
Training loss: 1.9075686931610107
Validation loss: 2.132429758707682

Epoch: 6| Step: 1
Training loss: 1.5863583087921143
Validation loss: 2.1391714413960776

Epoch: 6| Step: 2
Training loss: 1.9455291032791138
Validation loss: 2.135980208714803

Epoch: 6| Step: 3
Training loss: 2.1140804290771484
Validation loss: 2.11641925573349

Epoch: 6| Step: 4
Training loss: 1.9159748554229736
Validation loss: 2.1000366608301797

Epoch: 6| Step: 5
Training loss: 2.2664477825164795
Validation loss: 2.1079836090405784

Epoch: 6| Step: 6
Training loss: 1.1509363651275635
Validation loss: 2.08459464708964

Epoch: 6| Step: 7
Training loss: 2.2984964847564697
Validation loss: 2.092951476573944

Epoch: 6| Step: 8
Training loss: 1.6865272521972656
Validation loss: 2.1008862257003784

Epoch: 6| Step: 9
Training loss: 2.5866470336914062
Validation loss: 2.0950106183687844

Epoch: 6| Step: 10
Training loss: 1.8397846221923828
Validation loss: 2.1047394076983132

Epoch: 6| Step: 11
Training loss: 1.6711208820343018
Validation loss: 2.099411348501841

Epoch: 6| Step: 12
Training loss: 1.8418464660644531
Validation loss: 2.114186704158783

Epoch: 6| Step: 13
Training loss: 2.132479190826416
Validation loss: 2.1275664567947388

Epoch: 188| Step: 0
Training loss: 2.4464163780212402
Validation loss: 2.149228016535441

Epoch: 6| Step: 1
Training loss: 2.7084949016571045
Validation loss: 2.1729568044344583

Epoch: 6| Step: 2
Training loss: 1.9722929000854492
Validation loss: 2.14867373307546

Epoch: 6| Step: 3
Training loss: 2.187885284423828
Validation loss: 2.1527252793312073

Epoch: 6| Step: 4
Training loss: 1.5006682872772217
Validation loss: 2.1499029795328775

Epoch: 6| Step: 5
Training loss: 1.7857246398925781
Validation loss: 2.1311837236086526

Epoch: 6| Step: 6
Training loss: 1.8295068740844727
Validation loss: 2.1245513558387756

Epoch: 6| Step: 7
Training loss: 2.037289619445801
Validation loss: 2.1280741492907205

Epoch: 6| Step: 8
Training loss: 1.5386319160461426
Validation loss: 2.1452274918556213

Epoch: 6| Step: 9
Training loss: 2.0340728759765625
Validation loss: 2.161055068174998

Epoch: 6| Step: 10
Training loss: 1.5478951930999756
Validation loss: 2.14605313539505

Epoch: 6| Step: 11
Training loss: 1.679656982421875
Validation loss: 2.1544533371925354

Epoch: 6| Step: 12
Training loss: 2.0249216556549072
Validation loss: 2.136732757091522

Epoch: 6| Step: 13
Training loss: 1.457326889038086
Validation loss: 2.137352546056112

Epoch: 189| Step: 0
Training loss: 1.3300387859344482
Validation loss: 2.1382728020350137

Epoch: 6| Step: 1
Training loss: 1.8376884460449219
Validation loss: 2.125449856122335

Epoch: 6| Step: 2
Training loss: 1.8921846151351929
Validation loss: 2.1203903357187905

Epoch: 6| Step: 3
Training loss: 1.8444414138793945
Validation loss: 2.116161863009135

Epoch: 6| Step: 4
Training loss: 2.5325114727020264
Validation loss: 2.116644481817881

Epoch: 6| Step: 5
Training loss: 1.6269779205322266
Validation loss: 2.120827555656433

Epoch: 6| Step: 6
Training loss: 1.7105807065963745
Validation loss: 2.11435874303182

Epoch: 6| Step: 7
Training loss: 2.5178325176239014
Validation loss: 2.1023303270339966

Epoch: 6| Step: 8
Training loss: 2.7573513984680176
Validation loss: 2.1022570530573526

Epoch: 6| Step: 9
Training loss: 1.2278623580932617
Validation loss: 2.10274746020635

Epoch: 6| Step: 10
Training loss: 1.6507371664047241
Validation loss: 2.0867717266082764

Epoch: 6| Step: 11
Training loss: 1.5338258743286133
Validation loss: 2.106479048728943

Epoch: 6| Step: 12
Training loss: 1.9074617624282837
Validation loss: 2.1169606645902

Epoch: 6| Step: 13
Training loss: 2.0441014766693115
Validation loss: 2.1277677615483603

Epoch: 190| Step: 0
Training loss: 1.4779754877090454
Validation loss: 2.1426102916399636

Epoch: 6| Step: 1
Training loss: 1.7957944869995117
Validation loss: 2.164677699406942

Epoch: 6| Step: 2
Training loss: 2.579317569732666
Validation loss: 2.1809648672739663

Epoch: 6| Step: 3
Training loss: 1.943585991859436
Validation loss: 2.187230904897054

Epoch: 6| Step: 4
Training loss: 2.8139195442199707
Validation loss: 2.181201457977295

Epoch: 6| Step: 5
Training loss: 1.7856507301330566
Validation loss: 2.1720873514811196

Epoch: 6| Step: 6
Training loss: 2.1159892082214355
Validation loss: 2.1807014544804892

Epoch: 6| Step: 7
Training loss: 1.8980250358581543
Validation loss: 2.1819495360056558

Epoch: 6| Step: 8
Training loss: 1.3080272674560547
Validation loss: 2.1708219250043235

Epoch: 6| Step: 9
Training loss: 1.2432348728179932
Validation loss: 2.1413520971934

Epoch: 6| Step: 10
Training loss: 2.1337344646453857
Validation loss: 2.1305772264798484

Epoch: 6| Step: 11
Training loss: 1.5178604125976562
Validation loss: 2.116812288761139

Epoch: 6| Step: 12
Training loss: 1.967273473739624
Validation loss: 2.113115827242533

Epoch: 6| Step: 13
Training loss: 1.921513319015503
Validation loss: 2.10754930973053

Epoch: 191| Step: 0
Training loss: 2.6391091346740723
Validation loss: 2.102587878704071

Epoch: 6| Step: 1
Training loss: 1.9455223083496094
Validation loss: 2.1026893655459085

Epoch: 6| Step: 2
Training loss: 1.5941154956817627
Validation loss: 2.0971380869547525

Epoch: 6| Step: 3
Training loss: 1.96321439743042
Validation loss: 2.1020549138387046

Epoch: 6| Step: 4
Training loss: 2.3768370151519775
Validation loss: 2.103514850139618

Epoch: 6| Step: 5
Training loss: 1.9938180446624756
Validation loss: 2.1098589102427163

Epoch: 6| Step: 6
Training loss: 1.6481555700302124
Validation loss: 2.1137263774871826

Epoch: 6| Step: 7
Training loss: 1.7371386289596558
Validation loss: 2.120051602522532

Epoch: 6| Step: 8
Training loss: 1.529393196105957
Validation loss: 2.127930998802185

Epoch: 6| Step: 9
Training loss: 2.141868829727173
Validation loss: 2.139095366001129

Epoch: 6| Step: 10
Training loss: 1.9631309509277344
Validation loss: 2.161844770113627

Epoch: 6| Step: 11
Training loss: 1.6411857604980469
Validation loss: 2.185694694519043

Epoch: 6| Step: 12
Training loss: 1.8928771018981934
Validation loss: 2.1868271629015603

Epoch: 6| Step: 13
Training loss: 2.173297643661499
Validation loss: 2.1908306082089744

Epoch: 192| Step: 0
Training loss: 2.152007579803467
Validation loss: 2.1819951931635537

Epoch: 6| Step: 1
Training loss: 1.7589643001556396
Validation loss: 2.1958700815836587

Epoch: 6| Step: 2
Training loss: 1.7491545677185059
Validation loss: 2.1853226025899253

Epoch: 6| Step: 3
Training loss: 1.701805830001831
Validation loss: 2.1842198173205056

Epoch: 6| Step: 4
Training loss: 2.6524410247802734
Validation loss: 2.171685496966044

Epoch: 6| Step: 5
Training loss: 1.9505610466003418
Validation loss: 2.1635319590568542

Epoch: 6| Step: 6
Training loss: 2.0028958320617676
Validation loss: 2.1501605113347373

Epoch: 6| Step: 7
Training loss: 2.2954351902008057
Validation loss: 2.1355007092158

Epoch: 6| Step: 8
Training loss: 1.7883844375610352
Validation loss: 2.1242159605026245

Epoch: 6| Step: 9
Training loss: 2.0370450019836426
Validation loss: 2.138277053833008

Epoch: 6| Step: 10
Training loss: 1.7613441944122314
Validation loss: 2.1374627351760864

Epoch: 6| Step: 11
Training loss: 0.9712549448013306
Validation loss: 2.1398843129475913

Epoch: 6| Step: 12
Training loss: 1.7385656833648682
Validation loss: 2.1475197474161782

Epoch: 6| Step: 13
Training loss: 1.9616581201553345
Validation loss: 2.1357641418774924

Epoch: 193| Step: 0
Training loss: 1.815793514251709
Validation loss: 2.1536308924357095

Epoch: 6| Step: 1
Training loss: 1.747086524963379
Validation loss: 2.1592722733815513

Epoch: 6| Step: 2
Training loss: 1.728256344795227
Validation loss: 2.1462490955988565

Epoch: 6| Step: 3
Training loss: 1.4604847431182861
Validation loss: 2.173619250456492

Epoch: 6| Step: 4
Training loss: 2.1711292266845703
Validation loss: 2.1553489764531455

Epoch: 6| Step: 5
Training loss: 2.409945011138916
Validation loss: 2.1675508419672647

Epoch: 6| Step: 6
Training loss: 1.8374511003494263
Validation loss: 2.1807154019673667

Epoch: 6| Step: 7
Training loss: 1.177636981010437
Validation loss: 2.178187310695648

Epoch: 6| Step: 8
Training loss: 2.4468469619750977
Validation loss: 2.174170434474945

Epoch: 6| Step: 9
Training loss: 1.938372254371643
Validation loss: 2.1840414007504783

Epoch: 6| Step: 10
Training loss: 1.676554560661316
Validation loss: 2.1541113456090293

Epoch: 6| Step: 11
Training loss: 2.162208080291748
Validation loss: 2.146972417831421

Epoch: 6| Step: 12
Training loss: 1.981763243675232
Validation loss: 2.134903907775879

Epoch: 6| Step: 13
Training loss: 1.644352912902832
Validation loss: 2.1387055118878684

Epoch: 194| Step: 0
Training loss: 2.135437488555908
Validation loss: 2.1104180018107095

Epoch: 6| Step: 1
Training loss: 1.7272114753723145
Validation loss: 2.1139434576034546

Epoch: 6| Step: 2
Training loss: 2.1885290145874023
Validation loss: 2.1075350244839988

Epoch: 6| Step: 3
Training loss: 1.9389314651489258
Validation loss: 2.095524549484253

Epoch: 6| Step: 4
Training loss: 2.095512628555298
Validation loss: 2.1127145489056907

Epoch: 6| Step: 5
Training loss: 1.4937946796417236
Validation loss: 2.11599592367808

Epoch: 6| Step: 6
Training loss: 1.7733427286148071
Validation loss: 2.1184277137120566

Epoch: 6| Step: 7
Training loss: 2.1922860145568848
Validation loss: 2.1242755651474

Epoch: 6| Step: 8
Training loss: 2.4755125045776367
Validation loss: 2.1309236884117126

Epoch: 6| Step: 9
Training loss: 1.385669231414795
Validation loss: 2.145622710386912

Epoch: 6| Step: 10
Training loss: 1.9040082693099976
Validation loss: 2.170543611049652

Epoch: 6| Step: 11
Training loss: 1.7749505043029785
Validation loss: 2.1862102349599204

Epoch: 6| Step: 12
Training loss: 1.8190876245498657
Validation loss: 2.192498048146566

Epoch: 6| Step: 13
Training loss: 1.5526903867721558
Validation loss: 2.1825222770373025

Epoch: 195| Step: 0
Training loss: 2.176589012145996
Validation loss: 2.1698602636655173

Epoch: 6| Step: 1
Training loss: 1.3114930391311646
Validation loss: 2.1652055184046426

Epoch: 6| Step: 2
Training loss: 2.2274160385131836
Validation loss: 2.1532206932703652

Epoch: 6| Step: 3
Training loss: 1.2367331981658936
Validation loss: 2.1560126543045044

Epoch: 6| Step: 4
Training loss: 1.8680620193481445
Validation loss: 2.1322418451309204

Epoch: 6| Step: 5
Training loss: 1.9374120235443115
Validation loss: 2.1315137346585593

Epoch: 6| Step: 6
Training loss: 2.089181423187256
Validation loss: 2.120133320490519

Epoch: 6| Step: 7
Training loss: 1.8200914859771729
Validation loss: 2.116262594858805

Epoch: 6| Step: 8
Training loss: 1.6118215322494507
Validation loss: 2.116643269856771

Epoch: 6| Step: 9
Training loss: 1.9582841396331787
Validation loss: 2.110562483469645

Epoch: 6| Step: 10
Training loss: 2.015334367752075
Validation loss: 2.124809126059214

Epoch: 6| Step: 11
Training loss: 2.006948471069336
Validation loss: 2.126485228538513

Epoch: 6| Step: 12
Training loss: 1.7317973375320435
Validation loss: 2.132474899291992

Epoch: 6| Step: 13
Training loss: 2.2286124229431152
Validation loss: 2.1397650837898254

Epoch: 196| Step: 0
Training loss: 2.313209056854248
Validation loss: 2.154752572377523

Epoch: 6| Step: 1
Training loss: 1.425551414489746
Validation loss: 2.163641393184662

Epoch: 6| Step: 2
Training loss: 1.6845555305480957
Validation loss: 2.15859983364741

Epoch: 6| Step: 3
Training loss: 1.7165722846984863
Validation loss: 2.157253841559092

Epoch: 6| Step: 4
Training loss: 1.4587246179580688
Validation loss: 2.157471776008606

Epoch: 6| Step: 5
Training loss: 1.600520372390747
Validation loss: 2.1578998366991677

Epoch: 6| Step: 6
Training loss: 1.7146402597427368
Validation loss: 2.14539110660553

Epoch: 6| Step: 7
Training loss: 2.7716257572174072
Validation loss: 2.140583574771881

Epoch: 6| Step: 8
Training loss: 1.9781311750411987
Validation loss: 2.1372817357381186

Epoch: 6| Step: 9
Training loss: 2.2648158073425293
Validation loss: 2.1265501578648887

Epoch: 6| Step: 10
Training loss: 1.40304434299469
Validation loss: 2.141016364097595

Epoch: 6| Step: 11
Training loss: 1.8137097358703613
Validation loss: 2.1211243271827698

Epoch: 6| Step: 12
Training loss: 2.075326442718506
Validation loss: 2.137635290622711

Epoch: 6| Step: 13
Training loss: 1.5838005542755127
Validation loss: 2.144484798113505

Epoch: 197| Step: 0
Training loss: 2.234318256378174
Validation loss: 2.1490655740102134

Epoch: 6| Step: 1
Training loss: 1.8013633489608765
Validation loss: 2.1662477254867554

Epoch: 6| Step: 2
Training loss: 1.7537782192230225
Validation loss: 2.188869078954061

Epoch: 6| Step: 3
Training loss: 2.63144850730896
Validation loss: 2.187059203783671

Epoch: 6| Step: 4
Training loss: 1.44508957862854
Validation loss: 2.201505939165751

Epoch: 6| Step: 5
Training loss: 2.39371919631958
Validation loss: 2.2092371781667075

Epoch: 6| Step: 6
Training loss: 1.6754014492034912
Validation loss: 2.220603346824646

Epoch: 6| Step: 7
Training loss: 1.3798394203186035
Validation loss: 2.2154540618260703

Epoch: 6| Step: 8
Training loss: 1.6479711532592773
Validation loss: 2.202311078707377

Epoch: 6| Step: 9
Training loss: 1.757753849029541
Validation loss: 2.1808990637461343

Epoch: 6| Step: 10
Training loss: 1.6046600341796875
Validation loss: 2.1510616342226663

Epoch: 6| Step: 11
Training loss: 2.3057069778442383
Validation loss: 2.1498934626579285

Epoch: 6| Step: 12
Training loss: 1.8553485870361328
Validation loss: 2.133654157320658

Epoch: 6| Step: 13
Training loss: 1.8866775035858154
Validation loss: 2.1156202952067056

Epoch: 198| Step: 0
Training loss: 2.0318336486816406
Validation loss: 2.1142478585243225

Epoch: 6| Step: 1
Training loss: 1.5055625438690186
Validation loss: 2.111791729927063

Epoch: 6| Step: 2
Training loss: 2.0705485343933105
Validation loss: 2.106910288333893

Epoch: 6| Step: 3
Training loss: 1.9797296524047852
Validation loss: 2.1041836738586426

Epoch: 6| Step: 4
Training loss: 2.05631160736084
Validation loss: 2.111097574234009

Epoch: 6| Step: 5
Training loss: 1.7835687398910522
Validation loss: 2.1099575559298196

Epoch: 6| Step: 6
Training loss: 1.9811391830444336
Validation loss: 2.121773580710093

Epoch: 6| Step: 7
Training loss: 1.6061034202575684
Validation loss: 2.130322535832723

Epoch: 6| Step: 8
Training loss: 1.8077943325042725
Validation loss: 2.1577541828155518

Epoch: 6| Step: 9
Training loss: 1.7666301727294922
Validation loss: 2.1647877295811973

Epoch: 6| Step: 10
Training loss: 1.7835395336151123
Validation loss: 2.16787580649058

Epoch: 6| Step: 11
Training loss: 2.238161563873291
Validation loss: 2.1964272260665894

Epoch: 6| Step: 12
Training loss: 1.8338978290557861
Validation loss: 2.2061766386032104

Epoch: 6| Step: 13
Training loss: 1.8643999099731445
Validation loss: 2.205114742120107

Epoch: 199| Step: 0
Training loss: 2.219146251678467
Validation loss: 2.207152565320333

Epoch: 6| Step: 1
Training loss: 1.6686944961547852
Validation loss: 2.187402129173279

Epoch: 6| Step: 2
Training loss: 1.9772804975509644
Validation loss: 2.180611252784729

Epoch: 6| Step: 3
Training loss: 1.249704360961914
Validation loss: 2.2027154167493186

Epoch: 6| Step: 4
Training loss: 2.4863533973693848
Validation loss: 2.1750980814297995

Epoch: 6| Step: 5
Training loss: 2.0670900344848633
Validation loss: 2.172098954518636

Epoch: 6| Step: 6
Training loss: 2.634531021118164
Validation loss: 2.1773126324017844

Epoch: 6| Step: 7
Training loss: 2.2352499961853027
Validation loss: 2.179228663444519

Epoch: 6| Step: 8
Training loss: 1.8476386070251465
Validation loss: 2.1508094469706216

Epoch: 6| Step: 9
Training loss: 1.9514186382293701
Validation loss: 2.1672848661740622

Epoch: 6| Step: 10
Training loss: 1.6445344686508179
Validation loss: 2.1504832903544107

Epoch: 6| Step: 11
Training loss: 1.1741580963134766
Validation loss: 2.175690531730652

Epoch: 6| Step: 12
Training loss: 1.6970851421356201
Validation loss: 2.16177898645401

Epoch: 6| Step: 13
Training loss: 1.1954492330551147
Validation loss: 2.1465216279029846

Epoch: 200| Step: 0
Training loss: 2.2778799533843994
Validation loss: 2.1586616237958274

Epoch: 6| Step: 1
Training loss: 1.9625734090805054
Validation loss: 2.1677934328715005

Epoch: 6| Step: 2
Training loss: 1.9084219932556152
Validation loss: 2.163335084915161

Epoch: 6| Step: 3
Training loss: 2.129314661026001
Validation loss: 2.1562648018201194

Epoch: 6| Step: 4
Training loss: 1.7647616863250732
Validation loss: 2.165551722049713

Epoch: 6| Step: 5
Training loss: 1.711523413658142
Validation loss: 2.174646536509196

Epoch: 6| Step: 6
Training loss: 2.1062936782836914
Validation loss: 2.178486386934916

Epoch: 6| Step: 7
Training loss: 1.2784807682037354
Validation loss: 2.159518380959829

Epoch: 6| Step: 8
Training loss: 1.3038806915283203
Validation loss: 2.1717076698939004

Epoch: 6| Step: 9
Training loss: 1.9507102966308594
Validation loss: 2.178827961285909

Epoch: 6| Step: 10
Training loss: 1.3046026229858398
Validation loss: 2.1976287762324014

Epoch: 6| Step: 11
Training loss: 1.7611466646194458
Validation loss: 2.188909908135732

Epoch: 6| Step: 12
Training loss: 2.2235641479492188
Validation loss: 2.2015532851219177

Epoch: 6| Step: 13
Training loss: 2.255131244659424
Validation loss: 2.181958794593811

Testing loss: 1.8314923228119775

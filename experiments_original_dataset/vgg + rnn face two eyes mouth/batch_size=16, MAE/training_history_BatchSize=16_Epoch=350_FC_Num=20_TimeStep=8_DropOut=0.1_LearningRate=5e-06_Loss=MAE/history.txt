Epoch: 1| Step: 0
Training loss: 5.760349273681641
Validation loss: 5.372330665588379

Epoch: 6| Step: 1
Training loss: 5.607787132263184
Validation loss: 5.3694908618927

Epoch: 6| Step: 2
Training loss: 4.58626651763916
Validation loss: 5.366672436396281

Epoch: 6| Step: 3
Training loss: 4.096006393432617
Validation loss: 5.364059686660767

Epoch: 6| Step: 4
Training loss: 4.993976593017578
Validation loss: 5.361454010009766

Epoch: 6| Step: 5
Training loss: 5.978323936462402
Validation loss: 5.359072367350261

Epoch: 6| Step: 6
Training loss: 6.102082252502441
Validation loss: 5.356677134831746

Epoch: 6| Step: 7
Training loss: 4.992703914642334
Validation loss: 5.354399919509888

Epoch: 6| Step: 8
Training loss: 5.68391227722168
Validation loss: 5.352221488952637

Epoch: 6| Step: 9
Training loss: 4.794466495513916
Validation loss: 5.350022077560425

Epoch: 6| Step: 10
Training loss: 5.674022197723389
Validation loss: 5.347767273585002

Epoch: 6| Step: 11
Training loss: 5.821469306945801
Validation loss: 5.345622777938843

Epoch: 6| Step: 12
Training loss: 5.03775691986084
Validation loss: 5.343345006306966

Epoch: 6| Step: 13
Training loss: 6.814336776733398
Validation loss: 5.341085910797119

Epoch: 2| Step: 0
Training loss: 4.665316581726074
Validation loss: 5.338709433873494

Epoch: 6| Step: 1
Training loss: 6.260590553283691
Validation loss: 5.336231629053752

Epoch: 6| Step: 2
Training loss: 6.009450912475586
Validation loss: 5.3336968421936035

Epoch: 6| Step: 3
Training loss: 5.496829986572266
Validation loss: 5.331041097640991

Epoch: 6| Step: 4
Training loss: 5.027849197387695
Validation loss: 5.328266143798828

Epoch: 6| Step: 5
Training loss: 5.4003400802612305
Validation loss: 5.325427293777466

Epoch: 6| Step: 6
Training loss: 4.8605194091796875
Validation loss: 5.322426160176595

Epoch: 6| Step: 7
Training loss: 5.66509485244751
Validation loss: 5.319325129191081

Epoch: 6| Step: 8
Training loss: 4.689807415008545
Validation loss: 5.316122810045878

Epoch: 6| Step: 9
Training loss: 6.2508745193481445
Validation loss: 5.312687794367473

Epoch: 6| Step: 10
Training loss: 4.793583869934082
Validation loss: 5.309076150258382

Epoch: 6| Step: 11
Training loss: 5.68915319442749
Validation loss: 5.305354992548625

Epoch: 6| Step: 12
Training loss: 4.742574214935303
Validation loss: 5.301344633102417

Epoch: 6| Step: 13
Training loss: 5.886308670043945
Validation loss: 5.297122160593669

Epoch: 3| Step: 0
Training loss: 6.011414527893066
Validation loss: 5.292502800623576

Epoch: 6| Step: 1
Training loss: 4.69998836517334
Validation loss: 5.28790299097697

Epoch: 6| Step: 2
Training loss: 5.790328502655029
Validation loss: 5.283023039499919

Epoch: 6| Step: 3
Training loss: 5.776648998260498
Validation loss: 5.27770749727885

Epoch: 6| Step: 4
Training loss: 5.223435878753662
Validation loss: 5.272198438644409

Epoch: 6| Step: 5
Training loss: 5.569031715393066
Validation loss: 5.266370137532552

Epoch: 6| Step: 6
Training loss: 4.727215766906738
Validation loss: 5.260103940963745

Epoch: 6| Step: 7
Training loss: 5.034905433654785
Validation loss: 5.253533522288005

Epoch: 6| Step: 8
Training loss: 6.033574104309082
Validation loss: 5.24675710995992

Epoch: 6| Step: 9
Training loss: 5.089380264282227
Validation loss: 5.2396213213602705

Epoch: 6| Step: 10
Training loss: 4.833390235900879
Validation loss: 5.232257604598999

Epoch: 6| Step: 11
Training loss: 4.809393882751465
Validation loss: 5.224386692047119

Epoch: 6| Step: 12
Training loss: 5.0178680419921875
Validation loss: 5.216135740280151

Epoch: 6| Step: 13
Training loss: 5.9358696937561035
Validation loss: 5.207883516947429

Epoch: 4| Step: 0
Training loss: 5.7153472900390625
Validation loss: 5.199448347091675

Epoch: 6| Step: 1
Training loss: 3.713308572769165
Validation loss: 5.190505584081014

Epoch: 6| Step: 2
Training loss: 5.840609550476074
Validation loss: 5.181039611498515

Epoch: 6| Step: 3
Training loss: 4.657034873962402
Validation loss: 5.171420892079671

Epoch: 6| Step: 4
Training loss: 5.87952995300293
Validation loss: 5.161503235499064

Epoch: 6| Step: 5
Training loss: 5.089015007019043
Validation loss: 5.151301145553589

Epoch: 6| Step: 6
Training loss: 5.271096706390381
Validation loss: 5.140895128250122

Epoch: 6| Step: 7
Training loss: 5.3345723152160645
Validation loss: 5.130176226298015

Epoch: 6| Step: 8
Training loss: 5.117762565612793
Validation loss: 5.119329452514648

Epoch: 6| Step: 9
Training loss: 5.679446697235107
Validation loss: 5.108562548955281

Epoch: 6| Step: 10
Training loss: 4.89316463470459
Validation loss: 5.09685476620992

Epoch: 6| Step: 11
Training loss: 3.7218098640441895
Validation loss: 5.085678736368815

Epoch: 6| Step: 12
Training loss: 5.694821357727051
Validation loss: 5.074029962221782

Epoch: 6| Step: 13
Training loss: 6.3072404861450195
Validation loss: 5.062442898750305

Epoch: 5| Step: 0
Training loss: 4.7166748046875
Validation loss: 5.050131718317668

Epoch: 6| Step: 1
Training loss: 5.005998611450195
Validation loss: 5.038832902908325

Epoch: 6| Step: 2
Training loss: 5.5428948402404785
Validation loss: 5.026471853256226

Epoch: 6| Step: 3
Training loss: 5.363732814788818
Validation loss: 5.01464056968689

Epoch: 6| Step: 4
Training loss: 3.846224784851074
Validation loss: 5.002828041712443

Epoch: 6| Step: 5
Training loss: 4.049975395202637
Validation loss: 4.991175373395284

Epoch: 6| Step: 6
Training loss: 5.257594108581543
Validation loss: 4.978796641031901

Epoch: 6| Step: 7
Training loss: 6.370428562164307
Validation loss: 4.967190663019816

Epoch: 6| Step: 8
Training loss: 4.962158203125
Validation loss: 4.955319881439209

Epoch: 6| Step: 9
Training loss: 4.640507698059082
Validation loss: 4.943580547968547

Epoch: 6| Step: 10
Training loss: 5.066817760467529
Validation loss: 4.931724548339844

Epoch: 6| Step: 11
Training loss: 5.8022871017456055
Validation loss: 4.920000831286113

Epoch: 6| Step: 12
Training loss: 4.988505840301514
Validation loss: 4.908762137095134

Epoch: 6| Step: 13
Training loss: 5.182229518890381
Validation loss: 4.897513031959534

Epoch: 6| Step: 0
Training loss: 4.8042449951171875
Validation loss: 4.886831323305766

Epoch: 6| Step: 1
Training loss: 5.086474418640137
Validation loss: 4.875933090845744

Epoch: 6| Step: 2
Training loss: 4.6811113357543945
Validation loss: 4.865100383758545

Epoch: 6| Step: 3
Training loss: 5.581233024597168
Validation loss: 4.854437748591105

Epoch: 6| Step: 4
Training loss: 5.197579860687256
Validation loss: 4.844020326932271

Epoch: 6| Step: 5
Training loss: 5.459529876708984
Validation loss: 4.8333704471588135

Epoch: 6| Step: 6
Training loss: 4.639446258544922
Validation loss: 4.823081096013387

Epoch: 6| Step: 7
Training loss: 4.4856109619140625
Validation loss: 4.812559763590495

Epoch: 6| Step: 8
Training loss: 5.425882339477539
Validation loss: 4.802146991093953

Epoch: 6| Step: 9
Training loss: 4.512089729309082
Validation loss: 4.792410532633464

Epoch: 6| Step: 10
Training loss: 5.277390956878662
Validation loss: 4.781867543856303

Epoch: 6| Step: 11
Training loss: 4.855340003967285
Validation loss: 4.7717593510945635

Epoch: 6| Step: 12
Training loss: 3.609196186065674
Validation loss: 4.761852741241455

Epoch: 6| Step: 13
Training loss: 5.053249359130859
Validation loss: 4.751901586850484

Epoch: 7| Step: 0
Training loss: 4.629137992858887
Validation loss: 4.7424144347508745

Epoch: 6| Step: 1
Training loss: 4.805118083953857
Validation loss: 4.732450564702352

Epoch: 6| Step: 2
Training loss: 5.0862603187561035
Validation loss: 4.721976041793823

Epoch: 6| Step: 3
Training loss: 4.773689270019531
Validation loss: 4.712357759475708

Epoch: 6| Step: 4
Training loss: 5.634435176849365
Validation loss: 4.701979796091716

Epoch: 6| Step: 5
Training loss: 4.256833076477051
Validation loss: 4.6914857228597

Epoch: 6| Step: 6
Training loss: 4.483969211578369
Validation loss: 4.681626598040263

Epoch: 6| Step: 7
Training loss: 4.33967924118042
Validation loss: 4.670819997787476

Epoch: 6| Step: 8
Training loss: 4.198616981506348
Validation loss: 4.660754243532817

Epoch: 6| Step: 9
Training loss: 4.881544589996338
Validation loss: 4.649972041447957

Epoch: 6| Step: 10
Training loss: 3.8431050777435303
Validation loss: 4.639892061551412

Epoch: 6| Step: 11
Training loss: 5.140080451965332
Validation loss: 4.629770437876384

Epoch: 6| Step: 12
Training loss: 5.399524211883545
Validation loss: 4.620395580927531

Epoch: 6| Step: 13
Training loss: 5.3290910720825195
Validation loss: 4.609922488530477

Epoch: 8| Step: 0
Training loss: 6.091921806335449
Validation loss: 4.60092822710673

Epoch: 6| Step: 1
Training loss: 4.562249660491943
Validation loss: 4.591322024663289

Epoch: 6| Step: 2
Training loss: 4.601201057434082
Validation loss: 4.582009951273601

Epoch: 6| Step: 3
Training loss: 4.834367752075195
Validation loss: 4.57279356320699

Epoch: 6| Step: 4
Training loss: 4.760747909545898
Validation loss: 4.56433121363322

Epoch: 6| Step: 5
Training loss: 4.1189775466918945
Validation loss: 4.555647452672322

Epoch: 6| Step: 6
Training loss: 4.557817459106445
Validation loss: 4.548023223876953

Epoch: 6| Step: 7
Training loss: 4.157233715057373
Validation loss: 4.5391996304194135

Epoch: 6| Step: 8
Training loss: 4.353333473205566
Validation loss: 4.531292041142781

Epoch: 6| Step: 9
Training loss: 4.74667501449585
Validation loss: 4.523668050765991

Epoch: 6| Step: 10
Training loss: 4.955593109130859
Validation loss: 4.515267491340637

Epoch: 6| Step: 11
Training loss: 5.2089385986328125
Validation loss: 4.5075340668360395

Epoch: 6| Step: 12
Training loss: 4.618847370147705
Validation loss: 4.499617616335551

Epoch: 6| Step: 13
Training loss: 3.4936201572418213
Validation loss: 4.491367181142171

Epoch: 9| Step: 0
Training loss: 4.416903495788574
Validation loss: 4.483413815498352

Epoch: 6| Step: 1
Training loss: 4.912531852722168
Validation loss: 4.475603858629863

Epoch: 6| Step: 2
Training loss: 3.5314745903015137
Validation loss: 4.467905640602112

Epoch: 6| Step: 3
Training loss: 5.258931636810303
Validation loss: 4.4603564739227295

Epoch: 6| Step: 4
Training loss: 4.406890392303467
Validation loss: 4.452778140703837

Epoch: 6| Step: 5
Training loss: 4.176703453063965
Validation loss: 4.446685115496318

Epoch: 6| Step: 6
Training loss: 5.021772384643555
Validation loss: 4.438845952351888

Epoch: 6| Step: 7
Training loss: 4.882148742675781
Validation loss: 4.4318777322769165

Epoch: 6| Step: 8
Training loss: 4.948805809020996
Validation loss: 4.425001303354899

Epoch: 6| Step: 9
Training loss: 4.765015602111816
Validation loss: 4.418284614880879

Epoch: 6| Step: 10
Training loss: 4.08880615234375
Validation loss: 4.411411325136821

Epoch: 6| Step: 11
Training loss: 4.50351095199585
Validation loss: 4.4051187833150225

Epoch: 6| Step: 12
Training loss: 3.6534860134124756
Validation loss: 4.398495674133301

Epoch: 6| Step: 13
Training loss: 5.126681327819824
Validation loss: 4.391857226689656

Epoch: 10| Step: 0
Training loss: 4.851515769958496
Validation loss: 4.38593606154124

Epoch: 6| Step: 1
Training loss: 4.033138275146484
Validation loss: 4.380259116490682

Epoch: 6| Step: 2
Training loss: 3.7449300289154053
Validation loss: 4.374175945917766

Epoch: 6| Step: 3
Training loss: 5.45530891418457
Validation loss: 4.368845462799072

Epoch: 6| Step: 4
Training loss: 4.9832353591918945
Validation loss: 4.36322299639384

Epoch: 6| Step: 5
Training loss: 4.954690933227539
Validation loss: 4.357358654340108

Epoch: 6| Step: 6
Training loss: 4.08139705657959
Validation loss: 4.351185878117879

Epoch: 6| Step: 7
Training loss: 4.508070945739746
Validation loss: 4.34494670232137

Epoch: 6| Step: 8
Training loss: 4.734234809875488
Validation loss: 4.339043140411377

Epoch: 6| Step: 9
Training loss: 5.388794898986816
Validation loss: 4.333892742792766

Epoch: 6| Step: 10
Training loss: 5.38006591796875
Validation loss: 4.327798207600911

Epoch: 6| Step: 11
Training loss: 2.2505011558532715
Validation loss: 4.3216047286987305

Epoch: 6| Step: 12
Training loss: 4.08973503112793
Validation loss: 4.315906008084615

Epoch: 6| Step: 13
Training loss: 4.093976020812988
Validation loss: 4.308980743090312

Epoch: 11| Step: 0
Training loss: 4.504677772521973
Validation loss: 4.30251677831014

Epoch: 6| Step: 1
Training loss: 4.842815399169922
Validation loss: 4.295970916748047

Epoch: 6| Step: 2
Training loss: 5.369857311248779
Validation loss: 4.28907310962677

Epoch: 6| Step: 3
Training loss: 4.3534040451049805
Validation loss: 4.283151467641194

Epoch: 6| Step: 4
Training loss: 4.275473594665527
Validation loss: 4.276984453201294

Epoch: 6| Step: 5
Training loss: 3.9229207038879395
Validation loss: 4.270210146903992

Epoch: 6| Step: 6
Training loss: 4.128314018249512
Validation loss: 4.263916412989299

Epoch: 6| Step: 7
Training loss: 4.716316223144531
Validation loss: 4.257860700289409

Epoch: 6| Step: 8
Training loss: 4.471996784210205
Validation loss: 4.251622041066487

Epoch: 6| Step: 9
Training loss: 4.651594638824463
Validation loss: 4.245832284291585

Epoch: 6| Step: 10
Training loss: 4.209759712219238
Validation loss: 4.240078091621399

Epoch: 6| Step: 11
Training loss: 4.514319896697998
Validation loss: 4.234311819076538

Epoch: 6| Step: 12
Training loss: 4.392547607421875
Validation loss: 4.227973302205403

Epoch: 6| Step: 13
Training loss: 3.1005282402038574
Validation loss: 4.222823739051819

Epoch: 12| Step: 0
Training loss: 4.699594497680664
Validation loss: 4.21772023042043

Epoch: 6| Step: 1
Training loss: 4.027323246002197
Validation loss: 4.211177229881287

Epoch: 6| Step: 2
Training loss: 4.438418865203857
Validation loss: 4.2062294483184814

Epoch: 6| Step: 3
Training loss: 4.996387004852295
Validation loss: 4.201027115186055

Epoch: 6| Step: 4
Training loss: 3.972464084625244
Validation loss: 4.195359985033671

Epoch: 6| Step: 5
Training loss: 3.7701053619384766
Validation loss: 4.189304908116658

Epoch: 6| Step: 6
Training loss: 4.673330783843994
Validation loss: 4.184213995933533

Epoch: 6| Step: 7
Training loss: 4.640432834625244
Validation loss: 4.1790473858515425

Epoch: 6| Step: 8
Training loss: 3.4064249992370605
Validation loss: 4.173976302146912

Epoch: 6| Step: 9
Training loss: 4.952014446258545
Validation loss: 4.168781836827596

Epoch: 6| Step: 10
Training loss: 5.075950622558594
Validation loss: 4.163212498029073

Epoch: 6| Step: 11
Training loss: 4.216123104095459
Validation loss: 4.1576222976048784

Epoch: 6| Step: 12
Training loss: 4.106142044067383
Validation loss: 4.152230620384216

Epoch: 6| Step: 13
Training loss: 3.449035167694092
Validation loss: 4.146967212359111

Epoch: 13| Step: 0
Training loss: 4.000560283660889
Validation loss: 4.140907804171245

Epoch: 6| Step: 1
Training loss: 4.274182319641113
Validation loss: 4.135218540827434

Epoch: 6| Step: 2
Training loss: 3.375138521194458
Validation loss: 4.1294727722803755

Epoch: 6| Step: 3
Training loss: 4.266490459442139
Validation loss: 4.124126672744751

Epoch: 6| Step: 4
Training loss: 2.9789228439331055
Validation loss: 4.118604461352031

Epoch: 6| Step: 5
Training loss: 3.8657593727111816
Validation loss: 4.113011320432027

Epoch: 6| Step: 6
Training loss: 5.22714376449585
Validation loss: 4.1077626546223955

Epoch: 6| Step: 7
Training loss: 4.889482498168945
Validation loss: 4.102010687192281

Epoch: 6| Step: 8
Training loss: 4.752257347106934
Validation loss: 4.095548550287883

Epoch: 6| Step: 9
Training loss: 4.387159824371338
Validation loss: 4.089381774266561

Epoch: 6| Step: 10
Training loss: 4.783936977386475
Validation loss: 4.0842658678690595

Epoch: 6| Step: 11
Training loss: 5.028838157653809
Validation loss: 4.0779662529627485

Epoch: 6| Step: 12
Training loss: 3.35404896736145
Validation loss: 4.072256247202556

Epoch: 6| Step: 13
Training loss: 4.245682716369629
Validation loss: 4.06681764125824

Epoch: 14| Step: 0
Training loss: 4.352646350860596
Validation loss: 4.062036434809367

Epoch: 6| Step: 1
Training loss: 3.493640899658203
Validation loss: 4.056463400522868

Epoch: 6| Step: 2
Training loss: 4.550321578979492
Validation loss: 4.050630489985148

Epoch: 6| Step: 3
Training loss: 3.723461866378784
Validation loss: 4.044086853663127

Epoch: 6| Step: 4
Training loss: 4.451664924621582
Validation loss: 4.038605451583862

Epoch: 6| Step: 5
Training loss: 4.083589553833008
Validation loss: 4.032817999521892

Epoch: 6| Step: 6
Training loss: 4.314751148223877
Validation loss: 4.027261416117351

Epoch: 6| Step: 7
Training loss: 4.053047180175781
Validation loss: 4.020827492078145

Epoch: 6| Step: 8
Training loss: 4.867818832397461
Validation loss: 4.016175429026286

Epoch: 6| Step: 9
Training loss: 3.468385934829712
Validation loss: 4.010778069496155

Epoch: 6| Step: 10
Training loss: 4.331280708312988
Validation loss: 4.005207896232605

Epoch: 6| Step: 11
Training loss: 5.080426216125488
Validation loss: 3.9988057613372803

Epoch: 6| Step: 12
Training loss: 4.034260272979736
Validation loss: 3.9930260181427

Epoch: 6| Step: 13
Training loss: 3.5406901836395264
Validation loss: 3.987844983736674

Epoch: 15| Step: 0
Training loss: 4.264544486999512
Validation loss: 3.9821940263112388

Epoch: 6| Step: 1
Training loss: 3.5075087547302246
Validation loss: 3.976330121358236

Epoch: 6| Step: 2
Training loss: 4.607214450836182
Validation loss: 3.9709513584772744

Epoch: 6| Step: 3
Training loss: 3.350219488143921
Validation loss: 3.9645580848058066

Epoch: 6| Step: 4
Training loss: 3.928525447845459
Validation loss: 3.960265278816223

Epoch: 6| Step: 5
Training loss: 3.9200048446655273
Validation loss: 3.9546679655710855

Epoch: 6| Step: 6
Training loss: 4.598477363586426
Validation loss: 3.9501429398854575

Epoch: 6| Step: 7
Training loss: 4.234899997711182
Validation loss: 3.9436521530151367

Epoch: 6| Step: 8
Training loss: 3.2926502227783203
Validation loss: 3.938129981358846

Epoch: 6| Step: 9
Training loss: 5.168629169464111
Validation loss: 3.931626319885254

Epoch: 6| Step: 10
Training loss: 4.246639251708984
Validation loss: 3.926001230875651

Epoch: 6| Step: 11
Training loss: 3.4528493881225586
Validation loss: 3.921096603075663

Epoch: 6| Step: 12
Training loss: 4.127652168273926
Validation loss: 3.914714733759562

Epoch: 6| Step: 13
Training loss: 4.602248191833496
Validation loss: 3.909544308980306

Epoch: 16| Step: 0
Training loss: 4.663053512573242
Validation loss: 3.903956095377604

Epoch: 6| Step: 1
Training loss: 3.479863166809082
Validation loss: 3.899176319440206

Epoch: 6| Step: 2
Training loss: 3.3678224086761475
Validation loss: 3.8945553302764893

Epoch: 6| Step: 3
Training loss: 4.7932233810424805
Validation loss: 3.888472000757853

Epoch: 6| Step: 4
Training loss: 4.150446891784668
Validation loss: 3.8829116821289062

Epoch: 6| Step: 5
Training loss: 3.814066171646118
Validation loss: 3.877560575803121

Epoch: 6| Step: 6
Training loss: 3.2513749599456787
Validation loss: 3.871839483579

Epoch: 6| Step: 7
Training loss: 4.748813629150391
Validation loss: 3.866556485493978

Epoch: 6| Step: 8
Training loss: 3.806428909301758
Validation loss: 3.860919237136841

Epoch: 6| Step: 9
Training loss: 4.424717426300049
Validation loss: 3.8557598193486533

Epoch: 6| Step: 10
Training loss: 4.005769729614258
Validation loss: 3.851084311803182

Epoch: 6| Step: 11
Training loss: 3.356560468673706
Validation loss: 3.846086263656616

Epoch: 6| Step: 12
Training loss: 3.8105568885803223
Validation loss: 3.840179681777954

Epoch: 6| Step: 13
Training loss: 4.6251630783081055
Validation loss: 3.835593899091085

Epoch: 17| Step: 0
Training loss: 3.847266674041748
Validation loss: 3.829746127128601

Epoch: 6| Step: 1
Training loss: 3.1860523223876953
Validation loss: 3.825367053349813

Epoch: 6| Step: 2
Training loss: 4.281764507293701
Validation loss: 3.8196678161621094

Epoch: 6| Step: 3
Training loss: 4.043711185455322
Validation loss: 3.8149392207463584

Epoch: 6| Step: 4
Training loss: 4.00830602645874
Validation loss: 3.809044679005941

Epoch: 6| Step: 5
Training loss: 3.9847211837768555
Validation loss: 3.8038766781489053

Epoch: 6| Step: 6
Training loss: 4.331051826477051
Validation loss: 3.799399455388387

Epoch: 6| Step: 7
Training loss: 3.8672852516174316
Validation loss: 3.7948413689931235

Epoch: 6| Step: 8
Training loss: 4.84453821182251
Validation loss: 3.7900797526041665

Epoch: 6| Step: 9
Training loss: 4.079488277435303
Validation loss: 3.7840609550476074

Epoch: 6| Step: 10
Training loss: 4.026874542236328
Validation loss: 3.7784926493962607

Epoch: 6| Step: 11
Training loss: 3.459935426712036
Validation loss: 3.773867964744568

Epoch: 6| Step: 12
Training loss: 4.100739479064941
Validation loss: 3.7689900000890098

Epoch: 6| Step: 13
Training loss: 3.2844581604003906
Validation loss: 3.7642125288645425

Epoch: 18| Step: 0
Training loss: 4.41614294052124
Validation loss: 3.7594460248947144

Epoch: 6| Step: 1
Training loss: 3.09024715423584
Validation loss: 3.7534327507019043

Epoch: 6| Step: 2
Training loss: 4.584786415100098
Validation loss: 3.7485241889953613

Epoch: 6| Step: 3
Training loss: 3.968756914138794
Validation loss: 3.7439120610555015

Epoch: 6| Step: 4
Training loss: 2.772813558578491
Validation loss: 3.7389864126841226

Epoch: 6| Step: 5
Training loss: 3.3824963569641113
Validation loss: 3.7340297301610312

Epoch: 6| Step: 6
Training loss: 3.734957456588745
Validation loss: 3.730486035346985

Epoch: 6| Step: 7
Training loss: 5.114402770996094
Validation loss: 3.7247595389684043

Epoch: 6| Step: 8
Training loss: 4.1229705810546875
Validation loss: 3.7192153930664062

Epoch: 6| Step: 9
Training loss: 3.3702523708343506
Validation loss: 3.7137445211410522

Epoch: 6| Step: 10
Training loss: 3.8955907821655273
Validation loss: 3.7079832553863525

Epoch: 6| Step: 11
Training loss: 4.390533924102783
Validation loss: 3.7033002773920694

Epoch: 6| Step: 12
Training loss: 3.697324275970459
Validation loss: 3.697697321573893

Epoch: 6| Step: 13
Training loss: 3.87760591506958
Validation loss: 3.6926066080729165

Epoch: 19| Step: 0
Training loss: 3.4373159408569336
Validation loss: 3.6874358654022217

Epoch: 6| Step: 1
Training loss: 3.848863363265991
Validation loss: 3.682796915372213

Epoch: 6| Step: 2
Training loss: 4.033919334411621
Validation loss: 3.678343574206034

Epoch: 6| Step: 3
Training loss: 4.470361709594727
Validation loss: 3.6727968056996665

Epoch: 6| Step: 4
Training loss: 2.889528274536133
Validation loss: 3.6683163245519004

Epoch: 6| Step: 5
Training loss: 4.057497024536133
Validation loss: 3.664129892985026

Epoch: 6| Step: 6
Training loss: 2.826916217803955
Validation loss: 3.6592456499735513

Epoch: 6| Step: 7
Training loss: 3.324368476867676
Validation loss: 3.6542982260386148

Epoch: 6| Step: 8
Training loss: 5.161785125732422
Validation loss: 3.6499808629353843

Epoch: 6| Step: 9
Training loss: 4.115407943725586
Validation loss: 3.645687143007914

Epoch: 6| Step: 10
Training loss: 2.8399312496185303
Validation loss: 3.6408071517944336

Epoch: 6| Step: 11
Training loss: 3.835019111633301
Validation loss: 3.635297934214274

Epoch: 6| Step: 12
Training loss: 4.046259880065918
Validation loss: 3.631485342979431

Epoch: 6| Step: 13
Training loss: 4.5544962882995605
Validation loss: 3.6274087031682334

Epoch: 20| Step: 0
Training loss: 2.798957109451294
Validation loss: 3.6216811736424765

Epoch: 6| Step: 1
Training loss: 3.211580276489258
Validation loss: 3.616953174273173

Epoch: 6| Step: 2
Training loss: 4.501002311706543
Validation loss: 3.6123945315678916

Epoch: 6| Step: 3
Training loss: 2.9756228923797607
Validation loss: 3.6083404620488486

Epoch: 6| Step: 4
Training loss: 3.779611110687256
Validation loss: 3.603482166926066

Epoch: 6| Step: 5
Training loss: 4.546290397644043
Validation loss: 3.5986223618189492

Epoch: 6| Step: 6
Training loss: 4.3142170906066895
Validation loss: 3.5935802459716797

Epoch: 6| Step: 7
Training loss: 4.164736747741699
Validation loss: 3.5897612969080606

Epoch: 6| Step: 8
Training loss: 4.207265853881836
Validation loss: 3.5852320194244385

Epoch: 6| Step: 9
Training loss: 4.515416622161865
Validation loss: 3.580161770184835

Epoch: 6| Step: 10
Training loss: 2.530134677886963
Validation loss: 3.57561985651652

Epoch: 6| Step: 11
Training loss: 3.526329517364502
Validation loss: 3.5704967975616455

Epoch: 6| Step: 12
Training loss: 3.756072998046875
Validation loss: 3.5654563109079995

Epoch: 6| Step: 13
Training loss: 3.7260847091674805
Validation loss: 3.5604255199432373

Epoch: 21| Step: 0
Training loss: 4.101212501525879
Validation loss: 3.556805729866028

Epoch: 6| Step: 1
Training loss: 4.065682888031006
Validation loss: 3.5509599447250366

Epoch: 6| Step: 2
Training loss: 4.294589996337891
Validation loss: 3.5460692246754966

Epoch: 6| Step: 3
Training loss: 3.3841893672943115
Validation loss: 3.5410106976826987

Epoch: 6| Step: 4
Training loss: 3.5419812202453613
Validation loss: 3.5363059838612876

Epoch: 6| Step: 5
Training loss: 4.045751571655273
Validation loss: 3.5313148895899453

Epoch: 6| Step: 6
Training loss: 2.6473772525787354
Validation loss: 3.5270585219065347

Epoch: 6| Step: 7
Training loss: 3.622588634490967
Validation loss: 3.5225743452707925

Epoch: 6| Step: 8
Training loss: 2.7270219326019287
Validation loss: 3.5174114306767783

Epoch: 6| Step: 9
Training loss: 3.745945692062378
Validation loss: 3.513237237930298

Epoch: 6| Step: 10
Training loss: 3.729626417160034
Validation loss: 3.5090078115463257

Epoch: 6| Step: 11
Training loss: 3.7214787006378174
Validation loss: 3.5039983987808228

Epoch: 6| Step: 12
Training loss: 4.644563674926758
Validation loss: 3.4997976223627725

Epoch: 6| Step: 13
Training loss: 3.40162992477417
Validation loss: 3.4952503045399985

Epoch: 22| Step: 0
Training loss: 3.322567939758301
Validation loss: 3.489979902903239

Epoch: 6| Step: 1
Training loss: 3.809218168258667
Validation loss: 3.4854738314946494

Epoch: 6| Step: 2
Training loss: 3.8620364665985107
Validation loss: 3.4805469512939453

Epoch: 6| Step: 3
Training loss: 3.756535053253174
Validation loss: 3.475881735483805

Epoch: 6| Step: 4
Training loss: 3.825608730316162
Validation loss: 3.4710700114568076

Epoch: 6| Step: 5
Training loss: 3.7296133041381836
Validation loss: 3.466241717338562

Epoch: 6| Step: 6
Training loss: 3.7166600227355957
Validation loss: 3.462446133295695

Epoch: 6| Step: 7
Training loss: 2.64638090133667
Validation loss: 3.4563103914260864

Epoch: 6| Step: 8
Training loss: 3.8709216117858887
Validation loss: 3.452536384264628

Epoch: 6| Step: 9
Training loss: 3.2724170684814453
Validation loss: 3.447533210118612

Epoch: 6| Step: 10
Training loss: 2.9296560287475586
Validation loss: 3.4431904554367065

Epoch: 6| Step: 11
Training loss: 3.121912956237793
Validation loss: 3.4382592837015786

Epoch: 6| Step: 12
Training loss: 4.2720441818237305
Validation loss: 3.4341074228286743

Epoch: 6| Step: 13
Training loss: 4.653069972991943
Validation loss: 3.429568370183309

Epoch: 23| Step: 0
Training loss: 3.2913525104522705
Validation loss: 3.425032138824463

Epoch: 6| Step: 1
Training loss: 3.476975440979004
Validation loss: 3.4209700425465903

Epoch: 6| Step: 2
Training loss: 2.681626558303833
Validation loss: 3.4167209466298423

Epoch: 6| Step: 3
Training loss: 3.774925708770752
Validation loss: 3.4121479590733848

Epoch: 6| Step: 4
Training loss: 3.7552623748779297
Validation loss: 3.408329804738363

Epoch: 6| Step: 5
Training loss: 3.569769859313965
Validation loss: 3.4038248856862388

Epoch: 6| Step: 6
Training loss: 3.89189076423645
Validation loss: 3.399106740951538

Epoch: 6| Step: 7
Training loss: 4.104541778564453
Validation loss: 3.395295778910319

Epoch: 6| Step: 8
Training loss: 4.311990737915039
Validation loss: 3.390965739885966

Epoch: 6| Step: 9
Training loss: 2.398932456970215
Validation loss: 3.385913888613383

Epoch: 6| Step: 10
Training loss: 3.0665855407714844
Validation loss: 3.3816147247950235

Epoch: 6| Step: 11
Training loss: 3.6089959144592285
Validation loss: 3.377634366353353

Epoch: 6| Step: 12
Training loss: 4.004215240478516
Validation loss: 3.372759739557902

Epoch: 6| Step: 13
Training loss: 3.9700536727905273
Validation loss: 3.368454019228617

Epoch: 24| Step: 0
Training loss: 3.707568645477295
Validation loss: 3.364560882250468

Epoch: 6| Step: 1
Training loss: 3.0658793449401855
Validation loss: 3.359663407007853

Epoch: 6| Step: 2
Training loss: 2.9924919605255127
Validation loss: 3.355197032292684

Epoch: 6| Step: 3
Training loss: 2.8990464210510254
Validation loss: 3.3502779404322305

Epoch: 6| Step: 4
Training loss: 4.445085525512695
Validation loss: 3.3460414012273154

Epoch: 6| Step: 5
Training loss: 3.798092842102051
Validation loss: 3.3413882652918496

Epoch: 6| Step: 6
Training loss: 2.855224609375
Validation loss: 3.336663603782654

Epoch: 6| Step: 7
Training loss: 3.5126888751983643
Validation loss: 3.3325910170873008

Epoch: 6| Step: 8
Training loss: 4.502261161804199
Validation loss: 3.328442096710205

Epoch: 6| Step: 9
Training loss: 3.9568848609924316
Validation loss: 3.323705275853475

Epoch: 6| Step: 10
Training loss: 2.8560571670532227
Validation loss: 3.318546772003174

Epoch: 6| Step: 11
Training loss: 3.692577838897705
Validation loss: 3.314311941464742

Epoch: 6| Step: 12
Training loss: 3.321073532104492
Validation loss: 3.309407194455465

Epoch: 6| Step: 13
Training loss: 3.496676445007324
Validation loss: 3.3067532777786255

Epoch: 25| Step: 0
Training loss: 3.109950542449951
Validation loss: 3.303010861078898

Epoch: 6| Step: 1
Training loss: 3.3353452682495117
Validation loss: 3.296930472056071

Epoch: 6| Step: 2
Training loss: 4.325311183929443
Validation loss: 3.294145941734314

Epoch: 6| Step: 3
Training loss: 3.3312253952026367
Validation loss: 3.2895583311716714

Epoch: 6| Step: 4
Training loss: 3.193535089492798
Validation loss: 3.2850584983825684

Epoch: 6| Step: 5
Training loss: 3.4019296169281006
Validation loss: 3.2809120814005532

Epoch: 6| Step: 6
Training loss: 4.18464994430542
Validation loss: 3.2769217093785605

Epoch: 6| Step: 7
Training loss: 2.889854669570923
Validation loss: 3.2734436988830566

Epoch: 6| Step: 8
Training loss: 3.3874030113220215
Validation loss: 3.2693239053090415

Epoch: 6| Step: 9
Training loss: 4.06002950668335
Validation loss: 3.2651410897572837

Epoch: 6| Step: 10
Training loss: 4.099341869354248
Validation loss: 3.26103146870931

Epoch: 6| Step: 11
Training loss: 3.3444361686706543
Validation loss: 3.2567848761876426

Epoch: 6| Step: 12
Training loss: 2.764453887939453
Validation loss: 3.252415577570597

Epoch: 6| Step: 13
Training loss: 2.9061431884765625
Validation loss: 3.247678279876709

Epoch: 26| Step: 0
Training loss: 2.825637102127075
Validation loss: 3.244195302327474

Epoch: 6| Step: 1
Training loss: 3.7084527015686035
Validation loss: 3.2403735319773355

Epoch: 6| Step: 2
Training loss: 4.819538116455078
Validation loss: 3.23647940158844

Epoch: 6| Step: 3
Training loss: 3.2622013092041016
Validation loss: 3.232410212357839

Epoch: 6| Step: 4
Training loss: 3.2723965644836426
Validation loss: 3.2300302187601724

Epoch: 6| Step: 5
Training loss: 3.1780905723571777
Validation loss: 3.227669676144918

Epoch: 6| Step: 6
Training loss: 3.2836785316467285
Validation loss: 3.2225982745488486

Epoch: 6| Step: 7
Training loss: 3.9859414100646973
Validation loss: 3.2178005377451577

Epoch: 6| Step: 8
Training loss: 3.048976182937622
Validation loss: 3.2152721087137857

Epoch: 6| Step: 9
Training loss: 2.76259183883667
Validation loss: 3.214088757832845

Epoch: 6| Step: 10
Training loss: 2.911496639251709
Validation loss: 3.2116805712381997

Epoch: 6| Step: 11
Training loss: 3.5560405254364014
Validation loss: 3.2076921860376992

Epoch: 6| Step: 12
Training loss: 4.013003826141357
Validation loss: 3.2015879154205322

Epoch: 6| Step: 13
Training loss: 2.9594826698303223
Validation loss: 3.195500055948893

Epoch: 27| Step: 0
Training loss: 1.9917620420455933
Validation loss: 3.1906110445658364

Epoch: 6| Step: 1
Training loss: 3.8189001083374023
Validation loss: 3.1881752411524453

Epoch: 6| Step: 2
Training loss: 3.353260040283203
Validation loss: 3.187964161237081

Epoch: 6| Step: 3
Training loss: 3.0210657119750977
Validation loss: 3.202448844909668

Epoch: 6| Step: 4
Training loss: 3.829252243041992
Validation loss: 3.183650175730387

Epoch: 6| Step: 5
Training loss: 3.8947410583496094
Validation loss: 3.1822468042373657

Epoch: 6| Step: 6
Training loss: 2.5538711547851562
Validation loss: 3.1678221623102822

Epoch: 6| Step: 7
Training loss: 3.4037058353424072
Validation loss: 3.1633647680282593

Epoch: 6| Step: 8
Training loss: 3.2201924324035645
Validation loss: 3.160801331202189

Epoch: 6| Step: 9
Training loss: 4.1575140953063965
Validation loss: 3.1585931380589805

Epoch: 6| Step: 10
Training loss: 3.213944435119629
Validation loss: 3.1568599541982016

Epoch: 6| Step: 11
Training loss: 3.2437589168548584
Validation loss: 3.153846104939779

Epoch: 6| Step: 12
Training loss: 3.204420566558838
Validation loss: 3.1513324975967407

Epoch: 6| Step: 13
Training loss: 4.0333404541015625
Validation loss: 3.145424246788025

Epoch: 28| Step: 0
Training loss: 3.211996555328369
Validation loss: 3.140748898188273

Epoch: 6| Step: 1
Training loss: 3.035889148712158
Validation loss: 3.136501749356588

Epoch: 6| Step: 2
Training loss: 3.1681995391845703
Validation loss: 3.1309211254119873

Epoch: 6| Step: 3
Training loss: 3.099364757537842
Validation loss: 3.1309145291646323

Epoch: 6| Step: 4
Training loss: 3.231393337249756
Validation loss: 3.12676203250885

Epoch: 6| Step: 5
Training loss: 3.0893726348876953
Validation loss: 3.118368665377299

Epoch: 6| Step: 6
Training loss: 4.368074417114258
Validation loss: 3.1130663951238

Epoch: 6| Step: 7
Training loss: 3.0949110984802246
Validation loss: 3.1107300519943237

Epoch: 6| Step: 8
Training loss: 3.7500572204589844
Validation loss: 3.108331044514974

Epoch: 6| Step: 9
Training loss: 3.4273605346679688
Validation loss: 3.1054660876592

Epoch: 6| Step: 10
Training loss: 2.9402778148651123
Validation loss: 3.1005612214406333

Epoch: 6| Step: 11
Training loss: 3.1968297958374023
Validation loss: 3.097314437230428

Epoch: 6| Step: 12
Training loss: 3.212137222290039
Validation loss: 3.0927228132883706

Epoch: 6| Step: 13
Training loss: 3.4322896003723145
Validation loss: 3.0907886823018393

Epoch: 29| Step: 0
Training loss: 2.9269914627075195
Validation loss: 3.0875855684280396

Epoch: 6| Step: 1
Training loss: 3.5352492332458496
Validation loss: 3.0833324591318765

Epoch: 6| Step: 2
Training loss: 2.653602123260498
Validation loss: 3.0766817331314087

Epoch: 6| Step: 3
Training loss: 2.3586957454681396
Validation loss: 3.075480500857035

Epoch: 6| Step: 4
Training loss: 3.2408316135406494
Validation loss: 3.070226510365804

Epoch: 6| Step: 5
Training loss: 2.72170090675354
Validation loss: 3.06522003809611

Epoch: 6| Step: 6
Training loss: 3.516993522644043
Validation loss: 3.0627342065175376

Epoch: 6| Step: 7
Training loss: 3.904048442840576
Validation loss: 3.0586354732513428

Epoch: 6| Step: 8
Training loss: 3.6787891387939453
Validation loss: 3.055333058039347

Epoch: 6| Step: 9
Training loss: 3.405653953552246
Validation loss: 3.0525694290796914

Epoch: 6| Step: 10
Training loss: 3.4278059005737305
Validation loss: 3.049330155054728

Epoch: 6| Step: 11
Training loss: 3.5892889499664307
Validation loss: 3.044770280520121

Epoch: 6| Step: 12
Training loss: 3.303466320037842
Validation loss: 3.0423280000686646

Epoch: 6| Step: 13
Training loss: 3.263740301132202
Validation loss: 3.038802146911621

Epoch: 30| Step: 0
Training loss: 3.194166660308838
Validation loss: 3.0351231892903647

Epoch: 6| Step: 1
Training loss: 3.0204944610595703
Validation loss: 3.033802072207133

Epoch: 6| Step: 2
Training loss: 2.144662380218506
Validation loss: 3.028708577156067

Epoch: 6| Step: 3
Training loss: 4.150147438049316
Validation loss: 3.0265247424443564

Epoch: 6| Step: 4
Training loss: 3.0969338417053223
Validation loss: 3.0234519243240356

Epoch: 6| Step: 5
Training loss: 3.0168540477752686
Validation loss: 3.022136092185974

Epoch: 6| Step: 6
Training loss: 4.183721542358398
Validation loss: 3.0190931955973306

Epoch: 6| Step: 7
Training loss: 2.1810951232910156
Validation loss: 3.0148441394170127

Epoch: 6| Step: 8
Training loss: 2.362450122833252
Validation loss: 3.0102797746658325

Epoch: 6| Step: 9
Training loss: 2.603186845779419
Validation loss: 3.0066457986831665

Epoch: 6| Step: 10
Training loss: 3.5896852016448975
Validation loss: 3.0031286478042603

Epoch: 6| Step: 11
Training loss: 4.169065475463867
Validation loss: 2.999274571736654

Epoch: 6| Step: 12
Training loss: 3.1239891052246094
Validation loss: 2.996423602104187

Epoch: 6| Step: 13
Training loss: 4.057877063751221
Validation loss: 2.991940180460612

Epoch: 31| Step: 0
Training loss: 3.46415638923645
Validation loss: 2.989484190940857

Epoch: 6| Step: 1
Training loss: 2.602099657058716
Validation loss: 2.9864237705866494

Epoch: 6| Step: 2
Training loss: 2.8958287239074707
Validation loss: 2.9834042390187583

Epoch: 6| Step: 3
Training loss: 3.246156692504883
Validation loss: 2.978975017865499

Epoch: 6| Step: 4
Training loss: 4.101047992706299
Validation loss: 2.977966785430908

Epoch: 6| Step: 5
Training loss: 3.0113019943237305
Validation loss: 2.9733888308207193

Epoch: 6| Step: 6
Training loss: 3.620673418045044
Validation loss: 2.967730681101481

Epoch: 6| Step: 7
Training loss: 3.0992727279663086
Validation loss: 2.9634004831314087

Epoch: 6| Step: 8
Training loss: 3.3528616428375244
Validation loss: 2.960236668586731

Epoch: 6| Step: 9
Training loss: 2.601118326187134
Validation loss: 2.957591692606608

Epoch: 6| Step: 10
Training loss: 3.073727607727051
Validation loss: 2.954095443089803

Epoch: 6| Step: 11
Training loss: 2.915648937225342
Validation loss: 2.951919515927633

Epoch: 6| Step: 12
Training loss: 3.0719404220581055
Validation loss: 2.9496986865997314

Epoch: 6| Step: 13
Training loss: 3.227074146270752
Validation loss: 2.9477577606836953

Epoch: 32| Step: 0
Training loss: 3.098903179168701
Validation loss: 2.94415283203125

Epoch: 6| Step: 1
Training loss: 2.9229629039764404
Validation loss: 2.9396955569585166

Epoch: 6| Step: 2
Training loss: 4.163102149963379
Validation loss: 2.9344286918640137

Epoch: 6| Step: 3
Training loss: 2.6164937019348145
Validation loss: 2.929442048072815

Epoch: 6| Step: 4
Training loss: 2.715933322906494
Validation loss: 2.92588218053182

Epoch: 6| Step: 5
Training loss: 3.1462934017181396
Validation loss: 2.9226060112317405

Epoch: 6| Step: 6
Training loss: 3.3067328929901123
Validation loss: 2.918308138847351

Epoch: 6| Step: 7
Training loss: 2.2716853618621826
Validation loss: 2.915650169054667

Epoch: 6| Step: 8
Training loss: 2.6764259338378906
Validation loss: 2.911185304323832

Epoch: 6| Step: 9
Training loss: 3.3655686378479004
Validation loss: 2.90630308787028

Epoch: 6| Step: 10
Training loss: 3.1764769554138184
Validation loss: 2.9089467525482178

Epoch: 6| Step: 11
Training loss: 3.390420913696289
Validation loss: 2.903453270594279

Epoch: 6| Step: 12
Training loss: 3.9247825145721436
Validation loss: 2.9022775093714395

Epoch: 6| Step: 13
Training loss: 2.913477897644043
Validation loss: 2.8980822960535684

Epoch: 33| Step: 0
Training loss: 3.1079928874969482
Validation loss: 2.8946264187494912

Epoch: 6| Step: 1
Training loss: 3.648977756500244
Validation loss: 2.8883050282796225

Epoch: 6| Step: 2
Training loss: 3.1294054985046387
Validation loss: 2.8866310516993203

Epoch: 6| Step: 3
Training loss: 2.9810643196105957
Validation loss: 2.8851994276046753

Epoch: 6| Step: 4
Training loss: 2.8023343086242676
Validation loss: 2.879271388053894

Epoch: 6| Step: 5
Training loss: 2.943910598754883
Validation loss: 2.8758115768432617

Epoch: 6| Step: 6
Training loss: 4.177276611328125
Validation loss: 2.8728748162587485

Epoch: 6| Step: 7
Training loss: 3.366720676422119
Validation loss: 2.8686658143997192

Epoch: 6| Step: 8
Training loss: 2.257131338119507
Validation loss: 2.865640719731649

Epoch: 6| Step: 9
Training loss: 2.755321502685547
Validation loss: 2.8623887300491333

Epoch: 6| Step: 10
Training loss: 3.0606460571289062
Validation loss: 2.8597209453582764

Epoch: 6| Step: 11
Training loss: 2.788961410522461
Validation loss: 2.8568300008773804

Epoch: 6| Step: 12
Training loss: 2.8946101665496826
Validation loss: 2.8536718289057412

Epoch: 6| Step: 13
Training loss: 3.1955013275146484
Validation loss: 2.8513243595759072

Epoch: 34| Step: 0
Training loss: 3.363539457321167
Validation loss: 2.848093787829081

Epoch: 6| Step: 1
Training loss: 2.9791901111602783
Validation loss: 2.8445259730021157

Epoch: 6| Step: 2
Training loss: 3.448270559310913
Validation loss: 2.8414622942606607

Epoch: 6| Step: 3
Training loss: 2.8185272216796875
Validation loss: 2.8373401165008545

Epoch: 6| Step: 4
Training loss: 2.975114107131958
Validation loss: 2.8343085646629333

Epoch: 6| Step: 5
Training loss: 3.4038162231445312
Validation loss: 2.8306397596995034

Epoch: 6| Step: 6
Training loss: 1.972337007522583
Validation loss: 2.8289266427357993

Epoch: 6| Step: 7
Training loss: 3.009319305419922
Validation loss: 2.8264400164286294

Epoch: 6| Step: 8
Training loss: 2.62277889251709
Validation loss: 2.824861526489258

Epoch: 6| Step: 9
Training loss: 3.136584758758545
Validation loss: 2.8249939680099487

Epoch: 6| Step: 10
Training loss: 3.4592976570129395
Validation loss: 2.8229333559672036

Epoch: 6| Step: 11
Training loss: 3.3206543922424316
Validation loss: 2.8158973455429077

Epoch: 6| Step: 12
Training loss: 2.8782620429992676
Validation loss: 2.810409903526306

Epoch: 6| Step: 13
Training loss: 3.166637659072876
Validation loss: 2.807477037111918

Epoch: 35| Step: 0
Training loss: 2.486415386199951
Validation loss: 2.8051883776982627

Epoch: 6| Step: 1
Training loss: 3.0772054195404053
Validation loss: 2.804076592127482

Epoch: 6| Step: 2
Training loss: 3.061387538909912
Validation loss: 2.8024532000223794

Epoch: 6| Step: 3
Training loss: 3.3630311489105225
Validation loss: 2.8002682526906333

Epoch: 6| Step: 4
Training loss: 2.896331787109375
Validation loss: 2.797124226888021

Epoch: 6| Step: 5
Training loss: 3.1178221702575684
Validation loss: 2.794233759244283

Epoch: 6| Step: 6
Training loss: 2.783446788787842
Validation loss: 2.7893972595532737

Epoch: 6| Step: 7
Training loss: 2.6844046115875244
Validation loss: 2.785694440205892

Epoch: 6| Step: 8
Training loss: 2.8976597785949707
Validation loss: 2.783689816792806

Epoch: 6| Step: 9
Training loss: 3.2457876205444336
Validation loss: 2.778363744417826

Epoch: 6| Step: 10
Training loss: 2.5956430435180664
Validation loss: 2.7780142227808633

Epoch: 6| Step: 11
Training loss: 3.5357742309570312
Validation loss: 2.7728224198023477

Epoch: 6| Step: 12
Training loss: 3.3218164443969727
Validation loss: 2.7695232232411704

Epoch: 6| Step: 13
Training loss: 2.935734272003174
Validation loss: 2.768258353074392

Epoch: 36| Step: 0
Training loss: 2.740138292312622
Validation loss: 2.764437754948934

Epoch: 6| Step: 1
Training loss: 2.8910231590270996
Validation loss: 2.7602036794026694

Epoch: 6| Step: 2
Training loss: 2.7454471588134766
Validation loss: 2.7576531171798706

Epoch: 6| Step: 3
Training loss: 2.8980281352996826
Validation loss: 2.7538292010625205

Epoch: 6| Step: 4
Training loss: 3.284914016723633
Validation loss: 2.7543671131134033

Epoch: 6| Step: 5
Training loss: 2.9901957511901855
Validation loss: 2.7484883467356362

Epoch: 6| Step: 6
Training loss: 2.934112548828125
Validation loss: 2.7472912470499673

Epoch: 6| Step: 7
Training loss: 2.3619544506073
Validation loss: 2.744978189468384

Epoch: 6| Step: 8
Training loss: 3.2106688022613525
Validation loss: 2.740385055541992

Epoch: 6| Step: 9
Training loss: 2.8694100379943848
Validation loss: 2.735943635304769

Epoch: 6| Step: 10
Training loss: 3.0011701583862305
Validation loss: 2.7351365884145102

Epoch: 6| Step: 11
Training loss: 2.9368536472320557
Validation loss: 2.730129520098368

Epoch: 6| Step: 12
Training loss: 2.978952169418335
Validation loss: 2.729230364163717

Epoch: 6| Step: 13
Training loss: 3.5116026401519775
Validation loss: 2.725573976834615

Epoch: 37| Step: 0
Training loss: 3.1991899013519287
Validation loss: 2.7248266140619912

Epoch: 6| Step: 1
Training loss: 3.201669454574585
Validation loss: 2.726204752922058

Epoch: 6| Step: 2
Training loss: 2.5544633865356445
Validation loss: 2.7243541876475015

Epoch: 6| Step: 3
Training loss: 2.4398868083953857
Validation loss: 2.7212021748224893

Epoch: 6| Step: 4
Training loss: 2.9120020866394043
Validation loss: 2.725273370742798

Epoch: 6| Step: 5
Training loss: 2.7740769386291504
Validation loss: 2.707736094792684

Epoch: 6| Step: 6
Training loss: 2.592874050140381
Validation loss: 2.7045989831288657

Epoch: 6| Step: 7
Training loss: 2.961843490600586
Validation loss: 2.7004082997639975

Epoch: 6| Step: 8
Training loss: 2.5411465167999268
Validation loss: 2.699692726135254

Epoch: 6| Step: 9
Training loss: 3.4610321521759033
Validation loss: 2.699593424797058

Epoch: 6| Step: 10
Training loss: 2.9156665802001953
Validation loss: 2.6996171871821084

Epoch: 6| Step: 11
Training loss: 2.718303918838501
Validation loss: 2.6971500714619956

Epoch: 6| Step: 12
Training loss: 2.8838772773742676
Validation loss: 2.695696711540222

Epoch: 6| Step: 13
Training loss: 3.589973211288452
Validation loss: 2.693390170733134

Epoch: 38| Step: 0
Training loss: 2.7179813385009766
Validation loss: 2.687509377797445

Epoch: 6| Step: 1
Training loss: 2.2622456550598145
Validation loss: 2.683424393335978

Epoch: 6| Step: 2
Training loss: 3.248316526412964
Validation loss: 2.6792543729146323

Epoch: 6| Step: 3
Training loss: 2.8087191581726074
Validation loss: 2.674240152041117

Epoch: 6| Step: 4
Training loss: 2.724856376647949
Validation loss: 2.6710020303726196

Epoch: 6| Step: 5
Training loss: 2.853295087814331
Validation loss: 2.668994665145874

Epoch: 6| Step: 6
Training loss: 3.4450249671936035
Validation loss: 2.6659148534139

Epoch: 6| Step: 7
Training loss: 3.0791759490966797
Validation loss: 2.6618104378382363

Epoch: 6| Step: 8
Training loss: 2.754611015319824
Validation loss: 2.66281791528066

Epoch: 6| Step: 9
Training loss: 2.7233643531799316
Validation loss: 2.6585880517959595

Epoch: 6| Step: 10
Training loss: 2.822153091430664
Validation loss: 2.653196334838867

Epoch: 6| Step: 11
Training loss: 2.4698433876037598
Validation loss: 2.649512688318888

Epoch: 6| Step: 12
Training loss: 2.9933583736419678
Validation loss: 2.6460357109705606

Epoch: 6| Step: 13
Training loss: 3.2909138202667236
Validation loss: 2.6416921416918435

Epoch: 39| Step: 0
Training loss: 3.597705841064453
Validation loss: 2.6400402386983237

Epoch: 6| Step: 1
Training loss: 2.141808032989502
Validation loss: 2.6360132694244385

Epoch: 6| Step: 2
Training loss: 3.1427106857299805
Validation loss: 2.6328547398249307

Epoch: 6| Step: 3
Training loss: 3.3681087493896484
Validation loss: 2.6284107764561973

Epoch: 6| Step: 4
Training loss: 2.526578187942505
Validation loss: 2.6265806357065835

Epoch: 6| Step: 5
Training loss: 3.1242547035217285
Validation loss: 2.6238930026690164

Epoch: 6| Step: 6
Training loss: 2.3974509239196777
Validation loss: 2.6200788021087646

Epoch: 6| Step: 7
Training loss: 2.6745123863220215
Validation loss: 2.617905577023824

Epoch: 6| Step: 8
Training loss: 3.165919780731201
Validation loss: 2.613722244898478

Epoch: 6| Step: 9
Training loss: 2.1684722900390625
Validation loss: 2.609274903933207

Epoch: 6| Step: 10
Training loss: 3.0521039962768555
Validation loss: 2.6090487440427146

Epoch: 6| Step: 11
Training loss: 2.265641689300537
Validation loss: 2.6090912222862244

Epoch: 6| Step: 12
Training loss: 3.1557998657226562
Validation loss: 2.6116512616475425

Epoch: 6| Step: 13
Training loss: 2.7554612159729004
Validation loss: 2.6088791489601135

Epoch: 40| Step: 0
Training loss: 3.050199031829834
Validation loss: 2.604616562525431

Epoch: 6| Step: 1
Training loss: 2.6047840118408203
Validation loss: 2.5967524449030557

Epoch: 6| Step: 2
Training loss: 2.210291862487793
Validation loss: 2.612112581729889

Epoch: 6| Step: 3
Training loss: 3.0802197456359863
Validation loss: 2.627086877822876

Epoch: 6| Step: 4
Training loss: 2.693699836730957
Validation loss: 2.5865850845972695

Epoch: 6| Step: 5
Training loss: 3.104246139526367
Validation loss: 2.581432263056437

Epoch: 6| Step: 6
Training loss: 2.3824360370635986
Validation loss: 2.5802369515101113

Epoch: 6| Step: 7
Training loss: 2.9672539234161377
Validation loss: 2.583251198132833

Epoch: 6| Step: 8
Training loss: 2.4204294681549072
Validation loss: 2.5896764198939004

Epoch: 6| Step: 9
Training loss: 3.0230345726013184
Validation loss: 2.589776118596395

Epoch: 6| Step: 10
Training loss: 3.2166759967803955
Validation loss: 2.58594020207723

Epoch: 6| Step: 11
Training loss: 3.048718214035034
Validation loss: 2.578848441441854

Epoch: 6| Step: 12
Training loss: 2.8378992080688477
Validation loss: 2.5739099184672036

Epoch: 6| Step: 13
Training loss: 2.27632999420166
Validation loss: 2.5685229301452637

Epoch: 41| Step: 0
Training loss: 2.8840646743774414
Validation loss: 2.5642491579055786

Epoch: 6| Step: 1
Training loss: 2.475080728530884
Validation loss: 2.558558781941732

Epoch: 6| Step: 2
Training loss: 2.91361141204834
Validation loss: 2.551937739054362

Epoch: 6| Step: 3
Training loss: 2.9297423362731934
Validation loss: 2.55207759141922

Epoch: 6| Step: 4
Training loss: 2.5305261611938477
Validation loss: 2.54704083998998

Epoch: 6| Step: 5
Training loss: 2.821957588195801
Validation loss: 2.543018023173014

Epoch: 6| Step: 6
Training loss: 2.3875207901000977
Validation loss: 2.5406383673350015

Epoch: 6| Step: 7
Training loss: 2.9863638877868652
Validation loss: 2.5348164637883506

Epoch: 6| Step: 8
Training loss: 2.769819736480713
Validation loss: 2.531639893849691

Epoch: 6| Step: 9
Training loss: 2.4108779430389404
Validation loss: 2.5273114442825317

Epoch: 6| Step: 10
Training loss: 2.4938161373138428
Validation loss: 2.52472186088562

Epoch: 6| Step: 11
Training loss: 2.6255764961242676
Validation loss: 2.524802327156067

Epoch: 6| Step: 12
Training loss: 2.746142864227295
Validation loss: 2.5244167248408

Epoch: 6| Step: 13
Training loss: 3.4423394203186035
Validation loss: 2.5295368432998657

Epoch: 42| Step: 0
Training loss: 2.9821224212646484
Validation loss: 2.522549033164978

Epoch: 6| Step: 1
Training loss: 2.8553497791290283
Validation loss: 2.523022214571635

Epoch: 6| Step: 2
Training loss: 2.426499366760254
Validation loss: 2.5114280581474304

Epoch: 6| Step: 3
Training loss: 2.3447532653808594
Validation loss: 2.5053083896636963

Epoch: 6| Step: 4
Training loss: 2.3642449378967285
Validation loss: 2.499911904335022

Epoch: 6| Step: 5
Training loss: 3.2236275672912598
Validation loss: 2.4996719559033713

Epoch: 6| Step: 6
Training loss: 3.1598143577575684
Validation loss: 2.4957573215166726

Epoch: 6| Step: 7
Training loss: 2.6162338256835938
Validation loss: 2.4905882676442466

Epoch: 6| Step: 8
Training loss: 2.36537504196167
Validation loss: 2.4916537205378213

Epoch: 6| Step: 9
Training loss: 3.0652480125427246
Validation loss: 2.4925838708877563

Epoch: 6| Step: 10
Training loss: 3.0161972045898438
Validation loss: 2.486302057902018

Epoch: 6| Step: 11
Training loss: 2.1874608993530273
Validation loss: 2.489573836326599

Epoch: 6| Step: 12
Training loss: 2.619370937347412
Validation loss: 2.4856197039286294

Epoch: 6| Step: 13
Training loss: 2.4907026290893555
Validation loss: 2.4816814263661704

Epoch: 43| Step: 0
Training loss: 2.1354050636291504
Validation loss: 2.4722807010014853

Epoch: 6| Step: 1
Training loss: 3.048470973968506
Validation loss: 2.473941425482432

Epoch: 6| Step: 2
Training loss: 2.5264320373535156
Validation loss: 2.4657479524612427

Epoch: 6| Step: 3
Training loss: 3.5189757347106934
Validation loss: 2.4658982157707214

Epoch: 6| Step: 4
Training loss: 2.8062057495117188
Validation loss: 2.4621992309888205

Epoch: 6| Step: 5
Training loss: 2.682408332824707
Validation loss: 2.4563405513763428

Epoch: 6| Step: 6
Training loss: 3.023203134536743
Validation loss: 2.4558793703715005

Epoch: 6| Step: 7
Training loss: 2.4753475189208984
Validation loss: 2.4523821274439492

Epoch: 6| Step: 8
Training loss: 2.507089614868164
Validation loss: 2.4564265410105386

Epoch: 6| Step: 9
Training loss: 2.27527117729187
Validation loss: 2.4516961177190146

Epoch: 6| Step: 10
Training loss: 2.6427783966064453
Validation loss: 2.451099713643392

Epoch: 6| Step: 11
Training loss: 2.660933017730713
Validation loss: 2.44243315855662

Epoch: 6| Step: 12
Training loss: 2.6417489051818848
Validation loss: 2.43872598807017

Epoch: 6| Step: 13
Training loss: 2.0471978187561035
Validation loss: 2.4330952167510986

Epoch: 44| Step: 0
Training loss: 2.911076068878174
Validation loss: 2.4326493740081787

Epoch: 6| Step: 1
Training loss: 2.4313154220581055
Validation loss: 2.4292202591896057

Epoch: 6| Step: 2
Training loss: 2.767409086227417
Validation loss: 2.4268833001454673

Epoch: 6| Step: 3
Training loss: 2.6290907859802246
Validation loss: 2.423775553703308

Epoch: 6| Step: 4
Training loss: 2.6919641494750977
Validation loss: 2.4205212394396463

Epoch: 6| Step: 5
Training loss: 1.981550931930542
Validation loss: 2.4183189868927

Epoch: 6| Step: 6
Training loss: 2.863900899887085
Validation loss: 2.414390285809835

Epoch: 6| Step: 7
Training loss: 2.293440341949463
Validation loss: 2.4118259946505227

Epoch: 6| Step: 8
Training loss: 2.3693103790283203
Validation loss: 2.410536805788676

Epoch: 6| Step: 9
Training loss: 2.8394391536712646
Validation loss: 2.408227324485779

Epoch: 6| Step: 10
Training loss: 2.905592441558838
Validation loss: 2.40552548567454

Epoch: 6| Step: 11
Training loss: 2.2233850955963135
Validation loss: 2.4067344268163047

Epoch: 6| Step: 12
Training loss: 2.9924168586730957
Validation loss: 2.400657375653585

Epoch: 6| Step: 13
Training loss: 2.550996780395508
Validation loss: 2.398264010747274

Epoch: 45| Step: 0
Training loss: 2.423569679260254
Validation loss: 2.397549112637838

Epoch: 6| Step: 1
Training loss: 2.471487283706665
Validation loss: 2.3905229369799295

Epoch: 6| Step: 2
Training loss: 2.1163713932037354
Validation loss: 2.393700957298279

Epoch: 6| Step: 3
Training loss: 2.380228042602539
Validation loss: 2.3948195775349936

Epoch: 6| Step: 4
Training loss: 2.5438380241394043
Validation loss: 2.4038225213686624

Epoch: 6| Step: 5
Training loss: 2.5638608932495117
Validation loss: 2.3955142498016357

Epoch: 6| Step: 6
Training loss: 1.6487746238708496
Validation loss: 2.3940321604410806

Epoch: 6| Step: 7
Training loss: 3.050326347351074
Validation loss: 2.3782637119293213

Epoch: 6| Step: 8
Training loss: 3.0471951961517334
Validation loss: 2.370846410592397

Epoch: 6| Step: 9
Training loss: 1.9452264308929443
Validation loss: 2.367295265197754

Epoch: 6| Step: 10
Training loss: 2.40224552154541
Validation loss: 2.3652106523513794

Epoch: 6| Step: 11
Training loss: 2.8410942554473877
Validation loss: 2.3644238909085593

Epoch: 6| Step: 12
Training loss: 3.495248556137085
Validation loss: 2.3636767069498696

Epoch: 6| Step: 13
Training loss: 2.898200511932373
Validation loss: 2.363852580388387

Epoch: 46| Step: 0
Training loss: 2.7547597885131836
Validation loss: 2.3586263259251914

Epoch: 6| Step: 1
Training loss: 2.923509120941162
Validation loss: 2.359752972920736

Epoch: 6| Step: 2
Training loss: 2.013146162033081
Validation loss: 2.360394537448883

Epoch: 6| Step: 3
Training loss: 2.0192999839782715
Validation loss: 2.3593828678131104

Epoch: 6| Step: 4
Training loss: 2.5607190132141113
Validation loss: 2.351590911547343

Epoch: 6| Step: 5
Training loss: 2.6771702766418457
Validation loss: 2.350146929423014

Epoch: 6| Step: 6
Training loss: 2.2246878147125244
Validation loss: 2.349235455195109

Epoch: 6| Step: 7
Training loss: 2.346771717071533
Validation loss: 2.345077474912008

Epoch: 6| Step: 8
Training loss: 1.8278590440750122
Validation loss: 2.3391886750857034

Epoch: 6| Step: 9
Training loss: 2.6635191440582275
Validation loss: 2.3365286588668823

Epoch: 6| Step: 10
Training loss: 2.495009660720825
Validation loss: 2.338613510131836

Epoch: 6| Step: 11
Training loss: 2.818920135498047
Validation loss: 2.342085838317871

Epoch: 6| Step: 12
Training loss: 3.0789406299591064
Validation loss: 2.3329323728879294

Epoch: 6| Step: 13
Training loss: 2.938995838165283
Validation loss: 2.3264205853144326

Epoch: 47| Step: 0
Training loss: 3.203748941421509
Validation loss: 2.328393201033274

Epoch: 6| Step: 1
Training loss: 2.1335368156433105
Validation loss: 2.3319411675135293

Epoch: 6| Step: 2
Training loss: 3.055650234222412
Validation loss: 2.343887686729431

Epoch: 6| Step: 3
Training loss: 2.993129253387451
Validation loss: 2.3506454626719155

Epoch: 6| Step: 4
Training loss: 2.0666470527648926
Validation loss: 2.3536312580108643

Epoch: 6| Step: 5
Training loss: 1.7008237838745117
Validation loss: 2.3484209179878235

Epoch: 6| Step: 6
Training loss: 1.862344741821289
Validation loss: 2.3474779526392617

Epoch: 6| Step: 7
Training loss: 2.760050058364868
Validation loss: 2.3429721792538962

Epoch: 6| Step: 8
Training loss: 3.2410202026367188
Validation loss: 2.332454204559326

Epoch: 6| Step: 9
Training loss: 2.1976726055145264
Validation loss: 2.3278870979944863

Epoch: 6| Step: 10
Training loss: 2.5403244495391846
Validation loss: 2.322399457295736

Epoch: 6| Step: 11
Training loss: 2.564150810241699
Validation loss: 2.31621778011322

Epoch: 6| Step: 12
Training loss: 2.3143577575683594
Validation loss: 2.3082752426465354

Epoch: 6| Step: 13
Training loss: 2.607541084289551
Validation loss: 2.306557754675547

Epoch: 48| Step: 0
Training loss: 2.6376519203186035
Validation loss: 2.300834039847056

Epoch: 6| Step: 1
Training loss: 2.9074904918670654
Validation loss: 2.3047995964686074

Epoch: 6| Step: 2
Training loss: 2.6628642082214355
Validation loss: 2.2978716691335044

Epoch: 6| Step: 3
Training loss: 2.2801196575164795
Validation loss: 2.2932446002960205

Epoch: 6| Step: 4
Training loss: 2.1925251483917236
Validation loss: 2.2920006116231284

Epoch: 6| Step: 5
Training loss: 2.0415029525756836
Validation loss: 2.2923869291941323

Epoch: 6| Step: 6
Training loss: 2.7024412155151367
Validation loss: 2.2868146300315857

Epoch: 6| Step: 7
Training loss: 2.0591506958007812
Validation loss: 2.2863882780075073

Epoch: 6| Step: 8
Training loss: 2.8154420852661133
Validation loss: 2.2824371258417764

Epoch: 6| Step: 9
Training loss: 2.649217128753662
Validation loss: 2.276357094446818

Epoch: 6| Step: 10
Training loss: 2.2873952388763428
Validation loss: 2.2737965186436973

Epoch: 6| Step: 11
Training loss: 2.2554874420166016
Validation loss: 2.2724711497624717

Epoch: 6| Step: 12
Training loss: 1.947046160697937
Validation loss: 2.2660717566808066

Epoch: 6| Step: 13
Training loss: 2.9084982872009277
Validation loss: 2.2632490595181785

Epoch: 49| Step: 0
Training loss: 2.4884400367736816
Validation loss: 2.2601658503214517

Epoch: 6| Step: 1
Training loss: 2.0823960304260254
Validation loss: 2.260776162147522

Epoch: 6| Step: 2
Training loss: 2.613102912902832
Validation loss: 2.253599147001902

Epoch: 6| Step: 3
Training loss: 2.2096025943756104
Validation loss: 2.25300798813502

Epoch: 6| Step: 4
Training loss: 2.7626898288726807
Validation loss: 2.2514118353525796

Epoch: 6| Step: 5
Training loss: 3.041590929031372
Validation loss: 2.252885639667511

Epoch: 6| Step: 6
Training loss: 2.277482509613037
Validation loss: 2.247991998990377

Epoch: 6| Step: 7
Training loss: 2.327643394470215
Validation loss: 2.245568573474884

Epoch: 6| Step: 8
Training loss: 2.295099973678589
Validation loss: 2.243384917577108

Epoch: 6| Step: 9
Training loss: 2.3754327297210693
Validation loss: 2.231080154577891

Epoch: 6| Step: 10
Training loss: 2.7895965576171875
Validation loss: 2.2371572256088257

Epoch: 6| Step: 11
Training loss: 2.033132314682007
Validation loss: 2.2364251216252646

Epoch: 6| Step: 12
Training loss: 1.942805290222168
Validation loss: 2.2324026226997375

Epoch: 6| Step: 13
Training loss: 2.386371612548828
Validation loss: 2.228559454282125

Epoch: 50| Step: 0
Training loss: 2.2601547241210938
Validation loss: 2.2255539099375405

Epoch: 6| Step: 1
Training loss: 2.9256091117858887
Validation loss: 2.2277273337046304

Epoch: 6| Step: 2
Training loss: 2.5216805934906006
Validation loss: 2.2235160072644553

Epoch: 6| Step: 3
Training loss: 1.5989913940429688
Validation loss: 2.220593730608622

Epoch: 6| Step: 4
Training loss: 2.5179200172424316
Validation loss: 2.217772622903188

Epoch: 6| Step: 5
Training loss: 2.2808451652526855
Validation loss: 2.2133403619130454

Epoch: 6| Step: 6
Training loss: 2.3765289783477783
Validation loss: 2.2105407317479453

Epoch: 6| Step: 7
Training loss: 1.8956377506256104
Validation loss: 2.2094919880231223

Epoch: 6| Step: 8
Training loss: 2.944453477859497
Validation loss: 2.2128024101257324

Epoch: 6| Step: 9
Training loss: 2.196211099624634
Validation loss: 2.21139262119929

Epoch: 6| Step: 10
Training loss: 2.5688631534576416
Validation loss: 2.2065410812695823

Epoch: 6| Step: 11
Training loss: 2.253394365310669
Validation loss: 2.2062377333641052

Epoch: 6| Step: 12
Training loss: 2.092552423477173
Validation loss: 2.197783629099528

Epoch: 6| Step: 13
Training loss: 2.8127691745758057
Validation loss: 2.198447306950887

Epoch: 51| Step: 0
Training loss: 1.9144223928451538
Validation loss: 2.1973476807276406

Epoch: 6| Step: 1
Training loss: 2.267789363861084
Validation loss: 2.194237232208252

Epoch: 6| Step: 2
Training loss: 2.1119768619537354
Validation loss: 2.198620359102885

Epoch: 6| Step: 3
Training loss: 2.3865966796875
Validation loss: 2.1989726424217224

Epoch: 6| Step: 4
Training loss: 2.417483329772949
Validation loss: 2.1949129899342856

Epoch: 6| Step: 5
Training loss: 2.2398152351379395
Validation loss: 2.196806252002716

Epoch: 6| Step: 6
Training loss: 2.6499876976013184
Validation loss: 2.1949510176976523

Epoch: 6| Step: 7
Training loss: 2.055938959121704
Validation loss: 2.195089260737101

Epoch: 6| Step: 8
Training loss: 2.9760794639587402
Validation loss: 2.1938287218411765

Epoch: 6| Step: 9
Training loss: 2.5553574562072754
Validation loss: 2.194410741329193

Epoch: 6| Step: 10
Training loss: 2.305742025375366
Validation loss: 2.1900515953699746

Epoch: 6| Step: 11
Training loss: 2.5383453369140625
Validation loss: 2.1861074566841125

Epoch: 6| Step: 12
Training loss: 2.200378179550171
Validation loss: 2.1858816345532737

Epoch: 6| Step: 13
Training loss: 2.2902307510375977
Validation loss: 2.1799162228902182

Epoch: 52| Step: 0
Training loss: 2.403939962387085
Validation loss: 2.1768979032834372

Epoch: 6| Step: 1
Training loss: 2.4452648162841797
Validation loss: 2.1712900400161743

Epoch: 6| Step: 2
Training loss: 1.8587523698806763
Validation loss: 2.17380823691686

Epoch: 6| Step: 3
Training loss: 1.8149287700653076
Validation loss: 2.1779134273529053

Epoch: 6| Step: 4
Training loss: 1.8917090892791748
Validation loss: 2.176529844601949

Epoch: 6| Step: 5
Training loss: 2.081024646759033
Validation loss: 2.168648342291514

Epoch: 6| Step: 6
Training loss: 3.184380054473877
Validation loss: 2.1649636030197144

Epoch: 6| Step: 7
Training loss: 2.481412649154663
Validation loss: 2.164680083592733

Epoch: 6| Step: 8
Training loss: 2.629512310028076
Validation loss: 2.158724387486776

Epoch: 6| Step: 9
Training loss: 2.02752947807312
Validation loss: 2.16419517993927

Epoch: 6| Step: 10
Training loss: 2.0334227085113525
Validation loss: 2.159917950630188

Epoch: 6| Step: 11
Training loss: 2.7847933769226074
Validation loss: 2.160128871599833

Epoch: 6| Step: 12
Training loss: 2.2817366123199463
Validation loss: 2.160049041112264

Epoch: 6| Step: 13
Training loss: 2.614739179611206
Validation loss: 2.1605258782704673

Epoch: 53| Step: 0
Training loss: 2.5096139907836914
Validation loss: 2.1604572335879006

Epoch: 6| Step: 1
Training loss: 3.0652735233306885
Validation loss: 2.1533955136934915

Epoch: 6| Step: 2
Training loss: 2.055419445037842
Validation loss: 2.1517149408658347

Epoch: 6| Step: 3
Training loss: 2.222893714904785
Validation loss: 2.153759797414144

Epoch: 6| Step: 4
Training loss: 2.3837509155273438
Validation loss: 2.150182624657949

Epoch: 6| Step: 5
Training loss: 2.532900810241699
Validation loss: 2.1500173012415567

Epoch: 6| Step: 6
Training loss: 1.9883562326431274
Validation loss: 2.1502945820490518

Epoch: 6| Step: 7
Training loss: 1.9320595264434814
Validation loss: 2.151210149129232

Epoch: 6| Step: 8
Training loss: 2.5524086952209473
Validation loss: 2.1473567485809326

Epoch: 6| Step: 9
Training loss: 2.4626569747924805
Validation loss: 2.1452494859695435

Epoch: 6| Step: 10
Training loss: 2.762181520462036
Validation loss: 2.142296016216278

Epoch: 6| Step: 11
Training loss: 1.8393765687942505
Validation loss: 2.140189230442047

Epoch: 6| Step: 12
Training loss: 2.1530356407165527
Validation loss: 2.1408391992251077

Epoch: 6| Step: 13
Training loss: 1.9061059951782227
Validation loss: 2.137086868286133

Epoch: 54| Step: 0
Training loss: 2.6216859817504883
Validation loss: 2.1379850705464682

Epoch: 6| Step: 1
Training loss: 2.2180328369140625
Validation loss: 2.13308839003245

Epoch: 6| Step: 2
Training loss: 2.4985175132751465
Validation loss: 2.136819005012512

Epoch: 6| Step: 3
Training loss: 2.736962080001831
Validation loss: 2.1488051414489746

Epoch: 6| Step: 4
Training loss: 1.537348985671997
Validation loss: 2.1475447614987693

Epoch: 6| Step: 5
Training loss: 1.5300241708755493
Validation loss: 2.1535852750142417

Epoch: 6| Step: 6
Training loss: 2.324044704437256
Validation loss: 2.138025919596354

Epoch: 6| Step: 7
Training loss: 1.8229103088378906
Validation loss: 2.1295424501101174

Epoch: 6| Step: 8
Training loss: 1.641998052597046
Validation loss: 2.1308202743530273

Epoch: 6| Step: 9
Training loss: 2.8766379356384277
Validation loss: 2.1228399674097695

Epoch: 6| Step: 10
Training loss: 2.5776565074920654
Validation loss: 2.122980793317159

Epoch: 6| Step: 11
Training loss: 2.37094783782959
Validation loss: 2.1231621503829956

Epoch: 6| Step: 12
Training loss: 2.504664897918701
Validation loss: 2.118912657101949

Epoch: 6| Step: 13
Training loss: 2.8591971397399902
Validation loss: 2.1178624033927917

Epoch: 55| Step: 0
Training loss: 2.5841479301452637
Validation loss: 2.1214539607365928

Epoch: 6| Step: 1
Training loss: 2.721766948699951
Validation loss: 2.1195302605628967

Epoch: 6| Step: 2
Training loss: 2.386852264404297
Validation loss: 2.117121458053589

Epoch: 6| Step: 3
Training loss: 2.061720371246338
Validation loss: 2.121514141559601

Epoch: 6| Step: 4
Training loss: 2.1117758750915527
Validation loss: 2.1205766002337136

Epoch: 6| Step: 5
Training loss: 1.9263933897018433
Validation loss: 2.1194517016410828

Epoch: 6| Step: 6
Training loss: 2.4198033809661865
Validation loss: 2.1159064372380576

Epoch: 6| Step: 7
Training loss: 2.3143038749694824
Validation loss: 2.1219900250434875

Epoch: 6| Step: 8
Training loss: 2.519364356994629
Validation loss: 2.115301191806793

Epoch: 6| Step: 9
Training loss: 2.557952404022217
Validation loss: 2.116740345954895

Epoch: 6| Step: 10
Training loss: 2.261491298675537
Validation loss: 2.111296614011129

Epoch: 6| Step: 11
Training loss: 2.023693084716797
Validation loss: 2.113897919654846

Epoch: 6| Step: 12
Training loss: 2.3132028579711914
Validation loss: 2.116454243659973

Epoch: 6| Step: 13
Training loss: 1.944849967956543
Validation loss: 2.109202265739441

Epoch: 56| Step: 0
Training loss: 1.8661259412765503
Validation loss: 2.1019116640090942

Epoch: 6| Step: 1
Training loss: 1.9919731616973877
Validation loss: 2.1019811232884726

Epoch: 6| Step: 2
Training loss: 2.8456170558929443
Validation loss: 2.1047808726628623

Epoch: 6| Step: 3
Training loss: 2.536984443664551
Validation loss: 2.0959542989730835

Epoch: 6| Step: 4
Training loss: 1.7918941974639893
Validation loss: 2.0950034856796265

Epoch: 6| Step: 5
Training loss: 2.4234795570373535
Validation loss: 2.10741525888443

Epoch: 6| Step: 6
Training loss: 1.7789595127105713
Validation loss: 2.106439928213755

Epoch: 6| Step: 7
Training loss: 3.087967872619629
Validation loss: 2.114913582801819

Epoch: 6| Step: 8
Training loss: 2.204680919647217
Validation loss: 2.109623908996582

Epoch: 6| Step: 9
Training loss: 2.3037705421447754
Validation loss: 2.0980859994888306

Epoch: 6| Step: 10
Training loss: 2.2638871669769287
Validation loss: 2.0998403827349343

Epoch: 6| Step: 11
Training loss: 2.244234085083008
Validation loss: 2.087200085322062

Epoch: 6| Step: 12
Training loss: 2.297642707824707
Validation loss: 2.090907335281372

Epoch: 6| Step: 13
Training loss: 2.1345629692077637
Validation loss: 2.0976516803105674

Epoch: 57| Step: 0
Training loss: 2.6561970710754395
Validation loss: 2.0991995135943093

Epoch: 6| Step: 1
Training loss: 2.6472811698913574
Validation loss: 2.094792644182841

Epoch: 6| Step: 2
Training loss: 2.8556559085845947
Validation loss: 2.0940462152163186

Epoch: 6| Step: 3
Training loss: 1.9178707599639893
Validation loss: 2.095165491104126

Epoch: 6| Step: 4
Training loss: 2.498250722885132
Validation loss: 2.0979790687561035

Epoch: 6| Step: 5
Training loss: 2.301236629486084
Validation loss: 2.0964070359865823

Epoch: 6| Step: 6
Training loss: 2.087441921234131
Validation loss: 2.0965810418128967

Epoch: 6| Step: 7
Training loss: 2.34622859954834
Validation loss: 2.089446564515432

Epoch: 6| Step: 8
Training loss: 2.021270751953125
Validation loss: 2.0921908219655356

Epoch: 6| Step: 9
Training loss: 2.1457645893096924
Validation loss: 2.088864584763845

Epoch: 6| Step: 10
Training loss: 1.9387538433074951
Validation loss: 2.0865814089775085

Epoch: 6| Step: 11
Training loss: 1.5724363327026367
Validation loss: 2.086419324080149

Epoch: 6| Step: 12
Training loss: 2.6812071800231934
Validation loss: 2.085575520992279

Epoch: 6| Step: 13
Training loss: 2.148808717727661
Validation loss: 2.0813071926434836

Epoch: 58| Step: 0
Training loss: 1.9780608415603638
Validation loss: 2.075360635916392

Epoch: 6| Step: 1
Training loss: 2.0186853408813477
Validation loss: 2.0747095545132956

Epoch: 6| Step: 2
Training loss: 2.6660513877868652
Validation loss: 2.0781205097834268

Epoch: 6| Step: 3
Training loss: 2.2551982402801514
Validation loss: 2.0726879239082336

Epoch: 6| Step: 4
Training loss: 3.2470502853393555
Validation loss: 2.0787192384401956

Epoch: 6| Step: 5
Training loss: 1.7233827114105225
Validation loss: 2.0685940782229104

Epoch: 6| Step: 6
Training loss: 1.4337698221206665
Validation loss: 2.0652294754981995

Epoch: 6| Step: 7
Training loss: 2.5681610107421875
Validation loss: 2.0722352266311646

Epoch: 6| Step: 8
Training loss: 2.0344021320343018
Validation loss: 2.06589404741923

Epoch: 6| Step: 9
Training loss: 2.649362802505493
Validation loss: 2.0680835048357644

Epoch: 6| Step: 10
Training loss: 2.2764830589294434
Validation loss: 2.0630511244138083

Epoch: 6| Step: 11
Training loss: 2.0320069789886475
Validation loss: 2.0617083509763083

Epoch: 6| Step: 12
Training loss: 2.612417221069336
Validation loss: 2.0564462741216025

Epoch: 6| Step: 13
Training loss: 1.9339909553527832
Validation loss: 2.0679691235224404

Epoch: 59| Step: 0
Training loss: 1.856785774230957
Validation loss: 2.0566328366597495

Epoch: 6| Step: 1
Training loss: 2.441941738128662
Validation loss: 2.066219965616862

Epoch: 6| Step: 2
Training loss: 1.859647512435913
Validation loss: 2.053640087445577

Epoch: 6| Step: 3
Training loss: 2.1209349632263184
Validation loss: 2.0501757065455117

Epoch: 6| Step: 4
Training loss: 2.361124038696289
Validation loss: 2.056879182656606

Epoch: 6| Step: 5
Training loss: 2.6609697341918945
Validation loss: 2.054827650388082

Epoch: 6| Step: 6
Training loss: 2.434722423553467
Validation loss: 2.0541939735412598

Epoch: 6| Step: 7
Training loss: 2.0107648372650146
Validation loss: 2.055851856867472

Epoch: 6| Step: 8
Training loss: 2.00820255279541
Validation loss: 2.0480703115463257

Epoch: 6| Step: 9
Training loss: 2.5761260986328125
Validation loss: 2.049655298391978

Epoch: 6| Step: 10
Training loss: 2.2370588779449463
Validation loss: 2.0537485678990683

Epoch: 6| Step: 11
Training loss: 2.6183314323425293
Validation loss: 2.0482723911603293

Epoch: 6| Step: 12
Training loss: 1.318955898284912
Validation loss: 2.051458418369293

Epoch: 6| Step: 13
Training loss: 2.60844349861145
Validation loss: 2.0515732169151306

Epoch: 60| Step: 0
Training loss: 2.083275318145752
Validation loss: 2.0590526858965554

Epoch: 6| Step: 1
Training loss: 3.0492820739746094
Validation loss: 2.0576738516489663

Epoch: 6| Step: 2
Training loss: 2.182502031326294
Validation loss: 2.048314154148102

Epoch: 6| Step: 3
Training loss: 2.3611016273498535
Validation loss: 2.052240272363027

Epoch: 6| Step: 4
Training loss: 2.039463996887207
Validation loss: 2.058931529521942

Epoch: 6| Step: 5
Training loss: 1.7552015781402588
Validation loss: 2.0480441451072693

Epoch: 6| Step: 6
Training loss: 1.9118807315826416
Validation loss: 2.055788973967234

Epoch: 6| Step: 7
Training loss: 1.8774685859680176
Validation loss: 2.0547786553700766

Epoch: 6| Step: 8
Training loss: 1.992340087890625
Validation loss: 2.0461471676826477

Epoch: 6| Step: 9
Training loss: 2.533400535583496
Validation loss: 2.0473069548606873

Epoch: 6| Step: 10
Training loss: 2.738771915435791
Validation loss: 2.0409170985221863

Epoch: 6| Step: 11
Training loss: 2.560236692428589
Validation loss: 2.0386162400245667

Epoch: 6| Step: 12
Training loss: 1.967939853668213
Validation loss: 2.041759411493937

Epoch: 6| Step: 13
Training loss: 2.056605100631714
Validation loss: 2.0516746242841086

Epoch: 61| Step: 0
Training loss: 2.336426019668579
Validation loss: 2.047770082950592

Epoch: 6| Step: 1
Training loss: 2.2462592124938965
Validation loss: 2.0502830743789673

Epoch: 6| Step: 2
Training loss: 2.6127865314483643
Validation loss: 2.0458110769589744

Epoch: 6| Step: 3
Training loss: 2.5081756114959717
Validation loss: 2.0415310660998025

Epoch: 6| Step: 4
Training loss: 1.9084887504577637
Validation loss: 2.0342784921328225

Epoch: 6| Step: 5
Training loss: 1.9925812482833862
Validation loss: 2.0396512945493064

Epoch: 6| Step: 6
Training loss: 2.1000893115997314
Validation loss: 2.034686803817749

Epoch: 6| Step: 7
Training loss: 2.459451198577881
Validation loss: 2.0415151913960776

Epoch: 6| Step: 8
Training loss: 1.7991995811462402
Validation loss: 2.039739509423574

Epoch: 6| Step: 9
Training loss: 1.640974998474121
Validation loss: 2.037923256556193

Epoch: 6| Step: 10
Training loss: 2.305224895477295
Validation loss: 2.0330438017845154

Epoch: 6| Step: 11
Training loss: 2.356538772583008
Validation loss: 2.032392899195353

Epoch: 6| Step: 12
Training loss: 2.460508346557617
Validation loss: 2.0298970341682434

Epoch: 6| Step: 13
Training loss: 2.337038516998291
Validation loss: 2.0394634207089744

Epoch: 62| Step: 0
Training loss: 2.191469430923462
Validation loss: 2.0358595848083496

Epoch: 6| Step: 1
Training loss: 2.3353514671325684
Validation loss: 2.0368178288141885

Epoch: 6| Step: 2
Training loss: 1.8136364221572876
Validation loss: 2.0440176129341125

Epoch: 6| Step: 3
Training loss: 1.4578114748001099
Validation loss: 2.0401424964269004

Epoch: 6| Step: 4
Training loss: 2.1343464851379395
Validation loss: 2.057516098022461

Epoch: 6| Step: 5
Training loss: 2.1542279720306396
Validation loss: 2.0558823347091675

Epoch: 6| Step: 6
Training loss: 2.8335933685302734
Validation loss: 2.060193419456482

Epoch: 6| Step: 7
Training loss: 2.4930238723754883
Validation loss: 2.0628292759259543

Epoch: 6| Step: 8
Training loss: 2.7599167823791504
Validation loss: 2.0581876039505005

Epoch: 6| Step: 9
Training loss: 2.483808994293213
Validation loss: 2.044168849786123

Epoch: 6| Step: 10
Training loss: 2.1370275020599365
Validation loss: 2.035848299662272

Epoch: 6| Step: 11
Training loss: 2.3405680656433105
Validation loss: 2.030518889427185

Epoch: 6| Step: 12
Training loss: 1.445185899734497
Validation loss: 2.022880772749583

Epoch: 6| Step: 13
Training loss: 2.403528928756714
Validation loss: 2.0272682309150696

Epoch: 63| Step: 0
Training loss: 2.2638626098632812
Validation loss: 2.0343167185783386

Epoch: 6| Step: 1
Training loss: 1.9737231731414795
Validation loss: 2.0309342543284097

Epoch: 6| Step: 2
Training loss: 2.1436636447906494
Validation loss: 2.0218788186709085

Epoch: 6| Step: 3
Training loss: 2.0945920944213867
Validation loss: 2.028904298941294

Epoch: 6| Step: 4
Training loss: 2.131221055984497
Validation loss: 2.0286038319269815

Epoch: 6| Step: 5
Training loss: 2.6146247386932373
Validation loss: 2.037046710650126

Epoch: 6| Step: 6
Training loss: 2.723189353942871
Validation loss: 2.0279473662376404

Epoch: 6| Step: 7
Training loss: 2.1989946365356445
Validation loss: 2.026813248793284

Epoch: 6| Step: 8
Training loss: 1.4637774229049683
Validation loss: 2.030273218949636

Epoch: 6| Step: 9
Training loss: 2.5031304359436035
Validation loss: 2.0249177614847818

Epoch: 6| Step: 10
Training loss: 2.178314447402954
Validation loss: 2.0248233477274575

Epoch: 6| Step: 11
Training loss: 1.8787989616394043
Validation loss: 2.0419458150863647

Epoch: 6| Step: 12
Training loss: 1.9198501110076904
Validation loss: 2.0580469369888306

Epoch: 6| Step: 13
Training loss: 2.841108798980713
Validation loss: 2.0641050934791565

Epoch: 64| Step: 0
Training loss: 1.0030076503753662
Validation loss: 2.063933471838633

Epoch: 6| Step: 1
Training loss: 1.8320244550704956
Validation loss: 2.0635406772295632

Epoch: 6| Step: 2
Training loss: 2.3091912269592285
Validation loss: 2.0481883684794107

Epoch: 6| Step: 3
Training loss: 2.975632905960083
Validation loss: 2.040118932723999

Epoch: 6| Step: 4
Training loss: 2.762908458709717
Validation loss: 2.0326749881108603

Epoch: 6| Step: 5
Training loss: 2.6927287578582764
Validation loss: 2.037828207015991

Epoch: 6| Step: 6
Training loss: 2.336796522140503
Validation loss: 2.0379282236099243

Epoch: 6| Step: 7
Training loss: 2.095907211303711
Validation loss: 2.0420889655749

Epoch: 6| Step: 8
Training loss: 2.2431514263153076
Validation loss: 2.044550617535909

Epoch: 6| Step: 9
Training loss: 1.8894340991973877
Validation loss: 2.0452980995178223

Epoch: 6| Step: 10
Training loss: 1.8867332935333252
Validation loss: 2.0424697200457254

Epoch: 6| Step: 11
Training loss: 2.671952724456787
Validation loss: 2.038058598836263

Epoch: 6| Step: 12
Training loss: 2.088001012802124
Validation loss: 2.0451732675234475

Epoch: 6| Step: 13
Training loss: 2.056210517883301
Validation loss: 2.0371802051862082

Epoch: 65| Step: 0
Training loss: 1.8470653295516968
Validation loss: 2.0399859150250754

Epoch: 6| Step: 1
Training loss: 2.561109781265259
Validation loss: 2.046271642049154

Epoch: 6| Step: 2
Training loss: 2.0484371185302734
Validation loss: 2.037969470024109

Epoch: 6| Step: 3
Training loss: 2.2381997108459473
Validation loss: 2.035954395929972

Epoch: 6| Step: 4
Training loss: 2.6330907344818115
Validation loss: 2.0395403107007346

Epoch: 6| Step: 5
Training loss: 2.5085439682006836
Validation loss: 2.0374016165733337

Epoch: 6| Step: 6
Training loss: 1.6847347021102905
Validation loss: 2.033469100793203

Epoch: 6| Step: 7
Training loss: 1.7987380027770996
Validation loss: 2.0344325502713523

Epoch: 6| Step: 8
Training loss: 2.522282838821411
Validation loss: 2.036932726701101

Epoch: 6| Step: 9
Training loss: 2.6397809982299805
Validation loss: 2.0298580328623452

Epoch: 6| Step: 10
Training loss: 2.4675674438476562
Validation loss: 2.0324241518974304

Epoch: 6| Step: 11
Training loss: 2.2589306831359863
Validation loss: 2.032657206058502

Epoch: 6| Step: 12
Training loss: 1.542649745941162
Validation loss: 2.0322585701942444

Epoch: 6| Step: 13
Training loss: 2.1355626583099365
Validation loss: 2.026828110218048

Epoch: 66| Step: 0
Training loss: 3.24967622756958
Validation loss: 2.0258453687032065

Epoch: 6| Step: 1
Training loss: 2.2545034885406494
Validation loss: 2.0242979526519775

Epoch: 6| Step: 2
Training loss: 1.7972465753555298
Validation loss: 2.0222016970316568

Epoch: 6| Step: 3
Training loss: 1.6677132844924927
Validation loss: 2.022118866443634

Epoch: 6| Step: 4
Training loss: 2.105048179626465
Validation loss: 2.0233058532079062

Epoch: 6| Step: 5
Training loss: 1.919762372970581
Validation loss: 2.0214388370513916

Epoch: 6| Step: 6
Training loss: 2.5444910526275635
Validation loss: 2.0312421321868896

Epoch: 6| Step: 7
Training loss: 1.7466752529144287
Validation loss: 2.026975174744924

Epoch: 6| Step: 8
Training loss: 2.208242177963257
Validation loss: 2.021815816561381

Epoch: 6| Step: 9
Training loss: 2.017913818359375
Validation loss: 2.0261192520459494

Epoch: 6| Step: 10
Training loss: 2.537724494934082
Validation loss: 2.0296877225240073

Epoch: 6| Step: 11
Training loss: 1.9245359897613525
Validation loss: 2.02367111047109

Epoch: 6| Step: 12
Training loss: 2.6420230865478516
Validation loss: 2.0361589392026267

Epoch: 6| Step: 13
Training loss: 2.006739616394043
Validation loss: 2.0281063318252563

Epoch: 67| Step: 0
Training loss: 2.635878562927246
Validation loss: 2.038905163606008

Epoch: 6| Step: 1
Training loss: 2.4101200103759766
Validation loss: 2.032231589158376

Epoch: 6| Step: 2
Training loss: 2.4011030197143555
Validation loss: 2.0292506217956543

Epoch: 6| Step: 3
Training loss: 1.709600567817688
Validation loss: 2.0283169945081077

Epoch: 6| Step: 4
Training loss: 2.3992533683776855
Validation loss: 2.0381150245666504

Epoch: 6| Step: 5
Training loss: 1.855954647064209
Validation loss: 2.029691696166992

Epoch: 6| Step: 6
Training loss: 2.3357648849487305
Validation loss: 2.0287688970565796

Epoch: 6| Step: 7
Training loss: 1.9575154781341553
Validation loss: 2.027704397837321

Epoch: 6| Step: 8
Training loss: 2.1001806259155273
Validation loss: 2.029205600420634

Epoch: 6| Step: 9
Training loss: 2.1939120292663574
Validation loss: 2.0220523476600647

Epoch: 6| Step: 10
Training loss: 1.8060224056243896
Validation loss: 2.0262858271598816

Epoch: 6| Step: 11
Training loss: 2.506253719329834
Validation loss: 2.032957375049591

Epoch: 6| Step: 12
Training loss: 1.9458739757537842
Validation loss: 2.0283048152923584

Epoch: 6| Step: 13
Training loss: 2.1669554710388184
Validation loss: 2.0364227294921875

Epoch: 68| Step: 0
Training loss: 2.3009183406829834
Validation loss: 2.0412042140960693

Epoch: 6| Step: 1
Training loss: 1.581129789352417
Validation loss: 2.0456102093060813

Epoch: 6| Step: 2
Training loss: 2.766272783279419
Validation loss: 2.0575167139371238

Epoch: 6| Step: 3
Training loss: 2.0202088356018066
Validation loss: 2.046508312225342

Epoch: 6| Step: 4
Training loss: 2.1536364555358887
Validation loss: 2.0387554367383323

Epoch: 6| Step: 5
Training loss: 2.0931005477905273
Validation loss: 2.0287805596987405

Epoch: 6| Step: 6
Training loss: 1.7559175491333008
Validation loss: 2.0216678182284036

Epoch: 6| Step: 7
Training loss: 2.072434425354004
Validation loss: 2.018285055955251

Epoch: 6| Step: 8
Training loss: 2.4721944332122803
Validation loss: 2.0174270470937095

Epoch: 6| Step: 9
Training loss: 2.6521105766296387
Validation loss: 2.016476293404897

Epoch: 6| Step: 10
Training loss: 1.818769097328186
Validation loss: 2.012168268362681

Epoch: 6| Step: 11
Training loss: 2.8467483520507812
Validation loss: 2.0171939929326377

Epoch: 6| Step: 12
Training loss: 2.033184766769409
Validation loss: 2.01680060227712

Epoch: 6| Step: 13
Training loss: 1.9410600662231445
Validation loss: 2.0201916098594666

Epoch: 69| Step: 0
Training loss: 1.5853770971298218
Validation loss: 2.019786059856415

Epoch: 6| Step: 1
Training loss: 2.1081230640411377
Validation loss: 2.0190830628077188

Epoch: 6| Step: 2
Training loss: 2.386314868927002
Validation loss: 2.030502657095591

Epoch: 6| Step: 3
Training loss: 1.7328296899795532
Validation loss: 2.02523406346639

Epoch: 6| Step: 4
Training loss: 2.5006303787231445
Validation loss: 2.021806279818217

Epoch: 6| Step: 5
Training loss: 2.2801430225372314
Validation loss: 2.027566651503245

Epoch: 6| Step: 6
Training loss: 1.9374443292617798
Validation loss: 2.034460802872976

Epoch: 6| Step: 7
Training loss: 2.1548800468444824
Validation loss: 2.043234348297119

Epoch: 6| Step: 8
Training loss: 2.483301877975464
Validation loss: 2.0383078257242837

Epoch: 6| Step: 9
Training loss: 2.44284725189209
Validation loss: 2.0310773452123008

Epoch: 6| Step: 10
Training loss: 1.7135450839996338
Validation loss: 2.0377087791760764

Epoch: 6| Step: 11
Training loss: 2.12713623046875
Validation loss: 2.0217998226483664

Epoch: 6| Step: 12
Training loss: 2.900482177734375
Validation loss: 2.0189168651898703

Epoch: 6| Step: 13
Training loss: 1.985944390296936
Validation loss: 2.0279346704483032

Epoch: 70| Step: 0
Training loss: 2.2957069873809814
Validation loss: 2.022966821988424

Epoch: 6| Step: 1
Training loss: 1.6256686449050903
Validation loss: 2.0302612582842507

Epoch: 6| Step: 2
Training loss: 1.7722258567810059
Validation loss: 2.0327101151148477

Epoch: 6| Step: 3
Training loss: 2.7785377502441406
Validation loss: 2.034264703591665

Epoch: 6| Step: 4
Training loss: 2.592215061187744
Validation loss: 2.041949470837911

Epoch: 6| Step: 5
Training loss: 2.905064821243286
Validation loss: 2.040036618709564

Epoch: 6| Step: 6
Training loss: 2.3417623043060303
Validation loss: 2.028535266717275

Epoch: 6| Step: 7
Training loss: 1.628089189529419
Validation loss: 2.0303540229797363

Epoch: 6| Step: 8
Training loss: 1.8931190967559814
Validation loss: 2.03525576988856

Epoch: 6| Step: 9
Training loss: 1.8348538875579834
Validation loss: 2.0347843368848166

Epoch: 6| Step: 10
Training loss: 2.1281981468200684
Validation loss: 2.031785567601522

Epoch: 6| Step: 11
Training loss: 2.2646212577819824
Validation loss: 2.0296632250150046

Epoch: 6| Step: 12
Training loss: 2.6455612182617188
Validation loss: 2.029556234677633

Epoch: 6| Step: 13
Training loss: 2.0624499320983887
Validation loss: 2.024643858273824

Epoch: 71| Step: 0
Training loss: 2.4915919303894043
Validation loss: 2.0263969699541726

Epoch: 6| Step: 1
Training loss: 1.6672098636627197
Validation loss: 2.0204028685887656

Epoch: 6| Step: 2
Training loss: 1.7429273128509521
Validation loss: 2.0251854260762534

Epoch: 6| Step: 3
Training loss: 2.246509313583374
Validation loss: 2.0230981508890786

Epoch: 6| Step: 4
Training loss: 2.1155173778533936
Validation loss: 2.031400998433431

Epoch: 6| Step: 5
Training loss: 2.3484182357788086
Validation loss: 2.0276258985201516

Epoch: 6| Step: 6
Training loss: 2.3645334243774414
Validation loss: 2.028769075870514

Epoch: 6| Step: 7
Training loss: 2.045741319656372
Validation loss: 2.047316571076711

Epoch: 6| Step: 8
Training loss: 2.0130128860473633
Validation loss: 2.039604445298513

Epoch: 6| Step: 9
Training loss: 2.221945285797119
Validation loss: 2.0425994396209717

Epoch: 6| Step: 10
Training loss: 2.659907341003418
Validation loss: 2.040482501188914

Epoch: 6| Step: 11
Training loss: 1.9059083461761475
Validation loss: 2.0386097033818564

Epoch: 6| Step: 12
Training loss: 2.6049883365631104
Validation loss: 2.0305409828821817

Epoch: 6| Step: 13
Training loss: 2.0273869037628174
Validation loss: 2.0307745933532715

Epoch: 72| Step: 0
Training loss: 2.2787632942199707
Validation loss: 2.020663857460022

Epoch: 6| Step: 1
Training loss: 2.3566079139709473
Validation loss: 2.019754866758982

Epoch: 6| Step: 2
Training loss: 2.2071151733398438
Validation loss: 2.0254933834075928

Epoch: 6| Step: 3
Training loss: 1.7170884609222412
Validation loss: 2.028324007987976

Epoch: 6| Step: 4
Training loss: 1.9391703605651855
Validation loss: 2.0262268781661987

Epoch: 6| Step: 5
Training loss: 2.130854845046997
Validation loss: 2.0330726901690164

Epoch: 6| Step: 6
Training loss: 1.8212451934814453
Validation loss: 2.0244624416033425

Epoch: 6| Step: 7
Training loss: 2.007567882537842
Validation loss: 2.0214498043060303

Epoch: 6| Step: 8
Training loss: 2.204449415206909
Validation loss: 2.015023132165273

Epoch: 6| Step: 9
Training loss: 1.763810634613037
Validation loss: 2.0125813484191895

Epoch: 6| Step: 10
Training loss: 2.7844042778015137
Validation loss: 2.0034791628519693

Epoch: 6| Step: 11
Training loss: 2.1527469158172607
Validation loss: 2.0027461449305215

Epoch: 6| Step: 12
Training loss: 2.2268779277801514
Validation loss: 1.9997540513674419

Epoch: 6| Step: 13
Training loss: 3.022329807281494
Validation loss: 2.0054391622543335

Epoch: 73| Step: 0
Training loss: 2.2674643993377686
Validation loss: 2.002339998881022

Epoch: 6| Step: 1
Training loss: 2.0636391639709473
Validation loss: 2.001303215821584

Epoch: 6| Step: 2
Training loss: 2.454606533050537
Validation loss: 2.0076473553975425

Epoch: 6| Step: 3
Training loss: 2.4025352001190186
Validation loss: 2.0152933994928994

Epoch: 6| Step: 4
Training loss: 1.9472798109054565
Validation loss: 2.009908268849055

Epoch: 6| Step: 5
Training loss: 2.130892276763916
Validation loss: 2.005469640096029

Epoch: 6| Step: 6
Training loss: 1.7329490184783936
Validation loss: 2.006581962108612

Epoch: 6| Step: 7
Training loss: 1.8949847221374512
Validation loss: 2.0077974994977317

Epoch: 6| Step: 8
Training loss: 2.2651724815368652
Validation loss: 2.0181755224863687

Epoch: 6| Step: 9
Training loss: 2.198610782623291
Validation loss: 2.0140523314476013

Epoch: 6| Step: 10
Training loss: 2.40867018699646
Validation loss: 2.0156620144844055

Epoch: 6| Step: 11
Training loss: 2.2103660106658936
Validation loss: 2.015004555384318

Epoch: 6| Step: 12
Training loss: 1.899409294128418
Validation loss: 2.0192333459854126

Epoch: 6| Step: 13
Training loss: 2.5666677951812744
Validation loss: 2.0154349406560264

Epoch: 74| Step: 0
Training loss: 2.581960678100586
Validation loss: 2.0055525501569114

Epoch: 6| Step: 1
Training loss: 1.9558334350585938
Validation loss: 2.017350713411967

Epoch: 6| Step: 2
Training loss: 2.048325300216675
Validation loss: 2.018527348836263

Epoch: 6| Step: 3
Training loss: 2.071235418319702
Validation loss: 2.0202673077583313

Epoch: 6| Step: 4
Training loss: 2.2081680297851562
Validation loss: 2.0105775197347007

Epoch: 6| Step: 5
Training loss: 1.747157096862793
Validation loss: 2.0218743284543357

Epoch: 6| Step: 6
Training loss: 2.516939163208008
Validation loss: 2.025590737660726

Epoch: 6| Step: 7
Training loss: 2.174114465713501
Validation loss: 2.0282354752222695

Epoch: 6| Step: 8
Training loss: 2.3847479820251465
Validation loss: 2.0272263288497925

Epoch: 6| Step: 9
Training loss: 2.4991562366485596
Validation loss: 2.0308437744776406

Epoch: 6| Step: 10
Training loss: 1.7609968185424805
Validation loss: 2.0330183108647666

Epoch: 6| Step: 11
Training loss: 1.6473274230957031
Validation loss: 2.019508103529612

Epoch: 6| Step: 12
Training loss: 2.2062203884124756
Validation loss: 2.037450114885966

Epoch: 6| Step: 13
Training loss: 2.387022018432617
Validation loss: 2.0403358936309814

Epoch: 75| Step: 0
Training loss: 1.89384126663208
Validation loss: 2.0188228686650596

Epoch: 6| Step: 1
Training loss: 2.1124205589294434
Validation loss: 2.018965204556783

Epoch: 6| Step: 2
Training loss: 2.4250173568725586
Validation loss: 2.032858987649282

Epoch: 6| Step: 3
Training loss: 2.751789093017578
Validation loss: 2.0294970671335855

Epoch: 6| Step: 4
Training loss: 1.847226858139038
Validation loss: 2.0392767190933228

Epoch: 6| Step: 5
Training loss: 1.6421970129013062
Validation loss: 2.0381744305292764

Epoch: 6| Step: 6
Training loss: 2.3632538318634033
Validation loss: 2.0433759490648904

Epoch: 6| Step: 7
Training loss: 2.4412879943847656
Validation loss: 2.0436125795046487

Epoch: 6| Step: 8
Training loss: 2.2091002464294434
Validation loss: 2.0375608603159585

Epoch: 6| Step: 9
Training loss: 2.582425355911255
Validation loss: 2.0411181251207986

Epoch: 6| Step: 10
Training loss: 2.346972942352295
Validation loss: 2.044854998588562

Epoch: 6| Step: 11
Training loss: 2.581730604171753
Validation loss: 2.0434404810269675

Epoch: 6| Step: 12
Training loss: 2.0736420154571533
Validation loss: 2.045622169971466

Epoch: 6| Step: 13
Training loss: 1.9205808639526367
Validation loss: 2.0398619174957275

Epoch: 76| Step: 0
Training loss: 2.0514867305755615
Validation loss: 2.047487437725067

Epoch: 6| Step: 1
Training loss: 2.451071262359619
Validation loss: 2.0411935249964395

Epoch: 6| Step: 2
Training loss: 2.2286171913146973
Validation loss: 2.038659612337748

Epoch: 6| Step: 3
Training loss: 2.026231050491333
Validation loss: 2.0411632855733237

Epoch: 6| Step: 4
Training loss: 1.8609379529953003
Validation loss: 2.041912615299225

Epoch: 6| Step: 5
Training loss: 2.3611345291137695
Validation loss: 2.0408543745676675

Epoch: 6| Step: 6
Training loss: 2.742588758468628
Validation loss: 2.039352218310038

Epoch: 6| Step: 7
Training loss: 2.089132070541382
Validation loss: 2.038208305835724

Epoch: 6| Step: 8
Training loss: 1.4035327434539795
Validation loss: 2.034409840901693

Epoch: 6| Step: 9
Training loss: 2.933933734893799
Validation loss: 2.021454672018687

Epoch: 6| Step: 10
Training loss: 2.8636155128479004
Validation loss: 2.0122828682263694

Epoch: 6| Step: 11
Training loss: 2.349212646484375
Validation loss: 2.0126099785168967

Epoch: 6| Step: 12
Training loss: 1.6684188842773438
Validation loss: 2.0131572484970093

Epoch: 6| Step: 13
Training loss: 1.7592098712921143
Validation loss: 2.0185708006223044

Epoch: 77| Step: 0
Training loss: 2.085306406021118
Validation loss: 2.036862373352051

Epoch: 6| Step: 1
Training loss: 2.1672778129577637
Validation loss: 2.0373480717341104

Epoch: 6| Step: 2
Training loss: 1.9632244110107422
Validation loss: 2.0420586466789246

Epoch: 6| Step: 3
Training loss: 2.208254337310791
Validation loss: 2.0542500416437783

Epoch: 6| Step: 4
Training loss: 2.0498557090759277
Validation loss: 2.053790191809336

Epoch: 6| Step: 5
Training loss: 2.364135265350342
Validation loss: 2.045853396256765

Epoch: 6| Step: 6
Training loss: 2.2341105937957764
Validation loss: 2.016801198323568

Epoch: 6| Step: 7
Training loss: 2.1099677085876465
Validation loss: 2.0344606041908264

Epoch: 6| Step: 8
Training loss: 2.551013469696045
Validation loss: 2.02697483698527

Epoch: 6| Step: 9
Training loss: 2.267975330352783
Validation loss: 2.016849716504415

Epoch: 6| Step: 10
Training loss: 2.038456439971924
Validation loss: 2.004541277885437

Epoch: 6| Step: 11
Training loss: 1.9714477062225342
Validation loss: 1.9949142734209697

Epoch: 6| Step: 12
Training loss: 2.486121892929077
Validation loss: 1.9996276497840881

Epoch: 6| Step: 13
Training loss: 2.1074728965759277
Validation loss: 2.0001750588417053

Epoch: 78| Step: 0
Training loss: 1.7297649383544922
Validation loss: 2.0103381872177124

Epoch: 6| Step: 1
Training loss: 2.3770618438720703
Validation loss: 2.017773230870565

Epoch: 6| Step: 2
Training loss: 1.8476107120513916
Validation loss: 2.0188326636950173

Epoch: 6| Step: 3
Training loss: 1.898610234260559
Validation loss: 2.01548033952713

Epoch: 6| Step: 4
Training loss: 2.484121322631836
Validation loss: 2.008832653363546

Epoch: 6| Step: 5
Training loss: 1.8941419124603271
Validation loss: 2.016472101211548

Epoch: 6| Step: 6
Training loss: 2.7899861335754395
Validation loss: 2.0242870251337686

Epoch: 6| Step: 7
Training loss: 1.420868992805481
Validation loss: 2.0130130648612976

Epoch: 6| Step: 8
Training loss: 2.0830259323120117
Validation loss: 2.008457442124685

Epoch: 6| Step: 9
Training loss: 2.148890495300293
Validation loss: 2.003669579823812

Epoch: 6| Step: 10
Training loss: 2.189413070678711
Validation loss: 2.004282752672831

Epoch: 6| Step: 11
Training loss: 2.5667552947998047
Validation loss: 1.9981253743171692

Epoch: 6| Step: 12
Training loss: 2.9628939628601074
Validation loss: 2.013472060362498

Epoch: 6| Step: 13
Training loss: 2.2117581367492676
Validation loss: 2.0080408851305642

Epoch: 79| Step: 0
Training loss: 1.9754979610443115
Validation loss: 2.0126742720603943

Epoch: 6| Step: 1
Training loss: 2.510694980621338
Validation loss: 2.016432205835978

Epoch: 6| Step: 2
Training loss: 2.674145460128784
Validation loss: 2.0330923597017923

Epoch: 6| Step: 3
Training loss: 2.4534354209899902
Validation loss: 2.038840929667155

Epoch: 6| Step: 4
Training loss: 1.7864196300506592
Validation loss: 2.02927029132843

Epoch: 6| Step: 5
Training loss: 2.2392563819885254
Validation loss: 2.02936859925588

Epoch: 6| Step: 6
Training loss: 2.3464088439941406
Validation loss: 2.033712863922119

Epoch: 6| Step: 7
Training loss: 1.9352374076843262
Validation loss: 2.0324058135350547

Epoch: 6| Step: 8
Training loss: 2.2784879207611084
Validation loss: 2.029251277446747

Epoch: 6| Step: 9
Training loss: 2.204519510269165
Validation loss: 2.0258182287216187

Epoch: 6| Step: 10
Training loss: 2.2591004371643066
Validation loss: 2.032743215560913

Epoch: 6| Step: 11
Training loss: 1.7040578126907349
Validation loss: 2.0341750979423523

Epoch: 6| Step: 12
Training loss: 1.8957808017730713
Validation loss: 2.036843955516815

Epoch: 6| Step: 13
Training loss: 2.0864956378936768
Validation loss: 2.0450576543807983

Epoch: 80| Step: 0
Training loss: 2.1385133266448975
Validation loss: 2.0370545387268066

Epoch: 6| Step: 1
Training loss: 2.1604957580566406
Validation loss: 2.034070611000061

Epoch: 6| Step: 2
Training loss: 1.5936285257339478
Validation loss: 2.024439752101898

Epoch: 6| Step: 3
Training loss: 2.2389659881591797
Validation loss: 2.0121787985165915

Epoch: 6| Step: 4
Training loss: 2.3530659675598145
Validation loss: 2.0047976970672607

Epoch: 6| Step: 5
Training loss: 2.034229040145874
Validation loss: 2.004923721154531

Epoch: 6| Step: 6
Training loss: 1.8729488849639893
Validation loss: 2.0099261005719504

Epoch: 6| Step: 7
Training loss: 2.9663782119750977
Validation loss: 2.0112365086873374

Epoch: 6| Step: 8
Training loss: 1.7283755540847778
Validation loss: 2.012893537680308

Epoch: 6| Step: 9
Training loss: 2.4157166481018066
Validation loss: 2.0129478573799133

Epoch: 6| Step: 10
Training loss: 2.510187864303589
Validation loss: 2.0100286404291787

Epoch: 6| Step: 11
Training loss: 1.713106393814087
Validation loss: 2.0023229320844016

Epoch: 6| Step: 12
Training loss: 2.27571702003479
Validation loss: 2.012802322705587

Epoch: 6| Step: 13
Training loss: 2.289376735687256
Validation loss: 2.016883293787638

Epoch: 81| Step: 0
Training loss: 2.363896131515503
Validation loss: 2.0153007904688516

Epoch: 6| Step: 1
Training loss: 2.398287296295166
Validation loss: 2.014561414718628

Epoch: 6| Step: 2
Training loss: 2.5084781646728516
Validation loss: 2.0123720169067383

Epoch: 6| Step: 3
Training loss: 2.128779649734497
Validation loss: 2.008623460928599

Epoch: 6| Step: 4
Training loss: 2.085508346557617
Validation loss: 2.020573675632477

Epoch: 6| Step: 5
Training loss: 2.3697853088378906
Validation loss: 2.022264997164408

Epoch: 6| Step: 6
Training loss: 1.9570807218551636
Validation loss: 2.0165241161982217

Epoch: 6| Step: 7
Training loss: 2.153580904006958
Validation loss: 2.0177746415138245

Epoch: 6| Step: 8
Training loss: 2.2361886501312256
Validation loss: 2.012597302595774

Epoch: 6| Step: 9
Training loss: 2.161799907684326
Validation loss: 2.023187975088755

Epoch: 6| Step: 10
Training loss: 1.7728005647659302
Validation loss: 2.0215095480283103

Epoch: 6| Step: 11
Training loss: 2.013482093811035
Validation loss: 2.0183300773302713

Epoch: 6| Step: 12
Training loss: 1.7437412738800049
Validation loss: 2.0206592679023743

Epoch: 6| Step: 13
Training loss: 2.1676206588745117
Validation loss: 2.0228640834490457

Epoch: 82| Step: 0
Training loss: 2.278628349304199
Validation loss: 2.0334812800089517

Epoch: 6| Step: 1
Training loss: 1.891771674156189
Validation loss: 2.036201238632202

Epoch: 6| Step: 2
Training loss: 2.365485191345215
Validation loss: 2.023222506046295

Epoch: 6| Step: 3
Training loss: 1.7994980812072754
Validation loss: 2.0235128005345664

Epoch: 6| Step: 4
Training loss: 1.5430750846862793
Validation loss: 2.0151007175445557

Epoch: 6| Step: 5
Training loss: 2.419711112976074
Validation loss: 2.0178569157918296

Epoch: 6| Step: 6
Training loss: 2.5426840782165527
Validation loss: 2.025866150856018

Epoch: 6| Step: 7
Training loss: 1.842918038368225
Validation loss: 2.0369479656219482

Epoch: 6| Step: 8
Training loss: 1.928906798362732
Validation loss: 2.023219188054403

Epoch: 6| Step: 9
Training loss: 2.447368621826172
Validation loss: 2.033389627933502

Epoch: 6| Step: 10
Training loss: 2.1190695762634277
Validation loss: 2.018218457698822

Epoch: 6| Step: 11
Training loss: 1.9229710102081299
Validation loss: 2.026006499926249

Epoch: 6| Step: 12
Training loss: 2.6246538162231445
Validation loss: 2.0210164189338684

Epoch: 6| Step: 13
Training loss: 2.250598907470703
Validation loss: 2.0304630994796753

Epoch: 83| Step: 0
Training loss: 1.9629555940628052
Validation loss: 2.0294186075528464

Epoch: 6| Step: 1
Training loss: 2.2337000370025635
Validation loss: 2.0321619311968484

Epoch: 6| Step: 2
Training loss: 2.130244016647339
Validation loss: 2.039063572883606

Epoch: 6| Step: 3
Training loss: 1.90178382396698
Validation loss: 2.036532978216807

Epoch: 6| Step: 4
Training loss: 2.074530601501465
Validation loss: 2.0347335934638977

Epoch: 6| Step: 5
Training loss: 1.8760937452316284
Validation loss: 2.033597687880198

Epoch: 6| Step: 6
Training loss: 2.239248752593994
Validation loss: 2.039269725481669

Epoch: 6| Step: 7
Training loss: 2.040827751159668
Validation loss: 2.027703285217285

Epoch: 6| Step: 8
Training loss: 2.268339157104492
Validation loss: 2.0208066503206887

Epoch: 6| Step: 9
Training loss: 2.490489959716797
Validation loss: 2.0101073582967124

Epoch: 6| Step: 10
Training loss: 2.4251317977905273
Validation loss: 2.0142177740732827

Epoch: 6| Step: 11
Training loss: 2.25477933883667
Validation loss: 2.018701990445455

Epoch: 6| Step: 12
Training loss: 2.189603805541992
Validation loss: 2.0209327141443887

Epoch: 6| Step: 13
Training loss: 1.9359655380249023
Validation loss: 2.014985998471578

Epoch: 84| Step: 0
Training loss: 2.0409600734710693
Validation loss: 2.0132927099863687

Epoch: 6| Step: 1
Training loss: 2.523191452026367
Validation loss: 2.014510691165924

Epoch: 6| Step: 2
Training loss: 2.1908817291259766
Validation loss: 2.0088236133257547

Epoch: 6| Step: 3
Training loss: 2.159982919692993
Validation loss: 2.0121531883875527

Epoch: 6| Step: 4
Training loss: 1.8007322549819946
Validation loss: 2.012642204761505

Epoch: 6| Step: 5
Training loss: 2.022233009338379
Validation loss: 2.0079542795817056

Epoch: 6| Step: 6
Training loss: 1.6570143699645996
Validation loss: 2.01655379931132

Epoch: 6| Step: 7
Training loss: 2.2827372550964355
Validation loss: 2.0104097723960876

Epoch: 6| Step: 8
Training loss: 2.3903024196624756
Validation loss: 2.0165207783381143

Epoch: 6| Step: 9
Training loss: 2.0877480506896973
Validation loss: 2.014749844868978

Epoch: 6| Step: 10
Training loss: 2.4152872562408447
Validation loss: 2.0183359583218894

Epoch: 6| Step: 11
Training loss: 1.9926801919937134
Validation loss: 2.0115127563476562

Epoch: 6| Step: 12
Training loss: 2.1803054809570312
Validation loss: 2.0129158099492392

Epoch: 6| Step: 13
Training loss: 1.9843621253967285
Validation loss: 2.0214765866597495

Epoch: 85| Step: 0
Training loss: 2.232011079788208
Validation loss: 2.0168452660242715

Epoch: 6| Step: 1
Training loss: 2.2838902473449707
Validation loss: 2.018322984377543

Epoch: 6| Step: 2
Training loss: 2.2280619144439697
Validation loss: 2.0200118819872537

Epoch: 6| Step: 3
Training loss: 2.0079519748687744
Validation loss: 2.0293819109598794

Epoch: 6| Step: 4
Training loss: 2.432729721069336
Validation loss: 2.0246248642603555

Epoch: 6| Step: 5
Training loss: 1.8683470487594604
Validation loss: 2.029732823371887

Epoch: 6| Step: 6
Training loss: 2.395127773284912
Validation loss: 2.0307321747144065

Epoch: 6| Step: 7
Training loss: 2.2362070083618164
Validation loss: 2.0199798742930093

Epoch: 6| Step: 8
Training loss: 2.0156970024108887
Validation loss: 2.0200639565785727

Epoch: 6| Step: 9
Training loss: 2.7453203201293945
Validation loss: 2.0210766792297363

Epoch: 6| Step: 10
Training loss: 1.5289371013641357
Validation loss: 2.0155806144078574

Epoch: 6| Step: 11
Training loss: 2.0252909660339355
Validation loss: 2.0218222538630166

Epoch: 6| Step: 12
Training loss: 1.412807822227478
Validation loss: 2.0211865107218423

Epoch: 6| Step: 13
Training loss: 2.3773093223571777
Validation loss: 2.0252237717310586

Epoch: 86| Step: 0
Training loss: 2.0744519233703613
Validation loss: 2.0251596371332803

Epoch: 6| Step: 1
Training loss: 1.7724114656448364
Validation loss: 2.0209328730901084

Epoch: 6| Step: 2
Training loss: 1.590731143951416
Validation loss: 2.023658255736033

Epoch: 6| Step: 3
Training loss: 1.8709485530853271
Validation loss: 2.03392763932546

Epoch: 6| Step: 4
Training loss: 2.470785140991211
Validation loss: 2.0392510692278543

Epoch: 6| Step: 5
Training loss: 2.3090450763702393
Validation loss: 2.027465303738912

Epoch: 6| Step: 6
Training loss: 2.4627575874328613
Validation loss: 2.0239171981811523

Epoch: 6| Step: 7
Training loss: 1.2390062808990479
Validation loss: 2.024767498175303

Epoch: 6| Step: 8
Training loss: 2.0766913890838623
Validation loss: 2.018388648827871

Epoch: 6| Step: 9
Training loss: 2.3608438968658447
Validation loss: 2.0240994691848755

Epoch: 6| Step: 10
Training loss: 2.1173384189605713
Validation loss: 2.02986478805542

Epoch: 6| Step: 11
Training loss: 2.4679129123687744
Validation loss: 2.021481196085612

Epoch: 6| Step: 12
Training loss: 2.7551796436309814
Validation loss: 2.027306079864502

Epoch: 6| Step: 13
Training loss: 2.123227834701538
Validation loss: 2.0252541104952493

Epoch: 87| Step: 0
Training loss: 1.8504897356033325
Validation loss: 2.0249042312304177

Epoch: 6| Step: 1
Training loss: 1.9931175708770752
Validation loss: 2.012494385242462

Epoch: 6| Step: 2
Training loss: 2.0544090270996094
Validation loss: 2.022971828778585

Epoch: 6| Step: 3
Training loss: 1.773297667503357
Validation loss: 2.0188265244166055

Epoch: 6| Step: 4
Training loss: 2.3443729877471924
Validation loss: 2.0177359779675803

Epoch: 6| Step: 5
Training loss: 2.4795279502868652
Validation loss: 2.0213696360588074

Epoch: 6| Step: 6
Training loss: 1.5934946537017822
Validation loss: 2.023800790309906

Epoch: 6| Step: 7
Training loss: 2.3707082271575928
Validation loss: 2.024043103059133

Epoch: 6| Step: 8
Training loss: 2.320436716079712
Validation loss: 2.029710908730825

Epoch: 6| Step: 9
Training loss: 2.6089842319488525
Validation loss: 2.031416972478231

Epoch: 6| Step: 10
Training loss: 2.257129430770874
Validation loss: 2.02999218304952

Epoch: 6| Step: 11
Training loss: 2.0862863063812256
Validation loss: 2.0202801624933877

Epoch: 6| Step: 12
Training loss: 2.358405351638794
Validation loss: 2.025882681210836

Epoch: 6| Step: 13
Training loss: 1.7033835649490356
Validation loss: 2.0268296599388123

Epoch: 88| Step: 0
Training loss: 2.118640184402466
Validation loss: 2.015507400035858

Epoch: 6| Step: 1
Training loss: 2.2227678298950195
Validation loss: 2.0244547923405967

Epoch: 6| Step: 2
Training loss: 1.5696189403533936
Validation loss: 2.0139015118281045

Epoch: 6| Step: 3
Training loss: 2.2480883598327637
Validation loss: 2.0171872973442078

Epoch: 6| Step: 4
Training loss: 1.9598064422607422
Validation loss: 2.019387722015381

Epoch: 6| Step: 5
Training loss: 2.1282477378845215
Validation loss: 2.014256238937378

Epoch: 6| Step: 6
Training loss: 2.5072312355041504
Validation loss: 2.015182594458262

Epoch: 6| Step: 7
Training loss: 2.690396308898926
Validation loss: 2.0158313711484275

Epoch: 6| Step: 8
Training loss: 1.7270597219467163
Validation loss: 2.0105069677035012

Epoch: 6| Step: 9
Training loss: 2.171725273132324
Validation loss: 2.012813448905945

Epoch: 6| Step: 10
Training loss: 2.5347256660461426
Validation loss: 2.009028971195221

Epoch: 6| Step: 11
Training loss: 1.8649691343307495
Validation loss: 2.019841949144999

Epoch: 6| Step: 12
Training loss: 2.174901008605957
Validation loss: 2.019718289375305

Epoch: 6| Step: 13
Training loss: 1.9679722785949707
Validation loss: 2.0062576134999595

Epoch: 89| Step: 0
Training loss: 2.548521041870117
Validation loss: 2.0107993880907693

Epoch: 6| Step: 1
Training loss: 2.0644872188568115
Validation loss: 2.017029881477356

Epoch: 6| Step: 2
Training loss: 2.4618782997131348
Validation loss: 2.018985470136007

Epoch: 6| Step: 3
Training loss: 1.9545973539352417
Validation loss: 2.0265552004178367

Epoch: 6| Step: 4
Training loss: 2.8809032440185547
Validation loss: 2.026532610257467

Epoch: 6| Step: 5
Training loss: 2.21266770362854
Validation loss: 2.019005596637726

Epoch: 6| Step: 6
Training loss: 2.24393367767334
Validation loss: 2.026938239733378

Epoch: 6| Step: 7
Training loss: 2.088897466659546
Validation loss: 2.0227646430333457

Epoch: 6| Step: 8
Training loss: 1.975400686264038
Validation loss: 2.023294766743978

Epoch: 6| Step: 9
Training loss: 1.2144070863723755
Validation loss: 2.0239150126775107

Epoch: 6| Step: 10
Training loss: 2.1082682609558105
Validation loss: 2.0249085426330566

Epoch: 6| Step: 11
Training loss: 1.8629987239837646
Validation loss: 2.025107741355896

Epoch: 6| Step: 12
Training loss: 2.1891608238220215
Validation loss: 2.0286800066630044

Epoch: 6| Step: 13
Training loss: 2.119347333908081
Validation loss: 2.0272124807039895

Epoch: 90| Step: 0
Training loss: 1.4303730726242065
Validation loss: 2.024256447950999

Epoch: 6| Step: 1
Training loss: 2.2380805015563965
Validation loss: 2.026282548904419

Epoch: 6| Step: 2
Training loss: 2.5930538177490234
Validation loss: 2.0324930548667908

Epoch: 6| Step: 3
Training loss: 1.999701738357544
Validation loss: 2.04116823275884

Epoch: 6| Step: 4
Training loss: 2.764338254928589
Validation loss: 2.0263843139012656

Epoch: 6| Step: 5
Training loss: 2.1733317375183105
Validation loss: 2.02497265736262

Epoch: 6| Step: 6
Training loss: 2.213925838470459
Validation loss: 2.0322295228640237

Epoch: 6| Step: 7
Training loss: 1.6512278318405151
Validation loss: 2.0369107325871787

Epoch: 6| Step: 8
Training loss: 2.271976947784424
Validation loss: 2.032220403353373

Epoch: 6| Step: 9
Training loss: 1.8454506397247314
Validation loss: 2.025470276673635

Epoch: 6| Step: 10
Training loss: 2.2454771995544434
Validation loss: 2.021452267964681

Epoch: 6| Step: 11
Training loss: 2.360875129699707
Validation loss: 2.0291077693303428

Epoch: 6| Step: 12
Training loss: 1.9338181018829346
Validation loss: 2.028015414873759

Epoch: 6| Step: 13
Training loss: 2.2779812812805176
Validation loss: 2.0247435768445334

Epoch: 91| Step: 0
Training loss: 1.9759637117385864
Validation loss: 2.0226054191589355

Epoch: 6| Step: 1
Training loss: 2.0166447162628174
Validation loss: 2.0209421118100486

Epoch: 6| Step: 2
Training loss: 2.252490997314453
Validation loss: 2.01797749598821

Epoch: 6| Step: 3
Training loss: 2.499394178390503
Validation loss: 2.0158987045288086

Epoch: 6| Step: 4
Training loss: 2.1003942489624023
Validation loss: 2.020006855328878

Epoch: 6| Step: 5
Training loss: 2.4633021354675293
Validation loss: 2.0223206082979837

Epoch: 6| Step: 6
Training loss: 2.234867811203003
Validation loss: 2.024282912413279

Epoch: 6| Step: 7
Training loss: 2.291934013366699
Validation loss: 2.024247090021769

Epoch: 6| Step: 8
Training loss: 1.7105698585510254
Validation loss: 2.0196446776390076

Epoch: 6| Step: 9
Training loss: 2.2898082733154297
Validation loss: 2.0260847012201944

Epoch: 6| Step: 10
Training loss: 1.790825605392456
Validation loss: 2.0209925174713135

Epoch: 6| Step: 11
Training loss: 1.8723700046539307
Validation loss: 2.027817944685618

Epoch: 6| Step: 12
Training loss: 1.7050737142562866
Validation loss: 2.036604622999827

Epoch: 6| Step: 13
Training loss: 2.484074592590332
Validation loss: 2.034131944179535

Epoch: 92| Step: 0
Training loss: 2.3470020294189453
Validation loss: 2.0345338185628257

Epoch: 6| Step: 1
Training loss: 1.97161865234375
Validation loss: 2.0362163384755454

Epoch: 6| Step: 2
Training loss: 1.9349284172058105
Validation loss: 2.0262263417243958

Epoch: 6| Step: 3
Training loss: 2.327986478805542
Validation loss: 2.038140038649241

Epoch: 6| Step: 4
Training loss: 1.8051965236663818
Validation loss: 2.0277761022249856

Epoch: 6| Step: 5
Training loss: 2.3746931552886963
Validation loss: 2.0338948170344033

Epoch: 6| Step: 6
Training loss: 2.472123861312866
Validation loss: 2.0387731393178306

Epoch: 6| Step: 7
Training loss: 1.7389310598373413
Validation loss: 2.03129251797994

Epoch: 6| Step: 8
Training loss: 1.8859786987304688
Validation loss: 2.0286511381467185

Epoch: 6| Step: 9
Training loss: 2.678802490234375
Validation loss: 2.016514221827189

Epoch: 6| Step: 10
Training loss: 2.134312629699707
Validation loss: 2.0133981307347617

Epoch: 6| Step: 11
Training loss: 1.6930038928985596
Validation loss: 2.0196272134780884

Epoch: 6| Step: 12
Training loss: 2.100775718688965
Validation loss: 2.018942654132843

Epoch: 6| Step: 13
Training loss: 2.458707094192505
Validation loss: 2.0208373268445334

Epoch: 93| Step: 0
Training loss: 2.3110709190368652
Validation loss: 2.0184065302213035

Epoch: 6| Step: 1
Training loss: 2.498849630355835
Validation loss: 2.0075077613194785

Epoch: 6| Step: 2
Training loss: 2.4693801403045654
Validation loss: 2.024534742037455

Epoch: 6| Step: 3
Training loss: 2.319489002227783
Validation loss: 2.0234806140263877

Epoch: 6| Step: 4
Training loss: 2.361464262008667
Validation loss: 2.0239608685175576

Epoch: 6| Step: 5
Training loss: 2.4047675132751465
Validation loss: 2.030451794465383

Epoch: 6| Step: 6
Training loss: 1.7441915273666382
Validation loss: 2.0336281458536782

Epoch: 6| Step: 7
Training loss: 1.757213830947876
Validation loss: 2.0270307262738547

Epoch: 6| Step: 8
Training loss: 2.2472195625305176
Validation loss: 2.0462267994880676

Epoch: 6| Step: 9
Training loss: 2.1017184257507324
Validation loss: 2.0627851088841758

Epoch: 6| Step: 10
Training loss: 1.7157396078109741
Validation loss: 2.0358789761861167

Epoch: 6| Step: 11
Training loss: 2.5369157791137695
Validation loss: 2.0400158762931824

Epoch: 6| Step: 12
Training loss: 1.8292906284332275
Validation loss: 2.044405937194824

Epoch: 6| Step: 13
Training loss: 1.4933615922927856
Validation loss: 2.0442554354667664

Epoch: 94| Step: 0
Training loss: 2.400169849395752
Validation loss: 2.0324247082074485

Epoch: 6| Step: 1
Training loss: 1.9804458618164062
Validation loss: 2.033419609069824

Epoch: 6| Step: 2
Training loss: 2.09580135345459
Validation loss: 2.034872313340505

Epoch: 6| Step: 3
Training loss: 1.7311112880706787
Validation loss: 2.043146093686422

Epoch: 6| Step: 4
Training loss: 2.435412645339966
Validation loss: 2.03676700592041

Epoch: 6| Step: 5
Training loss: 1.8631082773208618
Validation loss: 2.0362001061439514

Epoch: 6| Step: 6
Training loss: 1.7178651094436646
Validation loss: 2.0326950748761496

Epoch: 6| Step: 7
Training loss: 2.5411086082458496
Validation loss: 2.030665119489034

Epoch: 6| Step: 8
Training loss: 1.9783287048339844
Validation loss: 2.032836933930715

Epoch: 6| Step: 9
Training loss: 2.288274049758911
Validation loss: 2.0337239106496177

Epoch: 6| Step: 10
Training loss: 1.7945383787155151
Validation loss: 2.0224144061406455

Epoch: 6| Step: 11
Training loss: 2.1291418075561523
Validation loss: 2.028900384902954

Epoch: 6| Step: 12
Training loss: 2.5351312160491943
Validation loss: 2.0335094730059304

Epoch: 6| Step: 13
Training loss: 2.3584864139556885
Validation loss: 2.0387174089749656

Epoch: 95| Step: 0
Training loss: 2.389220714569092
Validation loss: 2.033891240755717

Epoch: 6| Step: 1
Training loss: 1.442363977432251
Validation loss: 2.036380728085836

Epoch: 6| Step: 2
Training loss: 2.2939677238464355
Validation loss: 2.0416476726531982

Epoch: 6| Step: 3
Training loss: 1.9882457256317139
Validation loss: 2.0366249680519104

Epoch: 6| Step: 4
Training loss: 2.6542134284973145
Validation loss: 2.0382677714029946

Epoch: 6| Step: 5
Training loss: 1.5182394981384277
Validation loss: 2.0405369997024536

Epoch: 6| Step: 6
Training loss: 2.2129626274108887
Validation loss: 2.035859147707621

Epoch: 6| Step: 7
Training loss: 2.4076156616210938
Validation loss: 2.0421242316563926

Epoch: 6| Step: 8
Training loss: 2.0503954887390137
Validation loss: 2.0443290869394937

Epoch: 6| Step: 9
Training loss: 1.7718439102172852
Validation loss: 2.042413910230001

Epoch: 6| Step: 10
Training loss: 1.8258607387542725
Validation loss: 2.0444517135620117

Epoch: 6| Step: 11
Training loss: 2.3916375637054443
Validation loss: 2.033363938331604

Epoch: 6| Step: 12
Training loss: 2.458925247192383
Validation loss: 2.0291221737861633

Epoch: 6| Step: 13
Training loss: 2.222607135772705
Validation loss: 2.0439444184303284

Epoch: 96| Step: 0
Training loss: 1.8087794780731201
Validation loss: 2.0283823808034263

Epoch: 6| Step: 1
Training loss: 2.3179683685302734
Validation loss: 2.036267618338267

Epoch: 6| Step: 2
Training loss: 1.7670955657958984
Validation loss: 2.031670331954956

Epoch: 6| Step: 3
Training loss: 1.7744951248168945
Validation loss: 2.035993834336599

Epoch: 6| Step: 4
Training loss: 2.024944543838501
Validation loss: 2.0213422973950705

Epoch: 6| Step: 5
Training loss: 2.1809892654418945
Validation loss: 2.0340579549471536

Epoch: 6| Step: 6
Training loss: 1.7497589588165283
Validation loss: 2.03007302681605

Epoch: 6| Step: 7
Training loss: 2.3864686489105225
Validation loss: 2.030937612056732

Epoch: 6| Step: 8
Training loss: 2.1985714435577393
Validation loss: 2.028187016646067

Epoch: 6| Step: 9
Training loss: 2.5869929790496826
Validation loss: 2.020371655623118

Epoch: 6| Step: 10
Training loss: 1.5759795904159546
Validation loss: 2.032130479812622

Epoch: 6| Step: 11
Training loss: 2.505033493041992
Validation loss: 2.040040453275045

Epoch: 6| Step: 12
Training loss: 2.6109116077423096
Validation loss: 2.036315898100535

Epoch: 6| Step: 13
Training loss: 2.01680588722229
Validation loss: 2.0384528636932373

Epoch: 97| Step: 0
Training loss: 1.5577865839004517
Validation loss: 2.0349186062812805

Epoch: 6| Step: 1
Training loss: 1.9382975101470947
Validation loss: 2.0318309466044107

Epoch: 6| Step: 2
Training loss: 1.5722553730010986
Validation loss: 2.0328067342440286

Epoch: 6| Step: 3
Training loss: 2.3561410903930664
Validation loss: 2.018580595652262

Epoch: 6| Step: 4
Training loss: 2.811903953552246
Validation loss: 2.027546008427938

Epoch: 6| Step: 5
Training loss: 1.7257139682769775
Validation loss: 2.0237664779027305

Epoch: 6| Step: 6
Training loss: 2.353071689605713
Validation loss: 2.0097195704778037

Epoch: 6| Step: 7
Training loss: 1.819753885269165
Validation loss: 2.0087862412134805

Epoch: 6| Step: 8
Training loss: 2.1963248252868652
Validation loss: 2.0082262555758157

Epoch: 6| Step: 9
Training loss: 1.7494089603424072
Validation loss: 2.0149988532066345

Epoch: 6| Step: 10
Training loss: 2.1539855003356934
Validation loss: 2.0033000111579895

Epoch: 6| Step: 11
Training loss: 2.21579647064209
Validation loss: 2.0020750959714255

Epoch: 6| Step: 12
Training loss: 2.383234977722168
Validation loss: 2.015888830025991

Epoch: 6| Step: 13
Training loss: 2.841169595718384
Validation loss: 2.0207008123397827

Epoch: 98| Step: 0
Training loss: 2.3382115364074707
Validation loss: 2.021264453728994

Epoch: 6| Step: 1
Training loss: 1.7215962409973145
Validation loss: 2.020374079545339

Epoch: 6| Step: 2
Training loss: 2.459083080291748
Validation loss: 2.0225565234820047

Epoch: 6| Step: 3
Training loss: 2.9311752319335938
Validation loss: 2.025937298933665

Epoch: 6| Step: 4
Training loss: 1.527039647102356
Validation loss: 2.0259225368499756

Epoch: 6| Step: 5
Training loss: 2.235337257385254
Validation loss: 2.0236422220865884

Epoch: 6| Step: 6
Training loss: 2.136826992034912
Validation loss: 2.0178990165392556

Epoch: 6| Step: 7
Training loss: 2.020862102508545
Validation loss: 2.01421449581782

Epoch: 6| Step: 8
Training loss: 2.2114624977111816
Validation loss: 2.0076628724733987

Epoch: 6| Step: 9
Training loss: 2.342259168624878
Validation loss: 2.0073623061180115

Epoch: 6| Step: 10
Training loss: 2.4333531856536865
Validation loss: 2.008532722791036

Epoch: 6| Step: 11
Training loss: 1.7491300106048584
Validation loss: 2.0148509740829468

Epoch: 6| Step: 12
Training loss: 1.6913431882858276
Validation loss: 2.013965090115865

Epoch: 6| Step: 13
Training loss: 2.3191027641296387
Validation loss: 2.0292803843816123

Epoch: 99| Step: 0
Training loss: 2.1220970153808594
Validation loss: 2.0246669252713523

Epoch: 6| Step: 1
Training loss: 2.1219372749328613
Validation loss: 2.0336092511812844

Epoch: 6| Step: 2
Training loss: 2.1348719596862793
Validation loss: 2.0397854447364807

Epoch: 6| Step: 3
Training loss: 1.5469329357147217
Validation loss: 2.0318657954533896

Epoch: 6| Step: 4
Training loss: 2.3553943634033203
Validation loss: 2.03699920574824

Epoch: 6| Step: 5
Training loss: 2.0102546215057373
Validation loss: 2.0323848724365234

Epoch: 6| Step: 6
Training loss: 2.1284379959106445
Validation loss: 2.027758777141571

Epoch: 6| Step: 7
Training loss: 1.713680624961853
Validation loss: 2.0308496952056885

Epoch: 6| Step: 8
Training loss: 2.3219759464263916
Validation loss: 2.0316365559895835

Epoch: 6| Step: 9
Training loss: 2.097303867340088
Validation loss: 2.0332566698392234

Epoch: 6| Step: 10
Training loss: 2.380612373352051
Validation loss: 2.03881307442983

Epoch: 6| Step: 11
Training loss: 2.6432390213012695
Validation loss: 2.0361544489860535

Epoch: 6| Step: 12
Training loss: 1.5290005207061768
Validation loss: 2.0391916036605835

Epoch: 6| Step: 13
Training loss: 2.346529006958008
Validation loss: 2.030978182951609

Epoch: 100| Step: 0
Training loss: 1.8010616302490234
Validation loss: 2.03791751464208

Epoch: 6| Step: 1
Training loss: 2.410224437713623
Validation loss: 2.0272180438041687

Epoch: 6| Step: 2
Training loss: 2.2762656211853027
Validation loss: 2.026922106742859

Epoch: 6| Step: 3
Training loss: 1.4391298294067383
Validation loss: 2.031612972418467

Epoch: 6| Step: 4
Training loss: 2.162339210510254
Validation loss: 2.02752415339152

Epoch: 6| Step: 5
Training loss: 2.5965099334716797
Validation loss: 2.02980907758077

Epoch: 6| Step: 6
Training loss: 2.712451457977295
Validation loss: 2.036178688208262

Epoch: 6| Step: 7
Training loss: 2.516869306564331
Validation loss: 2.0337828993797302

Epoch: 6| Step: 8
Training loss: 1.781118631362915
Validation loss: 2.0375897685686746

Epoch: 6| Step: 9
Training loss: 1.6501377820968628
Validation loss: 2.0461087226867676

Epoch: 6| Step: 10
Training loss: 1.955419898033142
Validation loss: 2.0368728041648865

Epoch: 6| Step: 11
Training loss: 1.7728543281555176
Validation loss: 2.0428602496782937

Epoch: 6| Step: 12
Training loss: 2.1259121894836426
Validation loss: 2.0385101437568665

Epoch: 6| Step: 13
Training loss: 2.1892709732055664
Validation loss: 2.048204163710276

Epoch: 101| Step: 0
Training loss: 1.3651115894317627
Validation loss: 2.0461286902427673

Epoch: 6| Step: 1
Training loss: 1.634587287902832
Validation loss: 2.0579991141955056

Epoch: 6| Step: 2
Training loss: 2.1012637615203857
Validation loss: 2.0598087509473166

Epoch: 6| Step: 3
Training loss: 2.6495625972747803
Validation loss: 2.054656724135081

Epoch: 6| Step: 4
Training loss: 2.044881820678711
Validation loss: 2.0426418781280518

Epoch: 6| Step: 5
Training loss: 2.0900518894195557
Validation loss: 2.046634256839752

Epoch: 6| Step: 6
Training loss: 2.0961904525756836
Validation loss: 2.0318955977757773

Epoch: 6| Step: 7
Training loss: 2.2145161628723145
Validation loss: 2.0366876125335693

Epoch: 6| Step: 8
Training loss: 2.301027774810791
Validation loss: 2.0487616260846457

Epoch: 6| Step: 9
Training loss: 2.1893198490142822
Validation loss: 2.0305742422739663

Epoch: 6| Step: 10
Training loss: 2.6066207885742188
Validation loss: 2.022336502869924

Epoch: 6| Step: 11
Training loss: 2.244049072265625
Validation loss: 2.0206191738446555

Epoch: 6| Step: 12
Training loss: 1.9240972995758057
Validation loss: 2.015619933605194

Epoch: 6| Step: 13
Training loss: 1.8751121759414673
Validation loss: 2.022403101126353

Epoch: 102| Step: 0
Training loss: 2.181417465209961
Validation loss: 2.024497667948405

Epoch: 6| Step: 1
Training loss: 2.1837220191955566
Validation loss: 2.019718587398529

Epoch: 6| Step: 2
Training loss: 2.1629855632781982
Validation loss: 2.028621196746826

Epoch: 6| Step: 3
Training loss: 2.1080703735351562
Validation loss: 2.0235222975413003

Epoch: 6| Step: 4
Training loss: 1.9306955337524414
Validation loss: 2.022886335849762

Epoch: 6| Step: 5
Training loss: 2.4980428218841553
Validation loss: 2.015102763970693

Epoch: 6| Step: 6
Training loss: 1.8109121322631836
Validation loss: 2.0151782433191934

Epoch: 6| Step: 7
Training loss: 1.6383264064788818
Validation loss: 2.0123693346977234

Epoch: 6| Step: 8
Training loss: 2.1910457611083984
Validation loss: 2.006986141204834

Epoch: 6| Step: 9
Training loss: 1.937820315361023
Validation loss: 2.0066087444623313

Epoch: 6| Step: 10
Training loss: 2.650146484375
Validation loss: 2.0020514329274497

Epoch: 6| Step: 11
Training loss: 2.456085681915283
Validation loss: 2.0040641824404397

Epoch: 6| Step: 12
Training loss: 1.5416535139083862
Validation loss: 2.0193796952565513

Epoch: 6| Step: 13
Training loss: 2.3372554779052734
Validation loss: 2.017694572607676

Epoch: 103| Step: 0
Training loss: 2.679879903793335
Validation loss: 2.0249712467193604

Epoch: 6| Step: 1
Training loss: 1.6222915649414062
Validation loss: 2.028556783994039

Epoch: 6| Step: 2
Training loss: 2.5323638916015625
Validation loss: 2.023748993873596

Epoch: 6| Step: 3
Training loss: 2.02909779548645
Validation loss: 2.0116234620412192

Epoch: 6| Step: 4
Training loss: 1.6634373664855957
Validation loss: 2.013359288374583

Epoch: 6| Step: 5
Training loss: 1.973902702331543
Validation loss: 2.008750935395559

Epoch: 6| Step: 6
Training loss: 2.4619975090026855
Validation loss: 2.0168142120043435

Epoch: 6| Step: 7
Training loss: 2.0155951976776123
Validation loss: 2.0178670088450112

Epoch: 6| Step: 8
Training loss: 1.7057886123657227
Validation loss: 2.0186509092648826

Epoch: 6| Step: 9
Training loss: 1.990502953529358
Validation loss: 2.013049046198527

Epoch: 6| Step: 10
Training loss: 2.1057775020599365
Validation loss: 2.025958478450775

Epoch: 6| Step: 11
Training loss: 2.18027400970459
Validation loss: 2.025958021481832

Epoch: 6| Step: 12
Training loss: 2.1183042526245117
Validation loss: 2.0288968483606973

Epoch: 6| Step: 13
Training loss: 2.4211292266845703
Validation loss: 2.0304304162661233

Epoch: 104| Step: 0
Training loss: 2.726228952407837
Validation loss: 2.0205387473106384

Epoch: 6| Step: 1
Training loss: 1.8239099979400635
Validation loss: 2.0340868631998696

Epoch: 6| Step: 2
Training loss: 2.33143949508667
Validation loss: 2.022512217362722

Epoch: 6| Step: 3
Training loss: 1.5154141187667847
Validation loss: 2.0260489781697593

Epoch: 6| Step: 4
Training loss: 2.004535436630249
Validation loss: 2.0196384390195212

Epoch: 6| Step: 5
Training loss: 2.500123977661133
Validation loss: 2.0232081015904746

Epoch: 6| Step: 6
Training loss: 2.5035839080810547
Validation loss: 2.024575412273407

Epoch: 6| Step: 7
Training loss: 2.3184120655059814
Validation loss: 2.029917001724243

Epoch: 6| Step: 8
Training loss: 1.7426282167434692
Validation loss: 2.0323396722475686

Epoch: 6| Step: 9
Training loss: 2.54481840133667
Validation loss: 2.0268425941467285

Epoch: 6| Step: 10
Training loss: 1.6061749458312988
Validation loss: 2.033412237962087

Epoch: 6| Step: 11
Training loss: 1.909757375717163
Validation loss: 2.0353219509124756

Epoch: 6| Step: 12
Training loss: 1.5391037464141846
Validation loss: 2.0323707660039267

Epoch: 6| Step: 13
Training loss: 2.104595422744751
Validation loss: 2.039509971936544

Epoch: 105| Step: 0
Training loss: 2.2300782203674316
Validation loss: 2.0441553393999734

Epoch: 6| Step: 1
Training loss: 1.4204713106155396
Validation loss: 2.0470632712046304

Epoch: 6| Step: 2
Training loss: 1.985978603363037
Validation loss: 2.0396335323651633

Epoch: 6| Step: 3
Training loss: 2.376652956008911
Validation loss: 2.0423413117726645

Epoch: 6| Step: 4
Training loss: 2.214092493057251
Validation loss: 2.0544930497805276

Epoch: 6| Step: 5
Training loss: 1.662962794303894
Validation loss: 2.0317310094833374

Epoch: 6| Step: 6
Training loss: 1.7902450561523438
Validation loss: 2.04106213649114

Epoch: 6| Step: 7
Training loss: 3.0414814949035645
Validation loss: 2.0356149673461914

Epoch: 6| Step: 8
Training loss: 1.87786865234375
Validation loss: 2.0356276631355286

Epoch: 6| Step: 9
Training loss: 2.372347831726074
Validation loss: 2.035354475180308

Epoch: 6| Step: 10
Training loss: 2.264317274093628
Validation loss: 2.0300258795420327

Epoch: 6| Step: 11
Training loss: 2.1016533374786377
Validation loss: 2.0316261450449624

Epoch: 6| Step: 12
Training loss: 1.91652250289917
Validation loss: 2.0443442662556968

Epoch: 6| Step: 13
Training loss: 2.12876558303833
Validation loss: 2.0420746008555093

Epoch: 106| Step: 0
Training loss: 2.3313989639282227
Validation loss: 2.0429374972979226

Epoch: 6| Step: 1
Training loss: 1.720280647277832
Validation loss: 2.0427962144215903

Epoch: 6| Step: 2
Training loss: 2.150414228439331
Validation loss: 2.0404781301816306

Epoch: 6| Step: 3
Training loss: 1.8569741249084473
Validation loss: 2.034672200679779

Epoch: 6| Step: 4
Training loss: 1.7215287685394287
Validation loss: 2.0351529717445374

Epoch: 6| Step: 5
Training loss: 1.7901592254638672
Validation loss: 2.030128796895345

Epoch: 6| Step: 6
Training loss: 2.291457414627075
Validation loss: 2.0415937105814614

Epoch: 6| Step: 7
Training loss: 1.8365055322647095
Validation loss: 2.041923781236013

Epoch: 6| Step: 8
Training loss: 2.2706451416015625
Validation loss: 2.0393730799357095

Epoch: 6| Step: 9
Training loss: 2.1223769187927246
Validation loss: 2.042712231477102

Epoch: 6| Step: 10
Training loss: 1.7960928678512573
Validation loss: 2.0489779313405356

Epoch: 6| Step: 11
Training loss: 2.308889627456665
Validation loss: 2.0517385601997375

Epoch: 6| Step: 12
Training loss: 2.4923744201660156
Validation loss: 2.0476898749669394

Epoch: 6| Step: 13
Training loss: 2.3104307651519775
Validation loss: 2.036692261695862

Epoch: 107| Step: 0
Training loss: 2.3360791206359863
Validation loss: 2.041709542274475

Epoch: 6| Step: 1
Training loss: 2.236948013305664
Validation loss: 2.040846049785614

Epoch: 6| Step: 2
Training loss: 1.9756731986999512
Validation loss: 2.0353915294011435

Epoch: 6| Step: 3
Training loss: 2.2956953048706055
Validation loss: 2.027745564778646

Epoch: 6| Step: 4
Training loss: 1.8442754745483398
Validation loss: 2.035252312819163

Epoch: 6| Step: 5
Training loss: 2.0004076957702637
Validation loss: 2.02798585096995

Epoch: 6| Step: 6
Training loss: 2.059904098510742
Validation loss: 2.0343623757362366

Epoch: 6| Step: 7
Training loss: 2.229978084564209
Validation loss: 2.03202885389328

Epoch: 6| Step: 8
Training loss: 2.1068334579467773
Validation loss: 2.0258686542510986

Epoch: 6| Step: 9
Training loss: 1.9398659467697144
Validation loss: 2.0225455363591514

Epoch: 6| Step: 10
Training loss: 1.9985978603363037
Validation loss: 2.027728855609894

Epoch: 6| Step: 11
Training loss: 1.9330079555511475
Validation loss: 2.047154982884725

Epoch: 6| Step: 12
Training loss: 2.1597766876220703
Validation loss: 2.053041418393453

Epoch: 6| Step: 13
Training loss: 2.2780590057373047
Validation loss: 2.039803385734558

Epoch: 108| Step: 0
Training loss: 2.516108751296997
Validation loss: 2.0347729325294495

Epoch: 6| Step: 1
Training loss: 2.0293116569519043
Validation loss: 2.022321621576945

Epoch: 6| Step: 2
Training loss: 2.0931358337402344
Validation loss: 2.0238558650016785

Epoch: 6| Step: 3
Training loss: 2.209444999694824
Validation loss: 2.021180828412374

Epoch: 6| Step: 4
Training loss: 2.100851535797119
Validation loss: 2.0140966375668845

Epoch: 6| Step: 5
Training loss: 1.892381191253662
Validation loss: 2.019606908162435

Epoch: 6| Step: 6
Training loss: 2.130307197570801
Validation loss: 2.015080372492472

Epoch: 6| Step: 7
Training loss: 2.3677761554718018
Validation loss: 2.019712587197622

Epoch: 6| Step: 8
Training loss: 1.8889503479003906
Validation loss: 2.0233726501464844

Epoch: 6| Step: 9
Training loss: 1.8979231119155884
Validation loss: 2.0228653152783713

Epoch: 6| Step: 10
Training loss: 2.259519577026367
Validation loss: 2.027297794818878

Epoch: 6| Step: 11
Training loss: 1.8272007703781128
Validation loss: 2.0189491510391235

Epoch: 6| Step: 12
Training loss: 2.324462413787842
Validation loss: 2.0150180061658225

Epoch: 6| Step: 13
Training loss: 2.0711007118225098
Validation loss: 2.0186892549196878

Epoch: 109| Step: 0
Training loss: 2.317042112350464
Validation loss: 2.0280465881029763

Epoch: 6| Step: 1
Training loss: 2.454678773880005
Validation loss: 2.038728177547455

Epoch: 6| Step: 2
Training loss: 2.091606855392456
Validation loss: 2.04343048731486

Epoch: 6| Step: 3
Training loss: 2.078756332397461
Validation loss: 2.047486186027527

Epoch: 6| Step: 4
Training loss: 1.4578652381896973
Validation loss: 2.042559246222178

Epoch: 6| Step: 5
Training loss: 1.685774326324463
Validation loss: 2.051508903503418

Epoch: 6| Step: 6
Training loss: 2.38276743888855
Validation loss: 2.0525511304537454

Epoch: 6| Step: 7
Training loss: 2.6485588550567627
Validation loss: 2.0448922514915466

Epoch: 6| Step: 8
Training loss: 2.1788251399993896
Validation loss: 2.053002138932546

Epoch: 6| Step: 9
Training loss: 2.1573638916015625
Validation loss: 2.035661300023397

Epoch: 6| Step: 10
Training loss: 1.7551896572113037
Validation loss: 2.050065596898397

Epoch: 6| Step: 11
Training loss: 1.8790154457092285
Validation loss: 2.047934909661611

Epoch: 6| Step: 12
Training loss: 2.065938949584961
Validation loss: 2.05106790860494

Epoch: 6| Step: 13
Training loss: 1.9703428745269775
Validation loss: 2.049160679181417

Epoch: 110| Step: 0
Training loss: 2.240885019302368
Validation loss: 2.0509906808535256

Epoch: 6| Step: 1
Training loss: 2.1949729919433594
Validation loss: 2.0593299667040506

Epoch: 6| Step: 2
Training loss: 2.291311025619507
Validation loss: 2.0598895947138467

Epoch: 6| Step: 3
Training loss: 1.666878581047058
Validation loss: 2.0542917052904763

Epoch: 6| Step: 4
Training loss: 1.831902027130127
Validation loss: 2.0586666067441306

Epoch: 6| Step: 5
Training loss: 1.8473758697509766
Validation loss: 2.0655890504519143

Epoch: 6| Step: 6
Training loss: 2.7337260246276855
Validation loss: 2.049557685852051

Epoch: 6| Step: 7
Training loss: 2.1690940856933594
Validation loss: 2.057284673055013

Epoch: 6| Step: 8
Training loss: 2.492626190185547
Validation loss: 2.0484269658724465

Epoch: 6| Step: 9
Training loss: 2.02163028717041
Validation loss: 2.053868532180786

Epoch: 6| Step: 10
Training loss: 1.9511407613754272
Validation loss: 2.0492674708366394

Epoch: 6| Step: 11
Training loss: 2.348482608795166
Validation loss: 2.044915020465851

Epoch: 6| Step: 12
Training loss: 1.8245539665222168
Validation loss: 2.0476730465888977

Epoch: 6| Step: 13
Training loss: 1.4610483646392822
Validation loss: 2.042865494887034

Epoch: 111| Step: 0
Training loss: 2.292771339416504
Validation loss: 2.0426547129948935

Epoch: 6| Step: 1
Training loss: 2.0065813064575195
Validation loss: 2.0448944767316184

Epoch: 6| Step: 2
Training loss: 1.9561281204223633
Validation loss: 2.0439359148343406

Epoch: 6| Step: 3
Training loss: 1.9058061838150024
Validation loss: 2.053558647632599

Epoch: 6| Step: 4
Training loss: 1.8743433952331543
Validation loss: 2.049463709195455

Epoch: 6| Step: 5
Training loss: 1.8044612407684326
Validation loss: 2.0560391942660012

Epoch: 6| Step: 6
Training loss: 2.4417567253112793
Validation loss: 2.0704861680666604

Epoch: 6| Step: 7
Training loss: 1.9020183086395264
Validation loss: 2.0735257466634116

Epoch: 6| Step: 8
Training loss: 2.158865451812744
Validation loss: 2.059658408164978

Epoch: 6| Step: 9
Training loss: 1.9720035791397095
Validation loss: 2.0669723749160767

Epoch: 6| Step: 10
Training loss: 2.4082577228546143
Validation loss: 2.066547373930613

Epoch: 6| Step: 11
Training loss: 1.93363618850708
Validation loss: 2.0421231587727866

Epoch: 6| Step: 12
Training loss: 2.351069927215576
Validation loss: 2.050331930319468

Epoch: 6| Step: 13
Training loss: 2.3166003227233887
Validation loss: 2.0372511545817056

Epoch: 112| Step: 0
Training loss: 2.290987014770508
Validation loss: 2.0479098161061606

Epoch: 6| Step: 1
Training loss: 1.8842413425445557
Validation loss: 2.03637288014094

Epoch: 6| Step: 2
Training loss: 2.503933906555176
Validation loss: 2.049778719743093

Epoch: 6| Step: 3
Training loss: 2.131274700164795
Validation loss: 2.0546852350234985

Epoch: 6| Step: 4
Training loss: 2.184849977493286
Validation loss: 2.0490500728289285

Epoch: 6| Step: 5
Training loss: 2.0711474418640137
Validation loss: 2.0463279287020364

Epoch: 6| Step: 6
Training loss: 1.791864275932312
Validation loss: 2.050795336564382

Epoch: 6| Step: 7
Training loss: 1.777555227279663
Validation loss: 2.0599924325942993

Epoch: 6| Step: 8
Training loss: 2.568058967590332
Validation loss: 2.0533816615740457

Epoch: 6| Step: 9
Training loss: 1.7265362739562988
Validation loss: 2.0459131399790444

Epoch: 6| Step: 10
Training loss: 2.2717766761779785
Validation loss: 2.0475558638572693

Epoch: 6| Step: 11
Training loss: 1.9120509624481201
Validation loss: 2.0416674415270486

Epoch: 6| Step: 12
Training loss: 1.813334584236145
Validation loss: 2.0550225377082825

Epoch: 6| Step: 13
Training loss: 2.0907578468322754
Validation loss: 2.055341442426046

Epoch: 113| Step: 0
Training loss: 2.105788469314575
Validation loss: 2.052406350771586

Epoch: 6| Step: 1
Training loss: 1.6222333908081055
Validation loss: 2.0452205737431846

Epoch: 6| Step: 2
Training loss: 2.553138256072998
Validation loss: 2.0355899930000305

Epoch: 6| Step: 3
Training loss: 2.0995144844055176
Validation loss: 2.0407260855038962

Epoch: 6| Step: 4
Training loss: 2.211099147796631
Validation loss: 2.0342911879221597

Epoch: 6| Step: 5
Training loss: 2.5187206268310547
Validation loss: 2.0375367403030396

Epoch: 6| Step: 6
Training loss: 2.3565335273742676
Validation loss: 2.027764081954956

Epoch: 6| Step: 7
Training loss: 1.775552749633789
Validation loss: 2.0283196568489075

Epoch: 6| Step: 8
Training loss: 2.032238483428955
Validation loss: 2.018347998460134

Epoch: 6| Step: 9
Training loss: 1.629364013671875
Validation loss: 2.0308263103167215

Epoch: 6| Step: 10
Training loss: 2.4362311363220215
Validation loss: 2.0296510060628257

Epoch: 6| Step: 11
Training loss: 1.2844977378845215
Validation loss: 2.0341787338256836

Epoch: 6| Step: 12
Training loss: 2.0747838020324707
Validation loss: 2.0371312300364175

Epoch: 6| Step: 13
Training loss: 2.166848659515381
Validation loss: 2.05174986521403

Epoch: 114| Step: 0
Training loss: 2.0165486335754395
Validation loss: 2.060956438382467

Epoch: 6| Step: 1
Training loss: 2.3994622230529785
Validation loss: 2.083978017171224

Epoch: 6| Step: 2
Training loss: 2.024733066558838
Validation loss: 2.08868408203125

Epoch: 6| Step: 3
Training loss: 1.6726199388504028
Validation loss: 2.0752182602882385

Epoch: 6| Step: 4
Training loss: 2.796818971633911
Validation loss: 2.0570183396339417

Epoch: 6| Step: 5
Training loss: 1.9167006015777588
Validation loss: 2.0398762226104736

Epoch: 6| Step: 6
Training loss: 1.4631459712982178
Validation loss: 2.0263392130533853

Epoch: 6| Step: 7
Training loss: 2.2506091594696045
Validation loss: 2.0340323646863303

Epoch: 6| Step: 8
Training loss: 1.477250337600708
Validation loss: 2.028246541817983

Epoch: 6| Step: 9
Training loss: 2.3408212661743164
Validation loss: 2.0232529242833457

Epoch: 6| Step: 10
Training loss: 2.104160785675049
Validation loss: 2.0245739022890725

Epoch: 6| Step: 11
Training loss: 2.1659421920776367
Validation loss: 2.0197690526644387

Epoch: 6| Step: 12
Training loss: 2.29569149017334
Validation loss: 2.019937018553416

Epoch: 6| Step: 13
Training loss: 2.135781764984131
Validation loss: 2.022440254688263

Epoch: 115| Step: 0
Training loss: 2.2549610137939453
Validation loss: 2.028791387875875

Epoch: 6| Step: 1
Training loss: 2.418478012084961
Validation loss: 2.021826982498169

Epoch: 6| Step: 2
Training loss: 1.7128419876098633
Validation loss: 2.029609739780426

Epoch: 6| Step: 3
Training loss: 1.987776517868042
Validation loss: 2.029610554377238

Epoch: 6| Step: 4
Training loss: 2.006192684173584
Validation loss: 2.017265280087789

Epoch: 6| Step: 5
Training loss: 2.4485411643981934
Validation loss: 2.0253016153971353

Epoch: 6| Step: 6
Training loss: 1.8978593349456787
Validation loss: 2.0364025831222534

Epoch: 6| Step: 7
Training loss: 1.8634874820709229
Validation loss: 2.036900738875071

Epoch: 6| Step: 8
Training loss: 2.0575807094573975
Validation loss: 2.0359022418657937

Epoch: 6| Step: 9
Training loss: 2.923205852508545
Validation loss: 2.034760316212972

Epoch: 6| Step: 10
Training loss: 2.159966468811035
Validation loss: 2.03740918636322

Epoch: 6| Step: 11
Training loss: 1.51302170753479
Validation loss: 2.038494129975637

Epoch: 6| Step: 12
Training loss: 1.887458086013794
Validation loss: 2.033353010813395

Epoch: 6| Step: 13
Training loss: 1.941758155822754
Validation loss: 2.047044277191162

Epoch: 116| Step: 0
Training loss: 2.2134249210357666
Validation loss: 2.047904094060262

Epoch: 6| Step: 1
Training loss: 1.7632566690444946
Validation loss: 2.0438819527626038

Epoch: 6| Step: 2
Training loss: 1.7057256698608398
Validation loss: 2.0499075452486673

Epoch: 6| Step: 3
Training loss: 1.6100552082061768
Validation loss: 2.048156201839447

Epoch: 6| Step: 4
Training loss: 1.7261664867401123
Validation loss: 2.039529323577881

Epoch: 6| Step: 5
Training loss: 2.584294080734253
Validation loss: 2.0485549370447793

Epoch: 6| Step: 6
Training loss: 2.2295660972595215
Validation loss: 2.041418751080831

Epoch: 6| Step: 7
Training loss: 2.019618272781372
Validation loss: 2.028503696123759

Epoch: 6| Step: 8
Training loss: 1.648956298828125
Validation loss: 2.04228546222051

Epoch: 6| Step: 9
Training loss: 2.787227153778076
Validation loss: 2.0465139150619507

Epoch: 6| Step: 10
Training loss: 2.1878724098205566
Validation loss: 2.050362745920817

Epoch: 6| Step: 11
Training loss: 2.194979190826416
Validation loss: 2.048402408758799

Epoch: 6| Step: 12
Training loss: 1.9216862916946411
Validation loss: 2.0343016187349954

Epoch: 6| Step: 13
Training loss: 2.261481523513794
Validation loss: 2.0542383790016174

Epoch: 117| Step: 0
Training loss: 1.19602632522583
Validation loss: 2.0541212956110635

Epoch: 6| Step: 1
Training loss: 2.320458173751831
Validation loss: 2.048814912637075

Epoch: 6| Step: 2
Training loss: 2.138458728790283
Validation loss: 2.0467164715131125

Epoch: 6| Step: 3
Training loss: 1.9426363706588745
Validation loss: 2.0444547136624656

Epoch: 6| Step: 4
Training loss: 2.283062219619751
Validation loss: 2.063801864782969

Epoch: 6| Step: 5
Training loss: 1.9811265468597412
Validation loss: 2.049891710281372

Epoch: 6| Step: 6
Training loss: 2.330077648162842
Validation loss: 2.0601914723714194

Epoch: 6| Step: 7
Training loss: 1.928866982460022
Validation loss: 2.0459829767545066

Epoch: 6| Step: 8
Training loss: 2.0835049152374268
Validation loss: 2.0447670022646585

Epoch: 6| Step: 9
Training loss: 2.2298927307128906
Validation loss: 2.0427793065706887

Epoch: 6| Step: 10
Training loss: 1.8718252182006836
Validation loss: 2.046181797981262

Epoch: 6| Step: 11
Training loss: 2.2501559257507324
Validation loss: 2.044305443763733

Epoch: 6| Step: 12
Training loss: 2.2104203701019287
Validation loss: 2.0521899461746216

Epoch: 6| Step: 13
Training loss: 1.9774084091186523
Validation loss: 2.0474471847216287

Epoch: 118| Step: 0
Training loss: 2.1017467975616455
Validation loss: 2.044234315554301

Epoch: 6| Step: 1
Training loss: 2.058974027633667
Validation loss: 2.0519433418909707

Epoch: 6| Step: 2
Training loss: 1.6901050806045532
Validation loss: 2.04512886206309

Epoch: 6| Step: 3
Training loss: 2.3010940551757812
Validation loss: 2.052534798781077

Epoch: 6| Step: 4
Training loss: 2.556364059448242
Validation loss: 2.0613935788472495

Epoch: 6| Step: 5
Training loss: 1.8682036399841309
Validation loss: 2.046413242816925

Epoch: 6| Step: 6
Training loss: 2.2362399101257324
Validation loss: 2.062495688597361

Epoch: 6| Step: 7
Training loss: 2.0533437728881836
Validation loss: 2.0609290401140847

Epoch: 6| Step: 8
Training loss: 1.9101885557174683
Validation loss: 2.0473164121309915

Epoch: 6| Step: 9
Training loss: 1.8191896677017212
Validation loss: 2.0515876015027366

Epoch: 6| Step: 10
Training loss: 2.1435067653656006
Validation loss: 2.0459505518277488

Epoch: 6| Step: 11
Training loss: 1.555182933807373
Validation loss: 2.064243415991465

Epoch: 6| Step: 12
Training loss: 1.6246650218963623
Validation loss: 2.060576935609182

Epoch: 6| Step: 13
Training loss: 2.739311933517456
Validation loss: 2.071759899457296

Epoch: 119| Step: 0
Training loss: 2.1369664669036865
Validation loss: 2.068963090578715

Epoch: 6| Step: 1
Training loss: 2.0809245109558105
Validation loss: 2.0679591298103333

Epoch: 6| Step: 2
Training loss: 1.847606897354126
Validation loss: 2.058163344860077

Epoch: 6| Step: 3
Training loss: 2.1130995750427246
Validation loss: 2.0559332768122354

Epoch: 6| Step: 4
Training loss: 2.0529258251190186
Validation loss: 2.062021235624949

Epoch: 6| Step: 5
Training loss: 2.1854248046875
Validation loss: 2.0663413604100547

Epoch: 6| Step: 6
Training loss: 2.613569974899292
Validation loss: 2.071892579396566

Epoch: 6| Step: 7
Training loss: 2.1796817779541016
Validation loss: 2.0833020011583963

Epoch: 6| Step: 8
Training loss: 2.2019357681274414
Validation loss: 2.082834998766581

Epoch: 6| Step: 9
Training loss: 1.7402325868606567
Validation loss: 2.0844404697418213

Epoch: 6| Step: 10
Training loss: 2.3030667304992676
Validation loss: 2.0930211544036865

Epoch: 6| Step: 11
Training loss: 1.466684103012085
Validation loss: 2.084321995576223

Epoch: 6| Step: 12
Training loss: 1.6096960306167603
Validation loss: 2.100774268309275

Epoch: 6| Step: 13
Training loss: 2.331364631652832
Validation loss: 2.079261561234792

Epoch: 120| Step: 0
Training loss: 1.900068998336792
Validation loss: 2.071647902329763

Epoch: 6| Step: 1
Training loss: 1.7516725063323975
Validation loss: 2.0564076900482178

Epoch: 6| Step: 2
Training loss: 2.2232656478881836
Validation loss: 2.0547905564308167

Epoch: 6| Step: 3
Training loss: 2.3608152866363525
Validation loss: 2.046732723712921

Epoch: 6| Step: 4
Training loss: 2.30519962310791
Validation loss: 2.0489797393480935

Epoch: 6| Step: 5
Training loss: 2.543994665145874
Validation loss: 2.0302764972050986

Epoch: 6| Step: 6
Training loss: 2.4223837852478027
Validation loss: 2.0377988815307617

Epoch: 6| Step: 7
Training loss: 1.9796242713928223
Validation loss: 2.0358361999193826

Epoch: 6| Step: 8
Training loss: 1.8312461376190186
Validation loss: 2.040670871734619

Epoch: 6| Step: 9
Training loss: 1.8982415199279785
Validation loss: 2.0362536708513894

Epoch: 6| Step: 10
Training loss: 1.665695309638977
Validation loss: 2.0390442411104837

Epoch: 6| Step: 11
Training loss: 2.12052583694458
Validation loss: 2.0399741927782693

Epoch: 6| Step: 12
Training loss: 1.9638969898223877
Validation loss: 2.0406803091367087

Epoch: 6| Step: 13
Training loss: 1.971101999282837
Validation loss: 2.0437626242637634

Epoch: 121| Step: 0
Training loss: 1.72810959815979
Validation loss: 2.046135942141215

Epoch: 6| Step: 1
Training loss: 2.089754104614258
Validation loss: 2.0528206825256348

Epoch: 6| Step: 2
Training loss: 2.1037707328796387
Validation loss: 2.0654393434524536

Epoch: 6| Step: 3
Training loss: 2.7926597595214844
Validation loss: 2.067612131436666

Epoch: 6| Step: 4
Training loss: 2.911736488342285
Validation loss: 2.068832198778788

Epoch: 6| Step: 5
Training loss: 2.048039674758911
Validation loss: 2.073869248231252

Epoch: 6| Step: 6
Training loss: 2.249136209487915
Validation loss: 2.0419026215871177

Epoch: 6| Step: 7
Training loss: 1.8719555139541626
Validation loss: 2.0629425644874573

Epoch: 6| Step: 8
Training loss: 1.4731009006500244
Validation loss: 2.060287833213806

Epoch: 6| Step: 9
Training loss: 1.895766019821167
Validation loss: 2.0669331550598145

Epoch: 6| Step: 10
Training loss: 1.7088608741760254
Validation loss: 2.050869941711426

Epoch: 6| Step: 11
Training loss: 1.7662782669067383
Validation loss: 2.0603959361712136

Epoch: 6| Step: 12
Training loss: 2.020087242126465
Validation loss: 2.0519503951072693

Epoch: 6| Step: 13
Training loss: 2.01656436920166
Validation loss: 2.0450928807258606

Epoch: 122| Step: 0
Training loss: 2.706122398376465
Validation loss: 2.041105329990387

Epoch: 6| Step: 1
Training loss: 1.955976963043213
Validation loss: 2.0427098274230957

Epoch: 6| Step: 2
Training loss: 1.9567797183990479
Validation loss: 2.061200280984243

Epoch: 6| Step: 3
Training loss: 1.940702199935913
Validation loss: 2.0573265751202903

Epoch: 6| Step: 4
Training loss: 1.9176889657974243
Validation loss: 2.0550917983055115

Epoch: 6| Step: 5
Training loss: 2.1793904304504395
Validation loss: 2.0654947757720947

Epoch: 6| Step: 6
Training loss: 2.3744728565216064
Validation loss: 2.059453268845876

Epoch: 6| Step: 7
Training loss: 2.0947282314300537
Validation loss: 2.066770593325297

Epoch: 6| Step: 8
Training loss: 2.102674961090088
Validation loss: 2.059950590133667

Epoch: 6| Step: 9
Training loss: 2.5122578144073486
Validation loss: 2.0652941266695657

Epoch: 6| Step: 10
Training loss: 1.9175305366516113
Validation loss: 2.071676274140676

Epoch: 6| Step: 11
Training loss: 1.9335819482803345
Validation loss: 2.0746936003367105

Epoch: 6| Step: 12
Training loss: 1.366333246231079
Validation loss: 2.078168272972107

Epoch: 6| Step: 13
Training loss: 1.3600680828094482
Validation loss: 2.071621358394623

Epoch: 123| Step: 0
Training loss: 1.882213830947876
Validation loss: 2.090445876121521

Epoch: 6| Step: 1
Training loss: 2.0898656845092773
Validation loss: 2.082404295603434

Epoch: 6| Step: 2
Training loss: 1.8880068063735962
Validation loss: 2.0726155638694763

Epoch: 6| Step: 3
Training loss: 2.183065414428711
Validation loss: 2.0746633410453796

Epoch: 6| Step: 4
Training loss: 1.871098518371582
Validation loss: 2.0772897005081177

Epoch: 6| Step: 5
Training loss: 1.6042907238006592
Validation loss: 2.0829715728759766

Epoch: 6| Step: 6
Training loss: 1.8400826454162598
Validation loss: 2.079972724119822

Epoch: 6| Step: 7
Training loss: 2.540177345275879
Validation loss: 2.0765169858932495

Epoch: 6| Step: 8
Training loss: 2.4782907962799072
Validation loss: 2.077630400657654

Epoch: 6| Step: 9
Training loss: 2.5225257873535156
Validation loss: 2.070334871610006

Epoch: 6| Step: 10
Training loss: 2.0839638710021973
Validation loss: 2.0783740480740867

Epoch: 6| Step: 11
Training loss: 1.1341731548309326
Validation loss: 2.067337910334269

Epoch: 6| Step: 12
Training loss: 2.140099048614502
Validation loss: 2.065330386161804

Epoch: 6| Step: 13
Training loss: 2.2516775131225586
Validation loss: 2.0574796001116433

Epoch: 124| Step: 0
Training loss: 2.368521213531494
Validation loss: 2.0481829245885215

Epoch: 6| Step: 1
Training loss: 2.172790288925171
Validation loss: 2.0468400915463767

Epoch: 6| Step: 2
Training loss: 1.8620381355285645
Validation loss: 2.0503815015157065

Epoch: 6| Step: 3
Training loss: 2.1663331985473633
Validation loss: 2.0513995885849

Epoch: 6| Step: 4
Training loss: 2.4078426361083984
Validation loss: 2.049269179503123

Epoch: 6| Step: 5
Training loss: 1.7718737125396729
Validation loss: 2.0413477420806885

Epoch: 6| Step: 6
Training loss: 2.0213587284088135
Validation loss: 2.0549392104148865

Epoch: 6| Step: 7
Training loss: 1.979600191116333
Validation loss: 2.050799568494161

Epoch: 6| Step: 8
Training loss: 2.0104219913482666
Validation loss: 2.0652572909990945

Epoch: 6| Step: 9
Training loss: 1.628937840461731
Validation loss: 2.066512107849121

Epoch: 6| Step: 10
Training loss: 2.142275810241699
Validation loss: 2.0676324168841043

Epoch: 6| Step: 11
Training loss: 2.1706860065460205
Validation loss: 2.0765469670295715

Epoch: 6| Step: 12
Training loss: 2.106414318084717
Validation loss: 2.0940308372179666

Epoch: 6| Step: 13
Training loss: 2.1706387996673584
Validation loss: 2.081131637096405

Epoch: 125| Step: 0
Training loss: 2.7032113075256348
Validation loss: 2.0750845670700073

Epoch: 6| Step: 1
Training loss: 2.153474807739258
Validation loss: 2.0843669772148132

Epoch: 6| Step: 2
Training loss: 2.3345084190368652
Validation loss: 2.0808618863423667

Epoch: 6| Step: 3
Training loss: 2.041773796081543
Validation loss: 2.0761818091074624

Epoch: 6| Step: 4
Training loss: 1.961731195449829
Validation loss: 2.074334363142649

Epoch: 6| Step: 5
Training loss: 1.6935882568359375
Validation loss: 2.066639224688212

Epoch: 6| Step: 6
Training loss: 1.8224552869796753
Validation loss: 2.051267147064209

Epoch: 6| Step: 7
Training loss: 2.3364639282226562
Validation loss: 2.0518698493639627

Epoch: 6| Step: 8
Training loss: 1.7427793741226196
Validation loss: 2.0518833498160043

Epoch: 6| Step: 9
Training loss: 1.9108235836029053
Validation loss: 2.0439263582229614

Epoch: 6| Step: 10
Training loss: 1.7466727495193481
Validation loss: 2.0544063647588096

Epoch: 6| Step: 11
Training loss: 2.14491868019104
Validation loss: 2.047739624977112

Epoch: 6| Step: 12
Training loss: 2.4901914596557617
Validation loss: 2.065138498942057

Epoch: 6| Step: 13
Training loss: 1.72237229347229
Validation loss: 2.0492700934410095

Epoch: 126| Step: 0
Training loss: 2.48171329498291
Validation loss: 2.050736387570699

Epoch: 6| Step: 1
Training loss: 1.7981388568878174
Validation loss: 2.0637044509251914

Epoch: 6| Step: 2
Training loss: 1.8136422634124756
Validation loss: 2.069029211997986

Epoch: 6| Step: 3
Training loss: 2.020214557647705
Validation loss: 2.0640863378842673

Epoch: 6| Step: 4
Training loss: 1.4985021352767944
Validation loss: 2.070793569087982

Epoch: 6| Step: 5
Training loss: 2.0809624195098877
Validation loss: 2.0671703815460205

Epoch: 6| Step: 6
Training loss: 2.004110813140869
Validation loss: 2.077320178349813

Epoch: 6| Step: 7
Training loss: 2.5046772956848145
Validation loss: 2.0630335807800293

Epoch: 6| Step: 8
Training loss: 1.9716602563858032
Validation loss: 2.076479693253835

Epoch: 6| Step: 9
Training loss: 1.9956951141357422
Validation loss: 2.0646537144978843

Epoch: 6| Step: 10
Training loss: 1.9791967868804932
Validation loss: 2.0673398971557617

Epoch: 6| Step: 11
Training loss: 2.341007709503174
Validation loss: 2.064331889152527

Epoch: 6| Step: 12
Training loss: 2.425600528717041
Validation loss: 2.0661566257476807

Epoch: 6| Step: 13
Training loss: 1.8374106884002686
Validation loss: 2.0568986336390176

Epoch: 127| Step: 0
Training loss: 2.7421300411224365
Validation loss: 2.0540406902631125

Epoch: 6| Step: 1
Training loss: 1.6327390670776367
Validation loss: 2.061023771762848

Epoch: 6| Step: 2
Training loss: 2.5056347846984863
Validation loss: 2.0525125662485757

Epoch: 6| Step: 3
Training loss: 1.4174575805664062
Validation loss: 2.055037498474121

Epoch: 6| Step: 4
Training loss: 1.5566736459732056
Validation loss: 2.05811874071757

Epoch: 6| Step: 5
Training loss: 2.257021427154541
Validation loss: 2.054580052693685

Epoch: 6| Step: 6
Training loss: 1.849883794784546
Validation loss: 2.0668814182281494

Epoch: 6| Step: 7
Training loss: 2.0149974822998047
Validation loss: 2.0714051127433777

Epoch: 6| Step: 8
Training loss: 2.752155303955078
Validation loss: 2.0751826763153076

Epoch: 6| Step: 9
Training loss: 2.0385689735412598
Validation loss: 2.0814091165860495

Epoch: 6| Step: 10
Training loss: 1.7433940172195435
Validation loss: 2.091224253177643

Epoch: 6| Step: 11
Training loss: 2.07236909866333
Validation loss: 2.088054120540619

Epoch: 6| Step: 12
Training loss: 1.9153136014938354
Validation loss: 2.0833386381467185

Epoch: 6| Step: 13
Training loss: 1.846285104751587
Validation loss: 2.086497942606608

Epoch: 128| Step: 0
Training loss: 1.7987308502197266
Validation loss: 2.084108809630076

Epoch: 6| Step: 1
Training loss: 1.9175502061843872
Validation loss: 2.0750863949457803

Epoch: 6| Step: 2
Training loss: 1.8011881113052368
Validation loss: 2.0822885831197104

Epoch: 6| Step: 3
Training loss: 2.1178274154663086
Validation loss: 2.0807788570721946

Epoch: 6| Step: 4
Training loss: 2.2623419761657715
Validation loss: 2.068557540575663

Epoch: 6| Step: 5
Training loss: 1.8956279754638672
Validation loss: 2.0662197868029275

Epoch: 6| Step: 6
Training loss: 2.558699369430542
Validation loss: 2.057534178098043

Epoch: 6| Step: 7
Training loss: 2.251370906829834
Validation loss: 2.0512306292851767

Epoch: 6| Step: 8
Training loss: 2.0386288166046143
Validation loss: 2.056400418281555

Epoch: 6| Step: 9
Training loss: 2.866511821746826
Validation loss: 2.050584355990092

Epoch: 6| Step: 10
Training loss: 1.5507756471633911
Validation loss: 2.053896645704905

Epoch: 6| Step: 11
Training loss: 2.005429983139038
Validation loss: 2.071812887986501

Epoch: 6| Step: 12
Training loss: 1.6874406337738037
Validation loss: 2.0717533230781555

Epoch: 6| Step: 13
Training loss: 1.8139071464538574
Validation loss: 2.070068657398224

Epoch: 129| Step: 0
Training loss: 1.6093534231185913
Validation loss: 2.087828000386556

Epoch: 6| Step: 1
Training loss: 2.18341326713562
Validation loss: 2.078281839688619

Epoch: 6| Step: 2
Training loss: 1.9846742153167725
Validation loss: 2.0923317273457847

Epoch: 6| Step: 3
Training loss: 2.38413667678833
Validation loss: 2.082112451394399

Epoch: 6| Step: 4
Training loss: 2.4120750427246094
Validation loss: 2.10073322057724

Epoch: 6| Step: 5
Training loss: 1.3457332849502563
Validation loss: 2.092614690462748

Epoch: 6| Step: 6
Training loss: 2.32922625541687
Validation loss: 2.088698466618856

Epoch: 6| Step: 7
Training loss: 1.8284306526184082
Validation loss: 2.0779229601224265

Epoch: 6| Step: 8
Training loss: 2.282132148742676
Validation loss: 2.0726422468821206

Epoch: 6| Step: 9
Training loss: 1.734330654144287
Validation loss: 2.0670157273610434

Epoch: 6| Step: 10
Training loss: 1.9993996620178223
Validation loss: 2.070102055867513

Epoch: 6| Step: 11
Training loss: 1.672156572341919
Validation loss: 2.0584974686304727

Epoch: 6| Step: 12
Training loss: 2.4293160438537598
Validation loss: 2.0506859620412192

Epoch: 6| Step: 13
Training loss: 2.144774913787842
Validation loss: 2.048997382322947

Epoch: 130| Step: 0
Training loss: 2.1267499923706055
Validation loss: 2.0483919382095337

Epoch: 6| Step: 1
Training loss: 1.4718778133392334
Validation loss: 2.0547098318735757

Epoch: 6| Step: 2
Training loss: 1.871415615081787
Validation loss: 2.050303041934967

Epoch: 6| Step: 3
Training loss: 1.9803056716918945
Validation loss: 2.0439958572387695

Epoch: 6| Step: 4
Training loss: 2.0863687992095947
Validation loss: 2.0490633845329285

Epoch: 6| Step: 5
Training loss: 1.7657054662704468
Validation loss: 2.062562088171641

Epoch: 6| Step: 6
Training loss: 2.341792106628418
Validation loss: 2.060098091761271

Epoch: 6| Step: 7
Training loss: 1.8375827074050903
Validation loss: 2.0489781697591147

Epoch: 6| Step: 8
Training loss: 2.7639143466949463
Validation loss: 2.077136774857839

Epoch: 6| Step: 9
Training loss: 1.4503940343856812
Validation loss: 2.0771181186040244

Epoch: 6| Step: 10
Training loss: 1.9662904739379883
Validation loss: 2.1093305349349976

Epoch: 6| Step: 11
Training loss: 2.953989267349243
Validation loss: 2.1138070623079934

Epoch: 6| Step: 12
Training loss: 2.789628267288208
Validation loss: 2.1104846199353537

Epoch: 6| Step: 13
Training loss: 1.8408491611480713
Validation loss: 2.1022685766220093

Epoch: 131| Step: 0
Training loss: 2.0968098640441895
Validation loss: 2.0774892767270408

Epoch: 6| Step: 1
Training loss: 2.1129555702209473
Validation loss: 2.0787093242009482

Epoch: 6| Step: 2
Training loss: 2.222588062286377
Validation loss: 2.054311474164327

Epoch: 6| Step: 3
Training loss: 2.4873239994049072
Validation loss: 2.0664780735969543

Epoch: 6| Step: 4
Training loss: 1.7311515808105469
Validation loss: 2.0506074825922647

Epoch: 6| Step: 5
Training loss: 2.4141461849212646
Validation loss: 2.0563043554623923

Epoch: 6| Step: 6
Training loss: 2.331172227859497
Validation loss: 2.053178687890371

Epoch: 6| Step: 7
Training loss: 2.697498083114624
Validation loss: 2.0404098629951477

Epoch: 6| Step: 8
Training loss: 1.8459364175796509
Validation loss: 2.0414421359697976

Epoch: 6| Step: 9
Training loss: 1.8607292175292969
Validation loss: 2.058999458948771

Epoch: 6| Step: 10
Training loss: 1.5748814344406128
Validation loss: 2.057429393132528

Epoch: 6| Step: 11
Training loss: 1.4609789848327637
Validation loss: 2.060878853003184

Epoch: 6| Step: 12
Training loss: 2.2370612621307373
Validation loss: 2.065690795580546

Epoch: 6| Step: 13
Training loss: 1.6739871501922607
Validation loss: 2.0669612685839334

Epoch: 132| Step: 0
Training loss: 2.3282291889190674
Validation loss: 2.0646429459253945

Epoch: 6| Step: 1
Training loss: 1.8057047128677368
Validation loss: 2.069778581460317

Epoch: 6| Step: 2
Training loss: 2.1083006858825684
Validation loss: 2.0725120107332864

Epoch: 6| Step: 3
Training loss: 1.418168306350708
Validation loss: 2.052544891834259

Epoch: 6| Step: 4
Training loss: 2.307762622833252
Validation loss: 2.064051946004232

Epoch: 6| Step: 5
Training loss: 2.1021032333374023
Validation loss: 2.0574325919151306

Epoch: 6| Step: 6
Training loss: 1.7923442125320435
Validation loss: 2.04922487338384

Epoch: 6| Step: 7
Training loss: 2.174659013748169
Validation loss: 2.055403709411621

Epoch: 6| Step: 8
Training loss: 1.7020885944366455
Validation loss: 2.0515223145484924

Epoch: 6| Step: 9
Training loss: 2.004486560821533
Validation loss: 2.06122088432312

Epoch: 6| Step: 10
Training loss: 1.69382905960083
Validation loss: 2.064824561278025

Epoch: 6| Step: 11
Training loss: 1.8350145816802979
Validation loss: 2.0701982577641806

Epoch: 6| Step: 12
Training loss: 2.413424015045166
Validation loss: 2.0794772704442344

Epoch: 6| Step: 13
Training loss: 2.6945295333862305
Validation loss: 2.0949018001556396

Epoch: 133| Step: 0
Training loss: 2.2323150634765625
Validation loss: 2.0905298789342246

Epoch: 6| Step: 1
Training loss: 1.8605672121047974
Validation loss: 2.0880363384882608

Epoch: 6| Step: 2
Training loss: 2.283125877380371
Validation loss: 2.1021605928738913

Epoch: 6| Step: 3
Training loss: 1.120991587638855
Validation loss: 2.101838211218516

Epoch: 6| Step: 4
Training loss: 2.345231056213379
Validation loss: 2.0955229997634888

Epoch: 6| Step: 5
Training loss: 1.8995311260223389
Validation loss: 2.0777848760286965

Epoch: 6| Step: 6
Training loss: 1.813603401184082
Validation loss: 2.074393332004547

Epoch: 6| Step: 7
Training loss: 1.9525482654571533
Validation loss: 2.059634506702423

Epoch: 6| Step: 8
Training loss: 2.384740114212036
Validation loss: 2.05935808022817

Epoch: 6| Step: 9
Training loss: 2.1374006271362305
Validation loss: 2.048371414343516

Epoch: 6| Step: 10
Training loss: 2.0681300163269043
Validation loss: 2.051942765712738

Epoch: 6| Step: 11
Training loss: 1.9787453413009644
Validation loss: 2.060101946194967

Epoch: 6| Step: 12
Training loss: 1.8739657402038574
Validation loss: 2.052886108557383

Epoch: 6| Step: 13
Training loss: 2.37465500831604
Validation loss: 2.0651897192001343

Epoch: 134| Step: 0
Training loss: 2.119286060333252
Validation loss: 2.0599766174952188

Epoch: 6| Step: 1
Training loss: 1.7577530145645142
Validation loss: 2.059319575627645

Epoch: 6| Step: 2
Training loss: 1.5349886417388916
Validation loss: 2.0762515862782798

Epoch: 6| Step: 3
Training loss: 1.956274390220642
Validation loss: 2.0832029581069946

Epoch: 6| Step: 4
Training loss: 2.866218090057373
Validation loss: 2.101235886414846

Epoch: 6| Step: 5
Training loss: 1.7300670146942139
Validation loss: 2.102126638094584

Epoch: 6| Step: 6
Training loss: 2.997157573699951
Validation loss: 2.088331321875254

Epoch: 6| Step: 7
Training loss: 1.9208364486694336
Validation loss: 2.0962737600008645

Epoch: 6| Step: 8
Training loss: 1.4544832706451416
Validation loss: 2.097511072953542

Epoch: 6| Step: 9
Training loss: 2.071462631225586
Validation loss: 2.0888447562853494

Epoch: 6| Step: 10
Training loss: 2.0749149322509766
Validation loss: 2.109145442644755

Epoch: 6| Step: 11
Training loss: 2.097731590270996
Validation loss: 2.0809813340504966

Epoch: 6| Step: 12
Training loss: 2.3091766834259033
Validation loss: 2.1004817883173623

Epoch: 6| Step: 13
Training loss: 1.6094927787780762
Validation loss: 2.123315970102946

Epoch: 135| Step: 0
Training loss: 1.471115231513977
Validation loss: 2.113813797632853

Epoch: 6| Step: 1
Training loss: 1.8263806104660034
Validation loss: 2.097631017367045

Epoch: 6| Step: 2
Training loss: 2.4030208587646484
Validation loss: 2.0844823122024536

Epoch: 6| Step: 3
Training loss: 1.8898699283599854
Validation loss: 2.0756959319114685

Epoch: 6| Step: 4
Training loss: 2.182204008102417
Validation loss: 2.0686091780662537

Epoch: 6| Step: 5
Training loss: 2.4962642192840576
Validation loss: 2.0668890674908957

Epoch: 6| Step: 6
Training loss: 1.9634586572647095
Validation loss: 2.0678335428237915

Epoch: 6| Step: 7
Training loss: 2.0162007808685303
Validation loss: 2.074551304181417

Epoch: 6| Step: 8
Training loss: 1.7608985900878906
Validation loss: 2.081113954385122

Epoch: 6| Step: 9
Training loss: 2.4103941917419434
Validation loss: 2.0844088196754456

Epoch: 6| Step: 10
Training loss: 1.9623435735702515
Validation loss: 2.106727878252665

Epoch: 6| Step: 11
Training loss: 1.8277952671051025
Validation loss: 2.1077033082644143

Epoch: 6| Step: 12
Training loss: 1.9754526615142822
Validation loss: 2.12973161538442

Epoch: 6| Step: 13
Training loss: 2.0073981285095215
Validation loss: 2.1543323596318564

Epoch: 136| Step: 0
Training loss: 1.9336506128311157
Validation loss: 2.142560064792633

Epoch: 6| Step: 1
Training loss: 1.5737409591674805
Validation loss: 2.134259581565857

Epoch: 6| Step: 2
Training loss: 3.151836395263672
Validation loss: 2.1545033852259317

Epoch: 6| Step: 3
Training loss: 1.9927723407745361
Validation loss: 2.141518473625183

Epoch: 6| Step: 4
Training loss: 2.3763480186462402
Validation loss: 2.1424431602160134

Epoch: 6| Step: 5
Training loss: 2.874992847442627
Validation loss: 2.138430198033651

Epoch: 6| Step: 6
Training loss: 1.9706870317459106
Validation loss: 2.1019159952799478

Epoch: 6| Step: 7
Training loss: 1.3108890056610107
Validation loss: 2.1034401456514993

Epoch: 6| Step: 8
Training loss: 2.2490649223327637
Validation loss: 2.092369814713796

Epoch: 6| Step: 9
Training loss: 1.4692020416259766
Validation loss: 2.0772290031115213

Epoch: 6| Step: 10
Training loss: 1.4174907207489014
Validation loss: 2.062382082144419

Epoch: 6| Step: 11
Training loss: 2.011573314666748
Validation loss: 2.065439840157827

Epoch: 6| Step: 12
Training loss: 2.5136518478393555
Validation loss: 2.055504302183787

Epoch: 6| Step: 13
Training loss: 2.029818534851074
Validation loss: 2.058293044567108

Epoch: 137| Step: 0
Training loss: 2.604712963104248
Validation loss: 2.0733033617337546

Epoch: 6| Step: 1
Training loss: 2.08613920211792
Validation loss: 2.0691686073939004

Epoch: 6| Step: 2
Training loss: 1.6204125881195068
Validation loss: 2.0680274764696756

Epoch: 6| Step: 3
Training loss: 1.6509995460510254
Validation loss: 2.0860185027122498

Epoch: 6| Step: 4
Training loss: 1.4550355672836304
Validation loss: 2.088026841481527

Epoch: 6| Step: 5
Training loss: 2.557593584060669
Validation loss: 2.1044040520985923

Epoch: 6| Step: 6
Training loss: 1.361999750137329
Validation loss: 2.1063015460968018

Epoch: 6| Step: 7
Training loss: 2.043850898742676
Validation loss: 2.106679618358612

Epoch: 6| Step: 8
Training loss: 2.7437210083007812
Validation loss: 2.112032393614451

Epoch: 6| Step: 9
Training loss: 1.6707935333251953
Validation loss: 2.1136208971341452

Epoch: 6| Step: 10
Training loss: 1.5126116275787354
Validation loss: 2.100806395212809

Epoch: 6| Step: 11
Training loss: 2.791688919067383
Validation loss: 2.1056923866271973

Epoch: 6| Step: 12
Training loss: 1.9462320804595947
Validation loss: 2.103383560975393

Epoch: 6| Step: 13
Training loss: 1.712472915649414
Validation loss: 2.121374328931173

Epoch: 138| Step: 0
Training loss: 1.7914984226226807
Validation loss: 2.1128200689951577

Epoch: 6| Step: 1
Training loss: 1.8481947183609009
Validation loss: 2.1110308369000754

Epoch: 6| Step: 2
Training loss: 2.0699973106384277
Validation loss: 2.1117195884386697

Epoch: 6| Step: 3
Training loss: 1.8907579183578491
Validation loss: 2.1169247229894004

Epoch: 6| Step: 4
Training loss: 1.4527099132537842
Validation loss: 2.0943690141042075

Epoch: 6| Step: 5
Training loss: 1.7979187965393066
Validation loss: 2.0871174732844033

Epoch: 6| Step: 6
Training loss: 2.6309666633605957
Validation loss: 2.0744747320810952

Epoch: 6| Step: 7
Training loss: 1.8180738687515259
Validation loss: 2.0714449485143027

Epoch: 6| Step: 8
Training loss: 2.123084545135498
Validation loss: 2.062983493010203

Epoch: 6| Step: 9
Training loss: 2.825744152069092
Validation loss: 2.0713708798090615

Epoch: 6| Step: 10
Training loss: 2.0337255001068115
Validation loss: 2.073684553305308

Epoch: 6| Step: 11
Training loss: 1.9171466827392578
Validation loss: 2.0653186639149985

Epoch: 6| Step: 12
Training loss: 2.084023952484131
Validation loss: 2.068994700908661

Epoch: 6| Step: 13
Training loss: 2.0926871299743652
Validation loss: 2.0773669282595315

Epoch: 139| Step: 0
Training loss: 1.7416560649871826
Validation loss: 2.0927317142486572

Epoch: 6| Step: 1
Training loss: 1.8110270500183105
Validation loss: 2.0965048472086587

Epoch: 6| Step: 2
Training loss: 2.2232487201690674
Validation loss: 2.1050384839375815

Epoch: 6| Step: 3
Training loss: 2.697826385498047
Validation loss: 2.097042659918467

Epoch: 6| Step: 4
Training loss: 1.6546422243118286
Validation loss: 2.097907304763794

Epoch: 6| Step: 5
Training loss: 2.2283623218536377
Validation loss: 2.0940007170041404

Epoch: 6| Step: 6
Training loss: 1.6765949726104736
Validation loss: 2.0956955552101135

Epoch: 6| Step: 7
Training loss: 2.3446760177612305
Validation loss: 2.092718223730723

Epoch: 6| Step: 8
Training loss: 2.3629672527313232
Validation loss: 2.078582445780436

Epoch: 6| Step: 9
Training loss: 2.4522483348846436
Validation loss: 2.0789275964101157

Epoch: 6| Step: 10
Training loss: 1.8554463386535645
Validation loss: 2.077102263768514

Epoch: 6| Step: 11
Training loss: 2.174132823944092
Validation loss: 2.0804644425710044

Epoch: 6| Step: 12
Training loss: 1.5553892850875854
Validation loss: 2.077528158823649

Epoch: 6| Step: 13
Training loss: 1.5743179321289062
Validation loss: 2.073763926823934

Epoch: 140| Step: 0
Training loss: 2.6930932998657227
Validation loss: 2.0843199690183005

Epoch: 6| Step: 1
Training loss: 1.8458420038223267
Validation loss: 2.091577092806498

Epoch: 6| Step: 2
Training loss: 2.2792065143585205
Validation loss: 2.0907666285832724

Epoch: 6| Step: 3
Training loss: 2.2354326248168945
Validation loss: 2.0918388962745667

Epoch: 6| Step: 4
Training loss: 1.7480404376983643
Validation loss: 2.0923618276913962

Epoch: 6| Step: 5
Training loss: 1.5517269372940063
Validation loss: 2.086048106352488

Epoch: 6| Step: 6
Training loss: 1.7064223289489746
Validation loss: 2.088732282320658

Epoch: 6| Step: 7
Training loss: 1.8160269260406494
Validation loss: 2.10956871509552

Epoch: 6| Step: 8
Training loss: 1.8638532161712646
Validation loss: 2.114264706770579

Epoch: 6| Step: 9
Training loss: 2.1165072917938232
Validation loss: 2.111853321393331

Epoch: 6| Step: 10
Training loss: 1.8598419427871704
Validation loss: 2.1125269730885825

Epoch: 6| Step: 11
Training loss: 2.7471375465393066
Validation loss: 2.118106484413147

Epoch: 6| Step: 12
Training loss: 1.8490973711013794
Validation loss: 2.1136647661527

Epoch: 6| Step: 13
Training loss: 1.9648072719573975
Validation loss: 2.109215021133423

Epoch: 141| Step: 0
Training loss: 1.883228063583374
Validation loss: 2.095083256562551

Epoch: 6| Step: 1
Training loss: 2.085754156112671
Validation loss: 2.092882255713145

Epoch: 6| Step: 2
Training loss: 1.9616457223892212
Validation loss: 2.0802718003590903

Epoch: 6| Step: 3
Training loss: 1.5985665321350098
Validation loss: 2.0784907937049866

Epoch: 6| Step: 4
Training loss: 1.5255597829818726
Validation loss: 2.085974951585134

Epoch: 6| Step: 5
Training loss: 2.2011892795562744
Validation loss: 2.0785388946533203

Epoch: 6| Step: 6
Training loss: 2.2232093811035156
Validation loss: 2.088998476664225

Epoch: 6| Step: 7
Training loss: 3.0101797580718994
Validation loss: 2.0970199704170227

Epoch: 6| Step: 8
Training loss: 1.6352874040603638
Validation loss: 2.1035831371943154

Epoch: 6| Step: 9
Training loss: 2.468883991241455
Validation loss: 2.1102484464645386

Epoch: 6| Step: 10
Training loss: 2.4683942794799805
Validation loss: 2.1073919336001077

Epoch: 6| Step: 11
Training loss: 1.5173907279968262
Validation loss: 2.120857218901316

Epoch: 6| Step: 12
Training loss: 1.569556713104248
Validation loss: 2.1121329267819724

Epoch: 6| Step: 13
Training loss: 1.8774235248565674
Validation loss: 2.127520044644674

Epoch: 142| Step: 0
Training loss: 1.7224243879318237
Validation loss: 2.1261976957321167

Epoch: 6| Step: 1
Training loss: 2.3085575103759766
Validation loss: 2.1135154167811074

Epoch: 6| Step: 2
Training loss: 1.4517631530761719
Validation loss: 2.1102867126464844

Epoch: 6| Step: 3
Training loss: 1.9178674221038818
Validation loss: 2.121495763460795

Epoch: 6| Step: 4
Training loss: 1.947803020477295
Validation loss: 2.115960200627645

Epoch: 6| Step: 5
Training loss: 1.7505064010620117
Validation loss: 2.1181201934814453

Epoch: 6| Step: 6
Training loss: 2.42653489112854
Validation loss: 2.1166552901268005

Epoch: 6| Step: 7
Training loss: 2.0708303451538086
Validation loss: 2.115004301071167

Epoch: 6| Step: 8
Training loss: 1.944763422012329
Validation loss: 2.1109308004379272

Epoch: 6| Step: 9
Training loss: 2.1357359886169434
Validation loss: 2.092133402824402

Epoch: 6| Step: 10
Training loss: 1.6909677982330322
Validation loss: 2.0808884302775064

Epoch: 6| Step: 11
Training loss: 1.6970731019973755
Validation loss: 2.093522071838379

Epoch: 6| Step: 12
Training loss: 2.147495985031128
Validation loss: 2.0847947001457214

Epoch: 6| Step: 13
Training loss: 2.5240511894226074
Validation loss: 2.0849066178003945

Epoch: 143| Step: 0
Training loss: 2.0552330017089844
Validation loss: 2.0943755507469177

Epoch: 6| Step: 1
Training loss: 2.000645637512207
Validation loss: 2.1187825202941895

Epoch: 6| Step: 2
Training loss: 2.3695428371429443
Validation loss: 2.1237308581670127

Epoch: 6| Step: 3
Training loss: 2.0037193298339844
Validation loss: 2.1183435519536338

Epoch: 6| Step: 4
Training loss: 1.7757623195648193
Validation loss: 2.122955401738485

Epoch: 6| Step: 5
Training loss: 2.25827693939209
Validation loss: 2.1129414439201355

Epoch: 6| Step: 6
Training loss: 1.7354925870895386
Validation loss: 2.1051854689915976

Epoch: 6| Step: 7
Training loss: 1.9485931396484375
Validation loss: 2.105467975139618

Epoch: 6| Step: 8
Training loss: 2.4254279136657715
Validation loss: 2.1041797399520874

Epoch: 6| Step: 9
Training loss: 1.4362198114395142
Validation loss: 2.1117050846417746

Epoch: 6| Step: 10
Training loss: 1.8866082429885864
Validation loss: 2.093057870864868

Epoch: 6| Step: 11
Training loss: 1.5341622829437256
Validation loss: 2.0818763176600137

Epoch: 6| Step: 12
Training loss: 2.2805070877075195
Validation loss: 2.0863102277119956

Epoch: 6| Step: 13
Training loss: 2.1371312141418457
Validation loss: 2.0908837715784707

Epoch: 144| Step: 0
Training loss: 2.213315725326538
Validation loss: 2.0729130506515503

Epoch: 6| Step: 1
Training loss: 1.445925235748291
Validation loss: 2.0808252890904746

Epoch: 6| Step: 2
Training loss: 2.226161003112793
Validation loss: 2.0987800558408103

Epoch: 6| Step: 3
Training loss: 2.191206455230713
Validation loss: 2.0844061374664307

Epoch: 6| Step: 4
Training loss: 1.7980185747146606
Validation loss: 2.0869542360305786

Epoch: 6| Step: 5
Training loss: 1.5821647644042969
Validation loss: 2.0886833667755127

Epoch: 6| Step: 6
Training loss: 1.8936010599136353
Validation loss: 2.10388845205307

Epoch: 6| Step: 7
Training loss: 2.292088508605957
Validation loss: 2.1006667613983154

Epoch: 6| Step: 8
Training loss: 1.906426191329956
Validation loss: 2.096002459526062

Epoch: 6| Step: 9
Training loss: 2.03865647315979
Validation loss: 2.1404232382774353

Epoch: 6| Step: 10
Training loss: 2.2297396659851074
Validation loss: 2.1373201608657837

Epoch: 6| Step: 11
Training loss: 1.8470568656921387
Validation loss: 2.1382749478022256

Epoch: 6| Step: 12
Training loss: 2.2338554859161377
Validation loss: 2.129806399345398

Epoch: 6| Step: 13
Training loss: 1.9635744094848633
Validation loss: 2.1238189140955606

Epoch: 145| Step: 0
Training loss: 2.244593858718872
Validation loss: 2.125438133875529

Epoch: 6| Step: 1
Training loss: 2.210732936859131
Validation loss: 2.1257413228352866

Epoch: 6| Step: 2
Training loss: 1.6985318660736084
Validation loss: 2.110189994176229

Epoch: 6| Step: 3
Training loss: 3.0026512145996094
Validation loss: 2.1242621739705405

Epoch: 6| Step: 4
Training loss: 1.7164428234100342
Validation loss: 2.1073285142580667

Epoch: 6| Step: 5
Training loss: 2.149353504180908
Validation loss: 2.118100941181183

Epoch: 6| Step: 6
Training loss: 1.5299575328826904
Validation loss: 2.1106563409169516

Epoch: 6| Step: 7
Training loss: 1.553020715713501
Validation loss: 2.1125205159187317

Epoch: 6| Step: 8
Training loss: 2.1520652770996094
Validation loss: 2.101707180341085

Epoch: 6| Step: 9
Training loss: 2.3760828971862793
Validation loss: 2.087481379508972

Epoch: 6| Step: 10
Training loss: 1.544508695602417
Validation loss: 2.094769835472107

Epoch: 6| Step: 11
Training loss: 1.578466534614563
Validation loss: 2.0851347049077353

Epoch: 6| Step: 12
Training loss: 2.206726551055908
Validation loss: 2.0909799536069236

Epoch: 6| Step: 13
Training loss: 1.6519837379455566
Validation loss: 2.0980756680170694

Epoch: 146| Step: 0
Training loss: 2.0577478408813477
Validation loss: 2.118752578894297

Epoch: 6| Step: 1
Training loss: 1.8076180219650269
Validation loss: 2.0993736584981284

Epoch: 6| Step: 2
Training loss: 1.463530421257019
Validation loss: 2.1127832730611167

Epoch: 6| Step: 3
Training loss: 1.9224400520324707
Validation loss: 2.1155686179796853

Epoch: 6| Step: 4
Training loss: 1.9671369791030884
Validation loss: 2.1101073225339255

Epoch: 6| Step: 5
Training loss: 2.2242650985717773
Validation loss: 2.1119067668914795

Epoch: 6| Step: 6
Training loss: 2.388514518737793
Validation loss: 2.1108699440956116

Epoch: 6| Step: 7
Training loss: 1.6897921562194824
Validation loss: 2.1052918632825217

Epoch: 6| Step: 8
Training loss: 2.0025148391723633
Validation loss: 2.1160802046457925

Epoch: 6| Step: 9
Training loss: 1.8989163637161255
Validation loss: 2.1287650068600974

Epoch: 6| Step: 10
Training loss: 1.8365200757980347
Validation loss: 2.1376070380210876

Epoch: 6| Step: 11
Training loss: 2.4216814041137695
Validation loss: 2.125143607457479

Epoch: 6| Step: 12
Training loss: 2.5390822887420654
Validation loss: 2.149005194505056

Epoch: 6| Step: 13
Training loss: 1.7176344394683838
Validation loss: 2.139526903629303

Epoch: 147| Step: 0
Training loss: 1.7545753717422485
Validation loss: 2.1311785181363425

Epoch: 6| Step: 1
Training loss: 2.0482218265533447
Validation loss: 2.1210262179374695

Epoch: 6| Step: 2
Training loss: 2.4377574920654297
Validation loss: 2.131039241949717

Epoch: 6| Step: 3
Training loss: 1.518315076828003
Validation loss: 2.140234092871348

Epoch: 6| Step: 4
Training loss: 2.155243396759033
Validation loss: 2.1449070970217385

Epoch: 6| Step: 5
Training loss: 1.2757768630981445
Validation loss: 2.127080758412679

Epoch: 6| Step: 6
Training loss: 2.0926291942596436
Validation loss: 2.1061219771703086

Epoch: 6| Step: 7
Training loss: 1.975827932357788
Validation loss: 2.1107603907585144

Epoch: 6| Step: 8
Training loss: 2.425387382507324
Validation loss: 2.097225566705068

Epoch: 6| Step: 9
Training loss: 2.0500502586364746
Validation loss: 2.101056476434072

Epoch: 6| Step: 10
Training loss: 1.8782862424850464
Validation loss: 2.0921128392219543

Epoch: 6| Step: 11
Training loss: 2.047640800476074
Validation loss: 2.089176654815674

Epoch: 6| Step: 12
Training loss: 1.9145655632019043
Validation loss: 2.0886336962381997

Epoch: 6| Step: 13
Training loss: 1.939146876335144
Validation loss: 2.085587124029795

Epoch: 148| Step: 0
Training loss: 2.037435531616211
Validation loss: 2.092401603857676

Epoch: 6| Step: 1
Training loss: 2.0566794872283936
Validation loss: 2.094613532225291

Epoch: 6| Step: 2
Training loss: 1.9780876636505127
Validation loss: 2.0882829825083413

Epoch: 6| Step: 3
Training loss: 2.125959873199463
Validation loss: 2.089587648709615

Epoch: 6| Step: 4
Training loss: 2.452510118484497
Validation loss: 2.095266858736674

Epoch: 6| Step: 5
Training loss: 1.3797904253005981
Validation loss: 2.085643390814463

Epoch: 6| Step: 6
Training loss: 1.5459386110305786
Validation loss: 2.0805924932161965

Epoch: 6| Step: 7
Training loss: 2.18040132522583
Validation loss: 2.1046477556228638

Epoch: 6| Step: 8
Training loss: 2.232794761657715
Validation loss: 2.096875707308451

Epoch: 6| Step: 9
Training loss: 2.0638461112976074
Validation loss: 2.0921069979667664

Epoch: 6| Step: 10
Training loss: 2.499166488647461
Validation loss: 2.113207459449768

Epoch: 6| Step: 11
Training loss: 1.4610776901245117
Validation loss: 2.102240482966105

Epoch: 6| Step: 12
Training loss: 1.3001930713653564
Validation loss: 2.097080131371816

Epoch: 6| Step: 13
Training loss: 2.341736316680908
Validation loss: 2.07003523906072

Epoch: 149| Step: 0
Training loss: 2.0463380813598633
Validation loss: 2.093429684638977

Epoch: 6| Step: 1
Training loss: 1.343571662902832
Validation loss: 2.0806310574213662

Epoch: 6| Step: 2
Training loss: 1.7246569395065308
Validation loss: 2.083794275919596

Epoch: 6| Step: 3
Training loss: 1.9332860708236694
Validation loss: 2.077415426572164

Epoch: 6| Step: 4
Training loss: 1.4332709312438965
Validation loss: 2.0790568192799888

Epoch: 6| Step: 5
Training loss: 2.4328713417053223
Validation loss: 2.076617658138275

Epoch: 6| Step: 6
Training loss: 2.133741617202759
Validation loss: 2.073641081651052

Epoch: 6| Step: 7
Training loss: 2.083806037902832
Validation loss: 2.069475253423055

Epoch: 6| Step: 8
Training loss: 1.8298240900039673
Validation loss: 2.0759031772613525

Epoch: 6| Step: 9
Training loss: 1.9242610931396484
Validation loss: 2.091922918955485

Epoch: 6| Step: 10
Training loss: 2.088656425476074
Validation loss: 2.1019487380981445

Epoch: 6| Step: 11
Training loss: 2.3802127838134766
Validation loss: 2.1158056060473123

Epoch: 6| Step: 12
Training loss: 2.615736246109009
Validation loss: 2.123638927936554

Epoch: 6| Step: 13
Training loss: 1.8900642395019531
Validation loss: 2.1212951143582663

Epoch: 150| Step: 0
Training loss: 1.2618718147277832
Validation loss: 2.113904376824697

Epoch: 6| Step: 1
Training loss: 2.019501209259033
Validation loss: 2.1211186051368713

Epoch: 6| Step: 2
Training loss: 2.239703893661499
Validation loss: 2.1040972471237183

Epoch: 6| Step: 3
Training loss: 2.0862948894500732
Validation loss: 2.0841430028279624

Epoch: 6| Step: 4
Training loss: 2.010429859161377
Validation loss: 2.0916837056477866

Epoch: 6| Step: 5
Training loss: 2.2167909145355225
Validation loss: 2.0924587647120156

Epoch: 6| Step: 6
Training loss: 1.7393379211425781
Validation loss: 2.0933440725008645

Epoch: 6| Step: 7
Training loss: 2.3815250396728516
Validation loss: 2.100728988647461

Epoch: 6| Step: 8
Training loss: 2.6593103408813477
Validation loss: 2.096604863802592

Epoch: 6| Step: 9
Training loss: 1.8574718236923218
Validation loss: 2.0873479644457498

Epoch: 6| Step: 10
Training loss: 1.3713674545288086
Validation loss: 2.0791213313738504

Epoch: 6| Step: 11
Training loss: 2.167264223098755
Validation loss: 2.0883277455965676

Epoch: 6| Step: 12
Training loss: 1.997440218925476
Validation loss: 2.0788333217302957

Epoch: 6| Step: 13
Training loss: 1.632554054260254
Validation loss: 2.079297741254171

Epoch: 151| Step: 0
Training loss: 2.3400216102600098
Validation loss: 2.0729541778564453

Epoch: 6| Step: 1
Training loss: 2.149932384490967
Validation loss: 2.0807674328486123

Epoch: 6| Step: 2
Training loss: 2.2429282665252686
Validation loss: 2.0699793895085654

Epoch: 6| Step: 3
Training loss: 1.3406805992126465
Validation loss: 2.0849891304969788

Epoch: 6| Step: 4
Training loss: 2.049771308898926
Validation loss: 2.08002362648646

Epoch: 6| Step: 5
Training loss: 2.5745906829833984
Validation loss: 2.0796545147895813

Epoch: 6| Step: 6
Training loss: 1.4402098655700684
Validation loss: 2.076349059740702

Epoch: 6| Step: 7
Training loss: 1.9065897464752197
Validation loss: 2.079534928003947

Epoch: 6| Step: 8
Training loss: 2.1535143852233887
Validation loss: 2.0833547711372375

Epoch: 6| Step: 9
Training loss: 2.034271717071533
Validation loss: 2.089194595813751

Epoch: 6| Step: 10
Training loss: 1.9250744581222534
Validation loss: 2.11183754603068

Epoch: 6| Step: 11
Training loss: 1.8755115270614624
Validation loss: 2.1042313973108926

Epoch: 6| Step: 12
Training loss: 1.889888048171997
Validation loss: 2.112593412399292

Epoch: 6| Step: 13
Training loss: 2.1700849533081055
Validation loss: 2.1253504355748496

Epoch: 152| Step: 0
Training loss: 1.7781267166137695
Validation loss: 2.1345949172973633

Epoch: 6| Step: 1
Training loss: 1.826261281967163
Validation loss: 2.1415358185768127

Epoch: 6| Step: 2
Training loss: 1.986325979232788
Validation loss: 2.1435067653656006

Epoch: 6| Step: 3
Training loss: 2.1292877197265625
Validation loss: 2.13879003127416

Epoch: 6| Step: 4
Training loss: 1.9608242511749268
Validation loss: 2.1298654476801553

Epoch: 6| Step: 5
Training loss: 2.6574299335479736
Validation loss: 2.141347110271454

Epoch: 6| Step: 6
Training loss: 1.689445972442627
Validation loss: 2.121187925338745

Epoch: 6| Step: 7
Training loss: 2.7935259342193604
Validation loss: 2.1161247889200845

Epoch: 6| Step: 8
Training loss: 1.5253361463546753
Validation loss: 2.096659322579702

Epoch: 6| Step: 9
Training loss: 1.9752483367919922
Validation loss: 2.098413328329722

Epoch: 6| Step: 10
Training loss: 1.6131641864776611
Validation loss: 2.0833517710367837

Epoch: 6| Step: 11
Training loss: 1.3444020748138428
Validation loss: 2.0928799510002136

Epoch: 6| Step: 12
Training loss: 2.0980443954467773
Validation loss: 2.085694889227549

Epoch: 6| Step: 13
Training loss: 2.3652091026306152
Validation loss: 2.085015336672465

Epoch: 153| Step: 0
Training loss: 1.879201054573059
Validation loss: 2.0972997744878135

Epoch: 6| Step: 1
Training loss: 2.424234390258789
Validation loss: 2.10238645474116

Epoch: 6| Step: 2
Training loss: 1.654516577720642
Validation loss: 2.110351840655009

Epoch: 6| Step: 3
Training loss: 2.2720799446105957
Validation loss: 2.1055663228034973

Epoch: 6| Step: 4
Training loss: 2.4011991024017334
Validation loss: 2.1098899443944297

Epoch: 6| Step: 5
Training loss: 2.3696465492248535
Validation loss: 2.1181834936141968

Epoch: 6| Step: 6
Training loss: 2.054987668991089
Validation loss: 2.102088967959086

Epoch: 6| Step: 7
Training loss: 1.886059284210205
Validation loss: 2.1222270329793296

Epoch: 6| Step: 8
Training loss: 1.3400262594223022
Validation loss: 2.117962121963501

Epoch: 6| Step: 9
Training loss: 1.737143635749817
Validation loss: 2.0986652771631875

Epoch: 6| Step: 10
Training loss: 2.1029107570648193
Validation loss: 2.1073068181673684

Epoch: 6| Step: 11
Training loss: 1.953820824623108
Validation loss: 2.1091933250427246

Epoch: 6| Step: 12
Training loss: 2.080451726913452
Validation loss: 2.113350590070089

Epoch: 6| Step: 13
Training loss: 1.2363529205322266
Validation loss: 2.0940348307291665

Epoch: 154| Step: 0
Training loss: 1.7449345588684082
Validation loss: 2.114358146985372

Epoch: 6| Step: 1
Training loss: 2.615438938140869
Validation loss: 2.1161601742108664

Epoch: 6| Step: 2
Training loss: 1.8107783794403076
Validation loss: 2.1162800788879395

Epoch: 6| Step: 3
Training loss: 2.0263171195983887
Validation loss: 2.1218334635098777

Epoch: 6| Step: 4
Training loss: 2.378580093383789
Validation loss: 2.1308268308639526

Epoch: 6| Step: 5
Training loss: 1.7349718809127808
Validation loss: 2.1324055989583335

Epoch: 6| Step: 6
Training loss: 1.679978609085083
Validation loss: 2.13468599319458

Epoch: 6| Step: 7
Training loss: 1.6677472591400146
Validation loss: 2.150013486544291

Epoch: 6| Step: 8
Training loss: 1.946748971939087
Validation loss: 2.140712002913157

Epoch: 6| Step: 9
Training loss: 1.6369328498840332
Validation loss: 2.1327561736106873

Epoch: 6| Step: 10
Training loss: 1.8591728210449219
Validation loss: 2.1229779720306396

Epoch: 6| Step: 11
Training loss: 2.7593531608581543
Validation loss: 2.133357842763265

Epoch: 6| Step: 12
Training loss: 1.6083815097808838
Validation loss: 2.1322378516197205

Epoch: 6| Step: 13
Training loss: 1.801712989807129
Validation loss: 2.1398870944976807

Epoch: 155| Step: 0
Training loss: 1.811168909072876
Validation loss: 2.12519234418869

Epoch: 6| Step: 1
Training loss: 1.9563069343566895
Validation loss: 2.124221762021383

Epoch: 6| Step: 2
Training loss: 2.0155200958251953
Validation loss: 2.1190746823946633

Epoch: 6| Step: 3
Training loss: 1.684203863143921
Validation loss: 2.1204192439715066

Epoch: 6| Step: 4
Training loss: 1.5339291095733643
Validation loss: 2.111550807952881

Epoch: 6| Step: 5
Training loss: 1.8851351737976074
Validation loss: 2.1178104082743325

Epoch: 6| Step: 6
Training loss: 2.1559107303619385
Validation loss: 2.118957002957662

Epoch: 6| Step: 7
Training loss: 1.6927337646484375
Validation loss: 2.1015571753184

Epoch: 6| Step: 8
Training loss: 2.6823878288269043
Validation loss: 2.119409720102946

Epoch: 6| Step: 9
Training loss: 1.9505116939544678
Validation loss: 2.1119339863459268

Epoch: 6| Step: 10
Training loss: 1.9035124778747559
Validation loss: 2.1105061570803323

Epoch: 6| Step: 11
Training loss: 2.232692241668701
Validation loss: 2.1198639074961343

Epoch: 6| Step: 12
Training loss: 1.9742320775985718
Validation loss: 2.113844354947408

Epoch: 6| Step: 13
Training loss: 1.7097768783569336
Validation loss: 2.1172650853792825

Epoch: 156| Step: 0
Training loss: 2.118762969970703
Validation loss: 2.130941112836202

Epoch: 6| Step: 1
Training loss: 1.603034496307373
Validation loss: 2.129787007967631

Epoch: 6| Step: 2
Training loss: 2.126626968383789
Validation loss: 2.1264546116193137

Epoch: 6| Step: 3
Training loss: 1.9737186431884766
Validation loss: 2.1157801946004233

Epoch: 6| Step: 4
Training loss: 1.460949182510376
Validation loss: 2.1216936508814492

Epoch: 6| Step: 5
Training loss: 1.544220209121704
Validation loss: 2.1022809545199075

Epoch: 6| Step: 6
Training loss: 2.2298495769500732
Validation loss: 2.125334401925405

Epoch: 6| Step: 7
Training loss: 1.8190659284591675
Validation loss: 2.0982986887296042

Epoch: 6| Step: 8
Training loss: 1.918172001838684
Validation loss: 2.1162309447924295

Epoch: 6| Step: 9
Training loss: 1.4932496547698975
Validation loss: 2.1001076698303223

Epoch: 6| Step: 10
Training loss: 2.439663887023926
Validation loss: 2.1031991442044577

Epoch: 6| Step: 11
Training loss: 2.19096040725708
Validation loss: 2.1020753979682922

Epoch: 6| Step: 12
Training loss: 2.26029634475708
Validation loss: 2.1183180014292398

Epoch: 6| Step: 13
Training loss: 2.117797374725342
Validation loss: 2.100282867749532

Epoch: 157| Step: 0
Training loss: 1.3022074699401855
Validation loss: 2.111172000567118

Epoch: 6| Step: 1
Training loss: 2.281132221221924
Validation loss: 2.103718022505442

Epoch: 6| Step: 2
Training loss: 2.1495201587677
Validation loss: 2.0971301595369973

Epoch: 6| Step: 3
Training loss: 2.113617181777954
Validation loss: 2.0919891595840454

Epoch: 6| Step: 4
Training loss: 2.0082054138183594
Validation loss: 2.0971173842748008

Epoch: 6| Step: 5
Training loss: 2.4654428958892822
Validation loss: 2.105343242486318

Epoch: 6| Step: 6
Training loss: 1.8952616453170776
Validation loss: 2.101536273956299

Epoch: 6| Step: 7
Training loss: 1.827666163444519
Validation loss: 2.090750594933828

Epoch: 6| Step: 8
Training loss: 1.5987370014190674
Validation loss: 2.1041279633839927

Epoch: 6| Step: 9
Training loss: 2.150179386138916
Validation loss: 2.108404894669851

Epoch: 6| Step: 10
Training loss: 1.7460265159606934
Validation loss: 2.1286237239837646

Epoch: 6| Step: 11
Training loss: 1.8187894821166992
Validation loss: 2.124943653742472

Epoch: 6| Step: 12
Training loss: 2.2957916259765625
Validation loss: 2.1513131658236184

Epoch: 6| Step: 13
Training loss: 1.6287908554077148
Validation loss: 2.1528720458348594

Epoch: 158| Step: 0
Training loss: 1.905841588973999
Validation loss: 2.131922960281372

Epoch: 6| Step: 1
Training loss: 1.742517113685608
Validation loss: 2.1445430914560952

Epoch: 6| Step: 2
Training loss: 1.8203703165054321
Validation loss: 2.1504950722058616

Epoch: 6| Step: 3
Training loss: 1.9102333784103394
Validation loss: 2.13828045129776

Epoch: 6| Step: 4
Training loss: 2.2720417976379395
Validation loss: 2.1395930449167886

Epoch: 6| Step: 5
Training loss: 2.4082655906677246
Validation loss: 2.1358648538589478

Epoch: 6| Step: 6
Training loss: 1.671703577041626
Validation loss: 2.1318779985109964

Epoch: 6| Step: 7
Training loss: 2.1109137535095215
Validation loss: 2.1283324162165322

Epoch: 6| Step: 8
Training loss: 2.0890207290649414
Validation loss: 2.14416112502416

Epoch: 6| Step: 9
Training loss: 1.8081071376800537
Validation loss: 2.1532766222953796

Epoch: 6| Step: 10
Training loss: 1.7863799333572388
Validation loss: 2.140685796737671

Epoch: 6| Step: 11
Training loss: 2.3115627765655518
Validation loss: 2.15674622853597

Epoch: 6| Step: 12
Training loss: 1.6450027227401733
Validation loss: 2.154965341091156

Epoch: 6| Step: 13
Training loss: 2.265394687652588
Validation loss: 2.155313014984131

Epoch: 159| Step: 0
Training loss: 1.832978367805481
Validation loss: 2.1458857456843057

Epoch: 6| Step: 1
Training loss: 1.4818017482757568
Validation loss: 2.1656259298324585

Epoch: 6| Step: 2
Training loss: 2.239985466003418
Validation loss: 2.126716693242391

Epoch: 6| Step: 3
Training loss: 2.1453423500061035
Validation loss: 2.1346861720085144

Epoch: 6| Step: 4
Training loss: 1.97627592086792
Validation loss: 2.118795077006022

Epoch: 6| Step: 5
Training loss: 2.577544689178467
Validation loss: 2.1266498366991677

Epoch: 6| Step: 6
Training loss: 1.6226729154586792
Validation loss: 2.1308765610059104

Epoch: 6| Step: 7
Training loss: 1.2069084644317627
Validation loss: 2.1385690371195474

Epoch: 6| Step: 8
Training loss: 1.1358940601348877
Validation loss: 2.1458436648050943

Epoch: 6| Step: 9
Training loss: 2.1022024154663086
Validation loss: 2.132412771383921

Epoch: 6| Step: 10
Training loss: 2.4481911659240723
Validation loss: 2.137934684753418

Epoch: 6| Step: 11
Training loss: 2.1866092681884766
Validation loss: 2.1311649084091187

Epoch: 6| Step: 12
Training loss: 2.05002498626709
Validation loss: 2.1279286543528237

Epoch: 6| Step: 13
Training loss: 1.9180214405059814
Validation loss: 2.114066958427429

Epoch: 160| Step: 0
Training loss: 1.5427629947662354
Validation loss: 2.136341691017151

Epoch: 6| Step: 1
Training loss: 2.152757167816162
Validation loss: 2.1420949498812356

Epoch: 6| Step: 2
Training loss: 1.885035514831543
Validation loss: 2.1405237118403115

Epoch: 6| Step: 3
Training loss: 1.6900839805603027
Validation loss: 2.138667424519857

Epoch: 6| Step: 4
Training loss: 1.519666075706482
Validation loss: 2.1406577626864114

Epoch: 6| Step: 5
Training loss: 2.103341579437256
Validation loss: 2.164032260576884

Epoch: 6| Step: 6
Training loss: 1.8130300045013428
Validation loss: 2.145820160706838

Epoch: 6| Step: 7
Training loss: 1.7887978553771973
Validation loss: 2.165749192237854

Epoch: 6| Step: 8
Training loss: 1.9431943893432617
Validation loss: 2.1494973500569663

Epoch: 6| Step: 9
Training loss: 2.2359585762023926
Validation loss: 2.137373467286428

Epoch: 6| Step: 10
Training loss: 2.3147127628326416
Validation loss: 2.150635580221812

Epoch: 6| Step: 11
Training loss: 2.6605162620544434
Validation loss: 2.1360939343770347

Epoch: 6| Step: 12
Training loss: 1.4469729661941528
Validation loss: 2.1252442797025046

Epoch: 6| Step: 13
Training loss: 1.9810808897018433
Validation loss: 2.161015729109446

Epoch: 161| Step: 0
Training loss: 2.623088836669922
Validation loss: 2.147598922252655

Epoch: 6| Step: 1
Training loss: 1.6497278213500977
Validation loss: 2.149067680040995

Epoch: 6| Step: 2
Training loss: 1.713932752609253
Validation loss: 2.169781982898712

Epoch: 6| Step: 3
Training loss: 1.5095388889312744
Validation loss: 2.1366754174232483

Epoch: 6| Step: 4
Training loss: 1.573455810546875
Validation loss: 2.1447465817133584

Epoch: 6| Step: 5
Training loss: 1.4441330432891846
Validation loss: 2.1422460675239563

Epoch: 6| Step: 6
Training loss: 2.0041732788085938
Validation loss: 2.1437834103902182

Epoch: 6| Step: 7
Training loss: 2.132192850112915
Validation loss: 2.121745467185974

Epoch: 6| Step: 8
Training loss: 2.6762924194335938
Validation loss: 2.129198690255483

Epoch: 6| Step: 9
Training loss: 1.9033111333847046
Validation loss: 2.1256444454193115

Epoch: 6| Step: 10
Training loss: 1.5773948431015015
Validation loss: 2.114923278490702

Epoch: 6| Step: 11
Training loss: 2.2291369438171387
Validation loss: 2.1247110764185586

Epoch: 6| Step: 12
Training loss: 2.1777639389038086
Validation loss: 2.113306999206543

Epoch: 6| Step: 13
Training loss: 1.6434941291809082
Validation loss: 2.1295147140820823

Epoch: 162| Step: 0
Training loss: 1.944203495979309
Validation loss: 2.113014300664266

Epoch: 6| Step: 1
Training loss: 1.8149968385696411
Validation loss: 2.131553848584493

Epoch: 6| Step: 2
Training loss: 1.812239170074463
Validation loss: 2.112188140551249

Epoch: 6| Step: 3
Training loss: 2.0859322547912598
Validation loss: 2.1294475396474204

Epoch: 6| Step: 4
Training loss: 1.4960622787475586
Validation loss: 2.1251765489578247

Epoch: 6| Step: 5
Training loss: 2.48511004447937
Validation loss: 2.130953788757324

Epoch: 6| Step: 6
Training loss: 2.1424224376678467
Validation loss: 2.1363360484441123

Epoch: 6| Step: 7
Training loss: 1.6098027229309082
Validation loss: 2.135746876398722

Epoch: 6| Step: 8
Training loss: 1.751874327659607
Validation loss: 2.1307145158449807

Epoch: 6| Step: 9
Training loss: 1.4676377773284912
Validation loss: 2.137008468310038

Epoch: 6| Step: 10
Training loss: 1.7456008195877075
Validation loss: 2.1419225335121155

Epoch: 6| Step: 11
Training loss: 2.255138397216797
Validation loss: 2.135642965634664

Epoch: 6| Step: 12
Training loss: 2.355531692504883
Validation loss: 2.140448033809662

Epoch: 6| Step: 13
Training loss: 2.0778801441192627
Validation loss: 2.135179340839386

Epoch: 163| Step: 0
Training loss: 1.9772216081619263
Validation loss: 2.1208929419517517

Epoch: 6| Step: 1
Training loss: 2.4738729000091553
Validation loss: 2.114643077055613

Epoch: 6| Step: 2
Training loss: 2.0573973655700684
Validation loss: 2.1219404141108194

Epoch: 6| Step: 3
Training loss: 3.010204792022705
Validation loss: 2.145578662554423

Epoch: 6| Step: 4
Training loss: 1.4817845821380615
Validation loss: 2.141277273495992

Epoch: 6| Step: 5
Training loss: 2.2663238048553467
Validation loss: 2.1476135849952698

Epoch: 6| Step: 6
Training loss: 1.763814926147461
Validation loss: 2.1512136856714883

Epoch: 6| Step: 7
Training loss: 1.6496870517730713
Validation loss: 2.156442622343699

Epoch: 6| Step: 8
Training loss: 1.7140247821807861
Validation loss: 2.160429139931997

Epoch: 6| Step: 9
Training loss: 1.6432006359100342
Validation loss: 2.1560763716697693

Epoch: 6| Step: 10
Training loss: 2.1863908767700195
Validation loss: 2.155262569586436

Epoch: 6| Step: 11
Training loss: 1.263956069946289
Validation loss: 2.1317352652549744

Epoch: 6| Step: 12
Training loss: 1.8688781261444092
Validation loss: 2.1223899523417153

Epoch: 6| Step: 13
Training loss: 1.9744255542755127
Validation loss: 2.1229432423909507

Epoch: 164| Step: 0
Training loss: 2.4935154914855957
Validation loss: 2.1251664559046426

Epoch: 6| Step: 1
Training loss: 1.6180543899536133
Validation loss: 2.1228888630867004

Epoch: 6| Step: 2
Training loss: 1.599021553993225
Validation loss: 2.1216179529825845

Epoch: 6| Step: 3
Training loss: 2.1298811435699463
Validation loss: 2.12798535823822

Epoch: 6| Step: 4
Training loss: 1.8328986167907715
Validation loss: 2.1344142953554788

Epoch: 6| Step: 5
Training loss: 2.3913345336914062
Validation loss: 2.14057993888855

Epoch: 6| Step: 6
Training loss: 1.4685323238372803
Validation loss: 2.1290427645047507

Epoch: 6| Step: 7
Training loss: 1.7028381824493408
Validation loss: 2.122726639111837

Epoch: 6| Step: 8
Training loss: 2.015636444091797
Validation loss: 2.165241539478302

Epoch: 6| Step: 9
Training loss: 1.6697685718536377
Validation loss: 2.1512449185053506

Epoch: 6| Step: 10
Training loss: 2.1231226921081543
Validation loss: 2.153873880704244

Epoch: 6| Step: 11
Training loss: 1.5611083507537842
Validation loss: 2.1368194619814553

Epoch: 6| Step: 12
Training loss: 2.481119394302368
Validation loss: 2.1346686085065207

Epoch: 6| Step: 13
Training loss: 1.9141898155212402
Validation loss: 2.133596738179525

Epoch: 165| Step: 0
Training loss: 2.503774404525757
Validation loss: 2.135909994443258

Epoch: 6| Step: 1
Training loss: 2.232884407043457
Validation loss: 2.1143662532170615

Epoch: 6| Step: 2
Training loss: 1.5722968578338623
Validation loss: 2.1394606630007424

Epoch: 6| Step: 3
Training loss: 1.9968750476837158
Validation loss: 2.1319249669710794

Epoch: 6| Step: 4
Training loss: 1.1980445384979248
Validation loss: 2.1243011554082236

Epoch: 6| Step: 5
Training loss: 1.3708547353744507
Validation loss: 2.125613550345103

Epoch: 6| Step: 6
Training loss: 2.706317663192749
Validation loss: 2.11481245358785

Epoch: 6| Step: 7
Training loss: 1.9072620868682861
Validation loss: 2.143450895945231

Epoch: 6| Step: 8
Training loss: 2.0157127380371094
Validation loss: 2.1307037671407065

Epoch: 6| Step: 9
Training loss: 2.2166035175323486
Validation loss: 2.1530300974845886

Epoch: 6| Step: 10
Training loss: 1.9437662363052368
Validation loss: 2.1375836531321206

Epoch: 6| Step: 11
Training loss: 1.7826277017593384
Validation loss: 2.1458902955055237

Epoch: 6| Step: 12
Training loss: 1.9532171487808228
Validation loss: 2.144540627797445

Epoch: 6| Step: 13
Training loss: 1.426956057548523
Validation loss: 2.1382559736569724

Epoch: 166| Step: 0
Training loss: 2.166254758834839
Validation loss: 2.127906064192454

Epoch: 6| Step: 1
Training loss: 1.3564329147338867
Validation loss: 2.124531924724579

Epoch: 6| Step: 2
Training loss: 1.7663969993591309
Validation loss: 2.1449424227078757

Epoch: 6| Step: 3
Training loss: 1.807225227355957
Validation loss: 2.146323561668396

Epoch: 6| Step: 4
Training loss: 1.5136017799377441
Validation loss: 2.146541714668274

Epoch: 6| Step: 5
Training loss: 2.1281628608703613
Validation loss: 2.144767622152964

Epoch: 6| Step: 6
Training loss: 1.6735261678695679
Validation loss: 2.1425739924112954

Epoch: 6| Step: 7
Training loss: 1.977750301361084
Validation loss: 2.1384692589441934

Epoch: 6| Step: 8
Training loss: 2.8627161979675293
Validation loss: 2.153603653113047

Epoch: 6| Step: 9
Training loss: 1.8071014881134033
Validation loss: 2.1510465343793235

Epoch: 6| Step: 10
Training loss: 2.078127384185791
Validation loss: 2.13564670085907

Epoch: 6| Step: 11
Training loss: 1.635406494140625
Validation loss: 2.1430493791898093

Epoch: 6| Step: 12
Training loss: 2.1946280002593994
Validation loss: 2.126635432243347

Epoch: 6| Step: 13
Training loss: 1.7667183876037598
Validation loss: 2.129080613454183

Epoch: 167| Step: 0
Training loss: 1.8044085502624512
Validation loss: 2.1371291478474936

Epoch: 6| Step: 1
Training loss: 1.638871669769287
Validation loss: 2.1436426242192588

Epoch: 6| Step: 2
Training loss: 1.5108598470687866
Validation loss: 2.148734966913859

Epoch: 6| Step: 3
Training loss: 1.3567452430725098
Validation loss: 2.148315111796061

Epoch: 6| Step: 4
Training loss: 2.137850284576416
Validation loss: 2.1420698364575705

Epoch: 6| Step: 5
Training loss: 1.9873754978179932
Validation loss: 2.1774583061536155

Epoch: 6| Step: 6
Training loss: 2.224656343460083
Validation loss: 2.176580826441447

Epoch: 6| Step: 7
Training loss: 2.0402474403381348
Validation loss: 2.165856202443441

Epoch: 6| Step: 8
Training loss: 2.2060465812683105
Validation loss: 2.1671169797579446

Epoch: 6| Step: 9
Training loss: 1.9660495519638062
Validation loss: 2.1487604777018228

Epoch: 6| Step: 10
Training loss: 1.8375606536865234
Validation loss: 2.1538281639417014

Epoch: 6| Step: 11
Training loss: 1.6047042608261108
Validation loss: 2.1313721537590027

Epoch: 6| Step: 12
Training loss: 1.6726723909378052
Validation loss: 2.1441235144933066

Epoch: 6| Step: 13
Training loss: 3.1568315029144287
Validation loss: 2.142173628012339

Epoch: 168| Step: 0
Training loss: 2.148470878601074
Validation loss: 2.1411745150883994

Epoch: 6| Step: 1
Training loss: 1.7790721654891968
Validation loss: 2.1347789764404297

Epoch: 6| Step: 2
Training loss: 1.118278980255127
Validation loss: 2.1302030285199485

Epoch: 6| Step: 3
Training loss: 2.3861634731292725
Validation loss: 2.141706923643748

Epoch: 6| Step: 4
Training loss: 2.00635027885437
Validation loss: 2.1397369305292764

Epoch: 6| Step: 5
Training loss: 2.1167385578155518
Validation loss: 2.126293897628784

Epoch: 6| Step: 6
Training loss: 2.2813453674316406
Validation loss: 2.1400625705718994

Epoch: 6| Step: 7
Training loss: 1.9605697393417358
Validation loss: 2.1495882074038186

Epoch: 6| Step: 8
Training loss: 1.788221836090088
Validation loss: 2.157712757587433

Epoch: 6| Step: 9
Training loss: 2.4029579162597656
Validation loss: 2.133880853652954

Epoch: 6| Step: 10
Training loss: 1.7853806018829346
Validation loss: 2.134612242380778

Epoch: 6| Step: 11
Training loss: 2.075746536254883
Validation loss: 2.130970855553945

Epoch: 6| Step: 12
Training loss: 2.052830696105957
Validation loss: 2.141962726910909

Epoch: 6| Step: 13
Training loss: 1.307070016860962
Validation loss: 2.1246562004089355

Epoch: 169| Step: 0
Training loss: 2.0810256004333496
Validation loss: 2.1264998515446982

Epoch: 6| Step: 1
Training loss: 1.961371660232544
Validation loss: 2.1202587286631265

Epoch: 6| Step: 2
Training loss: 2.0399718284606934
Validation loss: 2.1290526588757834

Epoch: 6| Step: 3
Training loss: 1.6806702613830566
Validation loss: 2.1225831309954324

Epoch: 6| Step: 4
Training loss: 1.7771289348602295
Validation loss: 2.1377110679944358

Epoch: 6| Step: 5
Training loss: 1.3384371995925903
Validation loss: 2.146451453367869

Epoch: 6| Step: 6
Training loss: 2.03033709526062
Validation loss: 2.1598597367604575

Epoch: 6| Step: 7
Training loss: 1.8200297355651855
Validation loss: 2.1534448862075806

Epoch: 6| Step: 8
Training loss: 2.10666561126709
Validation loss: 2.138322909673055

Epoch: 6| Step: 9
Training loss: 1.807411551475525
Validation loss: 2.1451589266459146

Epoch: 6| Step: 10
Training loss: 1.5411683320999146
Validation loss: 2.139613370100657

Epoch: 6| Step: 11
Training loss: 2.506418228149414
Validation loss: 2.13021053870519

Epoch: 6| Step: 12
Training loss: 1.7571369409561157
Validation loss: 2.1167727510134378

Epoch: 6| Step: 13
Training loss: 2.5129213333129883
Validation loss: 2.1170883178710938

Epoch: 170| Step: 0
Training loss: 2.6474666595458984
Validation loss: 2.1243865887324014

Epoch: 6| Step: 1
Training loss: 1.756786823272705
Validation loss: 2.128977676232656

Epoch: 6| Step: 2
Training loss: 2.095289707183838
Validation loss: 2.1356637279192605

Epoch: 6| Step: 3
Training loss: 1.6949388980865479
Validation loss: 2.1396270593007407

Epoch: 6| Step: 4
Training loss: 1.869077444076538
Validation loss: 2.1478893756866455

Epoch: 6| Step: 5
Training loss: 2.0897774696350098
Validation loss: 2.167892316977183

Epoch: 6| Step: 6
Training loss: 1.9152565002441406
Validation loss: 2.159107804298401

Epoch: 6| Step: 7
Training loss: 1.7770500183105469
Validation loss: 2.179456114768982

Epoch: 6| Step: 8
Training loss: 2.3655707836151123
Validation loss: 2.151887853940328

Epoch: 6| Step: 9
Training loss: 1.7509379386901855
Validation loss: 2.138340393702189

Epoch: 6| Step: 10
Training loss: 1.885277271270752
Validation loss: 2.116608182589213

Epoch: 6| Step: 11
Training loss: 1.3526643514633179
Validation loss: 2.1153772274653115

Epoch: 6| Step: 12
Training loss: 1.9186723232269287
Validation loss: 2.1202774246533713

Epoch: 6| Step: 13
Training loss: 2.213855028152466
Validation loss: 2.1117364168167114

Epoch: 171| Step: 0
Training loss: 2.33258056640625
Validation loss: 2.113657772541046

Epoch: 6| Step: 1
Training loss: 2.194218635559082
Validation loss: 2.105680604775747

Epoch: 6| Step: 2
Training loss: 1.7304939031600952
Validation loss: 2.099444647630056

Epoch: 6| Step: 3
Training loss: 2.364323854446411
Validation loss: 2.096571147441864

Epoch: 6| Step: 4
Training loss: 2.319354295730591
Validation loss: 2.101342519124349

Epoch: 6| Step: 5
Training loss: 2.3635618686676025
Validation loss: 2.0987043579419455

Epoch: 6| Step: 6
Training loss: 1.4175488948822021
Validation loss: 2.0968605677286782

Epoch: 6| Step: 7
Training loss: 1.5460113286972046
Validation loss: 2.1027846733729043

Epoch: 6| Step: 8
Training loss: 1.972983479499817
Validation loss: 2.116158405939738

Epoch: 6| Step: 9
Training loss: 1.9314852952957153
Validation loss: 2.107747217019399

Epoch: 6| Step: 10
Training loss: 1.58069908618927
Validation loss: 2.122442305088043

Epoch: 6| Step: 11
Training loss: 1.7894940376281738
Validation loss: 2.134154180685679

Epoch: 6| Step: 12
Training loss: 2.180924892425537
Validation loss: 2.170241435368856

Epoch: 6| Step: 13
Training loss: 2.219010829925537
Validation loss: 2.1674527327219644

Epoch: 172| Step: 0
Training loss: 2.1622314453125
Validation loss: 2.1529349088668823

Epoch: 6| Step: 1
Training loss: 2.006828784942627
Validation loss: 2.1524500846862793

Epoch: 6| Step: 2
Training loss: 2.437960624694824
Validation loss: 2.1427952448527017

Epoch: 6| Step: 3
Training loss: 1.9055719375610352
Validation loss: 2.151368200778961

Epoch: 6| Step: 4
Training loss: 2.2833642959594727
Validation loss: 2.1363292932510376

Epoch: 6| Step: 5
Training loss: 2.348054885864258
Validation loss: 2.129863222440084

Epoch: 6| Step: 6
Training loss: 1.5157761573791504
Validation loss: 2.1246963342030845

Epoch: 6| Step: 7
Training loss: 1.2976747751235962
Validation loss: 2.123615324497223

Epoch: 6| Step: 8
Training loss: 1.8486653566360474
Validation loss: 2.1210466027259827

Epoch: 6| Step: 9
Training loss: 1.7768927812576294
Validation loss: 2.129730304082235

Epoch: 6| Step: 10
Training loss: 1.3809185028076172
Validation loss: 2.1292210817337036

Epoch: 6| Step: 11
Training loss: 1.9628726243972778
Validation loss: 2.1450344125429788

Epoch: 6| Step: 12
Training loss: 1.72909677028656
Validation loss: 2.147420565287272

Epoch: 6| Step: 13
Training loss: 2.224246025085449
Validation loss: 2.159232517083486

Epoch: 173| Step: 0
Training loss: 1.7588049173355103
Validation loss: 2.1454954743385315

Epoch: 6| Step: 1
Training loss: 1.8959059715270996
Validation loss: 2.1707332134246826

Epoch: 6| Step: 2
Training loss: 2.6164841651916504
Validation loss: 2.2040414015452066

Epoch: 6| Step: 3
Training loss: 1.8826282024383545
Validation loss: 2.175311009089152

Epoch: 6| Step: 4
Training loss: 2.5158467292785645
Validation loss: 2.1636090079943338

Epoch: 6| Step: 5
Training loss: 1.5102269649505615
Validation loss: 2.1364216009775796

Epoch: 6| Step: 6
Training loss: 2.876673936843872
Validation loss: 2.1178500652313232

Epoch: 6| Step: 7
Training loss: 1.707271933555603
Validation loss: 2.1322520971298218

Epoch: 6| Step: 8
Training loss: 1.8512789011001587
Validation loss: 2.1061823964118958

Epoch: 6| Step: 9
Training loss: 1.638209581375122
Validation loss: 2.115028460820516

Epoch: 6| Step: 10
Training loss: 2.2378458976745605
Validation loss: 2.104183614253998

Epoch: 6| Step: 11
Training loss: 1.8786836862564087
Validation loss: 2.1163212656974792

Epoch: 6| Step: 12
Training loss: 1.6215689182281494
Validation loss: 2.127182205518087

Epoch: 6| Step: 13
Training loss: 1.7875990867614746
Validation loss: 2.111719032128652

Epoch: 174| Step: 0
Training loss: 1.4750916957855225
Validation loss: 2.1123251716295877

Epoch: 6| Step: 1
Training loss: 1.775635004043579
Validation loss: 2.11263108253479

Epoch: 6| Step: 2
Training loss: 2.341169834136963
Validation loss: 2.118067741394043

Epoch: 6| Step: 3
Training loss: 1.6145966053009033
Validation loss: 2.1326036850611367

Epoch: 6| Step: 4
Training loss: 1.615476131439209
Validation loss: 2.1371925671895347

Epoch: 6| Step: 5
Training loss: 2.1056246757507324
Validation loss: 2.139560063680013

Epoch: 6| Step: 6
Training loss: 2.0236196517944336
Validation loss: 2.1479848424593606

Epoch: 6| Step: 7
Training loss: 2.6496667861938477
Validation loss: 2.129221200942993

Epoch: 6| Step: 8
Training loss: 2.1324968338012695
Validation loss: 2.1390246748924255

Epoch: 6| Step: 9
Training loss: 1.9600468873977661
Validation loss: 2.1216461658477783

Epoch: 6| Step: 10
Training loss: 2.1383509635925293
Validation loss: 2.138381858666738

Epoch: 6| Step: 11
Training loss: 2.071000576019287
Validation loss: 2.1243460377057395

Epoch: 6| Step: 12
Training loss: 2.0544681549072266
Validation loss: 2.134700298309326

Epoch: 6| Step: 13
Training loss: 1.6658430099487305
Validation loss: 2.1476189692815146

Epoch: 175| Step: 0
Training loss: 1.7323038578033447
Validation loss: 2.128740688165029

Epoch: 6| Step: 1
Training loss: 2.1588923931121826
Validation loss: 2.136317253112793

Epoch: 6| Step: 2
Training loss: 1.9370445013046265
Validation loss: 2.1413955688476562

Epoch: 6| Step: 3
Training loss: 1.9435293674468994
Validation loss: 2.1362130443255105

Epoch: 6| Step: 4
Training loss: 1.967695713043213
Validation loss: 2.130110502243042

Epoch: 6| Step: 5
Training loss: 1.6461470127105713
Validation loss: 2.1329105496406555

Epoch: 6| Step: 6
Training loss: 2.3039636611938477
Validation loss: 2.1329575181007385

Epoch: 6| Step: 7
Training loss: 2.0907223224639893
Validation loss: 2.1333797574043274

Epoch: 6| Step: 8
Training loss: 2.306457996368408
Validation loss: 2.1262495716412864

Epoch: 6| Step: 9
Training loss: 1.7687809467315674
Validation loss: 2.1222371061642966

Epoch: 6| Step: 10
Training loss: 2.088998317718506
Validation loss: 2.122413158416748

Epoch: 6| Step: 11
Training loss: 2.0509557723999023
Validation loss: 2.1376954913139343

Epoch: 6| Step: 12
Training loss: 1.5107522010803223
Validation loss: 2.1309046745300293

Epoch: 6| Step: 13
Training loss: 1.575875997543335
Validation loss: 2.137149671713511

Epoch: 176| Step: 0
Training loss: 1.9798524379730225
Validation loss: 2.140497088432312

Epoch: 6| Step: 1
Training loss: 1.9990341663360596
Validation loss: 2.1493705908457437

Epoch: 6| Step: 2
Training loss: 1.910597324371338
Validation loss: 2.151581108570099

Epoch: 6| Step: 3
Training loss: 1.9173685312271118
Validation loss: 2.1542394955952964

Epoch: 6| Step: 4
Training loss: 1.5807793140411377
Validation loss: 2.171728869279226

Epoch: 6| Step: 5
Training loss: 2.3756914138793945
Validation loss: 2.1518755157788596

Epoch: 6| Step: 6
Training loss: 2.2585699558258057
Validation loss: 2.141909380753835

Epoch: 6| Step: 7
Training loss: 1.8082399368286133
Validation loss: 2.151773154735565

Epoch: 6| Step: 8
Training loss: 1.4845972061157227
Validation loss: 2.1469656229019165

Epoch: 6| Step: 9
Training loss: 1.8761870861053467
Validation loss: 2.1411133209864297

Epoch: 6| Step: 10
Training loss: 1.6947077512741089
Validation loss: 2.1512672901153564

Epoch: 6| Step: 11
Training loss: 1.533539056777954
Validation loss: 2.1467328468958535

Epoch: 6| Step: 12
Training loss: 2.3045637607574463
Validation loss: 2.145462433497111

Epoch: 6| Step: 13
Training loss: 1.763309359550476
Validation loss: 2.156773249308268

Epoch: 177| Step: 0
Training loss: 2.1667418479919434
Validation loss: 2.154696226119995

Epoch: 6| Step: 1
Training loss: 0.9557095766067505
Validation loss: 2.177327891190847

Epoch: 6| Step: 2
Training loss: 1.9537038803100586
Validation loss: 2.173513929049174

Epoch: 6| Step: 3
Training loss: 2.4100475311279297
Validation loss: 2.160207907358805

Epoch: 6| Step: 4
Training loss: 1.9449090957641602
Validation loss: 2.1538947423299155

Epoch: 6| Step: 5
Training loss: 2.129873752593994
Validation loss: 2.1446314454078674

Epoch: 6| Step: 6
Training loss: 2.2182669639587402
Validation loss: 2.1566785971323648

Epoch: 6| Step: 7
Training loss: 1.5618828535079956
Validation loss: 2.1361587246259055

Epoch: 6| Step: 8
Training loss: 2.0504231452941895
Validation loss: 2.133321384588877

Epoch: 6| Step: 9
Training loss: 1.783968210220337
Validation loss: 2.122648298740387

Epoch: 6| Step: 10
Training loss: 1.9089213609695435
Validation loss: 2.1379488706588745

Epoch: 6| Step: 11
Training loss: 1.4972803592681885
Validation loss: 2.1362579663594565

Epoch: 6| Step: 12
Training loss: 1.678447961807251
Validation loss: 2.112664759159088

Epoch: 6| Step: 13
Training loss: 2.4095144271850586
Validation loss: 2.130110780398051

Epoch: 178| Step: 0
Training loss: 1.922853708267212
Validation loss: 2.1278584400812783

Epoch: 6| Step: 1
Training loss: 1.2512774467468262
Validation loss: 2.1297479470570884

Epoch: 6| Step: 2
Training loss: 1.8212478160858154
Validation loss: 2.1560264031092324

Epoch: 6| Step: 3
Training loss: 1.3456262350082397
Validation loss: 2.1675947507222495

Epoch: 6| Step: 4
Training loss: 1.7521204948425293
Validation loss: 2.136812766393026

Epoch: 6| Step: 5
Training loss: 1.8398563861846924
Validation loss: 2.1421348253885903

Epoch: 6| Step: 6
Training loss: 1.6980094909667969
Validation loss: 2.1489926973978677

Epoch: 6| Step: 7
Training loss: 2.1631217002868652
Validation loss: 2.1429993510246277

Epoch: 6| Step: 8
Training loss: 2.5037198066711426
Validation loss: 2.150114436944326

Epoch: 6| Step: 9
Training loss: 2.453789234161377
Validation loss: 2.1507949829101562

Epoch: 6| Step: 10
Training loss: 1.650941252708435
Validation loss: 2.1528308987617493

Epoch: 6| Step: 11
Training loss: 1.6779098510742188
Validation loss: 2.156883796056112

Epoch: 6| Step: 12
Training loss: 2.251953363418579
Validation loss: 2.155963977177938

Epoch: 6| Step: 13
Training loss: 2.056474208831787
Validation loss: 2.140860835711161

Epoch: 179| Step: 0
Training loss: 2.604170322418213
Validation loss: 2.161473015944163

Epoch: 6| Step: 1
Training loss: 1.805391550064087
Validation loss: 2.1557767589886985

Epoch: 6| Step: 2
Training loss: 2.212283134460449
Validation loss: 2.138474444548289

Epoch: 6| Step: 3
Training loss: 1.261033058166504
Validation loss: 2.1539680560429892

Epoch: 6| Step: 4
Training loss: 1.549829125404358
Validation loss: 2.1527589559555054

Epoch: 6| Step: 5
Training loss: 2.11397123336792
Validation loss: 2.136482377847036

Epoch: 6| Step: 6
Training loss: 1.875357985496521
Validation loss: 2.121740182240804

Epoch: 6| Step: 7
Training loss: 1.2153539657592773
Validation loss: 2.1339900493621826

Epoch: 6| Step: 8
Training loss: 2.750624895095825
Validation loss: 2.124154806137085

Epoch: 6| Step: 9
Training loss: 1.8644540309906006
Validation loss: 2.126250227292379

Epoch: 6| Step: 10
Training loss: 1.6575593948364258
Validation loss: 2.122602383295695

Epoch: 6| Step: 11
Training loss: 2.2513837814331055
Validation loss: 2.1218894322713218

Epoch: 6| Step: 12
Training loss: 1.1783676147460938
Validation loss: 2.1261370380719504

Epoch: 6| Step: 13
Training loss: 2.212740898132324
Validation loss: 2.124750336011251

Epoch: 180| Step: 0
Training loss: 1.78072190284729
Validation loss: 2.13469535112381

Epoch: 6| Step: 1
Training loss: 1.8345067501068115
Validation loss: 2.1271808743476868

Epoch: 6| Step: 2
Training loss: 1.4785799980163574
Validation loss: 2.132574677467346

Epoch: 6| Step: 3
Training loss: 1.7159204483032227
Validation loss: 2.145020584265391

Epoch: 6| Step: 4
Training loss: 1.7851042747497559
Validation loss: 2.1443304419517517

Epoch: 6| Step: 5
Training loss: 1.6537485122680664
Validation loss: 2.163260261217753

Epoch: 6| Step: 6
Training loss: 1.5656458139419556
Validation loss: 2.1705819567044577

Epoch: 6| Step: 7
Training loss: 2.2853384017944336
Validation loss: 2.1647481520970664

Epoch: 6| Step: 8
Training loss: 2.199615001678467
Validation loss: 2.172150492668152

Epoch: 6| Step: 9
Training loss: 1.761521577835083
Validation loss: 2.167760888735453

Epoch: 6| Step: 10
Training loss: 1.7112997770309448
Validation loss: 2.1767756144205728

Epoch: 6| Step: 11
Training loss: 2.2426774501800537
Validation loss: 2.1843459606170654

Epoch: 6| Step: 12
Training loss: 2.1893973350524902
Validation loss: 2.1593532164891562

Epoch: 6| Step: 13
Training loss: 2.3924355506896973
Validation loss: 2.165762782096863

Epoch: 181| Step: 0
Training loss: 1.6106071472167969
Validation loss: 2.159820278485616

Epoch: 6| Step: 1
Training loss: 2.8345634937286377
Validation loss: 2.158524533112844

Epoch: 6| Step: 2
Training loss: 1.580067753791809
Validation loss: 2.171790679295858

Epoch: 6| Step: 3
Training loss: 2.0893161296844482
Validation loss: 2.1522051890691123

Epoch: 6| Step: 4
Training loss: 1.3159213066101074
Validation loss: 2.166845699151357

Epoch: 6| Step: 5
Training loss: 1.69525945186615
Validation loss: 2.15905108054479

Epoch: 6| Step: 6
Training loss: 2.138845920562744
Validation loss: 2.1837841272354126

Epoch: 6| Step: 7
Training loss: 2.27998685836792
Validation loss: 2.165294329325358

Epoch: 6| Step: 8
Training loss: 1.82047438621521
Validation loss: 2.176500757535299

Epoch: 6| Step: 9
Training loss: 2.0004186630249023
Validation loss: 2.1824185053507485

Epoch: 6| Step: 10
Training loss: 1.6499760150909424
Validation loss: 2.1676854689915976

Epoch: 6| Step: 11
Training loss: 1.352184772491455
Validation loss: 2.172556380430857

Epoch: 6| Step: 12
Training loss: 2.4175753593444824
Validation loss: 2.146388073762258

Epoch: 6| Step: 13
Training loss: 1.9971410036087036
Validation loss: 2.1428544322649636

Epoch: 182| Step: 0
Training loss: 2.1188371181488037
Validation loss: 2.1377591093381247

Epoch: 6| Step: 1
Training loss: 2.047647476196289
Validation loss: 2.12457005182902

Epoch: 6| Step: 2
Training loss: 2.114871025085449
Validation loss: 2.1264774203300476

Epoch: 6| Step: 3
Training loss: 1.9911346435546875
Validation loss: 2.1113475958506265

Epoch: 6| Step: 4
Training loss: 1.8948676586151123
Validation loss: 2.11318568388621

Epoch: 6| Step: 5
Training loss: 1.3534014225006104
Validation loss: 2.116475760936737

Epoch: 6| Step: 6
Training loss: 2.237548351287842
Validation loss: 2.125358462333679

Epoch: 6| Step: 7
Training loss: 1.6045942306518555
Validation loss: 2.15491114060084

Epoch: 6| Step: 8
Training loss: 1.6600570678710938
Validation loss: 2.1632410486539206

Epoch: 6| Step: 9
Training loss: 2.142787218093872
Validation loss: 2.1731799046198526

Epoch: 6| Step: 10
Training loss: 2.2690608501434326
Validation loss: 2.158717771371206

Epoch: 6| Step: 11
Training loss: 2.014016628265381
Validation loss: 2.1534528732299805

Epoch: 6| Step: 12
Training loss: 2.0904593467712402
Validation loss: 2.182532807191213

Epoch: 6| Step: 13
Training loss: 1.6804478168487549
Validation loss: 2.1719106833140054

Epoch: 183| Step: 0
Training loss: 1.5357446670532227
Validation loss: 2.1669880747795105

Epoch: 6| Step: 1
Training loss: 1.6005470752716064
Validation loss: 2.180293162663778

Epoch: 6| Step: 2
Training loss: 1.646071434020996
Validation loss: 2.180341879526774

Epoch: 6| Step: 3
Training loss: 2.015153408050537
Validation loss: 2.1825624306996665

Epoch: 6| Step: 4
Training loss: 1.5799736976623535
Validation loss: 2.175074736277262

Epoch: 6| Step: 5
Training loss: 1.4869142770767212
Validation loss: 2.1975468397140503

Epoch: 6| Step: 6
Training loss: 1.6895709037780762
Validation loss: 2.180342892805735

Epoch: 6| Step: 7
Training loss: 2.021639823913574
Validation loss: 2.1709886391957602

Epoch: 6| Step: 8
Training loss: 1.6760729551315308
Validation loss: 2.1928454637527466

Epoch: 6| Step: 9
Training loss: 2.493360996246338
Validation loss: 2.174398879210154

Epoch: 6| Step: 10
Training loss: 1.8240113258361816
Validation loss: 2.1585503220558167

Epoch: 6| Step: 11
Training loss: 2.1953318119049072
Validation loss: 2.1568581064542136

Epoch: 6| Step: 12
Training loss: 2.006028175354004
Validation loss: 2.1655638615290322

Epoch: 6| Step: 13
Training loss: 2.461313009262085
Validation loss: 2.1461859941482544

Epoch: 184| Step: 0
Training loss: 2.3404717445373535
Validation loss: 2.1579290429751077

Epoch: 6| Step: 1
Training loss: 2.0941433906555176
Validation loss: 2.158953766028086

Epoch: 6| Step: 2
Training loss: 2.163609504699707
Validation loss: 2.15814216931661

Epoch: 6| Step: 3
Training loss: 1.6930487155914307
Validation loss: 2.1644005378087363

Epoch: 6| Step: 4
Training loss: 2.4200167655944824
Validation loss: 2.155226012070974

Epoch: 6| Step: 5
Training loss: 1.786046028137207
Validation loss: 2.150709092617035

Epoch: 6| Step: 6
Training loss: 2.4816999435424805
Validation loss: 2.1660187443097434

Epoch: 6| Step: 7
Training loss: 2.0629959106445312
Validation loss: 2.152877946694692

Epoch: 6| Step: 8
Training loss: 1.8172156810760498
Validation loss: 2.1501903533935547

Epoch: 6| Step: 9
Training loss: 1.4751145839691162
Validation loss: 2.1814346313476562

Epoch: 6| Step: 10
Training loss: 1.2724429368972778
Validation loss: 2.159062226613363

Epoch: 6| Step: 11
Training loss: 1.8131252527236938
Validation loss: 2.176438570022583

Epoch: 6| Step: 12
Training loss: 1.2086129188537598
Validation loss: 2.1663014888763428

Epoch: 6| Step: 13
Training loss: 1.4174500703811646
Validation loss: 2.1676177183787027

Epoch: 185| Step: 0
Training loss: 2.130661964416504
Validation loss: 2.179951786994934

Epoch: 6| Step: 1
Training loss: 1.3294014930725098
Validation loss: 2.170113762219747

Epoch: 6| Step: 2
Training loss: 2.189119815826416
Validation loss: 2.16057695945104

Epoch: 6| Step: 3
Training loss: 2.214139223098755
Validation loss: 2.155477523803711

Epoch: 6| Step: 4
Training loss: 1.4509844779968262
Validation loss: 2.1539400021235147

Epoch: 6| Step: 5
Training loss: 1.1824153661727905
Validation loss: 2.1200455029805503

Epoch: 6| Step: 6
Training loss: 1.59944486618042
Validation loss: 2.1578062176704407

Epoch: 6| Step: 7
Training loss: 1.8754639625549316
Validation loss: 2.152405599753062

Epoch: 6| Step: 8
Training loss: 2.039152145385742
Validation loss: 2.16620926062266

Epoch: 6| Step: 9
Training loss: 1.9909507036209106
Validation loss: 2.1713176170984902

Epoch: 6| Step: 10
Training loss: 1.9599636793136597
Validation loss: 2.1828582882881165

Epoch: 6| Step: 11
Training loss: 2.0377919673919678
Validation loss: 2.1947456200917563

Epoch: 6| Step: 12
Training loss: 2.305173873901367
Validation loss: 2.2228084206581116

Epoch: 6| Step: 13
Training loss: 1.8586616516113281
Validation loss: 2.194455921649933

Epoch: 186| Step: 0
Training loss: 1.9612340927124023
Validation loss: 2.20578670501709

Epoch: 6| Step: 1
Training loss: 1.3760684728622437
Validation loss: 2.186680873235067

Epoch: 6| Step: 2
Training loss: 1.459518551826477
Validation loss: 2.174368758996328

Epoch: 6| Step: 3
Training loss: 1.5300439596176147
Validation loss: 2.1787761052449546

Epoch: 6| Step: 4
Training loss: 1.5983474254608154
Validation loss: 2.1919076641400657

Epoch: 6| Step: 5
Training loss: 1.5892637968063354
Validation loss: 2.180599272251129

Epoch: 6| Step: 6
Training loss: 2.4094741344451904
Validation loss: 2.1654930909474692

Epoch: 6| Step: 7
Training loss: 2.9171512126922607
Validation loss: 2.14786958694458

Epoch: 6| Step: 8
Training loss: 1.418686866760254
Validation loss: 2.1482279896736145

Epoch: 6| Step: 9
Training loss: 2.1677064895629883
Validation loss: 2.1370571851730347

Epoch: 6| Step: 10
Training loss: 1.4695987701416016
Validation loss: 2.1312768856684365

Epoch: 6| Step: 11
Training loss: 1.9982526302337646
Validation loss: 2.1325536966323853

Epoch: 6| Step: 12
Training loss: 1.8323919773101807
Validation loss: 2.1371888319651284

Epoch: 6| Step: 13
Training loss: 2.216165065765381
Validation loss: 2.138052523136139

Epoch: 187| Step: 0
Training loss: 1.9648644924163818
Validation loss: 2.1463814973831177

Epoch: 6| Step: 1
Training loss: 2.3000664710998535
Validation loss: 2.129172384738922

Epoch: 6| Step: 2
Training loss: 2.65440034866333
Validation loss: 2.137000580628713

Epoch: 6| Step: 3
Training loss: 2.0244011878967285
Validation loss: 2.139954129854838

Epoch: 6| Step: 4
Training loss: 1.2008662223815918
Validation loss: 2.137667457262675

Epoch: 6| Step: 5
Training loss: 1.8313745260238647
Validation loss: 2.1410561998685202

Epoch: 6| Step: 6
Training loss: 1.583451271057129
Validation loss: 2.1459991931915283

Epoch: 6| Step: 7
Training loss: 1.4637119770050049
Validation loss: 2.1426576375961304

Epoch: 6| Step: 8
Training loss: 1.9228204488754272
Validation loss: 2.158844550450643

Epoch: 6| Step: 9
Training loss: 1.6730916500091553
Validation loss: 2.1626211206118264

Epoch: 6| Step: 10
Training loss: 1.7434366941452026
Validation loss: 2.1678856213887534

Epoch: 6| Step: 11
Training loss: 2.3211004734039307
Validation loss: 2.167081038157145

Epoch: 6| Step: 12
Training loss: 2.0656144618988037
Validation loss: 2.180254558722178

Epoch: 6| Step: 13
Training loss: 1.3481488227844238
Validation loss: 2.1565611958503723

Epoch: 188| Step: 0
Training loss: 2.0822646617889404
Validation loss: 2.1532921393712363

Epoch: 6| Step: 1
Training loss: 2.0759506225585938
Validation loss: 2.17863796154658

Epoch: 6| Step: 2
Training loss: 1.6381042003631592
Validation loss: 2.165788690249125

Epoch: 6| Step: 3
Training loss: 1.4766712188720703
Validation loss: 2.1615373889605203

Epoch: 6| Step: 4
Training loss: 1.8480757474899292
Validation loss: 2.172486742337545

Epoch: 6| Step: 5
Training loss: 2.1670494079589844
Validation loss: 2.162945826848348

Epoch: 6| Step: 6
Training loss: 1.9650660753250122
Validation loss: 2.1657469272613525

Epoch: 6| Step: 7
Training loss: 2.0903804302215576
Validation loss: 2.1632284919420877

Epoch: 6| Step: 8
Training loss: 1.7636765241622925
Validation loss: 2.152824878692627

Epoch: 6| Step: 9
Training loss: 1.6801042556762695
Validation loss: 2.145310322443644

Epoch: 6| Step: 10
Training loss: 2.070070743560791
Validation loss: 2.1458101868629456

Epoch: 6| Step: 11
Training loss: 2.3393168449401855
Validation loss: 2.1287613113721213

Epoch: 6| Step: 12
Training loss: 1.4958257675170898
Validation loss: 2.128167470296224

Epoch: 6| Step: 13
Training loss: 2.3208389282226562
Validation loss: 2.129346191883087

Epoch: 189| Step: 0
Training loss: 1.7955516576766968
Validation loss: 2.1229854822158813

Epoch: 6| Step: 1
Training loss: 1.837285041809082
Validation loss: 2.1224377353986106

Epoch: 6| Step: 2
Training loss: 2.312840461730957
Validation loss: 2.1299004356066384

Epoch: 6| Step: 3
Training loss: 1.8379836082458496
Validation loss: 2.134496589501699

Epoch: 6| Step: 4
Training loss: 1.857729196548462
Validation loss: 2.1240671475728354

Epoch: 6| Step: 5
Training loss: 2.4240386486053467
Validation loss: 2.1260034640630088

Epoch: 6| Step: 6
Training loss: 2.6492254734039307
Validation loss: 2.126985947291056

Epoch: 6| Step: 7
Training loss: 1.9342390298843384
Validation loss: 2.1409976482391357

Epoch: 6| Step: 8
Training loss: 2.0983996391296387
Validation loss: 2.1472996274630227

Epoch: 6| Step: 9
Training loss: 1.2635046243667603
Validation loss: 2.1649667223294577

Epoch: 6| Step: 10
Training loss: 1.3405712842941284
Validation loss: 2.171079774697622

Epoch: 6| Step: 11
Training loss: 2.352499485015869
Validation loss: 2.1829366286595664

Epoch: 6| Step: 12
Training loss: 1.9197514057159424
Validation loss: 2.189465125401815

Epoch: 6| Step: 13
Training loss: 1.9948824644088745
Validation loss: 2.225168526172638

Epoch: 190| Step: 0
Training loss: 2.426888942718506
Validation loss: 2.216806491216024

Epoch: 6| Step: 1
Training loss: 1.7774572372436523
Validation loss: 2.210211972395579

Epoch: 6| Step: 2
Training loss: 2.5481042861938477
Validation loss: 2.2211198210716248

Epoch: 6| Step: 3
Training loss: 1.5851835012435913
Validation loss: 2.189800043900808

Epoch: 6| Step: 4
Training loss: 1.2576245069503784
Validation loss: 2.1851963996887207

Epoch: 6| Step: 5
Training loss: 1.7527329921722412
Validation loss: 2.1732994318008423

Epoch: 6| Step: 6
Training loss: 1.8122758865356445
Validation loss: 2.154026508331299

Epoch: 6| Step: 7
Training loss: 2.497610330581665
Validation loss: 2.158918341000875

Epoch: 6| Step: 8
Training loss: 2.02242374420166
Validation loss: 2.138377567132314

Epoch: 6| Step: 9
Training loss: 1.4895341396331787
Validation loss: 2.1477298736572266

Epoch: 6| Step: 10
Training loss: 2.5075697898864746
Validation loss: 2.130995591481527

Epoch: 6| Step: 11
Training loss: 1.5003705024719238
Validation loss: 2.1464648644129434

Epoch: 6| Step: 12
Training loss: 1.577919602394104
Validation loss: 2.1382705171902976

Epoch: 6| Step: 13
Training loss: 1.3678114414215088
Validation loss: 2.157189210255941

Epoch: 191| Step: 0
Training loss: 1.7665021419525146
Validation loss: 2.163733204205831

Epoch: 6| Step: 1
Training loss: 1.8868159055709839
Validation loss: 2.168219347794851

Epoch: 6| Step: 2
Training loss: 1.822431206703186
Validation loss: 2.1661728620529175

Epoch: 6| Step: 3
Training loss: 2.1609320640563965
Validation loss: 2.1778311928113303

Epoch: 6| Step: 4
Training loss: 1.213385820388794
Validation loss: 2.1958878437678018

Epoch: 6| Step: 5
Training loss: 1.646846055984497
Validation loss: 2.1999878883361816

Epoch: 6| Step: 6
Training loss: 1.675170660018921
Validation loss: 2.2150126894315085

Epoch: 6| Step: 7
Training loss: 3.0166664123535156
Validation loss: 2.230440139770508

Epoch: 6| Step: 8
Training loss: 1.5009956359863281
Validation loss: 2.219548543294271

Epoch: 6| Step: 9
Training loss: 1.4261164665222168
Validation loss: 2.199180781841278

Epoch: 6| Step: 10
Training loss: 1.7745170593261719
Validation loss: 2.2078653971354165

Epoch: 6| Step: 11
Training loss: 2.5420713424682617
Validation loss: 2.2079521814982095

Epoch: 6| Step: 12
Training loss: 1.7987985610961914
Validation loss: 2.20757265885671

Epoch: 6| Step: 13
Training loss: 1.8436110019683838
Validation loss: 2.182505249977112

Epoch: 192| Step: 0
Training loss: 1.6312988996505737
Validation loss: 2.1831402579943338

Epoch: 6| Step: 1
Training loss: 1.9321368932724
Validation loss: 2.1735040148099265

Epoch: 6| Step: 2
Training loss: 1.4001952409744263
Validation loss: 2.186245600382487

Epoch: 6| Step: 3
Training loss: 1.6867269277572632
Validation loss: 2.1821319659550986

Epoch: 6| Step: 4
Training loss: 1.7438169717788696
Validation loss: 2.1670748790105185

Epoch: 6| Step: 5
Training loss: 1.7717511653900146
Validation loss: 2.1662719448407493

Epoch: 6| Step: 6
Training loss: 2.9157872200012207
Validation loss: 2.1758174101511636

Epoch: 6| Step: 7
Training loss: 1.8492461442947388
Validation loss: 2.190218766530355

Epoch: 6| Step: 8
Training loss: 2.5414938926696777
Validation loss: 2.2053509950637817

Epoch: 6| Step: 9
Training loss: 1.8174164295196533
Validation loss: 2.2064938147862754

Epoch: 6| Step: 10
Training loss: 1.4637198448181152
Validation loss: 2.1939024527867637

Epoch: 6| Step: 11
Training loss: 1.6296857595443726
Validation loss: 2.2108364701271057

Epoch: 6| Step: 12
Training loss: 2.109912872314453
Validation loss: 2.195447564125061

Epoch: 6| Step: 13
Training loss: 1.6398413181304932
Validation loss: 2.2077057361602783

Epoch: 193| Step: 0
Training loss: 2.136209011077881
Validation loss: 2.2018150091171265

Epoch: 6| Step: 1
Training loss: 2.474257469177246
Validation loss: 2.2104646364847818

Epoch: 6| Step: 2
Training loss: 1.2412381172180176
Validation loss: 2.188369075457255

Epoch: 6| Step: 3
Training loss: 1.9942522048950195
Validation loss: 2.2023956576983132

Epoch: 6| Step: 4
Training loss: 1.4332385063171387
Validation loss: 2.1972150206565857

Epoch: 6| Step: 5
Training loss: 1.3085479736328125
Validation loss: 2.198795278867086

Epoch: 6| Step: 6
Training loss: 2.151550769805908
Validation loss: 2.1928614179293313

Epoch: 6| Step: 7
Training loss: 1.233785629272461
Validation loss: 2.19582591454188

Epoch: 6| Step: 8
Training loss: 1.1894862651824951
Validation loss: 2.178588330745697

Epoch: 6| Step: 9
Training loss: 2.563246726989746
Validation loss: 2.176702936490377

Epoch: 6| Step: 10
Training loss: 1.8858444690704346
Validation loss: 2.1954506238301597

Epoch: 6| Step: 11
Training loss: 2.593798875808716
Validation loss: 2.201706031958262

Epoch: 6| Step: 12
Training loss: 1.671514868736267
Validation loss: 2.187498907248179

Epoch: 6| Step: 13
Training loss: 2.028412103652954
Validation loss: 2.161881705125173

Epoch: 194| Step: 0
Training loss: 2.69836688041687
Validation loss: 2.1961658795674643

Epoch: 6| Step: 1
Training loss: 1.6018102169036865
Validation loss: 2.2023457487424216

Epoch: 6| Step: 2
Training loss: 1.605229139328003
Validation loss: 2.165884852409363

Epoch: 6| Step: 3
Training loss: 1.3795315027236938
Validation loss: 2.2017236749331155

Epoch: 6| Step: 4
Training loss: 2.637932300567627
Validation loss: 2.199572523434957

Epoch: 6| Step: 5
Training loss: 0.8490164279937744
Validation loss: 2.199156880378723

Epoch: 6| Step: 6
Training loss: 1.8614588975906372
Validation loss: 2.1942398150761924

Epoch: 6| Step: 7
Training loss: 2.2173469066619873
Validation loss: 2.2048075199127197

Epoch: 6| Step: 8
Training loss: 2.2489173412323
Validation loss: 2.185686409473419

Epoch: 6| Step: 9
Training loss: 1.5243724584579468
Validation loss: 2.185667932033539

Epoch: 6| Step: 10
Training loss: 1.441745400428772
Validation loss: 2.172806223233541

Epoch: 6| Step: 11
Training loss: 1.95076322555542
Validation loss: 2.15503199895223

Epoch: 6| Step: 12
Training loss: 1.374281406402588
Validation loss: 2.1792715191841125

Epoch: 6| Step: 13
Training loss: 2.4884886741638184
Validation loss: 2.156099816163381

Epoch: 195| Step: 0
Training loss: 2.2505319118499756
Validation loss: 2.181731879711151

Epoch: 6| Step: 1
Training loss: 1.8844578266143799
Validation loss: 2.1617591381073

Epoch: 6| Step: 2
Training loss: 2.2433202266693115
Validation loss: 2.172296643257141

Epoch: 6| Step: 3
Training loss: 1.419175386428833
Validation loss: 2.15381129582723

Epoch: 6| Step: 4
Training loss: 1.8797664642333984
Validation loss: 2.1543116172154746

Epoch: 6| Step: 5
Training loss: 1.2332310676574707
Validation loss: 2.1688397924105325

Epoch: 6| Step: 6
Training loss: 1.9914741516113281
Validation loss: 2.169293761253357

Epoch: 6| Step: 7
Training loss: 1.3168385028839111
Validation loss: 2.161710739135742

Epoch: 6| Step: 8
Training loss: 1.5549299716949463
Validation loss: 2.1805957158406577

Epoch: 6| Step: 9
Training loss: 1.8078449964523315
Validation loss: 2.186178425947825

Epoch: 6| Step: 10
Training loss: 2.4027814865112305
Validation loss: 2.223758081595103

Epoch: 6| Step: 11
Training loss: 2.2976996898651123
Validation loss: 2.2511265675226846

Epoch: 6| Step: 12
Training loss: 2.3812484741210938
Validation loss: 2.2224915822347007

Epoch: 6| Step: 13
Training loss: 1.9227302074432373
Validation loss: 2.2148425380388894

Epoch: 196| Step: 0
Training loss: 2.2259364128112793
Validation loss: 2.197470486164093

Epoch: 6| Step: 1
Training loss: 1.7699878215789795
Validation loss: 2.183817724386851

Epoch: 6| Step: 2
Training loss: 1.7461642026901245
Validation loss: 2.158997813860575

Epoch: 6| Step: 3
Training loss: 1.8060016632080078
Validation loss: 2.1703070600827536

Epoch: 6| Step: 4
Training loss: 2.0792529582977295
Validation loss: 2.147179067134857

Epoch: 6| Step: 5
Training loss: 1.3360865116119385
Validation loss: 2.14731627702713

Epoch: 6| Step: 6
Training loss: 1.2943658828735352
Validation loss: 2.1476678252220154

Epoch: 6| Step: 7
Training loss: 2.741858720779419
Validation loss: 2.140729467074076

Epoch: 6| Step: 8
Training loss: 1.9733092784881592
Validation loss: 2.137868026892344

Epoch: 6| Step: 9
Training loss: 1.5449378490447998
Validation loss: 2.131512761116028

Epoch: 6| Step: 10
Training loss: 1.6326181888580322
Validation loss: 2.145435074965159

Epoch: 6| Step: 11
Training loss: 1.8353554010391235
Validation loss: 2.1643699606259665

Epoch: 6| Step: 12
Training loss: 2.080246925354004
Validation loss: 2.1539148092269897

Epoch: 6| Step: 13
Training loss: 2.097745895385742
Validation loss: 2.1549389362335205

Epoch: 197| Step: 0
Training loss: 1.852705717086792
Validation loss: 2.1546079913775125

Epoch: 6| Step: 1
Training loss: 2.092773914337158
Validation loss: 2.164322078227997

Epoch: 6| Step: 2
Training loss: 1.6941641569137573
Validation loss: 2.1630463202794394

Epoch: 6| Step: 3
Training loss: 1.4640896320343018
Validation loss: 2.1696051359176636

Epoch: 6| Step: 4
Training loss: 1.5024865865707397
Validation loss: 2.182643175125122

Epoch: 6| Step: 5
Training loss: 2.8544273376464844
Validation loss: 2.1840361754099527

Epoch: 6| Step: 6
Training loss: 1.5606752634048462
Validation loss: 2.1756174564361572

Epoch: 6| Step: 7
Training loss: 1.9973499774932861
Validation loss: 2.197001894315084

Epoch: 6| Step: 8
Training loss: 1.9252372980117798
Validation loss: 2.2013893922170005

Epoch: 6| Step: 9
Training loss: 1.5023791790008545
Validation loss: 2.202970286210378

Epoch: 6| Step: 10
Training loss: 1.7336666584014893
Validation loss: 2.186404228210449

Epoch: 6| Step: 11
Training loss: 2.595573902130127
Validation loss: 2.2064956227938333

Epoch: 6| Step: 12
Training loss: 1.0587077140808105
Validation loss: 2.199639876683553

Epoch: 6| Step: 13
Training loss: 1.7964028120040894
Validation loss: 2.1941243012746177

Epoch: 198| Step: 0
Training loss: 2.1053431034088135
Validation loss: 2.199112296104431

Epoch: 6| Step: 1
Training loss: 1.9030224084854126
Validation loss: 2.1999889810880027

Epoch: 6| Step: 2
Training loss: 1.9400285482406616
Validation loss: 2.218923807144165

Epoch: 6| Step: 3
Training loss: 1.7573542594909668
Validation loss: 2.195268710454305

Epoch: 6| Step: 4
Training loss: 1.3928332328796387
Validation loss: 2.1927260359128318

Epoch: 6| Step: 5
Training loss: 1.405084252357483
Validation loss: 2.183540463447571

Epoch: 6| Step: 6
Training loss: 1.958298683166504
Validation loss: 2.1804781953493753

Epoch: 6| Step: 7
Training loss: 1.9210458993911743
Validation loss: 2.194624920686086

Epoch: 6| Step: 8
Training loss: 1.7169103622436523
Validation loss: 2.18552698691686

Epoch: 6| Step: 9
Training loss: 1.3328776359558105
Validation loss: 2.1696689128875732

Epoch: 6| Step: 10
Training loss: 2.104090690612793
Validation loss: 2.164987564086914

Epoch: 6| Step: 11
Training loss: 1.702233910560608
Validation loss: 2.135994017124176

Epoch: 6| Step: 12
Training loss: 2.058743476867676
Validation loss: 2.1464579105377197

Epoch: 6| Step: 13
Training loss: 2.803798198699951
Validation loss: 2.130127469698588

Epoch: 199| Step: 0
Training loss: 1.570582389831543
Validation loss: 2.1465499798456826

Epoch: 6| Step: 1
Training loss: 1.4409719705581665
Validation loss: 2.134040137132009

Epoch: 6| Step: 2
Training loss: 2.091731071472168
Validation loss: 2.135508139928182

Epoch: 6| Step: 3
Training loss: 2.4044041633605957
Validation loss: 2.1524603962898254

Epoch: 6| Step: 4
Training loss: 1.7980639934539795
Validation loss: 2.1539812882741294

Epoch: 6| Step: 5
Training loss: 1.8709852695465088
Validation loss: 2.1669058203697205

Epoch: 6| Step: 6
Training loss: 1.9986826181411743
Validation loss: 2.1711981892585754

Epoch: 6| Step: 7
Training loss: 1.5236773490905762
Validation loss: 2.1649951338768005

Epoch: 6| Step: 8
Training loss: 1.8457276821136475
Validation loss: 2.1787018179893494

Epoch: 6| Step: 9
Training loss: 1.8384113311767578
Validation loss: 2.1636511286099753

Epoch: 6| Step: 10
Training loss: 1.7934701442718506
Validation loss: 2.1897570292154946

Epoch: 6| Step: 11
Training loss: 1.7981500625610352
Validation loss: 2.2022714416186013

Epoch: 6| Step: 12
Training loss: 1.7271344661712646
Validation loss: 2.2056285540262857

Epoch: 6| Step: 13
Training loss: 1.9550284147262573
Validation loss: 2.1958553989728293

Epoch: 200| Step: 0
Training loss: 1.8023998737335205
Validation loss: 2.183789292971293

Epoch: 6| Step: 1
Training loss: 2.418726921081543
Validation loss: 2.190526247024536

Epoch: 6| Step: 2
Training loss: 1.142439842224121
Validation loss: 2.1958371798197427

Epoch: 6| Step: 3
Training loss: 1.7966670989990234
Validation loss: 2.1889750162760415

Epoch: 6| Step: 4
Training loss: 1.6691510677337646
Validation loss: 2.1898136734962463

Epoch: 6| Step: 5
Training loss: 2.221221446990967
Validation loss: 2.183882474899292

Epoch: 6| Step: 6
Training loss: 1.5088391304016113
Validation loss: 2.186998724937439

Epoch: 6| Step: 7
Training loss: 1.2182738780975342
Validation loss: 2.1988779306411743

Epoch: 6| Step: 8
Training loss: 1.6223723888397217
Validation loss: 2.1791740457216897

Epoch: 6| Step: 9
Training loss: 2.201911449432373
Validation loss: 2.1962861021359763

Epoch: 6| Step: 10
Training loss: 2.2702174186706543
Validation loss: 2.214044729868571

Epoch: 6| Step: 11
Training loss: 1.599905014038086
Validation loss: 2.203214963277181

Epoch: 6| Step: 12
Training loss: 1.826292872428894
Validation loss: 2.2185348073641458

Epoch: 6| Step: 13
Training loss: 1.9442822933197021
Validation loss: 2.2132806976636252

Epoch: 201| Step: 0
Training loss: 1.6891363859176636
Validation loss: 2.1993228793144226

Epoch: 6| Step: 1
Training loss: 1.8398101329803467
Validation loss: 2.2322537501653037

Epoch: 6| Step: 2
Training loss: 2.179245948791504
Validation loss: 2.22559521595637

Epoch: 6| Step: 3
Training loss: 1.906205415725708
Validation loss: 2.228794197241465

Epoch: 6| Step: 4
Training loss: 1.67527437210083
Validation loss: 2.2066641251246133

Epoch: 6| Step: 5
Training loss: 2.187904119491577
Validation loss: 2.220641851425171

Epoch: 6| Step: 6
Training loss: 1.6239378452301025
Validation loss: 2.212896148363749

Epoch: 6| Step: 7
Training loss: 1.761893630027771
Validation loss: 2.217991511027018

Epoch: 6| Step: 8
Training loss: 2.0092554092407227
Validation loss: 2.225372056166331

Epoch: 6| Step: 9
Training loss: 1.1439876556396484
Validation loss: 2.206191102663676

Epoch: 6| Step: 10
Training loss: 2.358213424682617
Validation loss: 2.2043702801068625

Epoch: 6| Step: 11
Training loss: 1.6997700929641724
Validation loss: 2.1878068447113037

Epoch: 6| Step: 12
Training loss: 1.591398000717163
Validation loss: 2.177113970120748

Epoch: 6| Step: 13
Training loss: 1.5463392734527588
Validation loss: 2.1866561571756997

Epoch: 202| Step: 0
Training loss: 2.2608511447906494
Validation loss: 2.1880188981691995

Epoch: 6| Step: 1
Training loss: 1.5096979141235352
Validation loss: 2.1612866520881653

Epoch: 6| Step: 2
Training loss: 2.2593727111816406
Validation loss: 2.1782114505767822

Epoch: 6| Step: 3
Training loss: 1.8990416526794434
Validation loss: 2.203265289465586

Epoch: 6| Step: 4
Training loss: 1.3257861137390137
Validation loss: 2.172222375869751

Epoch: 6| Step: 5
Training loss: 2.0858092308044434
Validation loss: 2.2048100233078003

Epoch: 6| Step: 6
Training loss: 2.201266288757324
Validation loss: 2.190890451272329

Epoch: 6| Step: 7
Training loss: 1.4022548198699951
Validation loss: 2.219809134801229

Epoch: 6| Step: 8
Training loss: 1.2944878339767456
Validation loss: 2.206811487674713

Epoch: 6| Step: 9
Training loss: 1.3193334341049194
Validation loss: 2.240948259830475

Epoch: 6| Step: 10
Training loss: 1.987165093421936
Validation loss: 2.226350208123525

Epoch: 6| Step: 11
Training loss: 1.5968637466430664
Validation loss: 2.2198042472203574

Epoch: 6| Step: 12
Training loss: 1.7749834060668945
Validation loss: 2.21921843290329

Epoch: 6| Step: 13
Training loss: 2.4471116065979004
Validation loss: 2.218609849611918

Epoch: 203| Step: 0
Training loss: 2.596803665161133
Validation loss: 2.190788507461548

Epoch: 6| Step: 1
Training loss: 1.7594752311706543
Validation loss: 2.2228381435076394

Epoch: 6| Step: 2
Training loss: 1.7639038562774658
Validation loss: 2.2399311463038125

Epoch: 6| Step: 3
Training loss: 1.6668884754180908
Validation loss: 2.232169508934021

Epoch: 6| Step: 4
Training loss: 2.0192384719848633
Validation loss: 2.2040454546610513

Epoch: 6| Step: 5
Training loss: 2.019709348678589
Validation loss: 2.2055665850639343

Epoch: 6| Step: 6
Training loss: 1.9185080528259277
Validation loss: 2.1937732696533203

Epoch: 6| Step: 7
Training loss: 2.066248893737793
Validation loss: 2.1891133189201355

Epoch: 6| Step: 8
Training loss: 1.3696238994598389
Validation loss: 2.198451340198517

Epoch: 6| Step: 9
Training loss: 1.603583574295044
Validation loss: 2.1993457476298013

Epoch: 6| Step: 10
Training loss: 1.4344714879989624
Validation loss: 2.1869440277417502

Epoch: 6| Step: 11
Training loss: 1.1098687648773193
Validation loss: 2.2089772820472717

Epoch: 6| Step: 12
Training loss: 2.2076058387756348
Validation loss: 2.2295437256495156

Epoch: 6| Step: 13
Training loss: 1.5864464044570923
Validation loss: 2.2185142040252686

Epoch: 204| Step: 0
Training loss: 1.0325299501419067
Validation loss: 2.213004231452942

Epoch: 6| Step: 1
Training loss: 1.6042134761810303
Validation loss: 2.2219002644220986

Epoch: 6| Step: 2
Training loss: 2.0643067359924316
Validation loss: 2.197104752063751

Epoch: 6| Step: 3
Training loss: 1.7564066648483276
Validation loss: 2.2013825376828513

Epoch: 6| Step: 4
Training loss: 1.5227031707763672
Validation loss: 2.1948827306429544

Epoch: 6| Step: 5
Training loss: 2.8679165840148926
Validation loss: 2.170491794745127

Epoch: 6| Step: 6
Training loss: 2.079028606414795
Validation loss: 2.162866731484731

Epoch: 6| Step: 7
Training loss: 1.3894397020339966
Validation loss: 2.143186012903849

Epoch: 6| Step: 8
Training loss: 2.3567004203796387
Validation loss: 2.134847621122996

Epoch: 6| Step: 9
Training loss: 2.20965576171875
Validation loss: 2.166530728340149

Epoch: 6| Step: 10
Training loss: 1.492006540298462
Validation loss: 2.1696194807688394

Epoch: 6| Step: 11
Training loss: 1.5144567489624023
Validation loss: 2.1847945849100747

Epoch: 6| Step: 12
Training loss: 1.8951797485351562
Validation loss: 2.185942987600962

Epoch: 6| Step: 13
Training loss: 2.245732069015503
Validation loss: 2.181993246078491

Epoch: 205| Step: 0
Training loss: 1.6502008438110352
Validation loss: 2.195173521836599

Epoch: 6| Step: 1
Training loss: 2.013040542602539
Validation loss: 2.189803640047709

Epoch: 6| Step: 2
Training loss: 2.3342456817626953
Validation loss: 2.18976358572642

Epoch: 6| Step: 3
Training loss: 1.660224437713623
Validation loss: 2.223732511202494

Epoch: 6| Step: 4
Training loss: 1.6293392181396484
Validation loss: 2.2035211324691772

Epoch: 6| Step: 5
Training loss: 1.7953273057937622
Validation loss: 2.2188917795817056

Epoch: 6| Step: 6
Training loss: 2.0914957523345947
Validation loss: 2.2070390383402505

Epoch: 6| Step: 7
Training loss: 2.0947184562683105
Validation loss: 2.211504499117533

Epoch: 6| Step: 8
Training loss: 1.7292847633361816
Validation loss: 2.1979216933250427

Epoch: 6| Step: 9
Training loss: 1.8653994798660278
Validation loss: 2.193606992562612

Epoch: 6| Step: 10
Training loss: 1.8737387657165527
Validation loss: 2.190945029258728

Epoch: 6| Step: 11
Training loss: 1.4710314273834229
Validation loss: 2.194726586341858

Epoch: 6| Step: 12
Training loss: 1.5865122079849243
Validation loss: 2.2065957387288413

Epoch: 6| Step: 13
Training loss: 1.9907240867614746
Validation loss: 2.1777477264404297

Epoch: 206| Step: 0
Training loss: 1.134941816329956
Validation loss: 2.1763788859049478

Epoch: 6| Step: 1
Training loss: 1.6817669868469238
Validation loss: 2.1807010173797607

Epoch: 6| Step: 2
Training loss: 1.4563021659851074
Validation loss: 2.187543491522471

Epoch: 6| Step: 3
Training loss: 1.1367827653884888
Validation loss: 2.2045292258262634

Epoch: 6| Step: 4
Training loss: 2.4121651649475098
Validation loss: 2.2136968970298767

Epoch: 6| Step: 5
Training loss: 1.8225173950195312
Validation loss: 2.2150747179985046

Epoch: 6| Step: 6
Training loss: 1.9212946891784668
Validation loss: 2.2227195501327515

Epoch: 6| Step: 7
Training loss: 1.3640155792236328
Validation loss: 2.204811374346415

Epoch: 6| Step: 8
Training loss: 2.0601210594177246
Validation loss: 2.217062075932821

Epoch: 6| Step: 9
Training loss: 2.254868507385254
Validation loss: 2.2165035406748452

Epoch: 6| Step: 10
Training loss: 1.4857391119003296
Validation loss: 2.2391600012779236

Epoch: 6| Step: 11
Training loss: 2.0446338653564453
Validation loss: 2.214474360148112

Epoch: 6| Step: 12
Training loss: 1.8201996088027954
Validation loss: 2.2233490546544394

Epoch: 6| Step: 13
Training loss: 2.627164840698242
Validation loss: 2.2271414399147034

Epoch: 207| Step: 0
Training loss: 1.802025318145752
Validation loss: 2.2022563219070435

Epoch: 6| Step: 1
Training loss: 1.924773097038269
Validation loss: 2.223963717619578

Epoch: 6| Step: 2
Training loss: 2.44777250289917
Validation loss: 2.2108105619748435

Epoch: 6| Step: 3
Training loss: 1.8204081058502197
Validation loss: 2.214834670225779

Epoch: 6| Step: 4
Training loss: 2.0638580322265625
Validation loss: 2.2254839738210044

Epoch: 6| Step: 5
Training loss: 0.9371855854988098
Validation loss: 2.221005400021871

Epoch: 6| Step: 6
Training loss: 1.9589154720306396
Validation loss: 2.2305477062861123

Epoch: 6| Step: 7
Training loss: 2.22860050201416
Validation loss: 2.2106500466664634

Epoch: 6| Step: 8
Training loss: 1.3178110122680664
Validation loss: 2.2171464959780374

Epoch: 6| Step: 9
Training loss: 2.150188446044922
Validation loss: 2.2123019099235535

Epoch: 6| Step: 10
Training loss: 1.5507675409317017
Validation loss: 2.207922716935476

Epoch: 6| Step: 11
Training loss: 1.4179174900054932
Validation loss: 2.2075507640838623

Epoch: 6| Step: 12
Training loss: 1.764344334602356
Validation loss: 2.1946767767270408

Epoch: 6| Step: 13
Training loss: 1.5581704378128052
Validation loss: 2.201084097226461

Epoch: 208| Step: 0
Training loss: 2.3737807273864746
Validation loss: 2.19379460811615

Epoch: 6| Step: 1
Training loss: 2.1177611351013184
Validation loss: 2.1862226128578186

Epoch: 6| Step: 2
Training loss: 2.2293190956115723
Validation loss: 2.2014763752619424

Epoch: 6| Step: 3
Training loss: 1.6030327081680298
Validation loss: 2.227872669696808

Epoch: 6| Step: 4
Training loss: 1.5879795551300049
Validation loss: 2.221209486325582

Epoch: 6| Step: 5
Training loss: 1.8349920511245728
Validation loss: 2.1955566008885703

Epoch: 6| Step: 6
Training loss: 1.6599886417388916
Validation loss: 2.198384940624237

Epoch: 6| Step: 7
Training loss: 1.6198946237564087
Validation loss: 2.199397643407186

Epoch: 6| Step: 8
Training loss: 1.489795207977295
Validation loss: 2.208945711453756

Epoch: 6| Step: 9
Training loss: 1.4762762784957886
Validation loss: 2.204171101252238

Epoch: 6| Step: 10
Training loss: 1.9722542762756348
Validation loss: 2.1987620989481607

Epoch: 6| Step: 11
Training loss: 1.4687578678131104
Validation loss: 2.2089903354644775

Epoch: 6| Step: 12
Training loss: 1.9215284585952759
Validation loss: 2.207770307858785

Epoch: 6| Step: 13
Training loss: 1.3383845090866089
Validation loss: 2.2037232716878257

Epoch: 209| Step: 0
Training loss: 1.582106351852417
Validation loss: 2.204290270805359

Epoch: 6| Step: 1
Training loss: 1.8675692081451416
Validation loss: 2.236365874608358

Epoch: 6| Step: 2
Training loss: 1.3066809177398682
Validation loss: 2.2180190881093345

Epoch: 6| Step: 3
Training loss: 2.0569050312042236
Validation loss: 2.2232594887415567

Epoch: 6| Step: 4
Training loss: 2.312913417816162
Validation loss: 2.211388568083445

Epoch: 6| Step: 5
Training loss: 1.6020742654800415
Validation loss: 2.2272100845972695

Epoch: 6| Step: 6
Training loss: 2.1001391410827637
Validation loss: 2.2476910948753357

Epoch: 6| Step: 7
Training loss: 1.6663281917572021
Validation loss: 2.233915686607361

Epoch: 6| Step: 8
Training loss: 1.6460165977478027
Validation loss: 2.21466201543808

Epoch: 6| Step: 9
Training loss: 1.944108486175537
Validation loss: 2.221026301383972

Epoch: 6| Step: 10
Training loss: 1.26389479637146
Validation loss: 2.2152685125668845

Epoch: 6| Step: 11
Training loss: 1.812677264213562
Validation loss: 2.2189706762631736

Epoch: 6| Step: 12
Training loss: 1.687217116355896
Validation loss: 2.195577541987101

Epoch: 6| Step: 13
Training loss: 2.1369454860687256
Validation loss: 2.1820810238520303

Epoch: 210| Step: 0
Training loss: 1.6324936151504517
Validation loss: 2.162503639856974

Epoch: 6| Step: 1
Training loss: 1.8576078414916992
Validation loss: 2.1453348994255066

Epoch: 6| Step: 2
Training loss: 1.7288750410079956
Validation loss: 2.130069394906362

Epoch: 6| Step: 3
Training loss: 2.879049777984619
Validation loss: 2.138083815574646

Epoch: 6| Step: 4
Training loss: 1.4417908191680908
Validation loss: 2.1577121218045554

Epoch: 6| Step: 5
Training loss: 2.4218156337738037
Validation loss: 2.1481528878211975

Epoch: 6| Step: 6
Training loss: 1.8214282989501953
Validation loss: 2.156281570593516

Epoch: 6| Step: 7
Training loss: 1.684702754020691
Validation loss: 2.1844149629275003

Epoch: 6| Step: 8
Training loss: 1.9913880825042725
Validation loss: 2.198824723561605

Epoch: 6| Step: 9
Training loss: 1.3245980739593506
Validation loss: 2.2063975731531777

Epoch: 6| Step: 10
Training loss: 1.6959481239318848
Validation loss: 2.222847898801168

Epoch: 6| Step: 11
Training loss: 1.4805312156677246
Validation loss: 2.1949390371640525

Epoch: 6| Step: 12
Training loss: 2.1552529335021973
Validation loss: 2.218011955420176

Epoch: 6| Step: 13
Training loss: 1.812170147895813
Validation loss: 2.2374384005864463

Epoch: 211| Step: 0
Training loss: 1.3529236316680908
Validation loss: 2.2461735804875693

Epoch: 6| Step: 1
Training loss: 2.7845282554626465
Validation loss: 2.2198596795399985

Epoch: 6| Step: 2
Training loss: 1.1324079036712646
Validation loss: 2.1963486671447754

Epoch: 6| Step: 3
Training loss: 1.9853100776672363
Validation loss: 2.2245089014371238

Epoch: 6| Step: 4
Training loss: 2.597362995147705
Validation loss: 2.2011929551760354

Epoch: 6| Step: 5
Training loss: 1.599732756614685
Validation loss: 2.1992544333140054

Epoch: 6| Step: 6
Training loss: 1.8470590114593506
Validation loss: 2.2062664230664573

Epoch: 6| Step: 7
Training loss: 1.3623212575912476
Validation loss: 2.2156450152397156

Epoch: 6| Step: 8
Training loss: 1.6612026691436768
Validation loss: 2.226820707321167

Epoch: 6| Step: 9
Training loss: 2.0602993965148926
Validation loss: 2.197260101636251

Epoch: 6| Step: 10
Training loss: 0.8946192264556885
Validation loss: 2.197763125101725

Epoch: 6| Step: 11
Training loss: 1.7601637840270996
Validation loss: 2.1940150459607444

Epoch: 6| Step: 12
Training loss: 1.964195966720581
Validation loss: 2.2121671438217163

Epoch: 6| Step: 13
Training loss: 1.722865343093872
Validation loss: 2.2004550894101462

Epoch: 212| Step: 0
Training loss: 1.8939120769500732
Validation loss: 2.1932348012924194

Epoch: 6| Step: 1
Training loss: 1.7697474956512451
Validation loss: 2.2081218163172402

Epoch: 6| Step: 2
Training loss: 1.461127519607544
Validation loss: 2.1953934828440347

Epoch: 6| Step: 3
Training loss: 1.582133173942566
Validation loss: 2.21156640847524

Epoch: 6| Step: 4
Training loss: 1.8601922988891602
Validation loss: 2.218740542729696

Epoch: 6| Step: 5
Training loss: 1.4635957479476929
Validation loss: 2.184107263882955

Epoch: 6| Step: 6
Training loss: 1.9439671039581299
Validation loss: 2.206722855567932

Epoch: 6| Step: 7
Training loss: 1.309333324432373
Validation loss: 2.1802438497543335

Epoch: 6| Step: 8
Training loss: 1.764562726020813
Validation loss: 2.197502692540487

Epoch: 6| Step: 9
Training loss: 1.916771411895752
Validation loss: 2.196293850739797

Epoch: 6| Step: 10
Training loss: 1.8401691913604736
Validation loss: 2.2119259436925254

Epoch: 6| Step: 11
Training loss: 2.701695680618286
Validation loss: 2.1918692588806152

Epoch: 6| Step: 12
Training loss: 1.604811668395996
Validation loss: 2.204101860523224

Epoch: 6| Step: 13
Training loss: 1.9229363203048706
Validation loss: 2.2119784553845725

Epoch: 213| Step: 0
Training loss: 1.7582721710205078
Validation loss: 2.2052066723505654

Epoch: 6| Step: 1
Training loss: 1.7195394039154053
Validation loss: 2.1878350377082825

Epoch: 6| Step: 2
Training loss: 1.6123220920562744
Validation loss: 2.2003493110338845

Epoch: 6| Step: 3
Training loss: 2.2176272869110107
Validation loss: 2.1933030088742576

Epoch: 6| Step: 4
Training loss: 2.0318174362182617
Validation loss: 2.202590028444926

Epoch: 6| Step: 5
Training loss: 2.1805100440979004
Validation loss: 2.2193015416463218

Epoch: 6| Step: 6
Training loss: 1.6446702480316162
Validation loss: 2.1955883701642356

Epoch: 6| Step: 7
Training loss: 1.590330719947815
Validation loss: 2.2140922943751016

Epoch: 6| Step: 8
Training loss: 1.6928465366363525
Validation loss: 2.221331854661306

Epoch: 6| Step: 9
Training loss: 1.7083539962768555
Validation loss: 2.2307234009106955

Epoch: 6| Step: 10
Training loss: 1.7339730262756348
Validation loss: 2.2084279457728067

Epoch: 6| Step: 11
Training loss: 2.154338836669922
Validation loss: 2.2402109106381736

Epoch: 6| Step: 12
Training loss: 1.3709783554077148
Validation loss: 2.2296119928359985

Epoch: 6| Step: 13
Training loss: 1.1132326126098633
Validation loss: 2.228284994761149

Epoch: 214| Step: 0
Training loss: 1.5072617530822754
Validation loss: 2.2339912056922913

Epoch: 6| Step: 1
Training loss: 1.537988543510437
Validation loss: 2.2280555168787637

Epoch: 6| Step: 2
Training loss: 2.145763397216797
Validation loss: 2.21985924243927

Epoch: 6| Step: 3
Training loss: 1.5647114515304565
Validation loss: 2.1986766258875527

Epoch: 6| Step: 4
Training loss: 2.0625200271606445
Validation loss: 2.202379127343496

Epoch: 6| Step: 5
Training loss: 1.1960680484771729
Validation loss: 2.193998912970225

Epoch: 6| Step: 6
Training loss: 2.585397720336914
Validation loss: 2.179200748602549

Epoch: 6| Step: 7
Training loss: 1.7430917024612427
Validation loss: 2.176557501157125

Epoch: 6| Step: 8
Training loss: 1.8748576641082764
Validation loss: 2.180353581905365

Epoch: 6| Step: 9
Training loss: 1.2283086776733398
Validation loss: 2.206949293613434

Epoch: 6| Step: 10
Training loss: 2.015214204788208
Validation loss: 2.196066896120707

Epoch: 6| Step: 11
Training loss: 1.5921046733856201
Validation loss: 2.2355819741884866

Epoch: 6| Step: 12
Training loss: 1.7703306674957275
Validation loss: 2.2195806900660195

Epoch: 6| Step: 13
Training loss: 1.8685224056243896
Validation loss: 2.2243860562642417

Epoch: 215| Step: 0
Training loss: 2.4762039184570312
Validation loss: 2.2364912827809653

Epoch: 6| Step: 1
Training loss: 2.022425413131714
Validation loss: 2.2381295760472617

Epoch: 6| Step: 2
Training loss: 1.500687599182129
Validation loss: 2.2355271776517234

Epoch: 6| Step: 3
Training loss: 2.3867270946502686
Validation loss: 2.2284823656082153

Epoch: 6| Step: 4
Training loss: 1.2265734672546387
Validation loss: 2.2105122009913125

Epoch: 6| Step: 5
Training loss: 1.870140552520752
Validation loss: 2.2318992217381797

Epoch: 6| Step: 6
Training loss: 2.624363422393799
Validation loss: 2.2261061668395996

Epoch: 6| Step: 7
Training loss: 1.4744597673416138
Validation loss: 2.2341814835866294

Epoch: 6| Step: 8
Training loss: 1.4652044773101807
Validation loss: 2.231153945128123

Epoch: 6| Step: 9
Training loss: 1.6397045850753784
Validation loss: 2.2153817216555276

Epoch: 6| Step: 10
Training loss: 1.2377896308898926
Validation loss: 2.1992633740107217

Epoch: 6| Step: 11
Training loss: 1.4620275497436523
Validation loss: 2.2104385097821555

Epoch: 6| Step: 12
Training loss: 1.533987045288086
Validation loss: 2.1984403928120932

Epoch: 6| Step: 13
Training loss: 1.7061710357666016
Validation loss: 2.2041703263918557

Epoch: 216| Step: 0
Training loss: 2.4530444145202637
Validation loss: 2.2100401520729065

Epoch: 6| Step: 1
Training loss: 2.1727776527404785
Validation loss: 2.2085081934928894

Epoch: 6| Step: 2
Training loss: 1.2251322269439697
Validation loss: 2.1927562952041626

Epoch: 6| Step: 3
Training loss: 2.2256720066070557
Validation loss: 2.2171132564544678

Epoch: 6| Step: 4
Training loss: 1.7635185718536377
Validation loss: 2.21579380830129

Epoch: 6| Step: 5
Training loss: 1.2392443418502808
Validation loss: 2.2029837171236673

Epoch: 6| Step: 6
Training loss: 1.71174955368042
Validation loss: 2.1961734890937805

Epoch: 6| Step: 7
Training loss: 2.423736095428467
Validation loss: 2.197072903315226

Epoch: 6| Step: 8
Training loss: 1.370842695236206
Validation loss: 2.2081392804781594

Epoch: 6| Step: 9
Training loss: 1.5688049793243408
Validation loss: 2.194935917854309

Epoch: 6| Step: 10
Training loss: 1.5533446073532104
Validation loss: 2.192997992038727

Epoch: 6| Step: 11
Training loss: 1.6023653745651245
Validation loss: 2.187252918879191

Epoch: 6| Step: 12
Training loss: 1.8200926780700684
Validation loss: 2.172755499680837

Epoch: 6| Step: 13
Training loss: 1.6135830879211426
Validation loss: 2.1719905932744346

Epoch: 217| Step: 0
Training loss: 1.2262496948242188
Validation loss: 2.2131831844647727

Epoch: 6| Step: 1
Training loss: 1.948344349861145
Validation loss: 2.194060186545054

Epoch: 6| Step: 2
Training loss: 1.9370434284210205
Validation loss: 2.1783690452575684

Epoch: 6| Step: 3
Training loss: 1.3998568058013916
Validation loss: 2.1993378400802612

Epoch: 6| Step: 4
Training loss: 2.5874319076538086
Validation loss: 2.2224457462628684

Epoch: 6| Step: 5
Training loss: 2.303161382675171
Validation loss: 2.211720108985901

Epoch: 6| Step: 6
Training loss: 1.6730537414550781
Validation loss: 2.2274922927220664

Epoch: 6| Step: 7
Training loss: 2.136183261871338
Validation loss: 2.205087900161743

Epoch: 6| Step: 8
Training loss: 1.3942344188690186
Validation loss: 2.2123016516367593

Epoch: 6| Step: 9
Training loss: 1.6576242446899414
Validation loss: 2.1867633859316506

Epoch: 6| Step: 10
Training loss: 0.9721628427505493
Validation loss: 2.1900803049405417

Epoch: 6| Step: 11
Training loss: 1.3744661808013916
Validation loss: 2.1983337799708047

Epoch: 6| Step: 12
Training loss: 1.599118947982788
Validation loss: 2.2245463331540427

Epoch: 6| Step: 13
Training loss: 2.1920599937438965
Validation loss: 2.2335493564605713

Epoch: 218| Step: 0
Training loss: 1.3767802715301514
Validation loss: 2.2229882081349692

Epoch: 6| Step: 1
Training loss: 1.422621250152588
Validation loss: 2.2226831714312234

Epoch: 6| Step: 2
Training loss: 1.8552470207214355
Validation loss: 2.1981314420700073

Epoch: 6| Step: 3
Training loss: 1.5461170673370361
Validation loss: 2.22454297542572

Epoch: 6| Step: 4
Training loss: 2.3026065826416016
Validation loss: 2.211183230082194

Epoch: 6| Step: 5
Training loss: 1.9182424545288086
Validation loss: 2.222796459992727

Epoch: 6| Step: 6
Training loss: 1.2008517980575562
Validation loss: 2.2173636158307395

Epoch: 6| Step: 7
Training loss: 1.79423189163208
Validation loss: 2.2267187436421714

Epoch: 6| Step: 8
Training loss: 1.6384073495864868
Validation loss: 2.2065032521883645

Epoch: 6| Step: 9
Training loss: 2.3546714782714844
Validation loss: 2.1843599478403726

Epoch: 6| Step: 10
Training loss: 1.8436212539672852
Validation loss: 2.1638289093971252

Epoch: 6| Step: 11
Training loss: 2.4247937202453613
Validation loss: 2.1535624265670776

Epoch: 6| Step: 12
Training loss: 1.8804197311401367
Validation loss: 2.1816929976145425

Epoch: 6| Step: 13
Training loss: 1.8375873565673828
Validation loss: 2.1928070982297263

Epoch: 219| Step: 0
Training loss: 2.347917318344116
Validation loss: 2.175812919934591

Epoch: 6| Step: 1
Training loss: 2.4325366020202637
Validation loss: 2.220518092314402

Epoch: 6| Step: 2
Training loss: 1.001877784729004
Validation loss: 2.2000105381011963

Epoch: 6| Step: 3
Training loss: 1.1684839725494385
Validation loss: 2.23463241259257

Epoch: 6| Step: 4
Training loss: 1.0208775997161865
Validation loss: 2.2320884267489114

Epoch: 6| Step: 5
Training loss: 2.276285171508789
Validation loss: 2.2053792675336203

Epoch: 6| Step: 6
Training loss: 1.8230468034744263
Validation loss: 2.2237467964490256

Epoch: 6| Step: 7
Training loss: 1.790022850036621
Validation loss: 2.20464293162028

Epoch: 6| Step: 8
Training loss: 1.8707842826843262
Validation loss: 2.2085397044817605

Epoch: 6| Step: 9
Training loss: 1.7777175903320312
Validation loss: 2.2356695532798767

Epoch: 6| Step: 10
Training loss: 1.7933681011199951
Validation loss: 2.1957158843676248

Epoch: 6| Step: 11
Training loss: 1.206251859664917
Validation loss: 2.1933499375979104

Epoch: 6| Step: 12
Training loss: 2.3225643634796143
Validation loss: 2.207312842210134

Epoch: 6| Step: 13
Training loss: 2.1853010654449463
Validation loss: 2.1822431683540344

Epoch: 220| Step: 0
Training loss: 1.9811217784881592
Validation loss: 2.173571785291036

Epoch: 6| Step: 1
Training loss: 2.70163631439209
Validation loss: 2.179351051648458

Epoch: 6| Step: 2
Training loss: 1.8230780363082886
Validation loss: 2.172186493873596

Epoch: 6| Step: 3
Training loss: 1.3953065872192383
Validation loss: 2.164831976095835

Epoch: 6| Step: 4
Training loss: 1.452834129333496
Validation loss: 2.15288245677948

Epoch: 6| Step: 5
Training loss: 1.9481114149093628
Validation loss: 2.1753832697868347

Epoch: 6| Step: 6
Training loss: 1.8177704811096191
Validation loss: 2.1967878937721252

Epoch: 6| Step: 7
Training loss: 2.153319835662842
Validation loss: 2.1792737245559692

Epoch: 6| Step: 8
Training loss: 1.4398925304412842
Validation loss: 2.187606632709503

Epoch: 6| Step: 9
Training loss: 1.4880167245864868
Validation loss: 2.1798903346061707

Epoch: 6| Step: 10
Training loss: 1.7879291772842407
Validation loss: 2.2111454208691916

Epoch: 6| Step: 11
Training loss: 1.4803600311279297
Validation loss: 2.2171215216318765

Epoch: 6| Step: 12
Training loss: 1.7754428386688232
Validation loss: 2.192849576473236

Epoch: 6| Step: 13
Training loss: 1.7623093128204346
Validation loss: 2.2188916206359863

Epoch: 221| Step: 0
Training loss: 2.1149978637695312
Validation loss: 2.1987019578615823

Epoch: 6| Step: 1
Training loss: 1.9867441654205322
Validation loss: 2.211638331413269

Epoch: 6| Step: 2
Training loss: 1.644230842590332
Validation loss: 2.20749294757843

Epoch: 6| Step: 3
Training loss: 0.7863039970397949
Validation loss: 2.191013773282369

Epoch: 6| Step: 4
Training loss: 1.2140464782714844
Validation loss: 2.2008947134017944

Epoch: 6| Step: 5
Training loss: 1.5108342170715332
Validation loss: 2.2109171549479165

Epoch: 6| Step: 6
Training loss: 1.4556517601013184
Validation loss: 2.2033555706342063

Epoch: 6| Step: 7
Training loss: 2.637873649597168
Validation loss: 2.212002476056417

Epoch: 6| Step: 8
Training loss: 1.4693034887313843
Validation loss: 2.2309349377950034

Epoch: 6| Step: 9
Training loss: 2.126535415649414
Validation loss: 2.2319672107696533

Epoch: 6| Step: 10
Training loss: 2.1476588249206543
Validation loss: 2.243064045906067

Epoch: 6| Step: 11
Training loss: 1.9464881420135498
Validation loss: 2.2218469381332397

Epoch: 6| Step: 12
Training loss: 1.7947354316711426
Validation loss: 2.2329978744188943

Epoch: 6| Step: 13
Training loss: 1.5246763229370117
Validation loss: 2.217958092689514

Epoch: 222| Step: 0
Training loss: 1.7568106651306152
Validation loss: 2.222408433755239

Epoch: 6| Step: 1
Training loss: 2.255963087081909
Validation loss: 2.240036686261495

Epoch: 6| Step: 2
Training loss: 1.6367487907409668
Validation loss: 2.2350969910621643

Epoch: 6| Step: 3
Training loss: 1.0799576044082642
Validation loss: 2.2379276752471924

Epoch: 6| Step: 4
Training loss: 1.7742359638214111
Validation loss: 2.2129125595092773

Epoch: 6| Step: 5
Training loss: 1.5810751914978027
Validation loss: 2.206526597340902

Epoch: 6| Step: 6
Training loss: 1.2511048316955566
Validation loss: 2.2081034580866494

Epoch: 6| Step: 7
Training loss: 2.2577641010284424
Validation loss: 2.220671017964681

Epoch: 6| Step: 8
Training loss: 1.2670907974243164
Validation loss: 2.2228239377339682

Epoch: 6| Step: 9
Training loss: 2.0155420303344727
Validation loss: 2.2250264286994934

Epoch: 6| Step: 10
Training loss: 1.6973097324371338
Validation loss: 2.230917493502299

Epoch: 6| Step: 11
Training loss: 1.9309813976287842
Validation loss: 2.204066356023153

Epoch: 6| Step: 12
Training loss: 2.175607204437256
Validation loss: 2.200763165950775

Epoch: 6| Step: 13
Training loss: 1.51129150390625
Validation loss: 2.2127764225006104

Epoch: 223| Step: 0
Training loss: 1.5635031461715698
Validation loss: 2.218582352002462

Epoch: 6| Step: 1
Training loss: 1.4138426780700684
Validation loss: 2.2273040612538657

Epoch: 6| Step: 2
Training loss: 1.5557721853256226
Validation loss: 2.1953585346539817

Epoch: 6| Step: 3
Training loss: 2.0632541179656982
Validation loss: 2.2148630221684775

Epoch: 6| Step: 4
Training loss: 2.352785110473633
Validation loss: 2.2199136217435202

Epoch: 6| Step: 5
Training loss: 1.7879612445831299
Validation loss: 2.2094215750694275

Epoch: 6| Step: 6
Training loss: 1.8793678283691406
Validation loss: 2.2158106366793313

Epoch: 6| Step: 7
Training loss: 1.6473822593688965
Validation loss: 2.210560460885366

Epoch: 6| Step: 8
Training loss: 1.6524724960327148
Validation loss: 2.2095590035120645

Epoch: 6| Step: 9
Training loss: 1.501190423965454
Validation loss: 2.210079471270243

Epoch: 6| Step: 10
Training loss: 1.6005580425262451
Validation loss: 2.210031270980835

Epoch: 6| Step: 11
Training loss: 1.460003137588501
Validation loss: 2.224112013975779

Epoch: 6| Step: 12
Training loss: 1.1813228130340576
Validation loss: 2.2256693641344705

Epoch: 6| Step: 13
Training loss: 2.2485151290893555
Validation loss: 2.2221394379933677

Epoch: 224| Step: 0
Training loss: 1.7825050354003906
Validation loss: 2.2280137141545615

Epoch: 6| Step: 1
Training loss: 1.5751144886016846
Validation loss: 2.2173601388931274

Epoch: 6| Step: 2
Training loss: 1.4020121097564697
Validation loss: 2.224598705768585

Epoch: 6| Step: 3
Training loss: 1.6827493906021118
Validation loss: 2.2314563194910684

Epoch: 6| Step: 4
Training loss: 1.8828990459442139
Validation loss: 2.232991357644399

Epoch: 6| Step: 5
Training loss: 1.057135820388794
Validation loss: 2.225302974383036

Epoch: 6| Step: 6
Training loss: 1.47964608669281
Validation loss: 2.207890590031942

Epoch: 6| Step: 7
Training loss: 1.5181827545166016
Validation loss: 2.230855703353882

Epoch: 6| Step: 8
Training loss: 2.0371241569519043
Validation loss: 2.2156991561253867

Epoch: 6| Step: 9
Training loss: 1.4120044708251953
Validation loss: 2.2119580109914145

Epoch: 6| Step: 10
Training loss: 2.1822803020477295
Validation loss: 2.2223663131395974

Epoch: 6| Step: 11
Training loss: 1.3977596759796143
Validation loss: 2.2160256703694663

Epoch: 6| Step: 12
Training loss: 1.7406890392303467
Validation loss: 2.1955363750457764

Epoch: 6| Step: 13
Training loss: 2.566868543624878
Validation loss: 2.2060421109199524

Epoch: 225| Step: 0
Training loss: 2.2497286796569824
Validation loss: 2.1920960545539856

Epoch: 6| Step: 1
Training loss: 2.1152029037475586
Validation loss: 2.199341098467509

Epoch: 6| Step: 2
Training loss: 1.380328893661499
Validation loss: 2.1891602277755737

Epoch: 6| Step: 3
Training loss: 1.3276143074035645
Validation loss: 2.1955522894859314

Epoch: 6| Step: 4
Training loss: 2.31406307220459
Validation loss: 2.1917715469996133

Epoch: 6| Step: 5
Training loss: 2.5641098022460938
Validation loss: 2.184530337651571

Epoch: 6| Step: 6
Training loss: 1.549040675163269
Validation loss: 2.2006214459737143

Epoch: 6| Step: 7
Training loss: 1.591739535331726
Validation loss: 2.1891608238220215

Epoch: 6| Step: 8
Training loss: 1.0356435775756836
Validation loss: 2.176901896794637

Epoch: 6| Step: 9
Training loss: 1.6812164783477783
Validation loss: 2.195515275001526

Epoch: 6| Step: 10
Training loss: 1.1236778497695923
Validation loss: 2.1889172991116843

Epoch: 6| Step: 11
Training loss: 1.4015469551086426
Validation loss: 2.193145473798116

Epoch: 6| Step: 12
Training loss: 1.3577617406845093
Validation loss: 2.2011154294013977

Epoch: 6| Step: 13
Training loss: 2.4765613079071045
Validation loss: 2.211659630139669

Epoch: 226| Step: 0
Training loss: 1.7180711030960083
Validation loss: 2.177235941092173

Epoch: 6| Step: 1
Training loss: 1.874572515487671
Validation loss: 2.212227523326874

Epoch: 6| Step: 2
Training loss: 1.9834866523742676
Validation loss: 2.1837873458862305

Epoch: 6| Step: 3
Training loss: 1.7227402925491333
Validation loss: 2.1566960215568542

Epoch: 6| Step: 4
Training loss: 1.349571943283081
Validation loss: 2.1589373350143433

Epoch: 6| Step: 5
Training loss: 1.6698048114776611
Validation loss: 2.1770905256271362

Epoch: 6| Step: 6
Training loss: 1.655190110206604
Validation loss: 2.1451596220334372

Epoch: 6| Step: 7
Training loss: 0.8221719861030579
Validation loss: 2.1498414675394693

Epoch: 6| Step: 8
Training loss: 1.8498802185058594
Validation loss: 2.1724915901819863

Epoch: 6| Step: 9
Training loss: 2.4077651500701904
Validation loss: 2.187493403752645

Epoch: 6| Step: 10
Training loss: 1.725066900253296
Validation loss: 2.20084677139918

Epoch: 6| Step: 11
Training loss: 1.6503266096115112
Validation loss: 2.172189712524414

Epoch: 6| Step: 12
Training loss: 2.014625072479248
Validation loss: 2.1599791049957275

Epoch: 6| Step: 13
Training loss: 1.8438012599945068
Validation loss: 2.1662017504374185

Epoch: 227| Step: 0
Training loss: 1.5049238204956055
Validation loss: 2.170693020025889

Epoch: 6| Step: 1
Training loss: 1.7390530109405518
Validation loss: 2.1869302789370217

Epoch: 6| Step: 2
Training loss: 2.3455286026000977
Validation loss: 2.187918762365977

Epoch: 6| Step: 3
Training loss: 1.2611373662948608
Validation loss: 2.188532908757528

Epoch: 6| Step: 4
Training loss: 2.0493173599243164
Validation loss: 2.2201311389605203

Epoch: 6| Step: 5
Training loss: 1.719296932220459
Validation loss: 2.2333098649978638

Epoch: 6| Step: 6
Training loss: 1.9561814069747925
Validation loss: 2.193607985973358

Epoch: 6| Step: 7
Training loss: 1.4128670692443848
Validation loss: 2.1814194917678833

Epoch: 6| Step: 8
Training loss: 1.1776853799819946
Validation loss: 2.200962762037913

Epoch: 6| Step: 9
Training loss: 1.7467832565307617
Validation loss: 2.192877451578776

Epoch: 6| Step: 10
Training loss: 1.9022098779678345
Validation loss: 2.1976489226023355

Epoch: 6| Step: 11
Training loss: 2.1693763732910156
Validation loss: 2.2216784358024597

Epoch: 6| Step: 12
Training loss: 1.862337350845337
Validation loss: 2.2107033133506775

Epoch: 6| Step: 13
Training loss: 1.7974793910980225
Validation loss: 2.2133907675743103

Epoch: 228| Step: 0
Training loss: 1.716137170791626
Validation loss: 2.180780033270518

Epoch: 6| Step: 1
Training loss: 2.156362295150757
Validation loss: 2.2179720600446067

Epoch: 6| Step: 2
Training loss: 1.7643259763717651
Validation loss: 2.1998602946599326

Epoch: 6| Step: 3
Training loss: 2.029162883758545
Validation loss: 2.2022682825724282

Epoch: 6| Step: 4
Training loss: 1.1522691249847412
Validation loss: 2.2075567046801248

Epoch: 6| Step: 5
Training loss: 1.8011159896850586
Validation loss: 2.2043594320615134

Epoch: 6| Step: 6
Training loss: 1.3356328010559082
Validation loss: 2.2004851500193277

Epoch: 6| Step: 7
Training loss: 1.812379002571106
Validation loss: 2.207970917224884

Epoch: 6| Step: 8
Training loss: 0.9714970588684082
Validation loss: 2.206673800945282

Epoch: 6| Step: 9
Training loss: 1.4453704357147217
Validation loss: 2.212407906850179

Epoch: 6| Step: 10
Training loss: 1.8306461572647095
Validation loss: 2.1908751924832663

Epoch: 6| Step: 11
Training loss: 2.107398509979248
Validation loss: 2.227598786354065

Epoch: 6| Step: 12
Training loss: 2.0053977966308594
Validation loss: 2.195667266845703

Epoch: 6| Step: 13
Training loss: 1.5144438743591309
Validation loss: 2.2075763742129006

Epoch: 229| Step: 0
Training loss: 1.9129831790924072
Validation loss: 2.1929336388905845

Epoch: 6| Step: 1
Training loss: 1.873170256614685
Validation loss: 2.2202669382095337

Epoch: 6| Step: 2
Training loss: 2.014754295349121
Validation loss: 2.187068243821462

Epoch: 6| Step: 3
Training loss: 1.6751000881195068
Validation loss: 2.2024155457814536

Epoch: 6| Step: 4
Training loss: 1.1945600509643555
Validation loss: 2.1836910049120584

Epoch: 6| Step: 5
Training loss: 1.6127852201461792
Validation loss: 2.210877855618795

Epoch: 6| Step: 6
Training loss: 1.4899852275848389
Validation loss: 2.218378027280172

Epoch: 6| Step: 7
Training loss: 1.8315826654434204
Validation loss: 2.203808863957723

Epoch: 6| Step: 8
Training loss: 2.4150807857513428
Validation loss: 2.188851217428843

Epoch: 6| Step: 9
Training loss: 1.0203680992126465
Validation loss: 2.1958460807800293

Epoch: 6| Step: 10
Training loss: 1.937260627746582
Validation loss: 2.1889142990112305

Epoch: 6| Step: 11
Training loss: 1.8679667711257935
Validation loss: 2.1748420000076294

Epoch: 6| Step: 12
Training loss: 2.228806495666504
Validation loss: 2.199853003025055

Epoch: 6| Step: 13
Training loss: 1.2653234004974365
Validation loss: 2.1985963781674704

Epoch: 230| Step: 0
Training loss: 2.1877756118774414
Validation loss: 2.2032388051350913

Epoch: 6| Step: 1
Training loss: 1.7493996620178223
Validation loss: 2.1653954784075418

Epoch: 6| Step: 2
Training loss: 1.3706130981445312
Validation loss: 2.1925816933314004

Epoch: 6| Step: 3
Training loss: 1.7895963191986084
Validation loss: 2.1829784909884133

Epoch: 6| Step: 4
Training loss: 1.4713246822357178
Validation loss: 2.1920965711275735

Epoch: 6| Step: 5
Training loss: 1.9687232971191406
Validation loss: 2.2240198453267417

Epoch: 6| Step: 6
Training loss: 1.669713020324707
Validation loss: 2.180898904800415

Epoch: 6| Step: 7
Training loss: 1.555974006652832
Validation loss: 2.2034741640090942

Epoch: 6| Step: 8
Training loss: 2.1227917671203613
Validation loss: 2.2156017820040383

Epoch: 6| Step: 9
Training loss: 1.2945160865783691
Validation loss: 2.190382242202759

Epoch: 6| Step: 10
Training loss: 1.7217557430267334
Validation loss: 2.202945033709208

Epoch: 6| Step: 11
Training loss: 1.5299928188323975
Validation loss: 2.1774388750394187

Epoch: 6| Step: 12
Training loss: 1.4110335111618042
Validation loss: 2.2093600829442344

Epoch: 6| Step: 13
Training loss: 1.5016264915466309
Validation loss: 2.19490385055542

Epoch: 231| Step: 0
Training loss: 1.558664083480835
Validation loss: 2.206145763397217

Epoch: 6| Step: 1
Training loss: 0.8501307964324951
Validation loss: 2.191990335782369

Epoch: 6| Step: 2
Training loss: 1.754775047302246
Validation loss: 2.1845019459724426

Epoch: 6| Step: 3
Training loss: 0.9904963970184326
Validation loss: 2.201017657915751

Epoch: 6| Step: 4
Training loss: 1.850659728050232
Validation loss: 2.1967179775238037

Epoch: 6| Step: 5
Training loss: 1.8161802291870117
Validation loss: 2.1943370699882507

Epoch: 6| Step: 6
Training loss: 1.8169169425964355
Validation loss: 2.202592055002848

Epoch: 6| Step: 7
Training loss: 2.1864166259765625
Validation loss: 2.2135832707087197

Epoch: 6| Step: 8
Training loss: 2.322502851486206
Validation loss: 2.1961382230122886

Epoch: 6| Step: 9
Training loss: 1.0147098302841187
Validation loss: 2.182777166366577

Epoch: 6| Step: 10
Training loss: 2.7367897033691406
Validation loss: 2.1832667787869773

Epoch: 6| Step: 11
Training loss: 1.3346666097640991
Validation loss: 2.16671089331309

Epoch: 6| Step: 12
Training loss: 1.2561938762664795
Validation loss: 2.179540475209554

Epoch: 6| Step: 13
Training loss: 1.8623583316802979
Validation loss: 2.1710459192593894

Epoch: 232| Step: 0
Training loss: 1.8376491069793701
Validation loss: 2.1651437878608704

Epoch: 6| Step: 1
Training loss: 1.5375779867172241
Validation loss: 2.1631361842155457

Epoch: 6| Step: 2
Training loss: 1.5647218227386475
Validation loss: 2.194802204767863

Epoch: 6| Step: 3
Training loss: 1.6541852951049805
Validation loss: 2.1719043056170144

Epoch: 6| Step: 4
Training loss: 1.7166043519973755
Validation loss: 2.1879897912343345

Epoch: 6| Step: 5
Training loss: 2.2258639335632324
Validation loss: 2.207602381706238

Epoch: 6| Step: 6
Training loss: 1.9954638481140137
Validation loss: 2.1952141722043357

Epoch: 6| Step: 7
Training loss: 1.4237927198410034
Validation loss: 2.2161514361699424

Epoch: 6| Step: 8
Training loss: 1.2904845476150513
Validation loss: 2.2335241039594016

Epoch: 6| Step: 9
Training loss: 1.6974364519119263
Validation loss: 2.2191797693570456

Epoch: 6| Step: 10
Training loss: 1.2006993293762207
Validation loss: 2.2065396308898926

Epoch: 6| Step: 11
Training loss: 2.066469192504883
Validation loss: 2.20780086517334

Epoch: 6| Step: 12
Training loss: 1.7290211915969849
Validation loss: 2.2052294810613

Epoch: 6| Step: 13
Training loss: 1.7677899599075317
Validation loss: 2.1948472062746682

Epoch: 233| Step: 0
Training loss: 1.4264668226242065
Validation loss: 2.179180860519409

Epoch: 6| Step: 1
Training loss: 2.1302130222320557
Validation loss: 2.203629414240519

Epoch: 6| Step: 2
Training loss: 1.56648850440979
Validation loss: 2.1898240049680076

Epoch: 6| Step: 3
Training loss: 1.2184216976165771
Validation loss: 2.194871167341868

Epoch: 6| Step: 4
Training loss: 1.796276569366455
Validation loss: 2.1929068764050803

Epoch: 6| Step: 5
Training loss: 1.8544262647628784
Validation loss: 2.1930460135142007

Epoch: 6| Step: 6
Training loss: 1.7294952869415283
Validation loss: 2.1775425473848977

Epoch: 6| Step: 7
Training loss: 1.6128445863723755
Validation loss: 2.1850310365358987

Epoch: 6| Step: 8
Training loss: 1.9345170259475708
Validation loss: 2.186057229836782

Epoch: 6| Step: 9
Training loss: 1.4308109283447266
Validation loss: 2.1832074721654258

Epoch: 6| Step: 10
Training loss: 2.3994243144989014
Validation loss: 2.174561699231466

Epoch: 6| Step: 11
Training loss: 1.3658552169799805
Validation loss: 2.178456127643585

Epoch: 6| Step: 12
Training loss: 1.7685060501098633
Validation loss: 2.179620146751404

Epoch: 6| Step: 13
Training loss: 2.111560344696045
Validation loss: 2.178048769632975

Epoch: 234| Step: 0
Training loss: 2.045426607131958
Validation loss: 2.188317060470581

Epoch: 6| Step: 1
Training loss: 1.6267385482788086
Validation loss: 2.1775784691174827

Epoch: 6| Step: 2
Training loss: 0.9401203393936157
Validation loss: 2.1859721740086875

Epoch: 6| Step: 3
Training loss: 1.4586057662963867
Validation loss: 2.1775328715642295

Epoch: 6| Step: 4
Training loss: 1.4482951164245605
Validation loss: 2.2020073731740317

Epoch: 6| Step: 5
Training loss: 2.0480103492736816
Validation loss: 2.2001529137293496

Epoch: 6| Step: 6
Training loss: 1.6918977499008179
Validation loss: 2.200537840525309

Epoch: 6| Step: 7
Training loss: 1.6388344764709473
Validation loss: 2.2145698269208274

Epoch: 6| Step: 8
Training loss: 2.0146684646606445
Validation loss: 2.189351201057434

Epoch: 6| Step: 9
Training loss: 2.158205032348633
Validation loss: 2.2057625452677407

Epoch: 6| Step: 10
Training loss: 1.9443895816802979
Validation loss: 2.1687735517819724

Epoch: 6| Step: 11
Training loss: 2.0669407844543457
Validation loss: 2.1593757470448813

Epoch: 6| Step: 12
Training loss: 1.5720267295837402
Validation loss: 2.1479681531588235

Epoch: 6| Step: 13
Training loss: 1.6263660192489624
Validation loss: 2.1357784469922385

Epoch: 235| Step: 0
Training loss: 2.207286834716797
Validation loss: 2.1403438250223794

Epoch: 6| Step: 1
Training loss: 1.8920735120773315
Validation loss: 2.150308688481649

Epoch: 6| Step: 2
Training loss: 2.435671806335449
Validation loss: 2.162084400653839

Epoch: 6| Step: 3
Training loss: 1.5436958074569702
Validation loss: 2.1674590508143106

Epoch: 6| Step: 4
Training loss: 1.9744067192077637
Validation loss: 2.1666058897972107

Epoch: 6| Step: 5
Training loss: 1.1884076595306396
Validation loss: 2.165739973386129

Epoch: 6| Step: 6
Training loss: 1.4188076257705688
Validation loss: 2.192851404349009

Epoch: 6| Step: 7
Training loss: 2.0085256099700928
Validation loss: 2.1857963601748147

Epoch: 6| Step: 8
Training loss: 1.9510974884033203
Validation loss: 2.1887497901916504

Epoch: 6| Step: 9
Training loss: 1.8624144792556763
Validation loss: 2.1854785680770874

Epoch: 6| Step: 10
Training loss: 1.7700049877166748
Validation loss: 2.1978737711906433

Epoch: 6| Step: 11
Training loss: 1.1426031589508057
Validation loss: 2.204229017098745

Epoch: 6| Step: 12
Training loss: 0.8810927867889404
Validation loss: 2.220867951711019

Epoch: 6| Step: 13
Training loss: 1.864710807800293
Validation loss: 2.226129710674286

Epoch: 236| Step: 0
Training loss: 1.9999275207519531
Validation loss: 2.2187354365984597

Epoch: 6| Step: 1
Training loss: 1.465193510055542
Validation loss: 2.2097394863764444

Epoch: 6| Step: 2
Training loss: 1.4466571807861328
Validation loss: 2.2290148933728537

Epoch: 6| Step: 3
Training loss: 1.8319621086120605
Validation loss: 2.2157743771870932

Epoch: 6| Step: 4
Training loss: 1.405102014541626
Validation loss: 2.208865841229757

Epoch: 6| Step: 5
Training loss: 1.8527278900146484
Validation loss: 2.211261431376139

Epoch: 6| Step: 6
Training loss: 2.2648768424987793
Validation loss: 2.200265089670817

Epoch: 6| Step: 7
Training loss: 1.4710735082626343
Validation loss: 2.217772821585337

Epoch: 6| Step: 8
Training loss: 1.5111169815063477
Validation loss: 2.206422805786133

Epoch: 6| Step: 9
Training loss: 2.1908841133117676
Validation loss: 2.2213438749313354

Epoch: 6| Step: 10
Training loss: 1.261044979095459
Validation loss: 2.2183866699536643

Epoch: 6| Step: 11
Training loss: 2.2180566787719727
Validation loss: 2.229603588581085

Epoch: 6| Step: 12
Training loss: 1.1968752145767212
Validation loss: 2.2189252972602844

Epoch: 6| Step: 13
Training loss: 0.9079082012176514
Validation loss: 2.2170011599858603

Epoch: 237| Step: 0
Training loss: 0.869421124458313
Validation loss: 2.2186416188875833

Epoch: 6| Step: 1
Training loss: 1.431257724761963
Validation loss: 2.2395025889078775

Epoch: 6| Step: 2
Training loss: 2.238189458847046
Validation loss: 2.2490402261416116

Epoch: 6| Step: 3
Training loss: 1.9618237018585205
Validation loss: 2.2323232889175415

Epoch: 6| Step: 4
Training loss: 2.3243370056152344
Validation loss: 2.2277993162473044

Epoch: 6| Step: 5
Training loss: 1.5898592472076416
Validation loss: 2.235870440800985

Epoch: 6| Step: 6
Training loss: 2.095043659210205
Validation loss: 2.2150232394536338

Epoch: 6| Step: 7
Training loss: 1.7690317630767822
Validation loss: 2.1909024715423584

Epoch: 6| Step: 8
Training loss: 0.8977096080780029
Validation loss: 2.202515502770742

Epoch: 6| Step: 9
Training loss: 1.4645289182662964
Validation loss: 2.2296868165334067

Epoch: 6| Step: 10
Training loss: 1.347312092781067
Validation loss: 2.203206181526184

Epoch: 6| Step: 11
Training loss: 1.4938634634017944
Validation loss: 2.205508291721344

Epoch: 6| Step: 12
Training loss: 2.307462692260742
Validation loss: 2.1786921620368958

Epoch: 6| Step: 13
Training loss: 1.3444294929504395
Validation loss: 2.212041954199473

Epoch: 238| Step: 0
Training loss: 2.467458486557007
Validation loss: 2.2509843905766806

Epoch: 6| Step: 1
Training loss: 1.3945121765136719
Validation loss: 2.2327096660931907

Epoch: 6| Step: 2
Training loss: 2.2078757286071777
Validation loss: 2.235691408316294

Epoch: 6| Step: 3
Training loss: 1.3906373977661133
Validation loss: 2.233958681424459

Epoch: 6| Step: 4
Training loss: 2.0908515453338623
Validation loss: 2.2263916532198587

Epoch: 6| Step: 5
Training loss: 2.546353578567505
Validation loss: 2.2365697622299194

Epoch: 6| Step: 6
Training loss: 0.6959466934204102
Validation loss: 2.2393397092819214

Epoch: 6| Step: 7
Training loss: 1.5860631465911865
Validation loss: 2.2345994313557944

Epoch: 6| Step: 8
Training loss: 1.387367606163025
Validation loss: 2.2498651146888733

Epoch: 6| Step: 9
Training loss: 1.433525562286377
Validation loss: 2.273120403289795

Epoch: 6| Step: 10
Training loss: 1.287555456161499
Validation loss: 2.2475655674934387

Epoch: 6| Step: 11
Training loss: 1.3817769289016724
Validation loss: 2.23728354771932

Epoch: 6| Step: 12
Training loss: 1.3799152374267578
Validation loss: 2.236298600832621

Epoch: 6| Step: 13
Training loss: 1.65352201461792
Validation loss: 2.2288456161816916

Epoch: 239| Step: 0
Training loss: 1.6152153015136719
Validation loss: 2.2043797969818115

Epoch: 6| Step: 1
Training loss: 1.3602182865142822
Validation loss: 2.174524188041687

Epoch: 6| Step: 2
Training loss: 2.2245583534240723
Validation loss: 2.171527067820231

Epoch: 6| Step: 3
Training loss: 2.1788976192474365
Validation loss: 2.1765982508659363

Epoch: 6| Step: 4
Training loss: 2.008399486541748
Validation loss: 2.203706979751587

Epoch: 6| Step: 5
Training loss: 1.2729136943817139
Validation loss: 2.168760518232981

Epoch: 6| Step: 6
Training loss: 1.1025161743164062
Validation loss: 2.1754566033681235

Epoch: 6| Step: 7
Training loss: 1.6952193975448608
Validation loss: 2.1772990226745605

Epoch: 6| Step: 8
Training loss: 1.02543306350708
Validation loss: 2.1876054406166077

Epoch: 6| Step: 9
Training loss: 2.0905649662017822
Validation loss: 2.1990241408348083

Epoch: 6| Step: 10
Training loss: 2.0136566162109375
Validation loss: 2.202892243862152

Epoch: 6| Step: 11
Training loss: 1.2019944190979004
Validation loss: 2.1709463795026145

Epoch: 6| Step: 12
Training loss: 1.6821181774139404
Validation loss: 2.208103656768799

Epoch: 6| Step: 13
Training loss: 1.5086534023284912
Validation loss: 2.1912017862002053

Epoch: 240| Step: 0
Training loss: 1.7903485298156738
Validation loss: 2.157753348350525

Epoch: 6| Step: 1
Training loss: 1.8825013637542725
Validation loss: 2.147475838661194

Epoch: 6| Step: 2
Training loss: 1.651350498199463
Validation loss: 2.1389432350794473

Epoch: 6| Step: 3
Training loss: 1.4522624015808105
Validation loss: 2.1516182025273642

Epoch: 6| Step: 4
Training loss: 1.818878173828125
Validation loss: 2.1864160100618997

Epoch: 6| Step: 5
Training loss: 2.5775771141052246
Validation loss: 2.1763024727503457

Epoch: 6| Step: 6
Training loss: 1.3430078029632568
Validation loss: 2.176574985186259

Epoch: 6| Step: 7
Training loss: 1.3993130922317505
Validation loss: 2.173895279566447

Epoch: 6| Step: 8
Training loss: 1.368805170059204
Validation loss: 2.184158225854238

Epoch: 6| Step: 9
Training loss: 1.3388417959213257
Validation loss: 2.1950864791870117

Epoch: 6| Step: 10
Training loss: 1.6230652332305908
Validation loss: 2.181943158308665

Epoch: 6| Step: 11
Training loss: 2.1636695861816406
Validation loss: 2.1832393209139505

Epoch: 6| Step: 12
Training loss: 1.6979975700378418
Validation loss: 2.191225985685984

Epoch: 6| Step: 13
Training loss: 1.8202319145202637
Validation loss: 2.175138552983602

Epoch: 241| Step: 0
Training loss: 1.390688180923462
Validation loss: 2.1806617180506387

Epoch: 6| Step: 1
Training loss: 1.9644441604614258
Validation loss: 2.177264471848806

Epoch: 6| Step: 2
Training loss: 1.080869436264038
Validation loss: 2.1797165075937905

Epoch: 6| Step: 3
Training loss: 1.743964672088623
Validation loss: 2.164404511451721

Epoch: 6| Step: 4
Training loss: 1.3950612545013428
Validation loss: 2.1879671216011047

Epoch: 6| Step: 5
Training loss: 1.9847521781921387
Validation loss: 2.201704184214274

Epoch: 6| Step: 6
Training loss: 1.5427629947662354
Validation loss: 2.196956912676493

Epoch: 6| Step: 7
Training loss: 1.352630376815796
Validation loss: 2.207765539487203

Epoch: 6| Step: 8
Training loss: 0.9067880511283875
Validation loss: 2.173276702562968

Epoch: 6| Step: 9
Training loss: 1.9665038585662842
Validation loss: 2.1926504373550415

Epoch: 6| Step: 10
Training loss: 2.800571918487549
Validation loss: 2.18168177207311

Epoch: 6| Step: 11
Training loss: 2.0531065464019775
Validation loss: 2.1640732288360596

Epoch: 6| Step: 12
Training loss: 1.1403888463974
Validation loss: 2.17891389131546

Epoch: 6| Step: 13
Training loss: 1.775691032409668
Validation loss: 2.173025449117025

Epoch: 242| Step: 0
Training loss: 1.3732106685638428
Validation loss: 2.1735192934672036

Epoch: 6| Step: 1
Training loss: 2.2230708599090576
Validation loss: 2.1792224844296775

Epoch: 6| Step: 2
Training loss: 1.0930578708648682
Validation loss: 2.1849728425343833

Epoch: 6| Step: 3
Training loss: 1.523716688156128
Validation loss: 2.187447190284729

Epoch: 6| Step: 4
Training loss: 1.8470739126205444
Validation loss: 2.2003196477890015

Epoch: 6| Step: 5
Training loss: 1.7992196083068848
Validation loss: 2.1815176606178284

Epoch: 6| Step: 6
Training loss: 1.5699769258499146
Validation loss: 2.2079132795333862

Epoch: 6| Step: 7
Training loss: 1.486079454421997
Validation loss: 2.1794218023618064

Epoch: 6| Step: 8
Training loss: 1.80474853515625
Validation loss: 2.2064248919487

Epoch: 6| Step: 9
Training loss: 1.190751314163208
Validation loss: 2.2026315132776895

Epoch: 6| Step: 10
Training loss: 1.6503468751907349
Validation loss: 2.208258946736654

Epoch: 6| Step: 11
Training loss: 1.8277064561843872
Validation loss: 2.1901425321896872

Epoch: 6| Step: 12
Training loss: 2.3518288135528564
Validation loss: 2.1969834566116333

Epoch: 6| Step: 13
Training loss: 1.4605684280395508
Validation loss: 2.2451972365379333

Epoch: 243| Step: 0
Training loss: 2.039175033569336
Validation loss: 2.2340396841367087

Epoch: 6| Step: 1
Training loss: 1.694139838218689
Validation loss: 2.248107373714447

Epoch: 6| Step: 2
Training loss: 1.5646283626556396
Validation loss: 2.242147922515869

Epoch: 6| Step: 3
Training loss: 1.3455103635787964
Validation loss: 2.189576268196106

Epoch: 6| Step: 4
Training loss: 1.982304573059082
Validation loss: 2.1591879526774087

Epoch: 6| Step: 5
Training loss: 1.7225017547607422
Validation loss: 2.169526437918345

Epoch: 6| Step: 6
Training loss: 1.6993722915649414
Validation loss: 2.1668421824773154

Epoch: 6| Step: 7
Training loss: 1.321365475654602
Validation loss: 2.151974101861318

Epoch: 6| Step: 8
Training loss: 2.09969425201416
Validation loss: 2.1674426198005676

Epoch: 6| Step: 9
Training loss: 1.6400573253631592
Validation loss: 2.1700207591056824

Epoch: 6| Step: 10
Training loss: 1.5073708295822144
Validation loss: 2.190783758958181

Epoch: 6| Step: 11
Training loss: 1.1936566829681396
Validation loss: 2.2295307715733848

Epoch: 6| Step: 12
Training loss: 1.7297983169555664
Validation loss: 2.224265217781067

Epoch: 6| Step: 13
Training loss: 2.359504222869873
Validation loss: 2.2134844859441123

Epoch: 244| Step: 0
Training loss: 1.5474928617477417
Validation loss: 2.259313682715098

Epoch: 6| Step: 1
Training loss: 1.5814146995544434
Validation loss: 2.231504281361898

Epoch: 6| Step: 2
Training loss: 2.071834087371826
Validation loss: 2.201374371846517

Epoch: 6| Step: 3
Training loss: 1.3817470073699951
Validation loss: 2.2284980614980063

Epoch: 6| Step: 4
Training loss: 1.3754326105117798
Validation loss: 2.2132245898246765

Epoch: 6| Step: 5
Training loss: 1.4431753158569336
Validation loss: 2.189842681090037

Epoch: 6| Step: 6
Training loss: 1.888725996017456
Validation loss: 2.181494096914927

Epoch: 6| Step: 7
Training loss: 2.8190255165100098
Validation loss: 2.1825722257296243

Epoch: 6| Step: 8
Training loss: 1.7073149681091309
Validation loss: 2.210400183995565

Epoch: 6| Step: 9
Training loss: 1.4982690811157227
Validation loss: 2.198063532511393

Epoch: 6| Step: 10
Training loss: 1.824671983718872
Validation loss: 2.204008181889852

Epoch: 6| Step: 11
Training loss: 2.206155776977539
Validation loss: 2.2227022647857666

Epoch: 6| Step: 12
Training loss: 1.64670729637146
Validation loss: 2.2286868492762246

Epoch: 6| Step: 13
Training loss: 1.2446889877319336
Validation loss: 2.1887614528338113

Epoch: 245| Step: 0
Training loss: 0.9566514492034912
Validation loss: 2.2041155298550925

Epoch: 6| Step: 1
Training loss: 2.3767106533050537
Validation loss: 2.210053582986196

Epoch: 6| Step: 2
Training loss: 1.9943236112594604
Validation loss: 2.192753235499064

Epoch: 6| Step: 3
Training loss: 1.1703636646270752
Validation loss: 2.2312978903452554

Epoch: 6| Step: 4
Training loss: 1.8793509006500244
Validation loss: 2.215368906656901

Epoch: 6| Step: 5
Training loss: 1.66388738155365
Validation loss: 2.2161178588867188

Epoch: 6| Step: 6
Training loss: 2.201716423034668
Validation loss: 2.1857224106788635

Epoch: 6| Step: 7
Training loss: 0.9937489032745361
Validation loss: 2.2017215490341187

Epoch: 6| Step: 8
Training loss: 1.6776018142700195
Validation loss: 2.209829052289327

Epoch: 6| Step: 9
Training loss: 1.2275391817092896
Validation loss: 2.2168173591295877

Epoch: 6| Step: 10
Training loss: 2.129384994506836
Validation loss: 2.2329496145248413

Epoch: 6| Step: 11
Training loss: 1.8651459217071533
Validation loss: 2.217791179815928

Epoch: 6| Step: 12
Training loss: 1.2766035795211792
Validation loss: 2.2315272092819214

Epoch: 6| Step: 13
Training loss: 1.826850414276123
Validation loss: 2.232089896996816

Epoch: 246| Step: 0
Training loss: 1.6014997959136963
Validation loss: 2.2332992951075235

Epoch: 6| Step: 1
Training loss: 1.419367790222168
Validation loss: 2.225623309612274

Epoch: 6| Step: 2
Training loss: 1.4007186889648438
Validation loss: 2.2399688561757407

Epoch: 6| Step: 3
Training loss: 2.181581974029541
Validation loss: 2.2594725290934243

Epoch: 6| Step: 4
Training loss: 1.479541540145874
Validation loss: 2.2527655760447183

Epoch: 6| Step: 5
Training loss: 2.471885919570923
Validation loss: 2.2598383029301963

Epoch: 6| Step: 6
Training loss: 2.073885440826416
Validation loss: 2.2050040562947593

Epoch: 6| Step: 7
Training loss: 1.9517675638198853
Validation loss: 2.210542360941569

Epoch: 6| Step: 8
Training loss: 1.6606285572052002
Validation loss: 2.2028425335884094

Epoch: 6| Step: 9
Training loss: 1.1054015159606934
Validation loss: 2.19750307003657

Epoch: 6| Step: 10
Training loss: 1.2951849699020386
Validation loss: 2.235250473022461

Epoch: 6| Step: 11
Training loss: 2.202228307723999
Validation loss: 2.24760768810908

Epoch: 6| Step: 12
Training loss: 1.415226697921753
Validation loss: 2.253710309664408

Epoch: 6| Step: 13
Training loss: 1.4140257835388184
Validation loss: 2.265064279238383

Epoch: 247| Step: 0
Training loss: 1.1523268222808838
Validation loss: 2.2450446287790933

Epoch: 6| Step: 1
Training loss: 2.233454942703247
Validation loss: 2.2473138570785522

Epoch: 6| Step: 2
Training loss: 1.7976899147033691
Validation loss: 2.2473590771357217

Epoch: 6| Step: 3
Training loss: 2.094836711883545
Validation loss: 2.2401668230692544

Epoch: 6| Step: 4
Training loss: 1.6714890003204346
Validation loss: 2.2117757002512612

Epoch: 6| Step: 5
Training loss: 0.9028871655464172
Validation loss: 2.261615733305613

Epoch: 6| Step: 6
Training loss: 1.3095895051956177
Validation loss: 2.229621688524882

Epoch: 6| Step: 7
Training loss: 1.3206779956817627
Validation loss: 2.2238797346750894

Epoch: 6| Step: 8
Training loss: 2.2544260025024414
Validation loss: 2.231489280859629

Epoch: 6| Step: 9
Training loss: 1.801864743232727
Validation loss: 2.2050838669141135

Epoch: 6| Step: 10
Training loss: 1.8303216695785522
Validation loss: 2.208130876223246

Epoch: 6| Step: 11
Training loss: 1.4113922119140625
Validation loss: 2.203445315361023

Epoch: 6| Step: 12
Training loss: 1.4059081077575684
Validation loss: 2.1992430885632834

Epoch: 6| Step: 13
Training loss: 1.058469533920288
Validation loss: 2.2210922241210938

Epoch: 248| Step: 0
Training loss: 1.6756693124771118
Validation loss: 2.2249945998191833

Epoch: 6| Step: 1
Training loss: 1.676353931427002
Validation loss: 2.25437331199646

Epoch: 6| Step: 2
Training loss: 1.2423880100250244
Validation loss: 2.235673705736796

Epoch: 6| Step: 3
Training loss: 1.821244239807129
Validation loss: 2.2090745766957602

Epoch: 6| Step: 4
Training loss: 1.4435681104660034
Validation loss: 2.211206833521525

Epoch: 6| Step: 5
Training loss: 1.6991785764694214
Validation loss: 2.2149078448613486

Epoch: 6| Step: 6
Training loss: 1.2486729621887207
Validation loss: 2.2186098297437034

Epoch: 6| Step: 7
Training loss: 1.4383195638656616
Validation loss: 2.1977898875872293

Epoch: 6| Step: 8
Training loss: 1.9003973007202148
Validation loss: 2.2441662152608237

Epoch: 6| Step: 9
Training loss: 1.7423814535140991
Validation loss: 2.202755928039551

Epoch: 6| Step: 10
Training loss: 1.6089130640029907
Validation loss: 2.2313125133514404

Epoch: 6| Step: 11
Training loss: 2.1955623626708984
Validation loss: 2.200108448664347

Epoch: 6| Step: 12
Training loss: 1.3928474187850952
Validation loss: 2.2370636661847434

Epoch: 6| Step: 13
Training loss: 1.1883025169372559
Validation loss: 2.2366894682248435

Epoch: 249| Step: 0
Training loss: 2.216045379638672
Validation loss: 2.2147109707196555

Epoch: 6| Step: 1
Training loss: 1.540217638015747
Validation loss: 2.2376496394475303

Epoch: 6| Step: 2
Training loss: 1.5603017807006836
Validation loss: 2.2280513048171997

Epoch: 6| Step: 3
Training loss: 1.6387883424758911
Validation loss: 2.223165810108185

Epoch: 6| Step: 4
Training loss: 1.9755730628967285
Validation loss: 2.2193852265675864

Epoch: 6| Step: 5
Training loss: 1.2645200490951538
Validation loss: 2.202484210332235

Epoch: 6| Step: 6
Training loss: 1.7273763418197632
Validation loss: 2.2201443115870156

Epoch: 6| Step: 7
Training loss: 1.4592169523239136
Validation loss: 2.1974104046821594

Epoch: 6| Step: 8
Training loss: 2.4065957069396973
Validation loss: 2.2160077889760337

Epoch: 6| Step: 9
Training loss: 1.436413288116455
Validation loss: 2.183534026145935

Epoch: 6| Step: 10
Training loss: 1.6240534782409668
Validation loss: 2.201083461443583

Epoch: 6| Step: 11
Training loss: 1.3841288089752197
Validation loss: 2.233770489692688

Epoch: 6| Step: 12
Training loss: 1.6762454509735107
Validation loss: 2.192907432715098

Epoch: 6| Step: 13
Training loss: 1.6894632577896118
Validation loss: 2.1648155649503074

Epoch: 250| Step: 0
Training loss: 1.9069281816482544
Validation loss: 2.195448120435079

Epoch: 6| Step: 1
Training loss: 1.7346773147583008
Validation loss: 2.182669540246328

Epoch: 6| Step: 2
Training loss: 1.476603627204895
Validation loss: 2.183416426181793

Epoch: 6| Step: 3
Training loss: 1.834125280380249
Validation loss: 2.18392684062322

Epoch: 6| Step: 4
Training loss: 1.928943395614624
Validation loss: 2.178113321463267

Epoch: 6| Step: 5
Training loss: 1.8436775207519531
Validation loss: 2.199630379676819

Epoch: 6| Step: 6
Training loss: 0.9926350116729736
Validation loss: 2.206380784511566

Epoch: 6| Step: 7
Training loss: 1.3584004640579224
Validation loss: 2.217502474784851

Epoch: 6| Step: 8
Training loss: 1.8829386234283447
Validation loss: 2.2110642989476523

Epoch: 6| Step: 9
Training loss: 1.453908085823059
Validation loss: 2.191817363103231

Epoch: 6| Step: 10
Training loss: 2.0828487873077393
Validation loss: 2.208669126033783

Epoch: 6| Step: 11
Training loss: 2.2018399238586426
Validation loss: 2.2274728218714395

Epoch: 6| Step: 12
Training loss: 2.011026382446289
Validation loss: 2.21866766611735

Epoch: 6| Step: 13
Training loss: 1.6488020420074463
Validation loss: 2.2150980830192566

Epoch: 251| Step: 0
Training loss: 1.6524560451507568
Validation loss: 2.226885716120402

Epoch: 6| Step: 1
Training loss: 1.3018214702606201
Validation loss: 2.1833345890045166

Epoch: 6| Step: 2
Training loss: 2.111670732498169
Validation loss: 2.1750008265177407

Epoch: 6| Step: 3
Training loss: 1.8146041631698608
Validation loss: 2.1713799834251404

Epoch: 6| Step: 4
Training loss: 1.35430908203125
Validation loss: 2.159010092417399

Epoch: 6| Step: 5
Training loss: 1.6186280250549316
Validation loss: 2.178980032602946

Epoch: 6| Step: 6
Training loss: 1.7336666584014893
Validation loss: 2.176184356212616

Epoch: 6| Step: 7
Training loss: 1.4960378408432007
Validation loss: 2.1805027524630227

Epoch: 6| Step: 8
Training loss: 2.2016773223876953
Validation loss: 2.1863091786702475

Epoch: 6| Step: 9
Training loss: 1.466843843460083
Validation loss: 2.1814868648846946

Epoch: 6| Step: 10
Training loss: 1.6655102968215942
Validation loss: 2.204131066799164

Epoch: 6| Step: 11
Training loss: 1.1423392295837402
Validation loss: 2.2033007939656577

Epoch: 6| Step: 12
Training loss: 1.7878085374832153
Validation loss: 2.188956836859385

Epoch: 6| Step: 13
Training loss: 1.4641640186309814
Validation loss: 2.1707271536191306

Epoch: 252| Step: 0
Training loss: 1.3321802616119385
Validation loss: 2.2099300026893616

Epoch: 6| Step: 1
Training loss: 1.2472261190414429
Validation loss: 2.228482166926066

Epoch: 6| Step: 2
Training loss: 1.8037142753601074
Validation loss: 2.194737752278646

Epoch: 6| Step: 3
Training loss: 1.1012169122695923
Validation loss: 2.2187946240107217

Epoch: 6| Step: 4
Training loss: 1.5365605354309082
Validation loss: 2.229020833969116

Epoch: 6| Step: 5
Training loss: 1.068408727645874
Validation loss: 2.2121551831563315

Epoch: 6| Step: 6
Training loss: 1.8963944911956787
Validation loss: 2.233056823412577

Epoch: 6| Step: 7
Training loss: 1.6238147020339966
Validation loss: 2.220236817995707

Epoch: 6| Step: 8
Training loss: 1.9408583641052246
Validation loss: 2.2068960070610046

Epoch: 6| Step: 9
Training loss: 1.9035677909851074
Validation loss: 2.1597617864608765

Epoch: 6| Step: 10
Training loss: 1.9019405841827393
Validation loss: 2.1847232977549234

Epoch: 6| Step: 11
Training loss: 1.3055020570755005
Validation loss: 2.1766370932261148

Epoch: 6| Step: 12
Training loss: 1.726059913635254
Validation loss: 2.216445287068685

Epoch: 6| Step: 13
Training loss: 2.1380300521850586
Validation loss: 2.2395652135213218

Epoch: 253| Step: 0
Training loss: 1.3040101528167725
Validation loss: 2.2324682474136353

Epoch: 6| Step: 1
Training loss: 1.9755398035049438
Validation loss: 2.2100558082262673

Epoch: 6| Step: 2
Training loss: 1.9345207214355469
Validation loss: 2.2138384183247886

Epoch: 6| Step: 3
Training loss: 1.6984500885009766
Validation loss: 2.201169947783152

Epoch: 6| Step: 4
Training loss: 2.0987679958343506
Validation loss: 2.2148566444714866

Epoch: 6| Step: 5
Training loss: 1.406504511833191
Validation loss: 2.1843393246332803

Epoch: 6| Step: 6
Training loss: 1.188736915588379
Validation loss: 2.234111964702606

Epoch: 6| Step: 7
Training loss: 1.5445704460144043
Validation loss: 2.208286682764689

Epoch: 6| Step: 8
Training loss: 1.6962072849273682
Validation loss: 2.193323532740275

Epoch: 6| Step: 9
Training loss: 2.318140983581543
Validation loss: 2.1795954505602517

Epoch: 6| Step: 10
Training loss: 1.6401021480560303
Validation loss: 2.1942888498306274

Epoch: 6| Step: 11
Training loss: 1.5243275165557861
Validation loss: 2.1747240821520486

Epoch: 6| Step: 12
Training loss: 1.1698678731918335
Validation loss: 2.2179954846700034

Epoch: 6| Step: 13
Training loss: 1.3873714208602905
Validation loss: 2.206344207127889

Epoch: 254| Step: 0
Training loss: 1.3884761333465576
Validation loss: 2.2064809004465737

Epoch: 6| Step: 1
Training loss: 1.4625229835510254
Validation loss: 2.203417658805847

Epoch: 6| Step: 2
Training loss: 1.0772011280059814
Validation loss: 2.187174836794535

Epoch: 6| Step: 3
Training loss: 1.7835228443145752
Validation loss: 2.1992308100064597

Epoch: 6| Step: 4
Training loss: 1.2959449291229248
Validation loss: 2.1973396142323813

Epoch: 6| Step: 5
Training loss: 1.631093144416809
Validation loss: 2.220881164073944

Epoch: 6| Step: 6
Training loss: 1.1509628295898438
Validation loss: 2.205450256665548

Epoch: 6| Step: 7
Training loss: 1.6446672677993774
Validation loss: 2.227769593397776

Epoch: 6| Step: 8
Training loss: 1.5444796085357666
Validation loss: 2.2142892678578696

Epoch: 6| Step: 9
Training loss: 1.246113657951355
Validation loss: 2.213769316673279

Epoch: 6| Step: 10
Training loss: 2.017077922821045
Validation loss: 2.216265539328257

Epoch: 6| Step: 11
Training loss: 2.2945804595947266
Validation loss: 2.1869450012842813

Epoch: 6| Step: 12
Training loss: 1.6754838228225708
Validation loss: 2.2055216828982034

Epoch: 6| Step: 13
Training loss: 2.093348979949951
Validation loss: 2.204427699247996

Epoch: 255| Step: 0
Training loss: 1.584631323814392
Validation loss: 2.19478178024292

Epoch: 6| Step: 1
Training loss: 1.4416738748550415
Validation loss: 2.2122740546862283

Epoch: 6| Step: 2
Training loss: 2.4181199073791504
Validation loss: 2.1978023449579873

Epoch: 6| Step: 3
Training loss: 1.855582356452942
Validation loss: 2.225343406200409

Epoch: 6| Step: 4
Training loss: 1.439314603805542
Validation loss: 2.1789031823476157

Epoch: 6| Step: 5
Training loss: 0.8040690422058105
Validation loss: 2.1827534238497415

Epoch: 6| Step: 6
Training loss: 1.5601826906204224
Validation loss: 2.1726499795913696

Epoch: 6| Step: 7
Training loss: 1.3061702251434326
Validation loss: 2.147846202055613

Epoch: 6| Step: 8
Training loss: 1.8543972969055176
Validation loss: 2.1555439035097756

Epoch: 6| Step: 9
Training loss: 1.6553575992584229
Validation loss: 2.163813134034475

Epoch: 6| Step: 10
Training loss: 1.5108013153076172
Validation loss: 2.158179541428884

Epoch: 6| Step: 11
Training loss: 1.7060729265213013
Validation loss: 2.192114551862081

Epoch: 6| Step: 12
Training loss: 1.4698182344436646
Validation loss: 2.180719256401062

Epoch: 6| Step: 13
Training loss: 1.48738431930542
Validation loss: 2.164342919985453

Epoch: 256| Step: 0
Training loss: 1.3428947925567627
Validation loss: 2.175349692503611

Epoch: 6| Step: 1
Training loss: 1.595231294631958
Validation loss: 2.158754567305247

Epoch: 6| Step: 2
Training loss: 2.0485873222351074
Validation loss: 2.167350093523661

Epoch: 6| Step: 3
Training loss: 1.5346826314926147
Validation loss: 2.1785671710968018

Epoch: 6| Step: 4
Training loss: 1.2168591022491455
Validation loss: 2.1612724463144937

Epoch: 6| Step: 5
Training loss: 2.242882490158081
Validation loss: 2.160894791285197

Epoch: 6| Step: 6
Training loss: 1.323925495147705
Validation loss: 2.177831987539927

Epoch: 6| Step: 7
Training loss: 1.2853856086730957
Validation loss: 2.170512239138285

Epoch: 6| Step: 8
Training loss: 2.483839750289917
Validation loss: 2.1976993878682456

Epoch: 6| Step: 9
Training loss: 1.6608011722564697
Validation loss: 2.182951033115387

Epoch: 6| Step: 10
Training loss: 2.000159740447998
Validation loss: 2.178087294101715

Epoch: 6| Step: 11
Training loss: 0.9341819286346436
Validation loss: 2.2127473950386047

Epoch: 6| Step: 12
Training loss: 2.1706066131591797
Validation loss: 2.183356523513794

Epoch: 6| Step: 13
Training loss: 2.160998582839966
Validation loss: 2.1837971011797586

Epoch: 257| Step: 0
Training loss: 1.6066927909851074
Validation loss: 2.2060723106066384

Epoch: 6| Step: 1
Training loss: 1.4616796970367432
Validation loss: 2.2095263799031577

Epoch: 6| Step: 2
Training loss: 1.8003194332122803
Validation loss: 2.213310738404592

Epoch: 6| Step: 3
Training loss: 1.777113914489746
Validation loss: 2.205726067225138

Epoch: 6| Step: 4
Training loss: 1.5922852754592896
Validation loss: 2.2330121397972107

Epoch: 6| Step: 5
Training loss: 1.6749452352523804
Validation loss: 2.2076674898465476

Epoch: 6| Step: 6
Training loss: 1.4751818180084229
Validation loss: 2.2146424651145935

Epoch: 6| Step: 7
Training loss: 1.510831356048584
Validation loss: 2.216455399990082

Epoch: 6| Step: 8
Training loss: 1.3815710544586182
Validation loss: 2.21885351339976

Epoch: 6| Step: 9
Training loss: 1.6114130020141602
Validation loss: 2.239506483078003

Epoch: 6| Step: 10
Training loss: 1.364271640777588
Validation loss: 2.21956737836202

Epoch: 6| Step: 11
Training loss: 2.0656535625457764
Validation loss: 2.1876168847084045

Epoch: 6| Step: 12
Training loss: 0.9992573261260986
Validation loss: 2.1917086044947305

Epoch: 6| Step: 13
Training loss: 2.0051426887512207
Validation loss: 2.1899020075798035

Epoch: 258| Step: 0
Training loss: 1.9143019914627075
Validation loss: 2.1918976505597434

Epoch: 6| Step: 1
Training loss: 1.903062105178833
Validation loss: 2.1846299370129905

Epoch: 6| Step: 2
Training loss: 1.1468095779418945
Validation loss: 2.1886817614237466

Epoch: 6| Step: 3
Training loss: 1.45021653175354
Validation loss: 2.2125243743260703

Epoch: 6| Step: 4
Training loss: 2.1252400875091553
Validation loss: 2.2216457525889077

Epoch: 6| Step: 5
Training loss: 1.0331413745880127
Validation loss: 2.215982814629873

Epoch: 6| Step: 6
Training loss: 1.411712646484375
Validation loss: 2.2158703406651816

Epoch: 6| Step: 7
Training loss: 1.4981375932693481
Validation loss: 2.207600732644399

Epoch: 6| Step: 8
Training loss: 1.4944100379943848
Validation loss: 2.2225899696350098

Epoch: 6| Step: 9
Training loss: 2.344879627227783
Validation loss: 2.2178106904029846

Epoch: 6| Step: 10
Training loss: 1.5579921007156372
Validation loss: 2.2278380592664084

Epoch: 6| Step: 11
Training loss: 2.0271248817443848
Validation loss: 2.2350406448046365

Epoch: 6| Step: 12
Training loss: 0.9192267656326294
Validation loss: 2.215594788392385

Epoch: 6| Step: 13
Training loss: 0.9854342937469482
Validation loss: 2.1965612173080444

Epoch: 259| Step: 0
Training loss: 1.3286032676696777
Validation loss: 2.207334041595459

Epoch: 6| Step: 1
Training loss: 1.2062339782714844
Validation loss: 2.18363489707311

Epoch: 6| Step: 2
Training loss: 2.003631353378296
Validation loss: 2.197724680105845

Epoch: 6| Step: 3
Training loss: 1.6866259574890137
Validation loss: 2.2007316946983337

Epoch: 6| Step: 4
Training loss: 2.0277013778686523
Validation loss: 2.1833564043045044

Epoch: 6| Step: 5
Training loss: 1.136494517326355
Validation loss: 2.1996969978014627

Epoch: 6| Step: 6
Training loss: 1.8802037239074707
Validation loss: 2.190225680669149

Epoch: 6| Step: 7
Training loss: 1.7940667867660522
Validation loss: 2.1906529863675437

Epoch: 6| Step: 8
Training loss: 1.5027832984924316
Validation loss: 2.206968069076538

Epoch: 6| Step: 9
Training loss: 1.7182822227478027
Validation loss: 2.2213702400525412

Epoch: 6| Step: 10
Training loss: 1.1445138454437256
Validation loss: 2.2173947294553122

Epoch: 6| Step: 11
Training loss: 1.5555198192596436
Validation loss: 2.2168884873390198

Epoch: 6| Step: 12
Training loss: 1.0890283584594727
Validation loss: 2.2418406009674072

Epoch: 6| Step: 13
Training loss: 1.6192042827606201
Validation loss: 2.2268131375312805

Epoch: 260| Step: 0
Training loss: 2.2038869857788086
Validation loss: 2.259754180908203

Epoch: 6| Step: 1
Training loss: 1.2419850826263428
Validation loss: 2.2405036290486655

Epoch: 6| Step: 2
Training loss: 1.7189379930496216
Validation loss: 2.237367033958435

Epoch: 6| Step: 3
Training loss: 1.5158803462982178
Validation loss: 2.2267699241638184

Epoch: 6| Step: 4
Training loss: 1.6608898639678955
Validation loss: 2.2126721342404685

Epoch: 6| Step: 5
Training loss: 1.3428711891174316
Validation loss: 2.170323371887207

Epoch: 6| Step: 6
Training loss: 0.9637947082519531
Validation loss: 2.212167501449585

Epoch: 6| Step: 7
Training loss: 1.6462135314941406
Validation loss: 2.2048933506011963

Epoch: 6| Step: 8
Training loss: 1.867734432220459
Validation loss: 2.210510532061259

Epoch: 6| Step: 9
Training loss: 1.2018961906433105
Validation loss: 2.2125873963038125

Epoch: 6| Step: 10
Training loss: 1.5159330368041992
Validation loss: 2.220910350481669

Epoch: 6| Step: 11
Training loss: 1.5067613124847412
Validation loss: 2.201028068860372

Epoch: 6| Step: 12
Training loss: 1.4558286666870117
Validation loss: 2.2381985982259116

Epoch: 6| Step: 13
Training loss: 1.6240332126617432
Validation loss: 2.2328051328659058

Epoch: 261| Step: 0
Training loss: 1.580049991607666
Validation loss: 2.1925421754519143

Epoch: 6| Step: 1
Training loss: 0.8827430605888367
Validation loss: 2.1893691619237265

Epoch: 6| Step: 2
Training loss: 1.705452561378479
Validation loss: 2.2116421858469644

Epoch: 6| Step: 3
Training loss: 1.8819770812988281
Validation loss: 2.2195924520492554

Epoch: 6| Step: 4
Training loss: 1.4328985214233398
Validation loss: 2.209891378879547

Epoch: 6| Step: 5
Training loss: 2.1751339435577393
Validation loss: 2.2058499654134116

Epoch: 6| Step: 6
Training loss: 1.4772834777832031
Validation loss: 2.19944175084432

Epoch: 6| Step: 7
Training loss: 1.5572667121887207
Validation loss: 2.1949557264645896

Epoch: 6| Step: 8
Training loss: 1.717665433883667
Validation loss: 2.2027353644371033

Epoch: 6| Step: 9
Training loss: 1.4946529865264893
Validation loss: 2.235454797744751

Epoch: 6| Step: 10
Training loss: 1.5715535879135132
Validation loss: 2.248548746109009

Epoch: 6| Step: 11
Training loss: 1.5026460886001587
Validation loss: 2.2306819955507913

Epoch: 6| Step: 12
Training loss: 0.8402303457260132
Validation loss: 2.2409814596176147

Epoch: 6| Step: 13
Training loss: 1.3754276037216187
Validation loss: 2.230516235033671

Epoch: 262| Step: 0
Training loss: 1.5881199836730957
Validation loss: 2.2348440488179526

Epoch: 6| Step: 1
Training loss: 1.3358018398284912
Validation loss: 2.226789653301239

Epoch: 6| Step: 2
Training loss: 1.1030575037002563
Validation loss: 2.2358588774998984

Epoch: 6| Step: 3
Training loss: 1.6898365020751953
Validation loss: 2.231327017148336

Epoch: 6| Step: 4
Training loss: 1.7212605476379395
Validation loss: 2.239228983720144

Epoch: 6| Step: 5
Training loss: 1.7175068855285645
Validation loss: 2.252600351969401

Epoch: 6| Step: 6
Training loss: 1.699766755104065
Validation loss: 2.22017365694046

Epoch: 6| Step: 7
Training loss: 1.219059944152832
Validation loss: 2.227263887723287

Epoch: 6| Step: 8
Training loss: 1.9818634986877441
Validation loss: 2.233080824216207

Epoch: 6| Step: 9
Training loss: 0.9850478172302246
Validation loss: 2.2371294498443604

Epoch: 6| Step: 10
Training loss: 1.5852077007293701
Validation loss: 2.1895461082458496

Epoch: 6| Step: 11
Training loss: 1.6086535453796387
Validation loss: 2.1957122484842935

Epoch: 6| Step: 12
Training loss: 1.1452006101608276
Validation loss: 2.1827922463417053

Epoch: 6| Step: 13
Training loss: 1.7791330814361572
Validation loss: 2.1830619970957437

Epoch: 263| Step: 0
Training loss: 2.0962963104248047
Validation loss: 2.187462011973063

Epoch: 6| Step: 1
Training loss: 2.050466537475586
Validation loss: 2.208746353785197

Epoch: 6| Step: 2
Training loss: 1.475757122039795
Validation loss: 2.1900999546051025

Epoch: 6| Step: 3
Training loss: 1.5133333206176758
Validation loss: 2.2133368253707886

Epoch: 6| Step: 4
Training loss: 1.6613521575927734
Validation loss: 2.2112523714701333

Epoch: 6| Step: 5
Training loss: 1.6578590869903564
Validation loss: 2.164509395758311

Epoch: 6| Step: 6
Training loss: 1.849801778793335
Validation loss: 2.210776150226593

Epoch: 6| Step: 7
Training loss: 1.3451058864593506
Validation loss: 2.215688387552897

Epoch: 6| Step: 8
Training loss: 1.7127058506011963
Validation loss: 2.2126883467038474

Epoch: 6| Step: 9
Training loss: 1.0061860084533691
Validation loss: 2.178458650906881

Epoch: 6| Step: 10
Training loss: 1.059572696685791
Validation loss: 2.2246872782707214

Epoch: 6| Step: 11
Training loss: 1.403285264968872
Validation loss: 2.220682760079702

Epoch: 6| Step: 12
Training loss: 1.071183204650879
Validation loss: 2.2257768710454306

Epoch: 6| Step: 13
Training loss: 1.7340216636657715
Validation loss: 2.23894856373469

Epoch: 264| Step: 0
Training loss: 1.6808485984802246
Validation loss: 2.208954314390818

Epoch: 6| Step: 1
Training loss: 1.6643987894058228
Validation loss: 2.235575040181478

Epoch: 6| Step: 2
Training loss: 1.2448703050613403
Validation loss: 2.2064284284909568

Epoch: 6| Step: 3
Training loss: 2.0110058784484863
Validation loss: 2.215581556161245

Epoch: 6| Step: 4
Training loss: 1.2315795421600342
Validation loss: 2.2644607623418174

Epoch: 6| Step: 5
Training loss: 1.4534342288970947
Validation loss: 2.2408685882886252

Epoch: 6| Step: 6
Training loss: 1.7403428554534912
Validation loss: 2.228507181008657

Epoch: 6| Step: 7
Training loss: 1.1818817853927612
Validation loss: 2.195918599764506

Epoch: 6| Step: 8
Training loss: 1.0416655540466309
Validation loss: 2.2258930603663125

Epoch: 6| Step: 9
Training loss: 1.1257896423339844
Validation loss: 2.2295917669932046

Epoch: 6| Step: 10
Training loss: 1.6941804885864258
Validation loss: 2.2264228463172913

Epoch: 6| Step: 11
Training loss: 1.8713608980178833
Validation loss: 2.211058775583903

Epoch: 6| Step: 12
Training loss: 1.6022398471832275
Validation loss: 2.210192600886027

Epoch: 6| Step: 13
Training loss: 1.6903175115585327
Validation loss: 2.234772562980652

Epoch: 265| Step: 0
Training loss: 1.3616878986358643
Validation loss: 2.1817566553751626

Epoch: 6| Step: 1
Training loss: 1.4463775157928467
Validation loss: 2.1829140384991965

Epoch: 6| Step: 2
Training loss: 1.3029272556304932
Validation loss: 2.193784932295481

Epoch: 6| Step: 3
Training loss: 2.003164291381836
Validation loss: 2.1793708403905234

Epoch: 6| Step: 4
Training loss: 1.4102966785430908
Validation loss: 2.199572722117106

Epoch: 6| Step: 5
Training loss: 1.517418384552002
Validation loss: 2.203768491744995

Epoch: 6| Step: 6
Training loss: 1.6342806816101074
Validation loss: 2.2031201918919883

Epoch: 6| Step: 7
Training loss: 1.620452880859375
Validation loss: 2.213994642098745

Epoch: 6| Step: 8
Training loss: 1.4190516471862793
Validation loss: 2.2268390456835427

Epoch: 6| Step: 9
Training loss: 1.4192345142364502
Validation loss: 2.207819084326426

Epoch: 6| Step: 10
Training loss: 1.5364758968353271
Validation loss: 2.220719496409098

Epoch: 6| Step: 11
Training loss: 1.0684070587158203
Validation loss: 2.1980985601743064

Epoch: 6| Step: 12
Training loss: 1.207794189453125
Validation loss: 2.1928738951683044

Epoch: 6| Step: 13
Training loss: 1.7190709114074707
Validation loss: 2.1790528893470764

Epoch: 266| Step: 0
Training loss: 1.364030122756958
Validation loss: 2.185247222582499

Epoch: 6| Step: 1
Training loss: 1.5966050624847412
Validation loss: 2.1772875785827637

Epoch: 6| Step: 2
Training loss: 0.8979429602622986
Validation loss: 2.16935525337855

Epoch: 6| Step: 3
Training loss: 1.6803016662597656
Validation loss: 2.150684952735901

Epoch: 6| Step: 4
Training loss: 1.6023293733596802
Validation loss: 2.1595118045806885

Epoch: 6| Step: 5
Training loss: 1.9638035297393799
Validation loss: 2.1395357847213745

Epoch: 6| Step: 6
Training loss: 1.5137498378753662
Validation loss: 2.143442749977112

Epoch: 6| Step: 7
Training loss: 1.8266761302947998
Validation loss: 2.157438059647878

Epoch: 6| Step: 8
Training loss: 1.2306509017944336
Validation loss: 2.17944206794103

Epoch: 6| Step: 9
Training loss: 1.1619771718978882
Validation loss: 2.176464339097341

Epoch: 6| Step: 10
Training loss: 1.0221738815307617
Validation loss: 2.186023453871409

Epoch: 6| Step: 11
Training loss: 1.9455931186676025
Validation loss: 2.1907585064570108

Epoch: 6| Step: 12
Training loss: 1.5766098499298096
Validation loss: 2.201076328754425

Epoch: 6| Step: 13
Training loss: 1.2487566471099854
Validation loss: 2.192637085914612

Epoch: 267| Step: 0
Training loss: 1.4736944437026978
Validation loss: 2.166457931200663

Epoch: 6| Step: 1
Training loss: 1.6866614818572998
Validation loss: 2.22067791223526

Epoch: 6| Step: 2
Training loss: 1.7847015857696533
Validation loss: 2.2199421326319375

Epoch: 6| Step: 3
Training loss: 1.3893203735351562
Validation loss: 2.222013314565023

Epoch: 6| Step: 4
Training loss: 1.2562806606292725
Validation loss: 2.210096836090088

Epoch: 6| Step: 5
Training loss: 1.4641633033752441
Validation loss: 2.2015963196754456

Epoch: 6| Step: 6
Training loss: 0.7961677312850952
Validation loss: 2.2323467334111533

Epoch: 6| Step: 7
Training loss: 1.457765817642212
Validation loss: 2.207246402899424

Epoch: 6| Step: 8
Training loss: 1.2126909494400024
Validation loss: 2.2106711069742837

Epoch: 6| Step: 9
Training loss: 2.205491304397583
Validation loss: 2.2114707231521606

Epoch: 6| Step: 10
Training loss: 1.1158167123794556
Validation loss: 2.222093482812246

Epoch: 6| Step: 11
Training loss: 1.7488981485366821
Validation loss: 2.2453267772992453

Epoch: 6| Step: 12
Training loss: 1.6730958223342896
Validation loss: 2.243176519870758

Epoch: 6| Step: 13
Training loss: 1.944963812828064
Validation loss: 2.2304018139839172

Epoch: 268| Step: 0
Training loss: 1.3333630561828613
Validation loss: 2.213424801826477

Epoch: 6| Step: 1
Training loss: 2.3085975646972656
Validation loss: 2.224921782811483

Epoch: 6| Step: 2
Training loss: 1.3587658405303955
Validation loss: 2.208495080471039

Epoch: 6| Step: 3
Training loss: 1.5460563898086548
Validation loss: 2.207235852877299

Epoch: 6| Step: 4
Training loss: 1.7122561931610107
Validation loss: 2.2005234162012735

Epoch: 6| Step: 5
Training loss: 1.7150204181671143
Validation loss: 2.222202181816101

Epoch: 6| Step: 6
Training loss: 0.8692779541015625
Validation loss: 2.2068201700846353

Epoch: 6| Step: 7
Training loss: 1.810171365737915
Validation loss: 2.192229688167572

Epoch: 6| Step: 8
Training loss: 1.892225980758667
Validation loss: 2.196246087551117

Epoch: 6| Step: 9
Training loss: 1.1692724227905273
Validation loss: 2.187804639339447

Epoch: 6| Step: 10
Training loss: 2.010100841522217
Validation loss: 2.189345439275106

Epoch: 6| Step: 11
Training loss: 0.9717704057693481
Validation loss: 2.1614545981089273

Epoch: 6| Step: 12
Training loss: 0.9773423671722412
Validation loss: 2.2015032370885215

Epoch: 6| Step: 13
Training loss: 1.5199288129806519
Validation loss: 2.160557230313619

Epoch: 269| Step: 0
Training loss: 1.7869471311569214
Validation loss: 2.2092703183492026

Epoch: 6| Step: 1
Training loss: 0.8926565647125244
Validation loss: 2.18942399819692

Epoch: 6| Step: 2
Training loss: 1.439675211906433
Validation loss: 2.1697667439778647

Epoch: 6| Step: 3
Training loss: 1.6896483898162842
Validation loss: 2.188223580519358

Epoch: 6| Step: 4
Training loss: 1.1968671083450317
Validation loss: 2.1922499736150107

Epoch: 6| Step: 5
Training loss: 1.4627273082733154
Validation loss: 2.1892558336257935

Epoch: 6| Step: 6
Training loss: 0.9430133104324341
Validation loss: 2.2148016691207886

Epoch: 6| Step: 7
Training loss: 1.5076959133148193
Validation loss: 2.2134615778923035

Epoch: 6| Step: 8
Training loss: 1.7715903520584106
Validation loss: 2.2099654277165732

Epoch: 6| Step: 9
Training loss: 1.7485419511795044
Validation loss: 2.1978584130605063

Epoch: 6| Step: 10
Training loss: 1.4061827659606934
Validation loss: 2.2187608083089194

Epoch: 6| Step: 11
Training loss: 1.963371753692627
Validation loss: 2.2209574381510415

Epoch: 6| Step: 12
Training loss: 1.4077212810516357
Validation loss: 2.2207356095314026

Epoch: 6| Step: 13
Training loss: 1.1921536922454834
Validation loss: 2.2081631819407144

Epoch: 270| Step: 0
Training loss: 1.3799468278884888
Validation loss: 2.2094254891077676

Epoch: 6| Step: 1
Training loss: 1.925558090209961
Validation loss: 2.2077248891194663

Epoch: 6| Step: 2
Training loss: 1.2185920476913452
Validation loss: 2.1881763339042664

Epoch: 6| Step: 3
Training loss: 1.6446160078048706
Validation loss: 2.2115147709846497

Epoch: 6| Step: 4
Training loss: 1.0019629001617432
Validation loss: 2.2109591960906982

Epoch: 6| Step: 5
Training loss: 1.299058437347412
Validation loss: 2.241746664047241

Epoch: 6| Step: 6
Training loss: 2.437074661254883
Validation loss: 2.212313095728556

Epoch: 6| Step: 7
Training loss: 1.5511349439620972
Validation loss: 2.209186593691508

Epoch: 6| Step: 8
Training loss: 1.4716466665267944
Validation loss: 2.208328366279602

Epoch: 6| Step: 9
Training loss: 1.3505876064300537
Validation loss: 2.1965869863828025

Epoch: 6| Step: 10
Training loss: 1.0864622592926025
Validation loss: 2.2051321069399514

Epoch: 6| Step: 11
Training loss: 1.0487594604492188
Validation loss: 2.2280917167663574

Epoch: 6| Step: 12
Training loss: 2.094740867614746
Validation loss: 2.2349069317181907

Epoch: 6| Step: 13
Training loss: 1.8486976623535156
Validation loss: 2.2505855560302734

Epoch: 271| Step: 0
Training loss: 1.219425082206726
Validation loss: 2.255668342113495

Epoch: 6| Step: 1
Training loss: 1.103929042816162
Validation loss: 2.222114602724711

Epoch: 6| Step: 2
Training loss: 1.4865095615386963
Validation loss: 2.2049790620803833

Epoch: 6| Step: 3
Training loss: 1.434934377670288
Validation loss: 2.2279810309410095

Epoch: 6| Step: 4
Training loss: 1.7526741027832031
Validation loss: 2.2240450183550515

Epoch: 6| Step: 5
Training loss: 2.2104275226593018
Validation loss: 2.2052817940711975

Epoch: 6| Step: 6
Training loss: 1.039475440979004
Validation loss: 2.2060028314590454

Epoch: 6| Step: 7
Training loss: 2.2343640327453613
Validation loss: 2.224139710267385

Epoch: 6| Step: 8
Training loss: 2.098393440246582
Validation loss: 2.2396352092425027

Epoch: 6| Step: 9
Training loss: 0.8193569183349609
Validation loss: 2.1960431337356567

Epoch: 6| Step: 10
Training loss: 1.2321460247039795
Validation loss: 2.2002989451090493

Epoch: 6| Step: 11
Training loss: 1.6070730686187744
Validation loss: 2.2049561738967896

Epoch: 6| Step: 12
Training loss: 1.8474633693695068
Validation loss: 2.2038849194844565

Epoch: 6| Step: 13
Training loss: 1.1040496826171875
Validation loss: 2.200680196285248

Epoch: 272| Step: 0
Training loss: 0.9984384775161743
Validation loss: 2.201706329981486

Epoch: 6| Step: 1
Training loss: 1.4654297828674316
Validation loss: 2.179754853248596

Epoch: 6| Step: 2
Training loss: 1.3877952098846436
Validation loss: 2.2000663677851358

Epoch: 6| Step: 3
Training loss: 1.401510238647461
Validation loss: 2.2125083208084106

Epoch: 6| Step: 4
Training loss: 1.8122649192810059
Validation loss: 2.2119881312052407

Epoch: 6| Step: 5
Training loss: 1.19279146194458
Validation loss: 2.2346470952033997

Epoch: 6| Step: 6
Training loss: 1.5912895202636719
Validation loss: 2.2007172306378684

Epoch: 6| Step: 7
Training loss: 1.5464580059051514
Validation loss: 2.202169199784597

Epoch: 6| Step: 8
Training loss: 1.555162787437439
Validation loss: 2.2219847440719604

Epoch: 6| Step: 9
Training loss: 1.6653285026550293
Validation loss: 2.185307820638021

Epoch: 6| Step: 10
Training loss: 1.331428050994873
Validation loss: 2.1944494247436523

Epoch: 6| Step: 11
Training loss: 1.5476720333099365
Validation loss: 2.234327793121338

Epoch: 6| Step: 12
Training loss: 2.6085026264190674
Validation loss: 2.213588992754618

Epoch: 6| Step: 13
Training loss: 0.9360305666923523
Validation loss: 2.216022034486135

Epoch: 273| Step: 0
Training loss: 0.88178551197052
Validation loss: 2.2200491031010947

Epoch: 6| Step: 1
Training loss: 1.3388346433639526
Validation loss: 2.2112303376197815

Epoch: 6| Step: 2
Training loss: 1.481942057609558
Validation loss: 2.216722766558329

Epoch: 6| Step: 3
Training loss: 1.3026151657104492
Validation loss: 2.247794270515442

Epoch: 6| Step: 4
Training loss: 1.8482909202575684
Validation loss: 2.232937296231588

Epoch: 6| Step: 5
Training loss: 0.9654215574264526
Validation loss: 2.211254636446635

Epoch: 6| Step: 6
Training loss: 1.7300652265548706
Validation loss: 2.260676701863607

Epoch: 6| Step: 7
Training loss: 1.4645578861236572
Validation loss: 2.2294025818506875

Epoch: 6| Step: 8
Training loss: 1.7595030069351196
Validation loss: 2.2204423546791077

Epoch: 6| Step: 9
Training loss: 1.6598529815673828
Validation loss: 2.2233612736066184

Epoch: 6| Step: 10
Training loss: 1.810117244720459
Validation loss: 2.221300264199575

Epoch: 6| Step: 11
Training loss: 1.305261492729187
Validation loss: 2.2356939911842346

Epoch: 6| Step: 12
Training loss: 1.0230443477630615
Validation loss: 2.1951557795206704

Epoch: 6| Step: 13
Training loss: 1.977921485900879
Validation loss: 2.20936918258667

Epoch: 274| Step: 0
Training loss: 1.6460351943969727
Validation loss: 2.2531129717826843

Epoch: 6| Step: 1
Training loss: 0.7332249879837036
Validation loss: 2.227028171221415

Epoch: 6| Step: 2
Training loss: 1.443058967590332
Validation loss: 2.1843077143033347

Epoch: 6| Step: 3
Training loss: 1.8436174392700195
Validation loss: 2.196356256802877

Epoch: 6| Step: 4
Training loss: 1.2379121780395508
Validation loss: 2.1979031960169473

Epoch: 6| Step: 5
Training loss: 2.021080493927002
Validation loss: 2.2174771428108215

Epoch: 6| Step: 6
Training loss: 1.4456627368927002
Validation loss: 2.1644874811172485

Epoch: 6| Step: 7
Training loss: 1.2898597717285156
Validation loss: 2.188444952170054

Epoch: 6| Step: 8
Training loss: 1.6631462574005127
Validation loss: 2.1790279746055603

Epoch: 6| Step: 9
Training loss: 1.9260319471359253
Validation loss: 2.203973571459452

Epoch: 6| Step: 10
Training loss: 1.5683181285858154
Validation loss: 2.2225040396054587

Epoch: 6| Step: 11
Training loss: 2.052039623260498
Validation loss: 2.253338038921356

Epoch: 6| Step: 12
Training loss: 1.3691571950912476
Validation loss: 2.1662019888559976

Epoch: 6| Step: 13
Training loss: 1.6233538389205933
Validation loss: 2.2251179814338684

Epoch: 275| Step: 0
Training loss: 1.011185884475708
Validation loss: 2.1738662918408713

Epoch: 6| Step: 1
Training loss: 1.2003127336502075
Validation loss: 2.195486048857371

Epoch: 6| Step: 2
Training loss: 1.5327227115631104
Validation loss: 2.1799676616986594

Epoch: 6| Step: 3
Training loss: 1.1498318910598755
Validation loss: 2.17273086309433

Epoch: 6| Step: 4
Training loss: 1.1612964868545532
Validation loss: 2.1663436889648438

Epoch: 6| Step: 5
Training loss: 1.7032842636108398
Validation loss: 2.202880601088206

Epoch: 6| Step: 6
Training loss: 1.629408359527588
Validation loss: 2.1647849877675376

Epoch: 6| Step: 7
Training loss: 1.3043266534805298
Validation loss: 2.160513997077942

Epoch: 6| Step: 8
Training loss: 1.484345555305481
Validation loss: 2.2040505409240723

Epoch: 6| Step: 9
Training loss: 1.704384207725525
Validation loss: 2.1780014832814536

Epoch: 6| Step: 10
Training loss: 1.2579255104064941
Validation loss: 2.191070079803467

Epoch: 6| Step: 11
Training loss: 1.414918303489685
Validation loss: 2.2000660498936973

Epoch: 6| Step: 12
Training loss: 2.1182057857513428
Validation loss: 2.1757344007492065

Epoch: 6| Step: 13
Training loss: 2.0194554328918457
Validation loss: 2.1913729310035706

Epoch: 276| Step: 0
Training loss: 1.040724754333496
Validation loss: 2.181931972503662

Epoch: 6| Step: 1
Training loss: 1.0606173276901245
Validation loss: 2.236051599184672

Epoch: 6| Step: 2
Training loss: 2.0865983963012695
Validation loss: 2.2184630235036216

Epoch: 6| Step: 3
Training loss: 2.7420401573181152
Validation loss: 2.2028207182884216

Epoch: 6| Step: 4
Training loss: 1.5029170513153076
Validation loss: 2.2109469374020896

Epoch: 6| Step: 5
Training loss: 1.0956199169158936
Validation loss: 2.218699892361959

Epoch: 6| Step: 6
Training loss: 1.7319166660308838
Validation loss: 2.2020110885302224

Epoch: 6| Step: 7
Training loss: 0.8927828073501587
Validation loss: 2.195641040802002

Epoch: 6| Step: 8
Training loss: 1.6742017269134521
Validation loss: 2.192369600137075

Epoch: 6| Step: 9
Training loss: 0.6028971672058105
Validation loss: 2.1773907939592996

Epoch: 6| Step: 10
Training loss: 2.087148666381836
Validation loss: 2.199534833431244

Epoch: 6| Step: 11
Training loss: 0.9926849603652954
Validation loss: 2.198637863000234

Epoch: 6| Step: 12
Training loss: 1.3593456745147705
Validation loss: 2.1703785260518393

Epoch: 6| Step: 13
Training loss: 1.5886801481246948
Validation loss: 2.1900875171025596

Epoch: 277| Step: 0
Training loss: 1.318056344985962
Validation loss: 2.168986697991689

Epoch: 6| Step: 1
Training loss: 1.5407336950302124
Validation loss: 2.212719738483429

Epoch: 6| Step: 2
Training loss: 1.5758516788482666
Validation loss: 2.19550096988678

Epoch: 6| Step: 3
Training loss: 1.7407011985778809
Validation loss: 2.2108563979466758

Epoch: 6| Step: 4
Training loss: 1.413191318511963
Validation loss: 2.186148206392924

Epoch: 6| Step: 5
Training loss: 1.66701078414917
Validation loss: 2.196354607741038

Epoch: 6| Step: 6
Training loss: 0.8903998732566833
Validation loss: 2.2252790927886963

Epoch: 6| Step: 7
Training loss: 1.3187081813812256
Validation loss: 2.2364089488983154

Epoch: 6| Step: 8
Training loss: 1.8961460590362549
Validation loss: 2.2381338874499

Epoch: 6| Step: 9
Training loss: 1.4135773181915283
Validation loss: 2.240519165992737

Epoch: 6| Step: 10
Training loss: 1.4734996557235718
Validation loss: 2.1929314732551575

Epoch: 6| Step: 11
Training loss: 1.9607746601104736
Validation loss: 2.1931192874908447

Epoch: 6| Step: 12
Training loss: 1.3278642892837524
Validation loss: 2.180631995201111

Epoch: 6| Step: 13
Training loss: 0.9719624519348145
Validation loss: 2.198099672794342

Epoch: 278| Step: 0
Training loss: 1.020798921585083
Validation loss: 2.1846790711085

Epoch: 6| Step: 1
Training loss: 1.6167038679122925
Validation loss: 2.242399255434672

Epoch: 6| Step: 2
Training loss: 1.345251441001892
Validation loss: 2.192569633324941

Epoch: 6| Step: 3
Training loss: 1.3006590604782104
Validation loss: 2.2164551417032876

Epoch: 6| Step: 4
Training loss: 1.9664862155914307
Validation loss: 2.2111399372418723

Epoch: 6| Step: 5
Training loss: 1.988213300704956
Validation loss: 2.178732216358185

Epoch: 6| Step: 6
Training loss: 1.4344830513000488
Validation loss: 2.181830128033956

Epoch: 6| Step: 7
Training loss: 1.101749300956726
Validation loss: 2.229290167490641

Epoch: 6| Step: 8
Training loss: 1.355583667755127
Validation loss: 2.2204740842183432

Epoch: 6| Step: 9
Training loss: 0.8325071334838867
Validation loss: 2.232406755288442

Epoch: 6| Step: 10
Training loss: 1.8563525676727295
Validation loss: 2.234435200691223

Epoch: 6| Step: 11
Training loss: 1.3078300952911377
Validation loss: 2.2261420289675393

Epoch: 6| Step: 12
Training loss: 2.2081172466278076
Validation loss: 2.2330721020698547

Epoch: 6| Step: 13
Training loss: 1.7035138607025146
Validation loss: 2.205395221710205

Epoch: 279| Step: 0
Training loss: 1.3001384735107422
Validation loss: 2.2017333110173545

Epoch: 6| Step: 1
Training loss: 2.0426511764526367
Validation loss: 2.193815032641093

Epoch: 6| Step: 2
Training loss: 1.0184087753295898
Validation loss: 2.244135856628418

Epoch: 6| Step: 3
Training loss: 1.3663769960403442
Validation loss: 2.2502335906028748

Epoch: 6| Step: 4
Training loss: 1.0668023824691772
Validation loss: 2.229118744532267

Epoch: 6| Step: 5
Training loss: 1.911723256111145
Validation loss: 2.2509321173032126

Epoch: 6| Step: 6
Training loss: 1.7750530242919922
Validation loss: 2.187173624833425

Epoch: 6| Step: 7
Training loss: 1.162928819656372
Validation loss: 2.2037044962247214

Epoch: 6| Step: 8
Training loss: 1.532223105430603
Validation loss: 2.2232861121495566

Epoch: 6| Step: 9
Training loss: 1.0905237197875977
Validation loss: 2.2432018518447876

Epoch: 6| Step: 10
Training loss: 2.0578548908233643
Validation loss: 2.263502220312754

Epoch: 6| Step: 11
Training loss: 1.4886295795440674
Validation loss: 2.251981258392334

Epoch: 6| Step: 12
Training loss: 1.6860995292663574
Validation loss: 2.259231925010681

Epoch: 6| Step: 13
Training loss: 1.9814484119415283
Validation loss: 2.252721150716146

Epoch: 280| Step: 0
Training loss: 1.4441266059875488
Validation loss: 2.2164334058761597

Epoch: 6| Step: 1
Training loss: 1.7937726974487305
Validation loss: 2.2072040836016336

Epoch: 6| Step: 2
Training loss: 1.6404047012329102
Validation loss: 2.1679487228393555

Epoch: 6| Step: 3
Training loss: 1.014454960823059
Validation loss: 2.1632691820462546

Epoch: 6| Step: 4
Training loss: 1.9312818050384521
Validation loss: 2.1514858404795327

Epoch: 6| Step: 5
Training loss: 1.0018378496170044
Validation loss: 2.1461253563563027

Epoch: 6| Step: 6
Training loss: 1.1459873914718628
Validation loss: 2.1361822883288064

Epoch: 6| Step: 7
Training loss: 1.2715215682983398
Validation loss: 2.161393960316976

Epoch: 6| Step: 8
Training loss: 2.517925262451172
Validation loss: 2.18356454372406

Epoch: 6| Step: 9
Training loss: 1.060333013534546
Validation loss: 2.2081531087557473

Epoch: 6| Step: 10
Training loss: 1.481669545173645
Validation loss: 2.196496566136678

Epoch: 6| Step: 11
Training loss: 1.2447800636291504
Validation loss: 2.191617965698242

Epoch: 6| Step: 12
Training loss: 1.33416748046875
Validation loss: 2.2280375361442566

Epoch: 6| Step: 13
Training loss: 1.8902204036712646
Validation loss: 2.2209938367207847

Epoch: 281| Step: 0
Training loss: 0.9310837984085083
Validation loss: 2.211614986260732

Epoch: 6| Step: 1
Training loss: 2.5997509956359863
Validation loss: 2.242191473642985

Epoch: 6| Step: 2
Training loss: 2.2196168899536133
Validation loss: 2.2302045822143555

Epoch: 6| Step: 3
Training loss: 1.493126392364502
Validation loss: 2.2245821555455527

Epoch: 6| Step: 4
Training loss: 1.4000632762908936
Validation loss: 2.2106887300809226

Epoch: 6| Step: 5
Training loss: 0.9986157417297363
Validation loss: 2.235377927621206

Epoch: 6| Step: 6
Training loss: 1.5952067375183105
Validation loss: 2.2089191476504006

Epoch: 6| Step: 7
Training loss: 1.3989590406417847
Validation loss: 2.211013913154602

Epoch: 6| Step: 8
Training loss: 1.0759296417236328
Validation loss: 2.199221948782603

Epoch: 6| Step: 9
Training loss: 1.6088533401489258
Validation loss: 2.1937191088994346

Epoch: 6| Step: 10
Training loss: 0.7712408900260925
Validation loss: 2.212323526541392

Epoch: 6| Step: 11
Training loss: 1.6003152132034302
Validation loss: 2.207524279753367

Epoch: 6| Step: 12
Training loss: 1.5388410091400146
Validation loss: 2.1923810839653015

Epoch: 6| Step: 13
Training loss: 1.1211704015731812
Validation loss: 2.216527740160624

Epoch: 282| Step: 0
Training loss: 1.299882411956787
Validation loss: 2.23909064133962

Epoch: 6| Step: 1
Training loss: 1.267118215560913
Validation loss: 2.239779988924662

Epoch: 6| Step: 2
Training loss: 1.5568153858184814
Validation loss: 2.2167298793792725

Epoch: 6| Step: 3
Training loss: 1.4359076023101807
Validation loss: 2.2252326607704163

Epoch: 6| Step: 4
Training loss: 1.4807543754577637
Validation loss: 2.2303587595621743

Epoch: 6| Step: 5
Training loss: 1.146106481552124
Validation loss: 2.250083247820536

Epoch: 6| Step: 6
Training loss: 2.1390137672424316
Validation loss: 2.246020714441935

Epoch: 6| Step: 7
Training loss: 1.064232587814331
Validation loss: 2.2291088302930198

Epoch: 6| Step: 8
Training loss: 1.2049041986465454
Validation loss: 2.2324743469556174

Epoch: 6| Step: 9
Training loss: 1.6910161972045898
Validation loss: 2.2287843028704324

Epoch: 6| Step: 10
Training loss: 1.936124563217163
Validation loss: 2.2304926911989846

Epoch: 6| Step: 11
Training loss: 1.3326488733291626
Validation loss: 2.2511690656344094

Epoch: 6| Step: 12
Training loss: 1.2262651920318604
Validation loss: 2.230451246102651

Epoch: 6| Step: 13
Training loss: 1.548327922821045
Validation loss: 2.202769855658213

Epoch: 283| Step: 0
Training loss: 1.5921120643615723
Validation loss: 2.216535528500875

Epoch: 6| Step: 1
Training loss: 1.0847125053405762
Validation loss: 2.2129212419191995

Epoch: 6| Step: 2
Training loss: 1.4736175537109375
Validation loss: 2.216500918070475

Epoch: 6| Step: 3
Training loss: 2.022991180419922
Validation loss: 2.188159426053365

Epoch: 6| Step: 4
Training loss: 1.4174164533615112
Validation loss: 2.177078604698181

Epoch: 6| Step: 5
Training loss: 1.3766354322433472
Validation loss: 2.1750272115071616

Epoch: 6| Step: 6
Training loss: 0.7286523580551147
Validation loss: 2.1980002721150718

Epoch: 6| Step: 7
Training loss: 1.5615651607513428
Validation loss: 2.192790389060974

Epoch: 6| Step: 8
Training loss: 0.8991161584854126
Validation loss: 2.1724950075149536

Epoch: 6| Step: 9
Training loss: 1.219409704208374
Validation loss: 2.2105457385381064

Epoch: 6| Step: 10
Training loss: 2.1178882122039795
Validation loss: 2.2267910639444985

Epoch: 6| Step: 11
Training loss: 2.296825408935547
Validation loss: 2.232632597287496

Epoch: 6| Step: 12
Training loss: 1.1470941305160522
Validation loss: 2.2675419052441916

Epoch: 6| Step: 13
Training loss: 1.1399662494659424
Validation loss: 2.228413701057434

Epoch: 284| Step: 0
Training loss: 1.2276637554168701
Validation loss: 2.2223562796910605

Epoch: 6| Step: 1
Training loss: 1.2032520771026611
Validation loss: 2.2127671241760254

Epoch: 6| Step: 2
Training loss: 1.8636137247085571
Validation loss: 2.228880008061727

Epoch: 6| Step: 3
Training loss: 1.386231541633606
Validation loss: 2.184759815533956

Epoch: 6| Step: 4
Training loss: 1.3102211952209473
Validation loss: 2.209703723589579

Epoch: 6| Step: 5
Training loss: 1.5197811126708984
Validation loss: 2.2028679847717285

Epoch: 6| Step: 6
Training loss: 1.5441710948944092
Validation loss: 2.1947747270266214

Epoch: 6| Step: 7
Training loss: 0.904116690158844
Validation loss: 2.1751585006713867

Epoch: 6| Step: 8
Training loss: 1.6823952198028564
Validation loss: 2.1859888831774392

Epoch: 6| Step: 9
Training loss: 1.9485177993774414
Validation loss: 2.1967007915178933

Epoch: 6| Step: 10
Training loss: 1.3696846961975098
Validation loss: 2.228902757167816

Epoch: 6| Step: 11
Training loss: 2.0950570106506348
Validation loss: 2.209528843561808

Epoch: 6| Step: 12
Training loss: 0.9002799987792969
Validation loss: 2.2353604237238565

Epoch: 6| Step: 13
Training loss: 1.0085855722427368
Validation loss: 2.2097832361857095

Epoch: 285| Step: 0
Training loss: 1.718862533569336
Validation loss: 2.2128689090410867

Epoch: 6| Step: 1
Training loss: 1.535228967666626
Validation loss: 2.210958937803904

Epoch: 6| Step: 2
Training loss: 0.9050489664077759
Validation loss: 2.21615477403005

Epoch: 6| Step: 3
Training loss: 1.0439265966415405
Validation loss: 2.208065311113993

Epoch: 6| Step: 4
Training loss: 0.8991397023200989
Validation loss: 2.212167203426361

Epoch: 6| Step: 5
Training loss: 1.1554515361785889
Validation loss: 2.2114009261131287

Epoch: 6| Step: 6
Training loss: 1.1946278810501099
Validation loss: 2.2094238996505737

Epoch: 6| Step: 7
Training loss: 1.7139348983764648
Validation loss: 2.222418566544851

Epoch: 6| Step: 8
Training loss: 1.7324200868606567
Validation loss: 2.202432850996653

Epoch: 6| Step: 9
Training loss: 1.5249583721160889
Validation loss: 2.19201006491979

Epoch: 6| Step: 10
Training loss: 1.4508368968963623
Validation loss: 2.2014601627985635

Epoch: 6| Step: 11
Training loss: 1.8526661396026611
Validation loss: 2.189230461915334

Epoch: 6| Step: 12
Training loss: 1.8649080991744995
Validation loss: 2.1892404158910117

Epoch: 6| Step: 13
Training loss: 1.0293664932250977
Validation loss: 2.161245902379354

Epoch: 286| Step: 0
Training loss: 0.8146582841873169
Validation loss: 2.183963100115458

Epoch: 6| Step: 1
Training loss: 1.2563819885253906
Validation loss: 2.181463102499644

Epoch: 6| Step: 2
Training loss: 1.6585620641708374
Validation loss: 2.221567134062449

Epoch: 6| Step: 3
Training loss: 0.8885422945022583
Validation loss: 2.214789390563965

Epoch: 6| Step: 4
Training loss: 1.1334543228149414
Validation loss: 2.2257840236028037

Epoch: 6| Step: 5
Training loss: 1.018092155456543
Validation loss: 2.191593964894613

Epoch: 6| Step: 6
Training loss: 1.3820135593414307
Validation loss: 2.2299110690752664

Epoch: 6| Step: 7
Training loss: 2.5530269145965576
Validation loss: 2.2331103881200156

Epoch: 6| Step: 8
Training loss: 1.7488688230514526
Validation loss: 2.210259040196737

Epoch: 6| Step: 9
Training loss: 1.323202133178711
Validation loss: 2.227837860584259

Epoch: 6| Step: 10
Training loss: 1.27488374710083
Validation loss: 2.2096720139185586

Epoch: 6| Step: 11
Training loss: 1.4413129091262817
Validation loss: 2.204716523488363

Epoch: 6| Step: 12
Training loss: 1.6802138090133667
Validation loss: 2.2159207860628762

Epoch: 6| Step: 13
Training loss: 1.033094048500061
Validation loss: 2.2167306542396545

Epoch: 287| Step: 0
Training loss: 1.3450956344604492
Validation loss: 2.204136828581492

Epoch: 6| Step: 1
Training loss: 1.515084147453308
Validation loss: 2.20695428053538

Epoch: 6| Step: 2
Training loss: 1.1326000690460205
Validation loss: 2.201302925745646

Epoch: 6| Step: 3
Training loss: 1.4192659854888916
Validation loss: 2.2143808801968894

Epoch: 6| Step: 4
Training loss: 1.6847798824310303
Validation loss: 2.2011695305506387

Epoch: 6| Step: 5
Training loss: 1.569931983947754
Validation loss: 2.202647566795349

Epoch: 6| Step: 6
Training loss: 1.133842945098877
Validation loss: 2.2357855240503945

Epoch: 6| Step: 7
Training loss: 1.2565977573394775
Validation loss: 2.213977336883545

Epoch: 6| Step: 8
Training loss: 1.4454002380371094
Validation loss: 2.222186505794525

Epoch: 6| Step: 9
Training loss: 1.9277667999267578
Validation loss: 2.2090394496917725

Epoch: 6| Step: 10
Training loss: 1.2374039888381958
Validation loss: 2.2215458750724792

Epoch: 6| Step: 11
Training loss: 1.3467652797698975
Validation loss: 2.1876537799835205

Epoch: 6| Step: 12
Training loss: 1.0669255256652832
Validation loss: 2.201093236605326

Epoch: 6| Step: 13
Training loss: 1.0728152990341187
Validation loss: 2.2136115630467734

Epoch: 288| Step: 0
Training loss: 1.8710603713989258
Validation loss: 2.2556001345316568

Epoch: 6| Step: 1
Training loss: 1.540337324142456
Validation loss: 2.2288550535837808

Epoch: 6| Step: 2
Training loss: 1.1744040250778198
Validation loss: 2.242143174012502

Epoch: 6| Step: 3
Training loss: 1.438481330871582
Validation loss: 2.2374839981396994

Epoch: 6| Step: 4
Training loss: 1.3135108947753906
Validation loss: 2.2147807280222573

Epoch: 6| Step: 5
Training loss: 0.7247744798660278
Validation loss: 2.2086365818977356

Epoch: 6| Step: 6
Training loss: 1.3805038928985596
Validation loss: 2.205737590789795

Epoch: 6| Step: 7
Training loss: 1.0556713342666626
Validation loss: 2.2091238101323447

Epoch: 6| Step: 8
Training loss: 2.2736735343933105
Validation loss: 2.1595102151234946

Epoch: 6| Step: 9
Training loss: 1.1189122200012207
Validation loss: 2.152371605237325

Epoch: 6| Step: 10
Training loss: 1.4671204090118408
Validation loss: 2.190208892027537

Epoch: 6| Step: 11
Training loss: 1.8436322212219238
Validation loss: 2.1515514651934304

Epoch: 6| Step: 12
Training loss: 1.2071105241775513
Validation loss: 2.1681838035583496

Epoch: 6| Step: 13
Training loss: 1.165191650390625
Validation loss: 2.158742090066274

Epoch: 289| Step: 0
Training loss: 1.6002635955810547
Validation loss: 2.1903446515401206

Epoch: 6| Step: 1
Training loss: 1.58126699924469
Validation loss: 2.1984983682632446

Epoch: 6| Step: 2
Training loss: 1.0058923959732056
Validation loss: 2.215765436490377

Epoch: 6| Step: 3
Training loss: 0.8854369521141052
Validation loss: 2.1865606904029846

Epoch: 6| Step: 4
Training loss: 1.2157349586486816
Validation loss: 2.1842660903930664

Epoch: 6| Step: 5
Training loss: 1.710516095161438
Validation loss: 2.1868876218795776

Epoch: 6| Step: 6
Training loss: 1.487701654434204
Validation loss: 2.2177853186925254

Epoch: 6| Step: 7
Training loss: 2.216303825378418
Validation loss: 2.1807515223821006

Epoch: 6| Step: 8
Training loss: 1.0517373085021973
Validation loss: 2.215203841527303

Epoch: 6| Step: 9
Training loss: 1.0725622177124023
Validation loss: 2.234598716100057

Epoch: 6| Step: 10
Training loss: 1.2039620876312256
Validation loss: 2.240578293800354

Epoch: 6| Step: 11
Training loss: 1.0854451656341553
Validation loss: 2.184081812699636

Epoch: 6| Step: 12
Training loss: 1.950897455215454
Validation loss: 2.2177319327990213

Epoch: 6| Step: 13
Training loss: 1.5072917938232422
Validation loss: 2.2218252420425415

Epoch: 290| Step: 0
Training loss: 1.3898942470550537
Validation loss: 2.2121891379356384

Epoch: 6| Step: 1
Training loss: 0.9040802717208862
Validation loss: 2.204499880472819

Epoch: 6| Step: 2
Training loss: 1.839770793914795
Validation loss: 2.2005988160769143

Epoch: 6| Step: 3
Training loss: 1.1140367984771729
Validation loss: 2.2406834959983826

Epoch: 6| Step: 4
Training loss: 1.33978271484375
Validation loss: 2.2465359369913735

Epoch: 6| Step: 5
Training loss: 2.0920112133026123
Validation loss: 2.2327686150868735

Epoch: 6| Step: 6
Training loss: 0.6417139172554016
Validation loss: 2.2042552828788757

Epoch: 6| Step: 7
Training loss: 1.5102680921554565
Validation loss: 2.2284659147262573

Epoch: 6| Step: 8
Training loss: 1.5761892795562744
Validation loss: 2.226933221022288

Epoch: 6| Step: 9
Training loss: 1.0421966314315796
Validation loss: 2.212938825289408

Epoch: 6| Step: 10
Training loss: 0.8531450629234314
Validation loss: 2.234704375267029

Epoch: 6| Step: 11
Training loss: 1.3815934658050537
Validation loss: 2.2020920515060425

Epoch: 6| Step: 12
Training loss: 1.2170796394348145
Validation loss: 2.219809631506602

Epoch: 6| Step: 13
Training loss: 2.224531412124634
Validation loss: 2.201971928278605

Epoch: 291| Step: 0
Training loss: 0.7732467651367188
Validation loss: 2.203243613243103

Epoch: 6| Step: 1
Training loss: 0.9676246643066406
Validation loss: 2.180295785268148

Epoch: 6| Step: 2
Training loss: 1.1981159448623657
Validation loss: 2.171869079271952

Epoch: 6| Step: 3
Training loss: 1.6106712818145752
Validation loss: 2.1983107527097068

Epoch: 6| Step: 4
Training loss: 1.2842987775802612
Validation loss: 2.174936850865682

Epoch: 6| Step: 5
Training loss: 1.3797876834869385
Validation loss: 2.1581591963768005

Epoch: 6| Step: 6
Training loss: 0.9404802322387695
Validation loss: 2.1886732379595437

Epoch: 6| Step: 7
Training loss: 1.28677237033844
Validation loss: 2.2157532374064126

Epoch: 6| Step: 8
Training loss: 2.4264421463012695
Validation loss: 2.19012059768041

Epoch: 6| Step: 9
Training loss: 1.595447063446045
Validation loss: 2.194833676020304

Epoch: 6| Step: 10
Training loss: 1.3444160223007202
Validation loss: 2.186101714769999

Epoch: 6| Step: 11
Training loss: 1.2267467975616455
Validation loss: 2.208308299382528

Epoch: 6| Step: 12
Training loss: 0.9727157354354858
Validation loss: 2.223454018433889

Epoch: 6| Step: 13
Training loss: 1.9284542798995972
Validation loss: 2.2028130690256753

Epoch: 292| Step: 0
Training loss: 1.5414226055145264
Validation loss: 2.206806182861328

Epoch: 6| Step: 1
Training loss: 0.8696839809417725
Validation loss: 2.1973371307055154

Epoch: 6| Step: 2
Training loss: 1.2335479259490967
Validation loss: 2.2049556573232016

Epoch: 6| Step: 3
Training loss: 0.8503060340881348
Validation loss: 2.209324578444163

Epoch: 6| Step: 4
Training loss: 2.1299896240234375
Validation loss: 2.2228384415308633

Epoch: 6| Step: 5
Training loss: 1.4077869653701782
Validation loss: 2.183679759502411

Epoch: 6| Step: 6
Training loss: 1.136533498764038
Validation loss: 2.2055399815241494

Epoch: 6| Step: 7
Training loss: 1.497516393661499
Validation loss: 2.1745049556096396

Epoch: 6| Step: 8
Training loss: 1.072765588760376
Validation loss: 2.193615118662516

Epoch: 6| Step: 9
Training loss: 1.0343765020370483
Validation loss: 2.183895687262217

Epoch: 6| Step: 10
Training loss: 1.471535325050354
Validation loss: 2.2197975715001426

Epoch: 6| Step: 11
Training loss: 1.6124262809753418
Validation loss: 2.2225826183954873

Epoch: 6| Step: 12
Training loss: 1.4262524843215942
Validation loss: 2.2153289715449014

Epoch: 6| Step: 13
Training loss: 1.833051085472107
Validation loss: 2.2378520568211875

Epoch: 293| Step: 0
Training loss: 0.8962738513946533
Validation loss: 2.2089409828186035

Epoch: 6| Step: 1
Training loss: 1.5492233037948608
Validation loss: 2.223742584387461

Epoch: 6| Step: 2
Training loss: 0.7720509171485901
Validation loss: 2.2035295565923056

Epoch: 6| Step: 3
Training loss: 1.6827352046966553
Validation loss: 2.2321734031041465

Epoch: 6| Step: 4
Training loss: 1.2576333284378052
Validation loss: 2.1958365440368652

Epoch: 6| Step: 5
Training loss: 1.7378523349761963
Validation loss: 2.1921836535135903

Epoch: 6| Step: 6
Training loss: 0.9836236238479614
Validation loss: 2.2183584173520408

Epoch: 6| Step: 7
Training loss: 1.1601917743682861
Validation loss: 2.178851922353109

Epoch: 6| Step: 8
Training loss: 1.3383848667144775
Validation loss: 2.2142471273740134

Epoch: 6| Step: 9
Training loss: 1.5983912944793701
Validation loss: 2.2296324173609414

Epoch: 6| Step: 10
Training loss: 1.299438238143921
Validation loss: 2.212791601816813

Epoch: 6| Step: 11
Training loss: 1.7553671598434448
Validation loss: 2.223970413208008

Epoch: 6| Step: 12
Training loss: 1.7962573766708374
Validation loss: 2.2002779841423035

Epoch: 6| Step: 13
Training loss: 1.5747756958007812
Validation loss: 2.242625037829081

Epoch: 294| Step: 0
Training loss: 2.3509559631347656
Validation loss: 2.229255974292755

Epoch: 6| Step: 1
Training loss: 0.8682622313499451
Validation loss: 2.194248596827189

Epoch: 6| Step: 2
Training loss: 0.8923298120498657
Validation loss: 2.188122828801473

Epoch: 6| Step: 3
Training loss: 0.9205949902534485
Validation loss: 2.2411483923594155

Epoch: 6| Step: 4
Training loss: 0.9632142782211304
Validation loss: 2.1672683159510293

Epoch: 6| Step: 5
Training loss: 1.4558563232421875
Validation loss: 2.1761287252108255

Epoch: 6| Step: 6
Training loss: 1.546726942062378
Validation loss: 2.198415160179138

Epoch: 6| Step: 7
Training loss: 0.9397387504577637
Validation loss: 2.203950842221578

Epoch: 6| Step: 8
Training loss: 0.8882684111595154
Validation loss: 2.219001611073812

Epoch: 6| Step: 9
Training loss: 2.3069868087768555
Validation loss: 2.2171448866526284

Epoch: 6| Step: 10
Training loss: 1.945265769958496
Validation loss: 2.1844334999720254

Epoch: 6| Step: 11
Training loss: 0.9678679704666138
Validation loss: 2.2272456288337708

Epoch: 6| Step: 12
Training loss: 1.2231584787368774
Validation loss: 2.241191327571869

Epoch: 6| Step: 13
Training loss: 1.8028078079223633
Validation loss: 2.2422573963801065

Epoch: 295| Step: 0
Training loss: 1.455298662185669
Validation loss: 2.2662599881490073

Epoch: 6| Step: 1
Training loss: 0.9257289171218872
Validation loss: 2.2535534302393594

Epoch: 6| Step: 2
Training loss: 1.4743131399154663
Validation loss: 2.2316363056500754

Epoch: 6| Step: 3
Training loss: 1.5449883937835693
Validation loss: 2.218498945236206

Epoch: 6| Step: 4
Training loss: 1.4829620122909546
Validation loss: 2.2293216784795127

Epoch: 6| Step: 5
Training loss: 0.5651510953903198
Validation loss: 2.20936781167984

Epoch: 6| Step: 6
Training loss: 1.6019271612167358
Validation loss: 2.1799885431925454

Epoch: 6| Step: 7
Training loss: 1.1495773792266846
Validation loss: 2.208649456501007

Epoch: 6| Step: 8
Training loss: 1.93247652053833
Validation loss: 2.1591947078704834

Epoch: 6| Step: 9
Training loss: 1.0232179164886475
Validation loss: 2.1809229850769043

Epoch: 6| Step: 10
Training loss: 1.6894782781600952
Validation loss: 2.2175363103548684

Epoch: 6| Step: 11
Training loss: 1.432879090309143
Validation loss: 2.2050568660100303

Epoch: 6| Step: 12
Training loss: 1.0152064561843872
Validation loss: 2.2033945123354592

Epoch: 6| Step: 13
Training loss: 1.5903334617614746
Validation loss: 2.2165236473083496

Epoch: 296| Step: 0
Training loss: 1.415884256362915
Validation loss: 2.2011570731798806

Epoch: 6| Step: 1
Training loss: 1.6869864463806152
Validation loss: 2.2070430715878806

Epoch: 6| Step: 2
Training loss: 1.0642390251159668
Validation loss: 2.1926647424697876

Epoch: 6| Step: 3
Training loss: 1.3282830715179443
Validation loss: 2.1999252438545227

Epoch: 6| Step: 4
Training loss: 1.2565808296203613
Validation loss: 2.201649765173594

Epoch: 6| Step: 5
Training loss: 0.9332312345504761
Validation loss: 2.2145018378893533

Epoch: 6| Step: 6
Training loss: 0.9655150175094604
Validation loss: 2.2332616249720254

Epoch: 6| Step: 7
Training loss: 1.3793540000915527
Validation loss: 2.1918113430341086

Epoch: 6| Step: 8
Training loss: 1.2400918006896973
Validation loss: 2.2098652124404907

Epoch: 6| Step: 9
Training loss: 1.1600329875946045
Validation loss: 2.242226163546244

Epoch: 6| Step: 10
Training loss: 0.9044321179389954
Validation loss: 2.2119977474212646

Epoch: 6| Step: 11
Training loss: 1.5859432220458984
Validation loss: 2.2084639271100364

Epoch: 6| Step: 12
Training loss: 1.9110983610153198
Validation loss: 2.212288200855255

Epoch: 6| Step: 13
Training loss: 1.8502635955810547
Validation loss: 2.2362521489461265

Epoch: 297| Step: 0
Training loss: 1.9664592742919922
Validation loss: 2.175740440686544

Epoch: 6| Step: 1
Training loss: 1.204379916191101
Validation loss: 2.1956750750541687

Epoch: 6| Step: 2
Training loss: 1.1099432706832886
Validation loss: 2.1917483806610107

Epoch: 6| Step: 3
Training loss: 1.0381660461425781
Validation loss: 2.207452197869619

Epoch: 6| Step: 4
Training loss: 1.0719250440597534
Validation loss: 2.219321926434835

Epoch: 6| Step: 5
Training loss: 1.5067226886749268
Validation loss: 2.229855457941691

Epoch: 6| Step: 6
Training loss: 1.8633029460906982
Validation loss: 2.2085471947987876

Epoch: 6| Step: 7
Training loss: 1.5896813869476318
Validation loss: 2.2023390531539917

Epoch: 6| Step: 8
Training loss: 1.0084879398345947
Validation loss: 2.220741351445516

Epoch: 6| Step: 9
Training loss: 1.2089343070983887
Validation loss: 2.214737832546234

Epoch: 6| Step: 10
Training loss: 1.420060634613037
Validation loss: 2.2437044779459634

Epoch: 6| Step: 11
Training loss: 1.415160894393921
Validation loss: 2.208003063996633

Epoch: 6| Step: 12
Training loss: 0.8892632722854614
Validation loss: 2.2085987329483032

Epoch: 6| Step: 13
Training loss: 1.2345986366271973
Validation loss: 2.1887725989023843

Epoch: 298| Step: 0
Training loss: 1.174046516418457
Validation loss: 2.1946629881858826

Epoch: 6| Step: 1
Training loss: 1.2222893238067627
Validation loss: 2.183383882045746

Epoch: 6| Step: 2
Training loss: 1.1329363584518433
Validation loss: 2.193433920542399

Epoch: 6| Step: 3
Training loss: 1.5599431991577148
Validation loss: 2.206165313720703

Epoch: 6| Step: 4
Training loss: 1.5090739727020264
Validation loss: 2.1860235929489136

Epoch: 6| Step: 5
Training loss: 1.4573564529418945
Validation loss: 2.176353335380554

Epoch: 6| Step: 6
Training loss: 1.4243650436401367
Validation loss: 2.195618828137716

Epoch: 6| Step: 7
Training loss: 1.287759780883789
Validation loss: 2.2125302950541177

Epoch: 6| Step: 8
Training loss: 1.373986005783081
Validation loss: 2.2023551861445108

Epoch: 6| Step: 9
Training loss: 0.9527863264083862
Validation loss: 2.187869985898336

Epoch: 6| Step: 10
Training loss: 1.3099572658538818
Validation loss: 2.157236099243164

Epoch: 6| Step: 11
Training loss: 1.948216199874878
Validation loss: 2.149850865205129

Epoch: 6| Step: 12
Training loss: 1.5060079097747803
Validation loss: 2.204236090183258

Epoch: 6| Step: 13
Training loss: 0.9936693906784058
Validation loss: 2.1732506354649863

Epoch: 299| Step: 0
Training loss: 1.680238127708435
Validation loss: 2.183290958404541

Epoch: 6| Step: 1
Training loss: 0.8136898279190063
Validation loss: 2.2083457310994468

Epoch: 6| Step: 2
Training loss: 1.5627009868621826
Validation loss: 2.1945156256357827

Epoch: 6| Step: 3
Training loss: 1.291471004486084
Validation loss: 2.1721957524617515

Epoch: 6| Step: 4
Training loss: 2.1141088008880615
Validation loss: 2.1515597899754844

Epoch: 6| Step: 5
Training loss: 1.6793651580810547
Validation loss: 2.232700010140737

Epoch: 6| Step: 6
Training loss: 1.1695940494537354
Validation loss: 2.240041971206665

Epoch: 6| Step: 7
Training loss: 0.9159820079803467
Validation loss: 2.206546366214752

Epoch: 6| Step: 8
Training loss: 1.2698274850845337
Validation loss: 2.18149201075236

Epoch: 6| Step: 9
Training loss: 1.1057956218719482
Validation loss: 2.1788960297902427

Epoch: 6| Step: 10
Training loss: 0.926316499710083
Validation loss: 2.1769162019093833

Epoch: 6| Step: 11
Training loss: 0.8486024141311646
Validation loss: 2.182715197404226

Epoch: 6| Step: 12
Training loss: 1.8487083911895752
Validation loss: 2.1366822322209678

Epoch: 6| Step: 13
Training loss: 1.6402230262756348
Validation loss: 2.1775092283884683

Epoch: 300| Step: 0
Training loss: 1.0516753196716309
Validation loss: 2.2116153240203857

Epoch: 6| Step: 1
Training loss: 0.8864856362342834
Validation loss: 2.218557119369507

Epoch: 6| Step: 2
Training loss: 1.3871630430221558
Validation loss: 2.1905677119890847

Epoch: 6| Step: 3
Training loss: 1.2048635482788086
Validation loss: 2.2036394278208413

Epoch: 6| Step: 4
Training loss: 1.2425057888031006
Validation loss: 2.1678627729415894

Epoch: 6| Step: 5
Training loss: 1.501739263534546
Validation loss: 2.2075570623079934

Epoch: 6| Step: 6
Training loss: 1.2105519771575928
Validation loss: 2.2116849025090537

Epoch: 6| Step: 7
Training loss: 2.140374183654785
Validation loss: 2.2095432678858438

Epoch: 6| Step: 8
Training loss: 1.5784565210342407
Validation loss: 2.2238848408063254

Epoch: 6| Step: 9
Training loss: 1.3240394592285156
Validation loss: 2.226107676823934

Epoch: 6| Step: 10
Training loss: 2.534369707107544
Validation loss: 2.1882862051328025

Epoch: 6| Step: 11
Training loss: 0.5633572936058044
Validation loss: 2.244946281115214

Epoch: 6| Step: 12
Training loss: 1.3169629573822021
Validation loss: 2.1971320311228433

Epoch: 6| Step: 13
Training loss: 1.2077687978744507
Validation loss: 2.2002421220143638

Epoch: 301| Step: 0
Training loss: 1.0422635078430176
Validation loss: 2.210106114546458

Epoch: 6| Step: 1
Training loss: 1.1627428531646729
Validation loss: 2.224571963151296

Epoch: 6| Step: 2
Training loss: 1.3444006443023682
Validation loss: 2.2399054765701294

Epoch: 6| Step: 3
Training loss: 1.3712067604064941
Validation loss: 2.201756497224172

Epoch: 6| Step: 4
Training loss: 1.2232311964035034
Validation loss: 2.251335620880127

Epoch: 6| Step: 5
Training loss: 1.3635663986206055
Validation loss: 2.182845155398051

Epoch: 6| Step: 6
Training loss: 1.3142855167388916
Validation loss: 2.1915256579717

Epoch: 6| Step: 7
Training loss: 1.472237467765808
Validation loss: 2.2044150829315186

Epoch: 6| Step: 8
Training loss: 1.4590067863464355
Validation loss: 2.1789464553197226

Epoch: 6| Step: 9
Training loss: 1.9656319618225098
Validation loss: 2.2069440682729087

Epoch: 6| Step: 10
Training loss: 1.5286476612091064
Validation loss: 2.196525792280833

Epoch: 6| Step: 11
Training loss: 1.3234314918518066
Validation loss: 2.226243317127228

Epoch: 6| Step: 12
Training loss: 2.0571036338806152
Validation loss: 2.215928077697754

Epoch: 6| Step: 13
Training loss: 0.9619131088256836
Validation loss: 2.225557724634806

Epoch: 302| Step: 0
Training loss: 1.9303004741668701
Validation loss: 2.210699458916982

Epoch: 6| Step: 1
Training loss: 2.050969123840332
Validation loss: 2.229373256365458

Epoch: 6| Step: 2
Training loss: 0.9608197212219238
Validation loss: 2.258377989133199

Epoch: 6| Step: 3
Training loss: 1.3666014671325684
Validation loss: 2.2332229812939963

Epoch: 6| Step: 4
Training loss: 1.5251801013946533
Validation loss: 2.2108506759007773

Epoch: 6| Step: 5
Training loss: 1.2290211915969849
Validation loss: 2.252789855003357

Epoch: 6| Step: 6
Training loss: 0.9523013830184937
Validation loss: 2.2354372143745422

Epoch: 6| Step: 7
Training loss: 0.9806193709373474
Validation loss: 2.2210620244344077

Epoch: 6| Step: 8
Training loss: 1.3211359977722168
Validation loss: 2.272354861100515

Epoch: 6| Step: 9
Training loss: 1.3091909885406494
Validation loss: 2.2519965370496116

Epoch: 6| Step: 10
Training loss: 0.9840046763420105
Validation loss: 2.280886630217234

Epoch: 6| Step: 11
Training loss: 1.5482417345046997
Validation loss: 2.23555721839269

Epoch: 6| Step: 12
Training loss: 2.0426621437072754
Validation loss: 2.255018174648285

Epoch: 6| Step: 13
Training loss: 1.1092092990875244
Validation loss: 2.205025394757589

Epoch: 303| Step: 0
Training loss: 1.4129793643951416
Validation loss: 2.2574390172958374

Epoch: 6| Step: 1
Training loss: 1.6668455600738525
Validation loss: 2.2077088157335916

Epoch: 6| Step: 2
Training loss: 1.3152310848236084
Validation loss: 2.221420725186666

Epoch: 6| Step: 3
Training loss: 1.920750379562378
Validation loss: 2.212027986844381

Epoch: 6| Step: 4
Training loss: 2.1746463775634766
Validation loss: 2.191842714945475

Epoch: 6| Step: 5
Training loss: 0.9243310689926147
Validation loss: 2.184647520383199

Epoch: 6| Step: 6
Training loss: 1.289614200592041
Validation loss: 2.1825252374013266

Epoch: 6| Step: 7
Training loss: 1.349639654159546
Validation loss: 2.1504207253456116

Epoch: 6| Step: 8
Training loss: 1.0535147190093994
Validation loss: 2.153929909070333

Epoch: 6| Step: 9
Training loss: 1.5495011806488037
Validation loss: 2.159128208955129

Epoch: 6| Step: 10
Training loss: 0.9641810059547424
Validation loss: 2.1514962315559387

Epoch: 6| Step: 11
Training loss: 0.6738731265068054
Validation loss: 2.1466245452562966

Epoch: 6| Step: 12
Training loss: 1.020207166671753
Validation loss: 2.1328459978103638

Epoch: 6| Step: 13
Training loss: 1.4766383171081543
Validation loss: 2.144791861375173

Epoch: 304| Step: 0
Training loss: 1.6121443510055542
Validation loss: 2.121151308218638

Epoch: 6| Step: 1
Training loss: 0.9376394152641296
Validation loss: 2.122412840525309

Epoch: 6| Step: 2
Training loss: 1.4183738231658936
Validation loss: 2.20066899061203

Epoch: 6| Step: 3
Training loss: 1.2478275299072266
Validation loss: 2.1819782654444375

Epoch: 6| Step: 4
Training loss: 1.4112627506256104
Validation loss: 2.180236359437307

Epoch: 6| Step: 5
Training loss: 0.8371555209159851
Validation loss: 2.2121894160906472

Epoch: 6| Step: 6
Training loss: 1.9449611902236938
Validation loss: 2.196067770322164

Epoch: 6| Step: 7
Training loss: 0.5899436473846436
Validation loss: 2.1926902135213218

Epoch: 6| Step: 8
Training loss: 1.4770677089691162
Validation loss: 2.1952600280443826

Epoch: 6| Step: 9
Training loss: 1.6389989852905273
Validation loss: 2.2004714210828147

Epoch: 6| Step: 10
Training loss: 0.6919049024581909
Validation loss: 2.222970406214396

Epoch: 6| Step: 11
Training loss: 1.561100959777832
Validation loss: 2.224467118581136

Epoch: 6| Step: 12
Training loss: 1.6675686836242676
Validation loss: 2.2046861251195273

Epoch: 6| Step: 13
Training loss: 1.297914743423462
Validation loss: 2.2227304180463157

Epoch: 305| Step: 0
Training loss: 2.0009913444519043
Validation loss: 2.1779052217801413

Epoch: 6| Step: 1
Training loss: 0.8239191770553589
Validation loss: 2.183257043361664

Epoch: 6| Step: 2
Training loss: 1.5623118877410889
Validation loss: 2.192460536956787

Epoch: 6| Step: 3
Training loss: 1.0735477209091187
Validation loss: 2.2063099344571433

Epoch: 6| Step: 4
Training loss: 1.6090843677520752
Validation loss: 2.1919636726379395

Epoch: 6| Step: 5
Training loss: 0.9060799479484558
Validation loss: 2.1969645619392395

Epoch: 6| Step: 6
Training loss: 1.7109538316726685
Validation loss: 2.2177980740865073

Epoch: 6| Step: 7
Training loss: 1.0972559452056885
Validation loss: 2.215172012646993

Epoch: 6| Step: 8
Training loss: 1.0495043992996216
Validation loss: 2.1887659629185996

Epoch: 6| Step: 9
Training loss: 1.4690582752227783
Validation loss: 2.188336968421936

Epoch: 6| Step: 10
Training loss: 1.2200244665145874
Validation loss: 2.209424535433451

Epoch: 6| Step: 11
Training loss: 1.2189949750900269
Validation loss: 2.2016920248667398

Epoch: 6| Step: 12
Training loss: 1.282846212387085
Validation loss: 2.1622889041900635

Epoch: 6| Step: 13
Training loss: 1.0634043216705322
Validation loss: 2.2131614486376443

Epoch: 306| Step: 0
Training loss: 1.7945563793182373
Validation loss: 2.1880409518877664

Epoch: 6| Step: 1
Training loss: 1.158719539642334
Validation loss: 2.2081092397371926

Epoch: 6| Step: 2
Training loss: 1.3090524673461914
Validation loss: 2.1969385345776877

Epoch: 6| Step: 3
Training loss: 1.0623548030853271
Validation loss: 2.2326197624206543

Epoch: 6| Step: 4
Training loss: 0.9062210917472839
Validation loss: 2.189603408177694

Epoch: 6| Step: 5
Training loss: 1.4670531749725342
Validation loss: 2.1839407682418823

Epoch: 6| Step: 6
Training loss: 0.948104977607727
Validation loss: 2.2066622177759805

Epoch: 6| Step: 7
Training loss: 1.274348497390747
Validation loss: 2.230414946873983

Epoch: 6| Step: 8
Training loss: 1.0745255947113037
Validation loss: 2.204054117202759

Epoch: 6| Step: 9
Training loss: 1.7034492492675781
Validation loss: 2.2471057573954263

Epoch: 6| Step: 10
Training loss: 1.4442095756530762
Validation loss: 2.2107311487197876

Epoch: 6| Step: 11
Training loss: 1.3004153966903687
Validation loss: 2.1868932048479715

Epoch: 6| Step: 12
Training loss: 1.5969371795654297
Validation loss: 2.231469750404358

Epoch: 6| Step: 13
Training loss: 1.7048835754394531
Validation loss: 2.235634167989095

Epoch: 307| Step: 0
Training loss: 0.9777572154998779
Validation loss: 2.1942214965820312

Epoch: 6| Step: 1
Training loss: 1.583747386932373
Validation loss: 2.2006650964419046

Epoch: 6| Step: 2
Training loss: 2.3100552558898926
Validation loss: 2.193715294202169

Epoch: 6| Step: 3
Training loss: 1.7505269050598145
Validation loss: 2.1941264271736145

Epoch: 6| Step: 4
Training loss: 0.6474661231040955
Validation loss: 2.2126413583755493

Epoch: 6| Step: 5
Training loss: 1.2860054969787598
Validation loss: 2.2138440211613974

Epoch: 6| Step: 6
Training loss: 0.6687310934066772
Validation loss: 2.2043590744336448

Epoch: 6| Step: 7
Training loss: 1.1058743000030518
Validation loss: 2.208020289738973

Epoch: 6| Step: 8
Training loss: 1.7340975999832153
Validation loss: 2.216585357983907

Epoch: 6| Step: 9
Training loss: 1.8205317258834839
Validation loss: 2.2135371367136636

Epoch: 6| Step: 10
Training loss: 1.0012078285217285
Validation loss: 2.1993115743001304

Epoch: 6| Step: 11
Training loss: 0.9084240794181824
Validation loss: 2.1829997499783835

Epoch: 6| Step: 12
Training loss: 0.993934154510498
Validation loss: 2.2117591301600137

Epoch: 6| Step: 13
Training loss: 1.151361107826233
Validation loss: 2.1581761042277017

Epoch: 308| Step: 0
Training loss: 1.0706043243408203
Validation loss: 2.1873517632484436

Epoch: 6| Step: 1
Training loss: 1.6419925689697266
Validation loss: 2.216217041015625

Epoch: 6| Step: 2
Training loss: 1.0814335346221924
Validation loss: 2.2168664932250977

Epoch: 6| Step: 3
Training loss: 1.4820358753204346
Validation loss: 2.2184507052103677

Epoch: 6| Step: 4
Training loss: 0.7237428426742554
Validation loss: 2.194065511226654

Epoch: 6| Step: 5
Training loss: 2.8076930046081543
Validation loss: 2.1917661229769387

Epoch: 6| Step: 6
Training loss: 1.0035063028335571
Validation loss: 2.1884535749753318

Epoch: 6| Step: 7
Training loss: 1.2414050102233887
Validation loss: 2.2007178465525308

Epoch: 6| Step: 8
Training loss: 1.6645866632461548
Validation loss: 2.187532285849253

Epoch: 6| Step: 9
Training loss: 0.6570094227790833
Validation loss: 2.2080020705858865

Epoch: 6| Step: 10
Training loss: 1.1405935287475586
Validation loss: 2.216725528240204

Epoch: 6| Step: 11
Training loss: 0.9747555255889893
Validation loss: 2.213657816251119

Epoch: 6| Step: 12
Training loss: 1.191170573234558
Validation loss: 2.2211578687032065

Epoch: 6| Step: 13
Training loss: 1.0072087049484253
Validation loss: 2.187304755051931

Epoch: 309| Step: 0
Training loss: 1.8015220165252686
Validation loss: 2.188098927338918

Epoch: 6| Step: 1
Training loss: 1.5757458209991455
Validation loss: 2.222887317339579

Epoch: 6| Step: 2
Training loss: 1.3352172374725342
Validation loss: 2.1778385837872825

Epoch: 6| Step: 3
Training loss: 0.7712515592575073
Validation loss: 2.176656484603882

Epoch: 6| Step: 4
Training loss: 1.6870981454849243
Validation loss: 2.2297972440719604

Epoch: 6| Step: 5
Training loss: 0.8516936302185059
Validation loss: 2.2015293637911477

Epoch: 6| Step: 6
Training loss: 1.1541411876678467
Validation loss: 2.232366601626078

Epoch: 6| Step: 7
Training loss: 0.8533861041069031
Validation loss: 2.204533636569977

Epoch: 6| Step: 8
Training loss: 1.6820815801620483
Validation loss: 2.2000246047973633

Epoch: 6| Step: 9
Training loss: 1.7329087257385254
Validation loss: 2.173421581586202

Epoch: 6| Step: 10
Training loss: 1.0954844951629639
Validation loss: 2.174729824066162

Epoch: 6| Step: 11
Training loss: 0.5002371072769165
Validation loss: 2.2207149267196655

Epoch: 6| Step: 12
Training loss: 1.0884870290756226
Validation loss: 2.1991554697354636

Epoch: 6| Step: 13
Training loss: 1.6449923515319824
Validation loss: 2.2218582034111023

Epoch: 310| Step: 0
Training loss: 1.1541637182235718
Validation loss: 2.2297507723172507

Epoch: 6| Step: 1
Training loss: 1.6348515748977661
Validation loss: 2.212192177772522

Epoch: 6| Step: 2
Training loss: 1.5619876384735107
Validation loss: 2.195953627427419

Epoch: 6| Step: 3
Training loss: 1.1853399276733398
Validation loss: 2.21303254365921

Epoch: 6| Step: 4
Training loss: 1.1789735555648804
Validation loss: 2.1980026364326477

Epoch: 6| Step: 5
Training loss: 1.102341651916504
Validation loss: 2.215250253677368

Epoch: 6| Step: 6
Training loss: 1.1007845401763916
Validation loss: 2.236061215400696

Epoch: 6| Step: 7
Training loss: 1.4062602519989014
Validation loss: 2.233253240585327

Epoch: 6| Step: 8
Training loss: 1.172634243965149
Validation loss: 2.201508581638336

Epoch: 6| Step: 9
Training loss: 1.0742442607879639
Validation loss: 2.2040741443634033

Epoch: 6| Step: 10
Training loss: 1.2130783796310425
Validation loss: 2.237577438354492

Epoch: 6| Step: 11
Training loss: 1.473884105682373
Validation loss: 2.1986966530481973

Epoch: 6| Step: 12
Training loss: 0.8913834095001221
Validation loss: 2.2465989192326865

Epoch: 6| Step: 13
Training loss: 1.272127389907837
Validation loss: 2.2185886899630227

Epoch: 311| Step: 0
Training loss: 1.0323911905288696
Validation loss: 2.2370718320210776

Epoch: 6| Step: 1
Training loss: 1.3207240104675293
Validation loss: 2.2228838006655374

Epoch: 6| Step: 2
Training loss: 1.043074607849121
Validation loss: 2.2455183466275535

Epoch: 6| Step: 3
Training loss: 1.454521894454956
Validation loss: 2.2086617946624756

Epoch: 6| Step: 4
Training loss: 0.752823531627655
Validation loss: 2.2272247672080994

Epoch: 6| Step: 5
Training loss: 1.8644540309906006
Validation loss: 2.2419020732243857

Epoch: 6| Step: 6
Training loss: 1.6802784204483032
Validation loss: 2.2374806006749473

Epoch: 6| Step: 7
Training loss: 0.9365503787994385
Validation loss: 2.2732170820236206

Epoch: 6| Step: 8
Training loss: 1.2183947563171387
Validation loss: 2.279340167840322

Epoch: 6| Step: 9
Training loss: 1.4735857248306274
Validation loss: 2.254823883374532

Epoch: 6| Step: 10
Training loss: 1.6447862386703491
Validation loss: 2.251092274983724

Epoch: 6| Step: 11
Training loss: 0.8218753337860107
Validation loss: 2.2235255042711892

Epoch: 6| Step: 12
Training loss: 1.653550386428833
Validation loss: 2.227950394153595

Epoch: 6| Step: 13
Training loss: 1.200718641281128
Validation loss: 2.217063923676809

Epoch: 312| Step: 0
Training loss: 1.4265751838684082
Validation loss: 2.231598734855652

Epoch: 6| Step: 1
Training loss: 1.4200360774993896
Validation loss: 2.2340854008992515

Epoch: 6| Step: 2
Training loss: 1.1177433729171753
Validation loss: 2.178822418053945

Epoch: 6| Step: 3
Training loss: 0.9622406959533691
Validation loss: 2.2201120257377625

Epoch: 6| Step: 4
Training loss: 0.683880090713501
Validation loss: 2.1981183290481567

Epoch: 6| Step: 5
Training loss: 2.145775556564331
Validation loss: 2.1985217134157815

Epoch: 6| Step: 6
Training loss: 1.1703038215637207
Validation loss: 2.2112070520718894

Epoch: 6| Step: 7
Training loss: 1.2006429433822632
Validation loss: 2.195565660794576

Epoch: 6| Step: 8
Training loss: 0.8487297296524048
Validation loss: 2.216282149155935

Epoch: 6| Step: 9
Training loss: 1.0744774341583252
Validation loss: 2.2095077435175576

Epoch: 6| Step: 10
Training loss: 0.9660841226577759
Validation loss: 2.2186203996340432

Epoch: 6| Step: 11
Training loss: 1.020142912864685
Validation loss: 2.2603697379430137

Epoch: 6| Step: 12
Training loss: 1.9907677173614502
Validation loss: 2.1985355416933694

Epoch: 6| Step: 13
Training loss: 1.8078458309173584
Validation loss: 2.214353362719218

Epoch: 313| Step: 0
Training loss: 1.5390191078186035
Validation loss: 2.252434472242991

Epoch: 6| Step: 1
Training loss: 1.239612102508545
Validation loss: 2.236371477444967

Epoch: 6| Step: 2
Training loss: 2.3670642375946045
Validation loss: 2.2155781785647073

Epoch: 6| Step: 3
Training loss: 1.4480149745941162
Validation loss: 2.235060969988505

Epoch: 6| Step: 4
Training loss: 1.7291581630706787
Validation loss: 2.1668314337730408

Epoch: 6| Step: 5
Training loss: 1.019763469696045
Validation loss: 2.1905614932378135

Epoch: 6| Step: 6
Training loss: 1.3834819793701172
Validation loss: 2.2223382592201233

Epoch: 6| Step: 7
Training loss: 1.1427351236343384
Validation loss: 2.2398733099301658

Epoch: 6| Step: 8
Training loss: 1.355322241783142
Validation loss: 2.183627208073934

Epoch: 6| Step: 9
Training loss: 0.6018747091293335
Validation loss: 2.2188692490259805

Epoch: 6| Step: 10
Training loss: 1.2281495332717896
Validation loss: 2.193824211756388

Epoch: 6| Step: 11
Training loss: 0.5435045957565308
Validation loss: 2.1887375513712564

Epoch: 6| Step: 12
Training loss: 1.399086594581604
Validation loss: 2.197948773701986

Epoch: 6| Step: 13
Training loss: 1.4058626890182495
Validation loss: 2.1916529536247253

Epoch: 314| Step: 0
Training loss: 0.9455225467681885
Validation loss: 2.199965516726176

Epoch: 6| Step: 1
Training loss: 1.2989341020584106
Validation loss: 2.199428915977478

Epoch: 6| Step: 2
Training loss: 1.1106423139572144
Validation loss: 2.2233638366063437

Epoch: 6| Step: 3
Training loss: 1.4669004678726196
Validation loss: 2.216366767883301

Epoch: 6| Step: 4
Training loss: 0.9876211881637573
Validation loss: 2.2295051415761313

Epoch: 6| Step: 5
Training loss: 0.7412943243980408
Validation loss: 2.2177114884058633

Epoch: 6| Step: 6
Training loss: 1.235999345779419
Validation loss: 2.2361847956975303

Epoch: 6| Step: 7
Training loss: 1.045776605606079
Validation loss: 2.229103446006775

Epoch: 6| Step: 8
Training loss: 1.6592711210250854
Validation loss: 2.2658013105392456

Epoch: 6| Step: 9
Training loss: 2.150238513946533
Validation loss: 2.2294026215871177

Epoch: 6| Step: 10
Training loss: 0.9517170190811157
Validation loss: 2.230075160662333

Epoch: 6| Step: 11
Training loss: 1.0764961242675781
Validation loss: 2.2819841504096985

Epoch: 6| Step: 12
Training loss: 1.6361770629882812
Validation loss: 2.2655178904533386

Epoch: 6| Step: 13
Training loss: 1.8162848949432373
Validation loss: 2.257527152697245

Epoch: 315| Step: 0
Training loss: 1.1970555782318115
Validation loss: 2.2380871772766113

Epoch: 6| Step: 1
Training loss: 1.6896378993988037
Validation loss: 2.271488587061564

Epoch: 6| Step: 2
Training loss: 2.1105966567993164
Validation loss: 2.2605677048365274

Epoch: 6| Step: 3
Training loss: 1.1006616353988647
Validation loss: 2.2534029682477317

Epoch: 6| Step: 4
Training loss: 1.1130095720291138
Validation loss: 2.238533695538839

Epoch: 6| Step: 5
Training loss: 1.6616530418395996
Validation loss: 2.236364781856537

Epoch: 6| Step: 6
Training loss: 0.7041771411895752
Validation loss: 2.2848956187566123

Epoch: 6| Step: 7
Training loss: 1.9723708629608154
Validation loss: 2.2529327074686685

Epoch: 6| Step: 8
Training loss: 1.068272352218628
Validation loss: 2.2557807763417563

Epoch: 6| Step: 9
Training loss: 0.4975535273551941
Validation loss: 2.2689984838167825

Epoch: 6| Step: 10
Training loss: 1.4884467124938965
Validation loss: 2.200772245724996

Epoch: 6| Step: 11
Training loss: 1.3584496974945068
Validation loss: 2.18605774641037

Epoch: 6| Step: 12
Training loss: 0.8188942670822144
Validation loss: 2.2303267319997153

Epoch: 6| Step: 13
Training loss: 0.6835737228393555
Validation loss: 2.210877855618795

Epoch: 316| Step: 0
Training loss: 1.1762161254882812
Validation loss: 2.188166638215383

Epoch: 6| Step: 1
Training loss: 0.5560646653175354
Validation loss: 2.2093717654546103

Epoch: 6| Step: 2
Training loss: 0.9523088932037354
Validation loss: 2.228951930999756

Epoch: 6| Step: 3
Training loss: 1.2063589096069336
Validation loss: 2.221787909666697

Epoch: 6| Step: 4
Training loss: 1.2336394786834717
Validation loss: 2.233538051446279

Epoch: 6| Step: 5
Training loss: 0.6318025588989258
Validation loss: 2.1890379985173545

Epoch: 6| Step: 6
Training loss: 2.3716235160827637
Validation loss: 2.2189992864926658

Epoch: 6| Step: 7
Training loss: 1.6949622631072998
Validation loss: 2.228577276070913

Epoch: 6| Step: 8
Training loss: 1.3185207843780518
Validation loss: 2.232039531071981

Epoch: 6| Step: 9
Training loss: 1.2923150062561035
Validation loss: 2.2168959180514016

Epoch: 6| Step: 10
Training loss: 1.55885910987854
Validation loss: 2.1932016412417092

Epoch: 6| Step: 11
Training loss: 1.2184677124023438
Validation loss: 2.2215408086776733

Epoch: 6| Step: 12
Training loss: 0.9833035469055176
Validation loss: 2.1912657419840493

Epoch: 6| Step: 13
Training loss: 1.2411186695098877
Validation loss: 2.2368697921435037

Epoch: 317| Step: 0
Training loss: 1.2827513217926025
Validation loss: 2.25416092077891

Epoch: 6| Step: 1
Training loss: 1.4227056503295898
Validation loss: 2.2077063719431558

Epoch: 6| Step: 2
Training loss: 0.9897465705871582
Validation loss: 2.229250510533651

Epoch: 6| Step: 3
Training loss: 1.1037670373916626
Validation loss: 2.2285614808400473

Epoch: 6| Step: 4
Training loss: 0.6328195333480835
Validation loss: 2.223350207010905

Epoch: 6| Step: 5
Training loss: 1.6252286434173584
Validation loss: 2.2226642370224

Epoch: 6| Step: 6
Training loss: 1.2951610088348389
Validation loss: 2.2338748772939048

Epoch: 6| Step: 7
Training loss: 1.6096899509429932
Validation loss: 2.2777525385220847

Epoch: 6| Step: 8
Training loss: 1.3824493885040283
Validation loss: 2.2326967120170593

Epoch: 6| Step: 9
Training loss: 1.3567545413970947
Validation loss: 2.234406073888143

Epoch: 6| Step: 10
Training loss: 0.822746992111206
Validation loss: 2.237271507581075

Epoch: 6| Step: 11
Training loss: 1.5244888067245483
Validation loss: 2.249871631463369

Epoch: 6| Step: 12
Training loss: 1.3953797817230225
Validation loss: 2.244443674882253

Epoch: 6| Step: 13
Training loss: 0.9654761552810669
Validation loss: 2.2767334381739297

Epoch: 318| Step: 0
Training loss: 1.0821553468704224
Validation loss: 2.223753015200297

Epoch: 6| Step: 1
Training loss: 0.7748188376426697
Validation loss: 2.1979002952575684

Epoch: 6| Step: 2
Training loss: 1.2808550596237183
Validation loss: 2.218115985393524

Epoch: 6| Step: 3
Training loss: 1.2585400342941284
Validation loss: 2.256582776705424

Epoch: 6| Step: 4
Training loss: 0.8803282976150513
Validation loss: 2.2255226771036782

Epoch: 6| Step: 5
Training loss: 2.0400967597961426
Validation loss: 2.217607299486796

Epoch: 6| Step: 6
Training loss: 1.4268214702606201
Validation loss: 2.2015561064084372

Epoch: 6| Step: 7
Training loss: 1.204548716545105
Validation loss: 2.214313288529714

Epoch: 6| Step: 8
Training loss: 0.7978265285491943
Validation loss: 2.1974043448766074

Epoch: 6| Step: 9
Training loss: 0.913913369178772
Validation loss: 2.209477682908376

Epoch: 6| Step: 10
Training loss: 1.6358642578125
Validation loss: 2.2158608635266623

Epoch: 6| Step: 11
Training loss: 1.2787165641784668
Validation loss: 2.291362762451172

Epoch: 6| Step: 12
Training loss: 1.231365442276001
Validation loss: 2.251470367113749

Epoch: 6| Step: 13
Training loss: 1.743995189666748
Validation loss: 2.265824337800344

Epoch: 319| Step: 0
Training loss: 1.598081350326538
Validation loss: 2.2858489751815796

Epoch: 6| Step: 1
Training loss: 1.573486089706421
Validation loss: 2.2482656041781106

Epoch: 6| Step: 2
Training loss: 1.0213112831115723
Validation loss: 2.2274317344029746

Epoch: 6| Step: 3
Training loss: 1.4820178747177124
Validation loss: 2.2425467371940613

Epoch: 6| Step: 4
Training loss: 1.1815242767333984
Validation loss: 2.2340242664019265

Epoch: 6| Step: 5
Training loss: 1.4184906482696533
Validation loss: 2.238382637500763

Epoch: 6| Step: 6
Training loss: 1.5446665287017822
Validation loss: 2.2229804595311484

Epoch: 6| Step: 7
Training loss: 1.5581848621368408
Validation loss: 2.239251732826233

Epoch: 6| Step: 8
Training loss: 1.543548345565796
Validation loss: 2.2546072800954184

Epoch: 6| Step: 9
Training loss: 1.0085753202438354
Validation loss: 2.192873160044352

Epoch: 6| Step: 10
Training loss: 1.55424964427948
Validation loss: 2.1991487542788186

Epoch: 6| Step: 11
Training loss: 0.908320426940918
Validation loss: 2.2084993720054626

Epoch: 6| Step: 12
Training loss: 0.903070330619812
Validation loss: 2.202978730201721

Epoch: 6| Step: 13
Training loss: 0.7850194573402405
Validation loss: 2.191157857577006

Epoch: 320| Step: 0
Training loss: 1.3674557209014893
Validation loss: 2.215742508570353

Epoch: 6| Step: 1
Training loss: 1.452509880065918
Validation loss: 2.2015293637911477

Epoch: 6| Step: 2
Training loss: 0.9400880336761475
Validation loss: 2.2200727065404258

Epoch: 6| Step: 3
Training loss: 1.1280573606491089
Validation loss: 2.2183484633763633

Epoch: 6| Step: 4
Training loss: 1.8789911270141602
Validation loss: 2.2072914441426597

Epoch: 6| Step: 5
Training loss: 1.1669676303863525
Validation loss: 2.2300702929496765

Epoch: 6| Step: 6
Training loss: 1.2644938230514526
Validation loss: 2.2375882069269815

Epoch: 6| Step: 7
Training loss: 1.3103508949279785
Validation loss: 2.230880935986837

Epoch: 6| Step: 8
Training loss: 0.6714184880256653
Validation loss: 2.244104544321696

Epoch: 6| Step: 9
Training loss: 1.583777904510498
Validation loss: 2.242945969104767

Epoch: 6| Step: 10
Training loss: 0.7143068909645081
Validation loss: 2.197317878405253

Epoch: 6| Step: 11
Training loss: 1.1000324487686157
Validation loss: 2.2198524276415506

Epoch: 6| Step: 12
Training loss: 1.673384189605713
Validation loss: 2.2386313478151956

Epoch: 6| Step: 13
Training loss: 0.7955083847045898
Validation loss: 2.246396859486898

Epoch: 321| Step: 0
Training loss: 1.3228431940078735
Validation loss: 2.230518420537313

Epoch: 6| Step: 1
Training loss: 0.9857578277587891
Validation loss: 2.2418453097343445

Epoch: 6| Step: 2
Training loss: 0.9470244646072388
Validation loss: 2.2501553694407144

Epoch: 6| Step: 3
Training loss: 1.6366748809814453
Validation loss: 2.2266895174980164

Epoch: 6| Step: 4
Training loss: 1.574960708618164
Validation loss: 2.1992648045221963

Epoch: 6| Step: 5
Training loss: 1.9467778205871582
Validation loss: 2.2057238817214966

Epoch: 6| Step: 6
Training loss: 0.8747711181640625
Validation loss: 2.1943172812461853

Epoch: 6| Step: 7
Training loss: 0.6142585277557373
Validation loss: 2.19870932896932

Epoch: 6| Step: 8
Training loss: 0.980177104473114
Validation loss: 2.19782288869222

Epoch: 6| Step: 9
Training loss: 0.6269461512565613
Validation loss: 2.2174380819002786

Epoch: 6| Step: 10
Training loss: 1.9987326860427856
Validation loss: 2.2502816120783486

Epoch: 6| Step: 11
Training loss: 1.3461120128631592
Validation loss: 2.238150636355082

Epoch: 6| Step: 12
Training loss: 1.0766706466674805
Validation loss: 2.194403092066447

Epoch: 6| Step: 13
Training loss: 1.0660128593444824
Validation loss: 2.2661502361297607

Epoch: 322| Step: 0
Training loss: 1.0230909585952759
Validation loss: 2.227668205897013

Epoch: 6| Step: 1
Training loss: 1.046992540359497
Validation loss: 2.1968977451324463

Epoch: 6| Step: 2
Training loss: 0.7705879211425781
Validation loss: 2.21696803967158

Epoch: 6| Step: 3
Training loss: 0.8430066704750061
Validation loss: 2.2425742944081626

Epoch: 6| Step: 4
Training loss: 1.2570654153823853
Validation loss: 2.199117382367452

Epoch: 6| Step: 5
Training loss: 1.7613040208816528
Validation loss: 2.185208280881246

Epoch: 6| Step: 6
Training loss: 1.4683905839920044
Validation loss: 2.2241732080777488

Epoch: 6| Step: 7
Training loss: 0.5075433850288391
Validation loss: 2.2186289628346763

Epoch: 6| Step: 8
Training loss: 1.0187103748321533
Validation loss: 2.212321678797404

Epoch: 6| Step: 9
Training loss: 1.1186351776123047
Validation loss: 2.2224329511324563

Epoch: 6| Step: 10
Training loss: 1.359168529510498
Validation loss: 2.2397183179855347

Epoch: 6| Step: 11
Training loss: 1.5822449922561646
Validation loss: 2.2279597322146096

Epoch: 6| Step: 12
Training loss: 1.49916672706604
Validation loss: 2.1901996533075967

Epoch: 6| Step: 13
Training loss: 1.3186661005020142
Validation loss: 2.21959122021993

Epoch: 323| Step: 0
Training loss: 0.5883909463882446
Validation loss: 2.168644984563192

Epoch: 6| Step: 1
Training loss: 1.298666000366211
Validation loss: 2.2597087621688843

Epoch: 6| Step: 2
Training loss: 0.9135644435882568
Validation loss: 2.198412319024404

Epoch: 6| Step: 3
Training loss: 1.651464819908142
Validation loss: 2.2375958363215127

Epoch: 6| Step: 4
Training loss: 1.355013370513916
Validation loss: 2.2019887367884317

Epoch: 6| Step: 5
Training loss: 1.9183714389801025
Validation loss: 2.2083240946133933

Epoch: 6| Step: 6
Training loss: 1.7456332445144653
Validation loss: 2.2098408738772073

Epoch: 6| Step: 7
Training loss: 1.194429636001587
Validation loss: 2.2085630297660828

Epoch: 6| Step: 8
Training loss: 1.2061285972595215
Validation loss: 2.2202988862991333

Epoch: 6| Step: 9
Training loss: 0.9895880222320557
Validation loss: 2.1680880188941956

Epoch: 6| Step: 10
Training loss: 1.3590917587280273
Validation loss: 2.2413076361020408

Epoch: 6| Step: 11
Training loss: 1.275857925415039
Validation loss: 2.219925860563914

Epoch: 6| Step: 12
Training loss: 0.8394269943237305
Validation loss: 2.2236490845680237

Epoch: 6| Step: 13
Training loss: 0.890029788017273
Validation loss: 2.1878045399983725

Epoch: 324| Step: 0
Training loss: 2.0612568855285645
Validation loss: 2.214620590209961

Epoch: 6| Step: 1
Training loss: 1.323155164718628
Validation loss: 2.1827522913614907

Epoch: 6| Step: 2
Training loss: 1.2370120286941528
Validation loss: 2.249911387761434

Epoch: 6| Step: 3
Training loss: 1.099151372909546
Validation loss: 2.178114593029022

Epoch: 6| Step: 4
Training loss: 1.8675143718719482
Validation loss: 2.2447606523831687

Epoch: 6| Step: 5
Training loss: 0.8772901296615601
Validation loss: 2.214792569478353

Epoch: 6| Step: 6
Training loss: 0.7659900188446045
Validation loss: 2.225959817568461

Epoch: 6| Step: 7
Training loss: 1.2493996620178223
Validation loss: 2.2254576881726584

Epoch: 6| Step: 8
Training loss: 1.468912124633789
Validation loss: 2.2023608883221946

Epoch: 6| Step: 9
Training loss: 1.3622361421585083
Validation loss: 2.169785221417745

Epoch: 6| Step: 10
Training loss: 0.8327460885047913
Validation loss: 2.2148282130559287

Epoch: 6| Step: 11
Training loss: 0.7688603401184082
Validation loss: 2.2149277528127036

Epoch: 6| Step: 12
Training loss: 1.0041091442108154
Validation loss: 2.204577465852102

Epoch: 6| Step: 13
Training loss: 1.7070690393447876
Validation loss: 2.222961703936259

Epoch: 325| Step: 0
Training loss: 0.7441900968551636
Validation loss: 2.188540200392405

Epoch: 6| Step: 1
Training loss: 0.7652530670166016
Validation loss: 2.2189006010691323

Epoch: 6| Step: 2
Training loss: 1.5146071910858154
Validation loss: 2.1961401303609214

Epoch: 6| Step: 3
Training loss: 0.9822824001312256
Validation loss: 2.1915391286214194

Epoch: 6| Step: 4
Training loss: 1.589850664138794
Validation loss: 2.179109732309977

Epoch: 6| Step: 5
Training loss: 1.1751935482025146
Validation loss: 2.230733315149943

Epoch: 6| Step: 6
Training loss: 1.4737045764923096
Validation loss: 2.203153928120931

Epoch: 6| Step: 7
Training loss: 1.246619462966919
Validation loss: 2.21163547039032

Epoch: 6| Step: 8
Training loss: 0.8916198015213013
Validation loss: 2.181107481320699

Epoch: 6| Step: 9
Training loss: 0.865533709526062
Validation loss: 2.2334563930829368

Epoch: 6| Step: 10
Training loss: 1.6094003915786743
Validation loss: 2.214859902858734

Epoch: 6| Step: 11
Training loss: 1.524683952331543
Validation loss: 2.18988045056661

Epoch: 6| Step: 12
Training loss: 0.976909339427948
Validation loss: 2.2131251891454062

Epoch: 6| Step: 13
Training loss: 1.1706609725952148
Validation loss: 2.2394527395566306

Epoch: 326| Step: 0
Training loss: 1.0851689577102661
Validation loss: 2.2077497839927673

Epoch: 6| Step: 1
Training loss: 0.8428832292556763
Validation loss: 2.186260163784027

Epoch: 6| Step: 2
Training loss: 1.1428766250610352
Validation loss: 2.18331507841746

Epoch: 6| Step: 3
Training loss: 1.358583688735962
Validation loss: 2.2279826998710632

Epoch: 6| Step: 4
Training loss: 1.2848525047302246
Validation loss: 2.1987425684928894

Epoch: 6| Step: 5
Training loss: 0.6179720163345337
Validation loss: 2.2227463722229004

Epoch: 6| Step: 6
Training loss: 1.34394109249115
Validation loss: 2.2685175140698752

Epoch: 6| Step: 7
Training loss: 1.9191970825195312
Validation loss: 2.2505925496419272

Epoch: 6| Step: 8
Training loss: 0.814276397228241
Validation loss: 2.212194800376892

Epoch: 6| Step: 9
Training loss: 1.1649727821350098
Validation loss: 2.2342217564582825

Epoch: 6| Step: 10
Training loss: 1.6255663633346558
Validation loss: 2.2405256032943726

Epoch: 6| Step: 11
Training loss: 0.7364094257354736
Validation loss: 2.250828286012014

Epoch: 6| Step: 12
Training loss: 1.4293001890182495
Validation loss: 2.2229605317115784

Epoch: 6| Step: 13
Training loss: 1.404358148574829
Validation loss: 2.164160410563151

Epoch: 327| Step: 0
Training loss: 0.9331080913543701
Validation loss: 2.210634768009186

Epoch: 6| Step: 1
Training loss: 0.8117009997367859
Validation loss: 2.199646234512329

Epoch: 6| Step: 2
Training loss: 1.263612985610962
Validation loss: 2.238944093386332

Epoch: 6| Step: 3
Training loss: 1.1054577827453613
Validation loss: 2.2220833698908486

Epoch: 6| Step: 4
Training loss: 1.5895088911056519
Validation loss: 2.200178106625875

Epoch: 6| Step: 5
Training loss: 1.1689151525497437
Validation loss: 2.2047096689542136

Epoch: 6| Step: 6
Training loss: 0.8244853019714355
Validation loss: 2.2128857572873435

Epoch: 6| Step: 7
Training loss: 1.69461989402771
Validation loss: 2.2689280112584433

Epoch: 6| Step: 8
Training loss: 1.0882117748260498
Validation loss: 2.230846107006073

Epoch: 6| Step: 9
Training loss: 1.6694340705871582
Validation loss: 2.2069986859957376

Epoch: 6| Step: 10
Training loss: 0.7185227870941162
Validation loss: 2.1940934658050537

Epoch: 6| Step: 11
Training loss: 0.7252564430236816
Validation loss: 2.1863027612368264

Epoch: 6| Step: 12
Training loss: 1.329280138015747
Validation loss: 2.1995464166005454

Epoch: 6| Step: 13
Training loss: 0.9161046147346497
Validation loss: 2.1745789448420205

Epoch: 328| Step: 0
Training loss: 1.555734395980835
Validation loss: 2.2212539116541543

Epoch: 6| Step: 1
Training loss: 0.9556292295455933
Validation loss: 2.1926045417785645

Epoch: 6| Step: 2
Training loss: 1.5022122859954834
Validation loss: 2.170407017072042

Epoch: 6| Step: 3
Training loss: 1.429149866104126
Validation loss: 2.173371891180674

Epoch: 6| Step: 4
Training loss: 1.224589467048645
Validation loss: 2.165615200996399

Epoch: 6| Step: 5
Training loss: 1.150942325592041
Validation loss: 2.19374148050944

Epoch: 6| Step: 6
Training loss: 1.077638864517212
Validation loss: 2.197619100411733

Epoch: 6| Step: 7
Training loss: 1.4098446369171143
Validation loss: 2.197632670402527

Epoch: 6| Step: 8
Training loss: 0.9767903089523315
Validation loss: 2.235583027203878

Epoch: 6| Step: 9
Training loss: 1.2292258739471436
Validation loss: 2.203513423601786

Epoch: 6| Step: 10
Training loss: 1.3810760974884033
Validation loss: 2.227171619733175

Epoch: 6| Step: 11
Training loss: 0.8806169033050537
Validation loss: 2.2237860759099326

Epoch: 6| Step: 12
Training loss: 0.8698618412017822
Validation loss: 2.231584906578064

Epoch: 6| Step: 13
Training loss: 1.0261380672454834
Validation loss: 2.217609445254008

Epoch: 329| Step: 0
Training loss: 0.9865656495094299
Validation loss: 2.25582492351532

Epoch: 6| Step: 1
Training loss: 0.8012015223503113
Validation loss: 2.2846622268358865

Epoch: 6| Step: 2
Training loss: 1.41206693649292
Validation loss: 2.2754668394724527

Epoch: 6| Step: 3
Training loss: 1.1391932964324951
Validation loss: 2.275426189104716

Epoch: 6| Step: 4
Training loss: 0.7603857517242432
Validation loss: 2.297656019528707

Epoch: 6| Step: 5
Training loss: 1.5025889873504639
Validation loss: 2.2083951830863953

Epoch: 6| Step: 6
Training loss: 1.5448909997940063
Validation loss: 2.263359228769938

Epoch: 6| Step: 7
Training loss: 1.0648400783538818
Validation loss: 2.2294710278511047

Epoch: 6| Step: 8
Training loss: 1.0345723628997803
Validation loss: 2.258188009262085

Epoch: 6| Step: 9
Training loss: 1.236534833908081
Validation loss: 2.2444960276285806

Epoch: 6| Step: 10
Training loss: 1.4142029285430908
Validation loss: 2.246761123339335

Epoch: 6| Step: 11
Training loss: 1.032048225402832
Validation loss: 2.2389256159464517

Epoch: 6| Step: 12
Training loss: 1.0808417797088623
Validation loss: 2.234091639518738

Epoch: 6| Step: 13
Training loss: 0.8873540163040161
Validation loss: 2.214517652988434

Epoch: 330| Step: 0
Training loss: 0.9280272722244263
Validation loss: 2.2289475401242576

Epoch: 6| Step: 1
Training loss: 1.25684654712677
Validation loss: 2.2023509542147317

Epoch: 6| Step: 2
Training loss: 1.5437648296356201
Validation loss: 2.2058258652687073

Epoch: 6| Step: 3
Training loss: 1.0369058847427368
Validation loss: 2.2150914867719016

Epoch: 6| Step: 4
Training loss: 0.6057798862457275
Validation loss: 2.2101404468218484

Epoch: 6| Step: 5
Training loss: 0.41925275325775146
Validation loss: 2.1704237262407937

Epoch: 6| Step: 6
Training loss: 1.516707181930542
Validation loss: 2.1780147155125937

Epoch: 6| Step: 7
Training loss: 0.84751296043396
Validation loss: 2.2495670318603516

Epoch: 6| Step: 8
Training loss: 0.9088047742843628
Validation loss: 2.2088348269462585

Epoch: 6| Step: 9
Training loss: 2.0880978107452393
Validation loss: 2.2575507362683616

Epoch: 6| Step: 10
Training loss: 1.085528016090393
Validation loss: 2.221559683481852

Epoch: 6| Step: 11
Training loss: 1.6040334701538086
Validation loss: 2.2402939995129905

Epoch: 6| Step: 12
Training loss: 0.8619869947433472
Validation loss: 2.315104921658834

Epoch: 6| Step: 13
Training loss: 1.0620112419128418
Validation loss: 2.240999420483907

Epoch: 331| Step: 0
Training loss: 1.2335377931594849
Validation loss: 2.2235344648361206

Epoch: 6| Step: 1
Training loss: 0.8935495018959045
Validation loss: 2.2683788339296975

Epoch: 6| Step: 2
Training loss: 1.4122333526611328
Validation loss: 2.207949221134186

Epoch: 6| Step: 3
Training loss: 0.9286983013153076
Validation loss: 2.223408500353495

Epoch: 6| Step: 4
Training loss: 1.3367528915405273
Validation loss: 2.222240606943766

Epoch: 6| Step: 5
Training loss: 0.7186229228973389
Validation loss: 2.201072891553243

Epoch: 6| Step: 6
Training loss: 1.2797868251800537
Validation loss: 2.2351280649503074

Epoch: 6| Step: 7
Training loss: 1.8869600296020508
Validation loss: 2.2061538696289062

Epoch: 6| Step: 8
Training loss: 1.456342339515686
Validation loss: 2.2487961848576865

Epoch: 6| Step: 9
Training loss: 0.8413520455360413
Validation loss: 2.2250195344289145

Epoch: 6| Step: 10
Training loss: 0.8582586050033569
Validation loss: 2.212124745051066

Epoch: 6| Step: 11
Training loss: 1.080549716949463
Validation loss: 2.263944605986277

Epoch: 6| Step: 12
Training loss: 1.0419657230377197
Validation loss: 2.2434730728467307

Epoch: 6| Step: 13
Training loss: 0.9721564054489136
Validation loss: 2.226077755292257

Epoch: 332| Step: 0
Training loss: 0.780340313911438
Validation loss: 2.245230277379354

Epoch: 6| Step: 1
Training loss: 1.4268227815628052
Validation loss: 2.215567469596863

Epoch: 6| Step: 2
Training loss: 1.5943336486816406
Validation loss: 2.2045883337656655

Epoch: 6| Step: 3
Training loss: 0.8097822070121765
Validation loss: 2.1924381852149963

Epoch: 6| Step: 4
Training loss: 1.6383870840072632
Validation loss: 2.195675869782766

Epoch: 6| Step: 5
Training loss: 1.1333470344543457
Validation loss: 2.2199079195658364

Epoch: 6| Step: 6
Training loss: 1.191479206085205
Validation loss: 2.242617428302765

Epoch: 6| Step: 7
Training loss: 1.6042914390563965
Validation loss: 2.2318732738494873

Epoch: 6| Step: 8
Training loss: 0.823212206363678
Validation loss: 2.2593613862991333

Epoch: 6| Step: 9
Training loss: 0.8564330339431763
Validation loss: 2.2546912829081216

Epoch: 6| Step: 10
Training loss: 1.060868740081787
Validation loss: 2.249287764231364

Epoch: 6| Step: 11
Training loss: 0.9419276714324951
Validation loss: 2.2220590313275657

Epoch: 6| Step: 12
Training loss: 0.9663960933685303
Validation loss: 2.197710335254669

Epoch: 6| Step: 13
Training loss: 1.1724445819854736
Validation loss: 2.2004565596580505

Epoch: 333| Step: 0
Training loss: 1.1228512525558472
Validation loss: 2.218092600504557

Epoch: 6| Step: 1
Training loss: 1.2863160371780396
Validation loss: 2.2006240089734397

Epoch: 6| Step: 2
Training loss: 1.3410130739212036
Validation loss: 2.22506711880366

Epoch: 6| Step: 3
Training loss: 0.8275224566459656
Validation loss: 2.1936898827552795

Epoch: 6| Step: 4
Training loss: 0.9838789701461792
Validation loss: 2.215417265892029

Epoch: 6| Step: 5
Training loss: 0.7645719647407532
Validation loss: 2.203633745511373

Epoch: 6| Step: 6
Training loss: 1.0482888221740723
Validation loss: 2.2269285519917807

Epoch: 6| Step: 7
Training loss: 1.2850024700164795
Validation loss: 2.241882046063741

Epoch: 6| Step: 8
Training loss: 0.8961285948753357
Validation loss: 2.220982849597931

Epoch: 6| Step: 9
Training loss: 0.9526060223579407
Validation loss: 2.2464497884114585

Epoch: 6| Step: 10
Training loss: 1.0947808027267456
Validation loss: 2.2207224369049072

Epoch: 6| Step: 11
Training loss: 0.7928853631019592
Validation loss: 2.19780562321345

Epoch: 6| Step: 12
Training loss: 1.6444109678268433
Validation loss: 2.237899204095205

Epoch: 6| Step: 13
Training loss: 1.7587977647781372
Validation loss: 2.2318722208340964

Epoch: 334| Step: 0
Training loss: 0.9612404108047485
Validation loss: 2.2124829490979514

Epoch: 6| Step: 1
Training loss: 0.9948742985725403
Validation loss: 2.234089454015096

Epoch: 6| Step: 2
Training loss: 0.6112488508224487
Validation loss: 2.230212946732839

Epoch: 6| Step: 3
Training loss: 1.2372260093688965
Validation loss: 2.2332513332366943

Epoch: 6| Step: 4
Training loss: 1.7304524183273315
Validation loss: 2.204889098803202

Epoch: 6| Step: 5
Training loss: 1.5378460884094238
Validation loss: 2.1940263509750366

Epoch: 6| Step: 6
Training loss: 1.1684879064559937
Validation loss: 2.2191537419954934

Epoch: 6| Step: 7
Training loss: 0.9925767183303833
Validation loss: 2.240094840526581

Epoch: 6| Step: 8
Training loss: 0.9807008504867554
Validation loss: 2.209629774093628

Epoch: 6| Step: 9
Training loss: 1.9348740577697754
Validation loss: 2.2601252794265747

Epoch: 6| Step: 10
Training loss: 0.5229096412658691
Validation loss: 2.199026107788086

Epoch: 6| Step: 11
Training loss: 1.054833173751831
Validation loss: 2.235827108224233

Epoch: 6| Step: 12
Training loss: 0.9995672702789307
Validation loss: 2.223596751689911

Epoch: 6| Step: 13
Training loss: 0.884131908416748
Validation loss: 2.1951717734336853

Epoch: 335| Step: 0
Training loss: 0.5490957498550415
Validation loss: 2.200446387132009

Epoch: 6| Step: 1
Training loss: 1.2114038467407227
Validation loss: 2.200471262137095

Epoch: 6| Step: 2
Training loss: 0.7410920858383179
Validation loss: 2.218261162439982

Epoch: 6| Step: 3
Training loss: 1.1907849311828613
Validation loss: 2.230372170607249

Epoch: 6| Step: 4
Training loss: 1.2025856971740723
Validation loss: 2.2141339778900146

Epoch: 6| Step: 5
Training loss: 0.8777704238891602
Validation loss: 2.2278565963109336

Epoch: 6| Step: 6
Training loss: 1.1933027505874634
Validation loss: 2.240760107835134

Epoch: 6| Step: 7
Training loss: 1.279435396194458
Validation loss: 2.2568063338597617

Epoch: 6| Step: 8
Training loss: 2.1611642837524414
Validation loss: 2.2145925561587014

Epoch: 6| Step: 9
Training loss: 0.8089046478271484
Validation loss: 2.266462484995524

Epoch: 6| Step: 10
Training loss: 1.228508472442627
Validation loss: 2.238275428613027

Epoch: 6| Step: 11
Training loss: 1.277367353439331
Validation loss: 2.2470495303471885

Epoch: 6| Step: 12
Training loss: 0.8591740727424622
Validation loss: 2.220431407292684

Epoch: 6| Step: 13
Training loss: 1.375333547592163
Validation loss: 2.247904598712921

Epoch: 336| Step: 0
Training loss: 1.0931458473205566
Validation loss: 2.2626338799794516

Epoch: 6| Step: 1
Training loss: 1.3008391857147217
Validation loss: 2.254237790902456

Epoch: 6| Step: 2
Training loss: 2.172473192214966
Validation loss: 2.2336586912473044

Epoch: 6| Step: 3
Training loss: 1.1654090881347656
Validation loss: 2.2486568888028464

Epoch: 6| Step: 4
Training loss: 0.8126181364059448
Validation loss: 2.2389724453290305

Epoch: 6| Step: 5
Training loss: 0.9509239196777344
Validation loss: 2.2446327606836953

Epoch: 6| Step: 6
Training loss: 1.1489274501800537
Validation loss: 2.2800282637278237

Epoch: 6| Step: 7
Training loss: 0.9589916467666626
Validation loss: 2.233064353466034

Epoch: 6| Step: 8
Training loss: 0.6512081623077393
Validation loss: 2.1908439795176187

Epoch: 6| Step: 9
Training loss: 0.952950656414032
Validation loss: 2.224745055039724

Epoch: 6| Step: 10
Training loss: 1.5719510316848755
Validation loss: 2.216730078061422

Epoch: 6| Step: 11
Training loss: 0.8849659562110901
Validation loss: 2.2263113856315613

Epoch: 6| Step: 12
Training loss: 0.7396392822265625
Validation loss: 2.2624451915423074

Epoch: 6| Step: 13
Training loss: 1.253896713256836
Validation loss: 2.212930162747701

Epoch: 337| Step: 0
Training loss: 1.374741554260254
Validation loss: 2.221802572409312

Epoch: 6| Step: 1
Training loss: 1.1998239755630493
Validation loss: 2.2576199173927307

Epoch: 6| Step: 2
Training loss: 0.9434243440628052
Validation loss: 2.2285247445106506

Epoch: 6| Step: 3
Training loss: 0.8985121846199036
Validation loss: 2.269996464252472

Epoch: 6| Step: 4
Training loss: 0.9503204822540283
Validation loss: 2.2298452059427896

Epoch: 6| Step: 5
Training loss: 1.1975884437561035
Validation loss: 2.2742138107617698

Epoch: 6| Step: 6
Training loss: 1.303586483001709
Validation loss: 2.3024723331133523

Epoch: 6| Step: 7
Training loss: 1.2462191581726074
Validation loss: 2.246627410252889

Epoch: 6| Step: 8
Training loss: 1.0038368701934814
Validation loss: 2.2586680253346763

Epoch: 6| Step: 9
Training loss: 1.325779676437378
Validation loss: 2.2617421547571817

Epoch: 6| Step: 10
Training loss: 0.8161050081253052
Validation loss: 2.2255314191182456

Epoch: 6| Step: 11
Training loss: 0.9249941110610962
Validation loss: 2.251642723878225

Epoch: 6| Step: 12
Training loss: 1.5440731048583984
Validation loss: 2.244321286678314

Epoch: 6| Step: 13
Training loss: 1.459135890007019
Validation loss: 2.2311007579167685

Epoch: 338| Step: 0
Training loss: 0.9507864713668823
Validation loss: 2.2300808827082315

Epoch: 6| Step: 1
Training loss: 0.765395998954773
Validation loss: 2.2223276694615683

Epoch: 6| Step: 2
Training loss: 1.6221740245819092
Validation loss: 2.222754935423533

Epoch: 6| Step: 3
Training loss: 1.4326210021972656
Validation loss: 2.2813387513160706

Epoch: 6| Step: 4
Training loss: 1.0627310276031494
Validation loss: 2.2221707503000894

Epoch: 6| Step: 5
Training loss: 0.830001950263977
Validation loss: 2.30553936958313

Epoch: 6| Step: 6
Training loss: 1.2533104419708252
Validation loss: 2.294906198978424

Epoch: 6| Step: 7
Training loss: 2.031407594680786
Validation loss: 2.262125074863434

Epoch: 6| Step: 8
Training loss: 0.7006125450134277
Validation loss: 2.2641344467798867

Epoch: 6| Step: 9
Training loss: 0.9624003767967224
Validation loss: 2.2522087891896567

Epoch: 6| Step: 10
Training loss: 1.0616540908813477
Validation loss: 2.2528807322184243

Epoch: 6| Step: 11
Training loss: 0.9447695016860962
Validation loss: 2.2699370781580606

Epoch: 6| Step: 12
Training loss: 1.2781965732574463
Validation loss: 2.27310049533844

Epoch: 6| Step: 13
Training loss: 1.6499993801116943
Validation loss: 2.269606431325277

Epoch: 339| Step: 0
Training loss: 2.2859838008880615
Validation loss: 2.2828524907430015

Epoch: 6| Step: 1
Training loss: 0.849459171295166
Validation loss: 2.285945494969686

Epoch: 6| Step: 2
Training loss: 0.7085117101669312
Validation loss: 2.248871405919393

Epoch: 6| Step: 3
Training loss: 1.637333631515503
Validation loss: 2.285000522931417

Epoch: 6| Step: 4
Training loss: 1.1351814270019531
Validation loss: 2.296714504559835

Epoch: 6| Step: 5
Training loss: 0.7675814628601074
Validation loss: 2.3269472122192383

Epoch: 6| Step: 6
Training loss: 1.6512551307678223
Validation loss: 2.2709399263064065

Epoch: 6| Step: 7
Training loss: 1.2057273387908936
Validation loss: 2.2231604059537253

Epoch: 6| Step: 8
Training loss: 1.167980432510376
Validation loss: 2.2367985248565674

Epoch: 6| Step: 9
Training loss: 0.7409453392028809
Validation loss: 2.2407068610191345

Epoch: 6| Step: 10
Training loss: 1.2629801034927368
Validation loss: 2.2535866498947144

Epoch: 6| Step: 11
Training loss: 1.5586553812026978
Validation loss: 2.219704270362854

Epoch: 6| Step: 12
Training loss: 0.48161938786506653
Validation loss: 2.2160582741101584

Epoch: 6| Step: 13
Training loss: 0.8412058353424072
Validation loss: 2.2207618157068887

Epoch: 340| Step: 0
Training loss: 0.7737240791320801
Validation loss: 2.2540260950724282

Epoch: 6| Step: 1
Training loss: 1.3531701564788818
Validation loss: 2.1970753272374473

Epoch: 6| Step: 2
Training loss: 1.3654626607894897
Validation loss: 2.23186719417572

Epoch: 6| Step: 3
Training loss: 0.9502875804901123
Validation loss: 2.238462766011556

Epoch: 6| Step: 4
Training loss: 0.9805402755737305
Validation loss: 2.2330672343571982

Epoch: 6| Step: 5
Training loss: 0.9581277370452881
Validation loss: 2.2684739033381143

Epoch: 6| Step: 6
Training loss: 1.6137080192565918
Validation loss: 2.2713425556818643

Epoch: 6| Step: 7
Training loss: 0.9897022247314453
Validation loss: 2.2293126980463662

Epoch: 6| Step: 8
Training loss: 1.4685906171798706
Validation loss: 2.238789141178131

Epoch: 6| Step: 9
Training loss: 1.0348689556121826
Validation loss: 2.2255068818728128

Epoch: 6| Step: 10
Training loss: 1.0658116340637207
Validation loss: 2.2471611698468528

Epoch: 6| Step: 11
Training loss: 1.411069631576538
Validation loss: 2.2653686006863913

Epoch: 6| Step: 12
Training loss: 0.8024485111236572
Validation loss: 2.2487571239471436

Epoch: 6| Step: 13
Training loss: 1.7101904153823853
Validation loss: 2.247498552004496

Epoch: 341| Step: 0
Training loss: 0.98897385597229
Validation loss: 2.2798432111740112

Epoch: 6| Step: 1
Training loss: 1.070972204208374
Validation loss: 2.251515587170919

Epoch: 6| Step: 2
Training loss: 1.007340669631958
Validation loss: 2.2470699548721313

Epoch: 6| Step: 3
Training loss: 1.0319215059280396
Validation loss: 2.2329425613085427

Epoch: 6| Step: 4
Training loss: 1.3294250965118408
Validation loss: 2.264132539431254

Epoch: 6| Step: 5
Training loss: 1.1438981294631958
Validation loss: 2.290597081184387

Epoch: 6| Step: 6
Training loss: 1.2147760391235352
Validation loss: 2.2638482451438904

Epoch: 6| Step: 7
Training loss: 0.9370548725128174
Validation loss: 2.300969580809275

Epoch: 6| Step: 8
Training loss: 2.011331558227539
Validation loss: 2.2806005279223123

Epoch: 6| Step: 9
Training loss: 1.111159086227417
Validation loss: 2.263976534207662

Epoch: 6| Step: 10
Training loss: 0.8186096549034119
Validation loss: 2.2414275209108987

Epoch: 6| Step: 11
Training loss: 1.5097724199295044
Validation loss: 2.2195290525754294

Epoch: 6| Step: 12
Training loss: 1.2753190994262695
Validation loss: 2.2262540062268577

Epoch: 6| Step: 13
Training loss: 0.49935805797576904
Validation loss: 2.244577964146932

Epoch: 342| Step: 0
Training loss: 1.328549861907959
Validation loss: 2.2499478658040366

Epoch: 6| Step: 1
Training loss: 1.2571372985839844
Validation loss: 2.2661057511965432

Epoch: 6| Step: 2
Training loss: 0.682569682598114
Validation loss: 2.221853772799174

Epoch: 6| Step: 3
Training loss: 1.2115596532821655
Validation loss: 2.240579148133596

Epoch: 6| Step: 4
Training loss: 1.2863495349884033
Validation loss: 2.2416040102640786

Epoch: 6| Step: 5
Training loss: 1.3961751461029053
Validation loss: 2.2419377168019614

Epoch: 6| Step: 6
Training loss: 1.0424039363861084
Validation loss: 2.2392057180404663

Epoch: 6| Step: 7
Training loss: 0.981116533279419
Validation loss: 2.2646371126174927

Epoch: 6| Step: 8
Training loss: 0.8720952868461609
Validation loss: 2.2391369342803955

Epoch: 6| Step: 9
Training loss: 1.3194704055786133
Validation loss: 2.265607019265493

Epoch: 6| Step: 10
Training loss: 1.115201473236084
Validation loss: 2.2603702346483865

Epoch: 6| Step: 11
Training loss: 1.340088963508606
Validation loss: 2.2311969796816506

Epoch: 6| Step: 12
Training loss: 0.8422119617462158
Validation loss: 2.2433698574701944

Epoch: 6| Step: 13
Training loss: 0.7366787791252136
Validation loss: 2.2417993942896524

Epoch: 343| Step: 0
Training loss: 0.7495021820068359
Validation loss: 2.1837583780288696

Epoch: 6| Step: 1
Training loss: 1.144181489944458
Validation loss: 2.214158753554026

Epoch: 6| Step: 2
Training loss: 1.1825413703918457
Validation loss: 2.239244739214579

Epoch: 6| Step: 3
Training loss: 1.3051193952560425
Validation loss: 2.217545131842295

Epoch: 6| Step: 4
Training loss: 1.1703317165374756
Validation loss: 2.220519264539083

Epoch: 6| Step: 5
Training loss: 0.8029334545135498
Validation loss: 2.267173111438751

Epoch: 6| Step: 6
Training loss: 0.5792398452758789
Validation loss: 2.2507067720095315

Epoch: 6| Step: 7
Training loss: 1.3267755508422852
Validation loss: 2.2128321131070456

Epoch: 6| Step: 8
Training loss: 1.6828629970550537
Validation loss: 2.2022538582483926

Epoch: 6| Step: 9
Training loss: 1.0202598571777344
Validation loss: 2.232633431752523

Epoch: 6| Step: 10
Training loss: 1.0463571548461914
Validation loss: 2.218113382657369

Epoch: 6| Step: 11
Training loss: 0.9678829908370972
Validation loss: 2.2189004023869834

Epoch: 6| Step: 12
Training loss: 1.1036791801452637
Validation loss: 2.2274025479952493

Epoch: 6| Step: 13
Training loss: 1.2475075721740723
Validation loss: 2.2199158668518066

Epoch: 344| Step: 0
Training loss: 0.5874853730201721
Validation loss: 2.203301509221395

Epoch: 6| Step: 1
Training loss: 1.7590513229370117
Validation loss: 2.2028692960739136

Epoch: 6| Step: 2
Training loss: 1.3649506568908691
Validation loss: 2.1988678177197776

Epoch: 6| Step: 3
Training loss: 0.8904216885566711
Validation loss: 2.188996911048889

Epoch: 6| Step: 4
Training loss: 0.7894868850708008
Validation loss: 2.171971917152405

Epoch: 6| Step: 5
Training loss: 1.1077966690063477
Validation loss: 2.220609803994497

Epoch: 6| Step: 6
Training loss: 0.8393740653991699
Validation loss: 2.209829330444336

Epoch: 6| Step: 7
Training loss: 1.7039878368377686
Validation loss: 2.2391347885131836

Epoch: 6| Step: 8
Training loss: 1.3878225088119507
Validation loss: 2.2305851380030313

Epoch: 6| Step: 9
Training loss: 0.638231635093689
Validation loss: 2.2389986316363015

Epoch: 6| Step: 10
Training loss: 1.7155346870422363
Validation loss: 2.1977055867513022

Epoch: 6| Step: 11
Training loss: 1.192405343055725
Validation loss: 2.2149320244789124

Epoch: 6| Step: 12
Training loss: 1.1358833312988281
Validation loss: 2.240755538145701

Epoch: 6| Step: 13
Training loss: 0.5736083388328552
Validation loss: 2.252550224463145

Epoch: 345| Step: 0
Training loss: 0.6316350698471069
Validation loss: 2.2232093016306558

Epoch: 6| Step: 1
Training loss: 1.3396053314208984
Validation loss: 2.2198327779769897

Epoch: 6| Step: 2
Training loss: 1.2282406091690063
Validation loss: 2.2567292849222818

Epoch: 6| Step: 3
Training loss: 0.72580885887146
Validation loss: 2.2900023063023887

Epoch: 6| Step: 4
Training loss: 0.7131451368331909
Validation loss: 2.2520213524500527

Epoch: 6| Step: 5
Training loss: 1.0245065689086914
Validation loss: 2.1956360737482705

Epoch: 6| Step: 6
Training loss: 0.630170464515686
Validation loss: 2.248388866583506

Epoch: 6| Step: 7
Training loss: 0.9804533123970032
Validation loss: 2.214817464351654

Epoch: 6| Step: 8
Training loss: 1.9681206941604614
Validation loss: 2.221764087677002

Epoch: 6| Step: 9
Training loss: 1.7051658630371094
Validation loss: 2.2080111503601074

Epoch: 6| Step: 10
Training loss: 1.1869289875030518
Validation loss: 2.180645008881887

Epoch: 6| Step: 11
Training loss: 1.1552103757858276
Validation loss: 2.183881958325704

Epoch: 6| Step: 12
Training loss: 1.2244701385498047
Validation loss: 2.243721882502238

Epoch: 6| Step: 13
Training loss: 1.139648675918579
Validation loss: 2.186624228954315

Epoch: 346| Step: 0
Training loss: 0.6760526895523071
Validation loss: 2.1907758315404258

Epoch: 6| Step: 1
Training loss: 1.2732257843017578
Validation loss: 2.2203444639841714

Epoch: 6| Step: 2
Training loss: 1.1323349475860596
Validation loss: 2.213200410207113

Epoch: 6| Step: 3
Training loss: 0.918555736541748
Validation loss: 2.251932958761851

Epoch: 6| Step: 4
Training loss: 0.5882382988929749
Validation loss: 2.234705706437429

Epoch: 6| Step: 5
Training loss: 1.23583984375
Validation loss: 2.211598793665568

Epoch: 6| Step: 6
Training loss: 1.275888204574585
Validation loss: 2.2002252538998923

Epoch: 6| Step: 7
Training loss: 1.4438228607177734
Validation loss: 2.197572926680247

Epoch: 6| Step: 8
Training loss: 1.527823567390442
Validation loss: 2.236532708009084

Epoch: 6| Step: 9
Training loss: 0.5599499344825745
Validation loss: 2.237378776073456

Epoch: 6| Step: 10
Training loss: 1.3610752820968628
Validation loss: 2.230526924133301

Epoch: 6| Step: 11
Training loss: 1.009925365447998
Validation loss: 2.2136260271072388

Epoch: 6| Step: 12
Training loss: 1.3382585048675537
Validation loss: 2.235397775967916

Epoch: 6| Step: 13
Training loss: 1.137503981590271
Validation loss: 2.2280054489771524

Epoch: 347| Step: 0
Training loss: 0.9765875339508057
Validation loss: 2.2005138993263245

Epoch: 6| Step: 1
Training loss: 1.3001426458358765
Validation loss: 2.20365047454834

Epoch: 6| Step: 2
Training loss: 1.5438978672027588
Validation loss: 2.222654104232788

Epoch: 6| Step: 3
Training loss: 1.484006404876709
Validation loss: 2.2092333237330117

Epoch: 6| Step: 4
Training loss: 1.1981297731399536
Validation loss: 2.188425441582998

Epoch: 6| Step: 5
Training loss: 0.754595935344696
Validation loss: 2.201145668824514

Epoch: 6| Step: 6
Training loss: 1.1260790824890137
Validation loss: 2.2089912494023642

Epoch: 6| Step: 7
Training loss: 0.7769410610198975
Validation loss: 2.208356281121572

Epoch: 6| Step: 8
Training loss: 0.7814322710037231
Validation loss: 2.2076295614242554

Epoch: 6| Step: 9
Training loss: 1.3992388248443604
Validation loss: 2.229002137978872

Epoch: 6| Step: 10
Training loss: 0.5451390743255615
Validation loss: 2.227532386779785

Epoch: 6| Step: 11
Training loss: 0.7600648403167725
Validation loss: 2.233637015024821

Epoch: 6| Step: 12
Training loss: 0.834248423576355
Validation loss: 2.201853017012278

Epoch: 6| Step: 13
Training loss: 0.9591759443283081
Validation loss: 2.1737295389175415

Epoch: 348| Step: 0
Training loss: 0.8333567976951599
Validation loss: 2.161974549293518

Epoch: 6| Step: 1
Training loss: 1.7663655281066895
Validation loss: 2.2372926076253257

Epoch: 6| Step: 2
Training loss: 0.7723531723022461
Validation loss: 2.2579057614008584

Epoch: 6| Step: 3
Training loss: 0.7794298529624939
Validation loss: 2.2377281188964844

Epoch: 6| Step: 4
Training loss: 1.1124669313430786
Validation loss: 2.247620165348053

Epoch: 6| Step: 5
Training loss: 0.7161343097686768
Validation loss: 2.2810564041137695

Epoch: 6| Step: 6
Training loss: 0.5958532691001892
Validation loss: 2.241016368071238

Epoch: 6| Step: 7
Training loss: 1.54345703125
Validation loss: 2.243514100710551

Epoch: 6| Step: 8
Training loss: 1.0425907373428345
Validation loss: 2.233008007208506

Epoch: 6| Step: 9
Training loss: 0.8431481122970581
Validation loss: 2.2435220877329507

Epoch: 6| Step: 10
Training loss: 1.006585955619812
Validation loss: 2.263462940851847

Epoch: 6| Step: 11
Training loss: 1.337744116783142
Validation loss: 2.236447552839915

Epoch: 6| Step: 12
Training loss: 1.0404720306396484
Validation loss: 2.2746394077936807

Epoch: 6| Step: 13
Training loss: 1.5138914585113525
Validation loss: 2.237051010131836

Epoch: 349| Step: 0
Training loss: 1.163135290145874
Validation loss: 2.2422192096710205

Epoch: 6| Step: 1
Training loss: 1.2588508129119873
Validation loss: 2.1987077395121255

Epoch: 6| Step: 2
Training loss: 0.6657625436782837
Validation loss: 2.2255748907725015

Epoch: 6| Step: 3
Training loss: 0.9284824728965759
Validation loss: 2.1937828063964844

Epoch: 6| Step: 4
Training loss: 0.7299222946166992
Validation loss: 2.21583882967631

Epoch: 6| Step: 5
Training loss: 0.8224219083786011
Validation loss: 2.2280892729759216

Epoch: 6| Step: 6
Training loss: 0.741436779499054
Validation loss: 2.253020385901133

Epoch: 6| Step: 7
Training loss: 0.6044576168060303
Validation loss: 2.2472228010495505

Epoch: 6| Step: 8
Training loss: 0.5885241031646729
Validation loss: 2.2480465173721313

Epoch: 6| Step: 9
Training loss: 1.5597223043441772
Validation loss: 2.2492480675379434

Epoch: 6| Step: 10
Training loss: 1.58061683177948
Validation loss: 2.2293821573257446

Epoch: 6| Step: 11
Training loss: 1.4304022789001465
Validation loss: 2.208660622437795

Epoch: 6| Step: 12
Training loss: 1.7186901569366455
Validation loss: 2.218940774599711

Epoch: 6| Step: 13
Training loss: 0.8904457092285156
Validation loss: 2.1977880001068115

Epoch: 350| Step: 0
Training loss: 0.5999659895896912
Validation loss: 2.2270152966181436

Epoch: 6| Step: 1
Training loss: 1.348018765449524
Validation loss: 2.259233077367147

Epoch: 6| Step: 2
Training loss: 1.6897273063659668
Validation loss: 2.2009644309679666

Epoch: 6| Step: 3
Training loss: 0.9330861568450928
Validation loss: 2.2341930270195007

Epoch: 6| Step: 4
Training loss: 0.8072154521942139
Validation loss: 2.2399396300315857

Epoch: 6| Step: 5
Training loss: 0.7789592742919922
Validation loss: 2.229161560535431

Epoch: 6| Step: 6
Training loss: 1.1819473505020142
Validation loss: 2.2169849475224814

Epoch: 6| Step: 7
Training loss: 1.2222836017608643
Validation loss: 2.292709449927012

Epoch: 6| Step: 8
Training loss: 1.565106987953186
Validation loss: 2.2333314220110574

Epoch: 6| Step: 9
Training loss: 1.013354778289795
Validation loss: 2.274413069089254

Epoch: 6| Step: 10
Training loss: 0.9716687202453613
Validation loss: 2.239601194858551

Epoch: 6| Step: 11
Training loss: 1.0355154275894165
Validation loss: 2.2647668917973838

Epoch: 6| Step: 12
Training loss: 0.5607107877731323
Validation loss: 2.271078586578369

Epoch: 6| Step: 13
Training loss: 1.3244940042495728
Validation loss: 2.2490726312001548

Testing loss: 1.9692961089045025

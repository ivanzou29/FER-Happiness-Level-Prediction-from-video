Epoch: 1| Step: 0
Training loss: 5.124532699584961
Validation loss: 5.329769213994344

Epoch: 6| Step: 1
Training loss: 5.87802791595459
Validation loss: 5.327512900034587

Epoch: 6| Step: 2
Training loss: 5.650702476501465
Validation loss: 5.3252294063568115

Epoch: 6| Step: 3
Training loss: 4.669185638427734
Validation loss: 5.322973012924194

Epoch: 6| Step: 4
Training loss: 5.233285903930664
Validation loss: 5.320712169011434

Epoch: 6| Step: 5
Training loss: 4.09566593170166
Validation loss: 5.318445444107056

Epoch: 6| Step: 6
Training loss: 4.699071884155273
Validation loss: 5.316222349802653

Epoch: 6| Step: 7
Training loss: 4.95751428604126
Validation loss: 5.31401522954305

Epoch: 6| Step: 8
Training loss: 5.162765979766846
Validation loss: 5.311703443527222

Epoch: 6| Step: 9
Training loss: 6.122978687286377
Validation loss: 5.309408664703369

Epoch: 6| Step: 10
Training loss: 4.999975681304932
Validation loss: 5.306998650232951

Epoch: 6| Step: 11
Training loss: 6.172791004180908
Validation loss: 5.30449652671814

Epoch: 6| Step: 12
Training loss: 6.220803737640381
Validation loss: 5.30196475982666

Epoch: 6| Step: 13
Training loss: 6.379325866699219
Validation loss: 5.299254258473714

Epoch: 2| Step: 0
Training loss: 6.178753852844238
Validation loss: 5.2965677579243975

Epoch: 6| Step: 1
Training loss: 6.116574287414551
Validation loss: 5.293607076009114

Epoch: 6| Step: 2
Training loss: 4.064499855041504
Validation loss: 5.290512164433797

Epoch: 6| Step: 3
Training loss: 4.492499828338623
Validation loss: 5.287419239679973

Epoch: 6| Step: 4
Training loss: 4.661201000213623
Validation loss: 5.283851861953735

Epoch: 6| Step: 5
Training loss: 4.847836494445801
Validation loss: 5.280358870824178

Epoch: 6| Step: 6
Training loss: 6.165360927581787
Validation loss: 5.276610215504964

Epoch: 6| Step: 7
Training loss: 5.789709091186523
Validation loss: 5.27256973584493

Epoch: 6| Step: 8
Training loss: 5.347810745239258
Validation loss: 5.268373250961304

Epoch: 6| Step: 9
Training loss: 5.396187782287598
Validation loss: 5.263888518015544

Epoch: 6| Step: 10
Training loss: 5.902832984924316
Validation loss: 5.2591573397318525

Epoch: 6| Step: 11
Training loss: 6.031220436096191
Validation loss: 5.254014253616333

Epoch: 6| Step: 12
Training loss: 4.51058292388916
Validation loss: 5.248835484186809

Epoch: 6| Step: 13
Training loss: 5.2817277908325195
Validation loss: 5.243236144383748

Epoch: 3| Step: 0
Training loss: 5.637343406677246
Validation loss: 5.237331390380859

Epoch: 6| Step: 1
Training loss: 6.609415054321289
Validation loss: 5.231337229410808

Epoch: 6| Step: 2
Training loss: 4.911584377288818
Validation loss: 5.224775989850362

Epoch: 6| Step: 3
Training loss: 4.845791816711426
Validation loss: 5.217719316482544

Epoch: 6| Step: 4
Training loss: 5.6413655281066895
Validation loss: 5.210408449172974

Epoch: 6| Step: 5
Training loss: 4.664330005645752
Validation loss: 5.202744166056315

Epoch: 6| Step: 6
Training loss: 4.876191139221191
Validation loss: 5.194708903630574

Epoch: 6| Step: 7
Training loss: 4.58912467956543
Validation loss: 5.186285336812337

Epoch: 6| Step: 8
Training loss: 5.435652732849121
Validation loss: 5.177427212397258

Epoch: 6| Step: 9
Training loss: 4.9930315017700195
Validation loss: 5.168203115463257

Epoch: 6| Step: 10
Training loss: 4.5867509841918945
Validation loss: 5.158634662628174

Epoch: 6| Step: 11
Training loss: 5.806641578674316
Validation loss: 5.148370186487834

Epoch: 6| Step: 12
Training loss: 5.730637550354004
Validation loss: 5.137950261433919

Epoch: 6| Step: 13
Training loss: 5.2873101234436035
Validation loss: 5.126969496409099

Epoch: 4| Step: 0
Training loss: 4.806699752807617
Validation loss: 5.115981101989746

Epoch: 6| Step: 1
Training loss: 5.158306121826172
Validation loss: 5.104344924290975

Epoch: 6| Step: 2
Training loss: 4.489444255828857
Validation loss: 5.09252127011617

Epoch: 6| Step: 3
Training loss: 5.867671966552734
Validation loss: 5.0803749561309814

Epoch: 6| Step: 4
Training loss: 5.569023132324219
Validation loss: 5.067689458529155

Epoch: 6| Step: 5
Training loss: 4.188043594360352
Validation loss: 5.054752190907796

Epoch: 6| Step: 6
Training loss: 5.197545528411865
Validation loss: 5.041741927464803

Epoch: 6| Step: 7
Training loss: 5.796975135803223
Validation loss: 5.028100490570068

Epoch: 6| Step: 8
Training loss: 4.361817359924316
Validation loss: 5.014594316482544

Epoch: 6| Step: 9
Training loss: 4.661982536315918
Validation loss: 5.001112699508667

Epoch: 6| Step: 10
Training loss: 5.763817310333252
Validation loss: 4.987414598464966

Epoch: 6| Step: 11
Training loss: 4.669868469238281
Validation loss: 4.973824103673299

Epoch: 6| Step: 12
Training loss: 5.629651069641113
Validation loss: 4.959704001744588

Epoch: 6| Step: 13
Training loss: 5.383770942687988
Validation loss: 4.946094195048015

Epoch: 5| Step: 0
Training loss: 4.759893417358398
Validation loss: 4.93179988861084

Epoch: 6| Step: 1
Training loss: 4.1194233894348145
Validation loss: 4.917936086654663

Epoch: 6| Step: 2
Training loss: 5.262587547302246
Validation loss: 4.904299020767212

Epoch: 6| Step: 3
Training loss: 5.595588684082031
Validation loss: 4.890430450439453

Epoch: 6| Step: 4
Training loss: 3.588411808013916
Validation loss: 4.876044273376465

Epoch: 6| Step: 5
Training loss: 4.354313850402832
Validation loss: 4.86227019627889

Epoch: 6| Step: 6
Training loss: 5.595142841339111
Validation loss: 4.8483589092890425

Epoch: 6| Step: 7
Training loss: 6.462244987487793
Validation loss: 4.834475517272949

Epoch: 6| Step: 8
Training loss: 5.262857437133789
Validation loss: 4.820201873779297

Epoch: 6| Step: 9
Training loss: 4.703943252563477
Validation loss: 4.805782874425252

Epoch: 6| Step: 10
Training loss: 4.098624229431152
Validation loss: 4.792279561360677

Epoch: 6| Step: 11
Training loss: 5.3548808097839355
Validation loss: 4.778131087621053

Epoch: 6| Step: 12
Training loss: 5.035227298736572
Validation loss: 4.7638388474782305

Epoch: 6| Step: 13
Training loss: 4.780677795410156
Validation loss: 4.750281413396199

Epoch: 6| Step: 0
Training loss: 4.382348537445068
Validation loss: 4.735946218172709

Epoch: 6| Step: 1
Training loss: 4.14726448059082
Validation loss: 4.722402652104695

Epoch: 6| Step: 2
Training loss: 5.137907981872559
Validation loss: 4.708093086878459

Epoch: 6| Step: 3
Training loss: 4.940267562866211
Validation loss: 4.694780349731445

Epoch: 6| Step: 4
Training loss: 5.183856964111328
Validation loss: 4.681083917617798

Epoch: 6| Step: 5
Training loss: 4.375214576721191
Validation loss: 4.667560935020447

Epoch: 6| Step: 6
Training loss: 4.682175636291504
Validation loss: 4.654437939325969

Epoch: 6| Step: 7
Training loss: 4.334253311157227
Validation loss: 4.6413798332214355

Epoch: 6| Step: 8
Training loss: 4.562467098236084
Validation loss: 4.628666321436564

Epoch: 6| Step: 9
Training loss: 4.216285705566406
Validation loss: 4.616914828618367

Epoch: 6| Step: 10
Training loss: 5.811049938201904
Validation loss: 4.605273405710856

Epoch: 6| Step: 11
Training loss: 5.632608413696289
Validation loss: 4.594596028327942

Epoch: 6| Step: 12
Training loss: 4.763057708740234
Validation loss: 4.5833585659662885

Epoch: 6| Step: 13
Training loss: 4.276602745056152
Validation loss: 4.572492520014445

Epoch: 7| Step: 0
Training loss: 4.152059078216553
Validation loss: 4.562294642130534

Epoch: 6| Step: 1
Training loss: 3.6405344009399414
Validation loss: 4.55284587542216

Epoch: 6| Step: 2
Training loss: 4.263256072998047
Validation loss: 4.542428890864055

Epoch: 6| Step: 3
Training loss: 5.40521764755249
Validation loss: 4.53329078356425

Epoch: 6| Step: 4
Training loss: 5.6227126121521
Validation loss: 4.52513321240743

Epoch: 6| Step: 5
Training loss: 4.693525314331055
Validation loss: 4.51622720559438

Epoch: 6| Step: 6
Training loss: 4.362359046936035
Validation loss: 4.507128636042277

Epoch: 6| Step: 7
Training loss: 4.684814453125
Validation loss: 4.498137712478638

Epoch: 6| Step: 8
Training loss: 2.8851184844970703
Validation loss: 4.489433526992798

Epoch: 6| Step: 9
Training loss: 4.590419769287109
Validation loss: 4.481300393740336

Epoch: 6| Step: 10
Training loss: 5.628261566162109
Validation loss: 4.473579406738281

Epoch: 6| Step: 11
Training loss: 4.624846458435059
Validation loss: 4.465728521347046

Epoch: 6| Step: 12
Training loss: 5.338511943817139
Validation loss: 4.458351214726766

Epoch: 6| Step: 13
Training loss: 4.6089043617248535
Validation loss: 4.449787855148315

Epoch: 8| Step: 0
Training loss: 5.113968849182129
Validation loss: 4.441521565119426

Epoch: 6| Step: 1
Training loss: 5.358787536621094
Validation loss: 4.433847268422444

Epoch: 6| Step: 2
Training loss: 3.9087743759155273
Validation loss: 4.4258586168289185

Epoch: 6| Step: 3
Training loss: 5.5388031005859375
Validation loss: 4.4182586669921875

Epoch: 6| Step: 4
Training loss: 5.33920431137085
Validation loss: 4.410529732704163

Epoch: 6| Step: 5
Training loss: 4.126157283782959
Validation loss: 4.403685251871745

Epoch: 6| Step: 6
Training loss: 3.9392473697662354
Validation loss: 4.397354960441589

Epoch: 6| Step: 7
Training loss: 4.7383503913879395
Validation loss: 4.389907916386922

Epoch: 6| Step: 8
Training loss: 4.891135215759277
Validation loss: 4.382475773493449

Epoch: 6| Step: 9
Training loss: 4.966374397277832
Validation loss: 4.374801476796468

Epoch: 6| Step: 10
Training loss: 3.4229912757873535
Validation loss: 4.367774287859599

Epoch: 6| Step: 11
Training loss: 3.5417990684509277
Validation loss: 4.360573212305705

Epoch: 6| Step: 12
Training loss: 3.606950521469116
Validation loss: 4.35371736685435

Epoch: 6| Step: 13
Training loss: 4.646622657775879
Validation loss: 4.346757014592488

Epoch: 9| Step: 0
Training loss: 4.053830623626709
Validation loss: 4.339635531107585

Epoch: 6| Step: 1
Training loss: 5.294990539550781
Validation loss: 4.332441369692485

Epoch: 6| Step: 2
Training loss: 4.279151916503906
Validation loss: 4.324473778406779

Epoch: 6| Step: 3
Training loss: 4.4815263748168945
Validation loss: 4.317714850107829

Epoch: 6| Step: 4
Training loss: 3.3756842613220215
Validation loss: 4.310771981875102

Epoch: 6| Step: 5
Training loss: 3.4304702281951904
Validation loss: 4.30438764890035

Epoch: 6| Step: 6
Training loss: 4.561284065246582
Validation loss: 4.297937552134196

Epoch: 6| Step: 7
Training loss: 3.666198968887329
Validation loss: 4.291948318481445

Epoch: 6| Step: 8
Training loss: 5.98499870300293
Validation loss: 4.285209496815999

Epoch: 6| Step: 9
Training loss: 4.8582563400268555
Validation loss: 4.278950651486714

Epoch: 6| Step: 10
Training loss: 4.316100120544434
Validation loss: 4.271743933359782

Epoch: 6| Step: 11
Training loss: 5.021757125854492
Validation loss: 4.266045769055684

Epoch: 6| Step: 12
Training loss: 4.430335998535156
Validation loss: 4.259464939435323

Epoch: 6| Step: 13
Training loss: 4.121090888977051
Validation loss: 4.252423524856567

Epoch: 10| Step: 0
Training loss: 4.113842010498047
Validation loss: 4.245608011881511

Epoch: 6| Step: 1
Training loss: 4.374239921569824
Validation loss: 4.238356232643127

Epoch: 6| Step: 2
Training loss: 3.9588356018066406
Validation loss: 4.2324395179748535

Epoch: 6| Step: 3
Training loss: 4.722798824310303
Validation loss: 4.2254083553949995

Epoch: 6| Step: 4
Training loss: 4.622378349304199
Validation loss: 4.21864378452301

Epoch: 6| Step: 5
Training loss: 5.602669715881348
Validation loss: 4.212728341420491

Epoch: 6| Step: 6
Training loss: 4.901244640350342
Validation loss: 4.205054004987081

Epoch: 6| Step: 7
Training loss: 4.426091194152832
Validation loss: 4.198419253031413

Epoch: 6| Step: 8
Training loss: 3.7993674278259277
Validation loss: 4.191920002301534

Epoch: 6| Step: 9
Training loss: 3.3987884521484375
Validation loss: 4.185859759648641

Epoch: 6| Step: 10
Training loss: 3.262381076812744
Validation loss: 4.179331342379252

Epoch: 6| Step: 11
Training loss: 4.631486892700195
Validation loss: 4.172854542732239

Epoch: 6| Step: 12
Training loss: 4.6345319747924805
Validation loss: 4.165847222010295

Epoch: 6| Step: 13
Training loss: 4.258288860321045
Validation loss: 4.160479585329692

Epoch: 11| Step: 0
Training loss: 4.425179481506348
Validation loss: 4.153821547826131

Epoch: 6| Step: 1
Training loss: 5.411124229431152
Validation loss: 4.147909879684448

Epoch: 6| Step: 2
Training loss: 4.55136251449585
Validation loss: 4.142228523890178

Epoch: 6| Step: 3
Training loss: 3.895439386367798
Validation loss: 4.136066834131877

Epoch: 6| Step: 4
Training loss: 4.11493444442749
Validation loss: 4.1287886301676435

Epoch: 6| Step: 5
Training loss: 3.7469115257263184
Validation loss: 4.122446854909261

Epoch: 6| Step: 6
Training loss: 4.859076976776123
Validation loss: 4.1166796286900835

Epoch: 6| Step: 7
Training loss: 3.7232918739318848
Validation loss: 4.11138383547465

Epoch: 6| Step: 8
Training loss: 5.620487689971924
Validation loss: 4.105953693389893

Epoch: 6| Step: 9
Training loss: 4.952425956726074
Validation loss: 4.0997079610824585

Epoch: 6| Step: 10
Training loss: 3.8041775226593018
Validation loss: 4.092692255973816

Epoch: 6| Step: 11
Training loss: 2.949681520462036
Validation loss: 4.087509791056315

Epoch: 6| Step: 12
Training loss: 3.327772855758667
Validation loss: 4.081301887830098

Epoch: 6| Step: 13
Training loss: 4.175082206726074
Validation loss: 4.075750748316447

Epoch: 12| Step: 0
Training loss: 4.6123247146606445
Validation loss: 4.070352911949158

Epoch: 6| Step: 1
Training loss: 4.475442409515381
Validation loss: 4.06486447652181

Epoch: 6| Step: 2
Training loss: 2.910964250564575
Validation loss: 4.058578054110209

Epoch: 6| Step: 3
Training loss: 4.564198017120361
Validation loss: 4.052537639935811

Epoch: 6| Step: 4
Training loss: 4.760166168212891
Validation loss: 4.046656370162964

Epoch: 6| Step: 5
Training loss: 4.672787666320801
Validation loss: 4.040390332539876

Epoch: 6| Step: 6
Training loss: 3.3268415927886963
Validation loss: 4.03442398707072

Epoch: 6| Step: 7
Training loss: 3.1902990341186523
Validation loss: 4.028733332951863

Epoch: 6| Step: 8
Training loss: 5.40378475189209
Validation loss: 4.022988120714824

Epoch: 6| Step: 9
Training loss: 3.809121608734131
Validation loss: 4.01718004544576

Epoch: 6| Step: 10
Training loss: 4.076981544494629
Validation loss: 4.0122543176015215

Epoch: 6| Step: 11
Training loss: 5.006946563720703
Validation loss: 4.005789120992024

Epoch: 6| Step: 12
Training loss: 4.109488487243652
Validation loss: 4.000200112660726

Epoch: 6| Step: 13
Training loss: 3.5641837120056152
Validation loss: 3.9950599670410156

Epoch: 13| Step: 0
Training loss: 4.563521385192871
Validation loss: 3.9905253648757935

Epoch: 6| Step: 1
Training loss: 4.3614044189453125
Validation loss: 3.984660545984904

Epoch: 6| Step: 2
Training loss: 3.6009647846221924
Validation loss: 3.97872523466746

Epoch: 6| Step: 3
Training loss: 4.346220016479492
Validation loss: 3.9736247460047402

Epoch: 6| Step: 4
Training loss: 3.3880677223205566
Validation loss: 3.968259890874227

Epoch: 6| Step: 5
Training loss: 4.094580173492432
Validation loss: 3.962753335634867

Epoch: 6| Step: 6
Training loss: 4.628002643585205
Validation loss: 3.9578065872192383

Epoch: 6| Step: 7
Training loss: 4.1753740310668945
Validation loss: 3.951581279436747

Epoch: 6| Step: 8
Training loss: 3.5967254638671875
Validation loss: 3.945965886116028

Epoch: 6| Step: 9
Training loss: 4.412403106689453
Validation loss: 3.9400652249654136

Epoch: 6| Step: 10
Training loss: 4.318913459777832
Validation loss: 3.9349858363469443

Epoch: 6| Step: 11
Training loss: 3.9942562580108643
Validation loss: 3.929445743560791

Epoch: 6| Step: 12
Training loss: 3.43880033493042
Validation loss: 3.9250317811965942

Epoch: 6| Step: 13
Training loss: 4.483822822570801
Validation loss: 3.918343265851339

Epoch: 14| Step: 0
Training loss: 4.4142889976501465
Validation loss: 3.9127859274546304

Epoch: 6| Step: 1
Training loss: 3.806612014770508
Validation loss: 3.907486915588379

Epoch: 6| Step: 2
Training loss: 3.7573323249816895
Validation loss: 3.9022243420283

Epoch: 6| Step: 3
Training loss: 4.421281814575195
Validation loss: 3.896546483039856

Epoch: 6| Step: 4
Training loss: 3.9768309593200684
Validation loss: 3.890489856402079

Epoch: 6| Step: 5
Training loss: 4.5437211990356445
Validation loss: 3.8847901821136475

Epoch: 6| Step: 6
Training loss: 3.1964035034179688
Validation loss: 3.8789016008377075

Epoch: 6| Step: 7
Training loss: 2.89015793800354
Validation loss: 3.875593980153402

Epoch: 6| Step: 8
Training loss: 4.504192352294922
Validation loss: 3.871608018875122

Epoch: 6| Step: 9
Training loss: 4.554903507232666
Validation loss: 3.8641130526860556

Epoch: 6| Step: 10
Training loss: 5.212475299835205
Validation loss: 3.859037478764852

Epoch: 6| Step: 11
Training loss: 3.1454215049743652
Validation loss: 3.8536259730656943

Epoch: 6| Step: 12
Training loss: 3.596803665161133
Validation loss: 3.848105311393738

Epoch: 6| Step: 13
Training loss: 4.380215644836426
Validation loss: 3.842921495437622

Epoch: 15| Step: 0
Training loss: 4.81785249710083
Validation loss: 3.8375511964162192

Epoch: 6| Step: 1
Training loss: 3.9613771438598633
Validation loss: 3.8320820728937783

Epoch: 6| Step: 2
Training loss: 3.3837738037109375
Validation loss: 3.8269051710764566

Epoch: 6| Step: 3
Training loss: 4.231616973876953
Validation loss: 3.821510394414266

Epoch: 6| Step: 4
Training loss: 4.662688255310059
Validation loss: 3.816691001256307

Epoch: 6| Step: 5
Training loss: 3.4301798343658447
Validation loss: 3.8106576601664224

Epoch: 6| Step: 6
Training loss: 3.6262471675872803
Validation loss: 3.805144508679708

Epoch: 6| Step: 7
Training loss: 3.1669578552246094
Validation loss: 3.80022398630778

Epoch: 6| Step: 8
Training loss: 4.303470611572266
Validation loss: 3.7959612210591636

Epoch: 6| Step: 9
Training loss: 3.296874523162842
Validation loss: 3.7914570569992065

Epoch: 6| Step: 10
Training loss: 4.480638027191162
Validation loss: 3.7852392196655273

Epoch: 6| Step: 11
Training loss: 4.414663791656494
Validation loss: 3.7800405422846475

Epoch: 6| Step: 12
Training loss: 3.7002668380737305
Validation loss: 3.774452805519104

Epoch: 6| Step: 13
Training loss: 3.9409523010253906
Validation loss: 3.7694665988286338

Epoch: 16| Step: 0
Training loss: 4.718872547149658
Validation loss: 3.7645519177118936

Epoch: 6| Step: 1
Training loss: 3.7181429862976074
Validation loss: 3.7598408460617065

Epoch: 6| Step: 2
Training loss: 3.5645360946655273
Validation loss: 3.7546302874883017

Epoch: 6| Step: 3
Training loss: 3.115140438079834
Validation loss: 3.7498910824457803

Epoch: 6| Step: 4
Training loss: 4.073724746704102
Validation loss: 3.745031476020813

Epoch: 6| Step: 5
Training loss: 4.418979644775391
Validation loss: 3.740697701772054

Epoch: 6| Step: 6
Training loss: 4.172455310821533
Validation loss: 3.7356790701548257

Epoch: 6| Step: 7
Training loss: 3.8067679405212402
Validation loss: 3.731381893157959

Epoch: 6| Step: 8
Training loss: 3.712555408477783
Validation loss: 3.726224899291992

Epoch: 6| Step: 9
Training loss: 3.5410451889038086
Validation loss: 3.721748868624369

Epoch: 6| Step: 10
Training loss: 4.950831413269043
Validation loss: 3.717486580212911

Epoch: 6| Step: 11
Training loss: 3.6974267959594727
Validation loss: 3.712704141934713

Epoch: 6| Step: 12
Training loss: 3.6499953269958496
Validation loss: 3.708529512087504

Epoch: 6| Step: 13
Training loss: 3.3608086109161377
Validation loss: 3.7030793825785318

Epoch: 17| Step: 0
Training loss: 3.64797306060791
Validation loss: 3.698710004488627

Epoch: 6| Step: 1
Training loss: 4.190093040466309
Validation loss: 3.6942551533381143

Epoch: 6| Step: 2
Training loss: 3.3190362453460693
Validation loss: 3.68918784459432

Epoch: 6| Step: 3
Training loss: 3.199772834777832
Validation loss: 3.683575391769409

Epoch: 6| Step: 4
Training loss: 4.249983310699463
Validation loss: 3.678788423538208

Epoch: 6| Step: 5
Training loss: 5.2665252685546875
Validation loss: 3.673963268597921

Epoch: 6| Step: 6
Training loss: 2.773261547088623
Validation loss: 3.6687827110290527

Epoch: 6| Step: 7
Training loss: 5.032011032104492
Validation loss: 3.663466970125834

Epoch: 6| Step: 8
Training loss: 3.6151323318481445
Validation loss: 3.6583187580108643

Epoch: 6| Step: 9
Training loss: 3.184610366821289
Validation loss: 3.6526296933492026

Epoch: 6| Step: 10
Training loss: 3.493842601776123
Validation loss: 3.646489938100179

Epoch: 6| Step: 11
Training loss: 3.962679862976074
Validation loss: 3.6416730086008706

Epoch: 6| Step: 12
Training loss: 3.4634132385253906
Validation loss: 3.635307788848877

Epoch: 6| Step: 13
Training loss: 4.200810432434082
Validation loss: 3.6305819749832153

Epoch: 18| Step: 0
Training loss: 3.542250871658325
Validation loss: 3.624329090118408

Epoch: 6| Step: 1
Training loss: 4.6502685546875
Validation loss: 3.6190600395202637

Epoch: 6| Step: 2
Training loss: 3.361366033554077
Validation loss: 3.6137001514434814

Epoch: 6| Step: 3
Training loss: 4.180624961853027
Validation loss: 3.608523726463318

Epoch: 6| Step: 4
Training loss: 3.6196727752685547
Validation loss: 3.6032646894454956

Epoch: 6| Step: 5
Training loss: 4.137260437011719
Validation loss: 3.5977046887079873

Epoch: 6| Step: 6
Training loss: 3.164341926574707
Validation loss: 3.5927788813908896

Epoch: 6| Step: 7
Training loss: 4.347695350646973
Validation loss: 3.5871850649515786

Epoch: 6| Step: 8
Training loss: 3.824033737182617
Validation loss: 3.582488258679708

Epoch: 6| Step: 9
Training loss: 3.8948581218719482
Validation loss: 3.5784480571746826

Epoch: 6| Step: 10
Training loss: 3.2769083976745605
Validation loss: 3.5727058251698813

Epoch: 6| Step: 11
Training loss: 3.1292896270751953
Validation loss: 3.566251277923584

Epoch: 6| Step: 12
Training loss: 3.3343701362609863
Validation loss: 3.5614365339279175

Epoch: 6| Step: 13
Training loss: 4.091338157653809
Validation loss: 3.5566372076670327

Epoch: 19| Step: 0
Training loss: 3.32753849029541
Validation loss: 3.551522970199585

Epoch: 6| Step: 1
Training loss: 3.199502468109131
Validation loss: 3.546241283416748

Epoch: 6| Step: 2
Training loss: 4.5020880699157715
Validation loss: 3.5413646697998047

Epoch: 6| Step: 3
Training loss: 3.5069451332092285
Validation loss: 3.536035497983297

Epoch: 6| Step: 4
Training loss: 4.660233974456787
Validation loss: 3.530768076578776

Epoch: 6| Step: 5
Training loss: 3.3486862182617188
Validation loss: 3.52523144086202

Epoch: 6| Step: 6
Training loss: 4.819036483764648
Validation loss: 3.5202225049336753

Epoch: 6| Step: 7
Training loss: 2.976038932800293
Validation loss: 3.5150734583536782

Epoch: 6| Step: 8
Training loss: 2.588834762573242
Validation loss: 3.5095879236857095

Epoch: 6| Step: 9
Training loss: 3.1511921882629395
Validation loss: 3.504879593849182

Epoch: 6| Step: 10
Training loss: 4.170125961303711
Validation loss: 3.499521811803182

Epoch: 6| Step: 11
Training loss: 4.02820348739624
Validation loss: 3.495124657948812

Epoch: 6| Step: 12
Training loss: 3.507218599319458
Validation loss: 3.4907910426457724

Epoch: 6| Step: 13
Training loss: 3.810107707977295
Validation loss: 3.484660784403483

Epoch: 20| Step: 0
Training loss: 3.0679478645324707
Validation loss: 3.4799959659576416

Epoch: 6| Step: 1
Training loss: 3.099907636642456
Validation loss: 3.476029872894287

Epoch: 6| Step: 2
Training loss: 3.4464945793151855
Validation loss: 3.471902926762899

Epoch: 6| Step: 3
Training loss: 3.073979377746582
Validation loss: 3.466954310735067

Epoch: 6| Step: 4
Training loss: 3.422816753387451
Validation loss: 3.4619025786717734

Epoch: 6| Step: 5
Training loss: 4.305393218994141
Validation loss: 3.4572152694066367

Epoch: 6| Step: 6
Training loss: 3.896268367767334
Validation loss: 3.452544172604879

Epoch: 6| Step: 7
Training loss: 4.209663391113281
Validation loss: 3.4484616915384927

Epoch: 6| Step: 8
Training loss: 3.5690536499023438
Validation loss: 3.4436522324879966

Epoch: 6| Step: 9
Training loss: 3.9857912063598633
Validation loss: 3.439441442489624

Epoch: 6| Step: 10
Training loss: 4.581963539123535
Validation loss: 3.434703548749288

Epoch: 6| Step: 11
Training loss: 3.1081385612487793
Validation loss: 3.4292894204457602

Epoch: 6| Step: 12
Training loss: 2.9157001972198486
Validation loss: 3.4242385228474936

Epoch: 6| Step: 13
Training loss: 3.9402341842651367
Validation loss: 3.4194160302480063

Epoch: 21| Step: 0
Training loss: 3.8249826431274414
Validation loss: 3.414717117945353

Epoch: 6| Step: 1
Training loss: 2.9390125274658203
Validation loss: 3.410892208417257

Epoch: 6| Step: 2
Training loss: 4.488384246826172
Validation loss: 3.4062993129094443

Epoch: 6| Step: 3
Training loss: 4.21267032623291
Validation loss: 3.401280085245768

Epoch: 6| Step: 4
Training loss: 3.0841987133026123
Validation loss: 3.396766781806946

Epoch: 6| Step: 5
Training loss: 3.3882787227630615
Validation loss: 3.392285863558451

Epoch: 6| Step: 6
Training loss: 3.5979127883911133
Validation loss: 3.386578599611918

Epoch: 6| Step: 7
Training loss: 1.9395231008529663
Validation loss: 3.382008671760559

Epoch: 6| Step: 8
Training loss: 3.9802002906799316
Validation loss: 3.3782326380411782

Epoch: 6| Step: 9
Training loss: 4.47824764251709
Validation loss: 3.3737218777338662

Epoch: 6| Step: 10
Training loss: 3.0802345275878906
Validation loss: 3.369021415710449

Epoch: 6| Step: 11
Training loss: 3.103973388671875
Validation loss: 3.364625334739685

Epoch: 6| Step: 12
Training loss: 3.940147876739502
Validation loss: 3.3592498302459717

Epoch: 6| Step: 13
Training loss: 3.7027392387390137
Validation loss: 3.354907274246216

Epoch: 22| Step: 0
Training loss: 3.5356335639953613
Validation loss: 3.3505146900812783

Epoch: 6| Step: 1
Training loss: 2.7120394706726074
Validation loss: 3.346062262852987

Epoch: 6| Step: 2
Training loss: 3.3983705043792725
Validation loss: 3.341344674428304

Epoch: 6| Step: 3
Training loss: 3.343513011932373
Validation loss: 3.337128440539042

Epoch: 6| Step: 4
Training loss: 2.928194999694824
Validation loss: 3.3331711292266846

Epoch: 6| Step: 5
Training loss: 4.041469573974609
Validation loss: 3.32899276415507

Epoch: 6| Step: 6
Training loss: 3.8505325317382812
Validation loss: 3.3251243034998574

Epoch: 6| Step: 7
Training loss: 3.501786231994629
Validation loss: 3.3211436669031777

Epoch: 6| Step: 8
Training loss: 3.5516576766967773
Validation loss: 3.3163774808247886

Epoch: 6| Step: 9
Training loss: 3.483023166656494
Validation loss: 3.311950167020162

Epoch: 6| Step: 10
Training loss: 2.7110719680786133
Validation loss: 3.3074122269948325

Epoch: 6| Step: 11
Training loss: 3.5519771575927734
Validation loss: 3.303370793660482

Epoch: 6| Step: 12
Training loss: 4.436276912689209
Validation loss: 3.299339850743612

Epoch: 6| Step: 13
Training loss: 3.870384693145752
Validation loss: 3.295135259628296

Epoch: 23| Step: 0
Training loss: 3.9912519454956055
Validation loss: 3.2911040782928467

Epoch: 6| Step: 1
Training loss: 3.11087703704834
Validation loss: 3.286629875500997

Epoch: 6| Step: 2
Training loss: 3.5627923011779785
Validation loss: 3.2827715078989663

Epoch: 6| Step: 3
Training loss: 3.585087299346924
Validation loss: 3.27865997950236

Epoch: 6| Step: 4
Training loss: 3.379728317260742
Validation loss: 3.2742499907811484

Epoch: 6| Step: 5
Training loss: 3.240771770477295
Validation loss: 3.269663612047831

Epoch: 6| Step: 6
Training loss: 3.636780261993408
Validation loss: 3.2658042112986245

Epoch: 6| Step: 7
Training loss: 3.121809720993042
Validation loss: 3.2614678144454956

Epoch: 6| Step: 8
Training loss: 3.57525634765625
Validation loss: 3.257104833920797

Epoch: 6| Step: 9
Training loss: 3.2122583389282227
Validation loss: 3.2535948355992637

Epoch: 6| Step: 10
Training loss: 3.451115131378174
Validation loss: 3.249545693397522

Epoch: 6| Step: 11
Training loss: 3.02158522605896
Validation loss: 3.2450595696767173

Epoch: 6| Step: 12
Training loss: 4.037375450134277
Validation loss: 3.241232911745707

Epoch: 6| Step: 13
Training loss: 3.2475790977478027
Validation loss: 3.237255334854126

Epoch: 24| Step: 0
Training loss: 3.6649820804595947
Validation loss: 3.232916076978048

Epoch: 6| Step: 1
Training loss: 4.129765033721924
Validation loss: 3.2288856506347656

Epoch: 6| Step: 2
Training loss: 2.9312853813171387
Validation loss: 3.2249130805333457

Epoch: 6| Step: 3
Training loss: 3.8961572647094727
Validation loss: 3.2207398414611816

Epoch: 6| Step: 4
Training loss: 3.5566725730895996
Validation loss: 3.2167483965555825

Epoch: 6| Step: 5
Training loss: 2.653494119644165
Validation loss: 3.212261120478312

Epoch: 6| Step: 6
Training loss: 2.578791618347168
Validation loss: 3.2084550857543945

Epoch: 6| Step: 7
Training loss: 3.9473941326141357
Validation loss: 3.204417864481608

Epoch: 6| Step: 8
Training loss: 2.997342109680176
Validation loss: 3.200283964474996

Epoch: 6| Step: 9
Training loss: 4.2041754722595215
Validation loss: 3.1965579986572266

Epoch: 6| Step: 10
Training loss: 3.18692684173584
Validation loss: 3.192447861035665

Epoch: 6| Step: 11
Training loss: 3.1417598724365234
Validation loss: 3.1887457768122354

Epoch: 6| Step: 12
Training loss: 2.881425380706787
Validation loss: 3.184895873069763

Epoch: 6| Step: 13
Training loss: 3.666858434677124
Validation loss: 3.1810076236724854

Epoch: 25| Step: 0
Training loss: 2.6551551818847656
Validation loss: 3.177045782407125

Epoch: 6| Step: 1
Training loss: 3.596688985824585
Validation loss: 3.1734461784362793

Epoch: 6| Step: 2
Training loss: 3.170984983444214
Validation loss: 3.1693509419759116

Epoch: 6| Step: 3
Training loss: 3.436527729034424
Validation loss: 3.165659546852112

Epoch: 6| Step: 4
Training loss: 3.788757801055908
Validation loss: 3.1620009740193686

Epoch: 6| Step: 5
Training loss: 3.1074891090393066
Validation loss: 3.1578312714894614

Epoch: 6| Step: 6
Training loss: 3.753432273864746
Validation loss: 3.153928518295288

Epoch: 6| Step: 7
Training loss: 4.1082000732421875
Validation loss: 3.150151570638021

Epoch: 6| Step: 8
Training loss: 2.2602386474609375
Validation loss: 3.146508971850077

Epoch: 6| Step: 9
Training loss: 2.1851162910461426
Validation loss: 3.142579992612203

Epoch: 6| Step: 10
Training loss: 2.597325325012207
Validation loss: 3.1391984621683755

Epoch: 6| Step: 11
Training loss: 3.95940899848938
Validation loss: 3.1359970966974893

Epoch: 6| Step: 12
Training loss: 4.252233505249023
Validation loss: 3.132244825363159

Epoch: 6| Step: 13
Training loss: 3.8418636322021484
Validation loss: 3.1283300717671714

Epoch: 26| Step: 0
Training loss: 4.022359371185303
Validation loss: 3.1245212157567344

Epoch: 6| Step: 1
Training loss: 3.0401082038879395
Validation loss: 3.1205599705378213

Epoch: 6| Step: 2
Training loss: 2.8384573459625244
Validation loss: 3.1175758043924966

Epoch: 6| Step: 3
Training loss: 3.2687511444091797
Validation loss: 3.11359437306722

Epoch: 6| Step: 4
Training loss: 2.7315235137939453
Validation loss: 3.10898224512736

Epoch: 6| Step: 5
Training loss: 2.690042495727539
Validation loss: 3.1053189436594644

Epoch: 6| Step: 6
Training loss: 3.594632387161255
Validation loss: 3.1019524733225503

Epoch: 6| Step: 7
Training loss: 3.0855965614318848
Validation loss: 3.0986001094182334

Epoch: 6| Step: 8
Training loss: 3.3801333904266357
Validation loss: 3.0955836375554404

Epoch: 6| Step: 9
Training loss: 3.9685535430908203
Validation loss: 3.0922643343607583

Epoch: 6| Step: 10
Training loss: 3.0394949913024902
Validation loss: 3.0885727405548096

Epoch: 6| Step: 11
Training loss: 2.9751501083374023
Validation loss: 3.0851829846700034

Epoch: 6| Step: 12
Training loss: 3.80761456489563
Validation loss: 3.0816007455190024

Epoch: 6| Step: 13
Training loss: 3.6054158210754395
Validation loss: 3.0782256523768106

Epoch: 27| Step: 0
Training loss: 3.602965831756592
Validation loss: 3.074593702952067

Epoch: 6| Step: 1
Training loss: 2.9785008430480957
Validation loss: 3.0712795654932656

Epoch: 6| Step: 2
Training loss: 3.351485252380371
Validation loss: 3.067567825317383

Epoch: 6| Step: 3
Training loss: 3.3207509517669678
Validation loss: 3.064120332400004

Epoch: 6| Step: 4
Training loss: 3.380532741546631
Validation loss: 3.0604373613993325

Epoch: 6| Step: 5
Training loss: 2.3526363372802734
Validation loss: 3.056817372639974

Epoch: 6| Step: 6
Training loss: 3.9793248176574707
Validation loss: 3.053768595059713

Epoch: 6| Step: 7
Training loss: 3.111215591430664
Validation loss: 3.050402363141378

Epoch: 6| Step: 8
Training loss: 3.691645622253418
Validation loss: 3.0469743410746255

Epoch: 6| Step: 9
Training loss: 3.554884672164917
Validation loss: 3.0435644388198853

Epoch: 6| Step: 10
Training loss: 3.4912455081939697
Validation loss: 3.040208021799723

Epoch: 6| Step: 11
Training loss: 2.777386426925659
Validation loss: 3.0364720026652017

Epoch: 6| Step: 12
Training loss: 3.1195499897003174
Validation loss: 3.03310227394104

Epoch: 6| Step: 13
Training loss: 2.692746162414551
Validation loss: 3.029831369717916

Epoch: 28| Step: 0
Training loss: 3.6137614250183105
Validation loss: 3.0263654788335166

Epoch: 6| Step: 1
Training loss: 3.643505334854126
Validation loss: 3.0229185024897256

Epoch: 6| Step: 2
Training loss: 2.72141432762146
Validation loss: 3.0193063418070474

Epoch: 6| Step: 3
Training loss: 3.3756654262542725
Validation loss: 3.016013423601786

Epoch: 6| Step: 4
Training loss: 2.532021999359131
Validation loss: 3.012731989224752

Epoch: 6| Step: 5
Training loss: 2.774256944656372
Validation loss: 3.0094990730285645

Epoch: 6| Step: 6
Training loss: 3.2737207412719727
Validation loss: 3.0064131816228232

Epoch: 6| Step: 7
Training loss: 2.6984314918518066
Validation loss: 3.003177762031555

Epoch: 6| Step: 8
Training loss: 2.798200845718384
Validation loss: 3.0002557237943015

Epoch: 6| Step: 9
Training loss: 4.109947204589844
Validation loss: 2.9971070686976113

Epoch: 6| Step: 10
Training loss: 3.526243209838867
Validation loss: 2.9936097462972007

Epoch: 6| Step: 11
Training loss: 2.66656494140625
Validation loss: 2.9899975856145224

Epoch: 6| Step: 12
Training loss: 3.349968910217285
Validation loss: 2.9866881370544434

Epoch: 6| Step: 13
Training loss: 3.704796314239502
Validation loss: 2.9836165507634482

Epoch: 29| Step: 0
Training loss: 3.252078056335449
Validation loss: 2.980107625325521

Epoch: 6| Step: 1
Training loss: 3.2446343898773193
Validation loss: 2.977111577987671

Epoch: 6| Step: 2
Training loss: 2.6313507556915283
Validation loss: 2.973871370156606

Epoch: 6| Step: 3
Training loss: 3.467545986175537
Validation loss: 2.9705766439437866

Epoch: 6| Step: 4
Training loss: 2.757650375366211
Validation loss: 2.9682732025782266

Epoch: 6| Step: 5
Training loss: 3.3883020877838135
Validation loss: 2.9680397510528564

Epoch: 6| Step: 6
Training loss: 3.3150620460510254
Validation loss: 2.9735724925994873

Epoch: 6| Step: 7
Training loss: 3.2667438983917236
Validation loss: 2.962160031000773

Epoch: 6| Step: 8
Training loss: 3.3732352256774902
Validation loss: 2.956260561943054

Epoch: 6| Step: 9
Training loss: 2.9481863975524902
Validation loss: 2.952501734097799

Epoch: 6| Step: 10
Training loss: 3.0751914978027344
Validation loss: 2.94925320148468

Epoch: 6| Step: 11
Training loss: 3.6722965240478516
Validation loss: 2.9461634159088135

Epoch: 6| Step: 12
Training loss: 2.8233213424682617
Validation loss: 2.9430684248606362

Epoch: 6| Step: 13
Training loss: 2.9894673824310303
Validation loss: 2.93973970413208

Epoch: 30| Step: 0
Training loss: 3.112475633621216
Validation loss: 2.9365589221318564

Epoch: 6| Step: 1
Training loss: 3.807919502258301
Validation loss: 2.933666408061981

Epoch: 6| Step: 2
Training loss: 2.7528247833251953
Validation loss: 2.9308208227157593

Epoch: 6| Step: 3
Training loss: 3.3547887802124023
Validation loss: 2.928759058316549

Epoch: 6| Step: 4
Training loss: 3.2171313762664795
Validation loss: 2.924484928448995

Epoch: 6| Step: 5
Training loss: 2.9021689891815186
Validation loss: 2.9210991859436035

Epoch: 6| Step: 6
Training loss: 3.294856309890747
Validation loss: 2.917941451072693

Epoch: 6| Step: 7
Training loss: 2.7185821533203125
Validation loss: 2.9147628943125405

Epoch: 6| Step: 8
Training loss: 2.963561534881592
Validation loss: 2.911810358365377

Epoch: 6| Step: 9
Training loss: 4.155454158782959
Validation loss: 2.9086331129074097

Epoch: 6| Step: 10
Training loss: 2.5105230808258057
Validation loss: 2.9056343237559

Epoch: 6| Step: 11
Training loss: 3.3998537063598633
Validation loss: 2.902695814768473

Epoch: 6| Step: 12
Training loss: 2.6554040908813477
Validation loss: 2.8991928497950235

Epoch: 6| Step: 13
Training loss: 2.8457372188568115
Validation loss: 2.895819286505381

Epoch: 31| Step: 0
Training loss: 2.2246499061584473
Validation loss: 2.892895738283793

Epoch: 6| Step: 1
Training loss: 3.2669992446899414
Validation loss: 2.889411528905233

Epoch: 6| Step: 2
Training loss: 3.234362840652466
Validation loss: 2.885770877202352

Epoch: 6| Step: 3
Training loss: 3.102982521057129
Validation loss: 2.8821019331614175

Epoch: 6| Step: 4
Training loss: 3.7306034564971924
Validation loss: 2.8786280949910483

Epoch: 6| Step: 5
Training loss: 2.535931348800659
Validation loss: 2.8755319913228354

Epoch: 6| Step: 6
Training loss: 3.3848021030426025
Validation loss: 2.8726176023483276

Epoch: 6| Step: 7
Training loss: 2.7414658069610596
Validation loss: 2.8692691326141357

Epoch: 6| Step: 8
Training loss: 3.339458465576172
Validation loss: 2.866128166516622

Epoch: 6| Step: 9
Training loss: 2.9661850929260254
Validation loss: 2.8634078900019326

Epoch: 6| Step: 10
Training loss: 2.707362174987793
Validation loss: 2.8598258097966514

Epoch: 6| Step: 11
Training loss: 3.57004451751709
Validation loss: 2.8557021617889404

Epoch: 6| Step: 12
Training loss: 3.7189416885375977
Validation loss: 2.852760155995687

Epoch: 6| Step: 13
Training loss: 2.5906989574432373
Validation loss: 2.849872628847758

Epoch: 32| Step: 0
Training loss: 3.1665468215942383
Validation loss: 2.8461026748021445

Epoch: 6| Step: 1
Training loss: 3.0212554931640625
Validation loss: 2.8435351451238

Epoch: 6| Step: 2
Training loss: 3.4414727687835693
Validation loss: 2.840025703112284

Epoch: 6| Step: 3
Training loss: 3.258932590484619
Validation loss: 2.8371798197428384

Epoch: 6| Step: 4
Training loss: 3.0458061695098877
Validation loss: 2.8337550163269043

Epoch: 6| Step: 5
Training loss: 2.923513174057007
Validation loss: 2.830855965614319

Epoch: 6| Step: 6
Training loss: 2.6498637199401855
Validation loss: 2.8277713457743325

Epoch: 6| Step: 7
Training loss: 3.0007379055023193
Validation loss: 2.8251871267954507

Epoch: 6| Step: 8
Training loss: 3.682086229324341
Validation loss: 2.8221309185028076

Epoch: 6| Step: 9
Training loss: 3.1491167545318604
Validation loss: 2.819652477900187

Epoch: 6| Step: 10
Training loss: 3.005171775817871
Validation loss: 2.81582510471344

Epoch: 6| Step: 11
Training loss: 3.1121225357055664
Validation loss: 2.813779632250468

Epoch: 6| Step: 12
Training loss: 2.502309799194336
Validation loss: 2.8097867170969644

Epoch: 6| Step: 13
Training loss: 2.561856269836426
Validation loss: 2.808274567127228

Epoch: 33| Step: 0
Training loss: 3.04978609085083
Validation loss: 2.804242730140686

Epoch: 6| Step: 1
Training loss: 3.2359578609466553
Validation loss: 2.8033231099446616

Epoch: 6| Step: 2
Training loss: 2.691725969314575
Validation loss: 2.79952609539032

Epoch: 6| Step: 3
Training loss: 3.7016148567199707
Validation loss: 2.7995512882868447

Epoch: 6| Step: 4
Training loss: 2.564877986907959
Validation loss: 2.7971996466318765

Epoch: 6| Step: 5
Training loss: 3.58878755569458
Validation loss: 2.797610561052958

Epoch: 6| Step: 6
Training loss: 3.054786443710327
Validation loss: 2.788476506868998

Epoch: 6| Step: 7
Training loss: 2.201382637023926
Validation loss: 2.783863584200541

Epoch: 6| Step: 8
Training loss: 2.418637275695801
Validation loss: 2.780678073565165

Epoch: 6| Step: 9
Training loss: 3.241089344024658
Validation loss: 2.7793259620666504

Epoch: 6| Step: 10
Training loss: 3.0113344192504883
Validation loss: 2.7763869365056357

Epoch: 6| Step: 11
Training loss: 2.9590108394622803
Validation loss: 2.7734785874684653

Epoch: 6| Step: 12
Training loss: 2.8961033821105957
Validation loss: 2.770867665608724

Epoch: 6| Step: 13
Training loss: 3.373258113861084
Validation loss: 2.7674049933751426

Epoch: 34| Step: 0
Training loss: 2.035712242126465
Validation loss: 2.764441708723704

Epoch: 6| Step: 1
Training loss: 2.888723850250244
Validation loss: 2.761681159337362

Epoch: 6| Step: 2
Training loss: 3.453887701034546
Validation loss: 2.760637323061625

Epoch: 6| Step: 3
Training loss: 3.339137554168701
Validation loss: 2.7565666834513345

Epoch: 6| Step: 4
Training loss: 2.970736026763916
Validation loss: 2.754082361857096

Epoch: 6| Step: 5
Training loss: 3.3474559783935547
Validation loss: 2.7518295844395957

Epoch: 6| Step: 6
Training loss: 2.288851737976074
Validation loss: 2.748560388882955

Epoch: 6| Step: 7
Training loss: 3.4338793754577637
Validation loss: 2.74551260471344

Epoch: 6| Step: 8
Training loss: 2.497558116912842
Validation loss: 2.742637515068054

Epoch: 6| Step: 9
Training loss: 3.284047842025757
Validation loss: 2.73993714650472

Epoch: 6| Step: 10
Training loss: 3.535586357116699
Validation loss: 2.737390955289205

Epoch: 6| Step: 11
Training loss: 3.3331971168518066
Validation loss: 2.735379378000895

Epoch: 6| Step: 12
Training loss: 2.2592997550964355
Validation loss: 2.7315989335378013

Epoch: 6| Step: 13
Training loss: 2.745819091796875
Validation loss: 2.730858087539673

Epoch: 35| Step: 0
Training loss: 2.8411598205566406
Validation loss: 2.7305977741877236

Epoch: 6| Step: 1
Training loss: 3.0775718688964844
Validation loss: 2.7260082165400186

Epoch: 6| Step: 2
Training loss: 3.111828565597534
Validation loss: 2.725012183189392

Epoch: 6| Step: 3
Training loss: 3.184572219848633
Validation loss: 2.7221864064534507

Epoch: 6| Step: 4
Training loss: 2.6194076538085938
Validation loss: 2.716665744781494

Epoch: 6| Step: 5
Training loss: 2.8650248050689697
Validation loss: 2.7144967714945474

Epoch: 6| Step: 6
Training loss: 3.0324623584747314
Validation loss: 2.7131871382395425

Epoch: 6| Step: 7
Training loss: 2.874967098236084
Validation loss: 2.713720520337423

Epoch: 6| Step: 8
Training loss: 2.5964243412017822
Validation loss: 2.7091426054636636

Epoch: 6| Step: 9
Training loss: 2.9487733840942383
Validation loss: 2.7060609658559165

Epoch: 6| Step: 10
Training loss: 2.5396008491516113
Validation loss: 2.7041008472442627

Epoch: 6| Step: 11
Training loss: 3.2493515014648438
Validation loss: 2.701129158337911

Epoch: 6| Step: 12
Training loss: 2.776459217071533
Validation loss: 2.6970266103744507

Epoch: 6| Step: 13
Training loss: 3.0968074798583984
Validation loss: 2.6935484409332275

Epoch: 36| Step: 0
Training loss: 2.7677431106567383
Validation loss: 2.6920594771703086

Epoch: 6| Step: 1
Training loss: 2.6397182941436768
Validation loss: 2.6936798691749573

Epoch: 6| Step: 2
Training loss: 2.6726226806640625
Validation loss: 2.6886109511057534

Epoch: 6| Step: 3
Training loss: 2.839153289794922
Validation loss: 2.684943437576294

Epoch: 6| Step: 4
Training loss: 3.1193275451660156
Validation loss: 2.685545007387797

Epoch: 6| Step: 5
Training loss: 2.2063450813293457
Validation loss: 2.6889225045839944

Epoch: 6| Step: 6
Training loss: 3.7839314937591553
Validation loss: 2.673051575819651

Epoch: 6| Step: 7
Training loss: 2.3011984825134277
Validation loss: 2.670253793398539

Epoch: 6| Step: 8
Training loss: 2.7208797931671143
Validation loss: 2.667299826939901

Epoch: 6| Step: 9
Training loss: 3.2409720420837402
Validation loss: 2.6650031010309854

Epoch: 6| Step: 10
Training loss: 2.8818140029907227
Validation loss: 2.6632397174835205

Epoch: 6| Step: 11
Training loss: 3.4447178840637207
Validation loss: 2.660731871922811

Epoch: 6| Step: 12
Training loss: 2.816136360168457
Validation loss: 2.6590106884638467

Epoch: 6| Step: 13
Training loss: 2.7828240394592285
Validation loss: 2.6545738776524863

Epoch: 37| Step: 0
Training loss: 3.0546789169311523
Validation loss: 2.652008056640625

Epoch: 6| Step: 1
Training loss: 3.8312971591949463
Validation loss: 2.64724854628245

Epoch: 6| Step: 2
Training loss: 2.5111069679260254
Validation loss: 2.6517070134480796

Epoch: 6| Step: 3
Training loss: 2.5992774963378906
Validation loss: 2.656826138496399

Epoch: 6| Step: 4
Training loss: 1.9484678506851196
Validation loss: 2.6497140526771545

Epoch: 6| Step: 5
Training loss: 3.107649803161621
Validation loss: 2.6357425451278687

Epoch: 6| Step: 6
Training loss: 2.751610040664673
Validation loss: 2.633833368619283

Epoch: 6| Step: 7
Training loss: 3.3031601905822754
Validation loss: 2.631520469983419

Epoch: 6| Step: 8
Training loss: 2.767770290374756
Validation loss: 2.62792440255483

Epoch: 6| Step: 9
Training loss: 2.942688465118408
Validation loss: 2.626017173131307

Epoch: 6| Step: 10
Training loss: 3.3628592491149902
Validation loss: 2.6237302819887796

Epoch: 6| Step: 11
Training loss: 2.370236396789551
Validation loss: 2.621641000111898

Epoch: 6| Step: 12
Training loss: 2.8362417221069336
Validation loss: 2.6189374128977456

Epoch: 6| Step: 13
Training loss: 2.3304529190063477
Validation loss: 2.616856058438619

Epoch: 38| Step: 0
Training loss: 3.2120747566223145
Validation loss: 2.6127851208051047

Epoch: 6| Step: 1
Training loss: 2.9623990058898926
Validation loss: 2.610148866971334

Epoch: 6| Step: 2
Training loss: 2.176456928253174
Validation loss: 2.6053452094395957

Epoch: 6| Step: 3
Training loss: 2.7068634033203125
Validation loss: 2.6015941500663757

Epoch: 6| Step: 4
Training loss: 2.178837299346924
Validation loss: 2.5992873509724936

Epoch: 6| Step: 5
Training loss: 2.868661642074585
Validation loss: 2.595626711845398

Epoch: 6| Step: 6
Training loss: 3.1543538570404053
Validation loss: 2.598645289738973

Epoch: 6| Step: 7
Training loss: 2.925276517868042
Validation loss: 2.5924357175827026

Epoch: 6| Step: 8
Training loss: 3.2295432090759277
Validation loss: 2.588259140650431

Epoch: 6| Step: 9
Training loss: 2.409127950668335
Validation loss: 2.586107770601908

Epoch: 6| Step: 10
Training loss: 2.1436522006988525
Validation loss: 2.5857731103897095

Epoch: 6| Step: 11
Training loss: 2.700232982635498
Validation loss: 2.5832736690839133

Epoch: 6| Step: 12
Training loss: 3.258777379989624
Validation loss: 2.586741884549459

Epoch: 6| Step: 13
Training loss: 3.196444511413574
Validation loss: 2.581420620282491

Epoch: 39| Step: 0
Training loss: 2.2677431106567383
Validation loss: 2.581606109937032

Epoch: 6| Step: 1
Training loss: 2.6235852241516113
Validation loss: 2.5744482278823853

Epoch: 6| Step: 2
Training loss: 2.7192914485931396
Validation loss: 2.5676933924357095

Epoch: 6| Step: 3
Training loss: 3.0781748294830322
Validation loss: 2.5655848383903503

Epoch: 6| Step: 4
Training loss: 2.5515296459198
Validation loss: 2.56548011302948

Epoch: 6| Step: 5
Training loss: 3.419163942337036
Validation loss: 2.560200810432434

Epoch: 6| Step: 6
Training loss: 3.486996650695801
Validation loss: 2.5583873987197876

Epoch: 6| Step: 7
Training loss: 2.111527919769287
Validation loss: 2.5558074315389

Epoch: 6| Step: 8
Training loss: 3.3168461322784424
Validation loss: 2.5521419644355774

Epoch: 6| Step: 9
Training loss: 2.844677448272705
Validation loss: 2.5494162241617837

Epoch: 6| Step: 10
Training loss: 2.3506600856781006
Validation loss: 2.5475285847981772

Epoch: 6| Step: 11
Training loss: 3.0211033821105957
Validation loss: 2.5448071559270224

Epoch: 6| Step: 12
Training loss: 2.3217203617095947
Validation loss: 2.543176452318827

Epoch: 6| Step: 13
Training loss: 2.4225223064422607
Validation loss: 2.538098454475403

Epoch: 40| Step: 0
Training loss: 2.9684109687805176
Validation loss: 2.536816676457723

Epoch: 6| Step: 1
Training loss: 2.6429338455200195
Validation loss: 2.5372255643208823

Epoch: 6| Step: 2
Training loss: 2.8993146419525146
Validation loss: 2.5304505030314126

Epoch: 6| Step: 3
Training loss: 2.2848258018493652
Validation loss: 2.5300307273864746

Epoch: 6| Step: 4
Training loss: 2.1053075790405273
Validation loss: 2.5272141893704734

Epoch: 6| Step: 5
Training loss: 2.5704760551452637
Validation loss: 2.523202200730642

Epoch: 6| Step: 6
Training loss: 2.4984421730041504
Validation loss: 2.5280521710713706

Epoch: 6| Step: 7
Training loss: 3.2459468841552734
Validation loss: 2.5246336857477822

Epoch: 6| Step: 8
Training loss: 3.5607104301452637
Validation loss: 2.52372016509374

Epoch: 6| Step: 9
Training loss: 3.534785032272339
Validation loss: 2.5139315128326416

Epoch: 6| Step: 10
Training loss: 2.691877603530884
Validation loss: 2.5090452829996743

Epoch: 6| Step: 11
Training loss: 2.635167121887207
Validation loss: 2.5085831681887307

Epoch: 6| Step: 12
Training loss: 2.01149320602417
Validation loss: 2.504751682281494

Epoch: 6| Step: 13
Training loss: 2.370016098022461
Validation loss: 2.50503941377004

Epoch: 41| Step: 0
Training loss: 2.846508026123047
Validation loss: 2.5019481976826987

Epoch: 6| Step: 1
Training loss: 2.5827748775482178
Validation loss: 2.4994139273961387

Epoch: 6| Step: 2
Training loss: 2.8192319869995117
Validation loss: 2.4980628291765847

Epoch: 6| Step: 3
Training loss: 3.1455302238464355
Validation loss: 2.494697372118632

Epoch: 6| Step: 4
Training loss: 3.5027003288269043
Validation loss: 2.4906983375549316

Epoch: 6| Step: 5
Training loss: 2.403870105743408
Validation loss: 2.488712469736735

Epoch: 6| Step: 6
Training loss: 2.501652956008911
Validation loss: 2.4869028528531394

Epoch: 6| Step: 7
Training loss: 1.594815731048584
Validation loss: 2.4838956594467163

Epoch: 6| Step: 8
Training loss: 2.3657686710357666
Validation loss: 2.480561097462972

Epoch: 6| Step: 9
Training loss: 2.869662046432495
Validation loss: 2.474649707476298

Epoch: 6| Step: 10
Training loss: 3.3099770545959473
Validation loss: 2.4745280543963113

Epoch: 6| Step: 11
Training loss: 2.5190482139587402
Validation loss: 2.4730894565582275

Epoch: 6| Step: 12
Training loss: 2.4113216400146484
Validation loss: 2.469482183456421

Epoch: 6| Step: 13
Training loss: 2.5816564559936523
Validation loss: 2.4682135780652366

Epoch: 42| Step: 0
Training loss: 3.1156768798828125
Validation loss: 2.466894547144572

Epoch: 6| Step: 1
Training loss: 3.1365184783935547
Validation loss: 2.4648673931757608

Epoch: 6| Step: 2
Training loss: 2.5889270305633545
Validation loss: 2.459615151087443

Epoch: 6| Step: 3
Training loss: 2.247554302215576
Validation loss: 2.4559813340504966

Epoch: 6| Step: 4
Training loss: 2.8170089721679688
Validation loss: 2.4575719038645425

Epoch: 6| Step: 5
Training loss: 2.1210224628448486
Validation loss: 2.4557164510091147

Epoch: 6| Step: 6
Training loss: 3.250678539276123
Validation loss: 2.455702225367228

Epoch: 6| Step: 7
Training loss: 2.984805107116699
Validation loss: 2.454599936803182

Epoch: 6| Step: 8
Training loss: 2.1340866088867188
Validation loss: 2.44926925500234

Epoch: 6| Step: 9
Training loss: 2.93432354927063
Validation loss: 2.4423943758010864

Epoch: 6| Step: 10
Training loss: 2.4692506790161133
Validation loss: 2.441455920537313

Epoch: 6| Step: 11
Training loss: 2.4245409965515137
Validation loss: 2.43757434686025

Epoch: 6| Step: 12
Training loss: 2.42183256149292
Validation loss: 2.4378552039464316

Epoch: 6| Step: 13
Training loss: 2.2831153869628906
Validation loss: 2.432021975517273

Epoch: 43| Step: 0
Training loss: 2.5841097831726074
Validation loss: 2.4311815102895102

Epoch: 6| Step: 1
Training loss: 2.539012908935547
Validation loss: 2.4291054407755532

Epoch: 6| Step: 2
Training loss: 2.9874041080474854
Validation loss: 2.4282358487447104

Epoch: 6| Step: 3
Training loss: 2.439653158187866
Validation loss: 2.4261898597081504

Epoch: 6| Step: 4
Training loss: 2.3060176372528076
Validation loss: 2.425207813580831

Epoch: 6| Step: 5
Training loss: 2.724184513092041
Validation loss: 2.424352526664734

Epoch: 6| Step: 6
Training loss: 2.6857285499572754
Validation loss: 2.4211421807607016

Epoch: 6| Step: 7
Training loss: 2.525412082672119
Validation loss: 2.4162736336390176

Epoch: 6| Step: 8
Training loss: 2.848771572113037
Validation loss: 2.4138722022374473

Epoch: 6| Step: 9
Training loss: 1.9327441453933716
Validation loss: 2.414694905281067

Epoch: 6| Step: 10
Training loss: 2.465161085128784
Validation loss: 2.408567746480306

Epoch: 6| Step: 11
Training loss: 3.074640989303589
Validation loss: 2.4079307119051614

Epoch: 6| Step: 12
Training loss: 2.620450019836426
Validation loss: 2.401187479496002

Epoch: 6| Step: 13
Training loss: 2.7151544094085693
Validation loss: 2.4021977186203003

Epoch: 44| Step: 0
Training loss: 1.9270892143249512
Validation loss: 2.3995279471079507

Epoch: 6| Step: 1
Training loss: 2.8310348987579346
Validation loss: 2.397723933060964

Epoch: 6| Step: 2
Training loss: 2.874962329864502
Validation loss: 2.3932231267293296

Epoch: 6| Step: 3
Training loss: 2.5940823554992676
Validation loss: 2.3892372846603394

Epoch: 6| Step: 4
Training loss: 2.6968324184417725
Validation loss: 2.39056928952535

Epoch: 6| Step: 5
Training loss: 2.3972814083099365
Validation loss: 2.3864683707555137

Epoch: 6| Step: 6
Training loss: 2.667755126953125
Validation loss: 2.387114326159159

Epoch: 6| Step: 7
Training loss: 3.155919313430786
Validation loss: 2.383611520131429

Epoch: 6| Step: 8
Training loss: 2.347660779953003
Validation loss: 2.382654130458832

Epoch: 6| Step: 9
Training loss: 2.5377421379089355
Validation loss: 2.3808933099110923

Epoch: 6| Step: 10
Training loss: 3.0205037593841553
Validation loss: 2.3755082885424295

Epoch: 6| Step: 11
Training loss: 2.7660598754882812
Validation loss: 2.377569635709127

Epoch: 6| Step: 12
Training loss: 2.135585308074951
Validation loss: 2.371748924255371

Epoch: 6| Step: 13
Training loss: 1.934798002243042
Validation loss: 2.371160944302877

Epoch: 45| Step: 0
Training loss: 2.3918817043304443
Validation loss: 2.369277775287628

Epoch: 6| Step: 1
Training loss: 2.324707269668579
Validation loss: 2.369829257329305

Epoch: 6| Step: 2
Training loss: 2.864668130874634
Validation loss: 2.3747146129608154

Epoch: 6| Step: 3
Training loss: 2.636714220046997
Validation loss: 2.369896153608958

Epoch: 6| Step: 4
Training loss: 2.437926769256592
Validation loss: 2.371446192264557

Epoch: 6| Step: 5
Training loss: 2.4516143798828125
Validation loss: 2.3572742144266763

Epoch: 6| Step: 6
Training loss: 2.5014467239379883
Validation loss: 2.3515177567799888

Epoch: 6| Step: 7
Training loss: 2.477008581161499
Validation loss: 2.3501252929369607

Epoch: 6| Step: 8
Training loss: 2.65859055519104
Validation loss: 2.3512914180755615

Epoch: 6| Step: 9
Training loss: 2.888547420501709
Validation loss: 2.3488895495732627

Epoch: 6| Step: 10
Training loss: 2.3926076889038086
Validation loss: 2.3491675655047097

Epoch: 6| Step: 11
Training loss: 2.5902771949768066
Validation loss: 2.343739926815033

Epoch: 6| Step: 12
Training loss: 2.7773983478546143
Validation loss: 2.3426862955093384

Epoch: 6| Step: 13
Training loss: 1.9937939643859863
Validation loss: 2.3393786549568176

Epoch: 46| Step: 0
Training loss: 2.1557064056396484
Validation loss: 2.3379196325937905

Epoch: 6| Step: 1
Training loss: 2.6222782135009766
Validation loss: 2.3378027280171714

Epoch: 6| Step: 2
Training loss: 1.9659390449523926
Validation loss: 2.3375502030054727

Epoch: 6| Step: 3
Training loss: 1.7018136978149414
Validation loss: 2.3343745867411294

Epoch: 6| Step: 4
Training loss: 2.8888661861419678
Validation loss: 2.333810786406199

Epoch: 6| Step: 5
Training loss: 2.0869410037994385
Validation loss: 2.335182547569275

Epoch: 6| Step: 6
Training loss: 2.6075026988983154
Validation loss: 2.329615036646525

Epoch: 6| Step: 7
Training loss: 2.983145236968994
Validation loss: 2.323010563850403

Epoch: 6| Step: 8
Training loss: 2.5524399280548096
Validation loss: 2.3235172828038535

Epoch: 6| Step: 9
Training loss: 3.0259451866149902
Validation loss: 2.3147918979326882

Epoch: 6| Step: 10
Training loss: 2.4515552520751953
Validation loss: 2.3168862660725913

Epoch: 6| Step: 11
Training loss: 2.7338762283325195
Validation loss: 2.314074178536733

Epoch: 6| Step: 12
Training loss: 2.6903090476989746
Validation loss: 2.3130730390548706

Epoch: 6| Step: 13
Training loss: 2.3821651935577393
Validation loss: 2.3114030162493386

Epoch: 47| Step: 0
Training loss: 2.587573528289795
Validation loss: 2.3098095655441284

Epoch: 6| Step: 1
Training loss: 2.215040445327759
Validation loss: 2.3062895139058432

Epoch: 6| Step: 2
Training loss: 2.625370979309082
Validation loss: 2.299906770388285

Epoch: 6| Step: 3
Training loss: 2.80808162689209
Validation loss: 2.2998419801394143

Epoch: 6| Step: 4
Training loss: 2.1253790855407715
Validation loss: 2.300055464108785

Epoch: 6| Step: 5
Training loss: 2.4586658477783203
Validation loss: 2.2999114195505777

Epoch: 6| Step: 6
Training loss: 2.5405986309051514
Validation loss: 2.2964826623598733

Epoch: 6| Step: 7
Training loss: 2.4051127433776855
Validation loss: 2.2951096097628274

Epoch: 6| Step: 8
Training loss: 2.031104564666748
Validation loss: 2.291313409805298

Epoch: 6| Step: 9
Training loss: 2.684154987335205
Validation loss: 2.290673474470774

Epoch: 6| Step: 10
Training loss: 2.7089650630950928
Validation loss: 2.288556933403015

Epoch: 6| Step: 11
Training loss: 2.6106977462768555
Validation loss: 2.2833362817764282

Epoch: 6| Step: 12
Training loss: 2.42336368560791
Validation loss: 2.2862143913904824

Epoch: 6| Step: 13
Training loss: 2.1962673664093018
Validation loss: 2.2811981042226157

Epoch: 48| Step: 0
Training loss: 2.3456480503082275
Validation loss: 2.2814522981643677

Epoch: 6| Step: 1
Training loss: 2.6072216033935547
Validation loss: 2.2817604541778564

Epoch: 6| Step: 2
Training loss: 2.641561508178711
Validation loss: 2.27631405989329

Epoch: 6| Step: 3
Training loss: 2.692844867706299
Validation loss: 2.2720617055892944

Epoch: 6| Step: 4
Training loss: 2.5899741649627686
Validation loss: 2.2665374279022217

Epoch: 6| Step: 5
Training loss: 2.0022690296173096
Validation loss: 2.2646640141805015

Epoch: 6| Step: 6
Training loss: 2.6121726036071777
Validation loss: 2.261753777662913

Epoch: 6| Step: 7
Training loss: 2.553407669067383
Validation loss: 2.2616244355837503

Epoch: 6| Step: 8
Training loss: 1.954331874847412
Validation loss: 2.2573962608973184

Epoch: 6| Step: 9
Training loss: 2.741464138031006
Validation loss: 2.2537049055099487

Epoch: 6| Step: 10
Training loss: 2.4790029525756836
Validation loss: 2.2558846275011697

Epoch: 6| Step: 11
Training loss: 2.6717238426208496
Validation loss: 2.254102329413096

Epoch: 6| Step: 12
Training loss: 1.697177529335022
Validation loss: 2.2531660993893943

Epoch: 6| Step: 13
Training loss: 2.4147515296936035
Validation loss: 2.247629960378011

Epoch: 49| Step: 0
Training loss: 2.2619738578796387
Validation loss: 2.2530561089515686

Epoch: 6| Step: 1
Training loss: 2.434971809387207
Validation loss: 2.252165754636129

Epoch: 6| Step: 2
Training loss: 2.3057382106781006
Validation loss: 2.24962709347407

Epoch: 6| Step: 3
Training loss: 2.134289264678955
Validation loss: 2.243037740389506

Epoch: 6| Step: 4
Training loss: 2.5818099975585938
Validation loss: 2.2367470065752664

Epoch: 6| Step: 5
Training loss: 2.4322926998138428
Validation loss: 2.237657348314921

Epoch: 6| Step: 6
Training loss: 2.1807432174682617
Validation loss: 2.241541584332784

Epoch: 6| Step: 7
Training loss: 2.417933702468872
Validation loss: 2.245732545852661

Epoch: 6| Step: 8
Training loss: 2.5196242332458496
Validation loss: 2.2385987043380737

Epoch: 6| Step: 9
Training loss: 2.3906099796295166
Validation loss: 2.2374479174613953

Epoch: 6| Step: 10
Training loss: 1.832322597503662
Validation loss: 2.23833167552948

Epoch: 6| Step: 11
Training loss: 2.843295097351074
Validation loss: 2.2296306490898132

Epoch: 6| Step: 12
Training loss: 2.573507070541382
Validation loss: 2.2263517578442893

Epoch: 6| Step: 13
Training loss: 2.598257541656494
Validation loss: 2.2235076427459717

Epoch: 50| Step: 0
Training loss: 2.1739134788513184
Validation loss: 2.2219841678937278

Epoch: 6| Step: 1
Training loss: 2.736619472503662
Validation loss: 2.224027613798777

Epoch: 6| Step: 2
Training loss: 2.6371026039123535
Validation loss: 2.232274850209554

Epoch: 6| Step: 3
Training loss: 2.102782726287842
Validation loss: 2.238099992275238

Epoch: 6| Step: 4
Training loss: 2.9824271202087402
Validation loss: 2.2381003499031067

Epoch: 6| Step: 5
Training loss: 3.061772108078003
Validation loss: 2.231632173061371

Epoch: 6| Step: 6
Training loss: 2.580021381378174
Validation loss: 2.230853279431661

Epoch: 6| Step: 7
Training loss: 2.243734836578369
Validation loss: 2.2289896806081138

Epoch: 6| Step: 8
Training loss: 2.004643201828003
Validation loss: 2.2277901768684387

Epoch: 6| Step: 9
Training loss: 2.095458745956421
Validation loss: 2.2195158998171487

Epoch: 6| Step: 10
Training loss: 2.4440512657165527
Validation loss: 2.2120728294054666

Epoch: 6| Step: 11
Training loss: 2.0787386894226074
Validation loss: 2.2133075992266336

Epoch: 6| Step: 12
Training loss: 2.0853426456451416
Validation loss: 2.2108638286590576

Epoch: 6| Step: 13
Training loss: 2.08382248878479
Validation loss: 2.208791653315226

Epoch: 51| Step: 0
Training loss: 2.443451404571533
Validation loss: 2.204505701859792

Epoch: 6| Step: 1
Training loss: 2.2352757453918457
Validation loss: 2.1959704160690308

Epoch: 6| Step: 2
Training loss: 1.8056323528289795
Validation loss: 2.1950883070627847

Epoch: 6| Step: 3
Training loss: 2.2515439987182617
Validation loss: 2.1940818230311074

Epoch: 6| Step: 4
Training loss: 1.7358677387237549
Validation loss: 2.1919337709744773

Epoch: 6| Step: 5
Training loss: 2.536938190460205
Validation loss: 2.191052337487539

Epoch: 6| Step: 6
Training loss: 2.4524476528167725
Validation loss: 2.1891953349113464

Epoch: 6| Step: 7
Training loss: 2.0591230392456055
Validation loss: 2.1839348475138345

Epoch: 6| Step: 8
Training loss: 2.5132384300231934
Validation loss: 2.19398566087087

Epoch: 6| Step: 9
Training loss: 2.682197093963623
Validation loss: 2.191998998324076

Epoch: 6| Step: 10
Training loss: 2.50966477394104
Validation loss: 2.199510137240092

Epoch: 6| Step: 11
Training loss: 2.7581911087036133
Validation loss: 2.1854195594787598

Epoch: 6| Step: 12
Training loss: 1.7278891801834106
Validation loss: 2.175669511159261

Epoch: 6| Step: 13
Training loss: 3.101472854614258
Validation loss: 2.1782154043515525

Epoch: 52| Step: 0
Training loss: 2.48404860496521
Validation loss: 2.178657134373983

Epoch: 6| Step: 1
Training loss: 1.8978807926177979
Validation loss: 2.177814861138662

Epoch: 6| Step: 2
Training loss: 2.18068265914917
Validation loss: 2.18035622437795

Epoch: 6| Step: 3
Training loss: 2.584822654724121
Validation loss: 2.1830809712409973

Epoch: 6| Step: 4
Training loss: 2.039571762084961
Validation loss: 2.1792877117792764

Epoch: 6| Step: 5
Training loss: 2.0931010246276855
Validation loss: 2.1778711875279746

Epoch: 6| Step: 6
Training loss: 2.6988296508789062
Validation loss: 2.1792888840039573

Epoch: 6| Step: 7
Training loss: 2.62197208404541
Validation loss: 2.1771188775698342

Epoch: 6| Step: 8
Training loss: 2.4526267051696777
Validation loss: 2.176581700642904

Epoch: 6| Step: 9
Training loss: 1.9228308200836182
Validation loss: 2.174980640411377

Epoch: 6| Step: 10
Training loss: 2.808459520339966
Validation loss: 2.173432946205139

Epoch: 6| Step: 11
Training loss: 2.365436553955078
Validation loss: 2.168355484803518

Epoch: 6| Step: 12
Training loss: 1.6378644704818726
Validation loss: 2.1693804065386453

Epoch: 6| Step: 13
Training loss: 2.812469482421875
Validation loss: 2.167431732018789

Epoch: 53| Step: 0
Training loss: 2.501117706298828
Validation loss: 2.163435399532318

Epoch: 6| Step: 1
Training loss: 2.311396837234497
Validation loss: 2.1639935771624246

Epoch: 6| Step: 2
Training loss: 2.279963731765747
Validation loss: 2.1785267194112143

Epoch: 6| Step: 3
Training loss: 2.1444039344787598
Validation loss: 2.189792493979136

Epoch: 6| Step: 4
Training loss: 2.448521375656128
Validation loss: 2.1827032566070557

Epoch: 6| Step: 5
Training loss: 2.46435546875
Validation loss: 2.1724860270818076

Epoch: 6| Step: 6
Training loss: 2.642277479171753
Validation loss: 2.152862628300985

Epoch: 6| Step: 7
Training loss: 2.0334951877593994
Validation loss: 2.1482020219167075

Epoch: 6| Step: 8
Training loss: 2.318483829498291
Validation loss: 2.149494210879008

Epoch: 6| Step: 9
Training loss: 1.783034324645996
Validation loss: 2.1532697876294455

Epoch: 6| Step: 10
Training loss: 1.9698727130889893
Validation loss: 2.1568652192751565

Epoch: 6| Step: 11
Training loss: 2.2932558059692383
Validation loss: 2.155681292215983

Epoch: 6| Step: 12
Training loss: 2.926081895828247
Validation loss: 2.156229774157206

Epoch: 6| Step: 13
Training loss: 2.3308017253875732
Validation loss: 2.154571771621704

Epoch: 54| Step: 0
Training loss: 2.2653262615203857
Validation loss: 2.1558183232943215

Epoch: 6| Step: 1
Training loss: 2.4642295837402344
Validation loss: 2.1561512549718223

Epoch: 6| Step: 2
Training loss: 2.3152477741241455
Validation loss: 2.1548248330752053

Epoch: 6| Step: 3
Training loss: 2.328859329223633
Validation loss: 2.154682675997416

Epoch: 6| Step: 4
Training loss: 2.070592164993286
Validation loss: 2.151366353034973

Epoch: 6| Step: 5
Training loss: 2.087651014328003
Validation loss: 2.1507495840390525

Epoch: 6| Step: 6
Training loss: 2.2337851524353027
Validation loss: 2.147321403026581

Epoch: 6| Step: 7
Training loss: 1.8621301651000977
Validation loss: 2.1421013673146567

Epoch: 6| Step: 8
Training loss: 2.7392678260803223
Validation loss: 2.1392184098561606

Epoch: 6| Step: 9
Training loss: 2.350229501724243
Validation loss: 2.1352416276931763

Epoch: 6| Step: 10
Training loss: 2.5339479446411133
Validation loss: 2.1330947279930115

Epoch: 6| Step: 11
Training loss: 2.333285331726074
Validation loss: 2.1288447181383767

Epoch: 6| Step: 12
Training loss: 2.6039369106292725
Validation loss: 2.1264115373293557

Epoch: 6| Step: 13
Training loss: 2.171433925628662
Validation loss: 2.126700242360433

Epoch: 55| Step: 0
Training loss: 2.4301090240478516
Validation loss: 2.125597675641378

Epoch: 6| Step: 1
Training loss: 2.3112363815307617
Validation loss: 2.1378097931543985

Epoch: 6| Step: 2
Training loss: 2.567077398300171
Validation loss: 2.1324042677879333

Epoch: 6| Step: 3
Training loss: 1.9876515865325928
Validation loss: 2.128323753674825

Epoch: 6| Step: 4
Training loss: 1.9774521589279175
Validation loss: 2.1326878865559897

Epoch: 6| Step: 5
Training loss: 2.3288357257843018
Validation loss: 2.1339455445607505

Epoch: 6| Step: 6
Training loss: 1.6497557163238525
Validation loss: 2.127939601739248

Epoch: 6| Step: 7
Training loss: 2.738682508468628
Validation loss: 2.130520979563395

Epoch: 6| Step: 8
Training loss: 1.7054557800292969
Validation loss: 2.1266602873802185

Epoch: 6| Step: 9
Training loss: 2.682189464569092
Validation loss: 2.131338040033976

Epoch: 6| Step: 10
Training loss: 2.551534652709961
Validation loss: 2.134769300619761

Epoch: 6| Step: 11
Training loss: 1.997625470161438
Validation loss: 2.1193018158276877

Epoch: 6| Step: 12
Training loss: 2.914215564727783
Validation loss: 2.1286493937174478

Epoch: 6| Step: 13
Training loss: 2.25925612449646
Validation loss: 2.1210683584213257

Epoch: 56| Step: 0
Training loss: 2.6244657039642334
Validation loss: 2.1228508949279785

Epoch: 6| Step: 1
Training loss: 2.0082080364227295
Validation loss: 2.1169140934944153

Epoch: 6| Step: 2
Training loss: 2.163193702697754
Validation loss: 2.119211494922638

Epoch: 6| Step: 3
Training loss: 2.383446216583252
Validation loss: 2.1236157615979514

Epoch: 6| Step: 4
Training loss: 2.011974811553955
Validation loss: 2.1225094397862754

Epoch: 6| Step: 5
Training loss: 2.2060627937316895
Validation loss: 2.1271843115488687

Epoch: 6| Step: 6
Training loss: 2.182300329208374
Validation loss: 2.131142775217692

Epoch: 6| Step: 7
Training loss: 2.1428256034851074
Validation loss: 2.133518914381663

Epoch: 6| Step: 8
Training loss: 2.9116525650024414
Validation loss: 2.1345791021982827

Epoch: 6| Step: 9
Training loss: 2.378986120223999
Validation loss: 2.133220394452413

Epoch: 6| Step: 10
Training loss: 2.0111780166625977
Validation loss: 2.1289585630098977

Epoch: 6| Step: 11
Training loss: 2.367009162902832
Validation loss: 2.125088930130005

Epoch: 6| Step: 12
Training loss: 2.65297532081604
Validation loss: 2.122324506441752

Epoch: 6| Step: 13
Training loss: 2.13331937789917
Validation loss: 2.1166025201479592

Epoch: 57| Step: 0
Training loss: 2.290921449661255
Validation loss: 2.1136685411135354

Epoch: 6| Step: 1
Training loss: 2.2889299392700195
Validation loss: 2.1084912220637

Epoch: 6| Step: 2
Training loss: 2.039401054382324
Validation loss: 2.108536680539449

Epoch: 6| Step: 3
Training loss: 2.7191946506500244
Validation loss: 2.111739913622538

Epoch: 6| Step: 4
Training loss: 1.9853427410125732
Validation loss: 2.107486148675283

Epoch: 6| Step: 5
Training loss: 2.388425350189209
Validation loss: 2.1069279114405313

Epoch: 6| Step: 6
Training loss: 2.1262624263763428
Validation loss: 2.105005701382955

Epoch: 6| Step: 7
Training loss: 2.8096556663513184
Validation loss: 2.097104847431183

Epoch: 6| Step: 8
Training loss: 1.9371358156204224
Validation loss: 2.092739999294281

Epoch: 6| Step: 9
Training loss: 2.503192901611328
Validation loss: 2.0900204181671143

Epoch: 6| Step: 10
Training loss: 2.371558666229248
Validation loss: 2.0890917579332986

Epoch: 6| Step: 11
Training loss: 1.819888710975647
Validation loss: 2.0920949379603067

Epoch: 6| Step: 12
Training loss: 2.756052017211914
Validation loss: 2.0861432949701944

Epoch: 6| Step: 13
Training loss: 1.7322015762329102
Validation loss: 2.1044841607411704

Epoch: 58| Step: 0
Training loss: 1.7630646228790283
Validation loss: 2.105338474114736

Epoch: 6| Step: 1
Training loss: 1.8821457624435425
Validation loss: 2.1088781158129373

Epoch: 6| Step: 2
Training loss: 2.4468483924865723
Validation loss: 2.1100873748461404

Epoch: 6| Step: 3
Training loss: 2.126507043838501
Validation loss: 2.1006737550099692

Epoch: 6| Step: 4
Training loss: 2.1731109619140625
Validation loss: 2.0924580891927085

Epoch: 6| Step: 5
Training loss: 1.5658130645751953
Validation loss: 2.080855369567871

Epoch: 6| Step: 6
Training loss: 2.4498350620269775
Validation loss: 2.090894897778829

Epoch: 6| Step: 7
Training loss: 2.451718330383301
Validation loss: 2.089946726957957

Epoch: 6| Step: 8
Training loss: 2.6808009147644043
Validation loss: 2.098158518473307

Epoch: 6| Step: 9
Training loss: 2.5273661613464355
Validation loss: 2.098889728387197

Epoch: 6| Step: 10
Training loss: 2.150542736053467
Validation loss: 2.0985832611719766

Epoch: 6| Step: 11
Training loss: 2.6327786445617676
Validation loss: 2.100614388783773

Epoch: 6| Step: 12
Training loss: 2.549553394317627
Validation loss: 2.100248555342356

Epoch: 6| Step: 13
Training loss: 2.471644401550293
Validation loss: 2.0989115238189697

Epoch: 59| Step: 0
Training loss: 2.3159947395324707
Validation loss: 2.095474898815155

Epoch: 6| Step: 1
Training loss: 2.249175548553467
Validation loss: 2.093592643737793

Epoch: 6| Step: 2
Training loss: 2.1501917839050293
Validation loss: 2.0922316114107766

Epoch: 6| Step: 3
Training loss: 2.9254722595214844
Validation loss: 2.089700142542521

Epoch: 6| Step: 4
Training loss: 1.9570311307907104
Validation loss: 2.0871275464693704

Epoch: 6| Step: 5
Training loss: 2.330864667892456
Validation loss: 2.0800875425338745

Epoch: 6| Step: 6
Training loss: 2.7354536056518555
Validation loss: 2.078747510910034

Epoch: 6| Step: 7
Training loss: 1.9011708498001099
Validation loss: 2.0779630541801453

Epoch: 6| Step: 8
Training loss: 1.9455400705337524
Validation loss: 2.0774178504943848

Epoch: 6| Step: 9
Training loss: 2.1432650089263916
Validation loss: 2.077068328857422

Epoch: 6| Step: 10
Training loss: 2.262631416320801
Validation loss: 2.0803619027137756

Epoch: 6| Step: 11
Training loss: 2.2360615730285645
Validation loss: 2.075595517953237

Epoch: 6| Step: 12
Training loss: 1.772387146949768
Validation loss: 2.07894359032313

Epoch: 6| Step: 13
Training loss: 2.6695141792297363
Validation loss: 2.0668790340423584

Epoch: 60| Step: 0
Training loss: 2.5010719299316406
Validation loss: 2.072198490301768

Epoch: 6| Step: 1
Training loss: 2.0567288398742676
Validation loss: 2.0745864113171897

Epoch: 6| Step: 2
Training loss: 2.2124876976013184
Validation loss: 2.065187613169352

Epoch: 6| Step: 3
Training loss: 2.741591691970825
Validation loss: 2.0699570377667746

Epoch: 6| Step: 4
Training loss: 1.983338475227356
Validation loss: 2.066839953263601

Epoch: 6| Step: 5
Training loss: 2.386524200439453
Validation loss: 2.0760119557380676

Epoch: 6| Step: 6
Training loss: 1.6814452409744263
Validation loss: 2.061537206172943

Epoch: 6| Step: 7
Training loss: 2.063755989074707
Validation loss: 2.0560810367266336

Epoch: 6| Step: 8
Training loss: 2.085294008255005
Validation loss: 2.0559369921684265

Epoch: 6| Step: 9
Training loss: 2.6438443660736084
Validation loss: 2.061567405859629

Epoch: 6| Step: 10
Training loss: 2.2414307594299316
Validation loss: 2.065443138281504

Epoch: 6| Step: 11
Training loss: 2.6702301502227783
Validation loss: 2.0755491256713867

Epoch: 6| Step: 12
Training loss: 2.0726394653320312
Validation loss: 2.0768872499465942

Epoch: 6| Step: 13
Training loss: 2.256176233291626
Validation loss: 2.079371432463328

Epoch: 61| Step: 0
Training loss: 2.08573055267334
Validation loss: 2.080536206563314

Epoch: 6| Step: 1
Training loss: 2.15578031539917
Validation loss: 2.08098570505778

Epoch: 6| Step: 2
Training loss: 2.0914318561553955
Validation loss: 2.0818903048833213

Epoch: 6| Step: 3
Training loss: 2.7703254222869873
Validation loss: 2.0812732577323914

Epoch: 6| Step: 4
Training loss: 2.690016031265259
Validation loss: 2.0836008588473

Epoch: 6| Step: 5
Training loss: 2.909353256225586
Validation loss: 2.0757896502812705

Epoch: 6| Step: 6
Training loss: 1.9610520601272583
Validation loss: 2.0760385592778525

Epoch: 6| Step: 7
Training loss: 2.2587294578552246
Validation loss: 2.074169079462687

Epoch: 6| Step: 8
Training loss: 1.5651494264602661
Validation loss: 2.0690228740374246

Epoch: 6| Step: 9
Training loss: 2.3380370140075684
Validation loss: 2.068979879220327

Epoch: 6| Step: 10
Training loss: 2.198235034942627
Validation loss: 2.0682029922803244

Epoch: 6| Step: 11
Training loss: 2.2219772338867188
Validation loss: 2.061772664388021

Epoch: 6| Step: 12
Training loss: 2.237999200820923
Validation loss: 2.05187326669693

Epoch: 6| Step: 13
Training loss: 2.0112953186035156
Validation loss: 2.0517168839772544

Epoch: 62| Step: 0
Training loss: 2.0567100048065186
Validation loss: 2.0513128439585366

Epoch: 6| Step: 1
Training loss: 2.1021616458892822
Validation loss: 2.055338203907013

Epoch: 6| Step: 2
Training loss: 2.3372254371643066
Validation loss: 2.0625191728274026

Epoch: 6| Step: 3
Training loss: 1.990016222000122
Validation loss: 2.0616153478622437

Epoch: 6| Step: 4
Training loss: 2.815788745880127
Validation loss: 2.049878398577372

Epoch: 6| Step: 5
Training loss: 2.2458336353302
Validation loss: 2.05052242676417

Epoch: 6| Step: 6
Training loss: 1.9512685537338257
Validation loss: 2.053500235080719

Epoch: 6| Step: 7
Training loss: 2.700223684310913
Validation loss: 2.0490391651789346

Epoch: 6| Step: 8
Training loss: 2.1885170936584473
Validation loss: 2.036442140738169

Epoch: 6| Step: 9
Training loss: 2.580192804336548
Validation loss: 2.039034108320872

Epoch: 6| Step: 10
Training loss: 1.8743634223937988
Validation loss: 2.04545126358668

Epoch: 6| Step: 11
Training loss: 2.1480109691619873
Validation loss: 2.0478843450546265

Epoch: 6| Step: 12
Training loss: 1.6150484085083008
Validation loss: 2.047401507695516

Epoch: 6| Step: 13
Training loss: 2.651444911956787
Validation loss: 2.0459972620010376

Epoch: 63| Step: 0
Training loss: 1.8349711894989014
Validation loss: 2.0491474866867065

Epoch: 6| Step: 1
Training loss: 1.8780875205993652
Validation loss: 2.043618102868398

Epoch: 6| Step: 2
Training loss: 1.7965834140777588
Validation loss: 2.037054459253947

Epoch: 6| Step: 3
Training loss: 2.849475383758545
Validation loss: 2.03961447874705

Epoch: 6| Step: 4
Training loss: 2.6506471633911133
Validation loss: 2.034983058770498

Epoch: 6| Step: 5
Training loss: 2.0207653045654297
Validation loss: 2.037877380847931

Epoch: 6| Step: 6
Training loss: 2.016359329223633
Validation loss: 2.0382638374964395

Epoch: 6| Step: 7
Training loss: 2.5860564708709717
Validation loss: 2.0389466683069863

Epoch: 6| Step: 8
Training loss: 2.255859851837158
Validation loss: 2.0449939966201782

Epoch: 6| Step: 9
Training loss: 2.6157774925231934
Validation loss: 2.0438076853752136

Epoch: 6| Step: 10
Training loss: 2.2023205757141113
Validation loss: 2.044296065966288

Epoch: 6| Step: 11
Training loss: 1.88597571849823
Validation loss: 2.0458806455135345

Epoch: 6| Step: 12
Training loss: 2.4152791500091553
Validation loss: 2.0499527057011924

Epoch: 6| Step: 13
Training loss: 2.100609540939331
Validation loss: 2.0489023129145303

Epoch: 64| Step: 0
Training loss: 1.906472086906433
Validation loss: 2.0481011668841043

Epoch: 6| Step: 1
Training loss: 2.6988351345062256
Validation loss: 2.053722163041433

Epoch: 6| Step: 2
Training loss: 2.8886947631835938
Validation loss: 2.0578505992889404

Epoch: 6| Step: 3
Training loss: 2.2108161449432373
Validation loss: 2.044962922732035

Epoch: 6| Step: 4
Training loss: 1.990936040878296
Validation loss: 2.0406439701716104

Epoch: 6| Step: 5
Training loss: 2.234375476837158
Validation loss: 2.044049839178721

Epoch: 6| Step: 6
Training loss: 1.9092648029327393
Validation loss: 2.0521824757258096

Epoch: 6| Step: 7
Training loss: 2.0423085689544678
Validation loss: 2.0566119949022927

Epoch: 6| Step: 8
Training loss: 1.758933663368225
Validation loss: 2.0640116930007935

Epoch: 6| Step: 9
Training loss: 2.0664610862731934
Validation loss: 2.0587907433509827

Epoch: 6| Step: 10
Training loss: 2.1861824989318848
Validation loss: 2.0625205437342324

Epoch: 6| Step: 11
Training loss: 2.756896495819092
Validation loss: 2.0570064385732016

Epoch: 6| Step: 12
Training loss: 2.293670654296875
Validation loss: 2.0583086212476096

Epoch: 6| Step: 13
Training loss: 2.5435128211975098
Validation loss: 2.0610400438308716

Epoch: 65| Step: 0
Training loss: 2.102895736694336
Validation loss: 2.0566728512446084

Epoch: 6| Step: 1
Training loss: 2.5533480644226074
Validation loss: 2.055027882258097

Epoch: 6| Step: 2
Training loss: 2.0161025524139404
Validation loss: 2.0521939992904663

Epoch: 6| Step: 3
Training loss: 2.3300580978393555
Validation loss: 2.054403305053711

Epoch: 6| Step: 4
Training loss: 2.053568124771118
Validation loss: 2.0517309109369912

Epoch: 6| Step: 5
Training loss: 2.641892194747925
Validation loss: 2.048326770464579

Epoch: 6| Step: 6
Training loss: 1.8665990829467773
Validation loss: 2.046086331208547

Epoch: 6| Step: 7
Training loss: 3.273409366607666
Validation loss: 2.0393967231114707

Epoch: 6| Step: 8
Training loss: 2.0109479427337646
Validation loss: 2.04535573720932

Epoch: 6| Step: 9
Training loss: 2.7579643726348877
Validation loss: 2.0417164166768393

Epoch: 6| Step: 10
Training loss: 2.2271759510040283
Validation loss: 2.0347289045651755

Epoch: 6| Step: 11
Training loss: 1.665015697479248
Validation loss: 2.036131799221039

Epoch: 6| Step: 12
Training loss: 1.9239139556884766
Validation loss: 2.0367676814397178

Epoch: 6| Step: 13
Training loss: 1.823577642440796
Validation loss: 2.030820449193319

Epoch: 66| Step: 0
Training loss: 2.0543198585510254
Validation loss: 2.0286267002423606

Epoch: 6| Step: 1
Training loss: 2.3201870918273926
Validation loss: 2.02776829401652

Epoch: 6| Step: 2
Training loss: 2.1090946197509766
Validation loss: 2.0344372391700745

Epoch: 6| Step: 3
Training loss: 2.497676134109497
Validation loss: 2.0445779164632163

Epoch: 6| Step: 4
Training loss: 1.7395977973937988
Validation loss: 2.0466795365015664

Epoch: 6| Step: 5
Training loss: 2.402289390563965
Validation loss: 2.052732686201731

Epoch: 6| Step: 6
Training loss: 2.201422691345215
Validation loss: 2.0422425866127014

Epoch: 6| Step: 7
Training loss: 1.9649620056152344
Validation loss: 2.0404325127601624

Epoch: 6| Step: 8
Training loss: 1.9035472869873047
Validation loss: 2.034049312273661

Epoch: 6| Step: 9
Training loss: 2.8172621726989746
Validation loss: 2.0327590902646384

Epoch: 6| Step: 10
Training loss: 2.161348581314087
Validation loss: 2.0279401739438376

Epoch: 6| Step: 11
Training loss: 2.3868138790130615
Validation loss: 2.0263547698656716

Epoch: 6| Step: 12
Training loss: 2.0513064861297607
Validation loss: 2.025819698969523

Epoch: 6| Step: 13
Training loss: 2.309730052947998
Validation loss: 2.0262449979782104

Epoch: 67| Step: 0
Training loss: 2.395019769668579
Validation loss: 2.0300434827804565

Epoch: 6| Step: 1
Training loss: 1.970578670501709
Validation loss: 2.026818494002024

Epoch: 6| Step: 2
Training loss: 2.1855454444885254
Validation loss: 2.0232383807500205

Epoch: 6| Step: 3
Training loss: 2.163255214691162
Validation loss: 2.0218583146731057

Epoch: 6| Step: 4
Training loss: 3.0423734188079834
Validation loss: 2.0244370301564536

Epoch: 6| Step: 5
Training loss: 2.4063775539398193
Validation loss: 2.0307005246480307

Epoch: 6| Step: 6
Training loss: 2.0516157150268555
Validation loss: 2.0331770976384482

Epoch: 6| Step: 7
Training loss: 2.513731002807617
Validation loss: 2.0337007244428

Epoch: 6| Step: 8
Training loss: 2.2353737354278564
Validation loss: 2.0362724463144937

Epoch: 6| Step: 9
Training loss: 2.171217441558838
Validation loss: 2.0229007999102273

Epoch: 6| Step: 10
Training loss: 1.857450246810913
Validation loss: 2.02302751938502

Epoch: 6| Step: 11
Training loss: 1.7385934591293335
Validation loss: 2.0260112484296164

Epoch: 6| Step: 12
Training loss: 2.280028820037842
Validation loss: 2.019999404748281

Epoch: 6| Step: 13
Training loss: 1.6990814208984375
Validation loss: 2.022464950879415

Epoch: 68| Step: 0
Training loss: 2.213430881500244
Validation loss: 2.0258054534594216

Epoch: 6| Step: 1
Training loss: 2.0529189109802246
Validation loss: 2.025496224562327

Epoch: 6| Step: 2
Training loss: 2.041118860244751
Validation loss: 2.0242851972579956

Epoch: 6| Step: 3
Training loss: 1.9057656526565552
Validation loss: 2.027733862400055

Epoch: 6| Step: 4
Training loss: 2.376044988632202
Validation loss: 2.037130892276764

Epoch: 6| Step: 5
Training loss: 2.215597629547119
Validation loss: 2.034852226575216

Epoch: 6| Step: 6
Training loss: 1.891006588935852
Validation loss: 2.0355343222618103

Epoch: 6| Step: 7
Training loss: 2.5520310401916504
Validation loss: 2.0396949648857117

Epoch: 6| Step: 8
Training loss: 1.7956548929214478
Validation loss: 2.0373597343762717

Epoch: 6| Step: 9
Training loss: 1.988246202468872
Validation loss: 2.0303470293680825

Epoch: 6| Step: 10
Training loss: 2.744065284729004
Validation loss: 2.0219218929608664

Epoch: 6| Step: 11
Training loss: 2.3010752201080322
Validation loss: 2.0167146921157837

Epoch: 6| Step: 12
Training loss: 2.5053372383117676
Validation loss: 2.0186155438423157

Epoch: 6| Step: 13
Training loss: 1.9411776065826416
Validation loss: 2.014296233654022

Epoch: 69| Step: 0
Training loss: 2.1413984298706055
Validation loss: 2.024881442387899

Epoch: 6| Step: 1
Training loss: 2.6128005981445312
Validation loss: 2.0307274063428244

Epoch: 6| Step: 2
Training loss: 1.4699212312698364
Validation loss: 2.0410195787747702

Epoch: 6| Step: 3
Training loss: 2.351433753967285
Validation loss: 2.0431192914644876

Epoch: 6| Step: 4
Training loss: 2.2759053707122803
Validation loss: 2.049097796281179

Epoch: 6| Step: 5
Training loss: 2.4131383895874023
Validation loss: 2.0493547916412354

Epoch: 6| Step: 6
Training loss: 1.9190975427627563
Validation loss: 2.0535035928090415

Epoch: 6| Step: 7
Training loss: 2.2553694248199463
Validation loss: 2.051990350087484

Epoch: 6| Step: 8
Training loss: 2.479130983352661
Validation loss: 2.0450236399968467

Epoch: 6| Step: 9
Training loss: 2.4845151901245117
Validation loss: 2.048850437005361

Epoch: 6| Step: 10
Training loss: 2.503469467163086
Validation loss: 2.0507065653800964

Epoch: 6| Step: 11
Training loss: 2.450186252593994
Validation loss: 2.0488410194714866

Epoch: 6| Step: 12
Training loss: 1.5725955963134766
Validation loss: 2.0377907156944275

Epoch: 6| Step: 13
Training loss: 2.180471420288086
Validation loss: 2.032179117202759

Epoch: 70| Step: 0
Training loss: 2.3286828994750977
Validation loss: 2.0207849542299905

Epoch: 6| Step: 1
Training loss: 1.866506814956665
Validation loss: 2.0136327147483826

Epoch: 6| Step: 2
Training loss: 2.038015365600586
Validation loss: 2.01627782980601

Epoch: 6| Step: 3
Training loss: 2.6158666610717773
Validation loss: 2.0162304441134133

Epoch: 6| Step: 4
Training loss: 1.5511326789855957
Validation loss: 2.014349420865377

Epoch: 6| Step: 5
Training loss: 2.07905912399292
Validation loss: 2.0152864853541055

Epoch: 6| Step: 6
Training loss: 2.446606159210205
Validation loss: 2.0265909830729165

Epoch: 6| Step: 7
Training loss: 1.693915843963623
Validation loss: 2.0275923212369285

Epoch: 6| Step: 8
Training loss: 2.248684883117676
Validation loss: 2.024624288082123

Epoch: 6| Step: 9
Training loss: 2.6617119312286377
Validation loss: 2.0288551251093545

Epoch: 6| Step: 10
Training loss: 1.6627202033996582
Validation loss: 2.0422627131144204

Epoch: 6| Step: 11
Training loss: 2.4668874740600586
Validation loss: 2.0482371846834817

Epoch: 6| Step: 12
Training loss: 2.397355318069458
Validation loss: 2.0549499789873757

Epoch: 6| Step: 13
Training loss: 2.5157313346862793
Validation loss: 2.0438031355539956

Epoch: 71| Step: 0
Training loss: 2.2305445671081543
Validation loss: 2.0398770769437156

Epoch: 6| Step: 1
Training loss: 2.3494887351989746
Validation loss: 2.0311519702275596

Epoch: 6| Step: 2
Training loss: 2.5929431915283203
Validation loss: 2.0218684474627175

Epoch: 6| Step: 3
Training loss: 2.348897933959961
Validation loss: 2.0236922899881997

Epoch: 6| Step: 4
Training loss: 2.0610697269439697
Validation loss: 2.0177308917045593

Epoch: 6| Step: 5
Training loss: 2.1305813789367676
Validation loss: 2.0158636768658957

Epoch: 6| Step: 6
Training loss: 2.029615879058838
Validation loss: 2.0128051241238913

Epoch: 6| Step: 7
Training loss: 1.9080111980438232
Validation loss: 2.0159741640090942

Epoch: 6| Step: 8
Training loss: 2.343926429748535
Validation loss: 2.0206995209058127

Epoch: 6| Step: 9
Training loss: 1.6665830612182617
Validation loss: 2.0183709263801575

Epoch: 6| Step: 10
Training loss: 2.062117576599121
Validation loss: 2.017940084139506

Epoch: 6| Step: 11
Training loss: 2.341291904449463
Validation loss: 2.0177363753318787

Epoch: 6| Step: 12
Training loss: 2.3580100536346436
Validation loss: 2.01958700021108

Epoch: 6| Step: 13
Training loss: 2.258391857147217
Validation loss: 2.0147605339686074

Epoch: 72| Step: 0
Training loss: 1.8931432962417603
Validation loss: 2.0201076666514077

Epoch: 6| Step: 1
Training loss: 2.382413864135742
Validation loss: 2.0229678551355996

Epoch: 6| Step: 2
Training loss: 2.2379555702209473
Validation loss: 2.024519205093384

Epoch: 6| Step: 3
Training loss: 1.839693546295166
Validation loss: 2.033023476600647

Epoch: 6| Step: 4
Training loss: 1.8360531330108643
Validation loss: 2.041727840900421

Epoch: 6| Step: 5
Training loss: 2.651977777481079
Validation loss: 2.0436225136121116

Epoch: 6| Step: 6
Training loss: 2.2327017784118652
Validation loss: 2.0476953188578286

Epoch: 6| Step: 7
Training loss: 1.9698339700698853
Validation loss: 2.0474608143170676

Epoch: 6| Step: 8
Training loss: 1.707470178604126
Validation loss: 2.0534096558888755

Epoch: 6| Step: 9
Training loss: 2.2433128356933594
Validation loss: 2.052786568800608

Epoch: 6| Step: 10
Training loss: 1.8669970035552979
Validation loss: 2.0380464990933738

Epoch: 6| Step: 11
Training loss: 2.227586269378662
Validation loss: 2.0268152157465615

Epoch: 6| Step: 12
Training loss: 2.9323418140411377
Validation loss: 2.0175095597902932

Epoch: 6| Step: 13
Training loss: 2.4459495544433594
Validation loss: 2.012404421965281

Epoch: 73| Step: 0
Training loss: 1.9305577278137207
Validation loss: 2.013311823209127

Epoch: 6| Step: 1
Training loss: 2.1782453060150146
Validation loss: 2.0105743606885276

Epoch: 6| Step: 2
Training loss: 1.2553939819335938
Validation loss: 2.0125114719072976

Epoch: 6| Step: 3
Training loss: 2.4763636589050293
Validation loss: 2.014164169629415

Epoch: 6| Step: 4
Training loss: 1.9078481197357178
Validation loss: 2.0124180714289346

Epoch: 6| Step: 5
Training loss: 2.3527562618255615
Validation loss: 2.009675224622091

Epoch: 6| Step: 6
Training loss: 2.3184030055999756
Validation loss: 2.006459653377533

Epoch: 6| Step: 7
Training loss: 2.1033921241760254
Validation loss: 2.005811790625254

Epoch: 6| Step: 8
Training loss: 2.09621262550354
Validation loss: 2.0124544898668923

Epoch: 6| Step: 9
Training loss: 1.9657840728759766
Validation loss: 2.011293609937032

Epoch: 6| Step: 10
Training loss: 2.4971067905426025
Validation loss: 2.0156943599383035

Epoch: 6| Step: 11
Training loss: 2.438960075378418
Validation loss: 2.0106813112894693

Epoch: 6| Step: 12
Training loss: 2.2205047607421875
Validation loss: 2.008059859275818

Epoch: 6| Step: 13
Training loss: 2.7867918014526367
Validation loss: 2.0107174714406333

Epoch: 74| Step: 0
Training loss: 2.562913417816162
Validation loss: 2.0133949518203735

Epoch: 6| Step: 1
Training loss: 2.4084858894348145
Validation loss: 2.0112648407618203

Epoch: 6| Step: 2
Training loss: 2.1235599517822266
Validation loss: 2.0148016810417175

Epoch: 6| Step: 3
Training loss: 1.4542391300201416
Validation loss: 2.014910320440928

Epoch: 6| Step: 4
Training loss: 1.9275486469268799
Validation loss: 2.020402212937673

Epoch: 6| Step: 5
Training loss: 1.4896330833435059
Validation loss: 2.0230756998062134

Epoch: 6| Step: 6
Training loss: 2.900628089904785
Validation loss: 2.0387632052103677

Epoch: 6| Step: 7
Training loss: 2.2141079902648926
Validation loss: 2.0409987370173135

Epoch: 6| Step: 8
Training loss: 2.5042643547058105
Validation loss: 2.0477208693822226

Epoch: 6| Step: 9
Training loss: 2.338789701461792
Validation loss: 2.0452999472618103

Epoch: 6| Step: 10
Training loss: 2.692659378051758
Validation loss: 2.041268507639567

Epoch: 6| Step: 11
Training loss: 1.739189624786377
Validation loss: 2.0240073998769126

Epoch: 6| Step: 12
Training loss: 2.335371732711792
Validation loss: 2.0197766025861106

Epoch: 6| Step: 13
Training loss: 1.710924744606018
Validation loss: 2.0170141061147056

Epoch: 75| Step: 0
Training loss: 2.4009878635406494
Validation loss: 2.01450785001119

Epoch: 6| Step: 1
Training loss: 1.6805557012557983
Validation loss: 2.0153579910596213

Epoch: 6| Step: 2
Training loss: 2.0651161670684814
Validation loss: 2.018310228983561

Epoch: 6| Step: 3
Training loss: 2.1257309913635254
Validation loss: 2.0187084476153054

Epoch: 6| Step: 4
Training loss: 2.24826717376709
Validation loss: 2.014435609181722

Epoch: 6| Step: 5
Training loss: 2.444153308868408
Validation loss: 2.025997519493103

Epoch: 6| Step: 6
Training loss: 2.1510674953460693
Validation loss: 2.0242430369059243

Epoch: 6| Step: 7
Training loss: 2.1365914344787598
Validation loss: 2.0155239502588906

Epoch: 6| Step: 8
Training loss: 2.0119333267211914
Validation loss: 2.0249481002489724

Epoch: 6| Step: 9
Training loss: 2.3669629096984863
Validation loss: 2.031604746977488

Epoch: 6| Step: 10
Training loss: 2.1936593055725098
Validation loss: 2.0318374832471213

Epoch: 6| Step: 11
Training loss: 1.9640374183654785
Validation loss: 2.0275638699531555

Epoch: 6| Step: 12
Training loss: 2.0686492919921875
Validation loss: 2.021280507246653

Epoch: 6| Step: 13
Training loss: 2.3728294372558594
Validation loss: 2.028355836868286

Epoch: 76| Step: 0
Training loss: 1.5008680820465088
Validation loss: 2.020413100719452

Epoch: 6| Step: 1
Training loss: 1.325826644897461
Validation loss: 2.0183198650678

Epoch: 6| Step: 2
Training loss: 2.057583808898926
Validation loss: 2.0103466510772705

Epoch: 6| Step: 3
Training loss: 1.7647355794906616
Validation loss: 2.019220232963562

Epoch: 6| Step: 4
Training loss: 2.9784343242645264
Validation loss: 2.0179207722345986

Epoch: 6| Step: 5
Training loss: 2.214647054672241
Validation loss: 2.0206130544344583

Epoch: 6| Step: 6
Training loss: 2.526348114013672
Validation loss: 2.013995409011841

Epoch: 6| Step: 7
Training loss: 2.2338321208953857
Validation loss: 2.02150426308314

Epoch: 6| Step: 8
Training loss: 2.6722240447998047
Validation loss: 2.0206408500671387

Epoch: 6| Step: 9
Training loss: 1.6111621856689453
Validation loss: 2.01861306031545

Epoch: 6| Step: 10
Training loss: 2.0649688243865967
Validation loss: 2.019058366616567

Epoch: 6| Step: 11
Training loss: 1.8107917308807373
Validation loss: 2.0151882767677307

Epoch: 6| Step: 12
Training loss: 3.123070001602173
Validation loss: 2.0220044453938804

Epoch: 6| Step: 13
Training loss: 2.384247303009033
Validation loss: 2.0164486169815063

Epoch: 77| Step: 0
Training loss: 1.7734719514846802
Validation loss: 2.0132071375846863

Epoch: 6| Step: 1
Training loss: 1.956367015838623
Validation loss: 2.006463944911957

Epoch: 6| Step: 2
Training loss: 2.3560733795166016
Validation loss: 2.0103860894838967

Epoch: 6| Step: 3
Training loss: 2.192809820175171
Validation loss: 2.010489741961161

Epoch: 6| Step: 4
Training loss: 1.516390085220337
Validation loss: 1.9987576603889465

Epoch: 6| Step: 5
Training loss: 1.550611972808838
Validation loss: 2.007858693599701

Epoch: 6| Step: 6
Training loss: 2.587207317352295
Validation loss: 2.0099650025367737

Epoch: 6| Step: 7
Training loss: 2.1280877590179443
Validation loss: 2.0036709904670715

Epoch: 6| Step: 8
Training loss: 2.45699405670166
Validation loss: 2.0166531006495156

Epoch: 6| Step: 9
Training loss: 2.555449962615967
Validation loss: 2.0169713695844016

Epoch: 6| Step: 10
Training loss: 2.258505344390869
Validation loss: 2.0254743496576944

Epoch: 6| Step: 11
Training loss: 1.8117879629135132
Validation loss: 2.0206865072250366

Epoch: 6| Step: 12
Training loss: 2.831287384033203
Validation loss: 2.0137646396954856

Epoch: 6| Step: 13
Training loss: 2.231755495071411
Validation loss: 2.0140790541966758

Epoch: 78| Step: 0
Training loss: 1.7574728727340698
Validation loss: 2.007274309794108

Epoch: 6| Step: 1
Training loss: 2.09322452545166
Validation loss: 2.012049655119578

Epoch: 6| Step: 2
Training loss: 2.296335220336914
Validation loss: 2.0155356725056968

Epoch: 6| Step: 3
Training loss: 2.270505666732788
Validation loss: 2.0173938473065696

Epoch: 6| Step: 4
Training loss: 1.6941187381744385
Validation loss: 2.0121024449666343

Epoch: 6| Step: 5
Training loss: 1.9147825241088867
Validation loss: 2.0165295004844666

Epoch: 6| Step: 6
Training loss: 2.627269983291626
Validation loss: 2.0179503361384072

Epoch: 6| Step: 7
Training loss: 1.9695072174072266
Validation loss: 2.017990012963613

Epoch: 6| Step: 8
Training loss: 2.1921873092651367
Validation loss: 2.014778991540273

Epoch: 6| Step: 9
Training loss: 1.658044695854187
Validation loss: 2.0254926681518555

Epoch: 6| Step: 10
Training loss: 2.6136112213134766
Validation loss: 2.032701234022776

Epoch: 6| Step: 11
Training loss: 2.0457396507263184
Validation loss: 2.0284106731414795

Epoch: 6| Step: 12
Training loss: 2.3336868286132812
Validation loss: 2.022480607032776

Epoch: 6| Step: 13
Training loss: 2.771310806274414
Validation loss: 2.0195629596710205

Epoch: 79| Step: 0
Training loss: 1.8659993410110474
Validation loss: 2.0111493269602456

Epoch: 6| Step: 1
Training loss: 2.115196704864502
Validation loss: 2.0145636796951294

Epoch: 6| Step: 2
Training loss: 2.125782012939453
Validation loss: 2.013595223426819

Epoch: 6| Step: 3
Training loss: 2.4147348403930664
Validation loss: 2.0155546267827353

Epoch: 6| Step: 4
Training loss: 2.2579894065856934
Validation loss: 2.028044025103251

Epoch: 6| Step: 5
Training loss: 2.492117404937744
Validation loss: 2.0274033149083457

Epoch: 6| Step: 6
Training loss: 2.225627899169922
Validation loss: 2.0332088669141135

Epoch: 6| Step: 7
Training loss: 2.468857765197754
Validation loss: 2.0348207553227744

Epoch: 6| Step: 8
Training loss: 2.5163331031799316
Validation loss: 2.033233900864919

Epoch: 6| Step: 9
Training loss: 1.8615772724151611
Validation loss: 2.0394397179285684

Epoch: 6| Step: 10
Training loss: 2.3241448402404785
Validation loss: 2.0369780460993447

Epoch: 6| Step: 11
Training loss: 2.1893863677978516
Validation loss: 2.0382081866264343

Epoch: 6| Step: 12
Training loss: 1.9963467121124268
Validation loss: 2.031593362490336

Epoch: 6| Step: 13
Training loss: 1.8287062644958496
Validation loss: 2.0317310293515525

Epoch: 80| Step: 0
Training loss: 1.618694543838501
Validation loss: 2.03269229332606

Epoch: 6| Step: 1
Training loss: 2.007906675338745
Validation loss: 2.0158333778381348

Epoch: 6| Step: 2
Training loss: 2.604240894317627
Validation loss: 2.0179183880488076

Epoch: 6| Step: 3
Training loss: 2.21459698677063
Validation loss: 2.0095680952072144

Epoch: 6| Step: 4
Training loss: 2.025090217590332
Validation loss: 2.005852142969767

Epoch: 6| Step: 5
Training loss: 2.0887815952301025
Validation loss: 2.007875382900238

Epoch: 6| Step: 6
Training loss: 1.7694231271743774
Validation loss: 2.005336364110311

Epoch: 6| Step: 7
Training loss: 2.0150814056396484
Validation loss: 2.009315272172292

Epoch: 6| Step: 8
Training loss: 2.4311046600341797
Validation loss: 2.0130691528320312

Epoch: 6| Step: 9
Training loss: 2.2128074169158936
Validation loss: 2.0183067123095193

Epoch: 6| Step: 10
Training loss: 2.3870749473571777
Validation loss: 2.032026727994283

Epoch: 6| Step: 11
Training loss: 2.836423873901367
Validation loss: 2.045020500818888

Epoch: 6| Step: 12
Training loss: 2.45680570602417
Validation loss: 2.0647401412328086

Epoch: 6| Step: 13
Training loss: 2.1903796195983887
Validation loss: 2.0659362276395163

Epoch: 81| Step: 0
Training loss: 1.7786800861358643
Validation loss: 2.0558094580968223

Epoch: 6| Step: 1
Training loss: 1.9921069145202637
Validation loss: 2.0592031876246133

Epoch: 6| Step: 2
Training loss: 1.954978585243225
Validation loss: 2.0362937847773233

Epoch: 6| Step: 3
Training loss: 1.8271466493606567
Validation loss: 2.0276646812756858

Epoch: 6| Step: 4
Training loss: 1.9005486965179443
Validation loss: 2.0184653401374817

Epoch: 6| Step: 5
Training loss: 2.2961435317993164
Validation loss: 2.016523798306783

Epoch: 6| Step: 6
Training loss: 2.048525810241699
Validation loss: 2.0095161596934

Epoch: 6| Step: 7
Training loss: 2.7561869621276855
Validation loss: 2.0138429204622903

Epoch: 6| Step: 8
Training loss: 1.8481600284576416
Validation loss: 2.0070225993792215

Epoch: 6| Step: 9
Training loss: 3.1880311965942383
Validation loss: 2.005765716234843

Epoch: 6| Step: 10
Training loss: 2.349496364593506
Validation loss: 2.0093292792638144

Epoch: 6| Step: 11
Training loss: 2.3358211517333984
Validation loss: 2.0133650501569114

Epoch: 6| Step: 12
Training loss: 2.1681628227233887
Validation loss: 2.003993352254232

Epoch: 6| Step: 13
Training loss: 1.8780794143676758
Validation loss: 2.015409787495931

Epoch: 82| Step: 0
Training loss: 2.670276641845703
Validation loss: 2.021562099456787

Epoch: 6| Step: 1
Training loss: 2.396707534790039
Validation loss: 2.01899782816569

Epoch: 6| Step: 2
Training loss: 1.904870629310608
Validation loss: 2.017151335875193

Epoch: 6| Step: 3
Training loss: 2.364722728729248
Validation loss: 2.015575428803762

Epoch: 6| Step: 4
Training loss: 2.3982157707214355
Validation loss: 2.013975282510122

Epoch: 6| Step: 5
Training loss: 1.9128797054290771
Validation loss: 2.024169603983561

Epoch: 6| Step: 6
Training loss: 2.440110445022583
Validation loss: 2.026047945022583

Epoch: 6| Step: 7
Training loss: 2.4344534873962402
Validation loss: 2.0232562820116677

Epoch: 6| Step: 8
Training loss: 1.7276582717895508
Validation loss: 2.0387235283851624

Epoch: 6| Step: 9
Training loss: 2.4388556480407715
Validation loss: 2.0375781059265137

Epoch: 6| Step: 10
Training loss: 1.5622272491455078
Validation loss: 2.0412287513415017

Epoch: 6| Step: 11
Training loss: 1.805859088897705
Validation loss: 2.043992737929026

Epoch: 6| Step: 12
Training loss: 2.0275607109069824
Validation loss: 2.0307753483454385

Epoch: 6| Step: 13
Training loss: 2.0092554092407227
Validation loss: 2.0300334294637046

Epoch: 83| Step: 0
Training loss: 1.985072135925293
Validation loss: 2.0177751183509827

Epoch: 6| Step: 1
Training loss: 2.133193016052246
Validation loss: 2.009194254875183

Epoch: 6| Step: 2
Training loss: 2.468675136566162
Validation loss: 2.0090468327204385

Epoch: 6| Step: 3
Training loss: 2.285687208175659
Validation loss: 2.0119615594546

Epoch: 6| Step: 4
Training loss: 2.338477611541748
Validation loss: 2.017193833986918

Epoch: 6| Step: 5
Training loss: 2.034270763397217
Validation loss: 2.0167304078737893

Epoch: 6| Step: 6
Training loss: 1.9973816871643066
Validation loss: 2.020543932914734

Epoch: 6| Step: 7
Training loss: 1.8140724897384644
Validation loss: 2.0230680306752524

Epoch: 6| Step: 8
Training loss: 1.7989764213562012
Validation loss: 2.0298678278923035

Epoch: 6| Step: 9
Training loss: 2.30324649810791
Validation loss: 2.0353163282076516

Epoch: 6| Step: 10
Training loss: 2.72747802734375
Validation loss: 2.0284613768259683

Epoch: 6| Step: 11
Training loss: 1.844030499458313
Validation loss: 2.0314642190933228

Epoch: 6| Step: 12
Training loss: 2.8276569843292236
Validation loss: 2.0236979325612388

Epoch: 6| Step: 13
Training loss: 1.9415501356124878
Validation loss: 2.0116161902745566

Epoch: 84| Step: 0
Training loss: 2.282471179962158
Validation loss: 2.009455144405365

Epoch: 6| Step: 1
Training loss: 2.319084882736206
Validation loss: 2.0033607880274453

Epoch: 6| Step: 2
Training loss: 2.281395196914673
Validation loss: 2.011806825796763

Epoch: 6| Step: 3
Training loss: 2.483365774154663
Validation loss: 2.0102463364601135

Epoch: 6| Step: 4
Training loss: 1.8972433805465698
Validation loss: 2.0122166872024536

Epoch: 6| Step: 5
Training loss: 1.6103448867797852
Validation loss: 2.0166430473327637

Epoch: 6| Step: 6
Training loss: 1.3926631212234497
Validation loss: 2.0282418529192605

Epoch: 6| Step: 7
Training loss: 2.782345771789551
Validation loss: 2.0451069672902427

Epoch: 6| Step: 8
Training loss: 2.1172990798950195
Validation loss: 2.059517761071523

Epoch: 6| Step: 9
Training loss: 2.7730166912078857
Validation loss: 2.068327228228251

Epoch: 6| Step: 10
Training loss: 2.208792209625244
Validation loss: 2.0665440956751504

Epoch: 6| Step: 11
Training loss: 1.9548697471618652
Validation loss: 2.0642089446385703

Epoch: 6| Step: 12
Training loss: 1.9028663635253906
Validation loss: 2.066757341225942

Epoch: 6| Step: 13
Training loss: 2.213639259338379
Validation loss: 2.0494383772214255

Epoch: 85| Step: 0
Training loss: 1.708603024482727
Validation loss: 2.0568421483039856

Epoch: 6| Step: 1
Training loss: 2.6134369373321533
Validation loss: 2.0329288840293884

Epoch: 6| Step: 2
Training loss: 1.9112266302108765
Validation loss: 2.0227897564570108

Epoch: 6| Step: 3
Training loss: 2.9520046710968018
Validation loss: 2.013269821802775

Epoch: 6| Step: 4
Training loss: 2.084315776824951
Validation loss: 2.010354518890381

Epoch: 6| Step: 5
Training loss: 2.1950182914733887
Validation loss: 2.009083608786265

Epoch: 6| Step: 6
Training loss: 2.10776686668396
Validation loss: 2.0155904491742453

Epoch: 6| Step: 7
Training loss: 2.1832337379455566
Validation loss: 2.010502517223358

Epoch: 6| Step: 8
Training loss: 1.9895703792572021
Validation loss: 2.013963441054026

Epoch: 6| Step: 9
Training loss: 1.98919677734375
Validation loss: 2.0141715010007224

Epoch: 6| Step: 10
Training loss: 2.0220913887023926
Validation loss: 2.011060039202372

Epoch: 6| Step: 11
Training loss: 2.002821922302246
Validation loss: 2.0089402993520102

Epoch: 6| Step: 12
Training loss: 1.9792213439941406
Validation loss: 2.0093841751416526

Epoch: 6| Step: 13
Training loss: 2.3148608207702637
Validation loss: 2.0113014777501426

Epoch: 86| Step: 0
Training loss: 2.2738752365112305
Validation loss: 2.0157293478647866

Epoch: 6| Step: 1
Training loss: 2.41860294342041
Validation loss: 2.0086759328842163

Epoch: 6| Step: 2
Training loss: 2.25748872756958
Validation loss: 2.0079869429270425

Epoch: 6| Step: 3
Training loss: 2.2337934970855713
Validation loss: 2.021632273991903

Epoch: 6| Step: 4
Training loss: 2.2813849449157715
Validation loss: 2.0253852804501853

Epoch: 6| Step: 5
Training loss: 1.6583317518234253
Validation loss: 2.0344881812731423

Epoch: 6| Step: 6
Training loss: 2.0248584747314453
Validation loss: 2.036686281363169

Epoch: 6| Step: 7
Training loss: 2.815096378326416
Validation loss: 2.038014849026998

Epoch: 6| Step: 8
Training loss: 1.7939927577972412
Validation loss: 2.040571471055349

Epoch: 6| Step: 9
Training loss: 2.077519416809082
Validation loss: 2.0397324363390603

Epoch: 6| Step: 10
Training loss: 2.6161441802978516
Validation loss: 2.03004123767217

Epoch: 6| Step: 11
Training loss: 1.8298046588897705
Validation loss: 2.0267819364865622

Epoch: 6| Step: 12
Training loss: 1.6786643266677856
Validation loss: 2.020348072052002

Epoch: 6| Step: 13
Training loss: 1.9871753454208374
Validation loss: 2.022166589895884

Epoch: 87| Step: 0
Training loss: 2.7241907119750977
Validation loss: 2.011175354321798

Epoch: 6| Step: 1
Training loss: 2.1607778072357178
Validation loss: 2.021765391031901

Epoch: 6| Step: 2
Training loss: 1.9122278690338135
Validation loss: 2.00605309009552

Epoch: 6| Step: 3
Training loss: 1.7228318452835083
Validation loss: 2.0047497351964316

Epoch: 6| Step: 4
Training loss: 2.0086450576782227
Validation loss: 2.0077872077624

Epoch: 6| Step: 5
Training loss: 1.8762482404708862
Validation loss: 2.0149581829706826

Epoch: 6| Step: 6
Training loss: 2.411623001098633
Validation loss: 2.0121294260025024

Epoch: 6| Step: 7
Training loss: 2.656684160232544
Validation loss: 2.0058642824490867

Epoch: 6| Step: 8
Training loss: 1.8911051750183105
Validation loss: 2.0128862063090005

Epoch: 6| Step: 9
Training loss: 2.5414791107177734
Validation loss: 2.0143545667330423

Epoch: 6| Step: 10
Training loss: 1.8293060064315796
Validation loss: 2.0215853651364646

Epoch: 6| Step: 11
Training loss: 2.2054922580718994
Validation loss: 2.020102103551229

Epoch: 6| Step: 12
Training loss: 1.7621078491210938
Validation loss: 2.0140737295150757

Epoch: 6| Step: 13
Training loss: 2.106945753097534
Validation loss: 2.015375336011251

Epoch: 88| Step: 0
Training loss: 1.8921843767166138
Validation loss: 2.0198419094085693

Epoch: 6| Step: 1
Training loss: 2.0417871475219727
Validation loss: 2.0248865286509194

Epoch: 6| Step: 2
Training loss: 1.9145785570144653
Validation loss: 2.033180892467499

Epoch: 6| Step: 3
Training loss: 2.407525062561035
Validation loss: 2.0345763564109802

Epoch: 6| Step: 4
Training loss: 2.048189640045166
Validation loss: 2.0269449949264526

Epoch: 6| Step: 5
Training loss: 1.528887391090393
Validation loss: 2.0106781323750815

Epoch: 6| Step: 6
Training loss: 2.6258726119995117
Validation loss: 2.018182953198751

Epoch: 6| Step: 7
Training loss: 2.2238316535949707
Validation loss: 2.0143848260243735

Epoch: 6| Step: 8
Training loss: 2.177349090576172
Validation loss: 2.006752669811249

Epoch: 6| Step: 9
Training loss: 2.2656960487365723
Validation loss: 2.01344233751297

Epoch: 6| Step: 10
Training loss: 2.3184733390808105
Validation loss: 2.0082757274309793

Epoch: 6| Step: 11
Training loss: 2.660956859588623
Validation loss: 2.007437070210775

Epoch: 6| Step: 12
Training loss: 1.8735774755477905
Validation loss: 2.011014183362325

Epoch: 6| Step: 13
Training loss: 1.9362611770629883
Validation loss: 2.0087067087491355

Epoch: 89| Step: 0
Training loss: 1.9694464206695557
Validation loss: 2.0074985027313232

Epoch: 6| Step: 1
Training loss: 1.4555258750915527
Validation loss: 2.0145113865534463

Epoch: 6| Step: 2
Training loss: 2.1697850227355957
Validation loss: 2.0064636866251626

Epoch: 6| Step: 3
Training loss: 2.0440444946289062
Validation loss: 2.009869118531545

Epoch: 6| Step: 4
Training loss: 1.7717742919921875
Validation loss: 2.017997940381368

Epoch: 6| Step: 5
Training loss: 2.222257137298584
Validation loss: 2.025323530038198

Epoch: 6| Step: 6
Training loss: 2.3413450717926025
Validation loss: 2.0357877810796103

Epoch: 6| Step: 7
Training loss: 2.2857413291931152
Validation loss: 2.0414846738179526

Epoch: 6| Step: 8
Training loss: 2.8669471740722656
Validation loss: 2.083751698335012

Epoch: 6| Step: 9
Training loss: 2.2245020866394043
Validation loss: 2.083703875541687

Epoch: 6| Step: 10
Training loss: 2.706134080886841
Validation loss: 2.086290796597799

Epoch: 6| Step: 11
Training loss: 2.182095527648926
Validation loss: 2.0749918619791665

Epoch: 6| Step: 12
Training loss: 2.0672035217285156
Validation loss: 2.0491378903388977

Epoch: 6| Step: 13
Training loss: 2.0197319984436035
Validation loss: 2.0361934502919516

Epoch: 90| Step: 0
Training loss: 1.6508700847625732
Validation loss: 2.0233182112375894

Epoch: 6| Step: 1
Training loss: 2.2244176864624023
Validation loss: 2.012982805569967

Epoch: 6| Step: 2
Training loss: 2.077791213989258
Validation loss: 2.0101975401242576

Epoch: 6| Step: 3
Training loss: 2.186000347137451
Validation loss: 2.0047274827957153

Epoch: 6| Step: 4
Training loss: 2.7609260082244873
Validation loss: 2.0176072120666504

Epoch: 6| Step: 5
Training loss: 2.0810141563415527
Validation loss: 2.0210121472676597

Epoch: 6| Step: 6
Training loss: 1.8576793670654297
Validation loss: 2.0231944719950357

Epoch: 6| Step: 7
Training loss: 2.53338623046875
Validation loss: 2.0300932923952737

Epoch: 6| Step: 8
Training loss: 2.7835516929626465
Validation loss: 2.0283783674240112

Epoch: 6| Step: 9
Training loss: 1.9399638175964355
Validation loss: 2.029248853524526

Epoch: 6| Step: 10
Training loss: 2.0316154956817627
Validation loss: 2.031052807966868

Epoch: 6| Step: 11
Training loss: 2.50582218170166
Validation loss: 2.0283207098642984

Epoch: 6| Step: 12
Training loss: 2.1004064083099365
Validation loss: 2.0270107984542847

Epoch: 6| Step: 13
Training loss: 1.467012882232666
Validation loss: 2.023838539918264

Epoch: 91| Step: 0
Training loss: 2.538214683532715
Validation loss: 2.0262057781219482

Epoch: 6| Step: 1
Training loss: 2.590190887451172
Validation loss: 2.0222441951433816

Epoch: 6| Step: 2
Training loss: 2.239462375640869
Validation loss: 2.0225418408711753

Epoch: 6| Step: 3
Training loss: 2.0338454246520996
Validation loss: 2.0195083220799765

Epoch: 6| Step: 4
Training loss: 1.8148012161254883
Validation loss: 2.022231717904409

Epoch: 6| Step: 5
Training loss: 2.177121639251709
Validation loss: 2.0229329069455466

Epoch: 6| Step: 6
Training loss: 1.9860787391662598
Validation loss: 2.026696721712748

Epoch: 6| Step: 7
Training loss: 2.301969528198242
Validation loss: 2.0289878646532693

Epoch: 6| Step: 8
Training loss: 1.948869228363037
Validation loss: 2.034331202507019

Epoch: 6| Step: 9
Training loss: 2.5191116333007812
Validation loss: 2.0255484779675803

Epoch: 6| Step: 10
Training loss: 2.072993278503418
Validation loss: 2.0284900069236755

Epoch: 6| Step: 11
Training loss: 1.1725962162017822
Validation loss: 2.033007502555847

Epoch: 6| Step: 12
Training loss: 2.3859939575195312
Validation loss: 2.034475008646647

Epoch: 6| Step: 13
Training loss: 1.9703394174575806
Validation loss: 2.0494865576426187

Epoch: 92| Step: 0
Training loss: 2.0555038452148438
Validation loss: 2.03985458612442

Epoch: 6| Step: 1
Training loss: 2.4568099975585938
Validation loss: 2.0451631347338357

Epoch: 6| Step: 2
Training loss: 2.02207612991333
Validation loss: 2.0295162995656333

Epoch: 6| Step: 3
Training loss: 2.0353689193725586
Validation loss: 2.0345846017201743

Epoch: 6| Step: 4
Training loss: 1.5735846757888794
Validation loss: 2.0335059563318887

Epoch: 6| Step: 5
Training loss: 2.1434240341186523
Validation loss: 2.0263418555259705

Epoch: 6| Step: 6
Training loss: 1.506892204284668
Validation loss: 2.030149777730306

Epoch: 6| Step: 7
Training loss: 2.704932689666748
Validation loss: 2.0320955912272134

Epoch: 6| Step: 8
Training loss: 2.5445525646209717
Validation loss: 2.019582470258077

Epoch: 6| Step: 9
Training loss: 2.3785440921783447
Validation loss: 2.0266342957814536

Epoch: 6| Step: 10
Training loss: 1.8209080696105957
Validation loss: 2.0210309823354087

Epoch: 6| Step: 11
Training loss: 1.9775481224060059
Validation loss: 2.021275281906128

Epoch: 6| Step: 12
Training loss: 2.2669851779937744
Validation loss: 2.022534966468811

Epoch: 6| Step: 13
Training loss: 2.380506992340088
Validation loss: 2.02161035935084

Epoch: 93| Step: 0
Training loss: 2.0780885219573975
Validation loss: 2.022679348786672

Epoch: 6| Step: 1
Training loss: 2.1987411975860596
Validation loss: 2.0268558263778687

Epoch: 6| Step: 2
Training loss: 1.9219346046447754
Validation loss: 2.017376800378164

Epoch: 6| Step: 3
Training loss: 2.24090576171875
Validation loss: 2.033404231071472

Epoch: 6| Step: 4
Training loss: 2.4485111236572266
Validation loss: 2.0173624753952026

Epoch: 6| Step: 5
Training loss: 2.023768186569214
Validation loss: 2.0277730027834573

Epoch: 6| Step: 6
Training loss: 1.6660611629486084
Validation loss: 2.031861941019694

Epoch: 6| Step: 7
Training loss: 2.556097984313965
Validation loss: 2.033045788606008

Epoch: 6| Step: 8
Training loss: 2.097301483154297
Validation loss: 2.0309791366259256

Epoch: 6| Step: 9
Training loss: 1.7839504480361938
Validation loss: 2.030100146929423

Epoch: 6| Step: 10
Training loss: 1.9783954620361328
Validation loss: 2.0344775716463723

Epoch: 6| Step: 11
Training loss: 2.91890549659729
Validation loss: 2.026795208454132

Epoch: 6| Step: 12
Training loss: 1.9126458168029785
Validation loss: 2.0309400161107383

Epoch: 6| Step: 13
Training loss: 1.9902191162109375
Validation loss: 2.0205797950426736

Epoch: 94| Step: 0
Training loss: 2.240304708480835
Validation loss: 2.014202336470286

Epoch: 6| Step: 1
Training loss: 2.179033041000366
Validation loss: 2.006865660349528

Epoch: 6| Step: 2
Training loss: 1.920917272567749
Validation loss: 2.0084861119588218

Epoch: 6| Step: 3
Training loss: 2.3326783180236816
Validation loss: 2.007942815621694

Epoch: 6| Step: 4
Training loss: 1.9399356842041016
Validation loss: 2.0072096387545266

Epoch: 6| Step: 5
Training loss: 2.1935906410217285
Validation loss: 2.0060026248296103

Epoch: 6| Step: 6
Training loss: 1.7654684782028198
Validation loss: 2.0027907490730286

Epoch: 6| Step: 7
Training loss: 2.264237880706787
Validation loss: 1.9994962811470032

Epoch: 6| Step: 8
Training loss: 2.282496452331543
Validation loss: 1.9935743411382039

Epoch: 6| Step: 9
Training loss: 2.2336554527282715
Validation loss: 1.9940455357233684

Epoch: 6| Step: 10
Training loss: 1.9856436252593994
Validation loss: 1.9987642367680867

Epoch: 6| Step: 11
Training loss: 2.5801141262054443
Validation loss: 2.012918770313263

Epoch: 6| Step: 12
Training loss: 2.3494973182678223
Validation loss: 2.0164934198061624

Epoch: 6| Step: 13
Training loss: 1.9539439678192139
Validation loss: 2.0209638675053916

Epoch: 95| Step: 0
Training loss: 2.6310930252075195
Validation loss: 2.0230435132980347

Epoch: 6| Step: 1
Training loss: 2.1909050941467285
Validation loss: 2.0245182514190674

Epoch: 6| Step: 2
Training loss: 1.935500144958496
Validation loss: 2.020458400249481

Epoch: 6| Step: 3
Training loss: 1.6851069927215576
Validation loss: 2.022220234076182

Epoch: 6| Step: 4
Training loss: 2.3518614768981934
Validation loss: 2.026893655459086

Epoch: 6| Step: 5
Training loss: 1.7570534944534302
Validation loss: 2.011566678682963

Epoch: 6| Step: 6
Training loss: 2.224982261657715
Validation loss: 2.0127615928649902

Epoch: 6| Step: 7
Training loss: 1.4990891218185425
Validation loss: 2.022743582725525

Epoch: 6| Step: 8
Training loss: 2.4591031074523926
Validation loss: 2.014201005299886

Epoch: 6| Step: 9
Training loss: 1.931950330734253
Validation loss: 2.013124167919159

Epoch: 6| Step: 10
Training loss: 2.770381212234497
Validation loss: 2.0125948786735535

Epoch: 6| Step: 11
Training loss: 1.9444668292999268
Validation loss: 2.0044198830922446

Epoch: 6| Step: 12
Training loss: 2.3489432334899902
Validation loss: 2.004942019780477

Epoch: 6| Step: 13
Training loss: 2.238772392272949
Validation loss: 2.009049952030182

Epoch: 96| Step: 0
Training loss: 1.9364486932754517
Validation loss: 2.012198249499003

Epoch: 6| Step: 1
Training loss: 1.6797012090682983
Validation loss: 2.0046573479970298

Epoch: 6| Step: 2
Training loss: 2.1178853511810303
Validation loss: 2.008149246374766

Epoch: 6| Step: 3
Training loss: 1.7873480319976807
Validation loss: 2.00494654973348

Epoch: 6| Step: 4
Training loss: 2.6405413150787354
Validation loss: 2.0016446113586426

Epoch: 6| Step: 5
Training loss: 2.3489809036254883
Validation loss: 2.0072192351023355

Epoch: 6| Step: 6
Training loss: 1.7321230173110962
Validation loss: 2.010272045930227

Epoch: 6| Step: 7
Training loss: 2.086331605911255
Validation loss: 2.0146600206693015

Epoch: 6| Step: 8
Training loss: 1.362818956375122
Validation loss: 2.0238550702730813

Epoch: 6| Step: 9
Training loss: 2.891317844390869
Validation loss: 2.030285596847534

Epoch: 6| Step: 10
Training loss: 2.226452589035034
Validation loss: 2.038366218407949

Epoch: 6| Step: 11
Training loss: 2.815913200378418
Validation loss: 2.0521198312441506

Epoch: 6| Step: 12
Training loss: 1.854884147644043
Validation loss: 2.063787817955017

Epoch: 6| Step: 13
Training loss: 2.47367525100708
Validation loss: 2.055976629257202

Epoch: 97| Step: 0
Training loss: 2.210944175720215
Validation loss: 2.0496458411216736

Epoch: 6| Step: 1
Training loss: 2.5477170944213867
Validation loss: 2.0445303320884705

Epoch: 6| Step: 2
Training loss: 2.2327680587768555
Validation loss: 2.0403310656547546

Epoch: 6| Step: 3
Training loss: 2.385202407836914
Validation loss: 2.0453606049219766

Epoch: 6| Step: 4
Training loss: 1.7458068132400513
Validation loss: 2.038201908270518

Epoch: 6| Step: 5
Training loss: 2.2572739124298096
Validation loss: 2.0355544686317444

Epoch: 6| Step: 6
Training loss: 2.31036376953125
Validation loss: 2.0381849805514016

Epoch: 6| Step: 7
Training loss: 1.958024024963379
Validation loss: 2.0255428353945413

Epoch: 6| Step: 8
Training loss: 1.742583155632019
Validation loss: 2.0225333174069724

Epoch: 6| Step: 9
Training loss: 2.214139461517334
Validation loss: 2.0094668865203857

Epoch: 6| Step: 10
Training loss: 1.3329806327819824
Validation loss: 2.0098618865013123

Epoch: 6| Step: 11
Training loss: 2.666717052459717
Validation loss: 2.014955480893453

Epoch: 6| Step: 12
Training loss: 1.9907788038253784
Validation loss: 2.015143652757009

Epoch: 6| Step: 13
Training loss: 2.0440468788146973
Validation loss: 2.012207349141439

Epoch: 98| Step: 0
Training loss: 2.60868763923645
Validation loss: 2.022466798623403

Epoch: 6| Step: 1
Training loss: 1.8360146284103394
Validation loss: 2.0099197030067444

Epoch: 6| Step: 2
Training loss: 1.7595219612121582
Validation loss: 2.0171393752098083

Epoch: 6| Step: 3
Training loss: 1.8575620651245117
Validation loss: 2.012439747651418

Epoch: 6| Step: 4
Training loss: 1.8169981241226196
Validation loss: 2.0134635965029397

Epoch: 6| Step: 5
Training loss: 1.9798473119735718
Validation loss: 2.006597359975179

Epoch: 6| Step: 6
Training loss: 2.654797077178955
Validation loss: 2.023078362147013

Epoch: 6| Step: 7
Training loss: 1.3567298650741577
Validation loss: 2.0113470554351807

Epoch: 6| Step: 8
Training loss: 2.1232705116271973
Validation loss: 2.015892823537191

Epoch: 6| Step: 9
Training loss: 2.2120676040649414
Validation loss: 2.0118894378344216

Epoch: 6| Step: 10
Training loss: 2.309166669845581
Validation loss: 2.022319972515106

Epoch: 6| Step: 11
Training loss: 2.0612845420837402
Validation loss: 2.017848630746206

Epoch: 6| Step: 12
Training loss: 2.2098097801208496
Validation loss: 2.011780798435211

Epoch: 6| Step: 13
Training loss: 2.7586476802825928
Validation loss: 2.025886376698812

Epoch: 99| Step: 0
Training loss: 1.5314772129058838
Validation loss: 2.0434642235438027

Epoch: 6| Step: 1
Training loss: 1.8602280616760254
Validation loss: 2.0405649741490683

Epoch: 6| Step: 2
Training loss: 1.8691414594650269
Validation loss: 2.0376501083374023

Epoch: 6| Step: 3
Training loss: 2.010939598083496
Validation loss: 2.0364530881245932

Epoch: 6| Step: 4
Training loss: 2.17075252532959
Validation loss: 2.0374120275179544

Epoch: 6| Step: 5
Training loss: 2.2273688316345215
Validation loss: 2.0331915616989136

Epoch: 6| Step: 6
Training loss: 2.406785726547241
Validation loss: 2.0285766323407493

Epoch: 6| Step: 7
Training loss: 2.09710693359375
Validation loss: 2.0143410762151084

Epoch: 6| Step: 8
Training loss: 2.3754639625549316
Validation loss: 1.998550792535146

Epoch: 6| Step: 9
Training loss: 2.1869919300079346
Validation loss: 1.9959981044133503

Epoch: 6| Step: 10
Training loss: 1.9606833457946777
Validation loss: 2.0024858514467874

Epoch: 6| Step: 11
Training loss: 2.8941421508789062
Validation loss: 2.006167987982432

Epoch: 6| Step: 12
Training loss: 1.9140126705169678
Validation loss: 2.003929396470388

Epoch: 6| Step: 13
Training loss: 1.9964070320129395
Validation loss: 2.0120758414268494

Epoch: 100| Step: 0
Training loss: 1.9966298341751099
Validation loss: 2.025162617365519

Epoch: 6| Step: 1
Training loss: 2.387683868408203
Validation loss: 2.028714179992676

Epoch: 6| Step: 2
Training loss: 2.3313846588134766
Validation loss: 2.031671722730001

Epoch: 6| Step: 3
Training loss: 2.132624387741089
Validation loss: 2.028586467107137

Epoch: 6| Step: 4
Training loss: 1.86474609375
Validation loss: 2.02620792388916

Epoch: 6| Step: 5
Training loss: 2.1023271083831787
Validation loss: 2.035856525103251

Epoch: 6| Step: 6
Training loss: 2.2347164154052734
Validation loss: 2.0303627451260886

Epoch: 6| Step: 7
Training loss: 1.7603933811187744
Validation loss: 2.029537558555603

Epoch: 6| Step: 8
Training loss: 1.7314178943634033
Validation loss: 2.0312867561976113

Epoch: 6| Step: 9
Training loss: 2.9195973873138428
Validation loss: 2.028314789136251

Epoch: 6| Step: 10
Training loss: 2.4432625770568848
Validation loss: 2.0268007119496665

Epoch: 6| Step: 11
Training loss: 2.6143126487731934
Validation loss: 2.0223830739657083

Epoch: 6| Step: 12
Training loss: 1.8715991973876953
Validation loss: 2.023267169793447

Epoch: 6| Step: 13
Training loss: 1.7094539403915405
Validation loss: 2.017579813798269

Epoch: 101| Step: 0
Training loss: 2.0928478240966797
Validation loss: 2.0202359755833945

Epoch: 6| Step: 1
Training loss: 1.7951492071151733
Validation loss: 2.0208946665128074

Epoch: 6| Step: 2
Training loss: 2.2541098594665527
Validation loss: 2.0220983227094016

Epoch: 6| Step: 3
Training loss: 2.5214152336120605
Validation loss: 2.0240533153216043

Epoch: 6| Step: 4
Training loss: 2.2895312309265137
Validation loss: 2.0313095450401306

Epoch: 6| Step: 5
Training loss: 2.0881543159484863
Validation loss: 2.035088221232096

Epoch: 6| Step: 6
Training loss: 2.5758285522460938
Validation loss: 2.038709064324697

Epoch: 6| Step: 7
Training loss: 2.302304267883301
Validation loss: 2.0321656465530396

Epoch: 6| Step: 8
Training loss: 1.7436034679412842
Validation loss: 2.0260088046391806

Epoch: 6| Step: 9
Training loss: 1.5508668422698975
Validation loss: 2.0088374614715576

Epoch: 6| Step: 10
Training loss: 2.832716464996338
Validation loss: 2.0152273972829184

Epoch: 6| Step: 11
Training loss: 2.0197722911834717
Validation loss: 2.017106354236603

Epoch: 6| Step: 12
Training loss: 1.8098437786102295
Validation loss: 2.025421659151713

Epoch: 6| Step: 13
Training loss: 1.9034730195999146
Validation loss: 2.02506415049235

Epoch: 102| Step: 0
Training loss: 2.1747279167175293
Validation loss: 2.0216206510861716

Epoch: 6| Step: 1
Training loss: 1.6690078973770142
Validation loss: 2.019298036893209

Epoch: 6| Step: 2
Training loss: 2.1464622020721436
Validation loss: 2.021562079588572

Epoch: 6| Step: 3
Training loss: 1.8285433053970337
Validation loss: 2.0223859945933023

Epoch: 6| Step: 4
Training loss: 1.768583059310913
Validation loss: 2.0296941995620728

Epoch: 6| Step: 5
Training loss: 2.4349722862243652
Validation loss: 2.0372138222058616

Epoch: 6| Step: 6
Training loss: 1.6100492477416992
Validation loss: 2.0327210823694863

Epoch: 6| Step: 7
Training loss: 2.6774308681488037
Validation loss: 2.045718272527059

Epoch: 6| Step: 8
Training loss: 2.2639238834381104
Validation loss: 2.0377960205078125

Epoch: 6| Step: 9
Training loss: 2.314174175262451
Validation loss: 2.038630485534668

Epoch: 6| Step: 10
Training loss: 2.210822582244873
Validation loss: 2.028262197971344

Epoch: 6| Step: 11
Training loss: 1.754117488861084
Validation loss: 2.034337838490804

Epoch: 6| Step: 12
Training loss: 2.4745383262634277
Validation loss: 2.030325174331665

Epoch: 6| Step: 13
Training loss: 2.26703143119812
Validation loss: 2.0251254041989646

Epoch: 103| Step: 0
Training loss: 2.430237054824829
Validation loss: 2.02509343624115

Epoch: 6| Step: 1
Training loss: 1.9850237369537354
Validation loss: 2.0175758401552835

Epoch: 6| Step: 2
Training loss: 2.63686466217041
Validation loss: 2.021516998608907

Epoch: 6| Step: 3
Training loss: 2.725005626678467
Validation loss: 2.0261525313059487

Epoch: 6| Step: 4
Training loss: 2.5300474166870117
Validation loss: 2.035971244176229

Epoch: 6| Step: 5
Training loss: 1.626938819885254
Validation loss: 2.026556591192881

Epoch: 6| Step: 6
Training loss: 1.9645967483520508
Validation loss: 2.0353983640670776

Epoch: 6| Step: 7
Training loss: 1.8937222957611084
Validation loss: 2.0353014866511026

Epoch: 6| Step: 8
Training loss: 2.9886977672576904
Validation loss: 2.0340749621391296

Epoch: 6| Step: 9
Training loss: 1.877516746520996
Validation loss: 2.0265358487764993

Epoch: 6| Step: 10
Training loss: 1.3937934637069702
Validation loss: 2.0348867177963257

Epoch: 6| Step: 11
Training loss: 2.258852958679199
Validation loss: 2.0218986670176187

Epoch: 6| Step: 12
Training loss: 1.9203317165374756
Validation loss: 2.009599288304647

Epoch: 6| Step: 13
Training loss: 1.972991704940796
Validation loss: 2.0097774863243103

Epoch: 104| Step: 0
Training loss: 2.068286418914795
Validation loss: 2.0124706427256265

Epoch: 6| Step: 1
Training loss: 2.221280574798584
Validation loss: 2.012643019358317

Epoch: 6| Step: 2
Training loss: 2.5853536128997803
Validation loss: 2.011411706606547

Epoch: 6| Step: 3
Training loss: 1.9750676155090332
Validation loss: 2.012491762638092

Epoch: 6| Step: 4
Training loss: 2.124289035797119
Validation loss: 2.0174880425135293

Epoch: 6| Step: 5
Training loss: 2.771845817565918
Validation loss: 2.029572327931722

Epoch: 6| Step: 6
Training loss: 1.3987538814544678
Validation loss: 2.0257351398468018

Epoch: 6| Step: 7
Training loss: 2.7614688873291016
Validation loss: 2.0320880015691123

Epoch: 6| Step: 8
Training loss: 1.752148151397705
Validation loss: 2.027938187122345

Epoch: 6| Step: 9
Training loss: 1.9240520000457764
Validation loss: 2.024822394053141

Epoch: 6| Step: 10
Training loss: 2.3274831771850586
Validation loss: 2.016476631164551

Epoch: 6| Step: 11
Training loss: 1.670602560043335
Validation loss: 2.0166141390800476

Epoch: 6| Step: 12
Training loss: 2.140087366104126
Validation loss: 2.0108596285184226

Epoch: 6| Step: 13
Training loss: 1.8956749439239502
Validation loss: 2.0044960578282676

Epoch: 105| Step: 0
Training loss: 2.078765392303467
Validation loss: 2.01069575548172

Epoch: 6| Step: 1
Training loss: 2.068432569503784
Validation loss: 2.0073256293932595

Epoch: 6| Step: 2
Training loss: 1.68392014503479
Validation loss: 2.001774787902832

Epoch: 6| Step: 3
Training loss: 2.174882411956787
Validation loss: 2.0077743530273438

Epoch: 6| Step: 4
Training loss: 2.2137441635131836
Validation loss: 2.0093375047047934

Epoch: 6| Step: 5
Training loss: 1.9425923824310303
Validation loss: 2.0115354855855307

Epoch: 6| Step: 6
Training loss: 2.488703727722168
Validation loss: 2.013422946135203

Epoch: 6| Step: 7
Training loss: 2.0237538814544678
Validation loss: 2.0144759813944497

Epoch: 6| Step: 8
Training loss: 2.4275412559509277
Validation loss: 2.011914054552714

Epoch: 6| Step: 9
Training loss: 1.6618620157241821
Validation loss: 2.0169390042622886

Epoch: 6| Step: 10
Training loss: 2.7836198806762695
Validation loss: 2.0147021810213723

Epoch: 6| Step: 11
Training loss: 2.096228837966919
Validation loss: 2.024599333604177

Epoch: 6| Step: 12
Training loss: 1.9941318035125732
Validation loss: 2.0267574985822043

Epoch: 6| Step: 13
Training loss: 1.715383768081665
Validation loss: 2.0324259797732034

Epoch: 106| Step: 0
Training loss: 1.7793649435043335
Validation loss: 2.033471961816152

Epoch: 6| Step: 1
Training loss: 2.2299857139587402
Validation loss: 2.03016205628713

Epoch: 6| Step: 2
Training loss: 2.030857563018799
Validation loss: 2.0204866925875344

Epoch: 6| Step: 3
Training loss: 1.8673144578933716
Validation loss: 2.033324102560679

Epoch: 6| Step: 4
Training loss: 2.0554757118225098
Validation loss: 2.0244096318880715

Epoch: 6| Step: 5
Training loss: 1.8983330726623535
Validation loss: 2.0224653681119285

Epoch: 6| Step: 6
Training loss: 2.3149242401123047
Validation loss: 2.0255905787150064

Epoch: 6| Step: 7
Training loss: 1.9441592693328857
Validation loss: 2.025412996610006

Epoch: 6| Step: 8
Training loss: 2.064605236053467
Validation loss: 2.032152235507965

Epoch: 6| Step: 9
Training loss: 2.3579063415527344
Validation loss: 2.0242746074994407

Epoch: 6| Step: 10
Training loss: 1.7177011966705322
Validation loss: 2.0239678621292114

Epoch: 6| Step: 11
Training loss: 2.345781087875366
Validation loss: 2.020345767339071

Epoch: 6| Step: 12
Training loss: 2.680389404296875
Validation loss: 2.0161301692326865

Epoch: 6| Step: 13
Training loss: 2.2267167568206787
Validation loss: 2.027151127656301

Epoch: 107| Step: 0
Training loss: 2.102695941925049
Validation loss: 2.0273775458335876

Epoch: 6| Step: 1
Training loss: 2.251913070678711
Validation loss: 2.023837367693583

Epoch: 6| Step: 2
Training loss: 2.0705013275146484
Validation loss: 2.0250818332036338

Epoch: 6| Step: 3
Training loss: 2.240842819213867
Validation loss: 2.023576299349467

Epoch: 6| Step: 4
Training loss: 2.590847969055176
Validation loss: 2.0276596546173096

Epoch: 6| Step: 5
Training loss: 1.7409985065460205
Validation loss: 2.028103152910868

Epoch: 6| Step: 6
Training loss: 2.036480665206909
Validation loss: 2.022904654343923

Epoch: 6| Step: 7
Training loss: 1.299192190170288
Validation loss: 2.020991265773773

Epoch: 6| Step: 8
Training loss: 2.2696971893310547
Validation loss: 2.0280115008354187

Epoch: 6| Step: 9
Training loss: 2.053818702697754
Validation loss: 2.0290885170300803

Epoch: 6| Step: 10
Training loss: 2.49000883102417
Validation loss: 2.025043507417043

Epoch: 6| Step: 11
Training loss: 2.061760902404785
Validation loss: 2.0411411126454673

Epoch: 6| Step: 12
Training loss: 2.3504104614257812
Validation loss: 2.0437021255493164

Epoch: 6| Step: 13
Training loss: 2.061727523803711
Validation loss: 2.0392280220985413

Epoch: 108| Step: 0
Training loss: 2.226842164993286
Validation loss: 2.0367841323216758

Epoch: 6| Step: 1
Training loss: 2.7504496574401855
Validation loss: 2.0239508748054504

Epoch: 6| Step: 2
Training loss: 2.7636680603027344
Validation loss: 2.022463083267212

Epoch: 6| Step: 3
Training loss: 2.0926673412323
Validation loss: 2.0239174365997314

Epoch: 6| Step: 4
Training loss: 1.9736311435699463
Validation loss: 2.019285202026367

Epoch: 6| Step: 5
Training loss: 1.8767073154449463
Validation loss: 2.0279548168182373

Epoch: 6| Step: 6
Training loss: 1.584618091583252
Validation loss: 2.0258787274360657

Epoch: 6| Step: 7
Training loss: 1.5428125858306885
Validation loss: 2.033775826295217

Epoch: 6| Step: 8
Training loss: 2.01674222946167
Validation loss: 2.03254242738088

Epoch: 6| Step: 9
Training loss: 1.7329602241516113
Validation loss: 2.0227788289388022

Epoch: 6| Step: 10
Training loss: 2.230215311050415
Validation loss: 2.0292280515034995

Epoch: 6| Step: 11
Training loss: 2.7463152408599854
Validation loss: 2.0287723938624063

Epoch: 6| Step: 12
Training loss: 2.677978992462158
Validation loss: 2.0307579040527344

Epoch: 6| Step: 13
Training loss: 1.4707236289978027
Validation loss: 2.0306037267049155

Epoch: 109| Step: 0
Training loss: 1.9048858880996704
Validation loss: 2.035489559173584

Epoch: 6| Step: 1
Training loss: 2.2242422103881836
Validation loss: 2.03604519367218

Epoch: 6| Step: 2
Training loss: 2.2545876502990723
Validation loss: 2.0288016398747764

Epoch: 6| Step: 3
Training loss: 2.0985822677612305
Validation loss: 2.0213531057039895

Epoch: 6| Step: 4
Training loss: 1.6357619762420654
Validation loss: 2.028827170530955

Epoch: 6| Step: 5
Training loss: 1.7316720485687256
Validation loss: 2.0217258532842

Epoch: 6| Step: 6
Training loss: 1.9301650524139404
Validation loss: 2.0274491906166077

Epoch: 6| Step: 7
Training loss: 2.5971388816833496
Validation loss: 2.0283643007278442

Epoch: 6| Step: 8
Training loss: 2.2838754653930664
Validation loss: 2.0370812018712363

Epoch: 6| Step: 9
Training loss: 1.8949284553527832
Validation loss: 2.029585063457489

Epoch: 6| Step: 10
Training loss: 2.025521755218506
Validation loss: 2.04464989900589

Epoch: 6| Step: 11
Training loss: 1.7153151035308838
Validation loss: 2.042220969994863

Epoch: 6| Step: 12
Training loss: 2.430349588394165
Validation loss: 2.0421340664227805

Epoch: 6| Step: 13
Training loss: 2.7457737922668457
Validation loss: 2.03324156999588

Epoch: 110| Step: 0
Training loss: 2.4138681888580322
Validation loss: 2.0327016909917197

Epoch: 6| Step: 1
Training loss: 2.2874996662139893
Validation loss: 2.032645662625631

Epoch: 6| Step: 2
Training loss: 1.4247232675552368
Validation loss: 2.034120738506317

Epoch: 6| Step: 3
Training loss: 2.0307974815368652
Validation loss: 2.027308543523153

Epoch: 6| Step: 4
Training loss: 1.9685955047607422
Validation loss: 2.025413771470388

Epoch: 6| Step: 5
Training loss: 2.796977996826172
Validation loss: 2.02569580078125

Epoch: 6| Step: 6
Training loss: 2.7390499114990234
Validation loss: 2.018201986948649

Epoch: 6| Step: 7
Training loss: 2.069103240966797
Validation loss: 2.025641918182373

Epoch: 6| Step: 8
Training loss: 2.4900827407836914
Validation loss: 2.020210405190786

Epoch: 6| Step: 9
Training loss: 1.5214511156082153
Validation loss: 2.020918528238932

Epoch: 6| Step: 10
Training loss: 1.6934754848480225
Validation loss: 2.0226497451464334

Epoch: 6| Step: 11
Training loss: 1.933780312538147
Validation loss: 2.0191558996836343

Epoch: 6| Step: 12
Training loss: 2.314068555831909
Validation loss: 2.026300768057505

Epoch: 6| Step: 13
Training loss: 1.704914927482605
Validation loss: 2.029019852479299

Epoch: 111| Step: 0
Training loss: 1.9038197994232178
Validation loss: 2.0326451857884726

Epoch: 6| Step: 1
Training loss: 2.3319523334503174
Validation loss: 2.0415919621785483

Epoch: 6| Step: 2
Training loss: 2.4419844150543213
Validation loss: 2.0299216707547507

Epoch: 6| Step: 3
Training loss: 2.0354163646698
Validation loss: 2.03950834274292

Epoch: 6| Step: 4
Training loss: 1.792165756225586
Validation loss: 2.049993912378947

Epoch: 6| Step: 5
Training loss: 2.0927133560180664
Validation loss: 2.0515368382136026

Epoch: 6| Step: 6
Training loss: 2.317293405532837
Validation loss: 2.058892329533895

Epoch: 6| Step: 7
Training loss: 2.0498671531677246
Validation loss: 2.049948751926422

Epoch: 6| Step: 8
Training loss: 1.9502243995666504
Validation loss: 2.0410036047299704

Epoch: 6| Step: 9
Training loss: 1.8359041213989258
Validation loss: 2.0367369453112283

Epoch: 6| Step: 10
Training loss: 1.9396241903305054
Validation loss: 2.0219242771466575

Epoch: 6| Step: 11
Training loss: 2.269320011138916
Validation loss: 2.016396085421244

Epoch: 6| Step: 12
Training loss: 1.9795924425125122
Validation loss: 2.0112096667289734

Epoch: 6| Step: 13
Training loss: 2.5630712509155273
Validation loss: 2.0166836182276406

Epoch: 112| Step: 0
Training loss: 2.289483070373535
Validation loss: 2.0208876729011536

Epoch: 6| Step: 1
Training loss: 2.0907559394836426
Validation loss: 2.036937932173411

Epoch: 6| Step: 2
Training loss: 2.3429923057556152
Validation loss: 2.0490123629570007

Epoch: 6| Step: 3
Training loss: 2.150280714035034
Validation loss: 2.0481902360916138

Epoch: 6| Step: 4
Training loss: 2.52447247505188
Validation loss: 2.0509578784306846

Epoch: 6| Step: 5
Training loss: 1.7259562015533447
Validation loss: 2.0553284088770547

Epoch: 6| Step: 6
Training loss: 2.2761521339416504
Validation loss: 2.058298627535502

Epoch: 6| Step: 7
Training loss: 2.2669920921325684
Validation loss: 2.060845057169596

Epoch: 6| Step: 8
Training loss: 2.1770827770233154
Validation loss: 2.0589017470677695

Epoch: 6| Step: 9
Training loss: 2.5361485481262207
Validation loss: 2.0566634138425193

Epoch: 6| Step: 10
Training loss: 1.7612156867980957
Validation loss: 2.057915508747101

Epoch: 6| Step: 11
Training loss: 2.6421878337860107
Validation loss: 2.055703659852346

Epoch: 6| Step: 12
Training loss: 2.065403938293457
Validation loss: 2.049498200416565

Epoch: 6| Step: 13
Training loss: 1.7875773906707764
Validation loss: 2.048255662123362

Epoch: 113| Step: 0
Training loss: 1.819293737411499
Validation loss: 2.0352503458658853

Epoch: 6| Step: 1
Training loss: 2.573019027709961
Validation loss: 2.0273271401723227

Epoch: 6| Step: 2
Training loss: 1.5527383089065552
Validation loss: 2.0234060486157737

Epoch: 6| Step: 3
Training loss: 2.358883857727051
Validation loss: 2.008421778678894

Epoch: 6| Step: 4
Training loss: 1.9744806289672852
Validation loss: 2.006993810335795

Epoch: 6| Step: 5
Training loss: 2.277496099472046
Validation loss: 2.0147672295570374

Epoch: 6| Step: 6
Training loss: 1.502730369567871
Validation loss: 2.0138705174128213

Epoch: 6| Step: 7
Training loss: 2.1907429695129395
Validation loss: 2.0311968127886453

Epoch: 6| Step: 8
Training loss: 1.9750293493270874
Validation loss: 2.035298685232798

Epoch: 6| Step: 9
Training loss: 2.23513126373291
Validation loss: 2.0442267656326294

Epoch: 6| Step: 10
Training loss: 2.436296224594116
Validation loss: 2.050746520360311

Epoch: 6| Step: 11
Training loss: 2.466155529022217
Validation loss: 2.0460835893948874

Epoch: 6| Step: 12
Training loss: 2.247081995010376
Validation loss: 2.047605733076731

Epoch: 6| Step: 13
Training loss: 2.098233222961426
Validation loss: 2.046437660853068

Epoch: 114| Step: 0
Training loss: 1.8568323850631714
Validation loss: 2.041954497496287

Epoch: 6| Step: 1
Training loss: 2.314309597015381
Validation loss: 2.0465453267097473

Epoch: 6| Step: 2
Training loss: 2.3895840644836426
Validation loss: 2.0420024394989014

Epoch: 6| Step: 3
Training loss: 1.8296371698379517
Validation loss: 2.042271395524343

Epoch: 6| Step: 4
Training loss: 2.4218554496765137
Validation loss: 2.0370975136756897

Epoch: 6| Step: 5
Training loss: 2.295903205871582
Validation loss: 2.0603389739990234

Epoch: 6| Step: 6
Training loss: 1.8241666555404663
Validation loss: 2.0415032505989075

Epoch: 6| Step: 7
Training loss: 1.8539267778396606
Validation loss: 2.040894627571106

Epoch: 6| Step: 8
Training loss: 2.2394864559173584
Validation loss: 2.0390482346216836

Epoch: 6| Step: 9
Training loss: 2.295891761779785
Validation loss: 2.0398733019828796

Epoch: 6| Step: 10
Training loss: 1.5236514806747437
Validation loss: 2.0394517183303833

Epoch: 6| Step: 11
Training loss: 2.3257017135620117
Validation loss: 2.0418486992518106

Epoch: 6| Step: 12
Training loss: 1.9860475063323975
Validation loss: 2.0316762725512185

Epoch: 6| Step: 13
Training loss: 2.0572450160980225
Validation loss: 2.037603735923767

Epoch: 115| Step: 0
Training loss: 2.463195323944092
Validation loss: 2.0396374265352883

Epoch: 6| Step: 1
Training loss: 1.5651805400848389
Validation loss: 2.0338311791419983

Epoch: 6| Step: 2
Training loss: 2.2484092712402344
Validation loss: 2.0397934118906655

Epoch: 6| Step: 3
Training loss: 1.9884902238845825
Validation loss: 2.037262280782064

Epoch: 6| Step: 4
Training loss: 2.206249237060547
Validation loss: 2.0343355337778726

Epoch: 6| Step: 5
Training loss: 2.38240385055542
Validation loss: 2.034438649813334

Epoch: 6| Step: 6
Training loss: 1.9813882112503052
Validation loss: 2.042398671309153

Epoch: 6| Step: 7
Training loss: 2.158384323120117
Validation loss: 2.033827781677246

Epoch: 6| Step: 8
Training loss: 1.8144443035125732
Validation loss: 2.0239065090815225

Epoch: 6| Step: 9
Training loss: 2.0824809074401855
Validation loss: 2.025696039199829

Epoch: 6| Step: 10
Training loss: 2.4833998680114746
Validation loss: 2.0332099397977195

Epoch: 6| Step: 11
Training loss: 1.821988821029663
Validation loss: 2.0338429609934487

Epoch: 6| Step: 12
Training loss: 1.8696208000183105
Validation loss: 2.0366514722506204

Epoch: 6| Step: 13
Training loss: 2.2664058208465576
Validation loss: 2.0410614212354026

Epoch: 116| Step: 0
Training loss: 1.7822532653808594
Validation loss: 2.0348926782608032

Epoch: 6| Step: 1
Training loss: 2.8349623680114746
Validation loss: 2.0549978415171304

Epoch: 6| Step: 2
Training loss: 1.299652338027954
Validation loss: 2.0645641883214316

Epoch: 6| Step: 3
Training loss: 2.660560369491577
Validation loss: 2.07154514392217

Epoch: 6| Step: 4
Training loss: 2.146135091781616
Validation loss: 2.0465781887372336

Epoch: 6| Step: 5
Training loss: 1.9714722633361816
Validation loss: 2.03251322110494

Epoch: 6| Step: 6
Training loss: 2.122680425643921
Validation loss: 2.0307759841283164

Epoch: 6| Step: 7
Training loss: 2.187378168106079
Validation loss: 2.0224579771359763

Epoch: 6| Step: 8
Training loss: 2.31980299949646
Validation loss: 2.0158923665682473

Epoch: 6| Step: 9
Training loss: 2.032836437225342
Validation loss: 2.013963222503662

Epoch: 6| Step: 10
Training loss: 1.700561761856079
Validation loss: 2.009218076864878

Epoch: 6| Step: 11
Training loss: 2.2999422550201416
Validation loss: 2.0121041536331177

Epoch: 6| Step: 12
Training loss: 2.039057731628418
Validation loss: 2.0009644826253257

Epoch: 6| Step: 13
Training loss: 2.265460968017578
Validation loss: 2.0045753121376038

Epoch: 117| Step: 0
Training loss: 2.865051031112671
Validation loss: 2.0104846556981406

Epoch: 6| Step: 1
Training loss: 2.110569477081299
Validation loss: 2.012762745221456

Epoch: 6| Step: 2
Training loss: 1.6737277507781982
Validation loss: 2.00992614030838

Epoch: 6| Step: 3
Training loss: 2.133997917175293
Validation loss: 2.0196308890978494

Epoch: 6| Step: 4
Training loss: 2.168853282928467
Validation loss: 2.0192692279815674

Epoch: 6| Step: 5
Training loss: 1.3883626461029053
Validation loss: 2.026057004928589

Epoch: 6| Step: 6
Training loss: 2.0765786170959473
Validation loss: 2.030438184738159

Epoch: 6| Step: 7
Training loss: 2.4943113327026367
Validation loss: 2.037453750769297

Epoch: 6| Step: 8
Training loss: 2.4338345527648926
Validation loss: 2.037366350491842

Epoch: 6| Step: 9
Training loss: 2.0233232975006104
Validation loss: 2.0392203330993652

Epoch: 6| Step: 10
Training loss: 2.6004905700683594
Validation loss: 2.028743882973989

Epoch: 6| Step: 11
Training loss: 1.6699719429016113
Validation loss: 2.030937592188517

Epoch: 6| Step: 12
Training loss: 2.0032780170440674
Validation loss: 2.026758074760437

Epoch: 6| Step: 13
Training loss: 1.8098876476287842
Validation loss: 2.0165313680966697

Epoch: 118| Step: 0
Training loss: 2.1215643882751465
Validation loss: 2.0264735221862793

Epoch: 6| Step: 1
Training loss: 1.7507803440093994
Validation loss: 2.0233545303344727

Epoch: 6| Step: 2
Training loss: 1.8077099323272705
Validation loss: 2.0264322757720947

Epoch: 6| Step: 3
Training loss: 1.7145531177520752
Validation loss: 2.031430641810099

Epoch: 6| Step: 4
Training loss: 1.9049490690231323
Validation loss: 2.0260541637738547

Epoch: 6| Step: 5
Training loss: 2.1531457901000977
Validation loss: 2.021483321984609

Epoch: 6| Step: 6
Training loss: 1.8091604709625244
Validation loss: 2.0261295239130654

Epoch: 6| Step: 7
Training loss: 2.2876577377319336
Validation loss: 2.026889204978943

Epoch: 6| Step: 8
Training loss: 2.6919312477111816
Validation loss: 2.033328652381897

Epoch: 6| Step: 9
Training loss: 2.7817349433898926
Validation loss: 2.039473613103231

Epoch: 6| Step: 10
Training loss: 2.3741495609283447
Validation loss: 2.03795063495636

Epoch: 6| Step: 11
Training loss: 1.7583506107330322
Validation loss: 2.0316991408665976

Epoch: 6| Step: 12
Training loss: 1.6002299785614014
Validation loss: 2.0396244128545127

Epoch: 6| Step: 13
Training loss: 2.418369770050049
Validation loss: 2.0368582804997764

Epoch: 119| Step: 0
Training loss: 2.5770773887634277
Validation loss: 2.047204315662384

Epoch: 6| Step: 1
Training loss: 2.057981491088867
Validation loss: 2.0387797951698303

Epoch: 6| Step: 2
Training loss: 2.5015010833740234
Validation loss: 2.035505692164103

Epoch: 6| Step: 3
Training loss: 2.2493319511413574
Validation loss: 2.049368937810262

Epoch: 6| Step: 4
Training loss: 2.1947250366210938
Validation loss: 2.0443241198857627

Epoch: 6| Step: 5
Training loss: 1.7390085458755493
Validation loss: 2.0516461730003357

Epoch: 6| Step: 6
Training loss: 1.4951833486557007
Validation loss: 2.043862839539846

Epoch: 6| Step: 7
Training loss: 2.0464274883270264
Validation loss: 2.0346099734306335

Epoch: 6| Step: 8
Training loss: 1.5453739166259766
Validation loss: 2.047066787878672

Epoch: 6| Step: 9
Training loss: 1.7230764627456665
Validation loss: 2.0425321459770203

Epoch: 6| Step: 10
Training loss: 1.8829265832901
Validation loss: 2.036884605884552

Epoch: 6| Step: 11
Training loss: 2.1842756271362305
Validation loss: 2.0314519008000693

Epoch: 6| Step: 12
Training loss: 2.2260890007019043
Validation loss: 2.0341715812683105

Epoch: 6| Step: 13
Training loss: 2.53163480758667
Validation loss: 2.0291078289349875

Epoch: 120| Step: 0
Training loss: 2.0668203830718994
Validation loss: 2.0269091526667276

Epoch: 6| Step: 1
Training loss: 1.6827030181884766
Validation loss: 2.0245123505592346

Epoch: 6| Step: 2
Training loss: 2.6676652431488037
Validation loss: 2.0234349568684897

Epoch: 6| Step: 3
Training loss: 2.0874671936035156
Validation loss: 2.0295545061429343

Epoch: 6| Step: 4
Training loss: 2.0551908016204834
Validation loss: 2.0263769229253135

Epoch: 6| Step: 5
Training loss: 2.4991183280944824
Validation loss: 2.033148169517517

Epoch: 6| Step: 6
Training loss: 1.94736647605896
Validation loss: 2.0335246523221335

Epoch: 6| Step: 7
Training loss: 2.4132132530212402
Validation loss: 2.026094218095144

Epoch: 6| Step: 8
Training loss: 2.1693363189697266
Validation loss: 2.0263007283210754

Epoch: 6| Step: 9
Training loss: 1.8838605880737305
Validation loss: 2.0261831084887185

Epoch: 6| Step: 10
Training loss: 1.6467357873916626
Validation loss: 2.0307730436325073

Epoch: 6| Step: 11
Training loss: 1.669450044631958
Validation loss: 2.025574564933777

Epoch: 6| Step: 12
Training loss: 1.9902994632720947
Validation loss: 2.032344023386637

Epoch: 6| Step: 13
Training loss: 2.3592326641082764
Validation loss: 2.0333720445632935

Epoch: 121| Step: 0
Training loss: 1.8531908988952637
Validation loss: 2.052917003631592

Epoch: 6| Step: 1
Training loss: 1.8739479780197144
Validation loss: 2.0669469038645425

Epoch: 6| Step: 2
Training loss: 2.256873369216919
Validation loss: 2.0826693971951804

Epoch: 6| Step: 3
Training loss: 1.7756597995758057
Validation loss: 2.0827346444129944

Epoch: 6| Step: 4
Training loss: 2.190972089767456
Validation loss: 2.098525047302246

Epoch: 6| Step: 5
Training loss: 2.898632526397705
Validation loss: 2.0840288201967874

Epoch: 6| Step: 6
Training loss: 2.303297519683838
Validation loss: 2.058285435040792

Epoch: 6| Step: 7
Training loss: 2.209608316421509
Validation loss: 2.0445275704065957

Epoch: 6| Step: 8
Training loss: 2.2963480949401855
Validation loss: 2.024348258972168

Epoch: 6| Step: 9
Training loss: 1.590479850769043
Validation loss: 2.021811326344808

Epoch: 6| Step: 10
Training loss: 2.0912551879882812
Validation loss: 2.026494801044464

Epoch: 6| Step: 11
Training loss: 2.699235439300537
Validation loss: 2.034056802590688

Epoch: 6| Step: 12
Training loss: 2.0933141708374023
Validation loss: 2.047887146472931

Epoch: 6| Step: 13
Training loss: 1.6859478950500488
Validation loss: 2.0500296354293823

Epoch: 122| Step: 0
Training loss: 2.030094623565674
Validation loss: 2.059705913066864

Epoch: 6| Step: 1
Training loss: 1.9269074201583862
Validation loss: 2.060787638028463

Epoch: 6| Step: 2
Training loss: 1.757036566734314
Validation loss: 2.060564637184143

Epoch: 6| Step: 3
Training loss: 2.1603496074676514
Validation loss: 2.0634153882662454

Epoch: 6| Step: 4
Training loss: 2.3321101665496826
Validation loss: 2.061087667942047

Epoch: 6| Step: 5
Training loss: 2.560370445251465
Validation loss: 2.064797282218933

Epoch: 6| Step: 6
Training loss: 2.5036182403564453
Validation loss: 2.0658071835835776

Epoch: 6| Step: 7
Training loss: 1.7460322380065918
Validation loss: 2.064849336942037

Epoch: 6| Step: 8
Training loss: 1.8402434587478638
Validation loss: 2.0622756481170654

Epoch: 6| Step: 9
Training loss: 2.06032657623291
Validation loss: 2.057130138079325

Epoch: 6| Step: 10
Training loss: 2.9184651374816895
Validation loss: 2.052255868911743

Epoch: 6| Step: 11
Training loss: 2.09397029876709
Validation loss: 2.046410938103994

Epoch: 6| Step: 12
Training loss: 2.4068007469177246
Validation loss: 2.0400336384773254

Epoch: 6| Step: 13
Training loss: 1.8916860818862915
Validation loss: 2.0345149437586465

Epoch: 123| Step: 0
Training loss: 2.275888442993164
Validation loss: 2.0233620007832847

Epoch: 6| Step: 1
Training loss: 1.9000576734542847
Validation loss: 2.0177970925966897

Epoch: 6| Step: 2
Training loss: 2.2958831787109375
Validation loss: 2.010550320148468

Epoch: 6| Step: 3
Training loss: 1.8129997253417969
Validation loss: 2.0023258328437805

Epoch: 6| Step: 4
Training loss: 1.8693385124206543
Validation loss: 2.003202040990194

Epoch: 6| Step: 5
Training loss: 2.254901885986328
Validation loss: 2.0040465195973716

Epoch: 6| Step: 6
Training loss: 2.111168384552002
Validation loss: 2.0050860246022544

Epoch: 6| Step: 7
Training loss: 2.3769326210021973
Validation loss: 2.0179472963015237

Epoch: 6| Step: 8
Training loss: 1.945163369178772
Validation loss: 2.019773224989573

Epoch: 6| Step: 9
Training loss: 2.2007393836975098
Validation loss: 2.0343662103017173

Epoch: 6| Step: 10
Training loss: 2.1754910945892334
Validation loss: 2.025574564933777

Epoch: 6| Step: 11
Training loss: 2.4201178550720215
Validation loss: 2.022166629632314

Epoch: 6| Step: 12
Training loss: 2.27089524269104
Validation loss: 2.0236867467562356

Epoch: 6| Step: 13
Training loss: 1.4972457885742188
Validation loss: 2.0123852094014487

Epoch: 124| Step: 0
Training loss: 2.383939504623413
Validation loss: 2.014323055744171

Epoch: 6| Step: 1
Training loss: 1.9283175468444824
Validation loss: 2.0004157225290933

Epoch: 6| Step: 2
Training loss: 2.7960314750671387
Validation loss: 2.015132208665212

Epoch: 6| Step: 3
Training loss: 2.416884422302246
Validation loss: 2.008695046106974

Epoch: 6| Step: 4
Training loss: 2.173661708831787
Validation loss: 2.0145040353139243

Epoch: 6| Step: 5
Training loss: 1.8055367469787598
Validation loss: 2.0181181828180947

Epoch: 6| Step: 6
Training loss: 1.8205629587173462
Validation loss: 2.01510880390803

Epoch: 6| Step: 7
Training loss: 1.8367105722427368
Validation loss: 2.0285895268122354

Epoch: 6| Step: 8
Training loss: 2.231599807739258
Validation loss: 2.030582070350647

Epoch: 6| Step: 9
Training loss: 1.8985135555267334
Validation loss: 2.030204474925995

Epoch: 6| Step: 10
Training loss: 1.7553906440734863
Validation loss: 2.035493791103363

Epoch: 6| Step: 11
Training loss: 1.8121917247772217
Validation loss: 2.0293447971343994

Epoch: 6| Step: 12
Training loss: 2.3678975105285645
Validation loss: 2.0411430398623147

Epoch: 6| Step: 13
Training loss: 1.8700268268585205
Validation loss: 2.038774927457174

Epoch: 125| Step: 0
Training loss: 1.6450304985046387
Validation loss: 2.0443535248438516

Epoch: 6| Step: 1
Training loss: 2.1319212913513184
Validation loss: 2.0493518312772117

Epoch: 6| Step: 2
Training loss: 2.0467329025268555
Validation loss: 2.04386568069458

Epoch: 6| Step: 3
Training loss: 2.464986801147461
Validation loss: 2.0478164156277976

Epoch: 6| Step: 4
Training loss: 1.8197622299194336
Validation loss: 2.046959698200226

Epoch: 6| Step: 5
Training loss: 2.1447958946228027
Validation loss: 2.04367458820343

Epoch: 6| Step: 6
Training loss: 1.944077730178833
Validation loss: 2.035997768243154

Epoch: 6| Step: 7
Training loss: 2.3149185180664062
Validation loss: 2.036277731259664

Epoch: 6| Step: 8
Training loss: 1.6675631999969482
Validation loss: 2.0319806337356567

Epoch: 6| Step: 9
Training loss: 1.735363245010376
Validation loss: 2.0255778233210244

Epoch: 6| Step: 10
Training loss: 2.195603370666504
Validation loss: 2.030532658100128

Epoch: 6| Step: 11
Training loss: 2.6352806091308594
Validation loss: 2.0324555337429047

Epoch: 6| Step: 12
Training loss: 1.9172738790512085
Validation loss: 2.034266392389933

Epoch: 6| Step: 13
Training loss: 2.4364423751831055
Validation loss: 2.0309399366378784

Epoch: 126| Step: 0
Training loss: 2.3400206565856934
Validation loss: 2.035965383052826

Epoch: 6| Step: 1
Training loss: 2.2346816062927246
Validation loss: 2.040131390094757

Epoch: 6| Step: 2
Training loss: 1.397908091545105
Validation loss: 2.0307778120040894

Epoch: 6| Step: 3
Training loss: 1.7227098941802979
Validation loss: 2.0297314127286277

Epoch: 6| Step: 4
Training loss: 2.303863525390625
Validation loss: 2.0166492064793906

Epoch: 6| Step: 5
Training loss: 2.272435188293457
Validation loss: 2.021383047103882

Epoch: 6| Step: 6
Training loss: 2.637845516204834
Validation loss: 2.0285841822624207

Epoch: 6| Step: 7
Training loss: 2.58565092086792
Validation loss: 2.030786315600077

Epoch: 6| Step: 8
Training loss: 1.5555802583694458
Validation loss: 2.0211867491404214

Epoch: 6| Step: 9
Training loss: 1.857480764389038
Validation loss: 2.0230296850204468

Epoch: 6| Step: 10
Training loss: 2.1191558837890625
Validation loss: 2.0333083271980286

Epoch: 6| Step: 11
Training loss: 2.0491881370544434
Validation loss: 2.032078822453817

Epoch: 6| Step: 12
Training loss: 1.991607427597046
Validation loss: 2.0315117041269937

Epoch: 6| Step: 13
Training loss: 1.9984511137008667
Validation loss: 2.0450636943181357

Epoch: 127| Step: 0
Training loss: 1.933133840560913
Validation loss: 2.03036101659139

Epoch: 6| Step: 1
Training loss: 1.3708683252334595
Validation loss: 2.024346649646759

Epoch: 6| Step: 2
Training loss: 2.7007994651794434
Validation loss: 2.029048264026642

Epoch: 6| Step: 3
Training loss: 1.747167944908142
Validation loss: 2.0232871770858765

Epoch: 6| Step: 4
Training loss: 2.1425766944885254
Validation loss: 2.043495257695516

Epoch: 6| Step: 5
Training loss: 1.7185842990875244
Validation loss: 2.0463101069132485

Epoch: 6| Step: 6
Training loss: 2.6765308380126953
Validation loss: 2.0480474630991616

Epoch: 6| Step: 7
Training loss: 2.698753833770752
Validation loss: 2.0381171703338623

Epoch: 6| Step: 8
Training loss: 2.0100324153900146
Validation loss: 2.0448126792907715

Epoch: 6| Step: 9
Training loss: 2.141500473022461
Validation loss: 2.030678073565165

Epoch: 6| Step: 10
Training loss: 2.1679186820983887
Validation loss: 2.0322038531303406

Epoch: 6| Step: 11
Training loss: 1.9695532321929932
Validation loss: 2.034558335940043

Epoch: 6| Step: 12
Training loss: 1.7453843355178833
Validation loss: 2.038761774698893

Epoch: 6| Step: 13
Training loss: 2.0128393173217773
Validation loss: 2.0347474018732705

Epoch: 128| Step: 0
Training loss: 1.9478040933609009
Validation loss: 2.0372899572054544

Epoch: 6| Step: 1
Training loss: 2.6864237785339355
Validation loss: 2.045932432015737

Epoch: 6| Step: 2
Training loss: 2.0870447158813477
Validation loss: 2.0416415333747864

Epoch: 6| Step: 3
Training loss: 1.7562847137451172
Validation loss: 2.0397303700447083

Epoch: 6| Step: 4
Training loss: 1.7622737884521484
Validation loss: 2.0322778622309365

Epoch: 6| Step: 5
Training loss: 2.0619142055511475
Validation loss: 2.0404076973597207

Epoch: 6| Step: 6
Training loss: 1.706184983253479
Validation loss: 2.033403833707174

Epoch: 6| Step: 7
Training loss: 1.932987093925476
Validation loss: 2.036406914393107

Epoch: 6| Step: 8
Training loss: 2.6482791900634766
Validation loss: 2.039886951446533

Epoch: 6| Step: 9
Training loss: 2.0405256748199463
Validation loss: 2.037539780139923

Epoch: 6| Step: 10
Training loss: 1.5307918787002563
Validation loss: 2.0274808406829834

Epoch: 6| Step: 11
Training loss: 2.001889228820801
Validation loss: 2.0350029667218528

Epoch: 6| Step: 12
Training loss: 2.4556987285614014
Validation loss: 2.027993122736613

Epoch: 6| Step: 13
Training loss: 2.367152452468872
Validation loss: 2.0354925791422525

Epoch: 129| Step: 0
Training loss: 1.745173454284668
Validation loss: 2.0327959060668945

Epoch: 6| Step: 1
Training loss: 1.8676327466964722
Validation loss: 2.0253522594769797

Epoch: 6| Step: 2
Training loss: 1.6372979879379272
Validation loss: 2.0365793704986572

Epoch: 6| Step: 3
Training loss: 1.95771324634552
Validation loss: 2.0290015737215676

Epoch: 6| Step: 4
Training loss: 1.5652284622192383
Validation loss: 2.034426768620809

Epoch: 6| Step: 5
Training loss: 2.102522373199463
Validation loss: 2.0402349829673767

Epoch: 6| Step: 6
Training loss: 2.238499164581299
Validation loss: 2.03846408923467

Epoch: 6| Step: 7
Training loss: 2.4291672706604004
Validation loss: 2.0449397961298623

Epoch: 6| Step: 8
Training loss: 2.624157428741455
Validation loss: 2.050312598546346

Epoch: 6| Step: 9
Training loss: 1.9021810293197632
Validation loss: 2.0459326903025308

Epoch: 6| Step: 10
Training loss: 2.2836337089538574
Validation loss: 2.0339750250180564

Epoch: 6| Step: 11
Training loss: 2.129873275756836
Validation loss: 2.0304441849390664

Epoch: 6| Step: 12
Training loss: 2.316923141479492
Validation loss: 2.032311578591665

Epoch: 6| Step: 13
Training loss: 2.1948399543762207
Validation loss: 2.0247566302617392

Epoch: 130| Step: 0
Training loss: 2.388916492462158
Validation loss: 2.029054264227549

Epoch: 6| Step: 1
Training loss: 1.4948967695236206
Validation loss: 2.0374769171079

Epoch: 6| Step: 2
Training loss: 2.535348653793335
Validation loss: 2.0340608755747476

Epoch: 6| Step: 3
Training loss: 2.4229767322540283
Validation loss: 2.034795860449473

Epoch: 6| Step: 4
Training loss: 2.1804494857788086
Validation loss: 2.0439491271972656

Epoch: 6| Step: 5
Training loss: 2.3736982345581055
Validation loss: 2.042977452278137

Epoch: 6| Step: 6
Training loss: 1.7220653295516968
Validation loss: 2.0476864178975425

Epoch: 6| Step: 7
Training loss: 2.1848931312561035
Validation loss: 2.0386848052342734

Epoch: 6| Step: 8
Training loss: 2.1483798027038574
Validation loss: 2.033306578795115

Epoch: 6| Step: 9
Training loss: 2.2220542430877686
Validation loss: 2.038459380467733

Epoch: 6| Step: 10
Training loss: 1.5081286430358887
Validation loss: 2.02881787220637

Epoch: 6| Step: 11
Training loss: 1.9175853729248047
Validation loss: 2.024160544077555

Epoch: 6| Step: 12
Training loss: 2.3165884017944336
Validation loss: 2.0257277687390647

Epoch: 6| Step: 13
Training loss: 1.9884088039398193
Validation loss: 2.020254055658976

Epoch: 131| Step: 0
Training loss: 2.435157060623169
Validation loss: 2.017142196496328

Epoch: 6| Step: 1
Training loss: 2.736410140991211
Validation loss: 2.02657820781072

Epoch: 6| Step: 2
Training loss: 2.0350759029388428
Validation loss: 2.025778830051422

Epoch: 6| Step: 3
Training loss: 2.111164093017578
Validation loss: 2.0303610364596048

Epoch: 6| Step: 4
Training loss: 2.0478365421295166
Validation loss: 2.0173227787017822

Epoch: 6| Step: 5
Training loss: 1.8561261892318726
Validation loss: 2.0229958295822144

Epoch: 6| Step: 6
Training loss: 2.2177937030792236
Validation loss: 2.026418308417002

Epoch: 6| Step: 7
Training loss: 1.110519528388977
Validation loss: 2.0146177808443704

Epoch: 6| Step: 8
Training loss: 1.9363813400268555
Validation loss: 2.0215231577555337

Epoch: 6| Step: 9
Training loss: 2.1592466831207275
Validation loss: 2.025867303212484

Epoch: 6| Step: 10
Training loss: 2.5132389068603516
Validation loss: 2.0203481117884317

Epoch: 6| Step: 11
Training loss: 1.6620185375213623
Validation loss: 2.024040102958679

Epoch: 6| Step: 12
Training loss: 1.8453625440597534
Validation loss: 2.029276510079702

Epoch: 6| Step: 13
Training loss: 2.108128070831299
Validation loss: 2.0328041712443032

Epoch: 132| Step: 0
Training loss: 1.8991448879241943
Validation loss: 2.031380077203115

Epoch: 6| Step: 1
Training loss: 1.5823787450790405
Validation loss: 2.0393672386805215

Epoch: 6| Step: 2
Training loss: 1.7135188579559326
Validation loss: 2.0439151724179587

Epoch: 6| Step: 3
Training loss: 2.1383142471313477
Validation loss: 2.034001628557841

Epoch: 6| Step: 4
Training loss: 2.0338134765625
Validation loss: 2.047646184762319

Epoch: 6| Step: 5
Training loss: 2.694187641143799
Validation loss: 2.0468552708625793

Epoch: 6| Step: 6
Training loss: 2.122875213623047
Validation loss: 2.048611342906952

Epoch: 6| Step: 7
Training loss: 2.4904603958129883
Validation loss: 2.052834769090017

Epoch: 6| Step: 8
Training loss: 1.9920759201049805
Validation loss: 2.0637895663579306

Epoch: 6| Step: 9
Training loss: 2.260146141052246
Validation loss: 2.0752411683400473

Epoch: 6| Step: 10
Training loss: 2.0030698776245117
Validation loss: 2.0744213660558066

Epoch: 6| Step: 11
Training loss: 2.5383503437042236
Validation loss: 2.0681158105532327

Epoch: 6| Step: 12
Training loss: 2.119058609008789
Validation loss: 2.070705791314443

Epoch: 6| Step: 13
Training loss: 1.4261078834533691
Validation loss: 2.054198900858561

Epoch: 133| Step: 0
Training loss: 2.666471481323242
Validation loss: 2.056157886981964

Epoch: 6| Step: 1
Training loss: 1.9818679094314575
Validation loss: 2.043748438358307

Epoch: 6| Step: 2
Training loss: 2.207557201385498
Validation loss: 2.048142115275065

Epoch: 6| Step: 3
Training loss: 2.070763349533081
Validation loss: 2.044608732064565

Epoch: 6| Step: 4
Training loss: 1.9881395101547241
Validation loss: 2.04883482058843

Epoch: 6| Step: 5
Training loss: 1.7256914377212524
Validation loss: 2.0420143405596414

Epoch: 6| Step: 6
Training loss: 2.1612868309020996
Validation loss: 2.035163084665934

Epoch: 6| Step: 7
Training loss: 2.1207730770111084
Validation loss: 2.042123238245646

Epoch: 6| Step: 8
Training loss: 2.367201805114746
Validation loss: 2.0387375950813293

Epoch: 6| Step: 9
Training loss: 2.3448195457458496
Validation loss: 2.0361821254094443

Epoch: 6| Step: 10
Training loss: 2.1050755977630615
Validation loss: 2.030801832675934

Epoch: 6| Step: 11
Training loss: 1.6626017093658447
Validation loss: 2.029647429784139

Epoch: 6| Step: 12
Training loss: 1.6153144836425781
Validation loss: 2.030324161052704

Epoch: 6| Step: 13
Training loss: 1.8775030374526978
Validation loss: 2.0319459637006125

Epoch: 134| Step: 0
Training loss: 2.285059928894043
Validation loss: 2.035203754901886

Epoch: 6| Step: 1
Training loss: 2.0443296432495117
Validation loss: 2.02859636147817

Epoch: 6| Step: 2
Training loss: 2.3665771484375
Validation loss: 2.0403644839922586

Epoch: 6| Step: 3
Training loss: 1.4373903274536133
Validation loss: 2.040592670440674

Epoch: 6| Step: 4
Training loss: 2.1374340057373047
Validation loss: 2.04885071516037

Epoch: 6| Step: 5
Training loss: 2.1973352432250977
Validation loss: 2.0460680723190308

Epoch: 6| Step: 6
Training loss: 1.4793137311935425
Validation loss: 2.0396519700686135

Epoch: 6| Step: 7
Training loss: 1.7292673587799072
Validation loss: 2.039904793103536

Epoch: 6| Step: 8
Training loss: 1.9568290710449219
Validation loss: 2.0387139519055686

Epoch: 6| Step: 9
Training loss: 2.8226819038391113
Validation loss: 2.03877462943395

Epoch: 6| Step: 10
Training loss: 1.6559550762176514
Validation loss: 2.0487319827079773

Epoch: 6| Step: 11
Training loss: 1.8978090286254883
Validation loss: 2.041049341360728

Epoch: 6| Step: 12
Training loss: 2.709442138671875
Validation loss: 2.0447864929835

Epoch: 6| Step: 13
Training loss: 2.0540928840637207
Validation loss: 2.053714076677958

Epoch: 135| Step: 0
Training loss: 2.324756622314453
Validation loss: 2.0713762044906616

Epoch: 6| Step: 1
Training loss: 2.5031120777130127
Validation loss: 2.0641102393468223

Epoch: 6| Step: 2
Training loss: 1.7797352075576782
Validation loss: 2.0723976294199624

Epoch: 6| Step: 3
Training loss: 2.0320019721984863
Validation loss: 2.0672895113627114

Epoch: 6| Step: 4
Training loss: 2.093924045562744
Validation loss: 2.055607239405314

Epoch: 6| Step: 5
Training loss: 2.349109649658203
Validation loss: 2.0463353395462036

Epoch: 6| Step: 6
Training loss: 1.822676181793213
Validation loss: 2.0488821268081665

Epoch: 6| Step: 7
Training loss: 2.6131973266601562
Validation loss: 2.0118254820505777

Epoch: 6| Step: 8
Training loss: 1.878767967224121
Validation loss: 2.0232189496358237

Epoch: 6| Step: 9
Training loss: 2.3475611209869385
Validation loss: 2.025518476963043

Epoch: 6| Step: 10
Training loss: 1.4851094484329224
Validation loss: 2.0272823770840964

Epoch: 6| Step: 11
Training loss: 2.1804580688476562
Validation loss: 2.0292205214500427

Epoch: 6| Step: 12
Training loss: 2.3672537803649902
Validation loss: 2.02932471036911

Epoch: 6| Step: 13
Training loss: 1.4024641513824463
Validation loss: 2.02582315603892

Epoch: 136| Step: 0
Training loss: 2.1192197799682617
Validation loss: 2.0305956602096558

Epoch: 6| Step: 1
Training loss: 2.164973497390747
Validation loss: 2.035892387231191

Epoch: 6| Step: 2
Training loss: 2.4419546127319336
Validation loss: 2.0405542651812234

Epoch: 6| Step: 3
Training loss: 2.264772415161133
Validation loss: 2.0323499838511148

Epoch: 6| Step: 4
Training loss: 1.8685129880905151
Validation loss: 2.033665657043457

Epoch: 6| Step: 5
Training loss: 2.2498862743377686
Validation loss: 2.043297052383423

Epoch: 6| Step: 6
Training loss: 2.075892925262451
Validation loss: 2.0328964591026306

Epoch: 6| Step: 7
Training loss: 1.9664785861968994
Validation loss: 2.0490578611691794

Epoch: 6| Step: 8
Training loss: 1.460033655166626
Validation loss: 2.05235348145167

Epoch: 6| Step: 9
Training loss: 1.8542516231536865
Validation loss: 2.0342619816462197

Epoch: 6| Step: 10
Training loss: 2.4441680908203125
Validation loss: 2.0477423071861267

Epoch: 6| Step: 11
Training loss: 1.8548424243927002
Validation loss: 2.0598652760187783

Epoch: 6| Step: 12
Training loss: 1.933977484703064
Validation loss: 2.0482097864151

Epoch: 6| Step: 13
Training loss: 2.199329376220703
Validation loss: 2.051768203576406

Epoch: 137| Step: 0
Training loss: 2.1863698959350586
Validation loss: 2.042665719985962

Epoch: 6| Step: 1
Training loss: 1.804094910621643
Validation loss: 2.036019762357076

Epoch: 6| Step: 2
Training loss: 2.1962404251098633
Validation loss: 2.0468031764030457

Epoch: 6| Step: 3
Training loss: 1.827470064163208
Validation loss: 2.0477482676506042

Epoch: 6| Step: 4
Training loss: 2.1918327808380127
Validation loss: 2.048119684060415

Epoch: 6| Step: 5
Training loss: 1.7534795999526978
Validation loss: 2.051352540651957

Epoch: 6| Step: 6
Training loss: 1.9279520511627197
Validation loss: 2.0472158789634705

Epoch: 6| Step: 7
Training loss: 2.416759729385376
Validation loss: 2.0516990025838218

Epoch: 6| Step: 8
Training loss: 1.717866063117981
Validation loss: 2.050752580165863

Epoch: 6| Step: 9
Training loss: 2.035130500793457
Validation loss: 2.0510780811309814

Epoch: 6| Step: 10
Training loss: 2.792171001434326
Validation loss: 2.0418591698010764

Epoch: 6| Step: 11
Training loss: 1.7644240856170654
Validation loss: 2.0358426769574485

Epoch: 6| Step: 12
Training loss: 2.2639877796173096
Validation loss: 2.0452017386754355

Epoch: 6| Step: 13
Training loss: 1.9263544082641602
Validation loss: 2.039856712023417

Epoch: 138| Step: 0
Training loss: 2.424339771270752
Validation loss: 2.04532919327418

Epoch: 6| Step: 1
Training loss: 1.6741290092468262
Validation loss: 2.0411818822224936

Epoch: 6| Step: 2
Training loss: 1.9767463207244873
Validation loss: 2.044743518034617

Epoch: 6| Step: 3
Training loss: 2.4073100090026855
Validation loss: 2.053789973258972

Epoch: 6| Step: 4
Training loss: 2.1658496856689453
Validation loss: 2.0484328866004944

Epoch: 6| Step: 5
Training loss: 2.0612235069274902
Validation loss: 2.035684565703074

Epoch: 6| Step: 6
Training loss: 1.6874027252197266
Validation loss: 2.0486690203348794

Epoch: 6| Step: 7
Training loss: 2.1139557361602783
Validation loss: 2.0552197893460593

Epoch: 6| Step: 8
Training loss: 2.336998224258423
Validation loss: 2.056057075659434

Epoch: 6| Step: 9
Training loss: 1.883023738861084
Validation loss: 2.063353180885315

Epoch: 6| Step: 10
Training loss: 2.0530385971069336
Validation loss: 2.0548903346061707

Epoch: 6| Step: 11
Training loss: 1.832076072692871
Validation loss: 2.0609591404596963

Epoch: 6| Step: 12
Training loss: 2.157604694366455
Validation loss: 2.05292679866155

Epoch: 6| Step: 13
Training loss: 1.7297427654266357
Validation loss: 2.0686031381289163

Epoch: 139| Step: 0
Training loss: 2.215853691101074
Validation loss: 2.050058980782827

Epoch: 6| Step: 1
Training loss: 2.1889524459838867
Validation loss: 2.0472156405448914

Epoch: 6| Step: 2
Training loss: 1.929667353630066
Validation loss: 2.048305034637451

Epoch: 6| Step: 3
Training loss: 2.0512757301330566
Validation loss: 2.0384693344434104

Epoch: 6| Step: 4
Training loss: 1.6639626026153564
Validation loss: 2.0444448590278625

Epoch: 6| Step: 5
Training loss: 2.447188377380371
Validation loss: 2.0414104660352073

Epoch: 6| Step: 6
Training loss: 2.042752742767334
Validation loss: 2.0373640060424805

Epoch: 6| Step: 7
Training loss: 2.2301793098449707
Validation loss: 2.0417609016100564

Epoch: 6| Step: 8
Training loss: 2.4954943656921387
Validation loss: 2.0394752820332847

Epoch: 6| Step: 9
Training loss: 2.012228012084961
Validation loss: 2.0368767579396567

Epoch: 6| Step: 10
Training loss: 2.0162572860717773
Validation loss: 2.041377007961273

Epoch: 6| Step: 11
Training loss: 1.8375170230865479
Validation loss: 2.042030950387319

Epoch: 6| Step: 12
Training loss: 1.4926254749298096
Validation loss: 2.054431398709615

Epoch: 6| Step: 13
Training loss: 1.8709156513214111
Validation loss: 2.0512102246284485

Epoch: 140| Step: 0
Training loss: 2.502190589904785
Validation loss: 2.0464545090993247

Epoch: 6| Step: 1
Training loss: 1.744826316833496
Validation loss: 2.0513559182484946

Epoch: 6| Step: 2
Training loss: 1.8620525598526
Validation loss: 2.069016456604004

Epoch: 6| Step: 3
Training loss: 2.637816905975342
Validation loss: 2.0738593141237893

Epoch: 6| Step: 4
Training loss: 2.1817686557769775
Validation loss: 2.0767520864804587

Epoch: 6| Step: 5
Training loss: 1.9454948902130127
Validation loss: 2.068087716897329

Epoch: 6| Step: 6
Training loss: 2.189728021621704
Validation loss: 2.0794593691825867

Epoch: 6| Step: 7
Training loss: 2.184378147125244
Validation loss: 2.0721110701560974

Epoch: 6| Step: 8
Training loss: 1.8712717294692993
Validation loss: 2.0645185708999634

Epoch: 6| Step: 9
Training loss: 1.558271884918213
Validation loss: 2.061903178691864

Epoch: 6| Step: 10
Training loss: 1.9964494705200195
Validation loss: 2.0663941502571106

Epoch: 6| Step: 11
Training loss: 2.4220638275146484
Validation loss: 2.0542173782984414

Epoch: 6| Step: 12
Training loss: 1.83356511592865
Validation loss: 2.0475077629089355

Epoch: 6| Step: 13
Training loss: 2.064826488494873
Validation loss: 2.044888357321421

Epoch: 141| Step: 0
Training loss: 2.0879101753234863
Validation loss: 2.051984131336212

Epoch: 6| Step: 1
Training loss: 2.3092570304870605
Validation loss: 2.0436724026997886

Epoch: 6| Step: 2
Training loss: 1.9861462116241455
Validation loss: 2.0510411858558655

Epoch: 6| Step: 3
Training loss: 1.616732120513916
Validation loss: 2.0491450230280557

Epoch: 6| Step: 4
Training loss: 2.247194290161133
Validation loss: 2.0557653307914734

Epoch: 6| Step: 5
Training loss: 1.9255750179290771
Validation loss: 2.060830513636271

Epoch: 6| Step: 6
Training loss: 1.579569935798645
Validation loss: 2.046081026395162

Epoch: 6| Step: 7
Training loss: 2.0214998722076416
Validation loss: 2.055635631084442

Epoch: 6| Step: 8
Training loss: 1.7086600065231323
Validation loss: 2.0628734827041626

Epoch: 6| Step: 9
Training loss: 2.275233745574951
Validation loss: 2.079775015513102

Epoch: 6| Step: 10
Training loss: 2.181532859802246
Validation loss: 2.0751296281814575

Epoch: 6| Step: 11
Training loss: 2.1191601753234863
Validation loss: 2.079337239265442

Epoch: 6| Step: 12
Training loss: 2.4847259521484375
Validation loss: 2.0700812141100564

Epoch: 6| Step: 13
Training loss: 2.591832160949707
Validation loss: 2.0582244594891868

Epoch: 142| Step: 0
Training loss: 1.9628266096115112
Validation loss: 2.063392162322998

Epoch: 6| Step: 1
Training loss: 1.9263688325881958
Validation loss: 2.0624649922053018

Epoch: 6| Step: 2
Training loss: 1.998199701309204
Validation loss: 2.0596534411112466

Epoch: 6| Step: 3
Training loss: 2.3834869861602783
Validation loss: 2.0573963721593223

Epoch: 6| Step: 4
Training loss: 2.059980869293213
Validation loss: 2.0479538639386496

Epoch: 6| Step: 5
Training loss: 2.4734835624694824
Validation loss: 2.05108642578125

Epoch: 6| Step: 6
Training loss: 1.5798946619033813
Validation loss: 2.037047823270162

Epoch: 6| Step: 7
Training loss: 2.414154529571533
Validation loss: 2.0363905231157937

Epoch: 6| Step: 8
Training loss: 2.5136194229125977
Validation loss: 2.035793940226237

Epoch: 6| Step: 9
Training loss: 2.0200304985046387
Validation loss: 2.0481120546658835

Epoch: 6| Step: 10
Training loss: 1.7305448055267334
Validation loss: 2.0375019907951355

Epoch: 6| Step: 11
Training loss: 2.0432417392730713
Validation loss: 2.054757614930471

Epoch: 6| Step: 12
Training loss: 1.6459770202636719
Validation loss: 2.0596755345662436

Epoch: 6| Step: 13
Training loss: 1.9628586769104004
Validation loss: 2.063301424185435

Epoch: 143| Step: 0
Training loss: 2.4287805557250977
Validation loss: 2.065895934899648

Epoch: 6| Step: 1
Training loss: 2.31628680229187
Validation loss: 2.0456703503926597

Epoch: 6| Step: 2
Training loss: 2.1447625160217285
Validation loss: 2.042358716328939

Epoch: 6| Step: 3
Training loss: 1.9803500175476074
Validation loss: 2.032820781071981

Epoch: 6| Step: 4
Training loss: 1.3807209730148315
Validation loss: 2.042631288369497

Epoch: 6| Step: 5
Training loss: 2.1637725830078125
Validation loss: 2.0320255756378174

Epoch: 6| Step: 6
Training loss: 1.7237797975540161
Validation loss: 2.0267574191093445

Epoch: 6| Step: 7
Training loss: 1.6012850999832153
Validation loss: 2.040495534737905

Epoch: 6| Step: 8
Training loss: 2.065359115600586
Validation loss: 2.03800235191981

Epoch: 6| Step: 9
Training loss: 1.9028280973434448
Validation loss: 2.035774310429891

Epoch: 6| Step: 10
Training loss: 2.128119945526123
Validation loss: 2.0442675352096558

Epoch: 6| Step: 11
Training loss: 2.3889589309692383
Validation loss: 2.0500717163085938

Epoch: 6| Step: 12
Training loss: 2.1094393730163574
Validation loss: 2.0428646405537925

Epoch: 6| Step: 13
Training loss: 2.1265926361083984
Validation loss: 2.048492511113485

Epoch: 144| Step: 0
Training loss: 1.6641619205474854
Validation loss: 2.0472397009531655

Epoch: 6| Step: 1
Training loss: 2.844216823577881
Validation loss: 2.0467271407445273

Epoch: 6| Step: 2
Training loss: 1.9788695573806763
Validation loss: 2.0462400118509927

Epoch: 6| Step: 3
Training loss: 2.0670995712280273
Validation loss: 2.0584797461827598

Epoch: 6| Step: 4
Training loss: 2.008934497833252
Validation loss: 2.051512877146403

Epoch: 6| Step: 5
Training loss: 2.0080819129943848
Validation loss: 2.051654557387034

Epoch: 6| Step: 6
Training loss: 2.516843557357788
Validation loss: 2.052842597166697

Epoch: 6| Step: 7
Training loss: 1.7940099239349365
Validation loss: 2.0468260248502097

Epoch: 6| Step: 8
Training loss: 2.416630744934082
Validation loss: 2.0616273880004883

Epoch: 6| Step: 9
Training loss: 1.9364230632781982
Validation loss: 2.0608962774276733

Epoch: 6| Step: 10
Training loss: 1.821485996246338
Validation loss: 2.0676542123158774

Epoch: 6| Step: 11
Training loss: 1.7048485279083252
Validation loss: 2.0684215426445007

Epoch: 6| Step: 12
Training loss: 2.252859354019165
Validation loss: 2.0552573005358377

Epoch: 6| Step: 13
Training loss: 1.1567370891571045
Validation loss: 2.053417980670929

Epoch: 145| Step: 0
Training loss: 2.1173853874206543
Validation loss: 2.047354300816854

Epoch: 6| Step: 1
Training loss: 1.8216975927352905
Validation loss: 2.0512685974438987

Epoch: 6| Step: 2
Training loss: 1.9744614362716675
Validation loss: 2.0533752044041953

Epoch: 6| Step: 3
Training loss: 1.891161322593689
Validation loss: 2.0577086210250854

Epoch: 6| Step: 4
Training loss: 2.639035940170288
Validation loss: 2.0505923430124917

Epoch: 6| Step: 5
Training loss: 1.7125029563903809
Validation loss: 2.05326900879542

Epoch: 6| Step: 6
Training loss: 1.9952852725982666
Validation loss: 2.0384297569592795

Epoch: 6| Step: 7
Training loss: 2.1343204975128174
Validation loss: 2.0487927397092185

Epoch: 6| Step: 8
Training loss: 1.9777363538742065
Validation loss: 2.047833244005839

Epoch: 6| Step: 9
Training loss: 1.5312687158584595
Validation loss: 2.0553074876467385

Epoch: 6| Step: 10
Training loss: 2.5668540000915527
Validation loss: 2.0468783577283225

Epoch: 6| Step: 11
Training loss: 1.7765648365020752
Validation loss: 2.040821353594462

Epoch: 6| Step: 12
Training loss: 2.205173969268799
Validation loss: 2.0423652132352195

Epoch: 6| Step: 13
Training loss: 2.106210708618164
Validation loss: 2.0355131427447

Epoch: 146| Step: 0
Training loss: 1.997673749923706
Validation loss: 2.0443693002065024

Epoch: 6| Step: 1
Training loss: 2.204880714416504
Validation loss: 2.043016731739044

Epoch: 6| Step: 2
Training loss: 1.8623707294464111
Validation loss: 2.037681996822357

Epoch: 6| Step: 3
Training loss: 1.707809567451477
Validation loss: 2.042747418085734

Epoch: 6| Step: 4
Training loss: 1.9213309288024902
Validation loss: 2.0353182554244995

Epoch: 6| Step: 5
Training loss: 1.5623741149902344
Validation loss: 2.0429550210634866

Epoch: 6| Step: 6
Training loss: 2.182096481323242
Validation loss: 2.028700331846873

Epoch: 6| Step: 7
Training loss: 2.1627893447875977
Validation loss: 2.0427569150924683

Epoch: 6| Step: 8
Training loss: 2.1318092346191406
Validation loss: 2.0424245794614158

Epoch: 6| Step: 9
Training loss: 1.6908824443817139
Validation loss: 2.0414733290672302

Epoch: 6| Step: 10
Training loss: 2.3993349075317383
Validation loss: 2.0484381715456643

Epoch: 6| Step: 11
Training loss: 2.2418031692504883
Validation loss: 2.052886148293813

Epoch: 6| Step: 12
Training loss: 2.140936851501465
Validation loss: 2.051407516002655

Epoch: 6| Step: 13
Training loss: 2.1798744201660156
Validation loss: 2.056421995162964

Epoch: 147| Step: 0
Training loss: 1.7265353202819824
Validation loss: 2.0571050445238748

Epoch: 6| Step: 1
Training loss: 1.9403600692749023
Validation loss: 2.0522514382998147

Epoch: 6| Step: 2
Training loss: 1.8265118598937988
Validation loss: 2.043532391389211

Epoch: 6| Step: 3
Training loss: 1.889322280883789
Validation loss: 2.062094589074453

Epoch: 6| Step: 4
Training loss: 2.451293468475342
Validation loss: 2.0640464425086975

Epoch: 6| Step: 5
Training loss: 2.2926857471466064
Validation loss: 2.0671301086743674

Epoch: 6| Step: 6
Training loss: 2.383884906768799
Validation loss: 2.0643524130185447

Epoch: 6| Step: 7
Training loss: 1.7234182357788086
Validation loss: 2.0610804160435996

Epoch: 6| Step: 8
Training loss: 2.1851706504821777
Validation loss: 2.057802061239878

Epoch: 6| Step: 9
Training loss: 1.7896816730499268
Validation loss: 2.067324697971344

Epoch: 6| Step: 10
Training loss: 2.0008277893066406
Validation loss: 2.067057112852732

Epoch: 6| Step: 11
Training loss: 2.2979722023010254
Validation loss: 2.0573365887006125

Epoch: 6| Step: 12
Training loss: 1.7815287113189697
Validation loss: 2.058128833770752

Epoch: 6| Step: 13
Training loss: 2.1824419498443604
Validation loss: 2.072175840536753

Epoch: 148| Step: 0
Training loss: 1.7205111980438232
Validation loss: 2.063746968905131

Epoch: 6| Step: 1
Training loss: 2.6776838302612305
Validation loss: 2.0676542123158774

Epoch: 6| Step: 2
Training loss: 1.7212836742401123
Validation loss: 2.061980128288269

Epoch: 6| Step: 3
Training loss: 2.5689432621002197
Validation loss: 2.0549832781155906

Epoch: 6| Step: 4
Training loss: 2.017183303833008
Validation loss: 2.0644145011901855

Epoch: 6| Step: 5
Training loss: 1.5450530052185059
Validation loss: 2.062722643216451

Epoch: 6| Step: 6
Training loss: 1.636338233947754
Validation loss: 2.0684640606244407

Epoch: 6| Step: 7
Training loss: 2.5750162601470947
Validation loss: 2.0598616202672324

Epoch: 6| Step: 8
Training loss: 2.270331859588623
Validation loss: 2.060689111550649

Epoch: 6| Step: 9
Training loss: 1.7317602634429932
Validation loss: 2.068187733491262

Epoch: 6| Step: 10
Training loss: 1.9413738250732422
Validation loss: 2.055919090906779

Epoch: 6| Step: 11
Training loss: 1.8217129707336426
Validation loss: 2.051784098148346

Epoch: 6| Step: 12
Training loss: 2.2303338050842285
Validation loss: 2.0448543429374695

Epoch: 6| Step: 13
Training loss: 1.8475852012634277
Validation loss: 2.043258229891459

Epoch: 149| Step: 0
Training loss: 1.5872507095336914
Validation loss: 2.035892049471537

Epoch: 6| Step: 1
Training loss: 2.2427754402160645
Validation loss: 2.0440072615941367

Epoch: 6| Step: 2
Training loss: 1.933034062385559
Validation loss: 2.0411468545595803

Epoch: 6| Step: 3
Training loss: 2.5324344635009766
Validation loss: 2.0358726382255554

Epoch: 6| Step: 4
Training loss: 1.7960556745529175
Validation loss: 2.052959680557251

Epoch: 6| Step: 5
Training loss: 1.7999279499053955
Validation loss: 2.0422703623771667

Epoch: 6| Step: 6
Training loss: 2.057202100753784
Validation loss: 2.0351898670196533

Epoch: 6| Step: 7
Training loss: 1.3524696826934814
Validation loss: 2.0360621213912964

Epoch: 6| Step: 8
Training loss: 2.222961187362671
Validation loss: 2.0385312835375466

Epoch: 6| Step: 9
Training loss: 2.0653717517852783
Validation loss: 2.0254438519477844

Epoch: 6| Step: 10
Training loss: 2.6977028846740723
Validation loss: 2.031723062197367

Epoch: 6| Step: 11
Training loss: 2.8145556449890137
Validation loss: 2.0216164588928223

Epoch: 6| Step: 12
Training loss: 2.0000410079956055
Validation loss: 2.0179433027903237

Epoch: 6| Step: 13
Training loss: 1.4563953876495361
Validation loss: 2.033585011959076

Epoch: 150| Step: 0
Training loss: 1.8774759769439697
Validation loss: 2.0535744428634644

Epoch: 6| Step: 1
Training loss: 1.8041422367095947
Validation loss: 2.0600507855415344

Epoch: 6| Step: 2
Training loss: 1.8705775737762451
Validation loss: 2.0756749312082925

Epoch: 6| Step: 3
Training loss: 1.951054573059082
Validation loss: 2.078325013319651

Epoch: 6| Step: 4
Training loss: 2.298107862472534
Validation loss: 2.0780024329821267

Epoch: 6| Step: 5
Training loss: 1.6107392311096191
Validation loss: 2.0784741640090942

Epoch: 6| Step: 6
Training loss: 2.366604804992676
Validation loss: 2.079203168551127

Epoch: 6| Step: 7
Training loss: 2.411132574081421
Validation loss: 2.060359219710032

Epoch: 6| Step: 8
Training loss: 1.7248287200927734
Validation loss: 2.0678451458613076

Epoch: 6| Step: 9
Training loss: 2.1677355766296387
Validation loss: 2.066912750403086

Epoch: 6| Step: 10
Training loss: 1.7944138050079346
Validation loss: 2.0544705788294473

Epoch: 6| Step: 11
Training loss: 2.3832178115844727
Validation loss: 2.055904984474182

Epoch: 6| Step: 12
Training loss: 1.89691162109375
Validation loss: 2.0601945718129477

Epoch: 6| Step: 13
Training loss: 2.4980111122131348
Validation loss: 2.0520156621932983

Epoch: 151| Step: 0
Training loss: 1.9950170516967773
Validation loss: 2.053603251775106

Epoch: 6| Step: 1
Training loss: 2.0467898845672607
Validation loss: 2.052912871042887

Epoch: 6| Step: 2
Training loss: 1.4465546607971191
Validation loss: 2.0602990786234536

Epoch: 6| Step: 3
Training loss: 2.7053370475769043
Validation loss: 2.043494701385498

Epoch: 6| Step: 4
Training loss: 1.8647563457489014
Validation loss: 2.0601367751757302

Epoch: 6| Step: 5
Training loss: 2.072129726409912
Validation loss: 2.0616332491238913

Epoch: 6| Step: 6
Training loss: 1.9877145290374756
Validation loss: 2.0659579833348594

Epoch: 6| Step: 7
Training loss: 2.238905429840088
Validation loss: 2.0756950974464417

Epoch: 6| Step: 8
Training loss: 1.5225605964660645
Validation loss: 2.0893866221110025

Epoch: 6| Step: 9
Training loss: 1.7827038764953613
Validation loss: 2.0980945229530334

Epoch: 6| Step: 10
Training loss: 3.0254898071289062
Validation loss: 2.093745787938436

Epoch: 6| Step: 11
Training loss: 1.984513521194458
Validation loss: 2.0818013747533164

Epoch: 6| Step: 12
Training loss: 2.491276979446411
Validation loss: 2.0647079745928445

Epoch: 6| Step: 13
Training loss: 1.7598071098327637
Validation loss: 2.0534884532292685

Epoch: 152| Step: 0
Training loss: 1.643747091293335
Validation loss: 2.0506210724512735

Epoch: 6| Step: 1
Training loss: 1.9377453327178955
Validation loss: 2.048385818799337

Epoch: 6| Step: 2
Training loss: 1.819885015487671
Validation loss: 2.047912061214447

Epoch: 6| Step: 3
Training loss: 2.5303354263305664
Validation loss: 2.0517329772313437

Epoch: 6| Step: 4
Training loss: 2.275286912918091
Validation loss: 2.044827957948049

Epoch: 6| Step: 5
Training loss: 2.1550002098083496
Validation loss: 2.0509772102038064

Epoch: 6| Step: 6
Training loss: 1.6081311702728271
Validation loss: 2.0457190672556558

Epoch: 6| Step: 7
Training loss: 1.5830556154251099
Validation loss: 2.04379802942276

Epoch: 6| Step: 8
Training loss: 2.145334243774414
Validation loss: 2.0391406615575156

Epoch: 6| Step: 9
Training loss: 2.1391444206237793
Validation loss: 2.0299498240152993

Epoch: 6| Step: 10
Training loss: 1.6286354064941406
Validation loss: 2.0414324601491294

Epoch: 6| Step: 11
Training loss: 2.4751522541046143
Validation loss: 2.040729522705078

Epoch: 6| Step: 12
Training loss: 1.9956923723220825
Validation loss: 2.045211672782898

Epoch: 6| Step: 13
Training loss: 2.5235753059387207
Validation loss: 2.053280234336853

Epoch: 153| Step: 0
Training loss: 1.5694315433502197
Validation loss: 2.0430646737416587

Epoch: 6| Step: 1
Training loss: 2.229433536529541
Validation loss: 2.0521471897761026

Epoch: 6| Step: 2
Training loss: 1.7872960567474365
Validation loss: 2.052070120970408

Epoch: 6| Step: 3
Training loss: 2.4881038665771484
Validation loss: 2.0560685197512307

Epoch: 6| Step: 4
Training loss: 1.953852891921997
Validation loss: 2.0504462321599326

Epoch: 6| Step: 5
Training loss: 1.903993844985962
Validation loss: 2.056553522745768

Epoch: 6| Step: 6
Training loss: 1.7132009267807007
Validation loss: 2.0522116223971048

Epoch: 6| Step: 7
Training loss: 2.200669765472412
Validation loss: 2.0452833573023477

Epoch: 6| Step: 8
Training loss: 2.3532373905181885
Validation loss: 2.0533119241396585

Epoch: 6| Step: 9
Training loss: 1.033740758895874
Validation loss: 2.0511474609375

Epoch: 6| Step: 10
Training loss: 2.268439769744873
Validation loss: 2.0562556783358255

Epoch: 6| Step: 11
Training loss: 1.9039583206176758
Validation loss: 2.0556836326917014

Epoch: 6| Step: 12
Training loss: 2.635838508605957
Validation loss: 2.053948005040487

Epoch: 6| Step: 13
Training loss: 2.120407819747925
Validation loss: 2.0608197848002114

Epoch: 154| Step: 0
Training loss: 2.1280531883239746
Validation loss: 2.058094243208567

Epoch: 6| Step: 1
Training loss: 1.165516972541809
Validation loss: 2.054748813311259

Epoch: 6| Step: 2
Training loss: 1.7897889614105225
Validation loss: 2.0596541364987693

Epoch: 6| Step: 3
Training loss: 2.4468116760253906
Validation loss: 2.057864189147949

Epoch: 6| Step: 4
Training loss: 1.9058148860931396
Validation loss: 2.046059330304464

Epoch: 6| Step: 5
Training loss: 1.8927786350250244
Validation loss: 2.056125601132711

Epoch: 6| Step: 6
Training loss: 2.2612967491149902
Validation loss: 2.050355156262716

Epoch: 6| Step: 7
Training loss: 1.8553508520126343
Validation loss: 2.0435983339945474

Epoch: 6| Step: 8
Training loss: 2.5938949584960938
Validation loss: 2.059165835380554

Epoch: 6| Step: 9
Training loss: 2.0327630043029785
Validation loss: 2.036510248978933

Epoch: 6| Step: 10
Training loss: 2.360790729522705
Validation loss: 2.034955898920695

Epoch: 6| Step: 11
Training loss: 1.7914564609527588
Validation loss: 2.0336740811665854

Epoch: 6| Step: 12
Training loss: 2.0029215812683105
Validation loss: 2.035242478052775

Epoch: 6| Step: 13
Training loss: 2.023021936416626
Validation loss: 2.03821591536204

Epoch: 155| Step: 0
Training loss: 2.7313666343688965
Validation loss: 2.0420709451039634

Epoch: 6| Step: 1
Training loss: 2.1539998054504395
Validation loss: 2.041938861211141

Epoch: 6| Step: 2
Training loss: 2.2716615200042725
Validation loss: 2.0521344343821206

Epoch: 6| Step: 3
Training loss: 1.8782708644866943
Validation loss: 2.048893709977468

Epoch: 6| Step: 4
Training loss: 2.008693218231201
Validation loss: 2.050883869330088

Epoch: 6| Step: 5
Training loss: 1.078932762145996
Validation loss: 2.053311586380005

Epoch: 6| Step: 6
Training loss: 2.682131767272949
Validation loss: 2.0490585764249167

Epoch: 6| Step: 7
Training loss: 2.391902446746826
Validation loss: 2.043498158454895

Epoch: 6| Step: 8
Training loss: 2.315399169921875
Validation loss: 2.036965827147166

Epoch: 6| Step: 9
Training loss: 2.0509049892425537
Validation loss: 2.0490038990974426

Epoch: 6| Step: 10
Training loss: 1.9526445865631104
Validation loss: 2.054803490638733

Epoch: 6| Step: 11
Training loss: 1.4873688220977783
Validation loss: 2.0406219561894736

Epoch: 6| Step: 12
Training loss: 1.6155022382736206
Validation loss: 2.055355111757914

Epoch: 6| Step: 13
Training loss: 1.8364840745925903
Validation loss: 2.0469465454419455

Epoch: 156| Step: 0
Training loss: 2.230743885040283
Validation loss: 2.044591248035431

Epoch: 6| Step: 1
Training loss: 2.0039749145507812
Validation loss: 2.0527939995129905

Epoch: 6| Step: 2
Training loss: 1.823388934135437
Validation loss: 2.0451521078745523

Epoch: 6| Step: 3
Training loss: 2.1001648902893066
Validation loss: 2.0592982371648154

Epoch: 6| Step: 4
Training loss: 0.9653618335723877
Validation loss: 2.062132934729258

Epoch: 6| Step: 5
Training loss: 1.8608754873275757
Validation loss: 2.0618307987848916

Epoch: 6| Step: 6
Training loss: 2.396372079849243
Validation loss: 2.0587318936983743

Epoch: 6| Step: 7
Training loss: 2.2477736473083496
Validation loss: 2.079574922720591

Epoch: 6| Step: 8
Training loss: 2.3024542331695557
Validation loss: 2.097925841808319

Epoch: 6| Step: 9
Training loss: 2.677135705947876
Validation loss: 2.111421783765157

Epoch: 6| Step: 10
Training loss: 2.018204927444458
Validation loss: 2.116946220397949

Epoch: 6| Step: 11
Training loss: 2.409543991088867
Validation loss: 2.0814322431882224

Epoch: 6| Step: 12
Training loss: 1.4387381076812744
Validation loss: 2.066950798034668

Epoch: 6| Step: 13
Training loss: 2.3685290813446045
Validation loss: 2.0745924909909568

Epoch: 157| Step: 0
Training loss: 2.368732452392578
Validation loss: 2.0677144130071006

Epoch: 6| Step: 1
Training loss: 1.5812076330184937
Validation loss: 2.0460053086280823

Epoch: 6| Step: 2
Training loss: 1.937496542930603
Validation loss: 2.0641754269599915

Epoch: 6| Step: 3
Training loss: 1.698715329170227
Validation loss: 2.0647172927856445

Epoch: 6| Step: 4
Training loss: 1.8158994913101196
Validation loss: 2.070501724878947

Epoch: 6| Step: 5
Training loss: 2.8148374557495117
Validation loss: 2.0716764330863953

Epoch: 6| Step: 6
Training loss: 2.0218307971954346
Validation loss: 2.07509175936381

Epoch: 6| Step: 7
Training loss: 2.6622085571289062
Validation loss: 2.0715434153874717

Epoch: 6| Step: 8
Training loss: 2.17244291305542
Validation loss: 2.072060465812683

Epoch: 6| Step: 9
Training loss: 1.883253574371338
Validation loss: 2.0695411364237466

Epoch: 6| Step: 10
Training loss: 1.6758689880371094
Validation loss: 2.0626474221547446

Epoch: 6| Step: 11
Training loss: 2.0477147102355957
Validation loss: 2.060678025086721

Epoch: 6| Step: 12
Training loss: 2.619821310043335
Validation loss: 2.0559521118799844

Epoch: 6| Step: 13
Training loss: 1.8606481552124023
Validation loss: 2.0552647511164346

Epoch: 158| Step: 0
Training loss: 2.0068588256835938
Validation loss: 2.057470440864563

Epoch: 6| Step: 1
Training loss: 1.5185520648956299
Validation loss: 2.0561975042025247

Epoch: 6| Step: 2
Training loss: 2.246830940246582
Validation loss: 2.062166968981425

Epoch: 6| Step: 3
Training loss: 1.673396110534668
Validation loss: 2.077064653237661

Epoch: 6| Step: 4
Training loss: 2.0285701751708984
Validation loss: 2.0914061069488525

Epoch: 6| Step: 5
Training loss: 1.8930864334106445
Validation loss: 2.104218304157257

Epoch: 6| Step: 6
Training loss: 2.024752140045166
Validation loss: 2.10636701186498

Epoch: 6| Step: 7
Training loss: 2.5314621925354004
Validation loss: 2.1050076484680176

Epoch: 6| Step: 8
Training loss: 2.179248809814453
Validation loss: 2.089060604572296

Epoch: 6| Step: 9
Training loss: 2.49576473236084
Validation loss: 2.0910204648971558

Epoch: 6| Step: 10
Training loss: 2.1267309188842773
Validation loss: 2.075214942296346

Epoch: 6| Step: 11
Training loss: 2.083695411682129
Validation loss: 2.064118981361389

Epoch: 6| Step: 12
Training loss: 1.9565740823745728
Validation loss: 2.066593050956726

Epoch: 6| Step: 13
Training loss: 1.8562378883361816
Validation loss: 2.0596981843312583

Epoch: 159| Step: 0
Training loss: 1.8003618717193604
Validation loss: 2.055144111315409

Epoch: 6| Step: 1
Training loss: 2.005500555038452
Validation loss: 2.0504000782966614

Epoch: 6| Step: 2
Training loss: 2.2057671546936035
Validation loss: 2.0548792282740274

Epoch: 6| Step: 3
Training loss: 2.0209553241729736
Validation loss: 2.067439874013265

Epoch: 6| Step: 4
Training loss: 1.8004006147384644
Validation loss: 2.0622106989224753

Epoch: 6| Step: 5
Training loss: 2.0098860263824463
Validation loss: 2.0585562586784363

Epoch: 6| Step: 6
Training loss: 1.2970166206359863
Validation loss: 2.0667722821235657

Epoch: 6| Step: 7
Training loss: 1.9788832664489746
Validation loss: 2.0682578682899475

Epoch: 6| Step: 8
Training loss: 2.080336570739746
Validation loss: 2.068263212839762

Epoch: 6| Step: 9
Training loss: 2.367957353591919
Validation loss: 2.056720475355784

Epoch: 6| Step: 10
Training loss: 1.5756163597106934
Validation loss: 2.055094579855601

Epoch: 6| Step: 11
Training loss: 2.658358335494995
Validation loss: 2.072829782962799

Epoch: 6| Step: 12
Training loss: 2.085145950317383
Validation loss: 2.059805770715078

Epoch: 6| Step: 13
Training loss: 2.2724051475524902
Validation loss: 2.0652301708857217

Epoch: 160| Step: 0
Training loss: 1.950929880142212
Validation loss: 2.080390771230062

Epoch: 6| Step: 1
Training loss: 1.8650872707366943
Validation loss: 2.075848480065664

Epoch: 6| Step: 2
Training loss: 1.6620557308197021
Validation loss: 2.0654304027557373

Epoch: 6| Step: 3
Training loss: 2.509194850921631
Validation loss: 2.063329537709554

Epoch: 6| Step: 4
Training loss: 2.10906982421875
Validation loss: 2.05284583568573

Epoch: 6| Step: 5
Training loss: 1.6653507947921753
Validation loss: 2.0563068191210427

Epoch: 6| Step: 6
Training loss: 1.7034249305725098
Validation loss: 2.057028849919637

Epoch: 6| Step: 7
Training loss: 2.327458381652832
Validation loss: 2.058512190977732

Epoch: 6| Step: 8
Training loss: 1.9041112661361694
Validation loss: 2.0577728549639382

Epoch: 6| Step: 9
Training loss: 1.9805595874786377
Validation loss: 2.0646032094955444

Epoch: 6| Step: 10
Training loss: 2.10420298576355
Validation loss: 2.06257963180542

Epoch: 6| Step: 11
Training loss: 1.967938780784607
Validation loss: 2.066880981127421

Epoch: 6| Step: 12
Training loss: 1.9813907146453857
Validation loss: 2.0666823188463845

Epoch: 6| Step: 13
Training loss: 2.5902323722839355
Validation loss: 2.0728824536005654

Epoch: 161| Step: 0
Training loss: 2.6203255653381348
Validation loss: 2.0705565015474954

Epoch: 6| Step: 1
Training loss: 2.3953490257263184
Validation loss: 2.0716986656188965

Epoch: 6| Step: 2
Training loss: 2.087480306625366
Validation loss: 2.08886726697286

Epoch: 6| Step: 3
Training loss: 1.4911284446716309
Validation loss: 2.085428257783254

Epoch: 6| Step: 4
Training loss: 2.3065993785858154
Validation loss: 2.0771048267682395

Epoch: 6| Step: 5
Training loss: 1.7487813234329224
Validation loss: 2.072093645731608

Epoch: 6| Step: 6
Training loss: 2.02146577835083
Validation loss: 2.051363527774811

Epoch: 6| Step: 7
Training loss: 2.0079407691955566
Validation loss: 2.05226335922877

Epoch: 6| Step: 8
Training loss: 2.4342684745788574
Validation loss: 2.0492515563964844

Epoch: 6| Step: 9
Training loss: 1.4224607944488525
Validation loss: 2.043733557065328

Epoch: 6| Step: 10
Training loss: 2.0224714279174805
Validation loss: 2.0450543761253357

Epoch: 6| Step: 11
Training loss: 2.247995376586914
Validation loss: 2.0505854288736978

Epoch: 6| Step: 12
Training loss: 2.010646343231201
Validation loss: 2.0555846293767295

Epoch: 6| Step: 13
Training loss: 1.443878412246704
Validation loss: 2.062318960825602

Epoch: 162| Step: 0
Training loss: 2.593050956726074
Validation loss: 2.0518873731295266

Epoch: 6| Step: 1
Training loss: 1.4971610307693481
Validation loss: 2.059893270333608

Epoch: 6| Step: 2
Training loss: 2.134209394454956
Validation loss: 2.056275645891825

Epoch: 6| Step: 3
Training loss: 1.943498969078064
Validation loss: 2.064243415991465

Epoch: 6| Step: 4
Training loss: 2.2236809730529785
Validation loss: 2.0679855744043985

Epoch: 6| Step: 5
Training loss: 1.9384791851043701
Validation loss: 2.087982992331187

Epoch: 6| Step: 6
Training loss: 1.8906753063201904
Validation loss: 2.0728509028752646

Epoch: 6| Step: 7
Training loss: 1.9545385837554932
Validation loss: 2.078373432159424

Epoch: 6| Step: 8
Training loss: 1.772949457168579
Validation loss: 2.0840347607930503

Epoch: 6| Step: 9
Training loss: 1.4261441230773926
Validation loss: 2.0773515701293945

Epoch: 6| Step: 10
Training loss: 1.9044649600982666
Validation loss: 2.066017985343933

Epoch: 6| Step: 11
Training loss: 2.1636781692504883
Validation loss: 2.0843547582626343

Epoch: 6| Step: 12
Training loss: 1.8428664207458496
Validation loss: 2.061822017033895

Epoch: 6| Step: 13
Training loss: 2.9454445838928223
Validation loss: 2.0549054543177285

Epoch: 163| Step: 0
Training loss: 1.7300043106079102
Validation loss: 2.0702863335609436

Epoch: 6| Step: 1
Training loss: 1.8756262063980103
Validation loss: 2.0625346104303994

Epoch: 6| Step: 2
Training loss: 2.3908848762512207
Validation loss: 2.0598649382591248

Epoch: 6| Step: 3
Training loss: 1.9635577201843262
Validation loss: 2.0673011938730874

Epoch: 6| Step: 4
Training loss: 2.24554443359375
Validation loss: 2.055326839288076

Epoch: 6| Step: 5
Training loss: 1.5680747032165527
Validation loss: 2.0574952562650046

Epoch: 6| Step: 6
Training loss: 1.5972514152526855
Validation loss: 2.0663992762565613

Epoch: 6| Step: 7
Training loss: 2.0038833618164062
Validation loss: 2.0643114844957986

Epoch: 6| Step: 8
Training loss: 1.783518671989441
Validation loss: 2.0718890031178794

Epoch: 6| Step: 9
Training loss: 2.170413017272949
Validation loss: 2.0771257082621255

Epoch: 6| Step: 10
Training loss: 2.3955907821655273
Validation loss: 2.0957980155944824

Epoch: 6| Step: 11
Training loss: 2.388150930404663
Validation loss: 2.101778487364451

Epoch: 6| Step: 12
Training loss: 1.8761249780654907
Validation loss: 2.090690831343333

Epoch: 6| Step: 13
Training loss: 2.3849008083343506
Validation loss: 2.120695730050405

Epoch: 164| Step: 0
Training loss: 1.997819185256958
Validation loss: 2.118158757686615

Epoch: 6| Step: 1
Training loss: 2.337338924407959
Validation loss: 2.1054144700368247

Epoch: 6| Step: 2
Training loss: 1.9538614749908447
Validation loss: 2.123686969280243

Epoch: 6| Step: 3
Training loss: 1.6164319515228271
Validation loss: 2.1210667292277017

Epoch: 6| Step: 4
Training loss: 1.5596330165863037
Validation loss: 2.1238845785458884

Epoch: 6| Step: 5
Training loss: 2.0638179779052734
Validation loss: 2.096382737159729

Epoch: 6| Step: 6
Training loss: 2.2773070335388184
Validation loss: 2.0936516523361206

Epoch: 6| Step: 7
Training loss: 1.5866265296936035
Validation loss: 2.083297391732534

Epoch: 6| Step: 8
Training loss: 1.887230634689331
Validation loss: 2.0793928503990173

Epoch: 6| Step: 9
Training loss: 2.4900095462799072
Validation loss: 2.0674794912338257

Epoch: 6| Step: 10
Training loss: 1.5369641780853271
Validation loss: 2.072273095448812

Epoch: 6| Step: 11
Training loss: 3.1341805458068848
Validation loss: 2.068781793117523

Epoch: 6| Step: 12
Training loss: 2.171786308288574
Validation loss: 2.0651552081108093

Epoch: 6| Step: 13
Training loss: 1.630538821220398
Validation loss: 2.0614766478538513

Epoch: 165| Step: 0
Training loss: 1.5822820663452148
Validation loss: 2.0655571023623147

Epoch: 6| Step: 1
Training loss: 2.4117536544799805
Validation loss: 2.0710068543752036

Epoch: 6| Step: 2
Training loss: 2.0748982429504395
Validation loss: 2.0679598251978555

Epoch: 6| Step: 3
Training loss: 2.01263165473938
Validation loss: 2.06379101673762

Epoch: 6| Step: 4
Training loss: 2.274871349334717
Validation loss: 2.0689271887143454

Epoch: 6| Step: 5
Training loss: 1.9481825828552246
Validation loss: 2.07747217019399

Epoch: 6| Step: 6
Training loss: 1.7425620555877686
Validation loss: 2.0681568582852683

Epoch: 6| Step: 7
Training loss: 2.19893741607666
Validation loss: 2.0734757582346597

Epoch: 6| Step: 8
Training loss: 1.5420646667480469
Validation loss: 2.0647907654444375

Epoch: 6| Step: 9
Training loss: 2.1479763984680176
Validation loss: 2.0662479996681213

Epoch: 6| Step: 10
Training loss: 1.9018096923828125
Validation loss: 2.063134769598643

Epoch: 6| Step: 11
Training loss: 1.7833234071731567
Validation loss: 2.0692824920018515

Epoch: 6| Step: 12
Training loss: 2.574866771697998
Validation loss: 2.0661624868710837

Epoch: 6| Step: 13
Training loss: 1.7483640909194946
Validation loss: 2.076114614804586

Epoch: 166| Step: 0
Training loss: 1.8551082611083984
Validation loss: 2.0727658867836

Epoch: 6| Step: 1
Training loss: 1.5370923280715942
Validation loss: 2.071573495864868

Epoch: 6| Step: 2
Training loss: 1.5159302949905396
Validation loss: 2.095694979031881

Epoch: 6| Step: 3
Training loss: 1.6921370029449463
Validation loss: 2.0786202351252236

Epoch: 6| Step: 4
Training loss: 2.136931896209717
Validation loss: 2.0854321916898093

Epoch: 6| Step: 5
Training loss: 2.299941062927246
Validation loss: 2.092202385266622

Epoch: 6| Step: 6
Training loss: 2.6055073738098145
Validation loss: 2.0925195018450418

Epoch: 6| Step: 7
Training loss: 1.6765987873077393
Validation loss: 2.0821043054262796

Epoch: 6| Step: 8
Training loss: 2.1017322540283203
Validation loss: 2.0817848841349282

Epoch: 6| Step: 9
Training loss: 1.8786020278930664
Validation loss: 2.083098808924357

Epoch: 6| Step: 10
Training loss: 1.9558230638504028
Validation loss: 2.0798447529474893

Epoch: 6| Step: 11
Training loss: 2.27040696144104
Validation loss: 2.072608232498169

Epoch: 6| Step: 12
Training loss: 2.0413427352905273
Validation loss: 2.0750410556793213

Epoch: 6| Step: 13
Training loss: 2.4055800437927246
Validation loss: 2.076189716657003

Epoch: 167| Step: 0
Training loss: 1.7858083248138428
Validation loss: 2.068451941013336

Epoch: 6| Step: 1
Training loss: 1.817716121673584
Validation loss: 2.073305348555247

Epoch: 6| Step: 2
Training loss: 1.6202175617218018
Validation loss: 2.0733304023742676

Epoch: 6| Step: 3
Training loss: 2.2017741203308105
Validation loss: 2.081288476785024

Epoch: 6| Step: 4
Training loss: 1.8321185111999512
Validation loss: 2.083499093850454

Epoch: 6| Step: 5
Training loss: 1.9416463375091553
Validation loss: 2.1058528423309326

Epoch: 6| Step: 6
Training loss: 2.250298023223877
Validation loss: 2.1069550116856894

Epoch: 6| Step: 7
Training loss: 2.6247754096984863
Validation loss: 2.1020103295644126

Epoch: 6| Step: 8
Training loss: 1.6599376201629639
Validation loss: 2.08926252524058

Epoch: 6| Step: 9
Training loss: 1.6315633058547974
Validation loss: 2.0881014466285706

Epoch: 6| Step: 10
Training loss: 1.7872002124786377
Validation loss: 2.062749445438385

Epoch: 6| Step: 11
Training loss: 2.2532577514648438
Validation loss: 2.0642531712849936

Epoch: 6| Step: 12
Training loss: 1.6451172828674316
Validation loss: 2.0719364086786904

Epoch: 6| Step: 13
Training loss: 3.029486656188965
Validation loss: 2.0554314057032266

Epoch: 168| Step: 0
Training loss: 2.0178003311157227
Validation loss: 2.0650523702303567

Epoch: 6| Step: 1
Training loss: 1.254495620727539
Validation loss: 2.054438610871633

Epoch: 6| Step: 2
Training loss: 2.346130847930908
Validation loss: 2.0581485430399575

Epoch: 6| Step: 3
Training loss: 2.3189759254455566
Validation loss: 2.05477774143219

Epoch: 6| Step: 4
Training loss: 2.6580140590667725
Validation loss: 2.05802192290624

Epoch: 6| Step: 5
Training loss: 2.119643211364746
Validation loss: 2.062559803326925

Epoch: 6| Step: 6
Training loss: 1.9085493087768555
Validation loss: 2.0621508955955505

Epoch: 6| Step: 7
Training loss: 2.0214271545410156
Validation loss: 2.078817049662272

Epoch: 6| Step: 8
Training loss: 1.4615525007247925
Validation loss: 2.064550300439199

Epoch: 6| Step: 9
Training loss: 2.4332098960876465
Validation loss: 2.071290453275045

Epoch: 6| Step: 10
Training loss: 1.5407960414886475
Validation loss: 2.07332444190979

Epoch: 6| Step: 11
Training loss: 2.1936545372009277
Validation loss: 2.056296726067861

Epoch: 6| Step: 12
Training loss: 1.7210701704025269
Validation loss: 2.070796032746633

Epoch: 6| Step: 13
Training loss: 2.0092427730560303
Validation loss: 2.0668452382087708

Epoch: 169| Step: 0
Training loss: 1.3385155200958252
Validation loss: 2.074362337589264

Epoch: 6| Step: 1
Training loss: 2.1933093070983887
Validation loss: 2.0731389919916787

Epoch: 6| Step: 2
Training loss: 1.466309666633606
Validation loss: 2.085105299949646

Epoch: 6| Step: 3
Training loss: 2.2867321968078613
Validation loss: 2.073896825313568

Epoch: 6| Step: 4
Training loss: 2.1568775177001953
Validation loss: 2.0635517636934915

Epoch: 6| Step: 5
Training loss: 1.8495559692382812
Validation loss: 2.056169410546621

Epoch: 6| Step: 6
Training loss: 1.8200323581695557
Validation loss: 2.0616766611735025

Epoch: 6| Step: 7
Training loss: 2.8681399822235107
Validation loss: 2.0441927512486777

Epoch: 6| Step: 8
Training loss: 2.027146816253662
Validation loss: 2.051537652810415

Epoch: 6| Step: 9
Training loss: 2.1563897132873535
Validation loss: 2.0550614396731057

Epoch: 6| Step: 10
Training loss: 2.369292736053467
Validation loss: 2.052308996518453

Epoch: 6| Step: 11
Training loss: 2.2424795627593994
Validation loss: 2.059644957383474

Epoch: 6| Step: 12
Training loss: 1.713202953338623
Validation loss: 2.0557791789372764

Epoch: 6| Step: 13
Training loss: 1.7655489444732666
Validation loss: 2.054606099923452

Epoch: 170| Step: 0
Training loss: 1.7328996658325195
Validation loss: 2.0539154609044394

Epoch: 6| Step: 1
Training loss: 2.280296802520752
Validation loss: 2.0544759233792624

Epoch: 6| Step: 2
Training loss: 1.538635015487671
Validation loss: 2.0521242221196494

Epoch: 6| Step: 3
Training loss: 2.676236629486084
Validation loss: 2.055409053961436

Epoch: 6| Step: 4
Training loss: 2.7339954376220703
Validation loss: 2.0632432103157043

Epoch: 6| Step: 5
Training loss: 1.8944811820983887
Validation loss: 2.0730950633684793

Epoch: 6| Step: 6
Training loss: 1.9628729820251465
Validation loss: 2.0806884765625

Epoch: 6| Step: 7
Training loss: 2.1561386585235596
Validation loss: 2.0747249325116477

Epoch: 6| Step: 8
Training loss: 1.9822304248809814
Validation loss: 2.078307032585144

Epoch: 6| Step: 9
Training loss: 1.6919515132904053
Validation loss: 2.0746289690335593

Epoch: 6| Step: 10
Training loss: 1.987821102142334
Validation loss: 2.07320108016332

Epoch: 6| Step: 11
Training loss: 1.480035662651062
Validation loss: 2.078596810499827

Epoch: 6| Step: 12
Training loss: 1.9808306694030762
Validation loss: 2.0899198849995932

Epoch: 6| Step: 13
Training loss: 1.6934611797332764
Validation loss: 2.1024108131726584

Epoch: 171| Step: 0
Training loss: 1.4688968658447266
Validation loss: 2.0920122067133584

Epoch: 6| Step: 1
Training loss: 2.1548829078674316
Validation loss: 2.0829239885012307

Epoch: 6| Step: 2
Training loss: 1.3738124370574951
Validation loss: 2.080749770005544

Epoch: 6| Step: 3
Training loss: 1.9839985370635986
Validation loss: 2.0817968050638833

Epoch: 6| Step: 4
Training loss: 2.018481969833374
Validation loss: 2.085906128088633

Epoch: 6| Step: 5
Training loss: 2.338214874267578
Validation loss: 2.07368266582489

Epoch: 6| Step: 6
Training loss: 1.9442477226257324
Validation loss: 2.070088028907776

Epoch: 6| Step: 7
Training loss: 2.4553000926971436
Validation loss: 2.056683977444967

Epoch: 6| Step: 8
Training loss: 1.7981836795806885
Validation loss: 2.0690380533536277

Epoch: 6| Step: 9
Training loss: 1.695268154144287
Validation loss: 2.0726135969161987

Epoch: 6| Step: 10
Training loss: 2.5992650985717773
Validation loss: 2.0694138010342917

Epoch: 6| Step: 11
Training loss: 2.2424726486206055
Validation loss: 2.0783244371414185

Epoch: 6| Step: 12
Training loss: 2.0287163257598877
Validation loss: 2.065266569455465

Epoch: 6| Step: 13
Training loss: 2.5560731887817383
Validation loss: 2.074173172314962

Epoch: 172| Step: 0
Training loss: 1.6814496517181396
Validation loss: 2.063247561454773

Epoch: 6| Step: 1
Training loss: 1.751892328262329
Validation loss: 2.056836783885956

Epoch: 6| Step: 2
Training loss: 2.2174606323242188
Validation loss: 2.0557660659154258

Epoch: 6| Step: 3
Training loss: 1.628151535987854
Validation loss: 2.0713440974553428

Epoch: 6| Step: 4
Training loss: 1.939687967300415
Validation loss: 2.0532795985539756

Epoch: 6| Step: 5
Training loss: 2.17330265045166
Validation loss: 2.0622042417526245

Epoch: 6| Step: 6
Training loss: 1.5406054258346558
Validation loss: 2.062628130118052

Epoch: 6| Step: 7
Training loss: 1.3671553134918213
Validation loss: 2.0796270767847695

Epoch: 6| Step: 8
Training loss: 1.6149380207061768
Validation loss: 2.081850508848826

Epoch: 6| Step: 9
Training loss: 2.7899246215820312
Validation loss: 2.0959628423055015

Epoch: 6| Step: 10
Training loss: 2.241048812866211
Validation loss: 2.0980148315429688

Epoch: 6| Step: 11
Training loss: 2.7242088317871094
Validation loss: 2.089511533578237

Epoch: 6| Step: 12
Training loss: 2.451785087585449
Validation loss: 2.0827422936757407

Epoch: 6| Step: 13
Training loss: 2.285367965698242
Validation loss: 2.0770501295725503

Epoch: 173| Step: 0
Training loss: 1.6660265922546387
Validation loss: 2.063293695449829

Epoch: 6| Step: 1
Training loss: 1.8243472576141357
Validation loss: 2.0679321885108948

Epoch: 6| Step: 2
Training loss: 2.0971107482910156
Validation loss: 2.0459521810213723

Epoch: 6| Step: 3
Training loss: 2.594313621520996
Validation loss: 2.0581613779067993

Epoch: 6| Step: 4
Training loss: 2.122019052505493
Validation loss: 2.058412949244181

Epoch: 6| Step: 5
Training loss: 1.6933854818344116
Validation loss: 2.0493905544281006

Epoch: 6| Step: 6
Training loss: 1.7613221406936646
Validation loss: 2.0705538789431253

Epoch: 6| Step: 7
Training loss: 2.2215933799743652
Validation loss: 2.067811131477356

Epoch: 6| Step: 8
Training loss: 2.3185958862304688
Validation loss: 2.0690558354059854

Epoch: 6| Step: 9
Training loss: 1.83085036277771
Validation loss: 2.07686177889506

Epoch: 6| Step: 10
Training loss: 1.9258191585540771
Validation loss: 2.06028683980306

Epoch: 6| Step: 11
Training loss: 1.7967557907104492
Validation loss: 2.0738288164138794

Epoch: 6| Step: 12
Training loss: 1.5020830631256104
Validation loss: 2.07597949107488

Epoch: 6| Step: 13
Training loss: 2.435988664627075
Validation loss: 2.0810432036717734

Epoch: 174| Step: 0
Training loss: 1.8871492147445679
Validation loss: 2.0708109935124717

Epoch: 6| Step: 1
Training loss: 0.9044904112815857
Validation loss: 2.063648998737335

Epoch: 6| Step: 2
Training loss: 1.9770872592926025
Validation loss: 2.069342772165934

Epoch: 6| Step: 3
Training loss: 2.024329662322998
Validation loss: 2.0679556727409363

Epoch: 6| Step: 4
Training loss: 2.3662352561950684
Validation loss: 2.065735856691996

Epoch: 6| Step: 5
Training loss: 1.623866081237793
Validation loss: 2.0631931026776633

Epoch: 6| Step: 6
Training loss: 2.3095719814300537
Validation loss: 2.0652587612469993

Epoch: 6| Step: 7
Training loss: 1.9195358753204346
Validation loss: 2.0693451166152954

Epoch: 6| Step: 8
Training loss: 2.171870708465576
Validation loss: 2.0674227277437844

Epoch: 6| Step: 9
Training loss: 2.033336877822876
Validation loss: 2.0622675816218057

Epoch: 6| Step: 10
Training loss: 2.226414680480957
Validation loss: 2.070403059323629

Epoch: 6| Step: 11
Training loss: 2.3832194805145264
Validation loss: 2.061604599157969

Epoch: 6| Step: 12
Training loss: 1.966731309890747
Validation loss: 2.062075674533844

Epoch: 6| Step: 13
Training loss: 1.6244909763336182
Validation loss: 2.0588671366373696

Epoch: 175| Step: 0
Training loss: 1.8505046367645264
Validation loss: 2.068813065687815

Epoch: 6| Step: 1
Training loss: 1.4120968580245972
Validation loss: 2.0726165572802224

Epoch: 6| Step: 2
Training loss: 2.605748176574707
Validation loss: 2.075676222642263

Epoch: 6| Step: 3
Training loss: 1.789635419845581
Validation loss: 2.077724119027456

Epoch: 6| Step: 4
Training loss: 2.343790292739868
Validation loss: 2.063060541947683

Epoch: 6| Step: 5
Training loss: 1.7752556800842285
Validation loss: 2.077705363432566

Epoch: 6| Step: 6
Training loss: 2.57443904876709
Validation loss: 2.0943893988927207

Epoch: 6| Step: 7
Training loss: 2.0760767459869385
Validation loss: 2.0854392846425376

Epoch: 6| Step: 8
Training loss: 1.6219052076339722
Validation loss: 2.0827900568644204

Epoch: 6| Step: 9
Training loss: 1.9339449405670166
Validation loss: 2.096672256787618

Epoch: 6| Step: 10
Training loss: 1.9535572528839111
Validation loss: 2.0983974734942117

Epoch: 6| Step: 11
Training loss: 2.0300867557525635
Validation loss: 2.1057602763175964

Epoch: 6| Step: 12
Training loss: 1.2720011472702026
Validation loss: 2.099508285522461

Epoch: 6| Step: 13
Training loss: 2.1895875930786133
Validation loss: 2.1340017914772034

Epoch: 176| Step: 0
Training loss: 1.555079698562622
Validation loss: 2.1392468015352883

Epoch: 6| Step: 1
Training loss: 1.7561110258102417
Validation loss: 2.12594997882843

Epoch: 6| Step: 2
Training loss: 2.3558218479156494
Validation loss: 2.116616110006968

Epoch: 6| Step: 3
Training loss: 2.641446352005005
Validation loss: 2.0888566772143045

Epoch: 6| Step: 4
Training loss: 2.084099769592285
Validation loss: 2.083500385284424

Epoch: 6| Step: 5
Training loss: 1.9020659923553467
Validation loss: 2.0865172743797302

Epoch: 6| Step: 6
Training loss: 2.6646318435668945
Validation loss: 2.0736472606658936

Epoch: 6| Step: 7
Training loss: 1.3823919296264648
Validation loss: 2.074769298235575

Epoch: 6| Step: 8
Training loss: 1.615163803100586
Validation loss: 2.0768304467201233

Epoch: 6| Step: 9
Training loss: 1.6822502613067627
Validation loss: 2.066675345102946

Epoch: 6| Step: 10
Training loss: 2.2332029342651367
Validation loss: 2.075713058312734

Epoch: 6| Step: 11
Training loss: 1.8751370906829834
Validation loss: 2.0750743548075357

Epoch: 6| Step: 12
Training loss: 1.9618706703186035
Validation loss: 2.075679143269857

Epoch: 6| Step: 13
Training loss: 1.8980700969696045
Validation loss: 2.0760276714960733

Epoch: 177| Step: 0
Training loss: 1.7828452587127686
Validation loss: 2.0718855460484824

Epoch: 6| Step: 1
Training loss: 2.2632575035095215
Validation loss: 2.0755764047304788

Epoch: 6| Step: 2
Training loss: 1.925176978111267
Validation loss: 2.076647957166036

Epoch: 6| Step: 3
Training loss: 1.4765048027038574
Validation loss: 2.0762548446655273

Epoch: 6| Step: 4
Training loss: 2.260847806930542
Validation loss: 2.0847057501475015

Epoch: 6| Step: 5
Training loss: 2.3245692253112793
Validation loss: 2.078192432721456

Epoch: 6| Step: 6
Training loss: 1.7774226665496826
Validation loss: 2.0778112014134726

Epoch: 6| Step: 7
Training loss: 1.6152124404907227
Validation loss: 2.087063809235891

Epoch: 6| Step: 8
Training loss: 2.9598283767700195
Validation loss: 2.0836283564567566

Epoch: 6| Step: 9
Training loss: 1.7087337970733643
Validation loss: 2.096981922785441

Epoch: 6| Step: 10
Training loss: 1.5945881605148315
Validation loss: 2.110488494237264

Epoch: 6| Step: 11
Training loss: 2.3868308067321777
Validation loss: 2.1101391514142356

Epoch: 6| Step: 12
Training loss: 1.4213427305221558
Validation loss: 2.1120375593503318

Epoch: 6| Step: 13
Training loss: 1.9142218828201294
Validation loss: 2.1151490608851113

Epoch: 178| Step: 0
Training loss: 1.8094298839569092
Validation loss: 2.087575157483419

Epoch: 6| Step: 1
Training loss: 2.2669849395751953
Validation loss: 2.0992006063461304

Epoch: 6| Step: 2
Training loss: 1.694535493850708
Validation loss: 2.077585538228353

Epoch: 6| Step: 3
Training loss: 2.11956524848938
Validation loss: 2.071441868940989

Epoch: 6| Step: 4
Training loss: 1.3809293508529663
Validation loss: 2.0668007135391235

Epoch: 6| Step: 5
Training loss: 2.0445656776428223
Validation loss: 2.0771464308102927

Epoch: 6| Step: 6
Training loss: 1.5426433086395264
Validation loss: 2.0699310302734375

Epoch: 6| Step: 7
Training loss: 1.845534086227417
Validation loss: 2.0762850244839988

Epoch: 6| Step: 8
Training loss: 3.108910083770752
Validation loss: 2.0817110737164817

Epoch: 6| Step: 9
Training loss: 2.0047290325164795
Validation loss: 2.080097218354543

Epoch: 6| Step: 10
Training loss: 1.7895729541778564
Validation loss: 2.0923433105150857

Epoch: 6| Step: 11
Training loss: 1.1657588481903076
Validation loss: 2.088045577208201

Epoch: 6| Step: 12
Training loss: 2.19502854347229
Validation loss: 2.092596928278605

Epoch: 6| Step: 13
Training loss: 2.325596809387207
Validation loss: 2.066043754418691

Epoch: 179| Step: 0
Training loss: 2.2589550018310547
Validation loss: 2.0896958907445273

Epoch: 6| Step: 1
Training loss: 2.6308093070983887
Validation loss: 2.103837172190348

Epoch: 6| Step: 2
Training loss: 1.653694987297058
Validation loss: 2.0975394248962402

Epoch: 6| Step: 3
Training loss: 1.728636384010315
Validation loss: 2.1082374850908914

Epoch: 6| Step: 4
Training loss: 2.1225814819335938
Validation loss: 2.101953148841858

Epoch: 6| Step: 5
Training loss: 1.4203753471374512
Validation loss: 2.1153356234232583

Epoch: 6| Step: 6
Training loss: 1.8558186292648315
Validation loss: 2.1069790919621787

Epoch: 6| Step: 7
Training loss: 1.6734224557876587
Validation loss: 2.105856239795685

Epoch: 6| Step: 8
Training loss: 1.8080934286117554
Validation loss: 2.102232257525126

Epoch: 6| Step: 9
Training loss: 2.0453085899353027
Validation loss: 2.0955833991368613

Epoch: 6| Step: 10
Training loss: 1.7958149909973145
Validation loss: 2.104783773422241

Epoch: 6| Step: 11
Training loss: 1.7741360664367676
Validation loss: 2.0902046958605447

Epoch: 6| Step: 12
Training loss: 2.656245231628418
Validation loss: 2.0907674630482993

Epoch: 6| Step: 13
Training loss: 1.839314341545105
Validation loss: 2.0953197677930198

Epoch: 180| Step: 0
Training loss: 2.220489501953125
Validation loss: 2.092061539491018

Epoch: 6| Step: 1
Training loss: 2.7284040451049805
Validation loss: 2.0707746346791587

Epoch: 6| Step: 2
Training loss: 2.613186836242676
Validation loss: 2.09123961130778

Epoch: 6| Step: 3
Training loss: 1.6630711555480957
Validation loss: 2.0760034720102944

Epoch: 6| Step: 4
Training loss: 1.3328073024749756
Validation loss: 2.0794044137001038

Epoch: 6| Step: 5
Training loss: 1.9926373958587646
Validation loss: 2.0822999477386475

Epoch: 6| Step: 6
Training loss: 1.3799185752868652
Validation loss: 2.1080294052759805

Epoch: 6| Step: 7
Training loss: 1.793394923210144
Validation loss: 2.1067258715629578

Epoch: 6| Step: 8
Training loss: 2.236562728881836
Validation loss: 2.1093719005584717

Epoch: 6| Step: 9
Training loss: 1.8553907871246338
Validation loss: 2.1073086659113565

Epoch: 6| Step: 10
Training loss: 2.379415988922119
Validation loss: 2.1064506371816

Epoch: 6| Step: 11
Training loss: 1.6912922859191895
Validation loss: 2.1201639970143638

Epoch: 6| Step: 12
Training loss: 1.9059935808181763
Validation loss: 2.111856698989868

Epoch: 6| Step: 13
Training loss: 1.7868475914001465
Validation loss: 2.1151068011919656

Epoch: 181| Step: 0
Training loss: 1.6812551021575928
Validation loss: 2.114807446797689

Epoch: 6| Step: 1
Training loss: 2.0996856689453125
Validation loss: 2.117958883444468

Epoch: 6| Step: 2
Training loss: 1.8196985721588135
Validation loss: 2.1051549911499023

Epoch: 6| Step: 3
Training loss: 1.8272500038146973
Validation loss: 2.1006046732266745

Epoch: 6| Step: 4
Training loss: 2.1628336906433105
Validation loss: 2.089717964331309

Epoch: 6| Step: 5
Training loss: 1.866150140762329
Validation loss: 2.0831883549690247

Epoch: 6| Step: 6
Training loss: 2.03659725189209
Validation loss: 2.0854939222335815

Epoch: 6| Step: 7
Training loss: 1.9671958684921265
Validation loss: 2.0951128204663596

Epoch: 6| Step: 8
Training loss: 1.8916902542114258
Validation loss: 2.096114178498586

Epoch: 6| Step: 9
Training loss: 1.4941898584365845
Validation loss: 2.0937902132670083

Epoch: 6| Step: 10
Training loss: 3.1473536491394043
Validation loss: 2.086322327454885

Epoch: 6| Step: 11
Training loss: 1.714522361755371
Validation loss: 2.0977848172187805

Epoch: 6| Step: 12
Training loss: 2.0789918899536133
Validation loss: 2.072769065697988

Epoch: 6| Step: 13
Training loss: 1.3460018634796143
Validation loss: 2.0975860555966697

Epoch: 182| Step: 0
Training loss: 2.3005356788635254
Validation loss: 2.103898545106252

Epoch: 6| Step: 1
Training loss: 2.2171518802642822
Validation loss: 2.1001465121905007

Epoch: 6| Step: 2
Training loss: 2.6455345153808594
Validation loss: 2.1103488206863403

Epoch: 6| Step: 3
Training loss: 2.7919774055480957
Validation loss: 2.117407659689585

Epoch: 6| Step: 4
Training loss: 1.893938660621643
Validation loss: 2.1215713222821555

Epoch: 6| Step: 5
Training loss: 1.6509435176849365
Validation loss: 2.1135541200637817

Epoch: 6| Step: 6
Training loss: 1.4537572860717773
Validation loss: 2.102580467859904

Epoch: 6| Step: 7
Training loss: 1.7214279174804688
Validation loss: 2.077489177385966

Epoch: 6| Step: 8
Training loss: 1.4874980449676514
Validation loss: 2.0819225708643594

Epoch: 6| Step: 9
Training loss: 1.7265280485153198
Validation loss: 2.0688489079475403

Epoch: 6| Step: 10
Training loss: 1.9047497510910034
Validation loss: 2.0746869246164956

Epoch: 6| Step: 11
Training loss: 2.2807517051696777
Validation loss: 2.0742850303649902

Epoch: 6| Step: 12
Training loss: 1.8568809032440186
Validation loss: 2.0673884948094687

Epoch: 6| Step: 13
Training loss: 1.8408474922180176
Validation loss: 2.069058875242869

Epoch: 183| Step: 0
Training loss: 1.800445795059204
Validation loss: 2.072726011276245

Epoch: 6| Step: 1
Training loss: 2.2699320316314697
Validation loss: 2.078733662764231

Epoch: 6| Step: 2
Training loss: 1.2994027137756348
Validation loss: 2.0861817995707193

Epoch: 6| Step: 3
Training loss: 2.2428746223449707
Validation loss: 2.086593965689341

Epoch: 6| Step: 4
Training loss: 1.847713828086853
Validation loss: 2.0608837405840554

Epoch: 6| Step: 5
Training loss: 1.313685417175293
Validation loss: 2.076630194981893

Epoch: 6| Step: 6
Training loss: 2.5081701278686523
Validation loss: 2.0917873779932656

Epoch: 6| Step: 7
Training loss: 2.220327377319336
Validation loss: 2.0958677530288696

Epoch: 6| Step: 8
Training loss: 2.0017945766448975
Validation loss: 2.11936354637146

Epoch: 6| Step: 9
Training loss: 1.7051819562911987
Validation loss: 2.1162436803181968

Epoch: 6| Step: 10
Training loss: 2.005403518676758
Validation loss: 2.0966097116470337

Epoch: 6| Step: 11
Training loss: 1.8064963817596436
Validation loss: 2.09260626633962

Epoch: 6| Step: 12
Training loss: 2.4383151531219482
Validation loss: 2.103513995806376

Epoch: 6| Step: 13
Training loss: 1.7699800729751587
Validation loss: 2.097130835056305

Epoch: 184| Step: 0
Training loss: 2.362901210784912
Validation loss: 2.089524964491526

Epoch: 6| Step: 1
Training loss: 2.3083038330078125
Validation loss: 2.0868293245633445

Epoch: 6| Step: 2
Training loss: 2.870828151702881
Validation loss: 2.0945701201756797

Epoch: 6| Step: 3
Training loss: 1.2346404790878296
Validation loss: 2.091543992360433

Epoch: 6| Step: 4
Training loss: 1.4108469486236572
Validation loss: 2.097617983818054

Epoch: 6| Step: 5
Training loss: 1.867628812789917
Validation loss: 2.0903762181599936

Epoch: 6| Step: 6
Training loss: 2.0165462493896484
Validation loss: 2.0989677906036377

Epoch: 6| Step: 7
Training loss: 2.182452440261841
Validation loss: 2.102722982565562

Epoch: 6| Step: 8
Training loss: 1.445637583732605
Validation loss: 2.0823413729667664

Epoch: 6| Step: 9
Training loss: 1.7627006769180298
Validation loss: 2.086085081100464

Epoch: 6| Step: 10
Training loss: 2.1153500080108643
Validation loss: 2.1026446421941123

Epoch: 6| Step: 11
Training loss: 1.6745383739471436
Validation loss: 2.106474757194519

Epoch: 6| Step: 12
Training loss: 2.269099235534668
Validation loss: 2.1118170817693076

Epoch: 6| Step: 13
Training loss: 1.5143483877182007
Validation loss: 2.0973917643229165

Epoch: 185| Step: 0
Training loss: 1.9147427082061768
Validation loss: 2.102845788002014

Epoch: 6| Step: 1
Training loss: 1.6073646545410156
Validation loss: 2.0927611192067466

Epoch: 6| Step: 2
Training loss: 2.32070255279541
Validation loss: 2.0978516936302185

Epoch: 6| Step: 3
Training loss: 1.5861730575561523
Validation loss: 2.089261809984843

Epoch: 6| Step: 4
Training loss: 1.73507821559906
Validation loss: 2.097740729649862

Epoch: 6| Step: 5
Training loss: 1.6541001796722412
Validation loss: 2.082506080468496

Epoch: 6| Step: 6
Training loss: 1.9951605796813965
Validation loss: 2.0925525029500327

Epoch: 6| Step: 7
Training loss: 1.7190824747085571
Validation loss: 2.0938907464345298

Epoch: 6| Step: 8
Training loss: 2.6371800899505615
Validation loss: 2.0990841388702393

Epoch: 6| Step: 9
Training loss: 2.568540334701538
Validation loss: 2.091030399004618

Epoch: 6| Step: 10
Training loss: 1.6594018936157227
Validation loss: 2.116040130456289

Epoch: 6| Step: 11
Training loss: 1.8888440132141113
Validation loss: 2.0854047934214273

Epoch: 6| Step: 12
Training loss: 1.7853204011917114
Validation loss: 2.0976408322652182

Epoch: 6| Step: 13
Training loss: 1.8278958797454834
Validation loss: 2.093813419342041

Epoch: 186| Step: 0
Training loss: 1.553377628326416
Validation loss: 2.1156453688939414

Epoch: 6| Step: 1
Training loss: 1.9512152671813965
Validation loss: 2.1121265292167664

Epoch: 6| Step: 2
Training loss: 2.643339157104492
Validation loss: 2.090900997320811

Epoch: 6| Step: 3
Training loss: 1.5123376846313477
Validation loss: 2.1078503330548606

Epoch: 6| Step: 4
Training loss: 2.1052913665771484
Validation loss: 2.1093525290489197

Epoch: 6| Step: 5
Training loss: 1.428134799003601
Validation loss: 2.10548859834671

Epoch: 6| Step: 6
Training loss: 2.242990016937256
Validation loss: 2.0973770221074424

Epoch: 6| Step: 7
Training loss: 1.5509769916534424
Validation loss: 2.0971436500549316

Epoch: 6| Step: 8
Training loss: 1.5064780712127686
Validation loss: 2.100009580453237

Epoch: 6| Step: 9
Training loss: 1.9368153810501099
Validation loss: 2.113719920317332

Epoch: 6| Step: 10
Training loss: 2.616783857345581
Validation loss: 2.1041948795318604

Epoch: 6| Step: 11
Training loss: 2.5891520977020264
Validation loss: 2.114586273829142

Epoch: 6| Step: 12
Training loss: 1.8785924911499023
Validation loss: 2.107895016670227

Epoch: 6| Step: 13
Training loss: 1.3574631214141846
Validation loss: 2.1358931064605713

Epoch: 187| Step: 0
Training loss: 2.2353999614715576
Validation loss: 2.1447503765424094

Epoch: 6| Step: 1
Training loss: 1.996323823928833
Validation loss: 2.132908880710602

Epoch: 6| Step: 2
Training loss: 2.186171531677246
Validation loss: 2.0998257398605347

Epoch: 6| Step: 3
Training loss: 1.6763486862182617
Validation loss: 2.092928330103556

Epoch: 6| Step: 4
Training loss: 1.728991150856018
Validation loss: 2.0928799708684287

Epoch: 6| Step: 5
Training loss: 1.623775839805603
Validation loss: 2.0928128759066262

Epoch: 6| Step: 6
Training loss: 2.2808175086975098
Validation loss: 2.0869068702061973

Epoch: 6| Step: 7
Training loss: 1.9772484302520752
Validation loss: 2.080869495868683

Epoch: 6| Step: 8
Training loss: 2.55234432220459
Validation loss: 2.085229456424713

Epoch: 6| Step: 9
Training loss: 1.5751020908355713
Validation loss: 2.0805612206459045

Epoch: 6| Step: 10
Training loss: 1.6707639694213867
Validation loss: 2.0853524605433145

Epoch: 6| Step: 11
Training loss: 2.3675971031188965
Validation loss: 2.078484853108724

Epoch: 6| Step: 12
Training loss: 2.095885753631592
Validation loss: 2.0808154741923013

Epoch: 6| Step: 13
Training loss: 2.134603500366211
Validation loss: 2.093174775441488

Epoch: 188| Step: 0
Training loss: 1.9124619960784912
Validation loss: 2.0928450425465903

Epoch: 6| Step: 1
Training loss: 2.1400673389434814
Validation loss: 2.1030508875846863

Epoch: 6| Step: 2
Training loss: 1.401687741279602
Validation loss: 2.096987803777059

Epoch: 6| Step: 3
Training loss: 1.6920478343963623
Validation loss: 2.0928044517834983

Epoch: 6| Step: 4
Training loss: 2.0291662216186523
Validation loss: 2.1028000911076865

Epoch: 6| Step: 5
Training loss: 1.809829831123352
Validation loss: 2.101103107134501

Epoch: 6| Step: 6
Training loss: 1.9156841039657593
Validation loss: 2.0996147990226746

Epoch: 6| Step: 7
Training loss: 1.9281823635101318
Validation loss: 2.0985401471455893

Epoch: 6| Step: 8
Training loss: 1.4619817733764648
Validation loss: 2.1033982833226523

Epoch: 6| Step: 9
Training loss: 1.9270946979522705
Validation loss: 2.1060196359952292

Epoch: 6| Step: 10
Training loss: 2.757957935333252
Validation loss: 2.0940391421318054

Epoch: 6| Step: 11
Training loss: 2.2504541873931885
Validation loss: 2.1112480958302817

Epoch: 6| Step: 12
Training loss: 1.9149839878082275
Validation loss: 2.097947637240092

Epoch: 6| Step: 13
Training loss: 1.630985140800476
Validation loss: 2.101398686567942

Epoch: 189| Step: 0
Training loss: 1.816265344619751
Validation loss: 2.118572016557058

Epoch: 6| Step: 1
Training loss: 1.7529691457748413
Validation loss: 2.1208974917729697

Epoch: 6| Step: 2
Training loss: 2.2343037128448486
Validation loss: 2.1400658090909324

Epoch: 6| Step: 3
Training loss: 1.8509492874145508
Validation loss: 2.136692484219869

Epoch: 6| Step: 4
Training loss: 2.795149803161621
Validation loss: 2.1371713479359946

Epoch: 6| Step: 5
Training loss: 2.2645010948181152
Validation loss: 2.1353523333867392

Epoch: 6| Step: 6
Training loss: 2.1066060066223145
Validation loss: 2.1190624634424844

Epoch: 6| Step: 7
Training loss: 2.175021171569824
Validation loss: 2.0992846488952637

Epoch: 6| Step: 8
Training loss: 1.9276903867721558
Validation loss: 2.113960385322571

Epoch: 6| Step: 9
Training loss: 1.4254391193389893
Validation loss: 2.1034867564837136

Epoch: 6| Step: 10
Training loss: 2.0962038040161133
Validation loss: 2.098104238510132

Epoch: 6| Step: 11
Training loss: 1.361518144607544
Validation loss: 2.0841197768847146

Epoch: 6| Step: 12
Training loss: 1.431820273399353
Validation loss: 2.110222657521566

Epoch: 6| Step: 13
Training loss: 2.0839309692382812
Validation loss: 2.100375215212504

Epoch: 190| Step: 0
Training loss: 1.7509005069732666
Validation loss: 2.1082987785339355

Epoch: 6| Step: 1
Training loss: 2.41300892829895
Validation loss: 2.1096922556559243

Epoch: 6| Step: 2
Training loss: 1.9144337177276611
Validation loss: 2.112987995147705

Epoch: 6| Step: 3
Training loss: 1.2791156768798828
Validation loss: 2.1098366181055703

Epoch: 6| Step: 4
Training loss: 2.1663615703582764
Validation loss: 2.111190696557363

Epoch: 6| Step: 5
Training loss: 2.587197780609131
Validation loss: 2.1319601933161416

Epoch: 6| Step: 6
Training loss: 1.5390491485595703
Validation loss: 2.1222050388654075

Epoch: 6| Step: 7
Training loss: 1.7216448783874512
Validation loss: 2.1152138312657676

Epoch: 6| Step: 8
Training loss: 1.622604250907898
Validation loss: 2.0953799883524575

Epoch: 6| Step: 9
Training loss: 2.3198466300964355
Validation loss: 2.113405187924703

Epoch: 6| Step: 10
Training loss: 1.8030112981796265
Validation loss: 2.1042245626449585

Epoch: 6| Step: 11
Training loss: 1.6865469217300415
Validation loss: 2.102647066116333

Epoch: 6| Step: 12
Training loss: 1.8456929922103882
Validation loss: 2.0963056484858194

Epoch: 6| Step: 13
Training loss: 2.2273190021514893
Validation loss: 2.102910041809082

Epoch: 191| Step: 0
Training loss: 1.2434108257293701
Validation loss: 2.093597650527954

Epoch: 6| Step: 1
Training loss: 1.9996213912963867
Validation loss: 2.085853656133016

Epoch: 6| Step: 2
Training loss: 1.6812260150909424
Validation loss: 2.0940974752108255

Epoch: 6| Step: 3
Training loss: 2.1470465660095215
Validation loss: 2.0880801677703857

Epoch: 6| Step: 4
Training loss: 1.6065300703048706
Validation loss: 2.0947285095850625

Epoch: 6| Step: 5
Training loss: 1.7537906169891357
Validation loss: 2.0842163960138955

Epoch: 6| Step: 6
Training loss: 2.117762327194214
Validation loss: 2.1014005541801453

Epoch: 6| Step: 7
Training loss: 1.6749672889709473
Validation loss: 2.0987118085225425

Epoch: 6| Step: 8
Training loss: 1.7908921241760254
Validation loss: 2.0901750922203064

Epoch: 6| Step: 9
Training loss: 2.350905418395996
Validation loss: 2.1146517197291055

Epoch: 6| Step: 10
Training loss: 1.6161768436431885
Validation loss: 2.109897494316101

Epoch: 6| Step: 11
Training loss: 2.70100736618042
Validation loss: 2.099685808022817

Epoch: 6| Step: 12
Training loss: 2.1988160610198975
Validation loss: 2.105684002240499

Epoch: 6| Step: 13
Training loss: 2.014146566390991
Validation loss: 2.106539249420166

Epoch: 192| Step: 0
Training loss: 2.1402392387390137
Validation loss: 2.1159579157829285

Epoch: 6| Step: 1
Training loss: 1.7453985214233398
Validation loss: 2.109738051891327

Epoch: 6| Step: 2
Training loss: 1.8856897354125977
Validation loss: 2.115445097287496

Epoch: 6| Step: 3
Training loss: 2.324296474456787
Validation loss: 2.1034998496373496

Epoch: 6| Step: 4
Training loss: 2.601339817047119
Validation loss: 2.110197643438975

Epoch: 6| Step: 5
Training loss: 1.8565864562988281
Validation loss: 2.102915326754252

Epoch: 6| Step: 6
Training loss: 2.1048150062561035
Validation loss: 2.1038343707720437

Epoch: 6| Step: 7
Training loss: 1.6637043952941895
Validation loss: 2.101585547129313

Epoch: 6| Step: 8
Training loss: 2.095954418182373
Validation loss: 2.1194881200790405

Epoch: 6| Step: 9
Training loss: 2.268360137939453
Validation loss: 2.1242850025494895

Epoch: 6| Step: 10
Training loss: 1.3751707077026367
Validation loss: 2.115011910597483

Epoch: 6| Step: 11
Training loss: 1.413257360458374
Validation loss: 2.137865980466207

Epoch: 6| Step: 12
Training loss: 1.833064079284668
Validation loss: 2.1324251095453897

Epoch: 6| Step: 13
Training loss: 1.6024560928344727
Validation loss: 2.122019608815511

Epoch: 193| Step: 0
Training loss: 1.5845296382904053
Validation loss: 2.138466954231262

Epoch: 6| Step: 1
Training loss: 1.8220880031585693
Validation loss: 2.1371580759684243

Epoch: 6| Step: 2
Training loss: 2.1214139461517334
Validation loss: 2.121276875336965

Epoch: 6| Step: 3
Training loss: 1.6797459125518799
Validation loss: 2.109481473763784

Epoch: 6| Step: 4
Training loss: 2.000056743621826
Validation loss: 2.1078739364941916

Epoch: 6| Step: 5
Training loss: 1.954298496246338
Validation loss: 2.089166005452474

Epoch: 6| Step: 6
Training loss: 2.199462890625
Validation loss: 2.0878190994262695

Epoch: 6| Step: 7
Training loss: 1.6229865550994873
Validation loss: 2.0855338176091514

Epoch: 6| Step: 8
Training loss: 2.2727720737457275
Validation loss: 2.0779176553090415

Epoch: 6| Step: 9
Training loss: 1.5831844806671143
Validation loss: 2.0897913376490274

Epoch: 6| Step: 10
Training loss: 2.353210926055908
Validation loss: 2.0892778237660727

Epoch: 6| Step: 11
Training loss: 1.827149748802185
Validation loss: 2.0797305901845298

Epoch: 6| Step: 12
Training loss: 1.9604387283325195
Validation loss: 2.0932349960009256

Epoch: 6| Step: 13
Training loss: 2.398153781890869
Validation loss: 2.0889289180437722

Epoch: 194| Step: 0
Training loss: 2.3965930938720703
Validation loss: 2.0983614325523376

Epoch: 6| Step: 1
Training loss: 1.6013330221176147
Validation loss: 2.0939441323280334

Epoch: 6| Step: 2
Training loss: 1.6324915885925293
Validation loss: 2.114114999771118

Epoch: 6| Step: 3
Training loss: 1.2570111751556396
Validation loss: 2.1170323888460794

Epoch: 6| Step: 4
Training loss: 1.34836745262146
Validation loss: 2.1153151194254556

Epoch: 6| Step: 5
Training loss: 2.3301377296447754
Validation loss: 2.1084435184796653

Epoch: 6| Step: 6
Training loss: 2.042296886444092
Validation loss: 2.0903995037078857

Epoch: 6| Step: 7
Training loss: 1.916657567024231
Validation loss: 2.112696349620819

Epoch: 6| Step: 8
Training loss: 2.1578307151794434
Validation loss: 2.1078659296035767

Epoch: 6| Step: 9
Training loss: 1.859432339668274
Validation loss: 2.1060607035954795

Epoch: 6| Step: 10
Training loss: 2.663684606552124
Validation loss: 2.097672939300537

Epoch: 6| Step: 11
Training loss: 1.5813143253326416
Validation loss: 2.0892043113708496

Epoch: 6| Step: 12
Training loss: 1.8929731845855713
Validation loss: 2.1046089331309

Epoch: 6| Step: 13
Training loss: 1.7992051839828491
Validation loss: 2.1000730792681375

Epoch: 195| Step: 0
Training loss: 2.0463318824768066
Validation loss: 2.1015557448069253

Epoch: 6| Step: 1
Training loss: 1.8863768577575684
Validation loss: 2.094625155131022

Epoch: 6| Step: 2
Training loss: 1.9258781671524048
Validation loss: 2.096859892209371

Epoch: 6| Step: 3
Training loss: 1.8155436515808105
Validation loss: 2.1059865951538086

Epoch: 6| Step: 4
Training loss: 1.6673542261123657
Validation loss: 2.107141137123108

Epoch: 6| Step: 5
Training loss: 1.3396713733673096
Validation loss: 2.1169790029525757

Epoch: 6| Step: 6
Training loss: 1.5394563674926758
Validation loss: 2.110334575176239

Epoch: 6| Step: 7
Training loss: 2.4343905448913574
Validation loss: 2.1116637190183005

Epoch: 6| Step: 8
Training loss: 1.910438895225525
Validation loss: 2.1021906534830728

Epoch: 6| Step: 9
Training loss: 1.5735207796096802
Validation loss: 2.1036185224850974

Epoch: 6| Step: 10
Training loss: 2.537262439727783
Validation loss: 2.1151138742764792

Epoch: 6| Step: 11
Training loss: 1.7742314338684082
Validation loss: 2.1004231174786887

Epoch: 6| Step: 12
Training loss: 2.1512668132781982
Validation loss: 2.099808096885681

Epoch: 6| Step: 13
Training loss: 2.214416742324829
Validation loss: 2.0906852881113687

Epoch: 196| Step: 0
Training loss: 2.7388763427734375
Validation loss: 2.092442512512207

Epoch: 6| Step: 1
Training loss: 1.3567116260528564
Validation loss: 2.1046228408813477

Epoch: 6| Step: 2
Training loss: 1.6068646907806396
Validation loss: 2.1164004802703857

Epoch: 6| Step: 3
Training loss: 2.0985379219055176
Validation loss: 2.0968974033991494

Epoch: 6| Step: 4
Training loss: 1.8147549629211426
Validation loss: 2.11293355623881

Epoch: 6| Step: 5
Training loss: 1.6598496437072754
Validation loss: 2.110287666320801

Epoch: 6| Step: 6
Training loss: 2.1099133491516113
Validation loss: 2.124457319577535

Epoch: 6| Step: 7
Training loss: 2.3522989749908447
Validation loss: 2.111714164415995

Epoch: 6| Step: 8
Training loss: 1.7552759647369385
Validation loss: 2.1209530433019004

Epoch: 6| Step: 9
Training loss: 2.3346030712127686
Validation loss: 2.1130846738815308

Epoch: 6| Step: 10
Training loss: 1.7406333684921265
Validation loss: 2.109928846359253

Epoch: 6| Step: 11
Training loss: 1.4504027366638184
Validation loss: 2.112520456314087

Epoch: 6| Step: 12
Training loss: 1.7650415897369385
Validation loss: 2.131083846092224

Epoch: 6| Step: 13
Training loss: 1.7815959453582764
Validation loss: 2.1567154924074807

Epoch: 197| Step: 0
Training loss: 1.84467351436615
Validation loss: 2.1769614020983377

Epoch: 6| Step: 1
Training loss: 1.9865434169769287
Validation loss: 2.150601784388224

Epoch: 6| Step: 2
Training loss: 1.2654259204864502
Validation loss: 2.155801276365916

Epoch: 6| Step: 3
Training loss: 1.8722033500671387
Validation loss: 2.1348127524058023

Epoch: 6| Step: 4
Training loss: 1.718186378479004
Validation loss: 2.0998710791269937

Epoch: 6| Step: 5
Training loss: 2.1549296379089355
Validation loss: 2.097992459932963

Epoch: 6| Step: 6
Training loss: 2.3842251300811768
Validation loss: 2.094168225924174

Epoch: 6| Step: 7
Training loss: 1.8590972423553467
Validation loss: 2.107093552748362

Epoch: 6| Step: 8
Training loss: 1.7119336128234863
Validation loss: 2.079872409502665

Epoch: 6| Step: 9
Training loss: 1.8496737480163574
Validation loss: 2.0854729215304055

Epoch: 6| Step: 10
Training loss: 1.5108084678649902
Validation loss: 2.092792590459188

Epoch: 6| Step: 11
Training loss: 2.2113394737243652
Validation loss: 2.0823620557785034

Epoch: 6| Step: 12
Training loss: 1.8905539512634277
Validation loss: 2.1018255949020386

Epoch: 6| Step: 13
Training loss: 2.894740104675293
Validation loss: 2.094636003176371

Epoch: 198| Step: 0
Training loss: 1.6027991771697998
Validation loss: 2.097145358721415

Epoch: 6| Step: 1
Training loss: 2.1487579345703125
Validation loss: 2.113093098004659

Epoch: 6| Step: 2
Training loss: 2.155755043029785
Validation loss: 2.112890640894572

Epoch: 6| Step: 3
Training loss: 2.0111594200134277
Validation loss: 2.13028218348821

Epoch: 6| Step: 4
Training loss: 1.7884125709533691
Validation loss: 2.1480247577031455

Epoch: 6| Step: 5
Training loss: 2.0216140747070312
Validation loss: 2.139392892519633

Epoch: 6| Step: 6
Training loss: 1.6041970252990723
Validation loss: 2.108432173728943

Epoch: 6| Step: 7
Training loss: 2.031982898712158
Validation loss: 2.1122610370318093

Epoch: 6| Step: 8
Training loss: 2.263942003250122
Validation loss: 2.1109861532847085

Epoch: 6| Step: 9
Training loss: 2.2942545413970947
Validation loss: 2.104446073373159

Epoch: 6| Step: 10
Training loss: 1.5020601749420166
Validation loss: 2.099201500415802

Epoch: 6| Step: 11
Training loss: 1.746607780456543
Validation loss: 2.0863290230433145

Epoch: 6| Step: 12
Training loss: 1.8948419094085693
Validation loss: 2.0852792263031006

Epoch: 6| Step: 13
Training loss: 2.0192344188690186
Validation loss: 2.0870453317960105

Epoch: 199| Step: 0
Training loss: 1.7645114660263062
Validation loss: 2.086581230163574

Epoch: 6| Step: 1
Training loss: 1.4828643798828125
Validation loss: 2.0910326838493347

Epoch: 6| Step: 2
Training loss: 1.4837948083877563
Validation loss: 2.081690013408661

Epoch: 6| Step: 3
Training loss: 3.201202154159546
Validation loss: 2.0920242269833884

Epoch: 6| Step: 4
Training loss: 1.5814049243927002
Validation loss: 2.092395782470703

Epoch: 6| Step: 5
Training loss: 1.6265679597854614
Validation loss: 2.0905822118123374

Epoch: 6| Step: 6
Training loss: 1.7699180841445923
Validation loss: 2.089051524798075

Epoch: 6| Step: 7
Training loss: 1.6866066455841064
Validation loss: 2.1016147136688232

Epoch: 6| Step: 8
Training loss: 2.132305383682251
Validation loss: 2.0894704262415567

Epoch: 6| Step: 9
Training loss: 1.7102549076080322
Validation loss: 2.0939063231150308

Epoch: 6| Step: 10
Training loss: 2.3356003761291504
Validation loss: 2.0948389768600464

Epoch: 6| Step: 11
Training loss: 1.8670358657836914
Validation loss: 2.108560800552368

Epoch: 6| Step: 12
Training loss: 1.9751332998275757
Validation loss: 2.1282073656717935

Epoch: 6| Step: 13
Training loss: 2.2023258209228516
Validation loss: 2.12704203526179

Epoch: 200| Step: 0
Training loss: 1.3865153789520264
Validation loss: 2.1326003074645996

Epoch: 6| Step: 1
Training loss: 1.6825449466705322
Validation loss: 2.1153431336085

Epoch: 6| Step: 2
Training loss: 1.9532665014266968
Validation loss: 2.1158875624338784

Epoch: 6| Step: 3
Training loss: 2.4620611667633057
Validation loss: 2.124795377254486

Epoch: 6| Step: 4
Training loss: 1.5755414962768555
Validation loss: 2.1181494990984597

Epoch: 6| Step: 5
Training loss: 1.6839078664779663
Validation loss: 2.1257968743642173

Epoch: 6| Step: 6
Training loss: 1.7304167747497559
Validation loss: 2.1183069348335266

Epoch: 6| Step: 7
Training loss: 2.0223770141601562
Validation loss: 2.11070450146993

Epoch: 6| Step: 8
Training loss: 1.7825089693069458
Validation loss: 2.1123767693837485

Epoch: 6| Step: 9
Training loss: 2.249183177947998
Validation loss: 2.1051250100135803

Epoch: 6| Step: 10
Training loss: 1.7961704730987549
Validation loss: 2.0968613227208457

Epoch: 6| Step: 11
Training loss: 2.0965616703033447
Validation loss: 2.1110742489496865

Epoch: 6| Step: 12
Training loss: 2.009645462036133
Validation loss: 2.1049488186836243

Epoch: 6| Step: 13
Training loss: 2.0361857414245605
Validation loss: 2.1041895151138306

Epoch: 201| Step: 0
Training loss: 2.2198033332824707
Validation loss: 2.10950775941213

Epoch: 6| Step: 1
Training loss: 1.9847971200942993
Validation loss: 2.112601896127065

Epoch: 6| Step: 2
Training loss: 2.2105798721313477
Validation loss: 2.109129031499227

Epoch: 6| Step: 3
Training loss: 1.9793837070465088
Validation loss: 2.1162177522977195

Epoch: 6| Step: 4
Training loss: 2.0699639320373535
Validation loss: 2.109301428000132

Epoch: 6| Step: 5
Training loss: 1.7546961307525635
Validation loss: 2.1156624952952066

Epoch: 6| Step: 6
Training loss: 2.7336583137512207
Validation loss: 2.1245292027791343

Epoch: 6| Step: 7
Training loss: 1.395280361175537
Validation loss: 2.1166348854700723

Epoch: 6| Step: 8
Training loss: 2.038827896118164
Validation loss: 2.124118427435557

Epoch: 6| Step: 9
Training loss: 1.9702949523925781
Validation loss: 2.144661804040273

Epoch: 6| Step: 10
Training loss: 1.497902512550354
Validation loss: 2.138052483399709

Epoch: 6| Step: 11
Training loss: 1.0788301229476929
Validation loss: 2.136666735013326

Epoch: 6| Step: 12
Training loss: 1.4859294891357422
Validation loss: 2.119179129600525

Epoch: 6| Step: 13
Training loss: 2.04154372215271
Validation loss: 2.1168681184450784

Epoch: 202| Step: 0
Training loss: 2.2178220748901367
Validation loss: 2.1050116221110025

Epoch: 6| Step: 1
Training loss: 1.3687951564788818
Validation loss: 2.1083603700002036

Epoch: 6| Step: 2
Training loss: 1.9567747116088867
Validation loss: 2.1161459485689798

Epoch: 6| Step: 3
Training loss: 1.6338355541229248
Validation loss: 2.1130176981290183

Epoch: 6| Step: 4
Training loss: 1.9289225339889526
Validation loss: 2.116845111052195

Epoch: 6| Step: 5
Training loss: 1.5228266716003418
Validation loss: 2.113177716732025

Epoch: 6| Step: 6
Training loss: 2.841494083404541
Validation loss: 2.1204753319422402

Epoch: 6| Step: 7
Training loss: 2.041128635406494
Validation loss: 2.113562226295471

Epoch: 6| Step: 8
Training loss: 1.153990626335144
Validation loss: 2.1319660743077598

Epoch: 6| Step: 9
Training loss: 1.4433112144470215
Validation loss: 2.10286815961202

Epoch: 6| Step: 10
Training loss: 3.1679539680480957
Validation loss: 2.1200592120488486

Epoch: 6| Step: 11
Training loss: 1.2487871646881104
Validation loss: 2.109477420647939

Epoch: 6| Step: 12
Training loss: 1.7899514436721802
Validation loss: 2.1238908767700195

Epoch: 6| Step: 13
Training loss: 1.9729914665222168
Validation loss: 2.1303087870279946

Epoch: 203| Step: 0
Training loss: 1.2776501178741455
Validation loss: 2.125973403453827

Epoch: 6| Step: 1
Training loss: 1.795454978942871
Validation loss: 2.1403170824050903

Epoch: 6| Step: 2
Training loss: 2.0306015014648438
Validation loss: 2.1354637145996094

Epoch: 6| Step: 3
Training loss: 2.03956937789917
Validation loss: 2.1290944814682007

Epoch: 6| Step: 4
Training loss: 2.4450035095214844
Validation loss: 2.150589108467102

Epoch: 6| Step: 5
Training loss: 2.176894426345825
Validation loss: 2.147702634334564

Epoch: 6| Step: 6
Training loss: 2.2723512649536133
Validation loss: 2.150996287663778

Epoch: 6| Step: 7
Training loss: 2.018404960632324
Validation loss: 2.1368499795595803

Epoch: 6| Step: 8
Training loss: 1.6162512302398682
Validation loss: 2.115761935710907

Epoch: 6| Step: 9
Training loss: 1.5042203664779663
Validation loss: 2.097089727719625

Epoch: 6| Step: 10
Training loss: 2.0204663276672363
Validation loss: 2.0967243909835815

Epoch: 6| Step: 11
Training loss: 1.7127060890197754
Validation loss: 2.093668599923452

Epoch: 6| Step: 12
Training loss: 1.3022328615188599
Validation loss: 2.1005592147509256

Epoch: 6| Step: 13
Training loss: 2.104478359222412
Validation loss: 2.116228997707367

Epoch: 204| Step: 0
Training loss: 1.9129486083984375
Validation loss: 2.1045717199643454

Epoch: 6| Step: 1
Training loss: 1.4180092811584473
Validation loss: 2.109845817089081

Epoch: 6| Step: 2
Training loss: 2.193286895751953
Validation loss: 2.0930707653363547

Epoch: 6| Step: 3
Training loss: 2.069681406021118
Validation loss: 2.1266155441602073

Epoch: 6| Step: 4
Training loss: 2.1781792640686035
Validation loss: 2.113542079925537

Epoch: 6| Step: 5
Training loss: 1.8255575895309448
Validation loss: 2.1483496824900308

Epoch: 6| Step: 6
Training loss: 1.3304107189178467
Validation loss: 2.1509154240290322

Epoch: 6| Step: 7
Training loss: 1.9033268690109253
Validation loss: 2.132344047228495

Epoch: 6| Step: 8
Training loss: 1.4349005222320557
Validation loss: 2.126215140024821

Epoch: 6| Step: 9
Training loss: 2.4828243255615234
Validation loss: 2.121078093846639

Epoch: 6| Step: 10
Training loss: 1.4786653518676758
Validation loss: 2.111310044924418

Epoch: 6| Step: 11
Training loss: 1.651644229888916
Validation loss: 2.1102243065834045

Epoch: 6| Step: 12
Training loss: 1.8760290145874023
Validation loss: 2.123184541861216

Epoch: 6| Step: 13
Training loss: 2.1032254695892334
Validation loss: 2.1226593057314553

Epoch: 205| Step: 0
Training loss: 2.248319625854492
Validation loss: 2.116567591826121

Epoch: 6| Step: 1
Training loss: 1.3810560703277588
Validation loss: 2.125696361064911

Epoch: 6| Step: 2
Training loss: 2.1456332206726074
Validation loss: 2.113140801588694

Epoch: 6| Step: 3
Training loss: 2.0715596675872803
Validation loss: 2.1256351470947266

Epoch: 6| Step: 4
Training loss: 1.684828519821167
Validation loss: 2.122278650601705

Epoch: 6| Step: 5
Training loss: 2.2625198364257812
Validation loss: 2.131950239340464

Epoch: 6| Step: 6
Training loss: 1.4128602743148804
Validation loss: 2.1243657072385154

Epoch: 6| Step: 7
Training loss: 1.8247978687286377
Validation loss: 2.1112785736719766

Epoch: 6| Step: 8
Training loss: 1.8141003847122192
Validation loss: 2.1183661421140036

Epoch: 6| Step: 9
Training loss: 2.011343240737915
Validation loss: 2.1305010120073953

Epoch: 6| Step: 10
Training loss: 1.482831358909607
Validation loss: 2.141517460346222

Epoch: 6| Step: 11
Training loss: 2.2598750591278076
Validation loss: 2.12498531738917

Epoch: 6| Step: 12
Training loss: 1.7187120914459229
Validation loss: 2.125860412915548

Epoch: 6| Step: 13
Training loss: 1.7635600566864014
Validation loss: 2.1467480262120566

Epoch: 206| Step: 0
Training loss: 1.83677077293396
Validation loss: 2.1330753167470298

Epoch: 6| Step: 1
Training loss: 1.4909213781356812
Validation loss: 2.112541675567627

Epoch: 6| Step: 2
Training loss: 1.5156798362731934
Validation loss: 2.1247148911158242

Epoch: 6| Step: 3
Training loss: 1.7064626216888428
Validation loss: 2.1185259222984314

Epoch: 6| Step: 4
Training loss: 1.9057226181030273
Validation loss: 2.1233852903048196

Epoch: 6| Step: 5
Training loss: 2.076368808746338
Validation loss: 2.132185618082682

Epoch: 6| Step: 6
Training loss: 1.4571659564971924
Validation loss: 2.1228202978769937

Epoch: 6| Step: 7
Training loss: 2.3720767498016357
Validation loss: 2.114763637383779

Epoch: 6| Step: 8
Training loss: 1.522702932357788
Validation loss: 2.1216188073158264

Epoch: 6| Step: 9
Training loss: 2.71907114982605
Validation loss: 2.125077505906423

Epoch: 6| Step: 10
Training loss: 1.9720885753631592
Validation loss: 2.119385302066803

Epoch: 6| Step: 11
Training loss: 2.5459327697753906
Validation loss: 2.117462158203125

Epoch: 6| Step: 12
Training loss: 1.292949914932251
Validation loss: 2.125328282515208

Epoch: 6| Step: 13
Training loss: 1.5001602172851562
Validation loss: 2.125012695789337

Epoch: 207| Step: 0
Training loss: 1.4099371433258057
Validation loss: 2.1121920545895896

Epoch: 6| Step: 1
Training loss: 1.9952852725982666
Validation loss: 2.121798018614451

Epoch: 6| Step: 2
Training loss: 2.3163182735443115
Validation loss: 2.1196055809656777

Epoch: 6| Step: 3
Training loss: 1.5755889415740967
Validation loss: 2.120966056982676

Epoch: 6| Step: 4
Training loss: 1.5984814167022705
Validation loss: 2.1032365957895913

Epoch: 6| Step: 5
Training loss: 2.3779821395874023
Validation loss: 2.1164071559906006

Epoch: 6| Step: 6
Training loss: 1.6460208892822266
Validation loss: 2.1210464239120483

Epoch: 6| Step: 7
Training loss: 2.6828949451446533
Validation loss: 2.118898034095764

Epoch: 6| Step: 8
Training loss: 2.2927253246307373
Validation loss: 2.1164480447769165

Epoch: 6| Step: 9
Training loss: 1.4956638813018799
Validation loss: 2.116754492123922

Epoch: 6| Step: 10
Training loss: 2.304710865020752
Validation loss: 2.109721601009369

Epoch: 6| Step: 11
Training loss: 1.9105149507522583
Validation loss: 2.112975060939789

Epoch: 6| Step: 12
Training loss: 1.468733310699463
Validation loss: 2.12091988325119

Epoch: 6| Step: 13
Training loss: 1.2569817304611206
Validation loss: 2.1319716771443686

Epoch: 208| Step: 0
Training loss: 2.1617860794067383
Validation loss: 2.1266757249832153

Epoch: 6| Step: 1
Training loss: 1.5874621868133545
Validation loss: 2.1253251830736795

Epoch: 6| Step: 2
Training loss: 1.784278392791748
Validation loss: 2.1525890827178955

Epoch: 6| Step: 3
Training loss: 2.070240020751953
Validation loss: 2.1524826288223267

Epoch: 6| Step: 4
Training loss: 2.5369300842285156
Validation loss: 2.174447496732076

Epoch: 6| Step: 5
Training loss: 1.8123934268951416
Validation loss: 2.157394548257192

Epoch: 6| Step: 6
Training loss: 2.0872344970703125
Validation loss: 2.148166596889496

Epoch: 6| Step: 7
Training loss: 1.1128814220428467
Validation loss: 2.153012235959371

Epoch: 6| Step: 8
Training loss: 1.4007755517959595
Validation loss: 2.1555782357851663

Epoch: 6| Step: 9
Training loss: 1.7869648933410645
Validation loss: 2.1449842850367227

Epoch: 6| Step: 10
Training loss: 2.146292209625244
Validation loss: 2.1418896118799844

Epoch: 6| Step: 11
Training loss: 1.1484026908874512
Validation loss: 2.1389426986376443

Epoch: 6| Step: 12
Training loss: 1.9977543354034424
Validation loss: 2.1445310910542807

Epoch: 6| Step: 13
Training loss: 2.0886635780334473
Validation loss: 2.134672443072001

Epoch: 209| Step: 0
Training loss: 1.863226056098938
Validation loss: 2.1541560888290405

Epoch: 6| Step: 1
Training loss: 1.1724720001220703
Validation loss: 2.1402583718299866

Epoch: 6| Step: 2
Training loss: 1.7703945636749268
Validation loss: 2.1470643480618796

Epoch: 6| Step: 3
Training loss: 2.018312692642212
Validation loss: 2.133793671925863

Epoch: 6| Step: 4
Training loss: 1.4154108762741089
Validation loss: 2.1255505681037903

Epoch: 6| Step: 5
Training loss: 2.425471067428589
Validation loss: 2.1498317321141562

Epoch: 6| Step: 6
Training loss: 1.3792190551757812
Validation loss: 2.167575021584829

Epoch: 6| Step: 7
Training loss: 1.871332049369812
Validation loss: 2.1380022366841636

Epoch: 6| Step: 8
Training loss: 1.913833737373352
Validation loss: 2.1662957668304443

Epoch: 6| Step: 9
Training loss: 2.2500386238098145
Validation loss: 2.1419474482536316

Epoch: 6| Step: 10
Training loss: 2.294613838195801
Validation loss: 2.1191980640093484

Epoch: 6| Step: 11
Training loss: 1.899430513381958
Validation loss: 2.1276545325915017

Epoch: 6| Step: 12
Training loss: 1.9980288743972778
Validation loss: 2.1381937066713967

Epoch: 6| Step: 13
Training loss: 1.7085092067718506
Validation loss: 2.1405073404312134

Epoch: 210| Step: 0
Training loss: 1.6444580554962158
Validation loss: 2.135819951693217

Epoch: 6| Step: 1
Training loss: 1.9329979419708252
Validation loss: 2.1174851258595786

Epoch: 6| Step: 2
Training loss: 1.7731411457061768
Validation loss: 2.1309837897618613

Epoch: 6| Step: 3
Training loss: 1.105661153793335
Validation loss: 2.137341856956482

Epoch: 6| Step: 4
Training loss: 1.9941104650497437
Validation loss: 2.129618446032206

Epoch: 6| Step: 5
Training loss: 1.5924837589263916
Validation loss: 2.1186744570732117

Epoch: 6| Step: 6
Training loss: 1.7311326265335083
Validation loss: 2.1243465344111123

Epoch: 6| Step: 7
Training loss: 1.2663062810897827
Validation loss: 2.114743173122406

Epoch: 6| Step: 8
Training loss: 2.1297194957733154
Validation loss: 2.1077083746592202

Epoch: 6| Step: 9
Training loss: 2.1951136589050293
Validation loss: 2.1168978412946067

Epoch: 6| Step: 10
Training loss: 2.679388999938965
Validation loss: 2.1122319300969443

Epoch: 6| Step: 11
Training loss: 2.4547696113586426
Validation loss: 2.113711933294932

Epoch: 6| Step: 12
Training loss: 1.826088786125183
Validation loss: 2.105496962865194

Epoch: 6| Step: 13
Training loss: 1.36870539188385
Validation loss: 2.117771943410238

Epoch: 211| Step: 0
Training loss: 1.6553997993469238
Validation loss: 2.114153027534485

Epoch: 6| Step: 1
Training loss: 1.8658579587936401
Validation loss: 2.124554177125295

Epoch: 6| Step: 2
Training loss: 1.450976014137268
Validation loss: 2.1292943557103476

Epoch: 6| Step: 3
Training loss: 2.963052749633789
Validation loss: 2.129591623942057

Epoch: 6| Step: 4
Training loss: 1.5271317958831787
Validation loss: 2.142182389895121

Epoch: 6| Step: 5
Training loss: 1.5464262962341309
Validation loss: 2.1300302743911743

Epoch: 6| Step: 6
Training loss: 2.8958816528320312
Validation loss: 2.1526743173599243

Epoch: 6| Step: 7
Training loss: 1.8787667751312256
Validation loss: 2.1675803462664285

Epoch: 6| Step: 8
Training loss: 1.747698187828064
Validation loss: 2.186512072881063

Epoch: 6| Step: 9
Training loss: 1.6973867416381836
Validation loss: 2.1874154011408486

Epoch: 6| Step: 10
Training loss: 1.6656336784362793
Validation loss: 2.2119396726290383

Epoch: 6| Step: 11
Training loss: 1.9109282493591309
Validation loss: 2.1727786858876548

Epoch: 6| Step: 12
Training loss: 1.3962173461914062
Validation loss: 2.1477903723716736

Epoch: 6| Step: 13
Training loss: 1.7973791360855103
Validation loss: 2.1433095932006836

Epoch: 212| Step: 0
Training loss: 1.9489049911499023
Validation loss: 2.1358596682548523

Epoch: 6| Step: 1
Training loss: 1.366502046585083
Validation loss: 2.132269024848938

Epoch: 6| Step: 2
Training loss: 1.5253655910491943
Validation loss: 2.141760687033335

Epoch: 6| Step: 3
Training loss: 2.3390674591064453
Validation loss: 2.124539097150167

Epoch: 6| Step: 4
Training loss: 1.4557182788848877
Validation loss: 2.1267619927724204

Epoch: 6| Step: 5
Training loss: 1.7794290781021118
Validation loss: 2.1209837992986045

Epoch: 6| Step: 6
Training loss: 2.125429630279541
Validation loss: 2.124773303667704

Epoch: 6| Step: 7
Training loss: 1.9762742519378662
Validation loss: 2.1489599347114563

Epoch: 6| Step: 8
Training loss: 1.5865602493286133
Validation loss: 2.1375378171602883

Epoch: 6| Step: 9
Training loss: 1.882911205291748
Validation loss: 2.1577678124109902

Epoch: 6| Step: 10
Training loss: 2.2467405796051025
Validation loss: 2.1448090275128684

Epoch: 6| Step: 11
Training loss: 2.1427760124206543
Validation loss: 2.1565973361333213

Epoch: 6| Step: 12
Training loss: 2.2006847858428955
Validation loss: 2.1369829376538596

Epoch: 6| Step: 13
Training loss: 1.9581416845321655
Validation loss: 2.144457141558329

Epoch: 213| Step: 0
Training loss: 1.2670767307281494
Validation loss: 2.1432263255119324

Epoch: 6| Step: 1
Training loss: 1.6773996353149414
Validation loss: 2.1358095010121665

Epoch: 6| Step: 2
Training loss: 1.6097149848937988
Validation loss: 2.137960652510325

Epoch: 6| Step: 3
Training loss: 1.887582540512085
Validation loss: 2.1276729106903076

Epoch: 6| Step: 4
Training loss: 1.3591614961624146
Validation loss: 2.1202142039934793

Epoch: 6| Step: 5
Training loss: 1.9024406671524048
Validation loss: 2.11471418539683

Epoch: 6| Step: 6
Training loss: 2.057216167449951
Validation loss: 2.1213007966677346

Epoch: 6| Step: 7
Training loss: 2.0049052238464355
Validation loss: 2.12630295753479

Epoch: 6| Step: 8
Training loss: 1.8855069875717163
Validation loss: 2.1126704812049866

Epoch: 6| Step: 9
Training loss: 2.069862127304077
Validation loss: 2.121087590853373

Epoch: 6| Step: 10
Training loss: 2.1678617000579834
Validation loss: 2.145839591821035

Epoch: 6| Step: 11
Training loss: 2.663875102996826
Validation loss: 2.116958757241567

Epoch: 6| Step: 12
Training loss: 1.5631468296051025
Validation loss: 2.1362995306650796

Epoch: 6| Step: 13
Training loss: 1.5460327863693237
Validation loss: 2.133682688077291

Epoch: 214| Step: 0
Training loss: 1.941510558128357
Validation loss: 2.1205421487490335

Epoch: 6| Step: 1
Training loss: 1.8484952449798584
Validation loss: 2.1407332022984824

Epoch: 6| Step: 2
Training loss: 1.72072172164917
Validation loss: 2.140616714954376

Epoch: 6| Step: 3
Training loss: 2.1192550659179688
Validation loss: 2.1351396640141806

Epoch: 6| Step: 4
Training loss: 1.9403834342956543
Validation loss: 2.1405588189760842

Epoch: 6| Step: 5
Training loss: 1.854195237159729
Validation loss: 2.1456594467163086

Epoch: 6| Step: 6
Training loss: 1.613081455230713
Validation loss: 2.1367615461349487

Epoch: 6| Step: 7
Training loss: 1.7858517169952393
Validation loss: 2.131945490837097

Epoch: 6| Step: 8
Training loss: 2.6949923038482666
Validation loss: 2.1580286224683127

Epoch: 6| Step: 9
Training loss: 2.164104461669922
Validation loss: 2.1513147354125977

Epoch: 6| Step: 10
Training loss: 1.2037560939788818
Validation loss: 2.182466149330139

Epoch: 6| Step: 11
Training loss: 1.54819917678833
Validation loss: 2.1575050354003906

Epoch: 6| Step: 12
Training loss: 1.5302358865737915
Validation loss: 2.153835336367289

Epoch: 6| Step: 13
Training loss: 1.5573525428771973
Validation loss: 2.1480838855107627

Epoch: 215| Step: 0
Training loss: 1.3528614044189453
Validation loss: 2.1449920733769736

Epoch: 6| Step: 1
Training loss: 1.6580778360366821
Validation loss: 2.1393845677375793

Epoch: 6| Step: 2
Training loss: 1.886338472366333
Validation loss: 2.138056536515554

Epoch: 6| Step: 3
Training loss: 2.4080328941345215
Validation loss: 2.143752098083496

Epoch: 6| Step: 4
Training loss: 2.7746741771698
Validation loss: 2.1387635866800943

Epoch: 6| Step: 5
Training loss: 1.1791051626205444
Validation loss: 2.143424868583679

Epoch: 6| Step: 6
Training loss: 1.5229361057281494
Validation loss: 2.138277848561605

Epoch: 6| Step: 7
Training loss: 1.3794505596160889
Validation loss: 2.133301536242167

Epoch: 6| Step: 8
Training loss: 1.2335608005523682
Validation loss: 2.137653032938639

Epoch: 6| Step: 9
Training loss: 1.7191203832626343
Validation loss: 2.1390610535939536

Epoch: 6| Step: 10
Training loss: 2.786982536315918
Validation loss: 2.140536447366079

Epoch: 6| Step: 11
Training loss: 1.6432850360870361
Validation loss: 2.1647881468137107

Epoch: 6| Step: 12
Training loss: 1.692443609237671
Validation loss: 2.157317121823629

Epoch: 6| Step: 13
Training loss: 2.2558252811431885
Validation loss: 2.14530485868454

Epoch: 216| Step: 0
Training loss: 1.239174246788025
Validation loss: 2.161525249481201

Epoch: 6| Step: 1
Training loss: 2.1901979446411133
Validation loss: 2.1586670676867166

Epoch: 6| Step: 2
Training loss: 1.588901162147522
Validation loss: 2.1603728334108987

Epoch: 6| Step: 3
Training loss: 1.8639614582061768
Validation loss: 2.1560944318771362

Epoch: 6| Step: 4
Training loss: 1.804880976676941
Validation loss: 2.1814263065656028

Epoch: 6| Step: 5
Training loss: 1.8488590717315674
Validation loss: 2.181060314178467

Epoch: 6| Step: 6
Training loss: 1.9156757593154907
Validation loss: 2.1446794271469116

Epoch: 6| Step: 7
Training loss: 1.8892159461975098
Validation loss: 2.154839118321737

Epoch: 6| Step: 8
Training loss: 2.141664505004883
Validation loss: 2.1254310806592307

Epoch: 6| Step: 9
Training loss: 1.712250828742981
Validation loss: 2.1338113943735757

Epoch: 6| Step: 10
Training loss: 1.7072339057922363
Validation loss: 2.121347188949585

Epoch: 6| Step: 11
Training loss: 1.9949655532836914
Validation loss: 2.123694042364756

Epoch: 6| Step: 12
Training loss: 1.5690033435821533
Validation loss: 2.1173502802848816

Epoch: 6| Step: 13
Training loss: 2.3750016689300537
Validation loss: 2.115331788857778

Epoch: 217| Step: 0
Training loss: 1.5351073741912842
Validation loss: 2.11940461397171

Epoch: 6| Step: 1
Training loss: 2.1439621448516846
Validation loss: 2.119265695412954

Epoch: 6| Step: 2
Training loss: 2.080160617828369
Validation loss: 2.1319868167241416

Epoch: 6| Step: 3
Training loss: 2.2920098304748535
Validation loss: 2.12406196196874

Epoch: 6| Step: 4
Training loss: 1.405256986618042
Validation loss: 2.1376484036445618

Epoch: 6| Step: 5
Training loss: 1.4007428884506226
Validation loss: 2.1397915482521057

Epoch: 6| Step: 6
Training loss: 1.6003104448318481
Validation loss: 2.143813967704773

Epoch: 6| Step: 7
Training loss: 2.3108105659484863
Validation loss: 2.1564452250798545

Epoch: 6| Step: 8
Training loss: 1.982569694519043
Validation loss: 2.1357169151306152

Epoch: 6| Step: 9
Training loss: 1.302870512008667
Validation loss: 2.1277700662612915

Epoch: 6| Step: 10
Training loss: 2.069688320159912
Validation loss: 2.1434102853139243

Epoch: 6| Step: 11
Training loss: 2.3276426792144775
Validation loss: 2.1418864130973816

Epoch: 6| Step: 12
Training loss: 1.280745029449463
Validation loss: 2.141072392463684

Epoch: 6| Step: 13
Training loss: 2.050363063812256
Validation loss: 2.1290664672851562

Epoch: 218| Step: 0
Training loss: 1.9131174087524414
Validation loss: 2.1372833053270974

Epoch: 6| Step: 1
Training loss: 1.5620298385620117
Validation loss: 2.126572549343109

Epoch: 6| Step: 2
Training loss: 1.9282814264297485
Validation loss: 2.1424946586290994

Epoch: 6| Step: 3
Training loss: 1.4711003303527832
Validation loss: 2.1425997813542685

Epoch: 6| Step: 4
Training loss: 1.9998902082443237
Validation loss: 2.139729678630829

Epoch: 6| Step: 5
Training loss: 1.4097914695739746
Validation loss: 2.1576010982195535

Epoch: 6| Step: 6
Training loss: 2.368058681488037
Validation loss: 2.158954640229543

Epoch: 6| Step: 7
Training loss: 1.5682777166366577
Validation loss: 2.1960338751475015

Epoch: 6| Step: 8
Training loss: 1.605180263519287
Validation loss: 2.1786498427391052

Epoch: 6| Step: 9
Training loss: 2.1913113594055176
Validation loss: 2.1624507506688437

Epoch: 6| Step: 10
Training loss: 1.3966000080108643
Validation loss: 2.161466360092163

Epoch: 6| Step: 11
Training loss: 1.9734325408935547
Validation loss: 2.142254809538523

Epoch: 6| Step: 12
Training loss: 1.5364210605621338
Validation loss: 2.1504050493240356

Epoch: 6| Step: 13
Training loss: 2.605903148651123
Validation loss: 2.165332555770874

Epoch: 219| Step: 0
Training loss: 2.883274793624878
Validation loss: 2.1516337394714355

Epoch: 6| Step: 1
Training loss: 1.261134147644043
Validation loss: 2.150651733080546

Epoch: 6| Step: 2
Training loss: 2.1022696495056152
Validation loss: 2.1371514002482095

Epoch: 6| Step: 3
Training loss: 1.5489872694015503
Validation loss: 2.16279935836792

Epoch: 6| Step: 4
Training loss: 1.827246904373169
Validation loss: 2.1617402831713357

Epoch: 6| Step: 5
Training loss: 2.190988063812256
Validation loss: 2.1705679098765054

Epoch: 6| Step: 6
Training loss: 1.5913288593292236
Validation loss: 2.1654634873072305

Epoch: 6| Step: 7
Training loss: 2.190690517425537
Validation loss: 2.1487961610158286

Epoch: 6| Step: 8
Training loss: 1.4173094034194946
Validation loss: 2.1500527461369834

Epoch: 6| Step: 9
Training loss: 1.8204119205474854
Validation loss: 2.1436172127723694

Epoch: 6| Step: 10
Training loss: 1.4499530792236328
Validation loss: 2.138857364654541

Epoch: 6| Step: 11
Training loss: 1.6017082929611206
Validation loss: 2.1625569065411887

Epoch: 6| Step: 12
Training loss: 1.4510818719863892
Validation loss: 2.142492095629374

Epoch: 6| Step: 13
Training loss: 1.8438117504119873
Validation loss: 2.137699325879415

Epoch: 220| Step: 0
Training loss: 1.6572234630584717
Validation loss: 2.150459329287211

Epoch: 6| Step: 1
Training loss: 2.2847721576690674
Validation loss: 2.152234435081482

Epoch: 6| Step: 2
Training loss: 2.1940531730651855
Validation loss: 2.159814635912577

Epoch: 6| Step: 3
Training loss: 1.3563029766082764
Validation loss: 2.142296254634857

Epoch: 6| Step: 4
Training loss: 1.6978371143341064
Validation loss: 2.1432876189549765

Epoch: 6| Step: 5
Training loss: 2.6174707412719727
Validation loss: 2.156844198703766

Epoch: 6| Step: 6
Training loss: 1.281158208847046
Validation loss: 2.183286945025126

Epoch: 6| Step: 7
Training loss: 1.875544548034668
Validation loss: 2.1729790767033896

Epoch: 6| Step: 8
Training loss: 1.715047836303711
Validation loss: 2.1557119886080423

Epoch: 6| Step: 9
Training loss: 1.5969668626785278
Validation loss: 2.164209246635437

Epoch: 6| Step: 10
Training loss: 2.2018022537231445
Validation loss: 2.158637305100759

Epoch: 6| Step: 11
Training loss: 1.7290802001953125
Validation loss: 2.13715926806132

Epoch: 6| Step: 12
Training loss: 1.377549409866333
Validation loss: 2.1349127491315207

Epoch: 6| Step: 13
Training loss: 1.5045757293701172
Validation loss: 2.1406212647755942

Epoch: 221| Step: 0
Training loss: 2.1722707748413086
Validation loss: 2.138814926147461

Epoch: 6| Step: 1
Training loss: 1.3931207656860352
Validation loss: 2.1345847845077515

Epoch: 6| Step: 2
Training loss: 1.7233257293701172
Validation loss: 2.122572958469391

Epoch: 6| Step: 3
Training loss: 2.4426894187927246
Validation loss: 2.1484013199806213

Epoch: 6| Step: 4
Training loss: 1.9902297258377075
Validation loss: 2.138141691684723

Epoch: 6| Step: 5
Training loss: 2.644556760787964
Validation loss: 2.133842945098877

Epoch: 6| Step: 6
Training loss: 0.9154221415519714
Validation loss: 2.141435205936432

Epoch: 6| Step: 7
Training loss: 1.6033833026885986
Validation loss: 2.134692112604777

Epoch: 6| Step: 8
Training loss: 1.0875667333602905
Validation loss: 2.1375500162442527

Epoch: 6| Step: 9
Training loss: 1.887467861175537
Validation loss: 2.1445339918136597

Epoch: 6| Step: 10
Training loss: 1.6622693538665771
Validation loss: 2.148011863231659

Epoch: 6| Step: 11
Training loss: 1.599531650543213
Validation loss: 2.158895492553711

Epoch: 6| Step: 12
Training loss: 2.0865345001220703
Validation loss: 2.1837796171506247

Epoch: 6| Step: 13
Training loss: 1.8510299921035767
Validation loss: 2.187077065308889

Epoch: 222| Step: 0
Training loss: 2.2210564613342285
Validation loss: 2.1855695048967996

Epoch: 6| Step: 1
Training loss: 2.065345287322998
Validation loss: 2.1996819575627646

Epoch: 6| Step: 2
Training loss: 1.529036521911621
Validation loss: 2.2036580443382263

Epoch: 6| Step: 3
Training loss: 1.6264338493347168
Validation loss: 2.2044562101364136

Epoch: 6| Step: 4
Training loss: 1.7146848440170288
Validation loss: 2.208197275797526

Epoch: 6| Step: 5
Training loss: 2.2752864360809326
Validation loss: 2.202974021434784

Epoch: 6| Step: 6
Training loss: 1.6197328567504883
Validation loss: 2.190342664718628

Epoch: 6| Step: 7
Training loss: 1.6028838157653809
Validation loss: 2.175015846888224

Epoch: 6| Step: 8
Training loss: 2.039523124694824
Validation loss: 2.157076120376587

Epoch: 6| Step: 9
Training loss: 1.5503308773040771
Validation loss: 2.147689859072367

Epoch: 6| Step: 10
Training loss: 2.074435234069824
Validation loss: 2.126267989476522

Epoch: 6| Step: 11
Training loss: 1.813946008682251
Validation loss: 2.111415922641754

Epoch: 6| Step: 12
Training loss: 2.002455234527588
Validation loss: 2.115542749563853

Epoch: 6| Step: 13
Training loss: 1.7316434383392334
Validation loss: 2.1071988344192505

Epoch: 223| Step: 0
Training loss: 2.4660372734069824
Validation loss: 2.1122304797172546

Epoch: 6| Step: 1
Training loss: 1.4481823444366455
Validation loss: 2.111514449119568

Epoch: 6| Step: 2
Training loss: 2.242745876312256
Validation loss: 2.109983523686727

Epoch: 6| Step: 3
Training loss: 1.8955843448638916
Validation loss: 2.104110598564148

Epoch: 6| Step: 4
Training loss: 1.5757081508636475
Validation loss: 2.1213110089302063

Epoch: 6| Step: 5
Training loss: 1.8498649597167969
Validation loss: 2.125367283821106

Epoch: 6| Step: 6
Training loss: 2.0212740898132324
Validation loss: 2.134754995505015

Epoch: 6| Step: 7
Training loss: 1.6377484798431396
Validation loss: 2.139498710632324

Epoch: 6| Step: 8
Training loss: 1.449487328529358
Validation loss: 2.1740558544794717

Epoch: 6| Step: 9
Training loss: 2.4012858867645264
Validation loss: 2.1771494150161743

Epoch: 6| Step: 10
Training loss: 1.9582180976867676
Validation loss: 2.205772280693054

Epoch: 6| Step: 11
Training loss: 2.2962613105773926
Validation loss: 2.1597594618797302

Epoch: 6| Step: 12
Training loss: 1.9480830430984497
Validation loss: 2.1771046121915183

Epoch: 6| Step: 13
Training loss: 2.1984238624572754
Validation loss: 2.15329239765803

Epoch: 224| Step: 0
Training loss: 1.7856850624084473
Validation loss: 2.150960405667623

Epoch: 6| Step: 1
Training loss: 0.9389699697494507
Validation loss: 2.1484933694203696

Epoch: 6| Step: 2
Training loss: 1.4786521196365356
Validation loss: 2.129982511202494

Epoch: 6| Step: 3
Training loss: 1.7708439826965332
Validation loss: 2.1129844188690186

Epoch: 6| Step: 4
Training loss: 3.0432252883911133
Validation loss: 2.121510863304138

Epoch: 6| Step: 5
Training loss: 2.325385570526123
Validation loss: 2.1152740915616355

Epoch: 6| Step: 6
Training loss: 2.014620304107666
Validation loss: 2.130062679449717

Epoch: 6| Step: 7
Training loss: 2.121507406234741
Validation loss: 2.1293092171351113

Epoch: 6| Step: 8
Training loss: 1.6319444179534912
Validation loss: 2.131150782108307

Epoch: 6| Step: 9
Training loss: 1.9062689542770386
Validation loss: 2.112099349498749

Epoch: 6| Step: 10
Training loss: 2.1006035804748535
Validation loss: 2.1187856992085776

Epoch: 6| Step: 11
Training loss: 2.344127655029297
Validation loss: 2.108660797278086

Epoch: 6| Step: 12
Training loss: 2.3838014602661133
Validation loss: 2.1043404738108316

Epoch: 6| Step: 13
Training loss: 2.0636308193206787
Validation loss: 2.079335411389669

Epoch: 225| Step: 0
Training loss: 2.263699531555176
Validation loss: 2.082588036855062

Epoch: 6| Step: 1
Training loss: 1.9409360885620117
Validation loss: 2.0797557632128396

Epoch: 6| Step: 2
Training loss: 2.1322357654571533
Validation loss: 2.094831029574076

Epoch: 6| Step: 3
Training loss: 2.028780937194824
Validation loss: 2.0911965568860373

Epoch: 6| Step: 4
Training loss: 2.3148789405822754
Validation loss: 2.121645669142405

Epoch: 6| Step: 5
Training loss: 1.8029099702835083
Validation loss: 2.120675186316172

Epoch: 6| Step: 6
Training loss: 1.486738920211792
Validation loss: 2.118949035803477

Epoch: 6| Step: 7
Training loss: 2.13299298286438
Validation loss: 2.1421485940615335

Epoch: 6| Step: 8
Training loss: 1.7181222438812256
Validation loss: 2.1266488830248513

Epoch: 6| Step: 9
Training loss: 1.6126247644424438
Validation loss: 2.1410040259361267

Epoch: 6| Step: 10
Training loss: 1.265251636505127
Validation loss: 2.151451746622721

Epoch: 6| Step: 11
Training loss: 2.123903512954712
Validation loss: 2.1522496541341147

Epoch: 6| Step: 12
Training loss: 1.4407538175582886
Validation loss: 2.1474157571792603

Epoch: 6| Step: 13
Training loss: 1.8030765056610107
Validation loss: 2.151931405067444

Epoch: 226| Step: 0
Training loss: 2.5155935287475586
Validation loss: 2.130467196305593

Epoch: 6| Step: 1
Training loss: 1.2667479515075684
Validation loss: 2.132031043370565

Epoch: 6| Step: 2
Training loss: 2.502933979034424
Validation loss: 2.1604495843251548

Epoch: 6| Step: 3
Training loss: 1.4536941051483154
Validation loss: 2.143693208694458

Epoch: 6| Step: 4
Training loss: 1.9016073942184448
Validation loss: 2.1402628223101297

Epoch: 6| Step: 5
Training loss: 1.3423632383346558
Validation loss: 2.1367361346880593

Epoch: 6| Step: 6
Training loss: 1.5910162925720215
Validation loss: 2.147135337193807

Epoch: 6| Step: 7
Training loss: 1.611623764038086
Validation loss: 2.144452750682831

Epoch: 6| Step: 8
Training loss: 1.5244364738464355
Validation loss: 2.1491955320040383

Epoch: 6| Step: 9
Training loss: 1.8341563940048218
Validation loss: 2.1361116766929626

Epoch: 6| Step: 10
Training loss: 1.7125827074050903
Validation loss: 2.149026890595754

Epoch: 6| Step: 11
Training loss: 2.35012149810791
Validation loss: 2.1639396945635476

Epoch: 6| Step: 12
Training loss: 2.0869333744049072
Validation loss: 2.169954518477122

Epoch: 6| Step: 13
Training loss: 1.5649001598358154
Validation loss: 2.169838627179464

Epoch: 227| Step: 0
Training loss: 1.7295916080474854
Validation loss: 2.169449190298716

Epoch: 6| Step: 1
Training loss: 2.2130775451660156
Validation loss: 2.168341418107351

Epoch: 6| Step: 2
Training loss: 1.3129010200500488
Validation loss: 2.1728402376174927

Epoch: 6| Step: 3
Training loss: 2.0188629627227783
Validation loss: 2.1695779959360757

Epoch: 6| Step: 4
Training loss: 1.5094590187072754
Validation loss: 2.1647557616233826

Epoch: 6| Step: 5
Training loss: 1.0793182849884033
Validation loss: 2.174070735772451

Epoch: 6| Step: 6
Training loss: 2.1799588203430176
Validation loss: 2.1635981599489846

Epoch: 6| Step: 7
Training loss: 1.3906357288360596
Validation loss: 2.1723508636156716

Epoch: 6| Step: 8
Training loss: 2.0918068885803223
Validation loss: 2.169038156668345

Epoch: 6| Step: 9
Training loss: 2.022799015045166
Validation loss: 2.15560911099116

Epoch: 6| Step: 10
Training loss: 2.1549758911132812
Validation loss: 2.1622930566469827

Epoch: 6| Step: 11
Training loss: 1.8821446895599365
Validation loss: 2.151575187842051

Epoch: 6| Step: 12
Training loss: 1.7234790325164795
Validation loss: 2.1340094606081643

Epoch: 6| Step: 13
Training loss: 2.044628143310547
Validation loss: 2.150042732556661

Epoch: 228| Step: 0
Training loss: 1.3697336912155151
Validation loss: 2.1530864437421164

Epoch: 6| Step: 1
Training loss: 1.7255425453186035
Validation loss: 2.1361738046010337

Epoch: 6| Step: 2
Training loss: 1.7657697200775146
Validation loss: 2.1602341135342917

Epoch: 6| Step: 3
Training loss: 1.7038836479187012
Validation loss: 2.1627195676167807

Epoch: 6| Step: 4
Training loss: 2.0754058361053467
Validation loss: 2.1575844486554465

Epoch: 6| Step: 5
Training loss: 2.0254063606262207
Validation loss: 2.1565678119659424

Epoch: 6| Step: 6
Training loss: 1.1367871761322021
Validation loss: 2.172274351119995

Epoch: 6| Step: 7
Training loss: 1.8685109615325928
Validation loss: 2.1830485264460244

Epoch: 6| Step: 8
Training loss: 1.4758727550506592
Validation loss: 2.1687833070755005

Epoch: 6| Step: 9
Training loss: 1.255958080291748
Validation loss: 2.1891228755315146

Epoch: 6| Step: 10
Training loss: 2.57961106300354
Validation loss: 2.1690265933672586

Epoch: 6| Step: 11
Training loss: 1.8469810485839844
Validation loss: 2.1618624130884805

Epoch: 6| Step: 12
Training loss: 1.7778441905975342
Validation loss: 2.1828500032424927

Epoch: 6| Step: 13
Training loss: 2.1578259468078613
Validation loss: 2.153456687927246

Epoch: 229| Step: 0
Training loss: 1.7261631488800049
Validation loss: 2.1551700234413147

Epoch: 6| Step: 1
Training loss: 1.5098427534103394
Validation loss: 2.1557735204696655

Epoch: 6| Step: 2
Training loss: 1.9936532974243164
Validation loss: 2.1532546083132424

Epoch: 6| Step: 3
Training loss: 1.8781360387802124
Validation loss: 2.149980127811432

Epoch: 6| Step: 4
Training loss: 1.5698914527893066
Validation loss: 2.1228045225143433

Epoch: 6| Step: 5
Training loss: 1.538804054260254
Validation loss: 2.137780805428823

Epoch: 6| Step: 6
Training loss: 1.885536551475525
Validation loss: 2.1373921831448874

Epoch: 6| Step: 7
Training loss: 1.3690794706344604
Validation loss: 2.1507277290026345

Epoch: 6| Step: 8
Training loss: 2.1810977458953857
Validation loss: 2.144020597139994

Epoch: 6| Step: 9
Training loss: 1.9449121952056885
Validation loss: 2.1580867767333984

Epoch: 6| Step: 10
Training loss: 1.4972646236419678
Validation loss: 2.1533864537874856

Epoch: 6| Step: 11
Training loss: 1.9054179191589355
Validation loss: 2.150315999984741

Epoch: 6| Step: 12
Training loss: 1.867463231086731
Validation loss: 2.1632367173830667

Epoch: 6| Step: 13
Training loss: 1.9057608842849731
Validation loss: 2.164070705572764

Epoch: 230| Step: 0
Training loss: 1.7268211841583252
Validation loss: 2.1687684853871665

Epoch: 6| Step: 1
Training loss: 1.6243196725845337
Validation loss: 2.1663623054822287

Epoch: 6| Step: 2
Training loss: 1.4172899723052979
Validation loss: 2.1756773789723716

Epoch: 6| Step: 3
Training loss: 1.582565188407898
Validation loss: 2.1814318895339966

Epoch: 6| Step: 4
Training loss: 1.602510690689087
Validation loss: 2.188747743765513

Epoch: 6| Step: 5
Training loss: 2.03192138671875
Validation loss: 2.1615413427352905

Epoch: 6| Step: 6
Training loss: 3.190342903137207
Validation loss: 2.163709123929342

Epoch: 6| Step: 7
Training loss: 1.455386996269226
Validation loss: 2.1678059498469033

Epoch: 6| Step: 8
Training loss: 1.9991745948791504
Validation loss: 2.178511679172516

Epoch: 6| Step: 9
Training loss: 1.6093308925628662
Validation loss: 2.1712592244148254

Epoch: 6| Step: 10
Training loss: 1.9544987678527832
Validation loss: 2.1780114571253457

Epoch: 6| Step: 11
Training loss: 1.1794646978378296
Validation loss: 2.1693865855534873

Epoch: 6| Step: 12
Training loss: 2.04300594329834
Validation loss: 2.1789215207099915

Epoch: 6| Step: 13
Training loss: 1.6042134761810303
Validation loss: 2.1720987359682717

Epoch: 231| Step: 0
Training loss: 1.4914376735687256
Validation loss: 2.185445547103882

Epoch: 6| Step: 1
Training loss: 1.2697956562042236
Validation loss: 2.185393214225769

Epoch: 6| Step: 2
Training loss: 1.4615013599395752
Validation loss: 2.182857116063436

Epoch: 6| Step: 3
Training loss: 2.038316488265991
Validation loss: 2.179472267627716

Epoch: 6| Step: 4
Training loss: 1.920358419418335
Validation loss: 2.184873183568319

Epoch: 6| Step: 5
Training loss: 1.8350766897201538
Validation loss: 2.1438294649124146

Epoch: 6| Step: 6
Training loss: 1.807242751121521
Validation loss: 2.146654188632965

Epoch: 6| Step: 7
Training loss: 1.4424247741699219
Validation loss: 2.1402523716290793

Epoch: 6| Step: 8
Training loss: 2.3683032989501953
Validation loss: 2.143424113591512

Epoch: 6| Step: 9
Training loss: 1.892015814781189
Validation loss: 2.134259303410848

Epoch: 6| Step: 10
Training loss: 1.446382761001587
Validation loss: 2.1193116108576455

Epoch: 6| Step: 11
Training loss: 2.518883228302002
Validation loss: 2.123341957728068

Epoch: 6| Step: 12
Training loss: 2.0716288089752197
Validation loss: 2.1094414393107095

Epoch: 6| Step: 13
Training loss: 2.2124385833740234
Validation loss: 2.121565878391266

Epoch: 232| Step: 0
Training loss: 2.0098330974578857
Validation loss: 2.1288081407546997

Epoch: 6| Step: 1
Training loss: 1.4948155879974365
Validation loss: 2.1255138913790383

Epoch: 6| Step: 2
Training loss: 1.147723913192749
Validation loss: 2.1349134842554727

Epoch: 6| Step: 3
Training loss: 2.1427664756774902
Validation loss: 2.129314104715983

Epoch: 6| Step: 4
Training loss: 1.65542471408844
Validation loss: 2.1430615385373435

Epoch: 6| Step: 5
Training loss: 2.3758034706115723
Validation loss: 2.146525045235952

Epoch: 6| Step: 6
Training loss: 1.9908168315887451
Validation loss: 2.169472793738047

Epoch: 6| Step: 7
Training loss: 1.7387189865112305
Validation loss: 2.1835784912109375

Epoch: 6| Step: 8
Training loss: 1.4806654453277588
Validation loss: 2.192222237586975

Epoch: 6| Step: 9
Training loss: 1.8437589406967163
Validation loss: 2.189222832520803

Epoch: 6| Step: 10
Training loss: 1.4179080724716187
Validation loss: 2.2104226549466452

Epoch: 6| Step: 11
Training loss: 1.9056100845336914
Validation loss: 2.216266175111135

Epoch: 6| Step: 12
Training loss: 1.6080574989318848
Validation loss: 2.2359940012296042

Epoch: 6| Step: 13
Training loss: 1.9807194471359253
Validation loss: 2.2071149746576944

Epoch: 233| Step: 0
Training loss: 0.7622148990631104
Validation loss: 2.207495848337809

Epoch: 6| Step: 1
Training loss: 2.1978049278259277
Validation loss: 2.185291369756063

Epoch: 6| Step: 2
Training loss: 1.8200578689575195
Validation loss: 2.18568746248881

Epoch: 6| Step: 3
Training loss: 2.1442630290985107
Validation loss: 2.1859500408172607

Epoch: 6| Step: 4
Training loss: 1.3856498003005981
Validation loss: 2.176122506459554

Epoch: 6| Step: 5
Training loss: 1.782238245010376
Validation loss: 2.148671587308248

Epoch: 6| Step: 6
Training loss: 2.0701632499694824
Validation loss: 2.1518723567326865

Epoch: 6| Step: 7
Training loss: 1.7264454364776611
Validation loss: 2.1729018886884055

Epoch: 6| Step: 8
Training loss: 2.1119184494018555
Validation loss: 2.1595619320869446

Epoch: 6| Step: 9
Training loss: 1.9590157270431519
Validation loss: 2.146507898966471

Epoch: 6| Step: 10
Training loss: 2.2131340503692627
Validation loss: 2.14942467212677

Epoch: 6| Step: 11
Training loss: 1.5829167366027832
Validation loss: 2.139345328013102

Epoch: 6| Step: 12
Training loss: 1.444098949432373
Validation loss: 2.1539263327916465

Epoch: 6| Step: 13
Training loss: 1.1658676862716675
Validation loss: 2.1752886970837912

Epoch: 234| Step: 0
Training loss: 2.039890766143799
Validation loss: 2.1699837843577066

Epoch: 6| Step: 1
Training loss: 1.473564624786377
Validation loss: 2.158790171146393

Epoch: 6| Step: 2
Training loss: 1.418526530265808
Validation loss: 2.1614184379577637

Epoch: 6| Step: 3
Training loss: 1.1769747734069824
Validation loss: 2.163857420285543

Epoch: 6| Step: 4
Training loss: 1.6494429111480713
Validation loss: 2.150568207105001

Epoch: 6| Step: 5
Training loss: 1.5182020664215088
Validation loss: 2.163874864578247

Epoch: 6| Step: 6
Training loss: 1.4406635761260986
Validation loss: 2.1468982100486755

Epoch: 6| Step: 7
Training loss: 1.5716408491134644
Validation loss: 2.1318230628967285

Epoch: 6| Step: 8
Training loss: 1.805809497833252
Validation loss: 2.1501861612002053

Epoch: 6| Step: 9
Training loss: 1.642261266708374
Validation loss: 2.1563382546106973

Epoch: 6| Step: 10
Training loss: 2.7192420959472656
Validation loss: 2.1503493189811707

Epoch: 6| Step: 11
Training loss: 1.9296541213989258
Validation loss: 2.1522864500681558

Epoch: 6| Step: 12
Training loss: 2.267887830734253
Validation loss: 2.159192125002543

Epoch: 6| Step: 13
Training loss: 1.795698881149292
Validation loss: 2.178353250026703

Epoch: 235| Step: 0
Training loss: 1.8197486400604248
Validation loss: 2.183233161767324

Epoch: 6| Step: 1
Training loss: 2.014634847640991
Validation loss: 2.1916181445121765

Epoch: 6| Step: 2
Training loss: 2.038154125213623
Validation loss: 2.203499674797058

Epoch: 6| Step: 3
Training loss: 1.7176135778427124
Validation loss: 2.2044546206792197

Epoch: 6| Step: 4
Training loss: 2.0363922119140625
Validation loss: 2.2104506293932595

Epoch: 6| Step: 5
Training loss: 2.335887908935547
Validation loss: 2.1890926559766135

Epoch: 6| Step: 6
Training loss: 1.9121721982955933
Validation loss: 2.1815200050671897

Epoch: 6| Step: 7
Training loss: 1.734654426574707
Validation loss: 2.1931586265563965

Epoch: 6| Step: 8
Training loss: 1.2135416269302368
Validation loss: 2.208536903063456

Epoch: 6| Step: 9
Training loss: 1.5111305713653564
Validation loss: 2.2026049296061196

Epoch: 6| Step: 10
Training loss: 1.0877330303192139
Validation loss: 2.205545504887899

Epoch: 6| Step: 11
Training loss: 2.073697090148926
Validation loss: 2.2305285135904946

Epoch: 6| Step: 12
Training loss: 1.5120817422866821
Validation loss: 2.204655965169271

Epoch: 6| Step: 13
Training loss: 1.5814050436019897
Validation loss: 2.2231953938802085

Epoch: 236| Step: 0
Training loss: 1.7051575183868408
Validation loss: 2.2099399964014688

Epoch: 6| Step: 1
Training loss: 1.8272942304611206
Validation loss: 2.213788469632467

Epoch: 6| Step: 2
Training loss: 1.5303099155426025
Validation loss: 2.226762036482493

Epoch: 6| Step: 3
Training loss: 1.5941015481948853
Validation loss: 2.18301914135615

Epoch: 6| Step: 4
Training loss: 1.491105079650879
Validation loss: 2.189371109008789

Epoch: 6| Step: 5
Training loss: 1.4324500560760498
Validation loss: 2.1749685605367026

Epoch: 6| Step: 6
Training loss: 1.6923011541366577
Validation loss: 2.1327826778093972

Epoch: 6| Step: 7
Training loss: 2.03889799118042
Validation loss: 2.13831627368927

Epoch: 6| Step: 8
Training loss: 1.9476590156555176
Validation loss: 2.1246201395988464

Epoch: 6| Step: 9
Training loss: 2.3240866661071777
Validation loss: 2.1314592560132346

Epoch: 6| Step: 10
Training loss: 2.0105528831481934
Validation loss: 2.1321838895479837

Epoch: 6| Step: 11
Training loss: 1.2608075141906738
Validation loss: 2.1519104838371277

Epoch: 6| Step: 12
Training loss: 1.584186315536499
Validation loss: 2.149917244911194

Epoch: 6| Step: 13
Training loss: 2.644444227218628
Validation loss: 2.1261107524236045

Epoch: 237| Step: 0
Training loss: 1.4835549592971802
Validation loss: 2.1349176367123923

Epoch: 6| Step: 1
Training loss: 1.9307892322540283
Validation loss: 2.155029912789663

Epoch: 6| Step: 2
Training loss: 2.7695062160491943
Validation loss: 2.1477712392807007

Epoch: 6| Step: 3
Training loss: 1.9750192165374756
Validation loss: 2.16099222501119

Epoch: 6| Step: 4
Training loss: 0.9805874228477478
Validation loss: 2.153444548447927

Epoch: 6| Step: 5
Training loss: 2.2580480575561523
Validation loss: 2.1779843171437583

Epoch: 6| Step: 6
Training loss: 1.525123119354248
Validation loss: 2.17927747964859

Epoch: 6| Step: 7
Training loss: 1.9253748655319214
Validation loss: 2.164312462011973

Epoch: 6| Step: 8
Training loss: 1.4779211282730103
Validation loss: 2.1590675115585327

Epoch: 6| Step: 9
Training loss: 1.615776777267456
Validation loss: 2.1826384464899697

Epoch: 6| Step: 10
Training loss: 1.6091195344924927
Validation loss: 2.1708268324534097

Epoch: 6| Step: 11
Training loss: 2.0851452350616455
Validation loss: 2.167201360066732

Epoch: 6| Step: 12
Training loss: 1.9601466655731201
Validation loss: 2.151395857334137

Epoch: 6| Step: 13
Training loss: 1.7489981651306152
Validation loss: 2.1506362557411194

Epoch: 238| Step: 0
Training loss: 1.7331266403198242
Validation loss: 2.1427002350489297

Epoch: 6| Step: 1
Training loss: 2.2396903038024902
Validation loss: 2.131085693836212

Epoch: 6| Step: 2
Training loss: 1.696854591369629
Validation loss: 2.1293365359306335

Epoch: 6| Step: 3
Training loss: 1.8290878534317017
Validation loss: 2.1320637663205466

Epoch: 6| Step: 4
Training loss: 1.3910325765609741
Validation loss: 2.145552317301432

Epoch: 6| Step: 5
Training loss: 2.1967315673828125
Validation loss: 2.1421748797098794

Epoch: 6| Step: 6
Training loss: 2.30342435836792
Validation loss: 2.1271554032961526

Epoch: 6| Step: 7
Training loss: 1.6090123653411865
Validation loss: 2.1456915934880576

Epoch: 6| Step: 8
Training loss: 1.1474437713623047
Validation loss: 2.149396022160848

Epoch: 6| Step: 9
Training loss: 1.9268383979797363
Validation loss: 2.1727911233901978

Epoch: 6| Step: 10
Training loss: 1.8298499584197998
Validation loss: 2.1830889781316123

Epoch: 6| Step: 11
Training loss: 1.7334834337234497
Validation loss: 2.166246692339579

Epoch: 6| Step: 12
Training loss: 1.4239552021026611
Validation loss: 2.18119215965271

Epoch: 6| Step: 13
Training loss: 1.6147682666778564
Validation loss: 2.173374672730764

Epoch: 239| Step: 0
Training loss: 1.9426227807998657
Validation loss: 2.1904258131980896

Epoch: 6| Step: 1
Training loss: 1.1917917728424072
Validation loss: 2.1794053316116333

Epoch: 6| Step: 2
Training loss: 1.3042454719543457
Validation loss: 2.19994980096817

Epoch: 6| Step: 3
Training loss: 1.878745436668396
Validation loss: 2.1962711811065674

Epoch: 6| Step: 4
Training loss: 2.0898056030273438
Validation loss: 2.1788594126701355

Epoch: 6| Step: 5
Training loss: 1.6998507976531982
Validation loss: 2.16196874777476

Epoch: 6| Step: 6
Training loss: 1.5646523237228394
Validation loss: 2.205962677796682

Epoch: 6| Step: 7
Training loss: 1.773538589477539
Validation loss: 2.176039377848307

Epoch: 6| Step: 8
Training loss: 1.276260256767273
Validation loss: 2.1962244113286338

Epoch: 6| Step: 9
Training loss: 1.1426799297332764
Validation loss: 2.2218234141667685

Epoch: 6| Step: 10
Training loss: 2.369579553604126
Validation loss: 2.2053000926971436

Epoch: 6| Step: 11
Training loss: 1.1851972341537476
Validation loss: 2.222171425819397

Epoch: 6| Step: 12
Training loss: 2.9730520248413086
Validation loss: 2.212697982788086

Epoch: 6| Step: 13
Training loss: 2.0744707584381104
Validation loss: 2.1918012301127114

Epoch: 240| Step: 0
Training loss: 2.5593626499176025
Validation loss: 2.1951209704081216

Epoch: 6| Step: 1
Training loss: 1.6252822875976562
Validation loss: 2.2001553575197854

Epoch: 6| Step: 2
Training loss: 1.692857027053833
Validation loss: 2.173263649145762

Epoch: 6| Step: 3
Training loss: 1.7858455181121826
Validation loss: 2.1644830306371055

Epoch: 6| Step: 4
Training loss: 1.3728513717651367
Validation loss: 2.1780821482340493

Epoch: 6| Step: 5
Training loss: 1.4807920455932617
Validation loss: 2.162248969078064

Epoch: 6| Step: 6
Training loss: 1.7643325328826904
Validation loss: 2.175453782081604

Epoch: 6| Step: 7
Training loss: 2.181394100189209
Validation loss: 2.1461312770843506

Epoch: 6| Step: 8
Training loss: 0.9368413090705872
Validation loss: 2.1566636761029563

Epoch: 6| Step: 9
Training loss: 2.4210493564605713
Validation loss: 2.151474714279175

Epoch: 6| Step: 10
Training loss: 1.5045580863952637
Validation loss: 2.1660138368606567

Epoch: 6| Step: 11
Training loss: 1.6975688934326172
Validation loss: 2.1528257528940835

Epoch: 6| Step: 12
Training loss: 1.1854567527770996
Validation loss: 2.1439202229181924

Epoch: 6| Step: 13
Training loss: 1.8411561250686646
Validation loss: 2.1401092211405435

Epoch: 241| Step: 0
Training loss: 1.4795138835906982
Validation loss: 2.1383676131566367

Epoch: 6| Step: 1
Training loss: 2.090250015258789
Validation loss: 2.1619959274927774

Epoch: 6| Step: 2
Training loss: 1.7437759637832642
Validation loss: 2.1637458205223083

Epoch: 6| Step: 3
Training loss: 1.8569780588150024
Validation loss: 2.171644608179728

Epoch: 6| Step: 4
Training loss: 1.4391006231307983
Validation loss: 2.162877897421519

Epoch: 6| Step: 5
Training loss: 2.8431625366210938
Validation loss: 2.1827109257380166

Epoch: 6| Step: 6
Training loss: 1.8088994026184082
Validation loss: 2.2072550853093467

Epoch: 6| Step: 7
Training loss: 1.260540246963501
Validation loss: 2.223820447921753

Epoch: 6| Step: 8
Training loss: 1.7170581817626953
Validation loss: 2.2119001547495523

Epoch: 6| Step: 9
Training loss: 1.7804491519927979
Validation loss: 2.2011419336001077

Epoch: 6| Step: 10
Training loss: 1.4797098636627197
Validation loss: 2.18832133213679

Epoch: 6| Step: 11
Training loss: 1.5574437379837036
Validation loss: 2.1888503432273865

Epoch: 6| Step: 12
Training loss: 1.601894736289978
Validation loss: 2.179732402165731

Epoch: 6| Step: 13
Training loss: 1.8585155010223389
Validation loss: 2.1893759767214456

Epoch: 242| Step: 0
Training loss: 1.9962589740753174
Validation loss: 2.150599996248881

Epoch: 6| Step: 1
Training loss: 1.5869419574737549
Validation loss: 2.151920219262441

Epoch: 6| Step: 2
Training loss: 1.6273581981658936
Validation loss: 2.151248296101888

Epoch: 6| Step: 3
Training loss: 1.9966440200805664
Validation loss: 2.1446593205134072

Epoch: 6| Step: 4
Training loss: 1.359950065612793
Validation loss: 2.1495028734207153

Epoch: 6| Step: 5
Training loss: 2.2756507396698
Validation loss: 2.156733433405558

Epoch: 6| Step: 6
Training loss: 1.4592491388320923
Validation loss: 2.1500569780667624

Epoch: 6| Step: 7
Training loss: 1.6875455379486084
Validation loss: 2.153015971183777

Epoch: 6| Step: 8
Training loss: 1.6361280679702759
Validation loss: 2.1686291694641113

Epoch: 6| Step: 9
Training loss: 1.8482692241668701
Validation loss: 2.167846957842509

Epoch: 6| Step: 10
Training loss: 1.6305720806121826
Validation loss: 2.1782957712809243

Epoch: 6| Step: 11
Training loss: 2.4418253898620605
Validation loss: 2.158304433027903

Epoch: 6| Step: 12
Training loss: 1.8910341262817383
Validation loss: 2.171142359574636

Epoch: 6| Step: 13
Training loss: 1.3005484342575073
Validation loss: 2.16079713900884

Epoch: 243| Step: 0
Training loss: 1.662060022354126
Validation loss: 2.1713433265686035

Epoch: 6| Step: 1
Training loss: 1.0637855529785156
Validation loss: 2.1662059227625527

Epoch: 6| Step: 2
Training loss: 1.9571528434753418
Validation loss: 2.180450677871704

Epoch: 6| Step: 3
Training loss: 0.9650835394859314
Validation loss: 2.1791247526804605

Epoch: 6| Step: 4
Training loss: 1.3023595809936523
Validation loss: 2.185316244761149

Epoch: 6| Step: 5
Training loss: 1.5040297508239746
Validation loss: 2.205042918523153

Epoch: 6| Step: 6
Training loss: 1.8651458024978638
Validation loss: 2.2089080810546875

Epoch: 6| Step: 7
Training loss: 2.4305851459503174
Validation loss: 2.214348395665487

Epoch: 6| Step: 8
Training loss: 1.3763329982757568
Validation loss: 2.2090967297554016

Epoch: 6| Step: 9
Training loss: 1.7504396438598633
Validation loss: 2.192528486251831

Epoch: 6| Step: 10
Training loss: 2.5768532752990723
Validation loss: 2.198096831639608

Epoch: 6| Step: 11
Training loss: 1.5948532819747925
Validation loss: 2.166773557662964

Epoch: 6| Step: 12
Training loss: 1.55927312374115
Validation loss: 2.193119168281555

Epoch: 6| Step: 13
Training loss: 2.691073417663574
Validation loss: 2.1840290824572244

Epoch: 244| Step: 0
Training loss: 1.7425895929336548
Validation loss: 2.156160612901052

Epoch: 6| Step: 1
Training loss: 1.280362844467163
Validation loss: 2.169489641984304

Epoch: 6| Step: 2
Training loss: 2.2212719917297363
Validation loss: 2.1608066956202188

Epoch: 6| Step: 3
Training loss: 1.5549311637878418
Validation loss: 2.149932006994883

Epoch: 6| Step: 4
Training loss: 1.3820186853408813
Validation loss: 2.1496275663375854

Epoch: 6| Step: 5
Training loss: 1.463597059249878
Validation loss: 2.1458619634310403

Epoch: 6| Step: 6
Training loss: 1.8208564519882202
Validation loss: 2.1533682147661843

Epoch: 6| Step: 7
Training loss: 2.017775058746338
Validation loss: 2.15208500623703

Epoch: 6| Step: 8
Training loss: 1.8565073013305664
Validation loss: 2.129007617632548

Epoch: 6| Step: 9
Training loss: 2.2517693042755127
Validation loss: 2.1557329098383584

Epoch: 6| Step: 10
Training loss: 1.442718744277954
Validation loss: 2.1528665026028952

Epoch: 6| Step: 11
Training loss: 1.3864831924438477
Validation loss: 2.180041750272115

Epoch: 6| Step: 12
Training loss: 1.5180948972702026
Validation loss: 2.1870388189951577

Epoch: 6| Step: 13
Training loss: 1.8507728576660156
Validation loss: 2.194763978322347

Epoch: 245| Step: 0
Training loss: 1.1317033767700195
Validation loss: 2.1864433884620667

Epoch: 6| Step: 1
Training loss: 1.541766881942749
Validation loss: 2.1823677023251853

Epoch: 6| Step: 2
Training loss: 1.5914220809936523
Validation loss: 2.1777459581693015

Epoch: 6| Step: 3
Training loss: 1.4870264530181885
Validation loss: 2.170451045036316

Epoch: 6| Step: 4
Training loss: 1.7956862449645996
Validation loss: 2.1700655619303384

Epoch: 6| Step: 5
Training loss: 1.7509055137634277
Validation loss: 2.168629844983419

Epoch: 6| Step: 6
Training loss: 1.8917686939239502
Validation loss: 2.164038916428884

Epoch: 6| Step: 7
Training loss: 1.8893696069717407
Validation loss: 2.177812337875366

Epoch: 6| Step: 8
Training loss: 1.3975917100906372
Validation loss: 2.1846038897832236

Epoch: 6| Step: 9
Training loss: 2.5093517303466797
Validation loss: 2.1961278716723123

Epoch: 6| Step: 10
Training loss: 2.1588315963745117
Validation loss: 2.1723837852478027

Epoch: 6| Step: 11
Training loss: 1.5977933406829834
Validation loss: 2.182061950365702

Epoch: 6| Step: 12
Training loss: 1.646828532218933
Validation loss: 2.197172780831655

Epoch: 6| Step: 13
Training loss: 1.5291985273361206
Validation loss: 2.1609611908594766

Epoch: 246| Step: 0
Training loss: 1.7865650653839111
Validation loss: 2.202041268348694

Epoch: 6| Step: 1
Training loss: 2.5379366874694824
Validation loss: 2.1895680030186973

Epoch: 6| Step: 2
Training loss: 1.7937159538269043
Validation loss: 2.1543740232785544

Epoch: 6| Step: 3
Training loss: 2.553744077682495
Validation loss: 2.1736698349316916

Epoch: 6| Step: 4
Training loss: 1.1391874551773071
Validation loss: 2.1641130248705545

Epoch: 6| Step: 5
Training loss: 1.725536823272705
Validation loss: 2.151189068953196

Epoch: 6| Step: 6
Training loss: 1.6162457466125488
Validation loss: 2.1597363352775574

Epoch: 6| Step: 7
Training loss: 1.3534444570541382
Validation loss: 2.1606956124305725

Epoch: 6| Step: 8
Training loss: 1.8332358598709106
Validation loss: 2.153361996014913

Epoch: 6| Step: 9
Training loss: 1.9751005172729492
Validation loss: 2.160524050394694

Epoch: 6| Step: 10
Training loss: 1.2288788557052612
Validation loss: 2.14579967657725

Epoch: 6| Step: 11
Training loss: 1.2723941802978516
Validation loss: 2.178142031033834

Epoch: 6| Step: 12
Training loss: 1.4755523204803467
Validation loss: 2.180519918600718

Epoch: 6| Step: 13
Training loss: 1.70952570438385
Validation loss: 2.179732163747152

Epoch: 247| Step: 0
Training loss: 1.186255931854248
Validation loss: 2.1798652013142905

Epoch: 6| Step: 1
Training loss: 1.3962575197219849
Validation loss: 2.177488605181376

Epoch: 6| Step: 2
Training loss: 1.591024398803711
Validation loss: 2.191637694835663

Epoch: 6| Step: 3
Training loss: 2.791426181793213
Validation loss: 2.1907624999682107

Epoch: 6| Step: 4
Training loss: 1.9729485511779785
Validation loss: 2.19382381439209

Epoch: 6| Step: 5
Training loss: 1.8963667154312134
Validation loss: 2.186761796474457

Epoch: 6| Step: 6
Training loss: 1.5993410348892212
Validation loss: 2.18424121538798

Epoch: 6| Step: 7
Training loss: 1.5519481897354126
Validation loss: 2.177253703276316

Epoch: 6| Step: 8
Training loss: 1.5460362434387207
Validation loss: 2.186905006567637

Epoch: 6| Step: 9
Training loss: 2.056305408477783
Validation loss: 2.199861168861389

Epoch: 6| Step: 10
Training loss: 1.3457010984420776
Validation loss: 2.1827661395072937

Epoch: 6| Step: 11
Training loss: 1.7463730573654175
Validation loss: 2.1753179828325906

Epoch: 6| Step: 12
Training loss: 1.4806156158447266
Validation loss: 2.1940815846125283

Epoch: 6| Step: 13
Training loss: 1.5602768659591675
Validation loss: 2.1984949906667075

Epoch: 248| Step: 0
Training loss: 1.5171080827713013
Validation loss: 2.184174040953318

Epoch: 6| Step: 1
Training loss: 1.7171082496643066
Validation loss: 2.190697948137919

Epoch: 6| Step: 2
Training loss: 1.527127981185913
Validation loss: 2.2019657492637634

Epoch: 6| Step: 3
Training loss: 1.4156498908996582
Validation loss: 2.2246491511662803

Epoch: 6| Step: 4
Training loss: 1.6314480304718018
Validation loss: 2.200597862402598

Epoch: 6| Step: 5
Training loss: 1.8939180374145508
Validation loss: 2.1864036917686462

Epoch: 6| Step: 6
Training loss: 2.2357897758483887
Validation loss: 2.176419675350189

Epoch: 6| Step: 7
Training loss: 1.4255374670028687
Validation loss: 2.1731403271357217

Epoch: 6| Step: 8
Training loss: 1.5915038585662842
Validation loss: 2.1743791103363037

Epoch: 6| Step: 9
Training loss: 2.0528078079223633
Validation loss: 2.1554845770200095

Epoch: 6| Step: 10
Training loss: 2.712940216064453
Validation loss: 2.1675397555033364

Epoch: 6| Step: 11
Training loss: 1.274619698524475
Validation loss: 2.15598201751709

Epoch: 6| Step: 12
Training loss: 1.4913642406463623
Validation loss: 2.14553435643514

Epoch: 6| Step: 13
Training loss: 1.5461616516113281
Validation loss: 2.170424540837606

Epoch: 249| Step: 0
Training loss: 2.6025490760803223
Validation loss: 2.176693240801493

Epoch: 6| Step: 1
Training loss: 1.8234367370605469
Validation loss: 2.1984750827153525

Epoch: 6| Step: 2
Training loss: 1.7114380598068237
Validation loss: 2.2131268978118896

Epoch: 6| Step: 3
Training loss: 1.7257229089736938
Validation loss: 2.200991690158844

Epoch: 6| Step: 4
Training loss: 1.8723450899124146
Validation loss: 2.2266407012939453

Epoch: 6| Step: 5
Training loss: 2.2105236053466797
Validation loss: 2.2291117906570435

Epoch: 6| Step: 6
Training loss: 1.517444133758545
Validation loss: 2.216908017794291

Epoch: 6| Step: 7
Training loss: 1.1499184370040894
Validation loss: 2.2236955960591636

Epoch: 6| Step: 8
Training loss: 1.5320745706558228
Validation loss: 2.2166966994603476

Epoch: 6| Step: 9
Training loss: 1.3377532958984375
Validation loss: 2.183015545209249

Epoch: 6| Step: 10
Training loss: 1.8055691719055176
Validation loss: 2.1892157594362893

Epoch: 6| Step: 11
Training loss: 2.2360239028930664
Validation loss: 2.173582911491394

Epoch: 6| Step: 12
Training loss: 0.768839955329895
Validation loss: 2.1791775822639465

Epoch: 6| Step: 13
Training loss: 1.616051435470581
Validation loss: 2.170536915461222

Epoch: 250| Step: 0
Training loss: 1.7089728116989136
Validation loss: 2.2037493189175925

Epoch: 6| Step: 1
Training loss: 1.577575445175171
Validation loss: 2.1889358957608542

Epoch: 6| Step: 2
Training loss: 1.6857839822769165
Validation loss: 2.205876588821411

Epoch: 6| Step: 3
Training loss: 1.7558115720748901
Validation loss: 2.1632956067721048

Epoch: 6| Step: 4
Training loss: 1.8821735382080078
Validation loss: 2.198485533396403

Epoch: 6| Step: 5
Training loss: 1.3688602447509766
Validation loss: 2.183930238087972

Epoch: 6| Step: 6
Training loss: 1.394888162612915
Validation loss: 2.1911861101786294

Epoch: 6| Step: 7
Training loss: 2.2311272621154785
Validation loss: 2.2052534222602844

Epoch: 6| Step: 8
Training loss: 1.5091679096221924
Validation loss: 2.1832926670710244

Epoch: 6| Step: 9
Training loss: 1.2980222702026367
Validation loss: 2.1765575210253396

Epoch: 6| Step: 10
Training loss: 1.7071361541748047
Validation loss: 2.160789728164673

Epoch: 6| Step: 11
Training loss: 2.0391013622283936
Validation loss: 2.179950992266337

Epoch: 6| Step: 12
Training loss: 1.9198576211929321
Validation loss: 2.1424031257629395

Epoch: 6| Step: 13
Training loss: 1.5452349185943604
Validation loss: 2.1757232944170632

Testing loss: 1.9065459049005302

Epoch: 1| Step: 0
Training loss: 4.723201274871826
Validation loss: 5.322232564290364

Epoch: 6| Step: 1
Training loss: 5.452666282653809
Validation loss: 5.320375045140584

Epoch: 6| Step: 2
Training loss: 4.931976795196533
Validation loss: 5.31842056910197

Epoch: 6| Step: 3
Training loss: 5.517801284790039
Validation loss: 5.316705067952474

Epoch: 6| Step: 4
Training loss: 6.296891212463379
Validation loss: 5.31495459874471

Epoch: 6| Step: 5
Training loss: 5.907551288604736
Validation loss: 5.3132898807525635

Epoch: 6| Step: 6
Training loss: 5.524905681610107
Validation loss: 5.311539808909099

Epoch: 6| Step: 7
Training loss: 5.941928863525391
Validation loss: 5.31000296274821

Epoch: 6| Step: 8
Training loss: 5.516753196716309
Validation loss: 5.308350245157878

Epoch: 6| Step: 9
Training loss: 4.158390998840332
Validation loss: 5.306656201680501

Epoch: 6| Step: 10
Training loss: 5.499833106994629
Validation loss: 5.304985523223877

Epoch: 6| Step: 11
Training loss: 6.030055522918701
Validation loss: 5.30315883954366

Epoch: 6| Step: 12
Training loss: 4.420436859130859
Validation loss: 5.301343997319539

Epoch: 6| Step: 13
Training loss: 5.393009185791016
Validation loss: 5.2993888060251875

Epoch: 2| Step: 0
Training loss: 5.785361289978027
Validation loss: 5.297405322392781

Epoch: 6| Step: 1
Training loss: 5.820591926574707
Validation loss: 5.295276244481404

Epoch: 6| Step: 2
Training loss: 4.66094970703125
Validation loss: 5.293097416559855

Epoch: 6| Step: 3
Training loss: 4.867249011993408
Validation loss: 5.290885289510091

Epoch: 6| Step: 4
Training loss: 5.183774948120117
Validation loss: 5.288462479909261

Epoch: 6| Step: 5
Training loss: 4.900167942047119
Validation loss: 5.285949468612671

Epoch: 6| Step: 6
Training loss: 5.467139720916748
Validation loss: 5.2833006381988525

Epoch: 6| Step: 7
Training loss: 5.467447280883789
Validation loss: 5.280534029006958

Epoch: 6| Step: 8
Training loss: 6.572950839996338
Validation loss: 5.277640183766683

Epoch: 6| Step: 9
Training loss: 4.19138765335083
Validation loss: 5.274596452713013

Epoch: 6| Step: 10
Training loss: 4.333054542541504
Validation loss: 5.27126669883728

Epoch: 6| Step: 11
Training loss: 5.554972171783447
Validation loss: 5.267650286356608

Epoch: 6| Step: 12
Training loss: 6.359724044799805
Validation loss: 5.26412034034729

Epoch: 6| Step: 13
Training loss: 5.734230041503906
Validation loss: 5.260101238886516

Epoch: 3| Step: 0
Training loss: 6.3462724685668945
Validation loss: 5.256107568740845

Epoch: 6| Step: 1
Training loss: 5.456239700317383
Validation loss: 5.251717805862427

Epoch: 6| Step: 2
Training loss: 5.137755393981934
Validation loss: 5.246949116388957

Epoch: 6| Step: 3
Training loss: 4.795192718505859
Validation loss: 5.24198842048645

Epoch: 6| Step: 4
Training loss: 6.0471367835998535
Validation loss: 5.236748298009236

Epoch: 6| Step: 5
Training loss: 5.543769836425781
Validation loss: 5.231291611989339

Epoch: 6| Step: 6
Training loss: 4.8422017097473145
Validation loss: 5.2256035804748535

Epoch: 6| Step: 7
Training loss: 5.474582195281982
Validation loss: 5.219407399495442

Epoch: 6| Step: 8
Training loss: 4.918659687042236
Validation loss: 5.212804079055786

Epoch: 6| Step: 9
Training loss: 4.969339847564697
Validation loss: 5.206088304519653

Epoch: 6| Step: 10
Training loss: 5.309317588806152
Validation loss: 5.1990125974019366

Epoch: 6| Step: 11
Training loss: 4.538392543792725
Validation loss: 5.191368023554484

Epoch: 6| Step: 12
Training loss: 5.286308765411377
Validation loss: 5.1836572488149

Epoch: 6| Step: 13
Training loss: 5.392616271972656
Validation loss: 5.175111611684163

Epoch: 4| Step: 0
Training loss: 4.706988334655762
Validation loss: 5.166706085205078

Epoch: 6| Step: 1
Training loss: 4.798028945922852
Validation loss: 5.157598535219829

Epoch: 6| Step: 2
Training loss: 6.155466079711914
Validation loss: 5.148417075475057

Epoch: 6| Step: 3
Training loss: 5.030497074127197
Validation loss: 5.13867712020874

Epoch: 6| Step: 4
Training loss: 5.323155403137207
Validation loss: 5.128507852554321

Epoch: 6| Step: 5
Training loss: 5.375244140625
Validation loss: 5.118072350819905

Epoch: 6| Step: 6
Training loss: 5.320441722869873
Validation loss: 5.107091506322225

Epoch: 6| Step: 7
Training loss: 4.7463226318359375
Validation loss: 5.096340974171956

Epoch: 6| Step: 8
Training loss: 4.853655815124512
Validation loss: 5.085034608840942

Epoch: 6| Step: 9
Training loss: 4.92518424987793
Validation loss: 5.07397214571635

Epoch: 6| Step: 10
Training loss: 4.83098030090332
Validation loss: 5.062370936075847

Epoch: 6| Step: 11
Training loss: 4.779640197753906
Validation loss: 5.050680875778198

Epoch: 6| Step: 12
Training loss: 5.091190814971924
Validation loss: 5.038699229558309

Epoch: 6| Step: 13
Training loss: 6.5034942626953125
Validation loss: 5.026562372843425

Epoch: 5| Step: 0
Training loss: 5.379424095153809
Validation loss: 5.01452898979187

Epoch: 6| Step: 1
Training loss: 5.216500282287598
Validation loss: 5.001919428507487

Epoch: 6| Step: 2
Training loss: 4.204572677612305
Validation loss: 4.989327669143677

Epoch: 6| Step: 3
Training loss: 5.102624416351318
Validation loss: 4.977025826772054

Epoch: 6| Step: 4
Training loss: 5.073099136352539
Validation loss: 4.964524666468303

Epoch: 6| Step: 5
Training loss: 5.196471691131592
Validation loss: 4.951141158739726

Epoch: 6| Step: 6
Training loss: 4.914783954620361
Validation loss: 4.938181320826213

Epoch: 6| Step: 7
Training loss: 5.811176300048828
Validation loss: 4.925708293914795

Epoch: 6| Step: 8
Training loss: 4.528724193572998
Validation loss: 4.912576834360759

Epoch: 6| Step: 9
Training loss: 4.337704658508301
Validation loss: 4.900140603383382

Epoch: 6| Step: 10
Training loss: 5.939373016357422
Validation loss: 4.888615210851033

Epoch: 6| Step: 11
Training loss: 5.048755645751953
Validation loss: 4.876453320185344

Epoch: 6| Step: 12
Training loss: 4.1000165939331055
Validation loss: 4.865166664123535

Epoch: 6| Step: 13
Training loss: 5.378994941711426
Validation loss: 4.853444258371989

Epoch: 6| Step: 0
Training loss: 5.7209367752075195
Validation loss: 4.842899243036906

Epoch: 6| Step: 1
Training loss: 5.327507019042969
Validation loss: 4.832154115041097

Epoch: 6| Step: 2
Training loss: 4.5900397300720215
Validation loss: 4.821652332941691

Epoch: 6| Step: 3
Training loss: 5.648756504058838
Validation loss: 4.810769160588582

Epoch: 6| Step: 4
Training loss: 4.15616512298584
Validation loss: 4.800482670466105

Epoch: 6| Step: 5
Training loss: 5.185211658477783
Validation loss: 4.790460864702861

Epoch: 6| Step: 6
Training loss: 4.881926536560059
Validation loss: 4.7805453936258955

Epoch: 6| Step: 7
Training loss: 5.133360862731934
Validation loss: 4.770766814549764

Epoch: 6| Step: 8
Training loss: 5.080075263977051
Validation loss: 4.760742584864299

Epoch: 6| Step: 9
Training loss: 3.805227756500244
Validation loss: 4.751225749651591

Epoch: 6| Step: 10
Training loss: 4.291460037231445
Validation loss: 4.74194860458374

Epoch: 6| Step: 11
Training loss: 5.071456432342529
Validation loss: 4.732431968053182

Epoch: 6| Step: 12
Training loss: 5.8088579177856445
Validation loss: 4.723211447397868

Epoch: 6| Step: 13
Training loss: 3.4387288093566895
Validation loss: 4.713956594467163

Epoch: 7| Step: 0
Training loss: 5.028169631958008
Validation loss: 4.704919735590617

Epoch: 6| Step: 1
Training loss: 5.064306735992432
Validation loss: 4.69602636496226

Epoch: 6| Step: 2
Training loss: 5.324102401733398
Validation loss: 4.687340656916301

Epoch: 6| Step: 3
Training loss: 4.600276470184326
Validation loss: 4.678010702133179

Epoch: 6| Step: 4
Training loss: 3.788386821746826
Validation loss: 4.668930093447368

Epoch: 6| Step: 5
Training loss: 4.95013952255249
Validation loss: 4.660296678543091

Epoch: 6| Step: 6
Training loss: 5.182950019836426
Validation loss: 4.650956352551778

Epoch: 6| Step: 7
Training loss: 3.767186164855957
Validation loss: 4.640837828318278

Epoch: 6| Step: 8
Training loss: 4.847129821777344
Validation loss: 4.63197127978007

Epoch: 6| Step: 9
Training loss: 4.458113193511963
Validation loss: 4.622097333272298

Epoch: 6| Step: 10
Training loss: 6.172997951507568
Validation loss: 4.613283157348633

Epoch: 6| Step: 11
Training loss: 4.2733049392700195
Validation loss: 4.6045699914296465

Epoch: 6| Step: 12
Training loss: 4.84864616394043
Validation loss: 4.5950376987457275

Epoch: 6| Step: 13
Training loss: 4.084990501403809
Validation loss: 4.586204528808594

Epoch: 8| Step: 0
Training loss: 3.9262921810150146
Validation loss: 4.577839136123657

Epoch: 6| Step: 1
Training loss: 5.298250198364258
Validation loss: 4.569303671518962

Epoch: 6| Step: 2
Training loss: 4.414580345153809
Validation loss: 4.562346458435059

Epoch: 6| Step: 3
Training loss: 4.729301452636719
Validation loss: 4.554162581761678

Epoch: 6| Step: 4
Training loss: 5.491820335388184
Validation loss: 4.5464195013046265

Epoch: 6| Step: 5
Training loss: 4.539341926574707
Validation loss: 4.539192239443461

Epoch: 6| Step: 6
Training loss: 5.473726272583008
Validation loss: 4.532100598017375

Epoch: 6| Step: 7
Training loss: 4.119256496429443
Validation loss: 4.524345676104228

Epoch: 6| Step: 8
Training loss: 4.41819953918457
Validation loss: 4.517433722813924

Epoch: 6| Step: 9
Training loss: 4.302192687988281
Validation loss: 4.50999101003011

Epoch: 6| Step: 10
Training loss: 3.947845220565796
Validation loss: 4.502829392751058

Epoch: 6| Step: 11
Training loss: 5.363022804260254
Validation loss: 4.495077768961589

Epoch: 6| Step: 12
Training loss: 5.048178672790527
Validation loss: 4.48835810025533

Epoch: 6| Step: 13
Training loss: 3.7692902088165283
Validation loss: 4.481563170750936

Epoch: 9| Step: 0
Training loss: 4.747600078582764
Validation loss: 4.47448484102885

Epoch: 6| Step: 1
Training loss: 3.6128830909729004
Validation loss: 4.467543085416158

Epoch: 6| Step: 2
Training loss: 5.256560325622559
Validation loss: 4.460904399553935

Epoch: 6| Step: 3
Training loss: 5.835242748260498
Validation loss: 4.4541566371917725

Epoch: 6| Step: 4
Training loss: 4.125480651855469
Validation loss: 4.447214603424072

Epoch: 6| Step: 5
Training loss: 3.652897357940674
Validation loss: 4.44072167078654

Epoch: 6| Step: 6
Training loss: 4.234439849853516
Validation loss: 4.434407075246175

Epoch: 6| Step: 7
Training loss: 4.576512813568115
Validation loss: 4.428399205207825

Epoch: 6| Step: 8
Training loss: 4.638985633850098
Validation loss: 4.421645005544026

Epoch: 6| Step: 9
Training loss: 5.3879289627075195
Validation loss: 4.415720105171204

Epoch: 6| Step: 10
Training loss: 4.078403472900391
Validation loss: 4.4088995059331255

Epoch: 6| Step: 11
Training loss: 4.5657758712768555
Validation loss: 4.4029877583185835

Epoch: 6| Step: 12
Training loss: 5.199967384338379
Validation loss: 4.397139191627502

Epoch: 6| Step: 13
Training loss: 3.7280590534210205
Validation loss: 4.390743851661682

Epoch: 10| Step: 0
Training loss: 3.1952590942382812
Validation loss: 4.384468833605449

Epoch: 6| Step: 1
Training loss: 5.231800079345703
Validation loss: 4.37838872273763

Epoch: 6| Step: 2
Training loss: 4.5225067138671875
Validation loss: 4.371757984161377

Epoch: 6| Step: 3
Training loss: 3.460475444793701
Validation loss: 4.365718046824138

Epoch: 6| Step: 4
Training loss: 4.273336410522461
Validation loss: 4.359195947647095

Epoch: 6| Step: 5
Training loss: 4.384232521057129
Validation loss: 4.353458960851033

Epoch: 6| Step: 6
Training loss: 4.9217987060546875
Validation loss: 4.3476715087890625

Epoch: 6| Step: 7
Training loss: 3.6557717323303223
Validation loss: 4.3418393929799395

Epoch: 6| Step: 8
Training loss: 5.262904167175293
Validation loss: 4.335867881774902

Epoch: 6| Step: 9
Training loss: 4.492650985717773
Validation loss: 4.330007632573445

Epoch: 6| Step: 10
Training loss: 5.0874481201171875
Validation loss: 4.324314991633098

Epoch: 6| Step: 11
Training loss: 5.119056224822998
Validation loss: 4.318254152933757

Epoch: 6| Step: 12
Training loss: 4.68574333190918
Validation loss: 4.312968015670776

Epoch: 6| Step: 13
Training loss: 4.2639312744140625
Validation loss: 4.306721766789754

Epoch: 11| Step: 0
Training loss: 4.5986528396606445
Validation loss: 4.300628622372945

Epoch: 6| Step: 1
Training loss: 3.730724573135376
Validation loss: 4.295008142789205

Epoch: 6| Step: 2
Training loss: 4.03397274017334
Validation loss: 4.289501865704854

Epoch: 6| Step: 3
Training loss: 4.390584468841553
Validation loss: 4.283457358678182

Epoch: 6| Step: 4
Training loss: 4.9636149406433105
Validation loss: 4.277325550715129

Epoch: 6| Step: 5
Training loss: 2.7990756034851074
Validation loss: 4.271005829175313

Epoch: 6| Step: 6
Training loss: 3.7167742252349854
Validation loss: 4.266136725743611

Epoch: 6| Step: 7
Training loss: 4.343758583068848
Validation loss: 4.260217666625977

Epoch: 6| Step: 8
Training loss: 5.829432010650635
Validation loss: 4.254580656687419

Epoch: 6| Step: 9
Training loss: 5.034450531005859
Validation loss: 4.248672366142273

Epoch: 6| Step: 10
Training loss: 4.240043640136719
Validation loss: 4.242990056673686

Epoch: 6| Step: 11
Training loss: 4.690535545349121
Validation loss: 4.237037897109985

Epoch: 6| Step: 12
Training loss: 3.738717555999756
Validation loss: 4.231155117352803

Epoch: 6| Step: 13
Training loss: 5.381399631500244
Validation loss: 4.225817044576009

Epoch: 12| Step: 0
Training loss: 3.776060104370117
Validation loss: 4.219836751619975

Epoch: 6| Step: 1
Training loss: 3.900458812713623
Validation loss: 4.21387795607249

Epoch: 6| Step: 2
Training loss: 3.178760051727295
Validation loss: 4.2075759172439575

Epoch: 6| Step: 3
Training loss: 4.739258289337158
Validation loss: 4.202229102452596

Epoch: 6| Step: 4
Training loss: 5.358780860900879
Validation loss: 4.196458339691162

Epoch: 6| Step: 5
Training loss: 4.281269550323486
Validation loss: 4.1906200249989825

Epoch: 6| Step: 6
Training loss: 4.953176975250244
Validation loss: 4.185157497723897

Epoch: 6| Step: 7
Training loss: 4.395732879638672
Validation loss: 4.178840359052022

Epoch: 6| Step: 8
Training loss: 4.531342029571533
Validation loss: 4.172628879547119

Epoch: 6| Step: 9
Training loss: 4.656429767608643
Validation loss: 4.167973677317302

Epoch: 6| Step: 10
Training loss: 4.193470001220703
Validation loss: 4.161792357762654

Epoch: 6| Step: 11
Training loss: 3.716890811920166
Validation loss: 4.155488689740499

Epoch: 6| Step: 12
Training loss: 4.471612930297852
Validation loss: 4.149847666422526

Epoch: 6| Step: 13
Training loss: 4.30352783203125
Validation loss: 4.144267757733663

Epoch: 13| Step: 0
Training loss: 4.1871442794799805
Validation loss: 4.138290087381999

Epoch: 6| Step: 1
Training loss: 3.930689811706543
Validation loss: 4.132207751274109

Epoch: 6| Step: 2
Training loss: 3.759922981262207
Validation loss: 4.126622120539348

Epoch: 6| Step: 3
Training loss: 3.677497386932373
Validation loss: 4.120642185211182

Epoch: 6| Step: 4
Training loss: 4.416674613952637
Validation loss: 4.115822156270345

Epoch: 6| Step: 5
Training loss: 4.1068878173828125
Validation loss: 4.10992689927419

Epoch: 6| Step: 6
Training loss: 4.282927513122559
Validation loss: 4.103437066078186

Epoch: 6| Step: 7
Training loss: 4.654863357543945
Validation loss: 4.0979805787404375

Epoch: 6| Step: 8
Training loss: 3.843708038330078
Validation loss: 4.0921434958775835

Epoch: 6| Step: 9
Training loss: 4.446110725402832
Validation loss: 4.085647424062093

Epoch: 6| Step: 10
Training loss: 4.824283123016357
Validation loss: 4.080177386601766

Epoch: 6| Step: 11
Training loss: 4.720465660095215
Validation loss: 4.074441035588582

Epoch: 6| Step: 12
Training loss: 4.11264181137085
Validation loss: 4.067874908447266

Epoch: 6| Step: 13
Training loss: 4.422211647033691
Validation loss: 4.0623505512873335

Epoch: 14| Step: 0
Training loss: 4.2368621826171875
Validation loss: 4.056449095408122

Epoch: 6| Step: 1
Training loss: 4.5950608253479
Validation loss: 4.050140380859375

Epoch: 6| Step: 2
Training loss: 4.651947975158691
Validation loss: 4.043820420900981

Epoch: 6| Step: 3
Training loss: 3.6316120624542236
Validation loss: 4.03820554415385

Epoch: 6| Step: 4
Training loss: 4.6049909591674805
Validation loss: 4.03337550163269

Epoch: 6| Step: 5
Training loss: 3.846544027328491
Validation loss: 4.02723769346873

Epoch: 6| Step: 6
Training loss: 4.413802623748779
Validation loss: 4.020148277282715

Epoch: 6| Step: 7
Training loss: 3.840024948120117
Validation loss: 4.013394117355347

Epoch: 6| Step: 8
Training loss: 4.961987495422363
Validation loss: 4.007252017656962

Epoch: 6| Step: 9
Training loss: 4.113826274871826
Validation loss: 4.0017805099487305

Epoch: 6| Step: 10
Training loss: 3.4283552169799805
Validation loss: 3.9952905575434365

Epoch: 6| Step: 11
Training loss: 3.0010361671447754
Validation loss: 3.9888686736424765

Epoch: 6| Step: 12
Training loss: 4.256852149963379
Validation loss: 3.982997179031372

Epoch: 6| Step: 13
Training loss: 4.71340799331665
Validation loss: 3.9777386585871377

Epoch: 15| Step: 0
Training loss: 4.335054874420166
Validation loss: 3.971860965092977

Epoch: 6| Step: 1
Training loss: 4.321758270263672
Validation loss: 3.9659535884857178

Epoch: 6| Step: 2
Training loss: 4.161774635314941
Validation loss: 3.960576136906942

Epoch: 6| Step: 3
Training loss: 4.272983551025391
Validation loss: 3.9548439582188926

Epoch: 6| Step: 4
Training loss: 3.5826878547668457
Validation loss: 3.9476540883382163

Epoch: 6| Step: 5
Training loss: 3.10365891456604
Validation loss: 3.9427505334218345

Epoch: 6| Step: 6
Training loss: 3.835163116455078
Validation loss: 3.9382903973261514

Epoch: 6| Step: 7
Training loss: 5.160279273986816
Validation loss: 3.9314170678456626

Epoch: 6| Step: 8
Training loss: 3.91107439994812
Validation loss: 3.9248411655426025

Epoch: 6| Step: 9
Training loss: 4.474983215332031
Validation loss: 3.9195146958033242

Epoch: 6| Step: 10
Training loss: 4.209538459777832
Validation loss: 3.913028875986735

Epoch: 6| Step: 11
Training loss: 4.167375564575195
Validation loss: 3.9069403409957886

Epoch: 6| Step: 12
Training loss: 4.319879531860352
Validation loss: 3.9017581144968667

Epoch: 6| Step: 13
Training loss: 3.311119556427002
Validation loss: 3.8953179121017456

Epoch: 16| Step: 0
Training loss: 4.104543209075928
Validation loss: 3.8899030288060508

Epoch: 6| Step: 1
Training loss: 4.320959091186523
Validation loss: 3.8848811388015747

Epoch: 6| Step: 2
Training loss: 3.8035926818847656
Validation loss: 3.8796340227127075

Epoch: 6| Step: 3
Training loss: 3.69103741645813
Validation loss: 3.874031980832418

Epoch: 6| Step: 4
Training loss: 4.1068644523620605
Validation loss: 3.8676697810490928

Epoch: 6| Step: 5
Training loss: 3.332400321960449
Validation loss: 3.862232724825541

Epoch: 6| Step: 6
Training loss: 3.3313217163085938
Validation loss: 3.856911778450012

Epoch: 6| Step: 7
Training loss: 4.456695556640625
Validation loss: 3.851711948712667

Epoch: 6| Step: 8
Training loss: 4.5528974533081055
Validation loss: 3.844581961631775

Epoch: 6| Step: 9
Training loss: 4.5267839431762695
Validation loss: 3.8391801516215005

Epoch: 6| Step: 10
Training loss: 3.688765525817871
Validation loss: 3.8326573769251504

Epoch: 6| Step: 11
Training loss: 3.659924030303955
Validation loss: 3.8275657892227173

Epoch: 6| Step: 12
Training loss: 4.535092353820801
Validation loss: 3.8217594623565674

Epoch: 6| Step: 13
Training loss: 3.9606235027313232
Validation loss: 3.8161381483078003

Epoch: 17| Step: 0
Training loss: 3.976367950439453
Validation loss: 3.809977650642395

Epoch: 6| Step: 1
Training loss: 3.78054141998291
Validation loss: 3.8043128649393716

Epoch: 6| Step: 2
Training loss: 3.7656455039978027
Validation loss: 3.799916625022888

Epoch: 6| Step: 3
Training loss: 3.6107494831085205
Validation loss: 3.794777830441793

Epoch: 6| Step: 4
Training loss: 4.463552951812744
Validation loss: 3.7882845401763916

Epoch: 6| Step: 5
Training loss: 3.65458607673645
Validation loss: 3.782803694407145

Epoch: 6| Step: 6
Training loss: 3.292576789855957
Validation loss: 3.7774654626846313

Epoch: 6| Step: 7
Training loss: 4.253573417663574
Validation loss: 3.771884044011434

Epoch: 6| Step: 8
Training loss: 3.3045811653137207
Validation loss: 3.7654652198155723

Epoch: 6| Step: 9
Training loss: 4.328499794006348
Validation loss: 3.7600433429082236

Epoch: 6| Step: 10
Training loss: 4.381006240844727
Validation loss: 3.754115581512451

Epoch: 6| Step: 11
Training loss: 3.4918718338012695
Validation loss: 3.748975475629171

Epoch: 6| Step: 12
Training loss: 4.659750938415527
Validation loss: 3.746755003929138

Epoch: 6| Step: 13
Training loss: 4.086361885070801
Validation loss: 3.739124576250712

Epoch: 18| Step: 0
Training loss: 3.9033327102661133
Validation loss: 3.733935554822286

Epoch: 6| Step: 1
Training loss: 3.5367178916931152
Validation loss: 3.7293102741241455

Epoch: 6| Step: 2
Training loss: 3.3347129821777344
Validation loss: 3.7234251499176025

Epoch: 6| Step: 3
Training loss: 4.127188682556152
Validation loss: 3.7182371616363525

Epoch: 6| Step: 4
Training loss: 4.489684104919434
Validation loss: 3.7129276593526206

Epoch: 6| Step: 5
Training loss: 3.742553234100342
Validation loss: 3.707018494606018

Epoch: 6| Step: 6
Training loss: 3.325500965118408
Validation loss: 3.702308019002279

Epoch: 6| Step: 7
Training loss: 3.451990842819214
Validation loss: 3.6974774996439614

Epoch: 6| Step: 8
Training loss: 3.9718127250671387
Validation loss: 3.69268798828125

Epoch: 6| Step: 9
Training loss: 4.982503414154053
Validation loss: 3.687316338221232

Epoch: 6| Step: 10
Training loss: 3.484060764312744
Validation loss: 3.6825083096822104

Epoch: 6| Step: 11
Training loss: 3.9178597927093506
Validation loss: 3.6769099632898965

Epoch: 6| Step: 12
Training loss: 3.6456315517425537
Validation loss: 3.672766168912252

Epoch: 6| Step: 13
Training loss: 4.146615982055664
Validation loss: 3.6678537130355835

Epoch: 19| Step: 0
Training loss: 3.857750177383423
Validation loss: 3.662281354268392

Epoch: 6| Step: 1
Training loss: 3.519620418548584
Validation loss: 3.657443046569824

Epoch: 6| Step: 2
Training loss: 3.142359733581543
Validation loss: 3.651628017425537

Epoch: 6| Step: 3
Training loss: 3.3361828327178955
Validation loss: 3.6470365126927695

Epoch: 6| Step: 4
Training loss: 3.6685242652893066
Validation loss: 3.641851544380188

Epoch: 6| Step: 5
Training loss: 3.841552972793579
Validation loss: 3.6375865936279297

Epoch: 6| Step: 6
Training loss: 4.058387756347656
Validation loss: 3.63297704855601

Epoch: 6| Step: 7
Training loss: 4.6896562576293945
Validation loss: 3.6276355187098184

Epoch: 6| Step: 8
Training loss: 4.242359161376953
Validation loss: 3.622257431348165

Epoch: 6| Step: 9
Training loss: 3.763153553009033
Validation loss: 3.617891550064087

Epoch: 6| Step: 10
Training loss: 3.9995384216308594
Validation loss: 3.6123955647150674

Epoch: 6| Step: 11
Training loss: 3.7756905555725098
Validation loss: 3.60719629128774

Epoch: 6| Step: 12
Training loss: 2.7101171016693115
Validation loss: 3.6029109954833984

Epoch: 6| Step: 13
Training loss: 4.477535247802734
Validation loss: 3.596827427546183

Epoch: 20| Step: 0
Training loss: 3.5919971466064453
Validation loss: 3.5921968619028726

Epoch: 6| Step: 1
Training loss: 3.334803342819214
Validation loss: 3.586635947227478

Epoch: 6| Step: 2
Training loss: 3.384080410003662
Validation loss: 3.5820654233296714

Epoch: 6| Step: 3
Training loss: 4.315107345581055
Validation loss: 3.577325781186422

Epoch: 6| Step: 4
Training loss: 3.5520472526550293
Validation loss: 3.5718624194463096

Epoch: 6| Step: 5
Training loss: 4.003536701202393
Validation loss: 3.5671914418538413

Epoch: 6| Step: 6
Training loss: 3.2197341918945312
Validation loss: 3.5626597801844277

Epoch: 6| Step: 7
Training loss: 4.839179992675781
Validation loss: 3.5572230021158853

Epoch: 6| Step: 8
Training loss: 4.555848121643066
Validation loss: 3.5519033670425415

Epoch: 6| Step: 9
Training loss: 2.8574070930480957
Validation loss: 3.5461347500483194

Epoch: 6| Step: 10
Training loss: 3.033902406692505
Validation loss: 3.540454069773356

Epoch: 6| Step: 11
Training loss: 4.502199172973633
Validation loss: 3.535908301671346

Epoch: 6| Step: 12
Training loss: 3.6753475666046143
Validation loss: 3.531606396039327

Epoch: 6| Step: 13
Training loss: 3.244391441345215
Validation loss: 3.5253469546635947

Epoch: 21| Step: 0
Training loss: 3.8483972549438477
Validation loss: 3.520722985267639

Epoch: 6| Step: 1
Training loss: 3.6996333599090576
Validation loss: 3.5140017668406167

Epoch: 6| Step: 2
Training loss: 3.694831371307373
Validation loss: 3.509183923403422

Epoch: 6| Step: 3
Training loss: 2.884824275970459
Validation loss: 3.504356344540914

Epoch: 6| Step: 4
Training loss: 3.074786901473999
Validation loss: 3.4990007877349854

Epoch: 6| Step: 5
Training loss: 3.857123374938965
Validation loss: 3.494091192881266

Epoch: 6| Step: 6
Training loss: 4.030762672424316
Validation loss: 3.4900625149408975

Epoch: 6| Step: 7
Training loss: 4.336160659790039
Validation loss: 3.484716773033142

Epoch: 6| Step: 8
Training loss: 3.8419220447540283
Validation loss: 3.479209860165914

Epoch: 6| Step: 9
Training loss: 3.924225330352783
Validation loss: 3.4731334447860718

Epoch: 6| Step: 10
Training loss: 3.717909336090088
Validation loss: 3.469280163447062

Epoch: 6| Step: 11
Training loss: 3.667933940887451
Validation loss: 3.4651074012120566

Epoch: 6| Step: 12
Training loss: 3.6430506706237793
Validation loss: 3.459967295328776

Epoch: 6| Step: 13
Training loss: 2.908865451812744
Validation loss: 3.4540621439615884

Epoch: 22| Step: 0
Training loss: 2.6039724349975586
Validation loss: 3.447749694188436

Epoch: 6| Step: 1
Training loss: 5.294639587402344
Validation loss: 3.4427730639775596

Epoch: 6| Step: 2
Training loss: 3.0259597301483154
Validation loss: 3.4382009506225586

Epoch: 6| Step: 3
Training loss: 2.9420957565307617
Validation loss: 3.4331223169962564

Epoch: 6| Step: 4
Training loss: 3.55415415763855
Validation loss: 3.427878419558207

Epoch: 6| Step: 5
Training loss: 3.5922932624816895
Validation loss: 3.422072490056356

Epoch: 6| Step: 6
Training loss: 3.3386926651000977
Validation loss: 3.4175670544306436

Epoch: 6| Step: 7
Training loss: 3.598249673843384
Validation loss: 3.4125452041625977

Epoch: 6| Step: 8
Training loss: 3.860785484313965
Validation loss: 3.4075942834218345

Epoch: 6| Step: 9
Training loss: 3.9276063442230225
Validation loss: 3.404204328854879

Epoch: 6| Step: 10
Training loss: 4.273464202880859
Validation loss: 3.39828093846639

Epoch: 6| Step: 11
Training loss: 3.4846739768981934
Validation loss: 3.3925756216049194

Epoch: 6| Step: 12
Training loss: 3.9640486240386963
Validation loss: 3.388089974721273

Epoch: 6| Step: 13
Training loss: 2.7070770263671875
Validation loss: 3.3826414346694946

Epoch: 23| Step: 0
Training loss: 3.8127219676971436
Validation loss: 3.3771371841430664

Epoch: 6| Step: 1
Training loss: 3.6885476112365723
Validation loss: 3.3714096546173096

Epoch: 6| Step: 2
Training loss: 3.145376205444336
Validation loss: 3.367420514424642

Epoch: 6| Step: 3
Training loss: 3.545036792755127
Validation loss: 3.362122734387716

Epoch: 6| Step: 4
Training loss: 2.514000415802002
Validation loss: 3.3561367988586426

Epoch: 6| Step: 5
Training loss: 3.3837928771972656
Validation loss: 3.3509040673573813

Epoch: 6| Step: 6
Training loss: 3.105909585952759
Validation loss: 3.3466376860936484

Epoch: 6| Step: 7
Training loss: 3.8136043548583984
Validation loss: 3.3421274423599243

Epoch: 6| Step: 8
Training loss: 3.1984171867370605
Validation loss: 3.3355799118677774

Epoch: 6| Step: 9
Training loss: 4.352424621582031
Validation loss: 3.3315665324529014

Epoch: 6| Step: 10
Training loss: 3.8407726287841797
Validation loss: 3.3266438245773315

Epoch: 6| Step: 11
Training loss: 3.41459584236145
Validation loss: 3.322015722592672

Epoch: 6| Step: 12
Training loss: 4.033720016479492
Validation loss: 3.315927823384603

Epoch: 6| Step: 13
Training loss: 3.3487181663513184
Validation loss: 3.311154921849569

Epoch: 24| Step: 0
Training loss: 3.455496072769165
Validation loss: 3.306257446606954

Epoch: 6| Step: 1
Training loss: 3.910935401916504
Validation loss: 3.3013322353363037

Epoch: 6| Step: 2
Training loss: 2.675842761993408
Validation loss: 3.2960591316223145

Epoch: 6| Step: 3
Training loss: 3.712287425994873
Validation loss: 3.2913558880488076

Epoch: 6| Step: 4
Training loss: 3.6888577938079834
Validation loss: 3.28571085135142

Epoch: 6| Step: 5
Training loss: 4.311666965484619
Validation loss: 3.2818210124969482

Epoch: 6| Step: 6
Training loss: 3.10597825050354
Validation loss: 3.2765386501948037

Epoch: 6| Step: 7
Training loss: 3.2636361122131348
Validation loss: 3.2719173034032187

Epoch: 6| Step: 8
Training loss: 2.2799344062805176
Validation loss: 3.267334779103597

Epoch: 6| Step: 9
Training loss: 3.602062702178955
Validation loss: 3.263326366742452

Epoch: 6| Step: 10
Training loss: 3.6504571437835693
Validation loss: 3.2589842875798545

Epoch: 6| Step: 11
Training loss: 4.422591209411621
Validation loss: 3.2543369928995767

Epoch: 6| Step: 12
Training loss: 3.4158456325531006
Validation loss: 3.2501962184906006

Epoch: 6| Step: 13
Training loss: 2.8123998641967773
Validation loss: 3.2447681427001953

Epoch: 25| Step: 0
Training loss: 3.8064727783203125
Validation loss: 3.240809520085653

Epoch: 6| Step: 1
Training loss: 3.6746044158935547
Validation loss: 3.2364046971003213

Epoch: 6| Step: 2
Training loss: 3.325003147125244
Validation loss: 3.2316982746124268

Epoch: 6| Step: 3
Training loss: 2.936929702758789
Validation loss: 3.2262444496154785

Epoch: 6| Step: 4
Training loss: 2.9721131324768066
Validation loss: 3.221882939338684

Epoch: 6| Step: 5
Training loss: 4.294046878814697
Validation loss: 3.2170867919921875

Epoch: 6| Step: 6
Training loss: 4.147737503051758
Validation loss: 3.2118333180745444

Epoch: 6| Step: 7
Training loss: 2.280447483062744
Validation loss: 3.2065969705581665

Epoch: 6| Step: 8
Training loss: 3.5238735675811768
Validation loss: 3.2026904821395874

Epoch: 6| Step: 9
Training loss: 3.607884407043457
Validation loss: 3.1979602575302124

Epoch: 6| Step: 10
Training loss: 2.869873523712158
Validation loss: 3.192636171976725

Epoch: 6| Step: 11
Training loss: 3.7287440299987793
Validation loss: 3.1885554790496826

Epoch: 6| Step: 12
Training loss: 3.626831531524658
Validation loss: 3.184847911198934

Epoch: 6| Step: 13
Training loss: 2.697798252105713
Validation loss: 3.1801355282465615

Epoch: 26| Step: 0
Training loss: 3.506432294845581
Validation loss: 3.176144242286682

Epoch: 6| Step: 1
Training loss: 3.4885780811309814
Validation loss: 3.172098994255066

Epoch: 6| Step: 2
Training loss: 3.3536019325256348
Validation loss: 3.166685183842977

Epoch: 6| Step: 3
Training loss: 3.0648458003997803
Validation loss: 3.162307858467102

Epoch: 6| Step: 4
Training loss: 2.897552013397217
Validation loss: 3.15925669670105

Epoch: 6| Step: 5
Training loss: 3.1923959255218506
Validation loss: 3.155073364575704

Epoch: 6| Step: 6
Training loss: 2.9831128120422363
Validation loss: 3.1497796773910522

Epoch: 6| Step: 7
Training loss: 3.191016912460327
Validation loss: 3.146183172861735

Epoch: 6| Step: 8
Training loss: 3.3772950172424316
Validation loss: 3.1422367890675864

Epoch: 6| Step: 9
Training loss: 3.1728579998016357
Validation loss: 3.137890100479126

Epoch: 6| Step: 10
Training loss: 3.2535853385925293
Validation loss: 3.1350152095158896

Epoch: 6| Step: 11
Training loss: 3.6115543842315674
Validation loss: 3.1304656664530435

Epoch: 6| Step: 12
Training loss: 4.2978315353393555
Validation loss: 3.1267627080281577

Epoch: 6| Step: 13
Training loss: 3.2841796875
Validation loss: 3.122294863065084

Epoch: 27| Step: 0
Training loss: 2.573585033416748
Validation loss: 3.117325266202291

Epoch: 6| Step: 1
Training loss: 3.1030216217041016
Validation loss: 3.113343358039856

Epoch: 6| Step: 2
Training loss: 4.284913063049316
Validation loss: 3.110583027203878

Epoch: 6| Step: 3
Training loss: 3.55330753326416
Validation loss: 3.1064959367116294

Epoch: 6| Step: 4
Training loss: 3.290724277496338
Validation loss: 3.1033539374669394

Epoch: 6| Step: 5
Training loss: 3.0954322814941406
Validation loss: 3.0980480909347534

Epoch: 6| Step: 6
Training loss: 3.3769607543945312
Validation loss: 3.095217784245809

Epoch: 6| Step: 7
Training loss: 3.054865837097168
Validation loss: 3.091546813646952

Epoch: 6| Step: 8
Training loss: 2.7114858627319336
Validation loss: 3.089607357978821

Epoch: 6| Step: 9
Training loss: 3.238384485244751
Validation loss: 3.0881309111913047

Epoch: 6| Step: 10
Training loss: 2.957890510559082
Validation loss: 3.0807018280029297

Epoch: 6| Step: 11
Training loss: 3.6753015518188477
Validation loss: 3.0764108498891196

Epoch: 6| Step: 12
Training loss: 3.1289680004119873
Validation loss: 3.0737603108088174

Epoch: 6| Step: 13
Training loss: 3.8728621006011963
Validation loss: 3.0701626936594644

Epoch: 28| Step: 0
Training loss: 3.8174209594726562
Validation loss: 3.0665948390960693

Epoch: 6| Step: 1
Training loss: 3.000985622406006
Validation loss: 3.0626001358032227

Epoch: 6| Step: 2
Training loss: 3.1457061767578125
Validation loss: 3.0582011938095093

Epoch: 6| Step: 3
Training loss: 3.7865285873413086
Validation loss: 3.053607225418091

Epoch: 6| Step: 4
Training loss: 3.129650115966797
Validation loss: 3.049780090649923

Epoch: 6| Step: 5
Training loss: 2.920253276824951
Validation loss: 3.045493245124817

Epoch: 6| Step: 6
Training loss: 2.874124526977539
Validation loss: 3.0419536431630454

Epoch: 6| Step: 7
Training loss: 3.1732187271118164
Validation loss: 3.0384854475657144

Epoch: 6| Step: 8
Training loss: 3.737548351287842
Validation loss: 3.0348798433939614

Epoch: 6| Step: 9
Training loss: 2.8103623390197754
Validation loss: 3.031399726867676

Epoch: 6| Step: 10
Training loss: 3.347033977508545
Validation loss: 3.0267040729522705

Epoch: 6| Step: 11
Training loss: 3.2536234855651855
Validation loss: 3.022904634475708

Epoch: 6| Step: 12
Training loss: 2.890716552734375
Validation loss: 3.019852558771769

Epoch: 6| Step: 13
Training loss: 3.3570408821105957
Validation loss: 3.015673359235128

Epoch: 29| Step: 0
Training loss: 3.885551929473877
Validation loss: 3.0119518836339316

Epoch: 6| Step: 1
Training loss: 3.1058645248413086
Validation loss: 3.00803279876709

Epoch: 6| Step: 2
Training loss: 3.1870405673980713
Validation loss: 3.003535191218058

Epoch: 6| Step: 3
Training loss: 3.3109140396118164
Validation loss: 3.0005462964375815

Epoch: 6| Step: 4
Training loss: 2.9165308475494385
Validation loss: 2.995909571647644

Epoch: 6| Step: 5
Training loss: 3.7889564037323
Validation loss: 2.992178281148275

Epoch: 6| Step: 6
Training loss: 3.204141139984131
Validation loss: 2.9885767300923667

Epoch: 6| Step: 7
Training loss: 3.5822396278381348
Validation loss: 2.9849687814712524

Epoch: 6| Step: 8
Training loss: 3.7491044998168945
Validation loss: 2.980621576309204

Epoch: 6| Step: 9
Training loss: 3.086526870727539
Validation loss: 2.9770875771840415

Epoch: 6| Step: 10
Training loss: 2.356065273284912
Validation loss: 2.9729732672373452

Epoch: 6| Step: 11
Training loss: 3.182443141937256
Validation loss: 2.9692949056625366

Epoch: 6| Step: 12
Training loss: 2.089607000350952
Validation loss: 2.9659319718678794

Epoch: 6| Step: 13
Training loss: 3.1352455615997314
Validation loss: 2.9628304640452066

Epoch: 30| Step: 0
Training loss: 3.185059070587158
Validation loss: 2.9600284496943154

Epoch: 6| Step: 1
Training loss: 3.52500581741333
Validation loss: 2.956432859102885

Epoch: 6| Step: 2
Training loss: 3.4060652256011963
Validation loss: 2.95430580774943

Epoch: 6| Step: 3
Training loss: 3.3564813137054443
Validation loss: 2.9518368244171143

Epoch: 6| Step: 4
Training loss: 2.408036947250366
Validation loss: 2.955925146738688

Epoch: 6| Step: 5
Training loss: 2.8960938453674316
Validation loss: 2.9564236799875894

Epoch: 6| Step: 6
Training loss: 2.682114601135254
Validation loss: 2.947007139523824

Epoch: 6| Step: 7
Training loss: 3.4317495822906494
Validation loss: 2.9368293285369873

Epoch: 6| Step: 8
Training loss: 3.3421852588653564
Validation loss: 2.9361719687779746

Epoch: 6| Step: 9
Training loss: 2.863919734954834
Validation loss: 2.934601823488871

Epoch: 6| Step: 10
Training loss: 4.001258850097656
Validation loss: 2.930837074915568

Epoch: 6| Step: 11
Training loss: 2.3875415325164795
Validation loss: 2.9254641930262246

Epoch: 6| Step: 12
Training loss: 3.7798662185668945
Validation loss: 2.9273972113927207

Epoch: 6| Step: 13
Training loss: 2.703303813934326
Validation loss: 2.9157897233963013

Epoch: 31| Step: 0
Training loss: 3.518162965774536
Validation loss: 2.9145085414250693

Epoch: 6| Step: 1
Training loss: 3.1666646003723145
Validation loss: 2.909812569618225

Epoch: 6| Step: 2
Training loss: 2.959299087524414
Validation loss: 2.9124656120936074

Epoch: 6| Step: 3
Training loss: 3.7309341430664062
Validation loss: 2.9084102710088096

Epoch: 6| Step: 4
Training loss: 4.100141525268555
Validation loss: 2.902961333592733

Epoch: 6| Step: 5
Training loss: 2.923460006713867
Validation loss: 2.901370326677958

Epoch: 6| Step: 6
Training loss: 2.9259250164031982
Validation loss: 2.89832595984141

Epoch: 6| Step: 7
Training loss: 3.0303502082824707
Validation loss: 2.8898216088612876

Epoch: 6| Step: 8
Training loss: 2.1240742206573486
Validation loss: 2.887873808542887

Epoch: 6| Step: 9
Training loss: 3.325227737426758
Validation loss: 2.8859130144119263

Epoch: 6| Step: 10
Training loss: 3.371643304824829
Validation loss: 2.8896199464797974

Epoch: 6| Step: 11
Training loss: 2.855678081512451
Validation loss: 2.8849143187204995

Epoch: 6| Step: 12
Training loss: 2.6935479640960693
Validation loss: 2.875575383504232

Epoch: 6| Step: 13
Training loss: 2.6971330642700195
Validation loss: 2.872028191884359

Epoch: 32| Step: 0
Training loss: 3.1181249618530273
Validation loss: 2.870039184888204

Epoch: 6| Step: 1
Training loss: 4.058531761169434
Validation loss: 2.868416945139567

Epoch: 6| Step: 2
Training loss: 3.060488700866699
Validation loss: 2.8651161193847656

Epoch: 6| Step: 3
Training loss: 2.1369287967681885
Validation loss: 2.8609530528386435

Epoch: 6| Step: 4
Training loss: 3.614935874938965
Validation loss: 2.8576212724049888

Epoch: 6| Step: 5
Training loss: 3.1310694217681885
Validation loss: 2.8510541915893555

Epoch: 6| Step: 6
Training loss: 2.757732629776001
Validation loss: 2.8460821310679116

Epoch: 6| Step: 7
Training loss: 3.250990867614746
Validation loss: 2.8425429264704385

Epoch: 6| Step: 8
Training loss: 2.2033920288085938
Validation loss: 2.839591304461161

Epoch: 6| Step: 9
Training loss: 2.5490903854370117
Validation loss: 2.8363015254338584

Epoch: 6| Step: 10
Training loss: 2.993995189666748
Validation loss: 2.8336482445398965

Epoch: 6| Step: 11
Training loss: 3.3198344707489014
Validation loss: 2.8313108086586

Epoch: 6| Step: 12
Training loss: 3.4382081031799316
Validation loss: 2.826745629310608

Epoch: 6| Step: 13
Training loss: 3.169346332550049
Validation loss: 2.825002113978068

Epoch: 33| Step: 0
Training loss: 3.3543925285339355
Validation loss: 2.8228125174840293

Epoch: 6| Step: 1
Training loss: 2.5549161434173584
Validation loss: 2.815983851750692

Epoch: 6| Step: 2
Training loss: 2.6139578819274902
Validation loss: 2.8130401372909546

Epoch: 6| Step: 3
Training loss: 2.8397669792175293
Validation loss: 2.810262759526571

Epoch: 6| Step: 4
Training loss: 3.0873706340789795
Validation loss: 2.8067163228988647

Epoch: 6| Step: 5
Training loss: 3.3536949157714844
Validation loss: 2.8050971825917563

Epoch: 6| Step: 6
Training loss: 3.4147491455078125
Validation loss: 2.803731123606364

Epoch: 6| Step: 7
Training loss: 2.7798330783843994
Validation loss: 2.7993338902791343

Epoch: 6| Step: 8
Training loss: 3.567373037338257
Validation loss: 2.7964577277501426

Epoch: 6| Step: 9
Training loss: 2.921701192855835
Validation loss: 2.7929463386535645

Epoch: 6| Step: 10
Training loss: 2.2726011276245117
Validation loss: 2.7888227303822837

Epoch: 6| Step: 11
Training loss: 3.293264389038086
Validation loss: 2.787467281023661

Epoch: 6| Step: 12
Training loss: 2.4266247749328613
Validation loss: 2.7834237019220986

Epoch: 6| Step: 13
Training loss: 3.7188916206359863
Validation loss: 2.780656894048055

Epoch: 34| Step: 0
Training loss: 2.5259134769439697
Validation loss: 2.776895443598429

Epoch: 6| Step: 1
Training loss: 2.873007297515869
Validation loss: 2.773621598879496

Epoch: 6| Step: 2
Training loss: 2.3082475662231445
Validation loss: 2.770657261212667

Epoch: 6| Step: 3
Training loss: 2.9174013137817383
Validation loss: 2.7680033445358276

Epoch: 6| Step: 4
Training loss: 1.635378360748291
Validation loss: 2.7654224236806235

Epoch: 6| Step: 5
Training loss: 3.7811851501464844
Validation loss: 2.7621946732203164

Epoch: 6| Step: 6
Training loss: 3.879387378692627
Validation loss: 2.760915478070577

Epoch: 6| Step: 7
Training loss: 3.734596014022827
Validation loss: 2.7574923435846963

Epoch: 6| Step: 8
Training loss: 2.6248204708099365
Validation loss: 2.7542462746302285

Epoch: 6| Step: 9
Training loss: 3.2413783073425293
Validation loss: 2.7514505783716836

Epoch: 6| Step: 10
Training loss: 2.7300567626953125
Validation loss: 2.7488590677579245

Epoch: 6| Step: 11
Training loss: 2.732426643371582
Validation loss: 2.746925671895345

Epoch: 6| Step: 12
Training loss: 3.080070734024048
Validation loss: 2.745068907737732

Epoch: 6| Step: 13
Training loss: 3.497009754180908
Validation loss: 2.741343061129252

Epoch: 35| Step: 0
Training loss: 2.803175926208496
Validation loss: 2.735903004805247

Epoch: 6| Step: 1
Training loss: 3.401998996734619
Validation loss: 2.7330710887908936

Epoch: 6| Step: 2
Training loss: 2.895030975341797
Validation loss: 2.730479121208191

Epoch: 6| Step: 3
Training loss: 2.098789930343628
Validation loss: 2.7267397244771323

Epoch: 6| Step: 4
Training loss: 2.93314266204834
Validation loss: 2.724921981493632

Epoch: 6| Step: 5
Training loss: 3.2716064453125
Validation loss: 2.72162131468455

Epoch: 6| Step: 6
Training loss: 2.718437671661377
Validation loss: 2.718273083368937

Epoch: 6| Step: 7
Training loss: 2.8340251445770264
Validation loss: 2.715480327606201

Epoch: 6| Step: 8
Training loss: 3.1533396244049072
Validation loss: 2.712341626485189

Epoch: 6| Step: 9
Training loss: 3.3105616569519043
Validation loss: 2.70945938428243

Epoch: 6| Step: 10
Training loss: 3.4145612716674805
Validation loss: 2.707017501195272

Epoch: 6| Step: 11
Training loss: 2.887016773223877
Validation loss: 2.701844096183777

Epoch: 6| Step: 12
Training loss: 2.6238832473754883
Validation loss: 2.699564576148987

Epoch: 6| Step: 13
Training loss: 2.6212496757507324
Validation loss: 2.6951164404551187

Epoch: 36| Step: 0
Training loss: 2.887051582336426
Validation loss: 2.6947125593821206

Epoch: 6| Step: 1
Training loss: 2.4696316719055176
Validation loss: 2.691020687421163

Epoch: 6| Step: 2
Training loss: 3.1114563941955566
Validation loss: 2.6867130200068154

Epoch: 6| Step: 3
Training loss: 2.5926713943481445
Validation loss: 2.6854613224665322

Epoch: 6| Step: 4
Training loss: 2.681305408477783
Validation loss: 2.6797699133555093

Epoch: 6| Step: 5
Training loss: 2.8608415126800537
Validation loss: 2.6781214475631714

Epoch: 6| Step: 6
Training loss: 3.616396903991699
Validation loss: 2.6750093698501587

Epoch: 6| Step: 7
Training loss: 2.3355746269226074
Validation loss: 2.672526756922404

Epoch: 6| Step: 8
Training loss: 3.7925522327423096
Validation loss: 2.6700901786486306

Epoch: 6| Step: 9
Training loss: 2.670562744140625
Validation loss: 2.665056347846985

Epoch: 6| Step: 10
Training loss: 2.9767799377441406
Validation loss: 2.663001378377279

Epoch: 6| Step: 11
Training loss: 2.8371262550354004
Validation loss: 2.657856067021688

Epoch: 6| Step: 12
Training loss: 2.370565414428711
Validation loss: 2.6580562591552734

Epoch: 6| Step: 13
Training loss: 3.1467063426971436
Validation loss: 2.6609025796254477

Epoch: 37| Step: 0
Training loss: 2.5048599243164062
Validation loss: 2.654374917348226

Epoch: 6| Step: 1
Training loss: 3.197377920150757
Validation loss: 2.6452895204226174

Epoch: 6| Step: 2
Training loss: 2.7411956787109375
Validation loss: 2.644955118497213

Epoch: 6| Step: 3
Training loss: 3.1298747062683105
Validation loss: 2.644385496775309

Epoch: 6| Step: 4
Training loss: 2.407072067260742
Validation loss: 2.642029285430908

Epoch: 6| Step: 5
Training loss: 3.027919054031372
Validation loss: 2.6439157724380493

Epoch: 6| Step: 6
Training loss: 2.298650026321411
Validation loss: 2.6471359729766846

Epoch: 6| Step: 7
Training loss: 3.401825428009033
Validation loss: 2.645134449005127

Epoch: 6| Step: 8
Training loss: 3.179706573486328
Validation loss: 2.636816680431366

Epoch: 6| Step: 9
Training loss: 3.3307015895843506
Validation loss: 2.6433794101079306

Epoch: 6| Step: 10
Training loss: 2.3680920600891113
Validation loss: 2.630791982014974

Epoch: 6| Step: 11
Training loss: 2.388439655303955
Validation loss: 2.6245913902918496

Epoch: 6| Step: 12
Training loss: 3.3440189361572266
Validation loss: 2.626861333847046

Epoch: 6| Step: 13
Training loss: 2.470700979232788
Validation loss: 2.6191298961639404

Epoch: 38| Step: 0
Training loss: 3.042783498764038
Validation loss: 2.615437070528666

Epoch: 6| Step: 1
Training loss: 3.622502326965332
Validation loss: 2.6133654514948526

Epoch: 6| Step: 2
Training loss: 2.744529962539673
Validation loss: 2.6097121636072793

Epoch: 6| Step: 3
Training loss: 2.709867000579834
Validation loss: 2.607693314552307

Epoch: 6| Step: 4
Training loss: 2.458247184753418
Validation loss: 2.6051756938298545

Epoch: 6| Step: 5
Training loss: 2.306955575942993
Validation loss: 2.6028910875320435

Epoch: 6| Step: 6
Training loss: 2.9793624877929688
Validation loss: 2.599583307902018

Epoch: 6| Step: 7
Training loss: 2.918757915496826
Validation loss: 2.5976930061976113

Epoch: 6| Step: 8
Training loss: 2.740363597869873
Validation loss: 2.5950918197631836

Epoch: 6| Step: 9
Training loss: 2.517714023590088
Validation loss: 2.591010411580404

Epoch: 6| Step: 10
Training loss: 2.8061792850494385
Validation loss: 2.588286578655243

Epoch: 6| Step: 11
Training loss: 2.7518115043640137
Validation loss: 2.5851063330968223

Epoch: 6| Step: 12
Training loss: 2.646836280822754
Validation loss: 2.581863363583883

Epoch: 6| Step: 13
Training loss: 2.9450531005859375
Validation loss: 2.5783062974611917

Epoch: 39| Step: 0
Training loss: 2.8307366371154785
Validation loss: 2.575254440307617

Epoch: 6| Step: 1
Training loss: 2.391906261444092
Validation loss: 2.568292737007141

Epoch: 6| Step: 2
Training loss: 2.713487148284912
Validation loss: 2.567042072614034

Epoch: 6| Step: 3
Training loss: 2.9353418350219727
Validation loss: 2.564569115638733

Epoch: 6| Step: 4
Training loss: 2.57584285736084
Validation loss: 2.5612888733545938

Epoch: 6| Step: 5
Training loss: 2.4836249351501465
Validation loss: 2.557100216547648

Epoch: 6| Step: 6
Training loss: 2.5897724628448486
Validation loss: 2.55397621790568

Epoch: 6| Step: 7
Training loss: 1.7312257289886475
Validation loss: 2.5528431336085

Epoch: 6| Step: 8
Training loss: 3.2092480659484863
Validation loss: 2.561103264490763

Epoch: 6| Step: 9
Training loss: 3.075880527496338
Validation loss: 2.547706961631775

Epoch: 6| Step: 10
Training loss: 2.8510024547576904
Validation loss: 2.5429982344309487

Epoch: 6| Step: 11
Training loss: 2.7563982009887695
Validation loss: 2.54573921362559

Epoch: 6| Step: 12
Training loss: 3.3866629600524902
Validation loss: 2.5440531174341836

Epoch: 6| Step: 13
Training loss: 3.0591442584991455
Validation loss: 2.543752431869507

Epoch: 40| Step: 0
Training loss: 2.4463231563568115
Validation loss: 2.543081800142924

Epoch: 6| Step: 1
Training loss: 2.4826719760894775
Validation loss: 2.5458563963572183

Epoch: 6| Step: 2
Training loss: 3.062645196914673
Validation loss: 2.546205520629883

Epoch: 6| Step: 3
Training loss: 2.9433915615081787
Validation loss: 2.543034791946411

Epoch: 6| Step: 4
Training loss: 2.016153573989868
Validation loss: 2.5394826332728067

Epoch: 6| Step: 5
Training loss: 2.1029727458953857
Validation loss: 2.535329262415568

Epoch: 6| Step: 6
Training loss: 3.093111515045166
Validation loss: 2.533397992451986

Epoch: 6| Step: 7
Training loss: 2.5545945167541504
Validation loss: 2.5278954903284707

Epoch: 6| Step: 8
Training loss: 3.041606903076172
Validation loss: 2.5248045126597085

Epoch: 6| Step: 9
Training loss: 3.3775806427001953
Validation loss: 2.521108309427897

Epoch: 6| Step: 10
Training loss: 3.5922999382019043
Validation loss: 2.5167868932088218

Epoch: 6| Step: 11
Training loss: 2.1423721313476562
Validation loss: 2.5136582056681314

Epoch: 6| Step: 12
Training loss: 2.9575958251953125
Validation loss: 2.5101656913757324

Epoch: 6| Step: 13
Training loss: 2.298456907272339
Validation loss: 2.5083823601404824

Epoch: 41| Step: 0
Training loss: 1.9493985176086426
Validation loss: 2.5042684078216553

Epoch: 6| Step: 1
Training loss: 2.2566022872924805
Validation loss: 2.501417795817057

Epoch: 6| Step: 2
Training loss: 2.591046094894409
Validation loss: 2.4981998602549234

Epoch: 6| Step: 3
Training loss: 2.2875664234161377
Validation loss: 2.496482809384664

Epoch: 6| Step: 4
Training loss: 2.854437828063965
Validation loss: 2.4956404765446982

Epoch: 6| Step: 5
Training loss: 2.899545669555664
Validation loss: 2.492814620335897

Epoch: 6| Step: 6
Training loss: 3.2164299488067627
Validation loss: 2.4885303179423013

Epoch: 6| Step: 7
Training loss: 2.7782394886016846
Validation loss: 2.4874014854431152

Epoch: 6| Step: 8
Training loss: 2.6898317337036133
Validation loss: 2.4832040866216025

Epoch: 6| Step: 9
Training loss: 3.205167770385742
Validation loss: 2.48050848642985

Epoch: 6| Step: 10
Training loss: 3.3130202293395996
Validation loss: 2.4787614941596985

Epoch: 6| Step: 11
Training loss: 2.60685658454895
Validation loss: 2.474866290887197

Epoch: 6| Step: 12
Training loss: 2.607362747192383
Validation loss: 2.4726960261662803

Epoch: 6| Step: 13
Training loss: 2.2714109420776367
Validation loss: 2.468120892842611

Epoch: 42| Step: 0
Training loss: 2.26705265045166
Validation loss: 2.465981125831604

Epoch: 6| Step: 1
Training loss: 2.50817608833313
Validation loss: 2.4635435342788696

Epoch: 6| Step: 2
Training loss: 1.9631764888763428
Validation loss: 2.460885524749756

Epoch: 6| Step: 3
Training loss: 3.2697153091430664
Validation loss: 2.456496795018514

Epoch: 6| Step: 4
Training loss: 2.0207808017730713
Validation loss: 2.4528332948684692

Epoch: 6| Step: 5
Training loss: 3.0671536922454834
Validation loss: 2.4510817925135293

Epoch: 6| Step: 6
Training loss: 2.5176546573638916
Validation loss: 2.4483022689819336

Epoch: 6| Step: 7
Training loss: 2.745114803314209
Validation loss: 2.4453779458999634

Epoch: 6| Step: 8
Training loss: 3.0560073852539062
Validation loss: 2.4424076080322266

Epoch: 6| Step: 9
Training loss: 3.3207879066467285
Validation loss: 2.4366649389266968

Epoch: 6| Step: 10
Training loss: 2.780409336090088
Validation loss: 2.436054309209188

Epoch: 6| Step: 11
Training loss: 2.3685450553894043
Validation loss: 2.431978483994802

Epoch: 6| Step: 12
Training loss: 3.079822063446045
Validation loss: 2.432270963986715

Epoch: 6| Step: 13
Training loss: 1.9870837926864624
Validation loss: 2.4303270181020102

Epoch: 43| Step: 0
Training loss: 2.412588357925415
Validation loss: 2.4281286795934043

Epoch: 6| Step: 1
Training loss: 2.3405418395996094
Validation loss: 2.429374873638153

Epoch: 6| Step: 2
Training loss: 2.1031389236450195
Validation loss: 2.426216701666514

Epoch: 6| Step: 3
Training loss: 1.8453540802001953
Validation loss: 2.4190426667531333

Epoch: 6| Step: 4
Training loss: 2.491072654724121
Validation loss: 2.4136523604393005

Epoch: 6| Step: 5
Training loss: 2.5775015354156494
Validation loss: 2.414973556995392

Epoch: 6| Step: 6
Training loss: 2.429750442504883
Validation loss: 2.4124993880589805

Epoch: 6| Step: 7
Training loss: 2.3953988552093506
Validation loss: 2.4086389541625977

Epoch: 6| Step: 8
Training loss: 3.4167933464050293
Validation loss: 2.407200654347738

Epoch: 6| Step: 9
Training loss: 3.3947668075561523
Validation loss: 2.4013260205586753

Epoch: 6| Step: 10
Training loss: 3.3057236671447754
Validation loss: 2.4011516173680625

Epoch: 6| Step: 11
Training loss: 2.236264944076538
Validation loss: 2.3996932903925576

Epoch: 6| Step: 12
Training loss: 2.881831169128418
Validation loss: 2.39905709028244

Epoch: 6| Step: 13
Training loss: 2.5709168910980225
Validation loss: 2.3972202936808267

Epoch: 44| Step: 0
Training loss: 2.6358838081359863
Validation loss: 2.396651029586792

Epoch: 6| Step: 1
Training loss: 1.8465139865875244
Validation loss: 2.395054737726847

Epoch: 6| Step: 2
Training loss: 2.3041892051696777
Validation loss: 2.390910188357035

Epoch: 6| Step: 3
Training loss: 2.819833278656006
Validation loss: 2.3883501489957175

Epoch: 6| Step: 4
Training loss: 2.4310803413391113
Validation loss: 2.390758534272512

Epoch: 6| Step: 5
Training loss: 2.1340010166168213
Validation loss: 2.3844234546025596

Epoch: 6| Step: 6
Training loss: 2.968545913696289
Validation loss: 2.3799251119295755

Epoch: 6| Step: 7
Training loss: 2.003061294555664
Validation loss: 2.378716746966044

Epoch: 6| Step: 8
Training loss: 3.7159945964813232
Validation loss: 2.377812663714091

Epoch: 6| Step: 9
Training loss: 2.878582715988159
Validation loss: 2.3736364444096885

Epoch: 6| Step: 10
Training loss: 2.7272727489471436
Validation loss: 2.368798613548279

Epoch: 6| Step: 11
Training loss: 2.179654836654663
Validation loss: 2.3680054545402527

Epoch: 6| Step: 12
Training loss: 2.585569143295288
Validation loss: 2.3629029194513955

Epoch: 6| Step: 13
Training loss: 2.5870776176452637
Validation loss: 2.3622480034828186

Epoch: 45| Step: 0
Training loss: 1.921459674835205
Validation loss: 2.3581019242604575

Epoch: 6| Step: 1
Training loss: 2.4625790119171143
Validation loss: 2.3548361460367837

Epoch: 6| Step: 2
Training loss: 2.36370849609375
Validation loss: 2.35521133740743

Epoch: 6| Step: 3
Training loss: 2.576918363571167
Validation loss: 2.3509367307027182

Epoch: 6| Step: 4
Training loss: 2.910749912261963
Validation loss: 2.3549944957097373

Epoch: 6| Step: 5
Training loss: 2.804405450820923
Validation loss: 2.3518444299697876

Epoch: 6| Step: 6
Training loss: 2.333223819732666
Validation loss: 2.351959486802419

Epoch: 6| Step: 7
Training loss: 2.154937267303467
Validation loss: 2.3495907386144004

Epoch: 6| Step: 8
Training loss: 2.5245790481567383
Validation loss: 2.3468688329060874

Epoch: 6| Step: 9
Training loss: 2.5327277183532715
Validation loss: 2.3421993454297385

Epoch: 6| Step: 10
Training loss: 2.400667905807495
Validation loss: 2.34111758073171

Epoch: 6| Step: 11
Training loss: 2.8859972953796387
Validation loss: 2.3338551918665567

Epoch: 6| Step: 12
Training loss: 2.949639081954956
Validation loss: 2.334636847178141

Epoch: 6| Step: 13
Training loss: 2.504190444946289
Validation loss: 2.328826129436493

Epoch: 46| Step: 0
Training loss: 2.196165084838867
Validation loss: 2.3245197534561157

Epoch: 6| Step: 1
Training loss: 2.887085199356079
Validation loss: 2.3240360021591187

Epoch: 6| Step: 2
Training loss: 2.3292932510375977
Validation loss: 2.325471063454946

Epoch: 6| Step: 3
Training loss: 2.426243782043457
Validation loss: 2.3172418673833213

Epoch: 6| Step: 4
Training loss: 2.6866469383239746
Validation loss: 2.320668896039327

Epoch: 6| Step: 5
Training loss: 3.455294132232666
Validation loss: 2.320471207300822

Epoch: 6| Step: 6
Training loss: 2.774226665496826
Validation loss: 2.3071083625157676

Epoch: 6| Step: 7
Training loss: 2.366030216217041
Validation loss: 2.3044387102127075

Epoch: 6| Step: 8
Training loss: 2.2433323860168457
Validation loss: 2.3091527422269187

Epoch: 6| Step: 9
Training loss: 2.5336835384368896
Validation loss: 2.3079174955685935

Epoch: 6| Step: 10
Training loss: 1.793060302734375
Validation loss: 2.308392365773519

Epoch: 6| Step: 11
Training loss: 1.9502801895141602
Validation loss: 2.3148237069447837

Epoch: 6| Step: 12
Training loss: 2.507964611053467
Validation loss: 2.3088772296905518

Epoch: 6| Step: 13
Training loss: 2.774284839630127
Validation loss: 2.301507751146952

Epoch: 47| Step: 0
Training loss: 2.7309393882751465
Validation loss: 2.2934259176254272

Epoch: 6| Step: 1
Training loss: 1.993006706237793
Validation loss: 2.2907764514287314

Epoch: 6| Step: 2
Training loss: 2.86683988571167
Validation loss: 2.2924581368764243

Epoch: 6| Step: 3
Training loss: 2.6965713500976562
Validation loss: 2.289494514465332

Epoch: 6| Step: 4
Training loss: 1.8238003253936768
Validation loss: 2.2871879736582437

Epoch: 6| Step: 5
Training loss: 3.1314878463745117
Validation loss: 2.282517194747925

Epoch: 6| Step: 6
Training loss: 2.1071343421936035
Validation loss: 2.2794034481048584

Epoch: 6| Step: 7
Training loss: 2.3050594329833984
Validation loss: 2.2835455735524497

Epoch: 6| Step: 8
Training loss: 2.233398914337158
Validation loss: 2.284129540125529

Epoch: 6| Step: 9
Training loss: 2.436951160430908
Validation loss: 2.2954293886820474

Epoch: 6| Step: 10
Training loss: 2.3265089988708496
Validation loss: 2.276377558708191

Epoch: 6| Step: 11
Training loss: 2.6952943801879883
Validation loss: 2.2746452490488687

Epoch: 6| Step: 12
Training loss: 2.446439504623413
Validation loss: 2.2623553474744162

Epoch: 6| Step: 13
Training loss: 2.5253233909606934
Validation loss: 2.259581128756205

Epoch: 48| Step: 0
Training loss: 2.450721025466919
Validation loss: 2.2617894609769187

Epoch: 6| Step: 1
Training loss: 2.3806142807006836
Validation loss: 2.2623120148976645

Epoch: 6| Step: 2
Training loss: 2.5298750400543213
Validation loss: 2.261503358681997

Epoch: 6| Step: 3
Training loss: 2.5935728549957275
Validation loss: 2.261778692404429

Epoch: 6| Step: 4
Training loss: 2.564444065093994
Validation loss: 2.2645612557729087

Epoch: 6| Step: 5
Training loss: 2.831660270690918
Validation loss: 2.2658838033676147

Epoch: 6| Step: 6
Training loss: 2.1865832805633545
Validation loss: 2.2681361039479575

Epoch: 6| Step: 7
Training loss: 1.8359936475753784
Validation loss: 2.2647854487101235

Epoch: 6| Step: 8
Training loss: 1.9401655197143555
Validation loss: 2.26137912273407

Epoch: 6| Step: 9
Training loss: 2.7134552001953125
Validation loss: 2.25948033730189

Epoch: 6| Step: 10
Training loss: 2.8076937198638916
Validation loss: 2.2547191182772317

Epoch: 6| Step: 11
Training loss: 2.4648478031158447
Validation loss: 2.2531564633051553

Epoch: 6| Step: 12
Training loss: 2.0411033630371094
Validation loss: 2.247204621632894

Epoch: 6| Step: 13
Training loss: 2.5963010787963867
Validation loss: 2.240817427635193

Epoch: 49| Step: 0
Training loss: 2.32131290435791
Validation loss: 2.235448877016703

Epoch: 6| Step: 1
Training loss: 2.2759861946105957
Validation loss: 2.2332170804341636

Epoch: 6| Step: 2
Training loss: 2.288486957550049
Validation loss: 2.230490982532501

Epoch: 6| Step: 3
Training loss: 2.6955361366271973
Validation loss: 2.229408860206604

Epoch: 6| Step: 4
Training loss: 1.931614875793457
Validation loss: 2.2307897210121155

Epoch: 6| Step: 5
Training loss: 2.336548328399658
Validation loss: 2.2286269664764404

Epoch: 6| Step: 6
Training loss: 2.502913475036621
Validation loss: 2.22913134098053

Epoch: 6| Step: 7
Training loss: 1.9257253408432007
Validation loss: 2.2239491740862527

Epoch: 6| Step: 8
Training loss: 2.8691258430480957
Validation loss: 2.218986213207245

Epoch: 6| Step: 9
Training loss: 2.5158634185791016
Validation loss: 2.2179887692133584

Epoch: 6| Step: 10
Training loss: 2.23268461227417
Validation loss: 2.218857526779175

Epoch: 6| Step: 11
Training loss: 3.0717031955718994
Validation loss: 2.2158599694569907

Epoch: 6| Step: 12
Training loss: 2.526911735534668
Validation loss: 2.2104092240333557

Epoch: 6| Step: 13
Training loss: 1.9103556871414185
Validation loss: 2.205561935901642

Epoch: 50| Step: 0
Training loss: 2.5545082092285156
Validation loss: 2.203418811162313

Epoch: 6| Step: 1
Training loss: 2.5675270557403564
Validation loss: 2.204168419043223

Epoch: 6| Step: 2
Training loss: 1.7128243446350098
Validation loss: 2.200491031010946

Epoch: 6| Step: 3
Training loss: 1.9490811824798584
Validation loss: 2.1963709791501365

Epoch: 6| Step: 4
Training loss: 2.8922953605651855
Validation loss: 2.199808915456136

Epoch: 6| Step: 5
Training loss: 2.3836162090301514
Validation loss: 2.2021167079607644

Epoch: 6| Step: 6
Training loss: 2.4767005443573
Validation loss: 2.1932504971822104

Epoch: 6| Step: 7
Training loss: 2.429912567138672
Validation loss: 2.2168302734692893

Epoch: 6| Step: 8
Training loss: 1.8975040912628174
Validation loss: 2.2201058665911355

Epoch: 6| Step: 9
Training loss: 2.8262460231781006
Validation loss: 2.1982308427492776

Epoch: 6| Step: 10
Training loss: 2.448066234588623
Validation loss: 2.177021543184916

Epoch: 6| Step: 11
Training loss: 2.103227376937866
Validation loss: 2.1856964429219565

Epoch: 6| Step: 12
Training loss: 2.0282511711120605
Validation loss: 2.189326206843058

Epoch: 6| Step: 13
Training loss: 2.8227553367614746
Validation loss: 2.1858925223350525

Epoch: 51| Step: 0
Training loss: 2.487797260284424
Validation loss: 2.1979655027389526

Epoch: 6| Step: 1
Training loss: 1.834208369255066
Validation loss: 2.1971790393193564

Epoch: 6| Step: 2
Training loss: 1.4844365119934082
Validation loss: 2.2092215021451316

Epoch: 6| Step: 3
Training loss: 2.5503482818603516
Validation loss: 2.22007163365682

Epoch: 6| Step: 4
Training loss: 2.316560983657837
Validation loss: 2.238689641157786

Epoch: 6| Step: 5
Training loss: 3.1102681159973145
Validation loss: 2.2254733443260193

Epoch: 6| Step: 6
Training loss: 2.342961311340332
Validation loss: 2.216827074686686

Epoch: 6| Step: 7
Training loss: 2.0868582725524902
Validation loss: 2.204283118247986

Epoch: 6| Step: 8
Training loss: 2.250209331512451
Validation loss: 2.1849387486775718

Epoch: 6| Step: 9
Training loss: 2.175262451171875
Validation loss: 2.1770336627960205

Epoch: 6| Step: 10
Training loss: 2.315852165222168
Validation loss: 2.1766051054000854

Epoch: 6| Step: 11
Training loss: 2.5025999546051025
Validation loss: 2.173579752445221

Epoch: 6| Step: 12
Training loss: 2.578064441680908
Validation loss: 2.177333176136017

Epoch: 6| Step: 13
Training loss: 2.887552261352539
Validation loss: 2.174330711364746

Epoch: 52| Step: 0
Training loss: 2.3197853565216064
Validation loss: 2.1691293716430664

Epoch: 6| Step: 1
Training loss: 2.308443069458008
Validation loss: 2.16621204217275

Epoch: 6| Step: 2
Training loss: 2.460646867752075
Validation loss: 2.1607083082199097

Epoch: 6| Step: 3
Training loss: 1.9770734310150146
Validation loss: 2.1586986978848777

Epoch: 6| Step: 4
Training loss: 2.776015043258667
Validation loss: 2.1647403240203857

Epoch: 6| Step: 5
Training loss: 2.981921672821045
Validation loss: 2.1617120504379272

Epoch: 6| Step: 6
Training loss: 2.2558016777038574
Validation loss: 2.156485120455424

Epoch: 6| Step: 7
Training loss: 2.423401355743408
Validation loss: 2.1625398794809976

Epoch: 6| Step: 8
Training loss: 1.6876534223556519
Validation loss: 2.153516868750254

Epoch: 6| Step: 9
Training loss: 2.2805380821228027
Validation loss: 2.155474821726481

Epoch: 6| Step: 10
Training loss: 2.1678738594055176
Validation loss: 2.160229802131653

Epoch: 6| Step: 11
Training loss: 2.3607091903686523
Validation loss: 2.1590836445490518

Epoch: 6| Step: 12
Training loss: 2.401829242706299
Validation loss: 2.14914733171463

Epoch: 6| Step: 13
Training loss: 2.23533296585083
Validation loss: 2.143563767274221

Epoch: 53| Step: 0
Training loss: 2.431421995162964
Validation loss: 2.1417990922927856

Epoch: 6| Step: 1
Training loss: 2.443979024887085
Validation loss: 2.1444125374158225

Epoch: 6| Step: 2
Training loss: 1.8640294075012207
Validation loss: 2.143984615802765

Epoch: 6| Step: 3
Training loss: 2.2302303314208984
Validation loss: 2.1426312724749246

Epoch: 6| Step: 4
Training loss: 2.790370464324951
Validation loss: 2.138364235560099

Epoch: 6| Step: 5
Training loss: 1.3293938636779785
Validation loss: 2.1437690258026123

Epoch: 6| Step: 6
Training loss: 2.9594900608062744
Validation loss: 2.1393091877301535

Epoch: 6| Step: 7
Training loss: 2.8675689697265625
Validation loss: 2.134787122408549

Epoch: 6| Step: 8
Training loss: 1.8115193843841553
Validation loss: 2.138895591100057

Epoch: 6| Step: 9
Training loss: 2.144587993621826
Validation loss: 2.1555456121762595

Epoch: 6| Step: 10
Training loss: 1.9265058040618896
Validation loss: 2.157824158668518

Epoch: 6| Step: 11
Training loss: 2.4995954036712646
Validation loss: 2.157913645108541

Epoch: 6| Step: 12
Training loss: 2.427865743637085
Validation loss: 2.1528214613596597

Epoch: 6| Step: 13
Training loss: 2.6612727642059326
Validation loss: 2.145539383093516

Epoch: 54| Step: 0
Training loss: 2.277327537536621
Validation loss: 2.1470420360565186

Epoch: 6| Step: 1
Training loss: 2.8997068405151367
Validation loss: 2.146125773588816

Epoch: 6| Step: 2
Training loss: 2.3175292015075684
Validation loss: 2.1416487296422324

Epoch: 6| Step: 3
Training loss: 2.464420795440674
Validation loss: 2.1342760125796

Epoch: 6| Step: 4
Training loss: 1.5664145946502686
Validation loss: 2.122061332066854

Epoch: 6| Step: 5
Training loss: 2.850055694580078
Validation loss: 2.1073241035143533

Epoch: 6| Step: 6
Training loss: 2.1128389835357666
Validation loss: 2.1177390416463218

Epoch: 6| Step: 7
Training loss: 2.490166187286377
Validation loss: 2.1227645874023438

Epoch: 6| Step: 8
Training loss: 2.480334758758545
Validation loss: 2.125260810057322

Epoch: 6| Step: 9
Training loss: 1.4866437911987305
Validation loss: 2.12975811958313

Epoch: 6| Step: 10
Training loss: 1.3687289953231812
Validation loss: 2.132744828859965

Epoch: 6| Step: 11
Training loss: 2.6374573707580566
Validation loss: 2.127708315849304

Epoch: 6| Step: 12
Training loss: 2.4704129695892334
Validation loss: 2.124707659085592

Epoch: 6| Step: 13
Training loss: 2.7762396335601807
Validation loss: 2.1175235509872437

Epoch: 55| Step: 0
Training loss: 2.2483737468719482
Validation loss: 2.1122067173322043

Epoch: 6| Step: 1
Training loss: 2.334595203399658
Validation loss: 2.1110302209854126

Epoch: 6| Step: 2
Training loss: 2.354179620742798
Validation loss: 2.1114712158838906

Epoch: 6| Step: 3
Training loss: 1.8025957345962524
Validation loss: 2.1052164435386658

Epoch: 6| Step: 4
Training loss: 2.4198756217956543
Validation loss: 2.1025169690450034

Epoch: 6| Step: 5
Training loss: 2.1359975337982178
Validation loss: 2.099436581134796

Epoch: 6| Step: 6
Training loss: 3.1986167430877686
Validation loss: 2.1076722145080566

Epoch: 6| Step: 7
Training loss: 1.9925411939620972
Validation loss: 2.108256379763285

Epoch: 6| Step: 8
Training loss: 1.9704275131225586
Validation loss: 2.0949957569440207

Epoch: 6| Step: 9
Training loss: 2.71661114692688
Validation loss: 2.0976216793060303

Epoch: 6| Step: 10
Training loss: 2.4270811080932617
Validation loss: 2.101331909497579

Epoch: 6| Step: 11
Training loss: 3.2340950965881348
Validation loss: 2.1099706888198853

Epoch: 6| Step: 12
Training loss: 1.9422510862350464
Validation loss: 2.112927575906118

Epoch: 6| Step: 13
Training loss: 1.4208495616912842
Validation loss: 2.1229087710380554

Epoch: 56| Step: 0
Training loss: 2.1125733852386475
Validation loss: 2.1218310395876565

Epoch: 6| Step: 1
Training loss: 2.2531471252441406
Validation loss: 2.1248300274213157

Epoch: 6| Step: 2
Training loss: 2.0967183113098145
Validation loss: 2.1214689215024314

Epoch: 6| Step: 3
Training loss: 2.772644519805908
Validation loss: 2.1206398804982505

Epoch: 6| Step: 4
Training loss: 2.1843490600585938
Validation loss: 2.1215391556421914

Epoch: 6| Step: 5
Training loss: 2.600860834121704
Validation loss: 2.1159852147102356

Epoch: 6| Step: 6
Training loss: 2.4090020656585693
Validation loss: 2.1174376010894775

Epoch: 6| Step: 7
Training loss: 2.2442679405212402
Validation loss: 2.113159497578939

Epoch: 6| Step: 8
Training loss: 1.2207825183868408
Validation loss: 2.113496502240499

Epoch: 6| Step: 9
Training loss: 1.9555342197418213
Validation loss: 2.1080817580223083

Epoch: 6| Step: 10
Training loss: 2.4441118240356445
Validation loss: 2.1026766498883567

Epoch: 6| Step: 11
Training loss: 2.501204013824463
Validation loss: 2.0983203649520874

Epoch: 6| Step: 12
Training loss: 2.9025819301605225
Validation loss: 2.09446789820989

Epoch: 6| Step: 13
Training loss: 2.362143039703369
Validation loss: 2.0909903049468994

Epoch: 57| Step: 0
Training loss: 2.4674363136291504
Validation loss: 2.0884465972582498

Epoch: 6| Step: 1
Training loss: 2.063908100128174
Validation loss: 2.0883512695630393

Epoch: 6| Step: 2
Training loss: 1.5343811511993408
Validation loss: 2.0864750146865845

Epoch: 6| Step: 3
Training loss: 2.071462392807007
Validation loss: 2.082937240600586

Epoch: 6| Step: 4
Training loss: 2.3827595710754395
Validation loss: 2.0864910880724588

Epoch: 6| Step: 5
Training loss: 2.38438081741333
Validation loss: 2.0825971961021423

Epoch: 6| Step: 6
Training loss: 2.0421743392944336
Validation loss: 2.081536908944448

Epoch: 6| Step: 7
Training loss: 2.2614336013793945
Validation loss: 2.081163167953491

Epoch: 6| Step: 8
Training loss: 2.5077297687530518
Validation loss: 2.082824170589447

Epoch: 6| Step: 9
Training loss: 2.5207042694091797
Validation loss: 2.0793464382489524

Epoch: 6| Step: 10
Training loss: 2.3091907501220703
Validation loss: 2.0831307768821716

Epoch: 6| Step: 11
Training loss: 2.139664649963379
Validation loss: 2.0776017904281616

Epoch: 6| Step: 12
Training loss: 2.3202130794525146
Validation loss: 2.0785792668660483

Epoch: 6| Step: 13
Training loss: 2.7121787071228027
Validation loss: 2.0773349603017173

Epoch: 58| Step: 0
Training loss: 2.3312807083129883
Validation loss: 2.079380472501119

Epoch: 6| Step: 1
Training loss: 2.1160173416137695
Validation loss: 2.077700138092041

Epoch: 6| Step: 2
Training loss: 2.69718074798584
Validation loss: 2.074023405710856

Epoch: 6| Step: 3
Training loss: 2.6956863403320312
Validation loss: 2.070002774397532

Epoch: 6| Step: 4
Training loss: 1.5855402946472168
Validation loss: 2.073340038458506

Epoch: 6| Step: 5
Training loss: 3.024885654449463
Validation loss: 2.0716501474380493

Epoch: 6| Step: 6
Training loss: 2.3620026111602783
Validation loss: 2.0718876918156943

Epoch: 6| Step: 7
Training loss: 1.6859486103057861
Validation loss: 2.067723512649536

Epoch: 6| Step: 8
Training loss: 1.6745107173919678
Validation loss: 2.071154455343882

Epoch: 6| Step: 9
Training loss: 2.2567694187164307
Validation loss: 2.082003593444824

Epoch: 6| Step: 10
Training loss: 1.8683969974517822
Validation loss: 2.0713571310043335

Epoch: 6| Step: 11
Training loss: 2.260430097579956
Validation loss: 2.08018430074056

Epoch: 6| Step: 12
Training loss: 2.6324350833892822
Validation loss: 2.0763601859410605

Epoch: 6| Step: 13
Training loss: 2.4312920570373535
Validation loss: 2.0717390378316245

Epoch: 59| Step: 0
Training loss: 2.1705870628356934
Validation loss: 2.06167725721995

Epoch: 6| Step: 1
Training loss: 2.627995014190674
Validation loss: 2.058411121368408

Epoch: 6| Step: 2
Training loss: 2.117744207382202
Validation loss: 2.0710116823514304

Epoch: 6| Step: 3
Training loss: 2.2320761680603027
Validation loss: 2.0724873145421348

Epoch: 6| Step: 4
Training loss: 2.03346586227417
Validation loss: 2.072797954082489

Epoch: 6| Step: 5
Training loss: 2.5279150009155273
Validation loss: 2.078674336274465

Epoch: 6| Step: 6
Training loss: 2.2228899002075195
Validation loss: 2.076489786307017

Epoch: 6| Step: 7
Training loss: 2.199756622314453
Validation loss: 2.083865304787954

Epoch: 6| Step: 8
Training loss: 2.1295113563537598
Validation loss: 2.080353617668152

Epoch: 6| Step: 9
Training loss: 2.2453036308288574
Validation loss: 2.0779632329940796

Epoch: 6| Step: 10
Training loss: 1.9598290920257568
Validation loss: 2.071171224117279

Epoch: 6| Step: 11
Training loss: 2.3878746032714844
Validation loss: 2.065896968046824

Epoch: 6| Step: 12
Training loss: 2.414107322692871
Validation loss: 2.0582783619562783

Epoch: 6| Step: 13
Training loss: 2.229818344116211
Validation loss: 2.0560934940973916

Epoch: 60| Step: 0
Training loss: 1.7775638103485107
Validation loss: 2.058432698249817

Epoch: 6| Step: 1
Training loss: 2.819772243499756
Validation loss: 2.0624260306358337

Epoch: 6| Step: 2
Training loss: 1.9556039571762085
Validation loss: 2.0576598246892295

Epoch: 6| Step: 3
Training loss: 2.455388069152832
Validation loss: 2.051494220892588

Epoch: 6| Step: 4
Training loss: 2.4957563877105713
Validation loss: 2.0533087253570557

Epoch: 6| Step: 5
Training loss: 2.066580295562744
Validation loss: 2.049558460712433

Epoch: 6| Step: 6
Training loss: 1.8285995721817017
Validation loss: 2.052111347516378

Epoch: 6| Step: 7
Training loss: 1.966857671737671
Validation loss: 2.0537733236948648

Epoch: 6| Step: 8
Training loss: 1.7605791091918945
Validation loss: 2.048376500606537

Epoch: 6| Step: 9
Training loss: 2.2110774517059326
Validation loss: 2.0445691347122192

Epoch: 6| Step: 10
Training loss: 2.3841772079467773
Validation loss: 2.044094502925873

Epoch: 6| Step: 11
Training loss: 1.4656026363372803
Validation loss: 2.0368812878926597

Epoch: 6| Step: 12
Training loss: 3.3376927375793457
Validation loss: 2.044043242931366

Epoch: 6| Step: 13
Training loss: 2.7143993377685547
Validation loss: 2.041576166947683

Epoch: 61| Step: 0
Training loss: 2.5314669609069824
Validation loss: 2.04788867632548

Epoch: 6| Step: 1
Training loss: 1.7036716938018799
Validation loss: 2.0537836154301963

Epoch: 6| Step: 2
Training loss: 1.7526484727859497
Validation loss: 2.0616564750671387

Epoch: 6| Step: 3
Training loss: 2.5507001876831055
Validation loss: 2.072468380133311

Epoch: 6| Step: 4
Training loss: 2.877415180206299
Validation loss: 2.092517296473185

Epoch: 6| Step: 5
Training loss: 2.1366970539093018
Validation loss: 2.0767684976259866

Epoch: 6| Step: 6
Training loss: 2.338653564453125
Validation loss: 2.055103878180186

Epoch: 6| Step: 7
Training loss: 2.3634800910949707
Validation loss: 2.0474358598391214

Epoch: 6| Step: 8
Training loss: 2.243072032928467
Validation loss: 2.0411088864008584

Epoch: 6| Step: 9
Training loss: 1.3535265922546387
Validation loss: 2.0312921603520713

Epoch: 6| Step: 10
Training loss: 2.19219970703125
Validation loss: 2.035421053568522

Epoch: 6| Step: 11
Training loss: 2.656208038330078
Validation loss: 2.0338962276776633

Epoch: 6| Step: 12
Training loss: 2.1584248542785645
Validation loss: 2.0302470326423645

Epoch: 6| Step: 13
Training loss: 2.4730682373046875
Validation loss: 2.0356875459353128

Epoch: 62| Step: 0
Training loss: 2.5650389194488525
Validation loss: 2.0300352374712625

Epoch: 6| Step: 1
Training loss: 2.5441877841949463
Validation loss: 2.03277321656545

Epoch: 6| Step: 2
Training loss: 1.6402524709701538
Validation loss: 2.0324689745903015

Epoch: 6| Step: 3
Training loss: 2.0445942878723145
Validation loss: 2.033968210220337

Epoch: 6| Step: 4
Training loss: 2.456540107727051
Validation loss: 2.033698618412018

Epoch: 6| Step: 5
Training loss: 2.1685891151428223
Validation loss: 2.0279653867085776

Epoch: 6| Step: 6
Training loss: 2.4353156089782715
Validation loss: 2.039502998193105

Epoch: 6| Step: 7
Training loss: 2.005732536315918
Validation loss: 2.0333274602890015

Epoch: 6| Step: 8
Training loss: 2.059255599975586
Validation loss: 2.045935809612274

Epoch: 6| Step: 9
Training loss: 1.6137912273406982
Validation loss: 2.052804450194041

Epoch: 6| Step: 10
Training loss: 2.1586670875549316
Validation loss: 2.050549030303955

Epoch: 6| Step: 11
Training loss: 2.585894823074341
Validation loss: 2.0646108786265054

Epoch: 6| Step: 12
Training loss: 2.6423707008361816
Validation loss: 2.055397609869639

Epoch: 6| Step: 13
Training loss: 2.146026372909546
Validation loss: 2.0323570171991983

Epoch: 63| Step: 0
Training loss: 2.9608609676361084
Validation loss: 2.032354931036631

Epoch: 6| Step: 1
Training loss: 1.7433385848999023
Validation loss: 2.034879287083944

Epoch: 6| Step: 2
Training loss: 1.961524486541748
Validation loss: 2.0483190218607583

Epoch: 6| Step: 3
Training loss: 2.1968040466308594
Validation loss: 2.057960291703542

Epoch: 6| Step: 4
Training loss: 1.6484999656677246
Validation loss: 2.059105634689331

Epoch: 6| Step: 5
Training loss: 2.6314804553985596
Validation loss: 2.06298565864563

Epoch: 6| Step: 6
Training loss: 2.478835105895996
Validation loss: 2.0709285537401834

Epoch: 6| Step: 7
Training loss: 2.044710159301758
Validation loss: 2.067542016506195

Epoch: 6| Step: 8
Training loss: 2.531231164932251
Validation loss: 2.0782166918118796

Epoch: 6| Step: 9
Training loss: 1.9493999481201172
Validation loss: 2.075626492500305

Epoch: 6| Step: 10
Training loss: 2.3337512016296387
Validation loss: 2.069116731484731

Epoch: 6| Step: 11
Training loss: 2.030470848083496
Validation loss: 2.0560313860575357

Epoch: 6| Step: 12
Training loss: 2.4285666942596436
Validation loss: 2.050799568494161

Epoch: 6| Step: 13
Training loss: 2.5440540313720703
Validation loss: 2.044531265894572

Epoch: 64| Step: 0
Training loss: 2.363776445388794
Validation loss: 2.0343359311421714

Epoch: 6| Step: 1
Training loss: 2.888984203338623
Validation loss: 2.030622939268748

Epoch: 6| Step: 2
Training loss: 2.240384101867676
Validation loss: 2.0251641074816384

Epoch: 6| Step: 3
Training loss: 2.067361831665039
Validation loss: 2.0253862539927163

Epoch: 6| Step: 4
Training loss: 1.9903125762939453
Validation loss: 2.0333972771962485

Epoch: 6| Step: 5
Training loss: 2.222511053085327
Validation loss: 2.038914124170939

Epoch: 6| Step: 6
Training loss: 1.886691689491272
Validation loss: 2.047042707602183

Epoch: 6| Step: 7
Training loss: 2.444827079772949
Validation loss: 2.056274652481079

Epoch: 6| Step: 8
Training loss: 2.606302261352539
Validation loss: 2.069832225640615

Epoch: 6| Step: 9
Training loss: 2.382676601409912
Validation loss: 2.0515861908594766

Epoch: 6| Step: 10
Training loss: 1.8265111446380615
Validation loss: 2.0360201398531594

Epoch: 6| Step: 11
Training loss: 1.9138734340667725
Validation loss: 2.0387519796689353

Epoch: 6| Step: 12
Training loss: 2.0869626998901367
Validation loss: 2.02896378437678

Epoch: 6| Step: 13
Training loss: 2.1742069721221924
Validation loss: 2.033171832561493

Epoch: 65| Step: 0
Training loss: 2.0338895320892334
Validation loss: 2.0347903966903687

Epoch: 6| Step: 1
Training loss: 2.4794225692749023
Validation loss: 2.0419722199440002

Epoch: 6| Step: 2
Training loss: 2.5565731525421143
Validation loss: 2.043777346611023

Epoch: 6| Step: 3
Training loss: 1.6232690811157227
Validation loss: 2.043978989124298

Epoch: 6| Step: 4
Training loss: 2.508124828338623
Validation loss: 2.0462361176808677

Epoch: 6| Step: 5
Training loss: 2.106363296508789
Validation loss: 2.0442704359690347

Epoch: 6| Step: 6
Training loss: 2.0233099460601807
Validation loss: 2.051506817340851

Epoch: 6| Step: 7
Training loss: 1.8072049617767334
Validation loss: 2.047309180100759

Epoch: 6| Step: 8
Training loss: 2.5640106201171875
Validation loss: 2.0506630341211953

Epoch: 6| Step: 9
Training loss: 1.955570936203003
Validation loss: 2.0643586119016013

Epoch: 6| Step: 10
Training loss: 2.1300225257873535
Validation loss: 2.0466569860776267

Epoch: 6| Step: 11
Training loss: 2.487429618835449
Validation loss: 2.0391234159469604

Epoch: 6| Step: 12
Training loss: 2.06622314453125
Validation loss: 2.031458000342051

Epoch: 6| Step: 13
Training loss: 2.494431257247925
Validation loss: 2.031532406806946

Epoch: 66| Step: 0
Training loss: 2.316889762878418
Validation loss: 2.0339678525924683

Epoch: 6| Step: 1
Training loss: 1.8322386741638184
Validation loss: 2.0316633780797324

Epoch: 6| Step: 2
Training loss: 2.037874460220337
Validation loss: 2.0369128783543906

Epoch: 6| Step: 3
Training loss: 1.8380675315856934
Validation loss: 2.0346038142840066

Epoch: 6| Step: 4
Training loss: 2.1531336307525635
Validation loss: 2.039633830388387

Epoch: 6| Step: 5
Training loss: 1.8331588506698608
Validation loss: 2.0347196658452353

Epoch: 6| Step: 6
Training loss: 2.0353102684020996
Validation loss: 2.0305784742037454

Epoch: 6| Step: 7
Training loss: 2.7391533851623535
Validation loss: 2.025783638159434

Epoch: 6| Step: 8
Training loss: 2.6989336013793945
Validation loss: 2.0345749656359353

Epoch: 6| Step: 9
Training loss: 1.9450352191925049
Validation loss: 2.0259647766749063

Epoch: 6| Step: 10
Training loss: 2.559483766555786
Validation loss: 2.0245893796284995

Epoch: 6| Step: 11
Training loss: 2.1115052700042725
Validation loss: 2.0329686800638833

Epoch: 6| Step: 12
Training loss: 2.6872007846832275
Validation loss: 2.0314222971598306

Epoch: 6| Step: 13
Training loss: 1.9746804237365723
Validation loss: 2.0336082776387534

Epoch: 67| Step: 0
Training loss: 2.139853000640869
Validation loss: 2.038263519605001

Epoch: 6| Step: 1
Training loss: 1.5689780712127686
Validation loss: 2.04595277706782

Epoch: 6| Step: 2
Training loss: 1.525667428970337
Validation loss: 2.050558090209961

Epoch: 6| Step: 3
Training loss: 2.765373468399048
Validation loss: 2.0344655911127725

Epoch: 6| Step: 4
Training loss: 2.483604907989502
Validation loss: 2.0291427771250405

Epoch: 6| Step: 5
Training loss: 2.0485644340515137
Validation loss: 2.023139218489329

Epoch: 6| Step: 6
Training loss: 1.6945818662643433
Validation loss: 2.0215760866800943

Epoch: 6| Step: 7
Training loss: 2.338787078857422
Validation loss: 2.028765320777893

Epoch: 6| Step: 8
Training loss: 1.7837088108062744
Validation loss: 2.0135425527890525

Epoch: 6| Step: 9
Training loss: 2.6862592697143555
Validation loss: 2.0166434248288474

Epoch: 6| Step: 10
Training loss: 2.753349781036377
Validation loss: 2.024256149927775

Epoch: 6| Step: 11
Training loss: 2.200296401977539
Validation loss: 2.017977714538574

Epoch: 6| Step: 12
Training loss: 2.021015167236328
Validation loss: 2.021916151046753

Epoch: 6| Step: 13
Training loss: 2.606238842010498
Validation loss: 2.017456511656443

Epoch: 68| Step: 0
Training loss: 2.0225706100463867
Validation loss: 2.0188520153363547

Epoch: 6| Step: 1
Training loss: 2.0072484016418457
Validation loss: 2.023823837439219

Epoch: 6| Step: 2
Training loss: 1.8268321752548218
Validation loss: 2.0241706172625222

Epoch: 6| Step: 3
Training loss: 2.152662992477417
Validation loss: 2.029009441534678

Epoch: 6| Step: 4
Training loss: 2.346550941467285
Validation loss: 2.0328246355056763

Epoch: 6| Step: 5
Training loss: 1.912980079650879
Validation loss: 2.0240779519081116

Epoch: 6| Step: 6
Training loss: 2.4525258541107178
Validation loss: 2.024112641811371

Epoch: 6| Step: 7
Training loss: 3.041626453399658
Validation loss: 2.01178248723348

Epoch: 6| Step: 8
Training loss: 2.608151912689209
Validation loss: 2.0116575161616006

Epoch: 6| Step: 9
Training loss: 1.844875693321228
Validation loss: 2.02472060918808

Epoch: 6| Step: 10
Training loss: 2.2426233291625977
Validation loss: 2.016236106554667

Epoch: 6| Step: 11
Training loss: 2.447604179382324
Validation loss: 2.022770961125692

Epoch: 6| Step: 12
Training loss: 1.5541036128997803
Validation loss: 2.0197956363360086

Epoch: 6| Step: 13
Training loss: 2.1062684059143066
Validation loss: 2.026168723901113

Epoch: 69| Step: 0
Training loss: 2.3877663612365723
Validation loss: 2.032934526602427

Epoch: 6| Step: 1
Training loss: 2.574026584625244
Validation loss: 2.027716954549154

Epoch: 6| Step: 2
Training loss: 2.621304512023926
Validation loss: 2.02546093861262

Epoch: 6| Step: 3
Training loss: 2.5288901329040527
Validation loss: 2.023845116297404

Epoch: 6| Step: 4
Training loss: 1.9317522048950195
Validation loss: 2.0255051255226135

Epoch: 6| Step: 5
Training loss: 2.752563714981079
Validation loss: 2.0189521312713623

Epoch: 6| Step: 6
Training loss: 2.0777063369750977
Validation loss: 2.014037847518921

Epoch: 6| Step: 7
Training loss: 2.3671393394470215
Validation loss: 2.021604577700297

Epoch: 6| Step: 8
Training loss: 1.5823982954025269
Validation loss: 2.0218162536621094

Epoch: 6| Step: 9
Training loss: 2.1465134620666504
Validation loss: 2.018432299296061

Epoch: 6| Step: 10
Training loss: 1.605419635772705
Validation loss: 2.0154236952463784

Epoch: 6| Step: 11
Training loss: 2.0906858444213867
Validation loss: 2.0169726610183716

Epoch: 6| Step: 12
Training loss: 2.07843279838562
Validation loss: 2.035602887471517

Epoch: 6| Step: 13
Training loss: 1.9043993949890137
Validation loss: 2.0307114919026694

Epoch: 70| Step: 0
Training loss: 2.3338732719421387
Validation loss: 2.0293630162874856

Epoch: 6| Step: 1
Training loss: 2.026965856552124
Validation loss: 2.0314900875091553

Epoch: 6| Step: 2
Training loss: 1.7135471105575562
Validation loss: 2.0217429399490356

Epoch: 6| Step: 3
Training loss: 2.034648895263672
Validation loss: 2.044372081756592

Epoch: 6| Step: 4
Training loss: 1.7931678295135498
Validation loss: 2.029367526372274

Epoch: 6| Step: 5
Training loss: 2.83278751373291
Validation loss: 2.0393885175387063

Epoch: 6| Step: 6
Training loss: 2.5996170043945312
Validation loss: 2.040382166703542

Epoch: 6| Step: 7
Training loss: 2.065277099609375
Validation loss: 2.026998500029246

Epoch: 6| Step: 8
Training loss: 2.077716112136841
Validation loss: 2.0107528964678445

Epoch: 6| Step: 9
Training loss: 2.63496470451355
Validation loss: 2.015529294808706

Epoch: 6| Step: 10
Training loss: 1.801120400428772
Validation loss: 2.006587862968445

Epoch: 6| Step: 11
Training loss: 2.636563777923584
Validation loss: 2.0055870016415915

Epoch: 6| Step: 12
Training loss: 1.8058338165283203
Validation loss: 2.012146751085917

Epoch: 6| Step: 13
Training loss: 1.8832899332046509
Validation loss: 2.0091216961542764

Epoch: 71| Step: 0
Training loss: 1.7760276794433594
Validation loss: 2.0128039121627808

Epoch: 6| Step: 1
Training loss: 2.8754220008850098
Validation loss: 2.0268115599950156

Epoch: 6| Step: 2
Training loss: 1.8960753679275513
Validation loss: 2.03372993071874

Epoch: 6| Step: 3
Training loss: 2.019070863723755
Validation loss: 2.038453976313273

Epoch: 6| Step: 4
Training loss: 2.0490975379943848
Validation loss: 2.0421780745188394

Epoch: 6| Step: 5
Training loss: 1.7957710027694702
Validation loss: 2.0400410294532776

Epoch: 6| Step: 6
Training loss: 2.683499574661255
Validation loss: 2.038419465223948

Epoch: 6| Step: 7
Training loss: 2.4303109645843506
Validation loss: 2.0306167801221213

Epoch: 6| Step: 8
Training loss: 2.160160541534424
Validation loss: 2.034140964349111

Epoch: 6| Step: 9
Training loss: 2.7627928256988525
Validation loss: 2.021428565184275

Epoch: 6| Step: 10
Training loss: 2.280493974685669
Validation loss: 2.02617617448171

Epoch: 6| Step: 11
Training loss: 1.9807721376419067
Validation loss: 2.018656571706136

Epoch: 6| Step: 12
Training loss: 1.9044363498687744
Validation loss: 2.020439068476359

Epoch: 6| Step: 13
Training loss: 2.1511096954345703
Validation loss: 2.011864344278971

Epoch: 72| Step: 0
Training loss: 2.2257227897644043
Validation loss: 2.016590734322866

Epoch: 6| Step: 1
Training loss: 2.20310640335083
Validation loss: 2.0251197616259256

Epoch: 6| Step: 2
Training loss: 2.5111887454986572
Validation loss: 2.0248249173164368

Epoch: 6| Step: 3
Training loss: 1.2627497911453247
Validation loss: 2.0295748909314475

Epoch: 6| Step: 4
Training loss: 2.6093034744262695
Validation loss: 2.035334368546804

Epoch: 6| Step: 5
Training loss: 1.8293943405151367
Validation loss: 2.0439395705858865

Epoch: 6| Step: 6
Training loss: 2.0925145149230957
Validation loss: 2.0471439162890115

Epoch: 6| Step: 7
Training loss: 2.755833148956299
Validation loss: 2.0521464347839355

Epoch: 6| Step: 8
Training loss: 1.7970542907714844
Validation loss: 2.048773248990377

Epoch: 6| Step: 9
Training loss: 2.518778085708618
Validation loss: 2.0584561626116433

Epoch: 6| Step: 10
Training loss: 2.323777198791504
Validation loss: 2.026460647583008

Epoch: 6| Step: 11
Training loss: 1.8996683359146118
Validation loss: 2.0196495850880942

Epoch: 6| Step: 12
Training loss: 2.2004830837249756
Validation loss: 2.027742783228556

Epoch: 6| Step: 13
Training loss: 2.333353042602539
Validation loss: 2.011397918065389

Epoch: 73| Step: 0
Training loss: 2.1307973861694336
Validation loss: 2.005537231763204

Epoch: 6| Step: 1
Training loss: 1.8537808656692505
Validation loss: 2.00549308458964

Epoch: 6| Step: 2
Training loss: 1.6273014545440674
Validation loss: 2.0107946197191873

Epoch: 6| Step: 3
Training loss: 2.231792688369751
Validation loss: 2.005498389403025

Epoch: 6| Step: 4
Training loss: 2.432356357574463
Validation loss: 2.0059125622113547

Epoch: 6| Step: 5
Training loss: 2.276541233062744
Validation loss: 2.0042723019917807

Epoch: 6| Step: 6
Training loss: 2.6252217292785645
Validation loss: 2.0039572715759277

Epoch: 6| Step: 7
Training loss: 2.320145606994629
Validation loss: 2.0158461928367615

Epoch: 6| Step: 8
Training loss: 2.052947521209717
Validation loss: 2.018164952596029

Epoch: 6| Step: 9
Training loss: 2.0140669345855713
Validation loss: 2.0297491947809854

Epoch: 6| Step: 10
Training loss: 2.546785354614258
Validation loss: 2.0237277944882712

Epoch: 6| Step: 11
Training loss: 2.2164337635040283
Validation loss: 2.0316280722618103

Epoch: 6| Step: 12
Training loss: 1.8729082345962524
Validation loss: 2.03509392340978

Epoch: 6| Step: 13
Training loss: 1.96437668800354
Validation loss: 2.01987890402476

Epoch: 74| Step: 0
Training loss: 2.517247200012207
Validation loss: 2.013786256313324

Epoch: 6| Step: 1
Training loss: 1.782578706741333
Validation loss: 2.0064011812210083

Epoch: 6| Step: 2
Training loss: 2.5907323360443115
Validation loss: 2.0095804731051126

Epoch: 6| Step: 3
Training loss: 2.23616361618042
Validation loss: 2.003952165444692

Epoch: 6| Step: 4
Training loss: 1.8616716861724854
Validation loss: 2.0102300445238748

Epoch: 6| Step: 5
Training loss: 2.7482340335845947
Validation loss: 2.0194871624310813

Epoch: 6| Step: 6
Training loss: 2.470221757888794
Validation loss: 2.0216405590375266

Epoch: 6| Step: 7
Training loss: 2.2812018394470215
Validation loss: 2.029015521208445

Epoch: 6| Step: 8
Training loss: 1.722385287284851
Validation loss: 2.016465981801351

Epoch: 6| Step: 9
Training loss: 2.4243736267089844
Validation loss: 2.023584226767222

Epoch: 6| Step: 10
Training loss: 2.391779661178589
Validation loss: 2.012381454308828

Epoch: 6| Step: 11
Training loss: 2.058154821395874
Validation loss: 2.009086787700653

Epoch: 6| Step: 12
Training loss: 1.4077740907669067
Validation loss: 2.005427678426107

Epoch: 6| Step: 13
Training loss: 1.9732245206832886
Validation loss: 2.00826495885849

Epoch: 75| Step: 0
Training loss: 1.6509689092636108
Validation loss: 2.001681923866272

Epoch: 6| Step: 1
Training loss: 1.9938457012176514
Validation loss: 2.005719562371572

Epoch: 6| Step: 2
Training loss: 2.7784886360168457
Validation loss: 2.00194259484609

Epoch: 6| Step: 3
Training loss: 2.429215908050537
Validation loss: 2.004975219567617

Epoch: 6| Step: 4
Training loss: 1.7409250736236572
Validation loss: 2.0063194036483765

Epoch: 6| Step: 5
Training loss: 2.331775188446045
Validation loss: 2.0084603230158486

Epoch: 6| Step: 6
Training loss: 2.777453660964966
Validation loss: 2.000761349995931

Epoch: 6| Step: 7
Training loss: 1.9990601539611816
Validation loss: 2.0078871051470437

Epoch: 6| Step: 8
Training loss: 2.1751370429992676
Validation loss: 2.008535166581472

Epoch: 6| Step: 9
Training loss: 2.383413076400757
Validation loss: 2.0081008871396384

Epoch: 6| Step: 10
Training loss: 2.467829942703247
Validation loss: 2.009570320447286

Epoch: 6| Step: 11
Training loss: 1.3331753015518188
Validation loss: 2.0115989247957864

Epoch: 6| Step: 12
Training loss: 2.144282579421997
Validation loss: 2.012820601463318

Epoch: 6| Step: 13
Training loss: 2.164167881011963
Validation loss: 2.0079156160354614

Epoch: 76| Step: 0
Training loss: 2.314619541168213
Validation loss: 2.0105372865994773

Epoch: 6| Step: 1
Training loss: 2.277987241744995
Validation loss: 2.0009989738464355

Epoch: 6| Step: 2
Training loss: 1.9585422277450562
Validation loss: 2.0042407512664795

Epoch: 6| Step: 3
Training loss: 1.9320974349975586
Validation loss: 2.0054547786712646

Epoch: 6| Step: 4
Training loss: 2.172919273376465
Validation loss: 2.016439735889435

Epoch: 6| Step: 5
Training loss: 2.1852827072143555
Validation loss: 2.017562905947367

Epoch: 6| Step: 6
Training loss: 1.3298192024230957
Validation loss: 2.01059756676356

Epoch: 6| Step: 7
Training loss: 1.9829397201538086
Validation loss: 2.0106217662493386

Epoch: 6| Step: 8
Training loss: 1.6873235702514648
Validation loss: 2.0113913218180337

Epoch: 6| Step: 9
Training loss: 2.4456372261047363
Validation loss: 2.0128204226493835

Epoch: 6| Step: 10
Training loss: 2.645869255065918
Validation loss: 2.0084773699442544

Epoch: 6| Step: 11
Training loss: 3.1174044609069824
Validation loss: 2.0097045501073203

Epoch: 6| Step: 12
Training loss: 2.010530948638916
Validation loss: 2.0120861728986106

Epoch: 6| Step: 13
Training loss: 2.0780787467956543
Validation loss: 2.0098849534988403

Epoch: 77| Step: 0
Training loss: 1.4823435544967651
Validation loss: 2.0130355954170227

Epoch: 6| Step: 1
Training loss: 1.9301409721374512
Validation loss: 2.004276692867279

Epoch: 6| Step: 2
Training loss: 2.3360249996185303
Validation loss: 2.010536511739095

Epoch: 6| Step: 3
Training loss: 2.301784038543701
Validation loss: 2.017460028330485

Epoch: 6| Step: 4
Training loss: 1.7368913888931274
Validation loss: 2.0143882632255554

Epoch: 6| Step: 5
Training loss: 2.6393380165100098
Validation loss: 2.015007754166921

Epoch: 6| Step: 6
Training loss: 2.421752452850342
Validation loss: 2.020217796166738

Epoch: 6| Step: 7
Training loss: 2.0457212924957275
Validation loss: 2.014609416325887

Epoch: 6| Step: 8
Training loss: 2.8494961261749268
Validation loss: 2.0142771005630493

Epoch: 6| Step: 9
Training loss: 1.678056001663208
Validation loss: 2.016302009423574

Epoch: 6| Step: 10
Training loss: 2.343937635421753
Validation loss: 2.014209429423014

Epoch: 6| Step: 11
Training loss: 1.9597511291503906
Validation loss: 2.0213751594225564

Epoch: 6| Step: 12
Training loss: 2.138713836669922
Validation loss: 2.006348709265391

Epoch: 6| Step: 13
Training loss: 2.1581246852874756
Validation loss: 2.020228366057078

Epoch: 78| Step: 0
Training loss: 1.8241957426071167
Validation loss: 2.019084950288137

Epoch: 6| Step: 1
Training loss: 1.3126097917556763
Validation loss: 2.0248688459396362

Epoch: 6| Step: 2
Training loss: 1.5196821689605713
Validation loss: 2.036959230899811

Epoch: 6| Step: 3
Training loss: 2.559143543243408
Validation loss: 2.031959652900696

Epoch: 6| Step: 4
Training loss: 2.4027774333953857
Validation loss: 2.0308081905047097

Epoch: 6| Step: 5
Training loss: 2.3544821739196777
Validation loss: 2.0435775319735208

Epoch: 6| Step: 6
Training loss: 2.914173126220703
Validation loss: 2.0399837692578635

Epoch: 6| Step: 7
Training loss: 1.3035774230957031
Validation loss: 2.0193418661753335

Epoch: 6| Step: 8
Training loss: 2.3976314067840576
Validation loss: 2.014380176862081

Epoch: 6| Step: 9
Training loss: 2.3902037143707275
Validation loss: 2.010951360066732

Epoch: 6| Step: 10
Training loss: 1.878459095954895
Validation loss: 2.0115437308947244

Epoch: 6| Step: 11
Training loss: 2.4340734481811523
Validation loss: 2.02846093972524

Epoch: 6| Step: 12
Training loss: 2.5687320232391357
Validation loss: 2.033067226409912

Epoch: 6| Step: 13
Training loss: 2.241495132446289
Validation loss: 2.043828864892324

Epoch: 79| Step: 0
Training loss: 1.6846351623535156
Validation loss: 2.0382991234461465

Epoch: 6| Step: 1
Training loss: 2.5292255878448486
Validation loss: 2.0438051422437034

Epoch: 6| Step: 2
Training loss: 2.318624973297119
Validation loss: 2.043823858102163

Epoch: 6| Step: 3
Training loss: 2.302492380142212
Validation loss: 2.039141674836477

Epoch: 6| Step: 4
Training loss: 1.661348819732666
Validation loss: 2.047875781853994

Epoch: 6| Step: 5
Training loss: 2.284083127975464
Validation loss: 2.0388633807500205

Epoch: 6| Step: 6
Training loss: 2.5290236473083496
Validation loss: 2.044156551361084

Epoch: 6| Step: 7
Training loss: 2.333132743835449
Validation loss: 2.034069041411082

Epoch: 6| Step: 8
Training loss: 1.6478986740112305
Validation loss: 2.0352561275164285

Epoch: 6| Step: 9
Training loss: 2.446641445159912
Validation loss: 2.035954316457113

Epoch: 6| Step: 10
Training loss: 2.2602906227111816
Validation loss: 2.0286866625150046

Epoch: 6| Step: 11
Training loss: 2.1843783855438232
Validation loss: 2.030472139517466

Epoch: 6| Step: 12
Training loss: 2.536365509033203
Validation loss: 2.0279465119043985

Epoch: 6| Step: 13
Training loss: 2.202425479888916
Validation loss: 2.0283918579419455

Epoch: 80| Step: 0
Training loss: 2.691929817199707
Validation loss: 2.0197320580482483

Epoch: 6| Step: 1
Training loss: 2.5868258476257324
Validation loss: 2.0199422041575112

Epoch: 6| Step: 2
Training loss: 1.985601782798767
Validation loss: 2.0195321838061013

Epoch: 6| Step: 3
Training loss: 1.8613479137420654
Validation loss: 2.0150799552599588

Epoch: 6| Step: 4
Training loss: 2.1040475368499756
Validation loss: 2.01250559091568

Epoch: 6| Step: 5
Training loss: 2.0989038944244385
Validation loss: 2.0096096793810525

Epoch: 6| Step: 6
Training loss: 2.539557456970215
Validation loss: 2.005375782648722

Epoch: 6| Step: 7
Training loss: 2.6764395236968994
Validation loss: 2.009909451007843

Epoch: 6| Step: 8
Training loss: 1.925277829170227
Validation loss: 2.0167758663495383

Epoch: 6| Step: 9
Training loss: 2.2332863807678223
Validation loss: 2.022224267323812

Epoch: 6| Step: 10
Training loss: 1.7068949937820435
Validation loss: 2.0107212464014688

Epoch: 6| Step: 11
Training loss: 1.9505386352539062
Validation loss: 2.027585585912069

Epoch: 6| Step: 12
Training loss: 1.4966511726379395
Validation loss: 2.0315606594085693

Epoch: 6| Step: 13
Training loss: 1.9997868537902832
Validation loss: 2.0298440059026084

Epoch: 81| Step: 0
Training loss: 2.7011163234710693
Validation loss: 2.0459004044532776

Epoch: 6| Step: 1
Training loss: 2.2728090286254883
Validation loss: 2.042783240477244

Epoch: 6| Step: 2
Training loss: 2.3912739753723145
Validation loss: 2.0315533677736917

Epoch: 6| Step: 3
Training loss: 1.7587342262268066
Validation loss: 2.035528818766276

Epoch: 6| Step: 4
Training loss: 1.8109817504882812
Validation loss: 2.0243098735809326

Epoch: 6| Step: 5
Training loss: 1.91261887550354
Validation loss: 2.02113938331604

Epoch: 6| Step: 6
Training loss: 2.060864210128784
Validation loss: 2.0135069688161216

Epoch: 6| Step: 7
Training loss: 2.1238479614257812
Validation loss: 2.009095847606659

Epoch: 6| Step: 8
Training loss: 2.140054702758789
Validation loss: 2.013426721096039

Epoch: 6| Step: 9
Training loss: 1.5292940139770508
Validation loss: 2.009709596633911

Epoch: 6| Step: 10
Training loss: 2.598841667175293
Validation loss: 2.0067185958226523

Epoch: 6| Step: 11
Training loss: 2.380885362625122
Validation loss: 2.0091116428375244

Epoch: 6| Step: 12
Training loss: 2.1993799209594727
Validation loss: 2.0061865051587424

Epoch: 6| Step: 13
Training loss: 2.2841649055480957
Validation loss: 2.0055604775746665

Epoch: 82| Step: 0
Training loss: 2.580850839614868
Validation loss: 2.0090167919794717

Epoch: 6| Step: 1
Training loss: 1.9481064081192017
Validation loss: 2.00904643535614

Epoch: 6| Step: 2
Training loss: 2.479809045791626
Validation loss: 2.0096351504325867

Epoch: 6| Step: 3
Training loss: 1.7015538215637207
Validation loss: 2.0123534003893533

Epoch: 6| Step: 4
Training loss: 2.2844009399414062
Validation loss: 2.019135614236196

Epoch: 6| Step: 5
Training loss: 2.698087692260742
Validation loss: 2.0194464127222695

Epoch: 6| Step: 6
Training loss: 2.2351622581481934
Validation loss: 2.023508866628011

Epoch: 6| Step: 7
Training loss: 1.6934194564819336
Validation loss: 2.0286956429481506

Epoch: 6| Step: 8
Training loss: 2.3442859649658203
Validation loss: 2.0324991941452026

Epoch: 6| Step: 9
Training loss: 2.1988179683685303
Validation loss: 2.030065635840098

Epoch: 6| Step: 10
Training loss: 1.9908215999603271
Validation loss: 2.0343441168467202

Epoch: 6| Step: 11
Training loss: 1.5949156284332275
Validation loss: 2.037356158097585

Epoch: 6| Step: 12
Training loss: 2.3653922080993652
Validation loss: 2.028820276260376

Epoch: 6| Step: 13
Training loss: 1.913059949874878
Validation loss: 2.037843326727549

Epoch: 83| Step: 0
Training loss: 1.932059407234192
Validation loss: 2.0246771574020386

Epoch: 6| Step: 1
Training loss: 2.3320584297180176
Validation loss: 2.0173832376797995

Epoch: 6| Step: 2
Training loss: 1.5149552822113037
Validation loss: 2.0192090272903442

Epoch: 6| Step: 3
Training loss: 2.332040548324585
Validation loss: 2.0197177131970725

Epoch: 6| Step: 4
Training loss: 2.071347236633301
Validation loss: 2.022103011608124

Epoch: 6| Step: 5
Training loss: 2.7945992946624756
Validation loss: 2.031369149684906

Epoch: 6| Step: 6
Training loss: 2.1913743019104004
Validation loss: 2.0350074768066406

Epoch: 6| Step: 7
Training loss: 2.376201629638672
Validation loss: 2.0351085662841797

Epoch: 6| Step: 8
Training loss: 2.263669967651367
Validation loss: 2.027151803175608

Epoch: 6| Step: 9
Training loss: 1.465169072151184
Validation loss: 2.0272809863090515

Epoch: 6| Step: 10
Training loss: 2.271138906478882
Validation loss: 2.030886193116506

Epoch: 6| Step: 11
Training loss: 1.5494582653045654
Validation loss: 2.0137718518575034

Epoch: 6| Step: 12
Training loss: 2.2230634689331055
Validation loss: 2.0017358859380088

Epoch: 6| Step: 13
Training loss: 2.667612075805664
Validation loss: 2.0058182080586753

Epoch: 84| Step: 0
Training loss: 2.301881790161133
Validation loss: 2.0142391522725425

Epoch: 6| Step: 1
Training loss: 1.9989527463912964
Validation loss: 2.0004913012186685

Epoch: 6| Step: 2
Training loss: 2.190920829772949
Validation loss: 2.0125765204429626

Epoch: 6| Step: 3
Training loss: 2.072296142578125
Validation loss: 2.0215014219284058

Epoch: 6| Step: 4
Training loss: 2.23356294631958
Validation loss: 2.0255232453346252

Epoch: 6| Step: 5
Training loss: 1.9835623502731323
Validation loss: 2.0246708194414773

Epoch: 6| Step: 6
Training loss: 1.8532087802886963
Validation loss: 2.021577537059784

Epoch: 6| Step: 7
Training loss: 2.694793701171875
Validation loss: 2.0227914452552795

Epoch: 6| Step: 8
Training loss: 2.8903608322143555
Validation loss: 2.015231649080912

Epoch: 6| Step: 9
Training loss: 1.8551876544952393
Validation loss: 2.0113996664683023

Epoch: 6| Step: 10
Training loss: 2.0387747287750244
Validation loss: 2.0133239229520163

Epoch: 6| Step: 11
Training loss: 2.4699978828430176
Validation loss: 2.014513293902079

Epoch: 6| Step: 12
Training loss: 1.9383468627929688
Validation loss: 2.009345849355062

Epoch: 6| Step: 13
Training loss: 1.8643851280212402
Validation loss: 2.0159069697062173

Epoch: 85| Step: 0
Training loss: 2.1301522254943848
Validation loss: 2.0210735400517783

Epoch: 6| Step: 1
Training loss: 1.9561195373535156
Validation loss: 2.017819623152415

Epoch: 6| Step: 2
Training loss: 2.1839261054992676
Validation loss: 2.0271747509638467

Epoch: 6| Step: 3
Training loss: 2.20340633392334
Validation loss: 2.0426663955052695

Epoch: 6| Step: 4
Training loss: 1.913396954536438
Validation loss: 2.0394378304481506

Epoch: 6| Step: 5
Training loss: 1.8463141918182373
Validation loss: 2.0471149682998657

Epoch: 6| Step: 6
Training loss: 1.9786326885223389
Validation loss: 2.0544676979382834

Epoch: 6| Step: 7
Training loss: 2.353654384613037
Validation loss: 2.0686848759651184

Epoch: 6| Step: 8
Training loss: 2.360004425048828
Validation loss: 2.080075979232788

Epoch: 6| Step: 9
Training loss: 2.547436237335205
Validation loss: 2.071235438187917

Epoch: 6| Step: 10
Training loss: 2.99862003326416
Validation loss: 2.053811331590017

Epoch: 6| Step: 11
Training loss: 1.917668342590332
Validation loss: 2.04893167813619

Epoch: 6| Step: 12
Training loss: 2.061779022216797
Validation loss: 2.0417860746383667

Epoch: 6| Step: 13
Training loss: 1.6212704181671143
Validation loss: 2.034966071446737

Epoch: 86| Step: 0
Training loss: 1.9323112964630127
Validation loss: 2.01911731561025

Epoch: 6| Step: 1
Training loss: 2.3119421005249023
Validation loss: 2.017414172490438

Epoch: 6| Step: 2
Training loss: 1.6847870349884033
Validation loss: 2.015348434448242

Epoch: 6| Step: 3
Training loss: 2.1847174167633057
Validation loss: 2.0065706968307495

Epoch: 6| Step: 4
Training loss: 2.104323625564575
Validation loss: 2.0099880496660867

Epoch: 6| Step: 5
Training loss: 2.3676810264587402
Validation loss: 2.0088897148768106

Epoch: 6| Step: 6
Training loss: 1.7401535511016846
Validation loss: 2.0105703274408975

Epoch: 6| Step: 7
Training loss: 3.1560709476470947
Validation loss: 2.0094567934672036

Epoch: 6| Step: 8
Training loss: 1.687821388244629
Validation loss: 2.00667542219162

Epoch: 6| Step: 9
Training loss: 1.952333688735962
Validation loss: 2.0214123129844666

Epoch: 6| Step: 10
Training loss: 2.1995980739593506
Validation loss: 2.021537184715271

Epoch: 6| Step: 11
Training loss: 2.5357236862182617
Validation loss: 2.03358002503713

Epoch: 6| Step: 12
Training loss: 2.550095319747925
Validation loss: 2.0291982094446817

Epoch: 6| Step: 13
Training loss: 1.6025569438934326
Validation loss: 2.020444631576538

Epoch: 87| Step: 0
Training loss: 1.4668402671813965
Validation loss: 2.027105391025543

Epoch: 6| Step: 1
Training loss: 2.216498374938965
Validation loss: 2.025828182697296

Epoch: 6| Step: 2
Training loss: 2.7662155628204346
Validation loss: 2.0249075889587402

Epoch: 6| Step: 3
Training loss: 2.827043056488037
Validation loss: 2.0318820079167685

Epoch: 6| Step: 4
Training loss: 2.221921443939209
Validation loss: 2.0250097115834556

Epoch: 6| Step: 5
Training loss: 1.7731330394744873
Validation loss: 2.020260194937388

Epoch: 6| Step: 6
Training loss: 1.773555040359497
Validation loss: 2.0232890049616494

Epoch: 6| Step: 7
Training loss: 2.3688724040985107
Validation loss: 2.0186289151509604

Epoch: 6| Step: 8
Training loss: 1.9955817461013794
Validation loss: 2.032149692376455

Epoch: 6| Step: 9
Training loss: 1.2819217443466187
Validation loss: 2.0327072739601135

Epoch: 6| Step: 10
Training loss: 1.9338067770004272
Validation loss: 2.0205252965291343

Epoch: 6| Step: 11
Training loss: 2.806013822555542
Validation loss: 2.0408302545547485

Epoch: 6| Step: 12
Training loss: 2.078216552734375
Validation loss: 2.0418473283449807

Epoch: 6| Step: 13
Training loss: 2.271815538406372
Validation loss: 2.038955648740133

Epoch: 88| Step: 0
Training loss: 1.828552007675171
Validation loss: 2.0368873874346414

Epoch: 6| Step: 1
Training loss: 1.776258111000061
Validation loss: 2.0354413787523904

Epoch: 6| Step: 2
Training loss: 2.9657773971557617
Validation loss: 2.030085027217865

Epoch: 6| Step: 3
Training loss: 1.9582797288894653
Validation loss: 2.0304227073987327

Epoch: 6| Step: 4
Training loss: 1.9354901313781738
Validation loss: 2.032085597515106

Epoch: 6| Step: 5
Training loss: 2.103820323944092
Validation loss: 2.022785405317942

Epoch: 6| Step: 6
Training loss: 1.6656726598739624
Validation loss: 2.0229281783103943

Epoch: 6| Step: 7
Training loss: 2.8212778568267822
Validation loss: 2.009734630584717

Epoch: 6| Step: 8
Training loss: 1.9912711381912231
Validation loss: 2.013911247253418

Epoch: 6| Step: 9
Training loss: 2.7401509284973145
Validation loss: 2.0149365862210593

Epoch: 6| Step: 10
Training loss: 2.0707285404205322
Validation loss: 2.0170993407567344

Epoch: 6| Step: 11
Training loss: 2.005370855331421
Validation loss: 2.018003741900126

Epoch: 6| Step: 12
Training loss: 1.9895782470703125
Validation loss: 2.0090912580490112

Epoch: 6| Step: 13
Training loss: 2.0016896724700928
Validation loss: 2.0141031742095947

Epoch: 89| Step: 0
Training loss: 1.8257968425750732
Validation loss: 2.0140778621037803

Epoch: 6| Step: 1
Training loss: 1.755332350730896
Validation loss: 2.019732634226481

Epoch: 6| Step: 2
Training loss: 1.7321336269378662
Validation loss: 2.027966856956482

Epoch: 6| Step: 3
Training loss: 2.097414016723633
Validation loss: 2.0205352505048118

Epoch: 6| Step: 4
Training loss: 2.2306602001190186
Validation loss: 2.028592864672343

Epoch: 6| Step: 5
Training loss: 1.755378007888794
Validation loss: 2.02874489625295

Epoch: 6| Step: 6
Training loss: 2.7957191467285156
Validation loss: 2.027592658996582

Epoch: 6| Step: 7
Training loss: 2.3058109283447266
Validation loss: 2.031642039616903

Epoch: 6| Step: 8
Training loss: 2.2380313873291016
Validation loss: 2.022177219390869

Epoch: 6| Step: 9
Training loss: 2.7301201820373535
Validation loss: 2.0137490034103394

Epoch: 6| Step: 10
Training loss: 2.428633689880371
Validation loss: 2.0243170261383057

Epoch: 6| Step: 11
Training loss: 1.9213740825653076
Validation loss: 2.0101435780525208

Epoch: 6| Step: 12
Training loss: 1.808743953704834
Validation loss: 2.0069957971572876

Epoch: 6| Step: 13
Training loss: 2.2679333686828613
Validation loss: 2.0172393321990967

Epoch: 90| Step: 0
Training loss: 2.1447081565856934
Validation loss: 2.014614979426066

Epoch: 6| Step: 1
Training loss: 2.431246280670166
Validation loss: 2.0090912779172263

Epoch: 6| Step: 2
Training loss: 1.9389379024505615
Validation loss: 2.017442544301351

Epoch: 6| Step: 3
Training loss: 1.686802864074707
Validation loss: 2.0188185373942056

Epoch: 6| Step: 4
Training loss: 1.68747079372406
Validation loss: 2.0200555324554443

Epoch: 6| Step: 5
Training loss: 2.087712526321411
Validation loss: 2.0240584015846252

Epoch: 6| Step: 6
Training loss: 1.9819128513336182
Validation loss: 2.023371636867523

Epoch: 6| Step: 7
Training loss: 2.6052041053771973
Validation loss: 2.013119955857595

Epoch: 6| Step: 8
Training loss: 2.092630386352539
Validation loss: 2.0184138218561807

Epoch: 6| Step: 9
Training loss: 2.632512092590332
Validation loss: 2.01532248655955

Epoch: 6| Step: 10
Training loss: 2.490659475326538
Validation loss: 2.0124645034472146

Epoch: 6| Step: 11
Training loss: 1.9948443174362183
Validation loss: 2.0189252297083535

Epoch: 6| Step: 12
Training loss: 1.9655449390411377
Validation loss: 2.010327676932017

Epoch: 6| Step: 13
Training loss: 2.0393319129943848
Validation loss: 2.0119709769884744

Epoch: 91| Step: 0
Training loss: 1.6762639284133911
Validation loss: 2.018952230612437

Epoch: 6| Step: 1
Training loss: 1.9436113834381104
Validation loss: 2.0224722226460776

Epoch: 6| Step: 2
Training loss: 1.9296376705169678
Validation loss: 2.01973162094752

Epoch: 6| Step: 3
Training loss: 2.4475674629211426
Validation loss: 2.017740865548452

Epoch: 6| Step: 4
Training loss: 2.7687368392944336
Validation loss: 2.027464667956034

Epoch: 6| Step: 5
Training loss: 1.8472527265548706
Validation loss: 2.0177449186642966

Epoch: 6| Step: 6
Training loss: 2.152642011642456
Validation loss: 2.01898056268692

Epoch: 6| Step: 7
Training loss: 2.022108793258667
Validation loss: 2.012667636076609

Epoch: 6| Step: 8
Training loss: 1.7284798622131348
Validation loss: 2.0192912419637046

Epoch: 6| Step: 9
Training loss: 2.1439926624298096
Validation loss: 2.018437842528025

Epoch: 6| Step: 10
Training loss: 1.8411076068878174
Validation loss: 2.0185784896214805

Epoch: 6| Step: 11
Training loss: 2.140352249145508
Validation loss: 2.024452785650889

Epoch: 6| Step: 12
Training loss: 2.4566433429718018
Validation loss: 2.0195035338401794

Epoch: 6| Step: 13
Training loss: 2.643393039703369
Validation loss: 2.0327462951342263

Epoch: 92| Step: 0
Training loss: 2.4572641849517822
Validation loss: 2.0424490769704184

Epoch: 6| Step: 1
Training loss: 2.491455554962158
Validation loss: 2.028514087200165

Epoch: 6| Step: 2
Training loss: 1.7511096000671387
Validation loss: 2.0531877875328064

Epoch: 6| Step: 3
Training loss: 1.5582671165466309
Validation loss: 2.0612800121307373

Epoch: 6| Step: 4
Training loss: 2.658505916595459
Validation loss: 2.045379082361857

Epoch: 6| Step: 5
Training loss: 1.91249418258667
Validation loss: 2.023595909277598

Epoch: 6| Step: 6
Training loss: 1.8093342781066895
Validation loss: 2.018655081590017

Epoch: 6| Step: 7
Training loss: 2.155421257019043
Validation loss: 2.0150684118270874

Epoch: 6| Step: 8
Training loss: 2.3559093475341797
Validation loss: 2.016616483529409

Epoch: 6| Step: 9
Training loss: 2.3177900314331055
Validation loss: 2.0143027901649475

Epoch: 6| Step: 10
Training loss: 2.082995891571045
Validation loss: 2.014229476451874

Epoch: 6| Step: 11
Training loss: 1.9256908893585205
Validation loss: 2.0146403710047402

Epoch: 6| Step: 12
Training loss: 2.0677685737609863
Validation loss: 2.0096800724665322

Epoch: 6| Step: 13
Training loss: 2.1260647773742676
Validation loss: 2.0197949409484863

Epoch: 93| Step: 0
Training loss: 2.185532569885254
Validation loss: 2.0148593386014304

Epoch: 6| Step: 1
Training loss: 1.989650011062622
Validation loss: 2.0214274326960244

Epoch: 6| Step: 2
Training loss: 2.538905382156372
Validation loss: 2.0075353384017944

Epoch: 6| Step: 3
Training loss: 2.2059919834136963
Validation loss: 2.016747077306112

Epoch: 6| Step: 4
Training loss: 2.3740901947021484
Validation loss: 2.021822532018026

Epoch: 6| Step: 5
Training loss: 2.2407033443450928
Validation loss: 2.0136414567629495

Epoch: 6| Step: 6
Training loss: 2.214813470840454
Validation loss: 2.0158933202425637

Epoch: 6| Step: 7
Training loss: 1.9410099983215332
Validation loss: 2.009856343269348

Epoch: 6| Step: 8
Training loss: 1.9942495822906494
Validation loss: 2.0158279140790305

Epoch: 6| Step: 9
Training loss: 2.0015459060668945
Validation loss: 2.033690571784973

Epoch: 6| Step: 10
Training loss: 2.031189441680908
Validation loss: 2.0384541948636374

Epoch: 6| Step: 11
Training loss: 2.2070047855377197
Validation loss: 2.036824663480123

Epoch: 6| Step: 12
Training loss: 2.1055426597595215
Validation loss: 2.038439393043518

Epoch: 6| Step: 13
Training loss: 1.577427864074707
Validation loss: 2.0453019539515176

Epoch: 94| Step: 0
Training loss: 2.8482751846313477
Validation loss: 2.0437671740849814

Epoch: 6| Step: 1
Training loss: 2.015018939971924
Validation loss: 2.039358218510946

Epoch: 6| Step: 2
Training loss: 1.8023312091827393
Validation loss: 2.0489873687426248

Epoch: 6| Step: 3
Training loss: 2.1517271995544434
Validation loss: 2.051962971687317

Epoch: 6| Step: 4
Training loss: 2.186495542526245
Validation loss: 2.052609662214915

Epoch: 6| Step: 5
Training loss: 1.7153640985488892
Validation loss: 2.0335723559061685

Epoch: 6| Step: 6
Training loss: 1.7650272846221924
Validation loss: 2.03059717019399

Epoch: 6| Step: 7
Training loss: 1.653543472290039
Validation loss: 2.038544217745463

Epoch: 6| Step: 8
Training loss: 1.9997761249542236
Validation loss: 2.0359415411949158

Epoch: 6| Step: 9
Training loss: 2.112452507019043
Validation loss: 2.0315999587376914

Epoch: 6| Step: 10
Training loss: 2.197082281112671
Validation loss: 2.0157580574353537

Epoch: 6| Step: 11
Training loss: 2.660090446472168
Validation loss: 2.008936663468679

Epoch: 6| Step: 12
Training loss: 2.857243061065674
Validation loss: 2.012026786804199

Epoch: 6| Step: 13
Training loss: 1.7464144229888916
Validation loss: 2.0118401447931924

Epoch: 95| Step: 0
Training loss: 1.9616680145263672
Validation loss: 2.0035663644472756

Epoch: 6| Step: 1
Training loss: 1.7703235149383545
Validation loss: 2.0173924565315247

Epoch: 6| Step: 2
Training loss: 2.04237699508667
Validation loss: 2.017635405063629

Epoch: 6| Step: 3
Training loss: 2.0793991088867188
Validation loss: 2.0135557452837625

Epoch: 6| Step: 4
Training loss: 2.1316170692443848
Validation loss: 2.0028701623280845

Epoch: 6| Step: 5
Training loss: 2.3802285194396973
Validation loss: 2.0101787646611533

Epoch: 6| Step: 6
Training loss: 2.964749574661255
Validation loss: 2.0177059173583984

Epoch: 6| Step: 7
Training loss: 2.467029571533203
Validation loss: 2.013157864411672

Epoch: 6| Step: 8
Training loss: 2.059631824493408
Validation loss: 2.0087212721506753

Epoch: 6| Step: 9
Training loss: 1.9740195274353027
Validation loss: 2.0098104079564414

Epoch: 6| Step: 10
Training loss: 1.7064692974090576
Validation loss: 2.004076619942983

Epoch: 6| Step: 11
Training loss: 1.8791446685791016
Validation loss: 2.007053871949514

Epoch: 6| Step: 12
Training loss: 1.6723076105117798
Validation loss: 2.01224023103714

Epoch: 6| Step: 13
Training loss: 2.5209732055664062
Validation loss: 2.022574702898661

Epoch: 96| Step: 0
Training loss: 2.889382839202881
Validation loss: 2.037310481071472

Epoch: 6| Step: 1
Training loss: 2.1718525886535645
Validation loss: 2.0460556149482727

Epoch: 6| Step: 2
Training loss: 1.6441245079040527
Validation loss: 2.060311814149221

Epoch: 6| Step: 3
Training loss: 1.8363991975784302
Validation loss: 2.0584142605463662

Epoch: 6| Step: 4
Training loss: 2.362607479095459
Validation loss: 2.0640146732330322

Epoch: 6| Step: 5
Training loss: 1.935214877128601
Validation loss: 2.060459236303965

Epoch: 6| Step: 6
Training loss: 2.2371582984924316
Validation loss: 2.055194536844889

Epoch: 6| Step: 7
Training loss: 2.070511817932129
Validation loss: 2.047509789466858

Epoch: 6| Step: 8
Training loss: 2.983362913131714
Validation loss: 2.036798576513926

Epoch: 6| Step: 9
Training loss: 1.5797204971313477
Validation loss: 2.024271229902903

Epoch: 6| Step: 10
Training loss: 2.0513687133789062
Validation loss: 2.0240442554155984

Epoch: 6| Step: 11
Training loss: 2.217725992202759
Validation loss: 2.0176814993222556

Epoch: 6| Step: 12
Training loss: 1.6182124614715576
Validation loss: 2.005106786886851

Epoch: 6| Step: 13
Training loss: 2.2036197185516357
Validation loss: 2.0001105467478433

Epoch: 97| Step: 0
Training loss: 2.139582395553589
Validation loss: 2.006990929444631

Epoch: 6| Step: 1
Training loss: 2.5618348121643066
Validation loss: 2.007944186528524

Epoch: 6| Step: 2
Training loss: 2.0464859008789062
Validation loss: 2.006708880265554

Epoch: 6| Step: 3
Training loss: 2.1145317554473877
Validation loss: 2.0084763566652932

Epoch: 6| Step: 4
Training loss: 1.9272348880767822
Validation loss: 2.0067358016967773

Epoch: 6| Step: 5
Training loss: 2.143766164779663
Validation loss: 2.0035807291666665

Epoch: 6| Step: 6
Training loss: 2.1283082962036133
Validation loss: 2.0071246226628623

Epoch: 6| Step: 7
Training loss: 2.0888733863830566
Validation loss: 2.0102703968683877

Epoch: 6| Step: 8
Training loss: 2.0984280109405518
Validation loss: 2.0107444127400718

Epoch: 6| Step: 9
Training loss: 2.4357168674468994
Validation loss: 2.0114746491114297

Epoch: 6| Step: 10
Training loss: 1.9402759075164795
Validation loss: 2.0061158339182534

Epoch: 6| Step: 11
Training loss: 2.0621118545532227
Validation loss: 1.9999603033065796

Epoch: 6| Step: 12
Training loss: 2.323908805847168
Validation loss: 2.001131276289622

Epoch: 6| Step: 13
Training loss: 1.8461081981658936
Validation loss: 2.0063332517941794

Epoch: 98| Step: 0
Training loss: 2.8726916313171387
Validation loss: 2.0018234848976135

Epoch: 6| Step: 1
Training loss: 1.9447381496429443
Validation loss: 2.0087501207987466

Epoch: 6| Step: 2
Training loss: 2.199002742767334
Validation loss: 2.0139381289482117

Epoch: 6| Step: 3
Training loss: 1.6819190979003906
Validation loss: 2.0200735926628113

Epoch: 6| Step: 4
Training loss: 2.0116353034973145
Validation loss: 2.029247204462687

Epoch: 6| Step: 5
Training loss: 2.625716209411621
Validation loss: 2.028874476750692

Epoch: 6| Step: 6
Training loss: 2.431029796600342
Validation loss: 2.0362051725387573

Epoch: 6| Step: 7
Training loss: 2.2581446170806885
Validation loss: 2.0296038389205933

Epoch: 6| Step: 8
Training loss: 1.9697248935699463
Validation loss: 2.016313989957174

Epoch: 6| Step: 9
Training loss: 2.126781940460205
Validation loss: 2.01737904548645

Epoch: 6| Step: 10
Training loss: 1.8862249851226807
Validation loss: 2.0229201714197793

Epoch: 6| Step: 11
Training loss: 1.7529538869857788
Validation loss: 2.018852790196737

Epoch: 6| Step: 12
Training loss: 1.6490198373794556
Validation loss: 2.0296711921691895

Epoch: 6| Step: 13
Training loss: 2.150881767272949
Validation loss: 2.0141602953275046

Epoch: 99| Step: 0
Training loss: 2.089360237121582
Validation loss: 2.016927401224772

Epoch: 6| Step: 1
Training loss: 2.3533382415771484
Validation loss: 2.009334623813629

Epoch: 6| Step: 2
Training loss: 1.9195313453674316
Validation loss: 2.009312391281128

Epoch: 6| Step: 3
Training loss: 2.320110321044922
Validation loss: 2.0063644647598267

Epoch: 6| Step: 4
Training loss: 2.0476107597351074
Validation loss: 2.0055134097735086

Epoch: 6| Step: 5
Training loss: 1.5315308570861816
Validation loss: 2.007374902566274

Epoch: 6| Step: 6
Training loss: 1.992192268371582
Validation loss: 2.0119258165359497

Epoch: 6| Step: 7
Training loss: 2.081991672515869
Validation loss: 2.0071076353391013

Epoch: 6| Step: 8
Training loss: 2.2173924446105957
Validation loss: 2.01230255762736

Epoch: 6| Step: 9
Training loss: 2.319326877593994
Validation loss: 2.0223767360051474

Epoch: 6| Step: 10
Training loss: 1.830855131149292
Validation loss: 2.0233119328816733

Epoch: 6| Step: 11
Training loss: 1.9453948736190796
Validation loss: 2.0211291909217834

Epoch: 6| Step: 12
Training loss: 2.0113608837127686
Validation loss: 2.028541684150696

Epoch: 6| Step: 13
Training loss: 2.711667537689209
Validation loss: 2.0366696317990622

Epoch: 100| Step: 0
Training loss: 2.5350284576416016
Validation loss: 2.0308972795804343

Epoch: 6| Step: 1
Training loss: 1.6708478927612305
Validation loss: 2.0247252186139426

Epoch: 6| Step: 2
Training loss: 1.9947214126586914
Validation loss: 2.034157931804657

Epoch: 6| Step: 3
Training loss: 2.658125162124634
Validation loss: 2.0221908688545227

Epoch: 6| Step: 4
Training loss: 1.779845118522644
Validation loss: 2.028435468673706

Epoch: 6| Step: 5
Training loss: 2.1068880558013916
Validation loss: 2.035073737303416

Epoch: 6| Step: 6
Training loss: 1.673828125
Validation loss: 2.0328040520350137

Epoch: 6| Step: 7
Training loss: 1.9739291667938232
Validation loss: 2.0339684089024863

Epoch: 6| Step: 8
Training loss: 2.078469753265381
Validation loss: 2.0287338495254517

Epoch: 6| Step: 9
Training loss: 1.5834424495697021
Validation loss: 2.036282022794088

Epoch: 6| Step: 10
Training loss: 3.1327898502349854
Validation loss: 2.03366090854009

Epoch: 6| Step: 11
Training loss: 1.8903498649597168
Validation loss: 2.0462632179260254

Epoch: 6| Step: 12
Training loss: 2.796398162841797
Validation loss: 2.037923057874044

Epoch: 6| Step: 13
Training loss: 1.539145588874817
Validation loss: 2.0343543887138367

Epoch: 101| Step: 0
Training loss: 1.9489009380340576
Validation loss: 2.029897709687551

Epoch: 6| Step: 1
Training loss: 2.6051316261291504
Validation loss: 2.0303784608840942

Epoch: 6| Step: 2
Training loss: 2.102397918701172
Validation loss: 2.0133853753407798

Epoch: 6| Step: 3
Training loss: 2.5141751766204834
Validation loss: 2.0240515073140464

Epoch: 6| Step: 4
Training loss: 1.673926591873169
Validation loss: 2.014409840106964

Epoch: 6| Step: 5
Training loss: 1.8840899467468262
Validation loss: 2.0216410756111145

Epoch: 6| Step: 6
Training loss: 2.4341602325439453
Validation loss: 2.019267658392588

Epoch: 6| Step: 7
Training loss: 1.7920725345611572
Validation loss: 2.0224536856015525

Epoch: 6| Step: 8
Training loss: 1.967298984527588
Validation loss: 2.0199697017669678

Epoch: 6| Step: 9
Training loss: 1.7496192455291748
Validation loss: 2.019224683443705

Epoch: 6| Step: 10
Training loss: 1.9830485582351685
Validation loss: 2.010477364063263

Epoch: 6| Step: 11
Training loss: 2.151981830596924
Validation loss: 2.024868925412496

Epoch: 6| Step: 12
Training loss: 2.0029430389404297
Validation loss: 2.0231375296910605

Epoch: 6| Step: 13
Training loss: 2.629549741744995
Validation loss: 2.024957815806071

Epoch: 102| Step: 0
Training loss: 2.089320182800293
Validation loss: 2.027939260005951

Epoch: 6| Step: 1
Training loss: 1.9056347608566284
Validation loss: 2.0308949947357178

Epoch: 6| Step: 2
Training loss: 2.1737794876098633
Validation loss: 2.0398073395093284

Epoch: 6| Step: 3
Training loss: 1.6712183952331543
Validation loss: 2.02860560019811

Epoch: 6| Step: 4
Training loss: 2.1855411529541016
Validation loss: 2.0261237223943076

Epoch: 6| Step: 5
Training loss: 2.289191246032715
Validation loss: 2.03405898809433

Epoch: 6| Step: 6
Training loss: 2.2692031860351562
Validation loss: 2.0301630099614463

Epoch: 6| Step: 7
Training loss: 2.162966728210449
Validation loss: 2.0266565481821694

Epoch: 6| Step: 8
Training loss: 1.8336174488067627
Validation loss: 2.0354102651278176

Epoch: 6| Step: 9
Training loss: 2.3341169357299805
Validation loss: 2.036666214466095

Epoch: 6| Step: 10
Training loss: 2.344852924346924
Validation loss: 2.03115314245224

Epoch: 6| Step: 11
Training loss: 1.6603126525878906
Validation loss: 2.040801445643107

Epoch: 6| Step: 12
Training loss: 2.4425368309020996
Validation loss: 2.0517292817433677

Epoch: 6| Step: 13
Training loss: 1.9080885648727417
Validation loss: 2.0436971187591553

Epoch: 103| Step: 0
Training loss: 2.13669490814209
Validation loss: 2.0399248600006104

Epoch: 6| Step: 1
Training loss: 2.1158182621002197
Validation loss: 2.032270888487498

Epoch: 6| Step: 2
Training loss: 2.4133715629577637
Validation loss: 2.0312674045562744

Epoch: 6| Step: 3
Training loss: 2.0301761627197266
Validation loss: 2.0190501610438027

Epoch: 6| Step: 4
Training loss: 2.7443957328796387
Validation loss: 2.0223081509272256

Epoch: 6| Step: 5
Training loss: 1.4065823554992676
Validation loss: 2.009627024332682

Epoch: 6| Step: 6
Training loss: 2.3438024520874023
Validation loss: 2.0198339025179544

Epoch: 6| Step: 7
Training loss: 2.2411134243011475
Validation loss: 2.0136916041374207

Epoch: 6| Step: 8
Training loss: 2.312257766723633
Validation loss: 2.0284462372461953

Epoch: 6| Step: 9
Training loss: 2.0106334686279297
Validation loss: 2.0125257770220437

Epoch: 6| Step: 10
Training loss: 1.7470649480819702
Validation loss: 2.0171383221944175

Epoch: 6| Step: 11
Training loss: 2.229071617126465
Validation loss: 2.0199547012646994

Epoch: 6| Step: 12
Training loss: 1.6034709215164185
Validation loss: 2.0200570623079934

Epoch: 6| Step: 13
Training loss: 2.2184813022613525
Validation loss: 2.0315491755803428

Epoch: 104| Step: 0
Training loss: 1.8550540208816528
Validation loss: 2.0380765199661255

Epoch: 6| Step: 1
Training loss: 2.6015267372131348
Validation loss: 2.0337828397750854

Epoch: 6| Step: 2
Training loss: 1.663750410079956
Validation loss: 2.048889458179474

Epoch: 6| Step: 3
Training loss: 3.1642966270446777
Validation loss: 2.0428282419840493

Epoch: 6| Step: 4
Training loss: 2.0424952507019043
Validation loss: 2.034308830897013

Epoch: 6| Step: 5
Training loss: 1.197493076324463
Validation loss: 2.038738509019216

Epoch: 6| Step: 6
Training loss: 1.783312439918518
Validation loss: 2.02518900235494

Epoch: 6| Step: 7
Training loss: 1.738126277923584
Validation loss: 2.0420895417531333

Epoch: 6| Step: 8
Training loss: 2.4782142639160156
Validation loss: 2.0314685702323914

Epoch: 6| Step: 9
Training loss: 2.4002771377563477
Validation loss: 2.0183316469192505

Epoch: 6| Step: 10
Training loss: 2.273407220840454
Validation loss: 2.015700081984202

Epoch: 6| Step: 11
Training loss: 1.9111583232879639
Validation loss: 2.0223713914553323

Epoch: 6| Step: 12
Training loss: 2.3956258296966553
Validation loss: 2.01987091700236

Epoch: 6| Step: 13
Training loss: 1.72279691696167
Validation loss: 2.0257389148076377

Epoch: 105| Step: 0
Training loss: 1.732212781906128
Validation loss: 2.0206660628318787

Epoch: 6| Step: 1
Training loss: 2.097390651702881
Validation loss: 2.0315909584363303

Epoch: 6| Step: 2
Training loss: 2.984086513519287
Validation loss: 2.0200873017311096

Epoch: 6| Step: 3
Training loss: 1.5960670709609985
Validation loss: 2.0233608881632485

Epoch: 6| Step: 4
Training loss: 1.671852707862854
Validation loss: 2.0260226329167685

Epoch: 6| Step: 5
Training loss: 2.5331783294677734
Validation loss: 2.032768706480662

Epoch: 6| Step: 6
Training loss: 1.397475242614746
Validation loss: 2.026519020398458

Epoch: 6| Step: 7
Training loss: 2.0146853923797607
Validation loss: 2.0247788429260254

Epoch: 6| Step: 8
Training loss: 1.8241808414459229
Validation loss: 2.0293946862220764

Epoch: 6| Step: 9
Training loss: 2.787537097930908
Validation loss: 2.03512704372406

Epoch: 6| Step: 10
Training loss: 1.8636233806610107
Validation loss: 2.0491095383961997

Epoch: 6| Step: 11
Training loss: 2.2445077896118164
Validation loss: 2.059002637863159

Epoch: 6| Step: 12
Training loss: 2.271824836730957
Validation loss: 2.050917645295461

Epoch: 6| Step: 13
Training loss: 1.837235689163208
Validation loss: 2.0515878001848855

Epoch: 106| Step: 0
Training loss: 1.5607273578643799
Validation loss: 2.0478436946868896

Epoch: 6| Step: 1
Training loss: 2.213833808898926
Validation loss: 2.0430129766464233

Epoch: 6| Step: 2
Training loss: 2.0650203227996826
Validation loss: 2.03303196032842

Epoch: 6| Step: 3
Training loss: 1.8058117628097534
Validation loss: 2.0260319908459983

Epoch: 6| Step: 4
Training loss: 2.017152786254883
Validation loss: 2.0229399601618447

Epoch: 6| Step: 5
Training loss: 2.0864837169647217
Validation loss: 2.018796900908152

Epoch: 6| Step: 6
Training loss: 2.2645561695098877
Validation loss: 2.023829758167267

Epoch: 6| Step: 7
Training loss: 2.729020595550537
Validation loss: 2.015609304110209

Epoch: 6| Step: 8
Training loss: 2.29065203666687
Validation loss: 2.0242935617764792

Epoch: 6| Step: 9
Training loss: 2.421999216079712
Validation loss: 2.0317466259002686

Epoch: 6| Step: 10
Training loss: 1.5981311798095703
Validation loss: 2.0321176648139954

Epoch: 6| Step: 11
Training loss: 2.1890766620635986
Validation loss: 2.032026469707489

Epoch: 6| Step: 12
Training loss: 2.0672712326049805
Validation loss: 2.0285369555155435

Epoch: 6| Step: 13
Training loss: 2.105376720428467
Validation loss: 2.0274693965911865

Epoch: 107| Step: 0
Training loss: 2.070364475250244
Validation loss: 2.032388746738434

Epoch: 6| Step: 1
Training loss: 1.9688498973846436
Validation loss: 2.021126468976339

Epoch: 6| Step: 2
Training loss: 2.4529104232788086
Validation loss: 2.031707445780436

Epoch: 6| Step: 3
Training loss: 2.4536261558532715
Validation loss: 2.0265418887138367

Epoch: 6| Step: 4
Training loss: 2.1741068363189697
Validation loss: 2.0393890539805093

Epoch: 6| Step: 5
Training loss: 2.7592434883117676
Validation loss: 2.030949850877126

Epoch: 6| Step: 6
Training loss: 1.5428264141082764
Validation loss: 2.02809206644694

Epoch: 6| Step: 7
Training loss: 1.8297452926635742
Validation loss: 2.042936384677887

Epoch: 6| Step: 8
Training loss: 1.81998610496521
Validation loss: 2.0483105580012

Epoch: 6| Step: 9
Training loss: 1.6470588445663452
Validation loss: 2.0266418854395547

Epoch: 6| Step: 10
Training loss: 2.1887781620025635
Validation loss: 2.0256295998891196

Epoch: 6| Step: 11
Training loss: 2.771359920501709
Validation loss: 2.0291133920351663

Epoch: 6| Step: 12
Training loss: 1.9541147947311401
Validation loss: 2.0220338106155396

Epoch: 6| Step: 13
Training loss: 1.714003324508667
Validation loss: 2.0356408953666687

Epoch: 108| Step: 0
Training loss: 1.7114200592041016
Validation loss: 2.026831408341726

Epoch: 6| Step: 1
Training loss: 2.163395881652832
Validation loss: 2.0375030239423118

Epoch: 6| Step: 2
Training loss: 2.2054715156555176
Validation loss: 2.0376192529996238

Epoch: 6| Step: 3
Training loss: 2.30557918548584
Validation loss: 2.0351592898368835

Epoch: 6| Step: 4
Training loss: 2.028191089630127
Validation loss: 2.0343711773554483

Epoch: 6| Step: 5
Training loss: 2.292153835296631
Validation loss: 2.0238052805264792

Epoch: 6| Step: 6
Training loss: 1.9988622665405273
Validation loss: 2.0315463145573935

Epoch: 6| Step: 7
Training loss: 2.5151262283325195
Validation loss: 2.0375691254933677

Epoch: 6| Step: 8
Training loss: 1.6027626991271973
Validation loss: 2.027587334314982

Epoch: 6| Step: 9
Training loss: 1.7468369007110596
Validation loss: 2.0267320473988852

Epoch: 6| Step: 10
Training loss: 1.9686309099197388
Validation loss: 2.0286020040512085

Epoch: 6| Step: 11
Training loss: 2.3167240619659424
Validation loss: 2.028226673603058

Epoch: 6| Step: 12
Training loss: 2.1714024543762207
Validation loss: 2.0274719993273416

Epoch: 6| Step: 13
Training loss: 2.0241827964782715
Validation loss: 2.028258124987284

Epoch: 109| Step: 0
Training loss: 2.4168589115142822
Validation loss: 2.0331666270891824

Epoch: 6| Step: 1
Training loss: 2.4144697189331055
Validation loss: 2.0265844066937766

Epoch: 6| Step: 2
Training loss: 2.129398822784424
Validation loss: 2.025028347969055

Epoch: 6| Step: 3
Training loss: 1.5971285104751587
Validation loss: 2.021871507167816

Epoch: 6| Step: 4
Training loss: 2.515043020248413
Validation loss: 2.030264377593994

Epoch: 6| Step: 5
Training loss: 2.2513604164123535
Validation loss: 2.0380619963010154

Epoch: 6| Step: 6
Training loss: 2.0496039390563965
Validation loss: 2.024065534273783

Epoch: 6| Step: 7
Training loss: 2.1455748081207275
Validation loss: 2.038958112398783

Epoch: 6| Step: 8
Training loss: 1.8740513324737549
Validation loss: 2.0428088704744973

Epoch: 6| Step: 9
Training loss: 2.008165121078491
Validation loss: 2.0302796761194863

Epoch: 6| Step: 10
Training loss: 1.6889570951461792
Validation loss: 2.026399473349253

Epoch: 6| Step: 11
Training loss: 2.1741714477539062
Validation loss: 2.012221336364746

Epoch: 6| Step: 12
Training loss: 1.5552561283111572
Validation loss: 2.0113510290781655

Epoch: 6| Step: 13
Training loss: 2.0908515453338623
Validation loss: 2.020958145459493

Epoch: 110| Step: 0
Training loss: 1.8633766174316406
Validation loss: 2.0163729588190713

Epoch: 6| Step: 1
Training loss: 2.4164350032806396
Validation loss: 2.014523903528849

Epoch: 6| Step: 2
Training loss: 2.1685988903045654
Validation loss: 2.0125834544499717

Epoch: 6| Step: 3
Training loss: 1.745959758758545
Validation loss: 2.0132609605789185

Epoch: 6| Step: 4
Training loss: 2.1677703857421875
Validation loss: 2.0156901677449546

Epoch: 6| Step: 5
Training loss: 1.9881479740142822
Validation loss: 2.0248393416404724

Epoch: 6| Step: 6
Training loss: 2.5495738983154297
Validation loss: 2.0233293970425925

Epoch: 6| Step: 7
Training loss: 2.141948938369751
Validation loss: 2.0282121300697327

Epoch: 6| Step: 8
Training loss: 1.8807049989700317
Validation loss: 2.0415116349856057

Epoch: 6| Step: 9
Training loss: 1.7994489669799805
Validation loss: 2.0373233556747437

Epoch: 6| Step: 10
Training loss: 2.0045900344848633
Validation loss: 2.048497955004374

Epoch: 6| Step: 11
Training loss: 1.4776442050933838
Validation loss: 2.0459163387616477

Epoch: 6| Step: 12
Training loss: 2.3248066902160645
Validation loss: 2.0394006768862405

Epoch: 6| Step: 13
Training loss: 2.61384654045105
Validation loss: 2.0447476307551065

Epoch: 111| Step: 0
Training loss: 2.434115409851074
Validation loss: 2.041252593199412

Epoch: 6| Step: 1
Training loss: 2.0250282287597656
Validation loss: 2.024980843067169

Epoch: 6| Step: 2
Training loss: 2.291778564453125
Validation loss: 2.0322206219037375

Epoch: 6| Step: 3
Training loss: 2.050262212753296
Validation loss: 2.034725308418274

Epoch: 6| Step: 4
Training loss: 2.2738914489746094
Validation loss: 2.0277887980143228

Epoch: 6| Step: 5
Training loss: 1.7187542915344238
Validation loss: 2.0329500834147134

Epoch: 6| Step: 6
Training loss: 2.134840965270996
Validation loss: 2.037634630997976

Epoch: 6| Step: 7
Training loss: 2.323831558227539
Validation loss: 2.0325511693954468

Epoch: 6| Step: 8
Training loss: 2.2739644050598145
Validation loss: 2.0364855329195657

Epoch: 6| Step: 9
Training loss: 1.4125033617019653
Validation loss: 2.0338221391042075

Epoch: 6| Step: 10
Training loss: 2.175999402999878
Validation loss: 2.0318395694096885

Epoch: 6| Step: 11
Training loss: 1.9516584873199463
Validation loss: 2.0460329254468284

Epoch: 6| Step: 12
Training loss: 1.4251527786254883
Validation loss: 2.043881416320801

Epoch: 6| Step: 13
Training loss: 2.3737869262695312
Validation loss: 2.039726754029592

Epoch: 112| Step: 0
Training loss: 2.0132455825805664
Validation loss: 2.039577523867289

Epoch: 6| Step: 1
Training loss: 2.1046714782714844
Validation loss: 2.041346271832784

Epoch: 6| Step: 2
Training loss: 1.5084259510040283
Validation loss: 2.05100017786026

Epoch: 6| Step: 3
Training loss: 2.84791898727417
Validation loss: 2.0398929715156555

Epoch: 6| Step: 4
Training loss: 1.7255274057388306
Validation loss: 2.0342074433962503

Epoch: 6| Step: 5
Training loss: 1.9532803297042847
Validation loss: 2.0396438439687095

Epoch: 6| Step: 6
Training loss: 2.0641722679138184
Validation loss: 2.0431508223215737

Epoch: 6| Step: 7
Training loss: 3.1898772716522217
Validation loss: 2.035702109336853

Epoch: 6| Step: 8
Training loss: 1.7469099760055542
Validation loss: 2.043776253859202

Epoch: 6| Step: 9
Training loss: 2.2495508193969727
Validation loss: 2.0244667132695517

Epoch: 6| Step: 10
Training loss: 1.8771859407424927
Validation loss: 2.027276317278544

Epoch: 6| Step: 11
Training loss: 1.8696173429489136
Validation loss: 2.026514013608297

Epoch: 6| Step: 12
Training loss: 1.4935340881347656
Validation loss: 2.0213093558947244

Epoch: 6| Step: 13
Training loss: 2.15384578704834
Validation loss: 2.039851168791453

Epoch: 113| Step: 0
Training loss: 1.8801727294921875
Validation loss: 2.0421494245529175

Epoch: 6| Step: 1
Training loss: 1.8351174592971802
Validation loss: 2.050555109977722

Epoch: 6| Step: 2
Training loss: 1.74186110496521
Validation loss: 2.0485240618387857

Epoch: 6| Step: 3
Training loss: 2.55940318107605
Validation loss: 2.047692815462748

Epoch: 6| Step: 4
Training loss: 1.5221474170684814
Validation loss: 2.057173411051432

Epoch: 6| Step: 5
Training loss: 2.152128219604492
Validation loss: 2.052600085735321

Epoch: 6| Step: 6
Training loss: 1.8660178184509277
Validation loss: 2.050848642985026

Epoch: 6| Step: 7
Training loss: 2.034074306488037
Validation loss: 2.053717533747355

Epoch: 6| Step: 8
Training loss: 2.382467031478882
Validation loss: 2.0432536005973816

Epoch: 6| Step: 9
Training loss: 1.626373529434204
Validation loss: 2.0314645369847617

Epoch: 6| Step: 10
Training loss: 2.381293773651123
Validation loss: 2.0277353127797446

Epoch: 6| Step: 11
Training loss: 2.5985069274902344
Validation loss: 2.026086767514547

Epoch: 6| Step: 12
Training loss: 1.855108618736267
Validation loss: 2.0223458210627236

Epoch: 6| Step: 13
Training loss: 2.311326265335083
Validation loss: 2.027061720689138

Epoch: 114| Step: 0
Training loss: 1.8549532890319824
Validation loss: 2.019911011060079

Epoch: 6| Step: 1
Training loss: 2.4313533306121826
Validation loss: 2.0192717909812927

Epoch: 6| Step: 2
Training loss: 2.4233338832855225
Validation loss: 2.020746171474457

Epoch: 6| Step: 3
Training loss: 1.8074562549591064
Validation loss: 2.0235154032707214

Epoch: 6| Step: 4
Training loss: 1.7291827201843262
Validation loss: 2.027698357899984

Epoch: 6| Step: 5
Training loss: 2.147477626800537
Validation loss: 2.021177808443705

Epoch: 6| Step: 6
Training loss: 2.3932456970214844
Validation loss: 2.0228488047917685

Epoch: 6| Step: 7
Training loss: 1.9756157398223877
Validation loss: 2.0281573136647544

Epoch: 6| Step: 8
Training loss: 1.3695085048675537
Validation loss: 2.027803659439087

Epoch: 6| Step: 9
Training loss: 2.049105167388916
Validation loss: 2.032990515232086

Epoch: 6| Step: 10
Training loss: 2.421539783477783
Validation loss: 2.0329016844431558

Epoch: 6| Step: 11
Training loss: 2.102968215942383
Validation loss: 2.0280158519744873

Epoch: 6| Step: 12
Training loss: 2.3109755516052246
Validation loss: 2.038433770338694

Epoch: 6| Step: 13
Training loss: 1.7053450345993042
Validation loss: 2.0487559040387473

Epoch: 115| Step: 0
Training loss: 1.4439256191253662
Validation loss: 2.0350298484166465

Epoch: 6| Step: 1
Training loss: 1.8942694664001465
Validation loss: 2.0348812540372214

Epoch: 6| Step: 2
Training loss: 1.88672935962677
Validation loss: 2.0456826090812683

Epoch: 6| Step: 3
Training loss: 2.4718027114868164
Validation loss: 2.0436259309450784

Epoch: 6| Step: 4
Training loss: 2.534715414047241
Validation loss: 2.043159286181132

Epoch: 6| Step: 5
Training loss: 1.9654380083084106
Validation loss: 2.0475454926490784

Epoch: 6| Step: 6
Training loss: 1.816894769668579
Validation loss: 2.052385071913401

Epoch: 6| Step: 7
Training loss: 1.7855509519577026
Validation loss: 2.0283084909121194

Epoch: 6| Step: 8
Training loss: 1.69227135181427
Validation loss: 2.022226413091024

Epoch: 6| Step: 9
Training loss: 2.428544759750366
Validation loss: 2.019369979699453

Epoch: 6| Step: 10
Training loss: 2.621260166168213
Validation loss: 2.0273218750953674

Epoch: 6| Step: 11
Training loss: 1.949174165725708
Validation loss: 2.0335437059402466

Epoch: 6| Step: 12
Training loss: 1.8120746612548828
Validation loss: 2.017251213391622

Epoch: 6| Step: 13
Training loss: 2.426485776901245
Validation loss: 2.0187442302703857

Epoch: 116| Step: 0
Training loss: 1.7915786504745483
Validation loss: 2.0269185503323874

Epoch: 6| Step: 1
Training loss: 2.489072799682617
Validation loss: 2.02459724744161

Epoch: 6| Step: 2
Training loss: 1.7297029495239258
Validation loss: 2.0254369974136353

Epoch: 6| Step: 3
Training loss: 2.4030075073242188
Validation loss: 2.0321932435035706

Epoch: 6| Step: 4
Training loss: 2.030158281326294
Validation loss: 2.021873354911804

Epoch: 6| Step: 5
Training loss: 2.4188833236694336
Validation loss: 2.0367582043011985

Epoch: 6| Step: 6
Training loss: 2.1883745193481445
Validation loss: 2.019407788912455

Epoch: 6| Step: 7
Training loss: 1.3513144254684448
Validation loss: 2.019742508729299

Epoch: 6| Step: 8
Training loss: 3.0666046142578125
Validation loss: 2.022505442301432

Epoch: 6| Step: 9
Training loss: 1.603895902633667
Validation loss: 2.0135390559832254

Epoch: 6| Step: 10
Training loss: 2.0685904026031494
Validation loss: 2.023811678091685

Epoch: 6| Step: 11
Training loss: 1.7459197044372559
Validation loss: 2.022419889767965

Epoch: 6| Step: 12
Training loss: 2.381603717803955
Validation loss: 2.0171574552853904

Epoch: 6| Step: 13
Training loss: 1.951951265335083
Validation loss: 2.0374770561854043

Epoch: 117| Step: 0
Training loss: 1.9002397060394287
Validation loss: 2.0353357990582785

Epoch: 6| Step: 1
Training loss: 2.5279948711395264
Validation loss: 2.04170689980189

Epoch: 6| Step: 2
Training loss: 2.2036213874816895
Validation loss: 2.0469170411427817

Epoch: 6| Step: 3
Training loss: 2.1670584678649902
Validation loss: 2.039182881514231

Epoch: 6| Step: 4
Training loss: 1.8704018592834473
Validation loss: 2.033189574877421

Epoch: 6| Step: 5
Training loss: 1.7239567041397095
Validation loss: 2.0341559449831643

Epoch: 6| Step: 6
Training loss: 2.486464738845825
Validation loss: 2.027908444404602

Epoch: 6| Step: 7
Training loss: 2.2076306343078613
Validation loss: 2.0217673977216086

Epoch: 6| Step: 8
Training loss: 1.9319807291030884
Validation loss: 2.0351749857266745

Epoch: 6| Step: 9
Training loss: 2.0966551303863525
Validation loss: 2.0260426799456277

Epoch: 6| Step: 10
Training loss: 1.938428282737732
Validation loss: 2.0239989956219993

Epoch: 6| Step: 11
Training loss: 1.9398456811904907
Validation loss: 2.0234196384747825

Epoch: 6| Step: 12
Training loss: 1.5194220542907715
Validation loss: 2.004822572072347

Epoch: 6| Step: 13
Training loss: 2.529926061630249
Validation loss: 2.016998032728831

Epoch: 118| Step: 0
Training loss: 1.436948299407959
Validation loss: 2.0069953997929892

Epoch: 6| Step: 1
Training loss: 2.6132593154907227
Validation loss: 2.0174997448921204

Epoch: 6| Step: 2
Training loss: 2.031012773513794
Validation loss: 2.0113874872525535

Epoch: 6| Step: 3
Training loss: 1.9320597648620605
Validation loss: 2.0098270972569785

Epoch: 6| Step: 4
Training loss: 2.4132978916168213
Validation loss: 2.0121699968973794

Epoch: 6| Step: 5
Training loss: 2.053772449493408
Validation loss: 2.019163986047109

Epoch: 6| Step: 6
Training loss: 2.310513973236084
Validation loss: 2.0247536301612854

Epoch: 6| Step: 7
Training loss: 1.6242375373840332
Validation loss: 2.0215757290522256

Epoch: 6| Step: 8
Training loss: 2.199343681335449
Validation loss: 2.0223911801973977

Epoch: 6| Step: 9
Training loss: 1.877489447593689
Validation loss: 2.0213305155436196

Epoch: 6| Step: 10
Training loss: 1.9526625871658325
Validation loss: 2.0365459521611533

Epoch: 6| Step: 11
Training loss: 2.2902958393096924
Validation loss: 2.0418065786361694

Epoch: 6| Step: 12
Training loss: 2.169509172439575
Validation loss: 2.0383341908454895

Epoch: 6| Step: 13
Training loss: 2.488326072692871
Validation loss: 2.0537412563959756

Epoch: 119| Step: 0
Training loss: 1.9790139198303223
Validation loss: 2.0459348360697427

Epoch: 6| Step: 1
Training loss: 1.9306846857070923
Validation loss: 2.0469086170196533

Epoch: 6| Step: 2
Training loss: 1.961580514907837
Validation loss: 2.0634923775990806

Epoch: 6| Step: 3
Training loss: 1.864572525024414
Validation loss: 2.073299288749695

Epoch: 6| Step: 4
Training loss: 1.424949288368225
Validation loss: 2.055824061234792

Epoch: 6| Step: 5
Training loss: 1.948265552520752
Validation loss: 2.0576720039049783

Epoch: 6| Step: 6
Training loss: 2.419177770614624
Validation loss: 2.035738984743754

Epoch: 6| Step: 7
Training loss: 2.2398526668548584
Validation loss: 2.0356573263804116

Epoch: 6| Step: 8
Training loss: 1.7708852291107178
Validation loss: 2.046865224838257

Epoch: 6| Step: 9
Training loss: 2.1805202960968018
Validation loss: 2.0479031801223755

Epoch: 6| Step: 10
Training loss: 2.3243980407714844
Validation loss: 2.057796816031138

Epoch: 6| Step: 11
Training loss: 1.7806122303009033
Validation loss: 2.0560704469680786

Epoch: 6| Step: 12
Training loss: 2.4793665409088135
Validation loss: 2.042531112829844

Epoch: 6| Step: 13
Training loss: 2.6234054565429688
Validation loss: 2.0458001693089805

Epoch: 120| Step: 0
Training loss: 1.6679915189743042
Validation loss: 2.0563311179478965

Epoch: 6| Step: 1
Training loss: 1.9111113548278809
Validation loss: 2.055387536684672

Epoch: 6| Step: 2
Training loss: 2.218104839324951
Validation loss: 2.060158292452494

Epoch: 6| Step: 3
Training loss: 2.230623245239258
Validation loss: 2.0451974868774414

Epoch: 6| Step: 4
Training loss: 2.0828847885131836
Validation loss: 2.048190097014109

Epoch: 6| Step: 5
Training loss: 1.2791759967803955
Validation loss: 2.0534233848253884

Epoch: 6| Step: 6
Training loss: 2.1775221824645996
Validation loss: 2.0640883843104043

Epoch: 6| Step: 7
Training loss: 1.7634190320968628
Validation loss: 2.0603178342183432

Epoch: 6| Step: 8
Training loss: 1.6722965240478516
Validation loss: 2.0631441871325173

Epoch: 6| Step: 9
Training loss: 2.705273151397705
Validation loss: 2.0678963462511697

Epoch: 6| Step: 10
Training loss: 2.400327682495117
Validation loss: 2.065712114175161

Epoch: 6| Step: 11
Training loss: 2.5069446563720703
Validation loss: 2.0608772238095603

Epoch: 6| Step: 12
Training loss: 1.7998298406600952
Validation loss: 2.0506822069485984

Epoch: 6| Step: 13
Training loss: 2.2634904384613037
Validation loss: 2.0462815165519714

Epoch: 121| Step: 0
Training loss: 1.4905563592910767
Validation loss: 2.038646399974823

Epoch: 6| Step: 1
Training loss: 2.3057665824890137
Validation loss: 2.0345175862312317

Epoch: 6| Step: 2
Training loss: 2.44502592086792
Validation loss: 2.040434797604879

Epoch: 6| Step: 3
Training loss: 1.450024127960205
Validation loss: 2.0354537963867188

Epoch: 6| Step: 4
Training loss: 2.770933151245117
Validation loss: 2.0354853669802346

Epoch: 6| Step: 5
Training loss: 2.0369396209716797
Validation loss: 2.0432563622792563

Epoch: 6| Step: 6
Training loss: 1.7071919441223145
Validation loss: 2.036803881327311

Epoch: 6| Step: 7
Training loss: 1.8537260293960571
Validation loss: 2.034692128499349

Epoch: 6| Step: 8
Training loss: 1.8519368171691895
Validation loss: 2.0465465982755027

Epoch: 6| Step: 9
Training loss: 2.148800849914551
Validation loss: 2.0457992951075235

Epoch: 6| Step: 10
Training loss: 2.6524767875671387
Validation loss: 2.053591708342234

Epoch: 6| Step: 11
Training loss: 1.667759656906128
Validation loss: 2.035123328367869

Epoch: 6| Step: 12
Training loss: 1.969780683517456
Validation loss: 2.0410712361335754

Epoch: 6| Step: 13
Training loss: 2.238990306854248
Validation loss: 2.0536916057268777

Epoch: 122| Step: 0
Training loss: 1.890712022781372
Validation loss: 2.071014861265818

Epoch: 6| Step: 1
Training loss: 2.1225733757019043
Validation loss: 2.069146911303202

Epoch: 6| Step: 2
Training loss: 1.8893022537231445
Validation loss: 2.075184245904287

Epoch: 6| Step: 3
Training loss: 2.8170719146728516
Validation loss: 2.083609322706858

Epoch: 6| Step: 4
Training loss: 1.4407553672790527
Validation loss: 2.0705271561940513

Epoch: 6| Step: 5
Training loss: 1.8643925189971924
Validation loss: 2.08746604124705

Epoch: 6| Step: 6
Training loss: 1.591861605644226
Validation loss: 2.0822261373202005

Epoch: 6| Step: 7
Training loss: 2.111084461212158
Validation loss: 2.057319760322571

Epoch: 6| Step: 8
Training loss: 2.0939083099365234
Validation loss: 2.0653688112894693

Epoch: 6| Step: 9
Training loss: 2.1206181049346924
Validation loss: 2.0615356961886087

Epoch: 6| Step: 10
Training loss: 2.514237403869629
Validation loss: 2.0414735674858093

Epoch: 6| Step: 11
Training loss: 2.1327567100524902
Validation loss: 2.0392814675966897

Epoch: 6| Step: 12
Training loss: 2.073362350463867
Validation loss: 2.037735025087992

Epoch: 6| Step: 13
Training loss: 2.0431935787200928
Validation loss: 2.0371182958285012

Epoch: 123| Step: 0
Training loss: 1.925107479095459
Validation loss: 2.030365824699402

Epoch: 6| Step: 1
Training loss: 1.909486174583435
Validation loss: 2.029720366001129

Epoch: 6| Step: 2
Training loss: 1.9681540727615356
Validation loss: 2.046241283416748

Epoch: 6| Step: 3
Training loss: 2.2873969078063965
Validation loss: 2.029583970705668

Epoch: 6| Step: 4
Training loss: 1.9015698432922363
Validation loss: 2.0405261317888894

Epoch: 6| Step: 5
Training loss: 1.8451080322265625
Validation loss: 2.063660442829132

Epoch: 6| Step: 6
Training loss: 2.221299171447754
Validation loss: 2.0670538544654846

Epoch: 6| Step: 7
Training loss: 1.4656319618225098
Validation loss: 2.0870662331581116

Epoch: 6| Step: 8
Training loss: 2.4970381259918213
Validation loss: 2.099529425303141

Epoch: 6| Step: 9
Training loss: 1.928088665008545
Validation loss: 2.1183552145957947

Epoch: 6| Step: 10
Training loss: 1.7065341472625732
Validation loss: 2.122972846031189

Epoch: 6| Step: 11
Training loss: 2.3221354484558105
Validation loss: 2.1228297551472983

Epoch: 6| Step: 12
Training loss: 2.892627239227295
Validation loss: 2.1049795150756836

Epoch: 6| Step: 13
Training loss: 2.2903780937194824
Validation loss: 2.085995078086853

Epoch: 124| Step: 0
Training loss: 2.1338653564453125
Validation loss: 2.0563517808914185

Epoch: 6| Step: 1
Training loss: 2.1299312114715576
Validation loss: 2.0309457182884216

Epoch: 6| Step: 2
Training loss: 1.743958592414856
Validation loss: 2.034283618132273

Epoch: 6| Step: 3
Training loss: 1.8916020393371582
Validation loss: 2.0279155572255454

Epoch: 6| Step: 4
Training loss: 1.941696286201477
Validation loss: 2.033543109893799

Epoch: 6| Step: 5
Training loss: 1.8130667209625244
Validation loss: 2.0371546745300293

Epoch: 6| Step: 6
Training loss: 1.4775872230529785
Validation loss: 2.0374539693196616

Epoch: 6| Step: 7
Training loss: 1.891419529914856
Validation loss: 2.0292749404907227

Epoch: 6| Step: 8
Training loss: 2.405123472213745
Validation loss: 2.0346240599950156

Epoch: 6| Step: 9
Training loss: 2.8361430168151855
Validation loss: 2.0417081713676453

Epoch: 6| Step: 10
Training loss: 2.6112895011901855
Validation loss: 2.0388267238934836

Epoch: 6| Step: 11
Training loss: 2.3575501441955566
Validation loss: 2.0296928683916726

Epoch: 6| Step: 12
Training loss: 2.054265022277832
Validation loss: 2.022220234076182

Epoch: 6| Step: 13
Training loss: 2.2602710723876953
Validation loss: 2.0306700468063354

Epoch: 125| Step: 0
Training loss: 2.2597270011901855
Validation loss: 2.0252771774927774

Epoch: 6| Step: 1
Training loss: 2.1600728034973145
Validation loss: 2.0363771518071494

Epoch: 6| Step: 2
Training loss: 1.9466183185577393
Validation loss: 2.0299657583236694

Epoch: 6| Step: 3
Training loss: 2.146333932876587
Validation loss: 2.03080544869105

Epoch: 6| Step: 4
Training loss: 1.8090342283248901
Validation loss: 2.0331753889719644

Epoch: 6| Step: 5
Training loss: 2.2960755825042725
Validation loss: 2.0283138950665793

Epoch: 6| Step: 6
Training loss: 1.8831663131713867
Validation loss: 2.032575488090515

Epoch: 6| Step: 7
Training loss: 1.8320517539978027
Validation loss: 2.0268227060635886

Epoch: 6| Step: 8
Training loss: 2.315248727798462
Validation loss: 2.025685727596283

Epoch: 6| Step: 9
Training loss: 1.6186392307281494
Validation loss: 2.031619369983673

Epoch: 6| Step: 10
Training loss: 1.893924355506897
Validation loss: 2.03905055920283

Epoch: 6| Step: 11
Training loss: 2.1381688117980957
Validation loss: 2.045891602834066

Epoch: 6| Step: 12
Training loss: 2.546670436859131
Validation loss: 2.0456860661506653

Epoch: 6| Step: 13
Training loss: 2.5664572715759277
Validation loss: 2.069127639134725

Epoch: 126| Step: 0
Training loss: 1.7428550720214844
Validation loss: 2.065133512020111

Epoch: 6| Step: 1
Training loss: 2.124838352203369
Validation loss: 2.0645787914594016

Epoch: 6| Step: 2
Training loss: 1.8745828866958618
Validation loss: 2.069559097290039

Epoch: 6| Step: 3
Training loss: 2.0136685371398926
Validation loss: 2.071679671605428

Epoch: 6| Step: 4
Training loss: 2.383742094039917
Validation loss: 2.0500630537668862

Epoch: 6| Step: 5
Training loss: 2.039522171020508
Validation loss: 2.050853987534841

Epoch: 6| Step: 6
Training loss: 2.0712733268737793
Validation loss: 2.0529685815175376

Epoch: 6| Step: 7
Training loss: 1.6500037908554077
Validation loss: 2.059389849503835

Epoch: 6| Step: 8
Training loss: 2.2144031524658203
Validation loss: 2.064366360505422

Epoch: 6| Step: 9
Training loss: 2.013517379760742
Validation loss: 2.0699753363927207

Epoch: 6| Step: 10
Training loss: 2.716026782989502
Validation loss: 2.081374684969584

Epoch: 6| Step: 11
Training loss: 1.940175175666809
Validation loss: 2.0745333234469094

Epoch: 6| Step: 12
Training loss: 2.315109968185425
Validation loss: 2.0782777865727744

Epoch: 6| Step: 13
Training loss: 1.4962882995605469
Validation loss: 2.087965250015259

Epoch: 127| Step: 0
Training loss: 2.3973124027252197
Validation loss: 2.092040161291758

Epoch: 6| Step: 1
Training loss: 1.710263729095459
Validation loss: 2.08931565284729

Epoch: 6| Step: 2
Training loss: 1.8256534337997437
Validation loss: 2.111909548441569

Epoch: 6| Step: 3
Training loss: 1.8063666820526123
Validation loss: 2.1237685481707254

Epoch: 6| Step: 4
Training loss: 2.2827115058898926
Validation loss: 2.1062559684117637

Epoch: 6| Step: 5
Training loss: 1.8250021934509277
Validation loss: 2.1074623664220176

Epoch: 6| Step: 6
Training loss: 2.019322156906128
Validation loss: 2.0876951018969216

Epoch: 6| Step: 7
Training loss: 2.8618249893188477
Validation loss: 2.0873047908147178

Epoch: 6| Step: 8
Training loss: 2.0947818756103516
Validation loss: 2.086307426293691

Epoch: 6| Step: 9
Training loss: 1.7159901857376099
Validation loss: 2.0571706096331277

Epoch: 6| Step: 10
Training loss: 1.9493284225463867
Validation loss: 2.0666906436284385

Epoch: 6| Step: 11
Training loss: 1.8827369213104248
Validation loss: 2.068146526813507

Epoch: 6| Step: 12
Training loss: 1.9451425075531006
Validation loss: 2.0664952794710794

Epoch: 6| Step: 13
Training loss: 2.2316792011260986
Validation loss: 2.0695144732793174

Epoch: 128| Step: 0
Training loss: 2.5010972023010254
Validation loss: 2.064919730027517

Epoch: 6| Step: 1
Training loss: 1.8861805200576782
Validation loss: 2.0851513346036277

Epoch: 6| Step: 2
Training loss: 2.307619333267212
Validation loss: 2.0782554745674133

Epoch: 6| Step: 3
Training loss: 2.0019822120666504
Validation loss: 2.085252126057943

Epoch: 6| Step: 4
Training loss: 2.0008420944213867
Validation loss: 2.090701778729757

Epoch: 6| Step: 5
Training loss: 2.0166726112365723
Validation loss: 2.089372158050537

Epoch: 6| Step: 6
Training loss: 1.8178478479385376
Validation loss: 2.0772810777028403

Epoch: 6| Step: 7
Training loss: 1.9783875942230225
Validation loss: 2.0846298933029175

Epoch: 6| Step: 8
Training loss: 1.5004957914352417
Validation loss: 2.0750137170155845

Epoch: 6| Step: 9
Training loss: 2.0134992599487305
Validation loss: 2.077680071194967

Epoch: 6| Step: 10
Training loss: 1.9894541501998901
Validation loss: 2.0660590330759683

Epoch: 6| Step: 11
Training loss: 2.1719045639038086
Validation loss: 2.067659020423889

Epoch: 6| Step: 12
Training loss: 1.8230127096176147
Validation loss: 2.060006558895111

Epoch: 6| Step: 13
Training loss: 2.113112449645996
Validation loss: 2.057137906551361

Epoch: 129| Step: 0
Training loss: 2.175847053527832
Validation loss: 2.0389121770858765

Epoch: 6| Step: 1
Training loss: 1.9988212585449219
Validation loss: 2.043917179107666

Epoch: 6| Step: 2
Training loss: 2.169160842895508
Validation loss: 2.0410489638646445

Epoch: 6| Step: 3
Training loss: 1.4245367050170898
Validation loss: 2.042052408059438

Epoch: 6| Step: 4
Training loss: 2.4274444580078125
Validation loss: 2.0484508872032166

Epoch: 6| Step: 5
Training loss: 2.4451255798339844
Validation loss: 2.0642110308011374

Epoch: 6| Step: 6
Training loss: 2.077256202697754
Validation loss: 2.0624740521113076

Epoch: 6| Step: 7
Training loss: 1.8770161867141724
Validation loss: 2.073545217514038

Epoch: 6| Step: 8
Training loss: 2.163975715637207
Validation loss: 2.0767507354418435

Epoch: 6| Step: 9
Training loss: 2.222959041595459
Validation loss: 2.0916267037391663

Epoch: 6| Step: 10
Training loss: 2.2374958992004395
Validation loss: 2.0906399885813394

Epoch: 6| Step: 11
Training loss: 2.00643253326416
Validation loss: 2.0874298810958862

Epoch: 6| Step: 12
Training loss: 2.232670307159424
Validation loss: 2.089599132537842

Epoch: 6| Step: 13
Training loss: 1.5722776651382446
Validation loss: 2.0836839278539023

Epoch: 130| Step: 0
Training loss: 2.6985132694244385
Validation loss: 2.0603725910186768

Epoch: 6| Step: 1
Training loss: 2.288938045501709
Validation loss: 2.0677276055018106

Epoch: 6| Step: 2
Training loss: 2.4571030139923096
Validation loss: 2.0571067134539285

Epoch: 6| Step: 3
Training loss: 2.22713041305542
Validation loss: 2.058491071065267

Epoch: 6| Step: 4
Training loss: 1.8871852159500122
Validation loss: 2.0398492415746055

Epoch: 6| Step: 5
Training loss: 1.8580420017242432
Validation loss: 2.0477352937062583

Epoch: 6| Step: 6
Training loss: 1.573891520500183
Validation loss: 2.0382155577341714

Epoch: 6| Step: 7
Training loss: 1.7359771728515625
Validation loss: 2.044971307118734

Epoch: 6| Step: 8
Training loss: 2.2898356914520264
Validation loss: 2.052297671635946

Epoch: 6| Step: 9
Training loss: 2.1827430725097656
Validation loss: 2.040897846221924

Epoch: 6| Step: 10
Training loss: 1.401078701019287
Validation loss: 2.052178661028544

Epoch: 6| Step: 11
Training loss: 1.680708885192871
Validation loss: 2.0683967073758445

Epoch: 6| Step: 12
Training loss: 2.4686193466186523
Validation loss: 2.059942921002706

Epoch: 6| Step: 13
Training loss: 1.9062633514404297
Validation loss: 2.0671146710713706

Epoch: 131| Step: 0
Training loss: 2.084202527999878
Validation loss: 2.070430556933085

Epoch: 6| Step: 1
Training loss: 2.385028123855591
Validation loss: 2.0708949168523154

Epoch: 6| Step: 2
Training loss: 2.1490085124969482
Validation loss: 2.066848715146383

Epoch: 6| Step: 3
Training loss: 1.467982292175293
Validation loss: 2.078337013721466

Epoch: 6| Step: 4
Training loss: 1.6093082427978516
Validation loss: 2.0825281937917075

Epoch: 6| Step: 5
Training loss: 1.855910062789917
Validation loss: 2.066325604915619

Epoch: 6| Step: 6
Training loss: 1.6872305870056152
Validation loss: 2.0751363039016724

Epoch: 6| Step: 7
Training loss: 2.116710662841797
Validation loss: 2.0705313285191855

Epoch: 6| Step: 8
Training loss: 2.26021146774292
Validation loss: 2.0725305676460266

Epoch: 6| Step: 9
Training loss: 1.8733127117156982
Validation loss: 2.068636159102122

Epoch: 6| Step: 10
Training loss: 1.6101269721984863
Validation loss: 2.0647059281667075

Epoch: 6| Step: 11
Training loss: 2.700883388519287
Validation loss: 2.0577762524286904

Epoch: 6| Step: 12
Training loss: 2.2598142623901367
Validation loss: 2.0669689973195395

Epoch: 6| Step: 13
Training loss: 2.1213197708129883
Validation loss: 2.052268862724304

Epoch: 132| Step: 0
Training loss: 1.6014724969863892
Validation loss: 2.0472094217936196

Epoch: 6| Step: 1
Training loss: 1.5807654857635498
Validation loss: 2.041000723838806

Epoch: 6| Step: 2
Training loss: 2.393852472305298
Validation loss: 2.0563719868659973

Epoch: 6| Step: 3
Training loss: 2.3842902183532715
Validation loss: 2.0502384901046753

Epoch: 6| Step: 4
Training loss: 1.772808313369751
Validation loss: 2.0662179787953696

Epoch: 6| Step: 5
Training loss: 2.1314735412597656
Validation loss: 2.0664679805437722

Epoch: 6| Step: 6
Training loss: 1.702547550201416
Validation loss: 2.084473510583242

Epoch: 6| Step: 7
Training loss: 1.908234715461731
Validation loss: 2.0809393723805747

Epoch: 6| Step: 8
Training loss: 2.2416622638702393
Validation loss: 2.092443108558655

Epoch: 6| Step: 9
Training loss: 2.167513132095337
Validation loss: 2.1051119764645896

Epoch: 6| Step: 10
Training loss: 2.0291810035705566
Validation loss: 2.0983224908510842

Epoch: 6| Step: 11
Training loss: 1.8478658199310303
Validation loss: 2.075965921084086

Epoch: 6| Step: 12
Training loss: 2.521233081817627
Validation loss: 2.067450006802877

Epoch: 6| Step: 13
Training loss: 2.0207910537719727
Validation loss: 2.05709437529246

Epoch: 133| Step: 0
Training loss: 1.5750244855880737
Validation loss: 2.0474931597709656

Epoch: 6| Step: 1
Training loss: 2.290383815765381
Validation loss: 2.0302852392196655

Epoch: 6| Step: 2
Training loss: 1.561669945716858
Validation loss: 2.0312732060750327

Epoch: 6| Step: 3
Training loss: 2.09482479095459
Validation loss: 2.022567709287008

Epoch: 6| Step: 4
Training loss: 2.2911734580993652
Validation loss: 2.0297169287999473

Epoch: 6| Step: 5
Training loss: 2.098506450653076
Validation loss: 2.0310402115186057

Epoch: 6| Step: 6
Training loss: 1.9560086727142334
Validation loss: 2.0355936686197915

Epoch: 6| Step: 7
Training loss: 2.1850147247314453
Validation loss: 2.0318756302197776

Epoch: 6| Step: 8
Training loss: 2.300133228302002
Validation loss: 2.034126877784729

Epoch: 6| Step: 9
Training loss: 2.6257898807525635
Validation loss: 2.0310057004292807

Epoch: 6| Step: 10
Training loss: 1.9528850317001343
Validation loss: 2.0360034306844077

Epoch: 6| Step: 11
Training loss: 2.1383190155029297
Validation loss: 2.0463371674219766

Epoch: 6| Step: 12
Training loss: 2.292884349822998
Validation loss: 2.0359625816345215

Epoch: 6| Step: 13
Training loss: 1.394491195678711
Validation loss: 2.025766452153524

Epoch: 134| Step: 0
Training loss: 2.5597667694091797
Validation loss: 2.0339411894480386

Epoch: 6| Step: 1
Training loss: 1.5706725120544434
Validation loss: 2.0369943181673684

Epoch: 6| Step: 2
Training loss: 1.7183271646499634
Validation loss: 2.0287036299705505

Epoch: 6| Step: 3
Training loss: 2.3170111179351807
Validation loss: 2.0346970160802207

Epoch: 6| Step: 4
Training loss: 2.328585624694824
Validation loss: 2.0505622227986655

Epoch: 6| Step: 5
Training loss: 1.95603609085083
Validation loss: 2.0466211239496865

Epoch: 6| Step: 6
Training loss: 2.3416683673858643
Validation loss: 2.057765086491903

Epoch: 6| Step: 7
Training loss: 2.052485704421997
Validation loss: 2.0570435921351113

Epoch: 6| Step: 8
Training loss: 2.376168727874756
Validation loss: 2.0755781730016074

Epoch: 6| Step: 9
Training loss: 1.7320516109466553
Validation loss: 2.0735652446746826

Epoch: 6| Step: 10
Training loss: 1.8784325122833252
Validation loss: 2.0695079962412515

Epoch: 6| Step: 11
Training loss: 1.5971527099609375
Validation loss: 2.0737465222676597

Epoch: 6| Step: 12
Training loss: 1.7801289558410645
Validation loss: 2.089422821998596

Epoch: 6| Step: 13
Training loss: 1.822378158569336
Validation loss: 2.0764430363972983

Epoch: 135| Step: 0
Training loss: 1.7588093280792236
Validation loss: 2.082969923814138

Epoch: 6| Step: 1
Training loss: 1.9486007690429688
Validation loss: 2.06171323855718

Epoch: 6| Step: 2
Training loss: 1.9687364101409912
Validation loss: 2.0570462942123413

Epoch: 6| Step: 3
Training loss: 2.313082218170166
Validation loss: 2.0648260911305747

Epoch: 6| Step: 4
Training loss: 1.7852530479431152
Validation loss: 2.054698566595713

Epoch: 6| Step: 5
Training loss: 1.898127555847168
Validation loss: 2.0600022872289023

Epoch: 6| Step: 6
Training loss: 1.9265871047973633
Validation loss: 2.066396713256836

Epoch: 6| Step: 7
Training loss: 2.015848159790039
Validation loss: 2.0782699584960938

Epoch: 6| Step: 8
Training loss: 2.776315927505493
Validation loss: 2.0690574049949646

Epoch: 6| Step: 9
Training loss: 2.0958099365234375
Validation loss: 2.078113834063212

Epoch: 6| Step: 10
Training loss: 2.6578712463378906
Validation loss: 2.079579492410024

Epoch: 6| Step: 11
Training loss: 1.8108140230178833
Validation loss: 2.0744407375653586

Epoch: 6| Step: 12
Training loss: 1.8698616027832031
Validation loss: 2.0614445209503174

Epoch: 6| Step: 13
Training loss: 1.7229368686676025
Validation loss: 2.0594139893849692

Epoch: 136| Step: 0
Training loss: 1.7459673881530762
Validation loss: 2.057430624961853

Epoch: 6| Step: 1
Training loss: 2.2533469200134277
Validation loss: 2.074599266052246

Epoch: 6| Step: 2
Training loss: 2.0287957191467285
Validation loss: 2.0733019709587097

Epoch: 6| Step: 3
Training loss: 2.1472206115722656
Validation loss: 2.07061630487442

Epoch: 6| Step: 4
Training loss: 1.4597383737564087
Validation loss: 2.0697981317838035

Epoch: 6| Step: 5
Training loss: 1.4985523223876953
Validation loss: 2.0781805515289307

Epoch: 6| Step: 6
Training loss: 1.9128284454345703
Validation loss: 2.0705644289652505

Epoch: 6| Step: 7
Training loss: 2.575413227081299
Validation loss: 2.059136430422465

Epoch: 6| Step: 8
Training loss: 2.891928195953369
Validation loss: 2.0637449820836387

Epoch: 6| Step: 9
Training loss: 1.9650832414627075
Validation loss: 2.0619776447614035

Epoch: 6| Step: 10
Training loss: 2.037938117980957
Validation loss: 2.0654165347417197

Epoch: 6| Step: 11
Training loss: 1.6366970539093018
Validation loss: 2.054130574067434

Epoch: 6| Step: 12
Training loss: 2.0274596214294434
Validation loss: 2.065527598063151

Epoch: 6| Step: 13
Training loss: 1.9547159671783447
Validation loss: 2.0622752706209817

Epoch: 137| Step: 0
Training loss: 1.9488954544067383
Validation loss: 2.0663992365201316

Epoch: 6| Step: 1
Training loss: 1.7545561790466309
Validation loss: 2.064488112926483

Epoch: 6| Step: 2
Training loss: 1.7254247665405273
Validation loss: 2.0433619022369385

Epoch: 6| Step: 3
Training loss: 1.5143754482269287
Validation loss: 2.056522250175476

Epoch: 6| Step: 4
Training loss: 2.10927152633667
Validation loss: 2.05993115901947

Epoch: 6| Step: 5
Training loss: 2.32725191116333
Validation loss: 2.044323225816091

Epoch: 6| Step: 6
Training loss: 2.2450051307678223
Validation loss: 2.0430365403493247

Epoch: 6| Step: 7
Training loss: 2.1269140243530273
Validation loss: 2.034196138381958

Epoch: 6| Step: 8
Training loss: 2.492002487182617
Validation loss: 2.0443556904792786

Epoch: 6| Step: 9
Training loss: 1.8634456396102905
Validation loss: 2.0418028434117637

Epoch: 6| Step: 10
Training loss: 1.9710969924926758
Validation loss: 2.0447200735410056

Epoch: 6| Step: 11
Training loss: 2.2769901752471924
Validation loss: 2.049971958001455

Epoch: 6| Step: 12
Training loss: 2.1814327239990234
Validation loss: 2.0442616939544678

Epoch: 6| Step: 13
Training loss: 1.935854196548462
Validation loss: 2.0373350381851196

Epoch: 138| Step: 0
Training loss: 2.220135450363159
Validation loss: 2.047678450743357

Epoch: 6| Step: 1
Training loss: 1.865544080734253
Validation loss: 2.0591208736101785

Epoch: 6| Step: 2
Training loss: 2.464268684387207
Validation loss: 2.073985755443573

Epoch: 6| Step: 3
Training loss: 1.9965707063674927
Validation loss: 2.0645424127578735

Epoch: 6| Step: 4
Training loss: 1.9118151664733887
Validation loss: 2.0873024066289267

Epoch: 6| Step: 5
Training loss: 2.2958152294158936
Validation loss: 2.0849748452504477

Epoch: 6| Step: 6
Training loss: 2.5793049335479736
Validation loss: 2.0907559593518577

Epoch: 6| Step: 7
Training loss: 2.298142910003662
Validation loss: 2.0806951920191445

Epoch: 6| Step: 8
Training loss: 1.8453974723815918
Validation loss: 2.0679271817207336

Epoch: 6| Step: 9
Training loss: 1.6330227851867676
Validation loss: 2.067712664604187

Epoch: 6| Step: 10
Training loss: 1.6334857940673828
Validation loss: 2.0509692231814065

Epoch: 6| Step: 11
Training loss: 1.7346022129058838
Validation loss: 2.04768576224645

Epoch: 6| Step: 12
Training loss: 1.849930763244629
Validation loss: 2.051475942134857

Epoch: 6| Step: 13
Training loss: 1.9020752906799316
Validation loss: 2.0455482602119446

Epoch: 139| Step: 0
Training loss: 2.282243490219116
Validation loss: 2.0365100304285684

Epoch: 6| Step: 1
Training loss: 2.6028003692626953
Validation loss: 2.0275554259618125

Epoch: 6| Step: 2
Training loss: 1.8625073432922363
Validation loss: 2.0396533211072287

Epoch: 6| Step: 3
Training loss: 2.2972986698150635
Validation loss: 2.043280839920044

Epoch: 6| Step: 4
Training loss: 2.007779359817505
Validation loss: 2.035775065422058

Epoch: 6| Step: 5
Training loss: 2.136843681335449
Validation loss: 2.039680282274882

Epoch: 6| Step: 6
Training loss: 1.5829277038574219
Validation loss: 2.042068302631378

Epoch: 6| Step: 7
Training loss: 2.2620768547058105
Validation loss: 2.0430704156557717

Epoch: 6| Step: 8
Training loss: 1.9686620235443115
Validation loss: 2.0482680996259055

Epoch: 6| Step: 9
Training loss: 1.2333693504333496
Validation loss: 2.045152962207794

Epoch: 6| Step: 10
Training loss: 1.8219265937805176
Validation loss: 2.0605685313542685

Epoch: 6| Step: 11
Training loss: 1.8232347965240479
Validation loss: 2.070911685625712

Epoch: 6| Step: 12
Training loss: 2.721832752227783
Validation loss: 2.064380645751953

Epoch: 6| Step: 13
Training loss: 1.6927824020385742
Validation loss: 2.0706041057904563

Epoch: 140| Step: 0
Training loss: 1.909909725189209
Validation loss: 2.080169439315796

Epoch: 6| Step: 1
Training loss: 1.7376444339752197
Validation loss: 2.077343225479126

Epoch: 6| Step: 2
Training loss: 1.1183489561080933
Validation loss: 2.086519698301951

Epoch: 6| Step: 3
Training loss: 1.8255938291549683
Validation loss: 2.103831688563029

Epoch: 6| Step: 4
Training loss: 1.6831653118133545
Validation loss: 2.0966214338938394

Epoch: 6| Step: 5
Training loss: 1.9679601192474365
Validation loss: 2.093012591203054

Epoch: 6| Step: 6
Training loss: 2.6290225982666016
Validation loss: 2.108159144719442

Epoch: 6| Step: 7
Training loss: 2.350605010986328
Validation loss: 2.124960720539093

Epoch: 6| Step: 8
Training loss: 2.232572078704834
Validation loss: 2.118259926637014

Epoch: 6| Step: 9
Training loss: 2.205134868621826
Validation loss: 2.107874115308126

Epoch: 6| Step: 10
Training loss: 2.1654324531555176
Validation loss: 2.0916154781977334

Epoch: 6| Step: 11
Training loss: 2.3676257133483887
Validation loss: 2.0599178075790405

Epoch: 6| Step: 12
Training loss: 2.5035758018493652
Validation loss: 2.057597557703654

Epoch: 6| Step: 13
Training loss: 1.6017100811004639
Validation loss: 2.05854594707489

Epoch: 141| Step: 0
Training loss: 2.4668710231781006
Validation loss: 2.063433070977529

Epoch: 6| Step: 1
Training loss: 1.9327507019042969
Validation loss: 2.0629279414812722

Epoch: 6| Step: 2
Training loss: 1.8211262226104736
Validation loss: 2.0432585875193277

Epoch: 6| Step: 3
Training loss: 2.376776933670044
Validation loss: 2.054854174455007

Epoch: 6| Step: 4
Training loss: 2.6423375606536865
Validation loss: 2.0555293560028076

Epoch: 6| Step: 5
Training loss: 1.7526097297668457
Validation loss: 2.061903496583303

Epoch: 6| Step: 6
Training loss: 2.0863149166107178
Validation loss: 2.061438798904419

Epoch: 6| Step: 7
Training loss: 1.521377682685852
Validation loss: 2.0574735005696616

Epoch: 6| Step: 8
Training loss: 1.7660664319992065
Validation loss: 2.051104029019674

Epoch: 6| Step: 9
Training loss: 2.0875611305236816
Validation loss: 2.065579434235891

Epoch: 6| Step: 10
Training loss: 1.7626793384552002
Validation loss: 2.074670592943827

Epoch: 6| Step: 11
Training loss: 2.429414749145508
Validation loss: 2.0961413979530334

Epoch: 6| Step: 12
Training loss: 1.8018938302993774
Validation loss: 2.081842223803202

Epoch: 6| Step: 13
Training loss: 1.6693024635314941
Validation loss: 2.1027681827545166

Epoch: 142| Step: 0
Training loss: 2.009183406829834
Validation loss: 2.126058260599772

Epoch: 6| Step: 1
Training loss: 1.7328922748565674
Validation loss: 2.1492934425671897

Epoch: 6| Step: 2
Training loss: 2.204883575439453
Validation loss: 2.14895232518514

Epoch: 6| Step: 3
Training loss: 2.4566354751586914
Validation loss: 2.116204023361206

Epoch: 6| Step: 4
Training loss: 2.595792293548584
Validation loss: 2.106087028980255

Epoch: 6| Step: 5
Training loss: 1.929936170578003
Validation loss: 2.0894832213719687

Epoch: 6| Step: 6
Training loss: 2.244738817214966
Validation loss: 2.0790454347928367

Epoch: 6| Step: 7
Training loss: 2.1858601570129395
Validation loss: 2.0686020255088806

Epoch: 6| Step: 8
Training loss: 1.527024745941162
Validation loss: 2.0551443894704184

Epoch: 6| Step: 9
Training loss: 2.263256072998047
Validation loss: 2.04765385389328

Epoch: 6| Step: 10
Training loss: 1.7024197578430176
Validation loss: 2.0394349694252014

Epoch: 6| Step: 11
Training loss: 2.270443916320801
Validation loss: 2.0429309805234275

Epoch: 6| Step: 12
Training loss: 1.617372751235962
Validation loss: 2.042633573214213

Epoch: 6| Step: 13
Training loss: 1.9472570419311523
Validation loss: 2.048075000445048

Epoch: 143| Step: 0
Training loss: 2.5740585327148438
Validation loss: 2.0438324411710105

Epoch: 6| Step: 1
Training loss: 1.9726507663726807
Validation loss: 2.0513389110565186

Epoch: 6| Step: 2
Training loss: 1.4715070724487305
Validation loss: 2.0402880708376565

Epoch: 6| Step: 3
Training loss: 2.189070701599121
Validation loss: 2.0534681280454

Epoch: 6| Step: 4
Training loss: 1.893385410308838
Validation loss: 2.0630411307017007

Epoch: 6| Step: 5
Training loss: 2.1193833351135254
Validation loss: 2.0630717277526855

Epoch: 6| Step: 6
Training loss: 1.9278035163879395
Validation loss: 2.0569879611333213

Epoch: 6| Step: 7
Training loss: 2.010265827178955
Validation loss: 2.0745245615641275

Epoch: 6| Step: 8
Training loss: 2.196877956390381
Validation loss: 2.07287206252416

Epoch: 6| Step: 9
Training loss: 1.8478965759277344
Validation loss: 2.0803147355715432

Epoch: 6| Step: 10
Training loss: 1.911472201347351
Validation loss: 2.0759290059407554

Epoch: 6| Step: 11
Training loss: 2.2320396900177
Validation loss: 2.097809453805288

Epoch: 6| Step: 12
Training loss: 2.024259090423584
Validation loss: 2.0945043563842773

Epoch: 6| Step: 13
Training loss: 1.8375482559204102
Validation loss: 2.094178477923075

Epoch: 144| Step: 0
Training loss: 1.9678289890289307
Validation loss: 2.0933751861254373

Epoch: 6| Step: 1
Training loss: 2.6582140922546387
Validation loss: 2.0946412086486816

Epoch: 6| Step: 2
Training loss: 1.4426484107971191
Validation loss: 2.0916640162467957

Epoch: 6| Step: 3
Training loss: 1.6220488548278809
Validation loss: 2.106936494509379

Epoch: 6| Step: 4
Training loss: 1.619030475616455
Validation loss: 2.098248541355133

Epoch: 6| Step: 5
Training loss: 2.2114808559417725
Validation loss: 2.09195613861084

Epoch: 6| Step: 6
Training loss: 1.6790850162506104
Validation loss: 2.072378138701121

Epoch: 6| Step: 7
Training loss: 2.443406581878662
Validation loss: 2.0708805521329245

Epoch: 6| Step: 8
Training loss: 1.7794901132583618
Validation loss: 2.064876119295756

Epoch: 6| Step: 9
Training loss: 2.4274981021881104
Validation loss: 2.0644428730010986

Epoch: 6| Step: 10
Training loss: 1.840195655822754
Validation loss: 2.063719550768534

Epoch: 6| Step: 11
Training loss: 2.038738489151001
Validation loss: 2.070196251074473

Epoch: 6| Step: 12
Training loss: 2.402003288269043
Validation loss: 2.070857048034668

Epoch: 6| Step: 13
Training loss: 2.1980037689208984
Validation loss: 2.0714314778645835

Epoch: 145| Step: 0
Training loss: 2.538271188735962
Validation loss: 2.0695352951685586

Epoch: 6| Step: 1
Training loss: 2.8316550254821777
Validation loss: 2.0951571265856423

Epoch: 6| Step: 2
Training loss: 1.9359955787658691
Validation loss: 2.0854135155677795

Epoch: 6| Step: 3
Training loss: 1.9388991594314575
Validation loss: 2.0893247723579407

Epoch: 6| Step: 4
Training loss: 2.037492275238037
Validation loss: 2.0938724279403687

Epoch: 6| Step: 5
Training loss: 2.3113889694213867
Validation loss: 2.0853917598724365

Epoch: 6| Step: 6
Training loss: 1.3274250030517578
Validation loss: 2.1065821250279746

Epoch: 6| Step: 7
Training loss: 2.0355381965637207
Validation loss: 2.0964927275975547

Epoch: 6| Step: 8
Training loss: 2.1798300743103027
Validation loss: 2.1094831426938376

Epoch: 6| Step: 9
Training loss: 1.4228546619415283
Validation loss: 2.0921806494394937

Epoch: 6| Step: 10
Training loss: 1.6777141094207764
Validation loss: 2.0983072320620217

Epoch: 6| Step: 11
Training loss: 2.112055540084839
Validation loss: 2.086786766846975

Epoch: 6| Step: 12
Training loss: 1.8643832206726074
Validation loss: 2.110431750615438

Epoch: 6| Step: 13
Training loss: 1.3972711563110352
Validation loss: 2.099245329697927

Epoch: 146| Step: 0
Training loss: 1.6889864206314087
Validation loss: 2.105729063351949

Epoch: 6| Step: 1
Training loss: 2.5640854835510254
Validation loss: 2.1172749400138855

Epoch: 6| Step: 2
Training loss: 1.7962616682052612
Validation loss: 2.0949899752934775

Epoch: 6| Step: 3
Training loss: 1.355698823928833
Validation loss: 2.1004651983579

Epoch: 6| Step: 4
Training loss: 1.817563772201538
Validation loss: 2.113070329030355

Epoch: 6| Step: 5
Training loss: 1.8711179494857788
Validation loss: 2.091513911883036

Epoch: 6| Step: 6
Training loss: 1.9132308959960938
Validation loss: 2.1038898825645447

Epoch: 6| Step: 7
Training loss: 2.5100932121276855
Validation loss: 2.1093101104100547

Epoch: 6| Step: 8
Training loss: 2.1262223720550537
Validation loss: 2.113965610663096

Epoch: 6| Step: 9
Training loss: 2.088604688644409
Validation loss: 2.107459386189779

Epoch: 6| Step: 10
Training loss: 2.276754379272461
Validation loss: 2.1196003357569375

Epoch: 6| Step: 11
Training loss: 2.0816309452056885
Validation loss: 2.110193391640981

Epoch: 6| Step: 12
Training loss: 2.4825804233551025
Validation loss: 2.1203031142552695

Epoch: 6| Step: 13
Training loss: 1.4615881443023682
Validation loss: 2.1048864126205444

Epoch: 147| Step: 0
Training loss: 1.946662187576294
Validation loss: 2.099168519179026

Epoch: 6| Step: 1
Training loss: 2.1956899166107178
Validation loss: 2.07696125904719

Epoch: 6| Step: 2
Training loss: 2.1119675636291504
Validation loss: 2.080342729886373

Epoch: 6| Step: 3
Training loss: 1.6554265022277832
Validation loss: 2.0870371659596763

Epoch: 6| Step: 4
Training loss: 1.6160576343536377
Validation loss: 2.076494733492533

Epoch: 6| Step: 5
Training loss: 2.1536450386047363
Validation loss: 2.087469736735026

Epoch: 6| Step: 6
Training loss: 1.3149996995925903
Validation loss: 2.0813236037890115

Epoch: 6| Step: 7
Training loss: 2.74118709564209
Validation loss: 2.096789379914602

Epoch: 6| Step: 8
Training loss: 1.9321479797363281
Validation loss: 2.096787691116333

Epoch: 6| Step: 9
Training loss: 1.456762671470642
Validation loss: 2.089193602403005

Epoch: 6| Step: 10
Training loss: 1.7508575916290283
Validation loss: 2.090741455554962

Epoch: 6| Step: 11
Training loss: 2.3843235969543457
Validation loss: 2.0777456164360046

Epoch: 6| Step: 12
Training loss: 2.051274299621582
Validation loss: 2.0866923332214355

Epoch: 6| Step: 13
Training loss: 2.324589252471924
Validation loss: 2.085283656915029

Epoch: 148| Step: 0
Training loss: 1.9747354984283447
Validation loss: 2.0806955893834433

Epoch: 6| Step: 1
Training loss: 2.2152628898620605
Validation loss: 2.087109128634135

Epoch: 6| Step: 2
Training loss: 1.9809324741363525
Validation loss: 2.086042602856954

Epoch: 6| Step: 3
Training loss: 1.7911938428878784
Validation loss: 2.0671831369400024

Epoch: 6| Step: 4
Training loss: 1.9888774156570435
Validation loss: 2.071273664633433

Epoch: 6| Step: 5
Training loss: 1.97273588180542
Validation loss: 2.069530646006266

Epoch: 6| Step: 6
Training loss: 1.6659364700317383
Validation loss: 2.077149271965027

Epoch: 6| Step: 7
Training loss: 1.758487343788147
Validation loss: 2.0788145860036216

Epoch: 6| Step: 8
Training loss: 2.097928524017334
Validation loss: 2.0702436765034995

Epoch: 6| Step: 9
Training loss: 1.8613158464431763
Validation loss: 2.0752477248509726

Epoch: 6| Step: 10
Training loss: 2.3524389266967773
Validation loss: 2.083436926205953

Epoch: 6| Step: 11
Training loss: 2.2325291633605957
Validation loss: 2.0845477183659873

Epoch: 6| Step: 12
Training loss: 1.4257009029388428
Validation loss: 2.095804830392202

Epoch: 6| Step: 13
Training loss: 2.574758529663086
Validation loss: 2.0888877511024475

Epoch: 149| Step: 0
Training loss: 1.7601406574249268
Validation loss: 2.104754646619161

Epoch: 6| Step: 1
Training loss: 2.2002227306365967
Validation loss: 2.108618915081024

Epoch: 6| Step: 2
Training loss: 1.8174805641174316
Validation loss: 2.124967316786448

Epoch: 6| Step: 3
Training loss: 1.594439148902893
Validation loss: 2.1359500686327615

Epoch: 6| Step: 4
Training loss: 1.939766764640808
Validation loss: 2.1242434779802957

Epoch: 6| Step: 5
Training loss: 1.6236594915390015
Validation loss: 2.123842239379883

Epoch: 6| Step: 6
Training loss: 1.7352620363235474
Validation loss: 2.125099261601766

Epoch: 6| Step: 7
Training loss: 2.0848727226257324
Validation loss: 2.1008988420168557

Epoch: 6| Step: 8
Training loss: 1.866330862045288
Validation loss: 2.079673171043396

Epoch: 6| Step: 9
Training loss: 2.2064905166625977
Validation loss: 2.081901033719381

Epoch: 6| Step: 10
Training loss: 2.4500460624694824
Validation loss: 2.0658066868782043

Epoch: 6| Step: 11
Training loss: 2.8283181190490723
Validation loss: 2.0694013635317483

Epoch: 6| Step: 12
Training loss: 1.6962075233459473
Validation loss: 2.0701129039128623

Epoch: 6| Step: 13
Training loss: 2.1388444900512695
Validation loss: 2.0611241261164346

Epoch: 150| Step: 0
Training loss: 2.015423536300659
Validation loss: 2.0669752756754556

Epoch: 6| Step: 1
Training loss: 1.683319330215454
Validation loss: 2.0727328260739646

Epoch: 6| Step: 2
Training loss: 2.253171682357788
Validation loss: 2.075002610683441

Epoch: 6| Step: 3
Training loss: 2.6409828662872314
Validation loss: 2.0841228365898132

Epoch: 6| Step: 4
Training loss: 1.713081955909729
Validation loss: 2.0773836175600686

Epoch: 6| Step: 5
Training loss: 2.2556803226470947
Validation loss: 2.0928863684336343

Epoch: 6| Step: 6
Training loss: 1.3733655214309692
Validation loss: 2.101145644982656

Epoch: 6| Step: 7
Training loss: 1.7611860036849976
Validation loss: 2.1133472124735513

Epoch: 6| Step: 8
Training loss: 1.529282569885254
Validation loss: 2.1191675464312234

Epoch: 6| Step: 9
Training loss: 1.8904311656951904
Validation loss: 2.119249244530996

Epoch: 6| Step: 10
Training loss: 2.5714986324310303
Validation loss: 2.1198007265726724

Epoch: 6| Step: 11
Training loss: 2.625429630279541
Validation loss: 2.1283284028371177

Epoch: 6| Step: 12
Training loss: 1.8397397994995117
Validation loss: 2.124117394288381

Epoch: 6| Step: 13
Training loss: 2.0521974563598633
Validation loss: 2.115674157937368

Epoch: 151| Step: 0
Training loss: 2.6143882274627686
Validation loss: 2.102027654647827

Epoch: 6| Step: 1
Training loss: 1.732012391090393
Validation loss: 2.1029038627942405

Epoch: 6| Step: 2
Training loss: 1.9144070148468018
Validation loss: 2.0834742188453674

Epoch: 6| Step: 3
Training loss: 2.4283056259155273
Validation loss: 2.1082499027252197

Epoch: 6| Step: 4
Training loss: 2.2891852855682373
Validation loss: 2.077792684237162

Epoch: 6| Step: 5
Training loss: 2.0877413749694824
Validation loss: 2.083855470021566

Epoch: 6| Step: 6
Training loss: 1.448974609375
Validation loss: 2.082447608311971

Epoch: 6| Step: 7
Training loss: 1.3671077489852905
Validation loss: 2.0878533522288003

Epoch: 6| Step: 8
Training loss: 2.0279359817504883
Validation loss: 2.0871522029240928

Epoch: 6| Step: 9
Training loss: 1.347510814666748
Validation loss: 2.0796278715133667

Epoch: 6| Step: 10
Training loss: 2.1848044395446777
Validation loss: 2.0739630460739136

Epoch: 6| Step: 11
Training loss: 2.239675998687744
Validation loss: 2.082328498363495

Epoch: 6| Step: 12
Training loss: 1.902668833732605
Validation loss: 2.082050005594889

Epoch: 6| Step: 13
Training loss: 1.8798558712005615
Validation loss: 2.0995276172955832

Epoch: 152| Step: 0
Training loss: 2.42783784866333
Validation loss: 2.108337640762329

Epoch: 6| Step: 1
Training loss: 2.5158305168151855
Validation loss: 2.0881518920262656

Epoch: 6| Step: 2
Training loss: 1.7470996379852295
Validation loss: 2.1224658091863

Epoch: 6| Step: 3
Training loss: 1.6733417510986328
Validation loss: 2.121478001276652

Epoch: 6| Step: 4
Training loss: 2.451119899749756
Validation loss: 2.117444018522898

Epoch: 6| Step: 5
Training loss: 1.8401987552642822
Validation loss: 2.132625679175059

Epoch: 6| Step: 6
Training loss: 1.7878457307815552
Validation loss: 2.1088867584864297

Epoch: 6| Step: 7
Training loss: 1.8734838962554932
Validation loss: 2.11262180407842

Epoch: 6| Step: 8
Training loss: 1.4617338180541992
Validation loss: 2.1009077231089273

Epoch: 6| Step: 9
Training loss: 1.2594201564788818
Validation loss: 2.0976714491844177

Epoch: 6| Step: 10
Training loss: 1.775220513343811
Validation loss: 2.096941033999125

Epoch: 6| Step: 11
Training loss: 2.2883567810058594
Validation loss: 2.074081599712372

Epoch: 6| Step: 12
Training loss: 2.2731170654296875
Validation loss: 2.070221185684204

Epoch: 6| Step: 13
Training loss: 2.012756586074829
Validation loss: 2.0536675254503884

Epoch: 153| Step: 0
Training loss: 1.6577703952789307
Validation loss: 2.056407650311788

Epoch: 6| Step: 1
Training loss: 2.5120716094970703
Validation loss: 2.053794582684835

Epoch: 6| Step: 2
Training loss: 2.1516060829162598
Validation loss: 2.0510058403015137

Epoch: 6| Step: 3
Training loss: 1.8077784776687622
Validation loss: 2.0616938273111978

Epoch: 6| Step: 4
Training loss: 2.563445568084717
Validation loss: 2.063698967297872

Epoch: 6| Step: 5
Training loss: 2.675799608230591
Validation loss: 2.0720409552256265

Epoch: 6| Step: 6
Training loss: 2.290807008743286
Validation loss: 2.06913959980011

Epoch: 6| Step: 7
Training loss: 2.282519578933716
Validation loss: 2.072706321875254

Epoch: 6| Step: 8
Training loss: 1.7173805236816406
Validation loss: 2.060971180597941

Epoch: 6| Step: 9
Training loss: 1.9036951065063477
Validation loss: 2.0573604504267373

Epoch: 6| Step: 10
Training loss: 1.4505925178527832
Validation loss: 2.0761758287747702

Epoch: 6| Step: 11
Training loss: 2.1194570064544678
Validation loss: 2.069239596525828

Epoch: 6| Step: 12
Training loss: 1.7886735200881958
Validation loss: 2.077966650327047

Epoch: 6| Step: 13
Training loss: 1.8169960975646973
Validation loss: 2.0749212900797525

Epoch: 154| Step: 0
Training loss: 1.4347686767578125
Validation loss: 2.1081426541010537

Epoch: 6| Step: 1
Training loss: 1.7592763900756836
Validation loss: 2.123968521753947

Epoch: 6| Step: 2
Training loss: 2.1745781898498535
Validation loss: 2.141948163509369

Epoch: 6| Step: 3
Training loss: 2.231050491333008
Validation loss: 2.1503675977389016

Epoch: 6| Step: 4
Training loss: 1.925144910812378
Validation loss: 2.130306820074717

Epoch: 6| Step: 5
Training loss: 2.65675950050354
Validation loss: 2.1190096139907837

Epoch: 6| Step: 6
Training loss: 2.8547072410583496
Validation loss: 2.1115076541900635

Epoch: 6| Step: 7
Training loss: 1.7121422290802002
Validation loss: 2.096534013748169

Epoch: 6| Step: 8
Training loss: 2.438897132873535
Validation loss: 2.0909884373346963

Epoch: 6| Step: 9
Training loss: 2.037832736968994
Validation loss: 2.0792479117711387

Epoch: 6| Step: 10
Training loss: 1.6223907470703125
Validation loss: 2.0901779731114707

Epoch: 6| Step: 11
Training loss: 1.2581820487976074
Validation loss: 2.0791674852371216

Epoch: 6| Step: 12
Training loss: 1.8840264081954956
Validation loss: 2.0727860728899636

Epoch: 6| Step: 13
Training loss: 1.9459298849105835
Validation loss: 2.099929134051005

Epoch: 155| Step: 0
Training loss: 1.7458616495132446
Validation loss: 2.0993147095044455

Epoch: 6| Step: 1
Training loss: 2.1134419441223145
Validation loss: 2.087260444959005

Epoch: 6| Step: 2
Training loss: 1.7848492860794067
Validation loss: 2.0906718969345093

Epoch: 6| Step: 3
Training loss: 1.6602654457092285
Validation loss: 2.091811021169027

Epoch: 6| Step: 4
Training loss: 2.0822558403015137
Validation loss: 2.096908668677012

Epoch: 6| Step: 5
Training loss: 1.798304557800293
Validation loss: 2.0913984775543213

Epoch: 6| Step: 6
Training loss: 2.823354482650757
Validation loss: 2.08414218823115

Epoch: 6| Step: 7
Training loss: 1.9069498777389526
Validation loss: 2.085458993911743

Epoch: 6| Step: 8
Training loss: 2.6060099601745605
Validation loss: 2.091023802757263

Epoch: 6| Step: 9
Training loss: 1.6977880001068115
Validation loss: 2.0903550386428833

Epoch: 6| Step: 10
Training loss: 1.2073655128479004
Validation loss: 2.0917306741078696

Epoch: 6| Step: 11
Training loss: 2.735292911529541
Validation loss: 2.0900827447573342

Epoch: 6| Step: 12
Training loss: 1.9139633178710938
Validation loss: 2.0982679526011148

Epoch: 6| Step: 13
Training loss: 1.3661730289459229
Validation loss: 2.1201695005098977

Epoch: 156| Step: 0
Training loss: 2.508315086364746
Validation loss: 2.1092456380526223

Epoch: 6| Step: 1
Training loss: 1.5785187482833862
Validation loss: 2.1245161493619285

Epoch: 6| Step: 2
Training loss: 1.963985562324524
Validation loss: 2.1433770855267844

Epoch: 6| Step: 3
Training loss: 1.8214976787567139
Validation loss: 2.134796063105265

Epoch: 6| Step: 4
Training loss: 2.04596209526062
Validation loss: 2.1358251174290976

Epoch: 6| Step: 5
Training loss: 1.4867616891860962
Validation loss: 2.1358169118563333

Epoch: 6| Step: 6
Training loss: 1.6034159660339355
Validation loss: 2.1256553332010903

Epoch: 6| Step: 7
Training loss: 2.2377521991729736
Validation loss: 2.1259748538335166

Epoch: 6| Step: 8
Training loss: 2.5249271392822266
Validation loss: 2.121637245019277

Epoch: 6| Step: 9
Training loss: 2.0522031784057617
Validation loss: 2.1122480432192483

Epoch: 6| Step: 10
Training loss: 1.6495757102966309
Validation loss: 2.0985200802485147

Epoch: 6| Step: 11
Training loss: 2.098830223083496
Validation loss: 2.077469209829966

Epoch: 6| Step: 12
Training loss: 1.9706023931503296
Validation loss: 2.091937998930613

Epoch: 6| Step: 13
Training loss: 1.9482935667037964
Validation loss: 2.0860281387964883

Epoch: 157| Step: 0
Training loss: 2.5011372566223145
Validation loss: 2.0743643045425415

Epoch: 6| Step: 1
Training loss: 2.3809878826141357
Validation loss: 2.0769611994425454

Epoch: 6| Step: 2
Training loss: 2.0850276947021484
Validation loss: 2.076840957005819

Epoch: 6| Step: 3
Training loss: 2.00709867477417
Validation loss: 2.068021535873413

Epoch: 6| Step: 4
Training loss: 2.339057445526123
Validation loss: 2.0785494446754456

Epoch: 6| Step: 5
Training loss: 1.7955224514007568
Validation loss: 2.087896386782328

Epoch: 6| Step: 6
Training loss: 2.3653883934020996
Validation loss: 2.0932268699010215

Epoch: 6| Step: 7
Training loss: 1.5334908962249756
Validation loss: 2.104931135972341

Epoch: 6| Step: 8
Training loss: 1.4775779247283936
Validation loss: 2.0995455781618753

Epoch: 6| Step: 9
Training loss: 1.7106986045837402
Validation loss: 2.1104024052619934

Epoch: 6| Step: 10
Training loss: 1.9549369812011719
Validation loss: 2.0989399353663125

Epoch: 6| Step: 11
Training loss: 1.9929800033569336
Validation loss: 2.1263196070988974

Epoch: 6| Step: 12
Training loss: 1.983302354812622
Validation loss: 2.124923805395762

Epoch: 6| Step: 13
Training loss: 1.4476606845855713
Validation loss: 2.1266029278437295

Epoch: 158| Step: 0
Training loss: 1.7857224941253662
Validation loss: 2.1336769660313926

Epoch: 6| Step: 1
Training loss: 1.5878467559814453
Validation loss: 2.1145366430282593

Epoch: 6| Step: 2
Training loss: 1.952703595161438
Validation loss: 2.1116015116373696

Epoch: 6| Step: 3
Training loss: 1.7940640449523926
Validation loss: 2.108291188875834

Epoch: 6| Step: 4
Training loss: 1.799365520477295
Validation loss: 2.102303663889567

Epoch: 6| Step: 5
Training loss: 2.580211877822876
Validation loss: 2.1000312765439353

Epoch: 6| Step: 6
Training loss: 1.6275067329406738
Validation loss: 2.096720337867737

Epoch: 6| Step: 7
Training loss: 1.9564146995544434
Validation loss: 2.1027283668518066

Epoch: 6| Step: 8
Training loss: 2.2794599533081055
Validation loss: 2.114024817943573

Epoch: 6| Step: 9
Training loss: 1.74494469165802
Validation loss: 2.1222692728042603

Epoch: 6| Step: 10
Training loss: 1.7197833061218262
Validation loss: 2.122671663761139

Epoch: 6| Step: 11
Training loss: 2.709096908569336
Validation loss: 2.114242355028788

Epoch: 6| Step: 12
Training loss: 1.8643524646759033
Validation loss: 2.1226550936698914

Epoch: 6| Step: 13
Training loss: 1.9836297035217285
Validation loss: 2.107098658879598

Epoch: 159| Step: 0
Training loss: 1.822706699371338
Validation loss: 2.1070489088694253

Epoch: 6| Step: 1
Training loss: 1.6868585348129272
Validation loss: 2.111109495162964

Epoch: 6| Step: 2
Training loss: 2.3906850814819336
Validation loss: 2.104031721750895

Epoch: 6| Step: 3
Training loss: 1.722145915031433
Validation loss: 2.08858189980189

Epoch: 6| Step: 4
Training loss: 1.9832323789596558
Validation loss: 2.097293794155121

Epoch: 6| Step: 5
Training loss: 2.389143466949463
Validation loss: 2.0944612622261047

Epoch: 6| Step: 6
Training loss: 2.2217321395874023
Validation loss: 2.092484255631765

Epoch: 6| Step: 7
Training loss: 1.9620380401611328
Validation loss: 2.0995463331540427

Epoch: 6| Step: 8
Training loss: 1.9345091581344604
Validation loss: 2.1148805220921836

Epoch: 6| Step: 9
Training loss: 2.031743049621582
Validation loss: 2.1108824412027993

Epoch: 6| Step: 10
Training loss: 1.921019434928894
Validation loss: 2.126008709271749

Epoch: 6| Step: 11
Training loss: 1.634406328201294
Validation loss: 2.1338442961374917

Epoch: 6| Step: 12
Training loss: 1.5749597549438477
Validation loss: 2.1272407372792563

Epoch: 6| Step: 13
Training loss: 1.9588384628295898
Validation loss: 2.132574200630188

Epoch: 160| Step: 0
Training loss: 1.9103004932403564
Validation loss: 2.137404978275299

Epoch: 6| Step: 1
Training loss: 2.3548543453216553
Validation loss: 2.1227789521217346

Epoch: 6| Step: 2
Training loss: 2.0479249954223633
Validation loss: 2.124980926513672

Epoch: 6| Step: 3
Training loss: 2.3686022758483887
Validation loss: 2.1131575306256614

Epoch: 6| Step: 4
Training loss: 1.971195101737976
Validation loss: 2.1245398918787637

Epoch: 6| Step: 5
Training loss: 1.5394203662872314
Validation loss: 2.126797537008921

Epoch: 6| Step: 6
Training loss: 2.1587367057800293
Validation loss: 2.1154222091039023

Epoch: 6| Step: 7
Training loss: 1.902350902557373
Validation loss: 2.10991895198822

Epoch: 6| Step: 8
Training loss: 1.9202781915664673
Validation loss: 2.1142945686976113

Epoch: 6| Step: 9
Training loss: 1.8069491386413574
Validation loss: 2.1149460077285767

Epoch: 6| Step: 10
Training loss: 1.7532050609588623
Validation loss: 2.0983024636904397

Epoch: 6| Step: 11
Training loss: 2.1487574577331543
Validation loss: 2.0943301717440286

Epoch: 6| Step: 12
Training loss: 1.6921186447143555
Validation loss: 2.098175863424937

Epoch: 6| Step: 13
Training loss: 1.5884573459625244
Validation loss: 2.099936783313751

Epoch: 161| Step: 0
Training loss: 2.087067127227783
Validation loss: 2.1004700859387717

Epoch: 6| Step: 1
Training loss: 1.6789214611053467
Validation loss: 2.1153579552968345

Epoch: 6| Step: 2
Training loss: 2.2223918437957764
Validation loss: 2.110075851281484

Epoch: 6| Step: 3
Training loss: 2.0818076133728027
Validation loss: 2.116442024707794

Epoch: 6| Step: 4
Training loss: 1.9687913656234741
Validation loss: 2.115572532018026

Epoch: 6| Step: 5
Training loss: 1.2186545133590698
Validation loss: 2.129453718662262

Epoch: 6| Step: 6
Training loss: 1.9511213302612305
Validation loss: 2.1394348541895547

Epoch: 6| Step: 7
Training loss: 2.7817893028259277
Validation loss: 2.135673920313517

Epoch: 6| Step: 8
Training loss: 1.7519826889038086
Validation loss: 2.1205886205037436

Epoch: 6| Step: 9
Training loss: 2.383389711380005
Validation loss: 2.1333038806915283

Epoch: 6| Step: 10
Training loss: 2.3618719577789307
Validation loss: 2.117542088031769

Epoch: 6| Step: 11
Training loss: 1.3373732566833496
Validation loss: 2.1054819226264954

Epoch: 6| Step: 12
Training loss: 1.611438512802124
Validation loss: 2.109530965487162

Epoch: 6| Step: 13
Training loss: 1.730675458908081
Validation loss: 2.105888068675995

Epoch: 162| Step: 0
Training loss: 1.76255464553833
Validation loss: 2.103409012158712

Epoch: 6| Step: 1
Training loss: 2.2171616554260254
Validation loss: 2.1181787252426147

Epoch: 6| Step: 2
Training loss: 1.90028977394104
Validation loss: 2.1353201071421304

Epoch: 6| Step: 3
Training loss: 2.075629711151123
Validation loss: 2.12341970205307

Epoch: 6| Step: 4
Training loss: 0.9222530722618103
Validation loss: 2.1245957215627036

Epoch: 6| Step: 5
Training loss: 1.9210844039916992
Validation loss: 2.134197950363159

Epoch: 6| Step: 6
Training loss: 2.4617443084716797
Validation loss: 2.1340596079826355

Epoch: 6| Step: 7
Training loss: 2.2671051025390625
Validation loss: 2.151910344759623

Epoch: 6| Step: 8
Training loss: 1.9762697219848633
Validation loss: 2.1402886311213174

Epoch: 6| Step: 9
Training loss: 2.32273006439209
Validation loss: 2.1514480312665305

Epoch: 6| Step: 10
Training loss: 2.435375928878784
Validation loss: 2.130153397719065

Epoch: 6| Step: 11
Training loss: 2.120069980621338
Validation loss: 2.1167536973953247

Epoch: 6| Step: 12
Training loss: 1.4095051288604736
Validation loss: 2.1283635099728904

Epoch: 6| Step: 13
Training loss: 1.603071689605713
Validation loss: 2.122302691141764

Epoch: 163| Step: 0
Training loss: 1.9802974462509155
Validation loss: 2.1031726797421775

Epoch: 6| Step: 1
Training loss: 2.367361068725586
Validation loss: 2.1204604705174765

Epoch: 6| Step: 2
Training loss: 1.9163389205932617
Validation loss: 2.111350138982137

Epoch: 6| Step: 3
Training loss: 2.271751880645752
Validation loss: 2.118107716242472

Epoch: 6| Step: 4
Training loss: 1.9038996696472168
Validation loss: 2.103553354740143

Epoch: 6| Step: 5
Training loss: 2.439271926879883
Validation loss: 2.122553745905558

Epoch: 6| Step: 6
Training loss: 1.6958080530166626
Validation loss: 2.1138300697008767

Epoch: 6| Step: 7
Training loss: 0.9449830055236816
Validation loss: 2.113556762536367

Epoch: 6| Step: 8
Training loss: 2.2606585025787354
Validation loss: 2.12691738208135

Epoch: 6| Step: 9
Training loss: 2.2181296348571777
Validation loss: 2.116130610307058

Epoch: 6| Step: 10
Training loss: 1.1665313243865967
Validation loss: 2.129982908566793

Epoch: 6| Step: 11
Training loss: 1.766983985900879
Validation loss: 2.1183003981908164

Epoch: 6| Step: 12
Training loss: 2.219672918319702
Validation loss: 2.1082744201024375

Epoch: 6| Step: 13
Training loss: 1.8171015977859497
Validation loss: 2.1153980096181235

Epoch: 164| Step: 0
Training loss: 1.730919361114502
Validation loss: 2.1252155105272927

Epoch: 6| Step: 1
Training loss: 2.307698965072632
Validation loss: 2.114810526371002

Epoch: 6| Step: 2
Training loss: 1.9859570264816284
Validation loss: 2.1149717966715493

Epoch: 6| Step: 3
Training loss: 2.554577350616455
Validation loss: 2.1067068576812744

Epoch: 6| Step: 4
Training loss: 1.855806589126587
Validation loss: 2.107768098513285

Epoch: 6| Step: 5
Training loss: 1.986149549484253
Validation loss: 2.0956245263417563

Epoch: 6| Step: 6
Training loss: 1.5754789113998413
Validation loss: 2.1143288811047873

Epoch: 6| Step: 7
Training loss: 2.672877311706543
Validation loss: 2.0988744099934897

Epoch: 6| Step: 8
Training loss: 1.74823796749115
Validation loss: 2.1057862440745034

Epoch: 6| Step: 9
Training loss: 1.6630942821502686
Validation loss: 2.1127721667289734

Epoch: 6| Step: 10
Training loss: 1.5122087001800537
Validation loss: 2.1030718286832175

Epoch: 6| Step: 11
Training loss: 2.2799413204193115
Validation loss: 2.106753706932068

Epoch: 6| Step: 12
Training loss: 1.4057610034942627
Validation loss: 2.129136602083842

Epoch: 6| Step: 13
Training loss: 1.9547991752624512
Validation loss: 2.1234127283096313

Epoch: 165| Step: 0
Training loss: 1.8529293537139893
Validation loss: 2.119689961274465

Epoch: 6| Step: 1
Training loss: 1.8079699277877808
Validation loss: 2.1232563257217407

Epoch: 6| Step: 2
Training loss: 1.5412421226501465
Validation loss: 2.1090564330418906

Epoch: 6| Step: 3
Training loss: 1.6453444957733154
Validation loss: 2.12179168065389

Epoch: 6| Step: 4
Training loss: 2.1448445320129395
Validation loss: 2.1310236851374307

Epoch: 6| Step: 5
Training loss: 1.9433729648590088
Validation loss: 2.1185930172602334

Epoch: 6| Step: 6
Training loss: 2.376631259918213
Validation loss: 2.1235814491907754

Epoch: 6| Step: 7
Training loss: 1.3523629903793335
Validation loss: 2.1123828689257302

Epoch: 6| Step: 8
Training loss: 2.6673269271850586
Validation loss: 2.1106887062390647

Epoch: 6| Step: 9
Training loss: 2.549740791320801
Validation loss: 2.1088008483250937

Epoch: 6| Step: 10
Training loss: 2.140881299972534
Validation loss: 2.094741960366567

Epoch: 6| Step: 11
Training loss: 1.4135454893112183
Validation loss: 2.1014674305915833

Epoch: 6| Step: 12
Training loss: 2.1250064373016357
Validation loss: 2.0923156340916953

Epoch: 6| Step: 13
Training loss: 1.58784818649292
Validation loss: 2.0940051277478537

Epoch: 166| Step: 0
Training loss: 1.6203553676605225
Validation loss: 2.0957603454589844

Epoch: 6| Step: 1
Training loss: 1.6334009170532227
Validation loss: 2.0938140749931335

Epoch: 6| Step: 2
Training loss: 2.3301949501037598
Validation loss: 2.0951361060142517

Epoch: 6| Step: 3
Training loss: 1.520218849182129
Validation loss: 2.100689788659414

Epoch: 6| Step: 4
Training loss: 2.011261463165283
Validation loss: 2.101574937502543

Epoch: 6| Step: 5
Training loss: 2.0003459453582764
Validation loss: 2.116564134756724

Epoch: 6| Step: 6
Training loss: 1.8542653322219849
Validation loss: 2.1015623013178506

Epoch: 6| Step: 7
Training loss: 1.9463400840759277
Validation loss: 2.1177891294161477

Epoch: 6| Step: 8
Training loss: 2.0063486099243164
Validation loss: 2.102997303009033

Epoch: 6| Step: 9
Training loss: 1.9045048952102661
Validation loss: 2.113108217716217

Epoch: 6| Step: 10
Training loss: 2.302105665206909
Validation loss: 2.1123111645380654

Epoch: 6| Step: 11
Training loss: 2.494631290435791
Validation loss: 2.1276216904322305

Epoch: 6| Step: 12
Training loss: 1.6251949071884155
Validation loss: 2.12925777832667

Epoch: 6| Step: 13
Training loss: 1.7442445755004883
Validation loss: 2.1158742109934487

Epoch: 167| Step: 0
Training loss: 1.8126142024993896
Validation loss: 2.1215820709864297

Epoch: 6| Step: 1
Training loss: 1.8323367834091187
Validation loss: 2.1134939988454184

Epoch: 6| Step: 2
Training loss: 2.053530216217041
Validation loss: 2.1063507795333862

Epoch: 6| Step: 3
Training loss: 1.6590064764022827
Validation loss: 2.1062387228012085

Epoch: 6| Step: 4
Training loss: 2.2057881355285645
Validation loss: 2.107853968938192

Epoch: 6| Step: 5
Training loss: 1.9609307050704956
Validation loss: 2.0997694532076516

Epoch: 6| Step: 6
Training loss: 2.426661252975464
Validation loss: 2.1036685506502786

Epoch: 6| Step: 7
Training loss: 2.297914505004883
Validation loss: 2.099561790625254

Epoch: 6| Step: 8
Training loss: 1.5760102272033691
Validation loss: 2.093331436316172

Epoch: 6| Step: 9
Training loss: 1.8710696697235107
Validation loss: 2.1056993206342063

Epoch: 6| Step: 10
Training loss: 2.075343608856201
Validation loss: 2.1263240973154702

Epoch: 6| Step: 11
Training loss: 2.2357211112976074
Validation loss: 2.1189809441566467

Epoch: 6| Step: 12
Training loss: 1.671493649482727
Validation loss: 2.127438267072042

Epoch: 6| Step: 13
Training loss: 1.2734464406967163
Validation loss: 2.1310340563456216

Epoch: 168| Step: 0
Training loss: 1.8253262042999268
Validation loss: 2.1363601287206015

Epoch: 6| Step: 1
Training loss: 1.603384017944336
Validation loss: 2.1285536885261536

Epoch: 6| Step: 2
Training loss: 1.499078392982483
Validation loss: 2.122858166694641

Epoch: 6| Step: 3
Training loss: 1.5799914598464966
Validation loss: 2.1192556420962014

Epoch: 6| Step: 4
Training loss: 2.339465856552124
Validation loss: 2.1015894015630088

Epoch: 6| Step: 5
Training loss: 1.5106770992279053
Validation loss: 2.108084201812744

Epoch: 6| Step: 6
Training loss: 1.7723414897918701
Validation loss: 2.1095436612764993

Epoch: 6| Step: 7
Training loss: 2.4287829399108887
Validation loss: 2.1209574540456138

Epoch: 6| Step: 8
Training loss: 1.8240528106689453
Validation loss: 2.112713615099589

Epoch: 6| Step: 9
Training loss: 2.148332357406616
Validation loss: 2.1061758399009705

Epoch: 6| Step: 10
Training loss: 1.7502272129058838
Validation loss: 2.116396685441335

Epoch: 6| Step: 11
Training loss: 2.028777599334717
Validation loss: 2.1167421340942383

Epoch: 6| Step: 12
Training loss: 2.1238327026367188
Validation loss: 2.137476404507955

Epoch: 6| Step: 13
Training loss: 2.6397690773010254
Validation loss: 2.141429384549459

Epoch: 169| Step: 0
Training loss: 2.542269468307495
Validation loss: 2.139525890350342

Epoch: 6| Step: 1
Training loss: 2.277935266494751
Validation loss: 2.1613875230153403

Epoch: 6| Step: 2
Training loss: 2.053271770477295
Validation loss: 2.1464552084604898

Epoch: 6| Step: 3
Training loss: 2.125084400177002
Validation loss: 2.1348126928011575

Epoch: 6| Step: 4
Training loss: 1.865436315536499
Validation loss: 2.137682636578878

Epoch: 6| Step: 5
Training loss: 1.177818775177002
Validation loss: 2.117800235748291

Epoch: 6| Step: 6
Training loss: 2.041167736053467
Validation loss: 2.1239912509918213

Epoch: 6| Step: 7
Training loss: 1.9386236667633057
Validation loss: 2.129028538862864

Epoch: 6| Step: 8
Training loss: 1.68746018409729
Validation loss: 2.1066580613454184

Epoch: 6| Step: 9
Training loss: 2.0664234161376953
Validation loss: 2.1012376149495444

Epoch: 6| Step: 10
Training loss: 1.948854684829712
Validation loss: 2.1063926617304483

Epoch: 6| Step: 11
Training loss: 1.5627176761627197
Validation loss: 2.117387334505717

Epoch: 6| Step: 12
Training loss: 1.920968770980835
Validation loss: 2.1013964215914407

Epoch: 6| Step: 13
Training loss: 1.6810851097106934
Validation loss: 2.1018972198168435

Epoch: 170| Step: 0
Training loss: 2.1030287742614746
Validation loss: 2.1066186229387918

Epoch: 6| Step: 1
Training loss: 1.8934228420257568
Validation loss: 2.120403230190277

Epoch: 6| Step: 2
Training loss: 1.420250654220581
Validation loss: 2.103742996851603

Epoch: 6| Step: 3
Training loss: 2.3460707664489746
Validation loss: 2.1159525712331138

Epoch: 6| Step: 4
Training loss: 2.054800033569336
Validation loss: 2.1347572207450867

Epoch: 6| Step: 5
Training loss: 2.1912059783935547
Validation loss: 2.1349594990412393

Epoch: 6| Step: 6
Training loss: 1.7363308668136597
Validation loss: 2.16118460893631

Epoch: 6| Step: 7
Training loss: 2.24810528755188
Validation loss: 2.1623513301213584

Epoch: 6| Step: 8
Training loss: 1.8249821662902832
Validation loss: 2.167872170607249

Epoch: 6| Step: 9
Training loss: 1.6442484855651855
Validation loss: 2.1662850578626

Epoch: 6| Step: 10
Training loss: 2.226386785507202
Validation loss: 2.1404401063919067

Epoch: 6| Step: 11
Training loss: 1.6592525243759155
Validation loss: 2.1452011863390603

Epoch: 6| Step: 12
Training loss: 2.001952648162842
Validation loss: 2.1292529900868735

Epoch: 6| Step: 13
Training loss: 1.9636950492858887
Validation loss: 2.1239591042200723

Epoch: 171| Step: 0
Training loss: 1.3285753726959229
Validation loss: 2.1045549313227334

Epoch: 6| Step: 1
Training loss: 1.9285331964492798
Validation loss: 2.106886168320974

Epoch: 6| Step: 2
Training loss: 1.6064856052398682
Validation loss: 2.103792210419973

Epoch: 6| Step: 3
Training loss: 2.27309513092041
Validation loss: 2.1057952443758645

Epoch: 6| Step: 4
Training loss: 1.871564269065857
Validation loss: 2.094919522603353

Epoch: 6| Step: 5
Training loss: 2.0230512619018555
Validation loss: 2.0929760535558066

Epoch: 6| Step: 6
Training loss: 2.294428825378418
Validation loss: 2.1052868366241455

Epoch: 6| Step: 7
Training loss: 1.8610420227050781
Validation loss: 2.1013335585594177

Epoch: 6| Step: 8
Training loss: 1.9797290563583374
Validation loss: 2.0957911610603333

Epoch: 6| Step: 9
Training loss: 1.6057542562484741
Validation loss: 2.100733240445455

Epoch: 6| Step: 10
Training loss: 2.240832567214966
Validation loss: 2.101022243499756

Epoch: 6| Step: 11
Training loss: 1.8172881603240967
Validation loss: 2.10736354192098

Epoch: 6| Step: 12
Training loss: 1.7415854930877686
Validation loss: 2.107779085636139

Epoch: 6| Step: 13
Training loss: 2.182504892349243
Validation loss: 2.1185457706451416

Epoch: 172| Step: 0
Training loss: 1.953606128692627
Validation loss: 2.11112380027771

Epoch: 6| Step: 1
Training loss: 2.2646543979644775
Validation loss: 2.119599918524424

Epoch: 6| Step: 2
Training loss: 1.607195258140564
Validation loss: 2.1213285525639853

Epoch: 6| Step: 3
Training loss: 2.145054817199707
Validation loss: 2.1292162338892617

Epoch: 6| Step: 4
Training loss: 1.9479807615280151
Validation loss: 2.1323829293251038

Epoch: 6| Step: 5
Training loss: 1.767275094985962
Validation loss: 2.153666893641154

Epoch: 6| Step: 6
Training loss: 2.08994460105896
Validation loss: 2.1333641608556113

Epoch: 6| Step: 7
Training loss: 2.619354248046875
Validation loss: 2.1539231141408286

Epoch: 6| Step: 8
Training loss: 1.7131695747375488
Validation loss: 2.1518529256184897

Epoch: 6| Step: 9
Training loss: 1.3577749729156494
Validation loss: 2.147057036558787

Epoch: 6| Step: 10
Training loss: 1.9631983041763306
Validation loss: 2.145742416381836

Epoch: 6| Step: 11
Training loss: 2.1132848262786865
Validation loss: 2.124842623869578

Epoch: 6| Step: 12
Training loss: 1.8889198303222656
Validation loss: 2.1204370260238647

Epoch: 6| Step: 13
Training loss: 1.4755702018737793
Validation loss: 2.1221588253974915

Epoch: 173| Step: 0
Training loss: 1.6288529634475708
Validation loss: 2.1224323908487954

Epoch: 6| Step: 1
Training loss: 2.068967342376709
Validation loss: 2.122423231601715

Epoch: 6| Step: 2
Training loss: 2.1214795112609863
Validation loss: 2.108174443244934

Epoch: 6| Step: 3
Training loss: 1.4159235954284668
Validation loss: 2.11816136042277

Epoch: 6| Step: 4
Training loss: 1.777321219444275
Validation loss: 2.121816714604696

Epoch: 6| Step: 5
Training loss: 1.0778554677963257
Validation loss: 2.1136486530303955

Epoch: 6| Step: 6
Training loss: 2.0532844066619873
Validation loss: 2.1124764482180276

Epoch: 6| Step: 7
Training loss: 2.44277024269104
Validation loss: 2.1114729841550193

Epoch: 6| Step: 8
Training loss: 2.0089118480682373
Validation loss: 2.1342258850733438

Epoch: 6| Step: 9
Training loss: 2.1008975505828857
Validation loss: 2.1135661602020264

Epoch: 6| Step: 10
Training loss: 1.7900410890579224
Validation loss: 2.125359614690145

Epoch: 6| Step: 11
Training loss: 1.9636313915252686
Validation loss: 2.1269404689470925

Epoch: 6| Step: 12
Training loss: 2.1313869953155518
Validation loss: 2.1226157546043396

Epoch: 6| Step: 13
Training loss: 2.1767520904541016
Validation loss: 2.133035937945048

Epoch: 174| Step: 0
Training loss: 2.133031129837036
Validation loss: 2.1217729647954306

Epoch: 6| Step: 1
Training loss: 1.8958921432495117
Validation loss: 2.1401604215304055

Epoch: 6| Step: 2
Training loss: 1.597886085510254
Validation loss: 2.1617804169654846

Epoch: 6| Step: 3
Training loss: 1.8320643901824951
Validation loss: 2.1607752442359924

Epoch: 6| Step: 4
Training loss: 1.1435067653656006
Validation loss: 2.1517554918924966

Epoch: 6| Step: 5
Training loss: 2.1400821208953857
Validation loss: 2.1339192390441895

Epoch: 6| Step: 6
Training loss: 2.309112787246704
Validation loss: 2.1389649907747903

Epoch: 6| Step: 7
Training loss: 1.8235429525375366
Validation loss: 2.1409124732017517

Epoch: 6| Step: 8
Training loss: 1.9647793769836426
Validation loss: 2.131313224633535

Epoch: 6| Step: 9
Training loss: 1.9493972063064575
Validation loss: 2.1120808323224387

Epoch: 6| Step: 10
Training loss: 1.7495331764221191
Validation loss: 2.1185877323150635

Epoch: 6| Step: 11
Training loss: 2.284971237182617
Validation loss: 2.1200177868207297

Epoch: 6| Step: 12
Training loss: 2.035466194152832
Validation loss: 2.097885251045227

Epoch: 6| Step: 13
Training loss: 1.8837729692459106
Validation loss: 2.1047666867574057

Epoch: 175| Step: 0
Training loss: 2.55477237701416
Validation loss: 2.104054808616638

Epoch: 6| Step: 1
Training loss: 1.772580862045288
Validation loss: 2.09914364417394

Epoch: 6| Step: 2
Training loss: 1.9850407838821411
Validation loss: 2.0922370751698813

Epoch: 6| Step: 3
Training loss: 2.3223636150360107
Validation loss: 2.1099072297414145

Epoch: 6| Step: 4
Training loss: 1.5869452953338623
Validation loss: 2.097280204296112

Epoch: 6| Step: 5
Training loss: 1.8027994632720947
Validation loss: 2.10220205783844

Epoch: 6| Step: 6
Training loss: 2.103243827819824
Validation loss: 2.10829230149587

Epoch: 6| Step: 7
Training loss: 1.7148653268814087
Validation loss: 2.116287430127462

Epoch: 6| Step: 8
Training loss: 1.6505730152130127
Validation loss: 2.1253067453702292

Epoch: 6| Step: 9
Training loss: 1.733313798904419
Validation loss: 2.133878787358602

Epoch: 6| Step: 10
Training loss: 1.7395761013031006
Validation loss: 2.135282595952352

Epoch: 6| Step: 11
Training loss: 1.7808805704116821
Validation loss: 2.126237233479818

Epoch: 6| Step: 12
Training loss: 2.383129835128784
Validation loss: 2.1445399125417075

Epoch: 6| Step: 13
Training loss: 1.505608320236206
Validation loss: 2.1376151045163474

Epoch: 176| Step: 0
Training loss: 1.881118893623352
Validation loss: 2.1440524458885193

Epoch: 6| Step: 1
Training loss: 1.9015357494354248
Validation loss: 2.1632814009984336

Epoch: 6| Step: 2
Training loss: 1.4797778129577637
Validation loss: 2.1315520207087197

Epoch: 6| Step: 3
Training loss: 2.1729612350463867
Validation loss: 2.1261590321858725

Epoch: 6| Step: 4
Training loss: 2.256589889526367
Validation loss: 2.150421996911367

Epoch: 6| Step: 5
Training loss: 1.4223484992980957
Validation loss: 2.122013529141744

Epoch: 6| Step: 6
Training loss: 1.8752741813659668
Validation loss: 2.1211374600728354

Epoch: 6| Step: 7
Training loss: 1.8431346416473389
Validation loss: 2.137321432431539

Epoch: 6| Step: 8
Training loss: 1.6658363342285156
Validation loss: 2.1233607729276023

Epoch: 6| Step: 9
Training loss: 1.935326099395752
Validation loss: 2.137187957763672

Epoch: 6| Step: 10
Training loss: 1.8341164588928223
Validation loss: 2.127195358276367

Epoch: 6| Step: 11
Training loss: 1.7843023538589478
Validation loss: 2.139625151952108

Epoch: 6| Step: 12
Training loss: 2.407931327819824
Validation loss: 2.125546375910441

Epoch: 6| Step: 13
Training loss: 2.3627891540527344
Validation loss: 2.126674691836039

Epoch: 177| Step: 0
Training loss: 1.9214481115341187
Validation loss: 2.1241069634755454

Epoch: 6| Step: 1
Training loss: 2.0844902992248535
Validation loss: 2.1377116640408835

Epoch: 6| Step: 2
Training loss: 1.2923860549926758
Validation loss: 2.1448559562365213

Epoch: 6| Step: 3
Training loss: 2.2010810375213623
Validation loss: 2.1343053380648294

Epoch: 6| Step: 4
Training loss: 1.8091516494750977
Validation loss: 2.1349239150683084

Epoch: 6| Step: 5
Training loss: 1.833113670349121
Validation loss: 2.1353409687678018

Epoch: 6| Step: 6
Training loss: 1.7096734046936035
Validation loss: 2.1412533124287925

Epoch: 6| Step: 7
Training loss: 2.1234030723571777
Validation loss: 2.1116963624954224

Epoch: 6| Step: 8
Training loss: 2.629241704940796
Validation loss: 2.1153323451677957

Epoch: 6| Step: 9
Training loss: 1.8403087854385376
Validation loss: 2.114719053109487

Epoch: 6| Step: 10
Training loss: 1.7900288105010986
Validation loss: 2.1273591915766397

Epoch: 6| Step: 11
Training loss: 1.9562007188796997
Validation loss: 2.105774998664856

Epoch: 6| Step: 12
Training loss: 1.7712292671203613
Validation loss: 2.106906553109487

Epoch: 6| Step: 13
Training loss: 1.9195016622543335
Validation loss: 2.111361583073934

Epoch: 178| Step: 0
Training loss: 2.0672900676727295
Validation loss: 2.111098130544027

Epoch: 6| Step: 1
Training loss: 1.3575869798660278
Validation loss: 2.124442458152771

Epoch: 6| Step: 2
Training loss: 2.0251026153564453
Validation loss: 2.1275394558906555

Epoch: 6| Step: 3
Training loss: 1.7604167461395264
Validation loss: 2.124561885992686

Epoch: 6| Step: 4
Training loss: 1.4964098930358887
Validation loss: 2.133893072605133

Epoch: 6| Step: 5
Training loss: 2.426652193069458
Validation loss: 2.1260462403297424

Epoch: 6| Step: 6
Training loss: 1.2273178100585938
Validation loss: 2.147861043612162

Epoch: 6| Step: 7
Training loss: 1.692758560180664
Validation loss: 2.1649102369944253

Epoch: 6| Step: 8
Training loss: 2.1463675498962402
Validation loss: 2.1571940978368125

Epoch: 6| Step: 9
Training loss: 2.2040305137634277
Validation loss: 2.16444985071818

Epoch: 6| Step: 10
Training loss: 2.0664658546447754
Validation loss: 2.1708381374677024

Epoch: 6| Step: 11
Training loss: 2.119652271270752
Validation loss: 2.156979282697042

Epoch: 6| Step: 12
Training loss: 1.7659586668014526
Validation loss: 2.1514368653297424

Epoch: 6| Step: 13
Training loss: 2.0654373168945312
Validation loss: 2.1480173071225486

Epoch: 179| Step: 0
Training loss: 2.7226991653442383
Validation loss: 2.1258421738942466

Epoch: 6| Step: 1
Training loss: 1.7937037944793701
Validation loss: 2.1156290769577026

Epoch: 6| Step: 2
Training loss: 1.4958469867706299
Validation loss: 2.138574163118998

Epoch: 6| Step: 3
Training loss: 1.5743728876113892
Validation loss: 2.1240684390068054

Epoch: 6| Step: 4
Training loss: 1.7335050106048584
Validation loss: 2.1319963137308755

Epoch: 6| Step: 5
Training loss: 2.0045430660247803
Validation loss: 2.1322301626205444

Epoch: 6| Step: 6
Training loss: 1.9477317333221436
Validation loss: 2.1226993203163147

Epoch: 6| Step: 7
Training loss: 2.088963031768799
Validation loss: 2.131019651889801

Epoch: 6| Step: 8
Training loss: 1.5734546184539795
Validation loss: 2.1337348024050393

Epoch: 6| Step: 9
Training loss: 2.5348434448242188
Validation loss: 2.1605761845906577

Epoch: 6| Step: 10
Training loss: 2.604426145553589
Validation loss: 2.139551877975464

Epoch: 6| Step: 11
Training loss: 1.9106471538543701
Validation loss: 2.129527529080709

Epoch: 6| Step: 12
Training loss: 1.5612874031066895
Validation loss: 2.1148130098978677

Epoch: 6| Step: 13
Training loss: 1.3609371185302734
Validation loss: 2.1219571034113565

Epoch: 180| Step: 0
Training loss: 2.1248745918273926
Validation loss: 2.1195403337478638

Epoch: 6| Step: 1
Training loss: 2.1748194694519043
Validation loss: 2.1280715465545654

Epoch: 6| Step: 2
Training loss: 1.5361077785491943
Validation loss: 2.1415555079778037

Epoch: 6| Step: 3
Training loss: 1.8202687501907349
Validation loss: 2.1203332344690957

Epoch: 6| Step: 4
Training loss: 1.9014956951141357
Validation loss: 2.134099801381429

Epoch: 6| Step: 5
Training loss: 1.6224830150604248
Validation loss: 2.134597738583883

Epoch: 6| Step: 6
Training loss: 2.643894910812378
Validation loss: 2.12540872891744

Epoch: 6| Step: 7
Training loss: 1.9532067775726318
Validation loss: 2.158689240614573

Epoch: 6| Step: 8
Training loss: 2.0852603912353516
Validation loss: 2.1383197704950967

Epoch: 6| Step: 9
Training loss: 1.875881314277649
Validation loss: 2.127842287222544

Epoch: 6| Step: 10
Training loss: 1.6493594646453857
Validation loss: 2.12263947725296

Epoch: 6| Step: 11
Training loss: 1.2018802165985107
Validation loss: 2.139703353246053

Epoch: 6| Step: 12
Training loss: 1.9576019048690796
Validation loss: 2.144449234008789

Epoch: 6| Step: 13
Training loss: 1.8219400644302368
Validation loss: 2.1315114895502725

Epoch: 181| Step: 0
Training loss: 1.569677472114563
Validation loss: 2.156611601511637

Epoch: 6| Step: 1
Training loss: 2.2517266273498535
Validation loss: 2.1525806983311973

Epoch: 6| Step: 2
Training loss: 1.9626312255859375
Validation loss: 2.1705233454704285

Epoch: 6| Step: 3
Training loss: 1.9716928005218506
Validation loss: 2.1595072746276855

Epoch: 6| Step: 4
Training loss: 1.145397663116455
Validation loss: 2.1727065245310464

Epoch: 6| Step: 5
Training loss: 1.9447684288024902
Validation loss: 2.16091779867808

Epoch: 6| Step: 6
Training loss: 1.621868371963501
Validation loss: 2.158493240674337

Epoch: 6| Step: 7
Training loss: 1.805452585220337
Validation loss: 2.1594623923301697

Epoch: 6| Step: 8
Training loss: 2.3673439025878906
Validation loss: 2.1410157879193625

Epoch: 6| Step: 9
Training loss: 2.15110182762146
Validation loss: 2.1433820724487305

Epoch: 6| Step: 10
Training loss: 2.2315306663513184
Validation loss: 2.13311634461085

Epoch: 6| Step: 11
Training loss: 1.3728108406066895
Validation loss: 2.1266035040219626

Epoch: 6| Step: 12
Training loss: 1.5869193077087402
Validation loss: 2.122503419717153

Epoch: 6| Step: 13
Training loss: 2.6018917560577393
Validation loss: 2.1099870204925537

Epoch: 182| Step: 0
Training loss: 1.1978719234466553
Validation loss: 2.110496679941813

Epoch: 6| Step: 1
Training loss: 1.7873793840408325
Validation loss: 2.114255726337433

Epoch: 6| Step: 2
Training loss: 2.437852382659912
Validation loss: 2.1145646572113037

Epoch: 6| Step: 3
Training loss: 1.939129114151001
Validation loss: 2.128001570701599

Epoch: 6| Step: 4
Training loss: 1.9978032112121582
Validation loss: 2.140976925690969

Epoch: 6| Step: 5
Training loss: 1.897763967514038
Validation loss: 2.1624051332473755

Epoch: 6| Step: 6
Training loss: 2.410517692565918
Validation loss: 2.154712955156962

Epoch: 6| Step: 7
Training loss: 2.070843458175659
Validation loss: 2.1705751419067383

Epoch: 6| Step: 8
Training loss: 1.8442580699920654
Validation loss: 2.1573259035746255

Epoch: 6| Step: 9
Training loss: 1.8712913990020752
Validation loss: 2.151522437731425

Epoch: 6| Step: 10
Training loss: 1.857065200805664
Validation loss: 2.157022714614868

Epoch: 6| Step: 11
Training loss: 1.7344865798950195
Validation loss: 2.148616890112559

Epoch: 6| Step: 12
Training loss: 1.654961109161377
Validation loss: 2.1579660375912986

Epoch: 6| Step: 13
Training loss: 1.957071304321289
Validation loss: 2.1378045678138733

Epoch: 183| Step: 0
Training loss: 2.019622802734375
Validation loss: 2.1222925980885825

Epoch: 6| Step: 1
Training loss: 2.1023969650268555
Validation loss: 2.121964613596598

Epoch: 6| Step: 2
Training loss: 1.8547475337982178
Validation loss: 2.1168761054674783

Epoch: 6| Step: 3
Training loss: 1.715682029724121
Validation loss: 2.1177766919136047

Epoch: 6| Step: 4
Training loss: 1.6506785154342651
Validation loss: 2.123278240362803

Epoch: 6| Step: 5
Training loss: 1.8052024841308594
Validation loss: 2.114128291606903

Epoch: 6| Step: 6
Training loss: 2.145169734954834
Validation loss: 2.1158682107925415

Epoch: 6| Step: 7
Training loss: 1.6285560131072998
Validation loss: 2.126406272252401

Epoch: 6| Step: 8
Training loss: 2.0065340995788574
Validation loss: 2.1158921917279563

Epoch: 6| Step: 9
Training loss: 1.7947278022766113
Validation loss: 2.1066763003667197

Epoch: 6| Step: 10
Training loss: 2.087801456451416
Validation loss: 2.110469480355581

Epoch: 6| Step: 11
Training loss: 2.4984071254730225
Validation loss: 2.1075122356414795

Epoch: 6| Step: 12
Training loss: 1.6060993671417236
Validation loss: 2.1036936044692993

Epoch: 6| Step: 13
Training loss: 1.8148036003112793
Validation loss: 2.106011470158895

Epoch: 184| Step: 0
Training loss: 1.526146650314331
Validation loss: 2.1083030303319297

Epoch: 6| Step: 1
Training loss: 1.9752352237701416
Validation loss: 2.111917515595754

Epoch: 6| Step: 2
Training loss: 2.141730785369873
Validation loss: 2.108242710431417

Epoch: 6| Step: 3
Training loss: 2.0502142906188965
Validation loss: 2.118915339310964

Epoch: 6| Step: 4
Training loss: 1.649646520614624
Validation loss: 2.0922511418660483

Epoch: 6| Step: 5
Training loss: 1.4186738729476929
Validation loss: 2.1128262082735696

Epoch: 6| Step: 6
Training loss: 1.989794373512268
Validation loss: 2.1098767121632895

Epoch: 6| Step: 7
Training loss: 1.9209967851638794
Validation loss: 2.128913402557373

Epoch: 6| Step: 8
Training loss: 2.2544355392456055
Validation loss: 2.1099655826886496

Epoch: 6| Step: 9
Training loss: 3.2648909091949463
Validation loss: 2.120623846848806

Epoch: 6| Step: 10
Training loss: 1.46168053150177
Validation loss: 2.125612258911133

Epoch: 6| Step: 11
Training loss: 1.6076631546020508
Validation loss: 2.1439671516418457

Epoch: 6| Step: 12
Training loss: 2.1214709281921387
Validation loss: 2.1445550123850503

Epoch: 6| Step: 13
Training loss: 1.2383531332015991
Validation loss: 2.15826823314031

Epoch: 185| Step: 0
Training loss: 1.8552333116531372
Validation loss: 2.1459492842356362

Epoch: 6| Step: 1
Training loss: 1.8073413372039795
Validation loss: 2.1502681374549866

Epoch: 6| Step: 2
Training loss: 2.1814751625061035
Validation loss: 2.1468301018079123

Epoch: 6| Step: 3
Training loss: 1.8281733989715576
Validation loss: 2.1408541599909463

Epoch: 6| Step: 4
Training loss: 1.9433194398880005
Validation loss: 2.131664276123047

Epoch: 6| Step: 5
Training loss: 2.6727490425109863
Validation loss: 2.1295358339945474

Epoch: 6| Step: 6
Training loss: 1.4660756587982178
Validation loss: 2.1374169985453286

Epoch: 6| Step: 7
Training loss: 1.960336446762085
Validation loss: 2.1266077359517417

Epoch: 6| Step: 8
Training loss: 2.2468299865722656
Validation loss: 2.1213428576787314

Epoch: 6| Step: 9
Training loss: 1.652380108833313
Validation loss: 2.117134133974711

Epoch: 6| Step: 10
Training loss: 0.9940466284751892
Validation loss: 2.141814112663269

Epoch: 6| Step: 11
Training loss: 2.062227964401245
Validation loss: 2.1291533708572388

Epoch: 6| Step: 12
Training loss: 1.6335749626159668
Validation loss: 2.143677016099294

Epoch: 6| Step: 13
Training loss: 2.3526365756988525
Validation loss: 2.1502680579821267

Epoch: 186| Step: 0
Training loss: 2.0775771141052246
Validation loss: 2.1411943634351096

Epoch: 6| Step: 1
Training loss: 1.9192514419555664
Validation loss: 2.1474912563959756

Epoch: 6| Step: 2
Training loss: 1.392033338546753
Validation loss: 2.160759468873342

Epoch: 6| Step: 3
Training loss: 1.271834373474121
Validation loss: 2.1577223936716714

Epoch: 6| Step: 4
Training loss: 1.899681806564331
Validation loss: 2.1455001632372537

Epoch: 6| Step: 5
Training loss: 2.2605702877044678
Validation loss: 2.1686577200889587

Epoch: 6| Step: 6
Training loss: 2.644261360168457
Validation loss: 2.1532075007756553

Epoch: 6| Step: 7
Training loss: 1.2105979919433594
Validation loss: 2.153091629346212

Epoch: 6| Step: 8
Training loss: 1.8318867683410645
Validation loss: 2.1556074221928916

Epoch: 6| Step: 9
Training loss: 1.684722661972046
Validation loss: 2.1567514141400657

Epoch: 6| Step: 10
Training loss: 1.7793818712234497
Validation loss: 2.1676766077677407

Epoch: 6| Step: 11
Training loss: 2.585930347442627
Validation loss: 2.1330862243970237

Epoch: 6| Step: 12
Training loss: 1.5695478916168213
Validation loss: 2.1445587873458862

Epoch: 6| Step: 13
Training loss: 2.0040769577026367
Validation loss: 2.1383195519447327

Epoch: 187| Step: 0
Training loss: 1.478452444076538
Validation loss: 2.1370848019917807

Epoch: 6| Step: 1
Training loss: 1.8258954286575317
Validation loss: 2.15109384059906

Epoch: 6| Step: 2
Training loss: 1.181833028793335
Validation loss: 2.1467057267824807

Epoch: 6| Step: 3
Training loss: 2.2529830932617188
Validation loss: 2.138464073340098

Epoch: 6| Step: 4
Training loss: 1.742527961730957
Validation loss: 2.1534562508265176

Epoch: 6| Step: 5
Training loss: 2.126396656036377
Validation loss: 2.1575393279393515

Epoch: 6| Step: 6
Training loss: 2.0406198501586914
Validation loss: 2.158932169278463

Epoch: 6| Step: 7
Training loss: 2.2028470039367676
Validation loss: 2.155837972958883

Epoch: 6| Step: 8
Training loss: 1.750156283378601
Validation loss: 2.1414479414621987

Epoch: 6| Step: 9
Training loss: 1.5744452476501465
Validation loss: 2.139734447002411

Epoch: 6| Step: 10
Training loss: 2.361100435256958
Validation loss: 2.150574187437693

Epoch: 6| Step: 11
Training loss: 2.0537426471710205
Validation loss: 2.1368672847747803

Epoch: 6| Step: 12
Training loss: 2.242279052734375
Validation loss: 2.1320972045262656

Epoch: 6| Step: 13
Training loss: 1.3434529304504395
Validation loss: 2.1354029178619385

Epoch: 188| Step: 0
Training loss: 2.751352310180664
Validation loss: 2.1289143164952598

Epoch: 6| Step: 1
Training loss: 1.7719037532806396
Validation loss: 2.13589616616567

Epoch: 6| Step: 2
Training loss: 1.8437658548355103
Validation loss: 2.152131895224253

Epoch: 6| Step: 3
Training loss: 1.5592029094696045
Validation loss: 2.145868102709452

Epoch: 6| Step: 4
Training loss: 1.6855443716049194
Validation loss: 2.1511452992757163

Epoch: 6| Step: 5
Training loss: 1.9079949855804443
Validation loss: 2.14338481426239

Epoch: 6| Step: 6
Training loss: 1.2075318098068237
Validation loss: 2.1256364583969116

Epoch: 6| Step: 7
Training loss: 1.8732997179031372
Validation loss: 2.15448127190272

Epoch: 6| Step: 8
Training loss: 2.4336345195770264
Validation loss: 2.1619029442469277

Epoch: 6| Step: 9
Training loss: 1.5880745649337769
Validation loss: 2.168743391831716

Epoch: 6| Step: 10
Training loss: 1.998201608657837
Validation loss: 2.1677729884783425

Epoch: 6| Step: 11
Training loss: 2.073827028274536
Validation loss: 2.1460129022598267

Epoch: 6| Step: 12
Training loss: 1.872552752494812
Validation loss: 2.156069258848826

Epoch: 6| Step: 13
Training loss: 1.8979220390319824
Validation loss: 2.1619481643040976

Epoch: 189| Step: 0
Training loss: 1.5546338558197021
Validation loss: 2.1339797576268515

Epoch: 6| Step: 1
Training loss: 1.7806047201156616
Validation loss: 2.13060869773229

Epoch: 6| Step: 2
Training loss: 1.8630096912384033
Validation loss: 2.1297740737597146

Epoch: 6| Step: 3
Training loss: 1.894491195678711
Validation loss: 2.1311209996541343

Epoch: 6| Step: 4
Training loss: 1.9564579725265503
Validation loss: 2.1301371852556863

Epoch: 6| Step: 5
Training loss: 1.8119995594024658
Validation loss: 2.1357812682787576

Epoch: 6| Step: 6
Training loss: 2.0184130668640137
Validation loss: 2.1267175475756326

Epoch: 6| Step: 7
Training loss: 1.4584678411483765
Validation loss: 2.130517065525055

Epoch: 6| Step: 8
Training loss: 1.8899000883102417
Validation loss: 2.1474340756734214

Epoch: 6| Step: 9
Training loss: 1.885939598083496
Validation loss: 2.152384877204895

Epoch: 6| Step: 10
Training loss: 2.47756028175354
Validation loss: 2.1521029273668923

Epoch: 6| Step: 11
Training loss: 1.9944891929626465
Validation loss: 2.169306457042694

Epoch: 6| Step: 12
Training loss: 1.854499101638794
Validation loss: 2.155318776766459

Epoch: 6| Step: 13
Training loss: 1.6307255029678345
Validation loss: 2.138648053010305

Epoch: 190| Step: 0
Training loss: 1.7309825420379639
Validation loss: 2.127917249997457

Epoch: 6| Step: 1
Training loss: 1.9984304904937744
Validation loss: 2.137025992075602

Epoch: 6| Step: 2
Training loss: 1.7719097137451172
Validation loss: 2.145788868268331

Epoch: 6| Step: 3
Training loss: 1.8759716749191284
Validation loss: 2.138106405735016

Epoch: 6| Step: 4
Training loss: 1.9980885982513428
Validation loss: 2.1433494687080383

Epoch: 6| Step: 5
Training loss: 1.9105405807495117
Validation loss: 2.13465150197347

Epoch: 6| Step: 6
Training loss: 1.8928983211517334
Validation loss: 2.152289847532908

Epoch: 6| Step: 7
Training loss: 1.6595282554626465
Validation loss: 2.157396992047628

Epoch: 6| Step: 8
Training loss: 1.5678216218948364
Validation loss: 2.1464932560920715

Epoch: 6| Step: 9
Training loss: 1.694977045059204
Validation loss: 2.146148443222046

Epoch: 6| Step: 10
Training loss: 2.4925856590270996
Validation loss: 2.1585957606633506

Epoch: 6| Step: 11
Training loss: 1.4477839469909668
Validation loss: 2.158601959546407

Epoch: 6| Step: 12
Training loss: 2.217911720275879
Validation loss: 2.1501504381497702

Epoch: 6| Step: 13
Training loss: 1.5629868507385254
Validation loss: 2.1649257938067117

Epoch: 191| Step: 0
Training loss: 2.0377659797668457
Validation loss: 2.152856489022573

Epoch: 6| Step: 1
Training loss: 1.8976134061813354
Validation loss: 2.143123288949331

Epoch: 6| Step: 2
Training loss: 2.4327592849731445
Validation loss: 2.1336709459622702

Epoch: 6| Step: 3
Training loss: 1.5451586246490479
Validation loss: 2.1367090145746865

Epoch: 6| Step: 4
Training loss: 2.1050772666931152
Validation loss: 2.1305011908213296

Epoch: 6| Step: 5
Training loss: 1.9247088432312012
Validation loss: 2.1262104312578836

Epoch: 6| Step: 6
Training loss: 1.67544686794281
Validation loss: 2.1333078940709433

Epoch: 6| Step: 7
Training loss: 1.2507023811340332
Validation loss: 2.111523389816284

Epoch: 6| Step: 8
Training loss: 2.3754701614379883
Validation loss: 2.113562822341919

Epoch: 6| Step: 9
Training loss: 2.365788459777832
Validation loss: 2.1115134755770364

Epoch: 6| Step: 10
Training loss: 1.6856756210327148
Validation loss: 2.1335177222887673

Epoch: 6| Step: 11
Training loss: 1.949594259262085
Validation loss: 2.136134684085846

Epoch: 6| Step: 12
Training loss: 1.5531009435653687
Validation loss: 2.171248435974121

Epoch: 6| Step: 13
Training loss: 1.5740981101989746
Validation loss: 2.177078207333883

Epoch: 192| Step: 0
Training loss: 2.1706323623657227
Validation loss: 2.2089502414067588

Epoch: 6| Step: 1
Training loss: 2.313347578048706
Validation loss: 2.2128525972366333

Epoch: 6| Step: 2
Training loss: 2.115553617477417
Validation loss: 2.219777762889862

Epoch: 6| Step: 3
Training loss: 1.7936242818832397
Validation loss: 2.211085240046183

Epoch: 6| Step: 4
Training loss: 1.6584185361862183
Validation loss: 2.20151424407959

Epoch: 6| Step: 5
Training loss: 1.6591622829437256
Validation loss: 2.1923072735468545

Epoch: 6| Step: 6
Training loss: 1.243584394454956
Validation loss: 2.176818609237671

Epoch: 6| Step: 7
Training loss: 1.7799898386001587
Validation loss: 2.1568647623062134

Epoch: 6| Step: 8
Training loss: 1.9587080478668213
Validation loss: 2.1136614084243774

Epoch: 6| Step: 9
Training loss: 2.2348461151123047
Validation loss: 2.1011714736620584

Epoch: 6| Step: 10
Training loss: 2.137132406234741
Validation loss: 2.114358425140381

Epoch: 6| Step: 11
Training loss: 2.402655601501465
Validation loss: 2.128467579682668

Epoch: 6| Step: 12
Training loss: 2.0007381439208984
Validation loss: 2.1183677117029824

Epoch: 6| Step: 13
Training loss: 2.1038818359375
Validation loss: 2.123061935106913

Epoch: 193| Step: 0
Training loss: 2.022975444793701
Validation loss: 2.1167399883270264

Epoch: 6| Step: 1
Training loss: 2.3455758094787598
Validation loss: 2.12125958998998

Epoch: 6| Step: 2
Training loss: 1.9270331859588623
Validation loss: 2.111126641432444

Epoch: 6| Step: 3
Training loss: 1.8406275510787964
Validation loss: 2.1108092069625854

Epoch: 6| Step: 4
Training loss: 1.5506808757781982
Validation loss: 2.097465475400289

Epoch: 6| Step: 5
Training loss: 1.54875910282135
Validation loss: 2.122981866200765

Epoch: 6| Step: 6
Training loss: 1.8675787448883057
Validation loss: 2.11250901222229

Epoch: 6| Step: 7
Training loss: 1.5920284986495972
Validation loss: 2.1061535477638245

Epoch: 6| Step: 8
Training loss: 1.5643515586853027
Validation loss: 2.128707687060038

Epoch: 6| Step: 9
Training loss: 2.2637484073638916
Validation loss: 2.1382760802904763

Epoch: 6| Step: 10
Training loss: 2.8119029998779297
Validation loss: 2.164516488711039

Epoch: 6| Step: 11
Training loss: 2.5886144638061523
Validation loss: 2.1721914807955423

Epoch: 6| Step: 12
Training loss: 2.138427257537842
Validation loss: 2.192708214124044

Epoch: 6| Step: 13
Training loss: 2.075991630554199
Validation loss: 2.1990896463394165

Epoch: 194| Step: 0
Training loss: 1.782456398010254
Validation loss: 2.185224493344625

Epoch: 6| Step: 1
Training loss: 2.7553467750549316
Validation loss: 2.1957683761914573

Epoch: 6| Step: 2
Training loss: 1.412548303604126
Validation loss: 2.191417853037516

Epoch: 6| Step: 3
Training loss: 1.841562032699585
Validation loss: 2.1692209442456565

Epoch: 6| Step: 4
Training loss: 2.053976058959961
Validation loss: 2.139234205087026

Epoch: 6| Step: 5
Training loss: 1.875958800315857
Validation loss: 2.1043036381403604

Epoch: 6| Step: 6
Training loss: 1.9474482536315918
Validation loss: 2.1090981364250183

Epoch: 6| Step: 7
Training loss: 2.233400583267212
Validation loss: 2.0937151511510215

Epoch: 6| Step: 8
Training loss: 2.154052495956421
Validation loss: 2.0941930611928306

Epoch: 6| Step: 9
Training loss: 1.5007476806640625
Validation loss: 2.112536132335663

Epoch: 6| Step: 10
Training loss: 2.2637829780578613
Validation loss: 2.0998353163401284

Epoch: 6| Step: 11
Training loss: 2.1826183795928955
Validation loss: 2.108982026576996

Epoch: 6| Step: 12
Training loss: 1.4657835960388184
Validation loss: 2.102204918861389

Epoch: 6| Step: 13
Training loss: 1.9226312637329102
Validation loss: 2.122004449367523

Epoch: 195| Step: 0
Training loss: 1.5302317142486572
Validation loss: 2.120154539744059

Epoch: 6| Step: 1
Training loss: 2.187528610229492
Validation loss: 2.142200450102488

Epoch: 6| Step: 2
Training loss: 2.154325008392334
Validation loss: 2.1458065509796143

Epoch: 6| Step: 3
Training loss: 1.51784348487854
Validation loss: 2.1493009328842163

Epoch: 6| Step: 4
Training loss: 1.1392738819122314
Validation loss: 2.153930981953939

Epoch: 6| Step: 5
Training loss: 1.8770861625671387
Validation loss: 2.1475735306739807

Epoch: 6| Step: 6
Training loss: 1.6335619688034058
Validation loss: 2.1582237680753074

Epoch: 6| Step: 7
Training loss: 2.3712096214294434
Validation loss: 2.17074716091156

Epoch: 6| Step: 8
Training loss: 1.9785901308059692
Validation loss: 2.170232633749644

Epoch: 6| Step: 9
Training loss: 1.5730090141296387
Validation loss: 2.18628066778183

Epoch: 6| Step: 10
Training loss: 2.3440260887145996
Validation loss: 2.19321217139562

Epoch: 6| Step: 11
Training loss: 1.6534218788146973
Validation loss: 2.1942028403282166

Epoch: 6| Step: 12
Training loss: 1.6210501194000244
Validation loss: 2.170977075894674

Epoch: 6| Step: 13
Training loss: 2.452737331390381
Validation loss: 2.151820739110311

Epoch: 196| Step: 0
Training loss: 1.9432299137115479
Validation loss: 2.166463335355123

Epoch: 6| Step: 1
Training loss: 1.5853831768035889
Validation loss: 2.161606192588806

Epoch: 6| Step: 2
Training loss: 1.067528486251831
Validation loss: 2.156446854273478

Epoch: 6| Step: 3
Training loss: 1.6560440063476562
Validation loss: 2.145966569582621

Epoch: 6| Step: 4
Training loss: 2.1532082557678223
Validation loss: 2.1408867835998535

Epoch: 6| Step: 5
Training loss: 2.0145697593688965
Validation loss: 2.1406496365865073

Epoch: 6| Step: 6
Training loss: 1.616389274597168
Validation loss: 2.175524870554606

Epoch: 6| Step: 7
Training loss: 1.7864596843719482
Validation loss: 2.164412577946981

Epoch: 6| Step: 8
Training loss: 2.115088939666748
Validation loss: 2.169374644756317

Epoch: 6| Step: 9
Training loss: 1.8311623334884644
Validation loss: 2.1587825218836465

Epoch: 6| Step: 10
Training loss: 2.0234947204589844
Validation loss: 2.165677309036255

Epoch: 6| Step: 11
Training loss: 1.902505874633789
Validation loss: 2.161726097265879

Epoch: 6| Step: 12
Training loss: 1.8282732963562012
Validation loss: 2.1667543252309165

Epoch: 6| Step: 13
Training loss: 2.2755656242370605
Validation loss: 2.156959295272827

Epoch: 197| Step: 0
Training loss: 1.958825945854187
Validation loss: 2.1749685804049173

Epoch: 6| Step: 1
Training loss: 2.012982130050659
Validation loss: 2.1886366407076516

Epoch: 6| Step: 2
Training loss: 1.2307119369506836
Validation loss: 2.167439083258311

Epoch: 6| Step: 3
Training loss: 1.7130178213119507
Validation loss: 2.16514124472936

Epoch: 6| Step: 4
Training loss: 1.7384377717971802
Validation loss: 2.157920996348063

Epoch: 6| Step: 5
Training loss: 1.7805366516113281
Validation loss: 2.159465948740641

Epoch: 6| Step: 6
Training loss: 2.014355182647705
Validation loss: 2.1510832707087197

Epoch: 6| Step: 7
Training loss: 1.8723628520965576
Validation loss: 2.1572991212209067

Epoch: 6| Step: 8
Training loss: 2.2072699069976807
Validation loss: 2.1623054146766663

Epoch: 6| Step: 9
Training loss: 2.1328437328338623
Validation loss: 2.152586877346039

Epoch: 6| Step: 10
Training loss: 2.3848304748535156
Validation loss: 2.153032581011454

Epoch: 6| Step: 11
Training loss: 1.431534767150879
Validation loss: 2.153348743915558

Epoch: 6| Step: 12
Training loss: 1.663491129875183
Validation loss: 2.1699211994806924

Epoch: 6| Step: 13
Training loss: 1.618902564048767
Validation loss: 2.185420791308085

Epoch: 198| Step: 0
Training loss: 1.8623321056365967
Validation loss: 2.186781664689382

Epoch: 6| Step: 1
Training loss: 2.1826272010803223
Validation loss: 2.1864200234413147

Epoch: 6| Step: 2
Training loss: 1.5348765850067139
Validation loss: 2.1939148704210916

Epoch: 6| Step: 3
Training loss: 1.589050054550171
Validation loss: 2.1854336063067117

Epoch: 6| Step: 4
Training loss: 2.189532518386841
Validation loss: 2.133491257826487

Epoch: 6| Step: 5
Training loss: 1.6531199216842651
Validation loss: 2.1465779542922974

Epoch: 6| Step: 6
Training loss: 1.7740914821624756
Validation loss: 2.1434656977653503

Epoch: 6| Step: 7
Training loss: 1.5765142440795898
Validation loss: 2.1558252771695456

Epoch: 6| Step: 8
Training loss: 2.3741471767425537
Validation loss: 2.1449935038884482

Epoch: 6| Step: 9
Training loss: 1.3825020790100098
Validation loss: 2.152388354142507

Epoch: 6| Step: 10
Training loss: 1.6308481693267822
Validation loss: 2.1520396868387857

Epoch: 6| Step: 11
Training loss: 2.3248450756073
Validation loss: 2.1705993016560874

Epoch: 6| Step: 12
Training loss: 1.8136868476867676
Validation loss: 2.156165281931559

Epoch: 6| Step: 13
Training loss: 2.2930541038513184
Validation loss: 2.1429028312365213

Epoch: 199| Step: 0
Training loss: 2.0482630729675293
Validation loss: 2.1615564227104187

Epoch: 6| Step: 1
Training loss: 2.506786823272705
Validation loss: 2.154494027296702

Epoch: 6| Step: 2
Training loss: 1.5927643775939941
Validation loss: 2.158792813618978

Epoch: 6| Step: 3
Training loss: 1.3269245624542236
Validation loss: 2.161341985066732

Epoch: 6| Step: 4
Training loss: 1.299917459487915
Validation loss: 2.1500568191210427

Epoch: 6| Step: 5
Training loss: 1.5508430004119873
Validation loss: 2.1459490855534873

Epoch: 6| Step: 6
Training loss: 2.2821598052978516
Validation loss: 2.178420821825663

Epoch: 6| Step: 7
Training loss: 1.7484413385391235
Validation loss: 2.169416844844818

Epoch: 6| Step: 8
Training loss: 2.045879364013672
Validation loss: 2.1871901750564575

Epoch: 6| Step: 9
Training loss: 1.9936926364898682
Validation loss: 2.1736255884170532

Epoch: 6| Step: 10
Training loss: 1.7258336544036865
Validation loss: 2.1749281883239746

Epoch: 6| Step: 11
Training loss: 1.3595874309539795
Validation loss: 2.1654250820477805

Epoch: 6| Step: 12
Training loss: 2.0173864364624023
Validation loss: 2.1438106695810952

Epoch: 6| Step: 13
Training loss: 2.2014758586883545
Validation loss: 2.1539138356844583

Epoch: 200| Step: 0
Training loss: 2.804295063018799
Validation loss: 2.1757147312164307

Epoch: 6| Step: 1
Training loss: 1.649141550064087
Validation loss: 2.150933086872101

Epoch: 6| Step: 2
Training loss: 1.7004945278167725
Validation loss: 2.1527841488520303

Epoch: 6| Step: 3
Training loss: 1.8113679885864258
Validation loss: 2.1530904372533164

Epoch: 6| Step: 4
Training loss: 2.5746889114379883
Validation loss: 2.1555617252985635

Epoch: 6| Step: 5
Training loss: 1.511352300643921
Validation loss: 2.156107564767202

Epoch: 6| Step: 6
Training loss: 1.1795886754989624
Validation loss: 2.1578624645868936

Epoch: 6| Step: 7
Training loss: 1.4974806308746338
Validation loss: 2.156754473845164

Epoch: 6| Step: 8
Training loss: 1.8535525798797607
Validation loss: 2.149959623813629

Epoch: 6| Step: 9
Training loss: 1.8631432056427002
Validation loss: 2.174378514289856

Epoch: 6| Step: 10
Training loss: 1.6320596933364868
Validation loss: 2.172032276789347

Epoch: 6| Step: 11
Training loss: 2.040278911590576
Validation loss: 2.1745334466298423

Epoch: 6| Step: 12
Training loss: 1.5863760709762573
Validation loss: 2.1787498195966086

Epoch: 6| Step: 13
Training loss: 2.129171848297119
Validation loss: 2.1862350702285767

Epoch: 201| Step: 0
Training loss: 1.838413119316101
Validation loss: 2.170406182607015

Epoch: 6| Step: 1
Training loss: 1.7305574417114258
Validation loss: 2.169281860192617

Epoch: 6| Step: 2
Training loss: 1.7582459449768066
Validation loss: 2.1560540397961936

Epoch: 6| Step: 3
Training loss: 1.7953770160675049
Validation loss: 2.1555620233217874

Epoch: 6| Step: 4
Training loss: 1.578650951385498
Validation loss: 2.1494306524594626

Epoch: 6| Step: 5
Training loss: 1.935238003730774
Validation loss: 2.139057755470276

Epoch: 6| Step: 6
Training loss: 1.7647310495376587
Validation loss: 2.1194524566332498

Epoch: 6| Step: 7
Training loss: 1.1779685020446777
Validation loss: 2.135646323362986

Epoch: 6| Step: 8
Training loss: 1.968512773513794
Validation loss: 2.158108870188395

Epoch: 6| Step: 9
Training loss: 1.4302533864974976
Validation loss: 2.1393284598986306

Epoch: 6| Step: 10
Training loss: 2.2869954109191895
Validation loss: 2.154967248439789

Epoch: 6| Step: 11
Training loss: 2.2683355808258057
Validation loss: 2.142049789428711

Epoch: 6| Step: 12
Training loss: 2.2969117164611816
Validation loss: 2.147014021873474

Epoch: 6| Step: 13
Training loss: 1.8861634731292725
Validation loss: 2.1571134328842163

Epoch: 202| Step: 0
Training loss: 1.8975555896759033
Validation loss: 2.180274566014608

Epoch: 6| Step: 1
Training loss: 1.8156423568725586
Validation loss: 2.191080331802368

Epoch: 6| Step: 2
Training loss: 2.2158455848693848
Validation loss: 2.1601155201594033

Epoch: 6| Step: 3
Training loss: 1.789754033088684
Validation loss: 2.171981851259867

Epoch: 6| Step: 4
Training loss: 1.9979254007339478
Validation loss: 2.162575383981069

Epoch: 6| Step: 5
Training loss: 2.273911714553833
Validation loss: 2.1691924929618835

Epoch: 6| Step: 6
Training loss: 2.4336562156677246
Validation loss: 2.150877137978872

Epoch: 6| Step: 7
Training loss: 1.486112117767334
Validation loss: 2.1510287125905356

Epoch: 6| Step: 8
Training loss: 1.8799412250518799
Validation loss: 2.1661209066708884

Epoch: 6| Step: 9
Training loss: 1.250659465789795
Validation loss: 2.1742546757062278

Epoch: 6| Step: 10
Training loss: 1.681729793548584
Validation loss: 2.1632114251454673

Epoch: 6| Step: 11
Training loss: 1.6262500286102295
Validation loss: 2.164871394634247

Epoch: 6| Step: 12
Training loss: 1.2814041376113892
Validation loss: 2.1890007853507996

Epoch: 6| Step: 13
Training loss: 1.9347357749938965
Validation loss: 2.198236326376597

Epoch: 203| Step: 0
Training loss: 1.7506121397018433
Validation loss: 2.189004143079122

Epoch: 6| Step: 1
Training loss: 2.125657081604004
Validation loss: 2.189431587855021

Epoch: 6| Step: 2
Training loss: 1.553293228149414
Validation loss: 2.180463274319967

Epoch: 6| Step: 3
Training loss: 1.6523864269256592
Validation loss: 2.1826200087865195

Epoch: 6| Step: 4
Training loss: 1.4609287977218628
Validation loss: 2.1605751117070517

Epoch: 6| Step: 5
Training loss: 1.6776453256607056
Validation loss: 2.1547828714052835

Epoch: 6| Step: 6
Training loss: 2.159024238586426
Validation loss: 2.154556632041931

Epoch: 6| Step: 7
Training loss: 2.1665401458740234
Validation loss: 2.147956927617391

Epoch: 6| Step: 8
Training loss: 1.6493178606033325
Validation loss: 2.1289008259773254

Epoch: 6| Step: 9
Training loss: 1.8612477779388428
Validation loss: 2.13138747215271

Epoch: 6| Step: 10
Training loss: 1.8854302167892456
Validation loss: 2.132431666056315

Epoch: 6| Step: 11
Training loss: 2.0531580448150635
Validation loss: 2.1582230925559998

Epoch: 6| Step: 12
Training loss: 1.664209008216858
Validation loss: 2.137877106666565

Epoch: 6| Step: 13
Training loss: 1.960031270980835
Validation loss: 2.1761037508646646

Epoch: 204| Step: 0
Training loss: 1.270551085472107
Validation loss: 2.1474703351656594

Epoch: 6| Step: 1
Training loss: 1.7428820133209229
Validation loss: 2.1615694761276245

Epoch: 6| Step: 2
Training loss: 1.4533812999725342
Validation loss: 2.1507380406061807

Epoch: 6| Step: 3
Training loss: 1.630497932434082
Validation loss: 2.1493459939956665

Epoch: 6| Step: 4
Training loss: 1.757197618484497
Validation loss: 2.1742101311683655

Epoch: 6| Step: 5
Training loss: 2.0986199378967285
Validation loss: 2.1634644865989685

Epoch: 6| Step: 6
Training loss: 2.4557547569274902
Validation loss: 2.145672917366028

Epoch: 6| Step: 7
Training loss: 1.818939208984375
Validation loss: 2.1675458351771035

Epoch: 6| Step: 8
Training loss: 2.3061342239379883
Validation loss: 2.1929054260253906

Epoch: 6| Step: 9
Training loss: 1.9138401746749878
Validation loss: 2.1769653956095376

Epoch: 6| Step: 10
Training loss: 2.392472743988037
Validation loss: 2.218793431917826

Epoch: 6| Step: 11
Training loss: 1.852879524230957
Validation loss: 2.2155328392982483

Epoch: 6| Step: 12
Training loss: 2.173832416534424
Validation loss: 2.199979523817698

Epoch: 6| Step: 13
Training loss: 1.2005012035369873
Validation loss: 2.211525003115336

Epoch: 205| Step: 0
Training loss: 1.8410043716430664
Validation loss: 2.2192347844441733

Epoch: 6| Step: 1
Training loss: 1.9757814407348633
Validation loss: 2.1961949467658997

Epoch: 6| Step: 2
Training loss: 1.5122027397155762
Validation loss: 2.1951544284820557

Epoch: 6| Step: 3
Training loss: 1.6559243202209473
Validation loss: 2.200739006201426

Epoch: 6| Step: 4
Training loss: 2.012140989303589
Validation loss: 2.1673293511072793

Epoch: 6| Step: 5
Training loss: 1.3639363050460815
Validation loss: 2.174322485923767

Epoch: 6| Step: 6
Training loss: 2.527782440185547
Validation loss: 2.184884488582611

Epoch: 6| Step: 7
Training loss: 1.2258388996124268
Validation loss: 2.19671094417572

Epoch: 6| Step: 8
Training loss: 1.784324288368225
Validation loss: 2.1673923333485923

Epoch: 6| Step: 9
Training loss: 1.6453518867492676
Validation loss: 2.1773998936017356

Epoch: 6| Step: 10
Training loss: 2.2095420360565186
Validation loss: 2.153045952320099

Epoch: 6| Step: 11
Training loss: 1.980268120765686
Validation loss: 2.1559105118115744

Epoch: 6| Step: 12
Training loss: 2.0777883529663086
Validation loss: 2.1616626183191934

Epoch: 6| Step: 13
Training loss: 1.8994426727294922
Validation loss: 2.150454024473826

Epoch: 206| Step: 0
Training loss: 2.005953550338745
Validation loss: 2.15931369860967

Epoch: 6| Step: 1
Training loss: 2.185173273086548
Validation loss: 2.183560927708944

Epoch: 6| Step: 2
Training loss: 1.8471229076385498
Validation loss: 2.1910396814346313

Epoch: 6| Step: 3
Training loss: 1.1998951435089111
Validation loss: 2.2114558815956116

Epoch: 6| Step: 4
Training loss: 1.9482030868530273
Validation loss: 2.214484214782715

Epoch: 6| Step: 5
Training loss: 1.7342138290405273
Validation loss: 2.2227384050687156

Epoch: 6| Step: 6
Training loss: 2.4400360584259033
Validation loss: 2.2257364789644876

Epoch: 6| Step: 7
Training loss: 1.2977287769317627
Validation loss: 2.244649966557821

Epoch: 6| Step: 8
Training loss: 1.6442396640777588
Validation loss: 2.223068435986837

Epoch: 6| Step: 9
Training loss: 1.6173111200332642
Validation loss: 2.2199520270029702

Epoch: 6| Step: 10
Training loss: 2.0913290977478027
Validation loss: 2.203753332297007

Epoch: 6| Step: 11
Training loss: 1.7604783773422241
Validation loss: 2.197246491909027

Epoch: 6| Step: 12
Training loss: 1.651111125946045
Validation loss: 2.173331320285797

Epoch: 6| Step: 13
Training loss: 2.207460880279541
Validation loss: 2.1745226780573526

Epoch: 207| Step: 0
Training loss: 2.234722852706909
Validation loss: 2.174226184686025

Epoch: 6| Step: 1
Training loss: 1.7478106021881104
Validation loss: 2.1551842292149863

Epoch: 6| Step: 2
Training loss: 2.212855577468872
Validation loss: 2.1567781368891397

Epoch: 6| Step: 3
Training loss: 1.2028748989105225
Validation loss: 2.1522743503252664

Epoch: 6| Step: 4
Training loss: 1.6140413284301758
Validation loss: 2.138697644074758

Epoch: 6| Step: 5
Training loss: 1.7322051525115967
Validation loss: 2.1579918265342712

Epoch: 6| Step: 6
Training loss: 1.1271922588348389
Validation loss: 2.1728201707204184

Epoch: 6| Step: 7
Training loss: 2.142758846282959
Validation loss: 2.189399838447571

Epoch: 6| Step: 8
Training loss: 2.082584857940674
Validation loss: 2.1853201389312744

Epoch: 6| Step: 9
Training loss: 2.0021910667419434
Validation loss: 2.200827976067861

Epoch: 6| Step: 10
Training loss: 1.576000690460205
Validation loss: 2.192029118537903

Epoch: 6| Step: 11
Training loss: 2.829305648803711
Validation loss: 2.1973147988319397

Epoch: 6| Step: 12
Training loss: 1.4757051467895508
Validation loss: 2.1858986218770347

Epoch: 6| Step: 13
Training loss: 1.3480966091156006
Validation loss: 2.1922481060028076

Epoch: 208| Step: 0
Training loss: 1.5667190551757812
Validation loss: 2.1699032386144004

Epoch: 6| Step: 1
Training loss: 1.4293097257614136
Validation loss: 2.1792959769566855

Epoch: 6| Step: 2
Training loss: 1.9456425905227661
Validation loss: 2.1859336296717324

Epoch: 6| Step: 3
Training loss: 1.8418378829956055
Validation loss: 2.198652466138204

Epoch: 6| Step: 4
Training loss: 2.168318271636963
Validation loss: 2.198088606198629

Epoch: 6| Step: 5
Training loss: 1.9808217287063599
Validation loss: 2.20477032661438

Epoch: 6| Step: 6
Training loss: 1.6584928035736084
Validation loss: 2.186880906422933

Epoch: 6| Step: 7
Training loss: 1.422844648361206
Validation loss: 2.195091942946116

Epoch: 6| Step: 8
Training loss: 1.6019623279571533
Validation loss: 2.162024219830831

Epoch: 6| Step: 9
Training loss: 1.7407639026641846
Validation loss: 2.1667702992757163

Epoch: 6| Step: 10
Training loss: 1.6930038928985596
Validation loss: 2.191192626953125

Epoch: 6| Step: 11
Training loss: 2.9831063747406006
Validation loss: 2.179046332836151

Epoch: 6| Step: 12
Training loss: 1.691169023513794
Validation loss: 2.1464382211367288

Epoch: 6| Step: 13
Training loss: 1.458416223526001
Validation loss: 2.1546011368433633

Epoch: 209| Step: 0
Training loss: 2.0485949516296387
Validation loss: 2.1492453614870706

Epoch: 6| Step: 1
Training loss: 1.7656927108764648
Validation loss: 2.1579744815826416

Epoch: 6| Step: 2
Training loss: 1.6553765535354614
Validation loss: 2.139118234316508

Epoch: 6| Step: 3
Training loss: 2.1025550365448
Validation loss: 2.1571027437845864

Epoch: 6| Step: 4
Training loss: 1.4357357025146484
Validation loss: 2.133979578812917

Epoch: 6| Step: 5
Training loss: 1.5167584419250488
Validation loss: 2.1403844555219016

Epoch: 6| Step: 6
Training loss: 1.6237674951553345
Validation loss: 2.1548816363016763

Epoch: 6| Step: 7
Training loss: 2.3249456882476807
Validation loss: 2.158335566520691

Epoch: 6| Step: 8
Training loss: 1.3641133308410645
Validation loss: 2.169719715913137

Epoch: 6| Step: 9
Training loss: 2.0150439739227295
Validation loss: 2.1908161441485086

Epoch: 6| Step: 10
Training loss: 1.4444093704223633
Validation loss: 2.203436811765035

Epoch: 6| Step: 11
Training loss: 2.123858690261841
Validation loss: 2.1994882424672446

Epoch: 6| Step: 12
Training loss: 1.730982780456543
Validation loss: 2.2400945822397866

Epoch: 6| Step: 13
Training loss: 2.1814768314361572
Validation loss: 2.22883007923762

Epoch: 210| Step: 0
Training loss: 1.5510189533233643
Validation loss: 2.203350087006887

Epoch: 6| Step: 1
Training loss: 2.8202908039093018
Validation loss: 2.195948918660482

Epoch: 6| Step: 2
Training loss: 1.9273368120193481
Validation loss: 2.184388021628062

Epoch: 6| Step: 3
Training loss: 1.9138330221176147
Validation loss: 2.173429807027181

Epoch: 6| Step: 4
Training loss: 2.7023544311523438
Validation loss: 2.16345485051473

Epoch: 6| Step: 5
Training loss: 1.5429065227508545
Validation loss: 2.150262991587321

Epoch: 6| Step: 6
Training loss: 1.369864821434021
Validation loss: 2.147987345854441

Epoch: 6| Step: 7
Training loss: 1.6946548223495483
Validation loss: 2.135126233100891

Epoch: 6| Step: 8
Training loss: 1.5025138854980469
Validation loss: 2.1314541697502136

Epoch: 6| Step: 9
Training loss: 1.683100938796997
Validation loss: 2.1307984987894693

Epoch: 6| Step: 10
Training loss: 1.5257277488708496
Validation loss: 2.1323782006899514

Epoch: 6| Step: 11
Training loss: 1.354668378829956
Validation loss: 2.128615458806356

Epoch: 6| Step: 12
Training loss: 2.454986572265625
Validation loss: 2.130253871281942

Epoch: 6| Step: 13
Training loss: 1.9637609720230103
Validation loss: 2.1258321007092795

Epoch: 211| Step: 0
Training loss: 1.520097255706787
Validation loss: 2.126963754494985

Epoch: 6| Step: 1
Training loss: 2.9718093872070312
Validation loss: 2.1163261930147805

Epoch: 6| Step: 2
Training loss: 1.7453978061676025
Validation loss: 2.13436230023702

Epoch: 6| Step: 3
Training loss: 2.4346938133239746
Validation loss: 2.136288583278656

Epoch: 6| Step: 4
Training loss: 1.2113044261932373
Validation loss: 2.1529568235079446

Epoch: 6| Step: 5
Training loss: 1.6127681732177734
Validation loss: 2.151661515235901

Epoch: 6| Step: 6
Training loss: 1.883011817932129
Validation loss: 2.190978487332662

Epoch: 6| Step: 7
Training loss: 1.3907890319824219
Validation loss: 2.1831167936325073

Epoch: 6| Step: 8
Training loss: 1.8078196048736572
Validation loss: 2.199295918146769

Epoch: 6| Step: 9
Training loss: 2.1546061038970947
Validation loss: 2.1827056407928467

Epoch: 6| Step: 10
Training loss: 1.508386254310608
Validation loss: 2.187976141770681

Epoch: 6| Step: 11
Training loss: 2.37123966217041
Validation loss: 2.189392864704132

Epoch: 6| Step: 12
Training loss: 1.4270895719528198
Validation loss: 2.1801772912343345

Epoch: 6| Step: 13
Training loss: 1.2587413787841797
Validation loss: 2.1576525370279946

Epoch: 212| Step: 0
Training loss: 2.42006516456604
Validation loss: 2.1694111426671348

Epoch: 6| Step: 1
Training loss: 1.8536467552185059
Validation loss: 2.1810115973154702

Epoch: 6| Step: 2
Training loss: 2.1494622230529785
Validation loss: 2.159097750981649

Epoch: 6| Step: 3
Training loss: 1.9601905345916748
Validation loss: 2.1815701723098755

Epoch: 6| Step: 4
Training loss: 1.3128056526184082
Validation loss: 2.1797532637914023

Epoch: 6| Step: 5
Training loss: 1.7987321615219116
Validation loss: 2.198925733566284

Epoch: 6| Step: 6
Training loss: 1.552362084388733
Validation loss: 2.1916557351748147

Epoch: 6| Step: 7
Training loss: 1.6065183877944946
Validation loss: 2.1972620685895285

Epoch: 6| Step: 8
Training loss: 1.6920452117919922
Validation loss: 2.2181480328241983

Epoch: 6| Step: 9
Training loss: 1.924107313156128
Validation loss: 2.2097358107566833

Epoch: 6| Step: 10
Training loss: 1.8662821054458618
Validation loss: 2.1847848097483316

Epoch: 6| Step: 11
Training loss: 1.4031248092651367
Validation loss: 2.1814324657122293

Epoch: 6| Step: 12
Training loss: 1.6080260276794434
Validation loss: 2.1548823714256287

Epoch: 6| Step: 13
Training loss: 1.5942974090576172
Validation loss: 2.174479524294535

Epoch: 213| Step: 0
Training loss: 0.7688233852386475
Validation loss: 2.177396595478058

Epoch: 6| Step: 1
Training loss: 1.4630684852600098
Validation loss: 2.171357492605845

Epoch: 6| Step: 2
Training loss: 1.1949132680892944
Validation loss: 2.169825534025828

Epoch: 6| Step: 3
Training loss: 1.8446967601776123
Validation loss: 2.1805362502733865

Epoch: 6| Step: 4
Training loss: 2.185333728790283
Validation loss: 2.155819376309713

Epoch: 6| Step: 5
Training loss: 1.8402419090270996
Validation loss: 2.1593074798583984

Epoch: 6| Step: 6
Training loss: 2.646277666091919
Validation loss: 2.1527344981829324

Epoch: 6| Step: 7
Training loss: 1.7864329814910889
Validation loss: 2.1608070135116577

Epoch: 6| Step: 8
Training loss: 1.6185461282730103
Validation loss: 2.142419219017029

Epoch: 6| Step: 9
Training loss: 2.042117118835449
Validation loss: 2.161460558573405

Epoch: 6| Step: 10
Training loss: 1.2823069095611572
Validation loss: 2.187986373901367

Epoch: 6| Step: 11
Training loss: 1.9049254655838013
Validation loss: 2.1679442723592124

Epoch: 6| Step: 12
Training loss: 2.7586445808410645
Validation loss: 2.18382598956426

Epoch: 6| Step: 13
Training loss: 1.6186378002166748
Validation loss: 2.1941813031832376

Epoch: 214| Step: 0
Training loss: 1.939971685409546
Validation loss: 2.1969717343648276

Epoch: 6| Step: 1
Training loss: 1.0145208835601807
Validation loss: 2.2057215770085654

Epoch: 6| Step: 2
Training loss: 1.6396005153656006
Validation loss: 2.2121294736862183

Epoch: 6| Step: 3
Training loss: 2.0987677574157715
Validation loss: 2.2058019439379373

Epoch: 6| Step: 4
Training loss: 1.771745204925537
Validation loss: 2.1870563427607217

Epoch: 6| Step: 5
Training loss: 2.5974411964416504
Validation loss: 2.188868522644043

Epoch: 6| Step: 6
Training loss: 1.7875494956970215
Validation loss: 2.2173189520835876

Epoch: 6| Step: 7
Training loss: 2.510129690170288
Validation loss: 2.18504935503006

Epoch: 6| Step: 8
Training loss: 1.510879635810852
Validation loss: 2.1727590759595237

Epoch: 6| Step: 9
Training loss: 2.023867130279541
Validation loss: 2.185001313686371

Epoch: 6| Step: 10
Training loss: 1.8749785423278809
Validation loss: 2.1796629826227822

Epoch: 6| Step: 11
Training loss: 1.3947927951812744
Validation loss: 2.1615313291549683

Epoch: 6| Step: 12
Training loss: 1.2234740257263184
Validation loss: 2.182970960934957

Epoch: 6| Step: 13
Training loss: 1.7057075500488281
Validation loss: 2.1978426774342856

Epoch: 215| Step: 0
Training loss: 1.9895055294036865
Validation loss: 2.197356363137563

Epoch: 6| Step: 1
Training loss: 2.061018705368042
Validation loss: 2.183784226576487

Epoch: 6| Step: 2
Training loss: 2.3510711193084717
Validation loss: 2.181499640146891

Epoch: 6| Step: 3
Training loss: 2.333555221557617
Validation loss: 2.1577630639076233

Epoch: 6| Step: 4
Training loss: 1.2701783180236816
Validation loss: 2.1660768389701843

Epoch: 6| Step: 5
Training loss: 2.1443185806274414
Validation loss: 2.176308731238047

Epoch: 6| Step: 6
Training loss: 1.6576576232910156
Validation loss: 2.157401700814565

Epoch: 6| Step: 7
Training loss: 1.3549859523773193
Validation loss: 2.1783722639083862

Epoch: 6| Step: 8
Training loss: 1.9867570400238037
Validation loss: 2.190959870815277

Epoch: 6| Step: 9
Training loss: 1.3970102071762085
Validation loss: 2.1727449099222818

Epoch: 6| Step: 10
Training loss: 1.1502121686935425
Validation loss: 2.1808239420255027

Epoch: 6| Step: 11
Training loss: 1.7269444465637207
Validation loss: 2.1824376781781516

Epoch: 6| Step: 12
Training loss: 1.5555732250213623
Validation loss: 2.2020424604415894

Epoch: 6| Step: 13
Training loss: 1.6044697761535645
Validation loss: 2.194752871990204

Epoch: 216| Step: 0
Training loss: 1.8798682689666748
Validation loss: 2.1932466228803

Epoch: 6| Step: 1
Training loss: 1.5635132789611816
Validation loss: 2.194586217403412

Epoch: 6| Step: 2
Training loss: 1.2045061588287354
Validation loss: 2.2064111034075418

Epoch: 6| Step: 3
Training loss: 2.0191152095794678
Validation loss: 2.181756873925527

Epoch: 6| Step: 4
Training loss: 2.004166603088379
Validation loss: 2.198901037375132

Epoch: 6| Step: 5
Training loss: 1.2926313877105713
Validation loss: 2.2048088113466897

Epoch: 6| Step: 6
Training loss: 1.1242390871047974
Validation loss: 2.178519368171692

Epoch: 6| Step: 7
Training loss: 2.124873161315918
Validation loss: 2.17791481812795

Epoch: 6| Step: 8
Training loss: 1.3219735622406006
Validation loss: 2.1761178572972617

Epoch: 6| Step: 9
Training loss: 1.850748896598816
Validation loss: 2.1613778471946716

Epoch: 6| Step: 10
Training loss: 1.9034218788146973
Validation loss: 2.1545574069023132

Epoch: 6| Step: 11
Training loss: 1.8110556602478027
Validation loss: 2.1611610054969788

Epoch: 6| Step: 12
Training loss: 2.222513198852539
Validation loss: 2.15543794631958

Epoch: 6| Step: 13
Training loss: 2.370020866394043
Validation loss: 2.1716277798016868

Epoch: 217| Step: 0
Training loss: 1.6675238609313965
Validation loss: 2.185990591843923

Epoch: 6| Step: 1
Training loss: 1.609950065612793
Validation loss: 2.2107344468434653

Epoch: 6| Step: 2
Training loss: 2.028550386428833
Validation loss: 2.23680979013443

Epoch: 6| Step: 3
Training loss: 2.3507440090179443
Validation loss: 2.227451205253601

Epoch: 6| Step: 4
Training loss: 1.6325476169586182
Validation loss: 2.247504413127899

Epoch: 6| Step: 5
Training loss: 1.4702677726745605
Validation loss: 2.253952999909719

Epoch: 6| Step: 6
Training loss: 1.9697370529174805
Validation loss: 2.2461289167404175

Epoch: 6| Step: 7
Training loss: 2.20070481300354
Validation loss: 2.213408907254537

Epoch: 6| Step: 8
Training loss: 1.6490981578826904
Validation loss: 2.224881112575531

Epoch: 6| Step: 9
Training loss: 1.3850038051605225
Validation loss: 2.2026668787002563

Epoch: 6| Step: 10
Training loss: 2.152714967727661
Validation loss: 2.184017598628998

Epoch: 6| Step: 11
Training loss: 2.353081226348877
Validation loss: 2.1722296675046286

Epoch: 6| Step: 12
Training loss: 1.2919349670410156
Validation loss: 2.146354913711548

Epoch: 6| Step: 13
Training loss: 1.6866559982299805
Validation loss: 2.1497400800387063

Epoch: 218| Step: 0
Training loss: 2.284773349761963
Validation loss: 2.140920559565226

Epoch: 6| Step: 1
Training loss: 1.583784580230713
Validation loss: 2.1343420346577964

Epoch: 6| Step: 2
Training loss: 2.307666063308716
Validation loss: 2.1370228926340737

Epoch: 6| Step: 3
Training loss: 1.9156889915466309
Validation loss: 2.148096283276876

Epoch: 6| Step: 4
Training loss: 1.6046197414398193
Validation loss: 2.1348745624224343

Epoch: 6| Step: 5
Training loss: 1.4830570220947266
Validation loss: 2.1596973141034446

Epoch: 6| Step: 6
Training loss: 1.6718878746032715
Validation loss: 2.1578997572263083

Epoch: 6| Step: 7
Training loss: 2.153158664703369
Validation loss: 2.1376825968424478

Epoch: 6| Step: 8
Training loss: 2.053363800048828
Validation loss: 2.1561977863311768

Epoch: 6| Step: 9
Training loss: 2.76552152633667
Validation loss: 2.1536797682444253

Epoch: 6| Step: 10
Training loss: 2.3499350547790527
Validation loss: 2.1483720938364663

Epoch: 6| Step: 11
Training loss: 1.393538475036621
Validation loss: 2.1506062150001526

Epoch: 6| Step: 12
Training loss: 1.1699596643447876
Validation loss: 2.144789715607961

Epoch: 6| Step: 13
Training loss: 1.6185507774353027
Validation loss: 2.1674309571584067

Epoch: 219| Step: 0
Training loss: 1.5754776000976562
Validation loss: 2.1458441615104675

Epoch: 6| Step: 1
Training loss: 1.6126065254211426
Validation loss: 2.16886568069458

Epoch: 6| Step: 2
Training loss: 1.7601081132888794
Validation loss: 2.1727015177408853

Epoch: 6| Step: 3
Training loss: 1.7542997598648071
Validation loss: 2.1652876933415732

Epoch: 6| Step: 4
Training loss: 1.5494580268859863
Validation loss: 2.1606974005699158

Epoch: 6| Step: 5
Training loss: 1.2011709213256836
Validation loss: 2.167060116926829

Epoch: 6| Step: 6
Training loss: 2.665972948074341
Validation loss: 2.186367630958557

Epoch: 6| Step: 7
Training loss: 2.0701141357421875
Validation loss: 2.184291362762451

Epoch: 6| Step: 8
Training loss: 1.9340753555297852
Validation loss: 2.1657298803329468

Epoch: 6| Step: 9
Training loss: 1.1951987743377686
Validation loss: 2.171923518180847

Epoch: 6| Step: 10
Training loss: 2.027858018875122
Validation loss: 2.1488842566808066

Epoch: 6| Step: 11
Training loss: 1.9415969848632812
Validation loss: 2.168434500694275

Epoch: 6| Step: 12
Training loss: 1.6307953596115112
Validation loss: 2.1522333224614463

Epoch: 6| Step: 13
Training loss: 2.09317946434021
Validation loss: 2.1691492001215615

Epoch: 220| Step: 0
Training loss: 1.8830115795135498
Validation loss: 2.160405913988749

Epoch: 6| Step: 1
Training loss: 1.6097204685211182
Validation loss: 2.14746085802714

Epoch: 6| Step: 2
Training loss: 1.5661675930023193
Validation loss: 2.160185714562734

Epoch: 6| Step: 3
Training loss: 1.5101399421691895
Validation loss: 2.148696502049764

Epoch: 6| Step: 4
Training loss: 2.314099073410034
Validation loss: 2.1538721124331155

Epoch: 6| Step: 5
Training loss: 1.6892063617706299
Validation loss: 2.1437492966651917

Epoch: 6| Step: 6
Training loss: 2.634700298309326
Validation loss: 2.15834774573644

Epoch: 6| Step: 7
Training loss: 1.496110200881958
Validation loss: 2.1469660003980002

Epoch: 6| Step: 8
Training loss: 1.378065586090088
Validation loss: 2.166320284207662

Epoch: 6| Step: 9
Training loss: 1.8613606691360474
Validation loss: 2.16379843155543

Epoch: 6| Step: 10
Training loss: 1.5489529371261597
Validation loss: 2.182297706604004

Epoch: 6| Step: 11
Training loss: 1.6664068698883057
Validation loss: 2.170317530632019

Epoch: 6| Step: 12
Training loss: 1.717162847518921
Validation loss: 2.166467567284902

Epoch: 6| Step: 13
Training loss: 1.9473809003829956
Validation loss: 2.1736571192741394

Epoch: 221| Step: 0
Training loss: 2.493116855621338
Validation loss: 2.2105495929718018

Epoch: 6| Step: 1
Training loss: 1.8178672790527344
Validation loss: 2.210269292195638

Epoch: 6| Step: 2
Training loss: 2.112781524658203
Validation loss: 2.200431148211161

Epoch: 6| Step: 3
Training loss: 1.3923394680023193
Validation loss: 2.2161522706349692

Epoch: 6| Step: 4
Training loss: 1.2168716192245483
Validation loss: 2.226321796576182

Epoch: 6| Step: 5
Training loss: 1.8820981979370117
Validation loss: 2.215993821620941

Epoch: 6| Step: 6
Training loss: 1.8717533349990845
Validation loss: 2.1924018065134683

Epoch: 6| Step: 7
Training loss: 1.954829454421997
Validation loss: 2.2007435162862143

Epoch: 6| Step: 8
Training loss: 1.5089809894561768
Validation loss: 2.1917069951693215

Epoch: 6| Step: 9
Training loss: 1.8350772857666016
Validation loss: 2.1866374611854553

Epoch: 6| Step: 10
Training loss: 1.8137649297714233
Validation loss: 2.174086312452952

Epoch: 6| Step: 11
Training loss: 1.3407618999481201
Validation loss: 2.1816854079564414

Epoch: 6| Step: 12
Training loss: 1.899073600769043
Validation loss: 2.1499741872151694

Epoch: 6| Step: 13
Training loss: 1.8966294527053833
Validation loss: 2.1619449257850647

Epoch: 222| Step: 0
Training loss: 1.9290409088134766
Validation loss: 2.1284266908963523

Epoch: 6| Step: 1
Training loss: 1.430032730102539
Validation loss: 2.122530162334442

Epoch: 6| Step: 2
Training loss: 1.7482386827468872
Validation loss: 2.1380895177523294

Epoch: 6| Step: 3
Training loss: 1.9250065088272095
Validation loss: 2.1348761320114136

Epoch: 6| Step: 4
Training loss: 2.47788667678833
Validation loss: 2.1564273039499917

Epoch: 6| Step: 5
Training loss: 2.222860813140869
Validation loss: 2.148076852162679

Epoch: 6| Step: 6
Training loss: 1.4570763111114502
Validation loss: 2.1707345247268677

Epoch: 6| Step: 7
Training loss: 2.4202470779418945
Validation loss: 2.2126612464586892

Epoch: 6| Step: 8
Training loss: 2.0006103515625
Validation loss: 2.2216223080952964

Epoch: 6| Step: 9
Training loss: 1.4574623107910156
Validation loss: 2.2108588417371116

Epoch: 6| Step: 10
Training loss: 1.2642874717712402
Validation loss: 2.2148995399475098

Epoch: 6| Step: 11
Training loss: 1.470200538635254
Validation loss: 2.2332361141840615

Epoch: 6| Step: 12
Training loss: 1.5242060422897339
Validation loss: 2.2426599264144897

Epoch: 6| Step: 13
Training loss: 2.226321220397949
Validation loss: 2.232420881589254

Epoch: 223| Step: 0
Training loss: 1.893871784210205
Validation loss: 2.215060551961263

Epoch: 6| Step: 1
Training loss: 2.135681629180908
Validation loss: 2.1989532510439553

Epoch: 6| Step: 2
Training loss: 1.7104612588882446
Validation loss: 2.1798980832099915

Epoch: 6| Step: 3
Training loss: 1.8921037912368774
Validation loss: 2.159478763739268

Epoch: 6| Step: 4
Training loss: 1.230325698852539
Validation loss: 2.173976500829061

Epoch: 6| Step: 5
Training loss: 1.4987930059432983
Validation loss: 2.160446365674337

Epoch: 6| Step: 6
Training loss: 1.0276587009429932
Validation loss: 2.138224482536316

Epoch: 6| Step: 7
Training loss: 2.1055850982666016
Validation loss: 2.1386952002843223

Epoch: 6| Step: 8
Training loss: 2.3380184173583984
Validation loss: 2.1545759638150535

Epoch: 6| Step: 9
Training loss: 1.731583595275879
Validation loss: 2.148426592350006

Epoch: 6| Step: 10
Training loss: 2.073986053466797
Validation loss: 2.152227501074473

Epoch: 6| Step: 11
Training loss: 1.80975341796875
Validation loss: 2.171237667401632

Epoch: 6| Step: 12
Training loss: 2.0376029014587402
Validation loss: 2.1627334356307983

Epoch: 6| Step: 13
Training loss: 2.1295595169067383
Validation loss: 2.153792361418406

Epoch: 224| Step: 0
Training loss: 1.5588475465774536
Validation loss: 2.164485454559326

Epoch: 6| Step: 1
Training loss: 1.7211408615112305
Validation loss: 2.1649091045061746

Epoch: 6| Step: 2
Training loss: 2.089104175567627
Validation loss: 2.1837424635887146

Epoch: 6| Step: 3
Training loss: 2.0605998039245605
Validation loss: 2.2071303327878318

Epoch: 6| Step: 4
Training loss: 1.8076716661453247
Validation loss: 2.197413126627604

Epoch: 6| Step: 5
Training loss: 1.4264483451843262
Validation loss: 2.203369359175364

Epoch: 6| Step: 6
Training loss: 1.5260570049285889
Validation loss: 2.199933707714081

Epoch: 6| Step: 7
Training loss: 1.9848144054412842
Validation loss: 2.185223162174225

Epoch: 6| Step: 8
Training loss: 1.3564558029174805
Validation loss: 2.1678995490074158

Epoch: 6| Step: 9
Training loss: 1.9597285985946655
Validation loss: 2.1884869734446206

Epoch: 6| Step: 10
Training loss: 1.78696870803833
Validation loss: 2.1757903893788657

Epoch: 6| Step: 11
Training loss: 2.3038454055786133
Validation loss: 2.1738011240959167

Epoch: 6| Step: 12
Training loss: 1.687431812286377
Validation loss: 2.1888547539711

Epoch: 6| Step: 13
Training loss: 1.7988369464874268
Validation loss: 2.1686618526776633

Epoch: 225| Step: 0
Training loss: 1.9966285228729248
Validation loss: 2.1352041761080423

Epoch: 6| Step: 1
Training loss: 1.7438750267028809
Validation loss: 2.1486913363138833

Epoch: 6| Step: 2
Training loss: 1.6568514108657837
Validation loss: 2.1685909032821655

Epoch: 6| Step: 3
Training loss: 1.7454118728637695
Validation loss: 2.162950058778127

Epoch: 6| Step: 4
Training loss: 1.4864306449890137
Validation loss: 2.1568246285120645

Epoch: 6| Step: 5
Training loss: 2.3474714756011963
Validation loss: 2.138531188170115

Epoch: 6| Step: 6
Training loss: 1.6168450117111206
Validation loss: 2.1424275835355124

Epoch: 6| Step: 7
Training loss: 2.479677677154541
Validation loss: 2.1442024310429892

Epoch: 6| Step: 8
Training loss: 1.9175573587417603
Validation loss: 2.1310461163520813

Epoch: 6| Step: 9
Training loss: 1.1085995435714722
Validation loss: 2.149688164393107

Epoch: 6| Step: 10
Training loss: 1.3414719104766846
Validation loss: 2.138340254624685

Epoch: 6| Step: 11
Training loss: 1.7949566841125488
Validation loss: 2.1452340881029763

Epoch: 6| Step: 12
Training loss: 1.7184702157974243
Validation loss: 2.137190500895182

Epoch: 6| Step: 13
Training loss: 1.8197720050811768
Validation loss: 2.171512226263682

Epoch: 226| Step: 0
Training loss: 1.4683079719543457
Validation loss: 2.2131826877593994

Epoch: 6| Step: 1
Training loss: 1.8230564594268799
Validation loss: 2.1963860392570496

Epoch: 6| Step: 2
Training loss: 2.14082670211792
Validation loss: 2.224240005016327

Epoch: 6| Step: 3
Training loss: 1.5103565454483032
Validation loss: 2.212943434715271

Epoch: 6| Step: 4
Training loss: 1.8587579727172852
Validation loss: 2.2428107062975564

Epoch: 6| Step: 5
Training loss: 1.5746185779571533
Validation loss: 2.22710527976354

Epoch: 6| Step: 6
Training loss: 1.8767428398132324
Validation loss: 2.217854380607605

Epoch: 6| Step: 7
Training loss: 0.9038835167884827
Validation loss: 2.2062554160753884

Epoch: 6| Step: 8
Training loss: 2.113572597503662
Validation loss: 2.168334702650706

Epoch: 6| Step: 9
Training loss: 1.7137587070465088
Validation loss: 2.1570497949918113

Epoch: 6| Step: 10
Training loss: 2.6455483436584473
Validation loss: 2.149489164352417

Epoch: 6| Step: 11
Training loss: 1.6187071800231934
Validation loss: 2.127518892288208

Epoch: 6| Step: 12
Training loss: 1.882164716720581
Validation loss: 2.1294487714767456

Epoch: 6| Step: 13
Training loss: 1.9080333709716797
Validation loss: 2.1366033355394998

Epoch: 227| Step: 0
Training loss: 1.4278879165649414
Validation loss: 2.135660727818807

Epoch: 6| Step: 1
Training loss: 2.0745911598205566
Validation loss: 2.1303795178731284

Epoch: 6| Step: 2
Training loss: 2.39615797996521
Validation loss: 2.15290900071462

Epoch: 6| Step: 3
Training loss: 1.685244083404541
Validation loss: 2.158206661542257

Epoch: 6| Step: 4
Training loss: 2.0174918174743652
Validation loss: 2.167711059252421

Epoch: 6| Step: 5
Training loss: 1.3120572566986084
Validation loss: 2.1923427184422812

Epoch: 6| Step: 6
Training loss: 1.1049079895019531
Validation loss: 2.171967645486196

Epoch: 6| Step: 7
Training loss: 2.3629143238067627
Validation loss: 2.1795079509417215

Epoch: 6| Step: 8
Training loss: 1.7202444076538086
Validation loss: 2.1765804290771484

Epoch: 6| Step: 9
Training loss: 1.489927053451538
Validation loss: 2.1835519075393677

Epoch: 6| Step: 10
Training loss: 2.3948729038238525
Validation loss: 2.1730974515279136

Epoch: 6| Step: 11
Training loss: 1.836155891418457
Validation loss: 2.1758421460787454

Epoch: 6| Step: 12
Training loss: 1.556825876235962
Validation loss: 2.160406450430552

Epoch: 6| Step: 13
Training loss: 1.3493236303329468
Validation loss: 2.176512857278188

Epoch: 228| Step: 0
Training loss: 2.1540536880493164
Validation loss: 2.1693944533665976

Epoch: 6| Step: 1
Training loss: 1.6063265800476074
Validation loss: 2.1682609717051187

Epoch: 6| Step: 2
Training loss: 1.382532000541687
Validation loss: 2.165708382924398

Epoch: 6| Step: 3
Training loss: 1.9401648044586182
Validation loss: 2.181497553984324

Epoch: 6| Step: 4
Training loss: 1.8141741752624512
Validation loss: 2.177747885386149

Epoch: 6| Step: 5
Training loss: 1.6714286804199219
Validation loss: 2.1564775109291077

Epoch: 6| Step: 6
Training loss: 2.058225154876709
Validation loss: 2.1323074301083884

Epoch: 6| Step: 7
Training loss: 1.729870319366455
Validation loss: 2.13758651415507

Epoch: 6| Step: 8
Training loss: 2.757267475128174
Validation loss: 2.130238632361094

Epoch: 6| Step: 9
Training loss: 1.2461038827896118
Validation loss: 2.1424376567204795

Epoch: 6| Step: 10
Training loss: 1.3839054107666016
Validation loss: 2.1351914008458457

Epoch: 6| Step: 11
Training loss: 1.693314552307129
Validation loss: 2.1673821608225503

Epoch: 6| Step: 12
Training loss: 1.4879083633422852
Validation loss: 2.181255360444387

Epoch: 6| Step: 13
Training loss: 1.8823673725128174
Validation loss: 2.18279238541921

Epoch: 229| Step: 0
Training loss: 1.9361116886138916
Validation loss: 2.1853334506352744

Epoch: 6| Step: 1
Training loss: 1.4639768600463867
Validation loss: 2.1833957036336265

Epoch: 6| Step: 2
Training loss: 1.60093092918396
Validation loss: 2.2033404310544333

Epoch: 6| Step: 3
Training loss: 1.9222766160964966
Validation loss: 2.1707105239232383

Epoch: 6| Step: 4
Training loss: 1.6703436374664307
Validation loss: 2.1791017850240073

Epoch: 6| Step: 5
Training loss: 1.9225847721099854
Validation loss: 2.171153744061788

Epoch: 6| Step: 6
Training loss: 2.076828956604004
Validation loss: 2.168006102244059

Epoch: 6| Step: 7
Training loss: 2.572265148162842
Validation loss: 2.172519584496816

Epoch: 6| Step: 8
Training loss: 1.0986459255218506
Validation loss: 2.186115324497223

Epoch: 6| Step: 9
Training loss: 1.771376609802246
Validation loss: 2.201809883117676

Epoch: 6| Step: 10
Training loss: 2.026355266571045
Validation loss: 2.211007912953695

Epoch: 6| Step: 11
Training loss: 1.0956366062164307
Validation loss: 2.2034717798233032

Epoch: 6| Step: 12
Training loss: 1.5310122966766357
Validation loss: 2.193282186985016

Epoch: 6| Step: 13
Training loss: 1.614976406097412
Validation loss: 2.193849245707194

Epoch: 230| Step: 0
Training loss: 1.6505643129348755
Validation loss: 2.2134344577789307

Epoch: 6| Step: 1
Training loss: 1.5531721115112305
Validation loss: 2.207275112469991

Epoch: 6| Step: 2
Training loss: 1.3824843168258667
Validation loss: 2.219186703364054

Epoch: 6| Step: 3
Training loss: 1.4738337993621826
Validation loss: 2.250615914662679

Epoch: 6| Step: 4
Training loss: 1.2065849304199219
Validation loss: 2.2328900893529258

Epoch: 6| Step: 5
Training loss: 2.1791915893554688
Validation loss: 2.234952370325724

Epoch: 6| Step: 6
Training loss: 1.6475510597229004
Validation loss: 2.2115935484568277

Epoch: 6| Step: 7
Training loss: 1.7280974388122559
Validation loss: 2.1979381442070007

Epoch: 6| Step: 8
Training loss: 1.3771789073944092
Validation loss: 2.2220046321551004

Epoch: 6| Step: 9
Training loss: 2.1619200706481934
Validation loss: 2.188311676184336

Epoch: 6| Step: 10
Training loss: 2.0877530574798584
Validation loss: 2.1962031722068787

Epoch: 6| Step: 11
Training loss: 2.2520198822021484
Validation loss: 2.1794428626696267

Epoch: 6| Step: 12
Training loss: 1.5294175148010254
Validation loss: 2.1632779439290366

Epoch: 6| Step: 13
Training loss: 1.8521080017089844
Validation loss: 2.146635870138804

Epoch: 231| Step: 0
Training loss: 1.7455308437347412
Validation loss: 2.167725702126821

Epoch: 6| Step: 1
Training loss: 1.3785561323165894
Validation loss: 2.1665772000948587

Epoch: 6| Step: 2
Training loss: 2.269320011138916
Validation loss: 2.1613546212514243

Epoch: 6| Step: 3
Training loss: 1.4928712844848633
Validation loss: 2.1593298514684043

Epoch: 6| Step: 4
Training loss: 1.9442217350006104
Validation loss: 2.177714725335439

Epoch: 6| Step: 5
Training loss: 1.4472286701202393
Validation loss: 2.1669811805089316

Epoch: 6| Step: 6
Training loss: 1.7405412197113037
Validation loss: 2.189004103342692

Epoch: 6| Step: 7
Training loss: 1.4613018035888672
Validation loss: 2.20644881327947

Epoch: 6| Step: 8
Training loss: 1.7779662609100342
Validation loss: 2.2336723804473877

Epoch: 6| Step: 9
Training loss: 1.681028962135315
Validation loss: 2.2316428422927856

Epoch: 6| Step: 10
Training loss: 1.7992483377456665
Validation loss: 2.22429221868515

Epoch: 6| Step: 11
Training loss: 1.8045744895935059
Validation loss: 2.23264741897583

Epoch: 6| Step: 12
Training loss: 1.5782362222671509
Validation loss: 2.2307805816332498

Epoch: 6| Step: 13
Training loss: 2.1904714107513428
Validation loss: 2.194114923477173

Epoch: 232| Step: 0
Training loss: 1.6373724937438965
Validation loss: 2.223126530647278

Epoch: 6| Step: 1
Training loss: 1.5041470527648926
Validation loss: 2.230763594309489

Epoch: 6| Step: 2
Training loss: 1.1902388334274292
Validation loss: 2.2136186957359314

Epoch: 6| Step: 3
Training loss: 1.8979499340057373
Validation loss: 2.2254290779431662

Epoch: 6| Step: 4
Training loss: 1.6686122417449951
Validation loss: 2.203455646832784

Epoch: 6| Step: 5
Training loss: 2.0728607177734375
Validation loss: 2.1909293135007224

Epoch: 6| Step: 6
Training loss: 1.58323073387146
Validation loss: 2.1578519344329834

Epoch: 6| Step: 7
Training loss: 1.8534672260284424
Validation loss: 2.1767802635828652

Epoch: 6| Step: 8
Training loss: 2.1276726722717285
Validation loss: 2.191344459851583

Epoch: 6| Step: 9
Training loss: 1.6345856189727783
Validation loss: 2.186128397782644

Epoch: 6| Step: 10
Training loss: 1.7027637958526611
Validation loss: 2.19220503171285

Epoch: 6| Step: 11
Training loss: 2.004960775375366
Validation loss: 2.2100873986879983

Epoch: 6| Step: 12
Training loss: 1.6976239681243896
Validation loss: 2.214330037434896

Epoch: 6| Step: 13
Training loss: 1.566178321838379
Validation loss: 2.2008336782455444

Epoch: 233| Step: 0
Training loss: 1.5542880296707153
Validation loss: 2.22860457499822

Epoch: 6| Step: 1
Training loss: 1.3668022155761719
Validation loss: 2.211361567179362

Epoch: 6| Step: 2
Training loss: 1.8453540802001953
Validation loss: 2.218966563542684

Epoch: 6| Step: 3
Training loss: 1.3233104944229126
Validation loss: 2.2015382846196494

Epoch: 6| Step: 4
Training loss: 1.4973807334899902
Validation loss: 2.188199539979299

Epoch: 6| Step: 5
Training loss: 2.10493803024292
Validation loss: 2.1801256934801736

Epoch: 6| Step: 6
Training loss: 1.8473763465881348
Validation loss: 2.196191350618998

Epoch: 6| Step: 7
Training loss: 1.589223861694336
Validation loss: 2.1860704024632773

Epoch: 6| Step: 8
Training loss: 1.7783466577529907
Validation loss: 2.174118081728617

Epoch: 6| Step: 9
Training loss: 1.576097846031189
Validation loss: 2.1888906160990396

Epoch: 6| Step: 10
Training loss: 1.8607451915740967
Validation loss: 2.196298619111379

Epoch: 6| Step: 11
Training loss: 2.019871950149536
Validation loss: 2.190042813618978

Epoch: 6| Step: 12
Training loss: 2.1265363693237305
Validation loss: 2.1807066599527993

Epoch: 6| Step: 13
Training loss: 1.7521464824676514
Validation loss: 2.169801712036133

Epoch: 234| Step: 0
Training loss: 1.395486831665039
Validation loss: 2.1863179405530295

Epoch: 6| Step: 1
Training loss: 1.2839127779006958
Validation loss: 2.172963539759318

Epoch: 6| Step: 2
Training loss: 1.4495059251785278
Validation loss: 2.160026947657267

Epoch: 6| Step: 3
Training loss: 1.4239693880081177
Validation loss: 2.166215797265371

Epoch: 6| Step: 4
Training loss: 2.3306570053100586
Validation loss: 2.1491475900014243

Epoch: 6| Step: 5
Training loss: 2.336002826690674
Validation loss: 2.1709555784861245

Epoch: 6| Step: 6
Training loss: 1.6518503427505493
Validation loss: 2.1606043179829917

Epoch: 6| Step: 7
Training loss: 1.5339550971984863
Validation loss: 2.1686519980430603

Epoch: 6| Step: 8
Training loss: 1.8056480884552002
Validation loss: 2.1804192264874778

Epoch: 6| Step: 9
Training loss: 1.516897439956665
Validation loss: 2.2045061190923056

Epoch: 6| Step: 10
Training loss: 1.2761163711547852
Validation loss: 2.1685519218444824

Epoch: 6| Step: 11
Training loss: 2.3125381469726562
Validation loss: 2.186184028784434

Epoch: 6| Step: 12
Training loss: 2.010218858718872
Validation loss: 2.1949784557024636

Epoch: 6| Step: 13
Training loss: 1.7993825674057007
Validation loss: 2.2022849321365356

Epoch: 235| Step: 0
Training loss: 1.6724622249603271
Validation loss: 2.189374625682831

Epoch: 6| Step: 1
Training loss: 1.8988131284713745
Validation loss: 2.1785711447397866

Epoch: 6| Step: 2
Training loss: 2.029484748840332
Validation loss: 2.1673336029052734

Epoch: 6| Step: 3
Training loss: 1.3342721462249756
Validation loss: 2.147636334101359

Epoch: 6| Step: 4
Training loss: 1.4963324069976807
Validation loss: 2.1559210618336997

Epoch: 6| Step: 5
Training loss: 1.6665542125701904
Validation loss: 2.159688154856364

Epoch: 6| Step: 6
Training loss: 1.7788511514663696
Validation loss: 2.1535903811454773

Epoch: 6| Step: 7
Training loss: 1.8269966840744019
Validation loss: 2.1428504586219788

Epoch: 6| Step: 8
Training loss: 2.138446569442749
Validation loss: 2.1528263688087463

Epoch: 6| Step: 9
Training loss: 1.18714439868927
Validation loss: 2.132834275563558

Epoch: 6| Step: 10
Training loss: 1.4908981323242188
Validation loss: 2.1407795945803323

Epoch: 6| Step: 11
Training loss: 1.6418416500091553
Validation loss: 2.156328499317169

Epoch: 6| Step: 12
Training loss: 2.3535566329956055
Validation loss: 2.1468077500661216

Epoch: 6| Step: 13
Training loss: 1.7535879611968994
Validation loss: 2.1456291476885476

Epoch: 236| Step: 0
Training loss: 1.548243522644043
Validation loss: 2.148624360561371

Epoch: 6| Step: 1
Training loss: 1.7525486946105957
Validation loss: 2.151535431543986

Epoch: 6| Step: 2
Training loss: 1.5303288698196411
Validation loss: 2.1644670168558755

Epoch: 6| Step: 3
Training loss: 1.2459930181503296
Validation loss: 2.15485010544459

Epoch: 6| Step: 4
Training loss: 2.3163228034973145
Validation loss: 2.1595500310262046

Epoch: 6| Step: 5
Training loss: 2.3558056354522705
Validation loss: 2.1494586865107217

Epoch: 6| Step: 6
Training loss: 2.1189770698547363
Validation loss: 2.153737187385559

Epoch: 6| Step: 7
Training loss: 1.52158784866333
Validation loss: 2.1620121002197266

Epoch: 6| Step: 8
Training loss: 1.3678468465805054
Validation loss: 2.158291459083557

Epoch: 6| Step: 9
Training loss: 1.735529899597168
Validation loss: 2.1616087555885315

Epoch: 6| Step: 10
Training loss: 1.6430330276489258
Validation loss: 2.1460838119188943

Epoch: 6| Step: 11
Training loss: 1.5662745237350464
Validation loss: 2.14632378021876

Epoch: 6| Step: 12
Training loss: 1.3849701881408691
Validation loss: 2.13589608669281

Epoch: 6| Step: 13
Training loss: 2.5043091773986816
Validation loss: 2.1330171624819436

Epoch: 237| Step: 0
Training loss: 1.812035322189331
Validation loss: 2.1450765331586203

Epoch: 6| Step: 1
Training loss: 1.9313496351242065
Validation loss: 2.16165820757548

Epoch: 6| Step: 2
Training loss: 1.560755968093872
Validation loss: 2.1640561620394387

Epoch: 6| Step: 3
Training loss: 1.572096824645996
Validation loss: 2.132711867491404

Epoch: 6| Step: 4
Training loss: 2.054826259613037
Validation loss: 2.177745799223582

Epoch: 6| Step: 5
Training loss: 1.6691532135009766
Validation loss: 2.1572033961613974

Epoch: 6| Step: 6
Training loss: 2.54964017868042
Validation loss: 2.1701790491739907

Epoch: 6| Step: 7
Training loss: 1.7083450555801392
Validation loss: 2.170392394065857

Epoch: 6| Step: 8
Training loss: 1.5552256107330322
Validation loss: 2.1424557169278464

Epoch: 6| Step: 9
Training loss: 1.5476503372192383
Validation loss: 2.1929683287938437

Epoch: 6| Step: 10
Training loss: 1.9368863105773926
Validation loss: 2.1668770909309387

Epoch: 6| Step: 11
Training loss: 1.4483532905578613
Validation loss: 2.172605554262797

Epoch: 6| Step: 12
Training loss: 1.7449463605880737
Validation loss: 2.1680197715759277

Epoch: 6| Step: 13
Training loss: 1.4195619821548462
Validation loss: 2.1637662649154663

Epoch: 238| Step: 0
Training loss: 1.2848231792449951
Validation loss: 2.169231196244558

Epoch: 6| Step: 1
Training loss: 1.6103910207748413
Validation loss: 2.1466349959373474

Epoch: 6| Step: 2
Training loss: 1.4763410091400146
Validation loss: 2.1409666736920676

Epoch: 6| Step: 3
Training loss: 2.369260787963867
Validation loss: 2.150306781133016

Epoch: 6| Step: 4
Training loss: 1.9933369159698486
Validation loss: 2.1592173178990683

Epoch: 6| Step: 5
Training loss: 2.153306007385254
Validation loss: 2.1498943169911704

Epoch: 6| Step: 6
Training loss: 2.0066943168640137
Validation loss: 2.1668219566345215

Epoch: 6| Step: 7
Training loss: 1.5314671993255615
Validation loss: 2.150312344233195

Epoch: 6| Step: 8
Training loss: 1.1959816217422485
Validation loss: 2.1641637682914734

Epoch: 6| Step: 9
Training loss: 1.909228801727295
Validation loss: 2.169725239276886

Epoch: 6| Step: 10
Training loss: 1.8475674390792847
Validation loss: 2.1887173652648926

Epoch: 6| Step: 11
Training loss: 2.2151923179626465
Validation loss: 2.1977906624476113

Epoch: 6| Step: 12
Training loss: 1.3765013217926025
Validation loss: 2.2084266543388367

Epoch: 6| Step: 13
Training loss: 1.8714910745620728
Validation loss: 2.218907574812571

Epoch: 239| Step: 0
Training loss: 2.0250442028045654
Validation loss: 2.23073277870814

Epoch: 6| Step: 1
Training loss: 1.4219012260437012
Validation loss: 2.223125378290812

Epoch: 6| Step: 2
Training loss: 1.622056245803833
Validation loss: 2.221200923124949

Epoch: 6| Step: 3
Training loss: 1.9821763038635254
Validation loss: 2.227654675642649

Epoch: 6| Step: 4
Training loss: 1.5022697448730469
Validation loss: 2.226237416267395

Epoch: 6| Step: 5
Training loss: 1.615898847579956
Validation loss: 2.193424423535665

Epoch: 6| Step: 6
Training loss: 2.4245879650115967
Validation loss: 2.199464122454325

Epoch: 6| Step: 7
Training loss: 1.5373833179473877
Validation loss: 2.187773128350576

Epoch: 6| Step: 8
Training loss: 1.3731358051300049
Validation loss: 2.1750794450441995

Epoch: 6| Step: 9
Training loss: 2.272170305252075
Validation loss: 2.189712663491567

Epoch: 6| Step: 10
Training loss: 1.496116280555725
Validation loss: 2.1574000914891562

Epoch: 6| Step: 11
Training loss: 1.259181022644043
Validation loss: 2.1524571577707925

Epoch: 6| Step: 12
Training loss: 1.7304863929748535
Validation loss: 2.155640959739685

Epoch: 6| Step: 13
Training loss: 1.779686689376831
Validation loss: 2.152305563290914

Epoch: 240| Step: 0
Training loss: 2.480999231338501
Validation loss: 2.177522341410319

Epoch: 6| Step: 1
Training loss: 1.9134434461593628
Validation loss: 2.1786239544550576

Epoch: 6| Step: 2
Training loss: 1.7702275514602661
Validation loss: 2.1955562829971313

Epoch: 6| Step: 3
Training loss: 1.5205345153808594
Validation loss: 2.217976172765096

Epoch: 6| Step: 4
Training loss: 2.181121349334717
Validation loss: 2.2040689984957376

Epoch: 6| Step: 5
Training loss: 1.2902498245239258
Validation loss: 2.2249239683151245

Epoch: 6| Step: 6
Training loss: 1.2164522409439087
Validation loss: 2.2327977418899536

Epoch: 6| Step: 7
Training loss: 1.5517855882644653
Validation loss: 2.2219881812731423

Epoch: 6| Step: 8
Training loss: 1.3074800968170166
Validation loss: 2.245941241582235

Epoch: 6| Step: 9
Training loss: 1.5981194972991943
Validation loss: 2.1959760785102844

Epoch: 6| Step: 10
Training loss: 1.6394368410110474
Validation loss: 2.1876773834228516

Epoch: 6| Step: 11
Training loss: 1.365572214126587
Validation loss: 2.1878151098887124

Epoch: 6| Step: 12
Training loss: 2.040860652923584
Validation loss: 2.1798706650733948

Epoch: 6| Step: 13
Training loss: 2.0033061504364014
Validation loss: 2.1837578614552817

Epoch: 241| Step: 0
Training loss: 1.7203432321548462
Validation loss: 2.1833478609720864

Epoch: 6| Step: 1
Training loss: 1.166492223739624
Validation loss: 2.188704570134481

Epoch: 6| Step: 2
Training loss: 2.7899136543273926
Validation loss: 2.2044054865837097

Epoch: 6| Step: 3
Training loss: 1.4144079685211182
Validation loss: 2.1770927906036377

Epoch: 6| Step: 4
Training loss: 2.0472936630249023
Validation loss: 2.190471053123474

Epoch: 6| Step: 5
Training loss: 1.541429042816162
Validation loss: 2.2158363858858743

Epoch: 6| Step: 6
Training loss: 1.6134753227233887
Validation loss: 2.214695155620575

Epoch: 6| Step: 7
Training loss: 1.203379511833191
Validation loss: 2.209596852461497

Epoch: 6| Step: 8
Training loss: 1.5245773792266846
Validation loss: 2.1975859999656677

Epoch: 6| Step: 9
Training loss: 1.7254009246826172
Validation loss: 2.1914395292599997

Epoch: 6| Step: 10
Training loss: 1.3021268844604492
Validation loss: 2.1722863912582397

Epoch: 6| Step: 11
Training loss: 2.3243422508239746
Validation loss: 2.18280690908432

Epoch: 6| Step: 12
Training loss: 1.6614603996276855
Validation loss: 2.1843610604604087

Epoch: 6| Step: 13
Training loss: 1.4522171020507812
Validation loss: 2.1834871570269265

Epoch: 242| Step: 0
Training loss: 1.6331530809402466
Validation loss: 2.17260871330897

Epoch: 6| Step: 1
Training loss: 1.4012796878814697
Validation loss: 2.1761684020360312

Epoch: 6| Step: 2
Training loss: 1.1673390865325928
Validation loss: 2.183384656906128

Epoch: 6| Step: 3
Training loss: 0.9945759773254395
Validation loss: 2.1632405519485474

Epoch: 6| Step: 4
Training loss: 1.5881820917129517
Validation loss: 2.169647534688314

Epoch: 6| Step: 5
Training loss: 1.40506911277771
Validation loss: 2.173960328102112

Epoch: 6| Step: 6
Training loss: 1.6495306491851807
Validation loss: 2.193326731522878

Epoch: 6| Step: 7
Training loss: 1.3593719005584717
Validation loss: 2.1710575222969055

Epoch: 6| Step: 8
Training loss: 1.7456952333450317
Validation loss: 2.1818447510401406

Epoch: 6| Step: 9
Training loss: 2.277564525604248
Validation loss: 2.2074031233787537

Epoch: 6| Step: 10
Training loss: 1.7350564002990723
Validation loss: 2.15997048219045

Epoch: 6| Step: 11
Training loss: 2.0332863330841064
Validation loss: 2.1879037618637085

Epoch: 6| Step: 12
Training loss: 1.7276684045791626
Validation loss: 2.181925098101298

Epoch: 6| Step: 13
Training loss: 2.485243082046509
Validation loss: 2.182092765967051

Epoch: 243| Step: 0
Training loss: 1.8384449481964111
Validation loss: 2.180642008781433

Epoch: 6| Step: 1
Training loss: 1.0387625694274902
Validation loss: 2.1960701942443848

Epoch: 6| Step: 2
Training loss: 1.4890797138214111
Validation loss: 2.1792215506235757

Epoch: 6| Step: 3
Training loss: 1.3084938526153564
Validation loss: 2.1869962414105735

Epoch: 6| Step: 4
Training loss: 2.6293106079101562
Validation loss: 2.190345128377279

Epoch: 6| Step: 5
Training loss: 2.1162760257720947
Validation loss: 2.167257567246755

Epoch: 6| Step: 6
Training loss: 1.7048413753509521
Validation loss: 2.198887507120768

Epoch: 6| Step: 7
Training loss: 1.379311442375183
Validation loss: 2.1939322352409363

Epoch: 6| Step: 8
Training loss: 2.2318646907806396
Validation loss: 2.179377853870392

Epoch: 6| Step: 9
Training loss: 1.4424675703048706
Validation loss: 2.1903227965037027

Epoch: 6| Step: 10
Training loss: 1.4529147148132324
Validation loss: 2.211500962575277

Epoch: 6| Step: 11
Training loss: 1.6289414167404175
Validation loss: 2.1834845542907715

Epoch: 6| Step: 12
Training loss: 1.503158450126648
Validation loss: 2.183874865372976

Epoch: 6| Step: 13
Training loss: 2.3030142784118652
Validation loss: 2.1669700543085733

Epoch: 244| Step: 0
Training loss: 1.4023468494415283
Validation loss: 2.134101986885071

Epoch: 6| Step: 1
Training loss: 2.00331711769104
Validation loss: 2.1351391474405923

Epoch: 6| Step: 2
Training loss: 1.2422581911087036
Validation loss: 2.137669781843821

Epoch: 6| Step: 3
Training loss: 2.0601766109466553
Validation loss: 2.1189651687939963

Epoch: 6| Step: 4
Training loss: 1.9338910579681396
Validation loss: 2.133734385172526

Epoch: 6| Step: 5
Training loss: 2.3954010009765625
Validation loss: 2.1291551987330117

Epoch: 6| Step: 6
Training loss: 1.3194564580917358
Validation loss: 2.1424179077148438

Epoch: 6| Step: 7
Training loss: 1.339714765548706
Validation loss: 2.1587084531784058

Epoch: 6| Step: 8
Training loss: 1.7186813354492188
Validation loss: 2.158863286177317

Epoch: 6| Step: 9
Training loss: 2.665987491607666
Validation loss: 2.1698793172836304

Epoch: 6| Step: 10
Training loss: 1.3460781574249268
Validation loss: 2.1876286268234253

Epoch: 6| Step: 11
Training loss: 1.4689819812774658
Validation loss: 2.183319946130117

Epoch: 6| Step: 12
Training loss: 2.193756103515625
Validation loss: 2.1896384954452515

Epoch: 6| Step: 13
Training loss: 1.4455645084381104
Validation loss: 2.1556047995885215

Epoch: 245| Step: 0
Training loss: 1.6168065071105957
Validation loss: 2.201279640197754

Epoch: 6| Step: 1
Training loss: 1.6267626285552979
Validation loss: 2.1867117484410605

Epoch: 6| Step: 2
Training loss: 1.3837153911590576
Validation loss: 2.1844043135643005

Epoch: 6| Step: 3
Training loss: 1.876206874847412
Validation loss: 2.2054775754610696

Epoch: 6| Step: 4
Training loss: 1.4480395317077637
Validation loss: 2.178866446018219

Epoch: 6| Step: 5
Training loss: 1.1505341529846191
Validation loss: 2.1797864039738974

Epoch: 6| Step: 6
Training loss: 1.3855674266815186
Validation loss: 2.149116853872935

Epoch: 6| Step: 7
Training loss: 2.0385513305664062
Validation loss: 2.1792081594467163

Epoch: 6| Step: 8
Training loss: 1.549008846282959
Validation loss: 2.178132096926371

Epoch: 6| Step: 9
Training loss: 1.4316003322601318
Validation loss: 2.1844701568285623

Epoch: 6| Step: 10
Training loss: 1.5447348356246948
Validation loss: 2.1968825260798135

Epoch: 6| Step: 11
Training loss: 2.3757331371307373
Validation loss: 2.2087905009587607

Epoch: 6| Step: 12
Training loss: 2.834641933441162
Validation loss: 2.1853232781092324

Epoch: 6| Step: 13
Training loss: 1.5323749780654907
Validation loss: 2.185767571131388

Epoch: 246| Step: 0
Training loss: 1.671823501586914
Validation loss: 2.190973937511444

Epoch: 6| Step: 1
Training loss: 2.0436954498291016
Validation loss: 2.1655378341674805

Epoch: 6| Step: 2
Training loss: 1.1699289083480835
Validation loss: 2.1750809152921042

Epoch: 6| Step: 3
Training loss: 1.9836091995239258
Validation loss: 2.1606163581212363

Epoch: 6| Step: 4
Training loss: 1.8786084651947021
Validation loss: 2.141386866569519

Epoch: 6| Step: 5
Training loss: 1.9041603803634644
Validation loss: 2.1453037659327188

Epoch: 6| Step: 6
Training loss: 1.8956454992294312
Validation loss: 2.175783077875773

Epoch: 6| Step: 7
Training loss: 1.3970309495925903
Validation loss: 2.154630104700724

Epoch: 6| Step: 8
Training loss: 1.4635231494903564
Validation loss: 2.1649895906448364

Epoch: 6| Step: 9
Training loss: 1.7827527523040771
Validation loss: 2.1816413005193076

Epoch: 6| Step: 10
Training loss: 1.607679009437561
Validation loss: 2.210858146349589

Epoch: 6| Step: 11
Training loss: 1.1334073543548584
Validation loss: 2.1794920365015664

Epoch: 6| Step: 12
Training loss: 1.6897354125976562
Validation loss: 2.204529821872711

Epoch: 6| Step: 13
Training loss: 1.939697027206421
Validation loss: 2.2258989810943604

Epoch: 247| Step: 0
Training loss: 1.4786994457244873
Validation loss: 2.1937052806218467

Epoch: 6| Step: 1
Training loss: 1.0364539623260498
Validation loss: 2.21120015780131

Epoch: 6| Step: 2
Training loss: 2.340708017349243
Validation loss: 2.1921894550323486

Epoch: 6| Step: 3
Training loss: 2.2540931701660156
Validation loss: 2.1841918428738913

Epoch: 6| Step: 4
Training loss: 2.200352191925049
Validation loss: 2.18092679977417

Epoch: 6| Step: 5
Training loss: 1.228895664215088
Validation loss: 2.1753650903701782

Epoch: 6| Step: 6
Training loss: 1.8878473043441772
Validation loss: 2.1748895843823752

Epoch: 6| Step: 7
Training loss: 1.3545500040054321
Validation loss: 2.155960281689962

Epoch: 6| Step: 8
Training loss: 1.7998144626617432
Validation loss: 2.177466114362081

Epoch: 6| Step: 9
Training loss: 1.3830987215042114
Validation loss: 2.1535319884618125

Epoch: 6| Step: 10
Training loss: 2.0046486854553223
Validation loss: 2.118415355682373

Epoch: 6| Step: 11
Training loss: 1.5697094202041626
Validation loss: 2.159070134162903

Epoch: 6| Step: 12
Training loss: 1.7023743391036987
Validation loss: 2.1441979805628457

Epoch: 6| Step: 13
Training loss: 1.534188985824585
Validation loss: 2.1520625352859497

Epoch: 248| Step: 0
Training loss: 1.6724259853363037
Validation loss: 2.132147709528605

Epoch: 6| Step: 1
Training loss: 0.9921671152114868
Validation loss: 2.1385721961657205

Epoch: 6| Step: 2
Training loss: 1.838404893875122
Validation loss: 2.1166254679361978

Epoch: 6| Step: 3
Training loss: 1.772391438484192
Validation loss: 2.1267833709716797

Epoch: 6| Step: 4
Training loss: 1.943018913269043
Validation loss: 2.111401160558065

Epoch: 6| Step: 5
Training loss: 1.4624963998794556
Validation loss: 2.107803444067637

Epoch: 6| Step: 6
Training loss: 1.837617039680481
Validation loss: 2.1376508474349976

Epoch: 6| Step: 7
Training loss: 1.0438387393951416
Validation loss: 2.1495098869005838

Epoch: 6| Step: 8
Training loss: 1.8670692443847656
Validation loss: 2.1204139391581216

Epoch: 6| Step: 9
Training loss: 1.5519660711288452
Validation loss: 2.1450661619504294

Epoch: 6| Step: 10
Training loss: 1.9524927139282227
Validation loss: 2.1328240633010864

Epoch: 6| Step: 11
Training loss: 1.7773686647415161
Validation loss: 2.137144168217977

Epoch: 6| Step: 12
Training loss: 2.7410311698913574
Validation loss: 2.153898596763611

Epoch: 6| Step: 13
Training loss: 1.2070050239562988
Validation loss: 2.1575207710266113

Epoch: 249| Step: 0
Training loss: 1.2351140975952148
Validation loss: 2.154531498750051

Epoch: 6| Step: 1
Training loss: 1.8562068939208984
Validation loss: 2.1495221654574075

Epoch: 6| Step: 2
Training loss: 1.5079946517944336
Validation loss: 2.154512882232666

Epoch: 6| Step: 3
Training loss: 2.094712734222412
Validation loss: 2.164739469687144

Epoch: 6| Step: 4
Training loss: 2.406355857849121
Validation loss: 2.155744731426239

Epoch: 6| Step: 5
Training loss: 1.9073207378387451
Validation loss: 2.1704407135645547

Epoch: 6| Step: 6
Training loss: 1.183741807937622
Validation loss: 2.1619734168052673

Epoch: 6| Step: 7
Training loss: 2.1937007904052734
Validation loss: 2.17915008465449

Epoch: 6| Step: 8
Training loss: 1.4108576774597168
Validation loss: 2.1685155431429544

Epoch: 6| Step: 9
Training loss: 1.6030551195144653
Validation loss: 2.1523138284683228

Epoch: 6| Step: 10
Training loss: 1.4210546016693115
Validation loss: 2.164401888847351

Epoch: 6| Step: 11
Training loss: 1.6738567352294922
Validation loss: 2.1854402820269265

Epoch: 6| Step: 12
Training loss: 1.2497540712356567
Validation loss: 2.1594045758247375

Epoch: 6| Step: 13
Training loss: 1.5848939418792725
Validation loss: 2.1814032196998596

Epoch: 250| Step: 0
Training loss: 1.319556474685669
Validation loss: 2.1810288230578103

Epoch: 6| Step: 1
Training loss: 2.29301118850708
Validation loss: 2.168280243873596

Epoch: 6| Step: 2
Training loss: 1.4327752590179443
Validation loss: 2.168833573659261

Epoch: 6| Step: 3
Training loss: 1.5028223991394043
Validation loss: 2.177850325902303

Epoch: 6| Step: 4
Training loss: 1.9296951293945312
Validation loss: 2.150834798812866

Epoch: 6| Step: 5
Training loss: 1.4020814895629883
Validation loss: 2.1557811896006265

Epoch: 6| Step: 6
Training loss: 1.1539111137390137
Validation loss: 2.15619166692098

Epoch: 6| Step: 7
Training loss: 1.1195875406265259
Validation loss: 2.196529050668081

Epoch: 6| Step: 8
Training loss: 1.4859174489974976
Validation loss: 2.188079377015432

Epoch: 6| Step: 9
Training loss: 2.143752336502075
Validation loss: 2.176002244154612

Epoch: 6| Step: 10
Training loss: 1.5794789791107178
Validation loss: 2.1711027026176453

Epoch: 6| Step: 11
Training loss: 1.7889351844787598
Validation loss: 2.161331375439962

Epoch: 6| Step: 12
Training loss: 1.748103380203247
Validation loss: 2.189769963423411

Epoch: 6| Step: 13
Training loss: 2.350602149963379
Validation loss: 2.2060298522313437

Epoch: 251| Step: 0
Training loss: 1.548649787902832
Validation loss: 2.1992823084195456

Epoch: 6| Step: 1
Training loss: 2.4392330646514893
Validation loss: 2.211644927660624

Epoch: 6| Step: 2
Training loss: 1.6688964366912842
Validation loss: 2.2170830368995667

Epoch: 6| Step: 3
Training loss: 1.6182334423065186
Validation loss: 2.209552804629008

Epoch: 6| Step: 4
Training loss: 1.4250001907348633
Validation loss: 2.2075182795524597

Epoch: 6| Step: 5
Training loss: 1.4997271299362183
Validation loss: 2.1972442070643106

Epoch: 6| Step: 6
Training loss: 2.3727943897247314
Validation loss: 2.1678563753763833

Epoch: 6| Step: 7
Training loss: 1.4372888803482056
Validation loss: 2.169135252634684

Epoch: 6| Step: 8
Training loss: 1.8557398319244385
Validation loss: 2.189332445462545

Epoch: 6| Step: 9
Training loss: 1.4386543035507202
Validation loss: 2.179450213909149

Epoch: 6| Step: 10
Training loss: 1.362833023071289
Validation loss: 2.170893450578054

Epoch: 6| Step: 11
Training loss: 1.584419846534729
Validation loss: 2.1706157525380454

Epoch: 6| Step: 12
Training loss: 2.064455509185791
Validation loss: 2.175002614657084

Epoch: 6| Step: 13
Training loss: 1.6019037961959839
Validation loss: 2.152649184068044

Epoch: 252| Step: 0
Training loss: 2.1145849227905273
Validation loss: 2.16632479429245

Epoch: 6| Step: 1
Training loss: 1.5597819089889526
Validation loss: 2.1446902553240457

Epoch: 6| Step: 2
Training loss: 1.873510718345642
Validation loss: 2.1569932301839194

Epoch: 6| Step: 3
Training loss: 1.2993260622024536
Validation loss: 2.158143103122711

Epoch: 6| Step: 4
Training loss: 1.0748076438903809
Validation loss: 2.1628347635269165

Epoch: 6| Step: 5
Training loss: 2.0951004028320312
Validation loss: 2.1845285892486572

Epoch: 6| Step: 6
Training loss: 1.0151691436767578
Validation loss: 2.1425901055336

Epoch: 6| Step: 7
Training loss: 1.8389583826065063
Validation loss: 2.146144986152649

Epoch: 6| Step: 8
Training loss: 1.8077157735824585
Validation loss: 2.1391987204551697

Epoch: 6| Step: 9
Training loss: 1.6712968349456787
Validation loss: 2.1497602264086404

Epoch: 6| Step: 10
Training loss: 1.7205665111541748
Validation loss: 2.143675128618876

Epoch: 6| Step: 11
Training loss: 2.0012145042419434
Validation loss: 2.1551818251609802

Epoch: 6| Step: 12
Training loss: 1.8628556728363037
Validation loss: 2.1447916825612388

Epoch: 6| Step: 13
Training loss: 1.6823530197143555
Validation loss: 2.141563355922699

Epoch: 253| Step: 0
Training loss: 1.6721596717834473
Validation loss: 2.1388157407442727

Epoch: 6| Step: 1
Training loss: 1.9121485948562622
Validation loss: 2.1416154901186624

Epoch: 6| Step: 2
Training loss: 1.7305617332458496
Validation loss: 2.1375255783398948

Epoch: 6| Step: 3
Training loss: 1.7212637662887573
Validation loss: 2.1610951821009317

Epoch: 6| Step: 4
Training loss: 1.2609453201293945
Validation loss: 2.1631561120351157

Epoch: 6| Step: 5
Training loss: 1.2876787185668945
Validation loss: 2.1639262040456138

Epoch: 6| Step: 6
Training loss: 1.5918203592300415
Validation loss: 2.1726963917414346

Epoch: 6| Step: 7
Training loss: 1.3551841974258423
Validation loss: 2.1683348417282104

Epoch: 6| Step: 8
Training loss: 2.4587478637695312
Validation loss: 2.167462428410848

Epoch: 6| Step: 9
Training loss: 1.6807883977890015
Validation loss: 2.1761178970336914

Epoch: 6| Step: 10
Training loss: 1.7854399681091309
Validation loss: 2.1808571815490723

Epoch: 6| Step: 11
Training loss: 1.1972577571868896
Validation loss: 2.148123244444529

Epoch: 6| Step: 12
Training loss: 1.5482091903686523
Validation loss: 2.1555976470311484

Epoch: 6| Step: 13
Training loss: 1.5890421867370605
Validation loss: 2.1831214825312295

Epoch: 254| Step: 0
Training loss: 1.7216875553131104
Validation loss: 2.175623059272766

Epoch: 6| Step: 1
Training loss: 1.9589396715164185
Validation loss: 2.1494502623875937

Epoch: 6| Step: 2
Training loss: 1.4406418800354004
Validation loss: 2.161273996035258

Epoch: 6| Step: 3
Training loss: 1.3454890251159668
Validation loss: 2.17508731285731

Epoch: 6| Step: 4
Training loss: 1.1208608150482178
Validation loss: 2.1724356412887573

Epoch: 6| Step: 5
Training loss: 1.918739676475525
Validation loss: 2.160411318143209

Epoch: 6| Step: 6
Training loss: 2.450993776321411
Validation loss: 2.1432960430781045

Epoch: 6| Step: 7
Training loss: 1.291980266571045
Validation loss: 2.1520320971806846

Epoch: 6| Step: 8
Training loss: 1.1416120529174805
Validation loss: 2.160657286643982

Epoch: 6| Step: 9
Training loss: 1.4783170223236084
Validation loss: 2.1757860581080117

Epoch: 6| Step: 10
Training loss: 1.7231117486953735
Validation loss: 2.2056585947672525

Epoch: 6| Step: 11
Training loss: 2.5560474395751953
Validation loss: 2.224019408226013

Epoch: 6| Step: 12
Training loss: 1.6895421743392944
Validation loss: 2.2199156483014426

Epoch: 6| Step: 13
Training loss: 1.585418701171875
Validation loss: 2.2218106985092163

Epoch: 255| Step: 0
Training loss: 2.018730640411377
Validation loss: 2.201486825942993

Epoch: 6| Step: 1
Training loss: 1.675935983657837
Validation loss: 2.1959818601608276

Epoch: 6| Step: 2
Training loss: 1.6015887260437012
Validation loss: 2.180350919564565

Epoch: 6| Step: 3
Training loss: 1.7710561752319336
Validation loss: 2.1592429677645364

Epoch: 6| Step: 4
Training loss: 1.3314261436462402
Validation loss: 2.1470570166905723

Epoch: 6| Step: 5
Training loss: 1.9735488891601562
Validation loss: 2.1476834813753762

Epoch: 6| Step: 6
Training loss: 1.526318907737732
Validation loss: 2.1518745024998984

Epoch: 6| Step: 7
Training loss: 2.0440311431884766
Validation loss: 2.1441978216171265

Epoch: 6| Step: 8
Training loss: 1.9457722902297974
Validation loss: 2.142921487490336

Epoch: 6| Step: 9
Training loss: 1.6597540378570557
Validation loss: 2.1592472195625305

Epoch: 6| Step: 10
Training loss: 1.9881584644317627
Validation loss: 2.151525298754374

Epoch: 6| Step: 11
Training loss: 1.5860580205917358
Validation loss: 2.172841469446818

Epoch: 6| Step: 12
Training loss: 1.2677512168884277
Validation loss: 2.1620686848958335

Epoch: 6| Step: 13
Training loss: 1.4157894849777222
Validation loss: 2.1711638967196145

Epoch: 256| Step: 0
Training loss: 1.9982092380523682
Validation loss: 2.1799890597661338

Epoch: 6| Step: 1
Training loss: 1.1938008069992065
Validation loss: 2.1988308429718018

Epoch: 6| Step: 2
Training loss: 2.1742756366729736
Validation loss: 2.2071473598480225

Epoch: 6| Step: 3
Training loss: 1.262796401977539
Validation loss: 2.208782911300659

Epoch: 6| Step: 4
Training loss: 1.9568095207214355
Validation loss: 2.2314466635386148

Epoch: 6| Step: 5
Training loss: 1.8091521263122559
Validation loss: 2.2424018383026123

Epoch: 6| Step: 6
Training loss: 1.0703705549240112
Validation loss: 2.1960888107617698

Epoch: 6| Step: 7
Training loss: 1.789625644683838
Validation loss: 2.2017255624135337

Epoch: 6| Step: 8
Training loss: 2.173729419708252
Validation loss: 2.188716987768809

Epoch: 6| Step: 9
Training loss: 1.6279815435409546
Validation loss: 2.184588293234507

Epoch: 6| Step: 10
Training loss: 1.4953217506408691
Validation loss: 2.2034719983736673

Epoch: 6| Step: 11
Training loss: 1.8941974639892578
Validation loss: 2.198981841405233

Epoch: 6| Step: 12
Training loss: 1.728219985961914
Validation loss: 2.1945655743281045

Epoch: 6| Step: 13
Training loss: 1.4644889831542969
Validation loss: 2.177311897277832

Epoch: 257| Step: 0
Training loss: 1.802264928817749
Validation loss: 2.1760097543398538

Epoch: 6| Step: 1
Training loss: 1.6594159603118896
Validation loss: 2.17134819428126

Epoch: 6| Step: 2
Training loss: 2.1500182151794434
Validation loss: 2.1642859180768332

Epoch: 6| Step: 3
Training loss: 1.2989811897277832
Validation loss: 2.176998575528463

Epoch: 6| Step: 4
Training loss: 1.9685429334640503
Validation loss: 2.190082828203837

Epoch: 6| Step: 5
Training loss: 1.4696598052978516
Validation loss: 2.185618579387665

Epoch: 6| Step: 6
Training loss: 2.2487905025482178
Validation loss: 2.198799788951874

Epoch: 6| Step: 7
Training loss: 1.4684453010559082
Validation loss: 2.2179529666900635

Epoch: 6| Step: 8
Training loss: 1.689182996749878
Validation loss: 2.215283731619517

Epoch: 6| Step: 9
Training loss: 2.38539981842041
Validation loss: 2.1847272713979087

Epoch: 6| Step: 10
Training loss: 1.9945842027664185
Validation loss: 2.207180996735891

Epoch: 6| Step: 11
Training loss: 1.9120877981185913
Validation loss: 2.209510604540507

Epoch: 6| Step: 12
Training loss: 0.751211404800415
Validation loss: 2.1975027521451316

Epoch: 6| Step: 13
Training loss: 0.8770687580108643
Validation loss: 2.1830434799194336

Epoch: 258| Step: 0
Training loss: 1.3607468605041504
Validation loss: 2.21750275293986

Epoch: 6| Step: 1
Training loss: 1.6438651084899902
Validation loss: 2.1867763996124268

Epoch: 6| Step: 2
Training loss: 1.890890121459961
Validation loss: 2.1629523833592734

Epoch: 6| Step: 3
Training loss: 1.638126015663147
Validation loss: 2.172062079111735

Epoch: 6| Step: 4
Training loss: 1.6257202625274658
Validation loss: 2.150461792945862

Epoch: 6| Step: 5
Training loss: 1.2771074771881104
Validation loss: 2.1618582804997764

Epoch: 6| Step: 6
Training loss: 1.3956106901168823
Validation loss: 2.1693961024284363

Epoch: 6| Step: 7
Training loss: 2.1952898502349854
Validation loss: 2.166143854459127

Epoch: 6| Step: 8
Training loss: 2.035463571548462
Validation loss: 2.1997398138046265

Epoch: 6| Step: 9
Training loss: 1.8544986248016357
Validation loss: 2.190175255139669

Epoch: 6| Step: 10
Training loss: 1.2185454368591309
Validation loss: 2.185528556505839

Epoch: 6| Step: 11
Training loss: 1.8779171705245972
Validation loss: 2.1980544924736023

Epoch: 6| Step: 12
Training loss: 2.248047351837158
Validation loss: 2.1806686321894326

Epoch: 6| Step: 13
Training loss: 1.1086490154266357
Validation loss: 2.196548899014791

Epoch: 259| Step: 0
Training loss: 1.6176317930221558
Validation loss: 2.195125679175059

Epoch: 6| Step: 1
Training loss: 1.0247821807861328
Validation loss: 2.189881364504496

Epoch: 6| Step: 2
Training loss: 2.090381622314453
Validation loss: 2.2051023642222085

Epoch: 6| Step: 3
Training loss: 1.6026688814163208
Validation loss: 2.2104164759318032

Epoch: 6| Step: 4
Training loss: 1.3142831325531006
Validation loss: 2.208095153172811

Epoch: 6| Step: 5
Training loss: 1.8058316707611084
Validation loss: 2.188215951124827

Epoch: 6| Step: 6
Training loss: 1.672327995300293
Validation loss: 2.2054806550343833

Epoch: 6| Step: 7
Training loss: 1.4887640476226807
Validation loss: 2.1947458585103354

Epoch: 6| Step: 8
Training loss: 1.8370221853256226
Validation loss: 2.1885725458463035

Epoch: 6| Step: 9
Training loss: 1.4304256439208984
Validation loss: 2.1574945648511252

Epoch: 6| Step: 10
Training loss: 1.485935926437378
Validation loss: 2.1851298411687217

Epoch: 6| Step: 11
Training loss: 1.4579484462738037
Validation loss: 2.170050243536631

Epoch: 6| Step: 12
Training loss: 2.074169158935547
Validation loss: 2.1825087467829385

Epoch: 6| Step: 13
Training loss: 2.019740581512451
Validation loss: 2.1786499619483948

Epoch: 260| Step: 0
Training loss: 2.2429139614105225
Validation loss: 2.1835631728172302

Epoch: 6| Step: 1
Training loss: 1.3032505512237549
Validation loss: 2.1907149155934653

Epoch: 6| Step: 2
Training loss: 1.952812671661377
Validation loss: 2.17254630724589

Epoch: 6| Step: 3
Training loss: 1.4521487951278687
Validation loss: 2.209993918736776

Epoch: 6| Step: 4
Training loss: 1.6330509185791016
Validation loss: 2.2185620069503784

Epoch: 6| Step: 5
Training loss: 1.4960479736328125
Validation loss: 2.191348652044932

Epoch: 6| Step: 6
Training loss: 1.4655073881149292
Validation loss: 2.184583286444346

Epoch: 6| Step: 7
Training loss: 1.9331485033035278
Validation loss: 2.2013160785039267

Epoch: 6| Step: 8
Training loss: 1.2235914468765259
Validation loss: 2.1978142658869424

Epoch: 6| Step: 9
Training loss: 2.2399566173553467
Validation loss: 2.2032998402913413

Epoch: 6| Step: 10
Training loss: 1.0314311981201172
Validation loss: 2.18347696463267

Epoch: 6| Step: 11
Training loss: 1.5862983465194702
Validation loss: 2.201084852218628

Epoch: 6| Step: 12
Training loss: 1.2567431926727295
Validation loss: 2.1977612574895224

Epoch: 6| Step: 13
Training loss: 1.7569804191589355
Validation loss: 2.193451484044393

Epoch: 261| Step: 0
Training loss: 1.1319023370742798
Validation loss: 2.1844582160313926

Epoch: 6| Step: 1
Training loss: 1.8929011821746826
Validation loss: 2.1729289293289185

Epoch: 6| Step: 2
Training loss: 1.4248709678649902
Validation loss: 2.1981125672658286

Epoch: 6| Step: 3
Training loss: 1.6862335205078125
Validation loss: 2.1886372367540994

Epoch: 6| Step: 4
Training loss: 1.8428807258605957
Validation loss: 2.1728344559669495

Epoch: 6| Step: 5
Training loss: 1.9402896165847778
Validation loss: 2.1832834084828696

Epoch: 6| Step: 6
Training loss: 1.5018937587738037
Validation loss: 2.210887392361959

Epoch: 6| Step: 7
Training loss: 1.5781850814819336
Validation loss: 2.182369669278463

Epoch: 6| Step: 8
Training loss: 1.6833643913269043
Validation loss: 2.1922449866930642

Epoch: 6| Step: 9
Training loss: 1.2951735258102417
Validation loss: 2.219753623008728

Epoch: 6| Step: 10
Training loss: 1.238725185394287
Validation loss: 2.2013140519460044

Epoch: 6| Step: 11
Training loss: 1.7127968072891235
Validation loss: 2.2015922466913858

Epoch: 6| Step: 12
Training loss: 2.2558720111846924
Validation loss: 2.2079529960950217

Epoch: 6| Step: 13
Training loss: 1.6441062688827515
Validation loss: 2.17512708902359

Epoch: 262| Step: 0
Training loss: 1.4899461269378662
Validation loss: 2.1948554515838623

Epoch: 6| Step: 1
Training loss: 1.092327356338501
Validation loss: 2.1814061800638833

Epoch: 6| Step: 2
Training loss: 1.4502743482589722
Validation loss: 2.1710527340571084

Epoch: 6| Step: 3
Training loss: 1.6944036483764648
Validation loss: 2.1820953488349915

Epoch: 6| Step: 4
Training loss: 2.0005359649658203
Validation loss: 2.1797906160354614

Epoch: 6| Step: 5
Training loss: 1.975521206855774
Validation loss: 2.1935593485832214

Epoch: 6| Step: 6
Training loss: 1.203953742980957
Validation loss: 2.1822205980618796

Epoch: 6| Step: 7
Training loss: 0.7980786561965942
Validation loss: 2.187645435333252

Epoch: 6| Step: 8
Training loss: 1.7782143354415894
Validation loss: 2.168587625026703

Epoch: 6| Step: 9
Training loss: 1.3829529285430908
Validation loss: 2.187499761581421

Epoch: 6| Step: 10
Training loss: 1.5713889598846436
Validation loss: 2.185082197189331

Epoch: 6| Step: 11
Training loss: 1.916651725769043
Validation loss: 2.166767100493113

Epoch: 6| Step: 12
Training loss: 2.3366808891296387
Validation loss: 2.1650479833285012

Epoch: 6| Step: 13
Training loss: 1.814105749130249
Validation loss: 2.165944774945577

Epoch: 263| Step: 0
Training loss: 2.0209012031555176
Validation loss: 2.1725268562634787

Epoch: 6| Step: 1
Training loss: 1.7542798519134521
Validation loss: 2.1327055295308432

Epoch: 6| Step: 2
Training loss: 1.4373241662979126
Validation loss: 2.174009164174398

Epoch: 6| Step: 3
Training loss: 1.6415181159973145
Validation loss: 2.1726633509000144

Epoch: 6| Step: 4
Training loss: 0.9858107566833496
Validation loss: 2.1749087969462075

Epoch: 6| Step: 5
Training loss: 1.1041274070739746
Validation loss: 2.1439515352249146

Epoch: 6| Step: 6
Training loss: 1.6274516582489014
Validation loss: 2.157362461090088

Epoch: 6| Step: 7
Training loss: 1.5473716259002686
Validation loss: 2.147166053454081

Epoch: 6| Step: 8
Training loss: 1.3454742431640625
Validation loss: 2.1518189708391824

Epoch: 6| Step: 9
Training loss: 2.0311107635498047
Validation loss: 2.164124011993408

Epoch: 6| Step: 10
Training loss: 1.5473246574401855
Validation loss: 2.186154047648112

Epoch: 6| Step: 11
Training loss: 1.689366102218628
Validation loss: 2.181577444076538

Epoch: 6| Step: 12
Training loss: 2.2814834117889404
Validation loss: 2.183867891629537

Epoch: 6| Step: 13
Training loss: 1.5395606756210327
Validation loss: 2.202714681625366

Epoch: 264| Step: 0
Training loss: 1.2987715005874634
Validation loss: 2.1674576799074807

Epoch: 6| Step: 1
Training loss: 1.7924244403839111
Validation loss: 2.17925896247228

Epoch: 6| Step: 2
Training loss: 1.143688678741455
Validation loss: 2.166546364625295

Epoch: 6| Step: 3
Training loss: 1.4049787521362305
Validation loss: 2.1640395323435464

Epoch: 6| Step: 4
Training loss: 2.1121506690979004
Validation loss: 2.1942346692085266

Epoch: 6| Step: 5
Training loss: 1.6941750049591064
Validation loss: 2.138112703959147

Epoch: 6| Step: 6
Training loss: 1.9320273399353027
Validation loss: 2.1612401405970254

Epoch: 6| Step: 7
Training loss: 1.720306396484375
Validation loss: 2.158948461214701

Epoch: 6| Step: 8
Training loss: 1.4274890422821045
Validation loss: 2.1488563815752664

Epoch: 6| Step: 9
Training loss: 1.9063177108764648
Validation loss: 2.151499350865682

Epoch: 6| Step: 10
Training loss: 1.791561484336853
Validation loss: 2.164087990919749

Epoch: 6| Step: 11
Training loss: 1.8874480724334717
Validation loss: 2.1675190130869546

Epoch: 6| Step: 12
Training loss: 1.1591827869415283
Validation loss: 2.180344899495443

Epoch: 6| Step: 13
Training loss: 1.6070729494094849
Validation loss: 2.1500564416249595

Epoch: 265| Step: 0
Training loss: 2.2015905380249023
Validation loss: 2.1736036936442056

Epoch: 6| Step: 1
Training loss: 1.161408543586731
Validation loss: 2.1540990074475608

Epoch: 6| Step: 2
Training loss: 2.0473427772521973
Validation loss: 2.1689393321673074

Epoch: 6| Step: 3
Training loss: 1.1857397556304932
Validation loss: 2.165436108907064

Epoch: 6| Step: 4
Training loss: 1.0078353881835938
Validation loss: 2.1549630959828696

Epoch: 6| Step: 5
Training loss: 1.4081521034240723
Validation loss: 2.1903015772501626

Epoch: 6| Step: 6
Training loss: 1.4987337589263916
Validation loss: 2.1781579852104187

Epoch: 6| Step: 7
Training loss: 1.8606702089309692
Validation loss: 2.182097315788269

Epoch: 6| Step: 8
Training loss: 1.8707785606384277
Validation loss: 2.1669764121373496

Epoch: 6| Step: 9
Training loss: 1.6339622735977173
Validation loss: 2.173034429550171

Epoch: 6| Step: 10
Training loss: 1.313967227935791
Validation loss: 2.161297082901001

Epoch: 6| Step: 11
Training loss: 1.2114980220794678
Validation loss: 2.136172076066335

Epoch: 6| Step: 12
Training loss: 2.2210302352905273
Validation loss: 2.1470613280932107

Epoch: 6| Step: 13
Training loss: 1.6140925884246826
Validation loss: 2.148410737514496

Epoch: 266| Step: 0
Training loss: 2.0390748977661133
Validation loss: 2.185366968313853

Epoch: 6| Step: 1
Training loss: 1.1330108642578125
Validation loss: 2.173884948094686

Epoch: 6| Step: 2
Training loss: 1.4238262176513672
Validation loss: 2.195739507675171

Epoch: 6| Step: 3
Training loss: 1.580019474029541
Validation loss: 2.1758160988489785

Epoch: 6| Step: 4
Training loss: 1.5306452512741089
Validation loss: 2.1933903098106384

Epoch: 6| Step: 5
Training loss: 1.3703778982162476
Validation loss: 2.17697141567866

Epoch: 6| Step: 6
Training loss: 1.6729975938796997
Validation loss: 2.1857871214548745

Epoch: 6| Step: 7
Training loss: 1.7171331644058228
Validation loss: 2.1896864573160806

Epoch: 6| Step: 8
Training loss: 1.2790160179138184
Validation loss: 2.1685139735539756

Epoch: 6| Step: 9
Training loss: 1.627086877822876
Validation loss: 2.160677214463552

Epoch: 6| Step: 10
Training loss: 1.456313967704773
Validation loss: 2.190761625766754

Epoch: 6| Step: 11
Training loss: 1.8955538272857666
Validation loss: 2.1776832342147827

Epoch: 6| Step: 12
Training loss: 1.4067975282669067
Validation loss: 2.1777019103368125

Epoch: 6| Step: 13
Training loss: 1.8358911275863647
Validation loss: 2.201518257459005

Epoch: 267| Step: 0
Training loss: 1.3876502513885498
Validation loss: 2.1896239519119263

Epoch: 6| Step: 1
Training loss: 1.5269262790679932
Validation loss: 2.209868530432383

Epoch: 6| Step: 2
Training loss: 1.317663550376892
Validation loss: 2.2037923534711203

Epoch: 6| Step: 3
Training loss: 1.268356204032898
Validation loss: 2.1562700072924295

Epoch: 6| Step: 4
Training loss: 1.3653048276901245
Validation loss: 2.212348222732544

Epoch: 6| Step: 5
Training loss: 2.3234777450561523
Validation loss: 2.1803016662597656

Epoch: 6| Step: 6
Training loss: 1.7209488153457642
Validation loss: 2.21734619140625

Epoch: 6| Step: 7
Training loss: 1.594637155532837
Validation loss: 2.2073766589164734

Epoch: 6| Step: 8
Training loss: 1.2424354553222656
Validation loss: 2.210382560888926

Epoch: 6| Step: 9
Training loss: 1.6673942804336548
Validation loss: 2.1937485138575235

Epoch: 6| Step: 10
Training loss: 1.9027327299118042
Validation loss: 2.1914101441701255

Epoch: 6| Step: 11
Training loss: 1.5625340938568115
Validation loss: 2.1739849845568338

Epoch: 6| Step: 12
Training loss: 1.6885485649108887
Validation loss: 2.1638327638308206

Epoch: 6| Step: 13
Training loss: 1.3179126977920532
Validation loss: 2.183020015557607

Epoch: 268| Step: 0
Training loss: 1.2650580406188965
Validation loss: 2.1799227197964988

Epoch: 6| Step: 1
Training loss: 1.8677769899368286
Validation loss: 2.165457288424174

Epoch: 6| Step: 2
Training loss: 1.8231533765792847
Validation loss: 2.1580726703008017

Epoch: 6| Step: 3
Training loss: 1.7885355949401855
Validation loss: 2.1609612902005515

Epoch: 6| Step: 4
Training loss: 2.1286587715148926
Validation loss: 2.1782281001408896

Epoch: 6| Step: 5
Training loss: 1.9472744464874268
Validation loss: 2.15416944026947

Epoch: 6| Step: 6
Training loss: 1.8813424110412598
Validation loss: 2.1844759980837503

Epoch: 6| Step: 7
Training loss: 1.165030598640442
Validation loss: 2.1976719299952188

Epoch: 6| Step: 8
Training loss: 1.6577318906784058
Validation loss: 2.1503988107045493

Epoch: 6| Step: 9
Training loss: 1.3375725746154785
Validation loss: 2.1787792444229126

Epoch: 6| Step: 10
Training loss: 1.288629174232483
Validation loss: 2.1821637550989785

Epoch: 6| Step: 11
Training loss: 1.4410269260406494
Validation loss: 2.169705073038737

Epoch: 6| Step: 12
Training loss: 1.5118705034255981
Validation loss: 2.141415456930796

Epoch: 6| Step: 13
Training loss: 1.4679698944091797
Validation loss: 2.1568264762560525

Epoch: 269| Step: 0
Training loss: 1.2251476049423218
Validation loss: 2.1546862522761026

Epoch: 6| Step: 1
Training loss: 1.2800501585006714
Validation loss: 2.141793151696523

Epoch: 6| Step: 2
Training loss: 1.2176134586334229
Validation loss: 2.1436155637105307

Epoch: 6| Step: 3
Training loss: 2.063176155090332
Validation loss: 2.1354852318763733

Epoch: 6| Step: 4
Training loss: 2.1140241622924805
Validation loss: 2.132025678952535

Epoch: 6| Step: 5
Training loss: 0.925011157989502
Validation loss: 2.140482028325399

Epoch: 6| Step: 6
Training loss: 1.7570269107818604
Validation loss: 2.173026442527771

Epoch: 6| Step: 7
Training loss: 1.73161780834198
Validation loss: 2.1747124195098877

Epoch: 6| Step: 8
Training loss: 1.591385006904602
Validation loss: 2.175651033719381

Epoch: 6| Step: 9
Training loss: 1.132375717163086
Validation loss: 2.178397317727407

Epoch: 6| Step: 10
Training loss: 1.9859638214111328
Validation loss: 2.1648921171824136

Epoch: 6| Step: 11
Training loss: 1.6876578330993652
Validation loss: 2.1325018405914307

Epoch: 6| Step: 12
Training loss: 1.890170693397522
Validation loss: 2.136435886224111

Epoch: 6| Step: 13
Training loss: 1.829399585723877
Validation loss: 2.1368542512257895

Epoch: 270| Step: 0
Training loss: 1.9246058464050293
Validation loss: 2.1348282297452292

Epoch: 6| Step: 1
Training loss: 1.551894187927246
Validation loss: 2.125641405582428

Epoch: 6| Step: 2
Training loss: 2.2863693237304688
Validation loss: 2.1476327578226724

Epoch: 6| Step: 3
Training loss: 1.6479222774505615
Validation loss: 2.152676204840342

Epoch: 6| Step: 4
Training loss: 1.3467884063720703
Validation loss: 2.1335513393084207

Epoch: 6| Step: 5
Training loss: 1.335008978843689
Validation loss: 2.1348716020584106

Epoch: 6| Step: 6
Training loss: 1.331632137298584
Validation loss: 2.144221385320028

Epoch: 6| Step: 7
Training loss: 1.2891128063201904
Validation loss: 2.1469699144363403

Epoch: 6| Step: 8
Training loss: 1.5406596660614014
Validation loss: 2.128053307533264

Epoch: 6| Step: 9
Training loss: 1.583913803100586
Validation loss: 2.1541862885157266

Epoch: 6| Step: 10
Training loss: 1.2894490957260132
Validation loss: 2.161608080069224

Epoch: 6| Step: 11
Training loss: 2.0382466316223145
Validation loss: 2.156385620435079

Epoch: 6| Step: 12
Training loss: 2.2658872604370117
Validation loss: 2.133279581864675

Epoch: 6| Step: 13
Training loss: 0.8992058038711548
Validation loss: 2.146475354830424

Epoch: 271| Step: 0
Training loss: 1.3532664775848389
Validation loss: 2.138752023379008

Epoch: 6| Step: 1
Training loss: 0.8341965675354004
Validation loss: 2.1581498980522156

Epoch: 6| Step: 2
Training loss: 1.5892937183380127
Validation loss: 2.151791989803314

Epoch: 6| Step: 3
Training loss: 1.6510778665542603
Validation loss: 2.1505126555760703

Epoch: 6| Step: 4
Training loss: 2.0893936157226562
Validation loss: 2.1525787115097046

Epoch: 6| Step: 5
Training loss: 1.214247465133667
Validation loss: 2.1695380409558616

Epoch: 6| Step: 6
Training loss: 1.0292706489562988
Validation loss: 2.156794627507528

Epoch: 6| Step: 7
Training loss: 2.6701292991638184
Validation loss: 2.1685601274172464

Epoch: 6| Step: 8
Training loss: 1.2722587585449219
Validation loss: 2.1712497671445212

Epoch: 6| Step: 9
Training loss: 1.9696905612945557
Validation loss: 2.1788421074549356

Epoch: 6| Step: 10
Training loss: 1.672835350036621
Validation loss: 2.1881319681803384

Epoch: 6| Step: 11
Training loss: 1.9979840517044067
Validation loss: 2.180257876714071

Epoch: 6| Step: 12
Training loss: 1.398770809173584
Validation loss: 2.1768839160601297

Epoch: 6| Step: 13
Training loss: 1.2298483848571777
Validation loss: 2.1796399354934692

Epoch: 272| Step: 0
Training loss: 2.6545491218566895
Validation loss: 2.1684190233548484

Epoch: 6| Step: 1
Training loss: 1.7750307321548462
Validation loss: 2.1615607738494873

Epoch: 6| Step: 2
Training loss: 1.639294147491455
Validation loss: 2.152122219403585

Epoch: 6| Step: 3
Training loss: 1.2786515951156616
Validation loss: 2.1452261805534363

Epoch: 6| Step: 4
Training loss: 1.5737534761428833
Validation loss: 2.1881773869196572

Epoch: 6| Step: 5
Training loss: 1.8046623468399048
Validation loss: 2.1739067832628884

Epoch: 6| Step: 6
Training loss: 1.1346696615219116
Validation loss: 2.160545547803243

Epoch: 6| Step: 7
Training loss: 1.1705031394958496
Validation loss: 2.1520974238713584

Epoch: 6| Step: 8
Training loss: 1.504723072052002
Validation loss: 2.1474626064300537

Epoch: 6| Step: 9
Training loss: 1.1433318853378296
Validation loss: 2.15738981962204

Epoch: 6| Step: 10
Training loss: 0.8077946901321411
Validation loss: 2.182869573434194

Epoch: 6| Step: 11
Training loss: 2.0388875007629395
Validation loss: 2.1534606218338013

Epoch: 6| Step: 12
Training loss: 1.8165132999420166
Validation loss: 2.165986438592275

Epoch: 6| Step: 13
Training loss: 1.2503482103347778
Validation loss: 2.179074545701345

Epoch: 273| Step: 0
Training loss: 1.0477595329284668
Validation loss: 2.157619833946228

Epoch: 6| Step: 1
Training loss: 1.335573434829712
Validation loss: 2.1696957548459372

Epoch: 6| Step: 2
Training loss: 1.7562766075134277
Validation loss: 2.13957017660141

Epoch: 6| Step: 3
Training loss: 1.2386291027069092
Validation loss: 2.1259543697039285

Epoch: 6| Step: 4
Training loss: 1.0907988548278809
Validation loss: 2.1448023517926535

Epoch: 6| Step: 5
Training loss: 1.62691068649292
Validation loss: 2.149959067503611

Epoch: 6| Step: 6
Training loss: 2.452885866165161
Validation loss: 2.163202166557312

Epoch: 6| Step: 7
Training loss: 1.5457191467285156
Validation loss: 2.168474555015564

Epoch: 6| Step: 8
Training loss: 1.9729437828063965
Validation loss: 2.1786191860834756

Epoch: 6| Step: 9
Training loss: 1.3973143100738525
Validation loss: 2.1707364519437156

Epoch: 6| Step: 10
Training loss: 1.8726744651794434
Validation loss: 2.1533562342325845

Epoch: 6| Step: 11
Training loss: 1.5852999687194824
Validation loss: 2.1825974583625793

Epoch: 6| Step: 12
Training loss: 1.1187736988067627
Validation loss: 2.154613117376963

Epoch: 6| Step: 13
Training loss: 2.037038803100586
Validation loss: 2.1621505419413247

Epoch: 274| Step: 0
Training loss: 1.6103715896606445
Validation loss: 2.1862643559773765

Epoch: 6| Step: 1
Training loss: 1.6356064081192017
Validation loss: 2.157122174898783

Epoch: 6| Step: 2
Training loss: 1.6051496267318726
Validation loss: 2.1897021333376565

Epoch: 6| Step: 3
Training loss: 1.971535563468933
Validation loss: 2.158805767695109

Epoch: 6| Step: 4
Training loss: 1.376412272453308
Validation loss: 2.1633984247843423

Epoch: 6| Step: 5
Training loss: 1.1626739501953125
Validation loss: 2.1764577627182007

Epoch: 6| Step: 6
Training loss: 1.6558642387390137
Validation loss: 2.1574546496073403

Epoch: 6| Step: 7
Training loss: 1.1887590885162354
Validation loss: 2.1957018772761026

Epoch: 6| Step: 8
Training loss: 2.1094393730163574
Validation loss: 2.1770578821500144

Epoch: 6| Step: 9
Training loss: 1.088996171951294
Validation loss: 2.1702160040537515

Epoch: 6| Step: 10
Training loss: 1.6107487678527832
Validation loss: 2.1562230587005615

Epoch: 6| Step: 11
Training loss: 1.3213143348693848
Validation loss: 2.1568532983462014

Epoch: 6| Step: 12
Training loss: 1.3428847789764404
Validation loss: 2.1420393188794455

Epoch: 6| Step: 13
Training loss: 2.0224432945251465
Validation loss: 2.147831896940867

Epoch: 275| Step: 0
Training loss: 1.9328668117523193
Validation loss: 2.1379407246907554

Epoch: 6| Step: 1
Training loss: 1.5553181171417236
Validation loss: 2.148189663887024

Epoch: 6| Step: 2
Training loss: 1.7901504039764404
Validation loss: 2.1331748962402344

Epoch: 6| Step: 3
Training loss: 1.76531982421875
Validation loss: 2.1558621724446616

Epoch: 6| Step: 4
Training loss: 1.4170238971710205
Validation loss: 2.1962903340657554

Epoch: 6| Step: 5
Training loss: 1.4530013799667358
Validation loss: 2.1534090638160706

Epoch: 6| Step: 6
Training loss: 2.1665918827056885
Validation loss: 2.1861987511316934

Epoch: 6| Step: 7
Training loss: 1.1885325908660889
Validation loss: 2.1660172740618386

Epoch: 6| Step: 8
Training loss: 0.8134809732437134
Validation loss: 2.1494496862093606

Epoch: 6| Step: 9
Training loss: 1.2771368026733398
Validation loss: 2.1835520466168723

Epoch: 6| Step: 10
Training loss: 1.3064162731170654
Validation loss: 2.190485695997874

Epoch: 6| Step: 11
Training loss: 1.3789962530136108
Validation loss: 2.175045967102051

Epoch: 6| Step: 12
Training loss: 1.410162329673767
Validation loss: 2.174645642439524

Epoch: 6| Step: 13
Training loss: 2.1365272998809814
Validation loss: 2.164366126060486

Epoch: 276| Step: 0
Training loss: 1.4002888202667236
Validation loss: 2.1595120429992676

Epoch: 6| Step: 1
Training loss: 1.3293683528900146
Validation loss: 2.190058092276255

Epoch: 6| Step: 2
Training loss: 1.491373062133789
Validation loss: 2.1960914532343545

Epoch: 6| Step: 3
Training loss: 1.291849970817566
Validation loss: 2.1504350503285727

Epoch: 6| Step: 4
Training loss: 1.4930238723754883
Validation loss: 2.1714654763539634

Epoch: 6| Step: 5
Training loss: 1.1253247261047363
Validation loss: 2.160104235013326

Epoch: 6| Step: 6
Training loss: 1.6440485715866089
Validation loss: 2.146380603313446

Epoch: 6| Step: 7
Training loss: 1.7010259628295898
Validation loss: 2.148961881796519

Epoch: 6| Step: 8
Training loss: 2.2636163234710693
Validation loss: 2.145492951075236

Epoch: 6| Step: 9
Training loss: 1.844780445098877
Validation loss: 2.171758552392324

Epoch: 6| Step: 10
Training loss: 1.867542028427124
Validation loss: 2.146474083264669

Epoch: 6| Step: 11
Training loss: 2.56394100189209
Validation loss: 2.139693001906077

Epoch: 6| Step: 12
Training loss: 1.350000023841858
Validation loss: 2.1405699650446572

Epoch: 6| Step: 13
Training loss: 1.1075791120529175
Validation loss: 2.138862152894338

Epoch: 277| Step: 0
Training loss: 1.476659893989563
Validation loss: 2.1643193562825522

Epoch: 6| Step: 1
Training loss: 0.9206098914146423
Validation loss: 2.183147350947062

Epoch: 6| Step: 2
Training loss: 1.6813809871673584
Validation loss: 2.1802950501441956

Epoch: 6| Step: 3
Training loss: 1.616851568222046
Validation loss: 2.17013289531072

Epoch: 6| Step: 4
Training loss: 1.4813110828399658
Validation loss: 2.1681145628293357

Epoch: 6| Step: 5
Training loss: 2.022135019302368
Validation loss: 2.179753839969635

Epoch: 6| Step: 6
Training loss: 0.969450831413269
Validation loss: 2.181130071481069

Epoch: 6| Step: 7
Training loss: 2.313215732574463
Validation loss: 2.2098456819852195

Epoch: 6| Step: 8
Training loss: 1.7279481887817383
Validation loss: 2.1839729944864907

Epoch: 6| Step: 9
Training loss: 0.8250779509544373
Validation loss: 2.180864691734314

Epoch: 6| Step: 10
Training loss: 1.2180975675582886
Validation loss: 2.1746652523676553

Epoch: 6| Step: 11
Training loss: 1.2461819648742676
Validation loss: 2.1800256768862405

Epoch: 6| Step: 12
Training loss: 2.059917688369751
Validation loss: 2.1700848937034607

Epoch: 6| Step: 13
Training loss: 1.9000625610351562
Validation loss: 2.178613801797231

Epoch: 278| Step: 0
Training loss: 1.244236946105957
Validation loss: 2.171016494433085

Epoch: 6| Step: 1
Training loss: 1.3908535242080688
Validation loss: 2.177879214286804

Epoch: 6| Step: 2
Training loss: 1.6666769981384277
Validation loss: 2.155168135960897

Epoch: 6| Step: 3
Training loss: 1.6013318300247192
Validation loss: 2.1886123418807983

Epoch: 6| Step: 4
Training loss: 1.3961609601974487
Validation loss: 2.166204790274302

Epoch: 6| Step: 5
Training loss: 1.2368685007095337
Validation loss: 2.1846440633138022

Epoch: 6| Step: 6
Training loss: 1.3877347707748413
Validation loss: 2.1768690745035806

Epoch: 6| Step: 7
Training loss: 1.2944674491882324
Validation loss: 2.1513254841168723

Epoch: 6| Step: 8
Training loss: 1.4901959896087646
Validation loss: 2.1531971295674643

Epoch: 6| Step: 9
Training loss: 1.72666335105896
Validation loss: 2.1530478398005166

Epoch: 6| Step: 10
Training loss: 2.225450277328491
Validation loss: 2.1409213145573935

Epoch: 6| Step: 11
Training loss: 1.7362293004989624
Validation loss: 2.1493887106577554

Epoch: 6| Step: 12
Training loss: 1.8554472923278809
Validation loss: 2.1623169581095376

Epoch: 6| Step: 13
Training loss: 1.486213207244873
Validation loss: 2.172996739546458

Epoch: 279| Step: 0
Training loss: 1.6588809490203857
Validation loss: 2.1402823328971863

Epoch: 6| Step: 1
Training loss: 1.9130761623382568
Validation loss: 2.1729603211085

Epoch: 6| Step: 2
Training loss: 1.634942889213562
Validation loss: 2.1699209014574685

Epoch: 6| Step: 3
Training loss: 1.3080917596817017
Validation loss: 2.167967995007833

Epoch: 6| Step: 4
Training loss: 1.4345571994781494
Validation loss: 2.1171348889668784

Epoch: 6| Step: 5
Training loss: 1.5798412561416626
Validation loss: 2.1617786089579263

Epoch: 6| Step: 6
Training loss: 1.6391994953155518
Validation loss: 2.1362891793251038

Epoch: 6| Step: 7
Training loss: 1.3979159593582153
Validation loss: 2.152474602063497

Epoch: 6| Step: 8
Training loss: 1.0227205753326416
Validation loss: 2.1729499300320945

Epoch: 6| Step: 9
Training loss: 1.399735450744629
Validation loss: 2.1622857451438904

Epoch: 6| Step: 10
Training loss: 2.226189136505127
Validation loss: 2.1728102564811707

Epoch: 6| Step: 11
Training loss: 1.8310225009918213
Validation loss: 2.1769941449165344

Epoch: 6| Step: 12
Training loss: 1.208881139755249
Validation loss: 2.1876739859580994

Epoch: 6| Step: 13
Training loss: 1.2801533937454224
Validation loss: 2.183957497278849

Epoch: 280| Step: 0
Training loss: 1.3264222145080566
Validation loss: 2.190375328063965

Epoch: 6| Step: 1
Training loss: 1.6756833791732788
Validation loss: 2.167539397875468

Epoch: 6| Step: 2
Training loss: 1.0567193031311035
Validation loss: 2.176814317703247

Epoch: 6| Step: 3
Training loss: 1.415429711341858
Validation loss: 2.196199337641398

Epoch: 6| Step: 4
Training loss: 1.7424700260162354
Validation loss: 2.1925031344095864

Epoch: 6| Step: 5
Training loss: 1.3825345039367676
Validation loss: 2.1757980386416116

Epoch: 6| Step: 6
Training loss: 1.368318796157837
Validation loss: 2.1836490631103516

Epoch: 6| Step: 7
Training loss: 1.6182348728179932
Validation loss: 2.159763753414154

Epoch: 6| Step: 8
Training loss: 1.8870203495025635
Validation loss: 2.159587025642395

Epoch: 6| Step: 9
Training loss: 1.5909351110458374
Validation loss: 2.1769688526789346

Epoch: 6| Step: 10
Training loss: 2.1358065605163574
Validation loss: 2.175350824991862

Epoch: 6| Step: 11
Training loss: 1.1547496318817139
Validation loss: 2.1701767643292746

Epoch: 6| Step: 12
Training loss: 1.0313092470169067
Validation loss: 2.1833614706993103

Epoch: 6| Step: 13
Training loss: 1.8030036687850952
Validation loss: 2.173300862312317

Epoch: 281| Step: 0
Training loss: 1.2065229415893555
Validation loss: 2.1849369605382285

Epoch: 6| Step: 1
Training loss: 1.67300283908844
Validation loss: 2.1814602613449097

Epoch: 6| Step: 2
Training loss: 0.944537878036499
Validation loss: 2.1571417450904846

Epoch: 6| Step: 3
Training loss: 1.2373087406158447
Validation loss: 2.144765933354696

Epoch: 6| Step: 4
Training loss: 1.1114336252212524
Validation loss: 2.1492876013120017

Epoch: 6| Step: 5
Training loss: 1.7416220903396606
Validation loss: 2.1475816567738852

Epoch: 6| Step: 6
Training loss: 0.6361994743347168
Validation loss: 2.126814603805542

Epoch: 6| Step: 7
Training loss: 1.7067945003509521
Validation loss: 2.1545194387435913

Epoch: 6| Step: 8
Training loss: 2.407153844833374
Validation loss: 2.1337196230888367

Epoch: 6| Step: 9
Training loss: 1.4981229305267334
Validation loss: 2.1432035764058432

Epoch: 6| Step: 10
Training loss: 2.1646246910095215
Validation loss: 2.131505012512207

Epoch: 6| Step: 11
Training loss: 1.8286913633346558
Validation loss: 2.175366520881653

Epoch: 6| Step: 12
Training loss: 1.550565242767334
Validation loss: 2.1821391185124717

Epoch: 6| Step: 13
Training loss: 1.5956600904464722
Validation loss: 2.1807149251302085

Epoch: 282| Step: 0
Training loss: 1.6304014921188354
Validation loss: 2.219462831815084

Epoch: 6| Step: 1
Training loss: 1.2389965057373047
Validation loss: 2.229316075642904

Epoch: 6| Step: 2
Training loss: 1.8129253387451172
Validation loss: 2.22390870253245

Epoch: 6| Step: 3
Training loss: 1.3801021575927734
Validation loss: 2.195999801158905

Epoch: 6| Step: 4
Training loss: 1.7568752765655518
Validation loss: 2.1630507707595825

Epoch: 6| Step: 5
Training loss: 1.686784029006958
Validation loss: 2.195451080799103

Epoch: 6| Step: 6
Training loss: 1.0399267673492432
Validation loss: 2.1952443520228067

Epoch: 6| Step: 7
Training loss: 2.048008441925049
Validation loss: 2.181293567021688

Epoch: 6| Step: 8
Training loss: 1.8372523784637451
Validation loss: 2.179103672504425

Epoch: 6| Step: 9
Training loss: 1.5759751796722412
Validation loss: 2.1764376958211265

Epoch: 6| Step: 10
Training loss: 1.5642204284667969
Validation loss: 2.1597684820493064

Epoch: 6| Step: 11
Training loss: 0.8002623915672302
Validation loss: 2.1679178277651467

Epoch: 6| Step: 12
Training loss: 1.4864088296890259
Validation loss: 2.1747933626174927

Epoch: 6| Step: 13
Training loss: 2.4865379333496094
Validation loss: 2.163735290368398

Epoch: 283| Step: 0
Training loss: 1.204653024673462
Validation loss: 2.165045181910197

Epoch: 6| Step: 1
Training loss: 1.6340429782867432
Validation loss: 2.1744001507759094

Epoch: 6| Step: 2
Training loss: 1.2800588607788086
Validation loss: 2.1914282043774924

Epoch: 6| Step: 3
Training loss: 1.464777946472168
Validation loss: 2.169000764687856

Epoch: 6| Step: 4
Training loss: 1.1848809719085693
Validation loss: 2.1584407091140747

Epoch: 6| Step: 5
Training loss: 1.325249433517456
Validation loss: 2.17038631439209

Epoch: 6| Step: 6
Training loss: 1.1441410779953003
Validation loss: 2.187761922677358

Epoch: 6| Step: 7
Training loss: 1.7816221714019775
Validation loss: 2.1695514917373657

Epoch: 6| Step: 8
Training loss: 1.3226476907730103
Validation loss: 2.1617455085118613

Epoch: 6| Step: 9
Training loss: 1.9486744403839111
Validation loss: 2.172738194465637

Epoch: 6| Step: 10
Training loss: 1.1084227561950684
Validation loss: 2.167711695035299

Epoch: 6| Step: 11
Training loss: 2.2248473167419434
Validation loss: 2.146457552909851

Epoch: 6| Step: 12
Training loss: 1.1377569437026978
Validation loss: 2.182542383670807

Epoch: 6| Step: 13
Training loss: 2.5136618614196777
Validation loss: 2.162974933783213

Epoch: 284| Step: 0
Training loss: 1.0291590690612793
Validation loss: 2.165766954421997

Epoch: 6| Step: 1
Training loss: 1.6437015533447266
Validation loss: 2.171613574028015

Epoch: 6| Step: 2
Training loss: 1.2140191793441772
Validation loss: 2.1905662218729653

Epoch: 6| Step: 3
Training loss: 1.0964462757110596
Validation loss: 2.172362426916758

Epoch: 6| Step: 4
Training loss: 1.5763044357299805
Validation loss: 2.166438023249308

Epoch: 6| Step: 5
Training loss: 2.347524642944336
Validation loss: 2.1773798863093057

Epoch: 6| Step: 6
Training loss: 1.1763076782226562
Validation loss: 2.1421683629353843

Epoch: 6| Step: 7
Training loss: 1.0752984285354614
Validation loss: 2.173785448074341

Epoch: 6| Step: 8
Training loss: 1.3401801586151123
Validation loss: 2.1831276019414267

Epoch: 6| Step: 9
Training loss: 1.8351316452026367
Validation loss: 2.190113584200541

Epoch: 6| Step: 10
Training loss: 1.5758039951324463
Validation loss: 2.1657400727272034

Epoch: 6| Step: 11
Training loss: 1.5081970691680908
Validation loss: 2.1500186125437417

Epoch: 6| Step: 12
Training loss: 1.9780622720718384
Validation loss: 2.1571004589398703

Epoch: 6| Step: 13
Training loss: 1.5868033170700073
Validation loss: 2.1579264203707376

Epoch: 285| Step: 0
Training loss: 1.8743765354156494
Validation loss: 2.157809635003408

Epoch: 6| Step: 1
Training loss: 1.2271935939788818
Validation loss: 2.1725768645604453

Epoch: 6| Step: 2
Training loss: 2.1006720066070557
Validation loss: 2.1910756826400757

Epoch: 6| Step: 3
Training loss: 1.8096662759780884
Validation loss: 2.1654697259267173

Epoch: 6| Step: 4
Training loss: 1.103581428527832
Validation loss: 2.1552457014719644

Epoch: 6| Step: 5
Training loss: 2.0605974197387695
Validation loss: 2.1421740452448526

Epoch: 6| Step: 6
Training loss: 1.1764317750930786
Validation loss: 2.1581937670707703

Epoch: 6| Step: 7
Training loss: 1.1768162250518799
Validation loss: 2.140266994635264

Epoch: 6| Step: 8
Training loss: 1.2379822731018066
Validation loss: 2.1404075225194297

Epoch: 6| Step: 9
Training loss: 1.762174129486084
Validation loss: 2.1539190212885537

Epoch: 6| Step: 10
Training loss: 1.231353521347046
Validation loss: 2.1282750368118286

Epoch: 6| Step: 11
Training loss: 1.1714324951171875
Validation loss: 2.1245773235956826

Epoch: 6| Step: 12
Training loss: 1.5177240371704102
Validation loss: 2.166982730229696

Epoch: 6| Step: 13
Training loss: 1.3520606756210327
Validation loss: 2.1291774113972983

Epoch: 286| Step: 0
Training loss: 1.1150777339935303
Validation loss: 2.1585837403933206

Epoch: 6| Step: 1
Training loss: 1.646435022354126
Validation loss: 2.1568604906400046

Epoch: 6| Step: 2
Training loss: 1.5765697956085205
Validation loss: 2.166173815727234

Epoch: 6| Step: 3
Training loss: 1.4166553020477295
Validation loss: 2.164660116036733

Epoch: 6| Step: 4
Training loss: 1.4806034564971924
Validation loss: 2.1698020895322165

Epoch: 6| Step: 5
Training loss: 1.424516201019287
Validation loss: 2.1913079420725503

Epoch: 6| Step: 6
Training loss: 1.6290040016174316
Validation loss: 2.173928658167521

Epoch: 6| Step: 7
Training loss: 2.0332136154174805
Validation loss: 2.1764292120933533

Epoch: 6| Step: 8
Training loss: 1.4297927618026733
Validation loss: 2.1987847487131753

Epoch: 6| Step: 9
Training loss: 1.5142732858657837
Validation loss: 2.187349518140157

Epoch: 6| Step: 10
Training loss: 1.116782784461975
Validation loss: 2.195065915584564

Epoch: 6| Step: 11
Training loss: 1.0472524166107178
Validation loss: 2.197319984436035

Epoch: 6| Step: 12
Training loss: 1.9704856872558594
Validation loss: 2.1914882262547812

Epoch: 6| Step: 13
Training loss: 1.6305779218673706
Validation loss: 2.187000811100006

Epoch: 287| Step: 0
Training loss: 1.4720481634140015
Validation loss: 2.1660122076670327

Epoch: 6| Step: 1
Training loss: 0.825608491897583
Validation loss: 2.154606660207113

Epoch: 6| Step: 2
Training loss: 1.4090535640716553
Validation loss: 2.169268250465393

Epoch: 6| Step: 3
Training loss: 1.0380911827087402
Validation loss: 2.163652857144674

Epoch: 6| Step: 4
Training loss: 1.7138142585754395
Validation loss: 2.143389801184336

Epoch: 6| Step: 5
Training loss: 1.0000420808792114
Validation loss: 2.170636216799418

Epoch: 6| Step: 6
Training loss: 1.6204721927642822
Validation loss: 2.165593445301056

Epoch: 6| Step: 7
Training loss: 1.1840271949768066
Validation loss: 2.184812823931376

Epoch: 6| Step: 8
Training loss: 1.4412744045257568
Validation loss: 2.1628761688868203

Epoch: 6| Step: 9
Training loss: 2.6469314098358154
Validation loss: 2.1347124179204306

Epoch: 6| Step: 10
Training loss: 2.4763894081115723
Validation loss: 2.1297508676846824

Epoch: 6| Step: 11
Training loss: 1.5129568576812744
Validation loss: 2.087569753328959

Epoch: 6| Step: 12
Training loss: 1.374672293663025
Validation loss: 2.10798446337382

Epoch: 6| Step: 13
Training loss: 1.7770746946334839
Validation loss: 2.122895618279775

Epoch: 288| Step: 0
Training loss: 0.8418624997138977
Validation loss: 2.149566570917765

Epoch: 6| Step: 1
Training loss: 1.5376418828964233
Validation loss: 2.1763202945391336

Epoch: 6| Step: 2
Training loss: 1.2354496717453003
Validation loss: 2.1599859595298767

Epoch: 6| Step: 3
Training loss: 1.393620491027832
Validation loss: 2.1678483486175537

Epoch: 6| Step: 4
Training loss: 1.288084864616394
Validation loss: 2.1470232009887695

Epoch: 6| Step: 5
Training loss: 1.39204740524292
Validation loss: 2.1637009580930076

Epoch: 6| Step: 6
Training loss: 1.4565101861953735
Validation loss: 2.1478209098180137

Epoch: 6| Step: 7
Training loss: 2.4626305103302
Validation loss: 2.1598939299583435

Epoch: 6| Step: 8
Training loss: 1.2182400226593018
Validation loss: 2.1841355760892234

Epoch: 6| Step: 9
Training loss: 0.9401400089263916
Validation loss: 2.1679553588231406

Epoch: 6| Step: 10
Training loss: 1.9605276584625244
Validation loss: 2.1549522479375205

Epoch: 6| Step: 11
Training loss: 2.020498752593994
Validation loss: 2.1688199440638223

Epoch: 6| Step: 12
Training loss: 1.9805939197540283
Validation loss: 2.1551846464474997

Epoch: 6| Step: 13
Training loss: 2.450259208679199
Validation loss: 2.1469868620236716

Epoch: 289| Step: 0
Training loss: 1.5645883083343506
Validation loss: 2.143858770529429

Epoch: 6| Step: 1
Training loss: 1.8085365295410156
Validation loss: 2.159531056880951

Epoch: 6| Step: 2
Training loss: 1.7569310665130615
Validation loss: 2.136792163054148

Epoch: 6| Step: 3
Training loss: 1.4858264923095703
Validation loss: 2.149632374445597

Epoch: 6| Step: 4
Training loss: 2.2111520767211914
Validation loss: 2.136087874571482

Epoch: 6| Step: 5
Training loss: 1.0748190879821777
Validation loss: 2.1475271185239158

Epoch: 6| Step: 6
Training loss: 1.4006370306015015
Validation loss: 2.1322006781895957

Epoch: 6| Step: 7
Training loss: 1.2364695072174072
Validation loss: 2.114385664463043

Epoch: 6| Step: 8
Training loss: 1.089544415473938
Validation loss: 2.142881711324056

Epoch: 6| Step: 9
Training loss: 0.9347506761550903
Validation loss: 2.1344468792279563

Epoch: 6| Step: 10
Training loss: 1.8640871047973633
Validation loss: 2.147671620051066

Epoch: 6| Step: 11
Training loss: 1.4126770496368408
Validation loss: 2.1470325191815696

Epoch: 6| Step: 12
Training loss: 2.0281660556793213
Validation loss: 2.1771918336550393

Epoch: 6| Step: 13
Training loss: 1.4574493169784546
Validation loss: 2.169516106446584

Epoch: 290| Step: 0
Training loss: 1.275264859199524
Validation loss: 2.1613581577936807

Epoch: 6| Step: 1
Training loss: 1.897587776184082
Validation loss: 2.178548256556193

Epoch: 6| Step: 2
Training loss: 1.0933890342712402
Validation loss: 2.1792002518971763

Epoch: 6| Step: 3
Training loss: 1.6100022792816162
Validation loss: 2.173919995625814

Epoch: 6| Step: 4
Training loss: 1.2949819564819336
Validation loss: 2.168383856614431

Epoch: 6| Step: 5
Training loss: 1.3298149108886719
Validation loss: 2.1531660755475364

Epoch: 6| Step: 6
Training loss: 1.5458627939224243
Validation loss: 2.1403896808624268

Epoch: 6| Step: 7
Training loss: 1.5518498420715332
Validation loss: 2.1405540704727173

Epoch: 6| Step: 8
Training loss: 1.7226554155349731
Validation loss: 2.1380051374435425

Epoch: 6| Step: 9
Training loss: 1.7790234088897705
Validation loss: 2.1489938298861184

Epoch: 6| Step: 10
Training loss: 1.4538012742996216
Validation loss: 2.1371121406555176

Epoch: 6| Step: 11
Training loss: 1.7536128759384155
Validation loss: 2.146101196606954

Epoch: 6| Step: 12
Training loss: 1.799646019935608
Validation loss: 2.1627673506736755

Epoch: 6| Step: 13
Training loss: 0.8804765343666077
Validation loss: 2.158389091491699

Epoch: 291| Step: 0
Training loss: 1.3417232036590576
Validation loss: 2.1665522257486978

Epoch: 6| Step: 1
Training loss: 1.6701792478561401
Validation loss: 2.1754114230473838

Epoch: 6| Step: 2
Training loss: 1.5329511165618896
Validation loss: 2.150236487388611

Epoch: 6| Step: 3
Training loss: 1.7059611082077026
Validation loss: 2.1750600934028625

Epoch: 6| Step: 4
Training loss: 1.28333580493927
Validation loss: 2.2029853065808616

Epoch: 6| Step: 5
Training loss: 1.137657642364502
Validation loss: 2.1757675210634866

Epoch: 6| Step: 6
Training loss: 1.661811113357544
Validation loss: 2.2216580311457315

Epoch: 6| Step: 7
Training loss: 2.4972996711730957
Validation loss: 2.1826846996943154

Epoch: 6| Step: 8
Training loss: 1.2713139057159424
Validation loss: 2.167995810508728

Epoch: 6| Step: 9
Training loss: 1.1915545463562012
Validation loss: 2.1338634888331094

Epoch: 6| Step: 10
Training loss: 1.6291022300720215
Validation loss: 2.1034357945124307

Epoch: 6| Step: 11
Training loss: 1.90486478805542
Validation loss: 2.1214598019917807

Epoch: 6| Step: 12
Training loss: 1.489654541015625
Validation loss: 2.1247988740603128

Epoch: 6| Step: 13
Training loss: 1.1406550407409668
Validation loss: 2.1052923599878945

Epoch: 292| Step: 0
Training loss: 2.461674928665161
Validation loss: 2.1347539822260537

Epoch: 6| Step: 1
Training loss: 1.953950047492981
Validation loss: 2.14378555615743

Epoch: 6| Step: 2
Training loss: 1.1076488494873047
Validation loss: 2.1027477184931436

Epoch: 6| Step: 3
Training loss: 2.207414150238037
Validation loss: 2.1015771627426147

Epoch: 6| Step: 4
Training loss: 1.9906587600708008
Validation loss: 2.070611536502838

Epoch: 6| Step: 5
Training loss: 1.761387825012207
Validation loss: 2.111752529939016

Epoch: 6| Step: 6
Training loss: 1.6300938129425049
Validation loss: 2.14290060599645

Epoch: 6| Step: 7
Training loss: 1.7826471328735352
Validation loss: 2.1178616682688394

Epoch: 6| Step: 8
Training loss: 1.5116783380508423
Validation loss: 2.1499337951342263

Epoch: 6| Step: 9
Training loss: 1.9112937450408936
Validation loss: 2.142391622066498

Epoch: 6| Step: 10
Training loss: 0.9396849870681763
Validation loss: 2.1285793781280518

Epoch: 6| Step: 11
Training loss: 1.401005506515503
Validation loss: 2.1384302576382956

Epoch: 6| Step: 12
Training loss: 1.2369029521942139
Validation loss: 2.1449089845021567

Epoch: 6| Step: 13
Training loss: 1.3559340238571167
Validation loss: 2.1489598552385965

Epoch: 293| Step: 0
Training loss: 1.6926076412200928
Validation loss: 2.1436107953389487

Epoch: 6| Step: 1
Training loss: 1.5782358646392822
Validation loss: 2.181130349636078

Epoch: 6| Step: 2
Training loss: 1.4865195751190186
Validation loss: 2.1967494090398154

Epoch: 6| Step: 3
Training loss: 0.974916934967041
Validation loss: 2.2140116691589355

Epoch: 6| Step: 4
Training loss: 1.6528496742248535
Validation loss: 2.1854902307192483

Epoch: 6| Step: 5
Training loss: 2.3192477226257324
Validation loss: 2.215588609377543

Epoch: 6| Step: 6
Training loss: 1.060997486114502
Validation loss: 2.199869235356649

Epoch: 6| Step: 7
Training loss: 1.5277609825134277
Validation loss: 2.201717972755432

Epoch: 6| Step: 8
Training loss: 1.977630615234375
Validation loss: 2.1959072748819985

Epoch: 6| Step: 9
Training loss: 1.8633837699890137
Validation loss: 2.162182648976644

Epoch: 6| Step: 10
Training loss: 1.8248825073242188
Validation loss: 2.1638541221618652

Epoch: 6| Step: 11
Training loss: 1.285854697227478
Validation loss: 2.187331815560659

Epoch: 6| Step: 12
Training loss: 1.2516262531280518
Validation loss: 2.222737272580465

Epoch: 6| Step: 13
Training loss: 1.3184897899627686
Validation loss: 2.2122366031010947

Epoch: 294| Step: 0
Training loss: 1.5979249477386475
Validation loss: 2.2198323210080466

Epoch: 6| Step: 1
Training loss: 2.5030198097229004
Validation loss: 2.1919442415237427

Epoch: 6| Step: 2
Training loss: 1.8492988348007202
Validation loss: 2.2133158445358276

Epoch: 6| Step: 3
Training loss: 1.5248370170593262
Validation loss: 2.1913729906082153

Epoch: 6| Step: 4
Training loss: 1.4331657886505127
Validation loss: 2.190475324789683

Epoch: 6| Step: 5
Training loss: 1.3801944255828857
Validation loss: 2.142524540424347

Epoch: 6| Step: 6
Training loss: 1.3998031616210938
Validation loss: 2.165287137031555

Epoch: 6| Step: 7
Training loss: 1.1692652702331543
Validation loss: 2.1317277749379477

Epoch: 6| Step: 8
Training loss: 1.5958877801895142
Validation loss: 2.162403881549835

Epoch: 6| Step: 9
Training loss: 2.2326302528381348
Validation loss: 2.178944945335388

Epoch: 6| Step: 10
Training loss: 1.3752338886260986
Validation loss: 2.1404847502708435

Epoch: 6| Step: 11
Training loss: 0.7487310767173767
Validation loss: 2.1411897937456765

Epoch: 6| Step: 12
Training loss: 1.5624014139175415
Validation loss: 2.1653926571210227

Epoch: 6| Step: 13
Training loss: 1.1863678693771362
Validation loss: 2.155137817064921

Epoch: 295| Step: 0
Training loss: 1.7366583347320557
Validation loss: 2.171222507953644

Epoch: 6| Step: 1
Training loss: 1.6432514190673828
Validation loss: 2.1743747194608054

Epoch: 6| Step: 2
Training loss: 1.2965061664581299
Validation loss: 2.1708695888519287

Epoch: 6| Step: 3
Training loss: 1.4245169162750244
Validation loss: 2.1829089323679605

Epoch: 6| Step: 4
Training loss: 2.3427724838256836
Validation loss: 2.1783646941184998

Epoch: 6| Step: 5
Training loss: 1.2687361240386963
Validation loss: 2.167030175526937

Epoch: 6| Step: 6
Training loss: 1.6742782592773438
Validation loss: 2.1369148095448813

Epoch: 6| Step: 7
Training loss: 1.6847553253173828
Validation loss: 2.1470388571421304

Epoch: 6| Step: 8
Training loss: 1.3586422204971313
Validation loss: 2.1436800956726074

Epoch: 6| Step: 9
Training loss: 1.7195703983306885
Validation loss: 2.136014004548391

Epoch: 6| Step: 10
Training loss: 0.887117862701416
Validation loss: 2.1321096618970237

Epoch: 6| Step: 11
Training loss: 1.781140685081482
Validation loss: 2.1018715302149453

Epoch: 6| Step: 12
Training loss: 1.1665217876434326
Validation loss: 2.103293240070343

Epoch: 6| Step: 13
Training loss: 1.331851840019226
Validation loss: 2.125248452027639

Epoch: 296| Step: 0
Training loss: 1.3449361324310303
Validation loss: 2.1228687365849814

Epoch: 6| Step: 1
Training loss: 1.4153361320495605
Validation loss: 2.1539993286132812

Epoch: 6| Step: 2
Training loss: 1.5663175582885742
Validation loss: 2.123257577419281

Epoch: 6| Step: 3
Training loss: 0.9682639241218567
Validation loss: 2.1185585657755532

Epoch: 6| Step: 4
Training loss: 1.4483089447021484
Validation loss: 2.117962956428528

Epoch: 6| Step: 5
Training loss: 1.5339304208755493
Validation loss: 2.059369961420695

Epoch: 6| Step: 6
Training loss: 1.7655606269836426
Validation loss: 2.054331342379252

Epoch: 6| Step: 7
Training loss: 1.8308699131011963
Validation loss: 2.0629578232765198

Epoch: 6| Step: 8
Training loss: 1.8938062191009521
Validation loss: 2.076295475165049

Epoch: 6| Step: 9
Training loss: 2.6138646602630615
Validation loss: 2.0968839526176453

Epoch: 6| Step: 10
Training loss: 1.5396661758422852
Validation loss: 2.0940386851628623

Epoch: 6| Step: 11
Training loss: 1.1511242389678955
Validation loss: 2.071106195449829

Epoch: 6| Step: 12
Training loss: 0.8282875418663025
Validation loss: 2.093469560146332

Epoch: 6| Step: 13
Training loss: 1.401233434677124
Validation loss: 2.126102566719055

Epoch: 297| Step: 0
Training loss: 1.2528908252716064
Validation loss: 2.1114893754323325

Epoch: 6| Step: 1
Training loss: 1.3002513647079468
Validation loss: 2.125478525956472

Epoch: 6| Step: 2
Training loss: 1.4807088375091553
Validation loss: 2.1413097182909646

Epoch: 6| Step: 3
Training loss: 1.2172205448150635
Validation loss: 2.153052806854248

Epoch: 6| Step: 4
Training loss: 1.4441602230072021
Validation loss: 2.1752871672312417

Epoch: 6| Step: 5
Training loss: 1.2462698221206665
Validation loss: 2.1694637139638266

Epoch: 6| Step: 6
Training loss: 2.021449089050293
Validation loss: 2.181129495302836

Epoch: 6| Step: 7
Training loss: 0.9653068780899048
Validation loss: 2.174203356107076

Epoch: 6| Step: 8
Training loss: 2.4579150676727295
Validation loss: 2.1592209935188293

Epoch: 6| Step: 9
Training loss: 1.3600387573242188
Validation loss: 2.1688358386357627

Epoch: 6| Step: 10
Training loss: 1.6673424243927002
Validation loss: 2.164284408092499

Epoch: 6| Step: 11
Training loss: 1.611109733581543
Validation loss: 2.1616361935933432

Epoch: 6| Step: 12
Training loss: 1.4586180448532104
Validation loss: 2.2022244930267334

Epoch: 6| Step: 13
Training loss: 1.583760142326355
Validation loss: 2.2089055379231772

Epoch: 298| Step: 0
Training loss: 1.9604995250701904
Validation loss: 2.1875385840733848

Epoch: 6| Step: 1
Training loss: 2.0283193588256836
Validation loss: 2.191619952519735

Epoch: 6| Step: 2
Training loss: 1.301708459854126
Validation loss: 2.196598708629608

Epoch: 6| Step: 3
Training loss: 0.8053122758865356
Validation loss: 2.21694282690684

Epoch: 6| Step: 4
Training loss: 1.1087642908096313
Validation loss: 2.1728844245274863

Epoch: 6| Step: 5
Training loss: 1.3700764179229736
Validation loss: 2.1950820088386536

Epoch: 6| Step: 6
Training loss: 1.9389846324920654
Validation loss: 2.1693855126698813

Epoch: 6| Step: 7
Training loss: 1.8172955513000488
Validation loss: 2.162528177102407

Epoch: 6| Step: 8
Training loss: 0.9770359992980957
Validation loss: 2.1355851689974465

Epoch: 6| Step: 9
Training loss: 1.1284406185150146
Validation loss: 2.135863423347473

Epoch: 6| Step: 10
Training loss: 1.51947021484375
Validation loss: 2.1395493547121682

Epoch: 6| Step: 11
Training loss: 2.18074369430542
Validation loss: 2.1288133462270102

Epoch: 6| Step: 12
Training loss: 1.6870616674423218
Validation loss: 2.1177559892336526

Epoch: 6| Step: 13
Training loss: 1.340501308441162
Validation loss: 2.1150132020314536

Epoch: 299| Step: 0
Training loss: 1.591156005859375
Validation loss: 2.1390475829442344

Epoch: 6| Step: 1
Training loss: 0.9510740637779236
Validation loss: 2.1156413356463113

Epoch: 6| Step: 2
Training loss: 2.018993854522705
Validation loss: 2.1777210036913552

Epoch: 6| Step: 3
Training loss: 0.8910738229751587
Validation loss: 2.1535680890083313

Epoch: 6| Step: 4
Training loss: 1.9764974117279053
Validation loss: 2.1756417949994407

Epoch: 6| Step: 5
Training loss: 1.2530572414398193
Validation loss: 2.1626713275909424

Epoch: 6| Step: 6
Training loss: 1.4865776300430298
Validation loss: 2.1704362432161965

Epoch: 6| Step: 7
Training loss: 1.5342903137207031
Validation loss: 2.1884730458259583

Epoch: 6| Step: 8
Training loss: 1.4244356155395508
Validation loss: 2.155067582925161

Epoch: 6| Step: 9
Training loss: 1.991489291191101
Validation loss: 2.1318902174631753

Epoch: 6| Step: 10
Training loss: 0.9915450811386108
Validation loss: 2.1423544883728027

Epoch: 6| Step: 11
Training loss: 1.5500640869140625
Validation loss: 2.1384921272595725

Epoch: 6| Step: 12
Training loss: 2.0135107040405273
Validation loss: 2.1516401370366416

Epoch: 6| Step: 13
Training loss: 1.0515385866165161
Validation loss: 2.1531878312428794

Epoch: 300| Step: 0
Training loss: 1.3308899402618408
Validation loss: 2.1666650772094727

Epoch: 6| Step: 1
Training loss: 1.9270365238189697
Validation loss: 2.1162461638450623

Epoch: 6| Step: 2
Training loss: 0.6721845865249634
Validation loss: 2.1289926369984946

Epoch: 6| Step: 3
Training loss: 1.7102441787719727
Validation loss: 2.1206390261650085

Epoch: 6| Step: 4
Training loss: 1.5640614032745361
Validation loss: 2.097530702749888

Epoch: 6| Step: 5
Training loss: 1.065307378768921
Validation loss: 2.170666515827179

Epoch: 6| Step: 6
Training loss: 1.3610830307006836
Validation loss: 2.128533720970154

Epoch: 6| Step: 7
Training loss: 1.648928165435791
Validation loss: 2.1260108153025308

Epoch: 6| Step: 8
Training loss: 2.1885008811950684
Validation loss: 2.1252132256825766

Epoch: 6| Step: 9
Training loss: 1.6356185674667358
Validation loss: 2.100850224494934

Epoch: 6| Step: 10
Training loss: 1.4111287593841553
Validation loss: 2.0921663840611777

Epoch: 6| Step: 11
Training loss: 2.057661771774292
Validation loss: 2.117546876271566

Epoch: 6| Step: 12
Training loss: 0.9598566293716431
Validation loss: 2.1293468276659646

Epoch: 6| Step: 13
Training loss: 1.197277545928955
Validation loss: 2.0763349334398904

Testing loss: 1.9031932405430636

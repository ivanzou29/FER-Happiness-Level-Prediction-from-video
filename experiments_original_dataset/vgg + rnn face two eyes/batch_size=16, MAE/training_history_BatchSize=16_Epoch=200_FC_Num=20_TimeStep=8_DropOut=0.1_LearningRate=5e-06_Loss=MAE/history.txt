Epoch: 1| Step: 0
Training loss: 5.769548416137695
Validation loss: 5.262137095133464

Epoch: 6| Step: 1
Training loss: 4.911385536193848
Validation loss: 5.260169982910156

Epoch: 6| Step: 2
Training loss: 5.621269702911377
Validation loss: 5.258071422576904

Epoch: 6| Step: 3
Training loss: 5.150754928588867
Validation loss: 5.256062110265096

Epoch: 6| Step: 4
Training loss: 5.0620222091674805
Validation loss: 5.254113117853801

Epoch: 6| Step: 5
Training loss: 5.946187496185303
Validation loss: 5.252103487650554

Epoch: 6| Step: 6
Training loss: 5.688171863555908
Validation loss: 5.250118811925252

Epoch: 6| Step: 7
Training loss: 4.293728828430176
Validation loss: 5.248037417729695

Epoch: 6| Step: 8
Training loss: 6.072961807250977
Validation loss: 5.2461763223012285

Epoch: 6| Step: 9
Training loss: 4.874800205230713
Validation loss: 5.244198640187581

Epoch: 6| Step: 10
Training loss: 4.493706703186035
Validation loss: 5.242183764775594

Epoch: 6| Step: 11
Training loss: 4.808868408203125
Validation loss: 5.2400572299957275

Epoch: 6| Step: 12
Training loss: 5.716713905334473
Validation loss: 5.237883806228638

Epoch: 6| Step: 13
Training loss: 6.047969818115234
Validation loss: 5.235533952713013

Epoch: 2| Step: 0
Training loss: 6.18717098236084
Validation loss: 5.233271598815918

Epoch: 6| Step: 1
Training loss: 5.770546913146973
Validation loss: 5.2307101885477705

Epoch: 6| Step: 2
Training loss: 6.392299652099609
Validation loss: 5.228275378545125

Epoch: 6| Step: 3
Training loss: 4.871896743774414
Validation loss: 5.2253332535425825

Epoch: 6| Step: 4
Training loss: 5.3516316413879395
Validation loss: 5.222723960876465

Epoch: 6| Step: 5
Training loss: 5.2786993980407715
Validation loss: 5.219741980234782

Epoch: 6| Step: 6
Training loss: 4.58450984954834
Validation loss: 5.216741641362508

Epoch: 6| Step: 7
Training loss: 4.8527116775512695
Validation loss: 5.213540712992351

Epoch: 6| Step: 8
Training loss: 4.935503959655762
Validation loss: 5.210326035817464

Epoch: 6| Step: 9
Training loss: 5.188689708709717
Validation loss: 5.206689516703288

Epoch: 6| Step: 10
Training loss: 4.9250006675720215
Validation loss: 5.203223466873169

Epoch: 6| Step: 11
Training loss: 5.206219673156738
Validation loss: 5.199456135431926

Epoch: 6| Step: 12
Training loss: 4.984039783477783
Validation loss: 5.195502201716105

Epoch: 6| Step: 13
Training loss: 5.450000762939453
Validation loss: 5.191240469614665

Epoch: 3| Step: 0
Training loss: 4.423978805541992
Validation loss: 5.186899423599243

Epoch: 6| Step: 1
Training loss: 5.313227653503418
Validation loss: 5.182390968004863

Epoch: 6| Step: 2
Training loss: 4.737424850463867
Validation loss: 5.177833636601766

Epoch: 6| Step: 3
Training loss: 5.767158031463623
Validation loss: 5.172852595647176

Epoch: 6| Step: 4
Training loss: 5.876830101013184
Validation loss: 5.16756288210551

Epoch: 6| Step: 5
Training loss: 5.1794633865356445
Validation loss: 5.162422736485799

Epoch: 6| Step: 6
Training loss: 5.593318939208984
Validation loss: 5.1565611362457275

Epoch: 6| Step: 7
Training loss: 4.365202903747559
Validation loss: 5.150555690129598

Epoch: 6| Step: 8
Training loss: 5.160121440887451
Validation loss: 5.144784768422444

Epoch: 6| Step: 9
Training loss: 4.687603950500488
Validation loss: 5.138325134913127

Epoch: 6| Step: 10
Training loss: 4.771300315856934
Validation loss: 5.1315282980601

Epoch: 6| Step: 11
Training loss: 5.2623395919799805
Validation loss: 5.124593178431193

Epoch: 6| Step: 12
Training loss: 5.3576226234436035
Validation loss: 5.117482821146647

Epoch: 6| Step: 13
Training loss: 6.631716728210449
Validation loss: 5.10991628964742

Epoch: 4| Step: 0
Training loss: 5.551543235778809
Validation loss: 5.102508544921875

Epoch: 6| Step: 1
Training loss: 4.999640941619873
Validation loss: 5.094303131103516

Epoch: 6| Step: 2
Training loss: 5.915830612182617
Validation loss: 5.0865059693654375

Epoch: 6| Step: 3
Training loss: 4.53156852722168
Validation loss: 5.078050931294759

Epoch: 6| Step: 4
Training loss: 5.832924842834473
Validation loss: 5.069544553756714

Epoch: 6| Step: 5
Training loss: 5.252174377441406
Validation loss: 5.060680468877156

Epoch: 6| Step: 6
Training loss: 5.013002395629883
Validation loss: 5.051735242207845

Epoch: 6| Step: 7
Training loss: 5.386188983917236
Validation loss: 5.042856772740682

Epoch: 6| Step: 8
Training loss: 5.091840744018555
Validation loss: 5.033647537231445

Epoch: 6| Step: 9
Training loss: 5.540752410888672
Validation loss: 5.024553616841634

Epoch: 6| Step: 10
Training loss: 4.778552055358887
Validation loss: 5.014649391174316

Epoch: 6| Step: 11
Training loss: 4.700300216674805
Validation loss: 5.005354086558024

Epoch: 6| Step: 12
Training loss: 4.777519702911377
Validation loss: 4.9954259395599365

Epoch: 6| Step: 13
Training loss: 4.356404781341553
Validation loss: 4.985417763392131

Epoch: 5| Step: 0
Training loss: 3.792215347290039
Validation loss: 4.975308974583943

Epoch: 6| Step: 1
Training loss: 4.569678783416748
Validation loss: 4.965412378311157

Epoch: 6| Step: 2
Training loss: 5.497766971588135
Validation loss: 4.955723524093628

Epoch: 6| Step: 3
Training loss: 5.599388599395752
Validation loss: 4.94582732518514

Epoch: 6| Step: 4
Training loss: 5.135341644287109
Validation loss: 4.935959180196126

Epoch: 6| Step: 5
Training loss: 4.917059421539307
Validation loss: 4.926168203353882

Epoch: 6| Step: 6
Training loss: 5.415160655975342
Validation loss: 4.916726430257161

Epoch: 6| Step: 7
Training loss: 4.234192848205566
Validation loss: 4.9068606694539385

Epoch: 6| Step: 8
Training loss: 4.657442092895508
Validation loss: 4.897350390752156

Epoch: 6| Step: 9
Training loss: 4.881741523742676
Validation loss: 4.887883583704631

Epoch: 6| Step: 10
Training loss: 5.944609642028809
Validation loss: 4.87851619720459

Epoch: 6| Step: 11
Training loss: 4.764717102050781
Validation loss: 4.869208415349324

Epoch: 6| Step: 12
Training loss: 5.3975749015808105
Validation loss: 4.859593391418457

Epoch: 6| Step: 13
Training loss: 5.139991760253906
Validation loss: 4.850245714187622

Epoch: 6| Step: 0
Training loss: 5.177874565124512
Validation loss: 4.840563217798869

Epoch: 6| Step: 1
Training loss: 4.630141258239746
Validation loss: 4.830479065577189

Epoch: 6| Step: 2
Training loss: 5.769198417663574
Validation loss: 4.8210147221883135

Epoch: 6| Step: 3
Training loss: 6.285532474517822
Validation loss: 4.811236619949341

Epoch: 6| Step: 4
Training loss: 5.019579887390137
Validation loss: 4.801391879717509

Epoch: 6| Step: 5
Training loss: 4.6681976318359375
Validation loss: 4.791463534037272

Epoch: 6| Step: 6
Training loss: 5.528322219848633
Validation loss: 4.782054583231608

Epoch: 6| Step: 7
Training loss: 3.5767459869384766
Validation loss: 4.772035837173462

Epoch: 6| Step: 8
Training loss: 3.9617860317230225
Validation loss: 4.7627958456675215

Epoch: 6| Step: 9
Training loss: 4.760026931762695
Validation loss: 4.753936251004537

Epoch: 6| Step: 10
Training loss: 4.652637481689453
Validation loss: 4.744892915089925

Epoch: 6| Step: 11
Training loss: 4.893042087554932
Validation loss: 4.735776464144389

Epoch: 6| Step: 12
Training loss: 4.6241960525512695
Validation loss: 4.726683775583903

Epoch: 6| Step: 13
Training loss: 4.6157073974609375
Validation loss: 4.71779735883077

Epoch: 7| Step: 0
Training loss: 4.082406997680664
Validation loss: 4.709016640981038

Epoch: 6| Step: 1
Training loss: 3.8636255264282227
Validation loss: 4.699855764706929

Epoch: 6| Step: 2
Training loss: 5.177746772766113
Validation loss: 4.691312034924825

Epoch: 6| Step: 3
Training loss: 4.667625904083252
Validation loss: 4.681861559549968

Epoch: 6| Step: 4
Training loss: 4.3652496337890625
Validation loss: 4.673608700434367

Epoch: 6| Step: 5
Training loss: 5.990871906280518
Validation loss: 4.664841651916504

Epoch: 6| Step: 6
Training loss: 4.419234275817871
Validation loss: 4.6554332574208575

Epoch: 6| Step: 7
Training loss: 4.15880012512207
Validation loss: 4.646343111991882

Epoch: 6| Step: 8
Training loss: 5.279387474060059
Validation loss: 4.636384129524231

Epoch: 6| Step: 9
Training loss: 4.651140213012695
Validation loss: 4.627245982487996

Epoch: 6| Step: 10
Training loss: 5.201045036315918
Validation loss: 4.616984566052754

Epoch: 6| Step: 11
Training loss: 4.566729545593262
Validation loss: 4.608381907145183

Epoch: 6| Step: 12
Training loss: 4.655505180358887
Validation loss: 4.599602619806926

Epoch: 6| Step: 13
Training loss: 5.383143901824951
Validation loss: 4.591428558031718

Epoch: 8| Step: 0
Training loss: 4.3319292068481445
Validation loss: 4.583063999811809

Epoch: 6| Step: 1
Training loss: 5.054747581481934
Validation loss: 4.575364430745442

Epoch: 6| Step: 2
Training loss: 4.717695713043213
Validation loss: 4.567054947217305

Epoch: 6| Step: 3
Training loss: 5.188439846038818
Validation loss: 4.5586174329121905

Epoch: 6| Step: 4
Training loss: 4.364882469177246
Validation loss: 4.550365447998047

Epoch: 6| Step: 5
Training loss: 4.974309921264648
Validation loss: 4.542954126993815

Epoch: 6| Step: 6
Training loss: 5.247433662414551
Validation loss: 4.53533951441447

Epoch: 6| Step: 7
Training loss: 5.576360702514648
Validation loss: 4.5285303592681885

Epoch: 6| Step: 8
Training loss: 5.674805164337158
Validation loss: 4.521706263224284

Epoch: 6| Step: 9
Training loss: 3.7739334106445312
Validation loss: 4.514759222666423

Epoch: 6| Step: 10
Training loss: 2.8054757118225098
Validation loss: 4.508336067199707

Epoch: 6| Step: 11
Training loss: 2.9915478229522705
Validation loss: 4.502221902211507

Epoch: 6| Step: 12
Training loss: 5.396951675415039
Validation loss: 4.4954460461934405

Epoch: 6| Step: 13
Training loss: 4.869566917419434
Validation loss: 4.489664236704509

Epoch: 9| Step: 0
Training loss: 4.1592607498168945
Validation loss: 4.4833574295043945

Epoch: 6| Step: 1
Training loss: 5.594114780426025
Validation loss: 4.47727370262146

Epoch: 6| Step: 2
Training loss: 4.7165374755859375
Validation loss: 4.471622149149577

Epoch: 6| Step: 3
Training loss: 3.932131290435791
Validation loss: 4.465687115987142

Epoch: 6| Step: 4
Training loss: 4.313946723937988
Validation loss: 4.459283828735352

Epoch: 6| Step: 5
Training loss: 4.647549152374268
Validation loss: 4.453639388084412

Epoch: 6| Step: 6
Training loss: 4.977248668670654
Validation loss: 4.447649399439494

Epoch: 6| Step: 7
Training loss: 4.188194751739502
Validation loss: 4.440812587738037

Epoch: 6| Step: 8
Training loss: 5.150806427001953
Validation loss: 4.434733748435974

Epoch: 6| Step: 9
Training loss: 3.717041492462158
Validation loss: 4.428982734680176

Epoch: 6| Step: 10
Training loss: 4.425605297088623
Validation loss: 4.423673311869304

Epoch: 6| Step: 11
Training loss: 4.276267051696777
Validation loss: 4.418351054191589

Epoch: 6| Step: 12
Training loss: 5.04732608795166
Validation loss: 4.41223684946696

Epoch: 6| Step: 13
Training loss: 4.679227352142334
Validation loss: 4.406223813692729

Epoch: 10| Step: 0
Training loss: 5.074061393737793
Validation loss: 4.399731596310933

Epoch: 6| Step: 1
Training loss: 3.6574928760528564
Validation loss: 4.392429749170939

Epoch: 6| Step: 2
Training loss: 4.866382598876953
Validation loss: 4.386091510454814

Epoch: 6| Step: 3
Training loss: 4.368454933166504
Validation loss: 4.3795329332351685

Epoch: 6| Step: 4
Training loss: 4.313267707824707
Validation loss: 4.373702883720398

Epoch: 6| Step: 5
Training loss: 4.541238784790039
Validation loss: 4.366177638371785

Epoch: 6| Step: 6
Training loss: 4.515953540802002
Validation loss: 4.359631419181824

Epoch: 6| Step: 7
Training loss: 4.400035858154297
Validation loss: 4.354004422823588

Epoch: 6| Step: 8
Training loss: 4.559329032897949
Validation loss: 4.347659945487976

Epoch: 6| Step: 9
Training loss: 4.428660869598389
Validation loss: 4.341107130050659

Epoch: 6| Step: 10
Training loss: 4.587937831878662
Validation loss: 4.335998455683391

Epoch: 6| Step: 11
Training loss: 4.061811447143555
Validation loss: 4.3302944501241045

Epoch: 6| Step: 12
Training loss: 4.742053031921387
Validation loss: 4.325293183326721

Epoch: 6| Step: 13
Training loss: 4.576757431030273
Validation loss: 4.318837563196818

Epoch: 11| Step: 0
Training loss: 3.5837883949279785
Validation loss: 4.313248793284099

Epoch: 6| Step: 1
Training loss: 4.6486687660217285
Validation loss: 4.307677149772644

Epoch: 6| Step: 2
Training loss: 4.834015369415283
Validation loss: 4.301583965619405

Epoch: 6| Step: 3
Training loss: 5.315121650695801
Validation loss: 4.295315980911255

Epoch: 6| Step: 4
Training loss: 5.063913345336914
Validation loss: 4.2898455063502

Epoch: 6| Step: 5
Training loss: 4.150043487548828
Validation loss: 4.284260590871175

Epoch: 6| Step: 6
Training loss: 3.878277063369751
Validation loss: 4.278265039126079

Epoch: 6| Step: 7
Training loss: 4.461551189422607
Validation loss: 4.272056937217712

Epoch: 6| Step: 8
Training loss: 5.056861877441406
Validation loss: 4.2663073142369585

Epoch: 6| Step: 9
Training loss: 3.529557704925537
Validation loss: 4.2607502937316895

Epoch: 6| Step: 10
Training loss: 5.069430351257324
Validation loss: 4.255981524785359

Epoch: 6| Step: 11
Training loss: 3.922985315322876
Validation loss: 4.250688791275024

Epoch: 6| Step: 12
Training loss: 4.195919990539551
Validation loss: 4.245278636614482

Epoch: 6| Step: 13
Training loss: 3.960421562194824
Validation loss: 4.239407142003377

Epoch: 12| Step: 0
Training loss: 3.2845582962036133
Validation loss: 4.234043439229329

Epoch: 6| Step: 1
Training loss: 4.93215274810791
Validation loss: 4.227219382921855

Epoch: 6| Step: 2
Training loss: 4.473725318908691
Validation loss: 4.221228003501892

Epoch: 6| Step: 3
Training loss: 4.877220153808594
Validation loss: 4.2160641352335615

Epoch: 6| Step: 4
Training loss: 3.9616599082946777
Validation loss: 4.2099833488464355

Epoch: 6| Step: 5
Training loss: 3.490576982498169
Validation loss: 4.204291820526123

Epoch: 6| Step: 6
Training loss: 3.4495961666107178
Validation loss: 4.199426333109538

Epoch: 6| Step: 7
Training loss: 4.882587432861328
Validation loss: 4.195254564285278

Epoch: 6| Step: 8
Training loss: 4.141354560852051
Validation loss: 4.188916325569153

Epoch: 6| Step: 9
Training loss: 3.272068977355957
Validation loss: 4.181897282600403

Epoch: 6| Step: 10
Training loss: 5.282736301422119
Validation loss: 4.175405581792195

Epoch: 6| Step: 11
Training loss: 5.023003578186035
Validation loss: 4.170684138933818

Epoch: 6| Step: 12
Training loss: 4.763924598693848
Validation loss: 4.165734767913818

Epoch: 6| Step: 13
Training loss: 4.772433280944824
Validation loss: 4.1594310601552325

Epoch: 13| Step: 0
Training loss: 4.88601541519165
Validation loss: 4.1538980801900225

Epoch: 6| Step: 1
Training loss: 3.786825180053711
Validation loss: 4.147404392560323

Epoch: 6| Step: 2
Training loss: 4.8064680099487305
Validation loss: 4.142418464024861

Epoch: 6| Step: 3
Training loss: 2.9359095096588135
Validation loss: 4.137657682100932

Epoch: 6| Step: 4
Training loss: 4.376791954040527
Validation loss: 4.132609486579895

Epoch: 6| Step: 5
Training loss: 3.893158435821533
Validation loss: 4.127047538757324

Epoch: 6| Step: 6
Training loss: 4.483792304992676
Validation loss: 4.121834437052409

Epoch: 6| Step: 7
Training loss: 4.452589511871338
Validation loss: 4.115988930066426

Epoch: 6| Step: 8
Training loss: 4.469684600830078
Validation loss: 4.1111241181691485

Epoch: 6| Step: 9
Training loss: 4.417460918426514
Validation loss: 4.105698108673096

Epoch: 6| Step: 10
Training loss: 4.064610004425049
Validation loss: 4.099821249643962

Epoch: 6| Step: 11
Training loss: 4.162082672119141
Validation loss: 4.094651420911153

Epoch: 6| Step: 12
Training loss: 4.017386436462402
Validation loss: 4.090617895126343

Epoch: 6| Step: 13
Training loss: 4.862343788146973
Validation loss: 4.0860774119695025

Epoch: 14| Step: 0
Training loss: 3.7333452701568604
Validation loss: 4.081254283587138

Epoch: 6| Step: 1
Training loss: 4.208700180053711
Validation loss: 4.075287580490112

Epoch: 6| Step: 2
Training loss: 2.950425148010254
Validation loss: 4.068748474121094

Epoch: 6| Step: 3
Training loss: 4.989627838134766
Validation loss: 4.064229408899943

Epoch: 6| Step: 4
Training loss: 4.4323835372924805
Validation loss: 4.059412558873494

Epoch: 6| Step: 5
Training loss: 4.3528218269348145
Validation loss: 4.054434140523274

Epoch: 6| Step: 6
Training loss: 4.624482154846191
Validation loss: 4.049516240755717

Epoch: 6| Step: 7
Training loss: 3.692195415496826
Validation loss: 4.044920921325684

Epoch: 6| Step: 8
Training loss: 3.8943161964416504
Validation loss: 4.04032023747762

Epoch: 6| Step: 9
Training loss: 4.465423583984375
Validation loss: 4.03262996673584

Epoch: 6| Step: 10
Training loss: 4.552541732788086
Validation loss: 4.02777886390686

Epoch: 6| Step: 11
Training loss: 3.5693564414978027
Validation loss: 4.022244771321614

Epoch: 6| Step: 12
Training loss: 4.701934814453125
Validation loss: 4.016773184140523

Epoch: 6| Step: 13
Training loss: 4.458887100219727
Validation loss: 4.011095762252808

Epoch: 15| Step: 0
Training loss: 4.415447235107422
Validation loss: 4.006064295768738

Epoch: 6| Step: 1
Training loss: 4.7612996101379395
Validation loss: 4.000465472539266

Epoch: 6| Step: 2
Training loss: 5.371984481811523
Validation loss: 3.995340426762899

Epoch: 6| Step: 3
Training loss: 4.602469444274902
Validation loss: 3.9898195266723633

Epoch: 6| Step: 4
Training loss: 3.753732919692993
Validation loss: 3.9839085737864175

Epoch: 6| Step: 5
Training loss: 4.078768253326416
Validation loss: 3.9788372913996377

Epoch: 6| Step: 6
Training loss: 3.9305601119995117
Validation loss: 3.9734050035476685

Epoch: 6| Step: 7
Training loss: 3.965169906616211
Validation loss: 3.9677565892537436

Epoch: 6| Step: 8
Training loss: 3.6281075477600098
Validation loss: 3.9625300963719687

Epoch: 6| Step: 9
Training loss: 3.2126317024230957
Validation loss: 3.9578808546066284

Epoch: 6| Step: 10
Training loss: 3.0309741497039795
Validation loss: 3.9532771507898965

Epoch: 6| Step: 11
Training loss: 4.231047630310059
Validation loss: 3.948751449584961

Epoch: 6| Step: 12
Training loss: 4.418286323547363
Validation loss: 3.9431859652201333

Epoch: 6| Step: 13
Training loss: 4.256775856018066
Validation loss: 3.937938610712687

Epoch: 16| Step: 0
Training loss: 4.891103744506836
Validation loss: 3.9338923692703247

Epoch: 6| Step: 1
Training loss: 4.123476028442383
Validation loss: 3.9275527397791543

Epoch: 6| Step: 2
Training loss: 3.665107488632202
Validation loss: 3.9228166341781616

Epoch: 6| Step: 3
Training loss: 3.238676071166992
Validation loss: 3.919133186340332

Epoch: 6| Step: 4
Training loss: 4.643373489379883
Validation loss: 3.91490638256073

Epoch: 6| Step: 5
Training loss: 3.4347498416900635
Validation loss: 3.9083652893702188

Epoch: 6| Step: 6
Training loss: 4.708443641662598
Validation loss: 3.9031649033228555

Epoch: 6| Step: 7
Training loss: 4.288229942321777
Validation loss: 3.899056315422058

Epoch: 6| Step: 8
Training loss: 3.7775514125823975
Validation loss: 3.8942665259043374

Epoch: 6| Step: 9
Training loss: 4.3596296310424805
Validation loss: 3.8909945090611777

Epoch: 6| Step: 10
Training loss: 4.610050201416016
Validation loss: 3.8842478593190513

Epoch: 6| Step: 11
Training loss: 3.5692877769470215
Validation loss: 3.8804598649342856

Epoch: 6| Step: 12
Training loss: 2.7064123153686523
Validation loss: 3.8781466086705527

Epoch: 6| Step: 13
Training loss: 4.709033012390137
Validation loss: 3.871618628501892

Epoch: 17| Step: 0
Training loss: 3.3501248359680176
Validation loss: 3.864715814590454

Epoch: 6| Step: 1
Training loss: 3.6018905639648438
Validation loss: 3.8623298009236655

Epoch: 6| Step: 2
Training loss: 4.072124004364014
Validation loss: 3.858695864677429

Epoch: 6| Step: 3
Training loss: 3.5075385570526123
Validation loss: 3.8548255364100137

Epoch: 6| Step: 4
Training loss: 3.6837143898010254
Validation loss: 3.849598526954651

Epoch: 6| Step: 5
Training loss: 3.0418198108673096
Validation loss: 3.8438610235850015

Epoch: 6| Step: 6
Training loss: 5.180696487426758
Validation loss: 3.839192748069763

Epoch: 6| Step: 7
Training loss: 5.207525730133057
Validation loss: 3.8364082177480063

Epoch: 6| Step: 8
Training loss: 3.9004247188568115
Validation loss: 3.8326515754063926

Epoch: 6| Step: 9
Training loss: 4.497829914093018
Validation loss: 3.8270007371902466

Epoch: 6| Step: 10
Training loss: 4.356943607330322
Validation loss: 3.8205689589182534

Epoch: 6| Step: 11
Training loss: 3.7577450275421143
Validation loss: 3.8165603478749595

Epoch: 6| Step: 12
Training loss: 4.2051100730896
Validation loss: 3.8140653371810913

Epoch: 6| Step: 13
Training loss: 3.4941744804382324
Validation loss: 3.811943451563517

Epoch: 18| Step: 0
Training loss: 4.634256362915039
Validation loss: 3.8056979179382324

Epoch: 6| Step: 1
Training loss: 2.9346842765808105
Validation loss: 3.8006781339645386

Epoch: 6| Step: 2
Training loss: 3.8050925731658936
Validation loss: 3.7942533095677695

Epoch: 6| Step: 3
Training loss: 3.729271411895752
Validation loss: 3.7887158393859863

Epoch: 6| Step: 4
Training loss: 3.8612701892852783
Validation loss: 3.7846950689951577

Epoch: 6| Step: 5
Training loss: 3.884192705154419
Validation loss: 3.7807012796401978

Epoch: 6| Step: 6
Training loss: 4.2824177742004395
Validation loss: 3.7761051257451377

Epoch: 6| Step: 7
Training loss: 4.157022476196289
Validation loss: 3.7710397640864053

Epoch: 6| Step: 8
Training loss: 3.2449798583984375
Validation loss: 3.7668351332346597

Epoch: 6| Step: 9
Training loss: 3.758887767791748
Validation loss: 3.7623842557271323

Epoch: 6| Step: 10
Training loss: 4.0613813400268555
Validation loss: 3.758816719055176

Epoch: 6| Step: 11
Training loss: 3.833845376968384
Validation loss: 3.7530173858006797

Epoch: 6| Step: 12
Training loss: 4.751400947570801
Validation loss: 3.7481528520584106

Epoch: 6| Step: 13
Training loss: 4.083491325378418
Validation loss: 3.743448575337728

Epoch: 19| Step: 0
Training loss: 3.800027370452881
Validation loss: 3.739992698033651

Epoch: 6| Step: 1
Training loss: 3.863877296447754
Validation loss: 3.734496613343557

Epoch: 6| Step: 2
Training loss: 3.720292091369629
Validation loss: 3.730469743410746

Epoch: 6| Step: 3
Training loss: 4.260519027709961
Validation loss: 3.726019342740377

Epoch: 6| Step: 4
Training loss: 3.703200340270996
Validation loss: 3.7218662897745767

Epoch: 6| Step: 5
Training loss: 3.7189688682556152
Validation loss: 3.7182681560516357

Epoch: 6| Step: 6
Training loss: 3.5205817222595215
Validation loss: 3.7145888010660806

Epoch: 6| Step: 7
Training loss: 3.224923849105835
Validation loss: 3.709868232409159

Epoch: 6| Step: 8
Training loss: 3.2102882862091064
Validation loss: 3.706225355466207

Epoch: 6| Step: 9
Training loss: 3.447676658630371
Validation loss: 3.7016395330429077

Epoch: 6| Step: 10
Training loss: 4.4338226318359375
Validation loss: 3.696846922238668

Epoch: 6| Step: 11
Training loss: 4.7307586669921875
Validation loss: 3.693630655606588

Epoch: 6| Step: 12
Training loss: 4.335487365722656
Validation loss: 3.6917137702306113

Epoch: 6| Step: 13
Training loss: 4.230258941650391
Validation loss: 3.6860474348068237

Epoch: 20| Step: 0
Training loss: 3.2426304817199707
Validation loss: 3.6803104082743325

Epoch: 6| Step: 1
Training loss: 3.419938087463379
Validation loss: 3.676081379254659

Epoch: 6| Step: 2
Training loss: 4.720850944519043
Validation loss: 3.671786983807882

Epoch: 6| Step: 3
Training loss: 3.96577787399292
Validation loss: 3.669358770052592

Epoch: 6| Step: 4
Training loss: 3.3503310680389404
Validation loss: 3.664353887240092

Epoch: 6| Step: 5
Training loss: 4.434866428375244
Validation loss: 3.6598466634750366

Epoch: 6| Step: 6
Training loss: 3.6072919368743896
Validation loss: 3.655602137247721

Epoch: 6| Step: 7
Training loss: 3.1232337951660156
Validation loss: 3.650858720143636

Epoch: 6| Step: 8
Training loss: 3.9586148262023926
Validation loss: 3.647164543469747

Epoch: 6| Step: 9
Training loss: 3.137876272201538
Validation loss: 3.643889546394348

Epoch: 6| Step: 10
Training loss: 4.996392726898193
Validation loss: 3.641967018445333

Epoch: 6| Step: 11
Training loss: 4.327605724334717
Validation loss: 3.6350683768590293

Epoch: 6| Step: 12
Training loss: 3.7210958003997803
Validation loss: 3.6295241912206015

Epoch: 6| Step: 13
Training loss: 3.3945159912109375
Validation loss: 3.625543713569641

Epoch: 21| Step: 0
Training loss: 4.2301459312438965
Validation loss: 3.6224222580591836

Epoch: 6| Step: 1
Training loss: 4.880532264709473
Validation loss: 3.6182570457458496

Epoch: 6| Step: 2
Training loss: 3.983529567718506
Validation loss: 3.6143043835957847

Epoch: 6| Step: 3
Training loss: 3.271772861480713
Validation loss: 3.609102964401245

Epoch: 6| Step: 4
Training loss: 4.397241592407227
Validation loss: 3.6042768557866416

Epoch: 6| Step: 5
Training loss: 3.209514617919922
Validation loss: 3.598953644434611

Epoch: 6| Step: 6
Training loss: 3.215155601501465
Validation loss: 3.594482978185018

Epoch: 6| Step: 7
Training loss: 3.6974985599517822
Validation loss: 3.5914822816848755

Epoch: 6| Step: 8
Training loss: 3.0635781288146973
Validation loss: 3.5892518361409507

Epoch: 6| Step: 9
Training loss: 4.277081489562988
Validation loss: 3.582581361134847

Epoch: 6| Step: 10
Training loss: 2.6136183738708496
Validation loss: 3.577308495839437

Epoch: 6| Step: 11
Training loss: 4.151174068450928
Validation loss: 3.5743165810902915

Epoch: 6| Step: 12
Training loss: 3.657761573791504
Validation loss: 3.5710395177205405

Epoch: 6| Step: 13
Training loss: 3.960969924926758
Validation loss: 3.5687973499298096

Epoch: 22| Step: 0
Training loss: 4.228089332580566
Validation loss: 3.5650572379430137

Epoch: 6| Step: 1
Training loss: 4.230457305908203
Validation loss: 3.5572204987208047

Epoch: 6| Step: 2
Training loss: 3.8713722229003906
Validation loss: 3.5525867144266763

Epoch: 6| Step: 3
Training loss: 4.751133918762207
Validation loss: 3.5478277603785195

Epoch: 6| Step: 4
Training loss: 4.093562602996826
Validation loss: 3.543003877003988

Epoch: 6| Step: 5
Training loss: 3.6224570274353027
Validation loss: 3.539689819018046

Epoch: 6| Step: 6
Training loss: 3.0750555992126465
Validation loss: 3.5355315605799356

Epoch: 6| Step: 7
Training loss: 3.4493470191955566
Validation loss: 3.530661940574646

Epoch: 6| Step: 8
Training loss: 4.1052374839782715
Validation loss: 3.5259797970453897

Epoch: 6| Step: 9
Training loss: 2.8318068981170654
Validation loss: 3.5204705794652305

Epoch: 6| Step: 10
Training loss: 3.8478660583496094
Validation loss: 3.5162466367085776

Epoch: 6| Step: 11
Training loss: 3.802013635635376
Validation loss: 3.5114010175069175

Epoch: 6| Step: 12
Training loss: 3.1506247520446777
Validation loss: 3.5075019200642905

Epoch: 6| Step: 13
Training loss: 2.752573013305664
Validation loss: 3.5025171041488647

Epoch: 23| Step: 0
Training loss: 3.892439365386963
Validation loss: 3.499508857727051

Epoch: 6| Step: 1
Training loss: 3.379335403442383
Validation loss: 3.4962404568990073

Epoch: 6| Step: 2
Training loss: 4.071096897125244
Validation loss: 3.4917688369750977

Epoch: 6| Step: 3
Training loss: 4.560811996459961
Validation loss: 3.4881176153818765

Epoch: 6| Step: 4
Training loss: 3.513861894607544
Validation loss: 3.4825284481048584

Epoch: 6| Step: 5
Training loss: 3.703662633895874
Validation loss: 3.478370706240336

Epoch: 6| Step: 6
Training loss: 4.291510105133057
Validation loss: 3.4738112688064575

Epoch: 6| Step: 7
Training loss: 3.9799964427948
Validation loss: 3.4686425924301147

Epoch: 6| Step: 8
Training loss: 2.945006847381592
Validation loss: 3.4640000661214194

Epoch: 6| Step: 9
Training loss: 3.006472587585449
Validation loss: 3.4589645067850747

Epoch: 6| Step: 10
Training loss: 3.6051223278045654
Validation loss: 3.454590400060018

Epoch: 6| Step: 11
Training loss: 3.4984259605407715
Validation loss: 3.4504310687383017

Epoch: 6| Step: 12
Training loss: 3.1317591667175293
Validation loss: 3.445894956588745

Epoch: 6| Step: 13
Training loss: 3.358177661895752
Validation loss: 3.441953738530477

Epoch: 24| Step: 0
Training loss: 3.686716318130493
Validation loss: 3.4378134409586587

Epoch: 6| Step: 1
Training loss: 3.9332780838012695
Validation loss: 3.4329686959584556

Epoch: 6| Step: 2
Training loss: 3.6181228160858154
Validation loss: 3.429286559422811

Epoch: 6| Step: 3
Training loss: 3.892608642578125
Validation loss: 3.4260520935058594

Epoch: 6| Step: 4
Training loss: 3.2112526893615723
Validation loss: 3.4223015705744424

Epoch: 6| Step: 5
Training loss: 3.875441551208496
Validation loss: 3.420251806577047

Epoch: 6| Step: 6
Training loss: 3.9945950508117676
Validation loss: 3.4159082969029746

Epoch: 6| Step: 7
Training loss: 2.852421760559082
Validation loss: 3.408240556716919

Epoch: 6| Step: 8
Training loss: 2.8453269004821777
Validation loss: 3.4052902857462564

Epoch: 6| Step: 9
Training loss: 3.5201892852783203
Validation loss: 3.4003988107045493

Epoch: 6| Step: 10
Training loss: 3.8811964988708496
Validation loss: 3.397156993548075

Epoch: 6| Step: 11
Training loss: 3.2720985412597656
Validation loss: 3.3929521640141806

Epoch: 6| Step: 12
Training loss: 4.3559465408325195
Validation loss: 3.3892311255137124

Epoch: 6| Step: 13
Training loss: 3.1546177864074707
Validation loss: 3.3862237532933555

Epoch: 25| Step: 0
Training loss: 4.407489776611328
Validation loss: 3.3812415599823

Epoch: 6| Step: 1
Training loss: 3.5895440578460693
Validation loss: 3.377294977506002

Epoch: 6| Step: 2
Training loss: 4.629669189453125
Validation loss: 3.3698394298553467

Epoch: 6| Step: 3
Training loss: 4.049274921417236
Validation loss: 3.367072025934855

Epoch: 6| Step: 4
Training loss: 3.338131904602051
Validation loss: 3.360673427581787

Epoch: 6| Step: 5
Training loss: 3.8290867805480957
Validation loss: 3.3569517532984414

Epoch: 6| Step: 6
Training loss: 3.9156532287597656
Validation loss: 3.3537528912226358

Epoch: 6| Step: 7
Training loss: 2.8294506072998047
Validation loss: 3.348924239476522

Epoch: 6| Step: 8
Training loss: 2.749955654144287
Validation loss: 3.345121741294861

Epoch: 6| Step: 9
Training loss: 3.0671579837799072
Validation loss: 3.339178760846456

Epoch: 6| Step: 10
Training loss: 2.778944969177246
Validation loss: 3.3348031441370645

Epoch: 6| Step: 11
Training loss: 3.1230015754699707
Validation loss: 3.3302272160847983

Epoch: 6| Step: 12
Training loss: 3.5332751274108887
Validation loss: 3.3268869320551553

Epoch: 6| Step: 13
Training loss: 3.500187635421753
Validation loss: 3.3216633001963296

Epoch: 26| Step: 0
Training loss: 4.0939130783081055
Validation loss: 3.318459630012512

Epoch: 6| Step: 1
Training loss: 3.4437217712402344
Validation loss: 3.31427001953125

Epoch: 6| Step: 2
Training loss: 4.310027122497559
Validation loss: 3.310939351717631

Epoch: 6| Step: 3
Training loss: 3.9514589309692383
Validation loss: 3.304985841115316

Epoch: 6| Step: 4
Training loss: 3.703644275665283
Validation loss: 3.302321990331014

Epoch: 6| Step: 5
Training loss: 3.382103443145752
Validation loss: 3.2980600595474243

Epoch: 6| Step: 6
Training loss: 2.951306104660034
Validation loss: 3.293650468190511

Epoch: 6| Step: 7
Training loss: 2.635308265686035
Validation loss: 3.2898298501968384

Epoch: 6| Step: 8
Training loss: 3.1182944774627686
Validation loss: 3.2861889600753784

Epoch: 6| Step: 9
Training loss: 2.6229958534240723
Validation loss: 3.284119804700216

Epoch: 6| Step: 10
Training loss: 3.397460460662842
Validation loss: 3.278956731160482

Epoch: 6| Step: 11
Training loss: 4.3002028465271
Validation loss: 3.2758538325627646

Epoch: 6| Step: 12
Training loss: 2.505168914794922
Validation loss: 3.2718449036280313

Epoch: 6| Step: 13
Training loss: 4.115124702453613
Validation loss: 3.267803192138672

Epoch: 27| Step: 0
Training loss: 3.3102164268493652
Validation loss: 3.2638816038767495

Epoch: 6| Step: 1
Training loss: 3.4316248893737793
Validation loss: 3.260156194368998

Epoch: 6| Step: 2
Training loss: 3.2468302249908447
Validation loss: 3.2553979953130088

Epoch: 6| Step: 3
Training loss: 3.6298017501831055
Validation loss: 3.252190868059794

Epoch: 6| Step: 4
Training loss: 2.982804536819458
Validation loss: 3.2467411756515503

Epoch: 6| Step: 5
Training loss: 3.286120653152466
Validation loss: 3.243407408396403

Epoch: 6| Step: 6
Training loss: 3.3220102787017822
Validation loss: 3.2393590211868286

Epoch: 6| Step: 7
Training loss: 3.9868645668029785
Validation loss: 3.234850287437439

Epoch: 6| Step: 8
Training loss: 3.644711494445801
Validation loss: 3.231741706530253

Epoch: 6| Step: 9
Training loss: 2.8459110260009766
Validation loss: 3.2273559967676797

Epoch: 6| Step: 10
Training loss: 3.5165889263153076
Validation loss: 3.223150372505188

Epoch: 6| Step: 11
Training loss: 3.264887809753418
Validation loss: 3.2195218404134116

Epoch: 6| Step: 12
Training loss: 3.3609745502471924
Validation loss: 3.21586283047994

Epoch: 6| Step: 13
Training loss: 3.973580837249756
Validation loss: 3.2113017241160073

Epoch: 28| Step: 0
Training loss: 3.3627824783325195
Validation loss: 3.207316239674886

Epoch: 6| Step: 1
Training loss: 3.389090061187744
Validation loss: 3.2037514050801597

Epoch: 6| Step: 2
Training loss: 2.981809616088867
Validation loss: 3.1979827086130777

Epoch: 6| Step: 3
Training loss: 3.717555046081543
Validation loss: 3.194204409917196

Epoch: 6| Step: 4
Training loss: 3.1991138458251953
Validation loss: 3.1903046369552612

Epoch: 6| Step: 5
Training loss: 2.4925687313079834
Validation loss: 3.1858448584874473

Epoch: 6| Step: 6
Training loss: 3.1118075847625732
Validation loss: 3.181008219718933

Epoch: 6| Step: 7
Training loss: 2.714853525161743
Validation loss: 3.1785003344217935

Epoch: 6| Step: 8
Training loss: 4.369691848754883
Validation loss: 3.1740575234095254

Epoch: 6| Step: 9
Training loss: 2.7309365272521973
Validation loss: 3.1715358098347983

Epoch: 6| Step: 10
Training loss: 3.7917256355285645
Validation loss: 3.166995366414388

Epoch: 6| Step: 11
Training loss: 2.852205276489258
Validation loss: 3.162689487139384

Epoch: 6| Step: 12
Training loss: 3.922772169113159
Validation loss: 3.1588418086369834

Epoch: 6| Step: 13
Training loss: 4.428752422332764
Validation loss: 3.1540846029917398

Epoch: 29| Step: 0
Training loss: 2.13470458984375
Validation loss: 3.150342027346293

Epoch: 6| Step: 1
Training loss: 3.2271149158477783
Validation loss: 3.1455589135487876

Epoch: 6| Step: 2
Training loss: 3.210512399673462
Validation loss: 3.140881141026815

Epoch: 6| Step: 3
Training loss: 3.8772876262664795
Validation loss: 3.1393193006515503

Epoch: 6| Step: 4
Training loss: 3.8260679244995117
Validation loss: 3.1343343257904053

Epoch: 6| Step: 5
Training loss: 3.0924789905548096
Validation loss: 3.129956086476644

Epoch: 6| Step: 6
Training loss: 3.182361125946045
Validation loss: 3.125784993171692

Epoch: 6| Step: 7
Training loss: 3.6753947734832764
Validation loss: 3.122470815976461

Epoch: 6| Step: 8
Training loss: 3.0467822551727295
Validation loss: 3.118383526802063

Epoch: 6| Step: 9
Training loss: 3.0394439697265625
Validation loss: 3.1151809692382812

Epoch: 6| Step: 10
Training loss: 4.278077602386475
Validation loss: 3.11005691687266

Epoch: 6| Step: 11
Training loss: 3.1248233318328857
Validation loss: 3.1074295043945312

Epoch: 6| Step: 12
Training loss: 2.552135944366455
Validation loss: 3.102650284767151

Epoch: 6| Step: 13
Training loss: 4.082037925720215
Validation loss: 3.099076747894287

Epoch: 30| Step: 0
Training loss: 3.3684630393981934
Validation loss: 3.095133980115255

Epoch: 6| Step: 1
Training loss: 3.447533369064331
Validation loss: 3.0943066279093423

Epoch: 6| Step: 2
Training loss: 2.531193256378174
Validation loss: 3.0873337984085083

Epoch: 6| Step: 3
Training loss: 2.836052417755127
Validation loss: 3.0854146480560303

Epoch: 6| Step: 4
Training loss: 3.4019880294799805
Validation loss: 3.079927841822306

Epoch: 6| Step: 5
Training loss: 3.554811477661133
Validation loss: 3.0762337843577066

Epoch: 6| Step: 6
Training loss: 2.579683780670166
Validation loss: 3.0735865434010825

Epoch: 6| Step: 7
Training loss: 3.897181510925293
Validation loss: 3.0698045094807944

Epoch: 6| Step: 8
Training loss: 3.4483489990234375
Validation loss: 3.065764546394348

Epoch: 6| Step: 9
Training loss: 3.477813959121704
Validation loss: 3.063028256098429

Epoch: 6| Step: 10
Training loss: 3.932023525238037
Validation loss: 3.059259295463562

Epoch: 6| Step: 11
Training loss: 2.5239787101745605
Validation loss: 3.056083083152771

Epoch: 6| Step: 12
Training loss: 3.445733070373535
Validation loss: 3.0518733263015747

Epoch: 6| Step: 13
Training loss: 3.193617820739746
Validation loss: 3.0477821429570517

Epoch: 31| Step: 0
Training loss: 3.4040935039520264
Validation loss: 3.043941855430603

Epoch: 6| Step: 1
Training loss: 4.914609909057617
Validation loss: 3.04083788394928

Epoch: 6| Step: 2
Training loss: 3.4839067459106445
Validation loss: 3.0367644627889

Epoch: 6| Step: 3
Training loss: 3.079294204711914
Validation loss: 3.0328685442606607

Epoch: 6| Step: 4
Training loss: 2.859804391860962
Validation loss: 3.027968406677246

Epoch: 6| Step: 5
Training loss: 3.2735519409179688
Validation loss: 3.0255072116851807

Epoch: 6| Step: 6
Training loss: 3.064626932144165
Validation loss: 3.021645267804464

Epoch: 6| Step: 7
Training loss: 2.7604475021362305
Validation loss: 3.018809715906779

Epoch: 6| Step: 8
Training loss: 1.9674471616744995
Validation loss: 3.0166663328806558

Epoch: 6| Step: 9
Training loss: 3.1777634620666504
Validation loss: 3.0150394439697266

Epoch: 6| Step: 10
Training loss: 4.120963096618652
Validation loss: 3.012789706389109

Epoch: 6| Step: 11
Training loss: 3.4108664989471436
Validation loss: 3.0056764284769693

Epoch: 6| Step: 12
Training loss: 2.180802345275879
Validation loss: 3.0013792912165322

Epoch: 6| Step: 13
Training loss: 3.280998706817627
Validation loss: 2.996236046155294

Epoch: 32| Step: 0
Training loss: 2.452730417251587
Validation loss: 2.993382533391317

Epoch: 6| Step: 1
Training loss: 2.774052619934082
Validation loss: 2.990851640701294

Epoch: 6| Step: 2
Training loss: 3.3590407371520996
Validation loss: 2.9888981183369956

Epoch: 6| Step: 3
Training loss: 3.242891788482666
Validation loss: 2.9874213139216104

Epoch: 6| Step: 4
Training loss: 3.2002274990081787
Validation loss: 2.980686624844869

Epoch: 6| Step: 5
Training loss: 3.205559253692627
Validation loss: 2.9776390393575034

Epoch: 6| Step: 6
Training loss: 3.0695719718933105
Validation loss: 2.974573075771332

Epoch: 6| Step: 7
Training loss: 3.4952495098114014
Validation loss: 2.9731353918711343

Epoch: 6| Step: 8
Training loss: 3.418102979660034
Validation loss: 2.9693379402160645

Epoch: 6| Step: 9
Training loss: 3.402992010116577
Validation loss: 2.962904373804728

Epoch: 6| Step: 10
Training loss: 3.5016396045684814
Validation loss: 2.959242502848307

Epoch: 6| Step: 11
Training loss: 2.9071950912475586
Validation loss: 2.9562050898869834

Epoch: 6| Step: 12
Training loss: 3.43062686920166
Validation loss: 2.9554684162139893

Epoch: 6| Step: 13
Training loss: 2.8667654991149902
Validation loss: 2.951280196507772

Epoch: 33| Step: 0
Training loss: 3.4607207775115967
Validation loss: 2.9488263924916587

Epoch: 6| Step: 1
Training loss: 3.909985303878784
Validation loss: 2.9446882804234824

Epoch: 6| Step: 2
Training loss: 2.6031460762023926
Validation loss: 2.9407349030176797

Epoch: 6| Step: 3
Training loss: 3.481898546218872
Validation loss: 2.9368913571039834

Epoch: 6| Step: 4
Training loss: 3.7135231494903564
Validation loss: 2.9338507652282715

Epoch: 6| Step: 5
Training loss: 3.7163519859313965
Validation loss: 2.9297892252604165

Epoch: 6| Step: 6
Training loss: 3.586630344390869
Validation loss: 2.925406813621521

Epoch: 6| Step: 7
Training loss: 2.830078125
Validation loss: 2.9216814835866294

Epoch: 6| Step: 8
Training loss: 3.2122912406921387
Validation loss: 2.9180507262547812

Epoch: 6| Step: 9
Training loss: 2.8451199531555176
Validation loss: 2.9138432343800864

Epoch: 6| Step: 10
Training loss: 2.301107883453369
Validation loss: 2.910852630933126

Epoch: 6| Step: 11
Training loss: 2.954244375228882
Validation loss: 2.9077365398406982

Epoch: 6| Step: 12
Training loss: 2.4679436683654785
Validation loss: 2.903476039568583

Epoch: 6| Step: 13
Training loss: 2.7016348838806152
Validation loss: 2.9002811511357627

Epoch: 34| Step: 0
Training loss: 2.806544303894043
Validation loss: 2.89792799949646

Epoch: 6| Step: 1
Training loss: 2.6972031593322754
Validation loss: 2.8973076343536377

Epoch: 6| Step: 2
Training loss: 2.557126760482788
Validation loss: 2.89220921198527

Epoch: 6| Step: 3
Training loss: 3.604278087615967
Validation loss: 2.8907204469045005

Epoch: 6| Step: 4
Training loss: 2.7518246173858643
Validation loss: 2.8850922187169394

Epoch: 6| Step: 5
Training loss: 3.461315631866455
Validation loss: 2.880938768386841

Epoch: 6| Step: 6
Training loss: 2.723170042037964
Validation loss: 2.8768699963887534

Epoch: 6| Step: 7
Training loss: 3.0105113983154297
Validation loss: 2.8752228418986

Epoch: 6| Step: 8
Training loss: 3.223353624343872
Validation loss: 2.8714821338653564

Epoch: 6| Step: 9
Training loss: 2.6577134132385254
Validation loss: 2.8686683177948

Epoch: 6| Step: 10
Training loss: 3.25640869140625
Validation loss: 2.8665522734324136

Epoch: 6| Step: 11
Training loss: 2.722071647644043
Validation loss: 2.861879905064901

Epoch: 6| Step: 12
Training loss: 4.312261581420898
Validation loss: 2.860025962193807

Epoch: 6| Step: 13
Training loss: 3.3870558738708496
Validation loss: 2.856188416481018

Epoch: 35| Step: 0
Training loss: 3.3780648708343506
Validation loss: 2.854802449544271

Epoch: 6| Step: 1
Training loss: 3.8653767108917236
Validation loss: 2.850116411844889

Epoch: 6| Step: 2
Training loss: 3.3852880001068115
Validation loss: 2.848686615626017

Epoch: 6| Step: 3
Training loss: 3.596228837966919
Validation loss: 2.843936045964559

Epoch: 6| Step: 4
Training loss: 2.8637068271636963
Validation loss: 2.8415437936782837

Epoch: 6| Step: 5
Training loss: 2.6799962520599365
Validation loss: 2.838050603866577

Epoch: 6| Step: 6
Training loss: 2.5835957527160645
Validation loss: 2.834339420000712

Epoch: 6| Step: 7
Training loss: 2.660579204559326
Validation loss: 2.8319970766703286

Epoch: 6| Step: 8
Training loss: 2.878577709197998
Validation loss: 2.8305238485336304

Epoch: 6| Step: 9
Training loss: 3.171473741531372
Validation loss: 2.8275535504023233

Epoch: 6| Step: 10
Training loss: 2.5367140769958496
Validation loss: 2.8254935344060264

Epoch: 6| Step: 11
Training loss: 3.1098313331604004
Validation loss: 2.8215347131093345

Epoch: 6| Step: 12
Training loss: 3.1003036499023438
Validation loss: 2.8160722255706787

Epoch: 6| Step: 13
Training loss: 2.790855646133423
Validation loss: 2.813337246576945

Epoch: 36| Step: 0
Training loss: 2.3083558082580566
Validation loss: 2.8108888467152915

Epoch: 6| Step: 1
Training loss: 3.7379415035247803
Validation loss: 2.8075661261876426

Epoch: 6| Step: 2
Training loss: 2.4359912872314453
Validation loss: 2.8051148653030396

Epoch: 6| Step: 3
Training loss: 3.356313467025757
Validation loss: 2.8009299437204995

Epoch: 6| Step: 4
Training loss: 3.1318845748901367
Validation loss: 2.799445390701294

Epoch: 6| Step: 5
Training loss: 3.258369207382202
Validation loss: 2.7964421113332114

Epoch: 6| Step: 6
Training loss: 2.5553741455078125
Validation loss: 2.793521285057068

Epoch: 6| Step: 7
Training loss: 2.987142562866211
Validation loss: 2.7897794246673584

Epoch: 6| Step: 8
Training loss: 4.057849407196045
Validation loss: 2.7862577040990195

Epoch: 6| Step: 9
Training loss: 3.0715444087982178
Validation loss: 2.7848037083943686

Epoch: 6| Step: 10
Training loss: 2.399433135986328
Validation loss: 2.780470371246338

Epoch: 6| Step: 11
Training loss: 2.793548822402954
Validation loss: 2.777690609296163

Epoch: 6| Step: 12
Training loss: 3.514249801635742
Validation loss: 2.777736028035482

Epoch: 6| Step: 13
Training loss: 2.441850185394287
Validation loss: 2.779261310895284

Epoch: 37| Step: 0
Training loss: 3.0806002616882324
Validation loss: 2.7694161335627236

Epoch: 6| Step: 1
Training loss: 2.676254987716675
Validation loss: 2.766183535257975

Epoch: 6| Step: 2
Training loss: 3.3791089057922363
Validation loss: 2.76288108030955

Epoch: 6| Step: 3
Training loss: 3.2939488887786865
Validation loss: 2.7614156007766724

Epoch: 6| Step: 4
Training loss: 4.354628086090088
Validation loss: 2.758651773134867

Epoch: 6| Step: 5
Training loss: 2.4339284896850586
Validation loss: 2.756428082784017

Epoch: 6| Step: 6
Training loss: 2.8673391342163086
Validation loss: 2.7548591693242392

Epoch: 6| Step: 7
Training loss: 2.8873634338378906
Validation loss: 2.7514774799346924

Epoch: 6| Step: 8
Training loss: 3.2023181915283203
Validation loss: 2.7462120850880942

Epoch: 6| Step: 9
Training loss: 2.5852203369140625
Validation loss: 2.7432574033737183

Epoch: 6| Step: 10
Training loss: 2.780940532684326
Validation loss: 2.7414482831954956

Epoch: 6| Step: 11
Training loss: 3.6782708168029785
Validation loss: 2.738796671231588

Epoch: 6| Step: 12
Training loss: 2.1823179721832275
Validation loss: 2.7343222300211587

Epoch: 6| Step: 13
Training loss: 2.0504229068756104
Validation loss: 2.732832749684652

Epoch: 38| Step: 0
Training loss: 3.1547765731811523
Validation loss: 2.7287354866663613

Epoch: 6| Step: 1
Training loss: 2.8544342517852783
Validation loss: 2.7271485726038613

Epoch: 6| Step: 2
Training loss: 2.9327385425567627
Validation loss: 2.722598751386007

Epoch: 6| Step: 3
Training loss: 2.1194260120391846
Validation loss: 2.720045189062754

Epoch: 6| Step: 4
Training loss: 3.588151216506958
Validation loss: 2.7191359202067056

Epoch: 6| Step: 5
Training loss: 2.423178195953369
Validation loss: 2.7150829633076987

Epoch: 6| Step: 6
Training loss: 2.842661142349243
Validation loss: 2.713009317715963

Epoch: 6| Step: 7
Training loss: 2.986466407775879
Validation loss: 2.714161435763041

Epoch: 6| Step: 8
Training loss: 3.022252082824707
Validation loss: 2.709683616956075

Epoch: 6| Step: 9
Training loss: 3.676173210144043
Validation loss: 2.7034862438837686

Epoch: 6| Step: 10
Training loss: 2.550875663757324
Validation loss: 2.699598173300425

Epoch: 6| Step: 11
Training loss: 3.076725482940674
Validation loss: 2.698178768157959

Epoch: 6| Step: 12
Training loss: 3.0091984272003174
Validation loss: 2.696080207824707

Epoch: 6| Step: 13
Training loss: 2.60042142868042
Validation loss: 2.6904635032018027

Epoch: 39| Step: 0
Training loss: 2.7348198890686035
Validation loss: 2.6910258134206138

Epoch: 6| Step: 1
Training loss: 3.3589541912078857
Validation loss: 2.6886559327443442

Epoch: 6| Step: 2
Training loss: 3.113062620162964
Validation loss: 2.687980055809021

Epoch: 6| Step: 3
Training loss: 2.2822866439819336
Validation loss: 2.685689091682434

Epoch: 6| Step: 4
Training loss: 2.9304137229919434
Validation loss: 2.6829529205958047

Epoch: 6| Step: 5
Training loss: 3.210505485534668
Validation loss: 2.677365481853485

Epoch: 6| Step: 6
Training loss: 2.3688135147094727
Validation loss: 2.672884225845337

Epoch: 6| Step: 7
Training loss: 3.804140090942383
Validation loss: 2.6710947354634604

Epoch: 6| Step: 8
Training loss: 3.0202364921569824
Validation loss: 2.667359153429667

Epoch: 6| Step: 9
Training loss: 2.687244415283203
Validation loss: 2.6641995708147683

Epoch: 6| Step: 10
Training loss: 3.437971591949463
Validation loss: 2.660419543584188

Epoch: 6| Step: 11
Training loss: 2.3953237533569336
Validation loss: 2.657293121019999

Epoch: 6| Step: 12
Training loss: 2.5665717124938965
Validation loss: 2.6557198762893677

Epoch: 6| Step: 13
Training loss: 2.376962184906006
Validation loss: 2.650194684664408

Epoch: 40| Step: 0
Training loss: 2.687213897705078
Validation loss: 2.648739735285441

Epoch: 6| Step: 1
Training loss: 3.4322447776794434
Validation loss: 2.648403604825338

Epoch: 6| Step: 2
Training loss: 2.748920440673828
Validation loss: 2.647708257039388

Epoch: 6| Step: 3
Training loss: 3.2059593200683594
Validation loss: 2.641709009806315

Epoch: 6| Step: 4
Training loss: 1.7158130407333374
Validation loss: 2.6377354860305786

Epoch: 6| Step: 5
Training loss: 1.7975326776504517
Validation loss: 2.6339749097824097

Epoch: 6| Step: 6
Training loss: 2.75665283203125
Validation loss: 2.634200612703959

Epoch: 6| Step: 7
Training loss: 3.143228054046631
Validation loss: 2.6316877404848733

Epoch: 6| Step: 8
Training loss: 2.763533115386963
Validation loss: 2.6274565060933432

Epoch: 6| Step: 9
Training loss: 3.2204036712646484
Validation loss: 2.6283170779546103

Epoch: 6| Step: 10
Training loss: 3.0792670249938965
Validation loss: 2.6213921308517456

Epoch: 6| Step: 11
Training loss: 3.7708675861358643
Validation loss: 2.6202044089635215

Epoch: 6| Step: 12
Training loss: 2.656202793121338
Validation loss: 2.6148506800333657

Epoch: 6| Step: 13
Training loss: 2.6400699615478516
Validation loss: 2.612651308377584

Epoch: 41| Step: 0
Training loss: 2.9830803871154785
Validation loss: 2.6094674865404763

Epoch: 6| Step: 1
Training loss: 2.3290326595306396
Validation loss: 2.6086275974909463

Epoch: 6| Step: 2
Training loss: 2.3250272274017334
Validation loss: 2.6072566509246826

Epoch: 6| Step: 3
Training loss: 2.7600085735321045
Validation loss: 2.617666482925415

Epoch: 6| Step: 4
Training loss: 3.143975257873535
Validation loss: 2.622186064720154

Epoch: 6| Step: 5
Training loss: 3.3590049743652344
Validation loss: 2.59648334980011

Epoch: 6| Step: 6
Training loss: 3.1572351455688477
Validation loss: 2.592952609062195

Epoch: 6| Step: 7
Training loss: 2.613369941711426
Validation loss: 2.59232767422994

Epoch: 6| Step: 8
Training loss: 2.522256374359131
Validation loss: 2.5944241682688394

Epoch: 6| Step: 9
Training loss: 2.66646146774292
Validation loss: 2.6105288664499917

Epoch: 6| Step: 10
Training loss: 2.8570473194122314
Validation loss: 2.5979055166244507

Epoch: 6| Step: 11
Training loss: 2.8773248195648193
Validation loss: 2.584361751874288

Epoch: 6| Step: 12
Training loss: 2.566014289855957
Validation loss: 2.576405922571818

Epoch: 6| Step: 13
Training loss: 3.0290768146514893
Validation loss: 2.5697046915690103

Epoch: 42| Step: 0
Training loss: 2.681114912033081
Validation loss: 2.5711872975031533

Epoch: 6| Step: 1
Training loss: 2.8561630249023438
Validation loss: 2.5734243392944336

Epoch: 6| Step: 2
Training loss: 2.9461567401885986
Validation loss: 2.575978954633077

Epoch: 6| Step: 3
Training loss: 3.140483856201172
Validation loss: 2.5734852155049643

Epoch: 6| Step: 4
Training loss: 2.8926284313201904
Validation loss: 2.5694067080815635

Epoch: 6| Step: 5
Training loss: 2.799177646636963
Validation loss: 2.56468673547109

Epoch: 6| Step: 6
Training loss: 2.0611929893493652
Validation loss: 2.5573331713676453

Epoch: 6| Step: 7
Training loss: 2.6326990127563477
Validation loss: 2.5529942512512207

Epoch: 6| Step: 8
Training loss: 3.3627054691314697
Validation loss: 2.5489906867345176

Epoch: 6| Step: 9
Training loss: 3.3822543621063232
Validation loss: 2.5457690159479776

Epoch: 6| Step: 10
Training loss: 1.8012439012527466
Validation loss: 2.5418361822764077

Epoch: 6| Step: 11
Training loss: 2.3726987838745117
Validation loss: 2.5440234740575156

Epoch: 6| Step: 12
Training loss: 2.887840747833252
Validation loss: 2.5453586975733438

Epoch: 6| Step: 13
Training loss: 2.7932496070861816
Validation loss: 2.5406057238578796

Epoch: 43| Step: 0
Training loss: 3.383648157119751
Validation loss: 2.534283697605133

Epoch: 6| Step: 1
Training loss: 2.469543933868408
Validation loss: 2.530339241027832

Epoch: 6| Step: 2
Training loss: 2.358583450317383
Validation loss: 2.5238318045934043

Epoch: 6| Step: 3
Training loss: 2.3623361587524414
Validation loss: 2.521936774253845

Epoch: 6| Step: 4
Training loss: 2.6904892921447754
Validation loss: 2.518255114555359

Epoch: 6| Step: 5
Training loss: 3.057499408721924
Validation loss: 2.5162558952967324

Epoch: 6| Step: 6
Training loss: 3.108081817626953
Validation loss: 2.5130420923233032

Epoch: 6| Step: 7
Training loss: 2.1563754081726074
Validation loss: 2.511842727661133

Epoch: 6| Step: 8
Training loss: 3.163384437561035
Validation loss: 2.5070560773213706

Epoch: 6| Step: 9
Training loss: 2.5564074516296387
Validation loss: 2.504216949144999

Epoch: 6| Step: 10
Training loss: 2.5968780517578125
Validation loss: 2.5004397432009378

Epoch: 6| Step: 11
Training loss: 2.3190152645111084
Validation loss: 2.4997443755467734

Epoch: 6| Step: 12
Training loss: 2.608747959136963
Validation loss: 2.498827894528707

Epoch: 6| Step: 13
Training loss: 3.049227714538574
Validation loss: 2.4981021881103516

Epoch: 44| Step: 0
Training loss: 3.065944194793701
Validation loss: 2.5144217809041343

Epoch: 6| Step: 1
Training loss: 3.1465649604797363
Validation loss: 2.5165451765060425

Epoch: 6| Step: 2
Training loss: 2.783381462097168
Validation loss: 2.5057281653086343

Epoch: 6| Step: 3
Training loss: 2.5232439041137695
Validation loss: 2.4908056457837424

Epoch: 6| Step: 4
Training loss: 2.448746919631958
Validation loss: 2.48780357837677

Epoch: 6| Step: 5
Training loss: 2.878776788711548
Validation loss: 2.481951912244161

Epoch: 6| Step: 6
Training loss: 2.5241293907165527
Validation loss: 2.4789310296376548

Epoch: 6| Step: 7
Training loss: 1.9905755519866943
Validation loss: 2.4767595529556274

Epoch: 6| Step: 8
Training loss: 2.6133291721343994
Validation loss: 2.4729883670806885

Epoch: 6| Step: 9
Training loss: 2.831057548522949
Validation loss: 2.474336266517639

Epoch: 6| Step: 10
Training loss: 2.064293622970581
Validation loss: 2.470442811648051

Epoch: 6| Step: 11
Training loss: 3.057420253753662
Validation loss: 2.4703904390335083

Epoch: 6| Step: 12
Training loss: 2.840407371520996
Validation loss: 2.4700478315353394

Epoch: 6| Step: 13
Training loss: 2.6291985511779785
Validation loss: 2.4664519826571145

Epoch: 45| Step: 0
Training loss: 3.190748691558838
Validation loss: 2.4607749382654824

Epoch: 6| Step: 1
Training loss: 2.5670251846313477
Validation loss: 2.4537416299184165

Epoch: 6| Step: 2
Training loss: 2.2105250358581543
Validation loss: 2.4498512347539267

Epoch: 6| Step: 3
Training loss: 2.320665121078491
Validation loss: 2.4478442072868347

Epoch: 6| Step: 4
Training loss: 2.4113192558288574
Validation loss: 2.4421865145365396

Epoch: 6| Step: 5
Training loss: 3.305929660797119
Validation loss: 2.438578804334005

Epoch: 6| Step: 6
Training loss: 2.3763210773468018
Validation loss: 2.435772736867269

Epoch: 6| Step: 7
Training loss: 2.2504329681396484
Validation loss: 2.4353285233179727

Epoch: 6| Step: 8
Training loss: 2.7673614025115967
Validation loss: 2.4329262177149453

Epoch: 6| Step: 9
Training loss: 2.5410664081573486
Validation loss: 2.431365728378296

Epoch: 6| Step: 10
Training loss: 2.5924630165100098
Validation loss: 2.42816162109375

Epoch: 6| Step: 11
Training loss: 2.7507529258728027
Validation loss: 2.428882122039795

Epoch: 6| Step: 12
Training loss: 3.339372396469116
Validation loss: 2.427385687828064

Epoch: 6| Step: 13
Training loss: 2.126382350921631
Validation loss: 2.4186656872431436

Epoch: 46| Step: 0
Training loss: 3.244015693664551
Validation loss: 2.4160250425338745

Epoch: 6| Step: 1
Training loss: 1.7662049531936646
Validation loss: 2.4126210610071817

Epoch: 6| Step: 2
Training loss: 3.054699420928955
Validation loss: 2.4104721546173096

Epoch: 6| Step: 3
Training loss: 2.0858466625213623
Validation loss: 2.4115745623906455

Epoch: 6| Step: 4
Training loss: 2.880455493927002
Validation loss: 2.404150386651357

Epoch: 6| Step: 5
Training loss: 3.074509859085083
Validation loss: 2.406546870867411

Epoch: 6| Step: 6
Training loss: 2.2721855640411377
Validation loss: 2.402764936288198

Epoch: 6| Step: 7
Training loss: 2.386625289916992
Validation loss: 2.3996006647745767

Epoch: 6| Step: 8
Training loss: 2.3058159351348877
Validation loss: 2.397715131441752

Epoch: 6| Step: 9
Training loss: 2.7079296112060547
Validation loss: 2.4021427631378174

Epoch: 6| Step: 10
Training loss: 3.1185178756713867
Validation loss: 2.4021830956141152

Epoch: 6| Step: 11
Training loss: 2.5064051151275635
Validation loss: 2.39329202969869

Epoch: 6| Step: 12
Training loss: 2.3355493545532227
Validation loss: 2.3907389839490256

Epoch: 6| Step: 13
Training loss: 2.562316656112671
Validation loss: 2.392092486222585

Epoch: 47| Step: 0
Training loss: 1.8897777795791626
Validation loss: 2.383496801058451

Epoch: 6| Step: 1
Training loss: 2.9740262031555176
Validation loss: 2.380885044733683

Epoch: 6| Step: 2
Training loss: 2.566211700439453
Validation loss: 2.380257765452067

Epoch: 6| Step: 3
Training loss: 2.320789337158203
Validation loss: 2.377468784650167

Epoch: 6| Step: 4
Training loss: 2.526029109954834
Validation loss: 2.375634551048279

Epoch: 6| Step: 5
Training loss: 2.3853366374969482
Validation loss: 2.3761622111002603

Epoch: 6| Step: 6
Training loss: 2.6215779781341553
Validation loss: 2.3720545768737793

Epoch: 6| Step: 7
Training loss: 2.781830310821533
Validation loss: 2.3655452728271484

Epoch: 6| Step: 8
Training loss: 3.0839157104492188
Validation loss: 2.363503317038218

Epoch: 6| Step: 9
Training loss: 2.6026594638824463
Validation loss: 2.360410491625468

Epoch: 6| Step: 10
Training loss: 2.400597095489502
Validation loss: 2.3609492977460227

Epoch: 6| Step: 11
Training loss: 2.5172252655029297
Validation loss: 2.355614423751831

Epoch: 6| Step: 12
Training loss: 2.482433319091797
Validation loss: 2.3512362241744995

Epoch: 6| Step: 13
Training loss: 2.4720139503479004
Validation loss: 2.3533310492833457

Epoch: 48| Step: 0
Training loss: 2.197319746017456
Validation loss: 2.3570668498675027

Epoch: 6| Step: 1
Training loss: 2.1670141220092773
Validation loss: 2.3519959251085916

Epoch: 6| Step: 2
Training loss: 2.8685121536254883
Validation loss: 2.3421440521876016

Epoch: 6| Step: 3
Training loss: 3.0727546215057373
Validation loss: 2.3437116543451944

Epoch: 6| Step: 4
Training loss: 2.244400978088379
Validation loss: 2.3396364053090415

Epoch: 6| Step: 5
Training loss: 2.445049285888672
Validation loss: 2.3373530308405557

Epoch: 6| Step: 6
Training loss: 3.2886767387390137
Validation loss: 2.3323625723520913

Epoch: 6| Step: 7
Training loss: 2.1294102668762207
Validation loss: 2.3292969465255737

Epoch: 6| Step: 8
Training loss: 2.187894821166992
Validation loss: 2.326763947804769

Epoch: 6| Step: 9
Training loss: 2.509014368057251
Validation loss: 2.3219195206960044

Epoch: 6| Step: 10
Training loss: 1.9642016887664795
Validation loss: 2.325169622898102

Epoch: 6| Step: 11
Training loss: 2.3707194328308105
Validation loss: 2.322587331136068

Epoch: 6| Step: 12
Training loss: 2.469630718231201
Validation loss: 2.3223086992899575

Epoch: 6| Step: 13
Training loss: 3.167752742767334
Validation loss: 2.3146947026252747

Epoch: 49| Step: 0
Training loss: 1.5152920484542847
Validation loss: 2.313443144162496

Epoch: 6| Step: 1
Training loss: 2.0423645973205566
Validation loss: 2.3119508425394693

Epoch: 6| Step: 2
Training loss: 2.3349761962890625
Validation loss: 2.309049050013224

Epoch: 6| Step: 3
Training loss: 2.942815065383911
Validation loss: 2.30777907371521

Epoch: 6| Step: 4
Training loss: 2.5496294498443604
Validation loss: 2.310309330622355

Epoch: 6| Step: 5
Training loss: 2.777088165283203
Validation loss: 2.305165966351827

Epoch: 6| Step: 6
Training loss: 2.882068634033203
Validation loss: 2.3082037369410195

Epoch: 6| Step: 7
Training loss: 2.4854490756988525
Validation loss: 2.3082014520963035

Epoch: 6| Step: 8
Training loss: 2.2155404090881348
Validation loss: 2.301949441432953

Epoch: 6| Step: 9
Training loss: 2.19500994682312
Validation loss: 2.2938908338546753

Epoch: 6| Step: 10
Training loss: 2.9426217079162598
Validation loss: 2.2936963637669883

Epoch: 6| Step: 11
Training loss: 2.4242382049560547
Validation loss: 2.2879237731297812

Epoch: 6| Step: 12
Training loss: 1.9722466468811035
Validation loss: 2.286521395047506

Epoch: 6| Step: 13
Training loss: 3.338869571685791
Validation loss: 2.2833520968755088

Epoch: 50| Step: 0
Training loss: 2.2667675018310547
Validation loss: 2.280294438203176

Epoch: 6| Step: 1
Training loss: 2.3258280754089355
Validation loss: 2.2794365882873535

Epoch: 6| Step: 2
Training loss: 1.9776232242584229
Validation loss: 2.2782007853190103

Epoch: 6| Step: 3
Training loss: 2.767688274383545
Validation loss: 2.2738065918286643

Epoch: 6| Step: 4
Training loss: 2.3793649673461914
Validation loss: 2.2761709292729697

Epoch: 6| Step: 5
Training loss: 2.021317481994629
Validation loss: 2.273243467013041

Epoch: 6| Step: 6
Training loss: 2.7092761993408203
Validation loss: 2.2744372288386026

Epoch: 6| Step: 7
Training loss: 2.4870171546936035
Validation loss: 2.261519491672516

Epoch: 6| Step: 8
Training loss: 2.2171030044555664
Validation loss: 2.2608456214269004

Epoch: 6| Step: 9
Training loss: 2.351893901824951
Validation loss: 2.256666680177053

Epoch: 6| Step: 10
Training loss: 2.209951639175415
Validation loss: 2.2568565209706626

Epoch: 6| Step: 11
Training loss: 2.575066089630127
Validation loss: 2.2572492559750876

Epoch: 6| Step: 12
Training loss: 2.839980125427246
Validation loss: 2.25614994764328

Epoch: 6| Step: 13
Training loss: 2.8103041648864746
Validation loss: 2.2603417237599692

Epoch: 51| Step: 0
Training loss: 2.1852450370788574
Validation loss: 2.2572202483812966

Epoch: 6| Step: 1
Training loss: 2.2049636840820312
Validation loss: 2.2480438947677612

Epoch: 6| Step: 2
Training loss: 2.5055673122406006
Validation loss: 2.244697650273641

Epoch: 6| Step: 3
Training loss: 2.5527939796447754
Validation loss: 2.2457658847173056

Epoch: 6| Step: 4
Training loss: 1.7262868881225586
Validation loss: 2.2389272848765054

Epoch: 6| Step: 5
Training loss: 2.116244316101074
Validation loss: 2.2362056970596313

Epoch: 6| Step: 6
Training loss: 2.043757677078247
Validation loss: 2.229766011238098

Epoch: 6| Step: 7
Training loss: 2.8731026649475098
Validation loss: 2.226697325706482

Epoch: 6| Step: 8
Training loss: 1.6284016370773315
Validation loss: 2.2219256361325583

Epoch: 6| Step: 9
Training loss: 2.79542875289917
Validation loss: 2.2244441509246826

Epoch: 6| Step: 10
Training loss: 2.937028408050537
Validation loss: 2.223633805910746

Epoch: 6| Step: 11
Training loss: 3.397808074951172
Validation loss: 2.220336119333903

Epoch: 6| Step: 12
Training loss: 2.00162935256958
Validation loss: 2.217347741127014

Epoch: 6| Step: 13
Training loss: 2.4675018787384033
Validation loss: 2.2164473136266074

Epoch: 52| Step: 0
Training loss: 2.7519266605377197
Validation loss: 2.215255777041117

Epoch: 6| Step: 1
Training loss: 1.7795395851135254
Validation loss: 2.21089901526769

Epoch: 6| Step: 2
Training loss: 2.245302200317383
Validation loss: 2.2072207927703857

Epoch: 6| Step: 3
Training loss: 2.219806671142578
Validation loss: 2.2070798873901367

Epoch: 6| Step: 4
Training loss: 2.3405814170837402
Validation loss: 2.2061904668807983

Epoch: 6| Step: 5
Training loss: 2.6813530921936035
Validation loss: 2.2040750980377197

Epoch: 6| Step: 6
Training loss: 2.9781203269958496
Validation loss: 2.2020680904388428

Epoch: 6| Step: 7
Training loss: 1.817989706993103
Validation loss: 2.2039916714032493

Epoch: 6| Step: 8
Training loss: 2.725135564804077
Validation loss: 2.1983935832977295

Epoch: 6| Step: 9
Training loss: 2.4031991958618164
Validation loss: 2.195347766081492

Epoch: 6| Step: 10
Training loss: 3.167742967605591
Validation loss: 2.1899117628733316

Epoch: 6| Step: 11
Training loss: 2.2933707237243652
Validation loss: 2.1944910883903503

Epoch: 6| Step: 12
Training loss: 1.7677066326141357
Validation loss: 2.19071102142334

Epoch: 6| Step: 13
Training loss: 1.8555947542190552
Validation loss: 2.199491719404856

Epoch: 53| Step: 0
Training loss: 2.925218105316162
Validation loss: 2.1981759071350098

Epoch: 6| Step: 1
Training loss: 2.065016746520996
Validation loss: 2.191790541013082

Epoch: 6| Step: 2
Training loss: 2.727290630340576
Validation loss: 2.1914999882380166

Epoch: 6| Step: 3
Training loss: 2.1605074405670166
Validation loss: 2.1897365053494773

Epoch: 6| Step: 4
Training loss: 1.7162697315216064
Validation loss: 2.1826454798380532

Epoch: 6| Step: 5
Training loss: 2.316967487335205
Validation loss: 2.174193342526754

Epoch: 6| Step: 6
Training loss: 2.4896535873413086
Validation loss: 2.1744714180628457

Epoch: 6| Step: 7
Training loss: 2.5095973014831543
Validation loss: 2.168660521507263

Epoch: 6| Step: 8
Training loss: 2.318152904510498
Validation loss: 2.167374531428019

Epoch: 6| Step: 9
Training loss: 2.7501726150512695
Validation loss: 2.1642163594563804

Epoch: 6| Step: 10
Training loss: 2.5526819229125977
Validation loss: 2.163494865099589

Epoch: 6| Step: 11
Training loss: 2.383592128753662
Validation loss: 2.1539449095726013

Epoch: 6| Step: 12
Training loss: 2.123204231262207
Validation loss: 2.157684405644735

Epoch: 6| Step: 13
Training loss: 1.5309133529663086
Validation loss: 2.155693769454956

Epoch: 54| Step: 0
Training loss: 2.3446767330169678
Validation loss: 2.169173320134481

Epoch: 6| Step: 1
Training loss: 2.915336847305298
Validation loss: 2.164979636669159

Epoch: 6| Step: 2
Training loss: 1.6322543621063232
Validation loss: 2.1523460745811462

Epoch: 6| Step: 3
Training loss: 2.531428813934326
Validation loss: 2.1576722860336304

Epoch: 6| Step: 4
Training loss: 2.042205572128296
Validation loss: 2.1525355776151023

Epoch: 6| Step: 5
Training loss: 2.233335494995117
Validation loss: 2.1562961538632712

Epoch: 6| Step: 6
Training loss: 2.4947919845581055
Validation loss: 2.156439403692881

Epoch: 6| Step: 7
Training loss: 2.439297676086426
Validation loss: 2.1643444299697876

Epoch: 6| Step: 8
Training loss: 2.1646223068237305
Validation loss: 2.1612943410873413

Epoch: 6| Step: 9
Training loss: 2.1360044479370117
Validation loss: 2.161021093527476

Epoch: 6| Step: 10
Training loss: 2.83933687210083
Validation loss: 2.163828730583191

Epoch: 6| Step: 11
Training loss: 2.340644598007202
Validation loss: 2.15436718861262

Epoch: 6| Step: 12
Training loss: 2.414367198944092
Validation loss: 2.1579156716664634

Epoch: 6| Step: 13
Training loss: 1.9753960371017456
Validation loss: 2.148879090944926

Epoch: 55| Step: 0
Training loss: 2.3710215091705322
Validation loss: 2.1421502828598022

Epoch: 6| Step: 1
Training loss: 2.1943001747131348
Validation loss: 2.145399292310079

Epoch: 6| Step: 2
Training loss: 2.249896287918091
Validation loss: 2.1403908928235373

Epoch: 6| Step: 3
Training loss: 2.194345474243164
Validation loss: 2.1422143379847207

Epoch: 6| Step: 4
Training loss: 2.264793872833252
Validation loss: 2.146384914716085

Epoch: 6| Step: 5
Training loss: 2.0182833671569824
Validation loss: 2.1508073012034097

Epoch: 6| Step: 6
Training loss: 2.0400304794311523
Validation loss: 2.145964801311493

Epoch: 6| Step: 7
Training loss: 1.741632342338562
Validation loss: 2.139613767464956

Epoch: 6| Step: 8
Training loss: 3.1799728870391846
Validation loss: 2.139020800590515

Epoch: 6| Step: 9
Training loss: 2.4679088592529297
Validation loss: 2.135746200879415

Epoch: 6| Step: 10
Training loss: 2.3394370079040527
Validation loss: 2.1420567631721497

Epoch: 6| Step: 11
Training loss: 2.508535385131836
Validation loss: 2.145843227704366

Epoch: 6| Step: 12
Training loss: 2.5218706130981445
Validation loss: 2.150673727194468

Epoch: 6| Step: 13
Training loss: 2.1267173290252686
Validation loss: 2.1522107323010764

Epoch: 56| Step: 0
Training loss: 2.5761053562164307
Validation loss: 2.1523452202479043

Epoch: 6| Step: 1
Training loss: 2.1890573501586914
Validation loss: 2.1581965486208596

Epoch: 6| Step: 2
Training loss: 1.9621033668518066
Validation loss: 2.1541464924812317

Epoch: 6| Step: 3
Training loss: 2.257446765899658
Validation loss: 2.1534403761227927

Epoch: 6| Step: 4
Training loss: 1.8141765594482422
Validation loss: 2.1510266860326133

Epoch: 6| Step: 5
Training loss: 2.2814981937408447
Validation loss: 2.1487006346384683

Epoch: 6| Step: 6
Training loss: 2.3391237258911133
Validation loss: 2.141191919644674

Epoch: 6| Step: 7
Training loss: 2.286513566970825
Validation loss: 2.139184296131134

Epoch: 6| Step: 8
Training loss: 2.5741381645202637
Validation loss: 2.124220053354899

Epoch: 6| Step: 9
Training loss: 2.793590545654297
Validation loss: 2.1231889128684998

Epoch: 6| Step: 10
Training loss: 2.0033905506134033
Validation loss: 2.1122210025787354

Epoch: 6| Step: 11
Training loss: 2.279432773590088
Validation loss: 2.114927113056183

Epoch: 6| Step: 12
Training loss: 2.550023078918457
Validation loss: 2.111826221148173

Epoch: 6| Step: 13
Training loss: 2.3134074211120605
Validation loss: 2.1104292074839273

Epoch: 57| Step: 0
Training loss: 2.438634157180786
Validation loss: 2.1076091527938843

Epoch: 6| Step: 1
Training loss: 2.627250909805298
Validation loss: 2.106354375680288

Epoch: 6| Step: 2
Training loss: 2.916036605834961
Validation loss: 2.0989075104395547

Epoch: 6| Step: 3
Training loss: 1.6829159259796143
Validation loss: 2.10143115123113

Epoch: 6| Step: 4
Training loss: 2.40622615814209
Validation loss: 2.1091582973798118

Epoch: 6| Step: 5
Training loss: 2.239424228668213
Validation loss: 2.115105668703715

Epoch: 6| Step: 6
Training loss: 2.7445755004882812
Validation loss: 2.1072740952173867

Epoch: 6| Step: 7
Training loss: 2.424193859100342
Validation loss: 2.111858288447062

Epoch: 6| Step: 8
Training loss: 1.5668739080429077
Validation loss: 2.1171427965164185

Epoch: 6| Step: 9
Training loss: 2.0967297554016113
Validation loss: 2.1238045891126

Epoch: 6| Step: 10
Training loss: 2.406942129135132
Validation loss: 2.120801250139872

Epoch: 6| Step: 11
Training loss: 1.984815001487732
Validation loss: 2.1186132629712424

Epoch: 6| Step: 12
Training loss: 1.743717908859253
Validation loss: 2.1216267347335815

Epoch: 6| Step: 13
Training loss: 2.614136219024658
Validation loss: 2.109696924686432

Epoch: 58| Step: 0
Training loss: 2.60280704498291
Validation loss: 2.114406108856201

Epoch: 6| Step: 1
Training loss: 2.9600110054016113
Validation loss: 2.1046259005864463

Epoch: 6| Step: 2
Training loss: 2.2697460651397705
Validation loss: 2.1025649905204773

Epoch: 6| Step: 3
Training loss: 1.864793300628662
Validation loss: 2.0980221231778464

Epoch: 6| Step: 4
Training loss: 1.6090320348739624
Validation loss: 2.0968169967333474

Epoch: 6| Step: 5
Training loss: 2.200796127319336
Validation loss: 2.0953007141749063

Epoch: 6| Step: 6
Training loss: 2.6778440475463867
Validation loss: 2.1100539366404214

Epoch: 6| Step: 7
Training loss: 2.7664127349853516
Validation loss: 2.130092183748881

Epoch: 6| Step: 8
Training loss: 2.1942336559295654
Validation loss: 2.124017119407654

Epoch: 6| Step: 9
Training loss: 2.752859592437744
Validation loss: 2.127934694290161

Epoch: 6| Step: 10
Training loss: 2.0457584857940674
Validation loss: 2.1072623332341514

Epoch: 6| Step: 11
Training loss: 1.8285491466522217
Validation loss: 2.0871928532918296

Epoch: 6| Step: 12
Training loss: 2.3343513011932373
Validation loss: 2.096110165119171

Epoch: 6| Step: 13
Training loss: 2.115589141845703
Validation loss: 2.098982294400533

Epoch: 59| Step: 0
Training loss: 2.5778982639312744
Validation loss: 2.1012406746546426

Epoch: 6| Step: 1
Training loss: 2.3669137954711914
Validation loss: 2.106174111366272

Epoch: 6| Step: 2
Training loss: 2.618973970413208
Validation loss: 2.121515989303589

Epoch: 6| Step: 3
Training loss: 1.627333164215088
Validation loss: 2.1168403228123984

Epoch: 6| Step: 4
Training loss: 2.2345237731933594
Validation loss: 2.1174203753471375

Epoch: 6| Step: 5
Training loss: 2.344573736190796
Validation loss: 2.1145508090655007

Epoch: 6| Step: 6
Training loss: 2.0781302452087402
Validation loss: 2.108107010523478

Epoch: 6| Step: 7
Training loss: 2.371840715408325
Validation loss: 2.1072097023328147

Epoch: 6| Step: 8
Training loss: 2.261801242828369
Validation loss: 2.1014128724733987

Epoch: 6| Step: 9
Training loss: 2.3043909072875977
Validation loss: 2.0995437701543174

Epoch: 6| Step: 10
Training loss: 2.365692377090454
Validation loss: 2.0968926747639975

Epoch: 6| Step: 11
Training loss: 2.480781316757202
Validation loss: 2.0909815231959024

Epoch: 6| Step: 12
Training loss: 2.1220993995666504
Validation loss: 2.085560977458954

Epoch: 6| Step: 13
Training loss: 2.0845603942871094
Validation loss: 2.0809214313824973

Epoch: 60| Step: 0
Training loss: 2.2016172409057617
Validation loss: 2.075435698032379

Epoch: 6| Step: 1
Training loss: 2.194638252258301
Validation loss: 2.0761945247650146

Epoch: 6| Step: 2
Training loss: 2.348184585571289
Validation loss: 2.077015737692515

Epoch: 6| Step: 3
Training loss: 1.9051281213760376
Validation loss: 2.080375691254934

Epoch: 6| Step: 4
Training loss: 2.5731093883514404
Validation loss: 2.0761823256810508

Epoch: 6| Step: 5
Training loss: 2.124826669692993
Validation loss: 2.07846870024999

Epoch: 6| Step: 6
Training loss: 2.5188190937042236
Validation loss: 2.0685738722483316

Epoch: 6| Step: 7
Training loss: 2.2873165607452393
Validation loss: 2.0662707487742105

Epoch: 6| Step: 8
Training loss: 1.96586012840271
Validation loss: 2.064437667528788

Epoch: 6| Step: 9
Training loss: 2.2732155323028564
Validation loss: 2.0688079396883645

Epoch: 6| Step: 10
Training loss: 2.486616611480713
Validation loss: 2.060751815636953

Epoch: 6| Step: 11
Training loss: 1.923870325088501
Validation loss: 2.05817445119222

Epoch: 6| Step: 12
Training loss: 2.2791852951049805
Validation loss: 2.0757983326911926

Epoch: 6| Step: 13
Training loss: 2.5794930458068848
Validation loss: 2.071088194847107

Epoch: 61| Step: 0
Training loss: 1.646437406539917
Validation loss: 2.0604744950930276

Epoch: 6| Step: 1
Training loss: 2.3624541759490967
Validation loss: 2.055142263571421

Epoch: 6| Step: 2
Training loss: 2.009917736053467
Validation loss: 2.058279116948446

Epoch: 6| Step: 3
Training loss: 2.7741127014160156
Validation loss: 2.054331143697103

Epoch: 6| Step: 4
Training loss: 2.1247286796569824
Validation loss: 2.0652696092923484

Epoch: 6| Step: 5
Training loss: 2.466603994369507
Validation loss: 2.067271371682485

Epoch: 6| Step: 6
Training loss: 2.2974371910095215
Validation loss: 2.068485736846924

Epoch: 6| Step: 7
Training loss: 2.089114189147949
Validation loss: 2.0684192180633545

Epoch: 6| Step: 8
Training loss: 2.522869348526001
Validation loss: 2.0712414979934692

Epoch: 6| Step: 9
Training loss: 2.0289790630340576
Validation loss: 2.0685015519460044

Epoch: 6| Step: 10
Training loss: 1.7125332355499268
Validation loss: 2.0664706031481423

Epoch: 6| Step: 11
Training loss: 2.6564645767211914
Validation loss: 2.068031350771586

Epoch: 6| Step: 12
Training loss: 2.5644783973693848
Validation loss: 2.0653926730155945

Epoch: 6| Step: 13
Training loss: 1.995845079421997
Validation loss: 2.0619445045789084

Epoch: 62| Step: 0
Training loss: 2.3143231868743896
Validation loss: 2.062719921271006

Epoch: 6| Step: 1
Training loss: 2.1376099586486816
Validation loss: 2.0592639644940696

Epoch: 6| Step: 2
Training loss: 2.0258893966674805
Validation loss: 2.04922878742218

Epoch: 6| Step: 3
Training loss: 1.8319764137268066
Validation loss: 2.048614025115967

Epoch: 6| Step: 4
Training loss: 2.6566624641418457
Validation loss: 2.043607175350189

Epoch: 6| Step: 5
Training loss: 2.145486354827881
Validation loss: 2.042632540067037

Epoch: 6| Step: 6
Training loss: 2.1867010593414307
Validation loss: 2.0456844170888266

Epoch: 6| Step: 7
Training loss: 2.225273370742798
Validation loss: 2.047254184881846

Epoch: 6| Step: 8
Training loss: 2.6428537368774414
Validation loss: 2.0407437682151794

Epoch: 6| Step: 9
Training loss: 2.2808353900909424
Validation loss: 2.0431981682777405

Epoch: 6| Step: 10
Training loss: 2.8041417598724365
Validation loss: 2.0408512552579245

Epoch: 6| Step: 11
Training loss: 1.4583351612091064
Validation loss: 2.0448068181673684

Epoch: 6| Step: 12
Training loss: 2.592507839202881
Validation loss: 2.044964869817098

Epoch: 6| Step: 13
Training loss: 1.8540961742401123
Validation loss: 2.052576959133148

Epoch: 63| Step: 0
Training loss: 2.2349538803100586
Validation loss: 2.064501961072286

Epoch: 6| Step: 1
Training loss: 2.35772967338562
Validation loss: 2.0447110136349997

Epoch: 6| Step: 2
Training loss: 2.626838445663452
Validation loss: 2.0637960036595664

Epoch: 6| Step: 3
Training loss: 1.926196575164795
Validation loss: 2.054090758164724

Epoch: 6| Step: 4
Training loss: 2.595076322555542
Validation loss: 2.0631479819615683

Epoch: 6| Step: 5
Training loss: 2.391561508178711
Validation loss: 2.062885264555613

Epoch: 6| Step: 6
Training loss: 2.6917459964752197
Validation loss: 2.055481751759847

Epoch: 6| Step: 7
Training loss: 1.863029956817627
Validation loss: 2.05619486172994

Epoch: 6| Step: 8
Training loss: 1.4166507720947266
Validation loss: 2.0478845636049905

Epoch: 6| Step: 9
Training loss: 2.086540460586548
Validation loss: 2.0473421613375344

Epoch: 6| Step: 10
Training loss: 2.694038152694702
Validation loss: 2.0469280083974204

Epoch: 6| Step: 11
Training loss: 1.483064889907837
Validation loss: 2.0457988381385803

Epoch: 6| Step: 12
Training loss: 2.1264939308166504
Validation loss: 2.0424647529919944

Epoch: 6| Step: 13
Training loss: 2.45906400680542
Validation loss: 2.034383396307627

Epoch: 64| Step: 0
Training loss: 2.121919631958008
Validation loss: 2.0310115019480386

Epoch: 6| Step: 1
Training loss: 1.9078227281570435
Validation loss: 2.0416318575541177

Epoch: 6| Step: 2
Training loss: 1.976933479309082
Validation loss: 2.0497888127962747

Epoch: 6| Step: 3
Training loss: 1.760334849357605
Validation loss: 2.043234427769979

Epoch: 6| Step: 4
Training loss: 1.812969446182251
Validation loss: 2.046165386835734

Epoch: 6| Step: 5
Training loss: 2.1007208824157715
Validation loss: 2.0410475532213845

Epoch: 6| Step: 6
Training loss: 2.2530624866485596
Validation loss: 2.041207432746887

Epoch: 6| Step: 7
Training loss: 2.9324951171875
Validation loss: 2.028374890486399

Epoch: 6| Step: 8
Training loss: 2.0470666885375977
Validation loss: 2.0328603982925415

Epoch: 6| Step: 9
Training loss: 2.773280143737793
Validation loss: 2.0370349884033203

Epoch: 6| Step: 10
Training loss: 2.5485880374908447
Validation loss: 2.036946654319763

Epoch: 6| Step: 11
Training loss: 2.9257068634033203
Validation loss: 2.0377920071283975

Epoch: 6| Step: 12
Training loss: 1.9271647930145264
Validation loss: 2.0437338948249817

Epoch: 6| Step: 13
Training loss: 1.8729840517044067
Validation loss: 2.037147879600525

Epoch: 65| Step: 0
Training loss: 2.2123007774353027
Validation loss: 2.0394275387128196

Epoch: 6| Step: 1
Training loss: 1.8685821294784546
Validation loss: 2.034429689248403

Epoch: 6| Step: 2
Training loss: 1.8948981761932373
Validation loss: 2.0238874157269797

Epoch: 6| Step: 3
Training loss: 2.729898452758789
Validation loss: 2.0261081059773765

Epoch: 6| Step: 4
Training loss: 1.8433921337127686
Validation loss: 2.0317472219467163

Epoch: 6| Step: 5
Training loss: 2.3536040782928467
Validation loss: 2.037055770556132

Epoch: 6| Step: 6
Training loss: 2.4326281547546387
Validation loss: 2.0388336181640625

Epoch: 6| Step: 7
Training loss: 1.778954267501831
Validation loss: 2.0471741755803428

Epoch: 6| Step: 8
Training loss: 2.238384246826172
Validation loss: 2.0318803787231445

Epoch: 6| Step: 9
Training loss: 1.6295509338378906
Validation loss: 2.03824919462204

Epoch: 6| Step: 10
Training loss: 2.4190242290496826
Validation loss: 2.046975831190745

Epoch: 6| Step: 11
Training loss: 2.243593215942383
Validation loss: 2.039347310860952

Epoch: 6| Step: 12
Training loss: 2.392561912536621
Validation loss: 2.029182732105255

Epoch: 6| Step: 13
Training loss: 2.881424903869629
Validation loss: 2.0297303199768066

Epoch: 66| Step: 0
Training loss: 2.579068183898926
Validation loss: 2.037271042664846

Epoch: 6| Step: 1
Training loss: 3.0432887077331543
Validation loss: 2.042533814907074

Epoch: 6| Step: 2
Training loss: 2.4886562824249268
Validation loss: 2.0315319498380027

Epoch: 6| Step: 3
Training loss: 2.395947217941284
Validation loss: 2.0347864429155984

Epoch: 6| Step: 4
Training loss: 2.4075517654418945
Validation loss: 2.0364071130752563

Epoch: 6| Step: 5
Training loss: 1.5447676181793213
Validation loss: 2.034815728664398

Epoch: 6| Step: 6
Training loss: 2.1191000938415527
Validation loss: 2.0411176681518555

Epoch: 6| Step: 7
Training loss: 2.5489094257354736
Validation loss: 2.038298467795054

Epoch: 6| Step: 8
Training loss: 2.0745606422424316
Validation loss: 2.0405412713686624

Epoch: 6| Step: 9
Training loss: 2.3746042251586914
Validation loss: 2.0424994627634683

Epoch: 6| Step: 10
Training loss: 2.2039074897766113
Validation loss: 2.036156872908274

Epoch: 6| Step: 11
Training loss: 1.9835875034332275
Validation loss: 2.033378779888153

Epoch: 6| Step: 12
Training loss: 1.5195029973983765
Validation loss: 2.0301672418912253

Epoch: 6| Step: 13
Training loss: 1.5611138343811035
Validation loss: 2.0234546065330505

Epoch: 67| Step: 0
Training loss: 1.9146826267242432
Validation loss: 2.0216244657834372

Epoch: 6| Step: 1
Training loss: 1.917151927947998
Validation loss: 2.022572875022888

Epoch: 6| Step: 2
Training loss: 1.8278450965881348
Validation loss: 2.0392836928367615

Epoch: 6| Step: 3
Training loss: 2.1653082370758057
Validation loss: 2.068843344847361

Epoch: 6| Step: 4
Training loss: 1.899360179901123
Validation loss: 2.108784258365631

Epoch: 6| Step: 5
Training loss: 3.045132637023926
Validation loss: 2.139885723590851

Epoch: 6| Step: 6
Training loss: 1.923640489578247
Validation loss: 2.148103892803192

Epoch: 6| Step: 7
Training loss: 1.8654687404632568
Validation loss: 2.117671310901642

Epoch: 6| Step: 8
Training loss: 2.5019454956054688
Validation loss: 2.081644296646118

Epoch: 6| Step: 9
Training loss: 3.4210543632507324
Validation loss: 2.0644938747088113

Epoch: 6| Step: 10
Training loss: 2.1687421798706055
Validation loss: 2.02907786766688

Epoch: 6| Step: 11
Training loss: 2.1119256019592285
Validation loss: 2.0237993697325387

Epoch: 6| Step: 12
Training loss: 2.4008469581604004
Validation loss: 2.047822872797648

Epoch: 6| Step: 13
Training loss: 2.2572922706604004
Validation loss: 2.0565582116444907

Epoch: 68| Step: 0
Training loss: 2.2899017333984375
Validation loss: 2.0927943785985312

Epoch: 6| Step: 1
Training loss: 2.2359800338745117
Validation loss: 2.1151911417643228

Epoch: 6| Step: 2
Training loss: 2.8675966262817383
Validation loss: 2.138685961564382

Epoch: 6| Step: 3
Training loss: 1.7986903190612793
Validation loss: 2.152235269546509

Epoch: 6| Step: 4
Training loss: 1.7961878776550293
Validation loss: 2.156179904937744

Epoch: 6| Step: 5
Training loss: 2.6092276573181152
Validation loss: 2.1422280271848044

Epoch: 6| Step: 6
Training loss: 1.9457203149795532
Validation loss: 2.1263117591540017

Epoch: 6| Step: 7
Training loss: 1.7793675661087036
Validation loss: 2.11637012163798

Epoch: 6| Step: 8
Training loss: 2.294182300567627
Validation loss: 2.0985915462176004

Epoch: 6| Step: 9
Training loss: 2.7848145961761475
Validation loss: 2.088898460070292

Epoch: 6| Step: 10
Training loss: 1.7011122703552246
Validation loss: 2.07305238644282

Epoch: 6| Step: 11
Training loss: 2.5405006408691406
Validation loss: 2.0656113624572754

Epoch: 6| Step: 12
Training loss: 1.9968411922454834
Validation loss: 2.0595260659853616

Epoch: 6| Step: 13
Training loss: 3.291154146194458
Validation loss: 2.045945187409719

Epoch: 69| Step: 0
Training loss: 1.9134658575057983
Validation loss: 2.0399141112963357

Epoch: 6| Step: 1
Training loss: 2.6724607944488525
Validation loss: 2.0353015661239624

Epoch: 6| Step: 2
Training loss: 2.5109479427337646
Validation loss: 2.0301512082417807

Epoch: 6| Step: 3
Training loss: 1.775960922241211
Validation loss: 2.023259162902832

Epoch: 6| Step: 4
Training loss: 1.713038444519043
Validation loss: 2.0264464219411216

Epoch: 6| Step: 5
Training loss: 2.0612058639526367
Validation loss: 2.018700341383616

Epoch: 6| Step: 6
Training loss: 2.167292356491089
Validation loss: 2.0181443293889365

Epoch: 6| Step: 7
Training loss: 2.571951150894165
Validation loss: 2.0159208178520203

Epoch: 6| Step: 8
Training loss: 1.782987117767334
Validation loss: 2.023446947336197

Epoch: 6| Step: 9
Training loss: 2.35463809967041
Validation loss: 2.0229412317276

Epoch: 6| Step: 10
Training loss: 2.223414897918701
Validation loss: 2.019234299659729

Epoch: 6| Step: 11
Training loss: 2.3116164207458496
Validation loss: 2.01917956272761

Epoch: 6| Step: 12
Training loss: 2.639721393585205
Validation loss: 2.0183380842208862

Epoch: 6| Step: 13
Training loss: 2.0192694664001465
Validation loss: 2.028271953264872

Epoch: 70| Step: 0
Training loss: 1.919878602027893
Validation loss: 2.0118240118026733

Epoch: 6| Step: 1
Training loss: 1.6389756202697754
Validation loss: 2.013371765613556

Epoch: 6| Step: 2
Training loss: 1.970271348953247
Validation loss: 2.027250647544861

Epoch: 6| Step: 3
Training loss: 1.9213292598724365
Validation loss: 2.0243020057678223

Epoch: 6| Step: 4
Training loss: 2.7899580001831055
Validation loss: 2.0209863583246865

Epoch: 6| Step: 5
Training loss: 2.485030174255371
Validation loss: 2.0324498414993286

Epoch: 6| Step: 6
Training loss: 2.398881435394287
Validation loss: 2.030902882417043

Epoch: 6| Step: 7
Training loss: 1.9246822595596313
Validation loss: 2.0352590878804526

Epoch: 6| Step: 8
Training loss: 1.9992998838424683
Validation loss: 2.0281469027201333

Epoch: 6| Step: 9
Training loss: 1.998079538345337
Validation loss: 2.0333054860432944

Epoch: 6| Step: 10
Training loss: 2.4795844554901123
Validation loss: 2.0261582136154175

Epoch: 6| Step: 11
Training loss: 2.105404853820801
Validation loss: 2.019899388154348

Epoch: 6| Step: 12
Training loss: 2.0599231719970703
Validation loss: 2.0149122873942056

Epoch: 6| Step: 13
Training loss: 2.7233176231384277
Validation loss: 2.013477881749471

Epoch: 71| Step: 0
Training loss: 2.0208802223205566
Validation loss: 2.012552003065745

Epoch: 6| Step: 1
Training loss: 1.9792966842651367
Validation loss: 2.023249566555023

Epoch: 6| Step: 2
Training loss: 2.016547679901123
Validation loss: 2.017976542313894

Epoch: 6| Step: 3
Training loss: 2.318904399871826
Validation loss: 2.0200347105662027

Epoch: 6| Step: 4
Training loss: 1.5816766023635864
Validation loss: 2.0175395011901855

Epoch: 6| Step: 5
Training loss: 2.9161641597747803
Validation loss: 2.026427666346232

Epoch: 6| Step: 6
Training loss: 1.9543875455856323
Validation loss: 2.014533797899882

Epoch: 6| Step: 7
Training loss: 1.699638843536377
Validation loss: 2.022735357284546

Epoch: 6| Step: 8
Training loss: 1.9659686088562012
Validation loss: 2.020069738229116

Epoch: 6| Step: 9
Training loss: 2.133574962615967
Validation loss: 2.029261608918508

Epoch: 6| Step: 10
Training loss: 2.7871787548065186
Validation loss: 2.0258497397104898

Epoch: 6| Step: 11
Training loss: 2.301879405975342
Validation loss: 2.014327565828959

Epoch: 6| Step: 12
Training loss: 2.6988892555236816
Validation loss: 2.018986185391744

Epoch: 6| Step: 13
Training loss: 2.0105345249176025
Validation loss: 2.0127578576405845

Epoch: 72| Step: 0
Training loss: 2.4595704078674316
Validation loss: 2.0151419838269553

Epoch: 6| Step: 1
Training loss: 2.316466808319092
Validation loss: 2.0174100399017334

Epoch: 6| Step: 2
Training loss: 2.104419708251953
Validation loss: 2.022988438606262

Epoch: 6| Step: 3
Training loss: 2.279604911804199
Validation loss: 2.020195742448171

Epoch: 6| Step: 4
Training loss: 2.5869011878967285
Validation loss: 2.023658017317454

Epoch: 6| Step: 5
Training loss: 1.7958322763442993
Validation loss: 2.031745493412018

Epoch: 6| Step: 6
Training loss: 1.380871057510376
Validation loss: 2.035767138004303

Epoch: 6| Step: 7
Training loss: 2.555025815963745
Validation loss: 2.0404239098230996

Epoch: 6| Step: 8
Training loss: 2.308378219604492
Validation loss: 2.0345271229743958

Epoch: 6| Step: 9
Training loss: 1.946153163909912
Validation loss: 2.0384002725283303

Epoch: 6| Step: 10
Training loss: 2.2105212211608887
Validation loss: 2.0322954058647156

Epoch: 6| Step: 11
Training loss: 1.8517504930496216
Validation loss: 2.0237493316332498

Epoch: 6| Step: 12
Training loss: 2.1450352668762207
Validation loss: 2.0225282112757363

Epoch: 6| Step: 13
Training loss: 2.651923418045044
Validation loss: 2.018739183743795

Epoch: 73| Step: 0
Training loss: 2.4817254543304443
Validation loss: 2.0101467768351235

Epoch: 6| Step: 1
Training loss: 2.584469795227051
Validation loss: 2.006418844064077

Epoch: 6| Step: 2
Training loss: 2.3366918563842773
Validation loss: 2.0120622913042703

Epoch: 6| Step: 3
Training loss: 2.2086715698242188
Validation loss: 2.018008530139923

Epoch: 6| Step: 4
Training loss: 1.9750241041183472
Validation loss: 2.0247578819592795

Epoch: 6| Step: 5
Training loss: 1.9797283411026
Validation loss: 2.0384035309155784

Epoch: 6| Step: 6
Training loss: 1.672537922859192
Validation loss: 2.025022784868876

Epoch: 6| Step: 7
Training loss: 2.336599349975586
Validation loss: 2.0256985227266946

Epoch: 6| Step: 8
Training loss: 2.0791776180267334
Validation loss: 2.022112786769867

Epoch: 6| Step: 9
Training loss: 1.757070541381836
Validation loss: 2.0283254782358804

Epoch: 6| Step: 10
Training loss: 2.6045682430267334
Validation loss: 2.0381942987442017

Epoch: 6| Step: 11
Training loss: 2.302422523498535
Validation loss: 2.028433899084727

Epoch: 6| Step: 12
Training loss: 2.0569350719451904
Validation loss: 2.0168538093566895

Epoch: 6| Step: 13
Training loss: 2.045278310775757
Validation loss: 2.0119218031565347

Epoch: 74| Step: 0
Training loss: 2.673415184020996
Validation loss: 2.0121352275212607

Epoch: 6| Step: 1
Training loss: 2.532640218734741
Validation loss: 2.0199820597966514

Epoch: 6| Step: 2
Training loss: 2.184713125228882
Validation loss: 2.0097406109174094

Epoch: 6| Step: 3
Training loss: 2.131767749786377
Validation loss: 2.0167558789253235

Epoch: 6| Step: 4
Training loss: 2.1429474353790283
Validation loss: 2.0071469148000083

Epoch: 6| Step: 5
Training loss: 2.1167876720428467
Validation loss: 2.0127942164738974

Epoch: 6| Step: 6
Training loss: 1.9829834699630737
Validation loss: 2.011719564596812

Epoch: 6| Step: 7
Training loss: 1.82010817527771
Validation loss: 2.0097904006640115

Epoch: 6| Step: 8
Training loss: 1.586775541305542
Validation loss: 2.011635482311249

Epoch: 6| Step: 9
Training loss: 2.184722423553467
Validation loss: 2.0086549321810403

Epoch: 6| Step: 10
Training loss: 1.7174739837646484
Validation loss: 2.0053692857424417

Epoch: 6| Step: 11
Training loss: 2.693751573562622
Validation loss: 2.0007975896199546

Epoch: 6| Step: 12
Training loss: 1.9393922090530396
Validation loss: 2.002209484577179

Epoch: 6| Step: 13
Training loss: 2.4637179374694824
Validation loss: 1.9997764627138774

Epoch: 75| Step: 0
Training loss: 2.5403361320495605
Validation loss: 2.0069289008776345

Epoch: 6| Step: 1
Training loss: 2.560720443725586
Validation loss: 2.0170955260594687

Epoch: 6| Step: 2
Training loss: 2.935558319091797
Validation loss: 2.025254964828491

Epoch: 6| Step: 3
Training loss: 1.3207237720489502
Validation loss: 2.0206377506256104

Epoch: 6| Step: 4
Training loss: 1.4844304323196411
Validation loss: 2.026330888271332

Epoch: 6| Step: 5
Training loss: 2.5351130962371826
Validation loss: 2.0274929801623025

Epoch: 6| Step: 6
Training loss: 1.8518669605255127
Validation loss: 2.0304612716039023

Epoch: 6| Step: 7
Training loss: 2.503483295440674
Validation loss: 2.0277623732884726

Epoch: 6| Step: 8
Training loss: 2.415745735168457
Validation loss: 2.0297070344289145

Epoch: 6| Step: 9
Training loss: 1.649113655090332
Validation loss: 2.0455262462298074

Epoch: 6| Step: 10
Training loss: 2.139791250228882
Validation loss: 2.033137559890747

Epoch: 6| Step: 11
Training loss: 2.267620801925659
Validation loss: 2.0445690751075745

Epoch: 6| Step: 12
Training loss: 1.9058244228363037
Validation loss: 2.0366374850273132

Epoch: 6| Step: 13
Training loss: 2.2522103786468506
Validation loss: 2.0314852396647134

Epoch: 76| Step: 0
Training loss: 2.307162046432495
Validation loss: 2.0211893121401467

Epoch: 6| Step: 1
Training loss: 2.428659677505493
Validation loss: 2.027034600575765

Epoch: 6| Step: 2
Training loss: 1.6990116834640503
Validation loss: 2.0133991638819375

Epoch: 6| Step: 3
Training loss: 1.592559814453125
Validation loss: 2.012885411580404

Epoch: 6| Step: 4
Training loss: 2.3366832733154297
Validation loss: 2.0151360630989075

Epoch: 6| Step: 5
Training loss: 2.1734328269958496
Validation loss: 2.021530588467916

Epoch: 6| Step: 6
Training loss: 1.3525075912475586
Validation loss: 2.0202237566312156

Epoch: 6| Step: 7
Training loss: 2.2318639755249023
Validation loss: 2.0331475337346396

Epoch: 6| Step: 8
Training loss: 2.409376621246338
Validation loss: 2.0332353115081787

Epoch: 6| Step: 9
Training loss: 2.2666778564453125
Validation loss: 2.0251951813697815

Epoch: 6| Step: 10
Training loss: 2.6930108070373535
Validation loss: 2.021881361802419

Epoch: 6| Step: 11
Training loss: 1.3627334833145142
Validation loss: 2.0200902223587036

Epoch: 6| Step: 12
Training loss: 2.5294158458709717
Validation loss: 2.0260658860206604

Epoch: 6| Step: 13
Training loss: 2.740561008453369
Validation loss: 2.01399560769399

Epoch: 77| Step: 0
Training loss: 1.845578670501709
Validation loss: 2.009422759215037

Epoch: 6| Step: 1
Training loss: 2.225705623626709
Validation loss: 2.0089252988497415

Epoch: 6| Step: 2
Training loss: 2.5701751708984375
Validation loss: 2.01757021745046

Epoch: 6| Step: 3
Training loss: 2.731675148010254
Validation loss: 2.026149789492289

Epoch: 6| Step: 4
Training loss: 1.9795751571655273
Validation loss: 2.0389409263928733

Epoch: 6| Step: 5
Training loss: 2.1400856971740723
Validation loss: 2.040685296058655

Epoch: 6| Step: 6
Training loss: 2.081531286239624
Validation loss: 2.0469049215316772

Epoch: 6| Step: 7
Training loss: 2.211514711380005
Validation loss: 2.0402392148971558

Epoch: 6| Step: 8
Training loss: 2.109496593475342
Validation loss: 2.0430710315704346

Epoch: 6| Step: 9
Training loss: 2.35756778717041
Validation loss: 2.03767337401708

Epoch: 6| Step: 10
Training loss: 1.874110460281372
Validation loss: 2.0297104120254517

Epoch: 6| Step: 11
Training loss: 1.6876522302627563
Validation loss: 2.0118174950281777

Epoch: 6| Step: 12
Training loss: 2.228006362915039
Validation loss: 1.9989236791928608

Epoch: 6| Step: 13
Training loss: 2.4980223178863525
Validation loss: 2.014091650644938

Epoch: 78| Step: 0
Training loss: 1.9173661470413208
Validation loss: 2.017054537932078

Epoch: 6| Step: 1
Training loss: 2.3680965900421143
Validation loss: 2.0297844211260476

Epoch: 6| Step: 2
Training loss: 1.9734218120574951
Validation loss: 2.045025885105133

Epoch: 6| Step: 3
Training loss: 2.1610002517700195
Validation loss: 2.0341691970825195

Epoch: 6| Step: 4
Training loss: 1.864835262298584
Validation loss: 2.037167966365814

Epoch: 6| Step: 5
Training loss: 2.179081916809082
Validation loss: 2.049527903397878

Epoch: 6| Step: 6
Training loss: 2.122310161590576
Validation loss: 2.0359783371289573

Epoch: 6| Step: 7
Training loss: 1.9987848997116089
Validation loss: 2.022715906302134

Epoch: 6| Step: 8
Training loss: 2.2204084396362305
Validation loss: 2.010469635327657

Epoch: 6| Step: 9
Training loss: 2.3621716499328613
Validation loss: 2.0182083447774253

Epoch: 6| Step: 10
Training loss: 2.140124559402466
Validation loss: 2.014748533566793

Epoch: 6| Step: 11
Training loss: 1.8487528562545776
Validation loss: 2.0098994771639505

Epoch: 6| Step: 12
Training loss: 2.6091904640197754
Validation loss: 2.014015793800354

Epoch: 6| Step: 13
Training loss: 2.3747973442077637
Validation loss: 2.016274650891622

Epoch: 79| Step: 0
Training loss: 1.9669804573059082
Validation loss: 2.0053389271100364

Epoch: 6| Step: 1
Training loss: 2.13063645362854
Validation loss: 2.0037090380986533

Epoch: 6| Step: 2
Training loss: 1.9669170379638672
Validation loss: 2.005827248096466

Epoch: 6| Step: 3
Training loss: 2.430856704711914
Validation loss: 2.0086596806844077

Epoch: 6| Step: 4
Training loss: 1.992297887802124
Validation loss: 2.0098398129145303

Epoch: 6| Step: 5
Training loss: 2.3918471336364746
Validation loss: 2.0080596009890237

Epoch: 6| Step: 6
Training loss: 2.0332696437835693
Validation loss: 2.009004811445872

Epoch: 6| Step: 7
Training loss: 2.3855528831481934
Validation loss: 2.0156502525011697

Epoch: 6| Step: 8
Training loss: 1.9695508480072021
Validation loss: 2.0210278034210205

Epoch: 6| Step: 9
Training loss: 2.562547206878662
Validation loss: 2.0116809606552124

Epoch: 6| Step: 10
Training loss: 2.0850367546081543
Validation loss: 2.0202358961105347

Epoch: 6| Step: 11
Training loss: 2.1352405548095703
Validation loss: 2.013883630434672

Epoch: 6| Step: 12
Training loss: 1.7644373178482056
Validation loss: 2.0115713278452554

Epoch: 6| Step: 13
Training loss: 2.0858776569366455
Validation loss: 2.0118710001309714

Epoch: 80| Step: 0
Training loss: 2.731769323348999
Validation loss: 2.0232064922650657

Epoch: 6| Step: 1
Training loss: 1.8992663621902466
Validation loss: 2.0202072064081826

Epoch: 6| Step: 2
Training loss: 2.1202731132507324
Validation loss: 2.0334001183509827

Epoch: 6| Step: 3
Training loss: 1.9516749382019043
Validation loss: 2.0321136514345803

Epoch: 6| Step: 4
Training loss: 2.491306781768799
Validation loss: 2.0460360447565713

Epoch: 6| Step: 5
Training loss: 2.8015780448913574
Validation loss: 2.0624930461247764

Epoch: 6| Step: 6
Training loss: 1.9519363641738892
Validation loss: 2.073327918847402

Epoch: 6| Step: 7
Training loss: 1.9340705871582031
Validation loss: 2.0652712980906167

Epoch: 6| Step: 8
Training loss: 2.1105566024780273
Validation loss: 2.0533183415730796

Epoch: 6| Step: 9
Training loss: 2.2075581550598145
Validation loss: 2.028586129347483

Epoch: 6| Step: 10
Training loss: 1.6927342414855957
Validation loss: 2.023910959561666

Epoch: 6| Step: 11
Training loss: 1.7535390853881836
Validation loss: 2.0088904102643332

Epoch: 6| Step: 12
Training loss: 2.3267107009887695
Validation loss: 2.0114471117655435

Epoch: 6| Step: 13
Training loss: 2.528998851776123
Validation loss: 2.022903561592102

Epoch: 81| Step: 0
Training loss: 1.7571078538894653
Validation loss: 2.02681299050649

Epoch: 6| Step: 1
Training loss: 2.670630931854248
Validation loss: 2.0308006207148233

Epoch: 6| Step: 2
Training loss: 2.1157689094543457
Validation loss: 2.0351290504137673

Epoch: 6| Step: 3
Training loss: 2.8166048526763916
Validation loss: 2.0245814323425293

Epoch: 6| Step: 4
Training loss: 2.3329248428344727
Validation loss: 2.027920683224996

Epoch: 6| Step: 5
Training loss: 2.023111581802368
Validation loss: 2.026846706867218

Epoch: 6| Step: 6
Training loss: 1.8350045680999756
Validation loss: 2.0259641806284585

Epoch: 6| Step: 7
Training loss: 1.8468000888824463
Validation loss: 2.0247829953829446

Epoch: 6| Step: 8
Training loss: 2.082223415374756
Validation loss: 2.0194635589917502

Epoch: 6| Step: 9
Training loss: 1.7984181642532349
Validation loss: 2.0248415072758994

Epoch: 6| Step: 10
Training loss: 2.0765628814697266
Validation loss: 2.0211061239242554

Epoch: 6| Step: 11
Training loss: 2.2475640773773193
Validation loss: 2.0105513532956443

Epoch: 6| Step: 12
Training loss: 2.6826868057250977
Validation loss: 2.0161174535751343

Epoch: 6| Step: 13
Training loss: 2.2330477237701416
Validation loss: 2.0164955059687295

Epoch: 82| Step: 0
Training loss: 2.266340970993042
Validation loss: 2.0042572021484375

Epoch: 6| Step: 1
Training loss: 2.272371292114258
Validation loss: 2.000852346420288

Epoch: 6| Step: 2
Training loss: 2.0831313133239746
Validation loss: 2.007932126522064

Epoch: 6| Step: 3
Training loss: 1.944429874420166
Validation loss: 2.021309038003286

Epoch: 6| Step: 4
Training loss: 1.766327142715454
Validation loss: 2.0194268226623535

Epoch: 6| Step: 5
Training loss: 1.8560580015182495
Validation loss: 2.0249599615732827

Epoch: 6| Step: 6
Training loss: 2.607774257659912
Validation loss: 2.0196233789126077

Epoch: 6| Step: 7
Training loss: 2.659961223602295
Validation loss: 2.016842484474182

Epoch: 6| Step: 8
Training loss: 2.6589760780334473
Validation loss: 2.0047896107037864

Epoch: 6| Step: 9
Training loss: 2.197352647781372
Validation loss: 2.008679370085398

Epoch: 6| Step: 10
Training loss: 2.056706428527832
Validation loss: 2.0046549240748086

Epoch: 6| Step: 11
Training loss: 1.7929221391677856
Validation loss: 2.007686893145243

Epoch: 6| Step: 12
Training loss: 2.171943187713623
Validation loss: 2.016265610853831

Epoch: 6| Step: 13
Training loss: 1.9745649099349976
Validation loss: 2.0156492590904236

Epoch: 83| Step: 0
Training loss: 2.080798625946045
Validation loss: 2.019650479157766

Epoch: 6| Step: 1
Training loss: 2.0262632369995117
Validation loss: 2.017757534980774

Epoch: 6| Step: 2
Training loss: 2.4834275245666504
Validation loss: 2.0238736669222512

Epoch: 6| Step: 3
Training loss: 1.647661805152893
Validation loss: 2.0257179737091064

Epoch: 6| Step: 4
Training loss: 1.768324375152588
Validation loss: 2.018035352230072

Epoch: 6| Step: 5
Training loss: 1.8765801191329956
Validation loss: 2.0253934264183044

Epoch: 6| Step: 6
Training loss: 2.0828495025634766
Validation loss: 2.0197552839914956

Epoch: 6| Step: 7
Training loss: 2.182192325592041
Validation loss: 2.029657264550527

Epoch: 6| Step: 8
Training loss: 3.215176820755005
Validation loss: 2.0252080162366233

Epoch: 6| Step: 9
Training loss: 1.8311052322387695
Validation loss: 2.0206251541773477

Epoch: 6| Step: 10
Training loss: 2.05202317237854
Validation loss: 2.018887996673584

Epoch: 6| Step: 11
Training loss: 2.079406499862671
Validation loss: 2.003461559613546

Epoch: 6| Step: 12
Training loss: 2.260772943496704
Validation loss: 2.0035839080810547

Epoch: 6| Step: 13
Training loss: 2.2903010845184326
Validation loss: 1.998840590318044

Epoch: 84| Step: 0
Training loss: 2.261934280395508
Validation loss: 2.0074373284975686

Epoch: 6| Step: 1
Training loss: 1.8410136699676514
Validation loss: 2.0063937306404114

Epoch: 6| Step: 2
Training loss: 2.0373988151550293
Validation loss: 2.007974902788798

Epoch: 6| Step: 3
Training loss: 2.703484296798706
Validation loss: 2.005144695440928

Epoch: 6| Step: 4
Training loss: 1.6406745910644531
Validation loss: 2.008340040842692

Epoch: 6| Step: 5
Training loss: 1.974608302116394
Validation loss: 2.0096837083498635

Epoch: 6| Step: 6
Training loss: 2.178896903991699
Validation loss: 2.0080960988998413

Epoch: 6| Step: 7
Training loss: 2.2698705196380615
Validation loss: 2.0196604331334433

Epoch: 6| Step: 8
Training loss: 2.1999168395996094
Validation loss: 2.0244151751200357

Epoch: 6| Step: 9
Training loss: 1.7729476690292358
Validation loss: 2.0158340334892273

Epoch: 6| Step: 10
Training loss: 2.619335651397705
Validation loss: 2.043076276779175

Epoch: 6| Step: 11
Training loss: 2.369184970855713
Validation loss: 2.029003659884135

Epoch: 6| Step: 12
Training loss: 2.221839427947998
Validation loss: 2.0273592472076416

Epoch: 6| Step: 13
Training loss: 2.409977436065674
Validation loss: 2.0291429360707602

Epoch: 85| Step: 0
Training loss: 2.0020127296447754
Validation loss: 2.02519557873408

Epoch: 6| Step: 1
Training loss: 2.426176071166992
Validation loss: 2.0124146739641824

Epoch: 6| Step: 2
Training loss: 2.0057525634765625
Validation loss: 2.008773903052012

Epoch: 6| Step: 3
Training loss: 1.907088041305542
Validation loss: 2.0020201404889426

Epoch: 6| Step: 4
Training loss: 2.6857519149780273
Validation loss: 2.0044666131337485

Epoch: 6| Step: 5
Training loss: 1.6283701658248901
Validation loss: 1.9996034105618794

Epoch: 6| Step: 6
Training loss: 2.1657392978668213
Validation loss: 2.005401333173116

Epoch: 6| Step: 7
Training loss: 2.152040481567383
Validation loss: 2.0034056107203164

Epoch: 6| Step: 8
Training loss: 2.512307643890381
Validation loss: 2.0051780541737876

Epoch: 6| Step: 9
Training loss: 2.1617298126220703
Validation loss: 1.9979816873868306

Epoch: 6| Step: 10
Training loss: 2.0005643367767334
Validation loss: 1.9988335768381755

Epoch: 6| Step: 11
Training loss: 1.8827747106552124
Validation loss: 2.0010347167650857

Epoch: 6| Step: 12
Training loss: 2.4120848178863525
Validation loss: 1.9919407765070598

Epoch: 6| Step: 13
Training loss: 2.2420127391815186
Validation loss: 1.9938889344533284

Epoch: 86| Step: 0
Training loss: 2.3259708881378174
Validation loss: 1.999600350856781

Epoch: 6| Step: 1
Training loss: 1.818570613861084
Validation loss: 2.0032458702723184

Epoch: 6| Step: 2
Training loss: 2.831658124923706
Validation loss: 2.0139628648757935

Epoch: 6| Step: 3
Training loss: 1.9893831014633179
Validation loss: 2.018633464972178

Epoch: 6| Step: 4
Training loss: 2.0975589752197266
Validation loss: 2.0250204205513

Epoch: 6| Step: 5
Training loss: 2.4843204021453857
Validation loss: 2.045840601126353

Epoch: 6| Step: 6
Training loss: 1.7320330142974854
Validation loss: 2.047947883605957

Epoch: 6| Step: 7
Training loss: 2.128387928009033
Validation loss: 2.0442376732826233

Epoch: 6| Step: 8
Training loss: 2.555370807647705
Validation loss: 2.031307836373647

Epoch: 6| Step: 9
Training loss: 2.187242031097412
Validation loss: 2.0110784570376077

Epoch: 6| Step: 10
Training loss: 2.2268331050872803
Validation loss: 1.9983750383059184

Epoch: 6| Step: 11
Training loss: 1.5532629489898682
Validation loss: 2.0095043182373047

Epoch: 6| Step: 12
Training loss: 2.315598487854004
Validation loss: 2.007994790871938

Epoch: 6| Step: 13
Training loss: 2.0484094619750977
Validation loss: 2.0139305194218955

Epoch: 87| Step: 0
Training loss: 2.8900420665740967
Validation loss: 2.015357514222463

Epoch: 6| Step: 1
Training loss: 2.1884260177612305
Validation loss: 2.010689397652944

Epoch: 6| Step: 2
Training loss: 1.760509967803955
Validation loss: 2.017021119594574

Epoch: 6| Step: 3
Training loss: 2.4955532550811768
Validation loss: 2.0106754302978516

Epoch: 6| Step: 4
Training loss: 2.2412068843841553
Validation loss: 2.006820797920227

Epoch: 6| Step: 5
Training loss: 1.742608666419983
Validation loss: 2.016747693220774

Epoch: 6| Step: 6
Training loss: 2.1433191299438477
Validation loss: 2.001330773035685

Epoch: 6| Step: 7
Training loss: 1.9419150352478027
Validation loss: 2.00292440255483

Epoch: 6| Step: 8
Training loss: 2.178828239440918
Validation loss: 2.0026098092397056

Epoch: 6| Step: 9
Training loss: 2.446211814880371
Validation loss: 2.007908602555593

Epoch: 6| Step: 10
Training loss: 2.169325828552246
Validation loss: 2.0027711391448975

Epoch: 6| Step: 11
Training loss: 2.064876079559326
Validation loss: 1.999957263469696

Epoch: 6| Step: 12
Training loss: 2.1784157752990723
Validation loss: 2.0002891222635903

Epoch: 6| Step: 13
Training loss: 1.7823909521102905
Validation loss: 2.0096747676531472

Epoch: 88| Step: 0
Training loss: 2.4614877700805664
Validation loss: 2.0170124769210815

Epoch: 6| Step: 1
Training loss: 1.9086024761199951
Validation loss: 2.0276764233907065

Epoch: 6| Step: 2
Training loss: 2.3454136848449707
Validation loss: 2.0196728706359863

Epoch: 6| Step: 3
Training loss: 1.6999382972717285
Validation loss: 2.0402854482332864

Epoch: 6| Step: 4
Training loss: 2.6861042976379395
Validation loss: 2.0459216038386026

Epoch: 6| Step: 5
Training loss: 2.4274487495422363
Validation loss: 2.0400584936141968

Epoch: 6| Step: 6
Training loss: 1.5474921464920044
Validation loss: 2.0433953007062278

Epoch: 6| Step: 7
Training loss: 2.3156914710998535
Validation loss: 2.025205055872599

Epoch: 6| Step: 8
Training loss: 2.280301332473755
Validation loss: 2.024797260761261

Epoch: 6| Step: 9
Training loss: 2.448514938354492
Validation loss: 2.0282903909683228

Epoch: 6| Step: 10
Training loss: 2.173572301864624
Validation loss: 2.0223225553830466

Epoch: 6| Step: 11
Training loss: 1.9388515949249268
Validation loss: 2.0184617241223655

Epoch: 6| Step: 12
Training loss: 1.8053267002105713
Validation loss: 2.0014623602231345

Epoch: 6| Step: 13
Training loss: 2.079763889312744
Validation loss: 2.0139541625976562

Epoch: 89| Step: 0
Training loss: 1.6906408071517944
Validation loss: 2.0029199918111167

Epoch: 6| Step: 1
Training loss: 2.547074317932129
Validation loss: 2.0115061601003013

Epoch: 6| Step: 2
Training loss: 2.198812246322632
Validation loss: 2.0052753686904907

Epoch: 6| Step: 3
Training loss: 2.2316741943359375
Validation loss: 2.0183090964953103

Epoch: 6| Step: 4
Training loss: 1.7834420204162598
Validation loss: 2.0064435998598733

Epoch: 6| Step: 5
Training loss: 1.8529795408248901
Validation loss: 2.012264589468638

Epoch: 6| Step: 6
Training loss: 2.511227607727051
Validation loss: 2.014726996421814

Epoch: 6| Step: 7
Training loss: 1.955283284187317
Validation loss: 2.025307019551595

Epoch: 6| Step: 8
Training loss: 1.887053370475769
Validation loss: 2.0086944500605264

Epoch: 6| Step: 9
Training loss: 1.4477760791778564
Validation loss: 2.0148362119992576

Epoch: 6| Step: 10
Training loss: 2.395601987838745
Validation loss: 2.0115349491437278

Epoch: 6| Step: 11
Training loss: 2.3571746349334717
Validation loss: 2.0144784847895303

Epoch: 6| Step: 12
Training loss: 2.5482096672058105
Validation loss: 2.012767771879832

Epoch: 6| Step: 13
Training loss: 2.350205898284912
Validation loss: 2.008950889110565

Epoch: 90| Step: 0
Training loss: 1.8969295024871826
Validation loss: 2.011450250943502

Epoch: 6| Step: 1
Training loss: 2.7305402755737305
Validation loss: 2.001933455467224

Epoch: 6| Step: 2
Training loss: 2.0732247829437256
Validation loss: 2.0013840993245444

Epoch: 6| Step: 3
Training loss: 1.5065581798553467
Validation loss: 2.0080236196517944

Epoch: 6| Step: 4
Training loss: 1.521299123764038
Validation loss: 2.0098488529523215

Epoch: 6| Step: 5
Training loss: 2.4348957538604736
Validation loss: 2.0136283238728843

Epoch: 6| Step: 6
Training loss: 2.275249481201172
Validation loss: 2.0145286917686462

Epoch: 6| Step: 7
Training loss: 1.944272756576538
Validation loss: 2.0148149132728577

Epoch: 6| Step: 8
Training loss: 2.266411781311035
Validation loss: 2.0088782707850137

Epoch: 6| Step: 9
Training loss: 2.06117582321167
Validation loss: 2.0076266129811606

Epoch: 6| Step: 10
Training loss: 2.8476126194000244
Validation loss: 2.0171646078427634

Epoch: 6| Step: 11
Training loss: 2.076516628265381
Validation loss: 2.026393949985504

Epoch: 6| Step: 12
Training loss: 2.0994863510131836
Validation loss: 2.0244518717130027

Epoch: 6| Step: 13
Training loss: 2.3870749473571777
Validation loss: 2.0313194195429483

Epoch: 91| Step: 0
Training loss: 2.050845146179199
Validation loss: 2.013620932896932

Epoch: 6| Step: 1
Training loss: 1.902937650680542
Validation loss: 2.0213729540506997

Epoch: 6| Step: 2
Training loss: 2.3601455688476562
Validation loss: 2.004239797592163

Epoch: 6| Step: 3
Training loss: 2.0874533653259277
Validation loss: 2.010586758454641

Epoch: 6| Step: 4
Training loss: 2.802729606628418
Validation loss: 2.0114482839902244

Epoch: 6| Step: 5
Training loss: 1.8356297016143799
Validation loss: 2.0120180249214172

Epoch: 6| Step: 6
Training loss: 2.2822189331054688
Validation loss: 2.006613075733185

Epoch: 6| Step: 7
Training loss: 2.0585246086120605
Validation loss: 2.0092983643213906

Epoch: 6| Step: 8
Training loss: 1.9199695587158203
Validation loss: 2.0118187069892883

Epoch: 6| Step: 9
Training loss: 2.050605297088623
Validation loss: 2.0104409058888755

Epoch: 6| Step: 10
Training loss: 2.2889952659606934
Validation loss: 2.015681485335032

Epoch: 6| Step: 11
Training loss: 1.82814621925354
Validation loss: 2.019197940826416

Epoch: 6| Step: 12
Training loss: 2.0947623252868652
Validation loss: 2.0198368430137634

Epoch: 6| Step: 13
Training loss: 2.2230465412139893
Validation loss: 2.0069558223088584

Epoch: 92| Step: 0
Training loss: 2.041430711746216
Validation loss: 2.0049208601315818

Epoch: 6| Step: 1
Training loss: 2.4372711181640625
Validation loss: 2.011914531389872

Epoch: 6| Step: 2
Training loss: 1.9996334314346313
Validation loss: 2.0212830106417337

Epoch: 6| Step: 3
Training loss: 1.8745535612106323
Validation loss: 2.0122036139170327

Epoch: 6| Step: 4
Training loss: 2.2514703273773193
Validation loss: 2.0189042687416077

Epoch: 6| Step: 5
Training loss: 1.7594867944717407
Validation loss: 2.014861543973287

Epoch: 6| Step: 6
Training loss: 2.414898157119751
Validation loss: 2.0140086015065513

Epoch: 6| Step: 7
Training loss: 2.403088092803955
Validation loss: 2.0188095370928445

Epoch: 6| Step: 8
Training loss: 2.3226184844970703
Validation loss: 2.0284733970959983

Epoch: 6| Step: 9
Training loss: 2.118018627166748
Validation loss: 2.0433629949887595

Epoch: 6| Step: 10
Training loss: 1.8274807929992676
Validation loss: 2.0472029050191245

Epoch: 6| Step: 11
Training loss: 2.4741463661193848
Validation loss: 2.055369178454081

Epoch: 6| Step: 12
Training loss: 1.6374949216842651
Validation loss: 2.04281485080719

Epoch: 6| Step: 13
Training loss: 2.182934284210205
Validation loss: 2.038770834604899

Epoch: 93| Step: 0
Training loss: 2.566063404083252
Validation loss: 2.026363432407379

Epoch: 6| Step: 1
Training loss: 1.5824580192565918
Validation loss: 2.0197763244311013

Epoch: 6| Step: 2
Training loss: 2.004490852355957
Validation loss: 2.0128137866655984

Epoch: 6| Step: 3
Training loss: 2.2336652278900146
Validation loss: 2.0002503395080566

Epoch: 6| Step: 4
Training loss: 2.447465419769287
Validation loss: 2.0095219810803733

Epoch: 6| Step: 5
Training loss: 2.2556278705596924
Validation loss: 2.0096245408058167

Epoch: 6| Step: 6
Training loss: 1.691710352897644
Validation loss: 2.0077773531277976

Epoch: 6| Step: 7
Training loss: 2.1011345386505127
Validation loss: 2.0074894428253174

Epoch: 6| Step: 8
Training loss: 2.5326757431030273
Validation loss: 2.011363406976064

Epoch: 6| Step: 9
Training loss: 1.7916903495788574
Validation loss: 2.009490887324015

Epoch: 6| Step: 10
Training loss: 2.010287284851074
Validation loss: 2.0121931036313376

Epoch: 6| Step: 11
Training loss: 1.92104971408844
Validation loss: 2.015409072240194

Epoch: 6| Step: 12
Training loss: 2.6267871856689453
Validation loss: 2.0144538084665933

Epoch: 6| Step: 13
Training loss: 2.0443358421325684
Validation loss: 2.0204888780911765

Epoch: 94| Step: 0
Training loss: 1.6176836490631104
Validation loss: 2.0243239998817444

Epoch: 6| Step: 1
Training loss: 1.7572814226150513
Validation loss: 2.0177507599194846

Epoch: 6| Step: 2
Training loss: 2.7055749893188477
Validation loss: 2.0231202443440757

Epoch: 6| Step: 3
Training loss: 1.8909833431243896
Validation loss: 2.032044251759847

Epoch: 6| Step: 4
Training loss: 2.8179683685302734
Validation loss: 2.0294541716575623

Epoch: 6| Step: 5
Training loss: 1.9624683856964111
Validation loss: 2.0370858709017434

Epoch: 6| Step: 6
Training loss: 1.6566059589385986
Validation loss: 2.022702674070994

Epoch: 6| Step: 7
Training loss: 1.7483344078063965
Validation loss: 2.02792626619339

Epoch: 6| Step: 8
Training loss: 2.6973581314086914
Validation loss: 2.026610334714254

Epoch: 6| Step: 9
Training loss: 2.1625709533691406
Validation loss: 2.009238839149475

Epoch: 6| Step: 10
Training loss: 2.3085570335388184
Validation loss: 2.017321785291036

Epoch: 6| Step: 11
Training loss: 2.127143383026123
Validation loss: 2.0032546520233154

Epoch: 6| Step: 12
Training loss: 2.1936137676239014
Validation loss: 2.009576459725698

Epoch: 6| Step: 13
Training loss: 1.8840460777282715
Validation loss: 2.007146934668223

Epoch: 95| Step: 0
Training loss: 2.0737712383270264
Validation loss: 2.003110667069753

Epoch: 6| Step: 1
Training loss: 1.972583532333374
Validation loss: 2.007878541946411

Epoch: 6| Step: 2
Training loss: 2.112604856491089
Validation loss: 2.0093579490979514

Epoch: 6| Step: 3
Training loss: 1.99348783493042
Validation loss: 1.9995113015174866

Epoch: 6| Step: 4
Training loss: 2.9503872394561768
Validation loss: 2.003693103790283

Epoch: 6| Step: 5
Training loss: 2.188936710357666
Validation loss: 2.0013524691263833

Epoch: 6| Step: 6
Training loss: 2.516374111175537
Validation loss: 2.013421813646952

Epoch: 6| Step: 7
Training loss: 2.326355218887329
Validation loss: 2.0109463135401406

Epoch: 6| Step: 8
Training loss: 2.388796329498291
Validation loss: 2.0112030704816184

Epoch: 6| Step: 9
Training loss: 1.3364758491516113
Validation loss: 2.0144410332043967

Epoch: 6| Step: 10
Training loss: 1.8876476287841797
Validation loss: 2.0178509950637817

Epoch: 6| Step: 11
Training loss: 2.0754857063293457
Validation loss: 2.0147602955500283

Epoch: 6| Step: 12
Training loss: 1.996992588043213
Validation loss: 2.027682880560557

Epoch: 6| Step: 13
Training loss: 1.747666597366333
Validation loss: 2.0313830773035684

Epoch: 96| Step: 0
Training loss: 2.3136954307556152
Validation loss: 2.0228620568911233

Epoch: 6| Step: 1
Training loss: 2.397404909133911
Validation loss: 2.026267687479655

Epoch: 6| Step: 2
Training loss: 1.9890649318695068
Validation loss: 2.0313828388849893

Epoch: 6| Step: 3
Training loss: 1.8882546424865723
Validation loss: 2.028843621412913

Epoch: 6| Step: 4
Training loss: 2.3399930000305176
Validation loss: 2.0258865555127463

Epoch: 6| Step: 5
Training loss: 2.024766445159912
Validation loss: 2.034803787867228

Epoch: 6| Step: 6
Training loss: 2.225365400314331
Validation loss: 2.030267874399821

Epoch: 6| Step: 7
Training loss: 1.9658823013305664
Validation loss: 2.0345608989397683

Epoch: 6| Step: 8
Training loss: 2.3161182403564453
Validation loss: 2.0337682565053306

Epoch: 6| Step: 9
Training loss: 1.9736727476119995
Validation loss: 2.024431268374125

Epoch: 6| Step: 10
Training loss: 2.185607433319092
Validation loss: 2.031085213025411

Epoch: 6| Step: 11
Training loss: 2.386565685272217
Validation loss: 2.0259409745534263

Epoch: 6| Step: 12
Training loss: 2.033834934234619
Validation loss: 2.0255538622538247

Epoch: 6| Step: 13
Training loss: 1.5186152458190918
Validation loss: 2.0196314652760825

Epoch: 97| Step: 0
Training loss: 1.9211597442626953
Validation loss: 2.0171717405319214

Epoch: 6| Step: 1
Training loss: 2.1318089962005615
Validation loss: 2.0134273171424866

Epoch: 6| Step: 2
Training loss: 1.9279329776763916
Validation loss: 2.0085508823394775

Epoch: 6| Step: 3
Training loss: 2.303603172302246
Validation loss: 2.0128627816836038

Epoch: 6| Step: 4
Training loss: 1.920680046081543
Validation loss: 2.003744582335154

Epoch: 6| Step: 5
Training loss: 1.8599892854690552
Validation loss: 2.0105863412221274

Epoch: 6| Step: 6
Training loss: 2.357919216156006
Validation loss: 2.004827161629995

Epoch: 6| Step: 7
Training loss: 2.1508285999298096
Validation loss: 2.005376617113749

Epoch: 6| Step: 8
Training loss: 1.8043901920318604
Validation loss: 2.008794605731964

Epoch: 6| Step: 9
Training loss: 2.3498339653015137
Validation loss: 2.0042214592297873

Epoch: 6| Step: 10
Training loss: 2.379159927368164
Validation loss: 2.006578286488851

Epoch: 6| Step: 11
Training loss: 1.964689016342163
Validation loss: 2.018253227074941

Epoch: 6| Step: 12
Training loss: 2.040463924407959
Validation loss: 2.0200377702713013

Epoch: 6| Step: 13
Training loss: 2.4928479194641113
Validation loss: 2.051852504412333

Epoch: 98| Step: 0
Training loss: 2.135472536087036
Validation loss: 2.0355326533317566

Epoch: 6| Step: 1
Training loss: 2.01503324508667
Validation loss: 2.044393519560496

Epoch: 6| Step: 2
Training loss: 1.7373820543289185
Validation loss: 2.0278517603874207

Epoch: 6| Step: 3
Training loss: 2.0594606399536133
Validation loss: 2.028848151365916

Epoch: 6| Step: 4
Training loss: 2.4273180961608887
Validation loss: 2.029146671295166

Epoch: 6| Step: 5
Training loss: 2.157876491546631
Validation loss: 2.0163875222206116

Epoch: 6| Step: 6
Training loss: 1.5517088174819946
Validation loss: 2.0176830490430198

Epoch: 6| Step: 7
Training loss: 2.237950086593628
Validation loss: 2.016393701235453

Epoch: 6| Step: 8
Training loss: 1.912285327911377
Validation loss: 2.0062695741653442

Epoch: 6| Step: 9
Training loss: 2.986607074737549
Validation loss: 2.0130602717399597

Epoch: 6| Step: 10
Training loss: 2.007376194000244
Validation loss: 2.0136985580126443

Epoch: 6| Step: 11
Training loss: 2.409750461578369
Validation loss: 2.00664754708608

Epoch: 6| Step: 12
Training loss: 2.095958709716797
Validation loss: 2.0044261614481607

Epoch: 6| Step: 13
Training loss: 1.8714556694030762
Validation loss: 2.015811006228129

Epoch: 99| Step: 0
Training loss: 2.4252772331237793
Validation loss: 2.0141109426816306

Epoch: 6| Step: 1
Training loss: 2.1040077209472656
Validation loss: 2.012201488018036

Epoch: 6| Step: 2
Training loss: 2.10787296295166
Validation loss: 2.021306296189626

Epoch: 6| Step: 3
Training loss: 2.087136745452881
Validation loss: 2.0350937048594155

Epoch: 6| Step: 4
Training loss: 1.7617288827896118
Validation loss: 2.0233728090922036

Epoch: 6| Step: 5
Training loss: 2.2564597129821777
Validation loss: 2.0377062956492105

Epoch: 6| Step: 6
Training loss: 2.195265293121338
Validation loss: 2.027403791745504

Epoch: 6| Step: 7
Training loss: 1.971336841583252
Validation loss: 2.022602637608846

Epoch: 6| Step: 8
Training loss: 1.9695647954940796
Validation loss: 2.0257901748021445

Epoch: 6| Step: 9
Training loss: 1.6424293518066406
Validation loss: 2.0183984835942588

Epoch: 6| Step: 10
Training loss: 2.2445242404937744
Validation loss: 2.0256118774414062

Epoch: 6| Step: 11
Training loss: 2.4673285484313965
Validation loss: 2.0193041364351907

Epoch: 6| Step: 12
Training loss: 1.9830721616744995
Validation loss: 2.022359391053518

Epoch: 6| Step: 13
Training loss: 2.1905689239501953
Validation loss: 2.0229803919792175

Epoch: 100| Step: 0
Training loss: 3.1024317741394043
Validation loss: 2.0269411404927573

Epoch: 6| Step: 1
Training loss: 1.5767983198165894
Validation loss: 2.0326969226201377

Epoch: 6| Step: 2
Training loss: 1.5219371318817139
Validation loss: 2.0367867151896157

Epoch: 6| Step: 3
Training loss: 1.6922497749328613
Validation loss: 2.0325430631637573

Epoch: 6| Step: 4
Training loss: 2.5076518058776855
Validation loss: 2.0314350525538125

Epoch: 6| Step: 5
Training loss: 1.9130256175994873
Validation loss: 2.0449240803718567

Epoch: 6| Step: 6
Training loss: 2.102158546447754
Validation loss: 2.0267722407976785

Epoch: 6| Step: 7
Training loss: 1.9688829183578491
Validation loss: 2.0379934310913086

Epoch: 6| Step: 8
Training loss: 1.9518117904663086
Validation loss: 2.040017227331797

Epoch: 6| Step: 9
Training loss: 2.422011137008667
Validation loss: 2.0335753560066223

Epoch: 6| Step: 10
Training loss: 1.9789355993270874
Validation loss: 2.0253412326176963

Epoch: 6| Step: 11
Training loss: 2.8093059062957764
Validation loss: 2.0249056021372476

Epoch: 6| Step: 12
Training loss: 1.9739197492599487
Validation loss: 2.0159247716267905

Epoch: 6| Step: 13
Training loss: 2.1182098388671875
Validation loss: 2.019038995107015

Epoch: 101| Step: 0
Training loss: 2.154770851135254
Validation loss: 2.0285771091779075

Epoch: 6| Step: 1
Training loss: 1.9719557762145996
Validation loss: 2.0299805998802185

Epoch: 6| Step: 2
Training loss: 2.0698258876800537
Validation loss: 2.0324220657348633

Epoch: 6| Step: 3
Training loss: 2.449672222137451
Validation loss: 2.032119552294413

Epoch: 6| Step: 4
Training loss: 2.3766157627105713
Validation loss: 2.034700254599253

Epoch: 6| Step: 5
Training loss: 1.8536405563354492
Validation loss: 2.0364275376001992

Epoch: 6| Step: 6
Training loss: 1.9969866275787354
Validation loss: 2.0443400740623474

Epoch: 6| Step: 7
Training loss: 2.7159698009490967
Validation loss: 2.0482587218284607

Epoch: 6| Step: 8
Training loss: 2.195864677429199
Validation loss: 2.036378502845764

Epoch: 6| Step: 9
Training loss: 2.020972490310669
Validation loss: 2.0459582805633545

Epoch: 6| Step: 10
Training loss: 2.0680150985717773
Validation loss: 2.03958668311437

Epoch: 6| Step: 11
Training loss: 2.2779836654663086
Validation loss: 2.0427061518033347

Epoch: 6| Step: 12
Training loss: 1.3841561079025269
Validation loss: 2.025729477405548

Epoch: 6| Step: 13
Training loss: 2.8984322547912598
Validation loss: 2.025723099708557

Epoch: 102| Step: 0
Training loss: 2.4751782417297363
Validation loss: 2.024583339691162

Epoch: 6| Step: 1
Training loss: 1.6577036380767822
Validation loss: 2.021263817946116

Epoch: 6| Step: 2
Training loss: 2.2051544189453125
Validation loss: 2.0064558386802673

Epoch: 6| Step: 3
Training loss: 2.2999587059020996
Validation loss: 2.014027198155721

Epoch: 6| Step: 4
Training loss: 1.9335567951202393
Validation loss: 2.0006911555926004

Epoch: 6| Step: 5
Training loss: 2.463383674621582
Validation loss: 2.0033268133799234

Epoch: 6| Step: 6
Training loss: 1.5717841386795044
Validation loss: 2.0038636525472007

Epoch: 6| Step: 7
Training loss: 2.4622461795806885
Validation loss: 1.9981061816215515

Epoch: 6| Step: 8
Training loss: 2.066620349884033
Validation loss: 2.000020444393158

Epoch: 6| Step: 9
Training loss: 1.8261257410049438
Validation loss: 1.9980120261510212

Epoch: 6| Step: 10
Training loss: 2.4096474647521973
Validation loss: 2.001870115598043

Epoch: 6| Step: 11
Training loss: 2.1444971561431885
Validation loss: 2.006730933984121

Epoch: 6| Step: 12
Training loss: 1.6960055828094482
Validation loss: 2.0015379587809243

Epoch: 6| Step: 13
Training loss: 2.910773754119873
Validation loss: 2.0021496613820395

Epoch: 103| Step: 0
Training loss: 2.311657190322876
Validation loss: 2.0144677559534707

Epoch: 6| Step: 1
Training loss: 2.4155564308166504
Validation loss: 2.0237052838007608

Epoch: 6| Step: 2
Training loss: 2.1987762451171875
Validation loss: 2.0190399090449014

Epoch: 6| Step: 3
Training loss: 1.2549277544021606
Validation loss: 2.02648933728536

Epoch: 6| Step: 4
Training loss: 2.6288678646087646
Validation loss: 2.0271762808163962

Epoch: 6| Step: 5
Training loss: 1.8648780584335327
Validation loss: 2.031022568543752

Epoch: 6| Step: 6
Training loss: 1.8077545166015625
Validation loss: 2.0337164402008057

Epoch: 6| Step: 7
Training loss: 1.7888500690460205
Validation loss: 2.016975144545237

Epoch: 6| Step: 8
Training loss: 2.745797872543335
Validation loss: 2.0174622734387717

Epoch: 6| Step: 9
Training loss: 2.2688000202178955
Validation loss: 2.024649679660797

Epoch: 6| Step: 10
Training loss: 2.162294387817383
Validation loss: 2.0225379864374795

Epoch: 6| Step: 11
Training loss: 1.668554663658142
Validation loss: 2.0228627721468606

Epoch: 6| Step: 12
Training loss: 2.3816134929656982
Validation loss: 2.0167718728383384

Epoch: 6| Step: 13
Training loss: 2.15773344039917
Validation loss: 2.0166393518447876

Epoch: 104| Step: 0
Training loss: 2.096403121948242
Validation loss: 2.0337663094202676

Epoch: 6| Step: 1
Training loss: 2.5161681175231934
Validation loss: 2.0333879590034485

Epoch: 6| Step: 2
Training loss: 2.0990827083587646
Validation loss: 2.0283076961835227

Epoch: 6| Step: 3
Training loss: 1.510263442993164
Validation loss: 2.0195979475975037

Epoch: 6| Step: 4
Training loss: 2.018833637237549
Validation loss: 2.021495540936788

Epoch: 6| Step: 5
Training loss: 2.6243491172790527
Validation loss: 2.0187253952026367

Epoch: 6| Step: 6
Training loss: 1.539046287536621
Validation loss: 2.025195221106211

Epoch: 6| Step: 7
Training loss: 2.475222110748291
Validation loss: 2.028034269809723

Epoch: 6| Step: 8
Training loss: 2.296416997909546
Validation loss: 2.0241264502207437

Epoch: 6| Step: 9
Training loss: 2.637974262237549
Validation loss: 2.03214031457901

Epoch: 6| Step: 10
Training loss: 2.2381486892700195
Validation loss: 2.0292657812436423

Epoch: 6| Step: 11
Training loss: 1.9711813926696777
Validation loss: 2.0256694157918296

Epoch: 6| Step: 12
Training loss: 1.7358119487762451
Validation loss: 2.019177714983622

Epoch: 6| Step: 13
Training loss: 1.5984889268875122
Validation loss: 2.0247340202331543

Epoch: 105| Step: 0
Training loss: 2.2516515254974365
Validation loss: 2.0221080581347146

Epoch: 6| Step: 1
Training loss: 1.719478726387024
Validation loss: 2.019335607687632

Epoch: 6| Step: 2
Training loss: 2.0377140045166016
Validation loss: 2.0189695954322815

Epoch: 6| Step: 3
Training loss: 2.148379325866699
Validation loss: 2.02429469426473

Epoch: 6| Step: 4
Training loss: 1.922568678855896
Validation loss: 2.0226654012997947

Epoch: 6| Step: 5
Training loss: 1.9094955921173096
Validation loss: 2.018909831841787

Epoch: 6| Step: 6
Training loss: 2.7266454696655273
Validation loss: 2.0135696729024253

Epoch: 6| Step: 7
Training loss: 1.5762479305267334
Validation loss: 2.012388229370117

Epoch: 6| Step: 8
Training loss: 1.8260579109191895
Validation loss: 2.0202746391296387

Epoch: 6| Step: 9
Training loss: 2.494290828704834
Validation loss: 2.023982306321462

Epoch: 6| Step: 10
Training loss: 2.1374502182006836
Validation loss: 2.0266019701957703

Epoch: 6| Step: 11
Training loss: 2.005453586578369
Validation loss: 2.0281920631726584

Epoch: 6| Step: 12
Training loss: 2.327198028564453
Validation loss: 2.036566217740377

Epoch: 6| Step: 13
Training loss: 2.2795681953430176
Validation loss: 2.0323155323664346

Epoch: 106| Step: 0
Training loss: 1.807800531387329
Validation loss: 2.0303309162457785

Epoch: 6| Step: 1
Training loss: 2.1213483810424805
Validation loss: 2.0162028471628823

Epoch: 6| Step: 2
Training loss: 2.7884321212768555
Validation loss: 2.021483580271403

Epoch: 6| Step: 3
Training loss: 2.3187437057495117
Validation loss: 2.0198357502619424

Epoch: 6| Step: 4
Training loss: 1.675485372543335
Validation loss: 2.024946630001068

Epoch: 6| Step: 5
Training loss: 2.3152475357055664
Validation loss: 2.016104280948639

Epoch: 6| Step: 6
Training loss: 1.8161027431488037
Validation loss: 2.020745853583018

Epoch: 6| Step: 7
Training loss: 2.1785712242126465
Validation loss: 2.010149339834849

Epoch: 6| Step: 8
Training loss: 1.9545938968658447
Validation loss: 2.016784906387329

Epoch: 6| Step: 9
Training loss: 1.8241331577301025
Validation loss: 2.0189649065335593

Epoch: 6| Step: 10
Training loss: 2.3347606658935547
Validation loss: 2.0198148687680564

Epoch: 6| Step: 11
Training loss: 2.442286491394043
Validation loss: 2.0184995333353677

Epoch: 6| Step: 12
Training loss: 2.1021595001220703
Validation loss: 2.0172699689865112

Epoch: 6| Step: 13
Training loss: 1.5103801488876343
Validation loss: 2.015113631884257

Epoch: 107| Step: 0
Training loss: 2.0325369834899902
Validation loss: 2.013022502263387

Epoch: 6| Step: 1
Training loss: 2.0451550483703613
Validation loss: 2.012003719806671

Epoch: 6| Step: 2
Training loss: 1.8548874855041504
Validation loss: 2.0111698706944785

Epoch: 6| Step: 3
Training loss: 2.3941848278045654
Validation loss: 2.018292327721914

Epoch: 6| Step: 4
Training loss: 1.4501688480377197
Validation loss: 2.0180146296819053

Epoch: 6| Step: 5
Training loss: 2.2591614723205566
Validation loss: 2.0157278378804526

Epoch: 6| Step: 6
Training loss: 2.052135705947876
Validation loss: 2.01561047633489

Epoch: 6| Step: 7
Training loss: 2.28371262550354
Validation loss: 2.0223673383394876

Epoch: 6| Step: 8
Training loss: 1.873849630355835
Validation loss: 2.016661544640859

Epoch: 6| Step: 9
Training loss: 2.0712203979492188
Validation loss: 2.0285790959993997

Epoch: 6| Step: 10
Training loss: 2.6292834281921387
Validation loss: 2.010867714881897

Epoch: 6| Step: 11
Training loss: 1.790163278579712
Validation loss: 2.012098014354706

Epoch: 6| Step: 12
Training loss: 2.4719650745391846
Validation loss: 2.0180288553237915

Epoch: 6| Step: 13
Training loss: 1.8421869277954102
Validation loss: 2.018182873725891

Epoch: 108| Step: 0
Training loss: 2.200582504272461
Validation loss: 2.0194984475771585

Epoch: 6| Step: 1
Training loss: 2.495678424835205
Validation loss: 2.024307588736216

Epoch: 6| Step: 2
Training loss: 2.6009671688079834
Validation loss: 2.023382822672526

Epoch: 6| Step: 3
Training loss: 2.3340840339660645
Validation loss: 2.026241679986318

Epoch: 6| Step: 4
Training loss: 1.6078729629516602
Validation loss: 2.0310407082239785

Epoch: 6| Step: 5
Training loss: 1.5669631958007812
Validation loss: 2.022840122381846

Epoch: 6| Step: 6
Training loss: 1.9958641529083252
Validation loss: 2.0206878185272217

Epoch: 6| Step: 7
Training loss: 1.9251925945281982
Validation loss: 2.0231018662452698

Epoch: 6| Step: 8
Training loss: 2.0067245960235596
Validation loss: 2.024438222249349

Epoch: 6| Step: 9
Training loss: 2.915595054626465
Validation loss: 2.0331323544184365

Epoch: 6| Step: 10
Training loss: 1.7723209857940674
Validation loss: 2.0128551920255027

Epoch: 6| Step: 11
Training loss: 1.8562002182006836
Validation loss: 2.018520494302114

Epoch: 6| Step: 12
Training loss: 1.970046877861023
Validation loss: 2.0171130100886026

Epoch: 6| Step: 13
Training loss: 1.7461919784545898
Validation loss: 2.0199132363001504

Epoch: 109| Step: 0
Training loss: 2.54723858833313
Validation loss: 2.0137682954470315

Epoch: 6| Step: 1
Training loss: 1.481777548789978
Validation loss: 2.02746852238973

Epoch: 6| Step: 2
Training loss: 1.911381483078003
Validation loss: 2.0153451760609946

Epoch: 6| Step: 3
Training loss: 1.9642033576965332
Validation loss: 2.0216512282689414

Epoch: 6| Step: 4
Training loss: 2.4375991821289062
Validation loss: 2.012064278125763

Epoch: 6| Step: 5
Training loss: 1.8142201900482178
Validation loss: 2.0203753312428794

Epoch: 6| Step: 6
Training loss: 1.9649327993392944
Validation loss: 2.0242570439974465

Epoch: 6| Step: 7
Training loss: 2.3301138877868652
Validation loss: 2.0236889719963074

Epoch: 6| Step: 8
Training loss: 2.2290215492248535
Validation loss: 2.0305741826693215

Epoch: 6| Step: 9
Training loss: 1.7247943878173828
Validation loss: 2.0396384596824646

Epoch: 6| Step: 10
Training loss: 2.319455862045288
Validation loss: 2.042076309521993

Epoch: 6| Step: 11
Training loss: 2.1979544162750244
Validation loss: 2.043335219224294

Epoch: 6| Step: 12
Training loss: 2.164456367492676
Validation loss: 2.0346370538075766

Epoch: 6| Step: 13
Training loss: 2.0678317546844482
Validation loss: 2.0390787720680237

Epoch: 110| Step: 0
Training loss: 1.837517261505127
Validation loss: 2.048595150311788

Epoch: 6| Step: 1
Training loss: 1.8056926727294922
Validation loss: 2.0419307549794516

Epoch: 6| Step: 2
Training loss: 1.7807893753051758
Validation loss: 2.0387786626815796

Epoch: 6| Step: 3
Training loss: 1.9277567863464355
Validation loss: 2.0428245266278586

Epoch: 6| Step: 4
Training loss: 2.4162001609802246
Validation loss: 2.047360340754191

Epoch: 6| Step: 5
Training loss: 2.044203758239746
Validation loss: 2.025828699270884

Epoch: 6| Step: 6
Training loss: 1.4633512496948242
Validation loss: 2.0362656513849893

Epoch: 6| Step: 7
Training loss: 1.5885658264160156
Validation loss: 2.0367669065793357

Epoch: 6| Step: 8
Training loss: 2.217960834503174
Validation loss: 2.040622591972351

Epoch: 6| Step: 9
Training loss: 2.2474372386932373
Validation loss: 2.033181369304657

Epoch: 6| Step: 10
Training loss: 2.3790972232818604
Validation loss: 2.0202041467030845

Epoch: 6| Step: 11
Training loss: 2.1758899688720703
Validation loss: 2.024373769760132

Epoch: 6| Step: 12
Training loss: 2.264636516571045
Validation loss: 2.012086033821106

Epoch: 6| Step: 13
Training loss: 2.717855930328369
Validation loss: 2.0111816922823587

Epoch: 111| Step: 0
Training loss: 2.5233287811279297
Validation loss: 2.0096702575683594

Epoch: 6| Step: 1
Training loss: 2.0473380088806152
Validation loss: 2.016301393508911

Epoch: 6| Step: 2
Training loss: 1.3426551818847656
Validation loss: 2.020307461420695

Epoch: 6| Step: 3
Training loss: 1.2768700122833252
Validation loss: 2.0159425934155784

Epoch: 6| Step: 4
Training loss: 2.232477903366089
Validation loss: 2.0126048922538757

Epoch: 6| Step: 5
Training loss: 1.8545408248901367
Validation loss: 2.018337825934092

Epoch: 6| Step: 6
Training loss: 2.156629800796509
Validation loss: 2.0236348509788513

Epoch: 6| Step: 7
Training loss: 2.1317477226257324
Validation loss: 2.037774900595347

Epoch: 6| Step: 8
Training loss: 2.1003823280334473
Validation loss: 2.021685004234314

Epoch: 6| Step: 9
Training loss: 1.8583757877349854
Validation loss: 2.03172504901886

Epoch: 6| Step: 10
Training loss: 2.7895278930664062
Validation loss: 2.021072785059611

Epoch: 6| Step: 11
Training loss: 2.372620105743408
Validation loss: 2.0379969477653503

Epoch: 6| Step: 12
Training loss: 2.7365689277648926
Validation loss: 2.040016770362854

Epoch: 6| Step: 13
Training loss: 1.9597465991973877
Validation loss: 2.0411357084910073

Epoch: 112| Step: 0
Training loss: 2.6772055625915527
Validation loss: 2.0441881815592446

Epoch: 6| Step: 1
Training loss: 2.483489751815796
Validation loss: 2.038737734158834

Epoch: 6| Step: 2
Training loss: 2.588581085205078
Validation loss: 2.029918909072876

Epoch: 6| Step: 3
Training loss: 1.886444330215454
Validation loss: 2.0189302364985147

Epoch: 6| Step: 4
Training loss: 1.242537021636963
Validation loss: 2.0219306349754333

Epoch: 6| Step: 5
Training loss: 1.4839892387390137
Validation loss: 2.024159848690033

Epoch: 6| Step: 6
Training loss: 2.4976859092712402
Validation loss: 2.037684738636017

Epoch: 6| Step: 7
Training loss: 1.611135721206665
Validation loss: 2.0358701745669046

Epoch: 6| Step: 8
Training loss: 2.092411756515503
Validation loss: 2.0348238746325173

Epoch: 6| Step: 9
Training loss: 1.8010399341583252
Validation loss: 2.0509258111317954

Epoch: 6| Step: 10
Training loss: 2.179574489593506
Validation loss: 2.0467452804247537

Epoch: 6| Step: 11
Training loss: 2.418795585632324
Validation loss: 2.0679472287495932

Epoch: 6| Step: 12
Training loss: 1.6218323707580566
Validation loss: 2.058111548423767

Epoch: 6| Step: 13
Training loss: 2.6274728775024414
Validation loss: 2.066500465075175

Epoch: 113| Step: 0
Training loss: 1.773419976234436
Validation loss: 2.071469267209371

Epoch: 6| Step: 1
Training loss: 1.8351216316223145
Validation loss: 2.067822436491648

Epoch: 6| Step: 2
Training loss: 2.645035982131958
Validation loss: 2.081316868464152

Epoch: 6| Step: 3
Training loss: 1.7662911415100098
Validation loss: 2.0808753967285156

Epoch: 6| Step: 4
Training loss: 1.6818617582321167
Validation loss: 2.0654269258181253

Epoch: 6| Step: 5
Training loss: 1.4119212627410889
Validation loss: 2.0675270160039267

Epoch: 6| Step: 6
Training loss: 1.9852807521820068
Validation loss: 2.0458115140597024

Epoch: 6| Step: 7
Training loss: 2.6569719314575195
Validation loss: 2.0423697034517923

Epoch: 6| Step: 8
Training loss: 2.312575101852417
Validation loss: 2.024952232837677

Epoch: 6| Step: 9
Training loss: 2.4500889778137207
Validation loss: 2.0302040179570517

Epoch: 6| Step: 10
Training loss: 2.0947117805480957
Validation loss: 2.026370942592621

Epoch: 6| Step: 11
Training loss: 2.2342190742492676
Validation loss: 2.017858843008677

Epoch: 6| Step: 12
Training loss: 2.010932207107544
Validation loss: 2.018222451210022

Epoch: 6| Step: 13
Training loss: 2.1916189193725586
Validation loss: 2.0294389526049295

Epoch: 114| Step: 0
Training loss: 1.7804151773452759
Validation loss: 2.0301897724469504

Epoch: 6| Step: 1
Training loss: 2.070772647857666
Validation loss: 2.027234971523285

Epoch: 6| Step: 2
Training loss: 1.7911946773529053
Validation loss: 2.0304367343584695

Epoch: 6| Step: 3
Training loss: 1.8335011005401611
Validation loss: 2.026996592680613

Epoch: 6| Step: 4
Training loss: 1.8036164045333862
Validation loss: 2.0319939255714417

Epoch: 6| Step: 5
Training loss: 2.0854179859161377
Validation loss: 2.036367138226827

Epoch: 6| Step: 6
Training loss: 2.3504462242126465
Validation loss: 2.0377157330513

Epoch: 6| Step: 7
Training loss: 2.559943437576294
Validation loss: 2.0452829202016196

Epoch: 6| Step: 8
Training loss: 1.943758487701416
Validation loss: 2.045745293299357

Epoch: 6| Step: 9
Training loss: 2.044179916381836
Validation loss: 2.0491546591122947

Epoch: 6| Step: 10
Training loss: 2.501903533935547
Validation loss: 2.0446039835611978

Epoch: 6| Step: 11
Training loss: 2.634993314743042
Validation loss: 2.0402645270029702

Epoch: 6| Step: 12
Training loss: 1.7985327243804932
Validation loss: 2.0341951052347818

Epoch: 6| Step: 13
Training loss: 1.812603235244751
Validation loss: 2.0277819434801736

Epoch: 115| Step: 0
Training loss: 2.2815370559692383
Validation loss: 2.029650092124939

Epoch: 6| Step: 1
Training loss: 2.1133525371551514
Validation loss: 2.029754916826884

Epoch: 6| Step: 2
Training loss: 2.2550697326660156
Validation loss: 2.022175888220469

Epoch: 6| Step: 3
Training loss: 1.868565320968628
Validation loss: 2.029349684715271

Epoch: 6| Step: 4
Training loss: 2.6376776695251465
Validation loss: 2.032245179017385

Epoch: 6| Step: 5
Training loss: 1.4930469989776611
Validation loss: 2.0363150040308633

Epoch: 6| Step: 6
Training loss: 2.226118326187134
Validation loss: 2.032017787297567

Epoch: 6| Step: 7
Training loss: 2.162118434906006
Validation loss: 2.03543750445048

Epoch: 6| Step: 8
Training loss: 1.674471139907837
Validation loss: 2.0258636673291526

Epoch: 6| Step: 9
Training loss: 1.8638582229614258
Validation loss: 2.0354772011439004

Epoch: 6| Step: 10
Training loss: 2.090029716491699
Validation loss: 2.0383317867914834

Epoch: 6| Step: 11
Training loss: 1.8746812343597412
Validation loss: 2.0285487373669944

Epoch: 6| Step: 12
Training loss: 2.6238784790039062
Validation loss: 2.025280555089315

Epoch: 6| Step: 13
Training loss: 1.601013422012329
Validation loss: 2.025659461816152

Epoch: 116| Step: 0
Training loss: 2.044278621673584
Validation loss: 2.0298737287521362

Epoch: 6| Step: 1
Training loss: 2.0391557216644287
Validation loss: 2.0363380908966064

Epoch: 6| Step: 2
Training loss: 1.6092255115509033
Validation loss: 2.0328141252199807

Epoch: 6| Step: 3
Training loss: 1.7009696960449219
Validation loss: 2.0374125242233276

Epoch: 6| Step: 4
Training loss: 2.2785027027130127
Validation loss: 2.041306952635447

Epoch: 6| Step: 5
Training loss: 2.5467615127563477
Validation loss: 2.0409419735272727

Epoch: 6| Step: 6
Training loss: 1.9639602899551392
Validation loss: 2.0309478044509888

Epoch: 6| Step: 7
Training loss: 2.294177532196045
Validation loss: 2.035890301068624

Epoch: 6| Step: 8
Training loss: 2.2359328269958496
Validation loss: 2.040018459161123

Epoch: 6| Step: 9
Training loss: 2.142507553100586
Validation loss: 2.048302928606669

Epoch: 6| Step: 10
Training loss: 2.23073148727417
Validation loss: 2.047215302785238

Epoch: 6| Step: 11
Training loss: 1.9088462591171265
Validation loss: 2.0452107191085815

Epoch: 6| Step: 12
Training loss: 1.6671724319458008
Validation loss: 2.0458011627197266

Epoch: 6| Step: 13
Training loss: 1.9747295379638672
Validation loss: 2.039374053478241

Epoch: 117| Step: 0
Training loss: 2.3568215370178223
Validation loss: 2.043203671773275

Epoch: 6| Step: 1
Training loss: 2.7686848640441895
Validation loss: 2.0269397099812827

Epoch: 6| Step: 2
Training loss: 1.4169188737869263
Validation loss: 2.0282986164093018

Epoch: 6| Step: 3
Training loss: 2.032038450241089
Validation loss: 2.0233551263809204

Epoch: 6| Step: 4
Training loss: 2.0262744426727295
Validation loss: 2.029310166835785

Epoch: 6| Step: 5
Training loss: 1.597510576248169
Validation loss: 2.022508442401886

Epoch: 6| Step: 6
Training loss: 1.9934790134429932
Validation loss: 2.0322668751080832

Epoch: 6| Step: 7
Training loss: 2.1932806968688965
Validation loss: 2.0339473485946655

Epoch: 6| Step: 8
Training loss: 2.8073959350585938
Validation loss: 2.0472537875175476

Epoch: 6| Step: 9
Training loss: 1.4847049713134766
Validation loss: 2.044449587663015

Epoch: 6| Step: 10
Training loss: 2.0929794311523438
Validation loss: 2.0377779603004456

Epoch: 6| Step: 11
Training loss: 2.0458481311798096
Validation loss: 2.040651520093282

Epoch: 6| Step: 12
Training loss: 1.855281114578247
Validation loss: 2.02813712755839

Epoch: 6| Step: 13
Training loss: 2.3353753089904785
Validation loss: 2.013981898625692

Epoch: 118| Step: 0
Training loss: 2.070727586746216
Validation loss: 2.0195569594701133

Epoch: 6| Step: 1
Training loss: 2.1900811195373535
Validation loss: 2.011448164780935

Epoch: 6| Step: 2
Training loss: 2.0250771045684814
Validation loss: 2.00759748617808

Epoch: 6| Step: 3
Training loss: 2.124995231628418
Validation loss: 2.0014549493789673

Epoch: 6| Step: 4
Training loss: 2.131558656692505
Validation loss: 2.017822484175364

Epoch: 6| Step: 5
Training loss: 1.5230839252471924
Validation loss: 2.020469685395559

Epoch: 6| Step: 6
Training loss: 2.1797690391540527
Validation loss: 2.0121474663416543

Epoch: 6| Step: 7
Training loss: 2.309335231781006
Validation loss: 2.0172280073165894

Epoch: 6| Step: 8
Training loss: 1.49411940574646
Validation loss: 2.0152057806650796

Epoch: 6| Step: 9
Training loss: 2.40671443939209
Validation loss: 2.0192427237828574

Epoch: 6| Step: 10
Training loss: 2.043252944946289
Validation loss: 2.011872172355652

Epoch: 6| Step: 11
Training loss: 2.1354780197143555
Validation loss: 2.015437106291453

Epoch: 6| Step: 12
Training loss: 2.5456886291503906
Validation loss: 2.0213541984558105

Epoch: 6| Step: 13
Training loss: 2.1242880821228027
Validation loss: 2.0299153923988342

Epoch: 119| Step: 0
Training loss: 2.275244951248169
Validation loss: 2.02799383799235

Epoch: 6| Step: 1
Training loss: 2.0340170860290527
Validation loss: 2.0226683219273887

Epoch: 6| Step: 2
Training loss: 2.3316755294799805
Validation loss: 2.0434693892796836

Epoch: 6| Step: 3
Training loss: 2.630683422088623
Validation loss: 2.035625378290812

Epoch: 6| Step: 4
Training loss: 2.123877763748169
Validation loss: 2.038379152615865

Epoch: 6| Step: 5
Training loss: 2.6856729984283447
Validation loss: 2.03214164574941

Epoch: 6| Step: 6
Training loss: 1.9232473373413086
Validation loss: 2.030912677447001

Epoch: 6| Step: 7
Training loss: 1.8272743225097656
Validation loss: 2.033529063065847

Epoch: 6| Step: 8
Training loss: 1.9778742790222168
Validation loss: 2.036500652631124

Epoch: 6| Step: 9
Training loss: 1.3767688274383545
Validation loss: 2.031619886557261

Epoch: 6| Step: 10
Training loss: 1.6472344398498535
Validation loss: 2.032346328099569

Epoch: 6| Step: 11
Training loss: 2.2053189277648926
Validation loss: 2.0407383839289346

Epoch: 6| Step: 12
Training loss: 1.4307124614715576
Validation loss: 2.044295330842336

Epoch: 6| Step: 13
Training loss: 2.1996824741363525
Validation loss: 2.053585390249888

Epoch: 120| Step: 0
Training loss: 2.4190914630889893
Validation loss: 2.064048707485199

Epoch: 6| Step: 1
Training loss: 1.8207125663757324
Validation loss: 2.055351138114929

Epoch: 6| Step: 2
Training loss: 2.0029852390289307
Validation loss: 2.0599406957626343

Epoch: 6| Step: 3
Training loss: 1.8121836185455322
Validation loss: 2.065890391667684

Epoch: 6| Step: 4
Training loss: 2.453434467315674
Validation loss: 2.0800858537356057

Epoch: 6| Step: 5
Training loss: 2.3579325675964355
Validation loss: 2.0625400145848594

Epoch: 6| Step: 6
Training loss: 2.024977684020996
Validation loss: 2.0618803898493447

Epoch: 6| Step: 7
Training loss: 2.0092029571533203
Validation loss: 2.064794103304545

Epoch: 6| Step: 8
Training loss: 2.3180506229400635
Validation loss: 2.0533958276112876

Epoch: 6| Step: 9
Training loss: 2.22505521774292
Validation loss: 2.047687272230784

Epoch: 6| Step: 10
Training loss: 1.9186822175979614
Validation loss: 2.0470972458521524

Epoch: 6| Step: 11
Training loss: 1.7480674982070923
Validation loss: 2.046081026395162

Epoch: 6| Step: 12
Training loss: 1.3724725246429443
Validation loss: 2.0372925202051797

Epoch: 6| Step: 13
Training loss: 2.2688469886779785
Validation loss: 2.0259192983309426

Epoch: 121| Step: 0
Training loss: 2.0218515396118164
Validation loss: 2.0319090286890664

Epoch: 6| Step: 1
Training loss: 2.1200203895568848
Validation loss: 2.036686817804972

Epoch: 6| Step: 2
Training loss: 2.2984137535095215
Validation loss: 2.0334008733431497

Epoch: 6| Step: 3
Training loss: 2.1271419525146484
Validation loss: 2.0282350977261863

Epoch: 6| Step: 4
Training loss: 2.198016881942749
Validation loss: 2.0280301570892334

Epoch: 6| Step: 5
Training loss: 1.6946152448654175
Validation loss: 2.022287746270498

Epoch: 6| Step: 6
Training loss: 1.3725391626358032
Validation loss: 2.0375106732050576

Epoch: 6| Step: 7
Training loss: 2.286445140838623
Validation loss: 2.036056915918986

Epoch: 6| Step: 8
Training loss: 2.3522775173187256
Validation loss: 2.04505048195521

Epoch: 6| Step: 9
Training loss: 1.6120284795761108
Validation loss: 2.070108413696289

Epoch: 6| Step: 10
Training loss: 2.2247109413146973
Validation loss: 2.055988093217214

Epoch: 6| Step: 11
Training loss: 1.8761298656463623
Validation loss: 2.058983266353607

Epoch: 6| Step: 12
Training loss: 2.1247293949127197
Validation loss: 2.0609886050224304

Epoch: 6| Step: 13
Training loss: 2.437561511993408
Validation loss: 2.0474417408307395

Epoch: 122| Step: 0
Training loss: 1.5032472610473633
Validation loss: 2.0385831594467163

Epoch: 6| Step: 1
Training loss: 1.675804615020752
Validation loss: 2.053484578927358

Epoch: 6| Step: 2
Training loss: 2.3727293014526367
Validation loss: 2.0492069522539773

Epoch: 6| Step: 3
Training loss: 1.6628878116607666
Validation loss: 2.043776035308838

Epoch: 6| Step: 4
Training loss: 1.5774484872817993
Validation loss: 2.0500460863113403

Epoch: 6| Step: 5
Training loss: 1.6580915451049805
Validation loss: 2.0427825252215066

Epoch: 6| Step: 6
Training loss: 2.9479026794433594
Validation loss: 2.0500287612279258

Epoch: 6| Step: 7
Training loss: 2.7237939834594727
Validation loss: 2.0372742215792337

Epoch: 6| Step: 8
Training loss: 2.203050374984741
Validation loss: 2.0452162822087607

Epoch: 6| Step: 9
Training loss: 1.6422269344329834
Validation loss: 2.035064458847046

Epoch: 6| Step: 10
Training loss: 1.8638455867767334
Validation loss: 2.044991056124369

Epoch: 6| Step: 11
Training loss: 2.5148191452026367
Validation loss: 2.039193948109945

Epoch: 6| Step: 12
Training loss: 2.206090211868286
Validation loss: 2.0382442275683084

Epoch: 6| Step: 13
Training loss: 1.9343957901000977
Validation loss: 2.048009932041168

Epoch: 123| Step: 0
Training loss: 2.925909996032715
Validation loss: 2.052614212036133

Epoch: 6| Step: 1
Training loss: 2.0264604091644287
Validation loss: 2.049968441327413

Epoch: 6| Step: 2
Training loss: 1.586099624633789
Validation loss: 2.06024036804835

Epoch: 6| Step: 3
Training loss: 2.6091227531433105
Validation loss: 2.0573707024256387

Epoch: 6| Step: 4
Training loss: 1.4236477613449097
Validation loss: 2.0639864404996238

Epoch: 6| Step: 5
Training loss: 2.167759418487549
Validation loss: 2.056429406007131

Epoch: 6| Step: 6
Training loss: 1.6876130104064941
Validation loss: 2.063371400038401

Epoch: 6| Step: 7
Training loss: 1.538365364074707
Validation loss: 2.056036412715912

Epoch: 6| Step: 8
Training loss: 2.487064838409424
Validation loss: 2.039604047934214

Epoch: 6| Step: 9
Training loss: 2.061770439147949
Validation loss: 2.0333275397618613

Epoch: 6| Step: 10
Training loss: 1.4940255880355835
Validation loss: 2.034057299296061

Epoch: 6| Step: 11
Training loss: 2.870119094848633
Validation loss: 2.038693130016327

Epoch: 6| Step: 12
Training loss: 2.3064687252044678
Validation loss: 2.0438596407572427

Epoch: 6| Step: 13
Training loss: 1.3808029890060425
Validation loss: 2.0340602000554404

Epoch: 124| Step: 0
Training loss: 1.8121483325958252
Validation loss: 2.0443785190582275

Epoch: 6| Step: 1
Training loss: 1.8761199712753296
Validation loss: 2.0496291518211365

Epoch: 6| Step: 2
Training loss: 1.815586805343628
Validation loss: 2.0228782494862876

Epoch: 6| Step: 3
Training loss: 2.5454578399658203
Validation loss: 2.045044799645742

Epoch: 6| Step: 4
Training loss: 1.6017637252807617
Validation loss: 2.0409650603930154

Epoch: 6| Step: 5
Training loss: 2.992091417312622
Validation loss: 2.042023519674937

Epoch: 6| Step: 6
Training loss: 1.8707894086837769
Validation loss: 2.044387181599935

Epoch: 6| Step: 7
Training loss: 1.705675721168518
Validation loss: 2.0538343588511148

Epoch: 6| Step: 8
Training loss: 1.7859514951705933
Validation loss: 2.043377677599589

Epoch: 6| Step: 9
Training loss: 1.6614910364151
Validation loss: 2.0470194816589355

Epoch: 6| Step: 10
Training loss: 2.9414453506469727
Validation loss: 2.0507140159606934

Epoch: 6| Step: 11
Training loss: 1.6886423826217651
Validation loss: 2.0507738987604776

Epoch: 6| Step: 12
Training loss: 2.1788086891174316
Validation loss: 2.045916497707367

Epoch: 6| Step: 13
Training loss: 2.0157604217529297
Validation loss: 2.0468110839525857

Epoch: 125| Step: 0
Training loss: 2.3679394721984863
Validation loss: 2.0507583618164062

Epoch: 6| Step: 1
Training loss: 1.5705653429031372
Validation loss: 2.0535513957341514

Epoch: 6| Step: 2
Training loss: 1.6151350736618042
Validation loss: 2.048272669315338

Epoch: 6| Step: 3
Training loss: 1.4871840476989746
Validation loss: 2.0564947525660195

Epoch: 6| Step: 4
Training loss: 2.290207862854004
Validation loss: 2.0577595035235086

Epoch: 6| Step: 5
Training loss: 1.935269832611084
Validation loss: 2.0432962775230408

Epoch: 6| Step: 6
Training loss: 1.7641127109527588
Validation loss: 2.0417538483937583

Epoch: 6| Step: 7
Training loss: 2.306717872619629
Validation loss: 2.036678433418274

Epoch: 6| Step: 8
Training loss: 1.7386095523834229
Validation loss: 2.0520055294036865

Epoch: 6| Step: 9
Training loss: 2.363161087036133
Validation loss: 2.0462230443954468

Epoch: 6| Step: 10
Training loss: 2.7201004028320312
Validation loss: 2.0395283897717795

Epoch: 6| Step: 11
Training loss: 1.8603365421295166
Validation loss: 2.0431718627611795

Epoch: 6| Step: 12
Training loss: 2.1367440223693848
Validation loss: 2.032291273276011

Epoch: 6| Step: 13
Training loss: 2.2227907180786133
Validation loss: 2.0401217540105185

Epoch: 126| Step: 0
Training loss: 1.6135551929473877
Validation loss: 2.03156441450119

Epoch: 6| Step: 1
Training loss: 2.3976640701293945
Validation loss: 2.03369277715683

Epoch: 6| Step: 2
Training loss: 1.8596947193145752
Validation loss: 2.0342626372973123

Epoch: 6| Step: 3
Training loss: 2.0733304023742676
Validation loss: 2.038931886355082

Epoch: 6| Step: 4
Training loss: 1.6270546913146973
Validation loss: 2.043589095274607

Epoch: 6| Step: 5
Training loss: 1.7289811372756958
Validation loss: 2.0491250356038413

Epoch: 6| Step: 6
Training loss: 1.9556068181991577
Validation loss: 2.045062561829885

Epoch: 6| Step: 7
Training loss: 1.8177820444107056
Validation loss: 2.066400130589803

Epoch: 6| Step: 8
Training loss: 2.544994831085205
Validation loss: 2.0486765702565513

Epoch: 6| Step: 9
Training loss: 2.366811752319336
Validation loss: 2.0572963754336038

Epoch: 6| Step: 10
Training loss: 2.2333357334136963
Validation loss: 2.0491557916005454

Epoch: 6| Step: 11
Training loss: 1.4257745742797852
Validation loss: 2.0545155803362527

Epoch: 6| Step: 12
Training loss: 2.5813980102539062
Validation loss: 2.0486958622932434

Epoch: 6| Step: 13
Training loss: 2.0200867652893066
Validation loss: 2.067389190196991

Epoch: 127| Step: 0
Training loss: 1.9115087985992432
Validation loss: 2.0642735759417215

Epoch: 6| Step: 1
Training loss: 1.9622348546981812
Validation loss: 2.061073442300161

Epoch: 6| Step: 2
Training loss: 1.7943251132965088
Validation loss: 2.0488094687461853

Epoch: 6| Step: 3
Training loss: 2.0072712898254395
Validation loss: 2.06614617506663

Epoch: 6| Step: 4
Training loss: 2.7881035804748535
Validation loss: 2.053946236769358

Epoch: 6| Step: 5
Training loss: 2.1365280151367188
Validation loss: 2.049701372782389

Epoch: 6| Step: 6
Training loss: 2.600511074066162
Validation loss: 2.0373658339182534

Epoch: 6| Step: 7
Training loss: 2.04927921295166
Validation loss: 2.036821722984314

Epoch: 6| Step: 8
Training loss: 1.633976697921753
Validation loss: 2.0488879879315696

Epoch: 6| Step: 9
Training loss: 1.5499396324157715
Validation loss: 2.041035850842794

Epoch: 6| Step: 10
Training loss: 1.8310956954956055
Validation loss: 2.0460791190465293

Epoch: 6| Step: 11
Training loss: 1.798466682434082
Validation loss: 2.057622969150543

Epoch: 6| Step: 12
Training loss: 2.0194144248962402
Validation loss: 2.034536381562551

Epoch: 6| Step: 13
Training loss: 2.1523966789245605
Validation loss: 2.0371554692586265

Epoch: 128| Step: 0
Training loss: 1.8892405033111572
Validation loss: 2.0344658692677817

Epoch: 6| Step: 1
Training loss: 2.3420329093933105
Validation loss: 2.04778645435969

Epoch: 6| Step: 2
Training loss: 1.9068400859832764
Validation loss: 2.049277146657308

Epoch: 6| Step: 3
Training loss: 1.6613900661468506
Validation loss: 2.041241010030111

Epoch: 6| Step: 4
Training loss: 2.36386775970459
Validation loss: 2.0518847505251565

Epoch: 6| Step: 5
Training loss: 1.6583428382873535
Validation loss: 2.0491108099619546

Epoch: 6| Step: 6
Training loss: 1.9608421325683594
Validation loss: 2.0412020285924277

Epoch: 6| Step: 7
Training loss: 3.0194642543792725
Validation loss: 2.035213887691498

Epoch: 6| Step: 8
Training loss: 1.5916924476623535
Validation loss: 2.0347713430722556

Epoch: 6| Step: 9
Training loss: 2.20317006111145
Validation loss: 2.047136604785919

Epoch: 6| Step: 10
Training loss: 2.1922669410705566
Validation loss: 2.048010269800822

Epoch: 6| Step: 11
Training loss: 1.8670600652694702
Validation loss: 2.0610451300938926

Epoch: 6| Step: 12
Training loss: 1.8127816915512085
Validation loss: 2.057193338871002

Epoch: 6| Step: 13
Training loss: 1.792319655418396
Validation loss: 2.070495069026947

Epoch: 129| Step: 0
Training loss: 1.8855736255645752
Validation loss: 2.061531941095988

Epoch: 6| Step: 1
Training loss: 2.0693864822387695
Validation loss: 2.0681666334470115

Epoch: 6| Step: 2
Training loss: 1.7059589624404907
Validation loss: 2.064866224924723

Epoch: 6| Step: 3
Training loss: 2.084489345550537
Validation loss: 2.067826271057129

Epoch: 6| Step: 4
Training loss: 2.1657772064208984
Validation loss: 2.0624072154363

Epoch: 6| Step: 5
Training loss: 2.0820164680480957
Validation loss: 2.048226257165273

Epoch: 6| Step: 6
Training loss: 1.7356207370758057
Validation loss: 2.0474944512049356

Epoch: 6| Step: 7
Training loss: 2.3319602012634277
Validation loss: 2.039280374844869

Epoch: 6| Step: 8
Training loss: 2.5155506134033203
Validation loss: 2.0401513973871865

Epoch: 6| Step: 9
Training loss: 1.8741565942764282
Validation loss: 2.0402106642723083

Epoch: 6| Step: 10
Training loss: 1.8711066246032715
Validation loss: 2.046297013759613

Epoch: 6| Step: 11
Training loss: 1.8731694221496582
Validation loss: 2.0421441793441772

Epoch: 6| Step: 12
Training loss: 2.2920780181884766
Validation loss: 2.037337839603424

Epoch: 6| Step: 13
Training loss: 1.9557183980941772
Validation loss: 2.038409491380056

Epoch: 130| Step: 0
Training loss: 1.703838586807251
Validation loss: 2.0309972763061523

Epoch: 6| Step: 1
Training loss: 1.6909077167510986
Validation loss: 2.0393455425898233

Epoch: 6| Step: 2
Training loss: 2.1996378898620605
Validation loss: 2.0468356211980185

Epoch: 6| Step: 3
Training loss: 2.4159865379333496
Validation loss: 2.0505559841791787

Epoch: 6| Step: 4
Training loss: 2.055964708328247
Validation loss: 2.0590243339538574

Epoch: 6| Step: 5
Training loss: 2.2848691940307617
Validation loss: 2.069187502066294

Epoch: 6| Step: 6
Training loss: 1.5812005996704102
Validation loss: 2.0657458504041037

Epoch: 6| Step: 7
Training loss: 2.1503639221191406
Validation loss: 2.072774112224579

Epoch: 6| Step: 8
Training loss: 1.7604212760925293
Validation loss: 2.073346654574076

Epoch: 6| Step: 9
Training loss: 1.645246148109436
Validation loss: 2.066179354985555

Epoch: 6| Step: 10
Training loss: 2.0753583908081055
Validation loss: 2.062036097049713

Epoch: 6| Step: 11
Training loss: 2.7319719791412354
Validation loss: 2.0502403577168784

Epoch: 6| Step: 12
Training loss: 1.9093669652938843
Validation loss: 2.044650693734487

Epoch: 6| Step: 13
Training loss: 2.408487319946289
Validation loss: 2.0367772380510965

Epoch: 131| Step: 0
Training loss: 1.8577461242675781
Validation loss: 2.031414488951365

Epoch: 6| Step: 1
Training loss: 1.7394863367080688
Validation loss: 2.0362218221028647

Epoch: 6| Step: 2
Training loss: 2.663322925567627
Validation loss: 2.0327059229214988

Epoch: 6| Step: 3
Training loss: 2.450098991394043
Validation loss: 2.045365293820699

Epoch: 6| Step: 4
Training loss: 1.8542836904525757
Validation loss: 2.041990955670675

Epoch: 6| Step: 5
Training loss: 1.531266689300537
Validation loss: 2.043780585130056

Epoch: 6| Step: 6
Training loss: 2.166344404220581
Validation loss: 2.0455778439839682

Epoch: 6| Step: 7
Training loss: 2.2562241554260254
Validation loss: 2.0430580178896585

Epoch: 6| Step: 8
Training loss: 1.8923829793930054
Validation loss: 2.0437172849973044

Epoch: 6| Step: 9
Training loss: 2.4428186416625977
Validation loss: 2.037549376487732

Epoch: 6| Step: 10
Training loss: 2.221696376800537
Validation loss: 2.031355619430542

Epoch: 6| Step: 11
Training loss: 1.9577637910842896
Validation loss: 2.034080525239309

Epoch: 6| Step: 12
Training loss: 1.646721601486206
Validation loss: 2.039791762828827

Epoch: 6| Step: 13
Training loss: 1.8570353984832764
Validation loss: 2.0469871958096824

Epoch: 132| Step: 0
Training loss: 1.875841498374939
Validation loss: 2.0550647974014282

Epoch: 6| Step: 1
Training loss: 1.996293544769287
Validation loss: 2.06977242231369

Epoch: 6| Step: 2
Training loss: 1.8319365978240967
Validation loss: 2.061036845048269

Epoch: 6| Step: 3
Training loss: 1.6718239784240723
Validation loss: 2.0847319960594177

Epoch: 6| Step: 4
Training loss: 1.725763201713562
Validation loss: 2.070950746536255

Epoch: 6| Step: 5
Training loss: 2.6788854598999023
Validation loss: 2.0639559427897134

Epoch: 6| Step: 6
Training loss: 2.177086591720581
Validation loss: 2.075839380423228

Epoch: 6| Step: 7
Training loss: 2.431919574737549
Validation loss: 2.0571052630742392

Epoch: 6| Step: 8
Training loss: 2.3103060722351074
Validation loss: 2.0642082691192627

Epoch: 6| Step: 9
Training loss: 2.3564019203186035
Validation loss: 2.0497856736183167

Epoch: 6| Step: 10
Training loss: 1.4833014011383057
Validation loss: 2.0398657520612082

Epoch: 6| Step: 11
Training loss: 2.097275733947754
Validation loss: 2.0287290811538696

Epoch: 6| Step: 12
Training loss: 2.3722686767578125
Validation loss: 2.030548334121704

Epoch: 6| Step: 13
Training loss: 1.655219554901123
Validation loss: 2.0288976232210794

Epoch: 133| Step: 0
Training loss: 2.324016809463501
Validation loss: 2.031801700592041

Epoch: 6| Step: 1
Training loss: 2.749351978302002
Validation loss: 2.028014878431956

Epoch: 6| Step: 2
Training loss: 2.2291030883789062
Validation loss: 2.028715988000234

Epoch: 6| Step: 3
Training loss: 1.757507562637329
Validation loss: 2.026712934176127

Epoch: 6| Step: 4
Training loss: 2.3461005687713623
Validation loss: 2.026625156402588

Epoch: 6| Step: 5
Training loss: 1.8429607152938843
Validation loss: 2.023380478223165

Epoch: 6| Step: 6
Training loss: 1.8650227785110474
Validation loss: 2.0148155291875205

Epoch: 6| Step: 7
Training loss: 1.8338309526443481
Validation loss: 2.0277350346247354

Epoch: 6| Step: 8
Training loss: 2.346994638442993
Validation loss: 2.035752534866333

Epoch: 6| Step: 9
Training loss: 2.0037548542022705
Validation loss: 2.0236181020736694

Epoch: 6| Step: 10
Training loss: 1.5674904584884644
Validation loss: 2.034992814064026

Epoch: 6| Step: 11
Training loss: 2.0370075702667236
Validation loss: 2.04065869251887

Epoch: 6| Step: 12
Training loss: 1.3826065063476562
Validation loss: 2.045330067475637

Epoch: 6| Step: 13
Training loss: 2.3635525703430176
Validation loss: 2.049665371576945

Epoch: 134| Step: 0
Training loss: 1.8260269165039062
Validation loss: 2.048534870147705

Epoch: 6| Step: 1
Training loss: 2.0822789669036865
Validation loss: 2.0507845083872476

Epoch: 6| Step: 2
Training loss: 1.8974499702453613
Validation loss: 2.0445215304692588

Epoch: 6| Step: 3
Training loss: 2.2673425674438477
Validation loss: 2.047658105691274

Epoch: 6| Step: 4
Training loss: 1.9830060005187988
Validation loss: 2.0347053607304892

Epoch: 6| Step: 5
Training loss: 1.4811115264892578
Validation loss: 2.042372544606527

Epoch: 6| Step: 6
Training loss: 2.288907289505005
Validation loss: 2.05324121316274

Epoch: 6| Step: 7
Training loss: 1.725170612335205
Validation loss: 2.0462732116381326

Epoch: 6| Step: 8
Training loss: 2.0700576305389404
Validation loss: 2.044442117214203

Epoch: 6| Step: 9
Training loss: 2.182328462600708
Validation loss: 2.0435503721237183

Epoch: 6| Step: 10
Training loss: 1.8079524040222168
Validation loss: 2.0339142282803855

Epoch: 6| Step: 11
Training loss: 2.303346633911133
Validation loss: 2.0457618832588196

Epoch: 6| Step: 12
Training loss: 1.7910759449005127
Validation loss: 2.0410022139549255

Epoch: 6| Step: 13
Training loss: 2.538649082183838
Validation loss: 2.035279929637909

Epoch: 135| Step: 0
Training loss: 1.8049198389053345
Validation loss: 2.0428838531176248

Epoch: 6| Step: 1
Training loss: 1.9327893257141113
Validation loss: 2.0383050441741943

Epoch: 6| Step: 2
Training loss: 1.4771382808685303
Validation loss: 2.0524176359176636

Epoch: 6| Step: 3
Training loss: 1.8808438777923584
Validation loss: 2.046426296234131

Epoch: 6| Step: 4
Training loss: 2.12135648727417
Validation loss: 2.0485288302103677

Epoch: 6| Step: 5
Training loss: 2.4447038173675537
Validation loss: 2.050549944241842

Epoch: 6| Step: 6
Training loss: 1.4731143712997437
Validation loss: 2.053998649120331

Epoch: 6| Step: 7
Training loss: 2.1069250106811523
Validation loss: 2.0552796920140586

Epoch: 6| Step: 8
Training loss: 2.2049851417541504
Validation loss: 2.0570359428723655

Epoch: 6| Step: 9
Training loss: 2.7080514430999756
Validation loss: 2.0705017844835916

Epoch: 6| Step: 10
Training loss: 1.8673324584960938
Validation loss: 2.0659693479537964

Epoch: 6| Step: 11
Training loss: 2.6336774826049805
Validation loss: 2.0870203375816345

Epoch: 6| Step: 12
Training loss: 1.9450417757034302
Validation loss: 2.0822776754697165

Epoch: 6| Step: 13
Training loss: 1.6786680221557617
Validation loss: 2.0843666394551597

Epoch: 136| Step: 0
Training loss: 1.9741822481155396
Validation loss: 2.0872967640558877

Epoch: 6| Step: 1
Training loss: 2.4149298667907715
Validation loss: 2.096586843331655

Epoch: 6| Step: 2
Training loss: 1.2947824001312256
Validation loss: 2.0864025155703225

Epoch: 6| Step: 3
Training loss: 2.344870090484619
Validation loss: 2.09530500570933

Epoch: 6| Step: 4
Training loss: 2.5929508209228516
Validation loss: 2.0984721183776855

Epoch: 6| Step: 5
Training loss: 2.017040252685547
Validation loss: 2.091805895169576

Epoch: 6| Step: 6
Training loss: 1.9956579208374023
Validation loss: 2.081642429033915

Epoch: 6| Step: 7
Training loss: 2.075798749923706
Validation loss: 2.0945624510447183

Epoch: 6| Step: 8
Training loss: 1.5129139423370361
Validation loss: 2.0856205423672995

Epoch: 6| Step: 9
Training loss: 1.8181791305541992
Validation loss: 2.083260258038839

Epoch: 6| Step: 10
Training loss: 2.872525691986084
Validation loss: 2.066489597161611

Epoch: 6| Step: 11
Training loss: 1.3776962757110596
Validation loss: 2.070038855075836

Epoch: 6| Step: 12
Training loss: 1.7824537754058838
Validation loss: 2.0501151084899902

Epoch: 6| Step: 13
Training loss: 2.1789069175720215
Validation loss: 2.054982324441274

Epoch: 137| Step: 0
Training loss: 2.0363271236419678
Validation loss: 2.0515463948249817

Epoch: 6| Step: 1
Training loss: 2.005917549133301
Validation loss: 2.046600798765818

Epoch: 6| Step: 2
Training loss: 2.2077035903930664
Validation loss: 2.0570937593777976

Epoch: 6| Step: 3
Training loss: 2.347923755645752
Validation loss: 2.05173929532369

Epoch: 6| Step: 4
Training loss: 1.9940712451934814
Validation loss: 2.0500919024149575

Epoch: 6| Step: 5
Training loss: 2.205634832382202
Validation loss: 2.0490443110466003

Epoch: 6| Step: 6
Training loss: 1.5320301055908203
Validation loss: 2.0522820750872293

Epoch: 6| Step: 7
Training loss: 1.8325916528701782
Validation loss: 2.0624459385871887

Epoch: 6| Step: 8
Training loss: 2.166363477706909
Validation loss: 2.0575825373331704

Epoch: 6| Step: 9
Training loss: 2.103513240814209
Validation loss: 2.0635597904523215

Epoch: 6| Step: 10
Training loss: 2.3043227195739746
Validation loss: 2.0535527070363364

Epoch: 6| Step: 11
Training loss: 1.8833173513412476
Validation loss: 2.0572554667790732

Epoch: 6| Step: 12
Training loss: 1.8814231157302856
Validation loss: 2.0581164757410684

Epoch: 6| Step: 13
Training loss: 2.0016021728515625
Validation loss: 2.0640780528386435

Epoch: 138| Step: 0
Training loss: 1.7760303020477295
Validation loss: 2.0589396953582764

Epoch: 6| Step: 1
Training loss: 1.9432177543640137
Validation loss: 2.0675373474756875

Epoch: 6| Step: 2
Training loss: 1.6373412609100342
Validation loss: 2.0547045469284058

Epoch: 6| Step: 3
Training loss: 1.690908432006836
Validation loss: 2.0527047316233316

Epoch: 6| Step: 4
Training loss: 2.51059889793396
Validation loss: 2.053186575571696

Epoch: 6| Step: 5
Training loss: 2.2454440593719482
Validation loss: 2.047800044218699

Epoch: 6| Step: 6
Training loss: 2.2703635692596436
Validation loss: 2.051126778125763

Epoch: 6| Step: 7
Training loss: 2.362283229827881
Validation loss: 2.0481781562169394

Epoch: 6| Step: 8
Training loss: 2.2639684677124023
Validation loss: 2.052117665608724

Epoch: 6| Step: 9
Training loss: 2.2690601348876953
Validation loss: 2.0488747159639993

Epoch: 6| Step: 10
Training loss: 2.150603771209717
Validation loss: 2.049336771170298

Epoch: 6| Step: 11
Training loss: 1.904672622680664
Validation loss: 2.046730101108551

Epoch: 6| Step: 12
Training loss: 1.537959337234497
Validation loss: 2.06018602848053

Epoch: 6| Step: 13
Training loss: 1.7159589529037476
Validation loss: 2.0628304481506348

Epoch: 139| Step: 0
Training loss: 1.6548936367034912
Validation loss: 2.0610395471254983

Epoch: 6| Step: 1
Training loss: 1.7123944759368896
Validation loss: 2.06257430712382

Epoch: 6| Step: 2
Training loss: 2.006742477416992
Validation loss: 2.0625889698664346

Epoch: 6| Step: 3
Training loss: 2.390568494796753
Validation loss: 2.064096530278524

Epoch: 6| Step: 4
Training loss: 1.6211928129196167
Validation loss: 2.0555317997932434

Epoch: 6| Step: 5
Training loss: 1.8536251783370972
Validation loss: 2.0590906540552774

Epoch: 6| Step: 6
Training loss: 2.080575942993164
Validation loss: 2.0582287510236106

Epoch: 6| Step: 7
Training loss: 1.8096647262573242
Validation loss: 2.056724965572357

Epoch: 6| Step: 8
Training loss: 2.391733407974243
Validation loss: 2.0552874406178794

Epoch: 6| Step: 9
Training loss: 2.0036911964416504
Validation loss: 2.0459328095118203

Epoch: 6| Step: 10
Training loss: 2.3089327812194824
Validation loss: 2.046500305334727

Epoch: 6| Step: 11
Training loss: 1.6504656076431274
Validation loss: 2.0583778023719788

Epoch: 6| Step: 12
Training loss: 2.110731840133667
Validation loss: 2.056174417336782

Epoch: 6| Step: 13
Training loss: 2.716306209564209
Validation loss: 2.0550629695256553

Epoch: 140| Step: 0
Training loss: 2.2371764183044434
Validation loss: 2.0670076608657837

Epoch: 6| Step: 1
Training loss: 1.754906415939331
Validation loss: 2.056414802869161

Epoch: 6| Step: 2
Training loss: 1.815382957458496
Validation loss: 2.0696834325790405

Epoch: 6| Step: 3
Training loss: 2.2328457832336426
Validation loss: 2.075366973876953

Epoch: 6| Step: 4
Training loss: 2.2997801303863525
Validation loss: 2.081539730230967

Epoch: 6| Step: 5
Training loss: 1.9677708148956299
Validation loss: 2.0875468651453652

Epoch: 6| Step: 6
Training loss: 1.8733606338500977
Validation loss: 2.0865447322527566

Epoch: 6| Step: 7
Training loss: 2.0290842056274414
Validation loss: 2.0845059951146445

Epoch: 6| Step: 8
Training loss: 1.7909482717514038
Validation loss: 2.0874787171681723

Epoch: 6| Step: 9
Training loss: 2.847287178039551
Validation loss: 2.059834440549215

Epoch: 6| Step: 10
Training loss: 1.7493644952774048
Validation loss: 2.055915911992391

Epoch: 6| Step: 11
Training loss: 2.0594065189361572
Validation loss: 2.0567090113957724

Epoch: 6| Step: 12
Training loss: 1.9307444095611572
Validation loss: 2.067383050918579

Epoch: 6| Step: 13
Training loss: 1.64524507522583
Validation loss: 2.0457069476445517

Epoch: 141| Step: 0
Training loss: 2.3026909828186035
Validation loss: 2.0426743229230246

Epoch: 6| Step: 1
Training loss: 1.755985975265503
Validation loss: 2.0550307432810464

Epoch: 6| Step: 2
Training loss: 1.9386563301086426
Validation loss: 2.058569351832072

Epoch: 6| Step: 3
Training loss: 2.053053855895996
Validation loss: 2.0472575426101685

Epoch: 6| Step: 4
Training loss: 1.927013874053955
Validation loss: 2.055346687634786

Epoch: 6| Step: 5
Training loss: 1.785581111907959
Validation loss: 2.068570931752523

Epoch: 6| Step: 6
Training loss: 1.764481544494629
Validation loss: 2.058310588200887

Epoch: 6| Step: 7
Training loss: 1.8006649017333984
Validation loss: 2.065073311328888

Epoch: 6| Step: 8
Training loss: 2.515942335128784
Validation loss: 2.069534262021383

Epoch: 6| Step: 9
Training loss: 2.34138560295105
Validation loss: 2.0736457109451294

Epoch: 6| Step: 10
Training loss: 1.4148180484771729
Validation loss: 2.07298610607783

Epoch: 6| Step: 11
Training loss: 1.919886589050293
Validation loss: 2.0813523332277932

Epoch: 6| Step: 12
Training loss: 2.6284289360046387
Validation loss: 2.08292692899704

Epoch: 6| Step: 13
Training loss: 1.9686734676361084
Validation loss: 2.075908104578654

Epoch: 142| Step: 0
Training loss: 1.4188673496246338
Validation loss: 2.07362163066864

Epoch: 6| Step: 1
Training loss: 1.8334529399871826
Validation loss: 2.0781563917795816

Epoch: 6| Step: 2
Training loss: 2.1067185401916504
Validation loss: 2.0808377067248025

Epoch: 6| Step: 3
Training loss: 2.0531649589538574
Validation loss: 2.0698058207829795

Epoch: 6| Step: 4
Training loss: 1.9073783159255981
Validation loss: 2.0642587741216025

Epoch: 6| Step: 5
Training loss: 1.5933880805969238
Validation loss: 2.057506044705709

Epoch: 6| Step: 6
Training loss: 2.014732837677002
Validation loss: 2.0506914456685386

Epoch: 6| Step: 7
Training loss: 1.9922173023223877
Validation loss: 2.0470966498057046

Epoch: 6| Step: 8
Training loss: 1.9403806924819946
Validation loss: 2.0450660387674966

Epoch: 6| Step: 9
Training loss: 2.4233880043029785
Validation loss: 2.036298910776774

Epoch: 6| Step: 10
Training loss: 2.0529112815856934
Validation loss: 2.039907912413279

Epoch: 6| Step: 11
Training loss: 1.7096538543701172
Validation loss: 2.0580138762791953

Epoch: 6| Step: 12
Training loss: 2.5196332931518555
Validation loss: 2.051129241784414

Epoch: 6| Step: 13
Training loss: 2.5273776054382324
Validation loss: 2.045868436495463

Epoch: 143| Step: 0
Training loss: 2.0341849327087402
Validation loss: 2.0626843571662903

Epoch: 6| Step: 1
Training loss: 1.7941668033599854
Validation loss: 2.0661252538363137

Epoch: 6| Step: 2
Training loss: 1.9243035316467285
Validation loss: 2.0662338534990945

Epoch: 6| Step: 3
Training loss: 2.03357195854187
Validation loss: 2.078566610813141

Epoch: 6| Step: 4
Training loss: 2.3227970600128174
Validation loss: 2.0762805143992105

Epoch: 6| Step: 5
Training loss: 1.5563377141952515
Validation loss: 2.0724246899286904

Epoch: 6| Step: 6
Training loss: 1.666702389717102
Validation loss: 2.0580360492070517

Epoch: 6| Step: 7
Training loss: 2.4213907718658447
Validation loss: 2.058776040871938

Epoch: 6| Step: 8
Training loss: 2.1850662231445312
Validation loss: 2.0494091510772705

Epoch: 6| Step: 9
Training loss: 1.809386134147644
Validation loss: 2.0549668272336326

Epoch: 6| Step: 10
Training loss: 2.326500415802002
Validation loss: 2.0497453212738037

Epoch: 6| Step: 11
Training loss: 1.6423382759094238
Validation loss: 2.0630082488059998

Epoch: 6| Step: 12
Training loss: 2.173157215118408
Validation loss: 2.050874173641205

Epoch: 6| Step: 13
Training loss: 2.1667139530181885
Validation loss: 2.057385881741842

Epoch: 144| Step: 0
Training loss: 2.1797051429748535
Validation loss: 2.0473116437594094

Epoch: 6| Step: 1
Training loss: 2.493673801422119
Validation loss: 2.047689417997996

Epoch: 6| Step: 2
Training loss: 2.404038906097412
Validation loss: 2.059102714061737

Epoch: 6| Step: 3
Training loss: 1.7470496892929077
Validation loss: 2.056418776512146

Epoch: 6| Step: 4
Training loss: 1.8007986545562744
Validation loss: 2.060168425242106

Epoch: 6| Step: 5
Training loss: 2.050096035003662
Validation loss: 2.071276048819224

Epoch: 6| Step: 6
Training loss: 2.4553332328796387
Validation loss: 2.0756009817123413

Epoch: 6| Step: 7
Training loss: 2.202535390853882
Validation loss: 2.065293033917745

Epoch: 6| Step: 8
Training loss: 1.8268698453903198
Validation loss: 2.0593894521395364

Epoch: 6| Step: 9
Training loss: 1.9939665794372559
Validation loss: 2.0568358500798545

Epoch: 6| Step: 10
Training loss: 1.5594837665557861
Validation loss: 2.059364100297292

Epoch: 6| Step: 11
Training loss: 2.113555431365967
Validation loss: 2.0760464866956077

Epoch: 6| Step: 12
Training loss: 1.458796739578247
Validation loss: 2.0709448059399924

Epoch: 6| Step: 13
Training loss: 1.6475389003753662
Validation loss: 2.0558864871660867

Epoch: 145| Step: 0
Training loss: 3.0674681663513184
Validation loss: 2.0775640408198037

Epoch: 6| Step: 1
Training loss: 2.3153302669525146
Validation loss: 2.0603425105412803

Epoch: 6| Step: 2
Training loss: 1.613276481628418
Validation loss: 2.0733195145924888

Epoch: 6| Step: 3
Training loss: 2.0841715335845947
Validation loss: 2.058756093184153

Epoch: 6| Step: 4
Training loss: 2.0796408653259277
Validation loss: 2.075940807660421

Epoch: 6| Step: 5
Training loss: 1.6232649087905884
Validation loss: 2.0501386721928916

Epoch: 6| Step: 6
Training loss: 2.3884544372558594
Validation loss: 2.0652801791826882

Epoch: 6| Step: 7
Training loss: 1.532400131225586
Validation loss: 2.05882861216863

Epoch: 6| Step: 8
Training loss: 2.11456298828125
Validation loss: 2.055983026822408

Epoch: 6| Step: 9
Training loss: 1.8896023035049438
Validation loss: 2.051895538965861

Epoch: 6| Step: 10
Training loss: 1.8534702062606812
Validation loss: 2.0650665561358132

Epoch: 6| Step: 11
Training loss: 1.7260262966156006
Validation loss: 2.0636317133903503

Epoch: 6| Step: 12
Training loss: 1.8586198091506958
Validation loss: 2.054088056087494

Epoch: 6| Step: 13
Training loss: 1.829122543334961
Validation loss: 2.056959311167399

Epoch: 146| Step: 0
Training loss: 2.1034746170043945
Validation loss: 2.063802421092987

Epoch: 6| Step: 1
Training loss: 1.832520842552185
Validation loss: 2.0701621373494468

Epoch: 6| Step: 2
Training loss: 1.7203835248947144
Validation loss: 2.0719337264696756

Epoch: 6| Step: 3
Training loss: 2.372528553009033
Validation loss: 2.069686849912008

Epoch: 6| Step: 4
Training loss: 2.2076590061187744
Validation loss: 2.0617442528406777

Epoch: 6| Step: 5
Training loss: 1.7320839166641235
Validation loss: 2.075167636076609

Epoch: 6| Step: 6
Training loss: 1.695674180984497
Validation loss: 2.0586655735969543

Epoch: 6| Step: 7
Training loss: 1.8652095794677734
Validation loss: 2.0746026039123535

Epoch: 6| Step: 8
Training loss: 2.1006717681884766
Validation loss: 2.0548749367396035

Epoch: 6| Step: 9
Training loss: 1.4612300395965576
Validation loss: 2.058148761590322

Epoch: 6| Step: 10
Training loss: 2.132126808166504
Validation loss: 2.058186193307241

Epoch: 6| Step: 11
Training loss: 2.332095146179199
Validation loss: 2.0630857149759927

Epoch: 6| Step: 12
Training loss: 2.3526766300201416
Validation loss: 2.051297664642334

Epoch: 6| Step: 13
Training loss: 2.0733797550201416
Validation loss: 2.060779392719269

Epoch: 147| Step: 0
Training loss: 2.3941545486450195
Validation loss: 2.0706401666005454

Epoch: 6| Step: 1
Training loss: 2.217466115951538
Validation loss: 2.0554130474726358

Epoch: 6| Step: 2
Training loss: 1.8438693284988403
Validation loss: 2.0533920526504517

Epoch: 6| Step: 3
Training loss: 1.625548005104065
Validation loss: 2.063227415084839

Epoch: 6| Step: 4
Training loss: 2.834695339202881
Validation loss: 2.0622597138086953

Epoch: 6| Step: 5
Training loss: 1.7486211061477661
Validation loss: 2.0681544740994773

Epoch: 6| Step: 6
Training loss: 1.7590608596801758
Validation loss: 2.063934008280436

Epoch: 6| Step: 7
Training loss: 2.4977965354919434
Validation loss: 2.0630059838294983

Epoch: 6| Step: 8
Training loss: 1.8221547603607178
Validation loss: 2.0550086895624795

Epoch: 6| Step: 9
Training loss: 1.9727462530136108
Validation loss: 2.074037571748098

Epoch: 6| Step: 10
Training loss: 1.5720360279083252
Validation loss: 2.075880448023478

Epoch: 6| Step: 11
Training loss: 1.92727792263031
Validation loss: 2.0717920660972595

Epoch: 6| Step: 12
Training loss: 1.9771074056625366
Validation loss: 2.091862360636393

Epoch: 6| Step: 13
Training loss: 1.6290526390075684
Validation loss: 2.077801843484243

Epoch: 148| Step: 0
Training loss: 1.375878095626831
Validation loss: 2.0798755288124084

Epoch: 6| Step: 1
Training loss: 2.0055065155029297
Validation loss: 2.0848910808563232

Epoch: 6| Step: 2
Training loss: 1.9587929248809814
Validation loss: 2.0603997111320496

Epoch: 6| Step: 3
Training loss: 2.318526268005371
Validation loss: 2.082326094309489

Epoch: 6| Step: 4
Training loss: 2.418549060821533
Validation loss: 2.0861098368962607

Epoch: 6| Step: 5
Training loss: 2.261061668395996
Validation loss: 2.079898178577423

Epoch: 6| Step: 6
Training loss: 1.6203060150146484
Validation loss: 2.059923013051351

Epoch: 6| Step: 7
Training loss: 2.3688466548919678
Validation loss: 2.058495064576467

Epoch: 6| Step: 8
Training loss: 2.1023871898651123
Validation loss: 2.052078982194265

Epoch: 6| Step: 9
Training loss: 2.3767786026000977
Validation loss: 2.0482105215390525

Epoch: 6| Step: 10
Training loss: 2.0476953983306885
Validation loss: 2.0549141565958657

Epoch: 6| Step: 11
Training loss: 1.5805420875549316
Validation loss: 2.0493874748547873

Epoch: 6| Step: 12
Training loss: 1.4518043994903564
Validation loss: 2.0598037242889404

Epoch: 6| Step: 13
Training loss: 2.367403984069824
Validation loss: 2.0729003151257834

Epoch: 149| Step: 0
Training loss: 2.009305715560913
Validation loss: 2.076203385988871

Epoch: 6| Step: 1
Training loss: 1.5085258483886719
Validation loss: 2.069302956263224

Epoch: 6| Step: 2
Training loss: 2.4419589042663574
Validation loss: 2.090960462888082

Epoch: 6| Step: 3
Training loss: 1.9790496826171875
Validation loss: 2.103952725728353

Epoch: 6| Step: 4
Training loss: 1.9708781242370605
Validation loss: 2.111839512983958

Epoch: 6| Step: 5
Training loss: 1.8119181394577026
Validation loss: 2.0975685914357505

Epoch: 6| Step: 6
Training loss: 1.8471519947052002
Validation loss: 2.1101770798365274

Epoch: 6| Step: 7
Training loss: 1.945088505744934
Validation loss: 2.099376142024994

Epoch: 6| Step: 8
Training loss: 1.9645729064941406
Validation loss: 2.0911171038945517

Epoch: 6| Step: 9
Training loss: 2.4473023414611816
Validation loss: 2.065000037352244

Epoch: 6| Step: 10
Training loss: 1.9884648323059082
Validation loss: 2.062444031238556

Epoch: 6| Step: 11
Training loss: 2.0455222129821777
Validation loss: 2.0453484654426575

Epoch: 6| Step: 12
Training loss: 2.367373466491699
Validation loss: 2.0541130900382996

Epoch: 6| Step: 13
Training loss: 1.763150691986084
Validation loss: 2.0484684705734253

Epoch: 150| Step: 0
Training loss: 2.2257161140441895
Validation loss: 2.0465515851974487

Epoch: 6| Step: 1
Training loss: 2.4889228343963623
Validation loss: 2.037281890710195

Epoch: 6| Step: 2
Training loss: 2.060133934020996
Validation loss: 2.040611962477366

Epoch: 6| Step: 3
Training loss: 2.36364483833313
Validation loss: 2.0534980297088623

Epoch: 6| Step: 4
Training loss: 1.7205904722213745
Validation loss: 2.0576937198638916

Epoch: 6| Step: 5
Training loss: 1.6980453729629517
Validation loss: 2.054167151451111

Epoch: 6| Step: 6
Training loss: 1.2492412328720093
Validation loss: 2.0564932028452554

Epoch: 6| Step: 7
Training loss: 1.987633466720581
Validation loss: 2.057494878768921

Epoch: 6| Step: 8
Training loss: 1.9952501058578491
Validation loss: 2.0763670404752097

Epoch: 6| Step: 9
Training loss: 1.9138073921203613
Validation loss: 2.0934146642684937

Epoch: 6| Step: 10
Training loss: 2.0709304809570312
Validation loss: 2.101496239503225

Epoch: 6| Step: 11
Training loss: 2.4280343055725098
Validation loss: 2.108507752418518

Epoch: 6| Step: 12
Training loss: 2.210969924926758
Validation loss: 2.0840497414271035

Epoch: 6| Step: 13
Training loss: 1.4767006635665894
Validation loss: 2.082153856754303

Epoch: 151| Step: 0
Training loss: 1.8746939897537231
Validation loss: 2.08248633146286

Epoch: 6| Step: 1
Training loss: 2.166167736053467
Validation loss: 2.07480925321579

Epoch: 6| Step: 2
Training loss: 1.8851419687271118
Validation loss: 2.075171689192454

Epoch: 6| Step: 3
Training loss: 2.090956211090088
Validation loss: 2.082038660844167

Epoch: 6| Step: 4
Training loss: 1.6341307163238525
Validation loss: 2.080248534679413

Epoch: 6| Step: 5
Training loss: 2.0571088790893555
Validation loss: 2.087877790133158

Epoch: 6| Step: 6
Training loss: 1.7097718715667725
Validation loss: 2.0732386112213135

Epoch: 6| Step: 7
Training loss: 2.6031084060668945
Validation loss: 2.0746915539105735

Epoch: 6| Step: 8
Training loss: 1.593685507774353
Validation loss: 2.0753278732299805

Epoch: 6| Step: 9
Training loss: 2.1604933738708496
Validation loss: 2.0879979530970254

Epoch: 6| Step: 10
Training loss: 1.7228403091430664
Validation loss: 2.084748109181722

Epoch: 6| Step: 11
Training loss: 1.603131651878357
Validation loss: 2.092150350411733

Epoch: 6| Step: 12
Training loss: 2.2272791862487793
Validation loss: 2.084227363268534

Epoch: 6| Step: 13
Training loss: 2.269146680831909
Validation loss: 2.0853418707847595

Epoch: 152| Step: 0
Training loss: 2.0998077392578125
Validation loss: 2.073196311791738

Epoch: 6| Step: 1
Training loss: 1.9136714935302734
Validation loss: 2.0844825903574624

Epoch: 6| Step: 2
Training loss: 2.0297324657440186
Validation loss: 2.0693772633870444

Epoch: 6| Step: 3
Training loss: 1.9397081136703491
Validation loss: 2.075887223084768

Epoch: 6| Step: 4
Training loss: 1.8121318817138672
Validation loss: 2.067342440287272

Epoch: 6| Step: 5
Training loss: 1.8989404439926147
Validation loss: 2.071087102095286

Epoch: 6| Step: 6
Training loss: 1.794691801071167
Validation loss: 2.0610117316246033

Epoch: 6| Step: 7
Training loss: 2.109855890274048
Validation loss: 2.0611544450124106

Epoch: 6| Step: 8
Training loss: 2.11733341217041
Validation loss: 2.056450525919596

Epoch: 6| Step: 9
Training loss: 2.647613525390625
Validation loss: 2.063522140185038

Epoch: 6| Step: 10
Training loss: 1.6681498289108276
Validation loss: 2.0698106487592063

Epoch: 6| Step: 11
Training loss: 1.8359646797180176
Validation loss: 2.0611809293429055

Epoch: 6| Step: 12
Training loss: 1.7664504051208496
Validation loss: 2.0535882115364075

Epoch: 6| Step: 13
Training loss: 2.0665197372436523
Validation loss: 2.0758103330930076

Epoch: 153| Step: 0
Training loss: 1.792797327041626
Validation loss: 2.083646913369497

Epoch: 6| Step: 1
Training loss: 1.7318077087402344
Validation loss: 2.0882627964019775

Epoch: 6| Step: 2
Training loss: 1.9790300130844116
Validation loss: 2.095923145612081

Epoch: 6| Step: 3
Training loss: 1.2297298908233643
Validation loss: 2.1047012209892273

Epoch: 6| Step: 4
Training loss: 1.9180474281311035
Validation loss: 2.0864309469858804

Epoch: 6| Step: 5
Training loss: 2.20827579498291
Validation loss: 2.0866852005322776

Epoch: 6| Step: 6
Training loss: 2.166635036468506
Validation loss: 2.086738626162211

Epoch: 6| Step: 7
Training loss: 1.480353593826294
Validation loss: 2.0905545552571616

Epoch: 6| Step: 8
Training loss: 2.2536683082580566
Validation loss: 2.0788627465566

Epoch: 6| Step: 9
Training loss: 2.3269402980804443
Validation loss: 2.088114321231842

Epoch: 6| Step: 10
Training loss: 2.56840443611145
Validation loss: 2.0817134976387024

Epoch: 6| Step: 11
Training loss: 1.6552950143814087
Validation loss: 2.091114064057668

Epoch: 6| Step: 12
Training loss: 1.6937412023544312
Validation loss: 2.106324772040049

Epoch: 6| Step: 13
Training loss: 2.3317346572875977
Validation loss: 2.1019696394602456

Epoch: 154| Step: 0
Training loss: 2.576430320739746
Validation loss: 2.1043579975763955

Epoch: 6| Step: 1
Training loss: 1.8215105533599854
Validation loss: 2.0867181619008384

Epoch: 6| Step: 2
Training loss: 2.062788724899292
Validation loss: 2.086979250113169

Epoch: 6| Step: 3
Training loss: 1.658127784729004
Validation loss: 2.068171441555023

Epoch: 6| Step: 4
Training loss: 1.7647793292999268
Validation loss: 2.0768953760464988

Epoch: 6| Step: 5
Training loss: 2.3781521320343018
Validation loss: 2.065418541431427

Epoch: 6| Step: 6
Training loss: 2.2048802375793457
Validation loss: 2.058633248011271

Epoch: 6| Step: 7
Training loss: 1.6064643859863281
Validation loss: 2.0661981105804443

Epoch: 6| Step: 8
Training loss: 1.3009066581726074
Validation loss: 2.0795345306396484

Epoch: 6| Step: 9
Training loss: 1.2601594924926758
Validation loss: 2.0808056195576987

Epoch: 6| Step: 10
Training loss: 2.262312889099121
Validation loss: 2.073527912298838

Epoch: 6| Step: 11
Training loss: 2.2862436771392822
Validation loss: 2.0778682231903076

Epoch: 6| Step: 12
Training loss: 2.1863622665405273
Validation loss: 2.0918095310529075

Epoch: 6| Step: 13
Training loss: 2.146679401397705
Validation loss: 2.0860001047452292

Epoch: 155| Step: 0
Training loss: 2.88443660736084
Validation loss: 2.0928054054578147

Epoch: 6| Step: 1
Training loss: 2.202409267425537
Validation loss: 2.110002338886261

Epoch: 6| Step: 2
Training loss: 2.108339309692383
Validation loss: 2.095264514287313

Epoch: 6| Step: 3
Training loss: 2.030811309814453
Validation loss: 2.0901098450024924

Epoch: 6| Step: 4
Training loss: 1.837109923362732
Validation loss: 2.1057836413383484

Epoch: 6| Step: 5
Training loss: 1.858502745628357
Validation loss: 2.092981994152069

Epoch: 6| Step: 6
Training loss: 1.8088115453720093
Validation loss: 2.087518850962321

Epoch: 6| Step: 7
Training loss: 1.1813410520553589
Validation loss: 2.0617923736572266

Epoch: 6| Step: 8
Training loss: 1.9079970121383667
Validation loss: 2.070703109105428

Epoch: 6| Step: 9
Training loss: 1.4844579696655273
Validation loss: 2.0578243732452393

Epoch: 6| Step: 10
Training loss: 2.3541414737701416
Validation loss: 2.07140851020813

Epoch: 6| Step: 11
Training loss: 1.7312328815460205
Validation loss: 2.069627503554026

Epoch: 6| Step: 12
Training loss: 1.7476431131362915
Validation loss: 2.0657074650128684

Epoch: 6| Step: 13
Training loss: 2.3285861015319824
Validation loss: 2.0730407436688743

Epoch: 156| Step: 0
Training loss: 1.8847310543060303
Validation loss: 2.06683741013209

Epoch: 6| Step: 1
Training loss: 1.974933385848999
Validation loss: 2.0695545276006064

Epoch: 6| Step: 2
Training loss: 1.6152429580688477
Validation loss: 2.0632694363594055

Epoch: 6| Step: 3
Training loss: 2.115669012069702
Validation loss: 2.082341194152832

Epoch: 6| Step: 4
Training loss: 2.3084640502929688
Validation loss: 2.077889879544576

Epoch: 6| Step: 5
Training loss: 2.1203317642211914
Validation loss: 2.0865809321403503

Epoch: 6| Step: 6
Training loss: 1.8368289470672607
Validation loss: 2.089633285999298

Epoch: 6| Step: 7
Training loss: 1.9031063318252563
Validation loss: 2.096684773763021

Epoch: 6| Step: 8
Training loss: 2.1344072818756104
Validation loss: 2.102126181125641

Epoch: 6| Step: 9
Training loss: 1.8797340393066406
Validation loss: 2.0761688947677612

Epoch: 6| Step: 10
Training loss: 2.22141170501709
Validation loss: 2.053549647331238

Epoch: 6| Step: 11
Training loss: 1.8643131256103516
Validation loss: 2.057091772556305

Epoch: 6| Step: 12
Training loss: 1.9513342380523682
Validation loss: 2.053620199362437

Epoch: 6| Step: 13
Training loss: 1.8230706453323364
Validation loss: 2.043984333674113

Epoch: 157| Step: 0
Training loss: 2.116481304168701
Validation loss: 2.0504514575004578

Epoch: 6| Step: 1
Training loss: 2.447634696960449
Validation loss: 2.056589663028717

Epoch: 6| Step: 2
Training loss: 2.187033176422119
Validation loss: 2.0556563337643943

Epoch: 6| Step: 3
Training loss: 1.3046014308929443
Validation loss: 2.0693241953849792

Epoch: 6| Step: 4
Training loss: 1.9978346824645996
Validation loss: 2.060480833053589

Epoch: 6| Step: 5
Training loss: 1.6243325471878052
Validation loss: 2.0694978634516397

Epoch: 6| Step: 6
Training loss: 1.5206493139266968
Validation loss: 2.0689921975135803

Epoch: 6| Step: 7
Training loss: 1.8603169918060303
Validation loss: 2.086082716782888

Epoch: 6| Step: 8
Training loss: 2.426954746246338
Validation loss: 2.097069263458252

Epoch: 6| Step: 9
Training loss: 2.1850037574768066
Validation loss: 2.0863682428995767

Epoch: 6| Step: 10
Training loss: 2.1643223762512207
Validation loss: 2.091593643029531

Epoch: 6| Step: 11
Training loss: 1.7345569133758545
Validation loss: 2.101887265841166

Epoch: 6| Step: 12
Training loss: 2.2955286502838135
Validation loss: 2.0858130057652793

Epoch: 6| Step: 13
Training loss: 2.015725612640381
Validation loss: 2.0858960151672363

Epoch: 158| Step: 0
Training loss: 2.207632541656494
Validation loss: 2.0720776120821633

Epoch: 6| Step: 1
Training loss: 2.688140392303467
Validation loss: 2.0749922394752502

Epoch: 6| Step: 2
Training loss: 1.3722063302993774
Validation loss: 2.075953165690104

Epoch: 6| Step: 3
Training loss: 1.3063275814056396
Validation loss: 2.0641465385754905

Epoch: 6| Step: 4
Training loss: 2.0700502395629883
Validation loss: 2.0526547034581504

Epoch: 6| Step: 5
Training loss: 1.9875590801239014
Validation loss: 2.061694165070852

Epoch: 6| Step: 6
Training loss: 2.0312492847442627
Validation loss: 2.0821436246236167

Epoch: 6| Step: 7
Training loss: 2.523838996887207
Validation loss: 2.0873553355534873

Epoch: 6| Step: 8
Training loss: 1.4956945180892944
Validation loss: 2.0871125857035318

Epoch: 6| Step: 9
Training loss: 1.9269936084747314
Validation loss: 2.094192941983541

Epoch: 6| Step: 10
Training loss: 2.1387252807617188
Validation loss: 2.1151745319366455

Epoch: 6| Step: 11
Training loss: 1.5119131803512573
Validation loss: 2.114593744277954

Epoch: 6| Step: 12
Training loss: 2.033342123031616
Validation loss: 2.11351877450943

Epoch: 6| Step: 13
Training loss: 1.7513549327850342
Validation loss: 2.1315632263819375

Epoch: 159| Step: 0
Training loss: 1.3723020553588867
Validation loss: 2.105615735054016

Epoch: 6| Step: 1
Training loss: 1.8839704990386963
Validation loss: 2.1073352495829263

Epoch: 6| Step: 2
Training loss: 2.8417420387268066
Validation loss: 2.0931279063224792

Epoch: 6| Step: 3
Training loss: 2.485199451446533
Validation loss: 2.0763872861862183

Epoch: 6| Step: 4
Training loss: 1.8331362009048462
Validation loss: 2.072831312815348

Epoch: 6| Step: 5
Training loss: 1.9733742475509644
Validation loss: 2.0549685756365457

Epoch: 6| Step: 6
Training loss: 2.158243179321289
Validation loss: 2.0620672702789307

Epoch: 6| Step: 7
Training loss: 1.7724637985229492
Validation loss: 2.073530932267507

Epoch: 6| Step: 8
Training loss: 2.0325279235839844
Validation loss: 2.0741668343544006

Epoch: 6| Step: 9
Training loss: 1.2043757438659668
Validation loss: 2.0788949926694236

Epoch: 6| Step: 10
Training loss: 1.9102725982666016
Validation loss: 2.067913989226023

Epoch: 6| Step: 11
Training loss: 2.1298093795776367
Validation loss: 2.0831657449404397

Epoch: 6| Step: 12
Training loss: 2.2108869552612305
Validation loss: 2.071033159891764

Epoch: 6| Step: 13
Training loss: 2.054025888442993
Validation loss: 2.082332670688629

Epoch: 160| Step: 0
Training loss: 1.7567708492279053
Validation loss: 2.0843428572018943

Epoch: 6| Step: 1
Training loss: 2.094536781311035
Validation loss: 2.0911734302838645

Epoch: 6| Step: 2
Training loss: 2.1673636436462402
Validation loss: 2.0851359566052756

Epoch: 6| Step: 3
Training loss: 2.6213393211364746
Validation loss: 2.101908107598623

Epoch: 6| Step: 4
Training loss: 1.8716962337493896
Validation loss: 2.1122793356577554

Epoch: 6| Step: 5
Training loss: 1.3185698986053467
Validation loss: 2.111062149206797

Epoch: 6| Step: 6
Training loss: 1.3730989694595337
Validation loss: 2.1205583810806274

Epoch: 6| Step: 7
Training loss: 1.6737005710601807
Validation loss: 2.116417487462362

Epoch: 6| Step: 8
Training loss: 2.2750322818756104
Validation loss: 2.1261316736539206

Epoch: 6| Step: 9
Training loss: 2.2394468784332275
Validation loss: 2.121351679166158

Epoch: 6| Step: 10
Training loss: 1.8545446395874023
Validation loss: 2.1064042250315347

Epoch: 6| Step: 11
Training loss: 2.154174327850342
Validation loss: 2.114596168200175

Epoch: 6| Step: 12
Training loss: 1.6976838111877441
Validation loss: 2.0990348060925803

Epoch: 6| Step: 13
Training loss: 2.2310025691986084
Validation loss: 2.0748560229937234

Epoch: 161| Step: 0
Training loss: 1.8629658222198486
Validation loss: 2.076200266679128

Epoch: 6| Step: 1
Training loss: 1.8669862747192383
Validation loss: 2.06157652537028

Epoch: 6| Step: 2
Training loss: 2.2726993560791016
Validation loss: 2.0570534467697144

Epoch: 6| Step: 3
Training loss: 1.6630902290344238
Validation loss: 2.0648647348086038

Epoch: 6| Step: 4
Training loss: 2.830056667327881
Validation loss: 2.0616267919540405

Epoch: 6| Step: 5
Training loss: 2.0088095664978027
Validation loss: 2.0627825061480203

Epoch: 6| Step: 6
Training loss: 1.9867759943008423
Validation loss: 2.074186642964681

Epoch: 6| Step: 7
Training loss: 1.9766852855682373
Validation loss: 2.0760396122932434

Epoch: 6| Step: 8
Training loss: 1.9695497751235962
Validation loss: 2.0664631128311157

Epoch: 6| Step: 9
Training loss: 1.8536460399627686
Validation loss: 2.084643006324768

Epoch: 6| Step: 10
Training loss: 1.8599668741226196
Validation loss: 2.070126930872599

Epoch: 6| Step: 11
Training loss: 1.8621885776519775
Validation loss: 2.0663065711657205

Epoch: 6| Step: 12
Training loss: 1.9028971195220947
Validation loss: 2.072391907374064

Epoch: 6| Step: 13
Training loss: 2.0440943241119385
Validation loss: 2.1079872250556946

Epoch: 162| Step: 0
Training loss: 1.692670464515686
Validation loss: 2.1011733611424765

Epoch: 6| Step: 1
Training loss: 1.4896882772445679
Validation loss: 2.122101902961731

Epoch: 6| Step: 2
Training loss: 2.298827648162842
Validation loss: 2.1165587504704795

Epoch: 6| Step: 3
Training loss: 2.5005693435668945
Validation loss: 2.129834254582723

Epoch: 6| Step: 4
Training loss: 2.0233120918273926
Validation loss: 2.1158032417297363

Epoch: 6| Step: 5
Training loss: 1.566730260848999
Validation loss: 2.099635104338328

Epoch: 6| Step: 6
Training loss: 2.147841691970825
Validation loss: 2.082153042157491

Epoch: 6| Step: 7
Training loss: 1.8776108026504517
Validation loss: 2.0869624416033425

Epoch: 6| Step: 8
Training loss: 2.0038962364196777
Validation loss: 2.0835121671358743

Epoch: 6| Step: 9
Training loss: 2.156135082244873
Validation loss: 2.07597553730011

Epoch: 6| Step: 10
Training loss: 1.9853742122650146
Validation loss: 2.0663893024126687

Epoch: 6| Step: 11
Training loss: 1.7356654405593872
Validation loss: 2.073428233464559

Epoch: 6| Step: 12
Training loss: 1.9952701330184937
Validation loss: 2.0774348378181458

Epoch: 6| Step: 13
Training loss: 2.1711926460266113
Validation loss: 2.075821657975515

Epoch: 163| Step: 0
Training loss: 2.118695020675659
Validation loss: 2.062726596991221

Epoch: 6| Step: 1
Training loss: 1.7819344997406006
Validation loss: 2.064001758893331

Epoch: 6| Step: 2
Training loss: 1.2950191497802734
Validation loss: 2.0665452678998313

Epoch: 6| Step: 3
Training loss: 2.051999568939209
Validation loss: 2.0553243358929953

Epoch: 6| Step: 4
Training loss: 2.6354198455810547
Validation loss: 2.072198212146759

Epoch: 6| Step: 5
Training loss: 2.2705578804016113
Validation loss: 2.1012677550315857

Epoch: 6| Step: 6
Training loss: 1.884615182876587
Validation loss: 2.0987659295399985

Epoch: 6| Step: 7
Training loss: 1.251659870147705
Validation loss: 2.108593304951986

Epoch: 6| Step: 8
Training loss: 2.001560688018799
Validation loss: 2.1022297938664756

Epoch: 6| Step: 9
Training loss: 1.9951775074005127
Validation loss: 2.1158023277918496

Epoch: 6| Step: 10
Training loss: 1.754835605621338
Validation loss: 2.114999532699585

Epoch: 6| Step: 11
Training loss: 2.2735633850097656
Validation loss: 2.132910370826721

Epoch: 6| Step: 12
Training loss: 2.0430502891540527
Validation loss: 2.105741878350576

Epoch: 6| Step: 13
Training loss: 1.843364953994751
Validation loss: 2.096171498298645

Epoch: 164| Step: 0
Training loss: 1.9907584190368652
Validation loss: 2.0964832305908203

Epoch: 6| Step: 1
Training loss: 1.7173799276351929
Validation loss: 2.0825418631235757

Epoch: 6| Step: 2
Training loss: 2.4266281127929688
Validation loss: 2.090026338895162

Epoch: 6| Step: 3
Training loss: 2.208117723464966
Validation loss: 2.0692291855812073

Epoch: 6| Step: 4
Training loss: 1.6241075992584229
Validation loss: 2.081071118513743

Epoch: 6| Step: 5
Training loss: 1.8921184539794922
Validation loss: 2.069197654724121

Epoch: 6| Step: 6
Training loss: 1.685662031173706
Validation loss: 2.082981288433075

Epoch: 6| Step: 7
Training loss: 1.6957546472549438
Validation loss: 2.0866269866625466

Epoch: 6| Step: 8
Training loss: 1.5925350189208984
Validation loss: 2.0733539859453836

Epoch: 6| Step: 9
Training loss: 2.3181567192077637
Validation loss: 2.086389203866323

Epoch: 6| Step: 10
Training loss: 2.7860801219940186
Validation loss: 2.0853158235549927

Epoch: 6| Step: 11
Training loss: 1.8187501430511475
Validation loss: 2.084426919619242

Epoch: 6| Step: 12
Training loss: 1.9852451086044312
Validation loss: 2.095771551132202

Epoch: 6| Step: 13
Training loss: 1.468317985534668
Validation loss: 2.1062230666478476

Epoch: 165| Step: 0
Training loss: 2.8314404487609863
Validation loss: 2.1106947660446167

Epoch: 6| Step: 1
Training loss: 1.7737298011779785
Validation loss: 2.115884701410929

Epoch: 6| Step: 2
Training loss: 2.051802396774292
Validation loss: 2.106976588567098

Epoch: 6| Step: 3
Training loss: 1.98720383644104
Validation loss: 2.1145333449045816

Epoch: 6| Step: 4
Training loss: 1.8137120008468628
Validation loss: 2.100657284259796

Epoch: 6| Step: 5
Training loss: 1.9674451351165771
Validation loss: 2.1126054922739663

Epoch: 6| Step: 6
Training loss: 2.1088485717773438
Validation loss: 2.1088404655456543

Epoch: 6| Step: 7
Training loss: 1.731923222541809
Validation loss: 2.1000842253367105

Epoch: 6| Step: 8
Training loss: 1.819347620010376
Validation loss: 2.0846594174702964

Epoch: 6| Step: 9
Training loss: 2.2569172382354736
Validation loss: 2.0939249396324158

Epoch: 6| Step: 10
Training loss: 1.313650369644165
Validation loss: 2.0902662674585977

Epoch: 6| Step: 11
Training loss: 1.5968785285949707
Validation loss: 2.089556614557902

Epoch: 6| Step: 12
Training loss: 2.5151467323303223
Validation loss: 2.093883534272512

Epoch: 6| Step: 13
Training loss: 1.397303581237793
Validation loss: 2.0902696450551352

Epoch: 166| Step: 0
Training loss: 2.022077798843384
Validation loss: 2.0741870601971946

Epoch: 6| Step: 1
Training loss: 1.872602939605713
Validation loss: 2.084238668282827

Epoch: 6| Step: 2
Training loss: 1.9801286458969116
Validation loss: 2.093128184477488

Epoch: 6| Step: 3
Training loss: 2.1294736862182617
Validation loss: 2.0691400369008384

Epoch: 6| Step: 4
Training loss: 1.6407911777496338
Validation loss: 2.0939945181210837

Epoch: 6| Step: 5
Training loss: 1.876892328262329
Validation loss: 2.091107428073883

Epoch: 6| Step: 6
Training loss: 1.8040432929992676
Validation loss: 2.086312790711721

Epoch: 6| Step: 7
Training loss: 2.738248825073242
Validation loss: 2.089832087357839

Epoch: 6| Step: 8
Training loss: 1.6080197095870972
Validation loss: 2.0971227288246155

Epoch: 6| Step: 9
Training loss: 2.0670766830444336
Validation loss: 2.0799156030019126

Epoch: 6| Step: 10
Training loss: 1.7325429916381836
Validation loss: 2.0918527444203696

Epoch: 6| Step: 11
Training loss: 2.181539535522461
Validation loss: 2.089855412642161

Epoch: 6| Step: 12
Training loss: 1.6805974245071411
Validation loss: 2.0921533505121865

Epoch: 6| Step: 13
Training loss: 1.758236050605774
Validation loss: 2.0857408245404563

Epoch: 167| Step: 0
Training loss: 1.4932801723480225
Validation loss: 2.0873915553092957

Epoch: 6| Step: 1
Training loss: 2.1443393230438232
Validation loss: 2.089274783929189

Epoch: 6| Step: 2
Training loss: 2.086057662963867
Validation loss: 2.0836633443832397

Epoch: 6| Step: 3
Training loss: 2.1353495121002197
Validation loss: 2.0849443078041077

Epoch: 6| Step: 4
Training loss: 1.9480547904968262
Validation loss: 2.0967514514923096

Epoch: 6| Step: 5
Training loss: 2.203486919403076
Validation loss: 2.0942623217900596

Epoch: 6| Step: 6
Training loss: 1.414035439491272
Validation loss: 2.0808080633481345

Epoch: 6| Step: 7
Training loss: 2.002362012863159
Validation loss: 2.0892820954322815

Epoch: 6| Step: 8
Training loss: 2.0564794540405273
Validation loss: 2.0735912322998047

Epoch: 6| Step: 9
Training loss: 1.2953131198883057
Validation loss: 2.064975698788961

Epoch: 6| Step: 10
Training loss: 2.161257266998291
Validation loss: 2.0840579668680825

Epoch: 6| Step: 11
Training loss: 1.641693353652954
Validation loss: 2.079110264778137

Epoch: 6| Step: 12
Training loss: 2.558056354522705
Validation loss: 2.0896658102671304

Epoch: 6| Step: 13
Training loss: 1.7608816623687744
Validation loss: 2.088609039783478

Epoch: 168| Step: 0
Training loss: 1.68831205368042
Validation loss: 2.0846752921740213

Epoch: 6| Step: 1
Training loss: 1.7946361303329468
Validation loss: 2.100883682568868

Epoch: 6| Step: 2
Training loss: 2.3920998573303223
Validation loss: 2.1070706446965537

Epoch: 6| Step: 3
Training loss: 1.4249835014343262
Validation loss: 2.1163028279940286

Epoch: 6| Step: 4
Training loss: 1.7889585494995117
Validation loss: 2.1282691160837808

Epoch: 6| Step: 5
Training loss: 2.3817732334136963
Validation loss: 2.1243980725606284

Epoch: 6| Step: 6
Training loss: 2.0794382095336914
Validation loss: 2.1319358150164285

Epoch: 6| Step: 7
Training loss: 1.998212218284607
Validation loss: 2.1350143551826477

Epoch: 6| Step: 8
Training loss: 2.0207695960998535
Validation loss: 2.134505808353424

Epoch: 6| Step: 9
Training loss: 1.9870332479476929
Validation loss: 2.100008209546407

Epoch: 6| Step: 10
Training loss: 2.516084671020508
Validation loss: 2.0979241530100503

Epoch: 6| Step: 11
Training loss: 2.0315966606140137
Validation loss: 2.1003465255101523

Epoch: 6| Step: 12
Training loss: 1.4710140228271484
Validation loss: 2.085622549057007

Epoch: 6| Step: 13
Training loss: 1.7642724514007568
Validation loss: 2.0794608990351358

Epoch: 169| Step: 0
Training loss: 1.9341130256652832
Validation loss: 2.0814066529273987

Epoch: 6| Step: 1
Training loss: 2.144639015197754
Validation loss: 2.0821070273717246

Epoch: 6| Step: 2
Training loss: 1.4329872131347656
Validation loss: 2.085678974787394

Epoch: 6| Step: 3
Training loss: 2.0274040699005127
Validation loss: 2.088619609673818

Epoch: 6| Step: 4
Training loss: 1.724435806274414
Validation loss: 2.082110365231832

Epoch: 6| Step: 5
Training loss: 1.9037871360778809
Validation loss: 2.0909590323766074

Epoch: 6| Step: 6
Training loss: 1.7964861392974854
Validation loss: 2.0883434812227883

Epoch: 6| Step: 7
Training loss: 2.4105939865112305
Validation loss: 2.1036861141522727

Epoch: 6| Step: 8
Training loss: 2.1482081413269043
Validation loss: 2.087893803914388

Epoch: 6| Step: 9
Training loss: 1.9844326972961426
Validation loss: 2.0955063700675964

Epoch: 6| Step: 10
Training loss: 1.6299941539764404
Validation loss: 2.10973467429479

Epoch: 6| Step: 11
Training loss: 2.4006967544555664
Validation loss: 2.106875995794932

Epoch: 6| Step: 12
Training loss: 1.9670889377593994
Validation loss: 2.1314512491226196

Epoch: 6| Step: 13
Training loss: 1.8150770664215088
Validation loss: 2.1145232518514

Epoch: 170| Step: 0
Training loss: 2.371856212615967
Validation loss: 2.106135110060374

Epoch: 6| Step: 1
Training loss: 1.9846234321594238
Validation loss: 2.107099493344625

Epoch: 6| Step: 2
Training loss: 2.0184435844421387
Validation loss: 2.0873804291089377

Epoch: 6| Step: 3
Training loss: 1.7820746898651123
Validation loss: 2.096372961997986

Epoch: 6| Step: 4
Training loss: 2.104269504547119
Validation loss: 2.1007208029429116

Epoch: 6| Step: 5
Training loss: 1.4318811893463135
Validation loss: 2.0793758432070413

Epoch: 6| Step: 6
Training loss: 1.8032612800598145
Validation loss: 2.07667475938797

Epoch: 6| Step: 7
Training loss: 2.278109312057495
Validation loss: 2.088776787122091

Epoch: 6| Step: 8
Training loss: 2.2253003120422363
Validation loss: 2.094666580359141

Epoch: 6| Step: 9
Training loss: 1.8090704679489136
Validation loss: 2.0845747788747153

Epoch: 6| Step: 10
Training loss: 1.637782335281372
Validation loss: 2.080271005630493

Epoch: 6| Step: 11
Training loss: 1.8343123197555542
Validation loss: 2.08754692475001

Epoch: 6| Step: 12
Training loss: 1.9603230953216553
Validation loss: 2.0896071195602417

Epoch: 6| Step: 13
Training loss: 1.8490095138549805
Validation loss: 2.0986925760904946

Epoch: 171| Step: 0
Training loss: 1.8177157640457153
Validation loss: 2.09760049978892

Epoch: 6| Step: 1
Training loss: 2.004202365875244
Validation loss: 2.0896197160085044

Epoch: 6| Step: 2
Training loss: 1.370107650756836
Validation loss: 2.1010790864626565

Epoch: 6| Step: 3
Training loss: 2.0494630336761475
Validation loss: 2.102837304274241

Epoch: 6| Step: 4
Training loss: 1.6535981893539429
Validation loss: 2.099879205226898

Epoch: 6| Step: 5
Training loss: 2.747459888458252
Validation loss: 2.1181869904200235

Epoch: 6| Step: 6
Training loss: 2.2962474822998047
Validation loss: 2.1332643032073975

Epoch: 6| Step: 7
Training loss: 1.9752137660980225
Validation loss: 2.13203493754069

Epoch: 6| Step: 8
Training loss: 1.2625333070755005
Validation loss: 2.1358516613642373

Epoch: 6| Step: 9
Training loss: 1.4242842197418213
Validation loss: 2.137978712717692

Epoch: 6| Step: 10
Training loss: 1.7769193649291992
Validation loss: 2.1414440870285034

Epoch: 6| Step: 11
Training loss: 1.7829946279525757
Validation loss: 2.1375449895858765

Epoch: 6| Step: 12
Training loss: 2.2000961303710938
Validation loss: 2.1288244326909385

Epoch: 6| Step: 13
Training loss: 2.1669516563415527
Validation loss: 2.1159015695254006

Epoch: 172| Step: 0
Training loss: 2.3946633338928223
Validation loss: 2.1198877890904746

Epoch: 6| Step: 1
Training loss: 1.6480433940887451
Validation loss: 2.1060446302096048

Epoch: 6| Step: 2
Training loss: 1.5208460092544556
Validation loss: 2.1291012366612754

Epoch: 6| Step: 3
Training loss: 2.790426254272461
Validation loss: 2.139887591203054

Epoch: 6| Step: 4
Training loss: 2.0170538425445557
Validation loss: 2.1226760149002075

Epoch: 6| Step: 5
Training loss: 1.4647972583770752
Validation loss: 2.1333895921707153

Epoch: 6| Step: 6
Training loss: 1.635026216506958
Validation loss: 2.1101128856341043

Epoch: 6| Step: 7
Training loss: 1.9510029554367065
Validation loss: 2.1089222033818564

Epoch: 6| Step: 8
Training loss: 2.126340866088867
Validation loss: 2.101457873980204

Epoch: 6| Step: 9
Training loss: 1.9117774963378906
Validation loss: 2.1073439915974936

Epoch: 6| Step: 10
Training loss: 1.8629337549209595
Validation loss: 2.087274412314097

Epoch: 6| Step: 11
Training loss: 1.8068087100982666
Validation loss: 2.1066210667292276

Epoch: 6| Step: 12
Training loss: 1.47751784324646
Validation loss: 2.0866491993268332

Epoch: 6| Step: 13
Training loss: 2.1658124923706055
Validation loss: 2.0769229928652444

Epoch: 173| Step: 0
Training loss: 1.7791590690612793
Validation loss: 2.0874053835868835

Epoch: 6| Step: 1
Training loss: 1.5930464267730713
Validation loss: 2.0894914468129477

Epoch: 6| Step: 2
Training loss: 2.127427101135254
Validation loss: 2.1046870152155557

Epoch: 6| Step: 3
Training loss: 2.2953171730041504
Validation loss: 2.1112080415089927

Epoch: 6| Step: 4
Training loss: 1.7082833051681519
Validation loss: 2.128561556339264

Epoch: 6| Step: 5
Training loss: 1.9831629991531372
Validation loss: 2.1455091635386148

Epoch: 6| Step: 6
Training loss: 1.8405345678329468
Validation loss: 2.163796345392863

Epoch: 6| Step: 7
Training loss: 1.5912866592407227
Validation loss: 2.153739313284556

Epoch: 6| Step: 8
Training loss: 2.071610927581787
Validation loss: 2.1518646081288657

Epoch: 6| Step: 9
Training loss: 1.7807049751281738
Validation loss: 2.1460417111714682

Epoch: 6| Step: 10
Training loss: 2.3046178817749023
Validation loss: 2.1346115271250405

Epoch: 6| Step: 11
Training loss: 2.1178815364837646
Validation loss: 2.1160956223805747

Epoch: 6| Step: 12
Training loss: 1.4990439414978027
Validation loss: 2.1111655235290527

Epoch: 6| Step: 13
Training loss: 2.0468010902404785
Validation loss: 2.1115156014760337

Epoch: 174| Step: 0
Training loss: 1.4690388441085815
Validation loss: 2.1207141677538552

Epoch: 6| Step: 1
Training loss: 2.5446689128875732
Validation loss: 2.105138679345449

Epoch: 6| Step: 2
Training loss: 1.639763355255127
Validation loss: 2.103890299797058

Epoch: 6| Step: 3
Training loss: 2.0733017921447754
Validation loss: 2.095198333263397

Epoch: 6| Step: 4
Training loss: 1.770533561706543
Validation loss: 2.094146728515625

Epoch: 6| Step: 5
Training loss: 2.96250319480896
Validation loss: 2.1087618867556253

Epoch: 6| Step: 6
Training loss: 1.4221307039260864
Validation loss: 2.11089289188385

Epoch: 6| Step: 7
Training loss: 1.8891479969024658
Validation loss: 2.120107809702555

Epoch: 6| Step: 8
Training loss: 1.5178847312927246
Validation loss: 2.1229068438212075

Epoch: 6| Step: 9
Training loss: 1.86614990234375
Validation loss: 2.1139668027559915

Epoch: 6| Step: 10
Training loss: 1.5220129489898682
Validation loss: 2.132477104663849

Epoch: 6| Step: 11
Training loss: 2.362487316131592
Validation loss: 2.1243724822998047

Epoch: 6| Step: 12
Training loss: 1.914392352104187
Validation loss: 2.1218150854110718

Epoch: 6| Step: 13
Training loss: 1.6727466583251953
Validation loss: 2.121184229850769

Epoch: 175| Step: 0
Training loss: 1.4296071529388428
Validation loss: 2.1182086070378623

Epoch: 6| Step: 1
Training loss: 1.5901180505752563
Validation loss: 2.1306600173314414

Epoch: 6| Step: 2
Training loss: 2.0764524936676025
Validation loss: 2.1114702622095742

Epoch: 6| Step: 3
Training loss: 2.1405935287475586
Validation loss: 2.1156771580378213

Epoch: 6| Step: 4
Training loss: 1.2782498598098755
Validation loss: 2.1110373735427856

Epoch: 6| Step: 5
Training loss: 1.922571063041687
Validation loss: 2.1103151639302573

Epoch: 6| Step: 6
Training loss: 1.6808514595031738
Validation loss: 2.091854373613993

Epoch: 6| Step: 7
Training loss: 2.1414225101470947
Validation loss: 2.102294147014618

Epoch: 6| Step: 8
Training loss: 2.32127046585083
Validation loss: 2.1092825531959534

Epoch: 6| Step: 9
Training loss: 1.6217639446258545
Validation loss: 2.1282259424527488

Epoch: 6| Step: 10
Training loss: 1.9344478845596313
Validation loss: 2.1338427662849426

Epoch: 6| Step: 11
Training loss: 1.9138425588607788
Validation loss: 2.1288164059321084

Epoch: 6| Step: 12
Training loss: 2.075104236602783
Validation loss: 2.1229528983434043

Epoch: 6| Step: 13
Training loss: 2.8067626953125
Validation loss: 2.1167128880818686

Epoch: 176| Step: 0
Training loss: 1.889305830001831
Validation loss: 2.115726053714752

Epoch: 6| Step: 1
Training loss: 1.4109666347503662
Validation loss: 2.108910302321116

Epoch: 6| Step: 2
Training loss: 1.7432235479354858
Validation loss: 2.1052029927571616

Epoch: 6| Step: 3
Training loss: 1.455570101737976
Validation loss: 2.0945507486661277

Epoch: 6| Step: 4
Training loss: 2.0073227882385254
Validation loss: 2.0897436141967773

Epoch: 6| Step: 5
Training loss: 1.818511962890625
Validation loss: 2.076123813788096

Epoch: 6| Step: 6
Training loss: 1.6878113746643066
Validation loss: 2.0802769859631858

Epoch: 6| Step: 7
Training loss: 2.416593074798584
Validation loss: 2.080044607321421

Epoch: 6| Step: 8
Training loss: 1.8231439590454102
Validation loss: 2.0782187978426614

Epoch: 6| Step: 9
Training loss: 1.7544794082641602
Validation loss: 2.086951792240143

Epoch: 6| Step: 10
Training loss: 2.042764663696289
Validation loss: 2.115636924902598

Epoch: 6| Step: 11
Training loss: 1.8182337284088135
Validation loss: 2.1127473513285318

Epoch: 6| Step: 12
Training loss: 2.665863513946533
Validation loss: 2.126737654209137

Epoch: 6| Step: 13
Training loss: 2.376802682876587
Validation loss: 2.1259337067604065

Epoch: 177| Step: 0
Training loss: 2.4231033325195312
Validation loss: 2.1389029026031494

Epoch: 6| Step: 1
Training loss: 1.829715609550476
Validation loss: 2.140002409617106

Epoch: 6| Step: 2
Training loss: 1.8677010536193848
Validation loss: 2.1346769531567893

Epoch: 6| Step: 3
Training loss: 2.161816358566284
Validation loss: 2.138322333494822

Epoch: 6| Step: 4
Training loss: 2.247502565383911
Validation loss: 2.1325364112854004

Epoch: 6| Step: 5
Training loss: 1.442143440246582
Validation loss: 2.0999905665715537

Epoch: 6| Step: 6
Training loss: 1.6835908889770508
Validation loss: 2.102169930934906

Epoch: 6| Step: 7
Training loss: 1.4627721309661865
Validation loss: 2.0894351800282798

Epoch: 6| Step: 8
Training loss: 2.220093250274658
Validation loss: 2.090805451075236

Epoch: 6| Step: 9
Training loss: 1.9929509162902832
Validation loss: 2.075056731700897

Epoch: 6| Step: 10
Training loss: 2.163486957550049
Validation loss: 2.082332491874695

Epoch: 6| Step: 11
Training loss: 1.5533583164215088
Validation loss: 2.0863629579544067

Epoch: 6| Step: 12
Training loss: 2.5547468662261963
Validation loss: 2.09941299756368

Epoch: 6| Step: 13
Training loss: 1.8819502592086792
Validation loss: 2.1033277908960977

Epoch: 178| Step: 0
Training loss: 2.141331195831299
Validation loss: 2.1122027039527893

Epoch: 6| Step: 1
Training loss: 2.546567440032959
Validation loss: 2.120598554611206

Epoch: 6| Step: 2
Training loss: 2.1141602993011475
Validation loss: 2.1087229450543723

Epoch: 6| Step: 3
Training loss: 1.708791971206665
Validation loss: 2.10416712363561

Epoch: 6| Step: 4
Training loss: 2.107489585876465
Validation loss: 2.105007211367289

Epoch: 6| Step: 5
Training loss: 1.1806931495666504
Validation loss: 2.105950951576233

Epoch: 6| Step: 6
Training loss: 2.7442994117736816
Validation loss: 2.1057609915733337

Epoch: 6| Step: 7
Training loss: 1.3929743766784668
Validation loss: 2.107858955860138

Epoch: 6| Step: 8
Training loss: 1.820936918258667
Validation loss: 2.1272444327672324

Epoch: 6| Step: 9
Training loss: 1.6370266675949097
Validation loss: 2.0973721941312156

Epoch: 6| Step: 10
Training loss: 1.7891812324523926
Validation loss: 2.1263238986333213

Epoch: 6| Step: 11
Training loss: 2.119140625
Validation loss: 2.113830268383026

Epoch: 6| Step: 12
Training loss: 1.7680877447128296
Validation loss: 2.122961938381195

Epoch: 6| Step: 13
Training loss: 1.9233291149139404
Validation loss: 2.1023354729016623

Epoch: 179| Step: 0
Training loss: 1.985504388809204
Validation loss: 2.1030967036883035

Epoch: 6| Step: 1
Training loss: 1.9843230247497559
Validation loss: 2.099364181359609

Epoch: 6| Step: 2
Training loss: 1.9789280891418457
Validation loss: 2.120876987775167

Epoch: 6| Step: 3
Training loss: 2.6501269340515137
Validation loss: 2.0958704352378845

Epoch: 6| Step: 4
Training loss: 1.5806331634521484
Validation loss: 2.1056730151176453

Epoch: 6| Step: 5
Training loss: 2.499610424041748
Validation loss: 2.1033459504445395

Epoch: 6| Step: 6
Training loss: 1.271257758140564
Validation loss: 2.10070010026296

Epoch: 6| Step: 7
Training loss: 1.7980928421020508
Validation loss: 2.120929559071859

Epoch: 6| Step: 8
Training loss: 1.7951778173446655
Validation loss: 2.1122250159581504

Epoch: 6| Step: 9
Training loss: 2.15901780128479
Validation loss: 2.1376930475234985

Epoch: 6| Step: 10
Training loss: 1.5411487817764282
Validation loss: 2.130790968736013

Epoch: 6| Step: 11
Training loss: 1.9374831914901733
Validation loss: 2.1391952435175576

Epoch: 6| Step: 12
Training loss: 1.3745349645614624
Validation loss: 2.1447050174077353

Epoch: 6| Step: 13
Training loss: 1.9919328689575195
Validation loss: 2.141759932041168

Epoch: 180| Step: 0
Training loss: 1.7998486757278442
Validation loss: 2.123118579387665

Epoch: 6| Step: 1
Training loss: 1.8305810689926147
Validation loss: 2.110483964284261

Epoch: 6| Step: 2
Training loss: 2.1967992782592773
Validation loss: 2.110500454902649

Epoch: 6| Step: 3
Training loss: 2.0138134956359863
Validation loss: 2.096357444922129

Epoch: 6| Step: 4
Training loss: 1.5049946308135986
Validation loss: 2.07722141345342

Epoch: 6| Step: 5
Training loss: 2.1776208877563477
Validation loss: 2.0734960238138833

Epoch: 6| Step: 6
Training loss: 1.2444100379943848
Validation loss: 2.08056249221166

Epoch: 6| Step: 7
Training loss: 2.1315717697143555
Validation loss: 2.0802883307139077

Epoch: 6| Step: 8
Training loss: 1.8763878345489502
Validation loss: 2.103347142537435

Epoch: 6| Step: 9
Training loss: 2.161400079727173
Validation loss: 2.107506036758423

Epoch: 6| Step: 10
Training loss: 1.5691983699798584
Validation loss: 2.1060604651769004

Epoch: 6| Step: 11
Training loss: 1.8848010301589966
Validation loss: 2.134497265021006

Epoch: 6| Step: 12
Training loss: 1.6322598457336426
Validation loss: 2.1378047466278076

Epoch: 6| Step: 13
Training loss: 2.743067741394043
Validation loss: 2.12552801767985

Epoch: 181| Step: 0
Training loss: 1.8552263975143433
Validation loss: 2.1314794222513833

Epoch: 6| Step: 1
Training loss: 2.023538112640381
Validation loss: 2.1254752477010093

Epoch: 6| Step: 2
Training loss: 1.270122766494751
Validation loss: 2.1407931049664817

Epoch: 6| Step: 3
Training loss: 2.2279884815216064
Validation loss: 2.1173956791559854

Epoch: 6| Step: 4
Training loss: 2.5229530334472656
Validation loss: 2.106954574584961

Epoch: 6| Step: 5
Training loss: 1.90413498878479
Validation loss: 2.1003082394599915

Epoch: 6| Step: 6
Training loss: 2.277817964553833
Validation loss: 2.091159919897715

Epoch: 6| Step: 7
Training loss: 1.7028861045837402
Validation loss: 2.088388502597809

Epoch: 6| Step: 8
Training loss: 1.5587396621704102
Validation loss: 2.0839311281840005

Epoch: 6| Step: 9
Training loss: 2.3131442070007324
Validation loss: 2.0914867719014487

Epoch: 6| Step: 10
Training loss: 1.5009896755218506
Validation loss: 2.0873620907465615

Epoch: 6| Step: 11
Training loss: 1.80708909034729
Validation loss: 2.0834346413612366

Epoch: 6| Step: 12
Training loss: 1.8540451526641846
Validation loss: 2.10407962401708

Epoch: 6| Step: 13
Training loss: 2.2819771766662598
Validation loss: 2.0971659819285073

Epoch: 182| Step: 0
Training loss: 2.452580451965332
Validation loss: 2.105059027671814

Epoch: 6| Step: 1
Training loss: 1.6928520202636719
Validation loss: 2.0995974938074746

Epoch: 6| Step: 2
Training loss: 2.206131935119629
Validation loss: 2.1051984826723733

Epoch: 6| Step: 3
Training loss: 1.3648689985275269
Validation loss: 2.1243112881978354

Epoch: 6| Step: 4
Training loss: 1.6366603374481201
Validation loss: 2.1277426282564798

Epoch: 6| Step: 5
Training loss: 1.898974895477295
Validation loss: 2.134199559688568

Epoch: 6| Step: 6
Training loss: 1.7061805725097656
Validation loss: 2.1355069080988565

Epoch: 6| Step: 7
Training loss: 2.322784423828125
Validation loss: 2.130042254924774

Epoch: 6| Step: 8
Training loss: 1.6412492990493774
Validation loss: 2.1331554651260376

Epoch: 6| Step: 9
Training loss: 1.7927148342132568
Validation loss: 2.1235539317131042

Epoch: 6| Step: 10
Training loss: 1.5022313594818115
Validation loss: 2.106175641218821

Epoch: 6| Step: 11
Training loss: 2.4445760250091553
Validation loss: 2.1021809577941895

Epoch: 6| Step: 12
Training loss: 1.7179641723632812
Validation loss: 2.098738133907318

Epoch: 6| Step: 13
Training loss: 2.6697325706481934
Validation loss: 2.1037944157918296

Epoch: 183| Step: 0
Training loss: 1.8704596757888794
Validation loss: 2.1059999465942383

Epoch: 6| Step: 1
Training loss: 2.024385929107666
Validation loss: 2.10176952679952

Epoch: 6| Step: 2
Training loss: 1.7228527069091797
Validation loss: 2.1049140890439353

Epoch: 6| Step: 3
Training loss: 1.5649710893630981
Validation loss: 2.115113357702891

Epoch: 6| Step: 4
Training loss: 1.7955107688903809
Validation loss: 2.132623871167501

Epoch: 6| Step: 5
Training loss: 1.5087124109268188
Validation loss: 2.1328389048576355

Epoch: 6| Step: 6
Training loss: 2.3312740325927734
Validation loss: 2.1569735209147134

Epoch: 6| Step: 7
Training loss: 2.0161683559417725
Validation loss: 2.144505421320597

Epoch: 6| Step: 8
Training loss: 1.9313145875930786
Validation loss: 2.1307246486345925

Epoch: 6| Step: 9
Training loss: 1.4246459007263184
Validation loss: 2.133660932381948

Epoch: 6| Step: 10
Training loss: 2.4684877395629883
Validation loss: 2.1336949865023294

Epoch: 6| Step: 11
Training loss: 1.798906683921814
Validation loss: 2.1244521141052246

Epoch: 6| Step: 12
Training loss: 2.094320774078369
Validation loss: 2.1070284446080527

Epoch: 6| Step: 13
Training loss: 1.9786641597747803
Validation loss: 2.1207730571428933

Epoch: 184| Step: 0
Training loss: 2.2773098945617676
Validation loss: 2.120234429836273

Epoch: 6| Step: 1
Training loss: 1.423840880393982
Validation loss: 2.1089491049448648

Epoch: 6| Step: 2
Training loss: 1.8522858619689941
Validation loss: 2.0905508995056152

Epoch: 6| Step: 3
Training loss: 1.9712862968444824
Validation loss: 2.1038643519083657

Epoch: 6| Step: 4
Training loss: 2.2298402786254883
Validation loss: 2.0937928756078086

Epoch: 6| Step: 5
Training loss: 1.473231554031372
Validation loss: 2.105349083741506

Epoch: 6| Step: 6
Training loss: 2.23746395111084
Validation loss: 2.1095988750457764

Epoch: 6| Step: 7
Training loss: 1.5439116954803467
Validation loss: 2.1195430358250937

Epoch: 6| Step: 8
Training loss: 1.8701493740081787
Validation loss: 2.1203801234563193

Epoch: 6| Step: 9
Training loss: 2.1393394470214844
Validation loss: 2.1271910071372986

Epoch: 6| Step: 10
Training loss: 1.5948686599731445
Validation loss: 2.125777085622152

Epoch: 6| Step: 11
Training loss: 1.714001178741455
Validation loss: 2.1385579307874045

Epoch: 6| Step: 12
Training loss: 2.18308424949646
Validation loss: 2.1610947847366333

Epoch: 6| Step: 13
Training loss: 1.7945269346237183
Validation loss: 2.153245449066162

Epoch: 185| Step: 0
Training loss: 1.817741870880127
Validation loss: 2.179704407850901

Epoch: 6| Step: 1
Training loss: 1.7469291687011719
Validation loss: 2.162906547387441

Epoch: 6| Step: 2
Training loss: 2.107570171356201
Validation loss: 2.1594128211339316

Epoch: 6| Step: 3
Training loss: 1.9610815048217773
Validation loss: 2.142600893974304

Epoch: 6| Step: 4
Training loss: 1.5634818077087402
Validation loss: 2.1310541232426963

Epoch: 6| Step: 5
Training loss: 1.7679541110992432
Validation loss: 2.136285662651062

Epoch: 6| Step: 6
Training loss: 2.3967185020446777
Validation loss: 2.126062552134196

Epoch: 6| Step: 7
Training loss: 1.3950955867767334
Validation loss: 2.10451732079188

Epoch: 6| Step: 8
Training loss: 1.7466374635696411
Validation loss: 2.1064400474230447

Epoch: 6| Step: 9
Training loss: 1.905626654624939
Validation loss: 2.0869487126668296

Epoch: 6| Step: 10
Training loss: 1.865812063217163
Validation loss: 2.090388317902883

Epoch: 6| Step: 11
Training loss: 2.5234649181365967
Validation loss: 2.0827077627182007

Epoch: 6| Step: 12
Training loss: 2.243776798248291
Validation loss: 2.101476510365804

Epoch: 6| Step: 13
Training loss: 2.3833065032958984
Validation loss: 2.090039531389872

Epoch: 186| Step: 0
Training loss: 1.991368293762207
Validation loss: 2.0958221356074014

Epoch: 6| Step: 1
Training loss: 2.6083364486694336
Validation loss: 2.0991135636965432

Epoch: 6| Step: 2
Training loss: 1.4538772106170654
Validation loss: 2.106617053349813

Epoch: 6| Step: 3
Training loss: 1.4141778945922852
Validation loss: 2.098031461238861

Epoch: 6| Step: 4
Training loss: 1.5922317504882812
Validation loss: 2.1104607582092285

Epoch: 6| Step: 5
Training loss: 1.5544787645339966
Validation loss: 2.0953285892804465

Epoch: 6| Step: 6
Training loss: 1.66334867477417
Validation loss: 2.1190194686253867

Epoch: 6| Step: 7
Training loss: 1.5513572692871094
Validation loss: 2.1008314291636148

Epoch: 6| Step: 8
Training loss: 2.0514912605285645
Validation loss: 2.1119054357210794

Epoch: 6| Step: 9
Training loss: 1.7183133363723755
Validation loss: 2.125860651334127

Epoch: 6| Step: 10
Training loss: 2.2735557556152344
Validation loss: 2.1174599726994834

Epoch: 6| Step: 11
Training loss: 2.0988380908966064
Validation loss: 2.129183769226074

Epoch: 6| Step: 12
Training loss: 1.7101030349731445
Validation loss: 2.114559233188629

Epoch: 6| Step: 13
Training loss: 2.5424489974975586
Validation loss: 2.119897206624349

Epoch: 187| Step: 0
Training loss: 2.059736967086792
Validation loss: 2.138057986895243

Epoch: 6| Step: 1
Training loss: 1.927323341369629
Validation loss: 2.128920833269755

Epoch: 6| Step: 2
Training loss: 2.256772518157959
Validation loss: 2.121428608894348

Epoch: 6| Step: 3
Training loss: 2.619819164276123
Validation loss: 2.1229312419891357

Epoch: 6| Step: 4
Training loss: 1.8565773963928223
Validation loss: 2.1049920320510864

Epoch: 6| Step: 5
Training loss: 1.6248424053192139
Validation loss: 2.110995868841807

Epoch: 6| Step: 6
Training loss: 1.793548822402954
Validation loss: 2.1104190746943154

Epoch: 6| Step: 7
Training loss: 2.1988000869750977
Validation loss: 2.099519729614258

Epoch: 6| Step: 8
Training loss: 1.6911218166351318
Validation loss: 2.1035786867141724

Epoch: 6| Step: 9
Training loss: 1.722358226776123
Validation loss: 2.108937124411265

Epoch: 6| Step: 10
Training loss: 2.307861328125
Validation loss: 2.1132086714108786

Epoch: 6| Step: 11
Training loss: 1.1057580709457397
Validation loss: 2.1097266475359597

Epoch: 6| Step: 12
Training loss: 1.4532191753387451
Validation loss: 2.103572964668274

Epoch: 6| Step: 13
Training loss: 1.6012120246887207
Validation loss: 2.1164828141530356

Epoch: 188| Step: 0
Training loss: 2.0721538066864014
Validation loss: 2.093105951944987

Epoch: 6| Step: 1
Training loss: 1.567997694015503
Validation loss: 2.1246491074562073

Epoch: 6| Step: 2
Training loss: 1.754355788230896
Validation loss: 2.121017575263977

Epoch: 6| Step: 3
Training loss: 1.9805448055267334
Validation loss: 2.1237624287605286

Epoch: 6| Step: 4
Training loss: 2.948310375213623
Validation loss: 2.1248432397842407

Epoch: 6| Step: 5
Training loss: 1.6588809490203857
Validation loss: 2.110884507497152

Epoch: 6| Step: 6
Training loss: 1.0632247924804688
Validation loss: 2.1125927766164145

Epoch: 6| Step: 7
Training loss: 2.3260886669158936
Validation loss: 2.1122967004776

Epoch: 6| Step: 8
Training loss: 1.5885411500930786
Validation loss: 2.1147409478823342

Epoch: 6| Step: 9
Training loss: 2.036088466644287
Validation loss: 2.110681176185608

Epoch: 6| Step: 10
Training loss: 1.58479642868042
Validation loss: 2.1055884957313538

Epoch: 6| Step: 11
Training loss: 1.7806320190429688
Validation loss: 2.094819188117981

Epoch: 6| Step: 12
Training loss: 1.975029468536377
Validation loss: 2.0910996794700623

Epoch: 6| Step: 13
Training loss: 1.744606375694275
Validation loss: 2.1185986598332724

Epoch: 189| Step: 0
Training loss: 2.2280542850494385
Validation loss: 2.118970215320587

Epoch: 6| Step: 1
Training loss: 1.6845468282699585
Validation loss: 2.1142591635386148

Epoch: 6| Step: 2
Training loss: 1.7373039722442627
Validation loss: 2.1086596846580505

Epoch: 6| Step: 3
Training loss: 1.9485299587249756
Validation loss: 2.1089821457862854

Epoch: 6| Step: 4
Training loss: 2.064629554748535
Validation loss: 2.103418489297231

Epoch: 6| Step: 5
Training loss: 1.2014697790145874
Validation loss: 2.1249950726826987

Epoch: 6| Step: 6
Training loss: 1.689097285270691
Validation loss: 2.1399521430333457

Epoch: 6| Step: 7
Training loss: 2.02917742729187
Validation loss: 2.1210784117380777

Epoch: 6| Step: 8
Training loss: 1.773576259613037
Validation loss: 2.1372381250063577

Epoch: 6| Step: 9
Training loss: 1.7250597476959229
Validation loss: 2.1510990858078003

Epoch: 6| Step: 10
Training loss: 1.8278263807296753
Validation loss: 2.127058744430542

Epoch: 6| Step: 11
Training loss: 2.3502376079559326
Validation loss: 2.139961043993632

Epoch: 6| Step: 12
Training loss: 1.9463311433792114
Validation loss: 2.130183736483256

Epoch: 6| Step: 13
Training loss: 1.8902404308319092
Validation loss: 2.1402870416641235

Epoch: 190| Step: 0
Training loss: 1.8730411529541016
Validation loss: 2.127692619959513

Epoch: 6| Step: 1
Training loss: 1.5666786432266235
Validation loss: 2.1186583836873374

Epoch: 6| Step: 2
Training loss: 1.6173015832901
Validation loss: 2.119626224040985

Epoch: 6| Step: 3
Training loss: 1.7175207138061523
Validation loss: 2.1367149353027344

Epoch: 6| Step: 4
Training loss: 1.1633787155151367
Validation loss: 2.138570249080658

Epoch: 6| Step: 5
Training loss: 1.8260005712509155
Validation loss: 2.146930476029714

Epoch: 6| Step: 6
Training loss: 1.9675891399383545
Validation loss: 2.135363459587097

Epoch: 6| Step: 7
Training loss: 2.455273151397705
Validation loss: 2.142150084177653

Epoch: 6| Step: 8
Training loss: 1.9059439897537231
Validation loss: 2.1471566557884216

Epoch: 6| Step: 9
Training loss: 1.6615631580352783
Validation loss: 2.150997817516327

Epoch: 6| Step: 10
Training loss: 1.679638147354126
Validation loss: 2.145769556363424

Epoch: 6| Step: 11
Training loss: 2.6716887950897217
Validation loss: 2.124467611312866

Epoch: 6| Step: 12
Training loss: 1.5500493049621582
Validation loss: 2.1357734004656472

Epoch: 6| Step: 13
Training loss: 2.218193292617798
Validation loss: 2.146125396092733

Epoch: 191| Step: 0
Training loss: 1.5686825513839722
Validation loss: 2.1344112157821655

Epoch: 6| Step: 1
Training loss: 2.134359359741211
Validation loss: 2.139393627643585

Epoch: 6| Step: 2
Training loss: 1.6634268760681152
Validation loss: 2.159396787484487

Epoch: 6| Step: 3
Training loss: 1.4819159507751465
Validation loss: 2.1379089752833047

Epoch: 6| Step: 4
Training loss: 1.3571528196334839
Validation loss: 2.124488055706024

Epoch: 6| Step: 5
Training loss: 1.9607365131378174
Validation loss: 2.1245054602622986

Epoch: 6| Step: 6
Training loss: 2.5143179893493652
Validation loss: 2.136701683203379

Epoch: 6| Step: 7
Training loss: 1.6085902452468872
Validation loss: 2.132594406604767

Epoch: 6| Step: 8
Training loss: 1.9015668630599976
Validation loss: 2.118365466594696

Epoch: 6| Step: 9
Training loss: 1.7227375507354736
Validation loss: 2.124598522981008

Epoch: 6| Step: 10
Training loss: 1.7300543785095215
Validation loss: 2.1056730151176453

Epoch: 6| Step: 11
Training loss: 2.224299907684326
Validation loss: 2.1035876671473184

Epoch: 6| Step: 12
Training loss: 2.3805863857269287
Validation loss: 2.1105334957440696

Epoch: 6| Step: 13
Training loss: 2.25626802444458
Validation loss: 2.0981728633244834

Epoch: 192| Step: 0
Training loss: 1.8547284603118896
Validation loss: 2.1000223755836487

Epoch: 6| Step: 1
Training loss: 1.596470594406128
Validation loss: 2.1122944553693137

Epoch: 6| Step: 2
Training loss: 2.3642635345458984
Validation loss: 2.11149525642395

Epoch: 6| Step: 3
Training loss: 1.5015625953674316
Validation loss: 2.1335105498631797

Epoch: 6| Step: 4
Training loss: 2.147801160812378
Validation loss: 2.1254405975341797

Epoch: 6| Step: 5
Training loss: 1.735111951828003
Validation loss: 2.1399231950441995

Epoch: 6| Step: 6
Training loss: 1.4388651847839355
Validation loss: 2.145993411540985

Epoch: 6| Step: 7
Training loss: 2.139526844024658
Validation loss: 2.1298460563023887

Epoch: 6| Step: 8
Training loss: 1.323258638381958
Validation loss: 2.1454639037450156

Epoch: 6| Step: 9
Training loss: 1.9717336893081665
Validation loss: 2.1144534945487976

Epoch: 6| Step: 10
Training loss: 2.5153586864471436
Validation loss: 2.1316893100738525

Epoch: 6| Step: 11
Training loss: 2.3243446350097656
Validation loss: 2.1030192772547402

Epoch: 6| Step: 12
Training loss: 1.9384571313858032
Validation loss: 2.091937323411306

Epoch: 6| Step: 13
Training loss: 1.543755054473877
Validation loss: 2.1273094614346824

Epoch: 193| Step: 0
Training loss: 1.7412936687469482
Validation loss: 2.1016759872436523

Epoch: 6| Step: 1
Training loss: 1.6971874237060547
Validation loss: 2.1215695341428122

Epoch: 6| Step: 2
Training loss: 1.541886806488037
Validation loss: 2.125958720842997

Epoch: 6| Step: 3
Training loss: 1.890817403793335
Validation loss: 2.1264464457829795

Epoch: 6| Step: 4
Training loss: 2.1359615325927734
Validation loss: 2.1464908520380654

Epoch: 6| Step: 5
Training loss: 1.649967908859253
Validation loss: 2.1507574717203775

Epoch: 6| Step: 6
Training loss: 1.3529239892959595
Validation loss: 2.148965537548065

Epoch: 6| Step: 7
Training loss: 1.9698792695999146
Validation loss: 2.1691872676213584

Epoch: 6| Step: 8
Training loss: 1.911467432975769
Validation loss: 2.162379741668701

Epoch: 6| Step: 9
Training loss: 1.5553932189941406
Validation loss: 2.1500192483266196

Epoch: 6| Step: 10
Training loss: 2.496004581451416
Validation loss: 2.120832542578379

Epoch: 6| Step: 11
Training loss: 1.543705940246582
Validation loss: 2.1317073901494346

Epoch: 6| Step: 12
Training loss: 1.8082711696624756
Validation loss: 2.1222712198893228

Epoch: 6| Step: 13
Training loss: 2.518984794616699
Validation loss: 2.1094396114349365

Epoch: 194| Step: 0
Training loss: 1.8122093677520752
Validation loss: 2.113195260365804

Epoch: 6| Step: 1
Training loss: 2.1664607524871826
Validation loss: 2.1267436742782593

Epoch: 6| Step: 2
Training loss: 2.030388832092285
Validation loss: 2.1374546885490417

Epoch: 6| Step: 3
Training loss: 1.1958414316177368
Validation loss: 2.1549317240715027

Epoch: 6| Step: 4
Training loss: 1.7303982973098755
Validation loss: 2.1588542660077414

Epoch: 6| Step: 5
Training loss: 1.7466013431549072
Validation loss: 2.1598807175954184

Epoch: 6| Step: 6
Training loss: 2.148158550262451
Validation loss: 2.184262673060099

Epoch: 6| Step: 7
Training loss: 1.9998581409454346
Validation loss: 2.1775290767351785

Epoch: 6| Step: 8
Training loss: 2.286080837249756
Validation loss: 2.1558915774027505

Epoch: 6| Step: 9
Training loss: 2.4851255416870117
Validation loss: 2.1432438294092813

Epoch: 6| Step: 10
Training loss: 1.2348430156707764
Validation loss: 2.144660552342733

Epoch: 6| Step: 11
Training loss: 1.0975887775421143
Validation loss: 2.1446058551470437

Epoch: 6| Step: 12
Training loss: 1.7730536460876465
Validation loss: 2.1306876142819724

Epoch: 6| Step: 13
Training loss: 1.9209191799163818
Validation loss: 2.1234166026115417

Epoch: 195| Step: 0
Training loss: 2.055779218673706
Validation loss: 2.127722760041555

Epoch: 6| Step: 1
Training loss: 1.0411306619644165
Validation loss: 2.1114590167999268

Epoch: 6| Step: 2
Training loss: 1.519312858581543
Validation loss: 2.122649093468984

Epoch: 6| Step: 3
Training loss: 1.6237475872039795
Validation loss: 2.115060488382975

Epoch: 6| Step: 4
Training loss: 2.256227493286133
Validation loss: 2.1217174331347146

Epoch: 6| Step: 5
Training loss: 1.7567334175109863
Validation loss: 2.1304001013437905

Epoch: 6| Step: 6
Training loss: 2.1348800659179688
Validation loss: 2.1270366509755454

Epoch: 6| Step: 7
Training loss: 2.1752166748046875
Validation loss: 2.1237574021021524

Epoch: 6| Step: 8
Training loss: 2.0136587619781494
Validation loss: 2.1161098082860312

Epoch: 6| Step: 9
Training loss: 1.8275723457336426
Validation loss: 2.1371413270632424

Epoch: 6| Step: 10
Training loss: 1.4314682483673096
Validation loss: 2.1228920420010886

Epoch: 6| Step: 11
Training loss: 2.151726245880127
Validation loss: 2.130622982978821

Epoch: 6| Step: 12
Training loss: 2.0599398612976074
Validation loss: 2.140705664952596

Epoch: 6| Step: 13
Training loss: 2.026967763900757
Validation loss: 2.144747813542684

Epoch: 196| Step: 0
Training loss: 1.361621379852295
Validation loss: 2.1415282487869263

Epoch: 6| Step: 1
Training loss: 2.2483768463134766
Validation loss: 2.190333922704061

Epoch: 6| Step: 2
Training loss: 1.884350299835205
Validation loss: 2.197076916694641

Epoch: 6| Step: 3
Training loss: 2.170624256134033
Validation loss: 2.1953419049580893

Epoch: 6| Step: 4
Training loss: 1.8994323015213013
Validation loss: 2.1909547050793967

Epoch: 6| Step: 5
Training loss: 1.450640082359314
Validation loss: 2.1861491401990256

Epoch: 6| Step: 6
Training loss: 1.130982756614685
Validation loss: 2.1828830242156982

Epoch: 6| Step: 7
Training loss: 2.351954221725464
Validation loss: 2.1744964917500815

Epoch: 6| Step: 8
Training loss: 1.92604398727417
Validation loss: 2.1496020555496216

Epoch: 6| Step: 9
Training loss: 1.681872010231018
Validation loss: 2.129169305165609

Epoch: 6| Step: 10
Training loss: 1.640342354774475
Validation loss: 2.116313417752584

Epoch: 6| Step: 11
Training loss: 2.5760505199432373
Validation loss: 2.108673850695292

Epoch: 6| Step: 12
Training loss: 2.2972283363342285
Validation loss: 2.1059894959131875

Epoch: 6| Step: 13
Training loss: 1.7133915424346924
Validation loss: 2.122368415196737

Epoch: 197| Step: 0
Training loss: 2.144160747528076
Validation loss: 2.1174185474713645

Epoch: 6| Step: 1
Training loss: 1.8049957752227783
Validation loss: 2.118123014767965

Epoch: 6| Step: 2
Training loss: 2.3368606567382812
Validation loss: 2.128479818503062

Epoch: 6| Step: 3
Training loss: 1.3174997568130493
Validation loss: 2.1320802172025046

Epoch: 6| Step: 4
Training loss: 2.152409553527832
Validation loss: 2.1286277770996094

Epoch: 6| Step: 5
Training loss: 1.4517605304718018
Validation loss: 2.157708009084066

Epoch: 6| Step: 6
Training loss: 1.5630648136138916
Validation loss: 2.1520068645477295

Epoch: 6| Step: 7
Training loss: 1.8142189979553223
Validation loss: 2.1395599842071533

Epoch: 6| Step: 8
Training loss: 1.4274005889892578
Validation loss: 2.1588746507962546

Epoch: 6| Step: 9
Training loss: 1.9851438999176025
Validation loss: 2.166238248348236

Epoch: 6| Step: 10
Training loss: 1.6197779178619385
Validation loss: 2.1510226726531982

Epoch: 6| Step: 11
Training loss: 2.375890016555786
Validation loss: 2.1550790866216025

Epoch: 6| Step: 12
Training loss: 1.5778021812438965
Validation loss: 2.1535235246022544

Epoch: 6| Step: 13
Training loss: 2.023090124130249
Validation loss: 2.154808779557546

Epoch: 198| Step: 0
Training loss: 2.422942638397217
Validation loss: 2.1401829719543457

Epoch: 6| Step: 1
Training loss: 1.7445595264434814
Validation loss: 2.140385687351227

Epoch: 6| Step: 2
Training loss: 1.5781604051589966
Validation loss: 2.1427895426750183

Epoch: 6| Step: 3
Training loss: 1.6403728723526
Validation loss: 2.142402946949005

Epoch: 6| Step: 4
Training loss: 2.03275465965271
Validation loss: 2.1514857014020285

Epoch: 6| Step: 5
Training loss: 1.769927740097046
Validation loss: 2.1351667642593384

Epoch: 6| Step: 6
Training loss: 2.2838833332061768
Validation loss: 2.1284152269363403

Epoch: 6| Step: 7
Training loss: 1.8326425552368164
Validation loss: 2.1356090704600015

Epoch: 6| Step: 8
Training loss: 2.401491403579712
Validation loss: 2.122741679350535

Epoch: 6| Step: 9
Training loss: 1.3552846908569336
Validation loss: 2.137845834096273

Epoch: 6| Step: 10
Training loss: 1.432755708694458
Validation loss: 2.150954325993856

Epoch: 6| Step: 11
Training loss: 1.7654509544372559
Validation loss: 2.157481829325358

Epoch: 6| Step: 12
Training loss: 1.5189447402954102
Validation loss: 2.1668600837389627

Epoch: 6| Step: 13
Training loss: 1.4735424518585205
Validation loss: 2.1713667710622153

Epoch: 199| Step: 0
Training loss: 2.101065158843994
Validation loss: 2.1601520776748657

Epoch: 6| Step: 1
Training loss: 1.3792318105697632
Validation loss: 2.150426427523295

Epoch: 6| Step: 2
Training loss: 2.3158483505249023
Validation loss: 2.141429324944814

Epoch: 6| Step: 3
Training loss: 1.0611594915390015
Validation loss: 2.148410181204478

Epoch: 6| Step: 4
Training loss: 2.287637233734131
Validation loss: 2.1435212890307107

Epoch: 6| Step: 5
Training loss: 2.033989429473877
Validation loss: 2.131513079007467

Epoch: 6| Step: 6
Training loss: 1.7551194429397583
Validation loss: 2.1243783036867776

Epoch: 6| Step: 7
Training loss: 1.5557262897491455
Validation loss: 2.137665569782257

Epoch: 6| Step: 8
Training loss: 2.215933322906494
Validation loss: 2.138559619585673

Epoch: 6| Step: 9
Training loss: 1.6761656999588013
Validation loss: 2.1164039770762124

Epoch: 6| Step: 10
Training loss: 1.7433942556381226
Validation loss: 2.104917367299398

Epoch: 6| Step: 11
Training loss: 1.5281310081481934
Validation loss: 2.103400389353434

Epoch: 6| Step: 12
Training loss: 1.5549756288528442
Validation loss: 2.1309796571731567

Epoch: 6| Step: 13
Training loss: 2.035468101501465
Validation loss: 2.1006198724110923

Epoch: 200| Step: 0
Training loss: 1.80950129032135
Validation loss: 2.1165300408999124

Epoch: 6| Step: 1
Training loss: 1.4163625240325928
Validation loss: 2.1174789667129517

Epoch: 6| Step: 2
Training loss: 1.9679960012435913
Validation loss: 2.133560617764791

Epoch: 6| Step: 3
Training loss: 1.4501588344573975
Validation loss: 2.1466899315516152

Epoch: 6| Step: 4
Training loss: 2.304739475250244
Validation loss: 2.1498913367589316

Epoch: 6| Step: 5
Training loss: 2.019540548324585
Validation loss: 2.1708568135897317

Epoch: 6| Step: 6
Training loss: 1.5601866245269775
Validation loss: 2.1600719690322876

Epoch: 6| Step: 7
Training loss: 1.9762365818023682
Validation loss: 2.177861491839091

Epoch: 6| Step: 8
Training loss: 1.5337605476379395
Validation loss: 2.178438901901245

Epoch: 6| Step: 9
Training loss: 2.6447033882141113
Validation loss: 2.147272606690725

Epoch: 6| Step: 10
Training loss: 2.33514666557312
Validation loss: 2.15538497765859

Epoch: 6| Step: 11
Training loss: 1.4654059410095215
Validation loss: 2.138471007347107

Epoch: 6| Step: 12
Training loss: 1.2792447805404663
Validation loss: 2.1436520417531333

Epoch: 6| Step: 13
Training loss: 1.9435219764709473
Validation loss: 2.1386311252911887

Testing loss: 1.83438904165364

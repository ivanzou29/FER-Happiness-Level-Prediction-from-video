Epoch: 1| Step: 0
Training loss: 5.137607574462891
Validation loss: 5.2927672068278

Epoch: 6| Step: 1
Training loss: 5.885513782501221
Validation loss: 5.290918668111165

Epoch: 6| Step: 2
Training loss: 5.086514949798584
Validation loss: 5.2891067663828535

Epoch: 6| Step: 3
Training loss: 6.144278526306152
Validation loss: 5.287356376647949

Epoch: 6| Step: 4
Training loss: 5.623600006103516
Validation loss: 5.285535295804341

Epoch: 6| Step: 5
Training loss: 5.25981330871582
Validation loss: 5.283816337585449

Epoch: 6| Step: 6
Training loss: 3.859163522720337
Validation loss: 5.282020330429077

Epoch: 6| Step: 7
Training loss: 5.349322319030762
Validation loss: 5.280250310897827

Epoch: 6| Step: 8
Training loss: 5.286343097686768
Validation loss: 5.2785459359486895

Epoch: 6| Step: 9
Training loss: 4.94727087020874
Validation loss: 5.276724100112915

Epoch: 6| Step: 10
Training loss: 5.339191913604736
Validation loss: 5.274936596552531

Epoch: 6| Step: 11
Training loss: 5.225131034851074
Validation loss: 5.273003260294597

Epoch: 6| Step: 12
Training loss: 6.096588134765625
Validation loss: 5.27119779586792

Epoch: 6| Step: 13
Training loss: 5.662530899047852
Validation loss: 5.269176721572876

Epoch: 2| Step: 0
Training loss: 6.659130096435547
Validation loss: 5.267001549402873

Epoch: 6| Step: 1
Training loss: 6.095783710479736
Validation loss: 5.264867226282756

Epoch: 6| Step: 2
Training loss: 5.209149360656738
Validation loss: 5.262528737386067

Epoch: 6| Step: 3
Training loss: 5.410417556762695
Validation loss: 5.260136842727661

Epoch: 6| Step: 4
Training loss: 5.657803535461426
Validation loss: 5.257613023122151

Epoch: 6| Step: 5
Training loss: 5.179436683654785
Validation loss: 5.255016406377156

Epoch: 6| Step: 6
Training loss: 4.659817695617676
Validation loss: 5.252094666163127

Epoch: 6| Step: 7
Training loss: 4.139074802398682
Validation loss: 5.24922251701355

Epoch: 6| Step: 8
Training loss: 6.022317886352539
Validation loss: 5.24624490737915

Epoch: 6| Step: 9
Training loss: 5.0780792236328125
Validation loss: 5.242951154708862

Epoch: 6| Step: 10
Training loss: 4.266322135925293
Validation loss: 5.239576657613118

Epoch: 6| Step: 11
Training loss: 6.0956315994262695
Validation loss: 5.236225843429565

Epoch: 6| Step: 12
Training loss: 4.79796838760376
Validation loss: 5.232348680496216

Epoch: 6| Step: 13
Training loss: 5.203278541564941
Validation loss: 5.228512605031331

Epoch: 3| Step: 0
Training loss: 5.175463676452637
Validation loss: 5.224473635355632

Epoch: 6| Step: 1
Training loss: 5.141024589538574
Validation loss: 5.2201346556345625

Epoch: 6| Step: 2
Training loss: 3.928175449371338
Validation loss: 5.21559206644694

Epoch: 6| Step: 3
Training loss: 6.628940105438232
Validation loss: 5.210944255193074

Epoch: 6| Step: 4
Training loss: 4.425352573394775
Validation loss: 5.206092278162639

Epoch: 6| Step: 5
Training loss: 4.685197353363037
Validation loss: 5.201059500376384

Epoch: 6| Step: 6
Training loss: 5.724076271057129
Validation loss: 5.195712248484294

Epoch: 6| Step: 7
Training loss: 4.378894329071045
Validation loss: 5.190143903096517

Epoch: 6| Step: 8
Training loss: 5.80745267868042
Validation loss: 5.184319178263347

Epoch: 6| Step: 9
Training loss: 5.610929012298584
Validation loss: 5.178470293680827

Epoch: 6| Step: 10
Training loss: 5.648776054382324
Validation loss: 5.17209005355835

Epoch: 6| Step: 11
Training loss: 5.335369110107422
Validation loss: 5.1655698617299395

Epoch: 6| Step: 12
Training loss: 5.729884147644043
Validation loss: 5.1587628920873

Epoch: 6| Step: 13
Training loss: 5.44060754776001
Validation loss: 5.151830116907756

Epoch: 4| Step: 0
Training loss: 4.820642471313477
Validation loss: 5.144604722658793

Epoch: 6| Step: 1
Training loss: 5.209612846374512
Validation loss: 5.137253761291504

Epoch: 6| Step: 2
Training loss: 5.449456691741943
Validation loss: 5.129626592000325

Epoch: 6| Step: 3
Training loss: 5.039788246154785
Validation loss: 5.121678113937378

Epoch: 6| Step: 4
Training loss: 5.28898286819458
Validation loss: 5.113668759663899

Epoch: 6| Step: 5
Training loss: 5.372603416442871
Validation loss: 5.105522871017456

Epoch: 6| Step: 6
Training loss: 5.982287406921387
Validation loss: 5.097129265467326

Epoch: 6| Step: 7
Training loss: 4.494875907897949
Validation loss: 5.088740746180217

Epoch: 6| Step: 8
Training loss: 5.102267265319824
Validation loss: 5.080271005630493

Epoch: 6| Step: 9
Training loss: 4.670592784881592
Validation loss: 5.071617484092712

Epoch: 6| Step: 10
Training loss: 5.866706371307373
Validation loss: 5.062858899434407

Epoch: 6| Step: 11
Training loss: 5.104400634765625
Validation loss: 5.0541672706604

Epoch: 6| Step: 12
Training loss: 5.21205472946167
Validation loss: 5.045401573181152

Epoch: 6| Step: 13
Training loss: 4.73668909072876
Validation loss: 5.036646286646525

Epoch: 5| Step: 0
Training loss: 5.830999374389648
Validation loss: 5.027744611104329

Epoch: 6| Step: 1
Training loss: 6.05464506149292
Validation loss: 5.019156614939372

Epoch: 6| Step: 2
Training loss: 5.859724044799805
Validation loss: 5.010430256525676

Epoch: 6| Step: 3
Training loss: 4.152270317077637
Validation loss: 5.001430114110311

Epoch: 6| Step: 4
Training loss: 6.132482528686523
Validation loss: 4.9927806456883745

Epoch: 6| Step: 5
Training loss: 5.121886253356934
Validation loss: 4.983889738718669

Epoch: 6| Step: 6
Training loss: 4.246587753295898
Validation loss: 4.975023667017619

Epoch: 6| Step: 7
Training loss: 4.767058372497559
Validation loss: 4.966464519500732

Epoch: 6| Step: 8
Training loss: 4.2953667640686035
Validation loss: 4.957950512568156

Epoch: 6| Step: 9
Training loss: 5.387442588806152
Validation loss: 4.949222246805827

Epoch: 6| Step: 10
Training loss: 4.185565948486328
Validation loss: 4.9405012130737305

Epoch: 6| Step: 11
Training loss: 4.147627830505371
Validation loss: 4.9319173494974775

Epoch: 6| Step: 12
Training loss: 4.992893218994141
Validation loss: 4.9229683081309

Epoch: 6| Step: 13
Training loss: 5.551966667175293
Validation loss: 4.914326111475627

Epoch: 6| Step: 0
Training loss: 5.236311435699463
Validation loss: 4.905807733535767

Epoch: 6| Step: 1
Training loss: 5.01145076751709
Validation loss: 4.897086222966512

Epoch: 6| Step: 2
Training loss: 5.035179138183594
Validation loss: 4.888163963953654

Epoch: 6| Step: 3
Training loss: 4.914819717407227
Validation loss: 4.879666646321614

Epoch: 6| Step: 4
Training loss: 5.431673049926758
Validation loss: 4.8706449667612715

Epoch: 6| Step: 5
Training loss: 5.049808502197266
Validation loss: 4.8619710604349775

Epoch: 6| Step: 6
Training loss: 5.160438060760498
Validation loss: 4.853070020675659

Epoch: 6| Step: 7
Training loss: 4.774381637573242
Validation loss: 4.844140251477559

Epoch: 6| Step: 8
Training loss: 5.099522590637207
Validation loss: 4.83536692460378

Epoch: 6| Step: 9
Training loss: 4.367255687713623
Validation loss: 4.826483329137166

Epoch: 6| Step: 10
Training loss: 4.091538429260254
Validation loss: 4.8175744613011675

Epoch: 6| Step: 11
Training loss: 4.717212200164795
Validation loss: 4.809558471043904

Epoch: 6| Step: 12
Training loss: 5.421998023986816
Validation loss: 4.801627993583679

Epoch: 6| Step: 13
Training loss: 4.801257133483887
Validation loss: 4.793870131174724

Epoch: 7| Step: 0
Training loss: 5.321640968322754
Validation loss: 4.786914587020874

Epoch: 6| Step: 1
Training loss: 3.5706002712249756
Validation loss: 4.779471357663472

Epoch: 6| Step: 2
Training loss: 3.897383213043213
Validation loss: 4.7723292509714765

Epoch: 6| Step: 3
Training loss: 5.316515922546387
Validation loss: 4.764621734619141

Epoch: 6| Step: 4
Training loss: 5.7102460861206055
Validation loss: 4.7569193840026855

Epoch: 6| Step: 5
Training loss: 5.702081680297852
Validation loss: 4.749656995137532

Epoch: 6| Step: 6
Training loss: 4.78806209564209
Validation loss: 4.742170095443726

Epoch: 6| Step: 7
Training loss: 3.312060832977295
Validation loss: 4.734798431396484

Epoch: 6| Step: 8
Training loss: 4.754511833190918
Validation loss: 4.727638562520345

Epoch: 6| Step: 9
Training loss: 3.9297826290130615
Validation loss: 4.719895720481873

Epoch: 6| Step: 10
Training loss: 6.161602020263672
Validation loss: 4.712869485219319

Epoch: 6| Step: 11
Training loss: 5.45261812210083
Validation loss: 4.705061674118042

Epoch: 6| Step: 12
Training loss: 5.155699729919434
Validation loss: 4.698687195777893

Epoch: 6| Step: 13
Training loss: 4.547676086425781
Validation loss: 4.691221714019775

Epoch: 8| Step: 0
Training loss: 4.156804084777832
Validation loss: 4.684497674306233

Epoch: 6| Step: 1
Training loss: 5.543764591217041
Validation loss: 4.677866617838542

Epoch: 6| Step: 2
Training loss: 5.4786858558654785
Validation loss: 4.670200347900391

Epoch: 6| Step: 3
Training loss: 3.8199143409729004
Validation loss: 4.663473129272461

Epoch: 6| Step: 4
Training loss: 4.951352119445801
Validation loss: 4.657253821690877

Epoch: 6| Step: 5
Training loss: 4.846719741821289
Validation loss: 4.6503950754801435

Epoch: 6| Step: 6
Training loss: 4.373647689819336
Validation loss: 4.644182721773784

Epoch: 6| Step: 7
Training loss: 3.8739166259765625
Validation loss: 4.6368666887283325

Epoch: 6| Step: 8
Training loss: 5.703836917877197
Validation loss: 4.630499402681987

Epoch: 6| Step: 9
Training loss: 5.706231117248535
Validation loss: 4.62379805246989

Epoch: 6| Step: 10
Training loss: 4.270757675170898
Validation loss: 4.616800904273987

Epoch: 6| Step: 11
Training loss: 4.679105758666992
Validation loss: 4.6102233330408735

Epoch: 6| Step: 12
Training loss: 4.418083190917969
Validation loss: 4.603587627410889

Epoch: 6| Step: 13
Training loss: 4.483309745788574
Validation loss: 4.597289959589641

Epoch: 9| Step: 0
Training loss: 5.745361328125
Validation loss: 4.590955018997192

Epoch: 6| Step: 1
Training loss: 4.2172675132751465
Validation loss: 4.58458145459493

Epoch: 6| Step: 2
Training loss: 4.769875526428223
Validation loss: 4.579034407933553

Epoch: 6| Step: 3
Training loss: 4.757936477661133
Validation loss: 4.573063810666402

Epoch: 6| Step: 4
Training loss: 4.190166473388672
Validation loss: 4.567416191101074

Epoch: 6| Step: 5
Training loss: 5.165652751922607
Validation loss: 4.561774532000224

Epoch: 6| Step: 6
Training loss: 5.452252388000488
Validation loss: 4.555973807970683

Epoch: 6| Step: 7
Training loss: 4.521267890930176
Validation loss: 4.550321658452352

Epoch: 6| Step: 8
Training loss: 4.294328212738037
Validation loss: 4.544233600298564

Epoch: 6| Step: 9
Training loss: 4.505462646484375
Validation loss: 4.538975437482198

Epoch: 6| Step: 10
Training loss: 3.978178024291992
Validation loss: 4.533234437306722

Epoch: 6| Step: 11
Training loss: 4.669896125793457
Validation loss: 4.528180440266927

Epoch: 6| Step: 12
Training loss: 4.824676513671875
Validation loss: 4.522622346878052

Epoch: 6| Step: 13
Training loss: 4.089184761047363
Validation loss: 4.51725709438324

Epoch: 10| Step: 0
Training loss: 4.81439208984375
Validation loss: 4.512650728225708

Epoch: 6| Step: 1
Training loss: 4.351937770843506
Validation loss: 4.506933371225993

Epoch: 6| Step: 2
Training loss: 4.337676048278809
Validation loss: 4.501503229141235

Epoch: 6| Step: 3
Training loss: 5.965932846069336
Validation loss: 4.495908896128337

Epoch: 6| Step: 4
Training loss: 4.7497148513793945
Validation loss: 4.489937702814738

Epoch: 6| Step: 5
Training loss: 4.728316307067871
Validation loss: 4.484933058420817

Epoch: 6| Step: 6
Training loss: 5.175541877746582
Validation loss: 4.478641629219055

Epoch: 6| Step: 7
Training loss: 5.049220085144043
Validation loss: 4.473482767740886

Epoch: 6| Step: 8
Training loss: 5.18776798248291
Validation loss: 4.466922163963318

Epoch: 6| Step: 9
Training loss: 4.178249835968018
Validation loss: 4.461174488067627

Epoch: 6| Step: 10
Training loss: 3.577524423599243
Validation loss: 4.4561472336451216

Epoch: 6| Step: 11
Training loss: 4.2903313636779785
Validation loss: 4.451491792996724

Epoch: 6| Step: 12
Training loss: 4.284506797790527
Validation loss: 4.4461774826049805

Epoch: 6| Step: 13
Training loss: 3.5030627250671387
Validation loss: 4.441088914871216

Epoch: 11| Step: 0
Training loss: 3.8626768589019775
Validation loss: 4.436187664667766

Epoch: 6| Step: 1
Training loss: 4.572958946228027
Validation loss: 4.4314117431640625

Epoch: 6| Step: 2
Training loss: 5.810731887817383
Validation loss: 4.425851742426555

Epoch: 6| Step: 3
Training loss: 4.069596290588379
Validation loss: 4.421590805053711

Epoch: 6| Step: 4
Training loss: 5.003010272979736
Validation loss: 4.416892449061076

Epoch: 6| Step: 5
Training loss: 4.55297327041626
Validation loss: 4.411431193351746

Epoch: 6| Step: 6
Training loss: 4.369293212890625
Validation loss: 4.407055099805196

Epoch: 6| Step: 7
Training loss: 5.113234043121338
Validation loss: 4.402181228001912

Epoch: 6| Step: 8
Training loss: 3.7905619144439697
Validation loss: 4.397367397944133

Epoch: 6| Step: 9
Training loss: 4.74833869934082
Validation loss: 4.392903844515483

Epoch: 6| Step: 10
Training loss: 4.503823280334473
Validation loss: 4.388075113296509

Epoch: 6| Step: 11
Training loss: 4.571791648864746
Validation loss: 4.382822434107463

Epoch: 6| Step: 12
Training loss: 3.25752592086792
Validation loss: 4.3770179351170855

Epoch: 6| Step: 13
Training loss: 5.03469705581665
Validation loss: 4.371957182884216

Epoch: 12| Step: 0
Training loss: 4.57025146484375
Validation loss: 4.366800983746846

Epoch: 6| Step: 1
Training loss: 5.3960652351379395
Validation loss: 4.3620829582214355

Epoch: 6| Step: 2
Training loss: 3.828372001647949
Validation loss: 4.357578674952189

Epoch: 6| Step: 3
Training loss: 5.3079118728637695
Validation loss: 4.352937022844951

Epoch: 6| Step: 4
Training loss: 5.477893829345703
Validation loss: 4.347707112630208

Epoch: 6| Step: 5
Training loss: 5.825736045837402
Validation loss: 4.341684023539226

Epoch: 6| Step: 6
Training loss: 3.8143703937530518
Validation loss: 4.337636470794678

Epoch: 6| Step: 7
Training loss: 3.1268906593322754
Validation loss: 4.332493583361308

Epoch: 6| Step: 8
Training loss: 4.013941287994385
Validation loss: 4.327604214350383

Epoch: 6| Step: 9
Training loss: 4.133406639099121
Validation loss: 4.322760423024495

Epoch: 6| Step: 10
Training loss: 4.466270446777344
Validation loss: 4.31726861000061

Epoch: 6| Step: 11
Training loss: 4.756555557250977
Validation loss: 4.312012632687886

Epoch: 6| Step: 12
Training loss: 4.204244136810303
Validation loss: 4.307368278503418

Epoch: 6| Step: 13
Training loss: 3.4740729331970215
Validation loss: 4.302223165829976

Epoch: 13| Step: 0
Training loss: 3.917545795440674
Validation loss: 4.296467582384746

Epoch: 6| Step: 1
Training loss: 4.503113746643066
Validation loss: 4.291574319203694

Epoch: 6| Step: 2
Training loss: 3.838275909423828
Validation loss: 4.287310282389323

Epoch: 6| Step: 3
Training loss: 5.026693344116211
Validation loss: 4.282868226369222

Epoch: 6| Step: 4
Training loss: 4.131880760192871
Validation loss: 4.277467489242554

Epoch: 6| Step: 5
Training loss: 5.259551048278809
Validation loss: 4.271767894426982

Epoch: 6| Step: 6
Training loss: 3.5954391956329346
Validation loss: 4.267454902331035

Epoch: 6| Step: 7
Training loss: 4.1836018562316895
Validation loss: 4.2621815999348955

Epoch: 6| Step: 8
Training loss: 4.794167518615723
Validation loss: 4.257485588391622

Epoch: 6| Step: 9
Training loss: 4.4429731369018555
Validation loss: 4.251752217610677

Epoch: 6| Step: 10
Training loss: 4.29307746887207
Validation loss: 4.246886928876241

Epoch: 6| Step: 11
Training loss: 5.235908508300781
Validation loss: 4.242889602979024

Epoch: 6| Step: 12
Training loss: 3.700869560241699
Validation loss: 4.237215240796407

Epoch: 6| Step: 13
Training loss: 4.546555995941162
Validation loss: 4.232802788416545

Epoch: 14| Step: 0
Training loss: 4.652935981750488
Validation loss: 4.22702956199646

Epoch: 6| Step: 1
Training loss: 4.554559707641602
Validation loss: 4.2233052253723145

Epoch: 6| Step: 2
Training loss: 4.551840782165527
Validation loss: 4.218960563341777

Epoch: 6| Step: 3
Training loss: 4.412126541137695
Validation loss: 4.213770627975464

Epoch: 6| Step: 4
Training loss: 3.859302520751953
Validation loss: 4.207877953847249

Epoch: 6| Step: 5
Training loss: 4.302730560302734
Validation loss: 4.202893336613973

Epoch: 6| Step: 6
Training loss: 4.556066513061523
Validation loss: 4.198946038881938

Epoch: 6| Step: 7
Training loss: 3.5340776443481445
Validation loss: 4.1941927671432495

Epoch: 6| Step: 8
Training loss: 4.130617141723633
Validation loss: 4.189673105875651

Epoch: 6| Step: 9
Training loss: 4.183595180511475
Validation loss: 4.183711210886638

Epoch: 6| Step: 10
Training loss: 4.064384460449219
Validation loss: 4.178381641705831

Epoch: 6| Step: 11
Training loss: 4.4266276359558105
Validation loss: 4.17383627096812

Epoch: 6| Step: 12
Training loss: 5.0077362060546875
Validation loss: 4.169396082560222

Epoch: 6| Step: 13
Training loss: 4.375078201293945
Validation loss: 4.164886832237244

Epoch: 15| Step: 0
Training loss: 5.577399253845215
Validation loss: 4.159340262413025

Epoch: 6| Step: 1
Training loss: 4.177262783050537
Validation loss: 4.155037045478821

Epoch: 6| Step: 2
Training loss: 3.5473523139953613
Validation loss: 4.150683164596558

Epoch: 6| Step: 3
Training loss: 5.842475891113281
Validation loss: 4.146680076917012

Epoch: 6| Step: 4
Training loss: 3.4470858573913574
Validation loss: 4.14183755715688

Epoch: 6| Step: 5
Training loss: 3.6066298484802246
Validation loss: 4.136467655499776

Epoch: 6| Step: 6
Training loss: 5.5367536544799805
Validation loss: 4.131894071896871

Epoch: 6| Step: 7
Training loss: 3.0491514205932617
Validation loss: 4.129122018814087

Epoch: 6| Step: 8
Training loss: 4.101991176605225
Validation loss: 4.1245981852213545

Epoch: 6| Step: 9
Training loss: 4.330948829650879
Validation loss: 4.118725220362346

Epoch: 6| Step: 10
Training loss: 4.222144603729248
Validation loss: 4.113280773162842

Epoch: 6| Step: 11
Training loss: 4.152480125427246
Validation loss: 4.108589609464009

Epoch: 6| Step: 12
Training loss: 3.9852051734924316
Validation loss: 4.103735049565633

Epoch: 6| Step: 13
Training loss: 4.170982837677002
Validation loss: 4.099183400472005

Epoch: 16| Step: 0
Training loss: 4.3916778564453125
Validation loss: 4.0938243468602495

Epoch: 6| Step: 1
Training loss: 4.418654918670654
Validation loss: 4.088651140530904

Epoch: 6| Step: 2
Training loss: 3.039856433868408
Validation loss: 4.0854372183481855

Epoch: 6| Step: 3
Training loss: 3.9013233184814453
Validation loss: 4.082570910453796

Epoch: 6| Step: 4
Training loss: 3.1470534801483154
Validation loss: 4.075173656145732

Epoch: 6| Step: 5
Training loss: 4.142190933227539
Validation loss: 4.072946270306905

Epoch: 6| Step: 6
Training loss: 4.76749324798584
Validation loss: 4.070889234542847

Epoch: 6| Step: 7
Training loss: 3.899095058441162
Validation loss: 4.066154718399048

Epoch: 6| Step: 8
Training loss: 4.262909889221191
Validation loss: 4.059223333994548

Epoch: 6| Step: 9
Training loss: 3.746279239654541
Validation loss: 4.051872968673706

Epoch: 6| Step: 10
Training loss: 4.488961219787598
Validation loss: 4.046655535697937

Epoch: 6| Step: 11
Training loss: 4.363893985748291
Validation loss: 4.04350217183431

Epoch: 6| Step: 12
Training loss: 5.26243782043457
Validation loss: 4.041775703430176

Epoch: 6| Step: 13
Training loss: 5.0325822830200195
Validation loss: 4.032259265581767

Epoch: 17| Step: 0
Training loss: 3.4146695137023926
Validation loss: 4.0299530029296875

Epoch: 6| Step: 1
Training loss: 3.0444862842559814
Validation loss: 4.029008070627849

Epoch: 6| Step: 2
Training loss: 3.7462663650512695
Validation loss: 4.024496714274089

Epoch: 6| Step: 3
Training loss: 4.283360481262207
Validation loss: 4.019336938858032

Epoch: 6| Step: 4
Training loss: 4.644402027130127
Validation loss: 4.013212243715922

Epoch: 6| Step: 5
Training loss: 4.015008926391602
Validation loss: 4.007118463516235

Epoch: 6| Step: 6
Training loss: 4.7742695808410645
Validation loss: 4.001357396443685

Epoch: 6| Step: 7
Training loss: 3.7165136337280273
Validation loss: 3.995906035105387

Epoch: 6| Step: 8
Training loss: 3.688248634338379
Validation loss: 3.9912689129511514

Epoch: 6| Step: 9
Training loss: 4.460224628448486
Validation loss: 3.9862008094787598

Epoch: 6| Step: 10
Training loss: 4.631734848022461
Validation loss: 3.980523188908895

Epoch: 6| Step: 11
Training loss: 4.165680885314941
Validation loss: 3.976966619491577

Epoch: 6| Step: 12
Training loss: 5.080251693725586
Validation loss: 3.9715416034062705

Epoch: 6| Step: 13
Training loss: 4.358687400817871
Validation loss: 3.9677257935206094

Epoch: 18| Step: 0
Training loss: 4.1581621170043945
Validation loss: 3.964860200881958

Epoch: 6| Step: 1
Training loss: 4.779685020446777
Validation loss: 3.9611215194066367

Epoch: 6| Step: 2
Training loss: 5.123987197875977
Validation loss: 3.956794500350952

Epoch: 6| Step: 3
Training loss: 4.0120697021484375
Validation loss: 3.950802803039551

Epoch: 6| Step: 4
Training loss: 3.7707295417785645
Validation loss: 3.9454023043314614

Epoch: 6| Step: 5
Training loss: 4.483222007751465
Validation loss: 3.940261125564575

Epoch: 6| Step: 6
Training loss: 3.433694839477539
Validation loss: 3.937968651453654

Epoch: 6| Step: 7
Training loss: 3.7950024604797363
Validation loss: 3.932577053705851

Epoch: 6| Step: 8
Training loss: 2.91473388671875
Validation loss: 3.9270012378692627

Epoch: 6| Step: 9
Training loss: 4.433425426483154
Validation loss: 3.9221829573313394

Epoch: 6| Step: 10
Training loss: 3.981076240539551
Validation loss: 3.9174514611562095

Epoch: 6| Step: 11
Training loss: 4.122666835784912
Validation loss: 3.9141387144724527

Epoch: 6| Step: 12
Training loss: 3.8271420001983643
Validation loss: 3.9092275301615396

Epoch: 6| Step: 13
Training loss: 4.312137603759766
Validation loss: 3.9052979946136475

Epoch: 19| Step: 0
Training loss: 3.572528839111328
Validation loss: 3.9013254642486572

Epoch: 6| Step: 1
Training loss: 3.844780921936035
Validation loss: 3.8968359231948853

Epoch: 6| Step: 2
Training loss: 4.66819429397583
Validation loss: 3.89193856716156

Epoch: 6| Step: 3
Training loss: 4.3465576171875
Validation loss: 3.8873751958211265

Epoch: 6| Step: 4
Training loss: 4.403367042541504
Validation loss: 3.8828421433766684

Epoch: 6| Step: 5
Training loss: 4.311046123504639
Validation loss: 3.8783172766367593

Epoch: 6| Step: 6
Training loss: 3.5105714797973633
Validation loss: 3.8741236130396524

Epoch: 6| Step: 7
Training loss: 3.958322048187256
Validation loss: 3.8703321615854898

Epoch: 6| Step: 8
Training loss: 4.524471282958984
Validation loss: 3.866575042406718

Epoch: 6| Step: 9
Training loss: 3.7095932960510254
Validation loss: 3.8622028827667236

Epoch: 6| Step: 10
Training loss: 4.507169723510742
Validation loss: 3.856816530227661

Epoch: 6| Step: 11
Training loss: 3.66316556930542
Validation loss: 3.8526068528493247

Epoch: 6| Step: 12
Training loss: 3.7081782817840576
Validation loss: 3.849273761113485

Epoch: 6| Step: 13
Training loss: 3.608867645263672
Validation loss: 3.8465811808904014

Epoch: 20| Step: 0
Training loss: 3.4886932373046875
Validation loss: 3.8433906634648642

Epoch: 6| Step: 1
Training loss: 3.519246816635132
Validation loss: 3.841291348139445

Epoch: 6| Step: 2
Training loss: 3.8654043674468994
Validation loss: 3.83646023273468

Epoch: 6| Step: 3
Training loss: 4.523829460144043
Validation loss: 3.8298588196436563

Epoch: 6| Step: 4
Training loss: 4.170673370361328
Validation loss: 3.8247812589009604

Epoch: 6| Step: 5
Training loss: 4.483604907989502
Validation loss: 3.819702625274658

Epoch: 6| Step: 6
Training loss: 3.981090545654297
Validation loss: 3.815412680308024

Epoch: 6| Step: 7
Training loss: 4.03336238861084
Validation loss: 3.8122227986653647

Epoch: 6| Step: 8
Training loss: 3.8114988803863525
Validation loss: 3.807028849919637

Epoch: 6| Step: 9
Training loss: 3.59837007522583
Validation loss: 3.803126891454061

Epoch: 6| Step: 10
Training loss: 3.8634424209594727
Validation loss: 3.7992485761642456

Epoch: 6| Step: 11
Training loss: 3.828866481781006
Validation loss: 3.7948102951049805

Epoch: 6| Step: 12
Training loss: 4.694541931152344
Validation loss: 3.791094978650411

Epoch: 6| Step: 13
Training loss: 3.6927614212036133
Validation loss: 3.7867273886998496

Epoch: 21| Step: 0
Training loss: 4.171713352203369
Validation loss: 3.7821579774220786

Epoch: 6| Step: 1
Training loss: 4.408205032348633
Validation loss: 3.7780892848968506

Epoch: 6| Step: 2
Training loss: 4.803040027618408
Validation loss: 3.7737963994344077

Epoch: 6| Step: 3
Training loss: 4.151360511779785
Validation loss: 3.768422524134318

Epoch: 6| Step: 4
Training loss: 3.791487216949463
Validation loss: 3.7638982931772866

Epoch: 6| Step: 5
Training loss: 3.51600980758667
Validation loss: 3.759591499964396

Epoch: 6| Step: 6
Training loss: 3.3489701747894287
Validation loss: 3.754767656326294

Epoch: 6| Step: 7
Training loss: 4.343517303466797
Validation loss: 3.751172184944153

Epoch: 6| Step: 8
Training loss: 4.211738586425781
Validation loss: 3.7473164796829224

Epoch: 6| Step: 9
Training loss: 3.525315284729004
Validation loss: 3.742350697517395

Epoch: 6| Step: 10
Training loss: 3.0357699394226074
Validation loss: 3.738795757293701

Epoch: 6| Step: 11
Training loss: 3.8435826301574707
Validation loss: 3.7344884475072226

Epoch: 6| Step: 12
Training loss: 3.4384026527404785
Validation loss: 3.729874769846598

Epoch: 6| Step: 13
Training loss: 4.178106784820557
Validation loss: 3.7265044450759888

Epoch: 22| Step: 0
Training loss: 3.0280983448028564
Validation loss: 3.7216257651646933

Epoch: 6| Step: 1
Training loss: 3.9802753925323486
Validation loss: 3.7176350355148315

Epoch: 6| Step: 2
Training loss: 4.086964130401611
Validation loss: 3.7145615418752036

Epoch: 6| Step: 3
Training loss: 4.283926010131836
Validation loss: 3.710047880808512

Epoch: 6| Step: 4
Training loss: 2.6618902683258057
Validation loss: 3.7061931689580283

Epoch: 6| Step: 5
Training loss: 4.696495532989502
Validation loss: 3.7025099992752075

Epoch: 6| Step: 6
Training loss: 2.894199848175049
Validation loss: 3.698320984840393

Epoch: 6| Step: 7
Training loss: 4.305217266082764
Validation loss: 3.6935613552729287

Epoch: 6| Step: 8
Training loss: 2.6581473350524902
Validation loss: 3.6900373299916587

Epoch: 6| Step: 9
Training loss: 4.587573528289795
Validation loss: 3.686095039049784

Epoch: 6| Step: 10
Training loss: 3.964057445526123
Validation loss: 3.6819027264912925

Epoch: 6| Step: 11
Training loss: 3.863659143447876
Validation loss: 3.678025722503662

Epoch: 6| Step: 12
Training loss: 4.199378490447998
Validation loss: 3.6747655073801675

Epoch: 6| Step: 13
Training loss: 4.7646803855896
Validation loss: 3.6702821254730225

Epoch: 23| Step: 0
Training loss: 3.2261085510253906
Validation loss: 3.66555388768514

Epoch: 6| Step: 1
Training loss: 3.3289952278137207
Validation loss: 3.661853075027466

Epoch: 6| Step: 2
Training loss: 3.3902058601379395
Validation loss: 3.657721757888794

Epoch: 6| Step: 3
Training loss: 3.7700061798095703
Validation loss: 3.6534870862960815

Epoch: 6| Step: 4
Training loss: 3.879079818725586
Validation loss: 3.6490673224131265

Epoch: 6| Step: 5
Training loss: 3.477680206298828
Validation loss: 3.645322640736898

Epoch: 6| Step: 6
Training loss: 3.976032257080078
Validation loss: 3.6408567825953164

Epoch: 6| Step: 7
Training loss: 4.250284194946289
Validation loss: 3.6361997922261557

Epoch: 6| Step: 8
Training loss: 4.049705982208252
Validation loss: 3.631927808125814

Epoch: 6| Step: 9
Training loss: 4.620211601257324
Validation loss: 3.6275632778803506

Epoch: 6| Step: 10
Training loss: 4.112198829650879
Validation loss: 3.623276193936666

Epoch: 6| Step: 11
Training loss: 3.7879703044891357
Validation loss: 3.6200919151306152

Epoch: 6| Step: 12
Training loss: 3.7766470909118652
Validation loss: 3.614970882733663

Epoch: 6| Step: 13
Training loss: 3.568075180053711
Validation loss: 3.610934019088745

Epoch: 24| Step: 0
Training loss: 3.0033979415893555
Validation loss: 3.606685479482015

Epoch: 6| Step: 1
Training loss: 4.5977373123168945
Validation loss: 3.602189779281616

Epoch: 6| Step: 2
Training loss: 3.423191547393799
Validation loss: 3.5977724393208823

Epoch: 6| Step: 3
Training loss: 5.3392558097839355
Validation loss: 3.5947605768839517

Epoch: 6| Step: 4
Training loss: 4.084601402282715
Validation loss: 3.589418609937032

Epoch: 6| Step: 5
Training loss: 3.6846120357513428
Validation loss: 3.584282875061035

Epoch: 6| Step: 6
Training loss: 3.2350871562957764
Validation loss: 3.580353061358134

Epoch: 6| Step: 7
Training loss: 3.370206832885742
Validation loss: 3.576261878013611

Epoch: 6| Step: 8
Training loss: 2.6200766563415527
Validation loss: 3.5724061330159507

Epoch: 6| Step: 9
Training loss: 2.7619874477386475
Validation loss: 3.568737030029297

Epoch: 6| Step: 10
Training loss: 4.015949249267578
Validation loss: 3.564363638559977

Epoch: 6| Step: 11
Training loss: 3.784376621246338
Validation loss: 3.5595436493555703

Epoch: 6| Step: 12
Training loss: 4.009987831115723
Validation loss: 3.5550010999043784

Epoch: 6| Step: 13
Training loss: 4.479582786560059
Validation loss: 3.550468921661377

Epoch: 25| Step: 0
Training loss: 3.7968356609344482
Validation loss: 3.5458408991495767

Epoch: 6| Step: 1
Training loss: 3.2331151962280273
Validation loss: 3.541475454966227

Epoch: 6| Step: 2
Training loss: 3.6210644245147705
Validation loss: 3.537087400754293

Epoch: 6| Step: 3
Training loss: 3.6083085536956787
Validation loss: 3.532156507174174

Epoch: 6| Step: 4
Training loss: 3.7168514728546143
Validation loss: 3.5264265537261963

Epoch: 6| Step: 5
Training loss: 3.810594081878662
Validation loss: 3.5210930903752646

Epoch: 6| Step: 6
Training loss: 3.1974339485168457
Validation loss: 3.5167768398920694

Epoch: 6| Step: 7
Training loss: 4.483970642089844
Validation loss: 3.511694073677063

Epoch: 6| Step: 8
Training loss: 3.2354366779327393
Validation loss: 3.506262183189392

Epoch: 6| Step: 9
Training loss: 4.152762413024902
Validation loss: 3.500899910926819

Epoch: 6| Step: 10
Training loss: 3.545605182647705
Validation loss: 3.5002578099568686

Epoch: 6| Step: 11
Training loss: 3.2981338500976562
Validation loss: 3.489397406578064

Epoch: 6| Step: 12
Training loss: 4.285269737243652
Validation loss: 3.488373637199402

Epoch: 6| Step: 13
Training loss: 3.558582305908203
Validation loss: 3.4869938691457114

Epoch: 26| Step: 0
Training loss: 3.374563694000244
Validation loss: 3.480328679084778

Epoch: 6| Step: 1
Training loss: 4.173516273498535
Validation loss: 3.4732757806777954

Epoch: 6| Step: 2
Training loss: 3.3975024223327637
Validation loss: 3.4677929480870566

Epoch: 6| Step: 3
Training loss: 4.476565837860107
Validation loss: 3.463687539100647

Epoch: 6| Step: 4
Training loss: 3.264038562774658
Validation loss: 3.4586130380630493

Epoch: 6| Step: 5
Training loss: 4.026077747344971
Validation loss: 3.452712615331014

Epoch: 6| Step: 6
Training loss: 3.6512229442596436
Validation loss: 3.44869863986969

Epoch: 6| Step: 7
Training loss: 3.5740866661071777
Validation loss: 3.442316015561422

Epoch: 6| Step: 8
Training loss: 3.2152175903320312
Validation loss: 3.436377843221029

Epoch: 6| Step: 9
Training loss: 4.082430362701416
Validation loss: 3.4307610193888345

Epoch: 6| Step: 10
Training loss: 3.2419304847717285
Validation loss: 3.425649642944336

Epoch: 6| Step: 11
Training loss: 3.7300431728363037
Validation loss: 3.4196261167526245

Epoch: 6| Step: 12
Training loss: 3.4460184574127197
Validation loss: 3.414871335029602

Epoch: 6| Step: 13
Training loss: 2.917180299758911
Validation loss: 3.4080586036046348

Epoch: 27| Step: 0
Training loss: 3.0862884521484375
Validation loss: 3.4026201168696084

Epoch: 6| Step: 1
Training loss: 3.9304404258728027
Validation loss: 3.3968860705693564

Epoch: 6| Step: 2
Training loss: 3.3854174613952637
Validation loss: 3.3928974072138467

Epoch: 6| Step: 3
Training loss: 3.7059450149536133
Validation loss: 3.390828569730123

Epoch: 6| Step: 4
Training loss: 3.1912832260131836
Validation loss: 3.379430095354716

Epoch: 6| Step: 5
Training loss: 4.10189962387085
Validation loss: 3.373660167058309

Epoch: 6| Step: 6
Training loss: 2.678605556488037
Validation loss: 3.36671773592631

Epoch: 6| Step: 7
Training loss: 3.532546281814575
Validation loss: 3.362310767173767

Epoch: 6| Step: 8
Training loss: 3.584264039993286
Validation loss: 3.359153230985006

Epoch: 6| Step: 9
Training loss: 4.035840034484863
Validation loss: 3.352126141389211

Epoch: 6| Step: 10
Training loss: 3.2018837928771973
Validation loss: 3.3488457202911377

Epoch: 6| Step: 11
Training loss: 3.7597126960754395
Validation loss: 3.344161033630371

Epoch: 6| Step: 12
Training loss: 3.9447760581970215
Validation loss: 3.3391033411026

Epoch: 6| Step: 13
Training loss: 3.3834242820739746
Validation loss: 3.3344217936197915

Epoch: 28| Step: 0
Training loss: 2.3247528076171875
Validation loss: 3.3285022179285684

Epoch: 6| Step: 1
Training loss: 4.527949333190918
Validation loss: 3.324358423550924

Epoch: 6| Step: 2
Training loss: 3.7917332649230957
Validation loss: 3.3193858861923218

Epoch: 6| Step: 3
Training loss: 4.678088665008545
Validation loss: 3.3142826159795127

Epoch: 6| Step: 4
Training loss: 3.2915444374084473
Validation loss: 3.3078966538111367

Epoch: 6| Step: 5
Training loss: 4.073561191558838
Validation loss: 3.3044999043146768

Epoch: 6| Step: 6
Training loss: 2.958873748779297
Validation loss: 3.2989463408788047

Epoch: 6| Step: 7
Training loss: 3.578725814819336
Validation loss: 3.2944186131159463

Epoch: 6| Step: 8
Training loss: 2.367100715637207
Validation loss: 3.288845419883728

Epoch: 6| Step: 9
Training loss: 2.9514927864074707
Validation loss: 3.286651094754537

Epoch: 6| Step: 10
Training loss: 3.091029644012451
Validation loss: 3.280340234438578

Epoch: 6| Step: 11
Training loss: 3.410851001739502
Validation loss: 3.2769116163253784

Epoch: 6| Step: 12
Training loss: 3.453953742980957
Validation loss: 3.271874109903971

Epoch: 6| Step: 13
Training loss: 4.120229721069336
Validation loss: 3.2687905629475913

Epoch: 29| Step: 0
Training loss: 1.8352688550949097
Validation loss: 3.262972275416056

Epoch: 6| Step: 1
Training loss: 3.5998988151550293
Validation loss: 3.260572671890259

Epoch: 6| Step: 2
Training loss: 3.72147798538208
Validation loss: 3.257127126057943

Epoch: 6| Step: 3
Training loss: 4.039525985717773
Validation loss: 3.2526721159617105

Epoch: 6| Step: 4
Training loss: 3.7054898738861084
Validation loss: 3.24866259098053

Epoch: 6| Step: 5
Training loss: 2.7989892959594727
Validation loss: 3.2435787518819175

Epoch: 6| Step: 6
Training loss: 4.046545028686523
Validation loss: 3.2395222187042236

Epoch: 6| Step: 7
Training loss: 3.6280994415283203
Validation loss: 3.2345163424809775

Epoch: 6| Step: 8
Training loss: 3.9098339080810547
Validation loss: 3.2310587962468467

Epoch: 6| Step: 9
Training loss: 3.9079654216766357
Validation loss: 3.226518909136454

Epoch: 6| Step: 10
Training loss: 3.3174195289611816
Validation loss: 3.2221588691075644

Epoch: 6| Step: 11
Training loss: 3.8867881298065186
Validation loss: 3.2161920070648193

Epoch: 6| Step: 12
Training loss: 2.327204704284668
Validation loss: 3.210963249206543

Epoch: 6| Step: 13
Training loss: 3.0603585243225098
Validation loss: 3.206578850746155

Epoch: 30| Step: 0
Training loss: 3.714815378189087
Validation loss: 3.201383590698242

Epoch: 6| Step: 1
Training loss: 3.0121312141418457
Validation loss: 3.197182615598043

Epoch: 6| Step: 2
Training loss: 3.4890599250793457
Validation loss: 3.1928669214248657

Epoch: 6| Step: 3
Training loss: 3.270510673522949
Validation loss: 3.188056389490763

Epoch: 6| Step: 4
Training loss: 4.1587300300598145
Validation loss: 3.183293263117472

Epoch: 6| Step: 5
Training loss: 2.921471118927002
Validation loss: 3.180408159891764

Epoch: 6| Step: 6
Training loss: 3.8535189628601074
Validation loss: 3.1744051774342856

Epoch: 6| Step: 7
Training loss: 3.367933511734009
Validation loss: 3.1702519257863364

Epoch: 6| Step: 8
Training loss: 3.280468463897705
Validation loss: 3.166032910346985

Epoch: 6| Step: 9
Training loss: 3.535252571105957
Validation loss: 3.1624629894892373

Epoch: 6| Step: 10
Training loss: 3.4963393211364746
Validation loss: 3.1575680573781333

Epoch: 6| Step: 11
Training loss: 3.4716005325317383
Validation loss: 3.1523640553156533

Epoch: 6| Step: 12
Training loss: 2.784651756286621
Validation loss: 3.1487324237823486

Epoch: 6| Step: 13
Training loss: 2.6039295196533203
Validation loss: 3.14616060256958

Epoch: 31| Step: 0
Training loss: 2.4897003173828125
Validation loss: 3.141656835873922

Epoch: 6| Step: 1
Training loss: 3.398601531982422
Validation loss: 3.138267437616984

Epoch: 6| Step: 2
Training loss: 4.109133243560791
Validation loss: 3.132913510004679

Epoch: 6| Step: 3
Training loss: 3.1762757301330566
Validation loss: 3.13003146648407

Epoch: 6| Step: 4
Training loss: 3.204066753387451
Validation loss: 3.1240639686584473

Epoch: 6| Step: 5
Training loss: 3.1084742546081543
Validation loss: 3.119780739148458

Epoch: 6| Step: 6
Training loss: 3.216430187225342
Validation loss: 3.116411288579305

Epoch: 6| Step: 7
Training loss: 3.5043444633483887
Validation loss: 3.1128068765004477

Epoch: 6| Step: 8
Training loss: 2.8260581493377686
Validation loss: 3.1093828678131104

Epoch: 6| Step: 9
Training loss: 3.709484338760376
Validation loss: 3.1038262446721396

Epoch: 6| Step: 10
Training loss: 3.255913496017456
Validation loss: 3.09933074315389

Epoch: 6| Step: 11
Training loss: 3.717350482940674
Validation loss: 3.0957523584365845

Epoch: 6| Step: 12
Training loss: 2.7821967601776123
Validation loss: 3.0922176837921143

Epoch: 6| Step: 13
Training loss: 3.7057957649230957
Validation loss: 3.0898818969726562

Epoch: 32| Step: 0
Training loss: 3.6963741779327393
Validation loss: 3.0874980290730796

Epoch: 6| Step: 1
Training loss: 3.183368682861328
Validation loss: 3.0831764141718545

Epoch: 6| Step: 2
Training loss: 3.4355320930480957
Validation loss: 3.0784452756245932

Epoch: 6| Step: 3
Training loss: 2.553004741668701
Validation loss: 3.073111414909363

Epoch: 6| Step: 4
Training loss: 2.820404052734375
Validation loss: 3.068374236424764

Epoch: 6| Step: 5
Training loss: 2.867083787918091
Validation loss: 3.065101941426595

Epoch: 6| Step: 6
Training loss: 3.4156761169433594
Validation loss: 3.061028560002645

Epoch: 6| Step: 7
Training loss: 2.916177988052368
Validation loss: 3.056890686353048

Epoch: 6| Step: 8
Training loss: 4.245664119720459
Validation loss: 3.0527300437291465

Epoch: 6| Step: 9
Training loss: 3.493675708770752
Validation loss: 3.048883557319641

Epoch: 6| Step: 10
Training loss: 2.6361498832702637
Validation loss: 3.045058786869049

Epoch: 6| Step: 11
Training loss: 2.9198904037475586
Validation loss: 3.041390538215637

Epoch: 6| Step: 12
Training loss: 3.7818856239318848
Validation loss: 3.037624994913737

Epoch: 6| Step: 13
Training loss: 3.501634359359741
Validation loss: 3.0338308811187744

Epoch: 33| Step: 0
Training loss: 1.8985605239868164
Validation loss: 3.029081145922343

Epoch: 6| Step: 1
Training loss: 3.523014545440674
Validation loss: 3.0265817642211914

Epoch: 6| Step: 2
Training loss: 3.6233913898468018
Validation loss: 3.022768417994181

Epoch: 6| Step: 3
Training loss: 3.010133981704712
Validation loss: 3.018385926882426

Epoch: 6| Step: 4
Training loss: 3.6709015369415283
Validation loss: 3.0142802000045776

Epoch: 6| Step: 5
Training loss: 2.658839225769043
Validation loss: 3.013536890347799

Epoch: 6| Step: 6
Training loss: 2.7449729442596436
Validation loss: 3.0154021183649697

Epoch: 6| Step: 7
Training loss: 3.263193368911743
Validation loss: 3.0038157304128013

Epoch: 6| Step: 8
Training loss: 3.494722843170166
Validation loss: 3.0014377435048423

Epoch: 6| Step: 9
Training loss: 2.93526554107666
Validation loss: 2.996585806210836

Epoch: 6| Step: 10
Training loss: 3.9477880001068115
Validation loss: 2.9933653672536216

Epoch: 6| Step: 11
Training loss: 3.4699888229370117
Validation loss: 2.9900559981664023

Epoch: 6| Step: 12
Training loss: 2.9933857917785645
Validation loss: 2.9879262844721475

Epoch: 6| Step: 13
Training loss: 3.539153814315796
Validation loss: 2.9851765632629395

Epoch: 34| Step: 0
Training loss: 2.905219078063965
Validation loss: 2.980692426363627

Epoch: 6| Step: 1
Training loss: 2.97251558303833
Validation loss: 2.9761509895324707

Epoch: 6| Step: 2
Training loss: 4.026910781860352
Validation loss: 2.97202197710673

Epoch: 6| Step: 3
Training loss: 2.7897517681121826
Validation loss: 2.9670674006144204

Epoch: 6| Step: 4
Training loss: 3.817356586456299
Validation loss: 2.9652228355407715

Epoch: 6| Step: 5
Training loss: 2.814175844192505
Validation loss: 2.961899757385254

Epoch: 6| Step: 6
Training loss: 3.288618564605713
Validation loss: 2.9581024249394736

Epoch: 6| Step: 7
Training loss: 2.7534961700439453
Validation loss: 2.955621083577474

Epoch: 6| Step: 8
Training loss: 2.3901376724243164
Validation loss: 2.9514899253845215

Epoch: 6| Step: 9
Training loss: 3.5467259883880615
Validation loss: 2.9468056758244834

Epoch: 6| Step: 10
Training loss: 3.6775619983673096
Validation loss: 2.942936579386393

Epoch: 6| Step: 11
Training loss: 3.5326504707336426
Validation loss: 2.937737544377645

Epoch: 6| Step: 12
Training loss: 2.6235127449035645
Validation loss: 2.9346280097961426

Epoch: 6| Step: 13
Training loss: 3.0266189575195312
Validation loss: 2.9314427773157754

Epoch: 35| Step: 0
Training loss: 2.732167959213257
Validation loss: 2.9271556536356607

Epoch: 6| Step: 1
Training loss: 3.6569743156433105
Validation loss: 2.9236940145492554

Epoch: 6| Step: 2
Training loss: 2.6857643127441406
Validation loss: 2.920313835144043

Epoch: 6| Step: 3
Training loss: 3.214693069458008
Validation loss: 2.9169482787450156

Epoch: 6| Step: 4
Training loss: 3.4038875102996826
Validation loss: 2.9142548640569053

Epoch: 6| Step: 5
Training loss: 2.753396511077881
Validation loss: 2.9102131525675454

Epoch: 6| Step: 6
Training loss: 3.3904852867126465
Validation loss: 2.9080647230148315

Epoch: 6| Step: 7
Training loss: 3.682828903198242
Validation loss: 2.904951532681783

Epoch: 6| Step: 8
Training loss: 2.7555322647094727
Validation loss: 2.901636004447937

Epoch: 6| Step: 9
Training loss: 2.918271780014038
Validation loss: 2.8986225922902427

Epoch: 6| Step: 10
Training loss: 2.917982578277588
Validation loss: 2.897671103477478

Epoch: 6| Step: 11
Training loss: 3.3051748275756836
Validation loss: 2.8926002979278564

Epoch: 6| Step: 12
Training loss: 3.0957210063934326
Validation loss: 2.8894808491071067

Epoch: 6| Step: 13
Training loss: 2.988239288330078
Validation loss: 2.8849039475123086

Epoch: 36| Step: 0
Training loss: 3.217705488204956
Validation loss: 2.884866237640381

Epoch: 6| Step: 1
Training loss: 3.8041253089904785
Validation loss: 2.880811015764872

Epoch: 6| Step: 2
Training loss: 2.910935878753662
Validation loss: 2.8755270640055337

Epoch: 6| Step: 3
Training loss: 2.697197437286377
Validation loss: 2.872920513153076

Epoch: 6| Step: 4
Training loss: 3.194253921508789
Validation loss: 2.8680440187454224

Epoch: 6| Step: 5
Training loss: 3.491291046142578
Validation loss: 2.86473282178243

Epoch: 6| Step: 6
Training loss: 2.9142017364501953
Validation loss: 2.861470580101013

Epoch: 6| Step: 7
Training loss: 3.341597080230713
Validation loss: 2.85839835802714

Epoch: 6| Step: 8
Training loss: 3.1729562282562256
Validation loss: 2.85473096370697

Epoch: 6| Step: 9
Training loss: 2.948284149169922
Validation loss: 2.8515751361846924

Epoch: 6| Step: 10
Training loss: 2.8623971939086914
Validation loss: 2.8476728598276773

Epoch: 6| Step: 11
Training loss: 3.395610809326172
Validation loss: 2.8457173903783164

Epoch: 6| Step: 12
Training loss: 2.4355709552764893
Validation loss: 2.846774101257324

Epoch: 6| Step: 13
Training loss: 2.548830032348633
Validation loss: 2.8687113523483276

Epoch: 37| Step: 0
Training loss: 2.6928634643554688
Validation loss: 2.8352869351704917

Epoch: 6| Step: 1
Training loss: 2.703108310699463
Validation loss: 2.832639535268148

Epoch: 6| Step: 2
Training loss: 3.1518654823303223
Validation loss: 2.8299863735834756

Epoch: 6| Step: 3
Training loss: 2.894580364227295
Validation loss: 2.827905058860779

Epoch: 6| Step: 4
Training loss: 3.035745620727539
Validation loss: 2.8257329066594443

Epoch: 6| Step: 5
Training loss: 3.40480899810791
Validation loss: 2.824339509010315

Epoch: 6| Step: 6
Training loss: 2.930149555206299
Validation loss: 2.8219565550486245

Epoch: 6| Step: 7
Training loss: 3.057070732116699
Validation loss: 2.8201690912246704

Epoch: 6| Step: 8
Training loss: 2.949298858642578
Validation loss: 2.8188283840815225

Epoch: 6| Step: 9
Training loss: 3.3728837966918945
Validation loss: 2.8208929300308228

Epoch: 6| Step: 10
Training loss: 2.5761873722076416
Validation loss: 2.8167048692703247

Epoch: 6| Step: 11
Training loss: 2.715325117111206
Validation loss: 2.809009552001953

Epoch: 6| Step: 12
Training loss: 3.778956890106201
Validation loss: 2.8056883215904236

Epoch: 6| Step: 13
Training loss: 3.115065574645996
Validation loss: 2.8018065690994263

Epoch: 38| Step: 0
Training loss: 2.1407346725463867
Validation loss: 2.7985750436782837

Epoch: 6| Step: 1
Training loss: 3.2879059314727783
Validation loss: 2.7940348784128823

Epoch: 6| Step: 2
Training loss: 3.3805713653564453
Validation loss: 2.7910317182540894

Epoch: 6| Step: 3
Training loss: 2.944075107574463
Validation loss: 2.7854663928349814

Epoch: 6| Step: 4
Training loss: 2.7301900386810303
Validation loss: 2.7808576424916587

Epoch: 6| Step: 5
Training loss: 2.953885555267334
Validation loss: 2.7788223028182983

Epoch: 6| Step: 6
Training loss: 3.53283429145813
Validation loss: 2.774871905644735

Epoch: 6| Step: 7
Training loss: 3.4392685890197754
Validation loss: 2.771434267361959

Epoch: 6| Step: 8
Training loss: 3.362884283065796
Validation loss: 2.769860108693441

Epoch: 6| Step: 9
Training loss: 2.998131036758423
Validation loss: 2.7666871150334678

Epoch: 6| Step: 10
Training loss: 3.0447442531585693
Validation loss: 2.7639485200246177

Epoch: 6| Step: 11
Training loss: 1.8797767162322998
Validation loss: 2.759526570638021

Epoch: 6| Step: 12
Training loss: 3.1849656105041504
Validation loss: 2.755787193775177

Epoch: 6| Step: 13
Training loss: 2.9147815704345703
Validation loss: 2.7514291206995645

Epoch: 39| Step: 0
Training loss: 3.215066432952881
Validation loss: 2.7483537197113037

Epoch: 6| Step: 1
Training loss: 3.9011597633361816
Validation loss: 2.7440588076909385

Epoch: 6| Step: 2
Training loss: 3.3087525367736816
Validation loss: 2.742212454477946

Epoch: 6| Step: 3
Training loss: 2.078108310699463
Validation loss: 2.7390294472376504

Epoch: 6| Step: 4
Training loss: 3.318103790283203
Validation loss: 2.7347012758255005

Epoch: 6| Step: 5
Training loss: 2.610231637954712
Validation loss: 2.7331549723943076

Epoch: 6| Step: 6
Training loss: 2.9293084144592285
Validation loss: 2.728438893953959

Epoch: 6| Step: 7
Training loss: 3.135493278503418
Validation loss: 2.725148836771647

Epoch: 6| Step: 8
Training loss: 3.055053234100342
Validation loss: 2.7227326234181723

Epoch: 6| Step: 9
Training loss: 3.2615966796875
Validation loss: 2.718790332476298

Epoch: 6| Step: 10
Training loss: 2.298872470855713
Validation loss: 2.7167886892954507

Epoch: 6| Step: 11
Training loss: 2.82965350151062
Validation loss: 2.713764707247416

Epoch: 6| Step: 12
Training loss: 2.351738929748535
Validation loss: 2.7102968295415244

Epoch: 6| Step: 13
Training loss: 2.8395309448242188
Validation loss: 2.7078320185343423

Epoch: 40| Step: 0
Training loss: 2.797856330871582
Validation loss: 2.704091231028239

Epoch: 6| Step: 1
Training loss: 3.018524169921875
Validation loss: 2.7014766534169516

Epoch: 6| Step: 2
Training loss: 2.9409680366516113
Validation loss: 2.7006032466888428

Epoch: 6| Step: 3
Training loss: 3.2196738719940186
Validation loss: 2.699554920196533

Epoch: 6| Step: 4
Training loss: 2.377441644668579
Validation loss: 2.7012317975362143

Epoch: 6| Step: 5
Training loss: 2.296787738800049
Validation loss: 2.69320547580719

Epoch: 6| Step: 6
Training loss: 3.3145997524261475
Validation loss: 2.6903801759084067

Epoch: 6| Step: 7
Training loss: 2.7567944526672363
Validation loss: 2.6860953172047934

Epoch: 6| Step: 8
Training loss: 2.8703970909118652
Validation loss: 2.6879650354385376

Epoch: 6| Step: 9
Training loss: 3.338552713394165
Validation loss: 2.6897793809572854

Epoch: 6| Step: 10
Training loss: 2.8707258701324463
Validation loss: 2.694679379463196

Epoch: 6| Step: 11
Training loss: 2.746065378189087
Validation loss: 2.6837119658788047

Epoch: 6| Step: 12
Training loss: 3.060344696044922
Validation loss: 2.6792505184809365

Epoch: 6| Step: 13
Training loss: 2.8955507278442383
Validation loss: 2.668148080507914

Epoch: 41| Step: 0
Training loss: 2.7860512733459473
Validation loss: 2.667642672856649

Epoch: 6| Step: 1
Training loss: 3.0403497219085693
Validation loss: 2.6716624895731607

Epoch: 6| Step: 2
Training loss: 2.5465922355651855
Validation loss: 2.669299284617106

Epoch: 6| Step: 3
Training loss: 2.8952736854553223
Validation loss: 2.6626371145248413

Epoch: 6| Step: 4
Training loss: 3.2023251056671143
Validation loss: 2.6570984919865928

Epoch: 6| Step: 5
Training loss: 2.84954833984375
Validation loss: 2.6523902813593545

Epoch: 6| Step: 6
Training loss: 3.161149740219116
Validation loss: 2.6487695376078286

Epoch: 6| Step: 7
Training loss: 2.7464828491210938
Validation loss: 2.6463754971822104

Epoch: 6| Step: 8
Training loss: 2.5573887825012207
Validation loss: 2.6410417556762695

Epoch: 6| Step: 9
Training loss: 2.953382968902588
Validation loss: 2.638036370277405

Epoch: 6| Step: 10
Training loss: 2.6767022609710693
Validation loss: 2.6378695964813232

Epoch: 6| Step: 11
Training loss: 2.801215648651123
Validation loss: 2.633107582728068

Epoch: 6| Step: 12
Training loss: 3.0588369369506836
Validation loss: 2.6331952015558877

Epoch: 6| Step: 13
Training loss: 2.653475046157837
Validation loss: 2.6292863289515176

Epoch: 42| Step: 0
Training loss: 3.3416190147399902
Validation loss: 2.628916164239248

Epoch: 6| Step: 1
Training loss: 2.693668842315674
Validation loss: 2.619860808054606

Epoch: 6| Step: 2
Training loss: 2.613373041152954
Validation loss: 2.6172217528025308

Epoch: 6| Step: 3
Training loss: 2.3561670780181885
Validation loss: 2.614400068918864

Epoch: 6| Step: 4
Training loss: 2.920266628265381
Validation loss: 2.6111270586649575

Epoch: 6| Step: 5
Training loss: 3.0166451930999756
Validation loss: 2.609152873357137

Epoch: 6| Step: 6
Training loss: 2.447939872741699
Validation loss: 2.6052329937616983

Epoch: 6| Step: 7
Training loss: 2.3492352962493896
Validation loss: 2.6038381656010947

Epoch: 6| Step: 8
Training loss: 2.9619293212890625
Validation loss: 2.6022809743881226

Epoch: 6| Step: 9
Training loss: 3.9352757930755615
Validation loss: 2.6015745202700296

Epoch: 6| Step: 10
Training loss: 3.0530202388763428
Validation loss: 2.5977428754170737

Epoch: 6| Step: 11
Training loss: 3.1126694679260254
Validation loss: 2.592656890551249

Epoch: 6| Step: 12
Training loss: 2.296281099319458
Validation loss: 2.585873325665792

Epoch: 6| Step: 13
Training loss: 2.1939148902893066
Validation loss: 2.5831259886423745

Epoch: 43| Step: 0
Training loss: 2.4639575481414795
Validation loss: 2.580989201863607

Epoch: 6| Step: 1
Training loss: 3.0382962226867676
Validation loss: 2.577666680018107

Epoch: 6| Step: 2
Training loss: 2.651099681854248
Validation loss: 2.5740561882654824

Epoch: 6| Step: 3
Training loss: 3.0782923698425293
Validation loss: 2.5706863602002463

Epoch: 6| Step: 4
Training loss: 2.0182790756225586
Validation loss: 2.56931209564209

Epoch: 6| Step: 5
Training loss: 2.872570514678955
Validation loss: 2.5643602212270102

Epoch: 6| Step: 6
Training loss: 3.6675662994384766
Validation loss: 2.5617859760920205

Epoch: 6| Step: 7
Training loss: 3.5528101921081543
Validation loss: 2.5580597718556723

Epoch: 6| Step: 8
Training loss: 3.248399496078491
Validation loss: 2.556035498778025

Epoch: 6| Step: 9
Training loss: 2.716752767562866
Validation loss: 2.5515282352765403

Epoch: 6| Step: 10
Training loss: 1.9755779504776
Validation loss: 2.552609841028849

Epoch: 6| Step: 11
Training loss: 2.4674277305603027
Validation loss: 2.5495922565460205

Epoch: 6| Step: 12
Training loss: 2.1526968479156494
Validation loss: 2.550163825352987

Epoch: 6| Step: 13
Training loss: 2.717106819152832
Validation loss: 2.5492237408955893

Epoch: 44| Step: 0
Training loss: 2.2135772705078125
Validation loss: 2.54280952612559

Epoch: 6| Step: 1
Training loss: 2.2960281372070312
Validation loss: 2.5465683539708457

Epoch: 6| Step: 2
Training loss: 2.639626979827881
Validation loss: 2.5441489219665527

Epoch: 6| Step: 3
Training loss: 2.9328866004943848
Validation loss: 2.537610570589701

Epoch: 6| Step: 4
Training loss: 3.2321512699127197
Validation loss: 2.529328385988871

Epoch: 6| Step: 5
Training loss: 2.539661407470703
Validation loss: 2.526962081591288

Epoch: 6| Step: 6
Training loss: 2.544869899749756
Validation loss: 2.5285213788350425

Epoch: 6| Step: 7
Training loss: 2.622199296951294
Validation loss: 2.5335221687952676

Epoch: 6| Step: 8
Training loss: 2.6606431007385254
Validation loss: 2.5240029295285544

Epoch: 6| Step: 9
Training loss: 2.9851434230804443
Validation loss: 2.522152225176493

Epoch: 6| Step: 10
Training loss: 2.313420057296753
Validation loss: 2.520336310068766

Epoch: 6| Step: 11
Training loss: 2.9183096885681152
Validation loss: 2.5138874451319375

Epoch: 6| Step: 12
Training loss: 3.392488956451416
Validation loss: 2.511187116305033

Epoch: 6| Step: 13
Training loss: 2.8659772872924805
Validation loss: 2.5082566340764365

Epoch: 45| Step: 0
Training loss: 2.8047120571136475
Validation loss: 2.5036895672480264

Epoch: 6| Step: 1
Training loss: 2.71627140045166
Validation loss: 2.5033488273620605

Epoch: 6| Step: 2
Training loss: 2.134470224380493
Validation loss: 2.509917418162028

Epoch: 6| Step: 3
Training loss: 2.635951042175293
Validation loss: 2.494220733642578

Epoch: 6| Step: 4
Training loss: 3.0809741020202637
Validation loss: 2.4884493748346963

Epoch: 6| Step: 5
Training loss: 2.799788475036621
Validation loss: 2.491188923517863

Epoch: 6| Step: 6
Training loss: 2.717332124710083
Validation loss: 2.488566795984904

Epoch: 6| Step: 7
Training loss: 3.0219202041625977
Validation loss: 2.4899590412775674

Epoch: 6| Step: 8
Training loss: 2.933236837387085
Validation loss: 2.4901768366495767

Epoch: 6| Step: 9
Training loss: 2.796302080154419
Validation loss: 2.485647956530253

Epoch: 6| Step: 10
Training loss: 3.1331984996795654
Validation loss: 2.481223483880361

Epoch: 6| Step: 11
Training loss: 2.0140128135681152
Validation loss: 2.4798530737559

Epoch: 6| Step: 12
Training loss: 2.6277174949645996
Validation loss: 2.4783561627070108

Epoch: 6| Step: 13
Training loss: 2.241509437561035
Validation loss: 2.475610335667928

Epoch: 46| Step: 0
Training loss: 3.0000123977661133
Validation loss: 2.4734682043393454

Epoch: 6| Step: 1
Training loss: 2.15622615814209
Validation loss: 2.470925251642863

Epoch: 6| Step: 2
Training loss: 2.5682430267333984
Validation loss: 2.4653389851252236

Epoch: 6| Step: 3
Training loss: 3.30958890914917
Validation loss: 2.4627633492151895

Epoch: 6| Step: 4
Training loss: 2.4129114151000977
Validation loss: 2.4621729850769043

Epoch: 6| Step: 5
Training loss: 1.760193109512329
Validation loss: 2.4596006671587625

Epoch: 6| Step: 6
Training loss: 2.599400043487549
Validation loss: 2.45571239789327

Epoch: 6| Step: 7
Training loss: 2.5325472354888916
Validation loss: 2.450480659802755

Epoch: 6| Step: 8
Training loss: 2.6840434074401855
Validation loss: 2.4517802397410073

Epoch: 6| Step: 9
Training loss: 2.3817944526672363
Validation loss: 2.448335846265157

Epoch: 6| Step: 10
Training loss: 3.858309507369995
Validation loss: 2.4495253960291543

Epoch: 6| Step: 11
Training loss: 2.412613868713379
Validation loss: 2.442133903503418

Epoch: 6| Step: 12
Training loss: 2.4116058349609375
Validation loss: 2.4435126980145774

Epoch: 6| Step: 13
Training loss: 2.95682954788208
Validation loss: 2.43737139304479

Epoch: 47| Step: 0
Training loss: 2.389993190765381
Validation loss: 2.4340364933013916

Epoch: 6| Step: 1
Training loss: 2.9889743328094482
Validation loss: 2.4319430589675903

Epoch: 6| Step: 2
Training loss: 3.0954818725585938
Validation loss: 2.433691064516703

Epoch: 6| Step: 3
Training loss: 2.710930824279785
Validation loss: 2.4314802090326944

Epoch: 6| Step: 4
Training loss: 2.5479471683502197
Validation loss: 2.424624423185984

Epoch: 6| Step: 5
Training loss: 2.508246421813965
Validation loss: 2.420477827390035

Epoch: 6| Step: 6
Training loss: 2.5409512519836426
Validation loss: 2.415745417277018

Epoch: 6| Step: 7
Training loss: 2.7557482719421387
Validation loss: 2.415137827396393

Epoch: 6| Step: 8
Training loss: 3.0855250358581543
Validation loss: 2.4098475575447083

Epoch: 6| Step: 9
Training loss: 2.6746788024902344
Validation loss: 2.4076455434163413

Epoch: 6| Step: 10
Training loss: 2.426032781600952
Validation loss: 2.4073630571365356

Epoch: 6| Step: 11
Training loss: 2.1655938625335693
Validation loss: 2.4042927821477256

Epoch: 6| Step: 12
Training loss: 2.206700086593628
Validation loss: 2.4008814493815103

Epoch: 6| Step: 13
Training loss: 2.386547327041626
Validation loss: 2.40078604221344

Epoch: 48| Step: 0
Training loss: 2.3483564853668213
Validation loss: 2.4014433224995932

Epoch: 6| Step: 1
Training loss: 2.8039040565490723
Validation loss: 2.399181524912516

Epoch: 6| Step: 2
Training loss: 2.414809226989746
Validation loss: 2.4044726888338723

Epoch: 6| Step: 3
Training loss: 2.22992205619812
Validation loss: 2.4032448132832847

Epoch: 6| Step: 4
Training loss: 2.739600658416748
Validation loss: 2.401293079058329

Epoch: 6| Step: 5
Training loss: 2.525824546813965
Validation loss: 2.393083135286967

Epoch: 6| Step: 6
Training loss: 1.9227409362792969
Validation loss: 2.392040948073069

Epoch: 6| Step: 7
Training loss: 2.60268497467041
Validation loss: 2.3843575716018677

Epoch: 6| Step: 8
Training loss: 2.9009203910827637
Validation loss: 2.377360383669535

Epoch: 6| Step: 9
Training loss: 2.70927357673645
Validation loss: 2.3828124602635703

Epoch: 6| Step: 10
Training loss: 2.3965110778808594
Validation loss: 2.385282516479492

Epoch: 6| Step: 11
Training loss: 3.25352144241333
Validation loss: 2.3686742186546326

Epoch: 6| Step: 12
Training loss: 2.4222447872161865
Validation loss: 2.3708118995030723

Epoch: 6| Step: 13
Training loss: 2.662102699279785
Validation loss: 2.3668768405914307

Epoch: 49| Step: 0
Training loss: 2.7172834873199463
Validation loss: 2.3682402769724527

Epoch: 6| Step: 1
Training loss: 2.30000638961792
Validation loss: 2.3667412996292114

Epoch: 6| Step: 2
Training loss: 2.0762245655059814
Validation loss: 2.3664944569269815

Epoch: 6| Step: 3
Training loss: 2.607633113861084
Validation loss: 2.3619133035341897

Epoch: 6| Step: 4
Training loss: 2.8779070377349854
Validation loss: 2.3608586192131042

Epoch: 6| Step: 5
Training loss: 2.3505570888519287
Validation loss: 2.3584458033243814

Epoch: 6| Step: 6
Training loss: 2.564943313598633
Validation loss: 2.35664826631546

Epoch: 6| Step: 7
Training loss: 2.2417945861816406
Validation loss: 2.354412853717804

Epoch: 6| Step: 8
Training loss: 2.305894136428833
Validation loss: 2.353625257809957

Epoch: 6| Step: 9
Training loss: 3.2569422721862793
Validation loss: 2.3518025279045105

Epoch: 6| Step: 10
Training loss: 2.7051851749420166
Validation loss: 2.3467260599136353

Epoch: 6| Step: 11
Training loss: 2.228090763092041
Validation loss: 2.345868388811747

Epoch: 6| Step: 12
Training loss: 2.549941301345825
Validation loss: 2.341630975405375

Epoch: 6| Step: 13
Training loss: 2.6820640563964844
Validation loss: 2.3357775608698526

Epoch: 50| Step: 0
Training loss: 2.339390754699707
Validation loss: 2.3339858055114746

Epoch: 6| Step: 1
Training loss: 2.500875949859619
Validation loss: 2.333878298600515

Epoch: 6| Step: 2
Training loss: 1.9777814149856567
Validation loss: 2.332869052886963

Epoch: 6| Step: 3
Training loss: 2.209031581878662
Validation loss: 2.336808681488037

Epoch: 6| Step: 4
Training loss: 2.7686173915863037
Validation loss: 2.3410817782084146

Epoch: 6| Step: 5
Training loss: 2.725621223449707
Validation loss: 2.325203994909922

Epoch: 6| Step: 6
Training loss: 2.610459804534912
Validation loss: 2.326180617014567

Epoch: 6| Step: 7
Training loss: 2.9581170082092285
Validation loss: 2.3217053016026816

Epoch: 6| Step: 8
Training loss: 2.6620945930480957
Validation loss: 2.320559104283651

Epoch: 6| Step: 9
Training loss: 2.2977075576782227
Validation loss: 2.318975110848745

Epoch: 6| Step: 10
Training loss: 1.9688340425491333
Validation loss: 2.3188327153523765

Epoch: 6| Step: 11
Training loss: 2.708272933959961
Validation loss: 2.317525585492452

Epoch: 6| Step: 12
Training loss: 2.3362107276916504
Validation loss: 2.3202101985613504

Epoch: 6| Step: 13
Training loss: 2.832127809524536
Validation loss: 2.3186221917470298

Epoch: 51| Step: 0
Training loss: 2.718690872192383
Validation loss: 2.315853695074717

Epoch: 6| Step: 1
Training loss: 2.4920454025268555
Validation loss: 2.313308537006378

Epoch: 6| Step: 2
Training loss: 2.5316715240478516
Validation loss: 2.3103259007136026

Epoch: 6| Step: 3
Training loss: 2.68947696685791
Validation loss: 2.3071128924687705

Epoch: 6| Step: 4
Training loss: 2.3634719848632812
Validation loss: 2.305068612098694

Epoch: 6| Step: 5
Training loss: 2.0638227462768555
Validation loss: 2.2986861864725747

Epoch: 6| Step: 6
Training loss: 2.5706124305725098
Validation loss: 2.2976120511690774

Epoch: 6| Step: 7
Training loss: 2.8485968112945557
Validation loss: 2.29240616162618

Epoch: 6| Step: 8
Training loss: 2.807849884033203
Validation loss: 2.2941182454427085

Epoch: 6| Step: 9
Training loss: 2.3472747802734375
Validation loss: 2.2899577220280967

Epoch: 6| Step: 10
Training loss: 2.4469070434570312
Validation loss: 2.288972099622091

Epoch: 6| Step: 11
Training loss: 2.2001800537109375
Validation loss: 2.286722640196482

Epoch: 6| Step: 12
Training loss: 1.9192490577697754
Validation loss: 2.287862539291382

Epoch: 6| Step: 13
Training loss: 2.5672824382781982
Validation loss: 2.28311417500178

Epoch: 52| Step: 0
Training loss: 2.309086561203003
Validation loss: 2.278802831967672

Epoch: 6| Step: 1
Training loss: 2.3185651302337646
Validation loss: 2.282279849052429

Epoch: 6| Step: 2
Training loss: 2.3264598846435547
Validation loss: 2.2786739667256675

Epoch: 6| Step: 3
Training loss: 3.110496997833252
Validation loss: 2.2732536792755127

Epoch: 6| Step: 4
Training loss: 2.24110746383667
Validation loss: 2.2683416604995728

Epoch: 6| Step: 5
Training loss: 2.3286008834838867
Validation loss: 2.27202037970225

Epoch: 6| Step: 6
Training loss: 2.291483163833618
Validation loss: 2.264422595500946

Epoch: 6| Step: 7
Training loss: 2.232469320297241
Validation loss: 2.2642572124799094

Epoch: 6| Step: 8
Training loss: 2.7503061294555664
Validation loss: 2.257407307624817

Epoch: 6| Step: 9
Training loss: 2.548704147338867
Validation loss: 2.2598683834075928

Epoch: 6| Step: 10
Training loss: 2.3581032752990723
Validation loss: 2.2581948041915894

Epoch: 6| Step: 11
Training loss: 2.620103120803833
Validation loss: 2.2549097736676535

Epoch: 6| Step: 12
Training loss: 1.92250657081604
Validation loss: 2.25258199373881

Epoch: 6| Step: 13
Training loss: 2.6388564109802246
Validation loss: 2.2479288975397744

Epoch: 53| Step: 0
Training loss: 2.430030584335327
Validation loss: 2.25181249777476

Epoch: 6| Step: 1
Training loss: 2.496950149536133
Validation loss: 2.245540976524353

Epoch: 6| Step: 2
Training loss: 3.065223217010498
Validation loss: 2.2438409328460693

Epoch: 6| Step: 3
Training loss: 1.9694406986236572
Validation loss: 2.2430824637413025

Epoch: 6| Step: 4
Training loss: 2.724785327911377
Validation loss: 2.2386265198389688

Epoch: 6| Step: 5
Training loss: 3.0486929416656494
Validation loss: 2.2389939427375793

Epoch: 6| Step: 6
Training loss: 2.2392258644104004
Validation loss: 2.2352221409479776

Epoch: 6| Step: 7
Training loss: 2.4447531700134277
Validation loss: 2.2296305894851685

Epoch: 6| Step: 8
Training loss: 1.959761142730713
Validation loss: 2.2226634422938027

Epoch: 6| Step: 9
Training loss: 2.0927414894104004
Validation loss: 2.2298582990964255

Epoch: 6| Step: 10
Training loss: 1.9576783180236816
Validation loss: 2.2368429700533548

Epoch: 6| Step: 11
Training loss: 1.9200685024261475
Validation loss: 2.227225144704183

Epoch: 6| Step: 12
Training loss: 2.7564356327056885
Validation loss: 2.224465330441793

Epoch: 6| Step: 13
Training loss: 2.4425015449523926
Validation loss: 2.2208159367243447

Epoch: 54| Step: 0
Training loss: 2.4676568508148193
Validation loss: 2.2270034154256186

Epoch: 6| Step: 1
Training loss: 2.8440496921539307
Validation loss: 2.221567749977112

Epoch: 6| Step: 2
Training loss: 1.8548941612243652
Validation loss: 2.219695230325063

Epoch: 6| Step: 3
Training loss: 2.740096092224121
Validation loss: 2.2166003386179605

Epoch: 6| Step: 4
Training loss: 2.821960210800171
Validation loss: 2.2195711930592856

Epoch: 6| Step: 5
Training loss: 2.3670713901519775
Validation loss: 2.222352147102356

Epoch: 6| Step: 6
Training loss: 1.917901873588562
Validation loss: 2.2219483057657876

Epoch: 6| Step: 7
Training loss: 2.4984402656555176
Validation loss: 2.2177289923032126

Epoch: 6| Step: 8
Training loss: 2.4459586143493652
Validation loss: 2.2182425260543823

Epoch: 6| Step: 9
Training loss: 1.9251735210418701
Validation loss: 2.215995748837789

Epoch: 6| Step: 10
Training loss: 2.2243313789367676
Validation loss: 2.2124697168668113

Epoch: 6| Step: 11
Training loss: 3.0179834365844727
Validation loss: 2.2138176560401917

Epoch: 6| Step: 12
Training loss: 2.2202396392822266
Validation loss: 2.2090981801350913

Epoch: 6| Step: 13
Training loss: 1.9165693521499634
Validation loss: 2.208165725072225

Epoch: 55| Step: 0
Training loss: 2.03585147857666
Validation loss: 2.207293967405955

Epoch: 6| Step: 1
Training loss: 2.593083381652832
Validation loss: 2.205226182937622

Epoch: 6| Step: 2
Training loss: 2.6887640953063965
Validation loss: 2.2025603652000427

Epoch: 6| Step: 3
Training loss: 1.9916630983352661
Validation loss: 2.205910603205363

Epoch: 6| Step: 4
Training loss: 2.9259543418884277
Validation loss: 2.200616478919983

Epoch: 6| Step: 5
Training loss: 2.7779922485351562
Validation loss: 2.1966020663579306

Epoch: 6| Step: 6
Training loss: 2.8038859367370605
Validation loss: 2.1960121393203735

Epoch: 6| Step: 7
Training loss: 2.239860773086548
Validation loss: 2.1941978931427

Epoch: 6| Step: 8
Training loss: 1.6094809770584106
Validation loss: 2.1895047227541604

Epoch: 6| Step: 9
Training loss: 2.436190128326416
Validation loss: 2.185288747151693

Epoch: 6| Step: 10
Training loss: 2.335299015045166
Validation loss: 2.1907681226730347

Epoch: 6| Step: 11
Training loss: 2.4513654708862305
Validation loss: 2.189104437828064

Epoch: 6| Step: 12
Training loss: 1.96023690700531
Validation loss: 2.198328177134196

Epoch: 6| Step: 13
Training loss: 2.105616569519043
Validation loss: 2.1975152095158896

Epoch: 56| Step: 0
Training loss: 2.2273502349853516
Validation loss: 2.1941262682278952

Epoch: 6| Step: 1
Training loss: 2.2856240272521973
Validation loss: 2.194147308667501

Epoch: 6| Step: 2
Training loss: 1.6928048133850098
Validation loss: 2.19484011332194

Epoch: 6| Step: 3
Training loss: 2.822223663330078
Validation loss: 2.182927350203196

Epoch: 6| Step: 4
Training loss: 2.361360549926758
Validation loss: 2.186606526374817

Epoch: 6| Step: 5
Training loss: 2.230649948120117
Validation loss: 2.1854260762532554

Epoch: 6| Step: 6
Training loss: 2.4541568756103516
Validation loss: 2.1827298998832703

Epoch: 6| Step: 7
Training loss: 2.283535957336426
Validation loss: 2.181776245435079

Epoch: 6| Step: 8
Training loss: 1.9816477298736572
Validation loss: 2.180707355340322

Epoch: 6| Step: 9
Training loss: 2.607520580291748
Validation loss: 2.181511958440145

Epoch: 6| Step: 10
Training loss: 2.8286798000335693
Validation loss: 2.183879574139913

Epoch: 6| Step: 11
Training loss: 2.4806742668151855
Validation loss: 2.176930069923401

Epoch: 6| Step: 12
Training loss: 1.635581612586975
Validation loss: 2.173498352368673

Epoch: 6| Step: 13
Training loss: 2.9127612113952637
Validation loss: 2.177159309387207

Epoch: 57| Step: 0
Training loss: 2.657623767852783
Validation loss: 2.1815027594566345

Epoch: 6| Step: 1
Training loss: 2.2091872692108154
Validation loss: 2.179876744747162

Epoch: 6| Step: 2
Training loss: 2.697103500366211
Validation loss: 2.1828338305155435

Epoch: 6| Step: 3
Training loss: 1.7633793354034424
Validation loss: 2.178271015485128

Epoch: 6| Step: 4
Training loss: 2.4874441623687744
Validation loss: 2.1764773527781167

Epoch: 6| Step: 5
Training loss: 1.4611443281173706
Validation loss: 2.180867354075114

Epoch: 6| Step: 6
Training loss: 1.9814438819885254
Validation loss: 2.1794133385022483

Epoch: 6| Step: 7
Training loss: 2.447596549987793
Validation loss: 2.176700750986735

Epoch: 6| Step: 8
Training loss: 2.2099857330322266
Validation loss: 2.171452005704244

Epoch: 6| Step: 9
Training loss: 2.8701508045196533
Validation loss: 2.1709094047546387

Epoch: 6| Step: 10
Training loss: 2.899930477142334
Validation loss: 2.1616114377975464

Epoch: 6| Step: 11
Training loss: 2.2868261337280273
Validation loss: 2.154356896877289

Epoch: 6| Step: 12
Training loss: 2.3668899536132812
Validation loss: 2.148163119951884

Epoch: 6| Step: 13
Training loss: 2.222148895263672
Validation loss: 2.1444134513537088

Epoch: 58| Step: 0
Training loss: 2.8534889221191406
Validation loss: 2.142944057782491

Epoch: 6| Step: 1
Training loss: 2.5952157974243164
Validation loss: 2.146644949913025

Epoch: 6| Step: 2
Training loss: 2.3076577186584473
Validation loss: 2.1420236031214395

Epoch: 6| Step: 3
Training loss: 2.6130449771881104
Validation loss: 2.1451814572016397

Epoch: 6| Step: 4
Training loss: 2.3511664867401123
Validation loss: 2.150675813357035

Epoch: 6| Step: 5
Training loss: 2.9107532501220703
Validation loss: 2.1497424046198526

Epoch: 6| Step: 6
Training loss: 2.2836499214172363
Validation loss: 2.152331511179606

Epoch: 6| Step: 7
Training loss: 2.661339521408081
Validation loss: 2.1432458559672036

Epoch: 6| Step: 8
Training loss: 2.0305681228637695
Validation loss: 2.1431312759717307

Epoch: 6| Step: 9
Training loss: 2.2257518768310547
Validation loss: 2.129955987135569

Epoch: 6| Step: 10
Training loss: 1.594923973083496
Validation loss: 2.1288708448410034

Epoch: 6| Step: 11
Training loss: 1.8043135404586792
Validation loss: 2.127918561299642

Epoch: 6| Step: 12
Training loss: 1.7701746225357056
Validation loss: 2.1238876382509866

Epoch: 6| Step: 13
Training loss: 2.34000825881958
Validation loss: 2.121241013209025

Epoch: 59| Step: 0
Training loss: 2.122361898422241
Validation loss: 2.120142122109731

Epoch: 6| Step: 1
Training loss: 2.413213014602661
Validation loss: 2.123907208442688

Epoch: 6| Step: 2
Training loss: 2.081916332244873
Validation loss: 2.118670483430227

Epoch: 6| Step: 3
Training loss: 2.6311635971069336
Validation loss: 2.11971378326416

Epoch: 6| Step: 4
Training loss: 2.6097092628479004
Validation loss: 2.118096431096395

Epoch: 6| Step: 5
Training loss: 2.4187679290771484
Validation loss: 2.1171252926190696

Epoch: 6| Step: 6
Training loss: 2.5740065574645996
Validation loss: 2.1165796518325806

Epoch: 6| Step: 7
Training loss: 2.169621706008911
Validation loss: 2.1256845196088157

Epoch: 6| Step: 8
Training loss: 1.7959727048873901
Validation loss: 2.141172687212626

Epoch: 6| Step: 9
Training loss: 1.9536192417144775
Validation loss: 2.132477958997091

Epoch: 6| Step: 10
Training loss: 2.4750289916992188
Validation loss: 2.1267336209615073

Epoch: 6| Step: 11
Training loss: 2.1053812503814697
Validation loss: 2.12150639295578

Epoch: 6| Step: 12
Training loss: 2.144423723220825
Validation loss: 2.1167942682902017

Epoch: 6| Step: 13
Training loss: 2.6667656898498535
Validation loss: 2.1307064096132913

Epoch: 60| Step: 0
Training loss: 2.009664297103882
Validation loss: 2.125940461953481

Epoch: 6| Step: 1
Training loss: 2.2702722549438477
Validation loss: 2.1344674825668335

Epoch: 6| Step: 2
Training loss: 1.924921989440918
Validation loss: 2.1374974648157754

Epoch: 6| Step: 3
Training loss: 2.535768508911133
Validation loss: 2.1393499175707498

Epoch: 6| Step: 4
Training loss: 2.140213966369629
Validation loss: 2.1414246559143066

Epoch: 6| Step: 5
Training loss: 2.7927980422973633
Validation loss: 2.136664072672526

Epoch: 6| Step: 6
Training loss: 2.152982234954834
Validation loss: 2.1317239801088967

Epoch: 6| Step: 7
Training loss: 2.786867618560791
Validation loss: 2.1287445227305093

Epoch: 6| Step: 8
Training loss: 2.0409834384918213
Validation loss: 2.1190889477729797

Epoch: 6| Step: 9
Training loss: 2.5247347354888916
Validation loss: 2.114395340283712

Epoch: 6| Step: 10
Training loss: 1.8684237003326416
Validation loss: 2.1149220069249473

Epoch: 6| Step: 11
Training loss: 2.5736351013183594
Validation loss: 2.111519773801168

Epoch: 6| Step: 12
Training loss: 2.15409255027771
Validation loss: 2.108691910902659

Epoch: 6| Step: 13
Training loss: 2.2524118423461914
Validation loss: 2.1142820715904236

Epoch: 61| Step: 0
Training loss: 2.181666851043701
Validation loss: 2.123176018397013

Epoch: 6| Step: 1
Training loss: 1.989174723625183
Validation loss: 2.121827562650045

Epoch: 6| Step: 2
Training loss: 2.6938579082489014
Validation loss: 2.1024770935376487

Epoch: 6| Step: 3
Training loss: 2.3773412704467773
Validation loss: 2.093788762887319

Epoch: 6| Step: 4
Training loss: 2.54374361038208
Validation loss: 2.091625432173411

Epoch: 6| Step: 5
Training loss: 2.3661422729492188
Validation loss: 2.0942554672559104

Epoch: 6| Step: 6
Training loss: 2.429035186767578
Validation loss: 2.092043618361155

Epoch: 6| Step: 7
Training loss: 2.338233232498169
Validation loss: 2.092393080393473

Epoch: 6| Step: 8
Training loss: 1.8679699897766113
Validation loss: 2.090166767438253

Epoch: 6| Step: 9
Training loss: 2.8477625846862793
Validation loss: 2.088379522164663

Epoch: 6| Step: 10
Training loss: 2.2913119792938232
Validation loss: 2.0919781923294067

Epoch: 6| Step: 11
Training loss: 2.268224000930786
Validation loss: 2.085956553618113

Epoch: 6| Step: 12
Training loss: 1.2675567865371704
Validation loss: 2.0895031094551086

Epoch: 6| Step: 13
Training loss: 2.4060170650482178
Validation loss: 2.09028689066569

Epoch: 62| Step: 0
Training loss: 2.6931939125061035
Validation loss: 2.0993280609448752

Epoch: 6| Step: 1
Training loss: 2.3227181434631348
Validation loss: 2.098689317703247

Epoch: 6| Step: 2
Training loss: 2.5978221893310547
Validation loss: 2.092784106731415

Epoch: 6| Step: 3
Training loss: 1.9363811016082764
Validation loss: 2.084879994392395

Epoch: 6| Step: 4
Training loss: 2.2740063667297363
Validation loss: 2.088468690713247

Epoch: 6| Step: 5
Training loss: 2.257068157196045
Validation loss: 2.0880669355392456

Epoch: 6| Step: 6
Training loss: 1.979211449623108
Validation loss: 2.0853883822758994

Epoch: 6| Step: 7
Training loss: 2.5960917472839355
Validation loss: 2.089187582333883

Epoch: 6| Step: 8
Training loss: 2.0470738410949707
Validation loss: 2.086486260096232

Epoch: 6| Step: 9
Training loss: 1.7958427667617798
Validation loss: 2.0836363633473716

Epoch: 6| Step: 10
Training loss: 2.5780398845672607
Validation loss: 2.0809230407079062

Epoch: 6| Step: 11
Training loss: 2.018202781677246
Validation loss: 2.0676408807436624

Epoch: 6| Step: 12
Training loss: 2.534597873687744
Validation loss: 2.081607699394226

Epoch: 6| Step: 13
Training loss: 2.262443780899048
Validation loss: 2.0703226129213967

Epoch: 63| Step: 0
Training loss: 2.6026320457458496
Validation loss: 2.075104812781016

Epoch: 6| Step: 1
Training loss: 1.767859935760498
Validation loss: 2.0697962840398154

Epoch: 6| Step: 2
Training loss: 2.1830670833587646
Validation loss: 2.067950745423635

Epoch: 6| Step: 3
Training loss: 2.2140676975250244
Validation loss: 2.069147209326426

Epoch: 6| Step: 4
Training loss: 1.9978071451187134
Validation loss: 2.0639805992444358

Epoch: 6| Step: 5
Training loss: 1.8912353515625
Validation loss: 2.0754560033480325

Epoch: 6| Step: 6
Training loss: 2.4095816612243652
Validation loss: 2.0717647274335227

Epoch: 6| Step: 7
Training loss: 1.6140042543411255
Validation loss: 2.06363312403361

Epoch: 6| Step: 8
Training loss: 2.2580533027648926
Validation loss: 2.074520925680796

Epoch: 6| Step: 9
Training loss: 2.793747901916504
Validation loss: 2.0954204201698303

Epoch: 6| Step: 10
Training loss: 2.4931108951568604
Validation loss: 2.131284793217977

Epoch: 6| Step: 11
Training loss: 3.1018989086151123
Validation loss: 2.108938157558441

Epoch: 6| Step: 12
Training loss: 1.7970479726791382
Validation loss: 2.120802640914917

Epoch: 6| Step: 13
Training loss: 2.6452553272247314
Validation loss: 2.1072720289230347

Epoch: 64| Step: 0
Training loss: 2.004215955734253
Validation loss: 2.0867203871409097

Epoch: 6| Step: 1
Training loss: 2.4007058143615723
Validation loss: 2.080611983935038

Epoch: 6| Step: 2
Training loss: 2.0831868648529053
Validation loss: 2.075793961683909

Epoch: 6| Step: 3
Training loss: 2.2699451446533203
Validation loss: 2.0566731095314026

Epoch: 6| Step: 4
Training loss: 2.2340903282165527
Validation loss: 2.0589258869489035

Epoch: 6| Step: 5
Training loss: 2.339320659637451
Validation loss: 2.0638096729914346

Epoch: 6| Step: 6
Training loss: 2.3425445556640625
Validation loss: 2.0707868337631226

Epoch: 6| Step: 7
Training loss: 2.4042789936065674
Validation loss: 2.0700066486994424

Epoch: 6| Step: 8
Training loss: 1.7528021335601807
Validation loss: 2.0747719605763755

Epoch: 6| Step: 9
Training loss: 2.5239977836608887
Validation loss: 2.076805075009664

Epoch: 6| Step: 10
Training loss: 2.327812433242798
Validation loss: 2.078203777472178

Epoch: 6| Step: 11
Training loss: 2.227886199951172
Validation loss: 2.0794134537378945

Epoch: 6| Step: 12
Training loss: 2.5259628295898438
Validation loss: 2.0780104796091714

Epoch: 6| Step: 13
Training loss: 1.9189863204956055
Validation loss: 2.077964464823405

Epoch: 65| Step: 0
Training loss: 2.5731847286224365
Validation loss: 2.0690713127454123

Epoch: 6| Step: 1
Training loss: 2.3336479663848877
Validation loss: 2.06271094083786

Epoch: 6| Step: 2
Training loss: 2.320622444152832
Validation loss: 2.068109929561615

Epoch: 6| Step: 3
Training loss: 2.020369052886963
Validation loss: 2.0593750278155007

Epoch: 6| Step: 4
Training loss: 1.8700062036514282
Validation loss: 2.0599873065948486

Epoch: 6| Step: 5
Training loss: 2.467867374420166
Validation loss: 2.0643703738848367

Epoch: 6| Step: 6
Training loss: 2.2218918800354004
Validation loss: 2.0574140350023904

Epoch: 6| Step: 7
Training loss: 2.283567428588867
Validation loss: 2.0579621394475303

Epoch: 6| Step: 8
Training loss: 2.416527271270752
Validation loss: 2.0503623684247336

Epoch: 6| Step: 9
Training loss: 2.8769636154174805
Validation loss: 2.050054669380188

Epoch: 6| Step: 10
Training loss: 1.90293288230896
Validation loss: 2.0548808177312217

Epoch: 6| Step: 11
Training loss: 1.91554856300354
Validation loss: 2.049705525239309

Epoch: 6| Step: 12
Training loss: 1.9240610599517822
Validation loss: 2.04567281405131

Epoch: 6| Step: 13
Training loss: 2.0799124240875244
Validation loss: 2.0437554915746055

Epoch: 66| Step: 0
Training loss: 1.9935628175735474
Validation loss: 2.04632959763209

Epoch: 6| Step: 1
Training loss: 2.777613401412964
Validation loss: 2.035851001739502

Epoch: 6| Step: 2
Training loss: 2.185029983520508
Validation loss: 2.045171876748403

Epoch: 6| Step: 3
Training loss: 1.885141134262085
Validation loss: 2.0339253743489585

Epoch: 6| Step: 4
Training loss: 2.4375970363616943
Validation loss: 2.0368780493736267

Epoch: 6| Step: 5
Training loss: 2.121492624282837
Validation loss: 2.035265545050303

Epoch: 6| Step: 6
Training loss: 2.000528335571289
Validation loss: 2.0449061592419944

Epoch: 6| Step: 7
Training loss: 2.1422619819641113
Validation loss: 2.0473111073176065

Epoch: 6| Step: 8
Training loss: 2.3378853797912598
Validation loss: 2.0434585014979043

Epoch: 6| Step: 9
Training loss: 2.947666645050049
Validation loss: 2.053033232688904

Epoch: 6| Step: 10
Training loss: 2.244286298751831
Validation loss: 2.060602347056071

Epoch: 6| Step: 11
Training loss: 2.379913806915283
Validation loss: 2.0549766620000205

Epoch: 6| Step: 12
Training loss: 1.8707159757614136
Validation loss: 2.05764768520991

Epoch: 6| Step: 13
Training loss: 1.9455866813659668
Validation loss: 2.0520838697751365

Epoch: 67| Step: 0
Training loss: 2.277693510055542
Validation loss: 2.054666221141815

Epoch: 6| Step: 1
Training loss: 2.666987180709839
Validation loss: 2.0420737067858377

Epoch: 6| Step: 2
Training loss: 1.4635627269744873
Validation loss: 2.0385042826334634

Epoch: 6| Step: 3
Training loss: 2.157155990600586
Validation loss: 2.041939655939738

Epoch: 6| Step: 4
Training loss: 2.2588136196136475
Validation loss: 2.0439786513646445

Epoch: 6| Step: 5
Training loss: 2.112271785736084
Validation loss: 2.0373942852020264

Epoch: 6| Step: 6
Training loss: 2.665987491607666
Validation loss: 2.0543583035469055

Epoch: 6| Step: 7
Training loss: 2.328000545501709
Validation loss: 2.051661749680837

Epoch: 6| Step: 8
Training loss: 2.061985969543457
Validation loss: 2.0485997796058655

Epoch: 6| Step: 9
Training loss: 1.923248529434204
Validation loss: 2.0393494168917337

Epoch: 6| Step: 10
Training loss: 2.523944616317749
Validation loss: 2.0425252318382263

Epoch: 6| Step: 11
Training loss: 2.4865407943725586
Validation loss: 2.0369706948598227

Epoch: 6| Step: 12
Training loss: 2.1094067096710205
Validation loss: 2.045484721660614

Epoch: 6| Step: 13
Training loss: 2.223717212677002
Validation loss: 2.043708324432373

Epoch: 68| Step: 0
Training loss: 2.256246566772461
Validation loss: 2.040599008401235

Epoch: 6| Step: 1
Training loss: 1.9430136680603027
Validation loss: 2.049242456754049

Epoch: 6| Step: 2
Training loss: 2.930471658706665
Validation loss: 2.0554797053337097

Epoch: 6| Step: 3
Training loss: 2.617762565612793
Validation loss: 2.0543394684791565

Epoch: 6| Step: 4
Training loss: 2.5234711170196533
Validation loss: 2.0547539591789246

Epoch: 6| Step: 5
Training loss: 2.295292377471924
Validation loss: 2.043674945831299

Epoch: 6| Step: 6
Training loss: 2.1499688625335693
Validation loss: 2.052777866522471

Epoch: 6| Step: 7
Training loss: 2.0259604454040527
Validation loss: 2.0440271496772766

Epoch: 6| Step: 8
Training loss: 2.0226688385009766
Validation loss: 2.050326685110728

Epoch: 6| Step: 9
Training loss: 1.4958992004394531
Validation loss: 2.048219621181488

Epoch: 6| Step: 10
Training loss: 2.818211793899536
Validation loss: 2.045992910861969

Epoch: 6| Step: 11
Training loss: 1.9767073392868042
Validation loss: 2.0368231336275735

Epoch: 6| Step: 12
Training loss: 1.7895991802215576
Validation loss: 2.03416516383489

Epoch: 6| Step: 13
Training loss: 2.3899641036987305
Validation loss: 2.0269572138786316

Epoch: 69| Step: 0
Training loss: 2.048992156982422
Validation loss: 2.0278926690419516

Epoch: 6| Step: 1
Training loss: 1.4012916088104248
Validation loss: 2.038265844186147

Epoch: 6| Step: 2
Training loss: 2.1460046768188477
Validation loss: 2.0485966205596924

Epoch: 6| Step: 3
Training loss: 2.4503672122955322
Validation loss: 2.0488421519597373

Epoch: 6| Step: 4
Training loss: 2.316539764404297
Validation loss: 2.046084443728129

Epoch: 6| Step: 5
Training loss: 2.5641422271728516
Validation loss: 2.0372881094614663

Epoch: 6| Step: 6
Training loss: 1.9429962635040283
Validation loss: 2.030095398426056

Epoch: 6| Step: 7
Training loss: 2.4527578353881836
Validation loss: 2.0331343611081443

Epoch: 6| Step: 8
Training loss: 2.017214298248291
Validation loss: 2.038821736971537

Epoch: 6| Step: 9
Training loss: 2.44169545173645
Validation loss: 2.044797877470652

Epoch: 6| Step: 10
Training loss: 2.1902058124542236
Validation loss: 2.0533276796340942

Epoch: 6| Step: 11
Training loss: 2.2664031982421875
Validation loss: 2.039060393969218

Epoch: 6| Step: 12
Training loss: 2.0453922748565674
Validation loss: 2.04658834139506

Epoch: 6| Step: 13
Training loss: 2.8896260261535645
Validation loss: 2.041552464167277

Epoch: 70| Step: 0
Training loss: 2.5917890071868896
Validation loss: 2.0406326254208884

Epoch: 6| Step: 1
Training loss: 1.87752103805542
Validation loss: 2.0366748571395874

Epoch: 6| Step: 2
Training loss: 2.425633668899536
Validation loss: 2.0373220245043435

Epoch: 6| Step: 3
Training loss: 2.1948554515838623
Validation loss: 2.035082697868347

Epoch: 6| Step: 4
Training loss: 2.500321865081787
Validation loss: 2.0285956859588623

Epoch: 6| Step: 5
Training loss: 2.13015079498291
Validation loss: 2.0280837615331015

Epoch: 6| Step: 6
Training loss: 2.4381117820739746
Validation loss: 2.027755637963613

Epoch: 6| Step: 7
Training loss: 1.7878354787826538
Validation loss: 2.023397207260132

Epoch: 6| Step: 8
Training loss: 1.9175901412963867
Validation loss: 2.0244233806928

Epoch: 6| Step: 9
Training loss: 2.1434621810913086
Validation loss: 2.028985937436422

Epoch: 6| Step: 10
Training loss: 2.3377609252929688
Validation loss: 2.0209835966428122

Epoch: 6| Step: 11
Training loss: 2.0973665714263916
Validation loss: 2.0272270242373147

Epoch: 6| Step: 12
Training loss: 2.05700945854187
Validation loss: 2.0204036633173623

Epoch: 6| Step: 13
Training loss: 2.507535696029663
Validation loss: 2.0210651755332947

Epoch: 71| Step: 0
Training loss: 2.3598403930664062
Validation loss: 2.0282302101453147

Epoch: 6| Step: 1
Training loss: 2.1514360904693604
Validation loss: 2.029723505179087

Epoch: 6| Step: 2
Training loss: 2.201474189758301
Validation loss: 2.0324661135673523

Epoch: 6| Step: 3
Training loss: 2.1376137733459473
Validation loss: 2.0427498817443848

Epoch: 6| Step: 4
Training loss: 2.2772107124328613
Validation loss: 2.036679128805796

Epoch: 6| Step: 5
Training loss: 2.5926663875579834
Validation loss: 2.039076785246531

Epoch: 6| Step: 6
Training loss: 2.2372827529907227
Validation loss: 2.045750876267751

Epoch: 6| Step: 7
Training loss: 2.644516944885254
Validation loss: 2.0317909518877664

Epoch: 6| Step: 8
Training loss: 2.2058558464050293
Validation loss: 2.0236739118893943

Epoch: 6| Step: 9
Training loss: 1.6858949661254883
Validation loss: 2.0190672477086387

Epoch: 6| Step: 10
Training loss: 2.154808759689331
Validation loss: 2.017319142818451

Epoch: 6| Step: 11
Training loss: 2.4533681869506836
Validation loss: 2.017083704471588

Epoch: 6| Step: 12
Training loss: 2.3039133548736572
Validation loss: 2.0247148871421814

Epoch: 6| Step: 13
Training loss: 1.3438018560409546
Validation loss: 2.0175912380218506

Epoch: 72| Step: 0
Training loss: 1.7597393989562988
Validation loss: 2.021371920903524

Epoch: 6| Step: 1
Training loss: 2.7990915775299072
Validation loss: 2.0165801644325256

Epoch: 6| Step: 2
Training loss: 2.5676896572113037
Validation loss: 2.0140099922815957

Epoch: 6| Step: 3
Training loss: 2.2092487812042236
Validation loss: 2.022406299908956

Epoch: 6| Step: 4
Training loss: 2.444136619567871
Validation loss: 2.023930291334788

Epoch: 6| Step: 5
Training loss: 2.156271457672119
Validation loss: 2.025067766507467

Epoch: 6| Step: 6
Training loss: 2.068068027496338
Validation loss: 2.028339902559916

Epoch: 6| Step: 7
Training loss: 2.2925853729248047
Validation loss: 2.0357101360956826

Epoch: 6| Step: 8
Training loss: 2.359720230102539
Validation loss: 2.029096245765686

Epoch: 6| Step: 9
Training loss: 2.306192398071289
Validation loss: 2.0392651160558066

Epoch: 6| Step: 10
Training loss: 1.8202306032180786
Validation loss: 2.035260856151581

Epoch: 6| Step: 11
Training loss: 1.9454386234283447
Validation loss: 2.0215848684310913

Epoch: 6| Step: 12
Training loss: 2.213118076324463
Validation loss: 2.027723471323649

Epoch: 6| Step: 13
Training loss: 1.7981666326522827
Validation loss: 2.021794239679972

Epoch: 73| Step: 0
Training loss: 2.1135177612304688
Validation loss: 2.021569848060608

Epoch: 6| Step: 1
Training loss: 2.561983823776245
Validation loss: 2.014195124308268

Epoch: 6| Step: 2
Training loss: 1.8282461166381836
Validation loss: 2.0148168206214905

Epoch: 6| Step: 3
Training loss: 1.3584181070327759
Validation loss: 2.0209923585255942

Epoch: 6| Step: 4
Training loss: 2.023618221282959
Validation loss: 2.0171695748964944

Epoch: 6| Step: 5
Training loss: 2.326925277709961
Validation loss: 2.018154521783193

Epoch: 6| Step: 6
Training loss: 1.8950811624526978
Validation loss: 2.023428519566854

Epoch: 6| Step: 7
Training loss: 2.3301138877868652
Validation loss: 2.0179439783096313

Epoch: 6| Step: 8
Training loss: 2.022796630859375
Validation loss: 2.017384688059489

Epoch: 6| Step: 9
Training loss: 2.740365982055664
Validation loss: 2.0139233469963074

Epoch: 6| Step: 10
Training loss: 2.697223663330078
Validation loss: 2.0165860454241433

Epoch: 6| Step: 11
Training loss: 2.0656423568725586
Validation loss: 2.0157248179117837

Epoch: 6| Step: 12
Training loss: 2.3115406036376953
Validation loss: 2.020345687866211

Epoch: 6| Step: 13
Training loss: 2.6047229766845703
Validation loss: 2.021295408407847

Epoch: 74| Step: 0
Training loss: 2.497771739959717
Validation loss: 2.0219130516052246

Epoch: 6| Step: 1
Training loss: 1.998784065246582
Validation loss: 2.023966451485952

Epoch: 6| Step: 2
Training loss: 2.7634267807006836
Validation loss: 2.026585360368093

Epoch: 6| Step: 3
Training loss: 2.5963778495788574
Validation loss: 2.035691420237223

Epoch: 6| Step: 4
Training loss: 2.0379369258880615
Validation loss: 2.027805427710215

Epoch: 6| Step: 5
Training loss: 1.5662498474121094
Validation loss: 2.0318451722462973

Epoch: 6| Step: 6
Training loss: 1.9753957986831665
Validation loss: 2.033706625302633

Epoch: 6| Step: 7
Training loss: 1.8929176330566406
Validation loss: 2.0311880707740784

Epoch: 6| Step: 8
Training loss: 2.544848918914795
Validation loss: 2.0321008761723838

Epoch: 6| Step: 9
Training loss: 2.6530439853668213
Validation loss: 2.029816190401713

Epoch: 6| Step: 10
Training loss: 1.75551438331604
Validation loss: 2.020420173803965

Epoch: 6| Step: 11
Training loss: 1.7833890914916992
Validation loss: 2.0265820821126304

Epoch: 6| Step: 12
Training loss: 1.9456768035888672
Validation loss: 2.018929580847422

Epoch: 6| Step: 13
Training loss: 2.5637128353118896
Validation loss: 2.012773036956787

Epoch: 75| Step: 0
Training loss: 2.415463924407959
Validation loss: 2.0172289411226907

Epoch: 6| Step: 1
Training loss: 2.393207550048828
Validation loss: 2.0115339954694114

Epoch: 6| Step: 2
Training loss: 2.4075379371643066
Validation loss: 2.01489249865214

Epoch: 6| Step: 3
Training loss: 2.585435152053833
Validation loss: 2.0113788843154907

Epoch: 6| Step: 4
Training loss: 2.037104368209839
Validation loss: 2.0102991263071694

Epoch: 6| Step: 5
Training loss: 2.0930535793304443
Validation loss: 2.015948235988617

Epoch: 6| Step: 6
Training loss: 1.888962984085083
Validation loss: 2.01883735259374

Epoch: 6| Step: 7
Training loss: 2.7181806564331055
Validation loss: 2.0208720167477927

Epoch: 6| Step: 8
Training loss: 2.806445598602295
Validation loss: 2.026012400786082

Epoch: 6| Step: 9
Training loss: 1.823972225189209
Validation loss: 2.0285408894220986

Epoch: 6| Step: 10
Training loss: 1.9319936037063599
Validation loss: 2.0188591281572976

Epoch: 6| Step: 11
Training loss: 2.321077346801758
Validation loss: 2.033318817615509

Epoch: 6| Step: 12
Training loss: 1.7584025859832764
Validation loss: 2.0357843240102134

Epoch: 6| Step: 13
Training loss: 1.2472761869430542
Validation loss: 2.037541647752126

Epoch: 76| Step: 0
Training loss: 1.9582499265670776
Validation loss: 2.054030974706014

Epoch: 6| Step: 1
Training loss: 2.502603054046631
Validation loss: 2.0332937637964883

Epoch: 6| Step: 2
Training loss: 2.2399327754974365
Validation loss: 2.0354149540265403

Epoch: 6| Step: 3
Training loss: 2.3879518508911133
Validation loss: 2.0233859419822693

Epoch: 6| Step: 4
Training loss: 2.054975986480713
Validation loss: 2.0179273088773093

Epoch: 6| Step: 5
Training loss: 1.7595772743225098
Validation loss: 2.027663290500641

Epoch: 6| Step: 6
Training loss: 1.6908715963363647
Validation loss: 2.0323551495869956

Epoch: 6| Step: 7
Training loss: 2.3536410331726074
Validation loss: 2.0224852164586387

Epoch: 6| Step: 8
Training loss: 1.8834975957870483
Validation loss: 2.034279942512512

Epoch: 6| Step: 9
Training loss: 2.2452759742736816
Validation loss: 2.029874781767527

Epoch: 6| Step: 10
Training loss: 2.386195421218872
Validation loss: 2.0304264624913535

Epoch: 6| Step: 11
Training loss: 2.6142008304595947
Validation loss: 2.0220936934153237

Epoch: 6| Step: 12
Training loss: 2.245206356048584
Validation loss: 2.0245389540990195

Epoch: 6| Step: 13
Training loss: 2.2511332035064697
Validation loss: 2.0260971983273826

Epoch: 77| Step: 0
Training loss: 1.8434100151062012
Validation loss: 2.025628407796224

Epoch: 6| Step: 1
Training loss: 2.3368301391601562
Validation loss: 2.0177061359087625

Epoch: 6| Step: 2
Training loss: 2.5773346424102783
Validation loss: 2.0225212574005127

Epoch: 6| Step: 3
Training loss: 2.134875774383545
Validation loss: 2.0218037168184915

Epoch: 6| Step: 4
Training loss: 2.3553390502929688
Validation loss: 2.0173416137695312

Epoch: 6| Step: 5
Training loss: 2.155827760696411
Validation loss: 2.017391820748647

Epoch: 6| Step: 6
Training loss: 1.967295527458191
Validation loss: 2.01444403330485

Epoch: 6| Step: 7
Training loss: 1.7800581455230713
Validation loss: 2.023279825846354

Epoch: 6| Step: 8
Training loss: 2.31669020652771
Validation loss: 2.0122943123181662

Epoch: 6| Step: 9
Training loss: 1.945819616317749
Validation loss: 2.020921051502228

Epoch: 6| Step: 10
Training loss: 2.723005533218384
Validation loss: 2.0245105028152466

Epoch: 6| Step: 11
Training loss: 2.023613452911377
Validation loss: 2.0325100421905518

Epoch: 6| Step: 12
Training loss: 2.1630420684814453
Validation loss: 2.0314068595568338

Epoch: 6| Step: 13
Training loss: 2.0043091773986816
Validation loss: 2.0279309352238974

Epoch: 78| Step: 0
Training loss: 1.9824562072753906
Validation loss: 2.0280548532803855

Epoch: 6| Step: 1
Training loss: 1.8502638339996338
Validation loss: 2.036679267883301

Epoch: 6| Step: 2
Training loss: 2.084902048110962
Validation loss: 2.0370484391848245

Epoch: 6| Step: 3
Training loss: 2.3077340126037598
Validation loss: 2.0527119239171348

Epoch: 6| Step: 4
Training loss: 2.3393049240112305
Validation loss: 2.03164271513621

Epoch: 6| Step: 5
Training loss: 1.8477330207824707
Validation loss: 2.041224777698517

Epoch: 6| Step: 6
Training loss: 1.9199541807174683
Validation loss: 2.049449900786082

Epoch: 6| Step: 7
Training loss: 1.7189387083053589
Validation loss: 2.0518852074941

Epoch: 6| Step: 8
Training loss: 2.5447745323181152
Validation loss: 2.0600892504056296

Epoch: 6| Step: 9
Training loss: 1.7616761922836304
Validation loss: 2.052196423212687

Epoch: 6| Step: 10
Training loss: 3.015859603881836
Validation loss: 2.0453131993611655

Epoch: 6| Step: 11
Training loss: 2.5410726070404053
Validation loss: 2.026323974132538

Epoch: 6| Step: 12
Training loss: 2.2525103092193604
Validation loss: 2.019454995791117

Epoch: 6| Step: 13
Training loss: 2.433544635772705
Validation loss: 2.026563068230947

Epoch: 79| Step: 0
Training loss: 2.146160364151001
Validation loss: 2.023160219192505

Epoch: 6| Step: 1
Training loss: 1.7218782901763916
Validation loss: 2.031193812688192

Epoch: 6| Step: 2
Training loss: 2.7754039764404297
Validation loss: 2.031308591365814

Epoch: 6| Step: 3
Training loss: 2.8546035289764404
Validation loss: 2.0409008264541626

Epoch: 6| Step: 4
Training loss: 1.943020224571228
Validation loss: 2.0403598149617515

Epoch: 6| Step: 5
Training loss: 1.918539047241211
Validation loss: 2.0388498504956565

Epoch: 6| Step: 6
Training loss: 2.454282760620117
Validation loss: 2.0422645807266235

Epoch: 6| Step: 7
Training loss: 2.049679756164551
Validation loss: 2.0362234711647034

Epoch: 6| Step: 8
Training loss: 1.9618210792541504
Validation loss: 2.0397439996401467

Epoch: 6| Step: 9
Training loss: 2.6610426902770996
Validation loss: 2.0327038168907166

Epoch: 6| Step: 10
Training loss: 1.9051264524459839
Validation loss: 2.0290500124295554

Epoch: 6| Step: 11
Training loss: 2.0637011528015137
Validation loss: 2.0295554399490356

Epoch: 6| Step: 12
Training loss: 1.8192492723464966
Validation loss: 2.0212092796961465

Epoch: 6| Step: 13
Training loss: 2.614713668823242
Validation loss: 2.0134536027908325

Epoch: 80| Step: 0
Training loss: 2.7792792320251465
Validation loss: 2.0096128384272256

Epoch: 6| Step: 1
Training loss: 2.3851025104522705
Validation loss: 2.0106916626294455

Epoch: 6| Step: 2
Training loss: 1.4843531847000122
Validation loss: 2.010970870653788

Epoch: 6| Step: 3
Training loss: 1.5019375085830688
Validation loss: 2.0003122687339783

Epoch: 6| Step: 4
Training loss: 1.9953131675720215
Validation loss: 2.0037763516108194

Epoch: 6| Step: 5
Training loss: 2.9033617973327637
Validation loss: 2.008481522401174

Epoch: 6| Step: 6
Training loss: 2.397329568862915
Validation loss: 2.0132609208424888

Epoch: 6| Step: 7
Training loss: 1.8541755676269531
Validation loss: 2.0038668711980185

Epoch: 6| Step: 8
Training loss: 2.12070894241333
Validation loss: 2.0115954677263894

Epoch: 6| Step: 9
Training loss: 1.2286454439163208
Validation loss: 2.0054775277773538

Epoch: 6| Step: 10
Training loss: 2.602457046508789
Validation loss: 2.0119568904240928

Epoch: 6| Step: 11
Training loss: 2.1510777473449707
Validation loss: 2.020928700764974

Epoch: 6| Step: 12
Training loss: 2.8957877159118652
Validation loss: 2.026802639166514

Epoch: 6| Step: 13
Training loss: 2.1471164226531982
Validation loss: 2.0276299715042114

Epoch: 81| Step: 0
Training loss: 2.297776699066162
Validation loss: 2.034924487272898

Epoch: 6| Step: 1
Training loss: 2.220407009124756
Validation loss: 2.038457969824473

Epoch: 6| Step: 2
Training loss: 1.7207646369934082
Validation loss: 2.039904753367106

Epoch: 6| Step: 3
Training loss: 2.605421543121338
Validation loss: 2.027894695599874

Epoch: 6| Step: 4
Training loss: 2.420442581176758
Validation loss: 2.0418136517206826

Epoch: 6| Step: 5
Training loss: 1.9283645153045654
Validation loss: 2.0236881573994956

Epoch: 6| Step: 6
Training loss: 1.978853464126587
Validation loss: 2.024089972178141

Epoch: 6| Step: 7
Training loss: 2.065416097640991
Validation loss: 2.01955509185791

Epoch: 6| Step: 8
Training loss: 2.2645468711853027
Validation loss: 2.0118237733840942

Epoch: 6| Step: 9
Training loss: 2.04616641998291
Validation loss: 2.008259872595469

Epoch: 6| Step: 10
Training loss: 1.8365097045898438
Validation loss: 2.0057995518048606

Epoch: 6| Step: 11
Training loss: 2.320930004119873
Validation loss: 2.0022268891334534

Epoch: 6| Step: 12
Training loss: 2.3103103637695312
Validation loss: 1.9962894916534424

Epoch: 6| Step: 13
Training loss: 2.283705234527588
Validation loss: 2.0013355811436973

Epoch: 82| Step: 0
Training loss: 1.5238893032073975
Validation loss: 2.0007416208585105

Epoch: 6| Step: 1
Training loss: 1.8468950986862183
Validation loss: 2.0141091545422873

Epoch: 6| Step: 2
Training loss: 2.0665998458862305
Validation loss: 2.0119782090187073

Epoch: 6| Step: 3
Training loss: 2.442307949066162
Validation loss: 2.0256775418917337

Epoch: 6| Step: 4
Training loss: 2.184582233428955
Validation loss: 2.0337402621905007

Epoch: 6| Step: 5
Training loss: 1.9586323499679565
Validation loss: 2.030918995539347

Epoch: 6| Step: 6
Training loss: 3.3704447746276855
Validation loss: 2.0197450518608093

Epoch: 6| Step: 7
Training loss: 1.5349292755126953
Validation loss: 2.0256860852241516

Epoch: 6| Step: 8
Training loss: 2.3181543350219727
Validation loss: 2.0353761315345764

Epoch: 6| Step: 9
Training loss: 2.0858733654022217
Validation loss: 2.0530200203259787

Epoch: 6| Step: 10
Training loss: 2.1431758403778076
Validation loss: 2.039559622605642

Epoch: 6| Step: 11
Training loss: 1.6741447448730469
Validation loss: 2.0520309607187905

Epoch: 6| Step: 12
Training loss: 2.7344508171081543
Validation loss: 2.0425262451171875

Epoch: 6| Step: 13
Training loss: 2.6171188354492188
Validation loss: 2.0271628300348916

Epoch: 83| Step: 0
Training loss: 2.0806221961975098
Validation loss: 2.0300268729527793

Epoch: 6| Step: 1
Training loss: 2.2598085403442383
Validation loss: 2.013334651788076

Epoch: 6| Step: 2
Training loss: 2.5186150074005127
Validation loss: 2.0165940721829734

Epoch: 6| Step: 3
Training loss: 1.734118938446045
Validation loss: 2.0155124068260193

Epoch: 6| Step: 4
Training loss: 2.242157220840454
Validation loss: 2.020494500796

Epoch: 6| Step: 5
Training loss: 2.4970760345458984
Validation loss: 2.0185407996177673

Epoch: 6| Step: 6
Training loss: 2.116121768951416
Validation loss: 2.017863134543101

Epoch: 6| Step: 7
Training loss: 1.9161872863769531
Validation loss: 2.012755493323008

Epoch: 6| Step: 8
Training loss: 1.6391663551330566
Validation loss: 2.015308996041616

Epoch: 6| Step: 9
Training loss: 2.900954008102417
Validation loss: 2.008168419202169

Epoch: 6| Step: 10
Training loss: 2.3322601318359375
Validation loss: 2.019950350125631

Epoch: 6| Step: 11
Training loss: 1.504815936088562
Validation loss: 2.0134501854578652

Epoch: 6| Step: 12
Training loss: 2.0542702674865723
Validation loss: 2.0139827728271484

Epoch: 6| Step: 13
Training loss: 2.3857614994049072
Validation loss: 2.0090949535369873

Epoch: 84| Step: 0
Training loss: 1.6840574741363525
Validation loss: 2.0114384094874063

Epoch: 6| Step: 1
Training loss: 2.411695718765259
Validation loss: 2.0253516832987466

Epoch: 6| Step: 2
Training loss: 2.3497555255889893
Validation loss: 2.0296994050343833

Epoch: 6| Step: 3
Training loss: 1.57562255859375
Validation loss: 2.0287088553110757

Epoch: 6| Step: 4
Training loss: 1.9367804527282715
Validation loss: 2.051074723402659

Epoch: 6| Step: 5
Training loss: 1.799384355545044
Validation loss: 2.049410124619802

Epoch: 6| Step: 6
Training loss: 3.0280847549438477
Validation loss: 2.0611231128374734

Epoch: 6| Step: 7
Training loss: 2.3323636054992676
Validation loss: 2.0575053691864014

Epoch: 6| Step: 8
Training loss: 2.5484437942504883
Validation loss: 2.0671982367833457

Epoch: 6| Step: 9
Training loss: 2.4290895462036133
Validation loss: 2.069973866144816

Epoch: 6| Step: 10
Training loss: 1.7097781896591187
Validation loss: 2.0743576685587564

Epoch: 6| Step: 11
Training loss: 2.607034683227539
Validation loss: 2.0488504767417908

Epoch: 6| Step: 12
Training loss: 2.389781951904297
Validation loss: 2.0452757279078164

Epoch: 6| Step: 13
Training loss: 1.653707504272461
Validation loss: 2.035212238629659

Epoch: 85| Step: 0
Training loss: 2.0589494705200195
Validation loss: 2.019812365372976

Epoch: 6| Step: 1
Training loss: 2.11319637298584
Validation loss: 2.0197675625483194

Epoch: 6| Step: 2
Training loss: 2.6993844509124756
Validation loss: 2.0107953747113547

Epoch: 6| Step: 3
Training loss: 2.490304946899414
Validation loss: 2.0123834013938904

Epoch: 6| Step: 4
Training loss: 1.8211604356765747
Validation loss: 2.0063610474268594

Epoch: 6| Step: 5
Training loss: 2.2206203937530518
Validation loss: 2.0033628940582275

Epoch: 6| Step: 6
Training loss: 1.879248023033142
Validation loss: 2.0114134351412454

Epoch: 6| Step: 7
Training loss: 1.8966529369354248
Validation loss: 2.0019965966542563

Epoch: 6| Step: 8
Training loss: 2.043036937713623
Validation loss: 2.010500172773997

Epoch: 6| Step: 9
Training loss: 2.191854476928711
Validation loss: 2.012208720048269

Epoch: 6| Step: 10
Training loss: 2.271481513977051
Validation loss: 2.005734304587046

Epoch: 6| Step: 11
Training loss: 2.552194118499756
Validation loss: 2.0070497592290244

Epoch: 6| Step: 12
Training loss: 1.9014263153076172
Validation loss: 2.0258456269900003

Epoch: 6| Step: 13
Training loss: 2.1439449787139893
Validation loss: 2.041455547014872

Epoch: 86| Step: 0
Training loss: 2.286959648132324
Validation loss: 2.03700457016627

Epoch: 6| Step: 1
Training loss: 2.065951347351074
Validation loss: 2.0622185468673706

Epoch: 6| Step: 2
Training loss: 1.622230052947998
Validation loss: 2.066690444946289

Epoch: 6| Step: 3
Training loss: 2.01084566116333
Validation loss: 2.089489738146464

Epoch: 6| Step: 4
Training loss: 2.0829851627349854
Validation loss: 2.077834963798523

Epoch: 6| Step: 5
Training loss: 2.6866445541381836
Validation loss: 2.069040616353353

Epoch: 6| Step: 6
Training loss: 2.1181206703186035
Validation loss: 2.0590428908665976

Epoch: 6| Step: 7
Training loss: 2.413477897644043
Validation loss: 2.0308744311332703

Epoch: 6| Step: 8
Training loss: 1.8323949575424194
Validation loss: 2.025721848011017

Epoch: 6| Step: 9
Training loss: 2.486595869064331
Validation loss: 2.0087945262591043

Epoch: 6| Step: 10
Training loss: 2.0802576541900635
Validation loss: 2.0072219173113504

Epoch: 6| Step: 11
Training loss: 2.7263307571411133
Validation loss: 2.000373641649882

Epoch: 6| Step: 12
Training loss: 2.0465643405914307
Validation loss: 2.006470481554667

Epoch: 6| Step: 13
Training loss: 1.7050408124923706
Validation loss: 2.0055303970972695

Epoch: 87| Step: 0
Training loss: 2.1855571269989014
Validation loss: 2.0074969132741294

Epoch: 6| Step: 1
Training loss: 2.074404001235962
Validation loss: 2.0064011812210083

Epoch: 6| Step: 2
Training loss: 2.7826285362243652
Validation loss: 2.0045586824417114

Epoch: 6| Step: 3
Training loss: 1.518131136894226
Validation loss: 2.0136051972707114

Epoch: 6| Step: 4
Training loss: 2.1053965091705322
Validation loss: 2.0105395317077637

Epoch: 6| Step: 5
Training loss: 2.1597368717193604
Validation loss: 2.009764870007833

Epoch: 6| Step: 6
Training loss: 2.160313129425049
Validation loss: 2.003759443759918

Epoch: 6| Step: 7
Training loss: 2.14105486869812
Validation loss: 2.010363539059957

Epoch: 6| Step: 8
Training loss: 2.6824488639831543
Validation loss: 2.0017539461453757

Epoch: 6| Step: 9
Training loss: 2.2026093006134033
Validation loss: 2.0127104123433432

Epoch: 6| Step: 10
Training loss: 1.9890583753585815
Validation loss: 2.013890266418457

Epoch: 6| Step: 11
Training loss: 1.8514289855957031
Validation loss: 2.004418690999349

Epoch: 6| Step: 12
Training loss: 1.8904350996017456
Validation loss: 1.9997612635294597

Epoch: 6| Step: 13
Training loss: 2.577949047088623
Validation loss: 2.0132842659950256

Epoch: 88| Step: 0
Training loss: 2.1535348892211914
Validation loss: 2.011252442995707

Epoch: 6| Step: 1
Training loss: 1.6979243755340576
Validation loss: 2.0236209829648337

Epoch: 6| Step: 2
Training loss: 2.191877841949463
Validation loss: 2.026546577612559

Epoch: 6| Step: 3
Training loss: 1.90259850025177
Validation loss: 2.0162870486577353

Epoch: 6| Step: 4
Training loss: 2.2710072994232178
Validation loss: 2.011699398358663

Epoch: 6| Step: 5
Training loss: 2.537097692489624
Validation loss: 2.0100311438242593

Epoch: 6| Step: 6
Training loss: 2.2840824127197266
Validation loss: 2.0201932787895203

Epoch: 6| Step: 7
Training loss: 2.0903797149658203
Validation loss: 2.0244330565134683

Epoch: 6| Step: 8
Training loss: 1.807621955871582
Validation loss: 2.0094140569368997

Epoch: 6| Step: 9
Training loss: 2.073430299758911
Validation loss: 2.017656763394674

Epoch: 6| Step: 10
Training loss: 2.071976661682129
Validation loss: 2.0301669239997864

Epoch: 6| Step: 11
Training loss: 2.506211996078491
Validation loss: 2.030068039894104

Epoch: 6| Step: 12
Training loss: 1.7738218307495117
Validation loss: 2.0302435557047525

Epoch: 6| Step: 13
Training loss: 2.6687822341918945
Validation loss: 2.0258697072664895

Epoch: 89| Step: 0
Training loss: 2.4511122703552246
Validation loss: 2.0250324408213296

Epoch: 6| Step: 1
Training loss: 2.5887985229492188
Validation loss: 2.023659567038218

Epoch: 6| Step: 2
Training loss: 2.2042665481567383
Validation loss: 2.033930778503418

Epoch: 6| Step: 3
Training loss: 2.147260904312134
Validation loss: 2.0245315035184226

Epoch: 6| Step: 4
Training loss: 1.3375213146209717
Validation loss: 2.0130204359690347

Epoch: 6| Step: 5
Training loss: 1.8900271654129028
Validation loss: 2.0255228678385415

Epoch: 6| Step: 6
Training loss: 2.437802791595459
Validation loss: 2.0224619706471763

Epoch: 6| Step: 7
Training loss: 2.4080796241760254
Validation loss: 2.024161418279012

Epoch: 6| Step: 8
Training loss: 2.0559659004211426
Validation loss: 2.0273234645525613

Epoch: 6| Step: 9
Training loss: 1.8323980569839478
Validation loss: 2.0119442343711853

Epoch: 6| Step: 10
Training loss: 2.06715726852417
Validation loss: 2.0189334750175476

Epoch: 6| Step: 11
Training loss: 2.4633901119232178
Validation loss: 2.013709306716919

Epoch: 6| Step: 12
Training loss: 1.9985026121139526
Validation loss: 2.0221696893374124

Epoch: 6| Step: 13
Training loss: 1.9776637554168701
Validation loss: 2.0135780771573386

Epoch: 90| Step: 0
Training loss: 2.2638564109802246
Validation loss: 2.0191863775253296

Epoch: 6| Step: 1
Training loss: 1.2925256490707397
Validation loss: 2.0109192927678428

Epoch: 6| Step: 2
Training loss: 1.9907245635986328
Validation loss: 2.0185834169387817

Epoch: 6| Step: 3
Training loss: 2.45624041557312
Validation loss: 2.011127789815267

Epoch: 6| Step: 4
Training loss: 1.924819827079773
Validation loss: 2.00340602795283

Epoch: 6| Step: 5
Training loss: 2.423914909362793
Validation loss: 2.0117286443710327

Epoch: 6| Step: 6
Training loss: 2.683818817138672
Validation loss: 2.0163872838020325

Epoch: 6| Step: 7
Training loss: 1.8847781419754028
Validation loss: 2.0030474265416465

Epoch: 6| Step: 8
Training loss: 2.340475559234619
Validation loss: 2.007583578427633

Epoch: 6| Step: 9
Training loss: 2.115431547164917
Validation loss: 2.015651802221934

Epoch: 6| Step: 10
Training loss: 2.500955104827881
Validation loss: 2.016734500726064

Epoch: 6| Step: 11
Training loss: 2.3665037155151367
Validation loss: 2.0018858114878335

Epoch: 6| Step: 12
Training loss: 1.9590280055999756
Validation loss: 2.0075737039248147

Epoch: 6| Step: 13
Training loss: 1.7450249195098877
Validation loss: 2.0212469498316445

Epoch: 91| Step: 0
Training loss: 1.8907123804092407
Validation loss: 2.0040284395217896

Epoch: 6| Step: 1
Training loss: 2.142368793487549
Validation loss: 2.0138614177703857

Epoch: 6| Step: 2
Training loss: 1.5098780393600464
Validation loss: 2.020024836063385

Epoch: 6| Step: 3
Training loss: 1.9302546977996826
Validation loss: 2.018140951792399

Epoch: 6| Step: 4
Training loss: 2.572394609451294
Validation loss: 2.012219568093618

Epoch: 6| Step: 5
Training loss: 2.521191120147705
Validation loss: 2.0224650502204895

Epoch: 6| Step: 6
Training loss: 2.490440845489502
Validation loss: 2.016400396823883

Epoch: 6| Step: 7
Training loss: 1.8344792127609253
Validation loss: 2.016347130139669

Epoch: 6| Step: 8
Training loss: 2.5655617713928223
Validation loss: 2.010421574115753

Epoch: 6| Step: 9
Training loss: 2.621670722961426
Validation loss: 2.007666905721029

Epoch: 6| Step: 10
Training loss: 1.8625935316085815
Validation loss: 2.0164162516593933

Epoch: 6| Step: 11
Training loss: 2.5135750770568848
Validation loss: 2.0140740275382996

Epoch: 6| Step: 12
Training loss: 1.8828814029693604
Validation loss: 2.0182363788286843

Epoch: 6| Step: 13
Training loss: 1.6200765371322632
Validation loss: 2.018604060014089

Epoch: 92| Step: 0
Training loss: 2.7023744583129883
Validation loss: 2.0189419190088906

Epoch: 6| Step: 1
Training loss: 2.1656107902526855
Validation loss: 2.037125905354818

Epoch: 6| Step: 2
Training loss: 1.8579130172729492
Validation loss: 2.035024801890055

Epoch: 6| Step: 3
Training loss: 1.862419843673706
Validation loss: 2.0474608341852822

Epoch: 6| Step: 4
Training loss: 2.5949673652648926
Validation loss: 2.0308478275934854

Epoch: 6| Step: 5
Training loss: 2.785825252532959
Validation loss: 2.0204932490984597

Epoch: 6| Step: 6
Training loss: 1.992321491241455
Validation loss: 2.010948737462362

Epoch: 6| Step: 7
Training loss: 2.1988844871520996
Validation loss: 1.999060372511546

Epoch: 6| Step: 8
Training loss: 1.554800033569336
Validation loss: 2.001241306463877

Epoch: 6| Step: 9
Training loss: 1.6774004697799683
Validation loss: 2.0004872481028237

Epoch: 6| Step: 10
Training loss: 2.14028263092041
Validation loss: 2.0043494502703347

Epoch: 6| Step: 11
Training loss: 2.486830234527588
Validation loss: 1.9991243084271748

Epoch: 6| Step: 12
Training loss: 1.6778813600540161
Validation loss: 1.9980778495470684

Epoch: 6| Step: 13
Training loss: 2.6048147678375244
Validation loss: 1.9995092153549194

Epoch: 93| Step: 0
Training loss: 3.05973219871521
Validation loss: 1.9936073422431946

Epoch: 6| Step: 1
Training loss: 1.7123668193817139
Validation loss: 1.996568242708842

Epoch: 6| Step: 2
Training loss: 2.120842933654785
Validation loss: 1.9911673069000244

Epoch: 6| Step: 3
Training loss: 2.2961459159851074
Validation loss: 2.0001571774482727

Epoch: 6| Step: 4
Training loss: 2.4684014320373535
Validation loss: 1.9981689651807149

Epoch: 6| Step: 5
Training loss: 2.5151638984680176
Validation loss: 2.0035229325294495

Epoch: 6| Step: 6
Training loss: 1.6037695407867432
Validation loss: 1.9915261467297871

Epoch: 6| Step: 7
Training loss: 2.009310245513916
Validation loss: 1.9944676160812378

Epoch: 6| Step: 8
Training loss: 1.9901047945022583
Validation loss: 2.002236247062683

Epoch: 6| Step: 9
Training loss: 1.9077335596084595
Validation loss: 2.003243565559387

Epoch: 6| Step: 10
Training loss: 1.6049927473068237
Validation loss: 2.0016247828801474

Epoch: 6| Step: 11
Training loss: 2.853550434112549
Validation loss: 2.0053586761156716

Epoch: 6| Step: 12
Training loss: 2.6369357109069824
Validation loss: 2.0026185512542725

Epoch: 6| Step: 13
Training loss: 1.2485984563827515
Validation loss: 2.0052571495374045

Epoch: 94| Step: 0
Training loss: 2.501389503479004
Validation loss: 2.0236579378445945

Epoch: 6| Step: 1
Training loss: 1.8917839527130127
Validation loss: 2.010044276714325

Epoch: 6| Step: 2
Training loss: 2.200113296508789
Validation loss: 2.0133506457010903

Epoch: 6| Step: 3
Training loss: 1.7401734590530396
Validation loss: 2.030030608177185

Epoch: 6| Step: 4
Training loss: 2.2084572315216064
Validation loss: 2.0371274749437966

Epoch: 6| Step: 5
Training loss: 1.5671558380126953
Validation loss: 2.0316521922747293

Epoch: 6| Step: 6
Training loss: 2.604647397994995
Validation loss: 2.024746755758921

Epoch: 6| Step: 7
Training loss: 2.1011481285095215
Validation loss: 2.023433049519857

Epoch: 6| Step: 8
Training loss: 2.471588611602783
Validation loss: 2.0169403155644736

Epoch: 6| Step: 9
Training loss: 2.7623634338378906
Validation loss: 2.020686308542887

Epoch: 6| Step: 10
Training loss: 2.1922335624694824
Validation loss: 2.0155182480812073

Epoch: 6| Step: 11
Training loss: 2.2700085639953613
Validation loss: 2.0139465928077698

Epoch: 6| Step: 12
Training loss: 1.9185796976089478
Validation loss: 2.016494790712992

Epoch: 6| Step: 13
Training loss: 1.4422733783721924
Validation loss: 2.018553694089254

Epoch: 95| Step: 0
Training loss: 1.7971930503845215
Validation loss: 2.015807032585144

Epoch: 6| Step: 1
Training loss: 2.089005947113037
Validation loss: 2.012786567211151

Epoch: 6| Step: 2
Training loss: 1.837327241897583
Validation loss: 2.0195825894673667

Epoch: 6| Step: 3
Training loss: 2.511082887649536
Validation loss: 2.010840674241384

Epoch: 6| Step: 4
Training loss: 1.7368865013122559
Validation loss: 2.0143595933914185

Epoch: 6| Step: 5
Training loss: 2.206674098968506
Validation loss: 2.0398430228233337

Epoch: 6| Step: 6
Training loss: 1.9468841552734375
Validation loss: 2.0340300599733987

Epoch: 6| Step: 7
Training loss: 2.971198797225952
Validation loss: 2.0501572688420615

Epoch: 6| Step: 8
Training loss: 1.757491946220398
Validation loss: 2.033118943373362

Epoch: 6| Step: 9
Training loss: 2.737919330596924
Validation loss: 2.0217872063318887

Epoch: 6| Step: 10
Training loss: 2.0229196548461914
Validation loss: 2.0127681692441306

Epoch: 6| Step: 11
Training loss: 1.9408338069915771
Validation loss: 2.005337178707123

Epoch: 6| Step: 12
Training loss: 1.7345585823059082
Validation loss: 2.004954914251963

Epoch: 6| Step: 13
Training loss: 2.6730570793151855
Validation loss: 2.0096728801727295

Epoch: 96| Step: 0
Training loss: 2.510063648223877
Validation loss: 2.0055816968282065

Epoch: 6| Step: 1
Training loss: 2.2627668380737305
Validation loss: 2.006927212079366

Epoch: 6| Step: 2
Training loss: 1.8096821308135986
Validation loss: 2.0108521580696106

Epoch: 6| Step: 3
Training loss: 1.990280270576477
Validation loss: 2.004502832889557

Epoch: 6| Step: 4
Training loss: 2.030207633972168
Validation loss: 2.0059984922409058

Epoch: 6| Step: 5
Training loss: 2.2692298889160156
Validation loss: 2.01354056596756

Epoch: 6| Step: 6
Training loss: 1.8766679763793945
Validation loss: 2.004784107208252

Epoch: 6| Step: 7
Training loss: 1.9744699001312256
Validation loss: 2.013840138912201

Epoch: 6| Step: 8
Training loss: 1.9808549880981445
Validation loss: 2.001699467500051

Epoch: 6| Step: 9
Training loss: 2.615510940551758
Validation loss: 2.0044636329015098

Epoch: 6| Step: 10
Training loss: 2.677295207977295
Validation loss: 2.00951353708903

Epoch: 6| Step: 11
Training loss: 1.9711532592773438
Validation loss: 2.0070791045824685

Epoch: 6| Step: 12
Training loss: 1.9085593223571777
Validation loss: 2.014648993810018

Epoch: 6| Step: 13
Training loss: 1.873999834060669
Validation loss: 2.0128188729286194

Epoch: 97| Step: 0
Training loss: 1.8948649168014526
Validation loss: 2.015838623046875

Epoch: 6| Step: 1
Training loss: 2.4785590171813965
Validation loss: 2.021461546421051

Epoch: 6| Step: 2
Training loss: 2.2330851554870605
Validation loss: 2.0265305240948996

Epoch: 6| Step: 3
Training loss: 2.2968106269836426
Validation loss: 2.023054083188375

Epoch: 6| Step: 4
Training loss: 1.8914930820465088
Validation loss: 2.023723065853119

Epoch: 6| Step: 5
Training loss: 1.5844974517822266
Validation loss: 2.0185183684031167

Epoch: 6| Step: 6
Training loss: 2.4352498054504395
Validation loss: 2.027370870113373

Epoch: 6| Step: 7
Training loss: 1.7581608295440674
Validation loss: 2.022838215033213

Epoch: 6| Step: 8
Training loss: 2.131222724914551
Validation loss: 2.025363822778066

Epoch: 6| Step: 9
Training loss: 2.3160760402679443
Validation loss: 2.0241827368736267

Epoch: 6| Step: 10
Training loss: 2.184512138366699
Validation loss: 2.0187928279240928

Epoch: 6| Step: 11
Training loss: 2.2662672996520996
Validation loss: 2.0159116983413696

Epoch: 6| Step: 12
Training loss: 1.8637773990631104
Validation loss: 2.0217949946721396

Epoch: 6| Step: 13
Training loss: 2.284724712371826
Validation loss: 2.019643187522888

Epoch: 98| Step: 0
Training loss: 1.987217903137207
Validation loss: 2.027368724346161

Epoch: 6| Step: 1
Training loss: 1.5457723140716553
Validation loss: 2.008603513240814

Epoch: 6| Step: 2
Training loss: 1.5715913772583008
Validation loss: 2.017990251382192

Epoch: 6| Step: 3
Training loss: 2.170710563659668
Validation loss: 2.023293952147166

Epoch: 6| Step: 4
Training loss: 1.9862122535705566
Validation loss: 2.022276779015859

Epoch: 6| Step: 5
Training loss: 2.357431650161743
Validation loss: 2.03523580233256

Epoch: 6| Step: 6
Training loss: 2.5554723739624023
Validation loss: 2.0215361515680947

Epoch: 6| Step: 7
Training loss: 2.1558663845062256
Validation loss: 2.023747901121775

Epoch: 6| Step: 8
Training loss: 2.652122974395752
Validation loss: 2.0235090851783752

Epoch: 6| Step: 9
Training loss: 1.9177035093307495
Validation loss: 2.0205185810724893

Epoch: 6| Step: 10
Training loss: 2.1533052921295166
Validation loss: 2.0336227814356485

Epoch: 6| Step: 11
Training loss: 2.0892529487609863
Validation loss: 2.024518668651581

Epoch: 6| Step: 12
Training loss: 2.2994027137756348
Validation loss: 2.0229875644048056

Epoch: 6| Step: 13
Training loss: 2.4035255908966064
Validation loss: 2.0116917490959167

Epoch: 99| Step: 0
Training loss: 1.906707763671875
Validation loss: 2.017443279425303

Epoch: 6| Step: 1
Training loss: 2.1874799728393555
Validation loss: 2.014115333557129

Epoch: 6| Step: 2
Training loss: 1.8645235300064087
Validation loss: 2.0103850960731506

Epoch: 6| Step: 3
Training loss: 2.314455509185791
Validation loss: 2.0112027327219644

Epoch: 6| Step: 4
Training loss: 1.4668464660644531
Validation loss: 2.0105243722597756

Epoch: 6| Step: 5
Training loss: 2.2118277549743652
Validation loss: 2.020976483821869

Epoch: 6| Step: 6
Training loss: 1.9687659740447998
Validation loss: 2.0178173780441284

Epoch: 6| Step: 7
Training loss: 2.3832764625549316
Validation loss: 2.015735407670339

Epoch: 6| Step: 8
Training loss: 1.3242518901824951
Validation loss: 2.026806871096293

Epoch: 6| Step: 9
Training loss: 2.32549786567688
Validation loss: 2.0201314091682434

Epoch: 6| Step: 10
Training loss: 2.4638381004333496
Validation loss: 2.024082839488983

Epoch: 6| Step: 11
Training loss: 2.2655727863311768
Validation loss: 2.0142219066619873

Epoch: 6| Step: 12
Training loss: 2.2947614192962646
Validation loss: 2.0255261659622192

Epoch: 6| Step: 13
Training loss: 2.768437385559082
Validation loss: 2.0463464657465615

Epoch: 100| Step: 0
Training loss: 2.248161792755127
Validation loss: 2.037878473599752

Epoch: 6| Step: 1
Training loss: 2.1512298583984375
Validation loss: 2.03591521581014

Epoch: 6| Step: 2
Training loss: 2.393815279006958
Validation loss: 2.022801140944163

Epoch: 6| Step: 3
Training loss: 2.497148036956787
Validation loss: 2.036699374516805

Epoch: 6| Step: 4
Training loss: 1.7718887329101562
Validation loss: 2.025725702444712

Epoch: 6| Step: 5
Training loss: 2.377758502960205
Validation loss: 2.0312587221463523

Epoch: 6| Step: 6
Training loss: 2.228936195373535
Validation loss: 2.029467463493347

Epoch: 6| Step: 7
Training loss: 2.3963534832000732
Validation loss: 2.0263107419013977

Epoch: 6| Step: 8
Training loss: 1.3442280292510986
Validation loss: 2.012649973233541

Epoch: 6| Step: 9
Training loss: 1.7882754802703857
Validation loss: 2.016766826311747

Epoch: 6| Step: 10
Training loss: 1.9557965993881226
Validation loss: 2.0216310222943625

Epoch: 6| Step: 11
Training loss: 1.6894142627716064
Validation loss: 2.0249498089154563

Epoch: 6| Step: 12
Training loss: 2.664856433868408
Validation loss: 2.0193192958831787

Epoch: 6| Step: 13
Training loss: 2.3056082725524902
Validation loss: 2.0256142616271973

Testing loss: 1.631473365447504

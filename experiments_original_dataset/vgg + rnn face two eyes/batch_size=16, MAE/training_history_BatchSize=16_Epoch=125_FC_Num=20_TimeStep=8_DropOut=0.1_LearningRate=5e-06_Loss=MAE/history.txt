Epoch: 1| Step: 0
Training loss: 6.521334648132324
Validation loss: 5.276469866434733

Epoch: 6| Step: 1
Training loss: 5.669209957122803
Validation loss: 5.274233897527059

Epoch: 6| Step: 2
Training loss: 5.042239189147949
Validation loss: 5.271933873494466

Epoch: 6| Step: 3
Training loss: 5.826957702636719
Validation loss: 5.269687970479329

Epoch: 6| Step: 4
Training loss: 4.3120622634887695
Validation loss: 5.267534891764323

Epoch: 6| Step: 5
Training loss: 5.184257984161377
Validation loss: 5.265323241551717

Epoch: 6| Step: 6
Training loss: 4.646758556365967
Validation loss: 5.263124148050944

Epoch: 6| Step: 7
Training loss: 6.317008972167969
Validation loss: 5.260926723480225

Epoch: 6| Step: 8
Training loss: 6.0227508544921875
Validation loss: 5.258711973826091

Epoch: 6| Step: 9
Training loss: 4.42009162902832
Validation loss: 5.256530125935872

Epoch: 6| Step: 10
Training loss: 4.580485820770264
Validation loss: 5.254175901412964

Epoch: 6| Step: 11
Training loss: 5.188580513000488
Validation loss: 5.251737197240193

Epoch: 6| Step: 12
Training loss: 5.082756996154785
Validation loss: 5.249215443929036

Epoch: 6| Step: 13
Training loss: 5.82481575012207
Validation loss: 5.246581077575684

Epoch: 2| Step: 0
Training loss: 4.693155765533447
Validation loss: 5.243949890136719

Epoch: 6| Step: 1
Training loss: 5.0965423583984375
Validation loss: 5.24107551574707

Epoch: 6| Step: 2
Training loss: 5.363383769989014
Validation loss: 5.238062143325806

Epoch: 6| Step: 3
Training loss: 4.601667404174805
Validation loss: 5.235086441040039

Epoch: 6| Step: 4
Training loss: 4.819808483123779
Validation loss: 5.231753587722778

Epoch: 6| Step: 5
Training loss: 6.027866363525391
Validation loss: 5.228391249974568

Epoch: 6| Step: 6
Training loss: 5.106816291809082
Validation loss: 5.224615414937337

Epoch: 6| Step: 7
Training loss: 5.553603172302246
Validation loss: 5.2207716306050616

Epoch: 6| Step: 8
Training loss: 5.592703342437744
Validation loss: 5.216854731241862

Epoch: 6| Step: 9
Training loss: 5.5455780029296875
Validation loss: 5.212405284245809

Epoch: 6| Step: 10
Training loss: 5.915329933166504
Validation loss: 5.208035548528035

Epoch: 6| Step: 11
Training loss: 5.011701583862305
Validation loss: 5.203114628791809

Epoch: 6| Step: 12
Training loss: 5.49322509765625
Validation loss: 5.198160330454509

Epoch: 6| Step: 13
Training loss: 5.247246265411377
Validation loss: 5.193121830622355

Epoch: 3| Step: 0
Training loss: 5.083443641662598
Validation loss: 5.187569459279378

Epoch: 6| Step: 1
Training loss: 5.991585731506348
Validation loss: 5.181813478469849

Epoch: 6| Step: 2
Training loss: 4.803155422210693
Validation loss: 5.175641218821208

Epoch: 6| Step: 3
Training loss: 5.384059906005859
Validation loss: 5.169394890467326

Epoch: 6| Step: 4
Training loss: 4.8668036460876465
Validation loss: 5.162873029708862

Epoch: 6| Step: 5
Training loss: 4.949886322021484
Validation loss: 5.156160593032837

Epoch: 6| Step: 6
Training loss: 5.038354873657227
Validation loss: 5.1489888827006025

Epoch: 6| Step: 7
Training loss: 5.718179702758789
Validation loss: 5.1417631308237715

Epoch: 6| Step: 8
Training loss: 5.627569198608398
Validation loss: 5.134059349695842

Epoch: 6| Step: 9
Training loss: 4.776795387268066
Validation loss: 5.126396894454956

Epoch: 6| Step: 10
Training loss: 5.451683044433594
Validation loss: 5.118285258611043

Epoch: 6| Step: 11
Training loss: 5.468908786773682
Validation loss: 5.1100380420684814

Epoch: 6| Step: 12
Training loss: 5.138118267059326
Validation loss: 5.101299365361531

Epoch: 6| Step: 13
Training loss: 4.733604907989502
Validation loss: 5.092519601186116

Epoch: 4| Step: 0
Training loss: 6.087948322296143
Validation loss: 5.0835981369018555

Epoch: 6| Step: 1
Training loss: 5.645900726318359
Validation loss: 5.0745909214019775

Epoch: 6| Step: 2
Training loss: 5.381991386413574
Validation loss: 5.065463463465373

Epoch: 6| Step: 3
Training loss: 4.503186225891113
Validation loss: 5.055742025375366

Epoch: 6| Step: 4
Training loss: 5.332217693328857
Validation loss: 5.046328663825989

Epoch: 6| Step: 5
Training loss: 5.052361488342285
Validation loss: 5.036662896474202

Epoch: 6| Step: 6
Training loss: 5.604435443878174
Validation loss: 5.027016083399455

Epoch: 6| Step: 7
Training loss: 4.158956527709961
Validation loss: 5.017000516255696

Epoch: 6| Step: 8
Training loss: 3.4853923320770264
Validation loss: 5.007271766662598

Epoch: 6| Step: 9
Training loss: 4.905543327331543
Validation loss: 4.997267484664917

Epoch: 6| Step: 10
Training loss: 5.172611236572266
Validation loss: 4.9873857498168945

Epoch: 6| Step: 11
Training loss: 5.495005130767822
Validation loss: 4.9777616659800215

Epoch: 6| Step: 12
Training loss: 5.697785377502441
Validation loss: 4.967889229456584

Epoch: 6| Step: 13
Training loss: 4.882096767425537
Validation loss: 4.958214282989502

Epoch: 5| Step: 0
Training loss: 4.931500434875488
Validation loss: 4.948294003804524

Epoch: 6| Step: 1
Training loss: 5.501181602478027
Validation loss: 4.938828627268474

Epoch: 6| Step: 2
Training loss: 4.751282691955566
Validation loss: 4.928982257843018

Epoch: 6| Step: 3
Training loss: 4.816429138183594
Validation loss: 4.919349352518718

Epoch: 6| Step: 4
Training loss: 4.024251937866211
Validation loss: 4.909879406293233

Epoch: 6| Step: 5
Training loss: 4.900599479675293
Validation loss: 4.900208950042725

Epoch: 6| Step: 6
Training loss: 5.295932769775391
Validation loss: 4.890879273414612

Epoch: 6| Step: 7
Training loss: 5.273326873779297
Validation loss: 4.881309986114502

Epoch: 6| Step: 8
Training loss: 4.923635482788086
Validation loss: 4.872279008229573

Epoch: 6| Step: 9
Training loss: 5.194524765014648
Validation loss: 4.862837791442871

Epoch: 6| Step: 10
Training loss: 4.37730598449707
Validation loss: 4.853780031204224

Epoch: 6| Step: 11
Training loss: 4.945645332336426
Validation loss: 4.844844977060954

Epoch: 6| Step: 12
Training loss: 5.922756195068359
Validation loss: 4.836210648218791

Epoch: 6| Step: 13
Training loss: 4.756219863891602
Validation loss: 4.8280711968739825

Epoch: 6| Step: 0
Training loss: 4.251259803771973
Validation loss: 4.81977915763855

Epoch: 6| Step: 1
Training loss: 5.234118461608887
Validation loss: 4.8115073045094805

Epoch: 6| Step: 2
Training loss: 5.449995517730713
Validation loss: 4.803785959879558

Epoch: 6| Step: 3
Training loss: 4.878636837005615
Validation loss: 4.795648097991943

Epoch: 6| Step: 4
Training loss: 5.309660911560059
Validation loss: 4.788253943125407

Epoch: 6| Step: 5
Training loss: 4.4619574546813965
Validation loss: 4.78032374382019

Epoch: 6| Step: 6
Training loss: 4.731563568115234
Validation loss: 4.7720052401224775

Epoch: 6| Step: 7
Training loss: 4.98984432220459
Validation loss: 4.763924598693848

Epoch: 6| Step: 8
Training loss: 5.380862712860107
Validation loss: 4.756379763285319

Epoch: 6| Step: 9
Training loss: 4.195498466491699
Validation loss: 4.74823260307312

Epoch: 6| Step: 10
Training loss: 4.578530311584473
Validation loss: 4.73980188369751

Epoch: 6| Step: 11
Training loss: 5.012404441833496
Validation loss: 4.7311515013376875

Epoch: 6| Step: 12
Training loss: 4.467825889587402
Validation loss: 4.722168922424316

Epoch: 6| Step: 13
Training loss: 5.067480564117432
Validation loss: 4.713379700978597

Epoch: 7| Step: 0
Training loss: 4.134218215942383
Validation loss: 4.70450758934021

Epoch: 6| Step: 1
Training loss: 5.212359428405762
Validation loss: 4.69538152217865

Epoch: 6| Step: 2
Training loss: 4.86328125
Validation loss: 4.686028877894084

Epoch: 6| Step: 3
Training loss: 5.55037260055542
Validation loss: 4.676442782084147

Epoch: 6| Step: 4
Training loss: 4.977478981018066
Validation loss: 4.666773398717244

Epoch: 6| Step: 5
Training loss: 4.766348838806152
Validation loss: 4.657584309577942

Epoch: 6| Step: 6
Training loss: 4.804261207580566
Validation loss: 4.647737503051758

Epoch: 6| Step: 7
Training loss: 3.8799238204956055
Validation loss: 4.637867609659831

Epoch: 6| Step: 8
Training loss: 3.8189005851745605
Validation loss: 4.628283818562825

Epoch: 6| Step: 9
Training loss: 5.8497467041015625
Validation loss: 4.619394143422444

Epoch: 6| Step: 10
Training loss: 3.9717965126037598
Validation loss: 4.610001881917317

Epoch: 6| Step: 11
Training loss: 5.0763044357299805
Validation loss: 4.601638515790303

Epoch: 6| Step: 12
Training loss: 5.077810764312744
Validation loss: 4.592521667480469

Epoch: 6| Step: 13
Training loss: 4.381068229675293
Validation loss: 4.583770751953125

Epoch: 8| Step: 0
Training loss: 4.518818378448486
Validation loss: 4.574415564537048

Epoch: 6| Step: 1
Training loss: 5.300225257873535
Validation loss: 4.565284013748169

Epoch: 6| Step: 2
Training loss: 4.822466850280762
Validation loss: 4.556463082631429

Epoch: 6| Step: 3
Training loss: 4.337634086608887
Validation loss: 4.547737161318461

Epoch: 6| Step: 4
Training loss: 5.031129837036133
Validation loss: 4.5387938022613525

Epoch: 6| Step: 5
Training loss: 3.847344398498535
Validation loss: 4.529418547948201

Epoch: 6| Step: 6
Training loss: 4.879609107971191
Validation loss: 4.5208436250686646

Epoch: 6| Step: 7
Training loss: 5.138422966003418
Validation loss: 4.512485543886821

Epoch: 6| Step: 8
Training loss: 4.6798295974731445
Validation loss: 4.504509528477986

Epoch: 6| Step: 9
Training loss: 4.724040985107422
Validation loss: 4.495940526326497

Epoch: 6| Step: 10
Training loss: 4.7013654708862305
Validation loss: 4.487517436345418

Epoch: 6| Step: 11
Training loss: 4.4141764640808105
Validation loss: 4.4796061515808105

Epoch: 6| Step: 12
Training loss: 4.020803451538086
Validation loss: 4.47154434521993

Epoch: 6| Step: 13
Training loss: 4.297842502593994
Validation loss: 4.46334973971049

Epoch: 9| Step: 0
Training loss: 5.652563095092773
Validation loss: 4.455064694086711

Epoch: 6| Step: 1
Training loss: 4.849132537841797
Validation loss: 4.446362535158793

Epoch: 6| Step: 2
Training loss: 3.497267246246338
Validation loss: 4.438651959101359

Epoch: 6| Step: 3
Training loss: 6.422433853149414
Validation loss: 4.431664387385051

Epoch: 6| Step: 4
Training loss: 4.315431594848633
Validation loss: 4.423586130142212

Epoch: 6| Step: 5
Training loss: 3.737596273422241
Validation loss: 4.415916522343953

Epoch: 6| Step: 6
Training loss: 3.7817158699035645
Validation loss: 4.407960772514343

Epoch: 6| Step: 7
Training loss: 4.012205123901367
Validation loss: 4.401424169540405

Epoch: 6| Step: 8
Training loss: 4.408264636993408
Validation loss: 4.39399774869283

Epoch: 6| Step: 9
Training loss: 5.590895652770996
Validation loss: 4.385966618855794

Epoch: 6| Step: 10
Training loss: 4.445626258850098
Validation loss: 4.378659248352051

Epoch: 6| Step: 11
Training loss: 3.3793649673461914
Validation loss: 4.37148118019104

Epoch: 6| Step: 12
Training loss: 6.126784324645996
Validation loss: 4.363080342610677

Epoch: 6| Step: 13
Training loss: 3.0673999786376953
Validation loss: 4.3561233679453535

Epoch: 10| Step: 0
Training loss: 4.501565933227539
Validation loss: 4.347426970799764

Epoch: 6| Step: 1
Training loss: 4.593991279602051
Validation loss: 4.339900255203247

Epoch: 6| Step: 2
Training loss: 3.4009296894073486
Validation loss: 4.332459568977356

Epoch: 6| Step: 3
Training loss: 5.584993362426758
Validation loss: 4.324909448623657

Epoch: 6| Step: 4
Training loss: 4.313726902008057
Validation loss: 4.317915598551433

Epoch: 6| Step: 5
Training loss: 4.425256252288818
Validation loss: 4.310151020685832

Epoch: 6| Step: 6
Training loss: 4.280063629150391
Validation loss: 4.303607940673828

Epoch: 6| Step: 7
Training loss: 3.9367077350616455
Validation loss: 4.295998732248942

Epoch: 6| Step: 8
Training loss: 4.220355987548828
Validation loss: 4.288979212443034

Epoch: 6| Step: 9
Training loss: 4.2477946281433105
Validation loss: 4.281201203664144

Epoch: 6| Step: 10
Training loss: 4.686179161071777
Validation loss: 4.274009466171265

Epoch: 6| Step: 11
Training loss: 3.9618661403656006
Validation loss: 4.267360647519429

Epoch: 6| Step: 12
Training loss: 4.682103157043457
Validation loss: 4.2608654499053955

Epoch: 6| Step: 13
Training loss: 5.125697612762451
Validation loss: 4.2540565729141235

Epoch: 11| Step: 0
Training loss: 4.658383369445801
Validation loss: 4.2466663519541425

Epoch: 6| Step: 1
Training loss: 5.491628646850586
Validation loss: 4.240333437919617

Epoch: 6| Step: 2
Training loss: 4.597574710845947
Validation loss: 4.2342028220494585

Epoch: 6| Step: 3
Training loss: 3.4576950073242188
Validation loss: 4.2275426387786865

Epoch: 6| Step: 4
Training loss: 4.27751350402832
Validation loss: 4.220751523971558

Epoch: 6| Step: 5
Training loss: 3.82643985748291
Validation loss: 4.214556535085042

Epoch: 6| Step: 6
Training loss: 4.616291046142578
Validation loss: 4.209361433982849

Epoch: 6| Step: 7
Training loss: 3.944244146347046
Validation loss: 4.2041175365448

Epoch: 6| Step: 8
Training loss: 4.620537281036377
Validation loss: 4.1975425481796265

Epoch: 6| Step: 9
Training loss: 4.480341911315918
Validation loss: 4.1925435066223145

Epoch: 6| Step: 10
Training loss: 4.423088073730469
Validation loss: 4.184677879015605

Epoch: 6| Step: 11
Training loss: 4.7599196434021
Validation loss: 4.177935282389323

Epoch: 6| Step: 12
Training loss: 4.931992530822754
Validation loss: 4.172380208969116

Epoch: 6| Step: 13
Training loss: 2.654672145843506
Validation loss: 4.167180935541789

Epoch: 12| Step: 0
Training loss: 4.743875026702881
Validation loss: 4.161118666330974

Epoch: 6| Step: 1
Training loss: 4.386662483215332
Validation loss: 4.154278715451558

Epoch: 6| Step: 2
Training loss: 3.8950135707855225
Validation loss: 4.149731715520223

Epoch: 6| Step: 3
Training loss: 5.398829936981201
Validation loss: 4.14223845799764

Epoch: 6| Step: 4
Training loss: 4.01961088180542
Validation loss: 4.137676477432251

Epoch: 6| Step: 5
Training loss: 3.124030351638794
Validation loss: 4.130906422932942

Epoch: 6| Step: 6
Training loss: 3.9656810760498047
Validation loss: 4.125091155370076

Epoch: 6| Step: 7
Training loss: 3.4931693077087402
Validation loss: 4.119391918182373

Epoch: 6| Step: 8
Training loss: 4.046013832092285
Validation loss: 4.114067316055298

Epoch: 6| Step: 9
Training loss: 4.849967002868652
Validation loss: 4.107786377271016

Epoch: 6| Step: 10
Training loss: 4.241941452026367
Validation loss: 4.101220528284709

Epoch: 6| Step: 11
Training loss: 5.099769592285156
Validation loss: 4.09540593624115

Epoch: 6| Step: 12
Training loss: 4.472951889038086
Validation loss: 4.088596741358439

Epoch: 6| Step: 13
Training loss: 3.903458595275879
Validation loss: 4.083755850791931

Epoch: 13| Step: 0
Training loss: 3.7230918407440186
Validation loss: 4.078621745109558

Epoch: 6| Step: 1
Training loss: 4.694064140319824
Validation loss: 4.072767734527588

Epoch: 6| Step: 2
Training loss: 4.019526481628418
Validation loss: 4.068101207415263

Epoch: 6| Step: 3
Training loss: 4.220348358154297
Validation loss: 4.060842116673787

Epoch: 6| Step: 4
Training loss: 3.0901150703430176
Validation loss: 4.05518905321757

Epoch: 6| Step: 5
Training loss: 4.073473930358887
Validation loss: 4.049097537994385

Epoch: 6| Step: 6
Training loss: 4.452301502227783
Validation loss: 4.044006268183391

Epoch: 6| Step: 7
Training loss: 3.956202507019043
Validation loss: 4.038477619489034

Epoch: 6| Step: 8
Training loss: 4.94725227355957
Validation loss: 4.032183726628621

Epoch: 6| Step: 9
Training loss: 4.551698684692383
Validation loss: 4.026928345362346

Epoch: 6| Step: 10
Training loss: 4.042945861816406
Validation loss: 4.022042314211528

Epoch: 6| Step: 11
Training loss: 3.920027017593384
Validation loss: 4.016386230786641

Epoch: 6| Step: 12
Training loss: 4.275762557983398
Validation loss: 4.01131808757782

Epoch: 6| Step: 13
Training loss: 4.603487014770508
Validation loss: 4.005314826965332

Epoch: 14| Step: 0
Training loss: 3.5947463512420654
Validation loss: 3.999653458595276

Epoch: 6| Step: 1
Training loss: 3.6583151817321777
Validation loss: 3.9938122431437173

Epoch: 6| Step: 2
Training loss: 3.506232976913452
Validation loss: 3.9879762729008994

Epoch: 6| Step: 3
Training loss: 4.362786769866943
Validation loss: 3.98321541150411

Epoch: 6| Step: 4
Training loss: 3.872816562652588
Validation loss: 3.9777705669403076

Epoch: 6| Step: 5
Training loss: 4.248959541320801
Validation loss: 3.972747484842936

Epoch: 6| Step: 6
Training loss: 4.236741065979004
Validation loss: 3.9691096941630044

Epoch: 6| Step: 7
Training loss: 4.311421871185303
Validation loss: 3.9622181256612143

Epoch: 6| Step: 8
Training loss: 4.154018402099609
Validation loss: 3.9574621518452964

Epoch: 6| Step: 9
Training loss: 4.1844940185546875
Validation loss: 3.9513289531071982

Epoch: 6| Step: 10
Training loss: 4.429973602294922
Validation loss: 3.9468509753545127

Epoch: 6| Step: 11
Training loss: 4.337574481964111
Validation loss: 3.94167431195577

Epoch: 6| Step: 12
Training loss: 4.9522247314453125
Validation loss: 3.9355256160100303

Epoch: 6| Step: 13
Training loss: 3.6963634490966797
Validation loss: 3.930259943008423

Epoch: 15| Step: 0
Training loss: 3.8960108757019043
Validation loss: 3.9261393547058105

Epoch: 6| Step: 1
Training loss: 4.458896636962891
Validation loss: 3.921389579772949

Epoch: 6| Step: 2
Training loss: 4.398349761962891
Validation loss: 3.915327231089274

Epoch: 6| Step: 3
Training loss: 4.494232654571533
Validation loss: 3.9095494747161865

Epoch: 6| Step: 4
Training loss: 4.183477401733398
Validation loss: 3.9041717449824014

Epoch: 6| Step: 5
Training loss: 4.778252601623535
Validation loss: 3.8989752928415933

Epoch: 6| Step: 6
Training loss: 3.3380684852600098
Validation loss: 3.8962838649749756

Epoch: 6| Step: 7
Training loss: 3.596705436706543
Validation loss: 3.8888920545578003

Epoch: 6| Step: 8
Training loss: 3.712526798248291
Validation loss: 3.883813500404358

Epoch: 6| Step: 9
Training loss: 3.923140525817871
Validation loss: 3.878631273905436

Epoch: 6| Step: 10
Training loss: 3.886631488800049
Validation loss: 3.8742857376734414

Epoch: 6| Step: 11
Training loss: 3.779911518096924
Validation loss: 3.8692208925882974

Epoch: 6| Step: 12
Training loss: 4.0216522216796875
Validation loss: 3.8645294507344565

Epoch: 6| Step: 13
Training loss: 4.116677284240723
Validation loss: 3.85920512676239

Epoch: 16| Step: 0
Training loss: 3.9862935543060303
Validation loss: 3.853419462839762

Epoch: 6| Step: 1
Training loss: 4.46764612197876
Validation loss: 3.8483908971150718

Epoch: 6| Step: 2
Training loss: 4.058538436889648
Validation loss: 3.843577822049459

Epoch: 6| Step: 3
Training loss: 3.430536985397339
Validation loss: 3.837928851445516

Epoch: 6| Step: 4
Training loss: 4.279636859893799
Validation loss: 3.834377328554789

Epoch: 6| Step: 5
Training loss: 4.15425968170166
Validation loss: 3.8296560049057007

Epoch: 6| Step: 6
Training loss: 3.519883871078491
Validation loss: 3.824356516202291

Epoch: 6| Step: 7
Training loss: 3.71944522857666
Validation loss: 3.8191935618718467

Epoch: 6| Step: 8
Training loss: 4.005435466766357
Validation loss: 3.814617156982422

Epoch: 6| Step: 9
Training loss: 3.7490813732147217
Validation loss: 3.809135317802429

Epoch: 6| Step: 10
Training loss: 3.738079309463501
Validation loss: 3.8040300607681274

Epoch: 6| Step: 11
Training loss: 4.398883819580078
Validation loss: 3.8003525336583457

Epoch: 6| Step: 12
Training loss: 4.088505744934082
Validation loss: 3.7955828507741294

Epoch: 6| Step: 13
Training loss: 4.071643829345703
Validation loss: 3.7905961672465005

Epoch: 17| Step: 0
Training loss: 3.728747844696045
Validation loss: 3.7851444085439048

Epoch: 6| Step: 1
Training loss: 3.8336753845214844
Validation loss: 3.7800671259562173

Epoch: 6| Step: 2
Training loss: 5.130873680114746
Validation loss: 3.77559224764506

Epoch: 6| Step: 3
Training loss: 4.219350814819336
Validation loss: 3.7710960308710733

Epoch: 6| Step: 4
Training loss: 3.205662727355957
Validation loss: 3.7654211123784385

Epoch: 6| Step: 5
Training loss: 4.4253764152526855
Validation loss: 3.7601869901021323

Epoch: 6| Step: 6
Training loss: 4.068911552429199
Validation loss: 3.7553049325942993

Epoch: 6| Step: 7
Training loss: 4.447813987731934
Validation loss: 3.7512123584747314

Epoch: 6| Step: 8
Training loss: 4.370110034942627
Validation loss: 3.7461092869440713

Epoch: 6| Step: 9
Training loss: 3.3930845260620117
Validation loss: 3.741083820660909

Epoch: 6| Step: 10
Training loss: 3.7808210849761963
Validation loss: 3.7364559173583984

Epoch: 6| Step: 11
Training loss: 3.3413548469543457
Validation loss: 3.7313866217931113

Epoch: 6| Step: 12
Training loss: 4.062691688537598
Validation loss: 3.7272273302078247

Epoch: 6| Step: 13
Training loss: 2.764960289001465
Validation loss: 3.722333312034607

Epoch: 18| Step: 0
Training loss: 2.677252769470215
Validation loss: 3.7170724868774414

Epoch: 6| Step: 1
Training loss: 4.795112133026123
Validation loss: 3.7121437390645347

Epoch: 6| Step: 2
Training loss: 3.3308329582214355
Validation loss: 3.708348592122396

Epoch: 6| Step: 3
Training loss: 3.7340660095214844
Validation loss: 3.703892191251119

Epoch: 6| Step: 4
Training loss: 3.876582145690918
Validation loss: 3.6994399627049765

Epoch: 6| Step: 5
Training loss: 3.0386486053466797
Validation loss: 3.695016781489054

Epoch: 6| Step: 6
Training loss: 4.539650917053223
Validation loss: 3.691640297571818

Epoch: 6| Step: 7
Training loss: 4.112288475036621
Validation loss: 3.688091238339742

Epoch: 6| Step: 8
Training loss: 4.346196174621582
Validation loss: 3.682639479637146

Epoch: 6| Step: 9
Training loss: 4.133922100067139
Validation loss: 3.6780624389648438

Epoch: 6| Step: 10
Training loss: 3.894571304321289
Validation loss: 3.6742460330327353

Epoch: 6| Step: 11
Training loss: 4.651706695556641
Validation loss: 3.670546213785807

Epoch: 6| Step: 12
Training loss: 3.271653175354004
Validation loss: 3.665369709332784

Epoch: 6| Step: 13
Training loss: 3.4861154556274414
Validation loss: 3.660221219062805

Epoch: 19| Step: 0
Training loss: 4.150363922119141
Validation loss: 3.6571539640426636

Epoch: 6| Step: 1
Training loss: 4.324563503265381
Validation loss: 3.651134649912516

Epoch: 6| Step: 2
Training loss: 3.4463601112365723
Validation loss: 3.647074739138285

Epoch: 6| Step: 3
Training loss: 3.1789350509643555
Validation loss: 3.6432802279790244

Epoch: 6| Step: 4
Training loss: 4.340053558349609
Validation loss: 3.6382673581441245

Epoch: 6| Step: 5
Training loss: 3.350250720977783
Validation loss: 3.633601943651835

Epoch: 6| Step: 6
Training loss: 4.477446556091309
Validation loss: 3.62984033425649

Epoch: 6| Step: 7
Training loss: 3.957030773162842
Validation loss: 3.62495493888855

Epoch: 6| Step: 8
Training loss: 3.8435001373291016
Validation loss: 3.6209035317103067

Epoch: 6| Step: 9
Training loss: 3.6892685890197754
Validation loss: 3.617212255795797

Epoch: 6| Step: 10
Training loss: 5.123478889465332
Validation loss: 3.6138028701146445

Epoch: 6| Step: 11
Training loss: 3.2648003101348877
Validation loss: 3.6102814277013144

Epoch: 6| Step: 12
Training loss: 2.7600624561309814
Validation loss: 3.603111147880554

Epoch: 6| Step: 13
Training loss: 3.179069995880127
Validation loss: 3.598818063735962

Epoch: 20| Step: 0
Training loss: 4.327070236206055
Validation loss: 3.595093528429667

Epoch: 6| Step: 1
Training loss: 3.391019344329834
Validation loss: 3.590906858444214

Epoch: 6| Step: 2
Training loss: 4.055532455444336
Validation loss: 3.5863803227742515

Epoch: 6| Step: 3
Training loss: 2.9657037258148193
Validation loss: 3.582645297050476

Epoch: 6| Step: 4
Training loss: 4.36720085144043
Validation loss: 3.5779693126678467

Epoch: 6| Step: 5
Training loss: 3.6131935119628906
Validation loss: 3.57367213567098

Epoch: 6| Step: 6
Training loss: 3.7603845596313477
Validation loss: 3.5710013707478843

Epoch: 6| Step: 7
Training loss: 3.1379342079162598
Validation loss: 3.5652946631113687

Epoch: 6| Step: 8
Training loss: 3.6431751251220703
Validation loss: 3.5611642599105835

Epoch: 6| Step: 9
Training loss: 4.0966386795043945
Validation loss: 3.5565893252690635

Epoch: 6| Step: 10
Training loss: 4.527834892272949
Validation loss: 3.551858067512512

Epoch: 6| Step: 11
Training loss: 3.1632490158081055
Validation loss: 3.5467169682184854

Epoch: 6| Step: 12
Training loss: 4.166040897369385
Validation loss: 3.542332331339518

Epoch: 6| Step: 13
Training loss: 3.025905132293701
Validation loss: 3.5370483001073203

Epoch: 21| Step: 0
Training loss: 3.170497417449951
Validation loss: 3.5332123041152954

Epoch: 6| Step: 1
Training loss: 4.458091735839844
Validation loss: 3.527819792429606

Epoch: 6| Step: 2
Training loss: 3.554433584213257
Validation loss: 3.522216002146403

Epoch: 6| Step: 3
Training loss: 3.598684787750244
Validation loss: 3.5170690615971885

Epoch: 6| Step: 4
Training loss: 2.9667186737060547
Validation loss: 3.5117114782333374

Epoch: 6| Step: 5
Training loss: 3.605529308319092
Validation loss: 3.507420460383097

Epoch: 6| Step: 6
Training loss: 3.4254424571990967
Validation loss: 3.5026313066482544

Epoch: 6| Step: 7
Training loss: 3.572309970855713
Validation loss: 3.498803456624349

Epoch: 6| Step: 8
Training loss: 3.7240822315216064
Validation loss: 3.4939244190851846

Epoch: 6| Step: 9
Training loss: 3.6692955493927
Validation loss: 3.489444375038147

Epoch: 6| Step: 10
Training loss: 3.7769062519073486
Validation loss: 3.4849239190419516

Epoch: 6| Step: 11
Training loss: 3.770317792892456
Validation loss: 3.480188767115275

Epoch: 6| Step: 12
Training loss: 3.934217691421509
Validation loss: 3.477720618247986

Epoch: 6| Step: 13
Training loss: 4.139473915100098
Validation loss: 3.472929914792379

Epoch: 22| Step: 0
Training loss: 3.2363743782043457
Validation loss: 3.467676122983297

Epoch: 6| Step: 1
Training loss: 3.187523365020752
Validation loss: 3.4639841318130493

Epoch: 6| Step: 2
Training loss: 4.245832443237305
Validation loss: 3.4616405963897705

Epoch: 6| Step: 3
Training loss: 2.7214293479919434
Validation loss: 3.458197236061096

Epoch: 6| Step: 4
Training loss: 4.434422969818115
Validation loss: 3.452471295992533

Epoch: 6| Step: 5
Training loss: 3.446830987930298
Validation loss: 3.446389079093933

Epoch: 6| Step: 6
Training loss: 2.7714784145355225
Validation loss: 3.4421512285868325

Epoch: 6| Step: 7
Training loss: 3.315819263458252
Validation loss: 3.4377933343251548

Epoch: 6| Step: 8
Training loss: 3.930431365966797
Validation loss: 3.4341370264689126

Epoch: 6| Step: 9
Training loss: 4.355002403259277
Validation loss: 3.431225577990214

Epoch: 6| Step: 10
Training loss: 3.4044227600097656
Validation loss: 3.424553910891215

Epoch: 6| Step: 11
Training loss: 3.6151673793792725
Validation loss: 3.4195849895477295

Epoch: 6| Step: 12
Training loss: 3.5311970710754395
Validation loss: 3.415986180305481

Epoch: 6| Step: 13
Training loss: 4.3261590003967285
Validation loss: 3.4118481874465942

Epoch: 23| Step: 0
Training loss: 4.326305389404297
Validation loss: 3.405421257019043

Epoch: 6| Step: 1
Training loss: 2.700911521911621
Validation loss: 3.399638811747233

Epoch: 6| Step: 2
Training loss: 3.287879705429077
Validation loss: 3.393444577852885

Epoch: 6| Step: 3
Training loss: 3.3298816680908203
Validation loss: 3.38813849290212

Epoch: 6| Step: 4
Training loss: 3.715397357940674
Validation loss: 3.382658044497172

Epoch: 6| Step: 5
Training loss: 3.6337473392486572
Validation loss: 3.377593437830607

Epoch: 6| Step: 6
Training loss: 4.1490864753723145
Validation loss: 3.373479127883911

Epoch: 6| Step: 7
Training loss: 3.2435646057128906
Validation loss: 3.368704160054525

Epoch: 6| Step: 8
Training loss: 3.387136459350586
Validation loss: 3.364610473314921

Epoch: 6| Step: 9
Training loss: 3.5618655681610107
Validation loss: 3.3597175677617392

Epoch: 6| Step: 10
Training loss: 3.5051333904266357
Validation loss: 3.3547010819117227

Epoch: 6| Step: 11
Training loss: 3.6571946144104004
Validation loss: 3.349746267000834

Epoch: 6| Step: 12
Training loss: 3.327291965484619
Validation loss: 3.3455439805984497

Epoch: 6| Step: 13
Training loss: 3.7689270973205566
Validation loss: 3.3406986792882285

Epoch: 24| Step: 0
Training loss: 2.2159738540649414
Validation loss: 3.3364284435908

Epoch: 6| Step: 1
Training loss: 3.6343231201171875
Validation loss: 3.3321752150853476

Epoch: 6| Step: 2
Training loss: 3.8776767253875732
Validation loss: 3.327663779258728

Epoch: 6| Step: 3
Training loss: 4.052851676940918
Validation loss: 3.3233915170033774

Epoch: 6| Step: 4
Training loss: 2.8161568641662598
Validation loss: 3.318596680959066

Epoch: 6| Step: 5
Training loss: 4.2001261711120605
Validation loss: 3.314969460169474

Epoch: 6| Step: 6
Training loss: 3.1742701530456543
Validation loss: 3.3116905291875205

Epoch: 6| Step: 7
Training loss: 3.6602344512939453
Validation loss: 3.3057034015655518

Epoch: 6| Step: 8
Training loss: 3.8151443004608154
Validation loss: 3.3013768593470254

Epoch: 6| Step: 9
Training loss: 3.886066436767578
Validation loss: 3.2992012103398642

Epoch: 6| Step: 10
Training loss: 3.2492847442626953
Validation loss: 3.293537219365438

Epoch: 6| Step: 11
Training loss: 3.150858163833618
Validation loss: 3.2898831367492676

Epoch: 6| Step: 12
Training loss: 4.099318981170654
Validation loss: 3.286206046740214

Epoch: 6| Step: 13
Training loss: 2.915924072265625
Validation loss: 3.2813713550567627

Epoch: 25| Step: 0
Training loss: 3.3155994415283203
Validation loss: 3.27737287680308

Epoch: 6| Step: 1
Training loss: 3.358583450317383
Validation loss: 3.273023327191671

Epoch: 6| Step: 2
Training loss: 4.351053237915039
Validation loss: 3.269390106201172

Epoch: 6| Step: 3
Training loss: 3.2706141471862793
Validation loss: 3.2657717863718667

Epoch: 6| Step: 4
Training loss: 2.599365234375
Validation loss: 3.261288563410441

Epoch: 6| Step: 5
Training loss: 2.531491279602051
Validation loss: 3.2577008803685508

Epoch: 6| Step: 6
Training loss: 3.8046653270721436
Validation loss: 3.2538556257883706

Epoch: 6| Step: 7
Training loss: 3.509139060974121
Validation loss: 3.2503309647242227

Epoch: 6| Step: 8
Training loss: 3.902954339981079
Validation loss: 3.2456307808558145

Epoch: 6| Step: 9
Training loss: 4.043710708618164
Validation loss: 3.2414968609809875

Epoch: 6| Step: 10
Training loss: 3.4058589935302734
Validation loss: 3.2378402948379517

Epoch: 6| Step: 11
Training loss: 2.9950876235961914
Validation loss: 3.233482837677002

Epoch: 6| Step: 12
Training loss: 2.6754250526428223
Validation loss: 3.2293801307678223

Epoch: 6| Step: 13
Training loss: 4.214232444763184
Validation loss: 3.2256322701772056

Epoch: 26| Step: 0
Training loss: 2.933407783508301
Validation loss: 3.2210753361384072

Epoch: 6| Step: 1
Training loss: 2.7624058723449707
Validation loss: 3.2169333696365356

Epoch: 6| Step: 2
Training loss: 2.888864040374756
Validation loss: 3.2131015062332153

Epoch: 6| Step: 3
Training loss: 2.3368754386901855
Validation loss: 3.2089982430140176

Epoch: 6| Step: 4
Training loss: 3.375309467315674
Validation loss: 3.205069104830424

Epoch: 6| Step: 5
Training loss: 3.305295705795288
Validation loss: 3.200996438662211

Epoch: 6| Step: 6
Training loss: 3.318373680114746
Validation loss: 3.196711699167887

Epoch: 6| Step: 7
Training loss: 2.9312117099761963
Validation loss: 3.192408045132955

Epoch: 6| Step: 8
Training loss: 4.428377628326416
Validation loss: 3.1882277727127075

Epoch: 6| Step: 9
Training loss: 3.4077906608581543
Validation loss: 3.183905084927877

Epoch: 6| Step: 10
Training loss: 3.780961513519287
Validation loss: 3.17965304851532

Epoch: 6| Step: 11
Training loss: 4.214033126831055
Validation loss: 3.175621191660563

Epoch: 6| Step: 12
Training loss: 3.5873379707336426
Validation loss: 3.171193838119507

Epoch: 6| Step: 13
Training loss: 3.9852447509765625
Validation loss: 3.167020837465922

Epoch: 27| Step: 0
Training loss: 3.307316780090332
Validation loss: 3.1624321937561035

Epoch: 6| Step: 1
Training loss: 4.445107460021973
Validation loss: 3.1582497358322144

Epoch: 6| Step: 2
Training loss: 3.4192023277282715
Validation loss: 3.153932054837545

Epoch: 6| Step: 3
Training loss: 3.132711410522461
Validation loss: 3.149659355481466

Epoch: 6| Step: 4
Training loss: 3.487198829650879
Validation loss: 3.1453105211257935

Epoch: 6| Step: 5
Training loss: 4.510146141052246
Validation loss: 3.1412209272384644

Epoch: 6| Step: 6
Training loss: 4.28798770904541
Validation loss: 3.13652241230011

Epoch: 6| Step: 7
Training loss: 2.94118595123291
Validation loss: 3.132245659828186

Epoch: 6| Step: 8
Training loss: 2.1495723724365234
Validation loss: 3.127908150355021

Epoch: 6| Step: 9
Training loss: 3.3602218627929688
Validation loss: 3.123523990313212

Epoch: 6| Step: 10
Training loss: 2.822502613067627
Validation loss: 3.1194987297058105

Epoch: 6| Step: 11
Training loss: 2.6713790893554688
Validation loss: 3.1154038111368814

Epoch: 6| Step: 12
Training loss: 2.9982359409332275
Validation loss: 3.111482342084249

Epoch: 6| Step: 13
Training loss: 3.0216760635375977
Validation loss: 3.1072502930959067

Epoch: 28| Step: 0
Training loss: 4.201740264892578
Validation loss: 3.103832999865214

Epoch: 6| Step: 1
Training loss: 3.3983778953552246
Validation loss: 3.1004501581192017

Epoch: 6| Step: 2
Training loss: 3.7219061851501465
Validation loss: 3.0967502991358438

Epoch: 6| Step: 3
Training loss: 3.0918190479278564
Validation loss: 3.093070308367411

Epoch: 6| Step: 4
Training loss: 2.0755186080932617
Validation loss: 3.089366356531779

Epoch: 6| Step: 5
Training loss: 2.723888874053955
Validation loss: 3.0861169894536338

Epoch: 6| Step: 6
Training loss: 2.8213014602661133
Validation loss: 3.0829402208328247

Epoch: 6| Step: 7
Training loss: 3.7175710201263428
Validation loss: 3.079747041066488

Epoch: 6| Step: 8
Training loss: 3.5080933570861816
Validation loss: 3.076754172643026

Epoch: 6| Step: 9
Training loss: 3.177595376968384
Validation loss: 3.073413530985514

Epoch: 6| Step: 10
Training loss: 3.6940531730651855
Validation loss: 3.0703248977661133

Epoch: 6| Step: 11
Training loss: 2.9724695682525635
Validation loss: 3.0668298403422036

Epoch: 6| Step: 12
Training loss: 3.476644992828369
Validation loss: 3.0631150007247925

Epoch: 6| Step: 13
Training loss: 3.2075414657592773
Validation loss: 3.059895714124044

Epoch: 29| Step: 0
Training loss: 2.7316434383392334
Validation loss: 3.0562036832173667

Epoch: 6| Step: 1
Training loss: 3.2974109649658203
Validation loss: 3.0524837970733643

Epoch: 6| Step: 2
Training loss: 3.5707900524139404
Validation loss: 3.0493995745976767

Epoch: 6| Step: 3
Training loss: 3.27717661857605
Validation loss: 3.0456865231196084

Epoch: 6| Step: 4
Training loss: 4.054569721221924
Validation loss: 3.0419124762217202

Epoch: 6| Step: 5
Training loss: 3.655087947845459
Validation loss: 3.0384151140848794

Epoch: 6| Step: 6
Training loss: 3.215463638305664
Validation loss: 3.034784952799479

Epoch: 6| Step: 7
Training loss: 2.9759507179260254
Validation loss: 3.030829429626465

Epoch: 6| Step: 8
Training loss: 2.832193613052368
Validation loss: 3.0268342097600303

Epoch: 6| Step: 9
Training loss: 3.0477476119995117
Validation loss: 3.0231456756591797

Epoch: 6| Step: 10
Training loss: 3.3553075790405273
Validation loss: 3.0192533334096274

Epoch: 6| Step: 11
Training loss: 3.4463162422180176
Validation loss: 3.015339811642965

Epoch: 6| Step: 12
Training loss: 3.3366646766662598
Validation loss: 3.011749505996704

Epoch: 6| Step: 13
Training loss: 2.364445686340332
Validation loss: 3.008033116658529

Epoch: 30| Step: 0
Training loss: 3.7320847511291504
Validation loss: 3.0045893589655557

Epoch: 6| Step: 1
Training loss: 3.616847515106201
Validation loss: 3.001004219055176

Epoch: 6| Step: 2
Training loss: 3.2224700450897217
Validation loss: 2.997306783994039

Epoch: 6| Step: 3
Training loss: 2.85685396194458
Validation loss: 2.993837833404541

Epoch: 6| Step: 4
Training loss: 3.1491315364837646
Validation loss: 2.9900431632995605

Epoch: 6| Step: 5
Training loss: 3.296403169631958
Validation loss: 2.986893892288208

Epoch: 6| Step: 6
Training loss: 3.774984359741211
Validation loss: 2.982761104901632

Epoch: 6| Step: 7
Training loss: 3.2450315952301025
Validation loss: 2.979502479235331

Epoch: 6| Step: 8
Training loss: 3.2340774536132812
Validation loss: 2.9757431745529175

Epoch: 6| Step: 9
Training loss: 2.4798827171325684
Validation loss: 2.9727569023768106

Epoch: 6| Step: 10
Training loss: 3.674318552017212
Validation loss: 2.968842705090841

Epoch: 6| Step: 11
Training loss: 2.9056077003479004
Validation loss: 2.9653550386428833

Epoch: 6| Step: 12
Training loss: 2.64864444732666
Validation loss: 2.9621192614237466

Epoch: 6| Step: 13
Training loss: 2.6729650497436523
Validation loss: 2.9588496685028076

Epoch: 31| Step: 0
Training loss: 3.957624673843384
Validation loss: 2.955689469973246

Epoch: 6| Step: 1
Training loss: 2.808990001678467
Validation loss: 2.9520872433980307

Epoch: 6| Step: 2
Training loss: 3.3846254348754883
Validation loss: 2.949007987976074

Epoch: 6| Step: 3
Training loss: 3.220905303955078
Validation loss: 2.945813616116842

Epoch: 6| Step: 4
Training loss: 3.4975101947784424
Validation loss: 2.942517360051473

Epoch: 6| Step: 5
Training loss: 2.2010154724121094
Validation loss: 2.9390496015548706

Epoch: 6| Step: 6
Training loss: 3.249725818634033
Validation loss: 2.9358837604522705

Epoch: 6| Step: 7
Training loss: 3.1393935680389404
Validation loss: 2.9322464068730674

Epoch: 6| Step: 8
Training loss: 3.0459203720092773
Validation loss: 2.928720752398173

Epoch: 6| Step: 9
Training loss: 2.6589126586914062
Validation loss: 2.925380746523539

Epoch: 6| Step: 10
Training loss: 3.0529322624206543
Validation loss: 2.921745498975118

Epoch: 6| Step: 11
Training loss: 3.1292073726654053
Validation loss: 2.91840926806132

Epoch: 6| Step: 12
Training loss: 3.217711925506592
Validation loss: 2.915317634741465

Epoch: 6| Step: 13
Training loss: 3.338129758834839
Validation loss: 2.9120227098464966

Epoch: 32| Step: 0
Training loss: 2.882603883743286
Validation loss: 2.908684492111206

Epoch: 6| Step: 1
Training loss: 2.8987646102905273
Validation loss: 2.904821236928304

Epoch: 6| Step: 2
Training loss: 2.941594123840332
Validation loss: 2.9016863902409873

Epoch: 6| Step: 3
Training loss: 2.9605093002319336
Validation loss: 2.898630420366923

Epoch: 6| Step: 4
Training loss: 3.2264585494995117
Validation loss: 2.8952196836471558

Epoch: 6| Step: 5
Training loss: 3.3784983158111572
Validation loss: 2.89179527759552

Epoch: 6| Step: 6
Training loss: 2.553557872772217
Validation loss: 2.8889363606770835

Epoch: 6| Step: 7
Training loss: 3.1574554443359375
Validation loss: 2.8853197495142617

Epoch: 6| Step: 8
Training loss: 2.9190804958343506
Validation loss: 2.8823522329330444

Epoch: 6| Step: 9
Training loss: 3.010498523712158
Validation loss: 2.8787200848261514

Epoch: 6| Step: 10
Training loss: 3.6480207443237305
Validation loss: 2.8751195271809897

Epoch: 6| Step: 11
Training loss: 3.07275128364563
Validation loss: 2.871705651283264

Epoch: 6| Step: 12
Training loss: 3.214521884918213
Validation loss: 2.868662675221761

Epoch: 6| Step: 13
Training loss: 3.4486076831817627
Validation loss: 2.86494517326355

Epoch: 33| Step: 0
Training loss: 2.920619249343872
Validation loss: 2.8615696827570596

Epoch: 6| Step: 1
Training loss: 3.4838857650756836
Validation loss: 2.858425815900167

Epoch: 6| Step: 2
Training loss: 3.4166202545166016
Validation loss: 2.8552993535995483

Epoch: 6| Step: 3
Training loss: 3.6501657962799072
Validation loss: 2.8514649868011475

Epoch: 6| Step: 4
Training loss: 2.374342441558838
Validation loss: 2.8481501738230386

Epoch: 6| Step: 5
Training loss: 2.1366472244262695
Validation loss: 2.8446929454803467

Epoch: 6| Step: 6
Training loss: 3.238316059112549
Validation loss: 2.842165549596151

Epoch: 6| Step: 7
Training loss: 2.662808418273926
Validation loss: 2.838862498601278

Epoch: 6| Step: 8
Training loss: 3.2677087783813477
Validation loss: 2.8351344664891562

Epoch: 6| Step: 9
Training loss: 2.643766403198242
Validation loss: 2.8323910236358643

Epoch: 6| Step: 10
Training loss: 3.038877248764038
Validation loss: 2.8294458389282227

Epoch: 6| Step: 11
Training loss: 3.1392908096313477
Validation loss: 2.8257521788279214

Epoch: 6| Step: 12
Training loss: 3.937375545501709
Validation loss: 2.82288924853007

Epoch: 6| Step: 13
Training loss: 2.8260576725006104
Validation loss: 2.81943412621816

Epoch: 34| Step: 0
Training loss: 3.221320629119873
Validation loss: 2.816252867380778

Epoch: 6| Step: 1
Training loss: 2.629885196685791
Validation loss: 2.8129120667775473

Epoch: 6| Step: 2
Training loss: 3.50470232963562
Validation loss: 2.8102031151453652

Epoch: 6| Step: 3
Training loss: 2.304823160171509
Validation loss: 2.806997855504354

Epoch: 6| Step: 4
Training loss: 2.6629128456115723
Validation loss: 2.804126779238383

Epoch: 6| Step: 5
Training loss: 3.390320062637329
Validation loss: 2.8012777169545493

Epoch: 6| Step: 6
Training loss: 2.7752885818481445
Validation loss: 2.798544963200887

Epoch: 6| Step: 7
Training loss: 3.398642063140869
Validation loss: 2.7956175009409585

Epoch: 6| Step: 8
Training loss: 2.401909828186035
Validation loss: 2.792699615160624

Epoch: 6| Step: 9
Training loss: 3.4955382347106934
Validation loss: 2.789841334025065

Epoch: 6| Step: 10
Training loss: 3.4299726486206055
Validation loss: 2.78692360719045

Epoch: 6| Step: 11
Training loss: 2.691173553466797
Validation loss: 2.7840633392333984

Epoch: 6| Step: 12
Training loss: 2.6697335243225098
Validation loss: 2.78063972791036

Epoch: 6| Step: 13
Training loss: 3.5602123737335205
Validation loss: 2.7778457403182983

Epoch: 35| Step: 0
Training loss: 2.799006938934326
Validation loss: 2.774300694465637

Epoch: 6| Step: 1
Training loss: 3.903313636779785
Validation loss: 2.7715837160746255

Epoch: 6| Step: 2
Training loss: 2.673259735107422
Validation loss: 2.7682023843129477

Epoch: 6| Step: 3
Training loss: 3.2842936515808105
Validation loss: 2.7644908825556436

Epoch: 6| Step: 4
Training loss: 2.5235133171081543
Validation loss: 2.762672543525696

Epoch: 6| Step: 5
Training loss: 2.821040153503418
Validation loss: 2.7589011987050376

Epoch: 6| Step: 6
Training loss: 3.069108247756958
Validation loss: 2.756653149922689

Epoch: 6| Step: 7
Training loss: 2.8235607147216797
Validation loss: 2.752968668937683

Epoch: 6| Step: 8
Training loss: 3.0626296997070312
Validation loss: 2.7513164281845093

Epoch: 6| Step: 9
Training loss: 2.6301004886627197
Validation loss: 2.748259425163269

Epoch: 6| Step: 10
Training loss: 3.0093462467193604
Validation loss: 2.744069536526998

Epoch: 6| Step: 11
Training loss: 3.029008626937866
Validation loss: 2.7427894274393716

Epoch: 6| Step: 12
Training loss: 2.6372060775756836
Validation loss: 2.7397667169570923

Epoch: 6| Step: 13
Training loss: 3.2777938842773438
Validation loss: 2.736253579457601

Epoch: 36| Step: 0
Training loss: 2.8117899894714355
Validation loss: 2.733560482660929

Epoch: 6| Step: 1
Training loss: 2.285717487335205
Validation loss: 2.7310166358947754

Epoch: 6| Step: 2
Training loss: 3.3537240028381348
Validation loss: 2.728110194206238

Epoch: 6| Step: 3
Training loss: 2.422145366668701
Validation loss: 2.7259851694107056

Epoch: 6| Step: 4
Training loss: 3.225956439971924
Validation loss: 2.7233822345733643

Epoch: 6| Step: 5
Training loss: 3.068014621734619
Validation loss: 2.720121701558431

Epoch: 6| Step: 6
Training loss: 2.1163249015808105
Validation loss: 2.716405669848124

Epoch: 6| Step: 7
Training loss: 2.8269729614257812
Validation loss: 2.713165362675985

Epoch: 6| Step: 8
Training loss: 3.486513376235962
Validation loss: 2.7102957566579184

Epoch: 6| Step: 9
Training loss: 3.033301591873169
Validation loss: 2.709233601888021

Epoch: 6| Step: 10
Training loss: 3.184030294418335
Validation loss: 2.713015874226888

Epoch: 6| Step: 11
Training loss: 2.757563591003418
Validation loss: 2.702011307080587

Epoch: 6| Step: 12
Training loss: 3.534796953201294
Validation loss: 2.699981908003489

Epoch: 6| Step: 13
Training loss: 2.824604034423828
Validation loss: 2.698087235291799

Epoch: 37| Step: 0
Training loss: 2.7133402824401855
Validation loss: 2.696001966794332

Epoch: 6| Step: 1
Training loss: 3.083418369293213
Validation loss: 2.6961685617764792

Epoch: 6| Step: 2
Training loss: 2.7952699661254883
Validation loss: 2.695544719696045

Epoch: 6| Step: 3
Training loss: 3.4738717079162598
Validation loss: 2.69094971815745

Epoch: 6| Step: 4
Training loss: 2.7853667736053467
Validation loss: 2.6863317092259726

Epoch: 6| Step: 5
Training loss: 2.8762125968933105
Validation loss: 2.682275414466858

Epoch: 6| Step: 6
Training loss: 2.9244110584259033
Validation loss: 2.6770130395889282

Epoch: 6| Step: 7
Training loss: 2.727155923843384
Validation loss: 2.673546036084493

Epoch: 6| Step: 8
Training loss: 2.879204034805298
Validation loss: 2.6691988507906594

Epoch: 6| Step: 9
Training loss: 3.4422919750213623
Validation loss: 2.6667126615842185

Epoch: 6| Step: 10
Training loss: 3.475149154663086
Validation loss: 2.6633706092834473

Epoch: 6| Step: 11
Training loss: 3.100048542022705
Validation loss: 2.6595948139826455

Epoch: 6| Step: 12
Training loss: 1.9299769401550293
Validation loss: 2.6576004028320312

Epoch: 6| Step: 13
Training loss: 2.1478898525238037
Validation loss: 2.6566017866134644

Epoch: 38| Step: 0
Training loss: 2.8133230209350586
Validation loss: 2.656336784362793

Epoch: 6| Step: 1
Training loss: 2.9332330226898193
Validation loss: 2.6467807292938232

Epoch: 6| Step: 2
Training loss: 3.445368766784668
Validation loss: 2.644656856854757

Epoch: 6| Step: 3
Training loss: 2.618104934692383
Validation loss: 2.6424648761749268

Epoch: 6| Step: 4
Training loss: 2.4141876697540283
Validation loss: 2.6396430333455405

Epoch: 6| Step: 5
Training loss: 3.1552958488464355
Validation loss: 2.63764222462972

Epoch: 6| Step: 6
Training loss: 2.9421122074127197
Validation loss: 2.6349730491638184

Epoch: 6| Step: 7
Training loss: 2.5674853324890137
Validation loss: 2.63338311513265

Epoch: 6| Step: 8
Training loss: 2.5900251865386963
Validation loss: 2.631260871887207

Epoch: 6| Step: 9
Training loss: 2.6687209606170654
Validation loss: 2.6273988485336304

Epoch: 6| Step: 10
Training loss: 2.695849895477295
Validation loss: 2.6238234837849936

Epoch: 6| Step: 11
Training loss: 3.287980079650879
Validation loss: 2.6202358404795327

Epoch: 6| Step: 12
Training loss: 2.9963531494140625
Validation loss: 2.6165490547815957

Epoch: 6| Step: 13
Training loss: 2.5843751430511475
Validation loss: 2.6131515900293985

Epoch: 39| Step: 0
Training loss: 2.804461717605591
Validation loss: 2.6095670064290366

Epoch: 6| Step: 1
Training loss: 2.8962111473083496
Validation loss: 2.605407436688741

Epoch: 6| Step: 2
Training loss: 3.1336889266967773
Validation loss: 2.602514147758484

Epoch: 6| Step: 3
Training loss: 3.1049866676330566
Validation loss: 2.5983036359151206

Epoch: 6| Step: 4
Training loss: 2.1435091495513916
Validation loss: 2.5946042935053506

Epoch: 6| Step: 5
Training loss: 2.8598995208740234
Validation loss: 2.5912224849065146

Epoch: 6| Step: 6
Training loss: 2.966169595718384
Validation loss: 2.5886135498682656

Epoch: 6| Step: 7
Training loss: 3.3500776290893555
Validation loss: 2.585402488708496

Epoch: 6| Step: 8
Training loss: 2.1182756423950195
Validation loss: 2.582485874493917

Epoch: 6| Step: 9
Training loss: 2.519611358642578
Validation loss: 2.5796249310175576

Epoch: 6| Step: 10
Training loss: 2.467247724533081
Validation loss: 2.577095150947571

Epoch: 6| Step: 11
Training loss: 2.520416498184204
Validation loss: 2.573780059814453

Epoch: 6| Step: 12
Training loss: 3.192995071411133
Validation loss: 2.5714975595474243

Epoch: 6| Step: 13
Training loss: 3.0129141807556152
Validation loss: 2.568365772565206

Epoch: 40| Step: 0
Training loss: 2.4714269638061523
Validation loss: 2.5652901331583657

Epoch: 6| Step: 1
Training loss: 3.18453311920166
Validation loss: 2.5628283421198526

Epoch: 6| Step: 2
Training loss: 2.655158519744873
Validation loss: 2.5589511593182883

Epoch: 6| Step: 3
Training loss: 2.6716184616088867
Validation loss: 2.5566150744756064

Epoch: 6| Step: 4
Training loss: 2.3500638008117676
Validation loss: 2.5536511143048606

Epoch: 6| Step: 5
Training loss: 3.2252392768859863
Validation loss: 2.5527997414271035

Epoch: 6| Step: 6
Training loss: 2.606841564178467
Validation loss: 2.54885462919871

Epoch: 6| Step: 7
Training loss: 2.7621779441833496
Validation loss: 2.545607884724935

Epoch: 6| Step: 8
Training loss: 2.6403355598449707
Validation loss: 2.5423242251078286

Epoch: 6| Step: 9
Training loss: 3.2697346210479736
Validation loss: 2.5403725703557334

Epoch: 6| Step: 10
Training loss: 1.8160319328308105
Validation loss: 2.5365079641342163

Epoch: 6| Step: 11
Training loss: 3.1813833713531494
Validation loss: 2.5351279973983765

Epoch: 6| Step: 12
Training loss: 2.7902846336364746
Validation loss: 2.531133453051249

Epoch: 6| Step: 13
Training loss: 2.8326306343078613
Validation loss: 2.529308636983236

Epoch: 41| Step: 0
Training loss: 2.626422643661499
Validation loss: 2.525140563646952

Epoch: 6| Step: 1
Training loss: 3.199080467224121
Validation loss: 2.5220210552215576

Epoch: 6| Step: 2
Training loss: 2.844364881515503
Validation loss: 2.517684976259867

Epoch: 6| Step: 3
Training loss: 2.460822582244873
Validation loss: 2.5212321678797402

Epoch: 6| Step: 4
Training loss: 2.38755464553833
Validation loss: 2.5145813624064126

Epoch: 6| Step: 5
Training loss: 2.0224175453186035
Validation loss: 2.5096388260523477

Epoch: 6| Step: 6
Training loss: 1.9826679229736328
Validation loss: 2.5088303089141846

Epoch: 6| Step: 7
Training loss: 2.675253391265869
Validation loss: 2.504796862602234

Epoch: 6| Step: 8
Training loss: 2.8172767162323
Validation loss: 2.5043157736460366

Epoch: 6| Step: 9
Training loss: 3.2727584838867188
Validation loss: 2.501686970392863

Epoch: 6| Step: 10
Training loss: 2.928088903427124
Validation loss: 2.498319466908773

Epoch: 6| Step: 11
Training loss: 2.4352409839630127
Validation loss: 2.495271404584249

Epoch: 6| Step: 12
Training loss: 3.121983051300049
Validation loss: 2.4922178586324057

Epoch: 6| Step: 13
Training loss: 3.0783324241638184
Validation loss: 2.4926671187082925

Epoch: 42| Step: 0
Training loss: 2.8299455642700195
Validation loss: 2.4888956546783447

Epoch: 6| Step: 1
Training loss: 2.2799220085144043
Validation loss: 2.486316998799642

Epoch: 6| Step: 2
Training loss: 2.4504761695861816
Validation loss: 2.4846585988998413

Epoch: 6| Step: 3
Training loss: 2.6912131309509277
Validation loss: 2.4832562605539956

Epoch: 6| Step: 4
Training loss: 2.851625442504883
Validation loss: 2.4807482957839966

Epoch: 6| Step: 5
Training loss: 1.9353611469268799
Validation loss: 2.4795414010683694

Epoch: 6| Step: 6
Training loss: 3.4014525413513184
Validation loss: 2.4757715463638306

Epoch: 6| Step: 7
Training loss: 3.7678611278533936
Validation loss: 2.4720436334609985

Epoch: 6| Step: 8
Training loss: 2.162895441055298
Validation loss: 2.4653242031733194

Epoch: 6| Step: 9
Training loss: 2.0738964080810547
Validation loss: 2.4625874757766724

Epoch: 6| Step: 10
Training loss: 3.087205648422241
Validation loss: 2.4588717222213745

Epoch: 6| Step: 11
Training loss: 2.617293357849121
Validation loss: 2.455068667729696

Epoch: 6| Step: 12
Training loss: 2.3641281127929688
Validation loss: 2.4507803916931152

Epoch: 6| Step: 13
Training loss: 2.7703521251678467
Validation loss: 2.44943638642629

Epoch: 43| Step: 0
Training loss: 2.891822576522827
Validation loss: 2.445190946261088

Epoch: 6| Step: 1
Training loss: 2.9286298751831055
Validation loss: 2.4456013838450112

Epoch: 6| Step: 2
Training loss: 2.694406747817993
Validation loss: 2.4412500858306885

Epoch: 6| Step: 3
Training loss: 3.095837116241455
Validation loss: 2.4382435083389282

Epoch: 6| Step: 4
Training loss: 3.2365598678588867
Validation loss: 2.440395394961039

Epoch: 6| Step: 5
Training loss: 2.451347589492798
Validation loss: 2.424840052922567

Epoch: 6| Step: 6
Training loss: 2.655270576477051
Validation loss: 2.4257633686065674

Epoch: 6| Step: 7
Training loss: 2.0559592247009277
Validation loss: 2.426324208577474

Epoch: 6| Step: 8
Training loss: 2.0444083213806152
Validation loss: 2.4239389896392822

Epoch: 6| Step: 9
Training loss: 2.3644840717315674
Validation loss: 2.422353982925415

Epoch: 6| Step: 10
Training loss: 2.7812399864196777
Validation loss: 2.4208593368530273

Epoch: 6| Step: 11
Training loss: 2.614431858062744
Validation loss: 2.4184968868891397

Epoch: 6| Step: 12
Training loss: 2.277920961380005
Validation loss: 2.4160977602005005

Epoch: 6| Step: 13
Training loss: 2.546740770339966
Validation loss: 2.4127875169118247

Epoch: 44| Step: 0
Training loss: 2.87467098236084
Validation loss: 2.4108564456303916

Epoch: 6| Step: 1
Training loss: 2.915070056915283
Validation loss: 2.4061988989512124

Epoch: 6| Step: 2
Training loss: 3.3040053844451904
Validation loss: 2.3998420437177024

Epoch: 6| Step: 3
Training loss: 2.309112548828125
Validation loss: 2.3980384866396585

Epoch: 6| Step: 4
Training loss: 2.2393126487731934
Validation loss: 2.398622671763102

Epoch: 6| Step: 5
Training loss: 2.446007490158081
Validation loss: 2.394680619239807

Epoch: 6| Step: 6
Training loss: 2.2971749305725098
Validation loss: 2.407012422879537

Epoch: 6| Step: 7
Training loss: 2.0238656997680664
Validation loss: 2.389456113179525

Epoch: 6| Step: 8
Training loss: 3.1267194747924805
Validation loss: 2.3894624511400857

Epoch: 6| Step: 9
Training loss: 2.7758378982543945
Validation loss: 2.389374574025472

Epoch: 6| Step: 10
Training loss: 2.5977959632873535
Validation loss: 2.3870219786961875

Epoch: 6| Step: 11
Training loss: 2.5811538696289062
Validation loss: 2.3855466842651367

Epoch: 6| Step: 12
Training loss: 2.5864198207855225
Validation loss: 2.3852286338806152

Epoch: 6| Step: 13
Training loss: 2.0247342586517334
Validation loss: 2.384098529815674

Epoch: 45| Step: 0
Training loss: 2.2792916297912598
Validation loss: 2.382662216822306

Epoch: 6| Step: 1
Training loss: 3.2232158184051514
Validation loss: 2.3794779777526855

Epoch: 6| Step: 2
Training loss: 2.551520824432373
Validation loss: 2.378957192103068

Epoch: 6| Step: 3
Training loss: 2.7026708126068115
Validation loss: 2.373371402422587

Epoch: 6| Step: 4
Training loss: 1.9372416734695435
Validation loss: 2.3674360513687134

Epoch: 6| Step: 5
Training loss: 2.94805645942688
Validation loss: 2.3657858769098916

Epoch: 6| Step: 6
Training loss: 2.739136219024658
Validation loss: 2.3684579531351724

Epoch: 6| Step: 7
Training loss: 1.8753547668457031
Validation loss: 2.3678969343503318

Epoch: 6| Step: 8
Training loss: 2.5310301780700684
Validation loss: 2.3608956734339395

Epoch: 6| Step: 9
Training loss: 1.8862684965133667
Validation loss: 2.354876081148783

Epoch: 6| Step: 10
Training loss: 2.8998963832855225
Validation loss: 2.353390097618103

Epoch: 6| Step: 11
Training loss: 2.6492114067077637
Validation loss: 2.3552590211232505

Epoch: 6| Step: 12
Training loss: 2.333413600921631
Validation loss: 2.3539246320724487

Epoch: 6| Step: 13
Training loss: 3.0320792198181152
Validation loss: 2.352608939011892

Epoch: 46| Step: 0
Training loss: 2.91719126701355
Validation loss: 2.3480446338653564

Epoch: 6| Step: 1
Training loss: 2.4860682487487793
Validation loss: 2.347366372744242

Epoch: 6| Step: 2
Training loss: 3.0595176219940186
Validation loss: 2.342211127281189

Epoch: 6| Step: 3
Training loss: 2.4894423484802246
Validation loss: 2.3389585614204407

Epoch: 6| Step: 4
Training loss: 2.481293201446533
Validation loss: 2.335916439692179

Epoch: 6| Step: 5
Training loss: 2.2239880561828613
Validation loss: 2.3325747648874917

Epoch: 6| Step: 6
Training loss: 2.6098337173461914
Validation loss: 2.327501197655996

Epoch: 6| Step: 7
Training loss: 2.7221386432647705
Validation loss: 2.320009171962738

Epoch: 6| Step: 8
Training loss: 2.3994269371032715
Validation loss: 2.3147472540537515

Epoch: 6| Step: 9
Training loss: 2.0623323917388916
Validation loss: 2.3139108419418335

Epoch: 6| Step: 10
Training loss: 2.099246025085449
Validation loss: 2.3083279927571616

Epoch: 6| Step: 11
Training loss: 2.509415864944458
Validation loss: 2.304699261983236

Epoch: 6| Step: 12
Training loss: 2.228121042251587
Validation loss: 2.314063290754954

Epoch: 6| Step: 13
Training loss: 2.7786734104156494
Validation loss: 2.3028534253438315

Epoch: 47| Step: 0
Training loss: 1.8046642541885376
Validation loss: 2.305721958478292

Epoch: 6| Step: 1
Training loss: 3.06183123588562
Validation loss: 2.306560238202413

Epoch: 6| Step: 2
Training loss: 1.8913098573684692
Validation loss: 2.3099241058031716

Epoch: 6| Step: 3
Training loss: 2.8991775512695312
Validation loss: 2.3190855185190835

Epoch: 6| Step: 4
Training loss: 2.8700578212738037
Validation loss: 2.322795550028483

Epoch: 6| Step: 5
Training loss: 2.426006555557251
Validation loss: 2.31903076171875

Epoch: 6| Step: 6
Training loss: 2.171487808227539
Validation loss: 2.3137076099713645

Epoch: 6| Step: 7
Training loss: 2.580028533935547
Validation loss: 2.313010315100352

Epoch: 6| Step: 8
Training loss: 2.969480276107788
Validation loss: 2.3116016586621604

Epoch: 6| Step: 9
Training loss: 1.9968150854110718
Validation loss: 2.3027453422546387

Epoch: 6| Step: 10
Training loss: 2.6824958324432373
Validation loss: 2.2964046200116477

Epoch: 6| Step: 11
Training loss: 2.1935067176818848
Validation loss: 2.2914390166600547

Epoch: 6| Step: 12
Training loss: 2.6640677452087402
Validation loss: 2.2843198577562966

Epoch: 6| Step: 13
Training loss: 2.392573833465576
Validation loss: 2.28296826283137

Epoch: 48| Step: 0
Training loss: 2.3049817085266113
Validation loss: 2.2796006997426352

Epoch: 6| Step: 1
Training loss: 2.002047061920166
Validation loss: 2.273780345916748

Epoch: 6| Step: 2
Training loss: 1.966402530670166
Validation loss: 2.2720143795013428

Epoch: 6| Step: 3
Training loss: 2.4336700439453125
Validation loss: 2.2663265466690063

Epoch: 6| Step: 4
Training loss: 2.556593418121338
Validation loss: 2.263194958368937

Epoch: 6| Step: 5
Training loss: 2.6040396690368652
Validation loss: 2.264348109563192

Epoch: 6| Step: 6
Training loss: 1.917110800743103
Validation loss: 2.2626859943072

Epoch: 6| Step: 7
Training loss: 2.8412294387817383
Validation loss: 2.259399712085724

Epoch: 6| Step: 8
Training loss: 2.7709920406341553
Validation loss: 2.2562617858250937

Epoch: 6| Step: 9
Training loss: 2.456455707550049
Validation loss: 2.257373015085856

Epoch: 6| Step: 10
Training loss: 2.7210257053375244
Validation loss: 2.2531118194262185

Epoch: 6| Step: 11
Training loss: 2.714615821838379
Validation loss: 2.2506818771362305

Epoch: 6| Step: 12
Training loss: 2.42179799079895
Validation loss: 2.2507338325182595

Epoch: 6| Step: 13
Training loss: 2.32023286819458
Validation loss: 2.248067796230316

Epoch: 49| Step: 0
Training loss: 1.8873482942581177
Validation loss: 2.2457937399546304

Epoch: 6| Step: 1
Training loss: 2.0144028663635254
Validation loss: 2.243709603945414

Epoch: 6| Step: 2
Training loss: 2.5809364318847656
Validation loss: 2.241600235303243

Epoch: 6| Step: 3
Training loss: 2.218475818634033
Validation loss: 2.242450495560964

Epoch: 6| Step: 4
Training loss: 2.210282564163208
Validation loss: 2.2398761908213296

Epoch: 6| Step: 5
Training loss: 2.5030455589294434
Validation loss: 2.238599340120951

Epoch: 6| Step: 6
Training loss: 3.0278143882751465
Validation loss: 2.2341568867365518

Epoch: 6| Step: 7
Training loss: 1.8687083721160889
Validation loss: 2.2304343779881797

Epoch: 6| Step: 8
Training loss: 2.709129810333252
Validation loss: 2.2300390799840293

Epoch: 6| Step: 9
Training loss: 2.1512556076049805
Validation loss: 2.2315425674120584

Epoch: 6| Step: 10
Training loss: 2.07476544380188
Validation loss: 2.2247254451115928

Epoch: 6| Step: 11
Training loss: 2.794532537460327
Validation loss: 2.2269846399625144

Epoch: 6| Step: 12
Training loss: 2.4720821380615234
Validation loss: 2.220006227493286

Epoch: 6| Step: 13
Training loss: 3.0399463176727295
Validation loss: 2.221276879310608

Epoch: 50| Step: 0
Training loss: 2.070891857147217
Validation loss: 2.2187801798184714

Epoch: 6| Step: 1
Training loss: 2.6975317001342773
Validation loss: 2.214345852533976

Epoch: 6| Step: 2
Training loss: 2.706479549407959
Validation loss: 2.216243624687195

Epoch: 6| Step: 3
Training loss: 2.4300150871276855
Validation loss: 2.2051571210225425

Epoch: 6| Step: 4
Training loss: 2.5985469818115234
Validation loss: 2.2030926744143167

Epoch: 6| Step: 5
Training loss: 2.0183701515197754
Validation loss: 2.1993225812911987

Epoch: 6| Step: 6
Training loss: 2.386753559112549
Validation loss: 2.193858822186788

Epoch: 6| Step: 7
Training loss: 2.5501160621643066
Validation loss: 2.190301497777303

Epoch: 6| Step: 8
Training loss: 2.010974884033203
Validation loss: 2.188101132710775

Epoch: 6| Step: 9
Training loss: 2.122063636779785
Validation loss: 2.1848047176996865

Epoch: 6| Step: 10
Training loss: 2.5628912448883057
Validation loss: 2.185429334640503

Epoch: 6| Step: 11
Training loss: 1.7080293893814087
Validation loss: 2.1871858636538186

Epoch: 6| Step: 12
Training loss: 2.88350248336792
Validation loss: 2.1960232257843018

Epoch: 6| Step: 13
Training loss: 2.399531364440918
Validation loss: 2.199312130610148

Epoch: 51| Step: 0
Training loss: 2.2109246253967285
Validation loss: 2.183585266272227

Epoch: 6| Step: 1
Training loss: 2.4987401962280273
Validation loss: 2.185574491818746

Epoch: 6| Step: 2
Training loss: 1.8348468542099
Validation loss: 2.19157075881958

Epoch: 6| Step: 3
Training loss: 2.016242742538452
Validation loss: 2.1841174165407815

Epoch: 6| Step: 4
Training loss: 2.8674912452697754
Validation loss: 2.2046866218249

Epoch: 6| Step: 5
Training loss: 2.269627571105957
Validation loss: 2.1870925227801004

Epoch: 6| Step: 6
Training loss: 2.0068106651306152
Validation loss: 2.1722273230552673

Epoch: 6| Step: 7
Training loss: 2.427405834197998
Validation loss: 2.171486496925354

Epoch: 6| Step: 8
Training loss: 2.4882171154022217
Validation loss: 2.1714213291803994

Epoch: 6| Step: 9
Training loss: 1.7435762882232666
Validation loss: 2.1694596211115518

Epoch: 6| Step: 10
Training loss: 2.7819857597351074
Validation loss: 2.1666966875394187

Epoch: 6| Step: 11
Training loss: 2.654006242752075
Validation loss: 2.1647837360699973

Epoch: 6| Step: 12
Training loss: 2.704470634460449
Validation loss: 2.1612261533737183

Epoch: 6| Step: 13
Training loss: 2.3065199851989746
Validation loss: 2.160561124483744

Epoch: 52| Step: 0
Training loss: 2.701605796813965
Validation loss: 2.1637815038363137

Epoch: 6| Step: 1
Training loss: 2.4852118492126465
Validation loss: 2.1699645121892295

Epoch: 6| Step: 2
Training loss: 2.237591505050659
Validation loss: 2.164838949839274

Epoch: 6| Step: 3
Training loss: 2.37637996673584
Validation loss: 2.1494411627451577

Epoch: 6| Step: 4
Training loss: 2.2580931186676025
Validation loss: 2.1522922913233438

Epoch: 6| Step: 5
Training loss: 2.5990190505981445
Validation loss: 2.157630960146586

Epoch: 6| Step: 6
Training loss: 2.4522931575775146
Validation loss: 2.155755857626597

Epoch: 6| Step: 7
Training loss: 3.0388104915618896
Validation loss: 2.1555638710657754

Epoch: 6| Step: 8
Training loss: 2.1858253479003906
Validation loss: 2.1583909591039023

Epoch: 6| Step: 9
Training loss: 2.137026786804199
Validation loss: 2.1568979819615683

Epoch: 6| Step: 10
Training loss: 1.7031104564666748
Validation loss: 2.154015084107717

Epoch: 6| Step: 11
Training loss: 1.9963586330413818
Validation loss: 2.1491687893867493

Epoch: 6| Step: 12
Training loss: 2.080392837524414
Validation loss: 2.148250997066498

Epoch: 6| Step: 13
Training loss: 2.361355781555176
Validation loss: 2.1505794525146484

Epoch: 53| Step: 0
Training loss: 1.5796680450439453
Validation loss: 2.1494038105010986

Epoch: 6| Step: 1
Training loss: 2.8354880809783936
Validation loss: 2.148124019304911

Epoch: 6| Step: 2
Training loss: 2.379244089126587
Validation loss: 2.1492015719413757

Epoch: 6| Step: 3
Training loss: 2.1576364040374756
Validation loss: 2.1434375445048013

Epoch: 6| Step: 4
Training loss: 3.0612149238586426
Validation loss: 2.159862756729126

Epoch: 6| Step: 5
Training loss: 2.056173801422119
Validation loss: 2.1611180305480957

Epoch: 6| Step: 6
Training loss: 2.861682176589966
Validation loss: 2.1413719852765403

Epoch: 6| Step: 7
Training loss: 2.6017868518829346
Validation loss: 2.1430774132410684

Epoch: 6| Step: 8
Training loss: 2.150944232940674
Validation loss: 2.13821933666865

Epoch: 6| Step: 9
Training loss: 1.698219895362854
Validation loss: 2.136642356713613

Epoch: 6| Step: 10
Training loss: 2.2479445934295654
Validation loss: 2.1297523776690164

Epoch: 6| Step: 11
Training loss: 2.248206853866577
Validation loss: 2.129996041456858

Epoch: 6| Step: 12
Training loss: 2.953756332397461
Validation loss: 2.1276365319887796

Epoch: 6| Step: 13
Training loss: 1.6411263942718506
Validation loss: 2.128986676534017

Epoch: 54| Step: 0
Training loss: 2.332831621170044
Validation loss: 2.126019755999247

Epoch: 6| Step: 1
Training loss: 2.586764097213745
Validation loss: 2.1247010231018066

Epoch: 6| Step: 2
Training loss: 2.827873945236206
Validation loss: 2.1179440220197043

Epoch: 6| Step: 3
Training loss: 2.6749024391174316
Validation loss: 2.12211020787557

Epoch: 6| Step: 4
Training loss: 2.4367237091064453
Validation loss: 2.1277610659599304

Epoch: 6| Step: 5
Training loss: 2.5632638931274414
Validation loss: 2.1128233671188354

Epoch: 6| Step: 6
Training loss: 1.7895920276641846
Validation loss: 2.1069187919298806

Epoch: 6| Step: 7
Training loss: 2.151226043701172
Validation loss: 2.1075827280680337

Epoch: 6| Step: 8
Training loss: 2.488558530807495
Validation loss: 2.0981551806131997

Epoch: 6| Step: 9
Training loss: 2.0768771171569824
Validation loss: 2.100366453329722

Epoch: 6| Step: 10
Training loss: 2.30216121673584
Validation loss: 2.102813243865967

Epoch: 6| Step: 11
Training loss: 1.3852059841156006
Validation loss: 2.118072271347046

Epoch: 6| Step: 12
Training loss: 1.703174352645874
Validation loss: 2.1129287083943686

Epoch: 6| Step: 13
Training loss: 2.7836203575134277
Validation loss: 2.1037384271621704

Epoch: 55| Step: 0
Training loss: 2.612837076187134
Validation loss: 2.1030073960622153

Epoch: 6| Step: 1
Training loss: 2.302323341369629
Validation loss: 2.100803335507711

Epoch: 6| Step: 2
Training loss: 2.482501268386841
Validation loss: 2.0994058648745217

Epoch: 6| Step: 3
Training loss: 1.7516158819198608
Validation loss: 2.101434290409088

Epoch: 6| Step: 4
Training loss: 2.2131288051605225
Validation loss: 2.0988651712735495

Epoch: 6| Step: 5
Training loss: 2.446367025375366
Validation loss: 2.0974213679631553

Epoch: 6| Step: 6
Training loss: 1.814971923828125
Validation loss: 2.1040189464886985

Epoch: 6| Step: 7
Training loss: 2.7122373580932617
Validation loss: 2.090487480163574

Epoch: 6| Step: 8
Training loss: 2.859774589538574
Validation loss: 2.103498895963033

Epoch: 6| Step: 9
Training loss: 2.215259313583374
Validation loss: 2.10761429866155

Epoch: 6| Step: 10
Training loss: 2.0579757690429688
Validation loss: 2.110505700111389

Epoch: 6| Step: 11
Training loss: 2.156221389770508
Validation loss: 2.112499495347341

Epoch: 6| Step: 12
Training loss: 2.535499334335327
Validation loss: 2.1119479139645896

Epoch: 6| Step: 13
Training loss: 1.8023829460144043
Validation loss: 2.117322623729706

Epoch: 56| Step: 0
Training loss: 2.558201313018799
Validation loss: 2.122427682081858

Epoch: 6| Step: 1
Training loss: 2.2221784591674805
Validation loss: 2.1165548960367837

Epoch: 6| Step: 2
Training loss: 2.494311809539795
Validation loss: 2.118198732535044

Epoch: 6| Step: 3
Training loss: 2.458303928375244
Validation loss: 2.112857381502787

Epoch: 6| Step: 4
Training loss: 2.3646697998046875
Validation loss: 2.1000205278396606

Epoch: 6| Step: 5
Training loss: 1.9760714769363403
Validation loss: 2.0885489185651145

Epoch: 6| Step: 6
Training loss: 1.8128191232681274
Validation loss: 2.0872761607170105

Epoch: 6| Step: 7
Training loss: 2.415215015411377
Validation loss: 2.0795142451922097

Epoch: 6| Step: 8
Training loss: 2.461775779724121
Validation loss: 2.0737433632214866

Epoch: 6| Step: 9
Training loss: 2.2574634552001953
Validation loss: 2.0944185058275857

Epoch: 6| Step: 10
Training loss: 2.1909706592559814
Validation loss: 2.1091327468554177

Epoch: 6| Step: 11
Training loss: 1.5374503135681152
Validation loss: 2.157711068789164

Epoch: 6| Step: 12
Training loss: 3.313465118408203
Validation loss: 2.1665660738945007

Epoch: 6| Step: 13
Training loss: 1.8753304481506348
Validation loss: 2.121454735596975

Epoch: 57| Step: 0
Training loss: 2.193399429321289
Validation loss: 2.0731011827786765

Epoch: 6| Step: 1
Training loss: 1.399178147315979
Validation loss: 2.0626884500185647

Epoch: 6| Step: 2
Training loss: 2.82981014251709
Validation loss: 2.0755033691724143

Epoch: 6| Step: 3
Training loss: 2.0484914779663086
Validation loss: 2.0812315543492637

Epoch: 6| Step: 4
Training loss: 2.9215896129608154
Validation loss: 2.08990869919459

Epoch: 6| Step: 5
Training loss: 2.250093460083008
Validation loss: 2.0947661797205606

Epoch: 6| Step: 6
Training loss: 2.2222273349761963
Validation loss: 2.1017373402913413

Epoch: 6| Step: 7
Training loss: 2.328179359436035
Validation loss: 2.108504076798757

Epoch: 6| Step: 8
Training loss: 2.1810760498046875
Validation loss: 2.1113644440968833

Epoch: 6| Step: 9
Training loss: 2.5589818954467773
Validation loss: 2.1097466746966043

Epoch: 6| Step: 10
Training loss: 1.805782675743103
Validation loss: 2.1054016749064126

Epoch: 6| Step: 11
Training loss: 2.0210213661193848
Validation loss: 2.114049712816874

Epoch: 6| Step: 12
Training loss: 2.5636515617370605
Validation loss: 2.1142911314964294

Epoch: 6| Step: 13
Training loss: 2.7628588676452637
Validation loss: 2.1093359192212424

Epoch: 58| Step: 0
Training loss: 1.7691731452941895
Validation loss: 2.1048705180486045

Epoch: 6| Step: 1
Training loss: 2.4751670360565186
Validation loss: 2.0967024167378745

Epoch: 6| Step: 2
Training loss: 2.7682127952575684
Validation loss: 2.092110554377238

Epoch: 6| Step: 3
Training loss: 2.369321823120117
Validation loss: 2.0912291407585144

Epoch: 6| Step: 4
Training loss: 1.9142684936523438
Validation loss: 2.0885250767072043

Epoch: 6| Step: 5
Training loss: 1.9407085180282593
Validation loss: 2.08916703859965

Epoch: 6| Step: 6
Training loss: 2.4630637168884277
Validation loss: 2.0849540630976358

Epoch: 6| Step: 7
Training loss: 1.7528622150421143
Validation loss: 2.082395911216736

Epoch: 6| Step: 8
Training loss: 1.8570328950881958
Validation loss: 2.079074045022329

Epoch: 6| Step: 9
Training loss: 2.08526611328125
Validation loss: 2.07684326171875

Epoch: 6| Step: 10
Training loss: 3.1460094451904297
Validation loss: 2.074336806933085

Epoch: 6| Step: 11
Training loss: 2.687058448791504
Validation loss: 2.0722867846488953

Epoch: 6| Step: 12
Training loss: 2.2743372917175293
Validation loss: 2.064793070157369

Epoch: 6| Step: 13
Training loss: 2.2434005737304688
Validation loss: 2.0660031040509543

Epoch: 59| Step: 0
Training loss: 1.639983892440796
Validation loss: 2.0626213947931924

Epoch: 6| Step: 1
Training loss: 2.4781975746154785
Validation loss: 2.085027734438578

Epoch: 6| Step: 2
Training loss: 1.8422619104385376
Validation loss: 2.0878058075904846

Epoch: 6| Step: 3
Training loss: 2.947179079055786
Validation loss: 2.0914425055185952

Epoch: 6| Step: 4
Training loss: 2.194528341293335
Validation loss: 2.07451593875885

Epoch: 6| Step: 5
Training loss: 2.501274585723877
Validation loss: 2.049377461274465

Epoch: 6| Step: 6
Training loss: 2.870013952255249
Validation loss: 2.0471920172373452

Epoch: 6| Step: 7
Training loss: 1.5620543956756592
Validation loss: 2.0492743253707886

Epoch: 6| Step: 8
Training loss: 2.1549439430236816
Validation loss: 2.054377575715383

Epoch: 6| Step: 9
Training loss: 2.139415740966797
Validation loss: 2.0538507103919983

Epoch: 6| Step: 10
Training loss: 2.4219980239868164
Validation loss: 2.0523032943407693

Epoch: 6| Step: 11
Training loss: 1.8989195823669434
Validation loss: 2.055934190750122

Epoch: 6| Step: 12
Training loss: 2.542372703552246
Validation loss: 2.038966735204061

Epoch: 6| Step: 13
Training loss: 2.4964780807495117
Validation loss: 2.0445804794629416

Epoch: 60| Step: 0
Training loss: 2.725998878479004
Validation loss: 2.036117653052012

Epoch: 6| Step: 1
Training loss: 1.9669140577316284
Validation loss: 2.0391324162483215

Epoch: 6| Step: 2
Training loss: 1.5801401138305664
Validation loss: 2.043659031391144

Epoch: 6| Step: 3
Training loss: 2.6166317462921143
Validation loss: 2.039285123348236

Epoch: 6| Step: 4
Training loss: 2.5173885822296143
Validation loss: 2.0389495293299356

Epoch: 6| Step: 5
Training loss: 1.9917562007904053
Validation loss: 2.038209875424703

Epoch: 6| Step: 6
Training loss: 2.7228362560272217
Validation loss: 2.04337469736735

Epoch: 6| Step: 7
Training loss: 2.4317758083343506
Validation loss: 2.046152492364248

Epoch: 6| Step: 8
Training loss: 2.3302078247070312
Validation loss: 2.0457236568133035

Epoch: 6| Step: 9
Training loss: 2.2964425086975098
Validation loss: 2.0402656197547913

Epoch: 6| Step: 10
Training loss: 1.6643340587615967
Validation loss: 2.040120760599772

Epoch: 6| Step: 11
Training loss: 2.0623786449432373
Validation loss: 2.0349552035331726

Epoch: 6| Step: 12
Training loss: 2.3772337436676025
Validation loss: 2.0348015228907266

Epoch: 6| Step: 13
Training loss: 1.9416614770889282
Validation loss: 2.03537118434906

Epoch: 61| Step: 0
Training loss: 1.8461135625839233
Validation loss: 2.0382532676060996

Epoch: 6| Step: 1
Training loss: 2.1282458305358887
Validation loss: 2.037775158882141

Epoch: 6| Step: 2
Training loss: 2.2305049896240234
Validation loss: 2.0298067529996238

Epoch: 6| Step: 3
Training loss: 2.059513568878174
Validation loss: 2.0342144767443338

Epoch: 6| Step: 4
Training loss: 2.060016393661499
Validation loss: 2.035266180833181

Epoch: 6| Step: 5
Training loss: 2.626041889190674
Validation loss: 2.031174043814341

Epoch: 6| Step: 6
Training loss: 2.006394863128662
Validation loss: 2.0359984834988913

Epoch: 6| Step: 7
Training loss: 2.198228359222412
Validation loss: 2.028113921483358

Epoch: 6| Step: 8
Training loss: 1.9702255725860596
Validation loss: 2.032773951689402

Epoch: 6| Step: 9
Training loss: 2.3885605335235596
Validation loss: 2.0381807486216226

Epoch: 6| Step: 10
Training loss: 2.9569711685180664
Validation loss: 2.034315745035807

Epoch: 6| Step: 11
Training loss: 2.1280999183654785
Validation loss: 2.0360235373179116

Epoch: 6| Step: 12
Training loss: 2.1093363761901855
Validation loss: 2.039924661318461

Epoch: 6| Step: 13
Training loss: 2.3191733360290527
Validation loss: 2.046617070833842

Epoch: 62| Step: 0
Training loss: 2.3038058280944824
Validation loss: 2.0358487168947854

Epoch: 6| Step: 1
Training loss: 2.9732627868652344
Validation loss: 2.0379251837730408

Epoch: 6| Step: 2
Training loss: 1.8138647079467773
Validation loss: 2.0327524741490683

Epoch: 6| Step: 3
Training loss: 2.0867972373962402
Validation loss: 2.03099658091863

Epoch: 6| Step: 4
Training loss: 2.1577110290527344
Validation loss: 2.0363391439119973

Epoch: 6| Step: 5
Training loss: 2.7607412338256836
Validation loss: 2.042121092478434

Epoch: 6| Step: 6
Training loss: 2.511772632598877
Validation loss: 2.0459029277165732

Epoch: 6| Step: 7
Training loss: 2.57647967338562
Validation loss: 2.0591631134351096

Epoch: 6| Step: 8
Training loss: 1.5266565084457397
Validation loss: 2.048499604066213

Epoch: 6| Step: 9
Training loss: 2.1964571475982666
Validation loss: 2.0422052343686423

Epoch: 6| Step: 10
Training loss: 2.381988048553467
Validation loss: 2.053912858168284

Epoch: 6| Step: 11
Training loss: 1.8490710258483887
Validation loss: 2.03760302066803

Epoch: 6| Step: 12
Training loss: 1.9361944198608398
Validation loss: 2.0271957516670227

Epoch: 6| Step: 13
Training loss: 1.9967957735061646
Validation loss: 2.034912407398224

Epoch: 63| Step: 0
Training loss: 2.486053228378296
Validation loss: 2.030799090862274

Epoch: 6| Step: 1
Training loss: 2.2904539108276367
Validation loss: 2.0274316469828286

Epoch: 6| Step: 2
Training loss: 2.2216663360595703
Validation loss: 2.036618252595266

Epoch: 6| Step: 3
Training loss: 1.8120089769363403
Validation loss: 2.0357712705930076

Epoch: 6| Step: 4
Training loss: 1.9537616968154907
Validation loss: 2.0364606380462646

Epoch: 6| Step: 5
Training loss: 1.8828165531158447
Validation loss: 2.036194105943044

Epoch: 6| Step: 6
Training loss: 2.556162118911743
Validation loss: 2.0323362747828164

Epoch: 6| Step: 7
Training loss: 2.8294765949249268
Validation loss: 2.0313335259755454

Epoch: 6| Step: 8
Training loss: 2.670840263366699
Validation loss: 2.0310385624567666

Epoch: 6| Step: 9
Training loss: 1.9750958681106567
Validation loss: 2.02680238087972

Epoch: 6| Step: 10
Training loss: 2.604898691177368
Validation loss: 2.028558870156606

Epoch: 6| Step: 11
Training loss: 1.862977147102356
Validation loss: 2.030738890171051

Epoch: 6| Step: 12
Training loss: 2.1687655448913574
Validation loss: 2.0248205860455832

Epoch: 6| Step: 13
Training loss: 1.6611238718032837
Validation loss: 2.015089273452759

Epoch: 64| Step: 0
Training loss: 2.338919162750244
Validation loss: 2.0255571405092874

Epoch: 6| Step: 1
Training loss: 2.0370702743530273
Validation loss: 2.0230111281077066

Epoch: 6| Step: 2
Training loss: 1.8728805780410767
Validation loss: 2.0186734398206077

Epoch: 6| Step: 3
Training loss: 2.355950355529785
Validation loss: 2.0224756399790444

Epoch: 6| Step: 4
Training loss: 2.688624858856201
Validation loss: 2.0303074717521667

Epoch: 6| Step: 5
Training loss: 1.7617976665496826
Validation loss: 2.035493810971578

Epoch: 6| Step: 6
Training loss: 2.602199077606201
Validation loss: 2.0216461022694907

Epoch: 6| Step: 7
Training loss: 1.9839084148406982
Validation loss: 2.024551490942637

Epoch: 6| Step: 8
Training loss: 2.073641061782837
Validation loss: 2.018279731273651

Epoch: 6| Step: 9
Training loss: 1.7801058292388916
Validation loss: 2.0200945337613425

Epoch: 6| Step: 10
Training loss: 2.06213116645813
Validation loss: 2.0163537859916687

Epoch: 6| Step: 11
Training loss: 2.680652141571045
Validation loss: 2.024063209692637

Epoch: 6| Step: 12
Training loss: 2.5416066646575928
Validation loss: 2.0250809590021768

Epoch: 6| Step: 13
Training loss: 1.9916284084320068
Validation loss: 2.0223686695098877

Epoch: 65| Step: 0
Training loss: 2.408299446105957
Validation loss: 2.027535597483317

Epoch: 6| Step: 1
Training loss: 2.3016180992126465
Validation loss: 2.0308591723442078

Epoch: 6| Step: 2
Training loss: 1.7200908660888672
Validation loss: 2.0240924755732217

Epoch: 6| Step: 3
Training loss: 2.226649761199951
Validation loss: 2.0243388414382935

Epoch: 6| Step: 4
Training loss: 2.060093879699707
Validation loss: 2.023894449075063

Epoch: 6| Step: 5
Training loss: 2.506527900695801
Validation loss: 2.024117946624756

Epoch: 6| Step: 6
Training loss: 2.0795464515686035
Validation loss: 2.019515037536621

Epoch: 6| Step: 7
Training loss: 2.2013511657714844
Validation loss: 2.0358528097470603

Epoch: 6| Step: 8
Training loss: 1.7779314517974854
Validation loss: 2.042743504047394

Epoch: 6| Step: 9
Training loss: 2.049438953399658
Validation loss: 2.0317394733428955

Epoch: 6| Step: 10
Training loss: 2.1983654499053955
Validation loss: 2.045340120792389

Epoch: 6| Step: 11
Training loss: 2.7791428565979004
Validation loss: 2.0308696826299033

Epoch: 6| Step: 12
Training loss: 2.282228708267212
Validation loss: 2.018079320589701

Epoch: 6| Step: 13
Training loss: 2.119131088256836
Validation loss: 2.021877864996592

Epoch: 66| Step: 0
Training loss: 1.7283530235290527
Validation loss: 2.014578660329183

Epoch: 6| Step: 1
Training loss: 2.0770835876464844
Validation loss: 2.0235876639684043

Epoch: 6| Step: 2
Training loss: 2.5403389930725098
Validation loss: 2.030735711256663

Epoch: 6| Step: 3
Training loss: 2.647164821624756
Validation loss: 2.0377843976020813

Epoch: 6| Step: 4
Training loss: 2.3187801837921143
Validation loss: 2.035823424657186

Epoch: 6| Step: 5
Training loss: 1.8506278991699219
Validation loss: 2.0432501633961997

Epoch: 6| Step: 6
Training loss: 2.222426414489746
Validation loss: 2.0443609754244485

Epoch: 6| Step: 7
Training loss: 2.494382381439209
Validation loss: 2.0485281348228455

Epoch: 6| Step: 8
Training loss: 2.3266005516052246
Validation loss: 2.042455573876699

Epoch: 6| Step: 9
Training loss: 2.0852630138397217
Validation loss: 2.049083431561788

Epoch: 6| Step: 10
Training loss: 2.062168598175049
Validation loss: 2.0437779625256858

Epoch: 6| Step: 11
Training loss: 2.0399622917175293
Validation loss: 2.0468990802764893

Epoch: 6| Step: 12
Training loss: 2.443272590637207
Validation loss: 2.0426428516705832

Epoch: 6| Step: 13
Training loss: 2.281337022781372
Validation loss: 2.0366459290186563

Epoch: 67| Step: 0
Training loss: 2.007077693939209
Validation loss: 2.041325787703196

Epoch: 6| Step: 1
Training loss: 2.3345110416412354
Validation loss: 2.03380678097407

Epoch: 6| Step: 2
Training loss: 2.1397924423217773
Validation loss: 2.0308034221331277

Epoch: 6| Step: 3
Training loss: 2.449401378631592
Validation loss: 2.013025720914205

Epoch: 6| Step: 4
Training loss: 2.8405160903930664
Validation loss: 2.016275703907013

Epoch: 6| Step: 5
Training loss: 2.3223447799682617
Validation loss: 2.007468044757843

Epoch: 6| Step: 6
Training loss: 1.6550889015197754
Validation loss: 2.0110650658607483

Epoch: 6| Step: 7
Training loss: 1.6790801286697388
Validation loss: 2.011312405268351

Epoch: 6| Step: 8
Training loss: 2.625654935836792
Validation loss: 2.033272941907247

Epoch: 6| Step: 9
Training loss: 2.084249973297119
Validation loss: 2.0287192463874817

Epoch: 6| Step: 10
Training loss: 2.0210471153259277
Validation loss: 2.0307879050572715

Epoch: 6| Step: 11
Training loss: 2.194972515106201
Validation loss: 2.02755868434906

Epoch: 6| Step: 12
Training loss: 2.555567741394043
Validation loss: 2.0221052964528403

Epoch: 6| Step: 13
Training loss: 1.9771647453308105
Validation loss: 2.018483797709147

Epoch: 68| Step: 0
Training loss: 2.250622272491455
Validation loss: 2.0154266357421875

Epoch: 6| Step: 1
Training loss: 2.259645462036133
Validation loss: 2.0177581310272217

Epoch: 6| Step: 2
Training loss: 2.8091068267822266
Validation loss: 2.0216335455576577

Epoch: 6| Step: 3
Training loss: 2.2611913681030273
Validation loss: 2.0321037769317627

Epoch: 6| Step: 4
Training loss: 2.1491851806640625
Validation loss: 2.01902445157369

Epoch: 6| Step: 5
Training loss: 2.018281936645508
Validation loss: 2.01731538772583

Epoch: 6| Step: 6
Training loss: 2.256760835647583
Validation loss: 2.023982346057892

Epoch: 6| Step: 7
Training loss: 2.233443260192871
Validation loss: 2.0247334837913513

Epoch: 6| Step: 8
Training loss: 2.202793598175049
Validation loss: 2.024906953175863

Epoch: 6| Step: 9
Training loss: 1.714388370513916
Validation loss: 2.0176198879877725

Epoch: 6| Step: 10
Training loss: 1.5431523323059082
Validation loss: 2.016714950402578

Epoch: 6| Step: 11
Training loss: 2.1476845741271973
Validation loss: 2.023723820845286

Epoch: 6| Step: 12
Training loss: 2.3935012817382812
Validation loss: 2.0198893944422402

Epoch: 6| Step: 13
Training loss: 2.351857900619507
Validation loss: 2.022600849469503

Epoch: 69| Step: 0
Training loss: 2.637289047241211
Validation loss: 2.0265262126922607

Epoch: 6| Step: 1
Training loss: 1.6353799104690552
Validation loss: 2.0149692495663962

Epoch: 6| Step: 2
Training loss: 2.3472728729248047
Validation loss: 2.038210948308309

Epoch: 6| Step: 3
Training loss: 2.1451823711395264
Validation loss: 2.042209247748057

Epoch: 6| Step: 4
Training loss: 2.2811853885650635
Validation loss: 2.0339808066685996

Epoch: 6| Step: 5
Training loss: 1.9873695373535156
Validation loss: 2.031402806440989

Epoch: 6| Step: 6
Training loss: 1.8774878978729248
Validation loss: 2.034190833568573

Epoch: 6| Step: 7
Training loss: 2.332543134689331
Validation loss: 2.0458328326543174

Epoch: 6| Step: 8
Training loss: 2.620522975921631
Validation loss: 2.049623449643453

Epoch: 6| Step: 9
Training loss: 2.2478840351104736
Validation loss: 2.0628381967544556

Epoch: 6| Step: 10
Training loss: 2.2129359245300293
Validation loss: 2.040270745754242

Epoch: 6| Step: 11
Training loss: 1.983154296875
Validation loss: 2.0137621760368347

Epoch: 6| Step: 12
Training loss: 2.1169729232788086
Validation loss: 2.022537132104238

Epoch: 6| Step: 13
Training loss: 2.2619473934173584
Validation loss: 2.0149131615956626

Epoch: 70| Step: 0
Training loss: 2.1307692527770996
Validation loss: 2.020834267139435

Epoch: 6| Step: 1
Training loss: 2.549905776977539
Validation loss: 2.0162397225697837

Epoch: 6| Step: 2
Training loss: 1.132736086845398
Validation loss: 2.018933037916819

Epoch: 6| Step: 3
Training loss: 2.586124897003174
Validation loss: 2.024459183216095

Epoch: 6| Step: 4
Training loss: 2.2365708351135254
Validation loss: 2.0273377497990928

Epoch: 6| Step: 5
Training loss: 1.6480262279510498
Validation loss: 2.023976186911265

Epoch: 6| Step: 6
Training loss: 2.321922540664673
Validation loss: 2.0201110442479453

Epoch: 6| Step: 7
Training loss: 2.7667250633239746
Validation loss: 2.0165260831514993

Epoch: 6| Step: 8
Training loss: 2.0354371070861816
Validation loss: 2.0280748407046

Epoch: 6| Step: 9
Training loss: 2.3231163024902344
Validation loss: 2.035207748413086

Epoch: 6| Step: 10
Training loss: 2.7206549644470215
Validation loss: 2.046606798966726

Epoch: 6| Step: 11
Training loss: 2.4684057235717773
Validation loss: 2.0529146790504456

Epoch: 6| Step: 12
Training loss: 2.3140082359313965
Validation loss: 2.044369101524353

Epoch: 6| Step: 13
Training loss: 1.9342234134674072
Validation loss: 2.030907412370046

Epoch: 71| Step: 0
Training loss: 2.171536684036255
Validation loss: 2.0148363510767617

Epoch: 6| Step: 1
Training loss: 2.363332748413086
Validation loss: 2.0097937186559043

Epoch: 6| Step: 2
Training loss: 2.285151243209839
Validation loss: 2.021842976411184

Epoch: 6| Step: 3
Training loss: 2.2444024085998535
Validation loss: 2.0266517400741577

Epoch: 6| Step: 4
Training loss: 1.9140675067901611
Validation loss: 2.0348942279815674

Epoch: 6| Step: 5
Training loss: 2.542750358581543
Validation loss: 2.0304335157076516

Epoch: 6| Step: 6
Training loss: 2.380504608154297
Validation loss: 2.0418166518211365

Epoch: 6| Step: 7
Training loss: 1.8937172889709473
Validation loss: 2.0371105472246804

Epoch: 6| Step: 8
Training loss: 2.0865063667297363
Validation loss: 2.0334992011388144

Epoch: 6| Step: 9
Training loss: 2.501040458679199
Validation loss: 2.0332375963528952

Epoch: 6| Step: 10
Training loss: 2.349748373031616
Validation loss: 2.032886266708374

Epoch: 6| Step: 11
Training loss: 2.426677703857422
Validation loss: 2.023255228996277

Epoch: 6| Step: 12
Training loss: 2.22190523147583
Validation loss: 2.020550847053528

Epoch: 6| Step: 13
Training loss: 1.442023754119873
Validation loss: 2.0120795567830405

Epoch: 72| Step: 0
Training loss: 2.4884274005889893
Validation loss: 2.007180710633596

Epoch: 6| Step: 1
Training loss: 2.529074192047119
Validation loss: 2.010537405808767

Epoch: 6| Step: 2
Training loss: 2.1480672359466553
Validation loss: 2.0101943214734397

Epoch: 6| Step: 3
Training loss: 2.244926691055298
Validation loss: 2.0091074307759604

Epoch: 6| Step: 4
Training loss: 1.970953345298767
Validation loss: 2.0111567775408425

Epoch: 6| Step: 5
Training loss: 2.1409199237823486
Validation loss: 2.0172635912895203

Epoch: 6| Step: 6
Training loss: 1.8670417070388794
Validation loss: 2.0192391872406006

Epoch: 6| Step: 7
Training loss: 2.4018235206604004
Validation loss: 2.0209876696268716

Epoch: 6| Step: 8
Training loss: 1.977667212486267
Validation loss: 2.0198825001716614

Epoch: 6| Step: 9
Training loss: 1.9720323085784912
Validation loss: 2.025697628657023

Epoch: 6| Step: 10
Training loss: 1.902930736541748
Validation loss: 2.0198116501172385

Epoch: 6| Step: 11
Training loss: 1.9239500761032104
Validation loss: 2.0149191419283548

Epoch: 6| Step: 12
Training loss: 2.730247735977173
Validation loss: 2.0191696286201477

Epoch: 6| Step: 13
Training loss: 2.3551416397094727
Validation loss: 2.0135461886723838

Epoch: 73| Step: 0
Training loss: 2.1415793895721436
Validation loss: 2.0134238600730896

Epoch: 6| Step: 1
Training loss: 2.1101629734039307
Validation loss: 2.014257291952769

Epoch: 6| Step: 2
Training loss: 2.1273062229156494
Validation loss: 2.0079533457756042

Epoch: 6| Step: 3
Training loss: 2.3607778549194336
Validation loss: 2.013837297757467

Epoch: 6| Step: 4
Training loss: 2.7308096885681152
Validation loss: 2.0270778139432273

Epoch: 6| Step: 5
Training loss: 1.5279884338378906
Validation loss: 2.027347962061564

Epoch: 6| Step: 6
Training loss: 1.7959308624267578
Validation loss: 2.02521413564682

Epoch: 6| Step: 7
Training loss: 2.5143280029296875
Validation loss: 2.0398617585500083

Epoch: 6| Step: 8
Training loss: 2.1626057624816895
Validation loss: 2.0574055711428323

Epoch: 6| Step: 9
Training loss: 1.6702229976654053
Validation loss: 2.0495405197143555

Epoch: 6| Step: 10
Training loss: 2.4248600006103516
Validation loss: 2.0445674657821655

Epoch: 6| Step: 11
Training loss: 1.933009386062622
Validation loss: 2.0467100143432617

Epoch: 6| Step: 12
Training loss: 2.704732894897461
Validation loss: 2.0359134475390115

Epoch: 6| Step: 13
Training loss: 2.46018385887146
Validation loss: 2.0287499825159707

Epoch: 74| Step: 0
Training loss: 2.5879592895507812
Validation loss: 2.007595340410868

Epoch: 6| Step: 1
Training loss: 2.5021653175354004
Validation loss: 2.003814458847046

Epoch: 6| Step: 2
Training loss: 2.2983646392822266
Validation loss: 2.0016062458356223

Epoch: 6| Step: 3
Training loss: 2.6327977180480957
Validation loss: 2.0055928230285645

Epoch: 6| Step: 4
Training loss: 2.0147173404693604
Validation loss: 2.0178666512171426

Epoch: 6| Step: 5
Training loss: 2.4815330505371094
Validation loss: 2.0163399378458657

Epoch: 6| Step: 6
Training loss: 2.1613082885742188
Validation loss: 2.006207982699076

Epoch: 6| Step: 7
Training loss: 2.0804498195648193
Validation loss: 2.015325129032135

Epoch: 6| Step: 8
Training loss: 2.0719892978668213
Validation loss: 2.0094857017199197

Epoch: 6| Step: 9
Training loss: 1.913711667060852
Validation loss: 2.003490368525187

Epoch: 6| Step: 10
Training loss: 2.00154185295105
Validation loss: 2.0071749687194824

Epoch: 6| Step: 11
Training loss: 1.7410497665405273
Validation loss: 2.0062178572018943

Epoch: 6| Step: 12
Training loss: 2.11038875579834
Validation loss: 2.0036385655403137

Epoch: 6| Step: 13
Training loss: 1.77549147605896
Validation loss: 2.0104375878969827

Epoch: 75| Step: 0
Training loss: 2.2984142303466797
Validation loss: 1.9999403357505798

Epoch: 6| Step: 1
Training loss: 1.8810255527496338
Validation loss: 2.0150089065233865

Epoch: 6| Step: 2
Training loss: 2.214787721633911
Validation loss: 2.0181705156962075

Epoch: 6| Step: 3
Training loss: 1.8217034339904785
Validation loss: 2.0350257953008017

Epoch: 6| Step: 4
Training loss: 1.8543299436569214
Validation loss: 2.0244958202044168

Epoch: 6| Step: 5
Training loss: 1.759474515914917
Validation loss: 2.0269853870073953

Epoch: 6| Step: 6
Training loss: 2.3442375659942627
Validation loss: 2.019278089205424

Epoch: 6| Step: 7
Training loss: 2.439378261566162
Validation loss: 2.013850728670756

Epoch: 6| Step: 8
Training loss: 2.4286763668060303
Validation loss: 2.0072484215100608

Epoch: 6| Step: 9
Training loss: 2.3667876720428467
Validation loss: 2.013130327065786

Epoch: 6| Step: 10
Training loss: 2.067578077316284
Validation loss: 2.0027413169542947

Epoch: 6| Step: 11
Training loss: 2.4663877487182617
Validation loss: 2.017875015735626

Epoch: 6| Step: 12
Training loss: 2.6707444190979004
Validation loss: 2.021626671155294

Epoch: 6| Step: 13
Training loss: 1.7945835590362549
Validation loss: 2.012315511703491

Epoch: 76| Step: 0
Training loss: 2.0751261711120605
Validation loss: 2.020373225212097

Epoch: 6| Step: 1
Training loss: 2.4758267402648926
Validation loss: 2.0163167913754783

Epoch: 6| Step: 2
Training loss: 2.089461326599121
Validation loss: 2.012949307759603

Epoch: 6| Step: 3
Training loss: 2.629873752593994
Validation loss: 2.0157201091448465

Epoch: 6| Step: 4
Training loss: 1.8510493040084839
Validation loss: 2.011202255884806

Epoch: 6| Step: 5
Training loss: 2.19437313079834
Validation loss: 2.025854468345642

Epoch: 6| Step: 6
Training loss: 1.907442569732666
Validation loss: 2.024466117223104

Epoch: 6| Step: 7
Training loss: 2.5668392181396484
Validation loss: 2.036409338315328

Epoch: 6| Step: 8
Training loss: 2.312906265258789
Validation loss: 2.043056150277456

Epoch: 6| Step: 9
Training loss: 1.6783032417297363
Validation loss: 2.04051943620046

Epoch: 6| Step: 10
Training loss: 2.1588573455810547
Validation loss: 2.041442632675171

Epoch: 6| Step: 11
Training loss: 1.8788847923278809
Validation loss: 2.0541743437449136

Epoch: 6| Step: 12
Training loss: 1.8949415683746338
Validation loss: 2.053468644618988

Epoch: 6| Step: 13
Training loss: 2.4871044158935547
Validation loss: 2.039534409840902

Epoch: 77| Step: 0
Training loss: 2.128875732421875
Validation loss: 2.054913600285848

Epoch: 6| Step: 1
Training loss: 2.5032520294189453
Validation loss: 2.0325437784194946

Epoch: 6| Step: 2
Training loss: 2.175861358642578
Validation loss: 2.0292454759279885

Epoch: 6| Step: 3
Training loss: 1.4739642143249512
Validation loss: 2.0180453658103943

Epoch: 6| Step: 4
Training loss: 2.7382702827453613
Validation loss: 2.00081201394399

Epoch: 6| Step: 5
Training loss: 1.791446328163147
Validation loss: 2.0006864070892334

Epoch: 6| Step: 6
Training loss: 2.341503143310547
Validation loss: 1.9959511160850525

Epoch: 6| Step: 7
Training loss: 2.2396774291992188
Validation loss: 2.0041499932607016

Epoch: 6| Step: 8
Training loss: 1.9112193584442139
Validation loss: 1.9993053873380024

Epoch: 6| Step: 9
Training loss: 2.119478940963745
Validation loss: 2.0057952404022217

Epoch: 6| Step: 10
Training loss: 2.0309128761291504
Validation loss: 2.013219932715098

Epoch: 6| Step: 11
Training loss: 2.1352810859680176
Validation loss: 2.0111916065216064

Epoch: 6| Step: 12
Training loss: 2.5058693885803223
Validation loss: 2.0090606411298118

Epoch: 6| Step: 13
Training loss: 2.1465237140655518
Validation loss: 2.018922448158264

Epoch: 78| Step: 0
Training loss: 1.6913220882415771
Validation loss: 2.0217822790145874

Epoch: 6| Step: 1
Training loss: 1.6245732307434082
Validation loss: 2.013753096262614

Epoch: 6| Step: 2
Training loss: 1.831132411956787
Validation loss: 2.0272881786028543

Epoch: 6| Step: 3
Training loss: 2.130988597869873
Validation loss: 2.0312921603520713

Epoch: 6| Step: 4
Training loss: 2.428588390350342
Validation loss: 2.0280949672063193

Epoch: 6| Step: 5
Training loss: 2.544290065765381
Validation loss: 2.0167264143625894

Epoch: 6| Step: 6
Training loss: 3.0720417499542236
Validation loss: 2.0122860868771872

Epoch: 6| Step: 7
Training loss: 2.0697431564331055
Validation loss: 2.0130945444107056

Epoch: 6| Step: 8
Training loss: 1.9498038291931152
Validation loss: 2.0093604922294617

Epoch: 6| Step: 9
Training loss: 2.103074789047241
Validation loss: 2.023163298765818

Epoch: 6| Step: 10
Training loss: 2.0318384170532227
Validation loss: 2.032131552696228

Epoch: 6| Step: 11
Training loss: 1.9981619119644165
Validation loss: 2.017654279867808

Epoch: 6| Step: 12
Training loss: 2.7839300632476807
Validation loss: 2.025861124197642

Epoch: 6| Step: 13
Training loss: 1.6583635807037354
Validation loss: 2.0209723114967346

Epoch: 79| Step: 0
Training loss: 2.2159128189086914
Validation loss: 2.032968203226725

Epoch: 6| Step: 1
Training loss: 2.482032299041748
Validation loss: 2.020243207613627

Epoch: 6| Step: 2
Training loss: 1.7546528577804565
Validation loss: 2.0204007824261985

Epoch: 6| Step: 3
Training loss: 2.235316038131714
Validation loss: 2.0196072657903037

Epoch: 6| Step: 4
Training loss: 2.295991897583008
Validation loss: 2.012835423151652

Epoch: 6| Step: 5
Training loss: 1.9840149879455566
Validation loss: 2.007190922896067

Epoch: 6| Step: 6
Training loss: 2.176910638809204
Validation loss: 2.0152366956075034

Epoch: 6| Step: 7
Training loss: 2.283310890197754
Validation loss: 2.0082253416379294

Epoch: 6| Step: 8
Training loss: 2.174119234085083
Validation loss: 2.006148954232534

Epoch: 6| Step: 9
Training loss: 2.7035489082336426
Validation loss: 2.0157501498858132

Epoch: 6| Step: 10
Training loss: 2.539783477783203
Validation loss: 2.009295483430227

Epoch: 6| Step: 11
Training loss: 2.0998435020446777
Validation loss: 2.021694302558899

Epoch: 6| Step: 12
Training loss: 1.588269591331482
Validation loss: 2.0329107443491616

Epoch: 6| Step: 13
Training loss: 1.5103461742401123
Validation loss: 2.025612751642863

Epoch: 80| Step: 0
Training loss: 2.283125877380371
Validation loss: 2.0339273611704507

Epoch: 6| Step: 1
Training loss: 2.0421507358551025
Validation loss: 2.042106866836548

Epoch: 6| Step: 2
Training loss: 2.6594409942626953
Validation loss: 2.0286619067192078

Epoch: 6| Step: 3
Training loss: 2.4393367767333984
Validation loss: 2.031711975733439

Epoch: 6| Step: 4
Training loss: 1.907407283782959
Validation loss: 2.0418396393458047

Epoch: 6| Step: 5
Training loss: 2.022047519683838
Validation loss: 2.036966383457184

Epoch: 6| Step: 6
Training loss: 2.3423399925231934
Validation loss: 2.028921902179718

Epoch: 6| Step: 7
Training loss: 2.6178979873657227
Validation loss: 2.032527208328247

Epoch: 6| Step: 8
Training loss: 2.2042460441589355
Validation loss: 2.0323721766471863

Epoch: 6| Step: 9
Training loss: 2.491678476333618
Validation loss: 2.0143526593844094

Epoch: 6| Step: 10
Training loss: 1.6261173486709595
Validation loss: 2.002884805202484

Epoch: 6| Step: 11
Training loss: 1.9508254528045654
Validation loss: 2.0127607583999634

Epoch: 6| Step: 12
Training loss: 1.2378886938095093
Validation loss: 2.009364108244578

Epoch: 6| Step: 13
Training loss: 2.3474154472351074
Validation loss: 2.002607762813568

Epoch: 81| Step: 0
Training loss: 2.289093017578125
Validation loss: 2.0078575015068054

Epoch: 6| Step: 1
Training loss: 2.430377244949341
Validation loss: 2.0031665563583374

Epoch: 6| Step: 2
Training loss: 2.8740386962890625
Validation loss: 2.0063975056012473

Epoch: 6| Step: 3
Training loss: 2.036424160003662
Validation loss: 2.0003772576649985

Epoch: 6| Step: 4
Training loss: 2.359577178955078
Validation loss: 1.999966025352478

Epoch: 6| Step: 5
Training loss: 2.014826774597168
Validation loss: 2.006262242794037

Epoch: 6| Step: 6
Training loss: 2.4419124126434326
Validation loss: 2.008211354414622

Epoch: 6| Step: 7
Training loss: 2.1881728172302246
Validation loss: 2.008922755718231

Epoch: 6| Step: 8
Training loss: 1.3133782148361206
Validation loss: 2.003556032975515

Epoch: 6| Step: 9
Training loss: 1.9683616161346436
Validation loss: 2.005611797173818

Epoch: 6| Step: 10
Training loss: 1.6093624830245972
Validation loss: 2.00320831934611

Epoch: 6| Step: 11
Training loss: 2.4373645782470703
Validation loss: 2.0231299002965293

Epoch: 6| Step: 12
Training loss: 2.381129741668701
Validation loss: 2.002592305342356

Epoch: 6| Step: 13
Training loss: 1.7278262376785278
Validation loss: 2.0219536225001016

Epoch: 82| Step: 0
Training loss: 1.5836284160614014
Validation loss: 2.0221824844678244

Epoch: 6| Step: 1
Training loss: 1.905399203300476
Validation loss: 2.0303388237953186

Epoch: 6| Step: 2
Training loss: 2.18215274810791
Validation loss: 2.0399482250213623

Epoch: 6| Step: 3
Training loss: 2.068204402923584
Validation loss: 2.028351445992788

Epoch: 6| Step: 4
Training loss: 1.797651767730713
Validation loss: 2.033078749974569

Epoch: 6| Step: 5
Training loss: 2.32381010055542
Validation loss: 2.0253615180651345

Epoch: 6| Step: 6
Training loss: 1.9847099781036377
Validation loss: 2.0259405374526978

Epoch: 6| Step: 7
Training loss: 2.521129608154297
Validation loss: 2.030208388964335

Epoch: 6| Step: 8
Training loss: 2.157395839691162
Validation loss: 2.0125234127044678

Epoch: 6| Step: 9
Training loss: 1.965928554534912
Validation loss: 2.0018426179885864

Epoch: 6| Step: 10
Training loss: 2.5534563064575195
Validation loss: 2.0084065397580466

Epoch: 6| Step: 11
Training loss: 2.7294211387634277
Validation loss: 2.013286848862966

Epoch: 6| Step: 12
Training loss: 1.9398159980773926
Validation loss: 2.0231277346611023

Epoch: 6| Step: 13
Training loss: 2.252028465270996
Validation loss: 2.0180936455726624

Epoch: 83| Step: 0
Training loss: 2.1902503967285156
Validation loss: 2.0216747323671975

Epoch: 6| Step: 1
Training loss: 1.5864055156707764
Validation loss: 2.0180353919665017

Epoch: 6| Step: 2
Training loss: 2.4095239639282227
Validation loss: 2.015584667523702

Epoch: 6| Step: 3
Training loss: 2.4495930671691895
Validation loss: 2.0145989855130515

Epoch: 6| Step: 4
Training loss: 2.2591917514801025
Validation loss: 2.0047486821810403

Epoch: 6| Step: 5
Training loss: 2.2398221492767334
Validation loss: 2.014466126759847

Epoch: 6| Step: 6
Training loss: 1.9588572978973389
Validation loss: 2.016309837500254

Epoch: 6| Step: 7
Training loss: 2.009274482727051
Validation loss: 2.024785319964091

Epoch: 6| Step: 8
Training loss: 1.7647761106491089
Validation loss: 2.0380847652753196

Epoch: 6| Step: 9
Training loss: 2.476271390914917
Validation loss: 2.0246757864952087

Epoch: 6| Step: 10
Training loss: 2.10261607170105
Validation loss: 2.0318798820177713

Epoch: 6| Step: 11
Training loss: 2.037703037261963
Validation loss: 2.040791471799215

Epoch: 6| Step: 12
Training loss: 2.5767359733581543
Validation loss: 2.0433587034543357

Epoch: 6| Step: 13
Training loss: 2.028640031814575
Validation loss: 2.044442097345988

Epoch: 84| Step: 0
Training loss: 2.0733871459960938
Validation loss: 2.051870584487915

Epoch: 6| Step: 1
Training loss: 1.7472703456878662
Validation loss: 2.0483344197273254

Epoch: 6| Step: 2
Training loss: 2.4748032093048096
Validation loss: 2.0319960713386536

Epoch: 6| Step: 3
Training loss: 1.7328208684921265
Validation loss: 2.027569671471914

Epoch: 6| Step: 4
Training loss: 1.9022548198699951
Validation loss: 2.018488963445028

Epoch: 6| Step: 5
Training loss: 1.9003641605377197
Validation loss: 2.0158138275146484

Epoch: 6| Step: 6
Training loss: 2.2579102516174316
Validation loss: 2.0110182960828147

Epoch: 6| Step: 7
Training loss: 2.781447410583496
Validation loss: 2.0104763905207315

Epoch: 6| Step: 8
Training loss: 1.7077888250350952
Validation loss: 2.010585923989614

Epoch: 6| Step: 9
Training loss: 2.7518296241760254
Validation loss: 2.0106322964032493

Epoch: 6| Step: 10
Training loss: 2.250645160675049
Validation loss: 2.0156142711639404

Epoch: 6| Step: 11
Training loss: 2.480100154876709
Validation loss: 2.0124537547429404

Epoch: 6| Step: 12
Training loss: 2.229612350463867
Validation loss: 2.021327177683512

Epoch: 6| Step: 13
Training loss: 1.8384191989898682
Validation loss: 2.0142565766970315

Epoch: 85| Step: 0
Training loss: 2.5063953399658203
Validation loss: 2.0125122666358948

Epoch: 6| Step: 1
Training loss: 2.0432004928588867
Validation loss: 2.0233943661053977

Epoch: 6| Step: 2
Training loss: 2.050747871398926
Validation loss: 2.029165267944336

Epoch: 6| Step: 3
Training loss: 1.8671514987945557
Validation loss: 2.0360469023386636

Epoch: 6| Step: 4
Training loss: 2.046023368835449
Validation loss: 2.0300049781799316

Epoch: 6| Step: 5
Training loss: 2.287722110748291
Validation loss: 2.0333288311958313

Epoch: 6| Step: 6
Training loss: 2.0501925945281982
Validation loss: 2.0298897425333657

Epoch: 6| Step: 7
Training loss: 2.4823365211486816
Validation loss: 2.0258928140004477

Epoch: 6| Step: 8
Training loss: 2.581141233444214
Validation loss: 2.0352389216423035

Epoch: 6| Step: 9
Training loss: 2.315871238708496
Validation loss: 2.0422977606455484

Epoch: 6| Step: 10
Training loss: 1.555762767791748
Validation loss: 2.0428660909334817

Epoch: 6| Step: 11
Training loss: 1.4697997570037842
Validation loss: 2.0466830333073935

Epoch: 6| Step: 12
Training loss: 2.157557725906372
Validation loss: 2.0485485394795737

Epoch: 6| Step: 13
Training loss: 2.651442766189575
Validation loss: 2.0428101420402527

Epoch: 86| Step: 0
Training loss: 2.2973952293395996
Validation loss: 2.0377604365348816

Epoch: 6| Step: 1
Training loss: 2.07562255859375
Validation loss: 2.021625359853109

Epoch: 6| Step: 2
Training loss: 2.0866172313690186
Validation loss: 2.0068549712498984

Epoch: 6| Step: 3
Training loss: 2.993807315826416
Validation loss: 2.01753960053126

Epoch: 6| Step: 4
Training loss: 2.2260892391204834
Validation loss: 2.0112070639928183

Epoch: 6| Step: 5
Training loss: 2.038315534591675
Validation loss: 2.018298923969269

Epoch: 6| Step: 6
Training loss: 1.9286742210388184
Validation loss: 2.010644555091858

Epoch: 6| Step: 7
Training loss: 2.0819458961486816
Validation loss: 2.019598662853241

Epoch: 6| Step: 8
Training loss: 2.3937182426452637
Validation loss: 2.019863784313202

Epoch: 6| Step: 9
Training loss: 1.7375890016555786
Validation loss: 2.0090349912643433

Epoch: 6| Step: 10
Training loss: 2.042112350463867
Validation loss: 2.004065672556559

Epoch: 6| Step: 11
Training loss: 2.085904598236084
Validation loss: 2.0063217878341675

Epoch: 6| Step: 12
Training loss: 2.0640993118286133
Validation loss: 2.009874622027079

Epoch: 6| Step: 13
Training loss: 1.9961224794387817
Validation loss: 2.0043298403422036

Epoch: 87| Step: 0
Training loss: 2.0473906993865967
Validation loss: 1.9936283429463704

Epoch: 6| Step: 1
Training loss: 1.8837106227874756
Validation loss: 1.9984946846961975

Epoch: 6| Step: 2
Training loss: 2.2622427940368652
Validation loss: 2.0009504755338035

Epoch: 6| Step: 3
Training loss: 1.7153691053390503
Validation loss: 2.010081708431244

Epoch: 6| Step: 4
Training loss: 2.9543752670288086
Validation loss: 2.0308427015940347

Epoch: 6| Step: 5
Training loss: 2.3878989219665527
Validation loss: 2.0382524331410727

Epoch: 6| Step: 6
Training loss: 2.505826473236084
Validation loss: 2.0525464614232383

Epoch: 6| Step: 7
Training loss: 2.62834095954895
Validation loss: 2.034934182961782

Epoch: 6| Step: 8
Training loss: 1.9175680875778198
Validation loss: 2.0375225146611533

Epoch: 6| Step: 9
Training loss: 1.7252111434936523
Validation loss: 2.039998988310496

Epoch: 6| Step: 10
Training loss: 1.7810850143432617
Validation loss: 2.0348063906033835

Epoch: 6| Step: 11
Training loss: 1.5169310569763184
Validation loss: 2.0186749498049417

Epoch: 6| Step: 12
Training loss: 2.3803372383117676
Validation loss: 2.0138049523035684

Epoch: 6| Step: 13
Training loss: 2.467057704925537
Validation loss: 2.0017269253730774

Epoch: 88| Step: 0
Training loss: 2.12337064743042
Validation loss: 2.0004030664761863

Epoch: 6| Step: 1
Training loss: 2.341827630996704
Validation loss: 1.9964734117190044

Epoch: 6| Step: 2
Training loss: 2.2015068531036377
Validation loss: 2.002891262372335

Epoch: 6| Step: 3
Training loss: 1.9852712154388428
Validation loss: 2.000718355178833

Epoch: 6| Step: 4
Training loss: 2.0225954055786133
Validation loss: 2.0090019504229226

Epoch: 6| Step: 5
Training loss: 1.83688223361969
Validation loss: 2.000071426232656

Epoch: 6| Step: 6
Training loss: 1.61667799949646
Validation loss: 2.002459764480591

Epoch: 6| Step: 7
Training loss: 2.557537078857422
Validation loss: 2.0106696486473083

Epoch: 6| Step: 8
Training loss: 2.366276264190674
Validation loss: 2.014891425768534

Epoch: 6| Step: 9
Training loss: 2.205880641937256
Validation loss: 2.0221001704533896

Epoch: 6| Step: 10
Training loss: 1.7544893026351929
Validation loss: 2.016578753789266

Epoch: 6| Step: 11
Training loss: 2.4841084480285645
Validation loss: 2.0140241384506226

Epoch: 6| Step: 12
Training loss: 2.3795342445373535
Validation loss: 2.0194972157478333

Epoch: 6| Step: 13
Training loss: 2.0054800510406494
Validation loss: 2.027165492375692

Epoch: 89| Step: 0
Training loss: 2.1544885635375977
Validation loss: 2.0312240521113076

Epoch: 6| Step: 1
Training loss: 2.7597081661224365
Validation loss: 2.0277937054634094

Epoch: 6| Step: 2
Training loss: 1.8684853315353394
Validation loss: 2.03292053937912

Epoch: 6| Step: 3
Training loss: 2.1322145462036133
Validation loss: 2.03064755598704

Epoch: 6| Step: 4
Training loss: 2.0231480598449707
Validation loss: 2.042693773905436

Epoch: 6| Step: 5
Training loss: 2.2949910163879395
Validation loss: 2.034841020901998

Epoch: 6| Step: 6
Training loss: 2.135702133178711
Validation loss: 2.0445865591367087

Epoch: 6| Step: 7
Training loss: 2.4873013496398926
Validation loss: 2.0331217447916665

Epoch: 6| Step: 8
Training loss: 1.6556265354156494
Validation loss: 2.02700142065684

Epoch: 6| Step: 9
Training loss: 2.5516905784606934
Validation loss: 2.023224651813507

Epoch: 6| Step: 10
Training loss: 1.751139521598816
Validation loss: 2.0229879021644592

Epoch: 6| Step: 11
Training loss: 1.681449294090271
Validation loss: 2.0134553710619607

Epoch: 6| Step: 12
Training loss: 2.1720128059387207
Validation loss: 2.0296665827433267

Epoch: 6| Step: 13
Training loss: 2.0942342281341553
Validation loss: 2.0207219322522483

Epoch: 90| Step: 0
Training loss: 2.071627378463745
Validation loss: 2.014942010243734

Epoch: 6| Step: 1
Training loss: 1.8628507852554321
Validation loss: 2.0277296900749207

Epoch: 6| Step: 2
Training loss: 2.3187384605407715
Validation loss: 2.0225555102030435

Epoch: 6| Step: 3
Training loss: 2.7695517539978027
Validation loss: 2.0212611158688865

Epoch: 6| Step: 4
Training loss: 2.461766242980957
Validation loss: 2.019193629423777

Epoch: 6| Step: 5
Training loss: 1.6970078945159912
Validation loss: 2.0208969513575235

Epoch: 6| Step: 6
Training loss: 2.427361011505127
Validation loss: 2.0204870899518332

Epoch: 6| Step: 7
Training loss: 2.0977282524108887
Validation loss: 2.0281581481297812

Epoch: 6| Step: 8
Training loss: 2.228963613510132
Validation loss: 2.013172964255015

Epoch: 6| Step: 9
Training loss: 2.0105154514312744
Validation loss: 2.008467972278595

Epoch: 6| Step: 10
Training loss: 2.2938904762268066
Validation loss: 2.0124167005221048

Epoch: 6| Step: 11
Training loss: 2.027301788330078
Validation loss: 2.012044847011566

Epoch: 6| Step: 12
Training loss: 2.028533935546875
Validation loss: 2.0105768839518228

Epoch: 6| Step: 13
Training loss: 1.5009384155273438
Validation loss: 2.0093894799550376

Epoch: 91| Step: 0
Training loss: 1.8099265098571777
Validation loss: 2.0094594756762185

Epoch: 6| Step: 1
Training loss: 2.6609652042388916
Validation loss: 2.010703901449839

Epoch: 6| Step: 2
Training loss: 2.072355270385742
Validation loss: 2.0087485114733377

Epoch: 6| Step: 3
Training loss: 1.4158785343170166
Validation loss: 2.0105222264925637

Epoch: 6| Step: 4
Training loss: 1.6392654180526733
Validation loss: 1.9999473094940186

Epoch: 6| Step: 5
Training loss: 2.3461499214172363
Validation loss: 2.0042209029197693

Epoch: 6| Step: 6
Training loss: 2.3478903770446777
Validation loss: 2.012708604335785

Epoch: 6| Step: 7
Training loss: 1.7708892822265625
Validation loss: 2.011910359064738

Epoch: 6| Step: 8
Training loss: 1.9061347246170044
Validation loss: 2.0163405338923135

Epoch: 6| Step: 9
Training loss: 2.145458936691284
Validation loss: 2.019729653994242

Epoch: 6| Step: 10
Training loss: 2.3421812057495117
Validation loss: 2.022986590862274

Epoch: 6| Step: 11
Training loss: 2.521432876586914
Validation loss: 2.012925843397776

Epoch: 6| Step: 12
Training loss: 2.3450794219970703
Validation loss: 2.013120492299398

Epoch: 6| Step: 13
Training loss: 2.698577880859375
Validation loss: 2.009960432847341

Epoch: 92| Step: 0
Training loss: 1.6106908321380615
Validation loss: 2.01456755399704

Epoch: 6| Step: 1
Training loss: 2.071262836456299
Validation loss: 2.0189077854156494

Epoch: 6| Step: 2
Training loss: 2.245434045791626
Validation loss: 2.025266706943512

Epoch: 6| Step: 3
Training loss: 2.2471890449523926
Validation loss: 2.025891125202179

Epoch: 6| Step: 4
Training loss: 1.6988095045089722
Validation loss: 2.016971925894419

Epoch: 6| Step: 5
Training loss: 2.4697532653808594
Validation loss: 2.017953077952067

Epoch: 6| Step: 6
Training loss: 2.6723246574401855
Validation loss: 2.0158551931381226

Epoch: 6| Step: 7
Training loss: 2.0765490531921387
Validation loss: 2.0212637980779014

Epoch: 6| Step: 8
Training loss: 2.2282707691192627
Validation loss: 2.0154149929682412

Epoch: 6| Step: 9
Training loss: 1.8696963787078857
Validation loss: 2.0147885282834372

Epoch: 6| Step: 10
Training loss: 2.3141870498657227
Validation loss: 2.025249660015106

Epoch: 6| Step: 11
Training loss: 1.3680930137634277
Validation loss: 2.017453948656718

Epoch: 6| Step: 12
Training loss: 2.6683390140533447
Validation loss: 2.0162530144055686

Epoch: 6| Step: 13
Training loss: 2.59641170501709
Validation loss: 2.011557161808014

Epoch: 93| Step: 0
Training loss: 2.0326027870178223
Validation loss: 2.017185648282369

Epoch: 6| Step: 1
Training loss: 2.0826761722564697
Validation loss: 2.0231707294782004

Epoch: 6| Step: 2
Training loss: 2.050922393798828
Validation loss: 2.0208080808321633

Epoch: 6| Step: 3
Training loss: 2.367488145828247
Validation loss: 2.0256125728289285

Epoch: 6| Step: 4
Training loss: 2.9450106620788574
Validation loss: 2.0438289046287537

Epoch: 6| Step: 5
Training loss: 2.590615749359131
Validation loss: 2.0655253529548645

Epoch: 6| Step: 6
Training loss: 1.5972661972045898
Validation loss: 2.0613688429196677

Epoch: 6| Step: 7
Training loss: 2.3553314208984375
Validation loss: 2.0533133347829184

Epoch: 6| Step: 8
Training loss: 2.140353202819824
Validation loss: 2.0487919052441916

Epoch: 6| Step: 9
Training loss: 2.066807746887207
Validation loss: 2.046017607053121

Epoch: 6| Step: 10
Training loss: 2.1393675804138184
Validation loss: 2.0416305462519326

Epoch: 6| Step: 11
Training loss: 2.2034554481506348
Validation loss: 2.033560554186503

Epoch: 6| Step: 12
Training loss: 1.782204270362854
Validation loss: 2.0221406618754068

Epoch: 6| Step: 13
Training loss: 1.5557512044906616
Validation loss: 2.032524049282074

Epoch: 94| Step: 0
Training loss: 1.8969717025756836
Validation loss: 2.0162948966026306

Epoch: 6| Step: 1
Training loss: 1.9995677471160889
Validation loss: 2.028266509373983

Epoch: 6| Step: 2
Training loss: 2.0607998371124268
Validation loss: 2.0302826960881553

Epoch: 6| Step: 3
Training loss: 2.2117369174957275
Validation loss: 2.0363304813702903

Epoch: 6| Step: 4
Training loss: 2.649419069290161
Validation loss: 2.0344319343566895

Epoch: 6| Step: 5
Training loss: 1.759108066558838
Validation loss: 2.0401716430981955

Epoch: 6| Step: 6
Training loss: 2.195779323577881
Validation loss: 2.032449642817179

Epoch: 6| Step: 7
Training loss: 1.6620354652404785
Validation loss: 2.02502578496933

Epoch: 6| Step: 8
Training loss: 2.533555507659912
Validation loss: 2.0340632597605386

Epoch: 6| Step: 9
Training loss: 1.7613978385925293
Validation loss: 2.0307190815607705

Epoch: 6| Step: 10
Training loss: 2.435934066772461
Validation loss: 2.0320880810419717

Epoch: 6| Step: 11
Training loss: 2.2107796669006348
Validation loss: 2.0303208231925964

Epoch: 6| Step: 12
Training loss: 2.694293975830078
Validation loss: 2.0407291054725647

Epoch: 6| Step: 13
Training loss: 1.5880626440048218
Validation loss: 2.025907039642334

Epoch: 95| Step: 0
Training loss: 2.478908061981201
Validation loss: 2.034691552321116

Epoch: 6| Step: 1
Training loss: 1.8294801712036133
Validation loss: 2.0302527149518332

Epoch: 6| Step: 2
Training loss: 2.061875820159912
Validation loss: 2.0320637822151184

Epoch: 6| Step: 3
Training loss: 1.0427006483078003
Validation loss: 2.017884075641632

Epoch: 6| Step: 4
Training loss: 2.5540521144866943
Validation loss: 2.029900153477987

Epoch: 6| Step: 5
Training loss: 2.2350330352783203
Validation loss: 2.031376282374064

Epoch: 6| Step: 6
Training loss: 1.6000127792358398
Validation loss: 2.0297419230143228

Epoch: 6| Step: 7
Training loss: 1.883009672164917
Validation loss: 2.0225008527437844

Epoch: 6| Step: 8
Training loss: 2.104058265686035
Validation loss: 2.0373446146647134

Epoch: 6| Step: 9
Training loss: 1.9356698989868164
Validation loss: 2.0206203858057656

Epoch: 6| Step: 10
Training loss: 2.8385236263275146
Validation loss: 2.025582750638326

Epoch: 6| Step: 11
Training loss: 1.918468952178955
Validation loss: 2.015723546346029

Epoch: 6| Step: 12
Training loss: 2.029475212097168
Validation loss: 2.0147090355555215

Epoch: 6| Step: 13
Training loss: 2.972639560699463
Validation loss: 2.020226836204529

Epoch: 96| Step: 0
Training loss: 2.044759511947632
Validation loss: 2.0196807384490967

Epoch: 6| Step: 1
Training loss: 1.7971272468566895
Validation loss: 2.0259833534558616

Epoch: 6| Step: 2
Training loss: 1.7132084369659424
Validation loss: 2.022781014442444

Epoch: 6| Step: 3
Training loss: 1.8473434448242188
Validation loss: 2.0070667068163552

Epoch: 6| Step: 4
Training loss: 2.7330126762390137
Validation loss: 2.010098934173584

Epoch: 6| Step: 5
Training loss: 2.5838427543640137
Validation loss: 2.016967217127482

Epoch: 6| Step: 6
Training loss: 2.084834337234497
Validation loss: 2.0119378765424094

Epoch: 6| Step: 7
Training loss: 2.06280517578125
Validation loss: 2.015568276246389

Epoch: 6| Step: 8
Training loss: 1.8278553485870361
Validation loss: 2.0145543018976846

Epoch: 6| Step: 9
Training loss: 2.475925922393799
Validation loss: 2.0100592772165933

Epoch: 6| Step: 10
Training loss: 2.3580970764160156
Validation loss: 2.0206992626190186

Epoch: 6| Step: 11
Training loss: 2.3331613540649414
Validation loss: 2.0304333170255027

Epoch: 6| Step: 12
Training loss: 2.1925294399261475
Validation loss: 2.0305891831715903

Epoch: 6| Step: 13
Training loss: 1.7555609941482544
Validation loss: 2.0404672622680664

Epoch: 97| Step: 0
Training loss: 1.6199957132339478
Validation loss: 2.036673823992411

Epoch: 6| Step: 1
Training loss: 2.0897035598754883
Validation loss: 2.0388811429341636

Epoch: 6| Step: 2
Training loss: 2.2909934520721436
Validation loss: 2.043183167775472

Epoch: 6| Step: 3
Training loss: 2.2809338569641113
Validation loss: 2.044400135676066

Epoch: 6| Step: 4
Training loss: 2.6282901763916016
Validation loss: 2.045812408129374

Epoch: 6| Step: 5
Training loss: 2.581885814666748
Validation loss: 2.0250834027926126

Epoch: 6| Step: 6
Training loss: 1.9277570247650146
Validation loss: 2.0160839955012

Epoch: 6| Step: 7
Training loss: 1.6571965217590332
Validation loss: 2.010202169418335

Epoch: 6| Step: 8
Training loss: 2.1410410404205322
Validation loss: 2.0146623452504477

Epoch: 6| Step: 9
Training loss: 1.8549433946609497
Validation loss: 2.0234808127085366

Epoch: 6| Step: 10
Training loss: 2.4894657135009766
Validation loss: 2.0127169489860535

Epoch: 6| Step: 11
Training loss: 2.0613865852355957
Validation loss: 2.0140562057495117

Epoch: 6| Step: 12
Training loss: 2.1786246299743652
Validation loss: 2.008551319440206

Epoch: 6| Step: 13
Training loss: 1.9667561054229736
Validation loss: 2.016479790210724

Epoch: 98| Step: 0
Training loss: 1.559168815612793
Validation loss: 2.019502063592275

Epoch: 6| Step: 1
Training loss: 2.080679416656494
Validation loss: 2.00793194770813

Epoch: 6| Step: 2
Training loss: 1.8142362833023071
Validation loss: 2.023313283920288

Epoch: 6| Step: 3
Training loss: 2.032740592956543
Validation loss: 2.015942176183065

Epoch: 6| Step: 4
Training loss: 2.343522548675537
Validation loss: 2.033272862434387

Epoch: 6| Step: 5
Training loss: 2.3270528316497803
Validation loss: 2.0188964207967124

Epoch: 6| Step: 6
Training loss: 1.911262035369873
Validation loss: 2.028204162915548

Epoch: 6| Step: 7
Training loss: 2.053788423538208
Validation loss: 2.0233818689982095

Epoch: 6| Step: 8
Training loss: 2.213712453842163
Validation loss: 2.0385407209396362

Epoch: 6| Step: 9
Training loss: 2.3726062774658203
Validation loss: 2.0371286869049072

Epoch: 6| Step: 10
Training loss: 2.2287323474884033
Validation loss: 2.0270818074544272

Epoch: 6| Step: 11
Training loss: 1.902484655380249
Validation loss: 2.0326377948125205

Epoch: 6| Step: 12
Training loss: 1.9200446605682373
Validation loss: 2.0317672888437905

Epoch: 6| Step: 13
Training loss: 2.801074266433716
Validation loss: 2.0366624196370444

Epoch: 99| Step: 0
Training loss: 2.0522449016571045
Validation loss: 2.0378511150678

Epoch: 6| Step: 1
Training loss: 2.4347853660583496
Validation loss: 2.0319793621699014

Epoch: 6| Step: 2
Training loss: 2.289896249771118
Validation loss: 2.0340983271598816

Epoch: 6| Step: 3
Training loss: 2.2359461784362793
Validation loss: 2.041348318258921

Epoch: 6| Step: 4
Training loss: 1.6690163612365723
Validation loss: 2.0371784369150796

Epoch: 6| Step: 5
Training loss: 1.8901419639587402
Validation loss: 2.034420430660248

Epoch: 6| Step: 6
Training loss: 2.3627431392669678
Validation loss: 2.0458295742670694

Epoch: 6| Step: 7
Training loss: 2.0149550437927246
Validation loss: 2.0416150291760764

Epoch: 6| Step: 8
Training loss: 2.1000308990478516
Validation loss: 2.0340319673220315

Epoch: 6| Step: 9
Training loss: 2.589606761932373
Validation loss: 2.040930171807607

Epoch: 6| Step: 10
Training loss: 2.1512014865875244
Validation loss: 2.0383979082107544

Epoch: 6| Step: 11
Training loss: 1.80111825466156
Validation loss: 2.056466539700826

Epoch: 6| Step: 12
Training loss: 2.0198047161102295
Validation loss: 2.060666084289551

Epoch: 6| Step: 13
Training loss: 1.7674040794372559
Validation loss: 2.0487807591756186

Epoch: 100| Step: 0
Training loss: 1.9518009424209595
Validation loss: 2.0458818078041077

Epoch: 6| Step: 1
Training loss: 2.17274808883667
Validation loss: 2.055469830830892

Epoch: 6| Step: 2
Training loss: 1.7829456329345703
Validation loss: 2.047533949216207

Epoch: 6| Step: 3
Training loss: 1.9880146980285645
Validation loss: 2.0366083780924478

Epoch: 6| Step: 4
Training loss: 2.2510204315185547
Validation loss: 2.0412123998006186

Epoch: 6| Step: 5
Training loss: 2.705313205718994
Validation loss: 2.0228110551834106

Epoch: 6| Step: 6
Training loss: 1.630508542060852
Validation loss: 2.0192079544067383

Epoch: 6| Step: 7
Training loss: 3.160362958908081
Validation loss: 2.0290207465489707

Epoch: 6| Step: 8
Training loss: 1.8031504154205322
Validation loss: 2.0248374342918396

Epoch: 6| Step: 9
Training loss: 1.557297706604004
Validation loss: 2.0222912430763245

Epoch: 6| Step: 10
Training loss: 2.2175631523132324
Validation loss: 2.0227526823679605

Epoch: 6| Step: 11
Training loss: 2.6894350051879883
Validation loss: 2.028883675734202

Epoch: 6| Step: 12
Training loss: 1.5084028244018555
Validation loss: 2.0378266970316568

Epoch: 6| Step: 13
Training loss: 2.1178557872772217
Validation loss: 2.0290353496869407

Epoch: 101| Step: 0
Training loss: 2.2059121131896973
Validation loss: 2.045232812563578

Epoch: 6| Step: 1
Training loss: 2.0730793476104736
Validation loss: 2.0533884167671204

Epoch: 6| Step: 2
Training loss: 2.4283242225646973
Validation loss: 2.053825398286184

Epoch: 6| Step: 3
Training loss: 1.983575463294983
Validation loss: 2.0604758659998574

Epoch: 6| Step: 4
Training loss: 1.7213290929794312
Validation loss: 2.063943862915039

Epoch: 6| Step: 5
Training loss: 2.9448983669281006
Validation loss: 2.066928426424662

Epoch: 6| Step: 6
Training loss: 2.421085834503174
Validation loss: 2.046906530857086

Epoch: 6| Step: 7
Training loss: 2.162663459777832
Validation loss: 2.0368333061536155

Epoch: 6| Step: 8
Training loss: 2.1227774620056152
Validation loss: 2.0271921952565513

Epoch: 6| Step: 9
Training loss: 1.7574043273925781
Validation loss: 2.021988789240519

Epoch: 6| Step: 10
Training loss: 2.024001121520996
Validation loss: 2.020176331202189

Epoch: 6| Step: 11
Training loss: 2.2910549640655518
Validation loss: 2.019384741783142

Epoch: 6| Step: 12
Training loss: 1.5708438158035278
Validation loss: 2.0085520346959433

Epoch: 6| Step: 13
Training loss: 2.24062442779541
Validation loss: 2.0080762108167014

Epoch: 102| Step: 0
Training loss: 2.4473085403442383
Validation loss: 2.0098120172818503

Epoch: 6| Step: 1
Training loss: 1.733396291732788
Validation loss: 2.0159888664881387

Epoch: 6| Step: 2
Training loss: 2.3171401023864746
Validation loss: 2.015083372592926

Epoch: 6| Step: 3
Training loss: 2.248764991760254
Validation loss: 2.002168357372284

Epoch: 6| Step: 4
Training loss: 2.0107712745666504
Validation loss: 2.0097198287645974

Epoch: 6| Step: 5
Training loss: 1.740780234336853
Validation loss: 2.0151474674542746

Epoch: 6| Step: 6
Training loss: 2.19872784614563
Validation loss: 2.0076043009757996

Epoch: 6| Step: 7
Training loss: 1.7967846393585205
Validation loss: 2.011291960875193

Epoch: 6| Step: 8
Training loss: 2.129305362701416
Validation loss: 2.0065257946650186

Epoch: 6| Step: 9
Training loss: 2.0837087631225586
Validation loss: 2.0081518491109214

Epoch: 6| Step: 10
Training loss: 2.331486701965332
Validation loss: 2.013879915078481

Epoch: 6| Step: 11
Training loss: 2.177278518676758
Validation loss: 2.0145717660586038

Epoch: 6| Step: 12
Training loss: 2.7114744186401367
Validation loss: 2.0252078970273337

Epoch: 6| Step: 13
Training loss: 1.963450312614441
Validation loss: 2.0288692315419516

Epoch: 103| Step: 0
Training loss: 1.790247917175293
Validation loss: 2.03304930528005

Epoch: 6| Step: 1
Training loss: 1.8975831270217896
Validation loss: 2.0316487153371177

Epoch: 6| Step: 2
Training loss: 1.845503568649292
Validation loss: 2.03261536359787

Epoch: 6| Step: 3
Training loss: 2.6635444164276123
Validation loss: 2.026152511437734

Epoch: 6| Step: 4
Training loss: 1.3945013284683228
Validation loss: 2.027001976966858

Epoch: 6| Step: 5
Training loss: 1.716331958770752
Validation loss: 2.0372501015663147

Epoch: 6| Step: 6
Training loss: 2.5919907093048096
Validation loss: 2.0528380473454795

Epoch: 6| Step: 7
Training loss: 2.163813829421997
Validation loss: 2.072030504544576

Epoch: 6| Step: 8
Training loss: 2.614285945892334
Validation loss: 2.0474633971850076

Epoch: 6| Step: 9
Training loss: 2.0251758098602295
Validation loss: 2.06131245692571

Epoch: 6| Step: 10
Training loss: 1.8281564712524414
Validation loss: 2.063980281352997

Epoch: 6| Step: 11
Training loss: 2.5757524967193604
Validation loss: 2.0591041644414267

Epoch: 6| Step: 12
Training loss: 2.2729835510253906
Validation loss: 2.06326433022817

Epoch: 6| Step: 13
Training loss: 2.156148910522461
Validation loss: 2.0534047285715737

Epoch: 104| Step: 0
Training loss: 1.5702366828918457
Validation loss: 2.0363760391871133

Epoch: 6| Step: 1
Training loss: 2.1864688396453857
Validation loss: 2.0327503085136414

Epoch: 6| Step: 2
Training loss: 1.6331716775894165
Validation loss: 2.0336619218190513

Epoch: 6| Step: 3
Training loss: 1.9978801012039185
Validation loss: 2.0410212675730386

Epoch: 6| Step: 4
Training loss: 2.4803214073181152
Validation loss: 2.0360596577326455

Epoch: 6| Step: 5
Training loss: 2.412051200866699
Validation loss: 2.0299744407335916

Epoch: 6| Step: 6
Training loss: 2.6909897327423096
Validation loss: 2.048673391342163

Epoch: 6| Step: 7
Training loss: 1.6086853742599487
Validation loss: 2.0414947470029197

Epoch: 6| Step: 8
Training loss: 2.342367172241211
Validation loss: 2.0354462265968323

Epoch: 6| Step: 9
Training loss: 2.3695077896118164
Validation loss: 2.0427949825922647

Epoch: 6| Step: 10
Training loss: 2.27974796295166
Validation loss: 2.0267533659934998

Epoch: 6| Step: 11
Training loss: 1.8954215049743652
Validation loss: 2.037020762761434

Epoch: 6| Step: 12
Training loss: 2.310415267944336
Validation loss: 2.0316569010416665

Epoch: 6| Step: 13
Training loss: 1.4903678894042969
Validation loss: 2.035311142603556

Epoch: 105| Step: 0
Training loss: 1.9976493120193481
Validation loss: 2.0290976961453757

Epoch: 6| Step: 1
Training loss: 1.7848436832427979
Validation loss: 2.0302157203356423

Epoch: 6| Step: 2
Training loss: 2.4504141807556152
Validation loss: 2.028435548146566

Epoch: 6| Step: 3
Training loss: 1.5720860958099365
Validation loss: 2.036475638548533

Epoch: 6| Step: 4
Training loss: 2.8236048221588135
Validation loss: 2.049518624941508

Epoch: 6| Step: 5
Training loss: 2.2074217796325684
Validation loss: 2.045396367708842

Epoch: 6| Step: 6
Training loss: 1.3933541774749756
Validation loss: 2.0421112775802612

Epoch: 6| Step: 7
Training loss: 2.11855149269104
Validation loss: 2.032397468884786

Epoch: 6| Step: 8
Training loss: 2.227823257446289
Validation loss: 2.045946180820465

Epoch: 6| Step: 9
Training loss: 1.78385329246521
Validation loss: 2.0479834278424582

Epoch: 6| Step: 10
Training loss: 1.796492099761963
Validation loss: 2.0477267503738403

Epoch: 6| Step: 11
Training loss: 2.1896986961364746
Validation loss: 2.049615760644277

Epoch: 6| Step: 12
Training loss: 2.430039405822754
Validation loss: 2.0403672059377036

Epoch: 6| Step: 13
Training loss: 2.3771491050720215
Validation loss: 2.0329681634902954

Epoch: 106| Step: 0
Training loss: 2.056791067123413
Validation loss: 2.0373533964157104

Epoch: 6| Step: 1
Training loss: 1.8196823596954346
Validation loss: 2.0491191744804382

Epoch: 6| Step: 2
Training loss: 1.9633517265319824
Validation loss: 2.0351286927858987

Epoch: 6| Step: 3
Training loss: 1.7197128534317017
Validation loss: 2.0283647775650024

Epoch: 6| Step: 4
Training loss: 1.8225109577178955
Validation loss: 2.0266554752985635

Epoch: 6| Step: 5
Training loss: 2.2981770038604736
Validation loss: 2.0247782667477927

Epoch: 6| Step: 6
Training loss: 1.4840285778045654
Validation loss: 2.0354557434717813

Epoch: 6| Step: 7
Training loss: 2.3534092903137207
Validation loss: 2.0324862400690713

Epoch: 6| Step: 8
Training loss: 2.148477554321289
Validation loss: 2.0397433042526245

Epoch: 6| Step: 9
Training loss: 2.4681830406188965
Validation loss: 2.0378077228864035

Epoch: 6| Step: 10
Training loss: 2.4359545707702637
Validation loss: 2.0332882404327393

Epoch: 6| Step: 11
Training loss: 1.9630885124206543
Validation loss: 2.021653672059377

Epoch: 6| Step: 12
Training loss: 2.3810043334960938
Validation loss: 2.0293567975362143

Epoch: 6| Step: 13
Training loss: 2.1762750148773193
Validation loss: 2.0314906239509583

Epoch: 107| Step: 0
Training loss: 2.065145492553711
Validation loss: 2.041449864705404

Epoch: 6| Step: 1
Training loss: 2.060953140258789
Validation loss: 2.0386831760406494

Epoch: 6| Step: 2
Training loss: 1.765146255493164
Validation loss: 2.0475846926371255

Epoch: 6| Step: 3
Training loss: 1.4403941631317139
Validation loss: 2.04413511355718

Epoch: 6| Step: 4
Training loss: 2.5867767333984375
Validation loss: 2.044225573539734

Epoch: 6| Step: 5
Training loss: 2.7121500968933105
Validation loss: 2.0447083115577698

Epoch: 6| Step: 6
Training loss: 2.1719512939453125
Validation loss: 2.0530718763669333

Epoch: 6| Step: 7
Training loss: 2.5458621978759766
Validation loss: 2.049467662970225

Epoch: 6| Step: 8
Training loss: 1.9609564542770386
Validation loss: 2.0607232054074607

Epoch: 6| Step: 9
Training loss: 2.3708744049072266
Validation loss: 2.0631762544314065

Epoch: 6| Step: 10
Training loss: 2.3310065269470215
Validation loss: 2.062391996383667

Epoch: 6| Step: 11
Training loss: 2.149871826171875
Validation loss: 2.0621193051338196

Epoch: 6| Step: 12
Training loss: 1.5169293880462646
Validation loss: 2.043915112813314

Epoch: 6| Step: 13
Training loss: 1.691986322402954
Validation loss: 2.0655064384142556

Epoch: 108| Step: 0
Training loss: 2.088205337524414
Validation loss: 2.047507663567861

Epoch: 6| Step: 1
Training loss: 1.9552271366119385
Validation loss: 2.0393394033114114

Epoch: 6| Step: 2
Training loss: 1.846451997756958
Validation loss: 2.035343130429586

Epoch: 6| Step: 3
Training loss: 1.9731478691101074
Validation loss: 2.038389027118683

Epoch: 6| Step: 4
Training loss: 1.9071431159973145
Validation loss: 2.0401307344436646

Epoch: 6| Step: 5
Training loss: 2.7440426349639893
Validation loss: 2.03978830575943

Epoch: 6| Step: 6
Training loss: 1.913198471069336
Validation loss: 2.0356914003690085

Epoch: 6| Step: 7
Training loss: 1.9247002601623535
Validation loss: 2.0326275626818338

Epoch: 6| Step: 8
Training loss: 2.0985076427459717
Validation loss: 2.0307502150535583

Epoch: 6| Step: 9
Training loss: 2.2363479137420654
Validation loss: 2.0363311171531677

Epoch: 6| Step: 10
Training loss: 2.1796250343322754
Validation loss: 2.0230783224105835

Epoch: 6| Step: 11
Training loss: 1.479345679283142
Validation loss: 2.028148353099823

Epoch: 6| Step: 12
Training loss: 2.016519546508789
Validation loss: 2.0253960490226746

Epoch: 6| Step: 13
Training loss: 2.716500759124756
Validation loss: 2.026896516482035

Epoch: 109| Step: 0
Training loss: 2.205476999282837
Validation loss: 2.016717791557312

Epoch: 6| Step: 1
Training loss: 1.5156383514404297
Validation loss: 2.0212470491727195

Epoch: 6| Step: 2
Training loss: 1.4513474702835083
Validation loss: 2.0196771025657654

Epoch: 6| Step: 3
Training loss: 2.2779502868652344
Validation loss: 2.0349918206532798

Epoch: 6| Step: 4
Training loss: 2.0924320220947266
Validation loss: 2.031424045562744

Epoch: 6| Step: 5
Training loss: 2.4885413646698
Validation loss: 2.039963960647583

Epoch: 6| Step: 6
Training loss: 2.3429388999938965
Validation loss: 2.042650818824768

Epoch: 6| Step: 7
Training loss: 1.6258509159088135
Validation loss: 2.049688736597697

Epoch: 6| Step: 8
Training loss: 2.0165207386016846
Validation loss: 2.05436239639918

Epoch: 6| Step: 9
Training loss: 2.5080535411834717
Validation loss: 2.0594958662986755

Epoch: 6| Step: 10
Training loss: 1.8196572065353394
Validation loss: 2.0656479795773826

Epoch: 6| Step: 11
Training loss: 2.0227763652801514
Validation loss: 2.056654413541158

Epoch: 6| Step: 12
Training loss: 2.1480019092559814
Validation loss: 2.0544349749883017

Epoch: 6| Step: 13
Training loss: 2.809549331665039
Validation loss: 2.0484644174575806

Epoch: 110| Step: 0
Training loss: 2.2147364616394043
Validation loss: 2.0391056736310325

Epoch: 6| Step: 1
Training loss: 2.644719123840332
Validation loss: 2.0306489666303

Epoch: 6| Step: 2
Training loss: 1.73172926902771
Validation loss: 2.0273738304773965

Epoch: 6| Step: 3
Training loss: 2.016885280609131
Validation loss: 2.0262146592140198

Epoch: 6| Step: 4
Training loss: 2.4364771842956543
Validation loss: 2.0256784558296204

Epoch: 6| Step: 5
Training loss: 1.4869155883789062
Validation loss: 2.025404155254364

Epoch: 6| Step: 6
Training loss: 2.225217819213867
Validation loss: 2.0140212774276733

Epoch: 6| Step: 7
Training loss: 2.303405284881592
Validation loss: 2.0167662103970847

Epoch: 6| Step: 8
Training loss: 1.8057899475097656
Validation loss: 2.030444939931234

Epoch: 6| Step: 9
Training loss: 2.3378961086273193
Validation loss: 2.0408036510149636

Epoch: 6| Step: 10
Training loss: 1.9113519191741943
Validation loss: 2.039101004600525

Epoch: 6| Step: 11
Training loss: 2.1763641834259033
Validation loss: 2.0358208417892456

Epoch: 6| Step: 12
Training loss: 1.9138914346694946
Validation loss: 2.046898682912191

Epoch: 6| Step: 13
Training loss: 2.2384257316589355
Validation loss: 2.0501317381858826

Epoch: 111| Step: 0
Training loss: 1.9574934244155884
Validation loss: 2.053639034430186

Epoch: 6| Step: 1
Training loss: 2.1228039264678955
Validation loss: 2.0526113510131836

Epoch: 6| Step: 2
Training loss: 2.209228992462158
Validation loss: 2.0478386084238687

Epoch: 6| Step: 3
Training loss: 1.9917802810668945
Validation loss: 2.047510266304016

Epoch: 6| Step: 4
Training loss: 1.708921194076538
Validation loss: 2.036100149154663

Epoch: 6| Step: 5
Training loss: 1.627344012260437
Validation loss: 2.034508983294169

Epoch: 6| Step: 6
Training loss: 2.3867928981781006
Validation loss: 2.0159823894500732

Epoch: 6| Step: 7
Training loss: 2.0379178524017334
Validation loss: 2.0246679385503135

Epoch: 6| Step: 8
Training loss: 2.1154661178588867
Validation loss: 2.02119380235672

Epoch: 6| Step: 9
Training loss: 2.4120163917541504
Validation loss: 2.026842772960663

Epoch: 6| Step: 10
Training loss: 2.5789778232574463
Validation loss: 2.026409904162089

Epoch: 6| Step: 11
Training loss: 2.1051905155181885
Validation loss: 2.0266584356625876

Epoch: 6| Step: 12
Training loss: 2.6871323585510254
Validation loss: 2.0343472957611084

Epoch: 6| Step: 13
Training loss: 1.4541945457458496
Validation loss: 2.0195266803105674

Epoch: 112| Step: 0
Training loss: 2.0131325721740723
Validation loss: 2.041504164536794

Epoch: 6| Step: 1
Training loss: 1.759352207183838
Validation loss: 2.0514575839042664

Epoch: 6| Step: 2
Training loss: 1.8721460103988647
Validation loss: 2.045712133248647

Epoch: 6| Step: 3
Training loss: 2.0154926776885986
Validation loss: 2.038415332635244

Epoch: 6| Step: 4
Training loss: 2.2031772136688232
Validation loss: 2.0619914531707764

Epoch: 6| Step: 5
Training loss: 2.0977420806884766
Validation loss: 2.0736204385757446

Epoch: 6| Step: 6
Training loss: 2.632227897644043
Validation loss: 2.0809283455212912

Epoch: 6| Step: 7
Training loss: 2.2908565998077393
Validation loss: 2.0748895406723022

Epoch: 6| Step: 8
Training loss: 1.4048622846603394
Validation loss: 2.0525322953859964

Epoch: 6| Step: 9
Training loss: 1.881117582321167
Validation loss: 2.0685945749282837

Epoch: 6| Step: 10
Training loss: 1.7663098573684692
Validation loss: 2.051644206047058

Epoch: 6| Step: 11
Training loss: 2.659273147583008
Validation loss: 2.046672979990641

Epoch: 6| Step: 12
Training loss: 2.0224242210388184
Validation loss: 2.034839391708374

Epoch: 6| Step: 13
Training loss: 2.663609027862549
Validation loss: 2.0305222272872925

Epoch: 113| Step: 0
Training loss: 2.2974729537963867
Validation loss: 2.0257100065549216

Epoch: 6| Step: 1
Training loss: 2.1790945529937744
Validation loss: 2.0361033280690513

Epoch: 6| Step: 2
Training loss: 1.8850700855255127
Validation loss: 2.0317760705947876

Epoch: 6| Step: 3
Training loss: 1.9232439994812012
Validation loss: 2.0329312880833945

Epoch: 6| Step: 4
Training loss: 2.312592029571533
Validation loss: 2.0494914452234902

Epoch: 6| Step: 5
Training loss: 2.4004714488983154
Validation loss: 2.0422236919403076

Epoch: 6| Step: 6
Training loss: 1.6201380491256714
Validation loss: 2.0457164645195007

Epoch: 6| Step: 7
Training loss: 2.5030550956726074
Validation loss: 2.051310181617737

Epoch: 6| Step: 8
Training loss: 2.2087459564208984
Validation loss: 2.0590126713116965

Epoch: 6| Step: 9
Training loss: 1.6745014190673828
Validation loss: 2.054806709289551

Epoch: 6| Step: 10
Training loss: 1.807206630706787
Validation loss: 2.058236300945282

Epoch: 6| Step: 11
Training loss: 1.9764491319656372
Validation loss: 2.0606769323349

Epoch: 6| Step: 12
Training loss: 2.728867769241333
Validation loss: 2.065565844376882

Epoch: 6| Step: 13
Training loss: 1.8603904247283936
Validation loss: 2.0658522049585977

Epoch: 114| Step: 0
Training loss: 1.851529598236084
Validation loss: 2.04710723956426

Epoch: 6| Step: 1
Training loss: 1.987051248550415
Validation loss: 2.0511631766955056

Epoch: 6| Step: 2
Training loss: 2.1513962745666504
Validation loss: 2.049412031968435

Epoch: 6| Step: 3
Training loss: 1.9850943088531494
Validation loss: 2.0405918757120767

Epoch: 6| Step: 4
Training loss: 1.8569347858428955
Validation loss: 2.0452625155448914

Epoch: 6| Step: 5
Training loss: 1.453956127166748
Validation loss: 2.032532215118408

Epoch: 6| Step: 6
Training loss: 2.0208945274353027
Validation loss: 2.0307952960332236

Epoch: 6| Step: 7
Training loss: 2.5657992362976074
Validation loss: 2.024903118610382

Epoch: 6| Step: 8
Training loss: 2.1421945095062256
Validation loss: 2.0251227617263794

Epoch: 6| Step: 9
Training loss: 2.516103744506836
Validation loss: 2.024436275164286

Epoch: 6| Step: 10
Training loss: 2.5313901901245117
Validation loss: 2.0329230626424155

Epoch: 6| Step: 11
Training loss: 2.2352170944213867
Validation loss: 2.032357394695282

Epoch: 6| Step: 12
Training loss: 1.5543403625488281
Validation loss: 2.0412571827570596

Epoch: 6| Step: 13
Training loss: 2.336972951889038
Validation loss: 2.0415448347727456

Epoch: 115| Step: 0
Training loss: 2.5208067893981934
Validation loss: 2.05006076892217

Epoch: 6| Step: 1
Training loss: 2.319720983505249
Validation loss: 2.0498107075691223

Epoch: 6| Step: 2
Training loss: 1.8098297119140625
Validation loss: 2.0404842297236123

Epoch: 6| Step: 3
Training loss: 1.7827305793762207
Validation loss: 2.0515106320381165

Epoch: 6| Step: 4
Training loss: 2.1589794158935547
Validation loss: 2.054197450478872

Epoch: 6| Step: 5
Training loss: 1.9412645101547241
Validation loss: 2.0448211828867593

Epoch: 6| Step: 6
Training loss: 2.0906589031219482
Validation loss: 2.0532698035240173

Epoch: 6| Step: 7
Training loss: 1.6802994012832642
Validation loss: 2.0444677074750266

Epoch: 6| Step: 8
Training loss: 1.7878702878952026
Validation loss: 2.036640783150991

Epoch: 6| Step: 9
Training loss: 2.2408924102783203
Validation loss: 2.0321079095204673

Epoch: 6| Step: 10
Training loss: 2.319403648376465
Validation loss: 2.050824781258901

Epoch: 6| Step: 11
Training loss: 2.307790756225586
Validation loss: 2.056084950764974

Epoch: 6| Step: 12
Training loss: 2.142482280731201
Validation loss: 2.054383397102356

Epoch: 6| Step: 13
Training loss: 1.8087053298950195
Validation loss: 2.049382527669271

Epoch: 116| Step: 0
Training loss: 1.6237685680389404
Validation loss: 2.0594446261723838

Epoch: 6| Step: 1
Training loss: 1.9440768957138062
Validation loss: 2.0574899514516196

Epoch: 6| Step: 2
Training loss: 2.246823310852051
Validation loss: 2.0389317870140076

Epoch: 6| Step: 3
Training loss: 2.3358099460601807
Validation loss: 2.037236750125885

Epoch: 6| Step: 4
Training loss: 1.785300850868225
Validation loss: 2.0501750111579895

Epoch: 6| Step: 5
Training loss: 2.0175435543060303
Validation loss: 2.0384042461713157

Epoch: 6| Step: 6
Training loss: 1.552423119544983
Validation loss: 2.0422900517781577

Epoch: 6| Step: 7
Training loss: 2.03363037109375
Validation loss: 2.037537376085917

Epoch: 6| Step: 8
Training loss: 2.4837145805358887
Validation loss: 2.0352540214856467

Epoch: 6| Step: 9
Training loss: 1.703822135925293
Validation loss: 2.044246951738993

Epoch: 6| Step: 10
Training loss: 1.671177864074707
Validation loss: 2.046952803929647

Epoch: 6| Step: 11
Training loss: 2.7091708183288574
Validation loss: 2.044002195199331

Epoch: 6| Step: 12
Training loss: 3.0530781745910645
Validation loss: 2.0546615719795227

Epoch: 6| Step: 13
Training loss: 1.869260311126709
Validation loss: 2.0533846020698547

Epoch: 117| Step: 0
Training loss: 1.8718934059143066
Validation loss: 2.061990241209666

Epoch: 6| Step: 1
Training loss: 2.333667516708374
Validation loss: 2.0736746788024902

Epoch: 6| Step: 2
Training loss: 2.615354299545288
Validation loss: 2.0748929182688394

Epoch: 6| Step: 3
Training loss: 2.1920437812805176
Validation loss: 2.057606120904287

Epoch: 6| Step: 4
Training loss: 1.952728033065796
Validation loss: 2.053235193093618

Epoch: 6| Step: 5
Training loss: 1.9166749715805054
Validation loss: 2.0438355803489685

Epoch: 6| Step: 6
Training loss: 1.9314988851547241
Validation loss: 2.044603248437246

Epoch: 6| Step: 7
Training loss: 2.222504138946533
Validation loss: 2.046945095062256

Epoch: 6| Step: 8
Training loss: 1.9508795738220215
Validation loss: 2.0388280550638833

Epoch: 6| Step: 9
Training loss: 1.3920459747314453
Validation loss: 2.0463991165161133

Epoch: 6| Step: 10
Training loss: 2.6168410778045654
Validation loss: 2.038651943206787

Epoch: 6| Step: 11
Training loss: 1.614995002746582
Validation loss: 2.047618011633555

Epoch: 6| Step: 12
Training loss: 2.311746835708618
Validation loss: 2.049489756425222

Epoch: 6| Step: 13
Training loss: 2.1837894916534424
Validation loss: 2.0515498916308084

Epoch: 118| Step: 0
Training loss: 1.9333255290985107
Validation loss: 2.041616519292196

Epoch: 6| Step: 1
Training loss: 2.1228485107421875
Validation loss: 2.046995480855306

Epoch: 6| Step: 2
Training loss: 2.132080554962158
Validation loss: 2.0506516695022583

Epoch: 6| Step: 3
Training loss: 1.6511270999908447
Validation loss: 2.057898004849752

Epoch: 6| Step: 4
Training loss: 1.8066728115081787
Validation loss: 2.051144858201345

Epoch: 6| Step: 5
Training loss: 2.40689754486084
Validation loss: 2.050686160723368

Epoch: 6| Step: 6
Training loss: 2.177670955657959
Validation loss: 2.058371663093567

Epoch: 6| Step: 7
Training loss: 1.5487761497497559
Validation loss: 2.048648734887441

Epoch: 6| Step: 8
Training loss: 1.9492874145507812
Validation loss: 2.0419743259747825

Epoch: 6| Step: 9
Training loss: 1.9471731185913086
Validation loss: 2.0444066921869912

Epoch: 6| Step: 10
Training loss: 2.410100221633911
Validation loss: 2.056003232796987

Epoch: 6| Step: 11
Training loss: 2.485365867614746
Validation loss: 2.0435014367103577

Epoch: 6| Step: 12
Training loss: 2.033221483230591
Validation loss: 2.0386088689168296

Epoch: 6| Step: 13
Training loss: 2.386855125427246
Validation loss: 2.042991360028585

Epoch: 119| Step: 0
Training loss: 1.6403591632843018
Validation loss: 2.0413602391878762

Epoch: 6| Step: 1
Training loss: 1.9246445894241333
Validation loss: 2.0377237200737

Epoch: 6| Step: 2
Training loss: 1.48931884765625
Validation loss: 2.0570847392082214

Epoch: 6| Step: 3
Training loss: 1.9577257633209229
Validation loss: 2.0621438026428223

Epoch: 6| Step: 4
Training loss: 2.1543009281158447
Validation loss: 2.0623114506403604

Epoch: 6| Step: 5
Training loss: 2.446519374847412
Validation loss: 2.0667226711908975

Epoch: 6| Step: 6
Training loss: 2.319868564605713
Validation loss: 2.055397868156433

Epoch: 6| Step: 7
Training loss: 1.7626492977142334
Validation loss: 2.068954825401306

Epoch: 6| Step: 8
Training loss: 2.4777040481567383
Validation loss: 2.063993831475576

Epoch: 6| Step: 9
Training loss: 2.1377530097961426
Validation loss: 2.0694191455841064

Epoch: 6| Step: 10
Training loss: 2.688138008117676
Validation loss: 2.0542402466138205

Epoch: 6| Step: 11
Training loss: 1.9938068389892578
Validation loss: 2.0525852044423423

Epoch: 6| Step: 12
Training loss: 2.21109676361084
Validation loss: 2.043485184510549

Epoch: 6| Step: 13
Training loss: 1.847442388534546
Validation loss: 2.0471612016359964

Epoch: 120| Step: 0
Training loss: 1.7500386238098145
Validation loss: 2.0423017144203186

Epoch: 6| Step: 1
Training loss: 2.0236334800720215
Validation loss: 2.044753988583883

Epoch: 6| Step: 2
Training loss: 2.1489124298095703
Validation loss: 2.0361416935920715

Epoch: 6| Step: 3
Training loss: 2.1465327739715576
Validation loss: 2.046311895052592

Epoch: 6| Step: 4
Training loss: 2.2077465057373047
Validation loss: 2.0357184608777366

Epoch: 6| Step: 5
Training loss: 1.7804943323135376
Validation loss: 2.04869282245636

Epoch: 6| Step: 6
Training loss: 1.6302391290664673
Validation loss: 2.0364925861358643

Epoch: 6| Step: 7
Training loss: 1.9481391906738281
Validation loss: 2.0412761767705283

Epoch: 6| Step: 8
Training loss: 2.3473939895629883
Validation loss: 2.0398094058036804

Epoch: 6| Step: 9
Training loss: 2.072350025177002
Validation loss: 2.0384302139282227

Epoch: 6| Step: 10
Training loss: 2.3287618160247803
Validation loss: 2.043881595134735

Epoch: 6| Step: 11
Training loss: 2.169527292251587
Validation loss: 2.047134975592295

Epoch: 6| Step: 12
Training loss: 2.2880024909973145
Validation loss: 2.0553462902704873

Epoch: 6| Step: 13
Training loss: 2.30587100982666
Validation loss: 2.040457248687744

Epoch: 121| Step: 0
Training loss: 1.9728068113327026
Validation loss: 2.057272493839264

Epoch: 6| Step: 1
Training loss: 2.830322504043579
Validation loss: 2.0547067125638327

Epoch: 6| Step: 2
Training loss: 1.803830623626709
Validation loss: 2.056041101614634

Epoch: 6| Step: 3
Training loss: 1.7291072607040405
Validation loss: 2.053392171859741

Epoch: 6| Step: 4
Training loss: 1.8717124462127686
Validation loss: 2.055311620235443

Epoch: 6| Step: 5
Training loss: 2.1193630695343018
Validation loss: 2.053688168525696

Epoch: 6| Step: 6
Training loss: 2.1982061862945557
Validation loss: 2.0437610348065696

Epoch: 6| Step: 7
Training loss: 2.1261935234069824
Validation loss: 2.0633976658185325

Epoch: 6| Step: 8
Training loss: 2.339224338531494
Validation loss: 2.0600972771644592

Epoch: 6| Step: 9
Training loss: 1.8767998218536377
Validation loss: 2.061504900455475

Epoch: 6| Step: 10
Training loss: 2.3814125061035156
Validation loss: 2.0616931517918906

Epoch: 6| Step: 11
Training loss: 1.4829401969909668
Validation loss: 2.0768863360087075

Epoch: 6| Step: 12
Training loss: 2.317448377609253
Validation loss: 2.078089733918508

Epoch: 6| Step: 13
Training loss: 1.9266215562820435
Validation loss: 2.0670572320620217

Epoch: 122| Step: 0
Training loss: 1.896708607673645
Validation loss: 2.078045884768168

Epoch: 6| Step: 1
Training loss: 1.804236650466919
Validation loss: 2.091358482837677

Epoch: 6| Step: 2
Training loss: 2.241856575012207
Validation loss: 2.1179945270220437

Epoch: 6| Step: 3
Training loss: 2.525804042816162
Validation loss: 2.089624206225077

Epoch: 6| Step: 4
Training loss: 2.091369867324829
Validation loss: 2.09572426478068

Epoch: 6| Step: 5
Training loss: 2.4224658012390137
Validation loss: 2.0894721349080405

Epoch: 6| Step: 6
Training loss: 2.3216075897216797
Validation loss: 2.0862470666567483

Epoch: 6| Step: 7
Training loss: 1.6405280828475952
Validation loss: 2.0691537261009216

Epoch: 6| Step: 8
Training loss: 2.56752347946167
Validation loss: 2.059093177318573

Epoch: 6| Step: 9
Training loss: 1.5780999660491943
Validation loss: 2.061998963356018

Epoch: 6| Step: 10
Training loss: 2.593374729156494
Validation loss: 2.0474236408869424

Epoch: 6| Step: 11
Training loss: 1.7601432800292969
Validation loss: 2.054065227508545

Epoch: 6| Step: 12
Training loss: 2.026679515838623
Validation loss: 2.0342676440874734

Epoch: 6| Step: 13
Training loss: 1.837681531906128
Validation loss: 2.047607143719991

Epoch: 123| Step: 0
Training loss: 1.6098142862319946
Validation loss: 2.0499107440312705

Epoch: 6| Step: 1
Training loss: 1.8650777339935303
Validation loss: 2.0501503944396973

Epoch: 6| Step: 2
Training loss: 2.3113837242126465
Validation loss: 2.053412755330404

Epoch: 6| Step: 3
Training loss: 2.192133903503418
Validation loss: 2.0572412411371865

Epoch: 6| Step: 4
Training loss: 2.173623561859131
Validation loss: 2.0507270296414695

Epoch: 6| Step: 5
Training loss: 2.3262534141540527
Validation loss: 2.0527014334996543

Epoch: 6| Step: 6
Training loss: 1.9984838962554932
Validation loss: 2.0618695418039956

Epoch: 6| Step: 7
Training loss: 2.161268711090088
Validation loss: 2.046724518140157

Epoch: 6| Step: 8
Training loss: 2.2858781814575195
Validation loss: 2.048611561457316

Epoch: 6| Step: 9
Training loss: 2.3273048400878906
Validation loss: 2.0498114228248596

Epoch: 6| Step: 10
Training loss: 1.994276523590088
Validation loss: 2.057484269142151

Epoch: 6| Step: 11
Training loss: 2.071337938308716
Validation loss: 2.036611477533976

Epoch: 6| Step: 12
Training loss: 1.834128737449646
Validation loss: 2.039453625679016

Epoch: 6| Step: 13
Training loss: 1.842705488204956
Validation loss: 2.0480844577153525

Epoch: 124| Step: 0
Training loss: 1.8708635568618774
Validation loss: 2.0388623674710593

Epoch: 6| Step: 1
Training loss: 2.720533609390259
Validation loss: 2.0342041850090027

Epoch: 6| Step: 2
Training loss: 1.462411880493164
Validation loss: 2.0458263953526816

Epoch: 6| Step: 3
Training loss: 2.2784605026245117
Validation loss: 2.039636174837748

Epoch: 6| Step: 4
Training loss: 1.689767837524414
Validation loss: 2.0417529145876565

Epoch: 6| Step: 5
Training loss: 2.45159912109375
Validation loss: 2.0527556737264

Epoch: 6| Step: 6
Training loss: 2.416541576385498
Validation loss: 2.0571199854214988

Epoch: 6| Step: 7
Training loss: 2.0356855392456055
Validation loss: 2.0645631750424704

Epoch: 6| Step: 8
Training loss: 2.275554656982422
Validation loss: 2.0575218995412192

Epoch: 6| Step: 9
Training loss: 1.7788574695587158
Validation loss: 2.0648536880811057

Epoch: 6| Step: 10
Training loss: 1.8261206150054932
Validation loss: 2.0679075717926025

Epoch: 6| Step: 11
Training loss: 2.311359167098999
Validation loss: 2.0631867051124573

Epoch: 6| Step: 12
Training loss: 1.7976961135864258
Validation loss: 2.0460121830304465

Epoch: 6| Step: 13
Training loss: 2.025669813156128
Validation loss: 2.061404546101888

Epoch: 125| Step: 0
Training loss: 1.7956995964050293
Validation loss: 2.05770876010259

Epoch: 6| Step: 1
Training loss: 1.9083189964294434
Validation loss: 2.046992301940918

Epoch: 6| Step: 2
Training loss: 1.9822826385498047
Validation loss: 2.046115517616272

Epoch: 6| Step: 3
Training loss: 2.2524495124816895
Validation loss: 2.0555705030759177

Epoch: 6| Step: 4
Training loss: 2.3873870372772217
Validation loss: 2.064383109410604

Epoch: 6| Step: 5
Training loss: 2.20926570892334
Validation loss: 2.0548691948254905

Epoch: 6| Step: 6
Training loss: 1.6663784980773926
Validation loss: 2.0624965826670327

Epoch: 6| Step: 7
Training loss: 1.8235489130020142
Validation loss: 2.061889668305715

Epoch: 6| Step: 8
Training loss: 2.463380813598633
Validation loss: 2.0779316226641336

Epoch: 6| Step: 9
Training loss: 1.898737907409668
Validation loss: 2.076487342516581

Epoch: 6| Step: 10
Training loss: 1.9705052375793457
Validation loss: 2.072146236896515

Epoch: 6| Step: 11
Training loss: 1.920810341835022
Validation loss: 2.0678889751434326

Epoch: 6| Step: 12
Training loss: 2.0752012729644775
Validation loss: 2.0920838117599487

Epoch: 6| Step: 13
Training loss: 2.270728588104248
Validation loss: 2.0886286894480386

Testing loss: 1.6195033479937546

Epoch: 1| Step: 0
Training loss: 5.491797138954578
Validation loss: 5.8310949140711585

Epoch: 5| Step: 1
Training loss: 5.550180573146359
Validation loss: 5.829190563328342

Epoch: 5| Step: 2
Training loss: 5.508670215240989
Validation loss: 5.827121181135541

Epoch: 5| Step: 3
Training loss: 6.54206412822905
Validation loss: 5.825169104330363

Epoch: 5| Step: 4
Training loss: 6.588393507989127
Validation loss: 5.823200098372127

Epoch: 5| Step: 5
Training loss: 5.871950047750853
Validation loss: 5.821121992446212

Epoch: 5| Step: 6
Training loss: 5.512337458629733
Validation loss: 5.81899036521527

Epoch: 5| Step: 7
Training loss: 6.44512961388257
Validation loss: 5.816937547222223

Epoch: 5| Step: 8
Training loss: 5.937584806137772
Validation loss: 5.814841287709688

Epoch: 5| Step: 9
Training loss: 5.396430297978238
Validation loss: 5.812675849414801

Epoch: 5| Step: 10
Training loss: 6.407903988084389
Validation loss: 5.810458477640794

Epoch: 5| Step: 11
Training loss: 5.272578054965439
Validation loss: 5.808085840316857

Epoch: 2| Step: 0
Training loss: 6.138703878078366
Validation loss: 5.805776960185843

Epoch: 5| Step: 1
Training loss: 5.652616735482068
Validation loss: 5.803264541057567

Epoch: 5| Step: 2
Training loss: 5.807861117738565
Validation loss: 5.80071630987429

Epoch: 5| Step: 3
Training loss: 6.039900668508483
Validation loss: 5.798030282306751

Epoch: 5| Step: 4
Training loss: 6.563153334293899
Validation loss: 5.795280490874955

Epoch: 5| Step: 5
Training loss: 5.995251047765425
Validation loss: 5.792553678232852

Epoch: 5| Step: 6
Training loss: 6.256231024805764
Validation loss: 5.789441767669993

Epoch: 5| Step: 7
Training loss: 5.6625216788914825
Validation loss: 5.786359546366354

Epoch: 5| Step: 8
Training loss: 4.937766925018894
Validation loss: 5.7832202269114354

Epoch: 5| Step: 9
Training loss: 6.121566121291491
Validation loss: 5.779914935496923

Epoch: 5| Step: 10
Training loss: 5.561175928152014
Validation loss: 5.776366926092732

Epoch: 5| Step: 11
Training loss: 6.4081855641141585
Validation loss: 5.772893609017293

Epoch: 3| Step: 0
Training loss: 5.3406255857088984
Validation loss: 5.769015878685602

Epoch: 5| Step: 1
Training loss: 6.10118330437818
Validation loss: 5.765347909170234

Epoch: 5| Step: 2
Training loss: 5.504192834869408
Validation loss: 5.761502005600602

Epoch: 5| Step: 3
Training loss: 5.123711214609755
Validation loss: 5.757193785317868

Epoch: 5| Step: 4
Training loss: 5.893530010434046
Validation loss: 5.75300855296

Epoch: 5| Step: 5
Training loss: 6.0184592171815
Validation loss: 5.748526626287238

Epoch: 5| Step: 6
Training loss: 6.3115073452767545
Validation loss: 5.744104244528143

Epoch: 5| Step: 7
Training loss: 6.730014743647077
Validation loss: 5.739081243672563

Epoch: 5| Step: 8
Training loss: 6.1260524546201705
Validation loss: 5.734144859303912

Epoch: 5| Step: 9
Training loss: 5.988914739914311
Validation loss: 5.729225139897509

Epoch: 5| Step: 10
Training loss: 5.481671745751822
Validation loss: 5.723720780482913

Epoch: 5| Step: 11
Training loss: 4.042404238593444
Validation loss: 5.718279390131609

Epoch: 4| Step: 0
Training loss: 6.737513826317127
Validation loss: 5.712876449523901

Epoch: 5| Step: 1
Training loss: 5.385853831430099
Validation loss: 5.70739293067504

Epoch: 5| Step: 2
Training loss: 5.782481789170619
Validation loss: 5.701603851210224

Epoch: 5| Step: 3
Training loss: 6.639195618222982
Validation loss: 5.695664180856801

Epoch: 5| Step: 4
Training loss: 4.738054413173447
Validation loss: 5.689487459354361

Epoch: 5| Step: 5
Training loss: 6.020702884233272
Validation loss: 5.683285979057806

Epoch: 5| Step: 6
Training loss: 6.629061048023164
Validation loss: 5.677162928125339

Epoch: 5| Step: 7
Training loss: 5.011327120196329
Validation loss: 5.670873954715283

Epoch: 5| Step: 8
Training loss: 5.281031316258754
Validation loss: 5.663872916019441

Epoch: 5| Step: 9
Training loss: 5.934109974869665
Validation loss: 5.657579908459846

Epoch: 5| Step: 10
Training loss: 5.228266644881968
Validation loss: 5.65090633060344

Epoch: 5| Step: 11
Training loss: 5.793841532258863
Validation loss: 5.6440305353555695

Epoch: 5| Step: 0
Training loss: 5.057320099373226
Validation loss: 5.63694877974658

Epoch: 5| Step: 1
Training loss: 5.684937623016447
Validation loss: 5.629883264220132

Epoch: 5| Step: 2
Training loss: 4.708489384498551
Validation loss: 5.623011746285232

Epoch: 5| Step: 3
Training loss: 6.617010517298213
Validation loss: 5.615578227696611

Epoch: 5| Step: 4
Training loss: 5.222198626056867
Validation loss: 5.608224508165888

Epoch: 5| Step: 5
Training loss: 6.153084342826095
Validation loss: 5.6008650142865495

Epoch: 5| Step: 6
Training loss: 5.564093200890272
Validation loss: 5.593330140224692

Epoch: 5| Step: 7
Training loss: 5.73126951524247
Validation loss: 5.585602301652886

Epoch: 5| Step: 8
Training loss: 6.430812584299496
Validation loss: 5.577505046968159

Epoch: 5| Step: 9
Training loss: 5.831772877104931
Validation loss: 5.569449730212884

Epoch: 5| Step: 10
Training loss: 5.800855560340525
Validation loss: 5.561298090494676

Epoch: 5| Step: 11
Training loss: 4.530438370199056
Validation loss: 5.552835576936213

Epoch: 6| Step: 0
Training loss: 5.500699605662302
Validation loss: 5.544631991636472

Epoch: 5| Step: 1
Training loss: 6.078863025566027
Validation loss: 5.536221039484478

Epoch: 5| Step: 2
Training loss: 5.2973954836808685
Validation loss: 5.527416596308931

Epoch: 5| Step: 3
Training loss: 5.3308960988329845
Validation loss: 5.519088464584461

Epoch: 5| Step: 4
Training loss: 5.963828729330929
Validation loss: 5.5102130753730165

Epoch: 5| Step: 5
Training loss: 6.394629979288988
Validation loss: 5.501498343373422

Epoch: 5| Step: 6
Training loss: 5.891517955984145
Validation loss: 5.492785418084258

Epoch: 5| Step: 7
Training loss: 5.091623061139592
Validation loss: 5.483482172239761

Epoch: 5| Step: 8
Training loss: 5.789930270867163
Validation loss: 5.474821299128897

Epoch: 5| Step: 9
Training loss: 5.02131952277047
Validation loss: 5.466027248696165

Epoch: 5| Step: 10
Training loss: 5.057659142366774
Validation loss: 5.4569464248912976

Epoch: 5| Step: 11
Training loss: 6.55280219553204
Validation loss: 5.448226674330116

Epoch: 7| Step: 0
Training loss: 5.29845249688004
Validation loss: 5.43920557450704

Epoch: 5| Step: 1
Training loss: 5.632982650917028
Validation loss: 5.430172173619748

Epoch: 5| Step: 2
Training loss: 6.230158323967315
Validation loss: 5.421383918853791

Epoch: 5| Step: 3
Training loss: 5.103269142595995
Validation loss: 5.412553533828206

Epoch: 5| Step: 4
Training loss: 5.716123551055185
Validation loss: 5.40373175740389

Epoch: 5| Step: 5
Training loss: 5.9689033848092645
Validation loss: 5.394901064995867

Epoch: 5| Step: 6
Training loss: 5.79829261905111
Validation loss: 5.386038637922393

Epoch: 5| Step: 7
Training loss: 5.565875454407385
Validation loss: 5.377717174115426

Epoch: 5| Step: 8
Training loss: 5.151855445610901
Validation loss: 5.36900561623088

Epoch: 5| Step: 9
Training loss: 5.340530050024057
Validation loss: 5.3602722202868405

Epoch: 5| Step: 10
Training loss: 4.692227027363271
Validation loss: 5.35234367813454

Epoch: 5| Step: 11
Training loss: 5.515318951207167
Validation loss: 5.343774330723271

Epoch: 8| Step: 0
Training loss: 5.672200847755251
Validation loss: 5.3355437431807555

Epoch: 5| Step: 1
Training loss: 4.907798571295102
Validation loss: 5.327345484356482

Epoch: 5| Step: 2
Training loss: 5.67336957591965
Validation loss: 5.319708940699227

Epoch: 5| Step: 3
Training loss: 5.779369811448344
Validation loss: 5.312048163553281

Epoch: 5| Step: 4
Training loss: 5.210902361200865
Validation loss: 5.303578526307274

Epoch: 5| Step: 5
Training loss: 6.214024881235138
Validation loss: 5.296250763215499

Epoch: 5| Step: 6
Training loss: 5.06205691058641
Validation loss: 5.28853141338363

Epoch: 5| Step: 7
Training loss: 5.015828446394684
Validation loss: 5.281602843948168

Epoch: 5| Step: 8
Training loss: 5.740687583121202
Validation loss: 5.27453926539116

Epoch: 5| Step: 9
Training loss: 4.968898555046359
Validation loss: 5.267819808415096

Epoch: 5| Step: 10
Training loss: 5.160746515374231
Validation loss: 5.261013807893872

Epoch: 5| Step: 11
Training loss: 5.60505224933483
Validation loss: 5.254411153628495

Epoch: 9| Step: 0
Training loss: 5.215410271779059
Validation loss: 5.249109381866651

Epoch: 5| Step: 1
Training loss: 5.437548231590596
Validation loss: 5.242876836609322

Epoch: 5| Step: 2
Training loss: 5.623071467089764
Validation loss: 5.236997875279478

Epoch: 5| Step: 3
Training loss: 5.042590894660026
Validation loss: 5.231246301671722

Epoch: 5| Step: 4
Training loss: 6.101250829807968
Validation loss: 5.225310338613055

Epoch: 5| Step: 5
Training loss: 4.9997751185390245
Validation loss: 5.2198601948564916

Epoch: 5| Step: 6
Training loss: 5.3226250595372635
Validation loss: 5.214243551663603

Epoch: 5| Step: 7
Training loss: 4.4129006266069055
Validation loss: 5.207578324589483

Epoch: 5| Step: 8
Training loss: 5.753783598276945
Validation loss: 5.201972001238601

Epoch: 5| Step: 9
Training loss: 5.71034231359022
Validation loss: 5.196561780747486

Epoch: 5| Step: 10
Training loss: 4.870121397663359
Validation loss: 5.190755427783282

Epoch: 5| Step: 11
Training loss: 5.674620077044703
Validation loss: 5.185133873153283

Epoch: 10| Step: 0
Training loss: 5.365903209301013
Validation loss: 5.1790928794206

Epoch: 5| Step: 1
Training loss: 5.889563831701423
Validation loss: 5.17384738709726

Epoch: 5| Step: 2
Training loss: 4.633220069287582
Validation loss: 5.168263621819125

Epoch: 5| Step: 3
Training loss: 4.875682685196866
Validation loss: 5.162354017876911

Epoch: 5| Step: 4
Training loss: 5.684652097553099
Validation loss: 5.156738797533845

Epoch: 5| Step: 5
Training loss: 5.079186149645337
Validation loss: 5.151215794946585

Epoch: 5| Step: 6
Training loss: 4.824610545198627
Validation loss: 5.146074245646218

Epoch: 5| Step: 7
Training loss: 5.416460923420063
Validation loss: 5.1404701294610105

Epoch: 5| Step: 8
Training loss: 5.070632618818809
Validation loss: 5.1350963820215645

Epoch: 5| Step: 9
Training loss: 5.593298185205492
Validation loss: 5.1292326126796794

Epoch: 5| Step: 10
Training loss: 5.456921324445362
Validation loss: 5.123470752790803

Epoch: 5| Step: 11
Training loss: 5.216280232864549
Validation loss: 5.117627376840309

Epoch: 11| Step: 0
Training loss: 5.508989789957808
Validation loss: 5.111450271594905

Epoch: 5| Step: 1
Training loss: 5.310097723454965
Validation loss: 5.10528105682879

Epoch: 5| Step: 2
Training loss: 6.049555852050634
Validation loss: 5.099277149970823

Epoch: 5| Step: 3
Training loss: 4.530786951017208
Validation loss: 5.093241278085281

Epoch: 5| Step: 4
Training loss: 5.125975539239515
Validation loss: 5.087311390654027

Epoch: 5| Step: 5
Training loss: 5.302399671469606
Validation loss: 5.081311975705671

Epoch: 5| Step: 6
Training loss: 5.1316856877683845
Validation loss: 5.075778094120384

Epoch: 5| Step: 7
Training loss: 5.8812128858378285
Validation loss: 5.070297390092888

Epoch: 5| Step: 8
Training loss: 4.478624225269617
Validation loss: 5.0641370802937775

Epoch: 5| Step: 9
Training loss: 4.513606587246885
Validation loss: 5.05809839835337

Epoch: 5| Step: 10
Training loss: 5.247979274778613
Validation loss: 5.052700448115829

Epoch: 5| Step: 11
Training loss: 4.83199888201568
Validation loss: 5.047141801299194

Epoch: 12| Step: 0
Training loss: 5.050882643812602
Validation loss: 5.041178343377363

Epoch: 5| Step: 1
Training loss: 5.559770921886749
Validation loss: 5.035511463026859

Epoch: 5| Step: 2
Training loss: 6.047030819011444
Validation loss: 5.029592315983514

Epoch: 5| Step: 3
Training loss: 5.56263612730641
Validation loss: 5.024352730394312

Epoch: 5| Step: 4
Training loss: 5.775639301270371
Validation loss: 5.017928366033585

Epoch: 5| Step: 5
Training loss: 3.57778129426087
Validation loss: 5.012156855125634

Epoch: 5| Step: 6
Training loss: 5.151757705132548
Validation loss: 5.00644183829125

Epoch: 5| Step: 7
Training loss: 5.284306471260174
Validation loss: 5.000701457728282

Epoch: 5| Step: 8
Training loss: 4.796540786084618
Validation loss: 4.994996595371984

Epoch: 5| Step: 9
Training loss: 5.463096442929685
Validation loss: 4.989400659879288

Epoch: 5| Step: 10
Training loss: 3.990622017544892
Validation loss: 4.984032470668788

Epoch: 5| Step: 11
Training loss: 3.488614774438931
Validation loss: 4.9787347146004794

Epoch: 13| Step: 0
Training loss: 4.870465639636009
Validation loss: 4.973310628797405

Epoch: 5| Step: 1
Training loss: 4.731627822363593
Validation loss: 4.9680617473576625

Epoch: 5| Step: 2
Training loss: 4.188803626445486
Validation loss: 4.963144351178676

Epoch: 5| Step: 3
Training loss: 5.730573927545347
Validation loss: 4.957366314789744

Epoch: 5| Step: 4
Training loss: 5.163762209380802
Validation loss: 4.951862772051378

Epoch: 5| Step: 5
Training loss: 4.689056138183997
Validation loss: 4.94684269810804

Epoch: 5| Step: 6
Training loss: 5.290905674960396
Validation loss: 4.941814734420848

Epoch: 5| Step: 7
Training loss: 4.901721589629131
Validation loss: 4.936572281482312

Epoch: 5| Step: 8
Training loss: 5.635275208489381
Validation loss: 4.931900841422836

Epoch: 5| Step: 9
Training loss: 5.300919960360415
Validation loss: 4.926998051664085

Epoch: 5| Step: 10
Training loss: 5.13894653746199
Validation loss: 4.9211938295848

Epoch: 5| Step: 11
Training loss: 4.831145789045262
Validation loss: 4.916339712532759

Epoch: 14| Step: 0
Training loss: 5.2279457804897165
Validation loss: 4.911377060326646

Epoch: 5| Step: 1
Training loss: 4.390779865832514
Validation loss: 4.906504614311828

Epoch: 5| Step: 2
Training loss: 5.006788223423174
Validation loss: 4.901390755112757

Epoch: 5| Step: 3
Training loss: 5.139416398933916
Validation loss: 4.895820336290788

Epoch: 5| Step: 4
Training loss: 5.779828200905492
Validation loss: 4.891319128143167

Epoch: 5| Step: 5
Training loss: 5.06261641757057
Validation loss: 4.8854940716021

Epoch: 5| Step: 6
Training loss: 5.637339472546899
Validation loss: 4.880492645525965

Epoch: 5| Step: 7
Training loss: 4.954006369360663
Validation loss: 4.8753887412770025

Epoch: 5| Step: 8
Training loss: 3.8737166956171216
Validation loss: 4.870242519893724

Epoch: 5| Step: 9
Training loss: 4.912270709018056
Validation loss: 4.864863747704247

Epoch: 5| Step: 10
Training loss: 4.818823882846478
Validation loss: 4.860012441941022

Epoch: 5| Step: 11
Training loss: 5.273661019568513
Validation loss: 4.855197423226877

Epoch: 15| Step: 0
Training loss: 4.642372290478052
Validation loss: 4.850363123956249

Epoch: 5| Step: 1
Training loss: 5.072479396306322
Validation loss: 4.845467892438691

Epoch: 5| Step: 2
Training loss: 5.088268115194931
Validation loss: 4.840666447776625

Epoch: 5| Step: 3
Training loss: 5.0925800875629035
Validation loss: 4.835455661448256

Epoch: 5| Step: 4
Training loss: 4.905643109732745
Validation loss: 4.8306342870506445

Epoch: 5| Step: 5
Training loss: 5.12019178418742
Validation loss: 4.825795851513428

Epoch: 5| Step: 6
Training loss: 5.150004170240177
Validation loss: 4.821216113171128

Epoch: 5| Step: 7
Training loss: 3.8584678912060757
Validation loss: 4.81580382222823

Epoch: 5| Step: 8
Training loss: 5.425389450790517
Validation loss: 4.8113952541187475

Epoch: 5| Step: 9
Training loss: 5.043463436716876
Validation loss: 4.806051363600026

Epoch: 5| Step: 10
Training loss: 4.910298614744516
Validation loss: 4.801383005352619

Epoch: 5| Step: 11
Training loss: 4.931144003460184
Validation loss: 4.796117907161536

Epoch: 16| Step: 0
Training loss: 5.000405867316235
Validation loss: 4.791338315024245

Epoch: 5| Step: 1
Training loss: 5.003970858226054
Validation loss: 4.787256204093127

Epoch: 5| Step: 2
Training loss: 4.280399432929118
Validation loss: 4.781652130303415

Epoch: 5| Step: 3
Training loss: 4.606671694703586
Validation loss: 4.776826461937361

Epoch: 5| Step: 4
Training loss: 5.454687905618898
Validation loss: 4.772435284431067

Epoch: 5| Step: 5
Training loss: 5.207568954326241
Validation loss: 4.767380246300515

Epoch: 5| Step: 6
Training loss: 5.218789837165504
Validation loss: 4.762421670571588

Epoch: 5| Step: 7
Training loss: 4.553745807119068
Validation loss: 4.757636156243616

Epoch: 5| Step: 8
Training loss: 4.654114098908008
Validation loss: 4.752850580664235

Epoch: 5| Step: 9
Training loss: 5.353889705555023
Validation loss: 4.747716363104986

Epoch: 5| Step: 10
Training loss: 4.034299185208193
Validation loss: 4.74291713407737

Epoch: 5| Step: 11
Training loss: 6.011669889949269
Validation loss: 4.73844335351596

Epoch: 17| Step: 0
Training loss: 5.053847276995451
Validation loss: 4.7338078560964805

Epoch: 5| Step: 1
Training loss: 4.434055979828285
Validation loss: 4.729474728447238

Epoch: 5| Step: 2
Training loss: 5.057762849532811
Validation loss: 4.724600510301772

Epoch: 5| Step: 3
Training loss: 4.935192099932121
Validation loss: 4.719327853359729

Epoch: 5| Step: 4
Training loss: 4.3124630691495
Validation loss: 4.714541282528085

Epoch: 5| Step: 5
Training loss: 4.641434415877235
Validation loss: 4.710131684356963

Epoch: 5| Step: 6
Training loss: 5.23311898264305
Validation loss: 4.705685160574867

Epoch: 5| Step: 7
Training loss: 4.9510368494285535
Validation loss: 4.701109994191281

Epoch: 5| Step: 8
Training loss: 4.420113903666592
Validation loss: 4.695509269090749

Epoch: 5| Step: 9
Training loss: 5.518334088146389
Validation loss: 4.690768556136854

Epoch: 5| Step: 10
Training loss: 4.668117070734621
Validation loss: 4.686598080650525

Epoch: 5| Step: 11
Training loss: 3.9514773105113283
Validation loss: 4.681823260082062

Epoch: 18| Step: 0
Training loss: 4.92461849226024
Validation loss: 4.680000345666172

Epoch: 5| Step: 1
Training loss: 5.423691678829278
Validation loss: 4.671742640462846

Epoch: 5| Step: 2
Training loss: 4.64884718684385
Validation loss: 4.667105214198472

Epoch: 5| Step: 3
Training loss: 4.519882781947871
Validation loss: 4.663409031298357

Epoch: 5| Step: 4
Training loss: 4.24130414051833
Validation loss: 4.659435923208119

Epoch: 5| Step: 5
Training loss: 4.527309414303038
Validation loss: 4.654492244870127

Epoch: 5| Step: 6
Training loss: 4.583485040176895
Validation loss: 4.648845494421143

Epoch: 5| Step: 7
Training loss: 5.481343444932429
Validation loss: 4.644259877748292

Epoch: 5| Step: 8
Training loss: 5.442782061745195
Validation loss: 4.639013265260892

Epoch: 5| Step: 9
Training loss: 3.9725407796486722
Validation loss: 4.634577453025612

Epoch: 5| Step: 10
Training loss: 4.481125348643886
Validation loss: 4.630498092504334

Epoch: 5| Step: 11
Training loss: 5.185835973579674
Validation loss: 4.6258595071770365

Epoch: 19| Step: 0
Training loss: 4.705130013464401
Validation loss: 4.619984347150414

Epoch: 5| Step: 1
Training loss: 4.4771994358994505
Validation loss: 4.615389142890079

Epoch: 5| Step: 2
Training loss: 4.362880814772075
Validation loss: 4.611351270562368

Epoch: 5| Step: 3
Training loss: 5.243745984336216
Validation loss: 4.606991167623484

Epoch: 5| Step: 4
Training loss: 4.4469911325588045
Validation loss: 4.602468206091033

Epoch: 5| Step: 5
Training loss: 4.511534848118877
Validation loss: 4.597102708533959

Epoch: 5| Step: 6
Training loss: 5.396265766190575
Validation loss: 4.591969834973505

Epoch: 5| Step: 7
Training loss: 3.8933167242540705
Validation loss: 4.587141781090181

Epoch: 5| Step: 8
Training loss: 5.600642215143358
Validation loss: 4.583448264097081

Epoch: 5| Step: 9
Training loss: 4.592656427085633
Validation loss: 4.579012944166753

Epoch: 5| Step: 10
Training loss: 4.506622104386837
Validation loss: 4.574452269099114

Epoch: 5| Step: 11
Training loss: 4.569636368255787
Validation loss: 4.568339464813723

Epoch: 20| Step: 0
Training loss: 5.1774632437970185
Validation loss: 4.563323142641966

Epoch: 5| Step: 1
Training loss: 5.53570170598864
Validation loss: 4.558943542466702

Epoch: 5| Step: 2
Training loss: 5.0609028498519555
Validation loss: 4.553658178804969

Epoch: 5| Step: 3
Training loss: 4.2556917561071135
Validation loss: 4.548769159782461

Epoch: 5| Step: 4
Training loss: 4.555139954277119
Validation loss: 4.543492215700346

Epoch: 5| Step: 5
Training loss: 4.349897685985273
Validation loss: 4.5386510455691615

Epoch: 5| Step: 6
Training loss: 4.48745164765472
Validation loss: 4.533572581700456

Epoch: 5| Step: 7
Training loss: 5.021956490957515
Validation loss: 4.528991557330085

Epoch: 5| Step: 8
Training loss: 4.043389074618004
Validation loss: 4.523795246039592

Epoch: 5| Step: 9
Training loss: 4.4259053656471306
Validation loss: 4.519170097707582

Epoch: 5| Step: 10
Training loss: 4.587384647093661
Validation loss: 4.514654906720232

Epoch: 5| Step: 11
Training loss: 2.208404923724113
Validation loss: 4.509839858717227

Epoch: 21| Step: 0
Training loss: 5.123489901736253
Validation loss: 4.50546516024519

Epoch: 5| Step: 1
Training loss: 4.885900976363528
Validation loss: 4.500433900895078

Epoch: 5| Step: 2
Training loss: 4.387556965776022
Validation loss: 4.495748135970623

Epoch: 5| Step: 3
Training loss: 4.241469854698995
Validation loss: 4.491155312305587

Epoch: 5| Step: 4
Training loss: 4.730902377098448
Validation loss: 4.486261544725145

Epoch: 5| Step: 5
Training loss: 3.793163479207385
Validation loss: 4.481297366194763

Epoch: 5| Step: 6
Training loss: 4.864502584153425
Validation loss: 4.476891519417597

Epoch: 5| Step: 7
Training loss: 5.683988388361067
Validation loss: 4.472602117618994

Epoch: 5| Step: 8
Training loss: 4.422060473970163
Validation loss: 4.467249229265282

Epoch: 5| Step: 9
Training loss: 4.252807138913193
Validation loss: 4.4628268114197995

Epoch: 5| Step: 10
Training loss: 3.9626652487584995
Validation loss: 4.458352296854254

Epoch: 5| Step: 11
Training loss: 4.871915183595908
Validation loss: 4.453567179538254

Epoch: 22| Step: 0
Training loss: 5.0540684316491
Validation loss: 4.448534369477517

Epoch: 5| Step: 1
Training loss: 4.562115378693926
Validation loss: 4.4434679670653345

Epoch: 5| Step: 2
Training loss: 4.806193591755399
Validation loss: 4.438458236248034

Epoch: 5| Step: 3
Training loss: 4.180133318075486
Validation loss: 4.434070188491611

Epoch: 5| Step: 4
Training loss: 4.685596130290297
Validation loss: 4.428823325102587

Epoch: 5| Step: 5
Training loss: 4.665219059714353
Validation loss: 4.424473213729161

Epoch: 5| Step: 6
Training loss: 3.2486937905733235
Validation loss: 4.4189814253819035

Epoch: 5| Step: 7
Training loss: 4.312952432534207
Validation loss: 4.414206587428664

Epoch: 5| Step: 8
Training loss: 4.897413520925744
Validation loss: 4.409730938613301

Epoch: 5| Step: 9
Training loss: 4.172320052849863
Validation loss: 4.404576247462207

Epoch: 5| Step: 10
Training loss: 5.086050943495268
Validation loss: 4.399887293217413

Epoch: 5| Step: 11
Training loss: 5.104241361525626
Validation loss: 4.3950212671678885

Epoch: 23| Step: 0
Training loss: 4.611984910607777
Validation loss: 4.390097449523781

Epoch: 5| Step: 1
Training loss: 4.634871387199316
Validation loss: 4.38541822214469

Epoch: 5| Step: 2
Training loss: 4.3139217078789205
Validation loss: 4.380731865088316

Epoch: 5| Step: 3
Training loss: 4.3400085582736665
Validation loss: 4.375346115590209

Epoch: 5| Step: 4
Training loss: 4.6432037894824365
Validation loss: 4.370272880068196

Epoch: 5| Step: 5
Training loss: 4.314659131003056
Validation loss: 4.365340896205685

Epoch: 5| Step: 6
Training loss: 4.599519804064172
Validation loss: 4.36032331627685

Epoch: 5| Step: 7
Training loss: 4.361595326706194
Validation loss: 4.35565062914343

Epoch: 5| Step: 8
Training loss: 3.550383727060358
Validation loss: 4.351038665563443

Epoch: 5| Step: 9
Training loss: 4.906215400330825
Validation loss: 4.345894474002989

Epoch: 5| Step: 10
Training loss: 4.919638861451073
Validation loss: 4.340879200613517

Epoch: 5| Step: 11
Training loss: 4.955530009942767
Validation loss: 4.335937976550743

Epoch: 24| Step: 0
Training loss: 4.236017000302934
Validation loss: 4.330895612419472

Epoch: 5| Step: 1
Training loss: 3.53863522811393
Validation loss: 4.326418653902492

Epoch: 5| Step: 2
Training loss: 4.146247702486669
Validation loss: 4.321548216209498

Epoch: 5| Step: 3
Training loss: 4.844448900560979
Validation loss: 4.316323897782998

Epoch: 5| Step: 4
Training loss: 4.920225611840491
Validation loss: 4.311572892051921

Epoch: 5| Step: 5
Training loss: 4.860479002529813
Validation loss: 4.305659028146451

Epoch: 5| Step: 6
Training loss: 4.491207009164677
Validation loss: 4.300773336488128

Epoch: 5| Step: 7
Training loss: 3.4305538395419615
Validation loss: 4.296111738443969

Epoch: 5| Step: 8
Training loss: 4.422751835132288
Validation loss: 4.291147402796146

Epoch: 5| Step: 9
Training loss: 4.726017490207693
Validation loss: 4.285622885747422

Epoch: 5| Step: 10
Training loss: 4.480425540945202
Validation loss: 4.279970474423563

Epoch: 5| Step: 11
Training loss: 6.125469579018131
Validation loss: 4.275207692207861

Epoch: 25| Step: 0
Training loss: 5.1738841599928955
Validation loss: 4.269798305357721

Epoch: 5| Step: 1
Training loss: 4.308865107190394
Validation loss: 4.264274821313542

Epoch: 5| Step: 2
Training loss: 4.558871982324213
Validation loss: 4.2587542411082175

Epoch: 5| Step: 3
Training loss: 4.3217183976338935
Validation loss: 4.253776826223716

Epoch: 5| Step: 4
Training loss: 4.812953977103643
Validation loss: 4.248383972122578

Epoch: 5| Step: 5
Training loss: 3.8061372787245955
Validation loss: 4.242718629975175

Epoch: 5| Step: 6
Training loss: 4.5828081465868555
Validation loss: 4.237603597771842

Epoch: 5| Step: 7
Training loss: 3.6170426209933604
Validation loss: 4.232164404149128

Epoch: 5| Step: 8
Training loss: 4.2656744091260475
Validation loss: 4.227193619574266

Epoch: 5| Step: 9
Training loss: 4.583843318425172
Validation loss: 4.221961106241827

Epoch: 5| Step: 10
Training loss: 4.097737939709441
Validation loss: 4.21669923835106

Epoch: 5| Step: 11
Training loss: 3.0600146115022753
Validation loss: 4.21161190900592

Epoch: 26| Step: 0
Training loss: 4.1202670601518845
Validation loss: 4.206777949309658

Epoch: 5| Step: 1
Training loss: 4.375773770162996
Validation loss: 4.20164975791766

Epoch: 5| Step: 2
Training loss: 4.229948876615331
Validation loss: 4.19659513618334

Epoch: 5| Step: 3
Training loss: 4.028772820228477
Validation loss: 4.191989764444641

Epoch: 5| Step: 4
Training loss: 4.203794291362483
Validation loss: 4.1866575314083985

Epoch: 5| Step: 5
Training loss: 3.5707455962955907
Validation loss: 4.18186630549905

Epoch: 5| Step: 6
Training loss: 4.484758207848148
Validation loss: 4.177420122960803

Epoch: 5| Step: 7
Training loss: 4.588294286334281
Validation loss: 4.1722486140560315

Epoch: 5| Step: 8
Training loss: 4.845043378858189
Validation loss: 4.167581031925352

Epoch: 5| Step: 9
Training loss: 4.550290637113431
Validation loss: 4.162177130823265

Epoch: 5| Step: 10
Training loss: 4.301271277288853
Validation loss: 4.157666579224169

Epoch: 5| Step: 11
Training loss: 4.349770743807269
Validation loss: 4.151796675128991

Epoch: 27| Step: 0
Training loss: 3.869910312372325
Validation loss: 4.1470034000897

Epoch: 5| Step: 1
Training loss: 4.621671148812264
Validation loss: 4.142021330027301

Epoch: 5| Step: 2
Training loss: 4.434171906135807
Validation loss: 4.136851446985001

Epoch: 5| Step: 3
Training loss: 4.685386079332454
Validation loss: 4.130912377608891

Epoch: 5| Step: 4
Training loss: 4.7705488939185585
Validation loss: 4.126357654676021

Epoch: 5| Step: 5
Training loss: 4.650892209011487
Validation loss: 4.121216262032898

Epoch: 5| Step: 6
Training loss: 3.929795978482231
Validation loss: 4.115534016678361

Epoch: 5| Step: 7
Training loss: 4.103107502908
Validation loss: 4.110573283268388

Epoch: 5| Step: 8
Training loss: 4.530418793306975
Validation loss: 4.105002379492234

Epoch: 5| Step: 9
Training loss: 3.854742196216498
Validation loss: 4.099464795605958

Epoch: 5| Step: 10
Training loss: 3.2847440009619424
Validation loss: 4.094720565242001

Epoch: 5| Step: 11
Training loss: 3.127429628014999
Validation loss: 4.089335237823091

Epoch: 28| Step: 0
Training loss: 3.9232106157252575
Validation loss: 4.084652505955827

Epoch: 5| Step: 1
Training loss: 4.504301558364684
Validation loss: 4.079804655545706

Epoch: 5| Step: 2
Training loss: 4.409618848388319
Validation loss: 4.074729509484009

Epoch: 5| Step: 3
Training loss: 4.298240416933736
Validation loss: 4.069852484483053

Epoch: 5| Step: 4
Training loss: 3.7032050916806925
Validation loss: 4.065199791196761

Epoch: 5| Step: 5
Training loss: 4.5049714072079015
Validation loss: 4.060163496034356

Epoch: 5| Step: 6
Training loss: 4.066206898781857
Validation loss: 4.055572197646277

Epoch: 5| Step: 7
Training loss: 4.016786874790017
Validation loss: 4.050216669793827

Epoch: 5| Step: 8
Training loss: 4.530260491028871
Validation loss: 4.045323750861023

Epoch: 5| Step: 9
Training loss: 4.169931709224352
Validation loss: 4.040496369979783

Epoch: 5| Step: 10
Training loss: 3.87831281749729
Validation loss: 4.034575615978939

Epoch: 5| Step: 11
Training loss: 4.3079132898471615
Validation loss: 4.029918174654443

Epoch: 29| Step: 0
Training loss: 4.431000583876784
Validation loss: 4.02497440847313

Epoch: 5| Step: 1
Training loss: 3.6185209317088844
Validation loss: 4.019571356639706

Epoch: 5| Step: 2
Training loss: 4.377945262805497
Validation loss: 4.014964770554044

Epoch: 5| Step: 3
Training loss: 4.044532834532237
Validation loss: 4.009974337207936

Epoch: 5| Step: 4
Training loss: 4.3155352097658195
Validation loss: 4.005371852690065

Epoch: 5| Step: 5
Training loss: 3.8935402362097
Validation loss: 3.9996202010407997

Epoch: 5| Step: 6
Training loss: 4.444416941451701
Validation loss: 3.994824155288278

Epoch: 5| Step: 7
Training loss: 3.3926492440912037
Validation loss: 3.989574301222229

Epoch: 5| Step: 8
Training loss: 3.8117482429140566
Validation loss: 3.9850785176619987

Epoch: 5| Step: 9
Training loss: 4.354601143267126
Validation loss: 3.979318803937946

Epoch: 5| Step: 10
Training loss: 4.493796629259228
Validation loss: 3.9749225115070934

Epoch: 5| Step: 11
Training loss: 4.631710648375332
Validation loss: 3.969494617125201

Epoch: 30| Step: 0
Training loss: 4.831771115799931
Validation loss: 3.9653871762041324

Epoch: 5| Step: 1
Training loss: 4.966894223080113
Validation loss: 3.9591540573284387

Epoch: 5| Step: 2
Training loss: 3.1931939225104715
Validation loss: 3.9533354862752996

Epoch: 5| Step: 3
Training loss: 4.307468519843003
Validation loss: 3.948174690464089

Epoch: 5| Step: 4
Training loss: 4.2764313025066345
Validation loss: 3.94276027905584

Epoch: 5| Step: 5
Training loss: 4.2694611120439845
Validation loss: 3.937909594429814

Epoch: 5| Step: 6
Training loss: 3.7528343933061503
Validation loss: 3.931542279244759

Epoch: 5| Step: 7
Training loss: 3.409957268187069
Validation loss: 3.9265802788069455

Epoch: 5| Step: 8
Training loss: 3.6446686255761014
Validation loss: 3.9220213438494334

Epoch: 5| Step: 9
Training loss: 4.251177232047185
Validation loss: 3.9163932890886515

Epoch: 5| Step: 10
Training loss: 3.4614656962384234
Validation loss: 3.911388973970583

Epoch: 5| Step: 11
Training loss: 4.229978186007767
Validation loss: 3.9063912022819096

Epoch: 31| Step: 0
Training loss: 4.725030420598998
Validation loss: 3.901710028577841

Epoch: 5| Step: 1
Training loss: 4.367897371798189
Validation loss: 3.89676649813909

Epoch: 5| Step: 2
Training loss: 4.741658566182507
Validation loss: 3.8915489202789493

Epoch: 5| Step: 3
Training loss: 4.653832339189303
Validation loss: 3.886182784501989

Epoch: 5| Step: 4
Training loss: 4.44859581556075
Validation loss: 3.88088476783548

Epoch: 5| Step: 5
Training loss: 3.473275059768245
Validation loss: 3.8756869204596107

Epoch: 5| Step: 6
Training loss: 3.00753648160274
Validation loss: 3.870718308099261

Epoch: 5| Step: 7
Training loss: 2.949048978453789
Validation loss: 3.8654005192610623

Epoch: 5| Step: 8
Training loss: 3.9423656373480225
Validation loss: 3.860814432292353

Epoch: 5| Step: 9
Training loss: 3.934471903630959
Validation loss: 3.855769260680175

Epoch: 5| Step: 10
Training loss: 3.672689144698863
Validation loss: 3.850818894972924

Epoch: 5| Step: 11
Training loss: 1.9090016001499317
Validation loss: 3.8465773669003154

Epoch: 32| Step: 0
Training loss: 4.0154703904605045
Validation loss: 3.842381675749908

Epoch: 5| Step: 1
Training loss: 4.206367008117863
Validation loss: 3.8376211911427323

Epoch: 5| Step: 2
Training loss: 4.48005963081456
Validation loss: 3.833027514753305

Epoch: 5| Step: 3
Training loss: 3.858742974920276
Validation loss: 3.828822667910901

Epoch: 5| Step: 4
Training loss: 3.369956451270472
Validation loss: 3.8238800387194107

Epoch: 5| Step: 5
Training loss: 4.1947662168339725
Validation loss: 3.8192574852162604

Epoch: 5| Step: 6
Training loss: 3.9494976545153766
Validation loss: 3.815203864798825

Epoch: 5| Step: 7
Training loss: 3.9410506693930167
Validation loss: 3.810363645203814

Epoch: 5| Step: 8
Training loss: 3.9092576311942886
Validation loss: 3.805532744069348

Epoch: 5| Step: 9
Training loss: 3.8387657041486207
Validation loss: 3.8013487258596372

Epoch: 5| Step: 10
Training loss: 3.588646225128074
Validation loss: 3.7959474914223326

Epoch: 5| Step: 11
Training loss: 4.168311036202943
Validation loss: 3.7916350607899663

Epoch: 33| Step: 0
Training loss: 4.388482599459275
Validation loss: 3.78687166933991

Epoch: 5| Step: 1
Training loss: 4.575221263767115
Validation loss: 3.7823636645488836

Epoch: 5| Step: 2
Training loss: 3.9268555646795176
Validation loss: 3.7770124814176818

Epoch: 5| Step: 3
Training loss: 4.7858822392515465
Validation loss: 3.7719433250233307

Epoch: 5| Step: 4
Training loss: 3.832999104289471
Validation loss: 3.7668352358719166

Epoch: 5| Step: 5
Training loss: 3.703873570750031
Validation loss: 3.761693175992458

Epoch: 5| Step: 6
Training loss: 3.4128139141866805
Validation loss: 3.756746871972664

Epoch: 5| Step: 7
Training loss: 3.6588383829267355
Validation loss: 3.75193228741482

Epoch: 5| Step: 8
Training loss: 3.2650516864605357
Validation loss: 3.747222618922753

Epoch: 5| Step: 9
Training loss: 3.263219357981946
Validation loss: 3.7427025946850856

Epoch: 5| Step: 10
Training loss: 3.8588672875416288
Validation loss: 3.7384351474782513

Epoch: 5| Step: 11
Training loss: 3.35977766595658
Validation loss: 3.7336070097013128

Epoch: 34| Step: 0
Training loss: 4.001763431936669
Validation loss: 3.7288881531750535

Epoch: 5| Step: 1
Training loss: 3.5680342402418974
Validation loss: 3.724761597047055

Epoch: 5| Step: 2
Training loss: 3.3886068640197604
Validation loss: 3.7201240255760606

Epoch: 5| Step: 3
Training loss: 3.0702693247004063
Validation loss: 3.71601836804154

Epoch: 5| Step: 4
Training loss: 4.145539214214107
Validation loss: 3.7114593031920604

Epoch: 5| Step: 5
Training loss: 3.7599280859786344
Validation loss: 3.7073441572011316

Epoch: 5| Step: 6
Training loss: 4.023013905568736
Validation loss: 3.702678267175275

Epoch: 5| Step: 7
Training loss: 4.068963411186828
Validation loss: 3.698132305394854

Epoch: 5| Step: 8
Training loss: 3.855717458693888
Validation loss: 3.693718607468448

Epoch: 5| Step: 9
Training loss: 4.010345431929578
Validation loss: 3.6894847194704843

Epoch: 5| Step: 10
Training loss: 4.056713031972966
Validation loss: 3.6848219667541455

Epoch: 5| Step: 11
Training loss: 4.607138915709777
Validation loss: 3.6800790620254724

Epoch: 35| Step: 0
Training loss: 3.934564616487252
Validation loss: 3.6753918681043114

Epoch: 5| Step: 1
Training loss: 3.502012355492896
Validation loss: 3.670551402378621

Epoch: 5| Step: 2
Training loss: 4.145592124989199
Validation loss: 3.6659285423735835

Epoch: 5| Step: 3
Training loss: 3.6851217068122124
Validation loss: 3.6607345238235838

Epoch: 5| Step: 4
Training loss: 3.3484691937006246
Validation loss: 3.6561404698151216

Epoch: 5| Step: 5
Training loss: 3.414044203370952
Validation loss: 3.651356522519974

Epoch: 5| Step: 6
Training loss: 3.9653338049593527
Validation loss: 3.647247670452791

Epoch: 5| Step: 7
Training loss: 3.9708701650719944
Validation loss: 3.64254120917956

Epoch: 5| Step: 8
Training loss: 3.4647819573742296
Validation loss: 3.6380664910864926

Epoch: 5| Step: 9
Training loss: 3.549133119719414
Validation loss: 3.633695007107551

Epoch: 5| Step: 10
Training loss: 4.509103256359702
Validation loss: 3.6290646981426713

Epoch: 5| Step: 11
Training loss: 3.840466841691748
Validation loss: 3.62465155231252

Epoch: 36| Step: 0
Training loss: 3.4590151164553617
Validation loss: 3.6200120963988693

Epoch: 5| Step: 1
Training loss: 4.269415767465544
Validation loss: 3.6152065502039172

Epoch: 5| Step: 2
Training loss: 3.5680319683370545
Validation loss: 3.6108452195717957

Epoch: 5| Step: 3
Training loss: 3.0008195711254797
Validation loss: 3.606054151738474

Epoch: 5| Step: 4
Training loss: 3.2874221154331646
Validation loss: 3.6017334760241533

Epoch: 5| Step: 5
Training loss: 3.767895532411171
Validation loss: 3.597441140480466

Epoch: 5| Step: 6
Training loss: 3.4859131296247243
Validation loss: 3.5931719218280436

Epoch: 5| Step: 7
Training loss: 3.8425452352428717
Validation loss: 3.5888187880939593

Epoch: 5| Step: 8
Training loss: 3.803113670614352
Validation loss: 3.5849234560762664

Epoch: 5| Step: 9
Training loss: 3.7158594517895995
Validation loss: 3.5803757464228267

Epoch: 5| Step: 10
Training loss: 4.549493933400596
Validation loss: 3.5759689242910118

Epoch: 5| Step: 11
Training loss: 4.049420944073338
Validation loss: 3.5720173322076274

Epoch: 37| Step: 0
Training loss: 3.646371914274109
Validation loss: 3.5670035350592397

Epoch: 5| Step: 1
Training loss: 3.295912471034938
Validation loss: 3.5619132300451204

Epoch: 5| Step: 2
Training loss: 3.334812519574444
Validation loss: 3.557355680024715

Epoch: 5| Step: 3
Training loss: 3.314358747551433
Validation loss: 3.552996770624493

Epoch: 5| Step: 4
Training loss: 3.483465375329309
Validation loss: 3.5489645473170914

Epoch: 5| Step: 5
Training loss: 3.884551796606973
Validation loss: 3.544935853088115

Epoch: 5| Step: 6
Training loss: 3.6781244866201455
Validation loss: 3.5403115354012433

Epoch: 5| Step: 7
Training loss: 4.128126866631154
Validation loss: 3.5357341146532364

Epoch: 5| Step: 8
Training loss: 3.9098600262029217
Validation loss: 3.530939454233633

Epoch: 5| Step: 9
Training loss: 3.940665410777691
Validation loss: 3.526896986720732

Epoch: 5| Step: 10
Training loss: 3.5816045775312046
Validation loss: 3.5223768488787757

Epoch: 5| Step: 11
Training loss: 4.531147922813058
Validation loss: 3.5180900917076556

Epoch: 38| Step: 0
Training loss: 4.003846941252811
Validation loss: 3.513639626983191

Epoch: 5| Step: 1
Training loss: 3.52329458551625
Validation loss: 3.5088493893359476

Epoch: 5| Step: 2
Training loss: 2.7014959641531835
Validation loss: 3.5041271580945845

Epoch: 5| Step: 3
Training loss: 3.545919819071104
Validation loss: 3.499853244611195

Epoch: 5| Step: 4
Training loss: 3.7316539858803783
Validation loss: 3.49546378087824

Epoch: 5| Step: 5
Training loss: 3.6663160734468923
Validation loss: 3.491104305034945

Epoch: 5| Step: 6
Training loss: 3.4062609191159563
Validation loss: 3.4865023244440376

Epoch: 5| Step: 7
Training loss: 3.5235637633713197
Validation loss: 3.48256336323387

Epoch: 5| Step: 8
Training loss: 3.694739525148073
Validation loss: 3.4786736354106216

Epoch: 5| Step: 9
Training loss: 3.975338610735861
Validation loss: 3.47409297943356

Epoch: 5| Step: 10
Training loss: 3.780292105918125
Validation loss: 3.4694634525781844

Epoch: 5| Step: 11
Training loss: 4.51357679543411
Validation loss: 3.4655592565604048

Epoch: 39| Step: 0
Training loss: 3.8856550591827674
Validation loss: 3.460799668712706

Epoch: 5| Step: 1
Training loss: 2.9460606927466175
Validation loss: 3.456490402874581

Epoch: 5| Step: 2
Training loss: 4.177588276138715
Validation loss: 3.4521026795343395

Epoch: 5| Step: 3
Training loss: 3.683320732152681
Validation loss: 3.4478985227969883

Epoch: 5| Step: 4
Training loss: 3.7646505907500325
Validation loss: 3.4435730664262554

Epoch: 5| Step: 5
Training loss: 3.570338142686908
Validation loss: 3.438965213502453

Epoch: 5| Step: 6
Training loss: 3.3756237160126323
Validation loss: 3.4348425013884896

Epoch: 5| Step: 7
Training loss: 3.5253758818510974
Validation loss: 3.4304164207461807

Epoch: 5| Step: 8
Training loss: 3.3623587199314446
Validation loss: 3.4260015471210536

Epoch: 5| Step: 9
Training loss: 3.9529561244190443
Validation loss: 3.421850729846091

Epoch: 5| Step: 10
Training loss: 2.7661562479424413
Validation loss: 3.4176268023240164

Epoch: 5| Step: 11
Training loss: 4.04152700176325
Validation loss: 3.413485218536303

Epoch: 40| Step: 0
Training loss: 3.334598889114212
Validation loss: 3.409271186555781

Epoch: 5| Step: 1
Training loss: 3.440981956764991
Validation loss: 3.4049580016512584

Epoch: 5| Step: 2
Training loss: 3.3392840357019273
Validation loss: 3.400445951172014

Epoch: 5| Step: 3
Training loss: 3.33702909312085
Validation loss: 3.396600675575129

Epoch: 5| Step: 4
Training loss: 4.0441482371432995
Validation loss: 3.3925458152168013

Epoch: 5| Step: 5
Training loss: 3.51139651847363
Validation loss: 3.3887635613459395

Epoch: 5| Step: 6
Training loss: 3.326512192201941
Validation loss: 3.3844869990205013

Epoch: 5| Step: 7
Training loss: 3.7605736753420724
Validation loss: 3.380362800552339

Epoch: 5| Step: 8
Training loss: 4.201882785039224
Validation loss: 3.3764706339203787

Epoch: 5| Step: 9
Training loss: 2.784738303307461
Validation loss: 3.372519005056663

Epoch: 5| Step: 10
Training loss: 3.147830876297048
Validation loss: 3.368278084883359

Epoch: 5| Step: 11
Training loss: 4.960523212585799
Validation loss: 3.3645365311208804

Epoch: 41| Step: 0
Training loss: 3.786059512928582
Validation loss: 3.360213383586017

Epoch: 5| Step: 1
Training loss: 3.5649376276025206
Validation loss: 3.3557293531287637

Epoch: 5| Step: 2
Training loss: 2.8648858667777826
Validation loss: 3.351771741617723

Epoch: 5| Step: 3
Training loss: 3.3676474272607484
Validation loss: 3.3477683655026618

Epoch: 5| Step: 4
Training loss: 3.8197645172716115
Validation loss: 3.3434343783242237

Epoch: 5| Step: 5
Training loss: 3.1492924819071155
Validation loss: 3.3395726798481173

Epoch: 5| Step: 6
Training loss: 2.9275526075579807
Validation loss: 3.335515813805372

Epoch: 5| Step: 7
Training loss: 3.6188805327649054
Validation loss: 3.331514742591586

Epoch: 5| Step: 8
Training loss: 3.793647556084402
Validation loss: 3.3275301406148734

Epoch: 5| Step: 9
Training loss: 3.6546558262959863
Validation loss: 3.3237400330327964

Epoch: 5| Step: 10
Training loss: 3.744070069350212
Validation loss: 3.3197751676310845

Epoch: 5| Step: 11
Training loss: 2.301045064260226
Validation loss: 3.3154827819677815

Epoch: 42| Step: 0
Training loss: 3.2215692709935726
Validation loss: 3.3121327010635535

Epoch: 5| Step: 1
Training loss: 3.9060622513474317
Validation loss: 3.3081628307891915

Epoch: 5| Step: 2
Training loss: 2.963377900562628
Validation loss: 3.3041710957372805

Epoch: 5| Step: 3
Training loss: 3.7530082080973908
Validation loss: 3.300363692558529

Epoch: 5| Step: 4
Training loss: 3.8535157487408114
Validation loss: 3.2963965371592687

Epoch: 5| Step: 5
Training loss: 3.640451451696034
Validation loss: 3.292524121465742

Epoch: 5| Step: 6
Training loss: 3.485405739544607
Validation loss: 3.2885587764114046

Epoch: 5| Step: 7
Training loss: 3.008863072412135
Validation loss: 3.2848637919780863

Epoch: 5| Step: 8
Training loss: 2.9262797563776175
Validation loss: 3.280943562564129

Epoch: 5| Step: 9
Training loss: 3.8007033399886754
Validation loss: 3.277349688009673

Epoch: 5| Step: 10
Training loss: 3.123640451330699
Validation loss: 3.273715812365738

Epoch: 5| Step: 11
Training loss: 2.7874526391902625
Validation loss: 3.2699035955114177

Epoch: 43| Step: 0
Training loss: 3.8265196879224477
Validation loss: 3.266721571773914

Epoch: 5| Step: 1
Training loss: 3.8359669608482343
Validation loss: 3.2638012601483646

Epoch: 5| Step: 2
Training loss: 3.4527627746332454
Validation loss: 3.260031833829672

Epoch: 5| Step: 3
Training loss: 3.201645225547102
Validation loss: 3.256332994600512

Epoch: 5| Step: 4
Training loss: 2.510363175915767
Validation loss: 3.252895440935149

Epoch: 5| Step: 5
Training loss: 2.753829110871098
Validation loss: 3.2495349832567055

Epoch: 5| Step: 6
Training loss: 3.6534851575429617
Validation loss: 3.2464410111816813

Epoch: 5| Step: 7
Training loss: 3.669025240553724
Validation loss: 3.243080151092761

Epoch: 5| Step: 8
Training loss: 3.8670326278963363
Validation loss: 3.239512054337914

Epoch: 5| Step: 9
Training loss: 2.898787238312417
Validation loss: 3.2361126530433952

Epoch: 5| Step: 10
Training loss: 3.3102174488106737
Validation loss: 3.2326892630784543

Epoch: 5| Step: 11
Training loss: 3.407694519043576
Validation loss: 3.229504061064662

Epoch: 44| Step: 0
Training loss: 3.1537362843569423
Validation loss: 3.2259064931924324

Epoch: 5| Step: 1
Training loss: 3.0942132776353857
Validation loss: 3.222759767661511

Epoch: 5| Step: 2
Training loss: 3.6344305557373824
Validation loss: 3.2194241709661

Epoch: 5| Step: 3
Training loss: 3.589535173804131
Validation loss: 3.2159070759025674

Epoch: 5| Step: 4
Training loss: 3.241713155954092
Validation loss: 3.212620571733608

Epoch: 5| Step: 5
Training loss: 2.7751642573775746
Validation loss: 3.208961228240557

Epoch: 5| Step: 6
Training loss: 3.7942260795402762
Validation loss: 3.205749295768273

Epoch: 5| Step: 7
Training loss: 3.7330420760801326
Validation loss: 3.202243733323521

Epoch: 5| Step: 8
Training loss: 2.912805199083457
Validation loss: 3.198801226884604

Epoch: 5| Step: 9
Training loss: 3.412489749208362
Validation loss: 3.195648785020208

Epoch: 5| Step: 10
Training loss: 3.402451383199922
Validation loss: 3.191861117509379

Epoch: 5| Step: 11
Training loss: 3.0815160052323156
Validation loss: 3.1890297005371715

Epoch: 45| Step: 0
Training loss: 3.7188324478370154
Validation loss: 3.185487429720821

Epoch: 5| Step: 1
Training loss: 3.5934081868695262
Validation loss: 3.1819065014906975

Epoch: 5| Step: 2
Training loss: 2.193963664808513
Validation loss: 3.1785561707024814

Epoch: 5| Step: 3
Training loss: 3.869584760444164
Validation loss: 3.1750851251799976

Epoch: 5| Step: 4
Training loss: 3.4706715253379987
Validation loss: 3.1713822362572968

Epoch: 5| Step: 5
Training loss: 3.3036233257208774
Validation loss: 3.168209692356741

Epoch: 5| Step: 6
Training loss: 3.4552832025591904
Validation loss: 3.1646791053545873

Epoch: 5| Step: 7
Training loss: 2.8114561051007363
Validation loss: 3.1614637840932063

Epoch: 5| Step: 8
Training loss: 3.408613452421642
Validation loss: 3.157936548901186

Epoch: 5| Step: 9
Training loss: 3.315828342656089
Validation loss: 3.154557400584536

Epoch: 5| Step: 10
Training loss: 3.010943163246328
Validation loss: 3.15126141919462

Epoch: 5| Step: 11
Training loss: 2.950469260958887
Validation loss: 3.1479612704958004

Epoch: 46| Step: 0
Training loss: 3.2623609082033727
Validation loss: 3.144579572286581

Epoch: 5| Step: 1
Training loss: 3.880030166666012
Validation loss: 3.1413409522536417

Epoch: 5| Step: 2
Training loss: 3.051882966183701
Validation loss: 3.1379516541174985

Epoch: 5| Step: 3
Training loss: 3.2044875805644337
Validation loss: 3.134117254307043

Epoch: 5| Step: 4
Training loss: 3.0218994013403706
Validation loss: 3.131186376396083

Epoch: 5| Step: 5
Training loss: 3.686246238543589
Validation loss: 3.128144484620285

Epoch: 5| Step: 6
Training loss: 3.3305199830572922
Validation loss: 3.124833452237106

Epoch: 5| Step: 7
Training loss: 2.929947579601701
Validation loss: 3.121431353933485

Epoch: 5| Step: 8
Training loss: 3.6761104748452
Validation loss: 3.118320466342425

Epoch: 5| Step: 9
Training loss: 2.602489501023163
Validation loss: 3.115194663628772

Epoch: 5| Step: 10
Training loss: 2.985321693657825
Validation loss: 3.1120545839670686

Epoch: 5| Step: 11
Training loss: 3.8279602015308876
Validation loss: 3.109041336661858

Epoch: 47| Step: 0
Training loss: 2.765682931066993
Validation loss: 3.1059193026394065

Epoch: 5| Step: 1
Training loss: 3.4138971284319077
Validation loss: 3.1027272757392526

Epoch: 5| Step: 2
Training loss: 3.67441292569449
Validation loss: 3.0995145838119984

Epoch: 5| Step: 3
Training loss: 2.92878648254101
Validation loss: 3.096225823559686

Epoch: 5| Step: 4
Training loss: 3.306822031073101
Validation loss: 3.093211152773151

Epoch: 5| Step: 5
Training loss: 2.9365502303296336
Validation loss: 3.0900379639107145

Epoch: 5| Step: 6
Training loss: 2.771731429483654
Validation loss: 3.0866675853230103

Epoch: 5| Step: 7
Training loss: 3.1079558387869044
Validation loss: 3.0840613476563647

Epoch: 5| Step: 8
Training loss: 3.9076134095235897
Validation loss: 3.0810584308583393

Epoch: 5| Step: 9
Training loss: 3.3168851508888593
Validation loss: 3.077867794102432

Epoch: 5| Step: 10
Training loss: 3.28288489892389
Validation loss: 3.0749000689439896

Epoch: 5| Step: 11
Training loss: 2.927415785909776
Validation loss: 3.071841528145198

Epoch: 48| Step: 0
Training loss: 3.1542333826814204
Validation loss: 3.0689737800328705

Epoch: 5| Step: 1
Training loss: 3.0974144089567655
Validation loss: 3.0667539427784187

Epoch: 5| Step: 2
Training loss: 3.254735870661175
Validation loss: 3.0635759383931407

Epoch: 5| Step: 3
Training loss: 3.14980496605071
Validation loss: 3.061500353728681

Epoch: 5| Step: 4
Training loss: 3.1651042965643144
Validation loss: 3.0577274750571313

Epoch: 5| Step: 5
Training loss: 3.3283482149874226
Validation loss: 3.054920216937379

Epoch: 5| Step: 6
Training loss: 3.360994170565488
Validation loss: 3.052316153350864

Epoch: 5| Step: 7
Training loss: 3.2211441469101225
Validation loss: 3.049421716545792

Epoch: 5| Step: 8
Training loss: 3.3853052834992288
Validation loss: 3.0466794986440715

Epoch: 5| Step: 9
Training loss: 2.958887522909578
Validation loss: 3.043718564747829

Epoch: 5| Step: 10
Training loss: 3.095272421733844
Validation loss: 3.040954014279179

Epoch: 5| Step: 11
Training loss: 3.043087527267068
Validation loss: 3.037954754532873

Epoch: 49| Step: 0
Training loss: 3.4532095091822392
Validation loss: 3.0353272077738938

Epoch: 5| Step: 1
Training loss: 3.9329055138592923
Validation loss: 3.032318739582912

Epoch: 5| Step: 2
Training loss: 2.8358080285313068
Validation loss: 3.029187874192864

Epoch: 5| Step: 3
Training loss: 3.5016577745555004
Validation loss: 3.0264630388162166

Epoch: 5| Step: 4
Training loss: 2.310028611373411
Validation loss: 3.0234885527697566

Epoch: 5| Step: 5
Training loss: 3.116196859788791
Validation loss: 3.0205298315150704

Epoch: 5| Step: 6
Training loss: 3.134901002236473
Validation loss: 3.017872215186601

Epoch: 5| Step: 7
Training loss: 3.2321822985235027
Validation loss: 3.015046474326344

Epoch: 5| Step: 8
Training loss: 2.8555318718442217
Validation loss: 3.012307722569199

Epoch: 5| Step: 9
Training loss: 3.320032731779597
Validation loss: 3.0097060481303224

Epoch: 5| Step: 10
Training loss: 2.7512424436653493
Validation loss: 3.006805205015162

Epoch: 5| Step: 11
Training loss: 3.4452192159813273
Validation loss: 3.0041847447148116

Epoch: 50| Step: 0
Training loss: 3.033518620655359
Validation loss: 3.0014441213145324

Epoch: 5| Step: 1
Training loss: 2.842269679848026
Validation loss: 2.999156207391199

Epoch: 5| Step: 2
Training loss: 3.1599225121667063
Validation loss: 2.996339054335893

Epoch: 5| Step: 3
Training loss: 3.159126104404551
Validation loss: 2.9933375048519824

Epoch: 5| Step: 4
Training loss: 3.3612655642692797
Validation loss: 2.9912667580100725

Epoch: 5| Step: 5
Training loss: 3.344408629962475
Validation loss: 2.988406047001991

Epoch: 5| Step: 6
Training loss: 2.7333596224412258
Validation loss: 2.985897687682837

Epoch: 5| Step: 7
Training loss: 2.7442719886262354
Validation loss: 2.9833979846477425

Epoch: 5| Step: 8
Training loss: 2.987944380777465
Validation loss: 2.982414207290422

Epoch: 5| Step: 9
Training loss: 3.2156119743950833
Validation loss: 2.981033891566456

Epoch: 5| Step: 10
Training loss: 3.6431905850305144
Validation loss: 2.9802206993624254

Epoch: 5| Step: 11
Training loss: 3.6384294299093756
Validation loss: 2.9754973413915673

Epoch: 51| Step: 0
Training loss: 3.0287257430254138
Validation loss: 2.9732130834952617

Epoch: 5| Step: 1
Training loss: 3.0701747407171465
Validation loss: 2.96954315699681

Epoch: 5| Step: 2
Training loss: 3.0504395590301243
Validation loss: 2.9669961734721766

Epoch: 5| Step: 3
Training loss: 2.425765524304905
Validation loss: 2.9645121191661405

Epoch: 5| Step: 4
Training loss: 3.483709160517807
Validation loss: 2.961832489993325

Epoch: 5| Step: 5
Training loss: 3.3073003012877358
Validation loss: 2.960030791824864

Epoch: 5| Step: 6
Training loss: 3.006937905515668
Validation loss: 2.957694738038454

Epoch: 5| Step: 7
Training loss: 3.2931439589546363
Validation loss: 2.9555938163161484

Epoch: 5| Step: 8
Training loss: 2.850986704762908
Validation loss: 2.9525795791166565

Epoch: 5| Step: 9
Training loss: 3.358090745164399
Validation loss: 2.949737341550941

Epoch: 5| Step: 10
Training loss: 3.384081314038842
Validation loss: 2.9470895925727114

Epoch: 5| Step: 11
Training loss: 0.9748219657624739
Validation loss: 2.9443771570292663

Epoch: 52| Step: 0
Training loss: 3.180606493664613
Validation loss: 2.9422497293512846

Epoch: 5| Step: 1
Training loss: 3.0997229052341755
Validation loss: 2.940171082475397

Epoch: 5| Step: 2
Training loss: 3.250796000496005
Validation loss: 2.93784675512979

Epoch: 5| Step: 3
Training loss: 2.9832398817318673
Validation loss: 2.9360673637152526

Epoch: 5| Step: 4
Training loss: 3.1334526188106078
Validation loss: 2.933624677160658

Epoch: 5| Step: 5
Training loss: 2.9008742626020103
Validation loss: 2.9312223118638148

Epoch: 5| Step: 6
Training loss: 2.78971158550228
Validation loss: 2.9289519408101286

Epoch: 5| Step: 7
Training loss: 3.3682802909690093
Validation loss: 2.926948273869955

Epoch: 5| Step: 8
Training loss: 3.0088673513021362
Validation loss: 2.9243995551496784

Epoch: 5| Step: 9
Training loss: 3.223357079335282
Validation loss: 2.92221056529316

Epoch: 5| Step: 10
Training loss: 2.656816309978592
Validation loss: 2.92010125637312

Epoch: 5| Step: 11
Training loss: 3.6957675903620095
Validation loss: 2.9176586757484966

Epoch: 53| Step: 0
Training loss: 2.9698074866687127
Validation loss: 2.9152142564192736

Epoch: 5| Step: 1
Training loss: 3.322556255889035
Validation loss: 2.9127583896639573

Epoch: 5| Step: 2
Training loss: 2.5090750014953045
Validation loss: 2.9101543520381465

Epoch: 5| Step: 3
Training loss: 2.9048488069409952
Validation loss: 2.908127622561513

Epoch: 5| Step: 4
Training loss: 3.163077043201064
Validation loss: 2.9057167411703086

Epoch: 5| Step: 5
Training loss: 3.061521568480291
Validation loss: 2.9035092046550273

Epoch: 5| Step: 6
Training loss: 2.6492305682230035
Validation loss: 2.9015057715898083

Epoch: 5| Step: 7
Training loss: 2.8497964485158644
Validation loss: 2.899304656956902

Epoch: 5| Step: 8
Training loss: 3.0404910772613296
Validation loss: 2.897547747240808

Epoch: 5| Step: 9
Training loss: 3.4715541468159334
Validation loss: 2.8950928088834558

Epoch: 5| Step: 10
Training loss: 3.2613677241408814
Validation loss: 2.8928841157078473

Epoch: 5| Step: 11
Training loss: 3.7891314628067
Validation loss: 2.890823354015645

Epoch: 54| Step: 0
Training loss: 2.6750710094688483
Validation loss: 2.8882845742981513

Epoch: 5| Step: 1
Training loss: 2.915322693487897
Validation loss: 2.8860366662430823

Epoch: 5| Step: 2
Training loss: 2.9312162352612074
Validation loss: 2.8837942779387062

Epoch: 5| Step: 3
Training loss: 2.7398896330164177
Validation loss: 2.8811885184502044

Epoch: 5| Step: 4
Training loss: 3.351285198272981
Validation loss: 2.878982035701762

Epoch: 5| Step: 5
Training loss: 3.1735103506782227
Validation loss: 2.8769932070976667

Epoch: 5| Step: 6
Training loss: 3.216427159328289
Validation loss: 2.87495317973837

Epoch: 5| Step: 7
Training loss: 2.6289788019334854
Validation loss: 2.8747737525982417

Epoch: 5| Step: 8
Training loss: 2.7627365632414347
Validation loss: 2.8716983011628345

Epoch: 5| Step: 9
Training loss: 3.4291538527207965
Validation loss: 2.8704875176262505

Epoch: 5| Step: 10
Training loss: 3.3154844838531363
Validation loss: 2.866781481441508

Epoch: 5| Step: 11
Training loss: 2.6633281870359786
Validation loss: 2.864581130055823

Epoch: 55| Step: 0
Training loss: 2.7446319900384073
Validation loss: 2.8628445509561464

Epoch: 5| Step: 1
Training loss: 3.0942508792966867
Validation loss: 2.861335270062271

Epoch: 5| Step: 2
Training loss: 3.4267340175773318
Validation loss: 2.8591570084932294

Epoch: 5| Step: 3
Training loss: 3.0664748263132044
Validation loss: 2.8576234097613247

Epoch: 5| Step: 4
Training loss: 2.972172741661294
Validation loss: 2.855899320843172

Epoch: 5| Step: 5
Training loss: 3.0943205336259747
Validation loss: 2.8535364643287253

Epoch: 5| Step: 6
Training loss: 2.570386984073466
Validation loss: 2.85131395554493

Epoch: 5| Step: 7
Training loss: 3.003918949095394
Validation loss: 2.8493562951589313

Epoch: 5| Step: 8
Training loss: 2.9502384671309967
Validation loss: 2.847070721857436

Epoch: 5| Step: 9
Training loss: 2.656063836249778
Validation loss: 2.8449749805934745

Epoch: 5| Step: 10
Training loss: 3.19656981561808
Validation loss: 2.842651285936593

Epoch: 5| Step: 11
Training loss: 3.347056527651123
Validation loss: 2.840680285574314

Epoch: 56| Step: 0
Training loss: 3.3472995634803557
Validation loss: 2.838700387466024

Epoch: 5| Step: 1
Training loss: 3.6008689361509556
Validation loss: 2.8367418246747733

Epoch: 5| Step: 2
Training loss: 2.6769353229460147
Validation loss: 2.834682733470571

Epoch: 5| Step: 3
Training loss: 3.336244376939971
Validation loss: 2.8325330583450716

Epoch: 5| Step: 4
Training loss: 2.8677242930326496
Validation loss: 2.8297824814890844

Epoch: 5| Step: 5
Training loss: 1.8804427306368456
Validation loss: 2.827675936543379

Epoch: 5| Step: 6
Training loss: 2.5508463034870745
Validation loss: 2.8256852425881966

Epoch: 5| Step: 7
Training loss: 3.1572501231528878
Validation loss: 2.8240290253131892

Epoch: 5| Step: 8
Training loss: 2.8235640868460656
Validation loss: 2.8230559055480753

Epoch: 5| Step: 9
Training loss: 2.60780661955411
Validation loss: 2.8216923431727374

Epoch: 5| Step: 10
Training loss: 3.390050461430107
Validation loss: 2.8360491153551317

Epoch: 5| Step: 11
Training loss: 3.312419674457246
Validation loss: 2.8164322455516415

Epoch: 57| Step: 0
Training loss: 2.6148747944078865
Validation loss: 2.8160447000149156

Epoch: 5| Step: 1
Training loss: 2.427232489416336
Validation loss: 2.8151847634906986

Epoch: 5| Step: 2
Training loss: 3.1175715848019827
Validation loss: 2.816216676269091

Epoch: 5| Step: 3
Training loss: 3.0515267099995733
Validation loss: 2.8181523039687923

Epoch: 5| Step: 4
Training loss: 2.8808961199987118
Validation loss: 2.818699601844282

Epoch: 5| Step: 5
Training loss: 2.8124699061161236
Validation loss: 2.8164366580698976

Epoch: 5| Step: 6
Training loss: 3.2616269401375226
Validation loss: 2.8124978559980347

Epoch: 5| Step: 7
Training loss: 2.9277757434327185
Validation loss: 2.808726357560137

Epoch: 5| Step: 8
Training loss: 2.7285696255058087
Validation loss: 2.805639571142624

Epoch: 5| Step: 9
Training loss: 3.3492917635954007
Validation loss: 2.804295306115199

Epoch: 5| Step: 10
Training loss: 3.1765412081761526
Validation loss: 2.8003625419330618

Epoch: 5| Step: 11
Training loss: 2.953616631169058
Validation loss: 2.79760672589268

Epoch: 58| Step: 0
Training loss: 2.616784636962445
Validation loss: 2.796914290395489

Epoch: 5| Step: 1
Training loss: 2.8746599742667596
Validation loss: 2.794699360795538

Epoch: 5| Step: 2
Training loss: 2.736912449181757
Validation loss: 2.7944410662718067

Epoch: 5| Step: 3
Training loss: 2.8306879896288297
Validation loss: 2.7922925437290798

Epoch: 5| Step: 4
Training loss: 3.053935629175644
Validation loss: 2.7878661381385195

Epoch: 5| Step: 5
Training loss: 3.460487215875856
Validation loss: 2.7868461671729188

Epoch: 5| Step: 6
Training loss: 2.9617474336475373
Validation loss: 2.7849979828268494

Epoch: 5| Step: 7
Training loss: 2.7760782913359026
Validation loss: 2.7833452012233546

Epoch: 5| Step: 8
Training loss: 3.258202691804761
Validation loss: 2.781071975096677

Epoch: 5| Step: 9
Training loss: 2.7107804222661964
Validation loss: 2.7782806554555215

Epoch: 5| Step: 10
Training loss: 2.796225189931093
Validation loss: 2.7775585915926184

Epoch: 5| Step: 11
Training loss: 3.1412538117197015
Validation loss: 2.7758020091190945

Epoch: 59| Step: 0
Training loss: 3.144281640423096
Validation loss: 2.7749856952779677

Epoch: 5| Step: 1
Training loss: 3.061513936637969
Validation loss: 2.7727209141509594

Epoch: 5| Step: 2
Training loss: 2.8808856924115713
Validation loss: 2.771437168055335

Epoch: 5| Step: 3
Training loss: 2.587102160990197
Validation loss: 2.7681399833737066

Epoch: 5| Step: 4
Training loss: 3.143930242691565
Validation loss: 2.7665950955953713

Epoch: 5| Step: 5
Training loss: 3.265825913376948
Validation loss: 2.765121679968504

Epoch: 5| Step: 6
Training loss: 2.7665149528393025
Validation loss: 2.763782454975422

Epoch: 5| Step: 7
Training loss: 2.7761196867456417
Validation loss: 2.7615734819319995

Epoch: 5| Step: 8
Training loss: 2.9542218144429615
Validation loss: 2.760202505093208

Epoch: 5| Step: 9
Training loss: 2.5315272803376523
Validation loss: 2.759232199758677

Epoch: 5| Step: 10
Training loss: 2.944807823918568
Validation loss: 2.75776974912365

Epoch: 5| Step: 11
Training loss: 2.0132034304481414
Validation loss: 2.7564259646667026

Epoch: 60| Step: 0
Training loss: 2.955632028650004
Validation loss: 2.7548890184190356

Epoch: 5| Step: 1
Training loss: 2.7371775188976137
Validation loss: 2.7539519899145506

Epoch: 5| Step: 2
Training loss: 3.056766982711747
Validation loss: 2.7521544527908492

Epoch: 5| Step: 3
Training loss: 2.7765385014361454
Validation loss: 2.752055032475335

Epoch: 5| Step: 4
Training loss: 2.8156385923135865
Validation loss: 2.7497929003790036

Epoch: 5| Step: 5
Training loss: 2.975920199883017
Validation loss: 2.7484820865602284

Epoch: 5| Step: 6
Training loss: 2.9544021024806506
Validation loss: 2.7470298354566864

Epoch: 5| Step: 7
Training loss: 3.0029173018037256
Validation loss: 2.7453206123893663

Epoch: 5| Step: 8
Training loss: 2.875439154214508
Validation loss: 2.743467059568774

Epoch: 5| Step: 9
Training loss: 2.8201285431696195
Validation loss: 2.7419432760456073

Epoch: 5| Step: 10
Training loss: 2.6977398101300283
Validation loss: 2.740683020626765

Epoch: 5| Step: 11
Training loss: 3.3387844971388856
Validation loss: 2.7391155203696136

Epoch: 61| Step: 0
Training loss: 2.8055462810837275
Validation loss: 2.7376309800420966

Epoch: 5| Step: 1
Training loss: 3.4255806702166014
Validation loss: 2.7353612737768116

Epoch: 5| Step: 2
Training loss: 2.4483406403157484
Validation loss: 2.734123756128551

Epoch: 5| Step: 3
Training loss: 2.4647072634866474
Validation loss: 2.7323966907924864

Epoch: 5| Step: 4
Training loss: 3.2467056597405333
Validation loss: 2.7316809652484544

Epoch: 5| Step: 5
Training loss: 2.6280799235901924
Validation loss: 2.7298579881339102

Epoch: 5| Step: 6
Training loss: 2.679963923396505
Validation loss: 2.72912641063635

Epoch: 5| Step: 7
Training loss: 2.855247979559792
Validation loss: 2.7269222310047563

Epoch: 5| Step: 8
Training loss: 3.037225398457071
Validation loss: 2.726416076546838

Epoch: 5| Step: 9
Training loss: 3.1273547646688056
Validation loss: 2.7244358057592053

Epoch: 5| Step: 10
Training loss: 2.725869601723789
Validation loss: 2.723031626091233

Epoch: 5| Step: 11
Training loss: 2.7075437911024274
Validation loss: 2.7221601007687117

Epoch: 62| Step: 0
Training loss: 2.922830354082761
Validation loss: 2.7209281088974864

Epoch: 5| Step: 1
Training loss: 2.633871437173537
Validation loss: 2.7188636164109257

Epoch: 5| Step: 2
Training loss: 2.7090416691118153
Validation loss: 2.720013576957533

Epoch: 5| Step: 3
Training loss: 2.9821214405919134
Validation loss: 2.7166153650374048

Epoch: 5| Step: 4
Training loss: 3.319294852092009
Validation loss: 2.7147276892427503

Epoch: 5| Step: 5
Training loss: 3.300932931518571
Validation loss: 2.712357111798918

Epoch: 5| Step: 6
Training loss: 3.2433728360798733
Validation loss: 2.7142150114257437

Epoch: 5| Step: 7
Training loss: 2.4561482648674184
Validation loss: 2.71323545098564

Epoch: 5| Step: 8
Training loss: 2.550174939007804
Validation loss: 2.7132553137423723

Epoch: 5| Step: 9
Training loss: 2.338483054963634
Validation loss: 2.7124246078479346

Epoch: 5| Step: 10
Training loss: 2.7678903041558285
Validation loss: 2.711616196630654

Epoch: 5| Step: 11
Training loss: 2.8275429142428177
Validation loss: 2.711111134621832

Epoch: 63| Step: 0
Training loss: 2.7008369773299523
Validation loss: 2.7096975240902164

Epoch: 5| Step: 1
Training loss: 3.165619041083283
Validation loss: 2.709010878811215

Epoch: 5| Step: 2
Training loss: 2.7539350792207764
Validation loss: 2.7075089349244106

Epoch: 5| Step: 3
Training loss: 2.7546557982335913
Validation loss: 2.705981702037136

Epoch: 5| Step: 4
Training loss: 3.022387260562024
Validation loss: 2.7050440449868853

Epoch: 5| Step: 5
Training loss: 2.8655028006187684
Validation loss: 2.703613626641868

Epoch: 5| Step: 6
Training loss: 2.9318297831525575
Validation loss: 2.702094408971891

Epoch: 5| Step: 7
Training loss: 2.8832010146927822
Validation loss: 2.700148157923242

Epoch: 5| Step: 8
Training loss: 2.5655155581365174
Validation loss: 2.69898625875553

Epoch: 5| Step: 9
Training loss: 3.209717158991218
Validation loss: 2.697418775729139

Epoch: 5| Step: 10
Training loss: 2.3400803384275957
Validation loss: 2.695735041351051

Epoch: 5| Step: 11
Training loss: 2.67603393252631
Validation loss: 2.695232714517944

Epoch: 64| Step: 0
Training loss: 3.330288609092106
Validation loss: 2.690026156884454

Epoch: 5| Step: 1
Training loss: 3.302978506602137
Validation loss: 2.6908812580662884

Epoch: 5| Step: 2
Training loss: 2.521481349242956
Validation loss: 2.6914443413373963

Epoch: 5| Step: 3
Training loss: 2.500588633857554
Validation loss: 2.704671817750972

Epoch: 5| Step: 4
Training loss: 3.221564386529876
Validation loss: 2.713278735132477

Epoch: 5| Step: 5
Training loss: 2.5410387065991458
Validation loss: 2.707722581565331

Epoch: 5| Step: 6
Training loss: 3.225383995911424
Validation loss: 2.7041125101584473

Epoch: 5| Step: 7
Training loss: 2.531073387483731
Validation loss: 2.696683802039407

Epoch: 5| Step: 8
Training loss: 2.995649680360888
Validation loss: 2.6886098957727933

Epoch: 5| Step: 9
Training loss: 2.436566198428631
Validation loss: 2.6829727912937416

Epoch: 5| Step: 10
Training loss: 2.36412167068527
Validation loss: 2.684013674409591

Epoch: 5| Step: 11
Training loss: 2.688901934797868
Validation loss: 2.6803829154541248

Epoch: 65| Step: 0
Training loss: 2.969933203771304
Validation loss: 2.6825166972062267

Epoch: 5| Step: 1
Training loss: 2.945544812984568
Validation loss: 2.682767549292604

Epoch: 5| Step: 2
Training loss: 3.1066628133834495
Validation loss: 2.683637638094088

Epoch: 5| Step: 3
Training loss: 2.5922772006288026
Validation loss: 2.6818597328523723

Epoch: 5| Step: 4
Training loss: 2.944808309692663
Validation loss: 2.6821253654011685

Epoch: 5| Step: 5
Training loss: 2.492140910033019
Validation loss: 2.679376022431214

Epoch: 5| Step: 6
Training loss: 2.4710352497249612
Validation loss: 2.678397117271452

Epoch: 5| Step: 7
Training loss: 2.5331476892245317
Validation loss: 2.6771408673547206

Epoch: 5| Step: 8
Training loss: 2.905601675260372
Validation loss: 2.6749366596780075

Epoch: 5| Step: 9
Training loss: 3.134876056758122
Validation loss: 2.673012626978949

Epoch: 5| Step: 10
Training loss: 2.576420168302615
Validation loss: 2.671149540173783

Epoch: 5| Step: 11
Training loss: 3.798904943792705
Validation loss: 2.6688345067136736

Epoch: 66| Step: 0
Training loss: 3.0098546613412185
Validation loss: 2.6655394411834497

Epoch: 5| Step: 1
Training loss: 2.86468346623049
Validation loss: 2.665197751418248

Epoch: 5| Step: 2
Training loss: 3.078023376336417
Validation loss: 2.6631933306498703

Epoch: 5| Step: 3
Training loss: 2.72604338942636
Validation loss: 2.6611622921305313

Epoch: 5| Step: 4
Training loss: 2.876580011499947
Validation loss: 2.6625415155520136

Epoch: 5| Step: 5
Training loss: 2.2508597320887898
Validation loss: 2.658960111763354

Epoch: 5| Step: 6
Training loss: 3.066665774497303
Validation loss: 2.6556240428810014

Epoch: 5| Step: 7
Training loss: 2.886017627770616
Validation loss: 2.659947359581827

Epoch: 5| Step: 8
Training loss: 2.6680783965561576
Validation loss: 2.661372970336849

Epoch: 5| Step: 9
Training loss: 2.447087140588339
Validation loss: 2.650783542490031

Epoch: 5| Step: 10
Training loss: 2.7076131792444
Validation loss: 2.655348718363332

Epoch: 5| Step: 11
Training loss: 3.143455785858762
Validation loss: 2.6536882613623973

Epoch: 67| Step: 0
Training loss: 2.729702166565403
Validation loss: 2.6519652008951935

Epoch: 5| Step: 1
Training loss: 2.8122956943480766
Validation loss: 2.650941825640425

Epoch: 5| Step: 2
Training loss: 2.2644242886794377
Validation loss: 2.6527678347751316

Epoch: 5| Step: 3
Training loss: 3.2334351579291374
Validation loss: 2.650281206241669

Epoch: 5| Step: 4
Training loss: 2.8127136149346366
Validation loss: 2.650942848677139

Epoch: 5| Step: 5
Training loss: 2.929033292842301
Validation loss: 2.6506637361312486

Epoch: 5| Step: 6
Training loss: 2.5484696527133264
Validation loss: 2.648544354379031

Epoch: 5| Step: 7
Training loss: 3.214358398584511
Validation loss: 2.6464742474989764

Epoch: 5| Step: 8
Training loss: 2.6383499186274246
Validation loss: 2.6455567915015377

Epoch: 5| Step: 9
Training loss: 2.719396054031417
Validation loss: 2.6433142674970354

Epoch: 5| Step: 10
Training loss: 2.682041746675134
Validation loss: 2.643148217425872

Epoch: 5| Step: 11
Training loss: 2.3542549206780614
Validation loss: 2.6386948477918946

Epoch: 68| Step: 0
Training loss: 2.6488239377743503
Validation loss: 2.637748227182452

Epoch: 5| Step: 1
Training loss: 2.71511158891243
Validation loss: 2.635012687408864

Epoch: 5| Step: 2
Training loss: 2.4130604463571643
Validation loss: 2.6377682666936053

Epoch: 5| Step: 3
Training loss: 3.108363001241224
Validation loss: 2.638091894449814

Epoch: 5| Step: 4
Training loss: 2.869037083574604
Validation loss: 2.6382574230771536

Epoch: 5| Step: 5
Training loss: 2.437290280441132
Validation loss: 2.6307782918456475

Epoch: 5| Step: 6
Training loss: 3.153062627381502
Validation loss: 2.6313837326992413

Epoch: 5| Step: 7
Training loss: 3.0672988661507294
Validation loss: 2.6297002673051675

Epoch: 5| Step: 8
Training loss: 2.8574180368331192
Validation loss: 2.6312623408036475

Epoch: 5| Step: 9
Training loss: 2.6125646957344797
Validation loss: 2.627579674104099

Epoch: 5| Step: 10
Training loss: 2.538749511207823
Validation loss: 2.6263166107900617

Epoch: 5| Step: 11
Training loss: 2.661422450550951
Validation loss: 2.625400701850752

Epoch: 69| Step: 0
Training loss: 2.5687524763035032
Validation loss: 2.6262410802506633

Epoch: 5| Step: 1
Training loss: 2.6277605709008114
Validation loss: 2.6277085287630926

Epoch: 5| Step: 2
Training loss: 2.9280988531258227
Validation loss: 2.6261044177121664

Epoch: 5| Step: 3
Training loss: 2.7363288220466635
Validation loss: 2.6271064685183

Epoch: 5| Step: 4
Training loss: 2.9962151812399056
Validation loss: 2.6258011306907476

Epoch: 5| Step: 5
Training loss: 3.0298008551794124
Validation loss: 2.624784430477647

Epoch: 5| Step: 6
Training loss: 2.758721307243194
Validation loss: 2.62422142517008

Epoch: 5| Step: 7
Training loss: 2.955657841610284
Validation loss: 2.623101001672443

Epoch: 5| Step: 8
Training loss: 2.2323455129998133
Validation loss: 2.6234131247512886

Epoch: 5| Step: 9
Training loss: 2.614184396002878
Validation loss: 2.6212548643681606

Epoch: 5| Step: 10
Training loss: 2.7662594169942514
Validation loss: 2.619666846748055

Epoch: 5| Step: 11
Training loss: 3.0718139394006108
Validation loss: 2.6173273647001105

Epoch: 70| Step: 0
Training loss: 2.6146882379366003
Validation loss: 2.613978875846851

Epoch: 5| Step: 1
Training loss: 2.5786087969361926
Validation loss: 2.610908552580431

Epoch: 5| Step: 2
Training loss: 3.0236335316273935
Validation loss: 2.6105642825496704

Epoch: 5| Step: 3
Training loss: 2.8241131394770025
Validation loss: 2.6044377274904913

Epoch: 5| Step: 4
Training loss: 2.9236285001993214
Validation loss: 2.6126326234516255

Epoch: 5| Step: 5
Training loss: 2.462968648087588
Validation loss: 2.604584131476179

Epoch: 5| Step: 6
Training loss: 3.133919308702678
Validation loss: 2.6145560895151383

Epoch: 5| Step: 7
Training loss: 2.9747962817797826
Validation loss: 2.6054104784655068

Epoch: 5| Step: 8
Training loss: 2.4714543463310292
Validation loss: 2.6062601423180554

Epoch: 5| Step: 9
Training loss: 2.473531318085355
Validation loss: 2.606236471968928

Epoch: 5| Step: 10
Training loss: 2.7498035360729887
Validation loss: 2.60414545940665

Epoch: 5| Step: 11
Training loss: 2.3004457912479612
Validation loss: 2.607528265260091

Epoch: 71| Step: 0
Training loss: 2.7480671765974507
Validation loss: 2.6082579340309957

Epoch: 5| Step: 1
Training loss: 2.722040224370915
Validation loss: 2.6105150522890286

Epoch: 5| Step: 2
Training loss: 2.833341729394777
Validation loss: 2.6109269641699413

Epoch: 5| Step: 3
Training loss: 2.633108060190585
Validation loss: 2.609147212275695

Epoch: 5| Step: 4
Training loss: 3.1795114379427525
Validation loss: 2.6112014865340414

Epoch: 5| Step: 5
Training loss: 2.1513810401917266
Validation loss: 2.612034776475342

Epoch: 5| Step: 6
Training loss: 3.016727542438597
Validation loss: 2.611516199636126

Epoch: 5| Step: 7
Training loss: 2.72272183792167
Validation loss: 2.6122488159669617

Epoch: 5| Step: 8
Training loss: 2.747508394126114
Validation loss: 2.6058156565595376

Epoch: 5| Step: 9
Training loss: 2.7204186591348836
Validation loss: 2.601204649911661

Epoch: 5| Step: 10
Training loss: 2.7240224992380258
Validation loss: 2.598552734639731

Epoch: 5| Step: 11
Training loss: 2.1685889104550977
Validation loss: 2.593010705124

Epoch: 72| Step: 0
Training loss: 2.285182750580847
Validation loss: 2.5922725713350965

Epoch: 5| Step: 1
Training loss: 2.0997112302692478
Validation loss: 2.5992143131604637

Epoch: 5| Step: 2
Training loss: 2.8061320800474427
Validation loss: 2.607455668903098

Epoch: 5| Step: 3
Training loss: 2.900358072733993
Validation loss: 2.6017130520840146

Epoch: 5| Step: 4
Training loss: 2.775189944803117
Validation loss: 2.5950578624752225

Epoch: 5| Step: 5
Training loss: 2.558307474442637
Validation loss: 2.5900662049984504

Epoch: 5| Step: 6
Training loss: 3.0058336758769824
Validation loss: 2.5918889550393795

Epoch: 5| Step: 7
Training loss: 2.670014921853877
Validation loss: 2.594507099250946

Epoch: 5| Step: 8
Training loss: 3.058966486058582
Validation loss: 2.5964420588139228

Epoch: 5| Step: 9
Training loss: 3.005602056582704
Validation loss: 2.5967362230221345

Epoch: 5| Step: 10
Training loss: 2.907613126959867
Validation loss: 2.599087703647303

Epoch: 5| Step: 11
Training loss: 1.6205560366914966
Validation loss: 2.596496483751748

Epoch: 73| Step: 0
Training loss: 2.1232971773807123
Validation loss: 2.595939788094853

Epoch: 5| Step: 1
Training loss: 2.8503942317346476
Validation loss: 2.5945759955574474

Epoch: 5| Step: 2
Training loss: 2.28056471473423
Validation loss: 2.592346171527276

Epoch: 5| Step: 3
Training loss: 2.602567003397152
Validation loss: 2.59068816214986

Epoch: 5| Step: 4
Training loss: 2.3039050826945795
Validation loss: 2.5873984736183075

Epoch: 5| Step: 5
Training loss: 3.049662718337501
Validation loss: 2.5850704810250957

Epoch: 5| Step: 6
Training loss: 2.860271876023842
Validation loss: 2.585617253045493

Epoch: 5| Step: 7
Training loss: 2.977377151786366
Validation loss: 2.5820632024606827

Epoch: 5| Step: 8
Training loss: 2.8369031370809417
Validation loss: 2.5796426409882383

Epoch: 5| Step: 9
Training loss: 2.994869454520778
Validation loss: 2.581611172067532

Epoch: 5| Step: 10
Training loss: 3.11710581278071
Validation loss: 2.5801709312671655

Epoch: 5| Step: 11
Training loss: 1.2787296347986754
Validation loss: 2.5797839789984685

Epoch: 74| Step: 0
Training loss: 2.704381573507961
Validation loss: 2.578363338204223

Epoch: 5| Step: 1
Training loss: 2.5218918734793125
Validation loss: 2.577630652610644

Epoch: 5| Step: 2
Training loss: 2.760448201317272
Validation loss: 2.5790473598151538

Epoch: 5| Step: 3
Training loss: 2.347526153158768
Validation loss: 2.578343280067401

Epoch: 5| Step: 4
Training loss: 2.8955870233157777
Validation loss: 2.5807428459064132

Epoch: 5| Step: 5
Training loss: 2.779402815943588
Validation loss: 2.5724622438401545

Epoch: 5| Step: 6
Training loss: 2.7748281511587587
Validation loss: 2.5748218163780416

Epoch: 5| Step: 7
Training loss: 3.158765187259057
Validation loss: 2.5751795502441746

Epoch: 5| Step: 8
Training loss: 2.732550486441139
Validation loss: 2.5768712472776527

Epoch: 5| Step: 9
Training loss: 2.7074763384883203
Validation loss: 2.5759925866660813

Epoch: 5| Step: 10
Training loss: 2.433449828176793
Validation loss: 2.5774347132564572

Epoch: 5| Step: 11
Training loss: 2.569529683758359
Validation loss: 2.5759503681701084

Epoch: 75| Step: 0
Training loss: 3.1002434296551806
Validation loss: 2.573321516538165

Epoch: 5| Step: 1
Training loss: 2.355958903415707
Validation loss: 2.573787504512376

Epoch: 5| Step: 2
Training loss: 2.6263255450982177
Validation loss: 2.575709658426077

Epoch: 5| Step: 3
Training loss: 2.6652180690901153
Validation loss: 2.5826075339293766

Epoch: 5| Step: 4
Training loss: 2.5690671924492063
Validation loss: 2.589020707125592

Epoch: 5| Step: 5
Training loss: 2.8074145646993314
Validation loss: 2.583086360898076

Epoch: 5| Step: 6
Training loss: 2.6525343094745173
Validation loss: 2.583775387648323

Epoch: 5| Step: 7
Training loss: 3.0446271380564878
Validation loss: 2.5865709771648837

Epoch: 5| Step: 8
Training loss: 2.802360483145653
Validation loss: 2.5817770859303555

Epoch: 5| Step: 9
Training loss: 2.3499497874962483
Validation loss: 2.578310067826612

Epoch: 5| Step: 10
Training loss: 2.6929375399072204
Validation loss: 2.5696934086807732

Epoch: 5| Step: 11
Training loss: 3.0077566959109037
Validation loss: 2.566933382768394

Epoch: 76| Step: 0
Training loss: 2.4617594965858793
Validation loss: 2.566733882744339

Epoch: 5| Step: 1
Training loss: 2.0077433889643803
Validation loss: 2.572339229935449

Epoch: 5| Step: 2
Training loss: 3.038972907228951
Validation loss: 2.5733882237688483

Epoch: 5| Step: 3
Training loss: 2.5668536163546545
Validation loss: 2.5751145058082243

Epoch: 5| Step: 4
Training loss: 2.565280522256276
Validation loss: 2.5722893183029014

Epoch: 5| Step: 5
Training loss: 2.5207272551945334
Validation loss: 2.567225681051187

Epoch: 5| Step: 6
Training loss: 2.6680919791899464
Validation loss: 2.5662671907434134

Epoch: 5| Step: 7
Training loss: 3.1659562836627226
Validation loss: 2.5579240267019543

Epoch: 5| Step: 8
Training loss: 2.8079514067376765
Validation loss: 2.5590319231592975

Epoch: 5| Step: 9
Training loss: 2.726046450509546
Validation loss: 2.5627183162142377

Epoch: 5| Step: 10
Training loss: 2.8664007536699483
Validation loss: 2.567585558532079

Epoch: 5| Step: 11
Training loss: 3.3894613352279697
Validation loss: 2.5727239503128283

Epoch: 77| Step: 0
Training loss: 2.8705125700571243
Validation loss: 2.5747207707791415

Epoch: 5| Step: 1
Training loss: 2.4779428191403445
Validation loss: 2.5767595971828334

Epoch: 5| Step: 2
Training loss: 2.7563431151008873
Validation loss: 2.580188164589179

Epoch: 5| Step: 3
Training loss: 2.7953371903004487
Validation loss: 2.5762505855494857

Epoch: 5| Step: 4
Training loss: 2.7632696607161726
Validation loss: 2.5731735847685373

Epoch: 5| Step: 5
Training loss: 2.5920301503441214
Validation loss: 2.567343662099369

Epoch: 5| Step: 6
Training loss: 2.3010920005486555
Validation loss: 2.565972216486698

Epoch: 5| Step: 7
Training loss: 2.9038998525496185
Validation loss: 2.5632499357289418

Epoch: 5| Step: 8
Training loss: 2.931194599335705
Validation loss: 2.5613948291871527

Epoch: 5| Step: 9
Training loss: 2.8025104712393585
Validation loss: 2.5589576093616833

Epoch: 5| Step: 10
Training loss: 2.765350069507926
Validation loss: 2.5571827965667144

Epoch: 5| Step: 11
Training loss: 0.3545959572985998
Validation loss: 2.5545413163072515

Epoch: 78| Step: 0
Training loss: 2.674248516731746
Validation loss: 2.551445800921993

Epoch: 5| Step: 1
Training loss: 3.164895934553029
Validation loss: 2.549722818974006

Epoch: 5| Step: 2
Training loss: 2.6942097495028863
Validation loss: 2.5514112612646103

Epoch: 5| Step: 3
Training loss: 2.6752220739003776
Validation loss: 2.5538449623624686

Epoch: 5| Step: 4
Training loss: 2.329160114568003
Validation loss: 2.547993794539963

Epoch: 5| Step: 5
Training loss: 2.909083472047316
Validation loss: 2.5522225958267493

Epoch: 5| Step: 6
Training loss: 2.6125612279114496
Validation loss: 2.545571092895999

Epoch: 5| Step: 7
Training loss: 2.1636528426140944
Validation loss: 2.5464795396515765

Epoch: 5| Step: 8
Training loss: 2.862503091735503
Validation loss: 2.5451775064064153

Epoch: 5| Step: 9
Training loss: 2.7488128527246447
Validation loss: 2.5423516395378756

Epoch: 5| Step: 10
Training loss: 2.518270298261285
Validation loss: 2.5427025745039606

Epoch: 5| Step: 11
Training loss: 3.033482152481103
Validation loss: 2.5429676874317737

Epoch: 79| Step: 0
Training loss: 2.6781664387758655
Validation loss: 2.548531455828713

Epoch: 5| Step: 1
Training loss: 3.232675299261295
Validation loss: 2.5511154571199706

Epoch: 5| Step: 2
Training loss: 2.377259233961265
Validation loss: 2.552837078866363

Epoch: 5| Step: 3
Training loss: 2.4959882973616696
Validation loss: 2.5602426665870652

Epoch: 5| Step: 4
Training loss: 2.345018679253661
Validation loss: 2.5600748565465774

Epoch: 5| Step: 5
Training loss: 2.830078461978068
Validation loss: 2.562849455164909

Epoch: 5| Step: 6
Training loss: 2.162820507840006
Validation loss: 2.5641160304223307

Epoch: 5| Step: 7
Training loss: 2.875276137649533
Validation loss: 2.5694455092373496

Epoch: 5| Step: 8
Training loss: 2.6722775875426588
Validation loss: 2.5588459696225287

Epoch: 5| Step: 9
Training loss: 3.0095034119332693
Validation loss: 2.5524719850607487

Epoch: 5| Step: 10
Training loss: 2.758890692265222
Validation loss: 2.5518549693692765

Epoch: 5| Step: 11
Training loss: 2.9145412512016455
Validation loss: 2.547343973461117

Epoch: 80| Step: 0
Training loss: 2.3794537996190397
Validation loss: 2.545578507647346

Epoch: 5| Step: 1
Training loss: 2.7174856819681055
Validation loss: 2.543407009817052

Epoch: 5| Step: 2
Training loss: 2.5842794306104118
Validation loss: 2.5417140281412793

Epoch: 5| Step: 3
Training loss: 2.7127723311029897
Validation loss: 2.5461516962190807

Epoch: 5| Step: 4
Training loss: 2.7055169255734124
Validation loss: 2.5472016076358837

Epoch: 5| Step: 5
Training loss: 2.511360676872364
Validation loss: 2.549228836462042

Epoch: 5| Step: 6
Training loss: 3.298047621041385
Validation loss: 2.5492283883177085

Epoch: 5| Step: 7
Training loss: 2.4985405476631053
Validation loss: 2.549962927511415

Epoch: 5| Step: 8
Training loss: 2.577011330250652
Validation loss: 2.551826449849623

Epoch: 5| Step: 9
Training loss: 2.6491734205241677
Validation loss: 2.5484292684110654

Epoch: 5| Step: 10
Training loss: 2.841666831485449
Validation loss: 2.5480398934842907

Epoch: 5| Step: 11
Training loss: 2.076226306391961
Validation loss: 2.545176909230534

Epoch: 81| Step: 0
Training loss: 2.733441787599485
Validation loss: 2.5452707148666533

Epoch: 5| Step: 1
Training loss: 2.484398895724556
Validation loss: 2.543527169742578

Epoch: 5| Step: 2
Training loss: 2.6386682607765772
Validation loss: 2.543798995737263

Epoch: 5| Step: 3
Training loss: 2.4330572084849837
Validation loss: 2.540098665528829

Epoch: 5| Step: 4
Training loss: 2.92980208109268
Validation loss: 2.5368375449036518

Epoch: 5| Step: 5
Training loss: 2.9662230327809462
Validation loss: 2.536222252378281

Epoch: 5| Step: 6
Training loss: 2.5194832730715544
Validation loss: 2.5369212036987765

Epoch: 5| Step: 7
Training loss: 2.964073754642658
Validation loss: 2.5358962011196433

Epoch: 5| Step: 8
Training loss: 2.5740838948933304
Validation loss: 2.5345268741795106

Epoch: 5| Step: 9
Training loss: 2.5245674367092428
Validation loss: 2.534988617216258

Epoch: 5| Step: 10
Training loss: 2.4543983410569794
Validation loss: 2.5321297196125516

Epoch: 5| Step: 11
Training loss: 2.715824438569996
Validation loss: 2.5301422866445096

Epoch: 82| Step: 0
Training loss: 2.6031058833964744
Validation loss: 2.536276739671084

Epoch: 5| Step: 1
Training loss: 2.331257805287076
Validation loss: 2.534554247900644

Epoch: 5| Step: 2
Training loss: 2.3870709498088627
Validation loss: 2.5365786331338573

Epoch: 5| Step: 3
Training loss: 2.704579397561181
Validation loss: 2.536683539954981

Epoch: 5| Step: 4
Training loss: 2.552062386147824
Validation loss: 2.5442298051590444

Epoch: 5| Step: 5
Training loss: 2.9286443455354423
Validation loss: 2.531231236486395

Epoch: 5| Step: 6
Training loss: 2.65578663094183
Validation loss: 2.5235059667851445

Epoch: 5| Step: 7
Training loss: 2.9115940305765773
Validation loss: 2.522191813948954

Epoch: 5| Step: 8
Training loss: 2.615366105514722
Validation loss: 2.5228143952166198

Epoch: 5| Step: 9
Training loss: 2.8872910527184317
Validation loss: 2.5258440995707967

Epoch: 5| Step: 10
Training loss: 2.700164055255341
Validation loss: 2.525367990438069

Epoch: 5| Step: 11
Training loss: 2.190288619092086
Validation loss: 2.5320095563126164

Epoch: 83| Step: 0
Training loss: 2.258120083564666
Validation loss: 2.5309424900342194

Epoch: 5| Step: 1
Training loss: 2.9584907346589726
Validation loss: 2.535511037083232

Epoch: 5| Step: 2
Training loss: 2.6306917155536707
Validation loss: 2.5376306849371946

Epoch: 5| Step: 3
Training loss: 2.9767044313889457
Validation loss: 2.538439188033895

Epoch: 5| Step: 4
Training loss: 2.555577301080301
Validation loss: 2.5403827345274737

Epoch: 5| Step: 5
Training loss: 2.8631152095764687
Validation loss: 2.540823805828588

Epoch: 5| Step: 6
Training loss: 2.119731880363388
Validation loss: 2.542860983007061

Epoch: 5| Step: 7
Training loss: 2.439536002395214
Validation loss: 2.539284114458381

Epoch: 5| Step: 8
Training loss: 2.6165689676920825
Validation loss: 2.5351961012237005

Epoch: 5| Step: 9
Training loss: 2.9087233324717774
Validation loss: 2.529377562660859

Epoch: 5| Step: 10
Training loss: 2.7606358758989815
Validation loss: 2.5296214559446377

Epoch: 5| Step: 11
Training loss: 2.952645722103129
Validation loss: 2.528801125898848

Epoch: 84| Step: 0
Training loss: 2.77995055628107
Validation loss: 2.5232506398400147

Epoch: 5| Step: 1
Training loss: 2.4889402849230517
Validation loss: 2.5219666177210196

Epoch: 5| Step: 2
Training loss: 2.49892173402123
Validation loss: 2.5207497383212627

Epoch: 5| Step: 3
Training loss: 2.1901882547088167
Validation loss: 2.518487375954171

Epoch: 5| Step: 4
Training loss: 2.605256170611262
Validation loss: 2.522985912026883

Epoch: 5| Step: 5
Training loss: 2.876966218730811
Validation loss: 2.5190302229902235

Epoch: 5| Step: 6
Training loss: 2.5320838214203594
Validation loss: 2.5183455050136643

Epoch: 5| Step: 7
Training loss: 2.6750424889728497
Validation loss: 2.521369334580228

Epoch: 5| Step: 8
Training loss: 2.424506154850853
Validation loss: 2.526367218957367

Epoch: 5| Step: 9
Training loss: 3.1299073407287716
Validation loss: 2.523236368074529

Epoch: 5| Step: 10
Training loss: 2.89061774175609
Validation loss: 2.521450268046652

Epoch: 5| Step: 11
Training loss: 1.8694604422665875
Validation loss: 2.517431151778617

Epoch: 85| Step: 0
Training loss: 2.709835741055121
Validation loss: 2.51590014862295

Epoch: 5| Step: 1
Training loss: 2.655965183920343
Validation loss: 2.5180800603191007

Epoch: 5| Step: 2
Training loss: 2.882109434825736
Validation loss: 2.509836427949085

Epoch: 5| Step: 3
Training loss: 2.9228425897268724
Validation loss: 2.512904613488948

Epoch: 5| Step: 4
Training loss: 2.8419017444675174
Validation loss: 2.515134807080625

Epoch: 5| Step: 5
Training loss: 2.32565965626131
Validation loss: 2.5138553334087237

Epoch: 5| Step: 6
Training loss: 2.4508257739345995
Validation loss: 2.5167607158546312

Epoch: 5| Step: 7
Training loss: 2.7410739474420347
Validation loss: 2.5186468163993014

Epoch: 5| Step: 8
Training loss: 2.175486683381517
Validation loss: 2.5204828717548327

Epoch: 5| Step: 9
Training loss: 2.7190520897014845
Validation loss: 2.519907526509458

Epoch: 5| Step: 10
Training loss: 2.480980143057449
Validation loss: 2.5168659671810496

Epoch: 5| Step: 11
Training loss: 2.9297485345204803
Validation loss: 2.515420707952987

Epoch: 86| Step: 0
Training loss: 2.5535227135461693
Validation loss: 2.5177034475594526

Epoch: 5| Step: 1
Training loss: 2.502000771038324
Validation loss: 2.5109548163395514

Epoch: 5| Step: 2
Training loss: 2.794991652965877
Validation loss: 2.5145054729030676

Epoch: 5| Step: 3
Training loss: 2.348636466992999
Validation loss: 2.515236372378408

Epoch: 5| Step: 4
Training loss: 2.8050363477776035
Validation loss: 2.5151068072773546

Epoch: 5| Step: 5
Training loss: 2.975371034709697
Validation loss: 2.5163475562720463

Epoch: 5| Step: 6
Training loss: 2.7030758715448373
Validation loss: 2.514451003734276

Epoch: 5| Step: 7
Training loss: 2.3710256752158885
Validation loss: 2.509788321109177

Epoch: 5| Step: 8
Training loss: 2.6619264871490795
Validation loss: 2.5169304132151726

Epoch: 5| Step: 9
Training loss: 2.5193159617625676
Validation loss: 2.5138623793546646

Epoch: 5| Step: 10
Training loss: 2.6737924577606496
Validation loss: 2.5127129295295907

Epoch: 5| Step: 11
Training loss: 2.733039398981339
Validation loss: 2.512400703144456

Epoch: 87| Step: 0
Training loss: 2.7828012490834904
Validation loss: 2.51150723402741

Epoch: 5| Step: 1
Training loss: 2.7950912840101854
Validation loss: 2.5127277868835827

Epoch: 5| Step: 2
Training loss: 2.428355351620746
Validation loss: 2.5123826588642793

Epoch: 5| Step: 3
Training loss: 2.181574397844942
Validation loss: 2.512856901418803

Epoch: 5| Step: 4
Training loss: 2.786219839186968
Validation loss: 2.5159343229188558

Epoch: 5| Step: 5
Training loss: 3.082510365811358
Validation loss: 2.515918876431506

Epoch: 5| Step: 6
Training loss: 2.8922160950436755
Validation loss: 2.5135652588170414

Epoch: 5| Step: 7
Training loss: 2.375337375971826
Validation loss: 2.515148003099332

Epoch: 5| Step: 8
Training loss: 2.4768134630766374
Validation loss: 2.512094140366212

Epoch: 5| Step: 9
Training loss: 2.4089249444134584
Validation loss: 2.5106635519696505

Epoch: 5| Step: 10
Training loss: 2.7109805936438884
Validation loss: 2.5136816167358598

Epoch: 5| Step: 11
Training loss: 2.518805631491763
Validation loss: 2.5100069080944163

Epoch: 88| Step: 0
Training loss: 2.605778574216205
Validation loss: 2.5120213644200935

Epoch: 5| Step: 1
Training loss: 2.150944582123731
Validation loss: 2.510748150152097

Epoch: 5| Step: 2
Training loss: 2.5870347936271556
Validation loss: 2.510885455389957

Epoch: 5| Step: 3
Training loss: 2.3583548658306825
Validation loss: 2.5097796487973043

Epoch: 5| Step: 4
Training loss: 2.8990696631513013
Validation loss: 2.5121709200058904

Epoch: 5| Step: 5
Training loss: 2.4387430787098254
Validation loss: 2.5106905567504185

Epoch: 5| Step: 6
Training loss: 2.693662719020619
Validation loss: 2.509744725715513

Epoch: 5| Step: 7
Training loss: 2.455318660682231
Validation loss: 2.512698926056983

Epoch: 5| Step: 8
Training loss: 2.7007970763678792
Validation loss: 2.5082101417518214

Epoch: 5| Step: 9
Training loss: 2.9012943799241557
Validation loss: 2.5081800070715485

Epoch: 5| Step: 10
Training loss: 2.8839251430917954
Validation loss: 2.5078652159477137

Epoch: 5| Step: 11
Training loss: 3.63206792604896
Validation loss: 2.5037411195416905

Epoch: 89| Step: 0
Training loss: 2.353482194718484
Validation loss: 2.5059498359048944

Epoch: 5| Step: 1
Training loss: 2.3235611185564884
Validation loss: 2.508949125199585

Epoch: 5| Step: 2
Training loss: 2.7585254644532666
Validation loss: 2.512512402005688

Epoch: 5| Step: 3
Training loss: 2.6916542769423475
Validation loss: 2.5116099308818494

Epoch: 5| Step: 4
Training loss: 2.274937400375813
Validation loss: 2.514811457315738

Epoch: 5| Step: 5
Training loss: 2.9138023117128524
Validation loss: 2.511786889575308

Epoch: 5| Step: 6
Training loss: 2.5444514439230663
Validation loss: 2.5114613940551114

Epoch: 5| Step: 7
Training loss: 2.472893340448812
Validation loss: 2.5110581015153435

Epoch: 5| Step: 8
Training loss: 2.7167877482323335
Validation loss: 2.5073996904275964

Epoch: 5| Step: 9
Training loss: 2.77327106674915
Validation loss: 2.5050679297700493

Epoch: 5| Step: 10
Training loss: 2.9367004280300195
Validation loss: 2.5022639791966053

Epoch: 5| Step: 11
Training loss: 3.3113939850010854
Validation loss: 2.4988544303110154

Epoch: 90| Step: 0
Training loss: 2.6693179738914106
Validation loss: 2.4968050727712443

Epoch: 5| Step: 1
Training loss: 2.6547041040971138
Validation loss: 2.4993162571181324

Epoch: 5| Step: 2
Training loss: 2.657157025028434
Validation loss: 2.4966681968084146

Epoch: 5| Step: 3
Training loss: 2.7685761304559815
Validation loss: 2.4999222862562567

Epoch: 5| Step: 4
Training loss: 2.621096842688656
Validation loss: 2.499361866729437

Epoch: 5| Step: 5
Training loss: 2.7705739887067073
Validation loss: 2.5006349194453956

Epoch: 5| Step: 6
Training loss: 2.1838110199794962
Validation loss: 2.5009760898210804

Epoch: 5| Step: 7
Training loss: 2.386527145861142
Validation loss: 2.498335836766392

Epoch: 5| Step: 8
Training loss: 2.420182553491393
Validation loss: 2.496860666914229

Epoch: 5| Step: 9
Training loss: 3.107488472043273
Validation loss: 2.4960809069538388

Epoch: 5| Step: 10
Training loss: 2.6619498638186503
Validation loss: 2.500463138834569

Epoch: 5| Step: 11
Training loss: 1.6957020114349315
Validation loss: 2.498807797675507

Epoch: 91| Step: 0
Training loss: 2.3816936930606123
Validation loss: 2.4955016156668233

Epoch: 5| Step: 1
Training loss: 2.3956200767364986
Validation loss: 2.4953547952467745

Epoch: 5| Step: 2
Training loss: 2.5316822954423315
Validation loss: 2.495060268481974

Epoch: 5| Step: 3
Training loss: 2.3513277766795957
Validation loss: 2.4969101726297005

Epoch: 5| Step: 4
Training loss: 2.7745832628634983
Validation loss: 2.4893247850521956

Epoch: 5| Step: 5
Training loss: 2.5848284875870777
Validation loss: 2.486813246433306

Epoch: 5| Step: 6
Training loss: 2.342307498802263
Validation loss: 2.4952039689850274

Epoch: 5| Step: 7
Training loss: 2.9385628907849064
Validation loss: 2.498208238342532

Epoch: 5| Step: 8
Training loss: 2.6723427167951908
Validation loss: 2.498091839705543

Epoch: 5| Step: 9
Training loss: 2.9002405198426238
Validation loss: 2.497700102207045

Epoch: 5| Step: 10
Training loss: 2.68884332485144
Validation loss: 2.5000760484095967

Epoch: 5| Step: 11
Training loss: 3.244621962287998
Validation loss: 2.4953371472336774

Epoch: 92| Step: 0
Training loss: 2.468884814480589
Validation loss: 2.4943143206092713

Epoch: 5| Step: 1
Training loss: 2.0525379613230346
Validation loss: 2.498529685305635

Epoch: 5| Step: 2
Training loss: 2.997360976099629
Validation loss: 2.497772253399425

Epoch: 5| Step: 3
Training loss: 2.5718263382786786
Validation loss: 2.499376716005856

Epoch: 5| Step: 4
Training loss: 2.6509995572706018
Validation loss: 2.4981013359951323

Epoch: 5| Step: 5
Training loss: 2.667979294852423
Validation loss: 2.495553254146209

Epoch: 5| Step: 6
Training loss: 2.739026891865352
Validation loss: 2.496903500575313

Epoch: 5| Step: 7
Training loss: 2.3807534248241695
Validation loss: 2.498097474652255

Epoch: 5| Step: 8
Training loss: 2.6138716459859794
Validation loss: 2.50044632742216

Epoch: 5| Step: 9
Training loss: 3.1145379989199538
Validation loss: 2.494671336214071

Epoch: 5| Step: 10
Training loss: 2.3814787587098136
Validation loss: 2.4968077942194964

Epoch: 5| Step: 11
Training loss: 2.8904887399261128
Validation loss: 2.5016417318262465

Epoch: 93| Step: 0
Training loss: 3.1206223916108384
Validation loss: 2.4967372942251993

Epoch: 5| Step: 1
Training loss: 2.867244875798963
Validation loss: 2.498573822284897

Epoch: 5| Step: 2
Training loss: 2.415959079237993
Validation loss: 2.496584490322168

Epoch: 5| Step: 3
Training loss: 2.6000366318396284
Validation loss: 2.4954132481737004

Epoch: 5| Step: 4
Training loss: 2.6379729728210775
Validation loss: 2.491740024139581

Epoch: 5| Step: 5
Training loss: 2.5017402314160915
Validation loss: 2.495208224970491

Epoch: 5| Step: 6
Training loss: 2.2432270896470374
Validation loss: 2.488105959384623

Epoch: 5| Step: 7
Training loss: 2.8185991064989606
Validation loss: 2.4888056986381604

Epoch: 5| Step: 8
Training loss: 2.87807896812317
Validation loss: 2.4847404483102773

Epoch: 5| Step: 9
Training loss: 1.6967183274358948
Validation loss: 2.485039296296832

Epoch: 5| Step: 10
Training loss: 2.5591972280019264
Validation loss: 2.485629103077447

Epoch: 5| Step: 11
Training loss: 3.1334170093591145
Validation loss: 2.4842368553348493

Epoch: 94| Step: 0
Training loss: 2.4602541020043933
Validation loss: 2.486081738686218

Epoch: 5| Step: 1
Training loss: 2.4468034093134916
Validation loss: 2.48972432815984

Epoch: 5| Step: 2
Training loss: 2.8304870183714734
Validation loss: 2.4917776075647717

Epoch: 5| Step: 3
Training loss: 2.769401773242847
Validation loss: 2.4960281689803754

Epoch: 5| Step: 4
Training loss: 2.519246592446775
Validation loss: 2.499216588774519

Epoch: 5| Step: 5
Training loss: 2.5994486884504413
Validation loss: 2.4995208638719384

Epoch: 5| Step: 6
Training loss: 2.6137500564816003
Validation loss: 2.5046504279122073

Epoch: 5| Step: 7
Training loss: 2.588177128893017
Validation loss: 2.50390793616307

Epoch: 5| Step: 8
Training loss: 2.4986904529157243
Validation loss: 2.504402547877891

Epoch: 5| Step: 9
Training loss: 2.9395444532635326
Validation loss: 2.506687751871144

Epoch: 5| Step: 10
Training loss: 2.43892784357883
Validation loss: 2.506281372438393

Epoch: 5| Step: 11
Training loss: 3.183187671810647
Validation loss: 2.5052420375298876

Epoch: 95| Step: 0
Training loss: 2.1612249286669027
Validation loss: 2.5085994044418074

Epoch: 5| Step: 1
Training loss: 2.6493692475130697
Validation loss: 2.5086092490358265

Epoch: 5| Step: 2
Training loss: 3.218770591892187
Validation loss: 2.510373088780288

Epoch: 5| Step: 3
Training loss: 2.4051136454737523
Validation loss: 2.5040392748588234

Epoch: 5| Step: 4
Training loss: 2.784363964328667
Validation loss: 2.5069149268911395

Epoch: 5| Step: 5
Training loss: 2.303946993555839
Validation loss: 2.503959944809393

Epoch: 5| Step: 6
Training loss: 2.544118032614307
Validation loss: 2.5001982411622543

Epoch: 5| Step: 7
Training loss: 2.5545241744069274
Validation loss: 2.489681726104807

Epoch: 5| Step: 8
Training loss: 2.897916104289004
Validation loss: 2.496260719505526

Epoch: 5| Step: 9
Training loss: 2.0543102309668155
Validation loss: 2.493948638066951

Epoch: 5| Step: 10
Training loss: 2.9810081783721234
Validation loss: 2.489969532166131

Epoch: 5| Step: 11
Training loss: 3.2566264163770344
Validation loss: 2.4902842996616728

Epoch: 96| Step: 0
Training loss: 2.3300831387794445
Validation loss: 2.4889203562821614

Epoch: 5| Step: 1
Training loss: 2.324248395458257
Validation loss: 2.487505142487749

Epoch: 5| Step: 2
Training loss: 2.1312402092940896
Validation loss: 2.4882633761528026

Epoch: 5| Step: 3
Training loss: 2.4143348713027724
Validation loss: 2.4871246628201313

Epoch: 5| Step: 4
Training loss: 2.836659741451119
Validation loss: 2.486007729718566

Epoch: 5| Step: 5
Training loss: 3.0135685525365776
Validation loss: 2.4888911894526755

Epoch: 5| Step: 6
Training loss: 3.055528919990121
Validation loss: 2.4888611321933296

Epoch: 5| Step: 7
Training loss: 2.696979217460539
Validation loss: 2.48788224978182

Epoch: 5| Step: 8
Training loss: 2.6335865546857824
Validation loss: 2.4843521756897124

Epoch: 5| Step: 9
Training loss: 2.388542315259386
Validation loss: 2.489524363107983

Epoch: 5| Step: 10
Training loss: 2.460333468467641
Validation loss: 2.488558679402511

Epoch: 5| Step: 11
Training loss: 3.858437737081559
Validation loss: 2.4848108369171946

Epoch: 97| Step: 0
Training loss: 2.6997335691039455
Validation loss: 2.490293271234694

Epoch: 5| Step: 1
Training loss: 2.163481265899182
Validation loss: 2.4872477734228227

Epoch: 5| Step: 2
Training loss: 2.1082118465518134
Validation loss: 2.484932507033976

Epoch: 5| Step: 3
Training loss: 2.8129667954368434
Validation loss: 2.486433096380585

Epoch: 5| Step: 4
Training loss: 2.6665621578082663
Validation loss: 2.484873783576366

Epoch: 5| Step: 5
Training loss: 2.5728200258543175
Validation loss: 2.4858405990464565

Epoch: 5| Step: 6
Training loss: 2.598737402771202
Validation loss: 2.4843671626641153

Epoch: 5| Step: 7
Training loss: 2.8290681899567733
Validation loss: 2.48391864591866

Epoch: 5| Step: 8
Training loss: 2.562959676311487
Validation loss: 2.4869934773990394

Epoch: 5| Step: 9
Training loss: 2.993774630578643
Validation loss: 2.4881157772631433

Epoch: 5| Step: 10
Training loss: 2.3819488461092018
Validation loss: 2.486383945410832

Epoch: 5| Step: 11
Training loss: 3.4164272472574653
Validation loss: 2.492645014399579

Epoch: 98| Step: 0
Training loss: 2.5199359420930123
Validation loss: 2.4933314035041843

Epoch: 5| Step: 1
Training loss: 2.851798269897852
Validation loss: 2.4908275062435017

Epoch: 5| Step: 2
Training loss: 2.753522091307443
Validation loss: 2.4903939030231164

Epoch: 5| Step: 3
Training loss: 2.1527437091168875
Validation loss: 2.4916425645315328

Epoch: 5| Step: 4
Training loss: 2.5336994517270846
Validation loss: 2.4923463330333098

Epoch: 5| Step: 5
Training loss: 2.484536795475915
Validation loss: 2.490929576098811

Epoch: 5| Step: 6
Training loss: 2.2608908816516005
Validation loss: 2.492032427930911

Epoch: 5| Step: 7
Training loss: 3.008351146472665
Validation loss: 2.4920416323904973

Epoch: 5| Step: 8
Training loss: 2.8086494366002395
Validation loss: 2.4896874957992203

Epoch: 5| Step: 9
Training loss: 2.3881472044898113
Validation loss: 2.4878531406601647

Epoch: 5| Step: 10
Training loss: 2.819290863723302
Validation loss: 2.484243449440975

Epoch: 5| Step: 11
Training loss: 2.666429111707955
Validation loss: 2.484642314324905

Epoch: 99| Step: 0
Training loss: 2.697865391149493
Validation loss: 2.4852662117422946

Epoch: 5| Step: 1
Training loss: 2.5988501537082596
Validation loss: 2.481478550135511

Epoch: 5| Step: 2
Training loss: 2.480443566862813
Validation loss: 2.4835078607479217

Epoch: 5| Step: 3
Training loss: 2.6331838464759807
Validation loss: 2.4814820270006592

Epoch: 5| Step: 4
Training loss: 2.6214604810507707
Validation loss: 2.4745054075123294

Epoch: 5| Step: 5
Training loss: 2.7965080830378612
Validation loss: 2.475706239243907

Epoch: 5| Step: 6
Training loss: 1.9577017908845236
Validation loss: 2.4776932093655533

Epoch: 5| Step: 7
Training loss: 2.730286948246133
Validation loss: 2.481833707660796

Epoch: 5| Step: 8
Training loss: 2.512919232516802
Validation loss: 2.4792584327468945

Epoch: 5| Step: 9
Training loss: 2.6419390055671164
Validation loss: 2.4774207744896883

Epoch: 5| Step: 10
Training loss: 2.739579571816844
Validation loss: 2.48254410637624

Epoch: 5| Step: 11
Training loss: 3.09807076873627
Validation loss: 2.4816910943386934

Epoch: 100| Step: 0
Training loss: 2.3865008715187015
Validation loss: 2.4816440311046035

Epoch: 5| Step: 1
Training loss: 2.0808724046531593
Validation loss: 2.478971434901423

Epoch: 5| Step: 2
Training loss: 2.787635587807373
Validation loss: 2.4794601512480483

Epoch: 5| Step: 3
Training loss: 2.217871478595829
Validation loss: 2.480161901202521

Epoch: 5| Step: 4
Training loss: 3.001475130444895
Validation loss: 2.482483801780886

Epoch: 5| Step: 5
Training loss: 3.0625084078926355
Validation loss: 2.480901809495245

Epoch: 5| Step: 6
Training loss: 1.9671031179350933
Validation loss: 2.483637942336664

Epoch: 5| Step: 7
Training loss: 2.835301015169051
Validation loss: 2.4827681849377745

Epoch: 5| Step: 8
Training loss: 2.756148660401398
Validation loss: 2.4783988907110293

Epoch: 5| Step: 9
Training loss: 2.8085797432880484
Validation loss: 2.4814563357145776

Epoch: 5| Step: 10
Training loss: 2.35578099042751
Validation loss: 2.4798748957512187

Epoch: 5| Step: 11
Training loss: 2.7824980260905403
Validation loss: 2.4820910817908484

Epoch: 101| Step: 0
Training loss: 2.310737763207877
Validation loss: 2.478846650880381

Epoch: 5| Step: 1
Training loss: 2.908287400663559
Validation loss: 2.4776496185908754

Epoch: 5| Step: 2
Training loss: 2.472048620017459
Validation loss: 2.470670624972808

Epoch: 5| Step: 3
Training loss: 2.618238642255337
Validation loss: 2.478135653228759

Epoch: 5| Step: 4
Training loss: 2.56625063814879
Validation loss: 2.4797838562110064

Epoch: 5| Step: 5
Training loss: 2.8496763079013823
Validation loss: 2.484698768327574

Epoch: 5| Step: 6
Training loss: 3.068408171656069
Validation loss: 2.478031949945688

Epoch: 5| Step: 7
Training loss: 1.7214976456288402
Validation loss: 2.466227635145086

Epoch: 5| Step: 8
Training loss: 2.8142893715102324
Validation loss: 2.475764666544204

Epoch: 5| Step: 9
Training loss: 2.6912547651603953
Validation loss: 2.4738654082397296

Epoch: 5| Step: 10
Training loss: 2.248069464787946
Validation loss: 2.4732384179669054

Epoch: 5| Step: 11
Training loss: 2.674435197616727
Validation loss: 2.474556894004293

Epoch: 102| Step: 0
Training loss: 2.3851340069500537
Validation loss: 2.4758348388746474

Epoch: 5| Step: 1
Training loss: 2.400299343673514
Validation loss: 2.472794045319541

Epoch: 5| Step: 2
Training loss: 2.392668006921398
Validation loss: 2.4774828944432103

Epoch: 5| Step: 3
Training loss: 2.8483761896429005
Validation loss: 2.478850790678394

Epoch: 5| Step: 4
Training loss: 2.459256812313161
Validation loss: 2.472660239459009

Epoch: 5| Step: 5
Training loss: 2.520604010205377
Validation loss: 2.4759962555718857

Epoch: 5| Step: 6
Training loss: 2.6048403872979127
Validation loss: 2.475392126237726

Epoch: 5| Step: 7
Training loss: 3.063095034966644
Validation loss: 2.471946453728758

Epoch: 5| Step: 8
Training loss: 2.6078773815021834
Validation loss: 2.474929816923893

Epoch: 5| Step: 9
Training loss: 2.5052419066740463
Validation loss: 2.4715237306416

Epoch: 5| Step: 10
Training loss: 2.5422622941637356
Validation loss: 2.4741170624792415

Epoch: 5| Step: 11
Training loss: 2.9393814230249897
Validation loss: 2.4701035285918533

Epoch: 103| Step: 0
Training loss: 2.921747541452527
Validation loss: 2.476184984669823

Epoch: 5| Step: 1
Training loss: 2.356100677696601
Validation loss: 2.474682946078714

Epoch: 5| Step: 2
Training loss: 2.437329506413372
Validation loss: 2.472517350138856

Epoch: 5| Step: 3
Training loss: 1.9655256964945338
Validation loss: 2.46969655557818

Epoch: 5| Step: 4
Training loss: 2.4663854937062157
Validation loss: 2.4746618669257123

Epoch: 5| Step: 5
Training loss: 2.049416165835669
Validation loss: 2.4779943384437346

Epoch: 5| Step: 6
Training loss: 3.5512080004315574
Validation loss: 2.4741088352937366

Epoch: 5| Step: 7
Training loss: 2.832796569543769
Validation loss: 2.4739497348569053

Epoch: 5| Step: 8
Training loss: 2.3396617582995862
Validation loss: 2.4788688005226844

Epoch: 5| Step: 9
Training loss: 2.694749590930223
Validation loss: 2.4620309812288967

Epoch: 5| Step: 10
Training loss: 2.7003816123005246
Validation loss: 2.46781544762195

Epoch: 5| Step: 11
Training loss: 2.163483580126517
Validation loss: 2.4671250037245662

Epoch: 104| Step: 0
Training loss: 2.152043316429996
Validation loss: 2.4728126737544005

Epoch: 5| Step: 1
Training loss: 2.4603977156341816
Validation loss: 2.4735065402373797

Epoch: 5| Step: 2
Training loss: 2.877367454677675
Validation loss: 2.4761911107724464

Epoch: 5| Step: 3
Training loss: 2.6981013368616473
Validation loss: 2.472100933245267

Epoch: 5| Step: 4
Training loss: 2.195162404858377
Validation loss: 2.4655583147411524

Epoch: 5| Step: 5
Training loss: 2.5128289078602672
Validation loss: 2.4679207153411618

Epoch: 5| Step: 6
Training loss: 2.4658635322635587
Validation loss: 2.470911741433228

Epoch: 5| Step: 7
Training loss: 2.7905138017087205
Validation loss: 2.465449400347042

Epoch: 5| Step: 8
Training loss: 2.8115159750465395
Validation loss: 2.4668305455897768

Epoch: 5| Step: 9
Training loss: 2.48889401734293
Validation loss: 2.4718201817262586

Epoch: 5| Step: 10
Training loss: 2.8871569475113383
Validation loss: 2.4663917730384037

Epoch: 5| Step: 11
Training loss: 2.7976054511082866
Validation loss: 2.4653489874165193

Epoch: 105| Step: 0
Training loss: 2.409278252189267
Validation loss: 2.467403954321458

Epoch: 5| Step: 1
Training loss: 2.7893085104103417
Validation loss: 2.4718681634357496

Epoch: 5| Step: 2
Training loss: 2.785547943179223
Validation loss: 2.4705868378263727

Epoch: 5| Step: 3
Training loss: 2.3920500254232135
Validation loss: 2.4691563887440693

Epoch: 5| Step: 4
Training loss: 2.790188174857915
Validation loss: 2.4719561549455142

Epoch: 5| Step: 5
Training loss: 2.664247030987642
Validation loss: 2.472616310861734

Epoch: 5| Step: 6
Training loss: 2.214176891658874
Validation loss: 2.476496967367216

Epoch: 5| Step: 7
Training loss: 2.683647232966416
Validation loss: 2.476322160404686

Epoch: 5| Step: 8
Training loss: 2.9307478549854733
Validation loss: 2.4770060201268094

Epoch: 5| Step: 9
Training loss: 2.575548861627197
Validation loss: 2.4816401521608333

Epoch: 5| Step: 10
Training loss: 2.1320139749008127
Validation loss: 2.47882770710853

Epoch: 5| Step: 11
Training loss: 2.1483207532200166
Validation loss: 2.481648746675139

Epoch: 106| Step: 0
Training loss: 2.753196765600807
Validation loss: 2.4849565692985047

Epoch: 5| Step: 1
Training loss: 2.6549183817529176
Validation loss: 2.48173266471874

Epoch: 5| Step: 2
Training loss: 2.6521591998743723
Validation loss: 2.4815413611452892

Epoch: 5| Step: 3
Training loss: 2.7815787303259474
Validation loss: 2.4855467793977577

Epoch: 5| Step: 4
Training loss: 2.8226959217281142
Validation loss: 2.4802900074714387

Epoch: 5| Step: 5
Training loss: 2.123248331992535
Validation loss: 2.4801623538157314

Epoch: 5| Step: 6
Training loss: 2.267810267510011
Validation loss: 2.4773416667325168

Epoch: 5| Step: 7
Training loss: 2.8059763377762827
Validation loss: 2.4801092653276733

Epoch: 5| Step: 8
Training loss: 2.7968008361871894
Validation loss: 2.473832688738249

Epoch: 5| Step: 9
Training loss: 2.4255049543548357
Validation loss: 2.4773705946086295

Epoch: 5| Step: 10
Training loss: 2.4637870639469166
Validation loss: 2.473383884326771

Epoch: 5| Step: 11
Training loss: 1.8297916216587689
Validation loss: 2.4725862827616636

Epoch: 107| Step: 0
Training loss: 2.089306799813651
Validation loss: 2.4689356255258916

Epoch: 5| Step: 1
Training loss: 2.3703943066194197
Validation loss: 2.47084097697559

Epoch: 5| Step: 2
Training loss: 3.0064505528035275
Validation loss: 2.470487011691659

Epoch: 5| Step: 3
Training loss: 2.517997049852602
Validation loss: 2.4731502190211003

Epoch: 5| Step: 4
Training loss: 2.809577780121289
Validation loss: 2.470198282926651

Epoch: 5| Step: 5
Training loss: 2.1305789640628046
Validation loss: 2.4670366345391375

Epoch: 5| Step: 6
Training loss: 3.1910518229982645
Validation loss: 2.4649639029822783

Epoch: 5| Step: 7
Training loss: 2.243087322166322
Validation loss: 2.4643895939423786

Epoch: 5| Step: 8
Training loss: 2.89058804875031
Validation loss: 2.473441683712873

Epoch: 5| Step: 9
Training loss: 2.460301295802099
Validation loss: 2.4708727269519475

Epoch: 5| Step: 10
Training loss: 2.6374451943438983
Validation loss: 2.466362265274301

Epoch: 5| Step: 11
Training loss: 2.036525507320685
Validation loss: 2.466927656551297

Epoch: 108| Step: 0
Training loss: 2.5184106034905107
Validation loss: 2.4661865325007812

Epoch: 5| Step: 1
Training loss: 2.3285237425285237
Validation loss: 2.4675970725251513

Epoch: 5| Step: 2
Training loss: 2.9521679816070727
Validation loss: 2.4678204432203423

Epoch: 5| Step: 3
Training loss: 3.133346093604977
Validation loss: 2.4659819516624686

Epoch: 5| Step: 4
Training loss: 2.6358681896348046
Validation loss: 2.468213316772684

Epoch: 5| Step: 5
Training loss: 2.7594921843205005
Validation loss: 2.4732701612662527

Epoch: 5| Step: 6
Training loss: 2.100287976274318
Validation loss: 2.4710402267533764

Epoch: 5| Step: 7
Training loss: 2.547867283273654
Validation loss: 2.4773754947504734

Epoch: 5| Step: 8
Training loss: 2.5749201419632013
Validation loss: 2.475167805348701

Epoch: 5| Step: 9
Training loss: 2.384291593160241
Validation loss: 2.4754114695209095

Epoch: 5| Step: 10
Training loss: 2.2979713989084845
Validation loss: 2.475693675649368

Epoch: 5| Step: 11
Training loss: 2.361297228589744
Validation loss: 2.4792547303808856

Epoch: 109| Step: 0
Training loss: 2.254622479490616
Validation loss: 2.4803913935416086

Epoch: 5| Step: 1
Training loss: 2.5190226669957494
Validation loss: 2.4824400270100013

Epoch: 5| Step: 2
Training loss: 2.823304172021482
Validation loss: 2.480426501617679

Epoch: 5| Step: 3
Training loss: 2.3579927146805546
Validation loss: 2.47740029806117

Epoch: 5| Step: 4
Training loss: 2.7019932312502952
Validation loss: 2.481753599754826

Epoch: 5| Step: 5
Training loss: 2.621004697193651
Validation loss: 2.478974332214154

Epoch: 5| Step: 6
Training loss: 1.8786099333083115
Validation loss: 2.4780596671165167

Epoch: 5| Step: 7
Training loss: 3.1311198011424675
Validation loss: 2.4758448157551927

Epoch: 5| Step: 8
Training loss: 2.275380670746735
Validation loss: 2.4790173466911476

Epoch: 5| Step: 9
Training loss: 2.838254544263468
Validation loss: 2.4730367262334827

Epoch: 5| Step: 10
Training loss: 2.7141034111668567
Validation loss: 2.4749544661279708

Epoch: 5| Step: 11
Training loss: 3.1095453723640256
Validation loss: 2.4715352945039877

Epoch: 110| Step: 0
Training loss: 2.369594646829538
Validation loss: 2.4700119036736443

Epoch: 5| Step: 1
Training loss: 2.5509836020247314
Validation loss: 2.475530062213573

Epoch: 5| Step: 2
Training loss: 1.7962993860855416
Validation loss: 2.471348497540343

Epoch: 5| Step: 3
Training loss: 2.763931186763443
Validation loss: 2.4733451217137374

Epoch: 5| Step: 4
Training loss: 2.687692058709016
Validation loss: 2.48258659082443

Epoch: 5| Step: 5
Training loss: 2.5366436983924237
Validation loss: 2.48060804161195

Epoch: 5| Step: 6
Training loss: 2.9256360332474105
Validation loss: 2.4755882609279345

Epoch: 5| Step: 7
Training loss: 2.7688937078858893
Validation loss: 2.474874829970735

Epoch: 5| Step: 8
Training loss: 2.855183849394426
Validation loss: 2.475405028465183

Epoch: 5| Step: 9
Training loss: 2.4228648470199587
Validation loss: 2.4715058602077025

Epoch: 5| Step: 10
Training loss: 2.4056603836386987
Validation loss: 2.4702923338982026

Epoch: 5| Step: 11
Training loss: 2.984693420359551
Validation loss: 2.4671555090020596

Epoch: 111| Step: 0
Training loss: 2.4509619634346866
Validation loss: 2.478717419982037

Epoch: 5| Step: 1
Training loss: 3.0205049709007
Validation loss: 2.480569295772741

Epoch: 5| Step: 2
Training loss: 2.3654904971063897
Validation loss: 2.482558015841419

Epoch: 5| Step: 3
Training loss: 2.433924473414316
Validation loss: 2.4949444476487614

Epoch: 5| Step: 4
Training loss: 2.7996527660866692
Validation loss: 2.503638445120673

Epoch: 5| Step: 5
Training loss: 2.7581688323817968
Validation loss: 2.4985908629818785

Epoch: 5| Step: 6
Training loss: 3.2865949421988274
Validation loss: 2.490364017477865

Epoch: 5| Step: 7
Training loss: 2.407760839384608
Validation loss: 2.473571475383339

Epoch: 5| Step: 8
Training loss: 2.2887093854266296
Validation loss: 2.469877480526164

Epoch: 5| Step: 9
Training loss: 2.8612842960060436
Validation loss: 2.467318638993939

Epoch: 5| Step: 10
Training loss: 2.0257906983048923
Validation loss: 2.4706565681664756

Epoch: 5| Step: 11
Training loss: 1.6693925342046
Validation loss: 2.473475416466784

Epoch: 112| Step: 0
Training loss: 2.566488464989483
Validation loss: 2.476294530095529

Epoch: 5| Step: 1
Training loss: 2.4949186182605563
Validation loss: 2.4773629255864935

Epoch: 5| Step: 2
Training loss: 2.8991908819143646
Validation loss: 2.4829796165270297

Epoch: 5| Step: 3
Training loss: 2.4982247725455284
Validation loss: 2.4830799606108642

Epoch: 5| Step: 4
Training loss: 2.797500455172777
Validation loss: 2.489247999125797

Epoch: 5| Step: 5
Training loss: 2.950251558860546
Validation loss: 2.4852663356555085

Epoch: 5| Step: 6
Training loss: 2.442128701701163
Validation loss: 2.4850789438356045

Epoch: 5| Step: 7
Training loss: 2.5976465210696573
Validation loss: 2.4836090954593195

Epoch: 5| Step: 8
Training loss: 2.5550237311886668
Validation loss: 2.478820818063999

Epoch: 5| Step: 9
Training loss: 2.410538258256658
Validation loss: 2.4801363604656967

Epoch: 5| Step: 10
Training loss: 2.12043417551262
Validation loss: 2.473821733953277

Epoch: 5| Step: 11
Training loss: 3.0962523541770737
Validation loss: 2.472361192982248

Epoch: 113| Step: 0
Training loss: 2.42720979898621
Validation loss: 2.4712415869863396

Epoch: 5| Step: 1
Training loss: 2.765396539767084
Validation loss: 2.470507089027333

Epoch: 5| Step: 2
Training loss: 2.6749917752148398
Validation loss: 2.4720428533586967

Epoch: 5| Step: 3
Training loss: 2.5443854773054344
Validation loss: 2.4739965670124167

Epoch: 5| Step: 4
Training loss: 2.86287104438189
Validation loss: 2.4727660361169073

Epoch: 5| Step: 5
Training loss: 2.5790410119560563
Validation loss: 2.473521008570851

Epoch: 5| Step: 6
Training loss: 2.447088114883564
Validation loss: 2.4729696821418012

Epoch: 5| Step: 7
Training loss: 2.7190647162325488
Validation loss: 2.476263411278748

Epoch: 5| Step: 8
Training loss: 2.1816535204828145
Validation loss: 2.475779428638877

Epoch: 5| Step: 9
Training loss: 2.7147004233749024
Validation loss: 2.4869809668303096

Epoch: 5| Step: 10
Training loss: 2.53410818480533
Validation loss: 2.4761588913949804

Epoch: 5| Step: 11
Training loss: 2.294601041403273
Validation loss: 2.4754657503004855

Epoch: 114| Step: 0
Training loss: 2.4665301035975875
Validation loss: 2.467362408259345

Epoch: 5| Step: 1
Training loss: 2.143653453732601
Validation loss: 2.469226899742191

Epoch: 5| Step: 2
Training loss: 2.820579790885287
Validation loss: 2.4699420666496494

Epoch: 5| Step: 3
Training loss: 2.653837926398955
Validation loss: 2.4623556541763345

Epoch: 5| Step: 4
Training loss: 1.9358113220820172
Validation loss: 2.464994229470515

Epoch: 5| Step: 5
Training loss: 2.733102556891493
Validation loss: 2.462866891680562

Epoch: 5| Step: 6
Training loss: 2.855822247751844
Validation loss: 2.4610307100597275

Epoch: 5| Step: 7
Training loss: 2.937711018226832
Validation loss: 2.466225194140851

Epoch: 5| Step: 8
Training loss: 2.83193460233794
Validation loss: 2.4589590243526804

Epoch: 5| Step: 9
Training loss: 2.618950275783713
Validation loss: 2.458945656073411

Epoch: 5| Step: 10
Training loss: 2.1287880960976735
Validation loss: 2.466217226621766

Epoch: 5| Step: 11
Training loss: 2.030160934758747
Validation loss: 2.4646139546607135

Epoch: 115| Step: 0
Training loss: 2.7391370015694174
Validation loss: 2.461983025692204

Epoch: 5| Step: 1
Training loss: 2.172171044241377
Validation loss: 2.4617408571468324

Epoch: 5| Step: 2
Training loss: 2.7043223292645946
Validation loss: 2.4627769942141

Epoch: 5| Step: 3
Training loss: 2.618182639341033
Validation loss: 2.466134122010365

Epoch: 5| Step: 4
Training loss: 2.4861599250230317
Validation loss: 2.4642836734942297

Epoch: 5| Step: 5
Training loss: 2.8236241222489826
Validation loss: 2.464958268864031

Epoch: 5| Step: 6
Training loss: 2.206805570423701
Validation loss: 2.4632179351325596

Epoch: 5| Step: 7
Training loss: 2.556583459271849
Validation loss: 2.4629643242921735

Epoch: 5| Step: 8
Training loss: 2.739991006627603
Validation loss: 2.4708135326379064

Epoch: 5| Step: 9
Training loss: 2.812301459192362
Validation loss: 2.4626648950958523

Epoch: 5| Step: 10
Training loss: 2.0689853084922065
Validation loss: 2.4619567052307363

Epoch: 5| Step: 11
Training loss: 3.0703640610730996
Validation loss: 2.461822007690273

Epoch: 116| Step: 0
Training loss: 2.5295249806648714
Validation loss: 2.4607016031544946

Epoch: 5| Step: 1
Training loss: 2.43605380071225
Validation loss: 2.4635854774972192

Epoch: 5| Step: 2
Training loss: 2.222762871248149
Validation loss: 2.463855248965484

Epoch: 5| Step: 3
Training loss: 2.7457279488036597
Validation loss: 2.4697782612381416

Epoch: 5| Step: 4
Training loss: 2.627948558205193
Validation loss: 2.4734251605601836

Epoch: 5| Step: 5
Training loss: 2.439256670870099
Validation loss: 2.4813812499801378

Epoch: 5| Step: 6
Training loss: 3.046562452669424
Validation loss: 2.4864144461351314

Epoch: 5| Step: 7
Training loss: 2.3123236924054282
Validation loss: 2.4667898275154387

Epoch: 5| Step: 8
Training loss: 2.419423098883015
Validation loss: 2.4614185488780707

Epoch: 5| Step: 9
Training loss: 3.1373509284240892
Validation loss: 2.4641809957907053

Epoch: 5| Step: 10
Training loss: 2.507512059738961
Validation loss: 2.462416685699143

Epoch: 5| Step: 11
Training loss: 2.343844093977101
Validation loss: 2.458509032118491

Epoch: 117| Step: 0
Training loss: 3.1309001917567074
Validation loss: 2.461142734265102

Epoch: 5| Step: 1
Training loss: 2.337611284513867
Validation loss: 2.4635881469298964

Epoch: 5| Step: 2
Training loss: 3.425233629709194
Validation loss: 2.466188403564103

Epoch: 5| Step: 3
Training loss: 2.148567667399103
Validation loss: 2.4696169791566587

Epoch: 5| Step: 4
Training loss: 2.2998884505712014
Validation loss: 2.468108880618366

Epoch: 5| Step: 5
Training loss: 2.074452647439441
Validation loss: 2.4691968787017897

Epoch: 5| Step: 6
Training loss: 2.837923221702848
Validation loss: 2.4752035815064883

Epoch: 5| Step: 7
Training loss: 2.3041567433737375
Validation loss: 2.4736462600368583

Epoch: 5| Step: 8
Training loss: 2.076133519533408
Validation loss: 2.472425207961768

Epoch: 5| Step: 9
Training loss: 2.4477828834716324
Validation loss: 2.472702516079753

Epoch: 5| Step: 10
Training loss: 2.958199636814182
Validation loss: 2.4749793358463554

Epoch: 5| Step: 11
Training loss: 1.671618789578021
Validation loss: 2.472491324630987

Epoch: 118| Step: 0
Training loss: 2.636648309967661
Validation loss: 2.4734940437863098

Epoch: 5| Step: 1
Training loss: 2.391734744530798
Validation loss: 2.4736834055408723

Epoch: 5| Step: 2
Training loss: 2.5659169001989923
Validation loss: 2.4686503168897524

Epoch: 5| Step: 3
Training loss: 2.4514351631194065
Validation loss: 2.4692399468321082

Epoch: 5| Step: 4
Training loss: 2.6874696596783925
Validation loss: 2.469964714428889

Epoch: 5| Step: 5
Training loss: 2.6254732522787956
Validation loss: 2.4709583939675266

Epoch: 5| Step: 6
Training loss: 2.322537264324783
Validation loss: 2.4693760017850637

Epoch: 5| Step: 7
Training loss: 2.6196222168338403
Validation loss: 2.462303755213269

Epoch: 5| Step: 8
Training loss: 2.674699238776917
Validation loss: 2.4694981869772463

Epoch: 5| Step: 9
Training loss: 2.806342781419214
Validation loss: 2.462487329169656

Epoch: 5| Step: 10
Training loss: 2.3400940928329717
Validation loss: 2.4576182753286586

Epoch: 5| Step: 11
Training loss: 2.7376563737354433
Validation loss: 2.4557469240249787

Epoch: 119| Step: 0
Training loss: 2.5022455620773494
Validation loss: 2.4546524686395834

Epoch: 5| Step: 1
Training loss: 2.62218769741319
Validation loss: 2.4498982194272845

Epoch: 5| Step: 2
Training loss: 2.3844856766478326
Validation loss: 2.458209597711767

Epoch: 5| Step: 3
Training loss: 2.532653984366757
Validation loss: 2.4546634523155526

Epoch: 5| Step: 4
Training loss: 2.355795361600002
Validation loss: 2.455280195491971

Epoch: 5| Step: 5
Training loss: 2.7684585796810595
Validation loss: 2.4628682348537936

Epoch: 5| Step: 6
Training loss: 2.4846328505574204
Validation loss: 2.456383854856328

Epoch: 5| Step: 7
Training loss: 2.2180967846900734
Validation loss: 2.45764412890314

Epoch: 5| Step: 8
Training loss: 2.4501126010476537
Validation loss: 2.4605390922543213

Epoch: 5| Step: 9
Training loss: 2.7898472982750993
Validation loss: 2.45940577557643

Epoch: 5| Step: 10
Training loss: 3.0832630011982607
Validation loss: 2.4589590122327865

Epoch: 5| Step: 11
Training loss: 1.6880403995605988
Validation loss: 2.449247677443098

Epoch: 120| Step: 0
Training loss: 2.563432012261694
Validation loss: 2.4567492049748303

Epoch: 5| Step: 1
Training loss: 2.588110802838819
Validation loss: 2.4512218698213206

Epoch: 5| Step: 2
Training loss: 2.540227346983973
Validation loss: 2.4514060710260397

Epoch: 5| Step: 3
Training loss: 1.9137007390913627
Validation loss: 2.4550834257138936

Epoch: 5| Step: 4
Training loss: 2.912673578288035
Validation loss: 2.4523081745809203

Epoch: 5| Step: 5
Training loss: 2.0684313018655245
Validation loss: 2.4624983061382837

Epoch: 5| Step: 6
Training loss: 2.563597839393128
Validation loss: 2.4573012882339147

Epoch: 5| Step: 7
Training loss: 3.0771355757328
Validation loss: 2.4573280223977743

Epoch: 5| Step: 8
Training loss: 3.0265866618848722
Validation loss: 2.4558241951531556

Epoch: 5| Step: 9
Training loss: 2.2986787732919978
Validation loss: 2.461069061153524

Epoch: 5| Step: 10
Training loss: 2.5551996213565134
Validation loss: 2.460947293055188

Epoch: 5| Step: 11
Training loss: 0.6347942169792576
Validation loss: 2.4576189099490495

Epoch: 121| Step: 0
Training loss: 2.450754465984948
Validation loss: 2.455535506173336

Epoch: 5| Step: 1
Training loss: 2.871345353293194
Validation loss: 2.454366673435263

Epoch: 5| Step: 2
Training loss: 2.8815736667444405
Validation loss: 2.451913866708564

Epoch: 5| Step: 3
Training loss: 2.65677583762912
Validation loss: 2.450819953286144

Epoch: 5| Step: 4
Training loss: 2.116503052382833
Validation loss: 2.449015919765372

Epoch: 5| Step: 5
Training loss: 2.3003520986317385
Validation loss: 2.4546712104682613

Epoch: 5| Step: 6
Training loss: 2.498133821144974
Validation loss: 2.4542666894664475

Epoch: 5| Step: 7
Training loss: 2.5716558499787605
Validation loss: 2.460017372363345

Epoch: 5| Step: 8
Training loss: 2.541271012158831
Validation loss: 2.4604005540669576

Epoch: 5| Step: 9
Training loss: 2.3650910312803344
Validation loss: 2.459813599050251

Epoch: 5| Step: 10
Training loss: 2.388127037912042
Validation loss: 2.456153518781434

Epoch: 5| Step: 11
Training loss: 3.5566497417167615
Validation loss: 2.4643745398950303

Epoch: 122| Step: 0
Training loss: 2.573274152582535
Validation loss: 2.459829082844695

Epoch: 5| Step: 1
Training loss: 2.3890125097280177
Validation loss: 2.4628882895934296

Epoch: 5| Step: 2
Training loss: 2.426711833065423
Validation loss: 2.4644056717810705

Epoch: 5| Step: 3
Training loss: 2.151312108315303
Validation loss: 2.4549985238955867

Epoch: 5| Step: 4
Training loss: 3.0137088993391608
Validation loss: 2.4622511854189235

Epoch: 5| Step: 5
Training loss: 2.841295965035197
Validation loss: 2.4550860942750687

Epoch: 5| Step: 6
Training loss: 2.451978476288312
Validation loss: 2.4572883879761105

Epoch: 5| Step: 7
Training loss: 2.1847117501431117
Validation loss: 2.4618144455895528

Epoch: 5| Step: 8
Training loss: 3.00308069361177
Validation loss: 2.458904383224086

Epoch: 5| Step: 9
Training loss: 2.8101321955635767
Validation loss: 2.459085342721916

Epoch: 5| Step: 10
Training loss: 2.2865183365464263
Validation loss: 2.4565033054858847

Epoch: 5| Step: 11
Training loss: 1.4471446204211729
Validation loss: 2.462522563411402

Epoch: 123| Step: 0
Training loss: 2.6145014857720015
Validation loss: 2.463616937908174

Epoch: 5| Step: 1
Training loss: 2.5691209251480256
Validation loss: 2.464911603522638

Epoch: 5| Step: 2
Training loss: 2.3082096406409245
Validation loss: 2.4710321139506988

Epoch: 5| Step: 3
Training loss: 2.2635251541577
Validation loss: 2.4720120788644846

Epoch: 5| Step: 4
Training loss: 2.652681983651712
Validation loss: 2.467249144480001

Epoch: 5| Step: 5
Training loss: 2.3815209061920153
Validation loss: 2.4676972289269643

Epoch: 5| Step: 6
Training loss: 2.7048258639710503
Validation loss: 2.470243838998986

Epoch: 5| Step: 7
Training loss: 3.0932123506937534
Validation loss: 2.475377044792983

Epoch: 5| Step: 8
Training loss: 2.6567267494953963
Validation loss: 2.4695916250222805

Epoch: 5| Step: 9
Training loss: 2.5171242742098383
Validation loss: 2.464183136464942

Epoch: 5| Step: 10
Training loss: 2.3497942489482218
Validation loss: 2.462419271678834

Epoch: 5| Step: 11
Training loss: 2.4155423958338127
Validation loss: 2.4640670092567043

Epoch: 124| Step: 0
Training loss: 2.5044570293277677
Validation loss: 2.4610453546696034

Epoch: 5| Step: 1
Training loss: 2.900331767605668
Validation loss: 2.4564417226317823

Epoch: 5| Step: 2
Training loss: 2.6023723269759937
Validation loss: 2.457548166956194

Epoch: 5| Step: 3
Training loss: 2.6283450838486297
Validation loss: 2.458926413542018

Epoch: 5| Step: 4
Training loss: 2.3265885781388747
Validation loss: 2.4582492778977065

Epoch: 5| Step: 5
Training loss: 2.333019462546781
Validation loss: 2.463526083888208

Epoch: 5| Step: 6
Training loss: 3.234474217464091
Validation loss: 2.4655495392340274

Epoch: 5| Step: 7
Training loss: 1.8887798037850467
Validation loss: 2.464540151527875

Epoch: 5| Step: 8
Training loss: 1.8688899622528667
Validation loss: 2.460519288873288

Epoch: 5| Step: 9
Training loss: 3.1692166016869536
Validation loss: 2.462340775294419

Epoch: 5| Step: 10
Training loss: 2.2595548144025077
Validation loss: 2.459621057153766

Epoch: 5| Step: 11
Training loss: 1.9379972619626886
Validation loss: 2.4653275987727947

Epoch: 125| Step: 0
Training loss: 2.566095016861044
Validation loss: 2.4645448313006137

Epoch: 5| Step: 1
Training loss: 2.4908301026097983
Validation loss: 2.457556966992554

Epoch: 5| Step: 2
Training loss: 2.6132145575208545
Validation loss: 2.458243842570448

Epoch: 5| Step: 3
Training loss: 2.303003348834659
Validation loss: 2.4579394773942425

Epoch: 5| Step: 4
Training loss: 2.39806986920632
Validation loss: 2.461274167399583

Epoch: 5| Step: 5
Training loss: 2.1113536455586424
Validation loss: 2.4536746152483815

Epoch: 5| Step: 6
Training loss: 2.904914959501575
Validation loss: 2.461371580320581

Epoch: 5| Step: 7
Training loss: 2.0392998451275877
Validation loss: 2.4551433836792156

Epoch: 5| Step: 8
Training loss: 3.078493667444851
Validation loss: 2.4544536774642354

Epoch: 5| Step: 9
Training loss: 2.5890612218343727
Validation loss: 2.459772086330629

Epoch: 5| Step: 10
Training loss: 2.693133194619453
Validation loss: 2.456813525654577

Epoch: 5| Step: 11
Training loss: 2.39029417055124
Validation loss: 2.453403129118232

Epoch: 126| Step: 0
Training loss: 3.081285896633812
Validation loss: 2.4609639363155638

Epoch: 5| Step: 1
Training loss: 2.2380079065887175
Validation loss: 2.4600346841796443

Epoch: 5| Step: 2
Training loss: 2.1652170124489913
Validation loss: 2.4666392642724344

Epoch: 5| Step: 3
Training loss: 2.7461197093534295
Validation loss: 2.47490668875506

Epoch: 5| Step: 4
Training loss: 3.1500337023672946
Validation loss: 2.4771871210294236

Epoch: 5| Step: 5
Training loss: 1.9986523021417626
Validation loss: 2.4768796009781746

Epoch: 5| Step: 6
Training loss: 2.128550648519261
Validation loss: 2.478403031256958

Epoch: 5| Step: 7
Training loss: 2.1933813323283182
Validation loss: 2.477479482136594

Epoch: 5| Step: 8
Training loss: 2.829422626705437
Validation loss: 2.484139925153656

Epoch: 5| Step: 9
Training loss: 2.910515065442368
Validation loss: 2.480676461081104

Epoch: 5| Step: 10
Training loss: 2.577769861903405
Validation loss: 2.4775053650265115

Epoch: 5| Step: 11
Training loss: 2.8884090693501667
Validation loss: 2.4802297201653754

Epoch: 127| Step: 0
Training loss: 2.746347429216728
Validation loss: 2.4785993685651957

Epoch: 5| Step: 1
Training loss: 3.2849031003830613
Validation loss: 2.4725957022390523

Epoch: 5| Step: 2
Training loss: 2.6437392069999843
Validation loss: 2.4737238013210403

Epoch: 5| Step: 3
Training loss: 2.3091858493426205
Validation loss: 2.4709316022371635

Epoch: 5| Step: 4
Training loss: 2.3957785116364256
Validation loss: 2.4724044109094168

Epoch: 5| Step: 5
Training loss: 2.6816503159249967
Validation loss: 2.4678095422560937

Epoch: 5| Step: 6
Training loss: 2.557339006802839
Validation loss: 2.467981919183763

Epoch: 5| Step: 7
Training loss: 2.3148191009764374
Validation loss: 2.464368900398267

Epoch: 5| Step: 8
Training loss: 2.3288057087384373
Validation loss: 2.4630752199852464

Epoch: 5| Step: 9
Training loss: 2.739540757306524
Validation loss: 2.461530535254188

Epoch: 5| Step: 10
Training loss: 2.054618109049865
Validation loss: 2.453645829068689

Epoch: 5| Step: 11
Training loss: 2.431261147912364
Validation loss: 2.4590127997337374

Epoch: 128| Step: 0
Training loss: 2.788944017474173
Validation loss: 2.4551915739639516

Epoch: 5| Step: 1
Training loss: 2.3171981307111906
Validation loss: 2.4564845574150644

Epoch: 5| Step: 2
Training loss: 2.153342235220036
Validation loss: 2.453636403647009

Epoch: 5| Step: 3
Training loss: 2.6461776048851027
Validation loss: 2.4510110911288674

Epoch: 5| Step: 4
Training loss: 2.621689934729331
Validation loss: 2.455578583314015

Epoch: 5| Step: 5
Training loss: 2.5956796109732543
Validation loss: 2.450249989724202

Epoch: 5| Step: 6
Training loss: 2.589565624648105
Validation loss: 2.456113367915426

Epoch: 5| Step: 7
Training loss: 2.968068576709407
Validation loss: 2.453760498084012

Epoch: 5| Step: 8
Training loss: 2.408302620644623
Validation loss: 2.4520892773288536

Epoch: 5| Step: 9
Training loss: 2.6543565172199104
Validation loss: 2.454783112112476

Epoch: 5| Step: 10
Training loss: 2.259172815164182
Validation loss: 2.443239371211277

Epoch: 5| Step: 11
Training loss: 1.7323525123515655
Validation loss: 2.456286262328655

Epoch: 129| Step: 0
Training loss: 2.6438784448611043
Validation loss: 2.4556449290472018

Epoch: 5| Step: 1
Training loss: 1.737722242673404
Validation loss: 2.4574354048589786

Epoch: 5| Step: 2
Training loss: 2.6688140825711937
Validation loss: 2.4604867027795234

Epoch: 5| Step: 3
Training loss: 1.7921345417742254
Validation loss: 2.4604681102231583

Epoch: 5| Step: 4
Training loss: 2.355404778619288
Validation loss: 2.457892165496519

Epoch: 5| Step: 5
Training loss: 2.7013037394942043
Validation loss: 2.4625105054021295

Epoch: 5| Step: 6
Training loss: 2.702013967109458
Validation loss: 2.4623147451036442

Epoch: 5| Step: 7
Training loss: 2.7365490798995764
Validation loss: 2.4623578569532727

Epoch: 5| Step: 8
Training loss: 2.6968478487665877
Validation loss: 2.461146774678228

Epoch: 5| Step: 9
Training loss: 2.803775819820272
Validation loss: 2.4560131537159493

Epoch: 5| Step: 10
Training loss: 2.935069275644449
Validation loss: 2.452571093636747

Epoch: 5| Step: 11
Training loss: 2.452902619797009
Validation loss: 2.460149743887313

Epoch: 130| Step: 0
Training loss: 2.631444830488479
Validation loss: 2.4588055534273394

Epoch: 5| Step: 1
Training loss: 2.1042643483468013
Validation loss: 2.461140292253953

Epoch: 5| Step: 2
Training loss: 2.323669676477459
Validation loss: 2.4527346265994927

Epoch: 5| Step: 3
Training loss: 1.967054211961793
Validation loss: 2.462365070428794

Epoch: 5| Step: 4
Training loss: 2.5625854105787833
Validation loss: 2.4561480788163697

Epoch: 5| Step: 5
Training loss: 2.9139178447256446
Validation loss: 2.45334947777918

Epoch: 5| Step: 6
Training loss: 2.9214438053950404
Validation loss: 2.45914499709138

Epoch: 5| Step: 7
Training loss: 2.497735428353061
Validation loss: 2.4636872768046683

Epoch: 5| Step: 8
Training loss: 2.6466791398005114
Validation loss: 2.458151941288396

Epoch: 5| Step: 9
Training loss: 2.3343588301214777
Validation loss: 2.456904361038005

Epoch: 5| Step: 10
Training loss: 2.7034394186099298
Validation loss: 2.4606940053136657

Epoch: 5| Step: 11
Training loss: 3.2912233915314633
Validation loss: 2.459662350287861

Epoch: 131| Step: 0
Training loss: 2.575198645475659
Validation loss: 2.4557166370599437

Epoch: 5| Step: 1
Training loss: 2.461667101047954
Validation loss: 2.4548074172959993

Epoch: 5| Step: 2
Training loss: 2.325187724748749
Validation loss: 2.456954421159773

Epoch: 5| Step: 3
Training loss: 2.58136060722513
Validation loss: 2.453942124328098

Epoch: 5| Step: 4
Training loss: 2.651770730685003
Validation loss: 2.4512017155790153

Epoch: 5| Step: 5
Training loss: 2.5181308373416478
Validation loss: 2.448986519040571

Epoch: 5| Step: 6
Training loss: 2.6009983676845145
Validation loss: 2.448773531726857

Epoch: 5| Step: 7
Training loss: 2.643270397647688
Validation loss: 2.4449920200537516

Epoch: 5| Step: 8
Training loss: 2.325110410304203
Validation loss: 2.4470296991098675

Epoch: 5| Step: 9
Training loss: 2.379872393673337
Validation loss: 2.451663458621297

Epoch: 5| Step: 10
Training loss: 2.4858419058286723
Validation loss: 2.448855923310139

Epoch: 5| Step: 11
Training loss: 3.3616480035095155
Validation loss: 2.4497866258672896

Epoch: 132| Step: 0
Training loss: 2.7049777346274633
Validation loss: 2.454011578724797

Epoch: 5| Step: 1
Training loss: 2.580179401636426
Validation loss: 2.4567135523079333

Epoch: 5| Step: 2
Training loss: 2.727658901183811
Validation loss: 2.4692606538665887

Epoch: 5| Step: 3
Training loss: 2.647695431795043
Validation loss: 2.4631643322186485

Epoch: 5| Step: 4
Training loss: 3.1762180004587197
Validation loss: 2.475005252909625

Epoch: 5| Step: 5
Training loss: 2.6583430402652257
Validation loss: 2.4660361096720567

Epoch: 5| Step: 6
Training loss: 2.2136604067800034
Validation loss: 2.455491689942402

Epoch: 5| Step: 7
Training loss: 2.401579746241497
Validation loss: 2.4472759071046823

Epoch: 5| Step: 8
Training loss: 2.489018257739308
Validation loss: 2.4482344211474967

Epoch: 5| Step: 9
Training loss: 1.8851232001723073
Validation loss: 2.4395992987610113

Epoch: 5| Step: 10
Training loss: 1.902321065340556
Validation loss: 2.451887636686134

Epoch: 5| Step: 11
Training loss: 3.858215281340004
Validation loss: 2.4551575616747954

Epoch: 133| Step: 0
Training loss: 2.033490046210182
Validation loss: 2.4569838680516107

Epoch: 5| Step: 1
Training loss: 2.86167306741196
Validation loss: 2.4486905044940586

Epoch: 5| Step: 2
Training loss: 2.7936404827593013
Validation loss: 2.4505190442220957

Epoch: 5| Step: 3
Training loss: 2.4289031403264914
Validation loss: 2.4557251888066283

Epoch: 5| Step: 4
Training loss: 2.6577366875853428
Validation loss: 2.454823155239354

Epoch: 5| Step: 5
Training loss: 2.5750032887854455
Validation loss: 2.4604565468338544

Epoch: 5| Step: 6
Training loss: 2.460619224675275
Validation loss: 2.4509096083844923

Epoch: 5| Step: 7
Training loss: 2.9149929330520945
Validation loss: 2.457314342045407

Epoch: 5| Step: 8
Training loss: 2.5630059091750663
Validation loss: 2.45811400943785

Epoch: 5| Step: 9
Training loss: 2.680712267610746
Validation loss: 2.460236266809373

Epoch: 5| Step: 10
Training loss: 2.271152042340246
Validation loss: 2.4544947013681226

Epoch: 5| Step: 11
Training loss: 1.9628373020198182
Validation loss: 2.4503007493355415

Epoch: 134| Step: 0
Training loss: 2.397039944420557
Validation loss: 2.4546562566778207

Epoch: 5| Step: 1
Training loss: 1.9617073402088168
Validation loss: 2.459197912016613

Epoch: 5| Step: 2
Training loss: 2.622399813444772
Validation loss: 2.457768562604816

Epoch: 5| Step: 3
Training loss: 2.8957675039003923
Validation loss: 2.4591303532835465

Epoch: 5| Step: 4
Training loss: 2.9776602895849114
Validation loss: 2.4620446837920733

Epoch: 5| Step: 5
Training loss: 2.2537373127424174
Validation loss: 2.4663862428769

Epoch: 5| Step: 6
Training loss: 2.5774697135677718
Validation loss: 2.463780931194425

Epoch: 5| Step: 7
Training loss: 2.3542730481803047
Validation loss: 2.4665778579597872

Epoch: 5| Step: 8
Training loss: 2.97617287476194
Validation loss: 2.4666070872652806

Epoch: 5| Step: 9
Training loss: 2.5233083402570893
Validation loss: 2.4636591439064106

Epoch: 5| Step: 10
Training loss: 2.4181336083237994
Validation loss: 2.4676007561498583

Epoch: 5| Step: 11
Training loss: 2.1822408398617235
Validation loss: 2.4681275947269294

Epoch: 135| Step: 0
Training loss: 2.2536483431976166
Validation loss: 2.4597008277242165

Epoch: 5| Step: 1
Training loss: 2.3515712192918707
Validation loss: 2.4690233674782824

Epoch: 5| Step: 2
Training loss: 2.0865091404955747
Validation loss: 2.459557248124287

Epoch: 5| Step: 3
Training loss: 2.976190498715355
Validation loss: 2.4562480792231405

Epoch: 5| Step: 4
Training loss: 2.6843446350156595
Validation loss: 2.4511420117670335

Epoch: 5| Step: 5
Training loss: 2.6591393407814126
Validation loss: 2.444497526169262

Epoch: 5| Step: 6
Training loss: 2.1488502799485043
Validation loss: 2.4493651156215086

Epoch: 5| Step: 7
Training loss: 2.945468240763874
Validation loss: 2.451344984205924

Epoch: 5| Step: 8
Training loss: 2.73638737333251
Validation loss: 2.453788574435052

Epoch: 5| Step: 9
Training loss: 2.61798520813006
Validation loss: 2.4508824434670458

Epoch: 5| Step: 10
Training loss: 2.4556123794072118
Validation loss: 2.453323550602164

Epoch: 5| Step: 11
Training loss: 2.114426505250527
Validation loss: 2.4478740864290205

Epoch: 136| Step: 0
Training loss: 2.4068582930142286
Validation loss: 2.4526269735195476

Epoch: 5| Step: 1
Training loss: 2.6108720334039335
Validation loss: 2.4550752804137033

Epoch: 5| Step: 2
Training loss: 2.473340559003751
Validation loss: 2.455130593470834

Epoch: 5| Step: 3
Training loss: 2.3343851806745453
Validation loss: 2.4557161516240447

Epoch: 5| Step: 4
Training loss: 2.7746911882285135
Validation loss: 2.455959338946859

Epoch: 5| Step: 5
Training loss: 2.451662247077006
Validation loss: 2.4488672453440614

Epoch: 5| Step: 6
Training loss: 2.688262653813096
Validation loss: 2.4505396743250105

Epoch: 5| Step: 7
Training loss: 2.9523288521513176
Validation loss: 2.448762635222274

Epoch: 5| Step: 8
Training loss: 2.665554400780312
Validation loss: 2.460601775667214

Epoch: 5| Step: 9
Training loss: 2.1480499472893557
Validation loss: 2.453380927729059

Epoch: 5| Step: 10
Training loss: 2.2753375002180847
Validation loss: 2.454119660866982

Epoch: 5| Step: 11
Training loss: 2.4911353302790773
Validation loss: 2.452660194632447

Epoch: 137| Step: 0
Training loss: 2.663111094519286
Validation loss: 2.455515375199364

Epoch: 5| Step: 1
Training loss: 2.415465801959488
Validation loss: 2.4566783520197637

Epoch: 5| Step: 2
Training loss: 2.7129979287465473
Validation loss: 2.4515221496204873

Epoch: 5| Step: 3
Training loss: 1.766650281986096
Validation loss: 2.451907647536471

Epoch: 5| Step: 4
Training loss: 2.5670157859801312
Validation loss: 2.453770769167072

Epoch: 5| Step: 5
Training loss: 2.9354399904651554
Validation loss: 2.454791521425644

Epoch: 5| Step: 6
Training loss: 2.167875820608247
Validation loss: 2.4508985511401957

Epoch: 5| Step: 7
Training loss: 2.4112447447692333
Validation loss: 2.450137009325468

Epoch: 5| Step: 8
Training loss: 2.57853159154772
Validation loss: 2.4564466887873073

Epoch: 5| Step: 9
Training loss: 2.3751985567314002
Validation loss: 2.4555082589253128

Epoch: 5| Step: 10
Training loss: 2.986750271626102
Validation loss: 2.4568374791883345

Epoch: 5| Step: 11
Training loss: 2.5827981845761308
Validation loss: 2.447340261625765

Epoch: 138| Step: 0
Training loss: 2.515148615304446
Validation loss: 2.452103529700825

Epoch: 5| Step: 1
Training loss: 2.471778074888898
Validation loss: 2.4446458242892577

Epoch: 5| Step: 2
Training loss: 2.594691369992099
Validation loss: 2.456748893617724

Epoch: 5| Step: 3
Training loss: 2.2366325870803734
Validation loss: 2.446315072353462

Epoch: 5| Step: 4
Training loss: 2.4820313345309044
Validation loss: 2.4447381763975273

Epoch: 5| Step: 5
Training loss: 2.7844210773941094
Validation loss: 2.459644046335478

Epoch: 5| Step: 6
Training loss: 2.618365213534769
Validation loss: 2.4603529523491408

Epoch: 5| Step: 7
Training loss: 2.7265444866995403
Validation loss: 2.4597996094348926

Epoch: 5| Step: 8
Training loss: 2.700045645292653
Validation loss: 2.4646696058400592

Epoch: 5| Step: 9
Training loss: 2.5464018399748776
Validation loss: 2.4541942428654395

Epoch: 5| Step: 10
Training loss: 2.5400861818606266
Validation loss: 2.456138804558187

Epoch: 5| Step: 11
Training loss: 1.1922362891296914
Validation loss: 2.4628168871996645

Epoch: 139| Step: 0
Training loss: 2.326773231856803
Validation loss: 2.457001367012247

Epoch: 5| Step: 1
Training loss: 2.724858754548347
Validation loss: 2.46250973891577

Epoch: 5| Step: 2
Training loss: 2.240116239324655
Validation loss: 2.460966402721726

Epoch: 5| Step: 3
Training loss: 2.1542580290652826
Validation loss: 2.450040420860522

Epoch: 5| Step: 4
Training loss: 2.681253937404916
Validation loss: 2.4513763504560173

Epoch: 5| Step: 5
Training loss: 2.817883319791858
Validation loss: 2.4539516599012616

Epoch: 5| Step: 6
Training loss: 2.5343420663897476
Validation loss: 2.4513099499805233

Epoch: 5| Step: 7
Training loss: 2.6229511621301067
Validation loss: 2.4413476758923984

Epoch: 5| Step: 8
Training loss: 2.5509490210606987
Validation loss: 2.454155379551664

Epoch: 5| Step: 9
Training loss: 2.249954011235194
Validation loss: 2.447360687155036

Epoch: 5| Step: 10
Training loss: 2.8015931467655437
Validation loss: 2.4517702909721484

Epoch: 5| Step: 11
Training loss: 2.3240968816526597
Validation loss: 2.455068073843909

Epoch: 140| Step: 0
Training loss: 2.892250882221397
Validation loss: 2.4580976984813248

Epoch: 5| Step: 1
Training loss: 2.472073310005738
Validation loss: 2.4610194278380595

Epoch: 5| Step: 2
Training loss: 2.8620297974448836
Validation loss: 2.44687497044381

Epoch: 5| Step: 3
Training loss: 2.6227181825890487
Validation loss: 2.4518822034629033

Epoch: 5| Step: 4
Training loss: 2.6697244299078378
Validation loss: 2.453300162053107

Epoch: 5| Step: 5
Training loss: 2.256668909974253
Validation loss: 2.4542970752771636

Epoch: 5| Step: 6
Training loss: 2.587214866075995
Validation loss: 2.44701124585785

Epoch: 5| Step: 7
Training loss: 2.496211041722902
Validation loss: 2.4548995247161063

Epoch: 5| Step: 8
Training loss: 2.334207257598035
Validation loss: 2.439651532092682

Epoch: 5| Step: 9
Training loss: 2.4243187173484704
Validation loss: 2.4440111347591027

Epoch: 5| Step: 10
Training loss: 2.1529017450332213
Validation loss: 2.446520408508346

Epoch: 5| Step: 11
Training loss: 2.1213405739504623
Validation loss: 2.4557406093833785

Epoch: 141| Step: 0
Training loss: 2.162135729676493
Validation loss: 2.460311252899416

Epoch: 5| Step: 1
Training loss: 2.321566259052737
Validation loss: 2.4596655894108808

Epoch: 5| Step: 2
Training loss: 2.7515860232160576
Validation loss: 2.4583813183083043

Epoch: 5| Step: 3
Training loss: 2.636966676711967
Validation loss: 2.461992043906092

Epoch: 5| Step: 4
Training loss: 2.8177625800156454
Validation loss: 2.4611743712086893

Epoch: 5| Step: 5
Training loss: 2.0462834326005876
Validation loss: 2.458958967793175

Epoch: 5| Step: 6
Training loss: 2.2432734288818414
Validation loss: 2.4624039413383425

Epoch: 5| Step: 7
Training loss: 2.67866002617496
Validation loss: 2.463641333361726

Epoch: 5| Step: 8
Training loss: 2.635225000058598
Validation loss: 2.465310058173467

Epoch: 5| Step: 9
Training loss: 3.1274864985649504
Validation loss: 2.460408520234199

Epoch: 5| Step: 10
Training loss: 2.3798443427594504
Validation loss: 2.454409091117879

Epoch: 5| Step: 11
Training loss: 1.6577157066916015
Validation loss: 2.4572626317276187

Epoch: 142| Step: 0
Training loss: 2.440555027387279
Validation loss: 2.4478617655231676

Epoch: 5| Step: 1
Training loss: 2.728381230752005
Validation loss: 2.4509013722013986

Epoch: 5| Step: 2
Training loss: 2.2560543620073243
Validation loss: 2.4459184539730514

Epoch: 5| Step: 3
Training loss: 1.904511706631976
Validation loss: 2.4505051799170166

Epoch: 5| Step: 4
Training loss: 2.56960864415906
Validation loss: 2.447009737682493

Epoch: 5| Step: 5
Training loss: 2.8130564668956795
Validation loss: 2.454117968831171

Epoch: 5| Step: 6
Training loss: 2.5988219893678512
Validation loss: 2.4448359332785863

Epoch: 5| Step: 7
Training loss: 2.689968838138965
Validation loss: 2.4488071174834065

Epoch: 5| Step: 8
Training loss: 1.965459101579188
Validation loss: 2.455573287714526

Epoch: 5| Step: 9
Training loss: 3.19862399319287
Validation loss: 2.4504714471676134

Epoch: 5| Step: 10
Training loss: 2.403358803277713
Validation loss: 2.452340423753485

Epoch: 5| Step: 11
Training loss: 2.2167443895218417
Validation loss: 2.453567171438499

Epoch: 143| Step: 0
Training loss: 2.8239404902685403
Validation loss: 2.438076370922534

Epoch: 5| Step: 1
Training loss: 2.4755994205747225
Validation loss: 2.4447763767314217

Epoch: 5| Step: 2
Training loss: 2.663468191132716
Validation loss: 2.4483170094042164

Epoch: 5| Step: 3
Training loss: 2.3949011163864964
Validation loss: 2.456629968491584

Epoch: 5| Step: 4
Training loss: 2.2148076883707226
Validation loss: 2.4559207624421746

Epoch: 5| Step: 5
Training loss: 2.973162774581182
Validation loss: 2.4630447408045963

Epoch: 5| Step: 6
Training loss: 2.5200121043686576
Validation loss: 2.467504001808722

Epoch: 5| Step: 7
Training loss: 2.300724048259642
Validation loss: 2.466608374031431

Epoch: 5| Step: 8
Training loss: 2.4798275576905238
Validation loss: 2.4706332431806564

Epoch: 5| Step: 9
Training loss: 2.46242998672331
Validation loss: 2.4663872337151966

Epoch: 5| Step: 10
Training loss: 2.6085965942842573
Validation loss: 2.4661364986534386

Epoch: 5| Step: 11
Training loss: 1.7844991664141683
Validation loss: 2.4610753056124834

Epoch: 144| Step: 0
Training loss: 2.960861970043834
Validation loss: 2.4550530374965382

Epoch: 5| Step: 1
Training loss: 2.3418789325046836
Validation loss: 2.4522574079094883

Epoch: 5| Step: 2
Training loss: 2.5671877897590942
Validation loss: 2.450657723495422

Epoch: 5| Step: 3
Training loss: 2.985846511197073
Validation loss: 2.4483704829421757

Epoch: 5| Step: 4
Training loss: 2.2702399245526803
Validation loss: 2.452513021116334

Epoch: 5| Step: 5
Training loss: 2.266969268832844
Validation loss: 2.4466750113979856

Epoch: 5| Step: 6
Training loss: 2.495541793616962
Validation loss: 2.4521127341448956

Epoch: 5| Step: 7
Training loss: 2.4335003830341377
Validation loss: 2.4433895793280835

Epoch: 5| Step: 8
Training loss: 1.747731850651122
Validation loss: 2.443412048357553

Epoch: 5| Step: 9
Training loss: 2.882160392136596
Validation loss: 2.441742489866709

Epoch: 5| Step: 10
Training loss: 2.5425204630490574
Validation loss: 2.4466971355896217

Epoch: 5| Step: 11
Training loss: 2.121378673996019
Validation loss: 2.4480798274646194

Epoch: 145| Step: 0
Training loss: 2.3824845885732016
Validation loss: 2.447225153761672

Epoch: 5| Step: 1
Training loss: 2.7199208161103963
Validation loss: 2.470863467756966

Epoch: 5| Step: 2
Training loss: 2.7082919973495683
Validation loss: 2.4856454431922335

Epoch: 5| Step: 3
Training loss: 2.7719270273377066
Validation loss: 2.5073058467242664

Epoch: 5| Step: 4
Training loss: 2.0503629130610874
Validation loss: 2.4870906918004043

Epoch: 5| Step: 5
Training loss: 2.6181886494672644
Validation loss: 2.471131837663683

Epoch: 5| Step: 6
Training loss: 2.828755176790619
Validation loss: 2.470854755317325

Epoch: 5| Step: 7
Training loss: 2.7509999624603836
Validation loss: 2.450324786806967

Epoch: 5| Step: 8
Training loss: 2.509028725111097
Validation loss: 2.450625196795953

Epoch: 5| Step: 9
Training loss: 2.3344478897092706
Validation loss: 2.454083496383473

Epoch: 5| Step: 10
Training loss: 2.3411077228254675
Validation loss: 2.454828484831141

Epoch: 5| Step: 11
Training loss: 1.902797323746851
Validation loss: 2.4566547406535566

Epoch: 146| Step: 0
Training loss: 1.9907790048094214
Validation loss: 2.468307425267262

Epoch: 5| Step: 1
Training loss: 2.104296865935934
Validation loss: 2.4677081102591454

Epoch: 5| Step: 2
Training loss: 2.9715476255880025
Validation loss: 2.46394128898423

Epoch: 5| Step: 3
Training loss: 2.6958552159674642
Validation loss: 2.4694446640660583

Epoch: 5| Step: 4
Training loss: 3.1255675754104297
Validation loss: 2.4744976553403197

Epoch: 5| Step: 5
Training loss: 2.3999130233263144
Validation loss: 2.4746519876417614

Epoch: 5| Step: 6
Training loss: 2.4957585594766427
Validation loss: 2.4674335623586154

Epoch: 5| Step: 7
Training loss: 2.658346358678909
Validation loss: 2.4688458967295013

Epoch: 5| Step: 8
Training loss: 2.4933660225926775
Validation loss: 2.4699075315035754

Epoch: 5| Step: 9
Training loss: 1.9303316593315514
Validation loss: 2.462209669415932

Epoch: 5| Step: 10
Training loss: 3.1158156662799152
Validation loss: 2.469641367633356

Epoch: 5| Step: 11
Training loss: 0.9988835480149648
Validation loss: 2.4632530460019426

Epoch: 147| Step: 0
Training loss: 2.4773792159646457
Validation loss: 2.4578326666429624

Epoch: 5| Step: 1
Training loss: 2.843059749462482
Validation loss: 2.4584579328437277

Epoch: 5| Step: 2
Training loss: 1.8958490252282063
Validation loss: 2.460758880914678

Epoch: 5| Step: 3
Training loss: 2.577465551018483
Validation loss: 2.455458695264364

Epoch: 5| Step: 4
Training loss: 2.3316099978094034
Validation loss: 2.4619771507216734

Epoch: 5| Step: 5
Training loss: 2.445387866531512
Validation loss: 2.4484251197708056

Epoch: 5| Step: 6
Training loss: 2.6229404362537494
Validation loss: 2.453097531596158

Epoch: 5| Step: 7
Training loss: 2.6405556573735574
Validation loss: 2.456613591068102

Epoch: 5| Step: 8
Training loss: 3.0591621114443543
Validation loss: 2.4549933524860785

Epoch: 5| Step: 9
Training loss: 2.580109173734451
Validation loss: 2.45598028334455

Epoch: 5| Step: 10
Training loss: 2.4306775004964516
Validation loss: 2.464408457225227

Epoch: 5| Step: 11
Training loss: 1.3614621855786595
Validation loss: 2.4617089803121073

Epoch: 148| Step: 0
Training loss: 2.1173854608645
Validation loss: 2.473603178264195

Epoch: 5| Step: 1
Training loss: 2.4361896416207394
Validation loss: 2.4661402489109854

Epoch: 5| Step: 2
Training loss: 2.906451310599617
Validation loss: 2.4601376480008192

Epoch: 5| Step: 3
Training loss: 2.224168420616803
Validation loss: 2.4638488220506094

Epoch: 5| Step: 4
Training loss: 2.5871679599769135
Validation loss: 2.459463055240829

Epoch: 5| Step: 5
Training loss: 2.6661086293301466
Validation loss: 2.4537413829050383

Epoch: 5| Step: 6
Training loss: 2.067655304838126
Validation loss: 2.461382487599331

Epoch: 5| Step: 7
Training loss: 3.0466886023212414
Validation loss: 2.45282618409596

Epoch: 5| Step: 8
Training loss: 2.9069815094487366
Validation loss: 2.4526515025876066

Epoch: 5| Step: 9
Training loss: 2.771885827354091
Validation loss: 2.4561238273459773

Epoch: 5| Step: 10
Training loss: 1.7454012664652199
Validation loss: 2.457667807563728

Epoch: 5| Step: 11
Training loss: 1.5197570162355616
Validation loss: 2.461029501108574

Epoch: 149| Step: 0
Training loss: 2.6771765864615014
Validation loss: 2.4607803415646425

Epoch: 5| Step: 1
Training loss: 2.0673329919894536
Validation loss: 2.44921172494403

Epoch: 5| Step: 2
Training loss: 2.1363877732261836
Validation loss: 2.457113198923664

Epoch: 5| Step: 3
Training loss: 2.141707432083078
Validation loss: 2.4559842776449963

Epoch: 5| Step: 4
Training loss: 2.6287761049520824
Validation loss: 2.4543379601336484

Epoch: 5| Step: 5
Training loss: 2.5906232808547993
Validation loss: 2.4561839500069036

Epoch: 5| Step: 6
Training loss: 2.970213519297851
Validation loss: 2.4546774914377822

Epoch: 5| Step: 7
Training loss: 2.4359521108094935
Validation loss: 2.456334058117597

Epoch: 5| Step: 8
Training loss: 2.6624227270813785
Validation loss: 2.4501231874584732

Epoch: 5| Step: 9
Training loss: 2.438461211999891
Validation loss: 2.4542766508080396

Epoch: 5| Step: 10
Training loss: 2.483408902019095
Validation loss: 2.456164315761908

Epoch: 5| Step: 11
Training loss: 2.9805617011738565
Validation loss: 2.44929013105145

Epoch: 150| Step: 0
Training loss: 2.250177694297565
Validation loss: 2.453496317693107

Epoch: 5| Step: 1
Training loss: 2.723002298220345
Validation loss: 2.4602053728930287

Epoch: 5| Step: 2
Training loss: 2.2194415881674727
Validation loss: 2.465645266321887

Epoch: 5| Step: 3
Training loss: 3.0013500990374626
Validation loss: 2.462947807485145

Epoch: 5| Step: 4
Training loss: 2.5522648234288607
Validation loss: 2.45442524040551

Epoch: 5| Step: 5
Training loss: 2.531092980306586
Validation loss: 2.4611042188251777

Epoch: 5| Step: 6
Training loss: 2.7236069889697725
Validation loss: 2.461886550929164

Epoch: 5| Step: 7
Training loss: 2.4201543787115884
Validation loss: 2.4580369800347084

Epoch: 5| Step: 8
Training loss: 2.727243507835612
Validation loss: 2.4527167731780124

Epoch: 5| Step: 9
Training loss: 2.0593651732237808
Validation loss: 2.4543513899379037

Epoch: 5| Step: 10
Training loss: 2.4091732549252898
Validation loss: 2.461331823334415

Epoch: 5| Step: 11
Training loss: 1.6055835395113682
Validation loss: 2.4612504709636287

Epoch: 151| Step: 0
Training loss: 3.1469471624864385
Validation loss: 2.449198871310824

Epoch: 5| Step: 1
Training loss: 2.459285314686607
Validation loss: 2.4550611343139797

Epoch: 5| Step: 2
Training loss: 2.363842706743433
Validation loss: 2.457519688900905

Epoch: 5| Step: 3
Training loss: 2.261530682217539
Validation loss: 2.4588302592288453

Epoch: 5| Step: 4
Training loss: 2.244145512056798
Validation loss: 2.458855946534869

Epoch: 5| Step: 5
Training loss: 2.0879456643967402
Validation loss: 2.457753522569097

Epoch: 5| Step: 6
Training loss: 2.9019234559620446
Validation loss: 2.452926255130662

Epoch: 5| Step: 7
Training loss: 2.3724140343803612
Validation loss: 2.4546925462735256

Epoch: 5| Step: 8
Training loss: 2.4175397896081003
Validation loss: 2.4555207073343417

Epoch: 5| Step: 9
Training loss: 2.407193878137646
Validation loss: 2.4534188882169348

Epoch: 5| Step: 10
Training loss: 2.750435534580444
Validation loss: 2.4572817215361815

Epoch: 5| Step: 11
Training loss: 1.7969146060725099
Validation loss: 2.4556816773392307

Epoch: 152| Step: 0
Training loss: 2.4150489347250708
Validation loss: 2.4636679866307065

Epoch: 5| Step: 1
Training loss: 2.6657103870306678
Validation loss: 2.4587615712340734

Epoch: 5| Step: 2
Training loss: 2.538644327794024
Validation loss: 2.456165802137784

Epoch: 5| Step: 3
Training loss: 2.114933855542029
Validation loss: 2.4465740268845186

Epoch: 5| Step: 4
Training loss: 2.912505278357442
Validation loss: 2.453917984649508

Epoch: 5| Step: 5
Training loss: 2.557122519519545
Validation loss: 2.456349118977358

Epoch: 5| Step: 6
Training loss: 2.62284208337632
Validation loss: 2.4632513441082597

Epoch: 5| Step: 7
Training loss: 2.4694137646667254
Validation loss: 2.4584716351178564

Epoch: 5| Step: 8
Training loss: 2.3973019176195582
Validation loss: 2.460451835060129

Epoch: 5| Step: 9
Training loss: 2.2516730763842734
Validation loss: 2.4550068980759088

Epoch: 5| Step: 10
Training loss: 2.5168158514339694
Validation loss: 2.4639208920462448

Epoch: 5| Step: 11
Training loss: 1.5384029776174897
Validation loss: 2.4686337495949107

Epoch: 153| Step: 0
Training loss: 2.5588585636456496
Validation loss: 2.4621133124729395

Epoch: 5| Step: 1
Training loss: 2.376146240698126
Validation loss: 2.458553871402704

Epoch: 5| Step: 2
Training loss: 2.6729057955997773
Validation loss: 2.4558790908846455

Epoch: 5| Step: 3
Training loss: 2.2948957985553085
Validation loss: 2.4589668376318436

Epoch: 5| Step: 4
Training loss: 2.1398203653183834
Validation loss: 2.4557941599417132

Epoch: 5| Step: 5
Training loss: 2.3669517072320883
Validation loss: 2.459524427137538

Epoch: 5| Step: 6
Training loss: 1.9101334708948194
Validation loss: 2.464616699562568

Epoch: 5| Step: 7
Training loss: 2.1664593915340338
Validation loss: 2.4621177749454186

Epoch: 5| Step: 8
Training loss: 2.9373152654469914
Validation loss: 2.4596742809019436

Epoch: 5| Step: 9
Training loss: 3.1341703521157784
Validation loss: 2.45799899791157

Epoch: 5| Step: 10
Training loss: 2.7989932941221514
Validation loss: 2.466665756272672

Epoch: 5| Step: 11
Training loss: 1.741839044731592
Validation loss: 2.461603288556735

Epoch: 154| Step: 0
Training loss: 1.8825126187533578
Validation loss: 2.46030162689844

Epoch: 5| Step: 1
Training loss: 2.713889326106175
Validation loss: 2.450239501165435

Epoch: 5| Step: 2
Training loss: 2.460285596939075
Validation loss: 2.4683801438333757

Epoch: 5| Step: 3
Training loss: 2.1413948178464564
Validation loss: 2.466957308560137

Epoch: 5| Step: 4
Training loss: 2.8876267838820797
Validation loss: 2.4581083151642837

Epoch: 5| Step: 5
Training loss: 2.6062185418222388
Validation loss: 2.4620791234518244

Epoch: 5| Step: 6
Training loss: 2.5821934820736274
Validation loss: 2.4697330465215317

Epoch: 5| Step: 7
Training loss: 2.1396993598815444
Validation loss: 2.4575843168355025

Epoch: 5| Step: 8
Training loss: 2.4084302264547097
Validation loss: 2.462638896568531

Epoch: 5| Step: 9
Training loss: 2.446810327603339
Validation loss: 2.4670428195992704

Epoch: 5| Step: 10
Training loss: 3.022463619495983
Validation loss: 2.461080962747629

Epoch: 5| Step: 11
Training loss: 1.5454670056437463
Validation loss: 2.4611719938102445

Epoch: 155| Step: 0
Training loss: 3.0428756680043003
Validation loss: 2.4624198647190356

Epoch: 5| Step: 1
Training loss: 2.180658923842188
Validation loss: 2.465646926273119

Epoch: 5| Step: 2
Training loss: 2.4200308392664107
Validation loss: 2.466602783943873

Epoch: 5| Step: 3
Training loss: 2.4964103677016936
Validation loss: 2.4700979906534015

Epoch: 5| Step: 4
Training loss: 2.5827188889096813
Validation loss: 2.4598505354525617

Epoch: 5| Step: 5
Training loss: 2.3396709295526477
Validation loss: 2.4557138336663042

Epoch: 5| Step: 6
Training loss: 2.483133929736089
Validation loss: 2.4647202861351754

Epoch: 5| Step: 7
Training loss: 2.435897104492371
Validation loss: 2.4600684432843765

Epoch: 5| Step: 8
Training loss: 1.8921249366705304
Validation loss: 2.458952730079745

Epoch: 5| Step: 9
Training loss: 2.6674487834614813
Validation loss: 2.45841161285383

Epoch: 5| Step: 10
Training loss: 2.642045220368525
Validation loss: 2.4615823254140627

Epoch: 5| Step: 11
Training loss: 2.524690110481122
Validation loss: 2.457294084149689

Epoch: 156| Step: 0
Training loss: 2.750449664025793
Validation loss: 2.4587258709808157

Epoch: 5| Step: 1
Training loss: 2.1510007791273904
Validation loss: 2.4580919314249594

Epoch: 5| Step: 2
Training loss: 2.0664505548180467
Validation loss: 2.4559823098208287

Epoch: 5| Step: 3
Training loss: 2.3433566971433066
Validation loss: 2.4584290814476812

Epoch: 5| Step: 4
Training loss: 2.9850699529326556
Validation loss: 2.452319189011248

Epoch: 5| Step: 5
Training loss: 2.472536490916479
Validation loss: 2.4563948914531974

Epoch: 5| Step: 6
Training loss: 2.3707382714957834
Validation loss: 2.4712351069229137

Epoch: 5| Step: 7
Training loss: 2.2724365967486695
Validation loss: 2.4653595688434677

Epoch: 5| Step: 8
Training loss: 2.4358691114768605
Validation loss: 2.4600961003683652

Epoch: 5| Step: 9
Training loss: 1.810428487946166
Validation loss: 2.4615593825843516

Epoch: 5| Step: 10
Training loss: 3.3974610778510317
Validation loss: 2.4621484915460328

Epoch: 5| Step: 11
Training loss: 1.2536568557049728
Validation loss: 2.461147061260725

Epoch: 157| Step: 0
Training loss: 2.7518243806936837
Validation loss: 2.466574313769678

Epoch: 5| Step: 1
Training loss: 2.3580854314805144
Validation loss: 2.459645783035838

Epoch: 5| Step: 2
Training loss: 2.6164345640357336
Validation loss: 2.465922184718814

Epoch: 5| Step: 3
Training loss: 2.657448711313029
Validation loss: 2.4651682036111375

Epoch: 5| Step: 4
Training loss: 2.5460690586406307
Validation loss: 2.4673625370979093

Epoch: 5| Step: 5
Training loss: 2.7248933158779374
Validation loss: 2.4685997534957624

Epoch: 5| Step: 6
Training loss: 3.255065345166845
Validation loss: 2.4569759756884726

Epoch: 5| Step: 7
Training loss: 2.031326879366938
Validation loss: 2.4578751336370757

Epoch: 5| Step: 8
Training loss: 2.240844537043295
Validation loss: 2.4547719549571334

Epoch: 5| Step: 9
Training loss: 2.6406317716432683
Validation loss: 2.455704942073386

Epoch: 5| Step: 10
Training loss: 1.6184142246845006
Validation loss: 2.4539860815749677

Epoch: 5| Step: 11
Training loss: 2.186637817498767
Validation loss: 2.45433084044571

Epoch: 158| Step: 0
Training loss: 2.28316375724588
Validation loss: 2.4498197009929683

Epoch: 5| Step: 1
Training loss: 2.5182812805989987
Validation loss: 2.465392690914519

Epoch: 5| Step: 2
Training loss: 2.5890993455234566
Validation loss: 2.464238309585269

Epoch: 5| Step: 3
Training loss: 2.859716009432064
Validation loss: 2.4727423453546282

Epoch: 5| Step: 4
Training loss: 2.5333467441337723
Validation loss: 2.472088122272938

Epoch: 5| Step: 5
Training loss: 2.2091607277223275
Validation loss: 2.469797829489311

Epoch: 5| Step: 6
Training loss: 2.322537674942191
Validation loss: 2.47469271283926

Epoch: 5| Step: 7
Training loss: 2.4577136039844443
Validation loss: 2.480690803500836

Epoch: 5| Step: 8
Training loss: 2.3843325909884014
Validation loss: 2.484971464678942

Epoch: 5| Step: 9
Training loss: 2.289550520916625
Validation loss: 2.483912434890693

Epoch: 5| Step: 10
Training loss: 3.1325739819679344
Validation loss: 2.4905110204279617

Epoch: 5| Step: 11
Training loss: 1.1284669859675023
Validation loss: 2.4782416290187492

Epoch: 159| Step: 0
Training loss: 2.3793223350714636
Validation loss: 2.4667748062382944

Epoch: 5| Step: 1
Training loss: 2.612551006932585
Validation loss: 2.4686770448488007

Epoch: 5| Step: 2
Training loss: 2.949294739811583
Validation loss: 2.4713705857551846

Epoch: 5| Step: 3
Training loss: 2.3654849536314817
Validation loss: 2.4743069868190397

Epoch: 5| Step: 4
Training loss: 2.5824815412306665
Validation loss: 2.464368940709226

Epoch: 5| Step: 5
Training loss: 2.718223520801648
Validation loss: 2.4631022221995362

Epoch: 5| Step: 6
Training loss: 2.3376137323301234
Validation loss: 2.459280249237912

Epoch: 5| Step: 7
Training loss: 2.6581311913370427
Validation loss: 2.4649217676872794

Epoch: 5| Step: 8
Training loss: 2.032573679471839
Validation loss: 2.4642338872347103

Epoch: 5| Step: 9
Training loss: 2.265252444449616
Validation loss: 2.4497729926137364

Epoch: 5| Step: 10
Training loss: 2.3566997594636665
Validation loss: 2.459867594019259

Epoch: 5| Step: 11
Training loss: 2.486482122526138
Validation loss: 2.455361854912006

Epoch: 160| Step: 0
Training loss: 2.5924554374786273
Validation loss: 2.468421910210118

Epoch: 5| Step: 1
Training loss: 2.9409411139846533
Validation loss: 2.4648698824521422

Epoch: 5| Step: 2
Training loss: 2.5968907655313127
Validation loss: 2.4587725082722174

Epoch: 5| Step: 3
Training loss: 2.0067448133584804
Validation loss: 2.4558427257852276

Epoch: 5| Step: 4
Training loss: 2.43322751150436
Validation loss: 2.454750895030613

Epoch: 5| Step: 5
Training loss: 2.55016156975001
Validation loss: 2.4559686745559968

Epoch: 5| Step: 6
Training loss: 2.196399134705609
Validation loss: 2.4611868272859225

Epoch: 5| Step: 7
Training loss: 2.7428750591743105
Validation loss: 2.4675569226721494

Epoch: 5| Step: 8
Training loss: 2.315700481384072
Validation loss: 2.4640070104065783

Epoch: 5| Step: 9
Training loss: 2.656407789143851
Validation loss: 2.460583846097134

Epoch: 5| Step: 10
Training loss: 2.208329242726502
Validation loss: 2.461178686039728

Epoch: 5| Step: 11
Training loss: 2.9836456845686152
Validation loss: 2.455568627253886

Epoch: 161| Step: 0
Training loss: 2.9125797702850864
Validation loss: 2.470669680081297

Epoch: 5| Step: 1
Training loss: 2.3396293529175094
Validation loss: 2.465009535641439

Epoch: 5| Step: 2
Training loss: 2.62670915456318
Validation loss: 2.4801791525166457

Epoch: 5| Step: 3
Training loss: 2.3396084624175413
Validation loss: 2.467838682502986

Epoch: 5| Step: 4
Training loss: 1.6317296430119517
Validation loss: 2.4655617777942536

Epoch: 5| Step: 5
Training loss: 2.3746406885697025
Validation loss: 2.466902250615073

Epoch: 5| Step: 6
Training loss: 2.8386446219622
Validation loss: 2.461267923445154

Epoch: 5| Step: 7
Training loss: 2.0940906759921702
Validation loss: 2.4589133157681307

Epoch: 5| Step: 8
Training loss: 2.224086522525636
Validation loss: 2.47651317719804

Epoch: 5| Step: 9
Training loss: 2.699344527188933
Validation loss: 2.473458174624923

Epoch: 5| Step: 10
Training loss: 2.5119169403242396
Validation loss: 2.475189973864601

Epoch: 5| Step: 11
Training loss: 3.510613833058726
Validation loss: 2.47790914319388

Epoch: 162| Step: 0
Training loss: 1.7598409520157217
Validation loss: 2.478016972762832

Epoch: 5| Step: 1
Training loss: 2.5321251569010528
Validation loss: 2.466682038764051

Epoch: 5| Step: 2
Training loss: 3.128034720323266
Validation loss: 2.47891005756107

Epoch: 5| Step: 3
Training loss: 2.384050391663107
Validation loss: 2.480924633515064

Epoch: 5| Step: 4
Training loss: 2.6123502295971757
Validation loss: 2.4826652273516405

Epoch: 5| Step: 5
Training loss: 2.01251524925378
Validation loss: 2.4830942151350257

Epoch: 5| Step: 6
Training loss: 2.154567559702812
Validation loss: 2.4770637269737357

Epoch: 5| Step: 7
Training loss: 3.038330617608053
Validation loss: 2.4657955679580428

Epoch: 5| Step: 8
Training loss: 2.766548735238449
Validation loss: 2.4749950860035725

Epoch: 5| Step: 9
Training loss: 2.287303886970443
Validation loss: 2.4674326766188464

Epoch: 5| Step: 10
Training loss: 2.0600542537248367
Validation loss: 2.464933491487306

Epoch: 5| Step: 11
Training loss: 2.760334450319093
Validation loss: 2.4678190141823504

Epoch: 163| Step: 0
Training loss: 2.790524054364312
Validation loss: 2.4755936340974825

Epoch: 5| Step: 1
Training loss: 2.7625603371499494
Validation loss: 2.4630495403830066

Epoch: 5| Step: 2
Training loss: 1.9118967064644405
Validation loss: 2.4678982540961347

Epoch: 5| Step: 3
Training loss: 2.768716582016954
Validation loss: 2.4709950592501584

Epoch: 5| Step: 4
Training loss: 2.5514732929247086
Validation loss: 2.4606289099921503

Epoch: 5| Step: 5
Training loss: 2.7163021424261653
Validation loss: 2.4665265633664286

Epoch: 5| Step: 6
Training loss: 2.5931972236348946
Validation loss: 2.462106461391856

Epoch: 5| Step: 7
Training loss: 2.0862289396563525
Validation loss: 2.460877727610182

Epoch: 5| Step: 8
Training loss: 2.0509088966970888
Validation loss: 2.4670576318693733

Epoch: 5| Step: 9
Training loss: 2.4037522071673516
Validation loss: 2.461149037064499

Epoch: 5| Step: 10
Training loss: 2.202422767350215
Validation loss: 2.4656809953166055

Epoch: 5| Step: 11
Training loss: 2.717078736614365
Validation loss: 2.472376859367236

Epoch: 164| Step: 0
Training loss: 2.1901613667230952
Validation loss: 2.4829515181765625

Epoch: 5| Step: 1
Training loss: 2.8553665497699106
Validation loss: 2.5007429171429507

Epoch: 5| Step: 2
Training loss: 1.8903375832748643
Validation loss: 2.5066823284430524

Epoch: 5| Step: 3
Training loss: 2.3240664135673126
Validation loss: 2.5203605253118053

Epoch: 5| Step: 4
Training loss: 2.3413567085612677
Validation loss: 2.493705271397036

Epoch: 5| Step: 5
Training loss: 2.288873345693674
Validation loss: 2.486420549031835

Epoch: 5| Step: 6
Training loss: 2.05784115434652
Validation loss: 2.472927052562557

Epoch: 5| Step: 7
Training loss: 2.571506396887038
Validation loss: 2.4654536109925465

Epoch: 5| Step: 8
Training loss: 2.1958957318217456
Validation loss: 2.4647527558004065

Epoch: 5| Step: 9
Training loss: 2.753678808898101
Validation loss: 2.464552211699338

Epoch: 5| Step: 10
Training loss: 3.4884656492151955
Validation loss: 2.456465566557243

Epoch: 5| Step: 11
Training loss: 2.374407443361877
Validation loss: 2.45948269344501

Epoch: 165| Step: 0
Training loss: 2.7678399995193823
Validation loss: 2.4611874024597094

Epoch: 5| Step: 1
Training loss: 2.368920223994241
Validation loss: 2.466166492497815

Epoch: 5| Step: 2
Training loss: 2.1701299668870417
Validation loss: 2.469988592709649

Epoch: 5| Step: 3
Training loss: 2.422998081806351
Validation loss: 2.4822347366863164

Epoch: 5| Step: 4
Training loss: 2.7849405213173566
Validation loss: 2.46382839612327

Epoch: 5| Step: 5
Training loss: 3.0152061052871395
Validation loss: 2.478714073499189

Epoch: 5| Step: 6
Training loss: 2.5183681908582316
Validation loss: 2.4814815265885892

Epoch: 5| Step: 7
Training loss: 2.0526491214604894
Validation loss: 2.47677039428079

Epoch: 5| Step: 8
Training loss: 2.25239414419376
Validation loss: 2.477032621830818

Epoch: 5| Step: 9
Training loss: 2.5600699090352883
Validation loss: 2.4830454101532573

Epoch: 5| Step: 10
Training loss: 2.173043574732349
Validation loss: 2.475290988905224

Epoch: 5| Step: 11
Training loss: 2.685990641602484
Validation loss: 2.471420654324357

Epoch: 166| Step: 0
Training loss: 1.986422346162871
Validation loss: 2.4739655819167643

Epoch: 5| Step: 1
Training loss: 2.2388952454991284
Validation loss: 2.48469574175388

Epoch: 5| Step: 2
Training loss: 2.0953338098544676
Validation loss: 2.487037335792377

Epoch: 5| Step: 3
Training loss: 2.6773401772948606
Validation loss: 2.499722838932715

Epoch: 5| Step: 4
Training loss: 2.6607452950357597
Validation loss: 2.4878528531609683

Epoch: 5| Step: 5
Training loss: 2.2554139593411056
Validation loss: 2.4839225373017393

Epoch: 5| Step: 6
Training loss: 2.645466986897752
Validation loss: 2.4918720599800515

Epoch: 5| Step: 7
Training loss: 2.430194961199302
Validation loss: 2.4804011658600107

Epoch: 5| Step: 8
Training loss: 2.09934284736381
Validation loss: 2.474121406936099

Epoch: 5| Step: 9
Training loss: 3.1954851546893988
Validation loss: 2.4746267734107654

Epoch: 5| Step: 10
Training loss: 2.6817921194917345
Validation loss: 2.47192842158138

Epoch: 5| Step: 11
Training loss: 2.5654788497204644
Validation loss: 2.463807768425899

Epoch: 167| Step: 0
Training loss: 2.2426993287634076
Validation loss: 2.463458631754655

Epoch: 5| Step: 1
Training loss: 2.5221119520008015
Validation loss: 2.4644135887226444

Epoch: 5| Step: 2
Training loss: 2.0998458941999596
Validation loss: 2.4766630198459123

Epoch: 5| Step: 3
Training loss: 2.590113745146933
Validation loss: 2.470267702517242

Epoch: 5| Step: 4
Training loss: 2.8151458163125493
Validation loss: 2.4759335646878315

Epoch: 5| Step: 5
Training loss: 2.7427844840456626
Validation loss: 2.4759861649543726

Epoch: 5| Step: 6
Training loss: 2.64156555038415
Validation loss: 2.4898111340250044

Epoch: 5| Step: 7
Training loss: 2.7819886673258383
Validation loss: 2.474664183192593

Epoch: 5| Step: 8
Training loss: 2.254370259829139
Validation loss: 2.471105526206521

Epoch: 5| Step: 9
Training loss: 2.2365780086920384
Validation loss: 2.4777663681335884

Epoch: 5| Step: 10
Training loss: 1.912769114191282
Validation loss: 2.469302127760254

Epoch: 5| Step: 11
Training loss: 3.204177162606287
Validation loss: 2.477678137917892

Epoch: 168| Step: 0
Training loss: 2.630816056447841
Validation loss: 2.470584783119796

Epoch: 5| Step: 1
Training loss: 2.6158138484984628
Validation loss: 2.4783322321802332

Epoch: 5| Step: 2
Training loss: 2.4980157607156306
Validation loss: 2.4668016713051215

Epoch: 5| Step: 3
Training loss: 2.075585556137927
Validation loss: 2.475700814154374

Epoch: 5| Step: 4
Training loss: 2.47817353517355
Validation loss: 2.4674590352931007

Epoch: 5| Step: 5
Training loss: 2.6339938176458357
Validation loss: 2.4690102910792597

Epoch: 5| Step: 6
Training loss: 2.523161692944282
Validation loss: 2.476921986061961

Epoch: 5| Step: 7
Training loss: 2.257784833375581
Validation loss: 2.477914587502906

Epoch: 5| Step: 8
Training loss: 2.2362601057477822
Validation loss: 2.4624134905379584

Epoch: 5| Step: 9
Training loss: 2.693808581075922
Validation loss: 2.47564649238236

Epoch: 5| Step: 10
Training loss: 2.2878993076695733
Validation loss: 2.476817714565508

Epoch: 5| Step: 11
Training loss: 2.969376829127104
Validation loss: 2.4736202624705794

Epoch: 169| Step: 0
Training loss: 2.630798021944917
Validation loss: 2.485299108485079

Epoch: 5| Step: 1
Training loss: 2.4631962174421544
Validation loss: 2.480421263072504

Epoch: 5| Step: 2
Training loss: 2.1983639832915403
Validation loss: 2.4795880213403483

Epoch: 5| Step: 3
Training loss: 2.400850554588421
Validation loss: 2.4923924328962346

Epoch: 5| Step: 4
Training loss: 2.2652242371841607
Validation loss: 2.4928224884709174

Epoch: 5| Step: 5
Training loss: 1.9746486761783437
Validation loss: 2.4909505056254586

Epoch: 5| Step: 6
Training loss: 2.467778751200007
Validation loss: 2.5006337673812515

Epoch: 5| Step: 7
Training loss: 2.1302694173571903
Validation loss: 2.478398762446185

Epoch: 5| Step: 8
Training loss: 2.350422326144085
Validation loss: 2.4810643179477685

Epoch: 5| Step: 9
Training loss: 2.8117423838528124
Validation loss: 2.4803051671832965

Epoch: 5| Step: 10
Training loss: 2.686279463289126
Validation loss: 2.4725612684410914

Epoch: 5| Step: 11
Training loss: 4.310702405369818
Validation loss: 2.484631379212216

Epoch: 170| Step: 0
Training loss: 2.4248058673980215
Validation loss: 2.4758346803837616

Epoch: 5| Step: 1
Training loss: 2.119343353278197
Validation loss: 2.474727697035132

Epoch: 5| Step: 2
Training loss: 2.5320300560383155
Validation loss: 2.480311199000609

Epoch: 5| Step: 3
Training loss: 2.2799184388078375
Validation loss: 2.480187988411425

Epoch: 5| Step: 4
Training loss: 3.1041278367312803
Validation loss: 2.4715841057064925

Epoch: 5| Step: 5
Training loss: 2.6404815273696984
Validation loss: 2.4661160231939974

Epoch: 5| Step: 6
Training loss: 2.9980265166246887
Validation loss: 2.47578518258181

Epoch: 5| Step: 7
Training loss: 2.3086319612029107
Validation loss: 2.485239660195824

Epoch: 5| Step: 8
Training loss: 2.0646990987718374
Validation loss: 2.4748221137601107

Epoch: 5| Step: 9
Training loss: 1.9560730924230851
Validation loss: 2.477724723189702

Epoch: 5| Step: 10
Training loss: 2.4423973574199556
Validation loss: 2.4729723615301236

Epoch: 5| Step: 11
Training loss: 2.128964036629716
Validation loss: 2.469960531583503

Epoch: 171| Step: 0
Training loss: 2.2995128405918135
Validation loss: 2.4828591790681642

Epoch: 5| Step: 1
Training loss: 2.6551717926021245
Validation loss: 2.478241164029153

Epoch: 5| Step: 2
Training loss: 2.3373540455338775
Validation loss: 2.4956949559180184

Epoch: 5| Step: 3
Training loss: 2.6595756804686683
Validation loss: 2.495225320502194

Epoch: 5| Step: 4
Training loss: 2.5485332683856337
Validation loss: 2.4997595989354435

Epoch: 5| Step: 5
Training loss: 2.212995023360692
Validation loss: 2.4724950310978913

Epoch: 5| Step: 6
Training loss: 2.290899275405025
Validation loss: 2.4756050625776496

Epoch: 5| Step: 7
Training loss: 2.3920267022406287
Validation loss: 2.4663793150532944

Epoch: 5| Step: 8
Training loss: 2.167997562801216
Validation loss: 2.4576480052990064

Epoch: 5| Step: 9
Training loss: 2.662454516958103
Validation loss: 2.4585873073917326

Epoch: 5| Step: 10
Training loss: 2.5743612849089454
Validation loss: 2.461802004793802

Epoch: 5| Step: 11
Training loss: 3.44251121768495
Validation loss: 2.4642835646508727

Epoch: 172| Step: 0
Training loss: 2.7489520590477956
Validation loss: 2.4689397014695693

Epoch: 5| Step: 1
Training loss: 2.2824282803919727
Validation loss: 2.4691152141534647

Epoch: 5| Step: 2
Training loss: 2.1990346611313187
Validation loss: 2.4663872820487622

Epoch: 5| Step: 3
Training loss: 1.949916493021609
Validation loss: 2.474710756958442

Epoch: 5| Step: 4
Training loss: 2.3318788104817285
Validation loss: 2.4625410153996072

Epoch: 5| Step: 5
Training loss: 2.928270164970715
Validation loss: 2.47502457914767

Epoch: 5| Step: 6
Training loss: 2.5351264375835205
Validation loss: 2.4719460920422276

Epoch: 5| Step: 7
Training loss: 2.6453627120252956
Validation loss: 2.467268406654918

Epoch: 5| Step: 8
Training loss: 2.694737558272621
Validation loss: 2.4715531164925206

Epoch: 5| Step: 9
Training loss: 2.011285768597453
Validation loss: 2.4771782984923005

Epoch: 5| Step: 10
Training loss: 2.7741131886256705
Validation loss: 2.479646489381735

Epoch: 5| Step: 11
Training loss: 1.940481906105693
Validation loss: 2.4850062681908356

Epoch: 173| Step: 0
Training loss: 2.1765372094782034
Validation loss: 2.478765151958161

Epoch: 5| Step: 1
Training loss: 2.3901789629342645
Validation loss: 2.4863627696813735

Epoch: 5| Step: 2
Training loss: 2.7578978268658534
Validation loss: 2.4977728181599224

Epoch: 5| Step: 3
Training loss: 1.9816016092789006
Validation loss: 2.500447058441853

Epoch: 5| Step: 4
Training loss: 2.48646716428056
Validation loss: 2.4900286540906706

Epoch: 5| Step: 5
Training loss: 2.2663918085437
Validation loss: 2.491149769999968

Epoch: 5| Step: 6
Training loss: 2.6939513376849353
Validation loss: 2.4971876140478773

Epoch: 5| Step: 7
Training loss: 3.1598983678288333
Validation loss: 2.494459107492062

Epoch: 5| Step: 8
Training loss: 1.7557800071354102
Validation loss: 2.4863779443073244

Epoch: 5| Step: 9
Training loss: 2.72435122124905
Validation loss: 2.4843605129051474

Epoch: 5| Step: 10
Training loss: 2.0863407045351896
Validation loss: 2.478112935810139

Epoch: 5| Step: 11
Training loss: 3.0482716024943124
Validation loss: 2.4794907131042945

Epoch: 174| Step: 0
Training loss: 2.546232645671359
Validation loss: 2.4816384108373546

Epoch: 5| Step: 1
Training loss: 2.1208909642071805
Validation loss: 2.476794756438909

Epoch: 5| Step: 2
Training loss: 2.8175273468125597
Validation loss: 2.47951840595548

Epoch: 5| Step: 3
Training loss: 2.566478339217736
Validation loss: 2.4767921694249293

Epoch: 5| Step: 4
Training loss: 2.1511051885971706
Validation loss: 2.48931294067525

Epoch: 5| Step: 5
Training loss: 2.8924086557800317
Validation loss: 2.488315755703677

Epoch: 5| Step: 6
Training loss: 2.5515472990985986
Validation loss: 2.4855144294980387

Epoch: 5| Step: 7
Training loss: 2.58378211604488
Validation loss: 2.486032604745635

Epoch: 5| Step: 8
Training loss: 2.0190001625034517
Validation loss: 2.486783913076077

Epoch: 5| Step: 9
Training loss: 2.3880329915232568
Validation loss: 2.479431057437233

Epoch: 5| Step: 10
Training loss: 2.0680700285249776
Validation loss: 2.483111778132541

Epoch: 5| Step: 11
Training loss: 1.856473309477869
Validation loss: 2.4860894187736235

Epoch: 175| Step: 0
Training loss: 2.446520522202473
Validation loss: 2.4858023663709585

Epoch: 5| Step: 1
Training loss: 2.7977121474863833
Validation loss: 2.4898551303079577

Epoch: 5| Step: 2
Training loss: 2.571877510375493
Validation loss: 2.4844984987540597

Epoch: 5| Step: 3
Training loss: 2.5001164409223353
Validation loss: 2.482251011100243

Epoch: 5| Step: 4
Training loss: 3.0910274972522536
Validation loss: 2.4853220520066204

Epoch: 5| Step: 5
Training loss: 2.0577733759538894
Validation loss: 2.476244146862298

Epoch: 5| Step: 6
Training loss: 2.563234270543701
Validation loss: 2.4843138601269352

Epoch: 5| Step: 7
Training loss: 1.9708232559473366
Validation loss: 2.4876479316538647

Epoch: 5| Step: 8
Training loss: 2.086460233761447
Validation loss: 2.507154723087134

Epoch: 5| Step: 9
Training loss: 2.420677528904685
Validation loss: 2.5016688459078935

Epoch: 5| Step: 10
Training loss: 2.053640584788209
Validation loss: 2.500533027251046

Epoch: 5| Step: 11
Training loss: 2.0400629037620757
Validation loss: 2.4998044771983747

Epoch: 176| Step: 0
Training loss: 2.4852130364145335
Validation loss: 2.4975269682302947

Epoch: 5| Step: 1
Training loss: 2.221922097867177
Validation loss: 2.4936763338252517

Epoch: 5| Step: 2
Training loss: 2.4379875478011863
Validation loss: 2.508240173094746

Epoch: 5| Step: 3
Training loss: 2.852463169779538
Validation loss: 2.4864516426084835

Epoch: 5| Step: 4
Training loss: 1.9682579031026957
Validation loss: 2.4877474305561913

Epoch: 5| Step: 5
Training loss: 2.4806164795144614
Validation loss: 2.489585853852043

Epoch: 5| Step: 6
Training loss: 2.3426312637599533
Validation loss: 2.4915160068735505

Epoch: 5| Step: 7
Training loss: 2.4707809978772777
Validation loss: 2.4901170810177606

Epoch: 5| Step: 8
Training loss: 2.55914254163399
Validation loss: 2.485097096424817

Epoch: 5| Step: 9
Training loss: 2.44089662649764
Validation loss: 2.4855894243474355

Epoch: 5| Step: 10
Training loss: 2.4656599963355244
Validation loss: 2.485627800179972

Epoch: 5| Step: 11
Training loss: 1.879284350762414
Validation loss: 2.494904224243129

Epoch: 177| Step: 0
Training loss: 2.064741592680173
Validation loss: 2.49503495391284

Epoch: 5| Step: 1
Training loss: 2.769920160403062
Validation loss: 2.4793840053937473

Epoch: 5| Step: 2
Training loss: 2.209697146313058
Validation loss: 2.4949836471266345

Epoch: 5| Step: 3
Training loss: 1.7299017052481787
Validation loss: 2.487816975396315

Epoch: 5| Step: 4
Training loss: 2.650735044203145
Validation loss: 2.4934542813636584

Epoch: 5| Step: 5
Training loss: 2.7679349229141557
Validation loss: 2.4923833612631183

Epoch: 5| Step: 6
Training loss: 2.2172046706413173
Validation loss: 2.5040963309550524

Epoch: 5| Step: 7
Training loss: 2.167494713447926
Validation loss: 2.5050711775976393

Epoch: 5| Step: 8
Training loss: 2.617121362562194
Validation loss: 2.5009394390583766

Epoch: 5| Step: 9
Training loss: 2.855432345762299
Validation loss: 2.4882518341385773

Epoch: 5| Step: 10
Training loss: 2.4828815889918783
Validation loss: 2.4951436677785463

Epoch: 5| Step: 11
Training loss: 1.8391473509110239
Validation loss: 2.4878243785967973

Epoch: 178| Step: 0
Training loss: 2.662824505296641
Validation loss: 2.4676594035454698

Epoch: 5| Step: 1
Training loss: 2.4749369295349286
Validation loss: 2.476908911255413

Epoch: 5| Step: 2
Training loss: 2.0935684808646275
Validation loss: 2.4650226211735022

Epoch: 5| Step: 3
Training loss: 2.564346764502274
Validation loss: 2.4678948486637196

Epoch: 5| Step: 4
Training loss: 2.339621506263785
Validation loss: 2.4732005086314732

Epoch: 5| Step: 5
Training loss: 2.1324654880248795
Validation loss: 2.468631411573254

Epoch: 5| Step: 6
Training loss: 2.9268700645286234
Validation loss: 2.4706267695634114

Epoch: 5| Step: 7
Training loss: 2.6592538342065866
Validation loss: 2.4756223255821346

Epoch: 5| Step: 8
Training loss: 2.4046465993554538
Validation loss: 2.47851938057513

Epoch: 5| Step: 9
Training loss: 1.9024229561924568
Validation loss: 2.492157524351142

Epoch: 5| Step: 10
Training loss: 2.0425597851897064
Validation loss: 2.480630938416416

Epoch: 5| Step: 11
Training loss: 3.542609923170913
Validation loss: 2.495174204752817

Epoch: 179| Step: 0
Training loss: 2.3511177732368855
Validation loss: 2.549352088751078

Epoch: 5| Step: 1
Training loss: 2.921368560147999
Validation loss: 2.6439736894646493

Epoch: 5| Step: 2
Training loss: 2.890229894080609
Validation loss: 2.6681434533190047

Epoch: 5| Step: 3
Training loss: 2.6900201558488908
Validation loss: 2.6121064621296957

Epoch: 5| Step: 4
Training loss: 2.06835902919172
Validation loss: 2.532544927251883

Epoch: 5| Step: 5
Training loss: 2.5328799051096946
Validation loss: 2.4938902025957455

Epoch: 5| Step: 6
Training loss: 2.096503539519321
Validation loss: 2.4625438069870063

Epoch: 5| Step: 7
Training loss: 2.6219963873133443
Validation loss: 2.4574077986802862

Epoch: 5| Step: 8
Training loss: 2.392549525498712
Validation loss: 2.456201996629453

Epoch: 5| Step: 9
Training loss: 2.3679292731548327
Validation loss: 2.4617291625317295

Epoch: 5| Step: 10
Training loss: 2.823709486847626
Validation loss: 2.4623645863035817

Epoch: 5| Step: 11
Training loss: 3.6724730958206306
Validation loss: 2.4707204222544834

Epoch: 180| Step: 0
Training loss: 2.5304995707760463
Validation loss: 2.4769222146699277

Epoch: 5| Step: 1
Training loss: 2.921260993620166
Validation loss: 2.474659345925673

Epoch: 5| Step: 2
Training loss: 2.6596999260145213
Validation loss: 2.473498413434743

Epoch: 5| Step: 3
Training loss: 1.9339766306328168
Validation loss: 2.475351849987137

Epoch: 5| Step: 4
Training loss: 2.0606415207084603
Validation loss: 2.4762780500489288

Epoch: 5| Step: 5
Training loss: 2.70349665385583
Validation loss: 2.466378960605982

Epoch: 5| Step: 6
Training loss: 2.6112996387283203
Validation loss: 2.4612298297330053

Epoch: 5| Step: 7
Training loss: 2.6757226199667006
Validation loss: 2.4589928143846045

Epoch: 5| Step: 8
Training loss: 2.402766096004639
Validation loss: 2.458799848634624

Epoch: 5| Step: 9
Training loss: 2.515026992895682
Validation loss: 2.4603325034553043

Epoch: 5| Step: 10
Training loss: 2.435574382268733
Validation loss: 2.465369130771712

Epoch: 5| Step: 11
Training loss: 2.420231809311501
Validation loss: 2.4646108429621094

Epoch: 181| Step: 0
Training loss: 2.7782925086482235
Validation loss: 2.474961325796041

Epoch: 5| Step: 1
Training loss: 2.480192874976162
Validation loss: 2.473934251099208

Epoch: 5| Step: 2
Training loss: 2.6728071404899207
Validation loss: 2.488888461343789

Epoch: 5| Step: 3
Training loss: 2.8791346465859124
Validation loss: 2.491248504981476

Epoch: 5| Step: 4
Training loss: 2.7986118315597714
Validation loss: 2.4956963371502585

Epoch: 5| Step: 5
Training loss: 2.231122403810722
Validation loss: 2.501199498109184

Epoch: 5| Step: 6
Training loss: 2.2493788603558476
Validation loss: 2.495564523548968

Epoch: 5| Step: 7
Training loss: 2.461818379983168
Validation loss: 2.4916290765710256

Epoch: 5| Step: 8
Training loss: 2.2031021658072554
Validation loss: 2.4878083502805897

Epoch: 5| Step: 9
Training loss: 2.568064996942587
Validation loss: 2.4844301285615122

Epoch: 5| Step: 10
Training loss: 2.1261837971745705
Validation loss: 2.476527962910119

Epoch: 5| Step: 11
Training loss: 1.9251462806509787
Validation loss: 2.4796179646495737

Epoch: 182| Step: 0
Training loss: 2.8177517495656312
Validation loss: 2.4726472104164965

Epoch: 5| Step: 1
Training loss: 2.2564917268472486
Validation loss: 2.472023801186964

Epoch: 5| Step: 2
Training loss: 2.0316653487232785
Validation loss: 2.464433473678088

Epoch: 5| Step: 3
Training loss: 2.3145063820679037
Validation loss: 2.4616340095593925

Epoch: 5| Step: 4
Training loss: 2.4603905448420567
Validation loss: 2.4671862273064993

Epoch: 5| Step: 5
Training loss: 2.802667511996436
Validation loss: 2.4655699589608817

Epoch: 5| Step: 6
Training loss: 2.6093506954683034
Validation loss: 2.472104927618922

Epoch: 5| Step: 7
Training loss: 2.198519282117852
Validation loss: 2.46443625102885

Epoch: 5| Step: 8
Training loss: 2.490202014189244
Validation loss: 2.4723326967025203

Epoch: 5| Step: 9
Training loss: 2.7756253921037795
Validation loss: 2.4721644365924735

Epoch: 5| Step: 10
Training loss: 2.020141981441677
Validation loss: 2.463685510695524

Epoch: 5| Step: 11
Training loss: 3.3181392244144394
Validation loss: 2.472158752580944

Epoch: 183| Step: 0
Training loss: 2.1696854126898355
Validation loss: 2.4632412093473595

Epoch: 5| Step: 1
Training loss: 1.9163960459297933
Validation loss: 2.4690940874676155

Epoch: 5| Step: 2
Training loss: 2.6624585466336796
Validation loss: 2.4578434421008684

Epoch: 5| Step: 3
Training loss: 2.5505010150069083
Validation loss: 2.4637108006125787

Epoch: 5| Step: 4
Training loss: 2.765942485280318
Validation loss: 2.473944982526125

Epoch: 5| Step: 5
Training loss: 2.557091378152753
Validation loss: 2.466973532751458

Epoch: 5| Step: 6
Training loss: 2.4349815856738632
Validation loss: 2.4697816821738097

Epoch: 5| Step: 7
Training loss: 2.6144514215230985
Validation loss: 2.4719757280591987

Epoch: 5| Step: 8
Training loss: 2.0123842194228296
Validation loss: 2.4814179213912566

Epoch: 5| Step: 9
Training loss: 2.25149348813038
Validation loss: 2.4744871812381652

Epoch: 5| Step: 10
Training loss: 2.8002055705765234
Validation loss: 2.486437910742138

Epoch: 5| Step: 11
Training loss: 2.6103514711641864
Validation loss: 2.502684824447917

Epoch: 184| Step: 0
Training loss: 2.09726090497292
Validation loss: 2.4867199880884723

Epoch: 5| Step: 1
Training loss: 2.4513416003552178
Validation loss: 2.4845554798076206

Epoch: 5| Step: 2
Training loss: 2.6153043889993266
Validation loss: 2.4837187014116258

Epoch: 5| Step: 3
Training loss: 2.9114220650494724
Validation loss: 2.4878523380581585

Epoch: 5| Step: 4
Training loss: 2.012866949900492
Validation loss: 2.4911199772666888

Epoch: 5| Step: 5
Training loss: 2.5538801031445844
Validation loss: 2.477864321402801

Epoch: 5| Step: 6
Training loss: 1.9837745298660983
Validation loss: 2.493429960462645

Epoch: 5| Step: 7
Training loss: 2.968480629997674
Validation loss: 2.4846016002774443

Epoch: 5| Step: 8
Training loss: 2.2918563301900976
Validation loss: 2.47966916871341

Epoch: 5| Step: 9
Training loss: 2.2864741250248075
Validation loss: 2.472455772400333

Epoch: 5| Step: 10
Training loss: 2.4882650569466724
Validation loss: 2.459655997232444

Epoch: 5| Step: 11
Training loss: 2.3365466494173757
Validation loss: 2.4635366852422975

Epoch: 185| Step: 0
Training loss: 2.386377288316922
Validation loss: 2.4665453800999355

Epoch: 5| Step: 1
Training loss: 2.969309061515054
Validation loss: 2.467195266766168

Epoch: 5| Step: 2
Training loss: 1.9220659114638796
Validation loss: 2.467450902663599

Epoch: 5| Step: 3
Training loss: 2.477642990897647
Validation loss: 2.4705040128989646

Epoch: 5| Step: 4
Training loss: 2.6409055120688314
Validation loss: 2.4662430988105006

Epoch: 5| Step: 5
Training loss: 2.1564930212207343
Validation loss: 2.4616236381110523

Epoch: 5| Step: 6
Training loss: 3.2334115625205526
Validation loss: 2.4784334979267295

Epoch: 5| Step: 7
Training loss: 1.8064391647598856
Validation loss: 2.4666952483243914

Epoch: 5| Step: 8
Training loss: 1.9852757849916887
Validation loss: 2.4718680870772904

Epoch: 5| Step: 9
Training loss: 2.6888309110831043
Validation loss: 2.4652727038968543

Epoch: 5| Step: 10
Training loss: 2.419030469458872
Validation loss: 2.4648498962293472

Epoch: 5| Step: 11
Training loss: 2.356732435949869
Validation loss: 2.4673675577704763

Epoch: 186| Step: 0
Training loss: 2.1366353968902567
Validation loss: 2.465032321429155

Epoch: 5| Step: 1
Training loss: 2.40339570622803
Validation loss: 2.477091703622684

Epoch: 5| Step: 2
Training loss: 2.0353854996615914
Validation loss: 2.4890223407079914

Epoch: 5| Step: 3
Training loss: 2.439080728368684
Validation loss: 2.5052793351314118

Epoch: 5| Step: 4
Training loss: 1.9775791493815063
Validation loss: 2.5273872815902907

Epoch: 5| Step: 5
Training loss: 2.576552125112953
Validation loss: 2.52787719709792

Epoch: 5| Step: 6
Training loss: 2.4627802655439113
Validation loss: 2.5268611181813356

Epoch: 5| Step: 7
Training loss: 3.3165645494187848
Validation loss: 2.5386698218675483

Epoch: 5| Step: 8
Training loss: 2.7506986077463957
Validation loss: 2.506755400075879

Epoch: 5| Step: 9
Training loss: 2.421514668885154
Validation loss: 2.4848805119181976

Epoch: 5| Step: 10
Training loss: 2.3005393764124786
Validation loss: 2.473562125887636

Epoch: 5| Step: 11
Training loss: 2.9689852972646285
Validation loss: 2.4732086042795016

Epoch: 187| Step: 0
Training loss: 2.1254897955607115
Validation loss: 2.4684048624662336

Epoch: 5| Step: 1
Training loss: 2.6530397650840416
Validation loss: 2.466948600446207

Epoch: 5| Step: 2
Training loss: 3.015638440354197
Validation loss: 2.4685402954142397

Epoch: 5| Step: 3
Training loss: 2.398796328066925
Validation loss: 2.468805292371451

Epoch: 5| Step: 4
Training loss: 2.234514138751263
Validation loss: 2.464404055335189

Epoch: 5| Step: 5
Training loss: 2.4102657546727486
Validation loss: 2.461930658873893

Epoch: 5| Step: 6
Training loss: 3.014228457899489
Validation loss: 2.4619650617959006

Epoch: 5| Step: 7
Training loss: 2.0498626104697255
Validation loss: 2.4596257684995333

Epoch: 5| Step: 8
Training loss: 2.3871539479262442
Validation loss: 2.4551096337318907

Epoch: 5| Step: 9
Training loss: 2.610583271170746
Validation loss: 2.4680441519326446

Epoch: 5| Step: 10
Training loss: 1.6355681460582407
Validation loss: 2.4704143893233046

Epoch: 5| Step: 11
Training loss: 3.4353829540466942
Validation loss: 2.4662505506594483

Epoch: 188| Step: 0
Training loss: 3.0892743961747455
Validation loss: 2.4754259648448342

Epoch: 5| Step: 1
Training loss: 2.1572035740343702
Validation loss: 2.492380407793718

Epoch: 5| Step: 2
Training loss: 2.57229472891897
Validation loss: 2.529906618910449

Epoch: 5| Step: 3
Training loss: 2.230325296735939
Validation loss: 2.575499043052662

Epoch: 5| Step: 4
Training loss: 2.4959663275012383
Validation loss: 2.5847226619008348

Epoch: 5| Step: 5
Training loss: 2.3312459418949265
Validation loss: 2.6038836185491605

Epoch: 5| Step: 6
Training loss: 2.6141407099286282
Validation loss: 2.591247968082518

Epoch: 5| Step: 7
Training loss: 2.825308055454366
Validation loss: 2.5795910221684206

Epoch: 5| Step: 8
Training loss: 2.2057890240242513
Validation loss: 2.549848649887712

Epoch: 5| Step: 9
Training loss: 2.4082889587995724
Validation loss: 2.515406605024963

Epoch: 5| Step: 10
Training loss: 2.1725619759625254
Validation loss: 2.499193526364605

Epoch: 5| Step: 11
Training loss: 3.0181232441486077
Validation loss: 2.4731629622319606

Epoch: 189| Step: 0
Training loss: 2.273578757390724
Validation loss: 2.4732103756383004

Epoch: 5| Step: 1
Training loss: 2.5418327360670467
Validation loss: 2.4680902307681922

Epoch: 5| Step: 2
Training loss: 1.9738870841045284
Validation loss: 2.4691358136045345

Epoch: 5| Step: 3
Training loss: 2.134335264224223
Validation loss: 2.466103426855027

Epoch: 5| Step: 4
Training loss: 2.695372384553265
Validation loss: 2.470775477539864

Epoch: 5| Step: 5
Training loss: 2.4714122855156235
Validation loss: 2.46394675206111

Epoch: 5| Step: 6
Training loss: 2.5483428843959395
Validation loss: 2.4697852318169216

Epoch: 5| Step: 7
Training loss: 2.354348797349815
Validation loss: 2.458696024751071

Epoch: 5| Step: 8
Training loss: 2.7431975240761304
Validation loss: 2.4621576624730857

Epoch: 5| Step: 9
Training loss: 2.47110703374601
Validation loss: 2.4666673591544064

Epoch: 5| Step: 10
Training loss: 2.678231335802367
Validation loss: 2.459111326302281

Epoch: 5| Step: 11
Training loss: 3.4195303352394824
Validation loss: 2.4587254548249655

Epoch: 190| Step: 0
Training loss: 2.167176357864968
Validation loss: 2.4646498477394774

Epoch: 5| Step: 1
Training loss: 1.8874077022798343
Validation loss: 2.456708245002116

Epoch: 5| Step: 2
Training loss: 3.1167270253366537
Validation loss: 2.4626299896398804

Epoch: 5| Step: 3
Training loss: 2.3466351870390514
Validation loss: 2.464436319555607

Epoch: 5| Step: 4
Training loss: 2.214739008181129
Validation loss: 2.4647486245670978

Epoch: 5| Step: 5
Training loss: 2.666309829519936
Validation loss: 2.469425941835889

Epoch: 5| Step: 6
Training loss: 2.064114372300444
Validation loss: 2.461380166905773

Epoch: 5| Step: 7
Training loss: 2.947990191905296
Validation loss: 2.4722278338331716

Epoch: 5| Step: 8
Training loss: 2.759640787355854
Validation loss: 2.4690358282162332

Epoch: 5| Step: 9
Training loss: 2.669729698871978
Validation loss: 2.461440249965999

Epoch: 5| Step: 10
Training loss: 2.236985822023047
Validation loss: 2.471900028820617

Epoch: 5| Step: 11
Training loss: 2.0929319007537313
Validation loss: 2.477018280287306

Epoch: 191| Step: 0
Training loss: 2.4458453783023546
Validation loss: 2.487333176001264

Epoch: 5| Step: 1
Training loss: 2.6267065223180963
Validation loss: 2.4854850068706487

Epoch: 5| Step: 2
Training loss: 2.507415454842614
Validation loss: 2.482247475281281

Epoch: 5| Step: 3
Training loss: 2.2208775809951447
Validation loss: 2.4786101659645463

Epoch: 5| Step: 4
Training loss: 2.1652512573266725
Validation loss: 2.489631314526881

Epoch: 5| Step: 5
Training loss: 2.7146999842498403
Validation loss: 2.4749732970553664

Epoch: 5| Step: 6
Training loss: 2.3508328037561115
Validation loss: 2.477501690113686

Epoch: 5| Step: 7
Training loss: 2.709461876907396
Validation loss: 2.476641680778403

Epoch: 5| Step: 8
Training loss: 2.3717978121897234
Validation loss: 2.4895060353003884

Epoch: 5| Step: 9
Training loss: 2.7086066963936073
Validation loss: 2.4918341430996307

Epoch: 5| Step: 10
Training loss: 2.390016266356463
Validation loss: 2.486117939181929

Epoch: 5| Step: 11
Training loss: 1.6268562571984329
Validation loss: 2.4895304364340127

Epoch: 192| Step: 0
Training loss: 2.07628865958732
Validation loss: 2.495920799125944

Epoch: 5| Step: 1
Training loss: 2.451656801198287
Validation loss: 2.4879377019158304

Epoch: 5| Step: 2
Training loss: 2.4884544803318374
Validation loss: 2.5024185125032554

Epoch: 5| Step: 3
Training loss: 2.6535777394271585
Validation loss: 2.488874818758798

Epoch: 5| Step: 4
Training loss: 2.4778720029050656
Validation loss: 2.4719656431326973

Epoch: 5| Step: 5
Training loss: 1.79791282868519
Validation loss: 2.4776699486185176

Epoch: 5| Step: 6
Training loss: 2.618312491421598
Validation loss: 2.460415456794702

Epoch: 5| Step: 7
Training loss: 2.7237505471875436
Validation loss: 2.4585664175280204

Epoch: 5| Step: 8
Training loss: 2.925174094993826
Validation loss: 2.4645102144768627

Epoch: 5| Step: 9
Training loss: 2.661781744271717
Validation loss: 2.461672734627792

Epoch: 5| Step: 10
Training loss: 2.3256956392881687
Validation loss: 2.4694949768421064

Epoch: 5| Step: 11
Training loss: 1.279534284388002
Validation loss: 2.471916204495703

Epoch: 193| Step: 0
Training loss: 2.5301711997810377
Validation loss: 2.4743365102467303

Epoch: 5| Step: 1
Training loss: 3.1954882883529674
Validation loss: 2.485808432805254

Epoch: 5| Step: 2
Training loss: 2.694186564249209
Validation loss: 2.491076346280397

Epoch: 5| Step: 3
Training loss: 1.6520143726222432
Validation loss: 2.4965284921123088

Epoch: 5| Step: 4
Training loss: 1.6560716353047613
Validation loss: 2.5311774922413597

Epoch: 5| Step: 5
Training loss: 2.6358775965877315
Validation loss: 2.5397453759193227

Epoch: 5| Step: 6
Training loss: 2.4964001486926284
Validation loss: 2.5431141106175597

Epoch: 5| Step: 7
Training loss: 2.5974051217896714
Validation loss: 2.5359682488276425

Epoch: 5| Step: 8
Training loss: 2.397235780542291
Validation loss: 2.5341242495667267

Epoch: 5| Step: 9
Training loss: 2.548371419511816
Validation loss: 2.524805050742646

Epoch: 5| Step: 10
Training loss: 2.3438295478037037
Validation loss: 2.508037414447383

Epoch: 5| Step: 11
Training loss: 2.1631500854624783
Validation loss: 2.4809212079233767

Epoch: 194| Step: 0
Training loss: 2.2376063529370724
Validation loss: 2.4693200101195183

Epoch: 5| Step: 1
Training loss: 2.6154898988021507
Validation loss: 2.4717508579850827

Epoch: 5| Step: 2
Training loss: 1.9667796008233158
Validation loss: 2.4702127806908822

Epoch: 5| Step: 3
Training loss: 3.089302488198012
Validation loss: 2.4679155126426515

Epoch: 5| Step: 4
Training loss: 2.769796899448907
Validation loss: 2.468939564666041

Epoch: 5| Step: 5
Training loss: 2.1710368123915535
Validation loss: 2.470421124876412

Epoch: 5| Step: 6
Training loss: 2.375806972536269
Validation loss: 2.4696751563173898

Epoch: 5| Step: 7
Training loss: 2.641762843823069
Validation loss: 2.4687464468564246

Epoch: 5| Step: 8
Training loss: 2.51396750110023
Validation loss: 2.473622483328423

Epoch: 5| Step: 9
Training loss: 2.427353403239827
Validation loss: 2.464258337049793

Epoch: 5| Step: 10
Training loss: 2.4457092938225786
Validation loss: 2.4695715603570068

Epoch: 5| Step: 11
Training loss: 1.3621297525730978
Validation loss: 2.467712687408956

Epoch: 195| Step: 0
Training loss: 2.3388219267541337
Validation loss: 2.473283281416435

Epoch: 5| Step: 1
Training loss: 2.1982297147094503
Validation loss: 2.468227037336455

Epoch: 5| Step: 2
Training loss: 2.2771772843989373
Validation loss: 2.4705634076365146

Epoch: 5| Step: 3
Training loss: 2.35370687760261
Validation loss: 2.4746249247852496

Epoch: 5| Step: 4
Training loss: 2.713896793452447
Validation loss: 2.4765583904725643

Epoch: 5| Step: 5
Training loss: 2.153783078108468
Validation loss: 2.487495172438418

Epoch: 5| Step: 6
Training loss: 2.792885945833289
Validation loss: 2.4968485280568444

Epoch: 5| Step: 7
Training loss: 2.771900191509769
Validation loss: 2.522303323403447

Epoch: 5| Step: 8
Training loss: 2.029804125952523
Validation loss: 2.5137824740873884

Epoch: 5| Step: 9
Training loss: 2.392170126245797
Validation loss: 2.5198200701000695

Epoch: 5| Step: 10
Training loss: 2.555208018992721
Validation loss: 2.5211080225463776

Epoch: 5| Step: 11
Training loss: 2.528994273344037
Validation loss: 2.5093219251075234

Epoch: 196| Step: 0
Training loss: 2.2889260521539363
Validation loss: 2.5044600062377858

Epoch: 5| Step: 1
Training loss: 2.518169182763545
Validation loss: 2.509427207365688

Epoch: 5| Step: 2
Training loss: 2.618718670131849
Validation loss: 2.497739859000828

Epoch: 5| Step: 3
Training loss: 3.2272419433493185
Validation loss: 2.4946090350003938

Epoch: 5| Step: 4
Training loss: 2.700092709680381
Validation loss: 2.4989131213941866

Epoch: 5| Step: 5
Training loss: 1.9565789771806588
Validation loss: 2.4936809270498013

Epoch: 5| Step: 6
Training loss: 2.025379206626143
Validation loss: 2.4992803173505043

Epoch: 5| Step: 7
Training loss: 2.621808973533574
Validation loss: 2.476910018204899

Epoch: 5| Step: 8
Training loss: 2.2009897909867906
Validation loss: 2.473370947457561

Epoch: 5| Step: 9
Training loss: 1.7868203851893516
Validation loss: 2.479164577664903

Epoch: 5| Step: 10
Training loss: 2.4790597352974566
Validation loss: 2.4802729211435177

Epoch: 5| Step: 11
Training loss: 1.4132937876249085
Validation loss: 2.4688558676509813

Epoch: 197| Step: 0
Training loss: 2.1653844878718798
Validation loss: 2.479153722574331

Epoch: 5| Step: 1
Training loss: 2.4688659833873126
Validation loss: 2.47612162254843

Epoch: 5| Step: 2
Training loss: 2.8229397612092715
Validation loss: 2.4784180501975452

Epoch: 5| Step: 3
Training loss: 2.3628361054164486
Validation loss: 2.469041014471906

Epoch: 5| Step: 4
Training loss: 1.9989837806543638
Validation loss: 2.4801714941816524

Epoch: 5| Step: 5
Training loss: 1.962914553064666
Validation loss: 2.4788282361094125

Epoch: 5| Step: 6
Training loss: 2.080384825540426
Validation loss: 2.4733930055620923

Epoch: 5| Step: 7
Training loss: 2.9761871341504906
Validation loss: 2.4759297971659007

Epoch: 5| Step: 8
Training loss: 2.3220438550541895
Validation loss: 2.474435799685108

Epoch: 5| Step: 9
Training loss: 2.920160943390404
Validation loss: 2.483202971700012

Epoch: 5| Step: 10
Training loss: 2.2935962760615647
Validation loss: 2.4830000889537813

Epoch: 5| Step: 11
Training loss: 2.1428364934380464
Validation loss: 2.488220373793363

Epoch: 198| Step: 0
Training loss: 2.4103947402453585
Validation loss: 2.4951857903910253

Epoch: 5| Step: 1
Training loss: 2.4842452489211104
Validation loss: 2.496716035215584

Epoch: 5| Step: 2
Training loss: 2.5920117539975256
Validation loss: 2.4942024320881813

Epoch: 5| Step: 3
Training loss: 2.6148251932042466
Validation loss: 2.503690626012076

Epoch: 5| Step: 4
Training loss: 2.725430578808564
Validation loss: 2.494394722075758

Epoch: 5| Step: 5
Training loss: 2.692172668026513
Validation loss: 2.4879314669814794

Epoch: 5| Step: 6
Training loss: 1.8710032144515838
Validation loss: 2.4785600943317343

Epoch: 5| Step: 7
Training loss: 2.0584026443405157
Validation loss: 2.4850576490524676

Epoch: 5| Step: 8
Training loss: 2.3825819560602213
Validation loss: 2.477773216007673

Epoch: 5| Step: 9
Training loss: 2.054365705812028
Validation loss: 2.48539945884774

Epoch: 5| Step: 10
Training loss: 2.6110953867102893
Validation loss: 2.4860880601750814

Epoch: 5| Step: 11
Training loss: 2.224713652111845
Validation loss: 2.4845598659871024

Epoch: 199| Step: 0
Training loss: 3.222052500263322
Validation loss: 2.48214698147864

Epoch: 5| Step: 1
Training loss: 2.5476921970389736
Validation loss: 2.5004666210851143

Epoch: 5| Step: 2
Training loss: 3.0251214121263743
Validation loss: 2.5053010766091415

Epoch: 5| Step: 3
Training loss: 2.3262507940144443
Validation loss: 2.511576710309118

Epoch: 5| Step: 4
Training loss: 2.3071853808398837
Validation loss: 2.519350137041377

Epoch: 5| Step: 5
Training loss: 1.7719315508071551
Validation loss: 2.5183052332901106

Epoch: 5| Step: 6
Training loss: 2.448972650115886
Validation loss: 2.5135774928160224

Epoch: 5| Step: 7
Training loss: 1.6760655586558157
Validation loss: 2.5092679253865557

Epoch: 5| Step: 8
Training loss: 2.4083111345088626
Validation loss: 2.504806588781089

Epoch: 5| Step: 9
Training loss: 2.03088950846351
Validation loss: 2.506973268784083

Epoch: 5| Step: 10
Training loss: 2.3433910857997775
Validation loss: 2.4760848024874664

Epoch: 5| Step: 11
Training loss: 2.053448321520479
Validation loss: 2.490272551599217

Epoch: 200| Step: 0
Training loss: 2.340028274723955
Validation loss: 2.4762672424807293

Epoch: 5| Step: 1
Training loss: 2.7221407737570957
Validation loss: 2.4802246534335826

Epoch: 5| Step: 2
Training loss: 1.9146126092852396
Validation loss: 2.4743469709347465

Epoch: 5| Step: 3
Training loss: 2.2890526052007236
Validation loss: 2.472542049508749

Epoch: 5| Step: 4
Training loss: 2.700747199336306
Validation loss: 2.4752420019715293

Epoch: 5| Step: 5
Training loss: 2.1927277359515545
Validation loss: 2.477981201143159

Epoch: 5| Step: 6
Training loss: 2.5143245392130207
Validation loss: 2.4780604087490246

Epoch: 5| Step: 7
Training loss: 2.575497855047637
Validation loss: 2.4780404848148074

Epoch: 5| Step: 8
Training loss: 2.6275068755926445
Validation loss: 2.478513304316259

Epoch: 5| Step: 9
Training loss: 2.754781034875524
Validation loss: 2.4855999475856354

Epoch: 5| Step: 10
Training loss: 2.6387733423021915
Validation loss: 2.4808050693580106

Epoch: 5| Step: 11
Training loss: 1.3486991937806463
Validation loss: 2.4817671413867837

Testing loss: 2.0177498162922607

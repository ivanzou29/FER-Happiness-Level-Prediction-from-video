Epoch: 1| Step: 0
Training loss: 5.480634060118202
Validation loss: 5.86000387250232

Epoch: 6| Step: 1
Training loss: 6.609235613279776
Validation loss: 5.858331938409255

Epoch: 6| Step: 2
Training loss: 5.570686402685967
Validation loss: 5.856647205758297

Epoch: 6| Step: 3
Training loss: 5.898756976033568
Validation loss: 5.85503322210749

Epoch: 6| Step: 4
Training loss: 5.5375161749816675
Validation loss: 5.853357288525918

Epoch: 6| Step: 5
Training loss: 6.819433760866635
Validation loss: 5.8517353081577355

Epoch: 6| Step: 6
Training loss: 5.650307584509054
Validation loss: 5.8500048504236

Epoch: 6| Step: 7
Training loss: 6.588217778342869
Validation loss: 5.848282115658012

Epoch: 6| Step: 8
Training loss: 4.787567950929326
Validation loss: 5.8463892832992075

Epoch: 6| Step: 9
Training loss: 6.043800696206713
Validation loss: 5.8445739981132405

Epoch: 6| Step: 10
Training loss: 6.733627525675964
Validation loss: 5.842694137845268

Epoch: 6| Step: 11
Training loss: 5.0995626224637824
Validation loss: 5.8405863039964006

Epoch: 6| Step: 12
Training loss: 6.420749834901124
Validation loss: 5.838647338382787

Epoch: 6| Step: 13
Training loss: 5.8505037009378515
Validation loss: 5.836364430885113

Epoch: 2| Step: 0
Training loss: 4.949658068011807
Validation loss: 5.8340535763736945

Epoch: 6| Step: 1
Training loss: 6.093035690305764
Validation loss: 5.8317844877781315

Epoch: 6| Step: 2
Training loss: 6.16446660292155
Validation loss: 5.829373005325119

Epoch: 6| Step: 3
Training loss: 5.053272456068323
Validation loss: 5.826711638398553

Epoch: 6| Step: 4
Training loss: 5.855571356568481
Validation loss: 5.823971629470151

Epoch: 6| Step: 5
Training loss: 5.0659040144762635
Validation loss: 5.821098721715879

Epoch: 6| Step: 6
Training loss: 7.309909459504429
Validation loss: 5.818080834393792

Epoch: 6| Step: 7
Training loss: 5.470824453090521
Validation loss: 5.8148366681790895

Epoch: 6| Step: 8
Training loss: 6.512860413753927
Validation loss: 5.811428258389222

Epoch: 6| Step: 9
Training loss: 6.167886493078801
Validation loss: 5.807974006925695

Epoch: 6| Step: 10
Training loss: 6.0151379672975915
Validation loss: 5.804112141890057

Epoch: 6| Step: 11
Training loss: 5.722372939872783
Validation loss: 5.80010490541844

Epoch: 6| Step: 12
Training loss: 6.031144373467881
Validation loss: 5.79590016520088

Epoch: 6| Step: 13
Training loss: 6.183084781446134
Validation loss: 5.791561381894758

Epoch: 3| Step: 0
Training loss: 5.943529139766533
Validation loss: 5.787066199109105

Epoch: 6| Step: 1
Training loss: 6.464207330362487
Validation loss: 5.781972699057237

Epoch: 6| Step: 2
Training loss: 5.546427448108719
Validation loss: 5.7767950265530965

Epoch: 6| Step: 3
Training loss: 6.03796708410944
Validation loss: 5.771591448879867

Epoch: 6| Step: 4
Training loss: 4.431895410536178
Validation loss: 5.765956355689783

Epoch: 6| Step: 5
Training loss: 5.8684724297254105
Validation loss: 5.7600961312025145

Epoch: 6| Step: 6
Training loss: 5.43203548462504
Validation loss: 5.754453648126102

Epoch: 6| Step: 7
Training loss: 5.986698665632498
Validation loss: 5.747963558538985

Epoch: 6| Step: 8
Training loss: 6.0645210594021055
Validation loss: 5.741588632231432

Epoch: 6| Step: 9
Training loss: 6.341679277378264
Validation loss: 5.7348407829907595

Epoch: 6| Step: 10
Training loss: 5.903403590319344
Validation loss: 5.727859579160573

Epoch: 6| Step: 11
Training loss: 6.153684129782794
Validation loss: 5.720436167237666

Epoch: 6| Step: 12
Training loss: 5.12515091092362
Validation loss: 5.712984802687054

Epoch: 6| Step: 13
Training loss: 6.4639671445899145
Validation loss: 5.704977743120433

Epoch: 4| Step: 0
Training loss: 6.225024254686526
Validation loss: 5.697239037574732

Epoch: 6| Step: 1
Training loss: 6.357564382523364
Validation loss: 5.689191182535708

Epoch: 6| Step: 2
Training loss: 5.579547091736154
Validation loss: 5.6802592035708175

Epoch: 6| Step: 3
Training loss: 6.173611862512502
Validation loss: 5.671624716709054

Epoch: 6| Step: 4
Training loss: 4.804118995838038
Validation loss: 5.663167050491129

Epoch: 6| Step: 5
Training loss: 6.55726807347407
Validation loss: 5.6544936739373775

Epoch: 6| Step: 6
Training loss: 5.309171464149768
Validation loss: 5.645558152290269

Epoch: 6| Step: 7
Training loss: 5.771028627839221
Validation loss: 5.636677975153543

Epoch: 6| Step: 8
Training loss: 5.803327973677549
Validation loss: 5.627795789128501

Epoch: 6| Step: 9
Training loss: 5.605037276512419
Validation loss: 5.618818715615572

Epoch: 6| Step: 10
Training loss: 6.239936355853131
Validation loss: 5.610033540313786

Epoch: 6| Step: 11
Training loss: 5.935357600737533
Validation loss: 5.601044372402808

Epoch: 6| Step: 12
Training loss: 4.9597908677342595
Validation loss: 5.5926814701455685

Epoch: 6| Step: 13
Training loss: 4.955194949290455
Validation loss: 5.583769349491026

Epoch: 5| Step: 0
Training loss: 5.126074701533633
Validation loss: 5.575175270324737

Epoch: 6| Step: 1
Training loss: 5.790216863652431
Validation loss: 5.566736872262686

Epoch: 6| Step: 2
Training loss: 6.272357472411689
Validation loss: 5.558254893758405

Epoch: 6| Step: 3
Training loss: 5.082481604757199
Validation loss: 5.549793317457486

Epoch: 6| Step: 4
Training loss: 5.213449859064859
Validation loss: 5.541533800793432

Epoch: 6| Step: 5
Training loss: 5.523295226686042
Validation loss: 5.533220934971043

Epoch: 6| Step: 6
Training loss: 5.5102663458336
Validation loss: 5.525152117446069

Epoch: 6| Step: 7
Training loss: 6.030266399919983
Validation loss: 5.516761266506166

Epoch: 6| Step: 8
Training loss: 6.070353975485722
Validation loss: 5.5088088557007175

Epoch: 6| Step: 9
Training loss: 5.914496583516283
Validation loss: 5.500799843398649

Epoch: 6| Step: 10
Training loss: 4.743600449447553
Validation loss: 5.4924135482668035

Epoch: 6| Step: 11
Training loss: 6.15676211633031
Validation loss: 5.484593632498156

Epoch: 6| Step: 12
Training loss: 5.5236852609190485
Validation loss: 5.476708105947835

Epoch: 6| Step: 13
Training loss: 5.762172916422242
Validation loss: 5.46805389333647

Epoch: 6| Step: 0
Training loss: 6.180438703749175
Validation loss: 5.4603879230921155

Epoch: 6| Step: 1
Training loss: 5.350624525749471
Validation loss: 5.45223295098575

Epoch: 6| Step: 2
Training loss: 6.005319144910332
Validation loss: 5.444217580925757

Epoch: 6| Step: 3
Training loss: 6.085930291735943
Validation loss: 5.436196649669263

Epoch: 6| Step: 4
Training loss: 4.981734099371505
Validation loss: 5.428375949303879

Epoch: 6| Step: 5
Training loss: 6.0690919883848
Validation loss: 5.419662419887832

Epoch: 6| Step: 6
Training loss: 5.0712562470354134
Validation loss: 5.41150214648998

Epoch: 6| Step: 7
Training loss: 5.779030863681961
Validation loss: 5.403017963256007

Epoch: 6| Step: 8
Training loss: 5.193986261642672
Validation loss: 5.394545805310891

Epoch: 6| Step: 9
Training loss: 5.168145532100155
Validation loss: 5.385841200406346

Epoch: 6| Step: 10
Training loss: 5.4350534832664525
Validation loss: 5.3774474729996005

Epoch: 6| Step: 11
Training loss: 5.750070322684786
Validation loss: 5.3691504533755685

Epoch: 6| Step: 12
Training loss: 5.0333179465220805
Validation loss: 5.360368968258693

Epoch: 6| Step: 13
Training loss: 5.078023586727742
Validation loss: 5.351689736740171

Epoch: 7| Step: 0
Training loss: 5.3007524639970045
Validation loss: 5.343038916913804

Epoch: 6| Step: 1
Training loss: 5.427129919788153
Validation loss: 5.334454666910457

Epoch: 6| Step: 2
Training loss: 5.185259231664367
Validation loss: 5.325583678778662

Epoch: 6| Step: 3
Training loss: 5.741680262991979
Validation loss: 5.316450677069912

Epoch: 6| Step: 4
Training loss: 5.870806881818164
Validation loss: 5.308026094193797

Epoch: 6| Step: 5
Training loss: 4.782890885965389
Validation loss: 5.299210835129953

Epoch: 6| Step: 6
Training loss: 5.9909361088401765
Validation loss: 5.290215761415357

Epoch: 6| Step: 7
Training loss: 5.139213762524753
Validation loss: 5.281641567544153

Epoch: 6| Step: 8
Training loss: 5.468517712700682
Validation loss: 5.2730890304907865

Epoch: 6| Step: 9
Training loss: 6.169448259560378
Validation loss: 5.264722133548295

Epoch: 6| Step: 10
Training loss: 5.2239726539585405
Validation loss: 5.2568690458215706

Epoch: 6| Step: 11
Training loss: 4.973763293957734
Validation loss: 5.2490025435630665

Epoch: 6| Step: 12
Training loss: 5.127770024205705
Validation loss: 5.241209421192564

Epoch: 6| Step: 13
Training loss: 5.12929001801099
Validation loss: 5.233986911336053

Epoch: 8| Step: 0
Training loss: 5.781804238635743
Validation loss: 5.227069246656528

Epoch: 6| Step: 1
Training loss: 6.193377545010401
Validation loss: 5.220078442009743

Epoch: 6| Step: 2
Training loss: 5.424466000481601
Validation loss: 5.213243575693396

Epoch: 6| Step: 3
Training loss: 6.129743957648486
Validation loss: 5.2063350455309

Epoch: 6| Step: 4
Training loss: 5.17041788002451
Validation loss: 5.199475173874786

Epoch: 6| Step: 5
Training loss: 4.550484080291923
Validation loss: 5.192986953081343

Epoch: 6| Step: 6
Training loss: 5.570511780928647
Validation loss: 5.186375017244578

Epoch: 6| Step: 7
Training loss: 4.49382867439171
Validation loss: 5.179773359704266

Epoch: 6| Step: 8
Training loss: 5.099821813125902
Validation loss: 5.1739904527746905

Epoch: 6| Step: 9
Training loss: 4.799059839364047
Validation loss: 5.167484321293625

Epoch: 6| Step: 10
Training loss: 5.6438398419534455
Validation loss: 5.161723335717891

Epoch: 6| Step: 11
Training loss: 5.050141308006634
Validation loss: 5.1555243087420894

Epoch: 6| Step: 12
Training loss: 4.650582570324733
Validation loss: 5.1498715184057975

Epoch: 6| Step: 13
Training loss: 5.3984908502926405
Validation loss: 5.143963163770979

Epoch: 9| Step: 0
Training loss: 5.882338465504506
Validation loss: 5.13792439708768

Epoch: 6| Step: 1
Training loss: 5.7224129374469435
Validation loss: 5.131864788290096

Epoch: 6| Step: 2
Training loss: 5.443699601966048
Validation loss: 5.126284895688764

Epoch: 6| Step: 3
Training loss: 5.011855375549051
Validation loss: 5.120583096321931

Epoch: 6| Step: 4
Training loss: 5.0974302512163545
Validation loss: 5.115167287935485

Epoch: 6| Step: 5
Training loss: 5.682150536266782
Validation loss: 5.1095045658525065

Epoch: 6| Step: 6
Training loss: 4.808878539241325
Validation loss: 5.104269262826771

Epoch: 6| Step: 7
Training loss: 4.591986492854865
Validation loss: 5.098545181114627

Epoch: 6| Step: 8
Training loss: 6.047624408503568
Validation loss: 5.0939500453493025

Epoch: 6| Step: 9
Training loss: 4.856953793540545
Validation loss: 5.0884775595659795

Epoch: 6| Step: 10
Training loss: 4.9203820280297155
Validation loss: 5.083354897792081

Epoch: 6| Step: 11
Training loss: 4.662957397606645
Validation loss: 5.078610096772048

Epoch: 6| Step: 12
Training loss: 5.42148538118506
Validation loss: 5.073177960172523

Epoch: 6| Step: 13
Training loss: 4.752574624444427
Validation loss: 5.067684517229762

Epoch: 10| Step: 0
Training loss: 5.7186586132981585
Validation loss: 5.063143999244358

Epoch: 6| Step: 1
Training loss: 4.797378762101294
Validation loss: 5.057804237526316

Epoch: 6| Step: 2
Training loss: 5.622837244268867
Validation loss: 5.052927897015566

Epoch: 6| Step: 3
Training loss: 4.777624014418101
Validation loss: 5.0476228299688

Epoch: 6| Step: 4
Training loss: 4.435255960778597
Validation loss: 5.043398924705914

Epoch: 6| Step: 5
Training loss: 5.266511655691199
Validation loss: 5.038367881509232

Epoch: 6| Step: 6
Training loss: 6.014389584993487
Validation loss: 5.032880309645992

Epoch: 6| Step: 7
Training loss: 4.639550286894628
Validation loss: 5.02814310314396

Epoch: 6| Step: 8
Training loss: 3.9825820778376766
Validation loss: 5.024248388961552

Epoch: 6| Step: 9
Training loss: 5.396432065209737
Validation loss: 5.01975429647921

Epoch: 6| Step: 10
Training loss: 6.303724780314971
Validation loss: 5.013990252671609

Epoch: 6| Step: 11
Training loss: 4.925951235543362
Validation loss: 5.009933950708353

Epoch: 6| Step: 12
Training loss: 5.258897235859349
Validation loss: 5.005392091720886

Epoch: 6| Step: 13
Training loss: 4.527723953041559
Validation loss: 5.00004526753597

Epoch: 11| Step: 0
Training loss: 4.557774853477323
Validation loss: 4.995378265568412

Epoch: 6| Step: 1
Training loss: 4.790877479026464
Validation loss: 4.992060906959727

Epoch: 6| Step: 2
Training loss: 5.301497251523399
Validation loss: 4.987270522674409

Epoch: 6| Step: 3
Training loss: 5.012645371153176
Validation loss: 4.98160682590381

Epoch: 6| Step: 4
Training loss: 4.8916732103519545
Validation loss: 4.977173293162831

Epoch: 6| Step: 5
Training loss: 5.057594842855883
Validation loss: 4.973109382226774

Epoch: 6| Step: 6
Training loss: 4.762203601815194
Validation loss: 4.969238705063585

Epoch: 6| Step: 7
Training loss: 5.086998332265375
Validation loss: 4.964276121011987

Epoch: 6| Step: 8
Training loss: 5.298165223289697
Validation loss: 4.959721870366565

Epoch: 6| Step: 9
Training loss: 4.540112897502695
Validation loss: 4.954764045822587

Epoch: 6| Step: 10
Training loss: 6.1518129731360744
Validation loss: 4.950997747184597

Epoch: 6| Step: 11
Training loss: 4.855095944934712
Validation loss: 4.946193815408219

Epoch: 6| Step: 12
Training loss: 5.399770216114832
Validation loss: 4.941339376245857

Epoch: 6| Step: 13
Training loss: 5.3495011640776164
Validation loss: 4.936769165805482

Epoch: 12| Step: 0
Training loss: 4.999166419161759
Validation loss: 4.932615705223165

Epoch: 6| Step: 1
Training loss: 6.0342548376176905
Validation loss: 4.92859914527034

Epoch: 6| Step: 2
Training loss: 3.619301361615444
Validation loss: 4.924369769105902

Epoch: 6| Step: 3
Training loss: 5.297971718827507
Validation loss: 4.919683188392688

Epoch: 6| Step: 4
Training loss: 3.6280633544941896
Validation loss: 4.914792409440824

Epoch: 6| Step: 5
Training loss: 6.02822531324486
Validation loss: 4.91028870955483

Epoch: 6| Step: 6
Training loss: 6.006596435971781
Validation loss: 4.9054919910644825

Epoch: 6| Step: 7
Training loss: 5.710633234563157
Validation loss: 4.9003687959835815

Epoch: 6| Step: 8
Training loss: 4.996643083926878
Validation loss: 4.896503040149822

Epoch: 6| Step: 9
Training loss: 5.158179644553241
Validation loss: 4.891370560079659

Epoch: 6| Step: 10
Training loss: 4.752932396271316
Validation loss: 4.886186822349595

Epoch: 6| Step: 11
Training loss: 3.86770706539748
Validation loss: 4.881506270594009

Epoch: 6| Step: 12
Training loss: 4.954095306181985
Validation loss: 4.876461885802209

Epoch: 6| Step: 13
Training loss: 4.435050180660921
Validation loss: 4.871779135885836

Epoch: 13| Step: 0
Training loss: 5.600600155959501
Validation loss: 4.867634074632931

Epoch: 6| Step: 1
Training loss: 4.879281926714353
Validation loss: 4.863201715720337

Epoch: 6| Step: 2
Training loss: 4.915285185789488
Validation loss: 4.858584651477434

Epoch: 6| Step: 3
Training loss: 4.935055284432019
Validation loss: 4.853926831434131

Epoch: 6| Step: 4
Training loss: 5.44436755526133
Validation loss: 4.8493790599361715

Epoch: 6| Step: 5
Training loss: 4.9272052300867815
Validation loss: 4.843986554676047

Epoch: 6| Step: 6
Training loss: 3.817153342145318
Validation loss: 4.839088280403525

Epoch: 6| Step: 7
Training loss: 4.11334129934503
Validation loss: 4.834391883601975

Epoch: 6| Step: 8
Training loss: 5.419373212657199
Validation loss: 4.828978297476769

Epoch: 6| Step: 9
Training loss: 4.83085382374398
Validation loss: 4.8249046013185115

Epoch: 6| Step: 10
Training loss: 4.829407950499083
Validation loss: 4.820367797336539

Epoch: 6| Step: 11
Training loss: 6.019957411605902
Validation loss: 4.815237579505555

Epoch: 6| Step: 12
Training loss: 4.740973076712346
Validation loss: 4.809781879864466

Epoch: 6| Step: 13
Training loss: 4.583280297174798
Validation loss: 4.805653677044593

Epoch: 14| Step: 0
Training loss: 4.636661365285695
Validation loss: 4.800396516104965

Epoch: 6| Step: 1
Training loss: 4.891487606135434
Validation loss: 4.795396840072791

Epoch: 6| Step: 2
Training loss: 5.300507416817037
Validation loss: 4.790607744498135

Epoch: 6| Step: 3
Training loss: 5.682937301007948
Validation loss: 4.785750189011408

Epoch: 6| Step: 4
Training loss: 5.400179591548015
Validation loss: 4.780576571242806

Epoch: 6| Step: 5
Training loss: 5.002884986166927
Validation loss: 4.776004082529441

Epoch: 6| Step: 6
Training loss: 5.030848895841831
Validation loss: 4.7718767527119255

Epoch: 6| Step: 7
Training loss: 4.608184350326254
Validation loss: 4.766210567999225

Epoch: 6| Step: 8
Training loss: 4.013141502366641
Validation loss: 4.761893423838968

Epoch: 6| Step: 9
Training loss: 4.221145288977765
Validation loss: 4.7573834639746195

Epoch: 6| Step: 10
Training loss: 5.032640916468829
Validation loss: 4.753196076065066

Epoch: 6| Step: 11
Training loss: 4.964158149640637
Validation loss: 4.748277853930307

Epoch: 6| Step: 12
Training loss: 4.829108968175565
Validation loss: 4.744077805361927

Epoch: 6| Step: 13
Training loss: 4.6808037294322435
Validation loss: 4.738965585994058

Epoch: 15| Step: 0
Training loss: 4.001926435062293
Validation loss: 4.734426265224969

Epoch: 6| Step: 1
Training loss: 4.5538088440110185
Validation loss: 4.729650408277277

Epoch: 6| Step: 2
Training loss: 5.6348438171567805
Validation loss: 4.725052285956332

Epoch: 6| Step: 3
Training loss: 5.288247433043005
Validation loss: 4.720218135324466

Epoch: 6| Step: 4
Training loss: 5.005051731143335
Validation loss: 4.715929481041662

Epoch: 6| Step: 5
Training loss: 3.7606259161865783
Validation loss: 4.7113633266498445

Epoch: 6| Step: 6
Training loss: 5.081400310925429
Validation loss: 4.706206401134976

Epoch: 6| Step: 7
Training loss: 4.956047663942067
Validation loss: 4.702031478404354

Epoch: 6| Step: 8
Training loss: 5.736825196053835
Validation loss: 4.6971261091655245

Epoch: 6| Step: 9
Training loss: 4.818560462717193
Validation loss: 4.692030450913442

Epoch: 6| Step: 10
Training loss: 4.218628832171991
Validation loss: 4.686828904326048

Epoch: 6| Step: 11
Training loss: 4.110938873348783
Validation loss: 4.682384700791474

Epoch: 6| Step: 12
Training loss: 4.560018413238827
Validation loss: 4.677885681975739

Epoch: 6| Step: 13
Training loss: 5.392627642610391
Validation loss: 4.673704573723487

Epoch: 16| Step: 0
Training loss: 4.053958305396656
Validation loss: 4.668553969809784

Epoch: 6| Step: 1
Training loss: 4.796391664689054
Validation loss: 4.664473029407448

Epoch: 6| Step: 2
Training loss: 4.637230450387386
Validation loss: 4.659243617688274

Epoch: 6| Step: 3
Training loss: 4.947953758769961
Validation loss: 4.655315074890494

Epoch: 6| Step: 4
Training loss: 4.815563267356081
Validation loss: 4.6502086182424405

Epoch: 6| Step: 5
Training loss: 5.139163287743998
Validation loss: 4.645405987755716

Epoch: 6| Step: 6
Training loss: 4.167492339659792
Validation loss: 4.641450750692714

Epoch: 6| Step: 7
Training loss: 4.292060068611384
Validation loss: 4.637064380184839

Epoch: 6| Step: 8
Training loss: 4.964451303886183
Validation loss: 4.6315534058996715

Epoch: 6| Step: 9
Training loss: 4.762924279565138
Validation loss: 4.627135788873404

Epoch: 6| Step: 10
Training loss: 4.7074072723250815
Validation loss: 4.622580814868931

Epoch: 6| Step: 11
Training loss: 4.768947956181179
Validation loss: 4.6182881359659

Epoch: 6| Step: 12
Training loss: 5.078544716729325
Validation loss: 4.613799080895534

Epoch: 6| Step: 13
Training loss: 5.392233257602506
Validation loss: 4.607764149029799

Epoch: 17| Step: 0
Training loss: 4.557202752108812
Validation loss: 4.604382758729723

Epoch: 6| Step: 1
Training loss: 4.566214107912251
Validation loss: 4.600557071706168

Epoch: 6| Step: 2
Training loss: 4.103311104196496
Validation loss: 4.596019815562143

Epoch: 6| Step: 3
Training loss: 5.05489654344591
Validation loss: 4.5901423537065655

Epoch: 6| Step: 4
Training loss: 5.340288078166991
Validation loss: 4.585413992275976

Epoch: 6| Step: 5
Training loss: 5.541513177949828
Validation loss: 4.583373636010732

Epoch: 6| Step: 6
Training loss: 4.22072481629086
Validation loss: 4.5772727719904704

Epoch: 6| Step: 7
Training loss: 5.532965959663136
Validation loss: 4.571295216863247

Epoch: 6| Step: 8
Training loss: 4.682371462041956
Validation loss: 4.565484172747516

Epoch: 6| Step: 9
Training loss: 4.204485931724976
Validation loss: 4.559729479578905

Epoch: 6| Step: 10
Training loss: 4.64306288881617
Validation loss: 4.555316704921296

Epoch: 6| Step: 11
Training loss: 4.741920023732434
Validation loss: 4.5501625611885315

Epoch: 6| Step: 12
Training loss: 3.3835221530735327
Validation loss: 4.544901696020864

Epoch: 6| Step: 13
Training loss: 4.769853560720985
Validation loss: 4.539074230780793

Epoch: 18| Step: 0
Training loss: 4.128075349149629
Validation loss: 4.533729558813955

Epoch: 6| Step: 1
Training loss: 4.152769309169565
Validation loss: 4.529548693948395

Epoch: 6| Step: 2
Training loss: 4.8253477929826145
Validation loss: 4.525291260676767

Epoch: 6| Step: 3
Training loss: 4.727961360018308
Validation loss: 4.520686497332626

Epoch: 6| Step: 4
Training loss: 4.600285662198395
Validation loss: 4.516205078318288

Epoch: 6| Step: 5
Training loss: 4.241713130274254
Validation loss: 4.511566767258861

Epoch: 6| Step: 6
Training loss: 5.246578736657004
Validation loss: 4.506529203910504

Epoch: 6| Step: 7
Training loss: 3.762796313001761
Validation loss: 4.501011699402249

Epoch: 6| Step: 8
Training loss: 4.573149493408509
Validation loss: 4.496522937642769

Epoch: 6| Step: 9
Training loss: 5.518474415587
Validation loss: 4.49221699138304

Epoch: 6| Step: 10
Training loss: 3.8675459262165255
Validation loss: 4.487634836365427

Epoch: 6| Step: 11
Training loss: 3.861958615777179
Validation loss: 4.482789861551666

Epoch: 6| Step: 12
Training loss: 5.223128440812892
Validation loss: 4.477788165979402

Epoch: 6| Step: 13
Training loss: 5.567158494949263
Validation loss: 4.4727243387739435

Epoch: 19| Step: 0
Training loss: 4.602884980138094
Validation loss: 4.468127440722577

Epoch: 6| Step: 1
Training loss: 4.8214187218298035
Validation loss: 4.463894355695144

Epoch: 6| Step: 2
Training loss: 4.428484718071561
Validation loss: 4.459066788185691

Epoch: 6| Step: 3
Training loss: 5.030739325912564
Validation loss: 4.4548077532627905

Epoch: 6| Step: 4
Training loss: 5.358829392667469
Validation loss: 4.449962452219311

Epoch: 6| Step: 5
Training loss: 5.114489687226105
Validation loss: 4.44531521744022

Epoch: 6| Step: 6
Training loss: 5.247385373038065
Validation loss: 4.440542297523786

Epoch: 6| Step: 7
Training loss: 3.9796069768720446
Validation loss: 4.435572156185979

Epoch: 6| Step: 8
Training loss: 3.506638634234842
Validation loss: 4.430648098533525

Epoch: 6| Step: 9
Training loss: 4.055616445098804
Validation loss: 4.4257013774419605

Epoch: 6| Step: 10
Training loss: 4.479776290689528
Validation loss: 4.421464233730021

Epoch: 6| Step: 11
Training loss: 4.788400128159041
Validation loss: 4.416868613082711

Epoch: 6| Step: 12
Training loss: 4.878935570243164
Validation loss: 4.411906766411895

Epoch: 6| Step: 13
Training loss: 3.0094157912360693
Validation loss: 4.407198071220411

Epoch: 20| Step: 0
Training loss: 4.288331349907742
Validation loss: 4.402561312119831

Epoch: 6| Step: 1
Training loss: 3.9271851112405205
Validation loss: 4.397909978623261

Epoch: 6| Step: 2
Training loss: 3.990169007395622
Validation loss: 4.393785943019501

Epoch: 6| Step: 3
Training loss: 4.631958958445617
Validation loss: 4.389883266075824

Epoch: 6| Step: 4
Training loss: 4.469137961879724
Validation loss: 4.385448114398669

Epoch: 6| Step: 5
Training loss: 4.369595486492562
Validation loss: 4.381045823502579

Epoch: 6| Step: 6
Training loss: 4.589324895872805
Validation loss: 4.376768208991754

Epoch: 6| Step: 7
Training loss: 4.792041841276527
Validation loss: 4.371867266255705

Epoch: 6| Step: 8
Training loss: 5.3307247140105165
Validation loss: 4.367257087530712

Epoch: 6| Step: 9
Training loss: 4.16310987615537
Validation loss: 4.3619234573071095

Epoch: 6| Step: 10
Training loss: 4.744407172035564
Validation loss: 4.357414233059796

Epoch: 6| Step: 11
Training loss: 4.814641302737249
Validation loss: 4.352678376175686

Epoch: 6| Step: 12
Training loss: 4.090812269664151
Validation loss: 4.348717974875362

Epoch: 6| Step: 13
Training loss: 4.617992143825029
Validation loss: 4.343978436727379

Epoch: 21| Step: 0
Training loss: 4.4304301936086805
Validation loss: 4.338248851675069

Epoch: 6| Step: 1
Training loss: 4.214372186027627
Validation loss: 4.333566488204656

Epoch: 6| Step: 2
Training loss: 4.871735090849066
Validation loss: 4.328460019362682

Epoch: 6| Step: 3
Training loss: 4.67066536747984
Validation loss: 4.3240783723756415

Epoch: 6| Step: 4
Training loss: 4.240983936296482
Validation loss: 4.31924623986604

Epoch: 6| Step: 5
Training loss: 4.721913269009073
Validation loss: 4.31467773442289

Epoch: 6| Step: 6
Training loss: 4.362435309250165
Validation loss: 4.309787763487261

Epoch: 6| Step: 7
Training loss: 4.410186740442097
Validation loss: 4.30519245968389

Epoch: 6| Step: 8
Training loss: 4.40307112932247
Validation loss: 4.300580197011152

Epoch: 6| Step: 9
Training loss: 4.601763942693861
Validation loss: 4.295701707117806

Epoch: 6| Step: 10
Training loss: 4.361113389669958
Validation loss: 4.2905763827554155

Epoch: 6| Step: 11
Training loss: 3.9871145367040044
Validation loss: 4.28570675508655

Epoch: 6| Step: 12
Training loss: 4.772529178488225
Validation loss: 4.28103713274431

Epoch: 6| Step: 13
Training loss: 3.982005173760755
Validation loss: 4.276570828306233

Epoch: 22| Step: 0
Training loss: 3.9564861739355868
Validation loss: 4.271517678613407

Epoch: 6| Step: 1
Training loss: 4.426027538587945
Validation loss: 4.267129337040581

Epoch: 6| Step: 2
Training loss: 4.231569831965183
Validation loss: 4.262412252856066

Epoch: 6| Step: 3
Training loss: 4.484631042614198
Validation loss: 4.258058761334563

Epoch: 6| Step: 4
Training loss: 3.455771972268484
Validation loss: 4.253365549146887

Epoch: 6| Step: 5
Training loss: 4.620603791782495
Validation loss: 4.248642199664674

Epoch: 6| Step: 6
Training loss: 4.211024918648489
Validation loss: 4.24493403595804

Epoch: 6| Step: 7
Training loss: 4.083637355982504
Validation loss: 4.239810153351732

Epoch: 6| Step: 8
Training loss: 4.40918174228219
Validation loss: 4.235622921243718

Epoch: 6| Step: 9
Training loss: 5.086019067066258
Validation loss: 4.2304844361375755

Epoch: 6| Step: 10
Training loss: 4.660513545214406
Validation loss: 4.226206785077174

Epoch: 6| Step: 11
Training loss: 3.915512699547453
Validation loss: 4.221281596585052

Epoch: 6| Step: 12
Training loss: 5.334115249218314
Validation loss: 4.216459410679046

Epoch: 6| Step: 13
Training loss: 3.974459768567966
Validation loss: 4.211585887279687

Epoch: 23| Step: 0
Training loss: 4.5406302339821645
Validation loss: 4.206801573216889

Epoch: 6| Step: 1
Training loss: 4.528989451619518
Validation loss: 4.202017864019256

Epoch: 6| Step: 2
Training loss: 3.8983623979019675
Validation loss: 4.196757810560879

Epoch: 6| Step: 3
Training loss: 4.527179441852341
Validation loss: 4.192478238380835

Epoch: 6| Step: 4
Training loss: 4.812899288110558
Validation loss: 4.187866422648912

Epoch: 6| Step: 5
Training loss: 4.875723760626868
Validation loss: 4.18263607420195

Epoch: 6| Step: 6
Training loss: 4.582446249455676
Validation loss: 4.178182550840909

Epoch: 6| Step: 7
Training loss: 3.6630632872200284
Validation loss: 4.173065625238208

Epoch: 6| Step: 8
Training loss: 4.267358672132106
Validation loss: 4.167853714017734

Epoch: 6| Step: 9
Training loss: 3.853317882106474
Validation loss: 4.162738238782593

Epoch: 6| Step: 10
Training loss: 4.486936681557002
Validation loss: 4.158270017540668

Epoch: 6| Step: 11
Training loss: 3.9651607593402955
Validation loss: 4.153327564574308

Epoch: 6| Step: 12
Training loss: 3.379037914026161
Validation loss: 4.149016316582

Epoch: 6| Step: 13
Training loss: 4.613025732498293
Validation loss: 4.144083903166583

Epoch: 24| Step: 0
Training loss: 4.281104092304403
Validation loss: 4.1390271252639375

Epoch: 6| Step: 1
Training loss: 3.6835374392981843
Validation loss: 4.13476304943285

Epoch: 6| Step: 2
Training loss: 4.731419411855063
Validation loss: 4.1298754037962135

Epoch: 6| Step: 3
Training loss: 4.409228244991482
Validation loss: 4.124943048873941

Epoch: 6| Step: 4
Training loss: 4.275429437495139
Validation loss: 4.12019956991169

Epoch: 6| Step: 5
Training loss: 3.1892844796197406
Validation loss: 4.115468273688308

Epoch: 6| Step: 6
Training loss: 4.497458269944545
Validation loss: 4.111140076360532

Epoch: 6| Step: 7
Training loss: 4.150136940776444
Validation loss: 4.106194990574424

Epoch: 6| Step: 8
Training loss: 4.1523409939431115
Validation loss: 4.101786870723163

Epoch: 6| Step: 9
Training loss: 4.515358464911771
Validation loss: 4.09751420067279

Epoch: 6| Step: 10
Training loss: 4.634140056023112
Validation loss: 4.0931434084873

Epoch: 6| Step: 11
Training loss: 4.63967546709265
Validation loss: 4.088385486251996

Epoch: 6| Step: 12
Training loss: 4.088022666435335
Validation loss: 4.082862508968518

Epoch: 6| Step: 13
Training loss: 3.850636833332481
Validation loss: 4.077628074956286

Epoch: 25| Step: 0
Training loss: 4.187955006034373
Validation loss: 4.072559988236893

Epoch: 6| Step: 1
Training loss: 3.3075762448565795
Validation loss: 4.068375608033655

Epoch: 6| Step: 2
Training loss: 3.9849254751476177
Validation loss: 4.063649606660505

Epoch: 6| Step: 3
Training loss: 4.623038210939283
Validation loss: 4.058990410178705

Epoch: 6| Step: 4
Training loss: 3.743635371368612
Validation loss: 4.054262426852179

Epoch: 6| Step: 5
Training loss: 5.081789543156771
Validation loss: 4.048853485081978

Epoch: 6| Step: 6
Training loss: 4.088718263088337
Validation loss: 4.043863518766005

Epoch: 6| Step: 7
Training loss: 4.044787954989739
Validation loss: 4.039250500433853

Epoch: 6| Step: 8
Training loss: 4.270009676276864
Validation loss: 4.034603567269983

Epoch: 6| Step: 9
Training loss: 4.79722966675413
Validation loss: 4.030646641253631

Epoch: 6| Step: 10
Training loss: 3.9684099667479633
Validation loss: 4.025061280315191

Epoch: 6| Step: 11
Training loss: 3.6670245660604484
Validation loss: 4.020481206203459

Epoch: 6| Step: 12
Training loss: 4.3097578535629655
Validation loss: 4.01487143010212

Epoch: 6| Step: 13
Training loss: 4.043571626246038
Validation loss: 4.009239294778832

Epoch: 26| Step: 0
Training loss: 4.160174589161456
Validation loss: 4.003821018211223

Epoch: 6| Step: 1
Training loss: 4.472266887391734
Validation loss: 3.9997733568514597

Epoch: 6| Step: 2
Training loss: 4.173975954842869
Validation loss: 3.994658380479032

Epoch: 6| Step: 3
Training loss: 3.3393275882668734
Validation loss: 3.9898577098603463

Epoch: 6| Step: 4
Training loss: 4.44256603494574
Validation loss: 3.9845930251763195

Epoch: 6| Step: 5
Training loss: 3.8033856116894134
Validation loss: 3.9803717953619797

Epoch: 6| Step: 6
Training loss: 4.074333445323497
Validation loss: 3.9749545960462327

Epoch: 6| Step: 7
Training loss: 4.138201771023868
Validation loss: 3.9697760709871464

Epoch: 6| Step: 8
Training loss: 4.899349730025365
Validation loss: 3.9654002834146103

Epoch: 6| Step: 9
Training loss: 3.8907621428436174
Validation loss: 3.960128470819026

Epoch: 6| Step: 10
Training loss: 3.8264358219161556
Validation loss: 3.955746370855325

Epoch: 6| Step: 11
Training loss: 4.518557747830982
Validation loss: 3.949880884684268

Epoch: 6| Step: 12
Training loss: 3.5942296247115055
Validation loss: 3.9447344768855452

Epoch: 6| Step: 13
Training loss: 3.9044391554630966
Validation loss: 3.939839248508411

Epoch: 27| Step: 0
Training loss: 3.6158972265947904
Validation loss: 3.934578452532274

Epoch: 6| Step: 1
Training loss: 3.473984626239674
Validation loss: 3.930105825395517

Epoch: 6| Step: 2
Training loss: 3.6104062411331475
Validation loss: 3.9250448115032204

Epoch: 6| Step: 3
Training loss: 4.605080262187773
Validation loss: 3.919745932474521

Epoch: 6| Step: 4
Training loss: 3.8886809490257015
Validation loss: 3.915341146196204

Epoch: 6| Step: 5
Training loss: 4.396456175637018
Validation loss: 3.910413044497818

Epoch: 6| Step: 6
Training loss: 4.051570333582863
Validation loss: 3.9049761716967972

Epoch: 6| Step: 7
Training loss: 4.01983303349623
Validation loss: 3.8996864616669207

Epoch: 6| Step: 8
Training loss: 3.8012070294817466
Validation loss: 3.8952820671760553

Epoch: 6| Step: 9
Training loss: 3.034504509940562
Validation loss: 3.8914213782588702

Epoch: 6| Step: 10
Training loss: 4.634319915752792
Validation loss: 3.88706478450729

Epoch: 6| Step: 11
Training loss: 4.4155521546096725
Validation loss: 3.8823098791848594

Epoch: 6| Step: 12
Training loss: 4.635755250830814
Validation loss: 3.8776596540174872

Epoch: 6| Step: 13
Training loss: 3.958801777208663
Validation loss: 3.8725445669001366

Epoch: 28| Step: 0
Training loss: 4.098466093114136
Validation loss: 3.8667142533520114

Epoch: 6| Step: 1
Training loss: 3.8015726800383245
Validation loss: 3.8616305618563014

Epoch: 6| Step: 2
Training loss: 4.056729487922949
Validation loss: 3.8563005399766794

Epoch: 6| Step: 3
Training loss: 4.1099354176888525
Validation loss: 3.8518710473864743

Epoch: 6| Step: 4
Training loss: 3.863743460287769
Validation loss: 3.8471433759215494

Epoch: 6| Step: 5
Training loss: 3.6021015814691646
Validation loss: 3.842023466894767

Epoch: 6| Step: 6
Training loss: 3.324029973157759
Validation loss: 3.837810693479552

Epoch: 6| Step: 7
Training loss: 3.497947090808787
Validation loss: 3.832928546320785

Epoch: 6| Step: 8
Training loss: 4.131115628133375
Validation loss: 3.8282897187171696

Epoch: 6| Step: 9
Training loss: 3.5853606115315206
Validation loss: 3.8240282208027825

Epoch: 6| Step: 10
Training loss: 4.815731796476596
Validation loss: 3.818619845222708

Epoch: 6| Step: 11
Training loss: 3.679927745192678
Validation loss: 3.813849101021506

Epoch: 6| Step: 12
Training loss: 4.882352126734481
Validation loss: 3.8090469899248935

Epoch: 6| Step: 13
Training loss: 3.7841198331959975
Validation loss: 3.8042467346344515

Epoch: 29| Step: 0
Training loss: 4.318682454487734
Validation loss: 3.798885969360644

Epoch: 6| Step: 1
Training loss: 3.504903627994402
Validation loss: 3.7941071058916322

Epoch: 6| Step: 2
Training loss: 3.8771235892153095
Validation loss: 3.7892178742842924

Epoch: 6| Step: 3
Training loss: 4.108101186841449
Validation loss: 3.7843017873446874

Epoch: 6| Step: 4
Training loss: 3.747008592834475
Validation loss: 3.7788092562325786

Epoch: 6| Step: 5
Training loss: 4.118023608324836
Validation loss: 3.774425685218198

Epoch: 6| Step: 6
Training loss: 3.4821397942050973
Validation loss: 3.768949188991114

Epoch: 6| Step: 7
Training loss: 4.619514924382936
Validation loss: 3.764624350570101

Epoch: 6| Step: 8
Training loss: 3.6629363649564954
Validation loss: 3.7601147193181843

Epoch: 6| Step: 9
Training loss: 3.66111236428172
Validation loss: 3.755060215046881

Epoch: 6| Step: 10
Training loss: 4.3724015284901245
Validation loss: 3.750178481199101

Epoch: 6| Step: 11
Training loss: 3.849690135029336
Validation loss: 3.745285121927468

Epoch: 6| Step: 12
Training loss: 3.0897918966887965
Validation loss: 3.7401168711824173

Epoch: 6| Step: 13
Training loss: 3.9539590189483436
Validation loss: 3.7354632804303893

Epoch: 30| Step: 0
Training loss: 4.5304920483401405
Validation loss: 3.7307240450384245

Epoch: 6| Step: 1
Training loss: 3.873216895576366
Validation loss: 3.726205058252845

Epoch: 6| Step: 2
Training loss: 2.9218183705763106
Validation loss: 3.720348709060545

Epoch: 6| Step: 3
Training loss: 3.4614979309578375
Validation loss: 3.7156421272775004

Epoch: 6| Step: 4
Training loss: 3.2318968193750286
Validation loss: 3.711023140973138

Epoch: 6| Step: 5
Training loss: 3.9292493086720404
Validation loss: 3.7071555049479934

Epoch: 6| Step: 6
Training loss: 3.0590673399113504
Validation loss: 3.7021650097448475

Epoch: 6| Step: 7
Training loss: 4.3337005068426
Validation loss: 3.6976676248138634

Epoch: 6| Step: 8
Training loss: 3.9074356110893973
Validation loss: 3.6930126428870618

Epoch: 6| Step: 9
Training loss: 4.3763676276874754
Validation loss: 3.688490847639964

Epoch: 6| Step: 10
Training loss: 4.4750837776397585
Validation loss: 3.683449368497527

Epoch: 6| Step: 11
Training loss: 3.9007171509463503
Validation loss: 3.6787418735149537

Epoch: 6| Step: 12
Training loss: 3.0408639937409525
Validation loss: 3.6733603212596346

Epoch: 6| Step: 13
Training loss: 4.130737706910946
Validation loss: 3.6685424399380064

Epoch: 31| Step: 0
Training loss: 3.239431510548671
Validation loss: 3.6634750925623707

Epoch: 6| Step: 1
Training loss: 4.100465813203898
Validation loss: 3.6591613785362274

Epoch: 6| Step: 2
Training loss: 3.7012750696778074
Validation loss: 3.6546199458004094

Epoch: 6| Step: 3
Training loss: 4.21410673837493
Validation loss: 3.649963147051374

Epoch: 6| Step: 4
Training loss: 4.030238061504241
Validation loss: 3.6454342796280113

Epoch: 6| Step: 5
Training loss: 3.1928886789951005
Validation loss: 3.640154261027622

Epoch: 6| Step: 6
Training loss: 4.231439339981073
Validation loss: 3.6352845015817103

Epoch: 6| Step: 7
Training loss: 4.057776419295935
Validation loss: 3.630943249490707

Epoch: 6| Step: 8
Training loss: 3.150137544081347
Validation loss: 3.6258761125509102

Epoch: 6| Step: 9
Training loss: 3.810290212116305
Validation loss: 3.6210014169629297

Epoch: 6| Step: 10
Training loss: 3.7975591349313813
Validation loss: 3.616398109332927

Epoch: 6| Step: 11
Training loss: 3.269040306722684
Validation loss: 3.6125485338868892

Epoch: 6| Step: 12
Training loss: 3.9862064714947003
Validation loss: 3.6074340577772652

Epoch: 6| Step: 13
Training loss: 3.756363619134888
Validation loss: 3.60313178329513

Epoch: 32| Step: 0
Training loss: 3.480989873128931
Validation loss: 3.598643677900829

Epoch: 6| Step: 1
Training loss: 4.170125187154228
Validation loss: 3.594391453205133

Epoch: 6| Step: 2
Training loss: 3.727740791264912
Validation loss: 3.5895390261887705

Epoch: 6| Step: 3
Training loss: 4.085043231263771
Validation loss: 3.5851918804863985

Epoch: 6| Step: 4
Training loss: 3.735262585745834
Validation loss: 3.5798669431244026

Epoch: 6| Step: 5
Training loss: 3.6515869662143023
Validation loss: 3.5752184416540214

Epoch: 6| Step: 6
Training loss: 3.272036159157025
Validation loss: 3.5704208680337888

Epoch: 6| Step: 7
Training loss: 3.930956656423494
Validation loss: 3.566436121224649

Epoch: 6| Step: 8
Training loss: 4.017958857141963
Validation loss: 3.561429872461392

Epoch: 6| Step: 9
Training loss: 3.452119568700013
Validation loss: 3.5569369510199484

Epoch: 6| Step: 10
Training loss: 4.275295822883949
Validation loss: 3.5527583326441046

Epoch: 6| Step: 11
Training loss: 3.1473710973157822
Validation loss: 3.5483188463174833

Epoch: 6| Step: 12
Training loss: 3.8678641295687584
Validation loss: 3.5432541973024825

Epoch: 6| Step: 13
Training loss: 2.788762095111794
Validation loss: 3.538399741660205

Epoch: 33| Step: 0
Training loss: 2.735177320210144
Validation loss: 3.5344227066665392

Epoch: 6| Step: 1
Training loss: 3.6551410264806354
Validation loss: 3.5301198135373357

Epoch: 6| Step: 2
Training loss: 3.5863890519413486
Validation loss: 3.5263115212853418

Epoch: 6| Step: 3
Training loss: 4.027469488747008
Validation loss: 3.5220855464918572

Epoch: 6| Step: 4
Training loss: 3.9049486357648067
Validation loss: 3.517721136173577

Epoch: 6| Step: 5
Training loss: 4.179326749545646
Validation loss: 3.5129202603741914

Epoch: 6| Step: 6
Training loss: 3.689941244717229
Validation loss: 3.5086877038324897

Epoch: 6| Step: 7
Training loss: 2.9827537726670257
Validation loss: 3.503871887787019

Epoch: 6| Step: 8
Training loss: 3.928232458279023
Validation loss: 3.499862600081881

Epoch: 6| Step: 9
Training loss: 4.091121616044982
Validation loss: 3.495544015296527

Epoch: 6| Step: 10
Training loss: 4.038495317254959
Validation loss: 3.491064666315939

Epoch: 6| Step: 11
Training loss: 3.4678101512154678
Validation loss: 3.4862050039169428

Epoch: 6| Step: 12
Training loss: 3.2263150986226665
Validation loss: 3.4822423364125576

Epoch: 6| Step: 13
Training loss: 3.1525207242957594
Validation loss: 3.4782733493733082

Epoch: 34| Step: 0
Training loss: 3.2936705433130653
Validation loss: 3.4738773563370566

Epoch: 6| Step: 1
Training loss: 4.107629905870722
Validation loss: 3.4693461641865566

Epoch: 6| Step: 2
Training loss: 4.245444155979792
Validation loss: 3.464876090912692

Epoch: 6| Step: 3
Training loss: 3.155387836201849
Validation loss: 3.46055553845845

Epoch: 6| Step: 4
Training loss: 3.9893962261789295
Validation loss: 3.4563155358317053

Epoch: 6| Step: 5
Training loss: 4.032214382058074
Validation loss: 3.451698618516401

Epoch: 6| Step: 6
Training loss: 3.4873083793473176
Validation loss: 3.4472926670991395

Epoch: 6| Step: 7
Training loss: 3.373396881268892
Validation loss: 3.442933337331945

Epoch: 6| Step: 8
Training loss: 3.404633864828939
Validation loss: 3.438381590647088

Epoch: 6| Step: 9
Training loss: 3.0633076751043986
Validation loss: 3.434248432561261

Epoch: 6| Step: 10
Training loss: 3.8250156776256525
Validation loss: 3.4301390122584476

Epoch: 6| Step: 11
Training loss: 3.407759305809042
Validation loss: 3.4260291050121716

Epoch: 6| Step: 12
Training loss: 2.9817109525351
Validation loss: 3.4220790482856556

Epoch: 6| Step: 13
Training loss: 3.536109214183548
Validation loss: 3.417648445743332

Epoch: 35| Step: 0
Training loss: 3.747111670469894
Validation loss: 3.413631502206596

Epoch: 6| Step: 1
Training loss: 3.5799194900154774
Validation loss: 3.4093070587838525

Epoch: 6| Step: 2
Training loss: 3.8291183739827965
Validation loss: 3.405668226483339

Epoch: 6| Step: 3
Training loss: 3.4040860643186703
Validation loss: 3.4012895214360053

Epoch: 6| Step: 4
Training loss: 4.246309585490111
Validation loss: 3.3969027623665458

Epoch: 6| Step: 5
Training loss: 3.369269698614516
Validation loss: 3.392839121949946

Epoch: 6| Step: 6
Training loss: 3.1718515479813427
Validation loss: 3.389386714179038

Epoch: 6| Step: 7
Training loss: 3.226188139176448
Validation loss: 3.3847647562555987

Epoch: 6| Step: 8
Training loss: 3.4753071436546175
Validation loss: 3.381114366106159

Epoch: 6| Step: 9
Training loss: 3.894014039126472
Validation loss: 3.3769670099694062

Epoch: 6| Step: 10
Training loss: 3.451239852365333
Validation loss: 3.373037038205987

Epoch: 6| Step: 11
Training loss: 2.9541637067327122
Validation loss: 3.367990656136775

Epoch: 6| Step: 12
Training loss: 2.974368590981059
Validation loss: 3.3635114649344793

Epoch: 6| Step: 13
Training loss: 3.8184800072856264
Validation loss: 3.3597188137864715

Epoch: 36| Step: 0
Training loss: 3.331592311770476
Validation loss: 3.3557285005500805

Epoch: 6| Step: 1
Training loss: 3.329402338183896
Validation loss: 3.352209294056481

Epoch: 6| Step: 2
Training loss: 2.8967869421158152
Validation loss: 3.3489459894327553

Epoch: 6| Step: 3
Training loss: 3.755045992674989
Validation loss: 3.3457571404990953

Epoch: 6| Step: 4
Training loss: 3.593835050156979
Validation loss: 3.340416598503001

Epoch: 6| Step: 5
Training loss: 3.2274839549474694
Validation loss: 3.3366736166427846

Epoch: 6| Step: 6
Training loss: 3.1779660525564415
Validation loss: 3.333423176190464

Epoch: 6| Step: 7
Training loss: 3.6509321616271797
Validation loss: 3.3300825639174962

Epoch: 6| Step: 8
Training loss: 3.783449904428667
Validation loss: 3.3263589295321405

Epoch: 6| Step: 9
Training loss: 4.046811372116835
Validation loss: 3.3229385104203826

Epoch: 6| Step: 10
Training loss: 2.5629220010272666
Validation loss: 3.3183274502385127

Epoch: 6| Step: 11
Training loss: 3.449270729718752
Validation loss: 3.3145022548795042

Epoch: 6| Step: 12
Training loss: 3.9203000496841085
Validation loss: 3.310294706779999

Epoch: 6| Step: 13
Training loss: 3.5536992638153277
Validation loss: 3.306271641545656

Epoch: 37| Step: 0
Training loss: 4.058660717761391
Validation loss: 3.3018834307277323

Epoch: 6| Step: 1
Training loss: 3.247920030971805
Validation loss: 3.2979401469323557

Epoch: 6| Step: 2
Training loss: 3.743566462072903
Validation loss: 3.2940364869889254

Epoch: 6| Step: 3
Training loss: 2.810258777689878
Validation loss: 3.2905121279465313

Epoch: 6| Step: 4
Training loss: 2.856135967490855
Validation loss: 3.287126286856593

Epoch: 6| Step: 5
Training loss: 3.6684052651432957
Validation loss: 3.2856606645434145

Epoch: 6| Step: 6
Training loss: 3.081731087847013
Validation loss: 3.281329393183226

Epoch: 6| Step: 7
Training loss: 3.048153965843475
Validation loss: 3.27593743300475

Epoch: 6| Step: 8
Training loss: 2.9509110805722925
Validation loss: 3.2709425808749346

Epoch: 6| Step: 9
Training loss: 3.7715004318582976
Validation loss: 3.266381001308361

Epoch: 6| Step: 10
Training loss: 3.2607752366174814
Validation loss: 3.2632145845695013

Epoch: 6| Step: 11
Training loss: 3.153016804406864
Validation loss: 3.259411487271489

Epoch: 6| Step: 12
Training loss: 4.033774598208888
Validation loss: 3.2563019565920617

Epoch: 6| Step: 13
Training loss: 3.7967679985887237
Validation loss: 3.253694683889011

Epoch: 38| Step: 0
Training loss: 2.6700182257612433
Validation loss: 3.2493781937685418

Epoch: 6| Step: 1
Training loss: 3.2777941715958394
Validation loss: 3.2466340118452717

Epoch: 6| Step: 2
Training loss: 3.1369032942093833
Validation loss: 3.2419632799680023

Epoch: 6| Step: 3
Training loss: 3.1909333232343853
Validation loss: 3.2376391760173076

Epoch: 6| Step: 4
Training loss: 3.466236337826723
Validation loss: 3.233390154453991

Epoch: 6| Step: 5
Training loss: 3.065364043379648
Validation loss: 3.229538709476334

Epoch: 6| Step: 6
Training loss: 3.683064007010512
Validation loss: 3.2263557176932425

Epoch: 6| Step: 7
Training loss: 3.2147158925250796
Validation loss: 3.2230982991685044

Epoch: 6| Step: 8
Training loss: 3.2656801940050104
Validation loss: 3.218189227688826

Epoch: 6| Step: 9
Training loss: 3.8318067634615613
Validation loss: 3.2146114915927693

Epoch: 6| Step: 10
Training loss: 3.7675383839110603
Validation loss: 3.2116258908903306

Epoch: 6| Step: 11
Training loss: 3.2955244290108108
Validation loss: 3.2072825919941086

Epoch: 6| Step: 12
Training loss: 3.6160496679118865
Validation loss: 3.203436313790559

Epoch: 6| Step: 13
Training loss: 3.4060727694539104
Validation loss: 3.1991513423315827

Epoch: 39| Step: 0
Training loss: 3.119951213308518
Validation loss: 3.1954715505746285

Epoch: 6| Step: 1
Training loss: 3.365779856853357
Validation loss: 3.1914494922810235

Epoch: 6| Step: 2
Training loss: 3.8003072413667005
Validation loss: 3.187965234451239

Epoch: 6| Step: 3
Training loss: 3.5211702144251813
Validation loss: 3.1840205930302683

Epoch: 6| Step: 4
Training loss: 3.2310917356237523
Validation loss: 3.1807065013583706

Epoch: 6| Step: 5
Training loss: 3.3570335567502836
Validation loss: 3.177298508325058

Epoch: 6| Step: 6
Training loss: 3.907082064701859
Validation loss: 3.173413597228529

Epoch: 6| Step: 7
Training loss: 2.5981128262695066
Validation loss: 3.1696708800767617

Epoch: 6| Step: 8
Training loss: 3.582481963277961
Validation loss: 3.165557734084511

Epoch: 6| Step: 9
Training loss: 3.0316367934902826
Validation loss: 3.163390501897243

Epoch: 6| Step: 10
Training loss: 2.954710519621295
Validation loss: 3.1582342778141688

Epoch: 6| Step: 11
Training loss: 3.2089225591276764
Validation loss: 3.154191784441641

Epoch: 6| Step: 12
Training loss: 2.6410453506961202
Validation loss: 3.1511072115215653

Epoch: 6| Step: 13
Training loss: 3.731787387609755
Validation loss: 3.1480284520583792

Epoch: 40| Step: 0
Training loss: 3.796237468177527
Validation loss: 3.14379412934189

Epoch: 6| Step: 1
Training loss: 3.5593916064537297
Validation loss: 3.1403501652904002

Epoch: 6| Step: 2
Training loss: 3.523762554729569
Validation loss: 3.1372094378233313

Epoch: 6| Step: 3
Training loss: 3.5052099960043575
Validation loss: 3.133074298425323

Epoch: 6| Step: 4
Training loss: 3.4797917487874472
Validation loss: 3.129207017200717

Epoch: 6| Step: 5
Training loss: 3.1722150347870035
Validation loss: 3.1254298613855913

Epoch: 6| Step: 6
Training loss: 2.3073456516078497
Validation loss: 3.122067906984626

Epoch: 6| Step: 7
Training loss: 2.670010814287666
Validation loss: 3.117857030161999

Epoch: 6| Step: 8
Training loss: 3.2455556265095913
Validation loss: 3.11559238907797

Epoch: 6| Step: 9
Training loss: 2.728768928801186
Validation loss: 3.1132267365255912

Epoch: 6| Step: 10
Training loss: 3.0749988462864644
Validation loss: 3.1089190175906167

Epoch: 6| Step: 11
Training loss: 3.822282716053064
Validation loss: 3.1047467110114697

Epoch: 6| Step: 12
Training loss: 3.394359869630877
Validation loss: 3.1018186858973045

Epoch: 6| Step: 13
Training loss: 3.040784176524721
Validation loss: 3.0986693218110943

Epoch: 41| Step: 0
Training loss: 3.5217429947844683
Validation loss: 3.095633938213784

Epoch: 6| Step: 1
Training loss: 2.91905802831051
Validation loss: 3.0922326628072816

Epoch: 6| Step: 2
Training loss: 3.013448137048756
Validation loss: 3.089812524945601

Epoch: 6| Step: 3
Training loss: 2.297031319417577
Validation loss: 3.0867638105371507

Epoch: 6| Step: 4
Training loss: 3.5556265125214446
Validation loss: 3.0838395725534795

Epoch: 6| Step: 5
Training loss: 3.011551710115195
Validation loss: 3.080260825115723

Epoch: 6| Step: 6
Training loss: 3.6266738710144892
Validation loss: 3.07769596834246

Epoch: 6| Step: 7
Training loss: 3.9243653434273735
Validation loss: 3.0736728492125525

Epoch: 6| Step: 8
Training loss: 3.423244393916531
Validation loss: 3.0704398092418494

Epoch: 6| Step: 9
Training loss: 3.419983362748575
Validation loss: 3.067748587084635

Epoch: 6| Step: 10
Training loss: 2.487008192815044
Validation loss: 3.0637936616979897

Epoch: 6| Step: 11
Training loss: 3.155711515211162
Validation loss: 3.0598392475929233

Epoch: 6| Step: 12
Training loss: 3.591582863900222
Validation loss: 3.0562038966825056

Epoch: 6| Step: 13
Training loss: 2.634011377690912
Validation loss: 3.053006827736534

Epoch: 42| Step: 0
Training loss: 3.9196394061623603
Validation loss: 3.049897466822598

Epoch: 6| Step: 1
Training loss: 2.397931669990893
Validation loss: 3.0471626602405513

Epoch: 6| Step: 2
Training loss: 3.28844513161874
Validation loss: 3.0435749538879238

Epoch: 6| Step: 3
Training loss: 2.8333200192606145
Validation loss: 3.0412931125782006

Epoch: 6| Step: 4
Training loss: 3.065128055156397
Validation loss: 3.0389056063100583

Epoch: 6| Step: 5
Training loss: 3.4413225594605272
Validation loss: 3.0357572467018694

Epoch: 6| Step: 6
Training loss: 3.4996204170387526
Validation loss: 3.0327486220479756

Epoch: 6| Step: 7
Training loss: 2.8420931841870596
Validation loss: 3.029430721760303

Epoch: 6| Step: 8
Training loss: 2.942759897686131
Validation loss: 3.0270816550060924

Epoch: 6| Step: 9
Training loss: 2.913352409346896
Validation loss: 3.024604920263244

Epoch: 6| Step: 10
Training loss: 3.5475831522212173
Validation loss: 3.0205004716963417

Epoch: 6| Step: 11
Training loss: 3.272002349386328
Validation loss: 3.0228947851404313

Epoch: 6| Step: 12
Training loss: 2.8544988694114406
Validation loss: 3.023636028596181

Epoch: 6| Step: 13
Training loss: 3.3263851865687215
Validation loss: 3.0363359944653707

Epoch: 43| Step: 0
Training loss: 3.095299843112847
Validation loss: 3.0108083758325908

Epoch: 6| Step: 1
Training loss: 3.035974969009325
Validation loss: 3.006942781815474

Epoch: 6| Step: 2
Training loss: 2.5961498510433225
Validation loss: 3.0087059378478105

Epoch: 6| Step: 3
Training loss: 3.5889822473052395
Validation loss: 3.01422626952862

Epoch: 6| Step: 4
Training loss: 2.7701731208383498
Validation loss: 3.034117911775742

Epoch: 6| Step: 5
Training loss: 2.935165776472596
Validation loss: 3.0082617486131333

Epoch: 6| Step: 6
Training loss: 2.692375551144089
Validation loss: 2.9996318458788145

Epoch: 6| Step: 7
Training loss: 3.378722927875568
Validation loss: 2.99475177683041

Epoch: 6| Step: 8
Training loss: 3.423625341613233
Validation loss: 2.9975686287254995

Epoch: 6| Step: 9
Training loss: 3.9076757651914082
Validation loss: 2.9904365163924487

Epoch: 6| Step: 10
Training loss: 3.2821186187527527
Validation loss: 2.9862141825509436

Epoch: 6| Step: 11
Training loss: 2.717058729966992
Validation loss: 2.981524305966273

Epoch: 6| Step: 12
Training loss: 3.1375398428161496
Validation loss: 2.9784391666458787

Epoch: 6| Step: 13
Training loss: 3.1295291885353187
Validation loss: 2.978547469839157

Epoch: 44| Step: 0
Training loss: 2.9548257441383337
Validation loss: 2.9761194147981547

Epoch: 6| Step: 1
Training loss: 2.7600062994954184
Validation loss: 2.9710455468683006

Epoch: 6| Step: 2
Training loss: 2.9372649403126445
Validation loss: 2.9682337395697007

Epoch: 6| Step: 3
Training loss: 3.4169325996429794
Validation loss: 2.9650730451492553

Epoch: 6| Step: 4
Training loss: 3.7170570430817222
Validation loss: 2.9636719892347703

Epoch: 6| Step: 5
Training loss: 3.4239133572890914
Validation loss: 2.960648119095781

Epoch: 6| Step: 6
Training loss: 2.495225065702395
Validation loss: 2.9567505179567344

Epoch: 6| Step: 7
Training loss: 3.4772127036378335
Validation loss: 2.9540672481004635

Epoch: 6| Step: 8
Training loss: 2.6332352748753576
Validation loss: 2.951638804806288

Epoch: 6| Step: 9
Training loss: 3.0825225863901577
Validation loss: 2.94759212545136

Epoch: 6| Step: 10
Training loss: 2.497579547275863
Validation loss: 2.945465475168975

Epoch: 6| Step: 11
Training loss: 3.0632732738574906
Validation loss: 2.943405518890465

Epoch: 6| Step: 12
Training loss: 3.140871256572961
Validation loss: 2.9408255346217693

Epoch: 6| Step: 13
Training loss: 3.441271429636029
Validation loss: 2.9371230106719612

Epoch: 45| Step: 0
Training loss: 2.9895809762836874
Validation loss: 2.934346786000467

Epoch: 6| Step: 1
Training loss: 3.0779260871246947
Validation loss: 2.9318439871414266

Epoch: 6| Step: 2
Training loss: 2.7066348887991243
Validation loss: 2.929191811907205

Epoch: 6| Step: 3
Training loss: 2.8159843054680858
Validation loss: 2.9273903076009686

Epoch: 6| Step: 4
Training loss: 3.2468128716084315
Validation loss: 2.923407508020711

Epoch: 6| Step: 5
Training loss: 2.338742718341202
Validation loss: 2.921441996376227

Epoch: 6| Step: 6
Training loss: 2.9453102334416013
Validation loss: 2.9198771291512973

Epoch: 6| Step: 7
Training loss: 3.3484707601500108
Validation loss: 2.9157693481693014

Epoch: 6| Step: 8
Training loss: 3.3007211532990692
Validation loss: 2.91493625172027

Epoch: 6| Step: 9
Training loss: 2.8111243380443547
Validation loss: 2.910840336830967

Epoch: 6| Step: 10
Training loss: 3.4326506655106437
Validation loss: 2.90772238729605

Epoch: 6| Step: 11
Training loss: 2.884212164451553
Validation loss: 2.904551238580982

Epoch: 6| Step: 12
Training loss: 3.476963251568643
Validation loss: 2.9032231706076908

Epoch: 6| Step: 13
Training loss: 3.2609239535011514
Validation loss: 2.900262825116476

Epoch: 46| Step: 0
Training loss: 3.182938846557963
Validation loss: 2.899915717533496

Epoch: 6| Step: 1
Training loss: 2.9419698593182777
Validation loss: 2.8959273636369094

Epoch: 6| Step: 2
Training loss: 3.0453355861706704
Validation loss: 2.8930475754265847

Epoch: 6| Step: 3
Training loss: 2.7655434461735258
Validation loss: 2.8895183885056004

Epoch: 6| Step: 4
Training loss: 3.2858473768586376
Validation loss: 2.8873451251926348

Epoch: 6| Step: 5
Training loss: 3.354224226242502
Validation loss: 2.886407872427413

Epoch: 6| Step: 6
Training loss: 2.8507890044148954
Validation loss: 2.8823282167584785

Epoch: 6| Step: 7
Training loss: 2.821131706140916
Validation loss: 2.8807013551294522

Epoch: 6| Step: 8
Training loss: 2.037274976602753
Validation loss: 2.8782699482190974

Epoch: 6| Step: 9
Training loss: 3.3972721601135834
Validation loss: 2.8752703885073454

Epoch: 6| Step: 10
Training loss: 3.034417139737002
Validation loss: 2.8733113485042514

Epoch: 6| Step: 11
Training loss: 3.2504753352063998
Validation loss: 2.871577271036623

Epoch: 6| Step: 12
Training loss: 2.925244189119774
Validation loss: 2.8685213290117475

Epoch: 6| Step: 13
Training loss: 3.189995181758674
Validation loss: 2.867505325316872

Epoch: 47| Step: 0
Training loss: 2.7932500950953174
Validation loss: 2.8640346533376655

Epoch: 6| Step: 1
Training loss: 3.0681425779254465
Validation loss: 2.862113218260227

Epoch: 6| Step: 2
Training loss: 3.0710240284592407
Validation loss: 2.8593029853910257

Epoch: 6| Step: 3
Training loss: 3.5247997686994688
Validation loss: 2.856903550650658

Epoch: 6| Step: 4
Training loss: 2.392451168506677
Validation loss: 2.8547989128767086

Epoch: 6| Step: 5
Training loss: 3.1382610477085002
Validation loss: 2.8523677437980854

Epoch: 6| Step: 6
Training loss: 2.826570874186479
Validation loss: 2.8506251357823658

Epoch: 6| Step: 7
Training loss: 3.1294325863290187
Validation loss: 2.8466651841977955

Epoch: 6| Step: 8
Training loss: 3.473510225249396
Validation loss: 2.8464550525112453

Epoch: 6| Step: 9
Training loss: 3.1087644351367936
Validation loss: 2.8443196463236777

Epoch: 6| Step: 10
Training loss: 2.858816623887176
Validation loss: 2.8429338870104015

Epoch: 6| Step: 11
Training loss: 2.763760989204826
Validation loss: 2.840742022485825

Epoch: 6| Step: 12
Training loss: 3.0198761223655506
Validation loss: 2.8355085680827727

Epoch: 6| Step: 13
Training loss: 2.507244295296896
Validation loss: 2.834378320929654

Epoch: 48| Step: 0
Training loss: 2.9766954607565865
Validation loss: 2.8322482883114732

Epoch: 6| Step: 1
Training loss: 2.595319709561122
Validation loss: 2.8307674490765296

Epoch: 6| Step: 2
Training loss: 2.957342134004551
Validation loss: 2.8291120545439936

Epoch: 6| Step: 3
Training loss: 2.960907706963815
Validation loss: 2.8283526081364303

Epoch: 6| Step: 4
Training loss: 3.1380788626258393
Validation loss: 2.827413662694955

Epoch: 6| Step: 5
Training loss: 2.684684343022002
Validation loss: 2.824964791475168

Epoch: 6| Step: 6
Training loss: 3.449406481566538
Validation loss: 2.8227566090682874

Epoch: 6| Step: 7
Training loss: 3.3433068953356533
Validation loss: 2.8223127772562306

Epoch: 6| Step: 8
Training loss: 2.450173126643279
Validation loss: 2.8199506053701904

Epoch: 6| Step: 9
Training loss: 2.215629532436798
Validation loss: 2.8169882671638637

Epoch: 6| Step: 10
Training loss: 3.093358795401038
Validation loss: 2.8166633026584496

Epoch: 6| Step: 11
Training loss: 3.52025140471424
Validation loss: 2.8158579101649006

Epoch: 6| Step: 12
Training loss: 3.330782406967756
Validation loss: 2.813131628813353

Epoch: 6| Step: 13
Training loss: 2.4167078825845074
Validation loss: 2.8111687794609934

Epoch: 49| Step: 0
Training loss: 3.0111737382795587
Validation loss: 2.809485098181098

Epoch: 6| Step: 1
Training loss: 2.7124589943313344
Validation loss: 2.810667656129534

Epoch: 6| Step: 2
Training loss: 3.0335370117665965
Validation loss: 2.814885519442873

Epoch: 6| Step: 3
Training loss: 3.2610432730584495
Validation loss: 2.8139636152093668

Epoch: 6| Step: 4
Training loss: 2.9651021932710053
Validation loss: 2.805443551143264

Epoch: 6| Step: 5
Training loss: 3.377144591340488
Validation loss: 2.8025645489253495

Epoch: 6| Step: 6
Training loss: 2.395891360257441
Validation loss: 2.798418326057478

Epoch: 6| Step: 7
Training loss: 2.565810879143141
Validation loss: 2.794545658150147

Epoch: 6| Step: 8
Training loss: 3.3584590639064738
Validation loss: 2.794371893134538

Epoch: 6| Step: 9
Training loss: 3.0084319826754875
Validation loss: 2.7922794656350125

Epoch: 6| Step: 10
Training loss: 2.7359359209632346
Validation loss: 2.7915769439247295

Epoch: 6| Step: 11
Training loss: 2.7872610391734725
Validation loss: 2.790833360697246

Epoch: 6| Step: 12
Training loss: 2.512042224990798
Validation loss: 2.7883745728852083

Epoch: 6| Step: 13
Training loss: 3.2174051678960636
Validation loss: 2.786190474045328

Epoch: 50| Step: 0
Training loss: 3.4588029534932674
Validation loss: 2.7832914282638486

Epoch: 6| Step: 1
Training loss: 2.4956903504781938
Validation loss: 2.780896067869556

Epoch: 6| Step: 2
Training loss: 2.497390338678186
Validation loss: 2.7771112744432207

Epoch: 6| Step: 3
Training loss: 3.3769310972025925
Validation loss: 2.775147547536663

Epoch: 6| Step: 4
Training loss: 3.1906193448277196
Validation loss: 2.771228408373761

Epoch: 6| Step: 5
Training loss: 2.4932468757667197
Validation loss: 2.76937161278322

Epoch: 6| Step: 6
Training loss: 2.659952074637375
Validation loss: 2.769956124826997

Epoch: 6| Step: 7
Training loss: 3.313631548237611
Validation loss: 2.7686958146946474

Epoch: 6| Step: 8
Training loss: 2.726058957185137
Validation loss: 2.7728966987133568

Epoch: 6| Step: 9
Training loss: 2.7066533869462988
Validation loss: 2.7696249820000878

Epoch: 6| Step: 10
Training loss: 3.1498652172014667
Validation loss: 2.7671299519685633

Epoch: 6| Step: 11
Training loss: 2.3336404076104937
Validation loss: 2.7590942142029338

Epoch: 6| Step: 12
Training loss: 3.4725741364905156
Validation loss: 2.7577823136562425

Epoch: 6| Step: 13
Training loss: 2.5201201467785657
Validation loss: 2.755415727757582

Epoch: 51| Step: 0
Training loss: 3.279303563777425
Validation loss: 2.7543364493189197

Epoch: 6| Step: 1
Training loss: 3.141999053236189
Validation loss: 2.7569919190268193

Epoch: 6| Step: 2
Training loss: 2.340131280264958
Validation loss: 2.757288882190858

Epoch: 6| Step: 3
Training loss: 2.8797150799428737
Validation loss: 2.757180765648099

Epoch: 6| Step: 4
Training loss: 2.9456591010107864
Validation loss: 2.7563277328036957

Epoch: 6| Step: 5
Training loss: 3.1104038395822635
Validation loss: 2.754205709999191

Epoch: 6| Step: 6
Training loss: 3.2654643772175733
Validation loss: 2.7514503295840567

Epoch: 6| Step: 7
Training loss: 2.686056059683364
Validation loss: 2.746922302417451

Epoch: 6| Step: 8
Training loss: 3.0696141659709757
Validation loss: 2.744973993971483

Epoch: 6| Step: 9
Training loss: 2.8867088389968565
Validation loss: 2.7425551498603324

Epoch: 6| Step: 10
Training loss: 2.494140816182941
Validation loss: 2.7389877793688346

Epoch: 6| Step: 11
Training loss: 2.7650366550657917
Validation loss: 2.7369668502361115

Epoch: 6| Step: 12
Training loss: 2.9189288131538538
Validation loss: 2.735412197491921

Epoch: 6| Step: 13
Training loss: 2.4889564735833725
Validation loss: 2.7324494475690684

Epoch: 52| Step: 0
Training loss: 2.8068641143567357
Validation loss: 2.731297808708109

Epoch: 6| Step: 1
Training loss: 2.565690821966064
Validation loss: 2.728474687011137

Epoch: 6| Step: 2
Training loss: 2.7405287896453485
Validation loss: 2.7256260295686072

Epoch: 6| Step: 3
Training loss: 3.363426568661174
Validation loss: 2.7255468653893584

Epoch: 6| Step: 4
Training loss: 3.0543966715874977
Validation loss: 2.7228713459153298

Epoch: 6| Step: 5
Training loss: 2.929920238151324
Validation loss: 2.721554898733119

Epoch: 6| Step: 6
Training loss: 2.98052650485695
Validation loss: 2.7217415176520583

Epoch: 6| Step: 7
Training loss: 3.1049483337035193
Validation loss: 2.7202836019261754

Epoch: 6| Step: 8
Training loss: 2.8666900286350945
Validation loss: 2.719591485532365

Epoch: 6| Step: 9
Training loss: 2.491467506418812
Validation loss: 2.719936126725701

Epoch: 6| Step: 10
Training loss: 2.5933472653907947
Validation loss: 2.72024031968228

Epoch: 6| Step: 11
Training loss: 2.570155362134798
Validation loss: 2.721891597751213

Epoch: 6| Step: 12
Training loss: 2.7494155956409556
Validation loss: 2.722232678019194

Epoch: 6| Step: 13
Training loss: 3.1819824634598652
Validation loss: 2.7176612632557693

Epoch: 53| Step: 0
Training loss: 3.4476154268788206
Validation loss: 2.7157658755888883

Epoch: 6| Step: 1
Training loss: 3.1092244020730138
Validation loss: 2.7135190956704838

Epoch: 6| Step: 2
Training loss: 2.674977514582455
Validation loss: 2.71009960289471

Epoch: 6| Step: 3
Training loss: 2.036221567684882
Validation loss: 2.7093910694929066

Epoch: 6| Step: 4
Training loss: 3.2068950517288113
Validation loss: 2.705585660729966

Epoch: 6| Step: 5
Training loss: 2.6181580523173276
Validation loss: 2.7036820941968216

Epoch: 6| Step: 6
Training loss: 2.9418160405115303
Validation loss: 2.7031682328899507

Epoch: 6| Step: 7
Training loss: 2.6785720861524958
Validation loss: 2.703751890076312

Epoch: 6| Step: 8
Training loss: 2.4824701840241894
Validation loss: 2.6990369044510194

Epoch: 6| Step: 9
Training loss: 2.9077884334857607
Validation loss: 2.696206163059738

Epoch: 6| Step: 10
Training loss: 2.9322918112141023
Validation loss: 2.694261561629185

Epoch: 6| Step: 11
Training loss: 2.445637349254256
Validation loss: 2.693076122752541

Epoch: 6| Step: 12
Training loss: 3.3374960338554676
Validation loss: 2.7036913239932554

Epoch: 6| Step: 13
Training loss: 2.762680296386816
Validation loss: 2.700429583154022

Epoch: 54| Step: 0
Training loss: 2.4707415310132563
Validation loss: 2.690900484721748

Epoch: 6| Step: 1
Training loss: 2.842389294725653
Validation loss: 2.6863912764590703

Epoch: 6| Step: 2
Training loss: 3.1919103292045374
Validation loss: 2.6876468618320013

Epoch: 6| Step: 3
Training loss: 2.5779714827636053
Validation loss: 2.6861995093593167

Epoch: 6| Step: 4
Training loss: 2.530658417318798
Validation loss: 2.6865886651194106

Epoch: 6| Step: 5
Training loss: 2.884005168047757
Validation loss: 2.6886632161460784

Epoch: 6| Step: 6
Training loss: 2.8568023614989566
Validation loss: 2.68657248408443

Epoch: 6| Step: 7
Training loss: 2.820653667578981
Validation loss: 2.6861268756968846

Epoch: 6| Step: 8
Training loss: 2.5828212620193653
Validation loss: 2.6833740470198495

Epoch: 6| Step: 9
Training loss: 2.3843895867977696
Validation loss: 2.683319003696761

Epoch: 6| Step: 10
Training loss: 3.4432929038535938
Validation loss: 2.6819557843812825

Epoch: 6| Step: 11
Training loss: 2.3471105261817216
Validation loss: 2.6807770437161764

Epoch: 6| Step: 12
Training loss: 3.3766947129475953
Validation loss: 2.6780313428706966

Epoch: 6| Step: 13
Training loss: 2.9466917011296494
Validation loss: 2.675619464818939

Epoch: 55| Step: 0
Training loss: 3.0576076745001326
Validation loss: 2.673147549150955

Epoch: 6| Step: 1
Training loss: 3.051784218635757
Validation loss: 2.671301454849693

Epoch: 6| Step: 2
Training loss: 2.652172054975298
Validation loss: 2.6685579616471915

Epoch: 6| Step: 3
Training loss: 3.185249731073562
Validation loss: 2.6658524422548484

Epoch: 6| Step: 4
Training loss: 2.5057863030070804
Validation loss: 2.6672673939540488

Epoch: 6| Step: 5
Training loss: 3.1489557260966676
Validation loss: 2.6658081272035035

Epoch: 6| Step: 6
Training loss: 3.046676864050474
Validation loss: 2.663499550802829

Epoch: 6| Step: 7
Training loss: 2.061291195912927
Validation loss: 2.6618993335312373

Epoch: 6| Step: 8
Training loss: 2.543052472730132
Validation loss: 2.6608983973110627

Epoch: 6| Step: 9
Training loss: 2.5763391031240217
Validation loss: 2.658471379138762

Epoch: 6| Step: 10
Training loss: 2.783112748669709
Validation loss: 2.658176867644759

Epoch: 6| Step: 11
Training loss: 3.08448591255116
Validation loss: 2.657629051508153

Epoch: 6| Step: 12
Training loss: 2.600562742662005
Validation loss: 2.655406803112984

Epoch: 6| Step: 13
Training loss: 2.613336810560573
Validation loss: 2.6611777877506753

Epoch: 56| Step: 0
Training loss: 3.1011447577126376
Validation loss: 2.6674166061965616

Epoch: 6| Step: 1
Training loss: 2.701537619851815
Validation loss: 2.6710538587615997

Epoch: 6| Step: 2
Training loss: 2.5824010356246307
Validation loss: 2.6621490945742603

Epoch: 6| Step: 3
Training loss: 2.9101410116526374
Validation loss: 2.654888013332615

Epoch: 6| Step: 4
Training loss: 2.7453811176650302
Validation loss: 2.6450280042727035

Epoch: 6| Step: 5
Training loss: 2.1811925142686874
Validation loss: 2.6435943551687298

Epoch: 6| Step: 6
Training loss: 2.8697571426410384
Validation loss: 2.644932545989841

Epoch: 6| Step: 7
Training loss: 2.686334579080603
Validation loss: 2.646337045695381

Epoch: 6| Step: 8
Training loss: 2.864470064640352
Validation loss: 2.6489926996875712

Epoch: 6| Step: 9
Training loss: 2.7358274253605583
Validation loss: 2.6489074050101844

Epoch: 6| Step: 10
Training loss: 2.8740229397616797
Validation loss: 2.6490809766364984

Epoch: 6| Step: 11
Training loss: 2.9842379074164853
Validation loss: 2.6498157437163385

Epoch: 6| Step: 12
Training loss: 2.852687331658875
Validation loss: 2.6487190899583837

Epoch: 6| Step: 13
Training loss: 2.998913886237868
Validation loss: 2.646378638676093

Epoch: 57| Step: 0
Training loss: 2.5487637683446014
Validation loss: 2.644168875888014

Epoch: 6| Step: 1
Training loss: 2.668295263637935
Validation loss: 2.6403715324266517

Epoch: 6| Step: 2
Training loss: 2.5725213389350383
Validation loss: 2.6373127058761816

Epoch: 6| Step: 3
Training loss: 2.935997111805007
Validation loss: 2.6346577291639504

Epoch: 6| Step: 4
Training loss: 2.8607697967428374
Validation loss: 2.632757078770358

Epoch: 6| Step: 5
Training loss: 2.4012281851544586
Validation loss: 2.6320388215611787

Epoch: 6| Step: 6
Training loss: 2.961194673909426
Validation loss: 2.6305806919317103

Epoch: 6| Step: 7
Training loss: 2.9961618666881944
Validation loss: 2.6282957670937486

Epoch: 6| Step: 8
Training loss: 2.612243446273972
Validation loss: 2.627219064479877

Epoch: 6| Step: 9
Training loss: 2.4375145740562396
Validation loss: 2.6260234260656947

Epoch: 6| Step: 10
Training loss: 2.5355935211898037
Validation loss: 2.6255939508018957

Epoch: 6| Step: 11
Training loss: 3.1514989768015553
Validation loss: 2.625485012115549

Epoch: 6| Step: 12
Training loss: 2.762631450304655
Validation loss: 2.619321600198944

Epoch: 6| Step: 13
Training loss: 3.1732322156914625
Validation loss: 2.622459544696184

Epoch: 58| Step: 0
Training loss: 2.4980242551446077
Validation loss: 2.621237283292893

Epoch: 6| Step: 1
Training loss: 2.5751573533062224
Validation loss: 2.6179118733956934

Epoch: 6| Step: 2
Training loss: 2.97107838123408
Validation loss: 2.6192641488687376

Epoch: 6| Step: 3
Training loss: 3.163844725096199
Validation loss: 2.6168107402119283

Epoch: 6| Step: 4
Training loss: 2.6786912673300307
Validation loss: 2.615035034513922

Epoch: 6| Step: 5
Training loss: 2.5223928834460887
Validation loss: 2.6168881828754116

Epoch: 6| Step: 6
Training loss: 2.8918205469469624
Validation loss: 2.615796075146243

Epoch: 6| Step: 7
Training loss: 2.570100630634317
Validation loss: 2.6124126091171065

Epoch: 6| Step: 8
Training loss: 2.996022130316577
Validation loss: 2.6085601265229257

Epoch: 6| Step: 9
Training loss: 2.2522508171351903
Validation loss: 2.611268367383171

Epoch: 6| Step: 10
Training loss: 2.8446348720955372
Validation loss: 2.6176073183626203

Epoch: 6| Step: 11
Training loss: 2.0470692782621898
Validation loss: 2.6263279356446105

Epoch: 6| Step: 12
Training loss: 3.496640909522521
Validation loss: 2.6363465223011007

Epoch: 6| Step: 13
Training loss: 2.7912190111612207
Validation loss: 2.632911914189895

Epoch: 59| Step: 0
Training loss: 3.103593060707961
Validation loss: 2.629483813721339

Epoch: 6| Step: 1
Training loss: 2.203608033816182
Validation loss: 2.620812102950993

Epoch: 6| Step: 2
Training loss: 3.020222534165949
Validation loss: 2.6165631208969953

Epoch: 6| Step: 3
Training loss: 3.0031013352873863
Validation loss: 2.6136989440261202

Epoch: 6| Step: 4
Training loss: 2.682635495762516
Validation loss: 2.608342924090253

Epoch: 6| Step: 5
Training loss: 2.6079880003389797
Validation loss: 2.6048147895464204

Epoch: 6| Step: 6
Training loss: 2.1853105759908815
Validation loss: 2.6007322832950273

Epoch: 6| Step: 7
Training loss: 2.700316364685973
Validation loss: 2.6011456985252597

Epoch: 6| Step: 8
Training loss: 2.676487293511838
Validation loss: 2.5979005015180547

Epoch: 6| Step: 9
Training loss: 2.9303059243121004
Validation loss: 2.60057197172641

Epoch: 6| Step: 10
Training loss: 2.965454521143053
Validation loss: 2.605411313485249

Epoch: 6| Step: 11
Training loss: 2.8803836339175444
Validation loss: 2.594020048124301

Epoch: 6| Step: 12
Training loss: 2.811669290568136
Validation loss: 2.5928471843914065

Epoch: 6| Step: 13
Training loss: 2.412709075881066
Validation loss: 2.5923569243534357

Epoch: 60| Step: 0
Training loss: 2.8973332278267283
Validation loss: 2.5928904629702814

Epoch: 6| Step: 1
Training loss: 2.7424562327979376
Validation loss: 2.5927256204394826

Epoch: 6| Step: 2
Training loss: 2.189127071716168
Validation loss: 2.5921042403082293

Epoch: 6| Step: 3
Training loss: 2.7742465706163206
Validation loss: 2.5902781709517884

Epoch: 6| Step: 4
Training loss: 2.9129188072761663
Validation loss: 2.593150410423842

Epoch: 6| Step: 5
Training loss: 3.1420503484915794
Validation loss: 2.5898581418596005

Epoch: 6| Step: 6
Training loss: 1.9463342641279544
Validation loss: 2.5919558512980947

Epoch: 6| Step: 7
Training loss: 2.6815396239173257
Validation loss: 2.5923552229113547

Epoch: 6| Step: 8
Training loss: 2.5691902470215444
Validation loss: 2.586802250620347

Epoch: 6| Step: 9
Training loss: 3.0810752710782325
Validation loss: 2.589635442658053

Epoch: 6| Step: 10
Training loss: 2.3299287298869333
Validation loss: 2.586159595736574

Epoch: 6| Step: 11
Training loss: 2.9227228413541106
Validation loss: 2.5878986094609533

Epoch: 6| Step: 12
Training loss: 2.9955820295665108
Validation loss: 2.585192032204497

Epoch: 6| Step: 13
Training loss: 2.6705431379309887
Validation loss: 2.5863161606927934

Epoch: 61| Step: 0
Training loss: 2.6439449950829017
Validation loss: 2.5818228894843864

Epoch: 6| Step: 1
Training loss: 3.0239717864077758
Validation loss: 2.5892523104136886

Epoch: 6| Step: 2
Training loss: 2.6959591297198613
Validation loss: 2.5851330538312616

Epoch: 6| Step: 3
Training loss: 3.1704025985225837
Validation loss: 2.5820370980416314

Epoch: 6| Step: 4
Training loss: 2.156514027228617
Validation loss: 2.5820087964749963

Epoch: 6| Step: 5
Training loss: 2.2745964530775624
Validation loss: 2.580748885483892

Epoch: 6| Step: 6
Training loss: 2.4118869694405114
Validation loss: 2.577823783155228

Epoch: 6| Step: 7
Training loss: 2.533672539270788
Validation loss: 2.5801216177651995

Epoch: 6| Step: 8
Training loss: 2.6127303245532882
Validation loss: 2.582229383506664

Epoch: 6| Step: 9
Training loss: 2.7339751032914474
Validation loss: 2.581827837635558

Epoch: 6| Step: 10
Training loss: 2.552316200894663
Validation loss: 2.5836605715889127

Epoch: 6| Step: 11
Training loss: 3.35710372438204
Validation loss: 2.582298753571464

Epoch: 6| Step: 12
Training loss: 2.534196151650516
Validation loss: 2.581319983197926

Epoch: 6| Step: 13
Training loss: 3.0806063028032473
Validation loss: 2.57955975923861

Epoch: 62| Step: 0
Training loss: 2.0395776090713715
Validation loss: 2.5799255094937807

Epoch: 6| Step: 1
Training loss: 3.016595081582954
Validation loss: 2.5788210459057126

Epoch: 6| Step: 2
Training loss: 2.8609001386401944
Validation loss: 2.574868829713329

Epoch: 6| Step: 3
Training loss: 3.0201786427997646
Validation loss: 2.569976289939112

Epoch: 6| Step: 4
Training loss: 2.50263952150224
Validation loss: 2.5691967738842854

Epoch: 6| Step: 5
Training loss: 2.4352683586428783
Validation loss: 2.570939116153848

Epoch: 6| Step: 6
Training loss: 2.3906162236869135
Validation loss: 2.5685526070470215

Epoch: 6| Step: 7
Training loss: 3.4104616218513724
Validation loss: 2.564623728004558

Epoch: 6| Step: 8
Training loss: 2.381984279090641
Validation loss: 2.5659924874704925

Epoch: 6| Step: 9
Training loss: 2.649712719730354
Validation loss: 2.5728097551092484

Epoch: 6| Step: 10
Training loss: 2.6102528266200906
Validation loss: 2.570678236583471

Epoch: 6| Step: 11
Training loss: 2.7142416746108995
Validation loss: 2.5656379157301523

Epoch: 6| Step: 12
Training loss: 3.039530818521293
Validation loss: 2.567201101306153

Epoch: 6| Step: 13
Training loss: 2.5857990147137557
Validation loss: 2.559491834197867

Epoch: 63| Step: 0
Training loss: 2.527970249640776
Validation loss: 2.562472769739879

Epoch: 6| Step: 1
Training loss: 2.3464939585261884
Validation loss: 2.5584444036406544

Epoch: 6| Step: 2
Training loss: 3.0196837945580017
Validation loss: 2.5594756724920464

Epoch: 6| Step: 3
Training loss: 2.9394923819014798
Validation loss: 2.5611378530658633

Epoch: 6| Step: 4
Training loss: 2.710684552883138
Validation loss: 2.5652052857788723

Epoch: 6| Step: 5
Training loss: 2.663328545112109
Validation loss: 2.5691815857562874

Epoch: 6| Step: 6
Training loss: 2.168661470832468
Validation loss: 2.571178441143748

Epoch: 6| Step: 7
Training loss: 2.594253950683785
Validation loss: 2.5723906891894894

Epoch: 6| Step: 8
Training loss: 2.261069301725852
Validation loss: 2.572144231536232

Epoch: 6| Step: 9
Training loss: 2.8441217200498774
Validation loss: 2.571060427420118

Epoch: 6| Step: 10
Training loss: 3.0650222667986737
Validation loss: 2.570353823622388

Epoch: 6| Step: 11
Training loss: 2.656529041667878
Validation loss: 2.566175445598443

Epoch: 6| Step: 12
Training loss: 2.816731808452752
Validation loss: 2.56002576517014

Epoch: 6| Step: 13
Training loss: 2.9865947676700992
Validation loss: 2.5565559794783415

Epoch: 64| Step: 0
Training loss: 2.7103256722456415
Validation loss: 2.5537916940206724

Epoch: 6| Step: 1
Training loss: 2.884432371153771
Validation loss: 2.553657347370301

Epoch: 6| Step: 2
Training loss: 2.695342044391971
Validation loss: 2.550027203258945

Epoch: 6| Step: 3
Training loss: 2.804346175397407
Validation loss: 2.547780802221657

Epoch: 6| Step: 4
Training loss: 2.688114450873488
Validation loss: 2.5482507746038587

Epoch: 6| Step: 5
Training loss: 2.8140985820179054
Validation loss: 2.552054157220057

Epoch: 6| Step: 6
Training loss: 2.482810241429314
Validation loss: 2.5523893730757377

Epoch: 6| Step: 7
Training loss: 3.2446923564737506
Validation loss: 2.5487139718909786

Epoch: 6| Step: 8
Training loss: 2.454930121210055
Validation loss: 2.548677458030904

Epoch: 6| Step: 9
Training loss: 2.593026669307367
Validation loss: 2.5532363051794214

Epoch: 6| Step: 10
Training loss: 2.7679861733030613
Validation loss: 2.550687203029323

Epoch: 6| Step: 11
Training loss: 2.7030062787219085
Validation loss: 2.548238954619741

Epoch: 6| Step: 12
Training loss: 2.1608652672379915
Validation loss: 2.5491087309617897

Epoch: 6| Step: 13
Training loss: 2.3983596640019145
Validation loss: 2.543119591117901

Epoch: 65| Step: 0
Training loss: 2.673394468263274
Validation loss: 2.5448146981940614

Epoch: 6| Step: 1
Training loss: 2.83787062992999
Validation loss: 2.547012184988589

Epoch: 6| Step: 2
Training loss: 2.7059915150455094
Validation loss: 2.5526773864253345

Epoch: 6| Step: 3
Training loss: 2.4733081699472534
Validation loss: 2.5468782920270057

Epoch: 6| Step: 4
Training loss: 2.3118489354157865
Validation loss: 2.5446291345530647

Epoch: 6| Step: 5
Training loss: 2.470115863229314
Validation loss: 2.5439493580771972

Epoch: 6| Step: 6
Training loss: 2.9537657996169595
Validation loss: 2.5435356215386067

Epoch: 6| Step: 7
Training loss: 2.29805616253382
Validation loss: 2.551831203125033

Epoch: 6| Step: 8
Training loss: 2.785152825865707
Validation loss: 2.5576177157410838

Epoch: 6| Step: 9
Training loss: 3.0417114324193135
Validation loss: 2.5548284260399106

Epoch: 6| Step: 10
Training loss: 3.287595299329413
Validation loss: 2.5395538089486975

Epoch: 6| Step: 11
Training loss: 2.3627803050878606
Validation loss: 2.531417621352882

Epoch: 6| Step: 12
Training loss: 2.445953772556405
Validation loss: 2.5422107135109098

Epoch: 6| Step: 13
Training loss: 2.592769943206214
Validation loss: 2.5402798595342304

Epoch: 66| Step: 0
Training loss: 2.2515981085011174
Validation loss: 2.5460076600703947

Epoch: 6| Step: 1
Training loss: 2.8769685391325868
Validation loss: 2.5482515542842608

Epoch: 6| Step: 2
Training loss: 3.042298934925403
Validation loss: 2.5535300274013326

Epoch: 6| Step: 3
Training loss: 3.042145016475962
Validation loss: 2.550803137194135

Epoch: 6| Step: 4
Training loss: 2.640615068225128
Validation loss: 2.5479861216983086

Epoch: 6| Step: 5
Training loss: 1.997617971973291
Validation loss: 2.5447379041112512

Epoch: 6| Step: 6
Training loss: 2.6176744605859614
Validation loss: 2.543876927213845

Epoch: 6| Step: 7
Training loss: 2.8398761563767403
Validation loss: 2.5435386444939536

Epoch: 6| Step: 8
Training loss: 3.0590983592067236
Validation loss: 2.5417597915182166

Epoch: 6| Step: 9
Training loss: 2.514277885319547
Validation loss: 2.5408091323148727

Epoch: 6| Step: 10
Training loss: 2.912289159125911
Validation loss: 2.5368114802630184

Epoch: 6| Step: 11
Training loss: 2.6117090077265575
Validation loss: 2.535234917359119

Epoch: 6| Step: 12
Training loss: 2.3930766181906935
Validation loss: 2.5345738882850086

Epoch: 6| Step: 13
Training loss: 2.3722165511413653
Validation loss: 2.5262720586312932

Epoch: 67| Step: 0
Training loss: 3.068689125840186
Validation loss: 2.5301051435372237

Epoch: 6| Step: 1
Training loss: 2.7184545422195336
Validation loss: 2.5269562681183193

Epoch: 6| Step: 2
Training loss: 2.463610066814514
Validation loss: 2.527229538409837

Epoch: 6| Step: 3
Training loss: 2.704491242462101
Validation loss: 2.526198318710333

Epoch: 6| Step: 4
Training loss: 2.2364547760261018
Validation loss: 2.524216664489621

Epoch: 6| Step: 5
Training loss: 2.832897564157375
Validation loss: 2.5262428413326177

Epoch: 6| Step: 6
Training loss: 2.362570815059446
Validation loss: 2.5223993266053433

Epoch: 6| Step: 7
Training loss: 2.7924502871877226
Validation loss: 2.524746660547566

Epoch: 6| Step: 8
Training loss: 2.31851568554773
Validation loss: 2.516795168579098

Epoch: 6| Step: 9
Training loss: 2.760218967099873
Validation loss: 2.5219027770197466

Epoch: 6| Step: 10
Training loss: 2.2249628042476677
Validation loss: 2.5237652356168265

Epoch: 6| Step: 11
Training loss: 2.620308361834976
Validation loss: 2.523247883919175

Epoch: 6| Step: 12
Training loss: 3.1650010965595485
Validation loss: 2.5190598473177834

Epoch: 6| Step: 13
Training loss: 2.669863652094685
Validation loss: 2.5226662542767553

Epoch: 68| Step: 0
Training loss: 2.315816202648268
Validation loss: 2.5232034265630787

Epoch: 6| Step: 1
Training loss: 2.7661643499119637
Validation loss: 2.525087460806635

Epoch: 6| Step: 2
Training loss: 2.939776086649497
Validation loss: 2.5263464098236796

Epoch: 6| Step: 3
Training loss: 2.7132841391950318
Validation loss: 2.5221214838911727

Epoch: 6| Step: 4
Training loss: 2.9321682205550235
Validation loss: 2.5219024461326525

Epoch: 6| Step: 5
Training loss: 2.750534005769522
Validation loss: 2.522830650024573

Epoch: 6| Step: 6
Training loss: 2.792559144188625
Validation loss: 2.5193544310986615

Epoch: 6| Step: 7
Training loss: 2.33869582406546
Validation loss: 2.516114165645666

Epoch: 6| Step: 8
Training loss: 2.6949793512972335
Validation loss: 2.51793135282745

Epoch: 6| Step: 9
Training loss: 2.471108384500611
Validation loss: 2.5173503100868695

Epoch: 6| Step: 10
Training loss: 2.9618136033285105
Validation loss: 2.512827642786

Epoch: 6| Step: 11
Training loss: 1.8709298143663846
Validation loss: 2.5127610515401675

Epoch: 6| Step: 12
Training loss: 2.922444497161453
Validation loss: 2.5159653774946

Epoch: 6| Step: 13
Training loss: 2.3316071346700618
Validation loss: 2.5189325454229436

Epoch: 69| Step: 0
Training loss: 2.52658140308714
Validation loss: 2.5153980034243295

Epoch: 6| Step: 1
Training loss: 2.4518709318159835
Validation loss: 2.5134103318350407

Epoch: 6| Step: 2
Training loss: 3.162288215391214
Validation loss: 2.5141253848970386

Epoch: 6| Step: 3
Training loss: 2.2488755489329826
Validation loss: 2.5139344974078144

Epoch: 6| Step: 4
Training loss: 2.6951615885628
Validation loss: 2.51821311359095

Epoch: 6| Step: 5
Training loss: 2.7680063286582173
Validation loss: 2.5138100657976366

Epoch: 6| Step: 6
Training loss: 2.179832535707279
Validation loss: 2.5145846756934436

Epoch: 6| Step: 7
Training loss: 2.9700565073308423
Validation loss: 2.5136660101801644

Epoch: 6| Step: 8
Training loss: 2.722987238332576
Validation loss: 2.5127090392446343

Epoch: 6| Step: 9
Training loss: 2.6597395471108776
Validation loss: 2.512321609439685

Epoch: 6| Step: 10
Training loss: 2.819356064019346
Validation loss: 2.5105425907278693

Epoch: 6| Step: 11
Training loss: 2.4197959590046705
Validation loss: 2.5148407086688467

Epoch: 6| Step: 12
Training loss: 2.329191437272781
Validation loss: 2.511708941324322

Epoch: 6| Step: 13
Training loss: 2.8580666138645388
Validation loss: 2.5133418588046808

Epoch: 70| Step: 0
Training loss: 2.251697323936556
Validation loss: 2.5188277649844935

Epoch: 6| Step: 1
Training loss: 2.7506780655415612
Validation loss: 2.519723369360898

Epoch: 6| Step: 2
Training loss: 2.702658905241476
Validation loss: 2.5329852023931774

Epoch: 6| Step: 3
Training loss: 3.0772600072006435
Validation loss: 2.5517437040666713

Epoch: 6| Step: 4
Training loss: 2.7590420065181482
Validation loss: 2.5560634691102075

Epoch: 6| Step: 5
Training loss: 2.2526467967914923
Validation loss: 2.5438634623748757

Epoch: 6| Step: 6
Training loss: 2.3099583241389947
Validation loss: 2.541114955703295

Epoch: 6| Step: 7
Training loss: 2.9083003533214025
Validation loss: 2.5390644719042825

Epoch: 6| Step: 8
Training loss: 2.613432510628477
Validation loss: 2.5297735327231683

Epoch: 6| Step: 9
Training loss: 2.5624370102002305
Validation loss: 2.5261481088975666

Epoch: 6| Step: 10
Training loss: 2.309398220497847
Validation loss: 2.520604900907717

Epoch: 6| Step: 11
Training loss: 2.6583267171970735
Validation loss: 2.5192869398164635

Epoch: 6| Step: 12
Training loss: 3.249096084484258
Validation loss: 2.5151332903832277

Epoch: 6| Step: 13
Training loss: 2.7090639351601054
Validation loss: 2.5126739315225834

Epoch: 71| Step: 0
Training loss: 2.764010804222841
Validation loss: 2.5119899130748293

Epoch: 6| Step: 1
Training loss: 2.5720417733426455
Validation loss: 2.5122095778402627

Epoch: 6| Step: 2
Training loss: 3.0495196480131366
Validation loss: 2.5098940726139003

Epoch: 6| Step: 3
Training loss: 2.6481512134398906
Validation loss: 2.5027606662452717

Epoch: 6| Step: 4
Training loss: 2.0704125722055324
Validation loss: 2.5141750761740087

Epoch: 6| Step: 5
Training loss: 3.178248724315528
Validation loss: 2.5206721913770607

Epoch: 6| Step: 6
Training loss: 2.7015240288681954
Validation loss: 2.5064878044924974

Epoch: 6| Step: 7
Training loss: 2.4212655561948844
Validation loss: 2.503366460754527

Epoch: 6| Step: 8
Training loss: 1.8421762506531396
Validation loss: 2.5037292163968337

Epoch: 6| Step: 9
Training loss: 2.669449893559048
Validation loss: 2.5043895173736934

Epoch: 6| Step: 10
Training loss: 2.542491205851032
Validation loss: 2.498615326476439

Epoch: 6| Step: 11
Training loss: 2.50060722605999
Validation loss: 2.4995018621708534

Epoch: 6| Step: 12
Training loss: 3.0815890421631944
Validation loss: 2.5039928657361044

Epoch: 6| Step: 13
Training loss: 2.6261801110563625
Validation loss: 2.499330160685218

Epoch: 72| Step: 0
Training loss: 2.501225171287645
Validation loss: 2.499816538119015

Epoch: 6| Step: 1
Training loss: 1.915138782820869
Validation loss: 2.502161347387654

Epoch: 6| Step: 2
Training loss: 2.899310944349772
Validation loss: 2.4993326409017493

Epoch: 6| Step: 3
Training loss: 2.3267284531391317
Validation loss: 2.4970794944112003

Epoch: 6| Step: 4
Training loss: 2.6140013473845833
Validation loss: 2.5026383465441713

Epoch: 6| Step: 5
Training loss: 2.6755638312672616
Validation loss: 2.506352666209114

Epoch: 6| Step: 6
Training loss: 2.5309824861050902
Validation loss: 2.4967563249237914

Epoch: 6| Step: 7
Training loss: 2.9587029955941326
Validation loss: 2.4994254564663145

Epoch: 6| Step: 8
Training loss: 2.655723340051223
Validation loss: 2.5000899616585612

Epoch: 6| Step: 9
Training loss: 2.8050993295581788
Validation loss: 2.5054933751950657

Epoch: 6| Step: 10
Training loss: 2.7918432568926805
Validation loss: 2.5012537832261703

Epoch: 6| Step: 11
Training loss: 2.6474288775262798
Validation loss: 2.5066970768903305

Epoch: 6| Step: 12
Training loss: 2.9835812776419233
Validation loss: 2.4988121071558975

Epoch: 6| Step: 13
Training loss: 2.3806062083405255
Validation loss: 2.5000781682827453

Epoch: 73| Step: 0
Training loss: 3.087087373535654
Validation loss: 2.4961643835828506

Epoch: 6| Step: 1
Training loss: 2.55755100113626
Validation loss: 2.4969704868740332

Epoch: 6| Step: 2
Training loss: 2.7970688395752354
Validation loss: 2.494558117685287

Epoch: 6| Step: 3
Training loss: 2.9152008369578595
Validation loss: 2.4937507884343812

Epoch: 6| Step: 4
Training loss: 2.8858745409051876
Validation loss: 2.4918966890986938

Epoch: 6| Step: 5
Training loss: 2.529179420837052
Validation loss: 2.4963692526491146

Epoch: 6| Step: 6
Training loss: 2.3827216459369738
Validation loss: 2.4953109476860567

Epoch: 6| Step: 7
Training loss: 2.6275085089025816
Validation loss: 2.4924739884243947

Epoch: 6| Step: 8
Training loss: 2.525953898265146
Validation loss: 2.4955531466666727

Epoch: 6| Step: 9
Training loss: 1.9086089000498792
Validation loss: 2.4962759414111924

Epoch: 6| Step: 10
Training loss: 2.511611865010481
Validation loss: 2.4960894995042793

Epoch: 6| Step: 11
Training loss: 2.296714517605185
Validation loss: 2.4921215212069434

Epoch: 6| Step: 12
Training loss: 2.4444152355857267
Validation loss: 2.4908231949134585

Epoch: 6| Step: 13
Training loss: 2.9144309785045173
Validation loss: 2.4931688598793853

Epoch: 74| Step: 0
Training loss: 2.814592134324062
Validation loss: 2.493190129212681

Epoch: 6| Step: 1
Training loss: 2.2211625857694153
Validation loss: 2.4979072076418483

Epoch: 6| Step: 2
Training loss: 2.664412757915608
Validation loss: 2.5012008167256363

Epoch: 6| Step: 3
Training loss: 2.5324636304979626
Validation loss: 2.4946186162278803

Epoch: 6| Step: 4
Training loss: 2.627481105457029
Validation loss: 2.4967967174291292

Epoch: 6| Step: 5
Training loss: 2.5324493204301
Validation loss: 2.4988942243953405

Epoch: 6| Step: 6
Training loss: 2.5264553295046444
Validation loss: 2.494173747225864

Epoch: 6| Step: 7
Training loss: 2.6923695295170162
Validation loss: 2.4950495183817583

Epoch: 6| Step: 8
Training loss: 2.8187633708628272
Validation loss: 2.4978761234339144

Epoch: 6| Step: 9
Training loss: 2.346105180598819
Validation loss: 2.4899782535262123

Epoch: 6| Step: 10
Training loss: 2.447991898915967
Validation loss: 2.4890335518749467

Epoch: 6| Step: 11
Training loss: 2.971838699584427
Validation loss: 2.495433355855132

Epoch: 6| Step: 12
Training loss: 2.59755950977053
Validation loss: 2.4883617905098654

Epoch: 6| Step: 13
Training loss: 2.77779894396877
Validation loss: 2.495163674117467

Epoch: 75| Step: 0
Training loss: 3.1475262328880995
Validation loss: 2.497429718540825

Epoch: 6| Step: 1
Training loss: 2.3683243341384563
Validation loss: 2.4973962894528117

Epoch: 6| Step: 2
Training loss: 2.997291295769288
Validation loss: 2.4904666169022587

Epoch: 6| Step: 3
Training loss: 2.5337815047245256
Validation loss: 2.4876764142276833

Epoch: 6| Step: 4
Training loss: 2.397671555476157
Validation loss: 2.486736015430068

Epoch: 6| Step: 5
Training loss: 2.8048104176908306
Validation loss: 2.4905920073566783

Epoch: 6| Step: 6
Training loss: 2.7857572915526876
Validation loss: 2.485987014325354

Epoch: 6| Step: 7
Training loss: 2.5063390473411715
Validation loss: 2.486376645797669

Epoch: 6| Step: 8
Training loss: 2.646962883401185
Validation loss: 2.487596785996295

Epoch: 6| Step: 9
Training loss: 2.4327157826391397
Validation loss: 2.4928809051354466

Epoch: 6| Step: 10
Training loss: 2.3890013323301527
Validation loss: 2.4884976103821446

Epoch: 6| Step: 11
Training loss: 2.515237150443996
Validation loss: 2.49236014793559

Epoch: 6| Step: 12
Training loss: 2.6258386453009646
Validation loss: 2.4867329314132407

Epoch: 6| Step: 13
Training loss: 2.154621338468228
Validation loss: 2.489603023903415

Epoch: 76| Step: 0
Training loss: 2.774995865689884
Validation loss: 2.482676327177825

Epoch: 6| Step: 1
Training loss: 2.5811115883578486
Validation loss: 2.488002104699041

Epoch: 6| Step: 2
Training loss: 2.1122563780305557
Validation loss: 2.4946284602437294

Epoch: 6| Step: 3
Training loss: 3.009450649685192
Validation loss: 2.488572611136685

Epoch: 6| Step: 4
Training loss: 2.35956153384883
Validation loss: 2.4852313599063494

Epoch: 6| Step: 5
Training loss: 2.5641940030275308
Validation loss: 2.488350380696257

Epoch: 6| Step: 6
Training loss: 2.279706458888137
Validation loss: 2.4835283128202166

Epoch: 6| Step: 7
Training loss: 2.688964001427029
Validation loss: 2.4887272840734433

Epoch: 6| Step: 8
Training loss: 3.115559470693777
Validation loss: 2.491165405957945

Epoch: 6| Step: 9
Training loss: 2.4101852341057555
Validation loss: 2.4938153219312142

Epoch: 6| Step: 10
Training loss: 2.789266455963955
Validation loss: 2.4935890649351182

Epoch: 6| Step: 11
Training loss: 2.5012551018140043
Validation loss: 2.490484072065729

Epoch: 6| Step: 12
Training loss: 2.8256805140411116
Validation loss: 2.4972812333263903

Epoch: 6| Step: 13
Training loss: 2.4318947532869206
Validation loss: 2.4992257031773994

Epoch: 77| Step: 0
Training loss: 2.7127593237326706
Validation loss: 2.498481925361037

Epoch: 6| Step: 1
Training loss: 2.794201558804734
Validation loss: 2.4960819536625847

Epoch: 6| Step: 2
Training loss: 2.8586001153975658
Validation loss: 2.4979616438438343

Epoch: 6| Step: 3
Training loss: 2.4079601601508145
Validation loss: 2.491942454441589

Epoch: 6| Step: 4
Training loss: 2.4742202994869045
Validation loss: 2.488763990939283

Epoch: 6| Step: 5
Training loss: 2.8494492316528626
Validation loss: 2.4870392051493337

Epoch: 6| Step: 6
Training loss: 2.46683089594513
Validation loss: 2.488454368553616

Epoch: 6| Step: 7
Training loss: 2.635389566596295
Validation loss: 2.487285572503254

Epoch: 6| Step: 8
Training loss: 2.6106443686904943
Validation loss: 2.485604563728099

Epoch: 6| Step: 9
Training loss: 2.6859879786889174
Validation loss: 2.4836367903879184

Epoch: 6| Step: 10
Training loss: 2.33285492579315
Validation loss: 2.490345799564978

Epoch: 6| Step: 11
Training loss: 2.1943948259927497
Validation loss: 2.4870479287965535

Epoch: 6| Step: 12
Training loss: 2.6735200335249827
Validation loss: 2.4852527851093864

Epoch: 6| Step: 13
Training loss: 2.8828129962207396
Validation loss: 2.4885745591781134

Epoch: 78| Step: 0
Training loss: 2.4647038778285926
Validation loss: 2.4848865806030465

Epoch: 6| Step: 1
Training loss: 2.6560591685242367
Validation loss: 2.486181006582681

Epoch: 6| Step: 2
Training loss: 2.8182148833586447
Validation loss: 2.4915321469010063

Epoch: 6| Step: 3
Training loss: 2.684321542192034
Validation loss: 2.484443599625529

Epoch: 6| Step: 4
Training loss: 2.6463178856763943
Validation loss: 2.4849996441278646

Epoch: 6| Step: 5
Training loss: 2.5390737680038433
Validation loss: 2.4836756363647314

Epoch: 6| Step: 6
Training loss: 2.245087240516345
Validation loss: 2.483941622187835

Epoch: 6| Step: 7
Training loss: 2.6764955778379376
Validation loss: 2.487470413875511

Epoch: 6| Step: 8
Training loss: 2.762660274775609
Validation loss: 2.483250793476363

Epoch: 6| Step: 9
Training loss: 2.4981350618474263
Validation loss: 2.4863011632647605

Epoch: 6| Step: 10
Training loss: 2.514700583500589
Validation loss: 2.481810199522735

Epoch: 6| Step: 11
Training loss: 3.143835296383793
Validation loss: 2.48223173512285

Epoch: 6| Step: 12
Training loss: 2.4730456679863484
Validation loss: 2.484877257689783

Epoch: 6| Step: 13
Training loss: 2.109753610046676
Validation loss: 2.4839122669167653

Epoch: 79| Step: 0
Training loss: 2.644544322490077
Validation loss: 2.4887572531384485

Epoch: 6| Step: 1
Training loss: 2.6156965421420217
Validation loss: 2.4856080008502524

Epoch: 6| Step: 2
Training loss: 2.4032441227510777
Validation loss: 2.4779462428359387

Epoch: 6| Step: 3
Training loss: 2.9721472325674085
Validation loss: 2.4775178592883753

Epoch: 6| Step: 4
Training loss: 2.744839073703109
Validation loss: 2.473562997384219

Epoch: 6| Step: 5
Training loss: 2.6587156632703124
Validation loss: 2.476360629706093

Epoch: 6| Step: 6
Training loss: 2.3261927836749234
Validation loss: 2.4784604570698545

Epoch: 6| Step: 7
Training loss: 2.601713158996267
Validation loss: 2.4797813764724683

Epoch: 6| Step: 8
Training loss: 2.1268605333812065
Validation loss: 2.4818691835933286

Epoch: 6| Step: 9
Training loss: 2.2401459334724545
Validation loss: 2.4804057275971005

Epoch: 6| Step: 10
Training loss: 2.8043957401781396
Validation loss: 2.4818827285972884

Epoch: 6| Step: 11
Training loss: 2.7019738188123896
Validation loss: 2.4838763042371848

Epoch: 6| Step: 12
Training loss: 2.5057296897767642
Validation loss: 2.4860446525608437

Epoch: 6| Step: 13
Training loss: 2.902032560811265
Validation loss: 2.484593507766256

Epoch: 80| Step: 0
Training loss: 2.1123799713164755
Validation loss: 2.482980940818486

Epoch: 6| Step: 1
Training loss: 2.3463035405857933
Validation loss: 2.482387747581275

Epoch: 6| Step: 2
Training loss: 2.466947066204221
Validation loss: 2.4743105881861926

Epoch: 6| Step: 3
Training loss: 2.857103456497999
Validation loss: 2.4772338759414416

Epoch: 6| Step: 4
Training loss: 3.18642033261979
Validation loss: 2.4774443844257266

Epoch: 6| Step: 5
Training loss: 2.0275768226904796
Validation loss: 2.4745340473820594

Epoch: 6| Step: 6
Training loss: 2.5092812868626586
Validation loss: 2.48043017419971

Epoch: 6| Step: 7
Training loss: 2.793022955521852
Validation loss: 2.4976054326721497

Epoch: 6| Step: 8
Training loss: 2.8677331057105677
Validation loss: 2.5343319689715096

Epoch: 6| Step: 9
Training loss: 2.6774284249221405
Validation loss: 2.5287310150183058

Epoch: 6| Step: 10
Training loss: 2.286950292356498
Validation loss: 2.503005176586082

Epoch: 6| Step: 11
Training loss: 2.712026503762564
Validation loss: 2.4863531846079363

Epoch: 6| Step: 12
Training loss: 2.7362444781112196
Validation loss: 2.4803180398487066

Epoch: 6| Step: 13
Training loss: 2.7487918627484174
Validation loss: 2.4728901106177936

Epoch: 81| Step: 0
Training loss: 2.2299649125490633
Validation loss: 2.481391570875316

Epoch: 6| Step: 1
Training loss: 2.754276245368867
Validation loss: 2.490230513426336

Epoch: 6| Step: 2
Training loss: 2.581332990960947
Validation loss: 2.5027578004380087

Epoch: 6| Step: 3
Training loss: 2.6879422910603568
Validation loss: 2.519713583960286

Epoch: 6| Step: 4
Training loss: 3.238593405114687
Validation loss: 2.543598540449776

Epoch: 6| Step: 5
Training loss: 2.8869496667409873
Validation loss: 2.5674144945575534

Epoch: 6| Step: 6
Training loss: 2.54134212568963
Validation loss: 2.5628924340686168

Epoch: 6| Step: 7
Training loss: 2.6658108251809347
Validation loss: 2.568824174835657

Epoch: 6| Step: 8
Training loss: 2.2207938438277983
Validation loss: 2.56449052987247

Epoch: 6| Step: 9
Training loss: 2.596152055093088
Validation loss: 2.5404846277751

Epoch: 6| Step: 10
Training loss: 2.655716247786522
Validation loss: 2.526146992063944

Epoch: 6| Step: 11
Training loss: 2.6464015821055487
Validation loss: 2.513593386452471

Epoch: 6| Step: 12
Training loss: 2.708522193388198
Validation loss: 2.5047979250002688

Epoch: 6| Step: 13
Training loss: 2.541547762064579
Validation loss: 2.4990791691236125

Epoch: 82| Step: 0
Training loss: 2.6826658018836316
Validation loss: 2.490121493305521

Epoch: 6| Step: 1
Training loss: 2.438988451227715
Validation loss: 2.4826575846883245

Epoch: 6| Step: 2
Training loss: 2.700569841509764
Validation loss: 2.477943781303519

Epoch: 6| Step: 3
Training loss: 2.508171079674788
Validation loss: 2.4728074512183484

Epoch: 6| Step: 4
Training loss: 1.976699522794997
Validation loss: 2.474507342540602

Epoch: 6| Step: 5
Training loss: 3.020607424652728
Validation loss: 2.470131017013255

Epoch: 6| Step: 6
Training loss: 2.519869996002359
Validation loss: 2.475458904069321

Epoch: 6| Step: 7
Training loss: 2.5671341094712488
Validation loss: 2.471414938455178

Epoch: 6| Step: 8
Training loss: 2.878366572444554
Validation loss: 2.469518437282427

Epoch: 6| Step: 9
Training loss: 2.5370374849983723
Validation loss: 2.4723972468147446

Epoch: 6| Step: 10
Training loss: 2.2694455260273307
Validation loss: 2.4776310746375216

Epoch: 6| Step: 11
Training loss: 2.5981621041767347
Validation loss: 2.4694014064078846

Epoch: 6| Step: 12
Training loss: 3.038690931655439
Validation loss: 2.4741318303892794

Epoch: 6| Step: 13
Training loss: 2.422261262670439
Validation loss: 2.471394470550788

Epoch: 83| Step: 0
Training loss: 2.587818395218424
Validation loss: 2.469091182584467

Epoch: 6| Step: 1
Training loss: 2.9704893136813864
Validation loss: 2.4655608732508933

Epoch: 6| Step: 2
Training loss: 2.140014004349564
Validation loss: 2.463190909988606

Epoch: 6| Step: 3
Training loss: 1.9962381508510778
Validation loss: 2.4678763038683207

Epoch: 6| Step: 4
Training loss: 2.440374000527618
Validation loss: 2.475860432010325

Epoch: 6| Step: 5
Training loss: 2.911188994604213
Validation loss: 2.481807877916907

Epoch: 6| Step: 6
Training loss: 2.9952125496821487
Validation loss: 2.476288789371199

Epoch: 6| Step: 7
Training loss: 2.094367548269905
Validation loss: 2.47965127285104

Epoch: 6| Step: 8
Training loss: 2.5796857329003826
Validation loss: 2.475336150258779

Epoch: 6| Step: 9
Training loss: 2.462325706703883
Validation loss: 2.4743911899885878

Epoch: 6| Step: 10
Training loss: 2.468181496362619
Validation loss: 2.4789591202826955

Epoch: 6| Step: 11
Training loss: 2.9389648031883406
Validation loss: 2.4790997429371227

Epoch: 6| Step: 12
Training loss: 2.5080756408868843
Validation loss: 2.4812753402223144

Epoch: 6| Step: 13
Training loss: 2.901363571817
Validation loss: 2.4775527675050624

Epoch: 84| Step: 0
Training loss: 2.4244593460032204
Validation loss: 2.478270614534181

Epoch: 6| Step: 1
Training loss: 2.633744796363966
Validation loss: 2.4814436471035997

Epoch: 6| Step: 2
Training loss: 2.636458953491604
Validation loss: 2.480980575500859

Epoch: 6| Step: 3
Training loss: 2.517650665350874
Validation loss: 2.482290757200838

Epoch: 6| Step: 4
Training loss: 2.6138486602566693
Validation loss: 2.488339202381664

Epoch: 6| Step: 5
Training loss: 2.439484204353161
Validation loss: 2.4891943102249763

Epoch: 6| Step: 6
Training loss: 3.1402440385685213
Validation loss: 2.4896738575926727

Epoch: 6| Step: 7
Training loss: 2.6107804402402577
Validation loss: 2.48938597744159

Epoch: 6| Step: 8
Training loss: 2.3808778006362483
Validation loss: 2.4936119321814427

Epoch: 6| Step: 9
Training loss: 2.5168011682033558
Validation loss: 2.4900450391411186

Epoch: 6| Step: 10
Training loss: 2.879358719319544
Validation loss: 2.4906810404791955

Epoch: 6| Step: 11
Training loss: 2.841718178409916
Validation loss: 2.4898805693565347

Epoch: 6| Step: 12
Training loss: 2.442218614844976
Validation loss: 2.4882843161284094

Epoch: 6| Step: 13
Training loss: 2.4237362508398586
Validation loss: 2.488900754773525

Epoch: 85| Step: 0
Training loss: 2.9422530010466414
Validation loss: 2.483919545776543

Epoch: 6| Step: 1
Training loss: 2.2652103439243074
Validation loss: 2.4837277926927452

Epoch: 6| Step: 2
Training loss: 3.262511891310495
Validation loss: 2.479283760147904

Epoch: 6| Step: 3
Training loss: 2.449926441411835
Validation loss: 2.474754483796525

Epoch: 6| Step: 4
Training loss: 2.271721995578533
Validation loss: 2.474868146678692

Epoch: 6| Step: 5
Training loss: 2.253491765195343
Validation loss: 2.4785409880478446

Epoch: 6| Step: 6
Training loss: 2.7312268253975613
Validation loss: 2.4724351282869774

Epoch: 6| Step: 7
Training loss: 2.7422727044499586
Validation loss: 2.473267666963794

Epoch: 6| Step: 8
Training loss: 2.2249750200219505
Validation loss: 2.471571183532851

Epoch: 6| Step: 9
Training loss: 2.4376277401048365
Validation loss: 2.4692365030185863

Epoch: 6| Step: 10
Training loss: 2.615294543393928
Validation loss: 2.468381009110442

Epoch: 6| Step: 11
Training loss: 2.58059241390927
Validation loss: 2.472406584643516

Epoch: 6| Step: 12
Training loss: 2.493655164249066
Validation loss: 2.4695421871307963

Epoch: 6| Step: 13
Training loss: 2.704530207370242
Validation loss: 2.474486370286283

Epoch: 86| Step: 0
Training loss: 2.386312746768668
Validation loss: 2.478182049511914

Epoch: 6| Step: 1
Training loss: 2.4294615024076647
Validation loss: 2.4820392993143687

Epoch: 6| Step: 2
Training loss: 2.8418297624815216
Validation loss: 2.4916080211725045

Epoch: 6| Step: 3
Training loss: 2.829204711508445
Validation loss: 2.4808236256252414

Epoch: 6| Step: 4
Training loss: 2.5824224547923738
Validation loss: 2.477854619262568

Epoch: 6| Step: 5
Training loss: 2.7413745337994606
Validation loss: 2.4717687186096353

Epoch: 6| Step: 6
Training loss: 2.3766127681996245
Validation loss: 2.4752201650290866

Epoch: 6| Step: 7
Training loss: 1.8598017483339666
Validation loss: 2.4724529678968032

Epoch: 6| Step: 8
Training loss: 2.559153255409701
Validation loss: 2.4742647376131472

Epoch: 6| Step: 9
Training loss: 2.841252666158302
Validation loss: 2.45940360246932

Epoch: 6| Step: 10
Training loss: 2.3299354835713757
Validation loss: 2.473155954980403

Epoch: 6| Step: 11
Training loss: 2.679370027203369
Validation loss: 2.4712307694516107

Epoch: 6| Step: 12
Training loss: 2.4346626591679046
Validation loss: 2.47100566072226

Epoch: 6| Step: 13
Training loss: 3.101319272283603
Validation loss: 2.4723882786086446

Epoch: 87| Step: 0
Training loss: 2.175390568009011
Validation loss: 2.473502530054192

Epoch: 6| Step: 1
Training loss: 2.6055763599872885
Validation loss: 2.4763143838326953

Epoch: 6| Step: 2
Training loss: 2.8709465975257578
Validation loss: 2.4781304499280408

Epoch: 6| Step: 3
Training loss: 2.803059820227518
Validation loss: 2.4787705262565916

Epoch: 6| Step: 4
Training loss: 3.006081457033119
Validation loss: 2.4734544796420095

Epoch: 6| Step: 5
Training loss: 2.7895114227515063
Validation loss: 2.472810496359941

Epoch: 6| Step: 6
Training loss: 2.394891459767006
Validation loss: 2.4724774288462523

Epoch: 6| Step: 7
Training loss: 2.3973242944130457
Validation loss: 2.471649752520639

Epoch: 6| Step: 8
Training loss: 2.388280878637882
Validation loss: 2.466428010796424

Epoch: 6| Step: 9
Training loss: 2.537773110110208
Validation loss: 2.4723112358779673

Epoch: 6| Step: 10
Training loss: 2.853350687449555
Validation loss: 2.464322276301608

Epoch: 6| Step: 11
Training loss: 2.5438557145930036
Validation loss: 2.4619400969272083

Epoch: 6| Step: 12
Training loss: 2.6745308509198344
Validation loss: 2.4615363467189066

Epoch: 6| Step: 13
Training loss: 2.027493568720654
Validation loss: 2.4691748515068643

Epoch: 88| Step: 0
Training loss: 2.937370135094805
Validation loss: 2.4657059102225847

Epoch: 6| Step: 1
Training loss: 2.5934150203728494
Validation loss: 2.4681232155698183

Epoch: 6| Step: 2
Training loss: 2.3300906082675876
Validation loss: 2.456008560829485

Epoch: 6| Step: 3
Training loss: 2.3502319221495194
Validation loss: 2.4610169735947975

Epoch: 6| Step: 4
Training loss: 2.698348748404768
Validation loss: 2.4604245736180537

Epoch: 6| Step: 5
Training loss: 2.7758525810234786
Validation loss: 2.463978449763357

Epoch: 6| Step: 6
Training loss: 2.1863727389866945
Validation loss: 2.4639063935468597

Epoch: 6| Step: 7
Training loss: 2.3484903843573006
Validation loss: 2.470451476860432

Epoch: 6| Step: 8
Training loss: 2.982690785294163
Validation loss: 2.4629351465414664

Epoch: 6| Step: 9
Training loss: 2.5048685352297793
Validation loss: 2.454045870020363

Epoch: 6| Step: 10
Training loss: 2.421812585826306
Validation loss: 2.465850237674231

Epoch: 6| Step: 11
Training loss: 2.5276650367532687
Validation loss: 2.465954529788829

Epoch: 6| Step: 12
Training loss: 2.3883346856116305
Validation loss: 2.470937250877951

Epoch: 6| Step: 13
Training loss: 2.8359882781051486
Validation loss: 2.4718494193726315

Epoch: 89| Step: 0
Training loss: 2.7031986469092684
Validation loss: 2.475349169161226

Epoch: 6| Step: 1
Training loss: 2.5133736529096953
Validation loss: 2.477178663425141

Epoch: 6| Step: 2
Training loss: 2.4897269895171883
Validation loss: 2.473504072278641

Epoch: 6| Step: 3
Training loss: 2.4558428633183196
Validation loss: 2.4708684210084453

Epoch: 6| Step: 4
Training loss: 2.1279021248645176
Validation loss: 2.4708014226164057

Epoch: 6| Step: 5
Training loss: 2.5189212030956396
Validation loss: 2.4692333730063356

Epoch: 6| Step: 6
Training loss: 3.035849316583095
Validation loss: 2.459494855158404

Epoch: 6| Step: 7
Training loss: 2.3109669114124745
Validation loss: 2.461890949250223

Epoch: 6| Step: 8
Training loss: 2.1302505028863643
Validation loss: 2.455258715089319

Epoch: 6| Step: 9
Training loss: 2.973590638812183
Validation loss: 2.460006004729347

Epoch: 6| Step: 10
Training loss: 2.526832776543639
Validation loss: 2.4773159064431742

Epoch: 6| Step: 11
Training loss: 2.582287012495858
Validation loss: 2.472346796214182

Epoch: 6| Step: 12
Training loss: 2.6833143241477533
Validation loss: 2.464537733037189

Epoch: 6| Step: 13
Training loss: 2.941775679977342
Validation loss: 2.4593813542659997

Epoch: 90| Step: 0
Training loss: 3.349676566381482
Validation loss: 2.461671362553661

Epoch: 6| Step: 1
Training loss: 1.7396438572875668
Validation loss: 2.459434752847201

Epoch: 6| Step: 2
Training loss: 2.6678666951221333
Validation loss: 2.4695224277878194

Epoch: 6| Step: 3
Training loss: 2.6135168952928542
Validation loss: 2.4702421660507374

Epoch: 6| Step: 4
Training loss: 2.690135107511792
Validation loss: 2.4731535368815347

Epoch: 6| Step: 5
Training loss: 2.2421857986709575
Validation loss: 2.473623929093759

Epoch: 6| Step: 6
Training loss: 2.3004141807807583
Validation loss: 2.4660046438682373

Epoch: 6| Step: 7
Training loss: 2.684095488485571
Validation loss: 2.4688774108200247

Epoch: 6| Step: 8
Training loss: 2.4779907344124563
Validation loss: 2.4616864391850877

Epoch: 6| Step: 9
Training loss: 2.7669950206120353
Validation loss: 2.4619418158676107

Epoch: 6| Step: 10
Training loss: 2.7582917485939245
Validation loss: 2.4604003966006447

Epoch: 6| Step: 11
Training loss: 2.5937419109907753
Validation loss: 2.465691696186757

Epoch: 6| Step: 12
Training loss: 2.5764433954122072
Validation loss: 2.4672530581280476

Epoch: 6| Step: 13
Training loss: 2.444978441326681
Validation loss: 2.4650274854111665

Epoch: 91| Step: 0
Training loss: 2.820477763482729
Validation loss: 2.465639609586481

Epoch: 6| Step: 1
Training loss: 2.4097429165301203
Validation loss: 2.4638316257373436

Epoch: 6| Step: 2
Training loss: 2.7814821296431895
Validation loss: 2.461311659046239

Epoch: 6| Step: 3
Training loss: 2.501114501485735
Validation loss: 2.4671514301107447

Epoch: 6| Step: 4
Training loss: 1.7970201516610975
Validation loss: 2.463529338093422

Epoch: 6| Step: 5
Training loss: 2.2458318673756
Validation loss: 2.46697049449888

Epoch: 6| Step: 6
Training loss: 2.3647105537058053
Validation loss: 2.466093050033688

Epoch: 6| Step: 7
Training loss: 2.4991489869302534
Validation loss: 2.4662121794388767

Epoch: 6| Step: 8
Training loss: 2.5599924239404155
Validation loss: 2.4639105382878728

Epoch: 6| Step: 9
Training loss: 2.8683894909495034
Validation loss: 2.46367830915061

Epoch: 6| Step: 10
Training loss: 2.2355978227242668
Validation loss: 2.4592539685208163

Epoch: 6| Step: 11
Training loss: 2.8735950188247394
Validation loss: 2.4583294000971305

Epoch: 6| Step: 12
Training loss: 2.8845778438375005
Validation loss: 2.460658417877631

Epoch: 6| Step: 13
Training loss: 2.8334428728657906
Validation loss: 2.4628545973762193

Epoch: 92| Step: 0
Training loss: 2.7251083737442667
Validation loss: 2.457173386320239

Epoch: 6| Step: 1
Training loss: 1.8743365067526307
Validation loss: 2.4597421800993318

Epoch: 6| Step: 2
Training loss: 2.842128752635534
Validation loss: 2.4610248368523977

Epoch: 6| Step: 3
Training loss: 2.631010893697308
Validation loss: 2.4570232364946367

Epoch: 6| Step: 4
Training loss: 2.630576794690874
Validation loss: 2.4614261041232512

Epoch: 6| Step: 5
Training loss: 2.457992486977948
Validation loss: 2.459981395539952

Epoch: 6| Step: 6
Training loss: 2.503549440286186
Validation loss: 2.4718337295501387

Epoch: 6| Step: 7
Training loss: 3.0817816842379018
Validation loss: 2.4726967951386776

Epoch: 6| Step: 8
Training loss: 2.6559890618903355
Validation loss: 2.477251873480055

Epoch: 6| Step: 9
Training loss: 2.5417584470426755
Validation loss: 2.4781371845524673

Epoch: 6| Step: 10
Training loss: 2.5308023692742623
Validation loss: 2.47596704683268

Epoch: 6| Step: 11
Training loss: 2.3649544333651145
Validation loss: 2.478366808260109

Epoch: 6| Step: 12
Training loss: 2.52549428350919
Validation loss: 2.4746011213502683

Epoch: 6| Step: 13
Training loss: 2.63540078461175
Validation loss: 2.478264938514705

Epoch: 93| Step: 0
Training loss: 2.7015182041399988
Validation loss: 2.4736935818538455

Epoch: 6| Step: 1
Training loss: 2.776018000651546
Validation loss: 2.4763623466597826

Epoch: 6| Step: 2
Training loss: 2.3273110214856887
Validation loss: 2.471794504558192

Epoch: 6| Step: 3
Training loss: 2.6917318692493533
Validation loss: 2.470000292460106

Epoch: 6| Step: 4
Training loss: 2.5228599619668373
Validation loss: 2.4736166279701695

Epoch: 6| Step: 5
Training loss: 2.6051976922229847
Validation loss: 2.4745335495794727

Epoch: 6| Step: 6
Training loss: 2.3629156160853975
Validation loss: 2.4732247211878153

Epoch: 6| Step: 7
Training loss: 2.8167912276976743
Validation loss: 2.475252593280777

Epoch: 6| Step: 8
Training loss: 2.1930652125621557
Validation loss: 2.4749590218487922

Epoch: 6| Step: 9
Training loss: 2.27100024061987
Validation loss: 2.4732963773977454

Epoch: 6| Step: 10
Training loss: 2.613608757446208
Validation loss: 2.4730777712639216

Epoch: 6| Step: 11
Training loss: 2.416896304916341
Validation loss: 2.467516627231529

Epoch: 6| Step: 12
Training loss: 2.768729584836073
Validation loss: 2.468046868868985

Epoch: 6| Step: 13
Training loss: 2.871900463302683
Validation loss: 2.470302672972849

Epoch: 94| Step: 0
Training loss: 2.5166633311201143
Validation loss: 2.457799293141451

Epoch: 6| Step: 1
Training loss: 1.899402890993818
Validation loss: 2.472566109812638

Epoch: 6| Step: 2
Training loss: 2.6228736257821743
Validation loss: 2.465840028992441

Epoch: 6| Step: 3
Training loss: 2.8227199096427293
Validation loss: 2.478716710608206

Epoch: 6| Step: 4
Training loss: 2.818554613061724
Validation loss: 2.46310066539582

Epoch: 6| Step: 5
Training loss: 3.0049600922890805
Validation loss: 2.4695168442966184

Epoch: 6| Step: 6
Training loss: 2.647059259227642
Validation loss: 2.4699512569029864

Epoch: 6| Step: 7
Training loss: 2.3693485278857223
Validation loss: 2.4717430851167213

Epoch: 6| Step: 8
Training loss: 2.3059873891454186
Validation loss: 2.4535180038248696

Epoch: 6| Step: 9
Training loss: 3.262424927063322
Validation loss: 2.4568836873374074

Epoch: 6| Step: 10
Training loss: 1.8100639283498068
Validation loss: 2.466221556798375

Epoch: 6| Step: 11
Training loss: 2.615337663224748
Validation loss: 2.470341004784859

Epoch: 6| Step: 12
Training loss: 2.226129857911957
Validation loss: 2.475329504331502

Epoch: 6| Step: 13
Training loss: 2.590138874528403
Validation loss: 2.480819653298313

Epoch: 95| Step: 0
Training loss: 2.2145907679732355
Validation loss: 2.4727213179557843

Epoch: 6| Step: 1
Training loss: 2.565656625085804
Validation loss: 2.474693290895396

Epoch: 6| Step: 2
Training loss: 2.4188069260475515
Validation loss: 2.476258011480341

Epoch: 6| Step: 3
Training loss: 2.89635546983263
Validation loss: 2.472248704699383

Epoch: 6| Step: 4
Training loss: 2.3969289403065646
Validation loss: 2.477727798372197

Epoch: 6| Step: 5
Training loss: 2.8418591259826993
Validation loss: 2.477111546960246

Epoch: 6| Step: 6
Training loss: 2.2542150011597957
Validation loss: 2.478277853845243

Epoch: 6| Step: 7
Training loss: 2.955894503681505
Validation loss: 2.4793267452158956

Epoch: 6| Step: 8
Training loss: 2.596081891923904
Validation loss: 2.482691580354771

Epoch: 6| Step: 9
Training loss: 2.726115804987409
Validation loss: 2.481596522632633

Epoch: 6| Step: 10
Training loss: 2.3854842419332734
Validation loss: 2.4795564190183725

Epoch: 6| Step: 11
Training loss: 2.8527050498929207
Validation loss: 2.4762498917018783

Epoch: 6| Step: 12
Training loss: 2.813620280627279
Validation loss: 2.4749154551672934

Epoch: 6| Step: 13
Training loss: 2.259817533300748
Validation loss: 2.4725996174687235

Epoch: 96| Step: 0
Training loss: 2.358191389073932
Validation loss: 2.4706567330207894

Epoch: 6| Step: 1
Training loss: 3.055501141689801
Validation loss: 2.4703635403312423

Epoch: 6| Step: 2
Training loss: 2.5383658524465496
Validation loss: 2.469552871269939

Epoch: 6| Step: 3
Training loss: 2.8141752551934927
Validation loss: 2.466674415043526

Epoch: 6| Step: 4
Training loss: 2.63152148008341
Validation loss: 2.467718834544025

Epoch: 6| Step: 5
Training loss: 2.8728998225665525
Validation loss: 2.4759380945335536

Epoch: 6| Step: 6
Training loss: 2.0426683369140837
Validation loss: 2.469999938532408

Epoch: 6| Step: 7
Training loss: 2.95509974742172
Validation loss: 2.473606965393937

Epoch: 6| Step: 8
Training loss: 2.2251308188395815
Validation loss: 2.4696896209508177

Epoch: 6| Step: 9
Training loss: 2.620479870013412
Validation loss: 2.4711439862664

Epoch: 6| Step: 10
Training loss: 2.014745476031878
Validation loss: 2.4710037792356503

Epoch: 6| Step: 11
Training loss: 2.4638582850116615
Validation loss: 2.4731728093048524

Epoch: 6| Step: 12
Training loss: 2.8746126992120504
Validation loss: 2.467930846986398

Epoch: 6| Step: 13
Training loss: 2.1960804095112416
Validation loss: 2.4655803823040077

Epoch: 97| Step: 0
Training loss: 2.2757614161068753
Validation loss: 2.4591006008371306

Epoch: 6| Step: 1
Training loss: 2.91577965100341
Validation loss: 2.461016699106912

Epoch: 6| Step: 2
Training loss: 2.3490129508176687
Validation loss: 2.4596927300246816

Epoch: 6| Step: 3
Training loss: 2.304082862291135
Validation loss: 2.469958440158154

Epoch: 6| Step: 4
Training loss: 2.6891577287621367
Validation loss: 2.4633644448880734

Epoch: 6| Step: 5
Training loss: 2.4846206639545603
Validation loss: 2.4801441931465034

Epoch: 6| Step: 6
Training loss: 2.7277501533682367
Validation loss: 2.4725701114702625

Epoch: 6| Step: 7
Training loss: 1.8366875597578347
Validation loss: 2.460018943232971

Epoch: 6| Step: 8
Training loss: 2.919704671582846
Validation loss: 2.4603717779403222

Epoch: 6| Step: 9
Training loss: 2.940835236220601
Validation loss: 2.463124227071165

Epoch: 6| Step: 10
Training loss: 2.6156103136021756
Validation loss: 2.465346021707596

Epoch: 6| Step: 11
Training loss: 2.370852613677976
Validation loss: 2.4673495525525158

Epoch: 6| Step: 12
Training loss: 2.040260167828897
Validation loss: 2.469456704307974

Epoch: 6| Step: 13
Training loss: 3.1282545308705476
Validation loss: 2.4711484324310233

Epoch: 98| Step: 0
Training loss: 3.2405851885889305
Validation loss: 2.4767506043631804

Epoch: 6| Step: 1
Training loss: 2.306925472595159
Validation loss: 2.478951329942349

Epoch: 6| Step: 2
Training loss: 2.646960001079437
Validation loss: 2.479506915382895

Epoch: 6| Step: 3
Training loss: 2.360800912214547
Validation loss: 2.475410169271758

Epoch: 6| Step: 4
Training loss: 2.9243705006919667
Validation loss: 2.4826066543395267

Epoch: 6| Step: 5
Training loss: 3.04191177286369
Validation loss: 2.4782988581496763

Epoch: 6| Step: 6
Training loss: 2.529799622754067
Validation loss: 2.4780330604023755

Epoch: 6| Step: 7
Training loss: 2.7415981257102486
Validation loss: 2.482059639414253

Epoch: 6| Step: 8
Training loss: 2.222520659858102
Validation loss: 2.4742455379351673

Epoch: 6| Step: 9
Training loss: 2.562175451055643
Validation loss: 2.477710253342352

Epoch: 6| Step: 10
Training loss: 2.2322879460728466
Validation loss: 2.475346680966914

Epoch: 6| Step: 11
Training loss: 2.6155517020944825
Validation loss: 2.4741604264016717

Epoch: 6| Step: 12
Training loss: 2.4523338451349534
Validation loss: 2.473631045459709

Epoch: 6| Step: 13
Training loss: 2.222629111179604
Validation loss: 2.4721384093785104

Epoch: 99| Step: 0
Training loss: 2.9277755805660193
Validation loss: 2.471587502036409

Epoch: 6| Step: 1
Training loss: 2.691231908812713
Validation loss: 2.475603497585239

Epoch: 6| Step: 2
Training loss: 2.638410824978924
Validation loss: 2.4750896771557533

Epoch: 6| Step: 3
Training loss: 2.336827035408121
Validation loss: 2.4700939286862953

Epoch: 6| Step: 4
Training loss: 2.2015511593008785
Validation loss: 2.469413161237554

Epoch: 6| Step: 5
Training loss: 2.3807155700264175
Validation loss: 2.466204219933938

Epoch: 6| Step: 6
Training loss: 1.7049656292311999
Validation loss: 2.4614213255948068

Epoch: 6| Step: 7
Training loss: 2.6022863898802635
Validation loss: 2.456671271464338

Epoch: 6| Step: 8
Training loss: 2.3742236826155985
Validation loss: 2.4496999913733473

Epoch: 6| Step: 9
Training loss: 2.8144432136525257
Validation loss: 2.454706706638255

Epoch: 6| Step: 10
Training loss: 2.2387423215188083
Validation loss: 2.468421906185641

Epoch: 6| Step: 11
Training loss: 3.014679913839968
Validation loss: 2.4986568180512347

Epoch: 6| Step: 12
Training loss: 2.9062293267283867
Validation loss: 2.5059543432058247

Epoch: 6| Step: 13
Training loss: 3.1253504747315595
Validation loss: 2.484643133956641

Epoch: 100| Step: 0
Training loss: 2.8167533925114663
Validation loss: 2.4632006698867666

Epoch: 6| Step: 1
Training loss: 2.6605710953889927
Validation loss: 2.4597118575154604

Epoch: 6| Step: 2
Training loss: 2.3952375804633688
Validation loss: 2.46672863034012

Epoch: 6| Step: 3
Training loss: 2.1314015175217556
Validation loss: 2.4745115177001855

Epoch: 6| Step: 4
Training loss: 2.6755100084922114
Validation loss: 2.477704592076777

Epoch: 6| Step: 5
Training loss: 2.2364967782338283
Validation loss: 2.471862649545146

Epoch: 6| Step: 6
Training loss: 2.4343999075441722
Validation loss: 2.4750830867577083

Epoch: 6| Step: 7
Training loss: 2.1099919300015415
Validation loss: 2.4760818095170163

Epoch: 6| Step: 8
Training loss: 2.876154957858455
Validation loss: 2.480665528509461

Epoch: 6| Step: 9
Training loss: 2.4695361853275384
Validation loss: 2.484377599111033

Epoch: 6| Step: 10
Training loss: 2.614121739547959
Validation loss: 2.485269669318363

Epoch: 6| Step: 11
Training loss: 2.6527057114199732
Validation loss: 2.4863254080630446

Epoch: 6| Step: 12
Training loss: 3.1102928455505614
Validation loss: 2.4906600847569127

Epoch: 6| Step: 13
Training loss: 2.8436128352694534
Validation loss: 2.48468537061342

Epoch: 101| Step: 0
Training loss: 2.334080701300691
Validation loss: 2.483488352522018

Epoch: 6| Step: 1
Training loss: 2.7058824355644937
Validation loss: 2.480094228589983

Epoch: 6| Step: 2
Training loss: 2.5657610728227405
Validation loss: 2.4824343125026496

Epoch: 6| Step: 3
Training loss: 2.783553466162778
Validation loss: 2.481570806511411

Epoch: 6| Step: 4
Training loss: 2.9411912816740613
Validation loss: 2.4794817865811374

Epoch: 6| Step: 5
Training loss: 2.6868476963186603
Validation loss: 2.4788226896096957

Epoch: 6| Step: 6
Training loss: 2.5489815267534737
Validation loss: 2.4753168224400843

Epoch: 6| Step: 7
Training loss: 2.0294432601045753
Validation loss: 2.4736116079331967

Epoch: 6| Step: 8
Training loss: 2.4736993165532106
Validation loss: 2.470963222398242

Epoch: 6| Step: 9
Training loss: 2.4575707066280255
Validation loss: 2.4726543858397068

Epoch: 6| Step: 10
Training loss: 2.9821740467799804
Validation loss: 2.4670870689033513

Epoch: 6| Step: 11
Training loss: 3.0314437961568435
Validation loss: 2.47212767211854

Epoch: 6| Step: 12
Training loss: 2.1832030463717778
Validation loss: 2.480007507212874

Epoch: 6| Step: 13
Training loss: 2.256273636205402
Validation loss: 2.4745079126108838

Epoch: 102| Step: 0
Training loss: 2.752729881589342
Validation loss: 2.4709640586284145

Epoch: 6| Step: 1
Training loss: 2.5836326261674802
Validation loss: 2.4661665972299116

Epoch: 6| Step: 2
Training loss: 1.881643541811725
Validation loss: 2.471064372033278

Epoch: 6| Step: 3
Training loss: 2.59266437653159
Validation loss: 2.469510230980911

Epoch: 6| Step: 4
Training loss: 2.676260310816424
Validation loss: 2.4634592527727377

Epoch: 6| Step: 5
Training loss: 2.697522128405946
Validation loss: 2.4645758481743707

Epoch: 6| Step: 6
Training loss: 2.8550629333935347
Validation loss: 2.4692609073225116

Epoch: 6| Step: 7
Training loss: 2.05496480832747
Validation loss: 2.463901039240755

Epoch: 6| Step: 8
Training loss: 3.158744053233096
Validation loss: 2.4646488682946304

Epoch: 6| Step: 9
Training loss: 1.499543120422792
Validation loss: 2.4674851118654453

Epoch: 6| Step: 10
Training loss: 2.752753006812655
Validation loss: 2.4548157941516404

Epoch: 6| Step: 11
Training loss: 2.851580225223164
Validation loss: 2.4609888787936076

Epoch: 6| Step: 12
Training loss: 2.060701105935587
Validation loss: 2.4645568712952337

Epoch: 6| Step: 13
Training loss: 2.792479572272653
Validation loss: 2.4717338814302297

Epoch: 103| Step: 0
Training loss: 2.7910808024394154
Validation loss: 2.4692307418618626

Epoch: 6| Step: 1
Training loss: 2.471340413887714
Validation loss: 2.4698592321667157

Epoch: 6| Step: 2
Training loss: 2.968480469364262
Validation loss: 2.4721940800378284

Epoch: 6| Step: 3
Training loss: 2.726492544768771
Validation loss: 2.4690359086857607

Epoch: 6| Step: 4
Training loss: 2.891624978797453
Validation loss: 2.4655028931422764

Epoch: 6| Step: 5
Training loss: 2.3722500189235354
Validation loss: 2.467946062489903

Epoch: 6| Step: 6
Training loss: 2.9058788431722835
Validation loss: 2.464045488536283

Epoch: 6| Step: 7
Training loss: 2.526428245511441
Validation loss: 2.469607240593234

Epoch: 6| Step: 8
Training loss: 2.4106286573592457
Validation loss: 2.4681876785538295

Epoch: 6| Step: 9
Training loss: 2.4546144665222385
Validation loss: 2.4602139817190594

Epoch: 6| Step: 10
Training loss: 2.288029773004928
Validation loss: 2.46312532004995

Epoch: 6| Step: 11
Training loss: 2.255855041763966
Validation loss: 2.463113894162892

Epoch: 6| Step: 12
Training loss: 2.2135320206974263
Validation loss: 2.4662380154281913

Epoch: 6| Step: 13
Training loss: 2.3338039695575876
Validation loss: 2.465403642859096

Epoch: 104| Step: 0
Training loss: 2.0983844537343077
Validation loss: 2.464618186886965

Epoch: 6| Step: 1
Training loss: 2.194677294946817
Validation loss: 2.4654672905041464

Epoch: 6| Step: 2
Training loss: 2.2402394834292236
Validation loss: 2.464489556208202

Epoch: 6| Step: 3
Training loss: 2.2431644876468906
Validation loss: 2.465488420063036

Epoch: 6| Step: 4
Training loss: 3.1630093552592475
Validation loss: 2.459726784609323

Epoch: 6| Step: 5
Training loss: 2.4630857748815465
Validation loss: 2.457587069588825

Epoch: 6| Step: 6
Training loss: 2.539391035686076
Validation loss: 2.4592329308184806

Epoch: 6| Step: 7
Training loss: 2.5106130393904054
Validation loss: 2.460383018717934

Epoch: 6| Step: 8
Training loss: 2.839989156030718
Validation loss: 2.465000476076236

Epoch: 6| Step: 9
Training loss: 2.753567375814846
Validation loss: 2.4626367747216036

Epoch: 6| Step: 10
Training loss: 2.4982754481223965
Validation loss: 2.456989799436935

Epoch: 6| Step: 11
Training loss: 2.696597823909523
Validation loss: 2.460586748911872

Epoch: 6| Step: 12
Training loss: 2.2760686690927807
Validation loss: 2.4641708688847412

Epoch: 6| Step: 13
Training loss: 2.8163771290536785
Validation loss: 2.4601919831206334

Epoch: 105| Step: 0
Training loss: 2.36231528473362
Validation loss: 2.4589761820225653

Epoch: 6| Step: 1
Training loss: 2.846966276579417
Validation loss: 2.4626918130646516

Epoch: 6| Step: 2
Training loss: 2.234473699777274
Validation loss: 2.4545425725153605

Epoch: 6| Step: 3
Training loss: 2.8653192488068426
Validation loss: 2.4678408562332645

Epoch: 6| Step: 4
Training loss: 2.785004899244947
Validation loss: 2.465601817103844

Epoch: 6| Step: 5
Training loss: 2.6635861684109816
Validation loss: 2.4605790215226433

Epoch: 6| Step: 6
Training loss: 2.4418321894067874
Validation loss: 2.463435831410944

Epoch: 6| Step: 7
Training loss: 2.480402331326562
Validation loss: 2.4605916421050416

Epoch: 6| Step: 8
Training loss: 2.741253468258634
Validation loss: 2.4621033989766556

Epoch: 6| Step: 9
Training loss: 2.522968071419909
Validation loss: 2.4628044191984695

Epoch: 6| Step: 10
Training loss: 1.9873885453360791
Validation loss: 2.4668341176012207

Epoch: 6| Step: 11
Training loss: 2.686123783914924
Validation loss: 2.4538592395896557

Epoch: 6| Step: 12
Training loss: 2.530190234213106
Validation loss: 2.466811871980137

Epoch: 6| Step: 13
Training loss: 2.311915349512821
Validation loss: 2.459845358089553

Epoch: 106| Step: 0
Training loss: 2.4680344715647777
Validation loss: 2.4648688103983276

Epoch: 6| Step: 1
Training loss: 2.0672568750021494
Validation loss: 2.4663833025820487

Epoch: 6| Step: 2
Training loss: 2.53618320066906
Validation loss: 2.4628463043331172

Epoch: 6| Step: 3
Training loss: 2.6468403819589046
Validation loss: 2.460907749218213

Epoch: 6| Step: 4
Training loss: 2.8975637923583553
Validation loss: 2.4632175116696744

Epoch: 6| Step: 5
Training loss: 2.437579129230677
Validation loss: 2.4602039838488583

Epoch: 6| Step: 6
Training loss: 2.5965392041375086
Validation loss: 2.466347019860646

Epoch: 6| Step: 7
Training loss: 2.3156855525080435
Validation loss: 2.4546214761157175

Epoch: 6| Step: 8
Training loss: 2.2141476029951392
Validation loss: 2.4660602193524834

Epoch: 6| Step: 9
Training loss: 2.700748258681064
Validation loss: 2.4661108952390154

Epoch: 6| Step: 10
Training loss: 2.6357062762408634
Validation loss: 2.4609899283178094

Epoch: 6| Step: 11
Training loss: 2.7709752287898377
Validation loss: 2.4670355432931945

Epoch: 6| Step: 12
Training loss: 2.419983057167707
Validation loss: 2.4649343337932392

Epoch: 6| Step: 13
Training loss: 2.661414388056169
Validation loss: 2.457119712187244

Epoch: 107| Step: 0
Training loss: 2.8939562598391677
Validation loss: 2.4642523828467664

Epoch: 6| Step: 1
Training loss: 2.286416461071664
Validation loss: 2.4646292873772917

Epoch: 6| Step: 2
Training loss: 2.9086784143495525
Validation loss: 2.4624223377354792

Epoch: 6| Step: 3
Training loss: 1.885208188608938
Validation loss: 2.4592021333659146

Epoch: 6| Step: 4
Training loss: 3.0462320285121427
Validation loss: 2.463707276491107

Epoch: 6| Step: 5
Training loss: 2.22626265799058
Validation loss: 2.4605132448738756

Epoch: 6| Step: 6
Training loss: 2.443046226529436
Validation loss: 2.462336631945517

Epoch: 6| Step: 7
Training loss: 2.120530870475113
Validation loss: 2.4559491012861643

Epoch: 6| Step: 8
Training loss: 2.75101781562963
Validation loss: 2.460652685083137

Epoch: 6| Step: 9
Training loss: 1.80193789856285
Validation loss: 2.455641992072786

Epoch: 6| Step: 10
Training loss: 2.5348105172814215
Validation loss: 2.4507971731780698

Epoch: 6| Step: 11
Training loss: 2.7684404083832628
Validation loss: 2.456569844614937

Epoch: 6| Step: 12
Training loss: 2.8726828815884615
Validation loss: 2.460194841980448

Epoch: 6| Step: 13
Training loss: 2.559221263548652
Validation loss: 2.457190107699728

Epoch: 108| Step: 0
Training loss: 2.5504806364783055
Validation loss: 2.455964310122693

Epoch: 6| Step: 1
Training loss: 2.5658746223889866
Validation loss: 2.459652334020113

Epoch: 6| Step: 2
Training loss: 2.5718197562865255
Validation loss: 2.4626690862990044

Epoch: 6| Step: 3
Training loss: 2.3262763140059746
Validation loss: 2.4599996081406195

Epoch: 6| Step: 4
Training loss: 2.5199524993052713
Validation loss: 2.457193244967148

Epoch: 6| Step: 5
Training loss: 2.258829805012475
Validation loss: 2.44881702395631

Epoch: 6| Step: 6
Training loss: 3.3040915238124824
Validation loss: 2.449598146101713

Epoch: 6| Step: 7
Training loss: 2.5218778815837974
Validation loss: 2.444031791342888

Epoch: 6| Step: 8
Training loss: 2.555449299284926
Validation loss: 2.4557312971788847

Epoch: 6| Step: 9
Training loss: 2.630014081271807
Validation loss: 2.4532108575330094

Epoch: 6| Step: 10
Training loss: 2.7910555175322154
Validation loss: 2.4472597065798474

Epoch: 6| Step: 11
Training loss: 2.004466077152608
Validation loss: 2.4571745911042866

Epoch: 6| Step: 12
Training loss: 2.3540769801877555
Validation loss: 2.456887488109027

Epoch: 6| Step: 13
Training loss: 2.3982791413455673
Validation loss: 2.4559144765623575

Epoch: 109| Step: 0
Training loss: 1.8578971310934687
Validation loss: 2.459814891389457

Epoch: 6| Step: 1
Training loss: 2.1038386274902408
Validation loss: 2.466994389680571

Epoch: 6| Step: 2
Training loss: 2.7500494172251115
Validation loss: 2.470148117190708

Epoch: 6| Step: 3
Training loss: 2.5878441918004174
Validation loss: 2.4707414103922902

Epoch: 6| Step: 4
Training loss: 3.529687316273097
Validation loss: 2.4769693997048035

Epoch: 6| Step: 5
Training loss: 2.480007819655611
Validation loss: 2.4789031447006318

Epoch: 6| Step: 6
Training loss: 2.208587475913831
Validation loss: 2.4786711820099927

Epoch: 6| Step: 7
Training loss: 2.6234416877572295
Validation loss: 2.475327305071444

Epoch: 6| Step: 8
Training loss: 3.0820281556939473
Validation loss: 2.480155961147155

Epoch: 6| Step: 9
Training loss: 2.1116779330504283
Validation loss: 2.477984512535222

Epoch: 6| Step: 10
Training loss: 2.205127320192759
Validation loss: 2.47197847081452

Epoch: 6| Step: 11
Training loss: 2.37601439245598
Validation loss: 2.4673093704876465

Epoch: 6| Step: 12
Training loss: 2.7366607703433092
Validation loss: 2.4582320262487594

Epoch: 6| Step: 13
Training loss: 2.7951291565235317
Validation loss: 2.4557033684441705

Epoch: 110| Step: 0
Training loss: 3.1576752419065532
Validation loss: 2.4586735721637347

Epoch: 6| Step: 1
Training loss: 2.4400566582276992
Validation loss: 2.454468183232368

Epoch: 6| Step: 2
Training loss: 2.032927304778425
Validation loss: 2.4561827083361556

Epoch: 6| Step: 3
Training loss: 2.4071331632806237
Validation loss: 2.459567783786279

Epoch: 6| Step: 4
Training loss: 2.40928963239071
Validation loss: 2.4491513216105387

Epoch: 6| Step: 5
Training loss: 2.8300599281247285
Validation loss: 2.4614867712555597

Epoch: 6| Step: 6
Training loss: 2.205928236540261
Validation loss: 2.45140498092696

Epoch: 6| Step: 7
Training loss: 2.6243165352566904
Validation loss: 2.4584615533871643

Epoch: 6| Step: 8
Training loss: 2.9129112771872485
Validation loss: 2.45342633447733

Epoch: 6| Step: 9
Training loss: 2.556240344787122
Validation loss: 2.4463074338912207

Epoch: 6| Step: 10
Training loss: 2.576462550679697
Validation loss: 2.4560457566935043

Epoch: 6| Step: 11
Training loss: 2.4066531351436433
Validation loss: 2.4622572493596047

Epoch: 6| Step: 12
Training loss: 2.3448870126146186
Validation loss: 2.4596660740673246

Epoch: 6| Step: 13
Training loss: 2.2987908916974638
Validation loss: 2.4495609902184117

Epoch: 111| Step: 0
Training loss: 2.9800053907512063
Validation loss: 2.4511757332618207

Epoch: 6| Step: 1
Training loss: 2.3298624199454405
Validation loss: 2.4515875272686993

Epoch: 6| Step: 2
Training loss: 2.9815708587532517
Validation loss: 2.4558035770818827

Epoch: 6| Step: 3
Training loss: 1.933246064956944
Validation loss: 2.453299765223308

Epoch: 6| Step: 4
Training loss: 2.3433691096751463
Validation loss: 2.455225755718458

Epoch: 6| Step: 5
Training loss: 2.420683142970618
Validation loss: 2.4515777697633654

Epoch: 6| Step: 6
Training loss: 2.376105603321276
Validation loss: 2.4531483416246616

Epoch: 6| Step: 7
Training loss: 2.7957234489423857
Validation loss: 2.4602066004197827

Epoch: 6| Step: 8
Training loss: 2.964562283349829
Validation loss: 2.457043751339628

Epoch: 6| Step: 9
Training loss: 1.9401604629302507
Validation loss: 2.4501675233852636

Epoch: 6| Step: 10
Training loss: 2.290210423026295
Validation loss: 2.4539631588099793

Epoch: 6| Step: 11
Training loss: 3.091164327221732
Validation loss: 2.4522198669571313

Epoch: 6| Step: 12
Training loss: 1.64280621082422
Validation loss: 2.449655991685902

Epoch: 6| Step: 13
Training loss: 2.738705938168477
Validation loss: 2.4530292905372044

Epoch: 112| Step: 0
Training loss: 2.544703019657098
Validation loss: 2.4496846138765664

Epoch: 6| Step: 1
Training loss: 2.832878880425294
Validation loss: 2.4527417752220666

Epoch: 6| Step: 2
Training loss: 2.4711817100710043
Validation loss: 2.456094665404215

Epoch: 6| Step: 3
Training loss: 2.5146556431935596
Validation loss: 2.4519814581654846

Epoch: 6| Step: 4
Training loss: 2.057085733528841
Validation loss: 2.4558975685175004

Epoch: 6| Step: 5
Training loss: 2.8389431077524563
Validation loss: 2.4582462268418777

Epoch: 6| Step: 6
Training loss: 2.750680059095587
Validation loss: 2.4539193732036146

Epoch: 6| Step: 7
Training loss: 2.7223681348071893
Validation loss: 2.4545284071564715

Epoch: 6| Step: 8
Training loss: 2.03108942424197
Validation loss: 2.451696396999038

Epoch: 6| Step: 9
Training loss: 2.1507068093217834
Validation loss: 2.4465181346246925

Epoch: 6| Step: 10
Training loss: 1.9510860328769148
Validation loss: 2.4542674261456745

Epoch: 6| Step: 11
Training loss: 2.7603766864304533
Validation loss: 2.4631489661843995

Epoch: 6| Step: 12
Training loss: 2.563743103702519
Validation loss: 2.465137728144209

Epoch: 6| Step: 13
Training loss: 3.0168046768682295
Validation loss: 2.4546280162264553

Epoch: 113| Step: 0
Training loss: 2.376263934660965
Validation loss: 2.455602686440742

Epoch: 6| Step: 1
Training loss: 3.097626078628285
Validation loss: 2.463415022989491

Epoch: 6| Step: 2
Training loss: 2.7994560394514663
Validation loss: 2.464719983846064

Epoch: 6| Step: 3
Training loss: 2.1595094895610325
Validation loss: 2.475344321193224

Epoch: 6| Step: 4
Training loss: 3.1476078880914145
Validation loss: 2.484175780044043

Epoch: 6| Step: 5
Training loss: 2.4155399282855172
Validation loss: 2.479234771968451

Epoch: 6| Step: 6
Training loss: 2.6093050096456367
Validation loss: 2.4833229284473886

Epoch: 6| Step: 7
Training loss: 1.898267385137158
Validation loss: 2.48061702415265

Epoch: 6| Step: 8
Training loss: 2.8084850900399245
Validation loss: 2.479550249146855

Epoch: 6| Step: 9
Training loss: 3.1696775745355663
Validation loss: 2.4772169610232573

Epoch: 6| Step: 10
Training loss: 2.008095806632672
Validation loss: 2.48295600721246

Epoch: 6| Step: 11
Training loss: 2.330674791323353
Validation loss: 2.481991470205449

Epoch: 6| Step: 12
Training loss: 2.515600263580585
Validation loss: 2.4755007035068166

Epoch: 6| Step: 13
Training loss: 2.5210193113550514
Validation loss: 2.4782416009590347

Epoch: 114| Step: 0
Training loss: 2.3658917094041816
Validation loss: 2.4765979270928233

Epoch: 6| Step: 1
Training loss: 2.273336506596175
Validation loss: 2.481762942408012

Epoch: 6| Step: 2
Training loss: 2.7253773034344144
Validation loss: 2.4796275717519123

Epoch: 6| Step: 3
Training loss: 2.6896330773924197
Validation loss: 2.47956013695951

Epoch: 6| Step: 4
Training loss: 2.8945190594168055
Validation loss: 2.472957823704655

Epoch: 6| Step: 5
Training loss: 2.717910110705107
Validation loss: 2.4739767990761647

Epoch: 6| Step: 6
Training loss: 2.3869229243427466
Validation loss: 2.470853480813888

Epoch: 6| Step: 7
Training loss: 3.090039118053141
Validation loss: 2.461475261081686

Epoch: 6| Step: 8
Training loss: 2.4623111826739215
Validation loss: 2.460950062230255

Epoch: 6| Step: 9
Training loss: 2.926556269389056
Validation loss: 2.4612330587206683

Epoch: 6| Step: 10
Training loss: 2.476431184289695
Validation loss: 2.449853785362267

Epoch: 6| Step: 11
Training loss: 2.620420639545399
Validation loss: 2.454653253767647

Epoch: 6| Step: 12
Training loss: 2.157034026534115
Validation loss: 2.453089233915224

Epoch: 6| Step: 13
Training loss: 1.6772144653358836
Validation loss: 2.4571208118799497

Epoch: 115| Step: 0
Training loss: 2.359221598709805
Validation loss: 2.4605681449995083

Epoch: 6| Step: 1
Training loss: 2.911036333995472
Validation loss: 2.454100558605462

Epoch: 6| Step: 2
Training loss: 3.008722024113193
Validation loss: 2.4532364417030292

Epoch: 6| Step: 3
Training loss: 2.742614104297495
Validation loss: 2.453878040105016

Epoch: 6| Step: 4
Training loss: 2.5106267142105994
Validation loss: 2.457481225822608

Epoch: 6| Step: 5
Training loss: 2.682489381682044
Validation loss: 2.4531993976172433

Epoch: 6| Step: 6
Training loss: 2.3874445684099608
Validation loss: 2.458374869001116

Epoch: 6| Step: 7
Training loss: 2.319200037136913
Validation loss: 2.4629011766538937

Epoch: 6| Step: 8
Training loss: 2.2547791904514445
Validation loss: 2.462461469965253

Epoch: 6| Step: 9
Training loss: 2.2933620654938123
Validation loss: 2.4636488575966737

Epoch: 6| Step: 10
Training loss: 2.612138575326076
Validation loss: 2.4692638039597834

Epoch: 6| Step: 11
Training loss: 2.4508137110711847
Validation loss: 2.468342010973046

Epoch: 6| Step: 12
Training loss: 2.1511578347624365
Validation loss: 2.459919229385821

Epoch: 6| Step: 13
Training loss: 2.8110559783106064
Validation loss: 2.462608020691729

Epoch: 116| Step: 0
Training loss: 2.002814458380932
Validation loss: 2.4620461282863393

Epoch: 6| Step: 1
Training loss: 2.0979445843573714
Validation loss: 2.4555536384728467

Epoch: 6| Step: 2
Training loss: 2.2036106304860246
Validation loss: 2.462312465633351

Epoch: 6| Step: 3
Training loss: 2.1642645455799516
Validation loss: 2.4649954626726402

Epoch: 6| Step: 4
Training loss: 2.4248149132637784
Validation loss: 2.4617394810752686

Epoch: 6| Step: 5
Training loss: 2.9696507743741054
Validation loss: 2.4570071528010677

Epoch: 6| Step: 6
Training loss: 2.410400872823087
Validation loss: 2.451323882679812

Epoch: 6| Step: 7
Training loss: 3.007939957729649
Validation loss: 2.4591920425112352

Epoch: 6| Step: 8
Training loss: 2.2477068341463187
Validation loss: 2.460481906276801

Epoch: 6| Step: 9
Training loss: 2.7207666569047158
Validation loss: 2.4651129767505426

Epoch: 6| Step: 10
Training loss: 2.4821396854141216
Validation loss: 2.464959232065416

Epoch: 6| Step: 11
Training loss: 3.1445412582332968
Validation loss: 2.4593898447987126

Epoch: 6| Step: 12
Training loss: 2.9816901627779235
Validation loss: 2.464608287496846

Epoch: 6| Step: 13
Training loss: 2.203390335446018
Validation loss: 2.4713575378545043

Epoch: 117| Step: 0
Training loss: 2.7869319525145886
Validation loss: 2.464365611021787

Epoch: 6| Step: 1
Training loss: 2.6079176070766295
Validation loss: 2.459666784896603

Epoch: 6| Step: 2
Training loss: 2.316626707139485
Validation loss: 2.462230960071673

Epoch: 6| Step: 3
Training loss: 2.7016443155232106
Validation loss: 2.4555682833827417

Epoch: 6| Step: 4
Training loss: 2.5724139217882036
Validation loss: 2.4593847634082513

Epoch: 6| Step: 5
Training loss: 2.3196317656732313
Validation loss: 2.45829869639247

Epoch: 6| Step: 6
Training loss: 2.4090082782557203
Validation loss: 2.4553310736391083

Epoch: 6| Step: 7
Training loss: 2.7678844468136825
Validation loss: 2.452964753183464

Epoch: 6| Step: 8
Training loss: 1.8098507955605518
Validation loss: 2.450683147853221

Epoch: 6| Step: 9
Training loss: 2.5531406210674774
Validation loss: 2.455950670713015

Epoch: 6| Step: 10
Training loss: 2.0715074077881646
Validation loss: 2.4520386722314376

Epoch: 6| Step: 11
Training loss: 2.471474122384446
Validation loss: 2.45513402065437

Epoch: 6| Step: 12
Training loss: 2.538198658640631
Validation loss: 2.456649127916577

Epoch: 6| Step: 13
Training loss: 3.1246851953253234
Validation loss: 2.457568928038765

Epoch: 118| Step: 0
Training loss: 2.6221276871923447
Validation loss: 2.4629484649328166

Epoch: 6| Step: 1
Training loss: 2.725376953510616
Validation loss: 2.4638748965128277

Epoch: 6| Step: 2
Training loss: 2.0694940064687453
Validation loss: 2.4654511893685944

Epoch: 6| Step: 3
Training loss: 2.1857338451219137
Validation loss: 2.463354975997183

Epoch: 6| Step: 4
Training loss: 2.649514128570119
Validation loss: 2.460111279529351

Epoch: 6| Step: 5
Training loss: 1.8044269588377007
Validation loss: 2.4645687540310477

Epoch: 6| Step: 6
Training loss: 1.9651020706884101
Validation loss: 2.4592203275004634

Epoch: 6| Step: 7
Training loss: 2.536315934859721
Validation loss: 2.4560293349265137

Epoch: 6| Step: 8
Training loss: 2.4021438639530817
Validation loss: 2.4562780279966048

Epoch: 6| Step: 9
Training loss: 2.867969208885911
Validation loss: 2.452194977073958

Epoch: 6| Step: 10
Training loss: 2.392755494063302
Validation loss: 2.4592607952327072

Epoch: 6| Step: 11
Training loss: 2.850317780027966
Validation loss: 2.456447724075794

Epoch: 6| Step: 12
Training loss: 3.002651314587191
Validation loss: 2.454195092905004

Epoch: 6| Step: 13
Training loss: 2.924947663189456
Validation loss: 2.4621499117710233

Epoch: 119| Step: 0
Training loss: 2.2025060122318023
Validation loss: 2.4643076350406683

Epoch: 6| Step: 1
Training loss: 2.3661444351983705
Validation loss: 2.463845866631806

Epoch: 6| Step: 2
Training loss: 2.3750879371826605
Validation loss: 2.4664202936526056

Epoch: 6| Step: 3
Training loss: 2.8246988861836977
Validation loss: 2.4603925878739243

Epoch: 6| Step: 4
Training loss: 2.7692466342096624
Validation loss: 2.464982638952415

Epoch: 6| Step: 5
Training loss: 2.190973738965213
Validation loss: 2.450193901563339

Epoch: 6| Step: 6
Training loss: 2.370380627452107
Validation loss: 2.457441157286223

Epoch: 6| Step: 7
Training loss: 2.6393414148411076
Validation loss: 2.4571205693007188

Epoch: 6| Step: 8
Training loss: 2.9855226237484893
Validation loss: 2.4650987592957945

Epoch: 6| Step: 9
Training loss: 2.451724290341312
Validation loss: 2.468938168464889

Epoch: 6| Step: 10
Training loss: 2.690423484282776
Validation loss: 2.472525787527607

Epoch: 6| Step: 11
Training loss: 2.5356883003545954
Validation loss: 2.4794467853257114

Epoch: 6| Step: 12
Training loss: 2.454618934533328
Validation loss: 2.476091919804811

Epoch: 6| Step: 13
Training loss: 2.6543448403847694
Validation loss: 2.4762726944113402

Epoch: 120| Step: 0
Training loss: 2.5033213487212103
Validation loss: 2.481119810351443

Epoch: 6| Step: 1
Training loss: 2.9492058153689866
Validation loss: 2.476140304182029

Epoch: 6| Step: 2
Training loss: 2.4240179610011863
Validation loss: 2.4793879359473343

Epoch: 6| Step: 3
Training loss: 2.299894877809551
Validation loss: 2.4711647778463073

Epoch: 6| Step: 4
Training loss: 2.7426435737966726
Validation loss: 2.481299185725787

Epoch: 6| Step: 5
Training loss: 2.3984432655290027
Validation loss: 2.474487381968788

Epoch: 6| Step: 6
Training loss: 2.3765668218148264
Validation loss: 2.470291412990583

Epoch: 6| Step: 7
Training loss: 2.756779896149308
Validation loss: 2.47196956538834

Epoch: 6| Step: 8
Training loss: 2.7237041542007816
Validation loss: 2.4662910319319633

Epoch: 6| Step: 9
Training loss: 2.3078455091632626
Validation loss: 2.458656844711476

Epoch: 6| Step: 10
Training loss: 2.36411642654745
Validation loss: 2.465864934234255

Epoch: 6| Step: 11
Training loss: 2.4436377497110215
Validation loss: 2.4620630344551073

Epoch: 6| Step: 12
Training loss: 2.563395925071009
Validation loss: 2.464912191932995

Epoch: 6| Step: 13
Training loss: 2.4527745087619692
Validation loss: 2.453359507631881

Epoch: 121| Step: 0
Training loss: 2.5827271970680714
Validation loss: 2.4602495150161676

Epoch: 6| Step: 1
Training loss: 3.1473595830426975
Validation loss: 2.462963840284752

Epoch: 6| Step: 2
Training loss: 2.368086943009762
Validation loss: 2.465679270925053

Epoch: 6| Step: 3
Training loss: 2.6180493202663637
Validation loss: 2.463563952570327

Epoch: 6| Step: 4
Training loss: 1.5645758000923506
Validation loss: 2.4647150988488815

Epoch: 6| Step: 5
Training loss: 2.72841898065018
Validation loss: 2.4683888972036674

Epoch: 6| Step: 6
Training loss: 2.652567655975083
Validation loss: 2.467042517594741

Epoch: 6| Step: 7
Training loss: 2.112032085716837
Validation loss: 2.465471190863504

Epoch: 6| Step: 8
Training loss: 2.7599586154765197
Validation loss: 2.47575036179388

Epoch: 6| Step: 9
Training loss: 2.6651583121050733
Validation loss: 2.467246487061122

Epoch: 6| Step: 10
Training loss: 2.631158779130612
Validation loss: 2.472145819346592

Epoch: 6| Step: 11
Training loss: 2.787400805842405
Validation loss: 2.470133027858702

Epoch: 6| Step: 12
Training loss: 2.169868035940434
Validation loss: 2.4697943944981664

Epoch: 6| Step: 13
Training loss: 2.5241724603317164
Validation loss: 2.4710322667192806

Epoch: 122| Step: 0
Training loss: 2.391940982607247
Validation loss: 2.4710960829582382

Epoch: 6| Step: 1
Training loss: 2.0472877610423192
Validation loss: 2.4673332301774304

Epoch: 6| Step: 2
Training loss: 2.5206328593424074
Validation loss: 2.471748012491125

Epoch: 6| Step: 3
Training loss: 2.365033973627257
Validation loss: 2.471096822660556

Epoch: 6| Step: 4
Training loss: 2.299112799818722
Validation loss: 2.4666883495597824

Epoch: 6| Step: 5
Training loss: 2.2051853799203487
Validation loss: 2.4617358976360317

Epoch: 6| Step: 6
Training loss: 2.99952471464963
Validation loss: 2.4646356961343447

Epoch: 6| Step: 7
Training loss: 2.497521507010203
Validation loss: 2.454367086282653

Epoch: 6| Step: 8
Training loss: 2.659663083243746
Validation loss: 2.455694250274574

Epoch: 6| Step: 9
Training loss: 2.5764054546069257
Validation loss: 2.4582664081925016

Epoch: 6| Step: 10
Training loss: 2.7579802119020296
Validation loss: 2.459108882273848

Epoch: 6| Step: 11
Training loss: 2.6466783290609293
Validation loss: 2.4626460527460194

Epoch: 6| Step: 12
Training loss: 2.7160198490895078
Validation loss: 2.45190306114017

Epoch: 6| Step: 13
Training loss: 2.568481720885882
Validation loss: 2.4554907210044026

Epoch: 123| Step: 0
Training loss: 2.4989135288697097
Validation loss: 2.4535366449992733

Epoch: 6| Step: 1
Training loss: 2.617759801437336
Validation loss: 2.4546965082700583

Epoch: 6| Step: 2
Training loss: 2.5902995863454645
Validation loss: 2.4588680547865955

Epoch: 6| Step: 3
Training loss: 3.00386243770878
Validation loss: 2.457525287522648

Epoch: 6| Step: 4
Training loss: 2.0398402575985637
Validation loss: 2.4524413044963826

Epoch: 6| Step: 5
Training loss: 2.327119546151439
Validation loss: 2.4608164278150078

Epoch: 6| Step: 6
Training loss: 2.7359871607640938
Validation loss: 2.4595746015492477

Epoch: 6| Step: 7
Training loss: 2.191628946286735
Validation loss: 2.4554410882116517

Epoch: 6| Step: 8
Training loss: 2.2367215938996394
Validation loss: 2.45373683030048

Epoch: 6| Step: 9
Training loss: 2.598748228541326
Validation loss: 2.460628203478036

Epoch: 6| Step: 10
Training loss: 2.621071373379242
Validation loss: 2.456021876334014

Epoch: 6| Step: 11
Training loss: 2.3740413136281933
Validation loss: 2.4534648003891673

Epoch: 6| Step: 12
Training loss: 2.7009875716679423
Validation loss: 2.4528820055178078

Epoch: 6| Step: 13
Training loss: 2.6434251734528145
Validation loss: 2.46284268216837

Epoch: 124| Step: 0
Training loss: 2.826501707115987
Validation loss: 2.4490434582863894

Epoch: 6| Step: 1
Training loss: 2.6287515263138066
Validation loss: 2.452577469095083

Epoch: 6| Step: 2
Training loss: 2.3831900875908554
Validation loss: 2.456868265848401

Epoch: 6| Step: 3
Training loss: 2.574628273511564
Validation loss: 2.455246123739371

Epoch: 6| Step: 4
Training loss: 2.3847799214802325
Validation loss: 2.4523570160528743

Epoch: 6| Step: 5
Training loss: 2.474399813696
Validation loss: 2.4607568745200408

Epoch: 6| Step: 6
Training loss: 2.263913474960325
Validation loss: 2.460542241401143

Epoch: 6| Step: 7
Training loss: 1.8728626148654808
Validation loss: 2.4547054601733036

Epoch: 6| Step: 8
Training loss: 2.435249365506228
Validation loss: 2.449926360314644

Epoch: 6| Step: 9
Training loss: 2.946396038982247
Validation loss: 2.4489812497508563

Epoch: 6| Step: 10
Training loss: 2.53934155617956
Validation loss: 2.4586255875053302

Epoch: 6| Step: 11
Training loss: 2.8034995280687736
Validation loss: 2.4638388590898344

Epoch: 6| Step: 12
Training loss: 2.1013197067290106
Validation loss: 2.4565432599135186

Epoch: 6| Step: 13
Training loss: 2.6017727237111177
Validation loss: 2.4603891236014963

Epoch: 125| Step: 0
Training loss: 2.4136044953562013
Validation loss: 2.458374125470879

Epoch: 6| Step: 1
Training loss: 2.413410876575637
Validation loss: 2.452692402723904

Epoch: 6| Step: 2
Training loss: 2.663675408753293
Validation loss: 2.451563781776847

Epoch: 6| Step: 3
Training loss: 2.7135875402711074
Validation loss: 2.462494663290834

Epoch: 6| Step: 4
Training loss: 2.559609434315672
Validation loss: 2.4572751521037617

Epoch: 6| Step: 5
Training loss: 2.012051630718992
Validation loss: 2.461700619851042

Epoch: 6| Step: 6
Training loss: 2.1915359322702517
Validation loss: 2.468073563112612

Epoch: 6| Step: 7
Training loss: 2.1551104865549315
Validation loss: 2.4748155547621877

Epoch: 6| Step: 8
Training loss: 3.287048593141622
Validation loss: 2.4683548655355314

Epoch: 6| Step: 9
Training loss: 3.1092262424175705
Validation loss: 2.467187250035446

Epoch: 6| Step: 10
Training loss: 2.1206162057385622
Validation loss: 2.4700231689519643

Epoch: 6| Step: 11
Training loss: 1.7156261840149176
Validation loss: 2.4726033458704872

Epoch: 6| Step: 12
Training loss: 2.663617586364984
Validation loss: 2.468355203600767

Epoch: 6| Step: 13
Training loss: 2.7842253296909933
Validation loss: 2.475408547972476

Epoch: 126| Step: 0
Training loss: 2.2489280266344527
Validation loss: 2.4638037404406012

Epoch: 6| Step: 1
Training loss: 2.7213629082108906
Validation loss: 2.4573973042893864

Epoch: 6| Step: 2
Training loss: 2.132725752299504
Validation loss: 2.4607188778604443

Epoch: 6| Step: 3
Training loss: 2.5260016579097764
Validation loss: 2.472733981019073

Epoch: 6| Step: 4
Training loss: 2.411201435903262
Validation loss: 2.4833823406154325

Epoch: 6| Step: 5
Training loss: 2.171970831863106
Validation loss: 2.487993304541064

Epoch: 6| Step: 6
Training loss: 2.0212713599270695
Validation loss: 2.4959668369494676

Epoch: 6| Step: 7
Training loss: 2.7636865406523423
Validation loss: 2.489470688148236

Epoch: 6| Step: 8
Training loss: 2.299523519834908
Validation loss: 2.4623713035324015

Epoch: 6| Step: 9
Training loss: 2.8024058292298886
Validation loss: 2.453478971888876

Epoch: 6| Step: 10
Training loss: 2.772986748829823
Validation loss: 2.4517935117854828

Epoch: 6| Step: 11
Training loss: 3.0872663896035886
Validation loss: 2.4643577422994514

Epoch: 6| Step: 12
Training loss: 2.565700114517813
Validation loss: 2.461927991679118

Epoch: 6| Step: 13
Training loss: 2.480477592873017
Validation loss: 2.4694879772848513

Epoch: 127| Step: 0
Training loss: 2.035403187244128
Validation loss: 2.474402624022462

Epoch: 6| Step: 1
Training loss: 2.630494588753914
Validation loss: 2.4790478098184145

Epoch: 6| Step: 2
Training loss: 2.6881349390196947
Validation loss: 2.478908618886196

Epoch: 6| Step: 3
Training loss: 2.122909190495747
Validation loss: 2.4811970920511057

Epoch: 6| Step: 4
Training loss: 2.6187996069823214
Validation loss: 2.4776875921647314

Epoch: 6| Step: 5
Training loss: 2.304419033125424
Validation loss: 2.482621923950373

Epoch: 6| Step: 6
Training loss: 2.129282730830958
Validation loss: 2.4806244888875404

Epoch: 6| Step: 7
Training loss: 2.1801197348094137
Validation loss: 2.479138435609078

Epoch: 6| Step: 8
Training loss: 3.307866724837491
Validation loss: 2.483610839400965

Epoch: 6| Step: 9
Training loss: 2.583509890357817
Validation loss: 2.481055835522511

Epoch: 6| Step: 10
Training loss: 2.397741359618362
Validation loss: 2.483144267350159

Epoch: 6| Step: 11
Training loss: 3.0783980969726485
Validation loss: 2.480965792300061

Epoch: 6| Step: 12
Training loss: 3.1695319480603743
Validation loss: 2.482814290594404

Epoch: 6| Step: 13
Training loss: 2.643315676822196
Validation loss: 2.4814971233845475

Epoch: 128| Step: 0
Training loss: 2.431390882788789
Validation loss: 2.4766925893926968

Epoch: 6| Step: 1
Training loss: 2.3578728953135073
Validation loss: 2.4787202694986012

Epoch: 6| Step: 2
Training loss: 2.517807102962229
Validation loss: 2.4741392263567743

Epoch: 6| Step: 3
Training loss: 1.8441288445695803
Validation loss: 2.473360448501855

Epoch: 6| Step: 4
Training loss: 2.422229371755394
Validation loss: 2.4748165582819683

Epoch: 6| Step: 5
Training loss: 2.894083953768017
Validation loss: 2.4710983342254726

Epoch: 6| Step: 6
Training loss: 3.06157561383968
Validation loss: 2.4697428771023517

Epoch: 6| Step: 7
Training loss: 2.274750739894352
Validation loss: 2.4642874225402553

Epoch: 6| Step: 8
Training loss: 2.663871780683545
Validation loss: 2.46424177247775

Epoch: 6| Step: 9
Training loss: 2.9920572994579744
Validation loss: 2.4596558154855437

Epoch: 6| Step: 10
Training loss: 2.170177976807268
Validation loss: 2.4579068207229677

Epoch: 6| Step: 11
Training loss: 2.974082093522509
Validation loss: 2.4542058924296515

Epoch: 6| Step: 12
Training loss: 2.2361517824169153
Validation loss: 2.449210605475975

Epoch: 6| Step: 13
Training loss: 2.384308292360705
Validation loss: 2.456576824365649

Epoch: 129| Step: 0
Training loss: 2.174867832070514
Validation loss: 2.461832116011682

Epoch: 6| Step: 1
Training loss: 2.589754543235278
Validation loss: 2.4655461990534575

Epoch: 6| Step: 2
Training loss: 2.116667285929111
Validation loss: 2.474440812027965

Epoch: 6| Step: 3
Training loss: 2.00467397992993
Validation loss: 2.47951662708371

Epoch: 6| Step: 4
Training loss: 2.6598518634361583
Validation loss: 2.4846224151826646

Epoch: 6| Step: 5
Training loss: 2.8805882420080655
Validation loss: 2.4922557729778636

Epoch: 6| Step: 6
Training loss: 2.6498357182089785
Validation loss: 2.4804014341977187

Epoch: 6| Step: 7
Training loss: 2.834199230796274
Validation loss: 2.482620051267303

Epoch: 6| Step: 8
Training loss: 2.4651195051461703
Validation loss: 2.471759024695312

Epoch: 6| Step: 9
Training loss: 2.8171771683506126
Validation loss: 2.4731686318852852

Epoch: 6| Step: 10
Training loss: 2.3083790325869704
Validation loss: 2.458733499145475

Epoch: 6| Step: 11
Training loss: 2.6349669564073555
Validation loss: 2.458928381030094

Epoch: 6| Step: 12
Training loss: 2.2134401425285675
Validation loss: 2.46652108183808

Epoch: 6| Step: 13
Training loss: 2.979600852502626
Validation loss: 2.4668797841237917

Epoch: 130| Step: 0
Training loss: 3.1158268380095273
Validation loss: 2.4654155215136657

Epoch: 6| Step: 1
Training loss: 2.373790031204035
Validation loss: 2.4697309388141666

Epoch: 6| Step: 2
Training loss: 2.112050486033466
Validation loss: 2.466393033736268

Epoch: 6| Step: 3
Training loss: 2.6044309456558254
Validation loss: 2.4684509224950704

Epoch: 6| Step: 4
Training loss: 2.5655828400468903
Validation loss: 2.4661896865205875

Epoch: 6| Step: 5
Training loss: 2.8544495899468694
Validation loss: 2.464785438649889

Epoch: 6| Step: 6
Training loss: 2.9644469547245214
Validation loss: 2.4669696488613138

Epoch: 6| Step: 7
Training loss: 2.3819158148890884
Validation loss: 2.4608864753721567

Epoch: 6| Step: 8
Training loss: 2.7493588827310718
Validation loss: 2.465170168132443

Epoch: 6| Step: 9
Training loss: 2.5363115167654198
Validation loss: 2.461980866964964

Epoch: 6| Step: 10
Training loss: 1.806423986694486
Validation loss: 2.459371466111032

Epoch: 6| Step: 11
Training loss: 2.647379616079742
Validation loss: 2.4570100315392764

Epoch: 6| Step: 12
Training loss: 2.429775812929792
Validation loss: 2.4531246112410123

Epoch: 6| Step: 13
Training loss: 2.1418357821932337
Validation loss: 2.450594501962287

Epoch: 131| Step: 0
Training loss: 2.8282163826525886
Validation loss: 2.453595628564193

Epoch: 6| Step: 1
Training loss: 2.880423364780944
Validation loss: 2.4468603141022407

Epoch: 6| Step: 2
Training loss: 3.022391993609881
Validation loss: 2.4510026161551344

Epoch: 6| Step: 3
Training loss: 2.2547971660461537
Validation loss: 2.4527446265648614

Epoch: 6| Step: 4
Training loss: 2.652790374681568
Validation loss: 2.4505834676188374

Epoch: 6| Step: 5
Training loss: 2.287804684385287
Validation loss: 2.4590137693038514

Epoch: 6| Step: 6
Training loss: 2.670398147843383
Validation loss: 2.457896069793185

Epoch: 6| Step: 7
Training loss: 2.1497840684200167
Validation loss: 2.462906129784703

Epoch: 6| Step: 8
Training loss: 2.693209504957153
Validation loss: 2.453281600068077

Epoch: 6| Step: 9
Training loss: 2.0109164814121057
Validation loss: 2.4521380988535704

Epoch: 6| Step: 10
Training loss: 2.7669745132270633
Validation loss: 2.4582691884697114

Epoch: 6| Step: 11
Training loss: 1.9162587547226677
Validation loss: 2.4532968902299905

Epoch: 6| Step: 12
Training loss: 2.401910212405333
Validation loss: 2.4502120166309003

Epoch: 6| Step: 13
Training loss: 2.510894212901565
Validation loss: 2.4520152309125147

Epoch: 132| Step: 0
Training loss: 2.7062039342752273
Validation loss: 2.4535106347724702

Epoch: 6| Step: 1
Training loss: 2.3872249584652336
Validation loss: 2.4560992277946503

Epoch: 6| Step: 2
Training loss: 2.5577496483036053
Validation loss: 2.45515498827757

Epoch: 6| Step: 3
Training loss: 2.221971027429165
Validation loss: 2.457322031189001

Epoch: 6| Step: 4
Training loss: 1.8928692950966535
Validation loss: 2.454806875025132

Epoch: 6| Step: 5
Training loss: 2.2085469940277704
Validation loss: 2.4500990101536857

Epoch: 6| Step: 6
Training loss: 2.2538082954369845
Validation loss: 2.4505222305688443

Epoch: 6| Step: 7
Training loss: 2.955543294879592
Validation loss: 2.444888987241027

Epoch: 6| Step: 8
Training loss: 2.828523892255043
Validation loss: 2.4516727984326008

Epoch: 6| Step: 9
Training loss: 2.9684298694069566
Validation loss: 2.4534378176274196

Epoch: 6| Step: 10
Training loss: 2.1064115830198236
Validation loss: 2.446508438111392

Epoch: 6| Step: 11
Training loss: 2.119674741877286
Validation loss: 2.4530727599589186

Epoch: 6| Step: 12
Training loss: 3.0678343733502906
Validation loss: 2.4498554722332577

Epoch: 6| Step: 13
Training loss: 2.6503898153957404
Validation loss: 2.454411147223561

Epoch: 133| Step: 0
Training loss: 2.630359808963286
Validation loss: 2.457093505394123

Epoch: 6| Step: 1
Training loss: 2.374187129601874
Validation loss: 2.45417593051304

Epoch: 6| Step: 2
Training loss: 2.4370229205281273
Validation loss: 2.45315822246757

Epoch: 6| Step: 3
Training loss: 2.78788711186311
Validation loss: 2.450284978290945

Epoch: 6| Step: 4
Training loss: 3.019156646192451
Validation loss: 2.4532181222010037

Epoch: 6| Step: 5
Training loss: 2.5847805235492305
Validation loss: 2.4418398052537116

Epoch: 6| Step: 6
Training loss: 2.8663312168651043
Validation loss: 2.453110235214346

Epoch: 6| Step: 7
Training loss: 2.52788763075384
Validation loss: 2.4548069721482815

Epoch: 6| Step: 8
Training loss: 2.133709058854875
Validation loss: 2.4568259067979104

Epoch: 6| Step: 9
Training loss: 2.4890213229598257
Validation loss: 2.465572984837353

Epoch: 6| Step: 10
Training loss: 2.5193648882213235
Validation loss: 2.46478242389785

Epoch: 6| Step: 11
Training loss: 2.4752760954709303
Validation loss: 2.461117385667773

Epoch: 6| Step: 12
Training loss: 2.0608902631527504
Validation loss: 2.466137787679232

Epoch: 6| Step: 13
Training loss: 2.0621950328658682
Validation loss: 2.4549503621938

Epoch: 134| Step: 0
Training loss: 2.757729418316311
Validation loss: 2.455680439459876

Epoch: 6| Step: 1
Training loss: 2.5334448070427804
Validation loss: 2.4611633277900213

Epoch: 6| Step: 2
Training loss: 2.751444870504813
Validation loss: 2.46509379445101

Epoch: 6| Step: 3
Training loss: 2.4140431424941937
Validation loss: 2.458495576537102

Epoch: 6| Step: 4
Training loss: 2.3936992151834313
Validation loss: 2.4641017210005325

Epoch: 6| Step: 5
Training loss: 2.748291698793582
Validation loss: 2.459317985368203

Epoch: 6| Step: 6
Training loss: 2.1915359322702517
Validation loss: 2.458655988133563

Epoch: 6| Step: 7
Training loss: 2.412310115476698
Validation loss: 2.4647704615833628

Epoch: 6| Step: 8
Training loss: 2.301156134900442
Validation loss: 2.457622608529595

Epoch: 6| Step: 9
Training loss: 2.4323178493443773
Validation loss: 2.462735039326561

Epoch: 6| Step: 10
Training loss: 2.7170901438473622
Validation loss: 2.465030975405101

Epoch: 6| Step: 11
Training loss: 2.4134900053112514
Validation loss: 2.4524666617561492

Epoch: 6| Step: 12
Training loss: 2.65892181659138
Validation loss: 2.463374768679937

Epoch: 6| Step: 13
Training loss: 2.234417468114221
Validation loss: 2.4575283920236477

Epoch: 135| Step: 0
Training loss: 2.439657479149495
Validation loss: 2.4663327934081347

Epoch: 6| Step: 1
Training loss: 3.182285606432616
Validation loss: 2.466583170207925

Epoch: 6| Step: 2
Training loss: 2.776089112602187
Validation loss: 2.449784038717927

Epoch: 6| Step: 3
Training loss: 2.1476475598550424
Validation loss: 2.4587931822524283

Epoch: 6| Step: 4
Training loss: 2.424821402668425
Validation loss: 2.4545609873597014

Epoch: 6| Step: 5
Training loss: 2.4692526197026594
Validation loss: 2.466055514257087

Epoch: 6| Step: 6
Training loss: 2.55883359294509
Validation loss: 2.466422130302802

Epoch: 6| Step: 7
Training loss: 2.4579466068375466
Validation loss: 2.465411129480483

Epoch: 6| Step: 8
Training loss: 2.2537784639360527
Validation loss: 2.4649987028478337

Epoch: 6| Step: 9
Training loss: 2.830999610051944
Validation loss: 2.4673773172523132

Epoch: 6| Step: 10
Training loss: 1.7916123655503566
Validation loss: 2.468947551565234

Epoch: 6| Step: 11
Training loss: 2.6637238318000627
Validation loss: 2.462761484582672

Epoch: 6| Step: 12
Training loss: 2.710100731895452
Validation loss: 2.4669909427323478

Epoch: 6| Step: 13
Training loss: 2.221734524563621
Validation loss: 2.457831983575959

Epoch: 136| Step: 0
Training loss: 2.415796638500453
Validation loss: 2.4589105079389224

Epoch: 6| Step: 1
Training loss: 2.0932771518886697
Validation loss: 2.4617559616002733

Epoch: 6| Step: 2
Training loss: 1.989126569101135
Validation loss: 2.456854849802041

Epoch: 6| Step: 3
Training loss: 2.869832411867388
Validation loss: 2.463755081535375

Epoch: 6| Step: 4
Training loss: 2.8610871405559273
Validation loss: 2.461594484825328

Epoch: 6| Step: 5
Training loss: 3.298039669041192
Validation loss: 2.4670325312908186

Epoch: 6| Step: 6
Training loss: 2.5491292139872535
Validation loss: 2.464779884732176

Epoch: 6| Step: 7
Training loss: 2.1248705207422485
Validation loss: 2.4504127492118215

Epoch: 6| Step: 8
Training loss: 2.4547677340831857
Validation loss: 2.4597312594888607

Epoch: 6| Step: 9
Training loss: 2.438849344659494
Validation loss: 2.461815845830873

Epoch: 6| Step: 10
Training loss: 2.3921635482655423
Validation loss: 2.458413459528073

Epoch: 6| Step: 11
Training loss: 2.53083260944931
Validation loss: 2.4557724453674665

Epoch: 6| Step: 12
Training loss: 2.3433295317827465
Validation loss: 2.4571144401242004

Epoch: 6| Step: 13
Training loss: 2.289985861089111
Validation loss: 2.4693528899890436

Epoch: 137| Step: 0
Training loss: 2.4316135634789355
Validation loss: 2.4641355372674973

Epoch: 6| Step: 1
Training loss: 2.610049497493106
Validation loss: 2.45948512902378

Epoch: 6| Step: 2
Training loss: 2.363650055158748
Validation loss: 2.464795659760374

Epoch: 6| Step: 3
Training loss: 2.5099692889154697
Validation loss: 2.464422723003471

Epoch: 6| Step: 4
Training loss: 2.103444217755601
Validation loss: 2.4688332781044755

Epoch: 6| Step: 5
Training loss: 3.000938427698639
Validation loss: 2.4628625677065035

Epoch: 6| Step: 6
Training loss: 1.9959587753258565
Validation loss: 2.464655607512205

Epoch: 6| Step: 7
Training loss: 2.686064225736286
Validation loss: 2.464362273267635

Epoch: 6| Step: 8
Training loss: 2.1985096304853693
Validation loss: 2.4601638304423097

Epoch: 6| Step: 9
Training loss: 2.912310280576079
Validation loss: 2.468072701752805

Epoch: 6| Step: 10
Training loss: 2.301255700199127
Validation loss: 2.4603584617515386

Epoch: 6| Step: 11
Training loss: 2.5538448145476638
Validation loss: 2.461027226502707

Epoch: 6| Step: 12
Training loss: 2.507088815248707
Validation loss: 2.4596869788243825

Epoch: 6| Step: 13
Training loss: 2.5006650040221916
Validation loss: 2.47148130925246

Epoch: 138| Step: 0
Training loss: 2.6893197929004886
Validation loss: 2.453784781008844

Epoch: 6| Step: 1
Training loss: 2.209193643899035
Validation loss: 2.467227168345111

Epoch: 6| Step: 2
Training loss: 2.407968378188897
Validation loss: 2.465492223686746

Epoch: 6| Step: 3
Training loss: 2.263459110875751
Validation loss: 2.466199821249135

Epoch: 6| Step: 4
Training loss: 2.6495481429742838
Validation loss: 2.4648443464871916

Epoch: 6| Step: 5
Training loss: 2.9489747609649197
Validation loss: 2.464682467464241

Epoch: 6| Step: 6
Training loss: 2.4162308420525926
Validation loss: 2.4706423785917857

Epoch: 6| Step: 7
Training loss: 2.5537002942536335
Validation loss: 2.463579751514046

Epoch: 6| Step: 8
Training loss: 2.597888173240496
Validation loss: 2.4752912979299935

Epoch: 6| Step: 9
Training loss: 1.9120535759582142
Validation loss: 2.4691765654107245

Epoch: 6| Step: 10
Training loss: 2.477038605470396
Validation loss: 2.461939790261658

Epoch: 6| Step: 11
Training loss: 2.185859937293858
Validation loss: 2.464169062807967

Epoch: 6| Step: 12
Training loss: 2.589473461992567
Validation loss: 2.4610973568621204

Epoch: 6| Step: 13
Training loss: 2.8415975284462567
Validation loss: 2.4664107559383663

Epoch: 139| Step: 0
Training loss: 2.599050414467907
Validation loss: 2.464540264390716

Epoch: 6| Step: 1
Training loss: 2.7080511288457743
Validation loss: 2.462991606023406

Epoch: 6| Step: 2
Training loss: 2.16769071978468
Validation loss: 2.4572689909708743

Epoch: 6| Step: 3
Training loss: 2.8086870414084815
Validation loss: 2.4652603208493913

Epoch: 6| Step: 4
Training loss: 2.4049405585098813
Validation loss: 2.4623933875537354

Epoch: 6| Step: 5
Training loss: 2.152243610136419
Validation loss: 2.460430969101963

Epoch: 6| Step: 6
Training loss: 2.5490723474650703
Validation loss: 2.4661157774713387

Epoch: 6| Step: 7
Training loss: 2.275376479469174
Validation loss: 2.463186675310019

Epoch: 6| Step: 8
Training loss: 2.9444964262334508
Validation loss: 2.466194697496505

Epoch: 6| Step: 9
Training loss: 2.2950392672767683
Validation loss: 2.462015874427751

Epoch: 6| Step: 10
Training loss: 2.2576758531448964
Validation loss: 2.4573177783112903

Epoch: 6| Step: 11
Training loss: 2.8051385118417373
Validation loss: 2.464258744208716

Epoch: 6| Step: 12
Training loss: 2.4728946902276894
Validation loss: 2.463976312943029

Epoch: 6| Step: 13
Training loss: 2.385970327654865
Validation loss: 2.462775554182213

Epoch: 140| Step: 0
Training loss: 2.525906515274758
Validation loss: 2.4674859009623042

Epoch: 6| Step: 1
Training loss: 2.4163852615505603
Validation loss: 2.4714409049556143

Epoch: 6| Step: 2
Training loss: 2.0584404035923902
Validation loss: 2.4624252182115334

Epoch: 6| Step: 3
Training loss: 2.25007576285069
Validation loss: 2.459637543795332

Epoch: 6| Step: 4
Training loss: 2.2890860390347942
Validation loss: 2.462823248235809

Epoch: 6| Step: 5
Training loss: 2.749793478406908
Validation loss: 2.4550717479427755

Epoch: 6| Step: 6
Training loss: 2.4768678494970358
Validation loss: 2.453077651941759

Epoch: 6| Step: 7
Training loss: 3.0593066012068793
Validation loss: 2.459224609404214

Epoch: 6| Step: 8
Training loss: 2.079730109531333
Validation loss: 2.4721424760231794

Epoch: 6| Step: 9
Training loss: 2.7537684196448335
Validation loss: 2.4669941480722075

Epoch: 6| Step: 10
Training loss: 2.912280972476125
Validation loss: 2.4688705382717613

Epoch: 6| Step: 11
Training loss: 1.8557130030433873
Validation loss: 2.4775649487771125

Epoch: 6| Step: 12
Training loss: 2.6310480470696027
Validation loss: 2.4952227884280362

Epoch: 6| Step: 13
Training loss: 2.6095032803189033
Validation loss: 2.547863961332542

Epoch: 141| Step: 0
Training loss: 2.3277458547842658
Validation loss: 2.5202190081478117

Epoch: 6| Step: 1
Training loss: 2.4730851944849994
Validation loss: 2.4743955660822317

Epoch: 6| Step: 2
Training loss: 2.1008269997662294
Validation loss: 2.4598188734053656

Epoch: 6| Step: 3
Training loss: 2.8826874496505943
Validation loss: 2.4556030909893565

Epoch: 6| Step: 4
Training loss: 2.6893077359485678
Validation loss: 2.4677079492334917

Epoch: 6| Step: 5
Training loss: 2.5065126466947603
Validation loss: 2.4697975479328393

Epoch: 6| Step: 6
Training loss: 2.376647177246708
Validation loss: 2.476176138502748

Epoch: 6| Step: 7
Training loss: 2.4215256962063596
Validation loss: 2.477124364035581

Epoch: 6| Step: 8
Training loss: 2.416327540401181
Validation loss: 2.4732748807527507

Epoch: 6| Step: 9
Training loss: 2.806284500283819
Validation loss: 2.4827384116640143

Epoch: 6| Step: 10
Training loss: 2.5678457928760463
Validation loss: 2.488579612901431

Epoch: 6| Step: 11
Training loss: 2.211275539908407
Validation loss: 2.483353618777926

Epoch: 6| Step: 12
Training loss: 2.7359489924321823
Validation loss: 2.484871516804694

Epoch: 6| Step: 13
Training loss: 3.037157731729181
Validation loss: 2.479084435603171

Epoch: 142| Step: 0
Training loss: 2.451989366604772
Validation loss: 2.4856102789402232

Epoch: 6| Step: 1
Training loss: 2.271045383270342
Validation loss: 2.4842734965895645

Epoch: 6| Step: 2
Training loss: 2.021607974784166
Validation loss: 2.479817991422779

Epoch: 6| Step: 3
Training loss: 2.662740519230874
Validation loss: 2.4725983960945093

Epoch: 6| Step: 4
Training loss: 2.3763104888906703
Validation loss: 2.4781973143164984

Epoch: 6| Step: 5
Training loss: 3.011251014972166
Validation loss: 2.4776922110202992

Epoch: 6| Step: 6
Training loss: 2.395726364622627
Validation loss: 2.4782653954828557

Epoch: 6| Step: 7
Training loss: 2.6893793676342206
Validation loss: 2.4765932420172345

Epoch: 6| Step: 8
Training loss: 2.4861503351717706
Validation loss: 2.4733342450989366

Epoch: 6| Step: 9
Training loss: 2.8644733939604032
Validation loss: 2.471775141007889

Epoch: 6| Step: 10
Training loss: 2.7752591879833206
Validation loss: 2.4655433786320824

Epoch: 6| Step: 11
Training loss: 2.446471503301701
Validation loss: 2.4664911567477388

Epoch: 6| Step: 12
Training loss: 1.942099495426091
Validation loss: 2.4624607115320702

Epoch: 6| Step: 13
Training loss: 2.662859065957372
Validation loss: 2.459351285749834

Epoch: 143| Step: 0
Training loss: 2.869369632730053
Validation loss: 2.4782731078098132

Epoch: 6| Step: 1
Training loss: 2.944808309692663
Validation loss: 2.478552210552559

Epoch: 6| Step: 2
Training loss: 2.663155141112426
Validation loss: 2.475001006334916

Epoch: 6| Step: 3
Training loss: 2.3750989792928605
Validation loss: 2.4737301262790825

Epoch: 6| Step: 4
Training loss: 2.3855641970482635
Validation loss: 2.475938062435473

Epoch: 6| Step: 5
Training loss: 2.489462387999105
Validation loss: 2.474157053679407

Epoch: 6| Step: 6
Training loss: 2.301408717759981
Validation loss: 2.4709420431666294

Epoch: 6| Step: 7
Training loss: 2.1284994594151225
Validation loss: 2.4727496329899195

Epoch: 6| Step: 8
Training loss: 2.482869297767924
Validation loss: 2.467110777731638

Epoch: 6| Step: 9
Training loss: 1.7526388027931
Validation loss: 2.4690533061359887

Epoch: 6| Step: 10
Training loss: 2.5604421214449054
Validation loss: 2.4664952246480984

Epoch: 6| Step: 11
Training loss: 2.563973073196063
Validation loss: 2.4641523726608514

Epoch: 6| Step: 12
Training loss: 2.638253857236291
Validation loss: 2.4584818218666764

Epoch: 6| Step: 13
Training loss: 2.6550591884991177
Validation loss: 2.4683353220665247

Epoch: 144| Step: 0
Training loss: 2.9899486004037996
Validation loss: 2.4659080726441376

Epoch: 6| Step: 1
Training loss: 2.118350677219144
Validation loss: 2.4673718980068515

Epoch: 6| Step: 2
Training loss: 2.607316352909242
Validation loss: 2.459314592290905

Epoch: 6| Step: 3
Training loss: 2.813319616630827
Validation loss: 2.4677792825693787

Epoch: 6| Step: 4
Training loss: 2.269409596634632
Validation loss: 2.4672749051892024

Epoch: 6| Step: 5
Training loss: 2.2998347513295063
Validation loss: 2.4656618174361307

Epoch: 6| Step: 6
Training loss: 3.016763264776195
Validation loss: 2.4664482460317276

Epoch: 6| Step: 7
Training loss: 2.600006646367895
Validation loss: 2.472767614957335

Epoch: 6| Step: 8
Training loss: 2.74873409611853
Validation loss: 2.467961294000514

Epoch: 6| Step: 9
Training loss: 2.2480566321193556
Validation loss: 2.464477253875018

Epoch: 6| Step: 10
Training loss: 1.6439614656716877
Validation loss: 2.4665835810105645

Epoch: 6| Step: 11
Training loss: 2.2572886773409295
Validation loss: 2.4666326915703722

Epoch: 6| Step: 12
Training loss: 2.4625014406771246
Validation loss: 2.471459595843563

Epoch: 6| Step: 13
Training loss: 2.436339468881419
Validation loss: 2.470509960077021

Epoch: 145| Step: 0
Training loss: 2.9377902273099306
Validation loss: 2.4682860763459797

Epoch: 6| Step: 1
Training loss: 2.5664876289180993
Validation loss: 2.4777060514977522

Epoch: 6| Step: 2
Training loss: 2.298833103223157
Validation loss: 2.4727790725681

Epoch: 6| Step: 3
Training loss: 2.1051378432421695
Validation loss: 2.4715010529344235

Epoch: 6| Step: 4
Training loss: 2.1653946174623036
Validation loss: 2.4705923264177536

Epoch: 6| Step: 5
Training loss: 2.7902385038206186
Validation loss: 2.469523023144926

Epoch: 6| Step: 6
Training loss: 2.6064053386254193
Validation loss: 2.460649003168644

Epoch: 6| Step: 7
Training loss: 2.23347914015758
Validation loss: 2.465014847239362

Epoch: 6| Step: 8
Training loss: 2.2998778766883827
Validation loss: 2.465179742884644

Epoch: 6| Step: 9
Training loss: 3.0493632793259904
Validation loss: 2.4608153136261355

Epoch: 6| Step: 10
Training loss: 2.505401783623056
Validation loss: 2.4606918616040665

Epoch: 6| Step: 11
Training loss: 2.2986513910929247
Validation loss: 2.4554973073478656

Epoch: 6| Step: 12
Training loss: 1.94361668709129
Validation loss: 2.459292763381685

Epoch: 6| Step: 13
Training loss: 2.7671102210378815
Validation loss: 2.4578818914274194

Epoch: 146| Step: 0
Training loss: 2.4785342865850843
Validation loss: 2.453869247117054

Epoch: 6| Step: 1
Training loss: 2.6000157245747357
Validation loss: 2.4556193052429567

Epoch: 6| Step: 2
Training loss: 2.4526279213117403
Validation loss: 2.4528404605363883

Epoch: 6| Step: 3
Training loss: 2.122110758789654
Validation loss: 2.4525689873825747

Epoch: 6| Step: 4
Training loss: 2.3949532813690095
Validation loss: 2.451409422368501

Epoch: 6| Step: 5
Training loss: 2.4795346881819302
Validation loss: 2.467072476263971

Epoch: 6| Step: 6
Training loss: 2.7766102011237326
Validation loss: 2.4615773978691955

Epoch: 6| Step: 7
Training loss: 2.7132634015955746
Validation loss: 2.458435692245642

Epoch: 6| Step: 8
Training loss: 2.0309215573555144
Validation loss: 2.4592573455238598

Epoch: 6| Step: 9
Training loss: 2.9265337843333925
Validation loss: 2.456323413544474

Epoch: 6| Step: 10
Training loss: 2.7646053180141448
Validation loss: 2.466657467988192

Epoch: 6| Step: 11
Training loss: 2.6961611973371262
Validation loss: 2.4621147448225824

Epoch: 6| Step: 12
Training loss: 1.8584583491311615
Validation loss: 2.45948141304857

Epoch: 6| Step: 13
Training loss: 2.261641057974489
Validation loss: 2.4713855951774213

Epoch: 147| Step: 0
Training loss: 2.142658353849157
Validation loss: 2.462535706533915

Epoch: 6| Step: 1
Training loss: 2.7058943305518772
Validation loss: 2.465406487616725

Epoch: 6| Step: 2
Training loss: 1.7479623784789415
Validation loss: 2.474892535589921

Epoch: 6| Step: 3
Training loss: 2.725138295015361
Validation loss: 2.469714962012058

Epoch: 6| Step: 4
Training loss: 2.1622539358218993
Validation loss: 2.470019259693049

Epoch: 6| Step: 5
Training loss: 2.673706319415599
Validation loss: 2.480651592414216

Epoch: 6| Step: 6
Training loss: 1.6053659076654476
Validation loss: 2.472665120812484

Epoch: 6| Step: 7
Training loss: 2.743711390473471
Validation loss: 2.4716207014179696

Epoch: 6| Step: 8
Training loss: 3.092056273171619
Validation loss: 2.4666082048791114

Epoch: 6| Step: 9
Training loss: 2.838535768461953
Validation loss: 2.456597690709537

Epoch: 6| Step: 10
Training loss: 2.6238780575888416
Validation loss: 2.46672064028362

Epoch: 6| Step: 11
Training loss: 2.515250705334809
Validation loss: 2.476957720840651

Epoch: 6| Step: 12
Training loss: 1.975136163098056
Validation loss: 2.4689245202552734

Epoch: 6| Step: 13
Training loss: 2.724422544137588
Validation loss: 2.4740202618259812

Epoch: 148| Step: 0
Training loss: 2.127208683854964
Validation loss: 2.4740391982398102

Epoch: 6| Step: 1
Training loss: 2.443115319764445
Validation loss: 2.471564728449734

Epoch: 6| Step: 2
Training loss: 2.253585078530819
Validation loss: 2.471026356980412

Epoch: 6| Step: 3
Training loss: 2.7453505485733514
Validation loss: 2.471658643022282

Epoch: 6| Step: 4
Training loss: 2.534127001499531
Validation loss: 2.4692498356998467

Epoch: 6| Step: 5
Training loss: 2.170374180224193
Validation loss: 2.4722022211739723

Epoch: 6| Step: 6
Training loss: 2.793766105315278
Validation loss: 2.4667529386391602

Epoch: 6| Step: 7
Training loss: 2.6964268246792105
Validation loss: 2.4681098607026346

Epoch: 6| Step: 8
Training loss: 2.404685168051379
Validation loss: 2.4674587011311293

Epoch: 6| Step: 9
Training loss: 2.7997335818109326
Validation loss: 2.46567745789525

Epoch: 6| Step: 10
Training loss: 1.7540636565467804
Validation loss: 2.4627102799199885

Epoch: 6| Step: 11
Training loss: 2.622010117733171
Validation loss: 2.460434457540724

Epoch: 6| Step: 12
Training loss: 2.512015270332525
Validation loss: 2.466065577027235

Epoch: 6| Step: 13
Training loss: 2.6561715507142476
Validation loss: 2.4674089467268443

Epoch: 149| Step: 0
Training loss: 2.733076037725757
Validation loss: 2.4703139811605266

Epoch: 6| Step: 1
Training loss: 2.703872224582082
Validation loss: 2.4588372689245395

Epoch: 6| Step: 2
Training loss: 2.3563553642372326
Validation loss: 2.473120494649153

Epoch: 6| Step: 3
Training loss: 1.9510753405330188
Validation loss: 2.4641867526279304

Epoch: 6| Step: 4
Training loss: 1.6513396113623762
Validation loss: 2.469350878508313

Epoch: 6| Step: 5
Training loss: 1.756495614033041
Validation loss: 2.4593034597329884

Epoch: 6| Step: 6
Training loss: 2.2408485801161997
Validation loss: 2.475945356713564

Epoch: 6| Step: 7
Training loss: 2.573472049298241
Validation loss: 2.469902485828633

Epoch: 6| Step: 8
Training loss: 2.759586185310046
Validation loss: 2.4604553880694304

Epoch: 6| Step: 9
Training loss: 2.4492499974643964
Validation loss: 2.4615799403382947

Epoch: 6| Step: 10
Training loss: 2.6033608575880867
Validation loss: 2.4648609513429682

Epoch: 6| Step: 11
Training loss: 3.2038170043749337
Validation loss: 2.467151800553275

Epoch: 6| Step: 12
Training loss: 2.805224353977318
Validation loss: 2.47685937877025

Epoch: 6| Step: 13
Training loss: 2.416833367572396
Validation loss: 2.4741703678753124

Epoch: 150| Step: 0
Training loss: 2.1495227638915093
Validation loss: 2.477898735630228

Epoch: 6| Step: 1
Training loss: 2.9489851094850166
Validation loss: 2.4785627275965685

Epoch: 6| Step: 2
Training loss: 2.835320355650317
Validation loss: 2.475241323708802

Epoch: 6| Step: 3
Training loss: 2.6752416804577943
Validation loss: 2.473654514864619

Epoch: 6| Step: 4
Training loss: 2.2022798689298666
Validation loss: 2.4694916942935885

Epoch: 6| Step: 5
Training loss: 2.30303264624265
Validation loss: 2.4748866109851333

Epoch: 6| Step: 6
Training loss: 3.086025420276425
Validation loss: 2.466163245800599

Epoch: 6| Step: 7
Training loss: 1.6516465323587801
Validation loss: 2.472901358767283

Epoch: 6| Step: 8
Training loss: 2.772944274887189
Validation loss: 2.4688966602913114

Epoch: 6| Step: 9
Training loss: 2.1586138166225948
Validation loss: 2.4665017413299393

Epoch: 6| Step: 10
Training loss: 2.369761461896852
Validation loss: 2.469446904770386

Epoch: 6| Step: 11
Training loss: 2.3234620986385415
Validation loss: 2.4696762504194316

Epoch: 6| Step: 12
Training loss: 2.5501863448926154
Validation loss: 2.4711663938898547

Epoch: 6| Step: 13
Training loss: 2.1424968916203886
Validation loss: 2.4724795663553043

Epoch: 151| Step: 0
Training loss: 2.550615056978273
Validation loss: 2.462254417101744

Epoch: 6| Step: 1
Training loss: 1.922868177381874
Validation loss: 2.471061139811684

Epoch: 6| Step: 2
Training loss: 2.7052541296814328
Validation loss: 2.4705954064571

Epoch: 6| Step: 3
Training loss: 2.0671327752375817
Validation loss: 2.476422439288968

Epoch: 6| Step: 4
Training loss: 2.957026089823767
Validation loss: 2.4717468389248656

Epoch: 6| Step: 5
Training loss: 2.128062341694707
Validation loss: 2.4725299660332807

Epoch: 6| Step: 6
Training loss: 2.7551597393824845
Validation loss: 2.478333811483057

Epoch: 6| Step: 7
Training loss: 2.7225647395119443
Validation loss: 2.4688226713276813

Epoch: 6| Step: 8
Training loss: 2.1054556143780876
Validation loss: 2.4828980892242005

Epoch: 6| Step: 9
Training loss: 2.1793477330253817
Validation loss: 2.4930528435888624

Epoch: 6| Step: 10
Training loss: 2.5595542910124287
Validation loss: 2.4945645849507687

Epoch: 6| Step: 11
Training loss: 2.7807054307982884
Validation loss: 2.485722014692345

Epoch: 6| Step: 12
Training loss: 2.458280939955508
Validation loss: 2.4709782905793083

Epoch: 6| Step: 13
Training loss: 2.9613518336939237
Validation loss: 2.475769068297871

Epoch: 152| Step: 0
Training loss: 2.4291081857157737
Validation loss: 2.4717138180787117

Epoch: 6| Step: 1
Training loss: 2.522393450570791
Validation loss: 2.4671666665271665

Epoch: 6| Step: 2
Training loss: 2.400505044567526
Validation loss: 2.463404796150496

Epoch: 6| Step: 3
Training loss: 2.22054637130905
Validation loss: 2.4705368528846483

Epoch: 6| Step: 4
Training loss: 2.771313178788048
Validation loss: 2.4712948699901425

Epoch: 6| Step: 5
Training loss: 2.265502400205238
Validation loss: 2.465383004165084

Epoch: 6| Step: 6
Training loss: 2.6889057475035303
Validation loss: 2.4677491875601656

Epoch: 6| Step: 7
Training loss: 2.502337412092202
Validation loss: 2.4743749702712483

Epoch: 6| Step: 8
Training loss: 2.8563568362365945
Validation loss: 2.4648144977685718

Epoch: 6| Step: 9
Training loss: 2.060064322569215
Validation loss: 2.466778253488536

Epoch: 6| Step: 10
Training loss: 2.650116784743329
Validation loss: 2.4745869102351548

Epoch: 6| Step: 11
Training loss: 1.4468135153310295
Validation loss: 2.4698999519266076

Epoch: 6| Step: 12
Training loss: 2.7249871979858935
Validation loss: 2.471904919712919

Epoch: 6| Step: 13
Training loss: 2.8657735305614778
Validation loss: 2.46525261616888

Epoch: 153| Step: 0
Training loss: 1.961186731313611
Validation loss: 2.463560516952002

Epoch: 6| Step: 1
Training loss: 2.555907165369721
Validation loss: 2.461688674844735

Epoch: 6| Step: 2
Training loss: 2.4836094474476402
Validation loss: 2.471445045099971

Epoch: 6| Step: 3
Training loss: 2.7560960653141673
Validation loss: 2.4682148542523836

Epoch: 6| Step: 4
Training loss: 2.4806188823291007
Validation loss: 2.4702992386836975

Epoch: 6| Step: 5
Training loss: 2.2288814849493894
Validation loss: 2.4695700237219516

Epoch: 6| Step: 6
Training loss: 2.556079916670305
Validation loss: 2.47594597459972

Epoch: 6| Step: 7
Training loss: 2.9057188471594757
Validation loss: 2.4733664088938068

Epoch: 6| Step: 8
Training loss: 2.2780765774241747
Validation loss: 2.476222186368035

Epoch: 6| Step: 9
Training loss: 2.3286517270658567
Validation loss: 2.467015731659309

Epoch: 6| Step: 10
Training loss: 2.555067681537765
Validation loss: 2.466307063137806

Epoch: 6| Step: 11
Training loss: 2.068723363791301
Validation loss: 2.464376656214646

Epoch: 6| Step: 12
Training loss: 2.252412244672896
Validation loss: 2.4775852855139084

Epoch: 6| Step: 13
Training loss: 2.885156851592054
Validation loss: 2.4770585936093457

Epoch: 154| Step: 0
Training loss: 2.3898712166601066
Validation loss: 2.4759566953011993

Epoch: 6| Step: 1
Training loss: 2.4633838906818997
Validation loss: 2.4843525195754106

Epoch: 6| Step: 2
Training loss: 1.858835799309163
Validation loss: 2.4888755930912527

Epoch: 6| Step: 3
Training loss: 2.9015625920293133
Validation loss: 2.482679784359407

Epoch: 6| Step: 4
Training loss: 2.897467026609291
Validation loss: 2.4708735993971023

Epoch: 6| Step: 5
Training loss: 2.490599888922679
Validation loss: 2.4695755105505883

Epoch: 6| Step: 6
Training loss: 2.0668224932262227
Validation loss: 2.4744828053065415

Epoch: 6| Step: 7
Training loss: 2.392312545049644
Validation loss: 2.47380944590361

Epoch: 6| Step: 8
Training loss: 1.921004121294421
Validation loss: 2.466236299482194

Epoch: 6| Step: 9
Training loss: 1.8090448002452657
Validation loss: 2.474347075320587

Epoch: 6| Step: 10
Training loss: 2.785024589005409
Validation loss: 2.4679189884892554

Epoch: 6| Step: 11
Training loss: 2.5528164699344726
Validation loss: 2.4769532289551606

Epoch: 6| Step: 12
Training loss: 2.9972267047925945
Validation loss: 2.4676091902206925

Epoch: 6| Step: 13
Training loss: 2.8500069333711577
Validation loss: 2.465820231925535

Epoch: 155| Step: 0
Training loss: 2.1595769453779163
Validation loss: 2.475677340072272

Epoch: 6| Step: 1
Training loss: 2.4899125672888434
Validation loss: 2.474315791490012

Epoch: 6| Step: 2
Training loss: 3.114380148359557
Validation loss: 2.4719627818111305

Epoch: 6| Step: 3
Training loss: 2.2324313800172213
Validation loss: 2.498490577246885

Epoch: 6| Step: 4
Training loss: 2.390995171504169
Validation loss: 2.4938365379494254

Epoch: 6| Step: 5
Training loss: 2.4396545473602043
Validation loss: 2.495751745029336

Epoch: 6| Step: 6
Training loss: 2.565752430956032
Validation loss: 2.511774358138766

Epoch: 6| Step: 7
Training loss: 2.4716980148594523
Validation loss: 2.503863559940211

Epoch: 6| Step: 8
Training loss: 2.6036687349483723
Validation loss: 2.469414392232907

Epoch: 6| Step: 9
Training loss: 1.6263145851487508
Validation loss: 2.476386512255519

Epoch: 6| Step: 10
Training loss: 2.5900296105713685
Validation loss: 2.4756965607474615

Epoch: 6| Step: 11
Training loss: 2.934295204710779
Validation loss: 2.484997477411823

Epoch: 6| Step: 12
Training loss: 2.3847562272670073
Validation loss: 2.4809965758540287

Epoch: 6| Step: 13
Training loss: 2.530210587704815
Validation loss: 2.475957642187273

Epoch: 156| Step: 0
Training loss: 2.4401660912688947
Validation loss: 2.4817771044261967

Epoch: 6| Step: 1
Training loss: 2.3732516227900664
Validation loss: 2.4811122590211494

Epoch: 6| Step: 2
Training loss: 2.2097168912835925
Validation loss: 2.47057394260436

Epoch: 6| Step: 3
Training loss: 2.4001509420295104
Validation loss: 2.469401470773976

Epoch: 6| Step: 4
Training loss: 2.6392361752609697
Validation loss: 2.476434955060296

Epoch: 6| Step: 5
Training loss: 2.1733918964749184
Validation loss: 2.4682449596828424

Epoch: 6| Step: 6
Training loss: 2.3343868148075764
Validation loss: 2.4684051401570564

Epoch: 6| Step: 7
Training loss: 2.8518173312932324
Validation loss: 2.46393335842967

Epoch: 6| Step: 8
Training loss: 2.8712261143489273
Validation loss: 2.4694879451029332

Epoch: 6| Step: 9
Training loss: 2.353136619346676
Validation loss: 2.470582362504376

Epoch: 6| Step: 10
Training loss: 2.9666152506741126
Validation loss: 2.4626173472320345

Epoch: 6| Step: 11
Training loss: 2.0990365315929864
Validation loss: 2.4724281852688974

Epoch: 6| Step: 12
Training loss: 2.2663547787480884
Validation loss: 2.4606101004986187

Epoch: 6| Step: 13
Training loss: 2.5241894620225214
Validation loss: 2.4675302188134696

Epoch: 157| Step: 0
Training loss: 2.20704939843202
Validation loss: 2.478790147757299

Epoch: 6| Step: 1
Training loss: 2.323938279098874
Validation loss: 2.4708876067232177

Epoch: 6| Step: 2
Training loss: 2.22091461758133
Validation loss: 2.467733632692432

Epoch: 6| Step: 3
Training loss: 2.2891687362026456
Validation loss: 2.474105964405094

Epoch: 6| Step: 4
Training loss: 2.918044573144724
Validation loss: 2.469579227427577

Epoch: 6| Step: 5
Training loss: 2.2942207206827248
Validation loss: 2.471229016773373

Epoch: 6| Step: 6
Training loss: 2.5926468123210924
Validation loss: 2.477392023635354

Epoch: 6| Step: 7
Training loss: 2.8045348229466
Validation loss: 2.469153439678084

Epoch: 6| Step: 8
Training loss: 2.2153631876149675
Validation loss: 2.47582529128532

Epoch: 6| Step: 9
Training loss: 2.553708510096839
Validation loss: 2.476311768235286

Epoch: 6| Step: 10
Training loss: 1.6877593088482379
Validation loss: 2.4696386806086394

Epoch: 6| Step: 11
Training loss: 2.387017314111364
Validation loss: 2.475887980898576

Epoch: 6| Step: 12
Training loss: 2.452824179312566
Validation loss: 2.4750926793531867

Epoch: 6| Step: 13
Training loss: 2.9922407260075063
Validation loss: 2.4673139524034493

Epoch: 158| Step: 0
Training loss: 2.3264523840695768
Validation loss: 2.4667470830833937

Epoch: 6| Step: 1
Training loss: 1.8846128095432577
Validation loss: 2.4617403688634627

Epoch: 6| Step: 2
Training loss: 3.0647680486683933
Validation loss: 2.4728616204331035

Epoch: 6| Step: 3
Training loss: 2.566073089759866
Validation loss: 2.4781533395590714

Epoch: 6| Step: 4
Training loss: 3.0186605246731895
Validation loss: 2.4704068253758047

Epoch: 6| Step: 5
Training loss: 2.353605681789705
Validation loss: 2.4608395269358825

Epoch: 6| Step: 6
Training loss: 2.5023578968535483
Validation loss: 2.472447133876403

Epoch: 6| Step: 7
Training loss: 2.3742907619570213
Validation loss: 2.4796486607734676

Epoch: 6| Step: 8
Training loss: 2.525158369177805
Validation loss: 2.4746320161935325

Epoch: 6| Step: 9
Training loss: 2.4022207833429983
Validation loss: 2.4693535497543664

Epoch: 6| Step: 10
Training loss: 1.853827463333415
Validation loss: 2.47297125683514

Epoch: 6| Step: 11
Training loss: 2.454863108685544
Validation loss: 2.4732009263683636

Epoch: 6| Step: 12
Training loss: 2.565091938951397
Validation loss: 2.476050314903536

Epoch: 6| Step: 13
Training loss: 2.2552873158591558
Validation loss: 2.477190429472744

Epoch: 159| Step: 0
Training loss: 2.298234705792456
Validation loss: 2.4826850981661552

Epoch: 6| Step: 1
Training loss: 2.006040748783237
Validation loss: 2.4747901133927863

Epoch: 6| Step: 2
Training loss: 3.050878154470895
Validation loss: 2.475559356366171

Epoch: 6| Step: 3
Training loss: 1.8775210280580537
Validation loss: 2.4751813147820387

Epoch: 6| Step: 4
Training loss: 1.5841759731628722
Validation loss: 2.476171260058769

Epoch: 6| Step: 5
Training loss: 2.7357513451547217
Validation loss: 2.471176258968594

Epoch: 6| Step: 6
Training loss: 2.8903046301562294
Validation loss: 2.480236421052474

Epoch: 6| Step: 7
Training loss: 2.441956578598951
Validation loss: 2.4757455948629286

Epoch: 6| Step: 8
Training loss: 2.4507702259013993
Validation loss: 2.472177307444793

Epoch: 6| Step: 9
Training loss: 2.190891252953758
Validation loss: 2.4755166750717206

Epoch: 6| Step: 10
Training loss: 2.537445492464377
Validation loss: 2.4722460848005494

Epoch: 6| Step: 11
Training loss: 2.6240245278189027
Validation loss: 2.47355795314168

Epoch: 6| Step: 12
Training loss: 2.722998533256211
Validation loss: 2.4808382815344885

Epoch: 6| Step: 13
Training loss: 2.809352469857821
Validation loss: 2.5119383910210944

Epoch: 160| Step: 0
Training loss: 1.9942344172704474
Validation loss: 2.5063097482917636

Epoch: 6| Step: 1
Training loss: 2.4570535680002887
Validation loss: 2.518418161321355

Epoch: 6| Step: 2
Training loss: 3.244072129676519
Validation loss: 2.5302064259316324

Epoch: 6| Step: 3
Training loss: 2.3622205134987992
Validation loss: 2.5164073414698036

Epoch: 6| Step: 4
Training loss: 2.4769363843120216
Validation loss: 2.501217672713259

Epoch: 6| Step: 5
Training loss: 2.715931011908088
Validation loss: 2.4884798857944133

Epoch: 6| Step: 6
Training loss: 2.3839768863555983
Validation loss: 2.487213216956129

Epoch: 6| Step: 7
Training loss: 2.7156981077548696
Validation loss: 2.490169724765877

Epoch: 6| Step: 8
Training loss: 2.1483160920951963
Validation loss: 2.48480574354522

Epoch: 6| Step: 9
Training loss: 2.3132920841786566
Validation loss: 2.4885756769061134

Epoch: 6| Step: 10
Training loss: 2.1183263664621257
Validation loss: 2.482260069737388

Epoch: 6| Step: 11
Training loss: 2.3689598775600356
Validation loss: 2.4810998309068304

Epoch: 6| Step: 12
Training loss: 2.5799260177650662
Validation loss: 2.482557395599379

Epoch: 6| Step: 13
Training loss: 2.407842134148532
Validation loss: 2.4762637081469667

Epoch: 161| Step: 0
Training loss: 1.8897962408886908
Validation loss: 2.4812498369705316

Epoch: 6| Step: 1
Training loss: 2.1415409996281602
Validation loss: 2.4836050315905496

Epoch: 6| Step: 2
Training loss: 2.4891758243412343
Validation loss: 2.487226852678905

Epoch: 6| Step: 3
Training loss: 2.2232166515694494
Validation loss: 2.4831950266511345

Epoch: 6| Step: 4
Training loss: 2.5232424823055983
Validation loss: 2.4828640963852977

Epoch: 6| Step: 5
Training loss: 3.018335418981305
Validation loss: 2.4876049805538103

Epoch: 6| Step: 6
Training loss: 2.470801358286784
Validation loss: 2.4904460183755655

Epoch: 6| Step: 7
Training loss: 1.9742981979633172
Validation loss: 2.479791183243022

Epoch: 6| Step: 8
Training loss: 2.753925815810371
Validation loss: 2.482083605457355

Epoch: 6| Step: 9
Training loss: 2.244537398120441
Validation loss: 2.479399851777113

Epoch: 6| Step: 10
Training loss: 2.3269831782422727
Validation loss: 2.4738859038876675

Epoch: 6| Step: 11
Training loss: 2.5456045527659046
Validation loss: 2.4766076181114616

Epoch: 6| Step: 12
Training loss: 2.983477552398218
Validation loss: 2.476611709506953

Epoch: 6| Step: 13
Training loss: 2.5448245354142087
Validation loss: 2.4727911729044383

Epoch: 162| Step: 0
Training loss: 2.3080781467370883
Validation loss: 2.476273424543888

Epoch: 6| Step: 1
Training loss: 2.888207656876195
Validation loss: 2.4839202496651485

Epoch: 6| Step: 2
Training loss: 2.6908053099574056
Validation loss: 2.4803341726405836

Epoch: 6| Step: 3
Training loss: 1.5399908373919302
Validation loss: 2.4959587335262463

Epoch: 6| Step: 4
Training loss: 2.318156361227624
Validation loss: 2.4866048694308973

Epoch: 6| Step: 5
Training loss: 2.5032805852839726
Validation loss: 2.4743442167528986

Epoch: 6| Step: 6
Training loss: 2.2188141504604317
Validation loss: 2.479311984208333

Epoch: 6| Step: 7
Training loss: 2.786035171653734
Validation loss: 2.474278669532122

Epoch: 6| Step: 8
Training loss: 2.514284143818947
Validation loss: 2.476456857507916

Epoch: 6| Step: 9
Training loss: 2.2014437273203478
Validation loss: 2.47046979724689

Epoch: 6| Step: 10
Training loss: 3.0595057894967868
Validation loss: 2.4704569617313816

Epoch: 6| Step: 11
Training loss: 2.7561900091238347
Validation loss: 2.478713255914669

Epoch: 6| Step: 12
Training loss: 2.390064049026148
Validation loss: 2.475900154311985

Epoch: 6| Step: 13
Training loss: 1.5721885899217922
Validation loss: 2.484859035531043

Epoch: 163| Step: 0
Training loss: 2.7240853410703876
Validation loss: 2.4816620687139412

Epoch: 6| Step: 1
Training loss: 1.863300443346508
Validation loss: 2.4763425935972703

Epoch: 6| Step: 2
Training loss: 2.174005245865684
Validation loss: 2.4788183293468697

Epoch: 6| Step: 3
Training loss: 2.5288396591455697
Validation loss: 2.4858723492639814

Epoch: 6| Step: 4
Training loss: 2.7161532749697064
Validation loss: 2.487212665774199

Epoch: 6| Step: 5
Training loss: 2.788867334541762
Validation loss: 2.486049959175356

Epoch: 6| Step: 6
Training loss: 2.0389762070910726
Validation loss: 2.478930010573017

Epoch: 6| Step: 7
Training loss: 2.6025626977702565
Validation loss: 2.4882486881219554

Epoch: 6| Step: 8
Training loss: 2.1760062027800733
Validation loss: 2.4810792507094948

Epoch: 6| Step: 9
Training loss: 2.5049810379132964
Validation loss: 2.480157130734367

Epoch: 6| Step: 10
Training loss: 2.4830408892727265
Validation loss: 2.4878529769453483

Epoch: 6| Step: 11
Training loss: 2.321730055462834
Validation loss: 2.4772195836852786

Epoch: 6| Step: 12
Training loss: 2.484347937094639
Validation loss: 2.472322848292588

Epoch: 6| Step: 13
Training loss: 2.560321626277951
Validation loss: 2.483563864545038

Epoch: 164| Step: 0
Training loss: 2.991045940754195
Validation loss: 2.484973043759932

Epoch: 6| Step: 1
Training loss: 2.4780294123298017
Validation loss: 2.4837885070385806

Epoch: 6| Step: 2
Training loss: 2.205431224842935
Validation loss: 2.4758682802246863

Epoch: 6| Step: 3
Training loss: 3.3316867735273332
Validation loss: 2.4809083003475108

Epoch: 6| Step: 4
Training loss: 2.7042100084903784
Validation loss: 2.4906931734985602

Epoch: 6| Step: 5
Training loss: 2.306585222397732
Validation loss: 2.482878068074568

Epoch: 6| Step: 6
Training loss: 2.32987766730586
Validation loss: 2.4824488148401755

Epoch: 6| Step: 7
Training loss: 2.3440779392968243
Validation loss: 2.4800374775023633

Epoch: 6| Step: 8
Training loss: 2.195733841072291
Validation loss: 2.494517330517529

Epoch: 6| Step: 9
Training loss: 2.157461296079165
Validation loss: 2.487851667226432

Epoch: 6| Step: 10
Training loss: 2.1521525497113356
Validation loss: 2.491307947454976

Epoch: 6| Step: 11
Training loss: 2.1743481503303963
Validation loss: 2.4979089893247477

Epoch: 6| Step: 12
Training loss: 2.4518067529395062
Validation loss: 2.4974479842218615

Epoch: 6| Step: 13
Training loss: 1.9487737174361777
Validation loss: 2.500011571221594

Epoch: 165| Step: 0
Training loss: 2.497366949132503
Validation loss: 2.4987929768227453

Epoch: 6| Step: 1
Training loss: 2.173509490454651
Validation loss: 2.489565451543159

Epoch: 6| Step: 2
Training loss: 2.5295662637066676
Validation loss: 2.4884781292968623

Epoch: 6| Step: 3
Training loss: 1.8226932134367066
Validation loss: 2.4791730565434875

Epoch: 6| Step: 4
Training loss: 2.513335708630533
Validation loss: 2.4810083318035097

Epoch: 6| Step: 5
Training loss: 2.3242901446492588
Validation loss: 2.484818408990926

Epoch: 6| Step: 6
Training loss: 2.6593998801370855
Validation loss: 2.470267469271971

Epoch: 6| Step: 7
Training loss: 2.192378571300185
Validation loss: 2.4767009884914963

Epoch: 6| Step: 8
Training loss: 2.2117959233075424
Validation loss: 2.4771641783572718

Epoch: 6| Step: 9
Training loss: 3.23693215828866
Validation loss: 2.4837708928317865

Epoch: 6| Step: 10
Training loss: 2.674656808531987
Validation loss: 2.4771245725731794

Epoch: 6| Step: 11
Training loss: 2.764235497684254
Validation loss: 2.4952799903823677

Epoch: 6| Step: 12
Training loss: 2.5139509044699837
Validation loss: 2.5100511994528953

Epoch: 6| Step: 13
Training loss: 2.1378518958184616
Validation loss: 2.507491474460397

Epoch: 166| Step: 0
Training loss: 2.2161673975491927
Validation loss: 2.4895753634230813

Epoch: 6| Step: 1
Training loss: 2.680503787615291
Validation loss: 2.4772935785036987

Epoch: 6| Step: 2
Training loss: 2.5714411243253426
Validation loss: 2.4887652203464725

Epoch: 6| Step: 3
Training loss: 2.6689765780390604
Validation loss: 2.4809856206684024

Epoch: 6| Step: 4
Training loss: 2.951043419658456
Validation loss: 2.474781370621367

Epoch: 6| Step: 5
Training loss: 2.4550946340559685
Validation loss: 2.4793865897033824

Epoch: 6| Step: 6
Training loss: 2.3103662905577136
Validation loss: 2.4851864783039486

Epoch: 6| Step: 7
Training loss: 1.9243236542579498
Validation loss: 2.4864766730047823

Epoch: 6| Step: 8
Training loss: 2.7095928980094253
Validation loss: 2.4800373012547263

Epoch: 6| Step: 9
Training loss: 2.3842097953869725
Validation loss: 2.478307628596629

Epoch: 6| Step: 10
Training loss: 2.497439312813287
Validation loss: 2.47592162816362

Epoch: 6| Step: 11
Training loss: 2.2404356170168787
Validation loss: 2.4788948412277105

Epoch: 6| Step: 12
Training loss: 2.3479407926694718
Validation loss: 2.491655633789268

Epoch: 6| Step: 13
Training loss: 2.07386020930899
Validation loss: 2.477519719789245

Epoch: 167| Step: 0
Training loss: 2.45402724081028
Validation loss: 2.4819788223539385

Epoch: 6| Step: 1
Training loss: 3.0114904334460877
Validation loss: 2.47748852413739

Epoch: 6| Step: 2
Training loss: 1.8598503338680341
Validation loss: 2.484156856916327

Epoch: 6| Step: 3
Training loss: 1.9245669732475876
Validation loss: 2.4787001104010433

Epoch: 6| Step: 4
Training loss: 2.1386275056042487
Validation loss: 2.490516652582874

Epoch: 6| Step: 5
Training loss: 2.4538917072281783
Validation loss: 2.4854807602153013

Epoch: 6| Step: 6
Training loss: 2.5422162467583247
Validation loss: 2.49169891574705

Epoch: 6| Step: 7
Training loss: 2.4387560811399567
Validation loss: 2.5012666672293156

Epoch: 6| Step: 8
Training loss: 2.7928848360700536
Validation loss: 2.491721162444608

Epoch: 6| Step: 9
Training loss: 2.868116347864517
Validation loss: 2.487902921443704

Epoch: 6| Step: 10
Training loss: 2.5287116238920246
Validation loss: 2.488737486688567

Epoch: 6| Step: 11
Training loss: 2.065301322104459
Validation loss: 2.4848740354397574

Epoch: 6| Step: 12
Training loss: 2.201981484455703
Validation loss: 2.4859057175262804

Epoch: 6| Step: 13
Training loss: 2.4734575802150007
Validation loss: 2.4792093599206986

Epoch: 168| Step: 0
Training loss: 2.1680775108205945
Validation loss: 2.4899809186017907

Epoch: 6| Step: 1
Training loss: 2.4905782544429127
Validation loss: 2.4919869113984565

Epoch: 6| Step: 2
Training loss: 1.772429864784453
Validation loss: 2.493568842784381

Epoch: 6| Step: 3
Training loss: 2.3782231394585134
Validation loss: 2.4749865406325458

Epoch: 6| Step: 4
Training loss: 2.825167463622248
Validation loss: 2.483746671093657

Epoch: 6| Step: 5
Training loss: 2.542340788530931
Validation loss: 2.489069855114993

Epoch: 6| Step: 6
Training loss: 2.525696867911726
Validation loss: 2.4924116362707665

Epoch: 6| Step: 7
Training loss: 2.3136684455698036
Validation loss: 2.4932592273942906

Epoch: 6| Step: 8
Training loss: 2.3351974080384035
Validation loss: 2.4843997434267613

Epoch: 6| Step: 9
Training loss: 2.2466087215357873
Validation loss: 2.4828240053627706

Epoch: 6| Step: 10
Training loss: 2.790555153855846
Validation loss: 2.4791851256722506

Epoch: 6| Step: 11
Training loss: 2.3582456803853997
Validation loss: 2.4850946020006854

Epoch: 6| Step: 12
Training loss: 2.661182625680568
Validation loss: 2.4794590454370637

Epoch: 6| Step: 13
Training loss: 2.506816440412834
Validation loss: 2.4795000081929963

Epoch: 169| Step: 0
Training loss: 2.748512385889535
Validation loss: 2.499273321715208

Epoch: 6| Step: 1
Training loss: 2.011898292010866
Validation loss: 2.4895130823244567

Epoch: 6| Step: 2
Training loss: 2.8826986977876414
Validation loss: 2.498692568000819

Epoch: 6| Step: 3
Training loss: 1.7922784810473757
Validation loss: 2.480984515537342

Epoch: 6| Step: 4
Training loss: 2.918506659548025
Validation loss: 2.4898236583022357

Epoch: 6| Step: 5
Training loss: 2.571399400939183
Validation loss: 2.4824670466873453

Epoch: 6| Step: 6
Training loss: 2.167605037938043
Validation loss: 2.481427777741446

Epoch: 6| Step: 7
Training loss: 2.3656418793452776
Validation loss: 2.473070990726496

Epoch: 6| Step: 8
Training loss: 2.547585230366483
Validation loss: 2.47947959100409

Epoch: 6| Step: 9
Training loss: 2.7465819878472217
Validation loss: 2.477970986312645

Epoch: 6| Step: 10
Training loss: 2.3611071966799533
Validation loss: 2.4815616953186006

Epoch: 6| Step: 11
Training loss: 2.588537839174159
Validation loss: 2.4763020118721126

Epoch: 6| Step: 12
Training loss: 2.1922600318834897
Validation loss: 2.4768929246036824

Epoch: 6| Step: 13
Training loss: 2.1657088314416204
Validation loss: 2.479886781200061

Epoch: 170| Step: 0
Training loss: 2.129610893455806
Validation loss: 2.4813890727261927

Epoch: 6| Step: 1
Training loss: 1.8414091347310482
Validation loss: 2.4875951726396854

Epoch: 6| Step: 2
Training loss: 2.5224700112351317
Validation loss: 2.4951976904974273

Epoch: 6| Step: 3
Training loss: 2.8749416179534193
Validation loss: 2.5021055097229796

Epoch: 6| Step: 4
Training loss: 2.6859970325842717
Validation loss: 2.489804474866283

Epoch: 6| Step: 5
Training loss: 3.0019124610941383
Validation loss: 2.521134868168697

Epoch: 6| Step: 6
Training loss: 2.2592045805463203
Validation loss: 2.496464390864474

Epoch: 6| Step: 7
Training loss: 2.6691763115251184
Validation loss: 2.4866559895481384

Epoch: 6| Step: 8
Training loss: 2.6302976284669475
Validation loss: 2.4760574002234796

Epoch: 6| Step: 9
Training loss: 2.5718039965185504
Validation loss: 2.474135500269508

Epoch: 6| Step: 10
Training loss: 2.731253362518266
Validation loss: 2.4664754086741754

Epoch: 6| Step: 11
Training loss: 2.345442504126062
Validation loss: 2.479566643343087

Epoch: 6| Step: 12
Training loss: 2.0678712666949455
Validation loss: 2.472850886313357

Epoch: 6| Step: 13
Training loss: 2.0056676429406766
Validation loss: 2.4842136738537732

Epoch: 171| Step: 0
Training loss: 2.537971896185061
Validation loss: 2.4785566754920905

Epoch: 6| Step: 1
Training loss: 2.3488614103926895
Validation loss: 2.4751896628204446

Epoch: 6| Step: 2
Training loss: 2.4239559954435395
Validation loss: 2.475146702230118

Epoch: 6| Step: 3
Training loss: 2.8697882142827935
Validation loss: 2.483954340031353

Epoch: 6| Step: 4
Training loss: 2.7138099074109836
Validation loss: 2.481038634348322

Epoch: 6| Step: 5
Training loss: 1.9639035472415751
Validation loss: 2.479658964842633

Epoch: 6| Step: 6
Training loss: 2.530557325743824
Validation loss: 2.4813473564669324

Epoch: 6| Step: 7
Training loss: 2.0835221268623476
Validation loss: 2.4825734578174883

Epoch: 6| Step: 8
Training loss: 2.082466008526012
Validation loss: 2.4806364468334205

Epoch: 6| Step: 9
Training loss: 2.618238369073297
Validation loss: 2.4836649329828315

Epoch: 6| Step: 10
Training loss: 2.3365689958229803
Validation loss: 2.487919324486835

Epoch: 6| Step: 11
Training loss: 2.492193909639828
Validation loss: 2.4840078342475964

Epoch: 6| Step: 12
Training loss: 2.590001166207187
Validation loss: 2.482503958164631

Epoch: 6| Step: 13
Training loss: 2.5617859938888548
Validation loss: 2.4920374108604166

Epoch: 172| Step: 0
Training loss: 2.734888083139971
Validation loss: 2.5063071481431267

Epoch: 6| Step: 1
Training loss: 2.222039054898208
Validation loss: 2.494500708022189

Epoch: 6| Step: 2
Training loss: 2.6049782264077743
Validation loss: 2.4936301780219985

Epoch: 6| Step: 3
Training loss: 2.2497838234366574
Validation loss: 2.492623349845448

Epoch: 6| Step: 4
Training loss: 1.7966581794288239
Validation loss: 2.4802090246035915

Epoch: 6| Step: 5
Training loss: 3.122431347888666
Validation loss: 2.494192530630363

Epoch: 6| Step: 6
Training loss: 2.1146074741532703
Validation loss: 2.4816220063092986

Epoch: 6| Step: 7
Training loss: 2.4739159724306443
Validation loss: 2.47675570627843

Epoch: 6| Step: 8
Training loss: 2.348635248829298
Validation loss: 2.475460525335622

Epoch: 6| Step: 9
Training loss: 2.8822464219156396
Validation loss: 2.4826375615991516

Epoch: 6| Step: 10
Training loss: 2.549641142633072
Validation loss: 2.474392049148868

Epoch: 6| Step: 11
Training loss: 1.7820197332897254
Validation loss: 2.486988508330105

Epoch: 6| Step: 12
Training loss: 2.4808920191276544
Validation loss: 2.4940099878633757

Epoch: 6| Step: 13
Training loss: 2.7282283907565597
Validation loss: 2.503631632281641

Epoch: 173| Step: 0
Training loss: 2.141502256304514
Validation loss: 2.512742043218899

Epoch: 6| Step: 1
Training loss: 1.9656183069839908
Validation loss: 2.5023711086416784

Epoch: 6| Step: 2
Training loss: 2.43533111326275
Validation loss: 2.500580624866213

Epoch: 6| Step: 3
Training loss: 2.249713031694395
Validation loss: 2.4962167406102997

Epoch: 6| Step: 4
Training loss: 2.554383239359456
Validation loss: 2.49883417921201

Epoch: 6| Step: 5
Training loss: 2.311679797732301
Validation loss: 2.4955105844063157

Epoch: 6| Step: 6
Training loss: 2.053209825037939
Validation loss: 2.4875780166838886

Epoch: 6| Step: 7
Training loss: 2.9995871895646395
Validation loss: 2.4788114522991043

Epoch: 6| Step: 8
Training loss: 2.5920783481530543
Validation loss: 2.4765321266405644

Epoch: 6| Step: 9
Training loss: 2.3987947378116434
Validation loss: 2.4787684903566927

Epoch: 6| Step: 10
Training loss: 2.2949197972065063
Validation loss: 2.4815031563078205

Epoch: 6| Step: 11
Training loss: 2.227741344319933
Validation loss: 2.4748621417354335

Epoch: 6| Step: 12
Training loss: 2.868339120200343
Validation loss: 2.4787358355762694

Epoch: 6| Step: 13
Training loss: 3.025508831514989
Validation loss: 2.4795436786174494

Epoch: 174| Step: 0
Training loss: 2.2938345163350555
Validation loss: 2.472682878412148

Epoch: 6| Step: 1
Training loss: 2.252635895316346
Validation loss: 2.481609973050665

Epoch: 6| Step: 2
Training loss: 2.5748764378799076
Validation loss: 2.4879128159894797

Epoch: 6| Step: 3
Training loss: 1.9887421379197694
Validation loss: 2.5062506100978093

Epoch: 6| Step: 4
Training loss: 1.7102780399374284
Validation loss: 2.507992984539849

Epoch: 6| Step: 5
Training loss: 2.4467708638565413
Validation loss: 2.5460749892832837

Epoch: 6| Step: 6
Training loss: 1.820581440848121
Validation loss: 2.5621111970276482

Epoch: 6| Step: 7
Training loss: 3.0636508881106526
Validation loss: 2.5492010747460645

Epoch: 6| Step: 8
Training loss: 3.5755123278171017
Validation loss: 2.5363511618363055

Epoch: 6| Step: 9
Training loss: 2.631441840565419
Validation loss: 2.5042381760917665

Epoch: 6| Step: 10
Training loss: 2.2309964118699543
Validation loss: 2.47824147268605

Epoch: 6| Step: 11
Training loss: 3.0439263576667615
Validation loss: 2.477037161698094

Epoch: 6| Step: 12
Training loss: 2.425499154856072
Validation loss: 2.4803801192821773

Epoch: 6| Step: 13
Training loss: 2.0902563525594853
Validation loss: 2.4898117365004393

Epoch: 175| Step: 0
Training loss: 2.1340760899986795
Validation loss: 2.4899243449778505

Epoch: 6| Step: 1
Training loss: 2.823369701861354
Validation loss: 2.4880874853780397

Epoch: 6| Step: 2
Training loss: 2.973428192364653
Validation loss: 2.4871779650311168

Epoch: 6| Step: 3
Training loss: 1.9968127245437977
Validation loss: 2.4844452630109735

Epoch: 6| Step: 4
Training loss: 2.4635383546792173
Validation loss: 2.4854618710044005

Epoch: 6| Step: 5
Training loss: 2.959644850740894
Validation loss: 2.4869971123225993

Epoch: 6| Step: 6
Training loss: 3.142701786090495
Validation loss: 2.4770681224085402

Epoch: 6| Step: 7
Training loss: 2.3295267480746205
Validation loss: 2.473767184184489

Epoch: 6| Step: 8
Training loss: 1.5121290650560246
Validation loss: 2.4707568417867765

Epoch: 6| Step: 9
Training loss: 2.5431722861314414
Validation loss: 2.4818362974250316

Epoch: 6| Step: 10
Training loss: 2.3607051712331764
Validation loss: 2.46961330658438

Epoch: 6| Step: 11
Training loss: 2.492610022623682
Validation loss: 2.4774381772157024

Epoch: 6| Step: 12
Training loss: 2.69992564063639
Validation loss: 2.4747387321080256

Epoch: 6| Step: 13
Training loss: 2.4575600350731555
Validation loss: 2.4813500628405345

Epoch: 176| Step: 0
Training loss: 2.0925916057848157
Validation loss: 2.4678259782188388

Epoch: 6| Step: 1
Training loss: 2.259375536062512
Validation loss: 2.4767152917622925

Epoch: 6| Step: 2
Training loss: 2.9830060285608986
Validation loss: 2.469711486681901

Epoch: 6| Step: 3
Training loss: 2.2090456791338653
Validation loss: 2.47735002353098

Epoch: 6| Step: 4
Training loss: 2.8546614020897567
Validation loss: 2.4734927947401646

Epoch: 6| Step: 5
Training loss: 1.9857898384140837
Validation loss: 2.477944599141923

Epoch: 6| Step: 6
Training loss: 2.1925252685482683
Validation loss: 2.477454216486263

Epoch: 6| Step: 7
Training loss: 2.1609327910641594
Validation loss: 2.4758312056190013

Epoch: 6| Step: 8
Training loss: 2.387227255536007
Validation loss: 2.472967320099913

Epoch: 6| Step: 9
Training loss: 2.0243627134105138
Validation loss: 2.4698611627908

Epoch: 6| Step: 10
Training loss: 3.164121726152867
Validation loss: 2.4873544592908083

Epoch: 6| Step: 11
Training loss: 2.7067897408163732
Validation loss: 2.488877413170713

Epoch: 6| Step: 12
Training loss: 2.4309401640783643
Validation loss: 2.489835308736124

Epoch: 6| Step: 13
Training loss: 2.563430803163015
Validation loss: 2.4854174332299106

Epoch: 177| Step: 0
Training loss: 2.3827260486366373
Validation loss: 2.4898680174368812

Epoch: 6| Step: 1
Training loss: 1.8890147011438783
Validation loss: 2.491487538289637

Epoch: 6| Step: 2
Training loss: 1.9956846172484812
Validation loss: 2.4895638075399362

Epoch: 6| Step: 3
Training loss: 3.484857500137509
Validation loss: 2.48799292123039

Epoch: 6| Step: 4
Training loss: 2.5214161054734534
Validation loss: 2.483628806728455

Epoch: 6| Step: 5
Training loss: 2.4053077339084505
Validation loss: 2.4838089687986264

Epoch: 6| Step: 6
Training loss: 2.446985032298117
Validation loss: 2.4865652062714285

Epoch: 6| Step: 7
Training loss: 2.038469133238354
Validation loss: 2.4900432358732543

Epoch: 6| Step: 8
Training loss: 1.9895082895662761
Validation loss: 2.4819014369120467

Epoch: 6| Step: 9
Training loss: 2.771406262663324
Validation loss: 2.484592244308573

Epoch: 6| Step: 10
Training loss: 2.790213040384189
Validation loss: 2.490428475189253

Epoch: 6| Step: 11
Training loss: 2.7921252277348727
Validation loss: 2.479468228461152

Epoch: 6| Step: 12
Training loss: 2.245745769579556
Validation loss: 2.4942944588016585

Epoch: 6| Step: 13
Training loss: 2.276584295771983
Validation loss: 2.4878558838799925

Epoch: 178| Step: 0
Training loss: 2.7128698843924592
Validation loss: 2.4918840277369307

Epoch: 6| Step: 1
Training loss: 2.412226006163098
Validation loss: 2.488527198984707

Epoch: 6| Step: 2
Training loss: 2.3791141012055674
Validation loss: 2.5046501621724344

Epoch: 6| Step: 3
Training loss: 1.7188224083646868
Validation loss: 2.51359846101273

Epoch: 6| Step: 4
Training loss: 2.4360251243936806
Validation loss: 2.53059678616965

Epoch: 6| Step: 5
Training loss: 2.81658570965175
Validation loss: 2.5238856969675996

Epoch: 6| Step: 6
Training loss: 2.0527735161066736
Validation loss: 2.5450079847958755

Epoch: 6| Step: 7
Training loss: 2.62570625976695
Validation loss: 2.512688797014321

Epoch: 6| Step: 8
Training loss: 2.3307748346099153
Validation loss: 2.509951399323773

Epoch: 6| Step: 9
Training loss: 3.2913540719888017
Validation loss: 2.50389275271963

Epoch: 6| Step: 10
Training loss: 2.742394855191091
Validation loss: 2.490359649499074

Epoch: 6| Step: 11
Training loss: 2.454019371323116
Validation loss: 2.485938374031468

Epoch: 6| Step: 12
Training loss: 1.5465539155892571
Validation loss: 2.4819451371286663

Epoch: 6| Step: 13
Training loss: 2.3082142887565325
Validation loss: 2.48645123908367

Epoch: 179| Step: 0
Training loss: 2.084494902723727
Validation loss: 2.4824870471417944

Epoch: 6| Step: 1
Training loss: 2.7685176571046455
Validation loss: 2.480797965562518

Epoch: 6| Step: 2
Training loss: 2.485766806697493
Validation loss: 2.4790582285868727

Epoch: 6| Step: 3
Training loss: 2.71373250708791
Validation loss: 2.480974168924113

Epoch: 6| Step: 4
Training loss: 2.321619147558142
Validation loss: 2.4926918021797837

Epoch: 6| Step: 5
Training loss: 1.827573318054403
Validation loss: 2.490697776212994

Epoch: 6| Step: 6
Training loss: 2.2411573216626035
Validation loss: 2.4869008369826133

Epoch: 6| Step: 7
Training loss: 2.4724660984102664
Validation loss: 2.486898264476135

Epoch: 6| Step: 8
Training loss: 2.677330114565329
Validation loss: 2.4945856910576762

Epoch: 6| Step: 9
Training loss: 2.493501036030868
Validation loss: 2.49604455842691

Epoch: 6| Step: 10
Training loss: 2.5870530410478296
Validation loss: 2.5082558213315345

Epoch: 6| Step: 11
Training loss: 2.262573502331933
Validation loss: 2.508564575811339

Epoch: 6| Step: 12
Training loss: 1.823420347837502
Validation loss: 2.5137914922076114

Epoch: 6| Step: 13
Training loss: 3.0983970497095634
Validation loss: 2.4991354878070586

Epoch: 180| Step: 0
Training loss: 2.5656993711149116
Validation loss: 2.4982743029229724

Epoch: 6| Step: 1
Training loss: 1.940161200246174
Validation loss: 2.5110753304633935

Epoch: 6| Step: 2
Training loss: 2.181905513518373
Validation loss: 2.5016644182476773

Epoch: 6| Step: 3
Training loss: 2.432216689419755
Validation loss: 2.4938074505274446

Epoch: 6| Step: 4
Training loss: 1.581999300410409
Validation loss: 2.495848053096443

Epoch: 6| Step: 5
Training loss: 2.9791493626516603
Validation loss: 2.500880197706446

Epoch: 6| Step: 6
Training loss: 1.8864951307252618
Validation loss: 2.478229358876135

Epoch: 6| Step: 7
Training loss: 2.59440200148325
Validation loss: 2.48257621087284

Epoch: 6| Step: 8
Training loss: 2.435578787318497
Validation loss: 2.481578364452647

Epoch: 6| Step: 9
Training loss: 2.6536421596413073
Validation loss: 2.4817869113162483

Epoch: 6| Step: 10
Training loss: 3.057515038167133
Validation loss: 2.483542008774471

Epoch: 6| Step: 11
Training loss: 2.856327621774182
Validation loss: 2.490389678699923

Epoch: 6| Step: 12
Training loss: 2.5960578302957247
Validation loss: 2.4801819482836747

Epoch: 6| Step: 13
Training loss: 2.027618213222427
Validation loss: 2.4897405715724537

Epoch: 181| Step: 0
Training loss: 2.6362902936031145
Validation loss: 2.4827819451031816

Epoch: 6| Step: 1
Training loss: 2.7238866578999663
Validation loss: 2.4907349884878567

Epoch: 6| Step: 2
Training loss: 2.4883556983625725
Validation loss: 2.4955252734839264

Epoch: 6| Step: 3
Training loss: 2.393855287313057
Validation loss: 2.4990512001132257

Epoch: 6| Step: 4
Training loss: 2.8217729078985077
Validation loss: 2.5116408807164525

Epoch: 6| Step: 5
Training loss: 1.9569301921769804
Validation loss: 2.5131889379274184

Epoch: 6| Step: 6
Training loss: 2.105078156712458
Validation loss: 2.5145132716125684

Epoch: 6| Step: 7
Training loss: 2.3909855988251083
Validation loss: 2.505959227097066

Epoch: 6| Step: 8
Training loss: 2.531840691109188
Validation loss: 2.4961976381431557

Epoch: 6| Step: 9
Training loss: 2.1075370410637073
Validation loss: 2.512240231624844

Epoch: 6| Step: 10
Training loss: 2.80613819740703
Validation loss: 2.508599923204564

Epoch: 6| Step: 11
Training loss: 2.210995946320571
Validation loss: 2.5143861899307476

Epoch: 6| Step: 12
Training loss: 2.051150217924494
Validation loss: 2.5119051391990412

Epoch: 6| Step: 13
Training loss: 2.4323482357093322
Validation loss: 2.496425680219529

Epoch: 182| Step: 0
Training loss: 2.3390863431379136
Validation loss: 2.496185149826237

Epoch: 6| Step: 1
Training loss: 2.1883443020470423
Validation loss: 2.4885622641369802

Epoch: 6| Step: 2
Training loss: 2.5228347294609037
Validation loss: 2.4850602234643304

Epoch: 6| Step: 3
Training loss: 2.8205343988677987
Validation loss: 2.4784212407587383

Epoch: 6| Step: 4
Training loss: 2.7199577192217985
Validation loss: 2.476018675452792

Epoch: 6| Step: 5
Training loss: 2.4607473148547165
Validation loss: 2.4895487401200214

Epoch: 6| Step: 6
Training loss: 1.8033283614808726
Validation loss: 2.4915031521919055

Epoch: 6| Step: 7
Training loss: 3.1115785391419934
Validation loss: 2.495250680914515

Epoch: 6| Step: 8
Training loss: 1.4192397461265922
Validation loss: 2.492239095538436

Epoch: 6| Step: 9
Training loss: 2.439043387827506
Validation loss: 2.5103452258147674

Epoch: 6| Step: 10
Training loss: 2.2121871813307616
Validation loss: 2.504233606198077

Epoch: 6| Step: 11
Training loss: 2.4670000272023325
Validation loss: 2.521527932891752

Epoch: 6| Step: 12
Training loss: 2.4375447000170682
Validation loss: 2.515267230196128

Epoch: 6| Step: 13
Training loss: 2.758763481629188
Validation loss: 2.496930845142979

Epoch: 183| Step: 0
Training loss: 2.0187326767122076
Validation loss: 2.512313115901748

Epoch: 6| Step: 1
Training loss: 2.0738915941532037
Validation loss: 2.512978166339878

Epoch: 6| Step: 2
Training loss: 2.7954421821991873
Validation loss: 2.4881852559128115

Epoch: 6| Step: 3
Training loss: 2.37618125852218
Validation loss: 2.4991297240249426

Epoch: 6| Step: 4
Training loss: 2.6347471650808103
Validation loss: 2.490665174133921

Epoch: 6| Step: 5
Training loss: 1.9832245503038985
Validation loss: 2.4882112550098103

Epoch: 6| Step: 6
Training loss: 2.307551269622947
Validation loss: 2.497928030981366

Epoch: 6| Step: 7
Training loss: 2.6555479131594844
Validation loss: 2.4846460606394634

Epoch: 6| Step: 8
Training loss: 3.049877233066896
Validation loss: 2.4871298752635798

Epoch: 6| Step: 9
Training loss: 2.4714278172303614
Validation loss: 2.4812469303038447

Epoch: 6| Step: 10
Training loss: 2.102912327526562
Validation loss: 2.4875863390943542

Epoch: 6| Step: 11
Training loss: 2.6949421061452976
Validation loss: 2.4986392609492096

Epoch: 6| Step: 12
Training loss: 2.008606749900196
Validation loss: 2.488291614137601

Epoch: 6| Step: 13
Training loss: 2.494774793808064
Validation loss: 2.5034095640508296

Epoch: 184| Step: 0
Training loss: 1.9153614852399599
Validation loss: 2.495198136401951

Epoch: 6| Step: 1
Training loss: 2.7985662536139286
Validation loss: 2.498839888018997

Epoch: 6| Step: 2
Training loss: 2.2970306966521594
Validation loss: 2.5000175157569178

Epoch: 6| Step: 3
Training loss: 3.004356876300374
Validation loss: 2.4924046851286192

Epoch: 6| Step: 4
Training loss: 2.350393822318957
Validation loss: 2.4893319124028066

Epoch: 6| Step: 5
Training loss: 2.5159122468900845
Validation loss: 2.489924983333387

Epoch: 6| Step: 6
Training loss: 2.4405992806968513
Validation loss: 2.4928267166416465

Epoch: 6| Step: 7
Training loss: 2.009053243976821
Validation loss: 2.4939105177409964

Epoch: 6| Step: 8
Training loss: 2.691669600714471
Validation loss: 2.49603952777774

Epoch: 6| Step: 9
Training loss: 2.5298593727804164
Validation loss: 2.4921352177699534

Epoch: 6| Step: 10
Training loss: 2.4811120828498607
Validation loss: 2.5058709825944137

Epoch: 6| Step: 11
Training loss: 2.2176741019695356
Validation loss: 2.5058014313680252

Epoch: 6| Step: 12
Training loss: 2.187893423396376
Validation loss: 2.504436744199809

Epoch: 6| Step: 13
Training loss: 2.2356182987495554
Validation loss: 2.5059449559595666

Epoch: 185| Step: 0
Training loss: 2.838129042752967
Validation loss: 2.4977128693136432

Epoch: 6| Step: 1
Training loss: 2.8849277747780855
Validation loss: 2.4935514410860353

Epoch: 6| Step: 2
Training loss: 2.461513972504466
Validation loss: 2.496089403987438

Epoch: 6| Step: 3
Training loss: 2.4077211316873126
Validation loss: 2.4956467794921218

Epoch: 6| Step: 4
Training loss: 2.3598238284418054
Validation loss: 2.4854559955752094

Epoch: 6| Step: 5
Training loss: 2.0820427012173366
Validation loss: 2.4745729559647436

Epoch: 6| Step: 6
Training loss: 2.311786954079252
Validation loss: 2.486650364628895

Epoch: 6| Step: 7
Training loss: 1.7567994992356204
Validation loss: 2.4812747957286256

Epoch: 6| Step: 8
Training loss: 2.7359085578193705
Validation loss: 2.4876937132637047

Epoch: 6| Step: 9
Training loss: 2.0616084397074763
Validation loss: 2.4893167956909137

Epoch: 6| Step: 10
Training loss: 2.3727234419828642
Validation loss: 2.506055412444

Epoch: 6| Step: 11
Training loss: 2.2461338736225995
Validation loss: 2.51473512568014

Epoch: 6| Step: 12
Training loss: 2.527925167980102
Validation loss: 2.5231572045705835

Epoch: 6| Step: 13
Training loss: 2.7128145167397433
Validation loss: 2.527338891710096

Epoch: 186| Step: 0
Training loss: 2.4529243921756883
Validation loss: 2.5283386614857983

Epoch: 6| Step: 1
Training loss: 3.1140454358342993
Validation loss: 2.535065150179278

Epoch: 6| Step: 2
Training loss: 1.657973274682791
Validation loss: 2.516904912075716

Epoch: 6| Step: 3
Training loss: 2.3599701187699003
Validation loss: 2.510097567801097

Epoch: 6| Step: 4
Training loss: 2.2692862557362905
Validation loss: 2.5071326232430104

Epoch: 6| Step: 5
Training loss: 2.783762108066025
Validation loss: 2.4962808760744135

Epoch: 6| Step: 6
Training loss: 2.635874249887172
Validation loss: 2.4936164179616065

Epoch: 6| Step: 7
Training loss: 2.373669703503022
Validation loss: 2.50877286718439

Epoch: 6| Step: 8
Training loss: 2.653698582198275
Validation loss: 2.500517259174238

Epoch: 6| Step: 9
Training loss: 2.0867595981383107
Validation loss: 2.5133767358572148

Epoch: 6| Step: 10
Training loss: 2.6496757380911364
Validation loss: 2.5113501231296684

Epoch: 6| Step: 11
Training loss: 1.4913099814388475
Validation loss: 2.5041701187220013

Epoch: 6| Step: 12
Training loss: 3.0014273109155853
Validation loss: 2.503438666887892

Epoch: 6| Step: 13
Training loss: 1.6049327094687202
Validation loss: 2.513790670224286

Epoch: 187| Step: 0
Training loss: 2.5423650772168913
Validation loss: 2.5133262540918246

Epoch: 6| Step: 1
Training loss: 3.086383410722556
Validation loss: 2.5067279407523193

Epoch: 6| Step: 2
Training loss: 2.335508479395937
Validation loss: 2.505911283681372

Epoch: 6| Step: 3
Training loss: 2.0000296828928295
Validation loss: 2.5022258068963965

Epoch: 6| Step: 4
Training loss: 2.491368078332344
Validation loss: 2.5091910608761894

Epoch: 6| Step: 5
Training loss: 2.3146501803309087
Validation loss: 2.5063558846269323

Epoch: 6| Step: 6
Training loss: 2.6270968826835945
Validation loss: 2.4988410250082462

Epoch: 6| Step: 7
Training loss: 1.6204774719244852
Validation loss: 2.49703063257005

Epoch: 6| Step: 8
Training loss: 1.9581241563587581
Validation loss: 2.4949134419982912

Epoch: 6| Step: 9
Training loss: 2.5640901423422386
Validation loss: 2.4905544180064636

Epoch: 6| Step: 10
Training loss: 2.675641979296425
Validation loss: 2.491370422928669

Epoch: 6| Step: 11
Training loss: 2.89674611879989
Validation loss: 2.5067126119068845

Epoch: 6| Step: 12
Training loss: 2.3354597847117513
Validation loss: 2.506698131055213

Epoch: 6| Step: 13
Training loss: 1.9800065383417855
Validation loss: 2.521641079036825

Epoch: 188| Step: 0
Training loss: 1.8244618941990538
Validation loss: 2.523755174606029

Epoch: 6| Step: 1
Training loss: 2.426597863189579
Validation loss: 2.5270301588920927

Epoch: 6| Step: 2
Training loss: 2.6755767521365343
Validation loss: 2.5228460856948676

Epoch: 6| Step: 3
Training loss: 2.515643860171682
Validation loss: 2.5296318941875198

Epoch: 6| Step: 4
Training loss: 2.0067025170445274
Validation loss: 2.5226423272419796

Epoch: 6| Step: 5
Training loss: 2.34065856502083
Validation loss: 2.518894716432854

Epoch: 6| Step: 6
Training loss: 2.748790474975745
Validation loss: 2.5014945966728432

Epoch: 6| Step: 7
Training loss: 2.616536255844626
Validation loss: 2.499031364027846

Epoch: 6| Step: 8
Training loss: 2.0999150395236885
Validation loss: 2.486216200721396

Epoch: 6| Step: 9
Training loss: 2.750395919736196
Validation loss: 2.4907538137784817

Epoch: 6| Step: 10
Training loss: 2.7271331801981806
Validation loss: 2.4877577410129526

Epoch: 6| Step: 11
Training loss: 2.4669655253673146
Validation loss: 2.490740652043644

Epoch: 6| Step: 12
Training loss: 1.8635285731183113
Validation loss: 2.491770263950777

Epoch: 6| Step: 13
Training loss: 2.717463397189275
Validation loss: 2.488283908908447

Epoch: 189| Step: 0
Training loss: 3.058676999884758
Validation loss: 2.486169930395085

Epoch: 6| Step: 1
Training loss: 1.835221950610255
Validation loss: 2.4893008647800885

Epoch: 6| Step: 2
Training loss: 2.67465315379453
Validation loss: 2.4891828802819713

Epoch: 6| Step: 3
Training loss: 2.234237133120968
Validation loss: 2.4879691398818506

Epoch: 6| Step: 4
Training loss: 2.774634648234405
Validation loss: 2.4855096653062465

Epoch: 6| Step: 5
Training loss: 1.6039085510802265
Validation loss: 2.491467602112845

Epoch: 6| Step: 6
Training loss: 2.5618411822203035
Validation loss: 2.493276957849487

Epoch: 6| Step: 7
Training loss: 2.181674502800414
Validation loss: 2.4966031998825478

Epoch: 6| Step: 8
Training loss: 2.527011666745365
Validation loss: 2.503815743035056

Epoch: 6| Step: 9
Training loss: 2.8076903862740603
Validation loss: 2.5250084930377805

Epoch: 6| Step: 10
Training loss: 2.257374862970848
Validation loss: 2.5204309242145033

Epoch: 6| Step: 11
Training loss: 2.361797680483834
Validation loss: 2.537660557971241

Epoch: 6| Step: 12
Training loss: 2.2694496232026475
Validation loss: 2.530839533550701

Epoch: 6| Step: 13
Training loss: 2.3477377980520098
Validation loss: 2.543459577797767

Epoch: 190| Step: 0
Training loss: 2.021639699082528
Validation loss: 2.525473797602

Epoch: 6| Step: 1
Training loss: 2.14491543689846
Validation loss: 2.5357158652568685

Epoch: 6| Step: 2
Training loss: 2.1106014500942942
Validation loss: 2.5063724205705586

Epoch: 6| Step: 3
Training loss: 3.0433310218403937
Validation loss: 2.5180665798358564

Epoch: 6| Step: 4
Training loss: 2.427014023824003
Validation loss: 2.5077560909657293

Epoch: 6| Step: 5
Training loss: 2.626020142465639
Validation loss: 2.5114747082621958

Epoch: 6| Step: 6
Training loss: 1.6782042585867842
Validation loss: 2.4950121873036823

Epoch: 6| Step: 7
Training loss: 2.445207197463109
Validation loss: 2.4946906454913944

Epoch: 6| Step: 8
Training loss: 2.26253356487183
Validation loss: 2.4960631526371118

Epoch: 6| Step: 9
Training loss: 2.38427869369778
Validation loss: 2.4919892554125496

Epoch: 6| Step: 10
Training loss: 2.7195557901951353
Validation loss: 2.4849179472319407

Epoch: 6| Step: 11
Training loss: 2.6106756018478237
Validation loss: 2.487125561518057

Epoch: 6| Step: 12
Training loss: 2.3967665029785037
Validation loss: 2.481896097411689

Epoch: 6| Step: 13
Training loss: 2.868674908488381
Validation loss: 2.4845438246075613

Epoch: 191| Step: 0
Training loss: 1.9288969698652192
Validation loss: 2.4887052022139167

Epoch: 6| Step: 1
Training loss: 2.8846545446623266
Validation loss: 2.4964045896689973

Epoch: 6| Step: 2
Training loss: 2.3579705713068018
Validation loss: 2.484590069240677

Epoch: 6| Step: 3
Training loss: 2.4805229601605765
Validation loss: 2.4830733434112986

Epoch: 6| Step: 4
Training loss: 2.5048781486006777
Validation loss: 2.4830627334511273

Epoch: 6| Step: 5
Training loss: 2.5841520201723034
Validation loss: 2.4886789208733906

Epoch: 6| Step: 6
Training loss: 1.8260290406206419
Validation loss: 2.499496536422398

Epoch: 6| Step: 7
Training loss: 2.32139433741217
Validation loss: 2.4980064549878103

Epoch: 6| Step: 8
Training loss: 2.5679499658838205
Validation loss: 2.525129052442882

Epoch: 6| Step: 9
Training loss: 2.4670611048979496
Validation loss: 2.525646947060351

Epoch: 6| Step: 10
Training loss: 2.8815627451732015
Validation loss: 2.5377700881160443

Epoch: 6| Step: 11
Training loss: 2.669707908512177
Validation loss: 2.5332144033807897

Epoch: 6| Step: 12
Training loss: 2.27761771122967
Validation loss: 2.5090376415376134

Epoch: 6| Step: 13
Training loss: 1.734749143642858
Validation loss: 2.5085831009805

Epoch: 192| Step: 0
Training loss: 2.07994103494842
Validation loss: 2.501185136265699

Epoch: 6| Step: 1
Training loss: 2.0230196370384697
Validation loss: 2.502739327413272

Epoch: 6| Step: 2
Training loss: 3.21086384115635
Validation loss: 2.4924091730815583

Epoch: 6| Step: 3
Training loss: 2.547341894872009
Validation loss: 2.490594847276047

Epoch: 6| Step: 4
Training loss: 1.7337353875851524
Validation loss: 2.4801921059436616

Epoch: 6| Step: 5
Training loss: 2.5745797490258946
Validation loss: 2.479613709949323

Epoch: 6| Step: 6
Training loss: 2.929806312694958
Validation loss: 2.4861493602014875

Epoch: 6| Step: 7
Training loss: 2.4485364629808712
Validation loss: 2.4827953090756774

Epoch: 6| Step: 8
Training loss: 2.4159745727110473
Validation loss: 2.4924617843606165

Epoch: 6| Step: 9
Training loss: 2.800398072828663
Validation loss: 2.486494699528369

Epoch: 6| Step: 10
Training loss: 2.2391664575373906
Validation loss: 2.4948376125068195

Epoch: 6| Step: 11
Training loss: 1.7446922463706596
Validation loss: 2.4928531774152836

Epoch: 6| Step: 12
Training loss: 2.3487087435750724
Validation loss: 2.497613483023433

Epoch: 6| Step: 13
Training loss: 2.356651603816058
Validation loss: 2.4866832350706476

Epoch: 193| Step: 0
Training loss: 2.346944437637878
Validation loss: 2.5059722930450206

Epoch: 6| Step: 1
Training loss: 2.418988975528166
Validation loss: 2.512106051329364

Epoch: 6| Step: 2
Training loss: 2.3731706500642074
Validation loss: 2.527288783171909

Epoch: 6| Step: 3
Training loss: 2.084778996196522
Validation loss: 2.5530848399956585

Epoch: 6| Step: 4
Training loss: 2.484845194916616
Validation loss: 2.5605764069686097

Epoch: 6| Step: 5
Training loss: 2.8490047323201684
Validation loss: 2.548691848483134

Epoch: 6| Step: 6
Training loss: 2.2091941835044033
Validation loss: 2.531709794092986

Epoch: 6| Step: 7
Training loss: 2.4819654379750604
Validation loss: 2.526369279414037

Epoch: 6| Step: 8
Training loss: 2.8054514405054634
Validation loss: 2.5022900106642503

Epoch: 6| Step: 9
Training loss: 2.108241476035621
Validation loss: 2.500230619601771

Epoch: 6| Step: 10
Training loss: 3.2334560987100405
Validation loss: 2.4878631032854046

Epoch: 6| Step: 11
Training loss: 2.014953738076722
Validation loss: 2.4874163870760557

Epoch: 6| Step: 12
Training loss: 2.888912539100022
Validation loss: 2.4870531853384374

Epoch: 6| Step: 13
Training loss: 1.9293764921676257
Validation loss: 2.4860084170318943

Epoch: 194| Step: 0
Training loss: 2.519030995940364
Validation loss: 2.4947134229990513

Epoch: 6| Step: 1
Training loss: 2.502357039355273
Validation loss: 2.490011917849187

Epoch: 6| Step: 2
Training loss: 2.2553960943725575
Validation loss: 2.497560948445045

Epoch: 6| Step: 3
Training loss: 2.3784058894972726
Validation loss: 2.4910346127921423

Epoch: 6| Step: 4
Training loss: 2.903921691837054
Validation loss: 2.489306371970211

Epoch: 6| Step: 5
Training loss: 2.3659503587044077
Validation loss: 2.490062433427798

Epoch: 6| Step: 6
Training loss: 3.322968118841264
Validation loss: 2.4842509432674396

Epoch: 6| Step: 7
Training loss: 2.3819600566010077
Validation loss: 2.4889237529033754

Epoch: 6| Step: 8
Training loss: 2.2405082980825703
Validation loss: 2.4870475453401757

Epoch: 6| Step: 9
Training loss: 1.663227283611065
Validation loss: 2.5016807469431948

Epoch: 6| Step: 10
Training loss: 2.654551333145388
Validation loss: 2.51291001360487

Epoch: 6| Step: 11
Training loss: 1.9655296387457413
Validation loss: 2.5090680806803496

Epoch: 6| Step: 12
Training loss: 2.448413771348205
Validation loss: 2.5088177624015273

Epoch: 6| Step: 13
Training loss: 1.931685552277072
Validation loss: 2.5133707280585527

Epoch: 195| Step: 0
Training loss: 1.9492719562737728
Validation loss: 2.5263968830798884

Epoch: 6| Step: 1
Training loss: 2.010606654649402
Validation loss: 2.538882799880918

Epoch: 6| Step: 2
Training loss: 2.1746734593252395
Validation loss: 2.529385700408997

Epoch: 6| Step: 3
Training loss: 2.526635284413466
Validation loss: 2.5288799948788263

Epoch: 6| Step: 4
Training loss: 2.33928417717437
Validation loss: 2.5336198897752733

Epoch: 6| Step: 5
Training loss: 2.152372993505965
Validation loss: 2.522688479910223

Epoch: 6| Step: 6
Training loss: 2.141308306486419
Validation loss: 2.5175355641802426

Epoch: 6| Step: 7
Training loss: 2.768642525061621
Validation loss: 2.521621050332766

Epoch: 6| Step: 8
Training loss: 2.655685903452788
Validation loss: 2.5348459611298457

Epoch: 6| Step: 9
Training loss: 3.004885510185253
Validation loss: 2.546686208393689

Epoch: 6| Step: 10
Training loss: 2.4849326989253604
Validation loss: 2.516225154834273

Epoch: 6| Step: 11
Training loss: 2.6222281580748246
Validation loss: 2.5201735197857515

Epoch: 6| Step: 12
Training loss: 1.822857694126085
Validation loss: 2.502942531124585

Epoch: 6| Step: 13
Training loss: 2.5587823463411783
Validation loss: 2.505243508666301

Epoch: 196| Step: 0
Training loss: 3.2895571230294554
Validation loss: 2.506304310172938

Epoch: 6| Step: 1
Training loss: 2.611713754715908
Validation loss: 2.5001927301503635

Epoch: 6| Step: 2
Training loss: 2.368392788567796
Validation loss: 2.5017081147415112

Epoch: 6| Step: 3
Training loss: 2.056596804420981
Validation loss: 2.495660751268847

Epoch: 6| Step: 4
Training loss: 2.802810763604048
Validation loss: 2.49764798309864

Epoch: 6| Step: 5
Training loss: 1.9104459265988796
Validation loss: 2.496108348089363

Epoch: 6| Step: 6
Training loss: 2.3127212418580636
Validation loss: 2.488299646713783

Epoch: 6| Step: 7
Training loss: 2.388825980055534
Validation loss: 2.4972812969738944

Epoch: 6| Step: 8
Training loss: 2.6446418682724655
Validation loss: 2.5018438374426304

Epoch: 6| Step: 9
Training loss: 2.0683745905070663
Validation loss: 2.5169478861420576

Epoch: 6| Step: 10
Training loss: 1.7897979024462376
Validation loss: 2.5161441875355783

Epoch: 6| Step: 11
Training loss: 2.746602213467516
Validation loss: 2.5477170429854987

Epoch: 6| Step: 12
Training loss: 2.289142385918052
Validation loss: 2.561054814653857

Epoch: 6| Step: 13
Training loss: 2.1840426780247664
Validation loss: 2.584389706454088

Epoch: 197| Step: 0
Training loss: 2.3201176725339376
Validation loss: 2.5867628641609617

Epoch: 6| Step: 1
Training loss: 1.973652684311296
Validation loss: 2.5702191367773524

Epoch: 6| Step: 2
Training loss: 2.2005595969372997
Validation loss: 2.548109531563042

Epoch: 6| Step: 3
Training loss: 1.7831208292594452
Validation loss: 2.536713876456202

Epoch: 6| Step: 4
Training loss: 2.9097544552745744
Validation loss: 2.526606731857241

Epoch: 6| Step: 5
Training loss: 2.330593976126081
Validation loss: 2.5121038684479133

Epoch: 6| Step: 6
Training loss: 2.0902558963118967
Validation loss: 2.503251806024522

Epoch: 6| Step: 7
Training loss: 2.2701382637768606
Validation loss: 2.495073391504493

Epoch: 6| Step: 8
Training loss: 2.975692661976784
Validation loss: 2.5027053500285006

Epoch: 6| Step: 9
Training loss: 1.872499451579891
Validation loss: 2.489438365296148

Epoch: 6| Step: 10
Training loss: 2.6903135963497085
Validation loss: 2.490348863151505

Epoch: 6| Step: 11
Training loss: 2.7057204823114414
Validation loss: 2.4873481889467017

Epoch: 6| Step: 12
Training loss: 3.0427908888268087
Validation loss: 2.4933824534431097

Epoch: 6| Step: 13
Training loss: 2.435736384968732
Validation loss: 2.487860547746061

Epoch: 198| Step: 0
Training loss: 2.2038687674747077
Validation loss: 2.4877628443138513

Epoch: 6| Step: 1
Training loss: 2.071099588737762
Validation loss: 2.4928524202584956

Epoch: 6| Step: 2
Training loss: 2.7648645422316167
Validation loss: 2.48951543664855

Epoch: 6| Step: 3
Training loss: 2.367403933258798
Validation loss: 2.489736565592059

Epoch: 6| Step: 4
Training loss: 2.4464414872672835
Validation loss: 2.497462001609481

Epoch: 6| Step: 5
Training loss: 2.2845113773576107
Validation loss: 2.4914735351357313

Epoch: 6| Step: 6
Training loss: 2.2549515387832044
Validation loss: 2.4945155862359654

Epoch: 6| Step: 7
Training loss: 2.806305400072152
Validation loss: 2.508123772655155

Epoch: 6| Step: 8
Training loss: 1.9923081305389783
Validation loss: 2.4961316220749037

Epoch: 6| Step: 9
Training loss: 2.1623848151252085
Validation loss: 2.52966681368473

Epoch: 6| Step: 10
Training loss: 2.383944783326567
Validation loss: 2.5308933399243285

Epoch: 6| Step: 11
Training loss: 2.289535317396194
Validation loss: 2.5482785700628536

Epoch: 6| Step: 12
Training loss: 2.566337317436598
Validation loss: 2.5768659040975335

Epoch: 6| Step: 13
Training loss: 2.786226513687893
Validation loss: 2.563802171161134

Epoch: 199| Step: 0
Training loss: 2.7305448027621444
Validation loss: 2.5840329420120187

Epoch: 6| Step: 1
Training loss: 1.995412512463127
Validation loss: 2.5345316363816206

Epoch: 6| Step: 2
Training loss: 2.5210089083812646
Validation loss: 2.523210639322331

Epoch: 6| Step: 3
Training loss: 2.3897098958944043
Validation loss: 2.5076443466267517

Epoch: 6| Step: 4
Training loss: 2.508093702290689
Validation loss: 2.4954432762749783

Epoch: 6| Step: 5
Training loss: 2.488011032595199
Validation loss: 2.492153410643536

Epoch: 6| Step: 6
Training loss: 2.0651798755546684
Validation loss: 2.4932528842435167

Epoch: 6| Step: 7
Training loss: 1.9740672286971732
Validation loss: 2.488026548531713

Epoch: 6| Step: 8
Training loss: 1.8564924447840565
Validation loss: 2.4979460225887116

Epoch: 6| Step: 9
Training loss: 2.478940766458206
Validation loss: 2.494939020592776

Epoch: 6| Step: 10
Training loss: 2.777817826512174
Validation loss: 2.500040197049276

Epoch: 6| Step: 11
Training loss: 1.9029744882682684
Validation loss: 2.4931301857260104

Epoch: 6| Step: 12
Training loss: 2.996815262597888
Validation loss: 2.492709672170031

Epoch: 6| Step: 13
Training loss: 3.41493392053794
Validation loss: 2.495296806743132

Epoch: 200| Step: 0
Training loss: 2.885131068962936
Validation loss: 2.50976174597929

Epoch: 6| Step: 1
Training loss: 2.3380032084420987
Validation loss: 2.497006921382524

Epoch: 6| Step: 2
Training loss: 3.080030442124568
Validation loss: 2.4900601833480147

Epoch: 6| Step: 3
Training loss: 2.1101201754511973
Validation loss: 2.503371159216531

Epoch: 6| Step: 4
Training loss: 2.7034721372116426
Validation loss: 2.502987284846617

Epoch: 6| Step: 5
Training loss: 2.507452062450946
Validation loss: 2.518408410295268

Epoch: 6| Step: 6
Training loss: 2.2179566160315547
Validation loss: 2.5239662742306153

Epoch: 6| Step: 7
Training loss: 2.1035806828234445
Validation loss: 2.521480340656468

Epoch: 6| Step: 8
Training loss: 1.92155937766558
Validation loss: 2.5301189250286047

Epoch: 6| Step: 9
Training loss: 2.1450401494463494
Validation loss: 2.5250196663874345

Epoch: 6| Step: 10
Training loss: 2.0626622916440094
Validation loss: 2.51442234837695

Epoch: 6| Step: 11
Training loss: 2.3636498534211103
Validation loss: 2.519766989233998

Epoch: 6| Step: 12
Training loss: 2.7034014080929247
Validation loss: 2.50606866816346

Epoch: 6| Step: 13
Training loss: 2.0394967153961825
Validation loss: 2.5078317041345115

Testing loss: 2.0060591354415553

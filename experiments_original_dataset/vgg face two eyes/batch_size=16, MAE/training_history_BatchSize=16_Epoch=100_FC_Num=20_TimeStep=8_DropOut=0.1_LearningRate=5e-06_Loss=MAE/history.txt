Epoch: 1| Step: 0
Training loss: 5.885176658630371
Validation loss: 5.8041510581970215

Epoch: 6| Step: 1
Training loss: 5.955290794372559
Validation loss: 5.759514888127645

Epoch: 6| Step: 2
Training loss: 7.246443271636963
Validation loss: 5.723828236262004

Epoch: 6| Step: 3
Training loss: 7.35952091217041
Validation loss: 5.686937014261882

Epoch: 6| Step: 4
Training loss: 4.901911735534668
Validation loss: 5.6532886028289795

Epoch: 6| Step: 5
Training loss: 5.153148174285889
Validation loss: 5.619123299916585

Epoch: 6| Step: 6
Training loss: 6.5303120613098145
Validation loss: 5.58984621365865

Epoch: 6| Step: 7
Training loss: 5.190690994262695
Validation loss: 5.553842941919963

Epoch: 6| Step: 8
Training loss: 4.808119773864746
Validation loss: 5.522171338399251

Epoch: 6| Step: 9
Training loss: 5.482237815856934
Validation loss: 5.491343259811401

Epoch: 6| Step: 10
Training loss: 5.019281387329102
Validation loss: 5.458916823069255

Epoch: 6| Step: 11
Training loss: 5.2457966804504395
Validation loss: 5.426384846369426

Epoch: 6| Step: 12
Training loss: 5.443740367889404
Validation loss: 5.390873511632283

Epoch: 6| Step: 13
Training loss: 4.796651840209961
Validation loss: 5.354207674662272

Epoch: 2| Step: 0
Training loss: 5.451738357543945
Validation loss: 5.321689446767171

Epoch: 6| Step: 1
Training loss: 5.209628105163574
Validation loss: 5.2812527020772295

Epoch: 6| Step: 2
Training loss: 6.090374946594238
Validation loss: 5.241242249806722

Epoch: 6| Step: 3
Training loss: 5.082620620727539
Validation loss: 5.202376921971639

Epoch: 6| Step: 4
Training loss: 3.953183650970459
Validation loss: 5.158787329991658

Epoch: 6| Step: 5
Training loss: 4.00316047668457
Validation loss: 5.116998354593913

Epoch: 6| Step: 6
Training loss: 5.410041809082031
Validation loss: 5.07020115852356

Epoch: 6| Step: 7
Training loss: 4.618396759033203
Validation loss: 5.023748397827148

Epoch: 6| Step: 8
Training loss: 4.286257266998291
Validation loss: 4.978729089101155

Epoch: 6| Step: 9
Training loss: 5.471518516540527
Validation loss: 4.924075444539388

Epoch: 6| Step: 10
Training loss: 5.108031749725342
Validation loss: 4.870653390884399

Epoch: 6| Step: 11
Training loss: 5.507884979248047
Validation loss: 4.817806879679362

Epoch: 6| Step: 12
Training loss: 5.143315315246582
Validation loss: 4.762759208679199

Epoch: 6| Step: 13
Training loss: 6.016685962677002
Validation loss: 4.69948136806488

Epoch: 3| Step: 0
Training loss: 5.057024955749512
Validation loss: 4.63373867670695

Epoch: 6| Step: 1
Training loss: 4.416219234466553
Validation loss: 4.568324327468872

Epoch: 6| Step: 2
Training loss: 4.964504718780518
Validation loss: 4.5049383242925005

Epoch: 6| Step: 3
Training loss: 4.107192039489746
Validation loss: 4.4347546100616455

Epoch: 6| Step: 4
Training loss: 3.410778760910034
Validation loss: 4.354767640431722

Epoch: 6| Step: 5
Training loss: 4.504240036010742
Validation loss: 4.2817100286483765

Epoch: 6| Step: 6
Training loss: 3.8530831336975098
Validation loss: 4.200618386268616

Epoch: 6| Step: 7
Training loss: 3.2707858085632324
Validation loss: 4.116244395573934

Epoch: 6| Step: 8
Training loss: 4.373292922973633
Validation loss: 4.042519569396973

Epoch: 6| Step: 9
Training loss: 5.2501020431518555
Validation loss: 3.960559606552124

Epoch: 6| Step: 10
Training loss: 4.257888317108154
Validation loss: 3.8747712771097818

Epoch: 6| Step: 11
Training loss: 3.5109658241271973
Validation loss: 3.804911454518636

Epoch: 6| Step: 12
Training loss: 4.785566806793213
Validation loss: 3.7164196968078613

Epoch: 6| Step: 13
Training loss: 3.449601173400879
Validation loss: 3.615304787953695

Epoch: 4| Step: 0
Training loss: 3.02089786529541
Validation loss: 3.5344930489857993

Epoch: 6| Step: 1
Training loss: 3.2570691108703613
Validation loss: 3.429309527079264

Epoch: 6| Step: 2
Training loss: 2.719099998474121
Validation loss: 3.3464269638061523

Epoch: 6| Step: 3
Training loss: 3.332155704498291
Validation loss: 3.2489251693089805

Epoch: 6| Step: 4
Training loss: 3.068929672241211
Validation loss: 3.1420709689458213

Epoch: 6| Step: 5
Training loss: 3.4369382858276367
Validation loss: 3.0484891335169473

Epoch: 6| Step: 6
Training loss: 3.776792526245117
Validation loss: 2.9393606980641684

Epoch: 6| Step: 7
Training loss: 3.120598316192627
Validation loss: 2.8443769216537476

Epoch: 6| Step: 8
Training loss: 3.0063557624816895
Validation loss: 2.7399226824442544

Epoch: 6| Step: 9
Training loss: 3.6988275051116943
Validation loss: 2.630987763404846

Epoch: 6| Step: 10
Training loss: 2.2442712783813477
Validation loss: 2.5056777199109397

Epoch: 6| Step: 11
Training loss: 2.4682908058166504
Validation loss: 2.4482891162236533

Epoch: 6| Step: 12
Training loss: 2.4160571098327637
Validation loss: 2.3812204202016196

Epoch: 6| Step: 13
Training loss: 2.3193225860595703
Validation loss: 2.3294376929601035

Epoch: 5| Step: 0
Training loss: 2.831113815307617
Validation loss: 2.2685119112332663

Epoch: 6| Step: 1
Training loss: 2.0907223224639893
Validation loss: 2.235907276471456

Epoch: 6| Step: 2
Training loss: 2.7086734771728516
Validation loss: 2.184571703275045

Epoch: 6| Step: 3
Training loss: 1.8200713396072388
Validation loss: 2.1716498533884683

Epoch: 6| Step: 4
Training loss: 1.8575992584228516
Validation loss: 2.164439102013906

Epoch: 6| Step: 5
Training loss: 2.075112819671631
Validation loss: 2.1584142049153647

Epoch: 6| Step: 6
Training loss: 1.9115674495697021
Validation loss: 2.162336528301239

Epoch: 6| Step: 7
Training loss: 2.0039687156677246
Validation loss: 2.1686624884605408

Epoch: 6| Step: 8
Training loss: 1.3075132369995117
Validation loss: 2.1898401578267417

Epoch: 6| Step: 9
Training loss: 2.514972686767578
Validation loss: 2.2160613536834717

Epoch: 6| Step: 10
Training loss: 2.735079765319824
Validation loss: 2.24270361661911

Epoch: 6| Step: 11
Training loss: 2.6060781478881836
Validation loss: 2.229359189669291

Epoch: 6| Step: 12
Training loss: 2.9100513458251953
Validation loss: 2.2309104800224304

Epoch: 6| Step: 13
Training loss: 1.5757579803466797
Validation loss: 2.211921433607737

Epoch: 6| Step: 0
Training loss: 2.399991512298584
Validation loss: 2.2096111377080283

Epoch: 6| Step: 1
Training loss: 1.3849163055419922
Validation loss: 2.192998766899109

Epoch: 6| Step: 2
Training loss: 2.329035758972168
Validation loss: 2.1867720683415732

Epoch: 6| Step: 3
Training loss: 2.4083309173583984
Validation loss: 2.1845261454582214

Epoch: 6| Step: 4
Training loss: 2.2456226348876953
Validation loss: 2.153873562812805

Epoch: 6| Step: 5
Training loss: 2.2067623138427734
Validation loss: 2.151266554991404

Epoch: 6| Step: 6
Training loss: 2.131073474884033
Validation loss: 2.1453812519709268

Epoch: 6| Step: 7
Training loss: 2.3035826683044434
Validation loss: 2.132270336151123

Epoch: 6| Step: 8
Training loss: 1.550600290298462
Validation loss: 2.141643762588501

Epoch: 6| Step: 9
Training loss: 2.3752729892730713
Validation loss: 2.147732734680176

Epoch: 6| Step: 10
Training loss: 2.71835994720459
Validation loss: 2.1354872385660806

Epoch: 6| Step: 11
Training loss: 2.0131032466888428
Validation loss: 2.1422215700149536

Epoch: 6| Step: 12
Training loss: 2.322322368621826
Validation loss: 2.164780378341675

Epoch: 6| Step: 13
Training loss: 2.2605044841766357
Validation loss: 2.147937019666036

Epoch: 7| Step: 0
Training loss: 1.9863884449005127
Validation loss: 2.139996508757273

Epoch: 6| Step: 1
Training loss: 2.6620736122131348
Validation loss: 2.1504647930463157

Epoch: 6| Step: 2
Training loss: 2.6014602184295654
Validation loss: 2.1631164153416953

Epoch: 6| Step: 3
Training loss: 1.3970222473144531
Validation loss: 2.1629954179128013

Epoch: 6| Step: 4
Training loss: 2.547393321990967
Validation loss: 2.1430871287981668

Epoch: 6| Step: 5
Training loss: 1.6036646366119385
Validation loss: 2.134263575077057

Epoch: 6| Step: 6
Training loss: 2.220003843307495
Validation loss: 2.147910753885905

Epoch: 6| Step: 7
Training loss: 2.52347731590271
Validation loss: 2.1361953218777976

Epoch: 6| Step: 8
Training loss: 2.3974924087524414
Validation loss: 2.1105885108311973

Epoch: 6| Step: 9
Training loss: 2.5842041969299316
Validation loss: 2.1380682587623596

Epoch: 6| Step: 10
Training loss: 1.67771315574646
Validation loss: 2.112436294555664

Epoch: 6| Step: 11
Training loss: 2.2509217262268066
Validation loss: 2.130602260430654

Epoch: 6| Step: 12
Training loss: 1.9232162237167358
Validation loss: 2.1093324422836304

Epoch: 6| Step: 13
Training loss: 1.7094441652297974
Validation loss: 2.1166615088780723

Epoch: 8| Step: 0
Training loss: 2.5100040435791016
Validation loss: 2.1191308895746865

Epoch: 6| Step: 1
Training loss: 2.252013921737671
Validation loss: 2.115586002667745

Epoch: 6| Step: 2
Training loss: 1.912576675415039
Validation loss: 2.134535014629364

Epoch: 6| Step: 3
Training loss: 1.942712426185608
Validation loss: 2.0997617840766907

Epoch: 6| Step: 4
Training loss: 1.3607022762298584
Validation loss: 2.1225446859995523

Epoch: 6| Step: 5
Training loss: 2.078230857849121
Validation loss: 2.114899516105652

Epoch: 6| Step: 6
Training loss: 2.112028121948242
Validation loss: 2.111809770266215

Epoch: 6| Step: 7
Training loss: 2.410393238067627
Validation loss: 2.1231881380081177

Epoch: 6| Step: 8
Training loss: 1.7786357402801514
Validation loss: 2.1230502923329673

Epoch: 6| Step: 9
Training loss: 2.6638975143432617
Validation loss: 2.1147642930348716

Epoch: 6| Step: 10
Training loss: 1.9125474691390991
Validation loss: 2.1120683749516806

Epoch: 6| Step: 11
Training loss: 1.895209789276123
Validation loss: 2.1108025113741555

Epoch: 6| Step: 12
Training loss: 2.3059144020080566
Validation loss: 2.115652084350586

Epoch: 6| Step: 13
Training loss: 2.60137939453125
Validation loss: 2.0990029176076255

Epoch: 9| Step: 0
Training loss: 1.8387668132781982
Validation loss: 2.110965629418691

Epoch: 6| Step: 1
Training loss: 2.317352771759033
Validation loss: 2.124583105246226

Epoch: 6| Step: 2
Training loss: 2.442896604537964
Validation loss: 2.1236373583475747

Epoch: 6| Step: 3
Training loss: 2.565816640853882
Validation loss: 2.1176436146100364

Epoch: 6| Step: 4
Training loss: 1.8857430219650269
Validation loss: 2.1082959373792014

Epoch: 6| Step: 5
Training loss: 1.4573612213134766
Validation loss: 2.101724624633789

Epoch: 6| Step: 6
Training loss: 1.8958760499954224
Validation loss: 2.127928455670675

Epoch: 6| Step: 7
Training loss: 1.3317031860351562
Validation loss: 2.1050079663594565

Epoch: 6| Step: 8
Training loss: 3.02947998046875
Validation loss: 2.118761360645294

Epoch: 6| Step: 9
Training loss: 1.8322967290878296
Validation loss: 2.097228447596232

Epoch: 6| Step: 10
Training loss: 2.319718599319458
Validation loss: 2.1283504366874695

Epoch: 6| Step: 11
Training loss: 2.1807973384857178
Validation loss: 2.112031181653341

Epoch: 6| Step: 12
Training loss: 2.6869125366210938
Validation loss: 2.1218485832214355

Epoch: 6| Step: 13
Training loss: 2.0161118507385254
Validation loss: 2.0982250769933066

Epoch: 10| Step: 0
Training loss: 1.731508731842041
Validation loss: 2.115504721800486

Epoch: 6| Step: 1
Training loss: 1.9343655109405518
Validation loss: 2.1018545031547546

Epoch: 6| Step: 2
Training loss: 2.48746919631958
Validation loss: 2.110524515310923

Epoch: 6| Step: 3
Training loss: 2.49684476852417
Validation loss: 2.1070321003595986

Epoch: 6| Step: 4
Training loss: 2.7691965103149414
Validation loss: 2.082232574621836

Epoch: 6| Step: 5
Training loss: 2.191384792327881
Validation loss: 2.0970918933550515

Epoch: 6| Step: 6
Training loss: 1.6988022327423096
Validation loss: 2.112253745396932

Epoch: 6| Step: 7
Training loss: 2.0834767818450928
Validation loss: 2.089957118034363

Epoch: 6| Step: 8
Training loss: 2.480520248413086
Validation loss: 2.0870559016863504

Epoch: 6| Step: 9
Training loss: 2.025378465652466
Validation loss: 2.0853284994761148

Epoch: 6| Step: 10
Training loss: 2.271083116531372
Validation loss: 2.0937360922495523

Epoch: 6| Step: 11
Training loss: 1.8553028106689453
Validation loss: 2.1153623859087625

Epoch: 6| Step: 12
Training loss: 2.0117831230163574
Validation loss: 2.1028522650400796

Epoch: 6| Step: 13
Training loss: 1.3407922983169556
Validation loss: 2.0938432216644287

Epoch: 11| Step: 0
Training loss: 2.5400819778442383
Validation loss: 2.0643247763315835

Epoch: 6| Step: 1
Training loss: 1.753762125968933
Validation loss: 2.0728646914164224

Epoch: 6| Step: 2
Training loss: 1.8445082902908325
Validation loss: 2.0938971638679504

Epoch: 6| Step: 3
Training loss: 2.4931299686431885
Validation loss: 2.0974610845247903

Epoch: 6| Step: 4
Training loss: 1.9837836027145386
Validation loss: 2.1103532910346985

Epoch: 6| Step: 5
Training loss: 2.128931999206543
Validation loss: 2.125871260960897

Epoch: 6| Step: 6
Training loss: 2.0314645767211914
Validation loss: 2.1249660650889077

Epoch: 6| Step: 7
Training loss: 1.7959465980529785
Validation loss: 2.1100385387738547

Epoch: 6| Step: 8
Training loss: 2.8401544094085693
Validation loss: 2.1319976647694907

Epoch: 6| Step: 9
Training loss: 2.8265457153320312
Validation loss: 2.117703437805176

Epoch: 6| Step: 10
Training loss: 1.8713802099227905
Validation loss: 2.1124422947565713

Epoch: 6| Step: 11
Training loss: 1.8349415063858032
Validation loss: 2.0969582001368203

Epoch: 6| Step: 12
Training loss: 1.8095624446868896
Validation loss: 2.090710620085398

Epoch: 6| Step: 13
Training loss: 2.0434341430664062
Validation loss: 2.092537522315979

Epoch: 12| Step: 0
Training loss: 2.108105421066284
Validation loss: 2.085982322692871

Epoch: 6| Step: 1
Training loss: 2.20487904548645
Validation loss: 2.0683727264404297

Epoch: 6| Step: 2
Training loss: 2.0154473781585693
Validation loss: 2.0828195214271545

Epoch: 6| Step: 3
Training loss: 2.0905849933624268
Validation loss: 2.0854746301968894

Epoch: 6| Step: 4
Training loss: 1.898279070854187
Validation loss: 2.089581608772278

Epoch: 6| Step: 5
Training loss: 1.7162857055664062
Validation loss: 2.0976655085881553

Epoch: 6| Step: 6
Training loss: 2.1902236938476562
Validation loss: 2.07205323378245

Epoch: 6| Step: 7
Training loss: 2.414459705352783
Validation loss: 2.0715429186820984

Epoch: 6| Step: 8
Training loss: 2.1799092292785645
Validation loss: 2.0631166100502014

Epoch: 6| Step: 9
Training loss: 1.784226655960083
Validation loss: 2.0929102897644043

Epoch: 6| Step: 10
Training loss: 2.3411073684692383
Validation loss: 2.0716274778048196

Epoch: 6| Step: 11
Training loss: 2.293773651123047
Validation loss: 2.0963677962621055

Epoch: 6| Step: 12
Training loss: 1.9750370979309082
Validation loss: 2.090910812218984

Epoch: 6| Step: 13
Training loss: 1.8010839223861694
Validation loss: 2.0778579711914062

Epoch: 13| Step: 0
Training loss: 1.471081256866455
Validation loss: 2.0787707964579263

Epoch: 6| Step: 1
Training loss: 2.785494327545166
Validation loss: 2.1033012866973877

Epoch: 6| Step: 2
Training loss: 2.3424835205078125
Validation loss: 2.06675386428833

Epoch: 6| Step: 3
Training loss: 1.684017300605774
Validation loss: 2.0911946495374045

Epoch: 6| Step: 4
Training loss: 2.135922908782959
Validation loss: 2.099427282810211

Epoch: 6| Step: 5
Training loss: 2.523303985595703
Validation loss: 2.0997937321662903

Epoch: 6| Step: 6
Training loss: 1.72662353515625
Validation loss: 2.063992520173391

Epoch: 6| Step: 7
Training loss: 1.5220487117767334
Validation loss: 2.0938311020533242

Epoch: 6| Step: 8
Training loss: 2.76763916015625
Validation loss: 2.064244270324707

Epoch: 6| Step: 9
Training loss: 1.7625069618225098
Validation loss: 2.0748382409413657

Epoch: 6| Step: 10
Training loss: 2.2174572944641113
Validation loss: 2.08506449063619

Epoch: 6| Step: 11
Training loss: 2.3753397464752197
Validation loss: 2.0645440816879272

Epoch: 6| Step: 12
Training loss: 1.7888566255569458
Validation loss: 2.0832126140594482

Epoch: 6| Step: 13
Training loss: 2.0650625228881836
Validation loss: 2.0707935293515525

Epoch: 14| Step: 0
Training loss: 1.76548171043396
Validation loss: 2.071892261505127

Epoch: 6| Step: 1
Training loss: 2.2055039405822754
Validation loss: 2.058699985345205

Epoch: 6| Step: 2
Training loss: 2.7854044437408447
Validation loss: 2.0845436255137124

Epoch: 6| Step: 3
Training loss: 1.7493170499801636
Validation loss: 2.1008691589037576

Epoch: 6| Step: 4
Training loss: 1.9554840326309204
Validation loss: 2.087063431739807

Epoch: 6| Step: 5
Training loss: 2.64571475982666
Validation loss: 2.10044531027476

Epoch: 6| Step: 6
Training loss: 2.229228973388672
Validation loss: 2.0535966555277505

Epoch: 6| Step: 7
Training loss: 1.8311989307403564
Validation loss: 2.0716168681780496

Epoch: 6| Step: 8
Training loss: 2.127148151397705
Validation loss: 2.098040541013082

Epoch: 6| Step: 9
Training loss: 2.015169382095337
Validation loss: 2.0876731077829995

Epoch: 6| Step: 10
Training loss: 2.198115110397339
Validation loss: 2.0710192918777466

Epoch: 6| Step: 11
Training loss: 2.2577147483825684
Validation loss: 2.087811589241028

Epoch: 6| Step: 12
Training loss: 1.3816778659820557
Validation loss: 2.0768957138061523

Epoch: 6| Step: 13
Training loss: 2.0859062671661377
Validation loss: 2.086039423942566

Epoch: 15| Step: 0
Training loss: 2.271477699279785
Validation loss: 2.0880375703175864

Epoch: 6| Step: 1
Training loss: 1.9501993656158447
Validation loss: 2.0674673318862915

Epoch: 6| Step: 2
Training loss: 2.3209500312805176
Validation loss: 2.052313268184662

Epoch: 6| Step: 3
Training loss: 1.4808186292648315
Validation loss: 2.0509019692738852

Epoch: 6| Step: 4
Training loss: 2.070908308029175
Validation loss: 2.07272070646286

Epoch: 6| Step: 5
Training loss: 1.6104278564453125
Validation loss: 2.056452473004659

Epoch: 6| Step: 6
Training loss: 2.2189078330993652
Validation loss: 2.079414447148641

Epoch: 6| Step: 7
Training loss: 2.84953236579895
Validation loss: 2.1138750513394675

Epoch: 6| Step: 8
Training loss: 1.9866199493408203
Validation loss: 2.1106725136439004

Epoch: 6| Step: 9
Training loss: 2.387805938720703
Validation loss: 2.094141662120819

Epoch: 6| Step: 10
Training loss: 1.6008977890014648
Validation loss: 2.082876125971476

Epoch: 6| Step: 11
Training loss: 2.3894214630126953
Validation loss: 2.084537843863169

Epoch: 6| Step: 12
Training loss: 1.8031713962554932
Validation loss: 2.07793653011322

Epoch: 6| Step: 13
Training loss: 2.0176548957824707
Validation loss: 2.0573275486628213

Epoch: 16| Step: 0
Training loss: 1.840605616569519
Validation loss: 2.0540054639180503

Epoch: 6| Step: 1
Training loss: 2.3066976070404053
Validation loss: 2.068931778271993

Epoch: 6| Step: 2
Training loss: 2.066599130630493
Validation loss: 2.055804987748464

Epoch: 6| Step: 3
Training loss: 2.0254087448120117
Validation loss: 2.0438687403996787

Epoch: 6| Step: 4
Training loss: 1.2214654684066772
Validation loss: 2.051197111606598

Epoch: 6| Step: 5
Training loss: 2.3766679763793945
Validation loss: 2.048299868901571

Epoch: 6| Step: 6
Training loss: 2.4176695346832275
Validation loss: 2.0609874725341797

Epoch: 6| Step: 7
Training loss: 1.8725104331970215
Validation loss: 2.0698580344518027

Epoch: 6| Step: 8
Training loss: 1.8011958599090576
Validation loss: 2.085546910762787

Epoch: 6| Step: 9
Training loss: 1.9117376804351807
Validation loss: 2.0869310895601907

Epoch: 6| Step: 10
Training loss: 2.3474979400634766
Validation loss: 2.076291084289551

Epoch: 6| Step: 11
Training loss: 1.7592594623565674
Validation loss: 2.0399457613627114

Epoch: 6| Step: 12
Training loss: 2.26247239112854
Validation loss: 2.0620341499646506

Epoch: 6| Step: 13
Training loss: 2.196992874145508
Validation loss: 2.063081383705139

Epoch: 17| Step: 0
Training loss: 2.2080512046813965
Validation loss: 2.0623825589815774

Epoch: 6| Step: 1
Training loss: 2.5184736251831055
Validation loss: 2.0723090966542563

Epoch: 6| Step: 2
Training loss: 2.790098190307617
Validation loss: 2.0737030307451882

Epoch: 6| Step: 3
Training loss: 1.9168728590011597
Validation loss: 2.0702477296193442

Epoch: 6| Step: 4
Training loss: 1.7818092107772827
Validation loss: 2.05542262395223

Epoch: 6| Step: 5
Training loss: 1.232942819595337
Validation loss: 2.0763535102208457

Epoch: 6| Step: 6
Training loss: 2.3316235542297363
Validation loss: 2.044360876083374

Epoch: 6| Step: 7
Training loss: 2.264106273651123
Validation loss: 2.0766544540723166

Epoch: 6| Step: 8
Training loss: 1.7778263092041016
Validation loss: 2.0536757111549377

Epoch: 6| Step: 9
Training loss: 2.5190553665161133
Validation loss: 2.0952874620755515

Epoch: 6| Step: 10
Training loss: 1.7938814163208008
Validation loss: 2.0812023083368936

Epoch: 6| Step: 11
Training loss: 2.1462316513061523
Validation loss: 2.04966010649999

Epoch: 6| Step: 12
Training loss: 1.692420482635498
Validation loss: 2.0647276441256204

Epoch: 6| Step: 13
Training loss: 1.7907636165618896
Validation loss: 2.0639547308286033

Epoch: 18| Step: 0
Training loss: 1.7818667888641357
Validation loss: 2.078740199406942

Epoch: 6| Step: 1
Training loss: 2.099475860595703
Validation loss: 2.0595974723498025

Epoch: 6| Step: 2
Training loss: 1.5973773002624512
Validation loss: 2.0333690841992698

Epoch: 6| Step: 3
Training loss: 2.774148941040039
Validation loss: 2.0667837063471475

Epoch: 6| Step: 4
Training loss: 1.8928803205490112
Validation loss: 2.084963083267212

Epoch: 6| Step: 5
Training loss: 1.2680668830871582
Validation loss: 2.06599889198939

Epoch: 6| Step: 6
Training loss: 2.143129587173462
Validation loss: 2.053621550401052

Epoch: 6| Step: 7
Training loss: 2.5992965698242188
Validation loss: 2.0448538661003113

Epoch: 6| Step: 8
Training loss: 1.8986291885375977
Validation loss: 2.0641372203826904

Epoch: 6| Step: 9
Training loss: 2.2680461406707764
Validation loss: 2.047553221384684

Epoch: 6| Step: 10
Training loss: 1.4883484840393066
Validation loss: 2.0795161922772727

Epoch: 6| Step: 11
Training loss: 2.5684502124786377
Validation loss: 2.0587992072105408

Epoch: 6| Step: 12
Training loss: 1.7302219867706299
Validation loss: 2.0444942911465964

Epoch: 6| Step: 13
Training loss: 2.158703565597534
Validation loss: 2.0408472220102944

Epoch: 19| Step: 0
Training loss: 2.438467502593994
Validation loss: 2.063077449798584

Epoch: 6| Step: 1
Training loss: 2.9270682334899902
Validation loss: 2.073211352030436

Epoch: 6| Step: 2
Training loss: 1.7521812915802002
Validation loss: 2.0571333964665732

Epoch: 6| Step: 3
Training loss: 2.4778759479522705
Validation loss: 2.0461604595184326

Epoch: 6| Step: 4
Training loss: 1.5359914302825928
Validation loss: 2.036138574282328

Epoch: 6| Step: 5
Training loss: 2.057335376739502
Validation loss: 2.052816331386566

Epoch: 6| Step: 6
Training loss: 1.5307724475860596
Validation loss: 2.064505179723104

Epoch: 6| Step: 7
Training loss: 1.8098711967468262
Validation loss: 2.074667274951935

Epoch: 6| Step: 8
Training loss: 2.4031972885131836
Validation loss: 2.0684136748313904

Epoch: 6| Step: 9
Training loss: 2.21450138092041
Validation loss: 2.057894210020701

Epoch: 6| Step: 10
Training loss: 2.2785756587982178
Validation loss: 2.020033518473307

Epoch: 6| Step: 11
Training loss: 1.6904325485229492
Validation loss: 2.065425912539164

Epoch: 6| Step: 12
Training loss: 1.6015400886535645
Validation loss: 2.0537760257720947

Epoch: 6| Step: 13
Training loss: 1.4950662851333618
Validation loss: 2.0614097913106284

Epoch: 20| Step: 0
Training loss: 2.7350075244903564
Validation loss: 2.062597692012787

Epoch: 6| Step: 1
Training loss: 1.4211822748184204
Validation loss: 2.0818756818771362

Epoch: 6| Step: 2
Training loss: 1.6222572326660156
Validation loss: 2.049562454223633

Epoch: 6| Step: 3
Training loss: 1.9078643321990967
Validation loss: 2.060771028200785

Epoch: 6| Step: 4
Training loss: 1.5285348892211914
Validation loss: 2.0588208039601645

Epoch: 6| Step: 5
Training loss: 3.2126049995422363
Validation loss: 2.056289811929067

Epoch: 6| Step: 6
Training loss: 1.5828653573989868
Validation loss: 2.058946351210276

Epoch: 6| Step: 7
Training loss: 1.9434788227081299
Validation loss: 2.0740070939064026

Epoch: 6| Step: 8
Training loss: 2.4156103134155273
Validation loss: 2.039462149143219

Epoch: 6| Step: 9
Training loss: 1.8337793350219727
Validation loss: 2.073081076145172

Epoch: 6| Step: 10
Training loss: 2.1112406253814697
Validation loss: 2.0578657388687134

Epoch: 6| Step: 11
Training loss: 1.5709121227264404
Validation loss: 2.053778886795044

Epoch: 6| Step: 12
Training loss: 2.1636292934417725
Validation loss: 2.0443325440088906

Epoch: 6| Step: 13
Training loss: 2.144165515899658
Validation loss: 2.059367060661316

Epoch: 21| Step: 0
Training loss: 1.9204530715942383
Validation loss: 2.069307049115499

Epoch: 6| Step: 1
Training loss: 1.9574203491210938
Validation loss: 2.0653584599494934

Epoch: 6| Step: 2
Training loss: 2.5613720417022705
Validation loss: 2.0337711771329245

Epoch: 6| Step: 3
Training loss: 1.8055436611175537
Validation loss: 2.022325952847799

Epoch: 6| Step: 4
Training loss: 2.54227876663208
Validation loss: 2.055499871571859

Epoch: 6| Step: 5
Training loss: 1.5246214866638184
Validation loss: 2.0373656948407493

Epoch: 6| Step: 6
Training loss: 1.78774094581604
Validation loss: 2.0397220253944397

Epoch: 6| Step: 7
Training loss: 1.9844553470611572
Validation loss: 2.044443686803182

Epoch: 6| Step: 8
Training loss: 1.6163767576217651
Validation loss: 2.0318276484807334

Epoch: 6| Step: 9
Training loss: 2.094169855117798
Validation loss: 2.057568887869517

Epoch: 6| Step: 10
Training loss: 1.4144387245178223
Validation loss: 2.046635091304779

Epoch: 6| Step: 11
Training loss: 2.126316547393799
Validation loss: 2.0311666131019592

Epoch: 6| Step: 12
Training loss: 2.1209449768066406
Validation loss: 2.0469296177228293

Epoch: 6| Step: 13
Training loss: 2.420382022857666
Validation loss: 2.0866085290908813

Epoch: 22| Step: 0
Training loss: 2.7314817905426025
Validation loss: 2.060758133729299

Epoch: 6| Step: 1
Training loss: 1.7952584028244019
Validation loss: 2.0657227635383606

Epoch: 6| Step: 2
Training loss: 1.8606839179992676
Validation loss: 2.0739429791768393

Epoch: 6| Step: 3
Training loss: 1.8135756254196167
Validation loss: 2.0533276796340942

Epoch: 6| Step: 4
Training loss: 1.9281829595565796
Validation loss: 2.054235061009725

Epoch: 6| Step: 5
Training loss: 2.378837823867798
Validation loss: 2.069422165552775

Epoch: 6| Step: 6
Training loss: 1.9323749542236328
Validation loss: 2.079762279987335

Epoch: 6| Step: 7
Training loss: 1.5819528102874756
Validation loss: 2.0529944896698

Epoch: 6| Step: 8
Training loss: 2.154299259185791
Validation loss: 2.055932859579722

Epoch: 6| Step: 9
Training loss: 1.7720128297805786
Validation loss: 2.0737141768137612

Epoch: 6| Step: 10
Training loss: 2.1191115379333496
Validation loss: 2.0476205746332803

Epoch: 6| Step: 11
Training loss: 2.4320554733276367
Validation loss: 2.0332016746203103

Epoch: 6| Step: 12
Training loss: 1.8831689357757568
Validation loss: 2.063443958759308

Epoch: 6| Step: 13
Training loss: 1.9398081302642822
Validation loss: 2.0664309660593667

Epoch: 23| Step: 0
Training loss: 2.1224207878112793
Validation loss: 2.0438266595204673

Epoch: 6| Step: 1
Training loss: 2.135000705718994
Validation loss: 2.0521169900894165

Epoch: 6| Step: 2
Training loss: 1.8655214309692383
Validation loss: 2.0476107597351074

Epoch: 6| Step: 3
Training loss: 1.6023632287979126
Validation loss: 2.054279386997223

Epoch: 6| Step: 4
Training loss: 1.7061398029327393
Validation loss: 2.052391012509664

Epoch: 6| Step: 5
Training loss: 2.0345242023468018
Validation loss: 2.0630586743354797

Epoch: 6| Step: 6
Training loss: 2.713273048400879
Validation loss: 2.0568814277648926

Epoch: 6| Step: 7
Training loss: 1.6811422109603882
Validation loss: 2.059949735800425

Epoch: 6| Step: 8
Training loss: 1.5814578533172607
Validation loss: 2.049977640310923

Epoch: 6| Step: 9
Training loss: 2.4344749450683594
Validation loss: 2.038682281970978

Epoch: 6| Step: 10
Training loss: 2.0041160583496094
Validation loss: 2.0524034897486367

Epoch: 6| Step: 11
Training loss: 2.052561044692993
Validation loss: 2.0611628691355386

Epoch: 6| Step: 12
Training loss: 1.412359356880188
Validation loss: 2.0447272062301636

Epoch: 6| Step: 13
Training loss: 2.5620851516723633
Validation loss: 2.055208404858907

Epoch: 24| Step: 0
Training loss: 2.1383726596832275
Validation loss: 2.0498253305753074

Epoch: 6| Step: 1
Training loss: 2.375741958618164
Validation loss: 2.064169685045878

Epoch: 6| Step: 2
Training loss: 1.6632864475250244
Validation loss: 2.029847880204519

Epoch: 6| Step: 3
Training loss: 1.5043745040893555
Validation loss: 2.036161422729492

Epoch: 6| Step: 4
Training loss: 2.1284372806549072
Validation loss: 2.052886446317037

Epoch: 6| Step: 5
Training loss: 2.5922610759735107
Validation loss: 2.0751689871152244

Epoch: 6| Step: 6
Training loss: 2.418163299560547
Validation loss: 2.065232813358307

Epoch: 6| Step: 7
Training loss: 1.9769012928009033
Validation loss: 2.068608283996582

Epoch: 6| Step: 8
Training loss: 1.9663147926330566
Validation loss: 2.07817014058431

Epoch: 6| Step: 9
Training loss: 2.6096138954162598
Validation loss: 2.0460708340009055

Epoch: 6| Step: 10
Training loss: 1.6186193227767944
Validation loss: 2.053495764732361

Epoch: 6| Step: 11
Training loss: 1.5691773891448975
Validation loss: 2.0504124561945596

Epoch: 6| Step: 12
Training loss: 1.5117161273956299
Validation loss: 2.0669536193211875

Epoch: 6| Step: 13
Training loss: 1.7590672969818115
Validation loss: 2.0611777305603027

Epoch: 25| Step: 0
Training loss: 2.402672290802002
Validation loss: 2.072898288567861

Epoch: 6| Step: 1
Training loss: 1.5235273838043213
Validation loss: 2.0411774118741355

Epoch: 6| Step: 2
Training loss: 2.24228835105896
Validation loss: 2.054148316383362

Epoch: 6| Step: 3
Training loss: 2.2286272048950195
Validation loss: 2.044832726319631

Epoch: 6| Step: 4
Training loss: 2.215569019317627
Validation loss: 2.036903222401937

Epoch: 6| Step: 5
Training loss: 1.9433581829071045
Validation loss: 2.034462889035543

Epoch: 6| Step: 6
Training loss: 1.4024925231933594
Validation loss: 2.0480433106422424

Epoch: 6| Step: 7
Training loss: 1.6714386940002441
Validation loss: 2.0321063796679177

Epoch: 6| Step: 8
Training loss: 1.9919166564941406
Validation loss: 2.025187393029531

Epoch: 6| Step: 9
Training loss: 2.2434091567993164
Validation loss: 2.0537630120913186

Epoch: 6| Step: 10
Training loss: 2.6858434677124023
Validation loss: 2.0387186805407205

Epoch: 6| Step: 11
Training loss: 1.6858590841293335
Validation loss: 2.033999522527059

Epoch: 6| Step: 12
Training loss: 1.698756217956543
Validation loss: 2.018951336542765

Epoch: 6| Step: 13
Training loss: 1.9909892082214355
Validation loss: 2.04861581325531

Epoch: 26| Step: 0
Training loss: 2.503661870956421
Validation loss: 2.0512530406316123

Epoch: 6| Step: 1
Training loss: 2.168715476989746
Validation loss: 2.042440871397654

Epoch: 6| Step: 2
Training loss: 1.9611976146697998
Validation loss: 2.0863768458366394

Epoch: 6| Step: 3
Training loss: 1.863912582397461
Validation loss: 2.049382825692495

Epoch: 6| Step: 4
Training loss: 1.5546462535858154
Validation loss: 2.099242071310679

Epoch: 6| Step: 5
Training loss: 1.7097716331481934
Validation loss: 2.073429445425669

Epoch: 6| Step: 6
Training loss: 1.3575761318206787
Validation loss: 2.0815968116124473

Epoch: 6| Step: 7
Training loss: 2.581850051879883
Validation loss: 2.09162837266922

Epoch: 6| Step: 8
Training loss: 1.927497386932373
Validation loss: 2.0713623563448587

Epoch: 6| Step: 9
Training loss: 1.5715925693511963
Validation loss: 2.041773716608683

Epoch: 6| Step: 10
Training loss: 2.546334743499756
Validation loss: 2.0436996817588806

Epoch: 6| Step: 11
Training loss: 1.9256865978240967
Validation loss: 2.032010873158773

Epoch: 6| Step: 12
Training loss: 2.1005425453186035
Validation loss: 2.029703358809153

Epoch: 6| Step: 13
Training loss: 1.698403239250183
Validation loss: 2.0402878522872925

Epoch: 27| Step: 0
Training loss: 2.071697235107422
Validation loss: 2.0304357608159385

Epoch: 6| Step: 1
Training loss: 1.5154156684875488
Validation loss: 2.0516262650489807

Epoch: 6| Step: 2
Training loss: 1.721144437789917
Validation loss: 2.088126222292582

Epoch: 6| Step: 3
Training loss: 2.4125030040740967
Validation loss: 2.0566585659980774

Epoch: 6| Step: 4
Training loss: 1.808472752571106
Validation loss: 2.0612707138061523

Epoch: 6| Step: 5
Training loss: 1.5869754552841187
Validation loss: 2.048733651638031

Epoch: 6| Step: 6
Training loss: 1.5335510969161987
Validation loss: 2.06534077723821

Epoch: 6| Step: 7
Training loss: 2.1601314544677734
Validation loss: 2.047548154989878

Epoch: 6| Step: 8
Training loss: 2.129593849182129
Validation loss: 2.0477198362350464

Epoch: 6| Step: 9
Training loss: 2.6670022010803223
Validation loss: 2.034566561381022

Epoch: 6| Step: 10
Training loss: 2.2957053184509277
Validation loss: 2.0589475631713867

Epoch: 6| Step: 11
Training loss: 2.215956211090088
Validation loss: 2.0289875268936157

Epoch: 6| Step: 12
Training loss: 2.2922534942626953
Validation loss: 2.0322517355283103

Epoch: 6| Step: 13
Training loss: 1.6808133125305176
Validation loss: 2.0383641918500266

Epoch: 28| Step: 0
Training loss: 1.6819169521331787
Validation loss: 2.0453829566637673

Epoch: 6| Step: 1
Training loss: 1.595341444015503
Validation loss: 2.072825094064077

Epoch: 6| Step: 2
Training loss: 2.2473673820495605
Validation loss: 2.08203132947286

Epoch: 6| Step: 3
Training loss: 1.3465828895568848
Validation loss: 2.082276145617167

Epoch: 6| Step: 4
Training loss: 2.5382986068725586
Validation loss: 2.1003210743268332

Epoch: 6| Step: 5
Training loss: 2.361537218093872
Validation loss: 2.100158472855886

Epoch: 6| Step: 6
Training loss: 2.5670056343078613
Validation loss: 2.0847408374150596

Epoch: 6| Step: 7
Training loss: 1.398866891860962
Validation loss: 2.0676129261652627

Epoch: 6| Step: 8
Training loss: 1.766134262084961
Validation loss: 2.0649172266324363

Epoch: 6| Step: 9
Training loss: 2.0337162017822266
Validation loss: 2.051175673802694

Epoch: 6| Step: 10
Training loss: 1.7197089195251465
Validation loss: 2.0540053645769754

Epoch: 6| Step: 11
Training loss: 1.795649528503418
Validation loss: 2.0264297127723694

Epoch: 6| Step: 12
Training loss: 2.1510415077209473
Validation loss: 2.0338624517122903

Epoch: 6| Step: 13
Training loss: 2.333220958709717
Validation loss: 2.061832288901011

Epoch: 29| Step: 0
Training loss: 1.8183010816574097
Validation loss: 2.0429036219914756

Epoch: 6| Step: 1
Training loss: 2.079615592956543
Validation loss: 2.0273592869440713

Epoch: 6| Step: 2
Training loss: 2.0023539066314697
Validation loss: 2.0423851807912192

Epoch: 6| Step: 3
Training loss: 2.00911283493042
Validation loss: 2.033107260862986

Epoch: 6| Step: 4
Training loss: 2.351158618927002
Validation loss: 2.066879073778788

Epoch: 6| Step: 5
Training loss: 2.3381824493408203
Validation loss: 2.0723470052083335

Epoch: 6| Step: 6
Training loss: 1.7340497970581055
Validation loss: 2.0310840010643005

Epoch: 6| Step: 7
Training loss: 2.035759449005127
Validation loss: 2.0721824963887534

Epoch: 6| Step: 8
Training loss: 1.8581984043121338
Validation loss: 2.017391880353292

Epoch: 6| Step: 9
Training loss: 2.2825143337249756
Validation loss: 2.01581480105718

Epoch: 6| Step: 10
Training loss: 1.7331041097640991
Validation loss: 2.030261814594269

Epoch: 6| Step: 11
Training loss: 1.7815483808517456
Validation loss: 2.0744261344273887

Epoch: 6| Step: 12
Training loss: 1.9507472515106201
Validation loss: 2.1046836574872336

Epoch: 6| Step: 13
Training loss: 1.8127230405807495
Validation loss: 2.091620703538259

Epoch: 30| Step: 0
Training loss: 1.9961671829223633
Validation loss: 2.115026573340098

Epoch: 6| Step: 1
Training loss: 2.4869980812072754
Validation loss: 2.1064876715342202

Epoch: 6| Step: 2
Training loss: 2.345005989074707
Validation loss: 2.084722101688385

Epoch: 6| Step: 3
Training loss: 1.886308193206787
Validation loss: 2.1009945472081504

Epoch: 6| Step: 4
Training loss: 1.4422019720077515
Validation loss: 2.094468037287394

Epoch: 6| Step: 5
Training loss: 2.030574321746826
Validation loss: 2.084723432858785

Epoch: 6| Step: 6
Training loss: 1.613937497138977
Validation loss: 2.0470245281855264

Epoch: 6| Step: 7
Training loss: 1.971261978149414
Validation loss: 2.040465156237284

Epoch: 6| Step: 8
Training loss: 1.9144127368927002
Validation loss: 2.054544727007548

Epoch: 6| Step: 9
Training loss: 1.831345796585083
Validation loss: 2.0398208697636924

Epoch: 6| Step: 10
Training loss: 2.249329090118408
Validation loss: 2.0430052876472473

Epoch: 6| Step: 11
Training loss: 2.890589475631714
Validation loss: 2.020677387714386

Epoch: 6| Step: 12
Training loss: 1.8639353513717651
Validation loss: 2.0405646363894143

Epoch: 6| Step: 13
Training loss: 1.3096952438354492
Validation loss: 2.0551430781682334

Epoch: 31| Step: 0
Training loss: 2.4028987884521484
Validation loss: 2.0426674087842307

Epoch: 6| Step: 1
Training loss: 2.072023868560791
Validation loss: 2.057985703150431

Epoch: 6| Step: 2
Training loss: 1.2419407367706299
Validation loss: 2.032677193482717

Epoch: 6| Step: 3
Training loss: 1.2600579261779785
Validation loss: 2.007740239302317

Epoch: 6| Step: 4
Training loss: 1.8467390537261963
Validation loss: 2.064454118410746

Epoch: 6| Step: 5
Training loss: 2.2936692237854004
Validation loss: 2.0250491301218667

Epoch: 6| Step: 6
Training loss: 1.3884928226470947
Validation loss: 2.051877578099569

Epoch: 6| Step: 7
Training loss: 2.1735267639160156
Validation loss: 2.0252421498298645

Epoch: 6| Step: 8
Training loss: 2.2197909355163574
Validation loss: 2.044531007607778

Epoch: 6| Step: 9
Training loss: 1.5270535945892334
Validation loss: 2.0308863719304404

Epoch: 6| Step: 10
Training loss: 2.2929837703704834
Validation loss: 2.057921548684438

Epoch: 6| Step: 11
Training loss: 2.3566441535949707
Validation loss: 2.0662153561909995

Epoch: 6| Step: 12
Training loss: 2.2326040267944336
Validation loss: 2.0672537883122764

Epoch: 6| Step: 13
Training loss: 1.9078009128570557
Validation loss: 2.0810242096583047

Epoch: 32| Step: 0
Training loss: 2.1883456707000732
Validation loss: 2.102556824684143

Epoch: 6| Step: 1
Training loss: 2.0028321743011475
Validation loss: 2.10125341018041

Epoch: 6| Step: 2
Training loss: 2.1634840965270996
Validation loss: 2.092240333557129

Epoch: 6| Step: 3
Training loss: 1.0083601474761963
Validation loss: 2.090574105580648

Epoch: 6| Step: 4
Training loss: 1.371577262878418
Validation loss: 2.0887861251831055

Epoch: 6| Step: 5
Training loss: 2.4528441429138184
Validation loss: 2.085460285345713

Epoch: 6| Step: 6
Training loss: 2.879894256591797
Validation loss: 2.09384157260259

Epoch: 6| Step: 7
Training loss: 1.1215547323226929
Validation loss: 2.0387336015701294

Epoch: 6| Step: 8
Training loss: 1.9895490407943726
Validation loss: 2.0440340439478555

Epoch: 6| Step: 9
Training loss: 2.536405086517334
Validation loss: 2.0482861200968423

Epoch: 6| Step: 10
Training loss: 2.313844680786133
Validation loss: 2.013948162396749

Epoch: 6| Step: 11
Training loss: 1.8092598915100098
Validation loss: 2.0340028007825217

Epoch: 6| Step: 12
Training loss: 1.3073300123214722
Validation loss: 2.0544704596201577

Epoch: 6| Step: 13
Training loss: 2.2478444576263428
Validation loss: 2.011369287967682

Epoch: 33| Step: 0
Training loss: 1.4966378211975098
Validation loss: 2.053988059361776

Epoch: 6| Step: 1
Training loss: 2.8438339233398438
Validation loss: 2.0230417450269065

Epoch: 6| Step: 2
Training loss: 2.556361198425293
Validation loss: 2.023716370264689

Epoch: 6| Step: 3
Training loss: 1.636866569519043
Validation loss: 2.037639339764913

Epoch: 6| Step: 4
Training loss: 1.2286393642425537
Validation loss: 2.0422518650690713

Epoch: 6| Step: 5
Training loss: 2.727522373199463
Validation loss: 2.0471649368604026

Epoch: 6| Step: 6
Training loss: 1.6243503093719482
Validation loss: 2.0410568714141846

Epoch: 6| Step: 7
Training loss: 1.9505690336227417
Validation loss: 2.0431471268335977

Epoch: 6| Step: 8
Training loss: 1.7408450841903687
Validation loss: 2.0262880325317383

Epoch: 6| Step: 9
Training loss: 1.8237484693527222
Validation loss: 2.0578748186429343

Epoch: 6| Step: 10
Training loss: 1.9751795530319214
Validation loss: 2.052682956059774

Epoch: 6| Step: 11
Training loss: 1.5027101039886475
Validation loss: 2.0628333687782288

Epoch: 6| Step: 12
Training loss: 1.4575145244598389
Validation loss: 2.0709495147069297

Epoch: 6| Step: 13
Training loss: 2.4656496047973633
Validation loss: 2.0359196265538535

Epoch: 34| Step: 0
Training loss: 1.642621636390686
Validation loss: 2.10495525598526

Epoch: 6| Step: 1
Training loss: 2.2362895011901855
Validation loss: 2.1057123939196267

Epoch: 6| Step: 2
Training loss: 1.9296923875808716
Validation loss: 2.088096042474111

Epoch: 6| Step: 3
Training loss: 2.4936296939849854
Validation loss: 2.0648253758748374

Epoch: 6| Step: 4
Training loss: 1.5443607568740845
Validation loss: 2.097529669602712

Epoch: 6| Step: 5
Training loss: 2.097010612487793
Validation loss: 2.0869051416714988

Epoch: 6| Step: 6
Training loss: 1.6328705549240112
Validation loss: 2.067677696545919

Epoch: 6| Step: 7
Training loss: 1.9591134786605835
Validation loss: 2.047094225883484

Epoch: 6| Step: 8
Training loss: 2.3600425720214844
Validation loss: 2.077411333719889

Epoch: 6| Step: 9
Training loss: 1.7394808530807495
Validation loss: 2.043034474054972

Epoch: 6| Step: 10
Training loss: 1.8841052055358887
Validation loss: 2.040896415710449

Epoch: 6| Step: 11
Training loss: 2.6555709838867188
Validation loss: 2.0274192293485007

Epoch: 6| Step: 12
Training loss: 1.3575680255889893
Validation loss: 2.0142747163772583

Epoch: 6| Step: 13
Training loss: 1.7203526496887207
Validation loss: 2.0203106800715127

Epoch: 35| Step: 0
Training loss: 1.217280387878418
Validation loss: 2.0304372111956277

Epoch: 6| Step: 1
Training loss: 1.5966812372207642
Validation loss: 2.0287785728772483

Epoch: 6| Step: 2
Training loss: 2.1807069778442383
Validation loss: 2.0385401248931885

Epoch: 6| Step: 3
Training loss: 2.2895777225494385
Validation loss: 2.0191736419995627

Epoch: 6| Step: 4
Training loss: 1.9964426755905151
Validation loss: 2.03491340080897

Epoch: 6| Step: 5
Training loss: 1.9568285942077637
Validation loss: 2.0359882712364197

Epoch: 6| Step: 6
Training loss: 1.7995771169662476
Validation loss: 2.0263272523880005

Epoch: 6| Step: 7
Training loss: 2.3823065757751465
Validation loss: 2.089802106221517

Epoch: 6| Step: 8
Training loss: 2.0465455055236816
Validation loss: 2.054036299387614

Epoch: 6| Step: 9
Training loss: 2.2981033325195312
Validation loss: 2.0307780305544534

Epoch: 6| Step: 10
Training loss: 2.0748794078826904
Validation loss: 2.048491358757019

Epoch: 6| Step: 11
Training loss: 1.6779491901397705
Validation loss: 2.0171398917833963

Epoch: 6| Step: 12
Training loss: 1.5428876876831055
Validation loss: 2.047913451989492

Epoch: 6| Step: 13
Training loss: 2.049582004547119
Validation loss: 2.0367135802904763

Epoch: 36| Step: 0
Training loss: 1.9144930839538574
Validation loss: 2.0547660191853843

Epoch: 6| Step: 1
Training loss: 1.7963982820510864
Validation loss: 2.037828882535299

Epoch: 6| Step: 2
Training loss: 2.371122360229492
Validation loss: 2.0464872121810913

Epoch: 6| Step: 3
Training loss: 1.681213140487671
Validation loss: 2.027100125948588

Epoch: 6| Step: 4
Training loss: 1.125946283340454
Validation loss: 2.0261857906977334

Epoch: 6| Step: 5
Training loss: 1.562375783920288
Validation loss: 2.0404592752456665

Epoch: 6| Step: 6
Training loss: 2.1131019592285156
Validation loss: 2.0559382836023965

Epoch: 6| Step: 7
Training loss: 2.546151638031006
Validation loss: 2.028427322705587

Epoch: 6| Step: 8
Training loss: 1.994303584098816
Validation loss: 2.0557034413019815

Epoch: 6| Step: 9
Training loss: 1.478771686553955
Validation loss: 2.04205588499705

Epoch: 6| Step: 10
Training loss: 1.7740529775619507
Validation loss: 2.0288021167119346

Epoch: 6| Step: 11
Training loss: 2.5234429836273193
Validation loss: 2.0507755279541016

Epoch: 6| Step: 12
Training loss: 2.4872171878814697
Validation loss: 2.035828630129496

Epoch: 6| Step: 13
Training loss: 2.003134250640869
Validation loss: 2.0578758915265403

Epoch: 37| Step: 0
Training loss: 1.5919221639633179
Validation loss: 2.023696025212606

Epoch: 6| Step: 1
Training loss: 2.499494791030884
Validation loss: 2.024099071820577

Epoch: 6| Step: 2
Training loss: 1.2082858085632324
Validation loss: 2.049191157023112

Epoch: 6| Step: 3
Training loss: 2.9309887886047363
Validation loss: 2.0614050229390464

Epoch: 6| Step: 4
Training loss: 1.8257534503936768
Validation loss: 2.0508617162704468

Epoch: 6| Step: 5
Training loss: 1.7374491691589355
Validation loss: 2.0729538202285767

Epoch: 6| Step: 6
Training loss: 2.2426207065582275
Validation loss: 2.126684308052063

Epoch: 6| Step: 7
Training loss: 1.7237921953201294
Validation loss: 2.1108271876970925

Epoch: 6| Step: 8
Training loss: 2.261148452758789
Validation loss: 2.105705181757609

Epoch: 6| Step: 9
Training loss: 1.730994701385498
Validation loss: 2.096141835053762

Epoch: 6| Step: 10
Training loss: 1.856152892112732
Validation loss: 2.073543985684713

Epoch: 6| Step: 11
Training loss: 2.2408194541931152
Validation loss: 2.072476784388224

Epoch: 6| Step: 12
Training loss: 1.7340832948684692
Validation loss: 2.041024168332418

Epoch: 6| Step: 13
Training loss: 1.6139130592346191
Validation loss: 2.016724089781443

Epoch: 38| Step: 0
Training loss: 2.6816182136535645
Validation loss: 2.025270104408264

Epoch: 6| Step: 1
Training loss: 2.0766124725341797
Validation loss: 2.036104758580526

Epoch: 6| Step: 2
Training loss: 1.609747290611267
Validation loss: 2.0620375275611877

Epoch: 6| Step: 3
Training loss: 2.307699680328369
Validation loss: 2.0733967224756875

Epoch: 6| Step: 4
Training loss: 2.1549746990203857
Validation loss: 2.088401118914286

Epoch: 6| Step: 5
Training loss: 2.32985258102417
Validation loss: 2.0901474356651306

Epoch: 6| Step: 6
Training loss: 1.5796387195587158
Validation loss: 2.0979105631510415

Epoch: 6| Step: 7
Training loss: 1.795365571975708
Validation loss: 2.0838664770126343

Epoch: 6| Step: 8
Training loss: 1.8522793054580688
Validation loss: 2.083724876244863

Epoch: 6| Step: 9
Training loss: 1.4698340892791748
Validation loss: 2.0550994873046875

Epoch: 6| Step: 10
Training loss: 2.1026477813720703
Validation loss: 2.0496367613474527

Epoch: 6| Step: 11
Training loss: 1.889824628829956
Validation loss: 2.065719405810038

Epoch: 6| Step: 12
Training loss: 2.4596495628356934
Validation loss: 2.0507022539774575

Epoch: 6| Step: 13
Training loss: 1.8246163129806519
Validation loss: 2.044119040171305

Epoch: 39| Step: 0
Training loss: 1.974589228630066
Validation loss: 2.040916621685028

Epoch: 6| Step: 1
Training loss: 1.9104697704315186
Validation loss: 2.038636803627014

Epoch: 6| Step: 2
Training loss: 1.984744668006897
Validation loss: 2.072634438673655

Epoch: 6| Step: 3
Training loss: 1.8010138273239136
Validation loss: 2.0866689880688987

Epoch: 6| Step: 4
Training loss: 1.7770353555679321
Validation loss: 2.1262877186139426

Epoch: 6| Step: 5
Training loss: 2.3116462230682373
Validation loss: 2.1309699416160583

Epoch: 6| Step: 6
Training loss: 2.800752639770508
Validation loss: 2.111743211746216

Epoch: 6| Step: 7
Training loss: 1.9492079019546509
Validation loss: 2.102501690387726

Epoch: 6| Step: 8
Training loss: 1.7989370822906494
Validation loss: 2.0683586597442627

Epoch: 6| Step: 9
Training loss: 1.5790616273880005
Validation loss: 2.0495947202046714

Epoch: 6| Step: 10
Training loss: 1.7292732000350952
Validation loss: 2.0350786248842874

Epoch: 6| Step: 11
Training loss: 2.1252315044403076
Validation loss: 2.028909703095754

Epoch: 6| Step: 12
Training loss: 1.0518732070922852
Validation loss: 2.0440420309702554

Epoch: 6| Step: 13
Training loss: 1.9679335355758667
Validation loss: 2.0264304478963218

Epoch: 40| Step: 0
Training loss: 2.1825714111328125
Validation loss: 2.0471978783607483

Epoch: 6| Step: 1
Training loss: 2.0042154788970947
Validation loss: 2.040042241414388

Epoch: 6| Step: 2
Training loss: 1.7671363353729248
Validation loss: 2.0789756178855896

Epoch: 6| Step: 3
Training loss: 1.9328734874725342
Validation loss: 2.0133798122406006

Epoch: 6| Step: 4
Training loss: 2.5098466873168945
Validation loss: 2.0378399093945823

Epoch: 6| Step: 5
Training loss: 1.2227349281311035
Validation loss: 2.04450656970342

Epoch: 6| Step: 6
Training loss: 1.7042052745819092
Validation loss: 2.0258944431940713

Epoch: 6| Step: 7
Training loss: 2.1212453842163086
Validation loss: 2.046054482460022

Epoch: 6| Step: 8
Training loss: 1.8894953727722168
Validation loss: 2.0332058866818747

Epoch: 6| Step: 9
Training loss: 1.917201042175293
Validation loss: 2.0677872697512307

Epoch: 6| Step: 10
Training loss: 1.7021021842956543
Validation loss: 2.032254457473755

Epoch: 6| Step: 11
Training loss: 2.021501064300537
Validation loss: 2.0449007948239646

Epoch: 6| Step: 12
Training loss: 2.028125286102295
Validation loss: 2.0671944419542947

Epoch: 6| Step: 13
Training loss: 1.866340160369873
Validation loss: 2.0224605401357016

Epoch: 41| Step: 0
Training loss: 2.3169431686401367
Validation loss: 2.064695139726003

Epoch: 6| Step: 1
Training loss: 1.974327564239502
Validation loss: 2.0326253374417624

Epoch: 6| Step: 2
Training loss: 2.423252582550049
Validation loss: 2.0407519737879434

Epoch: 6| Step: 3
Training loss: 2.7205915451049805
Validation loss: 2.0599243442217507

Epoch: 6| Step: 4
Training loss: 1.9708260297775269
Validation loss: 2.0554979046185813

Epoch: 6| Step: 5
Training loss: 1.142338752746582
Validation loss: 2.0366150736808777

Epoch: 6| Step: 6
Training loss: 2.1034350395202637
Validation loss: 2.053805867830912

Epoch: 6| Step: 7
Training loss: 1.7245125770568848
Validation loss: 2.0823349356651306

Epoch: 6| Step: 8
Training loss: 1.4938615560531616
Validation loss: 2.051763892173767

Epoch: 6| Step: 9
Training loss: 2.057030200958252
Validation loss: 2.025615155696869

Epoch: 6| Step: 10
Training loss: 1.9972779750823975
Validation loss: 2.061355948448181

Epoch: 6| Step: 11
Training loss: 1.6816654205322266
Validation loss: 2.0243680675824485

Epoch: 6| Step: 12
Training loss: 2.0819735527038574
Validation loss: 2.0217138131459556

Epoch: 6| Step: 13
Training loss: 1.0648667812347412
Validation loss: 2.0717647671699524

Epoch: 42| Step: 0
Training loss: 1.3264466524124146
Validation loss: 2.0261549154917398

Epoch: 6| Step: 1
Training loss: 1.6532330513000488
Validation loss: 2.069087545077006

Epoch: 6| Step: 2
Training loss: 2.46638822555542
Validation loss: 2.071094036102295

Epoch: 6| Step: 3
Training loss: 1.9876809120178223
Validation loss: 2.044681489467621

Epoch: 6| Step: 4
Training loss: 1.288149118423462
Validation loss: 2.0659021139144897

Epoch: 6| Step: 5
Training loss: 2.612712860107422
Validation loss: 2.0639615456263223

Epoch: 6| Step: 6
Training loss: 2.1870241165161133
Validation loss: 2.0244824488957724

Epoch: 6| Step: 7
Training loss: 2.0686421394348145
Validation loss: 2.0685611764589944

Epoch: 6| Step: 8
Training loss: 2.7831099033355713
Validation loss: 2.0618693629900613

Epoch: 6| Step: 9
Training loss: 1.3776044845581055
Validation loss: 2.059213717778524

Epoch: 6| Step: 10
Training loss: 2.192877769470215
Validation loss: 2.027444064617157

Epoch: 6| Step: 11
Training loss: 1.3120757341384888
Validation loss: 2.0399604042371116

Epoch: 6| Step: 12
Training loss: 1.504692554473877
Validation loss: 2.0259429613749185

Epoch: 6| Step: 13
Training loss: 1.8195440769195557
Validation loss: 2.05212140083313

Epoch: 43| Step: 0
Training loss: 2.4660801887512207
Validation loss: 2.022181808948517

Epoch: 6| Step: 1
Training loss: 1.9617531299591064
Validation loss: 2.0206947922706604

Epoch: 6| Step: 2
Training loss: 1.9987478256225586
Validation loss: 2.029198189576467

Epoch: 6| Step: 3
Training loss: 1.5427145957946777
Validation loss: 2.0381429394086203

Epoch: 6| Step: 4
Training loss: 1.6439536809921265
Validation loss: 2.0631266037623086

Epoch: 6| Step: 5
Training loss: 1.8574998378753662
Validation loss: 2.0171757340431213

Epoch: 6| Step: 6
Training loss: 1.5208572149276733
Validation loss: 2.0358125368754068

Epoch: 6| Step: 7
Training loss: 2.063004493713379
Validation loss: 2.0038457910219827

Epoch: 6| Step: 8
Training loss: 2.3727571964263916
Validation loss: 2.033426264921824

Epoch: 6| Step: 9
Training loss: 2.62363862991333
Validation loss: 2.0352291067441306

Epoch: 6| Step: 10
Training loss: 1.7309048175811768
Validation loss: 2.0519651571909585

Epoch: 6| Step: 11
Training loss: 1.6935837268829346
Validation loss: 2.0241492986679077

Epoch: 6| Step: 12
Training loss: 1.428098201751709
Validation loss: 2.021024982134501

Epoch: 6| Step: 13
Training loss: 1.668875813484192
Validation loss: 2.062584340572357

Epoch: 44| Step: 0
Training loss: 2.429678440093994
Validation loss: 2.063015898068746

Epoch: 6| Step: 1
Training loss: 1.943397045135498
Validation loss: 2.022524416446686

Epoch: 6| Step: 2
Training loss: 1.4836368560791016
Validation loss: 2.051614264647166

Epoch: 6| Step: 3
Training loss: 1.4910176992416382
Validation loss: 2.0687077840169272

Epoch: 6| Step: 4
Training loss: 2.104532480239868
Validation loss: 2.0618587930997214

Epoch: 6| Step: 5
Training loss: 2.039064407348633
Validation loss: 2.091083586215973

Epoch: 6| Step: 6
Training loss: 1.9414459466934204
Validation loss: 2.0687062740325928

Epoch: 6| Step: 7
Training loss: 1.7328425645828247
Validation loss: 2.081095278263092

Epoch: 6| Step: 8
Training loss: 1.6205377578735352
Validation loss: 2.0406153400739035

Epoch: 6| Step: 9
Training loss: 1.8786929845809937
Validation loss: 2.0483162800470986

Epoch: 6| Step: 10
Training loss: 2.467182159423828
Validation loss: 2.0621790885925293

Epoch: 6| Step: 11
Training loss: 1.3315801620483398
Validation loss: 2.0393455425898233

Epoch: 6| Step: 12
Training loss: 1.9330412149429321
Validation loss: 2.0555550853411355

Epoch: 6| Step: 13
Training loss: 2.1913883686065674
Validation loss: 2.049786925315857

Epoch: 45| Step: 0
Training loss: 2.204350471496582
Validation loss: 2.0364509224891663

Epoch: 6| Step: 1
Training loss: 1.4033292531967163
Validation loss: 2.0547180771827698

Epoch: 6| Step: 2
Training loss: 2.5860755443573
Validation loss: 2.0387967030207315

Epoch: 6| Step: 3
Training loss: 2.4760637283325195
Validation loss: 2.0489531556765237

Epoch: 6| Step: 4
Training loss: 1.3828911781311035
Validation loss: 2.071538249651591

Epoch: 6| Step: 5
Training loss: 1.9483530521392822
Validation loss: 2.0509851773579917

Epoch: 6| Step: 6
Training loss: 1.751909852027893
Validation loss: 2.0617772142092385

Epoch: 6| Step: 7
Training loss: 2.718724250793457
Validation loss: 2.0356945395469666

Epoch: 6| Step: 8
Training loss: 1.7945704460144043
Validation loss: 2.0479063391685486

Epoch: 6| Step: 9
Training loss: 2.063960313796997
Validation loss: 2.0150788029034934

Epoch: 6| Step: 10
Training loss: 1.2908883094787598
Validation loss: 2.0159327189127603

Epoch: 6| Step: 11
Training loss: 2.0533971786499023
Validation loss: 2.0373343030611673

Epoch: 6| Step: 12
Training loss: 1.0679140090942383
Validation loss: 2.0290871063868203

Epoch: 6| Step: 13
Training loss: 1.5626153945922852
Validation loss: 2.036539832750956

Epoch: 46| Step: 0
Training loss: 1.1953479051589966
Validation loss: 2.0966675877571106

Epoch: 6| Step: 1
Training loss: 1.810050368309021
Validation loss: 2.1343050797780356

Epoch: 6| Step: 2
Training loss: 2.255737781524658
Validation loss: 2.1333587964375815

Epoch: 6| Step: 3
Training loss: 1.3004307746887207
Validation loss: 2.1297852794329324

Epoch: 6| Step: 4
Training loss: 1.7563034296035767
Validation loss: 2.1384504437446594

Epoch: 6| Step: 5
Training loss: 1.83241868019104
Validation loss: 2.1477531592051187

Epoch: 6| Step: 6
Training loss: 2.0197885036468506
Validation loss: 2.1382890939712524

Epoch: 6| Step: 7
Training loss: 1.9020659923553467
Validation loss: 2.1449410915374756

Epoch: 6| Step: 8
Training loss: 2.246044874191284
Validation loss: 2.164423108100891

Epoch: 6| Step: 9
Training loss: 2.2104010581970215
Validation loss: 2.136470595995585

Epoch: 6| Step: 10
Training loss: 1.6357359886169434
Validation loss: 2.1558595299720764

Epoch: 6| Step: 11
Training loss: 1.8470335006713867
Validation loss: 2.0987413922945657

Epoch: 6| Step: 12
Training loss: 2.36543869972229
Validation loss: 2.1111956040064492

Epoch: 6| Step: 13
Training loss: 2.1982924938201904
Validation loss: 2.073202967643738

Epoch: 47| Step: 0
Training loss: 2.2139906883239746
Validation loss: 2.021089414755503

Epoch: 6| Step: 1
Training loss: 1.8648874759674072
Validation loss: 2.084212859471639

Epoch: 6| Step: 2
Training loss: 1.4926936626434326
Validation loss: 2.044178088506063

Epoch: 6| Step: 3
Training loss: 1.65704345703125
Validation loss: 2.059657315413157

Epoch: 6| Step: 4
Training loss: 1.5384979248046875
Validation loss: 2.0844265023867288

Epoch: 6| Step: 5
Training loss: 2.3726205825805664
Validation loss: 2.080157160758972

Epoch: 6| Step: 6
Training loss: 1.888312816619873
Validation loss: 2.1073306997617087

Epoch: 6| Step: 7
Training loss: 2.648536205291748
Validation loss: 2.0455299417177835

Epoch: 6| Step: 8
Training loss: 2.018188953399658
Validation loss: 2.0659578839937844

Epoch: 6| Step: 9
Training loss: 1.5698240995407104
Validation loss: 2.106065650780996

Epoch: 6| Step: 10
Training loss: 1.9698485136032104
Validation loss: 2.0257334311803183

Epoch: 6| Step: 11
Training loss: 2.4144163131713867
Validation loss: 2.0621968110402427

Epoch: 6| Step: 12
Training loss: 1.2516956329345703
Validation loss: 2.0433527628580728

Epoch: 6| Step: 13
Training loss: 2.2258222103118896
Validation loss: 2.074046790599823

Epoch: 48| Step: 0
Training loss: 2.559885263442993
Validation loss: 2.060253063837687

Epoch: 6| Step: 1
Training loss: 1.8405442237854004
Validation loss: 2.0476429065068564

Epoch: 6| Step: 2
Training loss: 1.8413515090942383
Validation loss: 2.115182042121887

Epoch: 6| Step: 3
Training loss: 1.7757246494293213
Validation loss: 2.0904499689737954

Epoch: 6| Step: 4
Training loss: 1.788979172706604
Validation loss: 2.1212904850641885

Epoch: 6| Step: 5
Training loss: 2.4122889041900635
Validation loss: 2.0892917116483054

Epoch: 6| Step: 6
Training loss: 1.294730544090271
Validation loss: 2.0919405619303384

Epoch: 6| Step: 7
Training loss: 1.9082423448562622
Validation loss: 2.0664830803871155

Epoch: 6| Step: 8
Training loss: 2.7067649364471436
Validation loss: 2.0636918544769287

Epoch: 6| Step: 9
Training loss: 1.7796251773834229
Validation loss: 2.0633214116096497

Epoch: 6| Step: 10
Training loss: 1.9048398733139038
Validation loss: 2.066396474838257

Epoch: 6| Step: 11
Training loss: 1.4438369274139404
Validation loss: 2.049408217271169

Epoch: 6| Step: 12
Training loss: 1.4542306661605835
Validation loss: 2.032502214113871

Epoch: 6| Step: 13
Training loss: 1.8476006984710693
Validation loss: 2.021151582400004

Epoch: 49| Step: 0
Training loss: 1.7088429927825928
Validation loss: 2.0167780121167502

Epoch: 6| Step: 1
Training loss: 1.9179160594940186
Validation loss: 2.0505577524503074

Epoch: 6| Step: 2
Training loss: 1.6419050693511963
Validation loss: 2.0252707401911416

Epoch: 6| Step: 3
Training loss: 2.0387215614318848
Validation loss: 2.046215037504832

Epoch: 6| Step: 4
Training loss: 1.6636569499969482
Validation loss: 2.046993931134542

Epoch: 6| Step: 5
Training loss: 1.6094114780426025
Validation loss: 2.0462096134821572

Epoch: 6| Step: 6
Training loss: 1.9582250118255615
Validation loss: 2.0379502177238464

Epoch: 6| Step: 7
Training loss: 1.6259369850158691
Validation loss: 2.0642023285230002

Epoch: 6| Step: 8
Training loss: 1.81074857711792
Validation loss: 2.0168336033821106

Epoch: 6| Step: 9
Training loss: 2.7098803520202637
Validation loss: 2.0356915394465127

Epoch: 6| Step: 10
Training loss: 2.176501512527466
Validation loss: 2.064679761727651

Epoch: 6| Step: 11
Training loss: 1.8226467370986938
Validation loss: 2.0223559538523355

Epoch: 6| Step: 12
Training loss: 2.4505109786987305
Validation loss: 2.081777036190033

Epoch: 6| Step: 13
Training loss: 1.5566619634628296
Validation loss: 2.092533548672994

Epoch: 50| Step: 0
Training loss: 2.1461586952209473
Validation loss: 2.1164632439613342

Epoch: 6| Step: 1
Training loss: 2.1162619590759277
Validation loss: 2.1247496803601584

Epoch: 6| Step: 2
Training loss: 1.5834717750549316
Validation loss: 2.1447704235712686

Epoch: 6| Step: 3
Training loss: 2.1394479274749756
Validation loss: 2.171703338623047

Epoch: 6| Step: 4
Training loss: 1.8211090564727783
Validation loss: 2.1122498909632363

Epoch: 6| Step: 5
Training loss: 1.5943264961242676
Validation loss: 2.130469044049581

Epoch: 6| Step: 6
Training loss: 1.7548179626464844
Validation loss: 2.1192933519681296

Epoch: 6| Step: 7
Training loss: 2.261925220489502
Validation loss: 2.0895862579345703

Epoch: 6| Step: 8
Training loss: 2.5221261978149414
Validation loss: 2.075580914815267

Epoch: 6| Step: 9
Training loss: 1.513458013534546
Validation loss: 2.0174227356910706

Epoch: 6| Step: 10
Training loss: 1.8965599536895752
Validation loss: 1.9996618429819744

Epoch: 6| Step: 11
Training loss: 1.8367469310760498
Validation loss: 2.053878049055735

Epoch: 6| Step: 12
Training loss: 1.8315482139587402
Validation loss: 2.0727907021840415

Epoch: 6| Step: 13
Training loss: 1.7563936710357666
Validation loss: 2.0535386006037393

Epoch: 51| Step: 0
Training loss: 1.335705280303955
Validation loss: 2.0487547318140664

Epoch: 6| Step: 1
Training loss: 1.9886589050292969
Validation loss: 2.067664682865143

Epoch: 6| Step: 2
Training loss: 1.7900052070617676
Validation loss: 2.0481536189715066

Epoch: 6| Step: 3
Training loss: 1.274534821510315
Validation loss: 2.0238213737805686

Epoch: 6| Step: 4
Training loss: 2.2199573516845703
Validation loss: 2.022180199623108

Epoch: 6| Step: 5
Training loss: 1.9854246377944946
Validation loss: 2.0501232147216797

Epoch: 6| Step: 6
Training loss: 1.2024853229522705
Validation loss: 2.0069055954615274

Epoch: 6| Step: 7
Training loss: 2.034700870513916
Validation loss: 2.060426036516825

Epoch: 6| Step: 8
Training loss: 1.7834243774414062
Validation loss: 2.0468762715657554

Epoch: 6| Step: 9
Training loss: 2.207336187362671
Validation loss: 2.0951358874638877

Epoch: 6| Step: 10
Training loss: 2.3198750019073486
Validation loss: 2.0819053649902344

Epoch: 6| Step: 11
Training loss: 1.9463763236999512
Validation loss: 2.1180177132288613

Epoch: 6| Step: 12
Training loss: 1.5079256296157837
Validation loss: 2.1250157753626504

Epoch: 6| Step: 13
Training loss: 2.397063970565796
Validation loss: 2.1748130122820535

Epoch: 52| Step: 0
Training loss: 1.3394895792007446
Validation loss: 2.1538796027501426

Epoch: 6| Step: 1
Training loss: 2.1614127159118652
Validation loss: 2.165736178557078

Epoch: 6| Step: 2
Training loss: 1.7804194688796997
Validation loss: 2.117347518603007

Epoch: 6| Step: 3
Training loss: 1.8487035036087036
Validation loss: 2.137195428212484

Epoch: 6| Step: 4
Training loss: 2.2291674613952637
Validation loss: 2.128805716832479

Epoch: 6| Step: 5
Training loss: 1.7091774940490723
Validation loss: 2.1227489908536277

Epoch: 6| Step: 6
Training loss: 1.3891947269439697
Validation loss: 2.100450019041697

Epoch: 6| Step: 7
Training loss: 1.8462826013565063
Validation loss: 2.0565608143806458

Epoch: 6| Step: 8
Training loss: 1.3858168125152588
Validation loss: 2.087211032708486

Epoch: 6| Step: 9
Training loss: 2.383209466934204
Validation loss: 2.0402000546455383

Epoch: 6| Step: 10
Training loss: 1.6634374856948853
Validation loss: 2.0957136750221252

Epoch: 6| Step: 11
Training loss: 1.4277937412261963
Validation loss: 2.073902189731598

Epoch: 6| Step: 12
Training loss: 2.1141304969787598
Validation loss: 2.0645205775896707

Epoch: 6| Step: 13
Training loss: 2.6119980812072754
Validation loss: 2.0429290533065796

Epoch: 53| Step: 0
Training loss: 1.242919683456421
Validation loss: 2.015717347462972

Epoch: 6| Step: 1
Training loss: 2.0902676582336426
Validation loss: 2.0572210550308228

Epoch: 6| Step: 2
Training loss: 2.2946362495422363
Validation loss: 2.0734103123346963

Epoch: 6| Step: 3
Training loss: 2.723926544189453
Validation loss: 2.055368343989054

Epoch: 6| Step: 4
Training loss: 1.7260756492614746
Validation loss: 2.0375208854675293

Epoch: 6| Step: 5
Training loss: 1.7060503959655762
Validation loss: 2.0423619945844016

Epoch: 6| Step: 6
Training loss: 1.389883041381836
Validation loss: 2.043841282526652

Epoch: 6| Step: 7
Training loss: 1.5051281452178955
Validation loss: 2.034362276395162

Epoch: 6| Step: 8
Training loss: 2.685194253921509
Validation loss: 2.0548412203788757

Epoch: 6| Step: 9
Training loss: 2.0174202919006348
Validation loss: 2.029331068197886

Epoch: 6| Step: 10
Training loss: 1.4176974296569824
Validation loss: 2.1172980864842734

Epoch: 6| Step: 11
Training loss: 1.806890845298767
Validation loss: 2.079323132832845

Epoch: 6| Step: 12
Training loss: 1.2709691524505615
Validation loss: 2.0653629302978516

Epoch: 6| Step: 13
Training loss: 1.9571547508239746
Validation loss: 2.1152194142341614

Epoch: 54| Step: 0
Training loss: 1.7118353843688965
Validation loss: 2.0886791348457336

Epoch: 6| Step: 1
Training loss: 1.830935001373291
Validation loss: 2.089878042538961

Epoch: 6| Step: 2
Training loss: 2.017073154449463
Validation loss: 2.0951244831085205

Epoch: 6| Step: 3
Training loss: 1.5541820526123047
Validation loss: 2.0618234872817993

Epoch: 6| Step: 4
Training loss: 2.2341232299804688
Validation loss: 2.108444253603617

Epoch: 6| Step: 5
Training loss: 1.7660815715789795
Validation loss: 2.110022763411204

Epoch: 6| Step: 6
Training loss: 1.1664716005325317
Validation loss: 2.0851171016693115

Epoch: 6| Step: 7
Training loss: 1.5273857116699219
Validation loss: 2.062433401743571

Epoch: 6| Step: 8
Training loss: 2.144012928009033
Validation loss: 2.0736754139264426

Epoch: 6| Step: 9
Training loss: 1.8223415613174438
Validation loss: 2.0619420607884726

Epoch: 6| Step: 10
Training loss: 2.4777462482452393
Validation loss: 2.062525510787964

Epoch: 6| Step: 11
Training loss: 1.5488314628601074
Validation loss: 2.0694132645924888

Epoch: 6| Step: 12
Training loss: 1.8071341514587402
Validation loss: 2.0439931551615396

Epoch: 6| Step: 13
Training loss: 2.156679153442383
Validation loss: 2.08880349000295

Epoch: 55| Step: 0
Training loss: 1.7260762453079224
Validation loss: 2.1035369833310447

Epoch: 6| Step: 1
Training loss: 1.632749319076538
Validation loss: 2.035125255584717

Epoch: 6| Step: 2
Training loss: 1.772448182106018
Validation loss: 2.018989165623983

Epoch: 6| Step: 3
Training loss: 2.3027100563049316
Validation loss: 2.028092682361603

Epoch: 6| Step: 4
Training loss: 1.5258686542510986
Validation loss: 2.02108371257782

Epoch: 6| Step: 5
Training loss: 1.6966145038604736
Validation loss: 2.0490060249964395

Epoch: 6| Step: 6
Training loss: 2.231586217880249
Validation loss: 2.047530551751455

Epoch: 6| Step: 7
Training loss: 2.138911724090576
Validation loss: 2.0551053484280906

Epoch: 6| Step: 8
Training loss: 1.5621215105056763
Validation loss: 2.0472506086031594

Epoch: 6| Step: 9
Training loss: 1.7864702939987183
Validation loss: 2.0717031160990396

Epoch: 6| Step: 10
Training loss: 2.1183910369873047
Validation loss: 2.0337887008984885

Epoch: 6| Step: 11
Training loss: 1.969940423965454
Validation loss: 2.0540707111358643

Epoch: 6| Step: 12
Training loss: 1.3056010007858276
Validation loss: 2.0973960161209106

Epoch: 6| Step: 13
Training loss: 1.8639874458312988
Validation loss: 2.108275353908539

Epoch: 56| Step: 0
Training loss: 2.3852808475494385
Validation loss: 2.11263773838679

Epoch: 6| Step: 1
Training loss: 1.3964495658874512
Validation loss: 2.145678400993347

Epoch: 6| Step: 2
Training loss: 1.6131095886230469
Validation loss: 2.126638968785604

Epoch: 6| Step: 3
Training loss: 1.4370001554489136
Validation loss: 2.1325497229894004

Epoch: 6| Step: 4
Training loss: 1.7361998558044434
Validation loss: 2.093083401521047

Epoch: 6| Step: 5
Training loss: 1.898906946182251
Validation loss: 2.0722432335217795

Epoch: 6| Step: 6
Training loss: 1.6287765502929688
Validation loss: 2.0167473753293357

Epoch: 6| Step: 7
Training loss: 2.4607958793640137
Validation loss: 2.0456939737002053

Epoch: 6| Step: 8
Training loss: 1.5316554307937622
Validation loss: 2.03554904460907

Epoch: 6| Step: 9
Training loss: 1.5612940788269043
Validation loss: 2.071419835090637

Epoch: 6| Step: 10
Training loss: 1.638791799545288
Validation loss: 2.060858150323232

Epoch: 6| Step: 11
Training loss: 2.67231822013855
Validation loss: 2.0098082224527993

Epoch: 6| Step: 12
Training loss: 2.0952470302581787
Validation loss: 2.0457539558410645

Epoch: 6| Step: 13
Training loss: 1.8782284259796143
Validation loss: 2.0081421931584678

Epoch: 57| Step: 0
Training loss: 1.6195893287658691
Validation loss: 2.069562574227651

Epoch: 6| Step: 1
Training loss: 2.221914768218994
Validation loss: 2.099474787712097

Epoch: 6| Step: 2
Training loss: 1.8419005870819092
Validation loss: 2.107572635014852

Epoch: 6| Step: 3
Training loss: 1.7425438165664673
Validation loss: 2.0928550958633423

Epoch: 6| Step: 4
Training loss: 1.520161509513855
Validation loss: 2.055197616418203

Epoch: 6| Step: 5
Training loss: 1.8029015064239502
Validation loss: 2.054185072580973

Epoch: 6| Step: 6
Training loss: 2.210224151611328
Validation loss: 2.0897553165753684

Epoch: 6| Step: 7
Training loss: 1.4465211629867554
Validation loss: 2.0713440577189126

Epoch: 6| Step: 8
Training loss: 1.2291089296340942
Validation loss: 2.084867517153422

Epoch: 6| Step: 9
Training loss: 2.066042900085449
Validation loss: 2.0436065793037415

Epoch: 6| Step: 10
Training loss: 2.2879719734191895
Validation loss: 2.0198769768079123

Epoch: 6| Step: 11
Training loss: 1.8379631042480469
Validation loss: 2.0806427796681723

Epoch: 6| Step: 12
Training loss: 1.9657642841339111
Validation loss: 2.0436171690622964

Epoch: 6| Step: 13
Training loss: 1.9202861785888672
Validation loss: 2.022026538848877

Epoch: 58| Step: 0
Training loss: 2.6474695205688477
Validation loss: 2.0722222129503884

Epoch: 6| Step: 1
Training loss: 1.7968302965164185
Validation loss: 2.046212315559387

Epoch: 6| Step: 2
Training loss: 1.8468844890594482
Validation loss: 2.079296588897705

Epoch: 6| Step: 3
Training loss: 1.8877596855163574
Validation loss: 2.047088344891866

Epoch: 6| Step: 4
Training loss: 2.048316478729248
Validation loss: 2.05197282632192

Epoch: 6| Step: 5
Training loss: 1.9612401723861694
Validation loss: 2.0728083848953247

Epoch: 6| Step: 6
Training loss: 2.235023021697998
Validation loss: 2.10319991906484

Epoch: 6| Step: 7
Training loss: 1.0292201042175293
Validation loss: 2.082935929298401

Epoch: 6| Step: 8
Training loss: 1.4303147792816162
Validation loss: 2.058038512865702

Epoch: 6| Step: 9
Training loss: 1.6727474927902222
Validation loss: 2.096427341302236

Epoch: 6| Step: 10
Training loss: 1.7502198219299316
Validation loss: 2.053454041481018

Epoch: 6| Step: 11
Training loss: 1.3363711833953857
Validation loss: 2.0653854807217917

Epoch: 6| Step: 12
Training loss: 1.3988878726959229
Validation loss: 2.1147647897402444

Epoch: 6| Step: 13
Training loss: 2.0077052116394043
Validation loss: 2.096397419770559

Epoch: 59| Step: 0
Training loss: 1.4426226615905762
Validation loss: 2.08489724000295

Epoch: 6| Step: 1
Training loss: 1.7738183736801147
Validation loss: 2.0384897192319236

Epoch: 6| Step: 2
Training loss: 2.0902915000915527
Validation loss: 2.0478288729985556

Epoch: 6| Step: 3
Training loss: 2.1328682899475098
Validation loss: 2.0452621380488076

Epoch: 6| Step: 4
Training loss: 2.1854352951049805
Validation loss: 2.0656193693478904

Epoch: 6| Step: 5
Training loss: 1.4521946907043457
Validation loss: 2.041183034578959

Epoch: 6| Step: 6
Training loss: 1.9049065113067627
Validation loss: 2.0690966844558716

Epoch: 6| Step: 7
Training loss: 1.8441033363342285
Validation loss: 2.032527506351471

Epoch: 6| Step: 8
Training loss: 1.3511303663253784
Validation loss: 2.030808428923289

Epoch: 6| Step: 9
Training loss: 2.283618450164795
Validation loss: 2.049435039361318

Epoch: 6| Step: 10
Training loss: 1.4925628900527954
Validation loss: 2.0438395738601685

Epoch: 6| Step: 11
Training loss: 1.5800563097000122
Validation loss: 2.039386828740438

Epoch: 6| Step: 12
Training loss: 1.9481937885284424
Validation loss: 2.0625588297843933

Epoch: 6| Step: 13
Training loss: 1.8228175640106201
Validation loss: 2.1325334111849465

Epoch: 60| Step: 0
Training loss: 1.7696194648742676
Validation loss: 2.144264499346415

Epoch: 6| Step: 1
Training loss: 1.975934386253357
Validation loss: 2.087271491686503

Epoch: 6| Step: 2
Training loss: 2.487131357192993
Validation loss: 2.1367518504460654

Epoch: 6| Step: 3
Training loss: 1.7594244480133057
Validation loss: 2.118488589922587

Epoch: 6| Step: 4
Training loss: 2.37937593460083
Validation loss: 2.1445512970288596

Epoch: 6| Step: 5
Training loss: 1.247194766998291
Validation loss: 2.1209859053293862

Epoch: 6| Step: 6
Training loss: 1.563503623008728
Validation loss: 2.1117117007573447

Epoch: 6| Step: 7
Training loss: 1.9190564155578613
Validation loss: 2.0904367367426553

Epoch: 6| Step: 8
Training loss: 1.9451978206634521
Validation loss: 2.1077275474866233

Epoch: 6| Step: 9
Training loss: 1.6841919422149658
Validation loss: 2.0652401049931846

Epoch: 6| Step: 10
Training loss: 1.7045272588729858
Validation loss: 2.0775017738342285

Epoch: 6| Step: 11
Training loss: 1.7482900619506836
Validation loss: 2.0901452700297036

Epoch: 6| Step: 12
Training loss: 1.5312806367874146
Validation loss: 2.0507630904515586

Epoch: 6| Step: 13
Training loss: 1.636197566986084
Validation loss: 2.0658947825431824

Epoch: 61| Step: 0
Training loss: 1.433519721031189
Validation loss: 2.0467987656593323

Epoch: 6| Step: 1
Training loss: 1.8137991428375244
Validation loss: 2.0786913633346558

Epoch: 6| Step: 2
Training loss: 1.6372634172439575
Validation loss: 2.1159805059432983

Epoch: 6| Step: 3
Training loss: 1.6991796493530273
Validation loss: 2.0932746728261313

Epoch: 6| Step: 4
Training loss: 1.2480251789093018
Validation loss: 2.0873931447664895

Epoch: 6| Step: 5
Training loss: 1.7895838022232056
Validation loss: 2.145343323548635

Epoch: 6| Step: 6
Training loss: 1.480973243713379
Validation loss: 2.1216463247934976

Epoch: 6| Step: 7
Training loss: 1.4427473545074463
Validation loss: 2.10577662785848

Epoch: 6| Step: 8
Training loss: 2.383770227432251
Validation loss: 2.108147402604421

Epoch: 6| Step: 9
Training loss: 1.6513981819152832
Validation loss: 2.0688497026761374

Epoch: 6| Step: 10
Training loss: 2.063045024871826
Validation loss: 2.053341547648112

Epoch: 6| Step: 11
Training loss: 1.7083921432495117
Validation loss: 2.0578598380088806

Epoch: 6| Step: 12
Training loss: 1.7086482048034668
Validation loss: 2.035778224468231

Epoch: 6| Step: 13
Training loss: 3.1375391483306885
Validation loss: 2.0747929016749063

Epoch: 62| Step: 0
Training loss: 1.8536797761917114
Validation loss: 2.0460543235143027

Epoch: 6| Step: 1
Training loss: 1.7840251922607422
Validation loss: 2.0576388438542685

Epoch: 6| Step: 2
Training loss: 1.80341374874115
Validation loss: 2.0380671620368958

Epoch: 6| Step: 3
Training loss: 1.276482343673706
Validation loss: 2.02818234761556

Epoch: 6| Step: 4
Training loss: 1.7932558059692383
Validation loss: 2.0532331665356955

Epoch: 6| Step: 5
Training loss: 2.2782044410705566
Validation loss: 2.0604878862698874

Epoch: 6| Step: 6
Training loss: 1.796288013458252
Validation loss: 2.0742244323094687

Epoch: 6| Step: 7
Training loss: 1.6077120304107666
Validation loss: 2.0512170990308127

Epoch: 6| Step: 8
Training loss: 1.6595219373703003
Validation loss: 2.0541866421699524

Epoch: 6| Step: 9
Training loss: 1.692899227142334
Validation loss: 2.054280241330465

Epoch: 6| Step: 10
Training loss: 1.9017515182495117
Validation loss: 2.0976761976877847

Epoch: 6| Step: 11
Training loss: 1.394745945930481
Validation loss: 2.1010201374689736

Epoch: 6| Step: 12
Training loss: 1.5206583738327026
Validation loss: 2.1223543882369995

Epoch: 6| Step: 13
Training loss: 2.744591236114502
Validation loss: 2.20609458287557

Epoch: 63| Step: 0
Training loss: 1.9328250885009766
Validation loss: 2.200248122215271

Epoch: 6| Step: 1
Training loss: 2.4093852043151855
Validation loss: 2.2201497554779053

Epoch: 6| Step: 2
Training loss: 2.646454334259033
Validation loss: 2.241697390874227

Epoch: 6| Step: 3
Training loss: 1.5213518142700195
Validation loss: 2.2084978222846985

Epoch: 6| Step: 4
Training loss: 1.3381379842758179
Validation loss: 2.118586858113607

Epoch: 6| Step: 5
Training loss: 1.9362367391586304
Validation loss: 2.1000936031341553

Epoch: 6| Step: 6
Training loss: 2.3133668899536133
Validation loss: 2.082165459791819

Epoch: 6| Step: 7
Training loss: 1.0476069450378418
Validation loss: 2.0559648275375366

Epoch: 6| Step: 8
Training loss: 1.9096653461456299
Validation loss: 2.0419514179229736

Epoch: 6| Step: 9
Training loss: 2.026134490966797
Validation loss: 2.0592174331347146

Epoch: 6| Step: 10
Training loss: 1.269647479057312
Validation loss: 2.1088366309801736

Epoch: 6| Step: 11
Training loss: 1.8185397386550903
Validation loss: 2.127565562725067

Epoch: 6| Step: 12
Training loss: 2.7235758304595947
Validation loss: 2.0638102889060974

Epoch: 6| Step: 13
Training loss: 1.8115038871765137
Validation loss: 2.092160483201345

Epoch: 64| Step: 0
Training loss: 2.231316566467285
Validation loss: 2.0715133945147195

Epoch: 6| Step: 1
Training loss: 2.3759350776672363
Validation loss: 2.0669963359832764

Epoch: 6| Step: 2
Training loss: 1.5135712623596191
Validation loss: 2.020755489667257

Epoch: 6| Step: 3
Training loss: 1.906086802482605
Validation loss: 2.096687356630961

Epoch: 6| Step: 4
Training loss: 2.078078508377075
Validation loss: 2.0614152352015176

Epoch: 6| Step: 5
Training loss: 1.30717134475708
Validation loss: 2.0800597071647644

Epoch: 6| Step: 6
Training loss: 1.428372859954834
Validation loss: 2.0387823979059854

Epoch: 6| Step: 7
Training loss: 1.9874682426452637
Validation loss: 2.0838488340377808

Epoch: 6| Step: 8
Training loss: 1.828402042388916
Validation loss: 2.0725815296173096

Epoch: 6| Step: 9
Training loss: 1.4638609886169434
Validation loss: 2.122435748577118

Epoch: 6| Step: 10
Training loss: 1.5544724464416504
Validation loss: 2.132952113946279

Epoch: 6| Step: 11
Training loss: 2.2697720527648926
Validation loss: 2.1536732117335

Epoch: 6| Step: 12
Training loss: 1.6386723518371582
Validation loss: 2.186883866786957

Epoch: 6| Step: 13
Training loss: 1.3082561492919922
Validation loss: 2.206405202547709

Epoch: 65| Step: 0
Training loss: 2.1405858993530273
Validation loss: 2.1821183562278748

Epoch: 6| Step: 1
Training loss: 1.741618037223816
Validation loss: 2.173369030157725

Epoch: 6| Step: 2
Training loss: 1.4459726810455322
Validation loss: 2.1601701776186624

Epoch: 6| Step: 3
Training loss: 2.6709227561950684
Validation loss: 2.133613328138987

Epoch: 6| Step: 4
Training loss: 2.118969678878784
Validation loss: 2.115221679210663

Epoch: 6| Step: 5
Training loss: 1.3136630058288574
Validation loss: 2.098569671312968

Epoch: 6| Step: 6
Training loss: 1.205972671508789
Validation loss: 2.0725481112798056

Epoch: 6| Step: 7
Training loss: 1.5828132629394531
Validation loss: 2.0650985638300576

Epoch: 6| Step: 8
Training loss: 1.805821418762207
Validation loss: 2.064273715019226

Epoch: 6| Step: 9
Training loss: 1.6554230451583862
Validation loss: 2.0533331632614136

Epoch: 6| Step: 10
Training loss: 1.9975208044052124
Validation loss: 2.0527679721514382

Epoch: 6| Step: 11
Training loss: 1.7247917652130127
Validation loss: 2.0269174774487815

Epoch: 6| Step: 12
Training loss: 1.368990182876587
Validation loss: 2.077306071917216

Epoch: 6| Step: 13
Training loss: 1.7431542873382568
Validation loss: 2.030379593372345

Epoch: 66| Step: 0
Training loss: 2.0195047855377197
Validation loss: 2.032594303290049

Epoch: 6| Step: 1
Training loss: 1.3166701793670654
Validation loss: 2.0008084177970886

Epoch: 6| Step: 2
Training loss: 1.753644347190857
Validation loss: 2.055293162663778

Epoch: 6| Step: 3
Training loss: 2.02303147315979
Validation loss: 2.0829943418502808

Epoch: 6| Step: 4
Training loss: 1.743809700012207
Validation loss: 2.0958335995674133

Epoch: 6| Step: 5
Training loss: 2.106924057006836
Validation loss: 2.136201004187266

Epoch: 6| Step: 6
Training loss: 1.3872261047363281
Validation loss: 2.1401670575141907

Epoch: 6| Step: 7
Training loss: 1.4801386594772339
Validation loss: 2.184919854005178

Epoch: 6| Step: 8
Training loss: 2.4910387992858887
Validation loss: 2.1756133834520974

Epoch: 6| Step: 9
Training loss: 1.6970089673995972
Validation loss: 2.1065456668535867

Epoch: 6| Step: 10
Training loss: 2.009033679962158
Validation loss: 2.1172540187835693

Epoch: 6| Step: 11
Training loss: 0.8005881905555725
Validation loss: 2.062430997689565

Epoch: 6| Step: 12
Training loss: 2.0031535625457764
Validation loss: 2.0682774583498635

Epoch: 6| Step: 13
Training loss: 1.9482367038726807
Validation loss: 2.075875997543335

Epoch: 67| Step: 0
Training loss: 1.5600244998931885
Validation loss: 2.086255371570587

Epoch: 6| Step: 1
Training loss: 1.6284427642822266
Validation loss: 2.069313406944275

Epoch: 6| Step: 2
Training loss: 1.5610617399215698
Validation loss: 2.0690067211786904

Epoch: 6| Step: 3
Training loss: 2.124431610107422
Validation loss: 2.0479906598726907

Epoch: 6| Step: 4
Training loss: 1.4607326984405518
Validation loss: 2.0623687505722046

Epoch: 6| Step: 5
Training loss: 1.7465178966522217
Validation loss: 2.039235532283783

Epoch: 6| Step: 6
Training loss: 1.9517619609832764
Validation loss: 2.032302419344584

Epoch: 6| Step: 7
Training loss: 1.5758352279663086
Validation loss: 2.1015875140825906

Epoch: 6| Step: 8
Training loss: 1.894561767578125
Validation loss: 2.0597948829332986

Epoch: 6| Step: 9
Training loss: 1.2654082775115967
Validation loss: 2.1005802750587463

Epoch: 6| Step: 10
Training loss: 2.7266457080841064
Validation loss: 2.095461924870809

Epoch: 6| Step: 11
Training loss: 1.6593326330184937
Validation loss: 2.095672885576884

Epoch: 6| Step: 12
Training loss: 1.8738185167312622
Validation loss: 2.1042603651682534

Epoch: 6| Step: 13
Training loss: 1.7913377285003662
Validation loss: 2.0772021214167276

Epoch: 68| Step: 0
Training loss: 1.9256219863891602
Validation loss: 2.0667531490325928

Epoch: 6| Step: 1
Training loss: 1.41581392288208
Validation loss: 2.089038590590159

Epoch: 6| Step: 2
Training loss: 2.129725933074951
Validation loss: 2.0825706919034324

Epoch: 6| Step: 3
Training loss: 2.4537365436553955
Validation loss: 2.1343311071395874

Epoch: 6| Step: 4
Training loss: 1.2030527591705322
Validation loss: 2.097534716129303

Epoch: 6| Step: 5
Training loss: 1.3314611911773682
Validation loss: 2.1058011651039124

Epoch: 6| Step: 6
Training loss: 1.4977245330810547
Validation loss: 2.0672462781270347

Epoch: 6| Step: 7
Training loss: 2.1005635261535645
Validation loss: 2.091460367043813

Epoch: 6| Step: 8
Training loss: 1.369687795639038
Validation loss: 2.127245922883352

Epoch: 6| Step: 9
Training loss: 1.5197744369506836
Validation loss: 2.104429006576538

Epoch: 6| Step: 10
Training loss: 2.305419445037842
Validation loss: 2.1101043621699014

Epoch: 6| Step: 11
Training loss: 1.23016357421875
Validation loss: 2.1422112186749778

Epoch: 6| Step: 12
Training loss: 2.199120044708252
Validation loss: 2.138633688290914

Epoch: 6| Step: 13
Training loss: 1.816537857055664
Validation loss: 2.1371490955352783

Epoch: 69| Step: 0
Training loss: 1.8428046703338623
Validation loss: 2.0876347422599792

Epoch: 6| Step: 1
Training loss: 2.131146192550659
Validation loss: 2.068516492843628

Epoch: 6| Step: 2
Training loss: 1.7398862838745117
Validation loss: 2.058883786201477

Epoch: 6| Step: 3
Training loss: 1.4772549867630005
Validation loss: 2.070818066596985

Epoch: 6| Step: 4
Training loss: 1.6005096435546875
Validation loss: 2.030941446622213

Epoch: 6| Step: 5
Training loss: 1.496144413948059
Validation loss: 2.0826337536176047

Epoch: 6| Step: 6
Training loss: 1.6960207223892212
Validation loss: 2.08816796541214

Epoch: 6| Step: 7
Training loss: 1.4035097360610962
Validation loss: 2.073942760626475

Epoch: 6| Step: 8
Training loss: 1.4590060710906982
Validation loss: 2.0750062664349875

Epoch: 6| Step: 9
Training loss: 1.624491810798645
Validation loss: 2.030603309472402

Epoch: 6| Step: 10
Training loss: 1.9757685661315918
Validation loss: 2.097068428993225

Epoch: 6| Step: 11
Training loss: 1.979343056678772
Validation loss: 2.092678189277649

Epoch: 6| Step: 12
Training loss: 1.8609224557876587
Validation loss: 2.0598321557044983

Epoch: 6| Step: 13
Training loss: 2.1071434020996094
Validation loss: 2.057369649410248

Epoch: 70| Step: 0
Training loss: 2.110874652862549
Validation loss: 2.10182515780131

Epoch: 6| Step: 1
Training loss: 2.303405284881592
Validation loss: 2.0661725997924805

Epoch: 6| Step: 2
Training loss: 1.3193212747573853
Validation loss: 2.0916710098584494

Epoch: 6| Step: 3
Training loss: 2.057912826538086
Validation loss: 2.1069243947664895

Epoch: 6| Step: 4
Training loss: 1.791416049003601
Validation loss: 2.119824250539144

Epoch: 6| Step: 5
Training loss: 1.5194480419158936
Validation loss: 2.127888639767965

Epoch: 6| Step: 6
Training loss: 1.889061450958252
Validation loss: 2.1225940783818564

Epoch: 6| Step: 7
Training loss: 1.5007531642913818
Validation loss: 2.133739153544108

Epoch: 6| Step: 8
Training loss: 1.2533445358276367
Validation loss: 2.129218856493632

Epoch: 6| Step: 9
Training loss: 1.4333759546279907
Validation loss: 2.0753624637921653

Epoch: 6| Step: 10
Training loss: 1.9572383165359497
Validation loss: 2.100973685582479

Epoch: 6| Step: 11
Training loss: 1.7712234258651733
Validation loss: 2.0899463494618735

Epoch: 6| Step: 12
Training loss: 1.678166389465332
Validation loss: 2.1051314870516458

Epoch: 6| Step: 13
Training loss: 1.7894103527069092
Validation loss: 2.0652982592582703

Epoch: 71| Step: 0
Training loss: 1.7248270511627197
Validation loss: 2.072084347407023

Epoch: 6| Step: 1
Training loss: 1.2651467323303223
Validation loss: 2.0829086303710938

Epoch: 6| Step: 2
Training loss: 1.9165101051330566
Validation loss: 2.04471755027771

Epoch: 6| Step: 3
Training loss: 1.9379770755767822
Validation loss: 2.0741525491078696

Epoch: 6| Step: 4
Training loss: 1.647995948791504
Validation loss: 2.0431161522865295

Epoch: 6| Step: 5
Training loss: 1.3144474029541016
Validation loss: 2.1067800720532737

Epoch: 6| Step: 6
Training loss: 1.36654794216156
Validation loss: 2.0819478631019592

Epoch: 6| Step: 7
Training loss: 1.2987306118011475
Validation loss: 2.0754921436309814

Epoch: 6| Step: 8
Training loss: 1.7661771774291992
Validation loss: 2.1324296593666077

Epoch: 6| Step: 9
Training loss: 1.8482950925827026
Validation loss: 2.0785099267959595

Epoch: 6| Step: 10
Training loss: 1.4936715364456177
Validation loss: 2.1088568965593972

Epoch: 6| Step: 11
Training loss: 2.602538585662842
Validation loss: 2.1736663977305093

Epoch: 6| Step: 12
Training loss: 2.3386216163635254
Validation loss: 2.1731271346410117

Epoch: 6| Step: 13
Training loss: 1.454768419265747
Validation loss: 2.197108427683512

Epoch: 72| Step: 0
Training loss: 2.3709075450897217
Validation loss: 2.1842087507247925

Epoch: 6| Step: 1
Training loss: 1.9672611951828003
Validation loss: 2.1281302173932395

Epoch: 6| Step: 2
Training loss: 2.2742013931274414
Validation loss: 2.1544899940490723

Epoch: 6| Step: 3
Training loss: 1.9642367362976074
Validation loss: 2.147590418656667

Epoch: 6| Step: 4
Training loss: 2.1286368370056152
Validation loss: 2.058732827504476

Epoch: 6| Step: 5
Training loss: 1.9047130346298218
Validation loss: 2.052372137705485

Epoch: 6| Step: 6
Training loss: 1.9895849227905273
Validation loss: 2.0438172419865928

Epoch: 6| Step: 7
Training loss: 1.410429835319519
Validation loss: 2.059113005797068

Epoch: 6| Step: 8
Training loss: 1.2291460037231445
Validation loss: 2.0637224515279136

Epoch: 6| Step: 9
Training loss: 0.9553902745246887
Validation loss: 2.030259688695272

Epoch: 6| Step: 10
Training loss: 1.6316821575164795
Validation loss: 2.0870943665504456

Epoch: 6| Step: 11
Training loss: 1.4973446130752563
Validation loss: 2.1326725085576377

Epoch: 6| Step: 12
Training loss: 1.3689804077148438
Validation loss: 2.0908326506614685

Epoch: 6| Step: 13
Training loss: 1.7660032510757446
Validation loss: 2.128144105275472

Epoch: 73| Step: 0
Training loss: 1.7930262088775635
Validation loss: 2.1185529232025146

Epoch: 6| Step: 1
Training loss: 1.5186841487884521
Validation loss: 2.126287321249644

Epoch: 6| Step: 2
Training loss: 2.2005980014801025
Validation loss: 2.1472238500912986

Epoch: 6| Step: 3
Training loss: 1.8589671850204468
Validation loss: 2.1206164956092834

Epoch: 6| Step: 4
Training loss: 1.5936088562011719
Validation loss: 2.128867824872335

Epoch: 6| Step: 5
Training loss: 1.461018443107605
Validation loss: 2.15394659837087

Epoch: 6| Step: 6
Training loss: 1.9418318271636963
Validation loss: 2.1154515544573465

Epoch: 6| Step: 7
Training loss: 1.5985796451568604
Validation loss: 2.0736873745918274

Epoch: 6| Step: 8
Training loss: 2.236937999725342
Validation loss: 2.116123696168264

Epoch: 6| Step: 9
Training loss: 1.9162404537200928
Validation loss: 2.08144740263621

Epoch: 6| Step: 10
Training loss: 1.0842348337173462
Validation loss: 2.0552373131116233

Epoch: 6| Step: 11
Training loss: 1.814828634262085
Validation loss: 2.11601330836614

Epoch: 6| Step: 12
Training loss: 1.14901864528656
Validation loss: 2.078801771004995

Epoch: 6| Step: 13
Training loss: 1.6418447494506836
Validation loss: 2.145360986391703

Epoch: 74| Step: 0
Training loss: 2.3207616806030273
Validation loss: 2.142939507961273

Epoch: 6| Step: 1
Training loss: 2.3080825805664062
Validation loss: 2.0992033084233603

Epoch: 6| Step: 2
Training loss: 0.7973729968070984
Validation loss: 2.1033637523651123

Epoch: 6| Step: 3
Training loss: 1.3819267749786377
Validation loss: 2.072952151298523

Epoch: 6| Step: 4
Training loss: 1.58392333984375
Validation loss: 2.07749080657959

Epoch: 6| Step: 5
Training loss: 1.6439330577850342
Validation loss: 2.1011874079704285

Epoch: 6| Step: 6
Training loss: 1.6257065534591675
Validation loss: 2.1015557050704956

Epoch: 6| Step: 7
Training loss: 2.082627773284912
Validation loss: 2.1141477624575296

Epoch: 6| Step: 8
Training loss: 1.4582716226577759
Validation loss: 2.076446076234182

Epoch: 6| Step: 9
Training loss: 1.696563482284546
Validation loss: 2.1277425487836203

Epoch: 6| Step: 10
Training loss: 1.198781132698059
Validation loss: 2.12913316488266

Epoch: 6| Step: 11
Training loss: 1.764904499053955
Validation loss: 2.1060440142949424

Epoch: 6| Step: 12
Training loss: 1.7669801712036133
Validation loss: 2.126342256863912

Epoch: 6| Step: 13
Training loss: 1.9434053897857666
Validation loss: 2.120590925216675

Epoch: 75| Step: 0
Training loss: 2.060407876968384
Validation loss: 2.1336759527524314

Epoch: 6| Step: 1
Training loss: 0.9122608304023743
Validation loss: 2.108738879362742

Epoch: 6| Step: 2
Training loss: 1.0873692035675049
Validation loss: 2.1080350279808044

Epoch: 6| Step: 3
Training loss: 2.1151695251464844
Validation loss: 2.11741304397583

Epoch: 6| Step: 4
Training loss: 2.384474039077759
Validation loss: 2.0925631125768027

Epoch: 6| Step: 5
Training loss: 1.9219379425048828
Validation loss: 2.098054846127828

Epoch: 6| Step: 6
Training loss: 1.7970126867294312
Validation loss: 2.1002856890360513

Epoch: 6| Step: 7
Training loss: 1.3737635612487793
Validation loss: 2.1204800605773926

Epoch: 6| Step: 8
Training loss: 1.5351812839508057
Validation loss: 2.1156678795814514

Epoch: 6| Step: 9
Training loss: 1.8871662616729736
Validation loss: 2.155728499094645

Epoch: 6| Step: 10
Training loss: 2.0274503231048584
Validation loss: 2.1163092056910195

Epoch: 6| Step: 11
Training loss: 1.170691967010498
Validation loss: 2.1188897291819253

Epoch: 6| Step: 12
Training loss: 1.4187707901000977
Validation loss: 2.0632277528444924

Epoch: 6| Step: 13
Training loss: 1.6068804264068604
Validation loss: 2.1356413761774697

Epoch: 76| Step: 0
Training loss: 1.948335886001587
Validation loss: 2.117137869199117

Epoch: 6| Step: 1
Training loss: 2.0667712688446045
Validation loss: 2.0703125

Epoch: 6| Step: 2
Training loss: 1.685762643814087
Validation loss: 2.11342591047287

Epoch: 6| Step: 3
Training loss: 1.845230221748352
Validation loss: 2.0638935565948486

Epoch: 6| Step: 4
Training loss: 1.7442409992218018
Validation loss: 2.1104547580083213

Epoch: 6| Step: 5
Training loss: 1.4133869409561157
Validation loss: 2.112739384174347

Epoch: 6| Step: 6
Training loss: 1.2372223138809204
Validation loss: 2.1130200227101645

Epoch: 6| Step: 7
Training loss: 1.8524717092514038
Validation loss: 2.131563047568003

Epoch: 6| Step: 8
Training loss: 1.941878318786621
Validation loss: 2.13885106643041

Epoch: 6| Step: 9
Training loss: 1.3424406051635742
Validation loss: 2.1232024431228638

Epoch: 6| Step: 10
Training loss: 1.6959911584854126
Validation loss: 2.1174198190371194

Epoch: 6| Step: 11
Training loss: 1.8380993604660034
Validation loss: 2.0937514702479043

Epoch: 6| Step: 12
Training loss: 1.1082830429077148
Validation loss: 2.030156393845876

Epoch: 6| Step: 13
Training loss: 1.720694661140442
Validation loss: 2.0556618372599282

Epoch: 77| Step: 0
Training loss: 1.8684474229812622
Validation loss: 2.092557450135549

Epoch: 6| Step: 1
Training loss: 1.4099907875061035
Validation loss: 2.071928560733795

Epoch: 6| Step: 2
Training loss: 2.0514049530029297
Validation loss: 2.0423792799313865

Epoch: 6| Step: 3
Training loss: 1.486685872077942
Validation loss: 2.008085827032725

Epoch: 6| Step: 4
Training loss: 1.995827317237854
Validation loss: 2.0557878414789834

Epoch: 6| Step: 5
Training loss: 1.949427843093872
Validation loss: 2.0448065797487893

Epoch: 6| Step: 6
Training loss: 1.970639705657959
Validation loss: 2.1235285997390747

Epoch: 6| Step: 7
Training loss: 1.1466952562332153
Validation loss: 2.0767104029655457

Epoch: 6| Step: 8
Training loss: 1.8576984405517578
Validation loss: 2.090930998325348

Epoch: 6| Step: 9
Training loss: 1.6054980754852295
Validation loss: 2.1490588585535684

Epoch: 6| Step: 10
Training loss: 1.1514098644256592
Validation loss: 2.1720433235168457

Epoch: 6| Step: 11
Training loss: 1.5740740299224854
Validation loss: 2.1969949205716452

Epoch: 6| Step: 12
Training loss: 2.047541856765747
Validation loss: 2.139855166276296

Epoch: 6| Step: 13
Training loss: 1.7067655324935913
Validation loss: 2.1574297547340393

Epoch: 78| Step: 0
Training loss: 1.7258185148239136
Validation loss: 2.2054383556048074

Epoch: 6| Step: 1
Training loss: 1.347753643989563
Validation loss: 2.154147982597351

Epoch: 6| Step: 2
Training loss: 1.5140142440795898
Validation loss: 2.1333192189534507

Epoch: 6| Step: 3
Training loss: 1.6021733283996582
Validation loss: 2.147917111714681

Epoch: 6| Step: 4
Training loss: 2.1843347549438477
Validation loss: 2.140724698702494

Epoch: 6| Step: 5
Training loss: 2.0459201335906982
Validation loss: 2.101752281188965

Epoch: 6| Step: 6
Training loss: 2.525956869125366
Validation loss: 2.0439714193344116

Epoch: 6| Step: 7
Training loss: 1.1207236051559448
Validation loss: 2.0666438937187195

Epoch: 6| Step: 8
Training loss: 1.6685316562652588
Validation loss: 2.0468087991078696

Epoch: 6| Step: 9
Training loss: 1.1781320571899414
Validation loss: 2.0195902983347573

Epoch: 6| Step: 10
Training loss: 1.7884538173675537
Validation loss: 2.0616952578226724

Epoch: 6| Step: 11
Training loss: 1.5578892230987549
Validation loss: 2.0458494424819946

Epoch: 6| Step: 12
Training loss: 1.815110206604004
Validation loss: 2.065977851549784

Epoch: 6| Step: 13
Training loss: 1.3521478176116943
Validation loss: 2.049899697303772

Epoch: 79| Step: 0
Training loss: 1.3359806537628174
Validation loss: 2.0987521211306253

Epoch: 6| Step: 1
Training loss: 2.35964298248291
Validation loss: 2.139691114425659

Epoch: 6| Step: 2
Training loss: 2.404590129852295
Validation loss: 2.127724270025889

Epoch: 6| Step: 3
Training loss: 1.4022096395492554
Validation loss: 2.1044252713521323

Epoch: 6| Step: 4
Training loss: 1.6430637836456299
Validation loss: 2.1062640150388083

Epoch: 6| Step: 5
Training loss: 1.1779649257659912
Validation loss: 2.1299426356951394

Epoch: 6| Step: 6
Training loss: 1.9131380319595337
Validation loss: 2.110109031200409

Epoch: 6| Step: 7
Training loss: 1.247421383857727
Validation loss: 2.124648094177246

Epoch: 6| Step: 8
Training loss: 1.4120956659317017
Validation loss: 2.10187562306722

Epoch: 6| Step: 9
Training loss: 1.828971266746521
Validation loss: 2.1333481073379517

Epoch: 6| Step: 10
Training loss: 0.8841330409049988
Validation loss: 2.0998464624087014

Epoch: 6| Step: 11
Training loss: 1.71976900100708
Validation loss: 2.0753288865089417

Epoch: 6| Step: 12
Training loss: 1.4863518476486206
Validation loss: 2.1005364656448364

Epoch: 6| Step: 13
Training loss: 1.7914339303970337
Validation loss: 2.1246419747670493

Epoch: 80| Step: 0
Training loss: 2.2160444259643555
Validation loss: 2.1162379384040833

Epoch: 6| Step: 1
Training loss: 1.1641193628311157
Validation loss: 2.1205389698346457

Epoch: 6| Step: 2
Training loss: 1.7708418369293213
Validation loss: 2.1482479174931846

Epoch: 6| Step: 3
Training loss: 1.862871527671814
Validation loss: 2.098065674304962

Epoch: 6| Step: 4
Training loss: 1.1266604661941528
Validation loss: 2.1239093939463296

Epoch: 6| Step: 5
Training loss: 1.5195937156677246
Validation loss: 2.0953545371691384

Epoch: 6| Step: 6
Training loss: 2.1809868812561035
Validation loss: 2.082735856374105

Epoch: 6| Step: 7
Training loss: 1.696002721786499
Validation loss: 2.099362870057424

Epoch: 6| Step: 8
Training loss: 1.3849316835403442
Validation loss: 2.139553705851237

Epoch: 6| Step: 9
Training loss: 1.306622862815857
Validation loss: 2.134600043296814

Epoch: 6| Step: 10
Training loss: 1.347515344619751
Validation loss: 2.1821683049201965

Epoch: 6| Step: 11
Training loss: 1.5465556383132935
Validation loss: 2.168473502000173

Epoch: 6| Step: 12
Training loss: 2.1177191734313965
Validation loss: 2.212838967641195

Epoch: 6| Step: 13
Training loss: 1.8640141487121582
Validation loss: 2.1876777609189353

Epoch: 81| Step: 0
Training loss: 1.827283501625061
Validation loss: 2.1404760479927063

Epoch: 6| Step: 1
Training loss: 1.986867070198059
Validation loss: 2.0690974394480386

Epoch: 6| Step: 2
Training loss: 1.5103951692581177
Validation loss: 2.0623462994893393

Epoch: 6| Step: 3
Training loss: 1.4765561819076538
Validation loss: 2.039272904396057

Epoch: 6| Step: 4
Training loss: 1.4898936748504639
Validation loss: 2.0697771112124124

Epoch: 6| Step: 5
Training loss: 1.598634958267212
Validation loss: 2.06384269396464

Epoch: 6| Step: 6
Training loss: 1.190198540687561
Validation loss: 2.0444639722506204

Epoch: 6| Step: 7
Training loss: 1.9118995666503906
Validation loss: 2.074628710746765

Epoch: 6| Step: 8
Training loss: 1.3111326694488525
Validation loss: 2.0872066020965576

Epoch: 6| Step: 9
Training loss: 2.003159523010254
Validation loss: 2.1060163378715515

Epoch: 6| Step: 10
Training loss: 1.735727310180664
Validation loss: 2.157200276851654

Epoch: 6| Step: 11
Training loss: 1.5991861820220947
Validation loss: 2.1929269234339395

Epoch: 6| Step: 12
Training loss: 1.5961923599243164
Validation loss: 2.180831491947174

Epoch: 6| Step: 13
Training loss: 2.3354856967926025
Validation loss: 2.2201680143674216

Epoch: 82| Step: 0
Training loss: 1.7103790044784546
Validation loss: 2.1473983327547708

Epoch: 6| Step: 1
Training loss: 1.439295768737793
Validation loss: 2.221126616001129

Epoch: 6| Step: 2
Training loss: 1.7649116516113281
Validation loss: 2.1288235783576965

Epoch: 6| Step: 3
Training loss: 1.1932231187820435
Validation loss: 2.083089550336202

Epoch: 6| Step: 4
Training loss: 1.9975334405899048
Validation loss: 2.08275173107783

Epoch: 6| Step: 5
Training loss: 1.8427009582519531
Validation loss: 2.075985312461853

Epoch: 6| Step: 6
Training loss: 1.978990077972412
Validation loss: 2.1381969650586448

Epoch: 6| Step: 7
Training loss: 1.7412947416305542
Validation loss: 2.097697158654531

Epoch: 6| Step: 8
Training loss: 1.8772215843200684
Validation loss: 2.082712411880493

Epoch: 6| Step: 9
Training loss: 1.3270149230957031
Validation loss: 2.041315217812856

Epoch: 6| Step: 10
Training loss: 2.129176616668701
Validation loss: 2.020682136217753

Epoch: 6| Step: 11
Training loss: 1.1194007396697998
Validation loss: 2.0399641593297324

Epoch: 6| Step: 12
Training loss: 1.5620677471160889
Validation loss: 2.0312628944714866

Epoch: 6| Step: 13
Training loss: 1.1568665504455566
Validation loss: 2.0176146825154624

Epoch: 83| Step: 0
Training loss: 1.8462039232254028
Validation loss: 2.0963401198387146

Epoch: 6| Step: 1
Training loss: 0.7892469763755798
Validation loss: 2.0874438683191934

Epoch: 6| Step: 2
Training loss: 2.4806363582611084
Validation loss: 2.1529128352801004

Epoch: 6| Step: 3
Training loss: 1.5455946922302246
Validation loss: 2.1144407987594604

Epoch: 6| Step: 4
Training loss: 1.5841065645217896
Validation loss: 2.145766278107961

Epoch: 6| Step: 5
Training loss: 1.1826913356781006
Validation loss: 2.188546439011892

Epoch: 6| Step: 6
Training loss: 1.2410917282104492
Validation loss: 2.184705893198649

Epoch: 6| Step: 7
Training loss: 1.9308998584747314
Validation loss: 2.164456784725189

Epoch: 6| Step: 8
Training loss: 1.9360201358795166
Validation loss: 2.178259472052256

Epoch: 6| Step: 9
Training loss: 0.8378969430923462
Validation loss: 2.0815675258636475

Epoch: 6| Step: 10
Training loss: 2.3614449501037598
Validation loss: 2.1124115188916526

Epoch: 6| Step: 11
Training loss: 1.6626060009002686
Validation loss: 2.053625841935476

Epoch: 6| Step: 12
Training loss: 1.7053899765014648
Validation loss: 2.125832994778951

Epoch: 6| Step: 13
Training loss: 1.3245506286621094
Validation loss: 2.087628205617269

Epoch: 84| Step: 0
Training loss: 1.717292308807373
Validation loss: 2.1222607493400574

Epoch: 6| Step: 1
Training loss: 1.6803427934646606
Validation loss: 2.123081107934316

Epoch: 6| Step: 2
Training loss: 1.740321159362793
Validation loss: 2.083290974299113

Epoch: 6| Step: 3
Training loss: 1.8203257322311401
Validation loss: 2.1291388471921286

Epoch: 6| Step: 4
Training loss: 1.1330504417419434
Validation loss: 2.09653373559316

Epoch: 6| Step: 5
Training loss: 2.3282058238983154
Validation loss: 2.0534241000811257

Epoch: 6| Step: 6
Training loss: 1.5559898614883423
Validation loss: 2.0686707297960916

Epoch: 6| Step: 7
Training loss: 0.9547025561332703
Validation loss: 2.0704033374786377

Epoch: 6| Step: 8
Training loss: 1.7459661960601807
Validation loss: 2.04876975218455

Epoch: 6| Step: 9
Training loss: 1.3961243629455566
Validation loss: 2.101379096508026

Epoch: 6| Step: 10
Training loss: 1.8678677082061768
Validation loss: 2.1635108590126038

Epoch: 6| Step: 11
Training loss: 2.0435681343078613
Validation loss: 2.2254271507263184

Epoch: 6| Step: 12
Training loss: 1.0022183656692505
Validation loss: 2.217243214448293

Epoch: 6| Step: 13
Training loss: 1.1049833297729492
Validation loss: 2.188098927338918

Epoch: 85| Step: 0
Training loss: 1.1375505924224854
Validation loss: 2.192741056283315

Epoch: 6| Step: 1
Training loss: 1.2339179515838623
Validation loss: 2.2158403396606445

Epoch: 6| Step: 2
Training loss: 1.2883448600769043
Validation loss: 2.1790396173795066

Epoch: 6| Step: 3
Training loss: 1.275549292564392
Validation loss: 2.175498644510905

Epoch: 6| Step: 4
Training loss: 2.1037564277648926
Validation loss: 2.1397924621899924

Epoch: 6| Step: 5
Training loss: 1.6186864376068115
Validation loss: 2.138147532939911

Epoch: 6| Step: 6
Training loss: 2.01334810256958
Validation loss: 2.118238071600596

Epoch: 6| Step: 7
Training loss: 1.4925519227981567
Validation loss: 2.1235477924346924

Epoch: 6| Step: 8
Training loss: 0.9960032105445862
Validation loss: 2.081660350163778

Epoch: 6| Step: 9
Training loss: 1.5594184398651123
Validation loss: 2.085927168528239

Epoch: 6| Step: 10
Training loss: 1.7867993116378784
Validation loss: 2.091515223185221

Epoch: 6| Step: 11
Training loss: 1.91940176486969
Validation loss: 2.1001811027526855

Epoch: 6| Step: 12
Training loss: 1.6330480575561523
Validation loss: 2.080397367477417

Epoch: 6| Step: 13
Training loss: 1.3800442218780518
Validation loss: 2.0951343377431235

Epoch: 86| Step: 0
Training loss: 1.110023021697998
Validation loss: 2.0933053294817605

Epoch: 6| Step: 1
Training loss: 1.4045345783233643
Validation loss: 2.108039597670237

Epoch: 6| Step: 2
Training loss: 1.2583658695220947
Validation loss: 2.1014457543691

Epoch: 6| Step: 3
Training loss: 1.8092633485794067
Validation loss: 2.11647629737854

Epoch: 6| Step: 4
Training loss: 1.0024765729904175
Validation loss: 2.1297733386357627

Epoch: 6| Step: 5
Training loss: 1.9702471494674683
Validation loss: 2.1881644129753113

Epoch: 6| Step: 6
Training loss: 2.0704028606414795
Validation loss: 2.258881429831187

Epoch: 6| Step: 7
Training loss: 1.806901216506958
Validation loss: 2.2254559993743896

Epoch: 6| Step: 8
Training loss: 1.7707176208496094
Validation loss: 2.164790670077006

Epoch: 6| Step: 9
Training loss: 2.148817539215088
Validation loss: 2.203253706296285

Epoch: 6| Step: 10
Training loss: 0.946507453918457
Validation loss: 2.168573498725891

Epoch: 6| Step: 11
Training loss: 1.3321375846862793
Validation loss: 2.155392348766327

Epoch: 6| Step: 12
Training loss: 1.5537903308868408
Validation loss: 2.106912930806478

Epoch: 6| Step: 13
Training loss: 2.6497929096221924
Validation loss: 2.0777822732925415

Epoch: 87| Step: 0
Training loss: 1.593636155128479
Validation loss: 2.0309373339017234

Epoch: 6| Step: 1
Training loss: 1.7748808860778809
Validation loss: 2.0504160126050315

Epoch: 6| Step: 2
Training loss: 1.5009047985076904
Validation loss: 2.119832913080851

Epoch: 6| Step: 3
Training loss: 1.1197128295898438
Validation loss: 2.1008278131484985

Epoch: 6| Step: 4
Training loss: 1.2596766948699951
Validation loss: 2.082833786805471

Epoch: 6| Step: 5
Training loss: 2.2042384147644043
Validation loss: 2.1508215069770813

Epoch: 6| Step: 6
Training loss: 1.8691673278808594
Validation loss: 2.1447641253471375

Epoch: 6| Step: 7
Training loss: 1.7739570140838623
Validation loss: 2.242149511973063

Epoch: 6| Step: 8
Training loss: 1.9014811515808105
Validation loss: 2.2251358032226562

Epoch: 6| Step: 9
Training loss: 1.3833870887756348
Validation loss: 2.2992274363835654

Epoch: 6| Step: 10
Training loss: 1.0683859586715698
Validation loss: 2.2665764888127646

Epoch: 6| Step: 11
Training loss: 0.8541532754898071
Validation loss: 2.2205042839050293

Epoch: 6| Step: 12
Training loss: 2.0239500999450684
Validation loss: 2.219448665777842

Epoch: 6| Step: 13
Training loss: 1.6031190156936646
Validation loss: 2.209683358669281

Epoch: 88| Step: 0
Training loss: 1.11423659324646
Validation loss: 2.1472036838531494

Epoch: 6| Step: 1
Training loss: 1.373772382736206
Validation loss: 2.113462766011556

Epoch: 6| Step: 2
Training loss: 1.6566102504730225
Validation loss: 2.0757083892822266

Epoch: 6| Step: 3
Training loss: 0.9255728125572205
Validation loss: 2.0773293375968933

Epoch: 6| Step: 4
Training loss: 2.050326108932495
Validation loss: 2.14681476354599

Epoch: 6| Step: 5
Training loss: 1.5567110776901245
Validation loss: 2.092613855997721

Epoch: 6| Step: 6
Training loss: 1.900676965713501
Validation loss: 2.1318678061167398

Epoch: 6| Step: 7
Training loss: 1.2853529453277588
Validation loss: 2.1454604268074036

Epoch: 6| Step: 8
Training loss: 1.5613043308258057
Validation loss: 2.1368088722229004

Epoch: 6| Step: 9
Training loss: 1.7498362064361572
Validation loss: 2.139738619327545

Epoch: 6| Step: 10
Training loss: 1.1266791820526123
Validation loss: 2.154680291811625

Epoch: 6| Step: 11
Training loss: 1.7800390720367432
Validation loss: 2.078655183315277

Epoch: 6| Step: 12
Training loss: 1.6596776247024536
Validation loss: 2.1043604811032615

Epoch: 6| Step: 13
Training loss: 1.73940110206604
Validation loss: 2.0882801612218223

Epoch: 89| Step: 0
Training loss: 2.0832276344299316
Validation loss: 2.0458595951398215

Epoch: 6| Step: 1
Training loss: 1.3830510377883911
Validation loss: 2.113774518171946

Epoch: 6| Step: 2
Training loss: 1.3297333717346191
Validation loss: 2.125043570995331

Epoch: 6| Step: 3
Training loss: 1.4676493406295776
Validation loss: 2.0953395764033

Epoch: 6| Step: 4
Training loss: 1.7568843364715576
Validation loss: 2.1434085766474404

Epoch: 6| Step: 5
Training loss: 1.4287928342819214
Validation loss: 2.1675514380137124

Epoch: 6| Step: 6
Training loss: 1.3083720207214355
Validation loss: 2.137489060560862

Epoch: 6| Step: 7
Training loss: 1.0125985145568848
Validation loss: 2.1617232163747153

Epoch: 6| Step: 8
Training loss: 1.546867847442627
Validation loss: 2.1855863332748413

Epoch: 6| Step: 9
Training loss: 1.5495481491088867
Validation loss: 2.1344075997670493

Epoch: 6| Step: 10
Training loss: 2.129002332687378
Validation loss: 2.176132400830587

Epoch: 6| Step: 11
Training loss: 1.714704990386963
Validation loss: 2.1812077164649963

Epoch: 6| Step: 12
Training loss: 0.9915635585784912
Validation loss: 2.1637284755706787

Epoch: 6| Step: 13
Training loss: 1.2110249996185303
Validation loss: 2.1962931950887046

Epoch: 90| Step: 0
Training loss: 2.229010581970215
Validation loss: 2.1861069599787393

Epoch: 6| Step: 1
Training loss: 1.9823989868164062
Validation loss: 2.182819644610087

Epoch: 6| Step: 2
Training loss: 1.1565269231796265
Validation loss: 2.152505417664846

Epoch: 6| Step: 3
Training loss: 1.802278995513916
Validation loss: 2.1443647146224976

Epoch: 6| Step: 4
Training loss: 1.2291553020477295
Validation loss: 2.130103667577108

Epoch: 6| Step: 5
Training loss: 1.289076328277588
Validation loss: 2.118153969446818

Epoch: 6| Step: 6
Training loss: 1.592604637145996
Validation loss: 2.0913265347480774

Epoch: 6| Step: 7
Training loss: 1.2623512744903564
Validation loss: 2.1290929118792215

Epoch: 6| Step: 8
Training loss: 1.394362211227417
Validation loss: 2.1369165579477944

Epoch: 6| Step: 9
Training loss: 1.3149175643920898
Validation loss: 2.137752195199331

Epoch: 6| Step: 10
Training loss: 1.1783924102783203
Validation loss: 2.1589356064796448

Epoch: 6| Step: 11
Training loss: 1.4329614639282227
Validation loss: 2.1238507827123008

Epoch: 6| Step: 12
Training loss: 1.2977149486541748
Validation loss: 2.1954535047213235

Epoch: 6| Step: 13
Training loss: 1.3422659635543823
Validation loss: 2.1756094694137573

Epoch: 91| Step: 0
Training loss: 1.7820448875427246
Validation loss: 2.16364053885142

Epoch: 6| Step: 1
Training loss: 1.3992009162902832
Validation loss: 2.1541743874549866

Epoch: 6| Step: 2
Training loss: 1.832629680633545
Validation loss: 2.216016332308451

Epoch: 6| Step: 3
Training loss: 1.1067893505096436
Validation loss: 2.1773237387339273

Epoch: 6| Step: 4
Training loss: 1.425628662109375
Validation loss: 2.167000333468119

Epoch: 6| Step: 5
Training loss: 1.0913138389587402
Validation loss: 2.14431365331014

Epoch: 6| Step: 6
Training loss: 1.4706358909606934
Validation loss: 2.129963000615438

Epoch: 6| Step: 7
Training loss: 1.3056716918945312
Validation loss: 2.1657214363416037

Epoch: 6| Step: 8
Training loss: 1.5088975429534912
Validation loss: 2.1503487626711526

Epoch: 6| Step: 9
Training loss: 1.5666663646697998
Validation loss: 2.1614054242769876

Epoch: 6| Step: 10
Training loss: 1.3302111625671387
Validation loss: 2.147478918234507

Epoch: 6| Step: 11
Training loss: 1.7620108127593994
Validation loss: 2.1269272565841675

Epoch: 6| Step: 12
Training loss: 1.99116849899292
Validation loss: 2.1687174240748086

Epoch: 6| Step: 13
Training loss: 1.065262794494629
Validation loss: 2.1580471793810525

Epoch: 92| Step: 0
Training loss: 1.3963804244995117
Validation loss: 2.1741755604743958

Epoch: 6| Step: 1
Training loss: 1.4878909587860107
Validation loss: 2.2324225703875222

Epoch: 6| Step: 2
Training loss: 1.6949939727783203
Validation loss: 2.178450028101603

Epoch: 6| Step: 3
Training loss: 1.5990993976593018
Validation loss: 2.1608189741770425

Epoch: 6| Step: 4
Training loss: 1.3614026308059692
Validation loss: 2.1456002791722617

Epoch: 6| Step: 5
Training loss: 1.2111531496047974
Validation loss: 2.087878922621409

Epoch: 6| Step: 6
Training loss: 1.706430196762085
Validation loss: 2.134154717127482

Epoch: 6| Step: 7
Training loss: 1.8746531009674072
Validation loss: 2.070819139480591

Epoch: 6| Step: 8
Training loss: 0.7582710981369019
Validation loss: 2.086826662222544

Epoch: 6| Step: 9
Training loss: 1.7931885719299316
Validation loss: 2.0252188444137573

Epoch: 6| Step: 10
Training loss: 1.7892732620239258
Validation loss: 2.115884999434153

Epoch: 6| Step: 11
Training loss: 1.37929105758667
Validation loss: 2.1302289962768555

Epoch: 6| Step: 12
Training loss: 1.3602283000946045
Validation loss: 2.164750397205353

Epoch: 6| Step: 13
Training loss: 1.4993267059326172
Validation loss: 2.1699851353963218

Epoch: 93| Step: 0
Training loss: 1.0796329975128174
Validation loss: 2.180563529332479

Epoch: 6| Step: 1
Training loss: 1.1805158853530884
Validation loss: 2.164247294267019

Epoch: 6| Step: 2
Training loss: 1.6564488410949707
Validation loss: 2.2028983434041343

Epoch: 6| Step: 3
Training loss: 1.4932284355163574
Validation loss: 2.144867161909739

Epoch: 6| Step: 4
Training loss: 1.3267910480499268
Validation loss: 2.1244047482808432

Epoch: 6| Step: 5
Training loss: 1.758557915687561
Validation loss: 2.1434802611668906

Epoch: 6| Step: 6
Training loss: 1.3127288818359375
Validation loss: 2.066638151804606

Epoch: 6| Step: 7
Training loss: 2.3292112350463867
Validation loss: 2.0845872163772583

Epoch: 6| Step: 8
Training loss: 0.861217200756073
Validation loss: 2.082396388053894

Epoch: 6| Step: 9
Training loss: 1.6759954690933228
Validation loss: 2.1153632601102195

Epoch: 6| Step: 10
Training loss: 1.4035956859588623
Validation loss: 2.1144992907842

Epoch: 6| Step: 11
Training loss: 2.0476250648498535
Validation loss: 2.0977760752042136

Epoch: 6| Step: 12
Training loss: 1.2326364517211914
Validation loss: 2.1339128017425537

Epoch: 6| Step: 13
Training loss: 1.30916166305542
Validation loss: 2.138710300127665

Epoch: 94| Step: 0
Training loss: 1.9719867706298828
Validation loss: 2.1475835839907327

Epoch: 6| Step: 1
Training loss: 1.1680349111557007
Validation loss: 2.1361397902170816

Epoch: 6| Step: 2
Training loss: 0.9950593113899231
Validation loss: 2.1817026933034263

Epoch: 6| Step: 3
Training loss: 1.5433238744735718
Validation loss: 2.11699108282725

Epoch: 6| Step: 4
Training loss: 1.3663718700408936
Validation loss: 2.1300063530604043

Epoch: 6| Step: 5
Training loss: 1.7645230293273926
Validation loss: 2.088992158571879

Epoch: 6| Step: 6
Training loss: 1.2206339836120605
Validation loss: 2.1544633507728577

Epoch: 6| Step: 7
Training loss: 1.5419600009918213
Validation loss: 2.1288858453432717

Epoch: 6| Step: 8
Training loss: 1.5917136669158936
Validation loss: 2.186902383963267

Epoch: 6| Step: 9
Training loss: 0.8406944274902344
Validation loss: 2.1952757835388184

Epoch: 6| Step: 10
Training loss: 1.5872806310653687
Validation loss: 2.2038434545199075

Epoch: 6| Step: 11
Training loss: 1.4762988090515137
Validation loss: 2.169264018535614

Epoch: 6| Step: 12
Training loss: 1.523414134979248
Validation loss: 2.1970598498980203

Epoch: 6| Step: 13
Training loss: 1.566118836402893
Validation loss: 2.0913686553637185

Epoch: 95| Step: 0
Training loss: 0.9829777479171753
Validation loss: 2.0799069007237754

Epoch: 6| Step: 1
Training loss: 1.1346280574798584
Validation loss: 2.0452309250831604

Epoch: 6| Step: 2
Training loss: 1.1089746952056885
Validation loss: 2.100941280523936

Epoch: 6| Step: 3
Training loss: 1.565535068511963
Validation loss: 2.1217851837476096

Epoch: 6| Step: 4
Training loss: 1.9345890283584595
Validation loss: 2.011659622192383

Epoch: 6| Step: 5
Training loss: 1.3381297588348389
Validation loss: 2.0993149677912393

Epoch: 6| Step: 6
Training loss: 0.9583812952041626
Validation loss: 2.155488967895508

Epoch: 6| Step: 7
Training loss: 1.0487853288650513
Validation loss: 2.1870527863502502

Epoch: 6| Step: 8
Training loss: 1.3829468488693237
Validation loss: 2.2151657740275064

Epoch: 6| Step: 9
Training loss: 2.0934510231018066
Validation loss: 2.239038666089376

Epoch: 6| Step: 10
Training loss: 1.5782160758972168
Validation loss: 2.2664366563161216

Epoch: 6| Step: 11
Training loss: 1.9785115718841553
Validation loss: 2.302865982055664

Epoch: 6| Step: 12
Training loss: 1.507075309753418
Validation loss: 2.20748362938563

Epoch: 6| Step: 13
Training loss: 2.1062235832214355
Validation loss: 2.1975188851356506

Epoch: 96| Step: 0
Training loss: 0.8236121535301208
Validation loss: 2.099493145942688

Epoch: 6| Step: 1
Training loss: 1.038989543914795
Validation loss: 2.0753287076950073

Epoch: 6| Step: 2
Training loss: 1.723020315170288
Validation loss: 2.1114349166552224

Epoch: 6| Step: 3
Training loss: 1.182326078414917
Validation loss: 2.112411836783091

Epoch: 6| Step: 4
Training loss: 2.072782039642334
Validation loss: 2.1135501662890115

Epoch: 6| Step: 5
Training loss: 1.2777085304260254
Validation loss: 2.174730360507965

Epoch: 6| Step: 6
Training loss: 1.4020354747772217
Validation loss: 2.2341624101003013

Epoch: 6| Step: 7
Training loss: 1.1795532703399658
Validation loss: 2.276019593079885

Epoch: 6| Step: 8
Training loss: 1.901639699935913
Validation loss: 2.2225341399510703

Epoch: 6| Step: 9
Training loss: 1.0524003505706787
Validation loss: 2.2008859515190125

Epoch: 6| Step: 10
Training loss: 1.8026506900787354
Validation loss: 2.2161976099014282

Epoch: 6| Step: 11
Training loss: 1.6293227672576904
Validation loss: 2.194570004940033

Epoch: 6| Step: 12
Training loss: 1.5241966247558594
Validation loss: 2.2176451881726584

Epoch: 6| Step: 13
Training loss: 1.2098665237426758
Validation loss: 2.120558977127075

Epoch: 97| Step: 0
Training loss: 1.2893109321594238
Validation loss: 2.0871050159136453

Epoch: 6| Step: 1
Training loss: 1.626013159751892
Validation loss: 2.134993076324463

Epoch: 6| Step: 2
Training loss: 1.2856957912445068
Validation loss: 2.135936439037323

Epoch: 6| Step: 3
Training loss: 1.3458987474441528
Validation loss: 2.135110358397166

Epoch: 6| Step: 4
Training loss: 1.3359384536743164
Validation loss: 2.1463891863822937

Epoch: 6| Step: 5
Training loss: 1.0142563581466675
Validation loss: 2.055166165033976

Epoch: 6| Step: 6
Training loss: 0.9161341190338135
Validation loss: 2.151337961355845

Epoch: 6| Step: 7
Training loss: 1.253678798675537
Validation loss: 2.1324032147725425

Epoch: 6| Step: 8
Training loss: 1.9135938882827759
Validation loss: 2.1068063179651895

Epoch: 6| Step: 9
Training loss: 1.3972567319869995
Validation loss: 2.1864275336265564

Epoch: 6| Step: 10
Training loss: 2.1939408779144287
Validation loss: 2.130562404791514

Epoch: 6| Step: 11
Training loss: 0.7893321514129639
Validation loss: 2.2067295710245767

Epoch: 6| Step: 12
Training loss: 1.1983450651168823
Validation loss: 2.280433416366577

Epoch: 6| Step: 13
Training loss: 2.1297945976257324
Validation loss: 2.2591855923334756

Epoch: 98| Step: 0
Training loss: 1.4158108234405518
Validation loss: 2.2443868120511374

Epoch: 6| Step: 1
Training loss: 1.2581408023834229
Validation loss: 2.1902614633242288

Epoch: 6| Step: 2
Training loss: 1.6489464044570923
Validation loss: 2.1465503176053367

Epoch: 6| Step: 3
Training loss: 1.5349693298339844
Validation loss: 2.093568801879883

Epoch: 6| Step: 4
Training loss: 1.0299434661865234
Validation loss: 2.074424465497335

Epoch: 6| Step: 5
Training loss: 1.9711670875549316
Validation loss: 2.0886473655700684

Epoch: 6| Step: 6
Training loss: 1.6205323934555054
Validation loss: 2.0485838055610657

Epoch: 6| Step: 7
Training loss: 1.4556750059127808
Validation loss: 2.0685176253318787

Epoch: 6| Step: 8
Training loss: 1.2314977645874023
Validation loss: 2.1090135971705117

Epoch: 6| Step: 9
Training loss: 1.9956547021865845
Validation loss: 2.1058059533437095

Epoch: 6| Step: 10
Training loss: 1.1297812461853027
Validation loss: 2.1723689238230386

Epoch: 6| Step: 11
Training loss: 1.3451130390167236
Validation loss: 2.192854126294454

Epoch: 6| Step: 12
Training loss: 1.5419883728027344
Validation loss: 2.204967280228933

Epoch: 6| Step: 13
Training loss: 0.8503129482269287
Validation loss: 2.175737957159678

Epoch: 99| Step: 0
Training loss: 0.555819571018219
Validation loss: 2.2689072092374167

Epoch: 6| Step: 1
Training loss: 1.2990386486053467
Validation loss: 2.267618695894877

Epoch: 6| Step: 2
Training loss: 1.4721088409423828
Validation loss: 2.2139376401901245

Epoch: 6| Step: 3
Training loss: 0.9573068618774414
Validation loss: 2.208059628804525

Epoch: 6| Step: 4
Training loss: 1.9780468940734863
Validation loss: 2.165077726046244

Epoch: 6| Step: 5
Training loss: 1.6472980976104736
Validation loss: 2.135527034600576

Epoch: 6| Step: 6
Training loss: 1.4075497388839722
Validation loss: 2.1563257376352944

Epoch: 6| Step: 7
Training loss: 1.0518704652786255
Validation loss: 2.1164263486862183

Epoch: 6| Step: 8
Training loss: 1.3991154432296753
Validation loss: 2.1026427348454795

Epoch: 6| Step: 9
Training loss: 1.2928922176361084
Validation loss: 2.0890074372291565

Epoch: 6| Step: 10
Training loss: 1.9921858310699463
Validation loss: 2.1432587703069053

Epoch: 6| Step: 11
Training loss: 1.47221040725708
Validation loss: 2.1125198801358542

Epoch: 6| Step: 12
Training loss: 1.0992553234100342
Validation loss: 2.17279452085495

Epoch: 6| Step: 13
Training loss: 1.2971961498260498
Validation loss: 2.2249923944473267

Epoch: 100| Step: 0
Training loss: 1.242875337600708
Validation loss: 2.218608816464742

Epoch: 6| Step: 1
Training loss: 1.9877663850784302
Validation loss: 2.2589792609214783

Epoch: 6| Step: 2
Training loss: 1.0758330821990967
Validation loss: 2.248189091682434

Epoch: 6| Step: 3
Training loss: 1.1230695247650146
Validation loss: 2.3001163403193154

Epoch: 6| Step: 4
Training loss: 1.1449694633483887
Validation loss: 2.271012822786967

Epoch: 6| Step: 5
Training loss: 1.7452493906021118
Validation loss: 2.2137010296185813

Epoch: 6| Step: 6
Training loss: 1.6778146028518677
Validation loss: 2.157815396785736

Epoch: 6| Step: 7
Training loss: 0.993206262588501
Validation loss: 2.160770078500112

Epoch: 6| Step: 8
Training loss: 0.6828794479370117
Validation loss: 2.102462351322174

Epoch: 6| Step: 9
Training loss: 1.5330536365509033
Validation loss: 2.126756469408671

Epoch: 6| Step: 10
Training loss: 1.3066506385803223
Validation loss: 2.18454380830129

Epoch: 6| Step: 11
Training loss: 1.8173375129699707
Validation loss: 2.1230979760487876

Epoch: 6| Step: 12
Training loss: 1.3143818378448486
Validation loss: 2.1753066182136536

Epoch: 6| Step: 13
Training loss: 1.126647710800171
Validation loss: 2.143619179725647

Testing loss: 2.120094963114896

Epoch: 1| Step: 0
Training loss: 5.212636470794678
Validation loss: 4.892654895782471

Epoch: 6| Step: 1
Training loss: 5.5911865234375
Validation loss: 4.863390525182088

Epoch: 6| Step: 2
Training loss: 4.786530017852783
Validation loss: 4.840000708897908

Epoch: 6| Step: 3
Training loss: 5.040976524353027
Validation loss: 4.818264007568359

Epoch: 6| Step: 4
Training loss: 4.714000701904297
Validation loss: 4.7935411135355634

Epoch: 6| Step: 5
Training loss: 5.026699066162109
Validation loss: 4.763997634251912

Epoch: 6| Step: 6
Training loss: 5.779618263244629
Validation loss: 4.737344582875569

Epoch: 6| Step: 7
Training loss: 4.1179914474487305
Validation loss: 4.706753571828206

Epoch: 6| Step: 8
Training loss: 4.204145908355713
Validation loss: 4.67993950843811

Epoch: 6| Step: 9
Training loss: 4.918955326080322
Validation loss: 4.648813724517822

Epoch: 6| Step: 10
Training loss: 4.9659929275512695
Validation loss: 4.618739763895671

Epoch: 6| Step: 11
Training loss: 4.299271583557129
Validation loss: 4.590131123860677

Epoch: 6| Step: 12
Training loss: 3.8464550971984863
Validation loss: 4.557128190994263

Epoch: 6| Step: 13
Training loss: 4.7865777015686035
Validation loss: 4.5263956387837725

Epoch: 2| Step: 0
Training loss: 5.521045207977295
Validation loss: 4.49229355653127

Epoch: 6| Step: 1
Training loss: 5.086194038391113
Validation loss: 4.453551332155864

Epoch: 6| Step: 2
Training loss: 4.667105674743652
Validation loss: 4.420705437660217

Epoch: 6| Step: 3
Training loss: 4.589559555053711
Validation loss: 4.382361094156901

Epoch: 6| Step: 4
Training loss: 4.164259910583496
Validation loss: 4.342367649078369

Epoch: 6| Step: 5
Training loss: 4.342194557189941
Validation loss: 4.301761428515117

Epoch: 6| Step: 6
Training loss: 4.677981853485107
Validation loss: 4.264424641927083

Epoch: 6| Step: 7
Training loss: 4.061074733734131
Validation loss: 4.227437973022461

Epoch: 6| Step: 8
Training loss: 4.024320125579834
Validation loss: 4.186126907666524

Epoch: 6| Step: 9
Training loss: 4.247482776641846
Validation loss: 4.140199740727742

Epoch: 6| Step: 10
Training loss: 4.371977806091309
Validation loss: 4.094743649164836

Epoch: 6| Step: 11
Training loss: 4.2475504875183105
Validation loss: 4.046383102734883

Epoch: 6| Step: 12
Training loss: 3.1393930912017822
Validation loss: 4.007084568341573

Epoch: 6| Step: 13
Training loss: 3.8020071983337402
Validation loss: 3.9590287605921426

Epoch: 3| Step: 0
Training loss: 5.0712385177612305
Validation loss: 3.9080543915430703

Epoch: 6| Step: 1
Training loss: 3.47809100151062
Validation loss: 3.8497840960820517

Epoch: 6| Step: 2
Training loss: 3.851571559906006
Validation loss: 3.797581513722738

Epoch: 6| Step: 3
Training loss: 4.045473098754883
Validation loss: 3.7388764222462973

Epoch: 6| Step: 4
Training loss: 3.4716553688049316
Validation loss: 3.6817951599756875

Epoch: 6| Step: 5
Training loss: 3.752096176147461
Validation loss: 3.622195521990458

Epoch: 6| Step: 6
Training loss: 3.7640647888183594
Validation loss: 3.5622929334640503

Epoch: 6| Step: 7
Training loss: 3.103850841522217
Validation loss: 3.5000816186269126

Epoch: 6| Step: 8
Training loss: 3.391089677810669
Validation loss: 3.4328999121983848

Epoch: 6| Step: 9
Training loss: 2.9904000759124756
Validation loss: 3.361486554145813

Epoch: 6| Step: 10
Training loss: 3.698085069656372
Validation loss: 3.2991310358047485

Epoch: 6| Step: 11
Training loss: 3.7115416526794434
Validation loss: 3.2246086994806924

Epoch: 6| Step: 12
Training loss: 3.2951791286468506
Validation loss: 3.1441370248794556

Epoch: 6| Step: 13
Training loss: 2.8698067665100098
Validation loss: 3.0748215913772583

Epoch: 4| Step: 0
Training loss: 2.08797550201416
Validation loss: 2.9948713382085166

Epoch: 6| Step: 1
Training loss: 4.251363754272461
Validation loss: 2.928239345550537

Epoch: 6| Step: 2
Training loss: 2.4678802490234375
Validation loss: 2.8564498027165732

Epoch: 6| Step: 3
Training loss: 2.587456464767456
Validation loss: 2.773301641146342

Epoch: 6| Step: 4
Training loss: 2.771033763885498
Validation loss: 2.693396011988322

Epoch: 6| Step: 5
Training loss: 2.1395022869110107
Validation loss: 2.6072567303975425

Epoch: 6| Step: 6
Training loss: 2.042341470718384
Validation loss: 2.5336493055025735

Epoch: 6| Step: 7
Training loss: 1.7581827640533447
Validation loss: 2.462609847386678

Epoch: 6| Step: 8
Training loss: 2.673409938812256
Validation loss: 2.3630499243736267

Epoch: 6| Step: 9
Training loss: 3.0023458003997803
Validation loss: 2.31710954507192

Epoch: 6| Step: 10
Training loss: 1.7409954071044922
Validation loss: 2.2577345967292786

Epoch: 6| Step: 11
Training loss: 2.4724552631378174
Validation loss: 2.217769185702006

Epoch: 6| Step: 12
Training loss: 2.0290651321411133
Validation loss: 2.17450745900472

Epoch: 6| Step: 13
Training loss: 2.492988109588623
Validation loss: 2.1512213548024497

Epoch: 5| Step: 0
Training loss: 2.1680946350097656
Validation loss: 2.146316945552826

Epoch: 6| Step: 1
Training loss: 2.3273932933807373
Validation loss: 2.1639710664749146

Epoch: 6| Step: 2
Training loss: 2.5530052185058594
Validation loss: 2.188825706640879

Epoch: 6| Step: 3
Training loss: 2.305922746658325
Validation loss: 2.180272122224172

Epoch: 6| Step: 4
Training loss: 1.8624933958053589
Validation loss: 2.243534962336222

Epoch: 6| Step: 5
Training loss: 2.0690126419067383
Validation loss: 2.257269263267517

Epoch: 6| Step: 6
Training loss: 1.7403579950332642
Validation loss: 2.2845139304796853

Epoch: 6| Step: 7
Training loss: 2.006258726119995
Validation loss: 2.275713245073954

Epoch: 6| Step: 8
Training loss: 1.9210574626922607
Validation loss: 2.2736748854319253

Epoch: 6| Step: 9
Training loss: 1.6978991031646729
Validation loss: 2.2389082312583923

Epoch: 6| Step: 10
Training loss: 2.0715627670288086
Validation loss: 2.2084142168362937

Epoch: 6| Step: 11
Training loss: 2.9113712310791016
Validation loss: 2.1980193654696145

Epoch: 6| Step: 12
Training loss: 2.7853307723999023
Validation loss: 2.1903780897458396

Epoch: 6| Step: 13
Training loss: 2.274867296218872
Validation loss: 2.174466828505198

Epoch: 6| Step: 0
Training loss: 1.5025843381881714
Validation loss: 2.15588641166687

Epoch: 6| Step: 1
Training loss: 2.4870102405548096
Validation loss: 2.1574315627415976

Epoch: 6| Step: 2
Training loss: 2.3708672523498535
Validation loss: 2.134162743886312

Epoch: 6| Step: 3
Training loss: 2.6055893898010254
Validation loss: 2.1370978156725564

Epoch: 6| Step: 4
Training loss: 2.1900365352630615
Validation loss: 2.143497943878174

Epoch: 6| Step: 5
Training loss: 2.0480799674987793
Validation loss: 2.14903595050176

Epoch: 6| Step: 6
Training loss: 2.5499935150146484
Validation loss: 2.1846174001693726

Epoch: 6| Step: 7
Training loss: 1.6431708335876465
Validation loss: 2.160547614097595

Epoch: 6| Step: 8
Training loss: 2.938608169555664
Validation loss: 2.1677964329719543

Epoch: 6| Step: 9
Training loss: 1.5970323085784912
Validation loss: 2.151470502217611

Epoch: 6| Step: 10
Training loss: 2.1764516830444336
Validation loss: 2.146347482999166

Epoch: 6| Step: 11
Training loss: 1.9788539409637451
Validation loss: 2.169301430384318

Epoch: 6| Step: 12
Training loss: 1.9068853855133057
Validation loss: 2.160878360271454

Epoch: 6| Step: 13
Training loss: 1.7303450107574463
Validation loss: 2.1616297562917075

Epoch: 7| Step: 0
Training loss: 2.36720609664917
Validation loss: 2.1605886220932007

Epoch: 6| Step: 1
Training loss: 1.5182335376739502
Validation loss: 2.1434373458226523

Epoch: 6| Step: 2
Training loss: 1.9179863929748535
Validation loss: 2.1693523128827414

Epoch: 6| Step: 3
Training loss: 2.4563798904418945
Validation loss: 2.1403760512669883

Epoch: 6| Step: 4
Training loss: 2.2575907707214355
Validation loss: 2.156852126121521

Epoch: 6| Step: 5
Training loss: 1.691015362739563
Validation loss: 2.1289327144622803

Epoch: 6| Step: 6
Training loss: 2.3897557258605957
Validation loss: 2.132630705833435

Epoch: 6| Step: 7
Training loss: 1.9739673137664795
Validation loss: 2.148451010386149

Epoch: 6| Step: 8
Training loss: 2.3476805686950684
Validation loss: 2.14382803440094

Epoch: 6| Step: 9
Training loss: 2.212170124053955
Validation loss: 2.143700659275055

Epoch: 6| Step: 10
Training loss: 1.663445234298706
Validation loss: 2.1526028712590537

Epoch: 6| Step: 11
Training loss: 2.581374168395996
Validation loss: 2.152288496494293

Epoch: 6| Step: 12
Training loss: 1.9125568866729736
Validation loss: 2.1570811669031777

Epoch: 6| Step: 13
Training loss: 2.1548047065734863
Validation loss: 2.145959476629893

Epoch: 8| Step: 0
Training loss: 2.759352207183838
Validation loss: 2.148621459801992

Epoch: 6| Step: 1
Training loss: 1.7294292449951172
Validation loss: 2.1714686354001365

Epoch: 6| Step: 2
Training loss: 1.548633337020874
Validation loss: 2.163231352965037

Epoch: 6| Step: 3
Training loss: 2.5651116371154785
Validation loss: 2.155340234438578

Epoch: 6| Step: 4
Training loss: 1.8254965543746948
Validation loss: 2.1787965893745422

Epoch: 6| Step: 5
Training loss: 2.210432529449463
Validation loss: 2.1529322465260825

Epoch: 6| Step: 6
Training loss: 2.2121596336364746
Validation loss: 2.1676979064941406

Epoch: 6| Step: 7
Training loss: 1.5109169483184814
Validation loss: 2.1449273427327475

Epoch: 6| Step: 8
Training loss: 3.0741872787475586
Validation loss: 2.1560972531636557

Epoch: 6| Step: 9
Training loss: 1.8981925249099731
Validation loss: 2.1270556449890137

Epoch: 6| Step: 10
Training loss: 1.8743259906768799
Validation loss: 2.142511169115702

Epoch: 6| Step: 11
Training loss: 2.0373783111572266
Validation loss: 2.1244848171869912

Epoch: 6| Step: 12
Training loss: 1.495680570602417
Validation loss: 2.1238351265589395

Epoch: 6| Step: 13
Training loss: 2.636204481124878
Validation loss: 2.1273226737976074

Epoch: 9| Step: 0
Training loss: 2.566230058670044
Validation loss: 2.1474583943684897

Epoch: 6| Step: 1
Training loss: 2.1779379844665527
Validation loss: 2.13773250579834

Epoch: 6| Step: 2
Training loss: 2.4083714485168457
Validation loss: 2.1157944997151694

Epoch: 6| Step: 3
Training loss: 2.4686636924743652
Validation loss: 2.126468241214752

Epoch: 6| Step: 4
Training loss: 1.5290321111679077
Validation loss: 2.1125940084457397

Epoch: 6| Step: 5
Training loss: 1.248211145401001
Validation loss: 2.105358362197876

Epoch: 6| Step: 6
Training loss: 2.064882278442383
Validation loss: 2.1153927048047385

Epoch: 6| Step: 7
Training loss: 2.0570850372314453
Validation loss: 2.1245680252710977

Epoch: 6| Step: 8
Training loss: 2.0102062225341797
Validation loss: 2.128679633140564

Epoch: 6| Step: 9
Training loss: 1.8446080684661865
Validation loss: 2.1242050925890603

Epoch: 6| Step: 10
Training loss: 1.9348030090332031
Validation loss: 2.1246112982432046

Epoch: 6| Step: 11
Training loss: 2.4331116676330566
Validation loss: 2.114846428235372

Epoch: 6| Step: 12
Training loss: 2.1557374000549316
Validation loss: 2.120752294858297

Epoch: 6| Step: 13
Training loss: 2.227696180343628
Validation loss: 2.1346962054570517

Epoch: 10| Step: 0
Training loss: 1.9513646364212036
Validation loss: 2.1309827764829

Epoch: 6| Step: 1
Training loss: 1.5399601459503174
Validation loss: 2.136957903703054

Epoch: 6| Step: 2
Training loss: 1.9284733533859253
Validation loss: 2.117857257525126

Epoch: 6| Step: 3
Training loss: 2.3772802352905273
Validation loss: 2.121745685736338

Epoch: 6| Step: 4
Training loss: 2.2943038940429688
Validation loss: 2.100628077983856

Epoch: 6| Step: 5
Training loss: 2.113431453704834
Validation loss: 2.1231984893480935

Epoch: 6| Step: 6
Training loss: 1.6997840404510498
Validation loss: 2.1402688026428223

Epoch: 6| Step: 7
Training loss: 1.8999710083007812
Validation loss: 2.1216887633005777

Epoch: 6| Step: 8
Training loss: 2.3106417655944824
Validation loss: 2.097450236479441

Epoch: 6| Step: 9
Training loss: 1.8245031833648682
Validation loss: 2.112381557623545

Epoch: 6| Step: 10
Training loss: 2.8212497234344482
Validation loss: 2.1130247712135315

Epoch: 6| Step: 11
Training loss: 2.2399916648864746
Validation loss: 2.1209944685300193

Epoch: 6| Step: 12
Training loss: 1.5785558223724365
Validation loss: 2.119021018346151

Epoch: 6| Step: 13
Training loss: 2.3600454330444336
Validation loss: 2.1173922618230185

Epoch: 11| Step: 0
Training loss: 2.436171531677246
Validation loss: 2.1221832235654197

Epoch: 6| Step: 1
Training loss: 2.505801200866699
Validation loss: 2.1132182677586875

Epoch: 6| Step: 2
Training loss: 1.6484735012054443
Validation loss: 2.1058417757352195

Epoch: 6| Step: 3
Training loss: 2.728548288345337
Validation loss: 2.094197909037272

Epoch: 6| Step: 4
Training loss: 1.8843117952346802
Validation loss: 2.0942309896151223

Epoch: 6| Step: 5
Training loss: 1.7594215869903564
Validation loss: 2.112880051136017

Epoch: 6| Step: 6
Training loss: 2.7180063724517822
Validation loss: 2.108472764492035

Epoch: 6| Step: 7
Training loss: 1.9017648696899414
Validation loss: 2.115631580352783

Epoch: 6| Step: 8
Training loss: 1.9685031175613403
Validation loss: 2.1058586835861206

Epoch: 6| Step: 9
Training loss: 1.4235568046569824
Validation loss: 2.112286845842997

Epoch: 6| Step: 10
Training loss: 1.6472463607788086
Validation loss: 2.1322808464368186

Epoch: 6| Step: 11
Training loss: 2.3766732215881348
Validation loss: 2.122431536515554

Epoch: 6| Step: 12
Training loss: 2.142568349838257
Validation loss: 2.1036490400632224

Epoch: 6| Step: 13
Training loss: 1.6735014915466309
Validation loss: 2.0974684158960977

Epoch: 12| Step: 0
Training loss: 2.1543526649475098
Validation loss: 2.095427374045054

Epoch: 6| Step: 1
Training loss: 1.359560251235962
Validation loss: 2.0858753522237143

Epoch: 6| Step: 2
Training loss: 1.688459038734436
Validation loss: 2.1023650964101157

Epoch: 6| Step: 3
Training loss: 2.522029399871826
Validation loss: 2.099815766016642

Epoch: 6| Step: 4
Training loss: 2.3292603492736816
Validation loss: 2.1014100909233093

Epoch: 6| Step: 5
Training loss: 1.5502688884735107
Validation loss: 2.102139929930369

Epoch: 6| Step: 6
Training loss: 1.7871779203414917
Validation loss: 2.0986156463623047

Epoch: 6| Step: 7
Training loss: 2.205331325531006
Validation loss: 2.1226921876271567

Epoch: 6| Step: 8
Training loss: 2.0588879585266113
Validation loss: 2.103180686632792

Epoch: 6| Step: 9
Training loss: 1.6450930833816528
Validation loss: 2.1151960690816245

Epoch: 6| Step: 10
Training loss: 2.8514223098754883
Validation loss: 2.0875980059305825

Epoch: 6| Step: 11
Training loss: 1.9702849388122559
Validation loss: 2.0861701567967734

Epoch: 6| Step: 12
Training loss: 3.0678813457489014
Validation loss: 2.1161535183588662

Epoch: 6| Step: 13
Training loss: 1.4149421453475952
Validation loss: 2.1171990235646567

Epoch: 13| Step: 0
Training loss: 1.533933162689209
Validation loss: 2.1042065024375916

Epoch: 6| Step: 1
Training loss: 2.2443535327911377
Validation loss: 2.089589993158976

Epoch: 6| Step: 2
Training loss: 1.8966859579086304
Validation loss: 2.0857619841893515

Epoch: 6| Step: 3
Training loss: 2.4323954582214355
Validation loss: 2.107989807923635

Epoch: 6| Step: 4
Training loss: 1.5932128429412842
Validation loss: 2.09492160876592

Epoch: 6| Step: 5
Training loss: 2.1371359825134277
Validation loss: 2.081607242425283

Epoch: 6| Step: 6
Training loss: 1.786764144897461
Validation loss: 2.098731299241384

Epoch: 6| Step: 7
Training loss: 1.8641905784606934
Validation loss: 2.086095452308655

Epoch: 6| Step: 8
Training loss: 1.772573471069336
Validation loss: 2.0883249243100486

Epoch: 6| Step: 9
Training loss: 2.202913999557495
Validation loss: 2.104577422142029

Epoch: 6| Step: 10
Training loss: 2.932830810546875
Validation loss: 2.1019945542017617

Epoch: 6| Step: 11
Training loss: 2.60733962059021
Validation loss: 2.109771807988485

Epoch: 6| Step: 12
Training loss: 2.0368833541870117
Validation loss: 2.1163744926452637

Epoch: 6| Step: 13
Training loss: 1.5636916160583496
Validation loss: 2.092070996761322

Epoch: 14| Step: 0
Training loss: 2.0361645221710205
Validation loss: 2.103545844554901

Epoch: 6| Step: 1
Training loss: 2.5854294300079346
Validation loss: 2.0774596532185874

Epoch: 6| Step: 2
Training loss: 2.0518174171447754
Validation loss: 2.0955292185147605

Epoch: 6| Step: 3
Training loss: 1.6107887029647827
Validation loss: 2.0942874352137246

Epoch: 6| Step: 4
Training loss: 2.203242778778076
Validation loss: 2.107642908891042

Epoch: 6| Step: 5
Training loss: 1.9381544589996338
Validation loss: 2.095625877380371

Epoch: 6| Step: 6
Training loss: 2.1759238243103027
Validation loss: 2.0653459628423056

Epoch: 6| Step: 7
Training loss: 1.9183950424194336
Validation loss: 2.096684992313385

Epoch: 6| Step: 8
Training loss: 2.1240344047546387
Validation loss: 2.0644576152165732

Epoch: 6| Step: 9
Training loss: 2.2660651206970215
Validation loss: 2.091298838456472

Epoch: 6| Step: 10
Training loss: 1.8912932872772217
Validation loss: 2.0916857520739236

Epoch: 6| Step: 11
Training loss: 1.978003978729248
Validation loss: 2.0775418678919473

Epoch: 6| Step: 12
Training loss: 2.4871511459350586
Validation loss: 2.0977490345637

Epoch: 6| Step: 13
Training loss: 1.5389037132263184
Validation loss: 2.1004144748051963

Epoch: 15| Step: 0
Training loss: 2.109837055206299
Validation loss: 2.0970242818196616

Epoch: 6| Step: 1
Training loss: 2.0232646465301514
Validation loss: 2.0983302195866904

Epoch: 6| Step: 2
Training loss: 1.979283094406128
Validation loss: 2.0847925941149392

Epoch: 6| Step: 3
Training loss: 2.1767518520355225
Validation loss: 2.0763915181159973

Epoch: 6| Step: 4
Training loss: 1.743659496307373
Validation loss: 2.099763035774231

Epoch: 6| Step: 5
Training loss: 2.4949820041656494
Validation loss: 2.086591362953186

Epoch: 6| Step: 6
Training loss: 1.6750695705413818
Validation loss: 2.0884954929351807

Epoch: 6| Step: 7
Training loss: 2.582848072052002
Validation loss: 2.0882436434427896

Epoch: 6| Step: 8
Training loss: 2.389373779296875
Validation loss: 2.0917266408602395

Epoch: 6| Step: 9
Training loss: 1.2919549942016602
Validation loss: 2.097486654917399

Epoch: 6| Step: 10
Training loss: 2.1604719161987305
Validation loss: 2.0953510800997415

Epoch: 6| Step: 11
Training loss: 1.6146653890609741
Validation loss: 2.0840646624565125

Epoch: 6| Step: 12
Training loss: 2.429225444793701
Validation loss: 2.078574458758036

Epoch: 6| Step: 13
Training loss: 1.8004539012908936
Validation loss: 2.0934645533561707

Epoch: 16| Step: 0
Training loss: 2.7312445640563965
Validation loss: 2.072017709414164

Epoch: 6| Step: 1
Training loss: 1.2248473167419434
Validation loss: 2.0769469936688743

Epoch: 6| Step: 2
Training loss: 2.3491098880767822
Validation loss: 2.0921173890431723

Epoch: 6| Step: 3
Training loss: 2.686025857925415
Validation loss: 2.07129168510437

Epoch: 6| Step: 4
Training loss: 1.946160078048706
Validation loss: 2.0875823895136514

Epoch: 6| Step: 5
Training loss: 1.8366152048110962
Validation loss: 2.057813803354899

Epoch: 6| Step: 6
Training loss: 2.0435731410980225
Validation loss: 2.0763758222262063

Epoch: 6| Step: 7
Training loss: 1.5780280828475952
Validation loss: 2.0715922117233276

Epoch: 6| Step: 8
Training loss: 1.7764699459075928
Validation loss: 2.0748443603515625

Epoch: 6| Step: 9
Training loss: 1.7670819759368896
Validation loss: 2.0692856311798096

Epoch: 6| Step: 10
Training loss: 2.356353282928467
Validation loss: 2.0593006014823914

Epoch: 6| Step: 11
Training loss: 1.4569520950317383
Validation loss: 2.0695226192474365

Epoch: 6| Step: 12
Training loss: 1.8259245157241821
Validation loss: 2.089267452557882

Epoch: 6| Step: 13
Training loss: 2.9555675983428955
Validation loss: 2.07107683022817

Epoch: 17| Step: 0
Training loss: 1.6603041887283325
Validation loss: 2.080745816230774

Epoch: 6| Step: 1
Training loss: 1.9051735401153564
Validation loss: 2.077287793159485

Epoch: 6| Step: 2
Training loss: 2.3901376724243164
Validation loss: 2.071101665496826

Epoch: 6| Step: 3
Training loss: 1.75823974609375
Validation loss: 2.0804173946380615

Epoch: 6| Step: 4
Training loss: 1.8357023000717163
Validation loss: 2.076357126235962

Epoch: 6| Step: 5
Training loss: 2.5637004375457764
Validation loss: 2.083528677622477

Epoch: 6| Step: 6
Training loss: 2.2796287536621094
Validation loss: 2.095287005106608

Epoch: 6| Step: 7
Training loss: 1.8494515419006348
Validation loss: 2.0844770868619285

Epoch: 6| Step: 8
Training loss: 1.7111334800720215
Validation loss: 2.0868515372276306

Epoch: 6| Step: 9
Training loss: 2.3516175746917725
Validation loss: 2.0802104671796164

Epoch: 6| Step: 10
Training loss: 1.918306827545166
Validation loss: 2.092424154281616

Epoch: 6| Step: 11
Training loss: 1.9509952068328857
Validation loss: 2.0832300980885825

Epoch: 6| Step: 12
Training loss: 1.3194200992584229
Validation loss: 2.082387844721476

Epoch: 6| Step: 13
Training loss: 2.8746328353881836
Validation loss: 2.0734875798225403

Epoch: 18| Step: 0
Training loss: 1.7603267431259155
Validation loss: 2.0666972597440085

Epoch: 6| Step: 1
Training loss: 1.723073959350586
Validation loss: 2.0575353105862937

Epoch: 6| Step: 2
Training loss: 2.404911994934082
Validation loss: 2.081355035305023

Epoch: 6| Step: 3
Training loss: 1.9837524890899658
Validation loss: 2.0607725977897644

Epoch: 6| Step: 4
Training loss: 1.3441641330718994
Validation loss: 2.070471624533335

Epoch: 6| Step: 5
Training loss: 2.0446958541870117
Validation loss: 2.0730693340301514

Epoch: 6| Step: 6
Training loss: 2.2624149322509766
Validation loss: 2.0723771850268045

Epoch: 6| Step: 7
Training loss: 2.3864564895629883
Validation loss: 2.0572099089622498

Epoch: 6| Step: 8
Training loss: 2.4317069053649902
Validation loss: 2.068028767903646

Epoch: 6| Step: 9
Training loss: 2.1623592376708984
Validation loss: 2.0855656266212463

Epoch: 6| Step: 10
Training loss: 1.8260703086853027
Validation loss: 2.103040635585785

Epoch: 6| Step: 11
Training loss: 1.716808795928955
Validation loss: 2.0753175616264343

Epoch: 6| Step: 12
Training loss: 2.1693055629730225
Validation loss: 2.0751922925313315

Epoch: 6| Step: 13
Training loss: 2.063164234161377
Validation loss: 2.0709132154782615

Epoch: 19| Step: 0
Training loss: 2.868518352508545
Validation loss: 2.056941111882528

Epoch: 6| Step: 1
Training loss: 1.7968196868896484
Validation loss: 2.0719881455103555

Epoch: 6| Step: 2
Training loss: 1.9930293560028076
Validation loss: 2.075295845667521

Epoch: 6| Step: 3
Training loss: 1.5093085765838623
Validation loss: 2.061524232228597

Epoch: 6| Step: 4
Training loss: 2.056396484375
Validation loss: 2.075798730055491

Epoch: 6| Step: 5
Training loss: 2.3743767738342285
Validation loss: 2.049812893072764

Epoch: 6| Step: 6
Training loss: 1.7824203968048096
Validation loss: 2.048617660999298

Epoch: 6| Step: 7
Training loss: 1.7703853845596313
Validation loss: 2.06106964747111

Epoch: 6| Step: 8
Training loss: 2.087911367416382
Validation loss: 2.049694408973058

Epoch: 6| Step: 9
Training loss: 2.4424314498901367
Validation loss: 2.07218990723292

Epoch: 6| Step: 10
Training loss: 1.7963383197784424
Validation loss: 2.060875117778778

Epoch: 6| Step: 11
Training loss: 2.0926482677459717
Validation loss: 2.062854528427124

Epoch: 6| Step: 12
Training loss: 1.2785465717315674
Validation loss: 2.0683301091194153

Epoch: 6| Step: 13
Training loss: 2.355788230895996
Validation loss: 2.053687036037445

Epoch: 20| Step: 0
Training loss: 1.5875391960144043
Validation loss: 2.0578020215034485

Epoch: 6| Step: 1
Training loss: 1.7340391874313354
Validation loss: 2.0590616861979165

Epoch: 6| Step: 2
Training loss: 2.442507266998291
Validation loss: 2.0639740029970803

Epoch: 6| Step: 3
Training loss: 1.785001277923584
Validation loss: 2.052792946497599

Epoch: 6| Step: 4
Training loss: 2.405179738998413
Validation loss: 2.056858539581299

Epoch: 6| Step: 5
Training loss: 1.3766971826553345
Validation loss: 2.0651669104894004

Epoch: 6| Step: 6
Training loss: 2.282728672027588
Validation loss: 2.0513306061426797

Epoch: 6| Step: 7
Training loss: 2.965360164642334
Validation loss: 2.0809466441472373

Epoch: 6| Step: 8
Training loss: 1.8534436225891113
Validation loss: 2.0694196621576944

Epoch: 6| Step: 9
Training loss: 2.239328384399414
Validation loss: 2.05277556180954

Epoch: 6| Step: 10
Training loss: 2.0027239322662354
Validation loss: 2.068016529083252

Epoch: 6| Step: 11
Training loss: 1.6030867099761963
Validation loss: 2.0567670663197837

Epoch: 6| Step: 12
Training loss: 1.9175857305526733
Validation loss: 2.044903298219045

Epoch: 6| Step: 13
Training loss: 1.956521987915039
Validation loss: 2.0494388739267984

Epoch: 21| Step: 0
Training loss: 2.2265846729278564
Validation loss: 2.055492560068766

Epoch: 6| Step: 1
Training loss: 2.440323829650879
Validation loss: 2.065584381421407

Epoch: 6| Step: 2
Training loss: 1.8818117380142212
Validation loss: 2.0565001368522644

Epoch: 6| Step: 3
Training loss: 1.974130630493164
Validation loss: 2.0513996481895447

Epoch: 6| Step: 4
Training loss: 1.9954560995101929
Validation loss: 2.065629154443741

Epoch: 6| Step: 5
Training loss: 2.28403377532959
Validation loss: 2.066046357154846

Epoch: 6| Step: 6
Training loss: 2.15649151802063
Validation loss: 2.069863577683767

Epoch: 6| Step: 7
Training loss: 1.7580227851867676
Validation loss: 2.056530475616455

Epoch: 6| Step: 8
Training loss: 1.4678316116333008
Validation loss: 2.0494420727094016

Epoch: 6| Step: 9
Training loss: 2.160114288330078
Validation loss: 2.0462399323781333

Epoch: 6| Step: 10
Training loss: 1.6547260284423828
Validation loss: 2.0419511993726096

Epoch: 6| Step: 11
Training loss: 2.5610454082489014
Validation loss: 2.0331976612408957

Epoch: 6| Step: 12
Training loss: 1.6557152271270752
Validation loss: 2.060565630594889

Epoch: 6| Step: 13
Training loss: 1.7678735256195068
Validation loss: 2.0571463902791343

Epoch: 22| Step: 0
Training loss: 1.816920518875122
Validation loss: 2.0642411510149636

Epoch: 6| Step: 1
Training loss: 1.9112200736999512
Validation loss: 2.0828016797701516

Epoch: 6| Step: 2
Training loss: 1.8789312839508057
Validation loss: 2.0717087984085083

Epoch: 6| Step: 3
Training loss: 2.0973269939422607
Validation loss: 2.068641106287638

Epoch: 6| Step: 4
Training loss: 1.9997221231460571
Validation loss: 2.0489944418271384

Epoch: 6| Step: 5
Training loss: 1.18468177318573
Validation loss: 2.049600680669149

Epoch: 6| Step: 6
Training loss: 1.2508152723312378
Validation loss: 2.054309884707133

Epoch: 6| Step: 7
Training loss: 1.9633207321166992
Validation loss: 2.0758605202039084

Epoch: 6| Step: 8
Training loss: 2.4244377613067627
Validation loss: 2.0410598119099936

Epoch: 6| Step: 9
Training loss: 2.0326972007751465
Validation loss: 2.048623959223429

Epoch: 6| Step: 10
Training loss: 2.856088161468506
Validation loss: 2.0602160493532815

Epoch: 6| Step: 11
Training loss: 2.534491539001465
Validation loss: 2.0554785331090293

Epoch: 6| Step: 12
Training loss: 2.1899285316467285
Validation loss: 2.07183184226354

Epoch: 6| Step: 13
Training loss: 1.8500992059707642
Validation loss: 2.0378878712654114

Epoch: 23| Step: 0
Training loss: 1.7443288564682007
Validation loss: 2.0487334728240967

Epoch: 6| Step: 1
Training loss: 1.9846709966659546
Validation loss: 2.066459000110626

Epoch: 6| Step: 2
Training loss: 2.675459384918213
Validation loss: 2.061534066994985

Epoch: 6| Step: 3
Training loss: 2.4576947689056396
Validation loss: 2.06548935174942

Epoch: 6| Step: 4
Training loss: 1.7089382410049438
Validation loss: 2.053567190965017

Epoch: 6| Step: 5
Training loss: 2.306870937347412
Validation loss: 2.0583006540934243

Epoch: 6| Step: 6
Training loss: 1.3328640460968018
Validation loss: 2.065411369005839

Epoch: 6| Step: 7
Training loss: 1.7199926376342773
Validation loss: 2.071710209051768

Epoch: 6| Step: 8
Training loss: 2.1255922317504883
Validation loss: 2.0559958020846048

Epoch: 6| Step: 9
Training loss: 2.504847526550293
Validation loss: 2.0516278942426047

Epoch: 6| Step: 10
Training loss: 2.0293326377868652
Validation loss: 2.0592780709266663

Epoch: 6| Step: 11
Training loss: 2.046304941177368
Validation loss: 2.0606329639752707

Epoch: 6| Step: 12
Training loss: 1.7960968017578125
Validation loss: 2.046596944332123

Epoch: 6| Step: 13
Training loss: 1.5379492044448853
Validation loss: 2.053398927052816

Epoch: 24| Step: 0
Training loss: 1.6040889024734497
Validation loss: 2.0612012147903442

Epoch: 6| Step: 1
Training loss: 1.9444025754928589
Validation loss: 2.0273887316385903

Epoch: 6| Step: 2
Training loss: 2.009840250015259
Validation loss: 2.031672239303589

Epoch: 6| Step: 3
Training loss: 1.949000358581543
Validation loss: 2.0432053407033286

Epoch: 6| Step: 4
Training loss: 1.2694073915481567
Validation loss: 2.049063563346863

Epoch: 6| Step: 5
Training loss: 1.89108407497406
Validation loss: 2.0444616874059043

Epoch: 6| Step: 6
Training loss: 2.958016872406006
Validation loss: 2.0590880115826926

Epoch: 6| Step: 7
Training loss: 1.7858980894088745
Validation loss: 2.0528461933135986

Epoch: 6| Step: 8
Training loss: 2.0895590782165527
Validation loss: 2.0235045154889426

Epoch: 6| Step: 9
Training loss: 1.804290533065796
Validation loss: 2.043438812096914

Epoch: 6| Step: 10
Training loss: 2.598606586456299
Validation loss: 2.05294676621755

Epoch: 6| Step: 11
Training loss: 2.2697160243988037
Validation loss: 2.0384215315183005

Epoch: 6| Step: 12
Training loss: 1.8064582347869873
Validation loss: 2.03764279683431

Epoch: 6| Step: 13
Training loss: 1.8919047117233276
Validation loss: 2.0637905399004617

Epoch: 25| Step: 0
Training loss: 2.356064796447754
Validation loss: 2.0323046445846558

Epoch: 6| Step: 1
Training loss: 2.1314854621887207
Validation loss: 2.0286247928937278

Epoch: 6| Step: 2
Training loss: 1.943225383758545
Validation loss: 2.015342871348063

Epoch: 6| Step: 3
Training loss: 2.3841023445129395
Validation loss: 2.043360630671183

Epoch: 6| Step: 4
Training loss: 1.502515196800232
Validation loss: 2.0437432130177817

Epoch: 6| Step: 5
Training loss: 2.1037189960479736
Validation loss: 2.0461222529411316

Epoch: 6| Step: 6
Training loss: 1.487227439880371
Validation loss: 2.0537959933280945

Epoch: 6| Step: 7
Training loss: 1.676963210105896
Validation loss: 2.0437960624694824

Epoch: 6| Step: 8
Training loss: 2.2123377323150635
Validation loss: 2.0584137439727783

Epoch: 6| Step: 9
Training loss: 2.494810104370117
Validation loss: 2.043829023838043

Epoch: 6| Step: 10
Training loss: 1.761523962020874
Validation loss: 2.04041987657547

Epoch: 6| Step: 11
Training loss: 1.7339739799499512
Validation loss: 2.039383669694265

Epoch: 6| Step: 12
Training loss: 1.3816618919372559
Validation loss: 2.035928269227346

Epoch: 6| Step: 13
Training loss: 2.8053817749023438
Validation loss: 2.0386688510576882

Epoch: 26| Step: 0
Training loss: 1.8574501276016235
Validation loss: 2.0414071877797446

Epoch: 6| Step: 1
Training loss: 1.8878629207611084
Validation loss: 2.0585737228393555

Epoch: 6| Step: 2
Training loss: 1.7152453660964966
Validation loss: 2.042312761147817

Epoch: 6| Step: 3
Training loss: 2.018765449523926
Validation loss: 2.073518772919973

Epoch: 6| Step: 4
Training loss: 1.866830587387085
Validation loss: 2.0855212608973184

Epoch: 6| Step: 5
Training loss: 1.855180263519287
Validation loss: 2.0737129052480063

Epoch: 6| Step: 6
Training loss: 1.8411614894866943
Validation loss: 2.0702280402183533

Epoch: 6| Step: 7
Training loss: 2.6046485900878906
Validation loss: 2.054386874039968

Epoch: 6| Step: 8
Training loss: 1.9327125549316406
Validation loss: 2.066465159257253

Epoch: 6| Step: 9
Training loss: 2.125283718109131
Validation loss: 2.0560866792996726

Epoch: 6| Step: 10
Training loss: 1.7103625535964966
Validation loss: 2.0594778458277383

Epoch: 6| Step: 11
Training loss: 1.544661283493042
Validation loss: 2.0448150634765625

Epoch: 6| Step: 12
Training loss: 1.9079663753509521
Validation loss: 2.0474370320638022

Epoch: 6| Step: 13
Training loss: 2.9391934871673584
Validation loss: 2.0547289649645486

Epoch: 27| Step: 0
Training loss: 2.31379771232605
Validation loss: 2.0541250308354697

Epoch: 6| Step: 1
Training loss: 2.0305979251861572
Validation loss: 2.046668589115143

Epoch: 6| Step: 2
Training loss: 1.5303428173065186
Validation loss: 2.0350916981697083

Epoch: 6| Step: 3
Training loss: 2.4562604427337646
Validation loss: 2.0275349219640098

Epoch: 6| Step: 4
Training loss: 2.3800997734069824
Validation loss: 2.0380620757738748

Epoch: 6| Step: 5
Training loss: 1.8793284893035889
Validation loss: 2.0280348857243857

Epoch: 6| Step: 6
Training loss: 2.2042593955993652
Validation loss: 2.0422568718592324

Epoch: 6| Step: 7
Training loss: 2.2634315490722656
Validation loss: 2.0399238665898642

Epoch: 6| Step: 8
Training loss: 1.5370230674743652
Validation loss: 2.0358872016270957

Epoch: 6| Step: 9
Training loss: 2.074709177017212
Validation loss: 2.0328049063682556

Epoch: 6| Step: 10
Training loss: 1.7102296352386475
Validation loss: 2.052778124809265

Epoch: 6| Step: 11
Training loss: 1.979249358177185
Validation loss: 2.034326513608297

Epoch: 6| Step: 12
Training loss: 2.119628429412842
Validation loss: 2.0395761926968894

Epoch: 6| Step: 13
Training loss: 1.4838238954544067
Validation loss: 2.044996519883474

Epoch: 28| Step: 0
Training loss: 2.531669855117798
Validation loss: 2.0564148823420205

Epoch: 6| Step: 1
Training loss: 1.8830019235610962
Validation loss: 2.0451937715212503

Epoch: 6| Step: 2
Training loss: 1.8060057163238525
Validation loss: 2.0365729133288064

Epoch: 6| Step: 3
Training loss: 1.7615225315093994
Validation loss: 2.0358224511146545

Epoch: 6| Step: 4
Training loss: 1.9298374652862549
Validation loss: 2.043674329916636

Epoch: 6| Step: 5
Training loss: 2.176964521408081
Validation loss: 2.0250203609466553

Epoch: 6| Step: 6
Training loss: 2.2007384300231934
Validation loss: 2.0520853996276855

Epoch: 6| Step: 7
Training loss: 1.9796180725097656
Validation loss: 2.0347431898117065

Epoch: 6| Step: 8
Training loss: 2.217928886413574
Validation loss: 2.0404746929804483

Epoch: 6| Step: 9
Training loss: 1.504847526550293
Validation loss: 2.0287018616994223

Epoch: 6| Step: 10
Training loss: 1.6143304109573364
Validation loss: 2.031873106956482

Epoch: 6| Step: 11
Training loss: 2.147050380706787
Validation loss: 2.034453888734182

Epoch: 6| Step: 12
Training loss: 2.2579987049102783
Validation loss: 2.0508193969726562

Epoch: 6| Step: 13
Training loss: 1.724682092666626
Validation loss: 2.036282499631246

Epoch: 29| Step: 0
Training loss: 2.5055594444274902
Validation loss: 2.0602625012397766

Epoch: 6| Step: 1
Training loss: 1.159454584121704
Validation loss: 2.0459090073903403

Epoch: 6| Step: 2
Training loss: 2.0896053314208984
Validation loss: 2.065564771493276

Epoch: 6| Step: 3
Training loss: 1.4831147193908691
Validation loss: 2.078150987625122

Epoch: 6| Step: 4
Training loss: 2.0717945098876953
Validation loss: 2.0731236735979715

Epoch: 6| Step: 5
Training loss: 1.6362146139144897
Validation loss: 2.089135706424713

Epoch: 6| Step: 6
Training loss: 2.024962902069092
Validation loss: 2.1157131592432656

Epoch: 6| Step: 7
Training loss: 1.7032814025878906
Validation loss: 2.1359436313311257

Epoch: 6| Step: 8
Training loss: 1.7880518436431885
Validation loss: 2.1064764857292175

Epoch: 6| Step: 9
Training loss: 1.4909663200378418
Validation loss: 2.098718583583832

Epoch: 6| Step: 10
Training loss: 2.256638765335083
Validation loss: 2.0956438382466636

Epoch: 6| Step: 11
Training loss: 3.2045187950134277
Validation loss: 2.1047221223513284

Epoch: 6| Step: 12
Training loss: 1.9292278289794922
Validation loss: 2.0655932426452637

Epoch: 6| Step: 13
Training loss: 2.579806089401245
Validation loss: 2.0455549558003745

Epoch: 30| Step: 0
Training loss: 1.7571392059326172
Validation loss: 2.044040620326996

Epoch: 6| Step: 1
Training loss: 1.9073574542999268
Validation loss: 2.044159551461538

Epoch: 6| Step: 2
Training loss: 1.9677555561065674
Validation loss: 2.0266827940940857

Epoch: 6| Step: 3
Training loss: 1.8339474201202393
Validation loss: 2.0484044353167215

Epoch: 6| Step: 4
Training loss: 1.424429178237915
Validation loss: 2.0419883728027344

Epoch: 6| Step: 5
Training loss: 1.9886294603347778
Validation loss: 2.0381215612093606

Epoch: 6| Step: 6
Training loss: 2.196489095687866
Validation loss: 2.0394625067710876

Epoch: 6| Step: 7
Training loss: 1.922622561454773
Validation loss: 2.050455073515574

Epoch: 6| Step: 8
Training loss: 2.3747777938842773
Validation loss: 2.046332279841105

Epoch: 6| Step: 9
Training loss: 2.339223861694336
Validation loss: 2.067848483721415

Epoch: 6| Step: 10
Training loss: 1.8468725681304932
Validation loss: 2.025936702887217

Epoch: 6| Step: 11
Training loss: 2.336143970489502
Validation loss: 2.0282996892929077

Epoch: 6| Step: 12
Training loss: 2.65315842628479
Validation loss: 2.0290473103523254

Epoch: 6| Step: 13
Training loss: 1.5826387405395508
Validation loss: 2.033165613810221

Epoch: 31| Step: 0
Training loss: 2.413116693496704
Validation loss: 2.0298476219177246

Epoch: 6| Step: 1
Training loss: 1.650855302810669
Validation loss: 2.0242678125699363

Epoch: 6| Step: 2
Training loss: 1.619117259979248
Validation loss: 2.0086079438527427

Epoch: 6| Step: 3
Training loss: 1.6265312433242798
Validation loss: 2.0410985350608826

Epoch: 6| Step: 4
Training loss: 1.9722925424575806
Validation loss: 2.0393275221188865

Epoch: 6| Step: 5
Training loss: 1.745165467262268
Validation loss: 2.0525044401486716

Epoch: 6| Step: 6
Training loss: 1.786510944366455
Validation loss: 2.0500934521357217

Epoch: 6| Step: 7
Training loss: 1.902198314666748
Validation loss: 2.055032253265381

Epoch: 6| Step: 8
Training loss: 2.839848041534424
Validation loss: 2.037432233492533

Epoch: 6| Step: 9
Training loss: 1.4184834957122803
Validation loss: 2.034594019254049

Epoch: 6| Step: 10
Training loss: 1.9405019283294678
Validation loss: 2.0635850032170615

Epoch: 6| Step: 11
Training loss: 2.353609085083008
Validation loss: 2.0572550892829895

Epoch: 6| Step: 12
Training loss: 1.4150276184082031
Validation loss: 2.067824900150299

Epoch: 6| Step: 13
Training loss: 2.7910666465759277
Validation loss: 2.05482280254364

Epoch: 32| Step: 0
Training loss: 1.865981936454773
Validation loss: 2.0603527228037515

Epoch: 6| Step: 1
Training loss: 2.349435806274414
Validation loss: 2.0701060692469277

Epoch: 6| Step: 2
Training loss: 2.5341830253601074
Validation loss: 2.0721547603607178

Epoch: 6| Step: 3
Training loss: 1.267791986465454
Validation loss: 2.063749154408773

Epoch: 6| Step: 4
Training loss: 2.122579336166382
Validation loss: 2.0622801780700684

Epoch: 6| Step: 5
Training loss: 1.6231961250305176
Validation loss: 2.050130248069763

Epoch: 6| Step: 6
Training loss: 1.6808712482452393
Validation loss: 2.030421475569407

Epoch: 6| Step: 7
Training loss: 1.908847689628601
Validation loss: 2.0404269099235535

Epoch: 6| Step: 8
Training loss: 2.221734046936035
Validation loss: 2.0371323227882385

Epoch: 6| Step: 9
Training loss: 2.648322105407715
Validation loss: 2.0256978273391724

Epoch: 6| Step: 10
Training loss: 1.8896734714508057
Validation loss: 2.0279316703478494

Epoch: 6| Step: 11
Training loss: 1.8579905033111572
Validation loss: 2.03058131535848

Epoch: 6| Step: 12
Training loss: 1.7823508977890015
Validation loss: 2.0174756248792014

Epoch: 6| Step: 13
Training loss: 1.778282880783081
Validation loss: 2.0261409481366477

Epoch: 33| Step: 0
Training loss: 2.100494861602783
Validation loss: 2.014614979426066

Epoch: 6| Step: 1
Training loss: 2.4351391792297363
Validation loss: 2.039864500363668

Epoch: 6| Step: 2
Training loss: 2.361865282058716
Validation loss: 2.027620255947113

Epoch: 6| Step: 3
Training loss: 2.1851963996887207
Validation loss: 2.0212963819503784

Epoch: 6| Step: 4
Training loss: 1.4849623441696167
Validation loss: 2.027991612752279

Epoch: 6| Step: 5
Training loss: 2.063565254211426
Validation loss: 2.0171135663986206

Epoch: 6| Step: 6
Training loss: 2.4844613075256348
Validation loss: 2.0218655268351235

Epoch: 6| Step: 7
Training loss: 1.872615098953247
Validation loss: 2.0197520653406777

Epoch: 6| Step: 8
Training loss: 1.761466145515442
Validation loss: 2.0417311787605286

Epoch: 6| Step: 9
Training loss: 2.2613840103149414
Validation loss: 2.0223235487937927

Epoch: 6| Step: 10
Training loss: 1.9058421850204468
Validation loss: 2.0377018253008523

Epoch: 6| Step: 11
Training loss: 1.584592342376709
Validation loss: 2.0601078271865845

Epoch: 6| Step: 12
Training loss: 1.4617094993591309
Validation loss: 2.047106603781382

Epoch: 6| Step: 13
Training loss: 1.7454835176467896
Validation loss: 2.0200862288475037

Epoch: 34| Step: 0
Training loss: 2.042473793029785
Validation loss: 2.0298723578453064

Epoch: 6| Step: 1
Training loss: 1.9563734531402588
Validation loss: 2.0552470485369363

Epoch: 6| Step: 2
Training loss: 1.0868022441864014
Validation loss: 2.0529068311055503

Epoch: 6| Step: 3
Training loss: 1.8819162845611572
Validation loss: 2.045884430408478

Epoch: 6| Step: 4
Training loss: 2.1219732761383057
Validation loss: 2.028397560119629

Epoch: 6| Step: 5
Training loss: 1.8036689758300781
Validation loss: 2.055833180745443

Epoch: 6| Step: 6
Training loss: 1.5172771215438843
Validation loss: 2.0391597747802734

Epoch: 6| Step: 7
Training loss: 1.654342532157898
Validation loss: 2.0274034341176352

Epoch: 6| Step: 8
Training loss: 2.569335699081421
Validation loss: 2.0348830620447793

Epoch: 6| Step: 9
Training loss: 1.8131905794143677
Validation loss: 2.0343491435050964

Epoch: 6| Step: 10
Training loss: 2.072312116622925
Validation loss: 2.0530009865760803

Epoch: 6| Step: 11
Training loss: 2.265265703201294
Validation loss: 2.0278273622194924

Epoch: 6| Step: 12
Training loss: 2.356990337371826
Validation loss: 2.023259401321411

Epoch: 6| Step: 13
Training loss: 2.326618194580078
Validation loss: 2.0416996677716575

Epoch: 35| Step: 0
Training loss: 1.76192307472229
Validation loss: 2.027261753877004

Epoch: 6| Step: 1
Training loss: 2.1863765716552734
Validation loss: 2.029986341794332

Epoch: 6| Step: 2
Training loss: 1.7210173606872559
Validation loss: 2.0182995796203613

Epoch: 6| Step: 3
Training loss: 2.4355435371398926
Validation loss: 2.029093106587728

Epoch: 6| Step: 4
Training loss: 2.1686205863952637
Validation loss: 2.030463377634684

Epoch: 6| Step: 5
Training loss: 1.7036317586898804
Validation loss: 2.025300661722819

Epoch: 6| Step: 6
Training loss: 1.8796106576919556
Validation loss: 2.0399209459622702

Epoch: 6| Step: 7
Training loss: 1.9181108474731445
Validation loss: 2.0294443368911743

Epoch: 6| Step: 8
Training loss: 1.582000494003296
Validation loss: 2.0464733640352883

Epoch: 6| Step: 9
Training loss: 1.9408869743347168
Validation loss: 2.0288631319999695

Epoch: 6| Step: 10
Training loss: 2.121670722961426
Validation loss: 2.037720282872518

Epoch: 6| Step: 11
Training loss: 1.6553871631622314
Validation loss: 2.027573267618815

Epoch: 6| Step: 12
Training loss: 2.4048991203308105
Validation loss: 2.043426752090454

Epoch: 6| Step: 13
Training loss: 1.775803804397583
Validation loss: 2.0339654088020325

Epoch: 36| Step: 0
Training loss: 1.9408044815063477
Validation loss: 2.036391317844391

Epoch: 6| Step: 1
Training loss: 1.660293698310852
Validation loss: 2.039680600166321

Epoch: 6| Step: 2
Training loss: 2.579261064529419
Validation loss: 2.041323165098826

Epoch: 6| Step: 3
Training loss: 1.3171923160552979
Validation loss: 2.035483658313751

Epoch: 6| Step: 4
Training loss: 2.233654260635376
Validation loss: 2.056147495905558

Epoch: 6| Step: 5
Training loss: 1.6229569911956787
Validation loss: 2.0573707222938538

Epoch: 6| Step: 6
Training loss: 2.189310073852539
Validation loss: 2.0455763141314187

Epoch: 6| Step: 7
Training loss: 2.0317423343658447
Validation loss: 2.0473191340764365

Epoch: 6| Step: 8
Training loss: 1.724872350692749
Validation loss: 2.0304343700408936

Epoch: 6| Step: 9
Training loss: 2.4433486461639404
Validation loss: 2.056404630343119

Epoch: 6| Step: 10
Training loss: 1.9779366254806519
Validation loss: 2.0649481415748596

Epoch: 6| Step: 11
Training loss: 1.9940392971038818
Validation loss: 2.082531531651815

Epoch: 6| Step: 12
Training loss: 1.9645359516143799
Validation loss: 2.0586947798728943

Epoch: 6| Step: 13
Training loss: 1.763635516166687
Validation loss: 2.0688268740971885

Epoch: 37| Step: 0
Training loss: 2.060307264328003
Validation loss: 2.033061603705088

Epoch: 6| Step: 1
Training loss: 1.1815028190612793
Validation loss: 2.066861708958944

Epoch: 6| Step: 2
Training loss: 2.0497405529022217
Validation loss: 2.056855003039042

Epoch: 6| Step: 3
Training loss: 2.0666518211364746
Validation loss: 2.0511425733566284

Epoch: 6| Step: 4
Training loss: 1.1969269514083862
Validation loss: 2.0288069446881614

Epoch: 6| Step: 5
Training loss: 1.9952595233917236
Validation loss: 2.0524110396703086

Epoch: 6| Step: 6
Training loss: 1.3908107280731201
Validation loss: 2.032440940539042

Epoch: 6| Step: 7
Training loss: 2.233290672302246
Validation loss: 2.0438468058904014

Epoch: 6| Step: 8
Training loss: 2.0185317993164062
Validation loss: 2.0196231404940286

Epoch: 6| Step: 9
Training loss: 1.7174835205078125
Validation loss: 2.025871475537618

Epoch: 6| Step: 10
Training loss: 2.4927754402160645
Validation loss: 2.034399410088857

Epoch: 6| Step: 11
Training loss: 1.8139927387237549
Validation loss: 2.026890198389689

Epoch: 6| Step: 12
Training loss: 2.7912347316741943
Validation loss: 2.0300873716672263

Epoch: 6| Step: 13
Training loss: 2.314276695251465
Validation loss: 2.0416321555773416

Epoch: 38| Step: 0
Training loss: 1.6908252239227295
Validation loss: 2.026467820008596

Epoch: 6| Step: 1
Training loss: 1.63125741481781
Validation loss: 2.021350065867106

Epoch: 6| Step: 2
Training loss: 2.498728036880493
Validation loss: 2.0235381921132407

Epoch: 6| Step: 3
Training loss: 2.0346975326538086
Validation loss: 2.0118941267331443

Epoch: 6| Step: 4
Training loss: 2.381434440612793
Validation loss: 2.021151542663574

Epoch: 6| Step: 5
Training loss: 1.5083038806915283
Validation loss: 2.0322147806485495

Epoch: 6| Step: 6
Training loss: 2.3290791511535645
Validation loss: 2.020347078641256

Epoch: 6| Step: 7
Training loss: 2.1734566688537598
Validation loss: 2.0211166739463806

Epoch: 6| Step: 8
Training loss: 1.533027172088623
Validation loss: 2.020652194817861

Epoch: 6| Step: 9
Training loss: 2.2456741333007812
Validation loss: 2.026193598906199

Epoch: 6| Step: 10
Training loss: 2.327531337738037
Validation loss: 2.056658148765564

Epoch: 6| Step: 11
Training loss: 1.5878405570983887
Validation loss: 2.047590951124827

Epoch: 6| Step: 12
Training loss: 1.274045705795288
Validation loss: 2.0340391794840493

Epoch: 6| Step: 13
Training loss: 2.076707601547241
Validation loss: 2.0587833325068154

Epoch: 39| Step: 0
Training loss: 1.7549341917037964
Validation loss: 2.0490337014198303

Epoch: 6| Step: 1
Training loss: 2.228285789489746
Validation loss: 2.0516940156618753

Epoch: 6| Step: 2
Training loss: 1.5241672992706299
Validation loss: 2.0635567903518677

Epoch: 6| Step: 3
Training loss: 1.979335069656372
Validation loss: 2.072036544481913

Epoch: 6| Step: 4
Training loss: 1.8489378690719604
Validation loss: 2.0609402855237327

Epoch: 6| Step: 5
Training loss: 2.0706968307495117
Validation loss: 2.0750144322713218

Epoch: 6| Step: 6
Training loss: 2.0251352787017822
Validation loss: 2.0625979701677957

Epoch: 6| Step: 7
Training loss: 1.8918709754943848
Validation loss: 2.044404089450836

Epoch: 6| Step: 8
Training loss: 1.7842373847961426
Validation loss: 2.0378490487734475

Epoch: 6| Step: 9
Training loss: 2.2033536434173584
Validation loss: 2.03436932961146

Epoch: 6| Step: 10
Training loss: 1.9359209537506104
Validation loss: 2.0350419084231057

Epoch: 6| Step: 11
Training loss: 2.4763243198394775
Validation loss: 2.0330140590667725

Epoch: 6| Step: 12
Training loss: 1.545633316040039
Validation loss: 2.0277071595191956

Epoch: 6| Step: 13
Training loss: 1.8766764402389526
Validation loss: 2.0575862725575766

Epoch: 40| Step: 0
Training loss: 1.9858901500701904
Validation loss: 2.032132148742676

Epoch: 6| Step: 1
Training loss: 1.655749797821045
Validation loss: 2.0141273140907288

Epoch: 6| Step: 2
Training loss: 1.7807965278625488
Validation loss: 2.0314348538716636

Epoch: 6| Step: 3
Training loss: 2.4482717514038086
Validation loss: 2.025543491045634

Epoch: 6| Step: 4
Training loss: 1.3895785808563232
Validation loss: 1.9996960163116455

Epoch: 6| Step: 5
Training loss: 1.9522767066955566
Validation loss: 2.036286473274231

Epoch: 6| Step: 6
Training loss: 2.57251238822937
Validation loss: 2.0409686962763467

Epoch: 6| Step: 7
Training loss: 2.00825572013855
Validation loss: 2.017792205015818

Epoch: 6| Step: 8
Training loss: 1.5473968982696533
Validation loss: 2.0212111671765647

Epoch: 6| Step: 9
Training loss: 2.2057900428771973
Validation loss: 2.021949271361033

Epoch: 6| Step: 10
Training loss: 1.5871531963348389
Validation loss: 2.0322407484054565

Epoch: 6| Step: 11
Training loss: 1.9479525089263916
Validation loss: 2.0363136728604636

Epoch: 6| Step: 12
Training loss: 1.9639924764633179
Validation loss: 2.0422358910242715

Epoch: 6| Step: 13
Training loss: 2.1623735427856445
Validation loss: 2.042960246404012

Epoch: 41| Step: 0
Training loss: 2.566981315612793
Validation loss: 2.049884617328644

Epoch: 6| Step: 1
Training loss: 1.4833288192749023
Validation loss: 2.0339780847231546

Epoch: 6| Step: 2
Training loss: 1.945434808731079
Validation loss: 2.0655147433280945

Epoch: 6| Step: 3
Training loss: 2.1841073036193848
Validation loss: 2.067290782928467

Epoch: 6| Step: 4
Training loss: 1.3193504810333252
Validation loss: 2.092568337917328

Epoch: 6| Step: 5
Training loss: 1.686436414718628
Validation loss: 2.0793176889419556

Epoch: 6| Step: 6
Training loss: 2.148629665374756
Validation loss: 2.0728861490885415

Epoch: 6| Step: 7
Training loss: 2.0950140953063965
Validation loss: 2.084132134914398

Epoch: 6| Step: 8
Training loss: 1.2608563899993896
Validation loss: 2.075151284535726

Epoch: 6| Step: 9
Training loss: 2.0092873573303223
Validation loss: 2.0523273944854736

Epoch: 6| Step: 10
Training loss: 2.56518292427063
Validation loss: 2.0064253211021423

Epoch: 6| Step: 11
Training loss: 1.8427939414978027
Validation loss: 2.029442479213079

Epoch: 6| Step: 12
Training loss: 1.9419971704483032
Validation loss: 2.0145333409309387

Epoch: 6| Step: 13
Training loss: 2.240262031555176
Validation loss: 2.0514021714528403

Epoch: 42| Step: 0
Training loss: 1.515341877937317
Validation loss: 2.0370941956837973

Epoch: 6| Step: 1
Training loss: 1.651240348815918
Validation loss: 2.0331636468569436

Epoch: 6| Step: 2
Training loss: 2.59951114654541
Validation loss: 2.008559306462606

Epoch: 6| Step: 3
Training loss: 1.8302128314971924
Validation loss: 2.032967189947764

Epoch: 6| Step: 4
Training loss: 2.352281093597412
Validation loss: 2.043531874815623

Epoch: 6| Step: 5
Training loss: 1.750943899154663
Validation loss: 2.033907473087311

Epoch: 6| Step: 6
Training loss: 1.8583619594573975
Validation loss: 2.0272833704948425

Epoch: 6| Step: 7
Training loss: 1.813169002532959
Validation loss: 2.0396164854367576

Epoch: 6| Step: 8
Training loss: 2.3030309677124023
Validation loss: 2.035276234149933

Epoch: 6| Step: 9
Training loss: 2.2244577407836914
Validation loss: 2.0401125947634378

Epoch: 6| Step: 10
Training loss: 1.9036426544189453
Validation loss: 2.0503093798955283

Epoch: 6| Step: 11
Training loss: 1.6838641166687012
Validation loss: 2.0283496379852295

Epoch: 6| Step: 12
Training loss: 2.136408567428589
Validation loss: 2.037877301375071

Epoch: 6| Step: 13
Training loss: 1.5745060443878174
Validation loss: 2.038540303707123

Epoch: 43| Step: 0
Training loss: 2.4229602813720703
Validation loss: 2.024066150188446

Epoch: 6| Step: 1
Training loss: 1.9025700092315674
Validation loss: 2.0448340574900308

Epoch: 6| Step: 2
Training loss: 1.8214528560638428
Validation loss: 2.0294388930002847

Epoch: 6| Step: 3
Training loss: 2.0771098136901855
Validation loss: 2.018722494443258

Epoch: 6| Step: 4
Training loss: 2.176314353942871
Validation loss: 2.0235291918118796

Epoch: 6| Step: 5
Training loss: 2.1468753814697266
Validation loss: 2.014161308606466

Epoch: 6| Step: 6
Training loss: 1.9929786920547485
Validation loss: 2.0379050374031067

Epoch: 6| Step: 7
Training loss: 2.171370029449463
Validation loss: 2.040515959262848

Epoch: 6| Step: 8
Training loss: 2.4007511138916016
Validation loss: 2.05189710855484

Epoch: 6| Step: 9
Training loss: 1.6939009428024292
Validation loss: 2.05252214272817

Epoch: 6| Step: 10
Training loss: 1.7854872941970825
Validation loss: 2.0530879696210227

Epoch: 6| Step: 11
Training loss: 1.7397868633270264
Validation loss: 2.0531048576037088

Epoch: 6| Step: 12
Training loss: 1.6200965642929077
Validation loss: 2.0414928197860718

Epoch: 6| Step: 13
Training loss: 1.4471526145935059
Validation loss: 2.0424848794937134

Epoch: 44| Step: 0
Training loss: 2.8084139823913574
Validation loss: 2.047712723414103

Epoch: 6| Step: 1
Training loss: 2.4488444328308105
Validation loss: 2.0386460423469543

Epoch: 6| Step: 2
Training loss: 1.6951911449432373
Validation loss: 2.057374139626821

Epoch: 6| Step: 3
Training loss: 2.1186234951019287
Validation loss: 2.060621201992035

Epoch: 6| Step: 4
Training loss: 2.3007073402404785
Validation loss: 2.074677526950836

Epoch: 6| Step: 5
Training loss: 1.7711777687072754
Validation loss: 2.0627920826276145

Epoch: 6| Step: 6
Training loss: 2.4985945224761963
Validation loss: 2.0328224500020347

Epoch: 6| Step: 7
Training loss: 1.6227842569351196
Validation loss: 2.053068459033966

Epoch: 6| Step: 8
Training loss: 1.3235948085784912
Validation loss: 2.0328917503356934

Epoch: 6| Step: 9
Training loss: 1.702275276184082
Validation loss: 2.0342827240626016

Epoch: 6| Step: 10
Training loss: 1.686457872390747
Validation loss: 2.032894770304362

Epoch: 6| Step: 11
Training loss: 1.8161293268203735
Validation loss: 2.0062064131100974

Epoch: 6| Step: 12
Training loss: 1.33939528465271
Validation loss: 2.010304629802704

Epoch: 6| Step: 13
Training loss: 2.008277416229248
Validation loss: 2.0091455777486167

Epoch: 45| Step: 0
Training loss: 2.555851459503174
Validation loss: 2.0318557222684226

Epoch: 6| Step: 1
Training loss: 2.038538932800293
Validation loss: 2.0114824374516806

Epoch: 6| Step: 2
Training loss: 2.1053223609924316
Validation loss: 2.0007336735725403

Epoch: 6| Step: 3
Training loss: 2.0592358112335205
Validation loss: 1.9955695271492004

Epoch: 6| Step: 4
Training loss: 2.061436653137207
Validation loss: 1.9947006305058796

Epoch: 6| Step: 5
Training loss: 2.3749310970306396
Validation loss: 2.0097771286964417

Epoch: 6| Step: 6
Training loss: 2.092834234237671
Validation loss: 1.9985684553782146

Epoch: 6| Step: 7
Training loss: 1.1697640419006348
Validation loss: 2.0171092549959817

Epoch: 6| Step: 8
Training loss: 2.1614880561828613
Validation loss: 2.0222750306129456

Epoch: 6| Step: 9
Training loss: 1.1001728773117065
Validation loss: 2.0244210759798684

Epoch: 6| Step: 10
Training loss: 2.2419257164001465
Validation loss: 2.032534658908844

Epoch: 6| Step: 11
Training loss: 1.7199223041534424
Validation loss: 2.0504123767217

Epoch: 6| Step: 12
Training loss: 1.3994982242584229
Validation loss: 2.0294476747512817

Epoch: 6| Step: 13
Training loss: 2.2464447021484375
Validation loss: 2.062666654586792

Epoch: 46| Step: 0
Training loss: 1.4836373329162598
Validation loss: 2.0709917545318604

Epoch: 6| Step: 1
Training loss: 2.0251948833465576
Validation loss: 2.0591739614804587

Epoch: 6| Step: 2
Training loss: 1.5028661489486694
Validation loss: 2.0776308178901672

Epoch: 6| Step: 3
Training loss: 1.912635087966919
Validation loss: 2.056217988332113

Epoch: 6| Step: 4
Training loss: 2.1221747398376465
Validation loss: 2.0487913290659585

Epoch: 6| Step: 5
Training loss: 2.447596549987793
Validation loss: 2.0451353192329407

Epoch: 6| Step: 6
Training loss: 1.7456445693969727
Validation loss: 2.0551963249842324

Epoch: 6| Step: 7
Training loss: 2.1726789474487305
Validation loss: 2.041491528352102

Epoch: 6| Step: 8
Training loss: 1.9770240783691406
Validation loss: 2.0400017301241555

Epoch: 6| Step: 9
Training loss: 1.8575087785720825
Validation loss: 2.0128008127212524

Epoch: 6| Step: 10
Training loss: 2.873518466949463
Validation loss: 2.0301155050595603

Epoch: 6| Step: 11
Training loss: 1.5389626026153564
Validation loss: 2.019062598546346

Epoch: 6| Step: 12
Training loss: 1.882491111755371
Validation loss: 2.0241423845291138

Epoch: 6| Step: 13
Training loss: 1.6032847166061401
Validation loss: 2.010321239630381

Epoch: 47| Step: 0
Training loss: 1.715651512145996
Validation loss: 2.014315128326416

Epoch: 6| Step: 1
Training loss: 1.878934383392334
Validation loss: 2.025638302167257

Epoch: 6| Step: 2
Training loss: 1.7467000484466553
Validation loss: 2.008828620115916

Epoch: 6| Step: 3
Training loss: 1.9790089130401611
Validation loss: 2.0323821107546487

Epoch: 6| Step: 4
Training loss: 2.299544095993042
Validation loss: 2.0230455001195273

Epoch: 6| Step: 5
Training loss: 1.8583416938781738
Validation loss: 2.0467445850372314

Epoch: 6| Step: 6
Training loss: 1.4468506574630737
Validation loss: 2.0383045276006064

Epoch: 6| Step: 7
Training loss: 2.2414257526397705
Validation loss: 2.0461066365242004

Epoch: 6| Step: 8
Training loss: 2.7630093097686768
Validation loss: 2.0340388218561807

Epoch: 6| Step: 9
Training loss: 1.5380828380584717
Validation loss: 2.0290143291155496

Epoch: 6| Step: 10
Training loss: 2.041092872619629
Validation loss: 2.029032031695048

Epoch: 6| Step: 11
Training loss: 2.141771078109741
Validation loss: 2.0318692127863565

Epoch: 6| Step: 12
Training loss: 2.1716151237487793
Validation loss: 2.0324286023775735

Epoch: 6| Step: 13
Training loss: 1.2072343826293945
Validation loss: 2.048081874847412

Epoch: 48| Step: 0
Training loss: 2.54196834564209
Validation loss: 2.056157946586609

Epoch: 6| Step: 1
Training loss: 1.7540907859802246
Validation loss: 2.0412563482920327

Epoch: 6| Step: 2
Training loss: 1.9859297275543213
Validation loss: 2.0160136620203652

Epoch: 6| Step: 3
Training loss: 2.1208765506744385
Validation loss: 2.0356523593266806

Epoch: 6| Step: 4
Training loss: 1.9785915613174438
Validation loss: 1.997117320696513

Epoch: 6| Step: 5
Training loss: 1.2067246437072754
Validation loss: 2.01943031946818

Epoch: 6| Step: 6
Training loss: 1.62930166721344
Validation loss: 2.0031177202860513

Epoch: 6| Step: 7
Training loss: 1.8130160570144653
Validation loss: 2.0270707607269287

Epoch: 6| Step: 8
Training loss: 1.8065928220748901
Validation loss: 2.0326860745747886

Epoch: 6| Step: 9
Training loss: 1.5621964931488037
Validation loss: 2.012471854686737

Epoch: 6| Step: 10
Training loss: 1.7863209247589111
Validation loss: 2.008125901222229

Epoch: 6| Step: 11
Training loss: 2.0486721992492676
Validation loss: 2.0431425968805947

Epoch: 6| Step: 12
Training loss: 2.7498044967651367
Validation loss: 2.0377636353174844

Epoch: 6| Step: 13
Training loss: 1.8266704082489014
Validation loss: 2.03926028807958

Epoch: 49| Step: 0
Training loss: 1.4957709312438965
Validation loss: 2.0567264358202615

Epoch: 6| Step: 1
Training loss: 1.9867396354675293
Validation loss: 2.0651437441507974

Epoch: 6| Step: 2
Training loss: 1.6634087562561035
Validation loss: 2.0757658878962197

Epoch: 6| Step: 3
Training loss: 2.2878963947296143
Validation loss: 2.096170663833618

Epoch: 6| Step: 4
Training loss: 2.1744022369384766
Validation loss: 2.0907433231671653

Epoch: 6| Step: 5
Training loss: 1.9468722343444824
Validation loss: 2.0868962009747825

Epoch: 6| Step: 6
Training loss: 2.4472663402557373
Validation loss: 2.0752066572507224

Epoch: 6| Step: 7
Training loss: 2.2407455444335938
Validation loss: 2.0848228335380554

Epoch: 6| Step: 8
Training loss: 1.830456256866455
Validation loss: 2.0838088989257812

Epoch: 6| Step: 9
Training loss: 1.3346620798110962
Validation loss: 2.0815831820170083

Epoch: 6| Step: 10
Training loss: 1.5079705715179443
Validation loss: 2.0539413491884866

Epoch: 6| Step: 11
Training loss: 2.1014339923858643
Validation loss: 2.040412743886312

Epoch: 6| Step: 12
Training loss: 1.87595534324646
Validation loss: 2.0359904170036316

Epoch: 6| Step: 13
Training loss: 2.2558090686798096
Validation loss: 2.0290075540542603

Epoch: 50| Step: 0
Training loss: 1.9379198551177979
Validation loss: 2.014000733693441

Epoch: 6| Step: 1
Training loss: 2.2328548431396484
Validation loss: 2.0246072014172873

Epoch: 6| Step: 2
Training loss: 1.9291377067565918
Validation loss: 2.001226007938385

Epoch: 6| Step: 3
Training loss: 1.8548622131347656
Validation loss: 2.0093268156051636

Epoch: 6| Step: 4
Training loss: 1.9311822652816772
Validation loss: 2.0112704634666443

Epoch: 6| Step: 5
Training loss: 1.6531143188476562
Validation loss: 2.0057384371757507

Epoch: 6| Step: 6
Training loss: 1.5840187072753906
Validation loss: 2.018589754899343

Epoch: 6| Step: 7
Training loss: 1.930708408355713
Validation loss: 2.051348626613617

Epoch: 6| Step: 8
Training loss: 1.9167797565460205
Validation loss: 2.0385552048683167

Epoch: 6| Step: 9
Training loss: 1.8678557872772217
Validation loss: 2.0486303170522056

Epoch: 6| Step: 10
Training loss: 2.4407923221588135
Validation loss: 2.025832454363505

Epoch: 6| Step: 11
Training loss: 1.7286970615386963
Validation loss: 2.045481582482656

Epoch: 6| Step: 12
Training loss: 2.3715908527374268
Validation loss: 2.0522157351175943

Epoch: 6| Step: 13
Training loss: 1.405979871749878
Validation loss: 2.033180912335714

Epoch: 51| Step: 0
Training loss: 1.7307937145233154
Validation loss: 2.0442222952842712

Epoch: 6| Step: 1
Training loss: 1.682161808013916
Validation loss: 2.051562746365865

Epoch: 6| Step: 2
Training loss: 2.0139760971069336
Validation loss: 2.054499606291453

Epoch: 6| Step: 3
Training loss: 2.137007713317871
Validation loss: 2.0610292156537375

Epoch: 6| Step: 4
Training loss: 2.1230998039245605
Validation loss: 2.0468770265579224

Epoch: 6| Step: 5
Training loss: 1.6985280513763428
Validation loss: 2.051044762134552

Epoch: 6| Step: 6
Training loss: 1.9571809768676758
Validation loss: 2.035001277923584

Epoch: 6| Step: 7
Training loss: 1.9199261665344238
Validation loss: 2.032223323980967

Epoch: 6| Step: 8
Training loss: 1.8340075016021729
Validation loss: 2.0375688473383584

Epoch: 6| Step: 9
Training loss: 2.2527177333831787
Validation loss: 2.020858585834503

Epoch: 6| Step: 10
Training loss: 1.3800926208496094
Validation loss: 2.023573716481527

Epoch: 6| Step: 11
Training loss: 2.2006521224975586
Validation loss: 2.0334176222483316

Epoch: 6| Step: 12
Training loss: 2.1651611328125
Validation loss: 2.0432554880777993

Epoch: 6| Step: 13
Training loss: 1.7392470836639404
Validation loss: 2.0328147808710733

Epoch: 52| Step: 0
Training loss: 1.9001678228378296
Validation loss: 2.0674551129341125

Epoch: 6| Step: 1
Training loss: 2.0540950298309326
Validation loss: 2.0462739070256553

Epoch: 6| Step: 2
Training loss: 2.0199551582336426
Validation loss: 2.056196848551432

Epoch: 6| Step: 3
Training loss: 1.738686203956604
Validation loss: 2.050126930077871

Epoch: 6| Step: 4
Training loss: 1.95429527759552
Validation loss: 2.058369755744934

Epoch: 6| Step: 5
Training loss: 1.7086338996887207
Validation loss: 2.0519264340400696

Epoch: 6| Step: 6
Training loss: 1.6788458824157715
Validation loss: 2.0665573676427207

Epoch: 6| Step: 7
Training loss: 2.1389360427856445
Validation loss: 2.062077979246775

Epoch: 6| Step: 8
Training loss: 2.336355686187744
Validation loss: 2.05441822608312

Epoch: 6| Step: 9
Training loss: 1.5842704772949219
Validation loss: 2.051721473534902

Epoch: 6| Step: 10
Training loss: 1.9055858850479126
Validation loss: 2.048418124516805

Epoch: 6| Step: 11
Training loss: 1.4896122217178345
Validation loss: 2.040444254875183

Epoch: 6| Step: 12
Training loss: 1.986193299293518
Validation loss: 2.0510120193163552

Epoch: 6| Step: 13
Training loss: 2.1410131454467773
Validation loss: 2.043623169263204

Epoch: 53| Step: 0
Training loss: 2.0177321434020996
Validation loss: 2.0686739087104797

Epoch: 6| Step: 1
Training loss: 2.123717784881592
Validation loss: 2.040271004041036

Epoch: 6| Step: 2
Training loss: 1.842813491821289
Validation loss: 2.0516881346702576

Epoch: 6| Step: 3
Training loss: 1.973723292350769
Validation loss: 2.071910301844279

Epoch: 6| Step: 4
Training loss: 1.6626522541046143
Validation loss: 2.0480875770250955

Epoch: 6| Step: 5
Training loss: 1.6534006595611572
Validation loss: 2.046948413054148

Epoch: 6| Step: 6
Training loss: 1.58837890625
Validation loss: 2.0695340633392334

Epoch: 6| Step: 7
Training loss: 1.931859016418457
Validation loss: 2.065322001775106

Epoch: 6| Step: 8
Training loss: 2.4822404384613037
Validation loss: 2.0777244369188943

Epoch: 6| Step: 9
Training loss: 1.4765177965164185
Validation loss: 2.057193477948507

Epoch: 6| Step: 10
Training loss: 1.8309426307678223
Validation loss: 2.04362686475118

Epoch: 6| Step: 11
Training loss: 2.338336944580078
Validation loss: 2.023400863011678

Epoch: 6| Step: 12
Training loss: 2.116755485534668
Validation loss: 2.032862921555837

Epoch: 6| Step: 13
Training loss: 1.7682511806488037
Validation loss: 2.010767141977946

Epoch: 54| Step: 0
Training loss: 1.837633490562439
Validation loss: 2.0070373018582663

Epoch: 6| Step: 1
Training loss: 1.8143565654754639
Validation loss: 2.018918991088867

Epoch: 6| Step: 2
Training loss: 1.3702501058578491
Validation loss: 2.0066324869791665

Epoch: 6| Step: 3
Training loss: 2.142416477203369
Validation loss: 2.012911856174469

Epoch: 6| Step: 4
Training loss: 1.577574372291565
Validation loss: 2.0345362424850464

Epoch: 6| Step: 5
Training loss: 1.8969546556472778
Validation loss: 2.0253424843152366

Epoch: 6| Step: 6
Training loss: 1.3535945415496826
Validation loss: 2.0140124758084617

Epoch: 6| Step: 7
Training loss: 2.144484519958496
Validation loss: 2.0130504171053567

Epoch: 6| Step: 8
Training loss: 1.5087745189666748
Validation loss: 2.007009585698446

Epoch: 6| Step: 9
Training loss: 2.200207471847534
Validation loss: 2.00562318166097

Epoch: 6| Step: 10
Training loss: 1.7266689538955688
Validation loss: 2.0222308238347373

Epoch: 6| Step: 11
Training loss: 2.4530701637268066
Validation loss: 2.0174232721328735

Epoch: 6| Step: 12
Training loss: 2.753390312194824
Validation loss: 2.0207371910413108

Epoch: 6| Step: 13
Training loss: 2.2277631759643555
Validation loss: 2.0456042885780334

Epoch: 55| Step: 0
Training loss: 1.7183842658996582
Validation loss: 2.0241142908732095

Epoch: 6| Step: 1
Training loss: 1.961681842803955
Validation loss: 2.068966547648112

Epoch: 6| Step: 2
Training loss: 1.7296874523162842
Validation loss: 2.0853953758875527

Epoch: 6| Step: 3
Training loss: 1.0559159517288208
Validation loss: 2.057856102784475

Epoch: 6| Step: 4
Training loss: 1.6272039413452148
Validation loss: 2.0863882501920066

Epoch: 6| Step: 5
Training loss: 1.728144645690918
Validation loss: 2.062579949696859

Epoch: 6| Step: 6
Training loss: 2.379021167755127
Validation loss: 2.0948649644851685

Epoch: 6| Step: 7
Training loss: 2.2902398109436035
Validation loss: 2.104824662208557

Epoch: 6| Step: 8
Training loss: 2.433198928833008
Validation loss: 2.0977044900258384

Epoch: 6| Step: 9
Training loss: 1.9188469648361206
Validation loss: 2.089272439479828

Epoch: 6| Step: 10
Training loss: 2.3401947021484375
Validation loss: 2.0516316890716553

Epoch: 6| Step: 11
Training loss: 1.6025302410125732
Validation loss: 2.0713494618733725

Epoch: 6| Step: 12
Training loss: 1.9556089639663696
Validation loss: 2.0344470143318176

Epoch: 6| Step: 13
Training loss: 1.7847745418548584
Validation loss: 2.0269446969032288

Epoch: 56| Step: 0
Training loss: 2.4319515228271484
Validation loss: 2.0061978300412497

Epoch: 6| Step: 1
Training loss: 1.280216932296753
Validation loss: 2.035827378431956

Epoch: 6| Step: 2
Training loss: 1.9571188688278198
Validation loss: 2.0198187828063965

Epoch: 6| Step: 3
Training loss: 1.4440512657165527
Validation loss: 2.0219331781069436

Epoch: 6| Step: 4
Training loss: 1.6671640872955322
Validation loss: 2.0162632862726846

Epoch: 6| Step: 5
Training loss: 1.4439167976379395
Validation loss: 2.011214852333069

Epoch: 6| Step: 6
Training loss: 2.0953519344329834
Validation loss: 1.9965861439704895

Epoch: 6| Step: 7
Training loss: 1.8660715818405151
Validation loss: 2.01744415362676

Epoch: 6| Step: 8
Training loss: 2.742769241333008
Validation loss: 1.9999120632807414

Epoch: 6| Step: 9
Training loss: 2.5269365310668945
Validation loss: 2.018863578637441

Epoch: 6| Step: 10
Training loss: 1.7272560596466064
Validation loss: 2.0121519366900125

Epoch: 6| Step: 11
Training loss: 1.4594433307647705
Validation loss: 2.000898778438568

Epoch: 6| Step: 12
Training loss: 2.0640387535095215
Validation loss: 2.01172669728597

Epoch: 6| Step: 13
Training loss: 2.2294936180114746
Validation loss: 2.029212256272634

Epoch: 57| Step: 0
Training loss: 1.9918049573898315
Validation loss: 2.0327776273091636

Epoch: 6| Step: 1
Training loss: 1.807641625404358
Validation loss: 2.050504962603251

Epoch: 6| Step: 2
Training loss: 1.7129526138305664
Validation loss: 2.0738518238067627

Epoch: 6| Step: 3
Training loss: 2.122223377227783
Validation loss: 2.1114741365114846

Epoch: 6| Step: 4
Training loss: 1.637402892112732
Validation loss: 2.114400029182434

Epoch: 6| Step: 5
Training loss: 2.181342124938965
Validation loss: 2.10658723115921

Epoch: 6| Step: 6
Training loss: 1.6339023113250732
Validation loss: 2.0890419284502664

Epoch: 6| Step: 7
Training loss: 1.9587664604187012
Validation loss: 2.076814373334249

Epoch: 6| Step: 8
Training loss: 1.8625723123550415
Validation loss: 2.0670341650644937

Epoch: 6| Step: 9
Training loss: 2.6456871032714844
Validation loss: 2.042403757572174

Epoch: 6| Step: 10
Training loss: 2.051194906234741
Validation loss: 2.022712310155233

Epoch: 6| Step: 11
Training loss: 1.6937592029571533
Validation loss: 2.0231452782948813

Epoch: 6| Step: 12
Training loss: 1.9714655876159668
Validation loss: 1.9997185866038005

Epoch: 6| Step: 13
Training loss: 1.8798428773880005
Validation loss: 2.0318394104639688

Epoch: 58| Step: 0
Training loss: 1.9432488679885864
Validation loss: 1.9991892178853352

Epoch: 6| Step: 1
Training loss: 1.7660690546035767
Validation loss: 1.993380645910899

Epoch: 6| Step: 2
Training loss: 2.0697810649871826
Validation loss: 2.016666889190674

Epoch: 6| Step: 3
Training loss: 2.1987509727478027
Validation loss: 2.000607172648112

Epoch: 6| Step: 4
Training loss: 2.0764260292053223
Validation loss: 2.0123156110445657

Epoch: 6| Step: 5
Training loss: 1.5054757595062256
Validation loss: 2.0188130736351013

Epoch: 6| Step: 6
Training loss: 1.889350414276123
Validation loss: 2.010145366191864

Epoch: 6| Step: 7
Training loss: 2.0894811153411865
Validation loss: 2.005245327949524

Epoch: 6| Step: 8
Training loss: 1.5265426635742188
Validation loss: 2.014143228530884

Epoch: 6| Step: 9
Training loss: 2.5209720134735107
Validation loss: 2.007753014564514

Epoch: 6| Step: 10
Training loss: 2.0749194622039795
Validation loss: 2.003952940305074

Epoch: 6| Step: 11
Training loss: 1.7644813060760498
Validation loss: 2.0307626724243164

Epoch: 6| Step: 12
Training loss: 1.8701480627059937
Validation loss: 2.0107146898905435

Epoch: 6| Step: 13
Training loss: 1.6137441396713257
Validation loss: 2.0320610205332437

Epoch: 59| Step: 0
Training loss: 1.6049833297729492
Validation loss: 2.052816073099772

Epoch: 6| Step: 1
Training loss: 1.9844169616699219
Validation loss: 2.0473854144414267

Epoch: 6| Step: 2
Training loss: 2.152381420135498
Validation loss: 2.087395985921224

Epoch: 6| Step: 3
Training loss: 1.8178374767303467
Validation loss: 2.1005837519963584

Epoch: 6| Step: 4
Training loss: 2.5425872802734375
Validation loss: 2.118726829687754

Epoch: 6| Step: 5
Training loss: 2.7656478881835938
Validation loss: 2.1161100467046103

Epoch: 6| Step: 6
Training loss: 1.5019222497940063
Validation loss: 2.0965059399604797

Epoch: 6| Step: 7
Training loss: 1.83786940574646
Validation loss: 2.0930323799451194

Epoch: 6| Step: 8
Training loss: 2.284872055053711
Validation loss: 2.069076100985209

Epoch: 6| Step: 9
Training loss: 1.811797857284546
Validation loss: 2.058458924293518

Epoch: 6| Step: 10
Training loss: 1.7126518487930298
Validation loss: 2.0565096537272134

Epoch: 6| Step: 11
Training loss: 1.4251458644866943
Validation loss: 2.0367654959360757

Epoch: 6| Step: 12
Training loss: 1.660903811454773
Validation loss: 2.0387307008107505

Epoch: 6| Step: 13
Training loss: 1.7422865629196167
Validation loss: 2.0318320790926614

Epoch: 60| Step: 0
Training loss: 2.611246109008789
Validation loss: 2.0333464940389

Epoch: 6| Step: 1
Training loss: 1.8388526439666748
Validation loss: 2.0360333919525146

Epoch: 6| Step: 2
Training loss: 1.8013036251068115
Validation loss: 2.0533257921536765

Epoch: 6| Step: 3
Training loss: 1.4561790227890015
Validation loss: 2.039779702822367

Epoch: 6| Step: 4
Training loss: 2.3063406944274902
Validation loss: 2.0368419686953225

Epoch: 6| Step: 5
Training loss: 1.2181978225708008
Validation loss: 2.0378336906433105

Epoch: 6| Step: 6
Training loss: 2.0301337242126465
Validation loss: 2.077419956525167

Epoch: 6| Step: 7
Training loss: 1.2576866149902344
Validation loss: 2.055250366528829

Epoch: 6| Step: 8
Training loss: 2.342925548553467
Validation loss: 2.0713889400164285

Epoch: 6| Step: 9
Training loss: 1.4324307441711426
Validation loss: 2.0916191736857095

Epoch: 6| Step: 10
Training loss: 2.4659135341644287
Validation loss: 2.0776607592900596

Epoch: 6| Step: 11
Training loss: 1.3805499076843262
Validation loss: 2.078270157178243

Epoch: 6| Step: 12
Training loss: 2.2829349040985107
Validation loss: 2.0617271264394126

Epoch: 6| Step: 13
Training loss: 2.009489059448242
Validation loss: 2.044191916783651

Epoch: 61| Step: 0
Training loss: 1.9439629316329956
Validation loss: 2.0418447852134705

Epoch: 6| Step: 1
Training loss: 1.1581645011901855
Validation loss: 2.0464718540509543

Epoch: 6| Step: 2
Training loss: 2.2784881591796875
Validation loss: 2.044197758038839

Epoch: 6| Step: 3
Training loss: 1.6598892211914062
Validation loss: 2.0404096047083535

Epoch: 6| Step: 4
Training loss: 1.960339069366455
Validation loss: 2.01150635878245

Epoch: 6| Step: 5
Training loss: 1.5561063289642334
Validation loss: 2.0290816823641458

Epoch: 6| Step: 6
Training loss: 1.6797659397125244
Validation loss: 2.011718511581421

Epoch: 6| Step: 7
Training loss: 1.354170322418213
Validation loss: 2.021185557047526

Epoch: 6| Step: 8
Training loss: 2.3311524391174316
Validation loss: 2.0115555127461753

Epoch: 6| Step: 9
Training loss: 1.8534047603607178
Validation loss: 2.035757064819336

Epoch: 6| Step: 10
Training loss: 2.204981565475464
Validation loss: 2.045460065205892

Epoch: 6| Step: 11
Training loss: 2.268345594406128
Validation loss: 2.042976160844167

Epoch: 6| Step: 12
Training loss: 2.1277337074279785
Validation loss: 2.0472986102104187

Epoch: 6| Step: 13
Training loss: 1.911200761795044
Validation loss: 2.024717152118683

Epoch: 62| Step: 0
Training loss: 1.1972187757492065
Validation loss: 2.024018168449402

Epoch: 6| Step: 1
Training loss: 1.4287816286087036
Validation loss: 2.0450317660967507

Epoch: 6| Step: 2
Training loss: 2.0541436672210693
Validation loss: 2.028079350789388

Epoch: 6| Step: 3
Training loss: 1.5446288585662842
Validation loss: 2.025999287764231

Epoch: 6| Step: 4
Training loss: 1.7836472988128662
Validation loss: 2.036710580190023

Epoch: 6| Step: 5
Training loss: 1.973535180091858
Validation loss: 2.0150511264801025

Epoch: 6| Step: 6
Training loss: 2.1718549728393555
Validation loss: 2.0481831431388855

Epoch: 6| Step: 7
Training loss: 2.493340492248535
Validation loss: 2.025772452354431

Epoch: 6| Step: 8
Training loss: 1.828023910522461
Validation loss: 2.0471487641334534

Epoch: 6| Step: 9
Training loss: 1.8547312021255493
Validation loss: 2.0479499101638794

Epoch: 6| Step: 10
Training loss: 1.3844249248504639
Validation loss: 2.049579918384552

Epoch: 6| Step: 11
Training loss: 1.940542459487915
Validation loss: 2.0446517864863076

Epoch: 6| Step: 12
Training loss: 2.7968311309814453
Validation loss: 2.0560033122698465

Epoch: 6| Step: 13
Training loss: 1.6703656911849976
Validation loss: 2.02887370189031

Epoch: 63| Step: 0
Training loss: 2.319873332977295
Validation loss: 2.063440521558126

Epoch: 6| Step: 1
Training loss: 1.770829677581787
Validation loss: 2.0274174014727273

Epoch: 6| Step: 2
Training loss: 1.738378882408142
Validation loss: 2.052989979585012

Epoch: 6| Step: 3
Training loss: 1.6768264770507812
Validation loss: 2.0331362088521323

Epoch: 6| Step: 4
Training loss: 1.7060878276824951
Validation loss: 2.0313077370325723

Epoch: 6| Step: 5
Training loss: 2.3032872676849365
Validation loss: 2.0408305525779724

Epoch: 6| Step: 6
Training loss: 1.6853580474853516
Validation loss: 2.025098224480947

Epoch: 6| Step: 7
Training loss: 1.1899574995040894
Validation loss: 2.0451408425966897

Epoch: 6| Step: 8
Training loss: 1.811835527420044
Validation loss: 2.0106085737546286

Epoch: 6| Step: 9
Training loss: 2.372933864593506
Validation loss: 2.0264153281847634

Epoch: 6| Step: 10
Training loss: 1.6243624687194824
Validation loss: 2.025900582472483

Epoch: 6| Step: 11
Training loss: 2.395366907119751
Validation loss: 2.014569362004598

Epoch: 6| Step: 12
Training loss: 1.4355638027191162
Validation loss: 2.01747860511144

Epoch: 6| Step: 13
Training loss: 1.8809611797332764
Validation loss: 2.0338167548179626

Epoch: 64| Step: 0
Training loss: 2.269831657409668
Validation loss: 2.0322258869806924

Epoch: 6| Step: 1
Training loss: 1.9277033805847168
Validation loss: 2.0602681040763855

Epoch: 6| Step: 2
Training loss: 1.6368025541305542
Validation loss: 2.0444853703180947

Epoch: 6| Step: 3
Training loss: 1.8902299404144287
Validation loss: 2.047656853993734

Epoch: 6| Step: 4
Training loss: 2.1406426429748535
Validation loss: 2.0815172592798867

Epoch: 6| Step: 5
Training loss: 1.267763376235962
Validation loss: 2.054916044076284

Epoch: 6| Step: 6
Training loss: 2.004220485687256
Validation loss: 2.0515223344167075

Epoch: 6| Step: 7
Training loss: 1.1712610721588135
Validation loss: 2.0413655042648315

Epoch: 6| Step: 8
Training loss: 2.5191357135772705
Validation loss: 2.050898790359497

Epoch: 6| Step: 9
Training loss: 1.2284265756607056
Validation loss: 2.042026400566101

Epoch: 6| Step: 10
Training loss: 2.073434352874756
Validation loss: 2.0643008748690286

Epoch: 6| Step: 11
Training loss: 2.198259115219116
Validation loss: 2.0347127119700112

Epoch: 6| Step: 12
Training loss: 1.8343956470489502
Validation loss: 2.0322502851486206

Epoch: 6| Step: 13
Training loss: 1.8758761882781982
Validation loss: 2.016538838545481

Epoch: 65| Step: 0
Training loss: 1.7568548917770386
Validation loss: 2.0409774581591287

Epoch: 6| Step: 1
Training loss: 2.0627975463867188
Validation loss: 2.041529913743337

Epoch: 6| Step: 2
Training loss: 1.434798240661621
Validation loss: 2.0363393227259317

Epoch: 6| Step: 3
Training loss: 1.6202741861343384
Validation loss: 2.05364058415095

Epoch: 6| Step: 4
Training loss: 1.9099336862564087
Validation loss: 2.039378305276235

Epoch: 6| Step: 5
Training loss: 1.6292178630828857
Validation loss: 2.056169847647349

Epoch: 6| Step: 6
Training loss: 1.349160075187683
Validation loss: 2.065784990787506

Epoch: 6| Step: 7
Training loss: 1.4883947372436523
Validation loss: 2.049107531706492

Epoch: 6| Step: 8
Training loss: 2.244382381439209
Validation loss: 2.053113341331482

Epoch: 6| Step: 9
Training loss: 2.3285069465637207
Validation loss: 2.084679961204529

Epoch: 6| Step: 10
Training loss: 2.6428463459014893
Validation loss: 2.042623440424601

Epoch: 6| Step: 11
Training loss: 1.5435290336608887
Validation loss: 2.0330415964126587

Epoch: 6| Step: 12
Training loss: 1.84601628780365
Validation loss: 2.017290771007538

Epoch: 6| Step: 13
Training loss: 2.087782382965088
Validation loss: 2.024407684803009

Epoch: 66| Step: 0
Training loss: 1.360577940940857
Validation loss: 2.0406399170557656

Epoch: 6| Step: 1
Training loss: 2.028046131134033
Validation loss: 2.055527945359548

Epoch: 6| Step: 2
Training loss: 1.896280288696289
Validation loss: 2.0360516905784607

Epoch: 6| Step: 3
Training loss: 1.8103058338165283
Validation loss: 2.0603265166282654

Epoch: 6| Step: 4
Training loss: 2.3820323944091797
Validation loss: 2.0654192368189492

Epoch: 6| Step: 5
Training loss: 2.5554752349853516
Validation loss: 2.0643784403800964

Epoch: 6| Step: 6
Training loss: 2.0483126640319824
Validation loss: 2.061227798461914

Epoch: 6| Step: 7
Training loss: 1.586195707321167
Validation loss: 2.0509894092877707

Epoch: 6| Step: 8
Training loss: 1.8096952438354492
Validation loss: 2.029680867989858

Epoch: 6| Step: 9
Training loss: 1.4767999649047852
Validation loss: 2.01274840037028

Epoch: 6| Step: 10
Training loss: 1.6342620849609375
Validation loss: 2.0313668648401895

Epoch: 6| Step: 11
Training loss: 2.0243139266967773
Validation loss: 1.9926835695902507

Epoch: 6| Step: 12
Training loss: 1.5040221214294434
Validation loss: 2.005585531393687

Epoch: 6| Step: 13
Training loss: 2.0550358295440674
Validation loss: 2.0128467281659446

Epoch: 67| Step: 0
Training loss: 2.4100399017333984
Validation loss: 2.0179117918014526

Epoch: 6| Step: 1
Training loss: 1.1521999835968018
Validation loss: 2.0199440320332847

Epoch: 6| Step: 2
Training loss: 1.7510643005371094
Validation loss: 2.034167448679606

Epoch: 6| Step: 3
Training loss: 1.8066203594207764
Validation loss: 2.045199394226074

Epoch: 6| Step: 4
Training loss: 1.8419404029846191
Validation loss: 2.042261997858683

Epoch: 6| Step: 5
Training loss: 1.7559120655059814
Validation loss: 2.066174884637197

Epoch: 6| Step: 6
Training loss: 1.5517419576644897
Validation loss: 2.0411298473676047

Epoch: 6| Step: 7
Training loss: 1.3219423294067383
Validation loss: 2.0311472614606223

Epoch: 6| Step: 8
Training loss: 1.7936897277832031
Validation loss: 2.0265410343805947

Epoch: 6| Step: 9
Training loss: 1.9547029733657837
Validation loss: 2.028226097424825

Epoch: 6| Step: 10
Training loss: 2.0391533374786377
Validation loss: 2.0279722213745117

Epoch: 6| Step: 11
Training loss: 2.5162785053253174
Validation loss: 2.0464438994725547

Epoch: 6| Step: 12
Training loss: 2.059103488922119
Validation loss: 2.0406598647435508

Epoch: 6| Step: 13
Training loss: 1.8746232986450195
Validation loss: 2.04705540339152

Epoch: 68| Step: 0
Training loss: 1.7049908638000488
Validation loss: 2.0453174114227295

Epoch: 6| Step: 1
Training loss: 1.7699542045593262
Validation loss: 2.05493954817454

Epoch: 6| Step: 2
Training loss: 1.7237439155578613
Validation loss: 2.0663035114606223

Epoch: 6| Step: 3
Training loss: 1.9779129028320312
Validation loss: 2.0855650504430137

Epoch: 6| Step: 4
Training loss: 1.2820019721984863
Validation loss: 2.1004103620847068

Epoch: 6| Step: 5
Training loss: 2.4159629344940186
Validation loss: 2.1036887963612876

Epoch: 6| Step: 6
Training loss: 2.2300963401794434
Validation loss: 2.094543735186259

Epoch: 6| Step: 7
Training loss: 2.2269582748413086
Validation loss: 2.064801057179769

Epoch: 6| Step: 8
Training loss: 1.6142542362213135
Validation loss: 2.0579615434010825

Epoch: 6| Step: 9
Training loss: 1.9274117946624756
Validation loss: 2.0269001523653665

Epoch: 6| Step: 10
Training loss: 1.4386366605758667
Validation loss: 2.0309228698412576

Epoch: 6| Step: 11
Training loss: 1.874064564704895
Validation loss: 2.0284364024798074

Epoch: 6| Step: 12
Training loss: 1.593834638595581
Validation loss: 2.043276369571686

Epoch: 6| Step: 13
Training loss: 2.339150905609131
Validation loss: 2.000969151655833

Epoch: 69| Step: 0
Training loss: 1.6846034526824951
Validation loss: 2.0127865076065063

Epoch: 6| Step: 1
Training loss: 1.9170713424682617
Validation loss: 2.014608085155487

Epoch: 6| Step: 2
Training loss: 1.6174802780151367
Validation loss: 2.015297790368398

Epoch: 6| Step: 3
Training loss: 1.6842546463012695
Validation loss: 1.9986846844355266

Epoch: 6| Step: 4
Training loss: 1.509525179862976
Validation loss: 2.0136454900105796

Epoch: 6| Step: 5
Training loss: 2.5909554958343506
Validation loss: 2.0139484206835427

Epoch: 6| Step: 6
Training loss: 2.2483630180358887
Validation loss: 2.0284687081972756

Epoch: 6| Step: 7
Training loss: 1.4440182447433472
Validation loss: 2.0371758739153543

Epoch: 6| Step: 8
Training loss: 1.380932331085205
Validation loss: 2.043854375680288

Epoch: 6| Step: 9
Training loss: 1.9353125095367432
Validation loss: 2.0366161863009133

Epoch: 6| Step: 10
Training loss: 2.187241315841675
Validation loss: 2.082596162954966

Epoch: 6| Step: 11
Training loss: 1.7551896572113037
Validation loss: 2.0734627842903137

Epoch: 6| Step: 12
Training loss: 2.186974287033081
Validation loss: 2.07903516292572

Epoch: 6| Step: 13
Training loss: 1.9496104717254639
Validation loss: 2.0775052110354104

Epoch: 70| Step: 0
Training loss: 1.1281254291534424
Validation loss: 2.0656593839327493

Epoch: 6| Step: 1
Training loss: 1.9461438655853271
Validation loss: 2.0507469971974692

Epoch: 6| Step: 2
Training loss: 1.7290107011795044
Validation loss: 2.060897092024485

Epoch: 6| Step: 3
Training loss: 2.302103042602539
Validation loss: 2.0363218784332275

Epoch: 6| Step: 4
Training loss: 1.3464314937591553
Validation loss: 2.0251176953315735

Epoch: 6| Step: 5
Training loss: 2.209517478942871
Validation loss: 2.0257954597473145

Epoch: 6| Step: 6
Training loss: 1.8920716047286987
Validation loss: 2.012248416741689

Epoch: 6| Step: 7
Training loss: 0.9767917394638062
Validation loss: 2.0195492108662925

Epoch: 6| Step: 8
Training loss: 2.2176051139831543
Validation loss: 2.038687765598297

Epoch: 6| Step: 9
Training loss: 2.0324957370758057
Validation loss: 2.022101362546285

Epoch: 6| Step: 10
Training loss: 2.0987911224365234
Validation loss: 2.0164327224095664

Epoch: 6| Step: 11
Training loss: 1.5583279132843018
Validation loss: 2.026994983355204

Epoch: 6| Step: 12
Training loss: 2.416429042816162
Validation loss: 2.039580305417379

Epoch: 6| Step: 13
Training loss: 2.0407485961914062
Validation loss: 2.0391146341959634

Epoch: 71| Step: 0
Training loss: 2.1963605880737305
Validation loss: 2.042680283387502

Epoch: 6| Step: 1
Training loss: 1.2904841899871826
Validation loss: 2.051922877629598

Epoch: 6| Step: 2
Training loss: 1.584284782409668
Validation loss: 2.040007213751475

Epoch: 6| Step: 3
Training loss: 1.1984515190124512
Validation loss: 2.0451881885528564

Epoch: 6| Step: 4
Training loss: 1.7668031454086304
Validation loss: 2.0571393569310508

Epoch: 6| Step: 5
Training loss: 2.5024495124816895
Validation loss: 2.0575607220331826

Epoch: 6| Step: 6
Training loss: 2.021517515182495
Validation loss: 2.0815678040186563

Epoch: 6| Step: 7
Training loss: 2.1830921173095703
Validation loss: 2.029665986696879

Epoch: 6| Step: 8
Training loss: 2.0713424682617188
Validation loss: 2.0586254398028054

Epoch: 6| Step: 9
Training loss: 1.6710739135742188
Validation loss: 2.0582351088523865

Epoch: 6| Step: 10
Training loss: 2.2855119705200195
Validation loss: 2.0656147797902427

Epoch: 6| Step: 11
Training loss: 1.0987892150878906
Validation loss: 2.0679114063580832

Epoch: 6| Step: 12
Training loss: 2.2029759883880615
Validation loss: 2.0542205174764

Epoch: 6| Step: 13
Training loss: 1.5122880935668945
Validation loss: 2.0474718610445657

Epoch: 72| Step: 0
Training loss: 1.980634331703186
Validation loss: 2.0554341276486716

Epoch: 6| Step: 1
Training loss: 2.146095037460327
Validation loss: 2.053478260835012

Epoch: 6| Step: 2
Training loss: 2.034060001373291
Validation loss: 2.053400695323944

Epoch: 6| Step: 3
Training loss: 1.9830543994903564
Validation loss: 2.0642127195994058

Epoch: 6| Step: 4
Training loss: 1.1424849033355713
Validation loss: 2.037334938844045

Epoch: 6| Step: 5
Training loss: 1.9588874578475952
Validation loss: 2.0462448994318643

Epoch: 6| Step: 6
Training loss: 2.5805156230926514
Validation loss: 2.028250296910604

Epoch: 6| Step: 7
Training loss: 1.9319154024124146
Validation loss: 2.0328622460365295

Epoch: 6| Step: 8
Training loss: 1.584128975868225
Validation loss: 2.0167737007141113

Epoch: 6| Step: 9
Training loss: 1.4308279752731323
Validation loss: 2.03010622660319

Epoch: 6| Step: 10
Training loss: 2.0961413383483887
Validation loss: 2.0078492561976113

Epoch: 6| Step: 11
Training loss: 2.2474608421325684
Validation loss: 2.0283259948094687

Epoch: 6| Step: 12
Training loss: 1.0174731016159058
Validation loss: 2.0293919245402017

Epoch: 6| Step: 13
Training loss: 1.608832597732544
Validation loss: 2.027520736058553

Epoch: 73| Step: 0
Training loss: 1.3907513618469238
Validation loss: 2.056824525197347

Epoch: 6| Step: 1
Training loss: 1.7466216087341309
Validation loss: 2.044558564821879

Epoch: 6| Step: 2
Training loss: 1.4803675413131714
Validation loss: 2.0507131218910217

Epoch: 6| Step: 3
Training loss: 2.335895299911499
Validation loss: 2.0476092100143433

Epoch: 6| Step: 4
Training loss: 2.5952529907226562
Validation loss: 2.058930456638336

Epoch: 6| Step: 5
Training loss: 1.9561145305633545
Validation loss: 2.0741718411445618

Epoch: 6| Step: 6
Training loss: 1.3189024925231934
Validation loss: 2.04956591129303

Epoch: 6| Step: 7
Training loss: 1.8162376880645752
Validation loss: 2.0606209437052407

Epoch: 6| Step: 8
Training loss: 1.5439906120300293
Validation loss: 2.0490598479906716

Epoch: 6| Step: 9
Training loss: 2.0130865573883057
Validation loss: 2.0593737959861755

Epoch: 6| Step: 10
Training loss: 2.6438465118408203
Validation loss: 2.0710734724998474

Epoch: 6| Step: 11
Training loss: 1.7019106149673462
Validation loss: 2.074364105860392

Epoch: 6| Step: 12
Training loss: 1.4111053943634033
Validation loss: 2.070915718873342

Epoch: 6| Step: 13
Training loss: 1.5168607234954834
Validation loss: 2.045966605345408

Epoch: 74| Step: 0
Training loss: 1.22002375125885
Validation loss: 2.0757908821105957

Epoch: 6| Step: 1
Training loss: 2.2017247676849365
Validation loss: 2.02716859181722

Epoch: 6| Step: 2
Training loss: 2.455294370651245
Validation loss: 2.066221455732981

Epoch: 6| Step: 3
Training loss: 1.3669180870056152
Validation loss: 2.060446798801422

Epoch: 6| Step: 4
Training loss: 2.238165855407715
Validation loss: 2.047946512699127

Epoch: 6| Step: 5
Training loss: 1.806413173675537
Validation loss: 2.0547762910525003

Epoch: 6| Step: 6
Training loss: 1.4157612323760986
Validation loss: 2.0103875597318015

Epoch: 6| Step: 7
Training loss: 2.021233320236206
Validation loss: 2.0215886433919272

Epoch: 6| Step: 8
Training loss: 1.8666625022888184
Validation loss: 2.0225412448247275

Epoch: 6| Step: 9
Training loss: 1.9033293724060059
Validation loss: 2.030878782272339

Epoch: 6| Step: 10
Training loss: 1.9621877670288086
Validation loss: 2.0155572096506753

Epoch: 6| Step: 11
Training loss: 1.860636591911316
Validation loss: 2.0273843010266623

Epoch: 6| Step: 12
Training loss: 1.9794138669967651
Validation loss: 2.027439912160238

Epoch: 6| Step: 13
Training loss: 1.1641595363616943
Validation loss: 2.0325661500295005

Epoch: 75| Step: 0
Training loss: 2.407992124557495
Validation loss: 2.0318800608317056

Epoch: 6| Step: 1
Training loss: 1.1093776226043701
Validation loss: 2.069324533144633

Epoch: 6| Step: 2
Training loss: 1.951453685760498
Validation loss: 2.0829655726750693

Epoch: 6| Step: 3
Training loss: 1.5669039487838745
Validation loss: 2.0914255380630493

Epoch: 6| Step: 4
Training loss: 2.5720787048339844
Validation loss: 2.094840387503306

Epoch: 6| Step: 5
Training loss: 1.5568203926086426
Validation loss: 2.096471627553304

Epoch: 6| Step: 6
Training loss: 2.085714340209961
Validation loss: 2.0900838375091553

Epoch: 6| Step: 7
Training loss: 1.109413504600525
Validation loss: 2.0697174072265625

Epoch: 6| Step: 8
Training loss: 1.9934873580932617
Validation loss: 2.056724429130554

Epoch: 6| Step: 9
Training loss: 1.9792187213897705
Validation loss: 2.0619807640711465

Epoch: 6| Step: 10
Training loss: 2.127988815307617
Validation loss: 2.0646971265474954

Epoch: 6| Step: 11
Training loss: 1.739180326461792
Validation loss: 2.0537115335464478

Epoch: 6| Step: 12
Training loss: 0.9914911389350891
Validation loss: 2.0510210394859314

Epoch: 6| Step: 13
Training loss: 2.2428736686706543
Validation loss: 2.0405259927113852

Testing loss: 1.8456643862689999

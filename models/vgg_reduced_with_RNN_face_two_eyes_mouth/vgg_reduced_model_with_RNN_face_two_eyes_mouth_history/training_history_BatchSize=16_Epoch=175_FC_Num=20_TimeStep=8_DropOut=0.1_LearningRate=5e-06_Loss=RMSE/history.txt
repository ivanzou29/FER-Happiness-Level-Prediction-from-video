Epoch: 1| Step: 0
Training loss: 5.326879629875873
Validation loss: 5.851827387125961

Epoch: 6| Step: 1
Training loss: 5.578191676662948
Validation loss: 5.849557313172609

Epoch: 6| Step: 2
Training loss: 7.018424718016211
Validation loss: 5.847312915468143

Epoch: 6| Step: 3
Training loss: 5.528997792489654
Validation loss: 5.845069804213944

Epoch: 6| Step: 4
Training loss: 7.053964503891976
Validation loss: 5.842883095043523

Epoch: 6| Step: 5
Training loss: 6.220468868234438
Validation loss: 5.840619314485888

Epoch: 6| Step: 6
Training loss: 6.3208013659050035
Validation loss: 5.838408941370483

Epoch: 6| Step: 7
Training loss: 5.981726476172426
Validation loss: 5.836267369219219

Epoch: 6| Step: 8
Training loss: 5.310680481723136
Validation loss: 5.834054775130416

Epoch: 6| Step: 9
Training loss: 5.324683008922485
Validation loss: 5.831907733921715

Epoch: 6| Step: 10
Training loss: 5.1138963205556465
Validation loss: 5.829699483446932

Epoch: 6| Step: 11
Training loss: 5.257650341690527
Validation loss: 5.827508506861429

Epoch: 6| Step: 12
Training loss: 5.836186537733579
Validation loss: 5.825227100491292

Epoch: 6| Step: 13
Training loss: 6.965012715718971
Validation loss: 5.822995353080637

Epoch: 2| Step: 0
Training loss: 6.7961655520548865
Validation loss: 5.820531621644298

Epoch: 6| Step: 1
Training loss: 5.487331712788272
Validation loss: 5.817970081020309

Epoch: 6| Step: 2
Training loss: 6.846275386209523
Validation loss: 5.815401617751666

Epoch: 6| Step: 3
Training loss: 5.6812068790692
Validation loss: 5.812778917409107

Epoch: 6| Step: 4
Training loss: 5.288600884582448
Validation loss: 5.809879190673884

Epoch: 6| Step: 5
Training loss: 5.091412154026422
Validation loss: 5.806954202697255

Epoch: 6| Step: 6
Training loss: 6.547789152503655
Validation loss: 5.80398452630008

Epoch: 6| Step: 7
Training loss: 5.138304398205992
Validation loss: 5.800849888456498

Epoch: 6| Step: 8
Training loss: 5.988182509989923
Validation loss: 5.797612008869811

Epoch: 6| Step: 9
Training loss: 6.3126803835033165
Validation loss: 5.79420948696165

Epoch: 6| Step: 10
Training loss: 5.617333507099295
Validation loss: 5.790507642006548

Epoch: 6| Step: 11
Training loss: 5.805204673924283
Validation loss: 5.786861493761338

Epoch: 6| Step: 12
Training loss: 5.861127992980332
Validation loss: 5.782977751575013

Epoch: 6| Step: 13
Training loss: 6.0342883427312834
Validation loss: 5.7788863840415345

Epoch: 3| Step: 0
Training loss: 5.747098771302732
Validation loss: 5.774564489371541

Epoch: 6| Step: 1
Training loss: 6.226786112868352
Validation loss: 5.769922462308223

Epoch: 6| Step: 2
Training loss: 5.622203894892303
Validation loss: 5.765203913721416

Epoch: 6| Step: 3
Training loss: 6.565815696507026
Validation loss: 5.760294282336295

Epoch: 6| Step: 4
Training loss: 6.000234917174435
Validation loss: 5.754634399334794

Epoch: 6| Step: 5
Training loss: 5.960221673824141
Validation loss: 5.749180279227348

Epoch: 6| Step: 6
Training loss: 5.38258305961708
Validation loss: 5.743269395005565

Epoch: 6| Step: 7
Training loss: 5.960599597237995
Validation loss: 5.73749650610192

Epoch: 6| Step: 8
Training loss: 5.573255449021588
Validation loss: 5.731059516561754

Epoch: 6| Step: 9
Training loss: 4.955912001486658
Validation loss: 5.7245742795040435

Epoch: 6| Step: 10
Training loss: 6.11282820805328
Validation loss: 5.717771463803745

Epoch: 6| Step: 11
Training loss: 4.999246349756307
Validation loss: 5.710836246982739

Epoch: 6| Step: 12
Training loss: 5.742276519292974
Validation loss: 5.703643078548335

Epoch: 6| Step: 13
Training loss: 6.82123810210474
Validation loss: 5.696300363357769

Epoch: 4| Step: 0
Training loss: 6.842406258545485
Validation loss: 5.6882768037619105

Epoch: 6| Step: 1
Training loss: 6.196400723604811
Validation loss: 5.6804603355197445

Epoch: 6| Step: 2
Training loss: 5.7162767075232335
Validation loss: 5.6722046587293615

Epoch: 6| Step: 3
Training loss: 5.841319838534326
Validation loss: 5.663575769316736

Epoch: 6| Step: 4
Training loss: 5.939834296487282
Validation loss: 5.655068795808513

Epoch: 6| Step: 5
Training loss: 4.557841391695796
Validation loss: 5.646236428748461

Epoch: 6| Step: 6
Training loss: 6.293630668167992
Validation loss: 5.637398597498896

Epoch: 6| Step: 7
Training loss: 6.313400355422707
Validation loss: 5.62854869659053

Epoch: 6| Step: 8
Training loss: 5.207647883768952
Validation loss: 5.6193154327388655

Epoch: 6| Step: 9
Training loss: 5.734226318427558
Validation loss: 5.610154546693056

Epoch: 6| Step: 10
Training loss: 5.324555485041099
Validation loss: 5.601003621630595

Epoch: 6| Step: 11
Training loss: 4.1228595150762395
Validation loss: 5.591620297341543

Epoch: 6| Step: 12
Training loss: 6.028414202882352
Validation loss: 5.58277276294829

Epoch: 6| Step: 13
Training loss: 5.815318090999986
Validation loss: 5.573737890277855

Epoch: 5| Step: 0
Training loss: 5.208486549666949
Validation loss: 5.564536189892766

Epoch: 6| Step: 1
Training loss: 6.367356449062206
Validation loss: 5.555273269263305

Epoch: 6| Step: 2
Training loss: 5.777661338676194
Validation loss: 5.546116077639486

Epoch: 6| Step: 3
Training loss: 6.01677488756289
Validation loss: 5.536513559443393

Epoch: 6| Step: 4
Training loss: 5.951501980506653
Validation loss: 5.526962601423514

Epoch: 6| Step: 5
Training loss: 5.664331890145452
Validation loss: 5.5172330333177975

Epoch: 6| Step: 6
Training loss: 5.42820963335135
Validation loss: 5.5078704470208235

Epoch: 6| Step: 7
Training loss: 6.038512921710449
Validation loss: 5.497987754779462

Epoch: 6| Step: 8
Training loss: 5.733335811407455
Validation loss: 5.488551159214169

Epoch: 6| Step: 9
Training loss: 5.011149753967591
Validation loss: 5.478398558101954

Epoch: 6| Step: 10
Training loss: 4.447149825567486
Validation loss: 5.468950801977531

Epoch: 6| Step: 11
Training loss: 6.040837233160571
Validation loss: 5.459202190095895

Epoch: 6| Step: 12
Training loss: 5.185437263436625
Validation loss: 5.449707301034754

Epoch: 6| Step: 13
Training loss: 5.555530253458685
Validation loss: 5.440438452189583

Epoch: 6| Step: 0
Training loss: 6.029900750755757
Validation loss: 5.430757193088941

Epoch: 6| Step: 1
Training loss: 4.966201992614206
Validation loss: 5.421373210336885

Epoch: 6| Step: 2
Training loss: 5.415167185279809
Validation loss: 5.412115483651671

Epoch: 6| Step: 3
Training loss: 4.571898304272992
Validation loss: 5.402764521661277

Epoch: 6| Step: 4
Training loss: 5.583219365717039
Validation loss: 5.394029804396804

Epoch: 6| Step: 5
Training loss: 5.128924658132902
Validation loss: 5.385066224531914

Epoch: 6| Step: 6
Training loss: 4.828899235049912
Validation loss: 5.376454829193194

Epoch: 6| Step: 7
Training loss: 6.048620637384215
Validation loss: 5.3677396902062595

Epoch: 6| Step: 8
Training loss: 6.292159745387478
Validation loss: 5.359217279039305

Epoch: 6| Step: 9
Training loss: 6.007380079325801
Validation loss: 5.3501845021980525

Epoch: 6| Step: 10
Training loss: 5.22826719210402
Validation loss: 5.341903575735836

Epoch: 6| Step: 11
Training loss: 5.618432003947667
Validation loss: 5.333613010861755

Epoch: 6| Step: 12
Training loss: 5.028414667770044
Validation loss: 5.325313896753724

Epoch: 6| Step: 13
Training loss: 5.828551085086657
Validation loss: 5.3168119097794095

Epoch: 7| Step: 0
Training loss: 4.473095468335955
Validation loss: 5.3088338730721505

Epoch: 6| Step: 1
Training loss: 6.174912721580818
Validation loss: 5.301018608624301

Epoch: 6| Step: 2
Training loss: 5.812674571051205
Validation loss: 5.293220301957912

Epoch: 6| Step: 3
Training loss: 4.6420507841672825
Validation loss: 5.285527951406336

Epoch: 6| Step: 4
Training loss: 6.99439369218933
Validation loss: 5.277702750403303

Epoch: 6| Step: 5
Training loss: 4.979792386671905
Validation loss: 5.270116986032482

Epoch: 6| Step: 6
Training loss: 5.325765406084325
Validation loss: 5.262268747547893

Epoch: 6| Step: 7
Training loss: 4.1942744329381725
Validation loss: 5.254712457263915

Epoch: 6| Step: 8
Training loss: 5.689903673586862
Validation loss: 5.247018421568975

Epoch: 6| Step: 9
Training loss: 4.834347760962774
Validation loss: 5.239521711327135

Epoch: 6| Step: 10
Training loss: 5.176140170338153
Validation loss: 5.231986431757514

Epoch: 6| Step: 11
Training loss: 5.6383180418078735
Validation loss: 5.2245938899205875

Epoch: 6| Step: 12
Training loss: 5.236957334387247
Validation loss: 5.217616462571431

Epoch: 6| Step: 13
Training loss: 5.534935020058459
Validation loss: 5.21058451511694

Epoch: 8| Step: 0
Training loss: 4.45500127516192
Validation loss: 5.203909233492145

Epoch: 6| Step: 1
Training loss: 5.803482443997156
Validation loss: 5.197332172237741

Epoch: 6| Step: 2
Training loss: 4.825837911477297
Validation loss: 5.190700600732085

Epoch: 6| Step: 3
Training loss: 5.393411021449141
Validation loss: 5.183984568776998

Epoch: 6| Step: 4
Training loss: 5.045488385548624
Validation loss: 5.177700054602242

Epoch: 6| Step: 5
Training loss: 4.003747853193086
Validation loss: 5.171574671266483

Epoch: 6| Step: 6
Training loss: 5.239722045922882
Validation loss: 5.164796162485448

Epoch: 6| Step: 7
Training loss: 5.456520575492361
Validation loss: 5.158475237708894

Epoch: 6| Step: 8
Training loss: 5.303747529247966
Validation loss: 5.151680017406911

Epoch: 6| Step: 9
Training loss: 5.502419199785754
Validation loss: 5.145360124377071

Epoch: 6| Step: 10
Training loss: 5.2448039908644715
Validation loss: 5.138695351790965

Epoch: 6| Step: 11
Training loss: 6.041126446304064
Validation loss: 5.132973523761643

Epoch: 6| Step: 12
Training loss: 5.878302112853716
Validation loss: 5.125852870532857

Epoch: 6| Step: 13
Training loss: 5.40311936598264
Validation loss: 5.119165868016501

Epoch: 9| Step: 0
Training loss: 4.826627925134228
Validation loss: 5.112817691299598

Epoch: 6| Step: 1
Training loss: 5.644446602684937
Validation loss: 5.105423825685814

Epoch: 6| Step: 2
Training loss: 5.524117736799202
Validation loss: 5.09980470246017

Epoch: 6| Step: 3
Training loss: 5.324636441567
Validation loss: 5.093450213700673

Epoch: 6| Step: 4
Training loss: 4.517461594638432
Validation loss: 5.08635069716099

Epoch: 6| Step: 5
Training loss: 5.597433060471119
Validation loss: 5.080338528734937

Epoch: 6| Step: 6
Training loss: 5.435627055699045
Validation loss: 5.073829751476149

Epoch: 6| Step: 7
Training loss: 4.977760446230674
Validation loss: 5.067267821854866

Epoch: 6| Step: 8
Training loss: 4.361466757061117
Validation loss: 5.060527715593958

Epoch: 6| Step: 9
Training loss: 5.24445958934544
Validation loss: 5.054168438681816

Epoch: 6| Step: 10
Training loss: 5.417212864651644
Validation loss: 5.048073357661179

Epoch: 6| Step: 11
Training loss: 5.287282353965197
Validation loss: 5.0421336354461035

Epoch: 6| Step: 12
Training loss: 5.510662494024369
Validation loss: 5.036015688022602

Epoch: 6| Step: 13
Training loss: 4.856544577993402
Validation loss: 5.0295864537890695

Epoch: 10| Step: 0
Training loss: 5.328653706470925
Validation loss: 5.023704541486266

Epoch: 6| Step: 1
Training loss: 4.8418126323361985
Validation loss: 5.017176482232976

Epoch: 6| Step: 2
Training loss: 4.632465832044376
Validation loss: 5.011338443256341

Epoch: 6| Step: 3
Training loss: 4.761621012635167
Validation loss: 5.00471646065004

Epoch: 6| Step: 4
Training loss: 6.1722612863688
Validation loss: 4.998891326058819

Epoch: 6| Step: 5
Training loss: 4.204117781799433
Validation loss: 4.993065762262549

Epoch: 6| Step: 6
Training loss: 5.782000518746025
Validation loss: 4.986244943086577

Epoch: 6| Step: 7
Training loss: 5.884177972753633
Validation loss: 4.97980438787875

Epoch: 6| Step: 8
Training loss: 4.804491786537292
Validation loss: 4.973668477132585

Epoch: 6| Step: 9
Training loss: 4.772307366729334
Validation loss: 4.967075409593969

Epoch: 6| Step: 10
Training loss: 5.557091644701158
Validation loss: 4.960115908375925

Epoch: 6| Step: 11
Training loss: 5.178084687908865
Validation loss: 4.9534817475888495

Epoch: 6| Step: 12
Training loss: 4.651901775028329
Validation loss: 4.946658336515859

Epoch: 6| Step: 13
Training loss: 4.482742916403463
Validation loss: 4.939876837385565

Epoch: 11| Step: 0
Training loss: 5.432425751194058
Validation loss: 4.933295153564628

Epoch: 6| Step: 1
Training loss: 5.435778465916136
Validation loss: 4.927263876232962

Epoch: 6| Step: 2
Training loss: 5.102410939629479
Validation loss: 4.921130734639937

Epoch: 6| Step: 3
Training loss: 4.101758737865056
Validation loss: 4.914764143962518

Epoch: 6| Step: 4
Training loss: 4.8511658044087955
Validation loss: 4.908941741351066

Epoch: 6| Step: 5
Training loss: 5.925872661209908
Validation loss: 4.902980646553617

Epoch: 6| Step: 6
Training loss: 4.7401611423061665
Validation loss: 4.897072698760702

Epoch: 6| Step: 7
Training loss: 4.611498120675339
Validation loss: 4.8912421051463415

Epoch: 6| Step: 8
Training loss: 5.4135190572652485
Validation loss: 4.885671005731883

Epoch: 6| Step: 9
Training loss: 4.290421067494584
Validation loss: 4.879313068924438

Epoch: 6| Step: 10
Training loss: 5.6009670989247295
Validation loss: 4.874419348933554

Epoch: 6| Step: 11
Training loss: 4.6932858172814615
Validation loss: 4.868492249573716

Epoch: 6| Step: 12
Training loss: 5.1309783306062
Validation loss: 4.863566971113412

Epoch: 6| Step: 13
Training loss: 4.595188460610228
Validation loss: 4.857182833162456

Epoch: 12| Step: 0
Training loss: 5.490605046296269
Validation loss: 4.8521901583128395

Epoch: 6| Step: 1
Training loss: 5.575765158656899
Validation loss: 4.845797674037486

Epoch: 6| Step: 2
Training loss: 4.436897344365046
Validation loss: 4.840258383776674

Epoch: 6| Step: 3
Training loss: 4.670013363020605
Validation loss: 4.833936335777712

Epoch: 6| Step: 4
Training loss: 5.17235791121364
Validation loss: 4.827565824355237

Epoch: 6| Step: 5
Training loss: 5.142005804532668
Validation loss: 4.822084864600192

Epoch: 6| Step: 6
Training loss: 5.147749118195905
Validation loss: 4.815951608205602

Epoch: 6| Step: 7
Training loss: 3.4734768664917324
Validation loss: 4.8099231178764175

Epoch: 6| Step: 8
Training loss: 4.141260638121146
Validation loss: 4.804452054010561

Epoch: 6| Step: 9
Training loss: 5.085191147473973
Validation loss: 4.797972811349571

Epoch: 6| Step: 10
Training loss: 4.520787650491335
Validation loss: 4.793268328576515

Epoch: 6| Step: 11
Training loss: 5.326267489172031
Validation loss: 4.786908825528488

Epoch: 6| Step: 12
Training loss: 5.45709276540131
Validation loss: 4.781169500068047

Epoch: 6| Step: 13
Training loss: 5.053963705182377
Validation loss: 4.775230691073935

Epoch: 13| Step: 0
Training loss: 5.426028021521086
Validation loss: 4.7700558269014195

Epoch: 6| Step: 1
Training loss: 4.827979224513501
Validation loss: 4.7646842653397785

Epoch: 6| Step: 2
Training loss: 5.360281019672689
Validation loss: 4.7584964655199355

Epoch: 6| Step: 3
Training loss: 4.300072496379694
Validation loss: 4.751546306889848

Epoch: 6| Step: 4
Training loss: 4.411858634696813
Validation loss: 4.7461373870057555

Epoch: 6| Step: 5
Training loss: 3.9820983366154565
Validation loss: 4.740770709640651

Epoch: 6| Step: 6
Training loss: 5.895992676791194
Validation loss: 4.735145967698201

Epoch: 6| Step: 7
Training loss: 5.355516117701719
Validation loss: 4.727893988593951

Epoch: 6| Step: 8
Training loss: 4.4435233486227865
Validation loss: 4.7218887298606536

Epoch: 6| Step: 9
Training loss: 4.6156414302877
Validation loss: 4.71632877235876

Epoch: 6| Step: 10
Training loss: 5.068526836708433
Validation loss: 4.709839920217707

Epoch: 6| Step: 11
Training loss: 5.2515921222154365
Validation loss: 4.70390274528386

Epoch: 6| Step: 12
Training loss: 3.8006481370816023
Validation loss: 4.699055952939549

Epoch: 6| Step: 13
Training loss: 4.793745480054095
Validation loss: 4.693105338602356

Epoch: 14| Step: 0
Training loss: 3.630918997879881
Validation loss: 4.686931626511553

Epoch: 6| Step: 1
Training loss: 3.7749682494590373
Validation loss: 4.6819017677219215

Epoch: 6| Step: 2
Training loss: 5.346514784129383
Validation loss: 4.676535315858918

Epoch: 6| Step: 3
Training loss: 5.338131454958741
Validation loss: 4.671078259968279

Epoch: 6| Step: 4
Training loss: 5.04535701823035
Validation loss: 4.665147119582516

Epoch: 6| Step: 5
Training loss: 5.026042256989387
Validation loss: 4.660862560311305

Epoch: 6| Step: 6
Training loss: 5.490545469608007
Validation loss: 4.655285353445609

Epoch: 6| Step: 7
Training loss: 5.254260741119434
Validation loss: 4.648766873014416

Epoch: 6| Step: 8
Training loss: 4.964571749531877
Validation loss: 4.643595591357261

Epoch: 6| Step: 9
Training loss: 4.806761850551267
Validation loss: 4.639155573739426

Epoch: 6| Step: 10
Training loss: 4.29413220343939
Validation loss: 4.634170204622678

Epoch: 6| Step: 11
Training loss: 4.900698296750818
Validation loss: 4.628837608547409

Epoch: 6| Step: 12
Training loss: 4.641281338397642
Validation loss: 4.623082803173745

Epoch: 6| Step: 13
Training loss: 3.8862045484690464
Validation loss: 4.6173179509065205

Epoch: 15| Step: 0
Training loss: 5.04500656263607
Validation loss: 4.612187724582682

Epoch: 6| Step: 1
Training loss: 5.8915205459424245
Validation loss: 4.606534472738817

Epoch: 6| Step: 2
Training loss: 4.251877089711171
Validation loss: 4.600440414741019

Epoch: 6| Step: 3
Training loss: 5.031191049550315
Validation loss: 4.596742308512711

Epoch: 6| Step: 4
Training loss: 5.348049285912648
Validation loss: 4.591275402829333

Epoch: 6| Step: 5
Training loss: 4.756322968934558
Validation loss: 4.586808476658495

Epoch: 6| Step: 6
Training loss: 3.794710649157276
Validation loss: 4.581562717480151

Epoch: 6| Step: 7
Training loss: 3.7404444702742317
Validation loss: 4.5758091056505155

Epoch: 6| Step: 8
Training loss: 4.3754544703264155
Validation loss: 4.56982454047963

Epoch: 6| Step: 9
Training loss: 4.237227601604938
Validation loss: 4.565486975325731

Epoch: 6| Step: 10
Training loss: 4.3746921975986455
Validation loss: 4.559256859813818

Epoch: 6| Step: 11
Training loss: 5.215553629702857
Validation loss: 4.555427608915901

Epoch: 6| Step: 12
Training loss: 4.673695289391581
Validation loss: 4.550935398268878

Epoch: 6| Step: 13
Training loss: 4.684457020108327
Validation loss: 4.546806972908031

Epoch: 16| Step: 0
Training loss: 4.02959674379491
Validation loss: 4.542471309858887

Epoch: 6| Step: 1
Training loss: 4.909059145532465
Validation loss: 4.536426017053409

Epoch: 6| Step: 2
Training loss: 4.538705729712793
Validation loss: 4.531937259864912

Epoch: 6| Step: 3
Training loss: 4.56557371514125
Validation loss: 4.527650319719203

Epoch: 6| Step: 4
Training loss: 4.683849082776324
Validation loss: 4.523199959130698

Epoch: 6| Step: 5
Training loss: 4.146643529093506
Validation loss: 4.5174922052600115

Epoch: 6| Step: 6
Training loss: 4.867866578956081
Validation loss: 4.512620465334086

Epoch: 6| Step: 7
Training loss: 4.104632407676531
Validation loss: 4.508285700369534

Epoch: 6| Step: 8
Training loss: 4.509029019328169
Validation loss: 4.503798312294133

Epoch: 6| Step: 9
Training loss: 4.950472437344265
Validation loss: 4.500046570854802

Epoch: 6| Step: 10
Training loss: 4.594878388996598
Validation loss: 4.494395545428359

Epoch: 6| Step: 11
Training loss: 5.082883512679274
Validation loss: 4.4891950021891285

Epoch: 6| Step: 12
Training loss: 4.599401824868799
Validation loss: 4.48422049545328

Epoch: 6| Step: 13
Training loss: 5.176956492455801
Validation loss: 4.479436478879304

Epoch: 17| Step: 0
Training loss: 4.163659396529992
Validation loss: 4.474395174038824

Epoch: 6| Step: 1
Training loss: 4.574669481022176
Validation loss: 4.469623080241907

Epoch: 6| Step: 2
Training loss: 4.03355755668744
Validation loss: 4.4648124580254605

Epoch: 6| Step: 3
Training loss: 4.924475960517807
Validation loss: 4.460212216538679

Epoch: 6| Step: 4
Training loss: 4.125963271983032
Validation loss: 4.455578829534621

Epoch: 6| Step: 5
Training loss: 4.221326027395353
Validation loss: 4.45066835334918

Epoch: 6| Step: 6
Training loss: 4.285602222975695
Validation loss: 4.44597529167097

Epoch: 6| Step: 7
Training loss: 4.807267353937312
Validation loss: 4.442287065837278

Epoch: 6| Step: 8
Training loss: 4.338016595776172
Validation loss: 4.438125243934332

Epoch: 6| Step: 9
Training loss: 4.907601917392022
Validation loss: 4.432515940686841

Epoch: 6| Step: 10
Training loss: 5.703328669843582
Validation loss: 4.427490658440582

Epoch: 6| Step: 11
Training loss: 5.211350912140515
Validation loss: 4.4230304192487315

Epoch: 6| Step: 12
Training loss: 4.15312226283182
Validation loss: 4.416734472989864

Epoch: 6| Step: 13
Training loss: 4.215679266193651
Validation loss: 4.413180228763271

Epoch: 18| Step: 0
Training loss: 4.290813818081081
Validation loss: 4.4088688275776375

Epoch: 6| Step: 1
Training loss: 4.680800877049347
Validation loss: 4.403331466393336

Epoch: 6| Step: 2
Training loss: 4.161168450805202
Validation loss: 4.398463446188829

Epoch: 6| Step: 3
Training loss: 4.735543991147146
Validation loss: 4.393687581776117

Epoch: 6| Step: 4
Training loss: 4.315643491712284
Validation loss: 4.388050252059287

Epoch: 6| Step: 5
Training loss: 4.878326063881634
Validation loss: 4.383677822737477

Epoch: 6| Step: 6
Training loss: 4.779105908717986
Validation loss: 4.378340617390297

Epoch: 6| Step: 7
Training loss: 4.44017141818876
Validation loss: 4.373732828370203

Epoch: 6| Step: 8
Training loss: 4.363974930171129
Validation loss: 4.368512438862606

Epoch: 6| Step: 9
Training loss: 4.121893435444937
Validation loss: 4.3653199053072145

Epoch: 6| Step: 10
Training loss: 4.383815059130146
Validation loss: 4.36068490787904

Epoch: 6| Step: 11
Training loss: 4.573540276459845
Validation loss: 4.35578004554239

Epoch: 6| Step: 12
Training loss: 5.013454454832387
Validation loss: 4.352573498772326

Epoch: 6| Step: 13
Training loss: 4.257533856994802
Validation loss: 4.347543098597295

Epoch: 19| Step: 0
Training loss: 4.454683372471562
Validation loss: 4.344068373760825

Epoch: 6| Step: 1
Training loss: 4.988045133470631
Validation loss: 4.339071814609815

Epoch: 6| Step: 2
Training loss: 4.118193125379129
Validation loss: 4.334468264998503

Epoch: 6| Step: 3
Training loss: 4.054472283621782
Validation loss: 4.328557035364384

Epoch: 6| Step: 4
Training loss: 4.5001563998700105
Validation loss: 4.324042202068684

Epoch: 6| Step: 5
Training loss: 3.9136488766369997
Validation loss: 4.318391176104073

Epoch: 6| Step: 6
Training loss: 4.509991045010424
Validation loss: 4.315337385629391

Epoch: 6| Step: 7
Training loss: 5.165449039995462
Validation loss: 4.310380258063787

Epoch: 6| Step: 8
Training loss: 3.226480330559343
Validation loss: 4.305783556298598

Epoch: 6| Step: 9
Training loss: 4.155884461354518
Validation loss: 4.301038983227863

Epoch: 6| Step: 10
Training loss: 4.711043576490228
Validation loss: 4.296178606417018

Epoch: 6| Step: 11
Training loss: 4.915833655874308
Validation loss: 4.291707615826745

Epoch: 6| Step: 12
Training loss: 3.7990889511197956
Validation loss: 4.286877055090028

Epoch: 6| Step: 13
Training loss: 5.211939039165139
Validation loss: 4.282055521455854

Epoch: 20| Step: 0
Training loss: 4.013410737018895
Validation loss: 4.27770533782483

Epoch: 6| Step: 1
Training loss: 4.610799071594129
Validation loss: 4.27346000223785

Epoch: 6| Step: 2
Training loss: 4.078832104645964
Validation loss: 4.268430502213878

Epoch: 6| Step: 3
Training loss: 3.855695569015123
Validation loss: 4.263316385614762

Epoch: 6| Step: 4
Training loss: 4.97685492358577
Validation loss: 4.258525413067994

Epoch: 6| Step: 5
Training loss: 4.433582564688534
Validation loss: 4.254254175395164

Epoch: 6| Step: 6
Training loss: 3.9541887503122184
Validation loss: 4.24935928825836

Epoch: 6| Step: 7
Training loss: 4.080141000368216
Validation loss: 4.244579111717948

Epoch: 6| Step: 8
Training loss: 4.580151835094502
Validation loss: 4.240051087515529

Epoch: 6| Step: 9
Training loss: 4.518802567594369
Validation loss: 4.234917412395091

Epoch: 6| Step: 10
Training loss: 3.7949196132745446
Validation loss: 4.230912372861903

Epoch: 6| Step: 11
Training loss: 4.705072855045824
Validation loss: 4.225608280793713

Epoch: 6| Step: 12
Training loss: 4.7162222249587975
Validation loss: 4.221434899730271

Epoch: 6| Step: 13
Training loss: 4.754362010815206
Validation loss: 4.217367063310468

Epoch: 21| Step: 0
Training loss: 4.003495596319058
Validation loss: 4.21373488364747

Epoch: 6| Step: 1
Training loss: 4.812830925995302
Validation loss: 4.2081385400591

Epoch: 6| Step: 2
Training loss: 4.256747555323884
Validation loss: 4.204307210266194

Epoch: 6| Step: 3
Training loss: 3.7839860081484553
Validation loss: 4.19928544338995

Epoch: 6| Step: 4
Training loss: 4.4529911489615035
Validation loss: 4.193413331209258

Epoch: 6| Step: 5
Training loss: 4.4171475322630265
Validation loss: 4.1898026090147145

Epoch: 6| Step: 6
Training loss: 3.8601429287237257
Validation loss: 4.1857366502549676

Epoch: 6| Step: 7
Training loss: 4.504411230702446
Validation loss: 4.180569374481264

Epoch: 6| Step: 8
Training loss: 4.943711441989234
Validation loss: 4.17657683893671

Epoch: 6| Step: 9
Training loss: 4.688663185758007
Validation loss: 4.1719777145790236

Epoch: 6| Step: 10
Training loss: 4.413183722327521
Validation loss: 4.16714628638098

Epoch: 6| Step: 11
Training loss: 4.024253036658077
Validation loss: 4.161786543500019

Epoch: 6| Step: 12
Training loss: 3.4286977256492106
Validation loss: 4.157953357855116

Epoch: 6| Step: 13
Training loss: 4.546907510018549
Validation loss: 4.15312055975507

Epoch: 22| Step: 0
Training loss: 4.252674831808524
Validation loss: 4.148540545834271

Epoch: 6| Step: 1
Training loss: 3.5201881464613245
Validation loss: 4.143621317848968

Epoch: 6| Step: 2
Training loss: 4.533316305539932
Validation loss: 4.140275319046905

Epoch: 6| Step: 3
Training loss: 4.1497922362749735
Validation loss: 4.135790458076234

Epoch: 6| Step: 4
Training loss: 5.155109071215934
Validation loss: 4.131659690099081

Epoch: 6| Step: 5
Training loss: 4.238290475580397
Validation loss: 4.126834538506142

Epoch: 6| Step: 6
Training loss: 3.288967683925052
Validation loss: 4.122696686219216

Epoch: 6| Step: 7
Training loss: 4.36561493377508
Validation loss: 4.118144957308362

Epoch: 6| Step: 8
Training loss: 3.803627070039787
Validation loss: 4.113893391511353

Epoch: 6| Step: 9
Training loss: 3.9486364884346914
Validation loss: 4.109079792747284

Epoch: 6| Step: 10
Training loss: 4.729170050038655
Validation loss: 4.105155722374469

Epoch: 6| Step: 11
Training loss: 4.4017832956940275
Validation loss: 4.100875593522062

Epoch: 6| Step: 12
Training loss: 4.123471525976275
Validation loss: 4.096099024266067

Epoch: 6| Step: 13
Training loss: 4.647635664824559
Validation loss: 4.092155437190937

Epoch: 23| Step: 0
Training loss: 4.729611055218238
Validation loss: 4.087314410181459

Epoch: 6| Step: 1
Training loss: 4.169101436048015
Validation loss: 4.082424679038174

Epoch: 6| Step: 2
Training loss: 4.019382722444134
Validation loss: 4.077743083663057

Epoch: 6| Step: 3
Training loss: 4.334607499425094
Validation loss: 4.073368323894331

Epoch: 6| Step: 4
Training loss: 4.306829732951236
Validation loss: 4.068920324523466

Epoch: 6| Step: 5
Training loss: 3.8254110875844014
Validation loss: 4.064985092279642

Epoch: 6| Step: 6
Training loss: 3.7822733353436995
Validation loss: 4.0592447786145085

Epoch: 6| Step: 7
Training loss: 4.005727482135151
Validation loss: 4.055020651241449

Epoch: 6| Step: 8
Training loss: 3.8506724971646085
Validation loss: 4.050574341225545

Epoch: 6| Step: 9
Training loss: 4.659569907253356
Validation loss: 4.046065849801681

Epoch: 6| Step: 10
Training loss: 3.752454844285063
Validation loss: 4.042296285189027

Epoch: 6| Step: 11
Training loss: 4.473156870181718
Validation loss: 4.03729784039157

Epoch: 6| Step: 12
Training loss: 4.321880807824415
Validation loss: 4.03300631014438

Epoch: 6| Step: 13
Training loss: 4.2945363841894455
Validation loss: 4.028898573552442

Epoch: 24| Step: 0
Training loss: 4.681170653624421
Validation loss: 4.024381439192775

Epoch: 6| Step: 1
Training loss: 4.506737645359619
Validation loss: 4.020393380454694

Epoch: 6| Step: 2
Training loss: 3.9142074710396706
Validation loss: 4.016217001336068

Epoch: 6| Step: 3
Training loss: 3.412633251900224
Validation loss: 4.012064145382653

Epoch: 6| Step: 4
Training loss: 4.34080724038386
Validation loss: 4.006371509853704

Epoch: 6| Step: 5
Training loss: 3.991369592898134
Validation loss: 4.002287230625282

Epoch: 6| Step: 6
Training loss: 4.262148827548591
Validation loss: 3.9980004995414773

Epoch: 6| Step: 7
Training loss: 4.52237500508809
Validation loss: 3.993491460623319

Epoch: 6| Step: 8
Training loss: 4.044096828957896
Validation loss: 3.989598060892808

Epoch: 6| Step: 9
Training loss: 3.9035508252628346
Validation loss: 3.9849098993419685

Epoch: 6| Step: 10
Training loss: 4.378437980230908
Validation loss: 3.980222525326305

Epoch: 6| Step: 11
Training loss: 3.9635465858352084
Validation loss: 3.976337798739315

Epoch: 6| Step: 12
Training loss: 3.844198805495985
Validation loss: 3.9713398450582784

Epoch: 6| Step: 13
Training loss: 3.8813729332760554
Validation loss: 3.967069835903649

Epoch: 25| Step: 0
Training loss: 4.030621146899567
Validation loss: 3.9629999593458316

Epoch: 6| Step: 1
Training loss: 4.815572971308002
Validation loss: 3.9590126190909443

Epoch: 6| Step: 2
Training loss: 3.7124774508321274
Validation loss: 3.954577956735412

Epoch: 6| Step: 3
Training loss: 3.692036311556875
Validation loss: 3.949641103575877

Epoch: 6| Step: 4
Training loss: 4.765879739925733
Validation loss: 3.9456499515660757

Epoch: 6| Step: 5
Training loss: 3.576610527866012
Validation loss: 3.9407234520428025

Epoch: 6| Step: 6
Training loss: 3.6419586227555194
Validation loss: 3.9374162372378776

Epoch: 6| Step: 7
Training loss: 4.102559402658754
Validation loss: 3.933181412774845

Epoch: 6| Step: 8
Training loss: 4.006738708491608
Validation loss: 3.9286682141754796

Epoch: 6| Step: 9
Training loss: 4.530746958203204
Validation loss: 3.92430892335448

Epoch: 6| Step: 10
Training loss: 4.049467810055753
Validation loss: 3.9199278964009245

Epoch: 6| Step: 11
Training loss: 4.091031635260016
Validation loss: 3.915437843779956

Epoch: 6| Step: 12
Training loss: 3.4253003121023387
Validation loss: 3.9114548454150597

Epoch: 6| Step: 13
Training loss: 4.238179992310001
Validation loss: 3.906850214303744

Epoch: 26| Step: 0
Training loss: 3.554765704625653
Validation loss: 3.9025421244897216

Epoch: 6| Step: 1
Training loss: 4.3052275884815945
Validation loss: 3.898180324350838

Epoch: 6| Step: 2
Training loss: 4.220789663547703
Validation loss: 3.89394023955077

Epoch: 6| Step: 3
Training loss: 4.251489041838868
Validation loss: 3.889684111412168

Epoch: 6| Step: 4
Training loss: 4.721398425083283
Validation loss: 3.8853106377719198

Epoch: 6| Step: 5
Training loss: 4.266767299975319
Validation loss: 3.881005852515075

Epoch: 6| Step: 6
Training loss: 3.282184576726776
Validation loss: 3.8762460776543644

Epoch: 6| Step: 7
Training loss: 2.450919161835821
Validation loss: 3.8720137256465734

Epoch: 6| Step: 8
Training loss: 4.34514568053908
Validation loss: 3.867988436695108

Epoch: 6| Step: 9
Training loss: 4.322065277213897
Validation loss: 3.863494136898831

Epoch: 6| Step: 10
Training loss: 3.999299107180388
Validation loss: 3.8593093104570033

Epoch: 6| Step: 11
Training loss: 3.9895842363773184
Validation loss: 3.854792109424147

Epoch: 6| Step: 12
Training loss: 3.8750984425499597
Validation loss: 3.8503081243829844

Epoch: 6| Step: 13
Training loss: 4.033180425178251
Validation loss: 3.8461387882182434

Epoch: 27| Step: 0
Training loss: 4.449238469831189
Validation loss: 3.841843594899073

Epoch: 6| Step: 1
Training loss: 4.2141264268823315
Validation loss: 3.837157056231901

Epoch: 6| Step: 2
Training loss: 4.0281625205838365
Validation loss: 3.833098895049717

Epoch: 6| Step: 3
Training loss: 5.000222964083878
Validation loss: 3.8288361803518938

Epoch: 6| Step: 4
Training loss: 4.337152754106221
Validation loss: 3.823705912237449

Epoch: 6| Step: 5
Training loss: 3.513206633491599
Validation loss: 3.8190529586064748

Epoch: 6| Step: 6
Training loss: 3.2900570342155335
Validation loss: 3.814158803142576

Epoch: 6| Step: 7
Training loss: 4.191117730924849
Validation loss: 3.8093823051851996

Epoch: 6| Step: 8
Training loss: 3.2247547322534342
Validation loss: 3.8054203943782654

Epoch: 6| Step: 9
Training loss: 4.085033192680658
Validation loss: 3.800837454620404

Epoch: 6| Step: 10
Training loss: 3.5816586299597204
Validation loss: 3.796401906202874

Epoch: 6| Step: 11
Training loss: 3.4548302491991194
Validation loss: 3.7920898508755623

Epoch: 6| Step: 12
Training loss: 4.110147961925568
Validation loss: 3.7879913598874793

Epoch: 6| Step: 13
Training loss: 3.3729148357371463
Validation loss: 3.783297570237397

Epoch: 28| Step: 0
Training loss: 3.9686079825875984
Validation loss: 3.7794507162110085

Epoch: 6| Step: 1
Training loss: 3.7396078436569815
Validation loss: 3.774959596834535

Epoch: 6| Step: 2
Training loss: 3.5217317567278195
Validation loss: 3.770797595843561

Epoch: 6| Step: 3
Training loss: 3.6891515880202173
Validation loss: 3.7667830336721027

Epoch: 6| Step: 4
Training loss: 3.278777289791462
Validation loss: 3.7631702785009717

Epoch: 6| Step: 5
Training loss: 3.75555542476235
Validation loss: 3.759020044952419

Epoch: 6| Step: 6
Training loss: 4.469968829633001
Validation loss: 3.7549010991914917

Epoch: 6| Step: 7
Training loss: 3.8991617818074706
Validation loss: 3.7508236934117334

Epoch: 6| Step: 8
Training loss: 4.568366472712528
Validation loss: 3.7467484894796947

Epoch: 6| Step: 9
Training loss: 3.457293174002375
Validation loss: 3.742418211725117

Epoch: 6| Step: 10
Training loss: 4.657300587355977
Validation loss: 3.7379703638267068

Epoch: 6| Step: 11
Training loss: 4.147094739138549
Validation loss: 3.7329706293983023

Epoch: 6| Step: 12
Training loss: 3.4210151044818224
Validation loss: 3.7291694229087375

Epoch: 6| Step: 13
Training loss: 3.5176098455156386
Validation loss: 3.724821055677718

Epoch: 29| Step: 0
Training loss: 3.621029751406537
Validation loss: 3.720374342975683

Epoch: 6| Step: 1
Training loss: 3.8666248258979676
Validation loss: 3.715953042211675

Epoch: 6| Step: 2
Training loss: 4.130615804358689
Validation loss: 3.711408019101771

Epoch: 6| Step: 3
Training loss: 3.9547448740775892
Validation loss: 3.7071251490511536

Epoch: 6| Step: 4
Training loss: 3.969180947282821
Validation loss: 3.7031854766757593

Epoch: 6| Step: 5
Training loss: 4.717438168102794
Validation loss: 3.698606242327355

Epoch: 6| Step: 6
Training loss: 3.8095340135414975
Validation loss: 3.6940190079053177

Epoch: 6| Step: 7
Training loss: 3.8879269969507098
Validation loss: 3.6898856339482715

Epoch: 6| Step: 8
Training loss: 3.5725387210229096
Validation loss: 3.6851339454237366

Epoch: 6| Step: 9
Training loss: 3.3618009102879065
Validation loss: 3.6804180135558555

Epoch: 6| Step: 10
Training loss: 2.959301176832158
Validation loss: 3.6761209815355955

Epoch: 6| Step: 11
Training loss: 3.6562902986310823
Validation loss: 3.671939995373067

Epoch: 6| Step: 12
Training loss: 4.339775187914764
Validation loss: 3.667561219650746

Epoch: 6| Step: 13
Training loss: 3.451354250274618
Validation loss: 3.663385140194704

Epoch: 30| Step: 0
Training loss: 4.001586837724334
Validation loss: 3.6593797770012118

Epoch: 6| Step: 1
Training loss: 2.7074884906499515
Validation loss: 3.6548509882772

Epoch: 6| Step: 2
Training loss: 4.35493533028414
Validation loss: 3.650940694605394

Epoch: 6| Step: 3
Training loss: 3.9668912364530673
Validation loss: 3.6465910787146574

Epoch: 6| Step: 4
Training loss: 3.2047192583598245
Validation loss: 3.642586961178123

Epoch: 6| Step: 5
Training loss: 4.101540411026829
Validation loss: 3.6385003303883097

Epoch: 6| Step: 6
Training loss: 2.823764199892016
Validation loss: 3.6344406471872728

Epoch: 6| Step: 7
Training loss: 3.1597605906708255
Validation loss: 3.630218345303551

Epoch: 6| Step: 8
Training loss: 4.027227716411624
Validation loss: 3.626233910331285

Epoch: 6| Step: 9
Training loss: 3.727177151387128
Validation loss: 3.6221865116561562

Epoch: 6| Step: 10
Training loss: 3.9188269210639883
Validation loss: 3.6182443216047746

Epoch: 6| Step: 11
Training loss: 3.610200861787495
Validation loss: 3.614224268570275

Epoch: 6| Step: 12
Training loss: 4.779318026490793
Validation loss: 3.609964253820119

Epoch: 6| Step: 13
Training loss: 3.7953900231539373
Validation loss: 3.605875925621113

Epoch: 31| Step: 0
Training loss: 4.655908968936825
Validation loss: 3.6014946898767644

Epoch: 6| Step: 1
Training loss: 3.471181069150894
Validation loss: 3.597041118551894

Epoch: 6| Step: 2
Training loss: 3.509309648837757
Validation loss: 3.592737779563921

Epoch: 6| Step: 3
Training loss: 3.3082494275065124
Validation loss: 3.5882837270192027

Epoch: 6| Step: 4
Training loss: 3.800180134770148
Validation loss: 3.584291721785597

Epoch: 6| Step: 5
Training loss: 3.5834500197699777
Validation loss: 3.5800108402947624

Epoch: 6| Step: 6
Training loss: 3.672735365044085
Validation loss: 3.5757206997319373

Epoch: 6| Step: 7
Training loss: 3.8856084263230133
Validation loss: 3.571768922028833

Epoch: 6| Step: 8
Training loss: 3.9843629126271924
Validation loss: 3.56755566049883

Epoch: 6| Step: 9
Training loss: 3.2971574562894235
Validation loss: 3.563466415049325

Epoch: 6| Step: 10
Training loss: 2.5216694120537975
Validation loss: 3.559361330027868

Epoch: 6| Step: 11
Training loss: 3.670297876477493
Validation loss: 3.5553872785549903

Epoch: 6| Step: 12
Training loss: 3.848648547489085
Validation loss: 3.5513860217564264

Epoch: 6| Step: 13
Training loss: 4.319104872613626
Validation loss: 3.5470930080101892

Epoch: 32| Step: 0
Training loss: 3.739125569979721
Validation loss: 3.542921756431324

Epoch: 6| Step: 1
Training loss: 3.354270285828294
Validation loss: 3.538834913288974

Epoch: 6| Step: 2
Training loss: 4.049129609192699
Validation loss: 3.534429159966499

Epoch: 6| Step: 3
Training loss: 4.318308801928537
Validation loss: 3.5303703945523233

Epoch: 6| Step: 4
Training loss: 3.3667416731029727
Validation loss: 3.5259493989954143

Epoch: 6| Step: 5
Training loss: 3.6406548625719988
Validation loss: 3.521374941067163

Epoch: 6| Step: 6
Training loss: 2.0452499345108652
Validation loss: 3.517378645201839

Epoch: 6| Step: 7
Training loss: 3.6747316327361697
Validation loss: 3.513426029410872

Epoch: 6| Step: 8
Training loss: 3.0737779003116565
Validation loss: 3.5094916297416767

Epoch: 6| Step: 9
Training loss: 4.044814597880301
Validation loss: 3.5059272667448473

Epoch: 6| Step: 10
Training loss: 3.3473991375429
Validation loss: 3.5018298724374373

Epoch: 6| Step: 11
Training loss: 4.357661084662755
Validation loss: 3.4978858943370996

Epoch: 6| Step: 12
Training loss: 3.8238992840100967
Validation loss: 3.4938053988828366

Epoch: 6| Step: 13
Training loss: 3.7215529705287245
Validation loss: 3.489738829075737

Epoch: 33| Step: 0
Training loss: 3.752441882776889
Validation loss: 3.4856639354041272

Epoch: 6| Step: 1
Training loss: 3.4773294010179123
Validation loss: 3.481681866342526

Epoch: 6| Step: 2
Training loss: 3.2814816347741993
Validation loss: 3.477800767380437

Epoch: 6| Step: 3
Training loss: 2.8175220157605962
Validation loss: 3.473550493281803

Epoch: 6| Step: 4
Training loss: 3.531624968208921
Validation loss: 3.4698013797205296

Epoch: 6| Step: 5
Training loss: 3.1972644318873766
Validation loss: 3.4658247151458372

Epoch: 6| Step: 6
Training loss: 3.7451374317005004
Validation loss: 3.462398017088357

Epoch: 6| Step: 7
Training loss: 2.669421670265007
Validation loss: 3.458310429753828

Epoch: 6| Step: 8
Training loss: 3.956550892842321
Validation loss: 3.4548656512581513

Epoch: 6| Step: 9
Training loss: 4.230836635666202
Validation loss: 3.451063458463793

Epoch: 6| Step: 10
Training loss: 3.687498706882056
Validation loss: 3.4468216477775777

Epoch: 6| Step: 11
Training loss: 3.883661250293497
Validation loss: 3.443065645928827

Epoch: 6| Step: 12
Training loss: 3.802099330642002
Validation loss: 3.4388985765355278

Epoch: 6| Step: 13
Training loss: 3.9813041072504345
Validation loss: 3.4350837075789196

Epoch: 34| Step: 0
Training loss: 3.8817520382049646
Validation loss: 3.4311411564140943

Epoch: 6| Step: 1
Training loss: 3.543028236907577
Validation loss: 3.426785804950917

Epoch: 6| Step: 2
Training loss: 2.844723377690888
Validation loss: 3.422674123506506

Epoch: 6| Step: 3
Training loss: 4.34605686762505
Validation loss: 3.4189748217055507

Epoch: 6| Step: 4
Training loss: 2.917805331350972
Validation loss: 3.414776003702385

Epoch: 6| Step: 5
Training loss: 3.159638351799167
Validation loss: 3.410423917923856

Epoch: 6| Step: 6
Training loss: 3.7777312749605985
Validation loss: 3.40667473481026

Epoch: 6| Step: 7
Training loss: 3.703143542279527
Validation loss: 3.4029730569940755

Epoch: 6| Step: 8
Training loss: 2.812202013972132
Validation loss: 3.3988974658232523

Epoch: 6| Step: 9
Training loss: 3.742498524269929
Validation loss: 3.395061444197963

Epoch: 6| Step: 10
Training loss: 3.24749336412323
Validation loss: 3.391366880385052

Epoch: 6| Step: 11
Training loss: 3.8487889205684342
Validation loss: 3.387482530236799

Epoch: 6| Step: 12
Training loss: 3.7950080708655376
Validation loss: 3.3835892112323926

Epoch: 6| Step: 13
Training loss: 3.6323941553897843
Validation loss: 3.3797791869362577

Epoch: 35| Step: 0
Training loss: 3.6257610179925988
Validation loss: 3.376135105408177

Epoch: 6| Step: 1
Training loss: 3.14076209836605
Validation loss: 3.372045872424162

Epoch: 6| Step: 2
Training loss: 3.685080041313712
Validation loss: 3.3679870930603335

Epoch: 6| Step: 3
Training loss: 3.4046235007179027
Validation loss: 3.3641626352873932

Epoch: 6| Step: 4
Training loss: 3.338143184564481
Validation loss: 3.3601743589869826

Epoch: 6| Step: 5
Training loss: 4.361649770872815
Validation loss: 3.356367932928722

Epoch: 6| Step: 6
Training loss: 2.8651611485189488
Validation loss: 3.352409249902067

Epoch: 6| Step: 7
Training loss: 3.395519187453984
Validation loss: 3.3487125185883406

Epoch: 6| Step: 8
Training loss: 3.3363235571860925
Validation loss: 3.3445082425706785

Epoch: 6| Step: 9
Training loss: 3.2790464722556028
Validation loss: 3.3411787372890625

Epoch: 6| Step: 10
Training loss: 4.029657093538345
Validation loss: 3.3373971168385976

Epoch: 6| Step: 11
Training loss: 3.3641238691133655
Validation loss: 3.333429994771072

Epoch: 6| Step: 12
Training loss: 3.7293408031759325
Validation loss: 3.329665196268132

Epoch: 6| Step: 13
Training loss: 3.0485184369861584
Validation loss: 3.3258935913104635

Epoch: 36| Step: 0
Training loss: 2.579470658965255
Validation loss: 3.3220769046300367

Epoch: 6| Step: 1
Training loss: 4.007893641433121
Validation loss: 3.318911962394539

Epoch: 6| Step: 2
Training loss: 3.701924318509651
Validation loss: 3.315483776731863

Epoch: 6| Step: 3
Training loss: 2.425418451917243
Validation loss: 3.311857221125779

Epoch: 6| Step: 4
Training loss: 2.8760677511010306
Validation loss: 3.3082811132068355

Epoch: 6| Step: 5
Training loss: 3.015877668636879
Validation loss: 3.305003650834476

Epoch: 6| Step: 6
Training loss: 4.0866339183032485
Validation loss: 3.3016062049843464

Epoch: 6| Step: 7
Training loss: 4.063042237700932
Validation loss: 3.2978423810068

Epoch: 6| Step: 8
Training loss: 3.771466927312697
Validation loss: 3.294156971447631

Epoch: 6| Step: 9
Training loss: 3.272694505662357
Validation loss: 3.290202012126149

Epoch: 6| Step: 10
Training loss: 3.595002461990978
Validation loss: 3.286389410119389

Epoch: 6| Step: 11
Training loss: 2.9677552815440853
Validation loss: 3.2825935156372634

Epoch: 6| Step: 12
Training loss: 3.6286870686970647
Validation loss: 3.279459219407833

Epoch: 6| Step: 13
Training loss: 3.5789673992163324
Validation loss: 3.2756889205913717

Epoch: 37| Step: 0
Training loss: 3.8109653073055294
Validation loss: 3.272057654411475

Epoch: 6| Step: 1
Training loss: 3.8877232772637784
Validation loss: 3.269383094800426

Epoch: 6| Step: 2
Training loss: 3.3788712516251036
Validation loss: 3.2661714476891546

Epoch: 6| Step: 3
Training loss: 3.1642728759150898
Validation loss: 3.2612861392093944

Epoch: 6| Step: 4
Training loss: 2.8561469862911126
Validation loss: 3.2571705674384583

Epoch: 6| Step: 5
Training loss: 2.6092138183517903
Validation loss: 3.254775683325049

Epoch: 6| Step: 6
Training loss: 3.4373569805563964
Validation loss: 3.2501075433397473

Epoch: 6| Step: 7
Training loss: 3.9155513041046572
Validation loss: 3.2472036631648167

Epoch: 6| Step: 8
Training loss: 2.8877907544677393
Validation loss: 3.244111546511072

Epoch: 6| Step: 9
Training loss: 3.319938656573438
Validation loss: 3.242024024610793

Epoch: 6| Step: 10
Training loss: 3.5071697368374233
Validation loss: 3.237891431045877

Epoch: 6| Step: 11
Training loss: 3.31704544000377
Validation loss: 3.2341440459275077

Epoch: 6| Step: 12
Training loss: 3.3780750817943264
Validation loss: 3.2296623136722227

Epoch: 6| Step: 13
Training loss: 3.6892362483404795
Validation loss: 3.225955379532245

Epoch: 38| Step: 0
Training loss: 3.6654901061803264
Validation loss: 3.22208495962162

Epoch: 6| Step: 1
Training loss: 3.313370788127139
Validation loss: 3.2185265793772775

Epoch: 6| Step: 2
Training loss: 3.245032255132917
Validation loss: 3.2150260098019747

Epoch: 6| Step: 3
Training loss: 3.5974219097810387
Validation loss: 3.2116202118237

Epoch: 6| Step: 4
Training loss: 3.0479640481585717
Validation loss: 3.2082745848061798

Epoch: 6| Step: 5
Training loss: 3.5265869139779924
Validation loss: 3.2045894221551094

Epoch: 6| Step: 6
Training loss: 3.2215552096386615
Validation loss: 3.2013484711393065

Epoch: 6| Step: 7
Training loss: 3.0106500097387836
Validation loss: 3.1974198434867236

Epoch: 6| Step: 8
Training loss: 3.35819752484048
Validation loss: 3.194379202319293

Epoch: 6| Step: 9
Training loss: 3.7400852103904594
Validation loss: 3.1908249810116884

Epoch: 6| Step: 10
Training loss: 3.425704415828795
Validation loss: 3.187457664059

Epoch: 6| Step: 11
Training loss: 3.822948209682681
Validation loss: 3.184144504080301

Epoch: 6| Step: 12
Training loss: 3.0622287552272804
Validation loss: 3.1808679812227187

Epoch: 6| Step: 13
Training loss: 2.5069580523919095
Validation loss: 3.176918605015123

Epoch: 39| Step: 0
Training loss: 3.5777277163707963
Validation loss: 3.1735674472274966

Epoch: 6| Step: 1
Training loss: 3.281403601548039
Validation loss: 3.1700986702020386

Epoch: 6| Step: 2
Training loss: 3.6385396461397477
Validation loss: 3.16640465472263

Epoch: 6| Step: 3
Training loss: 3.558199914103077
Validation loss: 3.1631077083267516

Epoch: 6| Step: 4
Training loss: 3.0615829340311334
Validation loss: 3.1606554188043074

Epoch: 6| Step: 5
Training loss: 2.9813098445619883
Validation loss: 3.156637300202335

Epoch: 6| Step: 6
Training loss: 3.013379619912435
Validation loss: 3.1535285320984157

Epoch: 6| Step: 7
Training loss: 3.3367009953102618
Validation loss: 3.150000450093878

Epoch: 6| Step: 8
Training loss: 3.5227785022847615
Validation loss: 3.1462167586201186

Epoch: 6| Step: 9
Training loss: 3.0075517021168165
Validation loss: 3.1430015365656705

Epoch: 6| Step: 10
Training loss: 3.3462971050379773
Validation loss: 3.140753988524037

Epoch: 6| Step: 11
Training loss: 3.0386050941630973
Validation loss: 3.1384178997307584

Epoch: 6| Step: 12
Training loss: 3.209724289886881
Validation loss: 3.13685081270435

Epoch: 6| Step: 13
Training loss: 3.457933487048947
Validation loss: 3.1338117596089736

Epoch: 40| Step: 0
Training loss: 3.2031104110757704
Validation loss: 3.130786979791768

Epoch: 6| Step: 1
Training loss: 2.776958626658374
Validation loss: 3.1271322097025296

Epoch: 6| Step: 2
Training loss: 3.98950499348962
Validation loss: 3.1239343733640905

Epoch: 6| Step: 3
Training loss: 4.082687923780533
Validation loss: 3.119870706532424

Epoch: 6| Step: 4
Training loss: 3.003109274355614
Validation loss: 3.1162019986708147

Epoch: 6| Step: 5
Training loss: 2.8642904420874116
Validation loss: 3.1123411641069554

Epoch: 6| Step: 6
Training loss: 3.5281874618204556
Validation loss: 3.1087111590323335

Epoch: 6| Step: 7
Training loss: 3.1174588575302087
Validation loss: 3.105870685917428

Epoch: 6| Step: 8
Training loss: 3.0887262418937813
Validation loss: 3.102961265646757

Epoch: 6| Step: 9
Training loss: 3.0181785405904766
Validation loss: 3.1013045503984125

Epoch: 6| Step: 10
Training loss: 3.0456938190227523
Validation loss: 3.0982879853961336

Epoch: 6| Step: 11
Training loss: 3.233461112679711
Validation loss: 3.09470324488954

Epoch: 6| Step: 12
Training loss: 3.1721540055861492
Validation loss: 3.0910046788129524

Epoch: 6| Step: 13
Training loss: 3.1108631042750883
Validation loss: 3.0872999056947217

Epoch: 41| Step: 0
Training loss: 3.2181816710406927
Validation loss: 3.084033826351402

Epoch: 6| Step: 1
Training loss: 3.3014624620655213
Validation loss: 3.081531337457582

Epoch: 6| Step: 2
Training loss: 3.0664429486259377
Validation loss: 3.0783879511486365

Epoch: 6| Step: 3
Training loss: 3.087450646380513
Validation loss: 3.0753124574533914

Epoch: 6| Step: 4
Training loss: 3.70100438687658
Validation loss: 3.0720923702911267

Epoch: 6| Step: 5
Training loss: 3.463699100138294
Validation loss: 3.0694887125458417

Epoch: 6| Step: 6
Training loss: 3.087360141072958
Validation loss: 3.066348518751431

Epoch: 6| Step: 7
Training loss: 3.1865253828430316
Validation loss: 3.0631114323399937

Epoch: 6| Step: 8
Training loss: 2.93145519587622
Validation loss: 3.060372502419679

Epoch: 6| Step: 9
Training loss: 3.361005236707
Validation loss: 3.0571263976432834

Epoch: 6| Step: 10
Training loss: 2.7360566989589623
Validation loss: 3.0546969180274948

Epoch: 6| Step: 11
Training loss: 3.0679004308963553
Validation loss: 3.0519223523351346

Epoch: 6| Step: 12
Training loss: 3.3232386003598773
Validation loss: 3.0494406958590354

Epoch: 6| Step: 13
Training loss: 3.26824670713462
Validation loss: 3.046559152779022

Epoch: 42| Step: 0
Training loss: 2.5771632394658286
Validation loss: 3.0430820951625286

Epoch: 6| Step: 1
Training loss: 2.1049638755036373
Validation loss: 3.0404851569610263

Epoch: 6| Step: 2
Training loss: 3.3373743040724992
Validation loss: 3.0378784121888827

Epoch: 6| Step: 3
Training loss: 3.6819299558890637
Validation loss: 3.035386556678064

Epoch: 6| Step: 4
Training loss: 2.8424040575032326
Validation loss: 3.0326370395516644

Epoch: 6| Step: 5
Training loss: 3.210379076289366
Validation loss: 3.029824646050229

Epoch: 6| Step: 6
Training loss: 2.7356399661921316
Validation loss: 3.0273373708350126

Epoch: 6| Step: 7
Training loss: 3.756643767830065
Validation loss: 3.02453222835332

Epoch: 6| Step: 8
Training loss: 3.3521446309353933
Validation loss: 3.0218957983785164

Epoch: 6| Step: 9
Training loss: 2.9481954454000068
Validation loss: 3.019295061817165

Epoch: 6| Step: 10
Training loss: 2.9915079087094343
Validation loss: 3.0164325930652027

Epoch: 6| Step: 11
Training loss: 3.423070829282198
Validation loss: 3.013806323391797

Epoch: 6| Step: 12
Training loss: 3.3511234164777965
Validation loss: 3.0114012079528707

Epoch: 6| Step: 13
Training loss: 3.5719715495840108
Validation loss: 3.0084894384598497

Epoch: 43| Step: 0
Training loss: 3.8370876724451155
Validation loss: 3.0052980423908418

Epoch: 6| Step: 1
Training loss: 2.917120507670949
Validation loss: 3.0019958400396756

Epoch: 6| Step: 2
Training loss: 3.0451141746926442
Validation loss: 2.9991336074777872

Epoch: 6| Step: 3
Training loss: 3.000232846442678
Validation loss: 2.9960467331971685

Epoch: 6| Step: 4
Training loss: 3.0325412163130934
Validation loss: 2.9932662174783085

Epoch: 6| Step: 5
Training loss: 2.9882713567033945
Validation loss: 2.9894564709124816

Epoch: 6| Step: 6
Training loss: 3.4139606801088953
Validation loss: 2.987223412600186

Epoch: 6| Step: 7
Training loss: 3.269222924063405
Validation loss: 2.9837854944895184

Epoch: 6| Step: 8
Training loss: 2.916619618354259
Validation loss: 2.981046674767153

Epoch: 6| Step: 9
Training loss: 2.2261267520088612
Validation loss: 2.9786888930989304

Epoch: 6| Step: 10
Training loss: 3.5022184970301242
Validation loss: 2.9759269563177764

Epoch: 6| Step: 11
Training loss: 3.1712537636319187
Validation loss: 2.973484533796237

Epoch: 6| Step: 12
Training loss: 3.1194497219851396
Validation loss: 2.9707355618717632

Epoch: 6| Step: 13
Training loss: 3.1079021396818254
Validation loss: 2.968316672305328

Epoch: 44| Step: 0
Training loss: 3.096366315496474
Validation loss: 2.965704604039523

Epoch: 6| Step: 1
Training loss: 3.646734260778785
Validation loss: 2.963222712206704

Epoch: 6| Step: 2
Training loss: 2.545805349575982
Validation loss: 2.9601045374625023

Epoch: 6| Step: 3
Training loss: 2.419321695428886
Validation loss: 2.957451639733971

Epoch: 6| Step: 4
Training loss: 3.007035588583191
Validation loss: 2.9552350991110097

Epoch: 6| Step: 5
Training loss: 3.0796511202308072
Validation loss: 2.953086852780959

Epoch: 6| Step: 6
Training loss: 2.478254348053779
Validation loss: 2.9525595767873956

Epoch: 6| Step: 7
Training loss: 3.255755536689611
Validation loss: 2.949613660094336

Epoch: 6| Step: 8
Training loss: 3.3047590676352168
Validation loss: 2.9461595310611686

Epoch: 6| Step: 9
Training loss: 3.5164300293574997
Validation loss: 2.9425985985384697

Epoch: 6| Step: 10
Training loss: 3.41197297774031
Validation loss: 2.9402856266776087

Epoch: 6| Step: 11
Training loss: 2.925395619299883
Validation loss: 2.9374672772904864

Epoch: 6| Step: 12
Training loss: 2.9419692109945577
Validation loss: 2.9348995385856

Epoch: 6| Step: 13
Training loss: 3.3295101811324512
Validation loss: 2.931736926945964

Epoch: 45| Step: 0
Training loss: 2.8759080862497397
Validation loss: 2.9293641178945524

Epoch: 6| Step: 1
Training loss: 2.6921954278281235
Validation loss: 2.9273554764801113

Epoch: 6| Step: 2
Training loss: 3.1489018175961525
Validation loss: 2.92355872072309

Epoch: 6| Step: 3
Training loss: 2.9478363637242353
Validation loss: 2.922505519786677

Epoch: 6| Step: 4
Training loss: 3.514168263731182
Validation loss: 2.919911287388483

Epoch: 6| Step: 5
Training loss: 3.045947124340366
Validation loss: 2.9170370457131716

Epoch: 6| Step: 6
Training loss: 2.1187742698877297
Validation loss: 2.9141978759956157

Epoch: 6| Step: 7
Training loss: 3.3933146835094443
Validation loss: 2.91199570793667

Epoch: 6| Step: 8
Training loss: 3.1478481451050198
Validation loss: 2.9096443016121216

Epoch: 6| Step: 9
Training loss: 2.5204228679143243
Validation loss: 2.907403218065426

Epoch: 6| Step: 10
Training loss: 3.640652505009431
Validation loss: 2.9053568954661935

Epoch: 6| Step: 11
Training loss: 3.2999288146693053
Validation loss: 2.902152286613163

Epoch: 6| Step: 12
Training loss: 2.8924050288983194
Validation loss: 2.899357940038153

Epoch: 6| Step: 13
Training loss: 3.203558617991669
Validation loss: 2.8972160733886048

Epoch: 46| Step: 0
Training loss: 2.8313290481816393
Validation loss: 2.895227085204779

Epoch: 6| Step: 1
Training loss: 3.2432336061781246
Validation loss: 2.891904035897486

Epoch: 6| Step: 2
Training loss: 3.5814422822098892
Validation loss: 2.889397602855179

Epoch: 6| Step: 3
Training loss: 2.853456134946269
Validation loss: 2.887976579295547

Epoch: 6| Step: 4
Training loss: 3.1584597868419437
Validation loss: 2.8832682427646423

Epoch: 6| Step: 5
Training loss: 2.717411106203327
Validation loss: 2.8819120908297378

Epoch: 6| Step: 6
Training loss: 3.3423965424395146
Validation loss: 2.8786563290346634

Epoch: 6| Step: 7
Training loss: 2.757595410326178
Validation loss: 2.876499476062699

Epoch: 6| Step: 8
Training loss: 3.1387657621497587
Validation loss: 2.873991084329876

Epoch: 6| Step: 9
Training loss: 2.5417807714978973
Validation loss: 2.8728583278546522

Epoch: 6| Step: 10
Training loss: 2.658698266390943
Validation loss: 2.8703569708161605

Epoch: 6| Step: 11
Training loss: 3.046154386939747
Validation loss: 2.8689669319969315

Epoch: 6| Step: 12
Training loss: 2.7687954591071833
Validation loss: 2.8668293187846063

Epoch: 6| Step: 13
Training loss: 3.4527582172287454
Validation loss: 2.864992526334787

Epoch: 47| Step: 0
Training loss: 2.7510888804946574
Validation loss: 2.86233770029758

Epoch: 6| Step: 1
Training loss: 3.094115572821823
Validation loss: 2.8620437924855344

Epoch: 6| Step: 2
Training loss: 3.088507941042063
Validation loss: 2.8577094030094354

Epoch: 6| Step: 3
Training loss: 2.6633339162482943
Validation loss: 2.8569566961221606

Epoch: 6| Step: 4
Training loss: 2.599320003645907
Validation loss: 2.854394114729419

Epoch: 6| Step: 5
Training loss: 2.6870032450205974
Validation loss: 2.8529433717327874

Epoch: 6| Step: 6
Training loss: 3.3689144511945943
Validation loss: 2.8504059558212336

Epoch: 6| Step: 7
Training loss: 2.9396649762735705
Validation loss: 2.84753589349619

Epoch: 6| Step: 8
Training loss: 2.774868362328774
Validation loss: 2.845677571388142

Epoch: 6| Step: 9
Training loss: 3.6768795889942996
Validation loss: 2.843149226520035

Epoch: 6| Step: 10
Training loss: 2.9465332739255783
Validation loss: 2.8417097185365807

Epoch: 6| Step: 11
Training loss: 3.3303911257777488
Validation loss: 2.8404817769587933

Epoch: 6| Step: 12
Training loss: 2.7964359556872127
Validation loss: 2.839146293617053

Epoch: 6| Step: 13
Training loss: 2.95423698682357
Validation loss: 2.8387285410177614

Epoch: 48| Step: 0
Training loss: 2.765175820636142
Validation loss: 2.837505415072435

Epoch: 6| Step: 1
Training loss: 2.609074009878961
Validation loss: 2.8367884176404026

Epoch: 6| Step: 2
Training loss: 3.4807386368599817
Validation loss: 2.8357522026456348

Epoch: 6| Step: 3
Training loss: 3.1838037339683276
Validation loss: 2.831420299357007

Epoch: 6| Step: 4
Training loss: 2.9007680730831202
Validation loss: 2.8291863966540864

Epoch: 6| Step: 5
Training loss: 2.6667520588871967
Validation loss: 2.8264479187079274

Epoch: 6| Step: 6
Training loss: 3.211787721509937
Validation loss: 2.8240833311560363

Epoch: 6| Step: 7
Training loss: 2.388700421116734
Validation loss: 2.8218735788876153

Epoch: 6| Step: 8
Training loss: 3.150512768623741
Validation loss: 2.8196415406871758

Epoch: 6| Step: 9
Training loss: 3.0862332359282005
Validation loss: 2.818093213056982

Epoch: 6| Step: 10
Training loss: 2.9533326414062975
Validation loss: 2.8159401799417356

Epoch: 6| Step: 11
Training loss: 3.081455810358774
Validation loss: 2.8133861417318835

Epoch: 6| Step: 12
Training loss: 3.103504869887752
Validation loss: 2.811123122397768

Epoch: 6| Step: 13
Training loss: 2.7431433769578746
Validation loss: 2.8077593940435284

Epoch: 49| Step: 0
Training loss: 3.2781623481826645
Validation loss: 2.8184273179135357

Epoch: 6| Step: 1
Training loss: 3.107494609947778
Validation loss: 2.807197220062252

Epoch: 6| Step: 2
Training loss: 2.884610078268795
Validation loss: 2.8032596959781846

Epoch: 6| Step: 3
Training loss: 2.6351269247332936
Validation loss: 2.800464933259726

Epoch: 6| Step: 4
Training loss: 2.3838646738827753
Validation loss: 2.797693385012948

Epoch: 6| Step: 5
Training loss: 3.001868937397096
Validation loss: 2.796606963827193

Epoch: 6| Step: 6
Training loss: 3.2974748834053993
Validation loss: 2.7945870643895296

Epoch: 6| Step: 7
Training loss: 3.0170079356118573
Validation loss: 2.7919360548018246

Epoch: 6| Step: 8
Training loss: 2.917619513226905
Validation loss: 2.7900742691163365

Epoch: 6| Step: 9
Training loss: 3.2675283625450935
Validation loss: 2.788136945748921

Epoch: 6| Step: 10
Training loss: 2.829464421360192
Validation loss: 2.7866974249728695

Epoch: 6| Step: 11
Training loss: 2.518760101828184
Validation loss: 2.782904229427379

Epoch: 6| Step: 12
Training loss: 2.686403805042637
Validation loss: 2.7834095375304218

Epoch: 6| Step: 13
Training loss: 3.11574924722482
Validation loss: 2.7817582798250613

Epoch: 50| Step: 0
Training loss: 3.5522859278801766
Validation loss: 2.7803238173095073

Epoch: 6| Step: 1
Training loss: 2.684094600221276
Validation loss: 2.7789356589403345

Epoch: 6| Step: 2
Training loss: 2.903143422178162
Validation loss: 2.7776559903679625

Epoch: 6| Step: 3
Training loss: 2.9625883708923655
Validation loss: 2.7757628242961223

Epoch: 6| Step: 4
Training loss: 3.166269243967254
Validation loss: 2.774849474034804

Epoch: 6| Step: 5
Training loss: 2.793208953558567
Validation loss: 2.773110849549153

Epoch: 6| Step: 6
Training loss: 2.7847044847218934
Validation loss: 2.7709017771439517

Epoch: 6| Step: 7
Training loss: 2.886189785242895
Validation loss: 2.76907122422746

Epoch: 6| Step: 8
Training loss: 2.8072434364002277
Validation loss: 2.7658117055281997

Epoch: 6| Step: 9
Training loss: 2.2425704788478997
Validation loss: 2.764319965051317

Epoch: 6| Step: 10
Training loss: 2.864610225666856
Validation loss: 2.7614969023250233

Epoch: 6| Step: 11
Training loss: 2.9971926587422044
Validation loss: 2.760221356853939

Epoch: 6| Step: 12
Training loss: 2.790037182522555
Validation loss: 2.7571829850918914

Epoch: 6| Step: 13
Training loss: 3.140342737675428
Validation loss: 2.755148691686826

Epoch: 51| Step: 0
Training loss: 3.2698538380659237
Validation loss: 2.751219681268436

Epoch: 6| Step: 1
Training loss: 2.739187746290847
Validation loss: 2.757841763867953

Epoch: 6| Step: 2
Training loss: 3.1181744612064315
Validation loss: 2.7554758635485808

Epoch: 6| Step: 3
Training loss: 3.2076415434935073
Validation loss: 2.7465605033878537

Epoch: 6| Step: 4
Training loss: 2.734060214860654
Validation loss: 2.7462542917886243

Epoch: 6| Step: 5
Training loss: 2.5471636836726645
Validation loss: 2.746142919180705

Epoch: 6| Step: 6
Training loss: 2.8279516683330597
Validation loss: 2.7445092294187656

Epoch: 6| Step: 7
Training loss: 2.98270917007091
Validation loss: 2.7456003888141027

Epoch: 6| Step: 8
Training loss: 3.255878853467904
Validation loss: 2.7437109125433565

Epoch: 6| Step: 9
Training loss: 2.5341586132312557
Validation loss: 2.741788800735784

Epoch: 6| Step: 10
Training loss: 2.882481667161725
Validation loss: 2.7437619636987356

Epoch: 6| Step: 11
Training loss: 2.7702864678499655
Validation loss: 2.7438068300008465

Epoch: 6| Step: 12
Training loss: 2.748304277740304
Validation loss: 2.7381854437349236

Epoch: 6| Step: 13
Training loss: 2.7138947728784246
Validation loss: 2.7408607077070015

Epoch: 52| Step: 0
Training loss: 2.7581354659949713
Validation loss: 2.742064050284833

Epoch: 6| Step: 1
Training loss: 2.543131317708326
Validation loss: 2.741964521335759

Epoch: 6| Step: 2
Training loss: 3.0928206155621742
Validation loss: 2.7434590752415144

Epoch: 6| Step: 3
Training loss: 2.9111362522518256
Validation loss: 2.7395067724014344

Epoch: 6| Step: 4
Training loss: 2.7882607213165382
Validation loss: 2.7367086715432807

Epoch: 6| Step: 5
Training loss: 3.414624759282358
Validation loss: 2.7326163167531536

Epoch: 6| Step: 6
Training loss: 2.4788627651955837
Validation loss: 2.7284987314184126

Epoch: 6| Step: 7
Training loss: 3.057671457818436
Validation loss: 2.7275114257228195

Epoch: 6| Step: 8
Training loss: 2.962891751555076
Validation loss: 2.7230625661236365

Epoch: 6| Step: 9
Training loss: 2.774548031540884
Validation loss: 2.719842464732851

Epoch: 6| Step: 10
Training loss: 3.024459153683886
Validation loss: 2.7170642873838173

Epoch: 6| Step: 11
Training loss: 2.8099026024388944
Validation loss: 2.714072841169985

Epoch: 6| Step: 12
Training loss: 3.059997314501812
Validation loss: 2.7127394610063056

Epoch: 6| Step: 13
Training loss: 2.2441663350490924
Validation loss: 2.7117504912606574

Epoch: 53| Step: 0
Training loss: 3.2352973249490926
Validation loss: 2.708345432743531

Epoch: 6| Step: 1
Training loss: 2.9718576328480126
Validation loss: 2.7130487083330515

Epoch: 6| Step: 2
Training loss: 3.1204359676745383
Validation loss: 2.705259696659874

Epoch: 6| Step: 3
Training loss: 2.3481117860220713
Validation loss: 2.703185740470174

Epoch: 6| Step: 4
Training loss: 3.100883198102819
Validation loss: 2.7059228489382936

Epoch: 6| Step: 5
Training loss: 2.686629930754636
Validation loss: 2.709927256148612

Epoch: 6| Step: 6
Training loss: 2.719439890356237
Validation loss: 2.7088018647870404

Epoch: 6| Step: 7
Training loss: 2.7288046638021703
Validation loss: 2.7037403089937806

Epoch: 6| Step: 8
Training loss: 2.9588925186856296
Validation loss: 2.7011210928018996

Epoch: 6| Step: 9
Training loss: 3.0867286923344213
Validation loss: 2.700188454773917

Epoch: 6| Step: 10
Training loss: 2.8614434437086063
Validation loss: 2.702051658905507

Epoch: 6| Step: 11
Training loss: 3.0133440157045497
Validation loss: 2.6982624293886692

Epoch: 6| Step: 12
Training loss: 2.3871436607109673
Validation loss: 2.697520183950857

Epoch: 6| Step: 13
Training loss: 2.4322796207976496
Validation loss: 2.6948963525878327

Epoch: 54| Step: 0
Training loss: 2.689310572883295
Validation loss: 2.694535184809244

Epoch: 6| Step: 1
Training loss: 3.1166473150077505
Validation loss: 2.693895699203602

Epoch: 6| Step: 2
Training loss: 2.938381468634119
Validation loss: 2.692567107126751

Epoch: 6| Step: 3
Training loss: 2.734949542038675
Validation loss: 2.6897526400873697

Epoch: 6| Step: 4
Training loss: 2.9907972328074255
Validation loss: 2.6880135008656447

Epoch: 6| Step: 5
Training loss: 3.193615301766747
Validation loss: 2.6845968963724087

Epoch: 6| Step: 6
Training loss: 2.607497219182961
Validation loss: 2.6831233738170774

Epoch: 6| Step: 7
Training loss: 2.788723794154855
Validation loss: 2.681975060205947

Epoch: 6| Step: 8
Training loss: 2.491029476314497
Validation loss: 2.6807968616307094

Epoch: 6| Step: 9
Training loss: 2.4045346574004993
Validation loss: 2.680794741995389

Epoch: 6| Step: 10
Training loss: 2.2434002191889504
Validation loss: 2.6804912018064964

Epoch: 6| Step: 11
Training loss: 3.1868267470284874
Validation loss: 2.679156563138129

Epoch: 6| Step: 12
Training loss: 2.962907040474467
Validation loss: 2.6779654765089145

Epoch: 6| Step: 13
Training loss: 3.009425931917459
Validation loss: 2.675232679283367

Epoch: 55| Step: 0
Training loss: 2.8558277577625084
Validation loss: 2.6736879351479037

Epoch: 6| Step: 1
Training loss: 3.045447225316022
Validation loss: 2.6709463427871736

Epoch: 6| Step: 2
Training loss: 3.3857203577227466
Validation loss: 2.66784118079267

Epoch: 6| Step: 3
Training loss: 2.697607152753332
Validation loss: 2.6676887450913482

Epoch: 6| Step: 4
Training loss: 2.437036323461371
Validation loss: 2.6665197620061303

Epoch: 6| Step: 5
Training loss: 2.928452213532493
Validation loss: 2.6651690917201347

Epoch: 6| Step: 6
Training loss: 2.818402348836439
Validation loss: 2.6618110636997954

Epoch: 6| Step: 7
Training loss: 2.208858739381492
Validation loss: 2.6607572125988823

Epoch: 6| Step: 8
Training loss: 2.813918540106306
Validation loss: 2.6598780967868505

Epoch: 6| Step: 9
Training loss: 2.939701472845616
Validation loss: 2.6571879058721533

Epoch: 6| Step: 10
Training loss: 2.811851426683093
Validation loss: 2.6555351492187724

Epoch: 6| Step: 11
Training loss: 3.1348742314714246
Validation loss: 2.6561177987504294

Epoch: 6| Step: 12
Training loss: 2.295990546555945
Validation loss: 2.6561591338528983

Epoch: 6| Step: 13
Training loss: 2.6656670087431147
Validation loss: 2.654899927254043

Epoch: 56| Step: 0
Training loss: 3.0783304058805245
Validation loss: 2.6523059510808773

Epoch: 6| Step: 1
Training loss: 3.125085448050531
Validation loss: 2.6491196765634073

Epoch: 6| Step: 2
Training loss: 2.1728322506164273
Validation loss: 2.650728148462454

Epoch: 6| Step: 3
Training loss: 2.7030426189239485
Validation loss: 2.650771546357781

Epoch: 6| Step: 4
Training loss: 2.811881611975879
Validation loss: 2.652078172349565

Epoch: 6| Step: 5
Training loss: 2.793610953811372
Validation loss: 2.653475774994309

Epoch: 6| Step: 6
Training loss: 2.8377002460553165
Validation loss: 2.653095406650687

Epoch: 6| Step: 7
Training loss: 2.7650872693767345
Validation loss: 2.6532607068932164

Epoch: 6| Step: 8
Training loss: 2.7966964920448056
Validation loss: 2.652238906488606

Epoch: 6| Step: 9
Training loss: 2.9944412866818353
Validation loss: 2.650352663289761

Epoch: 6| Step: 10
Training loss: 2.716866377840298
Validation loss: 2.6460960187696716

Epoch: 6| Step: 11
Training loss: 2.2688443166921415
Validation loss: 2.6410823328178803

Epoch: 6| Step: 12
Training loss: 2.255258243939414
Validation loss: 2.6420906258180477

Epoch: 6| Step: 13
Training loss: 3.473617300578592
Validation loss: 2.6472454858135537

Epoch: 57| Step: 0
Training loss: 3.2634971293358896
Validation loss: 2.66004136220663

Epoch: 6| Step: 1
Training loss: 2.658797355341324
Validation loss: 2.678169895835907

Epoch: 6| Step: 2
Training loss: 2.8447857322523884
Validation loss: 2.6784445768927543

Epoch: 6| Step: 3
Training loss: 2.7218224714183634
Validation loss: 2.6690038085363397

Epoch: 6| Step: 4
Training loss: 2.3995528718560806
Validation loss: 2.660137279142233

Epoch: 6| Step: 5
Training loss: 3.182135162417607
Validation loss: 2.660192249444754

Epoch: 6| Step: 6
Training loss: 2.973381204718665
Validation loss: 2.6568550019254547

Epoch: 6| Step: 7
Training loss: 2.5042143586029146
Validation loss: 2.6560403030499327

Epoch: 6| Step: 8
Training loss: 2.7605554701737565
Validation loss: 2.658435177023084

Epoch: 6| Step: 9
Training loss: 3.0280682096874125
Validation loss: 2.65838055902544

Epoch: 6| Step: 10
Training loss: 2.5617724990906527
Validation loss: 2.6453958334868277

Epoch: 6| Step: 11
Training loss: 2.7022555502013037
Validation loss: 2.637569021011887

Epoch: 6| Step: 12
Training loss: 2.3695059020626363
Validation loss: 2.6381732159832363

Epoch: 6| Step: 13
Training loss: 3.0409706225094206
Validation loss: 2.6456023413281633

Epoch: 58| Step: 0
Training loss: 2.5478555862786836
Validation loss: 2.6357482631180362

Epoch: 6| Step: 1
Training loss: 2.7505868805694966
Validation loss: 2.6452219599645006

Epoch: 6| Step: 2
Training loss: 2.4806881785018327
Validation loss: 2.6441167734822724

Epoch: 6| Step: 3
Training loss: 2.8752729866787963
Validation loss: 2.6341963092359393

Epoch: 6| Step: 4
Training loss: 2.8155584024730897
Validation loss: 2.630224448176706

Epoch: 6| Step: 5
Training loss: 2.580631771360546
Validation loss: 2.6279667937098354

Epoch: 6| Step: 6
Training loss: 3.107522537260201
Validation loss: 2.624550008804988

Epoch: 6| Step: 7
Training loss: 2.7538145658773985
Validation loss: 2.6219600907689737

Epoch: 6| Step: 8
Training loss: 3.058120710446709
Validation loss: 2.619965675798922

Epoch: 6| Step: 9
Training loss: 2.596511198375581
Validation loss: 2.6176236069236554

Epoch: 6| Step: 10
Training loss: 2.9924029479386682
Validation loss: 2.6162652663508297

Epoch: 6| Step: 11
Training loss: 2.359273542029021
Validation loss: 2.616184661190289

Epoch: 6| Step: 12
Training loss: 2.8485440935790276
Validation loss: 2.6149295157226455

Epoch: 6| Step: 13
Training loss: 2.7510701611473265
Validation loss: 2.611733685891131

Epoch: 59| Step: 0
Training loss: 2.91019049502712
Validation loss: 2.6108065350307115

Epoch: 6| Step: 1
Training loss: 2.687322566362958
Validation loss: 2.6093971419013635

Epoch: 6| Step: 2
Training loss: 3.023524872062001
Validation loss: 2.6076297672370115

Epoch: 6| Step: 3
Training loss: 2.322696167839897
Validation loss: 2.6104290597859436

Epoch: 6| Step: 4
Training loss: 2.2710076944681936
Validation loss: 2.620462173800308

Epoch: 6| Step: 5
Training loss: 2.06116974442476
Validation loss: 2.6165108940101347

Epoch: 6| Step: 6
Training loss: 3.058672478881786
Validation loss: 2.614139797894246

Epoch: 6| Step: 7
Training loss: 3.0263013263243304
Validation loss: 2.6057272138987932

Epoch: 6| Step: 8
Training loss: 2.5937050505270713
Validation loss: 2.5993015824559143

Epoch: 6| Step: 9
Training loss: 2.4484932294880117
Validation loss: 2.598807478956073

Epoch: 6| Step: 10
Training loss: 2.8093065569627056
Validation loss: 2.5998480171630023

Epoch: 6| Step: 11
Training loss: 3.099522916937713
Validation loss: 2.6019413455516482

Epoch: 6| Step: 12
Training loss: 3.064733819402439
Validation loss: 2.606617717584145

Epoch: 6| Step: 13
Training loss: 2.8473260214475107
Validation loss: 2.606127760937936

Epoch: 60| Step: 0
Training loss: 2.6124129285402224
Validation loss: 2.6105570752092375

Epoch: 6| Step: 1
Training loss: 3.028057816491397
Validation loss: 2.611620760840627

Epoch: 6| Step: 2
Training loss: 2.937046908883634
Validation loss: 2.60803075341462

Epoch: 6| Step: 3
Training loss: 2.343220561946466
Validation loss: 2.6067340148297125

Epoch: 6| Step: 4
Training loss: 2.628050711689729
Validation loss: 2.605314434158018

Epoch: 6| Step: 5
Training loss: 2.4477733380718067
Validation loss: 2.6057582467531897

Epoch: 6| Step: 6
Training loss: 3.001086515131023
Validation loss: 2.601021635023983

Epoch: 6| Step: 7
Training loss: 2.48723912236907
Validation loss: 2.5993068718732073

Epoch: 6| Step: 8
Training loss: 3.1626417949989407
Validation loss: 2.5969292792237666

Epoch: 6| Step: 9
Training loss: 3.1900965268828587
Validation loss: 2.5932797461426413

Epoch: 6| Step: 10
Training loss: 2.6220957721063782
Validation loss: 2.597018101831451

Epoch: 6| Step: 11
Training loss: 3.015775844774086
Validation loss: 2.6065432321814805

Epoch: 6| Step: 12
Training loss: 2.0623483313078896
Validation loss: 2.6109117866983422

Epoch: 6| Step: 13
Training loss: 2.722603358188722
Validation loss: 2.5938854105582405

Epoch: 61| Step: 0
Training loss: 2.912916842907021
Validation loss: 2.590139595575433

Epoch: 6| Step: 1
Training loss: 2.2102502624214706
Validation loss: 2.5846127346526524

Epoch: 6| Step: 2
Training loss: 2.7851247478057144
Validation loss: 2.584530373591931

Epoch: 6| Step: 3
Training loss: 2.3775189744130008
Validation loss: 2.584604355683888

Epoch: 6| Step: 4
Training loss: 2.634879639281646
Validation loss: 2.584968070171812

Epoch: 6| Step: 5
Training loss: 2.7455511520252998
Validation loss: 2.584776295907059

Epoch: 6| Step: 6
Training loss: 2.947418350499523
Validation loss: 2.5828773547316906

Epoch: 6| Step: 7
Training loss: 2.3712286871483674
Validation loss: 2.583988865362847

Epoch: 6| Step: 8
Training loss: 2.677410793439014
Validation loss: 2.5831286339003956

Epoch: 6| Step: 9
Training loss: 3.057908489297621
Validation loss: 2.5802487036131923

Epoch: 6| Step: 10
Training loss: 2.9632216528268245
Validation loss: 2.5813108700278624

Epoch: 6| Step: 11
Training loss: 2.903869638527783
Validation loss: 2.5790938589473806

Epoch: 6| Step: 12
Training loss: 2.6328004559431024
Validation loss: 2.5735606315599773

Epoch: 6| Step: 13
Training loss: 2.755123394316072
Validation loss: 2.5815532971045596

Epoch: 62| Step: 0
Training loss: 3.15666301782136
Validation loss: 2.5780444662443402

Epoch: 6| Step: 1
Training loss: 2.907291184829313
Validation loss: 2.574401416771141

Epoch: 6| Step: 2
Training loss: 2.2641023978541144
Validation loss: 2.5742513821452966

Epoch: 6| Step: 3
Training loss: 2.7621762599515693
Validation loss: 2.5707868855225513

Epoch: 6| Step: 4
Training loss: 2.605341094634418
Validation loss: 2.5710099345613346

Epoch: 6| Step: 5
Training loss: 2.624961216957966
Validation loss: 2.5679685655513924

Epoch: 6| Step: 6
Training loss: 2.0855128395345655
Validation loss: 2.5667352799331327

Epoch: 6| Step: 7
Training loss: 2.716051626110863
Validation loss: 2.5661463497020174

Epoch: 6| Step: 8
Training loss: 2.794936120765363
Validation loss: 2.566245512861642

Epoch: 6| Step: 9
Training loss: 3.243288005056016
Validation loss: 2.5627455865052107

Epoch: 6| Step: 10
Training loss: 2.709249801118989
Validation loss: 2.566206925779006

Epoch: 6| Step: 11
Training loss: 2.8087506204017867
Validation loss: 2.564728380486878

Epoch: 6| Step: 12
Training loss: 2.1584309039160785
Validation loss: 2.5633675262029207

Epoch: 6| Step: 13
Training loss: 2.7726888156503637
Validation loss: 2.5640506704593387

Epoch: 63| Step: 0
Training loss: 2.6837691203956715
Validation loss: 2.5616704947302993

Epoch: 6| Step: 1
Training loss: 2.5687395749959534
Validation loss: 2.5623541999835027

Epoch: 6| Step: 2
Training loss: 2.3574718353110113
Validation loss: 2.5623900692317676

Epoch: 6| Step: 3
Training loss: 3.0196635820573974
Validation loss: 2.561019027557352

Epoch: 6| Step: 4
Training loss: 2.6545181013802526
Validation loss: 2.561422664295216

Epoch: 6| Step: 5
Training loss: 2.9053832371583517
Validation loss: 2.5591418894896725

Epoch: 6| Step: 6
Training loss: 2.8636962256842198
Validation loss: 2.5592767711353517

Epoch: 6| Step: 7
Training loss: 2.9514203677025828
Validation loss: 2.557838775058608

Epoch: 6| Step: 8
Training loss: 2.7988804281901993
Validation loss: 2.5585967008073793

Epoch: 6| Step: 9
Training loss: 3.046917254815173
Validation loss: 2.5579552394876837

Epoch: 6| Step: 10
Training loss: 2.1888815740386725
Validation loss: 2.5565360377916573

Epoch: 6| Step: 11
Training loss: 2.622181241834144
Validation loss: 2.561969640168275

Epoch: 6| Step: 12
Training loss: 2.788726529954938
Validation loss: 2.5585288935005117

Epoch: 6| Step: 13
Training loss: 2.068717370827579
Validation loss: 2.557232319336792

Epoch: 64| Step: 0
Training loss: 2.8891023491792747
Validation loss: 2.5550737934691106

Epoch: 6| Step: 1
Training loss: 2.5496375892262146
Validation loss: 2.5569848202177847

Epoch: 6| Step: 2
Training loss: 2.5560853266196824
Validation loss: 2.5548032837429697

Epoch: 6| Step: 3
Training loss: 2.3116024440580176
Validation loss: 2.5513319090011977

Epoch: 6| Step: 4
Training loss: 3.3648554610205132
Validation loss: 2.5505883853237608

Epoch: 6| Step: 5
Training loss: 2.425364386327057
Validation loss: 2.55224157867801

Epoch: 6| Step: 6
Training loss: 2.4187377298521944
Validation loss: 2.550098228432613

Epoch: 6| Step: 7
Training loss: 2.2460199758604356
Validation loss: 2.549465235255477

Epoch: 6| Step: 8
Training loss: 2.9408242104456304
Validation loss: 2.5502334481634485

Epoch: 6| Step: 9
Training loss: 2.9704122607852663
Validation loss: 2.546134560261761

Epoch: 6| Step: 10
Training loss: 2.800304675555848
Validation loss: 2.5501046639199356

Epoch: 6| Step: 11
Training loss: 2.7565424861252765
Validation loss: 2.546846502725823

Epoch: 6| Step: 12
Training loss: 2.7555186644344087
Validation loss: 2.5448719096847436

Epoch: 6| Step: 13
Training loss: 2.29993012363602
Validation loss: 2.5452748676200936

Epoch: 65| Step: 0
Training loss: 2.670408415257592
Validation loss: 2.547805070239661

Epoch: 6| Step: 1
Training loss: 2.866348185331933
Validation loss: 2.5461042521204456

Epoch: 6| Step: 2
Training loss: 2.873151101737945
Validation loss: 2.5476887344961154

Epoch: 6| Step: 3
Training loss: 3.184887282743956
Validation loss: 2.548705318995343

Epoch: 6| Step: 4
Training loss: 2.018158850316566
Validation loss: 2.550723158450901

Epoch: 6| Step: 5
Training loss: 2.695970095703491
Validation loss: 2.5518953889942746

Epoch: 6| Step: 6
Training loss: 2.1777217847520025
Validation loss: 2.5513379831576564

Epoch: 6| Step: 7
Training loss: 2.905692426457565
Validation loss: 2.5471614372334357

Epoch: 6| Step: 8
Training loss: 2.697346591493862
Validation loss: 2.545915902812767

Epoch: 6| Step: 9
Training loss: 2.5937774840587817
Validation loss: 2.5454893652951625

Epoch: 6| Step: 10
Training loss: 2.5136367807257836
Validation loss: 2.545021224992002

Epoch: 6| Step: 11
Training loss: 2.673360132985222
Validation loss: 2.546616866871441

Epoch: 6| Step: 12
Training loss: 2.7217488904572145
Validation loss: 2.549664286411827

Epoch: 6| Step: 13
Training loss: 2.8368799414363703
Validation loss: 2.551900846742985

Epoch: 66| Step: 0
Training loss: 3.223945794314814
Validation loss: 2.555830346988724

Epoch: 6| Step: 1
Training loss: 2.5593536413323905
Validation loss: 2.5601614849497643

Epoch: 6| Step: 2
Training loss: 2.4103192686967385
Validation loss: 2.557185640228352

Epoch: 6| Step: 3
Training loss: 2.8119046746778413
Validation loss: 2.5532157461598497

Epoch: 6| Step: 4
Training loss: 2.319877095584364
Validation loss: 2.550390380074105

Epoch: 6| Step: 5
Training loss: 3.0421462704255466
Validation loss: 2.5442177107440904

Epoch: 6| Step: 6
Training loss: 2.6236133546035982
Validation loss: 2.5427350055483884

Epoch: 6| Step: 7
Training loss: 2.9382084742902626
Validation loss: 2.541685226768725

Epoch: 6| Step: 8
Training loss: 3.112482559584201
Validation loss: 2.5368854912384853

Epoch: 6| Step: 9
Training loss: 2.5115195947361535
Validation loss: 2.5386414633633176

Epoch: 6| Step: 10
Training loss: 2.421073042469276
Validation loss: 2.53584845154069

Epoch: 6| Step: 11
Training loss: 1.6359000331956777
Validation loss: 2.5336608708442543

Epoch: 6| Step: 12
Training loss: 3.0607422435848854
Validation loss: 2.532849846261975

Epoch: 6| Step: 13
Training loss: 2.3877811845499006
Validation loss: 2.5337017728346334

Epoch: 67| Step: 0
Training loss: 2.477760097581235
Validation loss: 2.530082920233453

Epoch: 6| Step: 1
Training loss: 2.418104423697672
Validation loss: 2.530374721010053

Epoch: 6| Step: 2
Training loss: 2.3520444458505345
Validation loss: 2.5345274973817773

Epoch: 6| Step: 3
Training loss: 2.816951111721042
Validation loss: 2.5281300881440303

Epoch: 6| Step: 4
Training loss: 3.1693345593471616
Validation loss: 2.5300285940351217

Epoch: 6| Step: 5
Training loss: 2.7392696495956366
Validation loss: 2.543613865714212

Epoch: 6| Step: 6
Training loss: 2.9411760419957464
Validation loss: 2.5399050209730216

Epoch: 6| Step: 7
Training loss: 2.6591544932875077
Validation loss: 2.548313849977403

Epoch: 6| Step: 8
Training loss: 2.4114759099881393
Validation loss: 2.550068793272679

Epoch: 6| Step: 9
Training loss: 2.868084094316328
Validation loss: 2.5401350133035945

Epoch: 6| Step: 10
Training loss: 2.7861180082797627
Validation loss: 2.530010469337082

Epoch: 6| Step: 11
Training loss: 2.536400723178425
Validation loss: 2.525753961807633

Epoch: 6| Step: 12
Training loss: 2.4177502362390504
Validation loss: 2.530928430421286

Epoch: 6| Step: 13
Training loss: 2.72340590720748
Validation loss: 2.5357379921816223

Epoch: 68| Step: 0
Training loss: 2.5297012300454487
Validation loss: 2.5409081426784046

Epoch: 6| Step: 1
Training loss: 2.6805273580523004
Validation loss: 2.5548124448065117

Epoch: 6| Step: 2
Training loss: 2.358588889825545
Validation loss: 2.578195698086638

Epoch: 6| Step: 3
Training loss: 2.55366892442735
Validation loss: 2.596934879498584

Epoch: 6| Step: 4
Training loss: 2.524497361718029
Validation loss: 2.601935572790025

Epoch: 6| Step: 5
Training loss: 2.9630623392376503
Validation loss: 2.5897022053285057

Epoch: 6| Step: 6
Training loss: 2.7592134456496047
Validation loss: 2.569110856152724

Epoch: 6| Step: 7
Training loss: 2.8964081520757126
Validation loss: 2.5558408880811374

Epoch: 6| Step: 8
Training loss: 2.9773057225005255
Validation loss: 2.553306852017455

Epoch: 6| Step: 9
Training loss: 2.4201354639900843
Validation loss: 2.550546990737525

Epoch: 6| Step: 10
Training loss: 3.242982918477014
Validation loss: 2.5483692053230023

Epoch: 6| Step: 11
Training loss: 2.5497682204281378
Validation loss: 2.532835491310615

Epoch: 6| Step: 12
Training loss: 2.9983439643110184
Validation loss: 2.5334351609076786

Epoch: 6| Step: 13
Training loss: 2.0320427154722074
Validation loss: 2.528372475135229

Epoch: 69| Step: 0
Training loss: 2.5225374961160685
Validation loss: 2.52241859297114

Epoch: 6| Step: 1
Training loss: 2.107478214391148
Validation loss: 2.5262820231078345

Epoch: 6| Step: 2
Training loss: 2.641704271050883
Validation loss: 2.5216451288813313

Epoch: 6| Step: 3
Training loss: 2.4159435856655787
Validation loss: 2.5128339839643576

Epoch: 6| Step: 4
Training loss: 2.2522738940427787
Validation loss: 2.520604798437552

Epoch: 6| Step: 5
Training loss: 3.085135440171809
Validation loss: 2.5162366988185356

Epoch: 6| Step: 6
Training loss: 2.8666589234121855
Validation loss: 2.520229130576319

Epoch: 6| Step: 7
Training loss: 2.6226863655949852
Validation loss: 2.513946352232254

Epoch: 6| Step: 8
Training loss: 2.569994689401565
Validation loss: 2.516106569308476

Epoch: 6| Step: 9
Training loss: 3.0019175441124943
Validation loss: 2.515352886007011

Epoch: 6| Step: 10
Training loss: 2.871959239234807
Validation loss: 2.5107686652072436

Epoch: 6| Step: 11
Training loss: 2.964358645672919
Validation loss: 2.510432141732497

Epoch: 6| Step: 12
Training loss: 2.8064612090357253
Validation loss: 2.513152413884768

Epoch: 6| Step: 13
Training loss: 2.2105468836284015
Validation loss: 2.5068522959146824

Epoch: 70| Step: 0
Training loss: 2.9234464774715176
Validation loss: 2.5107871028958457

Epoch: 6| Step: 1
Training loss: 3.0754547155101015
Validation loss: 2.510076900826475

Epoch: 6| Step: 2
Training loss: 2.4432439372772903
Validation loss: 2.5104463715332543

Epoch: 6| Step: 3
Training loss: 2.373673520335123
Validation loss: 2.513104884400684

Epoch: 6| Step: 4
Training loss: 2.4350473829355175
Validation loss: 2.5108791310509697

Epoch: 6| Step: 5
Training loss: 2.280556769402331
Validation loss: 2.5116873779236095

Epoch: 6| Step: 6
Training loss: 3.2736455081957296
Validation loss: 2.5136598766073153

Epoch: 6| Step: 7
Training loss: 2.794837422564485
Validation loss: 2.5124926761501474

Epoch: 6| Step: 8
Training loss: 2.699448218301024
Validation loss: 2.510971076707602

Epoch: 6| Step: 9
Training loss: 3.04069604584827
Validation loss: 2.5055099009714907

Epoch: 6| Step: 10
Training loss: 2.0361968618148634
Validation loss: 2.506176487200481

Epoch: 6| Step: 11
Training loss: 2.1422821090808033
Validation loss: 2.502933211962257

Epoch: 6| Step: 12
Training loss: 2.3456136796018963
Validation loss: 2.511343873141195

Epoch: 6| Step: 13
Training loss: 2.8269338892036764
Validation loss: 2.510693077181804

Epoch: 71| Step: 0
Training loss: 2.7959369812659314
Validation loss: 2.516051325401379

Epoch: 6| Step: 1
Training loss: 2.9856759476302535
Validation loss: 2.515367419717192

Epoch: 6| Step: 2
Training loss: 2.380447665034461
Validation loss: 2.5023309807953305

Epoch: 6| Step: 3
Training loss: 2.842025737133171
Validation loss: 2.5012020400197956

Epoch: 6| Step: 4
Training loss: 2.6219673804284374
Validation loss: 2.5004150999683348

Epoch: 6| Step: 5
Training loss: 2.0424106046987798
Validation loss: 2.5015233484937713

Epoch: 6| Step: 6
Training loss: 2.748841561895404
Validation loss: 2.502828111000131

Epoch: 6| Step: 7
Training loss: 2.613020948096132
Validation loss: 2.5042669440157646

Epoch: 6| Step: 8
Training loss: 2.974180054068441
Validation loss: 2.503007549970485

Epoch: 6| Step: 9
Training loss: 3.0927381305708876
Validation loss: 2.5043058349309457

Epoch: 6| Step: 10
Training loss: 2.4022437098072698
Validation loss: 2.5047992575864972

Epoch: 6| Step: 11
Training loss: 2.4704625430978235
Validation loss: 2.5052924085904817

Epoch: 6| Step: 12
Training loss: 2.4967293325622832
Validation loss: 2.5035612331820514

Epoch: 6| Step: 13
Training loss: 2.41073032757301
Validation loss: 2.504273759093302

Epoch: 72| Step: 0
Training loss: 2.358253667269892
Validation loss: 2.513052689477223

Epoch: 6| Step: 1
Training loss: 2.681367575461736
Validation loss: 2.504333078839412

Epoch: 6| Step: 2
Training loss: 2.797386165344846
Validation loss: 2.50908184310597

Epoch: 6| Step: 3
Training loss: 2.976591335367219
Validation loss: 2.504635026857655

Epoch: 6| Step: 4
Training loss: 2.3240316363756754
Validation loss: 2.504761786260573

Epoch: 6| Step: 5
Training loss: 2.5205752553271115
Validation loss: 2.5003931213280626

Epoch: 6| Step: 6
Training loss: 2.7114824522345033
Validation loss: 2.49926543570285

Epoch: 6| Step: 7
Training loss: 3.3306850562655668
Validation loss: 2.499134088600484

Epoch: 6| Step: 8
Training loss: 2.558505596942755
Validation loss: 2.5022195341174087

Epoch: 6| Step: 9
Training loss: 2.1791650289477094
Validation loss: 2.5019770751886785

Epoch: 6| Step: 10
Training loss: 2.474498381982021
Validation loss: 2.505681654747212

Epoch: 6| Step: 11
Training loss: 2.3665378799679075
Validation loss: 2.5034000482147825

Epoch: 6| Step: 12
Training loss: 2.7424785752967904
Validation loss: 2.5019776787050603

Epoch: 6| Step: 13
Training loss: 2.687787595042974
Validation loss: 2.5032119462830797

Epoch: 73| Step: 0
Training loss: 3.100358167076855
Validation loss: 2.5040853895516006

Epoch: 6| Step: 1
Training loss: 2.4103971141482656
Validation loss: 2.5026042883982447

Epoch: 6| Step: 2
Training loss: 2.58624357964819
Validation loss: 2.5022658729086618

Epoch: 6| Step: 3
Training loss: 2.8442575190470643
Validation loss: 2.504415959111412

Epoch: 6| Step: 4
Training loss: 1.9264736207630697
Validation loss: 2.5040733055530002

Epoch: 6| Step: 5
Training loss: 1.882401797505569
Validation loss: 2.5043075247917814

Epoch: 6| Step: 6
Training loss: 3.4758670411311865
Validation loss: 2.5035989446807734

Epoch: 6| Step: 7
Training loss: 2.663410543278279
Validation loss: 2.502889346342013

Epoch: 6| Step: 8
Training loss: 2.8219803293030568
Validation loss: 2.500516265968792

Epoch: 6| Step: 9
Training loss: 3.066218395917819
Validation loss: 2.500617411985789

Epoch: 6| Step: 10
Training loss: 2.3837582571749807
Validation loss: 2.497953021941172

Epoch: 6| Step: 11
Training loss: 2.1967972591096876
Validation loss: 2.4974854300414138

Epoch: 6| Step: 12
Training loss: 2.245059417074267
Validation loss: 2.4983210091374914

Epoch: 6| Step: 13
Training loss: 2.7693342776394103
Validation loss: 2.4972909156842955

Epoch: 74| Step: 0
Training loss: 2.8441195405069317
Validation loss: 2.4945818521488756

Epoch: 6| Step: 1
Training loss: 2.6290660429821493
Validation loss: 2.500529455704944

Epoch: 6| Step: 2
Training loss: 2.310508695465942
Validation loss: 2.4972306966961964

Epoch: 6| Step: 3
Training loss: 2.6763694395150344
Validation loss: 2.4956422495944435

Epoch: 6| Step: 4
Training loss: 2.439015626426397
Validation loss: 2.4936573951500223

Epoch: 6| Step: 5
Training loss: 2.2419356892573044
Validation loss: 2.4953012257964047

Epoch: 6| Step: 6
Training loss: 2.0273077636347305
Validation loss: 2.493463522407191

Epoch: 6| Step: 7
Training loss: 3.083136646767639
Validation loss: 2.5002018052348527

Epoch: 6| Step: 8
Training loss: 2.148947915221303
Validation loss: 2.5051016252808225

Epoch: 6| Step: 9
Training loss: 3.153604437348734
Validation loss: 2.5033971117109504

Epoch: 6| Step: 10
Training loss: 2.87335771840923
Validation loss: 2.496890263835036

Epoch: 6| Step: 11
Training loss: 2.930036274812363
Validation loss: 2.4935112031774596

Epoch: 6| Step: 12
Training loss: 2.6064523558880572
Validation loss: 2.4953812849468884

Epoch: 6| Step: 13
Training loss: 2.4659203839532893
Validation loss: 2.495216768769723

Epoch: 75| Step: 0
Training loss: 1.8294617419958898
Validation loss: 2.4965064116088183

Epoch: 6| Step: 1
Training loss: 3.117135795604244
Validation loss: 2.4961175494584213

Epoch: 6| Step: 2
Training loss: 2.68596063929028
Validation loss: 2.4993640726633926

Epoch: 6| Step: 3
Training loss: 2.5803891499109253
Validation loss: 2.4968886405689994

Epoch: 6| Step: 4
Training loss: 2.6927035323689874
Validation loss: 2.499221617003507

Epoch: 6| Step: 5
Training loss: 2.6126861578949283
Validation loss: 2.4972946310902215

Epoch: 6| Step: 6
Training loss: 2.7338840588847795
Validation loss: 2.497106928551322

Epoch: 6| Step: 7
Training loss: 2.3837870621663053
Validation loss: 2.5004263752695493

Epoch: 6| Step: 8
Training loss: 3.0979374761508884
Validation loss: 2.4976934760132234

Epoch: 6| Step: 9
Training loss: 2.586089991196346
Validation loss: 2.497291400994591

Epoch: 6| Step: 10
Training loss: 2.4741142036457986
Validation loss: 2.496296595299484

Epoch: 6| Step: 11
Training loss: 2.57372051042408
Validation loss: 2.4941020455758474

Epoch: 6| Step: 12
Training loss: 2.6890753631018156
Validation loss: 2.4934878569249217

Epoch: 6| Step: 13
Training loss: 2.4976628823381697
Validation loss: 2.4923040986563496

Epoch: 76| Step: 0
Training loss: 2.716836277712332
Validation loss: 2.4882816172968316

Epoch: 6| Step: 1
Training loss: 2.1965904995800787
Validation loss: 2.4920318937602772

Epoch: 6| Step: 2
Training loss: 2.541445696454548
Validation loss: 2.4958409443539162

Epoch: 6| Step: 3
Training loss: 2.7022262578579954
Validation loss: 2.4921799582882476

Epoch: 6| Step: 4
Training loss: 2.764340032140304
Validation loss: 2.489640867028692

Epoch: 6| Step: 5
Training loss: 2.130992853755058
Validation loss: 2.492431644610709

Epoch: 6| Step: 6
Training loss: 2.772288941587253
Validation loss: 2.486458166916427

Epoch: 6| Step: 7
Training loss: 2.9670549874348326
Validation loss: 2.4892520657544495

Epoch: 6| Step: 8
Training loss: 2.494410847459101
Validation loss: 2.4932386200641092

Epoch: 6| Step: 9
Training loss: 2.4978780324035847
Validation loss: 2.5102639580548685

Epoch: 6| Step: 10
Training loss: 2.1549625699715795
Validation loss: 2.5236840142460966

Epoch: 6| Step: 11
Training loss: 2.8941368421194404
Validation loss: 2.525091276942554

Epoch: 6| Step: 12
Training loss: 2.9360885475692795
Validation loss: 2.517456099081009

Epoch: 6| Step: 13
Training loss: 2.9127847360417944
Validation loss: 2.489882261028089

Epoch: 77| Step: 0
Training loss: 2.649758698615531
Validation loss: 2.486987325977835

Epoch: 6| Step: 1
Training loss: 2.0556086226936254
Validation loss: 2.4866873258681927

Epoch: 6| Step: 2
Training loss: 2.3692237480909366
Validation loss: 2.4852217025156333

Epoch: 6| Step: 3
Training loss: 2.377389208020246
Validation loss: 2.488713967921301

Epoch: 6| Step: 4
Training loss: 2.764558575731819
Validation loss: 2.489021338924506

Epoch: 6| Step: 5
Training loss: 2.4306999623830388
Validation loss: 2.492252751595346

Epoch: 6| Step: 6
Training loss: 2.871201701308135
Validation loss: 2.4944085375809983

Epoch: 6| Step: 7
Training loss: 2.6919247774154433
Validation loss: 2.494502938166953

Epoch: 6| Step: 8
Training loss: 2.934381006009229
Validation loss: 2.4968065051127994

Epoch: 6| Step: 9
Training loss: 2.76060236660102
Validation loss: 2.496275225088301

Epoch: 6| Step: 10
Training loss: 2.533225524850367
Validation loss: 2.4960180678060095

Epoch: 6| Step: 11
Training loss: 2.354629696121956
Validation loss: 2.496564109395221

Epoch: 6| Step: 12
Training loss: 2.8350559682552388
Validation loss: 2.4952454655289524

Epoch: 6| Step: 13
Training loss: 2.884767939128219
Validation loss: 2.493030624713642

Epoch: 78| Step: 0
Training loss: 2.4084118136210133
Validation loss: 2.49303789288974

Epoch: 6| Step: 1
Training loss: 2.6739191634713904
Validation loss: 2.491624264265936

Epoch: 6| Step: 2
Training loss: 2.5128200839539847
Validation loss: 2.491530233069222

Epoch: 6| Step: 3
Training loss: 2.9024753463710082
Validation loss: 2.4872506251486293

Epoch: 6| Step: 4
Training loss: 2.5162692933117516
Validation loss: 2.486028045341633

Epoch: 6| Step: 5
Training loss: 2.536881011482103
Validation loss: 2.4855491854396985

Epoch: 6| Step: 6
Training loss: 2.81333978619069
Validation loss: 2.482245870454035

Epoch: 6| Step: 7
Training loss: 2.3081000456918783
Validation loss: 2.482153749229681

Epoch: 6| Step: 8
Training loss: 2.275679279404007
Validation loss: 2.493093423605965

Epoch: 6| Step: 9
Training loss: 2.431955732281502
Validation loss: 2.5188472795277264

Epoch: 6| Step: 10
Training loss: 3.171452837130726
Validation loss: 2.5546831447863614

Epoch: 6| Step: 11
Training loss: 3.1602329013803394
Validation loss: 2.6009527337623006

Epoch: 6| Step: 12
Training loss: 3.100555333264315
Validation loss: 2.623411015552387

Epoch: 6| Step: 13
Training loss: 1.8493586433721247
Validation loss: 2.5961776005055786

Epoch: 79| Step: 0
Training loss: 2.339168088029412
Validation loss: 2.5589885377973793

Epoch: 6| Step: 1
Training loss: 2.4621670028234237
Validation loss: 2.5362700145038892

Epoch: 6| Step: 2
Training loss: 2.3248117924469267
Validation loss: 2.4946662032422413

Epoch: 6| Step: 3
Training loss: 2.1865341233967333
Validation loss: 2.4783159180033345

Epoch: 6| Step: 4
Training loss: 2.9148239490876295
Validation loss: 2.4812607989969035

Epoch: 6| Step: 5
Training loss: 2.95262214375692
Validation loss: 2.4841390533687036

Epoch: 6| Step: 6
Training loss: 2.5374557340923265
Validation loss: 2.4834535837394665

Epoch: 6| Step: 7
Training loss: 2.1812080357403376
Validation loss: 2.4877003101866624

Epoch: 6| Step: 8
Training loss: 2.814715212413104
Validation loss: 2.4887431707564724

Epoch: 6| Step: 9
Training loss: 2.6104367774215094
Validation loss: 2.4889589641371535

Epoch: 6| Step: 10
Training loss: 3.1647351715514347
Validation loss: 2.490731622255639

Epoch: 6| Step: 11
Training loss: 2.6334314722372647
Validation loss: 2.493889477620837

Epoch: 6| Step: 12
Training loss: 2.6562959554848176
Validation loss: 2.498841796252407

Epoch: 6| Step: 13
Training loss: 2.607103192574505
Validation loss: 2.5006717733000348

Epoch: 80| Step: 0
Training loss: 2.224525135243822
Validation loss: 2.5002193036848084

Epoch: 6| Step: 1
Training loss: 2.2438633202119243
Validation loss: 2.4998614113858935

Epoch: 6| Step: 2
Training loss: 2.316678164702865
Validation loss: 2.497065745401613

Epoch: 6| Step: 3
Training loss: 2.7638637300897924
Validation loss: 2.493220721989479

Epoch: 6| Step: 4
Training loss: 2.8753423072373594
Validation loss: 2.488523123187573

Epoch: 6| Step: 5
Training loss: 2.493982702885094
Validation loss: 2.4839239290796136

Epoch: 6| Step: 6
Training loss: 2.3430426992643927
Validation loss: 2.487164400935017

Epoch: 6| Step: 7
Training loss: 2.5521634666681723
Validation loss: 2.4819145094333988

Epoch: 6| Step: 8
Training loss: 2.4973116725600724
Validation loss: 2.482461300228456

Epoch: 6| Step: 9
Training loss: 3.292838133623207
Validation loss: 2.477811336028737

Epoch: 6| Step: 10
Training loss: 2.7409056359977693
Validation loss: 2.479034970629672

Epoch: 6| Step: 11
Training loss: 2.464617009925728
Validation loss: 2.4800775815252463

Epoch: 6| Step: 12
Training loss: 2.7268517458280686
Validation loss: 2.4774786440958247

Epoch: 6| Step: 13
Training loss: 2.831864555977796
Validation loss: 2.475955844708316

Epoch: 81| Step: 0
Training loss: 2.1012135042178883
Validation loss: 2.4769542877574744

Epoch: 6| Step: 1
Training loss: 2.3546562248436236
Validation loss: 2.480282850132742

Epoch: 6| Step: 2
Training loss: 2.6949464411181916
Validation loss: 2.4772962812858137

Epoch: 6| Step: 3
Training loss: 2.544477773871022
Validation loss: 2.472158798792473

Epoch: 6| Step: 4
Training loss: 2.701128095270668
Validation loss: 2.4755966276581316

Epoch: 6| Step: 5
Training loss: 2.532597218612232
Validation loss: 2.476168162883755

Epoch: 6| Step: 6
Training loss: 2.9709430825897343
Validation loss: 2.4750758220476903

Epoch: 6| Step: 7
Training loss: 2.7155439393266265
Validation loss: 2.4714635269413594

Epoch: 6| Step: 8
Training loss: 2.7693940251066453
Validation loss: 2.4729272775226874

Epoch: 6| Step: 9
Training loss: 2.6192620552923747
Validation loss: 2.4751375192269376

Epoch: 6| Step: 10
Training loss: 2.6498222219466654
Validation loss: 2.476826618603458

Epoch: 6| Step: 11
Training loss: 2.644946112273105
Validation loss: 2.474745925552113

Epoch: 6| Step: 12
Training loss: 2.533551900202178
Validation loss: 2.4793964781687103

Epoch: 6| Step: 13
Training loss: 2.5705928011935963
Validation loss: 2.4770417978083863

Epoch: 82| Step: 0
Training loss: 2.5848359588282412
Validation loss: 2.474427844552232

Epoch: 6| Step: 1
Training loss: 2.7228142187629074
Validation loss: 2.4753556866131805

Epoch: 6| Step: 2
Training loss: 2.694307266973228
Validation loss: 2.475942058643293

Epoch: 6| Step: 3
Training loss: 2.492979491858321
Validation loss: 2.4703894052874147

Epoch: 6| Step: 4
Training loss: 2.195401878845295
Validation loss: 2.4755868122969567

Epoch: 6| Step: 5
Training loss: 3.0350836667841437
Validation loss: 2.4722712550021564

Epoch: 6| Step: 6
Training loss: 2.18659616598981
Validation loss: 2.474911746304369

Epoch: 6| Step: 7
Training loss: 2.9731254057034913
Validation loss: 2.4776715704427463

Epoch: 6| Step: 8
Training loss: 2.345109875673098
Validation loss: 2.4748627679200363

Epoch: 6| Step: 9
Training loss: 2.645862959648329
Validation loss: 2.478189986580654

Epoch: 6| Step: 10
Training loss: 2.8848786845228105
Validation loss: 2.477074394712584

Epoch: 6| Step: 11
Training loss: 2.956720010762923
Validation loss: 2.48277338252012

Epoch: 6| Step: 12
Training loss: 2.4452461989314433
Validation loss: 2.4853295146011654

Epoch: 6| Step: 13
Training loss: 2.1569504636718886
Validation loss: 2.48636004079595

Epoch: 83| Step: 0
Training loss: 2.652977397214193
Validation loss: 2.4880582109635188

Epoch: 6| Step: 1
Training loss: 2.7300403351651084
Validation loss: 2.4869624485166293

Epoch: 6| Step: 2
Training loss: 2.9642153188255174
Validation loss: 2.488667919670925

Epoch: 6| Step: 3
Training loss: 2.301592698323187
Validation loss: 2.485884945336101

Epoch: 6| Step: 4
Training loss: 3.179311969148037
Validation loss: 2.4839366310163604

Epoch: 6| Step: 5
Training loss: 2.561860167476007
Validation loss: 2.4817712603018607

Epoch: 6| Step: 6
Training loss: 2.730391821916808
Validation loss: 2.4847526743059154

Epoch: 6| Step: 7
Training loss: 2.581278496462567
Validation loss: 2.4794998639591275

Epoch: 6| Step: 8
Training loss: 2.52864439807436
Validation loss: 2.4759752156851107

Epoch: 6| Step: 9
Training loss: 2.662214785061133
Validation loss: 2.4742303692009155

Epoch: 6| Step: 10
Training loss: 2.7981849884494308
Validation loss: 2.47755667289637

Epoch: 6| Step: 11
Training loss: 2.0851486943416404
Validation loss: 2.4744582999464404

Epoch: 6| Step: 12
Training loss: 1.494624599287326
Validation loss: 2.4741766154045433

Epoch: 6| Step: 13
Training loss: 2.7690300104533905
Validation loss: 2.473369509577202

Epoch: 84| Step: 0
Training loss: 2.6587539539388465
Validation loss: 2.47249153958613

Epoch: 6| Step: 1
Training loss: 2.967072504837454
Validation loss: 2.4738067152176173

Epoch: 6| Step: 2
Training loss: 2.627580917956232
Validation loss: 2.469911302178598

Epoch: 6| Step: 3
Training loss: 2.3848580007446607
Validation loss: 2.4681356124282696

Epoch: 6| Step: 4
Training loss: 2.096332949576411
Validation loss: 2.47313967896036

Epoch: 6| Step: 5
Training loss: 2.7874132082988394
Validation loss: 2.4672835779276237

Epoch: 6| Step: 6
Training loss: 2.5100941008903845
Validation loss: 2.471667983641459

Epoch: 6| Step: 7
Training loss: 1.8312312414881542
Validation loss: 2.469479207696613

Epoch: 6| Step: 8
Training loss: 2.810406223406852
Validation loss: 2.472791654988325

Epoch: 6| Step: 9
Training loss: 2.7121055351360908
Validation loss: 2.473750590921402

Epoch: 6| Step: 10
Training loss: 2.751913272089891
Validation loss: 2.470975733658578

Epoch: 6| Step: 11
Training loss: 2.407647853918127
Validation loss: 2.475470590004645

Epoch: 6| Step: 12
Training loss: 2.4757661230961925
Validation loss: 2.4724750422268187

Epoch: 6| Step: 13
Training loss: 2.9816522610783527
Validation loss: 2.4728190371988057

Epoch: 85| Step: 0
Training loss: 2.376725623998795
Validation loss: 2.468703321828459

Epoch: 6| Step: 1
Training loss: 1.9223869076169096
Validation loss: 2.47257470774372

Epoch: 6| Step: 2
Training loss: 2.183022740041619
Validation loss: 2.47166912509164

Epoch: 6| Step: 3
Training loss: 3.3056483393736116
Validation loss: 2.472287536739368

Epoch: 6| Step: 4
Training loss: 2.524071392361514
Validation loss: 2.473989889378654

Epoch: 6| Step: 5
Training loss: 3.0332067407315058
Validation loss: 2.476685385558427

Epoch: 6| Step: 6
Training loss: 2.7083254496141764
Validation loss: 2.476995340061848

Epoch: 6| Step: 7
Training loss: 2.7873524785029438
Validation loss: 2.468974171693562

Epoch: 6| Step: 8
Training loss: 2.454501695114424
Validation loss: 2.472417963567908

Epoch: 6| Step: 9
Training loss: 2.0951182890571123
Validation loss: 2.474875050739834

Epoch: 6| Step: 10
Training loss: 2.7089314802818585
Validation loss: 2.4740380980365266

Epoch: 6| Step: 11
Training loss: 2.7849213446257246
Validation loss: 2.4672181088701204

Epoch: 6| Step: 12
Training loss: 2.377981623213644
Validation loss: 2.4674422345393774

Epoch: 6| Step: 13
Training loss: 2.628319140508628
Validation loss: 2.467136725107992

Epoch: 86| Step: 0
Training loss: 2.738184514969508
Validation loss: 2.471011289092453

Epoch: 6| Step: 1
Training loss: 2.318618515687534
Validation loss: 2.474368731272453

Epoch: 6| Step: 2
Training loss: 2.5425661298615694
Validation loss: 2.4740921840932355

Epoch: 6| Step: 3
Training loss: 2.6808572334771243
Validation loss: 2.4751736409834373

Epoch: 6| Step: 4
Training loss: 2.3141767248260896
Validation loss: 2.4698006852745644

Epoch: 6| Step: 5
Training loss: 2.9904568201368544
Validation loss: 2.477745102717782

Epoch: 6| Step: 6
Training loss: 2.293463736401072
Validation loss: 2.475319037762509

Epoch: 6| Step: 7
Training loss: 2.8732264480123417
Validation loss: 2.476473151840538

Epoch: 6| Step: 8
Training loss: 2.4805854349400307
Validation loss: 2.4763150577898623

Epoch: 6| Step: 9
Training loss: 2.557483694373214
Validation loss: 2.470309477194397

Epoch: 6| Step: 10
Training loss: 2.6071069420059594
Validation loss: 2.4714283076193513

Epoch: 6| Step: 11
Training loss: 2.150356476085573
Validation loss: 2.471776081457797

Epoch: 6| Step: 12
Training loss: 2.992773889995805
Validation loss: 2.474422970690841

Epoch: 6| Step: 13
Training loss: 2.5118387291523177
Validation loss: 2.4695981335334576

Epoch: 87| Step: 0
Training loss: 2.3587425344214585
Validation loss: 2.466618031804148

Epoch: 6| Step: 1
Training loss: 2.2189025826440982
Validation loss: 2.4670796679011193

Epoch: 6| Step: 2
Training loss: 2.564022821262342
Validation loss: 2.4690526462904367

Epoch: 6| Step: 3
Training loss: 2.7391182005420154
Validation loss: 2.4638197555744172

Epoch: 6| Step: 4
Training loss: 2.6133478495460665
Validation loss: 2.46462773153816

Epoch: 6| Step: 5
Training loss: 2.525098319075139
Validation loss: 2.4629947520344815

Epoch: 6| Step: 6
Training loss: 2.708939401353655
Validation loss: 2.4640973669410196

Epoch: 6| Step: 7
Training loss: 2.759431531247128
Validation loss: 2.4628617609940853

Epoch: 6| Step: 8
Training loss: 2.7721581315519046
Validation loss: 2.4695962509745093

Epoch: 6| Step: 9
Training loss: 2.6153613651545395
Validation loss: 2.4746108362464034

Epoch: 6| Step: 10
Training loss: 2.6967790677190986
Validation loss: 2.477703036429297

Epoch: 6| Step: 11
Training loss: 2.8732015540873834
Validation loss: 2.4733354741427402

Epoch: 6| Step: 12
Training loss: 2.2429431877951695
Validation loss: 2.4828606794710883

Epoch: 6| Step: 13
Training loss: 2.3973947053605955
Validation loss: 2.4767356996511993

Epoch: 88| Step: 0
Training loss: 2.523245978395006
Validation loss: 2.4757393833936745

Epoch: 6| Step: 1
Training loss: 3.060041413868421
Validation loss: 2.478449362411909

Epoch: 6| Step: 2
Training loss: 2.8096621289198676
Validation loss: 2.4723564074624496

Epoch: 6| Step: 3
Training loss: 2.2085468860750823
Validation loss: 2.4782273465829814

Epoch: 6| Step: 4
Training loss: 2.628855008990545
Validation loss: 2.4778874780930575

Epoch: 6| Step: 5
Training loss: 2.7831202872814864
Validation loss: 2.474704370278575

Epoch: 6| Step: 6
Training loss: 2.6046079846598147
Validation loss: 2.4769152561548173

Epoch: 6| Step: 7
Training loss: 2.345174737857805
Validation loss: 2.4818941761493796

Epoch: 6| Step: 8
Training loss: 2.2635748696792226
Validation loss: 2.4760504352658788

Epoch: 6| Step: 9
Training loss: 2.4234011855807975
Validation loss: 2.4738592402372044

Epoch: 6| Step: 10
Training loss: 2.6248619406588856
Validation loss: 2.4803455071858207

Epoch: 6| Step: 11
Training loss: 2.680083576436483
Validation loss: 2.472227448078114

Epoch: 6| Step: 12
Training loss: 2.5665290605723143
Validation loss: 2.4756680707580414

Epoch: 6| Step: 13
Training loss: 2.530618282656847
Validation loss: 2.471144018426752

Epoch: 89| Step: 0
Training loss: 2.8683856674583135
Validation loss: 2.4717081913082595

Epoch: 6| Step: 1
Training loss: 2.2449643687455834
Validation loss: 2.4721514933418436

Epoch: 6| Step: 2
Training loss: 2.3130925424602777
Validation loss: 2.473528522833623

Epoch: 6| Step: 3
Training loss: 2.826689382122944
Validation loss: 2.4736045878964097

Epoch: 6| Step: 4
Training loss: 2.3018533122822245
Validation loss: 2.476185718840092

Epoch: 6| Step: 5
Training loss: 2.6221585107955057
Validation loss: 2.4732435833600857

Epoch: 6| Step: 6
Training loss: 2.3914256033103376
Validation loss: 2.471682002542377

Epoch: 6| Step: 7
Training loss: 2.93714545524953
Validation loss: 2.471264150498918

Epoch: 6| Step: 8
Training loss: 2.380350811211666
Validation loss: 2.468934659852169

Epoch: 6| Step: 9
Training loss: 2.899085124171062
Validation loss: 2.4720875034225305

Epoch: 6| Step: 10
Training loss: 3.082464267495762
Validation loss: 2.4727176540116704

Epoch: 6| Step: 11
Training loss: 2.504529378543902
Validation loss: 2.475316405060275

Epoch: 6| Step: 12
Training loss: 2.4424753518554465
Validation loss: 2.4708833932764103

Epoch: 6| Step: 13
Training loss: 2.117089976637093
Validation loss: 2.4691765654107245

Epoch: 90| Step: 0
Training loss: 2.7094770119721927
Validation loss: 2.473289043174768

Epoch: 6| Step: 1
Training loss: 2.7062797879524454
Validation loss: 2.4715036253895155

Epoch: 6| Step: 2
Training loss: 3.5325042387659513
Validation loss: 2.4683146193432637

Epoch: 6| Step: 3
Training loss: 2.3516147138812356
Validation loss: 2.4682533231316373

Epoch: 6| Step: 4
Training loss: 2.7547566284576304
Validation loss: 2.4615886008561856

Epoch: 6| Step: 5
Training loss: 2.431857400446658
Validation loss: 2.4736784820216204

Epoch: 6| Step: 6
Training loss: 2.9254406067343552
Validation loss: 2.4678347697836593

Epoch: 6| Step: 7
Training loss: 2.274732293108369
Validation loss: 2.4686757812932667

Epoch: 6| Step: 8
Training loss: 2.6831498535483775
Validation loss: 2.4729697624833222

Epoch: 6| Step: 9
Training loss: 2.2358019347183427
Validation loss: 2.470874588435937

Epoch: 6| Step: 10
Training loss: 2.2871547209090837
Validation loss: 2.4751430097679

Epoch: 6| Step: 11
Training loss: 2.4453550097393277
Validation loss: 2.471019908514528

Epoch: 6| Step: 12
Training loss: 1.7755603604231143
Validation loss: 2.474511196534314

Epoch: 6| Step: 13
Training loss: 2.440519174779879
Validation loss: 2.472147651743083

Epoch: 91| Step: 0
Training loss: 2.9657828511072544
Validation loss: 2.474731522586164

Epoch: 6| Step: 1
Training loss: 2.905101261967499
Validation loss: 2.4727027249902718

Epoch: 6| Step: 2
Training loss: 2.225368674948247
Validation loss: 2.472040245297459

Epoch: 6| Step: 3
Training loss: 2.6521010365061346
Validation loss: 2.472300652058792

Epoch: 6| Step: 4
Training loss: 2.062868258805209
Validation loss: 2.4644752222911532

Epoch: 6| Step: 5
Training loss: 2.582287012495858
Validation loss: 2.46838636174788

Epoch: 6| Step: 6
Training loss: 2.210079822048113
Validation loss: 2.4681246162569455

Epoch: 6| Step: 7
Training loss: 3.0660645895946024
Validation loss: 2.461808340205268

Epoch: 6| Step: 8
Training loss: 1.8231726839030362
Validation loss: 2.471889415108967

Epoch: 6| Step: 9
Training loss: 2.7100603514985875
Validation loss: 2.471458398023155

Epoch: 6| Step: 10
Training loss: 2.478758984124797
Validation loss: 2.4752502334173885

Epoch: 6| Step: 11
Training loss: 2.848627288572378
Validation loss: 2.472274405277545

Epoch: 6| Step: 12
Training loss: 2.5807734901684327
Validation loss: 2.4748035847468723

Epoch: 6| Step: 13
Training loss: 2.5050967714420835
Validation loss: 2.4726162345263787

Epoch: 92| Step: 0
Training loss: 2.8349469769614064
Validation loss: 2.4700723961273257

Epoch: 6| Step: 1
Training loss: 2.216926792723854
Validation loss: 2.473007096905776

Epoch: 6| Step: 2
Training loss: 2.2909059360064075
Validation loss: 2.472666470715942

Epoch: 6| Step: 3
Training loss: 2.5477238276180456
Validation loss: 2.4755765956123046

Epoch: 6| Step: 4
Training loss: 2.719403944622034
Validation loss: 2.473129588738176

Epoch: 6| Step: 5
Training loss: 2.9844128671211005
Validation loss: 2.4769432986508275

Epoch: 6| Step: 6
Training loss: 2.4928504755600076
Validation loss: 2.468331925286358

Epoch: 6| Step: 7
Training loss: 2.3175533857235546
Validation loss: 2.4707938638743614

Epoch: 6| Step: 8
Training loss: 2.1059506347898744
Validation loss: 2.4710635197563526

Epoch: 6| Step: 9
Training loss: 2.6133832469701863
Validation loss: 2.4695470464889686

Epoch: 6| Step: 10
Training loss: 2.3147986045552384
Validation loss: 2.45738127963993

Epoch: 6| Step: 11
Training loss: 3.0399394596496427
Validation loss: 2.4614916545879617

Epoch: 6| Step: 12
Training loss: 2.6782745996130823
Validation loss: 2.464789807615551

Epoch: 6| Step: 13
Training loss: 2.6168299188732735
Validation loss: 2.456943342600244

Epoch: 93| Step: 0
Training loss: 1.8089450305539065
Validation loss: 2.4612248651562094

Epoch: 6| Step: 1
Training loss: 2.6363970977130577
Validation loss: 2.466294753758889

Epoch: 6| Step: 2
Training loss: 2.4027278934168694
Validation loss: 2.46647462730934

Epoch: 6| Step: 3
Training loss: 2.510295268773333
Validation loss: 2.4670054311575274

Epoch: 6| Step: 4
Training loss: 2.5570626606280924
Validation loss: 2.4753408377136643

Epoch: 6| Step: 5
Training loss: 2.3990642710285774
Validation loss: 2.4741437233511747

Epoch: 6| Step: 6
Training loss: 2.6633075975774867
Validation loss: 2.4756011862100222

Epoch: 6| Step: 7
Training loss: 2.6609107024567686
Validation loss: 2.472667772407864

Epoch: 6| Step: 8
Training loss: 2.688142566601337
Validation loss: 2.4725999870950113

Epoch: 6| Step: 9
Training loss: 3.18042583469419
Validation loss: 2.478759737571151

Epoch: 6| Step: 10
Training loss: 2.4273189272070206
Validation loss: 2.4734587047741923

Epoch: 6| Step: 11
Training loss: 2.5626452335046213
Validation loss: 2.474065779644892

Epoch: 6| Step: 12
Training loss: 2.824238335210445
Validation loss: 2.4712506116121187

Epoch: 6| Step: 13
Training loss: 2.731758217044543
Validation loss: 2.4657001891635955

Epoch: 94| Step: 0
Training loss: 2.4063673114666666
Validation loss: 2.470104538047706

Epoch: 6| Step: 1
Training loss: 2.6599332517122405
Validation loss: 2.4687302665082727

Epoch: 6| Step: 2
Training loss: 2.169716400389156
Validation loss: 2.465462390870424

Epoch: 6| Step: 3
Training loss: 2.4634828029533264
Validation loss: 2.473715058796855

Epoch: 6| Step: 4
Training loss: 2.458730913329598
Validation loss: 2.467032176937356

Epoch: 6| Step: 5
Training loss: 2.780183737741192
Validation loss: 2.4646318105754537

Epoch: 6| Step: 6
Training loss: 2.828043478112212
Validation loss: 2.4677573916667486

Epoch: 6| Step: 7
Training loss: 2.5605589795790356
Validation loss: 2.465676861609601

Epoch: 6| Step: 8
Training loss: 2.626666130640327
Validation loss: 2.461577672294558

Epoch: 6| Step: 9
Training loss: 2.6798345675579975
Validation loss: 2.4647178960349216

Epoch: 6| Step: 10
Training loss: 2.81813967371217
Validation loss: 2.470457572947918

Epoch: 6| Step: 11
Training loss: 2.465065633275305
Validation loss: 2.4677711187909566

Epoch: 6| Step: 12
Training loss: 2.9431092295828676
Validation loss: 2.4665381748223227

Epoch: 6| Step: 13
Training loss: 2.0658975995546593
Validation loss: 2.4701597317310275

Epoch: 95| Step: 0
Training loss: 2.384133094925459
Validation loss: 2.4655461184700345

Epoch: 6| Step: 1
Training loss: 2.691353807090663
Validation loss: 2.460247770666288

Epoch: 6| Step: 2
Training loss: 3.154798570541536
Validation loss: 2.4635952438969655

Epoch: 6| Step: 3
Training loss: 2.0598080725286754
Validation loss: 2.4627338776004404

Epoch: 6| Step: 4
Training loss: 3.189243512994996
Validation loss: 2.463076901834572

Epoch: 6| Step: 5
Training loss: 2.424543325939025
Validation loss: 2.4646574454774366

Epoch: 6| Step: 6
Training loss: 2.3009080835690074
Validation loss: 2.4660180504203497

Epoch: 6| Step: 7
Training loss: 2.474761404231433
Validation loss: 2.466495047432777

Epoch: 6| Step: 8
Training loss: 2.3328510421767934
Validation loss: 2.4647407248236366

Epoch: 6| Step: 9
Training loss: 3.0504944259875244
Validation loss: 2.466050245184121

Epoch: 6| Step: 10
Training loss: 2.2075287654812668
Validation loss: 2.467210821001651

Epoch: 6| Step: 11
Training loss: 2.3150146833246628
Validation loss: 2.4587963336353553

Epoch: 6| Step: 12
Training loss: 2.6605315762893356
Validation loss: 2.4716788354352457

Epoch: 6| Step: 13
Training loss: 2.2380908931720924
Validation loss: 2.4716191580183478

Epoch: 96| Step: 0
Training loss: 2.3569078493421265
Validation loss: 2.465176776972113

Epoch: 6| Step: 1
Training loss: 2.369041195486707
Validation loss: 2.466723193560537

Epoch: 6| Step: 2
Training loss: 2.567766406864329
Validation loss: 2.4655312588417955

Epoch: 6| Step: 3
Training loss: 3.0050013496789423
Validation loss: 2.4602034266158017

Epoch: 6| Step: 4
Training loss: 2.483749214867217
Validation loss: 2.471568450373938

Epoch: 6| Step: 5
Training loss: 1.7214885741914692
Validation loss: 2.4909467807568646

Epoch: 6| Step: 6
Training loss: 2.641708061623979
Validation loss: 2.509456266145878

Epoch: 6| Step: 7
Training loss: 2.277432631366157
Validation loss: 2.525581889569295

Epoch: 6| Step: 8
Training loss: 2.729872478818443
Validation loss: 2.5184892101338185

Epoch: 6| Step: 9
Training loss: 2.9219722629617104
Validation loss: 2.52883565224952

Epoch: 6| Step: 10
Training loss: 2.8685612103210207
Validation loss: 2.519571387786919

Epoch: 6| Step: 11
Training loss: 3.1107628566859784
Validation loss: 2.51317573557499

Epoch: 6| Step: 12
Training loss: 2.490585338319812
Validation loss: 2.4790304664790392

Epoch: 6| Step: 13
Training loss: 2.5275935384192976
Validation loss: 2.4748799477947823

Epoch: 97| Step: 0
Training loss: 2.631262982624129
Validation loss: 2.4685657448557063

Epoch: 6| Step: 1
Training loss: 2.311950824584167
Validation loss: 2.47490782068366

Epoch: 6| Step: 2
Training loss: 2.527434216255446
Validation loss: 2.479373383652526

Epoch: 6| Step: 3
Training loss: 2.439060298666978
Validation loss: 2.4783048547657565

Epoch: 6| Step: 4
Training loss: 3.098563101219068
Validation loss: 2.4841353502787515

Epoch: 6| Step: 5
Training loss: 2.1068164143073718
Validation loss: 2.489601499632206

Epoch: 6| Step: 6
Training loss: 2.9760975712399014
Validation loss: 2.4976784099080653

Epoch: 6| Step: 7
Training loss: 2.7380388400190974
Validation loss: 2.5023142117064907

Epoch: 6| Step: 8
Training loss: 2.8809278990791114
Validation loss: 2.5047552977310255

Epoch: 6| Step: 9
Training loss: 2.4100369465635314
Validation loss: 2.511674847968163

Epoch: 6| Step: 10
Training loss: 2.0299003225199153
Validation loss: 2.514233221939727

Epoch: 6| Step: 11
Training loss: 3.016113557918981
Validation loss: 2.5163033244178408

Epoch: 6| Step: 12
Training loss: 2.5893834137923735
Validation loss: 2.5104687528364296

Epoch: 6| Step: 13
Training loss: 2.793934048386941
Validation loss: 2.502790133857028

Epoch: 98| Step: 0
Training loss: 2.797071055781184
Validation loss: 2.5041950553220054

Epoch: 6| Step: 1
Training loss: 2.541557799545664
Validation loss: 2.496053282440041

Epoch: 6| Step: 2
Training loss: 3.135707668517182
Validation loss: 2.50120121389913

Epoch: 6| Step: 3
Training loss: 2.452660194632447
Validation loss: 2.4971319913799204

Epoch: 6| Step: 4
Training loss: 2.5873167848451804
Validation loss: 2.4927607627378445

Epoch: 6| Step: 5
Training loss: 2.8401714907166586
Validation loss: 2.4873291301986584

Epoch: 6| Step: 6
Training loss: 2.932497025250083
Validation loss: 2.482401898031342

Epoch: 6| Step: 7
Training loss: 2.365311587496418
Validation loss: 2.4815848014946162

Epoch: 6| Step: 8
Training loss: 2.0861146544477043
Validation loss: 2.47223902874019

Epoch: 6| Step: 9
Training loss: 2.548725602583696
Validation loss: 2.472142106328486

Epoch: 6| Step: 10
Training loss: 2.405457700609214
Validation loss: 2.467567321518584

Epoch: 6| Step: 11
Training loss: 2.6700401028825347
Validation loss: 2.4657514687171855

Epoch: 6| Step: 12
Training loss: 2.1860813445289407
Validation loss: 2.4677243093862047

Epoch: 6| Step: 13
Training loss: 2.759826097952982
Validation loss: 2.462969261162443

Epoch: 99| Step: 0
Training loss: 2.4925856317103112
Validation loss: 2.4647819724908104

Epoch: 6| Step: 1
Training loss: 2.3822655221972027
Validation loss: 2.466746954212684

Epoch: 6| Step: 2
Training loss: 2.316202241556532
Validation loss: 2.4677187862365404

Epoch: 6| Step: 3
Training loss: 2.303710627020168
Validation loss: 2.469866391556783

Epoch: 6| Step: 4
Training loss: 2.1700103219837157
Validation loss: 2.4685059438861434

Epoch: 6| Step: 5
Training loss: 3.3095667105413415
Validation loss: 2.4694829408095416

Epoch: 6| Step: 6
Training loss: 2.543661137558824
Validation loss: 2.4682454748527882

Epoch: 6| Step: 7
Training loss: 3.1153831518157102
Validation loss: 2.47198711901357

Epoch: 6| Step: 8
Training loss: 2.6047780552656694
Validation loss: 2.4704434264632757

Epoch: 6| Step: 9
Training loss: 2.792073822696004
Validation loss: 2.4691080566086527

Epoch: 6| Step: 10
Training loss: 2.046688974095319
Validation loss: 2.4689682811699423

Epoch: 6| Step: 11
Training loss: 2.3734624804851565
Validation loss: 2.466740172382089

Epoch: 6| Step: 12
Training loss: 2.5044859692650228
Validation loss: 2.4762899367147915

Epoch: 6| Step: 13
Training loss: 2.6644526668559934
Validation loss: 2.4734940478025345

Epoch: 100| Step: 0
Training loss: 2.99873500220749
Validation loss: 2.4705516984897695

Epoch: 6| Step: 1
Training loss: 3.050204604743482
Validation loss: 2.4742499464064

Epoch: 6| Step: 2
Training loss: 2.3406243399401396
Validation loss: 2.4728814495078217

Epoch: 6| Step: 3
Training loss: 2.1597412152151976
Validation loss: 2.4760939659294605

Epoch: 6| Step: 4
Training loss: 2.770985467697137
Validation loss: 2.474569310820155

Epoch: 6| Step: 5
Training loss: 2.554782410704705
Validation loss: 2.481404766185368

Epoch: 6| Step: 6
Training loss: 2.2808024346243965
Validation loss: 2.4814599006841536

Epoch: 6| Step: 7
Training loss: 2.3664896222726646
Validation loss: 2.4728246935800855

Epoch: 6| Step: 8
Training loss: 2.62150304472472
Validation loss: 2.478111288217947

Epoch: 6| Step: 9
Training loss: 2.7630767289289544
Validation loss: 2.4713323905047067

Epoch: 6| Step: 10
Training loss: 2.3962463907404676
Validation loss: 2.4699267065831614

Epoch: 6| Step: 11
Training loss: 2.2243362808394753
Validation loss: 2.4659821651707925

Epoch: 6| Step: 12
Training loss: 2.8971559719542332
Validation loss: 2.469249240276872

Epoch: 6| Step: 13
Training loss: 2.4407915241710696
Validation loss: 2.467856507034751

Epoch: 101| Step: 0
Training loss: 2.5628504629757374
Validation loss: 2.4668383379643335

Epoch: 6| Step: 1
Training loss: 2.0324662308743964
Validation loss: 2.4712315573524317

Epoch: 6| Step: 2
Training loss: 2.8269758893264383
Validation loss: 2.4695196440891736

Epoch: 6| Step: 3
Training loss: 2.340085432661235
Validation loss: 2.476302717926001

Epoch: 6| Step: 4
Training loss: 2.5789479184995203
Validation loss: 2.4758517010432337

Epoch: 6| Step: 5
Training loss: 3.0950156041061643
Validation loss: 2.4774526125638796

Epoch: 6| Step: 6
Training loss: 2.4265742825469534
Validation loss: 2.4725099975337526

Epoch: 6| Step: 7
Training loss: 2.244081767001761
Validation loss: 2.472934813675235

Epoch: 6| Step: 8
Training loss: 3.1096967214409865
Validation loss: 2.4728609937417643

Epoch: 6| Step: 9
Training loss: 2.3264688835498077
Validation loss: 2.466150710125579

Epoch: 6| Step: 10
Training loss: 2.718987640869066
Validation loss: 2.4644473523386607

Epoch: 6| Step: 11
Training loss: 2.6877710959744077
Validation loss: 2.4675478040192944

Epoch: 6| Step: 12
Training loss: 2.2284758260852393
Validation loss: 2.459695056349073

Epoch: 6| Step: 13
Training loss: 2.5031741019587104
Validation loss: 2.4580724114351065

Epoch: 102| Step: 0
Training loss: 2.7506698312813618
Validation loss: 2.4537873801339436

Epoch: 6| Step: 1
Training loss: 2.2615467065394372
Validation loss: 2.453809249921768

Epoch: 6| Step: 2
Training loss: 1.93251417443529
Validation loss: 2.453176574812805

Epoch: 6| Step: 3
Training loss: 2.7300901136791427
Validation loss: 2.4551936536913046

Epoch: 6| Step: 4
Training loss: 3.0292628745674124
Validation loss: 2.45708177248749

Epoch: 6| Step: 5
Training loss: 2.5070865329009626
Validation loss: 2.454399781956429

Epoch: 6| Step: 6
Training loss: 2.1553293003988876
Validation loss: 2.4482078920104473

Epoch: 6| Step: 7
Training loss: 2.6270391627406777
Validation loss: 2.45448058430099

Epoch: 6| Step: 8
Training loss: 2.4595772995687106
Validation loss: 2.4538875779513614

Epoch: 6| Step: 9
Training loss: 2.745081838558647
Validation loss: 2.4586241329203933

Epoch: 6| Step: 10
Training loss: 2.526988362673859
Validation loss: 2.4553766951670104

Epoch: 6| Step: 11
Training loss: 2.44044404867098
Validation loss: 2.4545588909074687

Epoch: 6| Step: 12
Training loss: 2.5413252387516656
Validation loss: 2.45278163701761

Epoch: 6| Step: 13
Training loss: 2.8465709737496008
Validation loss: 2.464013033741068

Epoch: 103| Step: 0
Training loss: 2.8457798682385254
Validation loss: 2.4667211154993978

Epoch: 6| Step: 1
Training loss: 2.0958141571239497
Validation loss: 2.4670466006928473

Epoch: 6| Step: 2
Training loss: 1.80014827965236
Validation loss: 2.4665166192769825

Epoch: 6| Step: 3
Training loss: 2.4801744181303347
Validation loss: 2.471110491033098

Epoch: 6| Step: 4
Training loss: 2.0318398426197692
Validation loss: 2.4684582147511045

Epoch: 6| Step: 5
Training loss: 2.5692741360512
Validation loss: 2.472044167434734

Epoch: 6| Step: 6
Training loss: 2.8436255794596494
Validation loss: 2.4728234883862954

Epoch: 6| Step: 7
Training loss: 2.3448000780788214
Validation loss: 2.466728348433325

Epoch: 6| Step: 8
Training loss: 2.1708904202716894
Validation loss: 2.4604511890578866

Epoch: 6| Step: 9
Training loss: 3.2191701077857515
Validation loss: 2.459650128820203

Epoch: 6| Step: 10
Training loss: 2.871642432690823
Validation loss: 2.4611040008572336

Epoch: 6| Step: 11
Training loss: 3.170643835299428
Validation loss: 2.4517465431985377

Epoch: 6| Step: 12
Training loss: 2.342548214834534
Validation loss: 2.4558923909111057

Epoch: 6| Step: 13
Training loss: 2.42135634252284
Validation loss: 2.4558094020989625

Epoch: 104| Step: 0
Training loss: 2.2504874866936926
Validation loss: 2.457605825417787

Epoch: 6| Step: 1
Training loss: 2.5505241977222064
Validation loss: 2.4572692901338518

Epoch: 6| Step: 2
Training loss: 2.2743264259629696
Validation loss: 2.4562308539630395

Epoch: 6| Step: 3
Training loss: 3.095312013197605
Validation loss: 2.4617755088771975

Epoch: 6| Step: 4
Training loss: 2.4173239986886803
Validation loss: 2.4577374679064325

Epoch: 6| Step: 5
Training loss: 2.406537620892361
Validation loss: 2.459957464543044

Epoch: 6| Step: 6
Training loss: 2.6611026195248173
Validation loss: 2.4602592058263104

Epoch: 6| Step: 7
Training loss: 2.2319134577992643
Validation loss: 2.461002409548241

Epoch: 6| Step: 8
Training loss: 2.4540233546469334
Validation loss: 2.4574880574590416

Epoch: 6| Step: 9
Training loss: 2.612069206676253
Validation loss: 2.456334050029033

Epoch: 6| Step: 10
Training loss: 3.0979545613177186
Validation loss: 2.4598237923572945

Epoch: 6| Step: 11
Training loss: 2.615897609491764
Validation loss: 2.4512210268561314

Epoch: 6| Step: 12
Training loss: 2.1899018317041565
Validation loss: 2.4614744619862705

Epoch: 6| Step: 13
Training loss: 2.5077950069970116
Validation loss: 2.4537376481100135

Epoch: 105| Step: 0
Training loss: 2.7951095379432918
Validation loss: 2.4556702613170804

Epoch: 6| Step: 1
Training loss: 2.3804289355953125
Validation loss: 2.4543380653704956

Epoch: 6| Step: 2
Training loss: 2.015827257521221
Validation loss: 2.4611066810543165

Epoch: 6| Step: 3
Training loss: 3.1509076218690915
Validation loss: 2.4672093634253733

Epoch: 6| Step: 4
Training loss: 2.5643981788251544
Validation loss: 2.481792443185705

Epoch: 6| Step: 5
Training loss: 2.3322079078817004
Validation loss: 2.489508118284664

Epoch: 6| Step: 6
Training loss: 3.26031851493802
Validation loss: 2.497572387776546

Epoch: 6| Step: 7
Training loss: 2.221686662935478
Validation loss: 2.47672880879871

Epoch: 6| Step: 8
Training loss: 2.3599872931445436
Validation loss: 2.47956064977854

Epoch: 6| Step: 9
Training loss: 2.200474544843376
Validation loss: 2.476896566328706

Epoch: 6| Step: 10
Training loss: 2.725886045121393
Validation loss: 2.469880694186259

Epoch: 6| Step: 11
Training loss: 2.377473446670651
Validation loss: 2.46329277399644

Epoch: 6| Step: 12
Training loss: 2.434966702713356
Validation loss: 2.464469933715777

Epoch: 6| Step: 13
Training loss: 2.636252128849825
Validation loss: 2.466995694365324

Epoch: 106| Step: 0
Training loss: 2.57390623843989
Validation loss: 2.4647294877979884

Epoch: 6| Step: 1
Training loss: 2.918815928986809
Validation loss: 2.4617952981175297

Epoch: 6| Step: 2
Training loss: 2.6674705922758997
Validation loss: 2.4659920590456665

Epoch: 6| Step: 3
Training loss: 3.1357754895946517
Validation loss: 2.464625039047393

Epoch: 6| Step: 4
Training loss: 2.58088083652345
Validation loss: 2.462924506296988

Epoch: 6| Step: 5
Training loss: 2.805029548040601
Validation loss: 2.4653299520162304

Epoch: 6| Step: 6
Training loss: 2.5224839998461794
Validation loss: 2.46477639438271

Epoch: 6| Step: 7
Training loss: 2.6490266307847286
Validation loss: 2.467838682502986

Epoch: 6| Step: 8
Training loss: 2.550923786007301
Validation loss: 2.467325081028194

Epoch: 6| Step: 9
Training loss: 2.3301094353643403
Validation loss: 2.466623170787061

Epoch: 6| Step: 10
Training loss: 1.7739106609982154
Validation loss: 2.467659383416866

Epoch: 6| Step: 11
Training loss: 2.6895199658655726
Validation loss: 2.4665323912527892

Epoch: 6| Step: 12
Training loss: 2.157553016479757
Validation loss: 2.4704027076189985

Epoch: 6| Step: 13
Training loss: 2.098278557161175
Validation loss: 2.470319852389759

Epoch: 107| Step: 0
Training loss: 3.0899443676332363
Validation loss: 2.46831455494883

Epoch: 6| Step: 1
Training loss: 2.168339768013385
Validation loss: 2.467107564490065

Epoch: 6| Step: 2
Training loss: 1.5020593176556323
Validation loss: 2.472014349390663

Epoch: 6| Step: 3
Training loss: 2.7615207023777035
Validation loss: 2.465299665923997

Epoch: 6| Step: 4
Training loss: 2.1342672340364826
Validation loss: 2.466198757829653

Epoch: 6| Step: 5
Training loss: 3.133894507564972
Validation loss: 2.4616423308999678

Epoch: 6| Step: 6
Training loss: 2.0896550458758485
Validation loss: 2.4690971371889057

Epoch: 6| Step: 7
Training loss: 2.5166868255346797
Validation loss: 2.470998617200929

Epoch: 6| Step: 8
Training loss: 2.8582164314171945
Validation loss: 2.4632768441598523

Epoch: 6| Step: 9
Training loss: 2.61319594536243
Validation loss: 2.462395469267124

Epoch: 6| Step: 10
Training loss: 2.7730697173066994
Validation loss: 2.4629396559261294

Epoch: 6| Step: 11
Training loss: 2.76858741162318
Validation loss: 2.4668010309928867

Epoch: 6| Step: 12
Training loss: 2.3051105692858425
Validation loss: 2.4578221538569944

Epoch: 6| Step: 13
Training loss: 2.63054326005439
Validation loss: 2.462377104944107

Epoch: 108| Step: 0
Training loss: 2.055917350042924
Validation loss: 2.4549016653905023

Epoch: 6| Step: 1
Training loss: 2.481264866705108
Validation loss: 2.4518727712615425

Epoch: 6| Step: 2
Training loss: 2.8821021551363515
Validation loss: 2.4538657493451272

Epoch: 6| Step: 3
Training loss: 2.6636379049014405
Validation loss: 2.4567122906865606

Epoch: 6| Step: 4
Training loss: 2.6207358830886966
Validation loss: 2.4634197815199763

Epoch: 6| Step: 5
Training loss: 2.3281540836847396
Validation loss: 2.4677433907265085

Epoch: 6| Step: 6
Training loss: 2.61016879314205
Validation loss: 2.4666571296902746

Epoch: 6| Step: 7
Training loss: 2.5643611754924076
Validation loss: 2.4614585365255506

Epoch: 6| Step: 8
Training loss: 2.5155597938948215
Validation loss: 2.463411909776437

Epoch: 6| Step: 9
Training loss: 2.493002635662156
Validation loss: 2.460505331532227

Epoch: 6| Step: 10
Training loss: 2.2970184488979704
Validation loss: 2.465553935049811

Epoch: 6| Step: 11
Training loss: 2.6786744452148836
Validation loss: 2.4601044390251925

Epoch: 6| Step: 12
Training loss: 2.7783582419972452
Validation loss: 2.458822700047579

Epoch: 6| Step: 13
Training loss: 2.5794430224667386
Validation loss: 2.457490943718668

Epoch: 109| Step: 0
Training loss: 2.3931176647712467
Validation loss: 2.4547216965438077

Epoch: 6| Step: 1
Training loss: 1.7495429940905673
Validation loss: 2.4578402450363286

Epoch: 6| Step: 2
Training loss: 2.904832391680264
Validation loss: 2.4597743116143316

Epoch: 6| Step: 3
Training loss: 2.5136148702683028
Validation loss: 2.456265425694296

Epoch: 6| Step: 4
Training loss: 2.7823581845182273
Validation loss: 2.458386830108808

Epoch: 6| Step: 5
Training loss: 3.0668232822508155
Validation loss: 2.4508173023734217

Epoch: 6| Step: 6
Training loss: 2.1688256022443206
Validation loss: 2.4568242004520138

Epoch: 6| Step: 7
Training loss: 3.2576181184515876
Validation loss: 2.4609127709493523

Epoch: 6| Step: 8
Training loss: 2.511460927305038
Validation loss: 2.4780536819336136

Epoch: 6| Step: 9
Training loss: 2.3091912182269416
Validation loss: 2.472122785685388

Epoch: 6| Step: 10
Training loss: 2.2361610583406355
Validation loss: 2.459806975801165

Epoch: 6| Step: 11
Training loss: 2.7381427201997455
Validation loss: 2.4549766727952655

Epoch: 6| Step: 12
Training loss: 2.0908912386527905
Validation loss: 2.4600542370572867

Epoch: 6| Step: 13
Training loss: 2.6298719335785012
Validation loss: 2.4575439953156377

Epoch: 110| Step: 0
Training loss: 2.4617817717208705
Validation loss: 2.46262827117983

Epoch: 6| Step: 1
Training loss: 2.731651475672894
Validation loss: 2.4646313913861615

Epoch: 6| Step: 2
Training loss: 2.3800132443356046
Validation loss: 2.4662626427469636

Epoch: 6| Step: 3
Training loss: 2.7351162151075123
Validation loss: 2.4692465125941134

Epoch: 6| Step: 4
Training loss: 2.490330784745262
Validation loss: 2.470633468349646

Epoch: 6| Step: 5
Training loss: 3.065799259439882
Validation loss: 2.470375387093848

Epoch: 6| Step: 6
Training loss: 2.8506381274127857
Validation loss: 2.473518498456608

Epoch: 6| Step: 7
Training loss: 2.8121163000526797
Validation loss: 2.4684333115274253

Epoch: 6| Step: 8
Training loss: 2.735308939973843
Validation loss: 2.4717188178554226

Epoch: 6| Step: 9
Training loss: 2.7558498585169673
Validation loss: 2.4650416952170047

Epoch: 6| Step: 10
Training loss: 2.298694746090826
Validation loss: 2.464737766443804

Epoch: 6| Step: 11
Training loss: 1.9227968191681992
Validation loss: 2.4621842228667172

Epoch: 6| Step: 12
Training loss: 1.9987566182383028
Validation loss: 2.4617394084380386

Epoch: 6| Step: 13
Training loss: 2.1492627587591437
Validation loss: 2.464309094333244

Epoch: 111| Step: 0
Training loss: 2.6663907524259836
Validation loss: 2.461303538393532

Epoch: 6| Step: 1
Training loss: 2.5863883095177074
Validation loss: 2.456895809352322

Epoch: 6| Step: 2
Training loss: 2.148080581148079
Validation loss: 2.457148740675315

Epoch: 6| Step: 3
Training loss: 2.703326002817371
Validation loss: 2.4528883072686956

Epoch: 6| Step: 4
Training loss: 1.9569223339358996
Validation loss: 2.4536851215125433

Epoch: 6| Step: 5
Training loss: 2.397285706745365
Validation loss: 2.445294998642567

Epoch: 6| Step: 6
Training loss: 2.6207696341903812
Validation loss: 2.4481659106258755

Epoch: 6| Step: 7
Training loss: 2.4216368527381036
Validation loss: 2.44801868199095

Epoch: 6| Step: 8
Training loss: 2.663599773952224
Validation loss: 2.4499729583182095

Epoch: 6| Step: 9
Training loss: 2.1991511007579363
Validation loss: 2.4471637190100624

Epoch: 6| Step: 10
Training loss: 3.0568387390868734
Validation loss: 2.4435116572790534

Epoch: 6| Step: 11
Training loss: 2.2487561708527863
Validation loss: 2.4480868314212554

Epoch: 6| Step: 12
Training loss: 2.6558037775759287
Validation loss: 2.457619213111599

Epoch: 6| Step: 13
Training loss: 3.070774499820454
Validation loss: 2.450508868964916

Epoch: 112| Step: 0
Training loss: 2.1840345998824944
Validation loss: 2.448772318752759

Epoch: 6| Step: 1
Training loss: 3.0531415612856065
Validation loss: 2.457944246525273

Epoch: 6| Step: 2
Training loss: 2.3632850836100103
Validation loss: 2.458170438207982

Epoch: 6| Step: 3
Training loss: 2.4499475590290065
Validation loss: 2.461048071259373

Epoch: 6| Step: 4
Training loss: 2.6421498968786166
Validation loss: 2.4594842565779307

Epoch: 6| Step: 5
Training loss: 2.592243538423811
Validation loss: 2.463524531384825

Epoch: 6| Step: 6
Training loss: 1.8770784937562979
Validation loss: 2.4655505827877047

Epoch: 6| Step: 7
Training loss: 2.297072317769457
Validation loss: 2.463761549015676

Epoch: 6| Step: 8
Training loss: 2.5330361551685585
Validation loss: 2.459178202864983

Epoch: 6| Step: 9
Training loss: 2.6358725313095577
Validation loss: 2.4493719658391155

Epoch: 6| Step: 10
Training loss: 2.8189203519581025
Validation loss: 2.4533271301318957

Epoch: 6| Step: 11
Training loss: 2.0517666438410602
Validation loss: 2.4528863551856395

Epoch: 6| Step: 12
Training loss: 2.86661417787389
Validation loss: 2.4563165463143863

Epoch: 6| Step: 13
Training loss: 2.815048843014257
Validation loss: 2.4566535598771018

Epoch: 113| Step: 0
Training loss: 2.1960168977723087
Validation loss: 2.447741487335716

Epoch: 6| Step: 1
Training loss: 2.2198960474093625
Validation loss: 2.453045238323956

Epoch: 6| Step: 2
Training loss: 2.367944476771882
Validation loss: 2.4533774049660018

Epoch: 6| Step: 3
Training loss: 3.1619561613592024
Validation loss: 2.454940852756008

Epoch: 6| Step: 4
Training loss: 2.115366810943797
Validation loss: 2.4523022966870434

Epoch: 6| Step: 5
Training loss: 2.5177285065879804
Validation loss: 2.454689879308009

Epoch: 6| Step: 6
Training loss: 2.872622792513637
Validation loss: 2.454760627782369

Epoch: 6| Step: 7
Training loss: 2.615081577523824
Validation loss: 2.453233299375532

Epoch: 6| Step: 8
Training loss: 2.285116081210789
Validation loss: 2.4537537775255127

Epoch: 6| Step: 9
Training loss: 2.6655154027914207
Validation loss: 2.452549852790659

Epoch: 6| Step: 10
Training loss: 2.257108268334807
Validation loss: 2.4531642481423974

Epoch: 6| Step: 11
Training loss: 2.831995891481857
Validation loss: 2.4581977084835827

Epoch: 6| Step: 12
Training loss: 1.9925959748016597
Validation loss: 2.4632477104403248

Epoch: 6| Step: 13
Training loss: 2.9001078355405463
Validation loss: 2.4482928588963313

Epoch: 114| Step: 0
Training loss: 2.670547244678433
Validation loss: 2.4469061910006213

Epoch: 6| Step: 1
Training loss: 2.2650695119805935
Validation loss: 2.459123893821699

Epoch: 6| Step: 2
Training loss: 2.1301852522690075
Validation loss: 2.4619937708801958

Epoch: 6| Step: 3
Training loss: 2.4853858091022145
Validation loss: 2.4502869243379277

Epoch: 6| Step: 4
Training loss: 2.641009601822558
Validation loss: 2.4623737483570816

Epoch: 6| Step: 5
Training loss: 3.280431881593177
Validation loss: 2.460448992648994

Epoch: 6| Step: 6
Training loss: 2.3189437373986586
Validation loss: 2.4614104608655416

Epoch: 6| Step: 7
Training loss: 2.4055699960550743
Validation loss: 2.4580196137335064

Epoch: 6| Step: 8
Training loss: 2.1388063153377885
Validation loss: 2.463256227975365

Epoch: 6| Step: 9
Training loss: 2.7786874860354573
Validation loss: 2.460319707921218

Epoch: 6| Step: 10
Training loss: 2.297889018533707
Validation loss: 2.4525721548641255

Epoch: 6| Step: 11
Training loss: 2.5315455511411287
Validation loss: 2.4640231290424275

Epoch: 6| Step: 12
Training loss: 2.137776282229113
Validation loss: 2.450985341871163

Epoch: 6| Step: 13
Training loss: 2.948375455024598
Validation loss: 2.4543004226719645

Epoch: 115| Step: 0
Training loss: 2.3150997497951282
Validation loss: 2.453037487177982

Epoch: 6| Step: 1
Training loss: 2.135138504395647
Validation loss: 2.458992923462148

Epoch: 6| Step: 2
Training loss: 2.2651905564071066
Validation loss: 2.458712230729083

Epoch: 6| Step: 3
Training loss: 2.6507070713699106
Validation loss: 2.4653812231529337

Epoch: 6| Step: 4
Training loss: 1.923158914578792
Validation loss: 2.4584059515965744

Epoch: 6| Step: 5
Training loss: 2.419688560685629
Validation loss: 2.4572263315745113

Epoch: 6| Step: 6
Training loss: 2.0715989056914497
Validation loss: 2.4559227930125256

Epoch: 6| Step: 7
Training loss: 3.220227837409816
Validation loss: 2.4574190530157938

Epoch: 6| Step: 8
Training loss: 2.279310682397125
Validation loss: 2.451371037673672

Epoch: 6| Step: 9
Training loss: 2.6638595190496526
Validation loss: 2.4570320909718024

Epoch: 6| Step: 10
Training loss: 2.7448004204298613
Validation loss: 2.458311158958472

Epoch: 6| Step: 11
Training loss: 2.408428840547395
Validation loss: 2.449951062394897

Epoch: 6| Step: 12
Training loss: 2.9047407928223694
Validation loss: 2.447599533857457

Epoch: 6| Step: 13
Training loss: 2.970897660666932
Validation loss: 2.4517239986054986

Epoch: 116| Step: 0
Training loss: 2.4058692928208263
Validation loss: 2.451239507180334

Epoch: 6| Step: 1
Training loss: 2.7656044824561135
Validation loss: 2.45648219569798

Epoch: 6| Step: 2
Training loss: 2.605316752471144
Validation loss: 2.455403640438914

Epoch: 6| Step: 3
Training loss: 2.6488660617548136
Validation loss: 2.4592650285924553

Epoch: 6| Step: 4
Training loss: 2.2982437311437836
Validation loss: 2.470617609969163

Epoch: 6| Step: 5
Training loss: 2.8809446160588266
Validation loss: 2.4821111733249555

Epoch: 6| Step: 6
Training loss: 2.203911715246846
Validation loss: 2.479062027419248

Epoch: 6| Step: 7
Training loss: 3.032320443142093
Validation loss: 2.481079266725279

Epoch: 6| Step: 8
Training loss: 1.6914868765064524
Validation loss: 2.467300874997343

Epoch: 6| Step: 9
Training loss: 2.3890187969913357
Validation loss: 2.486482649898539

Epoch: 6| Step: 10
Training loss: 2.4280097295826972
Validation loss: 2.484846146411352

Epoch: 6| Step: 11
Training loss: 2.539361085208115
Validation loss: 2.483978423769191

Epoch: 6| Step: 12
Training loss: 2.422447087799541
Validation loss: 2.483136794187642

Epoch: 6| Step: 13
Training loss: 2.7603934425000833
Validation loss: 2.4783418121996665

Epoch: 117| Step: 0
Training loss: 2.892843518906256
Validation loss: 2.482409277363008

Epoch: 6| Step: 1
Training loss: 2.5475915942136806
Validation loss: 2.4791495031409156

Epoch: 6| Step: 2
Training loss: 2.425333715856493
Validation loss: 2.4810616893399744

Epoch: 6| Step: 3
Training loss: 2.4429157442857496
Validation loss: 2.475532871256931

Epoch: 6| Step: 4
Training loss: 2.121366873170102
Validation loss: 2.472689660013844

Epoch: 6| Step: 5
Training loss: 2.627296896070663
Validation loss: 2.4753789951925533

Epoch: 6| Step: 6
Training loss: 2.141595773475988
Validation loss: 2.4788557920901986

Epoch: 6| Step: 7
Training loss: 1.8737218951152743
Validation loss: 2.4697686037563242

Epoch: 6| Step: 8
Training loss: 2.827727569137669
Validation loss: 2.4729011338047946

Epoch: 6| Step: 9
Training loss: 2.8600986586406774
Validation loss: 2.47709889825455

Epoch: 6| Step: 10
Training loss: 2.1513262938337094
Validation loss: 2.466391454842994

Epoch: 6| Step: 11
Training loss: 2.6236419797981543
Validation loss: 2.47224124682056

Epoch: 6| Step: 12
Training loss: 2.4305486103745793
Validation loss: 2.4672794710636476

Epoch: 6| Step: 13
Training loss: 2.962354014216058
Validation loss: 2.4748806703102657

Epoch: 118| Step: 0
Training loss: 1.9467053310373612
Validation loss: 2.476113167322713

Epoch: 6| Step: 1
Training loss: 2.589977416304585
Validation loss: 2.4800656128735623

Epoch: 6| Step: 2
Training loss: 3.0124955616518623
Validation loss: 2.47586332092648

Epoch: 6| Step: 3
Training loss: 2.2381521457367057
Validation loss: 2.476429339016883

Epoch: 6| Step: 4
Training loss: 2.701183967259855
Validation loss: 2.4792184156471837

Epoch: 6| Step: 5
Training loss: 2.7547793039310244
Validation loss: 2.478321064797028

Epoch: 6| Step: 6
Training loss: 2.689137957687343
Validation loss: 2.4634284920265426

Epoch: 6| Step: 7
Training loss: 2.810567234116671
Validation loss: 2.4720436128706202

Epoch: 6| Step: 8
Training loss: 2.0803478084053078
Validation loss: 2.473592411217845

Epoch: 6| Step: 9
Training loss: 2.6643796491481693
Validation loss: 2.4609744437537837

Epoch: 6| Step: 10
Training loss: 2.3972231496448377
Validation loss: 2.463427169321973

Epoch: 6| Step: 11
Training loss: 2.537034759716405
Validation loss: 2.4586391151040243

Epoch: 6| Step: 12
Training loss: 2.5374962303998037
Validation loss: 2.455559172802654

Epoch: 6| Step: 13
Training loss: 2.4284280045416597
Validation loss: 2.460367627237905

Epoch: 119| Step: 0
Training loss: 2.3904798875037234
Validation loss: 2.4652223936017554

Epoch: 6| Step: 1
Training loss: 2.598886757720614
Validation loss: 2.4745871511021926

Epoch: 6| Step: 2
Training loss: 2.3463420521842906
Validation loss: 2.4719895061040766

Epoch: 6| Step: 3
Training loss: 2.6791587730595974
Validation loss: 2.4712453053773453

Epoch: 6| Step: 4
Training loss: 2.3983633421327615
Validation loss: 2.4687752219412666

Epoch: 6| Step: 5
Training loss: 2.0310783900614733
Validation loss: 2.46426647618421

Epoch: 6| Step: 6
Training loss: 2.288172421874401
Validation loss: 2.465352211009128

Epoch: 6| Step: 7
Training loss: 2.5565843918388445
Validation loss: 2.4658857219820876

Epoch: 6| Step: 8
Training loss: 2.1667569459544365
Validation loss: 2.465012405034151

Epoch: 6| Step: 9
Training loss: 2.859764197705729
Validation loss: 2.461362448842829

Epoch: 6| Step: 10
Training loss: 2.6838261532828316
Validation loss: 2.464184027404142

Epoch: 6| Step: 11
Training loss: 3.0559004694923164
Validation loss: 2.462292914550148

Epoch: 6| Step: 12
Training loss: 2.9231668971959683
Validation loss: 2.4611074237590156

Epoch: 6| Step: 13
Training loss: 2.189626913123959
Validation loss: 2.4718976135119752

Epoch: 120| Step: 0
Training loss: 2.6271475227211996
Validation loss: 2.461126249633722

Epoch: 6| Step: 1
Training loss: 2.66990017550733
Validation loss: 2.46143128623314

Epoch: 6| Step: 2
Training loss: 2.220751866700688
Validation loss: 2.4648250814900696

Epoch: 6| Step: 3
Training loss: 2.4026719280097812
Validation loss: 2.463813659191339

Epoch: 6| Step: 4
Training loss: 2.8609806409632186
Validation loss: 2.4612745024012153

Epoch: 6| Step: 5
Training loss: 2.249843380033414
Validation loss: 2.464125684316305

Epoch: 6| Step: 6
Training loss: 2.9226954323172114
Validation loss: 2.4592819700374426

Epoch: 6| Step: 7
Training loss: 2.213585228584837
Validation loss: 2.4597903691372363

Epoch: 6| Step: 8
Training loss: 2.5343458293924654
Validation loss: 2.4655070029655484

Epoch: 6| Step: 9
Training loss: 2.6731560742344986
Validation loss: 2.4726105776682252

Epoch: 6| Step: 10
Training loss: 2.76284322548324
Validation loss: 2.4701946393697507

Epoch: 6| Step: 11
Training loss: 2.468892733153837
Validation loss: 2.465858681761091

Epoch: 6| Step: 12
Training loss: 2.4826395943262005
Validation loss: 2.46279904636193

Epoch: 6| Step: 13
Training loss: 2.013639788367359
Validation loss: 2.4621203612390574

Epoch: 121| Step: 0
Training loss: 2.777432683065417
Validation loss: 2.468325952737383

Epoch: 6| Step: 1
Training loss: 3.135240636469835
Validation loss: 2.4655966115267303

Epoch: 6| Step: 2
Training loss: 2.557406690280642
Validation loss: 2.466934534512168

Epoch: 6| Step: 3
Training loss: 2.6699335730072464
Validation loss: 2.4730245065890104

Epoch: 6| Step: 4
Training loss: 2.525520811096011
Validation loss: 2.4755625345602383

Epoch: 6| Step: 5
Training loss: 2.3938262050839207
Validation loss: 2.468537092082017

Epoch: 6| Step: 6
Training loss: 2.34856398515314
Validation loss: 2.477976719130873

Epoch: 6| Step: 7
Training loss: 3.0521399760713326
Validation loss: 2.466483109534987

Epoch: 6| Step: 8
Training loss: 1.8761463475679812
Validation loss: 2.4707374620627394

Epoch: 6| Step: 9
Training loss: 2.4182960892623955
Validation loss: 2.4621658246904743

Epoch: 6| Step: 10
Training loss: 2.3310953942952843
Validation loss: 2.470066411699568

Epoch: 6| Step: 11
Training loss: 2.53540997051106
Validation loss: 2.464251181523085

Epoch: 6| Step: 12
Training loss: 2.478371714448918
Validation loss: 2.4679229413279673

Epoch: 6| Step: 13
Training loss: 2.1982818830406368
Validation loss: 2.4583152403946933

Epoch: 122| Step: 0
Training loss: 2.607208814738456
Validation loss: 2.4662419870732677

Epoch: 6| Step: 1
Training loss: 2.198396627321391
Validation loss: 2.4750161462636044

Epoch: 6| Step: 2
Training loss: 2.3948387956620905
Validation loss: 2.4706987986570734

Epoch: 6| Step: 3
Training loss: 1.7051990024852766
Validation loss: 2.475188795910074

Epoch: 6| Step: 4
Training loss: 2.3270688317466606
Validation loss: 2.4736904173221728

Epoch: 6| Step: 5
Training loss: 2.7119557340041966
Validation loss: 2.4714744680620213

Epoch: 6| Step: 6
Training loss: 2.286099231184906
Validation loss: 2.4731026437870596

Epoch: 6| Step: 7
Training loss: 1.9905768372175296
Validation loss: 2.476195106571885

Epoch: 6| Step: 8
Training loss: 3.5490108562614076
Validation loss: 2.4596587880553757

Epoch: 6| Step: 9
Training loss: 2.443877850478432
Validation loss: 2.469634915551125

Epoch: 6| Step: 10
Training loss: 2.7687722095434006
Validation loss: 2.4642749418229872

Epoch: 6| Step: 11
Training loss: 2.643713685275532
Validation loss: 2.468265526149491

Epoch: 6| Step: 12
Training loss: 2.0805978618214445
Validation loss: 2.4679918050418697

Epoch: 6| Step: 13
Training loss: 3.045193095481719
Validation loss: 2.4649258220568826

Epoch: 123| Step: 0
Training loss: 1.9217346264197077
Validation loss: 2.4609436035080563

Epoch: 6| Step: 1
Training loss: 2.5675720632630408
Validation loss: 2.460761169898111

Epoch: 6| Step: 2
Training loss: 2.4280558808257684
Validation loss: 2.4525551346658334

Epoch: 6| Step: 3
Training loss: 2.2450902139935773
Validation loss: 2.4635329511836423

Epoch: 6| Step: 4
Training loss: 2.4303723317674
Validation loss: 2.4748475949408295

Epoch: 6| Step: 5
Training loss: 2.1246434080831613
Validation loss: 2.4732372129747207

Epoch: 6| Step: 6
Training loss: 2.8127923601571876
Validation loss: 2.4732674661341787

Epoch: 6| Step: 7
Training loss: 2.0001933481217775
Validation loss: 2.4662287911974032

Epoch: 6| Step: 8
Training loss: 1.9498044282702296
Validation loss: 2.4632166405457965

Epoch: 6| Step: 9
Training loss: 2.9851641984567694
Validation loss: 2.4638642683905205

Epoch: 6| Step: 10
Training loss: 3.181066115785022
Validation loss: 2.47130815940832

Epoch: 6| Step: 11
Training loss: 3.370282302034973
Validation loss: 2.465324818396662

Epoch: 6| Step: 12
Training loss: 2.5343473345919882
Validation loss: 2.4599413434972512

Epoch: 6| Step: 13
Training loss: 2.0065712264589632
Validation loss: 2.468625013189412

Epoch: 124| Step: 0
Training loss: 2.528565478529561
Validation loss: 2.4620390187840355

Epoch: 6| Step: 1
Training loss: 2.376035163554973
Validation loss: 2.4611365666686393

Epoch: 6| Step: 2
Training loss: 2.6101438566003288
Validation loss: 2.4597076087698015

Epoch: 6| Step: 3
Training loss: 2.1524225071726844
Validation loss: 2.461239637769924

Epoch: 6| Step: 4
Training loss: 1.7877144211429576
Validation loss: 2.45552970478016

Epoch: 6| Step: 5
Training loss: 2.583263755189495
Validation loss: 2.4567613316968786

Epoch: 6| Step: 6
Training loss: 3.0326515968715912
Validation loss: 2.455699970372502

Epoch: 6| Step: 7
Training loss: 2.233223144697742
Validation loss: 2.4544953327488153

Epoch: 6| Step: 8
Training loss: 2.4704570421546186
Validation loss: 2.4512378861048383

Epoch: 6| Step: 9
Training loss: 2.027090655866089
Validation loss: 2.456321690672299

Epoch: 6| Step: 10
Training loss: 2.7482804210682628
Validation loss: 2.4703799954932806

Epoch: 6| Step: 11
Training loss: 3.004319577992492
Validation loss: 2.4545754843993075

Epoch: 6| Step: 12
Training loss: 3.0735631919956403
Validation loss: 2.456757174894596

Epoch: 6| Step: 13
Training loss: 2.2244740110882573
Validation loss: 2.4588759895578565

Epoch: 125| Step: 0
Training loss: 2.0872655634097077
Validation loss: 2.458982791349685

Epoch: 6| Step: 1
Training loss: 2.794569205180202
Validation loss: 2.4626641568947507

Epoch: 6| Step: 2
Training loss: 2.3183710986566464
Validation loss: 2.4772325926894876

Epoch: 6| Step: 3
Training loss: 2.9000762666341937
Validation loss: 2.4826921245377362

Epoch: 6| Step: 4
Training loss: 2.9974098150002875
Validation loss: 2.474663151510732

Epoch: 6| Step: 5
Training loss: 3.099982255454192
Validation loss: 2.479887213834132

Epoch: 6| Step: 6
Training loss: 2.4823628080170352
Validation loss: 2.466053451746686

Epoch: 6| Step: 7
Training loss: 2.791305366783571
Validation loss: 2.4585472972882867

Epoch: 6| Step: 8
Training loss: 2.2290143350146234
Validation loss: 2.4708835219313814

Epoch: 6| Step: 9
Training loss: 2.4111820554281973
Validation loss: 2.462825184375445

Epoch: 6| Step: 10
Training loss: 2.3326407040334716
Validation loss: 2.4637158730806297

Epoch: 6| Step: 11
Training loss: 2.463030212948071
Validation loss: 2.4761613747612365

Epoch: 6| Step: 12
Training loss: 2.1348252632766274
Validation loss: 2.4699556006432104

Epoch: 6| Step: 13
Training loss: 2.1176713341368743
Validation loss: 2.4717542179144085

Epoch: 126| Step: 0
Training loss: 2.5661255844140074
Validation loss: 2.4732710569652334

Epoch: 6| Step: 1
Training loss: 2.739843861384372
Validation loss: 2.482366473732493

Epoch: 6| Step: 2
Training loss: 2.7146737244419965
Validation loss: 2.475442450564119

Epoch: 6| Step: 3
Training loss: 2.918976840694148
Validation loss: 2.469660329577958

Epoch: 6| Step: 4
Training loss: 2.366671968731739
Validation loss: 2.471256850443279

Epoch: 6| Step: 5
Training loss: 3.2209454798615766
Validation loss: 2.4759135714787823

Epoch: 6| Step: 6
Training loss: 2.430903581195547
Validation loss: 2.4661478420572984

Epoch: 6| Step: 7
Training loss: 2.53754649753908
Validation loss: 2.4740471807219753

Epoch: 6| Step: 8
Training loss: 2.262033918225938
Validation loss: 2.4756192758736857

Epoch: 6| Step: 9
Training loss: 2.5810060067119855
Validation loss: 2.4671039485816353

Epoch: 6| Step: 10
Training loss: 2.3931983610930225
Validation loss: 2.465183901603814

Epoch: 6| Step: 11
Training loss: 1.8666872767037248
Validation loss: 2.460590301726362

Epoch: 6| Step: 12
Training loss: 1.9496205131810291
Validation loss: 2.4668123874493695

Epoch: 6| Step: 13
Training loss: 2.43866975543767
Validation loss: 2.46291716538886

Epoch: 127| Step: 0
Training loss: 2.8805182200012687
Validation loss: 2.4553867126930182

Epoch: 6| Step: 1
Training loss: 3.078541993685447
Validation loss: 2.465053285457426

Epoch: 6| Step: 2
Training loss: 2.266898593069881
Validation loss: 2.463737469258563

Epoch: 6| Step: 3
Training loss: 1.6969925251370714
Validation loss: 2.463464043478399

Epoch: 6| Step: 4
Training loss: 2.2149202848259675
Validation loss: 2.4662061131383517

Epoch: 6| Step: 5
Training loss: 1.9437866354675897
Validation loss: 2.472702548219834

Epoch: 6| Step: 6
Training loss: 2.1755033414863694
Validation loss: 2.465815059039371

Epoch: 6| Step: 7
Training loss: 2.6749551430622196
Validation loss: 2.4756294522545983

Epoch: 6| Step: 8
Training loss: 2.3228063058225272
Validation loss: 2.4700299578560374

Epoch: 6| Step: 9
Training loss: 2.2300967358874852
Validation loss: 2.4687920900772746

Epoch: 6| Step: 10
Training loss: 3.0411495309110257
Validation loss: 2.47657637888071

Epoch: 6| Step: 11
Training loss: 2.3177580981680412
Validation loss: 2.477594587751124

Epoch: 6| Step: 12
Training loss: 3.0723042713021487
Validation loss: 2.480433362172724

Epoch: 6| Step: 13
Training loss: 2.888179590088624
Validation loss: 2.475684137547317

Epoch: 128| Step: 0
Training loss: 2.987649608999895
Validation loss: 2.469862015482623

Epoch: 6| Step: 1
Training loss: 2.119496117935891
Validation loss: 2.4871010049797233

Epoch: 6| Step: 2
Training loss: 2.732861955826619
Validation loss: 2.470338752827456

Epoch: 6| Step: 3
Training loss: 1.9885629031001715
Validation loss: 2.4772636472001643

Epoch: 6| Step: 4
Training loss: 2.7356351727860178
Validation loss: 2.469652003073756

Epoch: 6| Step: 5
Training loss: 2.7401007295456568
Validation loss: 2.462135596511188

Epoch: 6| Step: 6
Training loss: 2.460874913948706
Validation loss: 2.4732393739269547

Epoch: 6| Step: 7
Training loss: 2.086647627177939
Validation loss: 2.4689258722039322

Epoch: 6| Step: 8
Training loss: 2.1962816807345935
Validation loss: 2.466894603413281

Epoch: 6| Step: 9
Training loss: 2.48199684952529
Validation loss: 2.467885103304566

Epoch: 6| Step: 10
Training loss: 2.5971844466981735
Validation loss: 2.4747296118184967

Epoch: 6| Step: 11
Training loss: 2.753852400016504
Validation loss: 2.4694092590586867

Epoch: 6| Step: 12
Training loss: 2.70877912716065
Validation loss: 2.473700890782083

Epoch: 6| Step: 13
Training loss: 2.4332684686803114
Validation loss: 2.4598325317517284

Epoch: 129| Step: 0
Training loss: 2.4750450130184065
Validation loss: 2.468441907755848

Epoch: 6| Step: 1
Training loss: 3.157461406021866
Validation loss: 2.4641917031746767

Epoch: 6| Step: 2
Training loss: 2.770515729632678
Validation loss: 2.473813284921761

Epoch: 6| Step: 3
Training loss: 2.0501187080811007
Validation loss: 2.474285928552768

Epoch: 6| Step: 4
Training loss: 2.0576159129163707
Validation loss: 2.467293080057284

Epoch: 6| Step: 5
Training loss: 2.669466505864765
Validation loss: 2.4779201039628846

Epoch: 6| Step: 6
Training loss: 2.8113402624638124
Validation loss: 2.46546628317893

Epoch: 6| Step: 7
Training loss: 2.7016872044072318
Validation loss: 2.4838034174230015

Epoch: 6| Step: 8
Training loss: 2.13215733340886
Validation loss: 2.4711424586492226

Epoch: 6| Step: 9
Training loss: 2.5902588110380784
Validation loss: 2.4580822482361175

Epoch: 6| Step: 10
Training loss: 2.1949555999505765
Validation loss: 2.4576789394465273

Epoch: 6| Step: 11
Training loss: 2.3013440724487335
Validation loss: 2.4571492662575927

Epoch: 6| Step: 12
Training loss: 2.5178965862087255
Validation loss: 2.455458177411675

Epoch: 6| Step: 13
Training loss: 2.591620524773542
Validation loss: 2.461335165198083

Epoch: 130| Step: 0
Training loss: 2.188505758952947
Validation loss: 2.458086863517026

Epoch: 6| Step: 1
Training loss: 2.920645878572488
Validation loss: 2.459685282540827

Epoch: 6| Step: 2
Training loss: 2.0495564608962025
Validation loss: 2.4590465728674453

Epoch: 6| Step: 3
Training loss: 2.3706955300380894
Validation loss: 2.4619405569254615

Epoch: 6| Step: 4
Training loss: 2.845569573425431
Validation loss: 2.459854048945433

Epoch: 6| Step: 5
Training loss: 2.470931139892328
Validation loss: 2.4634428965559314

Epoch: 6| Step: 6
Training loss: 3.1881010199495434
Validation loss: 2.4676796407602852

Epoch: 6| Step: 7
Training loss: 2.5530468633454397
Validation loss: 2.471652975934062

Epoch: 6| Step: 8
Training loss: 2.1626088462293755
Validation loss: 2.4835274168205768

Epoch: 6| Step: 9
Training loss: 2.492994889206282
Validation loss: 2.4719175949959995

Epoch: 6| Step: 10
Training loss: 2.5199276161394546
Validation loss: 2.476041295735341

Epoch: 6| Step: 11
Training loss: 2.656962759093633
Validation loss: 2.468553028216296

Epoch: 6| Step: 12
Training loss: 2.557356720282759
Validation loss: 2.462777635572568

Epoch: 6| Step: 13
Training loss: 2.0954421308387037
Validation loss: 2.4661913461053793

Epoch: 131| Step: 0
Training loss: 3.3491768693986064
Validation loss: 2.467340767311263

Epoch: 6| Step: 1
Training loss: 1.5856397377444944
Validation loss: 2.4679382454360623

Epoch: 6| Step: 2
Training loss: 2.9761579744289994
Validation loss: 2.4682024416925947

Epoch: 6| Step: 3
Training loss: 2.0259306288024095
Validation loss: 2.4866640433876306

Epoch: 6| Step: 4
Training loss: 2.4774103007163957
Validation loss: 2.4835714324207636

Epoch: 6| Step: 5
Training loss: 2.539792938803687
Validation loss: 2.4826926046990776

Epoch: 6| Step: 6
Training loss: 2.909214435739943
Validation loss: 2.4709143024405953

Epoch: 6| Step: 7
Training loss: 2.379235356058034
Validation loss: 2.4753677583015223

Epoch: 6| Step: 8
Training loss: 1.5835974958137433
Validation loss: 2.473668626921836

Epoch: 6| Step: 9
Training loss: 3.0134927593806897
Validation loss: 2.474158081557156

Epoch: 6| Step: 10
Training loss: 2.2834253260140613
Validation loss: 2.479073584168868

Epoch: 6| Step: 11
Training loss: 2.4420484506922797
Validation loss: 2.4795038704523646

Epoch: 6| Step: 12
Training loss: 2.917958781724523
Validation loss: 2.4757070257203972

Epoch: 6| Step: 13
Training loss: 2.2989822374929534
Validation loss: 2.476479818766996

Epoch: 132| Step: 0
Training loss: 2.79336950454721
Validation loss: 2.472412900917336

Epoch: 6| Step: 1
Training loss: 2.390164199998423
Validation loss: 2.472798725541197

Epoch: 6| Step: 2
Training loss: 2.370287586903519
Validation loss: 2.4599650242756264

Epoch: 6| Step: 3
Training loss: 2.068022299758831
Validation loss: 2.4672914212135564

Epoch: 6| Step: 4
Training loss: 2.2955393118877043
Validation loss: 2.4707194251124656

Epoch: 6| Step: 5
Training loss: 2.0367231139972906
Validation loss: 2.480986621692489

Epoch: 6| Step: 6
Training loss: 3.056581968287031
Validation loss: 2.470476617092662

Epoch: 6| Step: 7
Training loss: 2.6552360843501757
Validation loss: 2.475260395262088

Epoch: 6| Step: 8
Training loss: 2.8150795021823414
Validation loss: 2.4727385207436106

Epoch: 6| Step: 9
Training loss: 2.394805046222264
Validation loss: 2.479532829192978

Epoch: 6| Step: 10
Training loss: 2.6427760645680167
Validation loss: 2.48030971708138

Epoch: 6| Step: 11
Training loss: 2.6841869781335173
Validation loss: 2.4763396891938005

Epoch: 6| Step: 12
Training loss: 2.4200122191380746
Validation loss: 2.478548827774345

Epoch: 6| Step: 13
Training loss: 2.382763471255326
Validation loss: 2.4750634438959787

Epoch: 133| Step: 0
Training loss: 2.32404507541113
Validation loss: 2.477785749011577

Epoch: 6| Step: 1
Training loss: 2.090790664198945
Validation loss: 2.4865120387473394

Epoch: 6| Step: 2
Training loss: 2.168027254981986
Validation loss: 2.478582382806134

Epoch: 6| Step: 3
Training loss: 2.7671586434202187
Validation loss: 2.4822218579523785

Epoch: 6| Step: 4
Training loss: 2.695187419255765
Validation loss: 2.4783728207450526

Epoch: 6| Step: 5
Training loss: 2.708955419450289
Validation loss: 2.4780816233535905

Epoch: 6| Step: 6
Training loss: 2.4639865938540297
Validation loss: 2.467877020383516

Epoch: 6| Step: 7
Training loss: 2.910883337756818
Validation loss: 2.4723641542942265

Epoch: 6| Step: 8
Training loss: 3.1120854358271877
Validation loss: 2.47523869092391

Epoch: 6| Step: 9
Training loss: 1.8244675787200992
Validation loss: 2.477325033251296

Epoch: 6| Step: 10
Training loss: 2.801499448918825
Validation loss: 2.4737038625379384

Epoch: 6| Step: 11
Training loss: 2.1242547691913916
Validation loss: 2.477681409620626

Epoch: 6| Step: 12
Training loss: 2.1199841486140305
Validation loss: 2.4686722884039036

Epoch: 6| Step: 13
Training loss: 2.3599314254560926
Validation loss: 2.4746791284811915

Epoch: 134| Step: 0
Training loss: 2.8192347953368113
Validation loss: 2.472608247424597

Epoch: 6| Step: 1
Training loss: 2.3568678918456665
Validation loss: 2.4642606550328625

Epoch: 6| Step: 2
Training loss: 2.4316836678858627
Validation loss: 2.473634491187107

Epoch: 6| Step: 3
Training loss: 2.57025758632449
Validation loss: 2.4685272888937613

Epoch: 6| Step: 4
Training loss: 2.2895715557578775
Validation loss: 2.4703280881727285

Epoch: 6| Step: 5
Training loss: 2.6857251361291494
Validation loss: 2.464610447953456

Epoch: 6| Step: 6
Training loss: 1.744681928988907
Validation loss: 2.473593664229752

Epoch: 6| Step: 7
Training loss: 2.316535727370524
Validation loss: 2.462953450235827

Epoch: 6| Step: 8
Training loss: 1.9689570726759198
Validation loss: 2.4703072010798186

Epoch: 6| Step: 9
Training loss: 2.553388446116786
Validation loss: 2.477668763824127

Epoch: 6| Step: 10
Training loss: 3.4805264280872468
Validation loss: 2.4756147815596505

Epoch: 6| Step: 11
Training loss: 2.5282749541786393
Validation loss: 2.464700669510239

Epoch: 6| Step: 12
Training loss: 2.4454939412330723
Validation loss: 2.4821191138386167

Epoch: 6| Step: 13
Training loss: 2.3032619399375407
Validation loss: 2.471538899909872

Epoch: 135| Step: 0
Training loss: 2.3895787960453965
Validation loss: 2.4817734698628167

Epoch: 6| Step: 1
Training loss: 2.9350023405465406
Validation loss: 2.4804435428329796

Epoch: 6| Step: 2
Training loss: 2.5757113168679826
Validation loss: 2.4766004140300315

Epoch: 6| Step: 3
Training loss: 2.6878014328933553
Validation loss: 2.4750655952264013

Epoch: 6| Step: 4
Training loss: 2.914143824529171
Validation loss: 2.475061645767623

Epoch: 6| Step: 5
Training loss: 2.000237570004198
Validation loss: 2.4796037101730977

Epoch: 6| Step: 6
Training loss: 2.4075165424986773
Validation loss: 2.499753383552415

Epoch: 6| Step: 7
Training loss: 2.463349047839981
Validation loss: 2.4879973452708235

Epoch: 6| Step: 8
Training loss: 1.5413339960455432
Validation loss: 2.486186193034513

Epoch: 6| Step: 9
Training loss: 2.9773512068179384
Validation loss: 2.482362439844441

Epoch: 6| Step: 10
Training loss: 2.8286644858373537
Validation loss: 2.4860959120652257

Epoch: 6| Step: 11
Training loss: 2.1438595357331267
Validation loss: 2.46694345811432

Epoch: 6| Step: 12
Training loss: 2.336272761038605
Validation loss: 2.4783558334333136

Epoch: 6| Step: 13
Training loss: 2.4358280022695826
Validation loss: 2.46818276822093

Epoch: 136| Step: 0
Training loss: 2.619358722160148
Validation loss: 2.479012770377388

Epoch: 6| Step: 1
Training loss: 2.497503273687804
Validation loss: 2.480423477841042

Epoch: 6| Step: 2
Training loss: 2.6203954366207203
Validation loss: 2.47359841924352

Epoch: 6| Step: 3
Training loss: 2.942100331559397
Validation loss: 2.4852570541389682

Epoch: 6| Step: 4
Training loss: 2.885259153471388
Validation loss: 2.485575371978114

Epoch: 6| Step: 5
Training loss: 2.3570951853303503
Validation loss: 2.489902098185179

Epoch: 6| Step: 6
Training loss: 2.903665521438064
Validation loss: 2.483205123979773

Epoch: 6| Step: 7
Training loss: 2.604255450960489
Validation loss: 2.480149888897835

Epoch: 6| Step: 8
Training loss: 2.333005257657402
Validation loss: 2.4800547817547822

Epoch: 6| Step: 9
Training loss: 2.660836153906014
Validation loss: 2.4847511150750243

Epoch: 6| Step: 10
Training loss: 2.6068412677244246
Validation loss: 2.4795090788838947

Epoch: 6| Step: 11
Training loss: 1.9073389413816284
Validation loss: 2.472487634229964

Epoch: 6| Step: 12
Training loss: 2.389319867678714
Validation loss: 2.476591573358035

Epoch: 6| Step: 13
Training loss: 2.3634618264560032
Validation loss: 2.471476019591008

Epoch: 137| Step: 0
Training loss: 2.812782697245311
Validation loss: 2.470785919129074

Epoch: 6| Step: 1
Training loss: 2.333079960099895
Validation loss: 2.4730305159943633

Epoch: 6| Step: 2
Training loss: 1.8604729239859983
Validation loss: 2.4695321143910207

Epoch: 6| Step: 3
Training loss: 2.5092530673415814
Validation loss: 2.4875601258032103

Epoch: 6| Step: 4
Training loss: 3.1835344367557523
Validation loss: 2.4888679615155156

Epoch: 6| Step: 5
Training loss: 2.0190138606010897
Validation loss: 2.514427026179906

Epoch: 6| Step: 6
Training loss: 2.2315953177151857
Validation loss: 2.5362477120454487

Epoch: 6| Step: 7
Training loss: 2.367862617734577
Validation loss: 2.521984835678682

Epoch: 6| Step: 8
Training loss: 2.586868533421075
Validation loss: 2.484515044225842

Epoch: 6| Step: 9
Training loss: 2.555104819460003
Validation loss: 2.4783834828491513

Epoch: 6| Step: 10
Training loss: 3.0962880830420927
Validation loss: 2.466603678036538

Epoch: 6| Step: 11
Training loss: 2.2130250814182673
Validation loss: 2.456540299749469

Epoch: 6| Step: 12
Training loss: 2.27929666579637
Validation loss: 2.4587382990589868

Epoch: 6| Step: 13
Training loss: 2.811769856939496
Validation loss: 2.4746096640378186

Epoch: 138| Step: 0
Training loss: 2.3631115560847875
Validation loss: 2.4739500962505074

Epoch: 6| Step: 1
Training loss: 2.007282825470956
Validation loss: 2.471499879250899

Epoch: 6| Step: 2
Training loss: 1.9871292824157616
Validation loss: 2.47738394767287

Epoch: 6| Step: 3
Training loss: 2.1657693790016608
Validation loss: 2.4767504519473116

Epoch: 6| Step: 4
Training loss: 2.87212692666702
Validation loss: 2.477223898639992

Epoch: 6| Step: 5
Training loss: 2.976052708661499
Validation loss: 2.4843924819683085

Epoch: 6| Step: 6
Training loss: 3.226561022150842
Validation loss: 2.479114777692495

Epoch: 6| Step: 7
Training loss: 2.190402802477953
Validation loss: 2.482076273185504

Epoch: 6| Step: 8
Training loss: 2.5015429503745
Validation loss: 2.4710414730190196

Epoch: 6| Step: 9
Training loss: 2.5710958197419314
Validation loss: 2.4691669900640543

Epoch: 6| Step: 10
Training loss: 2.313467699632137
Validation loss: 2.4669867628984847

Epoch: 6| Step: 11
Training loss: 2.3760271360986285
Validation loss: 2.4734281808432996

Epoch: 6| Step: 12
Training loss: 2.602178368972145
Validation loss: 2.478529300556777

Epoch: 6| Step: 13
Training loss: 2.512780708080563
Validation loss: 2.465241865036793

Epoch: 139| Step: 0
Training loss: 2.6679142378101397
Validation loss: 2.472126209404366

Epoch: 6| Step: 1
Training loss: 2.5730077647608316
Validation loss: 2.486835884373985

Epoch: 6| Step: 2
Training loss: 2.192756440880062
Validation loss: 2.471822492618112

Epoch: 6| Step: 3
Training loss: 2.5337159189981846
Validation loss: 2.479314179933806

Epoch: 6| Step: 4
Training loss: 2.6388170723317006
Validation loss: 2.476224208314266

Epoch: 6| Step: 5
Training loss: 2.883051006923136
Validation loss: 2.4759550101640717

Epoch: 6| Step: 6
Training loss: 2.4505415026083233
Validation loss: 2.4750230820218495

Epoch: 6| Step: 7
Training loss: 2.1582649878011644
Validation loss: 2.470972895312656

Epoch: 6| Step: 8
Training loss: 2.473286095019863
Validation loss: 2.472108463879951

Epoch: 6| Step: 9
Training loss: 2.105444743456599
Validation loss: 2.482418113323476

Epoch: 6| Step: 10
Training loss: 3.0503548337556414
Validation loss: 2.474302253241531

Epoch: 6| Step: 11
Training loss: 2.0195232221491795
Validation loss: 2.4783995320351497

Epoch: 6| Step: 12
Training loss: 2.4783037965404437
Validation loss: 2.4876269284169905

Epoch: 6| Step: 13
Training loss: 2.4091984903122152
Validation loss: 2.4755230636688292

Epoch: 140| Step: 0
Training loss: 2.4121973430948165
Validation loss: 2.486167133370211

Epoch: 6| Step: 1
Training loss: 2.1090220968536793
Validation loss: 2.4793884968820987

Epoch: 6| Step: 2
Training loss: 2.5178428966424518
Validation loss: 2.4779936529164983

Epoch: 6| Step: 3
Training loss: 1.8390641231092164
Validation loss: 2.4843927378790998

Epoch: 6| Step: 4
Training loss: 1.7468701712280574
Validation loss: 2.4733818319437777

Epoch: 6| Step: 5
Training loss: 2.1377787358093934
Validation loss: 2.477385551639709

Epoch: 6| Step: 6
Training loss: 2.500532856420815
Validation loss: 2.4809540521655995

Epoch: 6| Step: 7
Training loss: 2.6997901764331864
Validation loss: 2.482527311673602

Epoch: 6| Step: 8
Training loss: 2.9447624846502336
Validation loss: 2.4799705265329317

Epoch: 6| Step: 9
Training loss: 2.616238458993447
Validation loss: 2.4798903544347506

Epoch: 6| Step: 10
Training loss: 2.430624042333256
Validation loss: 2.489742614460372

Epoch: 6| Step: 11
Training loss: 2.511021542522429
Validation loss: 2.473084503580082

Epoch: 6| Step: 12
Training loss: 2.65298135141662
Validation loss: 2.4771183565410686

Epoch: 6| Step: 13
Training loss: 3.0834799723079156
Validation loss: 2.4802206080513605

Epoch: 141| Step: 0
Training loss: 2.844854454782614
Validation loss: 2.4758532418043697

Epoch: 6| Step: 1
Training loss: 2.207943348067968
Validation loss: 2.4886318021737504

Epoch: 6| Step: 2
Training loss: 2.5711744538643404
Validation loss: 2.4853523219419067

Epoch: 6| Step: 3
Training loss: 2.270523143429424
Validation loss: 2.479507219875743

Epoch: 6| Step: 4
Training loss: 2.9389479294895606
Validation loss: 2.4767747982579698

Epoch: 6| Step: 5
Training loss: 1.9706760854539
Validation loss: 2.481917327261191

Epoch: 6| Step: 6
Training loss: 2.4150826974298014
Validation loss: 2.4927925164726172

Epoch: 6| Step: 7
Training loss: 3.0643715881042346
Validation loss: 2.483930695988509

Epoch: 6| Step: 8
Training loss: 2.2140647958166135
Validation loss: 2.4775477474242362

Epoch: 6| Step: 9
Training loss: 2.775071127727456
Validation loss: 2.4919038808510967

Epoch: 6| Step: 10
Training loss: 2.6156748485486645
Validation loss: 2.500583183296791

Epoch: 6| Step: 11
Training loss: 1.9765418733870015
Validation loss: 2.499224001930715

Epoch: 6| Step: 12
Training loss: 1.9108177864960587
Validation loss: 2.4767517274271875

Epoch: 6| Step: 13
Training loss: 2.6337535772312894
Validation loss: 2.498194487592046

Epoch: 142| Step: 0
Training loss: 2.6972504214135236
Validation loss: 2.4705552852260557

Epoch: 6| Step: 1
Training loss: 2.149021804422037
Validation loss: 2.4739888293076553

Epoch: 6| Step: 2
Training loss: 2.5293267561277877
Validation loss: 2.4733750843712494

Epoch: 6| Step: 3
Training loss: 2.285576988677716
Validation loss: 2.486892432386375

Epoch: 6| Step: 4
Training loss: 2.3039086011665693
Validation loss: 2.4771028845688954

Epoch: 6| Step: 5
Training loss: 2.3085632838580348
Validation loss: 2.4842553419810893

Epoch: 6| Step: 6
Training loss: 2.838663099765403
Validation loss: 2.4801106672564828

Epoch: 6| Step: 7
Training loss: 2.432537602823017
Validation loss: 2.4859026644488007

Epoch: 6| Step: 8
Training loss: 2.431707395073496
Validation loss: 2.4811179365346168

Epoch: 6| Step: 9
Training loss: 2.944287675806784
Validation loss: 2.4881567172792027

Epoch: 6| Step: 10
Training loss: 2.007389484180569
Validation loss: 2.487263837303397

Epoch: 6| Step: 11
Training loss: 2.748058500719456
Validation loss: 2.4773680843849126

Epoch: 6| Step: 12
Training loss: 3.040196694987656
Validation loss: 2.483139178560995

Epoch: 6| Step: 13
Training loss: 1.9068118658776505
Validation loss: 2.482158383787756

Epoch: 143| Step: 0
Training loss: 1.9010408236009653
Validation loss: 2.4820364896298535

Epoch: 6| Step: 1
Training loss: 2.6665040403686335
Validation loss: 2.4840767239507184

Epoch: 6| Step: 2
Training loss: 2.3596300404114845
Validation loss: 2.5019075745390045

Epoch: 6| Step: 3
Training loss: 2.472401682689608
Validation loss: 2.5007388135858206

Epoch: 6| Step: 4
Training loss: 2.6153165136291214
Validation loss: 2.4914347788438422

Epoch: 6| Step: 5
Training loss: 2.771605840471643
Validation loss: 2.5022042173542833

Epoch: 6| Step: 6
Training loss: 2.607316627235638
Validation loss: 2.5091050759362856

Epoch: 6| Step: 7
Training loss: 2.4979926633582026
Validation loss: 2.502563322741556

Epoch: 6| Step: 8
Training loss: 2.287921191352127
Validation loss: 2.514701018046051

Epoch: 6| Step: 9
Training loss: 2.1161548306042834
Validation loss: 2.5101845239542286

Epoch: 6| Step: 10
Training loss: 2.7813155777304424
Validation loss: 2.4944917396341864

Epoch: 6| Step: 11
Training loss: 2.6200585858150247
Validation loss: 2.489545308438541

Epoch: 6| Step: 12
Training loss: 2.5429029324359798
Validation loss: 2.504015447543537

Epoch: 6| Step: 13
Training loss: 2.449873695231881
Validation loss: 2.4901082484399284

Epoch: 144| Step: 0
Training loss: 2.355893022527498
Validation loss: 2.4778049372945996

Epoch: 6| Step: 1
Training loss: 2.784009357595612
Validation loss: 2.490855045136766

Epoch: 6| Step: 2
Training loss: 2.9918529831470853
Validation loss: 2.477483247301911

Epoch: 6| Step: 3
Training loss: 2.2822738858018843
Validation loss: 2.4804050307212133

Epoch: 6| Step: 4
Training loss: 2.0074742845002245
Validation loss: 2.472518692087373

Epoch: 6| Step: 5
Training loss: 2.3922354067360208
Validation loss: 2.470825345078454

Epoch: 6| Step: 6
Training loss: 2.4124752616602225
Validation loss: 2.4854223894520757

Epoch: 6| Step: 7
Training loss: 2.044509335999176
Validation loss: 2.481655479757775

Epoch: 6| Step: 8
Training loss: 3.041590249830468
Validation loss: 2.4773001870814646

Epoch: 6| Step: 9
Training loss: 3.0103843889628985
Validation loss: 2.4774036603532714

Epoch: 6| Step: 10
Training loss: 1.9639091316497612
Validation loss: 2.471068713816825

Epoch: 6| Step: 11
Training loss: 2.2779533918132384
Validation loss: 2.4727569286385767

Epoch: 6| Step: 12
Training loss: 2.178472912010392
Validation loss: 2.4758617560973146

Epoch: 6| Step: 13
Training loss: 2.7339719638796196
Validation loss: 2.476006719278145

Epoch: 145| Step: 0
Training loss: 3.0569644646742704
Validation loss: 2.475285005054357

Epoch: 6| Step: 1
Training loss: 1.9817090486443776
Validation loss: 2.4642732648225683

Epoch: 6| Step: 2
Training loss: 2.6267912066734636
Validation loss: 2.4721645611623924

Epoch: 6| Step: 3
Training loss: 2.56922244811237
Validation loss: 2.472644703432818

Epoch: 6| Step: 4
Training loss: 2.746241254901115
Validation loss: 2.4713993423454452

Epoch: 6| Step: 5
Training loss: 2.4705049096008325
Validation loss: 2.464717001258176

Epoch: 6| Step: 6
Training loss: 2.4555491721625042
Validation loss: 2.4708616183261083

Epoch: 6| Step: 7
Training loss: 2.4604228132420403
Validation loss: 2.4696116734343834

Epoch: 6| Step: 8
Training loss: 2.072553810553043
Validation loss: 2.485682912945926

Epoch: 6| Step: 9
Training loss: 2.088355559494382
Validation loss: 2.476604505437424

Epoch: 6| Step: 10
Training loss: 2.632798101459376
Validation loss: 2.477570169303943

Epoch: 6| Step: 11
Training loss: 2.0656640030357067
Validation loss: 2.469164922106158

Epoch: 6| Step: 12
Training loss: 2.9597473168130963
Validation loss: 2.487045468283769

Epoch: 6| Step: 13
Training loss: 2.108003523691551
Validation loss: 2.476489510247817

Epoch: 146| Step: 0
Training loss: 2.259487599881835
Validation loss: 2.4899172273025365

Epoch: 6| Step: 1
Training loss: 3.1185084247484487
Validation loss: 2.494482213673429

Epoch: 6| Step: 2
Training loss: 2.7729159872243856
Validation loss: 2.5260133774474385

Epoch: 6| Step: 3
Training loss: 2.8158761000360784
Validation loss: 2.520173283275658

Epoch: 6| Step: 4
Training loss: 2.6466041903804864
Validation loss: 2.510548937677842

Epoch: 6| Step: 5
Training loss: 1.9411241414671732
Validation loss: 2.5125021654565693

Epoch: 6| Step: 6
Training loss: 2.4453456498601662
Validation loss: 2.4898777286224707

Epoch: 6| Step: 7
Training loss: 2.5267688032390248
Validation loss: 2.4738548712261115

Epoch: 6| Step: 8
Training loss: 2.266668460882636
Validation loss: 2.460830847634174

Epoch: 6| Step: 9
Training loss: 1.9805099212159145
Validation loss: 2.4604540637665635

Epoch: 6| Step: 10
Training loss: 1.9780039601107955
Validation loss: 2.468044228409412

Epoch: 6| Step: 11
Training loss: 2.7999227104418867
Validation loss: 2.4660852593324494

Epoch: 6| Step: 12
Training loss: 2.2273914600990588
Validation loss: 2.4774096992349617

Epoch: 6| Step: 13
Training loss: 2.549116587484165
Validation loss: 2.479131928101687

Epoch: 147| Step: 0
Training loss: 1.7771527198163868
Validation loss: 2.467495788807675

Epoch: 6| Step: 1
Training loss: 2.797096286309732
Validation loss: 2.4707903096493644

Epoch: 6| Step: 2
Training loss: 1.8359985341417366
Validation loss: 2.4728820922628505

Epoch: 6| Step: 3
Training loss: 2.4240252393913013
Validation loss: 2.4732199172555394

Epoch: 6| Step: 4
Training loss: 2.8103152796671003
Validation loss: 2.475666088484606

Epoch: 6| Step: 5
Training loss: 3.046718182562995
Validation loss: 2.4750520771340456

Epoch: 6| Step: 6
Training loss: 2.803187232033728
Validation loss: 2.4896098392198165

Epoch: 6| Step: 7
Training loss: 1.9116951140083347
Validation loss: 2.4852605237194063

Epoch: 6| Step: 8
Training loss: 2.7105826989431736
Validation loss: 2.4721239831838795

Epoch: 6| Step: 9
Training loss: 2.0357368797826685
Validation loss: 2.4770255954473916

Epoch: 6| Step: 10
Training loss: 3.177258362665051
Validation loss: 2.484325824306838

Epoch: 6| Step: 11
Training loss: 2.2518504269053503
Validation loss: 2.487818285133495

Epoch: 6| Step: 12
Training loss: 2.5216481386837355
Validation loss: 2.483144931452378

Epoch: 6| Step: 13
Training loss: 2.3805054549419333
Validation loss: 2.4982262995082505

Epoch: 148| Step: 0
Training loss: 2.585083973330445
Validation loss: 2.496992177366144

Epoch: 6| Step: 1
Training loss: 2.4824373378317097
Validation loss: 2.4964976573454067

Epoch: 6| Step: 2
Training loss: 2.5087023906058183
Validation loss: 2.50491599890703

Epoch: 6| Step: 3
Training loss: 3.077363050622548
Validation loss: 2.4938653143255745

Epoch: 6| Step: 4
Training loss: 2.778288389535407
Validation loss: 2.4959274021649853

Epoch: 6| Step: 5
Training loss: 2.354934859620155
Validation loss: 2.478978804408071

Epoch: 6| Step: 6
Training loss: 2.3855401108519216
Validation loss: 2.476374285071213

Epoch: 6| Step: 7
Training loss: 2.3561261779468117
Validation loss: 2.4771101032304577

Epoch: 6| Step: 8
Training loss: 2.2421598532430993
Validation loss: 2.486227820111933

Epoch: 6| Step: 9
Training loss: 2.1866012906947105
Validation loss: 2.494156580759191

Epoch: 6| Step: 10
Training loss: 2.590824729665477
Validation loss: 2.4870200002357175

Epoch: 6| Step: 11
Training loss: 2.407115631952146
Validation loss: 2.476776081747113

Epoch: 6| Step: 12
Training loss: 1.7536535951922039
Validation loss: 2.4924595603608455

Epoch: 6| Step: 13
Training loss: 2.7280571893083927
Validation loss: 2.4804695990453354

Epoch: 149| Step: 0
Training loss: 2.518215196499081
Validation loss: 2.4845408418271644

Epoch: 6| Step: 1
Training loss: 2.1083391825276934
Validation loss: 2.4861930416916196

Epoch: 6| Step: 2
Training loss: 2.833688227548554
Validation loss: 2.4934708211938275

Epoch: 6| Step: 3
Training loss: 2.2059881125135896
Validation loss: 2.5005275328682224

Epoch: 6| Step: 4
Training loss: 2.5407600244956616
Validation loss: 2.499558743636079

Epoch: 6| Step: 5
Training loss: 2.533956140790465
Validation loss: 2.487326214660618

Epoch: 6| Step: 6
Training loss: 2.800684375231441
Validation loss: 2.4943758963851956

Epoch: 6| Step: 7
Training loss: 2.476764947456652
Validation loss: 2.480363033575004

Epoch: 6| Step: 8
Training loss: 2.6659069667932758
Validation loss: 2.484187888850645

Epoch: 6| Step: 9
Training loss: 1.8708984018181043
Validation loss: 2.4726113008468356

Epoch: 6| Step: 10
Training loss: 2.6459672148065354
Validation loss: 2.48909012978116

Epoch: 6| Step: 11
Training loss: 2.2532589940029477
Validation loss: 2.491150292396476

Epoch: 6| Step: 12
Training loss: 2.745559748988558
Validation loss: 2.486421250214752

Epoch: 6| Step: 13
Training loss: 2.1925735492614375
Validation loss: 2.4927987013899555

Epoch: 150| Step: 0
Training loss: 2.6545831274717995
Validation loss: 2.4871005736005003

Epoch: 6| Step: 1
Training loss: 1.8214829247109148
Validation loss: 2.489809214880429

Epoch: 6| Step: 2
Training loss: 2.4834422153779987
Validation loss: 2.492458571915866

Epoch: 6| Step: 3
Training loss: 2.679077790854204
Validation loss: 2.4865016512113245

Epoch: 6| Step: 4
Training loss: 1.932486908980588
Validation loss: 2.492287676640429

Epoch: 6| Step: 5
Training loss: 2.3128119593345073
Validation loss: 2.489780263946199

Epoch: 6| Step: 6
Training loss: 2.026853527857569
Validation loss: 2.4962867260254344

Epoch: 6| Step: 7
Training loss: 2.5887881698262647
Validation loss: 2.4982735871730655

Epoch: 6| Step: 8
Training loss: 2.563877572801203
Validation loss: 2.499049498747747

Epoch: 6| Step: 9
Training loss: 2.5272824310929773
Validation loss: 2.494355792127199

Epoch: 6| Step: 10
Training loss: 2.742046906869289
Validation loss: 2.4923442922795687

Epoch: 6| Step: 11
Training loss: 2.972451562639218
Validation loss: 2.4948903477742657

Epoch: 6| Step: 12
Training loss: 2.1907969016896853
Validation loss: 2.496066639036104

Epoch: 6| Step: 13
Training loss: 2.622257980365086
Validation loss: 2.4943652548488497

Epoch: 151| Step: 0
Training loss: 1.9472373417031361
Validation loss: 2.4862684236217545

Epoch: 6| Step: 1
Training loss: 2.6097905176508913
Validation loss: 2.4942854418791103

Epoch: 6| Step: 2
Training loss: 3.0637729101363327
Validation loss: 2.488839047529888

Epoch: 6| Step: 3
Training loss: 2.810153490944877
Validation loss: 2.4898021607146843

Epoch: 6| Step: 4
Training loss: 2.2733413308898816
Validation loss: 2.48781707921697

Epoch: 6| Step: 5
Training loss: 2.471736984161557
Validation loss: 2.4880094035360356

Epoch: 6| Step: 6
Training loss: 3.077623045605138
Validation loss: 2.4904509645903925

Epoch: 6| Step: 7
Training loss: 1.927362500839036
Validation loss: 2.494959470538208

Epoch: 6| Step: 8
Training loss: 2.087452770315616
Validation loss: 2.4935627235196858

Epoch: 6| Step: 9
Training loss: 1.933422967323744
Validation loss: 2.4786400810079297

Epoch: 6| Step: 10
Training loss: 2.4606266854813095
Validation loss: 2.4924512063941586

Epoch: 6| Step: 11
Training loss: 2.1365893113787795
Validation loss: 2.4879457256524136

Epoch: 6| Step: 12
Training loss: 2.6926750215478066
Validation loss: 2.4911023909725927

Epoch: 6| Step: 13
Training loss: 2.6767679664505515
Validation loss: 2.4927501780697

Epoch: 152| Step: 0
Training loss: 2.8800134483659194
Validation loss: 2.4852863935259153

Epoch: 6| Step: 1
Training loss: 2.419781672347196
Validation loss: 2.479059639124468

Epoch: 6| Step: 2
Training loss: 2.593852213488641
Validation loss: 2.4903648830946703

Epoch: 6| Step: 3
Training loss: 2.262284334979244
Validation loss: 2.48225190555959

Epoch: 6| Step: 4
Training loss: 2.773724092531862
Validation loss: 2.4834321830157853

Epoch: 6| Step: 5
Training loss: 2.3405025108892756
Validation loss: 2.4954659513722506

Epoch: 6| Step: 6
Training loss: 2.2188977474395997
Validation loss: 2.48769567796656

Epoch: 6| Step: 7
Training loss: 2.6736567395653
Validation loss: 2.4888411630040577

Epoch: 6| Step: 8
Training loss: 2.673915686054886
Validation loss: 2.489594508721928

Epoch: 6| Step: 9
Training loss: 2.1236451261513003
Validation loss: 2.4956967829656893

Epoch: 6| Step: 10
Training loss: 2.094164451459307
Validation loss: 2.4894514541065695

Epoch: 6| Step: 11
Training loss: 2.900356428670462
Validation loss: 2.496912941706072

Epoch: 6| Step: 12
Training loss: 2.082252018372795
Validation loss: 2.4996040189582027

Epoch: 6| Step: 13
Training loss: 1.9993361921202006
Validation loss: 2.502612227401428

Epoch: 153| Step: 0
Training loss: 2.582460676486549
Validation loss: 2.5111257862709695

Epoch: 6| Step: 1
Training loss: 2.6764749115142723
Validation loss: 2.497266419325699

Epoch: 6| Step: 2
Training loss: 1.589178672602483
Validation loss: 2.503471237537498

Epoch: 6| Step: 3
Training loss: 2.162994451719409
Validation loss: 2.49353988769359

Epoch: 6| Step: 4
Training loss: 2.59674689646005
Validation loss: 2.5034877925346866

Epoch: 6| Step: 5
Training loss: 2.431494332321248
Validation loss: 2.4892747254144405

Epoch: 6| Step: 6
Training loss: 2.2758394641809936
Validation loss: 2.484076476005602

Epoch: 6| Step: 7
Training loss: 1.7426490343413585
Validation loss: 2.497734760175696

Epoch: 6| Step: 8
Training loss: 3.0150326161810823
Validation loss: 2.494967561280311

Epoch: 6| Step: 9
Training loss: 2.5927507245062125
Validation loss: 2.4931059353790355

Epoch: 6| Step: 10
Training loss: 2.4439719916391733
Validation loss: 2.5064901032388454

Epoch: 6| Step: 11
Training loss: 2.553689277513305
Validation loss: 2.507628468765596

Epoch: 6| Step: 12
Training loss: 2.8262402909123487
Validation loss: 2.5010519198824874

Epoch: 6| Step: 13
Training loss: 2.4372184786470603
Validation loss: 2.503426794050581

Epoch: 154| Step: 0
Training loss: 2.477719106060325
Validation loss: 2.5132601503785548

Epoch: 6| Step: 1
Training loss: 2.8578453767031475
Validation loss: 2.5168173671174134

Epoch: 6| Step: 2
Training loss: 2.0747629374932584
Validation loss: 2.5071092611879084

Epoch: 6| Step: 3
Training loss: 2.630073367643848
Validation loss: 2.53524074795683

Epoch: 6| Step: 4
Training loss: 2.6122328589716055
Validation loss: 2.5103006349290555

Epoch: 6| Step: 5
Training loss: 1.8120034293969651
Validation loss: 2.5164959744383903

Epoch: 6| Step: 6
Training loss: 2.3547262916331277
Validation loss: 2.503031021747896

Epoch: 6| Step: 7
Training loss: 2.0442026179568695
Validation loss: 2.50321094620972

Epoch: 6| Step: 8
Training loss: 2.5630209788618465
Validation loss: 2.499983517274562

Epoch: 6| Step: 9
Training loss: 2.494589768896238
Validation loss: 2.4932096770761114

Epoch: 6| Step: 10
Training loss: 2.874919724380293
Validation loss: 2.4961866461989266

Epoch: 6| Step: 11
Training loss: 2.7984091530987696
Validation loss: 2.488131420309597

Epoch: 6| Step: 12
Training loss: 2.5434513600945845
Validation loss: 2.491483630811555

Epoch: 6| Step: 13
Training loss: 1.853983845073516
Validation loss: 2.4971483019745224

Epoch: 155| Step: 0
Training loss: 2.4851704409690907
Validation loss: 2.4979361439237975

Epoch: 6| Step: 1
Training loss: 2.870869198276592
Validation loss: 2.495397957310074

Epoch: 6| Step: 2
Training loss: 2.388585036813297
Validation loss: 2.5141300474472863

Epoch: 6| Step: 3
Training loss: 1.9768396718489338
Validation loss: 2.515218176633815

Epoch: 6| Step: 4
Training loss: 1.9572146514417466
Validation loss: 2.510455916059647

Epoch: 6| Step: 5
Training loss: 2.3843428903320003
Validation loss: 2.5073153437748643

Epoch: 6| Step: 6
Training loss: 2.1606559520354938
Validation loss: 2.5107608073872196

Epoch: 6| Step: 7
Training loss: 2.2690996557402814
Validation loss: 2.5055998392507797

Epoch: 6| Step: 8
Training loss: 2.603143068688229
Validation loss: 2.486823005500235

Epoch: 6| Step: 9
Training loss: 3.134844418304924
Validation loss: 2.500737049808954

Epoch: 6| Step: 10
Training loss: 2.2012019861932246
Validation loss: 2.481568084365921

Epoch: 6| Step: 11
Training loss: 2.5544496944698576
Validation loss: 2.497639328294025

Epoch: 6| Step: 12
Training loss: 2.5208321458377614
Validation loss: 2.49553905486512

Epoch: 6| Step: 13
Training loss: 2.5357517665648905
Validation loss: 2.493425593870647

Epoch: 156| Step: 0
Training loss: 1.81532659130173
Validation loss: 2.493763265040658

Epoch: 6| Step: 1
Training loss: 2.4453635895970867
Validation loss: 2.4964446376868525

Epoch: 6| Step: 2
Training loss: 2.6597604331073574
Validation loss: 2.4872948464502347

Epoch: 6| Step: 3
Training loss: 2.237063411122559
Validation loss: 2.4984594366793664

Epoch: 6| Step: 4
Training loss: 2.2059106192928213
Validation loss: 2.496413471598034

Epoch: 6| Step: 5
Training loss: 3.281444144182058
Validation loss: 2.5000133831937514

Epoch: 6| Step: 6
Training loss: 2.125230215729259
Validation loss: 2.4962527961186805

Epoch: 6| Step: 7
Training loss: 2.6564699867257593
Validation loss: 2.5014858757513414

Epoch: 6| Step: 8
Training loss: 1.8195642071135039
Validation loss: 2.4952022610150126

Epoch: 6| Step: 9
Training loss: 2.980677366142662
Validation loss: 2.484018328193131

Epoch: 6| Step: 10
Training loss: 2.1578882600493676
Validation loss: 2.5010017772090167

Epoch: 6| Step: 11
Training loss: 2.7376192737531824
Validation loss: 2.5015213469964968

Epoch: 6| Step: 12
Training loss: 2.5254923010093053
Validation loss: 2.5094493147129717

Epoch: 6| Step: 13
Training loss: 1.9579346907130315
Validation loss: 2.510543397946836

Epoch: 157| Step: 0
Training loss: 2.2399691463797886
Validation loss: 2.506820894638795

Epoch: 6| Step: 1
Training loss: 2.987704511848866
Validation loss: 2.5120059057356627

Epoch: 6| Step: 2
Training loss: 2.007227949851131
Validation loss: 2.5108976233179705

Epoch: 6| Step: 3
Training loss: 2.5748271773035483
Validation loss: 2.5110104651334946

Epoch: 6| Step: 4
Training loss: 2.3823605968765107
Validation loss: 2.5161831475498673

Epoch: 6| Step: 5
Training loss: 2.2800511384549527
Validation loss: 2.5135960581069687

Epoch: 6| Step: 6
Training loss: 2.7595745217583265
Validation loss: 2.5138570761262993

Epoch: 6| Step: 7
Training loss: 2.548235851474983
Validation loss: 2.509049883755438

Epoch: 6| Step: 8
Training loss: 2.428599319377993
Validation loss: 2.5110006536910565

Epoch: 6| Step: 9
Training loss: 2.276907563354953
Validation loss: 2.515700321373053

Epoch: 6| Step: 10
Training loss: 2.7391044478569144
Validation loss: 2.5185310211785503

Epoch: 6| Step: 11
Training loss: 2.60454720323526
Validation loss: 2.5142690190853965

Epoch: 6| Step: 12
Training loss: 2.3280532108989904
Validation loss: 2.51441904546354

Epoch: 6| Step: 13
Training loss: 1.5521503237747283
Validation loss: 2.5226292058684923

Epoch: 158| Step: 0
Training loss: 2.1318498052686827
Validation loss: 2.5249176408536322

Epoch: 6| Step: 1
Training loss: 2.4838640179355083
Validation loss: 2.5174636913460513

Epoch: 6| Step: 2
Training loss: 2.877810887064851
Validation loss: 2.5251665834676085

Epoch: 6| Step: 3
Training loss: 1.9180094603900872
Validation loss: 2.5133023804327834

Epoch: 6| Step: 4
Training loss: 1.9873583137343878
Validation loss: 2.5026185627079003

Epoch: 6| Step: 5
Training loss: 2.601032924879297
Validation loss: 2.5057236557141818

Epoch: 6| Step: 6
Training loss: 2.6804561124374198
Validation loss: 2.5072312201290043

Epoch: 6| Step: 7
Training loss: 2.1486445656607462
Validation loss: 2.508292622621335

Epoch: 6| Step: 8
Training loss: 2.8951127141379964
Validation loss: 2.4972969382988284

Epoch: 6| Step: 9
Training loss: 2.2852029909749674
Validation loss: 2.50693863550736

Epoch: 6| Step: 10
Training loss: 2.5831037132185397
Validation loss: 2.5050021197068313

Epoch: 6| Step: 11
Training loss: 2.5371766584371227
Validation loss: 2.507241569328125

Epoch: 6| Step: 12
Training loss: 2.805357956511104
Validation loss: 2.512176598505741

Epoch: 6| Step: 13
Training loss: 2.2683903096387077
Validation loss: 2.4994731745350327

Epoch: 159| Step: 0
Training loss: 3.1200150976060277
Validation loss: 2.5146270889314257

Epoch: 6| Step: 1
Training loss: 2.6671854547672664
Validation loss: 2.5109498076502237

Epoch: 6| Step: 2
Training loss: 2.3069229922163856
Validation loss: 2.509113786212588

Epoch: 6| Step: 3
Training loss: 2.6400171065498754
Validation loss: 2.5123360657864575

Epoch: 6| Step: 4
Training loss: 2.268818781206817
Validation loss: 2.518364624882638

Epoch: 6| Step: 5
Training loss: 2.558692802084344
Validation loss: 2.511309094440228

Epoch: 6| Step: 6
Training loss: 2.0786310024460413
Validation loss: 2.5087679966900542

Epoch: 6| Step: 7
Training loss: 2.169046769738436
Validation loss: 2.514572112767665

Epoch: 6| Step: 8
Training loss: 2.7010525382622834
Validation loss: 2.5227206602642553

Epoch: 6| Step: 9
Training loss: 2.4696494286899635
Validation loss: 2.5211535295102276

Epoch: 6| Step: 10
Training loss: 1.8086614410717667
Validation loss: 2.527412378243142

Epoch: 6| Step: 11
Training loss: 2.313192006263859
Validation loss: 2.514891460243043

Epoch: 6| Step: 12
Training loss: 2.242165169953594
Validation loss: 2.5192396838018687

Epoch: 6| Step: 13
Training loss: 2.3058074816086513
Validation loss: 2.514196033460109

Epoch: 160| Step: 0
Training loss: 2.2430262043715214
Validation loss: 2.5089691917106123

Epoch: 6| Step: 1
Training loss: 2.4631983468731904
Validation loss: 2.5046260630274766

Epoch: 6| Step: 2
Training loss: 2.127371418493179
Validation loss: 2.506242080130553

Epoch: 6| Step: 3
Training loss: 2.3069567871478074
Validation loss: 2.5019105604384158

Epoch: 6| Step: 4
Training loss: 2.3977039718974957
Validation loss: 2.4982041584574275

Epoch: 6| Step: 5
Training loss: 2.402281622354679
Validation loss: 2.4957516415387544

Epoch: 6| Step: 6
Training loss: 2.78822660341847
Validation loss: 2.5098582289021882

Epoch: 6| Step: 7
Training loss: 2.8875404189942837
Validation loss: 2.521615385221993

Epoch: 6| Step: 8
Training loss: 2.6110006967380723
Validation loss: 2.522657874327265

Epoch: 6| Step: 9
Training loss: 1.6589464514277832
Validation loss: 2.5151196163669614

Epoch: 6| Step: 10
Training loss: 2.722506591536684
Validation loss: 2.534786650014477

Epoch: 6| Step: 11
Training loss: 2.526515536087074
Validation loss: 2.5232365924863625

Epoch: 6| Step: 12
Training loss: 2.756750318310471
Validation loss: 2.5239759722975075

Epoch: 6| Step: 13
Training loss: 1.7820212049924447
Validation loss: 2.5328125545965303

Epoch: 161| Step: 0
Training loss: 2.3958209051279775
Validation loss: 2.5226667110777132

Epoch: 6| Step: 1
Training loss: 2.259978736844931
Validation loss: 2.5239002287743326

Epoch: 6| Step: 2
Training loss: 1.9281846344812672
Validation loss: 2.524683806951055

Epoch: 6| Step: 3
Training loss: 2.431352149366882
Validation loss: 2.518340104719038

Epoch: 6| Step: 4
Training loss: 2.698117596030929
Validation loss: 2.51512374780779

Epoch: 6| Step: 5
Training loss: 2.3335570387096554
Validation loss: 2.515135439037604

Epoch: 6| Step: 6
Training loss: 2.2785272941637156
Validation loss: 2.510899609428407

Epoch: 6| Step: 7
Training loss: 2.003924215433296
Validation loss: 2.505616586350229

Epoch: 6| Step: 8
Training loss: 2.285935071942597
Validation loss: 2.5200666464969244

Epoch: 6| Step: 9
Training loss: 2.783810754668869
Validation loss: 2.5283599335694675

Epoch: 6| Step: 10
Training loss: 2.4776019974392263
Validation loss: 2.5135287125769157

Epoch: 6| Step: 11
Training loss: 2.566478896600706
Validation loss: 2.511347773452282

Epoch: 6| Step: 12
Training loss: 2.937614276367602
Validation loss: 2.510861121335255

Epoch: 6| Step: 13
Training loss: 2.6035361378599258
Validation loss: 2.508440551071621

Epoch: 162| Step: 0
Training loss: 2.0809176617839813
Validation loss: 2.512663367481382

Epoch: 6| Step: 1
Training loss: 1.947843567263685
Validation loss: 2.5189984608441445

Epoch: 6| Step: 2
Training loss: 2.6486574154812357
Validation loss: 2.5204077248381314

Epoch: 6| Step: 3
Training loss: 1.7856884586646353
Validation loss: 2.520309919342971

Epoch: 6| Step: 4
Training loss: 2.579703477778824
Validation loss: 2.518010290049605

Epoch: 6| Step: 5
Training loss: 2.709815065049196
Validation loss: 2.5117910482503105

Epoch: 6| Step: 6
Training loss: 2.6836106301211284
Validation loss: 2.5138152663782516

Epoch: 6| Step: 7
Training loss: 1.6667202066722873
Validation loss: 2.5193269789735813

Epoch: 6| Step: 8
Training loss: 2.6549933658667833
Validation loss: 2.5167246403448287

Epoch: 6| Step: 9
Training loss: 2.5688193021832593
Validation loss: 2.511922129005171

Epoch: 6| Step: 10
Training loss: 2.09071654160688
Validation loss: 2.511921243133576

Epoch: 6| Step: 11
Training loss: 2.6599234816652544
Validation loss: 2.5036315449883007

Epoch: 6| Step: 12
Training loss: 3.004752527288086
Validation loss: 2.5033577304807664

Epoch: 6| Step: 13
Training loss: 2.401919642283451
Validation loss: 2.5167101776285885

Epoch: 163| Step: 0
Training loss: 2.2327834643323996
Validation loss: 2.517604467679813

Epoch: 6| Step: 1
Training loss: 2.7660380771578525
Validation loss: 2.5127055047732014

Epoch: 6| Step: 2
Training loss: 2.250237558327718
Validation loss: 2.5278746230631888

Epoch: 6| Step: 3
Training loss: 1.9337886827640143
Validation loss: 2.537216658033334

Epoch: 6| Step: 4
Training loss: 2.2729723477441643
Validation loss: 2.5347043395318822

Epoch: 6| Step: 5
Training loss: 2.691055341414483
Validation loss: 2.541060787173923

Epoch: 6| Step: 6
Training loss: 2.072183936375899
Validation loss: 2.5352201056997954

Epoch: 6| Step: 7
Training loss: 2.375331654731756
Validation loss: 2.5363924277723324

Epoch: 6| Step: 8
Training loss: 2.284077812098614
Validation loss: 2.5315357957414184

Epoch: 6| Step: 9
Training loss: 2.508995276351259
Validation loss: 2.52807744896631

Epoch: 6| Step: 10
Training loss: 3.035209979314491
Validation loss: 2.5089604334189066

Epoch: 6| Step: 11
Training loss: 2.606198141556347
Validation loss: 2.5030702493674912

Epoch: 6| Step: 12
Training loss: 2.3938503075043704
Validation loss: 2.5037072351072553

Epoch: 6| Step: 13
Training loss: 2.6110602322033567
Validation loss: 2.503564486932236

Epoch: 164| Step: 0
Training loss: 2.1931654452891047
Validation loss: 2.4934950281520325

Epoch: 6| Step: 1
Training loss: 2.379022604816889
Validation loss: 2.50556287153557

Epoch: 6| Step: 2
Training loss: 2.033941979329981
Validation loss: 2.5053634251881314

Epoch: 6| Step: 3
Training loss: 2.3568835714503504
Validation loss: 2.5108169669162206

Epoch: 6| Step: 4
Training loss: 2.08373678751271
Validation loss: 2.516447955432076

Epoch: 6| Step: 5
Training loss: 1.5560036460421878
Validation loss: 2.514994816796869

Epoch: 6| Step: 6
Training loss: 2.816941378437475
Validation loss: 2.50725686324249

Epoch: 6| Step: 7
Training loss: 2.2762197138255154
Validation loss: 2.5247226430564966

Epoch: 6| Step: 8
Training loss: 2.342035403921434
Validation loss: 2.5347002948758957

Epoch: 6| Step: 9
Training loss: 3.2141204428190404
Validation loss: 2.541329507404915

Epoch: 6| Step: 10
Training loss: 2.9124237443494363
Validation loss: 2.554867161646306

Epoch: 6| Step: 11
Training loss: 2.599037388355415
Validation loss: 2.537398934818073

Epoch: 6| Step: 12
Training loss: 2.512537323024489
Validation loss: 2.5191600437394825

Epoch: 6| Step: 13
Training loss: 2.75894323407711
Validation loss: 2.5185237319067473

Epoch: 165| Step: 0
Training loss: 2.5418589055471252
Validation loss: 2.5097556028648387

Epoch: 6| Step: 1
Training loss: 2.1654996541176454
Validation loss: 2.5115801751732802

Epoch: 6| Step: 2
Training loss: 2.077937641159054
Validation loss: 2.5139777830528596

Epoch: 6| Step: 3
Training loss: 2.614050234568038
Validation loss: 2.513837633494422

Epoch: 6| Step: 4
Training loss: 2.070744998309174
Validation loss: 2.5182751898256646

Epoch: 6| Step: 5
Training loss: 2.353349785877816
Validation loss: 2.512100926300348

Epoch: 6| Step: 6
Training loss: 2.559043879625815
Validation loss: 2.512830773843648

Epoch: 6| Step: 7
Training loss: 2.310454933484812
Validation loss: 2.516425895752401

Epoch: 6| Step: 8
Training loss: 3.0699270069503166
Validation loss: 2.52544786742896

Epoch: 6| Step: 9
Training loss: 1.858413832536076
Validation loss: 2.5122073792314303

Epoch: 6| Step: 10
Training loss: 2.6192705206125897
Validation loss: 2.5264386654909257

Epoch: 6| Step: 11
Training loss: 2.4092386684612395
Validation loss: 2.5350350231933954

Epoch: 6| Step: 12
Training loss: 2.2683899943244588
Validation loss: 2.5225876517768038

Epoch: 6| Step: 13
Training loss: 2.843104697989588
Validation loss: 2.540802837496177

Epoch: 166| Step: 0
Training loss: 2.6747055676003013
Validation loss: 2.5396049350469667

Epoch: 6| Step: 1
Training loss: 2.335044766799507
Validation loss: 2.5349420771480253

Epoch: 6| Step: 2
Training loss: 2.1666081738523713
Validation loss: 2.526347023247306

Epoch: 6| Step: 3
Training loss: 3.014168026640563
Validation loss: 2.5309686386655303

Epoch: 6| Step: 4
Training loss: 2.1941527428953695
Validation loss: 2.5306221140057352

Epoch: 6| Step: 5
Training loss: 2.148234853724185
Validation loss: 2.5253087242163383

Epoch: 6| Step: 6
Training loss: 2.275006149619039
Validation loss: 2.5229613777144753

Epoch: 6| Step: 7
Training loss: 1.7921720575600228
Validation loss: 2.5342718069671095

Epoch: 6| Step: 8
Training loss: 2.8104482160248523
Validation loss: 2.527518249179198

Epoch: 6| Step: 9
Training loss: 2.1311467970754183
Validation loss: 2.5382583440230637

Epoch: 6| Step: 10
Training loss: 2.751742504498863
Validation loss: 2.5328384093728893

Epoch: 6| Step: 11
Training loss: 2.054516455199388
Validation loss: 2.5457321910672914

Epoch: 6| Step: 12
Training loss: 2.9075901674178635
Validation loss: 2.5439446720798538

Epoch: 6| Step: 13
Training loss: 2.4231511845230416
Validation loss: 2.521964636387095

Epoch: 167| Step: 0
Training loss: 1.8869850492021594
Validation loss: 2.5232231592375785

Epoch: 6| Step: 1
Training loss: 2.6407543805270315
Validation loss: 2.5176416058129147

Epoch: 6| Step: 2
Training loss: 2.3587938818314185
Validation loss: 2.514096650752462

Epoch: 6| Step: 3
Training loss: 2.8065993399560085
Validation loss: 2.514964188686707

Epoch: 6| Step: 4
Training loss: 2.30605221445567
Validation loss: 2.515708621828898

Epoch: 6| Step: 5
Training loss: 2.978163881638483
Validation loss: 2.5184949453880123

Epoch: 6| Step: 6
Training loss: 2.502448504182007
Validation loss: 2.516589364913021

Epoch: 6| Step: 7
Training loss: 1.9368733654033783
Validation loss: 2.5164026436616544

Epoch: 6| Step: 8
Training loss: 2.596094014491786
Validation loss: 2.522485748417106

Epoch: 6| Step: 9
Training loss: 2.5792050035757086
Validation loss: 2.5112274148146008

Epoch: 6| Step: 10
Training loss: 2.2942862941352344
Validation loss: 2.5227746476748067

Epoch: 6| Step: 11
Training loss: 2.273828636684468
Validation loss: 2.5216504708841754

Epoch: 6| Step: 12
Training loss: 1.9604365708262617
Validation loss: 2.5205732531934384

Epoch: 6| Step: 13
Training loss: 2.4166585549404767
Validation loss: 2.543540597305575

Epoch: 168| Step: 0
Training loss: 2.721035051876407
Validation loss: 2.519931589893448

Epoch: 6| Step: 1
Training loss: 2.2752082980867088
Validation loss: 2.5320051424743633

Epoch: 6| Step: 2
Training loss: 2.175189665493927
Validation loss: 2.530266606076146

Epoch: 6| Step: 3
Training loss: 3.078696570604602
Validation loss: 2.5305125414192013

Epoch: 6| Step: 4
Training loss: 1.3755840448282273
Validation loss: 2.5395700661425087

Epoch: 6| Step: 5
Training loss: 1.6873042381613665
Validation loss: 2.5296933996183997

Epoch: 6| Step: 6
Training loss: 2.364820348033419
Validation loss: 2.5336835254216545

Epoch: 6| Step: 7
Training loss: 2.332229375809584
Validation loss: 2.5468383661611553

Epoch: 6| Step: 8
Training loss: 2.2466989249263323
Validation loss: 2.5480394412321217

Epoch: 6| Step: 9
Training loss: 3.1843710108498997
Validation loss: 2.5470536370223087

Epoch: 6| Step: 10
Training loss: 2.0035583313042014
Validation loss: 2.548996476694882

Epoch: 6| Step: 11
Training loss: 2.488109349132338
Validation loss: 2.5386962313446517

Epoch: 6| Step: 12
Training loss: 2.369778364095276
Validation loss: 2.5283198410039636

Epoch: 6| Step: 13
Training loss: 2.7212623300829373
Validation loss: 2.5322977809500045

Epoch: 169| Step: 0
Training loss: 2.4290592080411364
Validation loss: 2.5364793989389023

Epoch: 6| Step: 1
Training loss: 3.12214774619528
Validation loss: 2.5191610059339107

Epoch: 6| Step: 2
Training loss: 2.157209763260754
Validation loss: 2.530586579592438

Epoch: 6| Step: 3
Training loss: 2.295603705228749
Validation loss: 2.5368141744570325

Epoch: 6| Step: 4
Training loss: 1.7261293308033283
Validation loss: 2.5268546195650745

Epoch: 6| Step: 5
Training loss: 2.8546168025741343
Validation loss: 2.526542446141031

Epoch: 6| Step: 6
Training loss: 1.9957065393340643
Validation loss: 2.531198069349791

Epoch: 6| Step: 7
Training loss: 2.9408514505204644
Validation loss: 2.5252983231774593

Epoch: 6| Step: 8
Training loss: 2.481600189480015
Validation loss: 2.5242179553401494

Epoch: 6| Step: 9
Training loss: 2.3956217686218078
Validation loss: 2.521822354422293

Epoch: 6| Step: 10
Training loss: 2.68976745763184
Validation loss: 2.519821993984496

Epoch: 6| Step: 11
Training loss: 2.3295351404608797
Validation loss: 2.528404763880123

Epoch: 6| Step: 12
Training loss: 2.2037477086704955
Validation loss: 2.512869706163901

Epoch: 6| Step: 13
Training loss: 2.208771092433422
Validation loss: 2.522692764347175

Epoch: 170| Step: 0
Training loss: 2.2880423814809734
Validation loss: 2.516203448526192

Epoch: 6| Step: 1
Training loss: 2.0204846844978874
Validation loss: 2.5291342664609298

Epoch: 6| Step: 2
Training loss: 2.125485757396417
Validation loss: 2.521327030818868

Epoch: 6| Step: 3
Training loss: 2.5820550729919334
Validation loss: 2.5357451536166185

Epoch: 6| Step: 4
Training loss: 3.0039989521619086
Validation loss: 2.540670861662731

Epoch: 6| Step: 5
Training loss: 2.3852860416367774
Validation loss: 2.5327630250190656

Epoch: 6| Step: 6
Training loss: 2.11566005660838
Validation loss: 2.5444236144648147

Epoch: 6| Step: 7
Training loss: 2.621931826097297
Validation loss: 2.5480555039323605

Epoch: 6| Step: 8
Training loss: 2.511260327497848
Validation loss: 2.54267656607802

Epoch: 6| Step: 9
Training loss: 2.4611587667144894
Validation loss: 2.549761869805678

Epoch: 6| Step: 10
Training loss: 2.3443537633793867
Validation loss: 2.541512833822977

Epoch: 6| Step: 11
Training loss: 2.6086443032172877
Validation loss: 2.5374512005327596

Epoch: 6| Step: 12
Training loss: 2.1906717466843797
Validation loss: 2.527434625028535

Epoch: 6| Step: 13
Training loss: 2.6366937932846692
Validation loss: 2.5283266462549014

Epoch: 171| Step: 0
Training loss: 2.246429577475692
Validation loss: 2.5328501129652192

Epoch: 6| Step: 1
Training loss: 1.602907908949277
Validation loss: 2.523569613404619

Epoch: 6| Step: 2
Training loss: 2.1172402012871117
Validation loss: 2.52158518421501

Epoch: 6| Step: 3
Training loss: 2.6037424479506437
Validation loss: 2.531990602255883

Epoch: 6| Step: 4
Training loss: 2.5887505020443196
Validation loss: 2.531299637672785

Epoch: 6| Step: 5
Training loss: 2.274979635294219
Validation loss: 2.5389514605377306

Epoch: 6| Step: 6
Training loss: 2.483346786291904
Validation loss: 2.5270895655702716

Epoch: 6| Step: 7
Training loss: 2.3527824811151548
Validation loss: 2.531996503099354

Epoch: 6| Step: 8
Training loss: 2.7616225769477123
Validation loss: 2.534195555806934

Epoch: 6| Step: 9
Training loss: 2.464110445222552
Validation loss: 2.525020846667961

Epoch: 6| Step: 10
Training loss: 2.3643574431713135
Validation loss: 2.5205008206029724

Epoch: 6| Step: 11
Training loss: 2.1675151728502695
Validation loss: 2.5337868054575274

Epoch: 6| Step: 12
Training loss: 2.8435450888785554
Validation loss: 2.546352707369636

Epoch: 6| Step: 13
Training loss: 2.5776458294941045
Validation loss: 2.5305222301007952

Epoch: 172| Step: 0
Training loss: 2.486599484099466
Validation loss: 2.5312372215168226

Epoch: 6| Step: 1
Training loss: 2.6411567468605144
Validation loss: 2.523028991198004

Epoch: 6| Step: 2
Training loss: 2.149421494300816
Validation loss: 2.5326008273077982

Epoch: 6| Step: 3
Training loss: 2.2381453281428567
Validation loss: 2.5502666832591006

Epoch: 6| Step: 4
Training loss: 2.477574860551388
Validation loss: 2.5507681487498144

Epoch: 6| Step: 5
Training loss: 2.1962470512392054
Validation loss: 2.555416940187018

Epoch: 6| Step: 6
Training loss: 2.5268602336156767
Validation loss: 2.5582485752375743

Epoch: 6| Step: 7
Training loss: 2.4282394951689668
Validation loss: 2.5621762109898514

Epoch: 6| Step: 8
Training loss: 3.0002406341683727
Validation loss: 2.547856818364684

Epoch: 6| Step: 9
Training loss: 2.7109967755699893
Validation loss: 2.5503571621711445

Epoch: 6| Step: 10
Training loss: 2.0203829653459655
Validation loss: 2.546696693716066

Epoch: 6| Step: 11
Training loss: 2.6142883640993553
Validation loss: 2.5329783155324086

Epoch: 6| Step: 12
Training loss: 2.0088644277837884
Validation loss: 2.5373747238779685

Epoch: 6| Step: 13
Training loss: 2.2093602677734934
Validation loss: 2.5258500541033717

Epoch: 173| Step: 0
Training loss: 2.4269391673306995
Validation loss: 2.5234785834433406

Epoch: 6| Step: 1
Training loss: 2.33655409824299
Validation loss: 2.5397943938384113

Epoch: 6| Step: 2
Training loss: 2.6092565818002584
Validation loss: 2.540980924017035

Epoch: 6| Step: 3
Training loss: 2.428997469244652
Validation loss: 2.530881281869435

Epoch: 6| Step: 4
Training loss: 2.2430758427770994
Validation loss: 2.543410146194102

Epoch: 6| Step: 5
Training loss: 1.7724170858039607
Validation loss: 2.5344125668853326

Epoch: 6| Step: 6
Training loss: 2.3264557659601524
Validation loss: 2.528719504501395

Epoch: 6| Step: 7
Training loss: 1.7211404995796284
Validation loss: 2.5261860652171304

Epoch: 6| Step: 8
Training loss: 2.7384595608848348
Validation loss: 2.5322711282072694

Epoch: 6| Step: 9
Training loss: 2.900338015095248
Validation loss: 2.5278816888804934

Epoch: 6| Step: 10
Training loss: 2.6631317750226433
Validation loss: 2.5376592739586643

Epoch: 6| Step: 11
Training loss: 2.0466622977360287
Validation loss: 2.5212953212474285

Epoch: 6| Step: 12
Training loss: 2.6710150154517205
Validation loss: 2.5443990330852237

Epoch: 6| Step: 13
Training loss: 2.624345879571475
Validation loss: 2.549307615114846

Epoch: 174| Step: 0
Training loss: 2.230302527224851
Validation loss: 2.5563870997143243

Epoch: 6| Step: 1
Training loss: 1.980052836609051
Validation loss: 2.5583685467644033

Epoch: 6| Step: 2
Training loss: 2.0914357618972135
Validation loss: 2.5399468549540174

Epoch: 6| Step: 3
Training loss: 2.3628230888211164
Validation loss: 2.5507674633079995

Epoch: 6| Step: 4
Training loss: 2.4564877926402615
Validation loss: 2.5461497141971288

Epoch: 6| Step: 5
Training loss: 2.584252122312613
Validation loss: 2.5295814069366616

Epoch: 6| Step: 6
Training loss: 3.0097176203281544
Validation loss: 2.527507259812511

Epoch: 6| Step: 7
Training loss: 3.0502307116690663
Validation loss: 2.530384700738021

Epoch: 6| Step: 8
Training loss: 1.9338192586695935
Validation loss: 2.5179889384221927

Epoch: 6| Step: 9
Training loss: 2.0248787360911327
Validation loss: 2.5280564652919257

Epoch: 6| Step: 10
Training loss: 2.3711201950684977
Validation loss: 2.519400990985416

Epoch: 6| Step: 11
Training loss: 2.7163612131593333
Validation loss: 2.5214824681431196

Epoch: 6| Step: 12
Training loss: 2.4970935138673305
Validation loss: 2.517699687302663

Epoch: 6| Step: 13
Training loss: 2.3969172030119195
Validation loss: 2.5048405674079395

Epoch: 175| Step: 0
Training loss: 2.4718579394189417
Validation loss: 2.515193404616258

Epoch: 6| Step: 1
Training loss: 2.222502852310393
Validation loss: 2.51821663244232

Epoch: 6| Step: 2
Training loss: 1.5420837954946776
Validation loss: 2.5155351200198544

Epoch: 6| Step: 3
Training loss: 2.7990022380202526
Validation loss: 2.5215451572773797

Epoch: 6| Step: 4
Training loss: 1.8824104101334955
Validation loss: 2.5257667994968322

Epoch: 6| Step: 5
Training loss: 2.1414321157458285
Validation loss: 2.5312152122336227

Epoch: 6| Step: 6
Training loss: 2.162086107633669
Validation loss: 2.5331943563771797

Epoch: 6| Step: 7
Training loss: 2.4630378600481393
Validation loss: 2.529879210608033

Epoch: 6| Step: 8
Training loss: 2.4973779279261237
Validation loss: 2.5214312424935685

Epoch: 6| Step: 9
Training loss: 2.238556903155913
Validation loss: 2.5223904652879394

Epoch: 6| Step: 10
Training loss: 2.765945588404869
Validation loss: 2.530251498356056

Epoch: 6| Step: 11
Training loss: 2.9262338041291613
Validation loss: 2.5363805133597412

Epoch: 6| Step: 12
Training loss: 3.209224494592027
Validation loss: 2.517610923093752

Epoch: 6| Step: 13
Training loss: 1.9118801832923822
Validation loss: 2.529707230476654

Testing loss: 2.0102598712855766

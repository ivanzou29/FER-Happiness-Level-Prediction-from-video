Epoch: 1| Step: 0
Training loss: 5.636642106580025
Validation loss: 5.929130538094046

Epoch: 5| Step: 1
Training loss: 5.683033289591525
Validation loss: 5.927236714805973

Epoch: 5| Step: 2
Training loss: 6.747657086698165
Validation loss: 5.925268846359788

Epoch: 5| Step: 3
Training loss: 7.110864370091989
Validation loss: 5.923356427015468

Epoch: 5| Step: 4
Training loss: 5.4616852386192996
Validation loss: 5.921297459550737

Epoch: 5| Step: 5
Training loss: 6.380786737706397
Validation loss: 5.919290010281631

Epoch: 5| Step: 6
Training loss: 5.8772351497427
Validation loss: 5.917250532793817

Epoch: 5| Step: 7
Training loss: 5.847542740631797
Validation loss: 5.915216398023935

Epoch: 5| Step: 8
Training loss: 5.66802057285923
Validation loss: 5.913014085933192

Epoch: 5| Step: 9
Training loss: 5.449679360013131
Validation loss: 5.910920693123802

Epoch: 5| Step: 10
Training loss: 6.100971344702166
Validation loss: 5.908710761002168

Epoch: 5| Step: 11
Training loss: 6.8945251637721015
Validation loss: 5.906405451913731

Epoch: 2| Step: 0
Training loss: 5.948791047700299
Validation loss: 5.903923027038631

Epoch: 5| Step: 1
Training loss: 5.600984636664029
Validation loss: 5.901542940815187

Epoch: 5| Step: 2
Training loss: 6.682262330400905
Validation loss: 5.899021353888233

Epoch: 5| Step: 3
Training loss: 5.572225918847317
Validation loss: 5.896386712550014

Epoch: 5| Step: 4
Training loss: 6.339373803246276
Validation loss: 5.893678024084427

Epoch: 5| Step: 5
Training loss: 6.1170322208500725
Validation loss: 5.890789889752266

Epoch: 5| Step: 6
Training loss: 6.402676809328035
Validation loss: 5.8877659950183245

Epoch: 5| Step: 7
Training loss: 6.262448844776879
Validation loss: 5.884621188888257

Epoch: 5| Step: 8
Training loss: 6.018403122711284
Validation loss: 5.881439582398722

Epoch: 5| Step: 9
Training loss: 5.8331064815916
Validation loss: 5.877970384223558

Epoch: 5| Step: 10
Training loss: 5.213403578640282
Validation loss: 5.874452551908755

Epoch: 5| Step: 11
Training loss: 5.63848988708742
Validation loss: 5.870789304053321

Epoch: 3| Step: 0
Training loss: 6.17051322496538
Validation loss: 5.866826696091702

Epoch: 5| Step: 1
Training loss: 6.6551564352563775
Validation loss: 5.862938783027505

Epoch: 5| Step: 2
Training loss: 5.614438292753751
Validation loss: 5.858606516987836

Epoch: 5| Step: 3
Training loss: 6.5894964157130635
Validation loss: 5.85420425466324

Epoch: 5| Step: 4
Training loss: 5.740489890429989
Validation loss: 5.849482976177248

Epoch: 5| Step: 5
Training loss: 6.3623741632095
Validation loss: 5.844718913979174

Epoch: 5| Step: 6
Training loss: 5.114673538603822
Validation loss: 5.839564852821956

Epoch: 5| Step: 7
Training loss: 5.946043914437722
Validation loss: 5.8343647067785085

Epoch: 5| Step: 8
Training loss: 6.304865989380184
Validation loss: 5.828942992322918

Epoch: 5| Step: 9
Training loss: 5.745746532039959
Validation loss: 5.823405348232947

Epoch: 5| Step: 10
Training loss: 5.335122980418213
Validation loss: 5.817382621242163

Epoch: 5| Step: 11
Training loss: 4.6936767572410485
Validation loss: 5.811253034492828

Epoch: 4| Step: 0
Training loss: 5.465777128620502
Validation loss: 5.804979305882112

Epoch: 5| Step: 1
Training loss: 6.395622258218837
Validation loss: 5.7984057630573105

Epoch: 5| Step: 2
Training loss: 5.51637794607303
Validation loss: 5.791595556888351

Epoch: 5| Step: 3
Training loss: 5.178733496173524
Validation loss: 5.784410537234107

Epoch: 5| Step: 4
Training loss: 5.74240605982867
Validation loss: 5.777211849554557

Epoch: 5| Step: 5
Training loss: 7.171030830118074
Validation loss: 5.7696218240283725

Epoch: 5| Step: 6
Training loss: 5.667025423848124
Validation loss: 5.761558235648138

Epoch: 5| Step: 7
Training loss: 5.480967623354186
Validation loss: 5.753614713072962

Epoch: 5| Step: 8
Training loss: 5.398961263993523
Validation loss: 5.744604703109429

Epoch: 5| Step: 9
Training loss: 6.918284429997491
Validation loss: 5.736134022853476

Epoch: 5| Step: 10
Training loss: 5.2093021966690225
Validation loss: 5.726749724154766

Epoch: 5| Step: 11
Training loss: 6.621639623054657
Validation loss: 5.717735999701749

Epoch: 5| Step: 0
Training loss: 5.522084719308171
Validation loss: 5.708247360454537

Epoch: 5| Step: 1
Training loss: 6.158597701837341
Validation loss: 5.698710310186175

Epoch: 5| Step: 2
Training loss: 5.375585302377747
Validation loss: 5.688994906361897

Epoch: 5| Step: 3
Training loss: 5.889431374632351
Validation loss: 5.679085798893592

Epoch: 5| Step: 4
Training loss: 6.073316226157679
Validation loss: 5.668844732399352

Epoch: 5| Step: 5
Training loss: 5.954153377588425
Validation loss: 5.6589501681891115

Epoch: 5| Step: 6
Training loss: 5.733863412763725
Validation loss: 5.64872172560808

Epoch: 5| Step: 7
Training loss: 5.610518230090752
Validation loss: 5.6385579288721965

Epoch: 5| Step: 8
Training loss: 5.336889313328936
Validation loss: 5.628097634934698

Epoch: 5| Step: 9
Training loss: 5.8457452152469
Validation loss: 5.618359022218511

Epoch: 5| Step: 10
Training loss: 6.167481067939217
Validation loss: 5.607462196466439

Epoch: 5| Step: 11
Training loss: 4.494310597136147
Validation loss: 5.596906769316401

Epoch: 6| Step: 0
Training loss: 5.250904913525732
Validation loss: 5.587362496250811

Epoch: 5| Step: 1
Training loss: 4.292644623867819
Validation loss: 5.577122852051002

Epoch: 5| Step: 2
Training loss: 6.188330392313713
Validation loss: 5.567291353584453

Epoch: 5| Step: 3
Training loss: 6.8555538856893135
Validation loss: 5.557799735853145

Epoch: 5| Step: 4
Training loss: 6.357113447779539
Validation loss: 5.547960044508857

Epoch: 5| Step: 5
Training loss: 4.710294107054375
Validation loss: 5.538407624651685

Epoch: 5| Step: 6
Training loss: 5.006773937682979
Validation loss: 5.529116562149407

Epoch: 5| Step: 7
Training loss: 5.362633606995807
Validation loss: 5.520167294998634

Epoch: 5| Step: 8
Training loss: 6.631640578981858
Validation loss: 5.5112006766059665

Epoch: 5| Step: 9
Training loss: 5.449421234197236
Validation loss: 5.502527710912573

Epoch: 5| Step: 10
Training loss: 5.649164978708361
Validation loss: 5.493709687890467

Epoch: 5| Step: 11
Training loss: 5.063213910286465
Validation loss: 5.485187213163573

Epoch: 7| Step: 0
Training loss: 6.783530981135229
Validation loss: 5.47680069300658

Epoch: 5| Step: 1
Training loss: 5.660775581842852
Validation loss: 5.4687371026749885

Epoch: 5| Step: 2
Training loss: 4.66985101139511
Validation loss: 5.460780995117806

Epoch: 5| Step: 3
Training loss: 5.832804664996914
Validation loss: 5.4533690112881406

Epoch: 5| Step: 4
Training loss: 5.709650189428817
Validation loss: 5.445805465735903

Epoch: 5| Step: 5
Training loss: 5.603041790048097
Validation loss: 5.4383993099363535

Epoch: 5| Step: 6
Training loss: 5.193352580559207
Validation loss: 5.431401695237191

Epoch: 5| Step: 7
Training loss: 5.511054633647613
Validation loss: 5.424436911206083

Epoch: 5| Step: 8
Training loss: 5.989870421674999
Validation loss: 5.417605387213181

Epoch: 5| Step: 9
Training loss: 4.774225397843514
Validation loss: 5.41091451644606

Epoch: 5| Step: 10
Training loss: 5.097532026743125
Validation loss: 5.4044593903324625

Epoch: 5| Step: 11
Training loss: 5.745378710396635
Validation loss: 5.398264266168305

Epoch: 8| Step: 0
Training loss: 5.339763202631804
Validation loss: 5.392003466401274

Epoch: 5| Step: 1
Training loss: 5.776719746346713
Validation loss: 5.385954826909219

Epoch: 5| Step: 2
Training loss: 5.089708468439148
Validation loss: 5.3797990089934355

Epoch: 5| Step: 3
Training loss: 4.949917208817285
Validation loss: 5.374219586397773

Epoch: 5| Step: 4
Training loss: 4.9252654493025805
Validation loss: 5.36837571793059

Epoch: 5| Step: 5
Training loss: 5.171744077010743
Validation loss: 5.362763641267042

Epoch: 5| Step: 6
Training loss: 5.408515940416038
Validation loss: 5.356927844985503

Epoch: 5| Step: 7
Training loss: 5.675113814636851
Validation loss: 5.351353743813674

Epoch: 5| Step: 8
Training loss: 5.742280837357913
Validation loss: 5.345639755399331

Epoch: 5| Step: 9
Training loss: 6.2808588769707185
Validation loss: 5.339661407892154

Epoch: 5| Step: 10
Training loss: 5.53966627610017
Validation loss: 5.333756812488898

Epoch: 5| Step: 11
Training loss: 6.6792151925491146
Validation loss: 5.327480333095931

Epoch: 9| Step: 0
Training loss: 5.537419386162832
Validation loss: 5.321319365302819

Epoch: 5| Step: 1
Training loss: 5.32484205128131
Validation loss: 5.315075022125209

Epoch: 5| Step: 2
Training loss: 5.340785581831901
Validation loss: 5.308943668872105

Epoch: 5| Step: 3
Training loss: 5.459665131133613
Validation loss: 5.30261863549403

Epoch: 5| Step: 4
Training loss: 5.66136823042615
Validation loss: 5.2964176516917485

Epoch: 5| Step: 5
Training loss: 5.358037575983339
Validation loss: 5.290354065286476

Epoch: 5| Step: 6
Training loss: 5.519085282264197
Validation loss: 5.28385868388798

Epoch: 5| Step: 7
Training loss: 5.342403844629431
Validation loss: 5.277583036120355

Epoch: 5| Step: 8
Training loss: 5.5146829378735305
Validation loss: 5.271345078525473

Epoch: 5| Step: 9
Training loss: 5.1609309363604945
Validation loss: 5.265190634843075

Epoch: 5| Step: 10
Training loss: 5.690951128208038
Validation loss: 5.2591119514213815

Epoch: 5| Step: 11
Training loss: 2.8314799436451334
Validation loss: 5.253131008504063

Epoch: 10| Step: 0
Training loss: 4.84894268663001
Validation loss: 5.247287556656779

Epoch: 5| Step: 1
Training loss: 5.173867570749027
Validation loss: 5.241776830711316

Epoch: 5| Step: 2
Training loss: 4.580258650160291
Validation loss: 5.236290485380075

Epoch: 5| Step: 3
Training loss: 6.033825892509913
Validation loss: 5.230985335351177

Epoch: 5| Step: 4
Training loss: 5.4110680428074085
Validation loss: 5.225782586160896

Epoch: 5| Step: 5
Training loss: 4.5171713111927785
Validation loss: 5.220227685228122

Epoch: 5| Step: 6
Training loss: 5.564943098546784
Validation loss: 5.215010569139001

Epoch: 5| Step: 7
Training loss: 5.81936437065654
Validation loss: 5.209607329638895

Epoch: 5| Step: 8
Training loss: 5.495079440374841
Validation loss: 5.204042409182581

Epoch: 5| Step: 9
Training loss: 6.041942230605052
Validation loss: 5.1989150404027225

Epoch: 5| Step: 10
Training loss: 5.131020150281304
Validation loss: 5.1935991181921475

Epoch: 5| Step: 11
Training loss: 4.709657106005951
Validation loss: 5.188329247373906

Epoch: 11| Step: 0
Training loss: 5.895533290098378
Validation loss: 5.183396213718406

Epoch: 5| Step: 1
Training loss: 4.920378733068325
Validation loss: 5.178175147793516

Epoch: 5| Step: 2
Training loss: 4.937460211098581
Validation loss: 5.172736980601604

Epoch: 5| Step: 3
Training loss: 5.408206651768442
Validation loss: 5.167545092674629

Epoch: 5| Step: 4
Training loss: 5.886431995234591
Validation loss: 5.162395190821449

Epoch: 5| Step: 5
Training loss: 5.425756291001808
Validation loss: 5.157059993123462

Epoch: 5| Step: 6
Training loss: 5.1988016214855275
Validation loss: 5.152151949197167

Epoch: 5| Step: 7
Training loss: 5.296156752960463
Validation loss: 5.147065515780973

Epoch: 5| Step: 8
Training loss: 5.512662356660792
Validation loss: 5.142219450781909

Epoch: 5| Step: 9
Training loss: 5.233844969375014
Validation loss: 5.137445805952934

Epoch: 5| Step: 10
Training loss: 3.6696404333441417
Validation loss: 5.132266901554072

Epoch: 5| Step: 11
Training loss: 6.845917380295502
Validation loss: 5.127819007052486

Epoch: 12| Step: 0
Training loss: 5.940485474755515
Validation loss: 5.122929480455074

Epoch: 5| Step: 1
Training loss: 4.958757441144648
Validation loss: 5.117703205553568

Epoch: 5| Step: 2
Training loss: 5.006425162500169
Validation loss: 5.11267417293914

Epoch: 5| Step: 3
Training loss: 4.619999631080778
Validation loss: 5.107600464868307

Epoch: 5| Step: 4
Training loss: 6.303666080519137
Validation loss: 5.102751511040281

Epoch: 5| Step: 5
Training loss: 4.996128490765648
Validation loss: 5.097760889531077

Epoch: 5| Step: 6
Training loss: 5.540058772742789
Validation loss: 5.092260412049444

Epoch: 5| Step: 7
Training loss: 4.681515140944497
Validation loss: 5.087271793192886

Epoch: 5| Step: 8
Training loss: 4.830428182131424
Validation loss: 5.082436195794903

Epoch: 5| Step: 9
Training loss: 4.802183115729965
Validation loss: 5.076953373266922

Epoch: 5| Step: 10
Training loss: 5.315440362754529
Validation loss: 5.071930684402722

Epoch: 5| Step: 11
Training loss: 5.974789105767536
Validation loss: 5.066987415354556

Epoch: 13| Step: 0
Training loss: 5.195077532280558
Validation loss: 5.061796924450368

Epoch: 5| Step: 1
Training loss: 4.594176188310932
Validation loss: 5.056785112882661

Epoch: 5| Step: 2
Training loss: 5.478279051543208
Validation loss: 5.051908842658772

Epoch: 5| Step: 3
Training loss: 4.509489861371733
Validation loss: 5.04684805690632

Epoch: 5| Step: 4
Training loss: 4.686365015587899
Validation loss: 5.041781860275612

Epoch: 5| Step: 5
Training loss: 5.298171703316829
Validation loss: 5.037050556235362

Epoch: 5| Step: 6
Training loss: 5.389587125517586
Validation loss: 5.031998762248847

Epoch: 5| Step: 7
Training loss: 5.2043307170355995
Validation loss: 5.026576713057323

Epoch: 5| Step: 8
Training loss: 5.184767589455196
Validation loss: 5.021084785760087

Epoch: 5| Step: 9
Training loss: 5.876984930538728
Validation loss: 5.016148580474413

Epoch: 5| Step: 10
Training loss: 5.181861110673703
Validation loss: 5.011017129596642

Epoch: 5| Step: 11
Training loss: 5.134638770601631
Validation loss: 5.005558976809543

Epoch: 14| Step: 0
Training loss: 4.6943472125475685
Validation loss: 5.0004132894733395

Epoch: 5| Step: 1
Training loss: 5.733238336234449
Validation loss: 4.995283365825912

Epoch: 5| Step: 2
Training loss: 5.181388472120522
Validation loss: 4.990474058642274

Epoch: 5| Step: 3
Training loss: 4.714455172147725
Validation loss: 4.985100560808099

Epoch: 5| Step: 4
Training loss: 5.533753254288241
Validation loss: 4.979614479751263

Epoch: 5| Step: 5
Training loss: 4.611249329258494
Validation loss: 4.974698598476167

Epoch: 5| Step: 6
Training loss: 5.043809461656739
Validation loss: 4.969449535846086

Epoch: 5| Step: 7
Training loss: 5.017029942806948
Validation loss: 4.96410248487853

Epoch: 5| Step: 8
Training loss: 4.955790960501608
Validation loss: 4.958964454597847

Epoch: 5| Step: 9
Training loss: 5.52185243041732
Validation loss: 4.95411234259866

Epoch: 5| Step: 10
Training loss: 5.0859939407990575
Validation loss: 4.948995190242769

Epoch: 5| Step: 11
Training loss: 4.376867059408739
Validation loss: 4.943760801706197

Epoch: 15| Step: 0
Training loss: 4.18743577594756
Validation loss: 4.938679783543297

Epoch: 5| Step: 1
Training loss: 5.42056425669523
Validation loss: 4.933879329563941

Epoch: 5| Step: 2
Training loss: 4.9221995413214366
Validation loss: 4.928453914782783

Epoch: 5| Step: 3
Training loss: 5.081297086190198
Validation loss: 4.9233131124218525

Epoch: 5| Step: 4
Training loss: 4.902270604533998
Validation loss: 4.9184042748109595

Epoch: 5| Step: 5
Training loss: 5.198753009344919
Validation loss: 4.913029847759754

Epoch: 5| Step: 6
Training loss: 5.6073225511492515
Validation loss: 4.907478543165175

Epoch: 5| Step: 7
Training loss: 4.837237896583095
Validation loss: 4.901872313832413

Epoch: 5| Step: 8
Training loss: 4.984337307300173
Validation loss: 4.89653182491749

Epoch: 5| Step: 9
Training loss: 5.4708837215817665
Validation loss: 4.890813594346484

Epoch: 5| Step: 10
Training loss: 4.980561618313087
Validation loss: 4.885010808856606

Epoch: 5| Step: 11
Training loss: 2.9586781761950673
Validation loss: 4.879531816174707

Epoch: 16| Step: 0
Training loss: 5.3275432548753985
Validation loss: 4.87368110193859

Epoch: 5| Step: 1
Training loss: 5.577649008052698
Validation loss: 4.868874165262044

Epoch: 5| Step: 2
Training loss: 5.427291231572587
Validation loss: 4.86333407331221

Epoch: 5| Step: 3
Training loss: 4.590698474272673
Validation loss: 4.857662696528572

Epoch: 5| Step: 4
Training loss: 4.658641226777823
Validation loss: 4.852280027738541

Epoch: 5| Step: 5
Training loss: 4.969866549198851
Validation loss: 4.847189873396453

Epoch: 5| Step: 6
Training loss: 5.686851380640106
Validation loss: 4.841591331445168

Epoch: 5| Step: 7
Training loss: 4.711139529120508
Validation loss: 4.836222831759534

Epoch: 5| Step: 8
Training loss: 5.064302100163333
Validation loss: 4.83130197205087

Epoch: 5| Step: 9
Training loss: 4.342338058096239
Validation loss: 4.826158388459379

Epoch: 5| Step: 10
Training loss: 4.517671009367211
Validation loss: 4.820741235594598

Epoch: 5| Step: 11
Training loss: 2.5344350109215585
Validation loss: 4.81617749060997

Epoch: 17| Step: 0
Training loss: 5.343675400954991
Validation loss: 4.811379405424045

Epoch: 5| Step: 1
Training loss: 5.262236005454298
Validation loss: 4.806629737367355

Epoch: 5| Step: 2
Training loss: 4.870773831089168
Validation loss: 4.802245957219908

Epoch: 5| Step: 3
Training loss: 4.308729652066791
Validation loss: 4.797080724158773

Epoch: 5| Step: 4
Training loss: 4.706568677024468
Validation loss: 4.7920696614291005

Epoch: 5| Step: 5
Training loss: 5.036562561197997
Validation loss: 4.786980936316202

Epoch: 5| Step: 6
Training loss: 5.533378407091759
Validation loss: 4.782576732189436

Epoch: 5| Step: 7
Training loss: 4.3103322648490145
Validation loss: 4.777957256159478

Epoch: 5| Step: 8
Training loss: 5.24287330473491
Validation loss: 4.772659812797282

Epoch: 5| Step: 9
Training loss: 4.49906509012491
Validation loss: 4.768063247713474

Epoch: 5| Step: 10
Training loss: 4.815601092854146
Validation loss: 4.763189066367734

Epoch: 5| Step: 11
Training loss: 4.592305481788153
Validation loss: 4.758225947614151

Epoch: 18| Step: 0
Training loss: 4.878436124705042
Validation loss: 4.753553533621047

Epoch: 5| Step: 1
Training loss: 4.742548267398394
Validation loss: 4.748533658351842

Epoch: 5| Step: 2
Training loss: 5.196115165027382
Validation loss: 4.744046043436304

Epoch: 5| Step: 3
Training loss: 5.148683857558929
Validation loss: 4.7390798979055315

Epoch: 5| Step: 4
Training loss: 4.501112058772254
Validation loss: 4.734499477516839

Epoch: 5| Step: 5
Training loss: 4.3044799090003085
Validation loss: 4.729252031650958

Epoch: 5| Step: 6
Training loss: 4.493065471268092
Validation loss: 4.724110470151168

Epoch: 5| Step: 7
Training loss: 5.328702923260064
Validation loss: 4.719836869869113

Epoch: 5| Step: 8
Training loss: 4.803092778730062
Validation loss: 4.715167355013058

Epoch: 5| Step: 9
Training loss: 5.158695635092031
Validation loss: 4.709485717565481

Epoch: 5| Step: 10
Training loss: 4.455372668090995
Validation loss: 4.704471872642476

Epoch: 5| Step: 11
Training loss: 6.076643030089617
Validation loss: 4.699246536219241

Epoch: 19| Step: 0
Training loss: 4.68873295145538
Validation loss: 4.694394657176977

Epoch: 5| Step: 1
Training loss: 5.154156994123128
Validation loss: 4.689188996248221

Epoch: 5| Step: 2
Training loss: 5.3872820675133735
Validation loss: 4.684443151017693

Epoch: 5| Step: 3
Training loss: 3.8012707543049573
Validation loss: 4.679438767485894

Epoch: 5| Step: 4
Training loss: 4.974608416457996
Validation loss: 4.674315607154603

Epoch: 5| Step: 5
Training loss: 4.355103728047779
Validation loss: 4.669724002200135

Epoch: 5| Step: 6
Training loss: 4.6323680439336865
Validation loss: 4.664326334977205

Epoch: 5| Step: 7
Training loss: 3.998357674091429
Validation loss: 4.659387883817316

Epoch: 5| Step: 8
Training loss: 5.185060961814661
Validation loss: 4.654324272012162

Epoch: 5| Step: 9
Training loss: 4.818889983008012
Validation loss: 4.650307620548211

Epoch: 5| Step: 10
Training loss: 5.089926566023302
Validation loss: 4.645045657020768

Epoch: 5| Step: 11
Training loss: 6.50533486562123
Validation loss: 4.640055140860991

Epoch: 20| Step: 0
Training loss: 4.730523182361273
Validation loss: 4.635545255899086

Epoch: 5| Step: 1
Training loss: 5.379769981033402
Validation loss: 4.630744897271294

Epoch: 5| Step: 2
Training loss: 4.480906776893812
Validation loss: 4.624745662672397

Epoch: 5| Step: 3
Training loss: 4.406448684433409
Validation loss: 4.619500245282124

Epoch: 5| Step: 4
Training loss: 4.123402604117688
Validation loss: 4.614727167280344

Epoch: 5| Step: 5
Training loss: 4.469440326856054
Validation loss: 4.610006041353891

Epoch: 5| Step: 6
Training loss: 4.339429724006352
Validation loss: 4.6051789058540935

Epoch: 5| Step: 7
Training loss: 5.607567966302692
Validation loss: 4.600200889872422

Epoch: 5| Step: 8
Training loss: 4.983181323196409
Validation loss: 4.595238684431368

Epoch: 5| Step: 9
Training loss: 4.2247403036807984
Validation loss: 4.591050503138288

Epoch: 5| Step: 10
Training loss: 5.027266732243191
Validation loss: 4.586430849640884

Epoch: 5| Step: 11
Training loss: 5.160877717353567
Validation loss: 4.581203540949806

Epoch: 21| Step: 0
Training loss: 4.555906155645422
Validation loss: 4.575368543973432

Epoch: 5| Step: 1
Training loss: 5.52769951096701
Validation loss: 4.570378242737124

Epoch: 5| Step: 2
Training loss: 4.76355035368011
Validation loss: 4.565325545756845

Epoch: 5| Step: 3
Training loss: 4.9309003157216615
Validation loss: 4.5605962430690274

Epoch: 5| Step: 4
Training loss: 4.451659272555646
Validation loss: 4.555401414044163

Epoch: 5| Step: 5
Training loss: 4.667191316994485
Validation loss: 4.550452958082114

Epoch: 5| Step: 6
Training loss: 4.618219095774598
Validation loss: 4.54524486636114

Epoch: 5| Step: 7
Training loss: 4.461925069921036
Validation loss: 4.540899958624349

Epoch: 5| Step: 8
Training loss: 4.838123031042201
Validation loss: 4.535887762978132

Epoch: 5| Step: 9
Training loss: 4.420715395654305
Validation loss: 4.53085535457457

Epoch: 5| Step: 10
Training loss: 4.181926605266518
Validation loss: 4.525892427162033

Epoch: 5| Step: 11
Training loss: 4.271746033938206
Validation loss: 4.521204325253923

Epoch: 22| Step: 0
Training loss: 4.720271591519399
Validation loss: 4.516258670472312

Epoch: 5| Step: 1
Training loss: 4.776099329121594
Validation loss: 4.511578234841761

Epoch: 5| Step: 2
Training loss: 3.9729435909634483
Validation loss: 4.507003041562254

Epoch: 5| Step: 3
Training loss: 4.423659325214988
Validation loss: 4.502264640660738

Epoch: 5| Step: 4
Training loss: 5.107394613335465
Validation loss: 4.4976175588949445

Epoch: 5| Step: 5
Training loss: 4.848892140440332
Validation loss: 4.492738746323728

Epoch: 5| Step: 6
Training loss: 4.1017898932501575
Validation loss: 4.488198545450354

Epoch: 5| Step: 7
Training loss: 4.526373402942065
Validation loss: 4.483312270435268

Epoch: 5| Step: 8
Training loss: 4.660784465593404
Validation loss: 4.478704822002606

Epoch: 5| Step: 9
Training loss: 4.942660761270288
Validation loss: 4.474081136182368

Epoch: 5| Step: 10
Training loss: 4.566470365180238
Validation loss: 4.469274227809502

Epoch: 5| Step: 11
Training loss: 4.905309113105635
Validation loss: 4.46456627082264

Epoch: 23| Step: 0
Training loss: 3.9626321572007224
Validation loss: 4.459945996090105

Epoch: 5| Step: 1
Training loss: 4.044579992945613
Validation loss: 4.4553696803031135

Epoch: 5| Step: 2
Training loss: 4.374057559276095
Validation loss: 4.450613132127601

Epoch: 5| Step: 3
Training loss: 5.130449142871682
Validation loss: 4.4460523511476

Epoch: 5| Step: 4
Training loss: 4.041713884739818
Validation loss: 4.44150794725567

Epoch: 5| Step: 5
Training loss: 5.119013010797288
Validation loss: 4.436901374520337

Epoch: 5| Step: 6
Training loss: 4.408090409987308
Validation loss: 4.432134464881434

Epoch: 5| Step: 7
Training loss: 4.481485638880721
Validation loss: 4.427675403571896

Epoch: 5| Step: 8
Training loss: 5.133477805115198
Validation loss: 4.423188642326331

Epoch: 5| Step: 9
Training loss: 4.501853561127888
Validation loss: 4.4185248085995505

Epoch: 5| Step: 10
Training loss: 4.5620361510386775
Validation loss: 4.413712920353023

Epoch: 5| Step: 11
Training loss: 5.65127023917297
Validation loss: 4.409124568511608

Epoch: 24| Step: 0
Training loss: 4.646724301878787
Validation loss: 4.404275158520578

Epoch: 5| Step: 1
Training loss: 4.231345356214684
Validation loss: 4.399241471653903

Epoch: 5| Step: 2
Training loss: 4.4998111685234266
Validation loss: 4.394602995110236

Epoch: 5| Step: 3
Training loss: 4.3148781673696375
Validation loss: 4.389861921834144

Epoch: 5| Step: 4
Training loss: 4.326354838822891
Validation loss: 4.385211992659915

Epoch: 5| Step: 5
Training loss: 4.3926250205922655
Validation loss: 4.3802335906992065

Epoch: 5| Step: 6
Training loss: 5.7374907715833325
Validation loss: 4.375592323351575

Epoch: 5| Step: 7
Training loss: 4.780219272388481
Validation loss: 4.370754102613669

Epoch: 5| Step: 8
Training loss: 4.2586259257406756
Validation loss: 4.366073247879634

Epoch: 5| Step: 9
Training loss: 3.8583682828326844
Validation loss: 4.361421877053667

Epoch: 5| Step: 10
Training loss: 4.376185229382462
Validation loss: 4.3568455867734865

Epoch: 5| Step: 11
Training loss: 4.192086512102437
Validation loss: 4.351958507368677

Epoch: 25| Step: 0
Training loss: 3.7882753793423527
Validation loss: 4.347371574466097

Epoch: 5| Step: 1
Training loss: 4.5084979033738675
Validation loss: 4.342683244893357

Epoch: 5| Step: 2
Training loss: 4.587929703962982
Validation loss: 4.338601489377814

Epoch: 5| Step: 3
Training loss: 3.9153758147061364
Validation loss: 4.333848572629492

Epoch: 5| Step: 4
Training loss: 4.481299857869109
Validation loss: 4.3294547571048305

Epoch: 5| Step: 5
Training loss: 4.582793371587893
Validation loss: 4.325041708993179

Epoch: 5| Step: 6
Training loss: 5.10252906323781
Validation loss: 4.3202838126455125

Epoch: 5| Step: 7
Training loss: 4.773617705144919
Validation loss: 4.315679097095503

Epoch: 5| Step: 8
Training loss: 4.377800399433519
Validation loss: 4.311059328749493

Epoch: 5| Step: 9
Training loss: 4.984044557338561
Validation loss: 4.305901985519403

Epoch: 5| Step: 10
Training loss: 3.808689902632408
Validation loss: 4.301130714273833

Epoch: 5| Step: 11
Training loss: 3.6719887573801437
Validation loss: 4.295866266390156

Epoch: 26| Step: 0
Training loss: 4.757717488660323
Validation loss: 4.290917811134764

Epoch: 5| Step: 1
Training loss: 4.591052658280931
Validation loss: 4.2860273543935845

Epoch: 5| Step: 2
Training loss: 4.00771184426782
Validation loss: 4.281146519285132

Epoch: 5| Step: 3
Training loss: 3.7895642135218877
Validation loss: 4.277004633775754

Epoch: 5| Step: 4
Training loss: 4.11428504860585
Validation loss: 4.272130390859165

Epoch: 5| Step: 5
Training loss: 3.6860959240938227
Validation loss: 4.2670817653202535

Epoch: 5| Step: 6
Training loss: 4.869370168576532
Validation loss: 4.262138245819587

Epoch: 5| Step: 7
Training loss: 4.384225111103341
Validation loss: 4.257842238202783

Epoch: 5| Step: 8
Training loss: 4.344210812941779
Validation loss: 4.253066634824795

Epoch: 5| Step: 9
Training loss: 4.861375403107286
Validation loss: 4.24851945258119

Epoch: 5| Step: 10
Training loss: 4.609522930293053
Validation loss: 4.2441217264362345

Epoch: 5| Step: 11
Training loss: 5.100782723294415
Validation loss: 4.239482487909262

Epoch: 27| Step: 0
Training loss: 4.186524690379157
Validation loss: 4.234137604687593

Epoch: 5| Step: 1
Training loss: 4.476702250218276
Validation loss: 4.229694012553633

Epoch: 5| Step: 2
Training loss: 4.5393135671070866
Validation loss: 4.224761456935337

Epoch: 5| Step: 3
Training loss: 4.618855699457617
Validation loss: 4.219760731313979

Epoch: 5| Step: 4
Training loss: 5.068963151958504
Validation loss: 4.214849764890625

Epoch: 5| Step: 5
Training loss: 3.5867416377782044
Validation loss: 4.209874459521958

Epoch: 5| Step: 6
Training loss: 4.21757208309746
Validation loss: 4.204763308261887

Epoch: 5| Step: 7
Training loss: 4.582456030835008
Validation loss: 4.200073084308899

Epoch: 5| Step: 8
Training loss: 4.799436997138514
Validation loss: 4.195197039293447

Epoch: 5| Step: 9
Training loss: 3.619274221343587
Validation loss: 4.190228678685004

Epoch: 5| Step: 10
Training loss: 3.7748272786826575
Validation loss: 4.185653848923173

Epoch: 5| Step: 11
Training loss: 4.4357165124017515
Validation loss: 4.180771826680127

Epoch: 28| Step: 0
Training loss: 4.480495782053915
Validation loss: 4.176223669592344

Epoch: 5| Step: 1
Training loss: 4.41886966872411
Validation loss: 4.171499914102326

Epoch: 5| Step: 2
Training loss: 4.3785346874863835
Validation loss: 4.167043223849634

Epoch: 5| Step: 3
Training loss: 4.275448397493974
Validation loss: 4.1622585852860166

Epoch: 5| Step: 4
Training loss: 4.033302435218012
Validation loss: 4.157471771213216

Epoch: 5| Step: 5
Training loss: 4.081999476550983
Validation loss: 4.152833456958285

Epoch: 5| Step: 6
Training loss: 4.098989613408416
Validation loss: 4.148260822832323

Epoch: 5| Step: 7
Training loss: 4.5844670136311665
Validation loss: 4.143270499140952

Epoch: 5| Step: 8
Training loss: 2.903425094812843
Validation loss: 4.138709492532705

Epoch: 5| Step: 9
Training loss: 4.51621841702582
Validation loss: 4.134413261938144

Epoch: 5| Step: 10
Training loss: 5.046474381835643
Validation loss: 4.129829123150954

Epoch: 5| Step: 11
Training loss: 4.210672968268033
Validation loss: 4.125242505507156

Epoch: 29| Step: 0
Training loss: 4.572860868051278
Validation loss: 4.1205804104357036

Epoch: 5| Step: 1
Training loss: 3.5672947256353655
Validation loss: 4.115787140074119

Epoch: 5| Step: 2
Training loss: 4.576847666370078
Validation loss: 4.111035329461072

Epoch: 5| Step: 3
Training loss: 4.0085124990242935
Validation loss: 4.106698855791538

Epoch: 5| Step: 4
Training loss: 3.6102159189338305
Validation loss: 4.102163690981432

Epoch: 5| Step: 5
Training loss: 4.000934968396498
Validation loss: 4.097689526340682

Epoch: 5| Step: 6
Training loss: 4.564160606533947
Validation loss: 4.0931259630891095

Epoch: 5| Step: 7
Training loss: 4.220385877133073
Validation loss: 4.0884961394996635

Epoch: 5| Step: 8
Training loss: 4.079492332439134
Validation loss: 4.083756831972689

Epoch: 5| Step: 9
Training loss: 4.718811034761522
Validation loss: 4.079329024263718

Epoch: 5| Step: 10
Training loss: 4.739609446471104
Validation loss: 4.074743332804954

Epoch: 5| Step: 11
Training loss: 1.8017826895301574
Validation loss: 4.069917075095785

Epoch: 30| Step: 0
Training loss: 4.775476897226838
Validation loss: 4.065447950600856

Epoch: 5| Step: 1
Training loss: 2.699361927082956
Validation loss: 4.060792676694866

Epoch: 5| Step: 2
Training loss: 3.6906308606106886
Validation loss: 4.056592935800059

Epoch: 5| Step: 3
Training loss: 4.557029056793789
Validation loss: 4.052348938043795

Epoch: 5| Step: 4
Training loss: 4.024259672140142
Validation loss: 4.048095278579533

Epoch: 5| Step: 5
Training loss: 4.271788674789012
Validation loss: 4.043323446001216

Epoch: 5| Step: 6
Training loss: 4.153360380530628
Validation loss: 4.038909879711816

Epoch: 5| Step: 7
Training loss: 4.232582979960461
Validation loss: 4.0344359354134705

Epoch: 5| Step: 8
Training loss: 4.2460843167731
Validation loss: 4.03014186542382

Epoch: 5| Step: 9
Training loss: 3.9375081137921732
Validation loss: 4.02557930341438

Epoch: 5| Step: 10
Training loss: 4.8451829328930645
Validation loss: 4.021308632003136

Epoch: 5| Step: 11
Training loss: 4.550289379601077
Validation loss: 4.016057823594429

Epoch: 31| Step: 0
Training loss: 3.459425757500119
Validation loss: 4.011158801023071

Epoch: 5| Step: 1
Training loss: 4.538136688204761
Validation loss: 4.006305135981323

Epoch: 5| Step: 2
Training loss: 4.330405395418756
Validation loss: 4.001491387810449

Epoch: 5| Step: 3
Training loss: 3.9249315243718965
Validation loss: 3.996933874552132

Epoch: 5| Step: 4
Training loss: 4.376601879592337
Validation loss: 3.9918157276256245

Epoch: 5| Step: 5
Training loss: 3.65682418096839
Validation loss: 3.9870071293875804

Epoch: 5| Step: 6
Training loss: 4.0293517855565275
Validation loss: 3.981952010283068

Epoch: 5| Step: 7
Training loss: 4.617200719101212
Validation loss: 3.9772850484368183

Epoch: 5| Step: 8
Training loss: 4.122833145218291
Validation loss: 3.972602321242461

Epoch: 5| Step: 9
Training loss: 4.205967052176484
Validation loss: 3.967695516637544

Epoch: 5| Step: 10
Training loss: 3.8916961543554125
Validation loss: 3.965223678150041

Epoch: 5| Step: 11
Training loss: 4.214704594334564
Validation loss: 3.9602148988241117

Epoch: 32| Step: 0
Training loss: 4.2094026220558085
Validation loss: 3.955798294370792

Epoch: 5| Step: 1
Training loss: 3.9369820148114383
Validation loss: 3.948962219942086

Epoch: 5| Step: 2
Training loss: 4.080809195812389
Validation loss: 3.9440849685885326

Epoch: 5| Step: 3
Training loss: 3.412318152867871
Validation loss: 3.9400013542334333

Epoch: 5| Step: 4
Training loss: 3.488390879173852
Validation loss: 3.9355846923828577

Epoch: 5| Step: 5
Training loss: 4.000818645628052
Validation loss: 3.931058857995919

Epoch: 5| Step: 6
Training loss: 4.066542037763468
Validation loss: 3.9260218507032665

Epoch: 5| Step: 7
Training loss: 4.003889100573029
Validation loss: 3.921537073174386

Epoch: 5| Step: 8
Training loss: 4.5612274903661705
Validation loss: 3.9171290462896744

Epoch: 5| Step: 9
Training loss: 4.723928073616506
Validation loss: 3.9123875935975216

Epoch: 5| Step: 10
Training loss: 4.198887159873156
Validation loss: 3.9077607348812844

Epoch: 5| Step: 11
Training loss: 3.1795913719249262
Validation loss: 3.9028876520319598

Epoch: 33| Step: 0
Training loss: 3.875989479971861
Validation loss: 3.899307801081908

Epoch: 5| Step: 1
Training loss: 4.011632218212335
Validation loss: 3.894435655650768

Epoch: 5| Step: 2
Training loss: 3.505582988757284
Validation loss: 3.88965703420721

Epoch: 5| Step: 3
Training loss: 3.759010171667952
Validation loss: 3.8848456030415983

Epoch: 5| Step: 4
Training loss: 4.101396481015024
Validation loss: 3.8806185187651576

Epoch: 5| Step: 5
Training loss: 4.495922360438818
Validation loss: 3.875881422968367

Epoch: 5| Step: 6
Training loss: 4.314422400085822
Validation loss: 3.8712622756278896

Epoch: 5| Step: 7
Training loss: 3.0162191645677963
Validation loss: 3.8667070238018812

Epoch: 5| Step: 8
Training loss: 4.387769754925754
Validation loss: 3.8618352258274893

Epoch: 5| Step: 9
Training loss: 4.778363322569818
Validation loss: 3.8571057411414658

Epoch: 5| Step: 10
Training loss: 3.9535367849232426
Validation loss: 3.852632220567802

Epoch: 5| Step: 11
Training loss: 0.7560143915552451
Validation loss: 3.8479752306706043

Epoch: 34| Step: 0
Training loss: 4.022054193709203
Validation loss: 3.8438715049747034

Epoch: 5| Step: 1
Training loss: 3.1476304603063614
Validation loss: 3.839689146223502

Epoch: 5| Step: 2
Training loss: 3.8092938058538364
Validation loss: 3.835188596382741

Epoch: 5| Step: 3
Training loss: 3.952171844043611
Validation loss: 3.8307756187307436

Epoch: 5| Step: 4
Training loss: 4.45813135182741
Validation loss: 3.8266547149766192

Epoch: 5| Step: 5
Training loss: 4.340367159452922
Validation loss: 3.8220364322083733

Epoch: 5| Step: 6
Training loss: 3.6065890306541935
Validation loss: 3.817285770208441

Epoch: 5| Step: 7
Training loss: 3.974806842069523
Validation loss: 3.8129783809684934

Epoch: 5| Step: 8
Training loss: 3.452266534622467
Validation loss: 3.8084281924528325

Epoch: 5| Step: 9
Training loss: 4.172755002137237
Validation loss: 3.8038581555221613

Epoch: 5| Step: 10
Training loss: 4.342767618088354
Validation loss: 3.7995282179359453

Epoch: 5| Step: 11
Training loss: 4.116257622251569
Validation loss: 3.7948021273486687

Epoch: 35| Step: 0
Training loss: 3.598232359842466
Validation loss: 3.7899684858479765

Epoch: 5| Step: 1
Training loss: 4.210861856492432
Validation loss: 3.785440802335381

Epoch: 5| Step: 2
Training loss: 4.855776322091613
Validation loss: 3.7806629849698883

Epoch: 5| Step: 3
Training loss: 3.849126513587536
Validation loss: 3.775688047841956

Epoch: 5| Step: 4
Training loss: 3.8587660830366164
Validation loss: 3.7710505610889853

Epoch: 5| Step: 5
Training loss: 3.7128487574079476
Validation loss: 3.765873796572453

Epoch: 5| Step: 6
Training loss: 3.710708000715921
Validation loss: 3.760983863738137

Epoch: 5| Step: 7
Training loss: 4.074524440982778
Validation loss: 3.756344794771417

Epoch: 5| Step: 8
Training loss: 3.7665207933069444
Validation loss: 3.7515175344962457

Epoch: 5| Step: 9
Training loss: 3.8040120427996986
Validation loss: 3.7468330573968234

Epoch: 5| Step: 10
Training loss: 3.6384595726108375
Validation loss: 3.7422240487094487

Epoch: 5| Step: 11
Training loss: 1.7872597219459139
Validation loss: 3.737366136813747

Epoch: 36| Step: 0
Training loss: 4.275247863365689
Validation loss: 3.733888698181769

Epoch: 5| Step: 1
Training loss: 4.009417653024169
Validation loss: 3.729336711622428

Epoch: 5| Step: 2
Training loss: 3.3329389974675268
Validation loss: 3.7241309848412474

Epoch: 5| Step: 3
Training loss: 3.9899760053864446
Validation loss: 3.719999926073149

Epoch: 5| Step: 4
Training loss: 3.8610319835382447
Validation loss: 3.715238050840627

Epoch: 5| Step: 5
Training loss: 3.6300784087316957
Validation loss: 3.7110198483616035

Epoch: 5| Step: 6
Training loss: 3.872319063179452
Validation loss: 3.7064789553894397

Epoch: 5| Step: 7
Training loss: 3.940772014056695
Validation loss: 3.702060052730167

Epoch: 5| Step: 8
Training loss: 3.8299553679811384
Validation loss: 3.6977360838017255

Epoch: 5| Step: 9
Training loss: 4.263740318821921
Validation loss: 3.6931580815297163

Epoch: 5| Step: 10
Training loss: 3.214086023818569
Validation loss: 3.6886971464715077

Epoch: 5| Step: 11
Training loss: 3.650681126415669
Validation loss: 3.684452370782212

Epoch: 37| Step: 0
Training loss: 4.1810898184208085
Validation loss: 3.6794592914156636

Epoch: 5| Step: 1
Training loss: 4.42425235962534
Validation loss: 3.6752966555624247

Epoch: 5| Step: 2
Training loss: 3.59749547410671
Validation loss: 3.6703458483249953

Epoch: 5| Step: 3
Training loss: 3.430076629183067
Validation loss: 3.6655134081623255

Epoch: 5| Step: 4
Training loss: 3.790720280065415
Validation loss: 3.6611463903143298

Epoch: 5| Step: 5
Training loss: 3.398214118017658
Validation loss: 3.6565291686680124

Epoch: 5| Step: 6
Training loss: 3.846798192116605
Validation loss: 3.6517096038643317

Epoch: 5| Step: 7
Training loss: 3.8134924425485623
Validation loss: 3.6472499747252227

Epoch: 5| Step: 8
Training loss: 4.05313462820211
Validation loss: 3.642741263574087

Epoch: 5| Step: 9
Training loss: 3.3684722502795417
Validation loss: 3.638078696851829

Epoch: 5| Step: 10
Training loss: 3.7902430003025893
Validation loss: 3.6334535094751903

Epoch: 5| Step: 11
Training loss: 3.2586561981268187
Validation loss: 3.6291388747461015

Epoch: 38| Step: 0
Training loss: 3.8516770165927627
Validation loss: 3.625389100162029

Epoch: 5| Step: 1
Training loss: 3.562246748639594
Validation loss: 3.620457466789155

Epoch: 5| Step: 2
Training loss: 3.796862190130783
Validation loss: 3.616287385848611

Epoch: 5| Step: 3
Training loss: 3.2536424619216433
Validation loss: 3.6121655695460957

Epoch: 5| Step: 4
Training loss: 3.802613117810261
Validation loss: 3.607598251384773

Epoch: 5| Step: 5
Training loss: 3.2619161032173576
Validation loss: 3.60383641194402

Epoch: 5| Step: 6
Training loss: 4.38016706243872
Validation loss: 3.5994816614443272

Epoch: 5| Step: 7
Training loss: 3.400026147405274
Validation loss: 3.595060750786671

Epoch: 5| Step: 8
Training loss: 3.785467900692443
Validation loss: 3.590811166344459

Epoch: 5| Step: 9
Training loss: 4.344508701726237
Validation loss: 3.586467036180281

Epoch: 5| Step: 10
Training loss: 3.680151519351248
Validation loss: 3.5822097639545474

Epoch: 5| Step: 11
Training loss: 2.7504873710859714
Validation loss: 3.577807744160992

Epoch: 39| Step: 0
Training loss: 3.293266889259667
Validation loss: 3.573490267309657

Epoch: 5| Step: 1
Training loss: 3.4854534857873105
Validation loss: 3.5689990899250597

Epoch: 5| Step: 2
Training loss: 3.3283000774629152
Validation loss: 3.564510999235856

Epoch: 5| Step: 3
Training loss: 3.5908209764668273
Validation loss: 3.560068100744911

Epoch: 5| Step: 4
Training loss: 3.6648445514848036
Validation loss: 3.5557815991924904

Epoch: 5| Step: 5
Training loss: 3.7515570904662163
Validation loss: 3.5515352350193434

Epoch: 5| Step: 6
Training loss: 4.565149243558944
Validation loss: 3.547089109526232

Epoch: 5| Step: 7
Training loss: 3.625610694745242
Validation loss: 3.542695387865504

Epoch: 5| Step: 8
Training loss: 3.5136760819787907
Validation loss: 3.538278376355685

Epoch: 5| Step: 9
Training loss: 3.309449284542002
Validation loss: 3.5334851637568794

Epoch: 5| Step: 10
Training loss: 3.9953611416442247
Validation loss: 3.529604373632452

Epoch: 5| Step: 11
Training loss: 4.789017495072996
Validation loss: 3.5248831907718037

Epoch: 40| Step: 0
Training loss: 3.4915376267605183
Validation loss: 3.5199930083591933

Epoch: 5| Step: 1
Training loss: 4.023107778130982
Validation loss: 3.515243803479989

Epoch: 5| Step: 2
Training loss: 4.024115821915835
Validation loss: 3.5153514162212587

Epoch: 5| Step: 3
Training loss: 3.1441340793916366
Validation loss: 3.5056976762777055

Epoch: 5| Step: 4
Training loss: 3.4635009916206925
Validation loss: 3.5020324561882434

Epoch: 5| Step: 5
Training loss: 3.350358084144468
Validation loss: 3.5046903653585937

Epoch: 5| Step: 6
Training loss: 4.184110265879834
Validation loss: 3.516185438908453

Epoch: 5| Step: 7
Training loss: 3.57409000685996
Validation loss: 3.492122518687269

Epoch: 5| Step: 8
Training loss: 3.0905864223313553
Validation loss: 3.48791459831116

Epoch: 5| Step: 9
Training loss: 3.8248787954237997
Validation loss: 3.4935321992466095

Epoch: 5| Step: 10
Training loss: 3.826420992520231
Validation loss: 3.4821951052946023

Epoch: 5| Step: 11
Training loss: 3.1502985979274274
Validation loss: 3.4850231816521

Epoch: 41| Step: 0
Training loss: 3.056855585991657
Validation loss: 3.4794161739463374

Epoch: 5| Step: 1
Training loss: 2.7172628259555283
Validation loss: 3.4740363840984387

Epoch: 5| Step: 2
Training loss: 3.6266883338486675
Validation loss: 3.4684127924373316

Epoch: 5| Step: 3
Training loss: 4.040467126864353
Validation loss: 3.462606660460176

Epoch: 5| Step: 4
Training loss: 3.645190605548803
Validation loss: 3.4582516917201556

Epoch: 5| Step: 5
Training loss: 3.39209093319266
Validation loss: 3.450303013482216

Epoch: 5| Step: 6
Training loss: 3.7771435342330113
Validation loss: 3.445682030061733

Epoch: 5| Step: 7
Training loss: 4.028226206287148
Validation loss: 3.4406277934013114

Epoch: 5| Step: 8
Training loss: 3.344323224996644
Validation loss: 3.4356408090998576

Epoch: 5| Step: 9
Training loss: 4.074092815497348
Validation loss: 3.4305661407664325

Epoch: 5| Step: 10
Training loss: 3.7799701889315775
Validation loss: 3.4252983805585813

Epoch: 5| Step: 11
Training loss: 1.7594863720837919
Validation loss: 3.4204400071469094

Epoch: 42| Step: 0
Training loss: 3.737233492430051
Validation loss: 3.415702664445431

Epoch: 5| Step: 1
Training loss: 4.054376079382179
Validation loss: 3.4108855176696324

Epoch: 5| Step: 2
Training loss: 3.607742799240094
Validation loss: 3.4063329657386974

Epoch: 5| Step: 3
Training loss: 3.032537599788075
Validation loss: 3.402025093254436

Epoch: 5| Step: 4
Training loss: 3.4433236469183024
Validation loss: 3.3976062152587887

Epoch: 5| Step: 5
Training loss: 3.217816560077134
Validation loss: 3.3932536727633202

Epoch: 5| Step: 6
Training loss: 3.858699229764944
Validation loss: 3.389278437463882

Epoch: 5| Step: 7
Training loss: 3.314447082621923
Validation loss: 3.3849719704261516

Epoch: 5| Step: 8
Training loss: 3.5805396049492124
Validation loss: 3.3808931069515835

Epoch: 5| Step: 9
Training loss: 3.792727744620224
Validation loss: 3.3768411841738923

Epoch: 5| Step: 10
Training loss: 3.1563143581967226
Validation loss: 3.3725388406996424

Epoch: 5| Step: 11
Training loss: 3.1735297335693318
Validation loss: 3.36855636469533

Epoch: 43| Step: 0
Training loss: 3.3515754468263195
Validation loss: 3.364479084978438

Epoch: 5| Step: 1
Training loss: 3.359624667094162
Validation loss: 3.360028982453765

Epoch: 5| Step: 2
Training loss: 3.571329785751889
Validation loss: 3.355986555570802

Epoch: 5| Step: 3
Training loss: 2.6240060831964738
Validation loss: 3.3516198652051683

Epoch: 5| Step: 4
Training loss: 3.7212627480634195
Validation loss: 3.34781246347557

Epoch: 5| Step: 5
Training loss: 3.824000912502611
Validation loss: 3.3435461570425473

Epoch: 5| Step: 6
Training loss: 3.008077238247823
Validation loss: 3.339310916983295

Epoch: 5| Step: 7
Training loss: 3.767147278857766
Validation loss: 3.33561730020659

Epoch: 5| Step: 8
Training loss: 4.004444275970211
Validation loss: 3.331161191638037

Epoch: 5| Step: 9
Training loss: 3.898013166371124
Validation loss: 3.3272818331582785

Epoch: 5| Step: 10
Training loss: 3.2569917152587538
Validation loss: 3.3229233623106333

Epoch: 5| Step: 11
Training loss: 1.0011403615027568
Validation loss: 3.3189336628929142

Epoch: 44| Step: 0
Training loss: 3.334116954882574
Validation loss: 3.3152049734286795

Epoch: 5| Step: 1
Training loss: 3.198558697363316
Validation loss: 3.31164510959571

Epoch: 5| Step: 2
Training loss: 3.9408781304612743
Validation loss: 3.3080126880054936

Epoch: 5| Step: 3
Training loss: 3.6983586150226833
Validation loss: 3.3039952388002516

Epoch: 5| Step: 4
Training loss: 2.9945893769428595
Validation loss: 3.300366401560894

Epoch: 5| Step: 5
Training loss: 3.698307428623111
Validation loss: 3.2964054273450114

Epoch: 5| Step: 6
Training loss: 3.348575283389614
Validation loss: 3.2928659853694855

Epoch: 5| Step: 7
Training loss: 3.5512074633329767
Validation loss: 3.288730033689211

Epoch: 5| Step: 8
Training loss: 3.4147183205201705
Validation loss: 3.2849946951859605

Epoch: 5| Step: 9
Training loss: 3.243677591974688
Validation loss: 3.281476439882441

Epoch: 5| Step: 10
Training loss: 3.2002194508168076
Validation loss: 3.2772682069082624

Epoch: 5| Step: 11
Training loss: 3.756565450158328
Validation loss: 3.273620139089645

Epoch: 45| Step: 0
Training loss: 3.1866025970381364
Validation loss: 3.269832935945775

Epoch: 5| Step: 1
Training loss: 3.4890330018275546
Validation loss: 3.2657758504531715

Epoch: 5| Step: 2
Training loss: 4.108667813214013
Validation loss: 3.2619448098063515

Epoch: 5| Step: 3
Training loss: 3.2992886210004553
Validation loss: 3.2588194305648037

Epoch: 5| Step: 4
Training loss: 3.1029486261283155
Validation loss: 3.2553752622400784

Epoch: 5| Step: 5
Training loss: 2.748457042284134
Validation loss: 3.2507462225599686

Epoch: 5| Step: 6
Training loss: 3.1488095955776143
Validation loss: 3.2490713797998256

Epoch: 5| Step: 7
Training loss: 3.363731079501975
Validation loss: 3.245150775335727

Epoch: 5| Step: 8
Training loss: 3.877631862619183
Validation loss: 3.239843969296792

Epoch: 5| Step: 9
Training loss: 3.2471409473050348
Validation loss: 3.2358971053395393

Epoch: 5| Step: 10
Training loss: 3.3796613081130116
Validation loss: 3.231593331814334

Epoch: 5| Step: 11
Training loss: 3.99677742845684
Validation loss: 3.2276032918982525

Epoch: 46| Step: 0
Training loss: 3.974956675359354
Validation loss: 3.223473986519179

Epoch: 5| Step: 1
Training loss: 2.929420886306025
Validation loss: 3.2194125657089097

Epoch: 5| Step: 2
Training loss: 3.0849329605113964
Validation loss: 3.2153145996486323

Epoch: 5| Step: 3
Training loss: 3.5156199815502376
Validation loss: 3.211579053756378

Epoch: 5| Step: 4
Training loss: 2.9441015855636103
Validation loss: 3.207862817438085

Epoch: 5| Step: 5
Training loss: 3.69048020791862
Validation loss: 3.203908554630711

Epoch: 5| Step: 6
Training loss: 2.8936365883933104
Validation loss: 3.199812837432111

Epoch: 5| Step: 7
Training loss: 3.6881658955000236
Validation loss: 3.1964573321021517

Epoch: 5| Step: 8
Training loss: 3.2810930487016488
Validation loss: 3.1928801819611268

Epoch: 5| Step: 9
Training loss: 3.6541961378752648
Validation loss: 3.188669794136405

Epoch: 5| Step: 10
Training loss: 2.933024791905387
Validation loss: 3.1855494259267227

Epoch: 5| Step: 11
Training loss: 3.2747650003932027
Validation loss: 3.1811795369896503

Epoch: 47| Step: 0
Training loss: 3.1555682664116738
Validation loss: 3.1775461620636274

Epoch: 5| Step: 1
Training loss: 3.106091015928374
Validation loss: 3.1741139726295136

Epoch: 5| Step: 2
Training loss: 4.07052992461516
Validation loss: 3.1701583883756594

Epoch: 5| Step: 3
Training loss: 3.6936404294255833
Validation loss: 3.1680403896122824

Epoch: 5| Step: 4
Training loss: 3.3172173648303827
Validation loss: 3.163597273670221

Epoch: 5| Step: 5
Training loss: 3.4678515396196365
Validation loss: 3.1610563620726024

Epoch: 5| Step: 6
Training loss: 3.3358437303858657
Validation loss: 3.1596232099472044

Epoch: 5| Step: 7
Training loss: 3.1443153070263166
Validation loss: 3.1508784270235703

Epoch: 5| Step: 8
Training loss: 2.315341132058145
Validation loss: 3.1473649361748977

Epoch: 5| Step: 9
Training loss: 3.4497751411468975
Validation loss: 3.143759392256007

Epoch: 5| Step: 10
Training loss: 2.818997232290305
Validation loss: 3.1400818236435555

Epoch: 5| Step: 11
Training loss: 3.886733226653316
Validation loss: 3.136758046361299

Epoch: 48| Step: 0
Training loss: 3.7100865080996437
Validation loss: 3.133902001180953

Epoch: 5| Step: 1
Training loss: 3.1882969477070064
Validation loss: 3.1301368201661406

Epoch: 5| Step: 2
Training loss: 3.2758238476913735
Validation loss: 3.1261374501586694

Epoch: 5| Step: 3
Training loss: 3.1426765990728134
Validation loss: 3.1229581520085365

Epoch: 5| Step: 4
Training loss: 3.6852226337068634
Validation loss: 3.1188931431662223

Epoch: 5| Step: 5
Training loss: 2.896585947572135
Validation loss: 3.115880502563291

Epoch: 5| Step: 6
Training loss: 2.885983261133876
Validation loss: 3.1117660811292653

Epoch: 5| Step: 7
Training loss: 3.725473176742117
Validation loss: 3.1089056897262344

Epoch: 5| Step: 8
Training loss: 3.0973363569258296
Validation loss: 3.1056983590963703

Epoch: 5| Step: 9
Training loss: 3.0246655239848295
Validation loss: 3.101695041082582

Epoch: 5| Step: 10
Training loss: 3.1443201598402992
Validation loss: 3.098346224472974

Epoch: 5| Step: 11
Training loss: 2.870528018774139
Validation loss: 3.0955985291276864

Epoch: 49| Step: 0
Training loss: 3.014633885513145
Validation loss: 3.0917748719342355

Epoch: 5| Step: 1
Training loss: 2.795764724033637
Validation loss: 3.089126613005

Epoch: 5| Step: 2
Training loss: 3.5589871403718267
Validation loss: 3.0850388292777944

Epoch: 5| Step: 3
Training loss: 3.200321813137173
Validation loss: 3.082977742662823

Epoch: 5| Step: 4
Training loss: 3.8027479174450693
Validation loss: 3.0798645429270732

Epoch: 5| Step: 5
Training loss: 3.1079394223018175
Validation loss: 3.075450316074801

Epoch: 5| Step: 6
Training loss: 3.192845667778503
Validation loss: 3.072780290368421

Epoch: 5| Step: 7
Training loss: 3.37180685941416
Validation loss: 3.0697403197475674

Epoch: 5| Step: 8
Training loss: 3.0914075826854597
Validation loss: 3.066875193510172

Epoch: 5| Step: 9
Training loss: 3.0656102793551065
Validation loss: 3.062887965170648

Epoch: 5| Step: 10
Training loss: 3.1305267290534924
Validation loss: 3.0594698388918142

Epoch: 5| Step: 11
Training loss: 2.95502794100853
Validation loss: 3.056384182086141

Epoch: 50| Step: 0
Training loss: 3.012400746590108
Validation loss: 3.0530235330932705

Epoch: 5| Step: 1
Training loss: 3.270472237321444
Validation loss: 3.0497826160027603

Epoch: 5| Step: 2
Training loss: 3.7103330099765808
Validation loss: 3.0471698748714195

Epoch: 5| Step: 3
Training loss: 3.217707344730942
Validation loss: 3.043289761797164

Epoch: 5| Step: 4
Training loss: 3.477105327618159
Validation loss: 3.040435529884464

Epoch: 5| Step: 5
Training loss: 3.4783271340091177
Validation loss: 3.036657324672598

Epoch: 5| Step: 6
Training loss: 2.9622117171880693
Validation loss: 3.032904858159813

Epoch: 5| Step: 7
Training loss: 2.8322418765964157
Validation loss: 3.0297676440441634

Epoch: 5| Step: 8
Training loss: 2.769917578176755
Validation loss: 3.026514299856053

Epoch: 5| Step: 9
Training loss: 3.187604865050843
Validation loss: 3.023812959868957

Epoch: 5| Step: 10
Training loss: 3.0071018717147107
Validation loss: 3.0209670409943064

Epoch: 5| Step: 11
Training loss: 2.757972345231505
Validation loss: 3.018044988077592

Epoch: 51| Step: 0
Training loss: 2.994494154153221
Validation loss: 3.0146594733689205

Epoch: 5| Step: 1
Training loss: 3.427150752709795
Validation loss: 3.012246039147213

Epoch: 5| Step: 2
Training loss: 3.067528469510085
Validation loss: 3.008901829821039

Epoch: 5| Step: 3
Training loss: 3.116449177815238
Validation loss: 3.0061420838991255

Epoch: 5| Step: 4
Training loss: 3.1627084354271378
Validation loss: 3.002512889355837

Epoch: 5| Step: 5
Training loss: 2.448776180795291
Validation loss: 3.0006694113076056

Epoch: 5| Step: 6
Training loss: 3.6322782390241635
Validation loss: 2.998047034049263

Epoch: 5| Step: 7
Training loss: 2.9591466475203223
Validation loss: 2.9955630771214814

Epoch: 5| Step: 8
Training loss: 3.2443375777436256
Validation loss: 2.9932820282881467

Epoch: 5| Step: 9
Training loss: 3.161680026296792
Validation loss: 2.9883061526389105

Epoch: 5| Step: 10
Training loss: 3.14903764707921
Validation loss: 2.985750064659379

Epoch: 5| Step: 11
Training loss: 3.397977249724965
Validation loss: 2.983730862458501

Epoch: 52| Step: 0
Training loss: 3.14182345929128
Validation loss: 2.980181605621312

Epoch: 5| Step: 1
Training loss: 3.160506598027721
Validation loss: 2.9779947794797392

Epoch: 5| Step: 2
Training loss: 2.9916810087605783
Validation loss: 2.976014271174885

Epoch: 5| Step: 3
Training loss: 3.2773798325635877
Validation loss: 2.9741744660485177

Epoch: 5| Step: 4
Training loss: 3.0463727488184458
Validation loss: 2.9710578715006837

Epoch: 5| Step: 5
Training loss: 3.0795325144918504
Validation loss: 2.968660667815431

Epoch: 5| Step: 6
Training loss: 3.0885505526041195
Validation loss: 2.9662766945462016

Epoch: 5| Step: 7
Training loss: 3.1958890735153567
Validation loss: 2.9633875886701193

Epoch: 5| Step: 8
Training loss: 3.477798208014862
Validation loss: 2.960741806609034

Epoch: 5| Step: 9
Training loss: 2.8765050846070164
Validation loss: 2.9579715451569273

Epoch: 5| Step: 10
Training loss: 2.8481250685758166
Validation loss: 2.9546984193069905

Epoch: 5| Step: 11
Training loss: 3.1110700271557996
Validation loss: 2.951785650075544

Epoch: 53| Step: 0
Training loss: 3.1404189616319726
Validation loss: 2.9487215376662523

Epoch: 5| Step: 1
Training loss: 3.297987040774746
Validation loss: 2.945651681596933

Epoch: 5| Step: 2
Training loss: 3.1136589251981053
Validation loss: 2.942624468438257

Epoch: 5| Step: 3
Training loss: 2.0450125804411563
Validation loss: 2.939245949643778

Epoch: 5| Step: 4
Training loss: 3.4588545134607913
Validation loss: 2.9358154431210446

Epoch: 5| Step: 5
Training loss: 3.403365007104297
Validation loss: 2.934239312671765

Epoch: 5| Step: 6
Training loss: 2.8920607841509196
Validation loss: 2.9301837890732556

Epoch: 5| Step: 7
Training loss: 3.5269909045386325
Validation loss: 2.92827488730358

Epoch: 5| Step: 8
Training loss: 2.9396095006162435
Validation loss: 2.9257381692387634

Epoch: 5| Step: 9
Training loss: 3.270877100819747
Validation loss: 2.9222400865584914

Epoch: 5| Step: 10
Training loss: 2.455906451323044
Validation loss: 2.918973151531855

Epoch: 5| Step: 11
Training loss: 2.940934466335202
Validation loss: 2.916208051774153

Epoch: 54| Step: 0
Training loss: 3.016981857300888
Validation loss: 2.9141541876528905

Epoch: 5| Step: 1
Training loss: 3.613525910213483
Validation loss: 2.9108989134910184

Epoch: 5| Step: 2
Training loss: 3.1525375136488667
Validation loss: 2.9089129051258995

Epoch: 5| Step: 3
Training loss: 3.017124732069383
Validation loss: 2.9056292694765

Epoch: 5| Step: 4
Training loss: 2.9237725119439646
Validation loss: 2.9027650269822014

Epoch: 5| Step: 5
Training loss: 2.981263141083295
Validation loss: 2.9014252227042903

Epoch: 5| Step: 6
Training loss: 2.97079718426455
Validation loss: 2.898812344313697

Epoch: 5| Step: 7
Training loss: 2.8887714113492624
Validation loss: 2.8958382892337537

Epoch: 5| Step: 8
Training loss: 2.7598598758399384
Validation loss: 2.893506831943789

Epoch: 5| Step: 9
Training loss: 2.9664377945756613
Validation loss: 2.8906606586078314

Epoch: 5| Step: 10
Training loss: 3.240187577363234
Validation loss: 2.8896493278967013

Epoch: 5| Step: 11
Training loss: 2.2956531414713472
Validation loss: 2.8871472169342494

Epoch: 55| Step: 0
Training loss: 3.009164323097175
Validation loss: 2.8847383994176043

Epoch: 5| Step: 1
Training loss: 2.860214860488061
Validation loss: 2.8822149572353997

Epoch: 5| Step: 2
Training loss: 2.799370105600305
Validation loss: 2.879783931365699

Epoch: 5| Step: 3
Training loss: 3.501915271424668
Validation loss: 2.8778455998330195

Epoch: 5| Step: 4
Training loss: 3.2818419876138214
Validation loss: 2.8753707238992368

Epoch: 5| Step: 5
Training loss: 3.336301546981348
Validation loss: 2.8740563848198915

Epoch: 5| Step: 6
Training loss: 3.367325994559733
Validation loss: 2.8762113880084015

Epoch: 5| Step: 7
Training loss: 2.505956610727942
Validation loss: 2.8673157901456365

Epoch: 5| Step: 8
Training loss: 2.7051407018371227
Validation loss: 2.865478428954418

Epoch: 5| Step: 9
Training loss: 2.776507330814894
Validation loss: 2.8632371941577675

Epoch: 5| Step: 10
Training loss: 2.8023119883974665
Validation loss: 2.861383920587742

Epoch: 5| Step: 11
Training loss: 3.252067641799349
Validation loss: 2.8591983685023363

Epoch: 56| Step: 0
Training loss: 2.6932612035396684
Validation loss: 2.8578776415225495

Epoch: 5| Step: 1
Training loss: 2.9385750609218535
Validation loss: 2.8560796693839166

Epoch: 5| Step: 2
Training loss: 2.609058018236312
Validation loss: 2.8547541241178256

Epoch: 5| Step: 3
Training loss: 3.0583510028491525
Validation loss: 2.8528197491162626

Epoch: 5| Step: 4
Training loss: 3.092861163452314
Validation loss: 2.850626690041698

Epoch: 5| Step: 5
Training loss: 3.2418345063862954
Validation loss: 2.8480520544587966

Epoch: 5| Step: 6
Training loss: 2.9705987895673664
Validation loss: 2.8455870078192875

Epoch: 5| Step: 7
Training loss: 2.8941519999734284
Validation loss: 2.8441943530618365

Epoch: 5| Step: 8
Training loss: 3.7849532979404557
Validation loss: 2.841163311189808

Epoch: 5| Step: 9
Training loss: 2.879041733264713
Validation loss: 2.838808989696514

Epoch: 5| Step: 10
Training loss: 2.5299624712426065
Validation loss: 2.836653332703208

Epoch: 5| Step: 11
Training loss: 2.9217090253557445
Validation loss: 2.8347097669507755

Epoch: 57| Step: 0
Training loss: 2.6543800503777266
Validation loss: 2.832666519648344

Epoch: 5| Step: 1
Training loss: 3.089936343034255
Validation loss: 2.8286566296107263

Epoch: 5| Step: 2
Training loss: 3.498314315310455
Validation loss: 2.8264136291179263

Epoch: 5| Step: 3
Training loss: 3.14046208352903
Validation loss: 2.8242989436820674

Epoch: 5| Step: 4
Training loss: 3.2887117828619252
Validation loss: 2.822465351433805

Epoch: 5| Step: 5
Training loss: 2.773199538647074
Validation loss: 2.8202071236238218

Epoch: 5| Step: 6
Training loss: 3.3964141785802475
Validation loss: 2.8177181403552938

Epoch: 5| Step: 7
Training loss: 2.671482347336195
Validation loss: 2.814935975015765

Epoch: 5| Step: 8
Training loss: 2.2759294518597297
Validation loss: 2.812969535911295

Epoch: 5| Step: 9
Training loss: 2.792102940948851
Validation loss: 2.8099506197241233

Epoch: 5| Step: 10
Training loss: 2.881640188148096
Validation loss: 2.809749119805279

Epoch: 5| Step: 11
Training loss: 2.4873169568062132
Validation loss: 2.8100152130269476

Epoch: 58| Step: 0
Training loss: 2.66383123652559
Validation loss: 2.8146007462948086

Epoch: 5| Step: 1
Training loss: 2.927516285291505
Validation loss: 2.805281690386068

Epoch: 5| Step: 2
Training loss: 2.6828691373255453
Validation loss: 2.799085698828202

Epoch: 5| Step: 3
Training loss: 2.756430563342927
Validation loss: 2.7996354678973794

Epoch: 5| Step: 4
Training loss: 3.0336448411669528
Validation loss: 2.7977762031281417

Epoch: 5| Step: 5
Training loss: 3.194654326984517
Validation loss: 2.7959730977988975

Epoch: 5| Step: 6
Training loss: 3.239877489152075
Validation loss: 2.7941176676774786

Epoch: 5| Step: 7
Training loss: 3.0557135300306384
Validation loss: 2.79236363611616

Epoch: 5| Step: 8
Training loss: 2.745008186230847
Validation loss: 2.791709203301134

Epoch: 5| Step: 9
Training loss: 3.415750256586194
Validation loss: 2.788802461585233

Epoch: 5| Step: 10
Training loss: 2.4694029511936217
Validation loss: 2.7853469426671134

Epoch: 5| Step: 11
Training loss: 2.8914562782756525
Validation loss: 2.7855508425819

Epoch: 59| Step: 0
Training loss: 2.871263812990555
Validation loss: 2.7826436262315455

Epoch: 5| Step: 1
Training loss: 2.6504821986413982
Validation loss: 2.780694777542869

Epoch: 5| Step: 2
Training loss: 2.9722925833418166
Validation loss: 2.7801020571330852

Epoch: 5| Step: 3
Training loss: 2.586791758888086
Validation loss: 2.777784849926105

Epoch: 5| Step: 4
Training loss: 2.8833037167242677
Validation loss: 2.776460921290457

Epoch: 5| Step: 5
Training loss: 3.042962952046386
Validation loss: 2.774970026112105

Epoch: 5| Step: 6
Training loss: 2.7548837644838393
Validation loss: 2.7729917857606385

Epoch: 5| Step: 7
Training loss: 2.545758148764716
Validation loss: 2.772181238046889

Epoch: 5| Step: 8
Training loss: 3.6137000918562454
Validation loss: 2.7678907168969946

Epoch: 5| Step: 9
Training loss: 3.0723544021264253
Validation loss: 2.765799999986327

Epoch: 5| Step: 10
Training loss: 2.8054069085184916
Validation loss: 2.7645648102378138

Epoch: 5| Step: 11
Training loss: 3.283463812105703
Validation loss: 2.763190280753196

Epoch: 60| Step: 0
Training loss: 2.9323768580761858
Validation loss: 2.759712360909266

Epoch: 5| Step: 1
Training loss: 2.876592775399987
Validation loss: 2.7583429584642567

Epoch: 5| Step: 2
Training loss: 2.950396856995603
Validation loss: 2.755323502573892

Epoch: 5| Step: 3
Training loss: 2.741522031716054
Validation loss: 2.7539023433143557

Epoch: 5| Step: 4
Training loss: 3.173567597480219
Validation loss: 2.752734984456509

Epoch: 5| Step: 5
Training loss: 2.0704122267401854
Validation loss: 2.7494066566424813

Epoch: 5| Step: 6
Training loss: 3.238409355031701
Validation loss: 2.749094210628953

Epoch: 5| Step: 7
Training loss: 3.1155796732613745
Validation loss: 2.7457712164023866

Epoch: 5| Step: 8
Training loss: 2.680994543608563
Validation loss: 2.743448396846423

Epoch: 5| Step: 9
Training loss: 2.7266910380585494
Validation loss: 2.742344644360276

Epoch: 5| Step: 10
Training loss: 2.931317092355424
Validation loss: 2.7423689946088974

Epoch: 5| Step: 11
Training loss: 3.679406027749726
Validation loss: 2.7370581078631204

Epoch: 61| Step: 0
Training loss: 2.8967711396101556
Validation loss: 2.739904168540293

Epoch: 5| Step: 1
Training loss: 2.835959358236183
Validation loss: 2.743227258906954

Epoch: 5| Step: 2
Training loss: 2.82814413664861
Validation loss: 2.7391938064359507

Epoch: 5| Step: 3
Training loss: 2.879770550329728
Validation loss: 2.7373464914360395

Epoch: 5| Step: 4
Training loss: 2.57650844871221
Validation loss: 2.7328348636166537

Epoch: 5| Step: 5
Training loss: 2.8936151658483102
Validation loss: 2.730922775831251

Epoch: 5| Step: 6
Training loss: 2.5829210465105827
Validation loss: 2.72846581411986

Epoch: 5| Step: 7
Training loss: 3.370250468242526
Validation loss: 2.725824298046753

Epoch: 5| Step: 8
Training loss: 2.7900688001565364
Validation loss: 2.726445061632965

Epoch: 5| Step: 9
Training loss: 3.0099034559412225
Validation loss: 2.723519077709731

Epoch: 5| Step: 10
Training loss: 3.0030644977361836
Validation loss: 2.7224278110173024

Epoch: 5| Step: 11
Training loss: 1.8813442346030396
Validation loss: 2.7201484613188196

Epoch: 62| Step: 0
Training loss: 2.6504037585456652
Validation loss: 2.7208918031151543

Epoch: 5| Step: 1
Training loss: 3.2107197857113596
Validation loss: 2.716798535073836

Epoch: 5| Step: 2
Training loss: 2.7728798748950783
Validation loss: 2.718128071094169

Epoch: 5| Step: 3
Training loss: 3.2330008273538087
Validation loss: 2.7151293560893666

Epoch: 5| Step: 4
Training loss: 2.9434405380834363
Validation loss: 2.713117823872116

Epoch: 5| Step: 5
Training loss: 2.38773155886982
Validation loss: 2.709166847249728

Epoch: 5| Step: 6
Training loss: 2.810601844276557
Validation loss: 2.7098983913226022

Epoch: 5| Step: 7
Training loss: 2.656752056471019
Validation loss: 2.7086618199802306

Epoch: 5| Step: 8
Training loss: 2.9923589672231343
Validation loss: 2.705217686988153

Epoch: 5| Step: 9
Training loss: 2.6634651476436484
Validation loss: 2.706803217279375

Epoch: 5| Step: 10
Training loss: 3.007402981709796
Validation loss: 2.7034347371423966

Epoch: 5| Step: 11
Training loss: 2.2881900309428627
Validation loss: 2.7012400661052247

Epoch: 63| Step: 0
Training loss: 2.8430830624066634
Validation loss: 2.7012935086049885

Epoch: 5| Step: 1
Training loss: 3.0028938005985726
Validation loss: 2.6983093113465144

Epoch: 5| Step: 2
Training loss: 2.6922079146414304
Validation loss: 2.702075787702587

Epoch: 5| Step: 3
Training loss: 2.7771990130733037
Validation loss: 2.697680357423843

Epoch: 5| Step: 4
Training loss: 2.653622932572359
Validation loss: 2.7062091652518263

Epoch: 5| Step: 5
Training loss: 2.950649293469449
Validation loss: 2.6990172609536023

Epoch: 5| Step: 6
Training loss: 3.1005336486732693
Validation loss: 2.7039293845354506

Epoch: 5| Step: 7
Training loss: 2.9242635337749263
Validation loss: 2.69072392880306

Epoch: 5| Step: 8
Training loss: 2.502718687000884
Validation loss: 2.6894975306276256

Epoch: 5| Step: 9
Training loss: 3.067930117471465
Validation loss: 2.690733214124714

Epoch: 5| Step: 10
Training loss: 2.748081144703474
Validation loss: 2.6896923977294005

Epoch: 5| Step: 11
Training loss: 1.9934192514668183
Validation loss: 2.689590214209074

Epoch: 64| Step: 0
Training loss: 3.0365255794418218
Validation loss: 2.6899948590287406

Epoch: 5| Step: 1
Training loss: 2.630871699813373
Validation loss: 2.689343747805423

Epoch: 5| Step: 2
Training loss: 3.043669750871169
Validation loss: 2.6860293238228103

Epoch: 5| Step: 3
Training loss: 2.5884958387241586
Validation loss: 2.6837542919426074

Epoch: 5| Step: 4
Training loss: 2.9055519496229985
Validation loss: 2.684008163297133

Epoch: 5| Step: 5
Training loss: 3.186125814383061
Validation loss: 2.6808161606108674

Epoch: 5| Step: 6
Training loss: 2.9717581516403415
Validation loss: 2.6777451707662685

Epoch: 5| Step: 7
Training loss: 2.9060302979477575
Validation loss: 2.6752463444207506

Epoch: 5| Step: 8
Training loss: 2.997973711444401
Validation loss: 2.6737132822196323

Epoch: 5| Step: 9
Training loss: 2.4793782397692095
Validation loss: 2.6710775014303705

Epoch: 5| Step: 10
Training loss: 2.12991684191637
Validation loss: 2.6691173056513295

Epoch: 5| Step: 11
Training loss: 2.753490746718944
Validation loss: 2.6696159412620735

Epoch: 65| Step: 0
Training loss: 2.941720730408121
Validation loss: 2.6663972276426375

Epoch: 5| Step: 1
Training loss: 2.6823402375696954
Validation loss: 2.6646920445694495

Epoch: 5| Step: 2
Training loss: 2.7533421581240805
Validation loss: 2.6651091696283644

Epoch: 5| Step: 3
Training loss: 2.8675889401997727
Validation loss: 2.6620667851363065

Epoch: 5| Step: 4
Training loss: 2.9770100574983434
Validation loss: 2.6611997114166073

Epoch: 5| Step: 5
Training loss: 2.3179026204374527
Validation loss: 2.658828452518673

Epoch: 5| Step: 6
Training loss: 2.4373945555743446
Validation loss: 2.65830650380825

Epoch: 5| Step: 7
Training loss: 3.038652956318051
Validation loss: 2.6584459764258686

Epoch: 5| Step: 8
Training loss: 2.6419306128816573
Validation loss: 2.658540344150265

Epoch: 5| Step: 9
Training loss: 3.143394805075196
Validation loss: 2.657171478504325

Epoch: 5| Step: 10
Training loss: 2.887984105602707
Validation loss: 2.6543575052574497

Epoch: 5| Step: 11
Training loss: 2.700576374558175
Validation loss: 2.6563233234532695

Epoch: 66| Step: 0
Training loss: 2.6641507399815496
Validation loss: 2.655519171761037

Epoch: 5| Step: 1
Training loss: 2.9829281800000773
Validation loss: 2.6484373462118067

Epoch: 5| Step: 2
Training loss: 3.0426268085518084
Validation loss: 2.6584560097417382

Epoch: 5| Step: 3
Training loss: 2.540924797809845
Validation loss: 2.6515175193098983

Epoch: 5| Step: 4
Training loss: 2.9281853245207636
Validation loss: 2.6470178158005235

Epoch: 5| Step: 5
Training loss: 2.794387051789487
Validation loss: 2.645870994440007

Epoch: 5| Step: 6
Training loss: 2.8042408367019855
Validation loss: 2.6427118192856986

Epoch: 5| Step: 7
Training loss: 2.6234527751030323
Validation loss: 2.6431191081668963

Epoch: 5| Step: 8
Training loss: 2.7041638974340647
Validation loss: 2.6412744983608722

Epoch: 5| Step: 9
Training loss: 2.735272418303439
Validation loss: 2.640772520079129

Epoch: 5| Step: 10
Training loss: 2.66801691640155
Validation loss: 2.638759879812505

Epoch: 5| Step: 11
Training loss: 3.271386134252556
Validation loss: 2.6399671160150264

Epoch: 67| Step: 0
Training loss: 2.951586448689402
Validation loss: 2.638041863666745

Epoch: 5| Step: 1
Training loss: 2.5064075372893684
Validation loss: 2.637101521906694

Epoch: 5| Step: 2
Training loss: 2.943399551816039
Validation loss: 2.638339820146167

Epoch: 5| Step: 3
Training loss: 2.614371717990363
Validation loss: 2.6353053244088174

Epoch: 5| Step: 4
Training loss: 2.5998099441022378
Validation loss: 2.6367456279373704

Epoch: 5| Step: 5
Training loss: 2.623461772177072
Validation loss: 2.6381439049730315

Epoch: 5| Step: 6
Training loss: 2.581623731994539
Validation loss: 2.642468232367198

Epoch: 5| Step: 7
Training loss: 3.060090810637512
Validation loss: 2.6390620803606817

Epoch: 5| Step: 8
Training loss: 2.6155476913056632
Validation loss: 2.627569292263635

Epoch: 5| Step: 9
Training loss: 3.0285333474225102
Validation loss: 2.6291197736843506

Epoch: 5| Step: 10
Training loss: 3.0006614591627763
Validation loss: 2.629632831472911

Epoch: 5| Step: 11
Training loss: 2.1770562591572795
Validation loss: 2.627812585545269

Epoch: 68| Step: 0
Training loss: 2.261474279918233
Validation loss: 2.6282470503932363

Epoch: 5| Step: 1
Training loss: 3.030636754877537
Validation loss: 2.6255346011180642

Epoch: 5| Step: 2
Training loss: 2.8698171255617457
Validation loss: 2.6246422152937443

Epoch: 5| Step: 3
Training loss: 2.8743082955905384
Validation loss: 2.6229497002018007

Epoch: 5| Step: 4
Training loss: 2.5630142812342163
Validation loss: 2.6329590655695765

Epoch: 5| Step: 5
Training loss: 2.665218963645581
Validation loss: 2.637073321579374

Epoch: 5| Step: 6
Training loss: 2.796749346590396
Validation loss: 2.624252932069336

Epoch: 5| Step: 7
Training loss: 2.9450816258774655
Validation loss: 2.6208263247527115

Epoch: 5| Step: 8
Training loss: 2.9090810133483944
Validation loss: 2.617830499261802

Epoch: 5| Step: 9
Training loss: 2.6693764766473778
Validation loss: 2.618002454358022

Epoch: 5| Step: 10
Training loss: 2.6522452289783973
Validation loss: 2.6223840015577577

Epoch: 5| Step: 11
Training loss: 3.135929982357394
Validation loss: 2.622156533185827

Epoch: 69| Step: 0
Training loss: 2.5875638244423547
Validation loss: 2.6253845637052424

Epoch: 5| Step: 1
Training loss: 3.087666859924888
Validation loss: 2.6263144244898724

Epoch: 5| Step: 2
Training loss: 2.5490451296511107
Validation loss: 2.6254153074490962

Epoch: 5| Step: 3
Training loss: 2.5535158042680064
Validation loss: 2.6228428711835825

Epoch: 5| Step: 4
Training loss: 3.168114899863607
Validation loss: 2.6244507737291567

Epoch: 5| Step: 5
Training loss: 2.6137897355987505
Validation loss: 2.623798640410487

Epoch: 5| Step: 6
Training loss: 2.820301848415222
Validation loss: 2.6202328135460564

Epoch: 5| Step: 7
Training loss: 2.8472655647100935
Validation loss: 2.61731338957349

Epoch: 5| Step: 8
Training loss: 2.154420601827238
Validation loss: 2.610819525253483

Epoch: 5| Step: 9
Training loss: 3.233665646520798
Validation loss: 2.6077339395186923

Epoch: 5| Step: 10
Training loss: 2.5111324403170174
Validation loss: 2.6076247004227238

Epoch: 5| Step: 11
Training loss: 2.522718297550316
Validation loss: 2.6055132830167818

Epoch: 70| Step: 0
Training loss: 2.390015767576167
Validation loss: 2.602390795140933

Epoch: 5| Step: 1
Training loss: 2.797892294915105
Validation loss: 2.6012904738064258

Epoch: 5| Step: 2
Training loss: 2.685321190801226
Validation loss: 2.59781830169851

Epoch: 5| Step: 3
Training loss: 2.767664098380149
Validation loss: 2.5977680383018718

Epoch: 5| Step: 4
Training loss: 2.5317975617607726
Validation loss: 2.599324654784647

Epoch: 5| Step: 5
Training loss: 2.88244146838484
Validation loss: 2.601065591089238

Epoch: 5| Step: 6
Training loss: 2.9026414351458207
Validation loss: 2.59836932268925

Epoch: 5| Step: 7
Training loss: 2.9174317219384673
Validation loss: 2.5923038112428762

Epoch: 5| Step: 8
Training loss: 2.807953953987644
Validation loss: 2.594462963148658

Epoch: 5| Step: 9
Training loss: 2.479382278512474
Validation loss: 2.5942082555627115

Epoch: 5| Step: 10
Training loss: 3.0169651038427965
Validation loss: 2.590009715658545

Epoch: 5| Step: 11
Training loss: 1.665720869958956
Validation loss: 2.588998540603582

Epoch: 71| Step: 0
Training loss: 2.360550745371124
Validation loss: 2.589231270028225

Epoch: 5| Step: 1
Training loss: 2.9036182259699808
Validation loss: 2.5895351036124588

Epoch: 5| Step: 2
Training loss: 2.4513598852510055
Validation loss: 2.5888222184814955

Epoch: 5| Step: 3
Training loss: 2.9833834134237085
Validation loss: 2.588362038339179

Epoch: 5| Step: 4
Training loss: 2.857572060091098
Validation loss: 2.58783112079659

Epoch: 5| Step: 5
Training loss: 3.0284032924075426
Validation loss: 2.588644226519083

Epoch: 5| Step: 6
Training loss: 3.232048930524821
Validation loss: 2.58622750061239

Epoch: 5| Step: 7
Training loss: 2.1052029641857004
Validation loss: 2.584861715997191

Epoch: 5| Step: 8
Training loss: 2.9382277865466397
Validation loss: 2.582011993690359

Epoch: 5| Step: 9
Training loss: 2.313415655362503
Validation loss: 2.581910111764088

Epoch: 5| Step: 10
Training loss: 2.456724308412559
Validation loss: 2.5812910695538016

Epoch: 5| Step: 11
Training loss: 2.9764658994843964
Validation loss: 2.5845199072303555

Epoch: 72| Step: 0
Training loss: 2.731052843742673
Validation loss: 2.581149583145641

Epoch: 5| Step: 1
Training loss: 2.0536440676524266
Validation loss: 2.5800798768916646

Epoch: 5| Step: 2
Training loss: 2.92321893312308
Validation loss: 2.5855602284999284

Epoch: 5| Step: 3
Training loss: 2.769912499791328
Validation loss: 2.58239658866308

Epoch: 5| Step: 4
Training loss: 2.9250145023345078
Validation loss: 2.5848529650451217

Epoch: 5| Step: 5
Training loss: 2.8906058645903796
Validation loss: 2.5826715165107315

Epoch: 5| Step: 6
Training loss: 2.433945338046821
Validation loss: 2.5761413533849193

Epoch: 5| Step: 7
Training loss: 2.312449686688728
Validation loss: 2.5795427797428654

Epoch: 5| Step: 8
Training loss: 2.9836782870486336
Validation loss: 2.5762301253567537

Epoch: 5| Step: 9
Training loss: 2.7923181162779245
Validation loss: 2.5759664419389328

Epoch: 5| Step: 10
Training loss: 2.867295265771086
Validation loss: 2.57436892544631

Epoch: 5| Step: 11
Training loss: 2.672192649474997
Validation loss: 2.574978551806162

Epoch: 73| Step: 0
Training loss: 2.9461612035150364
Validation loss: 2.572209065287863

Epoch: 5| Step: 1
Training loss: 2.6108523087269067
Validation loss: 2.571223942827533

Epoch: 5| Step: 2
Training loss: 2.806547690321566
Validation loss: 2.56700700128054

Epoch: 5| Step: 3
Training loss: 2.587431692060272
Validation loss: 2.568295387853758

Epoch: 5| Step: 4
Training loss: 2.505784019472966
Validation loss: 2.573500078031446

Epoch: 5| Step: 5
Training loss: 2.8432534591290173
Validation loss: 2.5707065315032724

Epoch: 5| Step: 6
Training loss: 2.3176933945306537
Validation loss: 2.5679549059500975

Epoch: 5| Step: 7
Training loss: 2.338342251732319
Validation loss: 2.568392959587879

Epoch: 5| Step: 8
Training loss: 2.8353807307387537
Validation loss: 2.5688192828473184

Epoch: 5| Step: 9
Training loss: 2.710219494281568
Validation loss: 2.567326210997053

Epoch: 5| Step: 10
Training loss: 3.0618011299997865
Validation loss: 2.5640378113588045

Epoch: 5| Step: 11
Training loss: 2.9098693296582985
Validation loss: 2.5663145775682095

Epoch: 74| Step: 0
Training loss: 2.8915878754214233
Validation loss: 2.5656284235452476

Epoch: 5| Step: 1
Training loss: 2.7869230554291096
Validation loss: 2.5622631676872407

Epoch: 5| Step: 2
Training loss: 2.803109833028836
Validation loss: 2.565808823257721

Epoch: 5| Step: 3
Training loss: 2.7192331520562374
Validation loss: 2.5597289927777074

Epoch: 5| Step: 4
Training loss: 2.0967609903236974
Validation loss: 2.5608642055596604

Epoch: 5| Step: 5
Training loss: 2.6522725563438545
Validation loss: 2.562683750170405

Epoch: 5| Step: 6
Training loss: 2.6027069781958487
Validation loss: 2.563713976326266

Epoch: 5| Step: 7
Training loss: 2.4469158534589397
Validation loss: 2.5698289464672355

Epoch: 5| Step: 8
Training loss: 2.5639049121286654
Validation loss: 2.5652374245873477

Epoch: 5| Step: 9
Training loss: 3.2095112475453513
Validation loss: 2.5741317958163816

Epoch: 5| Step: 10
Training loss: 2.8732045413697667
Validation loss: 2.5619487130674594

Epoch: 5| Step: 11
Training loss: 1.574844425328819
Validation loss: 2.5543223089886826

Epoch: 75| Step: 0
Training loss: 2.4547931806226257
Validation loss: 2.5599129224473933

Epoch: 5| Step: 1
Training loss: 2.953230598620421
Validation loss: 2.558027590169142

Epoch: 5| Step: 2
Training loss: 2.744798335743474
Validation loss: 2.5575998836600533

Epoch: 5| Step: 3
Training loss: 2.4495678033891775
Validation loss: 2.556990927566627

Epoch: 5| Step: 4
Training loss: 3.02919125203716
Validation loss: 2.5554782487456955

Epoch: 5| Step: 5
Training loss: 2.9590254676911494
Validation loss: 2.5555514858796244

Epoch: 5| Step: 6
Training loss: 2.8411212550614438
Validation loss: 2.5595019720842074

Epoch: 5| Step: 7
Training loss: 2.5651068104893313
Validation loss: 2.5547438954574897

Epoch: 5| Step: 8
Training loss: 2.346023779050182
Validation loss: 2.5548605048563218

Epoch: 5| Step: 9
Training loss: 2.5097353209807154
Validation loss: 2.5541719700030665

Epoch: 5| Step: 10
Training loss: 2.639281884984664
Validation loss: 2.552615769308097

Epoch: 5| Step: 11
Training loss: 2.429076973604391
Validation loss: 2.5523675928947296

Epoch: 76| Step: 0
Training loss: 2.5884592719879675
Validation loss: 2.552903733768353

Epoch: 5| Step: 1
Training loss: 2.8440206210304946
Validation loss: 2.5510729807412478

Epoch: 5| Step: 2
Training loss: 2.9883624695355504
Validation loss: 2.5476044037476133

Epoch: 5| Step: 3
Training loss: 2.5294154548096257
Validation loss: 2.548216445039262

Epoch: 5| Step: 4
Training loss: 2.486366769111261
Validation loss: 2.5460198337734314

Epoch: 5| Step: 5
Training loss: 2.5701646385447905
Validation loss: 2.547766874520806

Epoch: 5| Step: 6
Training loss: 2.63683756242728
Validation loss: 2.5464107971906085

Epoch: 5| Step: 7
Training loss: 2.412912335513516
Validation loss: 2.5496350839155526

Epoch: 5| Step: 8
Training loss: 2.4338939108128352
Validation loss: 2.5470111241072124

Epoch: 5| Step: 9
Training loss: 2.7597377206049023
Validation loss: 2.5465128003339306

Epoch: 5| Step: 10
Training loss: 2.9671649114422904
Validation loss: 2.5418240871040743

Epoch: 5| Step: 11
Training loss: 3.504064107717793
Validation loss: 2.5437508126147423

Epoch: 77| Step: 0
Training loss: 2.3684342629359847
Validation loss: 2.5458549102663826

Epoch: 5| Step: 1
Training loss: 2.704806648155511
Validation loss: 2.543844116306884

Epoch: 5| Step: 2
Training loss: 2.6808016493322935
Validation loss: 2.5423603179656244

Epoch: 5| Step: 3
Training loss: 2.8965052824494077
Validation loss: 2.5443765168641117

Epoch: 5| Step: 4
Training loss: 2.810456868971202
Validation loss: 2.5422144375127904

Epoch: 5| Step: 5
Training loss: 2.485560392288749
Validation loss: 2.5423114354849523

Epoch: 5| Step: 6
Training loss: 2.841767678651566
Validation loss: 2.541006766051425

Epoch: 5| Step: 7
Training loss: 2.6536969650079274
Validation loss: 2.5396401124906913

Epoch: 5| Step: 8
Training loss: 2.2553498985019713
Validation loss: 2.535311764187594

Epoch: 5| Step: 9
Training loss: 2.6828051523098
Validation loss: 2.5325736815537576

Epoch: 5| Step: 10
Training loss: 2.5562999431329403
Validation loss: 2.5334534610847643

Epoch: 5| Step: 11
Training loss: 3.979820850116678
Validation loss: 2.5352811189923457

Epoch: 78| Step: 0
Training loss: 2.9159490701652118
Validation loss: 2.538270785802404

Epoch: 5| Step: 1
Training loss: 2.909397512772894
Validation loss: 2.5396723283899942

Epoch: 5| Step: 2
Training loss: 2.8675959241631905
Validation loss: 2.535415659653258

Epoch: 5| Step: 3
Training loss: 2.7936776922029853
Validation loss: 2.5393394162708667

Epoch: 5| Step: 4
Training loss: 2.324944903633555
Validation loss: 2.5348734215416497

Epoch: 5| Step: 5
Training loss: 2.4498629901489815
Validation loss: 2.530845072033132

Epoch: 5| Step: 6
Training loss: 2.2626071166926125
Validation loss: 2.534025601386409

Epoch: 5| Step: 7
Training loss: 2.6620988068962097
Validation loss: 2.5279876403681247

Epoch: 5| Step: 8
Training loss: 2.7426129741924026
Validation loss: 2.532184993286474

Epoch: 5| Step: 9
Training loss: 2.7677695370359223
Validation loss: 2.5319942510503433

Epoch: 5| Step: 10
Training loss: 2.5376737112333925
Validation loss: 2.528661302911417

Epoch: 5| Step: 11
Training loss: 2.220820038870788
Validation loss: 2.531137915276438

Epoch: 79| Step: 0
Training loss: 2.5962676732467784
Validation loss: 2.5322497281349836

Epoch: 5| Step: 1
Training loss: 2.5771164279581846
Validation loss: 2.527601575794434

Epoch: 5| Step: 2
Training loss: 2.55198512504691
Validation loss: 2.526974913988236

Epoch: 5| Step: 3
Training loss: 2.5577055576084957
Validation loss: 2.527179679281449

Epoch: 5| Step: 4
Training loss: 2.4433237587019954
Validation loss: 2.5245851558354064

Epoch: 5| Step: 5
Training loss: 2.490838430107401
Validation loss: 2.528109148130635

Epoch: 5| Step: 6
Training loss: 2.760451396987568
Validation loss: 2.5283036607783576

Epoch: 5| Step: 7
Training loss: 2.84958829081045
Validation loss: 2.525187370851743

Epoch: 5| Step: 8
Training loss: 3.16466947785274
Validation loss: 2.5243548958766384

Epoch: 5| Step: 9
Training loss: 2.6896656980148363
Validation loss: 2.524034285862115

Epoch: 5| Step: 10
Training loss: 2.195415888079263
Validation loss: 2.5263197847354437

Epoch: 5| Step: 11
Training loss: 3.487041462386186
Validation loss: 2.5253585180064375

Epoch: 80| Step: 0
Training loss: 2.7941501066371526
Validation loss: 2.5200583879894505

Epoch: 5| Step: 1
Training loss: 3.3181495712442555
Validation loss: 2.525258610798747

Epoch: 5| Step: 2
Training loss: 2.4749384708655513
Validation loss: 2.5211520597769805

Epoch: 5| Step: 3
Training loss: 2.0200428416175273
Validation loss: 2.5255081058956916

Epoch: 5| Step: 4
Training loss: 2.6613714772563437
Validation loss: 2.527400008788535

Epoch: 5| Step: 5
Training loss: 2.4801782633178755
Validation loss: 2.522118584939897

Epoch: 5| Step: 6
Training loss: 2.4725657080394265
Validation loss: 2.522134454294053

Epoch: 5| Step: 7
Training loss: 2.621795696746044
Validation loss: 2.519318229088027

Epoch: 5| Step: 8
Training loss: 2.5644086846991927
Validation loss: 2.5226713578413773

Epoch: 5| Step: 9
Training loss: 2.7246098125279667
Validation loss: 2.5209896391232376

Epoch: 5| Step: 10
Training loss: 2.8368322048753893
Validation loss: 2.5211200248957546

Epoch: 5| Step: 11
Training loss: 2.7565628116453103
Validation loss: 2.5208946580494733

Epoch: 81| Step: 0
Training loss: 2.7927122198350767
Validation loss: 2.5248581260222935

Epoch: 5| Step: 1
Training loss: 2.6336399668415327
Validation loss: 2.5267288112379123

Epoch: 5| Step: 2
Training loss: 2.5952347332110612
Validation loss: 2.530372530332792

Epoch: 5| Step: 3
Training loss: 2.2785149469477606
Validation loss: 2.5311276009875776

Epoch: 5| Step: 4
Training loss: 2.5777781860124698
Validation loss: 2.5352732666157256

Epoch: 5| Step: 5
Training loss: 3.156479931000603
Validation loss: 2.5338107487930523

Epoch: 5| Step: 6
Training loss: 2.6707307027140863
Validation loss: 2.540826777273219

Epoch: 5| Step: 7
Training loss: 2.5511208931731306
Validation loss: 2.549444852344132

Epoch: 5| Step: 8
Training loss: 2.3658089731253784
Validation loss: 2.545809665347234

Epoch: 5| Step: 9
Training loss: 2.736987626013975
Validation loss: 2.532579167212217

Epoch: 5| Step: 10
Training loss: 2.8243403111805376
Validation loss: 2.5272865190663985

Epoch: 5| Step: 11
Training loss: 2.576512057595417
Validation loss: 2.5204958151179904

Epoch: 82| Step: 0
Training loss: 2.8941180594511025
Validation loss: 2.5175026269941316

Epoch: 5| Step: 1
Training loss: 2.2105952021494377
Validation loss: 2.515141071347165

Epoch: 5| Step: 2
Training loss: 1.9861514332671466
Validation loss: 2.5120820948925204

Epoch: 5| Step: 3
Training loss: 2.7971903021198243
Validation loss: 2.52157303437315

Epoch: 5| Step: 4
Training loss: 2.5047407976311384
Validation loss: 2.5163851906354044

Epoch: 5| Step: 5
Training loss: 2.855538885296338
Validation loss: 2.511818994051334

Epoch: 5| Step: 6
Training loss: 2.8144451620398283
Validation loss: 2.5194240458547195

Epoch: 5| Step: 7
Training loss: 2.334122683205908
Validation loss: 2.5217590655181574

Epoch: 5| Step: 8
Training loss: 2.9257235553181635
Validation loss: 2.529149325864902

Epoch: 5| Step: 9
Training loss: 3.1286072320251823
Validation loss: 2.527937075086809

Epoch: 5| Step: 10
Training loss: 2.5464641028959445
Validation loss: 2.515460311048055

Epoch: 5| Step: 11
Training loss: 2.2893211036396943
Validation loss: 2.5156513591830696

Epoch: 83| Step: 0
Training loss: 2.8972326689552435
Validation loss: 2.5103486725923543

Epoch: 5| Step: 1
Training loss: 2.1526656282260377
Validation loss: 2.514649037975252

Epoch: 5| Step: 2
Training loss: 2.649140301293516
Validation loss: 2.5165965768914953

Epoch: 5| Step: 3
Training loss: 2.7982592859822146
Validation loss: 2.5217159095006134

Epoch: 5| Step: 4
Training loss: 2.785870004500626
Validation loss: 2.525657999587278

Epoch: 5| Step: 5
Training loss: 2.7196061989467983
Validation loss: 2.5307110106628237

Epoch: 5| Step: 6
Training loss: 2.614367340609251
Validation loss: 2.534317250030857

Epoch: 5| Step: 7
Training loss: 2.8575159919137754
Validation loss: 2.538464363266263

Epoch: 5| Step: 8
Training loss: 2.4013553726135233
Validation loss: 2.535693530500502

Epoch: 5| Step: 9
Training loss: 2.17823901979465
Validation loss: 2.5353104829037667

Epoch: 5| Step: 10
Training loss: 2.8246376919293072
Validation loss: 2.5280177314864285

Epoch: 5| Step: 11
Training loss: 3.642880768258556
Validation loss: 2.5253574204923366

Epoch: 84| Step: 0
Training loss: 2.808985315438629
Validation loss: 2.5205137402261433

Epoch: 5| Step: 1
Training loss: 1.8432787276740075
Validation loss: 2.518665060341782

Epoch: 5| Step: 2
Training loss: 2.436646629995616
Validation loss: 2.5175923657070025

Epoch: 5| Step: 3
Training loss: 2.3771295536862405
Validation loss: 2.5141498315791413

Epoch: 5| Step: 4
Training loss: 2.7577391876764006
Validation loss: 2.5137261871659233

Epoch: 5| Step: 5
Training loss: 2.9712823607817342
Validation loss: 2.509391592532258

Epoch: 5| Step: 6
Training loss: 2.205086234173254
Validation loss: 2.5120201701224616

Epoch: 5| Step: 7
Training loss: 2.9222260126621946
Validation loss: 2.5080941973931283

Epoch: 5| Step: 8
Training loss: 2.558542125850856
Validation loss: 2.505391379243097

Epoch: 5| Step: 9
Training loss: 2.742830988923902
Validation loss: 2.503505629112163

Epoch: 5| Step: 10
Training loss: 2.9251054663269396
Validation loss: 2.497650170659397

Epoch: 5| Step: 11
Training loss: 3.343874777488962
Validation loss: 2.50329089128967

Epoch: 85| Step: 0
Training loss: 2.504941824353611
Validation loss: 2.5005102074070304

Epoch: 5| Step: 1
Training loss: 2.310928222967772
Validation loss: 2.4984920603107064

Epoch: 5| Step: 2
Training loss: 2.52786339159548
Validation loss: 2.503501577700491

Epoch: 5| Step: 3
Training loss: 3.0125175633848427
Validation loss: 2.5177360664663677

Epoch: 5| Step: 4
Training loss: 2.790607868411117
Validation loss: 2.5125955462278546

Epoch: 5| Step: 5
Training loss: 2.380434544426772
Validation loss: 2.5080986057809396

Epoch: 5| Step: 6
Training loss: 2.150719003438266
Validation loss: 2.5037976110955897

Epoch: 5| Step: 7
Training loss: 3.0560955109974666
Validation loss: 2.4915637926938574

Epoch: 5| Step: 8
Training loss: 2.4886104539636467
Validation loss: 2.5000951470706836

Epoch: 5| Step: 9
Training loss: 2.77094020972007
Validation loss: 2.4979066826814655

Epoch: 5| Step: 10
Training loss: 2.585935010649529
Validation loss: 2.509781829742088

Epoch: 5| Step: 11
Training loss: 3.136416524006425
Validation loss: 2.504589675893597

Epoch: 86| Step: 0
Training loss: 2.8311682076671074
Validation loss: 2.50790293402735

Epoch: 5| Step: 1
Training loss: 2.5766244856559495
Validation loss: 2.5106648260478877

Epoch: 5| Step: 2
Training loss: 2.531825812508683
Validation loss: 2.511677102415516

Epoch: 5| Step: 3
Training loss: 2.0619366194255524
Validation loss: 2.516380303292629

Epoch: 5| Step: 4
Training loss: 2.6115158344557825
Validation loss: 2.5149852578961567

Epoch: 5| Step: 5
Training loss: 2.5799824814608288
Validation loss: 2.518060973795678

Epoch: 5| Step: 6
Training loss: 2.49092256498711
Validation loss: 2.523686045402675

Epoch: 5| Step: 7
Training loss: 2.721685556679849
Validation loss: 2.519855969226064

Epoch: 5| Step: 8
Training loss: 2.8383504727335724
Validation loss: 2.5193457680573084

Epoch: 5| Step: 9
Training loss: 2.550649642470093
Validation loss: 2.5144814208037047

Epoch: 5| Step: 10
Training loss: 2.891326984018865
Validation loss: 2.512992286849414

Epoch: 5| Step: 11
Training loss: 3.674010090273653
Validation loss: 2.511184817566821

Epoch: 87| Step: 0
Training loss: 2.2778527032991502
Validation loss: 2.5041545957019427

Epoch: 5| Step: 1
Training loss: 2.1353930836437547
Validation loss: 2.49840314251829

Epoch: 5| Step: 2
Training loss: 3.0788314705716076
Validation loss: 2.505018815282742

Epoch: 5| Step: 3
Training loss: 2.783112834335866
Validation loss: 2.5008900885755447

Epoch: 5| Step: 4
Training loss: 2.6166073284183877
Validation loss: 2.492809002909575

Epoch: 5| Step: 5
Training loss: 2.378938821904733
Validation loss: 2.4969115253398058

Epoch: 5| Step: 6
Training loss: 2.3469496185590444
Validation loss: 2.4883941792443474

Epoch: 5| Step: 7
Training loss: 2.7111530493096487
Validation loss: 2.490543837922036

Epoch: 5| Step: 8
Training loss: 2.421278258608277
Validation loss: 2.4908654105298162

Epoch: 5| Step: 9
Training loss: 2.824315155198908
Validation loss: 2.5094929350316297

Epoch: 5| Step: 10
Training loss: 3.0878694691617947
Validation loss: 2.4939631212480804

Epoch: 5| Step: 11
Training loss: 3.053451561228306
Validation loss: 2.4957428169606937

Epoch: 88| Step: 0
Training loss: 2.829117574439069
Validation loss: 2.4929095849492295

Epoch: 5| Step: 1
Training loss: 2.4063881178019755
Validation loss: 2.4810769964868453

Epoch: 5| Step: 2
Training loss: 2.342834497302464
Validation loss: 2.4926595609500213

Epoch: 5| Step: 3
Training loss: 2.7159857015444047
Validation loss: 2.490663554787819

Epoch: 5| Step: 4
Training loss: 2.6368628794519706
Validation loss: 2.493723584276608

Epoch: 5| Step: 5
Training loss: 2.6850702702084037
Validation loss: 2.4924143625161785

Epoch: 5| Step: 6
Training loss: 3.030823038450987
Validation loss: 2.497369395498304

Epoch: 5| Step: 7
Training loss: 2.2638903060731828
Validation loss: 2.4942389667732536

Epoch: 5| Step: 8
Training loss: 2.5838232703816875
Validation loss: 2.4958897419213915

Epoch: 5| Step: 9
Training loss: 2.9443922957914617
Validation loss: 2.494136566342024

Epoch: 5| Step: 10
Training loss: 2.340163067409592
Validation loss: 2.492015055367561

Epoch: 5| Step: 11
Training loss: 1.5447284948800184
Validation loss: 2.4930778714641986

Epoch: 89| Step: 0
Training loss: 3.128549924603618
Validation loss: 2.5000408566633188

Epoch: 5| Step: 1
Training loss: 2.2501878659973027
Validation loss: 2.505691042979894

Epoch: 5| Step: 2
Training loss: 2.675946886268235
Validation loss: 2.503599944599145

Epoch: 5| Step: 3
Training loss: 2.650891902468027
Validation loss: 2.5137081543765234

Epoch: 5| Step: 4
Training loss: 2.660448503614938
Validation loss: 2.4999969561876205

Epoch: 5| Step: 5
Training loss: 2.74411725343139
Validation loss: 2.4997514044821965

Epoch: 5| Step: 6
Training loss: 2.419909263990546
Validation loss: 2.494438362721867

Epoch: 5| Step: 7
Training loss: 2.632504136679099
Validation loss: 2.5003664145568676

Epoch: 5| Step: 8
Training loss: 2.314562213113487
Validation loss: 2.499951624402261

Epoch: 5| Step: 9
Training loss: 2.6451511755216424
Validation loss: 2.5041484308980504

Epoch: 5| Step: 10
Training loss: 2.6893571159163545
Validation loss: 2.5011963385901277

Epoch: 5| Step: 11
Training loss: 2.1934762245908797
Validation loss: 2.500059480754251

Epoch: 90| Step: 0
Training loss: 2.51994029428506
Validation loss: 2.503191424856466

Epoch: 5| Step: 1
Training loss: 2.3514241022591613
Validation loss: 2.5040189466739715

Epoch: 5| Step: 2
Training loss: 2.722597841268435
Validation loss: 2.508862545946312

Epoch: 5| Step: 3
Training loss: 2.898325462952147
Validation loss: 2.498698328815616

Epoch: 5| Step: 4
Training loss: 2.0360579881710317
Validation loss: 2.5021884201071614

Epoch: 5| Step: 5
Training loss: 2.923544014356862
Validation loss: 2.5035928618354064

Epoch: 5| Step: 6
Training loss: 2.80902452839361
Validation loss: 2.5030941928359947

Epoch: 5| Step: 7
Training loss: 2.66660722030501
Validation loss: 2.506607629781667

Epoch: 5| Step: 8
Training loss: 2.469553176990108
Validation loss: 2.507995709691898

Epoch: 5| Step: 9
Training loss: 2.3500130957380603
Validation loss: 2.5057105408946723

Epoch: 5| Step: 10
Training loss: 2.9428749419902873
Validation loss: 2.508566599409581

Epoch: 5| Step: 11
Training loss: 2.848213633132771
Validation loss: 2.5013276786758922

Epoch: 91| Step: 0
Training loss: 2.9861170293074126
Validation loss: 2.505035843862407

Epoch: 5| Step: 1
Training loss: 2.505777359153247
Validation loss: 2.5019241557651526

Epoch: 5| Step: 2
Training loss: 2.4821465052139744
Validation loss: 2.5026083888965336

Epoch: 5| Step: 3
Training loss: 2.3474289566681255
Validation loss: 2.5020656515720083

Epoch: 5| Step: 4
Training loss: 2.810475362433889
Validation loss: 2.5075744920012513

Epoch: 5| Step: 5
Training loss: 2.814741131820146
Validation loss: 2.5052359983274175

Epoch: 5| Step: 6
Training loss: 2.4481670062249945
Validation loss: 2.5075065885719057

Epoch: 5| Step: 7
Training loss: 2.673357814223276
Validation loss: 2.5044250823584764

Epoch: 5| Step: 8
Training loss: 2.4243649388552226
Validation loss: 2.504337680285668

Epoch: 5| Step: 9
Training loss: 3.0331680679014816
Validation loss: 2.5037825698181546

Epoch: 5| Step: 10
Training loss: 2.2162041901115668
Validation loss: 2.502823415487842

Epoch: 5| Step: 11
Training loss: 2.3696748362377256
Validation loss: 2.499130530955239

Epoch: 92| Step: 0
Training loss: 2.32332100079117
Validation loss: 2.4958366655657582

Epoch: 5| Step: 1
Training loss: 2.6418865734069747
Validation loss: 2.4943286582906876

Epoch: 5| Step: 2
Training loss: 2.903170358818824
Validation loss: 2.492108980574215

Epoch: 5| Step: 3
Training loss: 2.6157155011014934
Validation loss: 2.494322409461336

Epoch: 5| Step: 4
Training loss: 2.6831075569401563
Validation loss: 2.500896650688799

Epoch: 5| Step: 5
Training loss: 2.6098490758354047
Validation loss: 2.4986151753943213

Epoch: 5| Step: 6
Training loss: 2.9007532785691863
Validation loss: 2.49436976316994

Epoch: 5| Step: 7
Training loss: 2.668067852095409
Validation loss: 2.4938883224406028

Epoch: 5| Step: 8
Training loss: 2.5358486082394416
Validation loss: 2.4966282797049404

Epoch: 5| Step: 9
Training loss: 2.2087167970744814
Validation loss: 2.49491278501039

Epoch: 5| Step: 10
Training loss: 2.765222724925575
Validation loss: 2.5002158310230547

Epoch: 5| Step: 11
Training loss: 1.4565146504538478
Validation loss: 2.4976429556790216

Epoch: 93| Step: 0
Training loss: 2.7074170739039545
Validation loss: 2.497290673029112

Epoch: 5| Step: 1
Training loss: 2.8116452083737373
Validation loss: 2.49191838388954

Epoch: 5| Step: 2
Training loss: 2.5794336869939816
Validation loss: 2.490174001323394

Epoch: 5| Step: 3
Training loss: 2.5872549521468593
Validation loss: 2.491715887842344

Epoch: 5| Step: 4
Training loss: 3.094483876124535
Validation loss: 2.492377318803759

Epoch: 5| Step: 5
Training loss: 2.3023622323735373
Validation loss: 2.4893108056484783

Epoch: 5| Step: 6
Training loss: 2.7369350111593915
Validation loss: 2.485099480916058

Epoch: 5| Step: 7
Training loss: 2.373698831803227
Validation loss: 2.487990936798145

Epoch: 5| Step: 8
Training loss: 2.104495019755925
Validation loss: 2.482954246807193

Epoch: 5| Step: 9
Training loss: 3.064897805232428
Validation loss: 2.4763338161907407

Epoch: 5| Step: 10
Training loss: 2.1847802829915715
Validation loss: 2.4857818850522597

Epoch: 5| Step: 11
Training loss: 1.5527102678454865
Validation loss: 2.4814240826155602

Epoch: 94| Step: 0
Training loss: 2.232138948709608
Validation loss: 2.4858379415208707

Epoch: 5| Step: 1
Training loss: 2.6294510660850006
Validation loss: 2.48198367337296

Epoch: 5| Step: 2
Training loss: 3.007466244274276
Validation loss: 2.4888662132746835

Epoch: 5| Step: 3
Training loss: 2.683599435947397
Validation loss: 2.4893760847606017

Epoch: 5| Step: 4
Training loss: 2.469030050497318
Validation loss: 2.4918367224704205

Epoch: 5| Step: 5
Training loss: 2.509526982817725
Validation loss: 2.487899715091332

Epoch: 5| Step: 6
Training loss: 2.8297109635558306
Validation loss: 2.4909322959900737

Epoch: 5| Step: 7
Training loss: 2.2339889086086844
Validation loss: 2.4926287181769062

Epoch: 5| Step: 8
Training loss: 2.6259061974872044
Validation loss: 2.491293767840327

Epoch: 5| Step: 9
Training loss: 2.4572224019567748
Validation loss: 2.4897824983191525

Epoch: 5| Step: 10
Training loss: 2.7807128044562064
Validation loss: 2.4900652779406425

Epoch: 5| Step: 11
Training loss: 2.8096577163694327
Validation loss: 2.4855874359979553

Epoch: 95| Step: 0
Training loss: 2.2370979416696444
Validation loss: 2.4863417056447705

Epoch: 5| Step: 1
Training loss: 2.6673443846997626
Validation loss: 2.4904922052685343

Epoch: 5| Step: 2
Training loss: 2.6331824883180177
Validation loss: 2.4837238490086735

Epoch: 5| Step: 3
Training loss: 2.5817994722110917
Validation loss: 2.476134815852451

Epoch: 5| Step: 4
Training loss: 2.562468737900005
Validation loss: 2.473464982214859

Epoch: 5| Step: 5
Training loss: 2.9780242614680104
Validation loss: 2.4722382692883014

Epoch: 5| Step: 6
Training loss: 2.4993241350207005
Validation loss: 2.4757947243070735

Epoch: 5| Step: 7
Training loss: 2.437497945931376
Validation loss: 2.474459299596448

Epoch: 5| Step: 8
Training loss: 2.8230954121847933
Validation loss: 2.4765027116203187

Epoch: 5| Step: 9
Training loss: 2.360217215676511
Validation loss: 2.4761373714577224

Epoch: 5| Step: 10
Training loss: 2.5538136332123282
Validation loss: 2.474451033402119

Epoch: 5| Step: 11
Training loss: 2.8812565933780987
Validation loss: 2.476099414229597

Epoch: 96| Step: 0
Training loss: 2.6174991251334667
Validation loss: 2.4721606070689983

Epoch: 5| Step: 1
Training loss: 2.6340381701114364
Validation loss: 2.4763583992691487

Epoch: 5| Step: 2
Training loss: 2.806187135803652
Validation loss: 2.477563729850677

Epoch: 5| Step: 3
Training loss: 2.813626635917423
Validation loss: 2.4780125349131605

Epoch: 5| Step: 4
Training loss: 2.449372939225498
Validation loss: 2.4716255968822693

Epoch: 5| Step: 5
Training loss: 2.4334434597578753
Validation loss: 2.4766198762330363

Epoch: 5| Step: 6
Training loss: 2.292021313593164
Validation loss: 2.4746122974919436

Epoch: 5| Step: 7
Training loss: 2.4946960452130655
Validation loss: 2.4794597586050737

Epoch: 5| Step: 8
Training loss: 2.5948908491460547
Validation loss: 2.471152279603179

Epoch: 5| Step: 9
Training loss: 2.370173720419206
Validation loss: 2.471028013316813

Epoch: 5| Step: 10
Training loss: 2.9224882247335153
Validation loss: 2.4725355708447063

Epoch: 5| Step: 11
Training loss: 1.9775689016845999
Validation loss: 2.469497358296144

Epoch: 97| Step: 0
Training loss: 3.0095604511609158
Validation loss: 2.477136314815791

Epoch: 5| Step: 1
Training loss: 2.3297492384325507
Validation loss: 2.4709905082819317

Epoch: 5| Step: 2
Training loss: 1.6952369075393905
Validation loss: 2.469471423655275

Epoch: 5| Step: 3
Training loss: 2.0539573851699053
Validation loss: 2.4699629045447358

Epoch: 5| Step: 4
Training loss: 2.824204989620048
Validation loss: 2.4719900004003073

Epoch: 5| Step: 5
Training loss: 3.1458981404144795
Validation loss: 2.4752656888763505

Epoch: 5| Step: 6
Training loss: 2.5123571174949704
Validation loss: 2.474557737049049

Epoch: 5| Step: 7
Training loss: 2.902982125600779
Validation loss: 2.4736649603649106

Epoch: 5| Step: 8
Training loss: 2.4969886763456866
Validation loss: 2.462924978211762

Epoch: 5| Step: 9
Training loss: 2.514475684296618
Validation loss: 2.4705616665478085

Epoch: 5| Step: 10
Training loss: 2.7048917960985195
Validation loss: 2.460074621634972

Epoch: 5| Step: 11
Training loss: 0.9547260782341965
Validation loss: 2.460086970213389

Epoch: 98| Step: 0
Training loss: 2.529556084389082
Validation loss: 2.465502373370022

Epoch: 5| Step: 1
Training loss: 2.401329459087516
Validation loss: 2.4734152361730715

Epoch: 5| Step: 2
Training loss: 2.9904903050686307
Validation loss: 2.4664298031358696

Epoch: 5| Step: 3
Training loss: 2.962642772469476
Validation loss: 2.4694767658837025

Epoch: 5| Step: 4
Training loss: 2.6173793437733526
Validation loss: 2.4583556502886377

Epoch: 5| Step: 5
Training loss: 2.736901995692776
Validation loss: 2.46637064316295

Epoch: 5| Step: 6
Training loss: 2.620184250242434
Validation loss: 2.4666760420835874

Epoch: 5| Step: 7
Training loss: 2.264502622212454
Validation loss: 2.4728333227504646

Epoch: 5| Step: 8
Training loss: 2.1890241490230533
Validation loss: 2.4748926118550667

Epoch: 5| Step: 9
Training loss: 2.2860044290961934
Validation loss: 2.4818306575795135

Epoch: 5| Step: 10
Training loss: 2.4766104419773076
Validation loss: 2.4878570738055563

Epoch: 5| Step: 11
Training loss: 2.81756694859708
Validation loss: 2.486516229706315

Epoch: 99| Step: 0
Training loss: 2.4316725885825923
Validation loss: 2.4854625904437553

Epoch: 5| Step: 1
Training loss: 2.106549440373705
Validation loss: 2.489489938002974

Epoch: 5| Step: 2
Training loss: 2.6103889185797384
Validation loss: 2.4839391366016916

Epoch: 5| Step: 3
Training loss: 2.6855797228104774
Validation loss: 2.486539525556377

Epoch: 5| Step: 4
Training loss: 2.421427826975737
Validation loss: 2.4835544686672417

Epoch: 5| Step: 5
Training loss: 2.2208126312875054
Validation loss: 2.4876240331950346

Epoch: 5| Step: 6
Training loss: 2.7263885704900144
Validation loss: 2.482565946922753

Epoch: 5| Step: 7
Training loss: 3.0402743318655454
Validation loss: 2.483001573268173

Epoch: 5| Step: 8
Training loss: 2.8123738578483115
Validation loss: 2.488834832542031

Epoch: 5| Step: 9
Training loss: 3.0926841672366687
Validation loss: 2.482127058329131

Epoch: 5| Step: 10
Training loss: 2.212210999488092
Validation loss: 2.4883044175407423

Epoch: 5| Step: 11
Training loss: 2.2373912166811447
Validation loss: 2.48742886350503

Epoch: 100| Step: 0
Training loss: 2.3197738073512575
Validation loss: 2.4818941881572734

Epoch: 5| Step: 1
Training loss: 2.7654688731807298
Validation loss: 2.482883549500489

Epoch: 5| Step: 2
Training loss: 2.308664595134943
Validation loss: 2.4839110111113274

Epoch: 5| Step: 3
Training loss: 2.7622725863514255
Validation loss: 2.4851097084187543

Epoch: 5| Step: 4
Training loss: 2.311535711541927
Validation loss: 2.481107178076008

Epoch: 5| Step: 5
Training loss: 3.121557857682972
Validation loss: 2.4856458568393363

Epoch: 5| Step: 6
Training loss: 2.383725251027831
Validation loss: 2.4753086875343895

Epoch: 5| Step: 7
Training loss: 2.5182596945794256
Validation loss: 2.4715628594468053

Epoch: 5| Step: 8
Training loss: 2.4548439757884997
Validation loss: 2.463826166437176

Epoch: 5| Step: 9
Training loss: 2.8943279570774485
Validation loss: 2.460484352978853

Epoch: 5| Step: 10
Training loss: 2.4455033005446656
Validation loss: 2.4580209211611983

Epoch: 5| Step: 11
Training loss: 1.8453360379209343
Validation loss: 2.4543522723034226

Epoch: 101| Step: 0
Training loss: 2.4874783692306024
Validation loss: 2.467784692869193

Epoch: 5| Step: 1
Training loss: 2.501437060268676
Validation loss: 2.4610310370213733

Epoch: 5| Step: 2
Training loss: 3.0738560851640084
Validation loss: 2.465614053390049

Epoch: 5| Step: 3
Training loss: 2.378683045287401
Validation loss: 2.4627402711210853

Epoch: 5| Step: 4
Training loss: 2.3223804031638555
Validation loss: 2.461126241560908

Epoch: 5| Step: 5
Training loss: 2.5184108875012656
Validation loss: 2.4591884553599646

Epoch: 5| Step: 6
Training loss: 2.837640928558212
Validation loss: 2.4633891210983805

Epoch: 5| Step: 7
Training loss: 2.4242152566543473
Validation loss: 2.4597844889297455

Epoch: 5| Step: 8
Training loss: 2.4146955826620236
Validation loss: 2.4657065387324812

Epoch: 5| Step: 9
Training loss: 2.4420337084319037
Validation loss: 2.462312497909051

Epoch: 5| Step: 10
Training loss: 2.7719298657273916
Validation loss: 2.4754333729980957

Epoch: 5| Step: 11
Training loss: 2.251704735799994
Validation loss: 2.4762100787268935

Epoch: 102| Step: 0
Training loss: 2.413379757822782
Validation loss: 2.4758828009606106

Epoch: 5| Step: 1
Training loss: 2.1862456949059665
Validation loss: 2.466607588681278

Epoch: 5| Step: 2
Training loss: 3.002563652742358
Validation loss: 2.458807613937035

Epoch: 5| Step: 3
Training loss: 1.8969144009515435
Validation loss: 2.4601591746398785

Epoch: 5| Step: 4
Training loss: 2.0958389565283064
Validation loss: 2.4569822972631195

Epoch: 5| Step: 5
Training loss: 2.4562899831376197
Validation loss: 2.456748153638989

Epoch: 5| Step: 6
Training loss: 3.06147141602533
Validation loss: 2.472820166065705

Epoch: 5| Step: 7
Training loss: 2.4870103018577607
Validation loss: 2.4613097943659294

Epoch: 5| Step: 8
Training loss: 2.8300661622525807
Validation loss: 2.462100788456343

Epoch: 5| Step: 9
Training loss: 2.7835234019393993
Validation loss: 2.4679773304576673

Epoch: 5| Step: 10
Training loss: 2.599518118731908
Validation loss: 2.472737568607385

Epoch: 5| Step: 11
Training loss: 3.007693122377347
Validation loss: 2.475908937268207

Epoch: 103| Step: 0
Training loss: 2.5861295415556405
Validation loss: 2.478578038152512

Epoch: 5| Step: 1
Training loss: 2.364775281557041
Validation loss: 2.476664227182394

Epoch: 5| Step: 2
Training loss: 3.0494076888483796
Validation loss: 2.476369924508925

Epoch: 5| Step: 3
Training loss: 2.4459666391812855
Validation loss: 2.476483043916314

Epoch: 5| Step: 4
Training loss: 2.405443427908747
Validation loss: 2.469812735847522

Epoch: 5| Step: 5
Training loss: 2.5708354197719854
Validation loss: 2.4696928831242353

Epoch: 5| Step: 6
Training loss: 2.45928977421175
Validation loss: 2.466366925478374

Epoch: 5| Step: 7
Training loss: 2.750547181224597
Validation loss: 2.4589484699224573

Epoch: 5| Step: 8
Training loss: 2.1543686993990696
Validation loss: 2.465257713672018

Epoch: 5| Step: 9
Training loss: 2.750617478145204
Validation loss: 2.4702122176725

Epoch: 5| Step: 10
Training loss: 2.7254981119404427
Validation loss: 2.463486823398302

Epoch: 5| Step: 11
Training loss: 2.175148342817781
Validation loss: 2.4633749844305943

Epoch: 104| Step: 0
Training loss: 2.2864214663187243
Validation loss: 2.4648710472018105

Epoch: 5| Step: 1
Training loss: 2.8729939303521563
Validation loss: 2.472695352848775

Epoch: 5| Step: 2
Training loss: 2.527043839219679
Validation loss: 2.4690971009785168

Epoch: 5| Step: 3
Training loss: 2.5522767804586954
Validation loss: 2.46766886799661

Epoch: 5| Step: 4
Training loss: 2.231653864023504
Validation loss: 2.470434740690669

Epoch: 5| Step: 5
Training loss: 2.5361377009483115
Validation loss: 2.471283546191766

Epoch: 5| Step: 6
Training loss: 2.7430042235216425
Validation loss: 2.473931628969858

Epoch: 5| Step: 7
Training loss: 2.3538602331343608
Validation loss: 2.4677181260341494

Epoch: 5| Step: 8
Training loss: 2.3262772364100646
Validation loss: 2.46838698957527

Epoch: 5| Step: 9
Training loss: 2.4913855909781106
Validation loss: 2.4618254961755452

Epoch: 5| Step: 10
Training loss: 3.095773670824739
Validation loss: 2.461278365007102

Epoch: 5| Step: 11
Training loss: 2.793180017550447
Validation loss: 2.45581308722416

Epoch: 105| Step: 0
Training loss: 2.5824791408704555
Validation loss: 2.451125210625595

Epoch: 5| Step: 1
Training loss: 2.583461532693251
Validation loss: 2.461286740019944

Epoch: 5| Step: 2
Training loss: 3.210204103344807
Validation loss: 2.4709787167325064

Epoch: 5| Step: 3
Training loss: 2.9588783371057934
Validation loss: 2.4581342524937857

Epoch: 5| Step: 4
Training loss: 2.497272529514935
Validation loss: 2.458404436268545

Epoch: 5| Step: 5
Training loss: 2.581107708793752
Validation loss: 2.4644836831819674

Epoch: 5| Step: 6
Training loss: 1.9094858065604816
Validation loss: 2.4690816431172893

Epoch: 5| Step: 7
Training loss: 2.938865182064262
Validation loss: 2.4781292673581725

Epoch: 5| Step: 8
Training loss: 2.4176373231499886
Validation loss: 2.4709147527273303

Epoch: 5| Step: 9
Training loss: 2.3347324082410004
Validation loss: 2.4731448646437233

Epoch: 5| Step: 10
Training loss: 2.5372952458497644
Validation loss: 2.470005842683294

Epoch: 5| Step: 11
Training loss: 1.1079456631319982
Validation loss: 2.4738783786636183

Epoch: 106| Step: 0
Training loss: 2.7359876836142587
Validation loss: 2.4727804264267346

Epoch: 5| Step: 1
Training loss: 2.522660930176732
Validation loss: 2.470815020252656

Epoch: 5| Step: 2
Training loss: 2.646530049623157
Validation loss: 2.4714534338954772

Epoch: 5| Step: 3
Training loss: 3.2585240600128484
Validation loss: 2.4720975617316965

Epoch: 5| Step: 4
Training loss: 2.531947852117473
Validation loss: 2.470515320165058

Epoch: 5| Step: 5
Training loss: 2.3474857312826076
Validation loss: 2.4651142501920345

Epoch: 5| Step: 6
Training loss: 2.90417422745613
Validation loss: 2.4716924523594557

Epoch: 5| Step: 7
Training loss: 2.272317722005977
Validation loss: 2.4702572909094656

Epoch: 5| Step: 8
Training loss: 2.623206116024518
Validation loss: 2.474146425559057

Epoch: 5| Step: 9
Training loss: 2.089268000787302
Validation loss: 2.4737835243437503

Epoch: 5| Step: 10
Training loss: 2.2413696498814444
Validation loss: 2.4699903623557646

Epoch: 5| Step: 11
Training loss: 2.246678974370703
Validation loss: 2.473307366641851

Epoch: 107| Step: 0
Training loss: 2.469461845429136
Validation loss: 2.4716988146687546

Epoch: 5| Step: 1
Training loss: 2.0024742080989615
Validation loss: 2.4718452919622798

Epoch: 5| Step: 2
Training loss: 2.6538199585054496
Validation loss: 2.4676989720366787

Epoch: 5| Step: 3
Training loss: 2.8485295300204343
Validation loss: 2.4660913178737

Epoch: 5| Step: 4
Training loss: 2.76714916579642
Validation loss: 2.4714669314760145

Epoch: 5| Step: 5
Training loss: 2.6323454581288144
Validation loss: 2.468279640844563

Epoch: 5| Step: 6
Training loss: 2.400953433276753
Validation loss: 2.4703035657263896

Epoch: 5| Step: 7
Training loss: 2.5846678045567306
Validation loss: 2.4675295786902893

Epoch: 5| Step: 8
Training loss: 2.908567591489862
Validation loss: 2.465946283427539

Epoch: 5| Step: 9
Training loss: 2.183567981875015
Validation loss: 2.4672172512380977

Epoch: 5| Step: 10
Training loss: 2.52838848990132
Validation loss: 2.4640351998077303

Epoch: 5| Step: 11
Training loss: 2.575522941894491
Validation loss: 2.47297659953721

Epoch: 108| Step: 0
Training loss: 2.1167288983826076
Validation loss: 2.466503602085129

Epoch: 5| Step: 1
Training loss: 2.48928204446449
Validation loss: 2.4561640993778617

Epoch: 5| Step: 2
Training loss: 3.0139373642577487
Validation loss: 2.454839678150253

Epoch: 5| Step: 3
Training loss: 2.3138574276246007
Validation loss: 2.461222576604021

Epoch: 5| Step: 4
Training loss: 2.364062471761659
Validation loss: 2.457362491777702

Epoch: 5| Step: 5
Training loss: 2.1868577286746085
Validation loss: 2.459369603998969

Epoch: 5| Step: 6
Training loss: 2.843923207132859
Validation loss: 2.4542364307832947

Epoch: 5| Step: 7
Training loss: 2.7634209077469185
Validation loss: 2.460616491463074

Epoch: 5| Step: 8
Training loss: 3.108014599887329
Validation loss: 2.4708060945507233

Epoch: 5| Step: 9
Training loss: 2.691234477946748
Validation loss: 2.474393088973294

Epoch: 5| Step: 10
Training loss: 2.119440098007143
Validation loss: 2.4746002983924384

Epoch: 5| Step: 11
Training loss: 2.0333781904473556
Validation loss: 2.476345257301325

Epoch: 109| Step: 0
Training loss: 2.518814055809142
Validation loss: 2.4780446019064977

Epoch: 5| Step: 1
Training loss: 2.4723548163052
Validation loss: 2.4814517218636993

Epoch: 5| Step: 2
Training loss: 2.0812641040221007
Validation loss: 2.478053637836432

Epoch: 5| Step: 3
Training loss: 3.452936227739295
Validation loss: 2.47988356848914

Epoch: 5| Step: 4
Training loss: 2.5981565065471766
Validation loss: 2.477467549050883

Epoch: 5| Step: 5
Training loss: 2.238091425810314
Validation loss: 2.481245525013358

Epoch: 5| Step: 6
Training loss: 2.736199429716948
Validation loss: 2.4777746493278685

Epoch: 5| Step: 7
Training loss: 2.541583596628272
Validation loss: 2.483283100857556

Epoch: 5| Step: 8
Training loss: 2.3563523288028967
Validation loss: 2.482270999288277

Epoch: 5| Step: 9
Training loss: 2.297226859409255
Validation loss: 2.481747007038803

Epoch: 5| Step: 10
Training loss: 2.6212218206416282
Validation loss: 2.4777576599197704

Epoch: 5| Step: 11
Training loss: 2.6854430910343874
Validation loss: 2.4738667695350904

Epoch: 110| Step: 0
Training loss: 2.76059226191331
Validation loss: 2.4733621394231338

Epoch: 5| Step: 1
Training loss: 2.0458841036565296
Validation loss: 2.468915261801261

Epoch: 5| Step: 2
Training loss: 3.1561622229730473
Validation loss: 2.4736165034733757

Epoch: 5| Step: 3
Training loss: 2.285057235268064
Validation loss: 2.4713291586336505

Epoch: 5| Step: 4
Training loss: 2.5252959314045182
Validation loss: 2.4748905105487555

Epoch: 5| Step: 5
Training loss: 2.5342665229282058
Validation loss: 2.4693199658663705

Epoch: 5| Step: 6
Training loss: 2.5154352531190853
Validation loss: 2.4714588401716955

Epoch: 5| Step: 7
Training loss: 2.172153043444851
Validation loss: 2.4748308965281485

Epoch: 5| Step: 8
Training loss: 2.537712982710454
Validation loss: 2.470672876627718

Epoch: 5| Step: 9
Training loss: 2.675834532715386
Validation loss: 2.467550863703479

Epoch: 5| Step: 10
Training loss: 2.6344251815704465
Validation loss: 2.475445990083495

Epoch: 5| Step: 11
Training loss: 2.640352223770546
Validation loss: 2.468232342010364

Epoch: 111| Step: 0
Training loss: 2.8636560962321913
Validation loss: 2.4719012465210843

Epoch: 5| Step: 1
Training loss: 2.346704477345134
Validation loss: 2.4739716352138625

Epoch: 5| Step: 2
Training loss: 2.831658111194112
Validation loss: 2.468511018573796

Epoch: 5| Step: 3
Training loss: 2.437753419662642
Validation loss: 2.4633276498967303

Epoch: 5| Step: 4
Training loss: 2.3966856283605877
Validation loss: 2.4687180738356465

Epoch: 5| Step: 5
Training loss: 2.0616102900566893
Validation loss: 2.4732057905879175

Epoch: 5| Step: 6
Training loss: 2.7110953601091246
Validation loss: 2.4780262974328147

Epoch: 5| Step: 7
Training loss: 2.6990121411802734
Validation loss: 2.484693794669515

Epoch: 5| Step: 8
Training loss: 2.149596411506318
Validation loss: 2.487448572461063

Epoch: 5| Step: 9
Training loss: 3.0250656120814075
Validation loss: 2.480538506916216

Epoch: 5| Step: 10
Training loss: 2.7570922602677657
Validation loss: 2.4801390781744197

Epoch: 5| Step: 11
Training loss: 2.3471785835357806
Validation loss: 2.473215727865511

Epoch: 112| Step: 0
Training loss: 2.4021908098865237
Validation loss: 2.481261279435228

Epoch: 5| Step: 1
Training loss: 3.0365788134342493
Validation loss: 2.4727768509527706

Epoch: 5| Step: 2
Training loss: 2.5024286870865784
Validation loss: 2.4767486871313054

Epoch: 5| Step: 3
Training loss: 2.5904343335859905
Validation loss: 2.477979148558932

Epoch: 5| Step: 4
Training loss: 2.552183458089868
Validation loss: 2.4771230807268956

Epoch: 5| Step: 5
Training loss: 2.588338423079354
Validation loss: 2.4707997540662943

Epoch: 5| Step: 6
Training loss: 2.8477791549051523
Validation loss: 2.4714425489555145

Epoch: 5| Step: 7
Training loss: 2.508362136455775
Validation loss: 2.4724611584042804

Epoch: 5| Step: 8
Training loss: 2.2776588496766266
Validation loss: 2.4691618603987773

Epoch: 5| Step: 9
Training loss: 2.387957312440404
Validation loss: 2.46681077660766

Epoch: 5| Step: 10
Training loss: 2.760043616848607
Validation loss: 2.466050780954287

Epoch: 5| Step: 11
Training loss: 1.8935533024830546
Validation loss: 2.457200623189816

Epoch: 113| Step: 0
Training loss: 2.700700322914761
Validation loss: 2.461057026296687

Epoch: 5| Step: 1
Training loss: 2.9127025550184884
Validation loss: 2.4618067946891973

Epoch: 5| Step: 2
Training loss: 2.743752579046962
Validation loss: 2.4617072016861687

Epoch: 5| Step: 3
Training loss: 2.496753587504199
Validation loss: 2.455773743878515

Epoch: 5| Step: 4
Training loss: 2.529055550746121
Validation loss: 2.4577564610622136

Epoch: 5| Step: 5
Training loss: 2.9135758144399224
Validation loss: 2.4515506122496205

Epoch: 5| Step: 6
Training loss: 2.748635647187526
Validation loss: 2.459030207498978

Epoch: 5| Step: 7
Training loss: 2.2529904308158204
Validation loss: 2.459101853153741

Epoch: 5| Step: 8
Training loss: 2.1932378448215597
Validation loss: 2.4551021480674744

Epoch: 5| Step: 9
Training loss: 2.4311136555624477
Validation loss: 2.457333811460122

Epoch: 5| Step: 10
Training loss: 2.347373297931394
Validation loss: 2.4618724923897046

Epoch: 5| Step: 11
Training loss: 1.7302021312047444
Validation loss: 2.4658048824419248

Epoch: 114| Step: 0
Training loss: 2.7316325358441915
Validation loss: 2.464470352932532

Epoch: 5| Step: 1
Training loss: 2.633204671477025
Validation loss: 2.4625628074472243

Epoch: 5| Step: 2
Training loss: 2.8743552438646356
Validation loss: 2.4700341204746032

Epoch: 5| Step: 3
Training loss: 2.409986690943521
Validation loss: 2.464988966189148

Epoch: 5| Step: 4
Training loss: 2.12276565668543
Validation loss: 2.468529582746719

Epoch: 5| Step: 5
Training loss: 2.5657060617332696
Validation loss: 2.4710472339742906

Epoch: 5| Step: 6
Training loss: 2.5059842013921303
Validation loss: 2.4666020630310825

Epoch: 5| Step: 7
Training loss: 3.1382751783863503
Validation loss: 2.467913783774452

Epoch: 5| Step: 8
Training loss: 2.414514394395318
Validation loss: 2.4703388051050617

Epoch: 5| Step: 9
Training loss: 2.0809176617839813
Validation loss: 2.469840030419198

Epoch: 5| Step: 10
Training loss: 2.5288554038285582
Validation loss: 2.463726058306879

Epoch: 5| Step: 11
Training loss: 2.727282147680254
Validation loss: 2.4689380195903676

Epoch: 115| Step: 0
Training loss: 1.9842264915824153
Validation loss: 2.463372046590579

Epoch: 5| Step: 1
Training loss: 2.5713880891489502
Validation loss: 2.4631914907439585

Epoch: 5| Step: 2
Training loss: 2.581362177371742
Validation loss: 2.4651658925261204

Epoch: 5| Step: 3
Training loss: 2.4668720683932612
Validation loss: 2.461785564930296

Epoch: 5| Step: 4
Training loss: 2.463967435039485
Validation loss: 2.460591153593619

Epoch: 5| Step: 5
Training loss: 2.837093737586845
Validation loss: 2.4649882286840468

Epoch: 5| Step: 6
Training loss: 2.458949663736837
Validation loss: 2.460885684159155

Epoch: 5| Step: 7
Training loss: 2.601854462607831
Validation loss: 2.4648977073248863

Epoch: 5| Step: 8
Training loss: 3.1457336780519074
Validation loss: 2.4643609510642572

Epoch: 5| Step: 9
Training loss: 1.7844522035268282
Validation loss: 2.4617396061727153

Epoch: 5| Step: 10
Training loss: 2.7224568495120054
Validation loss: 2.461683460992729

Epoch: 5| Step: 11
Training loss: 2.939767165533493
Validation loss: 2.460587822831338

Epoch: 116| Step: 0
Training loss: 2.357041272125331
Validation loss: 2.4666053695623154

Epoch: 5| Step: 1
Training loss: 2.07955103540406
Validation loss: 2.4692513846035333

Epoch: 5| Step: 2
Training loss: 2.4442804370634286
Validation loss: 2.4671490906843516

Epoch: 5| Step: 3
Training loss: 3.25268135966726
Validation loss: 2.4683958475587975

Epoch: 5| Step: 4
Training loss: 2.8710143616787813
Validation loss: 2.467295310636655

Epoch: 5| Step: 5
Training loss: 2.32729083997007
Validation loss: 2.4636202202196253

Epoch: 5| Step: 6
Training loss: 2.719877688799408
Validation loss: 2.463507070694987

Epoch: 5| Step: 7
Training loss: 2.3672154994605434
Validation loss: 2.4662430826983703

Epoch: 5| Step: 8
Training loss: 2.3563360385718255
Validation loss: 2.464347986983805

Epoch: 5| Step: 9
Training loss: 2.4537657854502313
Validation loss: 2.464226030185746

Epoch: 5| Step: 10
Training loss: 2.6489777590666526
Validation loss: 2.4641224671821385

Epoch: 5| Step: 11
Training loss: 1.896729128411899
Validation loss: 2.4653196968550275

Epoch: 117| Step: 0
Training loss: 2.5752627118360456
Validation loss: 2.460560086490336

Epoch: 5| Step: 1
Training loss: 2.6583619641363634
Validation loss: 2.457971086860464

Epoch: 5| Step: 2
Training loss: 2.939302256821848
Validation loss: 2.461559943546151

Epoch: 5| Step: 3
Training loss: 2.73409125899488
Validation loss: 2.4660472198932557

Epoch: 5| Step: 4
Training loss: 2.758588903137115
Validation loss: 2.471697785768298

Epoch: 5| Step: 5
Training loss: 2.1551614861046873
Validation loss: 2.4666374801399664

Epoch: 5| Step: 6
Training loss: 2.2341527228066713
Validation loss: 2.473423758858389

Epoch: 5| Step: 7
Training loss: 1.9949839393906192
Validation loss: 2.4762184674269063

Epoch: 5| Step: 8
Training loss: 1.9707687563595813
Validation loss: 2.4703305552830344

Epoch: 5| Step: 9
Training loss: 2.665620141511271
Validation loss: 2.4719052894430855

Epoch: 5| Step: 10
Training loss: 3.224643977422613
Validation loss: 2.4665056440810926

Epoch: 5| Step: 11
Training loss: 2.1752032568657116
Validation loss: 2.456253842519872

Epoch: 118| Step: 0
Training loss: 2.9492845540568546
Validation loss: 2.4608896826079696

Epoch: 5| Step: 1
Training loss: 2.9895061700058307
Validation loss: 2.4591911780405744

Epoch: 5| Step: 2
Training loss: 2.6792598638941176
Validation loss: 2.461854426806523

Epoch: 5| Step: 3
Training loss: 2.0629162223916633
Validation loss: 2.4529979494728797

Epoch: 5| Step: 4
Training loss: 2.3053873325037277
Validation loss: 2.452138078597559

Epoch: 5| Step: 5
Training loss: 2.5753457550208774
Validation loss: 2.458986302042502

Epoch: 5| Step: 6
Training loss: 2.5457278985782934
Validation loss: 2.454311954351383

Epoch: 5| Step: 7
Training loss: 2.1509965671740687
Validation loss: 2.457838482809343

Epoch: 5| Step: 8
Training loss: 1.985800043686947
Validation loss: 2.4599787403657327

Epoch: 5| Step: 9
Training loss: 2.783970391773761
Validation loss: 2.4683027486035334

Epoch: 5| Step: 10
Training loss: 2.5152258704442048
Validation loss: 2.4736761146326516

Epoch: 5| Step: 11
Training loss: 3.273101570900698
Validation loss: 2.471341708238645

Epoch: 119| Step: 0
Training loss: 2.5705088624473102
Validation loss: 2.4745717476079365

Epoch: 5| Step: 1
Training loss: 2.315301281004282
Validation loss: 2.4757432635574963

Epoch: 5| Step: 2
Training loss: 2.139219616288011
Validation loss: 2.476503870898833

Epoch: 5| Step: 3
Training loss: 2.2121900912551373
Validation loss: 2.4748329477094297

Epoch: 5| Step: 4
Training loss: 2.6786013701445723
Validation loss: 2.474321536791935

Epoch: 5| Step: 5
Training loss: 2.3653035236459967
Validation loss: 2.472033405653839

Epoch: 5| Step: 6
Training loss: 3.075637199317197
Validation loss: 2.4737296684731387

Epoch: 5| Step: 7
Training loss: 2.946445398984474
Validation loss: 2.4729065087957953

Epoch: 5| Step: 8
Training loss: 2.5480849311603118
Validation loss: 2.467501272196701

Epoch: 5| Step: 9
Training loss: 2.599476020539498
Validation loss: 2.4677203642805625

Epoch: 5| Step: 10
Training loss: 2.466961369651545
Validation loss: 2.463303037596077

Epoch: 5| Step: 11
Training loss: 1.667094946511507
Validation loss: 2.469724639815405

Epoch: 120| Step: 0
Training loss: 2.4222607705298778
Validation loss: 2.4686665138582304

Epoch: 5| Step: 1
Training loss: 2.617103415889259
Validation loss: 2.4635696745578173

Epoch: 5| Step: 2
Training loss: 2.412487713876382
Validation loss: 2.468504142993291

Epoch: 5| Step: 3
Training loss: 2.7517338402087246
Validation loss: 2.4684603959846507

Epoch: 5| Step: 4
Training loss: 2.227959873666811
Validation loss: 2.4585190490034847

Epoch: 5| Step: 5
Training loss: 2.139426571143354
Validation loss: 2.463284462264177

Epoch: 5| Step: 6
Training loss: 2.664871406243912
Validation loss: 2.463793023306509

Epoch: 5| Step: 7
Training loss: 2.912959076551718
Validation loss: 2.4578440281615532

Epoch: 5| Step: 8
Training loss: 2.8356962075056993
Validation loss: 2.45576426190042

Epoch: 5| Step: 9
Training loss: 2.130162867377219
Validation loss: 2.455526504703163

Epoch: 5| Step: 10
Training loss: 2.5547093381735757
Validation loss: 2.456710980540603

Epoch: 5| Step: 11
Training loss: 3.181785997005032
Validation loss: 2.455156707922241

Epoch: 121| Step: 0
Training loss: 2.2025181360613026
Validation loss: 2.456570762578649

Epoch: 5| Step: 1
Training loss: 2.3342684393362094
Validation loss: 2.4573859204926514

Epoch: 5| Step: 2
Training loss: 2.577084418018784
Validation loss: 2.457246420232023

Epoch: 5| Step: 3
Training loss: 2.889141795100203
Validation loss: 2.458502102301237

Epoch: 5| Step: 4
Training loss: 2.8548825519407934
Validation loss: 2.4528615530656617

Epoch: 5| Step: 5
Training loss: 2.1239578833274746
Validation loss: 2.4563622142470702

Epoch: 5| Step: 6
Training loss: 2.393756983973422
Validation loss: 2.4555792305981194

Epoch: 5| Step: 7
Training loss: 2.987756381346194
Validation loss: 2.4504345153351417

Epoch: 5| Step: 8
Training loss: 2.5641026021272704
Validation loss: 2.4523521793443814

Epoch: 5| Step: 9
Training loss: 2.4082070851042054
Validation loss: 2.452990586957769

Epoch: 5| Step: 10
Training loss: 2.4647878407758794
Validation loss: 2.4575113212619604

Epoch: 5| Step: 11
Training loss: 2.1558535667544403
Validation loss: 2.4510913161861083

Epoch: 122| Step: 0
Training loss: 2.0514750732945495
Validation loss: 2.454360205483748

Epoch: 5| Step: 1
Training loss: 2.039252729056074
Validation loss: 2.4561219021022445

Epoch: 5| Step: 2
Training loss: 2.427741347630524
Validation loss: 2.4507056554052236

Epoch: 5| Step: 3
Training loss: 3.1382326342174323
Validation loss: 2.4484189972282855

Epoch: 5| Step: 4
Training loss: 3.148113224567189
Validation loss: 2.4522237438253742

Epoch: 5| Step: 5
Training loss: 2.3030469324760308
Validation loss: 2.4557364468080625

Epoch: 5| Step: 6
Training loss: 2.742232015308196
Validation loss: 2.4551569142795207

Epoch: 5| Step: 7
Training loss: 2.1171536038208543
Validation loss: 2.4590827411169704

Epoch: 5| Step: 8
Training loss: 2.462157319522183
Validation loss: 2.4545655404592526

Epoch: 5| Step: 9
Training loss: 2.861561757206651
Validation loss: 2.45313604720882

Epoch: 5| Step: 10
Training loss: 2.342988869738711
Validation loss: 2.4554906522279953

Epoch: 5| Step: 11
Training loss: 1.870745218265004
Validation loss: 2.4555397702290374

Epoch: 123| Step: 0
Training loss: 2.5115622179487582
Validation loss: 2.455617703240631

Epoch: 5| Step: 1
Training loss: 2.3059603005173908
Validation loss: 2.450463473019947

Epoch: 5| Step: 2
Training loss: 3.062092968108074
Validation loss: 2.457859617324467

Epoch: 5| Step: 3
Training loss: 2.9082447712557022
Validation loss: 2.462718319291903

Epoch: 5| Step: 4
Training loss: 1.9425979732908558
Validation loss: 2.4562411592047777

Epoch: 5| Step: 5
Training loss: 2.5502607623642506
Validation loss: 2.459062934087205

Epoch: 5| Step: 6
Training loss: 2.40232290863673
Validation loss: 2.461504948503282

Epoch: 5| Step: 7
Training loss: 2.6350980623879785
Validation loss: 2.471163290442108

Epoch: 5| Step: 8
Training loss: 2.5540889307071306
Validation loss: 2.4679356169360833

Epoch: 5| Step: 9
Training loss: 2.1892197524557093
Validation loss: 2.469633475495558

Epoch: 5| Step: 10
Training loss: 2.5974697418947144
Validation loss: 2.469847121488295

Epoch: 5| Step: 11
Training loss: 3.578794724867474
Validation loss: 2.4621244726737426

Epoch: 124| Step: 0
Training loss: 2.4507360792876627
Validation loss: 2.464472440953384

Epoch: 5| Step: 1
Training loss: 2.632842655178986
Validation loss: 2.4645134311048706

Epoch: 5| Step: 2
Training loss: 2.8829775713876136
Validation loss: 2.4615183997416086

Epoch: 5| Step: 3
Training loss: 3.013791806893411
Validation loss: 2.458563346662685

Epoch: 5| Step: 4
Training loss: 2.282359859179624
Validation loss: 2.45458913145048

Epoch: 5| Step: 5
Training loss: 2.7991241277256305
Validation loss: 2.453252380044435

Epoch: 5| Step: 6
Training loss: 2.3716652195654193
Validation loss: 2.4514490262172552

Epoch: 5| Step: 7
Training loss: 2.327429341064189
Validation loss: 2.449534613335752

Epoch: 5| Step: 8
Training loss: 2.8397473682277523
Validation loss: 2.4528079911328784

Epoch: 5| Step: 9
Training loss: 2.2651941350134464
Validation loss: 2.456159078049269

Epoch: 5| Step: 10
Training loss: 2.062774293173254
Validation loss: 2.451471703012754

Epoch: 5| Step: 11
Training loss: 1.3064646238843605
Validation loss: 2.455606388058079

Epoch: 125| Step: 0
Training loss: 2.815444676322614
Validation loss: 2.4536814777266

Epoch: 5| Step: 1
Training loss: 3.1018702741068487
Validation loss: 2.4561870723813843

Epoch: 5| Step: 2
Training loss: 1.9796765791569955
Validation loss: 2.4469055942004743

Epoch: 5| Step: 3
Training loss: 2.391481532818927
Validation loss: 2.4549098760127923

Epoch: 5| Step: 4
Training loss: 2.298996756307032
Validation loss: 2.454708224248948

Epoch: 5| Step: 5
Training loss: 2.1802783014984266
Validation loss: 2.449668680709116

Epoch: 5| Step: 6
Training loss: 2.536935425889484
Validation loss: 2.448470841574119

Epoch: 5| Step: 7
Training loss: 2.823174458987419
Validation loss: 2.445517843153937

Epoch: 5| Step: 8
Training loss: 2.6950890489916337
Validation loss: 2.4491447425381048

Epoch: 5| Step: 9
Training loss: 2.8282266672312164
Validation loss: 2.446100321358523

Epoch: 5| Step: 10
Training loss: 1.9845327479821846
Validation loss: 2.450058427587572

Epoch: 5| Step: 11
Training loss: 1.8539513737989821
Validation loss: 2.4493981254636497

Epoch: 126| Step: 0
Training loss: 2.5360077778308865
Validation loss: 2.45658070648956

Epoch: 5| Step: 1
Training loss: 2.3759872994452738
Validation loss: 2.455293603958466

Epoch: 5| Step: 2
Training loss: 3.3256697933777133
Validation loss: 2.455891462581305

Epoch: 5| Step: 3
Training loss: 2.2939848070248505
Validation loss: 2.4549607780208396

Epoch: 5| Step: 4
Training loss: 2.1574385311347806
Validation loss: 2.454882903064535

Epoch: 5| Step: 5
Training loss: 2.6015850444194917
Validation loss: 2.4642649926788014

Epoch: 5| Step: 6
Training loss: 2.6534399989108146
Validation loss: 2.4592381761226654

Epoch: 5| Step: 7
Training loss: 2.779779023780672
Validation loss: 2.4691443068047345

Epoch: 5| Step: 8
Training loss: 2.324721440681424
Validation loss: 2.459240718990402

Epoch: 5| Step: 9
Training loss: 2.324814356293691
Validation loss: 2.456363277880797

Epoch: 5| Step: 10
Training loss: 2.1994149643957392
Validation loss: 2.4627473362446253

Epoch: 5| Step: 11
Training loss: 3.00811797733466
Validation loss: 2.4525349063184416

Epoch: 127| Step: 0
Training loss: 2.204410042934421
Validation loss: 2.457385301982298

Epoch: 5| Step: 1
Training loss: 2.4999132141308498
Validation loss: 2.452318885193495

Epoch: 5| Step: 2
Training loss: 2.9621289757280413
Validation loss: 2.444771606293663

Epoch: 5| Step: 3
Training loss: 2.521447782007178
Validation loss: 2.447251234852019

Epoch: 5| Step: 4
Training loss: 2.718203873427427
Validation loss: 2.444072929210171

Epoch: 5| Step: 5
Training loss: 2.468470400658473
Validation loss: 2.448781910967972

Epoch: 5| Step: 6
Training loss: 2.2482792314147475
Validation loss: 2.4499957460898494

Epoch: 5| Step: 7
Training loss: 2.671184071221261
Validation loss: 2.451357745536204

Epoch: 5| Step: 8
Training loss: 2.763236183357675
Validation loss: 2.4448478793581527

Epoch: 5| Step: 9
Training loss: 2.3281918714507417
Validation loss: 2.4512467209532893

Epoch: 5| Step: 10
Training loss: 2.190265324514923
Validation loss: 2.4438039922481507

Epoch: 5| Step: 11
Training loss: 2.9282791211127774
Validation loss: 2.453851021403546

Epoch: 128| Step: 0
Training loss: 2.022333145709155
Validation loss: 2.444429514413693

Epoch: 5| Step: 1
Training loss: 2.646755528371544
Validation loss: 2.4481507040652524

Epoch: 5| Step: 2
Training loss: 2.8985806247302355
Validation loss: 2.4506746068874437

Epoch: 5| Step: 3
Training loss: 2.3776595633920445
Validation loss: 2.4553003890839475

Epoch: 5| Step: 4
Training loss: 2.4050696314131534
Validation loss: 2.465012779828177

Epoch: 5| Step: 5
Training loss: 2.0766479290912376
Validation loss: 2.462472612451367

Epoch: 5| Step: 6
Training loss: 2.7773519910215287
Validation loss: 2.4668079536050276

Epoch: 5| Step: 7
Training loss: 2.033434236347117
Validation loss: 2.4606673723144192

Epoch: 5| Step: 8
Training loss: 2.2413913496286053
Validation loss: 2.4635433912103166

Epoch: 5| Step: 9
Training loss: 2.8439936271520967
Validation loss: 2.4631274253435125

Epoch: 5| Step: 10
Training loss: 2.9952389767278937
Validation loss: 2.4598177567453683

Epoch: 5| Step: 11
Training loss: 2.950196443902878
Validation loss: 2.4590466657833217

Epoch: 129| Step: 0
Training loss: 2.881605934781281
Validation loss: 2.458695374247106

Epoch: 5| Step: 1
Training loss: 2.9372564985927583
Validation loss: 2.4604124084250616

Epoch: 5| Step: 2
Training loss: 2.394200960314903
Validation loss: 2.463468801914195

Epoch: 5| Step: 3
Training loss: 2.4755629197956055
Validation loss: 2.4620833862618

Epoch: 5| Step: 4
Training loss: 2.812742604282656
Validation loss: 2.453197551067504

Epoch: 5| Step: 5
Training loss: 2.5967773786390365
Validation loss: 2.4546867712170743

Epoch: 5| Step: 6
Training loss: 2.4131942224540577
Validation loss: 2.453562679243442

Epoch: 5| Step: 7
Training loss: 2.2316110228063035
Validation loss: 2.4513152142645187

Epoch: 5| Step: 8
Training loss: 2.102894640885868
Validation loss: 2.452090346867317

Epoch: 5| Step: 9
Training loss: 2.3521981124871654
Validation loss: 2.453465630436407

Epoch: 5| Step: 10
Training loss: 2.3114313285342356
Validation loss: 2.446026296709998

Epoch: 5| Step: 11
Training loss: 2.5973639990675825
Validation loss: 2.449834463307615

Epoch: 130| Step: 0
Training loss: 2.2731165708564416
Validation loss: 2.448119554160564

Epoch: 5| Step: 1
Training loss: 2.095877520156722
Validation loss: 2.4427511391755967

Epoch: 5| Step: 2
Training loss: 2.7696006346540423
Validation loss: 2.4412019567845564

Epoch: 5| Step: 3
Training loss: 2.7638249115645275
Validation loss: 2.444891522685325

Epoch: 5| Step: 4
Training loss: 2.5123605338337356
Validation loss: 2.456404542874053

Epoch: 5| Step: 5
Training loss: 2.4961242196338165
Validation loss: 2.442965330636519

Epoch: 5| Step: 6
Training loss: 2.6363447362048906
Validation loss: 2.4383890943176754

Epoch: 5| Step: 7
Training loss: 2.682375524500113
Validation loss: 2.4536019122870933

Epoch: 5| Step: 8
Training loss: 2.47771390990361
Validation loss: 2.450629761261625

Epoch: 5| Step: 9
Training loss: 2.2974194770126184
Validation loss: 2.454889521388706

Epoch: 5| Step: 10
Training loss: 2.6385936259562617
Validation loss: 2.454079043589532

Epoch: 5| Step: 11
Training loss: 2.2098065506515474
Validation loss: 2.46065940092943

Epoch: 131| Step: 0
Training loss: 2.4895271284284397
Validation loss: 2.4619366469375668

Epoch: 5| Step: 1
Training loss: 2.4618156682787036
Validation loss: 2.462402012936123

Epoch: 5| Step: 2
Training loss: 2.326624239377155
Validation loss: 2.4563582751584576

Epoch: 5| Step: 3
Training loss: 2.6003600311427917
Validation loss: 2.4607465357080596

Epoch: 5| Step: 4
Training loss: 2.107683195611704
Validation loss: 2.462694193029753

Epoch: 5| Step: 5
Training loss: 2.4178003304885105
Validation loss: 2.4722849289362516

Epoch: 5| Step: 6
Training loss: 2.637694408610082
Validation loss: 2.4677289529284887

Epoch: 5| Step: 7
Training loss: 2.369065348788643
Validation loss: 2.466648486968225

Epoch: 5| Step: 8
Training loss: 2.616014177727267
Validation loss: 2.453556895454278

Epoch: 5| Step: 9
Training loss: 2.9085544760791096
Validation loss: 2.4492671947565516

Epoch: 5| Step: 10
Training loss: 2.5608115797756454
Validation loss: 2.4423212722128906

Epoch: 5| Step: 11
Training loss: 2.9895551372266187
Validation loss: 2.4496585505833783

Epoch: 132| Step: 0
Training loss: 2.7259218179150433
Validation loss: 2.452699539318719

Epoch: 5| Step: 1
Training loss: 2.3010804996923624
Validation loss: 2.4690890843876567

Epoch: 5| Step: 2
Training loss: 2.6050858566574426
Validation loss: 2.4649504423350534

Epoch: 5| Step: 3
Training loss: 2.6376984761122735
Validation loss: 2.4521105181133236

Epoch: 5| Step: 4
Training loss: 2.856183047522016
Validation loss: 2.4607692883036667

Epoch: 5| Step: 5
Training loss: 2.190062847193706
Validation loss: 2.455300449773715

Epoch: 5| Step: 6
Training loss: 2.578097164119378
Validation loss: 2.466165872161456

Epoch: 5| Step: 7
Training loss: 2.3725018664273314
Validation loss: 2.4655168221920394

Epoch: 5| Step: 8
Training loss: 2.4454755149836944
Validation loss: 2.47339200548113

Epoch: 5| Step: 9
Training loss: 2.5510831365074247
Validation loss: 2.4674937355531417

Epoch: 5| Step: 10
Training loss: 2.769514979649625
Validation loss: 2.479619883669238

Epoch: 5| Step: 11
Training loss: 2.0332444009979356
Validation loss: 2.476044577625079

Epoch: 133| Step: 0
Training loss: 2.6461970662483982
Validation loss: 2.4726756428083134

Epoch: 5| Step: 1
Training loss: 2.533425232408315
Validation loss: 2.474296620312645

Epoch: 5| Step: 2
Training loss: 2.3921195949332517
Validation loss: 2.4745913341560093

Epoch: 5| Step: 3
Training loss: 1.9011903748201004
Validation loss: 2.4651185218580585

Epoch: 5| Step: 4
Training loss: 2.896256523404865
Validation loss: 2.466500931780165

Epoch: 5| Step: 5
Training loss: 1.9411543561828328
Validation loss: 2.457140557749043

Epoch: 5| Step: 6
Training loss: 2.4410822050574503
Validation loss: 2.4530941177659655

Epoch: 5| Step: 7
Training loss: 2.6060841532753
Validation loss: 2.4545280469505655

Epoch: 5| Step: 8
Training loss: 2.4893140341275046
Validation loss: 2.449205575975501

Epoch: 5| Step: 9
Training loss: 2.7750293730134907
Validation loss: 2.4421848164497093

Epoch: 5| Step: 10
Training loss: 3.167273730932916
Validation loss: 2.4472549105382027

Epoch: 5| Step: 11
Training loss: 2.1265530240578623
Validation loss: 2.4403494701759487

Epoch: 134| Step: 0
Training loss: 2.661424869294623
Validation loss: 2.446467795984051

Epoch: 5| Step: 1
Training loss: 2.570651324819289
Validation loss: 2.4535966974460286

Epoch: 5| Step: 2
Training loss: 2.479926006382317
Validation loss: 2.46449475606189

Epoch: 5| Step: 3
Training loss: 2.8023943438933254
Validation loss: 2.468124274135163

Epoch: 5| Step: 4
Training loss: 2.315124465859111
Validation loss: 2.478282704091654

Epoch: 5| Step: 5
Training loss: 2.363073418676547
Validation loss: 2.477021059569423

Epoch: 5| Step: 6
Training loss: 3.004318625689207
Validation loss: 2.4764821814700797

Epoch: 5| Step: 7
Training loss: 2.2193961478675757
Validation loss: 2.4743844331242104

Epoch: 5| Step: 8
Training loss: 2.3978713171874513
Validation loss: 2.474969887309446

Epoch: 5| Step: 9
Training loss: 2.5229675989236378
Validation loss: 2.463591392988394

Epoch: 5| Step: 10
Training loss: 2.3665261934347246
Validation loss: 2.4623035010412155

Epoch: 5| Step: 11
Training loss: 2.461040191929816
Validation loss: 2.457383329216211

Epoch: 135| Step: 0
Training loss: 3.0650807619889315
Validation loss: 2.464720310318303

Epoch: 5| Step: 1
Training loss: 2.373014624105693
Validation loss: 2.4547756456866026

Epoch: 5| Step: 2
Training loss: 2.2396433149481947
Validation loss: 2.4555360321009103

Epoch: 5| Step: 3
Training loss: 2.1113687770716343
Validation loss: 2.454373453028222

Epoch: 5| Step: 4
Training loss: 2.387164834348499
Validation loss: 2.448628842801767

Epoch: 5| Step: 5
Training loss: 2.800141031255725
Validation loss: 2.4527780263157086

Epoch: 5| Step: 6
Training loss: 2.338456240809189
Validation loss: 2.4531986079744326

Epoch: 5| Step: 7
Training loss: 2.506269794490585
Validation loss: 2.4571956909009534

Epoch: 5| Step: 8
Training loss: 2.8313790668769734
Validation loss: 2.4526363866189262

Epoch: 5| Step: 9
Training loss: 2.1959696699038664
Validation loss: 2.4510941086549556

Epoch: 5| Step: 10
Training loss: 2.5768906846402966
Validation loss: 2.445718173013392

Epoch: 5| Step: 11
Training loss: 3.0937939265051537
Validation loss: 2.447316777333294

Epoch: 136| Step: 0
Training loss: 2.9657547145569043
Validation loss: 2.4607411341560588

Epoch: 5| Step: 1
Training loss: 2.107181340898352
Validation loss: 2.4767246514153944

Epoch: 5| Step: 2
Training loss: 2.288096774275447
Validation loss: 2.474723855421272

Epoch: 5| Step: 3
Training loss: 2.287585722771035
Validation loss: 2.4821015958470096

Epoch: 5| Step: 4
Training loss: 2.7359458552853306
Validation loss: 2.4875139044368257

Epoch: 5| Step: 5
Training loss: 2.8785903736668885
Validation loss: 2.474530293787506

Epoch: 5| Step: 6
Training loss: 2.6885456446298353
Validation loss: 2.456161375363987

Epoch: 5| Step: 7
Training loss: 2.8213494843337372
Validation loss: 2.455525768401671

Epoch: 5| Step: 8
Training loss: 2.6866631202895217
Validation loss: 2.4605268872536223

Epoch: 5| Step: 9
Training loss: 2.366274515983185
Validation loss: 2.4652775555522504

Epoch: 5| Step: 10
Training loss: 2.0018740456942385
Validation loss: 2.469041376583998

Epoch: 5| Step: 11
Training loss: 3.006600906290123
Validation loss: 2.475600463904825

Epoch: 137| Step: 0
Training loss: 2.5157705703223416
Validation loss: 2.4720451560052377

Epoch: 5| Step: 1
Training loss: 2.402278247963722
Validation loss: 2.467673489501884

Epoch: 5| Step: 2
Training loss: 2.6822287740401753
Validation loss: 2.4562989292334962

Epoch: 5| Step: 3
Training loss: 2.5028363350714344
Validation loss: 2.458421519012445

Epoch: 5| Step: 4
Training loss: 2.1691658083132044
Validation loss: 2.448056648526653

Epoch: 5| Step: 5
Training loss: 2.578491462325008
Validation loss: 2.4623984788729048

Epoch: 5| Step: 6
Training loss: 2.98900097924756
Validation loss: 2.4580812661743505

Epoch: 5| Step: 7
Training loss: 2.081191475076865
Validation loss: 2.4577368494845495

Epoch: 5| Step: 8
Training loss: 2.897726377871234
Validation loss: 2.4610739614607295

Epoch: 5| Step: 9
Training loss: 2.5794181586080196
Validation loss: 2.4711944292632007

Epoch: 5| Step: 10
Training loss: 2.3742969877878086
Validation loss: 2.468719711600784

Epoch: 5| Step: 11
Training loss: 2.3368816190158084
Validation loss: 2.4737940576455295

Epoch: 138| Step: 0
Training loss: 2.7324942960269514
Validation loss: 2.4701472364459836

Epoch: 5| Step: 1
Training loss: 2.5744030529030195
Validation loss: 2.468728552294571

Epoch: 5| Step: 2
Training loss: 1.90951190214201
Validation loss: 2.4588251766819487

Epoch: 5| Step: 3
Training loss: 2.456594261524107
Validation loss: 2.4541529994016256

Epoch: 5| Step: 4
Training loss: 2.2082816303846386
Validation loss: 2.4578171743133064

Epoch: 5| Step: 5
Training loss: 2.788693871166227
Validation loss: 2.4562433391542204

Epoch: 5| Step: 6
Training loss: 2.566938882074172
Validation loss: 2.457050968287136

Epoch: 5| Step: 7
Training loss: 2.1006819571412025
Validation loss: 2.4524555629239146

Epoch: 5| Step: 8
Training loss: 2.3936692346239394
Validation loss: 2.4632782597014002

Epoch: 5| Step: 9
Training loss: 2.3940564629215095
Validation loss: 2.453854061730777

Epoch: 5| Step: 10
Training loss: 3.0339450456926875
Validation loss: 2.4609854476536692

Epoch: 5| Step: 11
Training loss: 3.985638227484188
Validation loss: 2.461048184282359

Epoch: 139| Step: 0
Training loss: 2.884730251666164
Validation loss: 2.4645148459361614

Epoch: 5| Step: 1
Training loss: 2.55422185451342
Validation loss: 2.4598636040052613

Epoch: 5| Step: 2
Training loss: 2.204531173516792
Validation loss: 2.4628215783141503

Epoch: 5| Step: 3
Training loss: 1.895201133984685
Validation loss: 2.474468754097633

Epoch: 5| Step: 4
Training loss: 2.623985185191737
Validation loss: 2.4623755476819453

Epoch: 5| Step: 5
Training loss: 2.765257557743354
Validation loss: 2.4704019717300962

Epoch: 5| Step: 6
Training loss: 2.1625239552841338
Validation loss: 2.4790016180817704

Epoch: 5| Step: 7
Training loss: 2.6277947761553984
Validation loss: 2.4598044113066377

Epoch: 5| Step: 8
Training loss: 2.5163278486372542
Validation loss: 2.464163313992279

Epoch: 5| Step: 9
Training loss: 2.644265368575367
Validation loss: 2.458730428488818

Epoch: 5| Step: 10
Training loss: 2.5225758690667273
Validation loss: 2.465604214405398

Epoch: 5| Step: 11
Training loss: 3.1850235238396762
Validation loss: 2.4653005403414023

Epoch: 140| Step: 0
Training loss: 2.3705339100779272
Validation loss: 2.4653472144387876

Epoch: 5| Step: 1
Training loss: 2.2102147730727735
Validation loss: 2.4641506814669314

Epoch: 5| Step: 2
Training loss: 2.478982122482215
Validation loss: 2.46723576474951

Epoch: 5| Step: 3
Training loss: 2.354529046268939
Validation loss: 2.4717937489887123

Epoch: 5| Step: 4
Training loss: 2.0010438817449008
Validation loss: 2.4664643446861425

Epoch: 5| Step: 5
Training loss: 2.668347087525233
Validation loss: 2.471329641002733

Epoch: 5| Step: 6
Training loss: 2.4440089642224527
Validation loss: 2.4696895364801787

Epoch: 5| Step: 7
Training loss: 2.440181919570372
Validation loss: 2.4667891429015794

Epoch: 5| Step: 8
Training loss: 2.6113911223819337
Validation loss: 2.4589949757350267

Epoch: 5| Step: 9
Training loss: 3.2492278722408914
Validation loss: 2.4586867479825996

Epoch: 5| Step: 10
Training loss: 2.5024088698342144
Validation loss: 2.4582125114125524

Epoch: 5| Step: 11
Training loss: 2.8941050433199584
Validation loss: 2.4561534581127447

Epoch: 141| Step: 0
Training loss: 2.626148517577967
Validation loss: 2.4621570048142534

Epoch: 5| Step: 1
Training loss: 2.8371523942486623
Validation loss: 2.456852138684317

Epoch: 5| Step: 2
Training loss: 2.2012795368662905
Validation loss: 2.4638114617479725

Epoch: 5| Step: 3
Training loss: 2.801310341630672
Validation loss: 2.4660579091034256

Epoch: 5| Step: 4
Training loss: 2.683285447003374
Validation loss: 2.4609254342242552

Epoch: 5| Step: 5
Training loss: 2.1236343483422435
Validation loss: 2.464049479841828

Epoch: 5| Step: 6
Training loss: 2.459991176248248
Validation loss: 2.4565229592632782

Epoch: 5| Step: 7
Training loss: 2.387572789705344
Validation loss: 2.45957143500766

Epoch: 5| Step: 8
Training loss: 1.9007652674017548
Validation loss: 2.4539003948800473

Epoch: 5| Step: 9
Training loss: 3.0987819247430526
Validation loss: 2.4564483266458446

Epoch: 5| Step: 10
Training loss: 2.3872621108169474
Validation loss: 2.4548015979949156

Epoch: 5| Step: 11
Training loss: 2.1050848389697396
Validation loss: 2.455650511716159

Epoch: 142| Step: 0
Training loss: 2.4262456034837183
Validation loss: 2.4523063840730437

Epoch: 5| Step: 1
Training loss: 2.361395772591289
Validation loss: 2.4536135970537734

Epoch: 5| Step: 2
Training loss: 2.640545544755057
Validation loss: 2.4605339486391777

Epoch: 5| Step: 3
Training loss: 3.010668065398809
Validation loss: 2.4618147442001606

Epoch: 5| Step: 4
Training loss: 2.623809271911248
Validation loss: 2.464090710859246

Epoch: 5| Step: 5
Training loss: 2.3487684309856984
Validation loss: 2.4608415938162413

Epoch: 5| Step: 6
Training loss: 2.4981052848184935
Validation loss: 2.464912995959249

Epoch: 5| Step: 7
Training loss: 2.942475832416135
Validation loss: 2.461037386527188

Epoch: 5| Step: 8
Training loss: 2.083765392006341
Validation loss: 2.4659708290617774

Epoch: 5| Step: 9
Training loss: 2.370132276489361
Validation loss: 2.463705925710946

Epoch: 5| Step: 10
Training loss: 2.0147317489136283
Validation loss: 2.456265834178151

Epoch: 5| Step: 11
Training loss: 2.923198542979879
Validation loss: 2.4561274624527973

Epoch: 143| Step: 0
Training loss: 2.224966233243711
Validation loss: 2.4579290580232382

Epoch: 5| Step: 1
Training loss: 2.0553017049989952
Validation loss: 2.4675311004922955

Epoch: 5| Step: 2
Training loss: 2.1487305597354602
Validation loss: 2.455292940415075

Epoch: 5| Step: 3
Training loss: 2.875940915260828
Validation loss: 2.460584833216021

Epoch: 5| Step: 4
Training loss: 2.7548319241577857
Validation loss: 2.458346703595371

Epoch: 5| Step: 5
Training loss: 2.8385280410486233
Validation loss: 2.4687176714361834

Epoch: 5| Step: 6
Training loss: 2.5705811148583515
Validation loss: 2.4630072836053833

Epoch: 5| Step: 7
Training loss: 2.6023197389409396
Validation loss: 2.4667636831853903

Epoch: 5| Step: 8
Training loss: 2.6743207300808347
Validation loss: 2.464002454594942

Epoch: 5| Step: 9
Training loss: 2.161046649992453
Validation loss: 2.464016279239826

Epoch: 5| Step: 10
Training loss: 2.6907971582927646
Validation loss: 2.465901826323239

Epoch: 5| Step: 11
Training loss: 2.2630234086549166
Validation loss: 2.4609486372761276

Epoch: 144| Step: 0
Training loss: 2.7846368463186084
Validation loss: 2.46203589576121

Epoch: 5| Step: 1
Training loss: 2.5066544660650085
Validation loss: 2.4655148579469475

Epoch: 5| Step: 2
Training loss: 3.0110031841341054
Validation loss: 2.4692427992394377

Epoch: 5| Step: 3
Training loss: 2.6853359292152583
Validation loss: 2.461562841174128

Epoch: 5| Step: 4
Training loss: 2.4769074112578555
Validation loss: 2.466725082339395

Epoch: 5| Step: 5
Training loss: 2.5399110285878304
Validation loss: 2.4708759111742484

Epoch: 5| Step: 6
Training loss: 2.588035999749334
Validation loss: 2.4661526637937756

Epoch: 5| Step: 7
Training loss: 1.8345731170090283
Validation loss: 2.460858191367744

Epoch: 5| Step: 8
Training loss: 2.4462581670641343
Validation loss: 2.469136753048059

Epoch: 5| Step: 9
Training loss: 2.577513096181342
Validation loss: 2.463877497089643

Epoch: 5| Step: 10
Training loss: 2.203801044602133
Validation loss: 2.4610223059195566

Epoch: 5| Step: 11
Training loss: 2.0172387102572062
Validation loss: 2.4587334829841336

Epoch: 145| Step: 0
Training loss: 2.8316700672161423
Validation loss: 2.4611197146819546

Epoch: 5| Step: 1
Training loss: 2.260168410583968
Validation loss: 2.459278702133081

Epoch: 5| Step: 2
Training loss: 3.03170143777188
Validation loss: 2.4573563005442978

Epoch: 5| Step: 3
Training loss: 2.169818480849872
Validation loss: 2.457006505893141

Epoch: 5| Step: 4
Training loss: 1.9705384018318122
Validation loss: 2.45962005955089

Epoch: 5| Step: 5
Training loss: 2.495920381211493
Validation loss: 2.460234631472591

Epoch: 5| Step: 6
Training loss: 2.7895389438446765
Validation loss: 2.458260277831789

Epoch: 5| Step: 7
Training loss: 2.7130523259944663
Validation loss: 2.4578513720963535

Epoch: 5| Step: 8
Training loss: 2.200045333742016
Validation loss: 2.4572340775927164

Epoch: 5| Step: 9
Training loss: 2.027528493471695
Validation loss: 2.4577335027281295

Epoch: 5| Step: 10
Training loss: 2.8150196233305955
Validation loss: 2.458842032274071

Epoch: 5| Step: 11
Training loss: 2.7414659382194557
Validation loss: 2.4548024275901414

Epoch: 146| Step: 0
Training loss: 2.294510435192447
Validation loss: 2.4585178448782816

Epoch: 5| Step: 1
Training loss: 2.620938928980567
Validation loss: 2.4564053496850153

Epoch: 5| Step: 2
Training loss: 2.309764378078156
Validation loss: 2.4557082632772373

Epoch: 5| Step: 3
Training loss: 2.7543454949460835
Validation loss: 2.459969963126834

Epoch: 5| Step: 4
Training loss: 2.022986637978354
Validation loss: 2.4617466519734834

Epoch: 5| Step: 5
Training loss: 2.3109868228128962
Validation loss: 2.4560196719192042

Epoch: 5| Step: 6
Training loss: 3.0383723634970576
Validation loss: 2.458189666450951

Epoch: 5| Step: 7
Training loss: 2.4950479098442604
Validation loss: 2.4562613772516233

Epoch: 5| Step: 8
Training loss: 2.341846354186938
Validation loss: 2.4623338481920958

Epoch: 5| Step: 9
Training loss: 2.802733013936952
Validation loss: 2.4606664720276132

Epoch: 5| Step: 10
Training loss: 2.4557723806441567
Validation loss: 2.4561099056965836

Epoch: 5| Step: 11
Training loss: 2.473658739655132
Validation loss: 2.4501208094732014

Epoch: 147| Step: 0
Training loss: 2.492144067080879
Validation loss: 2.45243264003075

Epoch: 5| Step: 1
Training loss: 2.7936178666855453
Validation loss: 2.4550532236305727

Epoch: 5| Step: 2
Training loss: 2.816344198318469
Validation loss: 2.4521452411127926

Epoch: 5| Step: 3
Training loss: 2.3116722687550464
Validation loss: 2.4550904764549055

Epoch: 5| Step: 4
Training loss: 2.2867215525691744
Validation loss: 2.4545707937138412

Epoch: 5| Step: 5
Training loss: 2.15927453697567
Validation loss: 2.450517572663611

Epoch: 5| Step: 6
Training loss: 3.12630313405638
Validation loss: 2.448620319012558

Epoch: 5| Step: 7
Training loss: 2.296138515600639
Validation loss: 2.4589970684052136

Epoch: 5| Step: 8
Training loss: 2.3069956455396308
Validation loss: 2.4533383788915595

Epoch: 5| Step: 9
Training loss: 2.433851200893518
Validation loss: 2.4644229789724594

Epoch: 5| Step: 10
Training loss: 2.4488845426821273
Validation loss: 2.4573928049470504

Epoch: 5| Step: 11
Training loss: 2.1637300862630604
Validation loss: 2.455730281814748

Epoch: 148| Step: 0
Training loss: 2.859061969236257
Validation loss: 2.4590255636896923

Epoch: 5| Step: 1
Training loss: 2.52724356356922
Validation loss: 2.458960862535896

Epoch: 5| Step: 2
Training loss: 2.6493462097950635
Validation loss: 2.4467338396248546

Epoch: 5| Step: 3
Training loss: 2.514364365022543
Validation loss: 2.4491350320869425

Epoch: 5| Step: 4
Training loss: 2.508466783777725
Validation loss: 2.453691939426373

Epoch: 5| Step: 5
Training loss: 2.6282497453510745
Validation loss: 2.4528883761180693

Epoch: 5| Step: 6
Training loss: 2.6038552568525994
Validation loss: 2.4497906971751737

Epoch: 5| Step: 7
Training loss: 2.127523942895589
Validation loss: 2.451684014263105

Epoch: 5| Step: 8
Training loss: 2.3643159981287543
Validation loss: 2.454129440649195

Epoch: 5| Step: 9
Training loss: 1.9915007361186337
Validation loss: 2.45593646493652

Epoch: 5| Step: 10
Training loss: 2.7580139259501326
Validation loss: 2.4538334291298725

Epoch: 5| Step: 11
Training loss: 1.769105299994741
Validation loss: 2.4490191161805153

Epoch: 149| Step: 0
Training loss: 2.8493588646634582
Validation loss: 2.454853698054417

Epoch: 5| Step: 1
Training loss: 2.5031664822268893
Validation loss: 2.4551659211509436

Epoch: 5| Step: 2
Training loss: 2.617504499225703
Validation loss: 2.4598304882547026

Epoch: 5| Step: 3
Training loss: 2.7032583567628534
Validation loss: 2.4567591643378113

Epoch: 5| Step: 4
Training loss: 2.342259250837175
Validation loss: 2.45190784201245

Epoch: 5| Step: 5
Training loss: 2.536701125496982
Validation loss: 2.4608578502541674

Epoch: 5| Step: 6
Training loss: 2.310450083494095
Validation loss: 2.454309373996092

Epoch: 5| Step: 7
Training loss: 2.2776971611498653
Validation loss: 2.4586113689006663

Epoch: 5| Step: 8
Training loss: 2.3390916433920506
Validation loss: 2.4581112128196914

Epoch: 5| Step: 9
Training loss: 2.452252664177834
Validation loss: 2.456410522166043

Epoch: 5| Step: 10
Training loss: 2.423598039574215
Validation loss: 2.458811536981114

Epoch: 5| Step: 11
Training loss: 2.4334209252186505
Validation loss: 2.4574689571260695

Epoch: 150| Step: 0
Training loss: 2.418171666124666
Validation loss: 2.460011593654367

Epoch: 5| Step: 1
Training loss: 2.6368419929241482
Validation loss: 2.46626891030797

Epoch: 5| Step: 2
Training loss: 2.2596907145778418
Validation loss: 2.46124800484213

Epoch: 5| Step: 3
Training loss: 2.5343625746868077
Validation loss: 2.4666650635696907

Epoch: 5| Step: 4
Training loss: 2.188153305589768
Validation loss: 2.466134045474364

Epoch: 5| Step: 5
Training loss: 3.0892376600665634
Validation loss: 2.463081705383516

Epoch: 5| Step: 6
Training loss: 2.5558719047601897
Validation loss: 2.464703091871593

Epoch: 5| Step: 7
Training loss: 2.3919676955666693
Validation loss: 2.463691103370137

Epoch: 5| Step: 8
Training loss: 2.4404726730658055
Validation loss: 2.4581449296384084

Epoch: 5| Step: 9
Training loss: 2.481060672330271
Validation loss: 2.4634692293664515

Epoch: 5| Step: 10
Training loss: 2.4676366299385633
Validation loss: 2.461821349941773

Epoch: 5| Step: 11
Training loss: 1.9554852681942545
Validation loss: 2.4570395303252104

Epoch: 151| Step: 0
Training loss: 2.2313196593451545
Validation loss: 2.450528737027391

Epoch: 5| Step: 1
Training loss: 2.5886455082676663
Validation loss: 2.461722467763295

Epoch: 5| Step: 2
Training loss: 2.9446515624257956
Validation loss: 2.474829620058313

Epoch: 5| Step: 3
Training loss: 2.5998531373434144
Validation loss: 2.4623460321330892

Epoch: 5| Step: 4
Training loss: 2.0845642141222402
Validation loss: 2.460741937526651

Epoch: 5| Step: 5
Training loss: 2.4368462297140803
Validation loss: 2.460031659567385

Epoch: 5| Step: 6
Training loss: 2.354396088710521
Validation loss: 2.459879071331133

Epoch: 5| Step: 7
Training loss: 2.479486995059057
Validation loss: 2.4663433222798052

Epoch: 5| Step: 8
Training loss: 2.292342404239102
Validation loss: 2.468733791507355

Epoch: 5| Step: 9
Training loss: 2.878888858654644
Validation loss: 2.468575431188671

Epoch: 5| Step: 10
Training loss: 2.659386073805528
Validation loss: 2.467707981438623

Epoch: 5| Step: 11
Training loss: 2.383671039939855
Validation loss: 2.4686313270664058

Epoch: 152| Step: 0
Training loss: 2.301297244903277
Validation loss: 2.4684059893273482

Epoch: 5| Step: 1
Training loss: 3.0905637420662293
Validation loss: 2.4644753351569673

Epoch: 5| Step: 2
Training loss: 2.6758110100420773
Validation loss: 2.4667986147188046

Epoch: 5| Step: 3
Training loss: 2.348022736999797
Validation loss: 2.459295861612094

Epoch: 5| Step: 4
Training loss: 2.0142776362519
Validation loss: 2.4642929654741983

Epoch: 5| Step: 5
Training loss: 2.6548951227770856
Validation loss: 2.4605704543469704

Epoch: 5| Step: 6
Training loss: 2.10175285753528
Validation loss: 2.4627838393961747

Epoch: 5| Step: 7
Training loss: 2.200108620823165
Validation loss: 2.460390254133827

Epoch: 5| Step: 8
Training loss: 2.4126361472497493
Validation loss: 2.461564183041082

Epoch: 5| Step: 9
Training loss: 2.6623450865474374
Validation loss: 2.454934155661755

Epoch: 5| Step: 10
Training loss: 2.647891278048899
Validation loss: 2.46600316140848

Epoch: 5| Step: 11
Training loss: 3.283145320821458
Validation loss: 2.462559862587194

Epoch: 153| Step: 0
Training loss: 2.494842836708699
Validation loss: 2.453636646570703

Epoch: 5| Step: 1
Training loss: 2.069906288458313
Validation loss: 2.4584732999126953

Epoch: 5| Step: 2
Training loss: 2.372133030382269
Validation loss: 2.4661627825618138

Epoch: 5| Step: 3
Training loss: 1.8148041910104198
Validation loss: 2.4558333856522014

Epoch: 5| Step: 4
Training loss: 2.759458315532219
Validation loss: 2.4628516911840888

Epoch: 5| Step: 5
Training loss: 2.494763707991143
Validation loss: 2.4590274462615427

Epoch: 5| Step: 6
Training loss: 2.2098880069716054
Validation loss: 2.459615681402309

Epoch: 5| Step: 7
Training loss: 3.035121058354541
Validation loss: 2.460037955117955

Epoch: 5| Step: 8
Training loss: 2.9327597823203826
Validation loss: 2.4596782247770475

Epoch: 5| Step: 9
Training loss: 2.516282842642545
Validation loss: 2.460573223946139

Epoch: 5| Step: 10
Training loss: 2.6347204703610774
Validation loss: 2.4554160651035715

Epoch: 5| Step: 11
Training loss: 1.7171183383887687
Validation loss: 2.453220082118896

Epoch: 154| Step: 0
Training loss: 2.6893769740319726
Validation loss: 2.4494677654836203

Epoch: 5| Step: 1
Training loss: 2.958018935426017
Validation loss: 2.454395880193042

Epoch: 5| Step: 2
Training loss: 2.2597171972784613
Validation loss: 2.458697535858996

Epoch: 5| Step: 3
Training loss: 2.6346353169222163
Validation loss: 2.462167721000082

Epoch: 5| Step: 4
Training loss: 2.5730698471700157
Validation loss: 2.4690284048910587

Epoch: 5| Step: 5
Training loss: 1.8340257796386146
Validation loss: 2.4559559938131117

Epoch: 5| Step: 6
Training loss: 2.313456363356311
Validation loss: 2.4530276706444814

Epoch: 5| Step: 7
Training loss: 2.49763787256567
Validation loss: 2.4535524518268113

Epoch: 5| Step: 8
Training loss: 2.6178682722926796
Validation loss: 2.4454460473201403

Epoch: 5| Step: 9
Training loss: 2.421551098237349
Validation loss: 2.4577512954557426

Epoch: 5| Step: 10
Training loss: 2.4826652353544167
Validation loss: 2.4607084864066175

Epoch: 5| Step: 11
Training loss: 3.2385152219173423
Validation loss: 2.4619519640476426

Epoch: 155| Step: 0
Training loss: 2.2683159993686823
Validation loss: 2.4686638297965144

Epoch: 5| Step: 1
Training loss: 2.754653288249604
Validation loss: 2.4608269358830714

Epoch: 5| Step: 2
Training loss: 2.6080988887281498
Validation loss: 2.471100915140029

Epoch: 5| Step: 3
Training loss: 2.4211178488207503
Validation loss: 2.466498450770247

Epoch: 5| Step: 4
Training loss: 2.982735548030919
Validation loss: 2.4622870766295444

Epoch: 5| Step: 5
Training loss: 2.861512099460935
Validation loss: 2.4613955238024787

Epoch: 5| Step: 6
Training loss: 2.6799114345036568
Validation loss: 2.4605675595876564

Epoch: 5| Step: 7
Training loss: 2.0080502142991774
Validation loss: 2.462679084246152

Epoch: 5| Step: 8
Training loss: 2.148010211415944
Validation loss: 2.4578164104051434

Epoch: 5| Step: 9
Training loss: 2.28234116050671
Validation loss: 2.459086692000416

Epoch: 5| Step: 10
Training loss: 2.3824266465253907
Validation loss: 2.4563494465621645

Epoch: 5| Step: 11
Training loss: 2.2291168017172724
Validation loss: 2.45059798008082

Epoch: 156| Step: 0
Training loss: 2.5553423774188304
Validation loss: 2.4537924812125835

Epoch: 5| Step: 1
Training loss: 2.39546232806263
Validation loss: 2.466065971802809

Epoch: 5| Step: 2
Training loss: 2.690454322992927
Validation loss: 2.467383944337198

Epoch: 5| Step: 3
Training loss: 1.9916626481956048
Validation loss: 2.4586375716362503

Epoch: 5| Step: 4
Training loss: 2.2492238931441393
Validation loss: 2.473160212756717

Epoch: 5| Step: 5
Training loss: 2.8102127099218137
Validation loss: 2.4620176639267974

Epoch: 5| Step: 6
Training loss: 2.4934382155317327
Validation loss: 2.4581834227441566

Epoch: 5| Step: 7
Training loss: 2.080795522561952
Validation loss: 2.458430330065528

Epoch: 5| Step: 8
Training loss: 2.2493525739200018
Validation loss: 2.4610626996198954

Epoch: 5| Step: 9
Training loss: 3.0817663661521904
Validation loss: 2.4635211078089734

Epoch: 5| Step: 10
Training loss: 2.4420255074008517
Validation loss: 2.4599643862221714

Epoch: 5| Step: 11
Training loss: 3.01328340846314
Validation loss: 2.4622905503351786

Epoch: 157| Step: 0
Training loss: 2.75933761139334
Validation loss: 2.4630100343311128

Epoch: 5| Step: 1
Training loss: 2.543296031136731
Validation loss: 2.4625403417067986

Epoch: 5| Step: 2
Training loss: 2.435902096220081
Validation loss: 2.4604007559468313

Epoch: 5| Step: 3
Training loss: 2.6413517239755357
Validation loss: 2.4588923276722157

Epoch: 5| Step: 4
Training loss: 2.677669733047708
Validation loss: 2.4598444938484887

Epoch: 5| Step: 5
Training loss: 2.914568573316966
Validation loss: 2.4657071591844013

Epoch: 5| Step: 6
Training loss: 2.016399619125939
Validation loss: 2.464038749665833

Epoch: 5| Step: 7
Training loss: 2.702351365708678
Validation loss: 2.461335213630856

Epoch: 5| Step: 8
Training loss: 2.0950671934946548
Validation loss: 2.4523101676333985

Epoch: 5| Step: 9
Training loss: 1.8564799875937088
Validation loss: 2.4584981181525616

Epoch: 5| Step: 10
Training loss: 2.5810899735693362
Validation loss: 2.464468131889105

Epoch: 5| Step: 11
Training loss: 2.8084034225214816
Validation loss: 2.4648424602974703

Epoch: 158| Step: 0
Training loss: 2.295375412298875
Validation loss: 2.462502788083843

Epoch: 5| Step: 1
Training loss: 2.907562615728143
Validation loss: 2.460397323986728

Epoch: 5| Step: 2
Training loss: 2.2701331176041344
Validation loss: 2.4658663482900987

Epoch: 5| Step: 3
Training loss: 2.7149578262622365
Validation loss: 2.4711761665087884

Epoch: 5| Step: 4
Training loss: 2.1915572551303897
Validation loss: 2.4690007512995424

Epoch: 5| Step: 5
Training loss: 2.3655397830666463
Validation loss: 2.471223220062641

Epoch: 5| Step: 6
Training loss: 2.1330380722203492
Validation loss: 2.472193296462106

Epoch: 5| Step: 7
Training loss: 1.998864924673767
Validation loss: 2.4686658941500874

Epoch: 5| Step: 8
Training loss: 2.7335862248206206
Validation loss: 2.4633653804834936

Epoch: 5| Step: 9
Training loss: 2.4795818996238337
Validation loss: 2.4697621459730494

Epoch: 5| Step: 10
Training loss: 2.8939279192612735
Validation loss: 2.461822249806241

Epoch: 5| Step: 11
Training loss: 3.230764096469083
Validation loss: 2.463558664054919

Epoch: 159| Step: 0
Training loss: 1.9458621064481396
Validation loss: 2.4706472398069135

Epoch: 5| Step: 1
Training loss: 2.6495808971729966
Validation loss: 2.469240413516749

Epoch: 5| Step: 2
Training loss: 2.0513403720695695
Validation loss: 2.476172583979549

Epoch: 5| Step: 3
Training loss: 2.4240601556886365
Validation loss: 2.4686117254235715

Epoch: 5| Step: 4
Training loss: 3.0988677879804443
Validation loss: 2.4640347805169407

Epoch: 5| Step: 5
Training loss: 2.9642463655207423
Validation loss: 2.4640698394311307

Epoch: 5| Step: 6
Training loss: 2.8204323698232123
Validation loss: 2.45507037622589

Epoch: 5| Step: 7
Training loss: 1.95901942578314
Validation loss: 2.461626598215647

Epoch: 5| Step: 8
Training loss: 2.2117411632203927
Validation loss: 2.461680938808036

Epoch: 5| Step: 9
Training loss: 1.934664035088563
Validation loss: 2.461490407523005

Epoch: 5| Step: 10
Training loss: 2.95248583784591
Validation loss: 2.4626498446241967

Epoch: 5| Step: 11
Training loss: 2.307864930959085
Validation loss: 2.463962048611934

Epoch: 160| Step: 0
Training loss: 2.7173572349071167
Validation loss: 2.4604824149972297

Epoch: 5| Step: 1
Training loss: 2.660152485714572
Validation loss: 2.464539825031771

Epoch: 5| Step: 2
Training loss: 2.693297498184485
Validation loss: 2.475861186338757

Epoch: 5| Step: 3
Training loss: 2.5300196730561257
Validation loss: 2.4683714387267557

Epoch: 5| Step: 4
Training loss: 1.9262038071011824
Validation loss: 2.4682417197443893

Epoch: 5| Step: 5
Training loss: 2.4329548053612546
Validation loss: 2.4665914667969897

Epoch: 5| Step: 6
Training loss: 3.017057088663939
Validation loss: 2.4700916201990424

Epoch: 5| Step: 7
Training loss: 2.690213452630726
Validation loss: 2.466772438264543

Epoch: 5| Step: 8
Training loss: 2.2324438753153695
Validation loss: 2.463984380437991

Epoch: 5| Step: 9
Training loss: 1.9634940818734345
Validation loss: 2.464267677500436

Epoch: 5| Step: 10
Training loss: 2.164480010703689
Validation loss: 2.4633249922801728

Epoch: 5| Step: 11
Training loss: 2.59753316716744
Validation loss: 2.4643689427247737

Epoch: 161| Step: 0
Training loss: 2.2503307947072826
Validation loss: 2.463415002826207

Epoch: 5| Step: 1
Training loss: 2.600568151753394
Validation loss: 2.4592928764853412

Epoch: 5| Step: 2
Training loss: 2.0099032311370504
Validation loss: 2.457642689905829

Epoch: 5| Step: 3
Training loss: 2.3107525176938712
Validation loss: 2.4641825398176955

Epoch: 5| Step: 4
Training loss: 3.147364431162815
Validation loss: 2.473484938988535

Epoch: 5| Step: 5
Training loss: 2.346824054190451
Validation loss: 2.4660247818677985

Epoch: 5| Step: 6
Training loss: 2.051198106814863
Validation loss: 2.465496827111194

Epoch: 5| Step: 7
Training loss: 3.1701375778620777
Validation loss: 2.4667084920458096

Epoch: 5| Step: 8
Training loss: 2.167174487635266
Validation loss: 2.4588882067930746

Epoch: 5| Step: 9
Training loss: 2.3662351196917433
Validation loss: 2.464000966898875

Epoch: 5| Step: 10
Training loss: 2.697197738555544
Validation loss: 2.469877017984004

Epoch: 5| Step: 11
Training loss: 2.5543740923128317
Validation loss: 2.467462433274913

Epoch: 162| Step: 0
Training loss: 2.516828450524854
Validation loss: 2.465459554235924

Epoch: 5| Step: 1
Training loss: 2.7769648941398075
Validation loss: 2.487087072990709

Epoch: 5| Step: 2
Training loss: 2.233398010494056
Validation loss: 2.478832359907878

Epoch: 5| Step: 3
Training loss: 2.367603227727423
Validation loss: 2.4745624701378435

Epoch: 5| Step: 4
Training loss: 2.3556520507749164
Validation loss: 2.4736978065776363

Epoch: 5| Step: 5
Training loss: 2.7373842084250635
Validation loss: 2.463885556844384

Epoch: 5| Step: 6
Training loss: 2.6022266536812095
Validation loss: 2.463426564426354

Epoch: 5| Step: 7
Training loss: 2.4002732200870165
Validation loss: 2.4603765988835615

Epoch: 5| Step: 8
Training loss: 2.713125879181687
Validation loss: 2.466168462266127

Epoch: 5| Step: 9
Training loss: 2.2872871049712207
Validation loss: 2.467670860719893

Epoch: 5| Step: 10
Training loss: 2.3619361769785128
Validation loss: 2.4724139857719027

Epoch: 5| Step: 11
Training loss: 1.93813977907236
Validation loss: 2.4696920625532757

Epoch: 163| Step: 0
Training loss: 2.6418502041889678
Validation loss: 2.4733539217758636

Epoch: 5| Step: 1
Training loss: 2.50604813445338
Validation loss: 2.472949795526495

Epoch: 5| Step: 2
Training loss: 2.650652743866479
Validation loss: 2.4642473074517857

Epoch: 5| Step: 3
Training loss: 2.411328196189554
Validation loss: 2.464973446294821

Epoch: 5| Step: 4
Training loss: 2.351859951533768
Validation loss: 2.463252830240605

Epoch: 5| Step: 5
Training loss: 2.212577077094156
Validation loss: 2.4748868116833598

Epoch: 5| Step: 6
Training loss: 2.6129162908460635
Validation loss: 2.466382666189003

Epoch: 5| Step: 7
Training loss: 2.371490797204747
Validation loss: 2.476170566003047

Epoch: 5| Step: 8
Training loss: 2.2286169378468697
Validation loss: 2.481046654370587

Epoch: 5| Step: 9
Training loss: 3.0551890085942084
Validation loss: 2.4724111088972682

Epoch: 5| Step: 10
Training loss: 2.5130954131818424
Validation loss: 2.4617449086844494

Epoch: 5| Step: 11
Training loss: 2.0543073295215537
Validation loss: 2.4645511314457442

Epoch: 164| Step: 0
Training loss: 3.0497423031864064
Validation loss: 2.466800660497681

Epoch: 5| Step: 1
Training loss: 2.2919756478751867
Validation loss: 2.4527830100144707

Epoch: 5| Step: 2
Training loss: 2.451667790191137
Validation loss: 2.4610068821082933

Epoch: 5| Step: 3
Training loss: 2.634259559210703
Validation loss: 2.4618821304068246

Epoch: 5| Step: 4
Training loss: 2.992922540724673
Validation loss: 2.4572189352363223

Epoch: 5| Step: 5
Training loss: 2.198406170999342
Validation loss: 2.4521770630075537

Epoch: 5| Step: 6
Training loss: 2.4695929524703284
Validation loss: 2.4660766045354365

Epoch: 5| Step: 7
Training loss: 2.235600808822969
Validation loss: 2.460073244631379

Epoch: 5| Step: 8
Training loss: 2.066120207229937
Validation loss: 2.4614210309723945

Epoch: 5| Step: 9
Training loss: 2.223188983381498
Validation loss: 2.455840994485642

Epoch: 5| Step: 10
Training loss: 2.8369787737348457
Validation loss: 2.4549830096343817

Epoch: 5| Step: 11
Training loss: 2.1302952705929967
Validation loss: 2.4607752670824774

Epoch: 165| Step: 0
Training loss: 2.7680602478184935
Validation loss: 2.457225745366503

Epoch: 5| Step: 1
Training loss: 2.5285420944876895
Validation loss: 2.466254510200296

Epoch: 5| Step: 2
Training loss: 2.447852622202156
Validation loss: 2.5118278966180885

Epoch: 5| Step: 3
Training loss: 3.224934238680075
Validation loss: 2.5190019549364373

Epoch: 5| Step: 4
Training loss: 2.5988396035839245
Validation loss: 2.5069579473826322

Epoch: 5| Step: 5
Training loss: 2.5509263095238746
Validation loss: 2.485620360462443

Epoch: 5| Step: 6
Training loss: 2.5122006252196023
Validation loss: 2.4697905170551158

Epoch: 5| Step: 7
Training loss: 1.5928343592253889
Validation loss: 2.4513596684428673

Epoch: 5| Step: 8
Training loss: 2.1361852115741726
Validation loss: 2.4596526369320344

Epoch: 5| Step: 9
Training loss: 2.7945353349331077
Validation loss: 2.458722648801399

Epoch: 5| Step: 10
Training loss: 2.5734895590894062
Validation loss: 2.462987072533711

Epoch: 5| Step: 11
Training loss: 2.701801659680435
Validation loss: 2.4623996649631805

Epoch: 166| Step: 0
Training loss: 2.297723730108732
Validation loss: 2.4649877370138564

Epoch: 5| Step: 1
Training loss: 2.38810667149235
Validation loss: 2.467269904459877

Epoch: 5| Step: 2
Training loss: 2.5002616745377093
Validation loss: 2.4570188577619456

Epoch: 5| Step: 3
Training loss: 3.2281076643146873
Validation loss: 2.459721146572612

Epoch: 5| Step: 4
Training loss: 1.7925124574704543
Validation loss: 2.443884207966332

Epoch: 5| Step: 5
Training loss: 3.001566636789103
Validation loss: 2.4606874611397154

Epoch: 5| Step: 6
Training loss: 2.6646564078930033
Validation loss: 2.465276129069844

Epoch: 5| Step: 7
Training loss: 2.330805521874002
Validation loss: 2.4644245954059287

Epoch: 5| Step: 8
Training loss: 2.473743458836077
Validation loss: 2.4695881374508275

Epoch: 5| Step: 9
Training loss: 2.3094734800657384
Validation loss: 2.4732860387879914

Epoch: 5| Step: 10
Training loss: 2.416654805998362
Validation loss: 2.4820389190865932

Epoch: 5| Step: 11
Training loss: 1.3246928578795085
Validation loss: 2.47052747984154

Epoch: 167| Step: 0
Training loss: 1.6384773366255323
Validation loss: 2.4678095986127535

Epoch: 5| Step: 1
Training loss: 2.6416738560174613
Validation loss: 2.469004811041599

Epoch: 5| Step: 2
Training loss: 2.356532829167688
Validation loss: 2.457294193302638

Epoch: 5| Step: 3
Training loss: 2.386939505249597
Validation loss: 2.4642878821003436

Epoch: 5| Step: 4
Training loss: 2.45124219816329
Validation loss: 2.4615900294755004

Epoch: 5| Step: 5
Training loss: 2.601796824123624
Validation loss: 2.464463633363073

Epoch: 5| Step: 6
Training loss: 2.4423335153149166
Validation loss: 2.4561491384982417

Epoch: 5| Step: 7
Training loss: 2.381755657023958
Validation loss: 2.4554796783207316

Epoch: 5| Step: 8
Training loss: 2.903176600199899
Validation loss: 2.456603752430091

Epoch: 5| Step: 9
Training loss: 2.1729784021967578
Validation loss: 2.4659393019970355

Epoch: 5| Step: 10
Training loss: 3.059500022881949
Validation loss: 2.459422243465109

Epoch: 5| Step: 11
Training loss: 2.165883093753138
Validation loss: 2.4658234267010553

Epoch: 168| Step: 0
Training loss: 2.658924147942494
Validation loss: 2.4627972876831588

Epoch: 5| Step: 1
Training loss: 2.354158001999115
Validation loss: 2.4646900489793624

Epoch: 5| Step: 2
Training loss: 2.167431024072071
Validation loss: 2.4553491224843276

Epoch: 5| Step: 3
Training loss: 2.653648089456894
Validation loss: 2.462523180631432

Epoch: 5| Step: 4
Training loss: 2.322077532595071
Validation loss: 2.4679131598519866

Epoch: 5| Step: 5
Training loss: 2.32237301153887
Validation loss: 2.4769640656150274

Epoch: 5| Step: 6
Training loss: 2.721454108683026
Validation loss: 2.4717203591832617

Epoch: 5| Step: 7
Training loss: 2.479746507640848
Validation loss: 2.4806225786544096

Epoch: 5| Step: 8
Training loss: 2.504907654756199
Validation loss: 2.4783484540575116

Epoch: 5| Step: 9
Training loss: 2.7020230555422513
Validation loss: 2.4813409748583215

Epoch: 5| Step: 10
Training loss: 2.415144693214085
Validation loss: 2.456951485750689

Epoch: 5| Step: 11
Training loss: 2.3931963686240136
Validation loss: 2.4656174498854617

Epoch: 169| Step: 0
Training loss: 2.4498130649173113
Validation loss: 2.4651580847904433

Epoch: 5| Step: 1
Training loss: 2.4626172342810655
Validation loss: 2.4769508065421046

Epoch: 5| Step: 2
Training loss: 2.579321288946957
Validation loss: 2.4778589291022786

Epoch: 5| Step: 3
Training loss: 1.9803282558108743
Validation loss: 2.4848501962567213

Epoch: 5| Step: 4
Training loss: 2.369246289443439
Validation loss: 2.4821225998121443

Epoch: 5| Step: 5
Training loss: 2.9099646998096356
Validation loss: 2.489578008978562

Epoch: 5| Step: 6
Training loss: 1.8479989441707616
Validation loss: 2.482677771672104

Epoch: 5| Step: 7
Training loss: 2.800606651663187
Validation loss: 2.4938889279144476

Epoch: 5| Step: 8
Training loss: 2.8319399904477267
Validation loss: 2.4843323462001603

Epoch: 5| Step: 9
Training loss: 2.660328774032188
Validation loss: 2.475631615131429

Epoch: 5| Step: 10
Training loss: 2.586857381439487
Validation loss: 2.476110302769943

Epoch: 5| Step: 11
Training loss: 2.4504194951496516
Validation loss: 2.4755910939821892

Epoch: 170| Step: 0
Training loss: 2.5812264947075025
Validation loss: 2.4722148065301015

Epoch: 5| Step: 1
Training loss: 2.1512333104906474
Validation loss: 2.4783543423281875

Epoch: 5| Step: 2
Training loss: 2.5043645906434335
Validation loss: 2.468234970194352

Epoch: 5| Step: 3
Training loss: 2.4820632255302817
Validation loss: 2.465765998670207

Epoch: 5| Step: 4
Training loss: 2.8709841337493027
Validation loss: 2.471227895219445

Epoch: 5| Step: 5
Training loss: 2.4285785069883397
Validation loss: 2.477633307938282

Epoch: 5| Step: 6
Training loss: 2.324412618293182
Validation loss: 2.4818513916539953

Epoch: 5| Step: 7
Training loss: 1.8878198415824696
Validation loss: 2.489575343471672

Epoch: 5| Step: 8
Training loss: 2.5630261881156806
Validation loss: 2.474590679801608

Epoch: 5| Step: 9
Training loss: 2.99209156330781
Validation loss: 2.467178846734245

Epoch: 5| Step: 10
Training loss: 2.517340034016619
Validation loss: 2.4731902138048594

Epoch: 5| Step: 11
Training loss: 2.1270786665199886
Validation loss: 2.481973491025068

Epoch: 171| Step: 0
Training loss: 2.360241257248808
Validation loss: 2.470920714998419

Epoch: 5| Step: 1
Training loss: 2.87611384169391
Validation loss: 2.480226996547991

Epoch: 5| Step: 2
Training loss: 2.861342790064808
Validation loss: 2.477942670806822

Epoch: 5| Step: 3
Training loss: 2.41183200739541
Validation loss: 2.477674228708593

Epoch: 5| Step: 4
Training loss: 2.039975250692395
Validation loss: 2.4775178713174806

Epoch: 5| Step: 5
Training loss: 2.3983318293153433
Validation loss: 2.469524294312322

Epoch: 5| Step: 6
Training loss: 2.5215257896845777
Validation loss: 2.4713534176620673

Epoch: 5| Step: 7
Training loss: 2.361227659702385
Validation loss: 2.478516490746746

Epoch: 5| Step: 8
Training loss: 2.6840883823629835
Validation loss: 2.474866133659564

Epoch: 5| Step: 9
Training loss: 2.496144946413541
Validation loss: 2.4728629380912985

Epoch: 5| Step: 10
Training loss: 2.3681406047485107
Validation loss: 2.4693206739166382

Epoch: 5| Step: 11
Training loss: 1.2277048222100886
Validation loss: 2.4728971849063535

Epoch: 172| Step: 0
Training loss: 2.1886918227188645
Validation loss: 2.4675409680269746

Epoch: 5| Step: 1
Training loss: 2.373961974728849
Validation loss: 2.4663962599920857

Epoch: 5| Step: 2
Training loss: 2.5377687885008675
Validation loss: 2.4736371618233393

Epoch: 5| Step: 3
Training loss: 2.4040979447176043
Validation loss: 2.4796144711485857

Epoch: 5| Step: 4
Training loss: 2.207938812810931
Validation loss: 2.479324954186275

Epoch: 5| Step: 5
Training loss: 2.423306146895103
Validation loss: 2.482737391339879

Epoch: 5| Step: 6
Training loss: 2.440683486765205
Validation loss: 2.4693776189992462

Epoch: 5| Step: 7
Training loss: 2.435330525863509
Validation loss: 2.4808997793485763

Epoch: 5| Step: 8
Training loss: 2.742465534949926
Validation loss: 2.478320419444199

Epoch: 5| Step: 9
Training loss: 2.900020691370318
Validation loss: 2.488364716811224

Epoch: 5| Step: 10
Training loss: 2.4135465101364417
Validation loss: 2.4932479874157067

Epoch: 5| Step: 11
Training loss: 2.5951977102121395
Validation loss: 2.4887361375188575

Epoch: 173| Step: 0
Training loss: 2.465329081638783
Validation loss: 2.486660983251293

Epoch: 5| Step: 1
Training loss: 1.8670774930206988
Validation loss: 2.4899465915717554

Epoch: 5| Step: 2
Training loss: 2.1875954743394828
Validation loss: 2.465253339491113

Epoch: 5| Step: 3
Training loss: 2.616668011934295
Validation loss: 2.478770137512055

Epoch: 5| Step: 4
Training loss: 2.3521262472161264
Validation loss: 2.484012047428083

Epoch: 5| Step: 5
Training loss: 3.265418671291006
Validation loss: 2.482140477856616

Epoch: 5| Step: 6
Training loss: 2.544873439887085
Validation loss: 2.4788520630737807

Epoch: 5| Step: 7
Training loss: 2.538858916117094
Validation loss: 2.481431913233339

Epoch: 5| Step: 8
Training loss: 1.983806378453397
Validation loss: 2.4795105973399503

Epoch: 5| Step: 9
Training loss: 2.5606784393939055
Validation loss: 2.484385222487941

Epoch: 5| Step: 10
Training loss: 2.7206837584277968
Validation loss: 2.4791876460771824

Epoch: 5| Step: 11
Training loss: 2.1075931511290396
Validation loss: 2.481133295384641

Epoch: 174| Step: 0
Training loss: 2.263653337931003
Validation loss: 2.476333727935124

Epoch: 5| Step: 1
Training loss: 2.185979478172588
Validation loss: 2.4760582868900634

Epoch: 5| Step: 2
Training loss: 2.4113414453200046
Validation loss: 2.4726294786752643

Epoch: 5| Step: 3
Training loss: 2.793034052585573
Validation loss: 2.478087504240732

Epoch: 5| Step: 4
Training loss: 2.6061723436820414
Validation loss: 2.472279719349062

Epoch: 5| Step: 5
Training loss: 2.8812099230579395
Validation loss: 2.4779441180605417

Epoch: 5| Step: 6
Training loss: 2.7452342613331138
Validation loss: 2.4917042741095674

Epoch: 5| Step: 7
Training loss: 2.380851063389153
Validation loss: 2.500154707015948

Epoch: 5| Step: 8
Training loss: 1.7380947055803393
Validation loss: 2.4972195461983984

Epoch: 5| Step: 9
Training loss: 2.792631627791037
Validation loss: 2.5000541641407783

Epoch: 5| Step: 10
Training loss: 2.5653221592841526
Validation loss: 2.4958448728720257

Epoch: 5| Step: 11
Training loss: 1.2424780547815601
Validation loss: 2.478778958391123

Epoch: 175| Step: 0
Training loss: 1.7890089934897564
Validation loss: 2.469801087497574

Epoch: 5| Step: 1
Training loss: 2.223274238902981
Validation loss: 2.479887654479867

Epoch: 5| Step: 2
Training loss: 2.9204401583295265
Validation loss: 2.4660651661383053

Epoch: 5| Step: 3
Training loss: 2.6397732596851626
Validation loss: 2.471562489665413

Epoch: 5| Step: 4
Training loss: 1.7583860690749544
Validation loss: 2.4774084481531116

Epoch: 5| Step: 5
Training loss: 2.239199571480119
Validation loss: 2.4827496031918117

Epoch: 5| Step: 6
Training loss: 2.503403064079997
Validation loss: 2.471885757969443

Epoch: 5| Step: 7
Training loss: 2.4766064949821054
Validation loss: 2.4663902162971802

Epoch: 5| Step: 8
Training loss: 2.8805167301528436
Validation loss: 2.4712446099381555

Epoch: 5| Step: 9
Training loss: 2.3782164226508713
Validation loss: 2.470157037226885

Epoch: 5| Step: 10
Training loss: 2.942187850068852
Validation loss: 2.4768573092132793

Epoch: 5| Step: 11
Training loss: 2.7170811935609036
Validation loss: 2.4711679214922366

Epoch: 176| Step: 0
Training loss: 2.6857558512442408
Validation loss: 2.476571517270075

Epoch: 5| Step: 1
Training loss: 1.9623386195216195
Validation loss: 2.4816712275741337

Epoch: 5| Step: 2
Training loss: 2.4311118903072644
Validation loss: 2.487286453170248

Epoch: 5| Step: 3
Training loss: 2.805662277969977
Validation loss: 2.518439521143411

Epoch: 5| Step: 4
Training loss: 2.859773034913481
Validation loss: 2.5061481494772093

Epoch: 5| Step: 5
Training loss: 2.6272156764997328
Validation loss: 2.509315848217481

Epoch: 5| Step: 6
Training loss: 2.392488040395918
Validation loss: 2.503385095818847

Epoch: 5| Step: 7
Training loss: 2.0660770492957683
Validation loss: 2.4888772854458825

Epoch: 5| Step: 8
Training loss: 2.5794677012270544
Validation loss: 2.488571653082964

Epoch: 5| Step: 9
Training loss: 2.1313671762243303
Validation loss: 2.477241392977425

Epoch: 5| Step: 10
Training loss: 2.4523973296875634
Validation loss: 2.470353708198973

Epoch: 5| Step: 11
Training loss: 3.423397195945287
Validation loss: 2.4745578012810183

Epoch: 177| Step: 0
Training loss: 2.0416808873933423
Validation loss: 2.4767416519204555

Epoch: 5| Step: 1
Training loss: 2.522624354250928
Validation loss: 2.478640854529852

Epoch: 5| Step: 2
Training loss: 2.5453082923549957
Validation loss: 2.4806200677167984

Epoch: 5| Step: 3
Training loss: 2.4078622346230842
Validation loss: 2.4788629174815746

Epoch: 5| Step: 4
Training loss: 3.0074532430538756
Validation loss: 2.4758089926888567

Epoch: 5| Step: 5
Training loss: 2.47408201745032
Validation loss: 2.4788020263844572

Epoch: 5| Step: 6
Training loss: 2.3109218264158518
Validation loss: 2.4722163616127624

Epoch: 5| Step: 7
Training loss: 2.2215211782396835
Validation loss: 2.4737542372723964

Epoch: 5| Step: 8
Training loss: 2.157803072898431
Validation loss: 2.4696961251813407

Epoch: 5| Step: 9
Training loss: 2.476292929433689
Validation loss: 2.4740568496095423

Epoch: 5| Step: 10
Training loss: 2.896702661089449
Validation loss: 2.46251398282672

Epoch: 5| Step: 11
Training loss: 3.344566174302298
Validation loss: 2.474857553724181

Epoch: 178| Step: 0
Training loss: 2.6674024838299504
Validation loss: 2.462581852135075

Epoch: 5| Step: 1
Training loss: 2.7115900754828846
Validation loss: 2.474707208358967

Epoch: 5| Step: 2
Training loss: 2.577590794559761
Validation loss: 2.4822834575569095

Epoch: 5| Step: 3
Training loss: 2.8331892500500424
Validation loss: 2.4742586408683094

Epoch: 5| Step: 4
Training loss: 2.450839295944783
Validation loss: 2.4793766410981832

Epoch: 5| Step: 5
Training loss: 1.9105898749130434
Validation loss: 2.4816448517266054

Epoch: 5| Step: 6
Training loss: 2.4107909518368142
Validation loss: 2.4873547228845454

Epoch: 5| Step: 7
Training loss: 2.357998073547974
Validation loss: 2.4818259903875126

Epoch: 5| Step: 8
Training loss: 2.4365949662283763
Validation loss: 2.479292258653059

Epoch: 5| Step: 9
Training loss: 2.4262758693662185
Validation loss: 2.477715297150219

Epoch: 5| Step: 10
Training loss: 2.3802695791655055
Validation loss: 2.4673107233213174

Epoch: 5| Step: 11
Training loss: 1.7653878651942738
Validation loss: 2.4749893783893717

Epoch: 179| Step: 0
Training loss: 2.0811398084641666
Validation loss: 2.4624990080816915

Epoch: 5| Step: 1
Training loss: 2.09983658381824
Validation loss: 2.4744396999591807

Epoch: 5| Step: 2
Training loss: 2.4558068455653896
Validation loss: 2.4716455524307563

Epoch: 5| Step: 3
Training loss: 2.5588303318281462
Validation loss: 2.4702489221809434

Epoch: 5| Step: 4
Training loss: 2.369871726447846
Validation loss: 2.470308154152786

Epoch: 5| Step: 5
Training loss: 2.610597335599444
Validation loss: 2.475907392529421

Epoch: 5| Step: 6
Training loss: 2.6884546580393986
Validation loss: 2.4704598449027966

Epoch: 5| Step: 7
Training loss: 1.8994457565840621
Validation loss: 2.4747779666322223

Epoch: 5| Step: 8
Training loss: 2.863009118526966
Validation loss: 2.4690399402057217

Epoch: 5| Step: 9
Training loss: 2.536517842956986
Validation loss: 2.4750926352032505

Epoch: 5| Step: 10
Training loss: 3.0500848539461933
Validation loss: 2.466047816089352

Epoch: 5| Step: 11
Training loss: 0.8202747881258896
Validation loss: 2.4739285249754723

Epoch: 180| Step: 0
Training loss: 2.084754980130508
Validation loss: 2.4788887899340746

Epoch: 5| Step: 1
Training loss: 2.4846607739653574
Validation loss: 2.48273129139331

Epoch: 5| Step: 2
Training loss: 2.5526167850510704
Validation loss: 2.486730354738218

Epoch: 5| Step: 3
Training loss: 2.4930604942317727
Validation loss: 2.491820420961611

Epoch: 5| Step: 4
Training loss: 2.6713245561019354
Validation loss: 2.4746793613106517

Epoch: 5| Step: 5
Training loss: 1.8968276116926426
Validation loss: 2.491361124278568

Epoch: 5| Step: 6
Training loss: 2.78964646153793
Validation loss: 2.4853121911148666

Epoch: 5| Step: 7
Training loss: 2.257159709608825
Validation loss: 2.4892526883191914

Epoch: 5| Step: 8
Training loss: 2.826993768707414
Validation loss: 2.488499151297546

Epoch: 5| Step: 9
Training loss: 2.272961124180018
Validation loss: 2.467470018326438

Epoch: 5| Step: 10
Training loss: 2.6007476538622165
Validation loss: 2.4775646440455596

Epoch: 5| Step: 11
Training loss: 2.84156967251119
Validation loss: 2.485966860207978

Epoch: 181| Step: 0
Training loss: 2.0128198073381394
Validation loss: 2.480106962157195

Epoch: 5| Step: 1
Training loss: 2.82609670838297
Validation loss: 2.484303423412253

Epoch: 5| Step: 2
Training loss: 2.4135467077034396
Validation loss: 2.482583421622533

Epoch: 5| Step: 3
Training loss: 1.9839209089588603
Validation loss: 2.4883156918268474

Epoch: 5| Step: 4
Training loss: 3.054987509744088
Validation loss: 2.49045581905136

Epoch: 5| Step: 5
Training loss: 2.407362643529416
Validation loss: 2.4783582264146653

Epoch: 5| Step: 6
Training loss: 2.127924757623936
Validation loss: 2.4819514971711576

Epoch: 5| Step: 7
Training loss: 2.2629698881875893
Validation loss: 2.4825478598586983

Epoch: 5| Step: 8
Training loss: 2.593542067684298
Validation loss: 2.488948091893689

Epoch: 5| Step: 9
Training loss: 2.614724529045645
Validation loss: 2.479385207398564

Epoch: 5| Step: 10
Training loss: 2.361350539789706
Validation loss: 2.4813139229010495

Epoch: 5| Step: 11
Training loss: 2.9473720806859176
Validation loss: 2.4832830768552

Epoch: 182| Step: 0
Training loss: 2.567981903998768
Validation loss: 2.483523324818114

Epoch: 5| Step: 1
Training loss: 2.792570157719392
Validation loss: 2.4728751906719943

Epoch: 5| Step: 2
Training loss: 2.4383568847033397
Validation loss: 2.486602388504439

Epoch: 5| Step: 3
Training loss: 2.1624178920438903
Validation loss: 2.4761435217497065

Epoch: 5| Step: 4
Training loss: 2.835927747809506
Validation loss: 2.4716984810799816

Epoch: 5| Step: 5
Training loss: 2.2228735631489664
Validation loss: 2.4741525004970573

Epoch: 5| Step: 6
Training loss: 2.3248322005888458
Validation loss: 2.4721133905260877

Epoch: 5| Step: 7
Training loss: 2.4225203115834097
Validation loss: 2.4765044084187906

Epoch: 5| Step: 8
Training loss: 2.200356874997748
Validation loss: 2.4754467585839155

Epoch: 5| Step: 9
Training loss: 2.201087063202213
Validation loss: 2.4817631425501543

Epoch: 5| Step: 10
Training loss: 2.5578380138357457
Validation loss: 2.476311158463714

Epoch: 5| Step: 11
Training loss: 3.882972391624258
Validation loss: 2.476107221566626

Epoch: 183| Step: 0
Training loss: 2.5462437883094373
Validation loss: 2.484171909052468

Epoch: 5| Step: 1
Training loss: 1.982282240887468
Validation loss: 2.4910470551475066

Epoch: 5| Step: 2
Training loss: 2.8721754047735546
Validation loss: 2.4803852818179895

Epoch: 5| Step: 3
Training loss: 2.8115805076538867
Validation loss: 2.492213451365978

Epoch: 5| Step: 4
Training loss: 2.674309407869814
Validation loss: 2.484916228194219

Epoch: 5| Step: 5
Training loss: 2.6245836200180013
Validation loss: 2.4832778403356506

Epoch: 5| Step: 6
Training loss: 2.3259523225655645
Validation loss: 2.4825823732240453

Epoch: 5| Step: 7
Training loss: 2.184745580321064
Validation loss: 2.4825768191055877

Epoch: 5| Step: 8
Training loss: 2.380818818070018
Validation loss: 2.4746724286033737

Epoch: 5| Step: 9
Training loss: 2.4869658198506115
Validation loss: 2.4769151438558543

Epoch: 5| Step: 10
Training loss: 2.0509945713084825
Validation loss: 2.4775712839772175

Epoch: 5| Step: 11
Training loss: 2.46713860954369
Validation loss: 2.480309805195521

Epoch: 184| Step: 0
Training loss: 2.6776557537977035
Validation loss: 2.483801429643554

Epoch: 5| Step: 1
Training loss: 2.5717982488144324
Validation loss: 2.4798156999997336

Epoch: 5| Step: 2
Training loss: 2.109708632469426
Validation loss: 2.487075046202336

Epoch: 5| Step: 3
Training loss: 2.1520268090928707
Validation loss: 2.488867251043362

Epoch: 5| Step: 4
Training loss: 2.350022733862464
Validation loss: 2.4702197379788706

Epoch: 5| Step: 5
Training loss: 2.4324141042036023
Validation loss: 2.4974289906156635

Epoch: 5| Step: 6
Training loss: 2.517444118724318
Validation loss: 2.487858994456021

Epoch: 5| Step: 7
Training loss: 1.8080423062320412
Validation loss: 2.4819584575765776

Epoch: 5| Step: 8
Training loss: 2.873910614609327
Validation loss: 2.4795069274023502

Epoch: 5| Step: 9
Training loss: 2.0015712764645848
Validation loss: 2.4722317797824926

Epoch: 5| Step: 10
Training loss: 2.9130072025166815
Validation loss: 2.4801025120252205

Epoch: 5| Step: 11
Training loss: 3.5794496707844226
Validation loss: 2.4741701269676994

Epoch: 185| Step: 0
Training loss: 2.6437235152268435
Validation loss: 2.47798918696249

Epoch: 5| Step: 1
Training loss: 2.302714081627077
Validation loss: 2.4763465450246973

Epoch: 5| Step: 2
Training loss: 2.5235956573526357
Validation loss: 2.480534530127873

Epoch: 5| Step: 3
Training loss: 2.6168939682059187
Validation loss: 2.4845097243368923

Epoch: 5| Step: 4
Training loss: 2.121867732399361
Validation loss: 2.4741719538498437

Epoch: 5| Step: 5
Training loss: 2.4756822435655907
Validation loss: 2.476020874098758

Epoch: 5| Step: 6
Training loss: 2.9638729792982947
Validation loss: 2.4906304018352134

Epoch: 5| Step: 7
Training loss: 2.0600336529473577
Validation loss: 2.4816630214291204

Epoch: 5| Step: 8
Training loss: 2.3723356460546436
Validation loss: 2.495699632998165

Epoch: 5| Step: 9
Training loss: 2.6042259718182064
Validation loss: 2.490703515634414

Epoch: 5| Step: 10
Training loss: 2.0024482524152964
Validation loss: 2.4838167159065514

Epoch: 5| Step: 11
Training loss: 2.6684943333537574
Validation loss: 2.4873113413695394

Epoch: 186| Step: 0
Training loss: 2.171122029365011
Validation loss: 2.486652318171182

Epoch: 5| Step: 1
Training loss: 2.702694279747803
Validation loss: 2.4853038770776372

Epoch: 5| Step: 2
Training loss: 2.4036945794075
Validation loss: 2.491428213739419

Epoch: 5| Step: 3
Training loss: 2.806436062700064
Validation loss: 2.483109457743636

Epoch: 5| Step: 4
Training loss: 2.458126921528119
Validation loss: 2.4918223863974047

Epoch: 5| Step: 5
Training loss: 2.6369289738243586
Validation loss: 2.5034583688482877

Epoch: 5| Step: 6
Training loss: 2.183719201484897
Validation loss: 2.4830838173018726

Epoch: 5| Step: 7
Training loss: 2.4897719009896306
Validation loss: 2.488911778894914

Epoch: 5| Step: 8
Training loss: 2.034805355464066
Validation loss: 2.482065010581641

Epoch: 5| Step: 9
Training loss: 2.5256672270335665
Validation loss: 2.4866582946427607

Epoch: 5| Step: 10
Training loss: 2.3151147854321525
Validation loss: 2.484274488291228

Epoch: 5| Step: 11
Training loss: 2.4214442700706247
Validation loss: 2.4845055739759347

Epoch: 187| Step: 0
Training loss: 2.3347158650403674
Validation loss: 2.470783747989784

Epoch: 5| Step: 1
Training loss: 2.72760628119933
Validation loss: 2.48716471247869

Epoch: 5| Step: 2
Training loss: 2.170370335422805
Validation loss: 2.4871902828917527

Epoch: 5| Step: 3
Training loss: 2.400424792525338
Validation loss: 2.477229147956737

Epoch: 5| Step: 4
Training loss: 2.6354906174809885
Validation loss: 2.468571165504554

Epoch: 5| Step: 5
Training loss: 2.848452693461348
Validation loss: 2.476296122732974

Epoch: 5| Step: 6
Training loss: 1.8332387365153575
Validation loss: 2.46764066978323

Epoch: 5| Step: 7
Training loss: 2.316942947251635
Validation loss: 2.467176420763912

Epoch: 5| Step: 8
Training loss: 1.9069686535567891
Validation loss: 2.472751525200422

Epoch: 5| Step: 9
Training loss: 2.9244500711046153
Validation loss: 2.467495712313909

Epoch: 5| Step: 10
Training loss: 2.647141581220277
Validation loss: 2.4700614206435554

Epoch: 5| Step: 11
Training loss: 1.9280799007242049
Validation loss: 2.4747715158993833

Epoch: 188| Step: 0
Training loss: 2.6533210313534177
Validation loss: 2.4801642644033297

Epoch: 5| Step: 1
Training loss: 2.662858260143965
Validation loss: 2.484906921383302

Epoch: 5| Step: 2
Training loss: 2.432858375895882
Validation loss: 2.4851274570434843

Epoch: 5| Step: 3
Training loss: 2.369372979930277
Validation loss: 2.4937805895733796

Epoch: 5| Step: 4
Training loss: 2.8819055965721208
Validation loss: 2.489562091710172

Epoch: 5| Step: 5
Training loss: 2.3142848987847024
Validation loss: 2.4827657922056776

Epoch: 5| Step: 6
Training loss: 2.1393087753847007
Validation loss: 2.4953914444326513

Epoch: 5| Step: 7
Training loss: 2.4581745441253733
Validation loss: 2.4933178529624307

Epoch: 5| Step: 8
Training loss: 2.2015668621250146
Validation loss: 2.502493643858996

Epoch: 5| Step: 9
Training loss: 1.9872324637520982
Validation loss: 2.492545892343343

Epoch: 5| Step: 10
Training loss: 2.626760165136689
Validation loss: 2.487432601629056

Epoch: 5| Step: 11
Training loss: 2.649115821596041
Validation loss: 2.4916805441309386

Epoch: 189| Step: 0
Training loss: 2.2760651075842278
Validation loss: 2.488068959329215

Epoch: 5| Step: 1
Training loss: 2.1928716917340814
Validation loss: 2.494785477398951

Epoch: 5| Step: 2
Training loss: 2.2287112926602886
Validation loss: 2.4953850788349294

Epoch: 5| Step: 3
Training loss: 2.231818063266626
Validation loss: 2.4970225166855697

Epoch: 5| Step: 4
Training loss: 2.4670603317720117
Validation loss: 2.494370041952779

Epoch: 5| Step: 5
Training loss: 3.0398152260090363
Validation loss: 2.5005442583199513

Epoch: 5| Step: 6
Training loss: 2.6748549591387567
Validation loss: 2.5167668635799108

Epoch: 5| Step: 7
Training loss: 2.1275583462273278
Validation loss: 2.5277649235194795

Epoch: 5| Step: 8
Training loss: 2.376904376618542
Validation loss: 2.5052833836681887

Epoch: 5| Step: 9
Training loss: 2.6030405788768527
Validation loss: 2.505173452270632

Epoch: 5| Step: 10
Training loss: 2.519000708737033
Validation loss: 2.4913298068715326

Epoch: 5| Step: 11
Training loss: 2.590233958955771
Validation loss: 2.489772954340668

Epoch: 190| Step: 0
Training loss: 1.8861142408332208
Validation loss: 2.4886994581839996

Epoch: 5| Step: 1
Training loss: 2.4648819571313254
Validation loss: 2.4898556968642533

Epoch: 5| Step: 2
Training loss: 2.0388483980104013
Validation loss: 2.483643102100532

Epoch: 5| Step: 3
Training loss: 2.7740724508371977
Validation loss: 2.4823137165220945

Epoch: 5| Step: 4
Training loss: 2.744860441346383
Validation loss: 2.481996729451314

Epoch: 5| Step: 5
Training loss: 2.781949673200867
Validation loss: 2.4782154732184036

Epoch: 5| Step: 6
Training loss: 2.4689008449391228
Validation loss: 2.4847639966916506

Epoch: 5| Step: 7
Training loss: 2.037572206493204
Validation loss: 2.4789415519085

Epoch: 5| Step: 8
Training loss: 2.3307612297935707
Validation loss: 2.4730882674027885

Epoch: 5| Step: 9
Training loss: 2.3438596572654564
Validation loss: 2.478170941581616

Epoch: 5| Step: 10
Training loss: 2.661110145402316
Validation loss: 2.477666795179741

Epoch: 5| Step: 11
Training loss: 2.8892700318832563
Validation loss: 2.478592579085095

Epoch: 191| Step: 0
Training loss: 2.466084840390217
Validation loss: 2.4856007109483143

Epoch: 5| Step: 1
Training loss: 2.821264724388792
Validation loss: 2.5033055228082444

Epoch: 5| Step: 2
Training loss: 1.82319602642254
Validation loss: 2.516223634846079

Epoch: 5| Step: 3
Training loss: 3.103799239223959
Validation loss: 2.5167313249810324

Epoch: 5| Step: 4
Training loss: 2.2541844346714703
Validation loss: 2.514631754501951

Epoch: 5| Step: 5
Training loss: 2.5452940545019658
Validation loss: 2.518062326978933

Epoch: 5| Step: 6
Training loss: 2.1201857841706735
Validation loss: 2.5014348442487297

Epoch: 5| Step: 7
Training loss: 2.3811120140966775
Validation loss: 2.471992930007647

Epoch: 5| Step: 8
Training loss: 2.444409285880123
Validation loss: 2.4782581641524093

Epoch: 5| Step: 9
Training loss: 2.551108183070831
Validation loss: 2.478081863880946

Epoch: 5| Step: 10
Training loss: 2.5120812328045328
Validation loss: 2.4807554204495164

Epoch: 5| Step: 11
Training loss: 1.2470516242267733
Validation loss: 2.480653038087506

Epoch: 192| Step: 0
Training loss: 2.4410153984012335
Validation loss: 2.46676346974471

Epoch: 5| Step: 1
Training loss: 2.856886150544479
Validation loss: 2.473226681318037

Epoch: 5| Step: 2
Training loss: 2.2989031083445854
Validation loss: 2.4641452450466708

Epoch: 5| Step: 3
Training loss: 2.5584658059315255
Validation loss: 2.474721053488071

Epoch: 5| Step: 4
Training loss: 2.3481334131540437
Validation loss: 2.465895177126463

Epoch: 5| Step: 5
Training loss: 2.29825337889394
Validation loss: 2.4912375908909588

Epoch: 5| Step: 6
Training loss: 2.4044187438967857
Validation loss: 2.4951593344476235

Epoch: 5| Step: 7
Training loss: 1.8213801056961336
Validation loss: 2.471358964846381

Epoch: 5| Step: 8
Training loss: 2.7102037475681637
Validation loss: 2.4903321051275653

Epoch: 5| Step: 9
Training loss: 2.1125931668475215
Validation loss: 2.487716455003616

Epoch: 5| Step: 10
Training loss: 2.7363659395250255
Validation loss: 2.4864898293435025

Epoch: 5| Step: 11
Training loss: 2.156697323604332
Validation loss: 2.483224014355184

Epoch: 193| Step: 0
Training loss: 2.324353844058369
Validation loss: 2.4824809125657556

Epoch: 5| Step: 1
Training loss: 2.8302371741884467
Validation loss: 2.487153037552571

Epoch: 5| Step: 2
Training loss: 2.4614102348528335
Validation loss: 2.4799277028414295

Epoch: 5| Step: 3
Training loss: 2.8240967614722337
Validation loss: 2.495259451496343

Epoch: 5| Step: 4
Training loss: 1.9537964544063577
Validation loss: 2.499065033602626

Epoch: 5| Step: 5
Training loss: 2.0639978229747795
Validation loss: 2.504573703305872

Epoch: 5| Step: 6
Training loss: 2.9902975223982207
Validation loss: 2.507821658425726

Epoch: 5| Step: 7
Training loss: 2.3369932308727424
Validation loss: 2.4976189201778256

Epoch: 5| Step: 8
Training loss: 2.045184653230144
Validation loss: 2.510771601002775

Epoch: 5| Step: 9
Training loss: 2.4025268484911773
Validation loss: 2.515647907824102

Epoch: 5| Step: 10
Training loss: 2.354140379992437
Validation loss: 2.507603518769682

Epoch: 5| Step: 11
Training loss: 0.7695705190795479
Validation loss: 2.503669787115696

Epoch: 194| Step: 0
Training loss: 2.391216429311722
Validation loss: 2.4962941240050878

Epoch: 5| Step: 1
Training loss: 2.334023600611804
Validation loss: 2.486132833597862

Epoch: 5| Step: 2
Training loss: 2.6011775651311844
Validation loss: 2.491140984949148

Epoch: 5| Step: 3
Training loss: 2.7638212022102415
Validation loss: 2.485288114306182

Epoch: 5| Step: 4
Training loss: 2.862367325419196
Validation loss: 2.5024237288184064

Epoch: 5| Step: 5
Training loss: 1.8498632303148717
Validation loss: 2.50091942728486

Epoch: 5| Step: 6
Training loss: 2.4661648892905426
Validation loss: 2.4972742320910433

Epoch: 5| Step: 7
Training loss: 2.4043827490883745
Validation loss: 2.492568506117567

Epoch: 5| Step: 8
Training loss: 2.678458418505747
Validation loss: 2.5089066120157026

Epoch: 5| Step: 9
Training loss: 2.485000627544485
Validation loss: 2.4965581486787753

Epoch: 5| Step: 10
Training loss: 1.3984696794782006
Validation loss: 2.5098509045471826

Epoch: 5| Step: 11
Training loss: 1.4884354318768191
Validation loss: 2.5182753081698466

Epoch: 195| Step: 0
Training loss: 2.285674520555369
Validation loss: 2.519426360398955

Epoch: 5| Step: 1
Training loss: 2.431378331266485
Validation loss: 2.5172202716906504

Epoch: 5| Step: 2
Training loss: 2.741502464347593
Validation loss: 2.501698925992932

Epoch: 5| Step: 3
Training loss: 2.438660369899679
Validation loss: 2.5115502806742773

Epoch: 5| Step: 4
Training loss: 2.2585583151297635
Validation loss: 2.516499393053523

Epoch: 5| Step: 5
Training loss: 2.5460693395661207
Validation loss: 2.5279380653783057

Epoch: 5| Step: 6
Training loss: 2.0742681752438834
Validation loss: 2.5156983232573276

Epoch: 5| Step: 7
Training loss: 2.3921774018695565
Validation loss: 2.4966803405214955

Epoch: 5| Step: 8
Training loss: 3.028393057817013
Validation loss: 2.5027097242457277

Epoch: 5| Step: 9
Training loss: 2.4062200767340927
Validation loss: 2.4918481541936752

Epoch: 5| Step: 10
Training loss: 2.1142648040444447
Validation loss: 2.487614165454613

Epoch: 5| Step: 11
Training loss: 1.3643159191779548
Validation loss: 2.496475512901147

Epoch: 196| Step: 0
Training loss: 2.3164730480527367
Validation loss: 2.4934873589222732

Epoch: 5| Step: 1
Training loss: 2.632221280057492
Validation loss: 2.4926595051551956

Epoch: 5| Step: 2
Training loss: 2.6369934391060177
Validation loss: 2.496065317707379

Epoch: 5| Step: 3
Training loss: 2.0592981396515686
Validation loss: 2.4929422333894182

Epoch: 5| Step: 4
Training loss: 2.2569525641922916
Validation loss: 2.5016744926524224

Epoch: 5| Step: 5
Training loss: 2.2610147859747047
Validation loss: 2.4850359803140023

Epoch: 5| Step: 6
Training loss: 2.7605162596580417
Validation loss: 2.4838123684101387

Epoch: 5| Step: 7
Training loss: 1.8172192138633498
Validation loss: 2.506563895839874

Epoch: 5| Step: 8
Training loss: 3.0070154496390065
Validation loss: 2.5013868180868344

Epoch: 5| Step: 9
Training loss: 2.324284708064889
Validation loss: 2.5165522112514274

Epoch: 5| Step: 10
Training loss: 2.137270896189229
Validation loss: 2.497131398627037

Epoch: 5| Step: 11
Training loss: 2.812395390578426
Validation loss: 2.5245419536883955

Epoch: 197| Step: 0
Training loss: 2.3932484711431252
Validation loss: 2.513330479389648

Epoch: 5| Step: 1
Training loss: 2.703708740004209
Validation loss: 2.4966210816665093

Epoch: 5| Step: 2
Training loss: 2.2783356955050222
Validation loss: 2.4980234597873148

Epoch: 5| Step: 3
Training loss: 2.822322224508151
Validation loss: 2.496622931910542

Epoch: 5| Step: 4
Training loss: 2.4305946153196745
Validation loss: 2.4880405551149254

Epoch: 5| Step: 5
Training loss: 2.9543735347427016
Validation loss: 2.5002818703695335

Epoch: 5| Step: 6
Training loss: 1.8956225840486802
Validation loss: 2.49480482959394

Epoch: 5| Step: 7
Training loss: 2.3002842105576478
Validation loss: 2.497060223497983

Epoch: 5| Step: 8
Training loss: 2.3199281727425465
Validation loss: 2.4895439876389434

Epoch: 5| Step: 9
Training loss: 1.9412713416523657
Validation loss: 2.4997509117014163

Epoch: 5| Step: 10
Training loss: 2.335544514850267
Validation loss: 2.498265379892811

Epoch: 5| Step: 11
Training loss: 1.5390209831408574
Validation loss: 2.492606503491928

Epoch: 198| Step: 0
Training loss: 1.9834750079462098
Validation loss: 2.5064705559009197

Epoch: 5| Step: 1
Training loss: 2.720147573871483
Validation loss: 2.5230604624280244

Epoch: 5| Step: 2
Training loss: 2.2720418624782237
Validation loss: 2.5244140231478682

Epoch: 5| Step: 3
Training loss: 2.503585913961985
Validation loss: 2.520873874674585

Epoch: 5| Step: 4
Training loss: 2.629602937931036
Validation loss: 2.5090061607090086

Epoch: 5| Step: 5
Training loss: 2.8173605562649953
Validation loss: 2.5201969893603495

Epoch: 5| Step: 6
Training loss: 2.348782642050381
Validation loss: 2.50798157491398

Epoch: 5| Step: 7
Training loss: 2.573551629876149
Validation loss: 2.4926042736431318

Epoch: 5| Step: 8
Training loss: 2.1569755549772203
Validation loss: 2.5099195381201405

Epoch: 5| Step: 9
Training loss: 2.455009659788935
Validation loss: 2.4907564421287134

Epoch: 5| Step: 10
Training loss: 1.965784776558778
Validation loss: 2.5002437730988882

Epoch: 5| Step: 11
Training loss: 3.4261971268023945
Validation loss: 2.4983068136581004

Epoch: 199| Step: 0
Training loss: 2.3173770510621594
Validation loss: 2.5124212066827587

Epoch: 5| Step: 1
Training loss: 1.9193392516682362
Validation loss: 2.517951040021221

Epoch: 5| Step: 2
Training loss: 2.682646338461017
Validation loss: 2.514286107498944

Epoch: 5| Step: 3
Training loss: 2.9502740247795733
Validation loss: 2.5218664264493826

Epoch: 5| Step: 4
Training loss: 2.0847048885882296
Validation loss: 2.5318711739182156

Epoch: 5| Step: 5
Training loss: 2.438553313551091
Validation loss: 2.529843112085593

Epoch: 5| Step: 6
Training loss: 2.2838201806654452
Validation loss: 2.5284886113222687

Epoch: 5| Step: 7
Training loss: 2.209967410507533
Validation loss: 2.519969052466588

Epoch: 5| Step: 8
Training loss: 2.289557393707795
Validation loss: 2.5162084092513393

Epoch: 5| Step: 9
Training loss: 2.5979293794379545
Validation loss: 2.5170369460587496

Epoch: 5| Step: 10
Training loss: 2.607127975301951
Validation loss: 2.5008880984881587

Epoch: 5| Step: 11
Training loss: 1.4293304998211982
Validation loss: 2.500833122194513

Epoch: 200| Step: 0
Training loss: 2.427362341390623
Validation loss: 2.4957051817950737

Epoch: 5| Step: 1
Training loss: 2.5086500248853323
Validation loss: 2.497085628927512

Epoch: 5| Step: 2
Training loss: 2.373343241203001
Validation loss: 2.488913479207625

Epoch: 5| Step: 3
Training loss: 2.050827519939336
Validation loss: 2.486481938744821

Epoch: 5| Step: 4
Training loss: 2.3667158909724555
Validation loss: 2.487333156031869

Epoch: 5| Step: 5
Training loss: 2.1273658148902763
Validation loss: 2.493276252618059

Epoch: 5| Step: 6
Training loss: 2.883912246300215
Validation loss: 2.486559736956553

Epoch: 5| Step: 7
Training loss: 2.266539711295264
Validation loss: 2.487734700157907

Epoch: 5| Step: 8
Training loss: 1.6555995023613768
Validation loss: 2.4894591337744303

Epoch: 5| Step: 9
Training loss: 2.3199727744116374
Validation loss: 2.4972371132818854

Epoch: 5| Step: 10
Training loss: 3.3094330030622787
Validation loss: 2.4937429128433006

Epoch: 5| Step: 11
Training loss: 2.3129074150188584
Validation loss: 2.4931199373334256

Epoch: 201| Step: 0
Training loss: 2.3182657892994984
Validation loss: 2.4955465386566438

Epoch: 5| Step: 1
Training loss: 2.7174459377270037
Validation loss: 2.5203958806976883

Epoch: 5| Step: 2
Training loss: 2.7389573420322137
Validation loss: 2.5277808399554513

Epoch: 5| Step: 3
Training loss: 2.321078806507717
Validation loss: 2.536469521520693

Epoch: 5| Step: 4
Training loss: 2.2882599449012457
Validation loss: 2.5204594284905903

Epoch: 5| Step: 5
Training loss: 2.6494354798364887
Validation loss: 2.522518147956838

Epoch: 5| Step: 6
Training loss: 2.25731803997167
Validation loss: 2.520065199781819

Epoch: 5| Step: 7
Training loss: 1.9332429201535886
Validation loss: 2.519512395270718

Epoch: 5| Step: 8
Training loss: 1.8342749027320644
Validation loss: 2.4910710264460505

Epoch: 5| Step: 9
Training loss: 2.989242498837776
Validation loss: 2.5034172802641366

Epoch: 5| Step: 10
Training loss: 2.4819964652885456
Validation loss: 2.51943416556269

Epoch: 5| Step: 11
Training loss: 1.300572192372508
Validation loss: 2.4955681897760544

Epoch: 202| Step: 0
Training loss: 2.3866839866791136
Validation loss: 2.4911183482467045

Epoch: 5| Step: 1
Training loss: 2.188948451467314
Validation loss: 2.5009988100849054

Epoch: 5| Step: 2
Training loss: 2.545608299120041
Validation loss: 2.4923222783395236

Epoch: 5| Step: 3
Training loss: 2.5938931161393914
Validation loss: 2.5029991875508872

Epoch: 5| Step: 4
Training loss: 2.3055528987986667
Validation loss: 2.512995824870588

Epoch: 5| Step: 5
Training loss: 2.0862136258016504
Validation loss: 2.502135723517346

Epoch: 5| Step: 6
Training loss: 2.1490808252016746
Validation loss: 2.5098407580735773

Epoch: 5| Step: 7
Training loss: 2.719850076442445
Validation loss: 2.5146334097700236

Epoch: 5| Step: 8
Training loss: 2.507794341499923
Validation loss: 2.5088716767813826

Epoch: 5| Step: 9
Training loss: 2.704091070340109
Validation loss: 2.5076012408520034

Epoch: 5| Step: 10
Training loss: 2.0394217806754464
Validation loss: 2.5246023632977037

Epoch: 5| Step: 11
Training loss: 1.8853543167623685
Validation loss: 2.5179877114477103

Epoch: 203| Step: 0
Training loss: 2.250024053656963
Validation loss: 2.5136733925791845

Epoch: 5| Step: 1
Training loss: 2.3851313080219185
Validation loss: 2.5383803326652847

Epoch: 5| Step: 2
Training loss: 2.2696302067362404
Validation loss: 2.520165943568046

Epoch: 5| Step: 3
Training loss: 2.208669516927252
Validation loss: 2.5246659704726517

Epoch: 5| Step: 4
Training loss: 2.4212032247819635
Validation loss: 2.5161143354180124

Epoch: 5| Step: 5
Training loss: 2.6583147887377314
Validation loss: 2.5390390052686516

Epoch: 5| Step: 6
Training loss: 1.9549106827257614
Validation loss: 2.52272230234914

Epoch: 5| Step: 7
Training loss: 3.2093201808092036
Validation loss: 2.4983329738360505

Epoch: 5| Step: 8
Training loss: 2.708282841920921
Validation loss: 2.485993915479574

Epoch: 5| Step: 9
Training loss: 1.9403362434053884
Validation loss: 2.48394853702341

Epoch: 5| Step: 10
Training loss: 2.209580938731299
Validation loss: 2.4873117687182953

Epoch: 5| Step: 11
Training loss: 3.149030075913272
Validation loss: 2.4856046516545387

Epoch: 204| Step: 0
Training loss: 2.7309765432238104
Validation loss: 2.50345173011843

Epoch: 5| Step: 1
Training loss: 2.561027103553236
Validation loss: 2.501663266657637

Epoch: 5| Step: 2
Training loss: 2.6541972587255676
Validation loss: 2.502203613892541

Epoch: 5| Step: 3
Training loss: 2.1165989131546583
Validation loss: 2.50256775675052

Epoch: 5| Step: 4
Training loss: 2.8715353282340765
Validation loss: 2.501402348354427

Epoch: 5| Step: 5
Training loss: 2.4641921063132246
Validation loss: 2.491852258438628

Epoch: 5| Step: 6
Training loss: 1.5428356258087803
Validation loss: 2.5065665393177423

Epoch: 5| Step: 7
Training loss: 2.630464769206874
Validation loss: 2.4938202614634024

Epoch: 5| Step: 8
Training loss: 2.2299114539866487
Validation loss: 2.5046934377211616

Epoch: 5| Step: 9
Training loss: 2.4996824062794825
Validation loss: 2.5046229296456435

Epoch: 5| Step: 10
Training loss: 2.567315949229176
Validation loss: 2.5050393713098837

Epoch: 5| Step: 11
Training loss: 1.5537135461657077
Validation loss: 2.520203461784361

Epoch: 205| Step: 0
Training loss: 2.6665159321939718
Validation loss: 2.5416111444964775

Epoch: 5| Step: 1
Training loss: 2.7055345501390016
Validation loss: 2.5521047331762654

Epoch: 5| Step: 2
Training loss: 2.25124727222504
Validation loss: 2.568468160706567

Epoch: 5| Step: 3
Training loss: 1.9575490057256562
Validation loss: 2.541037815240469

Epoch: 5| Step: 4
Training loss: 2.4029319973394774
Validation loss: 2.532652470316022

Epoch: 5| Step: 5
Training loss: 2.6005400060192296
Validation loss: 2.5160828957529557

Epoch: 5| Step: 6
Training loss: 2.068508182464268
Validation loss: 2.5302450045214155

Epoch: 5| Step: 7
Training loss: 2.6760613733389884
Validation loss: 2.513583288679424

Epoch: 5| Step: 8
Training loss: 2.4521813977212865
Validation loss: 2.496695460396251

Epoch: 5| Step: 9
Training loss: 2.494982165958981
Validation loss: 2.5087916283986003

Epoch: 5| Step: 10
Training loss: 2.2639477012915625
Validation loss: 2.500091328542187

Epoch: 5| Step: 11
Training loss: 1.0958771320964948
Validation loss: 2.496820295336956

Epoch: 206| Step: 0
Training loss: 2.436023754185708
Validation loss: 2.4933613769931195

Epoch: 5| Step: 1
Training loss: 2.4115202025900593
Validation loss: 2.4972716782264457

Epoch: 5| Step: 2
Training loss: 2.4038083457907904
Validation loss: 2.4878484248705384

Epoch: 5| Step: 3
Training loss: 2.46245070664694
Validation loss: 2.487254334583992

Epoch: 5| Step: 4
Training loss: 2.3470211344199305
Validation loss: 2.5034090600854406

Epoch: 5| Step: 5
Training loss: 2.680259265457732
Validation loss: 2.504131459757198

Epoch: 5| Step: 6
Training loss: 2.3483798265513647
Validation loss: 2.4917846162706474

Epoch: 5| Step: 7
Training loss: 2.4158186466033347
Validation loss: 2.5000137845295125

Epoch: 5| Step: 8
Training loss: 2.4273051759462163
Validation loss: 2.49904336308949

Epoch: 5| Step: 9
Training loss: 1.9099232662225665
Validation loss: 2.5075439119154677

Epoch: 5| Step: 10
Training loss: 2.478015653836343
Validation loss: 2.522847272898761

Epoch: 5| Step: 11
Training loss: 2.2038679020198155
Validation loss: 2.525027995221896

Epoch: 207| Step: 0
Training loss: 2.4416593618793603
Validation loss: 2.5268687136390495

Epoch: 5| Step: 1
Training loss: 2.3869373077866354
Validation loss: 2.516294177122837

Epoch: 5| Step: 2
Training loss: 2.7337641442399137
Validation loss: 2.519203077920275

Epoch: 5| Step: 3
Training loss: 2.145084608494522
Validation loss: 2.517541285823129

Epoch: 5| Step: 4
Training loss: 1.645586123994288
Validation loss: 2.521398513793739

Epoch: 5| Step: 5
Training loss: 2.1110652227043385
Validation loss: 2.520214747097796

Epoch: 5| Step: 6
Training loss: 2.9757149358217196
Validation loss: 2.5283509595537788

Epoch: 5| Step: 7
Training loss: 1.962296216552894
Validation loss: 2.5330955111925815

Epoch: 5| Step: 8
Training loss: 1.6954937675359276
Validation loss: 2.5165980966544375

Epoch: 5| Step: 9
Training loss: 2.4219689135647315
Validation loss: 2.522767971176172

Epoch: 5| Step: 10
Training loss: 2.7978639186300023
Validation loss: 2.5146986734748733

Epoch: 5| Step: 11
Training loss: 4.347055837570534
Validation loss: 2.5010899393066897

Epoch: 208| Step: 0
Training loss: 2.5126290814847794
Validation loss: 2.5030395428138488

Epoch: 5| Step: 1
Training loss: 2.118497886399166
Validation loss: 2.5007190425293553

Epoch: 5| Step: 2
Training loss: 2.3117835507297833
Validation loss: 2.5030071729279624

Epoch: 5| Step: 3
Training loss: 2.802896166660753
Validation loss: 2.50077584069543

Epoch: 5| Step: 4
Training loss: 2.067261142240744
Validation loss: 2.505902348202176

Epoch: 5| Step: 5
Training loss: 2.6377336371475324
Validation loss: 2.512579936879925

Epoch: 5| Step: 6
Training loss: 2.3160624514314514
Validation loss: 2.5024237605767605

Epoch: 5| Step: 7
Training loss: 2.9117533762320984
Validation loss: 2.5174703128558105

Epoch: 5| Step: 8
Training loss: 2.148614494662452
Validation loss: 2.527525300271286

Epoch: 5| Step: 9
Training loss: 2.534328707684974
Validation loss: 2.521873733633638

Epoch: 5| Step: 10
Training loss: 1.939182043578794
Validation loss: 2.5419363532796018

Epoch: 5| Step: 11
Training loss: 1.754181770144913
Validation loss: 2.552832786649741

Epoch: 209| Step: 0
Training loss: 2.675609187698957
Validation loss: 2.57074893852048

Epoch: 5| Step: 1
Training loss: 2.060428966746502
Validation loss: 2.6012986768231205

Epoch: 5| Step: 2
Training loss: 2.4603528493884066
Validation loss: 2.621337235005283

Epoch: 5| Step: 3
Training loss: 2.8544781554350624
Validation loss: 2.637593453455597

Epoch: 5| Step: 4
Training loss: 2.5440787662932145
Validation loss: 2.611765742813444

Epoch: 5| Step: 5
Training loss: 2.5233798655255506
Validation loss: 2.585725543199123

Epoch: 5| Step: 6
Training loss: 2.342221282857859
Validation loss: 2.5503682673132535

Epoch: 5| Step: 7
Training loss: 2.3297863863164534
Validation loss: 2.5370367057873042

Epoch: 5| Step: 8
Training loss: 2.3497934372392972
Validation loss: 2.529977109501435

Epoch: 5| Step: 9
Training loss: 2.3000766990146433
Validation loss: 2.504614013368043

Epoch: 5| Step: 10
Training loss: 2.2843003455199318
Validation loss: 2.5015699662480952

Epoch: 5| Step: 11
Training loss: 2.7361239698164894
Validation loss: 2.4942776038210135

Epoch: 210| Step: 0
Training loss: 2.8424057350867424
Validation loss: 2.489957603083433

Epoch: 5| Step: 1
Training loss: 2.3359233128553814
Validation loss: 2.492963640141428

Epoch: 5| Step: 2
Training loss: 2.014606543811269
Validation loss: 2.494768215592184

Epoch: 5| Step: 3
Training loss: 2.6836842795126388
Validation loss: 2.498769375784963

Epoch: 5| Step: 4
Training loss: 2.3603358049225776
Validation loss: 2.4992546599673067

Epoch: 5| Step: 5
Training loss: 2.091633082369713
Validation loss: 2.4937258947891667

Epoch: 5| Step: 6
Training loss: 2.2529127552733894
Validation loss: 2.489897434153405

Epoch: 5| Step: 7
Training loss: 2.14253205603674
Validation loss: 2.501915472076371

Epoch: 5| Step: 8
Training loss: 2.591485011203703
Validation loss: 2.508361863188434

Epoch: 5| Step: 9
Training loss: 2.914623707584141
Validation loss: 2.5078672004984934

Epoch: 5| Step: 10
Training loss: 2.2245021991851766
Validation loss: 2.520958433646362

Epoch: 5| Step: 11
Training loss: 3.16328175649076
Validation loss: 2.530439011439382

Epoch: 211| Step: 0
Training loss: 2.325080365668599
Validation loss: 2.5349368689649667

Epoch: 5| Step: 1
Training loss: 2.4111921412050683
Validation loss: 2.5189055047729116

Epoch: 5| Step: 2
Training loss: 1.9543972296894627
Validation loss: 2.5205127982546363

Epoch: 5| Step: 3
Training loss: 2.88000881246702
Validation loss: 2.494733560213298

Epoch: 5| Step: 4
Training loss: 2.365826105113698
Validation loss: 2.496131017145058

Epoch: 5| Step: 5
Training loss: 2.328284955770258
Validation loss: 2.4990846507903544

Epoch: 5| Step: 6
Training loss: 2.5954218612311077
Validation loss: 2.502251354412935

Epoch: 5| Step: 7
Training loss: 2.3823638994024456
Validation loss: 2.503988914295597

Epoch: 5| Step: 8
Training loss: 2.2272489861406712
Validation loss: 2.49599238882881

Epoch: 5| Step: 9
Training loss: 2.392809798254207
Validation loss: 2.5095222642124626

Epoch: 5| Step: 10
Training loss: 2.322333589141614
Validation loss: 2.49071166008603

Epoch: 5| Step: 11
Training loss: 3.257897684643948
Validation loss: 2.497361746122148

Epoch: 212| Step: 0
Training loss: 2.4842895876999123
Validation loss: 2.49406127086514

Epoch: 5| Step: 1
Training loss: 2.5441885980118335
Validation loss: 2.511603337430106

Epoch: 5| Step: 2
Training loss: 2.3543706709880436
Validation loss: 2.4984722635229564

Epoch: 5| Step: 3
Training loss: 2.498842543643429
Validation loss: 2.498240873207063

Epoch: 5| Step: 4
Training loss: 2.168169551863378
Validation loss: 2.500831136034117

Epoch: 5| Step: 5
Training loss: 2.406278238502374
Validation loss: 2.500528450585933

Epoch: 5| Step: 6
Training loss: 2.2449527927524473
Validation loss: 2.5026271844150045

Epoch: 5| Step: 7
Training loss: 2.3704567671081884
Validation loss: 2.4955649216194877

Epoch: 5| Step: 8
Training loss: 2.3238849303996894
Validation loss: 2.504057615310219

Epoch: 5| Step: 9
Training loss: 2.455722284290517
Validation loss: 2.504109259839713

Epoch: 5| Step: 10
Training loss: 2.7374211374010375
Validation loss: 2.500235629898365

Epoch: 5| Step: 11
Training loss: 1.6157979109004441
Validation loss: 2.505805772429577

Epoch: 213| Step: 0
Training loss: 3.1527893432166056
Validation loss: 2.5156245537672572

Epoch: 5| Step: 1
Training loss: 2.696860402444561
Validation loss: 2.5280058286892415

Epoch: 5| Step: 2
Training loss: 2.150098567743624
Validation loss: 2.5269915076353247

Epoch: 5| Step: 3
Training loss: 2.482617698407085
Validation loss: 2.5406222360243236

Epoch: 5| Step: 4
Training loss: 2.2979578073876143
Validation loss: 2.5605125240657034

Epoch: 5| Step: 5
Training loss: 2.2396063751691373
Validation loss: 2.5824044862460944

Epoch: 5| Step: 6
Training loss: 2.35400851458336
Validation loss: 2.573509562413151

Epoch: 5| Step: 7
Training loss: 2.0456953070773256
Validation loss: 2.541550161995516

Epoch: 5| Step: 8
Training loss: 2.737714635368912
Validation loss: 2.5226081532461024

Epoch: 5| Step: 9
Training loss: 2.3740012427939785
Validation loss: 2.5209884490742525

Epoch: 5| Step: 10
Training loss: 2.0740295438805014
Validation loss: 2.5143718540217166

Epoch: 5| Step: 11
Training loss: 1.3381186632044637
Validation loss: 2.5063848502013264

Epoch: 214| Step: 0
Training loss: 2.2132446329122195
Validation loss: 2.4890837281130684

Epoch: 5| Step: 1
Training loss: 2.296487165651477
Validation loss: 2.508089820684177

Epoch: 5| Step: 2
Training loss: 2.3608056587655564
Validation loss: 2.4941541750546548

Epoch: 5| Step: 3
Training loss: 2.6858075157610886
Validation loss: 2.489960675126916

Epoch: 5| Step: 4
Training loss: 2.539427839551425
Validation loss: 2.4852041104587

Epoch: 5| Step: 5
Training loss: 2.1544352095346286
Validation loss: 2.4916193363283083

Epoch: 5| Step: 6
Training loss: 2.178443252759448
Validation loss: 2.4913684770733715

Epoch: 5| Step: 7
Training loss: 2.8296322678684422
Validation loss: 2.486882497829381

Epoch: 5| Step: 8
Training loss: 2.2986960944388937
Validation loss: 2.4997686755923594

Epoch: 5| Step: 9
Training loss: 2.656631532083349
Validation loss: 2.4925243983067813

Epoch: 5| Step: 10
Training loss: 2.41698044899296
Validation loss: 2.5102235269974487

Epoch: 5| Step: 11
Training loss: 3.0705525932882853
Validation loss: 2.520423163522663

Epoch: 215| Step: 0
Training loss: 2.6795050111251473
Validation loss: 2.510833829564669

Epoch: 5| Step: 1
Training loss: 2.3055230129452653
Validation loss: 2.5080154986432466

Epoch: 5| Step: 2
Training loss: 2.7245067289972544
Validation loss: 2.5406890393421286

Epoch: 5| Step: 3
Training loss: 2.5045603167154744
Validation loss: 2.5173699623624626

Epoch: 5| Step: 4
Training loss: 2.0357237626813607
Validation loss: 2.5289822808684854

Epoch: 5| Step: 5
Training loss: 2.1456149338870296
Validation loss: 2.534671421645141

Epoch: 5| Step: 6
Training loss: 2.387761813682813
Validation loss: 2.5148426837648294

Epoch: 5| Step: 7
Training loss: 2.196360816309265
Validation loss: 2.5446612053084574

Epoch: 5| Step: 8
Training loss: 2.3076434301433126
Validation loss: 2.5328229149728423

Epoch: 5| Step: 9
Training loss: 2.6009364935760586
Validation loss: 2.515979246308775

Epoch: 5| Step: 10
Training loss: 2.510610095495759
Validation loss: 2.5126766278762847

Epoch: 5| Step: 11
Training loss: 2.5846320138647836
Validation loss: 2.530200764338633

Epoch: 216| Step: 0
Training loss: 2.0445046714305803
Validation loss: 2.5269612214937665

Epoch: 5| Step: 1
Training loss: 2.2947753859496682
Validation loss: 2.521435816676718

Epoch: 5| Step: 2
Training loss: 2.3051983800490174
Validation loss: 2.5095995183814734

Epoch: 5| Step: 3
Training loss: 2.315388499343699
Validation loss: 2.5184338212640185

Epoch: 5| Step: 4
Training loss: 2.571352855385074
Validation loss: 2.5085858809320882

Epoch: 5| Step: 5
Training loss: 2.5149017621187335
Validation loss: 2.504957810457166

Epoch: 5| Step: 6
Training loss: 2.328434968321615
Validation loss: 2.516478956297883

Epoch: 5| Step: 7
Training loss: 2.617257188467017
Validation loss: 2.501454391224339

Epoch: 5| Step: 8
Training loss: 2.563023676512475
Validation loss: 2.5120719772036204

Epoch: 5| Step: 9
Training loss: 2.309353517837648
Validation loss: 2.514404059822241

Epoch: 5| Step: 10
Training loss: 2.44104469979134
Validation loss: 2.495820971306026

Epoch: 5| Step: 11
Training loss: 2.2100059245103507
Validation loss: 2.520153751407135

Epoch: 217| Step: 0
Training loss: 2.12323867507011
Validation loss: 2.546079677212421

Epoch: 5| Step: 1
Training loss: 2.2899888803803448
Validation loss: 2.547652283928342

Epoch: 5| Step: 2
Training loss: 2.7124061673718547
Validation loss: 2.5279636241960612

Epoch: 5| Step: 3
Training loss: 2.324371691888895
Validation loss: 2.5326565810008845

Epoch: 5| Step: 4
Training loss: 2.8673948789848964
Validation loss: 2.515599240789169

Epoch: 5| Step: 5
Training loss: 2.479587861086182
Validation loss: 2.53047264391686

Epoch: 5| Step: 6
Training loss: 2.3955778787398896
Validation loss: 2.524082585602349

Epoch: 5| Step: 7
Training loss: 2.3329855341783725
Validation loss: 2.529983607950245

Epoch: 5| Step: 8
Training loss: 1.5758851273832244
Validation loss: 2.525748561620468

Epoch: 5| Step: 9
Training loss: 2.3865690043871384
Validation loss: 2.533892990290571

Epoch: 5| Step: 10
Training loss: 2.5038108390923908
Validation loss: 2.5390075051636343

Epoch: 5| Step: 11
Training loss: 2.4445605599635583
Validation loss: 2.5249111057597418

Epoch: 218| Step: 0
Training loss: 2.0610837987145225
Validation loss: 2.5289595841770445

Epoch: 5| Step: 1
Training loss: 2.949339362703384
Validation loss: 2.5287169588236855

Epoch: 5| Step: 2
Training loss: 2.203638003527812
Validation loss: 2.5167326255916818

Epoch: 5| Step: 3
Training loss: 2.1274166950631916
Validation loss: 2.538797527083108

Epoch: 5| Step: 4
Training loss: 3.007340986276624
Validation loss: 2.52894491645951

Epoch: 5| Step: 5
Training loss: 2.095809379223341
Validation loss: 2.520416309341739

Epoch: 5| Step: 6
Training loss: 2.241330611081865
Validation loss: 2.534920135302132

Epoch: 5| Step: 7
Training loss: 2.0852119241130227
Validation loss: 2.5158696105058294

Epoch: 5| Step: 8
Training loss: 2.418502526844054
Validation loss: 2.521341947733309

Epoch: 5| Step: 9
Training loss: 1.9337665519178586
Validation loss: 2.5037815064904527

Epoch: 5| Step: 10
Training loss: 2.6604306699841813
Validation loss: 2.5199630899507794

Epoch: 5| Step: 11
Training loss: 2.5927576211852807
Validation loss: 2.5223325390737816

Epoch: 219| Step: 0
Training loss: 2.4442191802855233
Validation loss: 2.510911597272781

Epoch: 5| Step: 1
Training loss: 2.524603606732009
Validation loss: 2.511959687300538

Epoch: 5| Step: 2
Training loss: 2.7527114332381766
Validation loss: 2.5197800663752923

Epoch: 5| Step: 3
Training loss: 2.5109263070313395
Validation loss: 2.5236248344431282

Epoch: 5| Step: 4
Training loss: 2.3159026809564196
Validation loss: 2.5296832031408285

Epoch: 5| Step: 5
Training loss: 1.8947921101417742
Validation loss: 2.531787544440704

Epoch: 5| Step: 6
Training loss: 2.8188224089598823
Validation loss: 2.5582895306762863

Epoch: 5| Step: 7
Training loss: 2.331794253883444
Validation loss: 2.5660131609215697

Epoch: 5| Step: 8
Training loss: 2.072735214598436
Validation loss: 2.579998483903819

Epoch: 5| Step: 9
Training loss: 2.3157451644792615
Validation loss: 2.558736584585827

Epoch: 5| Step: 10
Training loss: 2.2240410699276887
Validation loss: 2.540980337583553

Epoch: 5| Step: 11
Training loss: 2.010146628578072
Validation loss: 2.537672869583243

Epoch: 220| Step: 0
Training loss: 2.2932793116121104
Validation loss: 2.5306639758250835

Epoch: 5| Step: 1
Training loss: 2.208353318417885
Validation loss: 2.522356177603863

Epoch: 5| Step: 2
Training loss: 2.0914938998460553
Validation loss: 2.523186926117625

Epoch: 5| Step: 3
Training loss: 2.05674843356146
Validation loss: 2.511705528054407

Epoch: 5| Step: 4
Training loss: 2.555584018198632
Validation loss: 2.50918318029975

Epoch: 5| Step: 5
Training loss: 2.5855622264165996
Validation loss: 2.5050574327942052

Epoch: 5| Step: 6
Training loss: 2.532804318037708
Validation loss: 2.5117690109516104

Epoch: 5| Step: 7
Training loss: 2.503396683139833
Validation loss: 2.5173087694857874

Epoch: 5| Step: 8
Training loss: 2.5793407925777028
Validation loss: 2.5212097372983044

Epoch: 5| Step: 9
Training loss: 2.464845587824939
Validation loss: 2.5370627602828844

Epoch: 5| Step: 10
Training loss: 2.4061991698606002
Validation loss: 2.5542016612440928

Epoch: 5| Step: 11
Training loss: 2.2319952823356624
Validation loss: 2.551227185933719

Epoch: 221| Step: 0
Training loss: 2.4549843126094903
Validation loss: 2.551245500612629

Epoch: 5| Step: 1
Training loss: 2.547665900311855
Validation loss: 2.5635319508215106

Epoch: 5| Step: 2
Training loss: 2.3193486841754996
Validation loss: 2.5495464029041264

Epoch: 5| Step: 3
Training loss: 2.157490912249224
Validation loss: 2.559898645522991

Epoch: 5| Step: 4
Training loss: 2.6612809950142107
Validation loss: 2.567226656187352

Epoch: 5| Step: 5
Training loss: 2.5133121828897638
Validation loss: 2.5343998788223194

Epoch: 5| Step: 6
Training loss: 2.3812639050816125
Validation loss: 2.504814732985357

Epoch: 5| Step: 7
Training loss: 2.433138735928629
Validation loss: 2.5071134474337713

Epoch: 5| Step: 8
Training loss: 1.612979075509542
Validation loss: 2.486400234618108

Epoch: 5| Step: 9
Training loss: 2.5880013611331525
Validation loss: 2.5067292960903993

Epoch: 5| Step: 10
Training loss: 2.437286856697889
Validation loss: 2.4906084166213627

Epoch: 5| Step: 11
Training loss: 1.792813429410819
Validation loss: 2.515921561410554

Epoch: 222| Step: 0
Training loss: 2.390192927789651
Validation loss: 2.508045649163257

Epoch: 5| Step: 1
Training loss: 2.531464179125937
Validation loss: 2.506397156901928

Epoch: 5| Step: 2
Training loss: 2.6970636401279346
Validation loss: 2.517325361736083

Epoch: 5| Step: 3
Training loss: 2.737419308383293
Validation loss: 2.5125258094891687

Epoch: 5| Step: 4
Training loss: 1.880102906657232
Validation loss: 2.5292621899832293

Epoch: 5| Step: 5
Training loss: 2.209346131132113
Validation loss: 2.5353636458429065

Epoch: 5| Step: 6
Training loss: 2.133515518078401
Validation loss: 2.545447985355018

Epoch: 5| Step: 7
Training loss: 2.1499032863776892
Validation loss: 2.5601271676476123

Epoch: 5| Step: 8
Training loss: 1.9267275576335654
Validation loss: 2.5442288739204204

Epoch: 5| Step: 9
Training loss: 3.0441256124918543
Validation loss: 2.548346408422079

Epoch: 5| Step: 10
Training loss: 2.0028263625226046
Validation loss: 2.5473149315567802

Epoch: 5| Step: 11
Training loss: 2.4704047343282842
Validation loss: 2.5353204941409015

Epoch: 223| Step: 0
Training loss: 2.2072215850587624
Validation loss: 2.511415964626495

Epoch: 5| Step: 1
Training loss: 2.2419468554511357
Validation loss: 2.5385600292202772

Epoch: 5| Step: 2
Training loss: 2.506256095897843
Validation loss: 2.5095717952705185

Epoch: 5| Step: 3
Training loss: 1.718106721660791
Validation loss: 2.5288398673468575

Epoch: 5| Step: 4
Training loss: 2.212037368941637
Validation loss: 2.503609293023016

Epoch: 5| Step: 5
Training loss: 2.2858590373968553
Validation loss: 2.5128592773582206

Epoch: 5| Step: 6
Training loss: 3.0223228903753028
Validation loss: 2.517607565175731

Epoch: 5| Step: 7
Training loss: 2.1611385491903072
Validation loss: 2.514249698172729

Epoch: 5| Step: 8
Training loss: 2.4216849960120057
Validation loss: 2.4982069022405415

Epoch: 5| Step: 9
Training loss: 2.521491655713007
Validation loss: 2.5119370978150237

Epoch: 5| Step: 10
Training loss: 2.296380541251924
Validation loss: 2.5131450714603827

Epoch: 5| Step: 11
Training loss: 3.367528061781699
Validation loss: 2.5144644166839942

Epoch: 224| Step: 0
Training loss: 2.1550928964086165
Validation loss: 2.5359493400280093

Epoch: 5| Step: 1
Training loss: 2.2122721064382667
Validation loss: 2.539627149348398

Epoch: 5| Step: 2
Training loss: 2.465750018335919
Validation loss: 2.557697296344554

Epoch: 5| Step: 3
Training loss: 2.3543783672198306
Validation loss: 2.536176173647718

Epoch: 5| Step: 4
Training loss: 2.0524262143438743
Validation loss: 2.5604878197403678

Epoch: 5| Step: 5
Training loss: 2.6452668151695966
Validation loss: 2.5726039917850514

Epoch: 5| Step: 6
Training loss: 2.266415583036734
Validation loss: 2.5633231368397413

Epoch: 5| Step: 7
Training loss: 2.7688725257095443
Validation loss: 2.595997047670485

Epoch: 5| Step: 8
Training loss: 1.9154120844991644
Validation loss: 2.558293771023686

Epoch: 5| Step: 9
Training loss: 2.5106030681203904
Validation loss: 2.5443226832739123

Epoch: 5| Step: 10
Training loss: 2.574963752713678
Validation loss: 2.518818133862771

Epoch: 5| Step: 11
Training loss: 1.9216747024491634
Validation loss: 2.517936260831644

Epoch: 225| Step: 0
Training loss: 2.5758130428815407
Validation loss: 2.5185825582527333

Epoch: 5| Step: 1
Training loss: 2.774693250455761
Validation loss: 2.526611465737678

Epoch: 5| Step: 2
Training loss: 1.9993307066647146
Validation loss: 2.5166102823568473

Epoch: 5| Step: 3
Training loss: 2.2736086436870435
Validation loss: 2.5187876468997064

Epoch: 5| Step: 4
Training loss: 2.032138454098233
Validation loss: 2.533435000138449

Epoch: 5| Step: 5
Training loss: 2.579586932361006
Validation loss: 2.5115060038863475

Epoch: 5| Step: 6
Training loss: 2.192942578873382
Validation loss: 2.533546540162281

Epoch: 5| Step: 7
Training loss: 2.452930029635098
Validation loss: 2.5391745048663212

Epoch: 5| Step: 8
Training loss: 2.2550918396308557
Validation loss: 2.537061981079579

Epoch: 5| Step: 9
Training loss: 1.886661504978807
Validation loss: 2.561887784257322

Epoch: 5| Step: 10
Training loss: 2.936018387521235
Validation loss: 2.547265578669065

Epoch: 5| Step: 11
Training loss: 2.205786429918831
Validation loss: 2.5447717106573187

Epoch: 226| Step: 0
Training loss: 2.048705818300359
Validation loss: 2.5266534136644334

Epoch: 5| Step: 1
Training loss: 2.435362147321161
Validation loss: 2.5199984233178903

Epoch: 5| Step: 2
Training loss: 2.5956012599639258
Validation loss: 2.521493294658491

Epoch: 5| Step: 3
Training loss: 2.7992489931568607
Validation loss: 2.510343702266335

Epoch: 5| Step: 4
Training loss: 2.55098191971841
Validation loss: 2.50589130170171

Epoch: 5| Step: 5
Training loss: 1.9704535849001803
Validation loss: 2.5135803087417785

Epoch: 5| Step: 6
Training loss: 2.605685155022708
Validation loss: 2.498246746403532

Epoch: 5| Step: 7
Training loss: 2.1198634729868817
Validation loss: 2.509548414357818

Epoch: 5| Step: 8
Training loss: 1.9996556939351107
Validation loss: 2.506801821460806

Epoch: 5| Step: 9
Training loss: 2.411646748398799
Validation loss: 2.5353328249528175

Epoch: 5| Step: 10
Training loss: 2.2768711234349506
Validation loss: 2.531204391979114

Epoch: 5| Step: 11
Training loss: 3.182857198874196
Validation loss: 2.5345142258840445

Epoch: 227| Step: 0
Training loss: 2.3628364081271145
Validation loss: 2.5882600033654977

Epoch: 5| Step: 1
Training loss: 2.8586374801289254
Validation loss: 2.6039741648414454

Epoch: 5| Step: 2
Training loss: 2.439590535749124
Validation loss: 2.6131313834104417

Epoch: 5| Step: 3
Training loss: 2.429572982787845
Validation loss: 2.612924426949369

Epoch: 5| Step: 4
Training loss: 2.3328512465778157
Validation loss: 2.569024997417811

Epoch: 5| Step: 5
Training loss: 1.955937611070524
Validation loss: 2.5652347253921475

Epoch: 5| Step: 6
Training loss: 2.7096220227261365
Validation loss: 2.5571628753074425

Epoch: 5| Step: 7
Training loss: 2.3276208933441724
Validation loss: 2.534026663783946

Epoch: 5| Step: 8
Training loss: 2.52311529694875
Validation loss: 2.5228412423697133

Epoch: 5| Step: 9
Training loss: 1.7250069908332146
Validation loss: 2.517384243709053

Epoch: 5| Step: 10
Training loss: 2.5966561823165186
Validation loss: 2.5100280307886496

Epoch: 5| Step: 11
Training loss: 1.5875898500982581
Validation loss: 2.5038068099963877

Epoch: 228| Step: 0
Training loss: 2.3737109852296987
Validation loss: 2.5079557629442846

Epoch: 5| Step: 1
Training loss: 2.698812848941108
Validation loss: 2.498238372024415

Epoch: 5| Step: 2
Training loss: 2.34438183215251
Validation loss: 2.513717030492842

Epoch: 5| Step: 3
Training loss: 2.2817182713485704
Validation loss: 2.52081110579304

Epoch: 5| Step: 4
Training loss: 2.3488561321842822
Validation loss: 2.5354272651503087

Epoch: 5| Step: 5
Training loss: 2.1518610639782882
Validation loss: 2.5398908623454544

Epoch: 5| Step: 6
Training loss: 2.2219098653083216
Validation loss: 2.5290339329552154

Epoch: 5| Step: 7
Training loss: 2.9108015945004544
Validation loss: 2.527537059908422

Epoch: 5| Step: 8
Training loss: 2.8801386148155714
Validation loss: 2.513586005794556

Epoch: 5| Step: 9
Training loss: 2.196944963581052
Validation loss: 2.5234285950872652

Epoch: 5| Step: 10
Training loss: 1.694797701396018
Validation loss: 2.5173536052080405

Epoch: 5| Step: 11
Training loss: 1.8306739187428045
Validation loss: 2.518268984637942

Epoch: 229| Step: 0
Training loss: 2.7903503522036823
Validation loss: 2.5165895267584135

Epoch: 5| Step: 1
Training loss: 2.5107625087299956
Validation loss: 2.5282640545439077

Epoch: 5| Step: 2
Training loss: 2.3760967733795786
Validation loss: 2.542773616743981

Epoch: 5| Step: 3
Training loss: 2.513536996490613
Validation loss: 2.591706965021753

Epoch: 5| Step: 4
Training loss: 2.475919910904309
Validation loss: 2.5952233454031823

Epoch: 5| Step: 5
Training loss: 2.4006183662114333
Validation loss: 2.6155712128231983

Epoch: 5| Step: 6
Training loss: 2.474062840446168
Validation loss: 2.592059622552256

Epoch: 5| Step: 7
Training loss: 2.3525435216852717
Validation loss: 2.5881673221095594

Epoch: 5| Step: 8
Training loss: 2.058128347756136
Validation loss: 2.5552620509011863

Epoch: 5| Step: 9
Training loss: 2.0116431360941305
Validation loss: 2.532708195415984

Epoch: 5| Step: 10
Training loss: 2.505948809174701
Validation loss: 2.5135126446311724

Epoch: 5| Step: 11
Training loss: 2.3799582474516305
Validation loss: 2.5035599515207565

Epoch: 230| Step: 0
Training loss: 3.029080745382721
Validation loss: 2.491477055867767

Epoch: 5| Step: 1
Training loss: 2.3450543652314573
Validation loss: 2.4992441822986713

Epoch: 5| Step: 2
Training loss: 2.448442107818883
Validation loss: 2.497770030150874

Epoch: 5| Step: 3
Training loss: 2.6794203911494225
Validation loss: 2.5021002212788783

Epoch: 5| Step: 4
Training loss: 2.317726929550718
Validation loss: 2.514546224251293

Epoch: 5| Step: 5
Training loss: 2.473783937966435
Validation loss: 2.510295652635886

Epoch: 5| Step: 6
Training loss: 2.405137833037983
Validation loss: 2.505938034428612

Epoch: 5| Step: 7
Training loss: 2.972820823815774
Validation loss: 2.5076520319825204

Epoch: 5| Step: 8
Training loss: 2.4119376797129504
Validation loss: 2.501598943555662

Epoch: 5| Step: 9
Training loss: 1.9295708624922034
Validation loss: 2.500877440967285

Epoch: 5| Step: 10
Training loss: 1.9572274420005071
Validation loss: 2.4950078712599137

Epoch: 5| Step: 11
Training loss: 3.260114044347731
Validation loss: 2.5007282388509324

Epoch: 231| Step: 0
Training loss: 2.338078465002469
Validation loss: 2.496745816876708

Epoch: 5| Step: 1
Training loss: 3.0430167002199284
Validation loss: 2.486588889184891

Epoch: 5| Step: 2
Training loss: 2.093872066754221
Validation loss: 2.498410892086439

Epoch: 5| Step: 3
Training loss: 2.1700507536898965
Validation loss: 2.4938456281971493

Epoch: 5| Step: 4
Training loss: 1.9672844290998284
Validation loss: 2.5045462755926637

Epoch: 5| Step: 5
Training loss: 2.703371775393398
Validation loss: 2.4968231997887673

Epoch: 5| Step: 6
Training loss: 2.2017290819876023
Validation loss: 2.5092947472407197

Epoch: 5| Step: 7
Training loss: 2.285998588574926
Validation loss: 2.497782434977524

Epoch: 5| Step: 8
Training loss: 2.6706733008975383
Validation loss: 2.4900429845327183

Epoch: 5| Step: 9
Training loss: 2.7523266746560875
Validation loss: 2.497201835779897

Epoch: 5| Step: 10
Training loss: 2.3196352602906543
Validation loss: 2.496585902894891

Epoch: 5| Step: 11
Training loss: 1.6894488207811862
Validation loss: 2.4980615332568332

Epoch: 232| Step: 0
Training loss: 2.382076862783042
Validation loss: 2.498776013040303

Epoch: 5| Step: 1
Training loss: 2.7313328850432086
Validation loss: 2.508329736246548

Epoch: 5| Step: 2
Training loss: 2.5852542216509318
Validation loss: 2.5097641050579056

Epoch: 5| Step: 3
Training loss: 2.300887566848037
Validation loss: 2.505218542848801

Epoch: 5| Step: 4
Training loss: 2.290367821883962
Validation loss: 2.5117659814011732

Epoch: 5| Step: 5
Training loss: 2.0074453055221917
Validation loss: 2.492806089795578

Epoch: 5| Step: 6
Training loss: 2.8493560197302217
Validation loss: 2.5121312729633383

Epoch: 5| Step: 7
Training loss: 1.8426544684743797
Validation loss: 2.5140745822955197

Epoch: 5| Step: 8
Training loss: 2.540921982865718
Validation loss: 2.5066501661042486

Epoch: 5| Step: 9
Training loss: 2.494555075197451
Validation loss: 2.5153173688993955

Epoch: 5| Step: 10
Training loss: 2.1785278516962907
Validation loss: 2.5138278706283494

Epoch: 5| Step: 11
Training loss: 1.9355020373527587
Validation loss: 2.508679645073244

Epoch: 233| Step: 0
Training loss: 2.1103551777213365
Validation loss: 2.508010080066808

Epoch: 5| Step: 1
Training loss: 2.922854988460655
Validation loss: 2.5100772491032135

Epoch: 5| Step: 2
Training loss: 2.341945716639277
Validation loss: 2.5191749064476574

Epoch: 5| Step: 3
Training loss: 2.105526047530311
Validation loss: 2.5140440931292414

Epoch: 5| Step: 4
Training loss: 2.703638369812594
Validation loss: 2.507613569313931

Epoch: 5| Step: 5
Training loss: 2.501772824177676
Validation loss: 2.5060885734131233

Epoch: 5| Step: 6
Training loss: 2.0945958948172914
Validation loss: 2.5108098095541944

Epoch: 5| Step: 7
Training loss: 1.9269375376493332
Validation loss: 2.5187717209629565

Epoch: 5| Step: 8
Training loss: 2.5617292221893013
Validation loss: 2.5322490612187347

Epoch: 5| Step: 9
Training loss: 2.7434560770391543
Validation loss: 2.53877084866554

Epoch: 5| Step: 10
Training loss: 1.8437186416286884
Validation loss: 2.529586747888271

Epoch: 5| Step: 11
Training loss: 1.984088426457522
Validation loss: 2.5514457561465025

Epoch: 234| Step: 0
Training loss: 2.7464009054860847
Validation loss: 2.544736662707593

Epoch: 5| Step: 1
Training loss: 2.8510825393380257
Validation loss: 2.565394104709723

Epoch: 5| Step: 2
Training loss: 2.414009068956383
Validation loss: 2.537311239534832

Epoch: 5| Step: 3
Training loss: 1.9799071951030187
Validation loss: 2.533907045215787

Epoch: 5| Step: 4
Training loss: 2.6455359304292
Validation loss: 2.534917744769852

Epoch: 5| Step: 5
Training loss: 2.378856840074675
Validation loss: 2.5401305236345864

Epoch: 5| Step: 6
Training loss: 1.902715063183087
Validation loss: 2.532392061697343

Epoch: 5| Step: 7
Training loss: 2.5027705099903765
Validation loss: 2.5318754035800604

Epoch: 5| Step: 8
Training loss: 2.3776540482891844
Validation loss: 2.544717908850346

Epoch: 5| Step: 9
Training loss: 1.7732144522196944
Validation loss: 2.5329748799388287

Epoch: 5| Step: 10
Training loss: 1.9346112897327592
Validation loss: 2.5232538721376208

Epoch: 5| Step: 11
Training loss: 3.6895093454143604
Validation loss: 2.5309146180119217

Epoch: 235| Step: 0
Training loss: 2.7110367023005613
Validation loss: 2.5371312822507623

Epoch: 5| Step: 1
Training loss: 2.279130552053397
Validation loss: 2.5393102789068736

Epoch: 5| Step: 2
Training loss: 2.101071665569707
Validation loss: 2.5507028556955307

Epoch: 5| Step: 3
Training loss: 1.6903105043105737
Validation loss: 2.5647348334907667

Epoch: 5| Step: 4
Training loss: 3.058716285560293
Validation loss: 2.5684285627167

Epoch: 5| Step: 5
Training loss: 2.2207154715672064
Validation loss: 2.556931174379258

Epoch: 5| Step: 6
Training loss: 2.6227290911838734
Validation loss: 2.570953393592361

Epoch: 5| Step: 7
Training loss: 2.8090539801881413
Validation loss: 2.5642904020321753

Epoch: 5| Step: 8
Training loss: 1.6730794400669258
Validation loss: 2.5415178330937964

Epoch: 5| Step: 9
Training loss: 2.3355815705591607
Validation loss: 2.5283094955729624

Epoch: 5| Step: 10
Training loss: 2.253867957254327
Validation loss: 2.5326798172276543

Epoch: 5| Step: 11
Training loss: 1.0633030269054853
Validation loss: 2.554364338538254

Epoch: 236| Step: 0
Training loss: 2.0479445829716916
Validation loss: 2.52957651759975

Epoch: 5| Step: 1
Training loss: 2.5894116807936536
Validation loss: 2.5388336978427857

Epoch: 5| Step: 2
Training loss: 2.521939804586978
Validation loss: 2.5602916568028906

Epoch: 5| Step: 3
Training loss: 2.837478933382184
Validation loss: 2.56701930372145

Epoch: 5| Step: 4
Training loss: 2.207596157999457
Validation loss: 2.529588742887891

Epoch: 5| Step: 5
Training loss: 2.615725162826058
Validation loss: 2.5550199655914825

Epoch: 5| Step: 6
Training loss: 2.3999120298793595
Validation loss: 2.5375848333932614

Epoch: 5| Step: 7
Training loss: 2.5110305626458924
Validation loss: 2.554707946073352

Epoch: 5| Step: 8
Training loss: 2.1176412736589283
Validation loss: 2.5533484586420427

Epoch: 5| Step: 9
Training loss: 1.7482579279670334
Validation loss: 2.545455068736828

Epoch: 5| Step: 10
Training loss: 1.9330774099496548
Validation loss: 2.5457059794557226

Epoch: 5| Step: 11
Training loss: 1.3623946840372942
Validation loss: 2.531064038451105

Epoch: 237| Step: 0
Training loss: 2.6277289964244606
Validation loss: 2.512918803593047

Epoch: 5| Step: 1
Training loss: 2.619261054016131
Validation loss: 2.5143578657147563

Epoch: 5| Step: 2
Training loss: 2.444051106432116
Validation loss: 2.515407337620903

Epoch: 5| Step: 3
Training loss: 1.9274620783695995
Validation loss: 2.52025759377597

Epoch: 5| Step: 4
Training loss: 2.282775055606288
Validation loss: 2.521619332678828

Epoch: 5| Step: 5
Training loss: 2.6346061777064302
Validation loss: 2.5148038135907593

Epoch: 5| Step: 6
Training loss: 2.560221333472399
Validation loss: 2.5234806423219527

Epoch: 5| Step: 7
Training loss: 2.36334228436811
Validation loss: 2.5354515612940283

Epoch: 5| Step: 8
Training loss: 2.3942953618964347
Validation loss: 2.535838951660795

Epoch: 5| Step: 9
Training loss: 2.3646886748881526
Validation loss: 2.5429249343183455

Epoch: 5| Step: 10
Training loss: 1.854364070313698
Validation loss: 2.562115973873009

Epoch: 5| Step: 11
Training loss: 0.827364734531224
Validation loss: 2.559455470216258

Epoch: 238| Step: 0
Training loss: 2.2509319706469837
Validation loss: 2.5464703447088337

Epoch: 5| Step: 1
Training loss: 1.748928491346524
Validation loss: 2.550407189461331

Epoch: 5| Step: 2
Training loss: 2.8169027835211287
Validation loss: 2.548582401829984

Epoch: 5| Step: 3
Training loss: 2.717858442374596
Validation loss: 2.534555435499551

Epoch: 5| Step: 4
Training loss: 1.8009932717976558
Validation loss: 2.549849943345989

Epoch: 5| Step: 5
Training loss: 2.580543077664776
Validation loss: 2.544734230647727

Epoch: 5| Step: 6
Training loss: 2.149550603873866
Validation loss: 2.5434783253102107

Epoch: 5| Step: 7
Training loss: 2.3876098367880485
Validation loss: 2.564011089484464

Epoch: 5| Step: 8
Training loss: 2.6974528342322714
Validation loss: 2.5570296148739193

Epoch: 5| Step: 9
Training loss: 2.154105958753391
Validation loss: 2.574951748642791

Epoch: 5| Step: 10
Training loss: 2.2394166633037105
Validation loss: 2.5913138841481937

Epoch: 5| Step: 11
Training loss: 1.1040627532621816
Validation loss: 2.5879917916730806

Epoch: 239| Step: 0
Training loss: 2.651425726650032
Validation loss: 2.566599149146214

Epoch: 5| Step: 1
Training loss: 2.6461919306247634
Validation loss: 2.556675257204172

Epoch: 5| Step: 2
Training loss: 2.2335023042706657
Validation loss: 2.540982636402028

Epoch: 5| Step: 3
Training loss: 2.7180621274481958
Validation loss: 2.53488238422334

Epoch: 5| Step: 4
Training loss: 1.9397583382308436
Validation loss: 2.523145034738215

Epoch: 5| Step: 5
Training loss: 1.9613508419566705
Validation loss: 2.5446301261564264

Epoch: 5| Step: 6
Training loss: 2.3813570173751057
Validation loss: 2.532986006381967

Epoch: 5| Step: 7
Training loss: 2.3979251078156865
Validation loss: 2.5502842472011764

Epoch: 5| Step: 8
Training loss: 2.438385338045079
Validation loss: 2.5391029276441457

Epoch: 5| Step: 9
Training loss: 2.021366075380574
Validation loss: 2.5680045304582766

Epoch: 5| Step: 10
Training loss: 2.3533926397456675
Validation loss: 2.5471196398421787

Epoch: 5| Step: 11
Training loss: 1.8509389092798905
Validation loss: 2.5606548054768106

Epoch: 240| Step: 0
Training loss: 1.7733366009077405
Validation loss: 2.548758553326423

Epoch: 5| Step: 1
Training loss: 2.1050299079202
Validation loss: 2.552235537810579

Epoch: 5| Step: 2
Training loss: 2.390541324522391
Validation loss: 2.5406573602821587

Epoch: 5| Step: 3
Training loss: 2.5955063721636082
Validation loss: 2.5112841572771707

Epoch: 5| Step: 4
Training loss: 1.6769208967629463
Validation loss: 2.5228524194113344

Epoch: 5| Step: 5
Training loss: 2.7415709930083136
Validation loss: 2.5135216736001675

Epoch: 5| Step: 6
Training loss: 2.1123940796792082
Validation loss: 2.5237333362489593

Epoch: 5| Step: 7
Training loss: 2.4645245280361876
Validation loss: 2.5181918662516245

Epoch: 5| Step: 8
Training loss: 2.0881458265675494
Validation loss: 2.5237508722914384

Epoch: 5| Step: 9
Training loss: 2.4479694983009526
Validation loss: 2.537399451607938

Epoch: 5| Step: 10
Training loss: 2.7458358795509246
Validation loss: 2.5408545874806325

Epoch: 5| Step: 11
Training loss: 3.480045656803586
Validation loss: 2.558735524683094

Epoch: 241| Step: 0
Training loss: 2.776935788866621
Validation loss: 2.5847562606324432

Epoch: 5| Step: 1
Training loss: 2.6250685728289924
Validation loss: 2.6850962201784343

Epoch: 5| Step: 2
Training loss: 2.2745784243155516
Validation loss: 2.745761480438073

Epoch: 5| Step: 3
Training loss: 2.8775165987571727
Validation loss: 2.718346573140381

Epoch: 5| Step: 4
Training loss: 1.703678146268895
Validation loss: 2.646956525775864

Epoch: 5| Step: 5
Training loss: 2.5012599631077244
Validation loss: 2.6046613693833542

Epoch: 5| Step: 6
Training loss: 2.7632009799246493
Validation loss: 2.5508563238510074

Epoch: 5| Step: 7
Training loss: 2.107739867365094
Validation loss: 2.5177724490500255

Epoch: 5| Step: 8
Training loss: 2.751768930415293
Validation loss: 2.515236715991159

Epoch: 5| Step: 9
Training loss: 2.0605959339538997
Validation loss: 2.5005880180878006

Epoch: 5| Step: 10
Training loss: 1.985274644103773
Validation loss: 2.511568008574392

Epoch: 5| Step: 11
Training loss: 2.683861509510855
Validation loss: 2.499880915030779

Epoch: 242| Step: 0
Training loss: 2.558668482031219
Validation loss: 2.5076531313033916

Epoch: 5| Step: 1
Training loss: 2.440104926620468
Validation loss: 2.515516393303397

Epoch: 5| Step: 2
Training loss: 2.6286644562258785
Validation loss: 2.5096891913544663

Epoch: 5| Step: 3
Training loss: 3.129822481380312
Validation loss: 2.514067106240753

Epoch: 5| Step: 4
Training loss: 2.5182268418173464
Validation loss: 2.4984038025645385

Epoch: 5| Step: 5
Training loss: 1.9285290905175405
Validation loss: 2.500673493424642

Epoch: 5| Step: 6
Training loss: 1.6478558277935347
Validation loss: 2.4984832772201755

Epoch: 5| Step: 7
Training loss: 2.1871240565275114
Validation loss: 2.4878807044913294

Epoch: 5| Step: 8
Training loss: 2.515152217437323
Validation loss: 2.50427077204472

Epoch: 5| Step: 9
Training loss: 2.730724404608744
Validation loss: 2.4997646260721766

Epoch: 5| Step: 10
Training loss: 2.129855275140072
Validation loss: 2.50036105091921

Epoch: 5| Step: 11
Training loss: 4.139567920417423
Validation loss: 2.5068140547832196

Epoch: 243| Step: 0
Training loss: 3.161257256152241
Validation loss: 2.53573470136226

Epoch: 5| Step: 1
Training loss: 2.361082356097082
Validation loss: 2.546994387864968

Epoch: 5| Step: 2
Training loss: 2.218913005160183
Validation loss: 2.552992029668965

Epoch: 5| Step: 3
Training loss: 2.409627451454574
Validation loss: 2.542792345873256

Epoch: 5| Step: 4
Training loss: 3.0060172135601975
Validation loss: 2.5503652602434195

Epoch: 5| Step: 5
Training loss: 2.1071623323293336
Validation loss: 2.5509223645809715

Epoch: 5| Step: 6
Training loss: 2.134309459925514
Validation loss: 2.5504967539136647

Epoch: 5| Step: 7
Training loss: 2.149555151406446
Validation loss: 2.5262671471601634

Epoch: 5| Step: 8
Training loss: 2.343387830090614
Validation loss: 2.5310472713337964

Epoch: 5| Step: 9
Training loss: 2.319064231617976
Validation loss: 2.5282651743715596

Epoch: 5| Step: 10
Training loss: 2.2414215587307247
Validation loss: 2.502984419298823

Epoch: 5| Step: 11
Training loss: 1.3553812399986789
Validation loss: 2.5250167038808815

Epoch: 244| Step: 0
Training loss: 3.053413613303755
Validation loss: 2.5117088899077764

Epoch: 5| Step: 1
Training loss: 2.022189665044393
Validation loss: 2.5069504996379544

Epoch: 5| Step: 2
Training loss: 2.519533331818423
Validation loss: 2.5280096148772575

Epoch: 5| Step: 3
Training loss: 2.3720321181054294
Validation loss: 2.5153583164112665

Epoch: 5| Step: 4
Training loss: 1.8670334373344464
Validation loss: 2.5285131823667752

Epoch: 5| Step: 5
Training loss: 2.5035932467256035
Validation loss: 2.5287503981389605

Epoch: 5| Step: 6
Training loss: 2.7555312969138064
Validation loss: 2.542442775227367

Epoch: 5| Step: 7
Training loss: 2.294715021428921
Validation loss: 2.54837546975715

Epoch: 5| Step: 8
Training loss: 2.405930336860907
Validation loss: 2.5362131103369694

Epoch: 5| Step: 9
Training loss: 2.1005447362373233
Validation loss: 2.5360851770548747

Epoch: 5| Step: 10
Training loss: 1.7464611511802337
Validation loss: 2.5316463874367154

Epoch: 5| Step: 11
Training loss: 1.7711038569838022
Validation loss: 2.5289248591804765

Epoch: 245| Step: 0
Training loss: 2.691326167848863
Validation loss: 2.5257743707137026

Epoch: 5| Step: 1
Training loss: 3.1750891425166707
Validation loss: 2.5265914842983324

Epoch: 5| Step: 2
Training loss: 1.9358300426300277
Validation loss: 2.551441887930523

Epoch: 5| Step: 3
Training loss: 2.1230924683898005
Validation loss: 2.5586172437681207

Epoch: 5| Step: 4
Training loss: 1.7779016981380573
Validation loss: 2.565774353054004

Epoch: 5| Step: 5
Training loss: 2.0290722249930586
Validation loss: 2.554868576989604

Epoch: 5| Step: 6
Training loss: 2.6811899138433777
Validation loss: 2.5479580111409956

Epoch: 5| Step: 7
Training loss: 1.5966112967960113
Validation loss: 2.53824774948091

Epoch: 5| Step: 8
Training loss: 2.823276220041982
Validation loss: 2.534102778900055

Epoch: 5| Step: 9
Training loss: 2.381413884167874
Validation loss: 2.51174099735831

Epoch: 5| Step: 10
Training loss: 2.1864583396336115
Validation loss: 2.51668066774562

Epoch: 5| Step: 11
Training loss: 2.55872765110619
Validation loss: 2.5179769527101667

Epoch: 246| Step: 0
Training loss: 1.9837903941319863
Validation loss: 2.517957212457392

Epoch: 5| Step: 1
Training loss: 2.1124087522764943
Validation loss: 2.520374320671879

Epoch: 5| Step: 2
Training loss: 2.9378658634983967
Validation loss: 2.5218717916185405

Epoch: 5| Step: 3
Training loss: 3.019066462702234
Validation loss: 2.539710274252329

Epoch: 5| Step: 4
Training loss: 2.866703335579249
Validation loss: 2.5466966235019974

Epoch: 5| Step: 5
Training loss: 2.0891417848525258
Validation loss: 2.533164703226783

Epoch: 5| Step: 6
Training loss: 2.066605152732327
Validation loss: 2.5501945642656856

Epoch: 5| Step: 7
Training loss: 2.078196818322566
Validation loss: 2.5695607323722465

Epoch: 5| Step: 8
Training loss: 2.7405102591598345
Validation loss: 2.5929409779969976

Epoch: 5| Step: 9
Training loss: 2.074250014477183
Validation loss: 2.5746551610693116

Epoch: 5| Step: 10
Training loss: 1.615285667333836
Validation loss: 2.572174541817846

Epoch: 5| Step: 11
Training loss: 1.170118719745283
Validation loss: 2.5814146304952272

Epoch: 247| Step: 0
Training loss: 2.0278560748145695
Validation loss: 2.5690364046839846

Epoch: 5| Step: 1
Training loss: 3.1863966883598818
Validation loss: 2.565210312456045

Epoch: 5| Step: 2
Training loss: 2.0952923915276482
Validation loss: 2.5567809615974255

Epoch: 5| Step: 3
Training loss: 1.8364741413592252
Validation loss: 2.5420895612905388

Epoch: 5| Step: 4
Training loss: 2.496389643125683
Validation loss: 2.523044094917501

Epoch: 5| Step: 5
Training loss: 2.496820908057208
Validation loss: 2.5027925312606105

Epoch: 5| Step: 6
Training loss: 2.1657123542551284
Validation loss: 2.5099012088394836

Epoch: 5| Step: 7
Training loss: 1.849869094547414
Validation loss: 2.5023521246141223

Epoch: 5| Step: 8
Training loss: 2.966866950085851
Validation loss: 2.4859297983436677

Epoch: 5| Step: 9
Training loss: 2.3275324944380036
Validation loss: 2.5009312921647435

Epoch: 5| Step: 10
Training loss: 1.9590330564738876
Validation loss: 2.5077425232945263

Epoch: 5| Step: 11
Training loss: 3.0418475023276548
Validation loss: 2.520416313283194

Epoch: 248| Step: 0
Training loss: 2.2904342342945405
Validation loss: 2.526644677355439

Epoch: 5| Step: 1
Training loss: 2.5442350782669214
Validation loss: 2.5354434155881878

Epoch: 5| Step: 2
Training loss: 2.8756543948904114
Validation loss: 2.548184376145604

Epoch: 5| Step: 3
Training loss: 2.4144115010543126
Validation loss: 2.5458549805036657

Epoch: 5| Step: 4
Training loss: 2.5252688349765475
Validation loss: 2.555700670892851

Epoch: 5| Step: 5
Training loss: 2.0486806810998077
Validation loss: 2.567353765111261

Epoch: 5| Step: 6
Training loss: 2.474188307396295
Validation loss: 2.580461014033352

Epoch: 5| Step: 7
Training loss: 2.3882516286982787
Validation loss: 2.5701297436086237

Epoch: 5| Step: 8
Training loss: 2.159012172486205
Validation loss: 2.5837220481451917

Epoch: 5| Step: 9
Training loss: 1.800386464169396
Validation loss: 2.568536026636704

Epoch: 5| Step: 10
Training loss: 2.572134375210717
Validation loss: 2.5626982286586526

Epoch: 5| Step: 11
Training loss: 1.4306361485384762
Validation loss: 2.5261389107355203

Epoch: 249| Step: 0
Training loss: 2.2000622393733997
Validation loss: 2.5196746351659662

Epoch: 5| Step: 1
Training loss: 2.0247472110780422
Validation loss: 2.5154430647429487

Epoch: 5| Step: 2
Training loss: 2.62772019542522
Validation loss: 2.5152830440391916

Epoch: 5| Step: 3
Training loss: 2.6523204684464674
Validation loss: 2.5266371323369983

Epoch: 5| Step: 4
Training loss: 2.088693234341764
Validation loss: 2.5150761015345164

Epoch: 5| Step: 5
Training loss: 2.1837376528359753
Validation loss: 2.5268476648870535

Epoch: 5| Step: 6
Training loss: 2.397149948802677
Validation loss: 2.5293275652070437

Epoch: 5| Step: 7
Training loss: 2.149931786807955
Validation loss: 2.522032449905283

Epoch: 5| Step: 8
Training loss: 2.2141858289313214
Validation loss: 2.502900939927137

Epoch: 5| Step: 9
Training loss: 2.56013919660507
Validation loss: 2.5230900669963283

Epoch: 5| Step: 10
Training loss: 2.2652458136687934
Validation loss: 2.5187323592509268

Epoch: 5| Step: 11
Training loss: 3.576265209902993
Validation loss: 2.5050235661650038

Epoch: 250| Step: 0
Training loss: 2.5055990462982196
Validation loss: 2.498896712995344

Epoch: 5| Step: 1
Training loss: 2.2364464607746863
Validation loss: 2.501365427819972

Epoch: 5| Step: 2
Training loss: 2.5294471254238426
Validation loss: 2.5053961571744723

Epoch: 5| Step: 3
Training loss: 2.0660949357147724
Validation loss: 2.5173357523083006

Epoch: 5| Step: 4
Training loss: 2.4853016786496815
Validation loss: 2.5277586315871505

Epoch: 5| Step: 5
Training loss: 1.9925058147963008
Validation loss: 2.5094918899579897

Epoch: 5| Step: 6
Training loss: 2.292000509278672
Validation loss: 2.5200769193328902

Epoch: 5| Step: 7
Training loss: 2.7390703269729157
Validation loss: 2.510011704944457

Epoch: 5| Step: 8
Training loss: 2.3288030469077072
Validation loss: 2.520693157701872

Epoch: 5| Step: 9
Training loss: 2.2433522884065003
Validation loss: 2.5288192946197854

Epoch: 5| Step: 10
Training loss: 2.1044261382109144
Validation loss: 2.517532383730367

Epoch: 5| Step: 11
Training loss: 3.0021589775711957
Validation loss: 2.5165411345224156

Epoch: 251| Step: 0
Training loss: 1.6509511142612
Validation loss: 2.520810920573661

Epoch: 5| Step: 1
Training loss: 2.745286543435203
Validation loss: 2.532160399024093

Epoch: 5| Step: 2
Training loss: 2.2392811297780213
Validation loss: 2.513417343456595

Epoch: 5| Step: 3
Training loss: 2.32846220505873
Validation loss: 2.5154268451308024

Epoch: 5| Step: 4
Training loss: 2.7233820950622345
Validation loss: 2.4978079921465857

Epoch: 5| Step: 5
Training loss: 2.776812152447563
Validation loss: 2.5084637264776704

Epoch: 5| Step: 6
Training loss: 2.4481732389573203
Validation loss: 2.5056013815427898

Epoch: 5| Step: 7
Training loss: 1.7368240935397068
Validation loss: 2.505320898740386

Epoch: 5| Step: 8
Training loss: 1.8850739380074089
Validation loss: 2.502942554938414

Epoch: 5| Step: 9
Training loss: 2.5895077126331603
Validation loss: 2.4956831298320097

Epoch: 5| Step: 10
Training loss: 2.4075794263118806
Validation loss: 2.50213953892588

Epoch: 5| Step: 11
Training loss: 1.3432328759455658
Validation loss: 2.52199835036398

Epoch: 252| Step: 0
Training loss: 2.149651091009849
Validation loss: 2.5294990056203033

Epoch: 5| Step: 1
Training loss: 2.7584813849446994
Validation loss: 2.528564767425293

Epoch: 5| Step: 2
Training loss: 2.3402135999302156
Validation loss: 2.550314342066676

Epoch: 5| Step: 3
Training loss: 2.477642605985699
Validation loss: 2.5506363964379264

Epoch: 5| Step: 4
Training loss: 2.300014918735052
Validation loss: 2.5497049706395796

Epoch: 5| Step: 5
Training loss: 2.282201699091672
Validation loss: 2.5628089485939256

Epoch: 5| Step: 6
Training loss: 2.2750075120068285
Validation loss: 2.5342354417375033

Epoch: 5| Step: 7
Training loss: 2.2435586704296173
Validation loss: 2.528741386224165

Epoch: 5| Step: 8
Training loss: 1.847576115424969
Validation loss: 2.5190207503881195

Epoch: 5| Step: 9
Training loss: 2.4255160618307046
Validation loss: 2.5068160639431953

Epoch: 5| Step: 10
Training loss: 2.507653727511446
Validation loss: 2.502760797230844

Epoch: 5| Step: 11
Training loss: 1.9692724836440605
Validation loss: 2.4994513942868433

Epoch: 253| Step: 0
Training loss: 2.150670226557498
Validation loss: 2.5072629807821145

Epoch: 5| Step: 1
Training loss: 2.09971861089387
Validation loss: 2.4978640074080234

Epoch: 5| Step: 2
Training loss: 2.397847652905962
Validation loss: 2.5094922620675932

Epoch: 5| Step: 3
Training loss: 1.7985008513613254
Validation loss: 2.5151507066749845

Epoch: 5| Step: 4
Training loss: 2.471077316957986
Validation loss: 2.502778047573862

Epoch: 5| Step: 5
Training loss: 2.3625595125644847
Validation loss: 2.5234248787925506

Epoch: 5| Step: 6
Training loss: 2.5552981518835587
Validation loss: 2.5173428792768284

Epoch: 5| Step: 7
Training loss: 2.6444045787320984
Validation loss: 2.519765329453297

Epoch: 5| Step: 8
Training loss: 2.230258697934097
Validation loss: 2.531595355677181

Epoch: 5| Step: 9
Training loss: 2.595340562791584
Validation loss: 2.5383982136646557

Epoch: 5| Step: 10
Training loss: 2.352167805680088
Validation loss: 2.5626152059197542

Epoch: 5| Step: 11
Training loss: 2.3523200452934763
Validation loss: 2.572968900719186

Epoch: 254| Step: 0
Training loss: 2.213250665423057
Validation loss: 2.578472214254246

Epoch: 5| Step: 1
Training loss: 2.497065538529363
Validation loss: 2.5833251758159514

Epoch: 5| Step: 2
Training loss: 2.2350975948932317
Validation loss: 2.594660411523171

Epoch: 5| Step: 3
Training loss: 2.3710375406807938
Validation loss: 2.571294227959042

Epoch: 5| Step: 4
Training loss: 2.1519112541874112
Validation loss: 2.567345225340883

Epoch: 5| Step: 5
Training loss: 1.84077951401241
Validation loss: 2.5394615740034787

Epoch: 5| Step: 6
Training loss: 2.602098564386395
Validation loss: 2.524159799507483

Epoch: 5| Step: 7
Training loss: 2.119795540720956
Validation loss: 2.5141198925598074

Epoch: 5| Step: 8
Training loss: 2.386893957421873
Validation loss: 2.5117101911415785

Epoch: 5| Step: 9
Training loss: 1.9888094517077362
Validation loss: 2.5057387685951897

Epoch: 5| Step: 10
Training loss: 2.8544246993176587
Validation loss: 2.4935604746064586

Epoch: 5| Step: 11
Training loss: 3.4358802447160866
Validation loss: 2.5055958625911643

Epoch: 255| Step: 0
Training loss: 2.008594523918985
Validation loss: 2.497509132708404

Epoch: 5| Step: 1
Training loss: 2.783259148284836
Validation loss: 2.5055818114123247

Epoch: 5| Step: 2
Training loss: 2.133683358723353
Validation loss: 2.518429403355546

Epoch: 5| Step: 3
Training loss: 2.0644764533479725
Validation loss: 2.525559199686797

Epoch: 5| Step: 4
Training loss: 2.5763503006349953
Validation loss: 2.51305950839996

Epoch: 5| Step: 5
Training loss: 1.8287389001620271
Validation loss: 2.499647763392611

Epoch: 5| Step: 6
Training loss: 2.2296105652910927
Validation loss: 2.507707801606036

Epoch: 5| Step: 7
Training loss: 2.629872749499175
Validation loss: 2.4964538258785605

Epoch: 5| Step: 8
Training loss: 2.2471875521145677
Validation loss: 2.4929396272681896

Epoch: 5| Step: 9
Training loss: 2.149890644042518
Validation loss: 2.5173717815713803

Epoch: 5| Step: 10
Training loss: 2.7272348531347337
Validation loss: 2.5203154297236368

Epoch: 5| Step: 11
Training loss: 3.7369700078375714
Validation loss: 2.5240853681618995

Epoch: 256| Step: 0
Training loss: 2.5900016264744665
Validation loss: 2.5234658758341535

Epoch: 5| Step: 1
Training loss: 2.5886316929800257
Validation loss: 2.5167131933328286

Epoch: 5| Step: 2
Training loss: 2.445947046793735
Validation loss: 2.5083038229067114

Epoch: 5| Step: 3
Training loss: 1.953110229436336
Validation loss: 2.520546898187167

Epoch: 5| Step: 4
Training loss: 2.5679375247664256
Validation loss: 2.5155778686973433

Epoch: 5| Step: 5
Training loss: 2.4685996287459573
Validation loss: 2.508169075560944

Epoch: 5| Step: 6
Training loss: 2.5189957870320963
Validation loss: 2.535047006628946

Epoch: 5| Step: 7
Training loss: 2.3362553102811545
Validation loss: 2.5143769704710017

Epoch: 5| Step: 8
Training loss: 1.5894383468934676
Validation loss: 2.543692492163818

Epoch: 5| Step: 9
Training loss: 1.670365980557161
Validation loss: 2.5523029051284607

Epoch: 5| Step: 10
Training loss: 2.642836327636521
Validation loss: 2.5516820838690037

Epoch: 5| Step: 11
Training loss: 2.331433499148047
Validation loss: 2.532029581309703

Epoch: 257| Step: 0
Training loss: 2.053980484495526
Validation loss: 2.5301720517790987

Epoch: 5| Step: 1
Training loss: 2.1691805365469876
Validation loss: 2.5247684625971125

Epoch: 5| Step: 2
Training loss: 2.5831910268493634
Validation loss: 2.522661020749534

Epoch: 5| Step: 3
Training loss: 1.8983547365334525
Validation loss: 2.530236853832343

Epoch: 5| Step: 4
Training loss: 1.7476448832906646
Validation loss: 2.5173456692861444

Epoch: 5| Step: 5
Training loss: 2.6687736134969664
Validation loss: 2.5309019202630414

Epoch: 5| Step: 6
Training loss: 2.1423956237793855
Validation loss: 2.5180831098887073

Epoch: 5| Step: 7
Training loss: 2.702870263240933
Validation loss: 2.530431650472175

Epoch: 5| Step: 8
Training loss: 2.4382346586685255
Validation loss: 2.534686996848075

Epoch: 5| Step: 9
Training loss: 2.5378335179225533
Validation loss: 2.545981264004737

Epoch: 5| Step: 10
Training loss: 2.5051790950811075
Validation loss: 2.532203749732785

Epoch: 5| Step: 11
Training loss: 1.3816442349909546
Validation loss: 2.5418998378099493

Epoch: 258| Step: 0
Training loss: 2.2740992471111876
Validation loss: 2.531231836952381

Epoch: 5| Step: 1
Training loss: 1.794772169907986
Validation loss: 2.512879972842

Epoch: 5| Step: 2
Training loss: 3.0083314918519477
Validation loss: 2.5180138723078644

Epoch: 5| Step: 3
Training loss: 2.2299990184422542
Validation loss: 2.515627876817152

Epoch: 5| Step: 4
Training loss: 2.1724939356325272
Validation loss: 2.5282552491449812

Epoch: 5| Step: 5
Training loss: 2.425089026321198
Validation loss: 2.5232387854220026

Epoch: 5| Step: 6
Training loss: 2.2640009880265954
Validation loss: 2.5104007812778284

Epoch: 5| Step: 7
Training loss: 2.291198983298241
Validation loss: 2.5124640498586386

Epoch: 5| Step: 8
Training loss: 2.3436142945738654
Validation loss: 2.5246925028279894

Epoch: 5| Step: 9
Training loss: 2.0055669554682565
Validation loss: 2.5226357586877928

Epoch: 5| Step: 10
Training loss: 2.6039377544247206
Validation loss: 2.527171955038034

Epoch: 5| Step: 11
Training loss: 1.630952496639668
Validation loss: 2.5255350817525386

Epoch: 259| Step: 0
Training loss: 2.555399850810431
Validation loss: 2.5556570696076095

Epoch: 5| Step: 1
Training loss: 2.0367797701949493
Validation loss: 2.585494250276706

Epoch: 5| Step: 2
Training loss: 2.59011346899853
Validation loss: 2.594018661801992

Epoch: 5| Step: 3
Training loss: 2.400327652182008
Validation loss: 2.610408098755273

Epoch: 5| Step: 4
Training loss: 2.0592013481221683
Validation loss: 2.6286473744292578

Epoch: 5| Step: 5
Training loss: 1.8091814639298873
Validation loss: 2.585000858687966

Epoch: 5| Step: 6
Training loss: 2.787157107761808
Validation loss: 2.5457793689467114

Epoch: 5| Step: 7
Training loss: 1.7135247999789271
Validation loss: 2.5417228846247557

Epoch: 5| Step: 8
Training loss: 2.3329101133268706
Validation loss: 2.535124094268836

Epoch: 5| Step: 9
Training loss: 1.9565050099067682
Validation loss: 2.502137593504982

Epoch: 5| Step: 10
Training loss: 3.071547398217164
Validation loss: 2.5082677901270896

Epoch: 5| Step: 11
Training loss: 1.8557662564736923
Validation loss: 2.501411174809274

Epoch: 260| Step: 0
Training loss: 2.2702132495417198
Validation loss: 2.5076708728890735

Epoch: 5| Step: 1
Training loss: 1.6495904732158042
Validation loss: 2.5151216386409487

Epoch: 5| Step: 2
Training loss: 2.3433498803970325
Validation loss: 2.5130683354185535

Epoch: 5| Step: 3
Training loss: 2.590255681529725
Validation loss: 2.501836543229563

Epoch: 5| Step: 4
Training loss: 2.647079434657417
Validation loss: 2.5048985689595913

Epoch: 5| Step: 5
Training loss: 1.979687899835632
Validation loss: 2.5187114988717942

Epoch: 5| Step: 6
Training loss: 2.060997849420261
Validation loss: 2.5248044408789885

Epoch: 5| Step: 7
Training loss: 1.9515386013403277
Validation loss: 2.5459544891793215

Epoch: 5| Step: 8
Training loss: 2.2333316100763914
Validation loss: 2.584604421024575

Epoch: 5| Step: 9
Training loss: 3.058673726055687
Validation loss: 2.599844356609929

Epoch: 5| Step: 10
Training loss: 2.692737001208663
Validation loss: 2.626972603991588

Epoch: 5| Step: 11
Training loss: 2.2966308463984597
Validation loss: 2.6202277862679213

Epoch: 261| Step: 0
Training loss: 2.331137532239114
Validation loss: 2.6136828894614226

Epoch: 5| Step: 1
Training loss: 2.4929516616278655
Validation loss: 2.570830481376694

Epoch: 5| Step: 2
Training loss: 2.382987694475207
Validation loss: 2.5406390728938066

Epoch: 5| Step: 3
Training loss: 2.5031479090334017
Validation loss: 2.5387921976827044

Epoch: 5| Step: 4
Training loss: 2.22110526571331
Validation loss: 2.5461555080955454

Epoch: 5| Step: 5
Training loss: 2.6316789399775455
Validation loss: 2.525683286485652

Epoch: 5| Step: 6
Training loss: 1.88085392429279
Validation loss: 2.5378630441054817

Epoch: 5| Step: 7
Training loss: 2.109645119785104
Validation loss: 2.5229896565302274

Epoch: 5| Step: 8
Training loss: 2.517421957281822
Validation loss: 2.5187787492119447

Epoch: 5| Step: 9
Training loss: 2.217306822885359
Validation loss: 2.5176247493036112

Epoch: 5| Step: 10
Training loss: 1.6332537771169913
Validation loss: 2.5211919117043555

Epoch: 5| Step: 11
Training loss: 3.0392068470700284
Validation loss: 2.5406968358806195

Epoch: 262| Step: 0
Training loss: 2.1382419659229366
Validation loss: 2.5373442602048435

Epoch: 5| Step: 1
Training loss: 1.8166512929050054
Validation loss: 2.5488275325769587

Epoch: 5| Step: 2
Training loss: 2.1990845336070435
Validation loss: 2.5377515959386585

Epoch: 5| Step: 3
Training loss: 2.341992138635226
Validation loss: 2.5591513125707324

Epoch: 5| Step: 4
Training loss: 2.727953099868432
Validation loss: 2.5866709818529565

Epoch: 5| Step: 5
Training loss: 2.035578531982833
Validation loss: 2.5584116591869495

Epoch: 5| Step: 6
Training loss: 2.2896143537739206
Validation loss: 2.569673599872975

Epoch: 5| Step: 7
Training loss: 1.8111862156643364
Validation loss: 2.551871475197361

Epoch: 5| Step: 8
Training loss: 2.7916287282007475
Validation loss: 2.5433141080280515

Epoch: 5| Step: 9
Training loss: 2.45265290400978
Validation loss: 2.5420939576192136

Epoch: 5| Step: 10
Training loss: 2.7739700021091287
Validation loss: 2.5241184673973827

Epoch: 5| Step: 11
Training loss: 1.9890520141996413
Validation loss: 2.5157678141048674

Epoch: 263| Step: 0
Training loss: 1.9340171889771072
Validation loss: 2.5223712065840993

Epoch: 5| Step: 1
Training loss: 2.7954014140829457
Validation loss: 2.5202403605905905

Epoch: 5| Step: 2
Training loss: 2.230205674031035
Validation loss: 2.524343396888059

Epoch: 5| Step: 3
Training loss: 1.867721864219057
Validation loss: 2.5137213223170063

Epoch: 5| Step: 4
Training loss: 2.5326408991729994
Validation loss: 2.5181421043286605

Epoch: 5| Step: 5
Training loss: 1.960124177696961
Validation loss: 2.513076381695323

Epoch: 5| Step: 6
Training loss: 2.723032943083622
Validation loss: 2.5278203476533947

Epoch: 5| Step: 7
Training loss: 2.506943786940367
Validation loss: 2.519456583341044

Epoch: 5| Step: 8
Training loss: 1.83117714434268
Validation loss: 2.530100129568602

Epoch: 5| Step: 9
Training loss: 2.5346560697472817
Validation loss: 2.557730426687213

Epoch: 5| Step: 10
Training loss: 2.3231572139252203
Validation loss: 2.5655567692904353

Epoch: 5| Step: 11
Training loss: 2.541321392266772
Validation loss: 2.5777341854694464

Epoch: 264| Step: 0
Training loss: 2.509075856697658
Validation loss: 2.5726172019388427

Epoch: 5| Step: 1
Training loss: 2.54379085723532
Validation loss: 2.564489766750346

Epoch: 5| Step: 2
Training loss: 2.2633699967903183
Validation loss: 2.5832533990634636

Epoch: 5| Step: 3
Training loss: 2.260875907222439
Validation loss: 2.5597018727541063

Epoch: 5| Step: 4
Training loss: 2.6782331162198885
Validation loss: 2.5694905931320755

Epoch: 5| Step: 5
Training loss: 2.339695997460924
Validation loss: 2.5704972414297367

Epoch: 5| Step: 6
Training loss: 2.361092251970357
Validation loss: 2.557233282744836

Epoch: 5| Step: 7
Training loss: 2.1529498069146316
Validation loss: 2.5558605087008015

Epoch: 5| Step: 8
Training loss: 2.2906585441100438
Validation loss: 2.5526916375798963

Epoch: 5| Step: 9
Training loss: 2.417202681806315
Validation loss: 2.5381351459816712

Epoch: 5| Step: 10
Training loss: 1.320284103900794
Validation loss: 2.539819487189173

Epoch: 5| Step: 11
Training loss: 2.7855902249444644
Validation loss: 2.520014658839869

Epoch: 265| Step: 0
Training loss: 2.6249750226966913
Validation loss: 2.527743453857077

Epoch: 5| Step: 1
Training loss: 2.3328287623580226
Validation loss: 2.530396451012433

Epoch: 5| Step: 2
Training loss: 2.2135919064075007
Validation loss: 2.5236066617846578

Epoch: 5| Step: 3
Training loss: 2.2902775687032615
Validation loss: 2.5325225213817135

Epoch: 5| Step: 4
Training loss: 1.4909112875302664
Validation loss: 2.5359083489328866

Epoch: 5| Step: 5
Training loss: 2.0697639170378173
Validation loss: 2.546538855401983

Epoch: 5| Step: 6
Training loss: 2.19273534714345
Validation loss: 2.529170547924384

Epoch: 5| Step: 7
Training loss: 2.6132880012641677
Validation loss: 2.5110022243156696

Epoch: 5| Step: 8
Training loss: 2.1427422515405032
Validation loss: 2.5113261120130943

Epoch: 5| Step: 9
Training loss: 2.9979027729008494
Validation loss: 2.527595020128029

Epoch: 5| Step: 10
Training loss: 2.216372008087736
Validation loss: 2.5165350257210353

Epoch: 5| Step: 11
Training loss: 2.346880640259326
Validation loss: 2.5195035908287298

Epoch: 266| Step: 0
Training loss: 2.4600847007932276
Validation loss: 2.519323852046361

Epoch: 5| Step: 1
Training loss: 2.409352370971017
Validation loss: 2.5287792957624715

Epoch: 5| Step: 2
Training loss: 1.692269684124659
Validation loss: 2.529595250188708

Epoch: 5| Step: 3
Training loss: 2.79968078360405
Validation loss: 2.5352040243716933

Epoch: 5| Step: 4
Training loss: 2.590255865618557
Validation loss: 2.5467978409232757

Epoch: 5| Step: 5
Training loss: 1.9108446125152905
Validation loss: 2.532399120794445

Epoch: 5| Step: 6
Training loss: 2.631745708100791
Validation loss: 2.524440251207796

Epoch: 5| Step: 7
Training loss: 1.9058415334859002
Validation loss: 2.5475396964200114

Epoch: 5| Step: 8
Training loss: 2.5275473181033554
Validation loss: 2.526520458871141

Epoch: 5| Step: 9
Training loss: 1.9907657112441508
Validation loss: 2.5186975287500752

Epoch: 5| Step: 10
Training loss: 2.0507471862870394
Validation loss: 2.5424438184784384

Epoch: 5| Step: 11
Training loss: 2.053574177047122
Validation loss: 2.533461483790529

Epoch: 267| Step: 0
Training loss: 1.966176242887937
Validation loss: 2.5228421440951823

Epoch: 5| Step: 1
Training loss: 2.0441836069210937
Validation loss: 2.522804487922805

Epoch: 5| Step: 2
Training loss: 2.637992765849088
Validation loss: 2.547704694128051

Epoch: 5| Step: 3
Training loss: 2.9045204840995202
Validation loss: 2.540535109502774

Epoch: 5| Step: 4
Training loss: 1.853213061448426
Validation loss: 2.532332051712394

Epoch: 5| Step: 5
Training loss: 2.628877500742458
Validation loss: 2.5529750719254234

Epoch: 5| Step: 6
Training loss: 2.1843742927255425
Validation loss: 2.559008072170909

Epoch: 5| Step: 7
Training loss: 2.4731053431156087
Validation loss: 2.5363067187376256

Epoch: 5| Step: 8
Training loss: 2.3675547903421825
Validation loss: 2.535533622237304

Epoch: 5| Step: 9
Training loss: 2.0236731432527972
Validation loss: 2.540353511367099

Epoch: 5| Step: 10
Training loss: 1.7518705861052681
Validation loss: 2.5554286919827174

Epoch: 5| Step: 11
Training loss: 2.418334341682138
Validation loss: 2.56979976823383

Epoch: 268| Step: 0
Training loss: 1.8599785858890474
Validation loss: 2.5459544423562965

Epoch: 5| Step: 1
Training loss: 3.0406006987522924
Validation loss: 2.5370292073369027

Epoch: 5| Step: 2
Training loss: 2.3840852933897367
Validation loss: 2.54797080713633

Epoch: 5| Step: 3
Training loss: 2.087936529327577
Validation loss: 2.522059065109993

Epoch: 5| Step: 4
Training loss: 2.4153107314091606
Validation loss: 2.505935410108899

Epoch: 5| Step: 5
Training loss: 2.6101082325560254
Validation loss: 2.537594039001294

Epoch: 5| Step: 6
Training loss: 2.208663256009953
Validation loss: 2.553543137823316

Epoch: 5| Step: 7
Training loss: 1.6470624060651942
Validation loss: 2.5492886182099475

Epoch: 5| Step: 8
Training loss: 2.155068115078371
Validation loss: 2.54928667369826

Epoch: 5| Step: 9
Training loss: 2.412639111871243
Validation loss: 2.540154677021755

Epoch: 5| Step: 10
Training loss: 1.7857562836749163
Validation loss: 2.561818396625534

Epoch: 5| Step: 11
Training loss: 3.2689958177261462
Validation loss: 2.571585760156246

Epoch: 269| Step: 0
Training loss: 2.5554720639213455
Validation loss: 2.6138998229914834

Epoch: 5| Step: 1
Training loss: 2.548375629583831
Validation loss: 2.6327062523115616

Epoch: 5| Step: 2
Training loss: 2.432748124110626
Validation loss: 2.6584778960651114

Epoch: 5| Step: 3
Training loss: 2.3821426200417215
Validation loss: 2.6700901033375923

Epoch: 5| Step: 4
Training loss: 2.084250921991581
Validation loss: 2.5871249696519882

Epoch: 5| Step: 5
Training loss: 2.511431213342178
Validation loss: 2.555920283031858

Epoch: 5| Step: 6
Training loss: 1.50447329445309
Validation loss: 2.5344478947903863

Epoch: 5| Step: 7
Training loss: 2.0921631678447623
Validation loss: 2.5293405104399413

Epoch: 5| Step: 8
Training loss: 1.897711344170835
Validation loss: 2.5091576815857835

Epoch: 5| Step: 9
Training loss: 2.931915493963821
Validation loss: 2.5360429738334194

Epoch: 5| Step: 10
Training loss: 2.0774582101449552
Validation loss: 2.5244306296930774

Epoch: 5| Step: 11
Training loss: 2.9292668968912094
Validation loss: 2.520532268221763

Epoch: 270| Step: 0
Training loss: 2.009404008101267
Validation loss: 2.530876218411288

Epoch: 5| Step: 1
Training loss: 2.3603909559833283
Validation loss: 2.5133763129402786

Epoch: 5| Step: 2
Training loss: 1.8264620137372478
Validation loss: 2.5184177984197658

Epoch: 5| Step: 3
Training loss: 2.8970641303846767
Validation loss: 2.524183696415671

Epoch: 5| Step: 4
Training loss: 2.7205047206462614
Validation loss: 2.5516490365635454

Epoch: 5| Step: 5
Training loss: 2.177315025838182
Validation loss: 2.542345418872652

Epoch: 5| Step: 6
Training loss: 1.7543092532284992
Validation loss: 2.558124310862839

Epoch: 5| Step: 7
Training loss: 1.877628264534882
Validation loss: 2.5611401803368308

Epoch: 5| Step: 8
Training loss: 2.1581991480449783
Validation loss: 2.552906249490832

Epoch: 5| Step: 9
Training loss: 1.8984624288044663
Validation loss: 2.5696860596463607

Epoch: 5| Step: 10
Training loss: 2.6746179432632124
Validation loss: 2.575340096226663

Epoch: 5| Step: 11
Training loss: 4.208510794847931
Validation loss: 2.578740749616823

Epoch: 271| Step: 0
Training loss: 2.595883698278448
Validation loss: 2.5854072795536642

Epoch: 5| Step: 1
Training loss: 2.147916085520338
Validation loss: 2.6174895306814756

Epoch: 5| Step: 2
Training loss: 2.314085854793048
Validation loss: 2.6022435538812236

Epoch: 5| Step: 3
Training loss: 2.2351580761472354
Validation loss: 2.57388745782444

Epoch: 5| Step: 4
Training loss: 2.9046211189946387
Validation loss: 2.5717811794480543

Epoch: 5| Step: 5
Training loss: 2.4590365379321337
Validation loss: 2.5429457640500206

Epoch: 5| Step: 6
Training loss: 1.8623731787598616
Validation loss: 2.5055667808457005

Epoch: 5| Step: 7
Training loss: 2.109149836072786
Validation loss: 2.5038803781649666

Epoch: 5| Step: 8
Training loss: 2.6755126818344066
Validation loss: 2.5207967413986645

Epoch: 5| Step: 9
Training loss: 2.316469034048495
Validation loss: 2.497694402726862

Epoch: 5| Step: 10
Training loss: 2.0240557694422603
Validation loss: 2.511265703458852

Epoch: 5| Step: 11
Training loss: 1.1270904192933855
Validation loss: 2.534775582423948

Epoch: 272| Step: 0
Training loss: 2.2485516442969042
Validation loss: 2.504647665407074

Epoch: 5| Step: 1
Training loss: 2.325856787211263
Validation loss: 2.4983045471455205

Epoch: 5| Step: 2
Training loss: 2.4471048727007196
Validation loss: 2.5152781229588244

Epoch: 5| Step: 3
Training loss: 1.5602386131952992
Validation loss: 2.52522277274209

Epoch: 5| Step: 4
Training loss: 2.892758628472998
Validation loss: 2.5282072923844656

Epoch: 5| Step: 5
Training loss: 2.449748832128538
Validation loss: 2.533963173968257

Epoch: 5| Step: 6
Training loss: 1.99124673808394
Validation loss: 2.5341834938643037

Epoch: 5| Step: 7
Training loss: 2.6379144966607835
Validation loss: 2.5597377869308797

Epoch: 5| Step: 8
Training loss: 2.283953488726461
Validation loss: 2.5721567469296227

Epoch: 5| Step: 9
Training loss: 2.310669251632932
Validation loss: 2.5577327298709305

Epoch: 5| Step: 10
Training loss: 2.1207775350948617
Validation loss: 2.548335633618864

Epoch: 5| Step: 11
Training loss: 2.5003461598115324
Validation loss: 2.5244695483958997

Epoch: 273| Step: 0
Training loss: 2.4579982098106528
Validation loss: 2.496646439837922

Epoch: 5| Step: 1
Training loss: 1.9227121282252038
Validation loss: 2.5024041854447376

Epoch: 5| Step: 2
Training loss: 2.4587791060260424
Validation loss: 2.5041111323206966

Epoch: 5| Step: 3
Training loss: 2.9517597897912387
Validation loss: 2.486350985110049

Epoch: 5| Step: 4
Training loss: 1.816100654736616
Validation loss: 2.5041768943928973

Epoch: 5| Step: 5
Training loss: 1.9860253267353989
Validation loss: 2.49690514372453

Epoch: 5| Step: 6
Training loss: 2.6760670752907294
Validation loss: 2.499576572481773

Epoch: 5| Step: 7
Training loss: 2.1765863925255062
Validation loss: 2.48635772343848

Epoch: 5| Step: 8
Training loss: 2.2129929763813494
Validation loss: 2.4898556090879342

Epoch: 5| Step: 9
Training loss: 1.4370771698429212
Validation loss: 2.4899204629747227

Epoch: 5| Step: 10
Training loss: 2.8321174555845245
Validation loss: 2.4760494924273706

Epoch: 5| Step: 11
Training loss: 2.4810501979068644
Validation loss: 2.5145819023737173

Epoch: 274| Step: 0
Training loss: 2.2380901474783688
Validation loss: 2.5540976995509084

Epoch: 5| Step: 1
Training loss: 2.3609061420208737
Validation loss: 2.5716024792865197

Epoch: 5| Step: 2
Training loss: 2.4705908949621294
Validation loss: 2.6005118829007907

Epoch: 5| Step: 3
Training loss: 2.7152461131125754
Validation loss: 2.620260139250024

Epoch: 5| Step: 4
Training loss: 2.1016345384014103
Validation loss: 2.5761521969868655

Epoch: 5| Step: 5
Training loss: 1.9311247504152236
Validation loss: 2.5726493717197805

Epoch: 5| Step: 6
Training loss: 2.2811622080174634
Validation loss: 2.5312464011523876

Epoch: 5| Step: 7
Training loss: 2.7190841820197313
Validation loss: 2.4967982054778193

Epoch: 5| Step: 8
Training loss: 1.9335021334434874
Validation loss: 2.5111107018551544

Epoch: 5| Step: 9
Training loss: 2.1194807069786314
Validation loss: 2.513857664935366

Epoch: 5| Step: 10
Training loss: 2.5399759850155212
Validation loss: 2.5041127032994246

Epoch: 5| Step: 11
Training loss: 2.212185133603907
Validation loss: 2.5051842362533034

Epoch: 275| Step: 0
Training loss: 2.5520414595509786
Validation loss: 2.5032891332793774

Epoch: 5| Step: 1
Training loss: 2.2256459273672324
Validation loss: 2.508241527615685

Epoch: 5| Step: 2
Training loss: 2.3267443358492867
Validation loss: 2.5011622230764163

Epoch: 5| Step: 3
Training loss: 1.987872068498819
Validation loss: 2.5020535578188166

Epoch: 5| Step: 4
Training loss: 2.195101690521917
Validation loss: 2.5167572324659053

Epoch: 5| Step: 5
Training loss: 2.3102977679667966
Validation loss: 2.5043173941909744

Epoch: 5| Step: 6
Training loss: 2.109197199356937
Validation loss: 2.5094507319201043

Epoch: 5| Step: 7
Training loss: 2.559015463516602
Validation loss: 2.5524100282686883

Epoch: 5| Step: 8
Training loss: 2.2419840756955898
Validation loss: 2.530374088933117

Epoch: 5| Step: 9
Training loss: 2.1493674745064673
Validation loss: 2.5422088944922683

Epoch: 5| Step: 10
Training loss: 2.6695301854123388
Validation loss: 2.5522937856562695

Epoch: 5| Step: 11
Training loss: 1.7255156091975026
Validation loss: 2.564763139752984

Epoch: 276| Step: 0
Training loss: 2.4665527222924193
Validation loss: 2.5501509740180555

Epoch: 5| Step: 1
Training loss: 2.0431127552168493
Validation loss: 2.556369542728669

Epoch: 5| Step: 2
Training loss: 2.589369602441744
Validation loss: 2.5431847898064737

Epoch: 5| Step: 3
Training loss: 2.212867137383626
Validation loss: 2.5439945150292247

Epoch: 5| Step: 4
Training loss: 2.4418551345146016
Validation loss: 2.5408671142903154

Epoch: 5| Step: 5
Training loss: 1.9076584787392468
Validation loss: 2.551676894281007

Epoch: 5| Step: 6
Training loss: 1.8741021867817784
Validation loss: 2.5398204043991526

Epoch: 5| Step: 7
Training loss: 2.071582447876113
Validation loss: 2.5447827191585266

Epoch: 5| Step: 8
Training loss: 2.2111973694036267
Validation loss: 2.5401991779411963

Epoch: 5| Step: 9
Training loss: 3.0120336464450217
Validation loss: 2.5552737334308406

Epoch: 5| Step: 10
Training loss: 2.1177861679892085
Validation loss: 2.546606494347061

Epoch: 5| Step: 11
Training loss: 2.3965902266252086
Validation loss: 2.5495354071788947

Epoch: 277| Step: 0
Training loss: 2.3972746673756564
Validation loss: 2.559137832993013

Epoch: 5| Step: 1
Training loss: 2.250530392332875
Validation loss: 2.578712928087604

Epoch: 5| Step: 2
Training loss: 1.8870609202211721
Validation loss: 2.563034443831098

Epoch: 5| Step: 3
Training loss: 2.4173655211986116
Validation loss: 2.5668484961416445

Epoch: 5| Step: 4
Training loss: 2.3704486201812496
Validation loss: 2.547129425259148

Epoch: 5| Step: 5
Training loss: 2.394448607171441
Validation loss: 2.5536203517857508

Epoch: 5| Step: 6
Training loss: 2.136557396858507
Validation loss: 2.5737682347719324

Epoch: 5| Step: 7
Training loss: 2.209128026903762
Validation loss: 2.564982847388151

Epoch: 5| Step: 8
Training loss: 1.7668193727149306
Validation loss: 2.592024593128184

Epoch: 5| Step: 9
Training loss: 2.3431111800304403
Validation loss: 2.547743694010013

Epoch: 5| Step: 10
Training loss: 2.6095495108537734
Validation loss: 2.562408011421338

Epoch: 5| Step: 11
Training loss: 1.6978097269592725
Validation loss: 2.5352124999966597

Epoch: 278| Step: 0
Training loss: 2.4123658572837634
Validation loss: 2.5202413066056537

Epoch: 5| Step: 1
Training loss: 1.880912296742977
Validation loss: 2.5238046447924387

Epoch: 5| Step: 2
Training loss: 2.1829627802459126
Validation loss: 2.504618975232849

Epoch: 5| Step: 3
Training loss: 2.277193931506641
Validation loss: 2.5246785933453184

Epoch: 5| Step: 4
Training loss: 2.5471648068915362
Validation loss: 2.5302333674024102

Epoch: 5| Step: 5
Training loss: 2.7462833604969785
Validation loss: 2.557921141140878

Epoch: 5| Step: 6
Training loss: 2.250533252679737
Validation loss: 2.5551607935936267

Epoch: 5| Step: 7
Training loss: 1.938197840872115
Validation loss: 2.576072018037448

Epoch: 5| Step: 8
Training loss: 2.054808870509306
Validation loss: 2.587493079018052

Epoch: 5| Step: 9
Training loss: 2.2644659826714166
Validation loss: 2.5653767720062866

Epoch: 5| Step: 10
Training loss: 2.2512968352686684
Validation loss: 2.5674285246159307

Epoch: 5| Step: 11
Training loss: 3.2076137445846316
Validation loss: 2.5684668998305797

Epoch: 279| Step: 0
Training loss: 2.0239266647502596
Validation loss: 2.589954506187324

Epoch: 5| Step: 1
Training loss: 2.2300784542840555
Validation loss: 2.593327108631492

Epoch: 5| Step: 2
Training loss: 1.9750869731826348
Validation loss: 2.5697861029032913

Epoch: 5| Step: 3
Training loss: 1.8970351200301505
Validation loss: 2.580503857449149

Epoch: 5| Step: 4
Training loss: 2.0344719788952994
Validation loss: 2.5808715639884583

Epoch: 5| Step: 5
Training loss: 2.5320325042241283
Validation loss: 2.5569155714965155

Epoch: 5| Step: 6
Training loss: 2.4895723308070052
Validation loss: 2.5580490153646958

Epoch: 5| Step: 7
Training loss: 2.6897309380728167
Validation loss: 2.54417770017457

Epoch: 5| Step: 8
Training loss: 2.2925491598967325
Validation loss: 2.54502922294267

Epoch: 5| Step: 9
Training loss: 2.3049949311637175
Validation loss: 2.531066358050582

Epoch: 5| Step: 10
Training loss: 2.334876900967089
Validation loss: 2.54478993321434

Epoch: 5| Step: 11
Training loss: 1.6132870874922762
Validation loss: 2.5459714000388587

Epoch: 280| Step: 0
Training loss: 2.503584675960813
Validation loss: 2.509986097891304

Epoch: 5| Step: 1
Training loss: 2.3891126049867637
Validation loss: 2.5186167237617885

Epoch: 5| Step: 2
Training loss: 1.8378823226433094
Validation loss: 2.5028598322691114

Epoch: 5| Step: 3
Training loss: 2.3653101763245785
Validation loss: 2.540162267927123

Epoch: 5| Step: 4
Training loss: 2.5383029212390515
Validation loss: 2.548102486749318

Epoch: 5| Step: 5
Training loss: 2.366818843121971
Validation loss: 2.5385117505710553

Epoch: 5| Step: 6
Training loss: 2.28636473954376
Validation loss: 2.5471107670222732

Epoch: 5| Step: 7
Training loss: 2.15571454904864
Validation loss: 2.5557564803854187

Epoch: 5| Step: 8
Training loss: 1.7730157829696682
Validation loss: 2.574788107301748

Epoch: 5| Step: 9
Training loss: 2.1306563996619468
Validation loss: 2.5459947683675654

Epoch: 5| Step: 10
Training loss: 2.2368134750683426
Validation loss: 2.5424297716724773

Epoch: 5| Step: 11
Training loss: 1.918954258089836
Validation loss: 2.5424100395886566

Epoch: 281| Step: 0
Training loss: 2.81620061919988
Validation loss: 2.5438770092210565

Epoch: 5| Step: 1
Training loss: 2.3993313891180454
Validation loss: 2.5077043076228533

Epoch: 5| Step: 2
Training loss: 2.1873017357486892
Validation loss: 2.5108477405709126

Epoch: 5| Step: 3
Training loss: 2.2038817492573126
Validation loss: 2.5098292559165807

Epoch: 5| Step: 4
Training loss: 1.9732925446371556
Validation loss: 2.502553549634076

Epoch: 5| Step: 5
Training loss: 2.3532089603435686
Validation loss: 2.5270734403646684

Epoch: 5| Step: 6
Training loss: 2.527002326272876
Validation loss: 2.515618930438126

Epoch: 5| Step: 7
Training loss: 2.3591916853066475
Validation loss: 2.5370076731520803

Epoch: 5| Step: 8
Training loss: 2.2943982115308956
Validation loss: 2.563311578194683

Epoch: 5| Step: 9
Training loss: 2.324401848256655
Validation loss: 2.5769457730765732

Epoch: 5| Step: 10
Training loss: 2.4266176118014617
Validation loss: 2.5432972732435735

Epoch: 5| Step: 11
Training loss: 2.195118091161172
Validation loss: 2.5192935801934113

Epoch: 282| Step: 0
Training loss: 2.092990680620977
Validation loss: 2.51590459071539

Epoch: 5| Step: 1
Training loss: 2.5785518408492907
Validation loss: 2.5344581642099606

Epoch: 5| Step: 2
Training loss: 2.235716303794434
Validation loss: 2.5331168962762045

Epoch: 5| Step: 3
Training loss: 2.6180139859308973
Validation loss: 2.5310475696261796

Epoch: 5| Step: 4
Training loss: 2.725736738931054
Validation loss: 2.5392573355234247

Epoch: 5| Step: 5
Training loss: 2.2730757697550708
Validation loss: 2.5314929555141568

Epoch: 5| Step: 6
Training loss: 2.7412341598978496
Validation loss: 2.5161381429150556

Epoch: 5| Step: 7
Training loss: 2.735961192413522
Validation loss: 2.5087580378856167

Epoch: 5| Step: 8
Training loss: 1.9168302493846592
Validation loss: 2.519624386034459

Epoch: 5| Step: 9
Training loss: 2.0837328972712434
Validation loss: 2.5240788191059638

Epoch: 5| Step: 10
Training loss: 1.8387414177150367
Validation loss: 2.547831849011828

Epoch: 5| Step: 11
Training loss: 1.9079629049913733
Validation loss: 2.562608596392492

Epoch: 283| Step: 0
Training loss: 2.3975063840206303
Validation loss: 2.5854596544534827

Epoch: 5| Step: 1
Training loss: 1.502140266655144
Validation loss: 2.596151599742682

Epoch: 5| Step: 2
Training loss: 2.718840235824053
Validation loss: 2.5905291080558723

Epoch: 5| Step: 3
Training loss: 2.0396167688743128
Validation loss: 2.5816807511398796

Epoch: 5| Step: 4
Training loss: 2.378465934305782
Validation loss: 2.606484801725892

Epoch: 5| Step: 5
Training loss: 2.01071835430114
Validation loss: 2.596383259469009

Epoch: 5| Step: 6
Training loss: 2.1982451158840446
Validation loss: 2.569262022267149

Epoch: 5| Step: 7
Training loss: 2.8362258470296204
Validation loss: 2.5784958890527454

Epoch: 5| Step: 8
Training loss: 2.391296192765937
Validation loss: 2.557403621566685

Epoch: 5| Step: 9
Training loss: 2.056328643871807
Validation loss: 2.550632069359881

Epoch: 5| Step: 10
Training loss: 2.648888833627693
Validation loss: 2.524466362908846

Epoch: 5| Step: 11
Training loss: 0.9038532396497666
Validation loss: 2.5168414482050885

Epoch: 284| Step: 0
Training loss: 3.0886754504986733
Validation loss: 2.5206949981585245

Epoch: 5| Step: 1
Training loss: 2.3390244719752866
Validation loss: 2.5227309537954965

Epoch: 5| Step: 2
Training loss: 1.9654344766583682
Validation loss: 2.518174808287642

Epoch: 5| Step: 3
Training loss: 2.7525087530705474
Validation loss: 2.526492821167662

Epoch: 5| Step: 4
Training loss: 2.1223442086960227
Validation loss: 2.5142085034365884

Epoch: 5| Step: 5
Training loss: 1.852734802233176
Validation loss: 2.511752332558784

Epoch: 5| Step: 6
Training loss: 1.6348872532598637
Validation loss: 2.527601127745833

Epoch: 5| Step: 7
Training loss: 2.4328587678933156
Validation loss: 2.533777137100391

Epoch: 5| Step: 8
Training loss: 1.9128508800687738
Validation loss: 2.5385626902523017

Epoch: 5| Step: 9
Training loss: 2.1933569835945335
Validation loss: 2.5530549684504793

Epoch: 5| Step: 10
Training loss: 2.5616148373538072
Validation loss: 2.5549168964950226

Epoch: 5| Step: 11
Training loss: 2.383959684786327
Validation loss: 2.572937626824094

Epoch: 285| Step: 0
Training loss: 1.9562440914759514
Validation loss: 2.5373265636222317

Epoch: 5| Step: 1
Training loss: 2.2444327673067974
Validation loss: 2.5382702183114367

Epoch: 5| Step: 2
Training loss: 2.0369435258395647
Validation loss: 2.533884882699928

Epoch: 5| Step: 3
Training loss: 2.69714443589192
Validation loss: 2.5143693550571813

Epoch: 5| Step: 4
Training loss: 2.3644243990774236
Validation loss: 2.51008052606834

Epoch: 5| Step: 5
Training loss: 1.550501262458846
Validation loss: 2.5062306050591023

Epoch: 5| Step: 6
Training loss: 2.220135108685476
Validation loss: 2.5102834125360953

Epoch: 5| Step: 7
Training loss: 3.165484224373241
Validation loss: 2.508466146180482

Epoch: 5| Step: 8
Training loss: 1.7124253166230627
Validation loss: 2.514301370416134

Epoch: 5| Step: 9
Training loss: 2.6737016824901545
Validation loss: 2.509393548164527

Epoch: 5| Step: 10
Training loss: 2.1166591759296343
Validation loss: 2.5174974675719812

Epoch: 5| Step: 11
Training loss: 1.8084716755676784
Validation loss: 2.5457616412489035

Epoch: 286| Step: 0
Training loss: 2.598401598014246
Validation loss: 2.522208699015309

Epoch: 5| Step: 1
Training loss: 2.56844746833832
Validation loss: 2.5358500616199

Epoch: 5| Step: 2
Training loss: 2.0517324802844783
Validation loss: 2.518888638974369

Epoch: 5| Step: 3
Training loss: 1.9655194495264285
Validation loss: 2.5226428943106036

Epoch: 5| Step: 4
Training loss: 2.435340217932866
Validation loss: 2.538095284151581

Epoch: 5| Step: 5
Training loss: 2.310897168574224
Validation loss: 2.5516189827461613

Epoch: 5| Step: 6
Training loss: 2.4684063636061047
Validation loss: 2.5506987935671996

Epoch: 5| Step: 7
Training loss: 1.94094462442388
Validation loss: 2.5606233773052947

Epoch: 5| Step: 8
Training loss: 2.5704866947407754
Validation loss: 2.5604459780036217

Epoch: 5| Step: 9
Training loss: 2.006652259310248
Validation loss: 2.5639704307873528

Epoch: 5| Step: 10
Training loss: 1.9235583120996378
Validation loss: 2.5487318563977013

Epoch: 5| Step: 11
Training loss: 1.9583178648101027
Validation loss: 2.569083208746861

Epoch: 287| Step: 0
Training loss: 2.539293296301377
Validation loss: 2.5967033034148956

Epoch: 5| Step: 1
Training loss: 1.9793434331826183
Validation loss: 2.590859177162873

Epoch: 5| Step: 2
Training loss: 2.184928799828737
Validation loss: 2.604975987877131

Epoch: 5| Step: 3
Training loss: 1.763438939187414
Validation loss: 2.595770405169369

Epoch: 5| Step: 4
Training loss: 2.3178664135689795
Validation loss: 2.5751386782576695

Epoch: 5| Step: 5
Training loss: 2.392342044315723
Validation loss: 2.5683129406733545

Epoch: 5| Step: 6
Training loss: 2.1593107531384605
Validation loss: 2.5595613120691083

Epoch: 5| Step: 7
Training loss: 2.097342298942427
Validation loss: 2.5449036416771462

Epoch: 5| Step: 8
Training loss: 1.9216140942138458
Validation loss: 2.566461203503056

Epoch: 5| Step: 9
Training loss: 2.6847389587294654
Validation loss: 2.5545285337804167

Epoch: 5| Step: 10
Training loss: 2.064622422767948
Validation loss: 2.5594381788045664

Epoch: 5| Step: 11
Training loss: 4.380408731140904
Validation loss: 2.551439567387473

Epoch: 288| Step: 0
Training loss: 2.4582236206228343
Validation loss: 2.587592568177589

Epoch: 5| Step: 1
Training loss: 2.5197289204495887
Validation loss: 2.590573602618256

Epoch: 5| Step: 2
Training loss: 2.0449964915947327
Validation loss: 2.6271097583233325

Epoch: 5| Step: 3
Training loss: 2.001297291584686
Validation loss: 2.6671943974345225

Epoch: 5| Step: 4
Training loss: 2.8441336236780326
Validation loss: 2.6678335547460716

Epoch: 5| Step: 5
Training loss: 2.6982243385787674
Validation loss: 2.636442039004318

Epoch: 5| Step: 6
Training loss: 1.7510519952902486
Validation loss: 2.6399945704748693

Epoch: 5| Step: 7
Training loss: 1.5087146333280244
Validation loss: 2.6303777369468486

Epoch: 5| Step: 8
Training loss: 2.2485635198570733
Validation loss: 2.572326385236146

Epoch: 5| Step: 9
Training loss: 2.6611449971051324
Validation loss: 2.539591467116497

Epoch: 5| Step: 10
Training loss: 1.9014386602995732
Validation loss: 2.5308091364476204

Epoch: 5| Step: 11
Training loss: 2.052006237563381
Validation loss: 2.521470524646198

Epoch: 289| Step: 0
Training loss: 2.100313403948541
Validation loss: 2.510048848559576

Epoch: 5| Step: 1
Training loss: 2.089399458102739
Validation loss: 2.51864151930077

Epoch: 5| Step: 2
Training loss: 2.553080295278749
Validation loss: 2.507566513245556

Epoch: 5| Step: 3
Training loss: 1.8479162440537833
Validation loss: 2.515808931975558

Epoch: 5| Step: 4
Training loss: 2.7593863429768453
Validation loss: 2.515892188373678

Epoch: 5| Step: 5
Training loss: 2.189088299290891
Validation loss: 2.5213157859444135

Epoch: 5| Step: 6
Training loss: 2.756939974209236
Validation loss: 2.523560598734843

Epoch: 5| Step: 7
Training loss: 1.7468789743777464
Validation loss: 2.540803889240262

Epoch: 5| Step: 8
Training loss: 2.0389496636868167
Validation loss: 2.533852826490894

Epoch: 5| Step: 9
Training loss: 2.405109482016026
Validation loss: 2.5620406716079227

Epoch: 5| Step: 10
Training loss: 2.068600735076653
Validation loss: 2.589320203241372

Epoch: 5| Step: 11
Training loss: 3.6546723964423107
Validation loss: 2.6105715031888894

Epoch: 290| Step: 0
Training loss: 2.803574960374907
Validation loss: 2.6291056383085896

Epoch: 5| Step: 1
Training loss: 2.0246637229977584
Validation loss: 2.71414623859703

Epoch: 5| Step: 2
Training loss: 2.301716586857691
Validation loss: 2.770444571439194

Epoch: 5| Step: 3
Training loss: 2.647229214401413
Validation loss: 2.7509511075948456

Epoch: 5| Step: 4
Training loss: 2.525317740513411
Validation loss: 2.6655970131827393

Epoch: 5| Step: 5
Training loss: 2.6370456973222183
Validation loss: 2.582710557646236

Epoch: 5| Step: 6
Training loss: 1.8476590243232012
Validation loss: 2.519762104508097

Epoch: 5| Step: 7
Training loss: 1.7684547905852708
Validation loss: 2.4790661608468074

Epoch: 5| Step: 8
Training loss: 2.325333015629254
Validation loss: 2.4875626736641645

Epoch: 5| Step: 9
Training loss: 2.5725444158903295
Validation loss: 2.4909359730231007

Epoch: 5| Step: 10
Training loss: 2.7154457796963474
Validation loss: 2.4977810330244097

Epoch: 5| Step: 11
Training loss: 1.0548405854069338
Validation loss: 2.486541842744415

Epoch: 291| Step: 0
Training loss: 2.4835728723945056
Validation loss: 2.4886023146109375

Epoch: 5| Step: 1
Training loss: 2.214547919644847
Validation loss: 2.4905865069980178

Epoch: 5| Step: 2
Training loss: 2.4994694146736256
Validation loss: 2.4905781188279557

Epoch: 5| Step: 3
Training loss: 2.400622835404131
Validation loss: 2.4972319995070857

Epoch: 5| Step: 4
Training loss: 2.7664257549814537
Validation loss: 2.493378907512193

Epoch: 5| Step: 5
Training loss: 1.9094329900098648
Validation loss: 2.490210391658406

Epoch: 5| Step: 6
Training loss: 2.600727577377565
Validation loss: 2.4814090378316886

Epoch: 5| Step: 7
Training loss: 2.450457927181563
Validation loss: 2.490040114062416

Epoch: 5| Step: 8
Training loss: 2.156720759653328
Validation loss: 2.4839751563611263

Epoch: 5| Step: 9
Training loss: 2.117983735192285
Validation loss: 2.4912510490702378

Epoch: 5| Step: 10
Training loss: 1.7072587549071978
Validation loss: 2.5123647093525836

Epoch: 5| Step: 11
Training loss: 2.982446656689363
Validation loss: 2.5320516698277347

Epoch: 292| Step: 0
Training loss: 2.3012754884185838
Validation loss: 2.56602325949344

Epoch: 5| Step: 1
Training loss: 2.1902870951553592
Validation loss: 2.5875519306543215

Epoch: 5| Step: 2
Training loss: 2.304213446015226
Validation loss: 2.6023846760011735

Epoch: 5| Step: 3
Training loss: 2.4702676864313617
Validation loss: 2.591366078331158

Epoch: 5| Step: 4
Training loss: 2.402675004156439
Validation loss: 2.5667288822729497

Epoch: 5| Step: 5
Training loss: 2.5910898378286644
Validation loss: 2.5306984296555797

Epoch: 5| Step: 6
Training loss: 1.7567345598963795
Validation loss: 2.5230760463097797

Epoch: 5| Step: 7
Training loss: 1.9146071301442165
Validation loss: 2.528141432379848

Epoch: 5| Step: 8
Training loss: 2.5204935291451647
Validation loss: 2.5186975110014242

Epoch: 5| Step: 9
Training loss: 2.729436283528576
Validation loss: 2.517466887667001

Epoch: 5| Step: 10
Training loss: 2.0666360709273905
Validation loss: 2.5156099228180064

Epoch: 5| Step: 11
Training loss: 2.088509220876469
Validation loss: 2.5304371250708932

Epoch: 293| Step: 0
Training loss: 2.2473332177932304
Validation loss: 2.5413892990689964

Epoch: 5| Step: 1
Training loss: 2.287549036052821
Validation loss: 2.5353648800812745

Epoch: 5| Step: 2
Training loss: 2.347621212918405
Validation loss: 2.5302633984414555

Epoch: 5| Step: 3
Training loss: 2.5637387328734946
Validation loss: 2.535828394020561

Epoch: 5| Step: 4
Training loss: 2.279850360577636
Validation loss: 2.5609744656004954

Epoch: 5| Step: 5
Training loss: 1.952036012329067
Validation loss: 2.5511907742404367

Epoch: 5| Step: 6
Training loss: 1.9890468000465118
Validation loss: 2.565800948155958

Epoch: 5| Step: 7
Training loss: 2.1356218441031976
Validation loss: 2.595147376919523

Epoch: 5| Step: 8
Training loss: 2.5332274071817125
Validation loss: 2.6178922169395817

Epoch: 5| Step: 9
Training loss: 2.0531453774658828
Validation loss: 2.5912713996456254

Epoch: 5| Step: 10
Training loss: 2.3633886896096987
Validation loss: 2.5520886531437035

Epoch: 5| Step: 11
Training loss: 2.1780813992705013
Validation loss: 2.5610944956210395

Epoch: 294| Step: 0
Training loss: 1.9977235952042247
Validation loss: 2.56430097420581

Epoch: 5| Step: 1
Training loss: 2.6386004931707707
Validation loss: 2.5207824084494965

Epoch: 5| Step: 2
Training loss: 2.0172458016785284
Validation loss: 2.5438506945332295

Epoch: 5| Step: 3
Training loss: 1.693632919938314
Validation loss: 2.5453825831497716

Epoch: 5| Step: 4
Training loss: 1.9412426639769527
Validation loss: 2.5411814997857616

Epoch: 5| Step: 5
Training loss: 2.333032338846593
Validation loss: 2.5437961371109483

Epoch: 5| Step: 6
Training loss: 2.36119504542541
Validation loss: 2.545384667241667

Epoch: 5| Step: 7
Training loss: 2.163365000339708
Validation loss: 2.5281003461216436

Epoch: 5| Step: 8
Training loss: 2.3936621627428805
Validation loss: 2.5347124601782904

Epoch: 5| Step: 9
Training loss: 2.2675952631444845
Validation loss: 2.548236030802547

Epoch: 5| Step: 10
Training loss: 2.779642990982496
Validation loss: 2.5618479449582385

Epoch: 5| Step: 11
Training loss: 1.8593842321855885
Validation loss: 2.5823797201450187

Epoch: 295| Step: 0
Training loss: 2.5912925384754413
Validation loss: 2.606614834477657

Epoch: 5| Step: 1
Training loss: 2.280781527976087
Validation loss: 2.6173169687615876

Epoch: 5| Step: 2
Training loss: 2.047283917996586
Validation loss: 2.588655163574924

Epoch: 5| Step: 3
Training loss: 2.2795547038796276
Validation loss: 2.569919359003271

Epoch: 5| Step: 4
Training loss: 2.020064322325655
Validation loss: 2.550338414587213

Epoch: 5| Step: 5
Training loss: 2.9612788906918293
Validation loss: 2.553947609898805

Epoch: 5| Step: 6
Training loss: 2.1059723713539666
Validation loss: 2.554594318195378

Epoch: 5| Step: 7
Training loss: 2.2658035734110915
Validation loss: 2.5489874973870967

Epoch: 5| Step: 8
Training loss: 2.4883806098012053
Validation loss: 2.53379752058882

Epoch: 5| Step: 9
Training loss: 2.1441363195097054
Validation loss: 2.522816092368374

Epoch: 5| Step: 10
Training loss: 1.799318592582627
Validation loss: 2.5401880635390586

Epoch: 5| Step: 11
Training loss: 0.599764477677892
Validation loss: 2.5361503528981864

Epoch: 296| Step: 0
Training loss: 2.068895999792881
Validation loss: 2.5437543840081553

Epoch: 5| Step: 1
Training loss: 2.444180162429184
Validation loss: 2.5534592221785357

Epoch: 5| Step: 2
Training loss: 2.193641977286111
Validation loss: 2.560379496170867

Epoch: 5| Step: 3
Training loss: 1.5797523945535055
Validation loss: 2.579257361913243

Epoch: 5| Step: 4
Training loss: 2.246535919452466
Validation loss: 2.5740635216798027

Epoch: 5| Step: 5
Training loss: 2.166955879667103
Validation loss: 2.590393165396688

Epoch: 5| Step: 6
Training loss: 1.9575796976831805
Validation loss: 2.577068605640446

Epoch: 5| Step: 7
Training loss: 1.904307892596333
Validation loss: 2.585522408030336

Epoch: 5| Step: 8
Training loss: 2.4883808014266897
Validation loss: 2.596085058414505

Epoch: 5| Step: 9
Training loss: 2.3720055826903033
Validation loss: 2.574174177278837

Epoch: 5| Step: 10
Training loss: 3.035137711594768
Validation loss: 2.5959814193718955

Epoch: 5| Step: 11
Training loss: 1.3209521307404608
Validation loss: 2.5606036108400945

Epoch: 297| Step: 0
Training loss: 2.1033798358127362
Validation loss: 2.5515195742887946

Epoch: 5| Step: 1
Training loss: 2.3168953030326693
Validation loss: 2.558074304398503

Epoch: 5| Step: 2
Training loss: 2.18020842422021
Validation loss: 2.541650609189783

Epoch: 5| Step: 3
Training loss: 2.764642228335486
Validation loss: 2.553432093921031

Epoch: 5| Step: 4
Training loss: 1.8460519935253916
Validation loss: 2.563685425984059

Epoch: 5| Step: 5
Training loss: 2.049191743568766
Validation loss: 2.575359522001369

Epoch: 5| Step: 6
Training loss: 1.9689427841498746
Validation loss: 2.5749052364386986

Epoch: 5| Step: 7
Training loss: 2.260627443767363
Validation loss: 2.5787938204124132

Epoch: 5| Step: 8
Training loss: 1.8963693412991534
Validation loss: 2.5414609526156795

Epoch: 5| Step: 9
Training loss: 2.6712701123934846
Validation loss: 2.5429078391222615

Epoch: 5| Step: 10
Training loss: 2.4239110448334076
Validation loss: 2.542785657472463

Epoch: 5| Step: 11
Training loss: 1.9170007690921647
Validation loss: 2.5491003210265806

Epoch: 298| Step: 0
Training loss: 2.3695421248179196
Validation loss: 2.5299359902287133

Epoch: 5| Step: 1
Training loss: 2.561113680353023
Validation loss: 2.5236651984155802

Epoch: 5| Step: 2
Training loss: 2.1853713441506266
Validation loss: 2.526095585722799

Epoch: 5| Step: 3
Training loss: 1.6044470884610127
Validation loss: 2.5219771703659384

Epoch: 5| Step: 4
Training loss: 2.7481380574946623
Validation loss: 2.511370890384723

Epoch: 5| Step: 5
Training loss: 1.4679436600811229
Validation loss: 2.5327419546517467

Epoch: 5| Step: 6
Training loss: 2.0164366278268573
Validation loss: 2.533163167912382

Epoch: 5| Step: 7
Training loss: 2.1471785862370805
Validation loss: 2.5626092011358708

Epoch: 5| Step: 8
Training loss: 2.255666696485983
Validation loss: 2.5539964485907998

Epoch: 5| Step: 9
Training loss: 2.3392096728629785
Validation loss: 2.6135331276249474

Epoch: 5| Step: 10
Training loss: 2.3368084664883244
Validation loss: 2.6040033149348996

Epoch: 5| Step: 11
Training loss: 2.4838959813906074
Validation loss: 2.5995216727419987

Epoch: 299| Step: 0
Training loss: 2.3805090605049157
Validation loss: 2.6333030334270626

Epoch: 5| Step: 1
Training loss: 2.35648133131403
Validation loss: 2.685182622077664

Epoch: 5| Step: 2
Training loss: 2.1824259083200044
Validation loss: 2.6885958773381997

Epoch: 5| Step: 3
Training loss: 2.8082926327466153
Validation loss: 2.632233919285914

Epoch: 5| Step: 4
Training loss: 1.8322635983183573
Validation loss: 2.6022347373121453

Epoch: 5| Step: 5
Training loss: 1.6163017316595014
Validation loss: 2.5575577752183656

Epoch: 5| Step: 6
Training loss: 1.9680698976044244
Validation loss: 2.5536373441460274

Epoch: 5| Step: 7
Training loss: 2.2499256121736426
Validation loss: 2.548860552156432

Epoch: 5| Step: 8
Training loss: 2.972092684084823
Validation loss: 2.5438794499106665

Epoch: 5| Step: 9
Training loss: 2.421655657280061
Validation loss: 2.5239084077988534

Epoch: 5| Step: 10
Training loss: 1.8579343455841546
Validation loss: 2.534139059790941

Epoch: 5| Step: 11
Training loss: 3.29724798759314
Validation loss: 2.5318449953715616

Epoch: 300| Step: 0
Training loss: 2.216858500829815
Validation loss: 2.5366192021881484

Epoch: 5| Step: 1
Training loss: 2.5416365970854855
Validation loss: 2.546397131182021

Epoch: 5| Step: 2
Training loss: 1.6333236259379582
Validation loss: 2.538680993786856

Epoch: 5| Step: 3
Training loss: 2.399891266743892
Validation loss: 2.5531264521837094

Epoch: 5| Step: 4
Training loss: 1.2989060916301696
Validation loss: 2.5611687784455786

Epoch: 5| Step: 5
Training loss: 2.314591158199274
Validation loss: 2.561067134050239

Epoch: 5| Step: 6
Training loss: 2.663098113168903
Validation loss: 2.5424264152753655

Epoch: 5| Step: 7
Training loss: 2.3539642539539174
Validation loss: 2.5607943518311957

Epoch: 5| Step: 8
Training loss: 2.051741544140645
Validation loss: 2.5465434117931456

Epoch: 5| Step: 9
Training loss: 2.0854812866117647
Validation loss: 2.5516422409551303

Epoch: 5| Step: 10
Training loss: 2.3802346214885035
Validation loss: 2.5555677929069245

Epoch: 5| Step: 11
Training loss: 2.8711867544460166
Validation loss: 2.5486562152050887

Epoch: 301| Step: 0
Training loss: 2.4025781532619783
Validation loss: 2.568213458926371

Epoch: 5| Step: 1
Training loss: 1.824800190373452
Validation loss: 2.6151576848806872

Epoch: 5| Step: 2
Training loss: 1.9443729690251768
Validation loss: 2.59379781349571

Epoch: 5| Step: 3
Training loss: 2.7194098187135163
Validation loss: 2.6019545136616817

Epoch: 5| Step: 4
Training loss: 2.2283266808185362
Validation loss: 2.5730532398862302

Epoch: 5| Step: 5
Training loss: 1.9553918729660231
Validation loss: 2.562006809876241

Epoch: 5| Step: 6
Training loss: 2.708980590553638
Validation loss: 2.541009909300615

Epoch: 5| Step: 7
Training loss: 1.931664323034612
Validation loss: 2.513593196749085

Epoch: 5| Step: 8
Training loss: 2.3038111168075974
Validation loss: 2.4889983816084116

Epoch: 5| Step: 9
Training loss: 2.1263388735470814
Validation loss: 2.4887913850210626

Epoch: 5| Step: 10
Training loss: 2.5534385871441376
Validation loss: 2.502007167449888

Epoch: 5| Step: 11
Training loss: 1.953080932120041
Validation loss: 2.492681089688887

Epoch: 302| Step: 0
Training loss: 2.02734445560862
Validation loss: 2.4842077114954795

Epoch: 5| Step: 1
Training loss: 1.8297114213508514
Validation loss: 2.505200892944284

Epoch: 5| Step: 2
Training loss: 1.8906468161593188
Validation loss: 2.4943400487232905

Epoch: 5| Step: 3
Training loss: 2.024861427557888
Validation loss: 2.5198350156472347

Epoch: 5| Step: 4
Training loss: 2.440796408208444
Validation loss: 2.5191306808590097

Epoch: 5| Step: 5
Training loss: 2.40674585955327
Validation loss: 2.5801960573814293

Epoch: 5| Step: 6
Training loss: 2.2064982893206935
Validation loss: 2.6238195058258817

Epoch: 5| Step: 7
Training loss: 2.7840009650031425
Validation loss: 2.700627990970546

Epoch: 5| Step: 8
Training loss: 2.523370039188722
Validation loss: 2.6626674060697137

Epoch: 5| Step: 9
Training loss: 2.5920098223735586
Validation loss: 2.632612524540948

Epoch: 5| Step: 10
Training loss: 2.4451590297911583
Validation loss: 2.585703079493029

Epoch: 5| Step: 11
Training loss: 2.615016207380379
Validation loss: 2.5467794162467743

Epoch: 303| Step: 0
Training loss: 2.102882282824281
Validation loss: 2.5136675277628897

Epoch: 5| Step: 1
Training loss: 2.5763937946374553
Validation loss: 2.481638683044324

Epoch: 5| Step: 2
Training loss: 2.6223392625684063
Validation loss: 2.484905502171529

Epoch: 5| Step: 3
Training loss: 1.78879361859874
Validation loss: 2.5018879358086012

Epoch: 5| Step: 4
Training loss: 2.785244762410594
Validation loss: 2.4557710477481103

Epoch: 5| Step: 5
Training loss: 2.190019192372013
Validation loss: 2.4805726277384603

Epoch: 5| Step: 6
Training loss: 2.0553714208638443
Validation loss: 2.4678977187270474

Epoch: 5| Step: 7
Training loss: 2.2908115409886745
Validation loss: 2.4846819881889357

Epoch: 5| Step: 8
Training loss: 2.275833807095125
Validation loss: 2.472734824685326

Epoch: 5| Step: 9
Training loss: 2.4712730745464517
Validation loss: 2.491106023888418

Epoch: 5| Step: 10
Training loss: 1.9755007338429573
Validation loss: 2.5027527197744077

Epoch: 5| Step: 11
Training loss: 1.1581603069656892
Validation loss: 2.525637192476833

Epoch: 304| Step: 0
Training loss: 1.8165901337114283
Validation loss: 2.5447502400392237

Epoch: 5| Step: 1
Training loss: 2.673082759114045
Validation loss: 2.560057016358007

Epoch: 5| Step: 2
Training loss: 2.1780284186997134
Validation loss: 2.5761745820165007

Epoch: 5| Step: 3
Training loss: 1.7455258395299322
Validation loss: 2.57112150591668

Epoch: 5| Step: 4
Training loss: 2.501768059180554
Validation loss: 2.579789943805908

Epoch: 5| Step: 5
Training loss: 2.3514889930566447
Validation loss: 2.57113280728863

Epoch: 5| Step: 6
Training loss: 2.2700143322933313
Validation loss: 2.559750714203575

Epoch: 5| Step: 7
Training loss: 2.001304320361232
Validation loss: 2.5807510988373004

Epoch: 5| Step: 8
Training loss: 2.2318623960593644
Validation loss: 2.548965649164944

Epoch: 5| Step: 9
Training loss: 2.0455138361651017
Validation loss: 2.558365894684068

Epoch: 5| Step: 10
Training loss: 2.662279175389922
Validation loss: 2.5415257208990636

Epoch: 5| Step: 11
Training loss: 2.687637325593038
Validation loss: 2.5071388401439987

Epoch: 305| Step: 0
Training loss: 1.9537590523557058
Validation loss: 2.53869295609919

Epoch: 5| Step: 1
Training loss: 2.6149635089202716
Validation loss: 2.5268350253299428

Epoch: 5| Step: 2
Training loss: 1.7136589945441039
Validation loss: 2.546109168244091

Epoch: 5| Step: 3
Training loss: 2.334007767436383
Validation loss: 2.58299635924869

Epoch: 5| Step: 4
Training loss: 3.174840283318983
Validation loss: 2.595700756002072

Epoch: 5| Step: 5
Training loss: 1.9705618135658634
Validation loss: 2.6247058620356367

Epoch: 5| Step: 6
Training loss: 2.047171418370388
Validation loss: 2.658380809397858

Epoch: 5| Step: 7
Training loss: 1.6748802768565216
Validation loss: 2.6430202132193124

Epoch: 5| Step: 8
Training loss: 2.551465069883994
Validation loss: 2.6550442146671425

Epoch: 5| Step: 9
Training loss: 2.4661852878098625
Validation loss: 2.570983829967245

Epoch: 5| Step: 10
Training loss: 1.967118934848402
Validation loss: 2.5380606665337195

Epoch: 5| Step: 11
Training loss: 1.5402793140516464
Validation loss: 2.4930250101924893

Epoch: 306| Step: 0
Training loss: 1.7473660128721382
Validation loss: 2.4953547395122126

Epoch: 5| Step: 1
Training loss: 2.1752489627107443
Validation loss: 2.4987173565056504

Epoch: 5| Step: 2
Training loss: 3.1660648744530815
Validation loss: 2.507252271115827

Epoch: 5| Step: 3
Training loss: 1.906568656927702
Validation loss: 2.4907965650064305

Epoch: 5| Step: 4
Training loss: 2.4153865405350246
Validation loss: 2.4993542572360132

Epoch: 5| Step: 5
Training loss: 2.941925610896514
Validation loss: 2.4688587567159437

Epoch: 5| Step: 6
Training loss: 1.9970062618663647
Validation loss: 2.4381906843166994

Epoch: 5| Step: 7
Training loss: 1.8673280798690497
Validation loss: 2.469073085326886

Epoch: 5| Step: 8
Training loss: 2.4155374607347007
Validation loss: 2.473876680063988

Epoch: 5| Step: 9
Training loss: 2.5342002911915316
Validation loss: 2.5017413472328096

Epoch: 5| Step: 10
Training loss: 2.2480139974382327
Validation loss: 2.5130491436300875

Epoch: 5| Step: 11
Training loss: 2.543053879023449
Validation loss: 2.518309880212032

Epoch: 307| Step: 0
Training loss: 2.182099788366508
Validation loss: 2.514491981157528

Epoch: 5| Step: 1
Training loss: 1.9436916968135567
Validation loss: 2.52737541119939

Epoch: 5| Step: 2
Training loss: 2.1136878110827357
Validation loss: 2.503823888478467

Epoch: 5| Step: 3
Training loss: 1.7728304727343622
Validation loss: 2.4911737921505197

Epoch: 5| Step: 4
Training loss: 2.39619166693729
Validation loss: 2.501373327069048

Epoch: 5| Step: 5
Training loss: 2.9903791655425467
Validation loss: 2.517381370872148

Epoch: 5| Step: 6
Training loss: 1.6725282907872998
Validation loss: 2.529095549162259

Epoch: 5| Step: 7
Training loss: 2.5393392089304405
Validation loss: 2.5358185375716746

Epoch: 5| Step: 8
Training loss: 2.236023408211075
Validation loss: 2.5432183123207266

Epoch: 5| Step: 9
Training loss: 2.9350676510242257
Validation loss: 2.553749499446193

Epoch: 5| Step: 10
Training loss: 1.7801381706873995
Validation loss: 2.5399521819316857

Epoch: 5| Step: 11
Training loss: 1.2829233268043734
Validation loss: 2.56112343169628

Epoch: 308| Step: 0
Training loss: 1.9827512570806338
Validation loss: 2.58786388071504

Epoch: 5| Step: 1
Training loss: 2.090307451659243
Validation loss: 2.587451249736835

Epoch: 5| Step: 2
Training loss: 2.1292320072228565
Validation loss: 2.6165837099102647

Epoch: 5| Step: 3
Training loss: 2.224598550477122
Validation loss: 2.590245978495722

Epoch: 5| Step: 4
Training loss: 1.8349064955418277
Validation loss: 2.5751765104245625

Epoch: 5| Step: 5
Training loss: 2.6627458020145744
Validation loss: 2.571161584027864

Epoch: 5| Step: 6
Training loss: 2.1786328023444366
Validation loss: 2.5431484074959494

Epoch: 5| Step: 7
Training loss: 1.8445239624758425
Validation loss: 2.5614507201646433

Epoch: 5| Step: 8
Training loss: 1.7746743078021932
Validation loss: 2.563947288285026

Epoch: 5| Step: 9
Training loss: 2.6276899814612595
Validation loss: 2.551489147121585

Epoch: 5| Step: 10
Training loss: 3.008265077304457
Validation loss: 2.547680401758259

Epoch: 5| Step: 11
Training loss: 2.3197156350383565
Validation loss: 2.551849048266721

Epoch: 309| Step: 0
Training loss: 1.9865603090891955
Validation loss: 2.56547037728148

Epoch: 5| Step: 1
Training loss: 2.2140260293631395
Validation loss: 2.584117763898812

Epoch: 5| Step: 2
Training loss: 1.5692412864755514
Validation loss: 2.5734969551647007

Epoch: 5| Step: 3
Training loss: 2.38121914981272
Validation loss: 2.5860244029522006

Epoch: 5| Step: 4
Training loss: 2.198588685867651
Validation loss: 2.5859361400773544

Epoch: 5| Step: 5
Training loss: 2.255639109241036
Validation loss: 2.5681416659798084

Epoch: 5| Step: 6
Training loss: 2.2677116519014127
Validation loss: 2.576026505578501

Epoch: 5| Step: 7
Training loss: 2.0313798129342953
Validation loss: 2.552641730948966

Epoch: 5| Step: 8
Training loss: 2.46067920116428
Validation loss: 2.576846219818907

Epoch: 5| Step: 9
Training loss: 2.7354020288041374
Validation loss: 2.567762832114827

Epoch: 5| Step: 10
Training loss: 2.532551183757606
Validation loss: 2.5559755591488584

Epoch: 5| Step: 11
Training loss: 1.593912752106628
Validation loss: 2.550026112466391

Epoch: 310| Step: 0
Training loss: 2.520526541544615
Validation loss: 2.568714669410807

Epoch: 5| Step: 1
Training loss: 2.2033680450352193
Validation loss: 2.5848429688620422

Epoch: 5| Step: 2
Training loss: 2.32115830954714
Validation loss: 2.56399184894258

Epoch: 5| Step: 3
Training loss: 2.237314384902325
Validation loss: 2.5776993101459422

Epoch: 5| Step: 4
Training loss: 2.2514155491479286
Validation loss: 2.637871922880902

Epoch: 5| Step: 5
Training loss: 1.928350564631848
Validation loss: 2.6106938704841194

Epoch: 5| Step: 6
Training loss: 2.3904741027680343
Validation loss: 2.616958769487093

Epoch: 5| Step: 7
Training loss: 2.048337690422736
Validation loss: 2.6152963894477637

Epoch: 5| Step: 8
Training loss: 2.0604994348063776
Validation loss: 2.6234787627794574

Epoch: 5| Step: 9
Training loss: 1.845572782889404
Validation loss: 2.6339058836574827

Epoch: 5| Step: 10
Training loss: 2.251012574278097
Validation loss: 2.655601848735703

Epoch: 5| Step: 11
Training loss: 1.8978280553081326
Validation loss: 2.610798856538234

Epoch: 311| Step: 0
Training loss: 2.1518022302560165
Validation loss: 2.59482261203326

Epoch: 5| Step: 1
Training loss: 2.8816944631806436
Validation loss: 2.563690587388686

Epoch: 5| Step: 2
Training loss: 1.5866188878123706
Validation loss: 2.547292051123145

Epoch: 5| Step: 3
Training loss: 1.3468177730147817
Validation loss: 2.5538745718292764

Epoch: 5| Step: 4
Training loss: 2.1687644804105584
Validation loss: 2.55696272951995

Epoch: 5| Step: 5
Training loss: 1.9096629123918294
Validation loss: 2.538609195249866

Epoch: 5| Step: 6
Training loss: 2.132489973002179
Validation loss: 2.5361958602258214

Epoch: 5| Step: 7
Training loss: 2.198777040940951
Validation loss: 2.536067087807602

Epoch: 5| Step: 8
Training loss: 2.216406323147525
Validation loss: 2.5457665736429913

Epoch: 5| Step: 9
Training loss: 1.932627303456976
Validation loss: 2.5347078119736364

Epoch: 5| Step: 10
Training loss: 2.937369161086324
Validation loss: 2.5604095732546934

Epoch: 5| Step: 11
Training loss: 1.5538461987153314
Validation loss: 2.577093869935321

Epoch: 312| Step: 0
Training loss: 2.6405501496129267
Validation loss: 2.5997281638504157

Epoch: 5| Step: 1
Training loss: 2.0494710751623813
Validation loss: 2.602384767616597

Epoch: 5| Step: 2
Training loss: 2.4225257245440925
Validation loss: 2.5974710881298075

Epoch: 5| Step: 3
Training loss: 1.9954835440526593
Validation loss: 2.56553249494381

Epoch: 5| Step: 4
Training loss: 2.3606209401971165
Validation loss: 2.5611379965809675

Epoch: 5| Step: 5
Training loss: 2.5234629154401707
Validation loss: 2.551392832914334

Epoch: 5| Step: 6
Training loss: 1.9699067849892367
Validation loss: 2.5407607204570546

Epoch: 5| Step: 7
Training loss: 2.085133601237524
Validation loss: 2.516751848499229

Epoch: 5| Step: 8
Training loss: 2.3759514759901488
Validation loss: 2.521504050211793

Epoch: 5| Step: 9
Training loss: 1.696938012289698
Validation loss: 2.5233075371216924

Epoch: 5| Step: 10
Training loss: 1.531821552964425
Validation loss: 2.5237697543989213

Epoch: 5| Step: 11
Training loss: 2.7191013679142086
Validation loss: 2.5300952176758456

Epoch: 313| Step: 0
Training loss: 2.3121081870878855
Validation loss: 2.5250564594062626

Epoch: 5| Step: 1
Training loss: 1.8177323945501498
Validation loss: 2.5572293203384606

Epoch: 5| Step: 2
Training loss: 2.4351792658433205
Validation loss: 2.549130955972122

Epoch: 5| Step: 3
Training loss: 2.4840729727784034
Validation loss: 2.5715331142735414

Epoch: 5| Step: 4
Training loss: 1.8494280008170902
Validation loss: 2.576514043250154

Epoch: 5| Step: 5
Training loss: 1.992621998994713
Validation loss: 2.574807400275067

Epoch: 5| Step: 6
Training loss: 2.475959872985234
Validation loss: 2.563868494499166

Epoch: 5| Step: 7
Training loss: 1.6976098873655856
Validation loss: 2.600085636403653

Epoch: 5| Step: 8
Training loss: 2.0676336266948994
Validation loss: 2.6068779157708977

Epoch: 5| Step: 9
Training loss: 2.603625788150144
Validation loss: 2.57357130075338

Epoch: 5| Step: 10
Training loss: 2.0040751900948504
Validation loss: 2.5995663382093954

Epoch: 5| Step: 11
Training loss: 1.4698712553360527
Validation loss: 2.595213588213086

Epoch: 314| Step: 0
Training loss: 1.9141294506586497
Validation loss: 2.575927152040963

Epoch: 5| Step: 1
Training loss: 1.946657749747839
Validation loss: 2.5651339508238054

Epoch: 5| Step: 2
Training loss: 2.3040040117997105
Validation loss: 2.539803663798128

Epoch: 5| Step: 3
Training loss: 1.7719026889689555
Validation loss: 2.536839542033039

Epoch: 5| Step: 4
Training loss: 2.1174759896998987
Validation loss: 2.544085943297324

Epoch: 5| Step: 5
Training loss: 2.457919155942367
Validation loss: 2.529301853248675

Epoch: 5| Step: 6
Training loss: 2.2570442555846086
Validation loss: 2.5202000048340185

Epoch: 5| Step: 7
Training loss: 2.2185228258342438
Validation loss: 2.5450860470925245

Epoch: 5| Step: 8
Training loss: 2.078537061179633
Validation loss: 2.536436904618271

Epoch: 5| Step: 9
Training loss: 2.458282976658476
Validation loss: 2.5814028315216224

Epoch: 5| Step: 10
Training loss: 2.4055852591090536
Validation loss: 2.6119830637655856

Epoch: 5| Step: 11
Training loss: 1.634411480644187
Validation loss: 2.6253774765117512

Epoch: 315| Step: 0
Training loss: 1.661677267384489
Validation loss: 2.5839050657557934

Epoch: 5| Step: 1
Training loss: 2.6825471527989486
Validation loss: 2.5742639837694985

Epoch: 5| Step: 2
Training loss: 2.2183296987540575
Validation loss: 2.5456534012788987

Epoch: 5| Step: 3
Training loss: 1.9891776891348913
Validation loss: 2.5641966723249676

Epoch: 5| Step: 4
Training loss: 1.6375837930687056
Validation loss: 2.5320295734629483

Epoch: 5| Step: 5
Training loss: 2.570489662812986
Validation loss: 2.5287117692474546

Epoch: 5| Step: 6
Training loss: 2.2043598583145925
Validation loss: 2.5269702712070092

Epoch: 5| Step: 7
Training loss: 2.423055053677455
Validation loss: 2.544311907048538

Epoch: 5| Step: 8
Training loss: 1.9675824169652105
Validation loss: 2.506840772120103

Epoch: 5| Step: 9
Training loss: 2.176530308427199
Validation loss: 2.5238960605317176

Epoch: 5| Step: 10
Training loss: 2.129687356703957
Validation loss: 2.5190382640262547

Epoch: 5| Step: 11
Training loss: 1.5646359911852052
Validation loss: 2.542059681685504

Epoch: 316| Step: 0
Training loss: 2.145690493474448
Validation loss: 2.506637032353547

Epoch: 5| Step: 1
Training loss: 2.5244170847419047
Validation loss: 2.537346558402424

Epoch: 5| Step: 2
Training loss: 2.0232694696007876
Validation loss: 2.529629588981078

Epoch: 5| Step: 3
Training loss: 2.250935677841828
Validation loss: 2.5259603166098326

Epoch: 5| Step: 4
Training loss: 1.953556714982589
Validation loss: 2.559813288887134

Epoch: 5| Step: 5
Training loss: 1.9697036855311933
Validation loss: 2.545911606731529

Epoch: 5| Step: 6
Training loss: 2.115870668017508
Validation loss: 2.561743717705968

Epoch: 5| Step: 7
Training loss: 2.0399927816076286
Validation loss: 2.555546444091126

Epoch: 5| Step: 8
Training loss: 2.245853311689286
Validation loss: 2.594834242790096

Epoch: 5| Step: 9
Training loss: 2.1396635917708435
Validation loss: 2.5717782901211494

Epoch: 5| Step: 10
Training loss: 2.3922297259091647
Validation loss: 2.5467712755671075

Epoch: 5| Step: 11
Training loss: 1.331249197659676
Validation loss: 2.566478749513545

Epoch: 317| Step: 0
Training loss: 2.7084327581691374
Validation loss: 2.5690704889068483

Epoch: 5| Step: 1
Training loss: 2.1592429577526118
Validation loss: 2.6056510121861964

Epoch: 5| Step: 2
Training loss: 2.035329976052944
Validation loss: 2.60474224341977

Epoch: 5| Step: 3
Training loss: 2.469601448120938
Validation loss: 2.6436650121990133

Epoch: 5| Step: 4
Training loss: 2.4173718333466945
Validation loss: 2.6944169846563257

Epoch: 5| Step: 5
Training loss: 2.15916091586424
Validation loss: 2.6403088069966247

Epoch: 5| Step: 6
Training loss: 1.819653502272317
Validation loss: 2.623511668268798

Epoch: 5| Step: 7
Training loss: 2.047747244153912
Validation loss: 2.5675699700971255

Epoch: 5| Step: 8
Training loss: 2.1120536468079187
Validation loss: 2.5455411565934347

Epoch: 5| Step: 9
Training loss: 2.075382574271511
Validation loss: 2.5131360846199673

Epoch: 5| Step: 10
Training loss: 2.296823202574168
Validation loss: 2.527801120493661

Epoch: 5| Step: 11
Training loss: 0.9756736677739893
Validation loss: 2.5447064003782596

Epoch: 318| Step: 0
Training loss: 2.357314871419416
Validation loss: 2.5257036330315503

Epoch: 5| Step: 1
Training loss: 2.85998345243562
Validation loss: 2.524815829566656

Epoch: 5| Step: 2
Training loss: 1.5021075701334432
Validation loss: 2.537866813630557

Epoch: 5| Step: 3
Training loss: 1.7474967218676642
Validation loss: 2.527180406499079

Epoch: 5| Step: 4
Training loss: 2.031522879244244
Validation loss: 2.537981724700332

Epoch: 5| Step: 5
Training loss: 2.079405999280797
Validation loss: 2.543217671717511

Epoch: 5| Step: 6
Training loss: 2.5653096124830523
Validation loss: 2.550318186673795

Epoch: 5| Step: 7
Training loss: 2.2690708658610284
Validation loss: 2.5669343889758207

Epoch: 5| Step: 8
Training loss: 2.214342171524987
Validation loss: 2.6091414097680645

Epoch: 5| Step: 9
Training loss: 2.198002697555095
Validation loss: 2.615140495865945

Epoch: 5| Step: 10
Training loss: 2.2123759951566355
Validation loss: 2.633061658500473

Epoch: 5| Step: 11
Training loss: 2.745653792798438
Validation loss: 2.6109077687818405

Epoch: 319| Step: 0
Training loss: 2.1425495267505155
Validation loss: 2.641111720296199

Epoch: 5| Step: 1
Training loss: 1.6534434597706096
Validation loss: 2.622620260768028

Epoch: 5| Step: 2
Training loss: 2.1369502693569715
Validation loss: 2.5602669678068577

Epoch: 5| Step: 3
Training loss: 2.527263940792141
Validation loss: 2.54100678168949

Epoch: 5| Step: 4
Training loss: 2.6878255602569534
Validation loss: 2.5216480185280803

Epoch: 5| Step: 5
Training loss: 2.422661930337131
Validation loss: 2.5187290974862693

Epoch: 5| Step: 6
Training loss: 2.3766034385470727
Validation loss: 2.5147641013991717

Epoch: 5| Step: 7
Training loss: 2.028065341724461
Validation loss: 2.5029795732610847

Epoch: 5| Step: 8
Training loss: 1.9587258561039411
Validation loss: 2.5402461652791555

Epoch: 5| Step: 9
Training loss: 2.329447940045731
Validation loss: 2.5384444399061823

Epoch: 5| Step: 10
Training loss: 2.179238112416895
Validation loss: 2.5616614357525296

Epoch: 5| Step: 11
Training loss: 3.1390539386607834
Validation loss: 2.570740209085052

Epoch: 320| Step: 0
Training loss: 1.3750609471078394
Validation loss: 2.599983729103724

Epoch: 5| Step: 1
Training loss: 1.9714894687140088
Validation loss: 2.5998200394020956

Epoch: 5| Step: 2
Training loss: 1.8700797054827365
Validation loss: 2.586585126071726

Epoch: 5| Step: 3
Training loss: 2.220101602995798
Validation loss: 2.6092716280321127

Epoch: 5| Step: 4
Training loss: 2.9523414500768363
Validation loss: 2.5915468464738227

Epoch: 5| Step: 5
Training loss: 2.3353610651190273
Validation loss: 2.573954235296462

Epoch: 5| Step: 6
Training loss: 2.1530616518398946
Validation loss: 2.577808738345354

Epoch: 5| Step: 7
Training loss: 2.66221612840796
Validation loss: 2.569471613995211

Epoch: 5| Step: 8
Training loss: 1.8936914214589005
Validation loss: 2.5400818407210286

Epoch: 5| Step: 9
Training loss: 2.038294271103663
Validation loss: 2.5446835394326497

Epoch: 5| Step: 10
Training loss: 1.9191704926126198
Validation loss: 2.555459943021664

Epoch: 5| Step: 11
Training loss: 2.1241902323159305
Validation loss: 2.555123909199516

Epoch: 321| Step: 0
Training loss: 2.743977280503242
Validation loss: 2.543698012424504

Epoch: 5| Step: 1
Training loss: 1.9910826368095862
Validation loss: 2.537959386398903

Epoch: 5| Step: 2
Training loss: 1.967887204954115
Validation loss: 2.5283818301912353

Epoch: 5| Step: 3
Training loss: 1.6359292541121204
Validation loss: 2.538969392292014

Epoch: 5| Step: 4
Training loss: 2.2720332577245745
Validation loss: 2.572417324017842

Epoch: 5| Step: 5
Training loss: 2.377658159548894
Validation loss: 2.553439660917009

Epoch: 5| Step: 6
Training loss: 2.0189489119000275
Validation loss: 2.583160603552709

Epoch: 5| Step: 7
Training loss: 2.2735356575137167
Validation loss: 2.60044438159144

Epoch: 5| Step: 8
Training loss: 2.1261111327701476
Validation loss: 2.5920676535750604

Epoch: 5| Step: 9
Training loss: 1.8877068058932958
Validation loss: 2.6139488526892904

Epoch: 5| Step: 10
Training loss: 2.3415275717950275
Validation loss: 2.6420616703461834

Epoch: 5| Step: 11
Training loss: 1.6537202192581955
Validation loss: 2.6089246326956537

Epoch: 322| Step: 0
Training loss: 2.6739862143104833
Validation loss: 2.608529035601991

Epoch: 5| Step: 1
Training loss: 2.163119003697604
Validation loss: 2.593064976050599

Epoch: 5| Step: 2
Training loss: 2.3115796500149064
Validation loss: 2.5709262760514418

Epoch: 5| Step: 3
Training loss: 1.8595319409213278
Validation loss: 2.5657548256697713

Epoch: 5| Step: 4
Training loss: 2.2827826798911106
Validation loss: 2.546818997834203

Epoch: 5| Step: 5
Training loss: 2.5643361653997796
Validation loss: 2.5175344632557404

Epoch: 5| Step: 6
Training loss: 2.0071958789891085
Validation loss: 2.551173364509082

Epoch: 5| Step: 7
Training loss: 1.7673242544724999
Validation loss: 2.544678975807406

Epoch: 5| Step: 8
Training loss: 1.4635180361685167
Validation loss: 2.5584551319884468

Epoch: 5| Step: 9
Training loss: 1.7076991540093496
Validation loss: 2.5760968486358258

Epoch: 5| Step: 10
Training loss: 2.616273635050331
Validation loss: 2.571495993410871

Epoch: 5| Step: 11
Training loss: 1.5570038834696747
Validation loss: 2.597405726080771

Epoch: 323| Step: 0
Training loss: 2.0863029931287986
Validation loss: 2.5977262788669195

Epoch: 5| Step: 1
Training loss: 2.4582806489978033
Validation loss: 2.6362746630027925

Epoch: 5| Step: 2
Training loss: 2.7422092360806096
Validation loss: 2.661871589980308

Epoch: 5| Step: 3
Training loss: 1.6633826644605065
Validation loss: 2.6296944270398783

Epoch: 5| Step: 4
Training loss: 2.668743149616821
Validation loss: 2.6514428827522933

Epoch: 5| Step: 5
Training loss: 2.5190021284578243
Validation loss: 2.629555794373165

Epoch: 5| Step: 6
Training loss: 1.9894983429922224
Validation loss: 2.6048038936453013

Epoch: 5| Step: 7
Training loss: 1.717509845333394
Validation loss: 2.5877714079261667

Epoch: 5| Step: 8
Training loss: 1.7060363803486132
Validation loss: 2.5679138532077723

Epoch: 5| Step: 9
Training loss: 1.5656269065860553
Validation loss: 2.5484453559409808

Epoch: 5| Step: 10
Training loss: 2.3188131606555844
Validation loss: 2.5108989803617257

Epoch: 5| Step: 11
Training loss: 1.6280177046260669
Validation loss: 2.5091568303704603

Epoch: 324| Step: 0
Training loss: 2.7097850625643045
Validation loss: 2.532026305287512

Epoch: 5| Step: 1
Training loss: 1.9753415403930512
Validation loss: 2.5197001358719016

Epoch: 5| Step: 2
Training loss: 1.9137348128101752
Validation loss: 2.5291209705902453

Epoch: 5| Step: 3
Training loss: 2.017782077285249
Validation loss: 2.5171488693622828

Epoch: 5| Step: 4
Training loss: 2.8572653403595063
Validation loss: 2.530542902828185

Epoch: 5| Step: 5
Training loss: 2.423193984644902
Validation loss: 2.540442939513161

Epoch: 5| Step: 6
Training loss: 1.6182755942345741
Validation loss: 2.548634816289613

Epoch: 5| Step: 7
Training loss: 2.3195773928592303
Validation loss: 2.5607765379914778

Epoch: 5| Step: 8
Training loss: 1.5974490304137712
Validation loss: 2.5915253953873756

Epoch: 5| Step: 9
Training loss: 2.283385962090899
Validation loss: 2.5968160546839507

Epoch: 5| Step: 10
Training loss: 2.3548003050090993
Validation loss: 2.6321548183803074

Epoch: 5| Step: 11
Training loss: 1.6994717113702724
Validation loss: 2.6414406549378797

Epoch: 325| Step: 0
Training loss: 1.901603037543914
Validation loss: 2.6256765901454626

Epoch: 5| Step: 1
Training loss: 2.9410849265269348
Validation loss: 2.63008296529284

Epoch: 5| Step: 2
Training loss: 1.87885447726228
Validation loss: 2.620255113919954

Epoch: 5| Step: 3
Training loss: 2.1462579942753206
Validation loss: 2.589309276667782

Epoch: 5| Step: 4
Training loss: 1.7034865529274479
Validation loss: 2.5892591703711143

Epoch: 5| Step: 5
Training loss: 1.6319531821519764
Validation loss: 2.5818359562767332

Epoch: 5| Step: 6
Training loss: 1.9713658106135405
Validation loss: 2.585029711623288

Epoch: 5| Step: 7
Training loss: 2.5379111158950853
Validation loss: 2.5519005664594556

Epoch: 5| Step: 8
Training loss: 2.6449711714040656
Validation loss: 2.5421189364374586

Epoch: 5| Step: 9
Training loss: 2.050845771864468
Validation loss: 2.566093774175467

Epoch: 5| Step: 10
Training loss: 1.846445150424789
Validation loss: 2.5555859462571138

Epoch: 5| Step: 11
Training loss: 1.6471373144648982
Validation loss: 2.5624823298271484

Epoch: 326| Step: 0
Training loss: 1.6769074610391577
Validation loss: 2.5632791730179956

Epoch: 5| Step: 1
Training loss: 2.3841577953434467
Validation loss: 2.5841557529292403

Epoch: 5| Step: 2
Training loss: 1.9075799273822858
Validation loss: 2.602675741017005

Epoch: 5| Step: 3
Training loss: 2.1857355903919298
Validation loss: 2.587523954314384

Epoch: 5| Step: 4
Training loss: 2.327977015765275
Validation loss: 2.579450359107881

Epoch: 5| Step: 5
Training loss: 2.42856063960588
Validation loss: 2.5858330335769537

Epoch: 5| Step: 6
Training loss: 2.058624093630139
Validation loss: 2.59337173905614

Epoch: 5| Step: 7
Training loss: 1.7747592117966813
Validation loss: 2.5867475564301845

Epoch: 5| Step: 8
Training loss: 2.426634019729891
Validation loss: 2.5747179734925756

Epoch: 5| Step: 9
Training loss: 2.4051794668001576
Validation loss: 2.592089125085061

Epoch: 5| Step: 10
Training loss: 1.6913148357541146
Validation loss: 2.5994747326664736

Epoch: 5| Step: 11
Training loss: 2.178608835957829
Validation loss: 2.5610281159601453

Epoch: 327| Step: 0
Training loss: 2.8401753521902418
Validation loss: 2.5912773495114263

Epoch: 5| Step: 1
Training loss: 2.095122044361097
Validation loss: 2.541713035400255

Epoch: 5| Step: 2
Training loss: 2.1125930539916298
Validation loss: 2.5557040701059504

Epoch: 5| Step: 3
Training loss: 1.5655194671453652
Validation loss: 2.522564322587552

Epoch: 5| Step: 4
Training loss: 1.9348004515709862
Validation loss: 2.5198985381616037

Epoch: 5| Step: 5
Training loss: 2.0300256413686646
Validation loss: 2.510895376083551

Epoch: 5| Step: 6
Training loss: 2.1325998724283344
Validation loss: 2.5319026882604083

Epoch: 5| Step: 7
Training loss: 1.881330738046011
Validation loss: 2.5419382291604893

Epoch: 5| Step: 8
Training loss: 2.6020577907199405
Validation loss: 2.5762735828280894

Epoch: 5| Step: 9
Training loss: 2.045478635703901
Validation loss: 2.5861432011934413

Epoch: 5| Step: 10
Training loss: 2.021216274303308
Validation loss: 2.5921083831823535

Epoch: 5| Step: 11
Training loss: 2.1720297778847475
Validation loss: 2.5922925945038027

Epoch: 328| Step: 0
Training loss: 2.012409691531783
Validation loss: 2.573270002548959

Epoch: 5| Step: 1
Training loss: 2.394470114495361
Validation loss: 2.55975241791312

Epoch: 5| Step: 2
Training loss: 2.0691231244171533
Validation loss: 2.553688460592108

Epoch: 5| Step: 3
Training loss: 1.839319757895143
Validation loss: 2.544148243371869

Epoch: 5| Step: 4
Training loss: 2.291638379933882
Validation loss: 2.550623259390714

Epoch: 5| Step: 5
Training loss: 2.241643540595168
Validation loss: 2.5364282842549417

Epoch: 5| Step: 6
Training loss: 2.4100076638887358
Validation loss: 2.547645616085129

Epoch: 5| Step: 7
Training loss: 2.3970826139890495
Validation loss: 2.5522277025662525

Epoch: 5| Step: 8
Training loss: 2.3506476057812113
Validation loss: 2.5519313312859033

Epoch: 5| Step: 9
Training loss: 2.926855727789456
Validation loss: 2.577933044223318

Epoch: 5| Step: 10
Training loss: 1.2911672806070926
Validation loss: 2.582863423975669

Epoch: 5| Step: 11
Training loss: 1.3632630617518797
Validation loss: 2.5974759261564153

Epoch: 329| Step: 0
Training loss: 2.5606952918108865
Validation loss: 2.6042246328896135

Epoch: 5| Step: 1
Training loss: 2.6523933686908867
Validation loss: 2.6407310720353614

Epoch: 5| Step: 2
Training loss: 1.9370907382026263
Validation loss: 2.6452106596677183

Epoch: 5| Step: 3
Training loss: 2.3432247336207848
Validation loss: 2.626779305191942

Epoch: 5| Step: 4
Training loss: 2.0290853850934907
Validation loss: 2.6172363200782263

Epoch: 5| Step: 5
Training loss: 2.4107078774180337
Validation loss: 2.618922289671435

Epoch: 5| Step: 6
Training loss: 2.188124540274996
Validation loss: 2.627552483122509

Epoch: 5| Step: 7
Training loss: 1.4929245331912122
Validation loss: 2.5926244813860215

Epoch: 5| Step: 8
Training loss: 2.3368105070361778
Validation loss: 2.583440295179033

Epoch: 5| Step: 9
Training loss: 2.020304725437296
Validation loss: 2.5871547934018415

Epoch: 5| Step: 10
Training loss: 1.773945336584513
Validation loss: 2.5688132345579486

Epoch: 5| Step: 11
Training loss: 1.5130333519667762
Validation loss: 2.560701105170712

Epoch: 330| Step: 0
Training loss: 2.1289895697571573
Validation loss: 2.5548528915203463

Epoch: 5| Step: 1
Training loss: 2.0916311445930753
Validation loss: 2.5444875030770393

Epoch: 5| Step: 2
Training loss: 2.398680137264955
Validation loss: 2.5705789313936607

Epoch: 5| Step: 3
Training loss: 1.8872184644503285
Validation loss: 2.5549450432452754

Epoch: 5| Step: 4
Training loss: 2.6004906814898185
Validation loss: 2.590507955326759

Epoch: 5| Step: 5
Training loss: 2.4519511530492806
Validation loss: 2.5756913345003647

Epoch: 5| Step: 6
Training loss: 1.5603585636880968
Validation loss: 2.581565718781763

Epoch: 5| Step: 7
Training loss: 1.6205284511819247
Validation loss: 2.59167277027732

Epoch: 5| Step: 8
Training loss: 2.4638110625791505
Validation loss: 2.557658052063379

Epoch: 5| Step: 9
Training loss: 1.8835987649516206
Validation loss: 2.623731980524678

Epoch: 5| Step: 10
Training loss: 2.2561077294972347
Validation loss: 2.6194713231092415

Epoch: 5| Step: 11
Training loss: 1.1289653300629163
Validation loss: 2.6642924495731446

Epoch: 331| Step: 0
Training loss: 2.459485888374545
Validation loss: 2.6757885229749756

Epoch: 5| Step: 1
Training loss: 2.333254699290297
Validation loss: 2.683815237619592

Epoch: 5| Step: 2
Training loss: 2.1171959458073584
Validation loss: 2.688578174988084

Epoch: 5| Step: 3
Training loss: 1.7346528664088119
Validation loss: 2.6563195836545903

Epoch: 5| Step: 4
Training loss: 2.3366475636802884
Validation loss: 2.651013448498829

Epoch: 5| Step: 5
Training loss: 2.385941249242409
Validation loss: 2.5971986295837755

Epoch: 5| Step: 6
Training loss: 1.5444058835790682
Validation loss: 2.592428230633691

Epoch: 5| Step: 7
Training loss: 2.2584225579497623
Validation loss: 2.572598574096402

Epoch: 5| Step: 8
Training loss: 2.192175310336187
Validation loss: 2.583025073046063

Epoch: 5| Step: 9
Training loss: 2.03754821907511
Validation loss: 2.5537762897947562

Epoch: 5| Step: 10
Training loss: 2.080183801966398
Validation loss: 2.555720914510326

Epoch: 5| Step: 11
Training loss: 1.373583150351283
Validation loss: 2.545404134294792

Epoch: 332| Step: 0
Training loss: 2.0341921113248786
Validation loss: 2.557622976783278

Epoch: 5| Step: 1
Training loss: 1.8853565297801809
Validation loss: 2.580160431820853

Epoch: 5| Step: 2
Training loss: 1.9817835188738278
Validation loss: 2.592055349294475

Epoch: 5| Step: 3
Training loss: 2.0598924510621
Validation loss: 2.5934149131185102

Epoch: 5| Step: 4
Training loss: 2.128848461801692
Validation loss: 2.5834973742075062

Epoch: 5| Step: 5
Training loss: 2.3281741553116118
Validation loss: 2.646203346862293

Epoch: 5| Step: 6
Training loss: 2.8507670926186557
Validation loss: 2.659956720592845

Epoch: 5| Step: 7
Training loss: 2.1075857980805317
Validation loss: 2.6723694631945425

Epoch: 5| Step: 8
Training loss: 2.1469992522552426
Validation loss: 2.6775179199761228

Epoch: 5| Step: 9
Training loss: 2.203411435384405
Validation loss: 2.660325708280689

Epoch: 5| Step: 10
Training loss: 1.770853521194574
Validation loss: 2.607921397238449

Epoch: 5| Step: 11
Training loss: 2.1790278267136207
Validation loss: 2.5588138165363747

Epoch: 333| Step: 0
Training loss: 1.8130778345274
Validation loss: 2.5493803709771146

Epoch: 5| Step: 1
Training loss: 2.1104308699958048
Validation loss: 2.5529388252464242

Epoch: 5| Step: 2
Training loss: 2.8856061925971264
Validation loss: 2.5648663670385754

Epoch: 5| Step: 3
Training loss: 2.231689012437908
Validation loss: 2.548922657583793

Epoch: 5| Step: 4
Training loss: 2.9335132897691873
Validation loss: 2.5478518393299385

Epoch: 5| Step: 5
Training loss: 2.1288410701748
Validation loss: 2.559468076765562

Epoch: 5| Step: 6
Training loss: 2.062273995703702
Validation loss: 2.5700463618093283

Epoch: 5| Step: 7
Training loss: 2.222473458817109
Validation loss: 2.599446384012011

Epoch: 5| Step: 8
Training loss: 2.227205739048985
Validation loss: 2.6232021623832305

Epoch: 5| Step: 9
Training loss: 1.301465103883303
Validation loss: 2.6051323333681435

Epoch: 5| Step: 10
Training loss: 1.9426871359097364
Validation loss: 2.615373669941882

Epoch: 5| Step: 11
Training loss: 0.8492539988960668
Validation loss: 2.661153963804035

Epoch: 334| Step: 0
Training loss: 2.0218082187006674
Validation loss: 2.6373010402140906

Epoch: 5| Step: 1
Training loss: 1.7466543095760663
Validation loss: 2.6332258735733833

Epoch: 5| Step: 2
Training loss: 2.3395328474386474
Validation loss: 2.6020719203184557

Epoch: 5| Step: 3
Training loss: 2.4053842547680273
Validation loss: 2.5977372350473655

Epoch: 5| Step: 4
Training loss: 2.404020787864849
Validation loss: 2.576538073406791

Epoch: 5| Step: 5
Training loss: 2.181486091843973
Validation loss: 2.5683525945963157

Epoch: 5| Step: 6
Training loss: 2.1897751012487725
Validation loss: 2.5682426358418193

Epoch: 5| Step: 7
Training loss: 2.1233298526338147
Validation loss: 2.566219835947987

Epoch: 5| Step: 8
Training loss: 1.9874244147749014
Validation loss: 2.5840230733674914

Epoch: 5| Step: 9
Training loss: 2.211407507014222
Validation loss: 2.6122195715421817

Epoch: 5| Step: 10
Training loss: 1.788753432851039
Validation loss: 2.6018896345902482

Epoch: 5| Step: 11
Training loss: 1.8316769775896442
Validation loss: 2.6291929771650486

Epoch: 335| Step: 0
Training loss: 2.1088152107326943
Validation loss: 2.58888524165622

Epoch: 5| Step: 1
Training loss: 2.3550734559932054
Validation loss: 2.5592615629526643

Epoch: 5| Step: 2
Training loss: 1.7635078227143015
Validation loss: 2.5648678059127152

Epoch: 5| Step: 3
Training loss: 1.765833276937981
Validation loss: 2.581795297403781

Epoch: 5| Step: 4
Training loss: 2.8926069733432307
Validation loss: 2.5587877544592588

Epoch: 5| Step: 5
Training loss: 2.339966632246463
Validation loss: 2.570659729945336

Epoch: 5| Step: 6
Training loss: 2.1589974853161737
Validation loss: 2.5634175224165947

Epoch: 5| Step: 7
Training loss: 1.669620741077705
Validation loss: 2.577277685858345

Epoch: 5| Step: 8
Training loss: 2.2233011553760185
Validation loss: 2.6036574412605584

Epoch: 5| Step: 9
Training loss: 1.7316989357921408
Validation loss: 2.5800878643474854

Epoch: 5| Step: 10
Training loss: 2.030493962365112
Validation loss: 2.588460611395794

Epoch: 5| Step: 11
Training loss: 1.7640906224403918
Validation loss: 2.596576074104181

Epoch: 336| Step: 0
Training loss: 1.7858168967602117
Validation loss: 2.633964313036744

Epoch: 5| Step: 1
Training loss: 2.335425891795613
Validation loss: 2.6497782386503332

Epoch: 5| Step: 2
Training loss: 2.1695360721456245
Validation loss: 2.6822211777890823

Epoch: 5| Step: 3
Training loss: 2.4743541414287913
Validation loss: 2.6376476394165977

Epoch: 5| Step: 4
Training loss: 2.0625771016823884
Validation loss: 2.6044879880398573

Epoch: 5| Step: 5
Training loss: 1.9172494320243723
Validation loss: 2.576936320616833

Epoch: 5| Step: 6
Training loss: 1.8771823897847066
Validation loss: 2.5708880606214897

Epoch: 5| Step: 7
Training loss: 2.1296457108828775
Validation loss: 2.542104735431032

Epoch: 5| Step: 8
Training loss: 2.6773549596431057
Validation loss: 2.5496924288079015

Epoch: 5| Step: 9
Training loss: 2.265785895586759
Validation loss: 2.5361633103096035

Epoch: 5| Step: 10
Training loss: 1.7724182291901789
Validation loss: 2.517836595697508

Epoch: 5| Step: 11
Training loss: 2.1297438907224455
Validation loss: 2.5215904790694115

Epoch: 337| Step: 0
Training loss: 2.3061824799037987
Validation loss: 2.502580796740309

Epoch: 5| Step: 1
Training loss: 1.5455457582688097
Validation loss: 2.526980964136577

Epoch: 5| Step: 2
Training loss: 2.283057763764541
Validation loss: 2.5417201272365846

Epoch: 5| Step: 3
Training loss: 1.89018660577814
Validation loss: 2.554169519701843

Epoch: 5| Step: 4
Training loss: 2.4009347644985453
Validation loss: 2.6039900427367653

Epoch: 5| Step: 5
Training loss: 1.797823315698457
Validation loss: 2.6423722755796684

Epoch: 5| Step: 6
Training loss: 1.8329494753594768
Validation loss: 2.6318207258493795

Epoch: 5| Step: 7
Training loss: 2.418009374027682
Validation loss: 2.620050858605804

Epoch: 5| Step: 8
Training loss: 2.1146867347829774
Validation loss: 2.6223793458593927

Epoch: 5| Step: 9
Training loss: 2.493460207667341
Validation loss: 2.6009560528283355

Epoch: 5| Step: 10
Training loss: 2.3150859498779446
Validation loss: 2.599125783465646

Epoch: 5| Step: 11
Training loss: 1.3357859743083604
Validation loss: 2.583501730834448

Epoch: 338| Step: 0
Training loss: 2.139547257705831
Validation loss: 2.578490869011923

Epoch: 5| Step: 1
Training loss: 2.1626326591947422
Validation loss: 2.561949525415955

Epoch: 5| Step: 2
Training loss: 2.208088903275148
Validation loss: 2.5575389250474534

Epoch: 5| Step: 3
Training loss: 2.0528948836361
Validation loss: 2.5561538399950354

Epoch: 5| Step: 4
Training loss: 2.2572332252878757
Validation loss: 2.5639432355165988

Epoch: 5| Step: 5
Training loss: 1.575289454428733
Validation loss: 2.539514272552448

Epoch: 5| Step: 6
Training loss: 1.8066782354615551
Validation loss: 2.5727734094704533

Epoch: 5| Step: 7
Training loss: 2.226917332266408
Validation loss: 2.5773091614834307

Epoch: 5| Step: 8
Training loss: 2.086480116543984
Validation loss: 2.5736011580475884

Epoch: 5| Step: 9
Training loss: 2.5713670416691152
Validation loss: 2.622744154818183

Epoch: 5| Step: 10
Training loss: 2.228816019686097
Validation loss: 2.6148525013362622

Epoch: 5| Step: 11
Training loss: 1.413541539791198
Validation loss: 2.629549232213035

Epoch: 339| Step: 0
Training loss: 1.808866871588636
Validation loss: 2.643602314184035

Epoch: 5| Step: 1
Training loss: 2.8075771056921552
Validation loss: 2.615602166863504

Epoch: 5| Step: 2
Training loss: 1.8912314080876982
Validation loss: 2.6261027192207953

Epoch: 5| Step: 3
Training loss: 1.848889961232176
Validation loss: 2.627065485521314

Epoch: 5| Step: 4
Training loss: 1.6359399658884084
Validation loss: 2.616648642261247

Epoch: 5| Step: 5
Training loss: 2.2933135155257696
Validation loss: 2.634067877548521

Epoch: 5| Step: 6
Training loss: 2.063686838552484
Validation loss: 2.6137653771009997

Epoch: 5| Step: 7
Training loss: 2.2324195254389427
Validation loss: 2.6090883337385162

Epoch: 5| Step: 8
Training loss: 1.795542745541892
Validation loss: 2.5859146578145817

Epoch: 5| Step: 9
Training loss: 2.2872274809719157
Validation loss: 2.5960480915485293

Epoch: 5| Step: 10
Training loss: 2.153577613595749
Validation loss: 2.5592333899242146

Epoch: 5| Step: 11
Training loss: 1.3550083136919329
Validation loss: 2.565001181973389

Epoch: 340| Step: 0
Training loss: 2.565030035248335
Validation loss: 2.583799221472435

Epoch: 5| Step: 1
Training loss: 1.775402778273344
Validation loss: 2.5445281139116696

Epoch: 5| Step: 2
Training loss: 2.3916479178656305
Validation loss: 2.5559279165085558

Epoch: 5| Step: 3
Training loss: 1.706825156613774
Validation loss: 2.5727499329961074

Epoch: 5| Step: 4
Training loss: 2.4527561372076145
Validation loss: 2.614764915229976

Epoch: 5| Step: 5
Training loss: 2.654920716619546
Validation loss: 2.707199252935082

Epoch: 5| Step: 6
Training loss: 1.5176833029179482
Validation loss: 2.6715814120559243

Epoch: 5| Step: 7
Training loss: 2.300008284512402
Validation loss: 2.6478627536806654

Epoch: 5| Step: 8
Training loss: 1.9412405760782578
Validation loss: 2.6258053603801432

Epoch: 5| Step: 9
Training loss: 1.7843596770101304
Validation loss: 2.581283441808789

Epoch: 5| Step: 10
Training loss: 2.1008519669699677
Validation loss: 2.5641540600881365

Epoch: 5| Step: 11
Training loss: 1.6450154006376234
Validation loss: 2.534787228083122

Epoch: 341| Step: 0
Training loss: 2.2021310065482527
Validation loss: 2.5585111881359626

Epoch: 5| Step: 1
Training loss: 1.9966272286971236
Validation loss: 2.5333631431139123

Epoch: 5| Step: 2
Training loss: 1.9002095809590593
Validation loss: 2.5493771406312637

Epoch: 5| Step: 3
Training loss: 2.6413751924726006
Validation loss: 2.5602008965077676

Epoch: 5| Step: 4
Training loss: 2.5759340166324978
Validation loss: 2.5524862140105133

Epoch: 5| Step: 5
Training loss: 1.9137633420751965
Validation loss: 2.5545933674026395

Epoch: 5| Step: 6
Training loss: 2.384534370025879
Validation loss: 2.551094059362654

Epoch: 5| Step: 7
Training loss: 1.8401264208403407
Validation loss: 2.57854637016325

Epoch: 5| Step: 8
Training loss: 1.4841913210009667
Validation loss: 2.594005227457925

Epoch: 5| Step: 9
Training loss: 2.0045699122990643
Validation loss: 2.6541912066244935

Epoch: 5| Step: 10
Training loss: 2.3441300147651893
Validation loss: 2.6437002385493757

Epoch: 5| Step: 11
Training loss: 3.179212080071608
Validation loss: 2.6203142950529146

Epoch: 342| Step: 0
Training loss: 1.3939750267347752
Validation loss: 2.656066581526834

Epoch: 5| Step: 1
Training loss: 2.4989742081903694
Validation loss: 2.611874036616087

Epoch: 5| Step: 2
Training loss: 2.074241968517058
Validation loss: 2.5699768890833465

Epoch: 5| Step: 3
Training loss: 2.134735023439746
Validation loss: 2.563477895584971

Epoch: 5| Step: 4
Training loss: 2.680204736376317
Validation loss: 2.5334658127473095

Epoch: 5| Step: 5
Training loss: 2.2908754427820908
Validation loss: 2.520326411032869

Epoch: 5| Step: 6
Training loss: 1.9329786150637946
Validation loss: 2.5342078646524793

Epoch: 5| Step: 7
Training loss: 1.6813681777204252
Validation loss: 2.5225268532685017

Epoch: 5| Step: 8
Training loss: 1.861116891665362
Validation loss: 2.5586639084066434

Epoch: 5| Step: 9
Training loss: 2.3769421918080007
Validation loss: 2.549979093020258

Epoch: 5| Step: 10
Training loss: 2.133262614727107
Validation loss: 2.548860218922728

Epoch: 5| Step: 11
Training loss: 1.4831660919348375
Validation loss: 2.5688427024319336

Epoch: 343| Step: 0
Training loss: 1.37992220617866
Validation loss: 2.5708683015917217

Epoch: 5| Step: 1
Training loss: 2.7314905265854708
Validation loss: 2.5693727415980105

Epoch: 5| Step: 2
Training loss: 1.653940497950609
Validation loss: 2.572029788464486

Epoch: 5| Step: 3
Training loss: 2.2275023501262834
Validation loss: 2.5671005316328843

Epoch: 5| Step: 4
Training loss: 1.9750562514974046
Validation loss: 2.565020329739995

Epoch: 5| Step: 5
Training loss: 1.6468409178245529
Validation loss: 2.5727234792318865

Epoch: 5| Step: 6
Training loss: 1.5543830611844742
Validation loss: 2.580678827055919

Epoch: 5| Step: 7
Training loss: 2.6694005919140507
Validation loss: 2.5751641427966057

Epoch: 5| Step: 8
Training loss: 1.6305162262961654
Validation loss: 2.6046253614178108

Epoch: 5| Step: 9
Training loss: 2.57728072705418
Validation loss: 2.5783702078683026

Epoch: 5| Step: 10
Training loss: 2.0851887134070193
Validation loss: 2.5991276333593034

Epoch: 5| Step: 11
Training loss: 3.033298547426518
Validation loss: 2.6230702308037253

Epoch: 344| Step: 0
Training loss: 1.769019855092413
Validation loss: 2.6643903051454716

Epoch: 5| Step: 1
Training loss: 2.570378265003308
Validation loss: 2.7032972806446374

Epoch: 5| Step: 2
Training loss: 2.0612721111529746
Validation loss: 2.711121548314884

Epoch: 5| Step: 3
Training loss: 2.5596086891438166
Validation loss: 2.7363345690444194

Epoch: 5| Step: 4
Training loss: 2.232150270722203
Validation loss: 2.6813146546340954

Epoch: 5| Step: 5
Training loss: 2.415939440871399
Validation loss: 2.626623737620585

Epoch: 5| Step: 6
Training loss: 2.133112400754285
Validation loss: 2.5407912291840336

Epoch: 5| Step: 7
Training loss: 1.8403637064259695
Validation loss: 2.4883297726358062

Epoch: 5| Step: 8
Training loss: 2.1600339554837498
Validation loss: 2.462167708895983

Epoch: 5| Step: 9
Training loss: 2.183587744743811
Validation loss: 2.467922554900022

Epoch: 5| Step: 10
Training loss: 1.7097184528330283
Validation loss: 2.4411216203712183

Epoch: 5| Step: 11
Training loss: 2.6474635491099714
Validation loss: 2.471687078747499

Epoch: 345| Step: 0
Training loss: 2.059560820049841
Validation loss: 2.4870516155668345

Epoch: 5| Step: 1
Training loss: 1.990750501380406
Validation loss: 2.486284853430591

Epoch: 5| Step: 2
Training loss: 2.2443411980646077
Validation loss: 2.516514668206089

Epoch: 5| Step: 3
Training loss: 2.523593012030397
Validation loss: 2.5478305623271154

Epoch: 5| Step: 4
Training loss: 2.1674132039116305
Validation loss: 2.5732207807100473

Epoch: 5| Step: 5
Training loss: 2.2840535951462453
Validation loss: 2.5702450095053635

Epoch: 5| Step: 6
Training loss: 1.8989633844349934
Validation loss: 2.61345944179599

Epoch: 5| Step: 7
Training loss: 2.1427927666031286
Validation loss: 2.611112823017948

Epoch: 5| Step: 8
Training loss: 1.8763741861679226
Validation loss: 2.5578565549852956

Epoch: 5| Step: 9
Training loss: 2.0468355131344507
Validation loss: 2.581887553300015

Epoch: 5| Step: 10
Training loss: 1.6897713951249715
Validation loss: 2.5933453615758726

Epoch: 5| Step: 11
Training loss: 2.928812206485239
Validation loss: 2.6036142348216003

Epoch: 346| Step: 0
Training loss: 2.049620556023855
Validation loss: 2.6269646361901215

Epoch: 5| Step: 1
Training loss: 2.232235824828758
Validation loss: 2.657823033268034

Epoch: 5| Step: 2
Training loss: 2.5566446349460055
Validation loss: 2.63166703040169

Epoch: 5| Step: 3
Training loss: 2.076820021161938
Validation loss: 2.632900640293306

Epoch: 5| Step: 4
Training loss: 1.857386575678313
Validation loss: 2.6039087104194403

Epoch: 5| Step: 5
Training loss: 1.5988123658328819
Validation loss: 2.5843524127728097

Epoch: 5| Step: 6
Training loss: 2.12042529284726
Validation loss: 2.551917866124068

Epoch: 5| Step: 7
Training loss: 1.9281552057357847
Validation loss: 2.541168481958194

Epoch: 5| Step: 8
Training loss: 2.1045302527009895
Validation loss: 2.552856710898448

Epoch: 5| Step: 9
Training loss: 2.7928191885217686
Validation loss: 2.5598557405381714

Epoch: 5| Step: 10
Training loss: 1.9750229339233798
Validation loss: 2.5443564172495794

Epoch: 5| Step: 11
Training loss: 1.749839298499122
Validation loss: 2.569810402787316

Epoch: 347| Step: 0
Training loss: 2.2695180134214907
Validation loss: 2.5763387638047477

Epoch: 5| Step: 1
Training loss: 1.803485750971748
Validation loss: 2.5927821540566267

Epoch: 5| Step: 2
Training loss: 2.712521752476819
Validation loss: 2.5693072369745145

Epoch: 5| Step: 3
Training loss: 1.2836521704225146
Validation loss: 2.596756900377263

Epoch: 5| Step: 4
Training loss: 2.391039245219571
Validation loss: 2.5660603414491243

Epoch: 5| Step: 5
Training loss: 2.1625700393536076
Validation loss: 2.6083940691371623

Epoch: 5| Step: 6
Training loss: 1.8933396829691156
Validation loss: 2.6283980261390822

Epoch: 5| Step: 7
Training loss: 1.842163243671977
Validation loss: 2.6122245609912635

Epoch: 5| Step: 8
Training loss: 2.4220252205805823
Validation loss: 2.5789375411905575

Epoch: 5| Step: 9
Training loss: 1.6830927906715358
Validation loss: 2.5909226108930388

Epoch: 5| Step: 10
Training loss: 2.0251902643936024
Validation loss: 2.57450557918738

Epoch: 5| Step: 11
Training loss: 1.3846379531952868
Validation loss: 2.56647244024055

Epoch: 348| Step: 0
Training loss: 2.1947713707660035
Validation loss: 2.5444453669917237

Epoch: 5| Step: 1
Training loss: 1.8699671911737608
Validation loss: 2.5855560693703676

Epoch: 5| Step: 2
Training loss: 1.6166089630971492
Validation loss: 2.613872573316055

Epoch: 5| Step: 3
Training loss: 2.049573560881293
Validation loss: 2.627423772047929

Epoch: 5| Step: 4
Training loss: 2.2043460140804756
Validation loss: 2.640346015774342

Epoch: 5| Step: 5
Training loss: 2.451674500259896
Validation loss: 2.656627673058782

Epoch: 5| Step: 6
Training loss: 2.1533872978880995
Validation loss: 2.6536765029727163

Epoch: 5| Step: 7
Training loss: 1.764263742353106
Validation loss: 2.6647721032589695

Epoch: 5| Step: 8
Training loss: 1.7377592181588393
Validation loss: 2.6525414364628603

Epoch: 5| Step: 9
Training loss: 1.728955504917703
Validation loss: 2.6203991063757335

Epoch: 5| Step: 10
Training loss: 2.305642037001691
Validation loss: 2.5903761418619924

Epoch: 5| Step: 11
Training loss: 3.1240122950821143
Validation loss: 2.5817834886265487

Epoch: 349| Step: 0
Training loss: 2.2710305807791817
Validation loss: 2.562320695201854

Epoch: 5| Step: 1
Training loss: 1.6738550457742498
Validation loss: 2.565849123732011

Epoch: 5| Step: 2
Training loss: 2.2543491080510267
Validation loss: 2.550423250882153

Epoch: 5| Step: 3
Training loss: 2.0035143017183437
Validation loss: 2.5477002645896065

Epoch: 5| Step: 4
Training loss: 2.0815032804867846
Validation loss: 2.56549511300757

Epoch: 5| Step: 5
Training loss: 1.4502692926205685
Validation loss: 2.586009546065726

Epoch: 5| Step: 6
Training loss: 2.0105746376814126
Validation loss: 2.5845627025814215

Epoch: 5| Step: 7
Training loss: 1.9656638525199357
Validation loss: 2.5853319140879254

Epoch: 5| Step: 8
Training loss: 1.9893384719548748
Validation loss: 2.59109824566617

Epoch: 5| Step: 9
Training loss: 2.5591492493944683
Validation loss: 2.6034988398963588

Epoch: 5| Step: 10
Training loss: 2.676241424418522
Validation loss: 2.6245369729889614

Epoch: 5| Step: 11
Training loss: 1.2552759408221013
Validation loss: 2.6513753723430344

Epoch: 350| Step: 0
Training loss: 1.7111807872298217
Validation loss: 2.6213430294651943

Epoch: 5| Step: 1
Training loss: 2.0947570656287247
Validation loss: 2.613047720030905

Epoch: 5| Step: 2
Training loss: 2.1661173295272627
Validation loss: 2.5929292161484483

Epoch: 5| Step: 3
Training loss: 1.9025789777325959
Validation loss: 2.565988368246108

Epoch: 5| Step: 4
Training loss: 2.7748156065292044
Validation loss: 2.5616685441055154

Epoch: 5| Step: 5
Training loss: 2.135394200152554
Validation loss: 2.53960099794369

Epoch: 5| Step: 6
Training loss: 1.4792037027942946
Validation loss: 2.551819146683583

Epoch: 5| Step: 7
Training loss: 2.3589608542641574
Validation loss: 2.559605576496535

Epoch: 5| Step: 8
Training loss: 2.1184510686867237
Validation loss: 2.5484119957404503

Epoch: 5| Step: 9
Training loss: 2.3005069381320715
Validation loss: 2.56263811623127

Epoch: 5| Step: 10
Training loss: 1.8587134330349069
Validation loss: 2.590755802621061

Epoch: 5| Step: 11
Training loss: 2.0043192952726727
Validation loss: 2.616720957000483

Testing loss: 2.1858313952596164

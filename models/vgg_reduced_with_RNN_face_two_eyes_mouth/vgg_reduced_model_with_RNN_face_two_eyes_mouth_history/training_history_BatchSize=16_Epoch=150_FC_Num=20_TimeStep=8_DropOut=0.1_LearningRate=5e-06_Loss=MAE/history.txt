Epoch: 1| Step: 0
Training loss: 5.424560070037842
Validation loss: 5.267387628555298

Epoch: 6| Step: 1
Training loss: 5.90330171585083
Validation loss: 5.265099207560222

Epoch: 6| Step: 2
Training loss: 5.694924831390381
Validation loss: 5.262941598892212

Epoch: 6| Step: 3
Training loss: 5.237274169921875
Validation loss: 5.2607212861378985

Epoch: 6| Step: 4
Training loss: 5.116089820861816
Validation loss: 5.258651256561279

Epoch: 6| Step: 5
Training loss: 4.669863700866699
Validation loss: 5.256529808044434

Epoch: 6| Step: 6
Training loss: 6.0724196434021
Validation loss: 5.254412412643433

Epoch: 6| Step: 7
Training loss: 5.603193283081055
Validation loss: 5.252257426579793

Epoch: 6| Step: 8
Training loss: 4.588142395019531
Validation loss: 5.24997607866923

Epoch: 6| Step: 9
Training loss: 4.852182388305664
Validation loss: 5.247746189435323

Epoch: 6| Step: 10
Training loss: 5.091804504394531
Validation loss: 5.245314757029216

Epoch: 6| Step: 11
Training loss: 5.033320426940918
Validation loss: 5.242930014928182

Epoch: 6| Step: 12
Training loss: 5.407158851623535
Validation loss: 5.2404085000356035

Epoch: 6| Step: 13
Training loss: 5.819236755371094
Validation loss: 5.237856149673462

Epoch: 2| Step: 0
Training loss: 6.058823585510254
Validation loss: 5.235089540481567

Epoch: 6| Step: 1
Training loss: 4.394082069396973
Validation loss: 5.232348124186198

Epoch: 6| Step: 2
Training loss: 5.395551681518555
Validation loss: 5.229332685470581

Epoch: 6| Step: 3
Training loss: 4.509889602661133
Validation loss: 5.226283232371013

Epoch: 6| Step: 4
Training loss: 5.5402631759643555
Validation loss: 5.223029136657715

Epoch: 6| Step: 5
Training loss: 5.722659111022949
Validation loss: 5.2194623947143555

Epoch: 6| Step: 6
Training loss: 4.469611644744873
Validation loss: 5.215860843658447

Epoch: 6| Step: 7
Training loss: 5.684854984283447
Validation loss: 5.212104558944702

Epoch: 6| Step: 8
Training loss: 5.9786553382873535
Validation loss: 5.208029826482137

Epoch: 6| Step: 9
Training loss: 5.456223011016846
Validation loss: 5.203787883122762

Epoch: 6| Step: 10
Training loss: 4.597199440002441
Validation loss: 5.199289083480835

Epoch: 6| Step: 11
Training loss: 5.431581497192383
Validation loss: 5.194428364435832

Epoch: 6| Step: 12
Training loss: 4.640430450439453
Validation loss: 5.189580837885539

Epoch: 6| Step: 13
Training loss: 6.071342468261719
Validation loss: 5.184278964996338

Epoch: 3| Step: 0
Training loss: 4.970206260681152
Validation loss: 5.178710699081421

Epoch: 6| Step: 1
Training loss: 4.788630962371826
Validation loss: 5.172617117563884

Epoch: 6| Step: 2
Training loss: 4.355949878692627
Validation loss: 5.1662187576293945

Epoch: 6| Step: 3
Training loss: 6.308557510375977
Validation loss: 5.159662246704102

Epoch: 6| Step: 4
Training loss: 4.507823944091797
Validation loss: 5.152506351470947

Epoch: 6| Step: 5
Training loss: 4.946619033813477
Validation loss: 5.144823392232259

Epoch: 6| Step: 6
Training loss: 5.515017986297607
Validation loss: 5.137180725733439

Epoch: 6| Step: 7
Training loss: 5.6209869384765625
Validation loss: 5.128610531489055

Epoch: 6| Step: 8
Training loss: 4.5666069984436035
Validation loss: 5.120093663533528

Epoch: 6| Step: 9
Training loss: 5.752776145935059
Validation loss: 5.110571543375651

Epoch: 6| Step: 10
Training loss: 5.433293342590332
Validation loss: 5.101088047027588

Epoch: 6| Step: 11
Training loss: 5.690310478210449
Validation loss: 5.090732018152873

Epoch: 6| Step: 12
Training loss: 4.933261871337891
Validation loss: 5.080080032348633

Epoch: 6| Step: 13
Training loss: 5.426652908325195
Validation loss: 5.068450530370076

Epoch: 4| Step: 0
Training loss: 4.095879077911377
Validation loss: 5.056936025619507

Epoch: 6| Step: 1
Training loss: 5.255388259887695
Validation loss: 5.045036872227986

Epoch: 6| Step: 2
Training loss: 4.434965133666992
Validation loss: 5.0324076016743975

Epoch: 6| Step: 3
Training loss: 4.8127923011779785
Validation loss: 5.019469102223714

Epoch: 6| Step: 4
Training loss: 3.895604133605957
Validation loss: 5.006337722142537

Epoch: 6| Step: 5
Training loss: 6.1569294929504395
Validation loss: 4.992604653040568

Epoch: 6| Step: 6
Training loss: 5.870366096496582
Validation loss: 4.978169679641724

Epoch: 6| Step: 7
Training loss: 5.689694404602051
Validation loss: 4.963308374087016

Epoch: 6| Step: 8
Training loss: 5.696267604827881
Validation loss: 4.948860963185628

Epoch: 6| Step: 9
Training loss: 4.473613739013672
Validation loss: 4.933099428812663

Epoch: 6| Step: 10
Training loss: 4.832767486572266
Validation loss: 4.917916456858317

Epoch: 6| Step: 11
Training loss: 5.754203796386719
Validation loss: 4.901849587758382

Epoch: 6| Step: 12
Training loss: 4.195545673370361
Validation loss: 4.8863504727681475

Epoch: 6| Step: 13
Training loss: 5.4808430671691895
Validation loss: 4.870899200439453

Epoch: 5| Step: 0
Training loss: 5.079137802124023
Validation loss: 4.855697154998779

Epoch: 6| Step: 1
Training loss: 4.34658145904541
Validation loss: 4.840358654658

Epoch: 6| Step: 2
Training loss: 4.376650810241699
Validation loss: 4.825422088305156

Epoch: 6| Step: 3
Training loss: 5.13527774810791
Validation loss: 4.810078223546346

Epoch: 6| Step: 4
Training loss: 3.8995280265808105
Validation loss: 4.794693390528361

Epoch: 6| Step: 5
Training loss: 5.000341415405273
Validation loss: 4.7799248695373535

Epoch: 6| Step: 6
Training loss: 4.2371134757995605
Validation loss: 4.7651158173878985

Epoch: 6| Step: 7
Training loss: 5.45387601852417
Validation loss: 4.750703573226929

Epoch: 6| Step: 8
Training loss: 4.6445136070251465
Validation loss: 4.7357213497161865

Epoch: 6| Step: 9
Training loss: 4.363683700561523
Validation loss: 4.721700986226399

Epoch: 6| Step: 10
Training loss: 6.35503625869751
Validation loss: 4.708110809326172

Epoch: 6| Step: 11
Training loss: 5.909393310546875
Validation loss: 4.69471001625061

Epoch: 6| Step: 12
Training loss: 4.203474044799805
Validation loss: 4.6817013422648115

Epoch: 6| Step: 13
Training loss: 4.834742069244385
Validation loss: 4.6686844031016035

Epoch: 6| Step: 0
Training loss: 5.472870349884033
Validation loss: 4.655657370885213

Epoch: 6| Step: 1
Training loss: 4.839948654174805
Validation loss: 4.643125017484029

Epoch: 6| Step: 2
Training loss: 4.936165809631348
Validation loss: 4.631006240844727

Epoch: 6| Step: 3
Training loss: 5.046116352081299
Validation loss: 4.619278987248738

Epoch: 6| Step: 4
Training loss: 2.840070962905884
Validation loss: 4.607907931009929

Epoch: 6| Step: 5
Training loss: 4.834851264953613
Validation loss: 4.596440394719441

Epoch: 6| Step: 6
Training loss: 4.599211692810059
Validation loss: 4.58567754427592

Epoch: 6| Step: 7
Training loss: 4.993900299072266
Validation loss: 4.575064182281494

Epoch: 6| Step: 8
Training loss: 4.730066299438477
Validation loss: 4.564266482988994

Epoch: 6| Step: 9
Training loss: 4.473631858825684
Validation loss: 4.554066340128581

Epoch: 6| Step: 10
Training loss: 3.976762294769287
Validation loss: 4.543844183286031

Epoch: 6| Step: 11
Training loss: 4.932448387145996
Validation loss: 4.533611297607422

Epoch: 6| Step: 12
Training loss: 5.244484901428223
Validation loss: 4.523799618085225

Epoch: 6| Step: 13
Training loss: 4.647149085998535
Validation loss: 4.514066378275554

Epoch: 7| Step: 0
Training loss: 4.554121494293213
Validation loss: 4.504228035608928

Epoch: 6| Step: 1
Training loss: 5.334253311157227
Validation loss: 4.494792143503825

Epoch: 6| Step: 2
Training loss: 4.445027828216553
Validation loss: 4.48528258005778

Epoch: 6| Step: 3
Training loss: 3.8066763877868652
Validation loss: 4.47659166653951

Epoch: 6| Step: 4
Training loss: 4.645823955535889
Validation loss: 4.468111435572307

Epoch: 6| Step: 5
Training loss: 4.324464321136475
Validation loss: 4.459766109784444

Epoch: 6| Step: 6
Training loss: 4.453136444091797
Validation loss: 4.451345205307007

Epoch: 6| Step: 7
Training loss: 5.743929862976074
Validation loss: 4.443451642990112

Epoch: 6| Step: 8
Training loss: 4.595458984375
Validation loss: 4.436013857523601

Epoch: 6| Step: 9
Training loss: 4.426791191101074
Validation loss: 4.4284688631693525

Epoch: 6| Step: 10
Training loss: 3.261033535003662
Validation loss: 4.420547525087993

Epoch: 6| Step: 11
Training loss: 4.778806686401367
Validation loss: 4.413434584935506

Epoch: 6| Step: 12
Training loss: 4.693642616271973
Validation loss: 4.406230092048645

Epoch: 6| Step: 13
Training loss: 4.77861213684082
Validation loss: 4.398755073547363

Epoch: 8| Step: 0
Training loss: 3.7399561405181885
Validation loss: 4.391581376393636

Epoch: 6| Step: 1
Training loss: 5.278586387634277
Validation loss: 4.384104569753011

Epoch: 6| Step: 2
Training loss: 3.4174089431762695
Validation loss: 4.376953323682149

Epoch: 6| Step: 3
Training loss: 4.005258083343506
Validation loss: 4.369779586791992

Epoch: 6| Step: 4
Training loss: 5.467649459838867
Validation loss: 4.3629140456517534

Epoch: 6| Step: 5
Training loss: 4.470448017120361
Validation loss: 4.3559867938359575

Epoch: 6| Step: 6
Training loss: 4.582136154174805
Validation loss: 4.34915812810262

Epoch: 6| Step: 7
Training loss: 4.504454135894775
Validation loss: 4.34175976117452

Epoch: 6| Step: 8
Training loss: 4.653353691101074
Validation loss: 4.334738254547119

Epoch: 6| Step: 9
Training loss: 4.322553634643555
Validation loss: 4.327809572219849

Epoch: 6| Step: 10
Training loss: 4.620971202850342
Validation loss: 4.320629835128784

Epoch: 6| Step: 11
Training loss: 4.4833831787109375
Validation loss: 4.314372181892395

Epoch: 6| Step: 12
Training loss: 4.499397277832031
Validation loss: 4.307383179664612

Epoch: 6| Step: 13
Training loss: 4.485408306121826
Validation loss: 4.300455570220947

Epoch: 9| Step: 0
Training loss: 4.3744797706604
Validation loss: 4.293151577313741

Epoch: 6| Step: 1
Training loss: 3.923426866531372
Validation loss: 4.285828987757365

Epoch: 6| Step: 2
Training loss: 4.772770404815674
Validation loss: 4.279112656911214

Epoch: 6| Step: 3
Training loss: 4.890927791595459
Validation loss: 4.27213982741038

Epoch: 6| Step: 4
Training loss: 4.146580696105957
Validation loss: 4.2645416259765625

Epoch: 6| Step: 5
Training loss: 3.753105640411377
Validation loss: 4.257473270098369

Epoch: 6| Step: 6
Training loss: 3.762497901916504
Validation loss: 4.251134435335795

Epoch: 6| Step: 7
Training loss: 4.959092617034912
Validation loss: 4.244549194971721

Epoch: 6| Step: 8
Training loss: 4.066567897796631
Validation loss: 4.23719334602356

Epoch: 6| Step: 9
Training loss: 4.477497577667236
Validation loss: 4.230548580487569

Epoch: 6| Step: 10
Training loss: 5.229718208312988
Validation loss: 4.223584493001302

Epoch: 6| Step: 11
Training loss: 4.686354637145996
Validation loss: 4.217427810033162

Epoch: 6| Step: 12
Training loss: 4.223626136779785
Validation loss: 4.210901498794556

Epoch: 6| Step: 13
Training loss: 4.019525527954102
Validation loss: 4.203486442565918

Epoch: 10| Step: 0
Training loss: 4.58112096786499
Validation loss: 4.196905851364136

Epoch: 6| Step: 1
Training loss: 3.706913948059082
Validation loss: 4.19031838575999

Epoch: 6| Step: 2
Training loss: 5.3134989738464355
Validation loss: 4.183802962303162

Epoch: 6| Step: 3
Training loss: 4.267337799072266
Validation loss: 4.1773509581883745

Epoch: 6| Step: 4
Training loss: 4.764756202697754
Validation loss: 4.170353730519612

Epoch: 6| Step: 5
Training loss: 3.1689789295196533
Validation loss: 4.163792570432027

Epoch: 6| Step: 6
Training loss: 4.443779945373535
Validation loss: 4.157689015070598

Epoch: 6| Step: 7
Training loss: 3.953883171081543
Validation loss: 4.1514662106831866

Epoch: 6| Step: 8
Training loss: 4.455167293548584
Validation loss: 4.145237843195598

Epoch: 6| Step: 9
Training loss: 4.550479888916016
Validation loss: 4.139265656471252

Epoch: 6| Step: 10
Training loss: 4.6147003173828125
Validation loss: 4.133345524470012

Epoch: 6| Step: 11
Training loss: 4.037937164306641
Validation loss: 4.127272804578145

Epoch: 6| Step: 12
Training loss: 4.413573741912842
Validation loss: 4.120866060256958

Epoch: 6| Step: 13
Training loss: 3.8540101051330566
Validation loss: 4.11504860719045

Epoch: 11| Step: 0
Training loss: 3.9946696758270264
Validation loss: 4.1091554164886475

Epoch: 6| Step: 1
Training loss: 4.689188003540039
Validation loss: 4.10404634475708

Epoch: 6| Step: 2
Training loss: 4.6035308837890625
Validation loss: 4.097721497217814

Epoch: 6| Step: 3
Training loss: 3.758558511734009
Validation loss: 4.091743111610413

Epoch: 6| Step: 4
Training loss: 4.252623558044434
Validation loss: 4.085801879564921

Epoch: 6| Step: 5
Training loss: 4.080362796783447
Validation loss: 4.079683581988017

Epoch: 6| Step: 6
Training loss: 4.172536373138428
Validation loss: 4.073714852333069

Epoch: 6| Step: 7
Training loss: 4.171261310577393
Validation loss: 4.067902008692424

Epoch: 6| Step: 8
Training loss: 4.790653228759766
Validation loss: 4.061688383420308

Epoch: 6| Step: 9
Training loss: 3.7137551307678223
Validation loss: 4.054910659790039

Epoch: 6| Step: 10
Training loss: 3.9836220741271973
Validation loss: 4.049300193786621

Epoch: 6| Step: 11
Training loss: 3.9316294193267822
Validation loss: 4.04393454392751

Epoch: 6| Step: 12
Training loss: 4.360607147216797
Validation loss: 4.0383074680964155

Epoch: 6| Step: 13
Training loss: 4.500776290893555
Validation loss: 4.032721082369487

Epoch: 12| Step: 0
Training loss: 3.650364637374878
Validation loss: 4.026975591977437

Epoch: 6| Step: 1
Training loss: 3.824537754058838
Validation loss: 4.021745880444844

Epoch: 6| Step: 2
Training loss: 3.4908132553100586
Validation loss: 4.015481909116109

Epoch: 6| Step: 3
Training loss: 3.283585548400879
Validation loss: 4.009716073671977

Epoch: 6| Step: 4
Training loss: 4.814765930175781
Validation loss: 4.003853718439738

Epoch: 6| Step: 5
Training loss: 5.300490856170654
Validation loss: 3.9986661275227866

Epoch: 6| Step: 6
Training loss: 3.7875771522521973
Validation loss: 3.9928821325302124

Epoch: 6| Step: 7
Training loss: 5.0492658615112305
Validation loss: 3.987226923306783

Epoch: 6| Step: 8
Training loss: 3.3636341094970703
Validation loss: 3.9809772968292236

Epoch: 6| Step: 9
Training loss: 4.414310455322266
Validation loss: 3.9756601254145303

Epoch: 6| Step: 10
Training loss: 4.658165454864502
Validation loss: 3.9697993993759155

Epoch: 6| Step: 11
Training loss: 4.889891624450684
Validation loss: 3.9633302291234336

Epoch: 6| Step: 12
Training loss: 3.764659881591797
Validation loss: 3.9577221473058066

Epoch: 6| Step: 13
Training loss: 3.616147041320801
Validation loss: 3.952133059501648

Epoch: 13| Step: 0
Training loss: 3.276484489440918
Validation loss: 3.9460962216059365

Epoch: 6| Step: 1
Training loss: 3.828662633895874
Validation loss: 3.941201686859131

Epoch: 6| Step: 2
Training loss: 3.266847610473633
Validation loss: 3.9359899361928306

Epoch: 6| Step: 3
Training loss: 3.9918994903564453
Validation loss: 3.9310492674509683

Epoch: 6| Step: 4
Training loss: 4.477231979370117
Validation loss: 3.9257307052612305

Epoch: 6| Step: 5
Training loss: 3.9762861728668213
Validation loss: 3.9194725354512534

Epoch: 6| Step: 6
Training loss: 4.296972274780273
Validation loss: 3.9135677019755044

Epoch: 6| Step: 7
Training loss: 2.968963623046875
Validation loss: 3.908673644065857

Epoch: 6| Step: 8
Training loss: 4.11021614074707
Validation loss: 3.9027844270070395

Epoch: 6| Step: 9
Training loss: 5.523165702819824
Validation loss: 3.896794239679972

Epoch: 6| Step: 10
Training loss: 5.191699028015137
Validation loss: 3.8915918668111167

Epoch: 6| Step: 11
Training loss: 4.213006496429443
Validation loss: 3.885186791419983

Epoch: 6| Step: 12
Training loss: 3.4330997467041016
Validation loss: 3.8794661362965903

Epoch: 6| Step: 13
Training loss: 4.271160125732422
Validation loss: 3.874220689137777

Epoch: 14| Step: 0
Training loss: 4.207330703735352
Validation loss: 3.8682939608891806

Epoch: 6| Step: 1
Training loss: 4.167962074279785
Validation loss: 3.8625258604685464

Epoch: 6| Step: 2
Training loss: 3.2796630859375
Validation loss: 3.857142686843872

Epoch: 6| Step: 3
Training loss: 3.5473716259002686
Validation loss: 3.851213971773783

Epoch: 6| Step: 4
Training loss: 4.5015130043029785
Validation loss: 3.845861275990804

Epoch: 6| Step: 5
Training loss: 4.667969703674316
Validation loss: 3.8410704930623374

Epoch: 6| Step: 6
Training loss: 3.8228554725646973
Validation loss: 3.8350038528442383

Epoch: 6| Step: 7
Training loss: 3.675978660583496
Validation loss: 3.829796473185221

Epoch: 6| Step: 8
Training loss: 3.6084911823272705
Validation loss: 3.8247437874476113

Epoch: 6| Step: 9
Training loss: 3.2777204513549805
Validation loss: 3.819711764653524

Epoch: 6| Step: 10
Training loss: 3.5858540534973145
Validation loss: 3.8138105869293213

Epoch: 6| Step: 11
Training loss: 4.61381721496582
Validation loss: 3.8079659144083657

Epoch: 6| Step: 12
Training loss: 4.343099594116211
Validation loss: 3.8026404778162637

Epoch: 6| Step: 13
Training loss: 4.485212802886963
Validation loss: 3.7977646589279175

Epoch: 15| Step: 0
Training loss: 4.459022521972656
Validation loss: 3.7920371691385903

Epoch: 6| Step: 1
Training loss: 4.407194137573242
Validation loss: 3.7857830127080283

Epoch: 6| Step: 2
Training loss: 3.3138601779937744
Validation loss: 3.780571381251017

Epoch: 6| Step: 3
Training loss: 3.5071003437042236
Validation loss: 3.7766581773757935

Epoch: 6| Step: 4
Training loss: 3.908759355545044
Validation loss: 3.771839181582133

Epoch: 6| Step: 5
Training loss: 3.315633773803711
Validation loss: 3.7660842736562095

Epoch: 6| Step: 6
Training loss: 3.592043399810791
Validation loss: 3.7592782179514566

Epoch: 6| Step: 7
Training loss: 4.001437187194824
Validation loss: 3.7549588680267334

Epoch: 6| Step: 8
Training loss: 4.122542381286621
Validation loss: 3.7481295665105185

Epoch: 6| Step: 9
Training loss: 4.28794002532959
Validation loss: 3.7431538105010986

Epoch: 6| Step: 10
Training loss: 4.426396369934082
Validation loss: 3.7373289267222085

Epoch: 6| Step: 11
Training loss: 4.194299221038818
Validation loss: 3.7317437330881753

Epoch: 6| Step: 12
Training loss: 4.173689842224121
Validation loss: 3.7262340784072876

Epoch: 6| Step: 13
Training loss: 3.0854506492614746
Validation loss: 3.7207350730895996

Epoch: 16| Step: 0
Training loss: 3.6627750396728516
Validation loss: 3.715719779332479

Epoch: 6| Step: 1
Training loss: 3.7705821990966797
Validation loss: 3.710517724355062

Epoch: 6| Step: 2
Training loss: 3.349997043609619
Validation loss: 3.7060387134552

Epoch: 6| Step: 3
Training loss: 3.181893825531006
Validation loss: 3.7014432350794473

Epoch: 6| Step: 4
Training loss: 4.317132949829102
Validation loss: 3.697447498639425

Epoch: 6| Step: 5
Training loss: 3.853973627090454
Validation loss: 3.6932100852330527

Epoch: 6| Step: 6
Training loss: 3.615037202835083
Validation loss: 3.6881592671076455

Epoch: 6| Step: 7
Training loss: 4.500967502593994
Validation loss: 3.6825962464014688

Epoch: 6| Step: 8
Training loss: 4.747045993804932
Validation loss: 3.6799046993255615

Epoch: 6| Step: 9
Training loss: 5.156542778015137
Validation loss: 3.673486351966858

Epoch: 6| Step: 10
Training loss: 3.25571346282959
Validation loss: 3.6687146027882895

Epoch: 6| Step: 11
Training loss: 3.6549131870269775
Validation loss: 3.6635326147079468

Epoch: 6| Step: 12
Training loss: 3.5309174060821533
Validation loss: 3.6590745051701865

Epoch: 6| Step: 13
Training loss: 3.22037410736084
Validation loss: 3.654194196065267

Epoch: 17| Step: 0
Training loss: 4.042001247406006
Validation loss: 3.649465282758077

Epoch: 6| Step: 1
Training loss: 3.650920867919922
Validation loss: 3.6444801489512124

Epoch: 6| Step: 2
Training loss: 4.4797163009643555
Validation loss: 3.6398106813430786

Epoch: 6| Step: 3
Training loss: 4.257434844970703
Validation loss: 3.634211301803589

Epoch: 6| Step: 4
Training loss: 4.094977378845215
Validation loss: 3.629464944203695

Epoch: 6| Step: 5
Training loss: 3.6505703926086426
Validation loss: 3.624858299891154

Epoch: 6| Step: 6
Training loss: 3.333245277404785
Validation loss: 3.621151328086853

Epoch: 6| Step: 7
Training loss: 2.9968338012695312
Validation loss: 3.6157713333765664

Epoch: 6| Step: 8
Training loss: 4.729986190795898
Validation loss: 3.612529158592224

Epoch: 6| Step: 9
Training loss: 4.919038772583008
Validation loss: 3.6061423619588218

Epoch: 6| Step: 10
Training loss: 3.335484504699707
Validation loss: 3.600271304448446

Epoch: 6| Step: 11
Training loss: 3.007345199584961
Validation loss: 3.5954856872558594

Epoch: 6| Step: 12
Training loss: 2.9159531593322754
Validation loss: 3.590191682179769

Epoch: 6| Step: 13
Training loss: 3.5059611797332764
Validation loss: 3.5864622990290322

Epoch: 18| Step: 0
Training loss: 2.72271728515625
Validation loss: 3.5813514788945517

Epoch: 6| Step: 1
Training loss: 3.7754135131835938
Validation loss: 3.5754810174306235

Epoch: 6| Step: 2
Training loss: 3.5125393867492676
Validation loss: 3.5702101389567056

Epoch: 6| Step: 3
Training loss: 3.8739869594573975
Validation loss: 3.5643357833226523

Epoch: 6| Step: 4
Training loss: 4.3975510597229
Validation loss: 3.5598497788111367

Epoch: 6| Step: 5
Training loss: 3.591904401779175
Validation loss: 3.554338296254476

Epoch: 6| Step: 6
Training loss: 3.8851654529571533
Validation loss: 3.5499010483423867

Epoch: 6| Step: 7
Training loss: 3.1151680946350098
Validation loss: 3.5461488167444863

Epoch: 6| Step: 8
Training loss: 3.900747776031494
Validation loss: 3.541422406832377

Epoch: 6| Step: 9
Training loss: 3.89786696434021
Validation loss: 3.5346500078837075

Epoch: 6| Step: 10
Training loss: 3.4206109046936035
Validation loss: 3.5298650662104287

Epoch: 6| Step: 11
Training loss: 4.070084571838379
Validation loss: 3.5251949230829873

Epoch: 6| Step: 12
Training loss: 4.013530254364014
Validation loss: 3.5229421059290567

Epoch: 6| Step: 13
Training loss: 3.812608242034912
Validation loss: 3.5176000595092773

Epoch: 19| Step: 0
Training loss: 3.7338056564331055
Validation loss: 3.511988560358683

Epoch: 6| Step: 1
Training loss: 3.9520862102508545
Validation loss: 3.506229122479757

Epoch: 6| Step: 2
Training loss: 3.5357444286346436
Validation loss: 3.500435471534729

Epoch: 6| Step: 3
Training loss: 3.748149871826172
Validation loss: 3.4941155910491943

Epoch: 6| Step: 4
Training loss: 3.463383197784424
Validation loss: 3.488492568333944

Epoch: 6| Step: 5
Training loss: 3.330883026123047
Validation loss: 3.4823539654413858

Epoch: 6| Step: 6
Training loss: 3.912686824798584
Validation loss: 3.4751857121785483

Epoch: 6| Step: 7
Training loss: 3.6082682609558105
Validation loss: 3.4687304496765137

Epoch: 6| Step: 8
Training loss: 3.458723545074463
Validation loss: 3.463250517845154

Epoch: 6| Step: 9
Training loss: 3.561587333679199
Validation loss: 3.459774692853292

Epoch: 6| Step: 10
Training loss: 3.7385425567626953
Validation loss: 3.455923914909363

Epoch: 6| Step: 11
Training loss: 4.204298973083496
Validation loss: 3.4512285391489663

Epoch: 6| Step: 12
Training loss: 3.78763484954834
Validation loss: 3.4437065521876016

Epoch: 6| Step: 13
Training loss: 2.9446330070495605
Validation loss: 3.4370135068893433

Epoch: 20| Step: 0
Training loss: 3.630728006362915
Validation loss: 3.4310055573781333

Epoch: 6| Step: 1
Training loss: 2.904297351837158
Validation loss: 3.426164428393046

Epoch: 6| Step: 2
Training loss: 4.726082801818848
Validation loss: 3.420528769493103

Epoch: 6| Step: 3
Training loss: 3.61018705368042
Validation loss: 3.413273016611735

Epoch: 6| Step: 4
Training loss: 3.341362953186035
Validation loss: 3.4079796075820923

Epoch: 6| Step: 5
Training loss: 2.6946933269500732
Validation loss: 3.402732014656067

Epoch: 6| Step: 6
Training loss: 3.103996753692627
Validation loss: 3.397019306818644

Epoch: 6| Step: 7
Training loss: 3.3546440601348877
Validation loss: 3.3923411766688027

Epoch: 6| Step: 8
Training loss: 3.6019084453582764
Validation loss: 3.386689225832621

Epoch: 6| Step: 9
Training loss: 4.0638298988342285
Validation loss: 3.3803115288416543

Epoch: 6| Step: 10
Training loss: 3.326826333999634
Validation loss: 3.3741294542948403

Epoch: 6| Step: 11
Training loss: 4.173488616943359
Validation loss: 3.3688894510269165

Epoch: 6| Step: 12
Training loss: 3.5493924617767334
Validation loss: 3.363831321398417

Epoch: 6| Step: 13
Training loss: 3.8334579467773438
Validation loss: 3.359218875567118

Epoch: 21| Step: 0
Training loss: 3.2711727619171143
Validation loss: 3.354477326075236

Epoch: 6| Step: 1
Training loss: 2.4886672496795654
Validation loss: 3.349922299385071

Epoch: 6| Step: 2
Training loss: 3.3432364463806152
Validation loss: 3.344486196835836

Epoch: 6| Step: 3
Training loss: 4.126745700836182
Validation loss: 3.3400603930155435

Epoch: 6| Step: 4
Training loss: 3.5642824172973633
Validation loss: 3.334479053815206

Epoch: 6| Step: 5
Training loss: 3.4225101470947266
Validation loss: 3.3298300902048745

Epoch: 6| Step: 6
Training loss: 4.670842170715332
Validation loss: 3.3250913619995117

Epoch: 6| Step: 7
Training loss: 3.5012760162353516
Validation loss: 3.320634961128235

Epoch: 6| Step: 8
Training loss: 3.4781394004821777
Validation loss: 3.3155290285746255

Epoch: 6| Step: 9
Training loss: 3.895477056503296
Validation loss: 3.309507171312968

Epoch: 6| Step: 10
Training loss: 2.9759979248046875
Validation loss: 3.30591881275177

Epoch: 6| Step: 11
Training loss: 3.791294574737549
Validation loss: 3.3007582823435464

Epoch: 6| Step: 12
Training loss: 3.071587562561035
Validation loss: 3.2968696753184

Epoch: 6| Step: 13
Training loss: 3.355219841003418
Validation loss: 3.291885574658712

Epoch: 22| Step: 0
Training loss: 3.530499219894409
Validation loss: 3.2875104745229087

Epoch: 6| Step: 1
Training loss: 3.7653074264526367
Validation loss: 3.282851735750834

Epoch: 6| Step: 2
Training loss: 2.5322141647338867
Validation loss: 3.278486410776774

Epoch: 6| Step: 3
Training loss: 3.7179880142211914
Validation loss: 3.2740535338719687

Epoch: 6| Step: 4
Training loss: 3.6577517986297607
Validation loss: 3.269842028617859

Epoch: 6| Step: 5
Training loss: 4.324072360992432
Validation loss: 3.2648701667785645

Epoch: 6| Step: 6
Training loss: 2.5746359825134277
Validation loss: 3.26048477490743

Epoch: 6| Step: 7
Training loss: 3.12212872505188
Validation loss: 3.2565420071283975

Epoch: 6| Step: 8
Training loss: 3.9418485164642334
Validation loss: 3.2520158688227334

Epoch: 6| Step: 9
Training loss: 3.067016839981079
Validation loss: 3.24729331334432

Epoch: 6| Step: 10
Training loss: 3.041541337966919
Validation loss: 3.2432302236557007

Epoch: 6| Step: 11
Training loss: 3.1706314086914062
Validation loss: 3.238730708758036

Epoch: 6| Step: 12
Training loss: 4.261528491973877
Validation loss: 3.2357587019602456

Epoch: 6| Step: 13
Training loss: 3.3966288566589355
Validation loss: 3.2312641541163125

Epoch: 23| Step: 0
Training loss: 2.5989561080932617
Validation loss: 3.2271130879720054

Epoch: 6| Step: 1
Training loss: 3.299867868423462
Validation loss: 3.2227848768234253

Epoch: 6| Step: 2
Training loss: 3.5585551261901855
Validation loss: 3.217201828956604

Epoch: 6| Step: 3
Training loss: 3.8944573402404785
Validation loss: 3.2133915026982627

Epoch: 6| Step: 4
Training loss: 3.283111572265625
Validation loss: 3.208904504776001

Epoch: 6| Step: 5
Training loss: 4.051603317260742
Validation loss: 3.204493602116903

Epoch: 6| Step: 6
Training loss: 2.096492052078247
Validation loss: 3.2003289063771567

Epoch: 6| Step: 7
Training loss: 3.113616704940796
Validation loss: 3.1960432529449463

Epoch: 6| Step: 8
Training loss: 4.129787921905518
Validation loss: 3.1910071770350137

Epoch: 6| Step: 9
Training loss: 3.6642189025878906
Validation loss: 3.1855881214141846

Epoch: 6| Step: 10
Training loss: 3.557802200317383
Validation loss: 3.181513706843058

Epoch: 6| Step: 11
Training loss: 3.7223587036132812
Validation loss: 3.1769859790802

Epoch: 6| Step: 12
Training loss: 3.2065839767456055
Validation loss: 3.173196872075399

Epoch: 6| Step: 13
Training loss: 3.1654086112976074
Validation loss: 3.170214136441549

Epoch: 24| Step: 0
Training loss: 3.979924201965332
Validation loss: 3.168215195337931

Epoch: 6| Step: 1
Training loss: 3.401360034942627
Validation loss: 3.1646178166071572

Epoch: 6| Step: 2
Training loss: 2.8730406761169434
Validation loss: 3.1569391886393228

Epoch: 6| Step: 3
Training loss: 4.085409641265869
Validation loss: 3.152701218922933

Epoch: 6| Step: 4
Training loss: 4.016833305358887
Validation loss: 3.148797591527303

Epoch: 6| Step: 5
Training loss: 3.358826160430908
Validation loss: 3.144524892171224

Epoch: 6| Step: 6
Training loss: 2.4000842571258545
Validation loss: 3.140046238899231

Epoch: 6| Step: 7
Training loss: 3.6090383529663086
Validation loss: 3.136691431204478

Epoch: 6| Step: 8
Training loss: 2.859335422515869
Validation loss: 3.1324853897094727

Epoch: 6| Step: 9
Training loss: 2.755110740661621
Validation loss: 3.1279566287994385

Epoch: 6| Step: 10
Training loss: 2.537899971008301
Validation loss: 3.123809814453125

Epoch: 6| Step: 11
Training loss: 3.277156114578247
Validation loss: 3.1195838848749795

Epoch: 6| Step: 12
Training loss: 4.24090576171875
Validation loss: 3.115881005922953

Epoch: 6| Step: 13
Training loss: 3.1506943702697754
Validation loss: 3.1111638148625693

Epoch: 25| Step: 0
Training loss: 2.9942874908447266
Validation loss: 3.1070982217788696

Epoch: 6| Step: 1
Training loss: 3.7688870429992676
Validation loss: 3.102938711643219

Epoch: 6| Step: 2
Training loss: 3.474522829055786
Validation loss: 3.098730484644572

Epoch: 6| Step: 3
Training loss: 3.4290971755981445
Validation loss: 3.0958553552627563

Epoch: 6| Step: 4
Training loss: 3.143749237060547
Validation loss: 3.093580484390259

Epoch: 6| Step: 5
Training loss: 3.3624887466430664
Validation loss: 3.0889148314793906

Epoch: 6| Step: 6
Training loss: 2.555650234222412
Validation loss: 3.085621396700541

Epoch: 6| Step: 7
Training loss: 3.777285099029541
Validation loss: 3.0809452533721924

Epoch: 6| Step: 8
Training loss: 2.8790156841278076
Validation loss: 3.0764638980229697

Epoch: 6| Step: 9
Training loss: 3.3852291107177734
Validation loss: 3.07279642422994

Epoch: 6| Step: 10
Training loss: 2.8786330223083496
Validation loss: 3.069607138633728

Epoch: 6| Step: 11
Training loss: 3.1328861713409424
Validation loss: 3.0661709109942117

Epoch: 6| Step: 12
Training loss: 3.3565993309020996
Validation loss: 3.0618362029393515

Epoch: 6| Step: 13
Training loss: 3.648003101348877
Validation loss: 3.058293104171753

Epoch: 26| Step: 0
Training loss: 3.6512818336486816
Validation loss: 3.055767218271891

Epoch: 6| Step: 1
Training loss: 3.446877956390381
Validation loss: 3.0519672632217407

Epoch: 6| Step: 2
Training loss: 2.5866994857788086
Validation loss: 3.0476985375086465

Epoch: 6| Step: 3
Training loss: 3.239415407180786
Validation loss: 3.043372313181559

Epoch: 6| Step: 4
Training loss: 3.5333244800567627
Validation loss: 3.0399795373280845

Epoch: 6| Step: 5
Training loss: 3.3877899646759033
Validation loss: 3.0369815826416016

Epoch: 6| Step: 6
Training loss: 2.9305436611175537
Validation loss: 3.03626811504364

Epoch: 6| Step: 7
Training loss: 2.59818172454834
Validation loss: 3.02912708123525

Epoch: 6| Step: 8
Training loss: 3.02181339263916
Validation loss: 3.0252739985783896

Epoch: 6| Step: 9
Training loss: 3.427361488342285
Validation loss: 3.0211515029271445

Epoch: 6| Step: 10
Training loss: 3.447993755340576
Validation loss: 3.018126686414083

Epoch: 6| Step: 11
Training loss: 2.616741180419922
Validation loss: 3.014658292134603

Epoch: 6| Step: 12
Training loss: 3.6955018043518066
Validation loss: 3.0110180974006653

Epoch: 6| Step: 13
Training loss: 3.5185940265655518
Validation loss: 3.007821957270304

Epoch: 27| Step: 0
Training loss: 2.7530598640441895
Validation loss: 3.003533720970154

Epoch: 6| Step: 1
Training loss: 3.4661521911621094
Validation loss: 2.9999040365219116

Epoch: 6| Step: 2
Training loss: 3.8937649726867676
Validation loss: 2.9959115386009216

Epoch: 6| Step: 3
Training loss: 3.6924314498901367
Validation loss: 2.993028481801351

Epoch: 6| Step: 4
Training loss: 2.738217353820801
Validation loss: 2.989217440287272

Epoch: 6| Step: 5
Training loss: 3.0450217723846436
Validation loss: 2.9850983222325644

Epoch: 6| Step: 6
Training loss: 2.91487979888916
Validation loss: 2.9812868436177573

Epoch: 6| Step: 7
Training loss: 3.577836275100708
Validation loss: 2.9791088104248047

Epoch: 6| Step: 8
Training loss: 3.7643356323242188
Validation loss: 2.9748457272847495

Epoch: 6| Step: 9
Training loss: 3.1766653060913086
Validation loss: 2.9711309671401978

Epoch: 6| Step: 10
Training loss: 2.7695064544677734
Validation loss: 2.966921051343282

Epoch: 6| Step: 11
Training loss: 3.0390830039978027
Validation loss: 2.9628514448801675

Epoch: 6| Step: 12
Training loss: 2.8414177894592285
Validation loss: 2.9589497248331704

Epoch: 6| Step: 13
Training loss: 2.7813897132873535
Validation loss: 2.9535363912582397

Epoch: 28| Step: 0
Training loss: 2.767987012863159
Validation loss: 2.951011578241984

Epoch: 6| Step: 1
Training loss: 2.746084690093994
Validation loss: 2.947444279988607

Epoch: 6| Step: 2
Training loss: 2.989344596862793
Validation loss: 2.944337248802185

Epoch: 6| Step: 3
Training loss: 2.884829044342041
Validation loss: 2.9410919745763144

Epoch: 6| Step: 4
Training loss: 2.646026611328125
Validation loss: 2.9372669458389282

Epoch: 6| Step: 5
Training loss: 2.9029526710510254
Validation loss: 2.934399644533793

Epoch: 6| Step: 6
Training loss: 4.345193862915039
Validation loss: 2.930927038192749

Epoch: 6| Step: 7
Training loss: 3.4977259635925293
Validation loss: 2.928110202153524

Epoch: 6| Step: 8
Training loss: 3.9849352836608887
Validation loss: 2.925113638242086

Epoch: 6| Step: 9
Training loss: 3.076727867126465
Validation loss: 2.9202958742777505

Epoch: 6| Step: 10
Training loss: 2.990715265274048
Validation loss: 2.917185624440511

Epoch: 6| Step: 11
Training loss: 3.3374552726745605
Validation loss: 2.914469321568807

Epoch: 6| Step: 12
Training loss: 3.1528940200805664
Validation loss: 2.9100351333618164

Epoch: 6| Step: 13
Training loss: 2.4965553283691406
Validation loss: 2.907346566518148

Epoch: 29| Step: 0
Training loss: 3.773871898651123
Validation loss: 2.9030062754948935

Epoch: 6| Step: 1
Training loss: 3.068901777267456
Validation loss: 2.9004719853401184

Epoch: 6| Step: 2
Training loss: 3.6121277809143066
Validation loss: 2.8957467079162598

Epoch: 6| Step: 3
Training loss: 2.5079479217529297
Validation loss: 2.892198165257772

Epoch: 6| Step: 4
Training loss: 2.8991940021514893
Validation loss: 2.889749844868978

Epoch: 6| Step: 5
Training loss: 2.550924301147461
Validation loss: 2.887210249900818

Epoch: 6| Step: 6
Training loss: 2.9550890922546387
Validation loss: 2.882623831431071

Epoch: 6| Step: 7
Training loss: 3.300266742706299
Validation loss: 2.8822048902511597

Epoch: 6| Step: 8
Training loss: 3.1009521484375
Validation loss: 2.878262758255005

Epoch: 6| Step: 9
Training loss: 3.5285050868988037
Validation loss: 2.874721646308899

Epoch: 6| Step: 10
Training loss: 2.427271604537964
Validation loss: 2.8701524337132773

Epoch: 6| Step: 11
Training loss: 3.2613675594329834
Validation loss: 2.866097847620646

Epoch: 6| Step: 12
Training loss: 3.433703899383545
Validation loss: 2.862761894861857

Epoch: 6| Step: 13
Training loss: 2.7719953060150146
Validation loss: 2.859725912412008

Epoch: 30| Step: 0
Training loss: 3.0447423458099365
Validation loss: 2.855368455251058

Epoch: 6| Step: 1
Training loss: 2.491245746612549
Validation loss: 2.853018124898275

Epoch: 6| Step: 2
Training loss: 2.3753342628479004
Validation loss: 2.849477847417196

Epoch: 6| Step: 3
Training loss: 3.2555551528930664
Validation loss: 2.8474588791529336

Epoch: 6| Step: 4
Training loss: 2.7455124855041504
Validation loss: 2.844675143559774

Epoch: 6| Step: 5
Training loss: 3.1223995685577393
Validation loss: 2.8426133394241333

Epoch: 6| Step: 6
Training loss: 2.3364479541778564
Validation loss: 2.8383416732152305

Epoch: 6| Step: 7
Training loss: 2.7871389389038086
Validation loss: 2.8385212818781533

Epoch: 6| Step: 8
Training loss: 3.7851457595825195
Validation loss: 2.8360071976979575

Epoch: 6| Step: 9
Training loss: 3.6716365814208984
Validation loss: 2.8317660093307495

Epoch: 6| Step: 10
Training loss: 2.7327094078063965
Validation loss: 2.828131914138794

Epoch: 6| Step: 11
Training loss: 3.4707720279693604
Validation loss: 2.8253961404164634

Epoch: 6| Step: 12
Training loss: 3.4694156646728516
Validation loss: 2.823594411214193

Epoch: 6| Step: 13
Training loss: 3.309765338897705
Validation loss: 2.818050742149353

Epoch: 31| Step: 0
Training loss: 3.2574210166931152
Validation loss: 2.811438957850138

Epoch: 6| Step: 1
Training loss: 2.4919815063476562
Validation loss: 2.8091352383295694

Epoch: 6| Step: 2
Training loss: 3.5430898666381836
Validation loss: 2.80626388390859

Epoch: 6| Step: 3
Training loss: 3.6761727333068848
Validation loss: 2.804163694381714

Epoch: 6| Step: 4
Training loss: 2.4250447750091553
Validation loss: 2.8012279669443765

Epoch: 6| Step: 5
Training loss: 2.7888576984405518
Validation loss: 2.7980517943700156

Epoch: 6| Step: 6
Training loss: 3.0209226608276367
Validation loss: 2.7942072550455728

Epoch: 6| Step: 7
Training loss: 2.9326813220977783
Validation loss: 2.789811372756958

Epoch: 6| Step: 8
Training loss: 3.200040817260742
Validation loss: 2.7853848536809287

Epoch: 6| Step: 9
Training loss: 2.66929030418396
Validation loss: 2.781542936960856

Epoch: 6| Step: 10
Training loss: 3.0618910789489746
Validation loss: 2.778599421183268

Epoch: 6| Step: 11
Training loss: 2.7601938247680664
Validation loss: 2.7744909723599753

Epoch: 6| Step: 12
Training loss: 2.847590208053589
Validation loss: 2.773540218671163

Epoch: 6| Step: 13
Training loss: 3.3348231315612793
Validation loss: 2.7722787459691367

Epoch: 32| Step: 0
Training loss: 3.3286709785461426
Validation loss: 2.769000291824341

Epoch: 6| Step: 1
Training loss: 3.345010995864868
Validation loss: 2.7628378868103027

Epoch: 6| Step: 2
Training loss: 3.736691951751709
Validation loss: 2.7599900563557944

Epoch: 6| Step: 3
Training loss: 2.429983139038086
Validation loss: 2.7556365331014

Epoch: 6| Step: 4
Training loss: 2.915515661239624
Validation loss: 2.7528102000554404

Epoch: 6| Step: 5
Training loss: 2.292024850845337
Validation loss: 2.7500749429066977

Epoch: 6| Step: 6
Training loss: 2.4680497646331787
Validation loss: 2.7455780506134033

Epoch: 6| Step: 7
Training loss: 3.0585384368896484
Validation loss: 2.7413781881332397

Epoch: 6| Step: 8
Training loss: 3.223173141479492
Validation loss: 2.7412185271581015

Epoch: 6| Step: 9
Training loss: 2.5371408462524414
Validation loss: 2.7358239889144897

Epoch: 6| Step: 10
Training loss: 3.02537202835083
Validation loss: 2.732029398282369

Epoch: 6| Step: 11
Training loss: 2.5870046615600586
Validation loss: 2.7270505825678506

Epoch: 6| Step: 12
Training loss: 2.8227357864379883
Validation loss: 2.7244327863057456

Epoch: 6| Step: 13
Training loss: 3.5694122314453125
Validation loss: 2.7258344888687134

Epoch: 33| Step: 0
Training loss: 2.6467409133911133
Validation loss: 2.7207228342692056

Epoch: 6| Step: 1
Training loss: 3.1974825859069824
Validation loss: 2.7185492515563965

Epoch: 6| Step: 2
Training loss: 2.689952850341797
Validation loss: 2.7136669556299844

Epoch: 6| Step: 3
Training loss: 2.6559605598449707
Validation loss: 2.708884676297506

Epoch: 6| Step: 4
Training loss: 3.4581453800201416
Validation loss: 2.706224044164022

Epoch: 6| Step: 5
Training loss: 2.5571982860565186
Validation loss: 2.7031384706497192

Epoch: 6| Step: 6
Training loss: 2.128209114074707
Validation loss: 2.7014565467834473

Epoch: 6| Step: 7
Training loss: 2.7080094814300537
Validation loss: 2.69741960366567

Epoch: 6| Step: 8
Training loss: 3.195240020751953
Validation loss: 2.6934848626454673

Epoch: 6| Step: 9
Training loss: 2.709657907485962
Validation loss: 2.6902525822321572

Epoch: 6| Step: 10
Training loss: 2.9728269577026367
Validation loss: 2.688126047452291

Epoch: 6| Step: 11
Training loss: 3.403578758239746
Validation loss: 2.68410058816274

Epoch: 6| Step: 12
Training loss: 3.002697467803955
Validation loss: 2.6814496517181396

Epoch: 6| Step: 13
Training loss: 3.2905805110931396
Validation loss: 2.6791346867879233

Epoch: 34| Step: 0
Training loss: 2.9912102222442627
Validation loss: 2.6773337523142495

Epoch: 6| Step: 1
Training loss: 3.235499620437622
Validation loss: 2.6726232767105103

Epoch: 6| Step: 2
Training loss: 2.922719955444336
Validation loss: 2.6727750301361084

Epoch: 6| Step: 3
Training loss: 2.743408679962158
Validation loss: 2.670921246210734

Epoch: 6| Step: 4
Training loss: 3.0918774604797363
Validation loss: 2.713409423828125

Epoch: 6| Step: 5
Training loss: 3.049638509750366
Validation loss: 2.6955750385920205

Epoch: 6| Step: 6
Training loss: 2.451417922973633
Validation loss: 2.6671208143234253

Epoch: 6| Step: 7
Training loss: 2.856257200241089
Validation loss: 2.653735637664795

Epoch: 6| Step: 8
Training loss: 2.4571828842163086
Validation loss: 2.646868586540222

Epoch: 6| Step: 9
Training loss: 2.586527109146118
Validation loss: 2.6495227416356406

Epoch: 6| Step: 10
Training loss: 1.905364990234375
Validation loss: 2.6512662967046103

Epoch: 6| Step: 11
Training loss: 2.955465793609619
Validation loss: 2.643849730491638

Epoch: 6| Step: 12
Training loss: 3.7999215126037598
Validation loss: 2.644329865773519

Epoch: 6| Step: 13
Training loss: 3.043304443359375
Validation loss: 2.636965254942576

Epoch: 35| Step: 0
Training loss: 2.673858165740967
Validation loss: 2.632618725299835

Epoch: 6| Step: 1
Training loss: 3.369325637817383
Validation loss: 2.6283162037531533

Epoch: 6| Step: 2
Training loss: 2.468000888824463
Validation loss: 2.6261343955993652

Epoch: 6| Step: 3
Training loss: 2.4060235023498535
Validation loss: 2.621737241744995

Epoch: 6| Step: 4
Training loss: 2.453310966491699
Validation loss: 2.6195205450057983

Epoch: 6| Step: 5
Training loss: 3.0546109676361084
Validation loss: 2.619088610013326

Epoch: 6| Step: 6
Training loss: 2.3095555305480957
Validation loss: 2.6137439409891763

Epoch: 6| Step: 7
Training loss: 2.291499137878418
Validation loss: 2.6105565230051675

Epoch: 6| Step: 8
Training loss: 2.68550181388855
Validation loss: 2.6107097069422402

Epoch: 6| Step: 9
Training loss: 3.4325709342956543
Validation loss: 2.6100448767344155

Epoch: 6| Step: 10
Training loss: 3.8113627433776855
Validation loss: 2.59888748327891

Epoch: 6| Step: 11
Training loss: 3.3468923568725586
Validation loss: 2.5932263135910034

Epoch: 6| Step: 12
Training loss: 2.1234750747680664
Validation loss: 2.5891972382863364

Epoch: 6| Step: 13
Training loss: 2.9083995819091797
Validation loss: 2.585753162701925

Epoch: 36| Step: 0
Training loss: 2.1500253677368164
Validation loss: 2.5849181413650513

Epoch: 6| Step: 1
Training loss: 2.062411308288574
Validation loss: 2.5813553730646768

Epoch: 6| Step: 2
Training loss: 3.11152982711792
Validation loss: 2.580101748307546

Epoch: 6| Step: 3
Training loss: 2.7925233840942383
Validation loss: 2.5792545477549234

Epoch: 6| Step: 4
Training loss: 2.5700154304504395
Validation loss: 2.5733349720637

Epoch: 6| Step: 5
Training loss: 2.8892455101013184
Validation loss: 2.5685654481252036

Epoch: 6| Step: 6
Training loss: 2.787240505218506
Validation loss: 2.5661582549413047

Epoch: 6| Step: 7
Training loss: 2.803776264190674
Validation loss: 2.5612841844558716

Epoch: 6| Step: 8
Training loss: 3.411973237991333
Validation loss: 2.5634241104125977

Epoch: 6| Step: 9
Training loss: 2.3821959495544434
Validation loss: 2.556209087371826

Epoch: 6| Step: 10
Training loss: 3.0118014812469482
Validation loss: 2.557757298151652

Epoch: 6| Step: 11
Training loss: 2.7605133056640625
Validation loss: 2.5484378337860107

Epoch: 6| Step: 12
Training loss: 2.7156567573547363
Validation loss: 2.5447359482447305

Epoch: 6| Step: 13
Training loss: 3.2409982681274414
Validation loss: 2.540742556254069

Epoch: 37| Step: 0
Training loss: 2.8121042251586914
Validation loss: 2.536888321240743

Epoch: 6| Step: 1
Training loss: 2.660125732421875
Validation loss: 2.5337278048197427

Epoch: 6| Step: 2
Training loss: 2.5298359394073486
Validation loss: 2.534151077270508

Epoch: 6| Step: 3
Training loss: 3.104130506515503
Validation loss: 2.528769393761953

Epoch: 6| Step: 4
Training loss: 2.769355297088623
Validation loss: 2.526303251584371

Epoch: 6| Step: 5
Training loss: 2.1660728454589844
Validation loss: 2.5246423284212747

Epoch: 6| Step: 6
Training loss: 2.8739821910858154
Validation loss: 2.5245556036631265

Epoch: 6| Step: 7
Training loss: 2.632582664489746
Validation loss: 2.522060672442118

Epoch: 6| Step: 8
Training loss: 2.705746650695801
Validation loss: 2.519686301549276

Epoch: 6| Step: 9
Training loss: 2.373021125793457
Validation loss: 2.5172899961471558

Epoch: 6| Step: 10
Training loss: 2.776930332183838
Validation loss: 2.510196050008138

Epoch: 6| Step: 11
Training loss: 2.8873023986816406
Validation loss: 2.5071828365325928

Epoch: 6| Step: 12
Training loss: 2.808948516845703
Validation loss: 2.5069593588511148

Epoch: 6| Step: 13
Training loss: 2.863192081451416
Validation loss: 2.5016618172327676

Epoch: 38| Step: 0
Training loss: 2.520641803741455
Validation loss: 2.50248392422994

Epoch: 6| Step: 1
Training loss: 2.9344518184661865
Validation loss: 2.497385025024414

Epoch: 6| Step: 2
Training loss: 2.8623640537261963
Validation loss: 2.4956357876459756

Epoch: 6| Step: 3
Training loss: 3.1142425537109375
Validation loss: 2.4903606176376343

Epoch: 6| Step: 4
Training loss: 2.8322696685791016
Validation loss: 2.489193240801493

Epoch: 6| Step: 5
Training loss: 2.2946743965148926
Validation loss: 2.4844172398249307

Epoch: 6| Step: 6
Training loss: 2.6116271018981934
Validation loss: 2.4811383883158364

Epoch: 6| Step: 7
Training loss: 2.5723657608032227
Validation loss: 2.4780786832173667

Epoch: 6| Step: 8
Training loss: 2.8507447242736816
Validation loss: 2.471217632293701

Epoch: 6| Step: 9
Training loss: 2.749645233154297
Validation loss: 2.4682563940684

Epoch: 6| Step: 10
Training loss: 2.434685707092285
Validation loss: 2.4644415179888406

Epoch: 6| Step: 11
Training loss: 2.2900450229644775
Validation loss: 2.461172103881836

Epoch: 6| Step: 12
Training loss: 2.3641843795776367
Validation loss: 2.4578678607940674

Epoch: 6| Step: 13
Training loss: 2.9473154544830322
Validation loss: 2.456520597139994

Epoch: 39| Step: 0
Training loss: 2.6968822479248047
Validation loss: 2.4509077866872153

Epoch: 6| Step: 1
Training loss: 2.5073466300964355
Validation loss: 2.4545482794443765

Epoch: 6| Step: 2
Training loss: 2.296464443206787
Validation loss: 2.4488038221995034

Epoch: 6| Step: 3
Training loss: 2.8621201515197754
Validation loss: 2.443260828653971

Epoch: 6| Step: 4
Training loss: 3.3248000144958496
Validation loss: 2.4409581820170083

Epoch: 6| Step: 5
Training loss: 2.9908909797668457
Validation loss: 2.438954750696818

Epoch: 6| Step: 6
Training loss: 2.3123221397399902
Validation loss: 2.4363292455673218

Epoch: 6| Step: 7
Training loss: 2.1163010597229004
Validation loss: 2.4340931177139282

Epoch: 6| Step: 8
Training loss: 2.8077991008758545
Validation loss: 2.4335437615712485

Epoch: 6| Step: 9
Training loss: 2.753453254699707
Validation loss: 2.4292616844177246

Epoch: 6| Step: 10
Training loss: 2.4178919792175293
Validation loss: 2.424560864766439

Epoch: 6| Step: 11
Training loss: 3.010620594024658
Validation loss: 2.4223947525024414

Epoch: 6| Step: 12
Training loss: 2.011002779006958
Validation loss: 2.419156273206075

Epoch: 6| Step: 13
Training loss: 2.548255443572998
Validation loss: 2.4166027704874673

Epoch: 40| Step: 0
Training loss: 2.9051499366760254
Validation loss: 2.4150354862213135

Epoch: 6| Step: 1
Training loss: 2.8927574157714844
Validation loss: 2.4083256324132285

Epoch: 6| Step: 2
Training loss: 2.0148227214813232
Validation loss: 2.407961825529734

Epoch: 6| Step: 3
Training loss: 2.5880823135375977
Validation loss: 2.4129532178243003

Epoch: 6| Step: 4
Training loss: 2.845305919647217
Validation loss: 2.4091352820396423

Epoch: 6| Step: 5
Training loss: 2.3399605751037598
Validation loss: 2.3978473941485086

Epoch: 6| Step: 6
Training loss: 2.6009457111358643
Validation loss: 2.396989583969116

Epoch: 6| Step: 7
Training loss: 2.8800201416015625
Validation loss: 2.3930278023084006

Epoch: 6| Step: 8
Training loss: 2.8453521728515625
Validation loss: 2.3919449051221213

Epoch: 6| Step: 9
Training loss: 2.740365505218506
Validation loss: 2.39220263560613

Epoch: 6| Step: 10
Training loss: 2.4730224609375
Validation loss: 2.3849581480026245

Epoch: 6| Step: 11
Training loss: 2.5450947284698486
Validation loss: 2.385202248891195

Epoch: 6| Step: 12
Training loss: 2.147001266479492
Validation loss: 2.38020920753479

Epoch: 6| Step: 13
Training loss: 2.3316097259521484
Validation loss: 2.3740989367167153

Epoch: 41| Step: 0
Training loss: 2.5397887229919434
Validation loss: 2.374046544233958

Epoch: 6| Step: 1
Training loss: 2.7858805656433105
Validation loss: 2.3673749764760337

Epoch: 6| Step: 2
Training loss: 2.3516273498535156
Validation loss: 2.37112025419871

Epoch: 6| Step: 3
Training loss: 2.2859768867492676
Validation loss: 2.363530238469442

Epoch: 6| Step: 4
Training loss: 1.9383366107940674
Validation loss: 2.3601585626602173

Epoch: 6| Step: 5
Training loss: 2.1335372924804688
Validation loss: 2.3618789513905845

Epoch: 6| Step: 6
Training loss: 2.8333239555358887
Validation loss: 2.351550022761027

Epoch: 6| Step: 7
Training loss: 2.4733519554138184
Validation loss: 2.35649303595225

Epoch: 6| Step: 8
Training loss: 2.9545302391052246
Validation loss: 2.3548646767934165

Epoch: 6| Step: 9
Training loss: 2.225527286529541
Validation loss: 2.354569673538208

Epoch: 6| Step: 10
Training loss: 2.3888015747070312
Validation loss: 2.350836535294851

Epoch: 6| Step: 11
Training loss: 2.443777084350586
Validation loss: 2.346500734488169

Epoch: 6| Step: 12
Training loss: 3.101113796234131
Validation loss: 2.3480942646662393

Epoch: 6| Step: 13
Training loss: 2.9901716709136963
Validation loss: 2.338338037331899

Epoch: 42| Step: 0
Training loss: 1.7780675888061523
Validation loss: 2.3383938868840537

Epoch: 6| Step: 1
Training loss: 2.280661106109619
Validation loss: 2.335147420565287

Epoch: 6| Step: 2
Training loss: 2.278809070587158
Validation loss: 2.3385794957478843

Epoch: 6| Step: 3
Training loss: 3.2173285484313965
Validation loss: 2.341940402984619

Epoch: 6| Step: 4
Training loss: 2.473440647125244
Validation loss: 2.3245363235473633

Epoch: 6| Step: 5
Training loss: 2.4686203002929688
Validation loss: 2.3233601649602256

Epoch: 6| Step: 6
Training loss: 2.909379482269287
Validation loss: 2.32513036330541

Epoch: 6| Step: 7
Training loss: 2.953745126724243
Validation loss: 2.3247021436691284

Epoch: 6| Step: 8
Training loss: 2.4038844108581543
Validation loss: 2.322529673576355

Epoch: 6| Step: 9
Training loss: 2.4496448040008545
Validation loss: 2.320494771003723

Epoch: 6| Step: 10
Training loss: 1.9859638214111328
Validation loss: 2.3166175882021585

Epoch: 6| Step: 11
Training loss: 2.867042064666748
Validation loss: 2.314969837665558

Epoch: 6| Step: 12
Training loss: 2.9460716247558594
Validation loss: 2.3142279187838235

Epoch: 6| Step: 13
Training loss: 1.992497444152832
Validation loss: 2.3101287285486856

Epoch: 43| Step: 0
Training loss: 2.14983868598938
Validation loss: 2.307556629180908

Epoch: 6| Step: 1
Training loss: 1.885873556137085
Validation loss: 2.301326036453247

Epoch: 6| Step: 2
Training loss: 2.2830474376678467
Validation loss: 2.3043087323506675

Epoch: 6| Step: 3
Training loss: 2.9004316329956055
Validation loss: 2.2947651147842407

Epoch: 6| Step: 4
Training loss: 2.4467885494232178
Validation loss: 2.292646805445353

Epoch: 6| Step: 5
Training loss: 2.7311840057373047
Validation loss: 2.2917863925298056

Epoch: 6| Step: 6
Training loss: 2.8660566806793213
Validation loss: 2.2868362863858542

Epoch: 6| Step: 7
Training loss: 2.751706123352051
Validation loss: 2.2817773620287576

Epoch: 6| Step: 8
Training loss: 2.3986501693725586
Validation loss: 2.2797887523969016

Epoch: 6| Step: 9
Training loss: 1.6433781385421753
Validation loss: 2.2806919614473977

Epoch: 6| Step: 10
Training loss: 2.194016933441162
Validation loss: 2.27673210700353

Epoch: 6| Step: 11
Training loss: 2.622488498687744
Validation loss: 2.272748351097107

Epoch: 6| Step: 12
Training loss: 2.544247627258301
Validation loss: 2.279078185558319

Epoch: 6| Step: 13
Training loss: 2.9705090522766113
Validation loss: 2.2705554564793906

Epoch: 44| Step: 0
Training loss: 2.5678348541259766
Validation loss: 2.2632526556650796

Epoch: 6| Step: 1
Training loss: 2.531414031982422
Validation loss: 2.2632803320884705

Epoch: 6| Step: 2
Training loss: 1.7744617462158203
Validation loss: 2.264150301615397

Epoch: 6| Step: 3
Training loss: 2.380526304244995
Validation loss: 2.258147199948629

Epoch: 6| Step: 4
Training loss: 1.9359381198883057
Validation loss: 2.25560861825943

Epoch: 6| Step: 5
Training loss: 2.5493054389953613
Validation loss: 2.254002590974172

Epoch: 6| Step: 6
Training loss: 2.9641122817993164
Validation loss: 2.244359771410624

Epoch: 6| Step: 7
Training loss: 2.9273476600646973
Validation loss: 2.2488130927085876

Epoch: 6| Step: 8
Training loss: 3.1570076942443848
Validation loss: 2.245454410711924

Epoch: 6| Step: 9
Training loss: 1.6946051120758057
Validation loss: 2.233489175637563

Epoch: 6| Step: 10
Training loss: 2.2857813835144043
Validation loss: 2.2360235452651978

Epoch: 6| Step: 11
Training loss: 2.3814995288848877
Validation loss: 2.2349286476771035

Epoch: 6| Step: 12
Training loss: 2.6120994091033936
Validation loss: 2.2292778690656028

Epoch: 6| Step: 13
Training loss: 1.8603099584579468
Validation loss: 2.234651585419973

Epoch: 45| Step: 0
Training loss: 2.446031093597412
Validation loss: 2.2327271501223245

Epoch: 6| Step: 1
Training loss: 2.1193113327026367
Validation loss: 2.2333947817484536

Epoch: 6| Step: 2
Training loss: 1.7653542757034302
Validation loss: 2.2354671160380044

Epoch: 6| Step: 3
Training loss: 2.6282846927642822
Validation loss: 2.2317878007888794

Epoch: 6| Step: 4
Training loss: 2.1264545917510986
Validation loss: 2.232352534929911

Epoch: 6| Step: 5
Training loss: 3.2263107299804688
Validation loss: 2.2328324715296426

Epoch: 6| Step: 6
Training loss: 2.6157004833221436
Validation loss: 2.2324735124905906

Epoch: 6| Step: 7
Training loss: 2.746284008026123
Validation loss: 2.231454153855642

Epoch: 6| Step: 8
Training loss: 2.595703601837158
Validation loss: 2.225373327732086

Epoch: 6| Step: 9
Training loss: 2.1437811851501465
Validation loss: 2.21626748641332

Epoch: 6| Step: 10
Training loss: 2.0345959663391113
Validation loss: 2.213366985321045

Epoch: 6| Step: 11
Training loss: 1.6732823848724365
Validation loss: 2.209829350312551

Epoch: 6| Step: 12
Training loss: 2.6381003856658936
Validation loss: 2.2049429416656494

Epoch: 6| Step: 13
Training loss: 2.5646252632141113
Validation loss: 2.207933187484741

Epoch: 46| Step: 0
Training loss: 2.6963436603546143
Validation loss: 2.2037487228711448

Epoch: 6| Step: 1
Training loss: 1.8588446378707886
Validation loss: 2.214425563812256

Epoch: 6| Step: 2
Training loss: 2.2481608390808105
Validation loss: 2.2186693946520486

Epoch: 6| Step: 3
Training loss: 3.1333937644958496
Validation loss: 2.205635646979014

Epoch: 6| Step: 4
Training loss: 2.4389212131500244
Validation loss: 2.1895679235458374

Epoch: 6| Step: 5
Training loss: 2.782188892364502
Validation loss: 2.1870806018511453

Epoch: 6| Step: 6
Training loss: 2.0618906021118164
Validation loss: 2.1947155793507895

Epoch: 6| Step: 7
Training loss: 2.4656972885131836
Validation loss: 2.1928351322809854

Epoch: 6| Step: 8
Training loss: 2.5173258781433105
Validation loss: 2.196341037750244

Epoch: 6| Step: 9
Training loss: 2.1593825817108154
Validation loss: 2.2008057634035745

Epoch: 6| Step: 10
Training loss: 2.0392189025878906
Validation loss: 2.2027705113093057

Epoch: 6| Step: 11
Training loss: 2.0557632446289062
Validation loss: 2.2117215991020203

Epoch: 6| Step: 12
Training loss: 2.9223341941833496
Validation loss: 2.2089813351631165

Epoch: 6| Step: 13
Training loss: 1.8325729370117188
Validation loss: 2.2115880648295083

Epoch: 47| Step: 0
Training loss: 1.743378758430481
Validation loss: 2.2110188802083335

Epoch: 6| Step: 1
Training loss: 1.854872703552246
Validation loss: 2.209241807460785

Epoch: 6| Step: 2
Training loss: 3.126878499984741
Validation loss: 2.2024565935134888

Epoch: 6| Step: 3
Training loss: 1.5464308261871338
Validation loss: 2.192955791950226

Epoch: 6| Step: 4
Training loss: 2.3389525413513184
Validation loss: 2.1892650524775186

Epoch: 6| Step: 5
Training loss: 2.521646022796631
Validation loss: 2.18122935295105

Epoch: 6| Step: 6
Training loss: 2.713346004486084
Validation loss: 2.175198038419088

Epoch: 6| Step: 7
Training loss: 2.773909330368042
Validation loss: 2.1709023912747702

Epoch: 6| Step: 8
Training loss: 3.3937668800354004
Validation loss: 2.167637606461843

Epoch: 6| Step: 9
Training loss: 2.7335314750671387
Validation loss: 2.1645081440607705

Epoch: 6| Step: 10
Training loss: 1.8431384563446045
Validation loss: 2.161938806374868

Epoch: 6| Step: 11
Training loss: 2.248105049133301
Validation loss: 2.161383330821991

Epoch: 6| Step: 12
Training loss: 1.9063313007354736
Validation loss: 2.160018185774485

Epoch: 6| Step: 13
Training loss: 2.1914892196655273
Validation loss: 2.1623489260673523

Epoch: 48| Step: 0
Training loss: 1.6678154468536377
Validation loss: 2.157676855723063

Epoch: 6| Step: 1
Training loss: 2.2155799865722656
Validation loss: 2.1580574115117392

Epoch: 6| Step: 2
Training loss: 2.838794708251953
Validation loss: 2.1550052762031555

Epoch: 6| Step: 3
Training loss: 3.132943630218506
Validation loss: 2.1518141428629556

Epoch: 6| Step: 4
Training loss: 2.283477544784546
Validation loss: 2.15328178803126

Epoch: 6| Step: 5
Training loss: 1.545278549194336
Validation loss: 2.1530625422795615

Epoch: 6| Step: 6
Training loss: 2.637077808380127
Validation loss: 2.153367042541504

Epoch: 6| Step: 7
Training loss: 2.4688963890075684
Validation loss: 2.1529849966367087

Epoch: 6| Step: 8
Training loss: 2.5236573219299316
Validation loss: 2.1495672265688577

Epoch: 6| Step: 9
Training loss: 1.8378443717956543
Validation loss: 2.1476576924324036

Epoch: 6| Step: 10
Training loss: 1.8665940761566162
Validation loss: 2.143982946872711

Epoch: 6| Step: 11
Training loss: 2.3850948810577393
Validation loss: 2.1424914995829263

Epoch: 6| Step: 12
Training loss: 2.8893179893493652
Validation loss: 2.1363509694735208

Epoch: 6| Step: 13
Training loss: 2.0944719314575195
Validation loss: 2.132864793141683

Epoch: 49| Step: 0
Training loss: 2.3899893760681152
Validation loss: 2.133687893549601

Epoch: 6| Step: 1
Training loss: 1.766115665435791
Validation loss: 2.141801397005717

Epoch: 6| Step: 2
Training loss: 2.359309196472168
Validation loss: 2.137244542439779

Epoch: 6| Step: 3
Training loss: 2.160355567932129
Validation loss: 2.131430447101593

Epoch: 6| Step: 4
Training loss: 2.6840100288391113
Validation loss: 2.125963012377421

Epoch: 6| Step: 5
Training loss: 2.2796764373779297
Validation loss: 2.1240483125050864

Epoch: 6| Step: 6
Training loss: 2.163541793823242
Validation loss: 2.1238104899724326

Epoch: 6| Step: 7
Training loss: 2.092966079711914
Validation loss: 2.12488845984141

Epoch: 6| Step: 8
Training loss: 2.424243927001953
Validation loss: 2.1198387344678244

Epoch: 6| Step: 9
Training loss: 1.8813376426696777
Validation loss: 2.1142836809158325

Epoch: 6| Step: 10
Training loss: 2.980774164199829
Validation loss: 2.1166479190190635

Epoch: 6| Step: 11
Training loss: 1.8968191146850586
Validation loss: 2.1135788361231485

Epoch: 6| Step: 12
Training loss: 2.5510568618774414
Validation loss: 2.1104720632235208

Epoch: 6| Step: 13
Training loss: 2.467174530029297
Validation loss: 2.1125179131825766

Epoch: 50| Step: 0
Training loss: 2.547633171081543
Validation loss: 2.116961340109507

Epoch: 6| Step: 1
Training loss: 2.642958402633667
Validation loss: 2.1203318436940513

Epoch: 6| Step: 2
Training loss: 1.9794206619262695
Validation loss: 2.1153178612391152

Epoch: 6| Step: 3
Training loss: 2.295562744140625
Validation loss: 2.113593637943268

Epoch: 6| Step: 4
Training loss: 1.7523324489593506
Validation loss: 2.1088263988494873

Epoch: 6| Step: 5
Training loss: 2.430485725402832
Validation loss: 2.112022121747335

Epoch: 6| Step: 6
Training loss: 1.9444575309753418
Validation loss: 2.1052273313204446

Epoch: 6| Step: 7
Training loss: 2.9230690002441406
Validation loss: 2.099742809931437

Epoch: 6| Step: 8
Training loss: 1.9807767868041992
Validation loss: 2.105263630549113

Epoch: 6| Step: 9
Training loss: 1.85992431640625
Validation loss: 2.0987324515978494

Epoch: 6| Step: 10
Training loss: 2.8013617992401123
Validation loss: 2.1018507877985635

Epoch: 6| Step: 11
Training loss: 2.4943854808807373
Validation loss: 2.093774457772573

Epoch: 6| Step: 12
Training loss: 2.267087459564209
Validation loss: 2.097457150618235

Epoch: 6| Step: 13
Training loss: 1.891185998916626
Validation loss: 2.0959831873575845

Epoch: 51| Step: 0
Training loss: 2.600440502166748
Validation loss: 2.093492945035299

Epoch: 6| Step: 1
Training loss: 2.803661823272705
Validation loss: 2.0973918437957764

Epoch: 6| Step: 2
Training loss: 1.6759791374206543
Validation loss: 2.0899853905042014

Epoch: 6| Step: 3
Training loss: 2.4960155487060547
Validation loss: 2.10058601697286

Epoch: 6| Step: 4
Training loss: 2.5239837169647217
Validation loss: 2.10592128833135

Epoch: 6| Step: 5
Training loss: 2.5315098762512207
Validation loss: 2.122857908407847

Epoch: 6| Step: 6
Training loss: 2.6048669815063477
Validation loss: 2.132508635520935

Epoch: 6| Step: 7
Training loss: 1.6689692735671997
Validation loss: 2.107228934764862

Epoch: 6| Step: 8
Training loss: 2.2686853408813477
Validation loss: 2.0937283436457315

Epoch: 6| Step: 9
Training loss: 1.8615385293960571
Validation loss: 2.0819180806477866

Epoch: 6| Step: 10
Training loss: 2.1096818447113037
Validation loss: 2.0922993421554565

Epoch: 6| Step: 11
Training loss: 2.4142043590545654
Validation loss: 2.100854754447937

Epoch: 6| Step: 12
Training loss: 1.7960965633392334
Validation loss: 2.1034259597460427

Epoch: 6| Step: 13
Training loss: 2.4338412284851074
Validation loss: 2.107541779677073

Epoch: 52| Step: 0
Training loss: 2.482001781463623
Validation loss: 2.1130563418070474

Epoch: 6| Step: 1
Training loss: 1.9496451616287231
Validation loss: 2.1130703687667847

Epoch: 6| Step: 2
Training loss: 2.6321001052856445
Validation loss: 2.1201711893081665

Epoch: 6| Step: 3
Training loss: 2.381953239440918
Validation loss: 2.1228678027788797

Epoch: 6| Step: 4
Training loss: 2.1724162101745605
Validation loss: 2.1240187883377075

Epoch: 6| Step: 5
Training loss: 2.165742874145508
Validation loss: 2.114141325155894

Epoch: 6| Step: 6
Training loss: 2.215865135192871
Validation loss: 2.112218658129374

Epoch: 6| Step: 7
Training loss: 2.5132803916931152
Validation loss: 2.11295086145401

Epoch: 6| Step: 8
Training loss: 2.764033317565918
Validation loss: 2.104783038298289

Epoch: 6| Step: 9
Training loss: 2.2682039737701416
Validation loss: 2.1025409499804177

Epoch: 6| Step: 10
Training loss: 1.9275035858154297
Validation loss: 2.0974952379862466

Epoch: 6| Step: 11
Training loss: 2.0262649059295654
Validation loss: 2.093543072541555

Epoch: 6| Step: 12
Training loss: 2.7345736026763916
Validation loss: 2.095548093318939

Epoch: 6| Step: 13
Training loss: 1.8488311767578125
Validation loss: 2.094479521115621

Epoch: 53| Step: 0
Training loss: 2.117795467376709
Validation loss: 2.092350423336029

Epoch: 6| Step: 1
Training loss: 2.0427775382995605
Validation loss: 2.089763323465983

Epoch: 6| Step: 2
Training loss: 2.609137535095215
Validation loss: 2.086235503355662

Epoch: 6| Step: 3
Training loss: 2.182933807373047
Validation loss: 2.0816211899121604

Epoch: 6| Step: 4
Training loss: 1.911524772644043
Validation loss: 2.0733927885691323

Epoch: 6| Step: 5
Training loss: 2.3990726470947266
Validation loss: 2.070551892121633

Epoch: 6| Step: 6
Training loss: 2.2548656463623047
Validation loss: 2.0674025217692056

Epoch: 6| Step: 7
Training loss: 2.0248634815216064
Validation loss: 2.0652185678482056

Epoch: 6| Step: 8
Training loss: 2.390796661376953
Validation loss: 2.067591826121012

Epoch: 6| Step: 9
Training loss: 1.7606347799301147
Validation loss: 2.0594725608825684

Epoch: 6| Step: 10
Training loss: 2.968891143798828
Validation loss: 2.0706797440846763

Epoch: 6| Step: 11
Training loss: 2.338961124420166
Validation loss: 2.060796797275543

Epoch: 6| Step: 12
Training loss: 1.833726167678833
Validation loss: 2.063263197739919

Epoch: 6| Step: 13
Training loss: 2.7050564289093018
Validation loss: 2.0624506076176963

Epoch: 54| Step: 0
Training loss: 2.8860325813293457
Validation loss: 2.0698461731274924

Epoch: 6| Step: 1
Training loss: 1.6270530223846436
Validation loss: 2.0542751351992288

Epoch: 6| Step: 2
Training loss: 2.053422451019287
Validation loss: 2.0535892645517984

Epoch: 6| Step: 3
Training loss: 2.3300063610076904
Validation loss: 2.0613555113474527

Epoch: 6| Step: 4
Training loss: 1.9208937883377075
Validation loss: 2.0622671643892923

Epoch: 6| Step: 5
Training loss: 2.3408567905426025
Validation loss: 2.0619778831799827

Epoch: 6| Step: 6
Training loss: 2.116234302520752
Validation loss: 2.061499218146006

Epoch: 6| Step: 7
Training loss: 2.800854206085205
Validation loss: 2.063714385032654

Epoch: 6| Step: 8
Training loss: 2.104198455810547
Validation loss: 2.0606895883878074

Epoch: 6| Step: 9
Training loss: 1.3478180170059204
Validation loss: 2.061451812585195

Epoch: 6| Step: 10
Training loss: 2.26863169670105
Validation loss: 2.0663803021113076

Epoch: 6| Step: 11
Training loss: 2.507068395614624
Validation loss: 2.0621336301167807

Epoch: 6| Step: 12
Training loss: 2.5740575790405273
Validation loss: 2.063509225845337

Epoch: 6| Step: 13
Training loss: 2.6679952144622803
Validation loss: 2.0622804363568625

Epoch: 55| Step: 0
Training loss: 2.7207422256469727
Validation loss: 2.061897953351339

Epoch: 6| Step: 1
Training loss: 2.61714243888855
Validation loss: 2.0594162146250405

Epoch: 6| Step: 2
Training loss: 1.965047836303711
Validation loss: 2.050317863623301

Epoch: 6| Step: 3
Training loss: 2.6683449745178223
Validation loss: 2.0460519591967263

Epoch: 6| Step: 4
Training loss: 2.1236023902893066
Validation loss: 2.03900416692098

Epoch: 6| Step: 5
Training loss: 2.881197214126587
Validation loss: 2.0605632861455283

Epoch: 6| Step: 6
Training loss: 2.377289295196533
Validation loss: 2.0706312457720437

Epoch: 6| Step: 7
Training loss: 2.5082545280456543
Validation loss: 2.0596253673235574

Epoch: 6| Step: 8
Training loss: 1.3527443408966064
Validation loss: 2.0546196897824607

Epoch: 6| Step: 9
Training loss: 1.8509972095489502
Validation loss: 2.049879709879557

Epoch: 6| Step: 10
Training loss: 1.596022129058838
Validation loss: 2.053412357966105

Epoch: 6| Step: 11
Training loss: 2.1179919242858887
Validation loss: 2.039694289366404

Epoch: 6| Step: 12
Training loss: 2.4423136711120605
Validation loss: 2.0305126508076987

Epoch: 6| Step: 13
Training loss: 1.98184072971344
Validation loss: 2.039728105068207

Epoch: 56| Step: 0
Training loss: 1.3144426345825195
Validation loss: 2.0466405351956687

Epoch: 6| Step: 1
Training loss: 1.760783314704895
Validation loss: 2.0512841939926147

Epoch: 6| Step: 2
Training loss: 1.9611809253692627
Validation loss: 2.0509095788002014

Epoch: 6| Step: 3
Training loss: 2.6812217235565186
Validation loss: 2.0638205806414285

Epoch: 6| Step: 4
Training loss: 2.293565511703491
Validation loss: 2.0668344696362815

Epoch: 6| Step: 5
Training loss: 1.8643826246261597
Validation loss: 2.073263108730316

Epoch: 6| Step: 6
Training loss: 2.331725597381592
Validation loss: 2.079603989919027

Epoch: 6| Step: 7
Training loss: 2.2417654991149902
Validation loss: 2.068523625532786

Epoch: 6| Step: 8
Training loss: 2.9873733520507812
Validation loss: 2.068744639555613

Epoch: 6| Step: 9
Training loss: 2.619617462158203
Validation loss: 2.065364420413971

Epoch: 6| Step: 10
Training loss: 2.21995210647583
Validation loss: 2.0549205938975015

Epoch: 6| Step: 11
Training loss: 2.6071174144744873
Validation loss: 2.0539951721827188

Epoch: 6| Step: 12
Training loss: 2.696993112564087
Validation loss: 2.05008472998937

Epoch: 6| Step: 13
Training loss: 1.971531629562378
Validation loss: 2.0503007570902505

Epoch: 57| Step: 0
Training loss: 2.418091058731079
Validation loss: 2.0413222908973694

Epoch: 6| Step: 1
Training loss: 1.8120014667510986
Validation loss: 2.046499192714691

Epoch: 6| Step: 2
Training loss: 2.4895246028900146
Validation loss: 2.043649355570475

Epoch: 6| Step: 3
Training loss: 2.783766746520996
Validation loss: 2.0352654258410134

Epoch: 6| Step: 4
Training loss: 2.0533523559570312
Validation loss: 2.02983150879542

Epoch: 6| Step: 5
Training loss: 1.6946519613265991
Validation loss: 2.032714386781057

Epoch: 6| Step: 6
Training loss: 2.08425235748291
Validation loss: 2.0319818456967673

Epoch: 6| Step: 7
Training loss: 2.270197868347168
Validation loss: 2.034895956516266

Epoch: 6| Step: 8
Training loss: 2.2339608669281006
Validation loss: 2.0283963680267334

Epoch: 6| Step: 9
Training loss: 2.1778695583343506
Validation loss: 2.039744277795156

Epoch: 6| Step: 10
Training loss: 1.906517505645752
Validation loss: 2.0477529962857566

Epoch: 6| Step: 11
Training loss: 2.4454357624053955
Validation loss: 2.07144288221995

Epoch: 6| Step: 12
Training loss: 2.3838624954223633
Validation loss: 2.0782996813456216

Epoch: 6| Step: 13
Training loss: 2.2789573669433594
Validation loss: 2.0587318738301597

Epoch: 58| Step: 0
Training loss: 2.5760302543640137
Validation loss: 2.0457419951756797

Epoch: 6| Step: 1
Training loss: 2.615387201309204
Validation loss: 2.039482136567434

Epoch: 6| Step: 2
Training loss: 2.581890106201172
Validation loss: 2.0298760334650674

Epoch: 6| Step: 3
Training loss: 1.5149621963500977
Validation loss: 2.036276638507843

Epoch: 6| Step: 4
Training loss: 2.325286626815796
Validation loss: 2.0450310707092285

Epoch: 6| Step: 5
Training loss: 2.4168543815612793
Validation loss: 2.0419806838035583

Epoch: 6| Step: 6
Training loss: 2.00821590423584
Validation loss: 2.047452688217163

Epoch: 6| Step: 7
Training loss: 1.787659764289856
Validation loss: 2.0469600359598794

Epoch: 6| Step: 8
Training loss: 1.7737607955932617
Validation loss: 2.05014705657959

Epoch: 6| Step: 9
Training loss: 1.7787102460861206
Validation loss: 2.0542942682902017

Epoch: 6| Step: 10
Training loss: 2.5033938884735107
Validation loss: 2.050574858983358

Epoch: 6| Step: 11
Training loss: 2.438530921936035
Validation loss: 2.0531556407610574

Epoch: 6| Step: 12
Training loss: 2.699272632598877
Validation loss: 2.0497235854466758

Epoch: 6| Step: 13
Training loss: 2.1704018115997314
Validation loss: 2.045336743195852

Epoch: 59| Step: 0
Training loss: 1.9535293579101562
Validation loss: 2.0452612042427063

Epoch: 6| Step: 1
Training loss: 2.6763241291046143
Validation loss: 2.04052464167277

Epoch: 6| Step: 2
Training loss: 2.0685203075408936
Validation loss: 2.038884937763214

Epoch: 6| Step: 3
Training loss: 2.210247039794922
Validation loss: 2.0342817505200705

Epoch: 6| Step: 4
Training loss: 2.1991825103759766
Validation loss: 2.02952774365743

Epoch: 6| Step: 5
Training loss: 2.998232841491699
Validation loss: 2.0248902638753257

Epoch: 6| Step: 6
Training loss: 2.427950143814087
Validation loss: 2.018782357374827

Epoch: 6| Step: 7
Training loss: 2.0779738426208496
Validation loss: 2.0317198038101196

Epoch: 6| Step: 8
Training loss: 1.9739668369293213
Validation loss: 2.0280593236287436

Epoch: 6| Step: 9
Training loss: 1.360938549041748
Validation loss: 2.0394740303357444

Epoch: 6| Step: 10
Training loss: 1.5015634298324585
Validation loss: 2.0363080501556396

Epoch: 6| Step: 11
Training loss: 2.6299843788146973
Validation loss: 2.040712813536326

Epoch: 6| Step: 12
Training loss: 2.434931755065918
Validation loss: 2.0279553135236106

Epoch: 6| Step: 13
Training loss: 2.3949878215789795
Validation loss: 2.0211919943491616

Epoch: 60| Step: 0
Training loss: 2.4305379390716553
Validation loss: 2.0218658447265625

Epoch: 6| Step: 1
Training loss: 1.8602142333984375
Validation loss: 2.0123790303866067

Epoch: 6| Step: 2
Training loss: 2.0079174041748047
Validation loss: 2.01907616853714

Epoch: 6| Step: 3
Training loss: 2.4274914264678955
Validation loss: 2.0147428711255393

Epoch: 6| Step: 4
Training loss: 2.7551627159118652
Validation loss: 2.0105509757995605

Epoch: 6| Step: 5
Training loss: 2.004565954208374
Validation loss: 2.0129711031913757

Epoch: 6| Step: 6
Training loss: 1.8291916847229004
Validation loss: 2.011551558971405

Epoch: 6| Step: 7
Training loss: 2.7219064235687256
Validation loss: 2.0073536038398743

Epoch: 6| Step: 8
Training loss: 1.9164741039276123
Validation loss: 2.008737862110138

Epoch: 6| Step: 9
Training loss: 2.3697669506073
Validation loss: 2.016191303730011

Epoch: 6| Step: 10
Training loss: 1.6446545124053955
Validation loss: 2.0180922945340476

Epoch: 6| Step: 11
Training loss: 1.852323293685913
Validation loss: 2.0232670307159424

Epoch: 6| Step: 12
Training loss: 2.699908971786499
Validation loss: 2.0216665069262185

Epoch: 6| Step: 13
Training loss: 2.049443006515503
Validation loss: 2.031876186529795

Epoch: 61| Step: 0
Training loss: 2.1504018306732178
Validation loss: 2.032890240351359

Epoch: 6| Step: 1
Training loss: 2.7005107402801514
Validation loss: 2.0526280601819358

Epoch: 6| Step: 2
Training loss: 2.3034355640411377
Validation loss: 2.048570195833842

Epoch: 6| Step: 3
Training loss: 2.223334789276123
Validation loss: 2.038267135620117

Epoch: 6| Step: 4
Training loss: 2.07763671875
Validation loss: 2.031775732835134

Epoch: 6| Step: 5
Training loss: 1.69033944606781
Validation loss: 2.033313790957133

Epoch: 6| Step: 6
Training loss: 2.4460980892181396
Validation loss: 2.027912735939026

Epoch: 6| Step: 7
Training loss: 2.534451961517334
Validation loss: 2.0142482916514077

Epoch: 6| Step: 8
Training loss: 2.3352885246276855
Validation loss: 2.0259727040926614

Epoch: 6| Step: 9
Training loss: 1.7793641090393066
Validation loss: 2.015820403893789

Epoch: 6| Step: 10
Training loss: 2.0549135208129883
Validation loss: 2.0239078402519226

Epoch: 6| Step: 11
Training loss: 2.229464054107666
Validation loss: 2.0233720739682517

Epoch: 6| Step: 12
Training loss: 1.9385532140731812
Validation loss: 2.024936000506083

Epoch: 6| Step: 13
Training loss: 2.4613723754882812
Validation loss: 2.021969437599182

Epoch: 62| Step: 0
Training loss: 2.1281425952911377
Validation loss: 2.0228943626085916

Epoch: 6| Step: 1
Training loss: 2.866861343383789
Validation loss: 2.019137680530548

Epoch: 6| Step: 2
Training loss: 2.2990591526031494
Validation loss: 2.0136890610059104

Epoch: 6| Step: 3
Training loss: 2.108938217163086
Validation loss: 2.017335136731466

Epoch: 6| Step: 4
Training loss: 1.3791730403900146
Validation loss: 2.021282196044922

Epoch: 6| Step: 5
Training loss: 1.9132657051086426
Validation loss: 2.037666141986847

Epoch: 6| Step: 6
Training loss: 2.265491485595703
Validation loss: 2.0382635990778604

Epoch: 6| Step: 7
Training loss: 2.367788791656494
Validation loss: 2.0436932841936746

Epoch: 6| Step: 8
Training loss: 2.7262747287750244
Validation loss: 2.0360711415608725

Epoch: 6| Step: 9
Training loss: 2.243210792541504
Validation loss: 2.030773182710012

Epoch: 6| Step: 10
Training loss: 2.1286439895629883
Validation loss: 2.016439457734426

Epoch: 6| Step: 11
Training loss: 2.1035356521606445
Validation loss: 2.0157416661580405

Epoch: 6| Step: 12
Training loss: 2.61596417427063
Validation loss: 2.0082832972208657

Epoch: 6| Step: 13
Training loss: 1.6829047203063965
Validation loss: 2.011143664518992

Epoch: 63| Step: 0
Training loss: 2.230696678161621
Validation loss: 2.014860769112905

Epoch: 6| Step: 1
Training loss: 2.257472038269043
Validation loss: 2.015914797782898

Epoch: 6| Step: 2
Training loss: 1.868911862373352
Validation loss: 2.017410397529602

Epoch: 6| Step: 3
Training loss: 2.505579710006714
Validation loss: 2.02071475982666

Epoch: 6| Step: 4
Training loss: 2.089667320251465
Validation loss: 2.0261738101641336

Epoch: 6| Step: 5
Training loss: 2.1544039249420166
Validation loss: 2.017856260140737

Epoch: 6| Step: 6
Training loss: 2.3667826652526855
Validation loss: 2.0119693676630654

Epoch: 6| Step: 7
Training loss: 2.061793804168701
Validation loss: 2.021554410457611

Epoch: 6| Step: 8
Training loss: 2.009547233581543
Validation loss: 2.016788442929586

Epoch: 6| Step: 9
Training loss: 1.9421660900115967
Validation loss: 2.0347020626068115

Epoch: 6| Step: 10
Training loss: 1.8924812078475952
Validation loss: 2.0515970985094705

Epoch: 6| Step: 11
Training loss: 2.2280216217041016
Validation loss: 2.0654125610987344

Epoch: 6| Step: 12
Training loss: 2.3812012672424316
Validation loss: 2.057827651500702

Epoch: 6| Step: 13
Training loss: 2.760141372680664
Validation loss: 2.065653125445048

Epoch: 64| Step: 0
Training loss: 1.7042951583862305
Validation loss: 2.0486290653546653

Epoch: 6| Step: 1
Training loss: 1.4817519187927246
Validation loss: 2.0266117453575134

Epoch: 6| Step: 2
Training loss: 2.461184501647949
Validation loss: 2.018924136956533

Epoch: 6| Step: 3
Training loss: 1.9226343631744385
Validation loss: 2.0288280844688416

Epoch: 6| Step: 4
Training loss: 2.653346538543701
Validation loss: 2.0313390493392944

Epoch: 6| Step: 5
Training loss: 2.5717825889587402
Validation loss: 2.033762355645498

Epoch: 6| Step: 6
Training loss: 2.376504898071289
Validation loss: 2.0405840277671814

Epoch: 6| Step: 7
Training loss: 2.3505606651306152
Validation loss: 2.045770267645518

Epoch: 6| Step: 8
Training loss: 2.410031795501709
Validation loss: 2.04783695936203

Epoch: 6| Step: 9
Training loss: 2.260295867919922
Validation loss: 2.042445441087087

Epoch: 6| Step: 10
Training loss: 2.384603977203369
Validation loss: 2.0384413401285806

Epoch: 6| Step: 11
Training loss: 2.520890235900879
Validation loss: 2.0368152459462485

Epoch: 6| Step: 12
Training loss: 1.737222671508789
Validation loss: 2.0344059665997825

Epoch: 6| Step: 13
Training loss: 1.9815287590026855
Validation loss: 2.030228237311045

Epoch: 65| Step: 0
Training loss: 2.097626209259033
Validation loss: 2.026786208152771

Epoch: 6| Step: 1
Training loss: 1.8824409246444702
Validation loss: 2.0220064918200173

Epoch: 6| Step: 2
Training loss: 2.1724064350128174
Validation loss: 2.021705607573191

Epoch: 6| Step: 3
Training loss: 2.233628273010254
Validation loss: 2.0195218324661255

Epoch: 6| Step: 4
Training loss: 2.8345742225646973
Validation loss: 2.0314007798830667

Epoch: 6| Step: 5
Training loss: 1.9942176342010498
Validation loss: 2.0409729878107705

Epoch: 6| Step: 6
Training loss: 2.049790143966675
Validation loss: 2.057022213935852

Epoch: 6| Step: 7
Training loss: 2.403510808944702
Validation loss: 2.0583399534225464

Epoch: 6| Step: 8
Training loss: 1.7247140407562256
Validation loss: 2.0656422773996987

Epoch: 6| Step: 9
Training loss: 2.4344661235809326
Validation loss: 2.0709829727808633

Epoch: 6| Step: 10
Training loss: 2.2056734561920166
Validation loss: 2.0531569123268127

Epoch: 6| Step: 11
Training loss: 2.056255340576172
Validation loss: 2.050819456577301

Epoch: 6| Step: 12
Training loss: 2.18058180809021
Validation loss: 2.0293553471565247

Epoch: 6| Step: 13
Training loss: 2.3912220001220703
Validation loss: 2.0228002667427063

Epoch: 66| Step: 0
Training loss: 2.402005195617676
Validation loss: 2.0197158654530845

Epoch: 6| Step: 1
Training loss: 2.4735660552978516
Validation loss: 2.0147958397865295

Epoch: 6| Step: 2
Training loss: 1.7573062181472778
Validation loss: 2.0153022607167563

Epoch: 6| Step: 3
Training loss: 2.370023727416992
Validation loss: 2.0291213194529214

Epoch: 6| Step: 4
Training loss: 2.2952523231506348
Validation loss: 2.0212047894795737

Epoch: 6| Step: 5
Training loss: 2.3878207206726074
Validation loss: 2.021827975908915

Epoch: 6| Step: 6
Training loss: 2.3989477157592773
Validation loss: 2.0300676822662354

Epoch: 6| Step: 7
Training loss: 2.196251392364502
Validation loss: 2.0327802101771035

Epoch: 6| Step: 8
Training loss: 1.2475754022598267
Validation loss: 2.0228222409884133

Epoch: 6| Step: 9
Training loss: 2.28798246383667
Validation loss: 2.0183812181154885

Epoch: 6| Step: 10
Training loss: 2.069394111633301
Validation loss: 2.0251007080078125

Epoch: 6| Step: 11
Training loss: 2.303561210632324
Validation loss: 2.0375311374664307

Epoch: 6| Step: 12
Training loss: 2.0754761695861816
Validation loss: 2.0601033369700112

Epoch: 6| Step: 13
Training loss: 2.57578706741333
Validation loss: 2.0679872830708823

Epoch: 67| Step: 0
Training loss: 2.3990235328674316
Validation loss: 2.0747459332148233

Epoch: 6| Step: 1
Training loss: 2.930263042449951
Validation loss: 2.056052287419637

Epoch: 6| Step: 2
Training loss: 2.134953498840332
Validation loss: 2.054082155227661

Epoch: 6| Step: 3
Training loss: 1.517683744430542
Validation loss: 2.032719592253367

Epoch: 6| Step: 4
Training loss: 1.9791045188903809
Validation loss: 2.0176729361216226

Epoch: 6| Step: 5
Training loss: 2.288161277770996
Validation loss: 2.0148983796437583

Epoch: 6| Step: 6
Training loss: 2.306875705718994
Validation loss: 2.009820739428202

Epoch: 6| Step: 7
Training loss: 1.3510067462921143
Validation loss: 2.010020593802134

Epoch: 6| Step: 8
Training loss: 2.2740540504455566
Validation loss: 2.0193758408228555

Epoch: 6| Step: 9
Training loss: 2.6594254970550537
Validation loss: 2.0191248059272766

Epoch: 6| Step: 10
Training loss: 2.5766143798828125
Validation loss: 2.021462639172872

Epoch: 6| Step: 11
Training loss: 1.6241917610168457
Validation loss: 2.0198809107144675

Epoch: 6| Step: 12
Training loss: 2.1135239601135254
Validation loss: 2.0175153613090515

Epoch: 6| Step: 13
Training loss: 2.3249926567077637
Validation loss: 2.020488719145457

Epoch: 68| Step: 0
Training loss: 2.4024226665496826
Validation loss: 2.011446257432302

Epoch: 6| Step: 1
Training loss: 2.0363783836364746
Validation loss: 2.0106882055600486

Epoch: 6| Step: 2
Training loss: 1.5095148086547852
Validation loss: 2.0124300122261047

Epoch: 6| Step: 3
Training loss: 2.196094036102295
Validation loss: 2.003435651461283

Epoch: 6| Step: 4
Training loss: 2.6736197471618652
Validation loss: 2.0114510456720986

Epoch: 6| Step: 5
Training loss: 1.8233826160430908
Validation loss: 2.0157657265663147

Epoch: 6| Step: 6
Training loss: 2.2555689811706543
Validation loss: 2.0207569201787314

Epoch: 6| Step: 7
Training loss: 2.5352048873901367
Validation loss: 2.0318119525909424

Epoch: 6| Step: 8
Training loss: 1.9359321594238281
Validation loss: 2.0298699935277305

Epoch: 6| Step: 9
Training loss: 2.108863592147827
Validation loss: 2.0310448010762534

Epoch: 6| Step: 10
Training loss: 2.039672374725342
Validation loss: 2.028233289718628

Epoch: 6| Step: 11
Training loss: 2.046438694000244
Validation loss: 2.0357848604520163

Epoch: 6| Step: 12
Training loss: 2.40059757232666
Validation loss: 2.0262658993403115

Epoch: 6| Step: 13
Training loss: 2.1690778732299805
Validation loss: 2.0299530227979026

Epoch: 69| Step: 0
Training loss: 2.777339458465576
Validation loss: 2.0292643904685974

Epoch: 6| Step: 1
Training loss: 1.4538769721984863
Validation loss: 2.03311957915624

Epoch: 6| Step: 2
Training loss: 2.3192336559295654
Validation loss: 2.019448479016622

Epoch: 6| Step: 3
Training loss: 2.0813889503479004
Validation loss: 2.0174612204233804

Epoch: 6| Step: 4
Training loss: 2.0543932914733887
Validation loss: 2.0145389835039773

Epoch: 6| Step: 5
Training loss: 2.2528629302978516
Validation loss: 2.016208906968435

Epoch: 6| Step: 6
Training loss: 2.31558895111084
Validation loss: 2.011720299720764

Epoch: 6| Step: 7
Training loss: 1.8438634872436523
Validation loss: 2.0155510107676187

Epoch: 6| Step: 8
Training loss: 2.1910934448242188
Validation loss: 2.019562383492788

Epoch: 6| Step: 9
Training loss: 2.196418523788452
Validation loss: 2.0211605429649353

Epoch: 6| Step: 10
Training loss: 2.3546128273010254
Validation loss: 2.024961829185486

Epoch: 6| Step: 11
Training loss: 2.0649704933166504
Validation loss: 2.0294141372044883

Epoch: 6| Step: 12
Training loss: 1.7560932636260986
Validation loss: 2.0377800265947976

Epoch: 6| Step: 13
Training loss: 2.4689559936523438
Validation loss: 2.029059946537018

Epoch: 70| Step: 0
Training loss: 2.820951461791992
Validation loss: 2.0354265173276267

Epoch: 6| Step: 1
Training loss: 2.287478446960449
Validation loss: 2.028678854306539

Epoch: 6| Step: 2
Training loss: 1.9065272808074951
Validation loss: 2.0296917955080667

Epoch: 6| Step: 3
Training loss: 2.1882741451263428
Validation loss: 2.0133173863093057

Epoch: 6| Step: 4
Training loss: 1.799692988395691
Validation loss: 2.0127718448638916

Epoch: 6| Step: 5
Training loss: 1.933214545249939
Validation loss: 2.0219667156537375

Epoch: 6| Step: 6
Training loss: 2.0087244510650635
Validation loss: 2.01469624042511

Epoch: 6| Step: 7
Training loss: 2.1630210876464844
Validation loss: 2.0181678533554077

Epoch: 6| Step: 8
Training loss: 2.5638539791107178
Validation loss: 2.0178364912668862

Epoch: 6| Step: 9
Training loss: 2.5460903644561768
Validation loss: 2.0160705049832663

Epoch: 6| Step: 10
Training loss: 1.7013037204742432
Validation loss: 2.014213224252065

Epoch: 6| Step: 11
Training loss: 2.3425893783569336
Validation loss: 2.01343834400177

Epoch: 6| Step: 12
Training loss: 2.1543190479278564
Validation loss: 2.018008291721344

Epoch: 6| Step: 13
Training loss: 1.7095563411712646
Validation loss: 2.0171573758125305

Epoch: 71| Step: 0
Training loss: 2.1592350006103516
Validation loss: 2.0279317498207092

Epoch: 6| Step: 1
Training loss: 2.651097297668457
Validation loss: 2.0272545218467712

Epoch: 6| Step: 2
Training loss: 2.006923198699951
Validation loss: 2.03709884484609

Epoch: 6| Step: 3
Training loss: 2.3443048000335693
Validation loss: 2.0305261413256326

Epoch: 6| Step: 4
Training loss: 2.4336729049682617
Validation loss: 2.041534721851349

Epoch: 6| Step: 5
Training loss: 1.6427319049835205
Validation loss: 2.0406647324562073

Epoch: 6| Step: 6
Training loss: 2.1199710369110107
Validation loss: 2.039084533850352

Epoch: 6| Step: 7
Training loss: 1.3314025402069092
Validation loss: 2.0416104594866433

Epoch: 6| Step: 8
Training loss: 1.7567088603973389
Validation loss: 2.0406827330589294

Epoch: 6| Step: 9
Training loss: 2.3378424644470215
Validation loss: 2.0458668867746987

Epoch: 6| Step: 10
Training loss: 2.403286933898926
Validation loss: 2.0353121360143027

Epoch: 6| Step: 11
Training loss: 2.7082741260528564
Validation loss: 2.0225775837898254

Epoch: 6| Step: 12
Training loss: 1.7704644203186035
Validation loss: 2.0171226859092712

Epoch: 6| Step: 13
Training loss: 2.244253635406494
Validation loss: 2.0215514103571572

Epoch: 72| Step: 0
Training loss: 2.5202364921569824
Validation loss: 2.0166964332262673

Epoch: 6| Step: 1
Training loss: 2.31184983253479
Validation loss: 2.0242539445559182

Epoch: 6| Step: 2
Training loss: 1.5031659603118896
Validation loss: 2.0257794857025146

Epoch: 6| Step: 3
Training loss: 1.8387691974639893
Validation loss: 2.0142224629720054

Epoch: 6| Step: 4
Training loss: 2.335264205932617
Validation loss: 2.0154998699824014

Epoch: 6| Step: 5
Training loss: 2.0121610164642334
Validation loss: 2.01183811823527

Epoch: 6| Step: 6
Training loss: 2.2892136573791504
Validation loss: 2.0057324369748435

Epoch: 6| Step: 7
Training loss: 2.828416585922241
Validation loss: 2.0100377003351846

Epoch: 6| Step: 8
Training loss: 1.92681086063385
Validation loss: 2.0088727672894797

Epoch: 6| Step: 9
Training loss: 1.627928376197815
Validation loss: 2.014614979426066

Epoch: 6| Step: 10
Training loss: 1.9995098114013672
Validation loss: 2.0203936100006104

Epoch: 6| Step: 11
Training loss: 2.0220885276794434
Validation loss: 2.033055583635966

Epoch: 6| Step: 12
Training loss: 2.043238639831543
Validation loss: 2.0453120867411294

Epoch: 6| Step: 13
Training loss: 2.7910542488098145
Validation loss: 2.04656853278478

Epoch: 73| Step: 0
Training loss: 2.354985237121582
Validation loss: 2.052038391431173

Epoch: 6| Step: 1
Training loss: 2.132326126098633
Validation loss: 2.060052474339803

Epoch: 6| Step: 2
Training loss: 2.2918314933776855
Validation loss: 2.0553418596585593

Epoch: 6| Step: 3
Training loss: 1.4946709871292114
Validation loss: 2.0459096829096475

Epoch: 6| Step: 4
Training loss: 1.9202662706375122
Validation loss: 2.0356352726618447

Epoch: 6| Step: 5
Training loss: 1.9080373048782349
Validation loss: 2.026582976182302

Epoch: 6| Step: 6
Training loss: 1.9091553688049316
Validation loss: 2.01905757188797

Epoch: 6| Step: 7
Training loss: 2.5356202125549316
Validation loss: 2.0169320106506348

Epoch: 6| Step: 8
Training loss: 2.2422847747802734
Validation loss: 2.0199689467748008

Epoch: 6| Step: 9
Training loss: 2.3084001541137695
Validation loss: 2.0187544425328574

Epoch: 6| Step: 10
Training loss: 1.9122575521469116
Validation loss: 2.0168970028559365

Epoch: 6| Step: 11
Training loss: 2.6735222339630127
Validation loss: 2.014223595460256

Epoch: 6| Step: 12
Training loss: 2.0954089164733887
Validation loss: 2.0190938115119934

Epoch: 6| Step: 13
Training loss: 2.2710041999816895
Validation loss: 2.022673408190409

Epoch: 74| Step: 0
Training loss: 2.0178544521331787
Validation loss: 2.023743828137716

Epoch: 6| Step: 1
Training loss: 1.9839227199554443
Validation loss: 2.022067606449127

Epoch: 6| Step: 2
Training loss: 2.5943782329559326
Validation loss: 2.029979924360911

Epoch: 6| Step: 3
Training loss: 2.73874831199646
Validation loss: 2.0308228929837546

Epoch: 6| Step: 4
Training loss: 2.156139612197876
Validation loss: 2.032793958981832

Epoch: 6| Step: 5
Training loss: 2.093600273132324
Validation loss: 2.0341182947158813

Epoch: 6| Step: 6
Training loss: 1.8050321340560913
Validation loss: 2.03665429353714

Epoch: 6| Step: 7
Training loss: 1.8555846214294434
Validation loss: 2.0422568718592324

Epoch: 6| Step: 8
Training loss: 1.9020856618881226
Validation loss: 2.0495654940605164

Epoch: 6| Step: 9
Training loss: 1.631277084350586
Validation loss: 2.0553455154101052

Epoch: 6| Step: 10
Training loss: 2.457038164138794
Validation loss: 2.0509909788767495

Epoch: 6| Step: 11
Training loss: 2.409322738647461
Validation loss: 2.046327074368795

Epoch: 6| Step: 12
Training loss: 2.46488094329834
Validation loss: 2.0462984641393027

Epoch: 6| Step: 13
Training loss: 1.9908415079116821
Validation loss: 2.0311957597732544

Epoch: 75| Step: 0
Training loss: 2.0050864219665527
Validation loss: 2.0297231674194336

Epoch: 6| Step: 1
Training loss: 2.309685707092285
Validation loss: 2.0238600174585977

Epoch: 6| Step: 2
Training loss: 2.3654816150665283
Validation loss: 2.0362666845321655

Epoch: 6| Step: 3
Training loss: 1.994998812675476
Validation loss: 2.0327985286712646

Epoch: 6| Step: 4
Training loss: 2.1054937839508057
Validation loss: 2.0396030942598977

Epoch: 6| Step: 5
Training loss: 2.0895488262176514
Validation loss: 2.0504132310549417

Epoch: 6| Step: 6
Training loss: 2.477332592010498
Validation loss: 2.058661142985026

Epoch: 6| Step: 7
Training loss: 2.3492391109466553
Validation loss: 2.0584231416384378

Epoch: 6| Step: 8
Training loss: 1.6496899127960205
Validation loss: 2.058167338371277

Epoch: 6| Step: 9
Training loss: 2.0778276920318604
Validation loss: 2.0527379711469016

Epoch: 6| Step: 10
Training loss: 2.3006591796875
Validation loss: 2.0548147161801658

Epoch: 6| Step: 11
Training loss: 1.5781562328338623
Validation loss: 2.0392113526662192

Epoch: 6| Step: 12
Training loss: 1.9548033475875854
Validation loss: 2.03141454855601

Epoch: 6| Step: 13
Training loss: 2.869680404663086
Validation loss: 2.032997210820516

Epoch: 76| Step: 0
Training loss: 1.7581758499145508
Validation loss: 2.028164486090342

Epoch: 6| Step: 1
Training loss: 1.8713085651397705
Validation loss: 2.018715222676595

Epoch: 6| Step: 2
Training loss: 2.6032462120056152
Validation loss: 2.012161910533905

Epoch: 6| Step: 3
Training loss: 2.240877389907837
Validation loss: 2.007395644982656

Epoch: 6| Step: 4
Training loss: 1.9451780319213867
Validation loss: 2.0086711843808494

Epoch: 6| Step: 5
Training loss: 2.0828499794006348
Validation loss: 2.0093889435132346

Epoch: 6| Step: 6
Training loss: 1.7449265718460083
Validation loss: 2.0098619063695273

Epoch: 6| Step: 7
Training loss: 2.529226064682007
Validation loss: 2.0045137206713357

Epoch: 6| Step: 8
Training loss: 2.1950535774230957
Validation loss: 2.0007211764653525

Epoch: 6| Step: 9
Training loss: 2.841611862182617
Validation loss: 2.01010133822759

Epoch: 6| Step: 10
Training loss: 1.8019803762435913
Validation loss: 2.0083623925844827

Epoch: 6| Step: 11
Training loss: 2.077241897583008
Validation loss: 2.01365194718043

Epoch: 6| Step: 12
Training loss: 1.8431448936462402
Validation loss: 2.0189819733301797

Epoch: 6| Step: 13
Training loss: 2.3344411849975586
Validation loss: 2.025018493334452

Epoch: 77| Step: 0
Training loss: 2.537020206451416
Validation loss: 2.0207595825195312

Epoch: 6| Step: 1
Training loss: 2.314873695373535
Validation loss: 2.0227568546930947

Epoch: 6| Step: 2
Training loss: 2.2747888565063477
Validation loss: 2.021161158879598

Epoch: 6| Step: 3
Training loss: 1.579942226409912
Validation loss: 2.020459453264872

Epoch: 6| Step: 4
Training loss: 2.1257615089416504
Validation loss: 2.0235761205355325

Epoch: 6| Step: 5
Training loss: 2.4692296981811523
Validation loss: 2.0203046401341758

Epoch: 6| Step: 6
Training loss: 2.2353811264038086
Validation loss: 2.0135974685351052

Epoch: 6| Step: 7
Training loss: 2.018630266189575
Validation loss: 2.0132983128229776

Epoch: 6| Step: 8
Training loss: 1.926880121231079
Validation loss: 2.022267440954844

Epoch: 6| Step: 9
Training loss: 1.704176664352417
Validation loss: 2.0222380956014

Epoch: 6| Step: 10
Training loss: 1.6524146795272827
Validation loss: 2.028128703435262

Epoch: 6| Step: 11
Training loss: 2.155756711959839
Validation loss: 2.028720239798228

Epoch: 6| Step: 12
Training loss: 2.6419224739074707
Validation loss: 2.0240453481674194

Epoch: 6| Step: 13
Training loss: 1.964459776878357
Validation loss: 2.0233997106552124

Epoch: 78| Step: 0
Training loss: 1.4889857769012451
Validation loss: 2.0330817103385925

Epoch: 6| Step: 1
Training loss: 2.937239170074463
Validation loss: 2.041353980700175

Epoch: 6| Step: 2
Training loss: 2.274440288543701
Validation loss: 2.0391210516293845

Epoch: 6| Step: 3
Training loss: 2.484423875808716
Validation loss: 2.0301504731178284

Epoch: 6| Step: 4
Training loss: 1.661447286605835
Validation loss: 2.039827585220337

Epoch: 6| Step: 5
Training loss: 1.96640944480896
Validation loss: 2.03059975306193

Epoch: 6| Step: 6
Training loss: 2.2654731273651123
Validation loss: 2.0288278261820474

Epoch: 6| Step: 7
Training loss: 2.345926284790039
Validation loss: 2.0277058680852256

Epoch: 6| Step: 8
Training loss: 2.207132339477539
Validation loss: 2.031888405481974

Epoch: 6| Step: 9
Training loss: 2.058523178100586
Validation loss: 2.0287214318911233

Epoch: 6| Step: 10
Training loss: 2.2397501468658447
Validation loss: 2.0322789748509726

Epoch: 6| Step: 11
Training loss: 1.7214508056640625
Validation loss: 2.0344041188557944

Epoch: 6| Step: 12
Training loss: 1.7666906118392944
Validation loss: 2.0290061434110007

Epoch: 6| Step: 13
Training loss: 2.3559110164642334
Validation loss: 2.0352779825528464

Epoch: 79| Step: 0
Training loss: 2.149312973022461
Validation loss: 2.03447155157725

Epoch: 6| Step: 1
Training loss: 2.141903877258301
Validation loss: 2.03552116950353

Epoch: 6| Step: 2
Training loss: 2.0634679794311523
Validation loss: 2.0346091190973916

Epoch: 6| Step: 3
Training loss: 1.9252921342849731
Validation loss: 2.0244532028834024

Epoch: 6| Step: 4
Training loss: 2.219066619873047
Validation loss: 2.0346371134122214

Epoch: 6| Step: 5
Training loss: 2.144042491912842
Validation loss: 2.032086749871572

Epoch: 6| Step: 6
Training loss: 2.324911117553711
Validation loss: 2.02363787094752

Epoch: 6| Step: 7
Training loss: 2.722353458404541
Validation loss: 2.025507926940918

Epoch: 6| Step: 8
Training loss: 1.6649787425994873
Validation loss: 2.0250565012296042

Epoch: 6| Step: 9
Training loss: 2.5740699768066406
Validation loss: 2.0311623414357505

Epoch: 6| Step: 10
Training loss: 2.5175044536590576
Validation loss: 2.0238048235575357

Epoch: 6| Step: 11
Training loss: 2.0658910274505615
Validation loss: 2.02053169409434

Epoch: 6| Step: 12
Training loss: 1.7321529388427734
Validation loss: 2.0342146952946982

Epoch: 6| Step: 13
Training loss: 1.3176391124725342
Validation loss: 2.0279202461242676

Epoch: 80| Step: 0
Training loss: 1.36479914188385
Validation loss: 2.032747427622477

Epoch: 6| Step: 1
Training loss: 2.1049087047576904
Validation loss: 2.031731923421224

Epoch: 6| Step: 2
Training loss: 2.060896158218384
Validation loss: 2.0261348485946655

Epoch: 6| Step: 3
Training loss: 2.3893160820007324
Validation loss: 2.023109416166941

Epoch: 6| Step: 4
Training loss: 2.698817729949951
Validation loss: 2.0167129834493003

Epoch: 6| Step: 5
Training loss: 2.4318156242370605
Validation loss: 2.0232091347376504

Epoch: 6| Step: 6
Training loss: 1.7549656629562378
Validation loss: 2.0192145506540933

Epoch: 6| Step: 7
Training loss: 2.652578353881836
Validation loss: 2.016249497731527

Epoch: 6| Step: 8
Training loss: 1.8416507244110107
Validation loss: 2.0285916924476624

Epoch: 6| Step: 9
Training loss: 2.882427930831909
Validation loss: 2.022554079691569

Epoch: 6| Step: 10
Training loss: 2.186955451965332
Validation loss: 2.0280443827311196

Epoch: 6| Step: 11
Training loss: 1.8928574323654175
Validation loss: 2.0233553647994995

Epoch: 6| Step: 12
Training loss: 1.5225555896759033
Validation loss: 2.019366959730784

Epoch: 6| Step: 13
Training loss: 1.7945762872695923
Validation loss: 2.027306238810221

Epoch: 81| Step: 0
Training loss: 2.789000988006592
Validation loss: 2.0329564809799194

Epoch: 6| Step: 1
Training loss: 1.6664170026779175
Validation loss: 2.0278494159380593

Epoch: 6| Step: 2
Training loss: 1.8131630420684814
Validation loss: 2.0254050691922507

Epoch: 6| Step: 3
Training loss: 1.4089000225067139
Validation loss: 2.026039958000183

Epoch: 6| Step: 4
Training loss: 2.3641357421875
Validation loss: 2.0108492771784463

Epoch: 6| Step: 5
Training loss: 2.002523183822632
Validation loss: 2.0136581659317017

Epoch: 6| Step: 6
Training loss: 1.9779473543167114
Validation loss: 2.0164817770322165

Epoch: 6| Step: 7
Training loss: 1.7330011129379272
Validation loss: 2.012482762336731

Epoch: 6| Step: 8
Training loss: 2.1384198665618896
Validation loss: 2.010879119237264

Epoch: 6| Step: 9
Training loss: 2.298917770385742
Validation loss: 2.0067405303319297

Epoch: 6| Step: 10
Training loss: 2.512157917022705
Validation loss: 2.0091063181559243

Epoch: 6| Step: 11
Training loss: 2.7761192321777344
Validation loss: 2.0184945662816367

Epoch: 6| Step: 12
Training loss: 2.2323832511901855
Validation loss: 2.019918203353882

Epoch: 6| Step: 13
Training loss: 2.0106201171875
Validation loss: 2.026874542236328

Epoch: 82| Step: 0
Training loss: 2.5506515502929688
Validation loss: 2.0285083850224814

Epoch: 6| Step: 1
Training loss: 2.0141468048095703
Validation loss: 2.0451715191205344

Epoch: 6| Step: 2
Training loss: 2.1856024265289307
Validation loss: 2.0438102086385093

Epoch: 6| Step: 3
Training loss: 2.60658597946167
Validation loss: 2.0507280627886453

Epoch: 6| Step: 4
Training loss: 1.5575530529022217
Validation loss: 2.0321136116981506

Epoch: 6| Step: 5
Training loss: 2.494433879852295
Validation loss: 2.019166946411133

Epoch: 6| Step: 6
Training loss: 2.56038761138916
Validation loss: 2.015387316544851

Epoch: 6| Step: 7
Training loss: 2.183722734451294
Validation loss: 2.015531599521637

Epoch: 6| Step: 8
Training loss: 1.7356503009796143
Validation loss: 2.011171738306681

Epoch: 6| Step: 9
Training loss: 2.2689127922058105
Validation loss: 2.0138356486956277

Epoch: 6| Step: 10
Training loss: 1.7840750217437744
Validation loss: 2.018948495388031

Epoch: 6| Step: 11
Training loss: 2.0061721801757812
Validation loss: 2.013305922349294

Epoch: 6| Step: 12
Training loss: 1.9840354919433594
Validation loss: 2.019119660059611

Epoch: 6| Step: 13
Training loss: 1.9860808849334717
Validation loss: 2.023976425329844

Epoch: 83| Step: 0
Training loss: 2.8217859268188477
Validation loss: 2.0201494495073953

Epoch: 6| Step: 1
Training loss: 1.778403401374817
Validation loss: 2.020015219847361

Epoch: 6| Step: 2
Training loss: 1.6912317276000977
Validation loss: 2.0148608088493347

Epoch: 6| Step: 3
Training loss: 2.3470845222473145
Validation loss: 2.0121829509735107

Epoch: 6| Step: 4
Training loss: 1.5774786472320557
Validation loss: 2.015008489290873

Epoch: 6| Step: 5
Training loss: 1.8761208057403564
Validation loss: 2.011716981728872

Epoch: 6| Step: 6
Training loss: 2.751077175140381
Validation loss: 2.0162954330444336

Epoch: 6| Step: 7
Training loss: 2.581465721130371
Validation loss: 2.016098141670227

Epoch: 6| Step: 8
Training loss: 1.897659182548523
Validation loss: 2.02513188123703

Epoch: 6| Step: 9
Training loss: 2.264448642730713
Validation loss: 2.03472630182902

Epoch: 6| Step: 10
Training loss: 2.1709952354431152
Validation loss: 2.0374520222345986

Epoch: 6| Step: 11
Training loss: 1.4460357427597046
Validation loss: 2.0410062670707703

Epoch: 6| Step: 12
Training loss: 2.2591681480407715
Validation loss: 2.0381933450698853

Epoch: 6| Step: 13
Training loss: 2.286585807800293
Validation loss: 2.0392402013142905

Epoch: 84| Step: 0
Training loss: 2.362399101257324
Validation loss: 2.038647711277008

Epoch: 6| Step: 1
Training loss: 2.3852803707122803
Validation loss: 2.0318925182024636

Epoch: 6| Step: 2
Training loss: 2.0260095596313477
Validation loss: 2.028643469015757

Epoch: 6| Step: 3
Training loss: 2.046862840652466
Validation loss: 2.020098884900411

Epoch: 6| Step: 4
Training loss: 2.4223365783691406
Validation loss: 2.0303868850072226

Epoch: 6| Step: 5
Training loss: 2.08685564994812
Validation loss: 2.021602511405945

Epoch: 6| Step: 6
Training loss: 1.8626439571380615
Validation loss: 2.029879629611969

Epoch: 6| Step: 7
Training loss: 2.07352876663208
Validation loss: 2.0213489135106406

Epoch: 6| Step: 8
Training loss: 1.7687318325042725
Validation loss: 2.014689644177755

Epoch: 6| Step: 9
Training loss: 1.6169230937957764
Validation loss: 2.0088459253311157

Epoch: 6| Step: 10
Training loss: 2.562126636505127
Validation loss: 1.999899168809255

Epoch: 6| Step: 11
Training loss: 2.180053234100342
Validation loss: 2.015617589155833

Epoch: 6| Step: 12
Training loss: 1.8917523622512817
Validation loss: 2.0101271271705627

Epoch: 6| Step: 13
Training loss: 2.171881675720215
Validation loss: 2.0306950410207114

Epoch: 85| Step: 0
Training loss: 2.2752318382263184
Validation loss: 2.0278007984161377

Epoch: 6| Step: 1
Training loss: 1.94862699508667
Validation loss: 2.0246107379595437

Epoch: 6| Step: 2
Training loss: 2.282268762588501
Validation loss: 2.034724175930023

Epoch: 6| Step: 3
Training loss: 2.376702308654785
Validation loss: 2.0301607847213745

Epoch: 6| Step: 4
Training loss: 2.23264741897583
Validation loss: 2.0328421592712402

Epoch: 6| Step: 5
Training loss: 2.53863525390625
Validation loss: 2.032869338989258

Epoch: 6| Step: 6
Training loss: 2.048013210296631
Validation loss: 2.0330907106399536

Epoch: 6| Step: 7
Training loss: 2.0469970703125
Validation loss: 2.0302241444587708

Epoch: 6| Step: 8
Training loss: 1.2091494798660278
Validation loss: 2.03139994541804

Epoch: 6| Step: 9
Training loss: 2.0179526805877686
Validation loss: 2.0254218777020774

Epoch: 6| Step: 10
Training loss: 2.2497591972351074
Validation loss: 2.0185043613115945

Epoch: 6| Step: 11
Training loss: 2.202202558517456
Validation loss: 2.0211307605107627

Epoch: 6| Step: 12
Training loss: 1.8604596853256226
Validation loss: 2.0246522029240928

Epoch: 6| Step: 13
Training loss: 2.134467601776123
Validation loss: 2.01790581146876

Epoch: 86| Step: 0
Training loss: 2.2825183868408203
Validation loss: 2.0187987685203552

Epoch: 6| Step: 1
Training loss: 2.040566921234131
Validation loss: 2.020334760348002

Epoch: 6| Step: 2
Training loss: 1.87117600440979
Validation loss: 2.017380436261495

Epoch: 6| Step: 3
Training loss: 1.6769707202911377
Validation loss: 2.026281992594401

Epoch: 6| Step: 4
Training loss: 1.9035909175872803
Validation loss: 2.0145400762557983

Epoch: 6| Step: 5
Training loss: 2.0927538871765137
Validation loss: 2.033429503440857

Epoch: 6| Step: 6
Training loss: 2.039064884185791
Validation loss: 2.031587024529775

Epoch: 6| Step: 7
Training loss: 2.302900791168213
Validation loss: 2.0354355772336326

Epoch: 6| Step: 8
Training loss: 2.239121913909912
Validation loss: 2.0354363918304443

Epoch: 6| Step: 9
Training loss: 3.145608901977539
Validation loss: 2.0293792486190796

Epoch: 6| Step: 10
Training loss: 2.167276382446289
Validation loss: 2.0317116578420005

Epoch: 6| Step: 11
Training loss: 1.4358899593353271
Validation loss: 2.0241007606188455

Epoch: 6| Step: 12
Training loss: 1.9192954301834106
Validation loss: 2.019399086634318

Epoch: 6| Step: 13
Training loss: 2.6352295875549316
Validation loss: 2.0186453262964883

Epoch: 87| Step: 0
Training loss: 1.5226243734359741
Validation loss: 2.018343985080719

Epoch: 6| Step: 1
Training loss: 1.7216745615005493
Validation loss: 2.0234433015187583

Epoch: 6| Step: 2
Training loss: 2.2148513793945312
Validation loss: 2.018356959025065

Epoch: 6| Step: 3
Training loss: 2.1958537101745605
Validation loss: 2.015528917312622

Epoch: 6| Step: 4
Training loss: 1.9910732507705688
Validation loss: 2.0261253913243613

Epoch: 6| Step: 5
Training loss: 2.5069520473480225
Validation loss: 2.020451525847117

Epoch: 6| Step: 6
Training loss: 2.264801025390625
Validation loss: 2.022630989551544

Epoch: 6| Step: 7
Training loss: 2.4677774906158447
Validation loss: 2.0256616671880088

Epoch: 6| Step: 8
Training loss: 2.2714600563049316
Validation loss: 2.036431074142456

Epoch: 6| Step: 9
Training loss: 2.5105905532836914
Validation loss: 2.0185354153315225

Epoch: 6| Step: 10
Training loss: 1.5382394790649414
Validation loss: 2.0280723770459494

Epoch: 6| Step: 11
Training loss: 2.0513200759887695
Validation loss: 2.0254620711008706

Epoch: 6| Step: 12
Training loss: 2.7396185398101807
Validation loss: 2.022190809249878

Epoch: 6| Step: 13
Training loss: 1.2369897365570068
Validation loss: 2.0298908750216165

Epoch: 88| Step: 0
Training loss: 1.792624592781067
Validation loss: 2.032857040564219

Epoch: 6| Step: 1
Training loss: 1.8323180675506592
Validation loss: 2.032712936401367

Epoch: 6| Step: 2
Training loss: 1.7227909564971924
Validation loss: 2.0399377743403115

Epoch: 6| Step: 3
Training loss: 2.464107036590576
Validation loss: 2.025941570599874

Epoch: 6| Step: 4
Training loss: 2.1835598945617676
Validation loss: 2.0382119019826255

Epoch: 6| Step: 5
Training loss: 1.6771788597106934
Validation loss: 2.028366982936859

Epoch: 6| Step: 6
Training loss: 2.43296480178833
Validation loss: 2.0285618702570596

Epoch: 6| Step: 7
Training loss: 2.479433536529541
Validation loss: 2.023504932721456

Epoch: 6| Step: 8
Training loss: 1.89914870262146
Validation loss: 2.0307586789131165

Epoch: 6| Step: 9
Training loss: 1.8430240154266357
Validation loss: 2.0099593798319497

Epoch: 6| Step: 10
Training loss: 2.3077425956726074
Validation loss: 2.0239605704943338

Epoch: 6| Step: 11
Training loss: 2.867291212081909
Validation loss: 2.0182796120643616

Epoch: 6| Step: 12
Training loss: 1.8890531063079834
Validation loss: 2.0157337387402854

Epoch: 6| Step: 13
Training loss: 1.9092144966125488
Validation loss: 2.024162928263346

Epoch: 89| Step: 0
Training loss: 2.0351667404174805
Validation loss: 2.014438589413961

Epoch: 6| Step: 1
Training loss: 2.206423759460449
Validation loss: 2.0192313194274902

Epoch: 6| Step: 2
Training loss: 2.164599895477295
Validation loss: 2.0192915002504983

Epoch: 6| Step: 3
Training loss: 1.61623215675354
Validation loss: 2.017309049765269

Epoch: 6| Step: 4
Training loss: 2.207814931869507
Validation loss: 2.0318819483121238

Epoch: 6| Step: 5
Training loss: 1.794360637664795
Validation loss: 2.0176402926445007

Epoch: 6| Step: 6
Training loss: 2.1459453105926514
Validation loss: 2.0287541349728904

Epoch: 6| Step: 7
Training loss: 1.9276106357574463
Validation loss: 2.0280974308649697

Epoch: 6| Step: 8
Training loss: 1.6489189863204956
Validation loss: 2.03771964708964

Epoch: 6| Step: 9
Training loss: 2.557816505432129
Validation loss: 2.042992651462555

Epoch: 6| Step: 10
Training loss: 2.404184579849243
Validation loss: 2.056845188140869

Epoch: 6| Step: 11
Training loss: 2.2991535663604736
Validation loss: 2.042095144589742

Epoch: 6| Step: 12
Training loss: 2.0789077281951904
Validation loss: 2.034823795159658

Epoch: 6| Step: 13
Training loss: 2.351511240005493
Validation loss: 2.042728900909424

Epoch: 90| Step: 0
Training loss: 1.9021506309509277
Validation loss: 2.0222250620524087

Epoch: 6| Step: 1
Training loss: 2.6962227821350098
Validation loss: 2.026907821496328

Epoch: 6| Step: 2
Training loss: 1.7755481004714966
Validation loss: 2.0229288140932717

Epoch: 6| Step: 3
Training loss: 2.559783458709717
Validation loss: 2.0124363700548806

Epoch: 6| Step: 4
Training loss: 2.0725159645080566
Validation loss: 2.0155147910118103

Epoch: 6| Step: 5
Training loss: 2.049945831298828
Validation loss: 2.012600541114807

Epoch: 6| Step: 6
Training loss: 1.7180817127227783
Validation loss: 2.0164069533348083

Epoch: 6| Step: 7
Training loss: 2.2948880195617676
Validation loss: 2.017177085081736

Epoch: 6| Step: 8
Training loss: 2.0973143577575684
Validation loss: 2.023983359336853

Epoch: 6| Step: 9
Training loss: 1.624267578125
Validation loss: 2.0197332898775735

Epoch: 6| Step: 10
Training loss: 1.8960500955581665
Validation loss: 2.0171899596850076

Epoch: 6| Step: 11
Training loss: 1.7166242599487305
Validation loss: 2.0120805899302163

Epoch: 6| Step: 12
Training loss: 2.5410518646240234
Validation loss: 2.0154761473337808

Epoch: 6| Step: 13
Training loss: 2.391784191131592
Validation loss: 2.027127663294474

Epoch: 91| Step: 0
Training loss: 1.9811735153198242
Validation loss: 2.0244180162747702

Epoch: 6| Step: 1
Training loss: 1.4127790927886963
Validation loss: 2.0261057416598

Epoch: 6| Step: 2
Training loss: 2.4543991088867188
Validation loss: 2.028251667817434

Epoch: 6| Step: 3
Training loss: 1.4906758069992065
Validation loss: 2.011200507481893

Epoch: 6| Step: 4
Training loss: 2.372884750366211
Validation loss: 2.0077871481577554

Epoch: 6| Step: 5
Training loss: 2.761284351348877
Validation loss: 2.0157692432403564

Epoch: 6| Step: 6
Training loss: 1.731339454650879
Validation loss: 2.017337123552958

Epoch: 6| Step: 7
Training loss: 1.963538646697998
Validation loss: 2.026476422945658

Epoch: 6| Step: 8
Training loss: 2.5788116455078125
Validation loss: 2.0195162296295166

Epoch: 6| Step: 9
Training loss: 2.1858510971069336
Validation loss: 2.021426280339559

Epoch: 6| Step: 10
Training loss: 2.100358486175537
Validation loss: 2.022404650847117

Epoch: 6| Step: 11
Training loss: 2.482083797454834
Validation loss: 2.0244948665301004

Epoch: 6| Step: 12
Training loss: 1.4791313409805298
Validation loss: 2.0231910149256387

Epoch: 6| Step: 13
Training loss: 2.4028897285461426
Validation loss: 2.0301533738772073

Epoch: 92| Step: 0
Training loss: 2.745741128921509
Validation loss: 2.026344060897827

Epoch: 6| Step: 1
Training loss: 2.1230990886688232
Validation loss: 2.032684604326884

Epoch: 6| Step: 2
Training loss: 2.286945343017578
Validation loss: 2.027971088886261

Epoch: 6| Step: 3
Training loss: 1.9164116382598877
Validation loss: 2.0226343870162964

Epoch: 6| Step: 4
Training loss: 2.261721611022949
Validation loss: 2.0292221705118814

Epoch: 6| Step: 5
Training loss: 1.761055588722229
Validation loss: 2.0250139037768045

Epoch: 6| Step: 6
Training loss: 2.370169162750244
Validation loss: 2.0281989773114524

Epoch: 6| Step: 7
Training loss: 1.3835757970809937
Validation loss: 2.0302860140800476

Epoch: 6| Step: 8
Training loss: 2.0295045375823975
Validation loss: 2.025946259498596

Epoch: 6| Step: 9
Training loss: 2.4277503490448
Validation loss: 2.0280659000078836

Epoch: 6| Step: 10
Training loss: 2.1254653930664062
Validation loss: 2.0242177645365396

Epoch: 6| Step: 11
Training loss: 1.7509541511535645
Validation loss: 2.021877884864807

Epoch: 6| Step: 12
Training loss: 2.1449522972106934
Validation loss: 2.0204960107803345

Epoch: 6| Step: 13
Training loss: 1.7333265542984009
Validation loss: 2.023764510949453

Epoch: 93| Step: 0
Training loss: 1.8723292350769043
Validation loss: 2.0244339307149253

Epoch: 6| Step: 1
Training loss: 1.893677830696106
Validation loss: 2.02689536412557

Epoch: 6| Step: 2
Training loss: 2.4041731357574463
Validation loss: 2.035919666290283

Epoch: 6| Step: 3
Training loss: 2.280944347381592
Validation loss: 2.0294123689333596

Epoch: 6| Step: 4
Training loss: 2.5977163314819336
Validation loss: 2.0208611687024436

Epoch: 6| Step: 5
Training loss: 1.822798728942871
Validation loss: 2.018631895383199

Epoch: 6| Step: 6
Training loss: 1.6287322044372559
Validation loss: 2.0278598268826804

Epoch: 6| Step: 7
Training loss: 1.9803308248519897
Validation loss: 2.0321222146352134

Epoch: 6| Step: 8
Training loss: 2.4640450477600098
Validation loss: 2.029460390408834

Epoch: 6| Step: 9
Training loss: 1.5803178548812866
Validation loss: 2.024083395799001

Epoch: 6| Step: 10
Training loss: 2.241178274154663
Validation loss: 2.0265597105026245

Epoch: 6| Step: 11
Training loss: 2.105344772338867
Validation loss: 2.0298410256703696

Epoch: 6| Step: 12
Training loss: 1.779764175415039
Validation loss: 2.029818614323934

Epoch: 6| Step: 13
Training loss: 2.7568774223327637
Validation loss: 2.0351004004478455

Epoch: 94| Step: 0
Training loss: 1.489192008972168
Validation loss: 2.028349498907725

Epoch: 6| Step: 1
Training loss: 1.9027752876281738
Validation loss: 2.0297688643137612

Epoch: 6| Step: 2
Training loss: 2.3547496795654297
Validation loss: 2.023572405179342

Epoch: 6| Step: 3
Training loss: 1.977351188659668
Validation loss: 2.032337705294291

Epoch: 6| Step: 4
Training loss: 2.552103281021118
Validation loss: 2.035057783126831

Epoch: 6| Step: 5
Training loss: 1.7978887557983398
Validation loss: 2.0279850165049234

Epoch: 6| Step: 6
Training loss: 1.7672804594039917
Validation loss: 2.02393231789271

Epoch: 6| Step: 7
Training loss: 2.3005783557891846
Validation loss: 2.027822256088257

Epoch: 6| Step: 8
Training loss: 2.602423906326294
Validation loss: 2.0405784050623574

Epoch: 6| Step: 9
Training loss: 1.9939088821411133
Validation loss: 2.0347777803738913

Epoch: 6| Step: 10
Training loss: 2.0393154621124268
Validation loss: 2.0276066263516745

Epoch: 6| Step: 11
Training loss: 2.4592409133911133
Validation loss: 2.0258151491483054

Epoch: 6| Step: 12
Training loss: 1.9886530637741089
Validation loss: 2.022441327571869

Epoch: 6| Step: 13
Training loss: 2.0539779663085938
Validation loss: 2.0294212301572165

Epoch: 95| Step: 0
Training loss: 2.370760917663574
Validation loss: 2.0247687300046286

Epoch: 6| Step: 1
Training loss: 1.7582483291625977
Validation loss: 2.0188746054967246

Epoch: 6| Step: 2
Training loss: 2.1814756393432617
Validation loss: 2.026098847389221

Epoch: 6| Step: 3
Training loss: 2.454195022583008
Validation loss: 2.0212233861287436

Epoch: 6| Step: 4
Training loss: 1.7494704723358154
Validation loss: 2.026304999987284

Epoch: 6| Step: 5
Training loss: 2.1130177974700928
Validation loss: 2.023363014062246

Epoch: 6| Step: 6
Training loss: 1.8984886407852173
Validation loss: 2.018164058526357

Epoch: 6| Step: 7
Training loss: 2.137561798095703
Validation loss: 2.026560584704081

Epoch: 6| Step: 8
Training loss: 1.5318896770477295
Validation loss: 2.0371995766957602

Epoch: 6| Step: 9
Training loss: 1.871635913848877
Validation loss: 2.0347649852434793

Epoch: 6| Step: 10
Training loss: 2.363955020904541
Validation loss: 2.0430803696314492

Epoch: 6| Step: 11
Training loss: 2.264112949371338
Validation loss: 2.031578520933787

Epoch: 6| Step: 12
Training loss: 2.226886510848999
Validation loss: 2.04353129863739

Epoch: 6| Step: 13
Training loss: 2.4085960388183594
Validation loss: 2.0445252656936646

Epoch: 96| Step: 0
Training loss: 2.2004764080047607
Validation loss: 2.047612031300863

Epoch: 6| Step: 1
Training loss: 2.2070605754852295
Validation loss: 2.059619903564453

Epoch: 6| Step: 2
Training loss: 2.0346460342407227
Validation loss: 2.0393266876538596

Epoch: 6| Step: 3
Training loss: 1.51129150390625
Validation loss: 2.0418943564097085

Epoch: 6| Step: 4
Training loss: 2.5532941818237305
Validation loss: 2.0286930402119956

Epoch: 6| Step: 5
Training loss: 1.8910470008850098
Validation loss: 2.0267213185628257

Epoch: 6| Step: 6
Training loss: 2.2636537551879883
Validation loss: 2.019627829392751

Epoch: 6| Step: 7
Training loss: 1.9079666137695312
Validation loss: 2.020152509212494

Epoch: 6| Step: 8
Training loss: 2.199892282485962
Validation loss: 2.0199711124102273

Epoch: 6| Step: 9
Training loss: 2.190526247024536
Validation loss: 2.018352289994558

Epoch: 6| Step: 10
Training loss: 1.9526958465576172
Validation loss: 2.019038955370585

Epoch: 6| Step: 11
Training loss: 1.7419054508209229
Validation loss: 2.0241369207700095

Epoch: 6| Step: 12
Training loss: 2.7237987518310547
Validation loss: 2.027874767780304

Epoch: 6| Step: 13
Training loss: 1.797369360923767
Validation loss: 2.027651290098826

Epoch: 97| Step: 0
Training loss: 2.1694984436035156
Validation loss: 2.0229701598485312

Epoch: 6| Step: 1
Training loss: 2.3757731914520264
Validation loss: 2.0293189883232117

Epoch: 6| Step: 2
Training loss: 2.59768009185791
Validation loss: 2.0232802430788674

Epoch: 6| Step: 3
Training loss: 2.434297561645508
Validation loss: 2.030978242556254

Epoch: 6| Step: 4
Training loss: 1.6699230670928955
Validation loss: 2.0386871099472046

Epoch: 6| Step: 5
Training loss: 1.527302861213684
Validation loss: 2.028519411881765

Epoch: 6| Step: 6
Training loss: 2.0111706256866455
Validation loss: 2.037400503953298

Epoch: 6| Step: 7
Training loss: 2.3700613975524902
Validation loss: 2.0383167465527854

Epoch: 6| Step: 8
Training loss: 1.8428815603256226
Validation loss: 2.0451393127441406

Epoch: 6| Step: 9
Training loss: 1.7990247011184692
Validation loss: 2.0504481196403503

Epoch: 6| Step: 10
Training loss: 2.267129898071289
Validation loss: 2.06722084681193

Epoch: 6| Step: 11
Training loss: 2.309751510620117
Validation loss: 2.0652426878611245

Epoch: 6| Step: 12
Training loss: 1.855236291885376
Validation loss: 2.0689414143562317

Epoch: 6| Step: 13
Training loss: 1.9846901893615723
Validation loss: 2.061772624651591

Epoch: 98| Step: 0
Training loss: 2.334603786468506
Validation loss: 2.054257055123647

Epoch: 6| Step: 1
Training loss: 2.157829999923706
Validation loss: 2.047092914581299

Epoch: 6| Step: 2
Training loss: 2.142850160598755
Validation loss: 2.0476155082384744

Epoch: 6| Step: 3
Training loss: 2.3709964752197266
Validation loss: 2.030985434850057

Epoch: 6| Step: 4
Training loss: 1.5551745891571045
Validation loss: 2.0231125752131143

Epoch: 6| Step: 5
Training loss: 1.8526965379714966
Validation loss: 2.0307682156562805

Epoch: 6| Step: 6
Training loss: 1.8998255729675293
Validation loss: 2.033292611440023

Epoch: 6| Step: 7
Training loss: 1.8365397453308105
Validation loss: 2.0374695658683777

Epoch: 6| Step: 8
Training loss: 1.4789704084396362
Validation loss: 2.0316478411356607

Epoch: 6| Step: 9
Training loss: 2.0106210708618164
Validation loss: 2.0342226028442383

Epoch: 6| Step: 10
Training loss: 2.9210872650146484
Validation loss: 2.035111387570699

Epoch: 6| Step: 11
Training loss: 2.0202200412750244
Validation loss: 2.031926910082499

Epoch: 6| Step: 12
Training loss: 2.995497465133667
Validation loss: 2.033717075983683

Epoch: 6| Step: 13
Training loss: 1.8201172351837158
Validation loss: 2.02940966685613

Epoch: 99| Step: 0
Training loss: 2.175124406814575
Validation loss: 2.0248732566833496

Epoch: 6| Step: 1
Training loss: 2.0834712982177734
Validation loss: 2.0301235715548196

Epoch: 6| Step: 2
Training loss: 2.2899203300476074
Validation loss: 2.042991280555725

Epoch: 6| Step: 3
Training loss: 2.6049857139587402
Validation loss: 2.0396739641825357

Epoch: 6| Step: 4
Training loss: 2.34204363822937
Validation loss: 2.041184882322947

Epoch: 6| Step: 5
Training loss: 1.7287940979003906
Validation loss: 2.0431390007336936

Epoch: 6| Step: 6
Training loss: 1.966181755065918
Validation loss: 2.0275869965553284

Epoch: 6| Step: 7
Training loss: 1.4866071939468384
Validation loss: 2.0223239064216614

Epoch: 6| Step: 8
Training loss: 1.9879043102264404
Validation loss: 2.0296250780423484

Epoch: 6| Step: 9
Training loss: 2.1208624839782715
Validation loss: 2.0249053835868835

Epoch: 6| Step: 10
Training loss: 1.562308430671692
Validation loss: 2.028499166170756

Epoch: 6| Step: 11
Training loss: 2.931269645690918
Validation loss: 2.026302754878998

Epoch: 6| Step: 12
Training loss: 1.8457059860229492
Validation loss: 2.0243490735689798

Epoch: 6| Step: 13
Training loss: 2.04974365234375
Validation loss: 2.03305184841156

Epoch: 100| Step: 0
Training loss: 1.8631559610366821
Validation loss: 2.0364245176315308

Epoch: 6| Step: 1
Training loss: 2.2699859142303467
Validation loss: 2.0239256620407104

Epoch: 6| Step: 2
Training loss: 1.9538302421569824
Validation loss: 2.0274804631868997

Epoch: 6| Step: 3
Training loss: 2.6802978515625
Validation loss: 2.034603734811147

Epoch: 6| Step: 4
Training loss: 1.9833312034606934
Validation loss: 2.0358020067214966

Epoch: 6| Step: 5
Training loss: 2.2779815196990967
Validation loss: 2.0263313253720603

Epoch: 6| Step: 6
Training loss: 2.2314000129699707
Validation loss: 2.040203253428141

Epoch: 6| Step: 7
Training loss: 2.5188612937927246
Validation loss: 2.0363285144170127

Epoch: 6| Step: 8
Training loss: 1.9540278911590576
Validation loss: 2.030357281366984

Epoch: 6| Step: 9
Training loss: 1.3455407619476318
Validation loss: 2.035793125629425

Epoch: 6| Step: 10
Training loss: 1.2753210067749023
Validation loss: 2.0423738757769265

Epoch: 6| Step: 11
Training loss: 1.8664333820343018
Validation loss: 2.0447657903035483

Epoch: 6| Step: 12
Training loss: 2.1705827713012695
Validation loss: 2.0413137475649514

Epoch: 6| Step: 13
Training loss: 2.4801135063171387
Validation loss: 2.0520819226900735

Epoch: 101| Step: 0
Training loss: 2.511594772338867
Validation loss: 2.0641013383865356

Epoch: 6| Step: 1
Training loss: 1.9556472301483154
Validation loss: 2.0702552000681558

Epoch: 6| Step: 2
Training loss: 1.9188358783721924
Validation loss: 2.0765004555384317

Epoch: 6| Step: 3
Training loss: 2.651728630065918
Validation loss: 2.0722409884134927

Epoch: 6| Step: 4
Training loss: 1.6482446193695068
Validation loss: 2.064336597919464

Epoch: 6| Step: 5
Training loss: 2.4593653678894043
Validation loss: 2.055614431699117

Epoch: 6| Step: 6
Training loss: 2.11628794670105
Validation loss: 2.0516688426335654

Epoch: 6| Step: 7
Training loss: 1.5989609956741333
Validation loss: 2.0468892653783164

Epoch: 6| Step: 8
Training loss: 3.0975468158721924
Validation loss: 2.0503753821055093

Epoch: 6| Step: 9
Training loss: 1.9767969846725464
Validation loss: 2.0351352294286094

Epoch: 6| Step: 10
Training loss: 1.558354139328003
Validation loss: 2.039049426714579

Epoch: 6| Step: 11
Training loss: 1.9407641887664795
Validation loss: 2.0405860543251038

Epoch: 6| Step: 12
Training loss: 1.8886555433273315
Validation loss: 2.0412651697794595

Epoch: 6| Step: 13
Training loss: 1.7648948431015015
Validation loss: 2.0429372787475586

Epoch: 102| Step: 0
Training loss: 2.0291054248809814
Validation loss: 2.0389359990755715

Epoch: 6| Step: 1
Training loss: 2.7309398651123047
Validation loss: 2.0419203639030457

Epoch: 6| Step: 2
Training loss: 1.3468267917633057
Validation loss: 2.0394253730773926

Epoch: 6| Step: 3
Training loss: 2.279555320739746
Validation loss: 2.049968659877777

Epoch: 6| Step: 4
Training loss: 1.5066580772399902
Validation loss: 2.031022032101949

Epoch: 6| Step: 5
Training loss: 1.8574345111846924
Validation loss: 2.0443604985872903

Epoch: 6| Step: 6
Training loss: 2.244647741317749
Validation loss: 2.043878654638926

Epoch: 6| Step: 7
Training loss: 1.6677122116088867
Validation loss: 2.045699735482534

Epoch: 6| Step: 8
Training loss: 2.051025629043579
Validation loss: 2.040926694869995

Epoch: 6| Step: 9
Training loss: 2.3493056297302246
Validation loss: 2.0615640679995217

Epoch: 6| Step: 10
Training loss: 1.920373558998108
Validation loss: 2.057878017425537

Epoch: 6| Step: 11
Training loss: 2.1332712173461914
Validation loss: 2.036883453528086

Epoch: 6| Step: 12
Training loss: 2.221144914627075
Validation loss: 2.0508870283762612

Epoch: 6| Step: 13
Training loss: 2.362553834915161
Validation loss: 2.041933536529541

Epoch: 103| Step: 0
Training loss: 1.9981331825256348
Validation loss: 2.0470679799715676

Epoch: 6| Step: 1
Training loss: 2.9038522243499756
Validation loss: 2.0505215922991433

Epoch: 6| Step: 2
Training loss: 1.9090133905410767
Validation loss: 2.0462867816289267

Epoch: 6| Step: 3
Training loss: 1.8874573707580566
Validation loss: 2.0560505390167236

Epoch: 6| Step: 4
Training loss: 2.021803379058838
Validation loss: 2.0507447322209678

Epoch: 6| Step: 5
Training loss: 1.7922406196594238
Validation loss: 2.0525031685829163

Epoch: 6| Step: 6
Training loss: 2.363712787628174
Validation loss: 2.051163454850515

Epoch: 6| Step: 7
Training loss: 1.5792591571807861
Validation loss: 2.050321618715922

Epoch: 6| Step: 8
Training loss: 2.624606132507324
Validation loss: 2.046745777130127

Epoch: 6| Step: 9
Training loss: 1.8824927806854248
Validation loss: 2.0496931274731955

Epoch: 6| Step: 10
Training loss: 1.5025023221969604
Validation loss: 2.0516217947006226

Epoch: 6| Step: 11
Training loss: 1.7958106994628906
Validation loss: 2.049605667591095

Epoch: 6| Step: 12
Training loss: 2.227430820465088
Validation loss: 2.0437318881352744

Epoch: 6| Step: 13
Training loss: 2.2607715129852295
Validation loss: 2.041752815246582

Epoch: 104| Step: 0
Training loss: 1.7072577476501465
Validation loss: 2.0474857091903687

Epoch: 6| Step: 1
Training loss: 2.3297688961029053
Validation loss: 2.036411762237549

Epoch: 6| Step: 2
Training loss: 1.489997386932373
Validation loss: 2.0393848419189453

Epoch: 6| Step: 3
Training loss: 1.9950571060180664
Validation loss: 2.0492652455965676

Epoch: 6| Step: 4
Training loss: 1.8139536380767822
Validation loss: 2.042577862739563

Epoch: 6| Step: 5
Training loss: 1.783461093902588
Validation loss: 2.0619067351023355

Epoch: 6| Step: 6
Training loss: 2.0177226066589355
Validation loss: 2.0423405170440674

Epoch: 6| Step: 7
Training loss: 2.0675628185272217
Validation loss: 2.0380367040634155

Epoch: 6| Step: 8
Training loss: 2.1081676483154297
Validation loss: 2.0439287026723227

Epoch: 6| Step: 9
Training loss: 2.1860556602478027
Validation loss: 2.0393582383791604

Epoch: 6| Step: 10
Training loss: 1.702101230621338
Validation loss: 2.032688101132711

Epoch: 6| Step: 11
Training loss: 2.803866386413574
Validation loss: 2.0300746957461038

Epoch: 6| Step: 12
Training loss: 2.1139461994171143
Validation loss: 2.0306926170984902

Epoch: 6| Step: 13
Training loss: 2.7091784477233887
Validation loss: 2.0279216170310974

Epoch: 105| Step: 0
Training loss: 2.87249755859375
Validation loss: 2.024906853834788

Epoch: 6| Step: 1
Training loss: 2.646178722381592
Validation loss: 2.016238212585449

Epoch: 6| Step: 2
Training loss: 1.848340392112732
Validation loss: 2.0175246397654214

Epoch: 6| Step: 3
Training loss: 1.7572312355041504
Validation loss: 2.0159810980161033

Epoch: 6| Step: 4
Training loss: 2.0097248554229736
Validation loss: 2.0279357035954795

Epoch: 6| Step: 5
Training loss: 1.7146275043487549
Validation loss: 2.016974131266276

Epoch: 6| Step: 6
Training loss: 2.3592162132263184
Validation loss: 2.0279786586761475

Epoch: 6| Step: 7
Training loss: 2.191314220428467
Validation loss: 2.029564062754313

Epoch: 6| Step: 8
Training loss: 1.5108590126037598
Validation loss: 2.0414365331331887

Epoch: 6| Step: 9
Training loss: 2.347179889678955
Validation loss: 2.042772948741913

Epoch: 6| Step: 10
Training loss: 1.9935802221298218
Validation loss: 2.033835152784983

Epoch: 6| Step: 11
Training loss: 2.5040321350097656
Validation loss: 2.0357638200124106

Epoch: 6| Step: 12
Training loss: 1.5130343437194824
Validation loss: 2.0390785733858743

Epoch: 6| Step: 13
Training loss: 2.041962146759033
Validation loss: 2.047569215297699

Epoch: 106| Step: 0
Training loss: 2.0369153022766113
Validation loss: 2.044810195763906

Epoch: 6| Step: 1
Training loss: 2.3440656661987305
Validation loss: 2.050230920314789

Epoch: 6| Step: 2
Training loss: 1.9442498683929443
Validation loss: 2.0557703375816345

Epoch: 6| Step: 3
Training loss: 1.6672680377960205
Validation loss: 2.0537707010904946

Epoch: 6| Step: 4
Training loss: 1.9765403270721436
Validation loss: 2.0534698168436685

Epoch: 6| Step: 5
Training loss: 1.9658091068267822
Validation loss: 2.043815036614736

Epoch: 6| Step: 6
Training loss: 2.296844482421875
Validation loss: 2.0530611077944436

Epoch: 6| Step: 7
Training loss: 2.089134693145752
Validation loss: 2.0643569032351174

Epoch: 6| Step: 8
Training loss: 2.1023364067077637
Validation loss: 2.06266983350118

Epoch: 6| Step: 9
Training loss: 2.2829196453094482
Validation loss: 2.055220822493235

Epoch: 6| Step: 10
Training loss: 1.4863489866256714
Validation loss: 2.059036652247111

Epoch: 6| Step: 11
Training loss: 2.0694937705993652
Validation loss: 2.0689798394838967

Epoch: 6| Step: 12
Training loss: 2.193429708480835
Validation loss: 2.06875079870224

Epoch: 6| Step: 13
Training loss: 2.269000768661499
Validation loss: 2.075342317422231

Epoch: 107| Step: 0
Training loss: 1.3809418678283691
Validation loss: 2.076528231302897

Epoch: 6| Step: 1
Training loss: 1.7752810716629028
Validation loss: 2.0850932598114014

Epoch: 6| Step: 2
Training loss: 2.3643901348114014
Validation loss: 2.071624755859375

Epoch: 6| Step: 3
Training loss: 1.9966282844543457
Validation loss: 2.069556454817454

Epoch: 6| Step: 4
Training loss: 1.7809460163116455
Validation loss: 2.0615604321161904

Epoch: 6| Step: 5
Training loss: 2.3531289100646973
Validation loss: 2.0555995305379233

Epoch: 6| Step: 6
Training loss: 1.9691755771636963
Validation loss: 2.071329891681671

Epoch: 6| Step: 7
Training loss: 2.2712562084198
Validation loss: 2.0606512427330017

Epoch: 6| Step: 8
Training loss: 2.012063503265381
Validation loss: 2.0486037929852805

Epoch: 6| Step: 9
Training loss: 1.99878990650177
Validation loss: 2.0665870110193887

Epoch: 6| Step: 10
Training loss: 2.126132011413574
Validation loss: 2.0641165574391684

Epoch: 6| Step: 11
Training loss: 2.284440279006958
Validation loss: 2.0566778779029846

Epoch: 6| Step: 12
Training loss: 2.2765026092529297
Validation loss: 2.0587088664372764

Epoch: 6| Step: 13
Training loss: 2.1273293495178223
Validation loss: 2.04967733224233

Epoch: 108| Step: 0
Training loss: 2.171724557876587
Validation loss: 2.04781045516332

Epoch: 6| Step: 1
Training loss: 1.7128283977508545
Validation loss: 2.0441689093907676

Epoch: 6| Step: 2
Training loss: 1.9824254512786865
Validation loss: 2.045185128847758

Epoch: 6| Step: 3
Training loss: 2.0904924869537354
Validation loss: 2.041055659453074

Epoch: 6| Step: 4
Training loss: 1.7627519369125366
Validation loss: 2.052705923716227

Epoch: 6| Step: 5
Training loss: 1.9626446962356567
Validation loss: 2.060003916422526

Epoch: 6| Step: 6
Training loss: 1.9418292045593262
Validation loss: 2.0541224082310996

Epoch: 6| Step: 7
Training loss: 1.634969711303711
Validation loss: 2.0689687728881836

Epoch: 6| Step: 8
Training loss: 1.8267443180084229
Validation loss: 2.0668985644976297

Epoch: 6| Step: 9
Training loss: 1.8028091192245483
Validation loss: 2.0747162103652954

Epoch: 6| Step: 10
Training loss: 2.199561357498169
Validation loss: 2.077676832675934

Epoch: 6| Step: 11
Training loss: 2.7132349014282227
Validation loss: 2.085204462210337

Epoch: 6| Step: 12
Training loss: 2.182138681411743
Validation loss: 2.113271435101827

Epoch: 6| Step: 13
Training loss: 3.157961368560791
Validation loss: 2.1371883153915405

Epoch: 109| Step: 0
Training loss: 1.8698899745941162
Validation loss: 2.1091055671374

Epoch: 6| Step: 1
Training loss: 1.507444143295288
Validation loss: 2.0748446782430015

Epoch: 6| Step: 2
Training loss: 2.4363057613372803
Validation loss: 2.06345264116923

Epoch: 6| Step: 3
Training loss: 2.66516375541687
Validation loss: 2.0707814494768777

Epoch: 6| Step: 4
Training loss: 1.8256624937057495
Validation loss: 2.054237981637319

Epoch: 6| Step: 5
Training loss: 2.3227460384368896
Validation loss: 2.045588215192159

Epoch: 6| Step: 6
Training loss: 2.0591087341308594
Validation loss: 2.0575605034828186

Epoch: 6| Step: 7
Training loss: 2.0217485427856445
Validation loss: 2.043509582678477

Epoch: 6| Step: 8
Training loss: 1.8191074132919312
Validation loss: 2.042490084966024

Epoch: 6| Step: 9
Training loss: 2.005267858505249
Validation loss: 2.0405025084813437

Epoch: 6| Step: 10
Training loss: 1.6800966262817383
Validation loss: 2.0407710870107016

Epoch: 6| Step: 11
Training loss: 2.3662636280059814
Validation loss: 2.041488230228424

Epoch: 6| Step: 12
Training loss: 2.1406402587890625
Validation loss: 2.0392802953720093

Epoch: 6| Step: 13
Training loss: 1.9830951690673828
Validation loss: 2.0355769793192544

Epoch: 110| Step: 0
Training loss: 2.1897315979003906
Validation loss: 2.0418792963027954

Epoch: 6| Step: 1
Training loss: 1.91756010055542
Validation loss: 2.046992301940918

Epoch: 6| Step: 2
Training loss: 2.4057812690734863
Validation loss: 2.048448940118154

Epoch: 6| Step: 3
Training loss: 1.2581138610839844
Validation loss: 2.051287670930227

Epoch: 6| Step: 4
Training loss: 1.6524736881256104
Validation loss: 2.057845890522003

Epoch: 6| Step: 5
Training loss: 1.9046190977096558
Validation loss: 2.054921567440033

Epoch: 6| Step: 6
Training loss: 2.3055355548858643
Validation loss: 2.0592050552368164

Epoch: 6| Step: 7
Training loss: 1.7576076984405518
Validation loss: 2.057220757007599

Epoch: 6| Step: 8
Training loss: 1.876638412475586
Validation loss: 2.0691281159718833

Epoch: 6| Step: 9
Training loss: 2.228145122528076
Validation loss: 2.068222681681315

Epoch: 6| Step: 10
Training loss: 2.255370855331421
Validation loss: 2.0718608299891152

Epoch: 6| Step: 11
Training loss: 2.1274123191833496
Validation loss: 2.0712530612945557

Epoch: 6| Step: 12
Training loss: 3.0525803565979004
Validation loss: 2.0717355410257974

Epoch: 6| Step: 13
Training loss: 1.8394099473953247
Validation loss: 2.0683870712916055

Epoch: 111| Step: 0
Training loss: 2.5118236541748047
Validation loss: 2.063941399256388

Epoch: 6| Step: 1
Training loss: 1.9057645797729492
Validation loss: 2.0650417804718018

Epoch: 6| Step: 2
Training loss: 1.6723129749298096
Validation loss: 2.0667991240819297

Epoch: 6| Step: 3
Training loss: 2.4867186546325684
Validation loss: 2.0685090819994607

Epoch: 6| Step: 4
Training loss: 1.94474196434021
Validation loss: 2.07163276274999

Epoch: 6| Step: 5
Training loss: 1.5973625183105469
Validation loss: 2.063321908315023

Epoch: 6| Step: 6
Training loss: 2.081510543823242
Validation loss: 2.0637953877449036

Epoch: 6| Step: 7
Training loss: 2.614884853363037
Validation loss: 2.0633291403452554

Epoch: 6| Step: 8
Training loss: 1.8916398286819458
Validation loss: 2.0668211579322815

Epoch: 6| Step: 9
Training loss: 1.7667815685272217
Validation loss: 2.0630592902501426

Epoch: 6| Step: 10
Training loss: 2.170356512069702
Validation loss: 2.0682724912961326

Epoch: 6| Step: 11
Training loss: 2.1051955223083496
Validation loss: 2.052177687486013

Epoch: 6| Step: 12
Training loss: 1.8318672180175781
Validation loss: 2.0573904712994895

Epoch: 6| Step: 13
Training loss: 1.9311511516571045
Validation loss: 2.057131210962931

Epoch: 112| Step: 0
Training loss: 2.313211441040039
Validation loss: 2.0554742415746055

Epoch: 6| Step: 1
Training loss: 2.1543235778808594
Validation loss: 2.056660791238149

Epoch: 6| Step: 2
Training loss: 2.247162103652954
Validation loss: 2.048428555329641

Epoch: 6| Step: 3
Training loss: 1.8857437372207642
Validation loss: 2.047917664051056

Epoch: 6| Step: 4
Training loss: 1.4996190071105957
Validation loss: 2.0456956227620444

Epoch: 6| Step: 5
Training loss: 2.5406477451324463
Validation loss: 2.046032508214315

Epoch: 6| Step: 6
Training loss: 2.224398374557495
Validation loss: 2.043055613835653

Epoch: 6| Step: 7
Training loss: 2.922811985015869
Validation loss: 2.0408182541529336

Epoch: 6| Step: 8
Training loss: 2.0891621112823486
Validation loss: 2.0463157892227173

Epoch: 6| Step: 9
Training loss: 1.465318202972412
Validation loss: 2.0389711459477744

Epoch: 6| Step: 10
Training loss: 1.7546911239624023
Validation loss: 2.040059526761373

Epoch: 6| Step: 11
Training loss: 2.298686981201172
Validation loss: 2.0399277210235596

Epoch: 6| Step: 12
Training loss: 1.8750207424163818
Validation loss: 2.0477495590845742

Epoch: 6| Step: 13
Training loss: 1.6024408340454102
Validation loss: 2.040793756643931

Epoch: 113| Step: 0
Training loss: 1.4389405250549316
Validation loss: 2.051584084828695

Epoch: 6| Step: 1
Training loss: 2.029325485229492
Validation loss: 2.0514902075131736

Epoch: 6| Step: 2
Training loss: 2.272953510284424
Validation loss: 2.046165724595388

Epoch: 6| Step: 3
Training loss: 1.8852717876434326
Validation loss: 2.0548627773920694

Epoch: 6| Step: 4
Training loss: 1.4156968593597412
Validation loss: 2.0518118341763816

Epoch: 6| Step: 5
Training loss: 2.293381452560425
Validation loss: 2.048186103502909

Epoch: 6| Step: 6
Training loss: 2.3358263969421387
Validation loss: 2.0535462697347007

Epoch: 6| Step: 7
Training loss: 1.4021687507629395
Validation loss: 2.0644418597221375

Epoch: 6| Step: 8
Training loss: 2.1063597202301025
Validation loss: 2.051902453104655

Epoch: 6| Step: 9
Training loss: 2.385308265686035
Validation loss: 2.0638726353645325

Epoch: 6| Step: 10
Training loss: 1.8562636375427246
Validation loss: 2.069544553756714

Epoch: 6| Step: 11
Training loss: 2.628765821456909
Validation loss: 2.070138990879059

Epoch: 6| Step: 12
Training loss: 2.5235772132873535
Validation loss: 2.0687729716300964

Epoch: 6| Step: 13
Training loss: 1.9390063285827637
Validation loss: 2.0632399916648865

Epoch: 114| Step: 0
Training loss: 1.9324045181274414
Validation loss: 2.063508371512095

Epoch: 6| Step: 1
Training loss: 1.9987555742263794
Validation loss: 2.0603093902269998

Epoch: 6| Step: 2
Training loss: 1.9977588653564453
Validation loss: 2.0543485085169473

Epoch: 6| Step: 3
Training loss: 2.0330522060394287
Validation loss: 2.0538319746653237

Epoch: 6| Step: 4
Training loss: 1.7613236904144287
Validation loss: 2.0455828110376992

Epoch: 6| Step: 5
Training loss: 2.170876979827881
Validation loss: 2.0477139155069985

Epoch: 6| Step: 6
Training loss: 2.4409279823303223
Validation loss: 2.0564411282539368

Epoch: 6| Step: 7
Training loss: 1.7952882051467896
Validation loss: 2.0556092063585916

Epoch: 6| Step: 8
Training loss: 2.6759519577026367
Validation loss: 2.0608569979667664

Epoch: 6| Step: 9
Training loss: 2.0556392669677734
Validation loss: 2.0549099445343018

Epoch: 6| Step: 10
Training loss: 1.7418324947357178
Validation loss: 2.062028487523397

Epoch: 6| Step: 11
Training loss: 2.0858988761901855
Validation loss: 2.066193620363871

Epoch: 6| Step: 12
Training loss: 2.167980670928955
Validation loss: 2.0670419534047446

Epoch: 6| Step: 13
Training loss: 1.595566987991333
Validation loss: 2.062417725721995

Epoch: 115| Step: 0
Training loss: 1.7864084243774414
Validation loss: 2.0612281362215676

Epoch: 6| Step: 1
Training loss: 1.6212406158447266
Validation loss: 2.0505478978157043

Epoch: 6| Step: 2
Training loss: 2.313084363937378
Validation loss: 2.061843772729238

Epoch: 6| Step: 3
Training loss: 2.2094082832336426
Validation loss: 2.0552767515182495

Epoch: 6| Step: 4
Training loss: 1.8581538200378418
Validation loss: 2.061048229535421

Epoch: 6| Step: 5
Training loss: 2.1045444011688232
Validation loss: 2.0559972723325095

Epoch: 6| Step: 6
Training loss: 2.6130194664001465
Validation loss: 2.0722761352856955

Epoch: 6| Step: 7
Training loss: 1.979823350906372
Validation loss: 2.0573188066482544

Epoch: 6| Step: 8
Training loss: 1.660125732421875
Validation loss: 2.052196721235911

Epoch: 6| Step: 9
Training loss: 1.9837172031402588
Validation loss: 2.059073030948639

Epoch: 6| Step: 10
Training loss: 2.280210018157959
Validation loss: 2.057952344417572

Epoch: 6| Step: 11
Training loss: 1.9910759925842285
Validation loss: 2.041631559530894

Epoch: 6| Step: 12
Training loss: 1.9384007453918457
Validation loss: 2.04619707663854

Epoch: 6| Step: 13
Training loss: 1.9186291694641113
Validation loss: 2.039945960044861

Epoch: 116| Step: 0
Training loss: 2.0095925331115723
Validation loss: 2.0421971480051675

Epoch: 6| Step: 1
Training loss: 1.5452232360839844
Validation loss: 2.0449459552764893

Epoch: 6| Step: 2
Training loss: 2.3084592819213867
Validation loss: 2.0505887269973755

Epoch: 6| Step: 3
Training loss: 2.05983304977417
Validation loss: 2.051254947980245

Epoch: 6| Step: 4
Training loss: 1.8903390169143677
Validation loss: 2.0454299449920654

Epoch: 6| Step: 5
Training loss: 2.2185258865356445
Validation loss: 2.046551684538523

Epoch: 6| Step: 6
Training loss: 2.3552892208099365
Validation loss: 2.0517976681391397

Epoch: 6| Step: 7
Training loss: 2.693010091781616
Validation loss: 2.058384199937185

Epoch: 6| Step: 8
Training loss: 1.9438254833221436
Validation loss: 2.0564836462338767

Epoch: 6| Step: 9
Training loss: 1.6786181926727295
Validation loss: 2.0584089358647666

Epoch: 6| Step: 10
Training loss: 2.1753134727478027
Validation loss: 2.0687828063964844

Epoch: 6| Step: 11
Training loss: 1.989437222480774
Validation loss: 2.0677059094111123

Epoch: 6| Step: 12
Training loss: 1.965260624885559
Validation loss: 2.073518613974253

Epoch: 6| Step: 13
Training loss: 1.5367685556411743
Validation loss: 2.069697856903076

Epoch: 117| Step: 0
Training loss: 2.3914549350738525
Validation loss: 2.073834538459778

Epoch: 6| Step: 1
Training loss: 2.464053153991699
Validation loss: 2.088901460170746

Epoch: 6| Step: 2
Training loss: 1.8686628341674805
Validation loss: 2.086196502049764

Epoch: 6| Step: 3
Training loss: 1.9585931301116943
Validation loss: 2.0788228511810303

Epoch: 6| Step: 4
Training loss: 2.081899881362915
Validation loss: 2.074871818224589

Epoch: 6| Step: 5
Training loss: 1.6684763431549072
Validation loss: 2.0723424355189004

Epoch: 6| Step: 6
Training loss: 2.1284570693969727
Validation loss: 2.074126978715261

Epoch: 6| Step: 7
Training loss: 1.7492963075637817
Validation loss: 2.0749595959981284

Epoch: 6| Step: 8
Training loss: 2.0408759117126465
Validation loss: 2.061706066131592

Epoch: 6| Step: 9
Training loss: 1.763444185256958
Validation loss: 2.0729907155036926

Epoch: 6| Step: 10
Training loss: 1.7935199737548828
Validation loss: 2.0747300585110984

Epoch: 6| Step: 11
Training loss: 2.399383306503296
Validation loss: 2.0771305561065674

Epoch: 6| Step: 12
Training loss: 2.0250296592712402
Validation loss: 2.0719318191210427

Epoch: 6| Step: 13
Training loss: 1.7481248378753662
Validation loss: 2.0746900836626687

Epoch: 118| Step: 0
Training loss: 1.7210748195648193
Validation loss: 2.0734068950017295

Epoch: 6| Step: 1
Training loss: 2.2138314247131348
Validation loss: 2.0733796755472818

Epoch: 6| Step: 2
Training loss: 2.013308525085449
Validation loss: 2.0848232309023538

Epoch: 6| Step: 3
Training loss: 2.067802906036377
Validation loss: 2.0844918290774026

Epoch: 6| Step: 4
Training loss: 1.6249955892562866
Validation loss: 2.07871941725413

Epoch: 6| Step: 5
Training loss: 1.9293618202209473
Validation loss: 2.0729355017344155

Epoch: 6| Step: 6
Training loss: 1.6371196508407593
Validation loss: 2.062471350034078

Epoch: 6| Step: 7
Training loss: 2.1530239582061768
Validation loss: 2.072594424088796

Epoch: 6| Step: 8
Training loss: 2.4184184074401855
Validation loss: 2.0642905235290527

Epoch: 6| Step: 9
Training loss: 2.3265161514282227
Validation loss: 2.064567446708679

Epoch: 6| Step: 10
Training loss: 2.0308315753936768
Validation loss: 2.075438698132833

Epoch: 6| Step: 11
Training loss: 2.143582344055176
Validation loss: 2.068780263264974

Epoch: 6| Step: 12
Training loss: 1.72188138961792
Validation loss: 2.0622024536132812

Epoch: 6| Step: 13
Training loss: 2.100026845932007
Validation loss: 2.0729678670565286

Epoch: 119| Step: 0
Training loss: 2.3548471927642822
Validation loss: 2.0686569611231485

Epoch: 6| Step: 1
Training loss: 2.307640790939331
Validation loss: 2.0618526538213096

Epoch: 6| Step: 2
Training loss: 2.0166707038879395
Validation loss: 2.0727787415186563

Epoch: 6| Step: 3
Training loss: 1.8491480350494385
Validation loss: 2.0799349745114646

Epoch: 6| Step: 4
Training loss: 1.9977103471755981
Validation loss: 2.082818647225698

Epoch: 6| Step: 5
Training loss: 1.707682490348816
Validation loss: 2.0756802757581077

Epoch: 6| Step: 6
Training loss: 2.2092511653900146
Validation loss: 2.0833251674969993

Epoch: 6| Step: 7
Training loss: 2.0284571647644043
Validation loss: 2.0811354517936707

Epoch: 6| Step: 8
Training loss: 2.234752655029297
Validation loss: 2.070401350657145

Epoch: 6| Step: 9
Training loss: 1.6642045974731445
Validation loss: 2.079474091529846

Epoch: 6| Step: 10
Training loss: 1.8294103145599365
Validation loss: 2.066902240117391

Epoch: 6| Step: 11
Training loss: 1.8478083610534668
Validation loss: 2.0703442096710205

Epoch: 6| Step: 12
Training loss: 2.3187973499298096
Validation loss: 2.0635252793629966

Epoch: 6| Step: 13
Training loss: 1.7071138620376587
Validation loss: 2.065436085065206

Epoch: 120| Step: 0
Training loss: 2.131781816482544
Validation loss: 2.0649201075236

Epoch: 6| Step: 1
Training loss: 2.2431182861328125
Validation loss: 2.062425434589386

Epoch: 6| Step: 2
Training loss: 2.0882081985473633
Validation loss: 2.0565830866495767

Epoch: 6| Step: 3
Training loss: 2.1373291015625
Validation loss: 2.0702722668647766

Epoch: 6| Step: 4
Training loss: 2.520986318588257
Validation loss: 2.0607694387435913

Epoch: 6| Step: 5
Training loss: 1.4694957733154297
Validation loss: 2.07376637061437

Epoch: 6| Step: 6
Training loss: 2.0004141330718994
Validation loss: 2.068279961744944

Epoch: 6| Step: 7
Training loss: 1.4185112714767456
Validation loss: 2.072076757748922

Epoch: 6| Step: 8
Training loss: 1.8679003715515137
Validation loss: 2.0907163421312966

Epoch: 6| Step: 9
Training loss: 2.1654491424560547
Validation loss: 2.1080731948216758

Epoch: 6| Step: 10
Training loss: 2.8508942127227783
Validation loss: 2.1071075201034546

Epoch: 6| Step: 11
Training loss: 1.6707028150558472
Validation loss: 2.1029963294665017

Epoch: 6| Step: 12
Training loss: 2.022031307220459
Validation loss: 2.116725504398346

Epoch: 6| Step: 13
Training loss: 1.8292999267578125
Validation loss: 2.1045087973276773

Epoch: 121| Step: 0
Training loss: 1.59285306930542
Validation loss: 2.1148844957351685

Epoch: 6| Step: 1
Training loss: 3.030272960662842
Validation loss: 2.1024563709894815

Epoch: 6| Step: 2
Training loss: 1.7659556865692139
Validation loss: 2.1063522895177207

Epoch: 6| Step: 3
Training loss: 2.168762445449829
Validation loss: 2.0828112165133157

Epoch: 6| Step: 4
Training loss: 1.478191614151001
Validation loss: 2.081622282663981

Epoch: 6| Step: 5
Training loss: 1.6403636932373047
Validation loss: 2.06844162940979

Epoch: 6| Step: 6
Training loss: 1.7887715101242065
Validation loss: 2.060763438542684

Epoch: 6| Step: 7
Training loss: 1.7715389728546143
Validation loss: 2.057010372479757

Epoch: 6| Step: 8
Training loss: 2.282581329345703
Validation loss: 2.0480989615122476

Epoch: 6| Step: 9
Training loss: 2.174935817718506
Validation loss: 2.0510785977045694

Epoch: 6| Step: 10
Training loss: 2.4718689918518066
Validation loss: 2.049109995365143

Epoch: 6| Step: 11
Training loss: 1.9824506044387817
Validation loss: 2.0493913094202676

Epoch: 6| Step: 12
Training loss: 1.904256820678711
Validation loss: 2.070907473564148

Epoch: 6| Step: 13
Training loss: 2.4005610942840576
Validation loss: 2.0842770536740622

Epoch: 122| Step: 0
Training loss: 2.1150481700897217
Validation loss: 2.0714975396792092

Epoch: 6| Step: 1
Training loss: 2.1776058673858643
Validation loss: 2.078386922677358

Epoch: 6| Step: 2
Training loss: 1.9678555727005005
Validation loss: 2.0822363098462424

Epoch: 6| Step: 3
Training loss: 1.9287241697311401
Validation loss: 2.0702455838521323

Epoch: 6| Step: 4
Training loss: 2.027843475341797
Validation loss: 2.075805902481079

Epoch: 6| Step: 5
Training loss: 1.7636164426803589
Validation loss: 2.0801940162976584

Epoch: 6| Step: 6
Training loss: 2.096712112426758
Validation loss: 2.073715170224508

Epoch: 6| Step: 7
Training loss: 1.8111982345581055
Validation loss: 2.070849438508352

Epoch: 6| Step: 8
Training loss: 1.9359056949615479
Validation loss: 2.070352554321289

Epoch: 6| Step: 9
Training loss: 1.5842690467834473
Validation loss: 2.077297349770864

Epoch: 6| Step: 10
Training loss: 2.7164254188537598
Validation loss: 2.0733579993247986

Epoch: 6| Step: 11
Training loss: 1.8344371318817139
Validation loss: 2.0953290462493896

Epoch: 6| Step: 12
Training loss: 1.5998258590698242
Validation loss: 2.0784987409909568

Epoch: 6| Step: 13
Training loss: 2.687502145767212
Validation loss: 2.08190264304479

Epoch: 123| Step: 0
Training loss: 1.741388201713562
Validation loss: 2.079101781050364

Epoch: 6| Step: 1
Training loss: 2.1760740280151367
Validation loss: 2.083781083424886

Epoch: 6| Step: 2
Training loss: 1.753019094467163
Validation loss: 2.0802446603775024

Epoch: 6| Step: 3
Training loss: 1.6416139602661133
Validation loss: 2.0705310304959617

Epoch: 6| Step: 4
Training loss: 2.169935703277588
Validation loss: 2.069324334462484

Epoch: 6| Step: 5
Training loss: 1.5408731698989868
Validation loss: 2.0715019702911377

Epoch: 6| Step: 6
Training loss: 2.075620651245117
Validation loss: 2.067009528477987

Epoch: 6| Step: 7
Training loss: 2.406076669692993
Validation loss: 2.070969879627228

Epoch: 6| Step: 8
Training loss: 2.1787915229797363
Validation loss: 2.069053292274475

Epoch: 6| Step: 9
Training loss: 2.229883909225464
Validation loss: 2.0620336731274924

Epoch: 6| Step: 10
Training loss: 1.8665704727172852
Validation loss: 2.0713796416918435

Epoch: 6| Step: 11
Training loss: 1.4157801866531372
Validation loss: 2.069323261578878

Epoch: 6| Step: 12
Training loss: 2.8340892791748047
Validation loss: 2.0720906058947244

Epoch: 6| Step: 13
Training loss: 2.1094820499420166
Validation loss: 2.095810870329539

Epoch: 124| Step: 0
Training loss: 2.610138177871704
Validation loss: 2.093324820200602

Epoch: 6| Step: 1
Training loss: 2.1261582374572754
Validation loss: 2.102626383304596

Epoch: 6| Step: 2
Training loss: 1.7372663021087646
Validation loss: 2.1043092409769693

Epoch: 6| Step: 3
Training loss: 2.1453988552093506
Validation loss: 2.0993277629216514

Epoch: 6| Step: 4
Training loss: 1.3396947383880615
Validation loss: 2.1099100510279336

Epoch: 6| Step: 5
Training loss: 2.153020143508911
Validation loss: 2.096504012743632

Epoch: 6| Step: 6
Training loss: 1.7790218591690063
Validation loss: 2.1131303310394287

Epoch: 6| Step: 7
Training loss: 2.3800525665283203
Validation loss: 2.1136311491330466

Epoch: 6| Step: 8
Training loss: 2.718871593475342
Validation loss: 2.0924625595410666

Epoch: 6| Step: 9
Training loss: 1.8532428741455078
Validation loss: 2.073162098725637

Epoch: 6| Step: 10
Training loss: 1.8186023235321045
Validation loss: 2.0751517017682395

Epoch: 6| Step: 11
Training loss: 1.8388404846191406
Validation loss: 2.0606384674708047

Epoch: 6| Step: 12
Training loss: 1.7886440753936768
Validation loss: 2.067414164543152

Epoch: 6| Step: 13
Training loss: 2.0885331630706787
Validation loss: 2.0674172043800354

Epoch: 125| Step: 0
Training loss: 1.8729571104049683
Validation loss: 2.065645158290863

Epoch: 6| Step: 1
Training loss: 2.2573299407958984
Validation loss: 2.0662843386332193

Epoch: 6| Step: 2
Training loss: 1.825659155845642
Validation loss: 2.075327754020691

Epoch: 6| Step: 3
Training loss: 1.8038347959518433
Validation loss: 2.078044136365255

Epoch: 6| Step: 4
Training loss: 2.3035473823547363
Validation loss: 2.087006668249766

Epoch: 6| Step: 5
Training loss: 2.329031229019165
Validation loss: 2.093384007612864

Epoch: 6| Step: 6
Training loss: 1.881446123123169
Validation loss: 2.0927074750264487

Epoch: 6| Step: 7
Training loss: 1.670882225036621
Validation loss: 2.09149299065272

Epoch: 6| Step: 8
Training loss: 1.7856059074401855
Validation loss: 2.0926443139712014

Epoch: 6| Step: 9
Training loss: 2.0919315814971924
Validation loss: 2.096662759780884

Epoch: 6| Step: 10
Training loss: 2.2450485229492188
Validation loss: 2.093319276968638

Epoch: 6| Step: 11
Training loss: 1.911462664604187
Validation loss: 2.1071772376696267

Epoch: 6| Step: 12
Training loss: 2.256422519683838
Validation loss: 2.086953639984131

Epoch: 6| Step: 13
Training loss: 1.6361024379730225
Validation loss: 2.109200576941172

Epoch: 126| Step: 0
Training loss: 2.0283455848693848
Validation loss: 2.0929270585378013

Epoch: 6| Step: 1
Training loss: 1.9161350727081299
Validation loss: 2.0985782146453857

Epoch: 6| Step: 2
Training loss: 1.5459165573120117
Validation loss: 2.0910746455192566

Epoch: 6| Step: 3
Training loss: 2.279197931289673
Validation loss: 2.1039979259173074

Epoch: 6| Step: 4
Training loss: 2.0700292587280273
Validation loss: 2.0928619305292764

Epoch: 6| Step: 5
Training loss: 2.016756534576416
Validation loss: 2.1020604372024536

Epoch: 6| Step: 6
Training loss: 1.8994165658950806
Validation loss: 2.105581283569336

Epoch: 6| Step: 7
Training loss: 2.0041825771331787
Validation loss: 2.1130147178967795

Epoch: 6| Step: 8
Training loss: 2.3145785331726074
Validation loss: 2.0955806573232016

Epoch: 6| Step: 9
Training loss: 1.9091415405273438
Validation loss: 2.1068939765294394

Epoch: 6| Step: 10
Training loss: 2.085718870162964
Validation loss: 2.095094362894694

Epoch: 6| Step: 11
Training loss: 1.916527509689331
Validation loss: 2.091887652873993

Epoch: 6| Step: 12
Training loss: 1.3807042837142944
Validation loss: 2.0945903261502585

Epoch: 6| Step: 13
Training loss: 2.454008102416992
Validation loss: 2.100223978360494

Epoch: 127| Step: 0
Training loss: 1.968498706817627
Validation loss: 2.1011001666386924

Epoch: 6| Step: 1
Training loss: 1.6597704887390137
Validation loss: 2.092374602953593

Epoch: 6| Step: 2
Training loss: 2.268418788909912
Validation loss: 2.0783814986546836

Epoch: 6| Step: 3
Training loss: 1.898171067237854
Validation loss: 2.0839140812555947

Epoch: 6| Step: 4
Training loss: 1.7140817642211914
Validation loss: 2.0741782188415527

Epoch: 6| Step: 5
Training loss: 2.00532865524292
Validation loss: 2.0991239746411643

Epoch: 6| Step: 6
Training loss: 1.9217010736465454
Validation loss: 2.095890700817108

Epoch: 6| Step: 7
Training loss: 2.4505653381347656
Validation loss: 2.1099910736083984

Epoch: 6| Step: 8
Training loss: 2.2719500064849854
Validation loss: 2.118509590625763

Epoch: 6| Step: 9
Training loss: 1.5093446969985962
Validation loss: 2.111788809299469

Epoch: 6| Step: 10
Training loss: 2.2678987979888916
Validation loss: 2.102076530456543

Epoch: 6| Step: 11
Training loss: 1.7697054147720337
Validation loss: 2.1109317938486734

Epoch: 6| Step: 12
Training loss: 2.2328743934631348
Validation loss: 2.1089038451512656

Epoch: 6| Step: 13
Training loss: 2.2722175121307373
Validation loss: 2.089989423751831

Epoch: 128| Step: 0
Training loss: 2.1117382049560547
Validation loss: 2.090275983015696

Epoch: 6| Step: 1
Training loss: 1.7124216556549072
Validation loss: 2.08725635210673

Epoch: 6| Step: 2
Training loss: 1.996845006942749
Validation loss: 2.0676875909169516

Epoch: 6| Step: 3
Training loss: 2.1504180431365967
Validation loss: 2.0620503425598145

Epoch: 6| Step: 4
Training loss: 2.604642868041992
Validation loss: 2.060604194800059

Epoch: 6| Step: 5
Training loss: 1.846196174621582
Validation loss: 2.060552954673767

Epoch: 6| Step: 6
Training loss: 2.202666997909546
Validation loss: 2.0627860029538474

Epoch: 6| Step: 7
Training loss: 1.7098991870880127
Validation loss: 2.062161922454834

Epoch: 6| Step: 8
Training loss: 2.14373517036438
Validation loss: 2.0645600954691568

Epoch: 6| Step: 9
Training loss: 2.6923954486846924
Validation loss: 2.066714644432068

Epoch: 6| Step: 10
Training loss: 2.2490243911743164
Validation loss: 2.061060070991516

Epoch: 6| Step: 11
Training loss: 1.7297203540802002
Validation loss: 2.0742377042770386

Epoch: 6| Step: 12
Training loss: 2.053969383239746
Validation loss: 2.0751704970995584

Epoch: 6| Step: 13
Training loss: 1.421769380569458
Validation loss: 2.0842292507489524

Epoch: 129| Step: 0
Training loss: 2.343574047088623
Validation loss: 2.083832780520121

Epoch: 6| Step: 1
Training loss: 2.4109385013580322
Validation loss: 2.0937613248825073

Epoch: 6| Step: 2
Training loss: 2.1578149795532227
Validation loss: 2.1081028978029885

Epoch: 6| Step: 3
Training loss: 2.2713685035705566
Validation loss: 2.0935895641644797

Epoch: 6| Step: 4
Training loss: 2.467230796813965
Validation loss: 2.088632067044576

Epoch: 6| Step: 5
Training loss: 1.272557258605957
Validation loss: 2.086847424507141

Epoch: 6| Step: 6
Training loss: 2.084932565689087
Validation loss: 2.0841170152028403

Epoch: 6| Step: 7
Training loss: 1.5771257877349854
Validation loss: 2.07575931151708

Epoch: 6| Step: 8
Training loss: 1.984424352645874
Validation loss: 2.075196385383606

Epoch: 6| Step: 9
Training loss: 1.5670411586761475
Validation loss: 2.0655540029207864

Epoch: 6| Step: 10
Training loss: 2.30362868309021
Validation loss: 2.0605076948801675

Epoch: 6| Step: 11
Training loss: 1.7758747339248657
Validation loss: 2.0597251256306968

Epoch: 6| Step: 12
Training loss: 1.9310096502304077
Validation loss: 2.071594993273417

Epoch: 6| Step: 13
Training loss: 2.004020929336548
Validation loss: 2.0710513591766357

Epoch: 130| Step: 0
Training loss: 1.8591032028198242
Validation loss: 2.0694122513135276

Epoch: 6| Step: 1
Training loss: 1.6091645956039429
Validation loss: 2.0773746371269226

Epoch: 6| Step: 2
Training loss: 2.2764501571655273
Validation loss: 2.0763835310935974

Epoch: 6| Step: 3
Training loss: 1.3464295864105225
Validation loss: 2.0798789660135903

Epoch: 6| Step: 4
Training loss: 2.185947895050049
Validation loss: 2.083247939745585

Epoch: 6| Step: 5
Training loss: 2.4487504959106445
Validation loss: 2.0880902806917825

Epoch: 6| Step: 6
Training loss: 1.8930902481079102
Validation loss: 2.0906819303830466

Epoch: 6| Step: 7
Training loss: 2.0129451751708984
Validation loss: 2.0911401311556497

Epoch: 6| Step: 8
Training loss: 1.9343249797821045
Validation loss: 2.117692470550537

Epoch: 6| Step: 9
Training loss: 1.3911014795303345
Validation loss: 2.101500074068705

Epoch: 6| Step: 10
Training loss: 2.141205072402954
Validation loss: 2.081482708454132

Epoch: 6| Step: 11
Training loss: 2.9807112216949463
Validation loss: 2.077936589717865

Epoch: 6| Step: 12
Training loss: 2.158477306365967
Validation loss: 2.082806169986725

Epoch: 6| Step: 13
Training loss: 1.7033863067626953
Validation loss: 2.0736032525698342

Epoch: 131| Step: 0
Training loss: 1.9212477207183838
Validation loss: 2.064713637034098

Epoch: 6| Step: 1
Training loss: 2.310206413269043
Validation loss: 2.0701135198275247

Epoch: 6| Step: 2
Training loss: 2.230720281600952
Validation loss: 2.0691677729288735

Epoch: 6| Step: 3
Training loss: 2.1618990898132324
Validation loss: 2.0712032119433084

Epoch: 6| Step: 4
Training loss: 1.8111752271652222
Validation loss: 2.0741390585899353

Epoch: 6| Step: 5
Training loss: 1.7004774808883667
Validation loss: 2.0765347878138223

Epoch: 6| Step: 6
Training loss: 2.222513198852539
Validation loss: 2.0786702632904053

Epoch: 6| Step: 7
Training loss: 1.5199744701385498
Validation loss: 2.077707509199778

Epoch: 6| Step: 8
Training loss: 2.231613874435425
Validation loss: 2.096583445866903

Epoch: 6| Step: 9
Training loss: 1.7761013507843018
Validation loss: 2.1053794622421265

Epoch: 6| Step: 10
Training loss: 2.1320040225982666
Validation loss: 2.106463352839152

Epoch: 6| Step: 11
Training loss: 1.6396074295043945
Validation loss: 2.1277736822764077

Epoch: 6| Step: 12
Training loss: 2.0864059925079346
Validation loss: 2.114934265613556

Epoch: 6| Step: 13
Training loss: 1.8959540128707886
Validation loss: 2.106663386027018

Epoch: 132| Step: 0
Training loss: 1.9662292003631592
Validation loss: 2.1107355753580728

Epoch: 6| Step: 1
Training loss: 1.3173270225524902
Validation loss: 2.0905335942904153

Epoch: 6| Step: 2
Training loss: 2.4269063472747803
Validation loss: 2.1071048776308694

Epoch: 6| Step: 3
Training loss: 1.6420607566833496
Validation loss: 2.0892563660939536

Epoch: 6| Step: 4
Training loss: 2.436068296432495
Validation loss: 2.0859141747156777

Epoch: 6| Step: 5
Training loss: 2.569683313369751
Validation loss: 2.0981417894363403

Epoch: 6| Step: 6
Training loss: 1.7770957946777344
Validation loss: 2.0948375264803567

Epoch: 6| Step: 7
Training loss: 2.053865432739258
Validation loss: 2.086081941922506

Epoch: 6| Step: 8
Training loss: 1.9749739170074463
Validation loss: 2.0954082012176514

Epoch: 6| Step: 9
Training loss: 1.8933188915252686
Validation loss: 2.106605072816213

Epoch: 6| Step: 10
Training loss: 2.0068235397338867
Validation loss: 2.101478338241577

Epoch: 6| Step: 11
Training loss: 2.262242078781128
Validation loss: 2.109587768713633

Epoch: 6| Step: 12
Training loss: 1.5146245956420898
Validation loss: 2.1122469902038574

Epoch: 6| Step: 13
Training loss: 1.8360507488250732
Validation loss: 2.1357606252034507

Epoch: 133| Step: 0
Training loss: 1.5364990234375
Validation loss: 2.1357202728589377

Epoch: 6| Step: 1
Training loss: 2.279752254486084
Validation loss: 2.143268803755442

Epoch: 6| Step: 2
Training loss: 1.9829902648925781
Validation loss: 2.148990750312805

Epoch: 6| Step: 3
Training loss: 2.857553005218506
Validation loss: 2.1512902776400247

Epoch: 6| Step: 4
Training loss: 1.7399065494537354
Validation loss: 2.130411465962728

Epoch: 6| Step: 5
Training loss: 2.331407308578491
Validation loss: 2.1312090357144675

Epoch: 6| Step: 6
Training loss: 1.9821300506591797
Validation loss: 2.1333899100621543

Epoch: 6| Step: 7
Training loss: 1.9916818141937256
Validation loss: 2.1316901048024497

Epoch: 6| Step: 8
Training loss: 1.6932770013809204
Validation loss: 2.1222429871559143

Epoch: 6| Step: 9
Training loss: 0.876134991645813
Validation loss: 2.0982367396354675

Epoch: 6| Step: 10
Training loss: 2.35650897026062
Validation loss: 2.1046578884124756

Epoch: 6| Step: 11
Training loss: 2.45654296875
Validation loss: 2.099094788233439

Epoch: 6| Step: 12
Training loss: 2.1066596508026123
Validation loss: 2.078733762105306

Epoch: 6| Step: 13
Training loss: 1.696838617324829
Validation loss: 2.085929532845815

Epoch: 134| Step: 0
Training loss: 2.1040234565734863
Validation loss: 2.0759343107541404

Epoch: 6| Step: 1
Training loss: 2.0579111576080322
Validation loss: 2.0856491327285767

Epoch: 6| Step: 2
Training loss: 2.378849744796753
Validation loss: 2.1027875542640686

Epoch: 6| Step: 3
Training loss: 2.0709333419799805
Validation loss: 2.0956621964772544

Epoch: 6| Step: 4
Training loss: 2.612316608428955
Validation loss: 2.0785634915033975

Epoch: 6| Step: 5
Training loss: 2.675523281097412
Validation loss: 2.0974154074986777

Epoch: 6| Step: 6
Training loss: 1.76926589012146
Validation loss: 2.089586873849233

Epoch: 6| Step: 7
Training loss: 1.8173640966415405
Validation loss: 2.093972603480021

Epoch: 6| Step: 8
Training loss: 1.4455034732818604
Validation loss: 2.098278224468231

Epoch: 6| Step: 9
Training loss: 1.7831547260284424
Validation loss: 2.1124611496925354

Epoch: 6| Step: 10
Training loss: 1.5515272617340088
Validation loss: 2.114427169164022

Epoch: 6| Step: 11
Training loss: 1.6926283836364746
Validation loss: 2.1294628580411277

Epoch: 6| Step: 12
Training loss: 2.130464553833008
Validation loss: 2.124036967754364

Epoch: 6| Step: 13
Training loss: 1.681627631187439
Validation loss: 2.1191172997156777

Epoch: 135| Step: 0
Training loss: 2.747176170349121
Validation loss: 2.1178348064422607

Epoch: 6| Step: 1
Training loss: 2.027099370956421
Validation loss: 2.115562995274862

Epoch: 6| Step: 2
Training loss: 1.796675682067871
Validation loss: 2.1093781193097434

Epoch: 6| Step: 3
Training loss: 1.2133980989456177
Validation loss: 2.1040031711260476

Epoch: 6| Step: 4
Training loss: 2.304945707321167
Validation loss: 2.096654156843821

Epoch: 6| Step: 5
Training loss: 2.2482805252075195
Validation loss: 2.094775160153707

Epoch: 6| Step: 6
Training loss: 1.945136547088623
Validation loss: 2.1102985739707947

Epoch: 6| Step: 7
Training loss: 2.0573463439941406
Validation loss: 2.1272116899490356

Epoch: 6| Step: 8
Training loss: 2.5722861289978027
Validation loss: 2.1272664268811545

Epoch: 6| Step: 9
Training loss: 2.0528764724731445
Validation loss: 2.1150184075037637

Epoch: 6| Step: 10
Training loss: 1.3078126907348633
Validation loss: 2.1010886828104653

Epoch: 6| Step: 11
Training loss: 1.6726915836334229
Validation loss: 2.119577248891195

Epoch: 6| Step: 12
Training loss: 1.642160415649414
Validation loss: 2.1217522422472634

Epoch: 6| Step: 13
Training loss: 2.012885093688965
Validation loss: 2.1001561681429544

Epoch: 136| Step: 0
Training loss: 1.363619327545166
Validation loss: 2.098842362562815

Epoch: 6| Step: 1
Training loss: 2.1714649200439453
Validation loss: 2.099575459957123

Epoch: 6| Step: 2
Training loss: 2.051614284515381
Validation loss: 2.094165543715159

Epoch: 6| Step: 3
Training loss: 2.1499061584472656
Validation loss: 2.090736746788025

Epoch: 6| Step: 4
Training loss: 2.6505184173583984
Validation loss: 2.0946016112963357

Epoch: 6| Step: 5
Training loss: 1.4189157485961914
Validation loss: 2.0849516789118447

Epoch: 6| Step: 6
Training loss: 1.787109613418579
Validation loss: 2.0987810095151267

Epoch: 6| Step: 7
Training loss: 1.6671137809753418
Validation loss: 2.074187417825063

Epoch: 6| Step: 8
Training loss: 1.58039391040802
Validation loss: 2.0839634935061135

Epoch: 6| Step: 9
Training loss: 1.8779184818267822
Validation loss: 2.0919851859410605

Epoch: 6| Step: 10
Training loss: 2.3473153114318848
Validation loss: 2.095826586087545

Epoch: 6| Step: 11
Training loss: 2.43442440032959
Validation loss: 2.096567173798879

Epoch: 6| Step: 12
Training loss: 2.0726168155670166
Validation loss: 2.0935713251431785

Epoch: 6| Step: 13
Training loss: 1.9332345724105835
Validation loss: 2.1054171919822693

Epoch: 137| Step: 0
Training loss: 1.632615089416504
Validation loss: 2.1200260718663535

Epoch: 6| Step: 1
Training loss: 2.5505919456481934
Validation loss: 2.1203205982844033

Epoch: 6| Step: 2
Training loss: 1.4319173097610474
Validation loss: 2.1173826456069946

Epoch: 6| Step: 3
Training loss: 2.213747501373291
Validation loss: 2.12672350804011

Epoch: 6| Step: 4
Training loss: 1.7635037899017334
Validation loss: 2.1397035121917725

Epoch: 6| Step: 5
Training loss: 1.2952306270599365
Validation loss: 2.137540817260742

Epoch: 6| Step: 6
Training loss: 2.4030163288116455
Validation loss: 2.142371634642283

Epoch: 6| Step: 7
Training loss: 1.9998230934143066
Validation loss: 2.1335132320721946

Epoch: 6| Step: 8
Training loss: 1.7584713697433472
Validation loss: 2.1078837513923645

Epoch: 6| Step: 9
Training loss: 2.1783671379089355
Validation loss: 2.107944369316101

Epoch: 6| Step: 10
Training loss: 1.6222550868988037
Validation loss: 2.0994624495506287

Epoch: 6| Step: 11
Training loss: 2.347050189971924
Validation loss: 2.1072179476420083

Epoch: 6| Step: 12
Training loss: 1.8075525760650635
Validation loss: 2.1006189982096353

Epoch: 6| Step: 13
Training loss: 2.4350757598876953
Validation loss: 2.103823165098826

Epoch: 138| Step: 0
Training loss: 1.984855055809021
Validation loss: 2.0992026726404824

Epoch: 6| Step: 1
Training loss: 1.9687206745147705
Validation loss: 2.121738692124685

Epoch: 6| Step: 2
Training loss: 2.613938331604004
Validation loss: 2.1166696747144065

Epoch: 6| Step: 3
Training loss: 1.8744093179702759
Validation loss: 2.129414995511373

Epoch: 6| Step: 4
Training loss: 2.1384572982788086
Validation loss: 2.120982607205709

Epoch: 6| Step: 5
Training loss: 2.099264144897461
Validation loss: 2.115705668926239

Epoch: 6| Step: 6
Training loss: 1.6595921516418457
Validation loss: 2.1098016500473022

Epoch: 6| Step: 7
Training loss: 1.7186404466629028
Validation loss: 2.100997726122538

Epoch: 6| Step: 8
Training loss: 1.971670150756836
Validation loss: 2.1057730317115784

Epoch: 6| Step: 9
Training loss: 2.1984808444976807
Validation loss: 2.102086901664734

Epoch: 6| Step: 10
Training loss: 1.8319470882415771
Validation loss: 2.0904709696769714

Epoch: 6| Step: 11
Training loss: 1.5363802909851074
Validation loss: 2.1041164795557656

Epoch: 6| Step: 12
Training loss: 1.9786570072174072
Validation loss: 2.1114468375841775

Epoch: 6| Step: 13
Training loss: 1.8382477760314941
Validation loss: 2.0901517470677695

Epoch: 139| Step: 0
Training loss: 1.9906185865402222
Validation loss: 2.0931977232297263

Epoch: 6| Step: 1
Training loss: 2.791109561920166
Validation loss: 2.0841559767723083

Epoch: 6| Step: 2
Training loss: 1.4659197330474854
Validation loss: 2.0865083932876587

Epoch: 6| Step: 3
Training loss: 1.9332056045532227
Validation loss: 2.085726022720337

Epoch: 6| Step: 4
Training loss: 2.0397942066192627
Validation loss: 2.071252485116323

Epoch: 6| Step: 5
Training loss: 1.9824717044830322
Validation loss: 2.0921174685160318

Epoch: 6| Step: 6
Training loss: 1.7405426502227783
Validation loss: 2.1046560208002725

Epoch: 6| Step: 7
Training loss: 2.2595958709716797
Validation loss: 2.0973440408706665

Epoch: 6| Step: 8
Training loss: 1.6849898099899292
Validation loss: 2.091035544872284

Epoch: 6| Step: 9
Training loss: 1.7359919548034668
Validation loss: 2.106484134991964

Epoch: 6| Step: 10
Training loss: 2.032306671142578
Validation loss: 2.115153133869171

Epoch: 6| Step: 11
Training loss: 1.5446065664291382
Validation loss: 2.1103680531183877

Epoch: 6| Step: 12
Training loss: 1.9102990627288818
Validation loss: 2.12228125333786

Epoch: 6| Step: 13
Training loss: 2.348365545272827
Validation loss: 2.1279186407725015

Epoch: 140| Step: 0
Training loss: 2.2379884719848633
Validation loss: 2.147411584854126

Epoch: 6| Step: 1
Training loss: 2.2682018280029297
Validation loss: 2.134573499361674

Epoch: 6| Step: 2
Training loss: 1.718984842300415
Validation loss: 2.133837421735128

Epoch: 6| Step: 3
Training loss: 2.319869041442871
Validation loss: 2.1362147331237793

Epoch: 6| Step: 4
Training loss: 1.5989623069763184
Validation loss: 2.113541821638743

Epoch: 6| Step: 5
Training loss: 1.6380305290222168
Validation loss: 2.116110344727834

Epoch: 6| Step: 6
Training loss: 1.140927791595459
Validation loss: 2.1109179854393005

Epoch: 6| Step: 7
Training loss: 2.231346607208252
Validation loss: 2.1049331426620483

Epoch: 6| Step: 8
Training loss: 2.4068620204925537
Validation loss: 2.098392148812612

Epoch: 6| Step: 9
Training loss: 1.3452492952346802
Validation loss: 2.101480027039846

Epoch: 6| Step: 10
Training loss: 2.110633611679077
Validation loss: 2.0973670879999795

Epoch: 6| Step: 11
Training loss: 2.036447048187256
Validation loss: 2.098156770070394

Epoch: 6| Step: 12
Training loss: 2.5189208984375
Validation loss: 2.092598875363668

Epoch: 6| Step: 13
Training loss: 1.6284345388412476
Validation loss: 2.092024842898051

Epoch: 141| Step: 0
Training loss: 1.8545982837677002
Validation loss: 2.095310091972351

Epoch: 6| Step: 1
Training loss: 1.9755975008010864
Validation loss: 2.1036632855733237

Epoch: 6| Step: 2
Training loss: 2.050579071044922
Validation loss: 2.101111094156901

Epoch: 6| Step: 3
Training loss: 1.4422338008880615
Validation loss: 2.1102128426233926

Epoch: 6| Step: 4
Training loss: 1.9343255758285522
Validation loss: 2.123867094516754

Epoch: 6| Step: 5
Training loss: 1.9854180812835693
Validation loss: 2.1237664024035134

Epoch: 6| Step: 6
Training loss: 2.207392454147339
Validation loss: 2.111847380797068

Epoch: 6| Step: 7
Training loss: 1.8854187726974487
Validation loss: 2.121789356072744

Epoch: 6| Step: 8
Training loss: 1.7922817468643188
Validation loss: 2.1137694120407104

Epoch: 6| Step: 9
Training loss: 2.657008171081543
Validation loss: 2.1272176702817283

Epoch: 6| Step: 10
Training loss: 1.2218432426452637
Validation loss: 2.121685961882273

Epoch: 6| Step: 11
Training loss: 1.6486546993255615
Validation loss: 2.139758586883545

Epoch: 6| Step: 12
Training loss: 2.1031930446624756
Validation loss: 2.1294789910316467

Epoch: 6| Step: 13
Training loss: 2.3132822513580322
Validation loss: 2.1386776169141135

Epoch: 142| Step: 0
Training loss: 1.8805649280548096
Validation loss: 2.1296299497286477

Epoch: 6| Step: 1
Training loss: 1.639261245727539
Validation loss: 2.1200830936431885

Epoch: 6| Step: 2
Training loss: 2.34678053855896
Validation loss: 2.1173996527989707

Epoch: 6| Step: 3
Training loss: 2.320852279663086
Validation loss: 2.1279545426368713

Epoch: 6| Step: 4
Training loss: 2.414884328842163
Validation loss: 2.10906853278478

Epoch: 6| Step: 5
Training loss: 1.2164814472198486
Validation loss: 2.1122829715410867

Epoch: 6| Step: 6
Training loss: 1.8747153282165527
Validation loss: 2.111905892690023

Epoch: 6| Step: 7
Training loss: 1.713244915008545
Validation loss: 2.0966416597366333

Epoch: 6| Step: 8
Training loss: 1.5335910320281982
Validation loss: 2.1049912373224893

Epoch: 6| Step: 9
Training loss: 2.998117446899414
Validation loss: 2.11119681596756

Epoch: 6| Step: 10
Training loss: 2.3125486373901367
Validation loss: 2.105298340320587

Epoch: 6| Step: 11
Training loss: 1.4482498168945312
Validation loss: 2.0961415568987527

Epoch: 6| Step: 12
Training loss: 1.8056824207305908
Validation loss: 2.109280447165171

Epoch: 6| Step: 13
Training loss: 1.5551033020019531
Validation loss: 2.1228718757629395

Epoch: 143| Step: 0
Training loss: 1.9688785076141357
Validation loss: 2.125704010327657

Epoch: 6| Step: 1
Training loss: 2.1550121307373047
Validation loss: 2.136790712674459

Epoch: 6| Step: 2
Training loss: 1.5167999267578125
Validation loss: 2.1464203000068665

Epoch: 6| Step: 3
Training loss: 2.3309366703033447
Validation loss: 2.13297963142395

Epoch: 6| Step: 4
Training loss: 2.1529319286346436
Validation loss: 2.1032874981562295

Epoch: 6| Step: 5
Training loss: 1.7884831428527832
Validation loss: 2.1127153833707175

Epoch: 6| Step: 6
Training loss: 1.8132052421569824
Validation loss: 2.099027097225189

Epoch: 6| Step: 7
Training loss: 2.551818370819092
Validation loss: 2.089903096357981

Epoch: 6| Step: 8
Training loss: 1.5733189582824707
Validation loss: 2.0926155845324197

Epoch: 6| Step: 9
Training loss: 1.633007287979126
Validation loss: 2.1006691058476767

Epoch: 6| Step: 10
Training loss: 1.969358205795288
Validation loss: 2.109821836153666

Epoch: 6| Step: 11
Training loss: 1.6197590827941895
Validation loss: 2.1187802950541177

Epoch: 6| Step: 12
Training loss: 2.4347786903381348
Validation loss: 2.124792536099752

Epoch: 6| Step: 13
Training loss: 1.6469669342041016
Validation loss: 2.121080239613851

Epoch: 144| Step: 0
Training loss: 1.9101663827896118
Validation loss: 2.1317750811576843

Epoch: 6| Step: 1
Training loss: 2.019510269165039
Validation loss: 2.129947622617086

Epoch: 6| Step: 2
Training loss: 1.704643964767456
Validation loss: 2.1334137519200644

Epoch: 6| Step: 3
Training loss: 2.641977310180664
Validation loss: 2.132058540980021

Epoch: 6| Step: 4
Training loss: 1.5303964614868164
Validation loss: 2.128538171450297

Epoch: 6| Step: 5
Training loss: 1.4789632558822632
Validation loss: 2.1141156355539956

Epoch: 6| Step: 6
Training loss: 1.0541967153549194
Validation loss: 2.119952400525411

Epoch: 6| Step: 7
Training loss: 2.3292555809020996
Validation loss: 2.1196924646695456

Epoch: 6| Step: 8
Training loss: 1.7032753229141235
Validation loss: 2.1149975061416626

Epoch: 6| Step: 9
Training loss: 1.3871691226959229
Validation loss: 2.112040360768636

Epoch: 6| Step: 10
Training loss: 2.7284481525421143
Validation loss: 2.1125115950902305

Epoch: 6| Step: 11
Training loss: 2.011110305786133
Validation loss: 2.1119791070620217

Epoch: 6| Step: 12
Training loss: 2.7368197441101074
Validation loss: 2.1256508827209473

Epoch: 6| Step: 13
Training loss: 1.7040786743164062
Validation loss: 2.124342600504557

Epoch: 145| Step: 0
Training loss: 1.7909191846847534
Validation loss: 2.129173537095388

Epoch: 6| Step: 1
Training loss: 1.9234411716461182
Validation loss: 2.1397852102915444

Epoch: 6| Step: 2
Training loss: 1.8273494243621826
Validation loss: 2.1379230419794717

Epoch: 6| Step: 3
Training loss: 2.6970245838165283
Validation loss: 2.137954513231913

Epoch: 6| Step: 4
Training loss: 2.132042646408081
Validation loss: 2.127899408340454

Epoch: 6| Step: 5
Training loss: 2.0295956134796143
Validation loss: 2.1418834129969277

Epoch: 6| Step: 6
Training loss: 1.8995071649551392
Validation loss: 2.1093228260676065

Epoch: 6| Step: 7
Training loss: 2.2282612323760986
Validation loss: 2.1365928252538047

Epoch: 6| Step: 8
Training loss: 1.8450589179992676
Validation loss: 2.122427006562551

Epoch: 6| Step: 9
Training loss: 1.5631264448165894
Validation loss: 2.1094197630882263

Epoch: 6| Step: 10
Training loss: 1.3436942100524902
Validation loss: 2.1136196653048196

Epoch: 6| Step: 11
Training loss: 1.7289459705352783
Validation loss: 2.111650208632151

Epoch: 6| Step: 12
Training loss: 2.2761330604553223
Validation loss: 2.111101984977722

Epoch: 6| Step: 13
Training loss: 1.822853922843933
Validation loss: 2.1137362917264304

Epoch: 146| Step: 0
Training loss: 2.1831445693969727
Validation loss: 2.1083373626073203

Epoch: 6| Step: 1
Training loss: 2.8548083305358887
Validation loss: 2.121233900388082

Epoch: 6| Step: 2
Training loss: 2.0100207328796387
Validation loss: 2.1182854175567627

Epoch: 6| Step: 3
Training loss: 1.8263782262802124
Validation loss: 2.1206400990486145

Epoch: 6| Step: 4
Training loss: 2.1723594665527344
Validation loss: 2.129414757092794

Epoch: 6| Step: 5
Training loss: 1.665645956993103
Validation loss: 2.1001198291778564

Epoch: 6| Step: 6
Training loss: 1.9576654434204102
Validation loss: 2.0947925249735513

Epoch: 6| Step: 7
Training loss: 1.635728120803833
Validation loss: 2.102431853612264

Epoch: 6| Step: 8
Training loss: 1.52628755569458
Validation loss: 2.093318740526835

Epoch: 6| Step: 9
Training loss: 1.7906677722930908
Validation loss: 2.1029404600461326

Epoch: 6| Step: 10
Training loss: 1.6309574842453003
Validation loss: 2.1097277998924255

Epoch: 6| Step: 11
Training loss: 1.4500093460083008
Validation loss: 2.09443332751592

Epoch: 6| Step: 12
Training loss: 2.241194725036621
Validation loss: 2.1124327381451926

Epoch: 6| Step: 13
Training loss: 2.3059234619140625
Validation loss: 2.1012904047966003

Epoch: 147| Step: 0
Training loss: 2.1364822387695312
Validation loss: 2.1108157436052957

Epoch: 6| Step: 1
Training loss: 1.7762775421142578
Validation loss: 2.116066813468933

Epoch: 6| Step: 2
Training loss: 1.715240716934204
Validation loss: 2.105975091457367

Epoch: 6| Step: 3
Training loss: 1.420788288116455
Validation loss: 2.1085354685783386

Epoch: 6| Step: 4
Training loss: 1.9912147521972656
Validation loss: 2.119643807411194

Epoch: 6| Step: 5
Training loss: 1.9183881282806396
Validation loss: 2.1257431705792746

Epoch: 6| Step: 6
Training loss: 1.6879777908325195
Validation loss: 2.11088893810908

Epoch: 6| Step: 7
Training loss: 1.9097812175750732
Validation loss: 2.13018806775411

Epoch: 6| Step: 8
Training loss: 1.4267456531524658
Validation loss: 2.117135842641195

Epoch: 6| Step: 9
Training loss: 1.9679269790649414
Validation loss: 2.1274118224779763

Epoch: 6| Step: 10
Training loss: 2.523444890975952
Validation loss: 2.1310086051623025

Epoch: 6| Step: 11
Training loss: 1.6691535711288452
Validation loss: 2.1607468326886496

Epoch: 6| Step: 12
Training loss: 1.906292200088501
Validation loss: 2.1359484990437827

Epoch: 6| Step: 13
Training loss: 2.713243007659912
Validation loss: 2.1354989608128867

Epoch: 148| Step: 0
Training loss: 2.449493885040283
Validation loss: 2.131606101989746

Epoch: 6| Step: 1
Training loss: 1.1206055879592896
Validation loss: 2.1268104712168374

Epoch: 6| Step: 2
Training loss: 1.9547871351242065
Validation loss: 2.124608039855957

Epoch: 6| Step: 3
Training loss: 2.1126463413238525
Validation loss: 2.130537509918213

Epoch: 6| Step: 4
Training loss: 2.135230779647827
Validation loss: 2.1257726550102234

Epoch: 6| Step: 5
Training loss: 2.5503029823303223
Validation loss: 2.124764303366343

Epoch: 6| Step: 6
Training loss: 1.3979992866516113
Validation loss: 2.132362504800161

Epoch: 6| Step: 7
Training loss: 1.6576330661773682
Validation loss: 2.132053275903066

Epoch: 6| Step: 8
Training loss: 1.5228475332260132
Validation loss: 2.1502001881599426

Epoch: 6| Step: 9
Training loss: 1.8858519792556763
Validation loss: 2.1208706895510354

Epoch: 6| Step: 10
Training loss: 1.7435754537582397
Validation loss: 2.157233715057373

Epoch: 6| Step: 11
Training loss: 1.4584965705871582
Validation loss: 2.1345324516296387

Epoch: 6| Step: 12
Training loss: 2.881039619445801
Validation loss: 2.120250145594279

Epoch: 6| Step: 13
Training loss: 2.1122682094573975
Validation loss: 2.141346494356791

Epoch: 149| Step: 0
Training loss: 1.7878806591033936
Validation loss: 2.1101235151290894

Epoch: 6| Step: 1
Training loss: 1.82212233543396
Validation loss: 2.1209336519241333

Epoch: 6| Step: 2
Training loss: 1.4675123691558838
Validation loss: 2.1188129583994546

Epoch: 6| Step: 3
Training loss: 1.5975322723388672
Validation loss: 2.126720428466797

Epoch: 6| Step: 4
Training loss: 1.7432669401168823
Validation loss: 2.1262181997299194

Epoch: 6| Step: 5
Training loss: 2.3432040214538574
Validation loss: 2.1201666593551636

Epoch: 6| Step: 6
Training loss: 1.7485121488571167
Validation loss: 2.1199649771054587

Epoch: 6| Step: 7
Training loss: 1.7278776168823242
Validation loss: 2.127607266108195

Epoch: 6| Step: 8
Training loss: 2.179131507873535
Validation loss: 2.11701500415802

Epoch: 6| Step: 9
Training loss: 1.714714765548706
Validation loss: 2.100974957148234

Epoch: 6| Step: 10
Training loss: 1.9007558822631836
Validation loss: 2.10992564757665

Epoch: 6| Step: 11
Training loss: 2.0626888275146484
Validation loss: 2.110775093237559

Epoch: 6| Step: 12
Training loss: 2.343441963195801
Validation loss: 2.1332901318868003

Epoch: 6| Step: 13
Training loss: 2.7652838230133057
Validation loss: 2.143467426300049

Epoch: 150| Step: 0
Training loss: 1.743430495262146
Validation loss: 2.155718127886454

Epoch: 6| Step: 1
Training loss: 1.5891231298446655
Validation loss: 2.134871502717336

Epoch: 6| Step: 2
Training loss: 1.7378723621368408
Validation loss: 2.138296127319336

Epoch: 6| Step: 3
Training loss: 2.2343573570251465
Validation loss: 2.136564234892527

Epoch: 6| Step: 4
Training loss: 2.1675758361816406
Validation loss: 2.1183348298072815

Epoch: 6| Step: 5
Training loss: 1.4803004264831543
Validation loss: 2.1136303742726645

Epoch: 6| Step: 6
Training loss: 2.4530956745147705
Validation loss: 2.1199330488840737

Epoch: 6| Step: 7
Training loss: 1.8015908002853394
Validation loss: 2.12546577056249

Epoch: 6| Step: 8
Training loss: 1.6858563423156738
Validation loss: 2.10254297653834

Epoch: 6| Step: 9
Training loss: 1.7504940032958984
Validation loss: 2.1133991877237954

Epoch: 6| Step: 10
Training loss: 2.294224977493286
Validation loss: 2.1106061140696206

Epoch: 6| Step: 11
Training loss: 1.32571280002594
Validation loss: 2.126704831918081

Epoch: 6| Step: 12
Training loss: 2.4762580394744873
Validation loss: 2.116720120112101

Epoch: 6| Step: 13
Training loss: 2.2610745429992676
Validation loss: 2.1095892190933228

Testing loss: 1.767938656772641

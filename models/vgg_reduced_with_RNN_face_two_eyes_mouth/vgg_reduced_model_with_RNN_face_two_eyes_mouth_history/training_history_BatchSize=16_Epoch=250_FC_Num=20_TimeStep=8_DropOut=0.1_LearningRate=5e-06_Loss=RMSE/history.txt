Epoch: 1| Step: 0
Training loss: 6.899302306353384
Validation loss: 5.812080751017808

Epoch: 6| Step: 1
Training loss: 5.426103245935049
Validation loss: 5.8100075795774755

Epoch: 6| Step: 2
Training loss: 6.644367626402468
Validation loss: 5.807911062922303

Epoch: 6| Step: 3
Training loss: 5.113607071301342
Validation loss: 5.805837689266631

Epoch: 6| Step: 4
Training loss: 5.918323911578622
Validation loss: 5.803692013113111

Epoch: 6| Step: 5
Training loss: 6.795966287135252
Validation loss: 5.801383925447397

Epoch: 6| Step: 6
Training loss: 5.934775279417705
Validation loss: 5.799127708260711

Epoch: 6| Step: 7
Training loss: 5.50172899986132
Validation loss: 5.796693728783942

Epoch: 6| Step: 8
Training loss: 5.922352955233856
Validation loss: 5.794280699532214

Epoch: 6| Step: 9
Training loss: 6.553709700608026
Validation loss: 5.791656091620947

Epoch: 6| Step: 10
Training loss: 5.723876656344962
Validation loss: 5.7889383416840765

Epoch: 6| Step: 11
Training loss: 6.089375808873945
Validation loss: 5.786175473427471

Epoch: 6| Step: 12
Training loss: 4.125439707122092
Validation loss: 5.783378442960299

Epoch: 6| Step: 13
Training loss: 5.559794593145393
Validation loss: 5.780483573079626

Epoch: 2| Step: 0
Training loss: 6.732020982316355
Validation loss: 5.777613415377253

Epoch: 6| Step: 1
Training loss: 4.776787963377725
Validation loss: 5.7743921518729975

Epoch: 6| Step: 2
Training loss: 5.466950213049137
Validation loss: 5.771161516500129

Epoch: 6| Step: 3
Training loss: 5.998530525823661
Validation loss: 5.7677850303995415

Epoch: 6| Step: 4
Training loss: 5.881616640570909
Validation loss: 5.764255212788764

Epoch: 6| Step: 5
Training loss: 6.138104491755495
Validation loss: 5.760577107058279

Epoch: 6| Step: 6
Training loss: 6.873320218260888
Validation loss: 5.7566370046646975

Epoch: 6| Step: 7
Training loss: 4.872492218559397
Validation loss: 5.752509025821958

Epoch: 6| Step: 8
Training loss: 6.089528034632102
Validation loss: 5.748208112288291

Epoch: 6| Step: 9
Training loss: 5.859951143549335
Validation loss: 5.743883307248327

Epoch: 6| Step: 10
Training loss: 6.310973936628846
Validation loss: 5.739202977181889

Epoch: 6| Step: 11
Training loss: 5.418970742838825
Validation loss: 5.734342070856577

Epoch: 6| Step: 12
Training loss: 6.114526006461286
Validation loss: 5.729129509082866

Epoch: 6| Step: 13
Training loss: 5.211241111317733
Validation loss: 5.72378204694403

Epoch: 3| Step: 0
Training loss: 4.980875251486833
Validation loss: 5.71822350585094

Epoch: 6| Step: 1
Training loss: 4.82059145325846
Validation loss: 5.71255374273817

Epoch: 6| Step: 2
Training loss: 5.297908895859124
Validation loss: 5.70643877578568

Epoch: 6| Step: 3
Training loss: 6.886008032845878
Validation loss: 5.7004721128131095

Epoch: 6| Step: 4
Training loss: 5.386406085129121
Validation loss: 5.694137137993504

Epoch: 6| Step: 5
Training loss: 6.352148950958724
Validation loss: 5.687280839323561

Epoch: 6| Step: 6
Training loss: 5.233262948860772
Validation loss: 5.680364219512759

Epoch: 6| Step: 7
Training loss: 5.939439316306704
Validation loss: 5.673125522377176

Epoch: 6| Step: 8
Training loss: 5.775341747129998
Validation loss: 5.665824303941264

Epoch: 6| Step: 9
Training loss: 5.311705047369809
Validation loss: 5.657836972443305

Epoch: 6| Step: 10
Training loss: 5.9623474613406975
Validation loss: 5.650002135779953

Epoch: 6| Step: 11
Training loss: 5.921082617254345
Validation loss: 5.642028690294486

Epoch: 6| Step: 12
Training loss: 6.097666572400776
Validation loss: 5.63367485538528

Epoch: 6| Step: 13
Training loss: 6.656035943686195
Validation loss: 5.624928650580114

Epoch: 4| Step: 0
Training loss: 6.388377496274943
Validation loss: 5.616273454585606

Epoch: 6| Step: 1
Training loss: 5.207454678488871
Validation loss: 5.607106804840285

Epoch: 6| Step: 2
Training loss: 5.59829356897255
Validation loss: 5.59783229653092

Epoch: 6| Step: 3
Training loss: 5.552577566320427
Validation loss: 5.5888161664410605

Epoch: 6| Step: 4
Training loss: 4.86895866681424
Validation loss: 5.579364685160733

Epoch: 6| Step: 5
Training loss: 4.728642685328192
Validation loss: 5.570283680146197

Epoch: 6| Step: 6
Training loss: 5.742910907875968
Validation loss: 5.560956219245693

Epoch: 6| Step: 7
Training loss: 6.5192568237254935
Validation loss: 5.55130426930219

Epoch: 6| Step: 8
Training loss: 5.6396436180985905
Validation loss: 5.541680223644376

Epoch: 6| Step: 9
Training loss: 5.979883166546557
Validation loss: 5.531775268211942

Epoch: 6| Step: 10
Training loss: 4.735933657864554
Validation loss: 5.52212113049749

Epoch: 6| Step: 11
Training loss: 6.33253758434011
Validation loss: 5.5124195787902615

Epoch: 6| Step: 12
Training loss: 5.745583538170148
Validation loss: 5.502705833216794

Epoch: 6| Step: 13
Training loss: 5.98615829948753
Validation loss: 5.493138049706927

Epoch: 5| Step: 0
Training loss: 5.350066082983009
Validation loss: 5.483329013873006

Epoch: 6| Step: 1
Training loss: 5.483672789914229
Validation loss: 5.473787527084204

Epoch: 6| Step: 2
Training loss: 4.8438828973074015
Validation loss: 5.464528283518058

Epoch: 6| Step: 3
Training loss: 4.796717339823057
Validation loss: 5.455220021333739

Epoch: 6| Step: 4
Training loss: 6.765224383256666
Validation loss: 5.446355981514273

Epoch: 6| Step: 5
Training loss: 6.035613226024358
Validation loss: 5.436987052488109

Epoch: 6| Step: 6
Training loss: 5.3794508847361975
Validation loss: 5.427798916530325

Epoch: 6| Step: 7
Training loss: 5.2864325561897205
Validation loss: 5.418974790563407

Epoch: 6| Step: 8
Training loss: 5.448094712138515
Validation loss: 5.409813149563646

Epoch: 6| Step: 9
Training loss: 5.120998029592649
Validation loss: 5.4006561269217395

Epoch: 6| Step: 10
Training loss: 5.421656535471705
Validation loss: 5.39189983533914

Epoch: 6| Step: 11
Training loss: 6.662451206050071
Validation loss: 5.382952846548526

Epoch: 6| Step: 12
Training loss: 4.980330688956538
Validation loss: 5.374087707220252

Epoch: 6| Step: 13
Training loss: 5.611107008946161
Validation loss: 5.364705747917503

Epoch: 6| Step: 0
Training loss: 5.649439299452058
Validation loss: 5.35534973531673

Epoch: 6| Step: 1
Training loss: 5.851303359686633
Validation loss: 5.346704956180776

Epoch: 6| Step: 2
Training loss: 5.2655828004498755
Validation loss: 5.337344429554119

Epoch: 6| Step: 3
Training loss: 5.019112868628988
Validation loss: 5.3281924783231664

Epoch: 6| Step: 4
Training loss: 5.409674116885722
Validation loss: 5.319142904386415

Epoch: 6| Step: 5
Training loss: 5.053639888540425
Validation loss: 5.310190005258515

Epoch: 6| Step: 6
Training loss: 5.002326042814787
Validation loss: 5.300994981182999

Epoch: 6| Step: 7
Training loss: 5.81375600718957
Validation loss: 5.292251174163772

Epoch: 6| Step: 8
Training loss: 5.943310273889354
Validation loss: 5.2833181691879725

Epoch: 6| Step: 9
Training loss: 5.592249834300012
Validation loss: 5.274195246002997

Epoch: 6| Step: 10
Training loss: 5.016666102277316
Validation loss: 5.265242414602543

Epoch: 6| Step: 11
Training loss: 5.344321058668257
Validation loss: 5.256287095116267

Epoch: 6| Step: 12
Training loss: 5.575836310502976
Validation loss: 5.247260120489014

Epoch: 6| Step: 13
Training loss: 5.1954813764846115
Validation loss: 5.2378643494867125

Epoch: 7| Step: 0
Training loss: 5.594565561181616
Validation loss: 5.228241442204308

Epoch: 6| Step: 1
Training loss: 4.458066320296515
Validation loss: 5.218746984789314

Epoch: 6| Step: 2
Training loss: 6.05921013043657
Validation loss: 5.210271774691396

Epoch: 6| Step: 3
Training loss: 4.56607876819737
Validation loss: 5.201839742685031

Epoch: 6| Step: 4
Training loss: 4.424544643960154
Validation loss: 5.193124624979144

Epoch: 6| Step: 5
Training loss: 5.456169611982091
Validation loss: 5.185045297315073

Epoch: 6| Step: 6
Training loss: 5.208439675834935
Validation loss: 5.176850383386387

Epoch: 6| Step: 7
Training loss: 5.852618826238741
Validation loss: 5.16869161773515

Epoch: 6| Step: 8
Training loss: 5.306950801226692
Validation loss: 5.16034734517943

Epoch: 6| Step: 9
Training loss: 5.512556654450843
Validation loss: 5.152099264233129

Epoch: 6| Step: 10
Training loss: 5.2246272634039554
Validation loss: 5.143934890681087

Epoch: 6| Step: 11
Training loss: 5.062900857007831
Validation loss: 5.135578960255508

Epoch: 6| Step: 12
Training loss: 5.5581370184153
Validation loss: 5.127344169114876

Epoch: 6| Step: 13
Training loss: 5.549535151079449
Validation loss: 5.1190855432501765

Epoch: 8| Step: 0
Training loss: 4.756616501591535
Validation loss: 5.110634091595315

Epoch: 6| Step: 1
Training loss: 5.414485590092771
Validation loss: 5.102110821094458

Epoch: 6| Step: 2
Training loss: 5.860183863440641
Validation loss: 5.09388167946329

Epoch: 6| Step: 3
Training loss: 4.123455567661842
Validation loss: 5.0854783561410235

Epoch: 6| Step: 4
Training loss: 4.83395103366027
Validation loss: 5.0771613982090775

Epoch: 6| Step: 5
Training loss: 4.540396883249511
Validation loss: 5.068716619956301

Epoch: 6| Step: 6
Training loss: 4.737900834249863
Validation loss: 5.06085319578378

Epoch: 6| Step: 7
Training loss: 4.94605875034509
Validation loss: 5.052544180179689

Epoch: 6| Step: 8
Training loss: 4.799004793311378
Validation loss: 5.044463976338161

Epoch: 6| Step: 9
Training loss: 4.582385479567341
Validation loss: 5.036373333639422

Epoch: 6| Step: 10
Training loss: 6.428469944712981
Validation loss: 5.027745418315654

Epoch: 6| Step: 11
Training loss: 5.764411777948549
Validation loss: 5.02008175533841

Epoch: 6| Step: 12
Training loss: 5.437783420235733
Validation loss: 5.011938655683868

Epoch: 6| Step: 13
Training loss: 5.755176370339222
Validation loss: 5.003613787285347

Epoch: 9| Step: 0
Training loss: 5.125034425201169
Validation loss: 4.995362817635947

Epoch: 6| Step: 1
Training loss: 4.457981392822864
Validation loss: 4.986921084657552

Epoch: 6| Step: 2
Training loss: 4.348710555215418
Validation loss: 4.979315985618039

Epoch: 6| Step: 3
Training loss: 5.177520713009517
Validation loss: 4.972044230613476

Epoch: 6| Step: 4
Training loss: 6.256938287485217
Validation loss: 4.963934942505156

Epoch: 6| Step: 5
Training loss: 5.051247606925898
Validation loss: 4.956520112331035

Epoch: 6| Step: 6
Training loss: 5.182307574632723
Validation loss: 4.9480369580018575

Epoch: 6| Step: 7
Training loss: 5.570540200136598
Validation loss: 4.940584048496161

Epoch: 6| Step: 8
Training loss: 4.063263806652378
Validation loss: 4.932105034342185

Epoch: 6| Step: 9
Training loss: 4.7216751429980715
Validation loss: 4.924090852427626

Epoch: 6| Step: 10
Training loss: 4.876214096300927
Validation loss: 4.916711009628422

Epoch: 6| Step: 11
Training loss: 5.145951222087374
Validation loss: 4.908867674251627

Epoch: 6| Step: 12
Training loss: 4.358123490270247
Validation loss: 4.9020722859921895

Epoch: 6| Step: 13
Training loss: 6.10991071220626
Validation loss: 4.894265539330113

Epoch: 10| Step: 0
Training loss: 5.382323487856308
Validation loss: 4.886391754553383

Epoch: 6| Step: 1
Training loss: 4.603136916964165
Validation loss: 4.879384017694297

Epoch: 6| Step: 2
Training loss: 4.773055490544848
Validation loss: 4.871844843815357

Epoch: 6| Step: 3
Training loss: 5.264594546045586
Validation loss: 4.864346004907609

Epoch: 6| Step: 4
Training loss: 4.917627445464206
Validation loss: 4.857684496590566

Epoch: 6| Step: 5
Training loss: 4.5069721040169215
Validation loss: 4.849879924690425

Epoch: 6| Step: 6
Training loss: 3.9631236888171646
Validation loss: 4.842668802328801

Epoch: 6| Step: 7
Training loss: 4.507789651426063
Validation loss: 4.834941360843901

Epoch: 6| Step: 8
Training loss: 5.024225864277831
Validation loss: 4.828115584643092

Epoch: 6| Step: 9
Training loss: 5.944271362415186
Validation loss: 4.821470742823331

Epoch: 6| Step: 10
Training loss: 5.747913147760604
Validation loss: 4.813809303427549

Epoch: 6| Step: 11
Training loss: 5.020861592615569
Validation loss: 4.806933217790386

Epoch: 6| Step: 12
Training loss: 4.879074350336679
Validation loss: 4.799979266810674

Epoch: 6| Step: 13
Training loss: 4.620476237270319
Validation loss: 4.794060261287821

Epoch: 11| Step: 0
Training loss: 4.749523339699017
Validation loss: 4.786025544179829

Epoch: 6| Step: 1
Training loss: 5.139591565362002
Validation loss: 4.7794221197277915

Epoch: 6| Step: 2
Training loss: 4.8066195937420835
Validation loss: 4.772109292771123

Epoch: 6| Step: 3
Training loss: 4.396764623453561
Validation loss: 4.765828012613651

Epoch: 6| Step: 4
Training loss: 4.448956382189535
Validation loss: 4.759945947035656

Epoch: 6| Step: 5
Training loss: 3.7941329536529316
Validation loss: 4.752261242622454

Epoch: 6| Step: 6
Training loss: 5.370117276294553
Validation loss: 4.746239193962708

Epoch: 6| Step: 7
Training loss: 5.391898272972514
Validation loss: 4.7395429448792825

Epoch: 6| Step: 8
Training loss: 5.410897611095932
Validation loss: 4.734484546430563

Epoch: 6| Step: 9
Training loss: 5.027117246135849
Validation loss: 4.7296302444963

Epoch: 6| Step: 10
Training loss: 4.745945052968985
Validation loss: 4.723763284858865

Epoch: 6| Step: 11
Training loss: 5.036924017830859
Validation loss: 4.718035763881915

Epoch: 6| Step: 12
Training loss: 4.323199282067703
Validation loss: 4.710969755063362

Epoch: 6| Step: 13
Training loss: 5.255218092496926
Validation loss: 4.7035350599674945

Epoch: 12| Step: 0
Training loss: 4.469407253349046
Validation loss: 4.696270549269932

Epoch: 6| Step: 1
Training loss: 4.780701051003137
Validation loss: 4.693335533213822

Epoch: 6| Step: 2
Training loss: 4.721637978907046
Validation loss: 4.685370440472029

Epoch: 6| Step: 3
Training loss: 5.012881280186879
Validation loss: 4.67852133526846

Epoch: 6| Step: 4
Training loss: 5.214660869920618
Validation loss: 4.670070201773611

Epoch: 6| Step: 5
Training loss: 4.259106024843638
Validation loss: 4.665520163889361

Epoch: 6| Step: 6
Training loss: 4.963541239698084
Validation loss: 4.659146084568006

Epoch: 6| Step: 7
Training loss: 5.241041441938631
Validation loss: 4.6527889283206365

Epoch: 6| Step: 8
Training loss: 4.9583254688531815
Validation loss: 4.645855970569878

Epoch: 6| Step: 9
Training loss: 4.8593369264736355
Validation loss: 4.639968774295839

Epoch: 6| Step: 10
Training loss: 4.639744530583773
Validation loss: 4.633395402250161

Epoch: 6| Step: 11
Training loss: 4.495347903221295
Validation loss: 4.6286989318519405

Epoch: 6| Step: 12
Training loss: 4.421927192299093
Validation loss: 4.6219547659469224

Epoch: 6| Step: 13
Training loss: 4.836170197097386
Validation loss: 4.617672829635283

Epoch: 13| Step: 0
Training loss: 4.57355278763877
Validation loss: 4.6124222849679954

Epoch: 6| Step: 1
Training loss: 5.064141466560246
Validation loss: 4.606679285439756

Epoch: 6| Step: 2
Training loss: 5.036684690620358
Validation loss: 4.600959673579819

Epoch: 6| Step: 3
Training loss: 5.527930691561978
Validation loss: 4.596043055508374

Epoch: 6| Step: 4
Training loss: 4.9258641138450425
Validation loss: 4.58968034602194

Epoch: 6| Step: 5
Training loss: 4.433422955823707
Validation loss: 4.5832895565832255

Epoch: 6| Step: 6
Training loss: 4.813381560199376
Validation loss: 4.578502826050617

Epoch: 6| Step: 7
Training loss: 3.363592295158817
Validation loss: 4.573881784608069

Epoch: 6| Step: 8
Training loss: 4.547554935335315
Validation loss: 4.567782214040996

Epoch: 6| Step: 9
Training loss: 4.892367798934756
Validation loss: 4.562050888740984

Epoch: 6| Step: 10
Training loss: 4.794509751891879
Validation loss: 4.556721498698516

Epoch: 6| Step: 11
Training loss: 4.150373161727966
Validation loss: 4.55125546583661

Epoch: 6| Step: 12
Training loss: 4.909915016821789
Validation loss: 4.547146783595367

Epoch: 6| Step: 13
Training loss: 4.489196949534403
Validation loss: 4.539520958373745

Epoch: 14| Step: 0
Training loss: 5.218720030555789
Validation loss: 4.534412832384924

Epoch: 6| Step: 1
Training loss: 4.509799248399451
Validation loss: 4.530073394601873

Epoch: 6| Step: 2
Training loss: 4.102536621674172
Validation loss: 4.524812567325803

Epoch: 6| Step: 3
Training loss: 3.6471735788745945
Validation loss: 4.5200856318562765

Epoch: 6| Step: 4
Training loss: 4.728098520430437
Validation loss: 4.514733161546612

Epoch: 6| Step: 5
Training loss: 4.62717654589332
Validation loss: 4.509133765098786

Epoch: 6| Step: 6
Training loss: 5.43288708279892
Validation loss: 4.504676684577088

Epoch: 6| Step: 7
Training loss: 4.970808646350492
Validation loss: 4.498849032817913

Epoch: 6| Step: 8
Training loss: 5.21169713558655
Validation loss: 4.49379740739981

Epoch: 6| Step: 9
Training loss: 3.436218855277164
Validation loss: 4.488564354246139

Epoch: 6| Step: 10
Training loss: 4.126439623608366
Validation loss: 4.483167319550754

Epoch: 6| Step: 11
Training loss: 5.104657809962537
Validation loss: 4.478253518403529

Epoch: 6| Step: 12
Training loss: 4.440128031766265
Validation loss: 4.473222037653918

Epoch: 6| Step: 13
Training loss: 4.754756353094754
Validation loss: 4.467214965380074

Epoch: 15| Step: 0
Training loss: 4.509413304367806
Validation loss: 4.462599124751406

Epoch: 6| Step: 1
Training loss: 4.752539106646581
Validation loss: 4.4578433020029316

Epoch: 6| Step: 2
Training loss: 5.23726890618604
Validation loss: 4.452248541706549

Epoch: 6| Step: 3
Training loss: 4.349876858075401
Validation loss: 4.447431455973513

Epoch: 6| Step: 4
Training loss: 5.080444369667493
Validation loss: 4.441819931353771

Epoch: 6| Step: 5
Training loss: 5.014217761599728
Validation loss: 4.437420624931968

Epoch: 6| Step: 6
Training loss: 4.790058872442471
Validation loss: 4.432581777375866

Epoch: 6| Step: 7
Training loss: 3.622416792625419
Validation loss: 4.428664675256811

Epoch: 6| Step: 8
Training loss: 4.698356365369781
Validation loss: 4.423434967467889

Epoch: 6| Step: 9
Training loss: 4.848697424512849
Validation loss: 4.418175019908213

Epoch: 6| Step: 10
Training loss: 4.139979819649426
Validation loss: 4.410976841519496

Epoch: 6| Step: 11
Training loss: 4.612209883665999
Validation loss: 4.405436970618174

Epoch: 6| Step: 12
Training loss: 3.443906605339179
Validation loss: 4.400718987126767

Epoch: 6| Step: 13
Training loss: 4.364072832039998
Validation loss: 4.399118879988099

Epoch: 16| Step: 0
Training loss: 4.213958958480703
Validation loss: 4.38985792090876

Epoch: 6| Step: 1
Training loss: 4.348557700093782
Validation loss: 4.38686581889071

Epoch: 6| Step: 2
Training loss: 4.6814085990818635
Validation loss: 4.384703782855467

Epoch: 6| Step: 3
Training loss: 4.838425201339283
Validation loss: 4.380933534490828

Epoch: 6| Step: 4
Training loss: 5.046199599858425
Validation loss: 4.376647203164404

Epoch: 6| Step: 5
Training loss: 4.215793279932731
Validation loss: 4.370548744985661

Epoch: 6| Step: 6
Training loss: 5.221891086081036
Validation loss: 4.361935372963072

Epoch: 6| Step: 7
Training loss: 4.289441812832274
Validation loss: 4.354946133630463

Epoch: 6| Step: 8
Training loss: 4.332974908139375
Validation loss: 4.350027255268872

Epoch: 6| Step: 9
Training loss: 4.491973924950104
Validation loss: 4.345652358083617

Epoch: 6| Step: 10
Training loss: 4.368962072374141
Validation loss: 4.341091797173333

Epoch: 6| Step: 11
Training loss: 5.021391693767409
Validation loss: 4.33989696544453

Epoch: 6| Step: 12
Training loss: 3.7257433616805833
Validation loss: 4.33222005677466

Epoch: 6| Step: 13
Training loss: 3.771917759764359
Validation loss: 4.3223062965259595

Epoch: 17| Step: 0
Training loss: 4.179699821097493
Validation loss: 4.318795055516898

Epoch: 6| Step: 1
Training loss: 3.8198236882130048
Validation loss: 4.315600381827478

Epoch: 6| Step: 2
Training loss: 4.767578125
Validation loss: 4.311223748845077

Epoch: 6| Step: 3
Training loss: 4.570360493204206
Validation loss: 4.305929199878141

Epoch: 6| Step: 4
Training loss: 4.957457948661203
Validation loss: 4.299925268248315

Epoch: 6| Step: 5
Training loss: 4.265271741610979
Validation loss: 4.294338444011165

Epoch: 6| Step: 6
Training loss: 4.198592794915879
Validation loss: 4.289086495235654

Epoch: 6| Step: 7
Training loss: 4.191552322274092
Validation loss: 4.282542606697797

Epoch: 6| Step: 8
Training loss: 4.5562040182757775
Validation loss: 4.2775401356291365

Epoch: 6| Step: 9
Training loss: 4.1871560937663235
Validation loss: 4.273689127783567

Epoch: 6| Step: 10
Training loss: 4.198994134617684
Validation loss: 4.267852872126985

Epoch: 6| Step: 11
Training loss: 5.024004345084091
Validation loss: 4.262833442212411

Epoch: 6| Step: 12
Training loss: 4.499622329122337
Validation loss: 4.257003567347108

Epoch: 6| Step: 13
Training loss: 4.247655783003009
Validation loss: 4.253458186881162

Epoch: 18| Step: 0
Training loss: 4.724615026867228
Validation loss: 4.248897690031237

Epoch: 6| Step: 1
Training loss: 3.637962579317663
Validation loss: 4.2447953612266645

Epoch: 6| Step: 2
Training loss: 5.173293181456914
Validation loss: 4.237799579304457

Epoch: 6| Step: 3
Training loss: 3.026811319107058
Validation loss: 4.232530612132601

Epoch: 6| Step: 4
Training loss: 4.355836806548004
Validation loss: 4.227445330410133

Epoch: 6| Step: 5
Training loss: 4.9850520807145555
Validation loss: 4.222524729705782

Epoch: 6| Step: 6
Training loss: 4.871097861428197
Validation loss: 4.216430610462438

Epoch: 6| Step: 7
Training loss: 4.639796327527481
Validation loss: 4.212369431189885

Epoch: 6| Step: 8
Training loss: 4.473739292998504
Validation loss: 4.208615674408525

Epoch: 6| Step: 9
Training loss: 3.932692847934332
Validation loss: 4.203360757750899

Epoch: 6| Step: 10
Training loss: 4.414346817447291
Validation loss: 4.198667713058146

Epoch: 6| Step: 11
Training loss: 4.300976380677815
Validation loss: 4.1936035653712995

Epoch: 6| Step: 12
Training loss: 3.461763649324104
Validation loss: 4.187183842656807

Epoch: 6| Step: 13
Training loss: 4.308581797495887
Validation loss: 4.183518162984543

Epoch: 19| Step: 0
Training loss: 3.770089063730712
Validation loss: 4.177854959781625

Epoch: 6| Step: 1
Training loss: 5.0648816946509765
Validation loss: 4.173402009120648

Epoch: 6| Step: 2
Training loss: 4.758602584106683
Validation loss: 4.167257775657083

Epoch: 6| Step: 3
Training loss: 4.913270434663223
Validation loss: 4.162177942320734

Epoch: 6| Step: 4
Training loss: 3.4146838288397863
Validation loss: 4.158256390661804

Epoch: 6| Step: 5
Training loss: 4.153091951788422
Validation loss: 4.152888265023809

Epoch: 6| Step: 6
Training loss: 3.7445098105918553
Validation loss: 4.149008463177524

Epoch: 6| Step: 7
Training loss: 3.8376096976937997
Validation loss: 4.143233910858999

Epoch: 6| Step: 8
Training loss: 3.193323537642945
Validation loss: 4.138849705561122

Epoch: 6| Step: 9
Training loss: 3.7218406079910924
Validation loss: 4.134077180307751

Epoch: 6| Step: 10
Training loss: 4.795360214179977
Validation loss: 4.128813801202323

Epoch: 6| Step: 11
Training loss: 4.703911902413665
Validation loss: 4.125182639037928

Epoch: 6| Step: 12
Training loss: 4.855635697677952
Validation loss: 4.119820145783048

Epoch: 6| Step: 13
Training loss: 4.372268259877031
Validation loss: 4.114612775608747

Epoch: 20| Step: 0
Training loss: 3.802681333292444
Validation loss: 4.110232729050114

Epoch: 6| Step: 1
Training loss: 3.151210879151507
Validation loss: 4.1051636015939295

Epoch: 6| Step: 2
Training loss: 4.671263791932367
Validation loss: 4.100049091448392

Epoch: 6| Step: 3
Training loss: 4.24896496903722
Validation loss: 4.097595796239142

Epoch: 6| Step: 4
Training loss: 4.601893259281903
Validation loss: 4.09377853733046

Epoch: 6| Step: 5
Training loss: 4.233891195550698
Validation loss: 4.089680558828835

Epoch: 6| Step: 6
Training loss: 3.2454314767023975
Validation loss: 4.086154559990941

Epoch: 6| Step: 7
Training loss: 4.4436568410044295
Validation loss: 4.080903277388154

Epoch: 6| Step: 8
Training loss: 4.336620600047895
Validation loss: 4.075528054845423

Epoch: 6| Step: 9
Training loss: 4.819187422515343
Validation loss: 4.06947238913971

Epoch: 6| Step: 10
Training loss: 3.8989365130272624
Validation loss: 4.064558964820339

Epoch: 6| Step: 11
Training loss: 4.237316278396512
Validation loss: 4.0584658424212385

Epoch: 6| Step: 12
Training loss: 5.055538335824409
Validation loss: 4.056365619533964

Epoch: 6| Step: 13
Training loss: 3.7526319804099506
Validation loss: 4.049681074826815

Epoch: 21| Step: 0
Training loss: 4.417548877249208
Validation loss: 4.044796423361149

Epoch: 6| Step: 1
Training loss: 3.6660376066952196
Validation loss: 4.04138394314037

Epoch: 6| Step: 2
Training loss: 4.005718911329405
Validation loss: 4.037527809795271

Epoch: 6| Step: 3
Training loss: 4.232965326208196
Validation loss: 4.0331674397193

Epoch: 6| Step: 4
Training loss: 3.946853548234768
Validation loss: 4.028188346165435

Epoch: 6| Step: 5
Training loss: 4.37230621239984
Validation loss: 4.023081505096709

Epoch: 6| Step: 6
Training loss: 3.477274275364024
Validation loss: 4.018651432867441

Epoch: 6| Step: 7
Training loss: 4.2422529183923485
Validation loss: 4.013521744518337

Epoch: 6| Step: 8
Training loss: 3.7200363883422995
Validation loss: 4.0082816579954805

Epoch: 6| Step: 9
Training loss: 4.262835026884182
Validation loss: 4.003437631367003

Epoch: 6| Step: 10
Training loss: 4.159524875064374
Validation loss: 3.9991208739592428

Epoch: 6| Step: 11
Training loss: 4.306819547020249
Validation loss: 3.9944438293826767

Epoch: 6| Step: 12
Training loss: 4.314704663191492
Validation loss: 3.9900031935408964

Epoch: 6| Step: 13
Training loss: 4.754920418732217
Validation loss: 3.985249801043228

Epoch: 22| Step: 0
Training loss: 4.515413800748741
Validation loss: 3.9799303783225093

Epoch: 6| Step: 1
Training loss: 3.6592564408066512
Validation loss: 3.975287452236804

Epoch: 6| Step: 2
Training loss: 3.999114892307422
Validation loss: 3.9706927373011873

Epoch: 6| Step: 3
Training loss: 3.9898330703272724
Validation loss: 3.966206894736618

Epoch: 6| Step: 4
Training loss: 3.725189787875039
Validation loss: 3.9605286918313283

Epoch: 6| Step: 5
Training loss: 4.031161048558351
Validation loss: 3.958026305220647

Epoch: 6| Step: 6
Training loss: 4.276248878851295
Validation loss: 3.953366379034183

Epoch: 6| Step: 7
Training loss: 4.027692304266993
Validation loss: 3.947303637145852

Epoch: 6| Step: 8
Training loss: 3.6262667843135348
Validation loss: 3.94251043443704

Epoch: 6| Step: 9
Training loss: 4.820217731354169
Validation loss: 3.937497820172388

Epoch: 6| Step: 10
Training loss: 4.519820115890109
Validation loss: 3.93372497036671

Epoch: 6| Step: 11
Training loss: 3.8901803248956313
Validation loss: 3.928028482016628

Epoch: 6| Step: 12
Training loss: 3.7530223111374528
Validation loss: 3.9245949036007497

Epoch: 6| Step: 13
Training loss: 4.130838135288185
Validation loss: 3.919264148524805

Epoch: 23| Step: 0
Training loss: 4.079621139214567
Validation loss: 3.915076413178238

Epoch: 6| Step: 1
Training loss: 4.055321792647156
Validation loss: 3.910347988776816

Epoch: 6| Step: 2
Training loss: 4.591786075061735
Validation loss: 3.9051023297775873

Epoch: 6| Step: 3
Training loss: 3.720010752098378
Validation loss: 3.900704946955241

Epoch: 6| Step: 4
Training loss: 5.070713679778226
Validation loss: 3.8955801543248385

Epoch: 6| Step: 5
Training loss: 3.5244631744032464
Validation loss: 3.8903305371240418

Epoch: 6| Step: 6
Training loss: 3.954057545792607
Validation loss: 3.8858079619495967

Epoch: 6| Step: 7
Training loss: 3.5566412953476165
Validation loss: 3.8806982290123284

Epoch: 6| Step: 8
Training loss: 4.225698216998296
Validation loss: 3.8764094588895985

Epoch: 6| Step: 9
Training loss: 4.578275814762055
Validation loss: 3.8721363602369276

Epoch: 6| Step: 10
Training loss: 3.5930875748181395
Validation loss: 3.867029956215923

Epoch: 6| Step: 11
Training loss: 2.577343446581806
Validation loss: 3.8621816172646173

Epoch: 6| Step: 12
Training loss: 3.766463190394884
Validation loss: 3.8581819324710205

Epoch: 6| Step: 13
Training loss: 4.3894193358248295
Validation loss: 3.85341894096484

Epoch: 24| Step: 0
Training loss: 4.427425930754173
Validation loss: 3.8495890608609122

Epoch: 6| Step: 1
Training loss: 5.022527870416096
Validation loss: 3.844266841224645

Epoch: 6| Step: 2
Training loss: 3.9243472388692497
Validation loss: 3.839956505052285

Epoch: 6| Step: 3
Training loss: 3.7688637888670375
Validation loss: 3.834244447043702

Epoch: 6| Step: 4
Training loss: 3.7440043520616846
Validation loss: 3.829792681893817

Epoch: 6| Step: 5
Training loss: 4.118727104806434
Validation loss: 3.826155445218734

Epoch: 6| Step: 6
Training loss: 4.049342047852689
Validation loss: 3.8209317673400736

Epoch: 6| Step: 7
Training loss: 4.043520210729764
Validation loss: 3.8156194380152866

Epoch: 6| Step: 8
Training loss: 4.237343511251906
Validation loss: 3.810904580746077

Epoch: 6| Step: 9
Training loss: 3.664702597465264
Validation loss: 3.8069582200399363

Epoch: 6| Step: 10
Training loss: 3.121206193218567
Validation loss: 3.80255060677645

Epoch: 6| Step: 11
Training loss: 3.3164595924284153
Validation loss: 3.7976199702586197

Epoch: 6| Step: 12
Training loss: 3.7958455515112486
Validation loss: 3.7930302037120094

Epoch: 6| Step: 13
Training loss: 3.803435509323641
Validation loss: 3.7891847256482576

Epoch: 25| Step: 0
Training loss: 4.084015665978883
Validation loss: 3.784218665793879

Epoch: 6| Step: 1
Training loss: 3.2576280720053936
Validation loss: 3.77913108284603

Epoch: 6| Step: 2
Training loss: 4.478043715597116
Validation loss: 3.7746923241272734

Epoch: 6| Step: 3
Training loss: 3.727902217581311
Validation loss: 3.7699437576200103

Epoch: 6| Step: 4
Training loss: 3.462371184779501
Validation loss: 3.765426947106283

Epoch: 6| Step: 5
Training loss: 4.574639461464978
Validation loss: 3.7614840668345857

Epoch: 6| Step: 6
Training loss: 3.560488618257981
Validation loss: 3.7565494352415563

Epoch: 6| Step: 7
Training loss: 3.735755697092214
Validation loss: 3.75194446166225

Epoch: 6| Step: 8
Training loss: 3.866443292698955
Validation loss: 3.7471718189898766

Epoch: 6| Step: 9
Training loss: 4.209613768921811
Validation loss: 3.7428643895962725

Epoch: 6| Step: 10
Training loss: 4.158893174968059
Validation loss: 3.738468581355329

Epoch: 6| Step: 11
Training loss: 3.786012031177361
Validation loss: 3.732898329754947

Epoch: 6| Step: 12
Training loss: 3.9954073289367
Validation loss: 3.728486913747332

Epoch: 6| Step: 13
Training loss: 3.286525445525921
Validation loss: 3.724184974585006

Epoch: 26| Step: 0
Training loss: 4.625438566292065
Validation loss: 3.7191782258680584

Epoch: 6| Step: 1
Training loss: 4.11586651973214
Validation loss: 3.7146308375280825

Epoch: 6| Step: 2
Training loss: 3.8286223419358576
Validation loss: 3.7092335640548137

Epoch: 6| Step: 3
Training loss: 4.374335974301066
Validation loss: 3.7051424467663274

Epoch: 6| Step: 4
Training loss: 3.6928633140392235
Validation loss: 3.6996516854204873

Epoch: 6| Step: 5
Training loss: 4.041523462229553
Validation loss: 3.6945644640409325

Epoch: 6| Step: 6
Training loss: 3.7550900564556753
Validation loss: 3.6899285051437385

Epoch: 6| Step: 7
Training loss: 3.6401031513060778
Validation loss: 3.684635256482989

Epoch: 6| Step: 8
Training loss: 3.1902911363682844
Validation loss: 3.6796339588630085

Epoch: 6| Step: 9
Training loss: 3.9342695032690664
Validation loss: 3.6751226253335556

Epoch: 6| Step: 10
Training loss: 3.7896793453279494
Validation loss: 3.6713813612247286

Epoch: 6| Step: 11
Training loss: 3.3560587283282577
Validation loss: 3.666162167145268

Epoch: 6| Step: 12
Training loss: 3.4403147877226363
Validation loss: 3.6612987166984263

Epoch: 6| Step: 13
Training loss: 3.5385089635063034
Validation loss: 3.656890682590403

Epoch: 27| Step: 0
Training loss: 4.493746757240876
Validation loss: 3.652374452404098

Epoch: 6| Step: 1
Training loss: 3.569921426202237
Validation loss: 3.647884416827975

Epoch: 6| Step: 2
Training loss: 3.0800138767970195
Validation loss: 3.643555559975997

Epoch: 6| Step: 3
Training loss: 3.2208075011987027
Validation loss: 3.6397326771593597

Epoch: 6| Step: 4
Training loss: 3.962566213927013
Validation loss: 3.6350229846239914

Epoch: 6| Step: 5
Training loss: 3.6756315454205186
Validation loss: 3.630693294727528

Epoch: 6| Step: 6
Training loss: 3.835061029498173
Validation loss: 3.6267822532139116

Epoch: 6| Step: 7
Training loss: 4.15448029206932
Validation loss: 3.6222673178975002

Epoch: 6| Step: 8
Training loss: 3.9848622940374496
Validation loss: 3.618015290709673

Epoch: 6| Step: 9
Training loss: 3.797546829628013
Validation loss: 3.6126754335354674

Epoch: 6| Step: 10
Training loss: 3.6867071042823722
Validation loss: 3.6076949972603907

Epoch: 6| Step: 11
Training loss: 3.9222854460272507
Validation loss: 3.603224011814589

Epoch: 6| Step: 12
Training loss: 3.7101309773236424
Validation loss: 3.598740835651841

Epoch: 6| Step: 13
Training loss: 3.3181323265099705
Validation loss: 3.594191836410426

Epoch: 28| Step: 0
Training loss: 3.456960351938959
Validation loss: 3.59021117080762

Epoch: 6| Step: 1
Training loss: 2.6747946152318116
Validation loss: 3.5853213554509478

Epoch: 6| Step: 2
Training loss: 4.1395835862386035
Validation loss: 3.5819726105094922

Epoch: 6| Step: 3
Training loss: 3.6412684318162407
Validation loss: 3.5773131603125257

Epoch: 6| Step: 4
Training loss: 3.4683337262538507
Validation loss: 3.5729151837908426

Epoch: 6| Step: 5
Training loss: 3.5558170745623325
Validation loss: 3.5689018684848985

Epoch: 6| Step: 6
Training loss: 3.7814742092779863
Validation loss: 3.564546298546793

Epoch: 6| Step: 7
Training loss: 3.424392401269325
Validation loss: 3.560671822452818

Epoch: 6| Step: 8
Training loss: 4.3899559515826505
Validation loss: 3.5566635731615404

Epoch: 6| Step: 9
Training loss: 4.000544034200859
Validation loss: 3.552236574210469

Epoch: 6| Step: 10
Training loss: 3.770426494647268
Validation loss: 3.5481686914560546

Epoch: 6| Step: 11
Training loss: 4.1279445888881146
Validation loss: 3.543436161880606

Epoch: 6| Step: 12
Training loss: 2.897438720371091
Validation loss: 3.5384854933343557

Epoch: 6| Step: 13
Training loss: 4.045130762502269
Validation loss: 3.5341397729696062

Epoch: 29| Step: 0
Training loss: 4.210472293557393
Validation loss: 3.5299129599267767

Epoch: 6| Step: 1
Training loss: 3.7493907433541915
Validation loss: 3.525189851378349

Epoch: 6| Step: 2
Training loss: 3.820306758447245
Validation loss: 3.520343468049147

Epoch: 6| Step: 3
Training loss: 4.283353191483079
Validation loss: 3.5158425496925365

Epoch: 6| Step: 4
Training loss: 3.602410007811421
Validation loss: 3.51127022498048

Epoch: 6| Step: 5
Training loss: 3.1991385611706997
Validation loss: 3.5067670435252074

Epoch: 6| Step: 6
Training loss: 4.584084859558534
Validation loss: 3.502590174688402

Epoch: 6| Step: 7
Training loss: 3.1793158686579406
Validation loss: 3.497385433557443

Epoch: 6| Step: 8
Training loss: 2.743904293648938
Validation loss: 3.4925887569289413

Epoch: 6| Step: 9
Training loss: 2.968747108859612
Validation loss: 3.4886830017734773

Epoch: 6| Step: 10
Training loss: 2.994203371509032
Validation loss: 3.4843734946482874

Epoch: 6| Step: 11
Training loss: 3.866577470232943
Validation loss: 3.4800081358555075

Epoch: 6| Step: 12
Training loss: 3.3328591645269516
Validation loss: 3.4761115878286737

Epoch: 6| Step: 13
Training loss: 3.8990605225328347
Validation loss: 3.4722425502605905

Epoch: 30| Step: 0
Training loss: 3.551178996991951
Validation loss: 3.468487990281326

Epoch: 6| Step: 1
Training loss: 3.9038496042737205
Validation loss: 3.464483644508253

Epoch: 6| Step: 2
Training loss: 3.3851823148553715
Validation loss: 3.4599046163329286

Epoch: 6| Step: 3
Training loss: 3.6385368940509752
Validation loss: 3.4566059317873714

Epoch: 6| Step: 4
Training loss: 3.814276609619116
Validation loss: 3.452568458138069

Epoch: 6| Step: 5
Training loss: 3.3235916998654558
Validation loss: 3.448449492221221

Epoch: 6| Step: 6
Training loss: 2.683324542139789
Validation loss: 3.44419094062064

Epoch: 6| Step: 7
Training loss: 4.48543777897897
Validation loss: 3.4393435303157993

Epoch: 6| Step: 8
Training loss: 3.538942313942969
Validation loss: 3.4351944098320653

Epoch: 6| Step: 9
Training loss: 3.349838987438091
Validation loss: 3.431761766820153

Epoch: 6| Step: 10
Training loss: 2.7140247893582283
Validation loss: 3.427267624248958

Epoch: 6| Step: 11
Training loss: 4.013389826236528
Validation loss: 3.422863194858087

Epoch: 6| Step: 12
Training loss: 3.5789677989158086
Validation loss: 3.4179846539808563

Epoch: 6| Step: 13
Training loss: 3.744973947105901
Validation loss: 3.414639119469724

Epoch: 31| Step: 0
Training loss: 3.294922743312719
Validation loss: 3.4107017008041147

Epoch: 6| Step: 1
Training loss: 3.9354706636153756
Validation loss: 3.4064928720663668

Epoch: 6| Step: 2
Training loss: 3.5395367369897435
Validation loss: 3.4023214544594245

Epoch: 6| Step: 3
Training loss: 3.1999165166455477
Validation loss: 3.398244824556363

Epoch: 6| Step: 4
Training loss: 3.2903522490720705
Validation loss: 3.3942465946318756

Epoch: 6| Step: 5
Training loss: 3.6348952363041014
Validation loss: 3.390301338838704

Epoch: 6| Step: 6
Training loss: 3.467006485740684
Validation loss: 3.385917031895658

Epoch: 6| Step: 7
Training loss: 3.53096128650705
Validation loss: 3.3823885398708495

Epoch: 6| Step: 8
Training loss: 3.1769707998555425
Validation loss: 3.378420685735195

Epoch: 6| Step: 9
Training loss: 3.633024344881676
Validation loss: 3.374423743136931

Epoch: 6| Step: 10
Training loss: 4.208327013269091
Validation loss: 3.3706463295485527

Epoch: 6| Step: 11
Training loss: 3.33820375030711
Validation loss: 3.366384069618001

Epoch: 6| Step: 12
Training loss: 3.7067903173071355
Validation loss: 3.3628430585037283

Epoch: 6| Step: 13
Training loss: 3.221191073179553
Validation loss: 3.358768911172402

Epoch: 32| Step: 0
Training loss: 3.8771080927781645
Validation loss: 3.354677520770277

Epoch: 6| Step: 1
Training loss: 3.772996075007108
Validation loss: 3.350847477731654

Epoch: 6| Step: 2
Training loss: 4.20514344033111
Validation loss: 3.3461886987211797

Epoch: 6| Step: 3
Training loss: 3.7403802190886437
Validation loss: 3.341893592860277

Epoch: 6| Step: 4
Training loss: 3.3866967857202988
Validation loss: 3.3374532908244054

Epoch: 6| Step: 5
Training loss: 2.801787510871051
Validation loss: 3.3330710228388725

Epoch: 6| Step: 6
Training loss: 3.3861018656297666
Validation loss: 3.3293764391247267

Epoch: 6| Step: 7
Training loss: 3.4090721268569957
Validation loss: 3.325455264901619

Epoch: 6| Step: 8
Training loss: 3.741135738117898
Validation loss: 3.321157549862442

Epoch: 6| Step: 9
Training loss: 3.2754500218228007
Validation loss: 3.317645568892744

Epoch: 6| Step: 10
Training loss: 3.2807962739682845
Validation loss: 3.3134163602664124

Epoch: 6| Step: 11
Training loss: 3.312072726349605
Validation loss: 3.310034428097637

Epoch: 6| Step: 12
Training loss: 3.0062943231245636
Validation loss: 3.3066396275755596

Epoch: 6| Step: 13
Training loss: 3.1327242184802566
Validation loss: 3.302830287714489

Epoch: 33| Step: 0
Training loss: 2.840241835868605
Validation loss: 3.2993332555352963

Epoch: 6| Step: 1
Training loss: 2.9251656183810337
Validation loss: 3.2958947964871865

Epoch: 6| Step: 2
Training loss: 2.782631263207319
Validation loss: 3.29271733541122

Epoch: 6| Step: 3
Training loss: 3.688252259156476
Validation loss: 3.2897046837258705

Epoch: 6| Step: 4
Training loss: 3.4629856361225655
Validation loss: 3.2866122073239135

Epoch: 6| Step: 5
Training loss: 3.66797187000666
Validation loss: 3.2829392217610205

Epoch: 6| Step: 6
Training loss: 3.057265654696404
Validation loss: 3.27934511386637

Epoch: 6| Step: 7
Training loss: 3.0723953753195823
Validation loss: 3.2758786757798832

Epoch: 6| Step: 8
Training loss: 3.117607222290914
Validation loss: 3.2730469391018464

Epoch: 6| Step: 9
Training loss: 2.853486882769346
Validation loss: 3.2691132138838546

Epoch: 6| Step: 10
Training loss: 4.183308795588493
Validation loss: 3.265427542379503

Epoch: 6| Step: 11
Training loss: 3.809764193148634
Validation loss: 3.26215081617779

Epoch: 6| Step: 12
Training loss: 3.909683061715669
Validation loss: 3.2585196211718115

Epoch: 6| Step: 13
Training loss: 3.975034049026243
Validation loss: 3.254157292885331

Epoch: 34| Step: 0
Training loss: 3.346533926602181
Validation loss: 3.2508299697028034

Epoch: 6| Step: 1
Training loss: 2.751033588721594
Validation loss: 3.2465343337012658

Epoch: 6| Step: 2
Training loss: 3.9352323269150036
Validation loss: 3.2430285485119024

Epoch: 6| Step: 3
Training loss: 3.0812021743126916
Validation loss: 3.238267408463331

Epoch: 6| Step: 4
Training loss: 2.4156139536379264
Validation loss: 3.2348144281884994

Epoch: 6| Step: 5
Training loss: 3.2445411320073596
Validation loss: 3.230738464437206

Epoch: 6| Step: 6
Training loss: 3.466350791062794
Validation loss: 3.22703880015003

Epoch: 6| Step: 7
Training loss: 3.687409351333833
Validation loss: 3.2238976262816514

Epoch: 6| Step: 8
Training loss: 3.5544293991119704
Validation loss: 3.2201542306574367

Epoch: 6| Step: 9
Training loss: 3.717584764018318
Validation loss: 3.217603522499928

Epoch: 6| Step: 10
Training loss: 3.4365077580720027
Validation loss: 3.2133529376847285

Epoch: 6| Step: 11
Training loss: 3.5541288838198786
Validation loss: 3.2095794033195015

Epoch: 6| Step: 12
Training loss: 3.121331617627276
Validation loss: 3.206539114743604

Epoch: 6| Step: 13
Training loss: 3.5002449494886636
Validation loss: 3.2023951249392284

Epoch: 35| Step: 0
Training loss: 3.719169016079535
Validation loss: 3.198992226281459

Epoch: 6| Step: 1
Training loss: 3.4803798334514027
Validation loss: 3.1949278981645284

Epoch: 6| Step: 2
Training loss: 2.868544587404344
Validation loss: 3.1913172235915694

Epoch: 6| Step: 3
Training loss: 2.8946060397515603
Validation loss: 3.188039447289886

Epoch: 6| Step: 4
Training loss: 3.548615284963112
Validation loss: 3.18460314142127

Epoch: 6| Step: 5
Training loss: 2.995517560917257
Validation loss: 3.1808873567564904

Epoch: 6| Step: 6
Training loss: 3.7347810277111524
Validation loss: 3.177554403099942

Epoch: 6| Step: 7
Training loss: 3.0028298065198147
Validation loss: 3.173900051769116

Epoch: 6| Step: 8
Training loss: 3.428651692381911
Validation loss: 3.171073223435095

Epoch: 6| Step: 9
Training loss: 3.3261423428848977
Validation loss: 3.1677523726053347

Epoch: 6| Step: 10
Training loss: 3.498923544877823
Validation loss: 3.16384090699417

Epoch: 6| Step: 11
Training loss: 3.2783837288139486
Validation loss: 3.160477693019254

Epoch: 6| Step: 12
Training loss: 3.4117513205631296
Validation loss: 3.157031739872164

Epoch: 6| Step: 13
Training loss: 3.064146572362188
Validation loss: 3.1534011372083977

Epoch: 36| Step: 0
Training loss: 2.9071283141254773
Validation loss: 3.1501994665257516

Epoch: 6| Step: 1
Training loss: 2.740011106872323
Validation loss: 3.1458961446861924

Epoch: 6| Step: 2
Training loss: 3.027138349886815
Validation loss: 3.143420997629657

Epoch: 6| Step: 3
Training loss: 3.5189616327690176
Validation loss: 3.1399016551594543

Epoch: 6| Step: 4
Training loss: 3.0278439606910013
Validation loss: 3.136563358598783

Epoch: 6| Step: 5
Training loss: 3.348088667193369
Validation loss: 3.134144069573603

Epoch: 6| Step: 6
Training loss: 3.248818696349941
Validation loss: 3.1310187172514525

Epoch: 6| Step: 7
Training loss: 3.279998831399849
Validation loss: 3.1297863100819403

Epoch: 6| Step: 8
Training loss: 4.1099630304980295
Validation loss: 3.1328106227751804

Epoch: 6| Step: 9
Training loss: 3.317869764360017
Validation loss: 3.1207734072083797

Epoch: 6| Step: 10
Training loss: 3.501042347054537
Validation loss: 3.116419048128445

Epoch: 6| Step: 11
Training loss: 3.0513151241417864
Validation loss: 3.112443786583287

Epoch: 6| Step: 12
Training loss: 3.477422097908375
Validation loss: 3.111831806263871

Epoch: 6| Step: 13
Training loss: 2.9975146489107556
Validation loss: 3.1125132380437646

Epoch: 37| Step: 0
Training loss: 3.2257358332564356
Validation loss: 3.1076692921851996

Epoch: 6| Step: 1
Training loss: 3.1628117102395947
Validation loss: 3.1017631382589514

Epoch: 6| Step: 2
Training loss: 3.7246392043328993
Validation loss: 3.097623846550846

Epoch: 6| Step: 3
Training loss: 3.8120438349746277
Validation loss: 3.0945575828879655

Epoch: 6| Step: 4
Training loss: 3.1665129038639512
Validation loss: 3.091233087008277

Epoch: 6| Step: 5
Training loss: 3.141278858348262
Validation loss: 3.0882593618785434

Epoch: 6| Step: 6
Training loss: 3.461474925868242
Validation loss: 3.085761111083305

Epoch: 6| Step: 7
Training loss: 2.8870360493009746
Validation loss: 3.0839041533776053

Epoch: 6| Step: 8
Training loss: 3.270141982049659
Validation loss: 3.0786510992588645

Epoch: 6| Step: 9
Training loss: 3.10933804130634
Validation loss: 3.075840936853076

Epoch: 6| Step: 10
Training loss: 3.149895342344664
Validation loss: 3.0719392719963614

Epoch: 6| Step: 11
Training loss: 3.0092801563374207
Validation loss: 3.068524047654563

Epoch: 6| Step: 12
Training loss: 3.1797282898182218
Validation loss: 3.0657953192332945

Epoch: 6| Step: 13
Training loss: 2.680765719127807
Validation loss: 3.0632858176449878

Epoch: 38| Step: 0
Training loss: 3.7760357174881296
Validation loss: 3.0589716951038186

Epoch: 6| Step: 1
Training loss: 2.5819597797053526
Validation loss: 3.055940076927885

Epoch: 6| Step: 2
Training loss: 3.0014711587522025
Validation loss: 3.051986033133116

Epoch: 6| Step: 3
Training loss: 2.9192398346415804
Validation loss: 3.049373977805866

Epoch: 6| Step: 4
Training loss: 3.412921077474089
Validation loss: 3.046550374792292

Epoch: 6| Step: 5
Training loss: 3.7764373922715286
Validation loss: 3.043483248281045

Epoch: 6| Step: 6
Training loss: 2.8898905052905306
Validation loss: 3.040502316651823

Epoch: 6| Step: 7
Training loss: 2.9063763334646864
Validation loss: 3.036595079190237

Epoch: 6| Step: 8
Training loss: 3.9851643098391833
Validation loss: 3.0335961795639643

Epoch: 6| Step: 9
Training loss: 2.7272702585555955
Validation loss: 3.0304141907821927

Epoch: 6| Step: 10
Training loss: 2.8409602975565287
Validation loss: 3.027205610507581

Epoch: 6| Step: 11
Training loss: 3.819515714654756
Validation loss: 3.024608244106024

Epoch: 6| Step: 12
Training loss: 2.817650973616987
Validation loss: 3.021357041486742

Epoch: 6| Step: 13
Training loss: 2.572963750205079
Validation loss: 3.0185856492063032

Epoch: 39| Step: 0
Training loss: 3.624560888594898
Validation loss: 3.0161790091819296

Epoch: 6| Step: 1
Training loss: 3.600304505927296
Validation loss: 3.013624750058548

Epoch: 6| Step: 2
Training loss: 2.741628301829379
Validation loss: 3.0099911944576325

Epoch: 6| Step: 3
Training loss: 2.437489925265888
Validation loss: 3.0064855645849997

Epoch: 6| Step: 4
Training loss: 3.638637802611542
Validation loss: 3.0035076827722627

Epoch: 6| Step: 5
Training loss: 3.034847052300886
Validation loss: 3.0074542207898842

Epoch: 6| Step: 6
Training loss: 3.0439122589539207
Validation loss: 2.998552960188334

Epoch: 6| Step: 7
Training loss: 2.7442766800680167
Validation loss: 2.9951083781524614

Epoch: 6| Step: 8
Training loss: 3.3927324487244688
Validation loss: 2.992747242018812

Epoch: 6| Step: 9
Training loss: 3.36234312010601
Validation loss: 2.9904428812590957

Epoch: 6| Step: 10
Training loss: 2.6883689894876626
Validation loss: 2.9872398938919975

Epoch: 6| Step: 11
Training loss: 3.0574763608877986
Validation loss: 2.985263046567954

Epoch: 6| Step: 12
Training loss: 3.140120432542609
Validation loss: 2.9821263707958585

Epoch: 6| Step: 13
Training loss: 3.162215534428613
Validation loss: 2.979825798336507

Epoch: 40| Step: 0
Training loss: 3.096187363546584
Validation loss: 2.977334283743452

Epoch: 6| Step: 1
Training loss: 3.466927951967462
Validation loss: 2.9745046956609897

Epoch: 6| Step: 2
Training loss: 2.9870858388187775
Validation loss: 2.971728293257159

Epoch: 6| Step: 3
Training loss: 3.0501319106296387
Validation loss: 2.968272147286096

Epoch: 6| Step: 4
Training loss: 3.187705089478604
Validation loss: 2.9673425650994734

Epoch: 6| Step: 5
Training loss: 3.206990807449017
Validation loss: 2.962641699469723

Epoch: 6| Step: 6
Training loss: 2.9634466083390256
Validation loss: 2.960511424359948

Epoch: 6| Step: 7
Training loss: 3.2709945995317717
Validation loss: 2.957412406245505

Epoch: 6| Step: 8
Training loss: 2.8674096793173747
Validation loss: 2.9549252440479066

Epoch: 6| Step: 9
Training loss: 3.423203580552324
Validation loss: 2.9504963042453656

Epoch: 6| Step: 10
Training loss: 3.3650991951774305
Validation loss: 2.9473667957425715

Epoch: 6| Step: 11
Training loss: 3.1163178954243094
Validation loss: 2.944965669294769

Epoch: 6| Step: 12
Training loss: 2.9635513564735225
Validation loss: 2.943661970157337

Epoch: 6| Step: 13
Training loss: 2.2887709500298277
Validation loss: 2.9412193289607367

Epoch: 41| Step: 0
Training loss: 3.6705202890741364
Validation loss: 2.939408811655529

Epoch: 6| Step: 1
Training loss: 2.5339346883015033
Validation loss: 2.9365733152686095

Epoch: 6| Step: 2
Training loss: 3.3436818338736654
Validation loss: 2.933881018364659

Epoch: 6| Step: 3
Training loss: 2.6632553175415934
Validation loss: 2.931095595315924

Epoch: 6| Step: 4
Training loss: 2.554220361025932
Validation loss: 2.928343428122313

Epoch: 6| Step: 5
Training loss: 2.8533012210369924
Validation loss: 2.9259075142642836

Epoch: 6| Step: 6
Training loss: 2.6853607002228794
Validation loss: 2.9225951682509894

Epoch: 6| Step: 7
Training loss: 2.708672360450468
Validation loss: 2.9207979552937573

Epoch: 6| Step: 8
Training loss: 3.8144626255962075
Validation loss: 2.9178109558282235

Epoch: 6| Step: 9
Training loss: 3.181707167236934
Validation loss: 2.9157089477534344

Epoch: 6| Step: 10
Training loss: 3.1185706566749176
Validation loss: 2.9131866993508777

Epoch: 6| Step: 11
Training loss: 3.3194696770369174
Validation loss: 2.910662251630619

Epoch: 6| Step: 12
Training loss: 3.4913480633035285
Validation loss: 2.90756198706551

Epoch: 6| Step: 13
Training loss: 2.53408504007984
Validation loss: 2.906094468306374

Epoch: 42| Step: 0
Training loss: 3.4147769695412005
Validation loss: 2.903342169833249

Epoch: 6| Step: 1
Training loss: 3.6814520425839725
Validation loss: 2.9012560716680817

Epoch: 6| Step: 2
Training loss: 2.601776847371674
Validation loss: 2.897630344363176

Epoch: 6| Step: 3
Training loss: 3.285090932298782
Validation loss: 2.895982550932021

Epoch: 6| Step: 4
Training loss: 3.1006177717361827
Validation loss: 2.8937110442360643

Epoch: 6| Step: 5
Training loss: 3.2453829908727703
Validation loss: 2.8921953489367795

Epoch: 6| Step: 6
Training loss: 3.1630860882506138
Validation loss: 2.888994592226486

Epoch: 6| Step: 7
Training loss: 3.1800916260288736
Validation loss: 2.8845347262786314

Epoch: 6| Step: 8
Training loss: 2.955516674228976
Validation loss: 2.8840657915361176

Epoch: 6| Step: 9
Training loss: 2.981649062600648
Validation loss: 2.8801137806351154

Epoch: 6| Step: 10
Training loss: 2.8781753916683184
Validation loss: 2.877337486832702

Epoch: 6| Step: 11
Training loss: 2.200456233835139
Validation loss: 2.8755106679751328

Epoch: 6| Step: 12
Training loss: 2.753999402790656
Validation loss: 2.8726226818511984

Epoch: 6| Step: 13
Training loss: 2.681880131727477
Validation loss: 2.8712503195909016

Epoch: 43| Step: 0
Training loss: 3.3892405975384707
Validation loss: 2.868384420666606

Epoch: 6| Step: 1
Training loss: 3.2411551796857414
Validation loss: 2.8667547330709966

Epoch: 6| Step: 2
Training loss: 3.0063269972110747
Validation loss: 2.8648018817184546

Epoch: 6| Step: 3
Training loss: 3.1305291661486594
Validation loss: 2.8618292359964506

Epoch: 6| Step: 4
Training loss: 2.801146415623477
Validation loss: 2.8602368944044168

Epoch: 6| Step: 5
Training loss: 2.864006252537727
Validation loss: 2.8573520501506215

Epoch: 6| Step: 6
Training loss: 2.7651386587684126
Validation loss: 2.8563445105522254

Epoch: 6| Step: 7
Training loss: 3.0897989957054333
Validation loss: 2.8541154137532

Epoch: 6| Step: 8
Training loss: 3.39220704468834
Validation loss: 2.8520062663048256

Epoch: 6| Step: 9
Training loss: 3.3331707596870013
Validation loss: 2.8510143293120764

Epoch: 6| Step: 10
Training loss: 2.6924503775923374
Validation loss: 2.8484602265489762

Epoch: 6| Step: 11
Training loss: 2.887073541500667
Validation loss: 2.8471234472452287

Epoch: 6| Step: 12
Training loss: 2.614536046870833
Validation loss: 2.8437992971989114

Epoch: 6| Step: 13
Training loss: 2.569677767312436
Validation loss: 2.8417934631816575

Epoch: 44| Step: 0
Training loss: 2.620405627002128
Validation loss: 2.8376548618399866

Epoch: 6| Step: 1
Training loss: 2.8168795078225437
Validation loss: 2.8361695389972286

Epoch: 6| Step: 2
Training loss: 2.5161528418592125
Validation loss: 2.8348330754604745

Epoch: 6| Step: 3
Training loss: 2.9762039569367778
Validation loss: 2.833299402893949

Epoch: 6| Step: 4
Training loss: 2.6335287053832612
Validation loss: 2.832764755494735

Epoch: 6| Step: 5
Training loss: 2.9262294044079455
Validation loss: 2.8292818039475103

Epoch: 6| Step: 6
Training loss: 3.5220749751726332
Validation loss: 2.828188788325143

Epoch: 6| Step: 7
Training loss: 3.07727860175748
Validation loss: 2.8259160106792

Epoch: 6| Step: 8
Training loss: 3.456067656799627
Validation loss: 2.8232011593037005

Epoch: 6| Step: 9
Training loss: 2.493283404532052
Validation loss: 2.822080105818619

Epoch: 6| Step: 10
Training loss: 2.6755992966762627
Validation loss: 2.819230214537009

Epoch: 6| Step: 11
Training loss: 3.09795748579629
Validation loss: 2.8173834753889673

Epoch: 6| Step: 12
Training loss: 3.2385641050415197
Validation loss: 2.815895672714243

Epoch: 6| Step: 13
Training loss: 3.238944249218924
Validation loss: 2.81368158739419

Epoch: 45| Step: 0
Training loss: 2.686638627517323
Validation loss: 2.8116347500795364

Epoch: 6| Step: 1
Training loss: 2.438698693952423
Validation loss: 2.8095125226232063

Epoch: 6| Step: 2
Training loss: 2.945243207283369
Validation loss: 2.8062733564983686

Epoch: 6| Step: 3
Training loss: 2.956401319441709
Validation loss: 2.8044456441409524

Epoch: 6| Step: 4
Training loss: 3.312333444690603
Validation loss: 2.801725206332758

Epoch: 6| Step: 5
Training loss: 2.960200799368686
Validation loss: 2.7997347456311585

Epoch: 6| Step: 6
Training loss: 3.111373489154536
Validation loss: 2.7988437990227544

Epoch: 6| Step: 7
Training loss: 3.386658629485478
Validation loss: 2.7977756634193263

Epoch: 6| Step: 8
Training loss: 2.909501748443251
Validation loss: 2.7954474132115106

Epoch: 6| Step: 9
Training loss: 3.4295862830309294
Validation loss: 2.7916565273347502

Epoch: 6| Step: 10
Training loss: 2.8052322581210944
Validation loss: 2.789300375944597

Epoch: 6| Step: 11
Training loss: 2.5384686993451253
Validation loss: 2.786725943480956

Epoch: 6| Step: 12
Training loss: 2.734368547704441
Validation loss: 2.784436061887444

Epoch: 6| Step: 13
Training loss: 2.747998983457952
Validation loss: 2.7833930770568647

Epoch: 46| Step: 0
Training loss: 2.6266831042974355
Validation loss: 2.7821873717550996

Epoch: 6| Step: 1
Training loss: 3.0617581461566554
Validation loss: 2.779731450225712

Epoch: 6| Step: 2
Training loss: 3.132136017399969
Validation loss: 2.777451639632824

Epoch: 6| Step: 3
Training loss: 3.1701168204843144
Validation loss: 2.7764519012011046

Epoch: 6| Step: 4
Training loss: 3.173672622392476
Validation loss: 2.77540714725209

Epoch: 6| Step: 5
Training loss: 3.30888860800578
Validation loss: 2.774318257921974

Epoch: 6| Step: 6
Training loss: 3.1693092830752745
Validation loss: 2.7720505089371503

Epoch: 6| Step: 7
Training loss: 2.7934036449818
Validation loss: 2.771112633493728

Epoch: 6| Step: 8
Training loss: 2.5702347671210264
Validation loss: 2.771433490396272

Epoch: 6| Step: 9
Training loss: 2.6297374845254633
Validation loss: 2.7684445780320455

Epoch: 6| Step: 10
Training loss: 3.067807794501061
Validation loss: 2.766400029267818

Epoch: 6| Step: 11
Training loss: 2.6918693332333476
Validation loss: 2.7637211196519975

Epoch: 6| Step: 12
Training loss: 2.7668561188816403
Validation loss: 2.761600432498562

Epoch: 6| Step: 13
Training loss: 2.4464701389444894
Validation loss: 2.7590441236489975

Epoch: 47| Step: 0
Training loss: 3.11667607827899
Validation loss: 2.7581625798167253

Epoch: 6| Step: 1
Training loss: 3.0906439708285625
Validation loss: 2.763111200511741

Epoch: 6| Step: 2
Training loss: 3.2059569702687596
Validation loss: 2.7578009586254826

Epoch: 6| Step: 3
Training loss: 2.393299576336206
Validation loss: 2.751156433746756

Epoch: 6| Step: 4
Training loss: 2.931648594936285
Validation loss: 2.7489620330619897

Epoch: 6| Step: 5
Training loss: 3.0586643722390323
Validation loss: 2.7489156029952886

Epoch: 6| Step: 6
Training loss: 2.852812025571799
Validation loss: 2.7470674918194504

Epoch: 6| Step: 7
Training loss: 2.840479720526906
Validation loss: 2.7457879639168663

Epoch: 6| Step: 8
Training loss: 2.942262076690261
Validation loss: 2.7431100885520907

Epoch: 6| Step: 9
Training loss: 2.682518534031928
Validation loss: 2.7423580221716506

Epoch: 6| Step: 10
Training loss: 2.625301434602906
Validation loss: 2.742413025192345

Epoch: 6| Step: 11
Training loss: 2.8296592302347876
Validation loss: 2.7393476048767216

Epoch: 6| Step: 12
Training loss: 2.761821566857096
Validation loss: 2.736823564214361

Epoch: 6| Step: 13
Training loss: 3.0197283247460254
Validation loss: 2.737359175115381

Epoch: 48| Step: 0
Training loss: 2.5461538967298343
Validation loss: 2.733844393155944

Epoch: 6| Step: 1
Training loss: 2.7619052160353514
Validation loss: 2.7319886175619468

Epoch: 6| Step: 2
Training loss: 3.1413687209719705
Validation loss: 2.7298525222655714

Epoch: 6| Step: 3
Training loss: 3.1977134582039723
Validation loss: 2.7294537682010267

Epoch: 6| Step: 4
Training loss: 3.190332089540638
Validation loss: 2.7253175824556863

Epoch: 6| Step: 5
Training loss: 2.3857331933544135
Validation loss: 2.7240289468650967

Epoch: 6| Step: 6
Training loss: 2.799677462393234
Validation loss: 2.7239203271104753

Epoch: 6| Step: 7
Training loss: 2.92402969307702
Validation loss: 2.7221245850896447

Epoch: 6| Step: 8
Training loss: 2.9421207527777953
Validation loss: 2.721443172380384

Epoch: 6| Step: 9
Training loss: 2.8386540288588606
Validation loss: 2.722681294439602

Epoch: 6| Step: 10
Training loss: 3.208703519263837
Validation loss: 2.7237832552271652

Epoch: 6| Step: 11
Training loss: 2.67970778566028
Validation loss: 2.7209912704576356

Epoch: 6| Step: 12
Training loss: 3.071462478957468
Validation loss: 2.7162675522437425

Epoch: 6| Step: 13
Training loss: 2.262307098764147
Validation loss: 2.714359289620557

Epoch: 49| Step: 0
Training loss: 2.3460388197487934
Validation loss: 2.7124055740515756

Epoch: 6| Step: 1
Training loss: 2.617933115863219
Validation loss: 2.7103172420843027

Epoch: 6| Step: 2
Training loss: 3.409552836478527
Validation loss: 2.7104530744479947

Epoch: 6| Step: 3
Training loss: 2.6523061758085045
Validation loss: 2.7086325895819945

Epoch: 6| Step: 4
Training loss: 2.958043437956069
Validation loss: 2.7074018979696604

Epoch: 6| Step: 5
Training loss: 2.6777471963588013
Validation loss: 2.7060159794950494

Epoch: 6| Step: 6
Training loss: 2.759542381938506
Validation loss: 2.703777447645099

Epoch: 6| Step: 7
Training loss: 2.8341771907399758
Validation loss: 2.7025573516651047

Epoch: 6| Step: 8
Training loss: 2.8581076559795546
Validation loss: 2.7018057336222547

Epoch: 6| Step: 9
Training loss: 3.2221899816573614
Validation loss: 2.7012107480392515

Epoch: 6| Step: 10
Training loss: 3.0792753132470874
Validation loss: 2.6987856394617364

Epoch: 6| Step: 11
Training loss: 2.426484477501487
Validation loss: 2.6993262733724532

Epoch: 6| Step: 12
Training loss: 2.73527921712389
Validation loss: 2.6951180060789697

Epoch: 6| Step: 13
Training loss: 3.0333656714535637
Validation loss: 2.6949746182720338

Epoch: 50| Step: 0
Training loss: 2.6835066824258185
Validation loss: 2.692890660343692

Epoch: 6| Step: 1
Training loss: 2.789411079571708
Validation loss: 2.692348261848911

Epoch: 6| Step: 2
Training loss: 2.3664894207770932
Validation loss: 2.6915610037772586

Epoch: 6| Step: 3
Training loss: 2.597970493210758
Validation loss: 2.689904327700221

Epoch: 6| Step: 4
Training loss: 3.3690653297745743
Validation loss: 2.689313380263672

Epoch: 6| Step: 5
Training loss: 2.6592567928593605
Validation loss: 2.6879856498813943

Epoch: 6| Step: 6
Training loss: 2.526848911147892
Validation loss: 2.686585899263847

Epoch: 6| Step: 7
Training loss: 2.213914679719146
Validation loss: 2.6828442840678135

Epoch: 6| Step: 8
Training loss: 2.796101297952974
Validation loss: 2.68126491907055

Epoch: 6| Step: 9
Training loss: 3.2492859863016625
Validation loss: 2.6806554204712323

Epoch: 6| Step: 10
Training loss: 3.4461325709634667
Validation loss: 2.677433233488297

Epoch: 6| Step: 11
Training loss: 3.114663538755954
Validation loss: 2.6761025785762813

Epoch: 6| Step: 12
Training loss: 2.2080108359112427
Validation loss: 2.6806297906886316

Epoch: 6| Step: 13
Training loss: 3.1386875230533233
Validation loss: 2.687023963463605

Epoch: 51| Step: 0
Training loss: 2.589248611860869
Validation loss: 2.689480724391641

Epoch: 6| Step: 1
Training loss: 3.13137883513296
Validation loss: 2.6746453688899665

Epoch: 6| Step: 2
Training loss: 2.9256335884592417
Validation loss: 2.6712792161728127

Epoch: 6| Step: 3
Training loss: 2.045063527619798
Validation loss: 2.670016841692439

Epoch: 6| Step: 4
Training loss: 2.892872529434648
Validation loss: 2.669571632866493

Epoch: 6| Step: 5
Training loss: 2.1706607635302184
Validation loss: 2.6724273824505382

Epoch: 6| Step: 6
Training loss: 3.0904373775420977
Validation loss: 2.6760334944807522

Epoch: 6| Step: 7
Training loss: 2.7684916494146083
Validation loss: 2.6799719004320375

Epoch: 6| Step: 8
Training loss: 2.618547046599298
Validation loss: 2.679402965568482

Epoch: 6| Step: 9
Training loss: 3.3057987878749557
Validation loss: 2.6682524783885646

Epoch: 6| Step: 10
Training loss: 3.093495059828068
Validation loss: 2.660342067627533

Epoch: 6| Step: 11
Training loss: 2.9418108536465786
Validation loss: 2.658513544565302

Epoch: 6| Step: 12
Training loss: 2.3937712267539597
Validation loss: 2.6638146636623063

Epoch: 6| Step: 13
Training loss: 3.1189694008263396
Validation loss: 2.6817569730579165

Epoch: 52| Step: 0
Training loss: 2.2417929697675736
Validation loss: 2.6805968303390633

Epoch: 6| Step: 1
Training loss: 2.979042762011493
Validation loss: 2.6738250338580865

Epoch: 6| Step: 2
Training loss: 3.366153850464223
Validation loss: 2.6643097316244306

Epoch: 6| Step: 3
Training loss: 3.0480552539272727
Validation loss: 2.652783514241867

Epoch: 6| Step: 4
Training loss: 2.681163503678855
Validation loss: 2.6526184988442965

Epoch: 6| Step: 5
Training loss: 2.516819261720434
Validation loss: 2.6533282947572316

Epoch: 6| Step: 6
Training loss: 2.762582775964401
Validation loss: 2.653608676898591

Epoch: 6| Step: 7
Training loss: 2.7647473510597043
Validation loss: 2.6581909269099255

Epoch: 6| Step: 8
Training loss: 3.287040469466812
Validation loss: 2.6661346222679736

Epoch: 6| Step: 9
Training loss: 3.2496730933469364
Validation loss: 2.6880043798479094

Epoch: 6| Step: 10
Training loss: 2.5702844867513153
Validation loss: 2.664131842313927

Epoch: 6| Step: 11
Training loss: 2.671013051699874
Validation loss: 2.657754853266502

Epoch: 6| Step: 12
Training loss: 2.2914280044662325
Validation loss: 2.6509934866273714

Epoch: 6| Step: 13
Training loss: 2.5292011964589114
Validation loss: 2.647820294835178

Epoch: 53| Step: 0
Training loss: 3.159639106374609
Validation loss: 2.644825380422696

Epoch: 6| Step: 1
Training loss: 2.8550970041661006
Validation loss: 2.6431841516028474

Epoch: 6| Step: 2
Training loss: 2.2640082543023814
Validation loss: 2.6422216791054898

Epoch: 6| Step: 3
Training loss: 2.5022710022012067
Validation loss: 2.6412735091903965

Epoch: 6| Step: 4
Training loss: 2.957462737879763
Validation loss: 2.6401239856591414

Epoch: 6| Step: 5
Training loss: 2.9623751006375683
Validation loss: 2.63904500187218

Epoch: 6| Step: 6
Training loss: 2.4736025477446826
Validation loss: 2.635931128143888

Epoch: 6| Step: 7
Training loss: 2.5760310136693287
Validation loss: 2.637530452920074

Epoch: 6| Step: 8
Training loss: 2.910441667526295
Validation loss: 2.635941740860587

Epoch: 6| Step: 9
Training loss: 2.8041437413571337
Validation loss: 2.6341043806169164

Epoch: 6| Step: 10
Training loss: 3.099579376479362
Validation loss: 2.636900523007129

Epoch: 6| Step: 11
Training loss: 3.0558723824902123
Validation loss: 2.6318211712537987

Epoch: 6| Step: 12
Training loss: 2.198856845708124
Validation loss: 2.6287247253596027

Epoch: 6| Step: 13
Training loss: 2.8731504378853367
Validation loss: 2.6286591654157823

Epoch: 54| Step: 0
Training loss: 2.5995994699520164
Validation loss: 2.629838843248989

Epoch: 6| Step: 1
Training loss: 2.905117511572033
Validation loss: 2.6308787986380073

Epoch: 6| Step: 2
Training loss: 2.299782087534922
Validation loss: 2.627334374199233

Epoch: 6| Step: 3
Training loss: 3.198801984644078
Validation loss: 2.6285555087396255

Epoch: 6| Step: 4
Training loss: 2.7847490053059523
Validation loss: 2.6264212560337525

Epoch: 6| Step: 5
Training loss: 2.7326954942522668
Validation loss: 2.6248360840012777

Epoch: 6| Step: 6
Training loss: 3.2699788105129386
Validation loss: 2.6230914277103734

Epoch: 6| Step: 7
Training loss: 1.9594177192764441
Validation loss: 2.623307166649666

Epoch: 6| Step: 8
Training loss: 2.6651668105604185
Validation loss: 2.6221875610278818

Epoch: 6| Step: 9
Training loss: 2.9331423314361005
Validation loss: 2.6214592835584956

Epoch: 6| Step: 10
Training loss: 2.954358363063348
Validation loss: 2.620637796220471

Epoch: 6| Step: 11
Training loss: 2.2810670962234196
Validation loss: 2.620626696965058

Epoch: 6| Step: 12
Training loss: 2.943162695091842
Validation loss: 2.6184379664865807

Epoch: 6| Step: 13
Training loss: 2.8817393055003646
Validation loss: 2.618068611276612

Epoch: 55| Step: 0
Training loss: 2.4838137202543646
Validation loss: 2.6151897644082274

Epoch: 6| Step: 1
Training loss: 3.2769019973809796
Validation loss: 2.6138190460693504

Epoch: 6| Step: 2
Training loss: 2.3458327131343943
Validation loss: 2.6133308044813854

Epoch: 6| Step: 3
Training loss: 2.9575539936931583
Validation loss: 2.6126301576354707

Epoch: 6| Step: 4
Training loss: 2.9231205697604135
Validation loss: 2.611493664870106

Epoch: 6| Step: 5
Training loss: 2.570246640552989
Validation loss: 2.611796004065365

Epoch: 6| Step: 6
Training loss: 3.116566378633967
Validation loss: 2.6088526616035357

Epoch: 6| Step: 7
Training loss: 2.5530001699684406
Validation loss: 2.607263620743669

Epoch: 6| Step: 8
Training loss: 2.5912929065057715
Validation loss: 2.605345532936666

Epoch: 6| Step: 9
Training loss: 3.0229375066023514
Validation loss: 2.6029510152923887

Epoch: 6| Step: 10
Training loss: 2.706001118763356
Validation loss: 2.602847983590582

Epoch: 6| Step: 11
Training loss: 2.807322080278758
Validation loss: 2.6010356442146616

Epoch: 6| Step: 12
Training loss: 2.507851953879436
Validation loss: 2.5995728881629354

Epoch: 6| Step: 13
Training loss: 2.3965665496939192
Validation loss: 2.5992229431442624

Epoch: 56| Step: 0
Training loss: 2.5895647960274535
Validation loss: 2.5983084029722057

Epoch: 6| Step: 1
Training loss: 2.9625384750765
Validation loss: 2.597288223239432

Epoch: 6| Step: 2
Training loss: 2.6048568624757134
Validation loss: 2.5972885598217923

Epoch: 6| Step: 3
Training loss: 2.6123198378649453
Validation loss: 2.595102868904689

Epoch: 6| Step: 4
Training loss: 2.783955319133751
Validation loss: 2.596451849647876

Epoch: 6| Step: 5
Training loss: 2.787387547983123
Validation loss: 2.5965635826777955

Epoch: 6| Step: 6
Training loss: 2.2893827559339495
Validation loss: 2.5930903258464513

Epoch: 6| Step: 7
Training loss: 2.3919036038983252
Validation loss: 2.5965539873851107

Epoch: 6| Step: 8
Training loss: 2.4200245340538107
Validation loss: 2.594464475589329

Epoch: 6| Step: 9
Training loss: 2.6952101535991972
Validation loss: 2.5924179839253974

Epoch: 6| Step: 10
Training loss: 2.9872266317135616
Validation loss: 2.591300819145229

Epoch: 6| Step: 11
Training loss: 2.9001410482898216
Validation loss: 2.592311797439545

Epoch: 6| Step: 12
Training loss: 2.8874598314538122
Validation loss: 2.589886756566743

Epoch: 6| Step: 13
Training loss: 3.2146856331673823
Validation loss: 2.588817164742307

Epoch: 57| Step: 0
Training loss: 2.3505041839620797
Validation loss: 2.5902767596159006

Epoch: 6| Step: 1
Training loss: 2.8164551792841483
Validation loss: 2.587582574899676

Epoch: 6| Step: 2
Training loss: 3.2694310545523346
Validation loss: 2.5870346400287922

Epoch: 6| Step: 3
Training loss: 2.6230070632333367
Validation loss: 2.58733711897152

Epoch: 6| Step: 4
Training loss: 2.2864192765244837
Validation loss: 2.584698705915277

Epoch: 6| Step: 5
Training loss: 3.226952037434752
Validation loss: 2.582567214164993

Epoch: 6| Step: 6
Training loss: 2.992964919965818
Validation loss: 2.584742044063895

Epoch: 6| Step: 7
Training loss: 2.651276903389162
Validation loss: 2.581903897913813

Epoch: 6| Step: 8
Training loss: 2.6299386569219374
Validation loss: 2.582721565985859

Epoch: 6| Step: 9
Training loss: 2.720348808839438
Validation loss: 2.5808651012928534

Epoch: 6| Step: 10
Training loss: 2.3259066054929174
Validation loss: 2.5817163441776927

Epoch: 6| Step: 11
Training loss: 3.213434070205824
Validation loss: 2.577074457233987

Epoch: 6| Step: 12
Training loss: 2.5939095689338596
Validation loss: 2.5787912760131038

Epoch: 6| Step: 13
Training loss: 1.9883650066968508
Validation loss: 2.578191644597109

Epoch: 58| Step: 0
Training loss: 2.817186900819534
Validation loss: 2.5778325387008834

Epoch: 6| Step: 1
Training loss: 2.3036635370944847
Validation loss: 2.5739780944493797

Epoch: 6| Step: 2
Training loss: 2.9203934610817934
Validation loss: 2.5739471492938604

Epoch: 6| Step: 3
Training loss: 2.4060660948391206
Validation loss: 2.57521720821951

Epoch: 6| Step: 4
Training loss: 2.9841934867902857
Validation loss: 2.5758794079151093

Epoch: 6| Step: 5
Training loss: 2.8181928028944165
Validation loss: 2.5708026206120755

Epoch: 6| Step: 6
Training loss: 3.342345896601562
Validation loss: 2.5692443792678765

Epoch: 6| Step: 7
Training loss: 2.7316511265528156
Validation loss: 2.5720004768400804

Epoch: 6| Step: 8
Training loss: 2.3407936269729435
Validation loss: 2.5686272813823776

Epoch: 6| Step: 9
Training loss: 2.7649399938159736
Validation loss: 2.571313402268607

Epoch: 6| Step: 10
Training loss: 2.5615411220438906
Validation loss: 2.5666918390038758

Epoch: 6| Step: 11
Training loss: 2.248031072619036
Validation loss: 2.5670723788483754

Epoch: 6| Step: 12
Training loss: 2.6303652474207584
Validation loss: 2.5696116132454776

Epoch: 6| Step: 13
Training loss: 2.776864011726902
Validation loss: 2.566121387978198

Epoch: 59| Step: 0
Training loss: 2.5986285003546055
Validation loss: 2.563177313882744

Epoch: 6| Step: 1
Training loss: 2.745617495569315
Validation loss: 2.565051924791549

Epoch: 6| Step: 2
Training loss: 2.677013430938259
Validation loss: 2.5653869369772613

Epoch: 6| Step: 3
Training loss: 2.8370463407736692
Validation loss: 2.566559963567999

Epoch: 6| Step: 4
Training loss: 2.261746579330082
Validation loss: 2.565068368936275

Epoch: 6| Step: 5
Training loss: 2.7877189755496166
Validation loss: 2.5644733305166802

Epoch: 6| Step: 6
Training loss: 2.701517674618631
Validation loss: 2.5640285863963976

Epoch: 6| Step: 7
Training loss: 2.9231460173158994
Validation loss: 2.562092306688292

Epoch: 6| Step: 8
Training loss: 2.840059002008179
Validation loss: 2.561930600925032

Epoch: 6| Step: 9
Training loss: 2.1859513386635308
Validation loss: 2.5570913470733547

Epoch: 6| Step: 10
Training loss: 2.8249771556099383
Validation loss: 2.5591806141522846

Epoch: 6| Step: 11
Training loss: 3.0673552970052893
Validation loss: 2.556721118022218

Epoch: 6| Step: 12
Training loss: 2.7358993205179347
Validation loss: 2.5661321345565558

Epoch: 6| Step: 13
Training loss: 2.440070923884473
Validation loss: 2.57899384933888

Epoch: 60| Step: 0
Training loss: 2.597631285124398
Validation loss: 2.580874477777431

Epoch: 6| Step: 1
Training loss: 2.658630202248355
Validation loss: 2.579781295023159

Epoch: 6| Step: 2
Training loss: 2.8837601259448156
Validation loss: 2.58107051394678

Epoch: 6| Step: 3
Training loss: 2.4493246587946826
Validation loss: 2.5796998733602696

Epoch: 6| Step: 4
Training loss: 2.4698619993941007
Validation loss: 2.5760477039489054

Epoch: 6| Step: 5
Training loss: 3.0434306703720604
Validation loss: 2.5773071224789508

Epoch: 6| Step: 6
Training loss: 2.7555836432146634
Validation loss: 2.573632520378571

Epoch: 6| Step: 7
Training loss: 2.972140012972785
Validation loss: 2.5700080791796642

Epoch: 6| Step: 8
Training loss: 2.362839536135061
Validation loss: 2.5722437041557176

Epoch: 6| Step: 9
Training loss: 2.6556246750728834
Validation loss: 2.572987286503291

Epoch: 6| Step: 10
Training loss: 2.820841309063303
Validation loss: 2.570544803406268

Epoch: 6| Step: 11
Training loss: 2.3086973318754325
Validation loss: 2.567260940501204

Epoch: 6| Step: 12
Training loss: 2.9585761566082227
Validation loss: 2.568161308667533

Epoch: 6| Step: 13
Training loss: 2.801205995240442
Validation loss: 2.5698606640836315

Epoch: 61| Step: 0
Training loss: 3.0540068275360355
Validation loss: 2.5683548959903315

Epoch: 6| Step: 1
Training loss: 3.062927839016369
Validation loss: 2.569463201116428

Epoch: 6| Step: 2
Training loss: 2.5269147694581457
Validation loss: 2.567807841097693

Epoch: 6| Step: 3
Training loss: 2.585087846927173
Validation loss: 2.5684190634402158

Epoch: 6| Step: 4
Training loss: 2.69890357467085
Validation loss: 2.5684164178699893

Epoch: 6| Step: 5
Training loss: 2.167889787765276
Validation loss: 2.568121241757628

Epoch: 6| Step: 6
Training loss: 2.541789963863645
Validation loss: 2.568733758557929

Epoch: 6| Step: 7
Training loss: 2.97291048544277
Validation loss: 2.5651355154119586

Epoch: 6| Step: 8
Training loss: 2.284969276538869
Validation loss: 2.566132645559881

Epoch: 6| Step: 9
Training loss: 2.697320427914475
Validation loss: 2.562814786231104

Epoch: 6| Step: 10
Training loss: 2.349797597244571
Validation loss: 2.561384733698402

Epoch: 6| Step: 11
Training loss: 2.57135183545314
Validation loss: 2.557722880153351

Epoch: 6| Step: 12
Training loss: 3.023113854407238
Validation loss: 2.557755101328306

Epoch: 6| Step: 13
Training loss: 3.097122511995462
Validation loss: 2.556868560243322

Epoch: 62| Step: 0
Training loss: 2.7813568094725136
Validation loss: 2.557055994013742

Epoch: 6| Step: 1
Training loss: 2.337631172946755
Validation loss: 2.556520743360695

Epoch: 6| Step: 2
Training loss: 2.6829481389411085
Validation loss: 2.5585527489558335

Epoch: 6| Step: 3
Training loss: 2.5688106706048957
Validation loss: 2.559687784820778

Epoch: 6| Step: 4
Training loss: 2.7844144841914953
Validation loss: 2.5587320304951504

Epoch: 6| Step: 5
Training loss: 2.867336175790191
Validation loss: 2.558090603110438

Epoch: 6| Step: 6
Training loss: 2.5077758976532056
Validation loss: 2.5581063541615428

Epoch: 6| Step: 7
Training loss: 2.489690887383749
Validation loss: 2.556046321872016

Epoch: 6| Step: 8
Training loss: 2.9727650044782528
Validation loss: 2.5547735606094277

Epoch: 6| Step: 9
Training loss: 2.9493159196016494
Validation loss: 2.5573363420011637

Epoch: 6| Step: 10
Training loss: 2.3098607854467184
Validation loss: 2.554996592325923

Epoch: 6| Step: 11
Training loss: 2.8249260951814836
Validation loss: 2.553878811727455

Epoch: 6| Step: 12
Training loss: 2.881265530161677
Validation loss: 2.5544192205362446

Epoch: 6| Step: 13
Training loss: 2.543798542731121
Validation loss: 2.5522072950087265

Epoch: 63| Step: 0
Training loss: 2.5253447420063275
Validation loss: 2.5498192274306573

Epoch: 6| Step: 1
Training loss: 3.3530056920650324
Validation loss: 2.5452066429548688

Epoch: 6| Step: 2
Training loss: 2.3214149810057845
Validation loss: 2.5446981632784245

Epoch: 6| Step: 3
Training loss: 2.7196683538429864
Validation loss: 2.5465226973121253

Epoch: 6| Step: 4
Training loss: 2.446992632107697
Validation loss: 2.5496010730082825

Epoch: 6| Step: 5
Training loss: 2.559808014882968
Validation loss: 2.5465711166604934

Epoch: 6| Step: 6
Training loss: 2.6831380354455563
Validation loss: 2.549714656522663

Epoch: 6| Step: 7
Training loss: 2.760718697392991
Validation loss: 2.548745527427233

Epoch: 6| Step: 8
Training loss: 2.852775253134649
Validation loss: 2.5473555909100978

Epoch: 6| Step: 9
Training loss: 2.42605486077706
Validation loss: 2.5464744018790073

Epoch: 6| Step: 10
Training loss: 2.917665764578586
Validation loss: 2.548203478744524

Epoch: 6| Step: 11
Training loss: 2.7397481387696208
Validation loss: 2.5500612279609665

Epoch: 6| Step: 12
Training loss: 2.402733549422716
Validation loss: 2.546020067882535

Epoch: 6| Step: 13
Training loss: 2.520968620095915
Validation loss: 2.5453158483762284

Epoch: 64| Step: 0
Training loss: 2.757283708488606
Validation loss: 2.546396269007738

Epoch: 6| Step: 1
Training loss: 2.8621985664074603
Validation loss: 2.5410973010178806

Epoch: 6| Step: 2
Training loss: 2.6861393167548315
Validation loss: 2.541632407119072

Epoch: 6| Step: 3
Training loss: 2.292121171674222
Validation loss: 2.5406092700729372

Epoch: 6| Step: 4
Training loss: 2.7565655793641044
Validation loss: 2.538955842736811

Epoch: 6| Step: 5
Training loss: 2.6534326309844105
Validation loss: 2.538816516404905

Epoch: 6| Step: 6
Training loss: 2.4646555107771553
Validation loss: 2.538134126400221

Epoch: 6| Step: 7
Training loss: 2.6717125915781192
Validation loss: 2.536983104105738

Epoch: 6| Step: 8
Training loss: 2.873196243355478
Validation loss: 2.5393166126346016

Epoch: 6| Step: 9
Training loss: 2.79319854004445
Validation loss: 2.5316023326289634

Epoch: 6| Step: 10
Training loss: 2.5665926930885576
Validation loss: 2.5328345499994867

Epoch: 6| Step: 11
Training loss: 2.223306517184315
Validation loss: 2.5335953604141874

Epoch: 6| Step: 12
Training loss: 3.1097887233589785
Validation loss: 2.5348594033266387

Epoch: 6| Step: 13
Training loss: 2.51793209455217
Validation loss: 2.5351135062213013

Epoch: 65| Step: 0
Training loss: 2.345413838128043
Validation loss: 2.533207156360235

Epoch: 6| Step: 1
Training loss: 2.6833511975880397
Validation loss: 2.5316828290947866

Epoch: 6| Step: 2
Training loss: 2.588088785897846
Validation loss: 2.533621332671471

Epoch: 6| Step: 3
Training loss: 2.2836763203244765
Validation loss: 2.53293641352268

Epoch: 6| Step: 4
Training loss: 3.057962286647371
Validation loss: 2.53363819253897

Epoch: 6| Step: 5
Training loss: 3.1229036547640203
Validation loss: 2.5333753540502593

Epoch: 6| Step: 6
Training loss: 2.5437621770093983
Validation loss: 2.534096487023339

Epoch: 6| Step: 7
Training loss: 2.896892783759548
Validation loss: 2.5299813658891277

Epoch: 6| Step: 8
Training loss: 2.2723293684382537
Validation loss: 2.532296831593071

Epoch: 6| Step: 9
Training loss: 2.57206318608294
Validation loss: 2.530664603904122

Epoch: 6| Step: 10
Training loss: 2.6602989304155127
Validation loss: 2.5283243752205395

Epoch: 6| Step: 11
Training loss: 2.2098991193229227
Validation loss: 2.5308658952055456

Epoch: 6| Step: 12
Training loss: 2.623823174578012
Validation loss: 2.525135488620503

Epoch: 6| Step: 13
Training loss: 3.128193406186905
Validation loss: 2.5253079689229754

Epoch: 66| Step: 0
Training loss: 2.6383838962313315
Validation loss: 2.527535322692744

Epoch: 6| Step: 1
Training loss: 2.9981384859781772
Validation loss: 2.524380833341302

Epoch: 6| Step: 2
Training loss: 2.7052773082127928
Validation loss: 2.518620336711636

Epoch: 6| Step: 3
Training loss: 2.2452883661362817
Validation loss: 2.5059127346035583

Epoch: 6| Step: 4
Training loss: 2.9559261217189694
Validation loss: 2.510422486315639

Epoch: 6| Step: 5
Training loss: 2.4948672055546464
Validation loss: 2.514359608084024

Epoch: 6| Step: 6
Training loss: 2.3828770362994587
Validation loss: 2.5140550820743237

Epoch: 6| Step: 7
Training loss: 2.439711521167729
Validation loss: 2.515410988749508

Epoch: 6| Step: 8
Training loss: 2.249136653142551
Validation loss: 2.5107368064613995

Epoch: 6| Step: 9
Training loss: 2.9039149594426497
Validation loss: 2.5112153256245495

Epoch: 6| Step: 10
Training loss: 2.7566557016888793
Validation loss: 2.5146429384041813

Epoch: 6| Step: 11
Training loss: 2.6382605445959797
Validation loss: 2.5117221355662607

Epoch: 6| Step: 12
Training loss: 2.78362412865716
Validation loss: 2.5129118479022203

Epoch: 6| Step: 13
Training loss: 2.609404329603364
Validation loss: 2.5132843880816105

Epoch: 67| Step: 0
Training loss: 1.7813269615278289
Validation loss: 2.510001948965058

Epoch: 6| Step: 1
Training loss: 2.894770438624449
Validation loss: 2.511880555980037

Epoch: 6| Step: 2
Training loss: 2.9504330591993453
Validation loss: 2.5091027637487704

Epoch: 6| Step: 3
Training loss: 2.6528966942497645
Validation loss: 2.510157359384481

Epoch: 6| Step: 4
Training loss: 2.262496366550626
Validation loss: 2.507166810064251

Epoch: 6| Step: 5
Training loss: 2.425098857634327
Validation loss: 2.509206287481278

Epoch: 6| Step: 6
Training loss: 2.867435787582809
Validation loss: 2.508086224251558

Epoch: 6| Step: 7
Training loss: 2.2652160275409106
Validation loss: 2.50918275469645

Epoch: 6| Step: 8
Training loss: 2.691730363483855
Validation loss: 2.5032133114619413

Epoch: 6| Step: 9
Training loss: 2.9307515971182596
Validation loss: 2.5059829962890046

Epoch: 6| Step: 10
Training loss: 2.5255262865062855
Validation loss: 2.504221895807431

Epoch: 6| Step: 11
Training loss: 2.6721438445530334
Validation loss: 2.5025452055454145

Epoch: 6| Step: 12
Training loss: 3.2244220126991756
Validation loss: 2.5053597217480896

Epoch: 6| Step: 13
Training loss: 2.4272370078316414
Validation loss: 2.5004466452568357

Epoch: 68| Step: 0
Training loss: 2.6451929974587736
Validation loss: 2.505649120759938

Epoch: 6| Step: 1
Training loss: 2.885678404540456
Validation loss: 2.5008166092090223

Epoch: 6| Step: 2
Training loss: 2.8457889164315695
Validation loss: 2.5010842674108815

Epoch: 6| Step: 3
Training loss: 2.786337838583149
Validation loss: 2.5015795803817293

Epoch: 6| Step: 4
Training loss: 2.6087483093226993
Validation loss: 2.5031868570203795

Epoch: 6| Step: 5
Training loss: 2.513754486884272
Validation loss: 2.5055603023304585

Epoch: 6| Step: 6
Training loss: 2.736392862451326
Validation loss: 2.5028256977540027

Epoch: 6| Step: 7
Training loss: 2.3903329708276586
Validation loss: 2.5027575067124253

Epoch: 6| Step: 8
Training loss: 2.5575780352100628
Validation loss: 2.5034674360629565

Epoch: 6| Step: 9
Training loss: 2.3503670426095904
Validation loss: 2.500159242644624

Epoch: 6| Step: 10
Training loss: 2.5904459303569776
Validation loss: 2.4981585396392205

Epoch: 6| Step: 11
Training loss: 2.413597185541522
Validation loss: 2.4992688540378754

Epoch: 6| Step: 12
Training loss: 2.7041114374368265
Validation loss: 2.4961472785634675

Epoch: 6| Step: 13
Training loss: 2.712512699220525
Validation loss: 2.5052842243037996

Epoch: 69| Step: 0
Training loss: 3.0353541954033525
Validation loss: 2.5011857558602064

Epoch: 6| Step: 1
Training loss: 2.3516158291165383
Validation loss: 2.498483245411734

Epoch: 6| Step: 2
Training loss: 3.1578283612486677
Validation loss: 2.4996835905752612

Epoch: 6| Step: 3
Training loss: 2.5140974252194113
Validation loss: 2.5037857637661847

Epoch: 6| Step: 4
Training loss: 2.006617565293578
Validation loss: 2.5020819260346845

Epoch: 6| Step: 5
Training loss: 2.367663244401873
Validation loss: 2.5009572421728685

Epoch: 6| Step: 6
Training loss: 2.3816935929559744
Validation loss: 2.498069256047731

Epoch: 6| Step: 7
Training loss: 2.4057884269023098
Validation loss: 2.498081886060076

Epoch: 6| Step: 8
Training loss: 3.0470073964624174
Validation loss: 2.501118012618403

Epoch: 6| Step: 9
Training loss: 3.0989191816832675
Validation loss: 2.5019582311105153

Epoch: 6| Step: 10
Training loss: 2.2985686201897892
Validation loss: 2.502005630883239

Epoch: 6| Step: 11
Training loss: 2.5084881689329848
Validation loss: 2.505469680657495

Epoch: 6| Step: 12
Training loss: 2.6711688976980277
Validation loss: 2.5022328736792048

Epoch: 6| Step: 13
Training loss: 2.604555899459761
Validation loss: 2.5010828534068423

Epoch: 70| Step: 0
Training loss: 2.091648014588294
Validation loss: 2.498450792637896

Epoch: 6| Step: 1
Training loss: 2.5786060231299337
Validation loss: 2.4957606292817203

Epoch: 6| Step: 2
Training loss: 2.522400823180321
Validation loss: 2.494765683054086

Epoch: 6| Step: 3
Training loss: 2.4458421614930037
Validation loss: 2.491021548252123

Epoch: 6| Step: 4
Training loss: 2.8791306717458647
Validation loss: 2.492076827474841

Epoch: 6| Step: 5
Training loss: 2.2192554166864586
Validation loss: 2.494234828618749

Epoch: 6| Step: 6
Training loss: 2.8158261446387374
Validation loss: 2.4895457713167843

Epoch: 6| Step: 7
Training loss: 2.8791712479801044
Validation loss: 2.4922979922300574

Epoch: 6| Step: 8
Training loss: 2.4109456208661535
Validation loss: 2.4924856583550303

Epoch: 6| Step: 9
Training loss: 2.7861348662584438
Validation loss: 2.490884884908845

Epoch: 6| Step: 10
Training loss: 2.901104874438819
Validation loss: 2.494992565967032

Epoch: 6| Step: 11
Training loss: 2.742037081600568
Validation loss: 2.493781302629287

Epoch: 6| Step: 12
Training loss: 2.0948209158892834
Validation loss: 2.4942956217592607

Epoch: 6| Step: 13
Training loss: 2.9978276970982267
Validation loss: 2.4982454341807387

Epoch: 71| Step: 0
Training loss: 3.2049159555827096
Validation loss: 2.4969517084528285

Epoch: 6| Step: 1
Training loss: 2.0721036252653704
Validation loss: 2.4943426175374412

Epoch: 6| Step: 2
Training loss: 2.268453161403911
Validation loss: 2.496437077001358

Epoch: 6| Step: 3
Training loss: 2.519158198217993
Validation loss: 2.4976793008312788

Epoch: 6| Step: 4
Training loss: 2.513703743970473
Validation loss: 2.4970502140219457

Epoch: 6| Step: 5
Training loss: 2.773866174370027
Validation loss: 2.5036542332186063

Epoch: 6| Step: 6
Training loss: 3.0042508526636023
Validation loss: 2.503899933821418

Epoch: 6| Step: 7
Training loss: 2.794885620479178
Validation loss: 2.50613006216025

Epoch: 6| Step: 8
Training loss: 2.652461143446342
Validation loss: 2.500153822934937

Epoch: 6| Step: 9
Training loss: 1.8799109043152689
Validation loss: 2.4955283545965528

Epoch: 6| Step: 10
Training loss: 2.203343590247461
Validation loss: 2.4975051352111137

Epoch: 6| Step: 11
Training loss: 2.8829238167130242
Validation loss: 2.498738367900582

Epoch: 6| Step: 12
Training loss: 2.2512466367943196
Validation loss: 2.4954472412471604

Epoch: 6| Step: 13
Training loss: 3.0678377928346428
Validation loss: 2.4949631336727256

Epoch: 72| Step: 0
Training loss: 2.568663650880547
Validation loss: 2.498027174103702

Epoch: 6| Step: 1
Training loss: 2.6612164013534794
Validation loss: 2.4942975175382993

Epoch: 6| Step: 2
Training loss: 2.148654552246055
Validation loss: 2.494142911228376

Epoch: 6| Step: 3
Training loss: 2.409402540868352
Validation loss: 2.4971233745691297

Epoch: 6| Step: 4
Training loss: 2.6358863703501068
Validation loss: 2.5003766888706513

Epoch: 6| Step: 5
Training loss: 2.78926679787258
Validation loss: 2.5028994674138247

Epoch: 6| Step: 6
Training loss: 2.6367826778997947
Validation loss: 2.499430671078881

Epoch: 6| Step: 7
Training loss: 2.620175241914144
Validation loss: 2.499057019728442

Epoch: 6| Step: 8
Training loss: 2.623237290504559
Validation loss: 2.500253966465238

Epoch: 6| Step: 9
Training loss: 2.7306466979188246
Validation loss: 2.4965470628224926

Epoch: 6| Step: 10
Training loss: 2.626114744818684
Validation loss: 2.4930726555179095

Epoch: 6| Step: 11
Training loss: 2.888962386084819
Validation loss: 2.4947637398470093

Epoch: 6| Step: 12
Training loss: 2.578591044524576
Validation loss: 2.4921921876486577

Epoch: 6| Step: 13
Training loss: 2.6655246156705448
Validation loss: 2.4955700885690733

Epoch: 73| Step: 0
Training loss: 2.9424740498321387
Validation loss: 2.490093232187821

Epoch: 6| Step: 1
Training loss: 2.5350303207179254
Validation loss: 2.490164961501937

Epoch: 6| Step: 2
Training loss: 2.535402917837721
Validation loss: 2.4920517138059233

Epoch: 6| Step: 3
Training loss: 3.1442570726743226
Validation loss: 2.48806939054054

Epoch: 6| Step: 4
Training loss: 2.385452958765158
Validation loss: 2.488720721805932

Epoch: 6| Step: 5
Training loss: 2.363557657518648
Validation loss: 2.486113877416264

Epoch: 6| Step: 6
Training loss: 2.2587851570910358
Validation loss: 2.485559816759929

Epoch: 6| Step: 7
Training loss: 2.5912270282439276
Validation loss: 2.488035883600782

Epoch: 6| Step: 8
Training loss: 3.0291945577296784
Validation loss: 2.491700638079116

Epoch: 6| Step: 9
Training loss: 2.473430879816581
Validation loss: 2.484331222564334

Epoch: 6| Step: 10
Training loss: 2.8271108252571446
Validation loss: 2.4879368773800996

Epoch: 6| Step: 11
Training loss: 2.3926028378825746
Validation loss: 2.4886425640292353

Epoch: 6| Step: 12
Training loss: 2.467944146468698
Validation loss: 2.4848523591033214

Epoch: 6| Step: 13
Training loss: 2.3647478581354404
Validation loss: 2.4946745418272727

Epoch: 74| Step: 0
Training loss: 2.3750740842808997
Validation loss: 2.4875624580145925

Epoch: 6| Step: 1
Training loss: 2.1905189390532724
Validation loss: 2.489478381723293

Epoch: 6| Step: 2
Training loss: 2.673038162558412
Validation loss: 2.492784940724994

Epoch: 6| Step: 3
Training loss: 3.258184690735452
Validation loss: 2.4864790461849053

Epoch: 6| Step: 4
Training loss: 2.8397268824818074
Validation loss: 2.4879860855134575

Epoch: 6| Step: 5
Training loss: 2.2465794311853298
Validation loss: 2.488545258557897

Epoch: 6| Step: 6
Training loss: 2.9190122890860617
Validation loss: 2.4888695900127207

Epoch: 6| Step: 7
Training loss: 2.667203511400663
Validation loss: 2.4923408564755505

Epoch: 6| Step: 8
Training loss: 2.225680527930378
Validation loss: 2.4896354802840572

Epoch: 6| Step: 9
Training loss: 3.121856939895521
Validation loss: 2.4910444390676925

Epoch: 6| Step: 10
Training loss: 1.7926259097432222
Validation loss: 2.4891704605357994

Epoch: 6| Step: 11
Training loss: 2.5373046423966588
Validation loss: 2.4866969216383716

Epoch: 6| Step: 12
Training loss: 2.789588173445894
Validation loss: 2.4917631754689666

Epoch: 6| Step: 13
Training loss: 2.4058587883330587
Validation loss: 2.4870160857384582

Epoch: 75| Step: 0
Training loss: 2.0519838131494823
Validation loss: 2.4872171710837865

Epoch: 6| Step: 1
Training loss: 2.5614121151467613
Validation loss: 2.482622444139864

Epoch: 6| Step: 2
Training loss: 2.264071227715666
Validation loss: 2.482093755335047

Epoch: 6| Step: 3
Training loss: 2.6733867986030506
Validation loss: 2.486426508081494

Epoch: 6| Step: 4
Training loss: 2.594170593722768
Validation loss: 2.481688424363668

Epoch: 6| Step: 5
Training loss: 2.512295527702098
Validation loss: 2.484073372690434

Epoch: 6| Step: 6
Training loss: 2.5624759952072105
Validation loss: 2.4797172389163125

Epoch: 6| Step: 7
Training loss: 2.155254410597936
Validation loss: 2.481648866765952

Epoch: 6| Step: 8
Training loss: 2.3079557360167984
Validation loss: 2.4828512129137184

Epoch: 6| Step: 9
Training loss: 3.177980756909044
Validation loss: 2.481799640201462

Epoch: 6| Step: 10
Training loss: 2.861323458827253
Validation loss: 2.4854056661596347

Epoch: 6| Step: 11
Training loss: 3.3070210183406865
Validation loss: 2.4834336950729714

Epoch: 6| Step: 12
Training loss: 2.178258721558216
Validation loss: 2.481268293824167

Epoch: 6| Step: 13
Training loss: 2.795337787341078
Validation loss: 2.480713807663887

Epoch: 76| Step: 0
Training loss: 2.3249036789497417
Validation loss: 2.484583096216056

Epoch: 6| Step: 1
Training loss: 2.9853421386729737
Validation loss: 2.4794866745376547

Epoch: 6| Step: 2
Training loss: 2.4693843171511873
Validation loss: 2.4810716351924222

Epoch: 6| Step: 3
Training loss: 2.344532950278978
Validation loss: 2.475940574109013

Epoch: 6| Step: 4
Training loss: 2.765482408543821
Validation loss: 2.482041564670223

Epoch: 6| Step: 5
Training loss: 2.288604169665466
Validation loss: 2.4831275447176635

Epoch: 6| Step: 6
Training loss: 2.6453246781620336
Validation loss: 2.4847549211958886

Epoch: 6| Step: 7
Training loss: 2.6920909259852226
Validation loss: 2.4874285200446344

Epoch: 6| Step: 8
Training loss: 2.2266355268314895
Validation loss: 2.4874051246888613

Epoch: 6| Step: 9
Training loss: 2.897111532848649
Validation loss: 2.4851029327279077

Epoch: 6| Step: 10
Training loss: 2.998052759817107
Validation loss: 2.478395603922308

Epoch: 6| Step: 11
Training loss: 2.4915349698002043
Validation loss: 2.479491001573088

Epoch: 6| Step: 12
Training loss: 2.011324530928159
Validation loss: 2.4835596886037394

Epoch: 6| Step: 13
Training loss: 3.025920154191286
Validation loss: 2.4855323830649247

Epoch: 77| Step: 0
Training loss: 2.338993994474754
Validation loss: 2.4867751325758345

Epoch: 6| Step: 1
Training loss: 2.476243055662213
Validation loss: 2.4888656464931453

Epoch: 6| Step: 2
Training loss: 3.0066047126099207
Validation loss: 2.4921018611659513

Epoch: 6| Step: 3
Training loss: 2.391802528944097
Validation loss: 2.4893247132198906

Epoch: 6| Step: 4
Training loss: 2.675444599887628
Validation loss: 2.4911992456472585

Epoch: 6| Step: 5
Training loss: 2.6023518965589894
Validation loss: 2.4889117948602975

Epoch: 6| Step: 6
Training loss: 2.783545928724169
Validation loss: 2.485219240194778

Epoch: 6| Step: 7
Training loss: 2.5925866700917166
Validation loss: 2.4862262218496056

Epoch: 6| Step: 8
Training loss: 2.8930531793605865
Validation loss: 2.4866771787603623

Epoch: 6| Step: 9
Training loss: 2.047664577404614
Validation loss: 2.481235591869016

Epoch: 6| Step: 10
Training loss: 2.391488910232216
Validation loss: 2.4793567237343876

Epoch: 6| Step: 11
Training loss: 3.0837447089624184
Validation loss: 2.484183058131706

Epoch: 6| Step: 12
Training loss: 2.627176563162583
Validation loss: 2.480220463859397

Epoch: 6| Step: 13
Training loss: 2.4012163695840383
Validation loss: 2.4790069838529742

Epoch: 78| Step: 0
Training loss: 2.442510882912699
Validation loss: 2.4769920353685055

Epoch: 6| Step: 1
Training loss: 2.488193672112154
Validation loss: 2.4779923219507953

Epoch: 6| Step: 2
Training loss: 2.739397765475594
Validation loss: 2.478060585137264

Epoch: 6| Step: 3
Training loss: 2.7140562383549853
Validation loss: 2.4835933999294033

Epoch: 6| Step: 4
Training loss: 2.2463856383030323
Validation loss: 2.475766058895572

Epoch: 6| Step: 5
Training loss: 2.9485989557240195
Validation loss: 2.4790227084210397

Epoch: 6| Step: 6
Training loss: 2.495212930831842
Validation loss: 2.477953354806749

Epoch: 6| Step: 7
Training loss: 2.7443795422573114
Validation loss: 2.48219401916803

Epoch: 6| Step: 8
Training loss: 2.4757881759108513
Validation loss: 2.495623063117681

Epoch: 6| Step: 9
Training loss: 2.6513263621971994
Validation loss: 2.486338976736228

Epoch: 6| Step: 10
Training loss: 2.5612335797328543
Validation loss: 2.477216255229946

Epoch: 6| Step: 11
Training loss: 2.594370112918817
Validation loss: 2.477124877358869

Epoch: 6| Step: 12
Training loss: 2.478749750380665
Validation loss: 2.4726748433149752

Epoch: 6| Step: 13
Training loss: 2.6747513842694555
Validation loss: 2.48072870449265

Epoch: 79| Step: 0
Training loss: 2.690874332301903
Validation loss: 2.4818156432796084

Epoch: 6| Step: 1
Training loss: 2.4043428864403205
Validation loss: 2.479623309055552

Epoch: 6| Step: 2
Training loss: 2.2111337527463553
Validation loss: 2.4782218468411674

Epoch: 6| Step: 3
Training loss: 2.6025255041860715
Validation loss: 2.483265603078462

Epoch: 6| Step: 4
Training loss: 2.6398892252251724
Validation loss: 2.4825350587795247

Epoch: 6| Step: 5
Training loss: 2.842317828377433
Validation loss: 2.4814130772730376

Epoch: 6| Step: 6
Training loss: 2.418363819293839
Validation loss: 2.4816221424136864

Epoch: 6| Step: 7
Training loss: 2.416418610919524
Validation loss: 2.484261452181309

Epoch: 6| Step: 8
Training loss: 2.4956285404339753
Validation loss: 2.485077556702105

Epoch: 6| Step: 9
Training loss: 2.8210164298546307
Validation loss: 2.482050962272784

Epoch: 6| Step: 10
Training loss: 2.7649624995470314
Validation loss: 2.478076588308929

Epoch: 6| Step: 11
Training loss: 2.6094292732122426
Validation loss: 2.479016545235758

Epoch: 6| Step: 12
Training loss: 2.5299557803364396
Validation loss: 2.4812426543747614

Epoch: 6| Step: 13
Training loss: 2.748506313761551
Validation loss: 2.4790081860405984

Epoch: 80| Step: 0
Training loss: 2.9130862648759455
Validation loss: 2.4762342137143727

Epoch: 6| Step: 1
Training loss: 2.4558477174225266
Validation loss: 2.470891402035565

Epoch: 6| Step: 2
Training loss: 2.1805560388044833
Validation loss: 2.473283717213892

Epoch: 6| Step: 3
Training loss: 2.305858043248281
Validation loss: 2.4714089090429816

Epoch: 6| Step: 4
Training loss: 2.5509159350508854
Validation loss: 2.4773435594455786

Epoch: 6| Step: 5
Training loss: 2.781324749917196
Validation loss: 2.4713541934646805

Epoch: 6| Step: 6
Training loss: 2.866683541477423
Validation loss: 2.4793605942304024

Epoch: 6| Step: 7
Training loss: 2.9395150922662623
Validation loss: 2.4680997177172497

Epoch: 6| Step: 8
Training loss: 2.578574401527687
Validation loss: 2.4726940471538486

Epoch: 6| Step: 9
Training loss: 2.0721664476885704
Validation loss: 2.4720114720509403

Epoch: 6| Step: 10
Training loss: 2.3168193583859944
Validation loss: 2.468612779755272

Epoch: 6| Step: 11
Training loss: 2.5617380637662066
Validation loss: 2.47377010767139

Epoch: 6| Step: 12
Training loss: 2.5553023505432093
Validation loss: 2.472792522739084

Epoch: 6| Step: 13
Training loss: 2.900819360147246
Validation loss: 2.4742028901876965

Epoch: 81| Step: 0
Training loss: 2.5291662233992245
Validation loss: 2.472070931033228

Epoch: 6| Step: 1
Training loss: 2.3957064608829497
Validation loss: 2.4794383634755333

Epoch: 6| Step: 2
Training loss: 2.308239698289747
Validation loss: 2.4777588306390625

Epoch: 6| Step: 3
Training loss: 2.8336796455416637
Validation loss: 2.4796155368271613

Epoch: 6| Step: 4
Training loss: 2.2886279217649657
Validation loss: 2.4817343219154133

Epoch: 6| Step: 5
Training loss: 3.003614155987229
Validation loss: 2.479248179102809

Epoch: 6| Step: 6
Training loss: 2.300006729613701
Validation loss: 2.475840979887116

Epoch: 6| Step: 7
Training loss: 2.909005120486319
Validation loss: 2.4745687327350576

Epoch: 6| Step: 8
Training loss: 2.5534569812778942
Validation loss: 2.4737133721326847

Epoch: 6| Step: 9
Training loss: 2.242840714954196
Validation loss: 2.4717010050999413

Epoch: 6| Step: 10
Training loss: 2.0606014877241394
Validation loss: 2.470242093663528

Epoch: 6| Step: 11
Training loss: 2.5403450411630346
Validation loss: 2.467015248446739

Epoch: 6| Step: 12
Training loss: 3.334728012775473
Validation loss: 2.467717586600358

Epoch: 6| Step: 13
Training loss: 2.607267522350836
Validation loss: 2.4685253974695436

Epoch: 82| Step: 0
Training loss: 2.705189880927052
Validation loss: 2.4670710266593194

Epoch: 6| Step: 1
Training loss: 2.7338028663438827
Validation loss: 2.4644524232907017

Epoch: 6| Step: 2
Training loss: 2.7163297908490067
Validation loss: 2.4656897784137453

Epoch: 6| Step: 3
Training loss: 2.6327627839508057
Validation loss: 2.470863676822976

Epoch: 6| Step: 4
Training loss: 2.0825034459556107
Validation loss: 2.469715541233276

Epoch: 6| Step: 5
Training loss: 2.538493025079822
Validation loss: 2.4633567100812863

Epoch: 6| Step: 6
Training loss: 2.4627733921159507
Validation loss: 2.465579560364364

Epoch: 6| Step: 7
Training loss: 2.380767945684743
Validation loss: 2.4629614041126175

Epoch: 6| Step: 8
Training loss: 2.2502490011822642
Validation loss: 2.47361544725839

Epoch: 6| Step: 9
Training loss: 2.8435474365527615
Validation loss: 2.4614095487427003

Epoch: 6| Step: 10
Training loss: 2.2415105881178183
Validation loss: 2.465134496211438

Epoch: 6| Step: 11
Training loss: 2.436074940717295
Validation loss: 2.4756198055601595

Epoch: 6| Step: 12
Training loss: 2.9234547959615798
Validation loss: 2.463274714796675

Epoch: 6| Step: 13
Training loss: 2.9645433034939828
Validation loss: 2.462067908574806

Epoch: 83| Step: 0
Training loss: 3.124104485946545
Validation loss: 2.468403820119245

Epoch: 6| Step: 1
Training loss: 2.623022151782221
Validation loss: 2.476749497342446

Epoch: 6| Step: 2
Training loss: 2.7118950727413997
Validation loss: 2.4781910769234647

Epoch: 6| Step: 3
Training loss: 3.1044490267623375
Validation loss: 2.4818758680460107

Epoch: 6| Step: 4
Training loss: 2.8737027102408668
Validation loss: 2.484954334583968

Epoch: 6| Step: 5
Training loss: 2.739854651723927
Validation loss: 2.482502837702886

Epoch: 6| Step: 6
Training loss: 1.998360736914614
Validation loss: 2.480727567211028

Epoch: 6| Step: 7
Training loss: 2.506352174726226
Validation loss: 2.47826634148683

Epoch: 6| Step: 8
Training loss: 1.8264612305226118
Validation loss: 2.4809621485398536

Epoch: 6| Step: 9
Training loss: 2.735578087031215
Validation loss: 2.4787796637394797

Epoch: 6| Step: 10
Training loss: 2.5126736784921757
Validation loss: 2.479580537459726

Epoch: 6| Step: 11
Training loss: 2.625397969913751
Validation loss: 2.4705561859303145

Epoch: 6| Step: 12
Training loss: 2.3246259573035895
Validation loss: 2.468212970638379

Epoch: 6| Step: 13
Training loss: 2.3879422362416367
Validation loss: 2.471167382811504

Epoch: 84| Step: 0
Training loss: 2.2563384104813546
Validation loss: 2.467805544955439

Epoch: 6| Step: 1
Training loss: 2.968518699871095
Validation loss: 2.4668323779074544

Epoch: 6| Step: 2
Training loss: 2.984317000060172
Validation loss: 2.475946664706153

Epoch: 6| Step: 3
Training loss: 2.3662163785146895
Validation loss: 2.4737188176442966

Epoch: 6| Step: 4
Training loss: 2.158099169406702
Validation loss: 2.4735335189422543

Epoch: 6| Step: 5
Training loss: 2.4689340643531845
Validation loss: 2.4688729686130295

Epoch: 6| Step: 6
Training loss: 2.510917571391679
Validation loss: 2.473880828178762

Epoch: 6| Step: 7
Training loss: 2.360917351445502
Validation loss: 2.473582949351032

Epoch: 6| Step: 8
Training loss: 2.8576579957871497
Validation loss: 2.4708923830263676

Epoch: 6| Step: 9
Training loss: 2.5910086795156513
Validation loss: 2.4731904507912055

Epoch: 6| Step: 10
Training loss: 2.4496061514539864
Validation loss: 2.478578615302509

Epoch: 6| Step: 11
Training loss: 3.137051955371974
Validation loss: 2.470770737196583

Epoch: 6| Step: 12
Training loss: 2.731257465273783
Validation loss: 2.478133897416641

Epoch: 6| Step: 13
Training loss: 2.0907580505821852
Validation loss: 2.4723422638065493

Epoch: 85| Step: 0
Training loss: 2.5396483699074244
Validation loss: 2.4721385701155807

Epoch: 6| Step: 1
Training loss: 2.5569887208417565
Validation loss: 2.472356905703398

Epoch: 6| Step: 2
Training loss: 2.7171189249608436
Validation loss: 2.4750916518635657

Epoch: 6| Step: 3
Training loss: 2.580804622918687
Validation loss: 2.4758157817775266

Epoch: 6| Step: 4
Training loss: 2.6179671763088375
Validation loss: 2.4755976228356693

Epoch: 6| Step: 5
Training loss: 1.7225363899450887
Validation loss: 2.4798864927773057

Epoch: 6| Step: 6
Training loss: 2.8831695914401863
Validation loss: 2.4731644785592204

Epoch: 6| Step: 7
Training loss: 2.3860101974793793
Validation loss: 2.472926924013902

Epoch: 6| Step: 8
Training loss: 2.2882512969416893
Validation loss: 2.4718616207128847

Epoch: 6| Step: 9
Training loss: 2.728821089513605
Validation loss: 2.4757016407590204

Epoch: 6| Step: 10
Training loss: 2.9030192476029373
Validation loss: 2.4691393420479564

Epoch: 6| Step: 11
Training loss: 2.4563272556259594
Validation loss: 2.47385665416907

Epoch: 6| Step: 12
Training loss: 2.6800043124548534
Validation loss: 2.470886979531209

Epoch: 6| Step: 13
Training loss: 2.7760137063933055
Validation loss: 2.4748035526340773

Epoch: 86| Step: 0
Training loss: 2.8325986096556024
Validation loss: 2.4611789806911504

Epoch: 6| Step: 1
Training loss: 2.8018872406330724
Validation loss: 2.461916241461377

Epoch: 6| Step: 2
Training loss: 2.4609937873337198
Validation loss: 2.463158226147225

Epoch: 6| Step: 3
Training loss: 2.611288134568976
Validation loss: 2.4634944973086625

Epoch: 6| Step: 4
Training loss: 2.402259688728732
Validation loss: 2.4666191433718927

Epoch: 6| Step: 5
Training loss: 2.475276769710798
Validation loss: 2.4697124842308726

Epoch: 6| Step: 6
Training loss: 2.320862162312238
Validation loss: 2.468166250111341

Epoch: 6| Step: 7
Training loss: 2.0610067568487307
Validation loss: 2.472284081098282

Epoch: 6| Step: 8
Training loss: 2.965751981277788
Validation loss: 2.475752400175694

Epoch: 6| Step: 9
Training loss: 2.674091156198254
Validation loss: 2.4701242605605622

Epoch: 6| Step: 10
Training loss: 1.9822074285567188
Validation loss: 2.463631857470444

Epoch: 6| Step: 11
Training loss: 2.925769678558977
Validation loss: 2.4643173985784137

Epoch: 6| Step: 12
Training loss: 2.3670257412373394
Validation loss: 2.460046079948374

Epoch: 6| Step: 13
Training loss: 2.8378133323524546
Validation loss: 2.461834908402934

Epoch: 87| Step: 0
Training loss: 2.3697329894685883
Validation loss: 2.457609867603556

Epoch: 6| Step: 1
Training loss: 2.380619828751111
Validation loss: 2.4566605313024765

Epoch: 6| Step: 2
Training loss: 2.5309101394623243
Validation loss: 2.459795603156255

Epoch: 6| Step: 3
Training loss: 2.219131436939151
Validation loss: 2.461090091233324

Epoch: 6| Step: 4
Training loss: 2.489323037132188
Validation loss: 2.4573917740992637

Epoch: 6| Step: 5
Training loss: 2.7670980722289205
Validation loss: 2.4557368068367214

Epoch: 6| Step: 6
Training loss: 2.992255705611349
Validation loss: 2.456283884243452

Epoch: 6| Step: 7
Training loss: 2.1998771373080106
Validation loss: 2.4533972538476676

Epoch: 6| Step: 8
Training loss: 2.875009122087685
Validation loss: 2.453576566759931

Epoch: 6| Step: 9
Training loss: 2.39658246698442
Validation loss: 2.4620690221968267

Epoch: 6| Step: 10
Training loss: 2.2163364016176557
Validation loss: 2.4662941898460797

Epoch: 6| Step: 11
Training loss: 2.890361877013038
Validation loss: 2.470470521052169

Epoch: 6| Step: 12
Training loss: 2.460635018302865
Validation loss: 2.469332565912384

Epoch: 6| Step: 13
Training loss: 2.762130685000351
Validation loss: 2.470609535987864

Epoch: 88| Step: 0
Training loss: 2.666227562832753
Validation loss: 2.4709910268997586

Epoch: 6| Step: 1
Training loss: 2.8607127911308825
Validation loss: 2.473703701902578

Epoch: 6| Step: 2
Training loss: 2.745408125869065
Validation loss: 2.4690036401949715

Epoch: 6| Step: 3
Training loss: 2.5576109418177366
Validation loss: 2.4703852312036707

Epoch: 6| Step: 4
Training loss: 2.6606367800314925
Validation loss: 2.465976654233047

Epoch: 6| Step: 5
Training loss: 2.2502478357089997
Validation loss: 2.4653629374831865

Epoch: 6| Step: 6
Training loss: 2.663580350230683
Validation loss: 2.4679445167922403

Epoch: 6| Step: 7
Training loss: 2.639171855099533
Validation loss: 2.46578679329528

Epoch: 6| Step: 8
Training loss: 2.008024568156675
Validation loss: 2.460960283123498

Epoch: 6| Step: 9
Training loss: 2.5518938474311836
Validation loss: 2.466701220794457

Epoch: 6| Step: 10
Training loss: 2.1966710350020198
Validation loss: 2.463708744203637

Epoch: 6| Step: 11
Training loss: 2.456556313709156
Validation loss: 2.46209442555366

Epoch: 6| Step: 12
Training loss: 2.6989071965694893
Validation loss: 2.461245449921506

Epoch: 6| Step: 13
Training loss: 2.7172754608008747
Validation loss: 2.4629673412696254

Epoch: 89| Step: 0
Training loss: 1.96968171612851
Validation loss: 2.471942270217985

Epoch: 6| Step: 1
Training loss: 2.210178635916367
Validation loss: 2.4782029904908596

Epoch: 6| Step: 2
Training loss: 2.5060411894230143
Validation loss: 2.4758631283321746

Epoch: 6| Step: 3
Training loss: 2.6074426314424306
Validation loss: 2.4768233136849997

Epoch: 6| Step: 4
Training loss: 2.724068361665301
Validation loss: 2.472125253013861

Epoch: 6| Step: 5
Training loss: 2.4290021806866533
Validation loss: 2.4665166676080124

Epoch: 6| Step: 6
Training loss: 2.382184055190265
Validation loss: 2.4676962426403217

Epoch: 6| Step: 7
Training loss: 3.102151579369828
Validation loss: 2.473533615330103

Epoch: 6| Step: 8
Training loss: 2.8930503773949425
Validation loss: 2.4593002766846683

Epoch: 6| Step: 9
Training loss: 2.660366593389784
Validation loss: 2.4628931903145097

Epoch: 6| Step: 10
Training loss: 1.9210864674669432
Validation loss: 2.465217154985776

Epoch: 6| Step: 11
Training loss: 2.9272452385450167
Validation loss: 2.4756538497167058

Epoch: 6| Step: 12
Training loss: 2.7086649667219866
Validation loss: 2.4707236710047185

Epoch: 6| Step: 13
Training loss: 2.5837549870428598
Validation loss: 2.4631390608894614

Epoch: 90| Step: 0
Training loss: 2.1268974135767897
Validation loss: 2.4625894481853283

Epoch: 6| Step: 1
Training loss: 2.5483816172298277
Validation loss: 2.46036900003909

Epoch: 6| Step: 2
Training loss: 2.7103786277203095
Validation loss: 2.45288589348942

Epoch: 6| Step: 3
Training loss: 2.2284206199621868
Validation loss: 2.4603016188229203

Epoch: 6| Step: 4
Training loss: 2.7310743192479876
Validation loss: 2.462073831747015

Epoch: 6| Step: 5
Training loss: 2.839694978156937
Validation loss: 2.4650448386116564

Epoch: 6| Step: 6
Training loss: 2.475874459271659
Validation loss: 2.473414220035681

Epoch: 6| Step: 7
Training loss: 3.201169491686117
Validation loss: 2.4794322173498786

Epoch: 6| Step: 8
Training loss: 2.351949463495998
Validation loss: 2.4805150786150882

Epoch: 6| Step: 9
Training loss: 2.700480937509791
Validation loss: 2.485150821875335

Epoch: 6| Step: 10
Training loss: 2.3335228116440985
Validation loss: 2.4876223120339023

Epoch: 6| Step: 11
Training loss: 2.577265000679602
Validation loss: 2.4810071626175683

Epoch: 6| Step: 12
Training loss: 2.1171211710882467
Validation loss: 2.481639115372978

Epoch: 6| Step: 13
Training loss: 2.628798143904641
Validation loss: 2.4774740889984566

Epoch: 91| Step: 0
Training loss: 2.330698319271832
Validation loss: 2.480524642194481

Epoch: 6| Step: 1
Training loss: 2.0261525677006897
Validation loss: 2.4744096578541304

Epoch: 6| Step: 2
Training loss: 2.340276152789127
Validation loss: 2.463194894612857

Epoch: 6| Step: 3
Training loss: 2.278162290584499
Validation loss: 2.4649042040488998

Epoch: 6| Step: 4
Training loss: 2.8894142708326216
Validation loss: 2.462817480144121

Epoch: 6| Step: 5
Training loss: 2.9742568490323955
Validation loss: 2.46375678308109

Epoch: 6| Step: 6
Training loss: 2.327913210663541
Validation loss: 2.4574700607049045

Epoch: 6| Step: 7
Training loss: 2.285689019591468
Validation loss: 2.4615874385890124

Epoch: 6| Step: 8
Training loss: 2.8944973139247137
Validation loss: 2.4675427635839173

Epoch: 6| Step: 9
Training loss: 2.380276290179426
Validation loss: 2.477477457205064

Epoch: 6| Step: 10
Training loss: 2.483186737529742
Validation loss: 2.475319262505253

Epoch: 6| Step: 11
Training loss: 2.6829349869775347
Validation loss: 2.4764928958389665

Epoch: 6| Step: 12
Training loss: 2.8450720093099893
Validation loss: 2.4755234810137954

Epoch: 6| Step: 13
Training loss: 3.092155276723841
Validation loss: 2.473081129390503

Epoch: 92| Step: 0
Training loss: 2.5517004439470647
Validation loss: 2.458725466946011

Epoch: 6| Step: 1
Training loss: 2.434072579046968
Validation loss: 2.4580152084851608

Epoch: 6| Step: 2
Training loss: 2.574951252892557
Validation loss: 2.4592991456512783

Epoch: 6| Step: 3
Training loss: 2.4760267959894957
Validation loss: 2.4615991015923666

Epoch: 6| Step: 4
Training loss: 2.047598674478491
Validation loss: 2.4632705367226677

Epoch: 6| Step: 5
Training loss: 2.794199937606613
Validation loss: 2.4669663387914813

Epoch: 6| Step: 6
Training loss: 3.0888647175337898
Validation loss: 2.469670425929454

Epoch: 6| Step: 7
Training loss: 2.4522306913847913
Validation loss: 2.4705444928334197

Epoch: 6| Step: 8
Training loss: 2.4612626118334426
Validation loss: 2.4693662300877794

Epoch: 6| Step: 9
Training loss: 2.617590392117717
Validation loss: 2.472067547423112

Epoch: 6| Step: 10
Training loss: 2.3570264027880428
Validation loss: 2.470249798867913

Epoch: 6| Step: 11
Training loss: 2.636353870152528
Validation loss: 2.4662620788268117

Epoch: 6| Step: 12
Training loss: 2.6889462682505982
Validation loss: 2.462692426208423

Epoch: 6| Step: 13
Training loss: 2.631819450029525
Validation loss: 2.4623401418909974

Epoch: 93| Step: 0
Training loss: 2.608215531277827
Validation loss: 2.4598735386434947

Epoch: 6| Step: 1
Training loss: 2.7642310126191494
Validation loss: 2.4583704563075024

Epoch: 6| Step: 2
Training loss: 2.4308358082340766
Validation loss: 2.454361775926609

Epoch: 6| Step: 3
Training loss: 2.654763018705406
Validation loss: 2.4565392968515507

Epoch: 6| Step: 4
Training loss: 2.5353574981514178
Validation loss: 2.454193821893355

Epoch: 6| Step: 5
Training loss: 2.6963318597201087
Validation loss: 2.4558165215455188

Epoch: 6| Step: 6
Training loss: 2.611419607655678
Validation loss: 2.4530425007224284

Epoch: 6| Step: 7
Training loss: 2.2304271687441446
Validation loss: 2.4513980756040574

Epoch: 6| Step: 8
Training loss: 2.601512278395694
Validation loss: 2.4518332674466743

Epoch: 6| Step: 9
Training loss: 2.312862367848784
Validation loss: 2.4537036075698655

Epoch: 6| Step: 10
Training loss: 2.2648883542803464
Validation loss: 2.4512656874011234

Epoch: 6| Step: 11
Training loss: 2.7772917279872114
Validation loss: 2.4549014225919876

Epoch: 6| Step: 12
Training loss: 2.8357177312939146
Validation loss: 2.459554277451229

Epoch: 6| Step: 13
Training loss: 2.4748182441942834
Validation loss: 2.4486394315704536

Epoch: 94| Step: 0
Training loss: 2.434088642872791
Validation loss: 2.4489563269521093

Epoch: 6| Step: 1
Training loss: 2.0964764735082024
Validation loss: 2.454387348099626

Epoch: 6| Step: 2
Training loss: 2.947133763462183
Validation loss: 2.4560075819833505

Epoch: 6| Step: 3
Training loss: 2.262866215020251
Validation loss: 2.4556879071801045

Epoch: 6| Step: 4
Training loss: 2.6079489643099913
Validation loss: 2.4606896331130113

Epoch: 6| Step: 5
Training loss: 2.4846833235674737
Validation loss: 2.4622791972303055

Epoch: 6| Step: 6
Training loss: 2.3523701139586124
Validation loss: 2.4582961182013436

Epoch: 6| Step: 7
Training loss: 3.1863257545274375
Validation loss: 2.4547461884927406

Epoch: 6| Step: 8
Training loss: 2.991458176455819
Validation loss: 2.4619871777061912

Epoch: 6| Step: 9
Training loss: 2.580542708101333
Validation loss: 2.4615830155116964

Epoch: 6| Step: 10
Training loss: 2.695988755460206
Validation loss: 2.45845725399124

Epoch: 6| Step: 11
Training loss: 2.4004873655114247
Validation loss: 2.4599591121790483

Epoch: 6| Step: 12
Training loss: 2.425334797193782
Validation loss: 2.4589384002468

Epoch: 6| Step: 13
Training loss: 2.061255802218088
Validation loss: 2.454622091275391

Epoch: 95| Step: 0
Training loss: 2.5380803013115565
Validation loss: 2.449376062170864

Epoch: 6| Step: 1
Training loss: 2.5237387840471444
Validation loss: 2.4494431355869604

Epoch: 6| Step: 2
Training loss: 2.693463827629224
Validation loss: 2.4437599981937614

Epoch: 6| Step: 3
Training loss: 1.7973574446811647
Validation loss: 2.4505010206101967

Epoch: 6| Step: 4
Training loss: 2.2498644682012867
Validation loss: 2.4541146292833838

Epoch: 6| Step: 5
Training loss: 1.9754451563894935
Validation loss: 2.4515285602243893

Epoch: 6| Step: 6
Training loss: 3.306844814280286
Validation loss: 2.4585660781167773

Epoch: 6| Step: 7
Training loss: 2.5397251614237413
Validation loss: 2.449465603837859

Epoch: 6| Step: 8
Training loss: 2.7439041198683967
Validation loss: 2.4529707388352744

Epoch: 6| Step: 9
Training loss: 2.3535863335462257
Validation loss: 2.451572485782473

Epoch: 6| Step: 10
Training loss: 2.453696506284425
Validation loss: 2.45609601632567

Epoch: 6| Step: 11
Training loss: 2.674692642522522
Validation loss: 2.4619262404482996

Epoch: 6| Step: 12
Training loss: 2.6409436998530387
Validation loss: 2.4564110762153586

Epoch: 6| Step: 13
Training loss: 2.8461052499853445
Validation loss: 2.458053255021065

Epoch: 96| Step: 0
Training loss: 2.6853484479250884
Validation loss: 2.4545346156409735

Epoch: 6| Step: 1
Training loss: 2.459109060021452
Validation loss: 2.463158161617988

Epoch: 6| Step: 2
Training loss: 2.6174888323497365
Validation loss: 2.4540694943432846

Epoch: 6| Step: 3
Training loss: 2.6067749592497216
Validation loss: 2.4622323802490955

Epoch: 6| Step: 4
Training loss: 2.4602272583110327
Validation loss: 2.4604031744664288

Epoch: 6| Step: 5
Training loss: 2.4314285369669406
Validation loss: 2.4566701068707433

Epoch: 6| Step: 6
Training loss: 2.3413314547730355
Validation loss: 2.460649132358719

Epoch: 6| Step: 7
Training loss: 2.7172391354621532
Validation loss: 2.45631420061302

Epoch: 6| Step: 8
Training loss: 2.7510153889920703
Validation loss: 2.4604905868093883

Epoch: 6| Step: 9
Training loss: 2.631224291895595
Validation loss: 2.458594475362093

Epoch: 6| Step: 10
Training loss: 2.644291245629238
Validation loss: 2.4549485574267362

Epoch: 6| Step: 11
Training loss: 2.28382696630678
Validation loss: 2.458774629413621

Epoch: 6| Step: 12
Training loss: 2.562760595701465
Validation loss: 2.457319751130003

Epoch: 6| Step: 13
Training loss: 2.5268485337314255
Validation loss: 2.4481528851321177

Epoch: 97| Step: 0
Training loss: 2.260822125004133
Validation loss: 2.4518049377549347

Epoch: 6| Step: 1
Training loss: 2.0489240102086206
Validation loss: 2.4480113450807623

Epoch: 6| Step: 2
Training loss: 2.408671956087216
Validation loss: 2.455239439616053

Epoch: 6| Step: 3
Training loss: 2.8565185954808916
Validation loss: 2.452072322651124

Epoch: 6| Step: 4
Training loss: 2.1907958134161074
Validation loss: 2.4457671176540523

Epoch: 6| Step: 5
Training loss: 2.176600084712847
Validation loss: 2.45169433861664

Epoch: 6| Step: 6
Training loss: 2.599277535345064
Validation loss: 2.460116650159783

Epoch: 6| Step: 7
Training loss: 2.867708330377278
Validation loss: 2.458333974504118

Epoch: 6| Step: 8
Training loss: 3.194706120116323
Validation loss: 2.4490510922714224

Epoch: 6| Step: 9
Training loss: 3.4293782783438402
Validation loss: 2.4581351335014183

Epoch: 6| Step: 10
Training loss: 2.0674033400462934
Validation loss: 2.461349565834062

Epoch: 6| Step: 11
Training loss: 1.9841157037929007
Validation loss: 2.46341263565548

Epoch: 6| Step: 12
Training loss: 2.3824214426801134
Validation loss: 2.468012285064919

Epoch: 6| Step: 13
Training loss: 2.9615096291844876
Validation loss: 2.4719276740903724

Epoch: 98| Step: 0
Training loss: 2.283658989657274
Validation loss: 2.4683071012818076

Epoch: 6| Step: 1
Training loss: 2.372563266533652
Validation loss: 2.470111825421941

Epoch: 6| Step: 2
Training loss: 3.044563394699355
Validation loss: 2.469954152730651

Epoch: 6| Step: 3
Training loss: 2.5183229373066793
Validation loss: 2.471692211210329

Epoch: 6| Step: 4
Training loss: 2.6230533284486417
Validation loss: 2.473060522632804

Epoch: 6| Step: 5
Training loss: 2.507416786037736
Validation loss: 2.4651019832156247

Epoch: 6| Step: 6
Training loss: 2.2759369943207832
Validation loss: 2.465539832954919

Epoch: 6| Step: 7
Training loss: 3.0349362955210597
Validation loss: 2.459461039710795

Epoch: 6| Step: 8
Training loss: 2.6884744341391866
Validation loss: 2.453700603493812

Epoch: 6| Step: 9
Training loss: 2.5254298986698687
Validation loss: 2.457383026024561

Epoch: 6| Step: 10
Training loss: 2.666709949221622
Validation loss: 2.4581411793067467

Epoch: 6| Step: 11
Training loss: 2.3395694323495038
Validation loss: 2.456627404724351

Epoch: 6| Step: 12
Training loss: 2.503920342320913
Validation loss: 2.447042537749483

Epoch: 6| Step: 13
Training loss: 2.218666289993554
Validation loss: 2.4549980059458294

Epoch: 99| Step: 0
Training loss: 2.8237483264636127
Validation loss: 2.4540226421829856

Epoch: 6| Step: 1
Training loss: 2.6325379180351
Validation loss: 2.450480078028245

Epoch: 6| Step: 2
Training loss: 2.2130961848755564
Validation loss: 2.455393396415426

Epoch: 6| Step: 3
Training loss: 2.8551497796581926
Validation loss: 2.4539819180476075

Epoch: 6| Step: 4
Training loss: 2.0469740050155676
Validation loss: 2.454332965425796

Epoch: 6| Step: 5
Training loss: 2.663724816363047
Validation loss: 2.453226269618

Epoch: 6| Step: 6
Training loss: 2.840842804568918
Validation loss: 2.4514385346838883

Epoch: 6| Step: 7
Training loss: 2.339766716047235
Validation loss: 2.45312860411685

Epoch: 6| Step: 8
Training loss: 1.8032648994214222
Validation loss: 2.455479328368596

Epoch: 6| Step: 9
Training loss: 2.5223928834460887
Validation loss: 2.4530720958148007

Epoch: 6| Step: 10
Training loss: 2.5736238895032004
Validation loss: 2.460505331532227

Epoch: 6| Step: 11
Training loss: 2.9085379177885695
Validation loss: 2.4507837320043

Epoch: 6| Step: 12
Training loss: 2.6425238108216313
Validation loss: 2.453057257816403

Epoch: 6| Step: 13
Training loss: 2.476252106189554
Validation loss: 2.455682737222365

Epoch: 100| Step: 0
Training loss: 2.2965995986772025
Validation loss: 2.453753396963216

Epoch: 6| Step: 1
Training loss: 2.52744581909667
Validation loss: 2.4592855893677372

Epoch: 6| Step: 2
Training loss: 2.075830440491149
Validation loss: 2.453488527482474

Epoch: 6| Step: 3
Training loss: 2.530196170665101
Validation loss: 2.4597451444874467

Epoch: 6| Step: 4
Training loss: 2.5005812922831376
Validation loss: 2.4471140959464712

Epoch: 6| Step: 5
Training loss: 2.6330070989667362
Validation loss: 2.4500844136442197

Epoch: 6| Step: 6
Training loss: 2.4073634358272957
Validation loss: 2.448433619895576

Epoch: 6| Step: 7
Training loss: 2.3442058882792414
Validation loss: 2.449768345449291

Epoch: 6| Step: 8
Training loss: 2.9357307462790994
Validation loss: 2.443267796154858

Epoch: 6| Step: 9
Training loss: 2.820471423630041
Validation loss: 2.4472475855457523

Epoch: 6| Step: 10
Training loss: 3.092140318446497
Validation loss: 2.444745470305917

Epoch: 6| Step: 11
Training loss: 2.6089488155422176
Validation loss: 2.4538116465989934

Epoch: 6| Step: 12
Training loss: 2.5661054228774
Validation loss: 2.4561745060132973

Epoch: 6| Step: 13
Training loss: 2.099187821052857
Validation loss: 2.4598018871952405

Epoch: 101| Step: 0
Training loss: 2.4663897470593357
Validation loss: 2.4597502735974084

Epoch: 6| Step: 1
Training loss: 2.287208926333354
Validation loss: 2.464015331796531

Epoch: 6| Step: 2
Training loss: 2.728356762946431
Validation loss: 2.469050940347461

Epoch: 6| Step: 3
Training loss: 2.824517662934463
Validation loss: 2.4675369018505937

Epoch: 6| Step: 4
Training loss: 2.6691761328793047
Validation loss: 2.4633622268802684

Epoch: 6| Step: 5
Training loss: 1.9348402533029416
Validation loss: 2.4533879732399986

Epoch: 6| Step: 6
Training loss: 2.241340184692374
Validation loss: 2.4597011306301657

Epoch: 6| Step: 7
Training loss: 2.7786767606911877
Validation loss: 2.4584860727307354

Epoch: 6| Step: 8
Training loss: 2.4377083322553115
Validation loss: 2.4512142182805143

Epoch: 6| Step: 9
Training loss: 2.2895254246403254
Validation loss: 2.4477227533248875

Epoch: 6| Step: 10
Training loss: 2.953453247557891
Validation loss: 2.452358036864503

Epoch: 6| Step: 11
Training loss: 2.5767998266253787
Validation loss: 2.4468629774218065

Epoch: 6| Step: 12
Training loss: 2.792021391997049
Validation loss: 2.455084194518775

Epoch: 6| Step: 13
Training loss: 2.4461735681007304
Validation loss: 2.4521021725285523

Epoch: 102| Step: 0
Training loss: 2.307721019835241
Validation loss: 2.451971572580506

Epoch: 6| Step: 1
Training loss: 2.9224847983429116
Validation loss: 2.4498992980320944

Epoch: 6| Step: 2
Training loss: 2.649117981578447
Validation loss: 2.443720566490203

Epoch: 6| Step: 3
Training loss: 2.241148066417998
Validation loss: 2.4549006941963007

Epoch: 6| Step: 4
Training loss: 2.792112077698736
Validation loss: 2.4548230621636162

Epoch: 6| Step: 5
Training loss: 2.4338464988382538
Validation loss: 2.4539283360383335

Epoch: 6| Step: 6
Training loss: 2.633209470258265
Validation loss: 2.451824418504889

Epoch: 6| Step: 7
Training loss: 2.6604873070316395
Validation loss: 2.4590318355570515

Epoch: 6| Step: 8
Training loss: 2.2134456359377688
Validation loss: 2.4552683446771306

Epoch: 6| Step: 9
Training loss: 2.013495096167065
Validation loss: 2.4511053757683543

Epoch: 6| Step: 10
Training loss: 1.974457294407031
Validation loss: 2.4524544287338506

Epoch: 6| Step: 11
Training loss: 2.7534861575641636
Validation loss: 2.4535292112203773

Epoch: 6| Step: 12
Training loss: 2.9833878886883882
Validation loss: 2.453781639383553

Epoch: 6| Step: 13
Training loss: 2.538089507079541
Validation loss: 2.451416684270182

Epoch: 103| Step: 0
Training loss: 2.4899207063481694
Validation loss: 2.449381391447657

Epoch: 6| Step: 1
Training loss: 2.742537256090371
Validation loss: 2.4505556747981134

Epoch: 6| Step: 2
Training loss: 2.831669562033134
Validation loss: 2.453187492206359

Epoch: 6| Step: 3
Training loss: 2.1054735059808296
Validation loss: 2.4521227204414844

Epoch: 6| Step: 4
Training loss: 2.371240853235182
Validation loss: 2.4520758148786803

Epoch: 6| Step: 5
Training loss: 2.494653133827963
Validation loss: 2.4541872806255425

Epoch: 6| Step: 6
Training loss: 2.847676008912074
Validation loss: 2.4674200185575863

Epoch: 6| Step: 7
Training loss: 2.93365128990258
Validation loss: 2.4500007739682177

Epoch: 6| Step: 8
Training loss: 2.2421366722380305
Validation loss: 2.4509491878841896

Epoch: 6| Step: 9
Training loss: 2.8060177169534133
Validation loss: 2.4518889372545623

Epoch: 6| Step: 10
Training loss: 2.568554494430074
Validation loss: 2.456285226972115

Epoch: 6| Step: 11
Training loss: 2.285803861225234
Validation loss: 2.461204659628956

Epoch: 6| Step: 12
Training loss: 2.368395909237272
Validation loss: 2.467803081359061

Epoch: 6| Step: 13
Training loss: 2.2496774230298953
Validation loss: 2.458824049470204

Epoch: 104| Step: 0
Training loss: 2.937203737293782
Validation loss: 2.462960936238223

Epoch: 6| Step: 1
Training loss: 2.86706043762222
Validation loss: 2.464086191486388

Epoch: 6| Step: 2
Training loss: 2.9584138527957955
Validation loss: 2.459481429204996

Epoch: 6| Step: 3
Training loss: 1.702805095340775
Validation loss: 2.4659848884074127

Epoch: 6| Step: 4
Training loss: 2.90210534986774
Validation loss: 2.456683661412014

Epoch: 6| Step: 5
Training loss: 2.664674034294891
Validation loss: 2.457600554397557

Epoch: 6| Step: 6
Training loss: 2.6841665486767314
Validation loss: 2.456689274070096

Epoch: 6| Step: 7
Training loss: 1.8568471185779514
Validation loss: 2.452673414906271

Epoch: 6| Step: 8
Training loss: 2.889025931490527
Validation loss: 2.4475483285810204

Epoch: 6| Step: 9
Training loss: 2.231982036791747
Validation loss: 2.45309677431751

Epoch: 6| Step: 10
Training loss: 2.0833802281505958
Validation loss: 2.460504031480808

Epoch: 6| Step: 11
Training loss: 1.9385187485570212
Validation loss: 2.457982447739236

Epoch: 6| Step: 12
Training loss: 2.3122673948683126
Validation loss: 2.4587963013134995

Epoch: 6| Step: 13
Training loss: 2.682991593281261
Validation loss: 2.4573478070279755

Epoch: 105| Step: 0
Training loss: 2.377910737700418
Validation loss: 2.451584066762531

Epoch: 6| Step: 1
Training loss: 2.540815200348939
Validation loss: 2.4566522173497574

Epoch: 6| Step: 2
Training loss: 2.7459022162156588
Validation loss: 2.46507949628695

Epoch: 6| Step: 3
Training loss: 2.2836209870109565
Validation loss: 2.4701091067351713

Epoch: 6| Step: 4
Training loss: 2.6475387443610843
Validation loss: 2.4897944521803956

Epoch: 6| Step: 5
Training loss: 3.0879207370744783
Validation loss: 2.4944033443300047

Epoch: 6| Step: 6
Training loss: 2.1415643788809153
Validation loss: 2.4561189818767275

Epoch: 6| Step: 7
Training loss: 1.9145639190962458
Validation loss: 2.4521263989534674

Epoch: 6| Step: 8
Training loss: 2.47780580328966
Validation loss: 2.4590426461585047

Epoch: 6| Step: 9
Training loss: 2.1733296963401894
Validation loss: 2.4534729550849987

Epoch: 6| Step: 10
Training loss: 3.0399840068396444
Validation loss: 2.4563610333299972

Epoch: 6| Step: 11
Training loss: 2.0730173543377535
Validation loss: 2.4621433109456565

Epoch: 6| Step: 12
Training loss: 2.9762827824401707
Validation loss: 2.4621595386154143

Epoch: 6| Step: 13
Training loss: 2.6962429928323792
Validation loss: 2.4648314171919328

Epoch: 106| Step: 0
Training loss: 2.4819875317674587
Validation loss: 2.467011108921843

Epoch: 6| Step: 1
Training loss: 2.336826933381492
Validation loss: 2.47256328132784

Epoch: 6| Step: 2
Training loss: 2.0555158330613317
Validation loss: 2.469886309031659

Epoch: 6| Step: 3
Training loss: 1.7924330499082304
Validation loss: 2.469519499272395

Epoch: 6| Step: 4
Training loss: 2.6757621820146325
Validation loss: 2.4653592545443117

Epoch: 6| Step: 5
Training loss: 2.301612690820535
Validation loss: 2.4680755756343538

Epoch: 6| Step: 6
Training loss: 2.9036617443977364
Validation loss: 2.466917814751922

Epoch: 6| Step: 7
Training loss: 2.882497548005516
Validation loss: 2.4641169359591912

Epoch: 6| Step: 8
Training loss: 2.1616305240540004
Validation loss: 2.4585055328662726

Epoch: 6| Step: 9
Training loss: 2.306660883828266
Validation loss: 2.4569849152433805

Epoch: 6| Step: 10
Training loss: 2.80062010233646
Validation loss: 2.457426248648697

Epoch: 6| Step: 11
Training loss: 2.3776066178764976
Validation loss: 2.4549159823671083

Epoch: 6| Step: 12
Training loss: 2.8553114402846687
Validation loss: 2.4494581739264345

Epoch: 6| Step: 13
Training loss: 3.2129158552390815
Validation loss: 2.453011058582873

Epoch: 107| Step: 0
Training loss: 2.118256358927368
Validation loss: 2.453068499714259

Epoch: 6| Step: 1
Training loss: 2.5998859673949224
Validation loss: 2.45627767209165

Epoch: 6| Step: 2
Training loss: 2.2262204626952267
Validation loss: 2.4657339029311096

Epoch: 6| Step: 3
Training loss: 2.5618220106918694
Validation loss: 2.466337892706607

Epoch: 6| Step: 4
Training loss: 2.7529098547917656
Validation loss: 2.4646821772622416

Epoch: 6| Step: 5
Training loss: 2.5235436951580748
Validation loss: 2.4538916505518764

Epoch: 6| Step: 6
Training loss: 2.552235958180683
Validation loss: 2.4624180291179485

Epoch: 6| Step: 7
Training loss: 2.0949284667409813
Validation loss: 2.455240799100236

Epoch: 6| Step: 8
Training loss: 2.888225322302631
Validation loss: 2.453232781053084

Epoch: 6| Step: 9
Training loss: 2.317028560319168
Validation loss: 2.4542993217155327

Epoch: 6| Step: 10
Training loss: 2.8191386392635813
Validation loss: 2.4564825192210025

Epoch: 6| Step: 11
Training loss: 2.8332579172606605
Validation loss: 2.4622066071290982

Epoch: 6| Step: 12
Training loss: 2.814700304410738
Validation loss: 2.460882761513003

Epoch: 6| Step: 13
Training loss: 2.141267554750651
Validation loss: 2.4605222967365346

Epoch: 108| Step: 0
Training loss: 2.8239715594623727
Validation loss: 2.460479612996101

Epoch: 6| Step: 1
Training loss: 2.5118893199093892
Validation loss: 2.4645958245273376

Epoch: 6| Step: 2
Training loss: 2.801286085271703
Validation loss: 2.4623506394222194

Epoch: 6| Step: 3
Training loss: 2.741466720927304
Validation loss: 2.4632757310838755

Epoch: 6| Step: 4
Training loss: 2.9800285924269465
Validation loss: 2.4564136725615815

Epoch: 6| Step: 5
Training loss: 2.581591223822572
Validation loss: 2.462969091760196

Epoch: 6| Step: 6
Training loss: 2.5591494357208973
Validation loss: 2.4586665741226748

Epoch: 6| Step: 7
Training loss: 2.123547674805737
Validation loss: 2.454660765083232

Epoch: 6| Step: 8
Training loss: 2.061229892722861
Validation loss: 2.4544882499432745

Epoch: 6| Step: 9
Training loss: 2.504977801866076
Validation loss: 2.4550706877958692

Epoch: 6| Step: 10
Training loss: 3.389690780265278
Validation loss: 2.4508470783628615

Epoch: 6| Step: 11
Training loss: 1.6799944929759274
Validation loss: 2.4502770156992746

Epoch: 6| Step: 12
Training loss: 1.962070031657333
Validation loss: 2.4590743505182906

Epoch: 6| Step: 13
Training loss: 2.0949721683665556
Validation loss: 2.4621357740402865

Epoch: 109| Step: 0
Training loss: 2.507858798826659
Validation loss: 2.456529413432421

Epoch: 6| Step: 1
Training loss: 2.1182283327544833
Validation loss: 2.456065009659804

Epoch: 6| Step: 2
Training loss: 2.8065047898203246
Validation loss: 2.454050962465608

Epoch: 6| Step: 3
Training loss: 2.535670811608587
Validation loss: 2.4490792023299854

Epoch: 6| Step: 4
Training loss: 2.923158741010567
Validation loss: 2.4556110201287447

Epoch: 6| Step: 5
Training loss: 2.2489021589912115
Validation loss: 2.458247835213502

Epoch: 6| Step: 6
Training loss: 2.5031405749651996
Validation loss: 2.458623268250051

Epoch: 6| Step: 7
Training loss: 2.2858493373346294
Validation loss: 2.463128913567186

Epoch: 6| Step: 8
Training loss: 2.446270154929426
Validation loss: 2.455084744821069

Epoch: 6| Step: 9
Training loss: 2.1415571424728124
Validation loss: 2.462165663302355

Epoch: 6| Step: 10
Training loss: 2.1030532484582576
Validation loss: 2.46345754295634

Epoch: 6| Step: 11
Training loss: 2.676554547246285
Validation loss: 2.462460824490219

Epoch: 6| Step: 12
Training loss: 2.6916932505268205
Validation loss: 2.4562311451635566

Epoch: 6| Step: 13
Training loss: 3.2236881338028027
Validation loss: 2.4581050335755132

Epoch: 110| Step: 0
Training loss: 2.8945823181012917
Validation loss: 2.461029777613314

Epoch: 6| Step: 1
Training loss: 2.947726042462415
Validation loss: 2.4511528591923

Epoch: 6| Step: 2
Training loss: 2.383237907082071
Validation loss: 2.4471431538147392

Epoch: 6| Step: 3
Training loss: 2.7103452887630515
Validation loss: 2.4537984486514386

Epoch: 6| Step: 4
Training loss: 2.6133965664953207
Validation loss: 2.4523243822635536

Epoch: 6| Step: 5
Training loss: 2.2068730930610743
Validation loss: 2.457756351929795

Epoch: 6| Step: 6
Training loss: 2.1556579219242975
Validation loss: 2.4613143713059857

Epoch: 6| Step: 7
Training loss: 2.0366205669540496
Validation loss: 2.456651748273766

Epoch: 6| Step: 8
Training loss: 2.131485187161379
Validation loss: 2.455416870216436

Epoch: 6| Step: 9
Training loss: 3.077971478811563
Validation loss: 2.455996531511502

Epoch: 6| Step: 10
Training loss: 2.385948443934155
Validation loss: 2.4542060300544715

Epoch: 6| Step: 11
Training loss: 2.5075259416327933
Validation loss: 2.452214098230658

Epoch: 6| Step: 12
Training loss: 2.655318511995592
Validation loss: 2.4561821744579864

Epoch: 6| Step: 13
Training loss: 2.0508829727375937
Validation loss: 2.4583776653193428

Epoch: 111| Step: 0
Training loss: 2.57568669468119
Validation loss: 2.456993680908333

Epoch: 6| Step: 1
Training loss: 2.4089209854893654
Validation loss: 2.4583534035159738

Epoch: 6| Step: 2
Training loss: 2.357557190153866
Validation loss: 2.4576010556300285

Epoch: 6| Step: 3
Training loss: 2.2163285487542117
Validation loss: 2.4584947441473664

Epoch: 6| Step: 4
Training loss: 2.8180735992393777
Validation loss: 2.45978412545492

Epoch: 6| Step: 5
Training loss: 1.9432653969896152
Validation loss: 2.460042736333538

Epoch: 6| Step: 6
Training loss: 3.1294056164108746
Validation loss: 2.4616089969415014

Epoch: 6| Step: 7
Training loss: 2.4841692377448465
Validation loss: 2.456167209643908

Epoch: 6| Step: 8
Training loss: 2.705978034540023
Validation loss: 2.460636423252556

Epoch: 6| Step: 9
Training loss: 2.4120712215479507
Validation loss: 2.463016144803229

Epoch: 6| Step: 10
Training loss: 2.260457215773132
Validation loss: 2.4621685198705006

Epoch: 6| Step: 11
Training loss: 2.506066971562131
Validation loss: 2.462349622753205

Epoch: 6| Step: 12
Training loss: 2.1745086728833307
Validation loss: 2.466762289779862

Epoch: 6| Step: 13
Training loss: 3.000250646928848
Validation loss: 2.458911429068858

Epoch: 112| Step: 0
Training loss: 2.7465003894116027
Validation loss: 2.469207962631495

Epoch: 6| Step: 1
Training loss: 2.2150935817608817
Validation loss: 2.4744305825821797

Epoch: 6| Step: 2
Training loss: 2.708016440607874
Validation loss: 2.4679191092481694

Epoch: 6| Step: 3
Training loss: 2.8084797418239633
Validation loss: 2.4767014457482066

Epoch: 6| Step: 4
Training loss: 2.0178014552248493
Validation loss: 2.4661200957413785

Epoch: 6| Step: 5
Training loss: 2.7268706314293234
Validation loss: 2.477093163404753

Epoch: 6| Step: 6
Training loss: 2.2932825344950456
Validation loss: 2.456726459627833

Epoch: 6| Step: 7
Training loss: 2.452126561002326
Validation loss: 2.461567776819501

Epoch: 6| Step: 8
Training loss: 2.71854496325224
Validation loss: 2.455825922419465

Epoch: 6| Step: 9
Training loss: 2.085650592174816
Validation loss: 2.4539900791213283

Epoch: 6| Step: 10
Training loss: 2.5298727550850035
Validation loss: 2.456301938220447

Epoch: 6| Step: 11
Training loss: 2.7303482487492765
Validation loss: 2.4542393674158705

Epoch: 6| Step: 12
Training loss: 2.1772622456264816
Validation loss: 2.4597197572549905

Epoch: 6| Step: 13
Training loss: 2.9410709833413975
Validation loss: 2.451504182010481

Epoch: 113| Step: 0
Training loss: 2.5099531407940536
Validation loss: 2.4613874881820137

Epoch: 6| Step: 1
Training loss: 3.017383754358954
Validation loss: 2.4533248625571065

Epoch: 6| Step: 2
Training loss: 2.292957728292825
Validation loss: 2.4505649661320645

Epoch: 6| Step: 3
Training loss: 2.682311972136936
Validation loss: 2.4515661239192026

Epoch: 6| Step: 4
Training loss: 2.313965204371279
Validation loss: 2.449882275484251

Epoch: 6| Step: 5
Training loss: 2.3882692984998535
Validation loss: 2.4549877764157335

Epoch: 6| Step: 6
Training loss: 2.596923541224748
Validation loss: 2.4537697529916733

Epoch: 6| Step: 7
Training loss: 2.3906183180372813
Validation loss: 2.449110711172285

Epoch: 6| Step: 8
Training loss: 2.5271340331809427
Validation loss: 2.4532431960737675

Epoch: 6| Step: 9
Training loss: 1.8169944549123227
Validation loss: 2.4522497312426563

Epoch: 6| Step: 10
Training loss: 2.4482637091756985
Validation loss: 2.4511545289579635

Epoch: 6| Step: 11
Training loss: 2.542389834557599
Validation loss: 2.46301631420224

Epoch: 6| Step: 12
Training loss: 2.773760021960052
Validation loss: 2.4669160429017096

Epoch: 6| Step: 13
Training loss: 2.4426014652476127
Validation loss: 2.4590123472675534

Epoch: 114| Step: 0
Training loss: 3.0257138694630132
Validation loss: 2.4641924368867847

Epoch: 6| Step: 1
Training loss: 2.92200310574157
Validation loss: 2.467108039631313

Epoch: 6| Step: 2
Training loss: 2.0875882250022317
Validation loss: 2.4470208876029713

Epoch: 6| Step: 3
Training loss: 2.2093321023146046
Validation loss: 2.46119438324243

Epoch: 6| Step: 4
Training loss: 2.31852905372381
Validation loss: 2.4580405163272245

Epoch: 6| Step: 5
Training loss: 2.4510650733081953
Validation loss: 2.452772597089888

Epoch: 6| Step: 6
Training loss: 2.830949753089341
Validation loss: 2.453082770694817

Epoch: 6| Step: 7
Training loss: 2.4726323372165195
Validation loss: 2.452610698979373

Epoch: 6| Step: 8
Training loss: 1.840200596116563
Validation loss: 2.4591916143155252

Epoch: 6| Step: 9
Training loss: 2.4290010028270097
Validation loss: 2.462578007709846

Epoch: 6| Step: 10
Training loss: 3.028312911904453
Validation loss: 2.460410603626952

Epoch: 6| Step: 11
Training loss: 2.4721467998396203
Validation loss: 2.4604802589908803

Epoch: 6| Step: 12
Training loss: 2.167212662004141
Validation loss: 2.4575754926436426

Epoch: 6| Step: 13
Training loss: 2.2865958089202345
Validation loss: 2.4573645110502573

Epoch: 115| Step: 0
Training loss: 2.2973525400751824
Validation loss: 2.4606850307881407

Epoch: 6| Step: 1
Training loss: 2.4010501511121167
Validation loss: 2.46067051322276

Epoch: 6| Step: 2
Training loss: 2.057841501922267
Validation loss: 2.4500653407408

Epoch: 6| Step: 3
Training loss: 2.756896734158485
Validation loss: 2.4637451222276456

Epoch: 6| Step: 4
Training loss: 2.4844139623885613
Validation loss: 2.4784149878962562

Epoch: 6| Step: 5
Training loss: 2.665303139971492
Validation loss: 2.492858222464641

Epoch: 6| Step: 6
Training loss: 2.285091666588703
Validation loss: 2.504643260861002

Epoch: 6| Step: 7
Training loss: 2.6305452540177026
Validation loss: 2.48420214101635

Epoch: 6| Step: 8
Training loss: 2.0465096882978284
Validation loss: 2.4841747723071474

Epoch: 6| Step: 9
Training loss: 2.569535065390334
Validation loss: 2.4645574517287074

Epoch: 6| Step: 10
Training loss: 2.979639580455822
Validation loss: 2.457801475750773

Epoch: 6| Step: 11
Training loss: 2.5923897726965253
Validation loss: 2.460979287736318

Epoch: 6| Step: 12
Training loss: 2.732781954333426
Validation loss: 2.46300638417387

Epoch: 6| Step: 13
Training loss: 2.330280100149724
Validation loss: 2.4653362783615913

Epoch: 116| Step: 0
Training loss: 2.532844135668119
Validation loss: 2.4662263099137136

Epoch: 6| Step: 1
Training loss: 3.346002835880772
Validation loss: 2.4677903527385956

Epoch: 6| Step: 2
Training loss: 2.083611151926885
Validation loss: 2.4713023789623714

Epoch: 6| Step: 3
Training loss: 2.418308117150314
Validation loss: 2.470943008055843

Epoch: 6| Step: 4
Training loss: 2.051675772172858
Validation loss: 2.470571996453377

Epoch: 6| Step: 5
Training loss: 1.9955191485070007
Validation loss: 2.4791767590314673

Epoch: 6| Step: 6
Training loss: 2.688032408030739
Validation loss: 2.4719835986536673

Epoch: 6| Step: 7
Training loss: 2.4567073250679416
Validation loss: 2.4727851307784228

Epoch: 6| Step: 8
Training loss: 2.5651511457501317
Validation loss: 2.470113771935483

Epoch: 6| Step: 9
Training loss: 3.071377091594181
Validation loss: 2.469077108739327

Epoch: 6| Step: 10
Training loss: 2.6317583911349236
Validation loss: 2.470179373370929

Epoch: 6| Step: 11
Training loss: 2.6243412462804283
Validation loss: 2.464862918124301

Epoch: 6| Step: 12
Training loss: 2.2232289841779385
Validation loss: 2.458869848596595

Epoch: 6| Step: 13
Training loss: 2.5666814044017343
Validation loss: 2.4637812739193254

Epoch: 117| Step: 0
Training loss: 1.8218059389297954
Validation loss: 2.4622726613039707

Epoch: 6| Step: 1
Training loss: 2.2480945996373145
Validation loss: 2.462771020294808

Epoch: 6| Step: 2
Training loss: 2.3687326153844452
Validation loss: 2.455329665653674

Epoch: 6| Step: 3
Training loss: 2.3443262027715384
Validation loss: 2.4565147095487068

Epoch: 6| Step: 4
Training loss: 2.808126482340194
Validation loss: 2.465562573550387

Epoch: 6| Step: 5
Training loss: 2.286990324679626
Validation loss: 2.4571009607336656

Epoch: 6| Step: 6
Training loss: 3.0149279169224394
Validation loss: 2.4499857227402493

Epoch: 6| Step: 7
Training loss: 1.97411644402201
Validation loss: 2.465676168628802

Epoch: 6| Step: 8
Training loss: 2.211283841992791
Validation loss: 2.463163227157967

Epoch: 6| Step: 9
Training loss: 2.9122110575494657
Validation loss: 2.466954330690647

Epoch: 6| Step: 10
Training loss: 3.2406753874552
Validation loss: 2.4605915290610847

Epoch: 6| Step: 11
Training loss: 2.3426510078016847
Validation loss: 2.4616028062995152

Epoch: 6| Step: 12
Training loss: 2.6453425235024652
Validation loss: 2.4620399548831284

Epoch: 6| Step: 13
Training loss: 2.514166746947688
Validation loss: 2.457712965346492

Epoch: 118| Step: 0
Training loss: 2.3631880308319486
Validation loss: 2.4632678588800014

Epoch: 6| Step: 1
Training loss: 2.7535733501971165
Validation loss: 2.4591729352150056

Epoch: 6| Step: 2
Training loss: 2.066366790221658
Validation loss: 2.45693443119713

Epoch: 6| Step: 3
Training loss: 3.2500344788116258
Validation loss: 2.461387811060288

Epoch: 6| Step: 4
Training loss: 1.7783766208136405
Validation loss: 2.457600538228766

Epoch: 6| Step: 5
Training loss: 1.8981758219899267
Validation loss: 2.4603856350984583

Epoch: 6| Step: 6
Training loss: 2.424456592513451
Validation loss: 2.459924269288459

Epoch: 6| Step: 7
Training loss: 2.8748576916843245
Validation loss: 2.4636562447117365

Epoch: 6| Step: 8
Training loss: 2.1540336828546947
Validation loss: 2.454987501253734

Epoch: 6| Step: 9
Training loss: 2.2929768602789915
Validation loss: 2.4574750005279005

Epoch: 6| Step: 10
Training loss: 2.7959339114279427
Validation loss: 2.449180485103529

Epoch: 6| Step: 11
Training loss: 2.2991382062403387
Validation loss: 2.4618903681880746

Epoch: 6| Step: 12
Training loss: 2.7504263460678313
Validation loss: 2.457344152502638

Epoch: 6| Step: 13
Training loss: 2.69405736032661
Validation loss: 2.464547261876747

Epoch: 119| Step: 0
Training loss: 2.4860268145808364
Validation loss: 2.4670294065358043

Epoch: 6| Step: 1
Training loss: 2.5853121367253347
Validation loss: 2.4592123130492567

Epoch: 6| Step: 2
Training loss: 1.799073176720873
Validation loss: 2.46320281544597

Epoch: 6| Step: 3
Training loss: 2.6321464624322912
Validation loss: 2.4564313616137454

Epoch: 6| Step: 4
Training loss: 2.5884315472416612
Validation loss: 2.467097054986517

Epoch: 6| Step: 5
Training loss: 2.8759783863003556
Validation loss: 2.4586820247364245

Epoch: 6| Step: 6
Training loss: 2.6656753266962534
Validation loss: 2.4592164414643802

Epoch: 6| Step: 7
Training loss: 2.4703750090921166
Validation loss: 2.4653532586758184

Epoch: 6| Step: 8
Training loss: 1.9331536302687988
Validation loss: 2.468499826881609

Epoch: 6| Step: 9
Training loss: 2.729143293052542
Validation loss: 2.4660576412191313

Epoch: 6| Step: 10
Training loss: 2.3899720740183144
Validation loss: 2.462120813134079

Epoch: 6| Step: 11
Training loss: 2.369612455753093
Validation loss: 2.4642430463694525

Epoch: 6| Step: 12
Training loss: 2.76824999012237
Validation loss: 2.46193410080176

Epoch: 6| Step: 13
Training loss: 2.231771165691219
Validation loss: 2.459593746114363

Epoch: 120| Step: 0
Training loss: 2.4519183842080436
Validation loss: 2.4563808500139275

Epoch: 6| Step: 1
Training loss: 1.6338110247935465
Validation loss: 2.4627302714054493

Epoch: 6| Step: 2
Training loss: 2.614007914372833
Validation loss: 2.4600369051876134

Epoch: 6| Step: 3
Training loss: 2.427726223871322
Validation loss: 2.455545450231013

Epoch: 6| Step: 4
Training loss: 2.5548176863510133
Validation loss: 2.4555846920509588

Epoch: 6| Step: 5
Training loss: 2.8980337514656016
Validation loss: 2.4594603611351817

Epoch: 6| Step: 6
Training loss: 3.1427359247982607
Validation loss: 2.462972697605515

Epoch: 6| Step: 7
Training loss: 2.9798748179519325
Validation loss: 2.4554988446982193

Epoch: 6| Step: 8
Training loss: 1.9482043728707983
Validation loss: 2.4601232402707285

Epoch: 6| Step: 9
Training loss: 2.205095857020157
Validation loss: 2.4571110278319668

Epoch: 6| Step: 10
Training loss: 2.750360551953319
Validation loss: 2.4617971705007102

Epoch: 6| Step: 11
Training loss: 2.048280655525221
Validation loss: 2.459429211081648

Epoch: 6| Step: 12
Training loss: 2.4443316457713022
Validation loss: 2.461819122473155

Epoch: 6| Step: 13
Training loss: 2.4597634073010886
Validation loss: 2.4681904718044145

Epoch: 121| Step: 0
Training loss: 2.055092891255043
Validation loss: 2.4747014478953644

Epoch: 6| Step: 1
Training loss: 2.080301851231289
Validation loss: 2.4826097754940344

Epoch: 6| Step: 2
Training loss: 2.4170008680399677
Validation loss: 2.471841381566173

Epoch: 6| Step: 3
Training loss: 2.4326081706476805
Validation loss: 2.4676820561716952

Epoch: 6| Step: 4
Training loss: 2.7127195981345045
Validation loss: 2.467163429199203

Epoch: 6| Step: 5
Training loss: 1.9414913534934162
Validation loss: 2.474675499550221

Epoch: 6| Step: 6
Training loss: 2.693618640219281
Validation loss: 2.4732039308585323

Epoch: 6| Step: 7
Training loss: 2.062292377541038
Validation loss: 2.4643172050817572

Epoch: 6| Step: 8
Training loss: 2.4543281083257535
Validation loss: 2.4629901378835286

Epoch: 6| Step: 9
Training loss: 2.6317851158996155
Validation loss: 2.469235030545451

Epoch: 6| Step: 10
Training loss: 2.324579393632277
Validation loss: 2.46546774178571

Epoch: 6| Step: 11
Training loss: 2.64642329409448
Validation loss: 2.471675668324057

Epoch: 6| Step: 12
Training loss: 3.2497361883049987
Validation loss: 2.469966138203491

Epoch: 6| Step: 13
Training loss: 2.6195209178726486
Validation loss: 2.464130675300597

Epoch: 122| Step: 0
Training loss: 2.8015685524478857
Validation loss: 2.4693669864000247

Epoch: 6| Step: 1
Training loss: 2.1840884172127466
Validation loss: 2.471858582440113

Epoch: 6| Step: 2
Training loss: 2.142988918430658
Validation loss: 2.4803613674519265

Epoch: 6| Step: 3
Training loss: 3.2593204658313084
Validation loss: 2.4993356934725623

Epoch: 6| Step: 4
Training loss: 2.856570343150368
Validation loss: 2.5037729562083775

Epoch: 6| Step: 5
Training loss: 2.5271619586920435
Validation loss: 2.5172791659877114

Epoch: 6| Step: 6
Training loss: 2.6711947818916495
Validation loss: 2.524150436674952

Epoch: 6| Step: 7
Training loss: 2.030686872625147
Validation loss: 2.5161289634341726

Epoch: 6| Step: 8
Training loss: 2.011048910971724
Validation loss: 2.499552941086422

Epoch: 6| Step: 9
Training loss: 3.0216101513595888
Validation loss: 2.4811117945695433

Epoch: 6| Step: 10
Training loss: 2.3676777448467057
Validation loss: 2.4605672244897754

Epoch: 6| Step: 11
Training loss: 1.9928950231441458
Validation loss: 2.460002822590064

Epoch: 6| Step: 12
Training loss: 2.3516314423552487
Validation loss: 2.4655332412051862

Epoch: 6| Step: 13
Training loss: 2.2528179324525506
Validation loss: 2.4563293910118076

Epoch: 123| Step: 0
Training loss: 2.345162436546834
Validation loss: 2.459646970453518

Epoch: 6| Step: 1
Training loss: 2.525799381133211
Validation loss: 2.462443961394992

Epoch: 6| Step: 2
Training loss: 2.2126404367478107
Validation loss: 2.4665638704617114

Epoch: 6| Step: 3
Training loss: 3.336979651874054
Validation loss: 2.4672648151403984

Epoch: 6| Step: 4
Training loss: 2.557374340412113
Validation loss: 2.468335104737128

Epoch: 6| Step: 5
Training loss: 2.330814114235545
Validation loss: 2.4629464401547643

Epoch: 6| Step: 6
Training loss: 2.9554963455706793
Validation loss: 2.462894174491065

Epoch: 6| Step: 7
Training loss: 2.4845582906418535
Validation loss: 2.4599046426863427

Epoch: 6| Step: 8
Training loss: 2.5062146191465966
Validation loss: 2.4542474062018877

Epoch: 6| Step: 9
Training loss: 2.3152593697244117
Validation loss: 2.4551574645655148

Epoch: 6| Step: 10
Training loss: 2.624975749312507
Validation loss: 2.4462799498489205

Epoch: 6| Step: 11
Training loss: 2.194871851454071
Validation loss: 2.447178008187328

Epoch: 6| Step: 12
Training loss: 2.85791011453795
Validation loss: 2.436694346252003

Epoch: 6| Step: 13
Training loss: 1.7654832090985422
Validation loss: 2.447962437195033

Epoch: 124| Step: 0
Training loss: 2.9487802343690297
Validation loss: 2.4538425117059677

Epoch: 6| Step: 1
Training loss: 2.1757470612776206
Validation loss: 2.456887148465845

Epoch: 6| Step: 2
Training loss: 2.3070879315971236
Validation loss: 2.4509764250180917

Epoch: 6| Step: 3
Training loss: 3.0240471592409857
Validation loss: 2.4611113633193247

Epoch: 6| Step: 4
Training loss: 2.4876001884181895
Validation loss: 2.4502715018731793

Epoch: 6| Step: 5
Training loss: 3.2789480218051197
Validation loss: 2.4529270651106443

Epoch: 6| Step: 6
Training loss: 1.8714554662030303
Validation loss: 2.455954456748919

Epoch: 6| Step: 7
Training loss: 2.9431639912133343
Validation loss: 2.461739319659199

Epoch: 6| Step: 8
Training loss: 2.489596176646369
Validation loss: 2.445572421851379

Epoch: 6| Step: 9
Training loss: 2.046067173083528
Validation loss: 2.4570764760846813

Epoch: 6| Step: 10
Training loss: 2.9591140970430403
Validation loss: 2.467001267455391

Epoch: 6| Step: 11
Training loss: 2.0413342678899307
Validation loss: 2.455707340945802

Epoch: 6| Step: 12
Training loss: 1.4653453730961257
Validation loss: 2.4546437188964374

Epoch: 6| Step: 13
Training loss: 1.9486418273539847
Validation loss: 2.4546502265714625

Epoch: 125| Step: 0
Training loss: 2.4377887261391247
Validation loss: 2.458748706920072

Epoch: 6| Step: 1
Training loss: 3.010062508171647
Validation loss: 2.4583806555983205

Epoch: 6| Step: 2
Training loss: 2.2059461778869145
Validation loss: 2.468534725798761

Epoch: 6| Step: 3
Training loss: 2.73627427760856
Validation loss: 2.4821092842474006

Epoch: 6| Step: 4
Training loss: 2.66193427939508
Validation loss: 2.484796740165139

Epoch: 6| Step: 5
Training loss: 1.835792438376555
Validation loss: 2.502828015740459

Epoch: 6| Step: 6
Training loss: 2.5468654515374114
Validation loss: 2.481956216160057

Epoch: 6| Step: 7
Training loss: 2.355833819236026
Validation loss: 2.463345515135436

Epoch: 6| Step: 8
Training loss: 2.383880075907468
Validation loss: 2.4618407675719713

Epoch: 6| Step: 9
Training loss: 2.0643277017393475
Validation loss: 2.4637236551456527

Epoch: 6| Step: 10
Training loss: 2.5315357564999923
Validation loss: 2.4549218013970484

Epoch: 6| Step: 11
Training loss: 2.3974969367842056
Validation loss: 2.4569880446863945

Epoch: 6| Step: 12
Training loss: 3.0766625972678185
Validation loss: 2.4627654537668286

Epoch: 6| Step: 13
Training loss: 2.3702704871621743
Validation loss: 2.4624419442706054

Epoch: 126| Step: 0
Training loss: 2.719870325531627
Validation loss: 2.457268675636886

Epoch: 6| Step: 1
Training loss: 1.892691184683334
Validation loss: 2.467325660810452

Epoch: 6| Step: 2
Training loss: 2.6484492028802094
Validation loss: 2.4693383911992806

Epoch: 6| Step: 3
Training loss: 3.1991053224607624
Validation loss: 2.4625436536918266

Epoch: 6| Step: 4
Training loss: 2.9103613864360907
Validation loss: 2.460647355994587

Epoch: 6| Step: 5
Training loss: 2.570890413842841
Validation loss: 2.461508217487854

Epoch: 6| Step: 6
Training loss: 2.4973081401641113
Validation loss: 2.4641011404597086

Epoch: 6| Step: 7
Training loss: 1.8519369784851978
Validation loss: 2.4547989432883095

Epoch: 6| Step: 8
Training loss: 2.6727921545850375
Validation loss: 2.461172587150988

Epoch: 6| Step: 9
Training loss: 2.5781970389734066
Validation loss: 2.4597011306301657

Epoch: 6| Step: 10
Training loss: 2.375732308974698
Validation loss: 2.4600979619305243

Epoch: 6| Step: 11
Training loss: 2.182761373173723
Validation loss: 2.462067569646265

Epoch: 6| Step: 12
Training loss: 2.1422008644761132
Validation loss: 2.4605008257610193

Epoch: 6| Step: 13
Training loss: 2.4849062177741863
Validation loss: 2.46020663272311

Epoch: 127| Step: 0
Training loss: 2.4904232656608345
Validation loss: 2.4641394558607037

Epoch: 6| Step: 1
Training loss: 2.59807162298157
Validation loss: 2.4640034302650453

Epoch: 6| Step: 2
Training loss: 2.3672418871382113
Validation loss: 2.4685376393853042

Epoch: 6| Step: 3
Training loss: 2.87164160243908
Validation loss: 2.4718490898230807

Epoch: 6| Step: 4
Training loss: 2.298339584412164
Validation loss: 2.4611912026396334

Epoch: 6| Step: 5
Training loss: 2.281186978240408
Validation loss: 2.4719656431326973

Epoch: 6| Step: 6
Training loss: 2.7878090315096893
Validation loss: 2.4632148499013016

Epoch: 6| Step: 7
Training loss: 2.1981539003122617
Validation loss: 2.467976541517935

Epoch: 6| Step: 8
Training loss: 2.814827951624631
Validation loss: 2.4687249709522456

Epoch: 6| Step: 9
Training loss: 2.036565077017488
Validation loss: 2.473876475268209

Epoch: 6| Step: 10
Training loss: 2.238469247429098
Validation loss: 2.470186596178759

Epoch: 6| Step: 11
Training loss: 2.5419811200176854
Validation loss: 2.479943559770897

Epoch: 6| Step: 12
Training loss: 2.2791784626750227
Validation loss: 2.470376038543504

Epoch: 6| Step: 13
Training loss: 2.5031801977274672
Validation loss: 2.4647348161214153

Epoch: 128| Step: 0
Training loss: 2.434831381096054
Validation loss: 2.476433494890234

Epoch: 6| Step: 1
Training loss: 2.0398372186901734
Validation loss: 2.4743199830323537

Epoch: 6| Step: 2
Training loss: 2.7828535107991472
Validation loss: 2.4760557713238605

Epoch: 6| Step: 3
Training loss: 2.778556119167083
Validation loss: 2.4654504802070534

Epoch: 6| Step: 4
Training loss: 2.8885417957950144
Validation loss: 2.4629129867083073

Epoch: 6| Step: 5
Training loss: 2.590247213429341
Validation loss: 2.4672510449353835

Epoch: 6| Step: 6
Training loss: 2.3180918744091885
Validation loss: 2.471801176065918

Epoch: 6| Step: 7
Training loss: 2.4014289416749484
Validation loss: 2.4679746577220447

Epoch: 6| Step: 8
Training loss: 2.069988412502125
Validation loss: 2.4735431737731064

Epoch: 6| Step: 9
Training loss: 2.4837176334938853
Validation loss: 2.4778145594449383

Epoch: 6| Step: 10
Training loss: 1.8433541341692161
Validation loss: 2.4750419625986027

Epoch: 6| Step: 11
Training loss: 2.5654369364310416
Validation loss: 2.4681505207755774

Epoch: 6| Step: 12
Training loss: 2.4119545829001723
Validation loss: 2.473489252425801

Epoch: 6| Step: 13
Training loss: 2.6313508727973707
Validation loss: 2.4713418127514233

Epoch: 129| Step: 0
Training loss: 2.687076357785461
Validation loss: 2.461736930700131

Epoch: 6| Step: 1
Training loss: 2.596019074074569
Validation loss: 2.474326784236333

Epoch: 6| Step: 2
Training loss: 2.051344091290289
Validation loss: 2.475678953170164

Epoch: 6| Step: 3
Training loss: 1.7648402808004582
Validation loss: 2.4701123402025313

Epoch: 6| Step: 4
Training loss: 3.031343124374955
Validation loss: 2.4703077238625046

Epoch: 6| Step: 5
Training loss: 2.27887110586599
Validation loss: 2.478342341304263

Epoch: 6| Step: 6
Training loss: 1.920288733885461
Validation loss: 2.4696264763308435

Epoch: 6| Step: 7
Training loss: 2.249614894546862
Validation loss: 2.4648560021403756

Epoch: 6| Step: 8
Training loss: 2.502990745725452
Validation loss: 2.467197493407497

Epoch: 6| Step: 9
Training loss: 2.4279211560163034
Validation loss: 2.4712278027615717

Epoch: 6| Step: 10
Training loss: 2.846173605582631
Validation loss: 2.477821054375294

Epoch: 6| Step: 11
Training loss: 2.589224210487866
Validation loss: 2.466577394798869

Epoch: 6| Step: 12
Training loss: 2.1817597413906125
Validation loss: 2.4833768362890165

Epoch: 6| Step: 13
Training loss: 2.6678200452701666
Validation loss: 2.4858873510507378

Epoch: 130| Step: 0
Training loss: 1.9119492679074301
Validation loss: 2.494376438019065

Epoch: 6| Step: 1
Training loss: 1.8584061350339796
Validation loss: 2.5074771645020553

Epoch: 6| Step: 2
Training loss: 2.5858292571402544
Validation loss: 2.518020579165353

Epoch: 6| Step: 3
Training loss: 2.7370447694784596
Validation loss: 2.524776375187553

Epoch: 6| Step: 4
Training loss: 2.29631762326613
Validation loss: 2.5329197215518726

Epoch: 6| Step: 5
Training loss: 2.3848951899692934
Validation loss: 2.5444458686856093

Epoch: 6| Step: 6
Training loss: 2.615225167290316
Validation loss: 2.5127801150657048

Epoch: 6| Step: 7
Training loss: 1.4600738583786699
Validation loss: 2.498485551522723

Epoch: 6| Step: 8
Training loss: 2.7782911356112967
Validation loss: 2.4980384601832064

Epoch: 6| Step: 9
Training loss: 2.4150567337543434
Validation loss: 2.4899055054364014

Epoch: 6| Step: 10
Training loss: 2.726178423655983
Validation loss: 2.487194213091258

Epoch: 6| Step: 11
Training loss: 2.5140441286922477
Validation loss: 2.4690328186540285

Epoch: 6| Step: 12
Training loss: 2.194583432373987
Validation loss: 2.465269911369943

Epoch: 6| Step: 13
Training loss: 3.12594376618027
Validation loss: 2.466391954288935

Epoch: 131| Step: 0
Training loss: 2.4659347900406674
Validation loss: 2.475695806368869

Epoch: 6| Step: 1
Training loss: 2.737487852165407
Validation loss: 2.472444353470652

Epoch: 6| Step: 2
Training loss: 2.8156716159767874
Validation loss: 2.470646029530096

Epoch: 6| Step: 3
Training loss: 2.1046037755001215
Validation loss: 2.4781618218975447

Epoch: 6| Step: 4
Training loss: 2.1696856324623113
Validation loss: 2.4697118889194383

Epoch: 6| Step: 5
Training loss: 2.328968688885403
Validation loss: 2.476258813828546

Epoch: 6| Step: 6
Training loss: 2.741955347912115
Validation loss: 2.46682063494244

Epoch: 6| Step: 7
Training loss: 2.1995461255834887
Validation loss: 2.464556806802617

Epoch: 6| Step: 8
Training loss: 2.3096231660178566
Validation loss: 2.4634202331766293

Epoch: 6| Step: 9
Training loss: 2.8242991160332456
Validation loss: 2.467979520166623

Epoch: 6| Step: 10
Training loss: 2.1895031068322055
Validation loss: 2.4632041382710144

Epoch: 6| Step: 11
Training loss: 2.6939951455427242
Validation loss: 2.4752883441332174

Epoch: 6| Step: 12
Training loss: 2.786385413430844
Validation loss: 2.488797136821876

Epoch: 6| Step: 13
Training loss: 2.7314337906442727
Validation loss: 2.4895570878649904

Epoch: 132| Step: 0
Training loss: 2.00285755103826
Validation loss: 2.485049294188142

Epoch: 6| Step: 1
Training loss: 2.802593501468355
Validation loss: 2.4799011221781067

Epoch: 6| Step: 2
Training loss: 1.5556459012740518
Validation loss: 2.478580988028864

Epoch: 6| Step: 3
Training loss: 2.832568476792788
Validation loss: 2.477421560421886

Epoch: 6| Step: 4
Training loss: 1.7490023085075659
Validation loss: 2.481707470389538

Epoch: 6| Step: 5
Training loss: 2.897994262036737
Validation loss: 2.489301646961457

Epoch: 6| Step: 6
Training loss: 2.4511329678187055
Validation loss: 2.485354216548599

Epoch: 6| Step: 7
Training loss: 2.6032179871948835
Validation loss: 2.4857237171869544

Epoch: 6| Step: 8
Training loss: 2.3503622749778206
Validation loss: 2.47363408155543

Epoch: 6| Step: 9
Training loss: 2.5235819583324695
Validation loss: 2.4826332880652027

Epoch: 6| Step: 10
Training loss: 2.5724083608129313
Validation loss: 2.473363565258468

Epoch: 6| Step: 11
Training loss: 2.7814027551427927
Validation loss: 2.467956077299219

Epoch: 6| Step: 12
Training loss: 2.750140793404025
Validation loss: 2.4707829277811113

Epoch: 6| Step: 13
Training loss: 2.1567309299349753
Validation loss: 2.463631720372175

Epoch: 133| Step: 0
Training loss: 2.5654042231439864
Validation loss: 2.462803297841661

Epoch: 6| Step: 1
Training loss: 2.415138770118741
Validation loss: 2.4624072091294513

Epoch: 6| Step: 2
Training loss: 2.2274131889852256
Validation loss: 2.465471029691752

Epoch: 6| Step: 3
Training loss: 2.4039107011785448
Validation loss: 2.459279077800923

Epoch: 6| Step: 4
Training loss: 1.922976542567319
Validation loss: 2.456707349329946

Epoch: 6| Step: 5
Training loss: 2.7986306588684227
Validation loss: 2.462941495171886

Epoch: 6| Step: 6
Training loss: 2.8428626719720205
Validation loss: 2.464202700770622

Epoch: 6| Step: 7
Training loss: 2.2743787357376166
Validation loss: 2.4653498416689708

Epoch: 6| Step: 8
Training loss: 2.224868504785491
Validation loss: 2.4749713363015173

Epoch: 6| Step: 9
Training loss: 3.0799632513653052
Validation loss: 2.4897786520044654

Epoch: 6| Step: 10
Training loss: 2.410901911059377
Validation loss: 2.4948654535540555

Epoch: 6| Step: 11
Training loss: 2.471919572237849
Validation loss: 2.4862189817083933

Epoch: 6| Step: 12
Training loss: 2.461238394613498
Validation loss: 2.472916431207386

Epoch: 6| Step: 13
Training loss: 2.544907353931375
Validation loss: 2.4710205839157338

Epoch: 134| Step: 0
Training loss: 2.3616182892052415
Validation loss: 2.462514228908785

Epoch: 6| Step: 1
Training loss: 2.198999316039766
Validation loss: 2.466764472513028

Epoch: 6| Step: 2
Training loss: 2.559514236854741
Validation loss: 2.465801248508195

Epoch: 6| Step: 3
Training loss: 2.659193046611917
Validation loss: 2.4642449007675893

Epoch: 6| Step: 4
Training loss: 2.1714125765304595
Validation loss: 2.4757246330980403

Epoch: 6| Step: 5
Training loss: 2.181487403345385
Validation loss: 2.4773874202597663

Epoch: 6| Step: 6
Training loss: 2.7239379493043074
Validation loss: 2.4727281235568688

Epoch: 6| Step: 7
Training loss: 2.059427573845529
Validation loss: 2.4801915612121626

Epoch: 6| Step: 8
Training loss: 2.593548042983478
Validation loss: 2.4795937424073036

Epoch: 6| Step: 9
Training loss: 2.6029322228827603
Validation loss: 2.480910598769843

Epoch: 6| Step: 10
Training loss: 2.196384154788892
Validation loss: 2.4760610431773706

Epoch: 6| Step: 11
Training loss: 2.6795422928951993
Validation loss: 2.4814789404574027

Epoch: 6| Step: 12
Training loss: 2.933533283123503
Validation loss: 2.4840412355543715

Epoch: 6| Step: 13
Training loss: 2.7657107754784143
Validation loss: 2.4801896866940316

Epoch: 135| Step: 0
Training loss: 2.3943231439314783
Validation loss: 2.475441407167419

Epoch: 6| Step: 1
Training loss: 2.188566765209325
Validation loss: 2.47711552523353

Epoch: 6| Step: 2
Training loss: 2.248732103633862
Validation loss: 2.4750397871653402

Epoch: 6| Step: 3
Training loss: 2.2280836831819117
Validation loss: 2.4858415861266647

Epoch: 6| Step: 4
Training loss: 2.5248590486673126
Validation loss: 2.4864787025933106

Epoch: 6| Step: 5
Training loss: 2.862060286552732
Validation loss: 2.483576712320404

Epoch: 6| Step: 6
Training loss: 2.8468486964730584
Validation loss: 2.479885915931694

Epoch: 6| Step: 7
Training loss: 2.5755994969239078
Validation loss: 2.4700895932328466

Epoch: 6| Step: 8
Training loss: 2.570381882493453
Validation loss: 2.483167038753799

Epoch: 6| Step: 9
Training loss: 2.285448678371165
Validation loss: 2.479675422419505

Epoch: 6| Step: 10
Training loss: 2.47616063657185
Validation loss: 2.479517941205321

Epoch: 6| Step: 11
Training loss: 2.3331149089849457
Validation loss: 2.4878254567309517

Epoch: 6| Step: 12
Training loss: 2.6436621901626918
Validation loss: 2.484787712764614

Epoch: 6| Step: 13
Training loss: 2.2961346737179538
Validation loss: 2.4842630996934556

Epoch: 136| Step: 0
Training loss: 2.330546815583254
Validation loss: 2.4788011767679405

Epoch: 6| Step: 1
Training loss: 2.863472092996222
Validation loss: 2.4789374483287068

Epoch: 6| Step: 2
Training loss: 2.556946202165353
Validation loss: 2.4894277026533786

Epoch: 6| Step: 3
Training loss: 1.5493493375695717
Validation loss: 2.479114945991479

Epoch: 6| Step: 4
Training loss: 2.918631972350549
Validation loss: 2.484915028864872

Epoch: 6| Step: 5
Training loss: 1.8344676525625465
Validation loss: 2.4864055564551943

Epoch: 6| Step: 6
Training loss: 2.51144194078785
Validation loss: 2.4871983509775224

Epoch: 6| Step: 7
Training loss: 2.3256521725419144
Validation loss: 2.4881661077554935

Epoch: 6| Step: 8
Training loss: 1.857083038005958
Validation loss: 2.4878412054225314

Epoch: 6| Step: 9
Training loss: 2.5014212383197947
Validation loss: 2.4774644655257747

Epoch: 6| Step: 10
Training loss: 3.3090243549113514
Validation loss: 2.482945916872617

Epoch: 6| Step: 11
Training loss: 2.5864030586038163
Validation loss: 2.478981657631515

Epoch: 6| Step: 12
Training loss: 2.266327847607622
Validation loss: 2.4719844023892468

Epoch: 6| Step: 13
Training loss: 2.1840444246461943
Validation loss: 2.4801932594923226

Epoch: 137| Step: 0
Training loss: 2.2362726862669833
Validation loss: 2.484209051127865

Epoch: 6| Step: 1
Training loss: 2.2641733715008177
Validation loss: 2.489043609553843

Epoch: 6| Step: 2
Training loss: 3.5303697417275806
Validation loss: 2.4783029146856714

Epoch: 6| Step: 3
Training loss: 2.029729420711359
Validation loss: 2.4771900525109403

Epoch: 6| Step: 4
Training loss: 1.944792773820766
Validation loss: 2.486272275366838

Epoch: 6| Step: 5
Training loss: 2.0091631313415754
Validation loss: 2.487592025792595

Epoch: 6| Step: 6
Training loss: 2.660021090889975
Validation loss: 2.486514355960992

Epoch: 6| Step: 7
Training loss: 2.2497310477682615
Validation loss: 2.4816707312032062

Epoch: 6| Step: 8
Training loss: 2.9254533204491495
Validation loss: 2.4910935060584367

Epoch: 6| Step: 9
Training loss: 2.180469003675303
Validation loss: 2.4866360785552124

Epoch: 6| Step: 10
Training loss: 2.469712275067412
Validation loss: 2.489002308952882

Epoch: 6| Step: 11
Training loss: 2.3355148086094704
Validation loss: 2.4965148315895744

Epoch: 6| Step: 12
Training loss: 2.1237142824578887
Validation loss: 2.485440295720766

Epoch: 6| Step: 13
Training loss: 2.529769087495411
Validation loss: 2.4954542157612534

Epoch: 138| Step: 0
Training loss: 2.6265773575343836
Validation loss: 2.489686945165061

Epoch: 6| Step: 1
Training loss: 2.756490590500682
Validation loss: 2.4981124109773827

Epoch: 6| Step: 2
Training loss: 2.377458404273415
Validation loss: 2.4846470361996382

Epoch: 6| Step: 3
Training loss: 2.4139354882268087
Validation loss: 2.480539956660229

Epoch: 6| Step: 4
Training loss: 2.6165779884360165
Validation loss: 2.4870504612048965

Epoch: 6| Step: 5
Training loss: 2.3808717922922704
Validation loss: 2.4842743603297452

Epoch: 6| Step: 6
Training loss: 2.1930178124104045
Validation loss: 2.481641349069826

Epoch: 6| Step: 7
Training loss: 2.3597817986042537
Validation loss: 2.47462271084864

Epoch: 6| Step: 8
Training loss: 2.353576000911258
Validation loss: 2.481695301444935

Epoch: 6| Step: 9
Training loss: 2.3927849878685183
Validation loss: 2.477064841874908

Epoch: 6| Step: 10
Training loss: 2.506364350790031
Validation loss: 2.480266765070132

Epoch: 6| Step: 11
Training loss: 2.1648139612907875
Validation loss: 2.469937968228055

Epoch: 6| Step: 12
Training loss: 2.7225750729077074
Validation loss: 2.484286404675419

Epoch: 6| Step: 13
Training loss: 2.226568710586185
Validation loss: 2.4902176840229036

Epoch: 139| Step: 0
Training loss: 2.9379004347558433
Validation loss: 2.502647865276442

Epoch: 6| Step: 1
Training loss: 2.539751728119231
Validation loss: 2.4948349048369067

Epoch: 6| Step: 2
Training loss: 2.435108772527278
Validation loss: 2.494447024650155

Epoch: 6| Step: 3
Training loss: 2.6982534976464008
Validation loss: 2.499340447208228

Epoch: 6| Step: 4
Training loss: 2.467930911390843
Validation loss: 2.498157513683417

Epoch: 6| Step: 5
Training loss: 2.6850967307399145
Validation loss: 2.5057359220521667

Epoch: 6| Step: 6
Training loss: 2.5943589931831434
Validation loss: 2.516732834794546

Epoch: 6| Step: 7
Training loss: 2.1749397444873915
Validation loss: 2.4960876050862515

Epoch: 6| Step: 8
Training loss: 2.3702787352881063
Validation loss: 2.496058655334416

Epoch: 6| Step: 9
Training loss: 2.2788173298558974
Validation loss: 2.493493864821036

Epoch: 6| Step: 10
Training loss: 2.1558276882424585
Validation loss: 2.485007679349827

Epoch: 6| Step: 11
Training loss: 1.9611876430773403
Validation loss: 2.4907116481206457

Epoch: 6| Step: 12
Training loss: 2.508560877097222
Validation loss: 2.4867564529371524

Epoch: 6| Step: 13
Training loss: 2.142221899333752
Validation loss: 2.4826522228057772

Epoch: 140| Step: 0
Training loss: 1.4508721721138818
Validation loss: 2.483863402019046

Epoch: 6| Step: 1
Training loss: 2.512047824689859
Validation loss: 2.480694073232552

Epoch: 6| Step: 2
Training loss: 2.093746356106904
Validation loss: 2.48659711902441

Epoch: 6| Step: 3
Training loss: 2.087256996493048
Validation loss: 2.486381200567017

Epoch: 6| Step: 4
Training loss: 2.261452351167891
Validation loss: 2.4902488638263143

Epoch: 6| Step: 5
Training loss: 2.3981297199201825
Validation loss: 2.482158816025892

Epoch: 6| Step: 6
Training loss: 2.2104659908830504
Validation loss: 2.4969939437882394

Epoch: 6| Step: 7
Training loss: 2.754724172779722
Validation loss: 2.49994015622039

Epoch: 6| Step: 8
Training loss: 2.389353694596273
Validation loss: 2.513031936120261

Epoch: 6| Step: 9
Training loss: 2.546254556358729
Validation loss: 2.502912771599689

Epoch: 6| Step: 10
Training loss: 3.2189636159584434
Validation loss: 2.501064201188851

Epoch: 6| Step: 11
Training loss: 2.0241977044222077
Validation loss: 2.500286530922942

Epoch: 6| Step: 12
Training loss: 2.7800581875207175
Validation loss: 2.4901180145404163

Epoch: 6| Step: 13
Training loss: 2.786990125065813
Validation loss: 2.493394437855446

Epoch: 141| Step: 0
Training loss: 1.9217810258917796
Validation loss: 2.5028701995436258

Epoch: 6| Step: 1
Training loss: 2.173354159744891
Validation loss: 2.4821789390291142

Epoch: 6| Step: 2
Training loss: 2.1869307731064245
Validation loss: 2.4802755606003606

Epoch: 6| Step: 3
Training loss: 2.456109630660298
Validation loss: 2.489428564605239

Epoch: 6| Step: 4
Training loss: 2.250948811854429
Validation loss: 2.4942973582291916

Epoch: 6| Step: 5
Training loss: 2.532478222918105
Validation loss: 2.4906642168851207

Epoch: 6| Step: 6
Training loss: 2.2057095781617715
Validation loss: 2.486336163919603

Epoch: 6| Step: 7
Training loss: 2.0511251106808497
Validation loss: 2.495788197329505

Epoch: 6| Step: 8
Training loss: 2.3216368110235943
Validation loss: 2.4996055927696093

Epoch: 6| Step: 9
Training loss: 2.618991150598188
Validation loss: 2.498635682720348

Epoch: 6| Step: 10
Training loss: 2.9149271727898207
Validation loss: 2.5027577766224223

Epoch: 6| Step: 11
Training loss: 2.745762855269761
Validation loss: 2.522302078835158

Epoch: 6| Step: 12
Training loss: 2.9197474603009064
Validation loss: 2.506656931108311

Epoch: 6| Step: 13
Training loss: 2.4327753690165785
Validation loss: 2.4832076042998605

Epoch: 142| Step: 0
Training loss: 3.180194786152045
Validation loss: 2.497335380854109

Epoch: 6| Step: 1
Training loss: 2.151360427354052
Validation loss: 2.494459354405162

Epoch: 6| Step: 2
Training loss: 2.2739989121702036
Validation loss: 2.486515906095816

Epoch: 6| Step: 3
Training loss: 2.548296198386183
Validation loss: 2.5004704668507594

Epoch: 6| Step: 4
Training loss: 2.324149507404757
Validation loss: 2.4931915078481053

Epoch: 6| Step: 5
Training loss: 2.344016606743999
Validation loss: 2.4892298289168138

Epoch: 6| Step: 6
Training loss: 2.379483257904118
Validation loss: 2.479288215756314

Epoch: 6| Step: 7
Training loss: 2.0615130288255705
Validation loss: 2.4852255558829848

Epoch: 6| Step: 8
Training loss: 2.028868115007598
Validation loss: 2.480399015152257

Epoch: 6| Step: 9
Training loss: 2.7869548794885746
Validation loss: 2.479569952617839

Epoch: 6| Step: 10
Training loss: 2.666915216386307
Validation loss: 2.481157806887345

Epoch: 6| Step: 11
Training loss: 2.617069526404749
Validation loss: 2.4747599912439755

Epoch: 6| Step: 12
Training loss: 2.4301525787049116
Validation loss: 2.488065349927785

Epoch: 6| Step: 13
Training loss: 2.394523085413793
Validation loss: 2.4909837739585203

Epoch: 143| Step: 0
Training loss: 2.5952712044826636
Validation loss: 2.4998808196588023

Epoch: 6| Step: 1
Training loss: 2.4516556342227016
Validation loss: 2.5250196349132796

Epoch: 6| Step: 2
Training loss: 2.893747323943316
Validation loss: 2.517192052374934

Epoch: 6| Step: 3
Training loss: 2.709643404116476
Validation loss: 2.51826060189026

Epoch: 6| Step: 4
Training loss: 2.0459657935660562
Validation loss: 2.5101149260286157

Epoch: 6| Step: 5
Training loss: 2.641317242959297
Validation loss: 2.5001888362773923

Epoch: 6| Step: 6
Training loss: 1.9375038146935104
Validation loss: 2.4807866410933688

Epoch: 6| Step: 7
Training loss: 2.4590271331729765
Validation loss: 2.4777672261235333

Epoch: 6| Step: 8
Training loss: 1.9672765516224084
Validation loss: 2.4818758280194015

Epoch: 6| Step: 9
Training loss: 2.6057894622294535
Validation loss: 2.4784922096422957

Epoch: 6| Step: 10
Training loss: 2.376727630275633
Validation loss: 2.480846370277922

Epoch: 6| Step: 11
Training loss: 2.476342705922198
Validation loss: 2.48103197166408

Epoch: 6| Step: 12
Training loss: 2.642147099543055
Validation loss: 2.481746290525199

Epoch: 6| Step: 13
Training loss: 1.9068219937009383
Validation loss: 2.480698221961263

Epoch: 144| Step: 0
Training loss: 2.084629977757453
Validation loss: 2.4942953350026422

Epoch: 6| Step: 1
Training loss: 2.3765071303158116
Validation loss: 2.4920023946071934

Epoch: 6| Step: 2
Training loss: 2.530760164313747
Validation loss: 2.508164964350062

Epoch: 6| Step: 3
Training loss: 2.437571402261991
Validation loss: 2.522590526560683

Epoch: 6| Step: 4
Training loss: 2.5942503664814733
Validation loss: 2.5458392199892037

Epoch: 6| Step: 5
Training loss: 2.1467711488795493
Validation loss: 2.5634269278428934

Epoch: 6| Step: 6
Training loss: 2.418380874737859
Validation loss: 2.5523753771122863

Epoch: 6| Step: 7
Training loss: 2.831224629202172
Validation loss: 2.534347287554517

Epoch: 6| Step: 8
Training loss: 2.38674452248002
Validation loss: 2.517165697486016

Epoch: 6| Step: 9
Training loss: 2.4305810787737907
Validation loss: 2.504045693833731

Epoch: 6| Step: 10
Training loss: 2.3986416708064584
Validation loss: 2.4939522548868394

Epoch: 6| Step: 11
Training loss: 2.110664595124382
Validation loss: 2.481267669256093

Epoch: 6| Step: 12
Training loss: 2.531142997834828
Validation loss: 2.4753539529084243

Epoch: 6| Step: 13
Training loss: 2.3411251374130133
Validation loss: 2.4813748044031883

Epoch: 145| Step: 0
Training loss: 2.0718145715523106
Validation loss: 2.474669895571145

Epoch: 6| Step: 1
Training loss: 2.1006660676691205
Validation loss: 2.482790027514306

Epoch: 6| Step: 2
Training loss: 2.2390205799978276
Validation loss: 2.4866163751444317

Epoch: 6| Step: 3
Training loss: 2.3433597494113303
Validation loss: 2.483689507495072

Epoch: 6| Step: 4
Training loss: 2.657059310589938
Validation loss: 2.4836700046957

Epoch: 6| Step: 5
Training loss: 2.6391217168078303
Validation loss: 2.4960742565621636

Epoch: 6| Step: 6
Training loss: 3.0519140585002047
Validation loss: 2.505590212789732

Epoch: 6| Step: 7
Training loss: 2.6191001166412082
Validation loss: 2.506350256356613

Epoch: 6| Step: 8
Training loss: 2.050107311138277
Validation loss: 2.520546657770857

Epoch: 6| Step: 9
Training loss: 2.3413478494004454
Validation loss: 2.5284077734938903

Epoch: 6| Step: 10
Training loss: 1.968216960135937
Validation loss: 2.543837985204978

Epoch: 6| Step: 11
Training loss: 3.101082791145548
Validation loss: 2.5342532892631375

Epoch: 6| Step: 12
Training loss: 1.8723382494423135
Validation loss: 2.518862881588133

Epoch: 6| Step: 13
Training loss: 2.4369046388923747
Validation loss: 2.507991130801007

Epoch: 146| Step: 0
Training loss: 2.6832021013791905
Validation loss: 2.493514278810816

Epoch: 6| Step: 1
Training loss: 2.4227009011059293
Validation loss: 2.499053585203068

Epoch: 6| Step: 2
Training loss: 2.3357749040336127
Validation loss: 2.489742933661458

Epoch: 6| Step: 3
Training loss: 2.6296497217593773
Validation loss: 2.4898740340674212

Epoch: 6| Step: 4
Training loss: 2.420623849897557
Validation loss: 2.4903738344323774

Epoch: 6| Step: 5
Training loss: 2.3574504961174236
Validation loss: 2.482239146976986

Epoch: 6| Step: 6
Training loss: 2.0988947730147114
Validation loss: 2.4859894439174437

Epoch: 6| Step: 7
Training loss: 2.6037692771340866
Validation loss: 2.4832521536271863

Epoch: 6| Step: 8
Training loss: 2.4122985518554065
Validation loss: 2.486480428541307

Epoch: 6| Step: 9
Training loss: 2.3134444086779906
Validation loss: 2.4877470472076335

Epoch: 6| Step: 10
Training loss: 2.291952866687104
Validation loss: 2.483993389003104

Epoch: 6| Step: 11
Training loss: 2.214479662052695
Validation loss: 2.494661751206919

Epoch: 6| Step: 12
Training loss: 2.1762183142496605
Validation loss: 2.4951809053192058

Epoch: 6| Step: 13
Training loss: 2.901568343855602
Validation loss: 2.5145060892153372

Epoch: 147| Step: 0
Training loss: 2.489656221182035
Validation loss: 2.5137886626869577

Epoch: 6| Step: 1
Training loss: 2.4631944751790287
Validation loss: 2.525788777595124

Epoch: 6| Step: 2
Training loss: 2.244902133805068
Validation loss: 2.531825404444568

Epoch: 6| Step: 3
Training loss: 2.02444680275322
Validation loss: 2.508151038464285

Epoch: 6| Step: 4
Training loss: 1.931399061258907
Validation loss: 2.503545265936922

Epoch: 6| Step: 5
Training loss: 2.918190167668189
Validation loss: 2.4970597301861135

Epoch: 6| Step: 6
Training loss: 2.996108869492082
Validation loss: 2.4997598850652363

Epoch: 6| Step: 7
Training loss: 1.9978733080580107
Validation loss: 2.50681881811451

Epoch: 6| Step: 8
Training loss: 2.343765055290187
Validation loss: 2.502258472734327

Epoch: 6| Step: 9
Training loss: 2.6811073034057196
Validation loss: 2.5031887222541935

Epoch: 6| Step: 10
Training loss: 2.7799599044982064
Validation loss: 2.49100564419579

Epoch: 6| Step: 11
Training loss: 1.8027274977861778
Validation loss: 2.497543638202616

Epoch: 6| Step: 12
Training loss: 2.406756062982724
Validation loss: 2.4929562203237765

Epoch: 6| Step: 13
Training loss: 2.193290457997319
Validation loss: 2.4932190325831005

Epoch: 148| Step: 0
Training loss: 2.1348958442528536
Validation loss: 2.5118828102427644

Epoch: 6| Step: 1
Training loss: 2.125935965462753
Validation loss: 2.4969329617185907

Epoch: 6| Step: 2
Training loss: 2.064380251086374
Validation loss: 2.492889974957999

Epoch: 6| Step: 3
Training loss: 2.6058181917266867
Validation loss: 2.5055131521910656

Epoch: 6| Step: 4
Training loss: 2.365294653378781
Validation loss: 2.5195831845473085

Epoch: 6| Step: 5
Training loss: 2.6471094273112157
Validation loss: 2.511027476817828

Epoch: 6| Step: 6
Training loss: 2.0338181912948117
Validation loss: 2.5079558619702262

Epoch: 6| Step: 7
Training loss: 2.2242738974849106
Validation loss: 2.5071487895309033

Epoch: 6| Step: 8
Training loss: 2.4920682011574185
Validation loss: 2.5096120349480544

Epoch: 6| Step: 9
Training loss: 2.616556939985126
Validation loss: 2.502315863212653

Epoch: 6| Step: 10
Training loss: 2.778001924584136
Validation loss: 2.506570938502617

Epoch: 6| Step: 11
Training loss: 1.8361731195423294
Validation loss: 2.524038174434734

Epoch: 6| Step: 12
Training loss: 2.5603595260646426
Validation loss: 2.5408274302086133

Epoch: 6| Step: 13
Training loss: 2.7127268050340048
Validation loss: 2.546390339113358

Epoch: 149| Step: 0
Training loss: 3.095093868724078
Validation loss: 2.582287243316994

Epoch: 6| Step: 1
Training loss: 2.4665627749790997
Validation loss: 2.5380396421035507

Epoch: 6| Step: 2
Training loss: 2.5572064314944085
Validation loss: 2.5194815224168496

Epoch: 6| Step: 3
Training loss: 2.6127576089546913
Validation loss: 2.515463790308922

Epoch: 6| Step: 4
Training loss: 2.4140231922492554
Validation loss: 2.4927184716186015

Epoch: 6| Step: 5
Training loss: 2.0412709637679103
Validation loss: 2.494341183781032

Epoch: 6| Step: 6
Training loss: 2.2728860192777853
Validation loss: 2.4917593720795255

Epoch: 6| Step: 7
Training loss: 2.7358087758838563
Validation loss: 2.4869207298776432

Epoch: 6| Step: 8
Training loss: 1.8353521184868664
Validation loss: 2.4892684120181023

Epoch: 6| Step: 9
Training loss: 2.3232971928912973
Validation loss: 2.5005768746154176

Epoch: 6| Step: 10
Training loss: 2.703157105007141
Validation loss: 2.5040338000711784

Epoch: 6| Step: 11
Training loss: 2.340085025122952
Validation loss: 2.5209149486686417

Epoch: 6| Step: 12
Training loss: 2.0561906661079377
Validation loss: 2.520172242630981

Epoch: 6| Step: 13
Training loss: 2.2978973189571503
Validation loss: 2.5338258705767798

Epoch: 150| Step: 0
Training loss: 2.13011317207666
Validation loss: 2.552980885354553

Epoch: 6| Step: 1
Training loss: 2.5622625473538547
Validation loss: 2.5672160844532965

Epoch: 6| Step: 2
Training loss: 2.0913117288712493
Validation loss: 2.5762267088820523

Epoch: 6| Step: 3
Training loss: 1.8448887557320661
Validation loss: 2.543826988236128

Epoch: 6| Step: 4
Training loss: 2.2324639530838675
Validation loss: 2.54807496617523

Epoch: 6| Step: 5
Training loss: 2.767411511668359
Validation loss: 2.5310868654154133

Epoch: 6| Step: 6
Training loss: 2.406136646325602
Validation loss: 2.519532432851588

Epoch: 6| Step: 7
Training loss: 2.521186604192153
Validation loss: 2.5124977608412955

Epoch: 6| Step: 8
Training loss: 2.3357906231585903
Validation loss: 2.495067275929771

Epoch: 6| Step: 9
Training loss: 2.6064709247289755
Validation loss: 2.497218539747164

Epoch: 6| Step: 10
Training loss: 2.437570815402609
Validation loss: 2.4943996007221223

Epoch: 6| Step: 11
Training loss: 2.5409595151976614
Validation loss: 2.492318607337667

Epoch: 6| Step: 12
Training loss: 2.8598000465867397
Validation loss: 2.497849699962205

Epoch: 6| Step: 13
Training loss: 2.0746681315671034
Validation loss: 2.501958254933713

Epoch: 151| Step: 0
Training loss: 2.2999015123593303
Validation loss: 2.5268598090240504

Epoch: 6| Step: 1
Training loss: 2.6889596568096197
Validation loss: 2.5318706049928457

Epoch: 6| Step: 2
Training loss: 2.413010648929197
Validation loss: 2.5451185882808844

Epoch: 6| Step: 3
Training loss: 1.8029969459165074
Validation loss: 2.549833627023033

Epoch: 6| Step: 4
Training loss: 2.150946466465198
Validation loss: 2.5721634188202085

Epoch: 6| Step: 5
Training loss: 2.2000131476616303
Validation loss: 2.547128879242451

Epoch: 6| Step: 6
Training loss: 2.733442223713693
Validation loss: 2.5426462089036974

Epoch: 6| Step: 7
Training loss: 2.2400418425807254
Validation loss: 2.5344243337625025

Epoch: 6| Step: 8
Training loss: 2.100895204925476
Validation loss: 2.5143886394922554

Epoch: 6| Step: 9
Training loss: 2.6678152193768536
Validation loss: 2.5019138798649614

Epoch: 6| Step: 10
Training loss: 2.437042975984805
Validation loss: 2.4938951180794002

Epoch: 6| Step: 11
Training loss: 2.9331478587669526
Validation loss: 2.490395885540596

Epoch: 6| Step: 12
Training loss: 1.9829800726600595
Validation loss: 2.4973601549853184

Epoch: 6| Step: 13
Training loss: 2.8408208160557304
Validation loss: 2.4939656546011535

Epoch: 152| Step: 0
Training loss: 1.7395816353734423
Validation loss: 2.4978080199864965

Epoch: 6| Step: 1
Training loss: 2.4521209216957307
Validation loss: 2.504621446245788

Epoch: 6| Step: 2
Training loss: 2.050770321771478
Validation loss: 2.5027035082505384

Epoch: 6| Step: 3
Training loss: 1.9887004178122558
Validation loss: 2.5230718294578938

Epoch: 6| Step: 4
Training loss: 2.307820921761779
Validation loss: 2.5439785204066574

Epoch: 6| Step: 5
Training loss: 2.82961676439146
Validation loss: 2.550381062901079

Epoch: 6| Step: 6
Training loss: 2.4929450626616743
Validation loss: 2.533312392984813

Epoch: 6| Step: 7
Training loss: 2.216222478584892
Validation loss: 2.5411804442887136

Epoch: 6| Step: 8
Training loss: 2.610901437506926
Validation loss: 2.529179185169123

Epoch: 6| Step: 9
Training loss: 2.2139608786554463
Validation loss: 2.5337331859932184

Epoch: 6| Step: 10
Training loss: 2.944469543765912
Validation loss: 2.518195077428035

Epoch: 6| Step: 11
Training loss: 2.6160509061264126
Validation loss: 2.505573877853575

Epoch: 6| Step: 12
Training loss: 2.1173810694419655
Validation loss: 2.4889042192745743

Epoch: 6| Step: 13
Training loss: 2.7654656833084443
Validation loss: 2.4956652413274947

Epoch: 153| Step: 0
Training loss: 2.7176453549588824
Validation loss: 2.4934375302664202

Epoch: 6| Step: 1
Training loss: 2.3642173741572168
Validation loss: 2.4865797324576464

Epoch: 6| Step: 2
Training loss: 2.3582022069954833
Validation loss: 2.4839833988596305

Epoch: 6| Step: 3
Training loss: 2.626414145161013
Validation loss: 2.487373166389108

Epoch: 6| Step: 4
Training loss: 2.6571160194987082
Validation loss: 2.4872967156136565

Epoch: 6| Step: 5
Training loss: 2.2346096315768196
Validation loss: 2.491630946469999

Epoch: 6| Step: 6
Training loss: 2.169445847453718
Validation loss: 2.4832760761581287

Epoch: 6| Step: 7
Training loss: 1.7073570656329187
Validation loss: 2.491586881867785

Epoch: 6| Step: 8
Training loss: 2.7344557396003863
Validation loss: 2.501962463695003

Epoch: 6| Step: 9
Training loss: 2.079259118398982
Validation loss: 2.51820792209273

Epoch: 6| Step: 10
Training loss: 2.4021289760447315
Validation loss: 2.5319612547715793

Epoch: 6| Step: 11
Training loss: 2.6028041683104433
Validation loss: 2.5350504786013244

Epoch: 6| Step: 12
Training loss: 2.6371191102295866
Validation loss: 2.5122520153589143

Epoch: 6| Step: 13
Training loss: 2.605494188816498
Validation loss: 2.5006220282146296

Epoch: 154| Step: 0
Training loss: 2.266749872288627
Validation loss: 2.496459074553234

Epoch: 6| Step: 1
Training loss: 2.681645248197945
Validation loss: 2.4990043882891433

Epoch: 6| Step: 2
Training loss: 2.250423179673073
Validation loss: 2.5003941543125903

Epoch: 6| Step: 3
Training loss: 2.59906738500949
Validation loss: 2.4980510585373086

Epoch: 6| Step: 4
Training loss: 2.294851852322691
Validation loss: 2.5069302346862115

Epoch: 6| Step: 5
Training loss: 2.2318967934267904
Validation loss: 2.516506861882436

Epoch: 6| Step: 6
Training loss: 2.5250966195231515
Validation loss: 2.5175118488183195

Epoch: 6| Step: 7
Training loss: 2.516677162535888
Validation loss: 2.515919117290038

Epoch: 6| Step: 8
Training loss: 2.4201682691160507
Validation loss: 2.5163031507103475

Epoch: 6| Step: 9
Training loss: 2.103246531723175
Validation loss: 2.503751641874621

Epoch: 6| Step: 10
Training loss: 2.352545345894992
Validation loss: 2.5305073830055216

Epoch: 6| Step: 11
Training loss: 2.468760623184719
Validation loss: 2.522311231911646

Epoch: 6| Step: 12
Training loss: 2.6567865166459534
Validation loss: 2.5129078314217947

Epoch: 6| Step: 13
Training loss: 2.0317456154300153
Validation loss: 2.51039718419874

Epoch: 155| Step: 0
Training loss: 1.5301082792176413
Validation loss: 2.510491023143022

Epoch: 6| Step: 1
Training loss: 2.114611194846725
Validation loss: 2.5150998754116496

Epoch: 6| Step: 2
Training loss: 1.9226015779921106
Validation loss: 2.514764832207128

Epoch: 6| Step: 3
Training loss: 2.5373114078888754
Validation loss: 2.517953604474008

Epoch: 6| Step: 4
Training loss: 1.7314230739415404
Validation loss: 2.5231365107022636

Epoch: 6| Step: 5
Training loss: 2.5718407073579495
Validation loss: 2.5307931055969384

Epoch: 6| Step: 6
Training loss: 1.876421071995492
Validation loss: 2.5210132271968058

Epoch: 6| Step: 7
Training loss: 1.8126199616820624
Validation loss: 2.542412778645485

Epoch: 6| Step: 8
Training loss: 3.5896340060887066
Validation loss: 2.5324082099168885

Epoch: 6| Step: 9
Training loss: 2.270930110257431
Validation loss: 2.5202892336290783

Epoch: 6| Step: 10
Training loss: 2.498018433123367
Validation loss: 2.50831507066772

Epoch: 6| Step: 11
Training loss: 2.088909532172194
Validation loss: 2.511333256036514

Epoch: 6| Step: 12
Training loss: 2.9907171954669534
Validation loss: 2.5070202488121436

Epoch: 6| Step: 13
Training loss: 2.8415240284610106
Validation loss: 2.514863746091917

Epoch: 156| Step: 0
Training loss: 2.6813156475575863
Validation loss: 2.5184276125225824

Epoch: 6| Step: 1
Training loss: 2.4516591351477905
Validation loss: 2.517275440614495

Epoch: 6| Step: 2
Training loss: 2.57645227903223
Validation loss: 2.52045565263847

Epoch: 6| Step: 3
Training loss: 2.2504933134334335
Validation loss: 2.5156715183419354

Epoch: 6| Step: 4
Training loss: 2.5979502117211153
Validation loss: 2.50476232564858

Epoch: 6| Step: 5
Training loss: 2.036148503012086
Validation loss: 2.507521282649596

Epoch: 6| Step: 6
Training loss: 1.92301634106212
Validation loss: 2.5206591385694734

Epoch: 6| Step: 7
Training loss: 1.8688829457635436
Validation loss: 2.5294559698747383

Epoch: 6| Step: 8
Training loss: 2.618921417234285
Validation loss: 2.524025327953408

Epoch: 6| Step: 9
Training loss: 2.291827202000213
Validation loss: 2.524946267591781

Epoch: 6| Step: 10
Training loss: 2.0741200109281683
Validation loss: 2.5194703718218476

Epoch: 6| Step: 11
Training loss: 2.554649796047266
Validation loss: 2.535156485112301

Epoch: 6| Step: 12
Training loss: 2.4643491318245005
Validation loss: 2.5279626339145764

Epoch: 6| Step: 13
Training loss: 2.6681929531813275
Validation loss: 2.5616894502273944

Epoch: 157| Step: 0
Training loss: 2.168256750824011
Validation loss: 2.555162562569318

Epoch: 6| Step: 1
Training loss: 2.38988737806619
Validation loss: 2.5416258251056454

Epoch: 6| Step: 2
Training loss: 2.5299507857048087
Validation loss: 2.5451290019977972

Epoch: 6| Step: 3
Training loss: 2.311318172854398
Validation loss: 2.5512502491296862

Epoch: 6| Step: 4
Training loss: 2.490278990102788
Validation loss: 2.5644757167405015

Epoch: 6| Step: 5
Training loss: 2.986409876660606
Validation loss: 2.560007402712331

Epoch: 6| Step: 6
Training loss: 2.51533061135343
Validation loss: 2.5298804985692938

Epoch: 6| Step: 7
Training loss: 2.6396294697917297
Validation loss: 2.5129728849642996

Epoch: 6| Step: 8
Training loss: 2.4598746774897045
Validation loss: 2.511966888842764

Epoch: 6| Step: 9
Training loss: 2.2072410281248835
Validation loss: 2.5261850585090135

Epoch: 6| Step: 10
Training loss: 2.0860089350551996
Validation loss: 2.497529987209911

Epoch: 6| Step: 11
Training loss: 2.4595140972815264
Validation loss: 2.500650464314172

Epoch: 6| Step: 12
Training loss: 1.5402192546770825
Validation loss: 2.491288169342579

Epoch: 6| Step: 13
Training loss: 2.2431096430327484
Validation loss: 2.493152539188876

Epoch: 158| Step: 0
Training loss: 2.503324967872807
Validation loss: 2.5015470645260747

Epoch: 6| Step: 1
Training loss: 2.0430046939058384
Validation loss: 2.5075099560548515

Epoch: 6| Step: 2
Training loss: 1.877962632864776
Validation loss: 2.5258910353800945

Epoch: 6| Step: 3
Training loss: 2.5138298880138215
Validation loss: 2.5341019595849064

Epoch: 6| Step: 4
Training loss: 2.705314411075847
Validation loss: 2.529476612043013

Epoch: 6| Step: 5
Training loss: 2.2103478821584517
Validation loss: 2.5332853352450164

Epoch: 6| Step: 6
Training loss: 2.396342801091492
Validation loss: 2.5301176136306625

Epoch: 6| Step: 7
Training loss: 2.0637838818121526
Validation loss: 2.5143703388395635

Epoch: 6| Step: 8
Training loss: 2.436946023442681
Validation loss: 2.517241880436041

Epoch: 6| Step: 9
Training loss: 2.7383459412773448
Validation loss: 2.5027833464933713

Epoch: 6| Step: 10
Training loss: 2.5022685248971417
Validation loss: 2.5072060363710658

Epoch: 6| Step: 11
Training loss: 2.8580329121749446
Validation loss: 2.4968316823601944

Epoch: 6| Step: 12
Training loss: 2.2582756016332746
Validation loss: 2.50965506264354

Epoch: 6| Step: 13
Training loss: 2.2027870081285505
Validation loss: 2.503583850626359

Epoch: 159| Step: 0
Training loss: 2.3148163200656096
Validation loss: 2.5176247966535144

Epoch: 6| Step: 1
Training loss: 2.5763209649038066
Validation loss: 2.5331873210648315

Epoch: 6| Step: 2
Training loss: 1.879801261421054
Validation loss: 2.5372977202771554

Epoch: 6| Step: 3
Training loss: 2.224181927080376
Validation loss: 2.5412611611829976

Epoch: 6| Step: 4
Training loss: 2.902370365477728
Validation loss: 2.5303313468176873

Epoch: 6| Step: 5
Training loss: 1.9823056943326605
Validation loss: 2.5201596760242175

Epoch: 6| Step: 6
Training loss: 2.2906338763097547
Validation loss: 2.5299436549872163

Epoch: 6| Step: 7
Training loss: 2.4161690714900614
Validation loss: 2.516920589327472

Epoch: 6| Step: 8
Training loss: 2.6033670850935593
Validation loss: 2.5086969893524786

Epoch: 6| Step: 9
Training loss: 2.0691478980456703
Validation loss: 2.5195923159589375

Epoch: 6| Step: 10
Training loss: 2.23776041998135
Validation loss: 2.5256050492842084

Epoch: 6| Step: 11
Training loss: 2.85175044864341
Validation loss: 2.553119080753383

Epoch: 6| Step: 12
Training loss: 2.3672422900012395
Validation loss: 2.5487270992909687

Epoch: 6| Step: 13
Training loss: 2.356007174417812
Validation loss: 2.56418183812449

Epoch: 160| Step: 0
Training loss: 2.048626798005758
Validation loss: 2.5544591212485916

Epoch: 6| Step: 1
Training loss: 2.3281892089182086
Validation loss: 2.5605689735729595

Epoch: 6| Step: 2
Training loss: 2.498809912661277
Validation loss: 2.549907260031141

Epoch: 6| Step: 3
Training loss: 2.6832937103410446
Validation loss: 2.54001191215275

Epoch: 6| Step: 4
Training loss: 2.755862835531137
Validation loss: 2.53682766104801

Epoch: 6| Step: 5
Training loss: 2.6726237357803737
Validation loss: 2.534673126534878

Epoch: 6| Step: 6
Training loss: 2.2273945642386317
Validation loss: 2.542380566212701

Epoch: 6| Step: 7
Training loss: 2.464501213539337
Validation loss: 2.5255804263464783

Epoch: 6| Step: 8
Training loss: 2.814214649990637
Validation loss: 2.5373584369577564

Epoch: 6| Step: 9
Training loss: 1.788293731632562
Validation loss: 2.5200974569952863

Epoch: 6| Step: 10
Training loss: 1.8063958079479492
Validation loss: 2.504706260403064

Epoch: 6| Step: 11
Training loss: 2.595784320235912
Validation loss: 2.5070072834267685

Epoch: 6| Step: 12
Training loss: 1.9306377588543913
Validation loss: 2.508907867188479

Epoch: 6| Step: 13
Training loss: 2.1850681411278847
Validation loss: 2.4981042667941353

Epoch: 161| Step: 0
Training loss: 2.0583118339785145
Validation loss: 2.4946498206687964

Epoch: 6| Step: 1
Training loss: 1.9493963432060895
Validation loss: 2.5111300192273927

Epoch: 6| Step: 2
Training loss: 2.1077551379362314
Validation loss: 2.500847696591241

Epoch: 6| Step: 3
Training loss: 2.23340441557464
Validation loss: 2.499146617831458

Epoch: 6| Step: 4
Training loss: 2.4878099794714066
Validation loss: 2.515163592559909

Epoch: 6| Step: 5
Training loss: 2.7506456484168034
Validation loss: 2.533234363952506

Epoch: 6| Step: 6
Training loss: 1.7597128535242677
Validation loss: 2.5363467673035625

Epoch: 6| Step: 7
Training loss: 2.699109661807238
Validation loss: 2.5303019094196455

Epoch: 6| Step: 8
Training loss: 2.7052640885424335
Validation loss: 2.5197883257949547

Epoch: 6| Step: 9
Training loss: 2.496699061300291
Validation loss: 2.5257983113420694

Epoch: 6| Step: 10
Training loss: 1.8732318806695325
Validation loss: 2.533510007679032

Epoch: 6| Step: 11
Training loss: 2.20515986410172
Validation loss: 2.5652172599427936

Epoch: 6| Step: 12
Training loss: 2.5060296777549493
Validation loss: 2.569163149537302

Epoch: 6| Step: 13
Training loss: 2.8090291116902804
Validation loss: 2.5627146840299164

Epoch: 162| Step: 0
Training loss: 2.4197985207412303
Validation loss: 2.54650252101156

Epoch: 6| Step: 1
Training loss: 2.401267503776875
Validation loss: 2.531775871250393

Epoch: 6| Step: 2
Training loss: 2.495943020133556
Validation loss: 2.5149411047832837

Epoch: 6| Step: 3
Training loss: 2.1824850089311
Validation loss: 2.4917994469143334

Epoch: 6| Step: 4
Training loss: 2.71867519582683
Validation loss: 2.4774146634573597

Epoch: 6| Step: 5
Training loss: 2.4588240979524403
Validation loss: 2.477622029147426

Epoch: 6| Step: 6
Training loss: 2.54943765535677
Validation loss: 2.478006850284717

Epoch: 6| Step: 7
Training loss: 1.9932706394138913
Validation loss: 2.4795805614979227

Epoch: 6| Step: 8
Training loss: 1.9513463580955919
Validation loss: 2.483723153063776

Epoch: 6| Step: 9
Training loss: 2.6848912551993784
Validation loss: 2.471788524299818

Epoch: 6| Step: 10
Training loss: 2.506611569167689
Validation loss: 2.4800052560217343

Epoch: 6| Step: 11
Training loss: 1.4621845165922576
Validation loss: 2.483041577407285

Epoch: 6| Step: 12
Training loss: 3.2696585681695858
Validation loss: 2.5074466030385043

Epoch: 6| Step: 13
Training loss: 2.050834960234161
Validation loss: 2.4986787483523063

Epoch: 163| Step: 0
Training loss: 2.2736394733494905
Validation loss: 2.503741436958112

Epoch: 6| Step: 1
Training loss: 2.1111241752934147
Validation loss: 2.518918016507708

Epoch: 6| Step: 2
Training loss: 2.3706993516586174
Validation loss: 2.565146513975013

Epoch: 6| Step: 3
Training loss: 3.0768832882729433
Validation loss: 2.57318792316084

Epoch: 6| Step: 4
Training loss: 2.552091159938307
Validation loss: 2.605737110903307

Epoch: 6| Step: 5
Training loss: 2.1349733466242378
Validation loss: 2.6265051326201694

Epoch: 6| Step: 6
Training loss: 2.8007453267069464
Validation loss: 2.590889667263769

Epoch: 6| Step: 7
Training loss: 1.9887838571520913
Validation loss: 2.5473120534770586

Epoch: 6| Step: 8
Training loss: 2.485890819751465
Validation loss: 2.5189675816914168

Epoch: 6| Step: 9
Training loss: 2.243644639625647
Validation loss: 2.497704819282692

Epoch: 6| Step: 10
Training loss: 1.978128529043801
Validation loss: 2.483359651191411

Epoch: 6| Step: 11
Training loss: 2.6031352836341646
Validation loss: 2.4748248513538575

Epoch: 6| Step: 12
Training loss: 1.861986138035099
Validation loss: 2.48449250909018

Epoch: 6| Step: 13
Training loss: 3.4068355232041254
Validation loss: 2.4791575332815294

Epoch: 164| Step: 0
Training loss: 2.805097714658393
Validation loss: 2.483055212034103

Epoch: 6| Step: 1
Training loss: 2.197872962939001
Validation loss: 2.485782264707507

Epoch: 6| Step: 2
Training loss: 2.290865763960501
Validation loss: 2.4870140565884746

Epoch: 6| Step: 3
Training loss: 2.076903364164045
Validation loss: 2.4914799306633957

Epoch: 6| Step: 4
Training loss: 2.49381683565986
Validation loss: 2.4966080543180955

Epoch: 6| Step: 5
Training loss: 2.680289064731642
Validation loss: 2.4924936295877185

Epoch: 6| Step: 6
Training loss: 2.383850771969848
Validation loss: 2.49302467945735

Epoch: 6| Step: 7
Training loss: 2.6940681570560314
Validation loss: 2.511896565156415

Epoch: 6| Step: 8
Training loss: 2.7991490842224045
Validation loss: 2.5066124410631545

Epoch: 6| Step: 9
Training loss: 2.392793855867242
Validation loss: 2.5213861936350024

Epoch: 6| Step: 10
Training loss: 1.79318388934279
Validation loss: 2.529633889151535

Epoch: 6| Step: 11
Training loss: 2.3316065211397454
Validation loss: 2.5449952910054328

Epoch: 6| Step: 12
Training loss: 2.3745260267632617
Validation loss: 2.5431953481571052

Epoch: 6| Step: 13
Training loss: 2.489286450250872
Validation loss: 2.540738723292949

Epoch: 165| Step: 0
Training loss: 2.134360733096248
Validation loss: 2.5370244380759783

Epoch: 6| Step: 1
Training loss: 2.3494297634357726
Validation loss: 2.5360893017612316

Epoch: 6| Step: 2
Training loss: 2.267025639587986
Validation loss: 2.5287818020932655

Epoch: 6| Step: 3
Training loss: 1.9352916314422663
Validation loss: 2.521662336708414

Epoch: 6| Step: 4
Training loss: 2.5475559377471826
Validation loss: 2.524728875664598

Epoch: 6| Step: 5
Training loss: 1.8359481486559535
Validation loss: 2.5321613915870826

Epoch: 6| Step: 6
Training loss: 2.8110467335043645
Validation loss: 2.50791376370931

Epoch: 6| Step: 7
Training loss: 2.4060734275337654
Validation loss: 2.5047711303481637

Epoch: 6| Step: 8
Training loss: 2.3090427433252354
Validation loss: 2.5107011963564405

Epoch: 6| Step: 9
Training loss: 1.9148513102761553
Validation loss: 2.519248059348392

Epoch: 6| Step: 10
Training loss: 2.0728273484301933
Validation loss: 2.520158225420657

Epoch: 6| Step: 11
Training loss: 2.654107879354927
Validation loss: 2.512484578580327

Epoch: 6| Step: 12
Training loss: 3.1974317615854004
Validation loss: 2.488956170246524

Epoch: 6| Step: 13
Training loss: 2.3119041087516465
Validation loss: 2.4949124226715336

Epoch: 166| Step: 0
Training loss: 2.5881730756829113
Validation loss: 2.497137751816719

Epoch: 6| Step: 1
Training loss: 2.173039186070794
Validation loss: 2.5005430426495576

Epoch: 6| Step: 2
Training loss: 1.85966995848926
Validation loss: 2.5035249178686114

Epoch: 6| Step: 3
Training loss: 2.2645528426891572
Validation loss: 2.508737268800463

Epoch: 6| Step: 4
Training loss: 2.400083095383594
Validation loss: 2.488549921131988

Epoch: 6| Step: 5
Training loss: 2.794885705784485
Validation loss: 2.4909925795038386

Epoch: 6| Step: 6
Training loss: 2.0246981077862793
Validation loss: 2.50062681923142

Epoch: 6| Step: 7
Training loss: 2.3763696836845813
Validation loss: 2.50046328384567

Epoch: 6| Step: 8
Training loss: 2.2416819358031486
Validation loss: 2.5045339954953048

Epoch: 6| Step: 9
Training loss: 2.1658643802160764
Validation loss: 2.4980601254952606

Epoch: 6| Step: 10
Training loss: 3.0396301210152212
Validation loss: 2.4990350132433474

Epoch: 6| Step: 11
Training loss: 1.6508534160678148
Validation loss: 2.5258281709305095

Epoch: 6| Step: 12
Training loss: 2.506520541612397
Validation loss: 2.54700945477827

Epoch: 6| Step: 13
Training loss: 2.6439842210549864
Validation loss: 2.5231966311171035

Epoch: 167| Step: 0
Training loss: 3.082321946198529
Validation loss: 2.5402736963659107

Epoch: 6| Step: 1
Training loss: 2.007743863962435
Validation loss: 2.53643603514179

Epoch: 6| Step: 2
Training loss: 2.258619329307008
Validation loss: 2.5262731596803563

Epoch: 6| Step: 3
Training loss: 2.2625070097588127
Validation loss: 2.5172846277537317

Epoch: 6| Step: 4
Training loss: 2.489779753232099
Validation loss: 2.4970829316517684

Epoch: 6| Step: 5
Training loss: 2.5454056134412992
Validation loss: 2.498593618267979

Epoch: 6| Step: 6
Training loss: 1.5754249060145542
Validation loss: 2.4997418429281058

Epoch: 6| Step: 7
Training loss: 1.8520946140467833
Validation loss: 2.4903044407779915

Epoch: 6| Step: 8
Training loss: 2.274745918589728
Validation loss: 2.504173482757618

Epoch: 6| Step: 9
Training loss: 2.596985327331434
Validation loss: 2.5057333213080817

Epoch: 6| Step: 10
Training loss: 2.4113631974658594
Validation loss: 2.486722225211149

Epoch: 6| Step: 11
Training loss: 2.2779422974550227
Validation loss: 2.5076208625689476

Epoch: 6| Step: 12
Training loss: 2.291992395544835
Validation loss: 2.5008583741002677

Epoch: 6| Step: 13
Training loss: 2.780931947503043
Validation loss: 2.5024029547985895

Epoch: 168| Step: 0
Training loss: 2.3118897741237046
Validation loss: 2.5063162010880076

Epoch: 6| Step: 1
Training loss: 2.725505285050442
Validation loss: 2.5255518697314656

Epoch: 6| Step: 2
Training loss: 2.4821999103226773
Validation loss: 2.513565831885679

Epoch: 6| Step: 3
Training loss: 2.7101287075038654
Validation loss: 2.493728982108641

Epoch: 6| Step: 4
Training loss: 2.5412040247701113
Validation loss: 2.476921464675291

Epoch: 6| Step: 5
Training loss: 1.8553572129305025
Validation loss: 2.492058905110946

Epoch: 6| Step: 6
Training loss: 2.099290605343818
Validation loss: 2.498706308058894

Epoch: 6| Step: 7
Training loss: 1.9493971381806228
Validation loss: 2.494897530897195

Epoch: 6| Step: 8
Training loss: 2.50131343671508
Validation loss: 2.507918691317955

Epoch: 6| Step: 9
Training loss: 2.5902822822302096
Validation loss: 2.5164185530101135

Epoch: 6| Step: 10
Training loss: 2.6745289788936013
Validation loss: 2.509256408726803

Epoch: 6| Step: 11
Training loss: 1.6447747202075318
Validation loss: 2.53723592151366

Epoch: 6| Step: 12
Training loss: 2.277725318625078
Validation loss: 2.5424197180972232

Epoch: 6| Step: 13
Training loss: 2.4162731727242783
Validation loss: 2.5421569591048345

Epoch: 169| Step: 0
Training loss: 1.8567079281060197
Validation loss: 2.558059799739079

Epoch: 6| Step: 1
Training loss: 2.122137947251094
Validation loss: 2.569704712467534

Epoch: 6| Step: 2
Training loss: 2.8613877846809386
Validation loss: 2.555944415467622

Epoch: 6| Step: 3
Training loss: 2.469271254738932
Validation loss: 2.5596651508217354

Epoch: 6| Step: 4
Training loss: 1.8748748101720683
Validation loss: 2.538148451384205

Epoch: 6| Step: 5
Training loss: 2.1931982754032826
Validation loss: 2.5099739433547072

Epoch: 6| Step: 6
Training loss: 2.3499642957856373
Validation loss: 2.5229195140903404

Epoch: 6| Step: 7
Training loss: 2.3358113436617076
Validation loss: 2.518063187019224

Epoch: 6| Step: 8
Training loss: 2.568387409245764
Validation loss: 2.52374736509269

Epoch: 6| Step: 9
Training loss: 2.656911072092426
Validation loss: 2.525957422064248

Epoch: 6| Step: 10
Training loss: 2.1458898246761033
Validation loss: 2.5300783184931546

Epoch: 6| Step: 11
Training loss: 2.3290681912572606
Validation loss: 2.548865696811602

Epoch: 6| Step: 12
Training loss: 2.9050083584427204
Validation loss: 2.548514433322645

Epoch: 6| Step: 13
Training loss: 2.2885354121983217
Validation loss: 2.5569942765010847

Epoch: 170| Step: 0
Training loss: 1.5558628259419895
Validation loss: 2.5659621196496905

Epoch: 6| Step: 1
Training loss: 2.6414798955345438
Validation loss: 2.5517699431987384

Epoch: 6| Step: 2
Training loss: 2.954997281558986
Validation loss: 2.560111809303006

Epoch: 6| Step: 3
Training loss: 2.0296435532720083
Validation loss: 2.5475069287885654

Epoch: 6| Step: 4
Training loss: 2.745623226742657
Validation loss: 2.543582105914772

Epoch: 6| Step: 5
Training loss: 2.672731139676252
Validation loss: 2.5442979876898777

Epoch: 6| Step: 6
Training loss: 1.9915371300724356
Validation loss: 2.5097610493382008

Epoch: 6| Step: 7
Training loss: 2.163625514671339
Validation loss: 2.4975519035401392

Epoch: 6| Step: 8
Training loss: 2.733539213785085
Validation loss: 2.5106161890373064

Epoch: 6| Step: 9
Training loss: 2.6950541054526416
Validation loss: 2.4878491915363528

Epoch: 6| Step: 10
Training loss: 1.763984187831638
Validation loss: 2.4958372506655953

Epoch: 6| Step: 11
Training loss: 2.087085193834512
Validation loss: 2.4947365108968476

Epoch: 6| Step: 12
Training loss: 2.520150325923572
Validation loss: 2.5007093535820455

Epoch: 6| Step: 13
Training loss: 2.274503267994879
Validation loss: 2.5042034494509355

Epoch: 171| Step: 0
Training loss: 2.440146354601002
Validation loss: 2.513350854790659

Epoch: 6| Step: 1
Training loss: 2.2679836229341923
Validation loss: 2.510240197739428

Epoch: 6| Step: 2
Training loss: 2.5315997976982
Validation loss: 2.541562880809916

Epoch: 6| Step: 3
Training loss: 2.3272314212937304
Validation loss: 2.5785910291144436

Epoch: 6| Step: 4
Training loss: 2.3793321550828974
Validation loss: 2.569480583578281

Epoch: 6| Step: 5
Training loss: 2.551996149143913
Validation loss: 2.559600507763558

Epoch: 6| Step: 6
Training loss: 1.7510326608414521
Validation loss: 2.538951648346418

Epoch: 6| Step: 7
Training loss: 2.510745890913984
Validation loss: 2.549859363754251

Epoch: 6| Step: 8
Training loss: 2.0701229368551637
Validation loss: 2.5530097110512298

Epoch: 6| Step: 9
Training loss: 2.284581925691288
Validation loss: 2.5498414033924592

Epoch: 6| Step: 10
Training loss: 2.7294573350022184
Validation loss: 2.56533118984499

Epoch: 6| Step: 11
Training loss: 2.023352661710841
Validation loss: 2.547906881102025

Epoch: 6| Step: 12
Training loss: 1.9377321904013283
Validation loss: 2.541166855702377

Epoch: 6| Step: 13
Training loss: 2.942553778896695
Validation loss: 2.5290617726762243

Epoch: 172| Step: 0
Training loss: 2.5869181176827407
Validation loss: 2.5161866376554722

Epoch: 6| Step: 1
Training loss: 2.5815423684419816
Validation loss: 2.5233806371433287

Epoch: 6| Step: 2
Training loss: 1.5937007821655078
Validation loss: 2.533273335633126

Epoch: 6| Step: 3
Training loss: 2.617548493498107
Validation loss: 2.52550166280064

Epoch: 6| Step: 4
Training loss: 3.004582084754604
Validation loss: 2.525259511661017

Epoch: 6| Step: 5
Training loss: 2.687878426811171
Validation loss: 2.509806520596408

Epoch: 6| Step: 6
Training loss: 2.2048047743547676
Validation loss: 2.5238382279150993

Epoch: 6| Step: 7
Training loss: 1.8728285613533737
Validation loss: 2.5204590816484607

Epoch: 6| Step: 8
Training loss: 2.220619166522493
Validation loss: 2.524120352584532

Epoch: 6| Step: 9
Training loss: 1.9779690649392596
Validation loss: 2.5318837804868846

Epoch: 6| Step: 10
Training loss: 2.359261314229948
Validation loss: 2.5473276683854533

Epoch: 6| Step: 11
Training loss: 2.681786518615343
Validation loss: 2.5606721856603096

Epoch: 6| Step: 12
Training loss: 1.685264378077616
Validation loss: 2.567400131660512

Epoch: 6| Step: 13
Training loss: 2.229139904206197
Validation loss: 2.574793117199826

Epoch: 173| Step: 0
Training loss: 1.660933153464371
Validation loss: 2.5612570376506607

Epoch: 6| Step: 1
Training loss: 2.467156584088776
Validation loss: 2.5721934971488243

Epoch: 6| Step: 2
Training loss: 2.2075883820446633
Validation loss: 2.5700634774855375

Epoch: 6| Step: 3
Training loss: 2.330544258042805
Validation loss: 2.5647233141315113

Epoch: 6| Step: 4
Training loss: 2.7673486198506074
Validation loss: 2.569983897127796

Epoch: 6| Step: 5
Training loss: 2.00128823753106
Validation loss: 2.568284754763002

Epoch: 6| Step: 6
Training loss: 2.5000926954250664
Validation loss: 2.5643140063158945

Epoch: 6| Step: 7
Training loss: 2.5146330779263923
Validation loss: 2.5218083936552955

Epoch: 6| Step: 8
Training loss: 2.095081759839892
Validation loss: 2.513705040219649

Epoch: 6| Step: 9
Training loss: 2.3068250151215683
Validation loss: 2.514483174939622

Epoch: 6| Step: 10
Training loss: 2.4303862618721683
Validation loss: 2.490721746884636

Epoch: 6| Step: 11
Training loss: 2.955991937773451
Validation loss: 2.499653156380168

Epoch: 6| Step: 12
Training loss: 2.3353842395864515
Validation loss: 2.5067019117782507

Epoch: 6| Step: 13
Training loss: 2.002790649885321
Validation loss: 2.5109019041357197

Epoch: 174| Step: 0
Training loss: 1.9375317478655198
Validation loss: 2.5197695676085

Epoch: 6| Step: 1
Training loss: 1.989228927709719
Validation loss: 2.5327596636561345

Epoch: 6| Step: 2
Training loss: 1.7886339365627746
Validation loss: 2.5532303833894714

Epoch: 6| Step: 3
Training loss: 2.9409873228399794
Validation loss: 2.5646469612399128

Epoch: 6| Step: 4
Training loss: 2.7257617550669653
Validation loss: 2.6108461751742786

Epoch: 6| Step: 5
Training loss: 2.3758687889783405
Validation loss: 2.617029244112414

Epoch: 6| Step: 6
Training loss: 2.1523453008019593
Validation loss: 2.6127405600393834

Epoch: 6| Step: 7
Training loss: 2.233905022551378
Validation loss: 2.5944900682491676

Epoch: 6| Step: 8
Training loss: 1.7360730493399905
Validation loss: 2.5735487270966058

Epoch: 6| Step: 9
Training loss: 2.9463070270520433
Validation loss: 2.5404449142563217

Epoch: 6| Step: 10
Training loss: 2.3259775383242767
Validation loss: 2.5256662830508394

Epoch: 6| Step: 11
Training loss: 2.6729324657501508
Validation loss: 2.5108838312806516

Epoch: 6| Step: 12
Training loss: 1.884066342305454
Validation loss: 2.5029579148106738

Epoch: 6| Step: 13
Training loss: 2.9240585572710573
Validation loss: 2.491559913245089

Epoch: 175| Step: 0
Training loss: 2.967087932922915
Validation loss: 2.499130717781428

Epoch: 6| Step: 1
Training loss: 2.674189853094175
Validation loss: 2.490533134141498

Epoch: 6| Step: 2
Training loss: 2.3473494292731814
Validation loss: 2.499776106981321

Epoch: 6| Step: 3
Training loss: 1.929748348867454
Validation loss: 2.5071245876081845

Epoch: 6| Step: 4
Training loss: 2.184823605992149
Validation loss: 2.5044444949309828

Epoch: 6| Step: 5
Training loss: 2.3487621374867254
Validation loss: 2.5177683337922887

Epoch: 6| Step: 6
Training loss: 1.8074847584526332
Validation loss: 2.537583148076331

Epoch: 6| Step: 7
Training loss: 2.4217033079262813
Validation loss: 2.555313438081641

Epoch: 6| Step: 8
Training loss: 2.258556837257274
Validation loss: 2.583783315620026

Epoch: 6| Step: 9
Training loss: 2.0135786921107677
Validation loss: 2.5841942988073368

Epoch: 6| Step: 10
Training loss: 2.351395306394136
Validation loss: 2.6209406725114297

Epoch: 6| Step: 11
Training loss: 3.185841147409632
Validation loss: 2.6242227766091837

Epoch: 6| Step: 12
Training loss: 1.9753247633838347
Validation loss: 2.575138172899175

Epoch: 6| Step: 13
Training loss: 2.555193089842604
Validation loss: 2.545438162252415

Epoch: 176| Step: 0
Training loss: 2.55965898775715
Validation loss: 2.5198136124608643

Epoch: 6| Step: 1
Training loss: 2.162055341395167
Validation loss: 2.498685236758365

Epoch: 6| Step: 2
Training loss: 2.2726447437214095
Validation loss: 2.4796944518573745

Epoch: 6| Step: 3
Training loss: 2.2265129217684776
Validation loss: 2.490158913659551

Epoch: 6| Step: 4
Training loss: 1.8362286275992272
Validation loss: 2.4870162614915285

Epoch: 6| Step: 5
Training loss: 2.2412697645125443
Validation loss: 2.4803088199190384

Epoch: 6| Step: 6
Training loss: 2.279852452104991
Validation loss: 2.493413593671502

Epoch: 6| Step: 7
Training loss: 1.690395766986181
Validation loss: 2.49063741376935

Epoch: 6| Step: 8
Training loss: 2.4514507241475574
Validation loss: 2.4904111233486215

Epoch: 6| Step: 9
Training loss: 3.1938519485834367
Validation loss: 2.492770916963087

Epoch: 6| Step: 10
Training loss: 2.68812961743829
Validation loss: 2.4770746834627713

Epoch: 6| Step: 11
Training loss: 2.70751385150723
Validation loss: 2.5158052162712745

Epoch: 6| Step: 12
Training loss: 2.1251279007062966
Validation loss: 2.541604687503082

Epoch: 6| Step: 13
Training loss: 2.339901829477946
Validation loss: 2.564150116124967

Epoch: 177| Step: 0
Training loss: 2.65973820251321
Validation loss: 2.5811986614845517

Epoch: 6| Step: 1
Training loss: 2.4041027049564674
Validation loss: 2.6039528008698856

Epoch: 6| Step: 2
Training loss: 1.8457611882194775
Validation loss: 2.557060819152439

Epoch: 6| Step: 3
Training loss: 3.1318817658694815
Validation loss: 2.528599312718083

Epoch: 6| Step: 4
Training loss: 1.9760085222933077
Validation loss: 2.5277598459588195

Epoch: 6| Step: 5
Training loss: 1.9473512070194798
Validation loss: 2.511731295556773

Epoch: 6| Step: 6
Training loss: 2.5330434968017137
Validation loss: 2.4983195538063403

Epoch: 6| Step: 7
Training loss: 2.730455914250275
Validation loss: 2.4959334997117444

Epoch: 6| Step: 8
Training loss: 1.9492568507321817
Validation loss: 2.5031057219729327

Epoch: 6| Step: 9
Training loss: 2.3864749965033254
Validation loss: 2.5018179006014356

Epoch: 6| Step: 10
Training loss: 2.1131660585157612
Validation loss: 2.497820444510031

Epoch: 6| Step: 11
Training loss: 2.3251776760670917
Validation loss: 2.4983241981388535

Epoch: 6| Step: 12
Training loss: 2.0621249696026642
Validation loss: 2.5037539272623532

Epoch: 6| Step: 13
Training loss: 2.837620427605163
Validation loss: 2.510399328992392

Epoch: 178| Step: 0
Training loss: 2.752290292177951
Validation loss: 2.5364998429574888

Epoch: 6| Step: 1
Training loss: 2.442540166362558
Validation loss: 2.5445026355883926

Epoch: 6| Step: 2
Training loss: 2.473926573432377
Validation loss: 2.588919097176593

Epoch: 6| Step: 3
Training loss: 1.960277735144798
Validation loss: 2.5932454303988166

Epoch: 6| Step: 4
Training loss: 2.5814027237681905
Validation loss: 2.5944972972491898

Epoch: 6| Step: 5
Training loss: 2.4767503155752104
Validation loss: 2.5802970521900357

Epoch: 6| Step: 6
Training loss: 2.170429984003217
Validation loss: 2.565540246961425

Epoch: 6| Step: 7
Training loss: 2.701855488163358
Validation loss: 2.553096123932458

Epoch: 6| Step: 8
Training loss: 1.2872600091014244
Validation loss: 2.5251147165457457

Epoch: 6| Step: 9
Training loss: 1.8248415420927808
Validation loss: 2.514577090538217

Epoch: 6| Step: 10
Training loss: 2.6009890179136423
Validation loss: 2.5050881582345026

Epoch: 6| Step: 11
Training loss: 2.629953433737782
Validation loss: 2.5058138717599276

Epoch: 6| Step: 12
Training loss: 2.535852180968347
Validation loss: 2.5014754153709653

Epoch: 6| Step: 13
Training loss: 2.319142570064295
Validation loss: 2.4932594664574466

Epoch: 179| Step: 0
Training loss: 2.89733224035887
Validation loss: 2.488155575407645

Epoch: 6| Step: 1
Training loss: 2.1442957681079093
Validation loss: 2.4950227145968364

Epoch: 6| Step: 2
Training loss: 2.28532338631571
Validation loss: 2.50196720449877

Epoch: 6| Step: 3
Training loss: 2.7981246628547005
Validation loss: 2.513546213102965

Epoch: 6| Step: 4
Training loss: 2.0514624054802058
Validation loss: 2.5441915811451032

Epoch: 6| Step: 5
Training loss: 2.3496642704842916
Validation loss: 2.5691997279802172

Epoch: 6| Step: 6
Training loss: 2.530304532023818
Validation loss: 2.583408144411127

Epoch: 6| Step: 7
Training loss: 2.1758579535122013
Validation loss: 2.599233292971898

Epoch: 6| Step: 8
Training loss: 2.5014586961452125
Validation loss: 2.5996756063957234

Epoch: 6| Step: 9
Training loss: 2.24704187699305
Validation loss: 2.574612029331857

Epoch: 6| Step: 10
Training loss: 1.7398739493960438
Validation loss: 2.570934293877948

Epoch: 6| Step: 11
Training loss: 2.472646800621261
Validation loss: 2.52861062734802

Epoch: 6| Step: 12
Training loss: 2.544885993757269
Validation loss: 2.521015323553507

Epoch: 6| Step: 13
Training loss: 2.315946330685625
Validation loss: 2.520712720859702

Epoch: 180| Step: 0
Training loss: 2.2282438655782952
Validation loss: 2.507221092770286

Epoch: 6| Step: 1
Training loss: 2.388725473546504
Validation loss: 2.502396134626065

Epoch: 6| Step: 2
Training loss: 2.5864113549277916
Validation loss: 2.5111124543896275

Epoch: 6| Step: 3
Training loss: 1.4463541761908179
Validation loss: 2.492099294028307

Epoch: 6| Step: 4
Training loss: 2.6468095755570054
Validation loss: 2.5096103565758945

Epoch: 6| Step: 5
Training loss: 2.199641293846394
Validation loss: 2.5121441723590405

Epoch: 6| Step: 6
Training loss: 3.261270933190067
Validation loss: 2.52041452780345

Epoch: 6| Step: 7
Training loss: 1.7444680516386266
Validation loss: 2.551702281504857

Epoch: 6| Step: 8
Training loss: 2.210434927282191
Validation loss: 2.5518365987242286

Epoch: 6| Step: 9
Training loss: 2.6507417900190724
Validation loss: 2.5548421519342934

Epoch: 6| Step: 10
Training loss: 2.460791980134572
Validation loss: 2.5498547431613297

Epoch: 6| Step: 11
Training loss: 2.5345785602518576
Validation loss: 2.560872638929152

Epoch: 6| Step: 12
Training loss: 2.3505583485346033
Validation loss: 2.5819821259273654

Epoch: 6| Step: 13
Training loss: 1.5834863822985041
Validation loss: 2.542719862517323

Epoch: 181| Step: 0
Training loss: 2.024982819660916
Validation loss: 2.529907514190964

Epoch: 6| Step: 1
Training loss: 2.8146598999391452
Validation loss: 2.5188342882540455

Epoch: 6| Step: 2
Training loss: 2.918620372564817
Validation loss: 2.526135941676491

Epoch: 6| Step: 3
Training loss: 1.926483088308479
Validation loss: 2.5239890158238425

Epoch: 6| Step: 4
Training loss: 2.180876595526338
Validation loss: 2.5063408230301274

Epoch: 6| Step: 5
Training loss: 2.5215237095111664
Validation loss: 2.504227370183858

Epoch: 6| Step: 6
Training loss: 2.3042260694060954
Validation loss: 2.4888599946363814

Epoch: 6| Step: 7
Training loss: 1.7944999589130846
Validation loss: 2.50206976089375

Epoch: 6| Step: 8
Training loss: 1.535770671861223
Validation loss: 2.508998063764944

Epoch: 6| Step: 9
Training loss: 2.5541529664942195
Validation loss: 2.5067860056322013

Epoch: 6| Step: 10
Training loss: 2.2232382067795777
Validation loss: 2.5149218759353755

Epoch: 6| Step: 11
Training loss: 1.99435857017107
Validation loss: 2.511827813564524

Epoch: 6| Step: 12
Training loss: 2.13284089345512
Validation loss: 2.5360846952511307

Epoch: 6| Step: 13
Training loss: 3.151915945452427
Validation loss: 2.5405189757924447

Epoch: 182| Step: 0
Training loss: 2.6922220840048228
Validation loss: 2.5402418791626706

Epoch: 6| Step: 1
Training loss: 1.9003334781722732
Validation loss: 2.5736595708058108

Epoch: 6| Step: 2
Training loss: 2.4494355271578145
Validation loss: 2.601576827031762

Epoch: 6| Step: 3
Training loss: 2.3416368240500214
Validation loss: 2.5887090269413835

Epoch: 6| Step: 4
Training loss: 2.2255196253901754
Validation loss: 2.569474459521507

Epoch: 6| Step: 5
Training loss: 1.6687975215931865
Validation loss: 2.5246731436402663

Epoch: 6| Step: 6
Training loss: 2.768997463766571
Validation loss: 2.5261586165474705

Epoch: 6| Step: 7
Training loss: 2.069191798491404
Validation loss: 2.4929231297403196

Epoch: 6| Step: 8
Training loss: 2.056412933294692
Validation loss: 2.4979457680664345

Epoch: 6| Step: 9
Training loss: 2.7319681964827147
Validation loss: 2.5003287734967503

Epoch: 6| Step: 10
Training loss: 2.646435726605449
Validation loss: 2.503417667165438

Epoch: 6| Step: 11
Training loss: 1.8600853596861096
Validation loss: 2.5015331971078263

Epoch: 6| Step: 12
Training loss: 2.450252625033418
Validation loss: 2.50225832981229

Epoch: 6| Step: 13
Training loss: 2.8252006290727283
Validation loss: 2.50860771651307

Epoch: 183| Step: 0
Training loss: 2.1414603949141435
Validation loss: 2.5219774618530773

Epoch: 6| Step: 1
Training loss: 2.0997373462090887
Validation loss: 2.5364822188133314

Epoch: 6| Step: 2
Training loss: 2.163257876208255
Validation loss: 2.5665977093074246

Epoch: 6| Step: 3
Training loss: 2.4606312394686793
Validation loss: 2.564370519349184

Epoch: 6| Step: 4
Training loss: 1.8354689629241983
Validation loss: 2.5783978250431834

Epoch: 6| Step: 5
Training loss: 2.112227030617123
Validation loss: 2.5959383300887686

Epoch: 6| Step: 6
Training loss: 2.710708036797858
Validation loss: 2.603696434799574

Epoch: 6| Step: 7
Training loss: 1.7117488800976692
Validation loss: 2.585552098500013

Epoch: 6| Step: 8
Training loss: 2.3824613718942254
Validation loss: 2.5524343651413433

Epoch: 6| Step: 9
Training loss: 2.561666864938259
Validation loss: 2.5464252940810126

Epoch: 6| Step: 10
Training loss: 2.8128292314896153
Validation loss: 2.5333952584561765

Epoch: 6| Step: 11
Training loss: 2.171761269644779
Validation loss: 2.5327744465596593

Epoch: 6| Step: 12
Training loss: 2.6311999173506218
Validation loss: 2.539724660753078

Epoch: 6| Step: 13
Training loss: 2.3742801177568924
Validation loss: 2.5454131535626416

Epoch: 184| Step: 0
Training loss: 2.0858739493186755
Validation loss: 2.5362146457635197

Epoch: 6| Step: 1
Training loss: 2.5835747657265267
Validation loss: 2.5343701006257553

Epoch: 6| Step: 2
Training loss: 1.6814360278140539
Validation loss: 2.526948830174411

Epoch: 6| Step: 3
Training loss: 2.8730689488126364
Validation loss: 2.53196393843267

Epoch: 6| Step: 4
Training loss: 2.7523470313090272
Validation loss: 2.562613988682583

Epoch: 6| Step: 5
Training loss: 2.2950415527305306
Validation loss: 2.548699519200203

Epoch: 6| Step: 6
Training loss: 2.1466898520423987
Validation loss: 2.5582370965689947

Epoch: 6| Step: 7
Training loss: 1.6625207914041846
Validation loss: 2.5628632233770268

Epoch: 6| Step: 8
Training loss: 2.7108078631535393
Validation loss: 2.562953071548609

Epoch: 6| Step: 9
Training loss: 2.0785039111722994
Validation loss: 2.559137728183923

Epoch: 6| Step: 10
Training loss: 2.290545194934835
Validation loss: 2.5438580654849567

Epoch: 6| Step: 11
Training loss: 2.2431996682728674
Validation loss: 2.5222645524497134

Epoch: 6| Step: 12
Training loss: 2.161839414040647
Validation loss: 2.532017956315983

Epoch: 6| Step: 13
Training loss: 2.469088567382033
Validation loss: 2.5265783205246737

Epoch: 185| Step: 0
Training loss: 2.5908755265271854
Validation loss: 2.5171287733410503

Epoch: 6| Step: 1
Training loss: 1.7100247949063008
Validation loss: 2.513140571126677

Epoch: 6| Step: 2
Training loss: 2.390524569128435
Validation loss: 2.556634003912597

Epoch: 6| Step: 3
Training loss: 2.096421544352656
Validation loss: 2.557480338310965

Epoch: 6| Step: 4
Training loss: 2.104890252013739
Validation loss: 2.5882179062900144

Epoch: 6| Step: 5
Training loss: 2.859412688126796
Validation loss: 2.593068228592292

Epoch: 6| Step: 6
Training loss: 2.279040376929038
Validation loss: 2.6060040108568616

Epoch: 6| Step: 7
Training loss: 2.298366970326028
Validation loss: 2.5900378185687587

Epoch: 6| Step: 8
Training loss: 2.178517345409319
Validation loss: 2.575347930586714

Epoch: 6| Step: 9
Training loss: 2.3715828857671664
Validation loss: 2.549921487709068

Epoch: 6| Step: 10
Training loss: 2.085235591906795
Validation loss: 2.5288331145454053

Epoch: 6| Step: 11
Training loss: 2.5366813880135664
Validation loss: 2.5261268575139484

Epoch: 6| Step: 12
Training loss: 3.174377957181385
Validation loss: 2.5143574113552245

Epoch: 6| Step: 13
Training loss: 1.4592516959335566
Validation loss: 2.495366921465405

Epoch: 186| Step: 0
Training loss: 2.5124846576582835
Validation loss: 2.502241425253734

Epoch: 6| Step: 1
Training loss: 2.1464554955689175
Validation loss: 2.495909841783914

Epoch: 6| Step: 2
Training loss: 1.5719547319761864
Validation loss: 2.50186494567157

Epoch: 6| Step: 3
Training loss: 1.5696392988626195
Validation loss: 2.5021901709504726

Epoch: 6| Step: 4
Training loss: 2.3888217882101705
Validation loss: 2.514550558117336

Epoch: 6| Step: 5
Training loss: 2.5729590243869636
Validation loss: 2.529001343889283

Epoch: 6| Step: 6
Training loss: 2.1108483714298307
Validation loss: 2.520018963591052

Epoch: 6| Step: 7
Training loss: 2.1961268750558958
Validation loss: 2.524048958501542

Epoch: 6| Step: 8
Training loss: 2.6790580343843096
Validation loss: 2.5362977610891995

Epoch: 6| Step: 9
Training loss: 2.5620827684098377
Validation loss: 2.5463835665328842

Epoch: 6| Step: 10
Training loss: 2.2316857006048543
Validation loss: 2.523733879454798

Epoch: 6| Step: 11
Training loss: 2.132095160387833
Validation loss: 2.550115883132606

Epoch: 6| Step: 12
Training loss: 2.6455633270984062
Validation loss: 2.535538012302448

Epoch: 6| Step: 13
Training loss: 2.596432321507899
Validation loss: 2.55810598135672

Epoch: 187| Step: 0
Training loss: 1.711480947087002
Validation loss: 2.5870994423686113

Epoch: 6| Step: 1
Training loss: 2.378182236813521
Validation loss: 2.581428938506327

Epoch: 6| Step: 2
Training loss: 1.8805882782423624
Validation loss: 2.5540594637487244

Epoch: 6| Step: 3
Training loss: 1.786583861749934
Validation loss: 2.5423010414869944

Epoch: 6| Step: 4
Training loss: 1.7479340756640198
Validation loss: 2.533717205009136

Epoch: 6| Step: 5
Training loss: 2.343177623793798
Validation loss: 2.5164395863566114

Epoch: 6| Step: 6
Training loss: 1.7871046389324197
Validation loss: 2.5207148095843435

Epoch: 6| Step: 7
Training loss: 3.0313896854896383
Validation loss: 2.5161482936052475

Epoch: 6| Step: 8
Training loss: 3.009737899622867
Validation loss: 2.5119774004472384

Epoch: 6| Step: 9
Training loss: 2.5891857203075226
Validation loss: 2.5232076786288875

Epoch: 6| Step: 10
Training loss: 2.4747848147471285
Validation loss: 2.527234585589387

Epoch: 6| Step: 11
Training loss: 2.362800385285555
Validation loss: 2.5371474179261053

Epoch: 6| Step: 12
Training loss: 2.580995568396907
Validation loss: 2.549002431704475

Epoch: 6| Step: 13
Training loss: 1.937889429148555
Validation loss: 2.552663438740992

Epoch: 188| Step: 0
Training loss: 2.143550572141551
Validation loss: 2.5577574006058916

Epoch: 6| Step: 1
Training loss: 2.1568045525001907
Validation loss: 2.5837516574149926

Epoch: 6| Step: 2
Training loss: 2.4101289472470966
Validation loss: 2.6026248080386956

Epoch: 6| Step: 3
Training loss: 2.669473204345888
Validation loss: 2.5984936428064485

Epoch: 6| Step: 4
Training loss: 1.8798116615999483
Validation loss: 2.6085147162878983

Epoch: 6| Step: 5
Training loss: 1.9385874987935199
Validation loss: 2.584766026551095

Epoch: 6| Step: 6
Training loss: 2.451594075473094
Validation loss: 2.5760236402951144

Epoch: 6| Step: 7
Training loss: 2.6032989480470414
Validation loss: 2.5750851521409093

Epoch: 6| Step: 8
Training loss: 2.474299507039742
Validation loss: 2.565240572999395

Epoch: 6| Step: 9
Training loss: 1.9394583495531657
Validation loss: 2.5481004789543467

Epoch: 6| Step: 10
Training loss: 2.1743786329835
Validation loss: 2.563748404485168

Epoch: 6| Step: 11
Training loss: 2.2568727009121625
Validation loss: 2.5457826545861457

Epoch: 6| Step: 12
Training loss: 1.735859622118334
Validation loss: 2.545752732478085

Epoch: 6| Step: 13
Training loss: 2.8523433086611205
Validation loss: 2.5528724436548305

Epoch: 189| Step: 0
Training loss: 1.8739967205136976
Validation loss: 2.5527214394367883

Epoch: 6| Step: 1
Training loss: 2.4050986768183034
Validation loss: 2.5480529307881623

Epoch: 6| Step: 2
Training loss: 2.078769899274188
Validation loss: 2.5749655273747205

Epoch: 6| Step: 3
Training loss: 2.817630411834617
Validation loss: 2.5468035884901195

Epoch: 6| Step: 4
Training loss: 2.7551813731229586
Validation loss: 2.5871156311804278

Epoch: 6| Step: 5
Training loss: 2.202389208705749
Validation loss: 2.6136897157041075

Epoch: 6| Step: 6
Training loss: 2.2356808987679173
Validation loss: 2.6085297363323656

Epoch: 6| Step: 7
Training loss: 2.045100367351863
Validation loss: 2.6475265871897697

Epoch: 6| Step: 8
Training loss: 2.254368461935717
Validation loss: 2.646618904176054

Epoch: 6| Step: 9
Training loss: 2.3474768952521505
Validation loss: 2.6488109464116

Epoch: 6| Step: 10
Training loss: 2.780072766730733
Validation loss: 2.602763909509118

Epoch: 6| Step: 11
Training loss: 2.5345208028487836
Validation loss: 2.56317968581032

Epoch: 6| Step: 12
Training loss: 1.3238431234627277
Validation loss: 2.5054568499877368

Epoch: 6| Step: 13
Training loss: 2.290691850423136
Validation loss: 2.4879441125220807

Epoch: 190| Step: 0
Training loss: 2.7227392635524317
Validation loss: 2.4900037551504313

Epoch: 6| Step: 1
Training loss: 2.3680013636121173
Validation loss: 2.4910821645869623

Epoch: 6| Step: 2
Training loss: 1.9742330462319666
Validation loss: 2.4877463523882217

Epoch: 6| Step: 3
Training loss: 2.5696516027938907
Validation loss: 2.4911136844981674

Epoch: 6| Step: 4
Training loss: 1.5149375693674016
Validation loss: 2.511626974103129

Epoch: 6| Step: 5
Training loss: 2.613187825310604
Validation loss: 2.5077541578246527

Epoch: 6| Step: 6
Training loss: 2.7293526001315267
Validation loss: 2.497621310612843

Epoch: 6| Step: 7
Training loss: 2.791545542656188
Validation loss: 2.4941246452407126

Epoch: 6| Step: 8
Training loss: 2.180724632149747
Validation loss: 2.49303813197412

Epoch: 6| Step: 9
Training loss: 2.576547868547137
Validation loss: 2.5029385780258933

Epoch: 6| Step: 10
Training loss: 2.474835199591423
Validation loss: 2.5270583371411863

Epoch: 6| Step: 11
Training loss: 2.2286369431126603
Validation loss: 2.517761105442185

Epoch: 6| Step: 12
Training loss: 2.097309900883276
Validation loss: 2.5574444157391847

Epoch: 6| Step: 13
Training loss: 2.305467376656868
Validation loss: 2.5703581522836356

Epoch: 191| Step: 0
Training loss: 2.5972094158688726
Validation loss: 2.5726302961810217

Epoch: 6| Step: 1
Training loss: 2.2513834620708515
Validation loss: 2.5676837375744754

Epoch: 6| Step: 2
Training loss: 2.273242535702944
Validation loss: 2.54764122543732

Epoch: 6| Step: 3
Training loss: 2.700896033202987
Validation loss: 2.5503982014873885

Epoch: 6| Step: 4
Training loss: 2.2947988663841987
Validation loss: 2.547167942541598

Epoch: 6| Step: 5
Training loss: 2.197461362809376
Validation loss: 2.520503871185008

Epoch: 6| Step: 6
Training loss: 1.8043061881559004
Validation loss: 2.5112014799492637

Epoch: 6| Step: 7
Training loss: 2.5397774496727004
Validation loss: 2.49857967084332

Epoch: 6| Step: 8
Training loss: 2.299105437088967
Validation loss: 2.5083351580928954

Epoch: 6| Step: 9
Training loss: 2.1173656430906136
Validation loss: 2.4978392004918724

Epoch: 6| Step: 10
Training loss: 2.1652610572080717
Validation loss: 2.5089401055119196

Epoch: 6| Step: 11
Training loss: 2.603782096441542
Validation loss: 2.5041573924709053

Epoch: 6| Step: 12
Training loss: 2.4573799698506424
Validation loss: 2.4992517941146577

Epoch: 6| Step: 13
Training loss: 2.0397939721955076
Validation loss: 2.497445000942492

Epoch: 192| Step: 0
Training loss: 2.198931443197428
Validation loss: 2.5300769835157335

Epoch: 6| Step: 1
Training loss: 1.7594727538141035
Validation loss: 2.5370504848663176

Epoch: 6| Step: 2
Training loss: 2.475234003275585
Validation loss: 2.5251726812229314

Epoch: 6| Step: 3
Training loss: 1.737124969295763
Validation loss: 2.516681867726209

Epoch: 6| Step: 4
Training loss: 2.2067404226248906
Validation loss: 2.5269110740158

Epoch: 6| Step: 5
Training loss: 2.798411538638912
Validation loss: 2.5162776629536068

Epoch: 6| Step: 6
Training loss: 2.500628297055642
Validation loss: 2.5108461342413633

Epoch: 6| Step: 7
Training loss: 2.2355533507108505
Validation loss: 2.5325660345613414

Epoch: 6| Step: 8
Training loss: 2.3378877695523497
Validation loss: 2.564246210726787

Epoch: 6| Step: 9
Training loss: 2.2900328158230367
Validation loss: 2.593465414099649

Epoch: 6| Step: 10
Training loss: 2.063708789212956
Validation loss: 2.6315933106264686

Epoch: 6| Step: 11
Training loss: 2.492110200358809
Validation loss: 2.6169588207337733

Epoch: 6| Step: 12
Training loss: 2.813887698830289
Validation loss: 2.613954861148802

Epoch: 6| Step: 13
Training loss: 1.9063841975768556
Validation loss: 2.6211780699400635

Epoch: 193| Step: 0
Training loss: 2.9431934778230926
Validation loss: 2.6142696531870984

Epoch: 6| Step: 1
Training loss: 2.262596473955327
Validation loss: 2.616294655404337

Epoch: 6| Step: 2
Training loss: 2.3886938335816126
Validation loss: 2.6015775754566257

Epoch: 6| Step: 3
Training loss: 1.6348017935813732
Validation loss: 2.5881684697546317

Epoch: 6| Step: 4
Training loss: 1.7236449849074438
Validation loss: 2.5754325293563816

Epoch: 6| Step: 5
Training loss: 2.1623133672540544
Validation loss: 2.547862760441966

Epoch: 6| Step: 6
Training loss: 2.3661084627199016
Validation loss: 2.528795755789201

Epoch: 6| Step: 7
Training loss: 3.0651630578532076
Validation loss: 2.4912672745532727

Epoch: 6| Step: 8
Training loss: 2.521258946158443
Validation loss: 2.5054884269567355

Epoch: 6| Step: 9
Training loss: 2.187901814205175
Validation loss: 2.4876441399427853

Epoch: 6| Step: 10
Training loss: 2.6220520359872816
Validation loss: 2.502749314136314

Epoch: 6| Step: 11
Training loss: 2.0338235837335983
Validation loss: 2.5040413140158

Epoch: 6| Step: 12
Training loss: 2.304287840247852
Validation loss: 2.5203121818326277

Epoch: 6| Step: 13
Training loss: 1.909037568639382
Validation loss: 2.538138447387512

Epoch: 194| Step: 0
Training loss: 2.4257682763089057
Validation loss: 2.5559723837777915

Epoch: 6| Step: 1
Training loss: 2.686666758689532
Validation loss: 2.5838631476320724

Epoch: 6| Step: 2
Training loss: 2.196039045664213
Validation loss: 2.5770762458607184

Epoch: 6| Step: 3
Training loss: 1.89436638498188
Validation loss: 2.572493318846256

Epoch: 6| Step: 4
Training loss: 2.6950480898103435
Validation loss: 2.5670988018418313

Epoch: 6| Step: 5
Training loss: 2.0864032125468768
Validation loss: 2.558157723039901

Epoch: 6| Step: 6
Training loss: 1.5542853520597308
Validation loss: 2.5581230837220397

Epoch: 6| Step: 7
Training loss: 1.830535343015482
Validation loss: 2.557021370851287

Epoch: 6| Step: 8
Training loss: 2.7625852787439458
Validation loss: 2.530319827876631

Epoch: 6| Step: 9
Training loss: 2.2352542881030137
Validation loss: 2.5364573724785355

Epoch: 6| Step: 10
Training loss: 2.4423006174318926
Validation loss: 2.5194860961968493

Epoch: 6| Step: 11
Training loss: 1.8836519894482353
Validation loss: 2.5111215216590588

Epoch: 6| Step: 12
Training loss: 2.8157594231664023
Validation loss: 2.5621301182460043

Epoch: 6| Step: 13
Training loss: 1.7391314195547218
Validation loss: 2.588368175277817

Epoch: 195| Step: 0
Training loss: 2.3117053754444763
Validation loss: 2.5787069415271198

Epoch: 6| Step: 1
Training loss: 1.871630502058719
Validation loss: 2.575946966750316

Epoch: 6| Step: 2
Training loss: 1.4657998182069965
Validation loss: 2.575000202462889

Epoch: 6| Step: 3
Training loss: 2.4603701951830277
Validation loss: 2.574965064419784

Epoch: 6| Step: 4
Training loss: 2.4294031105544787
Validation loss: 2.5774243722520263

Epoch: 6| Step: 5
Training loss: 2.214431535955741
Validation loss: 2.5635418673477393

Epoch: 6| Step: 6
Training loss: 1.9496276059728523
Validation loss: 2.548314723197823

Epoch: 6| Step: 7
Training loss: 2.8251748056257564
Validation loss: 2.5415611140945744

Epoch: 6| Step: 8
Training loss: 2.735448659190194
Validation loss: 2.5354493083958003

Epoch: 6| Step: 9
Training loss: 2.599341375120171
Validation loss: 2.5212155097061117

Epoch: 6| Step: 10
Training loss: 2.8426719550440813
Validation loss: 2.535625522090992

Epoch: 6| Step: 11
Training loss: 2.4008929578250844
Validation loss: 2.54825810359022

Epoch: 6| Step: 12
Training loss: 1.6880071372473369
Validation loss: 2.555827633971916

Epoch: 6| Step: 13
Training loss: 1.5548333334456466
Validation loss: 2.5908944984087965

Epoch: 196| Step: 0
Training loss: 2.575203089436494
Validation loss: 2.597747120427066

Epoch: 6| Step: 1
Training loss: 2.229970899828249
Validation loss: 2.623958509706918

Epoch: 6| Step: 2
Training loss: 1.616110622830953
Validation loss: 2.5760886270695758

Epoch: 6| Step: 3
Training loss: 2.2663200627652067
Validation loss: 2.5852422480700445

Epoch: 6| Step: 4
Training loss: 1.78470410520443
Validation loss: 2.5635062777295596

Epoch: 6| Step: 5
Training loss: 3.4248003713527835
Validation loss: 2.5436092572091304

Epoch: 6| Step: 6
Training loss: 1.5285397113137462
Validation loss: 2.530014270194397

Epoch: 6| Step: 7
Training loss: 2.299991698872057
Validation loss: 2.5373202562484614

Epoch: 6| Step: 8
Training loss: 2.096553917760792
Validation loss: 2.5403427261254308

Epoch: 6| Step: 9
Training loss: 1.7010533042956577
Validation loss: 2.521034860568412

Epoch: 6| Step: 10
Training loss: 2.756847007261693
Validation loss: 2.5347114568565905

Epoch: 6| Step: 11
Training loss: 2.233234567973011
Validation loss: 2.53253146100249

Epoch: 6| Step: 12
Training loss: 2.6036824704486117
Validation loss: 2.537361443781664

Epoch: 6| Step: 13
Training loss: 1.8559574808643562
Validation loss: 2.5294936880472925

Epoch: 197| Step: 0
Training loss: 2.862058620489654
Validation loss: 2.552196941312339

Epoch: 6| Step: 1
Training loss: 1.6821250084996568
Validation loss: 2.5538925505051213

Epoch: 6| Step: 2
Training loss: 2.1714399162691658
Validation loss: 2.5419368496065546

Epoch: 6| Step: 3
Training loss: 1.900371178961723
Validation loss: 2.518916170808894

Epoch: 6| Step: 4
Training loss: 2.7105714402416714
Validation loss: 2.555452704663519

Epoch: 6| Step: 5
Training loss: 2.153224425657041
Validation loss: 2.5542193809242937

Epoch: 6| Step: 6
Training loss: 2.189941351815077
Validation loss: 2.5417233184587666

Epoch: 6| Step: 7
Training loss: 1.7101762027195164
Validation loss: 2.538910862233767

Epoch: 6| Step: 8
Training loss: 2.0134159965260876
Validation loss: 2.5272561735675967

Epoch: 6| Step: 9
Training loss: 2.6683708546955156
Validation loss: 2.5401447043913534

Epoch: 6| Step: 10
Training loss: 2.819366380918939
Validation loss: 2.5440946821901185

Epoch: 6| Step: 11
Training loss: 2.094857677277699
Validation loss: 2.5570246653555087

Epoch: 6| Step: 12
Training loss: 2.074624691768209
Validation loss: 2.5878141571839315

Epoch: 6| Step: 13
Training loss: 2.184508539792144
Validation loss: 2.6268793977320475

Epoch: 198| Step: 0
Training loss: 2.139628157351973
Validation loss: 2.6697741423195573

Epoch: 6| Step: 1
Training loss: 1.3671839250790314
Validation loss: 2.6736282337075297

Epoch: 6| Step: 2
Training loss: 2.7468118393442458
Validation loss: 2.6763923633999176

Epoch: 6| Step: 3
Training loss: 2.7452169784923535
Validation loss: 2.705624286771654

Epoch: 6| Step: 4
Training loss: 2.6536948985966053
Validation loss: 2.628712692806233

Epoch: 6| Step: 5
Training loss: 2.525136181020052
Validation loss: 2.575989395477155

Epoch: 6| Step: 6
Training loss: 2.3351907716702707
Validation loss: 2.5589521239605815

Epoch: 6| Step: 7
Training loss: 1.9560329914080086
Validation loss: 2.52813175422134

Epoch: 6| Step: 8
Training loss: 2.5098566298554186
Validation loss: 2.51112623725919

Epoch: 6| Step: 9
Training loss: 2.7280173368937866
Validation loss: 2.51130425259926

Epoch: 6| Step: 10
Training loss: 2.090549699142831
Validation loss: 2.4977583849096767

Epoch: 6| Step: 11
Training loss: 1.959144349949385
Validation loss: 2.5279874144132206

Epoch: 6| Step: 12
Training loss: 2.0446002929597733
Validation loss: 2.514627104733542

Epoch: 6| Step: 13
Training loss: 1.7399084811422854
Validation loss: 2.5656833414374547

Epoch: 199| Step: 0
Training loss: 2.43640268519668
Validation loss: 2.5737082361553973

Epoch: 6| Step: 1
Training loss: 2.05658428408405
Validation loss: 2.5739458216310727

Epoch: 6| Step: 2
Training loss: 2.4800625205619307
Validation loss: 2.553627424169218

Epoch: 6| Step: 3
Training loss: 1.8405885262105686
Validation loss: 2.5226983482807364

Epoch: 6| Step: 4
Training loss: 2.610250360458725
Validation loss: 2.5154323938533256

Epoch: 6| Step: 5
Training loss: 2.1690729302069114
Validation loss: 2.524285771290801

Epoch: 6| Step: 6
Training loss: 2.00104566895063
Validation loss: 2.509251879644341

Epoch: 6| Step: 7
Training loss: 2.385165594178235
Validation loss: 2.497070113584739

Epoch: 6| Step: 8
Training loss: 2.3970484982876985
Validation loss: 2.513403343908642

Epoch: 6| Step: 9
Training loss: 2.1408191230909135
Validation loss: 2.519007823107622

Epoch: 6| Step: 10
Training loss: 2.4158339435970944
Validation loss: 2.505416041994316

Epoch: 6| Step: 11
Training loss: 1.6746327837479602
Validation loss: 2.518145891539865

Epoch: 6| Step: 12
Training loss: 2.862291526699102
Validation loss: 2.5219942577578025

Epoch: 6| Step: 13
Training loss: 2.3323680947510925
Validation loss: 2.540532505280169

Epoch: 200| Step: 0
Training loss: 2.049680112676253
Validation loss: 2.5575427354888376

Epoch: 6| Step: 1
Training loss: 2.7986959145326034
Validation loss: 2.5744233038077313

Epoch: 6| Step: 2
Training loss: 1.8719721824625097
Validation loss: 2.578891339702429

Epoch: 6| Step: 3
Training loss: 1.6478582150798733
Validation loss: 2.6082481189575843

Epoch: 6| Step: 4
Training loss: 1.9818571442010526
Validation loss: 2.5695856026947244

Epoch: 6| Step: 5
Training loss: 2.5850003206706633
Validation loss: 2.582067492255219

Epoch: 6| Step: 6
Training loss: 2.296678184329064
Validation loss: 2.5766923410963267

Epoch: 6| Step: 7
Training loss: 2.9462065212584405
Validation loss: 2.580884485482009

Epoch: 6| Step: 8
Training loss: 1.906565218018322
Validation loss: 2.585773566544249

Epoch: 6| Step: 9
Training loss: 2.3053089403164027
Validation loss: 2.589959250857412

Epoch: 6| Step: 10
Training loss: 2.6510833756938634
Validation loss: 2.597771357612773

Epoch: 6| Step: 11
Training loss: 1.6971129951285662
Validation loss: 2.590360379940304

Epoch: 6| Step: 12
Training loss: 2.378583814195145
Validation loss: 2.5996325477734725

Epoch: 6| Step: 13
Training loss: 2.302479141753877
Validation loss: 2.5802590679488246

Epoch: 201| Step: 0
Training loss: 2.3902571650735713
Validation loss: 2.552775594022931

Epoch: 6| Step: 1
Training loss: 2.020143161648374
Validation loss: 2.56012356671

Epoch: 6| Step: 2
Training loss: 2.4332348603307006
Validation loss: 2.536554618015805

Epoch: 6| Step: 3
Training loss: 3.0458472450310894
Validation loss: 2.5434658426068815

Epoch: 6| Step: 4
Training loss: 1.3562840365168984
Validation loss: 2.532557585382573

Epoch: 6| Step: 5
Training loss: 2.0809071209705534
Validation loss: 2.534102241837045

Epoch: 6| Step: 6
Training loss: 2.567022473167421
Validation loss: 2.537137496122842

Epoch: 6| Step: 7
Training loss: 2.2322434081182
Validation loss: 2.5456252200842022

Epoch: 6| Step: 8
Training loss: 1.9621525985449773
Validation loss: 2.546408300408903

Epoch: 6| Step: 9
Training loss: 2.4900430762919648
Validation loss: 2.5615236391665395

Epoch: 6| Step: 10
Training loss: 1.9127247397824343
Validation loss: 2.611888352726852

Epoch: 6| Step: 11
Training loss: 1.7998486667171483
Validation loss: 2.597691126998008

Epoch: 6| Step: 12
Training loss: 2.5733241840885097
Validation loss: 2.654568068624555

Epoch: 6| Step: 13
Training loss: 2.10121826983021
Validation loss: 2.6237131173550954

Epoch: 202| Step: 0
Training loss: 1.9313124020071761
Validation loss: 2.5969188666588425

Epoch: 6| Step: 1
Training loss: 2.884940832314769
Validation loss: 2.597302068977784

Epoch: 6| Step: 2
Training loss: 2.9670283093569214
Validation loss: 2.5877634230727375

Epoch: 6| Step: 3
Training loss: 2.3445216625186656
Validation loss: 2.5971146021241274

Epoch: 6| Step: 4
Training loss: 2.2502787735219014
Validation loss: 2.6190707894121896

Epoch: 6| Step: 5
Training loss: 2.4885339054728077
Validation loss: 2.6067179249905075

Epoch: 6| Step: 6
Training loss: 2.4553085619591544
Validation loss: 2.6224712877426097

Epoch: 6| Step: 7
Training loss: 2.0953673763044294
Validation loss: 2.60341231037879

Epoch: 6| Step: 8
Training loss: 1.922081664844282
Validation loss: 2.5931339528127006

Epoch: 6| Step: 9
Training loss: 2.012655273513077
Validation loss: 2.5581945209763313

Epoch: 6| Step: 10
Training loss: 1.7180823676688863
Validation loss: 2.546804493433293

Epoch: 6| Step: 11
Training loss: 1.8578237265335078
Validation loss: 2.5398509830906537

Epoch: 6| Step: 12
Training loss: 2.219995713100934
Validation loss: 2.5133539851965057

Epoch: 6| Step: 13
Training loss: 2.0800906185437595
Validation loss: 2.533043394834732

Epoch: 203| Step: 0
Training loss: 2.3000980522153154
Validation loss: 2.5363997361910204

Epoch: 6| Step: 1
Training loss: 1.7942048509606403
Validation loss: 2.565666754100564

Epoch: 6| Step: 2
Training loss: 1.7501301035883259
Validation loss: 2.5522841912980265

Epoch: 6| Step: 3
Training loss: 2.311753848558112
Validation loss: 2.5805165151575005

Epoch: 6| Step: 4
Training loss: 2.434756960792983
Validation loss: 2.550130639438562

Epoch: 6| Step: 5
Training loss: 1.9801938921465203
Validation loss: 2.557672962941458

Epoch: 6| Step: 6
Training loss: 2.186086470440704
Validation loss: 2.565572400934269

Epoch: 6| Step: 7
Training loss: 2.8553511860203487
Validation loss: 2.5527301798997386

Epoch: 6| Step: 8
Training loss: 2.0882827205060526
Validation loss: 2.57833137071564

Epoch: 6| Step: 9
Training loss: 2.215733801748848
Validation loss: 2.5953370796094593

Epoch: 6| Step: 10
Training loss: 1.9476276394073502
Validation loss: 2.6094211871233957

Epoch: 6| Step: 11
Training loss: 2.4631078444306693
Validation loss: 2.6256575593395324

Epoch: 6| Step: 12
Training loss: 2.5946367885003676
Validation loss: 2.582425193724863

Epoch: 6| Step: 13
Training loss: 1.9293042006877774
Validation loss: 2.590855741636054

Epoch: 204| Step: 0
Training loss: 1.8168627749761048
Validation loss: 2.606008935966209

Epoch: 6| Step: 1
Training loss: 2.057223767154004
Validation loss: 2.5833078326740706

Epoch: 6| Step: 2
Training loss: 2.844682310096074
Validation loss: 2.581765581045733

Epoch: 6| Step: 3
Training loss: 2.270027671006728
Validation loss: 2.5984470320614883

Epoch: 6| Step: 4
Training loss: 2.0846751533134675
Validation loss: 2.5935610353813643

Epoch: 6| Step: 5
Training loss: 2.1789797929489465
Validation loss: 2.647886355807109

Epoch: 6| Step: 6
Training loss: 2.1867791214187142
Validation loss: 2.675602563985101

Epoch: 6| Step: 7
Training loss: 2.732156256634848
Validation loss: 2.6740814675983957

Epoch: 6| Step: 8
Training loss: 2.5028745337819567
Validation loss: 2.6433930044025393

Epoch: 6| Step: 9
Training loss: 1.9879216016895915
Validation loss: 2.601387758849356

Epoch: 6| Step: 10
Training loss: 2.274777780935254
Validation loss: 2.585058164575506

Epoch: 6| Step: 11
Training loss: 1.6422827202134145
Validation loss: 2.56673164182222

Epoch: 6| Step: 12
Training loss: 2.1511211488338935
Validation loss: 2.53533295425558

Epoch: 6| Step: 13
Training loss: 2.0527809493482447
Validation loss: 2.5265839980985456

Epoch: 205| Step: 0
Training loss: 2.7703363838177477
Validation loss: 2.509276599466996

Epoch: 6| Step: 1
Training loss: 2.059382423306459
Validation loss: 2.4948534124985278

Epoch: 6| Step: 2
Training loss: 2.4317467111020354
Validation loss: 2.5092539541551546

Epoch: 6| Step: 3
Training loss: 2.0865242236495756
Validation loss: 2.5247472664904866

Epoch: 6| Step: 4
Training loss: 2.2491612460492867
Validation loss: 2.5354935667046243

Epoch: 6| Step: 5
Training loss: 2.14643894525827
Validation loss: 2.5789414933554644

Epoch: 6| Step: 6
Training loss: 2.7616762754203696
Validation loss: 2.639215126841367

Epoch: 6| Step: 7
Training loss: 1.833921258752358
Validation loss: 2.6676231149221983

Epoch: 6| Step: 8
Training loss: 2.632554944398753
Validation loss: 2.693969377168543

Epoch: 6| Step: 9
Training loss: 2.141251187031913
Validation loss: 2.685531028007222

Epoch: 6| Step: 10
Training loss: 1.940206851844655
Validation loss: 2.6265026817189474

Epoch: 6| Step: 11
Training loss: 2.3872884766512725
Validation loss: 2.5641694562084987

Epoch: 6| Step: 12
Training loss: 2.3120027987057337
Validation loss: 2.540735689186402

Epoch: 6| Step: 13
Training loss: 2.362390675109276
Validation loss: 2.5309208942481978

Epoch: 206| Step: 0
Training loss: 2.3530136142828084
Validation loss: 2.511255042509514

Epoch: 6| Step: 1
Training loss: 2.2377971771486993
Validation loss: 2.5028280792469078

Epoch: 6| Step: 2
Training loss: 2.6037965638698632
Validation loss: 2.5116584576795558

Epoch: 6| Step: 3
Training loss: 1.7694735149202636
Validation loss: 2.5117262963260227

Epoch: 6| Step: 4
Training loss: 2.482128735273753
Validation loss: 2.504330349701776

Epoch: 6| Step: 5
Training loss: 3.0205666961505013
Validation loss: 2.503497173122087

Epoch: 6| Step: 6
Training loss: 1.3292342098059442
Validation loss: 2.5052959297288515

Epoch: 6| Step: 7
Training loss: 2.0054884941933215
Validation loss: 2.514555504325425

Epoch: 6| Step: 8
Training loss: 1.6321066466902996
Validation loss: 2.528319715271672

Epoch: 6| Step: 9
Training loss: 1.9140578912173467
Validation loss: 2.5111176922054566

Epoch: 6| Step: 10
Training loss: 1.7865125316113089
Validation loss: 2.512587883900487

Epoch: 6| Step: 11
Training loss: 2.4414690421612533
Validation loss: 2.5446833130083224

Epoch: 6| Step: 12
Training loss: 3.170594506578614
Validation loss: 2.569516229629109

Epoch: 6| Step: 13
Training loss: 2.0824284559327286
Validation loss: 2.5706507528832767

Epoch: 207| Step: 0
Training loss: 2.0832870732575604
Validation loss: 2.5854135772006264

Epoch: 6| Step: 1
Training loss: 2.180545104933828
Validation loss: 2.6166787179641036

Epoch: 6| Step: 2
Training loss: 1.945344855717365
Validation loss: 2.6096696668224246

Epoch: 6| Step: 3
Training loss: 2.077632989218891
Validation loss: 2.608066162059999

Epoch: 6| Step: 4
Training loss: 2.53323079537461
Validation loss: 2.620153282088473

Epoch: 6| Step: 5
Training loss: 2.490630330040564
Validation loss: 2.5729132483661714

Epoch: 6| Step: 6
Training loss: 2.2405461806901403
Validation loss: 2.5685951501807804

Epoch: 6| Step: 7
Training loss: 1.859207017507129
Validation loss: 2.566878416153404

Epoch: 6| Step: 8
Training loss: 2.5216307416610464
Validation loss: 2.55975296123711

Epoch: 6| Step: 9
Training loss: 2.7414468052914462
Validation loss: 2.5450500471619013

Epoch: 6| Step: 10
Training loss: 2.604421425120221
Validation loss: 2.5220744070309715

Epoch: 6| Step: 11
Training loss: 1.2713865837309821
Validation loss: 2.5058488694464076

Epoch: 6| Step: 12
Training loss: 1.8351584222162804
Validation loss: 2.5065202245479106

Epoch: 6| Step: 13
Training loss: 2.2889020948224537
Validation loss: 2.5286834719233813

Epoch: 208| Step: 0
Training loss: 1.737256242956872
Validation loss: 2.5659232185810947

Epoch: 6| Step: 1
Training loss: 2.4038702356159276
Validation loss: 2.6091110187331914

Epoch: 6| Step: 2
Training loss: 2.82304778035045
Validation loss: 2.6404343936905597

Epoch: 6| Step: 3
Training loss: 1.6047957371773967
Validation loss: 2.623683659946274

Epoch: 6| Step: 4
Training loss: 2.4556257779689847
Validation loss: 2.5845981290931763

Epoch: 6| Step: 5
Training loss: 2.348293325905993
Validation loss: 2.5853001173022507

Epoch: 6| Step: 6
Training loss: 1.7870091144391236
Validation loss: 2.593202157744079

Epoch: 6| Step: 7
Training loss: 2.160075898426463
Validation loss: 2.611433530633158

Epoch: 6| Step: 8
Training loss: 2.308316338358356
Validation loss: 2.5966755098364525

Epoch: 6| Step: 9
Training loss: 2.168655644103936
Validation loss: 2.5865764462412

Epoch: 6| Step: 10
Training loss: 1.659512599261649
Validation loss: 2.6030387164985958

Epoch: 6| Step: 11
Training loss: 2.57921859204144
Validation loss: 2.5635536012729534

Epoch: 6| Step: 12
Training loss: 2.4291936734390225
Validation loss: 2.571763978634541

Epoch: 6| Step: 13
Training loss: 2.1750353317021847
Validation loss: 2.574081965253196

Epoch: 209| Step: 0
Training loss: 1.9684305083178986
Validation loss: 2.567902940000281

Epoch: 6| Step: 1
Training loss: 2.353533656894097
Validation loss: 2.5457080359667885

Epoch: 6| Step: 2
Training loss: 2.083091823566632
Validation loss: 2.5481585991055336

Epoch: 6| Step: 3
Training loss: 2.1917544817525685
Validation loss: 2.532566975972272

Epoch: 6| Step: 4
Training loss: 2.2365482671737054
Validation loss: 2.5469850855713796

Epoch: 6| Step: 5
Training loss: 2.322598445518195
Validation loss: 2.5343502822717974

Epoch: 6| Step: 6
Training loss: 1.4232627049476572
Validation loss: 2.5367335980224284

Epoch: 6| Step: 7
Training loss: 3.087899272600162
Validation loss: 2.5434490947534876

Epoch: 6| Step: 8
Training loss: 2.4939364332527543
Validation loss: 2.560349267414448

Epoch: 6| Step: 9
Training loss: 2.575444718262493
Validation loss: 2.5798894681836746

Epoch: 6| Step: 10
Training loss: 1.543636592761222
Validation loss: 2.600179906269497

Epoch: 6| Step: 11
Training loss: 1.7423124396851701
Validation loss: 2.5942082134399627

Epoch: 6| Step: 12
Training loss: 2.2791172666153297
Validation loss: 2.659962292740954

Epoch: 6| Step: 13
Training loss: 2.1710137505221043
Validation loss: 2.6938802553386734

Epoch: 210| Step: 0
Training loss: 2.3313940254291294
Validation loss: 2.69080308006609

Epoch: 6| Step: 1
Training loss: 1.9779656898986544
Validation loss: 2.6981024267003333

Epoch: 6| Step: 2
Training loss: 2.1313545358029007
Validation loss: 2.638724115034824

Epoch: 6| Step: 3
Training loss: 2.2829205392785066
Validation loss: 2.595123754501503

Epoch: 6| Step: 4
Training loss: 2.1180620815266895
Validation loss: 2.590002746457837

Epoch: 6| Step: 5
Training loss: 2.345567735866956
Validation loss: 2.562237857963156

Epoch: 6| Step: 6
Training loss: 1.8398565142081449
Validation loss: 2.538994264908441

Epoch: 6| Step: 7
Training loss: 1.8770047596508845
Validation loss: 2.5261778621072732

Epoch: 6| Step: 8
Training loss: 1.8212538483107104
Validation loss: 2.5106132768011133

Epoch: 6| Step: 9
Training loss: 2.2919652455342905
Validation loss: 2.5160404912624683

Epoch: 6| Step: 10
Training loss: 2.7201457332390473
Validation loss: 2.552962402183946

Epoch: 6| Step: 11
Training loss: 2.44406017861831
Validation loss: 2.5568162174409967

Epoch: 6| Step: 12
Training loss: 2.0709210347179714
Validation loss: 2.581324416620378

Epoch: 6| Step: 13
Training loss: 2.6438650985445076
Validation loss: 2.6104839504021102

Epoch: 211| Step: 0
Training loss: 1.8444661834628104
Validation loss: 2.652714789032761

Epoch: 6| Step: 1
Training loss: 2.087928764487364
Validation loss: 2.6273506024562723

Epoch: 6| Step: 2
Training loss: 2.16696204103918
Validation loss: 2.6290239493864545

Epoch: 6| Step: 3
Training loss: 2.309281454914644
Validation loss: 2.63659449155029

Epoch: 6| Step: 4
Training loss: 2.1173778040193048
Validation loss: 2.6002825241155123

Epoch: 6| Step: 5
Training loss: 2.5357256280810394
Validation loss: 2.628705663713767

Epoch: 6| Step: 6
Training loss: 1.6023945949602312
Validation loss: 2.608514640121124

Epoch: 6| Step: 7
Training loss: 2.4017569468796722
Validation loss: 2.605858967549967

Epoch: 6| Step: 8
Training loss: 2.751542525730711
Validation loss: 2.640947777397053

Epoch: 6| Step: 9
Training loss: 2.1020531063975234
Validation loss: 2.633195632239554

Epoch: 6| Step: 10
Training loss: 2.3431544246540557
Validation loss: 2.6388311368634803

Epoch: 6| Step: 11
Training loss: 2.279248130035219
Validation loss: 2.6509253745562638

Epoch: 6| Step: 12
Training loss: 2.401375527384867
Validation loss: 2.611620007686268

Epoch: 6| Step: 13
Training loss: 1.979922247429297
Validation loss: 2.536174614696086

Epoch: 212| Step: 0
Training loss: 2.704568113868858
Validation loss: 2.5003138345191083

Epoch: 6| Step: 1
Training loss: 1.3157604682385613
Validation loss: 2.4979119481881833

Epoch: 6| Step: 2
Training loss: 1.9004319001250582
Validation loss: 2.4961608336443066

Epoch: 6| Step: 3
Training loss: 2.777133338136523
Validation loss: 2.4881656366355034

Epoch: 6| Step: 4
Training loss: 2.012209578699704
Validation loss: 2.4870216858642724

Epoch: 6| Step: 5
Training loss: 2.523395360825111
Validation loss: 2.4915148186954603

Epoch: 6| Step: 6
Training loss: 2.251505983531506
Validation loss: 2.487010757219022

Epoch: 6| Step: 7
Training loss: 2.4476113528052412
Validation loss: 2.505485921114366

Epoch: 6| Step: 8
Training loss: 2.3943436566250274
Validation loss: 2.4819397176711657

Epoch: 6| Step: 9
Training loss: 2.2094210219277426
Validation loss: 2.5091716850261827

Epoch: 6| Step: 10
Training loss: 1.9702679065616202
Validation loss: 2.4971167150190907

Epoch: 6| Step: 11
Training loss: 2.7037411026228466
Validation loss: 2.5027873871600343

Epoch: 6| Step: 12
Training loss: 2.1823928069088434
Validation loss: 2.553427273596734

Epoch: 6| Step: 13
Training loss: 1.8432942489886124
Validation loss: 2.5647616833902465

Epoch: 213| Step: 0
Training loss: 2.403449274018647
Validation loss: 2.553683014444118

Epoch: 6| Step: 1
Training loss: 1.4682705685165205
Validation loss: 2.5737900480687386

Epoch: 6| Step: 2
Training loss: 2.3094146353452376
Validation loss: 2.5523820559520276

Epoch: 6| Step: 3
Training loss: 2.316608079219767
Validation loss: 2.5622657575775047

Epoch: 6| Step: 4
Training loss: 2.144788827349238
Validation loss: 2.5209268494801855

Epoch: 6| Step: 5
Training loss: 2.6280557920435688
Validation loss: 2.536028899408212

Epoch: 6| Step: 6
Training loss: 2.020057712901329
Validation loss: 2.5290939976298734

Epoch: 6| Step: 7
Training loss: 2.184155223088254
Validation loss: 2.586282420900859

Epoch: 6| Step: 8
Training loss: 2.0782447723062276
Validation loss: 2.6027426577405777

Epoch: 6| Step: 9
Training loss: 2.115376278387327
Validation loss: 2.625479260850616

Epoch: 6| Step: 10
Training loss: 2.5430255655015412
Validation loss: 2.6303482522038224

Epoch: 6| Step: 11
Training loss: 2.3989835732562694
Validation loss: 2.626272907135995

Epoch: 6| Step: 12
Training loss: 1.7823464214832976
Validation loss: 2.64333709847213

Epoch: 6| Step: 13
Training loss: 2.5646368281974885
Validation loss: 2.6093317359489943

Epoch: 214| Step: 0
Training loss: 2.29082434230505
Validation loss: 2.6008405781597848

Epoch: 6| Step: 1
Training loss: 2.3658608725909964
Validation loss: 2.583198625887634

Epoch: 6| Step: 2
Training loss: 1.6231639466370467
Validation loss: 2.5636366362472165

Epoch: 6| Step: 3
Training loss: 2.8360802480537957
Validation loss: 2.590846171215893

Epoch: 6| Step: 4
Training loss: 1.7433451411550624
Validation loss: 2.587560000622339

Epoch: 6| Step: 5
Training loss: 2.620286342465268
Validation loss: 2.594571684327283

Epoch: 6| Step: 6
Training loss: 1.6178173368255304
Validation loss: 2.56422890125714

Epoch: 6| Step: 7
Training loss: 1.8814357931964882
Validation loss: 2.537587430853981

Epoch: 6| Step: 8
Training loss: 1.8391308871431487
Validation loss: 2.5140180253101145

Epoch: 6| Step: 9
Training loss: 1.9841014042673755
Validation loss: 2.506579031397328

Epoch: 6| Step: 10
Training loss: 2.498279551749356
Validation loss: 2.475392768340838

Epoch: 6| Step: 11
Training loss: 2.6791094720963833
Validation loss: 2.4881480614006195

Epoch: 6| Step: 12
Training loss: 1.9798027890182752
Validation loss: 2.5056289721538585

Epoch: 6| Step: 13
Training loss: 2.5471721078021283
Validation loss: 2.5140355461387465

Epoch: 215| Step: 0
Training loss: 2.4342230263357876
Validation loss: 2.548787886690481

Epoch: 6| Step: 1
Training loss: 2.191949950308118
Validation loss: 2.564446152200393

Epoch: 6| Step: 2
Training loss: 2.102971168548987
Validation loss: 2.609098545437266

Epoch: 6| Step: 3
Training loss: 2.556611529389444
Validation loss: 2.6250202919917456

Epoch: 6| Step: 4
Training loss: 2.5989401491149504
Validation loss: 2.667579291064086

Epoch: 6| Step: 5
Training loss: 2.0793797426482117
Validation loss: 2.64488655832585

Epoch: 6| Step: 6
Training loss: 2.306691995253813
Validation loss: 2.615663469974332

Epoch: 6| Step: 7
Training loss: 2.115337844811762
Validation loss: 2.5624031730904546

Epoch: 6| Step: 8
Training loss: 2.571685053507683
Validation loss: 2.512354238909627

Epoch: 6| Step: 9
Training loss: 1.8719611019144304
Validation loss: 2.5039370530446132

Epoch: 6| Step: 10
Training loss: 1.9218104902156026
Validation loss: 2.5027897528125385

Epoch: 6| Step: 11
Training loss: 1.8137632768856187
Validation loss: 2.4715691899348835

Epoch: 6| Step: 12
Training loss: 1.8954636629083677
Validation loss: 2.473830704994127

Epoch: 6| Step: 13
Training loss: 2.5616257269356666
Validation loss: 2.4716004602984465

Epoch: 216| Step: 0
Training loss: 2.1382718482906666
Validation loss: 2.482151131781141

Epoch: 6| Step: 1
Training loss: 1.9531147460668334
Validation loss: 2.48818738791287

Epoch: 6| Step: 2
Training loss: 2.0345636189531824
Validation loss: 2.517519511943059

Epoch: 6| Step: 3
Training loss: 1.519069807501573
Validation loss: 2.524058609021533

Epoch: 6| Step: 4
Training loss: 1.9050115095376583
Validation loss: 2.5243949687734957

Epoch: 6| Step: 5
Training loss: 2.573636858965097
Validation loss: 2.5953116484106094

Epoch: 6| Step: 6
Training loss: 2.0880929618690893
Validation loss: 2.6004434953147175

Epoch: 6| Step: 7
Training loss: 2.4923554845178195
Validation loss: 2.660152605215779

Epoch: 6| Step: 8
Training loss: 2.416226007033578
Validation loss: 2.6256245521037127

Epoch: 6| Step: 9
Training loss: 2.770652038456053
Validation loss: 2.651144379210486

Epoch: 6| Step: 10
Training loss: 1.8945602572078641
Validation loss: 2.6991473205507504

Epoch: 6| Step: 11
Training loss: 2.2303056273128004
Validation loss: 2.7242081687436586

Epoch: 6| Step: 12
Training loss: 2.180637603749582
Validation loss: 2.7530143862790784

Epoch: 6| Step: 13
Training loss: 2.5208441573822262
Validation loss: 2.721698214793304

Epoch: 217| Step: 0
Training loss: 2.3775368242590917
Validation loss: 2.65815359980863

Epoch: 6| Step: 1
Training loss: 2.107573920024904
Validation loss: 2.6114991730482915

Epoch: 6| Step: 2
Training loss: 2.115920923179648
Validation loss: 2.5792111353412173

Epoch: 6| Step: 3
Training loss: 2.398290474327342
Validation loss: 2.541416419065863

Epoch: 6| Step: 4
Training loss: 1.9816798132013054
Validation loss: 2.5011223022308107

Epoch: 6| Step: 5
Training loss: 2.067994053955679
Validation loss: 2.4872203024289052

Epoch: 6| Step: 6
Training loss: 2.253084294008323
Validation loss: 2.472364817274034

Epoch: 6| Step: 7
Training loss: 2.3843832873279145
Validation loss: 2.457559250873735

Epoch: 6| Step: 8
Training loss: 2.6827347670298516
Validation loss: 2.4841824502922334

Epoch: 6| Step: 9
Training loss: 2.145705605072638
Validation loss: 2.4694686117397358

Epoch: 6| Step: 10
Training loss: 2.2018734237920796
Validation loss: 2.4801307427901054

Epoch: 6| Step: 11
Training loss: 2.174123025994487
Validation loss: 2.4913602151462495

Epoch: 6| Step: 12
Training loss: 2.172561207777145
Validation loss: 2.542878586381208

Epoch: 6| Step: 13
Training loss: 2.144374487311742
Validation loss: 2.580812413746004

Epoch: 218| Step: 0
Training loss: 2.6364011672167713
Validation loss: 2.584454398298224

Epoch: 6| Step: 1
Training loss: 1.5026219023699654
Validation loss: 2.6070708344521507

Epoch: 6| Step: 2
Training loss: 2.2985499496492814
Validation loss: 2.632497827155269

Epoch: 6| Step: 3
Training loss: 1.854209456503673
Validation loss: 2.638859883087652

Epoch: 6| Step: 4
Training loss: 2.2136133399280182
Validation loss: 2.6128711388142163

Epoch: 6| Step: 5
Training loss: 2.7591979785441723
Validation loss: 2.5726737990149076

Epoch: 6| Step: 6
Training loss: 1.54312774225349
Validation loss: 2.5787517055617397

Epoch: 6| Step: 7
Training loss: 2.175731391266987
Validation loss: 2.5592284213777954

Epoch: 6| Step: 8
Training loss: 2.4684069431343896
Validation loss: 2.583532376978764

Epoch: 6| Step: 9
Training loss: 1.8097568010312282
Validation loss: 2.5940156287406553

Epoch: 6| Step: 10
Training loss: 2.2624756068816785
Validation loss: 2.5750391052881554

Epoch: 6| Step: 11
Training loss: 1.7437630273475906
Validation loss: 2.577726067397987

Epoch: 6| Step: 12
Training loss: 2.370908525701414
Validation loss: 2.5416624871725104

Epoch: 6| Step: 13
Training loss: 2.394657200150303
Validation loss: 2.520019302609606

Epoch: 219| Step: 0
Training loss: 2.6431893081040077
Validation loss: 2.5415152885091743

Epoch: 6| Step: 1
Training loss: 1.6043884652452813
Validation loss: 2.549140421904881

Epoch: 6| Step: 2
Training loss: 2.3827732770849672
Validation loss: 2.549170491304134

Epoch: 6| Step: 3
Training loss: 1.9027385576139937
Validation loss: 2.541059239037895

Epoch: 6| Step: 4
Training loss: 2.018927301232601
Validation loss: 2.5364555238764335

Epoch: 6| Step: 5
Training loss: 2.2802918394600087
Validation loss: 2.53887743153156

Epoch: 6| Step: 6
Training loss: 1.9347784555257361
Validation loss: 2.5731298510203278

Epoch: 6| Step: 7
Training loss: 2.010751436127953
Validation loss: 2.582261929809498

Epoch: 6| Step: 8
Training loss: 2.0404181523077787
Validation loss: 2.5995666515681086

Epoch: 6| Step: 9
Training loss: 2.128050914040598
Validation loss: 2.6423806067098567

Epoch: 6| Step: 10
Training loss: 1.6362848184609715
Validation loss: 2.613194515991841

Epoch: 6| Step: 11
Training loss: 2.840708520918306
Validation loss: 2.60323619750045

Epoch: 6| Step: 12
Training loss: 2.5464152289721063
Validation loss: 2.6011760680510547

Epoch: 6| Step: 13
Training loss: 1.9348244805783137
Validation loss: 2.5986806737751604

Epoch: 220| Step: 0
Training loss: 1.9680752279045763
Validation loss: 2.5532138318748108

Epoch: 6| Step: 1
Training loss: 2.528210545850157
Validation loss: 2.570836045765077

Epoch: 6| Step: 2
Training loss: 1.759278834477581
Validation loss: 2.5339086408467204

Epoch: 6| Step: 3
Training loss: 1.8694704536088567
Validation loss: 2.522235989801453

Epoch: 6| Step: 4
Training loss: 2.585417173653511
Validation loss: 2.4937793586551598

Epoch: 6| Step: 5
Training loss: 2.1431675867725968
Validation loss: 2.489582099009784

Epoch: 6| Step: 6
Training loss: 2.455492954213748
Validation loss: 2.4790915523230685

Epoch: 6| Step: 7
Training loss: 1.9017313572709635
Validation loss: 2.4690300826852045

Epoch: 6| Step: 8
Training loss: 2.297852911342836
Validation loss: 2.466994389680571

Epoch: 6| Step: 9
Training loss: 2.10115733725828
Validation loss: 2.4919737721221753

Epoch: 6| Step: 10
Training loss: 2.2976801492588868
Validation loss: 2.49865080666606

Epoch: 6| Step: 11
Training loss: 2.5062014910410313
Validation loss: 2.51672834284998

Epoch: 6| Step: 12
Training loss: 2.2347057604560447
Validation loss: 2.556346514060474

Epoch: 6| Step: 13
Training loss: 1.7120939901166794
Validation loss: 2.5526641548069993

Epoch: 221| Step: 0
Training loss: 1.6832150345330295
Validation loss: 2.603003697433122

Epoch: 6| Step: 1
Training loss: 1.912011117719305
Validation loss: 2.6307840957198754

Epoch: 6| Step: 2
Training loss: 1.8837623574523004
Validation loss: 2.628677259942243

Epoch: 6| Step: 3
Training loss: 2.0738794081528815
Validation loss: 2.583766629172155

Epoch: 6| Step: 4
Training loss: 2.106579998664063
Validation loss: 2.5327345180096654

Epoch: 6| Step: 5
Training loss: 2.839932069100332
Validation loss: 2.5066848548861698

Epoch: 6| Step: 6
Training loss: 2.2175470234079344
Validation loss: 2.4828702420177553

Epoch: 6| Step: 7
Training loss: 2.3068528170618476
Validation loss: 2.463178585037164

Epoch: 6| Step: 8
Training loss: 2.345269587948266
Validation loss: 2.476196615024614

Epoch: 6| Step: 9
Training loss: 2.516872499482225
Validation loss: 2.4897374753172565

Epoch: 6| Step: 10
Training loss: 2.217237467401028
Validation loss: 2.4913244317536862

Epoch: 6| Step: 11
Training loss: 2.407097209062599
Validation loss: 2.494605506741932

Epoch: 6| Step: 12
Training loss: 2.0468288736751665
Validation loss: 2.5008969128551937

Epoch: 6| Step: 13
Training loss: 1.80712099029778
Validation loss: 2.533373024801048

Epoch: 222| Step: 0
Training loss: 1.8314737801639362
Validation loss: 2.5659977603777704

Epoch: 6| Step: 1
Training loss: 2.172076538395335
Validation loss: 2.6070934226891658

Epoch: 6| Step: 2
Training loss: 2.6683427986900035
Validation loss: 2.664742644923313

Epoch: 6| Step: 3
Training loss: 2.041068424107739
Validation loss: 2.7510795063929727

Epoch: 6| Step: 4
Training loss: 2.282297494852717
Validation loss: 2.7714241994463746

Epoch: 6| Step: 5
Training loss: 2.6915416932594516
Validation loss: 2.736006273777407

Epoch: 6| Step: 6
Training loss: 1.8954803291855782
Validation loss: 2.628057183091122

Epoch: 6| Step: 7
Training loss: 2.112713016401625
Validation loss: 2.542777933754098

Epoch: 6| Step: 8
Training loss: 2.4943253008910125
Validation loss: 2.4775948122874527

Epoch: 6| Step: 9
Training loss: 2.4647549524045655
Validation loss: 2.473985006623616

Epoch: 6| Step: 10
Training loss: 2.0703148032121623
Validation loss: 2.4852502908415177

Epoch: 6| Step: 11
Training loss: 2.356272191922134
Validation loss: 2.4792526187475796

Epoch: 6| Step: 12
Training loss: 2.1076870416471567
Validation loss: 2.491383150698787

Epoch: 6| Step: 13
Training loss: 2.3882727925129323
Validation loss: 2.4988285340000758

Epoch: 223| Step: 0
Training loss: 2.253694468156307
Validation loss: 2.4831815048075887

Epoch: 6| Step: 1
Training loss: 2.235884150115846
Validation loss: 2.4573985170487145

Epoch: 6| Step: 2
Training loss: 2.2807319784542734
Validation loss: 2.450253289941757

Epoch: 6| Step: 3
Training loss: 1.8177876131827462
Validation loss: 2.443259583002442

Epoch: 6| Step: 4
Training loss: 2.1618722787372877
Validation loss: 2.476380968310011

Epoch: 6| Step: 5
Training loss: 1.7140644362098796
Validation loss: 2.5144472702200784

Epoch: 6| Step: 6
Training loss: 2.54941006736644
Validation loss: 2.548483802660115

Epoch: 6| Step: 7
Training loss: 2.440185046136298
Validation loss: 2.557792697364555

Epoch: 6| Step: 8
Training loss: 2.1323107454098325
Validation loss: 2.5616344602848184

Epoch: 6| Step: 9
Training loss: 2.2714696803318755
Validation loss: 2.574372058831844

Epoch: 6| Step: 10
Training loss: 2.412494236440148
Validation loss: 2.585519532138196

Epoch: 6| Step: 11
Training loss: 2.5197324214140084
Validation loss: 2.591278123913058

Epoch: 6| Step: 12
Training loss: 2.628274237968843
Validation loss: 2.5533243289055014

Epoch: 6| Step: 13
Training loss: 2.328757897699942
Validation loss: 2.5415780307344193

Epoch: 224| Step: 0
Training loss: 2.3385886773111575
Validation loss: 2.548980747296355

Epoch: 6| Step: 1
Training loss: 2.327515592758444
Validation loss: 2.5444952177019156

Epoch: 6| Step: 2
Training loss: 2.6109753115716585
Validation loss: 2.5276036155936423

Epoch: 6| Step: 3
Training loss: 2.1155593073012016
Validation loss: 2.547274232555491

Epoch: 6| Step: 4
Training loss: 1.892811984168718
Validation loss: 2.569697916284327

Epoch: 6| Step: 5
Training loss: 2.2595173560073376
Validation loss: 2.569825502154067

Epoch: 6| Step: 6
Training loss: 2.15824167898963
Validation loss: 2.571776014967064

Epoch: 6| Step: 7
Training loss: 1.7580513346861764
Validation loss: 2.582019399980293

Epoch: 6| Step: 8
Training loss: 1.9511182928508317
Validation loss: 2.5893144790730367

Epoch: 6| Step: 9
Training loss: 2.31350366617592
Validation loss: 2.572611004257481

Epoch: 6| Step: 10
Training loss: 1.5903623892510435
Validation loss: 2.5411190839734212

Epoch: 6| Step: 11
Training loss: 1.7460025362431129
Validation loss: 2.5358749961710747

Epoch: 6| Step: 12
Training loss: 2.241457724021727
Validation loss: 2.5476024189564654

Epoch: 6| Step: 13
Training loss: 2.857946820906213
Validation loss: 2.552190160786155

Epoch: 225| Step: 0
Training loss: 1.8030468637868164
Validation loss: 2.5886705291009275

Epoch: 6| Step: 1
Training loss: 2.122831247674557
Validation loss: 2.5809245467046638

Epoch: 6| Step: 2
Training loss: 3.009795726548856
Validation loss: 2.6110967791842605

Epoch: 6| Step: 3
Training loss: 2.6393738440462076
Validation loss: 2.631839093057207

Epoch: 6| Step: 4
Training loss: 2.2092246170338363
Validation loss: 2.657316659420789

Epoch: 6| Step: 5
Training loss: 1.4583511714752595
Validation loss: 2.6751476865030197

Epoch: 6| Step: 6
Training loss: 1.7255371639524049
Validation loss: 2.6599876735038346

Epoch: 6| Step: 7
Training loss: 2.096520142865825
Validation loss: 2.5904205661879525

Epoch: 6| Step: 8
Training loss: 1.820266919527242
Validation loss: 2.541672836879891

Epoch: 6| Step: 9
Training loss: 2.2558333754589395
Validation loss: 2.492138725608674

Epoch: 6| Step: 10
Training loss: 2.305588988811134
Validation loss: 2.477228149424462

Epoch: 6| Step: 11
Training loss: 2.5809724746294607
Validation loss: 2.4558083989025614

Epoch: 6| Step: 12
Training loss: 1.931355732119088
Validation loss: 2.4383602091658965

Epoch: 6| Step: 13
Training loss: 2.3673921502914803
Validation loss: 2.455310261266425

Epoch: 226| Step: 0
Training loss: 1.9125762151354342
Validation loss: 2.478729759848007

Epoch: 6| Step: 1
Training loss: 2.4937066975517204
Validation loss: 2.4591987482079114

Epoch: 6| Step: 2
Training loss: 2.223816902136346
Validation loss: 2.4838756163338793

Epoch: 6| Step: 3
Training loss: 1.717758586357419
Validation loss: 2.5012605191374497

Epoch: 6| Step: 4
Training loss: 1.9463637854917135
Validation loss: 2.4958165770548275

Epoch: 6| Step: 5
Training loss: 2.1042474661947
Validation loss: 2.565281792443459

Epoch: 6| Step: 6
Training loss: 2.8012496577836745
Validation loss: 2.578938334705635

Epoch: 6| Step: 7
Training loss: 1.6816468631925525
Validation loss: 2.6442178214585366

Epoch: 6| Step: 8
Training loss: 2.386823436482757
Validation loss: 2.7101168457638636

Epoch: 6| Step: 9
Training loss: 2.2594963579311003
Validation loss: 2.7541838521801156

Epoch: 6| Step: 10
Training loss: 2.0891764778766038
Validation loss: 2.7407032281606223

Epoch: 6| Step: 11
Training loss: 2.312531548362186
Validation loss: 2.7042378980975847

Epoch: 6| Step: 12
Training loss: 2.1148756855532285
Validation loss: 2.61348344584194

Epoch: 6| Step: 13
Training loss: 2.4876009551605094
Validation loss: 2.55084912305712

Epoch: 227| Step: 0
Training loss: 2.2381259404968485
Validation loss: 2.529103518917411

Epoch: 6| Step: 1
Training loss: 2.0756458610168655
Validation loss: 2.4784373538316604

Epoch: 6| Step: 2
Training loss: 2.799763471967302
Validation loss: 2.477198169207737

Epoch: 6| Step: 3
Training loss: 2.415574177630548
Validation loss: 2.4664475532676584

Epoch: 6| Step: 4
Training loss: 2.6485047458449884
Validation loss: 2.4521098050935985

Epoch: 6| Step: 5
Training loss: 1.8122474889139257
Validation loss: 2.464032619555587

Epoch: 6| Step: 6
Training loss: 1.8532546797306964
Validation loss: 2.4644327682546825

Epoch: 6| Step: 7
Training loss: 1.4882839733509368
Validation loss: 2.4774068682603847

Epoch: 6| Step: 8
Training loss: 1.4994036760724512
Validation loss: 2.497772368737847

Epoch: 6| Step: 9
Training loss: 2.058006941313757
Validation loss: 2.5145720337553543

Epoch: 6| Step: 10
Training loss: 1.436410906006201
Validation loss: 2.516460382668255

Epoch: 6| Step: 11
Training loss: 1.4154912990732493
Validation loss: 2.567770956538316

Epoch: 6| Step: 12
Training loss: 2.71940762688982
Validation loss: 2.6150240026506615

Epoch: 6| Step: 13
Training loss: 3.2692128599548007
Validation loss: 2.6471126397174505

Epoch: 228| Step: 0
Training loss: 1.162056237296163
Validation loss: 2.6330313360501254

Epoch: 6| Step: 1
Training loss: 2.286786298467703
Validation loss: 2.676115481995817

Epoch: 6| Step: 2
Training loss: 2.3472927529336545
Validation loss: 2.620790845913711

Epoch: 6| Step: 3
Training loss: 2.085738840537159
Validation loss: 2.5895727292920685

Epoch: 6| Step: 4
Training loss: 2.4229873563819955
Validation loss: 2.5659146701780653

Epoch: 6| Step: 5
Training loss: 2.79815354772971
Validation loss: 2.5304005300291146

Epoch: 6| Step: 6
Training loss: 1.5046347698982445
Validation loss: 2.521283043925546

Epoch: 6| Step: 7
Training loss: 2.367221240316217
Validation loss: 2.521839687095448

Epoch: 6| Step: 8
Training loss: 1.527933148469532
Validation loss: 2.4962167246916382

Epoch: 6| Step: 9
Training loss: 3.213093648600634
Validation loss: 2.501797467009624

Epoch: 6| Step: 10
Training loss: 1.422104869328875
Validation loss: 2.4888951508929558

Epoch: 6| Step: 11
Training loss: 1.5558642050907325
Validation loss: 2.498116435339381

Epoch: 6| Step: 12
Training loss: 2.63488416355613
Validation loss: 2.503161354769296

Epoch: 6| Step: 13
Training loss: 2.156210083522913
Validation loss: 2.503744896794497

Epoch: 229| Step: 0
Training loss: 2.2161644928472004
Validation loss: 2.538396808705597

Epoch: 6| Step: 1
Training loss: 1.1183098443961876
Validation loss: 2.562283607588305

Epoch: 6| Step: 2
Training loss: 1.5653223106791183
Validation loss: 2.631069704525059

Epoch: 6| Step: 3
Training loss: 2.507369908526625
Validation loss: 2.6483692771804295

Epoch: 6| Step: 4
Training loss: 2.4290619563135607
Validation loss: 2.679181554425662

Epoch: 6| Step: 5
Training loss: 2.280936650742345
Validation loss: 2.706411109927147

Epoch: 6| Step: 6
Training loss: 1.7646390474509783
Validation loss: 2.6573017955085043

Epoch: 6| Step: 7
Training loss: 2.5086683672675254
Validation loss: 2.5795506629565352

Epoch: 6| Step: 8
Training loss: 2.4284945684179426
Validation loss: 2.5137409713484336

Epoch: 6| Step: 9
Training loss: 1.2691403300624609
Validation loss: 2.5010690628536216

Epoch: 6| Step: 10
Training loss: 3.0997353656163984
Validation loss: 2.487797137585442

Epoch: 6| Step: 11
Training loss: 2.076000188581278
Validation loss: 2.4725974961341763

Epoch: 6| Step: 12
Training loss: 1.998089473863156
Validation loss: 2.4691245362310283

Epoch: 6| Step: 13
Training loss: 2.603880253618262
Validation loss: 2.4767261214412617

Epoch: 230| Step: 0
Training loss: 2.2432379305629744
Validation loss: 2.4989078997385503

Epoch: 6| Step: 1
Training loss: 2.0165309792051285
Validation loss: 2.55099173315631

Epoch: 6| Step: 2
Training loss: 1.5277899751272637
Validation loss: 2.5602049590804645

Epoch: 6| Step: 3
Training loss: 1.8110009933544786
Validation loss: 2.620109066149068

Epoch: 6| Step: 4
Training loss: 2.1525189835512113
Validation loss: 2.6290211380846

Epoch: 6| Step: 5
Training loss: 1.9902583935966356
Validation loss: 2.6431000977121313

Epoch: 6| Step: 6
Training loss: 2.1696458532816156
Validation loss: 2.634173772388574

Epoch: 6| Step: 7
Training loss: 2.1249188800362755
Validation loss: 2.604168309529104

Epoch: 6| Step: 8
Training loss: 2.4433645466384615
Validation loss: 2.567684279220192

Epoch: 6| Step: 9
Training loss: 2.2434233871379514
Validation loss: 2.558684664365609

Epoch: 6| Step: 10
Training loss: 2.2100377493003416
Validation loss: 2.537183541732841

Epoch: 6| Step: 11
Training loss: 1.7453811864220772
Validation loss: 2.5453941783285248

Epoch: 6| Step: 12
Training loss: 2.4419988538597224
Validation loss: 2.5206018662126133

Epoch: 6| Step: 13
Training loss: 2.558868626396393
Validation loss: 2.5746707471199173

Epoch: 231| Step: 0
Training loss: 2.1729922268373416
Validation loss: 2.588198070377446

Epoch: 6| Step: 1
Training loss: 1.831420536367951
Validation loss: 2.5864150575378657

Epoch: 6| Step: 2
Training loss: 1.7887318401361194
Validation loss: 2.5895333389362496

Epoch: 6| Step: 3
Training loss: 2.5421897996025726
Validation loss: 2.579585022241574

Epoch: 6| Step: 4
Training loss: 2.5740278575548685
Validation loss: 2.5975040400757883

Epoch: 6| Step: 5
Training loss: 1.330308129677775
Validation loss: 2.5768786721978647

Epoch: 6| Step: 6
Training loss: 2.294759801457817
Validation loss: 2.5719805776526203

Epoch: 6| Step: 7
Training loss: 2.5844080340524034
Validation loss: 2.5950560364770454

Epoch: 6| Step: 8
Training loss: 1.8143142302109756
Validation loss: 2.612456704348288

Epoch: 6| Step: 9
Training loss: 1.9923911794623845
Validation loss: 2.6279513857751473

Epoch: 6| Step: 10
Training loss: 1.8880944459091482
Validation loss: 2.611586206807923

Epoch: 6| Step: 11
Training loss: 2.672817041845264
Validation loss: 2.6290831220585775

Epoch: 6| Step: 12
Training loss: 1.6163991582243837
Validation loss: 2.5926123655854423

Epoch: 6| Step: 13
Training loss: 2.253468806560209
Validation loss: 2.6139283645366023

Epoch: 232| Step: 0
Training loss: 2.59758640281448
Validation loss: 2.6045335943484704

Epoch: 6| Step: 1
Training loss: 1.4964946478618297
Validation loss: 2.5892995163748562

Epoch: 6| Step: 2
Training loss: 1.6109911757072994
Validation loss: 2.542948514249404

Epoch: 6| Step: 3
Training loss: 1.5199626712230867
Validation loss: 2.5938893093094175

Epoch: 6| Step: 4
Training loss: 2.4209230582526766
Validation loss: 2.552919917622617

Epoch: 6| Step: 5
Training loss: 2.3179828495513526
Validation loss: 2.589949554384131

Epoch: 6| Step: 6
Training loss: 1.472090797236236
Validation loss: 2.594555756452119

Epoch: 6| Step: 7
Training loss: 1.903621925659623
Validation loss: 2.593644703026087

Epoch: 6| Step: 8
Training loss: 2.3156467369799394
Validation loss: 2.574699129347742

Epoch: 6| Step: 9
Training loss: 2.4846438856186785
Validation loss: 2.5742985853396907

Epoch: 6| Step: 10
Training loss: 2.1126015181667714
Validation loss: 2.5790858934497756

Epoch: 6| Step: 11
Training loss: 2.40125261043504
Validation loss: 2.609141935192515

Epoch: 6| Step: 12
Training loss: 2.444098613098108
Validation loss: 2.6145164562248997

Epoch: 6| Step: 13
Training loss: 2.2203256089899344
Validation loss: 2.590550789807438

Epoch: 233| Step: 0
Training loss: 2.776602387235698
Validation loss: 2.594688536805994

Epoch: 6| Step: 1
Training loss: 1.4010395584185122
Validation loss: 2.6349988814803376

Epoch: 6| Step: 2
Training loss: 1.4410652232766659
Validation loss: 2.6151314777608694

Epoch: 6| Step: 3
Training loss: 2.1125385438912754
Validation loss: 2.6059277085928745

Epoch: 6| Step: 4
Training loss: 2.468334694225993
Validation loss: 2.6176774358741195

Epoch: 6| Step: 5
Training loss: 2.3293918516687153
Validation loss: 2.594612514376063

Epoch: 6| Step: 6
Training loss: 2.2697527936524744
Validation loss: 2.5623358231961615

Epoch: 6| Step: 7
Training loss: 2.183467963729446
Validation loss: 2.5323777374951235

Epoch: 6| Step: 8
Training loss: 1.6618356625024748
Validation loss: 2.498449448713399

Epoch: 6| Step: 9
Training loss: 2.0845805694462123
Validation loss: 2.513816088353534

Epoch: 6| Step: 10
Training loss: 2.251948043142575
Validation loss: 2.4958536732077823

Epoch: 6| Step: 11
Training loss: 2.508013280513956
Validation loss: 2.5302977713530423

Epoch: 6| Step: 12
Training loss: 1.738889574911343
Validation loss: 2.512116443717216

Epoch: 6| Step: 13
Training loss: 2.3590838524249658
Validation loss: 2.528135345714448

Epoch: 234| Step: 0
Training loss: 1.918958792990977
Validation loss: 2.557452961379422

Epoch: 6| Step: 1
Training loss: 1.932700704241739
Validation loss: 2.5778618418160684

Epoch: 6| Step: 2
Training loss: 1.8874391558610935
Validation loss: 2.596520212284926

Epoch: 6| Step: 3
Training loss: 2.0360948737472677
Validation loss: 2.631918071345169

Epoch: 6| Step: 4
Training loss: 2.3593345790204743
Validation loss: 2.6555048477070184

Epoch: 6| Step: 5
Training loss: 1.5140626858139317
Validation loss: 2.6542617088160934

Epoch: 6| Step: 6
Training loss: 1.930319246364005
Validation loss: 2.624447719059788

Epoch: 6| Step: 7
Training loss: 2.263755500571921
Validation loss: 2.6013679469833257

Epoch: 6| Step: 8
Training loss: 2.7059163581682166
Validation loss: 2.5728236939676203

Epoch: 6| Step: 9
Training loss: 2.101434526482278
Validation loss: 2.555674339956925

Epoch: 6| Step: 10
Training loss: 2.4920629392485796
Validation loss: 2.5420252333398348

Epoch: 6| Step: 11
Training loss: 1.629605296503972
Validation loss: 2.522517395766092

Epoch: 6| Step: 12
Training loss: 2.751243570225788
Validation loss: 2.5470825453992645

Epoch: 6| Step: 13
Training loss: 1.3505674017505171
Validation loss: 2.5233062221837756

Epoch: 235| Step: 0
Training loss: 2.157629594502155
Validation loss: 2.5368769702964813

Epoch: 6| Step: 1
Training loss: 2.555779833097392
Validation loss: 2.534478455990343

Epoch: 6| Step: 2
Training loss: 2.0853866249788955
Validation loss: 2.588075689260432

Epoch: 6| Step: 3
Training loss: 1.5174990993038313
Validation loss: 2.5964073448640272

Epoch: 6| Step: 4
Training loss: 2.768012185742451
Validation loss: 2.6354588491408726

Epoch: 6| Step: 5
Training loss: 2.2433588776213425
Validation loss: 2.653439085408855

Epoch: 6| Step: 6
Training loss: 2.073230802672757
Validation loss: 2.6234924892276585

Epoch: 6| Step: 7
Training loss: 2.2389400770816104
Validation loss: 2.6389369949060746

Epoch: 6| Step: 8
Training loss: 1.750441767927186
Validation loss: 2.6487269435621372

Epoch: 6| Step: 9
Training loss: 1.9587023637598908
Validation loss: 2.633134363802023

Epoch: 6| Step: 10
Training loss: 1.7021449357662632
Validation loss: 2.559401088235412

Epoch: 6| Step: 11
Training loss: 1.8017265172375274
Validation loss: 2.5229558022380396

Epoch: 6| Step: 12
Training loss: 2.0042987164028156
Validation loss: 2.5283006824739678

Epoch: 6| Step: 13
Training loss: 2.2200063452758836
Validation loss: 2.4796214821834397

Epoch: 236| Step: 0
Training loss: 2.312773920280438
Validation loss: 2.4956385078449936

Epoch: 6| Step: 1
Training loss: 1.3927177369916095
Validation loss: 2.501225052136745

Epoch: 6| Step: 2
Training loss: 1.9473948536255106
Validation loss: 2.5093735602641143

Epoch: 6| Step: 3
Training loss: 2.3085838356386095
Validation loss: 2.517771087819783

Epoch: 6| Step: 4
Training loss: 1.1957101098261307
Validation loss: 2.5198782748387414

Epoch: 6| Step: 5
Training loss: 2.34639844665632
Validation loss: 2.524957811047198

Epoch: 6| Step: 6
Training loss: 2.615951200537531
Validation loss: 2.541871521203817

Epoch: 6| Step: 7
Training loss: 2.406337984136161
Validation loss: 2.613942791009943

Epoch: 6| Step: 8
Training loss: 1.3720876020652213
Validation loss: 2.6632042823442905

Epoch: 6| Step: 9
Training loss: 2.695392905984776
Validation loss: 2.681993120979744

Epoch: 6| Step: 10
Training loss: 2.5216328217461634
Validation loss: 2.6680600852096035

Epoch: 6| Step: 11
Training loss: 2.033021594471851
Validation loss: 2.6774355487207737

Epoch: 6| Step: 12
Training loss: 1.5075680233109443
Validation loss: 2.6812049861944116

Epoch: 6| Step: 13
Training loss: 1.8657566477782246
Validation loss: 2.684842932716767

Epoch: 237| Step: 0
Training loss: 2.2129715368523613
Validation loss: 2.644914217124531

Epoch: 6| Step: 1
Training loss: 1.9145813530658142
Validation loss: 2.627618490330672

Epoch: 6| Step: 2
Training loss: 1.5317078509240185
Validation loss: 2.6209414987930426

Epoch: 6| Step: 3
Training loss: 1.889251840850753
Validation loss: 2.62371273872726

Epoch: 6| Step: 4
Training loss: 1.904034687479587
Validation loss: 2.599390736744977

Epoch: 6| Step: 5
Training loss: 2.3510892778431267
Validation loss: 2.5853589074268557

Epoch: 6| Step: 6
Training loss: 1.4579131293038126
Validation loss: 2.5769010316414556

Epoch: 6| Step: 7
Training loss: 2.2300595310630076
Validation loss: 2.535881115186073

Epoch: 6| Step: 8
Training loss: 2.373373578976926
Validation loss: 2.5566783773045456

Epoch: 6| Step: 9
Training loss: 1.7303123661790092
Validation loss: 2.52983322053718

Epoch: 6| Step: 10
Training loss: 2.525759169260505
Validation loss: 2.553022193784332

Epoch: 6| Step: 11
Training loss: 2.1901558149116487
Validation loss: 2.5631400138702367

Epoch: 6| Step: 12
Training loss: 2.0403847335343017
Validation loss: 2.6089520520894642

Epoch: 6| Step: 13
Training loss: 2.4670814960071086
Validation loss: 2.6577036003095964

Epoch: 238| Step: 0
Training loss: 1.8302805003957137
Validation loss: 2.6479842586779356

Epoch: 6| Step: 1
Training loss: 2.0808177510590777
Validation loss: 2.663583468180167

Epoch: 6| Step: 2
Training loss: 2.5425594721227283
Validation loss: 2.612492950405606

Epoch: 6| Step: 3
Training loss: 2.690438903682037
Validation loss: 2.617453126095971

Epoch: 6| Step: 4
Training loss: 1.4278156972287024
Validation loss: 2.6231583992661176

Epoch: 6| Step: 5
Training loss: 1.9755000097168092
Validation loss: 2.5380604336474737

Epoch: 6| Step: 6
Training loss: 2.0177478109609472
Validation loss: 2.503633322597539

Epoch: 6| Step: 7
Training loss: 1.8816506374251922
Validation loss: 2.4996004739046374

Epoch: 6| Step: 8
Training loss: 2.5565501664072494
Validation loss: 2.486288681176881

Epoch: 6| Step: 9
Training loss: 2.1092921487996956
Validation loss: 2.49075694865416

Epoch: 6| Step: 10
Training loss: 1.98510332333488
Validation loss: 2.500058856907067

Epoch: 6| Step: 11
Training loss: 2.184657075102891
Validation loss: 2.494584161866497

Epoch: 6| Step: 12
Training loss: 1.6831729655154402
Validation loss: 2.5341005326430577

Epoch: 6| Step: 13
Training loss: 1.8293308943216915
Validation loss: 2.527040725772324

Epoch: 239| Step: 0
Training loss: 1.982679468716864
Validation loss: 2.577444345038025

Epoch: 6| Step: 1
Training loss: 1.230141102598125
Validation loss: 2.6375050669737443

Epoch: 6| Step: 2
Training loss: 2.1271054552243673
Validation loss: 2.674787454687703

Epoch: 6| Step: 3
Training loss: 2.2662614651877595
Validation loss: 2.6751209493133197

Epoch: 6| Step: 4
Training loss: 2.4567979662455444
Validation loss: 2.6583240714155982

Epoch: 6| Step: 5
Training loss: 2.4268531089815144
Validation loss: 2.6239748724193714

Epoch: 6| Step: 6
Training loss: 2.1691464636154087
Validation loss: 2.5592144628160827

Epoch: 6| Step: 7
Training loss: 2.1751543713738206
Validation loss: 2.5113894739656524

Epoch: 6| Step: 8
Training loss: 1.8297696011282105
Validation loss: 2.5198355399816244

Epoch: 6| Step: 9
Training loss: 2.139009186085169
Validation loss: 2.4916466232650385

Epoch: 6| Step: 10
Training loss: 2.330552032957066
Validation loss: 2.50099535042169

Epoch: 6| Step: 11
Training loss: 2.5746421639402373
Validation loss: 2.4921679839802087

Epoch: 6| Step: 12
Training loss: 1.6308778681820117
Validation loss: 2.509569228190351

Epoch: 6| Step: 13
Training loss: 1.3887805493803398
Validation loss: 2.483059692881031

Epoch: 240| Step: 0
Training loss: 2.5485121257060177
Validation loss: 2.500991330692173

Epoch: 6| Step: 1
Training loss: 1.4986520114455897
Validation loss: 2.498098747186097

Epoch: 6| Step: 2
Training loss: 2.1561108972766174
Validation loss: 2.500470593983407

Epoch: 6| Step: 3
Training loss: 1.7912417802535558
Validation loss: 2.492518555467843

Epoch: 6| Step: 4
Training loss: 2.0707520216310575
Validation loss: 2.503431397162639

Epoch: 6| Step: 5
Training loss: 2.155502742708072
Validation loss: 2.5065422208015056

Epoch: 6| Step: 6
Training loss: 1.711332301744468
Validation loss: 2.546750664357633

Epoch: 6| Step: 7
Training loss: 2.3063514004877352
Validation loss: 2.63228823066933

Epoch: 6| Step: 8
Training loss: 1.9911997422020065
Validation loss: 2.677963992679447

Epoch: 6| Step: 9
Training loss: 1.625278889125723
Validation loss: 2.677556917527682

Epoch: 6| Step: 10
Training loss: 1.4472021997636448
Validation loss: 2.72954916760926

Epoch: 6| Step: 11
Training loss: 2.3472719306372283
Validation loss: 2.7313818398553047

Epoch: 6| Step: 12
Training loss: 2.5308376965460555
Validation loss: 2.6965329120866657

Epoch: 6| Step: 13
Training loss: 2.3940530769372828
Validation loss: 2.6324715322782914

Epoch: 241| Step: 0
Training loss: 1.9849983497824164
Validation loss: 2.597402796415016

Epoch: 6| Step: 1
Training loss: 2.4934106772797198
Validation loss: 2.565486268889942

Epoch: 6| Step: 2
Training loss: 1.9691328251734719
Validation loss: 2.5571708469376175

Epoch: 6| Step: 3
Training loss: 2.0022956071380316
Validation loss: 2.534722665818639

Epoch: 6| Step: 4
Training loss: 1.7895097465025886
Validation loss: 2.545541597581805

Epoch: 6| Step: 5
Training loss: 2.0662725221453013
Validation loss: 2.5731757621708984

Epoch: 6| Step: 6
Training loss: 1.9836948460661912
Validation loss: 2.5827115038575648

Epoch: 6| Step: 7
Training loss: 2.118452419212152
Validation loss: 2.5810850009107553

Epoch: 6| Step: 8
Training loss: 1.8865136455625755
Validation loss: 2.6124217963187544

Epoch: 6| Step: 9
Training loss: 2.0213451982584423
Validation loss: 2.600118303052805

Epoch: 6| Step: 10
Training loss: 1.8733971420408009
Validation loss: 2.6144788551479836

Epoch: 6| Step: 11
Training loss: 2.3250273508083374
Validation loss: 2.627649362942232

Epoch: 6| Step: 12
Training loss: 2.285313266674212
Validation loss: 2.6003573569443286

Epoch: 6| Step: 13
Training loss: 2.0069358485242006
Validation loss: 2.5875055604835016

Epoch: 242| Step: 0
Training loss: 1.7958676375485578
Validation loss: 2.581046527912676

Epoch: 6| Step: 1
Training loss: 1.7892136155986664
Validation loss: 2.5357827782415256

Epoch: 6| Step: 2
Training loss: 2.461336424449882
Validation loss: 2.511310352367971

Epoch: 6| Step: 3
Training loss: 1.5407327796113592
Validation loss: 2.531956781996773

Epoch: 6| Step: 4
Training loss: 2.100820303965678
Validation loss: 2.541387688590203

Epoch: 6| Step: 5
Training loss: 1.914984158084372
Validation loss: 2.5419652377837334

Epoch: 6| Step: 6
Training loss: 1.7408022997427224
Validation loss: 2.5459787863112058

Epoch: 6| Step: 7
Training loss: 2.9813776592696843
Validation loss: 2.57643591525701

Epoch: 6| Step: 8
Training loss: 1.5556594647413224
Validation loss: 2.5859975971103566

Epoch: 6| Step: 9
Training loss: 2.1334941738809907
Validation loss: 2.5613590972611746

Epoch: 6| Step: 10
Training loss: 2.309931798151057
Validation loss: 2.578761043503364

Epoch: 6| Step: 11
Training loss: 1.8815026376600832
Validation loss: 2.536485226675934

Epoch: 6| Step: 12
Training loss: 1.8279632757659077
Validation loss: 2.582320589060284

Epoch: 6| Step: 13
Training loss: 2.11130836329356
Validation loss: 2.600443250824534

Epoch: 243| Step: 0
Training loss: 2.0894393957531743
Validation loss: 2.572758230873113

Epoch: 6| Step: 1
Training loss: 1.2504920467869651
Validation loss: 2.5450986350430873

Epoch: 6| Step: 2
Training loss: 1.2073204410120213
Validation loss: 2.5616153725266466

Epoch: 6| Step: 3
Training loss: 2.648906924976089
Validation loss: 2.5405693708134494

Epoch: 6| Step: 4
Training loss: 1.7429093033156804
Validation loss: 2.555061460727489

Epoch: 6| Step: 5
Training loss: 2.2919967644820085
Validation loss: 2.5459053362348194

Epoch: 6| Step: 6
Training loss: 2.048517514549429
Validation loss: 2.5731099374024633

Epoch: 6| Step: 7
Training loss: 2.0365582870058176
Validation loss: 2.5588615452055508

Epoch: 6| Step: 8
Training loss: 2.6004066992834924
Validation loss: 2.605521976016882

Epoch: 6| Step: 9
Training loss: 1.8205450998119268
Validation loss: 2.5767141315666406

Epoch: 6| Step: 10
Training loss: 2.3180582418160185
Validation loss: 2.599985372062028

Epoch: 6| Step: 11
Training loss: 2.03724887915185
Validation loss: 2.584966671308848

Epoch: 6| Step: 12
Training loss: 2.156701303328717
Validation loss: 2.602675282991066

Epoch: 6| Step: 13
Training loss: 1.8900121531020089
Validation loss: 2.583230652357946

Epoch: 244| Step: 0
Training loss: 1.7116291616404966
Validation loss: 2.5692801059256825

Epoch: 6| Step: 1
Training loss: 1.9642984835717447
Validation loss: 2.5760786470092456

Epoch: 6| Step: 2
Training loss: 1.8736506375057709
Validation loss: 2.590622759343717

Epoch: 6| Step: 3
Training loss: 2.1563541553054377
Validation loss: 2.586064115653396

Epoch: 6| Step: 4
Training loss: 2.3126550570904296
Validation loss: 2.6419217463866063

Epoch: 6| Step: 5
Training loss: 2.2058993787669428
Validation loss: 2.6697602259143625

Epoch: 6| Step: 6
Training loss: 1.6770286630126994
Validation loss: 2.651807038767476

Epoch: 6| Step: 7
Training loss: 2.357432595317899
Validation loss: 2.646350139296276

Epoch: 6| Step: 8
Training loss: 2.4460857497224247
Validation loss: 2.642905106747496

Epoch: 6| Step: 9
Training loss: 1.9397095417572903
Validation loss: 2.618981971268894

Epoch: 6| Step: 10
Training loss: 2.395385390713558
Validation loss: 2.5915056192910124

Epoch: 6| Step: 11
Training loss: 1.8057991336294426
Validation loss: 2.587839447091243

Epoch: 6| Step: 12
Training loss: 1.3152094531325156
Validation loss: 2.5911752108085873

Epoch: 6| Step: 13
Training loss: 2.2305431453770788
Validation loss: 2.574202109607714

Epoch: 245| Step: 0
Training loss: 1.893719874982777
Validation loss: 2.5526596249077858

Epoch: 6| Step: 1
Training loss: 1.6557700703398193
Validation loss: 2.5644947909563007

Epoch: 6| Step: 2
Training loss: 1.8389068615442627
Validation loss: 2.523190134874119

Epoch: 6| Step: 3
Training loss: 1.6907123084801823
Validation loss: 2.5272304346387173

Epoch: 6| Step: 4
Training loss: 2.239680360570051
Validation loss: 2.5036482258967716

Epoch: 6| Step: 5
Training loss: 2.0285022637455934
Validation loss: 2.54287592986562

Epoch: 6| Step: 6
Training loss: 2.510730697385951
Validation loss: 2.545559955118389

Epoch: 6| Step: 7
Training loss: 2.3749893589785263
Validation loss: 2.5420160574567183

Epoch: 6| Step: 8
Training loss: 1.984268666219368
Validation loss: 2.546879977043921

Epoch: 6| Step: 9
Training loss: 1.3087233749639853
Validation loss: 2.505689790160337

Epoch: 6| Step: 10
Training loss: 2.166318413060822
Validation loss: 2.497177406175747

Epoch: 6| Step: 11
Training loss: 2.1596803882577276
Validation loss: 2.551894252286227

Epoch: 6| Step: 12
Training loss: 2.6886546293872495
Validation loss: 2.559673813240072

Epoch: 6| Step: 13
Training loss: 1.8757529971955362
Validation loss: 2.569424484507614

Epoch: 246| Step: 0
Training loss: 2.2504108901486215
Validation loss: 2.552939436171633

Epoch: 6| Step: 1
Training loss: 2.1044464177157423
Validation loss: 2.5753445515155824

Epoch: 6| Step: 2
Training loss: 1.4805740681330426
Validation loss: 2.6147590871963575

Epoch: 6| Step: 3
Training loss: 2.4266664422562605
Validation loss: 2.567490207984029

Epoch: 6| Step: 4
Training loss: 2.0829113214949095
Validation loss: 2.581821281140828

Epoch: 6| Step: 5
Training loss: 1.9856659302111639
Validation loss: 2.5758160973766904

Epoch: 6| Step: 6
Training loss: 1.996695590130811
Validation loss: 2.573184896430938

Epoch: 6| Step: 7
Training loss: 2.217818803545801
Validation loss: 2.5800797113283274

Epoch: 6| Step: 8
Training loss: 1.760786329954847
Validation loss: 2.533486935911292

Epoch: 6| Step: 9
Training loss: 1.7841786184657442
Validation loss: 2.525812989466273

Epoch: 6| Step: 10
Training loss: 1.731203771311377
Validation loss: 2.5340207167041777

Epoch: 6| Step: 11
Training loss: 2.8039463094558417
Validation loss: 2.5565613495630197

Epoch: 6| Step: 12
Training loss: 1.8608935910673106
Validation loss: 2.616465348398001

Epoch: 6| Step: 13
Training loss: 1.8172127194675916
Validation loss: 2.5811932195001863

Epoch: 247| Step: 0
Training loss: 2.177581973366886
Validation loss: 2.600435961950379

Epoch: 6| Step: 1
Training loss: 2.2159317815725954
Validation loss: 2.6142451357614296

Epoch: 6| Step: 2
Training loss: 1.5853359791712476
Validation loss: 2.585771322913426

Epoch: 6| Step: 3
Training loss: 1.5869433591045723
Validation loss: 2.69795504411382

Epoch: 6| Step: 4
Training loss: 1.9988976659376516
Validation loss: 2.7124146496401766

Epoch: 6| Step: 5
Training loss: 1.6927269411286927
Validation loss: 2.672542243316768

Epoch: 6| Step: 6
Training loss: 2.227940397386483
Validation loss: 2.6257313019983473

Epoch: 6| Step: 7
Training loss: 1.8464579981208515
Validation loss: 2.6393457959686963

Epoch: 6| Step: 8
Training loss: 1.6610474830881892
Validation loss: 2.6069955465363845

Epoch: 6| Step: 9
Training loss: 2.1206781531659127
Validation loss: 2.570697002000738

Epoch: 6| Step: 10
Training loss: 3.3650048211390517
Validation loss: 2.56806919020238

Epoch: 6| Step: 11
Training loss: 1.6163820481551212
Validation loss: 2.5601696567768344

Epoch: 6| Step: 12
Training loss: 1.4338069414240813
Validation loss: 2.577547582834016

Epoch: 6| Step: 13
Training loss: 1.917424763841034
Validation loss: 2.5566997011264565

Epoch: 248| Step: 0
Training loss: 1.2409317579018881
Validation loss: 2.530700184327114

Epoch: 6| Step: 1
Training loss: 2.2685320915052554
Validation loss: 2.546430350027684

Epoch: 6| Step: 2
Training loss: 2.495575613769891
Validation loss: 2.5406342791285805

Epoch: 6| Step: 3
Training loss: 2.1368822108479284
Validation loss: 2.5604561198717577

Epoch: 6| Step: 4
Training loss: 2.2825909161714413
Validation loss: 2.580150402056843

Epoch: 6| Step: 5
Training loss: 1.7039243589590698
Validation loss: 2.579611455704194

Epoch: 6| Step: 6
Training loss: 2.0602831631085863
Validation loss: 2.528650181010654

Epoch: 6| Step: 7
Training loss: 1.7501034024888575
Validation loss: 2.551053167535027

Epoch: 6| Step: 8
Training loss: 1.86920975863201
Validation loss: 2.541942860235503

Epoch: 6| Step: 9
Training loss: 1.880004434986159
Validation loss: 2.530504894086789

Epoch: 6| Step: 10
Training loss: 2.267859468576893
Validation loss: 2.5173379503843565

Epoch: 6| Step: 11
Training loss: 1.9728470813869141
Validation loss: 2.5295252712822465

Epoch: 6| Step: 12
Training loss: 2.4394242931705414
Validation loss: 2.5698278292887227

Epoch: 6| Step: 13
Training loss: 1.652475338288697
Validation loss: 2.592571021235002

Epoch: 249| Step: 0
Training loss: 1.9598953091629736
Validation loss: 2.58536162787869

Epoch: 6| Step: 1
Training loss: 2.192738500343781
Validation loss: 2.596630457975382

Epoch: 6| Step: 2
Training loss: 1.825079247308705
Validation loss: 2.5911295419049853

Epoch: 6| Step: 3
Training loss: 1.803369081792747
Validation loss: 2.6136150213483447

Epoch: 6| Step: 4
Training loss: 1.8492058673340606
Validation loss: 2.5860306030600597

Epoch: 6| Step: 5
Training loss: 1.7893675165156477
Validation loss: 2.559586116543556

Epoch: 6| Step: 6
Training loss: 1.514127561826703
Validation loss: 2.5466613367867077

Epoch: 6| Step: 7
Training loss: 2.5408695305030555
Validation loss: 2.5088499226902807

Epoch: 6| Step: 8
Training loss: 1.9188914517131086
Validation loss: 2.5055462033740317

Epoch: 6| Step: 9
Training loss: 2.1442475122519853
Validation loss: 2.4870776145316715

Epoch: 6| Step: 10
Training loss: 1.8593260173596686
Validation loss: 2.4833823966186905

Epoch: 6| Step: 11
Training loss: 2.241399433794424
Validation loss: 2.5044139678591555

Epoch: 6| Step: 12
Training loss: 1.5378332505728256
Validation loss: 2.477061328731384

Epoch: 6| Step: 13
Training loss: 2.785669822477761
Validation loss: 2.4914114609818636

Epoch: 250| Step: 0
Training loss: 2.628701189270923
Validation loss: 2.492278158199142

Epoch: 6| Step: 1
Training loss: 1.9590682889360242
Validation loss: 2.5290852383279154

Epoch: 6| Step: 2
Training loss: 1.3964004598093929
Validation loss: 2.552379409327647

Epoch: 6| Step: 3
Training loss: 1.460004375921511
Validation loss: 2.5302989413194283

Epoch: 6| Step: 4
Training loss: 2.481024732381112
Validation loss: 2.563852031034592

Epoch: 6| Step: 5
Training loss: 1.9500267173697416
Validation loss: 2.5472861584129896

Epoch: 6| Step: 6
Training loss: 1.9314185652268083
Validation loss: 2.5480412970249926

Epoch: 6| Step: 7
Training loss: 2.0483449069700534
Validation loss: 2.5916640231577976

Epoch: 6| Step: 8
Training loss: 1.8592072739805183
Validation loss: 2.539313561180349

Epoch: 6| Step: 9
Training loss: 2.2530863045638063
Validation loss: 2.597696480881682

Epoch: 6| Step: 10
Training loss: 1.7951116783232701
Validation loss: 2.575126399175495

Epoch: 6| Step: 11
Training loss: 2.574649664740561
Validation loss: 2.580056355240391

Epoch: 6| Step: 12
Training loss: 1.7975433433517054
Validation loss: 2.5613331658020573

Epoch: 6| Step: 13
Training loss: 1.4503510773617436
Validation loss: 2.5353439097020463

Testing loss: 2.1147390632422995

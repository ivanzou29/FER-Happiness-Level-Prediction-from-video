Epoch: 1| Step: 0
Training loss: 6.295614490286292
Validation loss: 5.9298714335733544

Epoch: 5| Step: 1
Training loss: 6.320417820603473
Validation loss: 5.9274880841458675

Epoch: 5| Step: 2
Training loss: 5.803818320587634
Validation loss: 5.925025511202502

Epoch: 5| Step: 3
Training loss: 6.152731262597619
Validation loss: 5.922555802129222

Epoch: 5| Step: 4
Training loss: 5.958490791996791
Validation loss: 5.920260300670032

Epoch: 5| Step: 5
Training loss: 6.068455867534256
Validation loss: 5.917879593684334

Epoch: 5| Step: 6
Training loss: 4.374159813995042
Validation loss: 5.91549059995123

Epoch: 5| Step: 7
Training loss: 5.824140807870491
Validation loss: 5.913050751039337

Epoch: 5| Step: 8
Training loss: 6.262752492843789
Validation loss: 5.9107059778619675

Epoch: 5| Step: 9
Training loss: 6.089828403103658
Validation loss: 5.908211410979038

Epoch: 5| Step: 10
Training loss: 7.078104250711171
Validation loss: 5.905680331252447

Epoch: 5| Step: 11
Training loss: 4.940846914366117
Validation loss: 5.9029429291703055

Epoch: 2| Step: 0
Training loss: 5.683206132078729
Validation loss: 5.9003220400232985

Epoch: 5| Step: 1
Training loss: 5.888233620196775
Validation loss: 5.897422888525092

Epoch: 5| Step: 2
Training loss: 5.7863397411574145
Validation loss: 5.894477940059836

Epoch: 5| Step: 3
Training loss: 5.990670261998091
Validation loss: 5.891386939030903

Epoch: 5| Step: 4
Training loss: 5.7414537025823424
Validation loss: 5.888034861594463

Epoch: 5| Step: 5
Training loss: 6.15929356970684
Validation loss: 5.8845718677695995

Epoch: 5| Step: 6
Training loss: 6.306841438619237
Validation loss: 5.88099745756502

Epoch: 5| Step: 7
Training loss: 5.541631578989258
Validation loss: 5.877068378691345

Epoch: 5| Step: 8
Training loss: 6.051929185209702
Validation loss: 5.872976475420455

Epoch: 5| Step: 9
Training loss: 6.857336484355129
Validation loss: 5.868702238766719

Epoch: 5| Step: 10
Training loss: 5.777645492669556
Validation loss: 5.864247492586066

Epoch: 5| Step: 11
Training loss: 6.431609782988608
Validation loss: 5.859302408404499

Epoch: 3| Step: 0
Training loss: 6.422797440783263
Validation loss: 5.8542405075176775

Epoch: 5| Step: 1
Training loss: 5.695510421265177
Validation loss: 5.848974479818321

Epoch: 5| Step: 2
Training loss: 5.528677821834222
Validation loss: 5.843403171848048

Epoch: 5| Step: 3
Training loss: 6.259734545554672
Validation loss: 5.837374318394753

Epoch: 5| Step: 4
Training loss: 6.466621970320459
Validation loss: 5.83135250610663

Epoch: 5| Step: 5
Training loss: 5.113806806093561
Validation loss: 5.825047421023598

Epoch: 5| Step: 6
Training loss: 5.219904240866251
Validation loss: 5.818336373230661

Epoch: 5| Step: 7
Training loss: 5.802470095589039
Validation loss: 5.811405714660297

Epoch: 5| Step: 8
Training loss: 5.818285943855191
Validation loss: 5.8044817662564565

Epoch: 5| Step: 9
Training loss: 6.520624744463177
Validation loss: 5.796847635566168

Epoch: 5| Step: 10
Training loss: 6.161441995357759
Validation loss: 5.789154367751864

Epoch: 5| Step: 11
Training loss: 6.4411684712973285
Validation loss: 5.780928717956301

Epoch: 4| Step: 0
Training loss: 6.181865245721534
Validation loss: 5.7726267863575025

Epoch: 5| Step: 1
Training loss: 6.272588575033486
Validation loss: 5.7634804380487585

Epoch: 5| Step: 2
Training loss: 5.88085061803981
Validation loss: 5.754734115280589

Epoch: 5| Step: 3
Training loss: 5.286622333958072
Validation loss: 5.74534722066027

Epoch: 5| Step: 4
Training loss: 6.14841671543898
Validation loss: 5.7360724034002715

Epoch: 5| Step: 5
Training loss: 5.76372250644908
Validation loss: 5.726548927363904

Epoch: 5| Step: 6
Training loss: 6.173229059337015
Validation loss: 5.716696935226932

Epoch: 5| Step: 7
Training loss: 5.22393577725481
Validation loss: 5.706996436320778

Epoch: 5| Step: 8
Training loss: 5.845513878284843
Validation loss: 5.696574367417569

Epoch: 5| Step: 9
Training loss: 6.097997192782528
Validation loss: 5.686470392674073

Epoch: 5| Step: 10
Training loss: 5.295666942288522
Validation loss: 5.675974223669127

Epoch: 5| Step: 11
Training loss: 5.543039806682062
Validation loss: 5.665820334381679

Epoch: 5| Step: 0
Training loss: 5.5709827335537225
Validation loss: 5.654915344814369

Epoch: 5| Step: 1
Training loss: 6.094531199926291
Validation loss: 5.644478591972513

Epoch: 5| Step: 2
Training loss: 5.337580460024592
Validation loss: 5.633520588875649

Epoch: 5| Step: 3
Training loss: 5.152767600671818
Validation loss: 5.623197725990372

Epoch: 5| Step: 4
Training loss: 5.852850534141462
Validation loss: 5.612111429188568

Epoch: 5| Step: 5
Training loss: 6.083232234306356
Validation loss: 5.601472351114944

Epoch: 5| Step: 6
Training loss: 5.025350393348481
Validation loss: 5.5903858105927045

Epoch: 5| Step: 7
Training loss: 6.092267447859623
Validation loss: 5.579520114284814

Epoch: 5| Step: 8
Training loss: 6.103780619247417
Validation loss: 5.567980827131823

Epoch: 5| Step: 9
Training loss: 5.378044153522387
Validation loss: 5.557180604358199

Epoch: 5| Step: 10
Training loss: 6.322565796955848
Validation loss: 5.545747963037842

Epoch: 5| Step: 11
Training loss: 3.869996563025237
Validation loss: 5.534751143221064

Epoch: 6| Step: 0
Training loss: 5.780253556441777
Validation loss: 5.523944333287908

Epoch: 5| Step: 1
Training loss: 5.426132772990096
Validation loss: 5.513478820642482

Epoch: 5| Step: 2
Training loss: 6.068046942391626
Validation loss: 5.502972615934025

Epoch: 5| Step: 3
Training loss: 5.7057524181073855
Validation loss: 5.492009456203142

Epoch: 5| Step: 4
Training loss: 6.017386042455274
Validation loss: 5.481329475328323

Epoch: 5| Step: 5
Training loss: 4.506519892595438
Validation loss: 5.470776449499965

Epoch: 5| Step: 6
Training loss: 5.392661774127118
Validation loss: 5.460385488862362

Epoch: 5| Step: 7
Training loss: 5.037356633496118
Validation loss: 5.44993808606178

Epoch: 5| Step: 8
Training loss: 5.549203990866706
Validation loss: 5.440021156954763

Epoch: 5| Step: 9
Training loss: 5.734019421771088
Validation loss: 5.430024558750019

Epoch: 5| Step: 10
Training loss: 5.774530908598158
Validation loss: 5.420196727851646

Epoch: 5| Step: 11
Training loss: 7.019102190646902
Validation loss: 5.410697570234201

Epoch: 7| Step: 0
Training loss: 4.853610529441849
Validation loss: 5.400346226248134

Epoch: 5| Step: 1
Training loss: 5.240621637060079
Validation loss: 5.391150952909413

Epoch: 5| Step: 2
Training loss: 5.267341130363417
Validation loss: 5.381957216418572

Epoch: 5| Step: 3
Training loss: 5.453889722133763
Validation loss: 5.372807299459951

Epoch: 5| Step: 4
Training loss: 5.733654507061588
Validation loss: 5.364033623341795

Epoch: 5| Step: 5
Training loss: 5.729917867714332
Validation loss: 5.355337395924862

Epoch: 5| Step: 6
Training loss: 5.242604087047332
Validation loss: 5.346431498116128

Epoch: 5| Step: 7
Training loss: 5.394448337014432
Validation loss: 5.338026949252185

Epoch: 5| Step: 8
Training loss: 6.200152167637135
Validation loss: 5.329500380688569

Epoch: 5| Step: 9
Training loss: 5.213022892840047
Validation loss: 5.321538282618536

Epoch: 5| Step: 10
Training loss: 5.92308591676195
Validation loss: 5.31317117976485

Epoch: 5| Step: 11
Training loss: 4.690517420745435
Validation loss: 5.305574330812068

Epoch: 8| Step: 0
Training loss: 4.254010720279387
Validation loss: 5.298028810879017

Epoch: 5| Step: 1
Training loss: 5.4930719610334435
Validation loss: 5.290578274671688

Epoch: 5| Step: 2
Training loss: 5.74054703926887
Validation loss: 5.283179936854632

Epoch: 5| Step: 3
Training loss: 5.400934682416145
Validation loss: 5.276114598554997

Epoch: 5| Step: 4
Training loss: 5.977876247050355
Validation loss: 5.269183970084137

Epoch: 5| Step: 5
Training loss: 5.075668914759017
Validation loss: 5.262032480381032

Epoch: 5| Step: 6
Training loss: 5.5167781499717625
Validation loss: 5.255430674945443

Epoch: 5| Step: 7
Training loss: 5.75959342422306
Validation loss: 5.248651217725504

Epoch: 5| Step: 8
Training loss: 5.944800777193259
Validation loss: 5.2418921397383675

Epoch: 5| Step: 9
Training loss: 5.293325849923354
Validation loss: 5.235343555021409

Epoch: 5| Step: 10
Training loss: 4.456792452246609
Validation loss: 5.228932191776899

Epoch: 5| Step: 11
Training loss: 5.548687362093916
Validation loss: 5.222434313786958

Epoch: 9| Step: 0
Training loss: 4.942791770924875
Validation loss: 5.216092268588928

Epoch: 5| Step: 1
Training loss: 5.376814247718418
Validation loss: 5.209502205976586

Epoch: 5| Step: 2
Training loss: 4.883545257518001
Validation loss: 5.203309362312703

Epoch: 5| Step: 3
Training loss: 6.320415406397259
Validation loss: 5.197080184302284

Epoch: 5| Step: 4
Training loss: 5.418330636345997
Validation loss: 5.190597712326207

Epoch: 5| Step: 5
Training loss: 5.857884250464983
Validation loss: 5.184187586062069

Epoch: 5| Step: 6
Training loss: 4.840056949627222
Validation loss: 5.178082677330192

Epoch: 5| Step: 7
Training loss: 4.337456844236073
Validation loss: 5.171762248151871

Epoch: 5| Step: 8
Training loss: 5.462205905984779
Validation loss: 5.165464363900555

Epoch: 5| Step: 9
Training loss: 4.789821944142788
Validation loss: 5.159386954090572

Epoch: 5| Step: 10
Training loss: 5.847453366762735
Validation loss: 5.153869780817705

Epoch: 5| Step: 11
Training loss: 5.314772534962214
Validation loss: 5.1478190458215165

Epoch: 10| Step: 0
Training loss: 5.623963663214428
Validation loss: 5.142349216820338

Epoch: 5| Step: 1
Training loss: 4.995482311160571
Validation loss: 5.136427072576755

Epoch: 5| Step: 2
Training loss: 5.147860458522068
Validation loss: 5.130694412466384

Epoch: 5| Step: 3
Training loss: 5.011227209734459
Validation loss: 5.124964605379774

Epoch: 5| Step: 4
Training loss: 5.435005755943472
Validation loss: 5.119481263270074

Epoch: 5| Step: 5
Training loss: 5.038104393532051
Validation loss: 5.113948536601612

Epoch: 5| Step: 6
Training loss: 5.642014266349338
Validation loss: 5.108402140134785

Epoch: 5| Step: 7
Training loss: 4.867602090338043
Validation loss: 5.103096490372715

Epoch: 5| Step: 8
Training loss: 5.470020430001946
Validation loss: 5.097748674940883

Epoch: 5| Step: 9
Training loss: 4.970739961798672
Validation loss: 5.092528432705208

Epoch: 5| Step: 10
Training loss: 5.427004803207125
Validation loss: 5.087347676014285

Epoch: 5| Step: 11
Training loss: 4.901514768991281
Validation loss: 5.082300951616213

Epoch: 11| Step: 0
Training loss: 5.549661285786988
Validation loss: 5.077948495780712

Epoch: 5| Step: 1
Training loss: 5.086056943741915
Validation loss: 5.073356862587991

Epoch: 5| Step: 2
Training loss: 6.271238937018409
Validation loss: 5.067955194036345

Epoch: 5| Step: 3
Training loss: 5.209960845975866
Validation loss: 5.062478815042258

Epoch: 5| Step: 4
Training loss: 4.831549852548469
Validation loss: 5.057090820766259

Epoch: 5| Step: 5
Training loss: 4.55332505016402
Validation loss: 5.052330926025815

Epoch: 5| Step: 6
Training loss: 4.8719852613655235
Validation loss: 5.047103388512979

Epoch: 5| Step: 7
Training loss: 5.03055183826639
Validation loss: 5.04183382224843

Epoch: 5| Step: 8
Training loss: 4.498307227593327
Validation loss: 5.036690591889534

Epoch: 5| Step: 9
Training loss: 4.975741092557572
Validation loss: 5.031993305592514

Epoch: 5| Step: 10
Training loss: 5.423750759035907
Validation loss: 5.027459827694462

Epoch: 5| Step: 11
Training loss: 6.9604026169355855
Validation loss: 5.022300745845775

Epoch: 12| Step: 0
Training loss: 5.317762214569501
Validation loss: 5.017333835878194

Epoch: 5| Step: 1
Training loss: 5.519085800651333
Validation loss: 5.012767021062152

Epoch: 5| Step: 2
Training loss: 4.904405964101848
Validation loss: 5.007468486997163

Epoch: 5| Step: 3
Training loss: 4.468938223336069
Validation loss: 5.00223483367664

Epoch: 5| Step: 4
Training loss: 5.517012726838736
Validation loss: 4.997390201069128

Epoch: 5| Step: 5
Training loss: 5.687902080031044
Validation loss: 4.992568517401651

Epoch: 5| Step: 6
Training loss: 5.15926813746227
Validation loss: 4.9874404760264905

Epoch: 5| Step: 7
Training loss: 4.002031763960163
Validation loss: 4.982508845151561

Epoch: 5| Step: 8
Training loss: 5.910615468259041
Validation loss: 4.9775307719941395

Epoch: 5| Step: 9
Training loss: 4.576585947108872
Validation loss: 4.97249520503766

Epoch: 5| Step: 10
Training loss: 4.970700630831889
Validation loss: 4.966744864038101

Epoch: 5| Step: 11
Training loss: 4.87606037051824
Validation loss: 4.961856570677498

Epoch: 13| Step: 0
Training loss: 5.246339884513573
Validation loss: 4.95656994576269

Epoch: 5| Step: 1
Training loss: 5.039839147998209
Validation loss: 4.951519277650218

Epoch: 5| Step: 2
Training loss: 4.87295528471041
Validation loss: 4.945979406336305

Epoch: 5| Step: 3
Training loss: 4.729052482044335
Validation loss: 4.940849383392404

Epoch: 5| Step: 4
Training loss: 4.897098242583026
Validation loss: 4.9355705048317855

Epoch: 5| Step: 5
Training loss: 4.805331352947587
Validation loss: 4.930511147861756

Epoch: 5| Step: 6
Training loss: 5.120868413115431
Validation loss: 4.92562619115735

Epoch: 5| Step: 7
Training loss: 4.971245480910164
Validation loss: 4.920416140441496

Epoch: 5| Step: 8
Training loss: 4.9323966778134025
Validation loss: 4.9144985165243344

Epoch: 5| Step: 9
Training loss: 5.018443423891886
Validation loss: 4.9094857481620675

Epoch: 5| Step: 10
Training loss: 5.886609234009489
Validation loss: 4.904184404706787

Epoch: 5| Step: 11
Training loss: 5.039746993541554
Validation loss: 4.898613098663763

Epoch: 14| Step: 0
Training loss: 3.8874349116429903
Validation loss: 4.89368104362381

Epoch: 5| Step: 1
Training loss: 5.451059207004771
Validation loss: 4.888265461017508

Epoch: 5| Step: 2
Training loss: 4.946183114456378
Validation loss: 4.883132647382183

Epoch: 5| Step: 3
Training loss: 5.424443848400785
Validation loss: 4.877541018388009

Epoch: 5| Step: 4
Training loss: 5.277101410921289
Validation loss: 4.872245948017967

Epoch: 5| Step: 5
Training loss: 4.557159224265979
Validation loss: 4.867161644080621

Epoch: 5| Step: 6
Training loss: 4.561467720202695
Validation loss: 4.860938610770396

Epoch: 5| Step: 7
Training loss: 5.377117360936178
Validation loss: 4.855166707422769

Epoch: 5| Step: 8
Training loss: 5.6120882830592205
Validation loss: 4.849387524462394

Epoch: 5| Step: 9
Training loss: 5.245150733432802
Validation loss: 4.843533984874187

Epoch: 5| Step: 10
Training loss: 4.006039351759016
Validation loss: 4.837043763035353

Epoch: 5| Step: 11
Training loss: 5.910447017074969
Validation loss: 4.831292513536513

Epoch: 15| Step: 0
Training loss: 4.572537394141812
Validation loss: 4.8253620970451

Epoch: 5| Step: 1
Training loss: 4.783506571504727
Validation loss: 4.819418768153226

Epoch: 5| Step: 2
Training loss: 4.603693573870303
Validation loss: 4.813600117170926

Epoch: 5| Step: 3
Training loss: 5.397530110970028
Validation loss: 4.808291109892486

Epoch: 5| Step: 4
Training loss: 4.3742278371322385
Validation loss: 4.802675896064014

Epoch: 5| Step: 5
Training loss: 4.800603852435906
Validation loss: 4.796686763155796

Epoch: 5| Step: 6
Training loss: 5.415617131988569
Validation loss: 4.791347628490424

Epoch: 5| Step: 7
Training loss: 5.3626462334032245
Validation loss: 4.7856652727375915

Epoch: 5| Step: 8
Training loss: 4.360257623393187
Validation loss: 4.780269040140449

Epoch: 5| Step: 9
Training loss: 4.491479861043917
Validation loss: 4.77478368811778

Epoch: 5| Step: 10
Training loss: 5.810621716954979
Validation loss: 4.769732371845677

Epoch: 5| Step: 11
Training loss: 4.521453580483687
Validation loss: 4.763979910316782

Epoch: 16| Step: 0
Training loss: 4.038498387151153
Validation loss: 4.758163572825075

Epoch: 5| Step: 1
Training loss: 4.604014444495907
Validation loss: 4.752667485342242

Epoch: 5| Step: 2
Training loss: 5.282308799652176
Validation loss: 4.748274682230502

Epoch: 5| Step: 3
Training loss: 5.333728874956792
Validation loss: 4.742352260218186

Epoch: 5| Step: 4
Training loss: 4.5418702992742634
Validation loss: 4.7367596553575835

Epoch: 5| Step: 5
Training loss: 4.487913642882046
Validation loss: 4.731085647019551

Epoch: 5| Step: 6
Training loss: 5.3241639389832764
Validation loss: 4.725216348596227

Epoch: 5| Step: 7
Training loss: 4.882298605769963
Validation loss: 4.718942415133306

Epoch: 5| Step: 8
Training loss: 5.590960952788917
Validation loss: 4.713942354428945

Epoch: 5| Step: 9
Training loss: 4.848528270996604
Validation loss: 4.707835853367651

Epoch: 5| Step: 10
Training loss: 4.483754610219329
Validation loss: 4.701968146882201

Epoch: 5| Step: 11
Training loss: 3.5521456506954894
Validation loss: 4.696467870485348

Epoch: 17| Step: 0
Training loss: 5.658013000559644
Validation loss: 4.69048353825211

Epoch: 5| Step: 1
Training loss: 5.19550156787081
Validation loss: 4.684983005811769

Epoch: 5| Step: 2
Training loss: 4.44632563250253
Validation loss: 4.679270985301871

Epoch: 5| Step: 3
Training loss: 5.105558037068333
Validation loss: 4.672960518869171

Epoch: 5| Step: 4
Training loss: 4.953274793796658
Validation loss: 4.667797780330928

Epoch: 5| Step: 5
Training loss: 4.136893955803376
Validation loss: 4.662667309142947

Epoch: 5| Step: 6
Training loss: 5.333646089602389
Validation loss: 4.657034617973517

Epoch: 5| Step: 7
Training loss: 4.008684981710682
Validation loss: 4.652381168687967

Epoch: 5| Step: 8
Training loss: 4.5166069467873315
Validation loss: 4.6456012354239125

Epoch: 5| Step: 9
Training loss: 4.585798791090617
Validation loss: 4.640720225169551

Epoch: 5| Step: 10
Training loss: 4.614243448224997
Validation loss: 4.635365392279911

Epoch: 5| Step: 11
Training loss: 4.081431017099134
Validation loss: 4.629671328127242

Epoch: 18| Step: 0
Training loss: 4.165918333882122
Validation loss: 4.624419158684838

Epoch: 5| Step: 1
Training loss: 4.496762276536273
Validation loss: 4.619245420185234

Epoch: 5| Step: 2
Training loss: 5.158973298357345
Validation loss: 4.614036332454645

Epoch: 5| Step: 3
Training loss: 5.3269632364977
Validation loss: 4.609187390663586

Epoch: 5| Step: 4
Training loss: 5.292458462486827
Validation loss: 4.602801930506085

Epoch: 5| Step: 5
Training loss: 4.204779657312941
Validation loss: 4.5976448545095945

Epoch: 5| Step: 6
Training loss: 4.806006670721396
Validation loss: 4.592441134496933

Epoch: 5| Step: 7
Training loss: 5.147005799605425
Validation loss: 4.5874359871316

Epoch: 5| Step: 8
Training loss: 4.550042271679711
Validation loss: 4.581549022607816

Epoch: 5| Step: 9
Training loss: 3.773524439345257
Validation loss: 4.575893079378874

Epoch: 5| Step: 10
Training loss: 4.908471254614113
Validation loss: 4.57094624790502

Epoch: 5| Step: 11
Training loss: 4.019484984017625
Validation loss: 4.565587705981634

Epoch: 19| Step: 0
Training loss: 5.391493398406742
Validation loss: 4.560375128159383

Epoch: 5| Step: 1
Training loss: 4.450028262959427
Validation loss: 4.554995256498946

Epoch: 5| Step: 2
Training loss: 4.158678764923328
Validation loss: 4.548896200484977

Epoch: 5| Step: 3
Training loss: 4.589185458111855
Validation loss: 4.543927840119937

Epoch: 5| Step: 4
Training loss: 5.0322586849171245
Validation loss: 4.538743117843869

Epoch: 5| Step: 5
Training loss: 5.021683595875487
Validation loss: 4.53277414930939

Epoch: 5| Step: 6
Training loss: 4.723904049641022
Validation loss: 4.527534443545049

Epoch: 5| Step: 7
Training loss: 3.7298742010262167
Validation loss: 4.52253904847002

Epoch: 5| Step: 8
Training loss: 4.854315279321553
Validation loss: 4.5162830951209045

Epoch: 5| Step: 9
Training loss: 3.977697903584763
Validation loss: 4.510947377593014

Epoch: 5| Step: 10
Training loss: 5.133741413756668
Validation loss: 4.506536037486498

Epoch: 5| Step: 11
Training loss: 4.27955181662995
Validation loss: 4.500609568529833

Epoch: 20| Step: 0
Training loss: 4.635099775933774
Validation loss: 4.49669168861691

Epoch: 5| Step: 1
Training loss: 4.8077444384756305
Validation loss: 4.490727434469976

Epoch: 5| Step: 2
Training loss: 5.507474502040386
Validation loss: 4.48536341160569

Epoch: 5| Step: 3
Training loss: 4.965589561840091
Validation loss: 4.480609078996471

Epoch: 5| Step: 4
Training loss: 4.26069999567266
Validation loss: 4.474941339587825

Epoch: 5| Step: 5
Training loss: 4.072648503808215
Validation loss: 4.468931719057394

Epoch: 5| Step: 6
Training loss: 4.540225905793099
Validation loss: 4.462730568450992

Epoch: 5| Step: 7
Training loss: 3.922640418024713
Validation loss: 4.456694050547587

Epoch: 5| Step: 8
Training loss: 4.483150727099696
Validation loss: 4.451612704292862

Epoch: 5| Step: 9
Training loss: 4.398720876126301
Validation loss: 4.44608913759448

Epoch: 5| Step: 10
Training loss: 4.755407768599271
Validation loss: 4.440836113554078

Epoch: 5| Step: 11
Training loss: 4.7121232345087565
Validation loss: 4.435881709130586

Epoch: 21| Step: 0
Training loss: 4.13410615054094
Validation loss: 4.430350418317816

Epoch: 5| Step: 1
Training loss: 4.65026655356067
Validation loss: 4.424282007462935

Epoch: 5| Step: 2
Training loss: 4.656176880127954
Validation loss: 4.419146438433405

Epoch: 5| Step: 3
Training loss: 4.58104812989165
Validation loss: 4.41363209114181

Epoch: 5| Step: 4
Training loss: 4.294880574068133
Validation loss: 4.408667886234482

Epoch: 5| Step: 5
Training loss: 4.69066441854221
Validation loss: 4.403760450222448

Epoch: 5| Step: 6
Training loss: 4.817579486945401
Validation loss: 4.398886278835866

Epoch: 5| Step: 7
Training loss: 4.684057573858866
Validation loss: 4.393360362857357

Epoch: 5| Step: 8
Training loss: 3.93602398046306
Validation loss: 4.387538743821639

Epoch: 5| Step: 9
Training loss: 4.5873320504609785
Validation loss: 4.38144097086573

Epoch: 5| Step: 10
Training loss: 4.269433413935085
Validation loss: 4.375674931372473

Epoch: 5| Step: 11
Training loss: 6.448859109445372
Validation loss: 4.370754338990991

Epoch: 22| Step: 0
Training loss: 4.656888981952906
Validation loss: 4.365724312797937

Epoch: 5| Step: 1
Training loss: 3.888622335351196
Validation loss: 4.359983258215232

Epoch: 5| Step: 2
Training loss: 4.175422446070507
Validation loss: 4.354077528386405

Epoch: 5| Step: 3
Training loss: 3.6473377865756174
Validation loss: 4.347840018785656

Epoch: 5| Step: 4
Training loss: 4.775907635931441
Validation loss: 4.34278578084889

Epoch: 5| Step: 5
Training loss: 4.328787081650772
Validation loss: 4.337812266986783

Epoch: 5| Step: 6
Training loss: 4.118488142520035
Validation loss: 4.33176546281845

Epoch: 5| Step: 7
Training loss: 4.460376069503163
Validation loss: 4.3270865065529796

Epoch: 5| Step: 8
Training loss: 4.964105510675429
Validation loss: 4.321668052361724

Epoch: 5| Step: 9
Training loss: 4.988681953365632
Validation loss: 4.316459607100927

Epoch: 5| Step: 10
Training loss: 4.558057320355299
Validation loss: 4.310389596673706

Epoch: 5| Step: 11
Training loss: 5.965211468487059
Validation loss: 4.305387611942104

Epoch: 23| Step: 0
Training loss: 4.129746682662735
Validation loss: 4.299270147291004

Epoch: 5| Step: 1
Training loss: 4.366854250618238
Validation loss: 4.293670985620652

Epoch: 5| Step: 2
Training loss: 5.010681187658567
Validation loss: 4.287754167339251

Epoch: 5| Step: 3
Training loss: 4.107599259127403
Validation loss: 4.282050709899734

Epoch: 5| Step: 4
Training loss: 5.0465562087904505
Validation loss: 4.276833439640411

Epoch: 5| Step: 5
Training loss: 4.41936084458581
Validation loss: 4.270886931044288

Epoch: 5| Step: 6
Training loss: 4.507936261096251
Validation loss: 4.264960701885496

Epoch: 5| Step: 7
Training loss: 4.145118204416958
Validation loss: 4.260030932635026

Epoch: 5| Step: 8
Training loss: 3.8104380049958815
Validation loss: 4.254415266200332

Epoch: 5| Step: 9
Training loss: 4.800624314079538
Validation loss: 4.249436425260092

Epoch: 5| Step: 10
Training loss: 4.146285193785291
Validation loss: 4.243781589396103

Epoch: 5| Step: 11
Training loss: 2.936198169288145
Validation loss: 4.23842041443443

Epoch: 24| Step: 0
Training loss: 4.063916238267023
Validation loss: 4.2333808332441665

Epoch: 5| Step: 1
Training loss: 4.1412753763984425
Validation loss: 4.228266176966778

Epoch: 5| Step: 2
Training loss: 4.234234233923472
Validation loss: 4.222972950923335

Epoch: 5| Step: 3
Training loss: 4.473878491910121
Validation loss: 4.2177522303560435

Epoch: 5| Step: 4
Training loss: 4.799283196651366
Validation loss: 4.212512493775326

Epoch: 5| Step: 5
Training loss: 4.619336965634266
Validation loss: 4.207654854245637

Epoch: 5| Step: 6
Training loss: 4.102531275098238
Validation loss: 4.2017786358404345

Epoch: 5| Step: 7
Training loss: 4.201082889283066
Validation loss: 4.196266757218385

Epoch: 5| Step: 8
Training loss: 4.6357017629438735
Validation loss: 4.191221941201642

Epoch: 5| Step: 9
Training loss: 4.175146527488703
Validation loss: 4.1859274041639205

Epoch: 5| Step: 10
Training loss: 4.048251945103609
Validation loss: 4.180662651035041

Epoch: 5| Step: 11
Training loss: 4.956049203352877
Validation loss: 4.174833179993841

Epoch: 25| Step: 0
Training loss: 4.1237374887915506
Validation loss: 4.169790692614784

Epoch: 5| Step: 1
Training loss: 4.440750435997778
Validation loss: 4.164331423894876

Epoch: 5| Step: 2
Training loss: 4.0580067361519125
Validation loss: 4.158920195200087

Epoch: 5| Step: 3
Training loss: 4.688130450767457
Validation loss: 4.15437275456004

Epoch: 5| Step: 4
Training loss: 4.903450136606622
Validation loss: 4.148110649226221

Epoch: 5| Step: 5
Training loss: 4.709465745888203
Validation loss: 4.14275554808806

Epoch: 5| Step: 6
Training loss: 4.353283416866901
Validation loss: 4.137377140529858

Epoch: 5| Step: 7
Training loss: 3.820094814222343
Validation loss: 4.132068386765679

Epoch: 5| Step: 8
Training loss: 4.18906157115455
Validation loss: 4.1268276924296154

Epoch: 5| Step: 9
Training loss: 3.758852872160705
Validation loss: 4.121633287639923

Epoch: 5| Step: 10
Training loss: 4.196974781977685
Validation loss: 4.1166800536711525

Epoch: 5| Step: 11
Training loss: 1.72471588946362
Validation loss: 4.110959708408639

Epoch: 26| Step: 0
Training loss: 3.879116947717274
Validation loss: 4.106546543006055

Epoch: 5| Step: 1
Training loss: 3.5532045058562294
Validation loss: 4.101674725647198

Epoch: 5| Step: 2
Training loss: 4.4743352811893855
Validation loss: 4.096930008864961

Epoch: 5| Step: 3
Training loss: 3.891348530238465
Validation loss: 4.091603412773734

Epoch: 5| Step: 4
Training loss: 4.502548979345074
Validation loss: 4.086675656261067

Epoch: 5| Step: 5
Training loss: 4.258428182621829
Validation loss: 4.081906422922581

Epoch: 5| Step: 6
Training loss: 4.401473249533322
Validation loss: 4.076926019811237

Epoch: 5| Step: 7
Training loss: 4.180066699358275
Validation loss: 4.071745919684524

Epoch: 5| Step: 8
Training loss: 4.231431677114252
Validation loss: 4.067391106824902

Epoch: 5| Step: 9
Training loss: 4.116217077212206
Validation loss: 4.061589187832201

Epoch: 5| Step: 10
Training loss: 4.705367152678302
Validation loss: 4.056406023211556

Epoch: 5| Step: 11
Training loss: 4.2958366110364
Validation loss: 4.051359438855437

Epoch: 27| Step: 0
Training loss: 4.530704018157276
Validation loss: 4.046168036060363

Epoch: 5| Step: 1
Training loss: 3.7897742163405783
Validation loss: 4.04091436555161

Epoch: 5| Step: 2
Training loss: 3.977418818577448
Validation loss: 4.035886331042607

Epoch: 5| Step: 3
Training loss: 3.8740463313855926
Validation loss: 4.031052636334966

Epoch: 5| Step: 4
Training loss: 4.429023921720216
Validation loss: 4.026356040389577

Epoch: 5| Step: 5
Training loss: 3.876904788544712
Validation loss: 4.021113211356962

Epoch: 5| Step: 6
Training loss: 4.121432061794812
Validation loss: 4.015977717839316

Epoch: 5| Step: 7
Training loss: 4.416044563289748
Validation loss: 4.010174720038403

Epoch: 5| Step: 8
Training loss: 4.042353751911233
Validation loss: 4.005144466821881

Epoch: 5| Step: 9
Training loss: 4.353342565397217
Validation loss: 4.000621777130769

Epoch: 5| Step: 10
Training loss: 4.053444967824003
Validation loss: 3.995025934597495

Epoch: 5| Step: 11
Training loss: 4.786095650775185
Validation loss: 3.9904786525448954

Epoch: 28| Step: 0
Training loss: 3.889670871647895
Validation loss: 3.985636133803833

Epoch: 5| Step: 1
Training loss: 4.144125786451896
Validation loss: 3.980060170835418

Epoch: 5| Step: 2
Training loss: 3.9360254342257583
Validation loss: 3.9749610239193363

Epoch: 5| Step: 3
Training loss: 4.700634876956512
Validation loss: 3.9699021867141693

Epoch: 5| Step: 4
Training loss: 4.818599848009716
Validation loss: 3.9646463780377665

Epoch: 5| Step: 5
Training loss: 3.4904405290700913
Validation loss: 3.9597695790068164

Epoch: 5| Step: 6
Training loss: 4.210007488760551
Validation loss: 3.9547721738177

Epoch: 5| Step: 7
Training loss: 4.123680134782996
Validation loss: 3.9496636346055305

Epoch: 5| Step: 8
Training loss: 3.6151139513894663
Validation loss: 3.944435072419102

Epoch: 5| Step: 9
Training loss: 3.424693022844561
Validation loss: 3.939550491564609

Epoch: 5| Step: 10
Training loss: 4.486466720758621
Validation loss: 3.934494304388334

Epoch: 5| Step: 11
Training loss: 3.7091363330344347
Validation loss: 3.929900323503231

Epoch: 29| Step: 0
Training loss: 5.08150071842872
Validation loss: 3.9247065855612573

Epoch: 5| Step: 1
Training loss: 4.334704304502292
Validation loss: 3.91929696751327

Epoch: 5| Step: 2
Training loss: 3.8742776012777784
Validation loss: 3.9140484696046043

Epoch: 5| Step: 3
Training loss: 3.8799135373500286
Validation loss: 3.9090859049858446

Epoch: 5| Step: 4
Training loss: 3.6412214191754275
Validation loss: 3.904130808415551

Epoch: 5| Step: 5
Training loss: 3.7424810688607697
Validation loss: 3.8989253328180187

Epoch: 5| Step: 6
Training loss: 3.580421343894723
Validation loss: 3.8943707666775595

Epoch: 5| Step: 7
Training loss: 4.091505993539056
Validation loss: 3.8899002166100356

Epoch: 5| Step: 8
Training loss: 4.517874926074897
Validation loss: 3.8851939419615156

Epoch: 5| Step: 9
Training loss: 3.9355014391375356
Validation loss: 3.8802167504067837

Epoch: 5| Step: 10
Training loss: 3.751309611206905
Validation loss: 3.8752546021691923

Epoch: 5| Step: 11
Training loss: 2.091026698638481
Validation loss: 3.870466066572857

Epoch: 30| Step: 0
Training loss: 3.5215887730209343
Validation loss: 3.8659242718205373

Epoch: 5| Step: 1
Training loss: 3.907958000613377
Validation loss: 3.8613693603001558

Epoch: 5| Step: 2
Training loss: 4.473392449304349
Validation loss: 3.8567038338572925

Epoch: 5| Step: 3
Training loss: 4.085617257108408
Validation loss: 3.851951115252469

Epoch: 5| Step: 4
Training loss: 4.723601619734977
Validation loss: 3.8471725753690107

Epoch: 5| Step: 5
Training loss: 3.676222025827427
Validation loss: 3.842704625626421

Epoch: 5| Step: 6
Training loss: 4.391402966413779
Validation loss: 3.837831473764243

Epoch: 5| Step: 7
Training loss: 3.7840777456027017
Validation loss: 3.8329618298147494

Epoch: 5| Step: 8
Training loss: 3.7183187539227895
Validation loss: 3.828079550337062

Epoch: 5| Step: 9
Training loss: 3.563104544759697
Validation loss: 3.8231012246107343

Epoch: 5| Step: 10
Training loss: 3.6167057779426988
Validation loss: 3.8184269865770464

Epoch: 5| Step: 11
Training loss: 4.380639066016394
Validation loss: 3.8139362704485813

Epoch: 31| Step: 0
Training loss: 4.156123797572921
Validation loss: 3.8093726667439274

Epoch: 5| Step: 1
Training loss: 4.298453634017567
Validation loss: 3.804578352660903

Epoch: 5| Step: 2
Training loss: 4.065132110063045
Validation loss: 3.799755055915425

Epoch: 5| Step: 3
Training loss: 4.13120404331887
Validation loss: 3.7950375458003873

Epoch: 5| Step: 4
Training loss: 4.059481394300733
Validation loss: 3.7903921199922253

Epoch: 5| Step: 5
Training loss: 3.0547394809056327
Validation loss: 3.7853131493015404

Epoch: 5| Step: 6
Training loss: 3.7237761054991907
Validation loss: 3.7806384535243365

Epoch: 5| Step: 7
Training loss: 4.099751274193827
Validation loss: 3.7762824760033036

Epoch: 5| Step: 8
Training loss: 3.7366931057033486
Validation loss: 3.772090558263872

Epoch: 5| Step: 9
Training loss: 3.6461668606742212
Validation loss: 3.767394049989275

Epoch: 5| Step: 10
Training loss: 3.9376669954279175
Validation loss: 3.7626698929797406

Epoch: 5| Step: 11
Training loss: 4.254235345109863
Validation loss: 3.7576688023914215

Epoch: 32| Step: 0
Training loss: 4.073513185700062
Validation loss: 3.7534279626116356

Epoch: 5| Step: 1
Training loss: 3.757224943102461
Validation loss: 3.7483236009605148

Epoch: 5| Step: 2
Training loss: 3.2941342211154105
Validation loss: 3.743411103683705

Epoch: 5| Step: 3
Training loss: 3.5032797841323844
Validation loss: 3.7388120969910275

Epoch: 5| Step: 4
Training loss: 4.414833744154517
Validation loss: 3.73440129844099

Epoch: 5| Step: 5
Training loss: 4.026257165797625
Validation loss: 3.7296968683434906

Epoch: 5| Step: 6
Training loss: 2.5725748141339655
Validation loss: 3.7252117884036617

Epoch: 5| Step: 7
Training loss: 4.554060773441462
Validation loss: 3.7209556994478064

Epoch: 5| Step: 8
Training loss: 4.135737921522298
Validation loss: 3.716086296490349

Epoch: 5| Step: 9
Training loss: 3.798501000134456
Validation loss: 3.7117349778306847

Epoch: 5| Step: 10
Training loss: 4.134946910143712
Validation loss: 3.706993807626016

Epoch: 5| Step: 11
Training loss: 3.017219398681553
Validation loss: 3.702482277722372

Epoch: 33| Step: 0
Training loss: 3.8984348090703387
Validation loss: 3.6974519177883423

Epoch: 5| Step: 1
Training loss: 4.17672459051991
Validation loss: 3.6929052414291084

Epoch: 5| Step: 2
Training loss: 4.327274610797306
Validation loss: 3.688445244884173

Epoch: 5| Step: 3
Training loss: 3.8286697934249907
Validation loss: 3.683937836073543

Epoch: 5| Step: 4
Training loss: 3.3014685282081704
Validation loss: 3.6790909982217634

Epoch: 5| Step: 5
Training loss: 3.9114421060211133
Validation loss: 3.674461871168858

Epoch: 5| Step: 6
Training loss: 3.433723039330289
Validation loss: 3.67007705281902

Epoch: 5| Step: 7
Training loss: 3.1146906363248608
Validation loss: 3.6653270369436672

Epoch: 5| Step: 8
Training loss: 4.153960780656978
Validation loss: 3.6609117290758215

Epoch: 5| Step: 9
Training loss: 3.756360191721983
Validation loss: 3.6565404814615325

Epoch: 5| Step: 10
Training loss: 3.844228327072166
Validation loss: 3.652603689589234

Epoch: 5| Step: 11
Training loss: 3.80639284363852
Validation loss: 3.648063645959304

Epoch: 34| Step: 0
Training loss: 3.6843506893944564
Validation loss: 3.643351836612712

Epoch: 5| Step: 1
Training loss: 4.195360236722127
Validation loss: 3.6385356818207364

Epoch: 5| Step: 2
Training loss: 3.541033130903363
Validation loss: 3.633760324521877

Epoch: 5| Step: 3
Training loss: 3.0545860332212915
Validation loss: 3.6290419149070923

Epoch: 5| Step: 4
Training loss: 3.4533638849956696
Validation loss: 3.6244127959909593

Epoch: 5| Step: 5
Training loss: 3.6454257991628563
Validation loss: 3.620043407806933

Epoch: 5| Step: 6
Training loss: 4.380148120274244
Validation loss: 3.615572542091903

Epoch: 5| Step: 7
Training loss: 4.153488963003983
Validation loss: 3.6109032856404086

Epoch: 5| Step: 8
Training loss: 3.393754209512309
Validation loss: 3.6061731368875414

Epoch: 5| Step: 9
Training loss: 3.524936940596337
Validation loss: 3.6016947900579375

Epoch: 5| Step: 10
Training loss: 4.258684597470494
Validation loss: 3.597376953063673

Epoch: 5| Step: 11
Training loss: 2.6336353499013487
Validation loss: 3.5928041567936018

Epoch: 35| Step: 0
Training loss: 3.9341083027176826
Validation loss: 3.5880772585877474

Epoch: 5| Step: 1
Training loss: 3.42865002349052
Validation loss: 3.5836451675699315

Epoch: 5| Step: 2
Training loss: 3.5240720190251333
Validation loss: 3.5793331055731397

Epoch: 5| Step: 3
Training loss: 4.204773760321509
Validation loss: 3.5752189918170836

Epoch: 5| Step: 4
Training loss: 3.6531861338464306
Validation loss: 3.5708050989721034

Epoch: 5| Step: 5
Training loss: 4.088962230393188
Validation loss: 3.5663291530068233

Epoch: 5| Step: 6
Training loss: 3.4439401120640003
Validation loss: 3.5617578469599187

Epoch: 5| Step: 7
Training loss: 4.013534060894337
Validation loss: 3.5573968252758656

Epoch: 5| Step: 8
Training loss: 4.019330522958171
Validation loss: 3.5533145810348743

Epoch: 5| Step: 9
Training loss: 2.8870880757907345
Validation loss: 3.5487081745025857

Epoch: 5| Step: 10
Training loss: 3.2369735525404266
Validation loss: 3.544529905620945

Epoch: 5| Step: 11
Training loss: 4.101246500103872
Validation loss: 3.540113165419632

Epoch: 36| Step: 0
Training loss: 3.376225425633606
Validation loss: 3.5357905372087344

Epoch: 5| Step: 1
Training loss: 3.9820953429843535
Validation loss: 3.5313858017366635

Epoch: 5| Step: 2
Training loss: 3.2720325158795633
Validation loss: 3.5269948590367974

Epoch: 5| Step: 3
Training loss: 3.844553173477866
Validation loss: 3.522596587370072

Epoch: 5| Step: 4
Training loss: 5.031853870617085
Validation loss: 3.5184919464816393

Epoch: 5| Step: 5
Training loss: 3.15942661080998
Validation loss: 3.5140114535859914

Epoch: 5| Step: 6
Training loss: 3.525657208367757
Validation loss: 3.5094907748882855

Epoch: 5| Step: 7
Training loss: 3.190010727532505
Validation loss: 3.5052506990786867

Epoch: 5| Step: 8
Training loss: 3.5158144751458402
Validation loss: 3.50073760071489

Epoch: 5| Step: 9
Training loss: 2.9404642286143767
Validation loss: 3.496659046694511

Epoch: 5| Step: 10
Training loss: 3.724261903282738
Validation loss: 3.4924614419813427

Epoch: 5| Step: 11
Training loss: 4.417203666622344
Validation loss: 3.488075891069422

Epoch: 37| Step: 0
Training loss: 3.68167740780697
Validation loss: 3.4837852400872658

Epoch: 5| Step: 1
Training loss: 3.8133083799741727
Validation loss: 3.4796967568950286

Epoch: 5| Step: 2
Training loss: 3.5949429066333063
Validation loss: 3.4756176010937527

Epoch: 5| Step: 3
Training loss: 3.3988317502099674
Validation loss: 3.471405273122347

Epoch: 5| Step: 4
Training loss: 3.395429871975407
Validation loss: 3.4671565969750913

Epoch: 5| Step: 5
Training loss: 3.4434197518458802
Validation loss: 3.4629384320999153

Epoch: 5| Step: 6
Training loss: 3.609428240230648
Validation loss: 3.458803700245163

Epoch: 5| Step: 7
Training loss: 4.174732158543954
Validation loss: 3.4545654641089834

Epoch: 5| Step: 8
Training loss: 3.458787099338056
Validation loss: 3.4503420379187855

Epoch: 5| Step: 9
Training loss: 3.4069158620698494
Validation loss: 3.4461557015334976

Epoch: 5| Step: 10
Training loss: 3.415779153568355
Validation loss: 3.442023393341282

Epoch: 5| Step: 11
Training loss: 4.236885255766579
Validation loss: 3.437484706497851

Epoch: 38| Step: 0
Training loss: 3.7595658682729978
Validation loss: 3.4330792813764273

Epoch: 5| Step: 1
Training loss: 3.298154754302047
Validation loss: 3.4285904701920833

Epoch: 5| Step: 2
Training loss: 3.2024209341969265
Validation loss: 3.424238674678243

Epoch: 5| Step: 3
Training loss: 3.239869983089479
Validation loss: 3.420010870373425

Epoch: 5| Step: 4
Training loss: 3.0673624479485566
Validation loss: 3.4159556245996234

Epoch: 5| Step: 5
Training loss: 3.2230434485669126
Validation loss: 3.412229611906427

Epoch: 5| Step: 6
Training loss: 4.315289465962722
Validation loss: 3.4083103283305514

Epoch: 5| Step: 7
Training loss: 3.718091120973544
Validation loss: 3.403897161138198

Epoch: 5| Step: 8
Training loss: 3.4580193955432947
Validation loss: 3.3992638880343167

Epoch: 5| Step: 9
Training loss: 3.558537203258881
Validation loss: 3.394743847966212

Epoch: 5| Step: 10
Training loss: 3.7736845389744524
Validation loss: 3.390452929934123

Epoch: 5| Step: 11
Training loss: 4.576352345859859
Validation loss: 3.3858323570723883

Epoch: 39| Step: 0
Training loss: 4.067339083836064
Validation loss: 3.3812484943341885

Epoch: 5| Step: 1
Training loss: 3.8132080921422964
Validation loss: 3.376652083649421

Epoch: 5| Step: 2
Training loss: 3.5185433037388703
Validation loss: 3.3721445920385693

Epoch: 5| Step: 3
Training loss: 3.7545921183395747
Validation loss: 3.367640536367543

Epoch: 5| Step: 4
Training loss: 3.675565382875002
Validation loss: 3.363008293863808

Epoch: 5| Step: 5
Training loss: 2.6207393400948336
Validation loss: 3.358261361467093

Epoch: 5| Step: 6
Training loss: 3.3206015786383007
Validation loss: 3.3536137762945315

Epoch: 5| Step: 7
Training loss: 3.662917488962156
Validation loss: 3.3496366538669196

Epoch: 5| Step: 8
Training loss: 3.5772386810808277
Validation loss: 3.3455000750999155

Epoch: 5| Step: 9
Training loss: 3.030105846809826
Validation loss: 3.340727090301555

Epoch: 5| Step: 10
Training loss: 3.236923908837148
Validation loss: 3.3364334690685213

Epoch: 5| Step: 11
Training loss: 3.4683623226139066
Validation loss: 3.332698849056085

Epoch: 40| Step: 0
Training loss: 3.302676912661714
Validation loss: 3.3288705495182187

Epoch: 5| Step: 1
Training loss: 2.969069132466723
Validation loss: 3.3249913956475847

Epoch: 5| Step: 2
Training loss: 2.919129086041191
Validation loss: 3.321294607882986

Epoch: 5| Step: 3
Training loss: 2.8792401434598918
Validation loss: 3.3175968508751676

Epoch: 5| Step: 4
Training loss: 3.646974061798916
Validation loss: 3.3142600391968022

Epoch: 5| Step: 5
Training loss: 3.0114692159108825
Validation loss: 3.3105722372008155

Epoch: 5| Step: 6
Training loss: 3.759191438501273
Validation loss: 3.3073219938714975

Epoch: 5| Step: 7
Training loss: 3.7619259976252706
Validation loss: 3.30375972196467

Epoch: 5| Step: 8
Training loss: 3.791815003787661
Validation loss: 3.2999304282435964

Epoch: 5| Step: 9
Training loss: 3.911688840586798
Validation loss: 3.2959939071580058

Epoch: 5| Step: 10
Training loss: 3.9043800455826836
Validation loss: 3.291810271997601

Epoch: 5| Step: 11
Training loss: 2.287268029630038
Validation loss: 3.288071332777667

Epoch: 41| Step: 0
Training loss: 3.349441390649272
Validation loss: 3.2838534244997346

Epoch: 5| Step: 1
Training loss: 2.8755953628415627
Validation loss: 3.279956956431718

Epoch: 5| Step: 2
Training loss: 3.377773840452989
Validation loss: 3.2764123495659043

Epoch: 5| Step: 3
Training loss: 3.5357278435517334
Validation loss: 3.272346812578319

Epoch: 5| Step: 4
Training loss: 3.6704600103559324
Validation loss: 3.2683823730710055

Epoch: 5| Step: 5
Training loss: 3.3868969933090867
Validation loss: 3.264475006995453

Epoch: 5| Step: 6
Training loss: 3.424154697996717
Validation loss: 3.2605955401866513

Epoch: 5| Step: 7
Training loss: 3.650781960531174
Validation loss: 3.2565726307698353

Epoch: 5| Step: 8
Training loss: 3.6119482602440716
Validation loss: 3.2525946151475056

Epoch: 5| Step: 9
Training loss: 2.9380357943562325
Validation loss: 3.248877074577042

Epoch: 5| Step: 10
Training loss: 3.130839575635305
Validation loss: 3.2449510003530846

Epoch: 5| Step: 11
Training loss: 5.00087234516604
Validation loss: 3.2414531718771853

Epoch: 42| Step: 0
Training loss: 3.1300538966989424
Validation loss: 3.237241703147625

Epoch: 5| Step: 1
Training loss: 3.4162980012560715
Validation loss: 3.2334704524064057

Epoch: 5| Step: 2
Training loss: 2.9503866750457637
Validation loss: 3.229289130483105

Epoch: 5| Step: 3
Training loss: 2.9913923437074748
Validation loss: 3.2252204265227715

Epoch: 5| Step: 4
Training loss: 3.141377980319206
Validation loss: 3.221542104153936

Epoch: 5| Step: 5
Training loss: 3.3875185188265857
Validation loss: 3.217934616289398

Epoch: 5| Step: 6
Training loss: 3.3938841735282
Validation loss: 3.214718673701752

Epoch: 5| Step: 7
Training loss: 3.309369461299883
Validation loss: 3.211045132679311

Epoch: 5| Step: 8
Training loss: 3.7312580971174705
Validation loss: 3.2076470282987875

Epoch: 5| Step: 9
Training loss: 3.7048435006184413
Validation loss: 3.2034419329398633

Epoch: 5| Step: 10
Training loss: 3.4792201481589906
Validation loss: 3.1998566739449563

Epoch: 5| Step: 11
Training loss: 4.248433216986509
Validation loss: 3.195943917686758

Epoch: 43| Step: 0
Training loss: 3.7748986489540997
Validation loss: 3.1919144249577647

Epoch: 5| Step: 1
Training loss: 3.3339823726744133
Validation loss: 3.188378558909112

Epoch: 5| Step: 2
Training loss: 2.61947895911428
Validation loss: 3.184417340362451

Epoch: 5| Step: 3
Training loss: 2.615929144491774
Validation loss: 3.1809065947583606

Epoch: 5| Step: 4
Training loss: 3.4213045942517235
Validation loss: 3.1774718479597137

Epoch: 5| Step: 5
Training loss: 3.4134856201512687
Validation loss: 3.184644897673084

Epoch: 5| Step: 6
Training loss: 3.866170976918392
Validation loss: 3.1706748972111654

Epoch: 5| Step: 7
Training loss: 3.2928657922905997
Validation loss: 3.1702334786925097

Epoch: 5| Step: 8
Training loss: 3.183947210056376
Validation loss: 3.1747847929680963

Epoch: 5| Step: 9
Training loss: 3.455092754085056
Validation loss: 3.17167823318989

Epoch: 5| Step: 10
Training loss: 2.985589863628504
Validation loss: 3.160915187745003

Epoch: 5| Step: 11
Training loss: 4.512687810228291
Validation loss: 3.1539352414399793

Epoch: 44| Step: 0
Training loss: 3.7318471868370118
Validation loss: 3.1497007948195574

Epoch: 5| Step: 1
Training loss: 3.4793411637516742
Validation loss: 3.1474638475417858

Epoch: 5| Step: 2
Training loss: 3.077260781976088
Validation loss: 3.1448819235432306

Epoch: 5| Step: 3
Training loss: 2.8063066744444547
Validation loss: 3.1430985150473276

Epoch: 5| Step: 4
Training loss: 3.268839590912678
Validation loss: 3.138735602981483

Epoch: 5| Step: 5
Training loss: 3.450763844503803
Validation loss: 3.133140667274315

Epoch: 5| Step: 6
Training loss: 3.77314697158126
Validation loss: 3.128104162257933

Epoch: 5| Step: 7
Training loss: 2.5674126218136797
Validation loss: 3.124056368454814

Epoch: 5| Step: 8
Training loss: 2.802016322434865
Validation loss: 3.120222546983021

Epoch: 5| Step: 9
Training loss: 3.6784741128226055
Validation loss: 3.116441304342889

Epoch: 5| Step: 10
Training loss: 3.207931559453337
Validation loss: 3.113672491145413

Epoch: 5| Step: 11
Training loss: 2.905048080808068
Validation loss: 3.1097424544012786

Epoch: 45| Step: 0
Training loss: 3.582136412629553
Validation loss: 3.1063222899746363

Epoch: 5| Step: 1
Training loss: 3.506771349438285
Validation loss: 3.1032196345501997

Epoch: 5| Step: 2
Training loss: 2.9525683650436187
Validation loss: 3.0991675020466194

Epoch: 5| Step: 3
Training loss: 3.387966256235967
Validation loss: 3.096604407967486

Epoch: 5| Step: 4
Training loss: 3.0383092736236548
Validation loss: 3.091805280335424

Epoch: 5| Step: 5
Training loss: 3.3816582734699603
Validation loss: 3.088132594241494

Epoch: 5| Step: 6
Training loss: 2.6475473894267227
Validation loss: 3.0848652775621033

Epoch: 5| Step: 7
Training loss: 2.8646513404865295
Validation loss: 3.0815983973159105

Epoch: 5| Step: 8
Training loss: 3.2718204701423907
Validation loss: 3.07917829587923

Epoch: 5| Step: 9
Training loss: 3.760146972340453
Validation loss: 3.0746408226909927

Epoch: 5| Step: 10
Training loss: 2.989273285599859
Validation loss: 3.071951004283318

Epoch: 5| Step: 11
Training loss: 3.115102888339942
Validation loss: 3.0682213564240026

Epoch: 46| Step: 0
Training loss: 3.1665264567070004
Validation loss: 3.0648163871311853

Epoch: 5| Step: 1
Training loss: 3.8277146722851167
Validation loss: 3.061581506335593

Epoch: 5| Step: 2
Training loss: 2.921020872559326
Validation loss: 3.057704284655585

Epoch: 5| Step: 3
Training loss: 3.0890297378091613
Validation loss: 3.054320909335874

Epoch: 5| Step: 4
Training loss: 3.1534784820195076
Validation loss: 3.0508090712759923

Epoch: 5| Step: 5
Training loss: 3.3514806919739693
Validation loss: 3.047694638853167

Epoch: 5| Step: 6
Training loss: 2.81703490120606
Validation loss: 3.044515028340873

Epoch: 5| Step: 7
Training loss: 3.2691765413921994
Validation loss: 3.0409292291689423

Epoch: 5| Step: 8
Training loss: 3.443254682363358
Validation loss: 3.037415523165428

Epoch: 5| Step: 9
Training loss: 2.9381820211616465
Validation loss: 3.0344399712073025

Epoch: 5| Step: 10
Training loss: 3.006640079434308
Validation loss: 3.0311629489315086

Epoch: 5| Step: 11
Training loss: 3.102514471593606
Validation loss: 3.028251465924431

Epoch: 47| Step: 0
Training loss: 3.371649597949428
Validation loss: 3.0249518020194994

Epoch: 5| Step: 1
Training loss: 2.9028231197955097
Validation loss: 3.0220246673412885

Epoch: 5| Step: 2
Training loss: 3.481248624791091
Validation loss: 3.0185435144848505

Epoch: 5| Step: 3
Training loss: 3.183426891236955
Validation loss: 3.015787291565789

Epoch: 5| Step: 4
Training loss: 3.092084956769916
Validation loss: 3.01254842225937

Epoch: 5| Step: 5
Training loss: 3.2288364036420885
Validation loss: 3.0092436156042877

Epoch: 5| Step: 6
Training loss: 2.7496367127871872
Validation loss: 3.0054735973488627

Epoch: 5| Step: 7
Training loss: 3.0801931491581183
Validation loss: 3.0045551645830035

Epoch: 5| Step: 8
Training loss: 2.9561750212953837
Validation loss: 3.0002379919065634

Epoch: 5| Step: 9
Training loss: 3.413385319711256
Validation loss: 2.9967761002531574

Epoch: 5| Step: 10
Training loss: 3.0851668156017644
Validation loss: 2.994163391995535

Epoch: 5| Step: 11
Training loss: 3.348637226821602
Validation loss: 2.991573028285583

Epoch: 48| Step: 0
Training loss: 3.05904239954625
Validation loss: 3.000136822653378

Epoch: 5| Step: 1
Training loss: 3.3469314413716615
Validation loss: 2.985466576048351

Epoch: 5| Step: 2
Training loss: 2.752889069267349
Validation loss: 2.9830523117910106

Epoch: 5| Step: 3
Training loss: 3.283784305435426
Validation loss: 2.9808863609324514

Epoch: 5| Step: 4
Training loss: 3.3260557520650083
Validation loss: 2.978567557794139

Epoch: 5| Step: 5
Training loss: 3.342642056878085
Validation loss: 2.9774242465314145

Epoch: 5| Step: 6
Training loss: 2.8781154172657044
Validation loss: 2.976070466846316

Epoch: 5| Step: 7
Training loss: 2.775826985620963
Validation loss: 2.9755313552375724

Epoch: 5| Step: 8
Training loss: 3.233972349304628
Validation loss: 2.973757579505714

Epoch: 5| Step: 9
Training loss: 3.417732553222807
Validation loss: 2.9697868142857153

Epoch: 5| Step: 10
Training loss: 2.7959748423239956
Validation loss: 2.9668408294964337

Epoch: 5| Step: 11
Training loss: 3.330945892480881
Validation loss: 2.963905973562973

Epoch: 49| Step: 0
Training loss: 3.1931648031797724
Validation loss: 2.9607007645821897

Epoch: 5| Step: 1
Training loss: 2.819753998381625
Validation loss: 2.957417747136268

Epoch: 5| Step: 2
Training loss: 3.1925375531287346
Validation loss: 2.9564340476340836

Epoch: 5| Step: 3
Training loss: 3.41624873016479
Validation loss: 2.953244477669045

Epoch: 5| Step: 4
Training loss: 3.0067568028621166
Validation loss: 2.9509523058843747

Epoch: 5| Step: 5
Training loss: 3.1981910480167155
Validation loss: 2.9472373622300077

Epoch: 5| Step: 6
Training loss: 2.7973034067083478
Validation loss: 2.944239197381162

Epoch: 5| Step: 7
Training loss: 3.4662284965412153
Validation loss: 2.9405180293756787

Epoch: 5| Step: 8
Training loss: 2.702928657236598
Validation loss: 2.937986462130603

Epoch: 5| Step: 9
Training loss: 3.1768283595763394
Validation loss: 2.9346432220321783

Epoch: 5| Step: 10
Training loss: 2.8429789074126393
Validation loss: 2.9312264160205235

Epoch: 5| Step: 11
Training loss: 3.3162449234723383
Validation loss: 2.927428989884814

Epoch: 50| Step: 0
Training loss: 3.300565850266557
Validation loss: 2.9248579611620587

Epoch: 5| Step: 1
Training loss: 2.46360997003841
Validation loss: 2.9211839072798473

Epoch: 5| Step: 2
Training loss: 3.590004868570496
Validation loss: 2.9185370760356535

Epoch: 5| Step: 3
Training loss: 2.8430181546699984
Validation loss: 2.914937778504595

Epoch: 5| Step: 4
Training loss: 2.8764150702943083
Validation loss: 2.9123728799433337

Epoch: 5| Step: 5
Training loss: 2.5735177227363875
Validation loss: 2.9093017040325186

Epoch: 5| Step: 6
Training loss: 3.464561476708288
Validation loss: 2.90677894031128

Epoch: 5| Step: 7
Training loss: 2.930193152977688
Validation loss: 2.904108406934703

Epoch: 5| Step: 8
Training loss: 3.2277655402290963
Validation loss: 2.9020650052936183

Epoch: 5| Step: 9
Training loss: 3.0372654325862594
Validation loss: 2.899194589396155

Epoch: 5| Step: 10
Training loss: 3.128171608317546
Validation loss: 2.8964815214781403

Epoch: 5| Step: 11
Training loss: 2.7140978769688098
Validation loss: 2.8939317364676866

Epoch: 51| Step: 0
Training loss: 2.946401541449677
Validation loss: 2.8918197602761158

Epoch: 5| Step: 1
Training loss: 2.8568120424305774
Validation loss: 2.889400002664787

Epoch: 5| Step: 2
Training loss: 2.795931950140796
Validation loss: 2.8875712992795264

Epoch: 5| Step: 3
Training loss: 3.1233899355726034
Validation loss: 2.8847807976580473

Epoch: 5| Step: 4
Training loss: 2.9103336971383076
Validation loss: 2.882631942346308

Epoch: 5| Step: 5
Training loss: 3.6742505769123746
Validation loss: 2.87987129120297

Epoch: 5| Step: 6
Training loss: 2.746371302679927
Validation loss: 2.8776946500933738

Epoch: 5| Step: 7
Training loss: 3.3673993462215903
Validation loss: 2.875586636442206

Epoch: 5| Step: 8
Training loss: 2.6368918128968364
Validation loss: 2.872467366374328

Epoch: 5| Step: 9
Training loss: 3.023749125978455
Validation loss: 2.8701266331542654

Epoch: 5| Step: 10
Training loss: 3.1030435941377204
Validation loss: 2.867625113897614

Epoch: 5| Step: 11
Training loss: 2.46020826404059
Validation loss: 2.8646390470230907

Epoch: 52| Step: 0
Training loss: 3.1371423950661512
Validation loss: 2.862846129811571

Epoch: 5| Step: 1
Training loss: 2.977250627889686
Validation loss: 2.859886658685701

Epoch: 5| Step: 2
Training loss: 2.797854118945167
Validation loss: 2.8572677045747525

Epoch: 5| Step: 3
Training loss: 3.346718442017368
Validation loss: 2.8550309742774433

Epoch: 5| Step: 4
Training loss: 3.1877187018070754
Validation loss: 2.852383127100058

Epoch: 5| Step: 5
Training loss: 2.306087986494149
Validation loss: 2.8505084628512773

Epoch: 5| Step: 6
Training loss: 3.1277072623754627
Validation loss: 2.8472756932569445

Epoch: 5| Step: 7
Training loss: 2.8970101433004585
Validation loss: 2.845689817601256

Epoch: 5| Step: 8
Training loss: 3.1043697691831684
Validation loss: 2.843559014191554

Epoch: 5| Step: 9
Training loss: 2.653656534928945
Validation loss: 2.8401750758712017

Epoch: 5| Step: 10
Training loss: 3.1829127794516583
Validation loss: 2.8385609524039626

Epoch: 5| Step: 11
Training loss: 3.286271095483861
Validation loss: 2.836201412927433

Epoch: 53| Step: 0
Training loss: 3.0724810447759987
Validation loss: 2.833810767217877

Epoch: 5| Step: 1
Training loss: 2.6678446214432765
Validation loss: 2.8318702108263683

Epoch: 5| Step: 2
Training loss: 3.1146425647695777
Validation loss: 2.830109326892865

Epoch: 5| Step: 3
Training loss: 2.628605682286101
Validation loss: 2.8269421121684024

Epoch: 5| Step: 4
Training loss: 2.8290215014452698
Validation loss: 2.8251687576165887

Epoch: 5| Step: 5
Training loss: 3.0305214232439805
Validation loss: 2.8234344455665177

Epoch: 5| Step: 6
Training loss: 2.9917377502440536
Validation loss: 2.820303031926628

Epoch: 5| Step: 7
Training loss: 3.0323449742881863
Validation loss: 2.8189988462755418

Epoch: 5| Step: 8
Training loss: 2.451667401201081
Validation loss: 2.8172225122009262

Epoch: 5| Step: 9
Training loss: 3.547496858774183
Validation loss: 2.813965076749025

Epoch: 5| Step: 10
Training loss: 2.9293564266057963
Validation loss: 2.811153573479647

Epoch: 5| Step: 11
Training loss: 3.675779952759468
Validation loss: 2.8089363833536973

Epoch: 54| Step: 0
Training loss: 3.066284799296691
Validation loss: 2.8075052556379068

Epoch: 5| Step: 1
Training loss: 2.3186435027227676
Validation loss: 2.804862736830073

Epoch: 5| Step: 2
Training loss: 3.0727662820171284
Validation loss: 2.80333915326183

Epoch: 5| Step: 3
Training loss: 2.821639322132239
Validation loss: 2.8013308139760493

Epoch: 5| Step: 4
Training loss: 2.9148880758112883
Validation loss: 2.7991630529494116

Epoch: 5| Step: 5
Training loss: 3.1400795837813518
Validation loss: 2.7969749049239634

Epoch: 5| Step: 6
Training loss: 2.9152677360055996
Validation loss: 2.794741617854863

Epoch: 5| Step: 7
Training loss: 2.6418153686868853
Validation loss: 2.7929882555727317

Epoch: 5| Step: 8
Training loss: 3.604591535688913
Validation loss: 2.790609420499355

Epoch: 5| Step: 9
Training loss: 3.1389457806257974
Validation loss: 2.7881574435620715

Epoch: 5| Step: 10
Training loss: 2.536146443726604
Validation loss: 2.786325516907426

Epoch: 5| Step: 11
Training loss: 2.7282368675271713
Validation loss: 2.784256710047259

Epoch: 55| Step: 0
Training loss: 2.8137914447621557
Validation loss: 2.7831223825264932

Epoch: 5| Step: 1
Training loss: 3.0400398467614633
Validation loss: 2.7811989297356994

Epoch: 5| Step: 2
Training loss: 3.043296551462983
Validation loss: 2.785091657990807

Epoch: 5| Step: 3
Training loss: 3.0524138357387542
Validation loss: 2.779122764225388

Epoch: 5| Step: 4
Training loss: 2.4716956033725515
Validation loss: 2.7757241721797925

Epoch: 5| Step: 5
Training loss: 2.6400817673476045
Validation loss: 2.7752389564387214

Epoch: 5| Step: 6
Training loss: 3.0269247441887983
Validation loss: 2.773174194588531

Epoch: 5| Step: 7
Training loss: 3.1528871958861626
Validation loss: 2.772763506162969

Epoch: 5| Step: 8
Training loss: 3.0496547441026074
Validation loss: 2.76915679968531

Epoch: 5| Step: 9
Training loss: 2.430789611688248
Validation loss: 2.771815023307366

Epoch: 5| Step: 10
Training loss: 3.3350481549722617
Validation loss: 2.7726091213382453

Epoch: 5| Step: 11
Training loss: 2.4272654932996884
Validation loss: 2.7716549549115315

Epoch: 56| Step: 0
Training loss: 2.9971945678767606
Validation loss: 2.77628224578708

Epoch: 5| Step: 1
Training loss: 3.0949418055373976
Validation loss: 2.7661478083656217

Epoch: 5| Step: 2
Training loss: 2.740751754630739
Validation loss: 2.763881158711429

Epoch: 5| Step: 3
Training loss: 2.6843309569825755
Validation loss: 2.7596558776041267

Epoch: 5| Step: 4
Training loss: 3.3625226555383665
Validation loss: 2.7591246238238973

Epoch: 5| Step: 5
Training loss: 2.233390751380522
Validation loss: 2.75850370928711

Epoch: 5| Step: 6
Training loss: 3.1184220319319
Validation loss: 2.757381992760495

Epoch: 5| Step: 7
Training loss: 3.15937122076848
Validation loss: 2.753089541994599

Epoch: 5| Step: 8
Training loss: 2.830798324025601
Validation loss: 2.7501800434822985

Epoch: 5| Step: 9
Training loss: 2.7185639065599125
Validation loss: 2.748714558406286

Epoch: 5| Step: 10
Training loss: 2.8229987964201904
Validation loss: 2.746957978466112

Epoch: 5| Step: 11
Training loss: 2.8282171413522894
Validation loss: 2.7455897513110497

Epoch: 57| Step: 0
Training loss: 2.3905599747335042
Validation loss: 2.7435783851034414

Epoch: 5| Step: 1
Training loss: 3.1660257159289373
Validation loss: 2.741104328658648

Epoch: 5| Step: 2
Training loss: 2.791814648326269
Validation loss: 2.7402997343317255

Epoch: 5| Step: 3
Training loss: 2.727424725718466
Validation loss: 2.737897718183353

Epoch: 5| Step: 4
Training loss: 3.1314225383437893
Validation loss: 2.7361643431552705

Epoch: 5| Step: 5
Training loss: 2.709724792639146
Validation loss: 2.7349472882068855

Epoch: 5| Step: 6
Training loss: 2.8748414369063062
Validation loss: 2.7328920393881666

Epoch: 5| Step: 7
Training loss: 3.056172900017243
Validation loss: 2.7312347327299586

Epoch: 5| Step: 8
Training loss: 2.9816712919497523
Validation loss: 2.72833500753323

Epoch: 5| Step: 9
Training loss: 2.325651762474402
Validation loss: 2.725769875057063

Epoch: 5| Step: 10
Training loss: 3.3136893962094245
Validation loss: 2.722895545605148

Epoch: 5| Step: 11
Training loss: 2.814531652134671
Validation loss: 2.723991581049553

Epoch: 58| Step: 0
Training loss: 2.1568517536335454
Validation loss: 2.7210953122402004

Epoch: 5| Step: 1
Training loss: 2.7786652630761544
Validation loss: 2.722969606337782

Epoch: 5| Step: 2
Training loss: 3.153170603638797
Validation loss: 2.7212659295240393

Epoch: 5| Step: 3
Training loss: 2.793587398704257
Validation loss: 2.7191517229825983

Epoch: 5| Step: 4
Training loss: 3.293986858848944
Validation loss: 2.7234634778988247

Epoch: 5| Step: 5
Training loss: 2.662121286456523
Validation loss: 2.7123982344492386

Epoch: 5| Step: 6
Training loss: 2.5207541166947434
Validation loss: 2.712060789068397

Epoch: 5| Step: 7
Training loss: 3.2531164339595886
Validation loss: 2.708419961032965

Epoch: 5| Step: 8
Training loss: 2.3448026200700567
Validation loss: 2.7080541295629006

Epoch: 5| Step: 9
Training loss: 2.880037124447496
Validation loss: 2.7077232969823064

Epoch: 5| Step: 10
Training loss: 3.1582941668521882
Validation loss: 2.7076504335442415

Epoch: 5| Step: 11
Training loss: 3.376616338170897
Validation loss: 2.706475905958737

Epoch: 59| Step: 0
Training loss: 2.833673419363534
Validation loss: 2.704570372814938

Epoch: 5| Step: 1
Training loss: 2.629912638645618
Validation loss: 2.703795399524966

Epoch: 5| Step: 2
Training loss: 2.912102661534431
Validation loss: 2.7020638207579357

Epoch: 5| Step: 3
Training loss: 2.8096354837983606
Validation loss: 2.7000536329040616

Epoch: 5| Step: 4
Training loss: 3.13189638208548
Validation loss: 2.7004521262178423

Epoch: 5| Step: 5
Training loss: 2.4286141431882795
Validation loss: 2.697863859349615

Epoch: 5| Step: 6
Training loss: 2.766837333869175
Validation loss: 2.6928188748915245

Epoch: 5| Step: 7
Training loss: 3.1922576403649288
Validation loss: 2.6915638346461876

Epoch: 5| Step: 8
Training loss: 3.231481760003354
Validation loss: 2.692205383335848

Epoch: 5| Step: 9
Training loss: 3.176146388910477
Validation loss: 2.689187832086959

Epoch: 5| Step: 10
Training loss: 2.0862397964255446
Validation loss: 2.6905426832449066

Epoch: 5| Step: 11
Training loss: 1.6544068355112882
Validation loss: 2.691764386935514

Epoch: 60| Step: 0
Training loss: 2.937956429591449
Validation loss: 2.6902079542190243

Epoch: 5| Step: 1
Training loss: 2.6108218995578647
Validation loss: 2.689734143895383

Epoch: 5| Step: 2
Training loss: 2.8493715831537796
Validation loss: 2.6933720703415935

Epoch: 5| Step: 3
Training loss: 3.497950089828701
Validation loss: 2.6872817143054535

Epoch: 5| Step: 4
Training loss: 3.094069030958256
Validation loss: 2.683651053137517

Epoch: 5| Step: 5
Training loss: 2.9384283668467814
Validation loss: 2.6811294939101087

Epoch: 5| Step: 6
Training loss: 2.5200066169773
Validation loss: 2.6791287201962204

Epoch: 5| Step: 7
Training loss: 2.9863587821349875
Validation loss: 2.679088677615443

Epoch: 5| Step: 8
Training loss: 2.397087885468107
Validation loss: 2.6762641749391927

Epoch: 5| Step: 9
Training loss: 2.692345531428276
Validation loss: 2.67347838717775

Epoch: 5| Step: 10
Training loss: 2.4757337657724015
Validation loss: 2.672602363020137

Epoch: 5| Step: 11
Training loss: 1.8334337409296118
Validation loss: 2.672617636197881

Epoch: 61| Step: 0
Training loss: 2.6115088047242154
Validation loss: 2.6737704330027396

Epoch: 5| Step: 1
Training loss: 2.731641525733174
Validation loss: 2.6736195206337876

Epoch: 5| Step: 2
Training loss: 2.8683356291255446
Validation loss: 2.689248174404021

Epoch: 5| Step: 3
Training loss: 3.0443470958667485
Validation loss: 2.6767273541285252

Epoch: 5| Step: 4
Training loss: 2.786263394295769
Validation loss: 2.667118959266911

Epoch: 5| Step: 5
Training loss: 2.8977535294211876
Validation loss: 2.6663920191554498

Epoch: 5| Step: 6
Training loss: 2.658731625255369
Validation loss: 2.666009763680933

Epoch: 5| Step: 7
Training loss: 2.912242494913864
Validation loss: 2.6666658421356195

Epoch: 5| Step: 8
Training loss: 2.681791141561779
Validation loss: 2.6661956092901145

Epoch: 5| Step: 9
Training loss: 3.035762770449909
Validation loss: 2.6676349496237757

Epoch: 5| Step: 10
Training loss: 2.6668342199933406
Validation loss: 2.669113580056921

Epoch: 5| Step: 11
Training loss: 2.6660521117008735
Validation loss: 2.667583245965948

Epoch: 62| Step: 0
Training loss: 3.211888230551337
Validation loss: 2.6662184492564323

Epoch: 5| Step: 1
Training loss: 2.8180698766877694
Validation loss: 2.6649812659713885

Epoch: 5| Step: 2
Training loss: 2.7041318925483946
Validation loss: 2.6599076874270073

Epoch: 5| Step: 3
Training loss: 2.525001348362931
Validation loss: 2.6593567762038104

Epoch: 5| Step: 4
Training loss: 2.802480099936621
Validation loss: 2.657006522511028

Epoch: 5| Step: 5
Training loss: 2.8031829793966363
Validation loss: 2.6555468657106234

Epoch: 5| Step: 6
Training loss: 2.849428480968044
Validation loss: 2.6512112566371124

Epoch: 5| Step: 7
Training loss: 2.4092149179122475
Validation loss: 2.648589059549769

Epoch: 5| Step: 8
Training loss: 3.286328699585041
Validation loss: 2.64979051295768

Epoch: 5| Step: 9
Training loss: 2.767311917937634
Validation loss: 2.6473010014979104

Epoch: 5| Step: 10
Training loss: 2.3898417866584367
Validation loss: 2.6442971438189122

Epoch: 5| Step: 11
Training loss: 2.9897983664204824
Validation loss: 2.64466807595116

Epoch: 63| Step: 0
Training loss: 2.335403432363353
Validation loss: 2.642310426643384

Epoch: 5| Step: 1
Training loss: 2.632533751993182
Validation loss: 2.641701172403756

Epoch: 5| Step: 2
Training loss: 2.584767148803059
Validation loss: 2.6395883876769206

Epoch: 5| Step: 3
Training loss: 2.6495806272224645
Validation loss: 2.642365124922611

Epoch: 5| Step: 4
Training loss: 2.7231101666095308
Validation loss: 2.6432290376309266

Epoch: 5| Step: 5
Training loss: 2.462849870022387
Validation loss: 2.643116650120537

Epoch: 5| Step: 6
Training loss: 2.8973531416900378
Validation loss: 2.6394093666666634

Epoch: 5| Step: 7
Training loss: 3.0175811273751245
Validation loss: 2.6375619740911262

Epoch: 5| Step: 8
Training loss: 3.2236681649928363
Validation loss: 2.6363399393624554

Epoch: 5| Step: 9
Training loss: 3.2246812411548005
Validation loss: 2.636761368772077

Epoch: 5| Step: 10
Training loss: 2.787313131731656
Validation loss: 2.631680283812198

Epoch: 5| Step: 11
Training loss: 1.7799135515029865
Validation loss: 2.6329330771405135

Epoch: 64| Step: 0
Training loss: 2.7694469702719857
Validation loss: 2.6336364965931303

Epoch: 5| Step: 1
Training loss: 2.9862082078707095
Validation loss: 2.6334655547949692

Epoch: 5| Step: 2
Training loss: 3.114809281096514
Validation loss: 2.6340095334391025

Epoch: 5| Step: 3
Training loss: 2.080843416657293
Validation loss: 2.6350593751904943

Epoch: 5| Step: 4
Training loss: 2.7325842524851693
Validation loss: 2.635336777955106

Epoch: 5| Step: 5
Training loss: 2.2833944196488685
Validation loss: 2.6353068699537787

Epoch: 5| Step: 6
Training loss: 2.786813464960055
Validation loss: 2.633912193579937

Epoch: 5| Step: 7
Training loss: 2.9412465653762188
Validation loss: 2.6317387701532806

Epoch: 5| Step: 8
Training loss: 2.610434676765568
Validation loss: 2.630618465035115

Epoch: 5| Step: 9
Training loss: 2.8935746273675935
Validation loss: 2.6274761676645135

Epoch: 5| Step: 10
Training loss: 3.1094716119373618
Validation loss: 2.6254425735377995

Epoch: 5| Step: 11
Training loss: 2.639533183989609
Validation loss: 2.6262539979122623

Epoch: 65| Step: 0
Training loss: 2.9636065448616122
Validation loss: 2.621963599204281

Epoch: 5| Step: 1
Training loss: 2.5772548247390334
Validation loss: 2.6211655555285005

Epoch: 5| Step: 2
Training loss: 2.829773817425712
Validation loss: 2.620114798869986

Epoch: 5| Step: 3
Training loss: 2.545344729655621
Validation loss: 2.6170817035908254

Epoch: 5| Step: 4
Training loss: 2.952051522660783
Validation loss: 2.614842403287287

Epoch: 5| Step: 5
Training loss: 3.159855511174754
Validation loss: 2.6141216293430216

Epoch: 5| Step: 6
Training loss: 2.4004638263413773
Validation loss: 2.613578300528476

Epoch: 5| Step: 7
Training loss: 2.639747970573049
Validation loss: 2.613348347515732

Epoch: 5| Step: 8
Training loss: 2.3508551157803463
Validation loss: 2.6121308551124014

Epoch: 5| Step: 9
Training loss: 2.8949707356556704
Validation loss: 2.611151172593269

Epoch: 5| Step: 10
Training loss: 2.9726063627651174
Validation loss: 2.6073734426584134

Epoch: 5| Step: 11
Training loss: 2.1355263906682302
Validation loss: 2.606730356324548

Epoch: 66| Step: 0
Training loss: 2.811744927668412
Validation loss: 2.606841774558569

Epoch: 5| Step: 1
Training loss: 2.6309045052852404
Validation loss: 2.6055896622312735

Epoch: 5| Step: 2
Training loss: 2.278956789143543
Validation loss: 2.6005490212441136

Epoch: 5| Step: 3
Training loss: 3.0652741304310682
Validation loss: 2.6060609692540697

Epoch: 5| Step: 4
Training loss: 2.6083241248247546
Validation loss: 2.6078172629198417

Epoch: 5| Step: 5
Training loss: 2.744923587840338
Validation loss: 2.608174796105555

Epoch: 5| Step: 6
Training loss: 2.415376570981174
Validation loss: 2.6004632607419484

Epoch: 5| Step: 7
Training loss: 3.104316008045511
Validation loss: 2.599354231551873

Epoch: 5| Step: 8
Training loss: 2.9624410953379594
Validation loss: 2.599138843533817

Epoch: 5| Step: 9
Training loss: 2.3986880889107027
Validation loss: 2.5992007107496145

Epoch: 5| Step: 10
Training loss: 3.002512515484265
Validation loss: 2.5959415522428526

Epoch: 5| Step: 11
Training loss: 2.334476792492022
Validation loss: 2.596051401578688

Epoch: 67| Step: 0
Training loss: 2.4285552400963044
Validation loss: 2.5958183957604413

Epoch: 5| Step: 1
Training loss: 2.8716643512500264
Validation loss: 2.597943653852637

Epoch: 5| Step: 2
Training loss: 3.0271481161586866
Validation loss: 2.5948806734157666

Epoch: 5| Step: 3
Training loss: 2.333723875695392
Validation loss: 2.600886072728003

Epoch: 5| Step: 4
Training loss: 2.926511787916121
Validation loss: 2.6049822458429777

Epoch: 5| Step: 5
Training loss: 2.6797188181478924
Validation loss: 2.6087256402141246

Epoch: 5| Step: 6
Training loss: 2.5667107574176824
Validation loss: 2.599654887293244

Epoch: 5| Step: 7
Training loss: 2.8371994531579046
Validation loss: 2.5933240249629357

Epoch: 5| Step: 8
Training loss: 3.0206082139593624
Validation loss: 2.587091129051829

Epoch: 5| Step: 9
Training loss: 2.364390114663578
Validation loss: 2.5838430822086367

Epoch: 5| Step: 10
Training loss: 2.779596758900442
Validation loss: 2.585940423447108

Epoch: 5| Step: 11
Training loss: 3.2765029719906966
Validation loss: 2.5873688676872226

Epoch: 68| Step: 0
Training loss: 2.5905474459056963
Validation loss: 2.5802375615209474

Epoch: 5| Step: 1
Training loss: 2.3471140814654863
Validation loss: 2.5832338980539595

Epoch: 5| Step: 2
Training loss: 3.09178561644152
Validation loss: 2.5844721988418358

Epoch: 5| Step: 3
Training loss: 2.744805197830193
Validation loss: 2.5849766939146805

Epoch: 5| Step: 4
Training loss: 2.3815880803285006
Validation loss: 2.582968441255109

Epoch: 5| Step: 5
Training loss: 2.4378433963777186
Validation loss: 2.5795694793623265

Epoch: 5| Step: 6
Training loss: 2.935478813717396
Validation loss: 2.5755613585479784

Epoch: 5| Step: 7
Training loss: 2.9383996438088267
Validation loss: 2.5779213718954845

Epoch: 5| Step: 8
Training loss: 2.8265711272336294
Validation loss: 2.583339903935926

Epoch: 5| Step: 9
Training loss: 2.6212843983384664
Validation loss: 2.5822752405905756

Epoch: 5| Step: 10
Training loss: 2.9259784463960794
Validation loss: 2.57894733684745

Epoch: 5| Step: 11
Training loss: 2.3289154554183216
Validation loss: 2.581436569669706

Epoch: 69| Step: 0
Training loss: 2.8136975599814225
Validation loss: 2.5761769882510026

Epoch: 5| Step: 1
Training loss: 2.488582095848622
Validation loss: 2.578215464536471

Epoch: 5| Step: 2
Training loss: 2.8768392567524335
Validation loss: 2.5752319942827864

Epoch: 5| Step: 3
Training loss: 2.852638355212999
Validation loss: 2.580291388854451

Epoch: 5| Step: 4
Training loss: 2.5281046410343238
Validation loss: 2.575857139875602

Epoch: 5| Step: 5
Training loss: 2.4491021282167953
Validation loss: 2.5744708781437255

Epoch: 5| Step: 6
Training loss: 2.0119403605940716
Validation loss: 2.5707695659929235

Epoch: 5| Step: 7
Training loss: 2.8289158173836246
Validation loss: 2.571153930095767

Epoch: 5| Step: 8
Training loss: 3.0760035296197885
Validation loss: 2.5653310349471306

Epoch: 5| Step: 9
Training loss: 3.021696471628213
Validation loss: 2.569325351246225

Epoch: 5| Step: 10
Training loss: 2.69277932364311
Validation loss: 2.5861487211109706

Epoch: 5| Step: 11
Training loss: 2.4042952883829045
Validation loss: 2.606289499350961

Epoch: 70| Step: 0
Training loss: 2.3660879067723717
Validation loss: 2.6220217567082402

Epoch: 5| Step: 1
Training loss: 2.7881933402036445
Validation loss: 2.6104136434742036

Epoch: 5| Step: 2
Training loss: 2.955384858140984
Validation loss: 2.573229741085095

Epoch: 5| Step: 3
Training loss: 2.611441062704788
Validation loss: 2.565246989861677

Epoch: 5| Step: 4
Training loss: 2.855429840866492
Validation loss: 2.568096330175841

Epoch: 5| Step: 5
Training loss: 3.179972267989629
Validation loss: 2.574338475068812

Epoch: 5| Step: 6
Training loss: 2.66784381713574
Validation loss: 2.5821630585654147

Epoch: 5| Step: 7
Training loss: 2.9888977611588117
Validation loss: 2.591222075038759

Epoch: 5| Step: 8
Training loss: 2.646865873534987
Validation loss: 2.5975133756195894

Epoch: 5| Step: 9
Training loss: 2.6320986360080227
Validation loss: 2.59807992792849

Epoch: 5| Step: 10
Training loss: 2.286121653546292
Validation loss: 2.599844662293055

Epoch: 5| Step: 11
Training loss: 2.7717359024120114
Validation loss: 2.5967086478583

Epoch: 71| Step: 0
Training loss: 2.543458484188609
Validation loss: 2.5889226235280156

Epoch: 5| Step: 1
Training loss: 3.2538391326173683
Validation loss: 2.5855741639365397

Epoch: 5| Step: 2
Training loss: 2.9308127721767216
Validation loss: 2.582385790516091

Epoch: 5| Step: 3
Training loss: 2.203382869265575
Validation loss: 2.5744280732430576

Epoch: 5| Step: 4
Training loss: 2.605354272251981
Validation loss: 2.571314043599077

Epoch: 5| Step: 5
Training loss: 2.715480724229076
Validation loss: 2.5723988800947324

Epoch: 5| Step: 6
Training loss: 2.6152263524428854
Validation loss: 2.577762142809729

Epoch: 5| Step: 7
Training loss: 3.1437301847466657
Validation loss: 2.5778852177866782

Epoch: 5| Step: 8
Training loss: 2.486371851294604
Validation loss: 2.5753427346844084

Epoch: 5| Step: 9
Training loss: 2.694167538033213
Validation loss: 2.569643503646166

Epoch: 5| Step: 10
Training loss: 2.365154337454118
Validation loss: 2.565514013140987

Epoch: 5| Step: 11
Training loss: 2.607394443324802
Validation loss: 2.5604773385164266

Epoch: 72| Step: 0
Training loss: 2.4001419025432122
Validation loss: 2.5585444477174795

Epoch: 5| Step: 1
Training loss: 2.7559597286388104
Validation loss: 2.554358870496819

Epoch: 5| Step: 2
Training loss: 2.5101607787171254
Validation loss: 2.5535792242039785

Epoch: 5| Step: 3
Training loss: 2.62292571082663
Validation loss: 2.553612952604524

Epoch: 5| Step: 4
Training loss: 2.410883912673309
Validation loss: 2.5566215116147273

Epoch: 5| Step: 5
Training loss: 3.051398416337508
Validation loss: 2.555390680190206

Epoch: 5| Step: 6
Training loss: 2.430599519846679
Validation loss: 2.5553167658888385

Epoch: 5| Step: 7
Training loss: 2.761058938983117
Validation loss: 2.5588143445304268

Epoch: 5| Step: 8
Training loss: 3.0539287590706032
Validation loss: 2.559801044953509

Epoch: 5| Step: 9
Training loss: 2.9022484580167403
Validation loss: 2.556337024292433

Epoch: 5| Step: 10
Training loss: 2.8175685563500785
Validation loss: 2.5567233910312845

Epoch: 5| Step: 11
Training loss: 1.1774998774599308
Validation loss: 2.547139861955785

Epoch: 73| Step: 0
Training loss: 2.6609307728635883
Validation loss: 2.5483418786442225

Epoch: 5| Step: 1
Training loss: 2.393267199878578
Validation loss: 2.544499324871205

Epoch: 5| Step: 2
Training loss: 2.461827096155859
Validation loss: 2.545929040738292

Epoch: 5| Step: 3
Training loss: 2.7864675551407467
Validation loss: 2.54827969473949

Epoch: 5| Step: 4
Training loss: 2.7388264199080288
Validation loss: 2.54901974324367

Epoch: 5| Step: 5
Training loss: 2.637965019416397
Validation loss: 2.5477704188393595

Epoch: 5| Step: 6
Training loss: 2.8160918294390713
Validation loss: 2.5455587102130988

Epoch: 5| Step: 7
Training loss: 3.108809836701023
Validation loss: 2.544813206993085

Epoch: 5| Step: 8
Training loss: 2.3677278915338156
Validation loss: 2.5433200216590754

Epoch: 5| Step: 9
Training loss: 2.8055834176051277
Validation loss: 2.5429423731750957

Epoch: 5| Step: 10
Training loss: 2.510916147099722
Validation loss: 2.539185103366306

Epoch: 5| Step: 11
Training loss: 3.039579607355781
Validation loss: 2.5373644114507248

Epoch: 74| Step: 0
Training loss: 2.5007226853095927
Validation loss: 2.5359391079856617

Epoch: 5| Step: 1
Training loss: 2.7725361823566534
Validation loss: 2.538512873704801

Epoch: 5| Step: 2
Training loss: 3.2584646473316474
Validation loss: 2.5418218633035967

Epoch: 5| Step: 3
Training loss: 2.4957242641488127
Validation loss: 2.534848555518568

Epoch: 5| Step: 4
Training loss: 2.9186945450475013
Validation loss: 2.532646292508952

Epoch: 5| Step: 5
Training loss: 2.4515103414225203
Validation loss: 2.5391224976322286

Epoch: 5| Step: 6
Training loss: 2.465890217913802
Validation loss: 2.542586583545263

Epoch: 5| Step: 7
Training loss: 2.9062682120460117
Validation loss: 2.5436153459017845

Epoch: 5| Step: 8
Training loss: 2.8173271292699242
Validation loss: 2.543827558392642

Epoch: 5| Step: 9
Training loss: 2.506188172602796
Validation loss: 2.5425830085579735

Epoch: 5| Step: 10
Training loss: 2.2157492964600447
Validation loss: 2.5444907825763026

Epoch: 5| Step: 11
Training loss: 2.714210139921886
Validation loss: 2.5443832049873536

Epoch: 75| Step: 0
Training loss: 2.7936264864182676
Validation loss: 2.5470745616965447

Epoch: 5| Step: 1
Training loss: 3.1830843090764915
Validation loss: 2.5438416326266187

Epoch: 5| Step: 2
Training loss: 2.494295287209869
Validation loss: 2.5454870159098015

Epoch: 5| Step: 3
Training loss: 2.269357802678628
Validation loss: 2.54580311754675

Epoch: 5| Step: 4
Training loss: 2.8687884359340488
Validation loss: 2.545807636233271

Epoch: 5| Step: 5
Training loss: 3.1072251445228236
Validation loss: 2.5420606039490345

Epoch: 5| Step: 6
Training loss: 2.6487721819666654
Validation loss: 2.5387918572580697

Epoch: 5| Step: 7
Training loss: 2.7706207155572735
Validation loss: 2.537002866651654

Epoch: 5| Step: 8
Training loss: 2.065990499919604
Validation loss: 2.537461262048645

Epoch: 5| Step: 9
Training loss: 2.3577456880056173
Validation loss: 2.536577097927634

Epoch: 5| Step: 10
Training loss: 2.525692997625724
Validation loss: 2.5368352697406107

Epoch: 5| Step: 11
Training loss: 2.9846097045000373
Validation loss: 2.533172285666254

Epoch: 76| Step: 0
Training loss: 2.5307215386214423
Validation loss: 2.5351805722520244

Epoch: 5| Step: 1
Training loss: 2.6537267031839678
Validation loss: 2.534815851134165

Epoch: 5| Step: 2
Training loss: 2.865955555860723
Validation loss: 2.534754548381985

Epoch: 5| Step: 3
Training loss: 2.647184812751336
Validation loss: 2.5329735935509925

Epoch: 5| Step: 4
Training loss: 2.6984738593200404
Validation loss: 2.5314609004099164

Epoch: 5| Step: 5
Training loss: 2.515446626961753
Validation loss: 2.5307091421614905

Epoch: 5| Step: 6
Training loss: 2.339068097940498
Validation loss: 2.529987126136714

Epoch: 5| Step: 7
Training loss: 2.887837978911533
Validation loss: 2.527746275617001

Epoch: 5| Step: 8
Training loss: 2.455527131772941
Validation loss: 2.52682345113572

Epoch: 5| Step: 9
Training loss: 2.672642826157004
Validation loss: 2.524718240087333

Epoch: 5| Step: 10
Training loss: 2.9447927648818664
Validation loss: 2.5296732226089227

Epoch: 5| Step: 11
Training loss: 2.476319310134424
Validation loss: 2.5232859035499975

Epoch: 77| Step: 0
Training loss: 2.6550393431024015
Validation loss: 2.525822361868055

Epoch: 5| Step: 1
Training loss: 2.58225285074027
Validation loss: 2.5230404292420636

Epoch: 5| Step: 2
Training loss: 2.9364904738651183
Validation loss: 2.5253240385273044

Epoch: 5| Step: 3
Training loss: 2.4052942532975647
Validation loss: 2.5230899646270935

Epoch: 5| Step: 4
Training loss: 2.66650377213136
Validation loss: 2.524687678784122

Epoch: 5| Step: 5
Training loss: 2.4081582763653846
Validation loss: 2.5244581286324568

Epoch: 5| Step: 6
Training loss: 2.611778477229153
Validation loss: 2.5217261874361467

Epoch: 5| Step: 7
Training loss: 2.5385038260069845
Validation loss: 2.5218197584503064

Epoch: 5| Step: 8
Training loss: 2.809002800070595
Validation loss: 2.5174619590173224

Epoch: 5| Step: 9
Training loss: 2.684341881650966
Validation loss: 2.5147255657922973

Epoch: 5| Step: 10
Training loss: 2.673967222687633
Validation loss: 2.510995989286227

Epoch: 5| Step: 11
Training loss: 3.362395875587811
Validation loss: 2.512580956946657

Epoch: 78| Step: 0
Training loss: 2.5739977543486883
Validation loss: 2.5137116004948625

Epoch: 5| Step: 1
Training loss: 2.8550737893210854
Validation loss: 2.515026325362336

Epoch: 5| Step: 2
Training loss: 2.66085138635179
Validation loss: 2.516867518357426

Epoch: 5| Step: 3
Training loss: 2.7025816266544975
Validation loss: 2.518035630077573

Epoch: 5| Step: 4
Training loss: 2.8140303792608266
Validation loss: 2.5219751693452106

Epoch: 5| Step: 5
Training loss: 2.366798494813462
Validation loss: 2.5195864373207897

Epoch: 5| Step: 6
Training loss: 2.0322704392938786
Validation loss: 2.5177514702401838

Epoch: 5| Step: 7
Training loss: 2.3756550337316304
Validation loss: 2.5170608987509895

Epoch: 5| Step: 8
Training loss: 3.1281569746976587
Validation loss: 2.5220377162482324

Epoch: 5| Step: 9
Training loss: 2.8841048654546118
Validation loss: 2.518143733619179

Epoch: 5| Step: 10
Training loss: 2.5176637337558665
Validation loss: 2.5170165649758616

Epoch: 5| Step: 11
Training loss: 2.7059662280290726
Validation loss: 2.5123976783097732

Epoch: 79| Step: 0
Training loss: 2.7720300676220635
Validation loss: 2.5186096635064055

Epoch: 5| Step: 1
Training loss: 2.4673108118996865
Validation loss: 2.5238636786806325

Epoch: 5| Step: 2
Training loss: 2.786826212238438
Validation loss: 2.515073621043942

Epoch: 5| Step: 3
Training loss: 3.005435787344422
Validation loss: 2.5151918010620706

Epoch: 5| Step: 4
Training loss: 2.1201159504904843
Validation loss: 2.512875470056885

Epoch: 5| Step: 5
Training loss: 2.7371145421215233
Validation loss: 2.510909073104366

Epoch: 5| Step: 6
Training loss: 2.1039749536582746
Validation loss: 2.5095622454023307

Epoch: 5| Step: 7
Training loss: 2.8670687533950656
Validation loss: 2.509256052418164

Epoch: 5| Step: 8
Training loss: 2.830312483556831
Validation loss: 2.5077141715764535

Epoch: 5| Step: 9
Training loss: 2.6325265066871872
Validation loss: 2.5055650561508065

Epoch: 5| Step: 10
Training loss: 2.3980700680483737
Validation loss: 2.509445589592083

Epoch: 5| Step: 11
Training loss: 3.123455123508154
Validation loss: 2.518028078988952

Epoch: 80| Step: 0
Training loss: 2.895584717834085
Validation loss: 2.5293987749504816

Epoch: 5| Step: 1
Training loss: 2.511385549985559
Validation loss: 2.53798139199534

Epoch: 5| Step: 2
Training loss: 2.6463150026521256
Validation loss: 2.534326869289633

Epoch: 5| Step: 3
Training loss: 3.1257485065494777
Validation loss: 2.534728246765874

Epoch: 5| Step: 4
Training loss: 2.409726393589461
Validation loss: 2.5229554793640268

Epoch: 5| Step: 5
Training loss: 2.8021624150987465
Validation loss: 2.505433531782174

Epoch: 5| Step: 6
Training loss: 2.7139220944258953
Validation loss: 2.501359911436669

Epoch: 5| Step: 7
Training loss: 2.401579646965766
Validation loss: 2.507673909367601

Epoch: 5| Step: 8
Training loss: 2.630070013557548
Validation loss: 2.5150845580930703

Epoch: 5| Step: 9
Training loss: 2.274458298799632
Validation loss: 2.514317956830978

Epoch: 5| Step: 10
Training loss: 2.616386632654976
Validation loss: 2.519766602871842

Epoch: 5| Step: 11
Training loss: 2.5384550805887804
Validation loss: 2.51927593031683

Epoch: 81| Step: 0
Training loss: 3.1585029643728606
Validation loss: 2.5227987191507335

Epoch: 5| Step: 1
Training loss: 2.564341371981918
Validation loss: 2.526284921213799

Epoch: 5| Step: 2
Training loss: 2.549145581583358
Validation loss: 2.52840483853116

Epoch: 5| Step: 3
Training loss: 3.0854259984512584
Validation loss: 2.5302511332255295

Epoch: 5| Step: 4
Training loss: 2.3574189420743457
Validation loss: 2.5294910056927065

Epoch: 5| Step: 5
Training loss: 2.540214582375221
Validation loss: 2.528445731252447

Epoch: 5| Step: 6
Training loss: 2.8256627951028443
Validation loss: 2.5236076951083986

Epoch: 5| Step: 7
Training loss: 2.4114018564771884
Validation loss: 2.520029906771575

Epoch: 5| Step: 8
Training loss: 2.525815050375061
Validation loss: 2.520210458448137

Epoch: 5| Step: 9
Training loss: 2.670929144590392
Validation loss: 2.522121216053423

Epoch: 5| Step: 10
Training loss: 2.409560762163685
Validation loss: 2.5194783562522107

Epoch: 5| Step: 11
Training loss: 2.2600801161685053
Validation loss: 2.518391645700189

Epoch: 82| Step: 0
Training loss: 3.0393941742411648
Validation loss: 2.5180847431625017

Epoch: 5| Step: 1
Training loss: 2.748321280914708
Validation loss: 2.518663628598399

Epoch: 5| Step: 2
Training loss: 2.824882798590969
Validation loss: 2.5158428863878086

Epoch: 5| Step: 3
Training loss: 2.734338989020685
Validation loss: 2.5274510348551646

Epoch: 5| Step: 4
Training loss: 1.9157755134533345
Validation loss: 2.5255190567594608

Epoch: 5| Step: 5
Training loss: 2.7222204749000114
Validation loss: 2.528066477748545

Epoch: 5| Step: 6
Training loss: 2.3515115016337202
Validation loss: 2.5311569699118652

Epoch: 5| Step: 7
Training loss: 2.588826297536379
Validation loss: 2.522813721868392

Epoch: 5| Step: 8
Training loss: 2.641111069585065
Validation loss: 2.524400960173752

Epoch: 5| Step: 9
Training loss: 2.400511202410426
Validation loss: 2.5131356577096606

Epoch: 5| Step: 10
Training loss: 3.0852298746141145
Validation loss: 2.5089922830456146

Epoch: 5| Step: 11
Training loss: 1.68029781165575
Validation loss: 2.5090549595863205

Epoch: 83| Step: 0
Training loss: 2.441955602256338
Validation loss: 2.5022231945637667

Epoch: 5| Step: 1
Training loss: 2.423752579906521
Validation loss: 2.50017697184746

Epoch: 5| Step: 2
Training loss: 2.6195606005918894
Validation loss: 2.4985579225657095

Epoch: 5| Step: 3
Training loss: 2.793175835034802
Validation loss: 2.5018569487073696

Epoch: 5| Step: 4
Training loss: 2.161113395896735
Validation loss: 2.49768711230513

Epoch: 5| Step: 5
Training loss: 2.637248210730268
Validation loss: 2.499093996229281

Epoch: 5| Step: 6
Training loss: 3.044052303270893
Validation loss: 2.508726454552877

Epoch: 5| Step: 7
Training loss: 2.5363797613633805
Validation loss: 2.5058526078427903

Epoch: 5| Step: 8
Training loss: 2.938342359171604
Validation loss: 2.507127151247931

Epoch: 5| Step: 9
Training loss: 2.4384668828965372
Validation loss: 2.5113804669981987

Epoch: 5| Step: 10
Training loss: 2.749685096050347
Validation loss: 2.5093477091684875

Epoch: 5| Step: 11
Training loss: 2.9760200226428077
Validation loss: 2.5078379509922772

Epoch: 84| Step: 0
Training loss: 2.402966624821515
Validation loss: 2.5088579052836604

Epoch: 5| Step: 1
Training loss: 2.9817282239154435
Validation loss: 2.514072397171875

Epoch: 5| Step: 2
Training loss: 2.3375898660122885
Validation loss: 2.5182603770349967

Epoch: 5| Step: 3
Training loss: 2.5936056809317836
Validation loss: 2.5186984911211936

Epoch: 5| Step: 4
Training loss: 2.8170178896031595
Validation loss: 2.5213090169313346

Epoch: 5| Step: 5
Training loss: 2.7619263653231307
Validation loss: 2.5175492921553806

Epoch: 5| Step: 6
Training loss: 2.9551037814381114
Validation loss: 2.52230130294892

Epoch: 5| Step: 7
Training loss: 2.5394821993869967
Validation loss: 2.510573106589829

Epoch: 5| Step: 8
Training loss: 2.4462453994356848
Validation loss: 2.5135737955501165

Epoch: 5| Step: 9
Training loss: 2.881521044247705
Validation loss: 2.505949752656516

Epoch: 5| Step: 10
Training loss: 2.2071425148253367
Validation loss: 2.5003315149483805

Epoch: 5| Step: 11
Training loss: 2.8584406425714866
Validation loss: 2.5018832544144076

Epoch: 85| Step: 0
Training loss: 2.437498239369857
Validation loss: 2.503379191026427

Epoch: 5| Step: 1
Training loss: 3.1905710722039506
Validation loss: 2.495346192204324

Epoch: 5| Step: 2
Training loss: 2.2659237368713754
Validation loss: 2.5041193482111734

Epoch: 5| Step: 3
Training loss: 2.4034095942883895
Validation loss: 2.515170563746763

Epoch: 5| Step: 4
Training loss: 2.7237914248813775
Validation loss: 2.5156717710708154

Epoch: 5| Step: 5
Training loss: 3.1363657447496247
Validation loss: 2.5170568020634825

Epoch: 5| Step: 6
Training loss: 2.609807144289267
Validation loss: 2.5201144704099705

Epoch: 5| Step: 7
Training loss: 2.081831530641386
Validation loss: 2.518219673956751

Epoch: 5| Step: 8
Training loss: 2.4039946055423176
Validation loss: 2.5113465986127648

Epoch: 5| Step: 9
Training loss: 2.8335889439191044
Validation loss: 2.514149673527976

Epoch: 5| Step: 10
Training loss: 2.6561813345617002
Validation loss: 2.511577604212887

Epoch: 5| Step: 11
Training loss: 2.8770605249810606
Validation loss: 2.5060148282592953

Epoch: 86| Step: 0
Training loss: 2.556205088773509
Validation loss: 2.502106053653909

Epoch: 5| Step: 1
Training loss: 2.362634895146966
Validation loss: 2.5038179410774375

Epoch: 5| Step: 2
Training loss: 2.555138131157142
Validation loss: 2.497548964137869

Epoch: 5| Step: 3
Training loss: 3.0969522260627684
Validation loss: 2.4906798917879085

Epoch: 5| Step: 4
Training loss: 2.7768561126938986
Validation loss: 2.491846461863409

Epoch: 5| Step: 5
Training loss: 2.4814382025306445
Validation loss: 2.493359691666269

Epoch: 5| Step: 6
Training loss: 2.604458133916858
Validation loss: 2.4968661693660334

Epoch: 5| Step: 7
Training loss: 2.5806891434713157
Validation loss: 2.4895616487864825

Epoch: 5| Step: 8
Training loss: 2.867755220424585
Validation loss: 2.4890193333607398

Epoch: 5| Step: 9
Training loss: 2.2350570598034833
Validation loss: 2.4888206108968043

Epoch: 5| Step: 10
Training loss: 2.727767372068338
Validation loss: 2.484131999097558

Epoch: 5| Step: 11
Training loss: 1.9115137686386965
Validation loss: 2.480502431245348

Epoch: 87| Step: 0
Training loss: 2.7465829427081823
Validation loss: 2.486566328897757

Epoch: 5| Step: 1
Training loss: 2.149002500281426
Validation loss: 2.4874369587729412

Epoch: 5| Step: 2
Training loss: 2.957069951034723
Validation loss: 2.491169940008506

Epoch: 5| Step: 3
Training loss: 2.751353797601096
Validation loss: 2.4951482861715233

Epoch: 5| Step: 4
Training loss: 2.311114643727969
Validation loss: 2.488948191675875

Epoch: 5| Step: 5
Training loss: 2.578390766663904
Validation loss: 2.4885897801952117

Epoch: 5| Step: 6
Training loss: 2.561309258672367
Validation loss: 2.4878679308517824

Epoch: 5| Step: 7
Training loss: 2.4895256918987156
Validation loss: 2.4865865161096012

Epoch: 5| Step: 8
Training loss: 2.3497843054945773
Validation loss: 2.4861551340977965

Epoch: 5| Step: 9
Training loss: 2.8972818790518344
Validation loss: 2.48118566130172

Epoch: 5| Step: 10
Training loss: 2.7258761616017466
Validation loss: 2.485608492438265

Epoch: 5| Step: 11
Training loss: 2.7267520696545247
Validation loss: 2.492262995581913

Epoch: 88| Step: 0
Training loss: 2.4911511218502485
Validation loss: 2.484672260694277

Epoch: 5| Step: 1
Training loss: 2.688415903556517
Validation loss: 2.4825835656772526

Epoch: 5| Step: 2
Training loss: 2.516861037354617
Validation loss: 2.4850828134142997

Epoch: 5| Step: 3
Training loss: 2.29214062269002
Validation loss: 2.4871003499223554

Epoch: 5| Step: 4
Training loss: 2.6271310058269544
Validation loss: 2.4797636656991138

Epoch: 5| Step: 5
Training loss: 2.9791261541284073
Validation loss: 2.477342143920904

Epoch: 5| Step: 6
Training loss: 2.0599802982906663
Validation loss: 2.4759913968343517

Epoch: 5| Step: 7
Training loss: 2.477110504266594
Validation loss: 2.475958187850269

Epoch: 5| Step: 8
Training loss: 2.8857853145571775
Validation loss: 2.472047896674241

Epoch: 5| Step: 9
Training loss: 2.696392429004923
Validation loss: 2.4758174188599518

Epoch: 5| Step: 10
Training loss: 2.8630467588155213
Validation loss: 2.480702284585116

Epoch: 5| Step: 11
Training loss: 1.9282037381953705
Validation loss: 2.4786369708862597

Epoch: 89| Step: 0
Training loss: 2.7827051192336443
Validation loss: 2.4797386356458326

Epoch: 5| Step: 1
Training loss: 2.478725126894813
Validation loss: 2.47880873513861

Epoch: 5| Step: 2
Training loss: 2.7606692984276044
Validation loss: 2.4853503733733904

Epoch: 5| Step: 3
Training loss: 2.745691565721151
Validation loss: 2.4868499416045315

Epoch: 5| Step: 4
Training loss: 2.4813082017966788
Validation loss: 2.487679720700931

Epoch: 5| Step: 5
Training loss: 2.668324749766198
Validation loss: 2.4832446127816175

Epoch: 5| Step: 6
Training loss: 2.4059850373472744
Validation loss: 2.4834159143422694

Epoch: 5| Step: 7
Training loss: 2.766727378669322
Validation loss: 2.4827099344599906

Epoch: 5| Step: 8
Training loss: 2.4039947047183197
Validation loss: 2.4811563935387366

Epoch: 5| Step: 9
Training loss: 2.549354329363234
Validation loss: 2.4801485310511313

Epoch: 5| Step: 10
Training loss: 2.769063848269786
Validation loss: 2.4864196360953756

Epoch: 5| Step: 11
Training loss: 1.077681021563986
Validation loss: 2.476454687328986

Epoch: 90| Step: 0
Training loss: 2.8988379024520126
Validation loss: 2.4745694954862

Epoch: 5| Step: 1
Training loss: 2.7561516880485657
Validation loss: 2.472460184062772

Epoch: 5| Step: 2
Training loss: 2.911839677998047
Validation loss: 2.46903523274165

Epoch: 5| Step: 3
Training loss: 2.821395454764239
Validation loss: 2.4736925577989886

Epoch: 5| Step: 4
Training loss: 2.5271254479111205
Validation loss: 2.475656831166395

Epoch: 5| Step: 5
Training loss: 2.3790312234658337
Validation loss: 2.475368119488099

Epoch: 5| Step: 6
Training loss: 2.2542675660891467
Validation loss: 2.4770097779907614

Epoch: 5| Step: 7
Training loss: 2.417135806970386
Validation loss: 2.478673951423085

Epoch: 5| Step: 8
Training loss: 2.444434645180375
Validation loss: 2.4785736053154137

Epoch: 5| Step: 9
Training loss: 2.3615542824907476
Validation loss: 2.486413027783834

Epoch: 5| Step: 10
Training loss: 2.7368891901143613
Validation loss: 2.485502674877431

Epoch: 5| Step: 11
Training loss: 2.7589159262866043
Validation loss: 2.485370384599703

Epoch: 91| Step: 0
Training loss: 2.5229584324784584
Validation loss: 2.482949949812731

Epoch: 5| Step: 1
Training loss: 2.4175848586801822
Validation loss: 2.49497748355223

Epoch: 5| Step: 2
Training loss: 2.6385842286863914
Validation loss: 2.499869033244041

Epoch: 5| Step: 3
Training loss: 3.269939292320863
Validation loss: 2.525563630690529

Epoch: 5| Step: 4
Training loss: 2.190957416122339
Validation loss: 2.523686549255217

Epoch: 5| Step: 5
Training loss: 2.8964265906344115
Validation loss: 2.512272316467773

Epoch: 5| Step: 6
Training loss: 2.652524422291348
Validation loss: 2.493368831462997

Epoch: 5| Step: 7
Training loss: 2.5854990607654442
Validation loss: 2.487374216762147

Epoch: 5| Step: 8
Training loss: 2.4913031943422976
Validation loss: 2.478594025959332

Epoch: 5| Step: 9
Training loss: 2.7252188708399063
Validation loss: 2.473848638946934

Epoch: 5| Step: 10
Training loss: 2.344927580961688
Validation loss: 2.4729818618866117

Epoch: 5| Step: 11
Training loss: 1.8094432969454555
Validation loss: 2.4741811524820787

Epoch: 92| Step: 0
Training loss: 2.35457937173325
Validation loss: 2.476447623202886

Epoch: 5| Step: 1
Training loss: 2.5755960718986652
Validation loss: 2.4731317176499457

Epoch: 5| Step: 2
Training loss: 2.4079130296558118
Validation loss: 2.4703102814746654

Epoch: 5| Step: 3
Training loss: 2.8638052883663883
Validation loss: 2.471688899426606

Epoch: 5| Step: 4
Training loss: 2.9714526272418316
Validation loss: 2.4733695899057357

Epoch: 5| Step: 5
Training loss: 2.476475181604645
Validation loss: 2.472604892673448

Epoch: 5| Step: 6
Training loss: 2.44896602999896
Validation loss: 2.4679731442439627

Epoch: 5| Step: 7
Training loss: 2.5680884852985018
Validation loss: 2.4689408864292277

Epoch: 5| Step: 8
Training loss: 3.0191212680515456
Validation loss: 2.468544613467303

Epoch: 5| Step: 9
Training loss: 2.4580546614464143
Validation loss: 2.4643732741334774

Epoch: 5| Step: 10
Training loss: 2.3072720793042616
Validation loss: 2.4701534740422666

Epoch: 5| Step: 11
Training loss: 2.339782306456664
Validation loss: 2.4658718232166144

Epoch: 93| Step: 0
Training loss: 3.0547964559572707
Validation loss: 2.4764387338480534

Epoch: 5| Step: 1
Training loss: 2.816159558966796
Validation loss: 2.4744928980399306

Epoch: 5| Step: 2
Training loss: 2.716131418152713
Validation loss: 2.476510060393968

Epoch: 5| Step: 3
Training loss: 2.2878783616630742
Validation loss: 2.470320652645266

Epoch: 5| Step: 4
Training loss: 2.7484609458664986
Validation loss: 2.4720357987275396

Epoch: 5| Step: 5
Training loss: 2.297486619547172
Validation loss: 2.4796385850190172

Epoch: 5| Step: 6
Training loss: 2.8876323983360352
Validation loss: 2.4710624946115485

Epoch: 5| Step: 7
Training loss: 1.6598743154445488
Validation loss: 2.468840134661279

Epoch: 5| Step: 8
Training loss: 2.4762062755192464
Validation loss: 2.4684555063163613

Epoch: 5| Step: 9
Training loss: 2.6115470571906965
Validation loss: 2.4772046336725237

Epoch: 5| Step: 10
Training loss: 2.5294398676097756
Validation loss: 2.474092123864431

Epoch: 5| Step: 11
Training loss: 2.7651057213823025
Validation loss: 2.4731984440461923

Epoch: 94| Step: 0
Training loss: 2.2652631799583682
Validation loss: 2.4735020481088545

Epoch: 5| Step: 1
Training loss: 2.486118113000577
Validation loss: 2.4788901124034814

Epoch: 5| Step: 2
Training loss: 2.8102728608715686
Validation loss: 2.485387132111577

Epoch: 5| Step: 3
Training loss: 2.9864868361398353
Validation loss: 2.4776461664189378

Epoch: 5| Step: 4
Training loss: 2.1885101165990855
Validation loss: 2.475179019066541

Epoch: 5| Step: 5
Training loss: 2.6262288623160708
Validation loss: 2.4772393698563535

Epoch: 5| Step: 6
Training loss: 2.4370694880669177
Validation loss: 2.4708987313139907

Epoch: 5| Step: 7
Training loss: 2.7911046349631876
Validation loss: 2.4722454659896833

Epoch: 5| Step: 8
Training loss: 2.6791206850305374
Validation loss: 2.474300342142363

Epoch: 5| Step: 9
Training loss: 2.1880197180115792
Validation loss: 2.474971055333592

Epoch: 5| Step: 10
Training loss: 2.9029517377677987
Validation loss: 2.471486281363122

Epoch: 5| Step: 11
Training loss: 2.697892079677018
Validation loss: 2.474961775346473

Epoch: 95| Step: 0
Training loss: 2.513591916250856
Validation loss: 2.4694182380837164

Epoch: 5| Step: 1
Training loss: 2.726224074907271
Validation loss: 2.4704218567383895

Epoch: 5| Step: 2
Training loss: 2.274650643321699
Validation loss: 2.4680187896907677

Epoch: 5| Step: 3
Training loss: 2.5622560338442355
Validation loss: 2.4749115456081645

Epoch: 5| Step: 4
Training loss: 2.4093680058834224
Validation loss: 2.474047024124233

Epoch: 5| Step: 5
Training loss: 2.4215127981738958
Validation loss: 2.4685153849699693

Epoch: 5| Step: 6
Training loss: 2.9877022774502886
Validation loss: 2.4688192993610207

Epoch: 5| Step: 7
Training loss: 2.576408971095298
Validation loss: 2.4688805774971248

Epoch: 5| Step: 8
Training loss: 2.279785522277894
Validation loss: 2.46982121063114

Epoch: 5| Step: 9
Training loss: 2.5654400962150854
Validation loss: 2.460615627492855

Epoch: 5| Step: 10
Training loss: 3.0189498993139767
Validation loss: 2.4637880114779915

Epoch: 5| Step: 11
Training loss: 1.9155800544979658
Validation loss: 2.4640893582735015

Epoch: 96| Step: 0
Training loss: 2.867782489461213
Validation loss: 2.457528622435675

Epoch: 5| Step: 1
Training loss: 2.834194856447588
Validation loss: 2.4584196177927673

Epoch: 5| Step: 2
Training loss: 2.3171572826676288
Validation loss: 2.461988630304184

Epoch: 5| Step: 3
Training loss: 2.8341874536882776
Validation loss: 2.456949633934339

Epoch: 5| Step: 4
Training loss: 2.5891894956854493
Validation loss: 2.4575207399066814

Epoch: 5| Step: 5
Training loss: 3.048399402061637
Validation loss: 2.460703639872451

Epoch: 5| Step: 6
Training loss: 2.756963582990717
Validation loss: 2.464787784350128

Epoch: 5| Step: 7
Training loss: 1.8845706819116257
Validation loss: 2.461426669151339

Epoch: 5| Step: 8
Training loss: 2.374497310242502
Validation loss: 2.467694620298579

Epoch: 5| Step: 9
Training loss: 2.363027007243652
Validation loss: 2.4631322287939263

Epoch: 5| Step: 10
Training loss: 2.2675873775112696
Validation loss: 2.4598206301750407

Epoch: 5| Step: 11
Training loss: 2.62435314746252
Validation loss: 2.4625406725020933

Epoch: 97| Step: 0
Training loss: 2.8140943458699597
Validation loss: 2.454736904891153

Epoch: 5| Step: 1
Training loss: 2.2519592655624217
Validation loss: 2.4621410272819046

Epoch: 5| Step: 2
Training loss: 2.5747025401327464
Validation loss: 2.463352528113591

Epoch: 5| Step: 3
Training loss: 2.4586040757225773
Validation loss: 2.4840343049801112

Epoch: 5| Step: 4
Training loss: 2.257137422054983
Validation loss: 2.4888839470853545

Epoch: 5| Step: 5
Training loss: 2.342639202106013
Validation loss: 2.487126148667193

Epoch: 5| Step: 6
Training loss: 2.4372543798085076
Validation loss: 2.4702215999549333

Epoch: 5| Step: 7
Training loss: 2.7064484908894704
Validation loss: 2.4644915071623448

Epoch: 5| Step: 8
Training loss: 2.8171000690825405
Validation loss: 2.467041390110839

Epoch: 5| Step: 9
Training loss: 2.9562372832397763
Validation loss: 2.46184621513398

Epoch: 5| Step: 10
Training loss: 2.863222962081511
Validation loss: 2.4671884499294974

Epoch: 5| Step: 11
Training loss: 1.8119194153220086
Validation loss: 2.4699557011926623

Epoch: 98| Step: 0
Training loss: 2.8317375925337345
Validation loss: 2.4665541963668276

Epoch: 5| Step: 1
Training loss: 2.5173503574419347
Validation loss: 2.4803729421780756

Epoch: 5| Step: 2
Training loss: 2.1221581698477525
Validation loss: 2.4955846698182618

Epoch: 5| Step: 3
Training loss: 3.0642268703322557
Validation loss: 2.4781687087653657

Epoch: 5| Step: 4
Training loss: 2.385989213444261
Validation loss: 2.479748967383785

Epoch: 5| Step: 5
Training loss: 2.2552667012638095
Validation loss: 2.479344695508414

Epoch: 5| Step: 6
Training loss: 2.729811167631434
Validation loss: 2.466512660156903

Epoch: 5| Step: 7
Training loss: 2.7547838043844615
Validation loss: 2.467071558181124

Epoch: 5| Step: 8
Training loss: 2.4450150086423954
Validation loss: 2.469683116703165

Epoch: 5| Step: 9
Training loss: 2.577112172324417
Validation loss: 2.468413040246643

Epoch: 5| Step: 10
Training loss: 2.7684987111294435
Validation loss: 2.466824819084876

Epoch: 5| Step: 11
Training loss: 2.735149513635128
Validation loss: 2.460213624364544

Epoch: 99| Step: 0
Training loss: 2.327759784496426
Validation loss: 2.459026999857833

Epoch: 5| Step: 1
Training loss: 2.3527474190370805
Validation loss: 2.460670622225934

Epoch: 5| Step: 2
Training loss: 2.82053972422611
Validation loss: 2.4603681642455184

Epoch: 5| Step: 3
Training loss: 2.8941022423726914
Validation loss: 2.4633003033289183

Epoch: 5| Step: 4
Training loss: 2.8723965342507265
Validation loss: 2.4592784153328617

Epoch: 5| Step: 5
Training loss: 2.498124754454501
Validation loss: 2.462678658673679

Epoch: 5| Step: 6
Training loss: 2.185271954055969
Validation loss: 2.461728077003907

Epoch: 5| Step: 7
Training loss: 2.701469399484532
Validation loss: 2.463618462127058

Epoch: 5| Step: 8
Training loss: 2.371917330197042
Validation loss: 2.4664899242934344

Epoch: 5| Step: 9
Training loss: 2.5971710440296
Validation loss: 2.4634599100839143

Epoch: 5| Step: 10
Training loss: 2.6477477489249717
Validation loss: 2.4619071584077896

Epoch: 5| Step: 11
Training loss: 2.052389041442997
Validation loss: 2.460438741369058

Epoch: 100| Step: 0
Training loss: 2.203693614115029
Validation loss: 2.463923710293035

Epoch: 5| Step: 1
Training loss: 2.1424204404073777
Validation loss: 2.4627977555886424

Epoch: 5| Step: 2
Training loss: 2.7655389632296115
Validation loss: 2.4647762492872443

Epoch: 5| Step: 3
Training loss: 2.4451674153245304
Validation loss: 2.458977567720371

Epoch: 5| Step: 4
Training loss: 2.862078279972174
Validation loss: 2.453950799656848

Epoch: 5| Step: 5
Training loss: 2.8958867203070264
Validation loss: 2.46137970680279

Epoch: 5| Step: 6
Training loss: 3.065619456434925
Validation loss: 2.4549781659599788

Epoch: 5| Step: 7
Training loss: 2.099126034482584
Validation loss: 2.455310348254742

Epoch: 5| Step: 8
Training loss: 2.2603332808857926
Validation loss: 2.466064400756783

Epoch: 5| Step: 9
Training loss: 2.1311231916631272
Validation loss: 2.463015931037794

Epoch: 5| Step: 10
Training loss: 3.0036633694681836
Validation loss: 2.459347218150973

Epoch: 5| Step: 11
Training loss: 2.655617223428826
Validation loss: 2.4641997377134954

Epoch: 101| Step: 0
Training loss: 2.6813030211001316
Validation loss: 2.466309649066275

Epoch: 5| Step: 1
Training loss: 2.451213602265597
Validation loss: 2.463004585309858

Epoch: 5| Step: 2
Training loss: 2.895398132408358
Validation loss: 2.4671569988224777

Epoch: 5| Step: 3
Training loss: 2.718459015105632
Validation loss: 2.4651818423939393

Epoch: 5| Step: 4
Training loss: 2.456279888408028
Validation loss: 2.465669283128761

Epoch: 5| Step: 5
Training loss: 2.279259427244325
Validation loss: 2.4663462747059115

Epoch: 5| Step: 6
Training loss: 2.3132127230567914
Validation loss: 2.464291534390629

Epoch: 5| Step: 7
Training loss: 2.5654141672838966
Validation loss: 2.467262568427985

Epoch: 5| Step: 8
Training loss: 2.77684392066407
Validation loss: 2.4688986922606055

Epoch: 5| Step: 9
Training loss: 2.608170374044365
Validation loss: 2.4738223684326157

Epoch: 5| Step: 10
Training loss: 2.3958807125106047
Validation loss: 2.4623262110089397

Epoch: 5| Step: 11
Training loss: 3.00281503846181
Validation loss: 2.4624607639769254

Epoch: 102| Step: 0
Training loss: 2.6889227716116904
Validation loss: 2.4614645438991034

Epoch: 5| Step: 1
Training loss: 2.5640296092414085
Validation loss: 2.460252070971049

Epoch: 5| Step: 2
Training loss: 2.723864950666647
Validation loss: 2.4582016971603586

Epoch: 5| Step: 3
Training loss: 2.4514444024917843
Validation loss: 2.461916495673412

Epoch: 5| Step: 4
Training loss: 2.707904273061178
Validation loss: 2.462764852742403

Epoch: 5| Step: 5
Training loss: 3.217977486654352
Validation loss: 2.453772275211112

Epoch: 5| Step: 6
Training loss: 2.509568974846908
Validation loss: 2.4557784484470613

Epoch: 5| Step: 7
Training loss: 2.1991221540522945
Validation loss: 2.4631558062996723

Epoch: 5| Step: 8
Training loss: 2.499562797464148
Validation loss: 2.4571982176895175

Epoch: 5| Step: 9
Training loss: 2.3066179886497773
Validation loss: 2.4596039605270663

Epoch: 5| Step: 10
Training loss: 2.272123186284389
Validation loss: 2.4605347722643036

Epoch: 5| Step: 11
Training loss: 2.087012424748225
Validation loss: 2.458239320530656

Epoch: 103| Step: 0
Training loss: 2.8584908541175587
Validation loss: 2.450770866348536

Epoch: 5| Step: 1
Training loss: 2.456791949482991
Validation loss: 2.455457259032027

Epoch: 5| Step: 2
Training loss: 2.7015565059103364
Validation loss: 2.4556217689263122

Epoch: 5| Step: 3
Training loss: 2.356284839969547
Validation loss: 2.4623083383746893

Epoch: 5| Step: 4
Training loss: 2.5170115565086593
Validation loss: 2.465874393486974

Epoch: 5| Step: 5
Training loss: 2.566686977785011
Validation loss: 2.4656970022862468

Epoch: 5| Step: 6
Training loss: 2.8350579865730166
Validation loss: 2.4629256598662757

Epoch: 5| Step: 7
Training loss: 2.690820549959841
Validation loss: 2.4537234052297654

Epoch: 5| Step: 8
Training loss: 2.372846329617553
Validation loss: 2.45139837750972

Epoch: 5| Step: 9
Training loss: 2.570461094975703
Validation loss: 2.456976282973577

Epoch: 5| Step: 10
Training loss: 2.1247914155801038
Validation loss: 2.457530556683157

Epoch: 5| Step: 11
Training loss: 2.3953131290641503
Validation loss: 2.467186452790083

Epoch: 104| Step: 0
Training loss: 2.736349733364003
Validation loss: 2.452580928199595

Epoch: 5| Step: 1
Training loss: 2.347135717790574
Validation loss: 2.4561760348507

Epoch: 5| Step: 2
Training loss: 2.76952918392906
Validation loss: 2.4536362821851534

Epoch: 5| Step: 3
Training loss: 2.911471690429911
Validation loss: 2.4528171483824983

Epoch: 5| Step: 4
Training loss: 2.544244449183236
Validation loss: 2.453780635358128

Epoch: 5| Step: 5
Training loss: 2.436178778541432
Validation loss: 2.462405841498885

Epoch: 5| Step: 6
Training loss: 2.395192986758507
Validation loss: 2.459153423805822

Epoch: 5| Step: 7
Training loss: 2.6450600482780313
Validation loss: 2.456969510562688

Epoch: 5| Step: 8
Training loss: 2.281770516152974
Validation loss: 2.4610737394536124

Epoch: 5| Step: 9
Training loss: 2.1566653749952422
Validation loss: 2.4606447318179536

Epoch: 5| Step: 10
Training loss: 2.861921166945048
Validation loss: 2.4621371660293643

Epoch: 5| Step: 11
Training loss: 2.5908042081870857
Validation loss: 2.4624599974750834

Epoch: 105| Step: 0
Training loss: 2.30378690030811
Validation loss: 2.4540797762770903

Epoch: 5| Step: 1
Training loss: 2.5443220390443306
Validation loss: 2.460597669762789

Epoch: 5| Step: 2
Training loss: 2.2625992136746604
Validation loss: 2.4560616404015962

Epoch: 5| Step: 3
Training loss: 2.802738288052582
Validation loss: 2.4692330833391916

Epoch: 5| Step: 4
Training loss: 2.312094163079288
Validation loss: 2.478914473766748

Epoch: 5| Step: 5
Training loss: 3.3194923734374764
Validation loss: 2.4838610743333738

Epoch: 5| Step: 6
Training loss: 2.7094309025577425
Validation loss: 2.472767542644055

Epoch: 5| Step: 7
Training loss: 2.1161730824083063
Validation loss: 2.459712317930424

Epoch: 5| Step: 8
Training loss: 1.9437540084944727
Validation loss: 2.45693617789938

Epoch: 5| Step: 9
Training loss: 2.8770122949299717
Validation loss: 2.4550523981664876

Epoch: 5| Step: 10
Training loss: 2.7205543230911853
Validation loss: 2.454703011758164

Epoch: 5| Step: 11
Training loss: 3.030454708262535
Validation loss: 2.4646289084942774

Epoch: 106| Step: 0
Training loss: 2.541331618274752
Validation loss: 2.464216016342546

Epoch: 5| Step: 1
Training loss: 2.340941717622456
Validation loss: 2.4773034512632495

Epoch: 5| Step: 2
Training loss: 2.677639103258197
Validation loss: 2.471418278743202

Epoch: 5| Step: 3
Training loss: 2.2703641587620065
Validation loss: 2.469282821103709

Epoch: 5| Step: 4
Training loss: 2.410005784245431
Validation loss: 2.473407998695281

Epoch: 5| Step: 5
Training loss: 2.8312264818305257
Validation loss: 2.474882247801672

Epoch: 5| Step: 6
Training loss: 2.9962503524098927
Validation loss: 2.4736490772433966

Epoch: 5| Step: 7
Training loss: 2.902185037824848
Validation loss: 2.470702131871764

Epoch: 5| Step: 8
Training loss: 1.8975352588338
Validation loss: 2.46986212206908

Epoch: 5| Step: 9
Training loss: 3.057359390331858
Validation loss: 2.46667740331925

Epoch: 5| Step: 10
Training loss: 2.299763323174985
Validation loss: 2.466042514773057

Epoch: 5| Step: 11
Training loss: 2.31675987696495
Validation loss: 2.4695011155179447

Epoch: 107| Step: 0
Training loss: 2.6923256952082095
Validation loss: 2.4644202842424856

Epoch: 5| Step: 1
Training loss: 1.9429449665130918
Validation loss: 2.4660524325758706

Epoch: 5| Step: 2
Training loss: 2.841819359339746
Validation loss: 2.458569306562302

Epoch: 5| Step: 3
Training loss: 2.6924628632232657
Validation loss: 2.463194253363033

Epoch: 5| Step: 4
Training loss: 2.4511752793489805
Validation loss: 2.4679418319453013

Epoch: 5| Step: 5
Training loss: 2.190290578437748
Validation loss: 2.4593604792255506

Epoch: 5| Step: 6
Training loss: 2.4408380198105397
Validation loss: 2.463772738039071

Epoch: 5| Step: 7
Training loss: 2.2640625926689726
Validation loss: 2.4661961999816735

Epoch: 5| Step: 8
Training loss: 3.025946628218378
Validation loss: 2.4653263052938534

Epoch: 5| Step: 9
Training loss: 2.92360729741814
Validation loss: 2.4663350006887046

Epoch: 5| Step: 10
Training loss: 2.539637573847939
Validation loss: 2.462485737686681

Epoch: 5| Step: 11
Training loss: 3.290891596327551
Validation loss: 2.4596288481355884

Epoch: 108| Step: 0
Training loss: 2.8505073162761825
Validation loss: 2.471214581250057

Epoch: 5| Step: 1
Training loss: 2.204273222262841
Validation loss: 2.464233548604076

Epoch: 5| Step: 2
Training loss: 2.9969422333845777
Validation loss: 2.4647647867184506

Epoch: 5| Step: 3
Training loss: 2.086321848917201
Validation loss: 2.468727196213875

Epoch: 5| Step: 4
Training loss: 2.163295789058785
Validation loss: 2.463005734808313

Epoch: 5| Step: 5
Training loss: 2.651361792121582
Validation loss: 2.4714164779627343

Epoch: 5| Step: 6
Training loss: 2.7471953742251056
Validation loss: 2.4645673472922467

Epoch: 5| Step: 7
Training loss: 2.8964973804390253
Validation loss: 2.4626084684632863

Epoch: 5| Step: 8
Training loss: 2.8570777545051316
Validation loss: 2.4655978162239593

Epoch: 5| Step: 9
Training loss: 2.40499379448675
Validation loss: 2.458920369667363

Epoch: 5| Step: 10
Training loss: 2.3108357807544944
Validation loss: 2.4585062117054295

Epoch: 5| Step: 11
Training loss: 1.0566318250702185
Validation loss: 2.4550732896059704

Epoch: 109| Step: 0
Training loss: 2.461853631869396
Validation loss: 2.454985134049964

Epoch: 5| Step: 1
Training loss: 3.2188836227383866
Validation loss: 2.450047658437289

Epoch: 5| Step: 2
Training loss: 2.5466103445519024
Validation loss: 2.458389559738573

Epoch: 5| Step: 3
Training loss: 2.1250374734603716
Validation loss: 2.456790235026447

Epoch: 5| Step: 4
Training loss: 2.6259175468103497
Validation loss: 2.45460828454602

Epoch: 5| Step: 5
Training loss: 2.11688748306725
Validation loss: 2.454439981091084

Epoch: 5| Step: 6
Training loss: 2.32338051947352
Validation loss: 2.4592763592570708

Epoch: 5| Step: 7
Training loss: 2.436605925300844
Validation loss: 2.4514310621236666

Epoch: 5| Step: 8
Training loss: 2.5858038092683784
Validation loss: 2.4522863238139827

Epoch: 5| Step: 9
Training loss: 2.7880066654941125
Validation loss: 2.444490820783137

Epoch: 5| Step: 10
Training loss: 2.5475149462320403
Validation loss: 2.4507930285537896

Epoch: 5| Step: 11
Training loss: 2.8025103010929207
Validation loss: 2.4502938611719935

Epoch: 110| Step: 0
Training loss: 2.385851313764972
Validation loss: 2.449649908717622

Epoch: 5| Step: 1
Training loss: 2.7449816952287884
Validation loss: 2.454803896580024

Epoch: 5| Step: 2
Training loss: 2.180474033438153
Validation loss: 2.4574809913637425

Epoch: 5| Step: 3
Training loss: 2.6834584386658795
Validation loss: 2.459208249260087

Epoch: 5| Step: 4
Training loss: 2.756765453209171
Validation loss: 2.45946295426241

Epoch: 5| Step: 5
Training loss: 2.8442040384875393
Validation loss: 2.4570025071897383

Epoch: 5| Step: 6
Training loss: 2.3476385791894234
Validation loss: 2.456456673641279

Epoch: 5| Step: 7
Training loss: 2.694694381823639
Validation loss: 2.46196446057608

Epoch: 5| Step: 8
Training loss: 2.2745368108716133
Validation loss: 2.4594901334641315

Epoch: 5| Step: 9
Training loss: 2.250452949708418
Validation loss: 2.4518436073784815

Epoch: 5| Step: 10
Training loss: 2.9025594597770956
Validation loss: 2.4535817654485075

Epoch: 5| Step: 11
Training loss: 1.5770628725821771
Validation loss: 2.456437617862104

Epoch: 111| Step: 0
Training loss: 2.7297985908074813
Validation loss: 2.4573986261970244

Epoch: 5| Step: 1
Training loss: 2.409217490899697
Validation loss: 2.4664752072915097

Epoch: 5| Step: 2
Training loss: 2.4534328777739542
Validation loss: 2.463248371840881

Epoch: 5| Step: 3
Training loss: 2.532621035907946
Validation loss: 2.467771964153881

Epoch: 5| Step: 4
Training loss: 2.5817133120547813
Validation loss: 2.4715769070793714

Epoch: 5| Step: 5
Training loss: 2.843405566473784
Validation loss: 2.4675621221008446

Epoch: 5| Step: 6
Training loss: 2.1965730245231976
Validation loss: 2.4566358400745996

Epoch: 5| Step: 7
Training loss: 2.4892023558517065
Validation loss: 2.4560193685592133

Epoch: 5| Step: 8
Training loss: 2.6998050124791817
Validation loss: 2.4598773630646718

Epoch: 5| Step: 9
Training loss: 2.4645889561627423
Validation loss: 2.463439255105309

Epoch: 5| Step: 10
Training loss: 2.831832226304124
Validation loss: 2.460263764531714

Epoch: 5| Step: 11
Training loss: 2.125554853816874
Validation loss: 2.470372523930359

Epoch: 112| Step: 0
Training loss: 2.73133786057977
Validation loss: 2.4640084194811265

Epoch: 5| Step: 1
Training loss: 2.2279366519284394
Validation loss: 2.4624709624629393

Epoch: 5| Step: 2
Training loss: 2.778685083561939
Validation loss: 2.4566637824708875

Epoch: 5| Step: 3
Training loss: 2.722850119476341
Validation loss: 2.4599721761166355

Epoch: 5| Step: 4
Training loss: 2.341855720499708
Validation loss: 2.457735172064933

Epoch: 5| Step: 5
Training loss: 2.4754229791039903
Validation loss: 2.4477504768502287

Epoch: 5| Step: 6
Training loss: 2.6873301075309004
Validation loss: 2.4623742970302738

Epoch: 5| Step: 7
Training loss: 2.2971099940906554
Validation loss: 2.452983516006612

Epoch: 5| Step: 8
Training loss: 2.5969130750818845
Validation loss: 2.4534421177212735

Epoch: 5| Step: 9
Training loss: 2.7526174573237605
Validation loss: 2.456536183012662

Epoch: 5| Step: 10
Training loss: 2.5236934378540954
Validation loss: 2.4525350723908557

Epoch: 5| Step: 11
Training loss: 2.164879599872889
Validation loss: 2.460634679176957

Epoch: 113| Step: 0
Training loss: 2.610270089684413
Validation loss: 2.4584842139888003

Epoch: 5| Step: 1
Training loss: 2.338221527366742
Validation loss: 2.4579454509314766

Epoch: 5| Step: 2
Training loss: 2.310978363067874
Validation loss: 2.4577547230268197

Epoch: 5| Step: 3
Training loss: 2.5918645785241563
Validation loss: 2.4573745204729023

Epoch: 5| Step: 4
Training loss: 2.818185950301377
Validation loss: 2.4549958248905748

Epoch: 5| Step: 5
Training loss: 2.9963934200179483
Validation loss: 2.452892733874516

Epoch: 5| Step: 6
Training loss: 2.8793976363096916
Validation loss: 2.4541337638186453

Epoch: 5| Step: 7
Training loss: 2.4641987822779887
Validation loss: 2.4577205724029607

Epoch: 5| Step: 8
Training loss: 1.9560755301482546
Validation loss: 2.4606807393142995

Epoch: 5| Step: 9
Training loss: 2.3383895609324763
Validation loss: 2.462418166283791

Epoch: 5| Step: 10
Training loss: 2.63561437013512
Validation loss: 2.458366560846118

Epoch: 5| Step: 11
Training loss: 2.8955806008979246
Validation loss: 2.455364828634412

Epoch: 114| Step: 0
Training loss: 2.328418687563456
Validation loss: 2.4553179263413885

Epoch: 5| Step: 1
Training loss: 2.565102349037004
Validation loss: 2.454183483767788

Epoch: 5| Step: 2
Training loss: 2.368966419328172
Validation loss: 2.454451759005223

Epoch: 5| Step: 3
Training loss: 2.0329947387732314
Validation loss: 2.458879051952544

Epoch: 5| Step: 4
Training loss: 2.7342307788417077
Validation loss: 2.459831816931816

Epoch: 5| Step: 5
Training loss: 2.8436837660233985
Validation loss: 2.453514659411524

Epoch: 5| Step: 6
Training loss: 2.831082815324653
Validation loss: 2.455883609180302

Epoch: 5| Step: 7
Training loss: 2.6579568203647215
Validation loss: 2.456453379735858

Epoch: 5| Step: 8
Training loss: 3.0364333992005754
Validation loss: 2.4533039643272123

Epoch: 5| Step: 9
Training loss: 2.0471990197826693
Validation loss: 2.4460671736033843

Epoch: 5| Step: 10
Training loss: 2.0779018425691564
Validation loss: 2.4587631711880182

Epoch: 5| Step: 11
Training loss: 3.243510442866025
Validation loss: 2.4521127300936514

Epoch: 115| Step: 0
Training loss: 2.1328225607163125
Validation loss: 2.4489644358137177

Epoch: 5| Step: 1
Training loss: 2.172579095451964
Validation loss: 2.446320962602715

Epoch: 5| Step: 2
Training loss: 2.6461530977795724
Validation loss: 2.449933845574062

Epoch: 5| Step: 3
Training loss: 2.819067175501541
Validation loss: 2.452698652308348

Epoch: 5| Step: 4
Training loss: 2.465671889870945
Validation loss: 2.4468257353440594

Epoch: 5| Step: 5
Training loss: 2.830793439082226
Validation loss: 2.4528766149969705

Epoch: 5| Step: 6
Training loss: 2.297960712452265
Validation loss: 2.4464294474530237

Epoch: 5| Step: 7
Training loss: 2.3596366080453124
Validation loss: 2.441546484416477

Epoch: 5| Step: 8
Training loss: 2.595455757794327
Validation loss: 2.4451761339735985

Epoch: 5| Step: 9
Training loss: 2.8652132393669163
Validation loss: 2.4424923324789987

Epoch: 5| Step: 10
Training loss: 2.3511399811512748
Validation loss: 2.444367572510113

Epoch: 5| Step: 11
Training loss: 3.4680832230593066
Validation loss: 2.4489483316450653

Epoch: 116| Step: 0
Training loss: 2.7252750362922087
Validation loss: 2.447157390339267

Epoch: 5| Step: 1
Training loss: 2.549040920684862
Validation loss: 2.454501480607455

Epoch: 5| Step: 2
Training loss: 2.6717216938497184
Validation loss: 2.4566656992024862

Epoch: 5| Step: 3
Training loss: 2.7513630696710862
Validation loss: 2.4578771706763125

Epoch: 5| Step: 4
Training loss: 2.8656246072983134
Validation loss: 2.454848188437897

Epoch: 5| Step: 5
Training loss: 2.2661556280215662
Validation loss: 2.457987499696209

Epoch: 5| Step: 6
Training loss: 2.4870590969376174
Validation loss: 2.460810114071395

Epoch: 5| Step: 7
Training loss: 2.4172321732183573
Validation loss: 2.4523193956072986

Epoch: 5| Step: 8
Training loss: 2.2319421928672405
Validation loss: 2.4534340601004434

Epoch: 5| Step: 9
Training loss: 2.457455451383041
Validation loss: 2.447117396333623

Epoch: 5| Step: 10
Training loss: 2.3903877290150306
Validation loss: 2.4504675472572535

Epoch: 5| Step: 11
Training loss: 2.8533993174448824
Validation loss: 2.4517093590957333

Epoch: 117| Step: 0
Training loss: 2.034601234712998
Validation loss: 2.4519202155148787

Epoch: 5| Step: 1
Training loss: 2.864848250652655
Validation loss: 2.4451774096726724

Epoch: 5| Step: 2
Training loss: 2.5000772464262266
Validation loss: 2.4565785470588914

Epoch: 5| Step: 3
Training loss: 2.034834647806744
Validation loss: 2.4486095719366587

Epoch: 5| Step: 4
Training loss: 2.250557512512503
Validation loss: 2.4510239190462744

Epoch: 5| Step: 5
Training loss: 2.791054321618769
Validation loss: 2.4474621587548366

Epoch: 5| Step: 6
Training loss: 2.749205474517935
Validation loss: 2.4411666874652354

Epoch: 5| Step: 7
Training loss: 2.660334061605034
Validation loss: 2.450043632157726

Epoch: 5| Step: 8
Training loss: 2.597513249412134
Validation loss: 2.4487273814928923

Epoch: 5| Step: 9
Training loss: 2.4574451674032236
Validation loss: 2.4475316712119772

Epoch: 5| Step: 10
Training loss: 2.875350599058293
Validation loss: 2.446905513003164

Epoch: 5| Step: 11
Training loss: 2.337008431709563
Validation loss: 2.4532862689226453

Epoch: 118| Step: 0
Training loss: 2.2304938694062217
Validation loss: 2.452968248190321

Epoch: 5| Step: 1
Training loss: 2.7547716877623047
Validation loss: 2.4591432196407346

Epoch: 5| Step: 2
Training loss: 2.3927206191623682
Validation loss: 2.4561161991655247

Epoch: 5| Step: 3
Training loss: 2.7974109668898812
Validation loss: 2.4607050165232764

Epoch: 5| Step: 4
Training loss: 2.4358356368891076
Validation loss: 2.4614605544558232

Epoch: 5| Step: 5
Training loss: 2.246716434576522
Validation loss: 2.4614876793139713

Epoch: 5| Step: 6
Training loss: 2.259269165261699
Validation loss: 2.452906611009241

Epoch: 5| Step: 7
Training loss: 2.6184427012769644
Validation loss: 2.4473505880672755

Epoch: 5| Step: 8
Training loss: 3.3277761070517577
Validation loss: 2.4406366625583296

Epoch: 5| Step: 9
Training loss: 1.933696705552906
Validation loss: 2.442573048818009

Epoch: 5| Step: 10
Training loss: 2.8815212097287604
Validation loss: 2.4412135096445207

Epoch: 5| Step: 11
Training loss: 0.9523239834668854
Validation loss: 2.443470656322909

Epoch: 119| Step: 0
Training loss: 2.665485259459522
Validation loss: 2.451554170055027

Epoch: 5| Step: 1
Training loss: 2.7284350591519084
Validation loss: 2.4504958741676695

Epoch: 5| Step: 2
Training loss: 2.4294946722807156
Validation loss: 2.448600159585309

Epoch: 5| Step: 3
Training loss: 2.4713203473418397
Validation loss: 2.4458382582628446

Epoch: 5| Step: 4
Training loss: 1.870925163048688
Validation loss: 2.4482918485624325

Epoch: 5| Step: 5
Training loss: 2.5266354731376333
Validation loss: 2.4605345300216483

Epoch: 5| Step: 6
Training loss: 2.390569050452057
Validation loss: 2.463153898651509

Epoch: 5| Step: 7
Training loss: 2.7850980391444904
Validation loss: 2.4517723776534273

Epoch: 5| Step: 8
Training loss: 2.816579614983227
Validation loss: 2.4573424546018487

Epoch: 5| Step: 9
Training loss: 2.3818284300888757
Validation loss: 2.4452804425203003

Epoch: 5| Step: 10
Training loss: 2.7519761701150953
Validation loss: 2.444363654733025

Epoch: 5| Step: 11
Training loss: 2.0271918032232827
Validation loss: 2.442628718192349

Epoch: 120| Step: 0
Training loss: 2.493859761434991
Validation loss: 2.441182663915543

Epoch: 5| Step: 1
Training loss: 2.3557151045637137
Validation loss: 2.4464899890487457

Epoch: 5| Step: 2
Training loss: 2.619303926501697
Validation loss: 2.441904750538921

Epoch: 5| Step: 3
Training loss: 2.1240234375
Validation loss: 2.451416003467813

Epoch: 5| Step: 4
Training loss: 2.8207002410218864
Validation loss: 2.4450902015140854

Epoch: 5| Step: 5
Training loss: 2.2818948539762456
Validation loss: 2.446716263170491

Epoch: 5| Step: 6
Training loss: 3.1080541824566743
Validation loss: 2.442762603373801

Epoch: 5| Step: 7
Training loss: 2.1982877396990665
Validation loss: 2.4475059989969976

Epoch: 5| Step: 8
Training loss: 2.279890308418458
Validation loss: 2.4495409926416842

Epoch: 5| Step: 9
Training loss: 2.6381369160715393
Validation loss: 2.4491359933989876

Epoch: 5| Step: 10
Training loss: 2.7100443399328094
Validation loss: 2.4448277213308534

Epoch: 5| Step: 11
Training loss: 2.594839212070571
Validation loss: 2.450211294949929

Epoch: 121| Step: 0
Training loss: 2.7102971708642083
Validation loss: 2.4523875470487217

Epoch: 5| Step: 1
Training loss: 2.7480137327536833
Validation loss: 2.4516816762853293

Epoch: 5| Step: 2
Training loss: 2.1505706739451305
Validation loss: 2.455323038401044

Epoch: 5| Step: 3
Training loss: 2.050887971553491
Validation loss: 2.4619642749645623

Epoch: 5| Step: 4
Training loss: 2.949006776581355
Validation loss: 2.464905247875789

Epoch: 5| Step: 5
Training loss: 1.9544949419176216
Validation loss: 2.459929700902738

Epoch: 5| Step: 6
Training loss: 2.842920874150332
Validation loss: 2.4601511753611813

Epoch: 5| Step: 7
Training loss: 2.8575488653535395
Validation loss: 2.460863005302819

Epoch: 5| Step: 8
Training loss: 2.3297316364735745
Validation loss: 2.4544656941069314

Epoch: 5| Step: 9
Training loss: 2.700008911541961
Validation loss: 2.452942146889058

Epoch: 5| Step: 10
Training loss: 2.266457661174155
Validation loss: 2.444998309636068

Epoch: 5| Step: 11
Training loss: 2.849644514969251
Validation loss: 2.444212307497897

Epoch: 122| Step: 0
Training loss: 2.615782220962296
Validation loss: 2.4526904707363784

Epoch: 5| Step: 1
Training loss: 2.939909413557553
Validation loss: 2.4734297753263825

Epoch: 5| Step: 2
Training loss: 2.4065566424883165
Validation loss: 2.4969516209259583

Epoch: 5| Step: 3
Training loss: 2.188454338073231
Validation loss: 2.512635949001043

Epoch: 5| Step: 4
Training loss: 2.8358651984800325
Validation loss: 2.4958633969257695

Epoch: 5| Step: 5
Training loss: 2.3080097628144
Validation loss: 2.4750363433943847

Epoch: 5| Step: 6
Training loss: 2.5993909049000403
Validation loss: 2.469896887108303

Epoch: 5| Step: 7
Training loss: 2.4988608626045465
Validation loss: 2.4545041963452423

Epoch: 5| Step: 8
Training loss: 2.5998174639962426
Validation loss: 2.445670238810546

Epoch: 5| Step: 9
Training loss: 2.174924507146372
Validation loss: 2.4451814195886756

Epoch: 5| Step: 10
Training loss: 3.085321987913742
Validation loss: 2.4496813412797462

Epoch: 5| Step: 11
Training loss: 2.050720097674558
Validation loss: 2.4590253192784344

Epoch: 123| Step: 0
Training loss: 2.2803107640480813
Validation loss: 2.4687981741667513

Epoch: 5| Step: 1
Training loss: 2.2106854729851393
Validation loss: 2.474721037431138

Epoch: 5| Step: 2
Training loss: 2.8266003962011834
Validation loss: 2.4727547150382025

Epoch: 5| Step: 3
Training loss: 2.249534346820883
Validation loss: 2.476014402531234

Epoch: 5| Step: 4
Training loss: 2.8293270696713897
Validation loss: 2.4805807213528963

Epoch: 5| Step: 5
Training loss: 2.597295979921833
Validation loss: 2.472113157454958

Epoch: 5| Step: 6
Training loss: 2.4703946972755486
Validation loss: 2.472270467432682

Epoch: 5| Step: 7
Training loss: 2.664541162351499
Validation loss: 2.476654761006819

Epoch: 5| Step: 8
Training loss: 2.3784774619336355
Validation loss: 2.471613796304879

Epoch: 5| Step: 9
Training loss: 3.024214612661856
Validation loss: 2.473073019266004

Epoch: 5| Step: 10
Training loss: 2.771308016928978
Validation loss: 2.4741845050959737

Epoch: 5| Step: 11
Training loss: 2.1466287663275185
Validation loss: 2.4666535775593417

Epoch: 124| Step: 0
Training loss: 2.899579504373877
Validation loss: 2.466811183345291

Epoch: 5| Step: 1
Training loss: 2.5290662034353
Validation loss: 2.4703726686971317

Epoch: 5| Step: 2
Training loss: 2.513161062699542
Validation loss: 2.4587192831850357

Epoch: 5| Step: 3
Training loss: 2.7518886669615763
Validation loss: 2.456225855015445

Epoch: 5| Step: 4
Training loss: 1.738395155700064
Validation loss: 2.452867271669457

Epoch: 5| Step: 5
Training loss: 3.036068733744028
Validation loss: 2.451659726738805

Epoch: 5| Step: 6
Training loss: 2.132842682006567
Validation loss: 2.4558169948261774

Epoch: 5| Step: 7
Training loss: 2.5556677941078987
Validation loss: 2.4514293844416137

Epoch: 5| Step: 8
Training loss: 2.494323102448919
Validation loss: 2.45558580456764

Epoch: 5| Step: 9
Training loss: 2.50641990333055
Validation loss: 2.4613194123879616

Epoch: 5| Step: 10
Training loss: 2.7655910338993848
Validation loss: 2.465561489710324

Epoch: 5| Step: 11
Training loss: 1.7345405018515643
Validation loss: 2.460570700623408

Epoch: 125| Step: 0
Training loss: 2.764110603151154
Validation loss: 2.4546766739413703

Epoch: 5| Step: 1
Training loss: 3.0108424394900792
Validation loss: 2.460936010451093

Epoch: 5| Step: 2
Training loss: 2.2712788509369295
Validation loss: 2.4561629810562726

Epoch: 5| Step: 3
Training loss: 2.7103740535340877
Validation loss: 2.4494922247983424

Epoch: 5| Step: 4
Training loss: 2.331779530292919
Validation loss: 2.4489011218874985

Epoch: 5| Step: 5
Training loss: 2.042257794291883
Validation loss: 2.450529588338446

Epoch: 5| Step: 6
Training loss: 2.3655129732442632
Validation loss: 2.4553418519836163

Epoch: 5| Step: 7
Training loss: 3.013688804989306
Validation loss: 2.4520518431887264

Epoch: 5| Step: 8
Training loss: 2.532172329370069
Validation loss: 2.4490591440311267

Epoch: 5| Step: 9
Training loss: 2.260950567475303
Validation loss: 2.450842502141891

Epoch: 5| Step: 10
Training loss: 2.4501712778128177
Validation loss: 2.447583144713857

Epoch: 5| Step: 11
Training loss: 2.9461257580799014
Validation loss: 2.4477728632358944

Epoch: 126| Step: 0
Training loss: 2.23067172808775
Validation loss: 2.4494422068407458

Epoch: 5| Step: 1
Training loss: 2.4044589027578054
Validation loss: 2.4535088674112826

Epoch: 5| Step: 2
Training loss: 2.3361169242206974
Validation loss: 2.4474215568191333

Epoch: 5| Step: 3
Training loss: 2.3663999550484736
Validation loss: 2.4554798583538915

Epoch: 5| Step: 4
Training loss: 2.3011733337926117
Validation loss: 2.4500736810997

Epoch: 5| Step: 5
Training loss: 2.6402902786018703
Validation loss: 2.4528204694464475

Epoch: 5| Step: 6
Training loss: 2.69734155325675
Validation loss: 2.4409418340835054

Epoch: 5| Step: 7
Training loss: 2.8846257820920136
Validation loss: 2.444213165073281

Epoch: 5| Step: 8
Training loss: 2.012932213283865
Validation loss: 2.446382081477117

Epoch: 5| Step: 9
Training loss: 3.2251467057800194
Validation loss: 2.4498549045364193

Epoch: 5| Step: 10
Training loss: 2.585981570326415
Validation loss: 2.446619507177466

Epoch: 5| Step: 11
Training loss: 2.4252650991043923
Validation loss: 2.454900479724196

Epoch: 127| Step: 0
Training loss: 2.330762559591151
Validation loss: 2.4496414655326153

Epoch: 5| Step: 1
Training loss: 2.7987124411825834
Validation loss: 2.4523941295739764

Epoch: 5| Step: 2
Training loss: 2.841285224290065
Validation loss: 2.4484490884240078

Epoch: 5| Step: 3
Training loss: 1.9024849903853007
Validation loss: 2.448652032551222

Epoch: 5| Step: 4
Training loss: 2.7614643244112966
Validation loss: 2.450908992293014

Epoch: 5| Step: 5
Training loss: 2.451470175293843
Validation loss: 2.4526156283375613

Epoch: 5| Step: 6
Training loss: 2.534745992842277
Validation loss: 2.4556629553586835

Epoch: 5| Step: 7
Training loss: 2.704057477520742
Validation loss: 2.463568868076946

Epoch: 5| Step: 8
Training loss: 2.865026340330248
Validation loss: 2.4598557289586416

Epoch: 5| Step: 9
Training loss: 2.3766105611883495
Validation loss: 2.4626311957883544

Epoch: 5| Step: 10
Training loss: 2.0821891567332984
Validation loss: 2.464477398988087

Epoch: 5| Step: 11
Training loss: 3.026105309942599
Validation loss: 2.458974695325509

Epoch: 128| Step: 0
Training loss: 2.9526098700227337
Validation loss: 2.4521545547820245

Epoch: 5| Step: 1
Training loss: 2.0461056260692474
Validation loss: 2.453158167799081

Epoch: 5| Step: 2
Training loss: 2.704305931075043
Validation loss: 2.453943929835349

Epoch: 5| Step: 3
Training loss: 2.7754350372777026
Validation loss: 2.44770754198185

Epoch: 5| Step: 4
Training loss: 2.552873626623441
Validation loss: 2.449672038483744

Epoch: 5| Step: 5
Training loss: 2.72327616351044
Validation loss: 2.447442081114844

Epoch: 5| Step: 6
Training loss: 2.546062129135399
Validation loss: 2.4519201547414435

Epoch: 5| Step: 7
Training loss: 2.776015681752921
Validation loss: 2.446637786760619

Epoch: 5| Step: 8
Training loss: 1.8351271768721642
Validation loss: 2.4421990879798403

Epoch: 5| Step: 9
Training loss: 2.371442037074716
Validation loss: 2.4481559852870722

Epoch: 5| Step: 10
Training loss: 2.1747776093186855
Validation loss: 2.4549193531976763

Epoch: 5| Step: 11
Training loss: 3.259941887132792
Validation loss: 2.4559485228631286

Epoch: 129| Step: 0
Training loss: 2.128768608473366
Validation loss: 2.446611967114154

Epoch: 5| Step: 1
Training loss: 3.0036924209187705
Validation loss: 2.4447835039268813

Epoch: 5| Step: 2
Training loss: 2.201217149947534
Validation loss: 2.4512850913843467

Epoch: 5| Step: 3
Training loss: 2.425566388871624
Validation loss: 2.4555556936201666

Epoch: 5| Step: 4
Training loss: 2.4532600900179546
Validation loss: 2.448548825120249

Epoch: 5| Step: 5
Training loss: 2.1343303491437595
Validation loss: 2.4464586758775093

Epoch: 5| Step: 6
Training loss: 2.4277384014510557
Validation loss: 2.4488448933027125

Epoch: 5| Step: 7
Training loss: 2.9748862044425035
Validation loss: 2.4504351497889933

Epoch: 5| Step: 8
Training loss: 2.131565609828871
Validation loss: 2.449704225034312

Epoch: 5| Step: 9
Training loss: 3.274455565415101
Validation loss: 2.453320027756718

Epoch: 5| Step: 10
Training loss: 2.2916334901199953
Validation loss: 2.4591068422151965

Epoch: 5| Step: 11
Training loss: 3.33645289993598
Validation loss: 2.4530783120347235

Epoch: 130| Step: 0
Training loss: 2.096516617508727
Validation loss: 2.4515554343308756

Epoch: 5| Step: 1
Training loss: 2.693924964126007
Validation loss: 2.448733397786654

Epoch: 5| Step: 2
Training loss: 2.255764464656566
Validation loss: 2.4483754005501264

Epoch: 5| Step: 3
Training loss: 2.5837954036154143
Validation loss: 2.4468934592325766

Epoch: 5| Step: 4
Training loss: 3.2295262946584944
Validation loss: 2.4638394800120795

Epoch: 5| Step: 5
Training loss: 2.0561713021010637
Validation loss: 2.4554911336628042

Epoch: 5| Step: 6
Training loss: 2.409890034964529
Validation loss: 2.4582313028811202

Epoch: 5| Step: 7
Training loss: 2.9653516090685224
Validation loss: 2.473106813284383

Epoch: 5| Step: 8
Training loss: 2.924347672714906
Validation loss: 2.47089130152419

Epoch: 5| Step: 9
Training loss: 2.5006639552591965
Validation loss: 2.458182622578644

Epoch: 5| Step: 10
Training loss: 2.457423241038406
Validation loss: 2.4416854658043254

Epoch: 5| Step: 11
Training loss: 1.0223620639275994
Validation loss: 2.4513468037835944

Epoch: 131| Step: 0
Training loss: 2.435107303894827
Validation loss: 2.447862918074302

Epoch: 5| Step: 1
Training loss: 2.0871463087677586
Validation loss: 2.459911427208487

Epoch: 5| Step: 2
Training loss: 2.0672505317933654
Validation loss: 2.4669693669820605

Epoch: 5| Step: 3
Training loss: 2.54568491085292
Validation loss: 2.4783950608000938

Epoch: 5| Step: 4
Training loss: 2.1013959513756584
Validation loss: 2.481739949971218

Epoch: 5| Step: 5
Training loss: 2.3695854907771965
Validation loss: 2.480165367897173

Epoch: 5| Step: 6
Training loss: 2.3258343378837307
Validation loss: 2.481167924518035

Epoch: 5| Step: 7
Training loss: 3.1495854483406105
Validation loss: 2.488437551862253

Epoch: 5| Step: 8
Training loss: 2.6812041414336156
Validation loss: 2.4810169285065102

Epoch: 5| Step: 9
Training loss: 3.2518620658703803
Validation loss: 2.492612453731052

Epoch: 5| Step: 10
Training loss: 2.9190578649574124
Validation loss: 2.4870914267456596

Epoch: 5| Step: 11
Training loss: 3.291935841302493
Validation loss: 2.4829558031655496

Epoch: 132| Step: 0
Training loss: 2.4148492117616067
Validation loss: 2.4791497355505405

Epoch: 5| Step: 1
Training loss: 2.9415860266794778
Validation loss: 2.485620434400116

Epoch: 5| Step: 2
Training loss: 2.2802272228320013
Validation loss: 2.481583912799517

Epoch: 5| Step: 3
Training loss: 2.2419118678579197
Validation loss: 2.477573457187699

Epoch: 5| Step: 4
Training loss: 2.5327228923292444
Validation loss: 2.4790388736859685

Epoch: 5| Step: 5
Training loss: 2.1926300928891687
Validation loss: 2.473972699307676

Epoch: 5| Step: 6
Training loss: 3.155008052809605
Validation loss: 2.4674381520959634

Epoch: 5| Step: 7
Training loss: 2.80290994659143
Validation loss: 2.4702197299357747

Epoch: 5| Step: 8
Training loss: 2.8046060619378776
Validation loss: 2.4653441620934555

Epoch: 5| Step: 9
Training loss: 2.7510556015668963
Validation loss: 2.467697611364536

Epoch: 5| Step: 10
Training loss: 1.9240083716521668
Validation loss: 2.4632438468896263

Epoch: 5| Step: 11
Training loss: 2.4934411797004508
Validation loss: 2.4579791336498213

Epoch: 133| Step: 0
Training loss: 2.569537663415529
Validation loss: 2.454515498396085

Epoch: 5| Step: 1
Training loss: 2.6263617661817977
Validation loss: 2.454033045755317

Epoch: 5| Step: 2
Training loss: 2.391735143268258
Validation loss: 2.454555426495993

Epoch: 5| Step: 3
Training loss: 2.475861748072547
Validation loss: 2.4546273241734826

Epoch: 5| Step: 4
Training loss: 2.7820801085477744
Validation loss: 2.468015497128589

Epoch: 5| Step: 5
Training loss: 2.205630021342234
Validation loss: 2.466578683594252

Epoch: 5| Step: 6
Training loss: 2.738637860159967
Validation loss: 2.475310898850818

Epoch: 5| Step: 7
Training loss: 2.6725350618694206
Validation loss: 2.4691722766255886

Epoch: 5| Step: 8
Training loss: 2.479856304343389
Validation loss: 2.46989819428193

Epoch: 5| Step: 9
Training loss: 2.640075084601107
Validation loss: 2.4526665455571974

Epoch: 5| Step: 10
Training loss: 2.695705219351454
Validation loss: 2.4560039780464944

Epoch: 5| Step: 11
Training loss: 1.8333494084549338
Validation loss: 2.464195996596823

Epoch: 134| Step: 0
Training loss: 2.448148210545497
Validation loss: 2.4573354042584032

Epoch: 5| Step: 1
Training loss: 2.5324337863833226
Validation loss: 2.4500191519922887

Epoch: 5| Step: 2
Training loss: 2.2710378245635066
Validation loss: 2.45481098656842

Epoch: 5| Step: 3
Training loss: 2.2324951373449937
Validation loss: 2.4575383785461637

Epoch: 5| Step: 4
Training loss: 2.7206714023074334
Validation loss: 2.4546323587532286

Epoch: 5| Step: 5
Training loss: 2.34868397488453
Validation loss: 2.4549367292904694

Epoch: 5| Step: 6
Training loss: 2.92673044125089
Validation loss: 2.4637596579649

Epoch: 5| Step: 7
Training loss: 2.8334871325169555
Validation loss: 2.4559224330111316

Epoch: 5| Step: 8
Training loss: 2.3672922444854123
Validation loss: 2.466854750195188

Epoch: 5| Step: 9
Training loss: 2.5726464524764565
Validation loss: 2.4562109430457975

Epoch: 5| Step: 10
Training loss: 2.5188426414850684
Validation loss: 2.452397119047304

Epoch: 5| Step: 11
Training loss: 2.496629732042959
Validation loss: 2.4511394321275444

Epoch: 135| Step: 0
Training loss: 2.8032889531901852
Validation loss: 2.4607346547002367

Epoch: 5| Step: 1
Training loss: 1.6891158809219513
Validation loss: 2.4510470374994626

Epoch: 5| Step: 2
Training loss: 2.5210866459290404
Validation loss: 2.463553881598118

Epoch: 5| Step: 3
Training loss: 2.130410207371426
Validation loss: 2.469969613175345

Epoch: 5| Step: 4
Training loss: 2.5321153645019274
Validation loss: 2.482201591220217

Epoch: 5| Step: 5
Training loss: 2.7468495095451404
Validation loss: 2.4808806910835646

Epoch: 5| Step: 6
Training loss: 2.733140590508983
Validation loss: 2.4631870382827543

Epoch: 5| Step: 7
Training loss: 3.1060546322333047
Validation loss: 2.4608086284823085

Epoch: 5| Step: 8
Training loss: 2.2049102043500155
Validation loss: 2.462081584706

Epoch: 5| Step: 9
Training loss: 2.4607610730099667
Validation loss: 2.4645212791758895

Epoch: 5| Step: 10
Training loss: 2.6471943596192737
Validation loss: 2.466989336034042

Epoch: 5| Step: 11
Training loss: 2.826822391874855
Validation loss: 2.4594879644706533

Epoch: 136| Step: 0
Training loss: 2.5578687732730545
Validation loss: 2.4692644436333895

Epoch: 5| Step: 1
Training loss: 2.637627610287885
Validation loss: 2.460497930919788

Epoch: 5| Step: 2
Training loss: 2.8913972390134175
Validation loss: 2.460489631952303

Epoch: 5| Step: 3
Training loss: 3.1329249792288474
Validation loss: 2.465543846016419

Epoch: 5| Step: 4
Training loss: 2.5615152467225126
Validation loss: 2.4564014591956265

Epoch: 5| Step: 5
Training loss: 2.203232782852182
Validation loss: 2.4561083990636123

Epoch: 5| Step: 6
Training loss: 2.3481237672747213
Validation loss: 2.4580479930432237

Epoch: 5| Step: 7
Training loss: 2.68816793258927
Validation loss: 2.457954451626266

Epoch: 5| Step: 8
Training loss: 2.3278980528542097
Validation loss: 2.4526369212689256

Epoch: 5| Step: 9
Training loss: 2.122866063985799
Validation loss: 2.4603831115833397

Epoch: 5| Step: 10
Training loss: 2.2741867875657817
Validation loss: 2.4642443686360926

Epoch: 5| Step: 11
Training loss: 2.6093450304697363
Validation loss: 2.458734264788873

Epoch: 137| Step: 0
Training loss: 2.185989948595118
Validation loss: 2.455855390932688

Epoch: 5| Step: 1
Training loss: 2.140958996632689
Validation loss: 2.4601337634137384

Epoch: 5| Step: 2
Training loss: 2.521085889370783
Validation loss: 2.455410833886421

Epoch: 5| Step: 3
Training loss: 2.6345429209704103
Validation loss: 2.4532196650290623

Epoch: 5| Step: 4
Training loss: 2.450761470404771
Validation loss: 2.4554359217799924

Epoch: 5| Step: 5
Training loss: 2.363124873764894
Validation loss: 2.4518795374921423

Epoch: 5| Step: 6
Training loss: 2.850074191549909
Validation loss: 2.4568555270755765

Epoch: 5| Step: 7
Training loss: 2.726073475351972
Validation loss: 2.4530796403194137

Epoch: 5| Step: 8
Training loss: 2.2033628511206977
Validation loss: 2.4510740060506135

Epoch: 5| Step: 9
Training loss: 2.5941355774860075
Validation loss: 2.4532294564979025

Epoch: 5| Step: 10
Training loss: 2.764603248261908
Validation loss: 2.4461610985216673

Epoch: 5| Step: 11
Training loss: 3.5468038812304834
Validation loss: 2.450888681453682

Epoch: 138| Step: 0
Training loss: 2.6152963666569833
Validation loss: 2.4491647514624786

Epoch: 5| Step: 1
Training loss: 2.5759488255766545
Validation loss: 2.449505328336715

Epoch: 5| Step: 2
Training loss: 2.60028741421706
Validation loss: 2.4455833975631966

Epoch: 5| Step: 3
Training loss: 2.3925392614828325
Validation loss: 2.4515870693799906

Epoch: 5| Step: 4
Training loss: 2.564960322082473
Validation loss: 2.44852749258073

Epoch: 5| Step: 5
Training loss: 2.4416548701534064
Validation loss: 2.450535956946529

Epoch: 5| Step: 6
Training loss: 3.0223719569898946
Validation loss: 2.4532451559716284

Epoch: 5| Step: 7
Training loss: 2.1432868390456443
Validation loss: 2.449230252877236

Epoch: 5| Step: 8
Training loss: 2.390204398859825
Validation loss: 2.4509531518787475

Epoch: 5| Step: 9
Training loss: 2.6322872192522366
Validation loss: 2.4533822882430925

Epoch: 5| Step: 10
Training loss: 2.4211997782905157
Validation loss: 2.4500833756669014

Epoch: 5| Step: 11
Training loss: 1.5336686320589026
Validation loss: 2.4489267957033976

Epoch: 139| Step: 0
Training loss: 2.367504841397742
Validation loss: 2.4625456545965787

Epoch: 5| Step: 1
Training loss: 2.7870366622325617
Validation loss: 2.4720547001107005

Epoch: 5| Step: 2
Training loss: 2.272132735082288
Validation loss: 2.465255964806517

Epoch: 5| Step: 3
Training loss: 2.4641288288794767
Validation loss: 2.4737496592537656

Epoch: 5| Step: 4
Training loss: 2.4832664271650176
Validation loss: 2.4747256638306294

Epoch: 5| Step: 5
Training loss: 2.5150731589140585
Validation loss: 2.4674213552261546

Epoch: 5| Step: 6
Training loss: 2.553723447925857
Validation loss: 2.4790989976083915

Epoch: 5| Step: 7
Training loss: 2.463555484488342
Validation loss: 2.4690125643659093

Epoch: 5| Step: 8
Training loss: 2.696936518916723
Validation loss: 2.4621329012951216

Epoch: 5| Step: 9
Training loss: 2.5087979005841308
Validation loss: 2.455940481555347

Epoch: 5| Step: 10
Training loss: 2.6784549469825203
Validation loss: 2.452839022770973

Epoch: 5| Step: 11
Training loss: 1.8401840122095137
Validation loss: 2.4380392187303115

Epoch: 140| Step: 0
Training loss: 2.913957445639026
Validation loss: 2.446852870189879

Epoch: 5| Step: 1
Training loss: 2.320975160859474
Validation loss: 2.4474160122058124

Epoch: 5| Step: 2
Training loss: 2.4090198576707706
Validation loss: 2.4550417965932714

Epoch: 5| Step: 3
Training loss: 2.6570488121307005
Validation loss: 2.4533336453485117

Epoch: 5| Step: 4
Training loss: 2.1725482583255586
Validation loss: 2.4560380979348033

Epoch: 5| Step: 5
Training loss: 2.7455069510518317
Validation loss: 2.4635295276194915

Epoch: 5| Step: 6
Training loss: 2.6294563250776237
Validation loss: 2.4578730925544012

Epoch: 5| Step: 7
Training loss: 2.4183160042574574
Validation loss: 2.4555066184169143

Epoch: 5| Step: 8
Training loss: 2.242882384973021
Validation loss: 2.448959393638296

Epoch: 5| Step: 9
Training loss: 2.662294220471776
Validation loss: 2.447236079438252

Epoch: 5| Step: 10
Training loss: 2.528529270889043
Validation loss: 2.4477845595945458

Epoch: 5| Step: 11
Training loss: 2.4617638547803242
Validation loss: 2.4466220773714427

Epoch: 141| Step: 0
Training loss: 2.765447923411694
Validation loss: 2.4461071563519017

Epoch: 5| Step: 1
Training loss: 2.495905097434919
Validation loss: 2.4472312346384015

Epoch: 5| Step: 2
Training loss: 2.659989451193866
Validation loss: 2.4433462791589973

Epoch: 5| Step: 3
Training loss: 2.374745405504935
Validation loss: 2.447567664624545

Epoch: 5| Step: 4
Training loss: 2.552209241187568
Validation loss: 2.4483698337522695

Epoch: 5| Step: 5
Training loss: 2.4489868638358847
Validation loss: 2.4431801579545707

Epoch: 5| Step: 6
Training loss: 2.845309154963111
Validation loss: 2.448028173680078

Epoch: 5| Step: 7
Training loss: 2.34851271863567
Validation loss: 2.4440624893377723

Epoch: 5| Step: 8
Training loss: 2.5964131299168924
Validation loss: 2.448080921073305

Epoch: 5| Step: 9
Training loss: 1.8954239148180054
Validation loss: 2.4435885835429496

Epoch: 5| Step: 10
Training loss: 2.3761138311919607
Validation loss: 2.446850375347536

Epoch: 5| Step: 11
Training loss: 2.8937463352517967
Validation loss: 2.44702023805502

Epoch: 142| Step: 0
Training loss: 2.4768568760499736
Validation loss: 2.443677507969231

Epoch: 5| Step: 1
Training loss: 2.4359261738218385
Validation loss: 2.4406753910933903

Epoch: 5| Step: 2
Training loss: 2.19016452362921
Validation loss: 2.455835185723911

Epoch: 5| Step: 3
Training loss: 1.7740758343629386
Validation loss: 2.45742937752969

Epoch: 5| Step: 4
Training loss: 2.434994706103075
Validation loss: 2.454763395842764

Epoch: 5| Step: 5
Training loss: 2.8183623358168353
Validation loss: 2.4630848270803263

Epoch: 5| Step: 6
Training loss: 2.3949008177287454
Validation loss: 2.4496133984245927

Epoch: 5| Step: 7
Training loss: 2.5852158568108297
Validation loss: 2.4464572303009557

Epoch: 5| Step: 8
Training loss: 2.995311251915168
Validation loss: 2.4453561553470764

Epoch: 5| Step: 9
Training loss: 2.5242501949050675
Validation loss: 2.4459077518867294

Epoch: 5| Step: 10
Training loss: 2.7286116542861145
Validation loss: 2.4509282612913235

Epoch: 5| Step: 11
Training loss: 2.4356146147606648
Validation loss: 2.4563387413916153

Epoch: 143| Step: 0
Training loss: 2.5999791364566303
Validation loss: 2.459575978832008

Epoch: 5| Step: 1
Training loss: 3.103049279832476
Validation loss: 2.4629668330624526

Epoch: 5| Step: 2
Training loss: 2.2389393316707134
Validation loss: 2.4651725013772525

Epoch: 5| Step: 3
Training loss: 2.445301969950371
Validation loss: 2.460529224901513

Epoch: 5| Step: 4
Training loss: 2.3457017020317292
Validation loss: 2.467948385059831

Epoch: 5| Step: 5
Training loss: 2.697568441376538
Validation loss: 2.4580688590227653

Epoch: 5| Step: 6
Training loss: 2.171175837329064
Validation loss: 2.4679443074789407

Epoch: 5| Step: 7
Training loss: 1.9779544799010282
Validation loss: 2.464642233855501

Epoch: 5| Step: 8
Training loss: 2.5755395120394042
Validation loss: 2.4670303326878322

Epoch: 5| Step: 9
Training loss: 2.4905105736842508
Validation loss: 2.4606672229395015

Epoch: 5| Step: 10
Training loss: 2.931915331327083
Validation loss: 2.4515517225448002

Epoch: 5| Step: 11
Training loss: 2.2009064887571705
Validation loss: 2.453693461713335

Epoch: 144| Step: 0
Training loss: 2.646035334367751
Validation loss: 2.4514425505677426

Epoch: 5| Step: 1
Training loss: 2.4082722278837565
Validation loss: 2.4654619677930425

Epoch: 5| Step: 2
Training loss: 2.3088579107406644
Validation loss: 2.467596448522607

Epoch: 5| Step: 3
Training loss: 2.3049042163401685
Validation loss: 2.480115117373824

Epoch: 5| Step: 4
Training loss: 2.438504036720725
Validation loss: 2.4793567037007485

Epoch: 5| Step: 5
Training loss: 2.527566089313185
Validation loss: 2.4674667552272465

Epoch: 5| Step: 6
Training loss: 2.3152540149116367
Validation loss: 2.4715132680528185

Epoch: 5| Step: 7
Training loss: 2.343750610351483
Validation loss: 2.462954680426361

Epoch: 5| Step: 8
Training loss: 2.908788413337989
Validation loss: 2.4564861912043217

Epoch: 5| Step: 9
Training loss: 2.490918832100475
Validation loss: 2.4469526719472627

Epoch: 5| Step: 10
Training loss: 2.9254437036699725
Validation loss: 2.449134030212806

Epoch: 5| Step: 11
Training loss: 2.839868264702477
Validation loss: 2.4526498500425116

Epoch: 145| Step: 0
Training loss: 2.4612144677661782
Validation loss: 2.451025438937253

Epoch: 5| Step: 1
Training loss: 2.5549301358877856
Validation loss: 2.4480152448460504

Epoch: 5| Step: 2
Training loss: 2.0078571477890885
Validation loss: 2.455511649171009

Epoch: 5| Step: 3
Training loss: 2.727822348814731
Validation loss: 2.449500666469985

Epoch: 5| Step: 4
Training loss: 2.3188979849672253
Validation loss: 2.449171139844003

Epoch: 5| Step: 5
Training loss: 2.2809963346566433
Validation loss: 2.451491729415054

Epoch: 5| Step: 6
Training loss: 2.1863948755262155
Validation loss: 2.4579703472496734

Epoch: 5| Step: 7
Training loss: 2.4647592085714565
Validation loss: 2.467152529358525

Epoch: 5| Step: 8
Training loss: 3.0615642441459094
Validation loss: 2.4608310252573995

Epoch: 5| Step: 9
Training loss: 2.9504566550394538
Validation loss: 2.4612815091718905

Epoch: 5| Step: 10
Training loss: 2.4435212518506666
Validation loss: 2.457662182993024

Epoch: 5| Step: 11
Training loss: 1.400379473483537
Validation loss: 2.457215367440195

Epoch: 146| Step: 0
Training loss: 2.5798153970900213
Validation loss: 2.461959759758327

Epoch: 5| Step: 1
Training loss: 2.4089627518107903
Validation loss: 2.4601953063429574

Epoch: 5| Step: 2
Training loss: 2.3069955421937123
Validation loss: 2.46913591418737

Epoch: 5| Step: 3
Training loss: 2.447260071914839
Validation loss: 2.478846121883428

Epoch: 5| Step: 4
Training loss: 2.614366793436097
Validation loss: 2.4855818426324454

Epoch: 5| Step: 5
Training loss: 2.0701737267390423
Validation loss: 2.495146084473179

Epoch: 5| Step: 6
Training loss: 2.7124977568867634
Validation loss: 2.483698404896459

Epoch: 5| Step: 7
Training loss: 2.768644591794221
Validation loss: 2.47029611403942

Epoch: 5| Step: 8
Training loss: 2.8451988805032062
Validation loss: 2.4573069439507833

Epoch: 5| Step: 9
Training loss: 2.4821535171013287
Validation loss: 2.4571666265813388

Epoch: 5| Step: 10
Training loss: 2.5333351683191716
Validation loss: 2.459798300933313

Epoch: 5| Step: 11
Training loss: 1.9950228989513097
Validation loss: 2.453950933247765

Epoch: 147| Step: 0
Training loss: 2.015180671865128
Validation loss: 2.4625850269077336

Epoch: 5| Step: 1
Training loss: 2.5836080794719987
Validation loss: 2.4729342050795466

Epoch: 5| Step: 2
Training loss: 2.956141954185539
Validation loss: 2.4787351622773013

Epoch: 5| Step: 3
Training loss: 2.454818044162102
Validation loss: 2.4812787152797937

Epoch: 5| Step: 4
Training loss: 2.0396902937568244
Validation loss: 2.4928818615330544

Epoch: 5| Step: 5
Training loss: 2.8388241875183393
Validation loss: 2.4913566782993812

Epoch: 5| Step: 6
Training loss: 2.88402699265037
Validation loss: 2.4990285177950633

Epoch: 5| Step: 7
Training loss: 2.746300462997948
Validation loss: 2.5002706778541257

Epoch: 5| Step: 8
Training loss: 2.5202139942195694
Validation loss: 2.4931356565653027

Epoch: 5| Step: 9
Training loss: 2.3220592564292266
Validation loss: 2.504304061765885

Epoch: 5| Step: 10
Training loss: 2.9524989196103517
Validation loss: 2.50186243222927

Epoch: 5| Step: 11
Training loss: 2.3301618230069185
Validation loss: 2.498914594268813

Epoch: 148| Step: 0
Training loss: 2.627687077998979
Validation loss: 2.503782637268031

Epoch: 5| Step: 1
Training loss: 2.3927195230858143
Validation loss: 2.496709271128149

Epoch: 5| Step: 2
Training loss: 2.837717889830775
Validation loss: 2.493504725211892

Epoch: 5| Step: 3
Training loss: 2.5161437453430606
Validation loss: 2.496391035911722

Epoch: 5| Step: 4
Training loss: 2.4909502264599115
Validation loss: 2.498398788594392

Epoch: 5| Step: 5
Training loss: 2.11794050838788
Validation loss: 2.492987214453114

Epoch: 5| Step: 6
Training loss: 1.8189045450415418
Validation loss: 2.485795904283359

Epoch: 5| Step: 7
Training loss: 2.346737496199376
Validation loss: 2.489065584642599

Epoch: 5| Step: 8
Training loss: 2.8120830650704396
Validation loss: 2.482757113530946

Epoch: 5| Step: 9
Training loss: 3.3360559947210944
Validation loss: 2.4734135693862234

Epoch: 5| Step: 10
Training loss: 2.704039490642762
Validation loss: 2.4786897702834128

Epoch: 5| Step: 11
Training loss: 2.4091185278642535
Validation loss: 2.47030487670623

Epoch: 149| Step: 0
Training loss: 2.5545056012925302
Validation loss: 2.4652347063203277

Epoch: 5| Step: 1
Training loss: 2.540156202244129
Validation loss: 2.450730732689085

Epoch: 5| Step: 2
Training loss: 2.4270544964519356
Validation loss: 2.45107602847734

Epoch: 5| Step: 3
Training loss: 2.289834058487946
Validation loss: 2.472662240212463

Epoch: 5| Step: 4
Training loss: 2.5671335522305405
Validation loss: 2.5036807025607746

Epoch: 5| Step: 5
Training loss: 2.7633191857552903
Validation loss: 2.507451476099526

Epoch: 5| Step: 6
Training loss: 2.728701738742458
Validation loss: 2.5420995810009868

Epoch: 5| Step: 7
Training loss: 2.720173342594811
Validation loss: 2.5601285063569628

Epoch: 5| Step: 8
Training loss: 2.2306890428989914
Validation loss: 2.537989764405351

Epoch: 5| Step: 9
Training loss: 2.7157730817202834
Validation loss: 2.535772651301912

Epoch: 5| Step: 10
Training loss: 2.9509416208881762
Validation loss: 2.493109413959565

Epoch: 5| Step: 11
Training loss: 2.657811502482268
Validation loss: 2.4716059024267

Epoch: 150| Step: 0
Training loss: 2.8011494797489744
Validation loss: 2.457773958564023

Epoch: 5| Step: 1
Training loss: 2.6377937442585906
Validation loss: 2.4486970239334944

Epoch: 5| Step: 2
Training loss: 2.4876517513127365
Validation loss: 2.452377841336177

Epoch: 5| Step: 3
Training loss: 2.881568702399009
Validation loss: 2.4523491331043203

Epoch: 5| Step: 4
Training loss: 2.630153683135573
Validation loss: 2.454476832417714

Epoch: 5| Step: 5
Training loss: 2.796968682281206
Validation loss: 2.4530129700691172

Epoch: 5| Step: 6
Training loss: 1.8128389501984696
Validation loss: 2.458048045582229

Epoch: 5| Step: 7
Training loss: 2.6196654474530883
Validation loss: 2.4572800033786293

Epoch: 5| Step: 8
Training loss: 2.315211175963575
Validation loss: 2.4576835635689314

Epoch: 5| Step: 9
Training loss: 2.1398900016251248
Validation loss: 2.460486876390137

Epoch: 5| Step: 10
Training loss: 2.4612958374714817
Validation loss: 2.4609476119545164

Epoch: 5| Step: 11
Training loss: 2.729947674847187
Validation loss: 2.4575271550745006

Epoch: 151| Step: 0
Training loss: 2.6373183635342103
Validation loss: 2.4589236501680163

Epoch: 5| Step: 1
Training loss: 3.0601151191648825
Validation loss: 2.457106374331011

Epoch: 5| Step: 2
Training loss: 3.0266049375695068
Validation loss: 2.4576839273544158

Epoch: 5| Step: 3
Training loss: 2.9374279256866056
Validation loss: 2.4555214921847948

Epoch: 5| Step: 4
Training loss: 1.7912992795672842
Validation loss: 2.455434080957941

Epoch: 5| Step: 5
Training loss: 1.8750596354855482
Validation loss: 2.4546379234897273

Epoch: 5| Step: 6
Training loss: 2.800966307247107
Validation loss: 2.4500804847355684

Epoch: 5| Step: 7
Training loss: 2.481905783753181
Validation loss: 2.448342149700013

Epoch: 5| Step: 8
Training loss: 2.388163677063112
Validation loss: 2.4522756009062667

Epoch: 5| Step: 9
Training loss: 2.385034944290301
Validation loss: 2.4453152584430056

Epoch: 5| Step: 10
Training loss: 2.0391408386382355
Validation loss: 2.4450261615648494

Epoch: 5| Step: 11
Training loss: 2.2059811955166673
Validation loss: 2.4446056591562133

Epoch: 152| Step: 0
Training loss: 2.7139485372050918
Validation loss: 2.445073303938824

Epoch: 5| Step: 1
Training loss: 2.528001278182805
Validation loss: 2.438788472857415

Epoch: 5| Step: 2
Training loss: 2.0075044506969024
Validation loss: 2.4421906353065377

Epoch: 5| Step: 3
Training loss: 2.5484504741476437
Validation loss: 2.4451283841598292

Epoch: 5| Step: 4
Training loss: 2.0502231383418215
Validation loss: 2.445870028182072

Epoch: 5| Step: 5
Training loss: 2.5463776834241667
Validation loss: 2.4470386303445566

Epoch: 5| Step: 6
Training loss: 2.6911426077330516
Validation loss: 2.4453092784271306

Epoch: 5| Step: 7
Training loss: 2.7722855015601557
Validation loss: 2.448048341875543

Epoch: 5| Step: 8
Training loss: 2.5360816712450522
Validation loss: 2.449825075937496

Epoch: 5| Step: 9
Training loss: 2.1314157236947313
Validation loss: 2.4562307973407127

Epoch: 5| Step: 10
Training loss: 2.861024474608968
Validation loss: 2.4546347505796984

Epoch: 5| Step: 11
Training loss: 2.848387238461353
Validation loss: 2.455334128317375

Epoch: 153| Step: 0
Training loss: 2.7064978223982528
Validation loss: 2.4525024612238124

Epoch: 5| Step: 1
Training loss: 2.2668792409886103
Validation loss: 2.446806117355333

Epoch: 5| Step: 2
Training loss: 2.153852595068135
Validation loss: 2.4551270894033825

Epoch: 5| Step: 3
Training loss: 2.0994419900575245
Validation loss: 2.448061063579376

Epoch: 5| Step: 4
Training loss: 2.174711830955765
Validation loss: 2.450917694570791

Epoch: 5| Step: 5
Training loss: 2.596895172371307
Validation loss: 2.4496904574989244

Epoch: 5| Step: 6
Training loss: 2.573689384617427
Validation loss: 2.446985775229261

Epoch: 5| Step: 7
Training loss: 2.8596524797057947
Validation loss: 2.4529428353675993

Epoch: 5| Step: 8
Training loss: 2.595644890653663
Validation loss: 2.447417593193102

Epoch: 5| Step: 9
Training loss: 2.6961627006280717
Validation loss: 2.4486945857382016

Epoch: 5| Step: 10
Training loss: 2.7481611346015757
Validation loss: 2.456548419972377

Epoch: 5| Step: 11
Training loss: 2.495794669368201
Validation loss: 2.446429345936631

Epoch: 154| Step: 0
Training loss: 2.4546595347831155
Validation loss: 2.449553937755617

Epoch: 5| Step: 1
Training loss: 2.7691827509050673
Validation loss: 2.455985776264399

Epoch: 5| Step: 2
Training loss: 2.4728118702880337
Validation loss: 2.4562050219131293

Epoch: 5| Step: 3
Training loss: 2.1904233744475454
Validation loss: 2.4585672498934903

Epoch: 5| Step: 4
Training loss: 3.317143047248738
Validation loss: 2.4565947710496037

Epoch: 5| Step: 5
Training loss: 2.5470310467616457
Validation loss: 2.4533854020357646

Epoch: 5| Step: 6
Training loss: 2.414518640379484
Validation loss: 2.4521475057272264

Epoch: 5| Step: 7
Training loss: 2.0811171251558482
Validation loss: 2.453174076277849

Epoch: 5| Step: 8
Training loss: 2.5807152884707145
Validation loss: 2.4553895933310566

Epoch: 5| Step: 9
Training loss: 2.279966437444109
Validation loss: 2.444318232038472

Epoch: 5| Step: 10
Training loss: 2.318108227695605
Validation loss: 2.4462313403478797

Epoch: 5| Step: 11
Training loss: 2.7975227841837755
Validation loss: 2.444308736103422

Epoch: 155| Step: 0
Training loss: 2.369344804712768
Validation loss: 2.445577072926983

Epoch: 5| Step: 1
Training loss: 2.4802078710622624
Validation loss: 2.4473042729871053

Epoch: 5| Step: 2
Training loss: 2.4743926834819674
Validation loss: 2.4470397244185653

Epoch: 5| Step: 3
Training loss: 2.622054309192318
Validation loss: 2.4526356190719434

Epoch: 5| Step: 4
Training loss: 2.627970876268166
Validation loss: 2.4521184058801997

Epoch: 5| Step: 5
Training loss: 2.5724383899365852
Validation loss: 2.4490865198161136

Epoch: 5| Step: 6
Training loss: 2.4355769274095684
Validation loss: 2.4507953004978447

Epoch: 5| Step: 7
Training loss: 2.371363013374108
Validation loss: 2.4474162313928147

Epoch: 5| Step: 8
Training loss: 2.6830596614464435
Validation loss: 2.447080921328011

Epoch: 5| Step: 9
Training loss: 2.96598108563659
Validation loss: 2.4482262124837697

Epoch: 5| Step: 10
Training loss: 1.9318385934006361
Validation loss: 2.4503661433551227

Epoch: 5| Step: 11
Training loss: 1.7951043734582361
Validation loss: 2.4482192089258805

Epoch: 156| Step: 0
Training loss: 2.6252136143552383
Validation loss: 2.4605689524639023

Epoch: 5| Step: 1
Training loss: 2.2669343519158196
Validation loss: 2.4541899157547102

Epoch: 5| Step: 2
Training loss: 2.545057525883962
Validation loss: 2.4661183011627235

Epoch: 5| Step: 3
Training loss: 2.5071917089345885
Validation loss: 2.468671340735488

Epoch: 5| Step: 4
Training loss: 2.775083413459883
Validation loss: 2.4690214925280913

Epoch: 5| Step: 5
Training loss: 2.2714880486521776
Validation loss: 2.4836735184906953

Epoch: 5| Step: 6
Training loss: 2.052942847963775
Validation loss: 2.4685097911522833

Epoch: 5| Step: 7
Training loss: 1.9128719441874877
Validation loss: 2.45630591379836

Epoch: 5| Step: 8
Training loss: 2.597147910493727
Validation loss: 2.4648300106030403

Epoch: 5| Step: 9
Training loss: 3.1075911269019145
Validation loss: 2.4626214699388527

Epoch: 5| Step: 10
Training loss: 2.633243061482078
Validation loss: 2.457565916560833

Epoch: 5| Step: 11
Training loss: 2.708530555782313
Validation loss: 2.455918917938568

Epoch: 157| Step: 0
Training loss: 2.025407340441511
Validation loss: 2.4600735111482637

Epoch: 5| Step: 1
Training loss: 2.714230167566276
Validation loss: 2.45906900185017

Epoch: 5| Step: 2
Training loss: 2.4257956978925117
Validation loss: 2.4615280391451644

Epoch: 5| Step: 3
Training loss: 2.603736404478681
Validation loss: 2.4586861095974193

Epoch: 5| Step: 4
Training loss: 2.8822477454306457
Validation loss: 2.4614853990777696

Epoch: 5| Step: 5
Training loss: 2.448779199025971
Validation loss: 2.46375484767355

Epoch: 5| Step: 6
Training loss: 2.3018659486160242
Validation loss: 2.4664102202463964

Epoch: 5| Step: 7
Training loss: 2.9043911147075874
Validation loss: 2.469542432512523

Epoch: 5| Step: 8
Training loss: 2.327891293258768
Validation loss: 2.4675446074504492

Epoch: 5| Step: 9
Training loss: 2.0924514615389493
Validation loss: 2.47023068863004

Epoch: 5| Step: 10
Training loss: 2.371073337825287
Validation loss: 2.465868358584617

Epoch: 5| Step: 11
Training loss: 3.538663256359206
Validation loss: 2.4706670384039637

Epoch: 158| Step: 0
Training loss: 2.6870764465133363
Validation loss: 2.463306695379403

Epoch: 5| Step: 1
Training loss: 2.467227804520327
Validation loss: 2.4581415026114257

Epoch: 5| Step: 2
Training loss: 2.42859941754922
Validation loss: 2.468604984933544

Epoch: 5| Step: 3
Training loss: 2.529896032587004
Validation loss: 2.4564714627975546

Epoch: 5| Step: 4
Training loss: 2.752570165004908
Validation loss: 2.465693082139434

Epoch: 5| Step: 5
Training loss: 2.2724282033256893
Validation loss: 2.474813005820116

Epoch: 5| Step: 6
Training loss: 2.18955439419289
Validation loss: 2.4884153975691996

Epoch: 5| Step: 7
Training loss: 2.3637905612412724
Validation loss: 2.505593923818635

Epoch: 5| Step: 8
Training loss: 2.719650382545078
Validation loss: 2.486355773660177

Epoch: 5| Step: 9
Training loss: 2.719331701026471
Validation loss: 2.4870492349443962

Epoch: 5| Step: 10
Training loss: 2.3539740784675534
Validation loss: 2.4681565822970857

Epoch: 5| Step: 11
Training loss: 2.4359919455697336
Validation loss: 2.4639059460111774

Epoch: 159| Step: 0
Training loss: 2.4643816386285073
Validation loss: 2.4534658814750294

Epoch: 5| Step: 1
Training loss: 3.1762511784274734
Validation loss: 2.4614858369640005

Epoch: 5| Step: 2
Training loss: 2.3862772781467503
Validation loss: 2.4647643312777316

Epoch: 5| Step: 3
Training loss: 2.1408276984038044
Validation loss: 2.4698068071016857

Epoch: 5| Step: 4
Training loss: 2.7061839353325206
Validation loss: 2.4723828301664725

Epoch: 5| Step: 5
Training loss: 2.5650794839720343
Validation loss: 2.470591908239792

Epoch: 5| Step: 6
Training loss: 2.1333647631277057
Validation loss: 2.4789901451503806

Epoch: 5| Step: 7
Training loss: 2.7489883989774135
Validation loss: 2.4790846479830586

Epoch: 5| Step: 8
Training loss: 2.471928156342661
Validation loss: 2.484785601835402

Epoch: 5| Step: 9
Training loss: 2.388886860789934
Validation loss: 2.4843715731679126

Epoch: 5| Step: 10
Training loss: 2.706419420115013
Validation loss: 2.476363834952789

Epoch: 5| Step: 11
Training loss: 2.582762644910261
Validation loss: 2.477596179552872

Epoch: 160| Step: 0
Training loss: 2.6103474523852324
Validation loss: 2.4769588398001443

Epoch: 5| Step: 1
Training loss: 2.32703205030506
Validation loss: 2.476229359455537

Epoch: 5| Step: 2
Training loss: 2.431512570360634
Validation loss: 2.466023722401226

Epoch: 5| Step: 3
Training loss: 3.2049251801182894
Validation loss: 2.4629077270462596

Epoch: 5| Step: 4
Training loss: 2.375274843073778
Validation loss: 2.466223981693955

Epoch: 5| Step: 5
Training loss: 2.129918744861102
Validation loss: 2.4603439705230308

Epoch: 5| Step: 6
Training loss: 2.2703591181149116
Validation loss: 2.455673954741819

Epoch: 5| Step: 7
Training loss: 2.771180343888151
Validation loss: 2.4624050749799467

Epoch: 5| Step: 8
Training loss: 2.9024142311954373
Validation loss: 2.452879632232107

Epoch: 5| Step: 9
Training loss: 2.3732257037814306
Validation loss: 2.4731222078270485

Epoch: 5| Step: 10
Training loss: 2.5210373745984636
Validation loss: 2.4815647137051613

Epoch: 5| Step: 11
Training loss: 1.7486708225434038
Validation loss: 2.4790659103969577

Epoch: 161| Step: 0
Training loss: 2.2509330298461334
Validation loss: 2.488746906906395

Epoch: 5| Step: 1
Training loss: 2.4123088306326252
Validation loss: 2.4718251049280404

Epoch: 5| Step: 2
Training loss: 2.582041407116463
Validation loss: 2.460151143057126

Epoch: 5| Step: 3
Training loss: 2.398658767086392
Validation loss: 2.4627272460700076

Epoch: 5| Step: 4
Training loss: 2.627156870148304
Validation loss: 2.448725413922271

Epoch: 5| Step: 5
Training loss: 2.158670255759737
Validation loss: 2.4547996737375932

Epoch: 5| Step: 6
Training loss: 1.844173253503417
Validation loss: 2.458232462693936

Epoch: 5| Step: 7
Training loss: 2.34249661310006
Validation loss: 2.4630886202996143

Epoch: 5| Step: 8
Training loss: 3.0990516627173093
Validation loss: 2.4585064097001483

Epoch: 5| Step: 9
Training loss: 2.897966619116358
Validation loss: 2.457122673674751

Epoch: 5| Step: 10
Training loss: 2.6877942478645496
Validation loss: 2.453511659153057

Epoch: 5| Step: 11
Training loss: 2.896734760597593
Validation loss: 2.4589343521649654

Epoch: 162| Step: 0
Training loss: 2.5195970157053913
Validation loss: 2.4616751075071313

Epoch: 5| Step: 1
Training loss: 2.4556961677629174
Validation loss: 2.460110366924819

Epoch: 5| Step: 2
Training loss: 2.433783803900005
Validation loss: 2.464381420950442

Epoch: 5| Step: 3
Training loss: 2.786134438392226
Validation loss: 2.4603875691186934

Epoch: 5| Step: 4
Training loss: 2.292731459652775
Validation loss: 2.4647984568549886

Epoch: 5| Step: 5
Training loss: 2.5920387965819183
Validation loss: 2.4662744246206447

Epoch: 5| Step: 6
Training loss: 2.1374657700681174
Validation loss: 2.4564865632551887

Epoch: 5| Step: 7
Training loss: 2.797877808559269
Validation loss: 2.4601032599044905

Epoch: 5| Step: 8
Training loss: 2.6732395546970484
Validation loss: 2.466438386208512

Epoch: 5| Step: 9
Training loss: 2.4528221380768804
Validation loss: 2.461314512569432

Epoch: 5| Step: 10
Training loss: 2.1362766178050916
Validation loss: 2.4628302465666763

Epoch: 5| Step: 11
Training loss: 3.87962016655223
Validation loss: 2.474251825424538

Epoch: 163| Step: 0
Training loss: 2.417013987433254
Validation loss: 2.463568295475367

Epoch: 5| Step: 1
Training loss: 2.3618743996620677
Validation loss: 2.4626997879556023

Epoch: 5| Step: 2
Training loss: 2.485622356778841
Validation loss: 2.47675594693455

Epoch: 5| Step: 3
Training loss: 2.6954669410576106
Validation loss: 2.4756410049644053

Epoch: 5| Step: 4
Training loss: 2.483552616687135
Validation loss: 2.4652379159585105

Epoch: 5| Step: 5
Training loss: 2.7049197374377347
Validation loss: 2.457734937630293

Epoch: 5| Step: 6
Training loss: 2.1374165791969806
Validation loss: 2.462418827907161

Epoch: 5| Step: 7
Training loss: 2.4666697755570626
Validation loss: 2.469105527926394

Epoch: 5| Step: 8
Training loss: 3.0464064017661334
Validation loss: 2.4591721232526176

Epoch: 5| Step: 9
Training loss: 2.5116529678582045
Validation loss: 2.455167009580158

Epoch: 5| Step: 10
Training loss: 1.9636252779238306
Validation loss: 2.4585621263967106

Epoch: 5| Step: 11
Training loss: 2.4361574437041496
Validation loss: 2.4583028384650283

Epoch: 164| Step: 0
Training loss: 2.9600854619035006
Validation loss: 2.4620540084314766

Epoch: 5| Step: 1
Training loss: 2.621902091030703
Validation loss: 2.4689354766512173

Epoch: 5| Step: 2
Training loss: 1.9016625283229955
Validation loss: 2.4878754576838973

Epoch: 5| Step: 3
Training loss: 2.6583708430626465
Validation loss: 2.479298228825493

Epoch: 5| Step: 4
Training loss: 2.5834441827761676
Validation loss: 2.480610128063225

Epoch: 5| Step: 5
Training loss: 2.8179397534982367
Validation loss: 2.486874232982726

Epoch: 5| Step: 6
Training loss: 2.145194084972483
Validation loss: 2.476150219653052

Epoch: 5| Step: 7
Training loss: 2.6901506171904512
Validation loss: 2.463115483223425

Epoch: 5| Step: 8
Training loss: 1.9943131419851492
Validation loss: 2.4524102840287596

Epoch: 5| Step: 9
Training loss: 2.8467672920160823
Validation loss: 2.4552258325945004

Epoch: 5| Step: 10
Training loss: 2.0994436934993215
Validation loss: 2.4580530367825766

Epoch: 5| Step: 11
Training loss: 2.297811200605731
Validation loss: 2.4388936533576038

Epoch: 165| Step: 0
Training loss: 2.5671859323284356
Validation loss: 2.4457937463765953

Epoch: 5| Step: 1
Training loss: 2.6821107278676934
Validation loss: 2.4482600451481193

Epoch: 5| Step: 2
Training loss: 2.7583890749858813
Validation loss: 2.4512427290646985

Epoch: 5| Step: 3
Training loss: 2.5893377439786756
Validation loss: 2.455065370868986

Epoch: 5| Step: 4
Training loss: 2.193752725713618
Validation loss: 2.45335630067496

Epoch: 5| Step: 5
Training loss: 2.193124787446667
Validation loss: 2.4515050856620317

Epoch: 5| Step: 6
Training loss: 2.4291103450294016
Validation loss: 2.4592775993658575

Epoch: 5| Step: 7
Training loss: 2.553084964508337
Validation loss: 2.458252567376173

Epoch: 5| Step: 8
Training loss: 2.290365011284929
Validation loss: 2.4573947049398774

Epoch: 5| Step: 9
Training loss: 2.3390346650306593
Validation loss: 2.459549063113637

Epoch: 5| Step: 10
Training loss: 2.8433787344572083
Validation loss: 2.4619704848727477

Epoch: 5| Step: 11
Training loss: 2.226697629877012
Validation loss: 2.45412050688445

Epoch: 166| Step: 0
Training loss: 2.6679473027139413
Validation loss: 2.4604209519256646

Epoch: 5| Step: 1
Training loss: 2.1079098734834476
Validation loss: 2.4735865316919607

Epoch: 5| Step: 2
Training loss: 2.5710994362224895
Validation loss: 2.4718522486742858

Epoch: 5| Step: 3
Training loss: 2.520327798474134
Validation loss: 2.460713141169721

Epoch: 5| Step: 4
Training loss: 2.6696892436773227
Validation loss: 2.472309188631741

Epoch: 5| Step: 5
Training loss: 2.589624087769225
Validation loss: 2.4733689111295436

Epoch: 5| Step: 6
Training loss: 2.580439875017025
Validation loss: 2.4678412909790906

Epoch: 5| Step: 7
Training loss: 2.8864674955672562
Validation loss: 2.4594233219322645

Epoch: 5| Step: 8
Training loss: 2.9522978416440924
Validation loss: 2.462192296233551

Epoch: 5| Step: 9
Training loss: 1.9381808038505894
Validation loss: 2.452658465136697

Epoch: 5| Step: 10
Training loss: 1.8783376868444908
Validation loss: 2.4662550499541007

Epoch: 5| Step: 11
Training loss: 1.0520980128910271
Validation loss: 2.4571779952224637

Epoch: 167| Step: 0
Training loss: 2.49777294145269
Validation loss: 2.46124521178501

Epoch: 5| Step: 1
Training loss: 2.202475594115808
Validation loss: 2.4600055726370713

Epoch: 5| Step: 2
Training loss: 2.4806187862165596
Validation loss: 2.4652143543365135

Epoch: 5| Step: 3
Training loss: 2.702917984108045
Validation loss: 2.4532661032945584

Epoch: 5| Step: 4
Training loss: 2.8966058665729184
Validation loss: 2.4546521610635725

Epoch: 5| Step: 5
Training loss: 2.309482358261816
Validation loss: 2.4548532812421415

Epoch: 5| Step: 6
Training loss: 2.888231430882893
Validation loss: 2.4563075396177894

Epoch: 5| Step: 7
Training loss: 2.6013242137024983
Validation loss: 2.458473598929124

Epoch: 5| Step: 8
Training loss: 2.1359171087133015
Validation loss: 2.461569508126939

Epoch: 5| Step: 9
Training loss: 2.0052347817266165
Validation loss: 2.4587718173857653

Epoch: 5| Step: 10
Training loss: 2.455597621486448
Validation loss: 2.4587182023888876

Epoch: 5| Step: 11
Training loss: 2.0946998149880303
Validation loss: 2.4675339468231576

Epoch: 168| Step: 0
Training loss: 2.6118488577186865
Validation loss: 2.4610895786020373

Epoch: 5| Step: 1
Training loss: 2.494688686297456
Validation loss: 2.4637989665235733

Epoch: 5| Step: 2
Training loss: 2.5923824151943493
Validation loss: 2.4784233050015425

Epoch: 5| Step: 3
Training loss: 1.9888632771224017
Validation loss: 2.501008565435546

Epoch: 5| Step: 4
Training loss: 2.2116416645529346
Validation loss: 2.4765827446881272

Epoch: 5| Step: 5
Training loss: 2.6866455494103616
Validation loss: 2.472584539079678

Epoch: 5| Step: 6
Training loss: 2.070007416900763
Validation loss: 2.4703016475123025

Epoch: 5| Step: 7
Training loss: 2.982896528464225
Validation loss: 2.4762993200398165

Epoch: 5| Step: 8
Training loss: 2.525221155886929
Validation loss: 2.4832633028354154

Epoch: 5| Step: 9
Training loss: 2.7853842878682853
Validation loss: 2.482493998042314

Epoch: 5| Step: 10
Training loss: 2.2721976868295255
Validation loss: 2.459470430693365

Epoch: 5| Step: 11
Training loss: 2.0347909434766964
Validation loss: 2.463702373357196

Epoch: 169| Step: 0
Training loss: 2.4180116418537736
Validation loss: 2.4580093765708484

Epoch: 5| Step: 1
Training loss: 1.7786358841877974
Validation loss: 2.4590246345227973

Epoch: 5| Step: 2
Training loss: 2.2976221439407536
Validation loss: 2.454574727576633

Epoch: 5| Step: 3
Training loss: 2.5875085397478057
Validation loss: 2.4540157502980398

Epoch: 5| Step: 4
Training loss: 2.8525017850504177
Validation loss: 2.457435162310802

Epoch: 5| Step: 5
Training loss: 2.5309095742460364
Validation loss: 2.4552213697339584

Epoch: 5| Step: 6
Training loss: 2.379105282429774
Validation loss: 2.464706828188015

Epoch: 5| Step: 7
Training loss: 2.658761127773254
Validation loss: 2.463470951272434

Epoch: 5| Step: 8
Training loss: 3.0184610106076564
Validation loss: 2.4669664455030476

Epoch: 5| Step: 9
Training loss: 2.400774087842443
Validation loss: 2.4594679749043964

Epoch: 5| Step: 10
Training loss: 2.6597520070169263
Validation loss: 2.456588341315354

Epoch: 5| Step: 11
Training loss: 1.0984348780029607
Validation loss: 2.4694791272415304

Epoch: 170| Step: 0
Training loss: 2.751276153620393
Validation loss: 2.4566729860039036

Epoch: 5| Step: 1
Training loss: 2.2806454210106737
Validation loss: 2.4502919070220113

Epoch: 5| Step: 2
Training loss: 2.132735589844978
Validation loss: 2.460035768435603

Epoch: 5| Step: 3
Training loss: 1.9675964124389005
Validation loss: 2.4580413650366717

Epoch: 5| Step: 4
Training loss: 2.7944975396891323
Validation loss: 2.4579954009360963

Epoch: 5| Step: 5
Training loss: 2.3004866251101723
Validation loss: 2.457491024566228

Epoch: 5| Step: 6
Training loss: 2.3725368375500993
Validation loss: 2.459834542938844

Epoch: 5| Step: 7
Training loss: 2.5890609455737073
Validation loss: 2.460644533995294

Epoch: 5| Step: 8
Training loss: 2.7224360066328797
Validation loss: 2.480158989255377

Epoch: 5| Step: 9
Training loss: 2.7908390488503874
Validation loss: 2.4868355288476582

Epoch: 5| Step: 10
Training loss: 2.6549221534595278
Validation loss: 2.482554566493397

Epoch: 5| Step: 11
Training loss: 0.9922841843483056
Validation loss: 2.4900927055800994

Epoch: 171| Step: 0
Training loss: 2.786962407707556
Validation loss: 2.493905893072479

Epoch: 5| Step: 1
Training loss: 2.969534037608038
Validation loss: 2.505795587771925

Epoch: 5| Step: 2
Training loss: 2.2174698266145136
Validation loss: 2.506497106475499

Epoch: 5| Step: 3
Training loss: 2.4775100002606654
Validation loss: 2.4990961964370637

Epoch: 5| Step: 4
Training loss: 1.9920105500267429
Validation loss: 2.514745744223703

Epoch: 5| Step: 5
Training loss: 2.3219053407640255
Validation loss: 2.5077465678547015

Epoch: 5| Step: 6
Training loss: 2.5368234004834624
Validation loss: 2.4838029854710637

Epoch: 5| Step: 7
Training loss: 2.3639694851101574
Validation loss: 2.485124299076733

Epoch: 5| Step: 8
Training loss: 2.6469947688761155
Validation loss: 2.4636575713255784

Epoch: 5| Step: 9
Training loss: 2.2669263588109723
Validation loss: 2.463703268502438

Epoch: 5| Step: 10
Training loss: 2.481034053754828
Validation loss: 2.463394141803831

Epoch: 5| Step: 11
Training loss: 2.5219089850838454
Validation loss: 2.46672161085134

Epoch: 172| Step: 0
Training loss: 2.2874056186904204
Validation loss: 2.4638873832883923

Epoch: 5| Step: 1
Training loss: 2.120602489366575
Validation loss: 2.4678720852802565

Epoch: 5| Step: 2
Training loss: 2.745185278526741
Validation loss: 2.4645128063215638

Epoch: 5| Step: 3
Training loss: 1.8190756594718134
Validation loss: 2.468400614597657

Epoch: 5| Step: 4
Training loss: 3.147369279275464
Validation loss: 2.4668823291820137

Epoch: 5| Step: 5
Training loss: 2.4458998684603634
Validation loss: 2.4655196184685066

Epoch: 5| Step: 6
Training loss: 2.827612814705881
Validation loss: 2.4724519192205974

Epoch: 5| Step: 7
Training loss: 2.622887806106165
Validation loss: 2.469033064086378

Epoch: 5| Step: 8
Training loss: 2.1268216347311997
Validation loss: 2.46913392666978

Epoch: 5| Step: 9
Training loss: 2.3016103083040584
Validation loss: 2.4775374746712244

Epoch: 5| Step: 10
Training loss: 2.3419287153710804
Validation loss: 2.4772054036324658

Epoch: 5| Step: 11
Training loss: 3.37907022949283
Validation loss: 2.46346354343885

Epoch: 173| Step: 0
Training loss: 1.793644862798822
Validation loss: 2.4718071482557518

Epoch: 5| Step: 1
Training loss: 2.9155811106167926
Validation loss: 2.472259304813868

Epoch: 5| Step: 2
Training loss: 2.6424952999423263
Validation loss: 2.469495354978543

Epoch: 5| Step: 3
Training loss: 1.9259941803556084
Validation loss: 2.4636392647979304

Epoch: 5| Step: 4
Training loss: 2.110716216759816
Validation loss: 2.4685098052374514

Epoch: 5| Step: 5
Training loss: 3.1291854295705646
Validation loss: 2.4570217324432906

Epoch: 5| Step: 6
Training loss: 2.3489631150923707
Validation loss: 2.472950380015702

Epoch: 5| Step: 7
Training loss: 2.367263943788129
Validation loss: 2.4568290364434655

Epoch: 5| Step: 8
Training loss: 2.597205376754593
Validation loss: 2.463172809711473

Epoch: 5| Step: 9
Training loss: 2.401396575536929
Validation loss: 2.453609471357687

Epoch: 5| Step: 10
Training loss: 2.7686873039225386
Validation loss: 2.460607758890304

Epoch: 5| Step: 11
Training loss: 2.1419321788661527
Validation loss: 2.4599754350231753

Epoch: 174| Step: 0
Training loss: 2.4384961171372526
Validation loss: 2.464021121274126

Epoch: 5| Step: 1
Training loss: 2.3279788592253676
Validation loss: 2.460470904162422

Epoch: 5| Step: 2
Training loss: 2.8057907612305812
Validation loss: 2.4582171102841572

Epoch: 5| Step: 3
Training loss: 2.838214895122206
Validation loss: 2.486810478095027

Epoch: 5| Step: 4
Training loss: 2.136402392635352
Validation loss: 2.4954126788982385

Epoch: 5| Step: 5
Training loss: 1.9897212301935807
Validation loss: 2.5064054881670366

Epoch: 5| Step: 6
Training loss: 2.2254485974654497
Validation loss: 2.5229866857338656

Epoch: 5| Step: 7
Training loss: 3.0990059643245194
Validation loss: 2.5290782622881527

Epoch: 5| Step: 8
Training loss: 2.442883342229562
Validation loss: 2.514188158693418

Epoch: 5| Step: 9
Training loss: 2.7176829907975764
Validation loss: 2.496859783656194

Epoch: 5| Step: 10
Training loss: 1.9730041197179367
Validation loss: 2.4787431416598604

Epoch: 5| Step: 11
Training loss: 2.3705631774838616
Validation loss: 2.4747710582860853

Epoch: 175| Step: 0
Training loss: 3.1727773200378224
Validation loss: 2.4771436737406947

Epoch: 5| Step: 1
Training loss: 2.6368129685133925
Validation loss: 2.4735632142542885

Epoch: 5| Step: 2
Training loss: 2.3087453518063237
Validation loss: 2.4720917188355487

Epoch: 5| Step: 3
Training loss: 1.9037064639628238
Validation loss: 2.4744725720197196

Epoch: 5| Step: 4
Training loss: 2.3482148431034453
Validation loss: 2.4774749671384777

Epoch: 5| Step: 5
Training loss: 2.5632641281025803
Validation loss: 2.4687195546651055

Epoch: 5| Step: 6
Training loss: 2.6200772402203065
Validation loss: 2.4739336166485635

Epoch: 5| Step: 7
Training loss: 2.429535692384969
Validation loss: 2.4800377038203325

Epoch: 5| Step: 8
Training loss: 2.5739620931719607
Validation loss: 2.4801822687152244

Epoch: 5| Step: 9
Training loss: 1.9416787399549689
Validation loss: 2.484990277664692

Epoch: 5| Step: 10
Training loss: 2.938141610707822
Validation loss: 2.480641825087004

Epoch: 5| Step: 11
Training loss: 2.4422176386071186
Validation loss: 2.479060741106406

Epoch: 176| Step: 0
Training loss: 2.453596324956957
Validation loss: 2.477299914397779

Epoch: 5| Step: 1
Training loss: 2.4253186753878464
Validation loss: 2.4777929215818992

Epoch: 5| Step: 2
Training loss: 2.898463822319829
Validation loss: 2.468788303609257

Epoch: 5| Step: 3
Training loss: 2.6386092578791165
Validation loss: 2.4752094652181715

Epoch: 5| Step: 4
Training loss: 2.1161379306453805
Validation loss: 2.4893785669133783

Epoch: 5| Step: 5
Training loss: 2.0131300040807294
Validation loss: 2.5142445300997545

Epoch: 5| Step: 6
Training loss: 2.7915841465231064
Validation loss: 2.5474307657167143

Epoch: 5| Step: 7
Training loss: 2.8635103932866977
Validation loss: 2.5932609909095863

Epoch: 5| Step: 8
Training loss: 2.683414103396115
Validation loss: 2.572779734181352

Epoch: 5| Step: 9
Training loss: 2.7664535919163376
Validation loss: 2.5353026051490626

Epoch: 5| Step: 10
Training loss: 2.4389190455691314
Validation loss: 2.4912685386132547

Epoch: 5| Step: 11
Training loss: 1.8557038810721054
Validation loss: 2.4853908733064465

Epoch: 177| Step: 0
Training loss: 2.786180818707736
Validation loss: 2.4584622807278365

Epoch: 5| Step: 1
Training loss: 2.7488523169154035
Validation loss: 2.4583429858993426

Epoch: 5| Step: 2
Training loss: 2.9736419527261004
Validation loss: 2.4680461322776504

Epoch: 5| Step: 3
Training loss: 2.5692508441284847
Validation loss: 2.4747864364550347

Epoch: 5| Step: 4
Training loss: 2.6885823576452985
Validation loss: 2.479851272902007

Epoch: 5| Step: 5
Training loss: 2.791964178221695
Validation loss: 2.4821367277603557

Epoch: 5| Step: 6
Training loss: 2.071266961957766
Validation loss: 2.4850748663804594

Epoch: 5| Step: 7
Training loss: 2.354257857545442
Validation loss: 2.4837557422742425

Epoch: 5| Step: 8
Training loss: 1.8864090627818204
Validation loss: 2.4863300907819457

Epoch: 5| Step: 9
Training loss: 2.654397295918159
Validation loss: 2.4838129843392562

Epoch: 5| Step: 10
Training loss: 2.116143000647223
Validation loss: 2.4782026978635447

Epoch: 5| Step: 11
Training loss: 2.9800554741426013
Validation loss: 2.482056609622455

Epoch: 178| Step: 0
Training loss: 2.829077628695793
Validation loss: 2.474874101432569

Epoch: 5| Step: 1
Training loss: 2.861791538038706
Validation loss: 2.4752655644624983

Epoch: 5| Step: 2
Training loss: 2.917254870004076
Validation loss: 2.4650770904152295

Epoch: 5| Step: 3
Training loss: 2.66572031474567
Validation loss: 2.4659669174072345

Epoch: 5| Step: 4
Training loss: 2.161182566749628
Validation loss: 2.460882587930319

Epoch: 5| Step: 5
Training loss: 2.84668136256383
Validation loss: 2.467656263481351

Epoch: 5| Step: 6
Training loss: 2.185857537683741
Validation loss: 2.4653021783621125

Epoch: 5| Step: 7
Training loss: 2.283036251186336
Validation loss: 2.4649121234194675

Epoch: 5| Step: 8
Training loss: 2.108317696534724
Validation loss: 2.4637952006061474

Epoch: 5| Step: 9
Training loss: 2.485412572925028
Validation loss: 2.4725962707412315

Epoch: 5| Step: 10
Training loss: 2.3852938380261635
Validation loss: 2.4623037995925143

Epoch: 5| Step: 11
Training loss: 2.22729351692193
Validation loss: 2.4645230809637053

Epoch: 179| Step: 0
Training loss: 2.639138971714814
Validation loss: 2.458527591002255

Epoch: 5| Step: 1
Training loss: 2.838359040617841
Validation loss: 2.469736843608123

Epoch: 5| Step: 2
Training loss: 2.6215735279214627
Validation loss: 2.468168987037725

Epoch: 5| Step: 3
Training loss: 2.147723713991594
Validation loss: 2.4796514811761265

Epoch: 5| Step: 4
Training loss: 2.5755085933588595
Validation loss: 2.482931617486526

Epoch: 5| Step: 5
Training loss: 2.4709256399923456
Validation loss: 2.493903530943045

Epoch: 5| Step: 6
Training loss: 2.5741360409405885
Validation loss: 2.495847065993428

Epoch: 5| Step: 7
Training loss: 2.3049262488889712
Validation loss: 2.497160765583906

Epoch: 5| Step: 8
Training loss: 2.494219869073202
Validation loss: 2.4806584483421203

Epoch: 5| Step: 9
Training loss: 2.289219561191285
Validation loss: 2.486005104339742

Epoch: 5| Step: 10
Training loss: 2.7326215080754386
Validation loss: 2.475746170666944

Epoch: 5| Step: 11
Training loss: 2.885653618035902
Validation loss: 2.4719927250558063

Epoch: 180| Step: 0
Training loss: 3.1109601298284626
Validation loss: 2.467189166644457

Epoch: 5| Step: 1
Training loss: 2.217026269277884
Validation loss: 2.4708905919137685

Epoch: 5| Step: 2
Training loss: 2.4427657347701275
Validation loss: 2.472221348318262

Epoch: 5| Step: 3
Training loss: 2.7157864258156477
Validation loss: 2.4657811872443895

Epoch: 5| Step: 4
Training loss: 2.0809410347015564
Validation loss: 2.472071654369706

Epoch: 5| Step: 5
Training loss: 2.6467210276743827
Validation loss: 2.476236111285694

Epoch: 5| Step: 6
Training loss: 2.946653996551109
Validation loss: 2.478750738280714

Epoch: 5| Step: 7
Training loss: 1.8542439162488966
Validation loss: 2.4812341305203622

Epoch: 5| Step: 8
Training loss: 2.291280979877317
Validation loss: 2.4799900564223827

Epoch: 5| Step: 9
Training loss: 2.7151266924837585
Validation loss: 2.4756003234565673

Epoch: 5| Step: 10
Training loss: 1.7101184852999656
Validation loss: 2.4783188120740007

Epoch: 5| Step: 11
Training loss: 3.147045196809884
Validation loss: 2.4786066590192837

Epoch: 181| Step: 0
Training loss: 2.3732751304833375
Validation loss: 2.4843605768836707

Epoch: 5| Step: 1
Training loss: 2.7642547316309316
Validation loss: 2.4847639087354634

Epoch: 5| Step: 2
Training loss: 2.3165914065900077
Validation loss: 2.48326773930227

Epoch: 5| Step: 3
Training loss: 1.7966795441844845
Validation loss: 2.4931082066177317

Epoch: 5| Step: 4
Training loss: 2.7697631566671475
Validation loss: 2.496822475665481

Epoch: 5| Step: 5
Training loss: 2.1535585717109136
Validation loss: 2.494687909787238

Epoch: 5| Step: 6
Training loss: 2.790327538573558
Validation loss: 2.4852027553734843

Epoch: 5| Step: 7
Training loss: 2.6603660556775672
Validation loss: 2.4732605134028534

Epoch: 5| Step: 8
Training loss: 2.446239259254762
Validation loss: 2.4718687260769565

Epoch: 5| Step: 9
Training loss: 2.2573010350414213
Validation loss: 2.465556050357088

Epoch: 5| Step: 10
Training loss: 2.691039659755764
Validation loss: 2.474670766677608

Epoch: 5| Step: 11
Training loss: 1.376105081202527
Validation loss: 2.468677346653371

Epoch: 182| Step: 0
Training loss: 2.102461271711765
Validation loss: 2.4667492899932513

Epoch: 5| Step: 1
Training loss: 2.1931040233914514
Validation loss: 2.471488825697208

Epoch: 5| Step: 2
Training loss: 2.5965397550675005
Validation loss: 2.4713062459937727

Epoch: 5| Step: 3
Training loss: 3.0689197128200263
Validation loss: 2.4625998720419617

Epoch: 5| Step: 4
Training loss: 1.8925199863885203
Validation loss: 2.463503856785473

Epoch: 5| Step: 5
Training loss: 2.2083877580761837
Validation loss: 2.472051256199842

Epoch: 5| Step: 6
Training loss: 2.4777589428997877
Validation loss: 2.4703703805768673

Epoch: 5| Step: 7
Training loss: 3.2230899033513554
Validation loss: 2.4753303792190837

Epoch: 5| Step: 8
Training loss: 2.0632440785385793
Validation loss: 2.478222151491908

Epoch: 5| Step: 9
Training loss: 2.230905146088355
Validation loss: 2.4774893982625468

Epoch: 5| Step: 10
Training loss: 2.529399336576914
Validation loss: 2.471417472813713

Epoch: 5| Step: 11
Training loss: 2.4254515787834126
Validation loss: 2.4765647182193935

Epoch: 183| Step: 0
Training loss: 2.1719702830099497
Validation loss: 2.495397169077647

Epoch: 5| Step: 1
Training loss: 2.515634098391932
Validation loss: 2.495782963165394

Epoch: 5| Step: 2
Training loss: 2.2590300237273975
Validation loss: 2.500684755643125

Epoch: 5| Step: 3
Training loss: 2.713368669634393
Validation loss: 2.501607489355059

Epoch: 5| Step: 4
Training loss: 2.5158352971287794
Validation loss: 2.5174487948584665

Epoch: 5| Step: 5
Training loss: 3.2905028172892328
Validation loss: 2.507459096676375

Epoch: 5| Step: 6
Training loss: 2.396895717645703
Validation loss: 2.51019882639365

Epoch: 5| Step: 7
Training loss: 2.651871786697746
Validation loss: 2.505036034213831

Epoch: 5| Step: 8
Training loss: 2.032621184955712
Validation loss: 2.491780474040149

Epoch: 5| Step: 9
Training loss: 2.042720159573893
Validation loss: 2.4765611040851496

Epoch: 5| Step: 10
Training loss: 2.341961597951969
Validation loss: 2.4755782970583935

Epoch: 5| Step: 11
Training loss: 2.066634571174872
Validation loss: 2.4788911343111764

Epoch: 184| Step: 0
Training loss: 2.537505532244585
Validation loss: 2.472434352823541

Epoch: 5| Step: 1
Training loss: 2.5485899598378374
Validation loss: 2.4717525560360603

Epoch: 5| Step: 2
Training loss: 2.5913368857539054
Validation loss: 2.489687515749731

Epoch: 5| Step: 3
Training loss: 2.2171543454896487
Validation loss: 2.4861831762629993

Epoch: 5| Step: 4
Training loss: 2.2564078320699172
Validation loss: 2.4939909919735674

Epoch: 5| Step: 5
Training loss: 2.406680576393459
Validation loss: 2.4970686018282375

Epoch: 5| Step: 6
Training loss: 2.874995190160294
Validation loss: 2.500823529025211

Epoch: 5| Step: 7
Training loss: 2.318534298141068
Validation loss: 2.496115368514151

Epoch: 5| Step: 8
Training loss: 2.4813791121322475
Validation loss: 2.4966461135615003

Epoch: 5| Step: 9
Training loss: 2.468441135062383
Validation loss: 2.4912109215493516

Epoch: 5| Step: 10
Training loss: 2.323329826071385
Validation loss: 2.4812907141124505

Epoch: 5| Step: 11
Training loss: 1.8697686012425128
Validation loss: 2.484458817961421

Epoch: 185| Step: 0
Training loss: 2.117101913918179
Validation loss: 2.474268176435598

Epoch: 5| Step: 1
Training loss: 2.5512196746773284
Validation loss: 2.4687148425661074

Epoch: 5| Step: 2
Training loss: 2.805171659116314
Validation loss: 2.4707861442841508

Epoch: 5| Step: 3
Training loss: 2.877719588000305
Validation loss: 2.4605564549040535

Epoch: 5| Step: 4
Training loss: 2.1596607378037156
Validation loss: 2.467474990468051

Epoch: 5| Step: 5
Training loss: 2.501453263365616
Validation loss: 2.4642486337474323

Epoch: 5| Step: 6
Training loss: 2.537399357646152
Validation loss: 2.4673937923296103

Epoch: 5| Step: 7
Training loss: 2.78984293984407
Validation loss: 2.4589403677252855

Epoch: 5| Step: 8
Training loss: 2.656396211078218
Validation loss: 2.4598169248030515

Epoch: 5| Step: 9
Training loss: 1.9544923192437125
Validation loss: 2.4658011921056335

Epoch: 5| Step: 10
Training loss: 1.9041727974401341
Validation loss: 2.4678782642211976

Epoch: 5| Step: 11
Training loss: 2.128281471897128
Validation loss: 2.46962128728213

Epoch: 186| Step: 0
Training loss: 2.5301936264730944
Validation loss: 2.477202949384316

Epoch: 5| Step: 1
Training loss: 2.3650719786128658
Validation loss: 2.4776671239554906

Epoch: 5| Step: 2
Training loss: 2.7084825816968485
Validation loss: 2.4716904628784553

Epoch: 5| Step: 3
Training loss: 2.075637475871906
Validation loss: 2.483815432056057

Epoch: 5| Step: 4
Training loss: 2.2261129360425795
Validation loss: 2.477471819466184

Epoch: 5| Step: 5
Training loss: 2.222407590234372
Validation loss: 2.505232056778361

Epoch: 5| Step: 6
Training loss: 2.097395498880253
Validation loss: 2.516697495052746

Epoch: 5| Step: 7
Training loss: 2.4627557728186353
Validation loss: 2.5149039939228977

Epoch: 5| Step: 8
Training loss: 2.9095427205616757
Validation loss: 2.5023719184966144

Epoch: 5| Step: 9
Training loss: 2.763720012555594
Validation loss: 2.48654081199552

Epoch: 5| Step: 10
Training loss: 2.329307409502895
Validation loss: 2.4892492362755503

Epoch: 5| Step: 11
Training loss: 2.938847983287341
Validation loss: 2.4917884993563146

Epoch: 187| Step: 0
Training loss: 2.14267715878975
Validation loss: 2.4745717556368874

Epoch: 5| Step: 1
Training loss: 2.1899520210340575
Validation loss: 2.4726925807571853

Epoch: 5| Step: 2
Training loss: 2.537967950677185
Validation loss: 2.4785373968356046

Epoch: 5| Step: 3
Training loss: 3.2126248049705723
Validation loss: 2.4730354568715156

Epoch: 5| Step: 4
Training loss: 2.2510278261516197
Validation loss: 2.471517862272248

Epoch: 5| Step: 5
Training loss: 2.274256712669722
Validation loss: 2.4807372941556705

Epoch: 5| Step: 6
Training loss: 2.3512219153860925
Validation loss: 2.4805089151329502

Epoch: 5| Step: 7
Training loss: 2.1510319251578056
Validation loss: 2.473058568396284

Epoch: 5| Step: 8
Training loss: 2.694602187172169
Validation loss: 2.4794057215242598

Epoch: 5| Step: 9
Training loss: 2.745453804734858
Validation loss: 2.4802392688238766

Epoch: 5| Step: 10
Training loss: 2.466713173745736
Validation loss: 2.473526920381852

Epoch: 5| Step: 11
Training loss: 3.2968463625274307
Validation loss: 2.4821390530608385

Epoch: 188| Step: 0
Training loss: 2.7394673911582226
Validation loss: 2.479949844831215

Epoch: 5| Step: 1
Training loss: 2.2629476578576377
Validation loss: 2.4893838744021695

Epoch: 5| Step: 2
Training loss: 2.2645258901189544
Validation loss: 2.4984124348400822

Epoch: 5| Step: 3
Training loss: 1.9608762207183084
Validation loss: 2.4973738785071378

Epoch: 5| Step: 4
Training loss: 2.774559116566234
Validation loss: 2.4961493560057653

Epoch: 5| Step: 5
Training loss: 2.3630377021329263
Validation loss: 2.5173782730979104

Epoch: 5| Step: 6
Training loss: 2.426696408145425
Validation loss: 2.522084380214274

Epoch: 5| Step: 7
Training loss: 2.8366936970673176
Validation loss: 2.5170551819361884

Epoch: 5| Step: 8
Training loss: 2.7065699681107547
Validation loss: 2.4984486972286755

Epoch: 5| Step: 9
Training loss: 2.571026734713502
Validation loss: 2.4947309360532017

Epoch: 5| Step: 10
Training loss: 2.259599658143404
Validation loss: 2.486325667770382

Epoch: 5| Step: 11
Training loss: 1.6874654554434054
Validation loss: 2.488363303563794

Epoch: 189| Step: 0
Training loss: 2.401515712542879
Validation loss: 2.493020897916313

Epoch: 5| Step: 1
Training loss: 2.623391794121238
Validation loss: 2.481517123649875

Epoch: 5| Step: 2
Training loss: 2.3842426948287456
Validation loss: 2.4672291815572582

Epoch: 5| Step: 3
Training loss: 2.2679693260809746
Validation loss: 2.4670397230444103

Epoch: 5| Step: 4
Training loss: 2.637945768462622
Validation loss: 2.472363728379832

Epoch: 5| Step: 5
Training loss: 2.0594907829372926
Validation loss: 2.4712810699832906

Epoch: 5| Step: 6
Training loss: 2.2580652854408325
Validation loss: 2.466376995033598

Epoch: 5| Step: 7
Training loss: 3.0925469419452254
Validation loss: 2.468359457584361

Epoch: 5| Step: 8
Training loss: 1.8194820490761652
Validation loss: 2.4667925941589215

Epoch: 5| Step: 9
Training loss: 2.2055191126176683
Validation loss: 2.478888669709548

Epoch: 5| Step: 10
Training loss: 2.8022189101612116
Validation loss: 2.4762564107948957

Epoch: 5| Step: 11
Training loss: 3.7369397665288306
Validation loss: 2.4811715679760473

Epoch: 190| Step: 0
Training loss: 2.661548043149406
Validation loss: 2.5012941626774676

Epoch: 5| Step: 1
Training loss: 2.339951043065143
Validation loss: 2.5049360303077584

Epoch: 5| Step: 2
Training loss: 2.7941047118982936
Validation loss: 2.5233598151910743

Epoch: 5| Step: 3
Training loss: 2.4530896226798187
Validation loss: 2.543223323863485

Epoch: 5| Step: 4
Training loss: 2.809103207282103
Validation loss: 2.555956359170592

Epoch: 5| Step: 5
Training loss: 2.6621185101067586
Validation loss: 2.5561615466146814

Epoch: 5| Step: 6
Training loss: 2.197505846243623
Validation loss: 2.576155119969907

Epoch: 5| Step: 7
Training loss: 2.6332990157229155
Validation loss: 2.5641206330646966

Epoch: 5| Step: 8
Training loss: 2.6152053842796614
Validation loss: 2.5522376591219698

Epoch: 5| Step: 9
Training loss: 2.3178468698538013
Validation loss: 2.540954014399933

Epoch: 5| Step: 10
Training loss: 2.61054765312278
Validation loss: 2.534019548456149

Epoch: 5| Step: 11
Training loss: 1.5738845356829665
Validation loss: 2.5109053442145646

Epoch: 191| Step: 0
Training loss: 2.5333127694382847
Validation loss: 2.5037056678418734

Epoch: 5| Step: 1
Training loss: 2.834157674211165
Validation loss: 2.5171587397177295

Epoch: 5| Step: 2
Training loss: 2.74083500300034
Validation loss: 2.496836759157451

Epoch: 5| Step: 3
Training loss: 2.2962598950168176
Validation loss: 2.4909607350271523

Epoch: 5| Step: 4
Training loss: 2.638517001087326
Validation loss: 2.5019625510465775

Epoch: 5| Step: 5
Training loss: 2.296637698001439
Validation loss: 2.5030751269797675

Epoch: 5| Step: 6
Training loss: 2.55978528880695
Validation loss: 2.495661431943151

Epoch: 5| Step: 7
Training loss: 2.5993259656628376
Validation loss: 2.499460698609519

Epoch: 5| Step: 8
Training loss: 2.356911491004323
Validation loss: 2.4983438907285285

Epoch: 5| Step: 9
Training loss: 1.6461595461477272
Validation loss: 2.496736983875996

Epoch: 5| Step: 10
Training loss: 2.4165176696447057
Validation loss: 2.502071781804092

Epoch: 5| Step: 11
Training loss: 3.0746747348852685
Validation loss: 2.5013657177375483

Epoch: 192| Step: 0
Training loss: 2.3483127177596783
Validation loss: 2.508397577789523

Epoch: 5| Step: 1
Training loss: 2.4821109652063122
Validation loss: 2.5191783214273835

Epoch: 5| Step: 2
Training loss: 2.297527298458999
Validation loss: 2.5116407383283104

Epoch: 5| Step: 3
Training loss: 2.795255309234727
Validation loss: 2.532436685294738

Epoch: 5| Step: 4
Training loss: 2.536229451614698
Validation loss: 2.5209367168654326

Epoch: 5| Step: 5
Training loss: 2.2863301188350906
Validation loss: 2.5151391478319236

Epoch: 5| Step: 6
Training loss: 1.8914908956237508
Validation loss: 2.5087391813849873

Epoch: 5| Step: 7
Training loss: 2.499120366794495
Validation loss: 2.5039034489830723

Epoch: 5| Step: 8
Training loss: 2.0065057561788495
Validation loss: 2.5050155792843234

Epoch: 5| Step: 9
Training loss: 2.617484278007735
Validation loss: 2.502482457266188

Epoch: 5| Step: 10
Training loss: 2.863437788822901
Validation loss: 2.492701099851267

Epoch: 5| Step: 11
Training loss: 3.8680619917579553
Validation loss: 2.4892229287521888

Epoch: 193| Step: 0
Training loss: 2.7287991594193057
Validation loss: 2.489422467087516

Epoch: 5| Step: 1
Training loss: 2.359068086379776
Validation loss: 2.4998429129838926

Epoch: 5| Step: 2
Training loss: 2.4400138607569497
Validation loss: 2.506908317128313

Epoch: 5| Step: 3
Training loss: 2.3732958250975145
Validation loss: 2.5078882144577994

Epoch: 5| Step: 4
Training loss: 2.3969607699688735
Validation loss: 2.5116490324241316

Epoch: 5| Step: 5
Training loss: 2.34595467189827
Validation loss: 2.5062352862603983

Epoch: 5| Step: 6
Training loss: 2.084021670750502
Validation loss: 2.5183447259360823

Epoch: 5| Step: 7
Training loss: 2.3774452671686404
Validation loss: 2.519589193303762

Epoch: 5| Step: 8
Training loss: 2.193140550603387
Validation loss: 2.526293816058293

Epoch: 5| Step: 9
Training loss: 2.423537735676917
Validation loss: 2.5112299149277937

Epoch: 5| Step: 10
Training loss: 2.8780226367221906
Validation loss: 2.4963325083829253

Epoch: 5| Step: 11
Training loss: 3.1483345109551326
Validation loss: 2.4966515408856926

Epoch: 194| Step: 0
Training loss: 2.8506953344887074
Validation loss: 2.507557357849623

Epoch: 5| Step: 1
Training loss: 2.5282293121357546
Validation loss: 2.4991155808398062

Epoch: 5| Step: 2
Training loss: 2.136778333764184
Validation loss: 2.5031651328969624

Epoch: 5| Step: 3
Training loss: 2.4935277127503817
Validation loss: 2.5044250268257686

Epoch: 5| Step: 4
Training loss: 2.0245050979365415
Validation loss: 2.5024719295966213

Epoch: 5| Step: 5
Training loss: 2.1107886204713346
Validation loss: 2.519259376544518

Epoch: 5| Step: 6
Training loss: 2.4393188341864813
Validation loss: 2.5158802321583

Epoch: 5| Step: 7
Training loss: 2.852162922393541
Validation loss: 2.512172968379879

Epoch: 5| Step: 8
Training loss: 2.6319096769006496
Validation loss: 2.508408044946514

Epoch: 5| Step: 9
Training loss: 2.578056380774791
Validation loss: 2.5328822250072394

Epoch: 5| Step: 10
Training loss: 1.9693134046659813
Validation loss: 2.528817428649055

Epoch: 5| Step: 11
Training loss: 2.6451936283881072
Validation loss: 2.53928467781059

Epoch: 195| Step: 0
Training loss: 2.31955796634633
Validation loss: 2.5410896112615937

Epoch: 5| Step: 1
Training loss: 2.1699741745216277
Validation loss: 2.545780773733687

Epoch: 5| Step: 2
Training loss: 2.580769055799561
Validation loss: 2.565126608090417

Epoch: 5| Step: 3
Training loss: 2.4627782325601744
Validation loss: 2.5560094794057546

Epoch: 5| Step: 4
Training loss: 2.971640213958995
Validation loss: 2.5397313337461505

Epoch: 5| Step: 5
Training loss: 2.2303210207891433
Validation loss: 2.5497451282423134

Epoch: 5| Step: 6
Training loss: 2.2143919145808595
Validation loss: 2.5386050668113835

Epoch: 5| Step: 7
Training loss: 2.777312674261361
Validation loss: 2.53809992223852

Epoch: 5| Step: 8
Training loss: 2.0020548278282417
Validation loss: 2.5254114970804826

Epoch: 5| Step: 9
Training loss: 2.4638458988874192
Validation loss: 2.518879572057393

Epoch: 5| Step: 10
Training loss: 2.5162611447280305
Validation loss: 2.5055029346246127

Epoch: 5| Step: 11
Training loss: 1.8991783172308092
Validation loss: 2.497515079222385

Epoch: 196| Step: 0
Training loss: 2.8637671585268305
Validation loss: 2.5033037925870185

Epoch: 5| Step: 1
Training loss: 2.14157473246845
Validation loss: 2.4973491204235123

Epoch: 5| Step: 2
Training loss: 2.592874310214075
Validation loss: 2.4925355179965285

Epoch: 5| Step: 3
Training loss: 2.277863798093735
Validation loss: 2.4943244485966627

Epoch: 5| Step: 4
Training loss: 2.1597479491287563
Validation loss: 2.507480167539951

Epoch: 5| Step: 5
Training loss: 2.1508019215968783
Validation loss: 2.501820906461626

Epoch: 5| Step: 6
Training loss: 2.6822185518737784
Validation loss: 2.5145618648502808

Epoch: 5| Step: 7
Training loss: 2.1544634286888837
Validation loss: 2.5162721911059975

Epoch: 5| Step: 8
Training loss: 2.9219748740033067
Validation loss: 2.528094263286808

Epoch: 5| Step: 9
Training loss: 2.1545196445691155
Validation loss: 2.520599718276858

Epoch: 5| Step: 10
Training loss: 2.4223094919603705
Validation loss: 2.512782945722034

Epoch: 5| Step: 11
Training loss: 2.6575544856632383
Validation loss: 2.498243567244395

Epoch: 197| Step: 0
Training loss: 2.688782851525249
Validation loss: 2.4995097116674785

Epoch: 5| Step: 1
Training loss: 2.5895780539260618
Validation loss: 2.5039826895739563

Epoch: 5| Step: 2
Training loss: 2.4794183384285993
Validation loss: 2.501296990445837

Epoch: 5| Step: 3
Training loss: 1.852612290370383
Validation loss: 2.5191288392594786

Epoch: 5| Step: 4
Training loss: 2.059853676719448
Validation loss: 2.5154784892443978

Epoch: 5| Step: 5
Training loss: 2.6591424788814804
Validation loss: 2.5306095325565385

Epoch: 5| Step: 6
Training loss: 2.047494343247943
Validation loss: 2.5262160185325193

Epoch: 5| Step: 7
Training loss: 2.6329687960724693
Validation loss: 2.5336895987587

Epoch: 5| Step: 8
Training loss: 2.8340365341297606
Validation loss: 2.5516414506311205

Epoch: 5| Step: 9
Training loss: 2.2732020515092985
Validation loss: 2.545527462500625

Epoch: 5| Step: 10
Training loss: 2.6228588090796925
Validation loss: 2.545721065701331

Epoch: 5| Step: 11
Training loss: 1.1680835680198791
Validation loss: 2.5360001314004728

Epoch: 198| Step: 0
Training loss: 1.8836456608116041
Validation loss: 2.516229623989046

Epoch: 5| Step: 1
Training loss: 1.898586314455764
Validation loss: 2.5004892109007133

Epoch: 5| Step: 2
Training loss: 2.280539624118128
Validation loss: 2.4942388114432275

Epoch: 5| Step: 3
Training loss: 2.012543089032178
Validation loss: 2.4974879637992355

Epoch: 5| Step: 4
Training loss: 2.423549048920445
Validation loss: 2.497666951175373

Epoch: 5| Step: 5
Training loss: 2.2799984359735848
Validation loss: 2.4846547447108263

Epoch: 5| Step: 6
Training loss: 2.5068238588346037
Validation loss: 2.4949620864946027

Epoch: 5| Step: 7
Training loss: 2.6490587614230394
Validation loss: 2.5046854180580986

Epoch: 5| Step: 8
Training loss: 3.1616445839498986
Validation loss: 2.5017313922240767

Epoch: 5| Step: 9
Training loss: 2.4063324356820743
Validation loss: 2.4914877535994755

Epoch: 5| Step: 10
Training loss: 2.4539158026343237
Validation loss: 2.509755828482066

Epoch: 5| Step: 11
Training loss: 3.6402888981960073
Validation loss: 2.520133852790862

Epoch: 199| Step: 0
Training loss: 2.281880017381897
Validation loss: 2.4992752733404333

Epoch: 5| Step: 1
Training loss: 2.5671859323284356
Validation loss: 2.500205071303609

Epoch: 5| Step: 2
Training loss: 2.14321419829236
Validation loss: 2.4919557812471047

Epoch: 5| Step: 3
Training loss: 2.3370866787481233
Validation loss: 2.4906494034130415

Epoch: 5| Step: 4
Training loss: 2.612818290631945
Validation loss: 2.4907084892571514

Epoch: 5| Step: 5
Training loss: 2.664311103743656
Validation loss: 2.4878321331661275

Epoch: 5| Step: 6
Training loss: 2.4489082979266605
Validation loss: 2.4907590944064846

Epoch: 5| Step: 7
Training loss: 2.127138968651453
Validation loss: 2.49466192642134

Epoch: 5| Step: 8
Training loss: 2.3450682937990925
Validation loss: 2.5011825824226164

Epoch: 5| Step: 9
Training loss: 2.639292544450666
Validation loss: 2.503557138212394

Epoch: 5| Step: 10
Training loss: 2.630407938921334
Validation loss: 2.5183798985833055

Epoch: 5| Step: 11
Training loss: 1.1896088596863212
Validation loss: 2.501636664780155

Epoch: 200| Step: 0
Training loss: 2.1048631805753195
Validation loss: 2.542015142992957

Epoch: 5| Step: 1
Training loss: 2.859914426523733
Validation loss: 2.5712880599284

Epoch: 5| Step: 2
Training loss: 2.2612690058426637
Validation loss: 2.573381705606707

Epoch: 5| Step: 3
Training loss: 2.5699938544708276
Validation loss: 2.5513383102272096

Epoch: 5| Step: 4
Training loss: 1.977043123576643
Validation loss: 2.532582658257892

Epoch: 5| Step: 5
Training loss: 2.523008705768239
Validation loss: 2.515724628363258

Epoch: 5| Step: 6
Training loss: 2.2150762526774965
Validation loss: 2.51108325455297

Epoch: 5| Step: 7
Training loss: 2.9148502870198576
Validation loss: 2.498244098098641

Epoch: 5| Step: 8
Training loss: 2.7088780564482837
Validation loss: 2.4815521917804375

Epoch: 5| Step: 9
Training loss: 2.400309773162854
Validation loss: 2.4747551943061046

Epoch: 5| Step: 10
Training loss: 2.4347978922411633
Validation loss: 2.488584902134828

Epoch: 5| Step: 11
Training loss: 3.185864795792551
Validation loss: 2.4881586017653823

Epoch: 201| Step: 0
Training loss: 2.9223443128692046
Validation loss: 2.474141009094818

Epoch: 5| Step: 1
Training loss: 2.616016638454989
Validation loss: 2.482667556158508

Epoch: 5| Step: 2
Training loss: 2.2456150772210997
Validation loss: 2.478372203463937

Epoch: 5| Step: 3
Training loss: 2.846139763065637
Validation loss: 2.480901713393665

Epoch: 5| Step: 4
Training loss: 2.2001667046242672
Validation loss: 2.494309244652028

Epoch: 5| Step: 5
Training loss: 2.330200294431878
Validation loss: 2.4830868658438456

Epoch: 5| Step: 6
Training loss: 2.6821621959265567
Validation loss: 2.488131879458275

Epoch: 5| Step: 7
Training loss: 2.6380803414373903
Validation loss: 2.5143205210382766

Epoch: 5| Step: 8
Training loss: 2.375100786178733
Validation loss: 2.5298174623057346

Epoch: 5| Step: 9
Training loss: 2.0962112536002806
Validation loss: 2.5337113885489146

Epoch: 5| Step: 10
Training loss: 2.182830404154941
Validation loss: 2.5181068317057416

Epoch: 5| Step: 11
Training loss: 1.6775324992003622
Validation loss: 2.528720138956166

Epoch: 202| Step: 0
Training loss: 2.3608379754549227
Validation loss: 2.5248926807170213

Epoch: 5| Step: 1
Training loss: 2.4344124434876115
Validation loss: 2.5012285591425347

Epoch: 5| Step: 2
Training loss: 2.027052783115099
Validation loss: 2.4907148747838916

Epoch: 5| Step: 3
Training loss: 2.34996632490988
Validation loss: 2.476628171288739

Epoch: 5| Step: 4
Training loss: 1.950420001844154
Validation loss: 2.4877673526202555

Epoch: 5| Step: 5
Training loss: 2.395911063439871
Validation loss: 2.4789516745772646

Epoch: 5| Step: 6
Training loss: 2.2945153188753102
Validation loss: 2.483028918901517

Epoch: 5| Step: 7
Training loss: 2.6571976710231073
Validation loss: 2.483204947956963

Epoch: 5| Step: 8
Training loss: 2.3887694893305924
Validation loss: 2.4811651859153043

Epoch: 5| Step: 9
Training loss: 2.104448570277408
Validation loss: 2.474079985722833

Epoch: 5| Step: 10
Training loss: 3.486806525737384
Validation loss: 2.4806771658899303

Epoch: 5| Step: 11
Training loss: 3.100077314335628
Validation loss: 2.4860250523540905

Epoch: 203| Step: 0
Training loss: 2.4121341843043473
Validation loss: 2.474795175194444

Epoch: 5| Step: 1
Training loss: 2.1258500586788482
Validation loss: 2.479278400985994

Epoch: 5| Step: 2
Training loss: 2.343328107373129
Validation loss: 2.481281101438658

Epoch: 5| Step: 3
Training loss: 2.7606420077110267
Validation loss: 2.495694599663469

Epoch: 5| Step: 4
Training loss: 2.8430674645586986
Validation loss: 2.5000890040448773

Epoch: 5| Step: 5
Training loss: 2.1617058547131305
Validation loss: 2.5005581908139263

Epoch: 5| Step: 6
Training loss: 2.1118991019755295
Validation loss: 2.4814268209325623

Epoch: 5| Step: 7
Training loss: 2.932423201886272
Validation loss: 2.5060466082925354

Epoch: 5| Step: 8
Training loss: 2.3588573570058626
Validation loss: 2.4903726895883955

Epoch: 5| Step: 9
Training loss: 2.925443214680356
Validation loss: 2.4885241730752297

Epoch: 5| Step: 10
Training loss: 1.9259333985678202
Validation loss: 2.5002233763082944

Epoch: 5| Step: 11
Training loss: 1.4826479049674506
Validation loss: 2.502925833605973

Epoch: 204| Step: 0
Training loss: 2.4170967464757656
Validation loss: 2.499748233197736

Epoch: 5| Step: 1
Training loss: 2.526192404323089
Validation loss: 2.5126088979158405

Epoch: 5| Step: 2
Training loss: 2.121294830513922
Validation loss: 2.500535228179029

Epoch: 5| Step: 3
Training loss: 1.7892879693827737
Validation loss: 2.5054568936026094

Epoch: 5| Step: 4
Training loss: 2.2309269476214992
Validation loss: 2.5091109573557135

Epoch: 5| Step: 5
Training loss: 2.599523805145721
Validation loss: 2.5011347380933477

Epoch: 5| Step: 6
Training loss: 2.1867726887997456
Validation loss: 2.5119124951347955

Epoch: 5| Step: 7
Training loss: 2.773134456787767
Validation loss: 2.5137901841474815

Epoch: 5| Step: 8
Training loss: 2.2086593699144266
Validation loss: 2.5138966999026278

Epoch: 5| Step: 9
Training loss: 2.499914835432944
Validation loss: 2.5167517853441264

Epoch: 5| Step: 10
Training loss: 2.9059649912227785
Validation loss: 2.503568851712429

Epoch: 5| Step: 11
Training loss: 2.3933120287032863
Validation loss: 2.510307077466643

Epoch: 205| Step: 0
Training loss: 2.341230131391873
Validation loss: 2.507734525266513

Epoch: 5| Step: 1
Training loss: 1.77161619795756
Validation loss: 2.4997180104165144

Epoch: 5| Step: 2
Training loss: 2.693777869272887
Validation loss: 2.4959109283659298

Epoch: 5| Step: 3
Training loss: 2.1520583834191234
Validation loss: 2.494972661778951

Epoch: 5| Step: 4
Training loss: 1.9197468333426229
Validation loss: 2.5034860346626635

Epoch: 5| Step: 5
Training loss: 2.7221389344724045
Validation loss: 2.5119893989668327

Epoch: 5| Step: 6
Training loss: 2.682216151881238
Validation loss: 2.507614813249204

Epoch: 5| Step: 7
Training loss: 2.393152334635478
Validation loss: 2.501139007817903

Epoch: 5| Step: 8
Training loss: 2.3118070646783817
Validation loss: 2.51262918032654

Epoch: 5| Step: 9
Training loss: 2.8118673672713457
Validation loss: 2.503139709797728

Epoch: 5| Step: 10
Training loss: 2.6327842461855644
Validation loss: 2.5026986299139997

Epoch: 5| Step: 11
Training loss: 1.960483209640726
Validation loss: 2.5087199881570035

Epoch: 206| Step: 0
Training loss: 2.1027395362797745
Validation loss: 2.5138732426392396

Epoch: 5| Step: 1
Training loss: 2.395130076370207
Validation loss: 2.5366487111756526

Epoch: 5| Step: 2
Training loss: 2.927739586803233
Validation loss: 2.535422614337984

Epoch: 5| Step: 3
Training loss: 2.6695725185206385
Validation loss: 2.555250850395815

Epoch: 5| Step: 4
Training loss: 2.4071650561044464
Validation loss: 2.5600486210462625

Epoch: 5| Step: 5
Training loss: 1.9581519171371176
Validation loss: 2.5598075142594916

Epoch: 5| Step: 6
Training loss: 2.4775830401410723
Validation loss: 2.5627427567656964

Epoch: 5| Step: 7
Training loss: 2.163763032429904
Validation loss: 2.5433160532001438

Epoch: 5| Step: 8
Training loss: 2.274991268099282
Validation loss: 2.5394300576244824

Epoch: 5| Step: 9
Training loss: 1.77934404095005
Validation loss: 2.5254851695037956

Epoch: 5| Step: 10
Training loss: 2.8126769963837526
Validation loss: 2.503816552422728

Epoch: 5| Step: 11
Training loss: 3.8508494492797385
Validation loss: 2.510047661238881

Epoch: 207| Step: 0
Training loss: 2.8192148370900108
Validation loss: 2.482574010029417

Epoch: 5| Step: 1
Training loss: 2.583344787654299
Validation loss: 2.4835652505240033

Epoch: 5| Step: 2
Training loss: 2.422851956116453
Validation loss: 2.482133023684261

Epoch: 5| Step: 3
Training loss: 2.0007473026301783
Validation loss: 2.4890185890068115

Epoch: 5| Step: 4
Training loss: 2.3467809787817493
Validation loss: 2.4853835348035074

Epoch: 5| Step: 5
Training loss: 2.082318313018492
Validation loss: 2.479209544241125

Epoch: 5| Step: 6
Training loss: 1.8547265425495856
Validation loss: 2.4941706843545073

Epoch: 5| Step: 7
Training loss: 2.7133611129663353
Validation loss: 2.486615628073676

Epoch: 5| Step: 8
Training loss: 2.5514230198297074
Validation loss: 2.4958865100075984

Epoch: 5| Step: 9
Training loss: 2.4267173349239926
Validation loss: 2.495481625970121

Epoch: 5| Step: 10
Training loss: 2.8056473218642144
Validation loss: 2.503444305672254

Epoch: 5| Step: 11
Training loss: 3.178312036942692
Validation loss: 2.5121407992248677

Epoch: 208| Step: 0
Training loss: 2.5062173779426455
Validation loss: 2.523559342976978

Epoch: 5| Step: 1
Training loss: 2.8506672329106615
Validation loss: 2.541910364370693

Epoch: 5| Step: 2
Training loss: 2.3686085077466834
Validation loss: 2.5550257879761116

Epoch: 5| Step: 3
Training loss: 2.3478184293308786
Validation loss: 2.5570081112058785

Epoch: 5| Step: 4
Training loss: 2.570884756834184
Validation loss: 2.5572948546431817

Epoch: 5| Step: 5
Training loss: 2.403852680490877
Validation loss: 2.5827140559339523

Epoch: 5| Step: 6
Training loss: 2.4046099139035446
Validation loss: 2.5689217071504378

Epoch: 5| Step: 7
Training loss: 2.3644821772735543
Validation loss: 2.5707264985175113

Epoch: 5| Step: 8
Training loss: 2.302915454374376
Validation loss: 2.537879049869629

Epoch: 5| Step: 9
Training loss: 2.4613416551812293
Validation loss: 2.5329298167560097

Epoch: 5| Step: 10
Training loss: 2.2514315395667612
Validation loss: 2.5117152497310187

Epoch: 5| Step: 11
Training loss: 2.398756770153739
Validation loss: 2.4975248282925993

Epoch: 209| Step: 0
Training loss: 2.3531004480142417
Validation loss: 2.490369115437944

Epoch: 5| Step: 1
Training loss: 2.4657008015592763
Validation loss: 2.4784897446439778

Epoch: 5| Step: 2
Training loss: 2.564987835771595
Validation loss: 2.4861323580967603

Epoch: 5| Step: 3
Training loss: 1.9222168657133898
Validation loss: 2.4868300761060533

Epoch: 5| Step: 4
Training loss: 2.1383002807528517
Validation loss: 2.4876640927841427

Epoch: 5| Step: 5
Training loss: 2.5238906878575684
Validation loss: 2.4818570834873825

Epoch: 5| Step: 6
Training loss: 1.6229534466566342
Validation loss: 2.4741008028200007

Epoch: 5| Step: 7
Training loss: 3.0757055698160163
Validation loss: 2.483890480202132

Epoch: 5| Step: 8
Training loss: 2.73011613787431
Validation loss: 2.4841921717070137

Epoch: 5| Step: 9
Training loss: 2.7684054433704355
Validation loss: 2.479484559095361

Epoch: 5| Step: 10
Training loss: 2.61295005170732
Validation loss: 2.4838377213661915

Epoch: 5| Step: 11
Training loss: 2.1416917356614573
Validation loss: 2.4882061326662117

Epoch: 210| Step: 0
Training loss: 2.7024134763405345
Validation loss: 2.4892648482544395

Epoch: 5| Step: 1
Training loss: 2.233707354906475
Validation loss: 2.485311947290132

Epoch: 5| Step: 2
Training loss: 2.28505660923925
Validation loss: 2.4852910821996606

Epoch: 5| Step: 3
Training loss: 2.6673245413223032
Validation loss: 2.484522193370956

Epoch: 5| Step: 4
Training loss: 2.384529570722729
Validation loss: 2.4870391412397175

Epoch: 5| Step: 5
Training loss: 2.237430643892782
Validation loss: 2.492920694950995

Epoch: 5| Step: 6
Training loss: 2.2897136920708
Validation loss: 2.488561014669984

Epoch: 5| Step: 7
Training loss: 2.7720357441875136
Validation loss: 2.479327478455837

Epoch: 5| Step: 8
Training loss: 2.025160597096132
Validation loss: 2.4890984112163905

Epoch: 5| Step: 9
Training loss: 2.3850696316836824
Validation loss: 2.4846934828160254

Epoch: 5| Step: 10
Training loss: 2.4158974004143086
Validation loss: 2.4968092146233274

Epoch: 5| Step: 11
Training loss: 2.6890333370243242
Validation loss: 2.5010728262681865

Epoch: 211| Step: 0
Training loss: 2.3028509548919525
Validation loss: 2.520774282336534

Epoch: 5| Step: 1
Training loss: 1.9771055297355928
Validation loss: 2.549592801069359

Epoch: 5| Step: 2
Training loss: 2.3229065853521664
Validation loss: 2.585469936423524

Epoch: 5| Step: 3
Training loss: 1.9233376744362924
Validation loss: 2.589311939257028

Epoch: 5| Step: 4
Training loss: 2.878102163085464
Validation loss: 2.5635737286272553

Epoch: 5| Step: 5
Training loss: 2.4223220904738407
Validation loss: 2.5702760842631722

Epoch: 5| Step: 6
Training loss: 2.231293373861729
Validation loss: 2.545389783797118

Epoch: 5| Step: 7
Training loss: 2.738353690197303
Validation loss: 2.5313655018454884

Epoch: 5| Step: 8
Training loss: 2.470856263009816
Validation loss: 2.512884973730849

Epoch: 5| Step: 9
Training loss: 2.7179325672623937
Validation loss: 2.50662491706142

Epoch: 5| Step: 10
Training loss: 2.8391948729975316
Validation loss: 2.4890128776295835

Epoch: 5| Step: 11
Training loss: 3.187852578204295
Validation loss: 2.4828808848088157

Epoch: 212| Step: 0
Training loss: 2.411999163252652
Validation loss: 2.4784900252130155

Epoch: 5| Step: 1
Training loss: 2.2958720606072016
Validation loss: 2.4830069343963226

Epoch: 5| Step: 2
Training loss: 1.4560874357065112
Validation loss: 2.4918418971506

Epoch: 5| Step: 3
Training loss: 2.5061548761975523
Validation loss: 2.5026641359530672

Epoch: 5| Step: 4
Training loss: 2.436501420710897
Validation loss: 2.4972583101745145

Epoch: 5| Step: 5
Training loss: 2.1406985082326315
Validation loss: 2.5032750532788164

Epoch: 5| Step: 6
Training loss: 2.5605801158990795
Validation loss: 2.499272928210141

Epoch: 5| Step: 7
Training loss: 2.8378378645947233
Validation loss: 2.5056998245930675

Epoch: 5| Step: 8
Training loss: 2.2072077587742087
Validation loss: 2.4997718428768803

Epoch: 5| Step: 9
Training loss: 2.2885802089644507
Validation loss: 2.5117162741040278

Epoch: 5| Step: 10
Training loss: 3.2542021068510616
Validation loss: 2.5162147201955385

Epoch: 5| Step: 11
Training loss: 0.8860510778536488
Validation loss: 2.5175992966287737

Epoch: 213| Step: 0
Training loss: 2.1833450093818603
Validation loss: 2.5180696767633264

Epoch: 5| Step: 1
Training loss: 2.4069226798557946
Validation loss: 2.5033125071753477

Epoch: 5| Step: 2
Training loss: 2.5821659670498294
Validation loss: 2.5016723205236513

Epoch: 5| Step: 3
Training loss: 2.3683824198622667
Validation loss: 2.489070765083864

Epoch: 5| Step: 4
Training loss: 2.308547276062994
Validation loss: 2.4664556912193407

Epoch: 5| Step: 5
Training loss: 2.1851154955056105
Validation loss: 2.483618043146942

Epoch: 5| Step: 6
Training loss: 2.7273165034624407
Validation loss: 2.478717580292479

Epoch: 5| Step: 7
Training loss: 2.6727841263871444
Validation loss: 2.480940506095585

Epoch: 5| Step: 8
Training loss: 2.538200443351766
Validation loss: 2.487651615538241

Epoch: 5| Step: 9
Training loss: 2.37312323051027
Validation loss: 2.4843256943486427

Epoch: 5| Step: 10
Training loss: 1.7585685629886063
Validation loss: 2.476550562495875

Epoch: 5| Step: 11
Training loss: 3.2440014280511273
Validation loss: 2.4976469052270898

Epoch: 214| Step: 0
Training loss: 2.286633970658943
Validation loss: 2.4971504064312855

Epoch: 5| Step: 1
Training loss: 2.665038436268935
Validation loss: 2.4924637174054003

Epoch: 5| Step: 2
Training loss: 2.620469588937569
Validation loss: 2.488238004411454

Epoch: 5| Step: 3
Training loss: 2.5639300194174988
Validation loss: 2.513960913831767

Epoch: 5| Step: 4
Training loss: 2.349544432290362
Validation loss: 2.5079095649354612

Epoch: 5| Step: 5
Training loss: 2.603355637461728
Validation loss: 2.5132534762733214

Epoch: 5| Step: 6
Training loss: 2.178460326030714
Validation loss: 2.5262862857211554

Epoch: 5| Step: 7
Training loss: 1.5604559689280357
Validation loss: 2.5293901423830594

Epoch: 5| Step: 8
Training loss: 2.5832843365688594
Validation loss: 2.523927206072359

Epoch: 5| Step: 9
Training loss: 2.1581224797574112
Validation loss: 2.527529651186189

Epoch: 5| Step: 10
Training loss: 2.7878947230839817
Validation loss: 2.5191592747723597

Epoch: 5| Step: 11
Training loss: 1.5846138678166692
Validation loss: 2.524343050579582

Epoch: 215| Step: 0
Training loss: 2.0043970886288496
Validation loss: 2.502683582031622

Epoch: 5| Step: 1
Training loss: 2.8438529530428034
Validation loss: 2.500313163057645

Epoch: 5| Step: 2
Training loss: 2.821266752572745
Validation loss: 2.5144936740496298

Epoch: 5| Step: 3
Training loss: 2.6046489930124053
Validation loss: 2.498876633182468

Epoch: 5| Step: 4
Training loss: 2.151305569646418
Validation loss: 2.5181367154328984

Epoch: 5| Step: 5
Training loss: 2.563567427705068
Validation loss: 2.5175057838055293

Epoch: 5| Step: 6
Training loss: 2.740132923382582
Validation loss: 2.5211231535355467

Epoch: 5| Step: 7
Training loss: 1.7544550727316692
Validation loss: 2.5169368979862448

Epoch: 5| Step: 8
Training loss: 2.300077942895848
Validation loss: 2.521231636916041

Epoch: 5| Step: 9
Training loss: 2.231951379465205
Validation loss: 2.5150595714675625

Epoch: 5| Step: 10
Training loss: 2.152673381065245
Validation loss: 2.513560887682308

Epoch: 5| Step: 11
Training loss: 1.42023910483902
Validation loss: 2.5061306408929918

Epoch: 216| Step: 0
Training loss: 2.8513221953005545
Validation loss: 2.4994604104582256

Epoch: 5| Step: 1
Training loss: 2.0033945839521454
Validation loss: 2.506180585817944

Epoch: 5| Step: 2
Training loss: 2.4668689756510753
Validation loss: 2.4986770725755774

Epoch: 5| Step: 3
Training loss: 2.355506302010728
Validation loss: 2.4985087834841835

Epoch: 5| Step: 4
Training loss: 2.736106455154029
Validation loss: 2.4874580135348343

Epoch: 5| Step: 5
Training loss: 2.8795398858539247
Validation loss: 2.4941156555853667

Epoch: 5| Step: 6
Training loss: 2.0013871151082903
Validation loss: 2.4974728208787043

Epoch: 5| Step: 7
Training loss: 2.3755920073865973
Validation loss: 2.503469892339964

Epoch: 5| Step: 8
Training loss: 2.0844880400836336
Validation loss: 2.493667577594043

Epoch: 5| Step: 9
Training loss: 1.8327846645313075
Validation loss: 2.4957965242052254

Epoch: 5| Step: 10
Training loss: 2.4022327924706195
Validation loss: 2.5002563583541866

Epoch: 5| Step: 11
Training loss: 2.3199803792189573
Validation loss: 2.5053163823694073

Epoch: 217| Step: 0
Training loss: 3.1055261762575594
Validation loss: 2.5179611439500755

Epoch: 5| Step: 1
Training loss: 2.305389917952624
Validation loss: 2.5254036808881124

Epoch: 5| Step: 2
Training loss: 1.4725502036689408
Validation loss: 2.5378350601975384

Epoch: 5| Step: 3
Training loss: 2.2265562893964916
Validation loss: 2.5617722547876163

Epoch: 5| Step: 4
Training loss: 2.0283442915048644
Validation loss: 2.544056768532342

Epoch: 5| Step: 5
Training loss: 2.692066836849513
Validation loss: 2.529539454547749

Epoch: 5| Step: 6
Training loss: 1.9767455366824085
Validation loss: 2.5144290529617623

Epoch: 5| Step: 7
Training loss: 2.2939020756089197
Validation loss: 2.5182162852912824

Epoch: 5| Step: 8
Training loss: 2.6372230782150416
Validation loss: 2.503778848169223

Epoch: 5| Step: 9
Training loss: 2.2360255407331633
Validation loss: 2.5130076129677588

Epoch: 5| Step: 10
Training loss: 3.1305020533580716
Validation loss: 2.5088373072900274

Epoch: 5| Step: 11
Training loss: 2.449774136155468
Validation loss: 2.4942618917858677

Epoch: 218| Step: 0
Training loss: 2.4568217420179272
Validation loss: 2.4952264909884363

Epoch: 5| Step: 1
Training loss: 2.4512413227836163
Validation loss: 2.511189493488723

Epoch: 5| Step: 2
Training loss: 2.4788913306777047
Validation loss: 2.5105336677754284

Epoch: 5| Step: 3
Training loss: 2.211762183492915
Validation loss: 2.509274224094229

Epoch: 5| Step: 4
Training loss: 2.365172079007756
Validation loss: 2.5206514593725604

Epoch: 5| Step: 5
Training loss: 2.237226254217717
Validation loss: 2.530475719772465

Epoch: 5| Step: 6
Training loss: 2.0805938511192976
Validation loss: 2.546913661546331

Epoch: 5| Step: 7
Training loss: 2.3497089165128706
Validation loss: 2.545128143297115

Epoch: 5| Step: 8
Training loss: 1.950904316165488
Validation loss: 2.548214811586041

Epoch: 5| Step: 9
Training loss: 3.1678374953543083
Validation loss: 2.5532284107533703

Epoch: 5| Step: 10
Training loss: 2.3970450170663824
Validation loss: 2.525277792408034

Epoch: 5| Step: 11
Training loss: 2.62351457256547
Validation loss: 2.5378095616526903

Epoch: 219| Step: 0
Training loss: 2.4268124365079693
Validation loss: 2.509914103864538

Epoch: 5| Step: 1
Training loss: 2.550930234989138
Validation loss: 2.5044369782294025

Epoch: 5| Step: 2
Training loss: 2.1268446152876623
Validation loss: 2.488254297450846

Epoch: 5| Step: 3
Training loss: 2.276269466310625
Validation loss: 2.4874656354584777

Epoch: 5| Step: 4
Training loss: 2.2076508049091497
Validation loss: 2.4828707301467645

Epoch: 5| Step: 5
Training loss: 2.684406718198816
Validation loss: 2.4818600294663438

Epoch: 5| Step: 6
Training loss: 2.7095436228513687
Validation loss: 2.484963489305191

Epoch: 5| Step: 7
Training loss: 2.130433037307823
Validation loss: 2.4854573385316776

Epoch: 5| Step: 8
Training loss: 3.033132381559925
Validation loss: 2.4850050769001855

Epoch: 5| Step: 9
Training loss: 2.068282834690977
Validation loss: 2.48962837375349

Epoch: 5| Step: 10
Training loss: 2.4630244050081047
Validation loss: 2.484691461764576

Epoch: 5| Step: 11
Training loss: 2.2360177570177044
Validation loss: 2.4897453595883734

Epoch: 220| Step: 0
Training loss: 2.4675284151955297
Validation loss: 2.489424909288816

Epoch: 5| Step: 1
Training loss: 2.064655102798519
Validation loss: 2.507130800566758

Epoch: 5| Step: 2
Training loss: 2.8918583068959456
Validation loss: 2.5095279368299592

Epoch: 5| Step: 3
Training loss: 2.581267505059912
Validation loss: 2.5206859140928293

Epoch: 5| Step: 4
Training loss: 2.250859520242101
Validation loss: 2.5436995589545326

Epoch: 5| Step: 5
Training loss: 2.6199192650564287
Validation loss: 2.5415378457135365

Epoch: 5| Step: 6
Training loss: 2.246895767986046
Validation loss: 2.544727683983858

Epoch: 5| Step: 7
Training loss: 2.143073136933353
Validation loss: 2.518615512870232

Epoch: 5| Step: 8
Training loss: 2.378996247592066
Validation loss: 2.5044211137500874

Epoch: 5| Step: 9
Training loss: 2.2982543125450317
Validation loss: 2.499441366564975

Epoch: 5| Step: 10
Training loss: 2.762794295985485
Validation loss: 2.4997345703998834

Epoch: 5| Step: 11
Training loss: 1.67170914157039
Validation loss: 2.4934155777721587

Epoch: 221| Step: 0
Training loss: 2.0966759349858712
Validation loss: 2.497665869334652

Epoch: 5| Step: 1
Training loss: 2.904205587628886
Validation loss: 2.485844183704287

Epoch: 5| Step: 2
Training loss: 2.487763978379296
Validation loss: 2.50292442461131

Epoch: 5| Step: 3
Training loss: 2.483236759876719
Validation loss: 2.4965886086657623

Epoch: 5| Step: 4
Training loss: 2.5340485349612116
Validation loss: 2.4996921151038274

Epoch: 5| Step: 5
Training loss: 2.295409065640695
Validation loss: 2.506744105687105

Epoch: 5| Step: 6
Training loss: 1.757317977856034
Validation loss: 2.4933780867679136

Epoch: 5| Step: 7
Training loss: 2.5434262381300923
Validation loss: 2.5122764110716216

Epoch: 5| Step: 8
Training loss: 2.491298026518223
Validation loss: 2.4949115984499834

Epoch: 5| Step: 9
Training loss: 2.356881143647418
Validation loss: 2.5106186066656018

Epoch: 5| Step: 10
Training loss: 2.3439986033516846
Validation loss: 2.5048463418448366

Epoch: 5| Step: 11
Training loss: 0.9068613949681245
Validation loss: 2.5188325765876116

Epoch: 222| Step: 0
Training loss: 2.169266156164457
Validation loss: 2.510968156970438

Epoch: 5| Step: 1
Training loss: 2.2517567240420977
Validation loss: 2.505923315210168

Epoch: 5| Step: 2
Training loss: 2.283566696419141
Validation loss: 2.510704796956672

Epoch: 5| Step: 3
Training loss: 2.338189407925805
Validation loss: 2.508844668261035

Epoch: 5| Step: 4
Training loss: 2.6380721172282273
Validation loss: 2.5057341617927458

Epoch: 5| Step: 5
Training loss: 1.7527841491637155
Validation loss: 2.499522598698987

Epoch: 5| Step: 6
Training loss: 2.7156681702653644
Validation loss: 2.512169575512403

Epoch: 5| Step: 7
Training loss: 2.647715062095774
Validation loss: 2.512840665113769

Epoch: 5| Step: 8
Training loss: 2.0423725490890394
Validation loss: 2.508265964315179

Epoch: 5| Step: 9
Training loss: 2.37658979511826
Validation loss: 2.507309147121815

Epoch: 5| Step: 10
Training loss: 2.5946854892131626
Validation loss: 2.518071684832733

Epoch: 5| Step: 11
Training loss: 2.891131712344666
Validation loss: 2.519695144566657

Epoch: 223| Step: 0
Training loss: 2.5470470534173053
Validation loss: 2.522175834671953

Epoch: 5| Step: 1
Training loss: 1.7222467398521963
Validation loss: 2.516572272402026

Epoch: 5| Step: 2
Training loss: 2.444903647273631
Validation loss: 2.513489237139856

Epoch: 5| Step: 3
Training loss: 2.557963193114135
Validation loss: 2.527102565508749

Epoch: 5| Step: 4
Training loss: 2.3012129114779096
Validation loss: 2.5172323182289733

Epoch: 5| Step: 5
Training loss: 2.0840984338295705
Validation loss: 2.5065223211360803

Epoch: 5| Step: 6
Training loss: 2.4715796563062744
Validation loss: 2.51540607581806

Epoch: 5| Step: 7
Training loss: 2.600514793783721
Validation loss: 2.5076810776607728

Epoch: 5| Step: 8
Training loss: 2.185152156254545
Validation loss: 2.508407918216157

Epoch: 5| Step: 9
Training loss: 2.6759026047412555
Validation loss: 2.5132179077564945

Epoch: 5| Step: 10
Training loss: 2.4675689962803085
Validation loss: 2.511672939596673

Epoch: 5| Step: 11
Training loss: 3.070347754200829
Validation loss: 2.5117841705139874

Epoch: 224| Step: 0
Training loss: 2.0116552250285746
Validation loss: 2.5190439388763517

Epoch: 5| Step: 1
Training loss: 2.370964939194463
Validation loss: 2.5180415991300342

Epoch: 5| Step: 2
Training loss: 2.698675738642953
Validation loss: 2.517759789580367

Epoch: 5| Step: 3
Training loss: 3.054580881741085
Validation loss: 2.5417074482934385

Epoch: 5| Step: 4
Training loss: 2.1613383309629923
Validation loss: 2.5556281804797094

Epoch: 5| Step: 5
Training loss: 1.9279558079708163
Validation loss: 2.547921591717838

Epoch: 5| Step: 6
Training loss: 2.016240225648217
Validation loss: 2.5691130795325776

Epoch: 5| Step: 7
Training loss: 2.338084787259031
Validation loss: 2.5776770462665106

Epoch: 5| Step: 8
Training loss: 2.8066872610784843
Validation loss: 2.5743571520633752

Epoch: 5| Step: 9
Training loss: 2.4945060444366938
Validation loss: 2.5609212815502573

Epoch: 5| Step: 10
Training loss: 2.366542010539108
Validation loss: 2.5482926898839624

Epoch: 5| Step: 11
Training loss: 2.048872344464421
Validation loss: 2.5445472322469014

Epoch: 225| Step: 0
Training loss: 2.489926643057015
Validation loss: 2.517869510760363

Epoch: 5| Step: 1
Training loss: 2.083858411104977
Validation loss: 2.5073641715972577

Epoch: 5| Step: 2
Training loss: 2.2665693748387743
Validation loss: 2.4986531046708307

Epoch: 5| Step: 3
Training loss: 2.4136381794864388
Validation loss: 2.5005823569716727

Epoch: 5| Step: 4
Training loss: 2.3871883049064806
Validation loss: 2.509407728432272

Epoch: 5| Step: 5
Training loss: 2.511635501603252
Validation loss: 2.519410728312288

Epoch: 5| Step: 6
Training loss: 2.996833242497387
Validation loss: 2.51161559878424

Epoch: 5| Step: 7
Training loss: 2.1114250111471526
Validation loss: 2.527165103437475

Epoch: 5| Step: 8
Training loss: 2.288671883333867
Validation loss: 2.524758413453467

Epoch: 5| Step: 9
Training loss: 1.5673720124725266
Validation loss: 2.5259442864488544

Epoch: 5| Step: 10
Training loss: 2.688209883505583
Validation loss: 2.5368746188090556

Epoch: 5| Step: 11
Training loss: 2.532956148812419
Validation loss: 2.5253535811561005

Epoch: 226| Step: 0
Training loss: 1.706834794880146
Validation loss: 2.514756074348615

Epoch: 5| Step: 1
Training loss: 1.997240427694995
Validation loss: 2.5250717988523186

Epoch: 5| Step: 2
Training loss: 2.5001406630044536
Validation loss: 2.519603856344169

Epoch: 5| Step: 3
Training loss: 2.2384696734676623
Validation loss: 2.5163192304463697

Epoch: 5| Step: 4
Training loss: 2.337730306611564
Validation loss: 2.507670090495119

Epoch: 5| Step: 5
Training loss: 2.5473874752510977
Validation loss: 2.515044431647837

Epoch: 5| Step: 6
Training loss: 2.438782574598849
Validation loss: 2.529428344608949

Epoch: 5| Step: 7
Training loss: 2.713437293761194
Validation loss: 2.524181681401844

Epoch: 5| Step: 8
Training loss: 3.007381259670218
Validation loss: 2.511268674275632

Epoch: 5| Step: 9
Training loss: 2.1629482664640083
Validation loss: 2.5145983644703613

Epoch: 5| Step: 10
Training loss: 2.1072649539367583
Validation loss: 2.4998712625735493

Epoch: 5| Step: 11
Training loss: 3.275705066525576
Validation loss: 2.5035492141096456

Epoch: 227| Step: 0
Training loss: 2.313387416991579
Validation loss: 2.492763484616251

Epoch: 5| Step: 1
Training loss: 2.215314112105868
Validation loss: 2.5033427659270573

Epoch: 5| Step: 2
Training loss: 2.9650640795314254
Validation loss: 2.4887486592226424

Epoch: 5| Step: 3
Training loss: 2.817687527526269
Validation loss: 2.484370809427562

Epoch: 5| Step: 4
Training loss: 2.849533738929384
Validation loss: 2.497031877799011

Epoch: 5| Step: 5
Training loss: 2.331523693064954
Validation loss: 2.4857459334357763

Epoch: 5| Step: 6
Training loss: 2.4867922940705918
Validation loss: 2.4861926021634337

Epoch: 5| Step: 7
Training loss: 2.184917996960614
Validation loss: 2.482707033504056

Epoch: 5| Step: 8
Training loss: 2.3492198947894827
Validation loss: 2.486889092909642

Epoch: 5| Step: 9
Training loss: 1.9170007690921647
Validation loss: 2.492063210316882

Epoch: 5| Step: 10
Training loss: 2.0750935177239134
Validation loss: 2.4908356463014343

Epoch: 5| Step: 11
Training loss: 1.280429996371854
Validation loss: 2.5073155458393797

Epoch: 228| Step: 0
Training loss: 1.911234545448746
Validation loss: 2.502327800870215

Epoch: 5| Step: 1
Training loss: 2.1904143402201424
Validation loss: 2.5219828307250296

Epoch: 5| Step: 2
Training loss: 2.1229283949877726
Validation loss: 2.544150578374773

Epoch: 5| Step: 3
Training loss: 2.2622248951276385
Validation loss: 2.5602934687943217

Epoch: 5| Step: 4
Training loss: 2.299117362625328
Validation loss: 2.572056249363361

Epoch: 5| Step: 5
Training loss: 2.194541279819773
Validation loss: 2.568173555292825

Epoch: 5| Step: 6
Training loss: 2.499873539587197
Validation loss: 2.5741789471757444

Epoch: 5| Step: 7
Training loss: 3.1181395948186603
Validation loss: 2.573665858599842

Epoch: 5| Step: 8
Training loss: 2.4784360712028275
Validation loss: 2.543066477033061

Epoch: 5| Step: 9
Training loss: 2.3343409565150135
Validation loss: 2.5233987622195957

Epoch: 5| Step: 10
Training loss: 2.4500965449715233
Validation loss: 2.513707804626864

Epoch: 5| Step: 11
Training loss: 3.308286470198203
Validation loss: 2.5047502528480265

Epoch: 229| Step: 0
Training loss: 2.1434355613609632
Validation loss: 2.4862506052275073

Epoch: 5| Step: 1
Training loss: 2.3637452734017885
Validation loss: 2.4951482861715233

Epoch: 5| Step: 2
Training loss: 2.295246195500915
Validation loss: 2.4891350926543416

Epoch: 5| Step: 3
Training loss: 2.490817754889693
Validation loss: 2.4898650190907037

Epoch: 5| Step: 4
Training loss: 2.510743232053215
Validation loss: 2.4973339885947405

Epoch: 5| Step: 5
Training loss: 3.04872317869706
Validation loss: 2.4959775691686565

Epoch: 5| Step: 6
Training loss: 2.2316367703696143
Validation loss: 2.4885941353130554

Epoch: 5| Step: 7
Training loss: 2.1732728699696664
Validation loss: 2.483208112365121

Epoch: 5| Step: 8
Training loss: 2.3141949602191705
Validation loss: 2.489705275630973

Epoch: 5| Step: 9
Training loss: 2.3620419618333774
Validation loss: 2.4803401563256897

Epoch: 5| Step: 10
Training loss: 2.476911550286343
Validation loss: 2.4898751990896915

Epoch: 5| Step: 11
Training loss: 3.2326437329452795
Validation loss: 2.4837314743921524

Epoch: 230| Step: 0
Training loss: 2.4650836229358237
Validation loss: 2.497376567513012

Epoch: 5| Step: 1
Training loss: 2.087917573931554
Validation loss: 2.4874885170637997

Epoch: 5| Step: 2
Training loss: 2.4620081918736636
Validation loss: 2.4997536657088846

Epoch: 5| Step: 3
Training loss: 2.3846859430471246
Validation loss: 2.4813083859611953

Epoch: 5| Step: 4
Training loss: 2.7343148361126666
Validation loss: 2.485227998211273

Epoch: 5| Step: 5
Training loss: 2.267982361450769
Validation loss: 2.4914752337001658

Epoch: 5| Step: 6
Training loss: 1.7842197090090894
Validation loss: 2.4987350243772775

Epoch: 5| Step: 7
Training loss: 2.6527432799523356
Validation loss: 2.4919942782924682

Epoch: 5| Step: 8
Training loss: 2.511258808460424
Validation loss: 2.504024976909957

Epoch: 5| Step: 9
Training loss: 2.5369793136890055
Validation loss: 2.5078130110030084

Epoch: 5| Step: 10
Training loss: 2.550872287031174
Validation loss: 2.513864920316303

Epoch: 5| Step: 11
Training loss: 1.0699818789650022
Validation loss: 2.5108315268821393

Epoch: 231| Step: 0
Training loss: 2.3503449288316944
Validation loss: 2.5092146410917247

Epoch: 5| Step: 1
Training loss: 2.218121184310576
Validation loss: 2.536058399595764

Epoch: 5| Step: 2
Training loss: 2.3892355477611553
Validation loss: 2.527565134248714

Epoch: 5| Step: 3
Training loss: 2.052037491907636
Validation loss: 2.541631922458332

Epoch: 5| Step: 4
Training loss: 1.924407654820031
Validation loss: 2.55359612736236

Epoch: 5| Step: 5
Training loss: 2.742336519110193
Validation loss: 2.555047537721606

Epoch: 5| Step: 6
Training loss: 2.3577351713550505
Validation loss: 2.5327669747128647

Epoch: 5| Step: 7
Training loss: 2.545626749833721
Validation loss: 2.5556301784735016

Epoch: 5| Step: 8
Training loss: 2.7473245090187697
Validation loss: 2.5565093113355193

Epoch: 5| Step: 9
Training loss: 2.6981274928686076
Validation loss: 2.5370622238464633

Epoch: 5| Step: 10
Training loss: 2.1745884911624076
Validation loss: 2.5205527036429

Epoch: 5| Step: 11
Training loss: 1.6178772418320437
Validation loss: 2.515851514107649

Epoch: 232| Step: 0
Training loss: 2.5101785401756485
Validation loss: 2.5097344679821925

Epoch: 5| Step: 1
Training loss: 2.4367116973862952
Validation loss: 2.505778604000063

Epoch: 5| Step: 2
Training loss: 2.0505236073278903
Validation loss: 2.506482247858998

Epoch: 5| Step: 3
Training loss: 2.785312386051271
Validation loss: 2.507636546362714

Epoch: 5| Step: 4
Training loss: 2.6171537938011427
Validation loss: 2.5008322244502095

Epoch: 5| Step: 5
Training loss: 2.304567702461147
Validation loss: 2.5199736371881576

Epoch: 5| Step: 6
Training loss: 1.951363463441672
Validation loss: 2.525139210265847

Epoch: 5| Step: 7
Training loss: 2.459400096411423
Validation loss: 2.5143677272718317

Epoch: 5| Step: 8
Training loss: 2.125698535772213
Validation loss: 2.5167739961065694

Epoch: 5| Step: 9
Training loss: 2.292653986627437
Validation loss: 2.5262704090229593

Epoch: 5| Step: 10
Training loss: 2.455864512549054
Validation loss: 2.5266996503323567

Epoch: 5| Step: 11
Training loss: 2.079544041799415
Validation loss: 2.5235248664715164

Epoch: 233| Step: 0
Training loss: 2.6591764597850407
Validation loss: 2.5141567660642443

Epoch: 5| Step: 1
Training loss: 2.069916654926837
Validation loss: 2.4966404395281394

Epoch: 5| Step: 2
Training loss: 2.399953356925397
Validation loss: 2.4882688736620273

Epoch: 5| Step: 3
Training loss: 2.9590021013289167
Validation loss: 2.4886049612008354

Epoch: 5| Step: 4
Training loss: 2.7727597550253313
Validation loss: 2.4866382997733996

Epoch: 5| Step: 5
Training loss: 2.6846719100227743
Validation loss: 2.4880217492225203

Epoch: 5| Step: 6
Training loss: 2.141563710905807
Validation loss: 2.4869627201408635

Epoch: 5| Step: 7
Training loss: 2.0639532344754237
Validation loss: 2.489838532545187

Epoch: 5| Step: 8
Training loss: 1.74240076786809
Validation loss: 2.4916226495301657

Epoch: 5| Step: 9
Training loss: 2.2693026455434087
Validation loss: 2.499982241726429

Epoch: 5| Step: 10
Training loss: 2.6492068093294288
Validation loss: 2.511463810869781

Epoch: 5| Step: 11
Training loss: 1.3040755887063737
Validation loss: 2.521160815118955

Epoch: 234| Step: 0
Training loss: 2.0910515548210267
Validation loss: 2.5373663376931654

Epoch: 5| Step: 1
Training loss: 2.7127691671537875
Validation loss: 2.528129447647036

Epoch: 5| Step: 2
Training loss: 2.3617940463556772
Validation loss: 2.551656474531845

Epoch: 5| Step: 3
Training loss: 2.5323077217156955
Validation loss: 2.533017581369152

Epoch: 5| Step: 4
Training loss: 2.2441846081280583
Validation loss: 2.5128677848633014

Epoch: 5| Step: 5
Training loss: 2.1190345280540392
Validation loss: 2.49526213879963

Epoch: 5| Step: 6
Training loss: 2.286262544375419
Validation loss: 2.489190498919507

Epoch: 5| Step: 7
Training loss: 2.3578830068877297
Validation loss: 2.495088481302477

Epoch: 5| Step: 8
Training loss: 2.5042462527237626
Validation loss: 2.4950148629285427

Epoch: 5| Step: 9
Training loss: 3.0551501457915418
Validation loss: 2.4921474194467663

Epoch: 5| Step: 10
Training loss: 2.3520401884525066
Validation loss: 2.4925484590210245

Epoch: 5| Step: 11
Training loss: 1.5742435122073486
Validation loss: 2.501894350376497

Epoch: 235| Step: 0
Training loss: 2.08427013951295
Validation loss: 2.504053683807939

Epoch: 5| Step: 1
Training loss: 2.333261954260277
Validation loss: 2.5094889684998236

Epoch: 5| Step: 2
Training loss: 2.25840228868153
Validation loss: 2.520117734323475

Epoch: 5| Step: 3
Training loss: 2.6334968379320203
Validation loss: 2.5110951623989206

Epoch: 5| Step: 4
Training loss: 2.070302826480938
Validation loss: 2.5334676556886215

Epoch: 5| Step: 5
Training loss: 2.41720850121043
Validation loss: 2.537278931012557

Epoch: 5| Step: 6
Training loss: 2.4904942036630007
Validation loss: 2.544405209686742

Epoch: 5| Step: 7
Training loss: 2.9113689993503775
Validation loss: 2.5369421727874233

Epoch: 5| Step: 8
Training loss: 1.7812415674913475
Validation loss: 2.5304850847188725

Epoch: 5| Step: 9
Training loss: 2.5540184521467832
Validation loss: 2.557293848527546

Epoch: 5| Step: 10
Training loss: 2.293735668335091
Validation loss: 2.5535580299361285

Epoch: 5| Step: 11
Training loss: 2.5692748784197668
Validation loss: 2.555998474561657

Epoch: 236| Step: 0
Training loss: 2.4262905108427084
Validation loss: 2.531216020709252

Epoch: 5| Step: 1
Training loss: 2.1196460595762674
Validation loss: 2.5276612913157646

Epoch: 5| Step: 2
Training loss: 1.3979766768391402
Validation loss: 2.5202702820866825

Epoch: 5| Step: 3
Training loss: 2.8111220481049983
Validation loss: 2.5026698638094174

Epoch: 5| Step: 4
Training loss: 1.863168453105249
Validation loss: 2.504127849705389

Epoch: 5| Step: 5
Training loss: 2.0720330915938363
Validation loss: 2.4943651592659823

Epoch: 5| Step: 6
Training loss: 3.0724763888886635
Validation loss: 2.497224248269541

Epoch: 5| Step: 7
Training loss: 3.173127476749652
Validation loss: 2.4901100955443876

Epoch: 5| Step: 8
Training loss: 2.3692522266389955
Validation loss: 2.4894402488121306

Epoch: 5| Step: 9
Training loss: 2.773010564905697
Validation loss: 2.4980512295372583

Epoch: 5| Step: 10
Training loss: 1.8600845265409514
Validation loss: 2.505447837552789

Epoch: 5| Step: 11
Training loss: 2.0608326501327143
Validation loss: 2.5101788112664547

Epoch: 237| Step: 0
Training loss: 2.6549171245161904
Validation loss: 2.538268746748062

Epoch: 5| Step: 1
Training loss: 2.1010825591173354
Validation loss: 2.5569715700956035

Epoch: 5| Step: 2
Training loss: 2.4493205704886902
Validation loss: 2.586190041372029

Epoch: 5| Step: 3
Training loss: 2.6461989583177514
Validation loss: 2.58048346947693

Epoch: 5| Step: 4
Training loss: 2.088609220279519
Validation loss: 2.5610117350896195

Epoch: 5| Step: 5
Training loss: 2.0329756229288267
Validation loss: 2.556637069664706

Epoch: 5| Step: 6
Training loss: 2.0433730824863425
Validation loss: 2.5492742778884856

Epoch: 5| Step: 7
Training loss: 2.746889435922231
Validation loss: 2.542306218953695

Epoch: 5| Step: 8
Training loss: 2.4385139117208765
Validation loss: 2.5193794461041286

Epoch: 5| Step: 9
Training loss: 1.9514794092990613
Validation loss: 2.5350692765452782

Epoch: 5| Step: 10
Training loss: 2.7704869868907975
Validation loss: 2.5149154570698906

Epoch: 5| Step: 11
Training loss: 2.3799222834103726
Validation loss: 2.505393724593952

Epoch: 238| Step: 0
Training loss: 2.3083427796245144
Validation loss: 2.506896753964144

Epoch: 5| Step: 1
Training loss: 2.13435704683095
Validation loss: 2.526311275343686

Epoch: 5| Step: 2
Training loss: 2.1051831449813516
Validation loss: 2.5166135823943825

Epoch: 5| Step: 3
Training loss: 2.1748634470901314
Validation loss: 2.523052768884462

Epoch: 5| Step: 4
Training loss: 2.7653576565423528
Validation loss: 2.525405321228426

Epoch: 5| Step: 5
Training loss: 2.182137810891011
Validation loss: 2.5182369840882606

Epoch: 5| Step: 6
Training loss: 2.461018781984807
Validation loss: 2.541131675937749

Epoch: 5| Step: 7
Training loss: 2.1357748956185993
Validation loss: 2.5366793633463645

Epoch: 5| Step: 8
Training loss: 2.72612472561185
Validation loss: 2.526402116732378

Epoch: 5| Step: 9
Training loss: 2.5240202900997457
Validation loss: 2.5261656045823586

Epoch: 5| Step: 10
Training loss: 2.4233712773279956
Validation loss: 2.535945863410288

Epoch: 5| Step: 11
Training loss: 1.2981705227006624
Validation loss: 2.5347827544162396

Epoch: 239| Step: 0
Training loss: 3.223487404655652
Validation loss: 2.526749715511711

Epoch: 5| Step: 1
Training loss: 2.6239255795334464
Validation loss: 2.5230081446879433

Epoch: 5| Step: 2
Training loss: 1.8864593011554136
Validation loss: 2.5313832401011354

Epoch: 5| Step: 3
Training loss: 2.2699892300925852
Validation loss: 2.5420186875151884

Epoch: 5| Step: 4
Training loss: 2.3575105689935434
Validation loss: 2.534267612664134

Epoch: 5| Step: 5
Training loss: 2.153403573361962
Validation loss: 2.53807889226249

Epoch: 5| Step: 6
Training loss: 2.351513225254615
Validation loss: 2.5268169249022883

Epoch: 5| Step: 7
Training loss: 1.8496254876954594
Validation loss: 2.521270805945071

Epoch: 5| Step: 8
Training loss: 2.0052574911325998
Validation loss: 2.532482502560862

Epoch: 5| Step: 9
Training loss: 2.5562444486301272
Validation loss: 2.5195155061924837

Epoch: 5| Step: 10
Training loss: 2.0320975076153562
Validation loss: 2.5201871072533786

Epoch: 5| Step: 11
Training loss: 2.853440593800942
Validation loss: 2.51675611935937

Epoch: 240| Step: 0
Training loss: 2.2059964344967677
Validation loss: 2.518070323764253

Epoch: 5| Step: 1
Training loss: 1.970158814646319
Validation loss: 2.5149407492800346

Epoch: 5| Step: 2
Training loss: 2.1037043324782
Validation loss: 2.514174483488038

Epoch: 5| Step: 3
Training loss: 2.2647959277448892
Validation loss: 2.5070310704195635

Epoch: 5| Step: 4
Training loss: 2.5579217159231553
Validation loss: 2.504792582750146

Epoch: 5| Step: 5
Training loss: 1.6736712209724032
Validation loss: 2.50905759648037

Epoch: 5| Step: 6
Training loss: 2.5183298484665366
Validation loss: 2.4961361510844866

Epoch: 5| Step: 7
Training loss: 2.324529341773642
Validation loss: 2.5028099560289063

Epoch: 5| Step: 8
Training loss: 3.005794492223309
Validation loss: 2.5009445154613688

Epoch: 5| Step: 9
Training loss: 2.4740481925840725
Validation loss: 2.5121313164623316

Epoch: 5| Step: 10
Training loss: 2.144978349838724
Validation loss: 2.554620743869945

Epoch: 5| Step: 11
Training loss: 4.080153388337659
Validation loss: 2.5847265245088766

Epoch: 241| Step: 0
Training loss: 2.3507304699865514
Validation loss: 2.6142785450670547

Epoch: 5| Step: 1
Training loss: 2.757776016988234
Validation loss: 2.6015713322120098

Epoch: 5| Step: 2
Training loss: 2.552969445260526
Validation loss: 2.5999611173069836

Epoch: 5| Step: 3
Training loss: 2.6631221062358685
Validation loss: 2.576273744780028

Epoch: 5| Step: 4
Training loss: 1.9323743269549218
Validation loss: 2.5769184872929096

Epoch: 5| Step: 5
Training loss: 2.2768941602337907
Validation loss: 2.5709929372395446

Epoch: 5| Step: 6
Training loss: 2.6606770144345555
Validation loss: 2.569247503436996

Epoch: 5| Step: 7
Training loss: 1.6777433987997648
Validation loss: 2.540367241171589

Epoch: 5| Step: 8
Training loss: 2.05382400760124
Validation loss: 2.527084113202416

Epoch: 5| Step: 9
Training loss: 2.780490042767585
Validation loss: 2.5290091293223482

Epoch: 5| Step: 10
Training loss: 2.030868729241296
Validation loss: 2.523548769392244

Epoch: 5| Step: 11
Training loss: 1.6229925226872521
Validation loss: 2.515644018128972

Epoch: 242| Step: 0
Training loss: 2.049735363882539
Validation loss: 2.5256996840917147

Epoch: 5| Step: 1
Training loss: 2.2605077370669626
Validation loss: 2.5308195972687435

Epoch: 5| Step: 2
Training loss: 2.2019337348564725
Validation loss: 2.538525228143208

Epoch: 5| Step: 3
Training loss: 2.3159521986289415
Validation loss: 2.5399201494966914

Epoch: 5| Step: 4
Training loss: 2.011334725178528
Validation loss: 2.5504875773486484

Epoch: 5| Step: 5
Training loss: 2.357173675897108
Validation loss: 2.5443059722962333

Epoch: 5| Step: 6
Training loss: 2.4359944902742963
Validation loss: 2.5601768662778253

Epoch: 5| Step: 7
Training loss: 2.112156708040402
Validation loss: 2.5643019272101593

Epoch: 5| Step: 8
Training loss: 2.7865244539681067
Validation loss: 2.561566306795434

Epoch: 5| Step: 9
Training loss: 2.894976830002693
Validation loss: 2.570877863304077

Epoch: 5| Step: 10
Training loss: 2.4302497041606146
Validation loss: 2.5684098116681096

Epoch: 5| Step: 11
Training loss: 2.043553343477767
Validation loss: 2.5754349478559475

Epoch: 243| Step: 0
Training loss: 2.1896197266704363
Validation loss: 2.5567463774504517

Epoch: 5| Step: 1
Training loss: 1.8010449661157
Validation loss: 2.54419192084723

Epoch: 5| Step: 2
Training loss: 2.484549750174081
Validation loss: 2.513173071379294

Epoch: 5| Step: 3
Training loss: 2.411967532045336
Validation loss: 2.525050892490124

Epoch: 5| Step: 4
Training loss: 2.611531445869901
Validation loss: 2.5267545198924943

Epoch: 5| Step: 5
Training loss: 2.320899555167828
Validation loss: 2.5174104503020396

Epoch: 5| Step: 6
Training loss: 2.338455731031411
Validation loss: 2.5259327710986907

Epoch: 5| Step: 7
Training loss: 2.4922038588988467
Validation loss: 2.5221155244945126

Epoch: 5| Step: 8
Training loss: 2.711654788059445
Validation loss: 2.519659116984835

Epoch: 5| Step: 9
Training loss: 1.980692653301067
Validation loss: 2.5225942165182036

Epoch: 5| Step: 10
Training loss: 2.3418865679823484
Validation loss: 2.511860912169909

Epoch: 5| Step: 11
Training loss: 1.474960184368104
Validation loss: 2.5178167971342003

Epoch: 244| Step: 0
Training loss: 2.1981528156810266
Validation loss: 2.5226433983714958

Epoch: 5| Step: 1
Training loss: 2.3122064558794477
Validation loss: 2.512181626498202

Epoch: 5| Step: 2
Training loss: 2.036584978645414
Validation loss: 2.507480967821211

Epoch: 5| Step: 3
Training loss: 2.905724098450491
Validation loss: 2.5130398342751685

Epoch: 5| Step: 4
Training loss: 2.1962447715354
Validation loss: 2.511690304735005

Epoch: 5| Step: 5
Training loss: 3.220663150240853
Validation loss: 2.51183989585183

Epoch: 5| Step: 6
Training loss: 1.9576071617054702
Validation loss: 2.51706271620993

Epoch: 5| Step: 7
Training loss: 1.5532726962133467
Validation loss: 2.507396469387654

Epoch: 5| Step: 8
Training loss: 2.3756553348088834
Validation loss: 2.524602422321498

Epoch: 5| Step: 9
Training loss: 2.114062603755252
Validation loss: 2.540460923305594

Epoch: 5| Step: 10
Training loss: 2.571143760878653
Validation loss: 2.5229970392098164

Epoch: 5| Step: 11
Training loss: 2.066697905929892
Validation loss: 2.5311136876203766

Epoch: 245| Step: 0
Training loss: 2.4417732146085127
Validation loss: 2.524979265052338

Epoch: 5| Step: 1
Training loss: 2.1135684680195124
Validation loss: 2.520792390685288

Epoch: 5| Step: 2
Training loss: 1.8610811499972268
Validation loss: 2.5117190545429464

Epoch: 5| Step: 3
Training loss: 1.971433052557439
Validation loss: 2.5034815209336996

Epoch: 5| Step: 4
Training loss: 1.8255578298987314
Validation loss: 2.51297543473159

Epoch: 5| Step: 5
Training loss: 2.1999316508339675
Validation loss: 2.510057595138055

Epoch: 5| Step: 6
Training loss: 2.407244390146967
Validation loss: 2.515148137389499

Epoch: 5| Step: 7
Training loss: 2.619984512865725
Validation loss: 2.5178333090994296

Epoch: 5| Step: 8
Training loss: 2.8911929010870905
Validation loss: 2.518081488449182

Epoch: 5| Step: 9
Training loss: 2.794347718749191
Validation loss: 2.5288054117645675

Epoch: 5| Step: 10
Training loss: 2.507143686066965
Validation loss: 2.5438682031934774

Epoch: 5| Step: 11
Training loss: 1.9391058757888702
Validation loss: 2.545941378698691

Epoch: 246| Step: 0
Training loss: 2.0240693155338563
Validation loss: 2.566802603406043

Epoch: 5| Step: 1
Training loss: 1.8990144206218167
Validation loss: 2.5721667711731713

Epoch: 5| Step: 2
Training loss: 2.67533668110563
Validation loss: 2.5585407668978415

Epoch: 5| Step: 3
Training loss: 2.7785285750974484
Validation loss: 2.5472627434326434

Epoch: 5| Step: 4
Training loss: 1.592166338210004
Validation loss: 2.566554831157359

Epoch: 5| Step: 5
Training loss: 2.548956365757349
Validation loss: 2.569581564618162

Epoch: 5| Step: 6
Training loss: 2.3794928768569963
Validation loss: 2.5345267507148925

Epoch: 5| Step: 7
Training loss: 3.0055081346319574
Validation loss: 2.516532418373507

Epoch: 5| Step: 8
Training loss: 1.8504069782780246
Validation loss: 2.521582020691784

Epoch: 5| Step: 9
Training loss: 2.4664703660472704
Validation loss: 2.525260695763336

Epoch: 5| Step: 10
Training loss: 2.358161462602511
Validation loss: 2.524971789809915

Epoch: 5| Step: 11
Training loss: 1.717386571922773
Validation loss: 2.519857404234307

Epoch: 247| Step: 0
Training loss: 2.8300046628232898
Validation loss: 2.520230324925599

Epoch: 5| Step: 1
Training loss: 2.3626550774917936
Validation loss: 2.518975551940643

Epoch: 5| Step: 2
Training loss: 1.7478364103834656
Validation loss: 2.522910288407075

Epoch: 5| Step: 3
Training loss: 2.3798853170188403
Validation loss: 2.515698718141984

Epoch: 5| Step: 4
Training loss: 2.5968352203443894
Validation loss: 2.521398214359831

Epoch: 5| Step: 5
Training loss: 2.4383848491588487
Validation loss: 2.5198835614528874

Epoch: 5| Step: 6
Training loss: 1.833660905646006
Validation loss: 2.5245632420233735

Epoch: 5| Step: 7
Training loss: 2.0158038392681474
Validation loss: 2.537965375134013

Epoch: 5| Step: 8
Training loss: 2.527550713910437
Validation loss: 2.539382667896093

Epoch: 5| Step: 9
Training loss: 2.5431537238613195
Validation loss: 2.538102922313026

Epoch: 5| Step: 10
Training loss: 2.203680955797333
Validation loss: 2.5523480388355058

Epoch: 5| Step: 11
Training loss: 1.5891948003467342
Validation loss: 2.5578152275342774

Epoch: 248| Step: 0
Training loss: 2.547304643771756
Validation loss: 2.5853360678143704

Epoch: 5| Step: 1
Training loss: 2.5690821337787777
Validation loss: 2.615648692185915

Epoch: 5| Step: 2
Training loss: 1.8654823338130422
Validation loss: 2.606896889325723

Epoch: 5| Step: 3
Training loss: 2.2102470263342586
Validation loss: 2.606154725660731

Epoch: 5| Step: 4
Training loss: 2.014329715874278
Validation loss: 2.5694154219377285

Epoch: 5| Step: 5
Training loss: 2.220498913557461
Validation loss: 2.5815644258218673

Epoch: 5| Step: 6
Training loss: 2.1617206337730654
Validation loss: 2.5547777640196627

Epoch: 5| Step: 7
Training loss: 2.280373287261134
Validation loss: 2.558887226075715

Epoch: 5| Step: 8
Training loss: 2.3248572233927347
Validation loss: 2.5307221274321523

Epoch: 5| Step: 9
Training loss: 2.928429743034259
Validation loss: 2.5313685825027887

Epoch: 5| Step: 10
Training loss: 2.420280965620555
Validation loss: 2.5323017294079784

Epoch: 5| Step: 11
Training loss: 2.361762550344078
Validation loss: 2.518882104014004

Epoch: 249| Step: 0
Training loss: 2.666530953371227
Validation loss: 2.512604928395642

Epoch: 5| Step: 1
Training loss: 1.7180308831650941
Validation loss: 2.51759032174237

Epoch: 5| Step: 2
Training loss: 2.389295719556695
Validation loss: 2.522236899620526

Epoch: 5| Step: 3
Training loss: 3.2243607885613566
Validation loss: 2.506917566036176

Epoch: 5| Step: 4
Training loss: 2.2817719789902813
Validation loss: 2.51750396074743

Epoch: 5| Step: 5
Training loss: 2.0218993714645785
Validation loss: 2.521158927718823

Epoch: 5| Step: 6
Training loss: 2.8068257206438942
Validation loss: 2.523868752271475

Epoch: 5| Step: 7
Training loss: 2.0713958784633473
Validation loss: 2.5229601964704966

Epoch: 5| Step: 8
Training loss: 2.0421731541194665
Validation loss: 2.5758373283319047

Epoch: 5| Step: 9
Training loss: 2.060840285685785
Validation loss: 2.580502571656566

Epoch: 5| Step: 10
Training loss: 1.9484204815424948
Validation loss: 2.5850659310549213

Epoch: 5| Step: 11
Training loss: 2.6641102895617386
Validation loss: 2.5499738006243207

Epoch: 250| Step: 0
Training loss: 2.1086789713566985
Validation loss: 2.551658646936187

Epoch: 5| Step: 1
Training loss: 2.9597730938828652
Validation loss: 2.5379984225207357

Epoch: 5| Step: 2
Training loss: 2.1361188028850506
Validation loss: 2.515440584614239

Epoch: 5| Step: 3
Training loss: 2.3570464308530847
Validation loss: 2.525052891073443

Epoch: 5| Step: 4
Training loss: 2.2320700540931493
Validation loss: 2.519082396592385

Epoch: 5| Step: 5
Training loss: 2.716414753130116
Validation loss: 2.5105873197640656

Epoch: 5| Step: 6
Training loss: 1.5041460911126365
Validation loss: 2.51596024058488

Epoch: 5| Step: 7
Training loss: 2.26743407535297
Validation loss: 2.5076450676250466

Epoch: 5| Step: 8
Training loss: 2.4373352777624966
Validation loss: 2.5014714480398594

Epoch: 5| Step: 9
Training loss: 2.2007876503404113
Validation loss: 2.523305387552167

Epoch: 5| Step: 10
Training loss: 2.5294944420908223
Validation loss: 2.5342573071930423

Epoch: 5| Step: 11
Training loss: 1.4031329896015483
Validation loss: 2.5427832079194066

Epoch: 251| Step: 0
Training loss: 1.8353396477129351
Validation loss: 2.527466241854476

Epoch: 5| Step: 1
Training loss: 2.4801440409391855
Validation loss: 2.5613657914600054

Epoch: 5| Step: 2
Training loss: 2.2281772045637984
Validation loss: 2.5400090023316775

Epoch: 5| Step: 3
Training loss: 2.36044236854057
Validation loss: 2.524625811375042

Epoch: 5| Step: 4
Training loss: 2.0846098485868274
Validation loss: 2.511065626089973

Epoch: 5| Step: 5
Training loss: 1.7582506100086601
Validation loss: 2.4969766972487837

Epoch: 5| Step: 6
Training loss: 2.2357096920549497
Validation loss: 2.520231554750009

Epoch: 5| Step: 7
Training loss: 1.9313327092604353
Validation loss: 2.507363236571614

Epoch: 5| Step: 8
Training loss: 2.3163714607289543
Validation loss: 2.5015510575499214

Epoch: 5| Step: 9
Training loss: 2.9086602173945835
Validation loss: 2.5081100762667328

Epoch: 5| Step: 10
Training loss: 2.959722667280148
Validation loss: 2.5184420140969865

Epoch: 5| Step: 11
Training loss: 2.970979194930349
Validation loss: 2.5350888894276467

Epoch: 252| Step: 0
Training loss: 2.607989188780346
Validation loss: 2.549333232460492

Epoch: 5| Step: 1
Training loss: 1.8430031379056184
Validation loss: 2.5706683475862673

Epoch: 5| Step: 2
Training loss: 2.654125665640425
Validation loss: 2.593865976077071

Epoch: 5| Step: 3
Training loss: 2.3787171236974634
Validation loss: 2.584868583763595

Epoch: 5| Step: 4
Training loss: 2.511673756340296
Validation loss: 2.5566786162659985

Epoch: 5| Step: 5
Training loss: 1.647291752529114
Validation loss: 2.5357491535121546

Epoch: 5| Step: 6
Training loss: 2.22863993853807
Validation loss: 2.5274826318236543

Epoch: 5| Step: 7
Training loss: 2.3332022789072737
Validation loss: 2.527789961403583

Epoch: 5| Step: 8
Training loss: 2.340835895892278
Validation loss: 2.5360909782781373

Epoch: 5| Step: 9
Training loss: 2.7461190147919847
Validation loss: 2.527752084188099

Epoch: 5| Step: 10
Training loss: 2.10125242306889
Validation loss: 2.550410319177433

Epoch: 5| Step: 11
Training loss: 2.3000534797753422
Validation loss: 2.556107265557082

Epoch: 253| Step: 0
Training loss: 2.726854893437361
Validation loss: 2.5359586083737664

Epoch: 5| Step: 1
Training loss: 1.6313864581446884
Validation loss: 2.5354663676831777

Epoch: 5| Step: 2
Training loss: 1.993430015692241
Validation loss: 2.535650110168216

Epoch: 5| Step: 3
Training loss: 1.7834325436004699
Validation loss: 2.533322302319536

Epoch: 5| Step: 4
Training loss: 2.047022457489728
Validation loss: 2.523374653162543

Epoch: 5| Step: 5
Training loss: 2.4875450781950157
Validation loss: 2.530351357614133

Epoch: 5| Step: 6
Training loss: 2.538007029722803
Validation loss: 2.5258782376516407

Epoch: 5| Step: 7
Training loss: 2.375440857777526
Validation loss: 2.5291948688218837

Epoch: 5| Step: 8
Training loss: 2.827713066997785
Validation loss: 2.539036669477484

Epoch: 5| Step: 9
Training loss: 2.38566943388352
Validation loss: 2.547465677161159

Epoch: 5| Step: 10
Training loss: 2.45332685478364
Validation loss: 2.5451999882200114

Epoch: 5| Step: 11
Training loss: 2.768700306879158
Validation loss: 2.5309569656233313

Epoch: 254| Step: 0
Training loss: 2.2588822625538327
Validation loss: 2.5411708157898443

Epoch: 5| Step: 1
Training loss: 2.9512143690583086
Validation loss: 2.533189515190261

Epoch: 5| Step: 2
Training loss: 1.9191276327788587
Validation loss: 2.5326598483630054

Epoch: 5| Step: 3
Training loss: 1.736042355341538
Validation loss: 2.532180057975902

Epoch: 5| Step: 4
Training loss: 2.2120874871651894
Validation loss: 2.515716906462086

Epoch: 5| Step: 5
Training loss: 2.6440990100391732
Validation loss: 2.512038381122353

Epoch: 5| Step: 6
Training loss: 2.188861314349219
Validation loss: 2.5114378507568147

Epoch: 5| Step: 7
Training loss: 1.9854888793472496
Validation loss: 2.5176343376408616

Epoch: 5| Step: 8
Training loss: 2.928786645351494
Validation loss: 2.5147155713252403

Epoch: 5| Step: 9
Training loss: 2.109277341489334
Validation loss: 2.523069154056615

Epoch: 5| Step: 10
Training loss: 2.270885385272545
Validation loss: 2.5298405636117525

Epoch: 5| Step: 11
Training loss: 1.9043221652968079
Validation loss: 2.557733883403986

Epoch: 255| Step: 0
Training loss: 1.6970602422664163
Validation loss: 2.537740388611467

Epoch: 5| Step: 1
Training loss: 2.2605474993789096
Validation loss: 2.5357976218721916

Epoch: 5| Step: 2
Training loss: 2.560926000488977
Validation loss: 2.5363102281531322

Epoch: 5| Step: 3
Training loss: 1.831198366758719
Validation loss: 2.532354031674507

Epoch: 5| Step: 4
Training loss: 2.5223181163896307
Validation loss: 2.5210971864957257

Epoch: 5| Step: 5
Training loss: 2.8513283829407094
Validation loss: 2.5172683174014163

Epoch: 5| Step: 6
Training loss: 2.182992596525044
Validation loss: 2.537220808307716

Epoch: 5| Step: 7
Training loss: 2.1385582741304496
Validation loss: 2.5291883938771034

Epoch: 5| Step: 8
Training loss: 2.0200603094634646
Validation loss: 2.5268474761787343

Epoch: 5| Step: 9
Training loss: 2.3017379248451397
Validation loss: 2.5230476503419728

Epoch: 5| Step: 10
Training loss: 2.9714595275618323
Validation loss: 2.536292880782953

Epoch: 5| Step: 11
Training loss: 1.6399584914978975
Validation loss: 2.5298910653242723

Epoch: 256| Step: 0
Training loss: 2.57854768004688
Validation loss: 2.524212311799743

Epoch: 5| Step: 1
Training loss: 2.1050282089989083
Validation loss: 2.5201963980905795

Epoch: 5| Step: 2
Training loss: 1.8874716194494938
Validation loss: 2.5179150624459212

Epoch: 5| Step: 3
Training loss: 2.3925044830822375
Validation loss: 2.511386807875007

Epoch: 5| Step: 4
Training loss: 1.836144098872676
Validation loss: 2.5109738421465817

Epoch: 5| Step: 5
Training loss: 2.2986428859495476
Validation loss: 2.5118742163496273

Epoch: 5| Step: 6
Training loss: 2.82535632426599
Validation loss: 2.5151899328806566

Epoch: 5| Step: 7
Training loss: 2.6571859169472694
Validation loss: 2.5137093952958276

Epoch: 5| Step: 8
Training loss: 2.3503506094550497
Validation loss: 2.5259852937261207

Epoch: 5| Step: 9
Training loss: 2.342288464386214
Validation loss: 2.5358346580951534

Epoch: 5| Step: 10
Training loss: 2.432293638001214
Validation loss: 2.566898598709159

Epoch: 5| Step: 11
Training loss: 1.279840671189575
Validation loss: 2.5701382663965067

Epoch: 257| Step: 0
Training loss: 2.2271438642744417
Validation loss: 2.566925855550324

Epoch: 5| Step: 1
Training loss: 1.892199971593428
Validation loss: 2.568447313628397

Epoch: 5| Step: 2
Training loss: 2.5707467590626187
Validation loss: 2.566373599390036

Epoch: 5| Step: 3
Training loss: 2.109626924517848
Validation loss: 2.5872855960700907

Epoch: 5| Step: 4
Training loss: 2.3933576536226875
Validation loss: 2.583472453237338

Epoch: 5| Step: 5
Training loss: 2.3376795164319275
Validation loss: 2.6040371608321493

Epoch: 5| Step: 6
Training loss: 2.515564532767559
Validation loss: 2.587355147329223

Epoch: 5| Step: 7
Training loss: 2.260182545809267
Validation loss: 2.5731264111256555

Epoch: 5| Step: 8
Training loss: 2.346989643329253
Validation loss: 2.55165019868669

Epoch: 5| Step: 9
Training loss: 2.4609302338992434
Validation loss: 2.5623684322107323

Epoch: 5| Step: 10
Training loss: 2.358639634081882
Validation loss: 2.534821839458037

Epoch: 5| Step: 11
Training loss: 1.4748867153048697
Validation loss: 2.5093304683586686

Epoch: 258| Step: 0
Training loss: 2.748360405166508
Validation loss: 2.523376314506

Epoch: 5| Step: 1
Training loss: 2.1932948061372
Validation loss: 2.5172652747374977

Epoch: 5| Step: 2
Training loss: 2.490829624017136
Validation loss: 2.520185439865733

Epoch: 5| Step: 3
Training loss: 2.7257858962951227
Validation loss: 2.5264547200393497

Epoch: 5| Step: 4
Training loss: 2.368737446693835
Validation loss: 2.5193771394029545

Epoch: 5| Step: 5
Training loss: 1.545695462037961
Validation loss: 2.524625811375042

Epoch: 5| Step: 6
Training loss: 1.8446332464652273
Validation loss: 2.532093500166404

Epoch: 5| Step: 7
Training loss: 2.154490762183941
Validation loss: 2.5348786729637123

Epoch: 5| Step: 8
Training loss: 2.3542232225314463
Validation loss: 2.5410901859416937

Epoch: 5| Step: 9
Training loss: 2.1805233463681137
Validation loss: 2.5535557346579445

Epoch: 5| Step: 10
Training loss: 1.8052572696815707
Validation loss: 2.567060165668104

Epoch: 5| Step: 11
Training loss: 4.334920348037334
Validation loss: 2.5516723898813694

Epoch: 259| Step: 0
Training loss: 1.4714036871304552
Validation loss: 2.580407228736448

Epoch: 5| Step: 1
Training loss: 2.4380647054070295
Validation loss: 2.6013222164350416

Epoch: 5| Step: 2
Training loss: 2.517478118164973
Validation loss: 2.565752806521633

Epoch: 5| Step: 3
Training loss: 2.276731641111268
Validation loss: 2.575474781412798

Epoch: 5| Step: 4
Training loss: 2.6721699869712667
Validation loss: 2.548281353490434

Epoch: 5| Step: 5
Training loss: 2.18451639790142
Validation loss: 2.5239048811389804

Epoch: 5| Step: 6
Training loss: 2.0668858222228224
Validation loss: 2.528615604983418

Epoch: 5| Step: 7
Training loss: 2.9250115679650723
Validation loss: 2.5044547128490366

Epoch: 5| Step: 8
Training loss: 2.1523748765968995
Validation loss: 2.500141342459082

Epoch: 5| Step: 9
Training loss: 1.9375756156686417
Validation loss: 2.4958157372106475

Epoch: 5| Step: 10
Training loss: 2.774140690591865
Validation loss: 2.4996676104832454

Epoch: 5| Step: 11
Training loss: 1.360955514361593
Validation loss: 2.504900020468376

Epoch: 260| Step: 0
Training loss: 2.109334309503367
Validation loss: 2.4896291378760824

Epoch: 5| Step: 1
Training loss: 2.062633394493691
Validation loss: 2.5225329987554335

Epoch: 5| Step: 2
Training loss: 2.5194557356052747
Validation loss: 2.5110887258393673

Epoch: 5| Step: 3
Training loss: 2.410977858813153
Validation loss: 2.501791550528447

Epoch: 5| Step: 4
Training loss: 1.8840548899767848
Validation loss: 2.5257270590855354

Epoch: 5| Step: 5
Training loss: 2.59357819505241
Validation loss: 2.5134236199084525

Epoch: 5| Step: 6
Training loss: 2.76264172013568
Validation loss: 2.5300245379777087

Epoch: 5| Step: 7
Training loss: 2.2101145584311146
Validation loss: 2.56789837895441

Epoch: 5| Step: 8
Training loss: 1.9223453597439004
Validation loss: 2.580837891588711

Epoch: 5| Step: 9
Training loss: 2.4216401017008136
Validation loss: 2.5920528159966256

Epoch: 5| Step: 10
Training loss: 2.486382399189143
Validation loss: 2.5983362287864513

Epoch: 5| Step: 11
Training loss: 2.161178484958532
Validation loss: 2.58943357139452

Epoch: 261| Step: 0
Training loss: 2.3453235684412577
Validation loss: 2.610216229764012

Epoch: 5| Step: 1
Training loss: 2.615412870530655
Validation loss: 2.5786253549564866

Epoch: 5| Step: 2
Training loss: 2.3295465008380174
Validation loss: 2.5688346258704406

Epoch: 5| Step: 3
Training loss: 2.196043605496583
Validation loss: 2.5446079594347193

Epoch: 5| Step: 4
Training loss: 2.3797329372944853
Validation loss: 2.5434563614137757

Epoch: 5| Step: 5
Training loss: 2.30412135525162
Validation loss: 2.5245467662057033

Epoch: 5| Step: 6
Training loss: 2.5415762796641697
Validation loss: 2.536000264586446

Epoch: 5| Step: 7
Training loss: 2.2206775727559176
Validation loss: 2.5257358850978777

Epoch: 5| Step: 8
Training loss: 2.1892229107196517
Validation loss: 2.50889883943339

Epoch: 5| Step: 9
Training loss: 2.04001662341058
Validation loss: 2.522980785473859

Epoch: 5| Step: 10
Training loss: 2.193638281952688
Validation loss: 2.5231921073714845

Epoch: 5| Step: 11
Training loss: 1.5222565597565754
Validation loss: 2.526747301523155

Epoch: 262| Step: 0
Training loss: 2.4191412478679766
Validation loss: 2.5277651514592696

Epoch: 5| Step: 1
Training loss: 2.3509666727252108
Validation loss: 2.5519117232772897

Epoch: 5| Step: 2
Training loss: 1.9421288970183381
Validation loss: 2.5877038353260335

Epoch: 5| Step: 3
Training loss: 2.583109066570474
Validation loss: 2.5594854456010867

Epoch: 5| Step: 4
Training loss: 2.198235354588725
Validation loss: 2.5367643000761477

Epoch: 5| Step: 5
Training loss: 1.8077931290151683
Validation loss: 2.5698926596298333

Epoch: 5| Step: 6
Training loss: 2.864634528425103
Validation loss: 2.543071254494494

Epoch: 5| Step: 7
Training loss: 2.447956642203234
Validation loss: 2.522831890394426

Epoch: 5| Step: 8
Training loss: 1.981298750509254
Validation loss: 2.522167815455105

Epoch: 5| Step: 9
Training loss: 2.4394307437111302
Validation loss: 2.5224225234177164

Epoch: 5| Step: 10
Training loss: 2.24292129046522
Validation loss: 2.520486403202775

Epoch: 5| Step: 11
Training loss: 1.6717820186249917
Validation loss: 2.5318628793660314

Epoch: 263| Step: 0
Training loss: 2.2056630983032184
Validation loss: 2.5085257786424884

Epoch: 5| Step: 1
Training loss: 2.1509340519498505
Validation loss: 2.4962773103832574

Epoch: 5| Step: 2
Training loss: 2.2773332808239206
Validation loss: 2.4891893655039796

Epoch: 5| Step: 3
Training loss: 1.9333233268522685
Validation loss: 2.499111695226164

Epoch: 5| Step: 4
Training loss: 2.7448789424641222
Validation loss: 2.4915022391242165

Epoch: 5| Step: 5
Training loss: 2.2355388464576045
Validation loss: 2.510896582785382

Epoch: 5| Step: 6
Training loss: 2.7238259122230364
Validation loss: 2.501516200281866

Epoch: 5| Step: 7
Training loss: 2.7623752099759544
Validation loss: 2.5078645742363475

Epoch: 5| Step: 8
Training loss: 2.2643282632324087
Validation loss: 2.5059149466636357

Epoch: 5| Step: 9
Training loss: 2.341339906676036
Validation loss: 2.513024820644134

Epoch: 5| Step: 10
Training loss: 1.9778628689371547
Validation loss: 2.521339335506647

Epoch: 5| Step: 11
Training loss: 1.363176926608096
Validation loss: 2.5379752232454678

Epoch: 264| Step: 0
Training loss: 2.1360333056335707
Validation loss: 2.5424906822816546

Epoch: 5| Step: 1
Training loss: 2.3258227543560097
Validation loss: 2.5682153233500786

Epoch: 5| Step: 2
Training loss: 2.3759519777229414
Validation loss: 2.5434097165536387

Epoch: 5| Step: 3
Training loss: 2.03486874356255
Validation loss: 2.5305219788550635

Epoch: 5| Step: 4
Training loss: 2.1215609603204224
Validation loss: 2.5464806241610902

Epoch: 5| Step: 5
Training loss: 1.7875572435509743
Validation loss: 2.5264546571267945

Epoch: 5| Step: 6
Training loss: 3.0607634311347995
Validation loss: 2.522499996862043

Epoch: 5| Step: 7
Training loss: 2.3405408123445506
Validation loss: 2.5186608045512138

Epoch: 5| Step: 8
Training loss: 2.081920971553246
Validation loss: 2.5118532238722135

Epoch: 5| Step: 9
Training loss: 2.577696146120099
Validation loss: 2.5338776140738837

Epoch: 5| Step: 10
Training loss: 2.3830592293496147
Validation loss: 2.5145033395132765

Epoch: 5| Step: 11
Training loss: 2.0522413886790285
Validation loss: 2.529068689841913

Epoch: 265| Step: 0
Training loss: 2.1015965341981584
Validation loss: 2.5281060713604404

Epoch: 5| Step: 1
Training loss: 1.949629868323437
Validation loss: 2.5173179111208666

Epoch: 5| Step: 2
Training loss: 2.098463759041422
Validation loss: 2.527943056127168

Epoch: 5| Step: 3
Training loss: 3.20444874276296
Validation loss: 2.5355098499319157

Epoch: 5| Step: 4
Training loss: 2.451305905511391
Validation loss: 2.53716755115823

Epoch: 5| Step: 5
Training loss: 2.116879711793488
Validation loss: 2.5310603667288056

Epoch: 5| Step: 6
Training loss: 1.9179106973536806
Validation loss: 2.521866966118162

Epoch: 5| Step: 7
Training loss: 1.8873010846348075
Validation loss: 2.5376933587917527

Epoch: 5| Step: 8
Training loss: 2.279870962038453
Validation loss: 2.5228080791244225

Epoch: 5| Step: 9
Training loss: 2.8973636745871896
Validation loss: 2.54389727273149

Epoch: 5| Step: 10
Training loss: 2.1353332379178633
Validation loss: 2.555066021360504

Epoch: 5| Step: 11
Training loss: 1.8188980566632775
Validation loss: 2.5672163475862977

Epoch: 266| Step: 0
Training loss: 2.439044169834229
Validation loss: 2.554161972370366

Epoch: 5| Step: 1
Training loss: 2.234169903925602
Validation loss: 2.585032178787947

Epoch: 5| Step: 2
Training loss: 1.978821500159941
Validation loss: 2.5748922135654437

Epoch: 5| Step: 3
Training loss: 3.008104661896156
Validation loss: 2.5868780686344226

Epoch: 5| Step: 4
Training loss: 2.1325702459788807
Validation loss: 2.591000614537817

Epoch: 5| Step: 5
Training loss: 2.371217425921472
Validation loss: 2.5629609941611977

Epoch: 5| Step: 6
Training loss: 2.527228563564027
Validation loss: 2.572277850153631

Epoch: 5| Step: 7
Training loss: 2.0896177365737643
Validation loss: 2.5998254232912528

Epoch: 5| Step: 8
Training loss: 2.3839980881190472
Validation loss: 2.5865893277137393

Epoch: 5| Step: 9
Training loss: 1.8539995982592496
Validation loss: 2.6047143372982093

Epoch: 5| Step: 10
Training loss: 2.1008574143204424
Validation loss: 2.5729749817101997

Epoch: 5| Step: 11
Training loss: 1.8445078052160169
Validation loss: 2.5618645764092536

Epoch: 267| Step: 0
Training loss: 2.843046667294962
Validation loss: 2.5160697875510185

Epoch: 5| Step: 1
Training loss: 1.9858037655969394
Validation loss: 2.5074490910740956

Epoch: 5| Step: 2
Training loss: 2.2715881796264488
Validation loss: 2.498043971970944

Epoch: 5| Step: 3
Training loss: 1.7947642658759695
Validation loss: 2.4956938891444675

Epoch: 5| Step: 4
Training loss: 1.911882615013458
Validation loss: 2.4945280550346776

Epoch: 5| Step: 5
Training loss: 2.9472596387758125
Validation loss: 2.5015692375421184

Epoch: 5| Step: 6
Training loss: 2.1451050593472587
Validation loss: 2.4952127437123126

Epoch: 5| Step: 7
Training loss: 2.254307015422144
Validation loss: 2.4928382573991428

Epoch: 5| Step: 8
Training loss: 2.8772478023941273
Validation loss: 2.5133212422232694

Epoch: 5| Step: 9
Training loss: 2.3890353633519514
Validation loss: 2.531527331351677

Epoch: 5| Step: 10
Training loss: 1.9494659328257806
Validation loss: 2.531630954411024

Epoch: 5| Step: 11
Training loss: 1.8013594685843783
Validation loss: 2.537068293011029

Epoch: 268| Step: 0
Training loss: 1.857434325873424
Validation loss: 2.5311659850004102

Epoch: 5| Step: 1
Training loss: 2.8097944173579963
Validation loss: 2.5283512228025034

Epoch: 5| Step: 2
Training loss: 2.1435021882807934
Validation loss: 2.5272242278541164

Epoch: 5| Step: 3
Training loss: 2.423493564333515
Validation loss: 2.5164058926509147

Epoch: 5| Step: 4
Training loss: 2.4024711761174564
Validation loss: 2.5160016711805135

Epoch: 5| Step: 5
Training loss: 2.0562467859121325
Validation loss: 2.512347947923859

Epoch: 5| Step: 6
Training loss: 2.5710841356930545
Validation loss: 2.5111144917588653

Epoch: 5| Step: 7
Training loss: 2.2760299112000366
Validation loss: 2.5164301237410625

Epoch: 5| Step: 8
Training loss: 2.497798236227287
Validation loss: 2.513561547700427

Epoch: 5| Step: 9
Training loss: 2.0222000403229834
Validation loss: 2.5255307392018658

Epoch: 5| Step: 10
Training loss: 2.0852964180478146
Validation loss: 2.5342155243306443

Epoch: 5| Step: 11
Training loss: 1.3474435389620645
Validation loss: 2.553526265436958

Epoch: 269| Step: 0
Training loss: 2.7313719907704503
Validation loss: 2.5832634629265745

Epoch: 5| Step: 1
Training loss: 2.2731685938206647
Validation loss: 2.5871956406681926

Epoch: 5| Step: 2
Training loss: 2.272732759815875
Validation loss: 2.6135624276699017

Epoch: 5| Step: 3
Training loss: 2.261812672616705
Validation loss: 2.5885849929243605

Epoch: 5| Step: 4
Training loss: 2.6471704023194307
Validation loss: 2.582621054487909

Epoch: 5| Step: 5
Training loss: 2.2258282437891763
Validation loss: 2.5728955956547335

Epoch: 5| Step: 6
Training loss: 2.2693809157462246
Validation loss: 2.5644886278773114

Epoch: 5| Step: 7
Training loss: 1.915144696159957
Validation loss: 2.547463542127049

Epoch: 5| Step: 8
Training loss: 2.789158239778345
Validation loss: 2.5464338220840137

Epoch: 5| Step: 9
Training loss: 1.5692787373627861
Validation loss: 2.5425024078655314

Epoch: 5| Step: 10
Training loss: 2.223112518730228
Validation loss: 2.548892285159401

Epoch: 5| Step: 11
Training loss: 1.5306303366415779
Validation loss: 2.5623547737713652

Epoch: 270| Step: 0
Training loss: 2.3899674851508177
Validation loss: 2.5305933041612043

Epoch: 5| Step: 1
Training loss: 2.486385467659152
Validation loss: 2.538657135516378

Epoch: 5| Step: 2
Training loss: 2.2853768005881445
Validation loss: 2.525560914661201

Epoch: 5| Step: 3
Training loss: 1.7048055074739754
Validation loss: 2.5321416108710384

Epoch: 5| Step: 4
Training loss: 2.51816066160914
Validation loss: 2.5230152733514752

Epoch: 5| Step: 5
Training loss: 2.236877640421253
Validation loss: 2.554519507807972

Epoch: 5| Step: 6
Training loss: 2.1417614224421966
Validation loss: 2.5193632321188173

Epoch: 5| Step: 7
Training loss: 2.142620298375677
Validation loss: 2.5399305141260946

Epoch: 5| Step: 8
Training loss: 1.9438383959329473
Validation loss: 2.54495256414717

Epoch: 5| Step: 9
Training loss: 2.4103441953411235
Validation loss: 2.5513028501923696

Epoch: 5| Step: 10
Training loss: 2.6000769787176625
Validation loss: 2.5550070824248112

Epoch: 5| Step: 11
Training loss: 1.577858250801444
Validation loss: 2.570048089616503

Epoch: 271| Step: 0
Training loss: 2.0972302108960563
Validation loss: 2.568737161794542

Epoch: 5| Step: 1
Training loss: 1.7787112833234058
Validation loss: 2.6021380203177134

Epoch: 5| Step: 2
Training loss: 2.7200653577413396
Validation loss: 2.6658958585151504

Epoch: 5| Step: 3
Training loss: 1.918095415520503
Validation loss: 2.654069042556324

Epoch: 5| Step: 4
Training loss: 1.8915169873726452
Validation loss: 2.624224510387183

Epoch: 5| Step: 5
Training loss: 2.6313820413907796
Validation loss: 2.6007759004803153

Epoch: 5| Step: 6
Training loss: 2.4220255158936483
Validation loss: 2.582952680281829

Epoch: 5| Step: 7
Training loss: 1.9471608155720521
Validation loss: 2.5779104008610547

Epoch: 5| Step: 8
Training loss: 2.621738405727468
Validation loss: 2.5663227781891385

Epoch: 5| Step: 9
Training loss: 2.4934107728991766
Validation loss: 2.543665507738695

Epoch: 5| Step: 10
Training loss: 2.329200547392157
Validation loss: 2.525921393346304

Epoch: 5| Step: 11
Training loss: 1.9105516894211025
Validation loss: 2.5092362692913572

Epoch: 272| Step: 0
Training loss: 2.3447691672449027
Validation loss: 2.5204092383615517

Epoch: 5| Step: 1
Training loss: 2.658448330615115
Validation loss: 2.5071873167823138

Epoch: 5| Step: 2
Training loss: 1.6751270929202067
Validation loss: 2.5039361326087928

Epoch: 5| Step: 3
Training loss: 2.2683841084504315
Validation loss: 2.506022819883481

Epoch: 5| Step: 4
Training loss: 1.8487850555748548
Validation loss: 2.5151309718883033

Epoch: 5| Step: 5
Training loss: 2.780063847693685
Validation loss: 2.523475548267409

Epoch: 5| Step: 6
Training loss: 2.139010746554807
Validation loss: 2.5238152724112326

Epoch: 5| Step: 7
Training loss: 2.8256875172098863
Validation loss: 2.536226204518757

Epoch: 5| Step: 8
Training loss: 1.5138812701687827
Validation loss: 2.5456965983101636

Epoch: 5| Step: 9
Training loss: 2.0745043656714
Validation loss: 2.569582405481923

Epoch: 5| Step: 10
Training loss: 1.9610715417529185
Validation loss: 2.5790397639527938

Epoch: 5| Step: 11
Training loss: 3.93038127379488
Validation loss: 2.600650772697995

Epoch: 273| Step: 0
Training loss: 2.2397143185184714
Validation loss: 2.638341322495379

Epoch: 5| Step: 1
Training loss: 2.609776722941949
Validation loss: 2.7051027446044213

Epoch: 5| Step: 2
Training loss: 2.5661465819749822
Validation loss: 2.7149068189288768

Epoch: 5| Step: 3
Training loss: 2.9789380783913817
Validation loss: 2.703916334625546

Epoch: 5| Step: 4
Training loss: 2.4285416921830656
Validation loss: 2.6755075170833993

Epoch: 5| Step: 5
Training loss: 2.5389515074899034
Validation loss: 2.6229198327523884

Epoch: 5| Step: 6
Training loss: 2.382928964238842
Validation loss: 2.548016520519237

Epoch: 5| Step: 7
Training loss: 1.7242902069426072
Validation loss: 2.520238606520055

Epoch: 5| Step: 8
Training loss: 2.366318848621433
Validation loss: 2.491975171362721

Epoch: 5| Step: 9
Training loss: 1.832795071336071
Validation loss: 2.475306779218896

Epoch: 5| Step: 10
Training loss: 2.0053536762886477
Validation loss: 2.465185072249757

Epoch: 5| Step: 11
Training loss: 1.9706590872495373
Validation loss: 2.4695542148293455

Epoch: 274| Step: 0
Training loss: 2.481150519925556
Validation loss: 2.472895061818471

Epoch: 5| Step: 1
Training loss: 2.367748433243583
Validation loss: 2.4812795480334833

Epoch: 5| Step: 2
Training loss: 2.1343066672337843
Validation loss: 2.4879441843942467

Epoch: 5| Step: 3
Training loss: 1.9507002771306283
Validation loss: 2.5169678533613173

Epoch: 5| Step: 4
Training loss: 2.0639412208290016
Validation loss: 2.534699928426609

Epoch: 5| Step: 5
Training loss: 2.3215306228172445
Validation loss: 2.5349121211317476

Epoch: 5| Step: 6
Training loss: 2.8746196661110632
Validation loss: 2.5698176161671387

Epoch: 5| Step: 7
Training loss: 2.6697165710945434
Validation loss: 2.5926476284617817

Epoch: 5| Step: 8
Training loss: 2.531345130168896
Validation loss: 2.611869442055935

Epoch: 5| Step: 9
Training loss: 2.483720609263524
Validation loss: 2.602420443944364

Epoch: 5| Step: 10
Training loss: 1.5281666443811903
Validation loss: 2.567326981014934

Epoch: 5| Step: 11
Training loss: 2.335973222644391
Validation loss: 2.5578618330131375

Epoch: 275| Step: 0
Training loss: 2.485522503023682
Validation loss: 2.5265420332916957

Epoch: 5| Step: 1
Training loss: 2.854310099392435
Validation loss: 2.512110286588508

Epoch: 5| Step: 2
Training loss: 1.9949048826062765
Validation loss: 2.50500444162791

Epoch: 5| Step: 3
Training loss: 2.1995904584642663
Validation loss: 2.4972820567658487

Epoch: 5| Step: 4
Training loss: 2.3436948642603315
Validation loss: 2.4842669905369785

Epoch: 5| Step: 5
Training loss: 2.092545347827579
Validation loss: 2.498164417005819

Epoch: 5| Step: 6
Training loss: 1.8509784533845333
Validation loss: 2.494309019628968

Epoch: 5| Step: 7
Training loss: 2.8268451640013454
Validation loss: 2.4928690856247258

Epoch: 5| Step: 8
Training loss: 1.398684772814884
Validation loss: 2.4976205787660923

Epoch: 5| Step: 9
Training loss: 2.4525997304130307
Validation loss: 2.5145513245431883

Epoch: 5| Step: 10
Training loss: 2.539996072360629
Validation loss: 2.5070722800677854

Epoch: 5| Step: 11
Training loss: 2.188637791779617
Validation loss: 2.5134464964151704

Epoch: 276| Step: 0
Training loss: 2.398815510438203
Validation loss: 2.5272624235122993

Epoch: 5| Step: 1
Training loss: 2.807411337562404
Validation loss: 2.524939162084804

Epoch: 5| Step: 2
Training loss: 1.8110465437386325
Validation loss: 2.5360281238044404

Epoch: 5| Step: 3
Training loss: 1.9929011244835437
Validation loss: 2.553677898944534

Epoch: 5| Step: 4
Training loss: 1.686836253424712
Validation loss: 2.55612133824667

Epoch: 5| Step: 5
Training loss: 1.969075312304106
Validation loss: 2.561035249344612

Epoch: 5| Step: 6
Training loss: 2.604013016300514
Validation loss: 2.5531864986849286

Epoch: 5| Step: 7
Training loss: 3.0165879683652905
Validation loss: 2.5521992689515027

Epoch: 5| Step: 8
Training loss: 2.180271084230679
Validation loss: 2.5661150468564546

Epoch: 5| Step: 9
Training loss: 2.1915461585657994
Validation loss: 2.5864681148971425

Epoch: 5| Step: 10
Training loss: 2.1389380720464546
Validation loss: 2.5591027217077817

Epoch: 5| Step: 11
Training loss: 1.3898490712673481
Validation loss: 2.557991690816798

Epoch: 277| Step: 0
Training loss: 1.824915685404353
Validation loss: 2.5829886211602946

Epoch: 5| Step: 1
Training loss: 1.8889023049507656
Validation loss: 2.549653743160891

Epoch: 5| Step: 2
Training loss: 2.647765307798844
Validation loss: 2.5552079334613804

Epoch: 5| Step: 3
Training loss: 2.3310759614917376
Validation loss: 2.556434803743729

Epoch: 5| Step: 4
Training loss: 2.7532340886546374
Validation loss: 2.5792419364578594

Epoch: 5| Step: 5
Training loss: 2.3228636822552313
Validation loss: 2.5987287749976766

Epoch: 5| Step: 6
Training loss: 2.6406536213604674
Validation loss: 2.610891373639902

Epoch: 5| Step: 7
Training loss: 2.0516239433065055
Validation loss: 2.6053279855841485

Epoch: 5| Step: 8
Training loss: 2.419407528949535
Validation loss: 2.5928504448689953

Epoch: 5| Step: 9
Training loss: 2.1363967011215377
Validation loss: 2.5638500781948776

Epoch: 5| Step: 10
Training loss: 2.205979790499017
Validation loss: 2.5545584542037707

Epoch: 5| Step: 11
Training loss: 1.7866929533196627
Validation loss: 2.5346928306658043

Epoch: 278| Step: 0
Training loss: 2.780714347777482
Validation loss: 2.515014246533075

Epoch: 5| Step: 1
Training loss: 1.8872884518133153
Validation loss: 2.5138340768991503

Epoch: 5| Step: 2
Training loss: 2.098877279706013
Validation loss: 2.5001027702030005

Epoch: 5| Step: 3
Training loss: 2.2368210428385966
Validation loss: 2.5203314010879434

Epoch: 5| Step: 4
Training loss: 2.129731688457336
Validation loss: 2.5142271529691373

Epoch: 5| Step: 5
Training loss: 2.201977478293293
Validation loss: 2.5103637418009033

Epoch: 5| Step: 6
Training loss: 2.17444968441528
Validation loss: 2.51784503509357

Epoch: 5| Step: 7
Training loss: 2.884261431374868
Validation loss: 2.516732822952875

Epoch: 5| Step: 8
Training loss: 1.946340143937359
Validation loss: 2.5297334547072605

Epoch: 5| Step: 9
Training loss: 2.3369132463172413
Validation loss: 2.5390710175200186

Epoch: 5| Step: 10
Training loss: 2.0219589192398915
Validation loss: 2.5450932251594045

Epoch: 5| Step: 11
Training loss: 2.4246651605107212
Validation loss: 2.5663673517926617

Epoch: 279| Step: 0
Training loss: 2.0575569335921533
Validation loss: 2.5806596183964743

Epoch: 5| Step: 1
Training loss: 3.0406726798134915
Validation loss: 2.6281359612973025

Epoch: 5| Step: 2
Training loss: 1.6726808121226129
Validation loss: 2.6450649870464824

Epoch: 5| Step: 3
Training loss: 2.184560053549336
Validation loss: 2.643074806604544

Epoch: 5| Step: 4
Training loss: 2.383323939733956
Validation loss: 2.6133286681384984

Epoch: 5| Step: 5
Training loss: 2.077973783340514
Validation loss: 2.573117871210136

Epoch: 5| Step: 6
Training loss: 2.4326820686475124
Validation loss: 2.554014768693931

Epoch: 5| Step: 7
Training loss: 2.457669852858975
Validation loss: 2.539019356385506

Epoch: 5| Step: 8
Training loss: 1.7631080038663909
Validation loss: 2.5204446561266876

Epoch: 5| Step: 9
Training loss: 2.888143598161891
Validation loss: 2.5307187535449276

Epoch: 5| Step: 10
Training loss: 2.0550511259601447
Validation loss: 2.5349856781163087

Epoch: 5| Step: 11
Training loss: 1.788505700767503
Validation loss: 2.524132376031031

Epoch: 280| Step: 0
Training loss: 2.1645970975299327
Validation loss: 2.521220714712333

Epoch: 5| Step: 1
Training loss: 2.4835988877763193
Validation loss: 2.5126446272690717

Epoch: 5| Step: 2
Training loss: 1.5492869367420676
Validation loss: 2.5003036711955344

Epoch: 5| Step: 3
Training loss: 1.7761885357610807
Validation loss: 2.50305235015557

Epoch: 5| Step: 4
Training loss: 2.5552023272645275
Validation loss: 2.5064601500123596

Epoch: 5| Step: 5
Training loss: 2.9464165922639753
Validation loss: 2.5031855473872966

Epoch: 5| Step: 6
Training loss: 1.985523942555665
Validation loss: 2.5258212488234184

Epoch: 5| Step: 7
Training loss: 1.7351791304754811
Validation loss: 2.523030483462115

Epoch: 5| Step: 8
Training loss: 2.0352295845862445
Validation loss: 2.5299996674893026

Epoch: 5| Step: 9
Training loss: 3.1151071743721284
Validation loss: 2.537653437173792

Epoch: 5| Step: 10
Training loss: 2.1645872946425446
Validation loss: 2.5573071610757623

Epoch: 5| Step: 11
Training loss: 1.6502179117514326
Validation loss: 2.624241734538714

Epoch: 281| Step: 0
Training loss: 2.2758588448315287
Validation loss: 2.652166912193273

Epoch: 5| Step: 1
Training loss: 2.4326908892137076
Validation loss: 2.710390195099099

Epoch: 5| Step: 2
Training loss: 2.381649746720169
Validation loss: 2.777207476299141

Epoch: 5| Step: 3
Training loss: 2.9451339222326127
Validation loss: 2.7820440794654213

Epoch: 5| Step: 4
Training loss: 2.6050864057799674
Validation loss: 2.7879301046546363

Epoch: 5| Step: 5
Training loss: 1.8410562783942386
Validation loss: 2.7195493904085173

Epoch: 5| Step: 6
Training loss: 2.082431203706419
Validation loss: 2.636791045528212

Epoch: 5| Step: 7
Training loss: 2.377900009440056
Validation loss: 2.6191265381754962

Epoch: 5| Step: 8
Training loss: 2.585276539396946
Validation loss: 2.5664097492227063

Epoch: 5| Step: 9
Training loss: 2.3709250174852845
Validation loss: 2.524575054807595

Epoch: 5| Step: 10
Training loss: 2.037397852484355
Validation loss: 2.5054388211204057

Epoch: 5| Step: 11
Training loss: 2.7484634615055294
Validation loss: 2.5138674494202045

Epoch: 282| Step: 0
Training loss: 2.1261595759823635
Validation loss: 2.4982604928899685

Epoch: 5| Step: 1
Training loss: 2.5602879164139534
Validation loss: 2.502123642013651

Epoch: 5| Step: 2
Training loss: 2.487632008025853
Validation loss: 2.4969833850213607

Epoch: 5| Step: 3
Training loss: 2.600136305830701
Validation loss: 2.508764525962101

Epoch: 5| Step: 4
Training loss: 2.016646015702539
Validation loss: 2.501832493085743

Epoch: 5| Step: 5
Training loss: 2.07077401253404
Validation loss: 2.52096082559392

Epoch: 5| Step: 6
Training loss: 2.04803690073923
Validation loss: 2.5308053524851655

Epoch: 5| Step: 7
Training loss: 2.0052218217932767
Validation loss: 2.5518971290908867

Epoch: 5| Step: 8
Training loss: 2.3104780481943004
Validation loss: 2.567884277959832

Epoch: 5| Step: 9
Training loss: 2.1026540424465447
Validation loss: 2.594493835908472

Epoch: 5| Step: 10
Training loss: 2.668489329988084
Validation loss: 2.6140446139565676

Epoch: 5| Step: 11
Training loss: 3.358391054349983
Validation loss: 2.61606076787199

Epoch: 283| Step: 0
Training loss: 2.3705330048946633
Validation loss: 2.6515023606268446

Epoch: 5| Step: 1
Training loss: 2.4011599717745775
Validation loss: 2.694703461764438

Epoch: 5| Step: 2
Training loss: 2.006702398233401
Validation loss: 2.697735564340692

Epoch: 5| Step: 3
Training loss: 2.69219578206476
Validation loss: 2.6909855558550433

Epoch: 5| Step: 4
Training loss: 2.3701252349729467
Validation loss: 2.6857740049549084

Epoch: 5| Step: 5
Training loss: 2.3773027600162253
Validation loss: 2.64403844887584

Epoch: 5| Step: 6
Training loss: 1.8099254212373737
Validation loss: 2.620010250571391

Epoch: 5| Step: 7
Training loss: 2.0389869646719303
Validation loss: 2.5388529255768666

Epoch: 5| Step: 8
Training loss: 2.5986712545012205
Validation loss: 2.5239505935111546

Epoch: 5| Step: 9
Training loss: 2.874065247211178
Validation loss: 2.5142663323416024

Epoch: 5| Step: 10
Training loss: 2.0285844185526467
Validation loss: 2.492490604507915

Epoch: 5| Step: 11
Training loss: 2.7559374089213438
Validation loss: 2.4666391877521066

Epoch: 284| Step: 0
Training loss: 3.116648538982147
Validation loss: 2.504220428037804

Epoch: 5| Step: 1
Training loss: 2.458732076947079
Validation loss: 2.49035450764025

Epoch: 5| Step: 2
Training loss: 2.2898962175598316
Validation loss: 2.491540328515307

Epoch: 5| Step: 3
Training loss: 2.2225178707335473
Validation loss: 2.4897074083244357

Epoch: 5| Step: 4
Training loss: 2.5558793673636355
Validation loss: 2.4921625747948957

Epoch: 5| Step: 5
Training loss: 1.5763473318544143
Validation loss: 2.4957303144265848

Epoch: 5| Step: 6
Training loss: 2.5828326160457564
Validation loss: 2.487662445527708

Epoch: 5| Step: 7
Training loss: 2.5841042281054856
Validation loss: 2.49707630779853

Epoch: 5| Step: 8
Training loss: 1.8191284126853138
Validation loss: 2.4990893692277325

Epoch: 5| Step: 9
Training loss: 1.902310662892446
Validation loss: 2.4944989239049424

Epoch: 5| Step: 10
Training loss: 2.3610960891345494
Validation loss: 2.5286805844235283

Epoch: 5| Step: 11
Training loss: 2.3628857494473587
Validation loss: 2.549663444823553

Epoch: 285| Step: 0
Training loss: 2.232158388356263
Validation loss: 2.564859989892194

Epoch: 5| Step: 1
Training loss: 2.507870777439346
Validation loss: 2.603728644090703

Epoch: 5| Step: 2
Training loss: 2.533419962288579
Validation loss: 2.6216721140612322

Epoch: 5| Step: 3
Training loss: 1.767542110518806
Validation loss: 2.630342062140862

Epoch: 5| Step: 4
Training loss: 2.4315147275390414
Validation loss: 2.6243153807071526

Epoch: 5| Step: 5
Training loss: 2.4500455540684336
Validation loss: 2.653495703216739

Epoch: 5| Step: 6
Training loss: 2.633948740293117
Validation loss: 2.604429957749302

Epoch: 5| Step: 7
Training loss: 2.02222350240441
Validation loss: 2.5933837210700914

Epoch: 5| Step: 8
Training loss: 2.262161343345973
Validation loss: 2.560347467102458

Epoch: 5| Step: 9
Training loss: 1.8288510176944088
Validation loss: 2.5376797476183777

Epoch: 5| Step: 10
Training loss: 2.446752057452032
Validation loss: 2.5272584416310777

Epoch: 5| Step: 11
Training loss: 2.396623155077751
Validation loss: 2.513351499053797

Epoch: 286| Step: 0
Training loss: 2.2676652863645166
Validation loss: 2.50771202052482

Epoch: 5| Step: 1
Training loss: 1.8374174605535138
Validation loss: 2.5184447397759993

Epoch: 5| Step: 2
Training loss: 2.4562678523300625
Validation loss: 2.479920894966028

Epoch: 5| Step: 3
Training loss: 2.223130857617048
Validation loss: 2.5181372716810912

Epoch: 5| Step: 4
Training loss: 2.6666927833072958
Validation loss: 2.5000777590102814

Epoch: 5| Step: 5
Training loss: 2.724263705951204
Validation loss: 2.5087206276696468

Epoch: 5| Step: 6
Training loss: 2.450731117774285
Validation loss: 2.5088508730003483

Epoch: 5| Step: 7
Training loss: 2.2230581446502584
Validation loss: 2.492716578623909

Epoch: 5| Step: 8
Training loss: 1.9420162601124162
Validation loss: 2.505713978189253

Epoch: 5| Step: 9
Training loss: 1.7062157400415552
Validation loss: 2.49800261338237

Epoch: 5| Step: 10
Training loss: 2.242722397645141
Validation loss: 2.5131498405799286

Epoch: 5| Step: 11
Training loss: 2.249013684575032
Validation loss: 2.5121945394755936

Epoch: 287| Step: 0
Training loss: 2.462615878869035
Validation loss: 2.5042969254580134

Epoch: 5| Step: 1
Training loss: 2.3962500721150897
Validation loss: 2.4990329739765182

Epoch: 5| Step: 2
Training loss: 2.323997987152121
Validation loss: 2.5101006389417466

Epoch: 5| Step: 3
Training loss: 2.7231548186694914
Validation loss: 2.4958003055210694

Epoch: 5| Step: 4
Training loss: 2.2308832374703242
Validation loss: 2.505744596465665

Epoch: 5| Step: 5
Training loss: 2.2491735424018158
Validation loss: 2.505212011883124

Epoch: 5| Step: 6
Training loss: 1.809877405598821
Validation loss: 2.4955103017700995

Epoch: 5| Step: 7
Training loss: 1.8680167810637938
Validation loss: 2.5125273119452602

Epoch: 5| Step: 8
Training loss: 2.61544906042714
Validation loss: 2.5198727457617456

Epoch: 5| Step: 9
Training loss: 2.134950565239304
Validation loss: 2.527731451517135

Epoch: 5| Step: 10
Training loss: 1.653263061757956
Validation loss: 2.532289546639797

Epoch: 5| Step: 11
Training loss: 2.9527080584676906
Validation loss: 2.5457975998996556

Epoch: 288| Step: 0
Training loss: 1.8540195949675289
Validation loss: 2.5511277427393804

Epoch: 5| Step: 1
Training loss: 2.326975596320856
Validation loss: 2.54136443425717

Epoch: 5| Step: 2
Training loss: 1.9396362218259922
Validation loss: 2.5679454436065163

Epoch: 5| Step: 3
Training loss: 2.182552737792851
Validation loss: 2.54795242018319

Epoch: 5| Step: 4
Training loss: 2.1525752502118727
Validation loss: 2.550522222993075

Epoch: 5| Step: 5
Training loss: 2.3984587727451365
Validation loss: 2.5402011333200747

Epoch: 5| Step: 6
Training loss: 2.4181266079895085
Validation loss: 2.564435553508679

Epoch: 5| Step: 7
Training loss: 2.788228997670551
Validation loss: 2.542492002926593

Epoch: 5| Step: 8
Training loss: 1.9438065670895908
Validation loss: 2.524947644625704

Epoch: 5| Step: 9
Training loss: 2.529962094290617
Validation loss: 2.535586574809593

Epoch: 5| Step: 10
Training loss: 2.1001307492197623
Validation loss: 2.547256694657735

Epoch: 5| Step: 11
Training loss: 2.1902917758147895
Validation loss: 2.556403047788018

Epoch: 289| Step: 0
Training loss: 1.7310981380910768
Validation loss: 2.548688823844447

Epoch: 5| Step: 1
Training loss: 1.8130120179019455
Validation loss: 2.5692445416628997

Epoch: 5| Step: 2
Training loss: 2.241676724303797
Validation loss: 2.5569690272919825

Epoch: 5| Step: 3
Training loss: 2.0429297711536054
Validation loss: 2.586787096735717

Epoch: 5| Step: 4
Training loss: 2.203372265081753
Validation loss: 2.6237763360785533

Epoch: 5| Step: 5
Training loss: 2.534101222593063
Validation loss: 2.601217214526528

Epoch: 5| Step: 6
Training loss: 1.787800172797627
Validation loss: 2.6011951824915496

Epoch: 5| Step: 7
Training loss: 2.1669512586265487
Validation loss: 2.590969337829395

Epoch: 5| Step: 8
Training loss: 2.4548906908736656
Validation loss: 2.615840185498121

Epoch: 5| Step: 9
Training loss: 2.8224766424680703
Validation loss: 2.606708012655003

Epoch: 5| Step: 10
Training loss: 2.3814604378376902
Validation loss: 2.6131814045214226

Epoch: 5| Step: 11
Training loss: 2.3759997421996624
Validation loss: 2.5814239915146864

Epoch: 290| Step: 0
Training loss: 1.6190195072829632
Validation loss: 2.57426320232009

Epoch: 5| Step: 1
Training loss: 2.295479071320433
Validation loss: 2.5644567663433704

Epoch: 5| Step: 2
Training loss: 2.060051476103934
Validation loss: 2.5537158584304187

Epoch: 5| Step: 3
Training loss: 2.5203813405104034
Validation loss: 2.555668882492509

Epoch: 5| Step: 4
Training loss: 2.13120564165686
Validation loss: 2.543289664353143

Epoch: 5| Step: 5
Training loss: 2.816312283073995
Validation loss: 2.5221072057361695

Epoch: 5| Step: 6
Training loss: 2.581600274434295
Validation loss: 2.5559887425322017

Epoch: 5| Step: 7
Training loss: 1.5801120749694026
Validation loss: 2.539165001780915

Epoch: 5| Step: 8
Training loss: 2.0667817724638984
Validation loss: 2.550979411833779

Epoch: 5| Step: 9
Training loss: 2.251356881340545
Validation loss: 2.522936193473046

Epoch: 5| Step: 10
Training loss: 2.3835841195891265
Validation loss: 2.541637945534028

Epoch: 5| Step: 11
Training loss: 2.6641432226933577
Validation loss: 2.5370266504189836

Epoch: 291| Step: 0
Training loss: 2.6113498547034384
Validation loss: 2.5553903108767058

Epoch: 5| Step: 1
Training loss: 2.090894545440441
Validation loss: 2.545154009576499

Epoch: 5| Step: 2
Training loss: 2.2341336206621896
Validation loss: 2.5408134761216608

Epoch: 5| Step: 3
Training loss: 2.406441519905856
Validation loss: 2.541433078684065

Epoch: 5| Step: 4
Training loss: 2.1969012284646285
Validation loss: 2.549002751279204

Epoch: 5| Step: 5
Training loss: 2.06377128955158
Validation loss: 2.54776136112659

Epoch: 5| Step: 6
Training loss: 2.4275800881497407
Validation loss: 2.573046338652852

Epoch: 5| Step: 7
Training loss: 1.8937743887259346
Validation loss: 2.5811046336192582

Epoch: 5| Step: 8
Training loss: 2.2392165009272555
Validation loss: 2.5970949297892454

Epoch: 5| Step: 9
Training loss: 2.315603699373818
Validation loss: 2.610600844084188

Epoch: 5| Step: 10
Training loss: 2.316287676138871
Validation loss: 2.5914156455390143

Epoch: 5| Step: 11
Training loss: 1.076203887518706
Validation loss: 2.60076763469982

Epoch: 292| Step: 0
Training loss: 2.2603926650031405
Validation loss: 2.5679157178490075

Epoch: 5| Step: 1
Training loss: 2.5879324510181316
Validation loss: 2.555316672585892

Epoch: 5| Step: 2
Training loss: 2.571472833646058
Validation loss: 2.530155632116182

Epoch: 5| Step: 3
Training loss: 1.9896963064131548
Validation loss: 2.515746238109009

Epoch: 5| Step: 4
Training loss: 2.478276282523708
Validation loss: 2.523493393159918

Epoch: 5| Step: 5
Training loss: 2.142264970064472
Validation loss: 2.53257015323158

Epoch: 5| Step: 6
Training loss: 1.8267634611634842
Validation loss: 2.5341739367822216

Epoch: 5| Step: 7
Training loss: 2.607146356434723
Validation loss: 2.555710357374983

Epoch: 5| Step: 8
Training loss: 2.428702396980611
Validation loss: 2.5481154457607427

Epoch: 5| Step: 9
Training loss: 2.1066236849315425
Validation loss: 2.5567037459494304

Epoch: 5| Step: 10
Training loss: 1.102385788189644
Validation loss: 2.551760101609311

Epoch: 5| Step: 11
Training loss: 2.16636430635785
Validation loss: 2.5262066593884933

Epoch: 293| Step: 0
Training loss: 1.8930007258039514
Validation loss: 2.5147984846958797

Epoch: 5| Step: 1
Training loss: 2.65982855796453
Validation loss: 2.5246834724936518

Epoch: 5| Step: 2
Training loss: 2.1216040009830546
Validation loss: 2.5074932711229847

Epoch: 5| Step: 3
Training loss: 2.17749416241044
Validation loss: 2.5080193645210356

Epoch: 5| Step: 4
Training loss: 2.385624661324417
Validation loss: 2.512357034458901

Epoch: 5| Step: 5
Training loss: 2.0434455388155026
Validation loss: 2.4992368963977376

Epoch: 5| Step: 6
Training loss: 1.963491653358587
Validation loss: 2.5064504852396117

Epoch: 5| Step: 7
Training loss: 2.9731900391312127
Validation loss: 2.506688538534748

Epoch: 5| Step: 8
Training loss: 2.132811605711809
Validation loss: 2.5242155389303904

Epoch: 5| Step: 9
Training loss: 1.854287954356801
Validation loss: 2.540203706596385

Epoch: 5| Step: 10
Training loss: 2.203922641372712
Validation loss: 2.5403901135862674

Epoch: 5| Step: 11
Training loss: 1.6951448559824236
Validation loss: 2.5779046976002653

Epoch: 294| Step: 0
Training loss: 2.385733792965174
Validation loss: 2.6422433126500127

Epoch: 5| Step: 1
Training loss: 2.0974238034254156
Validation loss: 2.682763357569607

Epoch: 5| Step: 2
Training loss: 2.3480933063430354
Validation loss: 2.66051912750201

Epoch: 5| Step: 3
Training loss: 2.8644256178470098
Validation loss: 2.704651917668498

Epoch: 5| Step: 4
Training loss: 2.1494989166079566
Validation loss: 2.638700374484189

Epoch: 5| Step: 5
Training loss: 2.2311262507809433
Validation loss: 2.6390871991269718

Epoch: 5| Step: 6
Training loss: 1.7875797840799426
Validation loss: 2.586447795063437

Epoch: 5| Step: 7
Training loss: 1.6523006063830394
Validation loss: 2.534346815219858

Epoch: 5| Step: 8
Training loss: 2.3017818431870314
Validation loss: 2.5236005267852804

Epoch: 5| Step: 9
Training loss: 2.3045542532985603
Validation loss: 2.5089096054235585

Epoch: 5| Step: 10
Training loss: 2.7279959247200694
Validation loss: 2.5066190793482335

Epoch: 5| Step: 11
Training loss: 3.4498747982582247
Validation loss: 2.5108832022100174

Epoch: 295| Step: 0
Training loss: 1.8334522064279766
Validation loss: 2.5186797248180572

Epoch: 5| Step: 1
Training loss: 1.5603006524358762
Validation loss: 2.5111058873241334

Epoch: 5| Step: 2
Training loss: 2.158877333887555
Validation loss: 2.5452850426179765

Epoch: 5| Step: 3
Training loss: 2.0764430991261693
Validation loss: 2.547402391650807

Epoch: 5| Step: 4
Training loss: 2.301709543206758
Validation loss: 2.5891798059213804

Epoch: 5| Step: 5
Training loss: 2.5393344205355084
Validation loss: 2.6158120749663896

Epoch: 5| Step: 6
Training loss: 2.7739686269326818
Validation loss: 2.649644417268348

Epoch: 5| Step: 7
Training loss: 2.1133373203701273
Validation loss: 2.6558236060389797

Epoch: 5| Step: 8
Training loss: 2.3701011930663967
Validation loss: 2.6371897260332013

Epoch: 5| Step: 9
Training loss: 2.086281623029259
Validation loss: 2.635743721481806

Epoch: 5| Step: 10
Training loss: 2.5392765835227467
Validation loss: 2.621117176267691

Epoch: 5| Step: 11
Training loss: 2.1540215075029208
Validation loss: 2.6020462495183705

Epoch: 296| Step: 0
Training loss: 2.907504559527272
Validation loss: 2.5875218312176407

Epoch: 5| Step: 1
Training loss: 2.760595284687015
Validation loss: 2.546270907320417

Epoch: 5| Step: 2
Training loss: 1.6832593687570983
Validation loss: 2.517955253613036

Epoch: 5| Step: 3
Training loss: 2.2571114372356758
Validation loss: 2.4953525638728102

Epoch: 5| Step: 4
Training loss: 2.2598063499058294
Validation loss: 2.4767423779037996

Epoch: 5| Step: 5
Training loss: 2.06186122828403
Validation loss: 2.468717562788317

Epoch: 5| Step: 6
Training loss: 1.6300580460211953
Validation loss: 2.4760705998877657

Epoch: 5| Step: 7
Training loss: 2.341032359959165
Validation loss: 2.4774226972162308

Epoch: 5| Step: 8
Training loss: 2.305705010754553
Validation loss: 2.4930256676778773

Epoch: 5| Step: 9
Training loss: 2.580047537934941
Validation loss: 2.5131586633284084

Epoch: 5| Step: 10
Training loss: 2.015999574381163
Validation loss: 2.5360446778011116

Epoch: 5| Step: 11
Training loss: 1.2343920935883788
Validation loss: 2.570986681547701

Epoch: 297| Step: 0
Training loss: 2.3914313857444016
Validation loss: 2.5849806176313024

Epoch: 5| Step: 1
Training loss: 2.300396768928054
Validation loss: 2.6264567533738274

Epoch: 5| Step: 2
Training loss: 1.9932928272690666
Validation loss: 2.611681860715451

Epoch: 5| Step: 3
Training loss: 1.8954113361347698
Validation loss: 2.58713379355508

Epoch: 5| Step: 4
Training loss: 2.2644699835696733
Validation loss: 2.5702487102813647

Epoch: 5| Step: 5
Training loss: 2.3623902714188927
Validation loss: 2.5692432154365776

Epoch: 5| Step: 6
Training loss: 2.477050348120527
Validation loss: 2.5504488414419035

Epoch: 5| Step: 7
Training loss: 2.5098194873536905
Validation loss: 2.5346106014031804

Epoch: 5| Step: 8
Training loss: 2.2123590758535276
Validation loss: 2.545500874129998

Epoch: 5| Step: 9
Training loss: 2.479727855178526
Validation loss: 2.5463880334730646

Epoch: 5| Step: 10
Training loss: 1.7433463719879043
Validation loss: 2.542537291255504

Epoch: 5| Step: 11
Training loss: 1.290400417637388
Validation loss: 2.5423618223276203

Epoch: 298| Step: 0
Training loss: 2.296505437690448
Validation loss: 2.5399585961924673

Epoch: 5| Step: 1
Training loss: 2.1044654508162397
Validation loss: 2.560286057857667

Epoch: 5| Step: 2
Training loss: 2.593907638723152
Validation loss: 2.548809069955662

Epoch: 5| Step: 3
Training loss: 1.4762307253490972
Validation loss: 2.5866406839784672

Epoch: 5| Step: 4
Training loss: 1.8333007708460953
Validation loss: 2.568704359046654

Epoch: 5| Step: 5
Training loss: 2.7789006114920745
Validation loss: 2.5481790819714165

Epoch: 5| Step: 6
Training loss: 2.531701789379606
Validation loss: 2.560572923052041

Epoch: 5| Step: 7
Training loss: 2.3611525351187925
Validation loss: 2.5304140625811558

Epoch: 5| Step: 8
Training loss: 1.3357700889943058
Validation loss: 2.539888026699231

Epoch: 5| Step: 9
Training loss: 2.4959839034050537
Validation loss: 2.5516957410388916

Epoch: 5| Step: 10
Training loss: 2.0485320627367654
Validation loss: 2.54551064235421

Epoch: 5| Step: 11
Training loss: 1.4353138429588514
Validation loss: 2.5613921336994285

Epoch: 299| Step: 0
Training loss: 2.4828596952068724
Validation loss: 2.5734782410457693

Epoch: 5| Step: 1
Training loss: 2.757169221629173
Validation loss: 2.5730248761813552

Epoch: 5| Step: 2
Training loss: 1.9028404887586294
Validation loss: 2.610270664356769

Epoch: 5| Step: 3
Training loss: 2.0994174603423987
Validation loss: 2.618122021286467

Epoch: 5| Step: 4
Training loss: 2.3159194614921135
Validation loss: 2.650513888111536

Epoch: 5| Step: 5
Training loss: 2.270895989163179
Validation loss: 2.6709512708832697

Epoch: 5| Step: 6
Training loss: 1.982499024341627
Validation loss: 2.6382408928978

Epoch: 5| Step: 7
Training loss: 2.3027320971813374
Validation loss: 2.6299256402612854

Epoch: 5| Step: 8
Training loss: 2.3735095919119327
Validation loss: 2.5655108495758068

Epoch: 5| Step: 9
Training loss: 1.8069527681149058
Validation loss: 2.565820724914321

Epoch: 5| Step: 10
Training loss: 2.0659349910113245
Validation loss: 2.5461320554058795

Epoch: 5| Step: 11
Training loss: 1.871845834851929
Validation loss: 2.5428211268925254

Epoch: 300| Step: 0
Training loss: 1.9464704751519937
Validation loss: 2.540352058607882

Epoch: 5| Step: 1
Training loss: 2.5096672068494943
Validation loss: 2.529466901683277

Epoch: 5| Step: 2
Training loss: 1.82013082620943
Validation loss: 2.5484066474648324

Epoch: 5| Step: 3
Training loss: 2.1170960579019056
Validation loss: 2.5267268218434302

Epoch: 5| Step: 4
Training loss: 1.741630704712305
Validation loss: 2.5399818321076393

Epoch: 5| Step: 5
Training loss: 1.7786632293103664
Validation loss: 2.5563589416395476

Epoch: 5| Step: 6
Training loss: 2.6515740020628367
Validation loss: 2.5782269428790157

Epoch: 5| Step: 7
Training loss: 2.3297451449655866
Validation loss: 2.589520805864313

Epoch: 5| Step: 8
Training loss: 2.188113208380501
Validation loss: 2.6163399761710338

Epoch: 5| Step: 9
Training loss: 1.838898952734467
Validation loss: 2.6337955348890394

Epoch: 5| Step: 10
Training loss: 2.7813048625206926
Validation loss: 2.6670203359341347

Epoch: 5| Step: 11
Training loss: 3.347623203964666
Validation loss: 2.6473402978231824

Testing loss: 2.1475370040075896

Epoch: 1| Step: 0
Training loss: 4.7148895263671875
Validation loss: 5.237225368458738

Epoch: 5| Step: 1
Training loss: 4.471518516540527
Validation loss: 5.219026980861541

Epoch: 5| Step: 2
Training loss: 5.04766845703125
Validation loss: 5.200089449523597

Epoch: 5| Step: 3
Training loss: 3.7034060955047607
Validation loss: 5.181180779651929

Epoch: 5| Step: 4
Training loss: 6.057805061340332
Validation loss: 5.160354491203062

Epoch: 5| Step: 5
Training loss: 5.242269039154053
Validation loss: 5.137032724195911

Epoch: 5| Step: 6
Training loss: 6.239242076873779
Validation loss: 5.110116384362661

Epoch: 5| Step: 7
Training loss: 3.9542860984802246
Validation loss: 5.079904930565947

Epoch: 5| Step: 8
Training loss: 4.194796562194824
Validation loss: 5.046187236744871

Epoch: 5| Step: 9
Training loss: 5.295420169830322
Validation loss: 5.00899996552416

Epoch: 5| Step: 10
Training loss: 5.201286315917969
Validation loss: 4.967790147309662

Epoch: 2| Step: 0
Training loss: 4.134610652923584
Validation loss: 4.923561106445969

Epoch: 5| Step: 1
Training loss: 4.4962873458862305
Validation loss: 4.877088557007492

Epoch: 5| Step: 2
Training loss: 4.19970178604126
Validation loss: 4.82810688531527

Epoch: 5| Step: 3
Training loss: 5.623086452484131
Validation loss: 4.775680818865376

Epoch: 5| Step: 4
Training loss: 4.407658576965332
Validation loss: 4.720178193943475

Epoch: 5| Step: 5
Training loss: 3.8078904151916504
Validation loss: 4.663474275219825

Epoch: 5| Step: 6
Training loss: 5.3611674308776855
Validation loss: 4.603496028530982

Epoch: 5| Step: 7
Training loss: 3.2773735523223877
Validation loss: 4.540018179083384

Epoch: 5| Step: 8
Training loss: 4.110354423522949
Validation loss: 4.474301891942178

Epoch: 5| Step: 9
Training loss: 5.091607093811035
Validation loss: 4.4082115132321595

Epoch: 5| Step: 10
Training loss: 4.288666248321533
Validation loss: 4.343744872718729

Epoch: 3| Step: 0
Training loss: 4.314873695373535
Validation loss: 4.28135065109499

Epoch: 5| Step: 1
Training loss: 3.8607516288757324
Validation loss: 4.2220148681312475

Epoch: 5| Step: 2
Training loss: 4.862936973571777
Validation loss: 4.171272805942002

Epoch: 5| Step: 3
Training loss: 4.345335960388184
Validation loss: 4.12171051579137

Epoch: 5| Step: 4
Training loss: 3.7893059253692627
Validation loss: 4.0768090576253915

Epoch: 5| Step: 5
Training loss: 3.844273805618286
Validation loss: 4.0372830847258205

Epoch: 5| Step: 6
Training loss: 3.8809616565704346
Validation loss: 3.9981822300982732

Epoch: 5| Step: 7
Training loss: 3.4803078174591064
Validation loss: 3.955766118982787

Epoch: 5| Step: 8
Training loss: 3.7877471446990967
Validation loss: 3.9173079870080434

Epoch: 5| Step: 9
Training loss: 2.847273349761963
Validation loss: 3.8858085370832876

Epoch: 5| Step: 10
Training loss: 3.8605597019195557
Validation loss: 3.866389254088043

Epoch: 4| Step: 0
Training loss: 4.723402500152588
Validation loss: 3.8554073046612483

Epoch: 5| Step: 1
Training loss: 3.6255593299865723
Validation loss: 3.830392732415148

Epoch: 5| Step: 2
Training loss: 3.4287033081054688
Validation loss: 3.797015702852639

Epoch: 5| Step: 3
Training loss: 4.007086753845215
Validation loss: 3.772728189345329

Epoch: 5| Step: 4
Training loss: 3.481679916381836
Validation loss: 3.743609756551763

Epoch: 5| Step: 5
Training loss: 3.4822592735290527
Validation loss: 3.7182650155918573

Epoch: 5| Step: 6
Training loss: 3.678539752960205
Validation loss: 3.699410028355096

Epoch: 5| Step: 7
Training loss: 4.054163932800293
Validation loss: 3.6879989408677623

Epoch: 5| Step: 8
Training loss: 2.143174648284912
Validation loss: 3.665403691671228

Epoch: 5| Step: 9
Training loss: 4.665165901184082
Validation loss: 3.641579971518568

Epoch: 5| Step: 10
Training loss: 2.5450642108917236
Validation loss: 3.6165213636172715

Epoch: 5| Step: 0
Training loss: 2.738450527191162
Validation loss: 3.5914465560707995

Epoch: 5| Step: 1
Training loss: 3.784910202026367
Validation loss: 3.5714959149719565

Epoch: 5| Step: 2
Training loss: 3.0011863708496094
Validation loss: 3.5556944442051712

Epoch: 5| Step: 3
Training loss: 3.5757362842559814
Validation loss: 3.5424257093860256

Epoch: 5| Step: 4
Training loss: 3.6673026084899902
Validation loss: 3.5302688050013717

Epoch: 5| Step: 5
Training loss: 4.07724142074585
Validation loss: 3.507112164651194

Epoch: 5| Step: 6
Training loss: 3.4574666023254395
Validation loss: 3.4838247658104025

Epoch: 5| Step: 7
Training loss: 4.2324018478393555
Validation loss: 3.461195335593275

Epoch: 5| Step: 8
Training loss: 2.9249019622802734
Validation loss: 3.4414033069405505

Epoch: 5| Step: 9
Training loss: 3.276940107345581
Validation loss: 3.421372752035818

Epoch: 5| Step: 10
Training loss: 3.0063977241516113
Validation loss: 3.404617517225204

Epoch: 6| Step: 0
Training loss: 3.9869141578674316
Validation loss: 3.3897301843089442

Epoch: 5| Step: 1
Training loss: 3.849971294403076
Validation loss: 3.3732702501358522

Epoch: 5| Step: 2
Training loss: 3.185849666595459
Validation loss: 3.36022880513181

Epoch: 5| Step: 3
Training loss: 3.251293659210205
Validation loss: 3.3459447583844586

Epoch: 5| Step: 4
Training loss: 3.450557231903076
Validation loss: 3.334284113299462

Epoch: 5| Step: 5
Training loss: 2.5663208961486816
Validation loss: 3.315241318877025

Epoch: 5| Step: 6
Training loss: 3.9753975868225098
Validation loss: 3.302564456898679

Epoch: 5| Step: 7
Training loss: 2.195214033126831
Validation loss: 3.293211103767477

Epoch: 5| Step: 8
Training loss: 3.1554019451141357
Validation loss: 3.307238978724326

Epoch: 5| Step: 9
Training loss: 2.858405351638794
Validation loss: 3.2731412610700055

Epoch: 5| Step: 10
Training loss: 3.7943105697631836
Validation loss: 3.2578767550888883

Epoch: 7| Step: 0
Training loss: 3.848268508911133
Validation loss: 3.248710791269938

Epoch: 5| Step: 1
Training loss: 2.7739486694335938
Validation loss: 3.2325877579309608

Epoch: 5| Step: 2
Training loss: 3.091290235519409
Validation loss: 3.221341153626801

Epoch: 5| Step: 3
Training loss: 2.907956600189209
Validation loss: 3.2110158628033054

Epoch: 5| Step: 4
Training loss: 2.771270751953125
Validation loss: 3.199228302125008

Epoch: 5| Step: 5
Training loss: 3.873502016067505
Validation loss: 3.1896795944500993

Epoch: 5| Step: 6
Training loss: 2.0010952949523926
Validation loss: 3.1766272565369964

Epoch: 5| Step: 7
Training loss: 3.1759250164031982
Validation loss: 3.166112453706803

Epoch: 5| Step: 8
Training loss: 3.5093657970428467
Validation loss: 3.1627038781360914

Epoch: 5| Step: 9
Training loss: 2.9394524097442627
Validation loss: 3.154561501677318

Epoch: 5| Step: 10
Training loss: 4.455078125
Validation loss: 3.1488125042248796

Epoch: 8| Step: 0
Training loss: 2.975571870803833
Validation loss: 3.1447216515899985

Epoch: 5| Step: 1
Training loss: 3.485214948654175
Validation loss: 3.14126605115911

Epoch: 5| Step: 2
Training loss: 2.9367387294769287
Validation loss: 3.126554807027181

Epoch: 5| Step: 3
Training loss: 3.2066829204559326
Validation loss: 3.1182968129393873

Epoch: 5| Step: 4
Training loss: 2.241995334625244
Validation loss: 3.10423122426515

Epoch: 5| Step: 5
Training loss: 3.3539023399353027
Validation loss: 3.0936768721508723

Epoch: 5| Step: 6
Training loss: 2.626931667327881
Validation loss: 3.0892562609846874

Epoch: 5| Step: 7
Training loss: 3.1370835304260254
Validation loss: 3.082555547837288

Epoch: 5| Step: 8
Training loss: 3.3868565559387207
Validation loss: 3.075978094531644

Epoch: 5| Step: 9
Training loss: 3.028287172317505
Validation loss: 3.068116085503691

Epoch: 5| Step: 10
Training loss: 4.23014497756958
Validation loss: 3.0615685114296536

Epoch: 9| Step: 0
Training loss: 3.5444176197052
Validation loss: 3.053268163434921

Epoch: 5| Step: 1
Training loss: 3.5627524852752686
Validation loss: 3.044787117229995

Epoch: 5| Step: 2
Training loss: 2.3709022998809814
Validation loss: 3.0349326441364903

Epoch: 5| Step: 3
Training loss: 3.6983578205108643
Validation loss: 3.029605734732843

Epoch: 5| Step: 4
Training loss: 2.865912675857544
Validation loss: 3.024582706471925

Epoch: 5| Step: 5
Training loss: 2.9444916248321533
Validation loss: 3.0167168776194253

Epoch: 5| Step: 6
Training loss: 2.7489264011383057
Validation loss: 3.0052172137844946

Epoch: 5| Step: 7
Training loss: 3.0691866874694824
Validation loss: 2.991312324359853

Epoch: 5| Step: 8
Training loss: 2.771929979324341
Validation loss: 2.9839355202131372

Epoch: 5| Step: 9
Training loss: 3.415409803390503
Validation loss: 2.985088643207345

Epoch: 5| Step: 10
Training loss: 2.832810640335083
Validation loss: 2.984169570348596

Epoch: 10| Step: 0
Training loss: 3.4144890308380127
Validation loss: 2.9695822833686747

Epoch: 5| Step: 1
Training loss: 2.9806137084960938
Validation loss: 2.9647079462646158

Epoch: 5| Step: 2
Training loss: 2.680616855621338
Validation loss: 2.9734598436663227

Epoch: 5| Step: 3
Training loss: 3.6419551372528076
Validation loss: 2.9579982449931483

Epoch: 5| Step: 4
Training loss: 2.9017391204833984
Validation loss: 2.944595072859077

Epoch: 5| Step: 5
Training loss: 2.8461828231811523
Validation loss: 2.957392923293575

Epoch: 5| Step: 6
Training loss: 3.5002830028533936
Validation loss: 3.029476947681878

Epoch: 5| Step: 7
Training loss: 2.9485788345336914
Validation loss: 3.009755442219396

Epoch: 5| Step: 8
Training loss: 2.026034116744995
Validation loss: 2.9953124933345343

Epoch: 5| Step: 9
Training loss: 2.9704432487487793
Validation loss: 3.0208850829832015

Epoch: 5| Step: 10
Training loss: 3.6973626613616943
Validation loss: 2.929486213191863

Epoch: 11| Step: 0
Training loss: 3.421952724456787
Validation loss: 2.9289501046621673

Epoch: 5| Step: 1
Training loss: 3.012665271759033
Validation loss: 2.9542850371330016

Epoch: 5| Step: 2
Training loss: 2.788118839263916
Validation loss: 2.9680673486442974

Epoch: 5| Step: 3
Training loss: 2.679582118988037
Validation loss: 2.9544181208456717

Epoch: 5| Step: 4
Training loss: 2.3740248680114746
Validation loss: 2.9356666047086

Epoch: 5| Step: 5
Training loss: 2.6299118995666504
Validation loss: 2.9177125038639193

Epoch: 5| Step: 6
Training loss: 3.090716600418091
Validation loss: 2.9029858599426928

Epoch: 5| Step: 7
Training loss: 3.584343433380127
Validation loss: 2.9013366391581874

Epoch: 5| Step: 8
Training loss: 2.8547024726867676
Validation loss: 2.8966222937389086

Epoch: 5| Step: 9
Training loss: 3.6602745056152344
Validation loss: 2.8952694913392425

Epoch: 5| Step: 10
Training loss: 2.908212184906006
Validation loss: 2.8721241284442205

Epoch: 12| Step: 0
Training loss: 2.375448703765869
Validation loss: 2.8669622764792493

Epoch: 5| Step: 1
Training loss: 2.8415446281433105
Validation loss: 2.877405999809183

Epoch: 5| Step: 2
Training loss: 2.778481960296631
Validation loss: 2.9146002492597027

Epoch: 5| Step: 3
Training loss: 3.7955150604248047
Validation loss: 2.9103790970258814

Epoch: 5| Step: 4
Training loss: 3.0663399696350098
Validation loss: 2.862328411430441

Epoch: 5| Step: 5
Training loss: 3.488532304763794
Validation loss: 2.847315178122572

Epoch: 5| Step: 6
Training loss: 2.9765055179595947
Validation loss: 2.8559293823857463

Epoch: 5| Step: 7
Training loss: 3.06327486038208
Validation loss: 2.86192653512442

Epoch: 5| Step: 8
Training loss: 2.947927713394165
Validation loss: 2.8637927039977042

Epoch: 5| Step: 9
Training loss: 2.446174383163452
Validation loss: 2.846860724110757

Epoch: 5| Step: 10
Training loss: 2.8088531494140625
Validation loss: 2.8319306681233067

Epoch: 13| Step: 0
Training loss: 3.2240829467773438
Validation loss: 2.826723870410714

Epoch: 5| Step: 1
Training loss: 2.686051845550537
Validation loss: 2.8177333775387017

Epoch: 5| Step: 2
Training loss: 3.1432785987854004
Validation loss: 2.813637174585814

Epoch: 5| Step: 3
Training loss: 2.5422661304473877
Validation loss: 2.811774943464546

Epoch: 5| Step: 4
Training loss: 2.452134609222412
Validation loss: 2.809523379930886

Epoch: 5| Step: 5
Training loss: 2.8910365104675293
Validation loss: 2.8079575671944568

Epoch: 5| Step: 6
Training loss: 3.0781702995300293
Validation loss: 2.8072820350687993

Epoch: 5| Step: 7
Training loss: 2.8639729022979736
Validation loss: 2.803071855216898

Epoch: 5| Step: 8
Training loss: 3.6922965049743652
Validation loss: 2.8002493868591967

Epoch: 5| Step: 9
Training loss: 2.644400119781494
Validation loss: 2.7849831350388063

Epoch: 5| Step: 10
Training loss: 2.9614579677581787
Validation loss: 2.77950539640201

Epoch: 14| Step: 0
Training loss: 2.9059224128723145
Validation loss: 2.784487073139478

Epoch: 5| Step: 1
Training loss: 3.0287420749664307
Validation loss: 2.777612973284978

Epoch: 5| Step: 2
Training loss: 2.6791069507598877
Validation loss: 2.780243017340219

Epoch: 5| Step: 3
Training loss: 2.6865429878234863
Validation loss: 2.7645618325920513

Epoch: 5| Step: 4
Training loss: 3.570544719696045
Validation loss: 2.7573215987092707

Epoch: 5| Step: 5
Training loss: 3.629157304763794
Validation loss: 2.748107607646655

Epoch: 5| Step: 6
Training loss: 2.8093652725219727
Validation loss: 2.7402684047657955

Epoch: 5| Step: 7
Training loss: 2.1780354976654053
Validation loss: 2.729979527893887

Epoch: 5| Step: 8
Training loss: 2.794820547103882
Validation loss: 2.732332314214399

Epoch: 5| Step: 9
Training loss: 2.700601100921631
Validation loss: 2.7288244949874056

Epoch: 5| Step: 10
Training loss: 2.7442944049835205
Validation loss: 2.723945858658001

Epoch: 15| Step: 0
Training loss: 3.067715883255005
Validation loss: 2.7182934002209733

Epoch: 5| Step: 1
Training loss: 3.1678035259246826
Validation loss: 2.716547037965508

Epoch: 5| Step: 2
Training loss: 2.7565646171569824
Validation loss: 2.7137796186631724

Epoch: 5| Step: 3
Training loss: 3.421299695968628
Validation loss: 2.7072518205129974

Epoch: 5| Step: 4
Training loss: 3.4710583686828613
Validation loss: 2.7033670461306007

Epoch: 5| Step: 5
Training loss: 3.107713222503662
Validation loss: 2.709120899118403

Epoch: 5| Step: 6
Training loss: 2.9929325580596924
Validation loss: 2.703215411914292

Epoch: 5| Step: 7
Training loss: 2.480186939239502
Validation loss: 2.6982358937622397

Epoch: 5| Step: 8
Training loss: 2.426626205444336
Validation loss: 2.6880611065895326

Epoch: 5| Step: 9
Training loss: 2.216809034347534
Validation loss: 2.6777662410530993

Epoch: 5| Step: 10
Training loss: 2.152008533477783
Validation loss: 2.677115435241371

Epoch: 16| Step: 0
Training loss: 2.8305511474609375
Validation loss: 2.678432523563344

Epoch: 5| Step: 1
Training loss: 2.968435287475586
Validation loss: 2.6739863426454606

Epoch: 5| Step: 2
Training loss: 2.53788423538208
Validation loss: 2.6687923195541545

Epoch: 5| Step: 3
Training loss: 3.430819272994995
Validation loss: 2.666628950385637

Epoch: 5| Step: 4
Training loss: 2.8643040657043457
Validation loss: 2.6621514110155005

Epoch: 5| Step: 5
Training loss: 2.6324150562286377
Validation loss: 2.6543984387510564

Epoch: 5| Step: 6
Training loss: 3.341188430786133
Validation loss: 2.659710466220815

Epoch: 5| Step: 7
Training loss: 3.171407699584961
Validation loss: 2.661249837567729

Epoch: 5| Step: 8
Training loss: 2.2530136108398438
Validation loss: 2.661391738922365

Epoch: 5| Step: 9
Training loss: 2.9729905128479004
Validation loss: 2.653528654447166

Epoch: 5| Step: 10
Training loss: 1.9630353450775146
Validation loss: 2.663551794585361

Epoch: 17| Step: 0
Training loss: 3.1641619205474854
Validation loss: 2.677935961754091

Epoch: 5| Step: 1
Training loss: 2.4355309009552
Validation loss: 2.6405435710824947

Epoch: 5| Step: 2
Training loss: 2.9546265602111816
Validation loss: 2.635918119902252

Epoch: 5| Step: 3
Training loss: 2.7721571922302246
Validation loss: 2.636640764051868

Epoch: 5| Step: 4
Training loss: 2.4266726970672607
Validation loss: 2.6387457052866616

Epoch: 5| Step: 5
Training loss: 2.669529676437378
Validation loss: 2.6365389747004353

Epoch: 5| Step: 6
Training loss: 3.165163516998291
Validation loss: 2.6437127231269755

Epoch: 5| Step: 7
Training loss: 2.099269151687622
Validation loss: 2.6405141456152803

Epoch: 5| Step: 8
Training loss: 2.8644444942474365
Validation loss: 2.641196927716655

Epoch: 5| Step: 9
Training loss: 3.030601978302002
Validation loss: 2.6265456561119325

Epoch: 5| Step: 10
Training loss: 3.4034500122070312
Validation loss: 2.620880737099596

Epoch: 18| Step: 0
Training loss: 2.55875301361084
Validation loss: 2.6156141091418523

Epoch: 5| Step: 1
Training loss: 2.929152250289917
Validation loss: 2.6115113765962663

Epoch: 5| Step: 2
Training loss: 2.604253053665161
Validation loss: 2.615271886189779

Epoch: 5| Step: 3
Training loss: 3.0588831901550293
Validation loss: 2.618823879508562

Epoch: 5| Step: 4
Training loss: 2.55975604057312
Validation loss: 2.659150315869239

Epoch: 5| Step: 5
Training loss: 3.4730770587921143
Validation loss: 2.7017818215072795

Epoch: 5| Step: 6
Training loss: 2.0745279788970947
Validation loss: 2.684916042512463

Epoch: 5| Step: 7
Training loss: 3.013047456741333
Validation loss: 2.6091299903008247

Epoch: 5| Step: 8
Training loss: 2.7596044540405273
Validation loss: 2.5999633035352154

Epoch: 5| Step: 9
Training loss: 3.6259970664978027
Validation loss: 2.619927995948381

Epoch: 5| Step: 10
Training loss: 2.096583604812622
Validation loss: 2.640495518202423

Epoch: 19| Step: 0
Training loss: 3.9682319164276123
Validation loss: 2.6396581703616726

Epoch: 5| Step: 1
Training loss: 2.0211727619171143
Validation loss: 2.630617908252183

Epoch: 5| Step: 2
Training loss: 2.573808193206787
Validation loss: 2.6172156513378186

Epoch: 5| Step: 3
Training loss: 2.9563050270080566
Validation loss: 2.5989783092211654

Epoch: 5| Step: 4
Training loss: 3.0924925804138184
Validation loss: 2.5904767462002334

Epoch: 5| Step: 5
Training loss: 2.871596097946167
Validation loss: 2.589124489856023

Epoch: 5| Step: 6
Training loss: 2.7124640941619873
Validation loss: 2.587094804292084

Epoch: 5| Step: 7
Training loss: 3.104921340942383
Validation loss: 2.5879813599330124

Epoch: 5| Step: 8
Training loss: 2.542092800140381
Validation loss: 2.5884163097668718

Epoch: 5| Step: 9
Training loss: 2.724637508392334
Validation loss: 2.60316978475099

Epoch: 5| Step: 10
Training loss: 1.9068377017974854
Validation loss: 2.6085868727776313

Epoch: 20| Step: 0
Training loss: 2.6037251949310303
Validation loss: 2.6206948013715845

Epoch: 5| Step: 1
Training loss: 2.8395867347717285
Validation loss: 2.62827391265541

Epoch: 5| Step: 2
Training loss: 2.6848459243774414
Validation loss: 2.618284494646134

Epoch: 5| Step: 3
Training loss: 3.3778717517852783
Validation loss: 2.620969415992819

Epoch: 5| Step: 4
Training loss: 2.7139878273010254
Validation loss: 2.581522818534605

Epoch: 5| Step: 5
Training loss: 3.164426803588867
Validation loss: 2.5612190231200187

Epoch: 5| Step: 6
Training loss: 2.4070279598236084
Validation loss: 2.5624909503485567

Epoch: 5| Step: 7
Training loss: 2.824688673019409
Validation loss: 2.5850269281735985

Epoch: 5| Step: 8
Training loss: 1.9873816967010498
Validation loss: 2.5827405427091863

Epoch: 5| Step: 9
Training loss: 3.043377637863159
Validation loss: 2.5762838471320366

Epoch: 5| Step: 10
Training loss: 2.7608277797698975
Validation loss: 2.5701547873917447

Epoch: 21| Step: 0
Training loss: 2.613736629486084
Validation loss: 2.5615803887767177

Epoch: 5| Step: 1
Training loss: 3.201756000518799
Validation loss: 2.558085354425574

Epoch: 5| Step: 2
Training loss: 2.189155101776123
Validation loss: 2.5542124317538355

Epoch: 5| Step: 3
Training loss: 2.921015739440918
Validation loss: 2.5416186240411576

Epoch: 5| Step: 4
Training loss: 2.6215415000915527
Validation loss: 2.5408096467295

Epoch: 5| Step: 5
Training loss: 2.2716434001922607
Validation loss: 2.5384863576581402

Epoch: 5| Step: 6
Training loss: 3.7029316425323486
Validation loss: 2.5462887107685046

Epoch: 5| Step: 7
Training loss: 2.6794135570526123
Validation loss: 2.546536760945474

Epoch: 5| Step: 8
Training loss: 3.6716666221618652
Validation loss: 2.5450223030582553

Epoch: 5| Step: 9
Training loss: 2.5310311317443848
Validation loss: 2.535390958991102

Epoch: 5| Step: 10
Training loss: 1.452101707458496
Validation loss: 2.5319899179602183

Epoch: 22| Step: 0
Training loss: 2.1087212562561035
Validation loss: 2.5278864932316605

Epoch: 5| Step: 1
Training loss: 2.750401020050049
Validation loss: 2.5303675795114167

Epoch: 5| Step: 2
Training loss: 2.7311577796936035
Validation loss: 2.5341948206706713

Epoch: 5| Step: 3
Training loss: 2.9732413291931152
Validation loss: 2.5422583190343713

Epoch: 5| Step: 4
Training loss: 2.9575119018554688
Validation loss: 2.545039576868857

Epoch: 5| Step: 5
Training loss: 2.271548271179199
Validation loss: 2.5457790846465738

Epoch: 5| Step: 6
Training loss: 2.9944217205047607
Validation loss: 2.520464612591651

Epoch: 5| Step: 7
Training loss: 3.3886280059814453
Validation loss: 2.516239845624534

Epoch: 5| Step: 8
Training loss: 2.708705425262451
Validation loss: 2.5138591412575013

Epoch: 5| Step: 9
Training loss: 2.689393997192383
Validation loss: 2.519934379926292

Epoch: 5| Step: 10
Training loss: 2.3479933738708496
Validation loss: 2.5167383070914977

Epoch: 23| Step: 0
Training loss: 3.019777297973633
Validation loss: 2.5142940808367986

Epoch: 5| Step: 1
Training loss: 1.966854453086853
Validation loss: 2.5098715879583873

Epoch: 5| Step: 2
Training loss: 2.3579599857330322
Validation loss: 2.509110022616643

Epoch: 5| Step: 3
Training loss: 2.41342830657959
Validation loss: 2.510075646062051

Epoch: 5| Step: 4
Training loss: 2.9608616828918457
Validation loss: 2.5119109102474746

Epoch: 5| Step: 5
Training loss: 2.9844236373901367
Validation loss: 2.507579985485282

Epoch: 5| Step: 6
Training loss: 2.8671517372131348
Validation loss: 2.507508365056848

Epoch: 5| Step: 7
Training loss: 3.006274461746216
Validation loss: 2.512187155344153

Epoch: 5| Step: 8
Training loss: 2.7674107551574707
Validation loss: 2.5104225835492535

Epoch: 5| Step: 9
Training loss: 2.752631664276123
Validation loss: 2.509862676743538

Epoch: 5| Step: 10
Training loss: 2.737868309020996
Validation loss: 2.509025332748249

Epoch: 24| Step: 0
Training loss: 2.13356614112854
Validation loss: 2.5171714264859437

Epoch: 5| Step: 1
Training loss: 2.7504513263702393
Validation loss: 2.518878162548106

Epoch: 5| Step: 2
Training loss: 3.444777727127075
Validation loss: 2.517298383097495

Epoch: 5| Step: 3
Training loss: 2.7166643142700195
Validation loss: 2.5054471287676083

Epoch: 5| Step: 4
Training loss: 3.065803050994873
Validation loss: 2.511112428480579

Epoch: 5| Step: 5
Training loss: 2.5151829719543457
Validation loss: 2.5033527971595846

Epoch: 5| Step: 6
Training loss: 2.8127121925354004
Validation loss: 2.501227819791404

Epoch: 5| Step: 7
Training loss: 2.7118513584136963
Validation loss: 2.499640139200354

Epoch: 5| Step: 8
Training loss: 2.348320722579956
Validation loss: 2.5085204057796027

Epoch: 5| Step: 9
Training loss: 2.7076284885406494
Validation loss: 2.5159922005027853

Epoch: 5| Step: 10
Training loss: 2.641713857650757
Validation loss: 2.5288792681950394

Epoch: 25| Step: 0
Training loss: 2.364107370376587
Validation loss: 2.50853899217421

Epoch: 5| Step: 1
Training loss: 3.101073741912842
Validation loss: 2.5001785934612317

Epoch: 5| Step: 2
Training loss: 2.2480597496032715
Validation loss: 2.4991959346238004

Epoch: 5| Step: 3
Training loss: 1.8751035928726196
Validation loss: 2.4979995604484313

Epoch: 5| Step: 4
Training loss: 2.6780500411987305
Validation loss: 2.49980350207257

Epoch: 5| Step: 5
Training loss: 3.149174213409424
Validation loss: 2.5034910709627214

Epoch: 5| Step: 6
Training loss: 3.3685104846954346
Validation loss: 2.506425460179647

Epoch: 5| Step: 7
Training loss: 3.1262221336364746
Validation loss: 2.5060648507969354

Epoch: 5| Step: 8
Training loss: 3.077075958251953
Validation loss: 2.494090152043168

Epoch: 5| Step: 9
Training loss: 2.514841079711914
Validation loss: 2.4871361947828725

Epoch: 5| Step: 10
Training loss: 2.3131539821624756
Validation loss: 2.487254878526093

Epoch: 26| Step: 0
Training loss: 2.902503252029419
Validation loss: 2.497087811910978

Epoch: 5| Step: 1
Training loss: 2.502378225326538
Validation loss: 2.5094808327254428

Epoch: 5| Step: 2
Training loss: 2.7161033153533936
Validation loss: 2.5193691792026645

Epoch: 5| Step: 3
Training loss: 2.4835782051086426
Validation loss: 2.5109968236697617

Epoch: 5| Step: 4
Training loss: 2.5516295433044434
Validation loss: 2.4941956356007564

Epoch: 5| Step: 5
Training loss: 3.147233724594116
Validation loss: 2.4931145585993284

Epoch: 5| Step: 6
Training loss: 3.0455880165100098
Validation loss: 2.49067735415633

Epoch: 5| Step: 7
Training loss: 2.549006938934326
Validation loss: 2.4930320170617875

Epoch: 5| Step: 8
Training loss: 3.0463685989379883
Validation loss: 2.5008019375544723

Epoch: 5| Step: 9
Training loss: 2.1864352226257324
Validation loss: 2.493644017045216

Epoch: 5| Step: 10
Training loss: 2.510735273361206
Validation loss: 2.4901340828146985

Epoch: 27| Step: 0
Training loss: 2.5977394580841064
Validation loss: 2.4779229664033458

Epoch: 5| Step: 1
Training loss: 2.745638847351074
Validation loss: 2.481426479995892

Epoch: 5| Step: 2
Training loss: 3.1866488456726074
Validation loss: 2.531145167607133

Epoch: 5| Step: 3
Training loss: 2.52089262008667
Validation loss: 2.480112921807074

Epoch: 5| Step: 4
Training loss: 2.1758956909179688
Validation loss: 2.4659732003365793

Epoch: 5| Step: 5
Training loss: 2.268195629119873
Validation loss: 2.4690682631666943

Epoch: 5| Step: 6
Training loss: 2.9910614490509033
Validation loss: 2.475298866148918

Epoch: 5| Step: 7
Training loss: 2.971597909927368
Validation loss: 2.4849489171017884

Epoch: 5| Step: 8
Training loss: 2.543846607208252
Validation loss: 2.4927985360545497

Epoch: 5| Step: 9
Training loss: 2.205364465713501
Validation loss: 2.4826572518194876

Epoch: 5| Step: 10
Training loss: 3.6474063396453857
Validation loss: 2.4777008948787564

Epoch: 28| Step: 0
Training loss: 2.477119207382202
Validation loss: 2.463632757945727

Epoch: 5| Step: 1
Training loss: 2.4876317977905273
Validation loss: 2.464805228735811

Epoch: 5| Step: 2
Training loss: 2.476870536804199
Validation loss: 2.4669677929211686

Epoch: 5| Step: 3
Training loss: 2.548374891281128
Validation loss: 2.473173274788805

Epoch: 5| Step: 4
Training loss: 2.366421699523926
Validation loss: 2.483011561055337

Epoch: 5| Step: 5
Training loss: 2.868145227432251
Validation loss: 2.489852951418969

Epoch: 5| Step: 6
Training loss: 2.9610626697540283
Validation loss: 2.498274951852778

Epoch: 5| Step: 7
Training loss: 3.378185749053955
Validation loss: 2.5153096106744584

Epoch: 5| Step: 8
Training loss: 2.651130199432373
Validation loss: 2.524917397447812

Epoch: 5| Step: 9
Training loss: 2.8031134605407715
Validation loss: 2.5168884697780816

Epoch: 5| Step: 10
Training loss: 2.527658462524414
Validation loss: 2.5073406106682232

Epoch: 29| Step: 0
Training loss: 2.9042282104492188
Validation loss: 2.506789397167903

Epoch: 5| Step: 1
Training loss: 2.1018013954162598
Validation loss: 2.50676384792533

Epoch: 5| Step: 2
Training loss: 2.5987234115600586
Validation loss: 2.5075857716221965

Epoch: 5| Step: 3
Training loss: 2.1317672729492188
Validation loss: 2.4983113658043647

Epoch: 5| Step: 4
Training loss: 2.191192150115967
Validation loss: 2.5031269109377297

Epoch: 5| Step: 5
Training loss: 2.843069076538086
Validation loss: 2.5095382249483498

Epoch: 5| Step: 6
Training loss: 2.672496795654297
Validation loss: 2.4957128378652755

Epoch: 5| Step: 7
Training loss: 3.0295891761779785
Validation loss: 2.4722475518462477

Epoch: 5| Step: 8
Training loss: 2.991039752960205
Validation loss: 2.466890993938651

Epoch: 5| Step: 9
Training loss: 2.69079852104187
Validation loss: 2.464620197972944

Epoch: 5| Step: 10
Training loss: 3.5025501251220703
Validation loss: 2.459544943224999

Epoch: 30| Step: 0
Training loss: 2.4157299995422363
Validation loss: 2.46018999109986

Epoch: 5| Step: 1
Training loss: 2.46262264251709
Validation loss: 2.467620075389903

Epoch: 5| Step: 2
Training loss: 3.1113479137420654
Validation loss: 2.4731290340423584

Epoch: 5| Step: 3
Training loss: 1.63687264919281
Validation loss: 2.462084147237962

Epoch: 5| Step: 4
Training loss: 2.7141518592834473
Validation loss: 2.4568437427602787

Epoch: 5| Step: 5
Training loss: 2.8933355808258057
Validation loss: 2.4558503525231474

Epoch: 5| Step: 6
Training loss: 2.811675548553467
Validation loss: 2.4485601943026305

Epoch: 5| Step: 7
Training loss: 2.732600688934326
Validation loss: 2.443867573174097

Epoch: 5| Step: 8
Training loss: 3.013878345489502
Validation loss: 2.444273782032792

Epoch: 5| Step: 9
Training loss: 2.538465976715088
Validation loss: 2.443375074735252

Epoch: 5| Step: 10
Training loss: 3.1008052825927734
Validation loss: 2.442287573250391

Epoch: 31| Step: 0
Training loss: 2.771965980529785
Validation loss: 2.4414403643659366

Epoch: 5| Step: 1
Training loss: 2.8144941329956055
Validation loss: 2.444078758198728

Epoch: 5| Step: 2
Training loss: 2.725595474243164
Validation loss: 2.4452813094662083

Epoch: 5| Step: 3
Training loss: 2.465419292449951
Validation loss: 2.456510651496149

Epoch: 5| Step: 4
Training loss: 2.569746732711792
Validation loss: 2.4653387172247774

Epoch: 5| Step: 5
Training loss: 2.94954514503479
Validation loss: 2.464391598137476

Epoch: 5| Step: 6
Training loss: 3.0049569606781006
Validation loss: 2.474388640414002

Epoch: 5| Step: 7
Training loss: 2.2144880294799805
Validation loss: 2.480269003939885

Epoch: 5| Step: 8
Training loss: 2.8466079235076904
Validation loss: 2.454198734734648

Epoch: 5| Step: 9
Training loss: 2.625140428543091
Validation loss: 2.442903523804039

Epoch: 5| Step: 10
Training loss: 2.323042154312134
Validation loss: 2.4405780300017326

Epoch: 32| Step: 0
Training loss: 3.0394692420959473
Validation loss: 2.4381526875239548

Epoch: 5| Step: 1
Training loss: 2.2057292461395264
Validation loss: 2.433864011559435

Epoch: 5| Step: 2
Training loss: 2.5421440601348877
Validation loss: 2.4289751129765667

Epoch: 5| Step: 3
Training loss: 2.2256550788879395
Validation loss: 2.4318093381902224

Epoch: 5| Step: 4
Training loss: 2.458484172821045
Validation loss: 2.4309997250956874

Epoch: 5| Step: 5
Training loss: 2.6904444694519043
Validation loss: 2.439242693685716

Epoch: 5| Step: 6
Training loss: 2.575077533721924
Validation loss: 2.451002674718057

Epoch: 5| Step: 7
Training loss: 2.794363498687744
Validation loss: 2.4549431031750095

Epoch: 5| Step: 8
Training loss: 2.6821212768554688
Validation loss: 2.4650084818563154

Epoch: 5| Step: 9
Training loss: 3.2804932594299316
Validation loss: 2.446843533105748

Epoch: 5| Step: 10
Training loss: 2.7906932830810547
Validation loss: 2.432134529595734

Epoch: 33| Step: 0
Training loss: 2.82780122756958
Validation loss: 2.4178340435028076

Epoch: 5| Step: 1
Training loss: 2.5106353759765625
Validation loss: 2.412019142540552

Epoch: 5| Step: 2
Training loss: 2.9382457733154297
Validation loss: 2.4174911001677155

Epoch: 5| Step: 3
Training loss: 2.414080858230591
Validation loss: 2.4183411521296345

Epoch: 5| Step: 4
Training loss: 2.6681575775146484
Validation loss: 2.416899355508948

Epoch: 5| Step: 5
Training loss: 2.5406367778778076
Validation loss: 2.416712091815087

Epoch: 5| Step: 6
Training loss: 3.2711243629455566
Validation loss: 2.420842129697082

Epoch: 5| Step: 7
Training loss: 2.1406331062316895
Validation loss: 2.419012333757134

Epoch: 5| Step: 8
Training loss: 3.007986068725586
Validation loss: 2.423528309791319

Epoch: 5| Step: 9
Training loss: 2.530726909637451
Validation loss: 2.4372237497760403

Epoch: 5| Step: 10
Training loss: 2.2913224697113037
Validation loss: 2.468225312489335

Epoch: 34| Step: 0
Training loss: 2.8484139442443848
Validation loss: 2.4793226821448213

Epoch: 5| Step: 1
Training loss: 2.118197441101074
Validation loss: 2.4922543251386253

Epoch: 5| Step: 2
Training loss: 2.5812363624572754
Validation loss: 2.470858707222887

Epoch: 5| Step: 3
Training loss: 2.9148125648498535
Validation loss: 2.469604520387547

Epoch: 5| Step: 4
Training loss: 2.919823169708252
Validation loss: 2.4320714114814677

Epoch: 5| Step: 5
Training loss: 2.4620895385742188
Validation loss: 2.4016085978477233

Epoch: 5| Step: 6
Training loss: 2.561965227127075
Validation loss: 2.3980417251586914

Epoch: 5| Step: 7
Training loss: 2.472447633743286
Validation loss: 2.4147760765526884

Epoch: 5| Step: 8
Training loss: 2.2219247817993164
Validation loss: 2.4349147094193326

Epoch: 5| Step: 9
Training loss: 3.161220073699951
Validation loss: 2.492990065646428

Epoch: 5| Step: 10
Training loss: 3.2389609813690186
Validation loss: 2.533775491099204

Epoch: 35| Step: 0
Training loss: 1.797370195388794
Validation loss: 2.548148521813013

Epoch: 5| Step: 1
Training loss: 3.1079964637756348
Validation loss: 2.562414671785088

Epoch: 5| Step: 2
Training loss: 2.9016530513763428
Validation loss: 2.561207159872978

Epoch: 5| Step: 3
Training loss: 2.1772561073303223
Validation loss: 2.5202251865017797

Epoch: 5| Step: 4
Training loss: 2.8561346530914307
Validation loss: 2.4898529668008127

Epoch: 5| Step: 5
Training loss: 2.8528122901916504
Validation loss: 2.4722186391071608

Epoch: 5| Step: 6
Training loss: 3.0813076496124268
Validation loss: 2.4585310464264243

Epoch: 5| Step: 7
Training loss: 2.4140939712524414
Validation loss: 2.445192672873056

Epoch: 5| Step: 8
Training loss: 3.4058902263641357
Validation loss: 2.4254750064624253

Epoch: 5| Step: 9
Training loss: 2.348588705062866
Validation loss: 2.4347846200389247

Epoch: 5| Step: 10
Training loss: 2.871208429336548
Validation loss: 2.4354198773701987

Epoch: 36| Step: 0
Training loss: 2.3395869731903076
Validation loss: 2.4167986557047856

Epoch: 5| Step: 1
Training loss: 2.6264145374298096
Validation loss: 2.400412969691779

Epoch: 5| Step: 2
Training loss: 2.612337589263916
Validation loss: 2.388358582732498

Epoch: 5| Step: 3
Training loss: 2.2104735374450684
Validation loss: 2.3940393642712663

Epoch: 5| Step: 4
Training loss: 2.9102301597595215
Validation loss: 2.412044773819626

Epoch: 5| Step: 5
Training loss: 2.4223275184631348
Validation loss: 2.4554405212402344

Epoch: 5| Step: 6
Training loss: 2.563633441925049
Validation loss: 2.4370207530195995

Epoch: 5| Step: 7
Training loss: 2.3784079551696777
Validation loss: 2.433562627402685

Epoch: 5| Step: 8
Training loss: 3.06882905960083
Validation loss: 2.4265414361030824

Epoch: 5| Step: 9
Training loss: 3.1999313831329346
Validation loss: 2.3942284250772126

Epoch: 5| Step: 10
Training loss: 2.6711978912353516
Validation loss: 2.380569306753015

Epoch: 37| Step: 0
Training loss: 3.11501145362854
Validation loss: 2.3804755569786153

Epoch: 5| Step: 1
Training loss: 2.624194622039795
Validation loss: 2.3878813764100433

Epoch: 5| Step: 2
Training loss: 2.850069761276245
Validation loss: 2.3807095173866517

Epoch: 5| Step: 3
Training loss: 2.324364185333252
Validation loss: 2.3772959555349042

Epoch: 5| Step: 4
Training loss: 3.256570339202881
Validation loss: 2.372396476807133

Epoch: 5| Step: 5
Training loss: 2.6360623836517334
Validation loss: 2.367867628733317

Epoch: 5| Step: 6
Training loss: 2.054150342941284
Validation loss: 2.364087268870364

Epoch: 5| Step: 7
Training loss: 2.265392541885376
Validation loss: 2.366298137172576

Epoch: 5| Step: 8
Training loss: 2.7949886322021484
Validation loss: 2.3633598076399935

Epoch: 5| Step: 9
Training loss: 2.2854878902435303
Validation loss: 2.3769559501319804

Epoch: 5| Step: 10
Training loss: 2.718376398086548
Validation loss: 2.392729410561182

Epoch: 38| Step: 0
Training loss: 2.718555212020874
Validation loss: 2.409392144090386

Epoch: 5| Step: 1
Training loss: 2.5629420280456543
Validation loss: 2.3954304084982923

Epoch: 5| Step: 2
Training loss: 2.690392255783081
Validation loss: 2.3931562131451023

Epoch: 5| Step: 3
Training loss: 2.7017009258270264
Validation loss: 2.3949027881827405

Epoch: 5| Step: 4
Training loss: 2.738910675048828
Validation loss: 2.4250763872618317

Epoch: 5| Step: 5
Training loss: 2.975367784500122
Validation loss: 2.3998740078300558

Epoch: 5| Step: 6
Training loss: 2.253093719482422
Validation loss: 2.360487602090323

Epoch: 5| Step: 7
Training loss: 2.101306200027466
Validation loss: 2.344547197382937

Epoch: 5| Step: 8
Training loss: 2.547384738922119
Validation loss: 2.345011998248357

Epoch: 5| Step: 9
Training loss: 2.7422597408294678
Validation loss: 2.3448172384692776

Epoch: 5| Step: 10
Training loss: 2.9090850353240967
Validation loss: 2.3464442658168014

Epoch: 39| Step: 0
Training loss: 2.911146879196167
Validation loss: 2.3487595281293316

Epoch: 5| Step: 1
Training loss: 2.159947156906128
Validation loss: 2.3531599198618243

Epoch: 5| Step: 2
Training loss: 2.555398941040039
Validation loss: 2.359352914235925

Epoch: 5| Step: 3
Training loss: 3.0868425369262695
Validation loss: 2.3721027681904454

Epoch: 5| Step: 4
Training loss: 2.5964794158935547
Validation loss: 2.3743336457078175

Epoch: 5| Step: 5
Training loss: 2.4477124214172363
Validation loss: 2.3991217664493028

Epoch: 5| Step: 6
Training loss: 2.4822874069213867
Validation loss: 2.3963312128538727

Epoch: 5| Step: 7
Training loss: 2.705887794494629
Validation loss: 2.4108645095620105

Epoch: 5| Step: 8
Training loss: 2.9210517406463623
Validation loss: 2.423215643052132

Epoch: 5| Step: 9
Training loss: 2.6788113117218018
Validation loss: 2.4054925608378586

Epoch: 5| Step: 10
Training loss: 2.131936550140381
Validation loss: 2.362179261381908

Epoch: 40| Step: 0
Training loss: 2.708498477935791
Validation loss: 2.3398808586981987

Epoch: 5| Step: 1
Training loss: 2.7429261207580566
Validation loss: 2.340429570085259

Epoch: 5| Step: 2
Training loss: 3.445035457611084
Validation loss: 2.3456010818481445

Epoch: 5| Step: 3
Training loss: 2.703606128692627
Validation loss: 2.349163891166769

Epoch: 5| Step: 4
Training loss: 2.814042329788208
Validation loss: 2.352116197668096

Epoch: 5| Step: 5
Training loss: 2.090874195098877
Validation loss: 2.3509853142564014

Epoch: 5| Step: 6
Training loss: 2.1076724529266357
Validation loss: 2.345794182951732

Epoch: 5| Step: 7
Training loss: 2.847557544708252
Validation loss: 2.3435373229365193

Epoch: 5| Step: 8
Training loss: 2.7470622062683105
Validation loss: 2.3356497287750244

Epoch: 5| Step: 9
Training loss: 2.870039463043213
Validation loss: 2.3409178282624934

Epoch: 5| Step: 10
Training loss: 1.6461080312728882
Validation loss: 2.348235187991973

Epoch: 41| Step: 0
Training loss: 3.1195850372314453
Validation loss: 2.3640717639718005

Epoch: 5| Step: 1
Training loss: 2.51234769821167
Validation loss: 2.35818507594447

Epoch: 5| Step: 2
Training loss: 2.7267138957977295
Validation loss: 2.344081054451645

Epoch: 5| Step: 3
Training loss: 3.11787748336792
Validation loss: 2.349787022477837

Epoch: 5| Step: 4
Training loss: 1.6711337566375732
Validation loss: 2.33955309724295

Epoch: 5| Step: 5
Training loss: 3.149653911590576
Validation loss: 2.3304454216393093

Epoch: 5| Step: 6
Training loss: 2.65514874458313
Validation loss: 2.32604823061215

Epoch: 5| Step: 7
Training loss: 2.2440237998962402
Validation loss: 2.3256309827168784

Epoch: 5| Step: 8
Training loss: 1.992767095565796
Validation loss: 2.322889022929694

Epoch: 5| Step: 9
Training loss: 2.2450499534606934
Validation loss: 2.321534797709475

Epoch: 5| Step: 10
Training loss: 3.234750509262085
Validation loss: 2.3211671511332193

Epoch: 42| Step: 0
Training loss: 2.5196750164031982
Validation loss: 2.3346418744774273

Epoch: 5| Step: 1
Training loss: 2.40000319480896
Validation loss: 2.3527299960454306

Epoch: 5| Step: 2
Training loss: 2.5679945945739746
Validation loss: 2.385320658324867

Epoch: 5| Step: 3
Training loss: 2.539720058441162
Validation loss: 2.3833542049572034

Epoch: 5| Step: 4
Training loss: 3.1062960624694824
Validation loss: 2.358863583175085

Epoch: 5| Step: 5
Training loss: 2.3591790199279785
Validation loss: 2.327322406153525

Epoch: 5| Step: 6
Training loss: 2.221048593521118
Validation loss: 2.328674534315704

Epoch: 5| Step: 7
Training loss: 1.8933448791503906
Validation loss: 2.3195006334653465

Epoch: 5| Step: 8
Training loss: 3.054962158203125
Validation loss: 2.322293153373144

Epoch: 5| Step: 9
Training loss: 2.9312031269073486
Validation loss: 2.323115487252512

Epoch: 5| Step: 10
Training loss: 3.025252342224121
Validation loss: 2.3182990986813783

Epoch: 43| Step: 0
Training loss: 2.468208074569702
Validation loss: 2.3193257162647862

Epoch: 5| Step: 1
Training loss: 2.3126235008239746
Validation loss: 2.318924726978425

Epoch: 5| Step: 2
Training loss: 3.041538715362549
Validation loss: 2.3244155812007126

Epoch: 5| Step: 3
Training loss: 2.4405863285064697
Validation loss: 2.328929288412935

Epoch: 5| Step: 4
Training loss: 2.292618989944458
Validation loss: 2.3424990151518132

Epoch: 5| Step: 5
Training loss: 2.6923489570617676
Validation loss: 2.368881945968956

Epoch: 5| Step: 6
Training loss: 2.5639357566833496
Validation loss: 2.350263311016944

Epoch: 5| Step: 7
Training loss: 2.5134949684143066
Validation loss: 2.340012841327216

Epoch: 5| Step: 8
Training loss: 2.8055834770202637
Validation loss: 2.3427909548564623

Epoch: 5| Step: 9
Training loss: 2.7761714458465576
Validation loss: 2.3386555025654454

Epoch: 5| Step: 10
Training loss: 2.6163177490234375
Validation loss: 2.3165009790851223

Epoch: 44| Step: 0
Training loss: 2.7009975910186768
Validation loss: 2.3113972897170694

Epoch: 5| Step: 1
Training loss: 2.8029732704162598
Validation loss: 2.3047535727100987

Epoch: 5| Step: 2
Training loss: 2.486326217651367
Validation loss: 2.303536104899581

Epoch: 5| Step: 3
Training loss: 2.4670639038085938
Validation loss: 2.3054156611042638

Epoch: 5| Step: 4
Training loss: 2.170936107635498
Validation loss: 2.308225316386069

Epoch: 5| Step: 5
Training loss: 2.9869086742401123
Validation loss: 2.3162865254186813

Epoch: 5| Step: 6
Training loss: 2.704007625579834
Validation loss: 2.3348557513247252

Epoch: 5| Step: 7
Training loss: 2.033047914505005
Validation loss: 2.3420857716632146

Epoch: 5| Step: 8
Training loss: 2.038170099258423
Validation loss: 2.3623014829492055

Epoch: 5| Step: 9
Training loss: 3.0804214477539062
Validation loss: 2.3764549045152563

Epoch: 5| Step: 10
Training loss: 2.9897806644439697
Validation loss: 2.3724091873374036

Epoch: 45| Step: 0
Training loss: 2.919180393218994
Validation loss: 2.3358016834464124

Epoch: 5| Step: 1
Training loss: 2.036440372467041
Validation loss: 2.31120926590376

Epoch: 5| Step: 2
Training loss: 2.618542194366455
Validation loss: 2.3051481785312777

Epoch: 5| Step: 3
Training loss: 2.661980152130127
Validation loss: 2.3097235361735025

Epoch: 5| Step: 4
Training loss: 2.583108425140381
Validation loss: 2.304690073895198

Epoch: 5| Step: 5
Training loss: 2.4462616443634033
Validation loss: 2.296348712777579

Epoch: 5| Step: 6
Training loss: 2.5729143619537354
Validation loss: 2.2951482598499586

Epoch: 5| Step: 7
Training loss: 2.488548517227173
Validation loss: 2.29243718424151

Epoch: 5| Step: 8
Training loss: 2.4448955059051514
Validation loss: 2.295716121632566

Epoch: 5| Step: 9
Training loss: 2.263335704803467
Validation loss: 2.2916875295741583

Epoch: 5| Step: 10
Training loss: 3.4510650634765625
Validation loss: 2.3108917231200845

Epoch: 46| Step: 0
Training loss: 2.6989595890045166
Validation loss: 2.3341784400324666

Epoch: 5| Step: 1
Training loss: 2.8785059452056885
Validation loss: 2.356639764642203

Epoch: 5| Step: 2
Training loss: 2.6417160034179688
Validation loss: 2.3537057471531693

Epoch: 5| Step: 3
Training loss: 2.5768799781799316
Validation loss: 2.3338558571313017

Epoch: 5| Step: 4
Training loss: 2.8350415229797363
Validation loss: 2.3186423188896588

Epoch: 5| Step: 5
Training loss: 1.4667997360229492
Validation loss: 2.297414425880678

Epoch: 5| Step: 6
Training loss: 3.172344923019409
Validation loss: 2.294234727018623

Epoch: 5| Step: 7
Training loss: 2.458106279373169
Validation loss: 2.296442098515008

Epoch: 5| Step: 8
Training loss: 2.730433225631714
Validation loss: 2.320339387462985

Epoch: 5| Step: 9
Training loss: 2.521139621734619
Validation loss: 2.3096716814143683

Epoch: 5| Step: 10
Training loss: 2.559513568878174
Validation loss: 2.300227303658762

Epoch: 47| Step: 0
Training loss: 2.5249710083007812
Validation loss: 2.2994323238249748

Epoch: 5| Step: 1
Training loss: 2.5806045532226562
Validation loss: 2.2940680467954246

Epoch: 5| Step: 2
Training loss: 1.9721062183380127
Validation loss: 2.304531048702937

Epoch: 5| Step: 3
Training loss: 2.5489096641540527
Validation loss: 2.3188244065930768

Epoch: 5| Step: 4
Training loss: 2.9038360118865967
Validation loss: 2.3474904491055395

Epoch: 5| Step: 5
Training loss: 3.167163372039795
Validation loss: 2.3519120600915726

Epoch: 5| Step: 6
Training loss: 2.328861713409424
Validation loss: 2.3624945225254184

Epoch: 5| Step: 7
Training loss: 2.604418992996216
Validation loss: 2.38726887395305

Epoch: 5| Step: 8
Training loss: 2.6370999813079834
Validation loss: 2.357344565852996

Epoch: 5| Step: 9
Training loss: 2.781912088394165
Validation loss: 2.363831188089104

Epoch: 5| Step: 10
Training loss: 2.1308889389038086
Validation loss: 2.32334311931364

Epoch: 48| Step: 0
Training loss: 2.6252737045288086
Validation loss: 2.2956278247217976

Epoch: 5| Step: 1
Training loss: 2.5904757976531982
Validation loss: 2.2769791592833815

Epoch: 5| Step: 2
Training loss: 2.784440279006958
Validation loss: 2.271810413688742

Epoch: 5| Step: 3
Training loss: 2.2866744995117188
Validation loss: 2.2791404647211873

Epoch: 5| Step: 4
Training loss: 2.4612629413604736
Validation loss: 2.2788884588467178

Epoch: 5| Step: 5
Training loss: 2.411011219024658
Validation loss: 2.2736359078397035

Epoch: 5| Step: 6
Training loss: 2.7252204418182373
Validation loss: 2.269178416139336

Epoch: 5| Step: 7
Training loss: 2.648746967315674
Validation loss: 2.276430868333386

Epoch: 5| Step: 8
Training loss: 2.8084053993225098
Validation loss: 2.2870189169401764

Epoch: 5| Step: 9
Training loss: 2.6444461345672607
Validation loss: 2.2937184841402116

Epoch: 5| Step: 10
Training loss: 2.477107524871826
Validation loss: 2.3016912321890555

Epoch: 49| Step: 0
Training loss: 2.3468894958496094
Validation loss: 2.289632233240271

Epoch: 5| Step: 1
Training loss: 2.080106735229492
Validation loss: 2.2831787575957594

Epoch: 5| Step: 2
Training loss: 2.7719435691833496
Validation loss: 2.2811512331808768

Epoch: 5| Step: 3
Training loss: 2.983264923095703
Validation loss: 2.273960218634657

Epoch: 5| Step: 4
Training loss: 2.288472890853882
Validation loss: 2.2771451960327806

Epoch: 5| Step: 5
Training loss: 2.556349277496338
Validation loss: 2.2791222885090816

Epoch: 5| Step: 6
Training loss: 2.0951247215270996
Validation loss: 2.2857692421123548

Epoch: 5| Step: 7
Training loss: 2.52062726020813
Validation loss: 2.303477341128934

Epoch: 5| Step: 8
Training loss: 3.142843246459961
Validation loss: 2.31557862348454

Epoch: 5| Step: 9
Training loss: 2.3073956966400146
Validation loss: 2.32611426999492

Epoch: 5| Step: 10
Training loss: 3.1225664615631104
Validation loss: 2.3514956979341406

Epoch: 50| Step: 0
Training loss: 2.1003365516662598
Validation loss: 2.3496488653203493

Epoch: 5| Step: 1
Training loss: 2.7407870292663574
Validation loss: 2.3577828202196347

Epoch: 5| Step: 2
Training loss: 1.8408416509628296
Validation loss: 2.317149059746855

Epoch: 5| Step: 3
Training loss: 2.28462815284729
Validation loss: 2.299301839643909

Epoch: 5| Step: 4
Training loss: 2.181877613067627
Validation loss: 2.2787382269418366

Epoch: 5| Step: 5
Training loss: 2.979269027709961
Validation loss: 2.2611512599452848

Epoch: 5| Step: 6
Training loss: 3.696450710296631
Validation loss: 2.2664165137916483

Epoch: 5| Step: 7
Training loss: 3.190098762512207
Validation loss: 2.270251081835839

Epoch: 5| Step: 8
Training loss: 2.374840497970581
Validation loss: 2.2846084435780845

Epoch: 5| Step: 9
Training loss: 2.578115224838257
Validation loss: 2.2883567579330935

Epoch: 5| Step: 10
Training loss: 2.345656156539917
Validation loss: 2.291561142090828

Epoch: 51| Step: 0
Training loss: 2.6088929176330566
Validation loss: 2.296193879137757

Epoch: 5| Step: 1
Training loss: 2.6097350120544434
Validation loss: 2.3022261152985277

Epoch: 5| Step: 2
Training loss: 2.8265655040740967
Validation loss: 2.306323687235514

Epoch: 5| Step: 3
Training loss: 2.753796339035034
Validation loss: 2.2835347114070768

Epoch: 5| Step: 4
Training loss: 2.555342197418213
Validation loss: 2.2685003742094962

Epoch: 5| Step: 5
Training loss: 3.1285414695739746
Validation loss: 2.2537052708287395

Epoch: 5| Step: 6
Training loss: 3.0771737098693848
Validation loss: 2.2515247355225267

Epoch: 5| Step: 7
Training loss: 2.432640552520752
Validation loss: 2.2505285714262273

Epoch: 5| Step: 8
Training loss: 2.698695659637451
Validation loss: 2.267505289405905

Epoch: 5| Step: 9
Training loss: 1.9029461145401
Validation loss: 2.3095420880984237

Epoch: 5| Step: 10
Training loss: 1.6522331237792969
Validation loss: 2.304908044876591

Epoch: 52| Step: 0
Training loss: 2.876993179321289
Validation loss: 2.2795318429188063

Epoch: 5| Step: 1
Training loss: 2.4585132598876953
Validation loss: 2.2656644390475367

Epoch: 5| Step: 2
Training loss: 2.180223226547241
Validation loss: 2.2542606143541235

Epoch: 5| Step: 3
Training loss: 2.939481019973755
Validation loss: 2.253959401961296

Epoch: 5| Step: 4
Training loss: 2.89351224899292
Validation loss: 2.2599961014204126

Epoch: 5| Step: 5
Training loss: 2.6344597339630127
Validation loss: 2.255948784530804

Epoch: 5| Step: 6
Training loss: 2.176058292388916
Validation loss: 2.248578327958302

Epoch: 5| Step: 7
Training loss: 2.4926598072052
Validation loss: 2.2411379147601385

Epoch: 5| Step: 8
Training loss: 2.1013073921203613
Validation loss: 2.2443392315218524

Epoch: 5| Step: 9
Training loss: 2.6068410873413086
Validation loss: 2.2396465732205297

Epoch: 5| Step: 10
Training loss: 2.6681156158447266
Validation loss: 2.2483105505666425

Epoch: 53| Step: 0
Training loss: 2.9241466522216797
Validation loss: 2.264312800540719

Epoch: 5| Step: 1
Training loss: 2.755053997039795
Validation loss: 2.304892704051028

Epoch: 5| Step: 2
Training loss: 2.1274094581604004
Validation loss: 2.3550843218321442

Epoch: 5| Step: 3
Training loss: 3.1481268405914307
Validation loss: 2.322126023231014

Epoch: 5| Step: 4
Training loss: 2.178941249847412
Validation loss: 2.2725231160399733

Epoch: 5| Step: 5
Training loss: 2.900599241256714
Validation loss: 2.240240316237173

Epoch: 5| Step: 6
Training loss: 1.8472131490707397
Validation loss: 2.2360430737977386

Epoch: 5| Step: 7
Training loss: 2.562025547027588
Validation loss: 2.236224066826605

Epoch: 5| Step: 8
Training loss: 2.2591326236724854
Validation loss: 2.2397259294345813

Epoch: 5| Step: 9
Training loss: 2.651198387145996
Validation loss: 2.2353672571079706

Epoch: 5| Step: 10
Training loss: 2.974561929702759
Validation loss: 2.2332764658876645

Epoch: 54| Step: 0
Training loss: 2.6409554481506348
Validation loss: 2.233316813745806

Epoch: 5| Step: 1
Training loss: 2.2080655097961426
Validation loss: 2.2421963163601455

Epoch: 5| Step: 2
Training loss: 2.536930561065674
Validation loss: 2.2475228232722126

Epoch: 5| Step: 3
Training loss: 2.529033660888672
Validation loss: 2.2602257420939784

Epoch: 5| Step: 4
Training loss: 2.6010422706604004
Validation loss: 2.261431065938806

Epoch: 5| Step: 5
Training loss: 3.19327974319458
Validation loss: 2.2708187154544297

Epoch: 5| Step: 6
Training loss: 2.939929485321045
Validation loss: 2.2708487728590607

Epoch: 5| Step: 7
Training loss: 2.657226800918579
Validation loss: 2.268923695369433

Epoch: 5| Step: 8
Training loss: 1.9132940769195557
Validation loss: 2.2553415067734255

Epoch: 5| Step: 9
Training loss: 2.69346022605896
Validation loss: 2.2417907817389375

Epoch: 5| Step: 10
Training loss: 1.9576886892318726
Validation loss: 2.232507500597226

Epoch: 55| Step: 0
Training loss: 3.1270086765289307
Validation loss: 2.2331134068068637

Epoch: 5| Step: 1
Training loss: 1.9630159139633179
Validation loss: 2.2359798364741827

Epoch: 5| Step: 2
Training loss: 2.2951951026916504
Validation loss: 2.248381814649028

Epoch: 5| Step: 3
Training loss: 2.4653351306915283
Validation loss: 2.265940648253246

Epoch: 5| Step: 4
Training loss: 2.449988603591919
Validation loss: 2.2599829217439056

Epoch: 5| Step: 5
Training loss: 2.9480607509613037
Validation loss: 2.252196547805622

Epoch: 5| Step: 6
Training loss: 1.9551922082901
Validation loss: 2.2452634688346618

Epoch: 5| Step: 7
Training loss: 2.009000539779663
Validation loss: 2.2371721831701135

Epoch: 5| Step: 8
Training loss: 2.471086263656616
Validation loss: 2.233336535833215

Epoch: 5| Step: 9
Training loss: 2.985595464706421
Validation loss: 2.2240683468439246

Epoch: 5| Step: 10
Training loss: 3.28731107711792
Validation loss: 2.220190332781884

Epoch: 56| Step: 0
Training loss: 2.6255669593811035
Validation loss: 2.22490018926641

Epoch: 5| Step: 1
Training loss: 2.938027858734131
Validation loss: 2.222154140472412

Epoch: 5| Step: 2
Training loss: 2.2065021991729736
Validation loss: 2.2234720004502164

Epoch: 5| Step: 3
Training loss: 2.486227035522461
Validation loss: 2.219975071568643

Epoch: 5| Step: 4
Training loss: 2.7241969108581543
Validation loss: 2.2294311138891403

Epoch: 5| Step: 5
Training loss: 2.3093044757843018
Validation loss: 2.231498892589282

Epoch: 5| Step: 6
Training loss: 2.6070094108581543
Validation loss: 2.2319923677752094

Epoch: 5| Step: 7
Training loss: 2.028566360473633
Validation loss: 2.2194948478411605

Epoch: 5| Step: 8
Training loss: 2.719034194946289
Validation loss: 2.218452253649312

Epoch: 5| Step: 9
Training loss: 2.3918590545654297
Validation loss: 2.2192609540877806

Epoch: 5| Step: 10
Training loss: 2.82564640045166
Validation loss: 2.2186990194423224

Epoch: 57| Step: 0
Training loss: 2.3579111099243164
Validation loss: 2.224836100814163

Epoch: 5| Step: 1
Training loss: 2.3111605644226074
Validation loss: 2.2320054167060444

Epoch: 5| Step: 2
Training loss: 3.03832745552063
Validation loss: 2.228350031760431

Epoch: 5| Step: 3
Training loss: 2.530189037322998
Validation loss: 2.223302084912536

Epoch: 5| Step: 4
Training loss: 2.7969703674316406
Validation loss: 2.2149096355643323

Epoch: 5| Step: 5
Training loss: 2.3016276359558105
Validation loss: 2.214994961215604

Epoch: 5| Step: 6
Training loss: 2.7040627002716064
Validation loss: 2.2277451651070708

Epoch: 5| Step: 7
Training loss: 2.3318278789520264
Validation loss: 2.219577876470422

Epoch: 5| Step: 8
Training loss: 2.5810065269470215
Validation loss: 2.2138293417551185

Epoch: 5| Step: 9
Training loss: 2.3461720943450928
Validation loss: 2.2054381139816774

Epoch: 5| Step: 10
Training loss: 2.3831944465637207
Validation loss: 2.208886556727912

Epoch: 58| Step: 0
Training loss: 2.476447820663452
Validation loss: 2.2192184437987623

Epoch: 5| Step: 1
Training loss: 2.9113125801086426
Validation loss: 2.218688416224654

Epoch: 5| Step: 2
Training loss: 2.441385269165039
Validation loss: 2.221493172389205

Epoch: 5| Step: 3
Training loss: 2.601060152053833
Validation loss: 2.2303577879423737

Epoch: 5| Step: 4
Training loss: 2.515153646469116
Validation loss: 2.247158460719611

Epoch: 5| Step: 5
Training loss: 3.1549696922302246
Validation loss: 2.256804943084717

Epoch: 5| Step: 6
Training loss: 2.737847089767456
Validation loss: 2.228321483058314

Epoch: 5| Step: 7
Training loss: 1.7727394104003906
Validation loss: 2.2106350365505425

Epoch: 5| Step: 8
Training loss: 2.0515189170837402
Validation loss: 2.200637055981544

Epoch: 5| Step: 9
Training loss: 2.8797643184661865
Validation loss: 2.2090928964717413

Epoch: 5| Step: 10
Training loss: 2.029212474822998
Validation loss: 2.2017579591402443

Epoch: 59| Step: 0
Training loss: 2.3928332328796387
Validation loss: 2.198955984525783

Epoch: 5| Step: 1
Training loss: 1.9795325994491577
Validation loss: 2.1978774711649907

Epoch: 5| Step: 2
Training loss: 2.173386812210083
Validation loss: 2.1987838168298044

Epoch: 5| Step: 3
Training loss: 2.902613878250122
Validation loss: 2.2044061627439273

Epoch: 5| Step: 4
Training loss: 2.4596943855285645
Validation loss: 2.2146257354367163

Epoch: 5| Step: 5
Training loss: 2.8313498497009277
Validation loss: 2.232911007378691

Epoch: 5| Step: 6
Training loss: 3.497626543045044
Validation loss: 2.2417368632490917

Epoch: 5| Step: 7
Training loss: 2.2784180641174316
Validation loss: 2.2574910963735273

Epoch: 5| Step: 8
Training loss: 1.9571082592010498
Validation loss: 2.3032911516004995

Epoch: 5| Step: 9
Training loss: 2.3476951122283936
Validation loss: 2.345702463580716

Epoch: 5| Step: 10
Training loss: 2.7762224674224854
Validation loss: 2.3414154257825626

Epoch: 60| Step: 0
Training loss: 2.8123700618743896
Validation loss: 2.2590895365643244

Epoch: 5| Step: 1
Training loss: 2.3653883934020996
Validation loss: 2.212420883999076

Epoch: 5| Step: 2
Training loss: 1.890622854232788
Validation loss: 2.213805526815435

Epoch: 5| Step: 3
Training loss: 3.169404983520508
Validation loss: 2.2192019031893824

Epoch: 5| Step: 4
Training loss: 2.0162410736083984
Validation loss: 2.247487170721895

Epoch: 5| Step: 5
Training loss: 2.2383666038513184
Validation loss: 2.2614537887675787

Epoch: 5| Step: 6
Training loss: 2.2705225944519043
Validation loss: 2.279052183192263

Epoch: 5| Step: 7
Training loss: 3.127443552017212
Validation loss: 2.2937946986126643

Epoch: 5| Step: 8
Training loss: 2.542299270629883
Validation loss: 2.2805160027678295

Epoch: 5| Step: 9
Training loss: 2.519620418548584
Validation loss: 2.2825763353737454

Epoch: 5| Step: 10
Training loss: 3.2789371013641357
Validation loss: 2.2712213095798286

Epoch: 61| Step: 0
Training loss: 2.173917531967163
Validation loss: 2.2467248657698273

Epoch: 5| Step: 1
Training loss: 3.046539783477783
Validation loss: 2.235518870815154

Epoch: 5| Step: 2
Training loss: 2.6823830604553223
Validation loss: 2.2211573162386493

Epoch: 5| Step: 3
Training loss: 2.7156455516815186
Validation loss: 2.2207766194497385

Epoch: 5| Step: 4
Training loss: 1.9060131311416626
Validation loss: 2.2308610177809194

Epoch: 5| Step: 5
Training loss: 1.9666919708251953
Validation loss: 2.2509670244750155

Epoch: 5| Step: 6
Training loss: 2.886596441268921
Validation loss: 2.2239875870366252

Epoch: 5| Step: 7
Training loss: 2.28782057762146
Validation loss: 2.2026230160908034

Epoch: 5| Step: 8
Training loss: 2.6323723793029785
Validation loss: 2.197607360860353

Epoch: 5| Step: 9
Training loss: 2.3178765773773193
Validation loss: 2.197480781103975

Epoch: 5| Step: 10
Training loss: 3.0884716510772705
Validation loss: 2.217872206882764

Epoch: 62| Step: 0
Training loss: 2.6064789295196533
Validation loss: 2.1996536947065786

Epoch: 5| Step: 1
Training loss: 2.326885938644409
Validation loss: 2.178287016448154

Epoch: 5| Step: 2
Training loss: 2.323701858520508
Validation loss: 2.1665120176089707

Epoch: 5| Step: 3
Training loss: 2.2594590187072754
Validation loss: 2.171613198454662

Epoch: 5| Step: 4
Training loss: 3.1401095390319824
Validation loss: 2.1674380199883574

Epoch: 5| Step: 5
Training loss: 2.708199977874756
Validation loss: 2.164058090538107

Epoch: 5| Step: 6
Training loss: 2.601428747177124
Validation loss: 2.160333521904484

Epoch: 5| Step: 7
Training loss: 1.9825677871704102
Validation loss: 2.1747890723648893

Epoch: 5| Step: 8
Training loss: 2.7619361877441406
Validation loss: 2.205856348878594

Epoch: 5| Step: 9
Training loss: 2.5568859577178955
Validation loss: 2.2484090610217025

Epoch: 5| Step: 10
Training loss: 2.3636395931243896
Validation loss: 2.3328400376022502

Epoch: 63| Step: 0
Training loss: 2.614410877227783
Validation loss: 2.3745351760618147

Epoch: 5| Step: 1
Training loss: 2.100619077682495
Validation loss: 2.3690718489308513

Epoch: 5| Step: 2
Training loss: 2.084782123565674
Validation loss: 2.273551438444404

Epoch: 5| Step: 3
Training loss: 2.944366931915283
Validation loss: 2.1999809613791843

Epoch: 5| Step: 4
Training loss: 2.724388599395752
Validation loss: 2.168179870933615

Epoch: 5| Step: 5
Training loss: 2.5715649127960205
Validation loss: 2.165429087095363

Epoch: 5| Step: 6
Training loss: 2.3381147384643555
Validation loss: 2.1724319740008284

Epoch: 5| Step: 7
Training loss: 3.025116443634033
Validation loss: 2.1972397732478317

Epoch: 5| Step: 8
Training loss: 3.1223790645599365
Validation loss: 2.2443018882505354

Epoch: 5| Step: 9
Training loss: 1.9258079528808594
Validation loss: 2.2857213661234868

Epoch: 5| Step: 10
Training loss: 2.9921560287475586
Validation loss: 2.327223257351947

Epoch: 64| Step: 0
Training loss: 2.1739094257354736
Validation loss: 2.3177984312016475

Epoch: 5| Step: 1
Training loss: 2.653210163116455
Validation loss: 2.2820916970570884

Epoch: 5| Step: 2
Training loss: 2.7628722190856934
Validation loss: 2.2750544727489515

Epoch: 5| Step: 3
Training loss: 2.7756500244140625
Validation loss: 2.2588759904266684

Epoch: 5| Step: 4
Training loss: 3.2484002113342285
Validation loss: 2.2554840246836343

Epoch: 5| Step: 5
Training loss: 2.5372211933135986
Validation loss: 2.251077103358443

Epoch: 5| Step: 6
Training loss: 2.41207218170166
Validation loss: 2.2495126121787616

Epoch: 5| Step: 7
Training loss: 2.455960750579834
Validation loss: 2.2594958659141295

Epoch: 5| Step: 8
Training loss: 2.307204246520996
Validation loss: 2.282914392409786

Epoch: 5| Step: 9
Training loss: 2.1411938667297363
Validation loss: 2.2856828422956568

Epoch: 5| Step: 10
Training loss: 2.516303539276123
Validation loss: 2.2574668251058108

Epoch: 65| Step: 0
Training loss: 2.189774751663208
Validation loss: 2.209234881144698

Epoch: 5| Step: 1
Training loss: 2.238367795944214
Validation loss: 2.175677499463481

Epoch: 5| Step: 2
Training loss: 2.7451119422912598
Validation loss: 2.1702165244728007

Epoch: 5| Step: 3
Training loss: 2.772660732269287
Validation loss: 2.1625604501334568

Epoch: 5| Step: 4
Training loss: 3.3811588287353516
Validation loss: 2.1639851126619565

Epoch: 5| Step: 5
Training loss: 2.4961493015289307
Validation loss: 2.1644903203492523

Epoch: 5| Step: 6
Training loss: 2.0120604038238525
Validation loss: 2.169473353252616

Epoch: 5| Step: 7
Training loss: 2.675351619720459
Validation loss: 2.1661692434741604

Epoch: 5| Step: 8
Training loss: 2.5195956230163574
Validation loss: 2.169360278755106

Epoch: 5| Step: 9
Training loss: 2.241084575653076
Validation loss: 2.167128873127763

Epoch: 5| Step: 10
Training loss: 2.0892646312713623
Validation loss: 2.1659883004362865

Epoch: 66| Step: 0
Training loss: 2.3559815883636475
Validation loss: 2.167087167821905

Epoch: 5| Step: 1
Training loss: 2.939640998840332
Validation loss: 2.1818805997089674

Epoch: 5| Step: 2
Training loss: 2.6178579330444336
Validation loss: 2.2073393483315744

Epoch: 5| Step: 3
Training loss: 2.0174307823181152
Validation loss: 2.210086399509061

Epoch: 5| Step: 4
Training loss: 2.075021266937256
Validation loss: 2.2690462707191386

Epoch: 5| Step: 5
Training loss: 2.9889423847198486
Validation loss: 2.2361647646914244

Epoch: 5| Step: 6
Training loss: 2.619706630706787
Validation loss: 2.1904828984250306

Epoch: 5| Step: 7
Training loss: 2.231907367706299
Validation loss: 2.1663211417454544

Epoch: 5| Step: 8
Training loss: 2.552929639816284
Validation loss: 2.15244956426723

Epoch: 5| Step: 9
Training loss: 2.2336246967315674
Validation loss: 2.1536135006976385

Epoch: 5| Step: 10
Training loss: 2.8265151977539062
Validation loss: 2.1631096998850503

Epoch: 67| Step: 0
Training loss: 2.7232322692871094
Validation loss: 2.166314414752427

Epoch: 5| Step: 1
Training loss: 2.699089765548706
Validation loss: 2.168914779540031

Epoch: 5| Step: 2
Training loss: 2.9253103733062744
Validation loss: 2.1751287368036087

Epoch: 5| Step: 3
Training loss: 2.1002461910247803
Validation loss: 2.169008542132634

Epoch: 5| Step: 4
Training loss: 2.7982757091522217
Validation loss: 2.173604908809867

Epoch: 5| Step: 5
Training loss: 2.141366481781006
Validation loss: 2.1768007534806446

Epoch: 5| Step: 6
Training loss: 2.2836270332336426
Validation loss: 2.1753596259701635

Epoch: 5| Step: 7
Training loss: 2.883253574371338
Validation loss: 2.1800457995424987

Epoch: 5| Step: 8
Training loss: 2.6675021648406982
Validation loss: 2.1783371433135

Epoch: 5| Step: 9
Training loss: 2.0365192890167236
Validation loss: 2.182756475223008

Epoch: 5| Step: 10
Training loss: 2.068269968032837
Validation loss: 2.1907475122841458

Epoch: 68| Step: 0
Training loss: 2.1709206104278564
Validation loss: 2.215586482837636

Epoch: 5| Step: 1
Training loss: 1.8063644170761108
Validation loss: 2.2335215999234106

Epoch: 5| Step: 2
Training loss: 2.6766109466552734
Validation loss: 2.259603387566023

Epoch: 5| Step: 3
Training loss: 1.9802286624908447
Validation loss: 2.282089747408385

Epoch: 5| Step: 4
Training loss: 2.519843816757202
Validation loss: 2.2740961890066824

Epoch: 5| Step: 5
Training loss: 2.751192331314087
Validation loss: 2.2759067525145826

Epoch: 5| Step: 6
Training loss: 2.2309935092926025
Validation loss: 2.250325133723597

Epoch: 5| Step: 7
Training loss: 3.267425060272217
Validation loss: 2.2447081765820904

Epoch: 5| Step: 8
Training loss: 2.6160035133361816
Validation loss: 2.184537632490999

Epoch: 5| Step: 9
Training loss: 2.6715891361236572
Validation loss: 2.1607278085524038

Epoch: 5| Step: 10
Training loss: 2.62540602684021
Validation loss: 2.142795667853407

Epoch: 69| Step: 0
Training loss: 2.2977774143218994
Validation loss: 2.134714316296321

Epoch: 5| Step: 1
Training loss: 1.900006890296936
Validation loss: 2.1361379623413086

Epoch: 5| Step: 2
Training loss: 2.3701260089874268
Validation loss: 2.146304561245826

Epoch: 5| Step: 3
Training loss: 3.0307013988494873
Validation loss: 2.159197535566104

Epoch: 5| Step: 4
Training loss: 2.6004462242126465
Validation loss: 2.165598137404329

Epoch: 5| Step: 5
Training loss: 2.818591594696045
Validation loss: 2.177702203873665

Epoch: 5| Step: 6
Training loss: 2.40655779838562
Validation loss: 2.149004372217322

Epoch: 5| Step: 7
Training loss: 2.5461790561676025
Validation loss: 2.157005038312686

Epoch: 5| Step: 8
Training loss: 2.9144082069396973
Validation loss: 2.1672969018259356

Epoch: 5| Step: 9
Training loss: 2.3197193145751953
Validation loss: 2.199269340884301

Epoch: 5| Step: 10
Training loss: 2.058260440826416
Validation loss: 2.2419121778139504

Epoch: 70| Step: 0
Training loss: 2.5196659564971924
Validation loss: 2.278056352369247

Epoch: 5| Step: 1
Training loss: 2.492729663848877
Validation loss: 2.242741959069365

Epoch: 5| Step: 2
Training loss: 2.83185076713562
Validation loss: 2.2180507926530737

Epoch: 5| Step: 3
Training loss: 2.9223175048828125
Validation loss: 2.165825184955392

Epoch: 5| Step: 4
Training loss: 2.398300886154175
Validation loss: 2.1459114936090287

Epoch: 5| Step: 5
Training loss: 2.1079659461975098
Validation loss: 2.139680129225536

Epoch: 5| Step: 6
Training loss: 2.3912014961242676
Validation loss: 2.1361345001446304

Epoch: 5| Step: 7
Training loss: 2.9024369716644287
Validation loss: 2.1306957314091344

Epoch: 5| Step: 8
Training loss: 2.4419167041778564
Validation loss: 2.134404287543348

Epoch: 5| Step: 9
Training loss: 2.411463975906372
Validation loss: 2.141927557606851

Epoch: 5| Step: 10
Training loss: 1.9729982614517212
Validation loss: 2.1533832973049534

Epoch: 71| Step: 0
Training loss: 2.3593413829803467
Validation loss: 2.1784033724056777

Epoch: 5| Step: 1
Training loss: 2.260422468185425
Validation loss: 2.205060061588082

Epoch: 5| Step: 2
Training loss: 2.744692802429199
Validation loss: 2.2490517631653817

Epoch: 5| Step: 3
Training loss: 2.723832845687866
Validation loss: 2.229477720875894

Epoch: 5| Step: 4
Training loss: 1.8476979732513428
Validation loss: 2.187050906560754

Epoch: 5| Step: 5
Training loss: 3.0011260509490967
Validation loss: 2.1603403373431136

Epoch: 5| Step: 6
Training loss: 2.5095982551574707
Validation loss: 2.1376014704345376

Epoch: 5| Step: 7
Training loss: 2.795144557952881
Validation loss: 2.13263887487432

Epoch: 5| Step: 8
Training loss: 2.0047078132629395
Validation loss: 2.1310179733460948

Epoch: 5| Step: 9
Training loss: 2.8367228507995605
Validation loss: 2.1377313265236477

Epoch: 5| Step: 10
Training loss: 2.0795717239379883
Validation loss: 2.1337365078669723

Epoch: 72| Step: 0
Training loss: 2.1864218711853027
Validation loss: 2.1361141256106797

Epoch: 5| Step: 1
Training loss: 3.039423942565918
Validation loss: 2.1346440802338305

Epoch: 5| Step: 2
Training loss: 2.307644844055176
Validation loss: 2.1428962753665064

Epoch: 5| Step: 3
Training loss: 1.8622032403945923
Validation loss: 2.15837533755969

Epoch: 5| Step: 4
Training loss: 2.971790313720703
Validation loss: 2.1740234205799718

Epoch: 5| Step: 5
Training loss: 3.099550247192383
Validation loss: 2.187336649945987

Epoch: 5| Step: 6
Training loss: 1.982576608657837
Validation loss: 2.22628971838182

Epoch: 5| Step: 7
Training loss: 2.6541800498962402
Validation loss: 2.2380591336116997

Epoch: 5| Step: 8
Training loss: 2.483121871948242
Validation loss: 2.237705942123167

Epoch: 5| Step: 9
Training loss: 2.083202838897705
Validation loss: 2.1917801518594064

Epoch: 5| Step: 10
Training loss: 2.3866686820983887
Validation loss: 2.1591074928160636

Epoch: 73| Step: 0
Training loss: 2.568136215209961
Validation loss: 2.1398179133733115

Epoch: 5| Step: 1
Training loss: 2.5793678760528564
Validation loss: 2.1169886166049587

Epoch: 5| Step: 2
Training loss: 2.195866823196411
Validation loss: 2.11709733932249

Epoch: 5| Step: 3
Training loss: 2.4834630489349365
Validation loss: 2.1117823687932824

Epoch: 5| Step: 4
Training loss: 2.212881565093994
Validation loss: 2.125934544429984

Epoch: 5| Step: 5
Training loss: 2.6476876735687256
Validation loss: 2.130089149680189

Epoch: 5| Step: 6
Training loss: 2.330760955810547
Validation loss: 2.146657174633395

Epoch: 5| Step: 7
Training loss: 2.257868528366089
Validation loss: 2.1546071101260442

Epoch: 5| Step: 8
Training loss: 2.247422218322754
Validation loss: 2.1611719336560977

Epoch: 5| Step: 9
Training loss: 2.609532594680786
Validation loss: 2.158602732484059

Epoch: 5| Step: 10
Training loss: 2.908182144165039
Validation loss: 2.1492612182453112

Epoch: 74| Step: 0
Training loss: 2.7898879051208496
Validation loss: 2.140387438958691

Epoch: 5| Step: 1
Training loss: 2.110097646713257
Validation loss: 2.133011899968629

Epoch: 5| Step: 2
Training loss: 2.092755079269409
Validation loss: 2.130863538352392

Epoch: 5| Step: 3
Training loss: 2.5784153938293457
Validation loss: 2.1268832119562293

Epoch: 5| Step: 4
Training loss: 2.8141579627990723
Validation loss: 2.133878477158085

Epoch: 5| Step: 5
Training loss: 2.2778127193450928
Validation loss: 2.144983947917979

Epoch: 5| Step: 6
Training loss: 2.7971017360687256
Validation loss: 2.1550341524103636

Epoch: 5| Step: 7
Training loss: 2.669851779937744
Validation loss: 2.1721423338818293

Epoch: 5| Step: 8
Training loss: 2.1777164936065674
Validation loss: 2.169382934929222

Epoch: 5| Step: 9
Training loss: 2.5067546367645264
Validation loss: 2.1612147182546635

Epoch: 5| Step: 10
Training loss: 1.8744105100631714
Validation loss: 2.154127505517775

Epoch: 75| Step: 0
Training loss: 2.599865674972534
Validation loss: 2.1252777653355754

Epoch: 5| Step: 1
Training loss: 2.4494900703430176
Validation loss: 2.120833020056448

Epoch: 5| Step: 2
Training loss: 2.5689239501953125
Validation loss: 2.1222350059017057

Epoch: 5| Step: 3
Training loss: 2.5388364791870117
Validation loss: 2.1246648039869083

Epoch: 5| Step: 4
Training loss: 2.1798274517059326
Validation loss: 2.113880249761766

Epoch: 5| Step: 5
Training loss: 2.2112061977386475
Validation loss: 2.1189836353384037

Epoch: 5| Step: 6
Training loss: 2.6361594200134277
Validation loss: 2.1331055959065757

Epoch: 5| Step: 7
Training loss: 2.4925241470336914
Validation loss: 2.139137424448485

Epoch: 5| Step: 8
Training loss: 1.8156684637069702
Validation loss: 2.145922455736386

Epoch: 5| Step: 9
Training loss: 2.628251552581787
Validation loss: 2.1636398094956593

Epoch: 5| Step: 10
Training loss: 2.6811485290527344
Validation loss: 2.1785887184963433

Epoch: 76| Step: 0
Training loss: 2.713371753692627
Validation loss: 2.1834604842688448

Epoch: 5| Step: 1
Training loss: 2.6645431518554688
Validation loss: 2.197498236933062

Epoch: 5| Step: 2
Training loss: 2.601137638092041
Validation loss: 2.2081645483611734

Epoch: 5| Step: 3
Training loss: 1.9744151830673218
Validation loss: 2.2125294490527083

Epoch: 5| Step: 4
Training loss: 2.75311541557312
Validation loss: 2.2016346057256064

Epoch: 5| Step: 5
Training loss: 1.9630515575408936
Validation loss: 2.180921951929728

Epoch: 5| Step: 6
Training loss: 2.8046562671661377
Validation loss: 2.172537807495363

Epoch: 5| Step: 7
Training loss: 1.9870541095733643
Validation loss: 2.141420818144275

Epoch: 5| Step: 8
Training loss: 2.7666807174682617
Validation loss: 2.1293707124648558

Epoch: 5| Step: 9
Training loss: 2.4623219966888428
Validation loss: 2.1228948280375493

Epoch: 5| Step: 10
Training loss: 1.8522142171859741
Validation loss: 2.124635470810757

Epoch: 77| Step: 0
Training loss: 2.2904272079467773
Validation loss: 2.1522761685873872

Epoch: 5| Step: 1
Training loss: 2.827498197555542
Validation loss: 2.181589247078024

Epoch: 5| Step: 2
Training loss: 2.4478187561035156
Validation loss: 2.1626317013976393

Epoch: 5| Step: 3
Training loss: 2.204432487487793
Validation loss: 2.136320221808649

Epoch: 5| Step: 4
Training loss: 1.726036787033081
Validation loss: 2.1261983712514243

Epoch: 5| Step: 5
Training loss: 2.5107650756835938
Validation loss: 2.1173129158635295

Epoch: 5| Step: 6
Training loss: 2.47298526763916
Validation loss: 2.103617916824997

Epoch: 5| Step: 7
Training loss: 2.8594489097595215
Validation loss: 2.106728192298643

Epoch: 5| Step: 8
Training loss: 2.508537769317627
Validation loss: 2.1994356724523727

Epoch: 5| Step: 9
Training loss: 2.854177951812744
Validation loss: 2.282414000521424

Epoch: 5| Step: 10
Training loss: 2.965468406677246
Validation loss: 2.256397006332233

Epoch: 78| Step: 0
Training loss: 2.500307559967041
Validation loss: 2.190131215639012

Epoch: 5| Step: 1
Training loss: 2.900266170501709
Validation loss: 2.170176212505628

Epoch: 5| Step: 2
Training loss: 2.1797139644622803
Validation loss: 2.132919770415111

Epoch: 5| Step: 3
Training loss: 2.48211407661438
Validation loss: 2.1184320860011603

Epoch: 5| Step: 4
Training loss: 2.855262279510498
Validation loss: 2.121902752948064

Epoch: 5| Step: 5
Training loss: 1.8875877857208252
Validation loss: 2.12386425336202

Epoch: 5| Step: 6
Training loss: 2.3805785179138184
Validation loss: 2.1284832467315016

Epoch: 5| Step: 7
Training loss: 2.4975364208221436
Validation loss: 2.133068109071383

Epoch: 5| Step: 8
Training loss: 2.5621895790100098
Validation loss: 2.128633286363335

Epoch: 5| Step: 9
Training loss: 1.9943736791610718
Validation loss: 2.144053307912683

Epoch: 5| Step: 10
Training loss: 2.469555616378784
Validation loss: 2.1356614635836695

Epoch: 79| Step: 0
Training loss: 2.469539165496826
Validation loss: 2.127531632300346

Epoch: 5| Step: 1
Training loss: 2.6771531105041504
Validation loss: 2.1255119795440347

Epoch: 5| Step: 2
Training loss: 1.8348594903945923
Validation loss: 2.1230538609207317

Epoch: 5| Step: 3
Training loss: 2.834645986557007
Validation loss: 2.113979777982158

Epoch: 5| Step: 4
Training loss: 2.813711404800415
Validation loss: 2.112004533890755

Epoch: 5| Step: 5
Training loss: 2.1811165809631348
Validation loss: 2.115936110096593

Epoch: 5| Step: 6
Training loss: 1.7654577493667603
Validation loss: 2.1239128446066253

Epoch: 5| Step: 7
Training loss: 2.7561001777648926
Validation loss: 2.140807000539636

Epoch: 5| Step: 8
Training loss: 2.428579330444336
Validation loss: 2.180742931622331

Epoch: 5| Step: 9
Training loss: 2.7941904067993164
Validation loss: 2.217989506260041

Epoch: 5| Step: 10
Training loss: 2.1974194049835205
Validation loss: 2.248185591031146

Epoch: 80| Step: 0
Training loss: 2.6169564723968506
Validation loss: 2.3130778112719135

Epoch: 5| Step: 1
Training loss: 1.9042068719863892
Validation loss: 2.345739118514522

Epoch: 5| Step: 2
Training loss: 3.116560697555542
Validation loss: 2.338605450045678

Epoch: 5| Step: 3
Training loss: 2.327533721923828
Validation loss: 2.350819456961847

Epoch: 5| Step: 4
Training loss: 2.992121458053589
Validation loss: 2.29261334993506

Epoch: 5| Step: 5
Training loss: 3.0470669269561768
Validation loss: 2.2236846775136967

Epoch: 5| Step: 6
Training loss: 2.0359795093536377
Validation loss: 2.149922385010668

Epoch: 5| Step: 7
Training loss: 2.4729180335998535
Validation loss: 2.1339760236842658

Epoch: 5| Step: 8
Training loss: 2.5045759677886963
Validation loss: 2.1161448109534478

Epoch: 5| Step: 9
Training loss: 1.7849647998809814
Validation loss: 2.1217093480530607

Epoch: 5| Step: 10
Training loss: 2.312110662460327
Validation loss: 2.1106340731343916

Epoch: 81| Step: 0
Training loss: 2.1546454429626465
Validation loss: 2.1128183769923385

Epoch: 5| Step: 1
Training loss: 1.8542811870574951
Validation loss: 2.113662281344014

Epoch: 5| Step: 2
Training loss: 2.304971218109131
Validation loss: 2.122469876402168

Epoch: 5| Step: 3
Training loss: 2.7652747631073
Validation loss: 2.1316922223696144

Epoch: 5| Step: 4
Training loss: 2.841585636138916
Validation loss: 2.1424423007554907

Epoch: 5| Step: 5
Training loss: 2.2225170135498047
Validation loss: 2.1495703420331402

Epoch: 5| Step: 6
Training loss: 2.8244621753692627
Validation loss: 2.156940198713733

Epoch: 5| Step: 7
Training loss: 2.8818368911743164
Validation loss: 2.176247812086536

Epoch: 5| Step: 8
Training loss: 2.495574474334717
Validation loss: 2.2167721717588362

Epoch: 5| Step: 9
Training loss: 2.4811699390411377
Validation loss: 2.2340152699460267

Epoch: 5| Step: 10
Training loss: 1.9760055541992188
Validation loss: 2.200183545389483

Epoch: 82| Step: 0
Training loss: 2.1740570068359375
Validation loss: 2.198433147963657

Epoch: 5| Step: 1
Training loss: 2.4564080238342285
Validation loss: 2.1614171997193368

Epoch: 5| Step: 2
Training loss: 2.8273494243621826
Validation loss: 2.1370781929262224

Epoch: 5| Step: 3
Training loss: 2.372767686843872
Validation loss: 2.122372873367802

Epoch: 5| Step: 4
Training loss: 2.1432576179504395
Validation loss: 2.133840681404196

Epoch: 5| Step: 5
Training loss: 2.563416004180908
Validation loss: 2.1273150700394825

Epoch: 5| Step: 6
Training loss: 2.1934525966644287
Validation loss: 2.1310625691567697

Epoch: 5| Step: 7
Training loss: 2.1589303016662598
Validation loss: 2.1351826498585362

Epoch: 5| Step: 8
Training loss: 2.4510364532470703
Validation loss: 2.1486755391602874

Epoch: 5| Step: 9
Training loss: 2.5192606449127197
Validation loss: 2.15741515415971

Epoch: 5| Step: 10
Training loss: 3.0391483306884766
Validation loss: 2.18566825825681

Epoch: 83| Step: 0
Training loss: 2.8034920692443848
Validation loss: 2.185648500278432

Epoch: 5| Step: 1
Training loss: 2.2060322761535645
Validation loss: 2.154704575897545

Epoch: 5| Step: 2
Training loss: 2.3662421703338623
Validation loss: 2.1250321095989597

Epoch: 5| Step: 3
Training loss: 2.4088521003723145
Validation loss: 2.116803592251193

Epoch: 5| Step: 4
Training loss: 2.7295169830322266
Validation loss: 2.083617470597708

Epoch: 5| Step: 5
Training loss: 2.5812935829162598
Validation loss: 2.08083237114773

Epoch: 5| Step: 6
Training loss: 2.308570146560669
Validation loss: 2.0762057945292485

Epoch: 5| Step: 7
Training loss: 2.0370819568634033
Validation loss: 2.076279893998177

Epoch: 5| Step: 8
Training loss: 2.811082363128662
Validation loss: 2.097337447186952

Epoch: 5| Step: 9
Training loss: 2.1123929023742676
Validation loss: 2.1056012543298865

Epoch: 5| Step: 10
Training loss: 2.403752088546753
Validation loss: 2.1271957710225093

Epoch: 84| Step: 0
Training loss: 2.31862211227417
Validation loss: 2.14584171131093

Epoch: 5| Step: 1
Training loss: 2.7846548557281494
Validation loss: 2.184879741361064

Epoch: 5| Step: 2
Training loss: 2.1466081142425537
Validation loss: 2.164825067725233

Epoch: 5| Step: 3
Training loss: 3.1257736682891846
Validation loss: 2.134565127793179

Epoch: 5| Step: 4
Training loss: 2.476121425628662
Validation loss: 2.1054687243635937

Epoch: 5| Step: 5
Training loss: 2.629110336303711
Validation loss: 2.092001143322196

Epoch: 5| Step: 6
Training loss: 2.2471699714660645
Validation loss: 2.0814081597071823

Epoch: 5| Step: 7
Training loss: 2.336298942565918
Validation loss: 2.0723953093251875

Epoch: 5| Step: 8
Training loss: 2.37499737739563
Validation loss: 2.0707270817090104

Epoch: 5| Step: 9
Training loss: 2.168240785598755
Validation loss: 2.060840411852765

Epoch: 5| Step: 10
Training loss: 1.863861322402954
Validation loss: 2.0694098998141546

Epoch: 85| Step: 0
Training loss: 2.5839524269104004
Validation loss: 2.068057821642968

Epoch: 5| Step: 1
Training loss: 2.1296448707580566
Validation loss: 2.073785728023898

Epoch: 5| Step: 2
Training loss: 2.4287309646606445
Validation loss: 2.0830326682777813

Epoch: 5| Step: 3
Training loss: 2.5857436656951904
Validation loss: 2.122845342082362

Epoch: 5| Step: 4
Training loss: 2.206573486328125
Validation loss: 2.115873227837265

Epoch: 5| Step: 5
Training loss: 1.9860870838165283
Validation loss: 2.1221467448819067

Epoch: 5| Step: 6
Training loss: 2.7043094635009766
Validation loss: 2.110082685306508

Epoch: 5| Step: 7
Training loss: 1.9708198308944702
Validation loss: 2.103034593725717

Epoch: 5| Step: 8
Training loss: 2.864320755004883
Validation loss: 2.0880765991826213

Epoch: 5| Step: 9
Training loss: 2.624906539916992
Validation loss: 2.0746442182089693

Epoch: 5| Step: 10
Training loss: 2.1884570121765137
Validation loss: 2.070704698562622

Epoch: 86| Step: 0
Training loss: 2.012253761291504
Validation loss: 2.067535409363367

Epoch: 5| Step: 1
Training loss: 2.5490119457244873
Validation loss: 2.06418796764907

Epoch: 5| Step: 2
Training loss: 1.945662260055542
Validation loss: 2.059554188482223

Epoch: 5| Step: 3
Training loss: 2.7396621704101562
Validation loss: 2.0668634958164667

Epoch: 5| Step: 4
Training loss: 1.6409717798233032
Validation loss: 2.0703844972836074

Epoch: 5| Step: 5
Training loss: 2.8224778175354004
Validation loss: 2.072040378406484

Epoch: 5| Step: 6
Training loss: 1.7382497787475586
Validation loss: 2.0904996395111084

Epoch: 5| Step: 7
Training loss: 2.7368998527526855
Validation loss: 2.1127635176463793

Epoch: 5| Step: 8
Training loss: 3.1019983291625977
Validation loss: 2.1294678923904256

Epoch: 5| Step: 9
Training loss: 2.711470603942871
Validation loss: 2.136310477410593

Epoch: 5| Step: 10
Training loss: 2.42033052444458
Validation loss: 2.1536517809796076

Epoch: 87| Step: 0
Training loss: 1.6471929550170898
Validation loss: 2.141907802192114

Epoch: 5| Step: 1
Training loss: 2.796360731124878
Validation loss: 2.138430885089341

Epoch: 5| Step: 2
Training loss: 2.620331048965454
Validation loss: 2.1140326440975232

Epoch: 5| Step: 3
Training loss: 3.0225260257720947
Validation loss: 2.1003836470265544

Epoch: 5| Step: 4
Training loss: 2.0792911052703857
Validation loss: 2.0847556027032996

Epoch: 5| Step: 5
Training loss: 2.2182087898254395
Validation loss: 2.081792954475649

Epoch: 5| Step: 6
Training loss: 2.5208897590637207
Validation loss: 2.070045255845593

Epoch: 5| Step: 7
Training loss: 2.335141658782959
Validation loss: 2.060959736506144

Epoch: 5| Step: 8
Training loss: 2.5682032108306885
Validation loss: 2.058846894130912

Epoch: 5| Step: 9
Training loss: 2.093763589859009
Validation loss: 2.0479991013003933

Epoch: 5| Step: 10
Training loss: 2.183962345123291
Validation loss: 2.052588039828885

Epoch: 88| Step: 0
Training loss: 2.5213019847869873
Validation loss: 2.0566651910863896

Epoch: 5| Step: 1
Training loss: 2.6420340538024902
Validation loss: 2.0646077074030393

Epoch: 5| Step: 2
Training loss: 3.0615594387054443
Validation loss: 2.0811185362518474

Epoch: 5| Step: 3
Training loss: 2.2610924243927
Validation loss: 2.073219915871979

Epoch: 5| Step: 4
Training loss: 2.218550205230713
Validation loss: 2.0996539426106278

Epoch: 5| Step: 5
Training loss: 2.9602952003479004
Validation loss: 2.1044770081837973

Epoch: 5| Step: 6
Training loss: 1.5827569961547852
Validation loss: 2.108869873067384

Epoch: 5| Step: 7
Training loss: 2.272796154022217
Validation loss: 2.0906777381896973

Epoch: 5| Step: 8
Training loss: 1.9498260021209717
Validation loss: 2.090286421519454

Epoch: 5| Step: 9
Training loss: 2.4522593021392822
Validation loss: 2.084319929922781

Epoch: 5| Step: 10
Training loss: 2.2118022441864014
Validation loss: 2.0951462791812037

Epoch: 89| Step: 0
Training loss: 2.1208252906799316
Validation loss: 2.091340118838895

Epoch: 5| Step: 1
Training loss: 2.525291919708252
Validation loss: 2.066790473076605

Epoch: 5| Step: 2
Training loss: 2.1446449756622314
Validation loss: 2.0705479037377144

Epoch: 5| Step: 3
Training loss: 2.116349458694458
Validation loss: 2.072294642848353

Epoch: 5| Step: 4
Training loss: 1.7492471933364868
Validation loss: 2.0957708589492308

Epoch: 5| Step: 5
Training loss: 2.4591972827911377
Validation loss: 2.081964759416478

Epoch: 5| Step: 6
Training loss: 3.0180013179779053
Validation loss: 2.0825594138073664

Epoch: 5| Step: 7
Training loss: 2.6140942573547363
Validation loss: 2.0740325373987996

Epoch: 5| Step: 8
Training loss: 2.4743874073028564
Validation loss: 2.0677297358871787

Epoch: 5| Step: 9
Training loss: 2.819228410720825
Validation loss: 2.0624456790185746

Epoch: 5| Step: 10
Training loss: 1.8894953727722168
Validation loss: 2.065473137363311

Epoch: 90| Step: 0
Training loss: 2.875734567642212
Validation loss: 2.063098003787379

Epoch: 5| Step: 1
Training loss: 2.4562127590179443
Validation loss: 2.0819447168739895

Epoch: 5| Step: 2
Training loss: 2.1294333934783936
Validation loss: 2.1006840018815893

Epoch: 5| Step: 3
Training loss: 2.9418411254882812
Validation loss: 2.109926087881929

Epoch: 5| Step: 4
Training loss: 2.363487482070923
Validation loss: 2.1269500691403627

Epoch: 5| Step: 5
Training loss: 1.3438292741775513
Validation loss: 2.135869282548146

Epoch: 5| Step: 6
Training loss: 2.6144444942474365
Validation loss: 2.116225032396214

Epoch: 5| Step: 7
Training loss: 2.0001227855682373
Validation loss: 2.071909725025136

Epoch: 5| Step: 8
Training loss: 2.4306442737579346
Validation loss: 2.0545049610958306

Epoch: 5| Step: 9
Training loss: 2.6758265495300293
Validation loss: 2.055662129514961

Epoch: 5| Step: 10
Training loss: 2.338343858718872
Validation loss: 2.057906158508793

Epoch: 91| Step: 0
Training loss: 2.02649188041687
Validation loss: 2.0669652633769537

Epoch: 5| Step: 1
Training loss: 2.1379590034484863
Validation loss: 2.05466083429193

Epoch: 5| Step: 2
Training loss: 2.439966917037964
Validation loss: 2.0463722136712845

Epoch: 5| Step: 3
Training loss: 2.5800273418426514
Validation loss: 2.0543394447654806

Epoch: 5| Step: 4
Training loss: 2.1703453063964844
Validation loss: 2.0701176927935694

Epoch: 5| Step: 5
Training loss: 2.7856342792510986
Validation loss: 2.1173215245687835

Epoch: 5| Step: 6
Training loss: 2.75942063331604
Validation loss: 2.168383372727261

Epoch: 5| Step: 7
Training loss: 2.3747076988220215
Validation loss: 2.1880637932849187

Epoch: 5| Step: 8
Training loss: 2.5843372344970703
Validation loss: 2.159081474427254

Epoch: 5| Step: 9
Training loss: 2.1569652557373047
Validation loss: 2.145567872190988

Epoch: 5| Step: 10
Training loss: 2.1929821968078613
Validation loss: 2.11558900853639

Epoch: 92| Step: 0
Training loss: 2.3137335777282715
Validation loss: 2.074663164795086

Epoch: 5| Step: 1
Training loss: 2.0280635356903076
Validation loss: 2.066922354441817

Epoch: 5| Step: 2
Training loss: 2.140038013458252
Validation loss: 2.069503745725078

Epoch: 5| Step: 3
Training loss: 2.4069149494171143
Validation loss: 2.0684719829149145

Epoch: 5| Step: 4
Training loss: 2.2835938930511475
Validation loss: 2.0859521486425914

Epoch: 5| Step: 5
Training loss: 3.419286012649536
Validation loss: 2.105668834460679

Epoch: 5| Step: 6
Training loss: 2.8389062881469727
Validation loss: 2.1158409041743123

Epoch: 5| Step: 7
Training loss: 2.688671588897705
Validation loss: 2.1181331783212642

Epoch: 5| Step: 8
Training loss: 2.00173020362854
Validation loss: 2.1018690447653494

Epoch: 5| Step: 9
Training loss: 1.600083351135254
Validation loss: 2.0593111489408757

Epoch: 5| Step: 10
Training loss: 2.154719591140747
Validation loss: 2.0483131793237503

Epoch: 93| Step: 0
Training loss: 2.557816982269287
Validation loss: 2.0464857739786946

Epoch: 5| Step: 1
Training loss: 2.0857043266296387
Validation loss: 2.042440688738259

Epoch: 5| Step: 2
Training loss: 2.468776226043701
Validation loss: 2.0389920767917427

Epoch: 5| Step: 3
Training loss: 2.2870640754699707
Validation loss: 2.04235254820957

Epoch: 5| Step: 4
Training loss: 1.811975121498108
Validation loss: 2.036085272348055

Epoch: 5| Step: 5
Training loss: 2.3641343116760254
Validation loss: 2.0360451462448284

Epoch: 5| Step: 6
Training loss: 2.9541287422180176
Validation loss: 2.0405860280477874

Epoch: 5| Step: 7
Training loss: 2.4971251487731934
Validation loss: 2.049564512827063

Epoch: 5| Step: 8
Training loss: 2.268878936767578
Validation loss: 2.062905981976499

Epoch: 5| Step: 9
Training loss: 2.121161699295044
Validation loss: 2.0920412719890638

Epoch: 5| Step: 10
Training loss: 2.430194616317749
Validation loss: 2.100063168874351

Epoch: 94| Step: 0
Training loss: 2.107034921646118
Validation loss: 2.1077946770575737

Epoch: 5| Step: 1
Training loss: 1.8336067199707031
Validation loss: 2.0906116180522467

Epoch: 5| Step: 2
Training loss: 2.1246466636657715
Validation loss: 2.0649823296454644

Epoch: 5| Step: 3
Training loss: 2.5700693130493164
Validation loss: 2.045462267373198

Epoch: 5| Step: 4
Training loss: 2.7818477153778076
Validation loss: 2.024384396050566

Epoch: 5| Step: 5
Training loss: 2.164827823638916
Validation loss: 2.027318036684426

Epoch: 5| Step: 6
Training loss: 2.3875129222869873
Validation loss: 2.0268958717264156

Epoch: 5| Step: 7
Training loss: 2.794588565826416
Validation loss: 2.0299614052618704

Epoch: 5| Step: 8
Training loss: 2.468418836593628
Validation loss: 2.0326308165827105

Epoch: 5| Step: 9
Training loss: 2.401181697845459
Validation loss: 2.03611369158632

Epoch: 5| Step: 10
Training loss: 2.17415189743042
Validation loss: 2.046169960370628

Epoch: 95| Step: 0
Training loss: 1.75174081325531
Validation loss: 2.0477167098752913

Epoch: 5| Step: 1
Training loss: 2.622304916381836
Validation loss: 2.0516812237360145

Epoch: 5| Step: 2
Training loss: 2.4341845512390137
Validation loss: 2.041086366099696

Epoch: 5| Step: 3
Training loss: 2.0660529136657715
Validation loss: 2.0388166519903366

Epoch: 5| Step: 4
Training loss: 2.639124631881714
Validation loss: 2.049023025779314

Epoch: 5| Step: 5
Training loss: 2.475729465484619
Validation loss: 2.052637574493244

Epoch: 5| Step: 6
Training loss: 1.7372719049453735
Validation loss: 2.045874144441338

Epoch: 5| Step: 7
Training loss: 2.0972495079040527
Validation loss: 2.0603485184331096

Epoch: 5| Step: 8
Training loss: 2.5096397399902344
Validation loss: 2.0578774021517847

Epoch: 5| Step: 9
Training loss: 2.667691707611084
Validation loss: 2.049030057845577

Epoch: 5| Step: 10
Training loss: 2.658893346786499
Validation loss: 2.033233073449904

Epoch: 96| Step: 0
Training loss: 2.118518352508545
Validation loss: 2.0345622826648015

Epoch: 5| Step: 1
Training loss: 1.7496353387832642
Validation loss: 2.0361332995917207

Epoch: 5| Step: 2
Training loss: 2.806520938873291
Validation loss: 2.037122946913524

Epoch: 5| Step: 3
Training loss: 1.4549884796142578
Validation loss: 2.048230045585222

Epoch: 5| Step: 4
Training loss: 2.069918632507324
Validation loss: 2.076059695213072

Epoch: 5| Step: 5
Training loss: 2.6074936389923096
Validation loss: 2.0777163902918496

Epoch: 5| Step: 6
Training loss: 2.7542166709899902
Validation loss: 2.0608367612285

Epoch: 5| Step: 7
Training loss: 2.8370742797851562
Validation loss: 2.0549217757358345

Epoch: 5| Step: 8
Training loss: 2.9086461067199707
Validation loss: 2.0349947457672446

Epoch: 5| Step: 9
Training loss: 2.0897889137268066
Validation loss: 2.0279621847214235

Epoch: 5| Step: 10
Training loss: 2.2759342193603516
Validation loss: 2.034153930602535

Epoch: 97| Step: 0
Training loss: 2.046299457550049
Validation loss: 2.0267957871960056

Epoch: 5| Step: 1
Training loss: 1.724700927734375
Validation loss: 2.0217847080640894

Epoch: 5| Step: 2
Training loss: 2.3070030212402344
Validation loss: 2.018503248050649

Epoch: 5| Step: 3
Training loss: 2.5391507148742676
Validation loss: 2.021404121511726

Epoch: 5| Step: 4
Training loss: 2.5858511924743652
Validation loss: 2.0183523034536712

Epoch: 5| Step: 5
Training loss: 2.8389573097229004
Validation loss: 2.0270968021885043

Epoch: 5| Step: 6
Training loss: 1.912904977798462
Validation loss: 2.0274451676235405

Epoch: 5| Step: 7
Training loss: 2.280796527862549
Validation loss: 2.026935887593095

Epoch: 5| Step: 8
Training loss: 2.9562244415283203
Validation loss: 2.040071274644585

Epoch: 5| Step: 9
Training loss: 2.5270330905914307
Validation loss: 2.062507451221507

Epoch: 5| Step: 10
Training loss: 1.8167080879211426
Validation loss: 2.0816741784413657

Epoch: 98| Step: 0
Training loss: 1.710885763168335
Validation loss: 2.0771622324502594

Epoch: 5| Step: 1
Training loss: 2.076181650161743
Validation loss: 2.0436660294891684

Epoch: 5| Step: 2
Training loss: 1.925316572189331
Validation loss: 2.0308162486681374

Epoch: 5| Step: 3
Training loss: 3.3137810230255127
Validation loss: 2.0266749910129014

Epoch: 5| Step: 4
Training loss: 2.419328212738037
Validation loss: 2.011997740755799

Epoch: 5| Step: 5
Training loss: 2.3829874992370605
Validation loss: 2.0242069305912143

Epoch: 5| Step: 6
Training loss: 2.462817668914795
Validation loss: 2.019476766227394

Epoch: 5| Step: 7
Training loss: 2.4715969562530518
Validation loss: 2.0247052818216305

Epoch: 5| Step: 8
Training loss: 2.0406272411346436
Validation loss: 2.02240482453377

Epoch: 5| Step: 9
Training loss: 2.431865692138672
Validation loss: 2.0291155833069996

Epoch: 5| Step: 10
Training loss: 2.0367767810821533
Validation loss: 2.0419225180020897

Epoch: 99| Step: 0
Training loss: 2.4874510765075684
Validation loss: 2.0622561195845246

Epoch: 5| Step: 1
Training loss: 2.3230645656585693
Validation loss: 2.074674126922443

Epoch: 5| Step: 2
Training loss: 1.7977495193481445
Validation loss: 2.071649295027538

Epoch: 5| Step: 3
Training loss: 2.4108710289001465
Validation loss: 2.063597712465512

Epoch: 5| Step: 4
Training loss: 2.4869441986083984
Validation loss: 2.045850548692929

Epoch: 5| Step: 5
Training loss: 2.129391670227051
Validation loss: 2.0276217306813886

Epoch: 5| Step: 6
Training loss: 2.1206002235412598
Validation loss: 2.0346949459404073

Epoch: 5| Step: 7
Training loss: 3.1961758136749268
Validation loss: 2.0321830318820093

Epoch: 5| Step: 8
Training loss: 2.130246639251709
Validation loss: 2.044321802354628

Epoch: 5| Step: 9
Training loss: 2.4872078895568848
Validation loss: 2.0599107691036758

Epoch: 5| Step: 10
Training loss: 2.4657561779022217
Validation loss: 2.0551399082265873

Epoch: 100| Step: 0
Training loss: 2.0571036338806152
Validation loss: 2.04640535641742

Epoch: 5| Step: 1
Training loss: 2.6160614490509033
Validation loss: 2.078985483415665

Epoch: 5| Step: 2
Training loss: 2.0849859714508057
Validation loss: 2.1116993529822237

Epoch: 5| Step: 3
Training loss: 2.149078845977783
Validation loss: 2.1719745461658766

Epoch: 5| Step: 4
Training loss: 2.280104637145996
Validation loss: 2.159657355277769

Epoch: 5| Step: 5
Training loss: 2.3315138816833496
Validation loss: 2.1332226594289145

Epoch: 5| Step: 6
Training loss: 2.075324296951294
Validation loss: 2.0888051012510895

Epoch: 5| Step: 7
Training loss: 2.6611030101776123
Validation loss: 2.0517350025074457

Epoch: 5| Step: 8
Training loss: 2.26530122756958
Validation loss: 2.0352948673309816

Epoch: 5| Step: 9
Training loss: 2.4366610050201416
Validation loss: 2.029193848691961

Epoch: 5| Step: 10
Training loss: 3.2800166606903076
Validation loss: 2.0379991377553632

Epoch: 101| Step: 0
Training loss: 2.494903087615967
Validation loss: 2.0488400151652675

Epoch: 5| Step: 1
Training loss: 2.1048715114593506
Validation loss: 2.033727772774235

Epoch: 5| Step: 2
Training loss: 1.292001485824585
Validation loss: 2.0419890585766045

Epoch: 5| Step: 3
Training loss: 2.670496940612793
Validation loss: 2.036940929710224

Epoch: 5| Step: 4
Training loss: 2.7229626178741455
Validation loss: 2.0355010378745293

Epoch: 5| Step: 5
Training loss: 2.282750368118286
Validation loss: 2.0638715913218837

Epoch: 5| Step: 6
Training loss: 2.789031744003296
Validation loss: 2.0804323688630135

Epoch: 5| Step: 7
Training loss: 2.890493869781494
Validation loss: 2.093237397491291

Epoch: 5| Step: 8
Training loss: 1.7151482105255127
Validation loss: 2.1023936528031544

Epoch: 5| Step: 9
Training loss: 1.9849376678466797
Validation loss: 2.103708824803752

Epoch: 5| Step: 10
Training loss: 2.788879632949829
Validation loss: 2.115363618378998

Epoch: 102| Step: 0
Training loss: 2.4362025260925293
Validation loss: 2.0972943869970178

Epoch: 5| Step: 1
Training loss: 2.093223810195923
Validation loss: 2.1051815953305972

Epoch: 5| Step: 2
Training loss: 2.1545650959014893
Validation loss: 2.0894351749009985

Epoch: 5| Step: 3
Training loss: 2.7547554969787598
Validation loss: 2.086454314570273

Epoch: 5| Step: 4
Training loss: 1.5586225986480713
Validation loss: 2.0797173797443347

Epoch: 5| Step: 5
Training loss: 2.363079071044922
Validation loss: 2.0566442653697026

Epoch: 5| Step: 6
Training loss: 2.1682400703430176
Validation loss: 2.0396224452603247

Epoch: 5| Step: 7
Training loss: 2.6542441844940186
Validation loss: 2.0287847467648086

Epoch: 5| Step: 8
Training loss: 2.557368040084839
Validation loss: 2.025561016093018

Epoch: 5| Step: 9
Training loss: 2.3424265384674072
Validation loss: 2.0141596947946856

Epoch: 5| Step: 10
Training loss: 2.1657848358154297
Validation loss: 2.0207896719696703

Epoch: 103| Step: 0
Training loss: 2.5304057598114014
Validation loss: 2.012040058771769

Epoch: 5| Step: 1
Training loss: 2.3294475078582764
Validation loss: 2.0135046141121977

Epoch: 5| Step: 2
Training loss: 2.3485217094421387
Validation loss: 2.0165576729723202

Epoch: 5| Step: 3
Training loss: 2.223384380340576
Validation loss: 2.0290043610398487

Epoch: 5| Step: 4
Training loss: 1.752720832824707
Validation loss: 2.0349695849162277

Epoch: 5| Step: 5
Training loss: 2.3370447158813477
Validation loss: 2.0544050919112338

Epoch: 5| Step: 6
Training loss: 2.0285286903381348
Validation loss: 2.045883937548566

Epoch: 5| Step: 7
Training loss: 3.0433382987976074
Validation loss: 2.0496994090336624

Epoch: 5| Step: 8
Training loss: 2.849883556365967
Validation loss: 2.0323815653401036

Epoch: 5| Step: 9
Training loss: 2.019116163253784
Validation loss: 2.008868308477504

Epoch: 5| Step: 10
Training loss: 1.8852936029434204
Validation loss: 1.9977049417393182

Epoch: 104| Step: 0
Training loss: 2.306718349456787
Validation loss: 1.9999963314302507

Epoch: 5| Step: 1
Training loss: 2.1614160537719727
Validation loss: 2.0126405710815103

Epoch: 5| Step: 2
Training loss: 1.32469642162323
Validation loss: 2.0233528319225518

Epoch: 5| Step: 3
Training loss: 2.821263313293457
Validation loss: 2.0447502854049846

Epoch: 5| Step: 4
Training loss: 1.9955930709838867
Validation loss: 2.071025440769811

Epoch: 5| Step: 5
Training loss: 2.0145435333251953
Validation loss: 2.0795985421826764

Epoch: 5| Step: 6
Training loss: 2.485701084136963
Validation loss: 2.07066261511977

Epoch: 5| Step: 7
Training loss: 2.791104555130005
Validation loss: 2.0589625822600497

Epoch: 5| Step: 8
Training loss: 2.7762837409973145
Validation loss: 2.0382930309541765

Epoch: 5| Step: 9
Training loss: 2.171389102935791
Validation loss: 2.0351342103814565

Epoch: 5| Step: 10
Training loss: 2.489919662475586
Validation loss: 2.025181396033174

Epoch: 105| Step: 0
Training loss: 2.495978832244873
Validation loss: 2.013399351027704

Epoch: 5| Step: 1
Training loss: 1.839247465133667
Validation loss: 2.016449446319252

Epoch: 5| Step: 2
Training loss: 1.5847721099853516
Validation loss: 2.019173924640943

Epoch: 5| Step: 3
Training loss: 2.6510658264160156
Validation loss: 2.0151822131167174

Epoch: 5| Step: 4
Training loss: 2.538522720336914
Validation loss: 2.016045101227299

Epoch: 5| Step: 5
Training loss: 2.29426646232605
Validation loss: 2.0197144375052503

Epoch: 5| Step: 6
Training loss: 1.8137363195419312
Validation loss: 2.015689347379951

Epoch: 5| Step: 7
Training loss: 2.3997111320495605
Validation loss: 2.035245899231203

Epoch: 5| Step: 8
Training loss: 2.7932186126708984
Validation loss: 2.0480470670166837

Epoch: 5| Step: 9
Training loss: 2.324040412902832
Validation loss: 2.054677322346677

Epoch: 5| Step: 10
Training loss: 2.4328560829162598
Validation loss: 2.036389157336245

Epoch: 106| Step: 0
Training loss: 2.3033008575439453
Validation loss: 2.0153963514553603

Epoch: 5| Step: 1
Training loss: 2.625256061553955
Validation loss: 2.0111552925520044

Epoch: 5| Step: 2
Training loss: 1.9213552474975586
Validation loss: 1.9973349725046465

Epoch: 5| Step: 3
Training loss: 2.3783230781555176
Validation loss: 2.000940904822401

Epoch: 5| Step: 4
Training loss: 1.751980185508728
Validation loss: 2.001559313907418

Epoch: 5| Step: 5
Training loss: 2.0123536586761475
Validation loss: 1.9982614453120897

Epoch: 5| Step: 6
Training loss: 2.258175849914551
Validation loss: 2.0091346386940248

Epoch: 5| Step: 7
Training loss: 3.0020813941955566
Validation loss: 2.0224571663846254

Epoch: 5| Step: 8
Training loss: 2.0491137504577637
Validation loss: 2.0258188042589413

Epoch: 5| Step: 9
Training loss: 2.5950660705566406
Validation loss: 2.009450521520389

Epoch: 5| Step: 10
Training loss: 2.0983047485351562
Validation loss: 2.006475874172744

Epoch: 107| Step: 0
Training loss: 2.424954652786255
Validation loss: 1.9940173138854325

Epoch: 5| Step: 1
Training loss: 1.308556079864502
Validation loss: 1.9936010337645007

Epoch: 5| Step: 2
Training loss: 2.5887181758880615
Validation loss: 1.9878994995547878

Epoch: 5| Step: 3
Training loss: 2.7085351943969727
Validation loss: 1.9845644338156587

Epoch: 5| Step: 4
Training loss: 3.123918294906616
Validation loss: 1.9815723831935594

Epoch: 5| Step: 5
Training loss: 2.1504170894622803
Validation loss: 1.9870553619118148

Epoch: 5| Step: 6
Training loss: 1.4776190519332886
Validation loss: 1.9858300109063425

Epoch: 5| Step: 7
Training loss: 2.3185322284698486
Validation loss: 2.0040711972021286

Epoch: 5| Step: 8
Training loss: 2.6314785480499268
Validation loss: 2.036560150884813

Epoch: 5| Step: 9
Training loss: 2.1562325954437256
Validation loss: 2.0315386761901197

Epoch: 5| Step: 10
Training loss: 2.316805601119995
Validation loss: 2.05904826682101

Epoch: 108| Step: 0
Training loss: 2.1175780296325684
Validation loss: 2.0530890828819683

Epoch: 5| Step: 1
Training loss: 2.56829833984375
Validation loss: 2.0478523303103704

Epoch: 5| Step: 2
Training loss: 2.5004451274871826
Validation loss: 2.0271886676870365

Epoch: 5| Step: 3
Training loss: 2.3643887042999268
Validation loss: 2.006592312166768

Epoch: 5| Step: 4
Training loss: 1.775028944015503
Validation loss: 1.9892440278043029

Epoch: 5| Step: 5
Training loss: 2.634718418121338
Validation loss: 1.994563882068921

Epoch: 5| Step: 6
Training loss: 2.0486507415771484
Validation loss: 1.9953214583858367

Epoch: 5| Step: 7
Training loss: 1.6194705963134766
Validation loss: 1.9970905293700516

Epoch: 5| Step: 8
Training loss: 2.6794815063476562
Validation loss: 2.00646113323909

Epoch: 5| Step: 9
Training loss: 2.450672149658203
Validation loss: 2.031218424920113

Epoch: 5| Step: 10
Training loss: 2.404777765274048
Validation loss: 2.047673035693425

Epoch: 109| Step: 0
Training loss: 2.1076951026916504
Validation loss: 2.062367231615128

Epoch: 5| Step: 1
Training loss: 1.8659080266952515
Validation loss: 2.068604635935958

Epoch: 5| Step: 2
Training loss: 2.9216084480285645
Validation loss: 2.0640244240401895

Epoch: 5| Step: 3
Training loss: 2.0987067222595215
Validation loss: 2.0286126700780724

Epoch: 5| Step: 4
Training loss: 2.2251217365264893
Validation loss: 2.030908873004298

Epoch: 5| Step: 5
Training loss: 2.221118450164795
Validation loss: 2.0253289489335913

Epoch: 5| Step: 6
Training loss: 2.98909330368042
Validation loss: 2.0073943881578344

Epoch: 5| Step: 7
Training loss: 1.9987726211547852
Validation loss: 2.012863620635002

Epoch: 5| Step: 8
Training loss: 2.1140522956848145
Validation loss: 2.0210926994200675

Epoch: 5| Step: 9
Training loss: 2.539429187774658
Validation loss: 2.0319610000938497

Epoch: 5| Step: 10
Training loss: 1.8508342504501343
Validation loss: 2.0230085542125087

Epoch: 110| Step: 0
Training loss: 2.133577823638916
Validation loss: 2.0184391301165343

Epoch: 5| Step: 1
Training loss: 3.0913920402526855
Validation loss: 2.0379664103190103

Epoch: 5| Step: 2
Training loss: 2.0867910385131836
Validation loss: 2.04971686614457

Epoch: 5| Step: 3
Training loss: 2.275812864303589
Validation loss: 2.0633079249371766

Epoch: 5| Step: 4
Training loss: 2.176990032196045
Validation loss: 2.0823047250829716

Epoch: 5| Step: 5
Training loss: 2.419457197189331
Validation loss: 2.105676808664876

Epoch: 5| Step: 6
Training loss: 2.1019837856292725
Validation loss: 2.1141370675897084

Epoch: 5| Step: 7
Training loss: 2.556565761566162
Validation loss: 2.1210944473102527

Epoch: 5| Step: 8
Training loss: 1.9334986209869385
Validation loss: 2.123435375511005

Epoch: 5| Step: 9
Training loss: 2.080057144165039
Validation loss: 2.087566283441359

Epoch: 5| Step: 10
Training loss: 2.177504301071167
Validation loss: 2.0739048873224566

Epoch: 111| Step: 0
Training loss: 1.94264817237854
Validation loss: 2.060833684859737

Epoch: 5| Step: 1
Training loss: 2.357609272003174
Validation loss: 2.064017847020139

Epoch: 5| Step: 2
Training loss: 2.253183364868164
Validation loss: 2.060682794099213

Epoch: 5| Step: 3
Training loss: 2.3536696434020996
Validation loss: 2.034895461092713

Epoch: 5| Step: 4
Training loss: 2.27718448638916
Validation loss: 2.021764729612617

Epoch: 5| Step: 5
Training loss: 2.2393436431884766
Validation loss: 2.018730878829956

Epoch: 5| Step: 6
Training loss: 2.251901865005493
Validation loss: 2.010235937692786

Epoch: 5| Step: 7
Training loss: 2.003180980682373
Validation loss: 2.044322847038187

Epoch: 5| Step: 8
Training loss: 2.6046948432922363
Validation loss: 2.0905452876962642

Epoch: 5| Step: 9
Training loss: 2.239384412765503
Validation loss: 2.116657369880266

Epoch: 5| Step: 10
Training loss: 2.9449870586395264
Validation loss: 2.118355369055143

Epoch: 112| Step: 0
Training loss: 2.7302803993225098
Validation loss: 2.089145771918758

Epoch: 5| Step: 1
Training loss: 2.4137585163116455
Validation loss: 2.0743397128197456

Epoch: 5| Step: 2
Training loss: 2.4586567878723145
Validation loss: 2.0530449395538657

Epoch: 5| Step: 3
Training loss: 2.0127792358398438
Validation loss: 2.014309279380306

Epoch: 5| Step: 4
Training loss: 2.1861250400543213
Validation loss: 1.9900131610132032

Epoch: 5| Step: 5
Training loss: 2.2226338386535645
Validation loss: 1.980871315925352

Epoch: 5| Step: 6
Training loss: 2.4159464836120605
Validation loss: 1.9749226749584239

Epoch: 5| Step: 7
Training loss: 1.7215076684951782
Validation loss: 1.977095260415026

Epoch: 5| Step: 8
Training loss: 2.4144368171691895
Validation loss: 1.9851183532386698

Epoch: 5| Step: 9
Training loss: 2.4617531299591064
Validation loss: 1.9824521246776785

Epoch: 5| Step: 10
Training loss: 1.769252896308899
Validation loss: 1.9870281911665393

Epoch: 113| Step: 0
Training loss: 2.7078697681427
Validation loss: 1.9762335951610277

Epoch: 5| Step: 1
Training loss: 1.7346165180206299
Validation loss: 1.9862937376063357

Epoch: 5| Step: 2
Training loss: 2.686643362045288
Validation loss: 2.00970769569438

Epoch: 5| Step: 3
Training loss: 3.0505688190460205
Validation loss: 2.042485740876967

Epoch: 5| Step: 4
Training loss: 2.392007350921631
Validation loss: 2.0233714272899013

Epoch: 5| Step: 5
Training loss: 1.9838323593139648
Validation loss: 2.0096493856881255

Epoch: 5| Step: 6
Training loss: 2.444244861602783
Validation loss: 2.006159109454001

Epoch: 5| Step: 7
Training loss: 2.0692481994628906
Validation loss: 2.0148089188401417

Epoch: 5| Step: 8
Training loss: 1.974179983139038
Validation loss: 2.0275983528424333

Epoch: 5| Step: 9
Training loss: 1.6071462631225586
Validation loss: 2.0403752980693692

Epoch: 5| Step: 10
Training loss: 2.2462291717529297
Validation loss: 2.055971690403518

Epoch: 114| Step: 0
Training loss: 2.1760382652282715
Validation loss: 2.049794545737646

Epoch: 5| Step: 1
Training loss: 1.846047043800354
Validation loss: 2.0582607074450423

Epoch: 5| Step: 2
Training loss: 2.302342414855957
Validation loss: 2.0367726228570424

Epoch: 5| Step: 3
Training loss: 2.435227155685425
Validation loss: 2.061281247805524

Epoch: 5| Step: 4
Training loss: 2.4307150840759277
Validation loss: 2.0902565089605187

Epoch: 5| Step: 5
Training loss: 2.290688991546631
Validation loss: 2.093471957791236

Epoch: 5| Step: 6
Training loss: 2.4591469764709473
Validation loss: 2.083846381915513

Epoch: 5| Step: 7
Training loss: 2.2531604766845703
Validation loss: 2.0487115959967337

Epoch: 5| Step: 8
Training loss: 2.5364999771118164
Validation loss: 2.020509191738662

Epoch: 5| Step: 9
Training loss: 2.2404398918151855
Validation loss: 1.9909476003339213

Epoch: 5| Step: 10
Training loss: 2.256796360015869
Validation loss: 2.0035371447122223

Epoch: 115| Step: 0
Training loss: 1.9029390811920166
Validation loss: 2.020164538455266

Epoch: 5| Step: 1
Training loss: 2.178554058074951
Validation loss: 2.0011618675724154

Epoch: 5| Step: 2
Training loss: 3.1747679710388184
Validation loss: 1.9889435614309003

Epoch: 5| Step: 3
Training loss: 2.305809736251831
Validation loss: 1.9761595879831622

Epoch: 5| Step: 4
Training loss: 2.201138973236084
Validation loss: 1.9728227571774555

Epoch: 5| Step: 5
Training loss: 1.9267324209213257
Validation loss: 1.9683165268231464

Epoch: 5| Step: 6
Training loss: 1.899702787399292
Validation loss: 1.9726383480974423

Epoch: 5| Step: 7
Training loss: 2.413800001144409
Validation loss: 1.994349205365745

Epoch: 5| Step: 8
Training loss: 2.3751063346862793
Validation loss: 2.007643493272925

Epoch: 5| Step: 9
Training loss: 2.0660014152526855
Validation loss: 2.010922203781784

Epoch: 5| Step: 10
Training loss: 2.374776840209961
Validation loss: 2.004958757790186

Epoch: 116| Step: 0
Training loss: 2.285060405731201
Validation loss: 1.9807328075490973

Epoch: 5| Step: 1
Training loss: 1.908413290977478
Validation loss: 1.9715012427299254

Epoch: 5| Step: 2
Training loss: 2.7119739055633545
Validation loss: 1.9662454256447413

Epoch: 5| Step: 3
Training loss: 2.0691287517547607
Validation loss: 1.966034976384973

Epoch: 5| Step: 4
Training loss: 2.0116260051727295
Validation loss: 1.9674921125494025

Epoch: 5| Step: 5
Training loss: 2.9308254718780518
Validation loss: 1.9768916150575042

Epoch: 5| Step: 6
Training loss: 2.186678409576416
Validation loss: 1.9810641042647823

Epoch: 5| Step: 7
Training loss: 2.403444290161133
Validation loss: 1.9924202503696564

Epoch: 5| Step: 8
Training loss: 2.3630073070526123
Validation loss: 2.026728101955947

Epoch: 5| Step: 9
Training loss: 2.069788694381714
Validation loss: 2.0662229490536514

Epoch: 5| Step: 10
Training loss: 1.7842406034469604
Validation loss: 2.0677300576240785

Epoch: 117| Step: 0
Training loss: 2.000871181488037
Validation loss: 2.0691484302602787

Epoch: 5| Step: 1
Training loss: 2.1088004112243652
Validation loss: 2.054581508841566

Epoch: 5| Step: 2
Training loss: 3.0068697929382324
Validation loss: 2.048481869441207

Epoch: 5| Step: 3
Training loss: 2.139334201812744
Validation loss: 2.0381489876777894

Epoch: 5| Step: 4
Training loss: 1.6950937509536743
Validation loss: 2.025501528093892

Epoch: 5| Step: 5
Training loss: 2.29284930229187
Validation loss: 2.0466235965810795

Epoch: 5| Step: 6
Training loss: 2.388465404510498
Validation loss: 2.053286892111583

Epoch: 5| Step: 7
Training loss: 2.905122756958008
Validation loss: 2.05182582716788

Epoch: 5| Step: 8
Training loss: 2.015836715698242
Validation loss: 2.048845242428523

Epoch: 5| Step: 9
Training loss: 1.5934350490570068
Validation loss: 2.016394022972353

Epoch: 5| Step: 10
Training loss: 2.741771936416626
Validation loss: 2.019558260517736

Epoch: 118| Step: 0
Training loss: 1.707463026046753
Validation loss: 2.0365281079405095

Epoch: 5| Step: 1
Training loss: 2.4300734996795654
Validation loss: 2.0316955171605593

Epoch: 5| Step: 2
Training loss: 2.1318068504333496
Validation loss: 2.0196097576490013

Epoch: 5| Step: 3
Training loss: 2.3639769554138184
Validation loss: 2.0030149977694274

Epoch: 5| Step: 4
Training loss: 2.2789101600646973
Validation loss: 1.9973921096453102

Epoch: 5| Step: 5
Training loss: 2.611088752746582
Validation loss: 1.9770813706100627

Epoch: 5| Step: 6
Training loss: 2.3380205631256104
Validation loss: 1.9799146844494728

Epoch: 5| Step: 7
Training loss: 2.5312514305114746
Validation loss: 1.982961731572305

Epoch: 5| Step: 8
Training loss: 2.19415545463562
Validation loss: 1.981548814363377

Epoch: 5| Step: 9
Training loss: 2.1056439876556396
Validation loss: 1.986150821050008

Epoch: 5| Step: 10
Training loss: 1.7029844522476196
Validation loss: 1.9818492192094044

Epoch: 119| Step: 0
Training loss: 2.2144196033477783
Validation loss: 1.9822700587652062

Epoch: 5| Step: 1
Training loss: 1.9576542377471924
Validation loss: 1.9807187511074928

Epoch: 5| Step: 2
Training loss: 2.2621233463287354
Validation loss: 1.9833642180247972

Epoch: 5| Step: 3
Training loss: 2.662666082382202
Validation loss: 1.9780320454669256

Epoch: 5| Step: 4
Training loss: 2.249133586883545
Validation loss: 1.9743703847290368

Epoch: 5| Step: 5
Training loss: 2.014831066131592
Validation loss: 1.9627195929968229

Epoch: 5| Step: 6
Training loss: 2.2826147079467773
Validation loss: 1.96236841781165

Epoch: 5| Step: 7
Training loss: 2.60420298576355
Validation loss: 1.9570543945476573

Epoch: 5| Step: 8
Training loss: 2.27437162399292
Validation loss: 1.9683368680297688

Epoch: 5| Step: 9
Training loss: 2.033478260040283
Validation loss: 1.9886861424292288

Epoch: 5| Step: 10
Training loss: 1.9116106033325195
Validation loss: 1.986488062848327

Epoch: 120| Step: 0
Training loss: 2.2365810871124268
Validation loss: 1.987682697593525

Epoch: 5| Step: 1
Training loss: 1.9817333221435547
Validation loss: 2.0098438955122426

Epoch: 5| Step: 2
Training loss: 2.281324863433838
Validation loss: 2.022205780911189

Epoch: 5| Step: 3
Training loss: 1.7239418029785156
Validation loss: 2.0424087073213313

Epoch: 5| Step: 4
Training loss: 1.7853819131851196
Validation loss: 2.0633933300613077

Epoch: 5| Step: 5
Training loss: 2.4044718742370605
Validation loss: 2.0661945855745705

Epoch: 5| Step: 6
Training loss: 2.10980486869812
Validation loss: 2.0560050420863654

Epoch: 5| Step: 7
Training loss: 2.6329376697540283
Validation loss: 2.05810776833565

Epoch: 5| Step: 8
Training loss: 2.5907368659973145
Validation loss: 2.056127767409048

Epoch: 5| Step: 9
Training loss: 1.8982330560684204
Validation loss: 2.060540456925669

Epoch: 5| Step: 10
Training loss: 2.919564723968506
Validation loss: 2.0982888565268567

Epoch: 121| Step: 0
Training loss: 2.5827980041503906
Validation loss: 2.179031788661916

Epoch: 5| Step: 1
Training loss: 2.4799299240112305
Validation loss: 2.2108166461349814

Epoch: 5| Step: 2
Training loss: 2.173246145248413
Validation loss: 2.1909766594568887

Epoch: 5| Step: 3
Training loss: 2.6628258228302
Validation loss: 2.146340675251458

Epoch: 5| Step: 4
Training loss: 1.760932207107544
Validation loss: 2.0527031985662316

Epoch: 5| Step: 5
Training loss: 1.9447753429412842
Validation loss: 1.9936497596002394

Epoch: 5| Step: 6
Training loss: 2.107170820236206
Validation loss: 1.9609024165779032

Epoch: 5| Step: 7
Training loss: 2.08454966545105
Validation loss: 1.9831316317281416

Epoch: 5| Step: 8
Training loss: 2.6606948375701904
Validation loss: 2.0056483848120576

Epoch: 5| Step: 9
Training loss: 2.3613743782043457
Validation loss: 2.015953525420158

Epoch: 5| Step: 10
Training loss: 2.575007677078247
Validation loss: 2.018279278150169

Epoch: 122| Step: 0
Training loss: 2.959373950958252
Validation loss: 1.9832949510184668

Epoch: 5| Step: 1
Training loss: 2.3727760314941406
Validation loss: 1.9640114486858409

Epoch: 5| Step: 2
Training loss: 1.947256088256836
Validation loss: 1.956096476124179

Epoch: 5| Step: 3
Training loss: 2.16459321975708
Validation loss: 1.9492623036907566

Epoch: 5| Step: 4
Training loss: 1.946584701538086
Validation loss: 1.9527771780567784

Epoch: 5| Step: 5
Training loss: 2.3491263389587402
Validation loss: 1.957857172976258

Epoch: 5| Step: 6
Training loss: 1.7677109241485596
Validation loss: 1.9546521735447708

Epoch: 5| Step: 7
Training loss: 2.6003756523132324
Validation loss: 1.9510059561780704

Epoch: 5| Step: 8
Training loss: 2.715928554534912
Validation loss: 1.9501387329511746

Epoch: 5| Step: 9
Training loss: 2.0442214012145996
Validation loss: 1.948122016845211

Epoch: 5| Step: 10
Training loss: 1.6396546363830566
Validation loss: 1.9715643031622774

Epoch: 123| Step: 0
Training loss: 1.722499132156372
Validation loss: 1.9970963142251457

Epoch: 5| Step: 1
Training loss: 1.9518096446990967
Validation loss: 2.013700421138476

Epoch: 5| Step: 2
Training loss: 2.059934616088867
Validation loss: 2.026308149419805

Epoch: 5| Step: 3
Training loss: 1.9388256072998047
Validation loss: 2.0091087869418565

Epoch: 5| Step: 4
Training loss: 2.386047601699829
Validation loss: 2.0100680551221295

Epoch: 5| Step: 5
Training loss: 2.200117588043213
Validation loss: 2.021960558429841

Epoch: 5| Step: 6
Training loss: 2.040926456451416
Validation loss: 1.9922505065958986

Epoch: 5| Step: 7
Training loss: 3.109710216522217
Validation loss: 1.9795436679676015

Epoch: 5| Step: 8
Training loss: 2.187086582183838
Validation loss: 1.9648156781350412

Epoch: 5| Step: 9
Training loss: 2.6084721088409424
Validation loss: 1.9655126781873806

Epoch: 5| Step: 10
Training loss: 2.302884101867676
Validation loss: 1.976471657394081

Epoch: 124| Step: 0
Training loss: 2.4937686920166016
Validation loss: 1.9848830328192761

Epoch: 5| Step: 1
Training loss: 2.5218491554260254
Validation loss: 2.0001587906191425

Epoch: 5| Step: 2
Training loss: 1.8088045120239258
Validation loss: 1.99827988173372

Epoch: 5| Step: 3
Training loss: 1.9904886484146118
Validation loss: 1.9832609174072102

Epoch: 5| Step: 4
Training loss: 2.272334337234497
Validation loss: 1.983071345154957

Epoch: 5| Step: 5
Training loss: 2.4357030391693115
Validation loss: 1.9741405569097048

Epoch: 5| Step: 6
Training loss: 2.0686910152435303
Validation loss: 1.980519030683784

Epoch: 5| Step: 7
Training loss: 1.8005859851837158
Validation loss: 1.9909375944445211

Epoch: 5| Step: 8
Training loss: 2.62394380569458
Validation loss: 2.015836501634249

Epoch: 5| Step: 9
Training loss: 2.1576952934265137
Validation loss: 2.032885325852261

Epoch: 5| Step: 10
Training loss: 2.458146572113037
Validation loss: 2.040969688405273

Epoch: 125| Step: 0
Training loss: 2.3817267417907715
Validation loss: 2.025031384601388

Epoch: 5| Step: 1
Training loss: 1.689371109008789
Validation loss: 2.0184715229977845

Epoch: 5| Step: 2
Training loss: 1.8280246257781982
Validation loss: 2.005665704768191

Epoch: 5| Step: 3
Training loss: 2.832603931427002
Validation loss: 2.013062051547471

Epoch: 5| Step: 4
Training loss: 2.4053666591644287
Validation loss: 2.0084530089491155

Epoch: 5| Step: 5
Training loss: 2.682507038116455
Validation loss: 1.9919922454382784

Epoch: 5| Step: 6
Training loss: 2.904231309890747
Validation loss: 1.9799113145438574

Epoch: 5| Step: 7
Training loss: 1.9657551050186157
Validation loss: 1.9761249352526922

Epoch: 5| Step: 8
Training loss: 2.187673330307007
Validation loss: 1.967922487566548

Epoch: 5| Step: 9
Training loss: 1.279979944229126
Validation loss: 1.9715045190626574

Epoch: 5| Step: 10
Training loss: 1.9119141101837158
Validation loss: 1.9911057461974442

Testing loss: 2.2841800583733454

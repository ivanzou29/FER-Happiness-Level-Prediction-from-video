Epoch: 1| Step: 0
Training loss: 5.905204236328656
Validation loss: 5.824290641481329

Epoch: 5| Step: 1
Training loss: 5.326187631760704
Validation loss: 5.79870629611862

Epoch: 5| Step: 2
Training loss: 5.053046170801139
Validation loss: 5.774632534678356

Epoch: 5| Step: 3
Training loss: 6.012558192488985
Validation loss: 5.7503969002912045

Epoch: 5| Step: 4
Training loss: 6.137378873231939
Validation loss: 5.722689020604177

Epoch: 5| Step: 5
Training loss: 4.802460739078522
Validation loss: 5.69128923873474

Epoch: 5| Step: 6
Training loss: 6.314875391850547
Validation loss: 5.655724668245245

Epoch: 5| Step: 7
Training loss: 5.309283550658136
Validation loss: 5.615601230930566

Epoch: 5| Step: 8
Training loss: 6.1712142397482115
Validation loss: 5.570291977260242

Epoch: 5| Step: 9
Training loss: 5.854125904018264
Validation loss: 5.519817302296698

Epoch: 5| Step: 10
Training loss: 5.651066549454386
Validation loss: 5.464358595915109

Epoch: 2| Step: 0
Training loss: 5.100360909949723
Validation loss: 5.403134469423657

Epoch: 5| Step: 1
Training loss: 5.344081578368707
Validation loss: 5.336117384395757

Epoch: 5| Step: 2
Training loss: 4.693685494092681
Validation loss: 5.26553757806361

Epoch: 5| Step: 3
Training loss: 5.552571383193025
Validation loss: 5.189503646147027

Epoch: 5| Step: 4
Training loss: 4.633121679580921
Validation loss: 5.112234827967179

Epoch: 5| Step: 5
Training loss: 5.338926759802311
Validation loss: 5.032211102870498

Epoch: 5| Step: 6
Training loss: 5.578766088750992
Validation loss: 4.953930062009245

Epoch: 5| Step: 7
Training loss: 5.3126829228043375
Validation loss: 4.875692042349066

Epoch: 5| Step: 8
Training loss: 5.3138632427703865
Validation loss: 4.802995195814564

Epoch: 5| Step: 9
Training loss: 4.917386190923344
Validation loss: 4.73335153520462

Epoch: 5| Step: 10
Training loss: 3.8950229284666924
Validation loss: 4.666133104459691

Epoch: 3| Step: 0
Training loss: 4.342745657974171
Validation loss: 4.602969940850697

Epoch: 5| Step: 1
Training loss: 4.246656336709418
Validation loss: 4.545508757889758

Epoch: 5| Step: 2
Training loss: 4.085633129796294
Validation loss: 4.492137573592503

Epoch: 5| Step: 3
Training loss: 4.93739666408133
Validation loss: 4.442793343492657

Epoch: 5| Step: 4
Training loss: 5.198858304501873
Validation loss: 4.396461456325538

Epoch: 5| Step: 5
Training loss: 4.2112205849013975
Validation loss: 4.348574154072251

Epoch: 5| Step: 6
Training loss: 4.60875136472404
Validation loss: 4.301930445567191

Epoch: 5| Step: 7
Training loss: 4.6681505750414445
Validation loss: 4.257945813096516

Epoch: 5| Step: 8
Training loss: 4.696612439967224
Validation loss: 4.222314093692541

Epoch: 5| Step: 9
Training loss: 3.7161893603692455
Validation loss: 4.193756904655663

Epoch: 5| Step: 10
Training loss: 4.336712522232513
Validation loss: 4.169804942106393

Epoch: 4| Step: 0
Training loss: 3.4200465224821697
Validation loss: 4.140942297596264

Epoch: 5| Step: 1
Training loss: 5.2295843719991195
Validation loss: 4.109062690459938

Epoch: 5| Step: 2
Training loss: 5.329722413927529
Validation loss: 4.086661756306759

Epoch: 5| Step: 3
Training loss: 4.074637489342538
Validation loss: 4.056788139823538

Epoch: 5| Step: 4
Training loss: 3.0958086350809064
Validation loss: 4.030497730142678

Epoch: 5| Step: 5
Training loss: 4.151155030710038
Validation loss: 4.007498670113903

Epoch: 5| Step: 6
Training loss: 3.119344400092616
Validation loss: 3.9846744972504844

Epoch: 5| Step: 7
Training loss: 4.538377930170706
Validation loss: 3.964536903833753

Epoch: 5| Step: 8
Training loss: 4.4430011445567095
Validation loss: 3.949970369758757

Epoch: 5| Step: 9
Training loss: 4.397519383390472
Validation loss: 3.936031126821837

Epoch: 5| Step: 10
Training loss: 3.2086706768554802
Validation loss: 3.9222075324593115

Epoch: 5| Step: 0
Training loss: 4.29293720487159
Validation loss: 3.909267162392048

Epoch: 5| Step: 1
Training loss: 4.432015051392705
Validation loss: 3.8910871655337513

Epoch: 5| Step: 2
Training loss: 4.01289696556919
Validation loss: 3.871618652650621

Epoch: 5| Step: 3
Training loss: 2.6896126893025007
Validation loss: 3.8597875730531364

Epoch: 5| Step: 4
Training loss: 3.613893925041608
Validation loss: 3.8493695222691278

Epoch: 5| Step: 5
Training loss: 4.095055757732188
Validation loss: 3.8415664645547447

Epoch: 5| Step: 6
Training loss: 4.443728092531265
Validation loss: 3.821317521281586

Epoch: 5| Step: 7
Training loss: 4.0948531433526405
Validation loss: 3.806425976106377

Epoch: 5| Step: 8
Training loss: 3.5928866468895815
Validation loss: 3.796834208397082

Epoch: 5| Step: 9
Training loss: 4.446993706004117
Validation loss: 3.7855036150396106

Epoch: 5| Step: 10
Training loss: 4.0349017023264055
Validation loss: 3.770636592435621

Epoch: 6| Step: 0
Training loss: 3.671263688516214
Validation loss: 3.757322797849827

Epoch: 5| Step: 1
Training loss: 4.1116677042064635
Validation loss: 3.740982516926403

Epoch: 5| Step: 2
Training loss: 3.883967329561624
Validation loss: 3.73023618172786

Epoch: 5| Step: 3
Training loss: 3.79789324634813
Validation loss: 3.719633377172613

Epoch: 5| Step: 4
Training loss: 4.23496819314193
Validation loss: 3.71047202713727

Epoch: 5| Step: 5
Training loss: 3.948985348529157
Validation loss: 3.7004600673998627

Epoch: 5| Step: 6
Training loss: 4.0974807628950405
Validation loss: 3.6883089858161204

Epoch: 5| Step: 7
Training loss: 3.207718398046654
Validation loss: 3.6773737445554593

Epoch: 5| Step: 8
Training loss: 3.9012667188252554
Validation loss: 3.6696801811369575

Epoch: 5| Step: 9
Training loss: 3.929031954438526
Validation loss: 3.6580584155327696

Epoch: 5| Step: 10
Training loss: 3.8684009844446887
Validation loss: 3.6466391385169405

Epoch: 7| Step: 0
Training loss: 3.536330897301603
Validation loss: 3.635771770054568

Epoch: 5| Step: 1
Training loss: 3.272098094253216
Validation loss: 3.6287108802462145

Epoch: 5| Step: 2
Training loss: 3.787767578342032
Validation loss: 3.620275066632804

Epoch: 5| Step: 3
Training loss: 4.050273160788288
Validation loss: 3.609567322526006

Epoch: 5| Step: 4
Training loss: 3.646480321243395
Validation loss: 3.6019392958240988

Epoch: 5| Step: 5
Training loss: 3.7495612841515737
Validation loss: 3.595776838562896

Epoch: 5| Step: 6
Training loss: 3.893306068843152
Validation loss: 3.5830152179131463

Epoch: 5| Step: 7
Training loss: 4.312169822547717
Validation loss: 3.5704367639278494

Epoch: 5| Step: 8
Training loss: 4.213310295074186
Validation loss: 3.5593265459600714

Epoch: 5| Step: 9
Training loss: 3.3534301675828857
Validation loss: 3.5500569922057847

Epoch: 5| Step: 10
Training loss: 3.7370364868163097
Validation loss: 3.5407663454460545

Epoch: 8| Step: 0
Training loss: 3.7164015847915195
Validation loss: 3.531629958108312

Epoch: 5| Step: 1
Training loss: 3.7307012515488434
Validation loss: 3.5243483810426723

Epoch: 5| Step: 2
Training loss: 3.928345346757476
Validation loss: 3.5144532082747517

Epoch: 5| Step: 3
Training loss: 3.3761864448788215
Validation loss: 3.503295227647049

Epoch: 5| Step: 4
Training loss: 3.761429978613026
Validation loss: 3.495393282069648

Epoch: 5| Step: 5
Training loss: 4.155293060507426
Validation loss: 3.491141771898803

Epoch: 5| Step: 6
Training loss: 3.080328757587699
Validation loss: 3.4817014885450717

Epoch: 5| Step: 7
Training loss: 3.8665123552465115
Validation loss: 3.469002404564241

Epoch: 5| Step: 8
Training loss: 3.4186698499695494
Validation loss: 3.4600204748794665

Epoch: 5| Step: 9
Training loss: 3.3438059008907843
Validation loss: 3.4582672102935375

Epoch: 5| Step: 10
Training loss: 4.271182509623954
Validation loss: 3.4476350816104926

Epoch: 9| Step: 0
Training loss: 3.4763195853074116
Validation loss: 3.438662518677346

Epoch: 5| Step: 1
Training loss: 3.2980581754847558
Validation loss: 3.449930697603005

Epoch: 5| Step: 2
Training loss: 3.817489610818992
Validation loss: 3.4333668581023433

Epoch: 5| Step: 3
Training loss: 3.6214936497511614
Validation loss: 3.436248215771009

Epoch: 5| Step: 4
Training loss: 3.789676199696666
Validation loss: 3.431687584855458

Epoch: 5| Step: 5
Training loss: 3.170844601290746
Validation loss: 3.424942572291655

Epoch: 5| Step: 6
Training loss: 4.206133024904628
Validation loss: 3.4220626044670426

Epoch: 5| Step: 7
Training loss: 4.165155721732297
Validation loss: 3.4172431620511086

Epoch: 5| Step: 8
Training loss: 3.1367050683660214
Validation loss: 3.4111575670004846

Epoch: 5| Step: 9
Training loss: 3.834973744447994
Validation loss: 3.4009412716042644

Epoch: 5| Step: 10
Training loss: 3.4212609702545222
Validation loss: 3.3888319641013327

Epoch: 10| Step: 0
Training loss: 3.76298753488277
Validation loss: 3.37883692967906

Epoch: 5| Step: 1
Training loss: 3.9961077110963967
Validation loss: 3.3700278195408404

Epoch: 5| Step: 2
Training loss: 3.5483402133036996
Validation loss: 3.362578872565012

Epoch: 5| Step: 3
Training loss: 2.649865679665627
Validation loss: 3.3645412059297692

Epoch: 5| Step: 4
Training loss: 4.2477051766310385
Validation loss: 3.3479346808331063

Epoch: 5| Step: 5
Training loss: 3.8792878085245475
Validation loss: 3.3307215130821373

Epoch: 5| Step: 6
Training loss: 2.760773536118222
Validation loss: 3.3238709973051703

Epoch: 5| Step: 7
Training loss: 3.3892858998810964
Validation loss: 3.3210794044540446

Epoch: 5| Step: 8
Training loss: 4.035892622488104
Validation loss: 3.3139208104898006

Epoch: 5| Step: 9
Training loss: 2.952801722033424
Validation loss: 3.3036603333712873

Epoch: 5| Step: 10
Training loss: 3.7456788916285784
Validation loss: 3.2993819003754488

Epoch: 11| Step: 0
Training loss: 3.3097358542897193
Validation loss: 3.2968653811091038

Epoch: 5| Step: 1
Training loss: 3.82067070590287
Validation loss: 3.2879199582912793

Epoch: 5| Step: 2
Training loss: 3.4716147200760106
Validation loss: 3.2823559234814548

Epoch: 5| Step: 3
Training loss: 3.866472274453949
Validation loss: 3.2755245786202822

Epoch: 5| Step: 4
Training loss: 2.620606605980166
Validation loss: 3.266348962128175

Epoch: 5| Step: 5
Training loss: 3.6187437594397833
Validation loss: 3.2619684992373066

Epoch: 5| Step: 6
Training loss: 3.208296309088233
Validation loss: 3.259552306210683

Epoch: 5| Step: 7
Training loss: 4.2447347001385785
Validation loss: 3.255073198954977

Epoch: 5| Step: 8
Training loss: 3.195632433555309
Validation loss: 3.2472336088451126

Epoch: 5| Step: 9
Training loss: 3.5041261601837874
Validation loss: 3.2454140541162064

Epoch: 5| Step: 10
Training loss: 3.564992601512706
Validation loss: 3.240407897282242

Epoch: 12| Step: 0
Training loss: 3.935771623351538
Validation loss: 3.23174403154718

Epoch: 5| Step: 1
Training loss: 3.050853616049131
Validation loss: 3.229539343333683

Epoch: 5| Step: 2
Training loss: 3.959130109937913
Validation loss: 3.224619323910299

Epoch: 5| Step: 3
Training loss: 3.222733486290361
Validation loss: 3.2186851060846515

Epoch: 5| Step: 4
Training loss: 3.3724093914373134
Validation loss: 3.2113187223439597

Epoch: 5| Step: 5
Training loss: 4.52040559822467
Validation loss: 3.2086918479672253

Epoch: 5| Step: 6
Training loss: 3.2513386830368076
Validation loss: 3.2043756340007974

Epoch: 5| Step: 7
Training loss: 2.283695634467301
Validation loss: 3.2275681290554052

Epoch: 5| Step: 8
Training loss: 3.204517936219463
Validation loss: 3.207187128752161

Epoch: 5| Step: 9
Training loss: 3.439209391651521
Validation loss: 3.1950535044841635

Epoch: 5| Step: 10
Training loss: 3.480850969385121
Validation loss: 3.195602030398187

Epoch: 13| Step: 0
Training loss: 3.526552164301771
Validation loss: 3.1911980339673858

Epoch: 5| Step: 1
Training loss: 3.2347983852570654
Validation loss: 3.1794462269408985

Epoch: 5| Step: 2
Training loss: 3.5617741966408323
Validation loss: 3.1752280679428146

Epoch: 5| Step: 3
Training loss: 3.945382115722443
Validation loss: 3.17123046546658

Epoch: 5| Step: 4
Training loss: 3.9269123933994976
Validation loss: 3.168043153703204

Epoch: 5| Step: 5
Training loss: 3.142760049261459
Validation loss: 3.1739974995516076

Epoch: 5| Step: 6
Training loss: 2.892961537194796
Validation loss: 3.16541143051905

Epoch: 5| Step: 7
Training loss: 3.305632183422723
Validation loss: 3.155890775024585

Epoch: 5| Step: 8
Training loss: 3.3915460276100426
Validation loss: 3.1533830736345143

Epoch: 5| Step: 9
Training loss: 3.7010858128186728
Validation loss: 3.149144180408997

Epoch: 5| Step: 10
Training loss: 2.8462443049880237
Validation loss: 3.145464754885832

Epoch: 14| Step: 0
Training loss: 2.648516898526378
Validation loss: 3.1464640824347265

Epoch: 5| Step: 1
Training loss: 2.8511280303648636
Validation loss: 3.1440706435823436

Epoch: 5| Step: 2
Training loss: 3.45678144511474
Validation loss: 3.143062705238081

Epoch: 5| Step: 3
Training loss: 2.7816475573371524
Validation loss: 3.1450567875474205

Epoch: 5| Step: 4
Training loss: 3.6548399203165722
Validation loss: 3.141960670182165

Epoch: 5| Step: 5
Training loss: 4.225476137678202
Validation loss: 3.139884995828036

Epoch: 5| Step: 6
Training loss: 3.1894884078345447
Validation loss: 3.1266123920941435

Epoch: 5| Step: 7
Training loss: 3.0951551848964387
Validation loss: 3.116553542991791

Epoch: 5| Step: 8
Training loss: 3.1664338444566944
Validation loss: 3.1170973333858836

Epoch: 5| Step: 9
Training loss: 4.32978739297996
Validation loss: 3.1031365748411828

Epoch: 5| Step: 10
Training loss: 3.5065793459019923
Validation loss: 3.0996988146890234

Epoch: 15| Step: 0
Training loss: 2.786765384096723
Validation loss: 3.094924571205082

Epoch: 5| Step: 1
Training loss: 2.5210979942756535
Validation loss: 3.101446178740187

Epoch: 5| Step: 2
Training loss: 3.338386297950077
Validation loss: 3.1092392385832066

Epoch: 5| Step: 3
Training loss: 3.467521931222045
Validation loss: 3.1052136788612184

Epoch: 5| Step: 4
Training loss: 3.4476948154833034
Validation loss: 3.0866492039911573

Epoch: 5| Step: 5
Training loss: 3.543278555484021
Validation loss: 3.071215316513383

Epoch: 5| Step: 6
Training loss: 4.257544832827957
Validation loss: 3.078581826954918

Epoch: 5| Step: 7
Training loss: 3.9108983573219733
Validation loss: 3.059912920895186

Epoch: 5| Step: 8
Training loss: 3.5335873320591036
Validation loss: 3.0569231437637527

Epoch: 5| Step: 9
Training loss: 2.626793838833687
Validation loss: 3.0567158313641065

Epoch: 5| Step: 10
Training loss: 3.0152665157411387
Validation loss: 3.0630717879071527

Epoch: 16| Step: 0
Training loss: 2.2511532265272836
Validation loss: 3.066261069755952

Epoch: 5| Step: 1
Training loss: 3.2271925932133962
Validation loss: 3.0726447940204435

Epoch: 5| Step: 2
Training loss: 3.8035403172981486
Validation loss: 3.0635441098303855

Epoch: 5| Step: 3
Training loss: 3.495143927583516
Validation loss: 3.056825617423974

Epoch: 5| Step: 4
Training loss: 3.29822935516505
Validation loss: 3.041773803606417

Epoch: 5| Step: 5
Training loss: 3.9200938766293487
Validation loss: 3.0762563417462854

Epoch: 5| Step: 6
Training loss: 3.2166854014534634
Validation loss: 3.06211502036184

Epoch: 5| Step: 7
Training loss: 3.262595491729149
Validation loss: 3.0373837812917377

Epoch: 5| Step: 8
Training loss: 2.72857827597296
Validation loss: 3.0294999522736306

Epoch: 5| Step: 9
Training loss: 3.503867464903444
Validation loss: 3.0299309398933607

Epoch: 5| Step: 10
Training loss: 3.712310858124442
Validation loss: 3.038855176358236

Epoch: 17| Step: 0
Training loss: 2.7195555271905025
Validation loss: 3.0398224737855353

Epoch: 5| Step: 1
Training loss: 3.6689254276893273
Validation loss: 3.044273345656259

Epoch: 5| Step: 2
Training loss: 3.522884756914147
Validation loss: 3.0415679383504237

Epoch: 5| Step: 3
Training loss: 3.699583525029134
Validation loss: 3.022706663593755

Epoch: 5| Step: 4
Training loss: 2.9389080163164585
Validation loss: 3.017023533206185

Epoch: 5| Step: 5
Training loss: 3.705505893367497
Validation loss: 3.0188529754184326

Epoch: 5| Step: 6
Training loss: 2.9426552196649354
Validation loss: 3.0179579331739212

Epoch: 5| Step: 7
Training loss: 3.3807847638990585
Validation loss: 3.0169903461599956

Epoch: 5| Step: 8
Training loss: 3.19531339538114
Validation loss: 3.020332354855868

Epoch: 5| Step: 9
Training loss: 3.3664809191486142
Validation loss: 3.0175433043667064

Epoch: 5| Step: 10
Training loss: 3.0723756647986544
Validation loss: 3.015094790167772

Epoch: 18| Step: 0
Training loss: 3.1509397043592906
Validation loss: 3.0116015429905993

Epoch: 5| Step: 1
Training loss: 2.7752456143943087
Validation loss: 3.007716850312213

Epoch: 5| Step: 2
Training loss: 2.9616284533003037
Validation loss: 3.0095666746408627

Epoch: 5| Step: 3
Training loss: 3.5280833942132714
Validation loss: 3.0054025286396877

Epoch: 5| Step: 4
Training loss: 4.032794508659258
Validation loss: 3.000649188840853

Epoch: 5| Step: 5
Training loss: 3.0953106267346677
Validation loss: 2.994373381706222

Epoch: 5| Step: 6
Training loss: 3.203303187926759
Validation loss: 2.986481191207285

Epoch: 5| Step: 7
Training loss: 3.5453686981656247
Validation loss: 2.985064185085532

Epoch: 5| Step: 8
Training loss: 3.4284039921975094
Validation loss: 2.9817128268733297

Epoch: 5| Step: 9
Training loss: 3.7391958363563296
Validation loss: 2.9822716985431335

Epoch: 5| Step: 10
Training loss: 2.1661890921859683
Validation loss: 2.989031867988457

Epoch: 19| Step: 0
Training loss: 3.473227695266889
Validation loss: 3.011557336998078

Epoch: 5| Step: 1
Training loss: 3.6019255151074177
Validation loss: 2.9912699050658933

Epoch: 5| Step: 2
Training loss: 3.1769906119068927
Validation loss: 2.979593950387203

Epoch: 5| Step: 3
Training loss: 2.4400032101281734
Validation loss: 2.9712738293424703

Epoch: 5| Step: 4
Training loss: 3.3921322614687175
Validation loss: 2.973039444637364

Epoch: 5| Step: 5
Training loss: 3.4568930388155668
Validation loss: 2.97349941303939

Epoch: 5| Step: 6
Training loss: 3.379062609279627
Validation loss: 2.974324222925931

Epoch: 5| Step: 7
Training loss: 2.8645922481513746
Validation loss: 2.9700378905892477

Epoch: 5| Step: 8
Training loss: 3.395795544997203
Validation loss: 2.9684413384607558

Epoch: 5| Step: 9
Training loss: 3.147390186675719
Validation loss: 2.9663173810277663

Epoch: 5| Step: 10
Training loss: 3.5529958199912084
Validation loss: 2.9701825496491696

Epoch: 20| Step: 0
Training loss: 3.0673332222491703
Validation loss: 2.980085566958343

Epoch: 5| Step: 1
Training loss: 3.2407470444713824
Validation loss: 3.0049829835559905

Epoch: 5| Step: 2
Training loss: 3.9909852488872892
Validation loss: 2.9882881474923035

Epoch: 5| Step: 3
Training loss: 2.7117484251270803
Validation loss: 2.9677728362768607

Epoch: 5| Step: 4
Training loss: 3.42852554971789
Validation loss: 2.9617773013319737

Epoch: 5| Step: 5
Training loss: 2.9631130308542337
Validation loss: 2.9584581180398595

Epoch: 5| Step: 6
Training loss: 3.462998441770172
Validation loss: 2.9561006418926254

Epoch: 5| Step: 7
Training loss: 2.966896522561233
Validation loss: 2.9543953497525073

Epoch: 5| Step: 8
Training loss: 3.2105589409125836
Validation loss: 2.9549120962437376

Epoch: 5| Step: 9
Training loss: 3.149467508025197
Validation loss: 2.953457059875697

Epoch: 5| Step: 10
Training loss: 3.562726331514819
Validation loss: 2.9535687120793463

Epoch: 21| Step: 0
Training loss: 3.319505588991632
Validation loss: 2.9548975793238066

Epoch: 5| Step: 1
Training loss: 3.2856851303986887
Validation loss: 2.957761656139652

Epoch: 5| Step: 2
Training loss: 3.0595592470574715
Validation loss: 2.953375795332691

Epoch: 5| Step: 3
Training loss: 3.8201849356418442
Validation loss: 2.949819435925737

Epoch: 5| Step: 4
Training loss: 3.0659750082412067
Validation loss: 2.9481957993119825

Epoch: 5| Step: 5
Training loss: 3.1827072313416718
Validation loss: 2.947828642796558

Epoch: 5| Step: 6
Training loss: 3.4557072577204817
Validation loss: 2.9483882246142974

Epoch: 5| Step: 7
Training loss: 2.7845115825064894
Validation loss: 2.94827605623359

Epoch: 5| Step: 8
Training loss: 3.471590133778151
Validation loss: 2.955335069715983

Epoch: 5| Step: 9
Training loss: 3.187473970194729
Validation loss: 2.9525012570601006

Epoch: 5| Step: 10
Training loss: 2.854010380459402
Validation loss: 2.959152128011009

Epoch: 22| Step: 0
Training loss: 3.5034840817477066
Validation loss: 2.9605334407181187

Epoch: 5| Step: 1
Training loss: 3.274830524048943
Validation loss: 2.945678937080958

Epoch: 5| Step: 2
Training loss: 3.5616863727039307
Validation loss: 2.9377441488061518

Epoch: 5| Step: 3
Training loss: 3.8031393735766024
Validation loss: 2.934101585169697

Epoch: 5| Step: 4
Training loss: 3.20623271260797
Validation loss: 2.9261361010761067

Epoch: 5| Step: 5
Training loss: 2.8448851279614766
Validation loss: 2.9222881048577993

Epoch: 5| Step: 6
Training loss: 3.1637802186485113
Validation loss: 2.9284981729907376

Epoch: 5| Step: 7
Training loss: 2.5932075208955885
Validation loss: 2.927877986906637

Epoch: 5| Step: 8
Training loss: 3.6126407529872164
Validation loss: 2.9263990157938875

Epoch: 5| Step: 9
Training loss: 3.0241994760106556
Validation loss: 2.9208674777316395

Epoch: 5| Step: 10
Training loss: 2.6143394346323654
Validation loss: 2.9267641349698814

Epoch: 23| Step: 0
Training loss: 2.994112914194175
Validation loss: 2.9415159750954873

Epoch: 5| Step: 1
Training loss: 3.6188186032870027
Validation loss: 2.9539351351774132

Epoch: 5| Step: 2
Training loss: 3.603047426816319
Validation loss: 2.924797633283763

Epoch: 5| Step: 3
Training loss: 3.214073858394034
Validation loss: 2.9143993290018897

Epoch: 5| Step: 4
Training loss: 3.2458601774608185
Validation loss: 2.911120923925525

Epoch: 5| Step: 5
Training loss: 3.047354948927581
Validation loss: 2.9107931993361036

Epoch: 5| Step: 6
Training loss: 3.012280284419406
Validation loss: 2.9125859633365243

Epoch: 5| Step: 7
Training loss: 3.0640992640459217
Validation loss: 2.9141548157731263

Epoch: 5| Step: 8
Training loss: 3.17060097349731
Validation loss: 2.9137675601165447

Epoch: 5| Step: 9
Training loss: 2.7505832400420807
Validation loss: 2.913910053280233

Epoch: 5| Step: 10
Training loss: 3.619173431659524
Validation loss: 2.9180715145566314

Epoch: 24| Step: 0
Training loss: 2.576847939211557
Validation loss: 2.916947658615622

Epoch: 5| Step: 1
Training loss: 4.012137356826651
Validation loss: 2.9093438761545767

Epoch: 5| Step: 2
Training loss: 3.303245861256723
Validation loss: 2.9063755907578086

Epoch: 5| Step: 3
Training loss: 3.425400542067481
Validation loss: 2.904804239292145

Epoch: 5| Step: 4
Training loss: 3.498869304578618
Validation loss: 2.9036277808326214

Epoch: 5| Step: 5
Training loss: 3.1115075854062333
Validation loss: 2.901857118590103

Epoch: 5| Step: 6
Training loss: 2.816005556594178
Validation loss: 2.9016922483251713

Epoch: 5| Step: 7
Training loss: 3.3665980555527217
Validation loss: 2.903239921908092

Epoch: 5| Step: 8
Training loss: 2.863739518238402
Validation loss: 2.9006555719015537

Epoch: 5| Step: 9
Training loss: 3.113145085782909
Validation loss: 2.9047719633055977

Epoch: 5| Step: 10
Training loss: 2.8837845980218573
Validation loss: 2.9074518852452274

Epoch: 25| Step: 0
Training loss: 2.4879192286563225
Validation loss: 2.9053985110479323

Epoch: 5| Step: 1
Training loss: 3.1188533605292927
Validation loss: 2.9056462580290985

Epoch: 5| Step: 2
Training loss: 3.2580962937973093
Validation loss: 2.897870145487587

Epoch: 5| Step: 3
Training loss: 3.3368624760517225
Validation loss: 2.896047940243459

Epoch: 5| Step: 4
Training loss: 3.78903796001235
Validation loss: 2.895396987561313

Epoch: 5| Step: 5
Training loss: 3.0661743853918844
Validation loss: 2.894941602693132

Epoch: 5| Step: 6
Training loss: 3.7513834626589606
Validation loss: 2.8899115101217707

Epoch: 5| Step: 7
Training loss: 3.0523979016224443
Validation loss: 2.8913938679901467

Epoch: 5| Step: 8
Training loss: 2.892961702021441
Validation loss: 2.890280772973137

Epoch: 5| Step: 9
Training loss: 3.1718686860120346
Validation loss: 2.8913545609657967

Epoch: 5| Step: 10
Training loss: 2.9952268776274553
Validation loss: 2.899355988144414

Epoch: 26| Step: 0
Training loss: 3.2028868051679176
Validation loss: 2.9635834014691502

Epoch: 5| Step: 1
Training loss: 2.907418949115987
Validation loss: 2.9814373123048012

Epoch: 5| Step: 2
Training loss: 4.079630022290552
Validation loss: 3.0784952779986567

Epoch: 5| Step: 3
Training loss: 3.294621569793255
Validation loss: 2.9003444252106365

Epoch: 5| Step: 4
Training loss: 3.348607323238875
Validation loss: 2.886138852885001

Epoch: 5| Step: 5
Training loss: 3.2480420303537985
Validation loss: 2.895170416697473

Epoch: 5| Step: 6
Training loss: 2.955993389581663
Validation loss: 2.928428176010848

Epoch: 5| Step: 7
Training loss: 3.1683519463257444
Validation loss: 2.9625368412901727

Epoch: 5| Step: 8
Training loss: 3.2009247456946093
Validation loss: 2.9309243300967256

Epoch: 5| Step: 9
Training loss: 3.0453200847595867
Validation loss: 2.8993451264838654

Epoch: 5| Step: 10
Training loss: 2.959859445430905
Validation loss: 2.893938094333146

Epoch: 27| Step: 0
Training loss: 2.9006903188494113
Validation loss: 2.8884504542704903

Epoch: 5| Step: 1
Training loss: 3.025140642411865
Validation loss: 2.892724959629279

Epoch: 5| Step: 2
Training loss: 2.948961340173832
Validation loss: 2.9079302401749896

Epoch: 5| Step: 3
Training loss: 3.178246023748883
Validation loss: 2.9324440122130744

Epoch: 5| Step: 4
Training loss: 3.535511247714373
Validation loss: 2.9409914572811218

Epoch: 5| Step: 5
Training loss: 3.8713240877328707
Validation loss: 2.930560485063803

Epoch: 5| Step: 6
Training loss: 2.4245081215893736
Validation loss: 2.9104188553785324

Epoch: 5| Step: 7
Training loss: 2.861611247458255
Validation loss: 2.888953158973374

Epoch: 5| Step: 8
Training loss: 2.8895119663511557
Validation loss: 2.8839783937495596

Epoch: 5| Step: 9
Training loss: 3.6450583515284585
Validation loss: 2.8795860811224627

Epoch: 5| Step: 10
Training loss: 3.6461213497656377
Validation loss: 2.8809732427491976

Epoch: 28| Step: 0
Training loss: 2.4340955973010874
Validation loss: 2.885105255003188

Epoch: 5| Step: 1
Training loss: 3.234651857772519
Validation loss: 2.8885604709452064

Epoch: 5| Step: 2
Training loss: 3.3509794141500806
Validation loss: 2.888608753130631

Epoch: 5| Step: 3
Training loss: 3.013055210472417
Validation loss: 2.8801479262428753

Epoch: 5| Step: 4
Training loss: 3.500817067278695
Validation loss: 2.875829708010919

Epoch: 5| Step: 5
Training loss: 2.958002331622522
Validation loss: 2.8732613669552305

Epoch: 5| Step: 6
Training loss: 3.172830973165344
Validation loss: 2.8767617397871614

Epoch: 5| Step: 7
Training loss: 3.233699414742692
Validation loss: 2.8746036240248807

Epoch: 5| Step: 8
Training loss: 3.348202032157637
Validation loss: 2.8709261495606087

Epoch: 5| Step: 9
Training loss: 3.284453218147536
Validation loss: 2.873509731039624

Epoch: 5| Step: 10
Training loss: 3.2839305282938986
Validation loss: 2.8791304384553507

Epoch: 29| Step: 0
Training loss: 3.1579583709562846
Validation loss: 2.882838182423592

Epoch: 5| Step: 1
Training loss: 3.301504924830002
Validation loss: 2.888983864428594

Epoch: 5| Step: 2
Training loss: 3.3144256464775057
Validation loss: 2.8756964946919

Epoch: 5| Step: 3
Training loss: 3.432483688949205
Validation loss: 2.8739092141076004

Epoch: 5| Step: 4
Training loss: 3.4260937190467944
Validation loss: 2.86843297721719

Epoch: 5| Step: 5
Training loss: 3.274457167271456
Validation loss: 2.869867525753896

Epoch: 5| Step: 6
Training loss: 3.3554866554928258
Validation loss: 2.866484550129488

Epoch: 5| Step: 7
Training loss: 3.0270421031674046
Validation loss: 2.8682988796300526

Epoch: 5| Step: 8
Training loss: 2.371046892234175
Validation loss: 2.8645609626381274

Epoch: 5| Step: 9
Training loss: 2.9644786423949525
Validation loss: 2.8655129191656386

Epoch: 5| Step: 10
Training loss: 3.0741705109084565
Validation loss: 2.865517019358853

Epoch: 30| Step: 0
Training loss: 3.0094406675342
Validation loss: 2.8650661964688537

Epoch: 5| Step: 1
Training loss: 3.2576043590771304
Validation loss: 2.8699317637232338

Epoch: 5| Step: 2
Training loss: 3.2445650873732172
Validation loss: 2.8742143146828183

Epoch: 5| Step: 3
Training loss: 3.127706652552145
Validation loss: 2.8664875819745954

Epoch: 5| Step: 4
Training loss: 3.2508875295239386
Validation loss: 2.8617859499287106

Epoch: 5| Step: 5
Training loss: 3.693316253136023
Validation loss: 2.8629699957775565

Epoch: 5| Step: 6
Training loss: 3.102502329761697
Validation loss: 2.864783632019528

Epoch: 5| Step: 7
Training loss: 1.9218003793511804
Validation loss: 2.86810366065408

Epoch: 5| Step: 8
Training loss: 3.028287560791129
Validation loss: 2.871730525500561

Epoch: 5| Step: 9
Training loss: 3.08175677296883
Validation loss: 2.872355246473526

Epoch: 5| Step: 10
Training loss: 3.9070939029825777
Validation loss: 2.9190849340218006

Epoch: 31| Step: 0
Training loss: 2.8283709845868232
Validation loss: 2.861769146117012

Epoch: 5| Step: 1
Training loss: 3.413366600361279
Validation loss: 2.8567901938818085

Epoch: 5| Step: 2
Training loss: 3.295217956373278
Validation loss: 2.860251062240389

Epoch: 5| Step: 3
Training loss: 3.939133804686709
Validation loss: 2.865526666375518

Epoch: 5| Step: 4
Training loss: 3.3936427877502973
Validation loss: 2.870923926966338

Epoch: 5| Step: 5
Training loss: 2.600216995867488
Validation loss: 2.877140473558609

Epoch: 5| Step: 6
Training loss: 2.438846411898728
Validation loss: 2.8889846542010402

Epoch: 5| Step: 7
Training loss: 2.630529483539299
Validation loss: 2.901348762667563

Epoch: 5| Step: 8
Training loss: 3.445589569092305
Validation loss: 2.9020971221227585

Epoch: 5| Step: 9
Training loss: 3.0750272237921714
Validation loss: 2.88394389080631

Epoch: 5| Step: 10
Training loss: 3.455177629158253
Validation loss: 2.870875554530598

Epoch: 32| Step: 0
Training loss: 2.540190085409639
Validation loss: 2.8561573480175766

Epoch: 5| Step: 1
Training loss: 2.879437049576383
Validation loss: 2.8576843295053878

Epoch: 5| Step: 2
Training loss: 2.9142968132170997
Validation loss: 2.8546913143220283

Epoch: 5| Step: 3
Training loss: 3.6960358183970596
Validation loss: 2.8519887432843936

Epoch: 5| Step: 4
Training loss: 2.936916536864273
Validation loss: 2.850620153513265

Epoch: 5| Step: 5
Training loss: 3.37358162356956
Validation loss: 2.8498971123537813

Epoch: 5| Step: 6
Training loss: 3.524952903054167
Validation loss: 2.8472017517741253

Epoch: 5| Step: 7
Training loss: 2.929831702180278
Validation loss: 2.846733157572603

Epoch: 5| Step: 8
Training loss: 3.5944689487628914
Validation loss: 2.8524978396049736

Epoch: 5| Step: 9
Training loss: 3.2816430719269087
Validation loss: 2.856683711784133

Epoch: 5| Step: 10
Training loss: 2.645509344574353
Validation loss: 2.859810279436947

Epoch: 33| Step: 0
Training loss: 2.867728616236522
Validation loss: 2.857534821384034

Epoch: 5| Step: 1
Training loss: 3.8443529850095777
Validation loss: 2.8603184765875413

Epoch: 5| Step: 2
Training loss: 2.88540590653783
Validation loss: 2.8499388883047874

Epoch: 5| Step: 3
Training loss: 3.1023192743600583
Validation loss: 2.844882309193224

Epoch: 5| Step: 4
Training loss: 3.3367181440784686
Validation loss: 2.84408095835423

Epoch: 5| Step: 5
Training loss: 3.3983709920755083
Validation loss: 2.8431642603659233

Epoch: 5| Step: 6
Training loss: 2.8248216083206827
Validation loss: 2.84283686106807

Epoch: 5| Step: 7
Training loss: 2.847961325715038
Validation loss: 2.839890731813555

Epoch: 5| Step: 8
Training loss: 3.464759249339525
Validation loss: 2.839227908903287

Epoch: 5| Step: 9
Training loss: 2.563826984977521
Validation loss: 2.8370123893781907

Epoch: 5| Step: 10
Training loss: 3.2311770344212025
Validation loss: 2.836118842326962

Epoch: 34| Step: 0
Training loss: 3.1013939953439436
Validation loss: 2.835924838779189

Epoch: 5| Step: 1
Training loss: 3.252564225614622
Validation loss: 2.833430249356008

Epoch: 5| Step: 2
Training loss: 3.444613500432
Validation loss: 2.8334487584494292

Epoch: 5| Step: 3
Training loss: 3.129518065720253
Validation loss: 2.833646612805373

Epoch: 5| Step: 4
Training loss: 2.796822744552651
Validation loss: 2.8307401750832417

Epoch: 5| Step: 5
Training loss: 3.5160199430939487
Validation loss: 2.8321625560494597

Epoch: 5| Step: 6
Training loss: 2.709174030598638
Validation loss: 2.8358727288443646

Epoch: 5| Step: 7
Training loss: 3.4115360785014412
Validation loss: 2.837939431385984

Epoch: 5| Step: 8
Training loss: 3.293507958337259
Validation loss: 2.834130880374354

Epoch: 5| Step: 9
Training loss: 2.745613327435734
Validation loss: 2.8299797836643172

Epoch: 5| Step: 10
Training loss: 2.9207386112136393
Validation loss: 2.8279978777331576

Epoch: 35| Step: 0
Training loss: 2.9104326564938483
Validation loss: 2.828106581159355

Epoch: 5| Step: 1
Training loss: 3.2153325873789385
Validation loss: 2.8252597821884833

Epoch: 5| Step: 2
Training loss: 3.251832298806405
Validation loss: 2.823614283042074

Epoch: 5| Step: 3
Training loss: 3.1883594625797875
Validation loss: 2.823337111470979

Epoch: 5| Step: 4
Training loss: 3.035325446991283
Validation loss: 2.822536804544341

Epoch: 5| Step: 5
Training loss: 3.448384202373299
Validation loss: 2.8228055477049114

Epoch: 5| Step: 6
Training loss: 2.891330282413608
Validation loss: 2.82377988166946

Epoch: 5| Step: 7
Training loss: 2.9698121429525997
Validation loss: 2.823509960099515

Epoch: 5| Step: 8
Training loss: 2.8184119078881347
Validation loss: 2.8220916609272266

Epoch: 5| Step: 9
Training loss: 3.0650810731305755
Validation loss: 2.8242088102903518

Epoch: 5| Step: 10
Training loss: 3.5904606900248566
Validation loss: 2.823167644820309

Epoch: 36| Step: 0
Training loss: 3.476221920861586
Validation loss: 2.824972613615087

Epoch: 5| Step: 1
Training loss: 2.575778239893174
Validation loss: 2.8203966404699856

Epoch: 5| Step: 2
Training loss: 3.101097552524499
Validation loss: 2.821199928959707

Epoch: 5| Step: 3
Training loss: 3.333094938018527
Validation loss: 2.820619420706818

Epoch: 5| Step: 4
Training loss: 3.3229067014028586
Validation loss: 2.821196088766537

Epoch: 5| Step: 5
Training loss: 2.835817444837438
Validation loss: 2.819142589564912

Epoch: 5| Step: 6
Training loss: 3.2218408651236428
Validation loss: 2.816518102328385

Epoch: 5| Step: 7
Training loss: 3.2554355063959894
Validation loss: 2.8185631847290957

Epoch: 5| Step: 8
Training loss: 3.1772496581185825
Validation loss: 2.8160993817262416

Epoch: 5| Step: 9
Training loss: 2.7636398691002273
Validation loss: 2.817595805212694

Epoch: 5| Step: 10
Training loss: 3.1138275318148536
Validation loss: 2.8188608202568064

Epoch: 37| Step: 0
Training loss: 2.917619349793267
Validation loss: 2.818159733270959

Epoch: 5| Step: 1
Training loss: 3.5484168108761542
Validation loss: 2.817326252073261

Epoch: 5| Step: 2
Training loss: 2.7711489409249204
Validation loss: 2.8164540961023654

Epoch: 5| Step: 3
Training loss: 2.8987025219449443
Validation loss: 2.812004207905916

Epoch: 5| Step: 4
Training loss: 3.5600183777924133
Validation loss: 2.8098460082245844

Epoch: 5| Step: 5
Training loss: 2.6478618345028315
Validation loss: 2.811023117478233

Epoch: 5| Step: 6
Training loss: 3.2034320963210297
Validation loss: 2.8145919612646226

Epoch: 5| Step: 7
Training loss: 3.008234484907472
Validation loss: 2.815239646421308

Epoch: 5| Step: 8
Training loss: 2.9261663410102354
Validation loss: 2.8156317253096663

Epoch: 5| Step: 9
Training loss: 3.3943761651857693
Validation loss: 2.807250859984125

Epoch: 5| Step: 10
Training loss: 3.254334713499863
Validation loss: 2.8102892765652685

Epoch: 38| Step: 0
Training loss: 3.0951225241718996
Validation loss: 2.806657783763615

Epoch: 5| Step: 1
Training loss: 2.872041257875394
Validation loss: 2.806273916041055

Epoch: 5| Step: 2
Training loss: 2.899782593569011
Validation loss: 2.805533901220243

Epoch: 5| Step: 3
Training loss: 3.5676188585334034
Validation loss: 2.8039905080692122

Epoch: 5| Step: 4
Training loss: 3.0797836559657417
Validation loss: 2.8045090907594394

Epoch: 5| Step: 5
Training loss: 2.3638045811315114
Validation loss: 2.8019162001747877

Epoch: 5| Step: 6
Training loss: 3.4464311578544664
Validation loss: 2.8058452562897567

Epoch: 5| Step: 7
Training loss: 3.563994244863057
Validation loss: 2.8027032963142737

Epoch: 5| Step: 8
Training loss: 2.8146532081572575
Validation loss: 2.7995279580414816

Epoch: 5| Step: 9
Training loss: 3.1155842647357406
Validation loss: 2.7992631729413398

Epoch: 5| Step: 10
Training loss: 3.130630456238461
Validation loss: 2.8021541381726993

Epoch: 39| Step: 0
Training loss: 3.2235175813905808
Validation loss: 2.8110797226494615

Epoch: 5| Step: 1
Training loss: 3.0202189028900723
Validation loss: 2.8236483301655797

Epoch: 5| Step: 2
Training loss: 3.181929414216806
Validation loss: 2.804499110446538

Epoch: 5| Step: 3
Training loss: 3.1644543063739925
Validation loss: 2.7951297921302096

Epoch: 5| Step: 4
Training loss: 2.927860921475009
Validation loss: 2.79595324548277

Epoch: 5| Step: 5
Training loss: 3.4380234753227206
Validation loss: 2.7955449353168773

Epoch: 5| Step: 6
Training loss: 3.1026695447361146
Validation loss: 2.797789514385445

Epoch: 5| Step: 7
Training loss: 3.1422873884701064
Validation loss: 2.796364863350191

Epoch: 5| Step: 8
Training loss: 3.1505748224301793
Validation loss: 2.8001049862207372

Epoch: 5| Step: 9
Training loss: 2.998949343760723
Validation loss: 2.796956818983871

Epoch: 5| Step: 10
Training loss: 2.779255355342541
Validation loss: 2.7944957819717136

Epoch: 40| Step: 0
Training loss: 3.4085472829459365
Validation loss: 2.7927316992014704

Epoch: 5| Step: 1
Training loss: 3.2801601961604843
Validation loss: 2.7934611578220125

Epoch: 5| Step: 2
Training loss: 3.3477966920110367
Validation loss: 2.795161002716702

Epoch: 5| Step: 3
Training loss: 3.1186501649461036
Validation loss: 2.801947440387424

Epoch: 5| Step: 4
Training loss: 3.387555961568935
Validation loss: 2.809893354750302

Epoch: 5| Step: 5
Training loss: 2.6313288552254432
Validation loss: 2.8140989764803894

Epoch: 5| Step: 6
Training loss: 2.639796200313011
Validation loss: 2.810535020672197

Epoch: 5| Step: 7
Training loss: 2.964863049591335
Validation loss: 2.793673106662282

Epoch: 5| Step: 8
Training loss: 2.2835155367956466
Validation loss: 2.7916501729958867

Epoch: 5| Step: 9
Training loss: 3.5323130940033307
Validation loss: 2.788005642063076

Epoch: 5| Step: 10
Training loss: 3.259149729769955
Validation loss: 2.786594670368014

Epoch: 41| Step: 0
Training loss: 3.107377680782526
Validation loss: 2.7870440200598385

Epoch: 5| Step: 1
Training loss: 3.453632714416289
Validation loss: 2.7881453897954596

Epoch: 5| Step: 2
Training loss: 3.3479683195322067
Validation loss: 2.7859027193986483

Epoch: 5| Step: 3
Training loss: 3.256163694442416
Validation loss: 2.7891834472139503

Epoch: 5| Step: 4
Training loss: 3.2736393904962697
Validation loss: 2.7888601423924806

Epoch: 5| Step: 5
Training loss: 2.4565401056402267
Validation loss: 2.786567518555987

Epoch: 5| Step: 6
Training loss: 3.092549717348528
Validation loss: 2.7841438437353703

Epoch: 5| Step: 7
Training loss: 2.9728726322055734
Validation loss: 2.789801887739397

Epoch: 5| Step: 8
Training loss: 2.9255605698442047
Validation loss: 2.8054482366969533

Epoch: 5| Step: 9
Training loss: 3.1069519723471233
Validation loss: 2.8152504318979767

Epoch: 5| Step: 10
Training loss: 2.987940710273608
Validation loss: 2.834562588421443

Epoch: 42| Step: 0
Training loss: 2.4487578766069604
Validation loss: 2.816934671143485

Epoch: 5| Step: 1
Training loss: 3.2022870712308515
Validation loss: 2.8021534556705765

Epoch: 5| Step: 2
Training loss: 3.4649530197855913
Validation loss: 2.7821206636580693

Epoch: 5| Step: 3
Training loss: 3.0641005090109705
Validation loss: 2.7777018888081337

Epoch: 5| Step: 4
Training loss: 2.8246406461651477
Validation loss: 2.776117954330661

Epoch: 5| Step: 5
Training loss: 3.323403604798211
Validation loss: 2.776858344108374

Epoch: 5| Step: 6
Training loss: 2.9741486300876994
Validation loss: 2.777373578357498

Epoch: 5| Step: 7
Training loss: 2.9024686106217463
Validation loss: 2.7765578519012553

Epoch: 5| Step: 8
Training loss: 3.101214718477671
Validation loss: 2.7779724204325085

Epoch: 5| Step: 9
Training loss: 3.228660067946235
Validation loss: 2.7827060672269286

Epoch: 5| Step: 10
Training loss: 3.379593160849591
Validation loss: 2.784926217977597

Epoch: 43| Step: 0
Training loss: 2.949783453024776
Validation loss: 2.7811659214556306

Epoch: 5| Step: 1
Training loss: 3.293779845708737
Validation loss: 2.7789543165065194

Epoch: 5| Step: 2
Training loss: 2.3178152909762884
Validation loss: 2.772272386880059

Epoch: 5| Step: 3
Training loss: 3.5641507791291867
Validation loss: 2.7710193926541993

Epoch: 5| Step: 4
Training loss: 2.2840731148649933
Validation loss: 2.7689405871565147

Epoch: 5| Step: 5
Training loss: 2.7305558044788776
Validation loss: 2.76941226512893

Epoch: 5| Step: 6
Training loss: 3.5110366379475813
Validation loss: 2.7747951264542223

Epoch: 5| Step: 7
Training loss: 3.270167499650292
Validation loss: 2.7770185211938982

Epoch: 5| Step: 8
Training loss: 3.487371687083915
Validation loss: 2.7801807003027843

Epoch: 5| Step: 9
Training loss: 3.2502978261787496
Validation loss: 2.768073515820067

Epoch: 5| Step: 10
Training loss: 2.941829331811226
Validation loss: 2.7663780759692473

Epoch: 44| Step: 0
Training loss: 3.5047637672646434
Validation loss: 2.7650524056718444

Epoch: 5| Step: 1
Training loss: 2.4618740661107985
Validation loss: 2.7633553877728136

Epoch: 5| Step: 2
Training loss: 2.5188985813890907
Validation loss: 2.762444813346508

Epoch: 5| Step: 3
Training loss: 3.5204721901370446
Validation loss: 2.7615412660447567

Epoch: 5| Step: 4
Training loss: 3.0438995700565306
Validation loss: 2.760258987080846

Epoch: 5| Step: 5
Training loss: 3.208116168974823
Validation loss: 2.7619026077512054

Epoch: 5| Step: 6
Training loss: 3.1338847696412913
Validation loss: 2.7602495294185623

Epoch: 5| Step: 7
Training loss: 3.4773187050638477
Validation loss: 2.762037800877206

Epoch: 5| Step: 8
Training loss: 2.483453639741118
Validation loss: 2.76069791123029

Epoch: 5| Step: 9
Training loss: 3.2963576069042047
Validation loss: 2.759947459734515

Epoch: 5| Step: 10
Training loss: 2.874221032803524
Validation loss: 2.7616566921132746

Epoch: 45| Step: 0
Training loss: 2.596292375743109
Validation loss: 2.7669417051716754

Epoch: 5| Step: 1
Training loss: 3.380938920749655
Validation loss: 2.7788600942625528

Epoch: 5| Step: 2
Training loss: 3.9504308151755145
Validation loss: 2.7927435161891494

Epoch: 5| Step: 3
Training loss: 2.8727962506466573
Validation loss: 2.7646571401361726

Epoch: 5| Step: 4
Training loss: 2.760884160215776
Validation loss: 2.7539595348096557

Epoch: 5| Step: 5
Training loss: 2.990583741990665
Validation loss: 2.755200331583823

Epoch: 5| Step: 6
Training loss: 2.709610496062996
Validation loss: 2.7563451185089143

Epoch: 5| Step: 7
Training loss: 2.7189537059719324
Validation loss: 2.752236909847579

Epoch: 5| Step: 8
Training loss: 3.153476516290227
Validation loss: 2.7567101590465537

Epoch: 5| Step: 9
Training loss: 3.312937725583774
Validation loss: 2.7570557286717734

Epoch: 5| Step: 10
Training loss: 3.1274061476471693
Validation loss: 2.759082777610447

Epoch: 46| Step: 0
Training loss: 3.109065514886184
Validation loss: 2.7554273707669505

Epoch: 5| Step: 1
Training loss: 2.446781290142871
Validation loss: 2.7551612439797375

Epoch: 5| Step: 2
Training loss: 3.290256020924138
Validation loss: 2.749938250992275

Epoch: 5| Step: 3
Training loss: 2.779183895471369
Validation loss: 2.7516034486639915

Epoch: 5| Step: 4
Training loss: 3.2630312899913925
Validation loss: 2.750881451866852

Epoch: 5| Step: 5
Training loss: 3.771496386042177
Validation loss: 2.750343726370714

Epoch: 5| Step: 6
Training loss: 2.7111684387517667
Validation loss: 2.7486664782035097

Epoch: 5| Step: 7
Training loss: 3.040743718275102
Validation loss: 2.7478599620918347

Epoch: 5| Step: 8
Training loss: 3.0467848544356966
Validation loss: 2.7474059738036583

Epoch: 5| Step: 9
Training loss: 3.2278250747895156
Validation loss: 2.751710172582942

Epoch: 5| Step: 10
Training loss: 2.790889707785485
Validation loss: 2.752479020361457

Epoch: 47| Step: 0
Training loss: 2.68182359086536
Validation loss: 2.761939919903299

Epoch: 5| Step: 1
Training loss: 3.0840909044397744
Validation loss: 2.7612328460231383

Epoch: 5| Step: 2
Training loss: 3.2498560653505266
Validation loss: 2.7567647473807195

Epoch: 5| Step: 3
Training loss: 3.0390337866674577
Validation loss: 2.7564475052296276

Epoch: 5| Step: 4
Training loss: 3.3659664338603426
Validation loss: 2.7564756215005315

Epoch: 5| Step: 5
Training loss: 2.7644614665450136
Validation loss: 2.7497476254997824

Epoch: 5| Step: 6
Training loss: 3.2566034282634186
Validation loss: 2.743029916779434

Epoch: 5| Step: 7
Training loss: 2.849089587510567
Validation loss: 2.742427986817771

Epoch: 5| Step: 8
Training loss: 2.8610691409031554
Validation loss: 2.740676078648261

Epoch: 5| Step: 9
Training loss: 3.094692356429767
Validation loss: 2.743152750609962

Epoch: 5| Step: 10
Training loss: 3.4340624180199364
Validation loss: 2.740328004704642

Epoch: 48| Step: 0
Training loss: 2.938528814133307
Validation loss: 2.7377949384446403

Epoch: 5| Step: 1
Training loss: 3.172204813229309
Validation loss: 2.742904466052565

Epoch: 5| Step: 2
Training loss: 3.1896968078028998
Validation loss: 2.7513691252448993

Epoch: 5| Step: 3
Training loss: 3.1431001996518133
Validation loss: 2.7591676210664113

Epoch: 5| Step: 4
Training loss: 2.281287310569512
Validation loss: 2.761845245297956

Epoch: 5| Step: 5
Training loss: 3.3103858390958787
Validation loss: 2.7600888473010268

Epoch: 5| Step: 6
Training loss: 3.0177092786020876
Validation loss: 2.7580167405479163

Epoch: 5| Step: 7
Training loss: 3.1142118778950096
Validation loss: 2.7509738003410047

Epoch: 5| Step: 8
Training loss: 3.261090941170901
Validation loss: 2.739630393495077

Epoch: 5| Step: 9
Training loss: 2.6021728716050054
Validation loss: 2.7350494818607345

Epoch: 5| Step: 10
Training loss: 3.496853094910167
Validation loss: 2.7311871619085086

Epoch: 49| Step: 0
Training loss: 3.067588782234289
Validation loss: 2.7338224438103373

Epoch: 5| Step: 1
Training loss: 3.004112127464193
Validation loss: 2.733658730351314

Epoch: 5| Step: 2
Training loss: 3.094830719196279
Validation loss: 2.7349045651151362

Epoch: 5| Step: 3
Training loss: 2.739217252652035
Validation loss: 2.7358047258701546

Epoch: 5| Step: 4
Training loss: 2.977642674330836
Validation loss: 2.742657265737781

Epoch: 5| Step: 5
Training loss: 3.0917088103870807
Validation loss: 2.7821575332700754

Epoch: 5| Step: 6
Training loss: 3.821112488533204
Validation loss: 2.7780156092878836

Epoch: 5| Step: 7
Training loss: 2.280966754207801
Validation loss: 2.7317245796836445

Epoch: 5| Step: 8
Training loss: 3.2819309073084826
Validation loss: 2.7288903105315225

Epoch: 5| Step: 9
Training loss: 3.20848253241741
Validation loss: 2.732195102863916

Epoch: 5| Step: 10
Training loss: 3.0194785050576005
Validation loss: 2.7708206848798334

Epoch: 50| Step: 0
Training loss: 3.2467021349066836
Validation loss: 2.821292662676193

Epoch: 5| Step: 1
Training loss: 3.0507640569497925
Validation loss: 2.8769649195188616

Epoch: 5| Step: 2
Training loss: 3.492277344778969
Validation loss: 2.8347098073085095

Epoch: 5| Step: 3
Training loss: 2.9695438327537778
Validation loss: 2.7431026848432105

Epoch: 5| Step: 4
Training loss: 2.867317550162814
Validation loss: 2.729068599975217

Epoch: 5| Step: 5
Training loss: 3.0926260399633
Validation loss: 2.732471745231265

Epoch: 5| Step: 6
Training loss: 3.0360842823899654
Validation loss: 2.7582576698826102

Epoch: 5| Step: 7
Training loss: 2.9999089227202713
Validation loss: 2.7546053189169957

Epoch: 5| Step: 8
Training loss: 3.515243848696118
Validation loss: 2.7378459617972575

Epoch: 5| Step: 9
Training loss: 2.735500866847458
Validation loss: 2.7253698882385082

Epoch: 5| Step: 10
Training loss: 2.5907849749050667
Validation loss: 2.7258973966883806

Epoch: 51| Step: 0
Training loss: 2.632757893796893
Validation loss: 2.7272203966115693

Epoch: 5| Step: 1
Training loss: 2.917419627046037
Validation loss: 2.726966832396327

Epoch: 5| Step: 2
Training loss: 2.745655268991382
Validation loss: 2.728511727646856

Epoch: 5| Step: 3
Training loss: 3.334520668916398
Validation loss: 2.731346823280656

Epoch: 5| Step: 4
Training loss: 3.042789948563907
Validation loss: 2.7304515253320294

Epoch: 5| Step: 5
Training loss: 2.794334749816271
Validation loss: 2.7263036736044337

Epoch: 5| Step: 6
Training loss: 3.1626889862165006
Validation loss: 2.7281087462373743

Epoch: 5| Step: 7
Training loss: 3.05449236858774
Validation loss: 2.740944955890521

Epoch: 5| Step: 8
Training loss: 3.7496354243759304
Validation loss: 2.7516861963735866

Epoch: 5| Step: 9
Training loss: 2.7516278737331783
Validation loss: 2.7327326498230047

Epoch: 5| Step: 10
Training loss: 3.271632313586032
Validation loss: 2.7193629858914563

Epoch: 52| Step: 0
Training loss: 3.245216517344536
Validation loss: 2.717888619847504

Epoch: 5| Step: 1
Training loss: 2.3083358594786483
Validation loss: 2.7193464907944516

Epoch: 5| Step: 2
Training loss: 3.140593115801306
Validation loss: 2.7168980968374554

Epoch: 5| Step: 3
Training loss: 3.188704169709286
Validation loss: 2.7144984202357847

Epoch: 5| Step: 4
Training loss: 3.3602746512921677
Validation loss: 2.7196771523359904

Epoch: 5| Step: 5
Training loss: 3.3661034204163243
Validation loss: 2.7222065907012025

Epoch: 5| Step: 6
Training loss: 2.9492413854677713
Validation loss: 2.7274560615642303

Epoch: 5| Step: 7
Training loss: 3.029723579463404
Validation loss: 2.7311801248079854

Epoch: 5| Step: 8
Training loss: 3.0523319772376625
Validation loss: 2.7260245328900456

Epoch: 5| Step: 9
Training loss: 2.8901944226399685
Validation loss: 2.7288412203639285

Epoch: 5| Step: 10
Training loss: 2.764016756031792
Validation loss: 2.7320669063351573

Epoch: 53| Step: 0
Training loss: 3.612725622397789
Validation loss: 2.730250137799743

Epoch: 5| Step: 1
Training loss: 3.403700548163693
Validation loss: 2.719413132370502

Epoch: 5| Step: 2
Training loss: 2.928336602608845
Validation loss: 2.710860383647178

Epoch: 5| Step: 3
Training loss: 2.7777020401696078
Validation loss: 2.711589770106353

Epoch: 5| Step: 4
Training loss: 3.2745263375938363
Validation loss: 2.7149204227093184

Epoch: 5| Step: 5
Training loss: 3.2242827039161672
Validation loss: 2.713947135393807

Epoch: 5| Step: 6
Training loss: 2.465091166947938
Validation loss: 2.720439869913703

Epoch: 5| Step: 7
Training loss: 2.742560814989081
Validation loss: 2.715613613007789

Epoch: 5| Step: 8
Training loss: 3.0321509212083053
Validation loss: 2.7140404232220443

Epoch: 5| Step: 9
Training loss: 3.0924732386572926
Validation loss: 2.7098234300759483

Epoch: 5| Step: 10
Training loss: 2.6944916733036015
Validation loss: 2.7146006542242485

Epoch: 54| Step: 0
Training loss: 3.8872981420773414
Validation loss: 2.7140946654474454

Epoch: 5| Step: 1
Training loss: 2.6927203553827272
Validation loss: 2.7167344798337516

Epoch: 5| Step: 2
Training loss: 3.247839870038413
Validation loss: 2.717037445615892

Epoch: 5| Step: 3
Training loss: 2.840285318038702
Validation loss: 2.720882016874733

Epoch: 5| Step: 4
Training loss: 3.482566192939914
Validation loss: 2.708571986876022

Epoch: 5| Step: 5
Training loss: 2.6545296876374973
Validation loss: 2.699633925337293

Epoch: 5| Step: 6
Training loss: 2.6266389907146976
Validation loss: 2.6962008502775183

Epoch: 5| Step: 7
Training loss: 2.7974786373410243
Validation loss: 2.7007984793092215

Epoch: 5| Step: 8
Training loss: 2.9561113062900355
Validation loss: 2.700845351179546

Epoch: 5| Step: 9
Training loss: 2.7824308481731803
Validation loss: 2.698708446602102

Epoch: 5| Step: 10
Training loss: 3.1945270048520493
Validation loss: 2.696856141843458

Epoch: 55| Step: 0
Training loss: 2.5693036450365203
Validation loss: 2.6990935681830903

Epoch: 5| Step: 1
Training loss: 3.4540437192552105
Validation loss: 2.6964311772309837

Epoch: 5| Step: 2
Training loss: 3.1691675516736337
Validation loss: 2.6989057299528447

Epoch: 5| Step: 3
Training loss: 2.95429735275763
Validation loss: 2.698865582912454

Epoch: 5| Step: 4
Training loss: 3.1207788568757877
Validation loss: 2.701797367019126

Epoch: 5| Step: 5
Training loss: 2.876996715290527
Validation loss: 2.712832878249042

Epoch: 5| Step: 6
Training loss: 2.939568298719544
Validation loss: 2.7116670982875357

Epoch: 5| Step: 7
Training loss: 3.090501409051707
Validation loss: 2.6968130316344032

Epoch: 5| Step: 8
Training loss: 3.1546454552942076
Validation loss: 2.694512523026051

Epoch: 5| Step: 9
Training loss: 2.779423145849088
Validation loss: 2.693871429213616

Epoch: 5| Step: 10
Training loss: 3.20476047352809
Validation loss: 2.695313878212853

Epoch: 56| Step: 0
Training loss: 2.6604321038483603
Validation loss: 2.7005805106284604

Epoch: 5| Step: 1
Training loss: 3.1248315384280616
Validation loss: 2.6997170566288435

Epoch: 5| Step: 2
Training loss: 2.963959211400883
Validation loss: 2.6963186988771093

Epoch: 5| Step: 3
Training loss: 3.1253024145664874
Validation loss: 2.696548260899306

Epoch: 5| Step: 4
Training loss: 3.1722729062204795
Validation loss: 2.6988271574607

Epoch: 5| Step: 5
Training loss: 2.9097782171097015
Validation loss: 2.6991697992905404

Epoch: 5| Step: 6
Training loss: 3.176467952644069
Validation loss: 2.6983935147386835

Epoch: 5| Step: 7
Training loss: 2.2058994868491952
Validation loss: 2.7070908042927875

Epoch: 5| Step: 8
Training loss: 3.367424976444889
Validation loss: 2.7225854411041874

Epoch: 5| Step: 9
Training loss: 3.5399360168192064
Validation loss: 2.717019471069488

Epoch: 5| Step: 10
Training loss: 2.949504429199701
Validation loss: 2.705847421079486

Epoch: 57| Step: 0
Training loss: 2.8993997544590546
Validation loss: 2.6894182331070455

Epoch: 5| Step: 1
Training loss: 3.135416294913201
Validation loss: 2.687864103881379

Epoch: 5| Step: 2
Training loss: 2.7102955874453407
Validation loss: 2.6879971715181727

Epoch: 5| Step: 3
Training loss: 2.862508422306114
Validation loss: 2.6892252871789983

Epoch: 5| Step: 4
Training loss: 2.7059071947016013
Validation loss: 2.694824998356767

Epoch: 5| Step: 5
Training loss: 2.2483896214607864
Validation loss: 2.695962476004194

Epoch: 5| Step: 6
Training loss: 3.300652242994862
Validation loss: 2.700995251224528

Epoch: 5| Step: 7
Training loss: 2.860999807849119
Validation loss: 2.696580061075567

Epoch: 5| Step: 8
Training loss: 3.3239206613007584
Validation loss: 2.6942697984394317

Epoch: 5| Step: 9
Training loss: 3.3181357754639973
Validation loss: 2.687398740249218

Epoch: 5| Step: 10
Training loss: 3.8008512547349333
Validation loss: 2.6848591571177227

Epoch: 58| Step: 0
Training loss: 3.188414498582408
Validation loss: 2.684414549285814

Epoch: 5| Step: 1
Training loss: 3.432559537878273
Validation loss: 2.6872947730889227

Epoch: 5| Step: 2
Training loss: 2.3913143385802833
Validation loss: 2.684411848518391

Epoch: 5| Step: 3
Training loss: 2.772646251154176
Validation loss: 2.690591718853212

Epoch: 5| Step: 4
Training loss: 3.643250136988805
Validation loss: 2.694305677963253

Epoch: 5| Step: 5
Training loss: 2.916085939268628
Validation loss: 2.6975609896530974

Epoch: 5| Step: 6
Training loss: 2.3027780672673175
Validation loss: 2.702700135140556

Epoch: 5| Step: 7
Training loss: 2.809954614646089
Validation loss: 2.7033375932613817

Epoch: 5| Step: 8
Training loss: 3.127244524747932
Validation loss: 2.700549689775259

Epoch: 5| Step: 9
Training loss: 2.808301886619236
Validation loss: 2.694605474252182

Epoch: 5| Step: 10
Training loss: 3.608964640752004
Validation loss: 2.687941226668638

Epoch: 59| Step: 0
Training loss: 2.53522292697491
Validation loss: 2.6798643777725184

Epoch: 5| Step: 1
Training loss: 3.2640252826590364
Validation loss: 2.6772542747051826

Epoch: 5| Step: 2
Training loss: 2.526851647415589
Validation loss: 2.680431174638003

Epoch: 5| Step: 3
Training loss: 3.3379846228170504
Validation loss: 2.680055349380222

Epoch: 5| Step: 4
Training loss: 3.0063577676280544
Validation loss: 2.6779772303238616

Epoch: 5| Step: 5
Training loss: 3.311543308514548
Validation loss: 2.6840672424921785

Epoch: 5| Step: 6
Training loss: 3.5039134626197366
Validation loss: 2.679943822386104

Epoch: 5| Step: 7
Training loss: 2.8732355757307855
Validation loss: 2.678688343541645

Epoch: 5| Step: 8
Training loss: 2.8592964536791166
Validation loss: 2.675620030126463

Epoch: 5| Step: 9
Training loss: 3.153748682530597
Validation loss: 2.6748763280369463

Epoch: 5| Step: 10
Training loss: 2.4950466676066716
Validation loss: 2.6758493300878134

Epoch: 60| Step: 0
Training loss: 3.324761925685476
Validation loss: 2.6760932349461863

Epoch: 5| Step: 1
Training loss: 2.8712262804233237
Validation loss: 2.680455090981143

Epoch: 5| Step: 2
Training loss: 3.3064139255203666
Validation loss: 2.6747718683855433

Epoch: 5| Step: 3
Training loss: 2.5727270779791884
Validation loss: 2.674713024531854

Epoch: 5| Step: 4
Training loss: 2.728268677029456
Validation loss: 2.6732528991690554

Epoch: 5| Step: 5
Training loss: 2.582779630162232
Validation loss: 2.673665470855732

Epoch: 5| Step: 6
Training loss: 2.6328559668147795
Validation loss: 2.672074562885595

Epoch: 5| Step: 7
Training loss: 3.103471989742018
Validation loss: 2.669858470774552

Epoch: 5| Step: 8
Training loss: 3.2996269419796223
Validation loss: 2.6694704577305717

Epoch: 5| Step: 9
Training loss: 2.967775044216413
Validation loss: 2.6667242473364077

Epoch: 5| Step: 10
Training loss: 3.578244403029578
Validation loss: 2.6650577839512177

Epoch: 61| Step: 0
Training loss: 3.220570169992983
Validation loss: 2.6716724110977994

Epoch: 5| Step: 1
Training loss: 3.2995671132848323
Validation loss: 2.672524404533374

Epoch: 5| Step: 2
Training loss: 2.958660447925406
Validation loss: 2.6822067621645784

Epoch: 5| Step: 3
Training loss: 3.20199311889824
Validation loss: 2.694529561184515

Epoch: 5| Step: 4
Training loss: 2.6368259888493193
Validation loss: 2.686544579991305

Epoch: 5| Step: 5
Training loss: 3.1971795706100203
Validation loss: 2.6853453547695434

Epoch: 5| Step: 6
Training loss: 3.182614790291365
Validation loss: 2.6835310127187713

Epoch: 5| Step: 7
Training loss: 2.538718614032069
Validation loss: 2.6737122076089634

Epoch: 5| Step: 8
Training loss: 3.180800184663963
Validation loss: 2.6709474465855068

Epoch: 5| Step: 9
Training loss: 2.771934682382024
Validation loss: 2.6641396411003044

Epoch: 5| Step: 10
Training loss: 2.6378155270990584
Validation loss: 2.667333506710387

Epoch: 62| Step: 0
Training loss: 3.267255458188867
Validation loss: 2.658643623905833

Epoch: 5| Step: 1
Training loss: 3.34177418739559
Validation loss: 2.658306174348955

Epoch: 5| Step: 2
Training loss: 2.467235825145688
Validation loss: 2.660658750671324

Epoch: 5| Step: 3
Training loss: 3.071189231038472
Validation loss: 2.664377250406315

Epoch: 5| Step: 4
Training loss: 3.040081412329497
Validation loss: 2.6647364449244773

Epoch: 5| Step: 5
Training loss: 3.270845757398072
Validation loss: 2.6648172559123493

Epoch: 5| Step: 6
Training loss: 3.259416729521865
Validation loss: 2.665426289456662

Epoch: 5| Step: 7
Training loss: 2.699289412063502
Validation loss: 2.655172452057133

Epoch: 5| Step: 8
Training loss: 2.497055226721638
Validation loss: 2.655012676673083

Epoch: 5| Step: 9
Training loss: 3.043454641937635
Validation loss: 2.658300063492457

Epoch: 5| Step: 10
Training loss: 2.929787758701158
Validation loss: 2.666784422015227

Epoch: 63| Step: 0
Training loss: 3.197113052111382
Validation loss: 2.6741377187014446

Epoch: 5| Step: 1
Training loss: 3.320438301844716
Validation loss: 2.689722239871098

Epoch: 5| Step: 2
Training loss: 3.210737458840578
Validation loss: 2.685077140767665

Epoch: 5| Step: 3
Training loss: 2.67564491982765
Validation loss: 2.680789073050461

Epoch: 5| Step: 4
Training loss: 3.072085113945683
Validation loss: 2.662295940289466

Epoch: 5| Step: 5
Training loss: 3.172381431101401
Validation loss: 2.6570161142925297

Epoch: 5| Step: 6
Training loss: 2.6430551756671847
Validation loss: 2.651639796458143

Epoch: 5| Step: 7
Training loss: 2.483120391557221
Validation loss: 2.6568673783855923

Epoch: 5| Step: 8
Training loss: 3.0303831139010464
Validation loss: 2.660313819019395

Epoch: 5| Step: 9
Training loss: 3.0467602830467437
Validation loss: 2.663267097773291

Epoch: 5| Step: 10
Training loss: 3.0291701584856634
Validation loss: 2.6623509565363297

Epoch: 64| Step: 0
Training loss: 3.1531226650565873
Validation loss: 2.661596106136424

Epoch: 5| Step: 1
Training loss: 2.3785957421917976
Validation loss: 2.6594650779836586

Epoch: 5| Step: 2
Training loss: 2.5606015313758075
Validation loss: 2.664726598205702

Epoch: 5| Step: 3
Training loss: 3.4383918298900276
Validation loss: 2.683242631825251

Epoch: 5| Step: 4
Training loss: 3.49173591988167
Validation loss: 2.66310174429531

Epoch: 5| Step: 5
Training loss: 2.6693017179361633
Validation loss: 2.6557582643760944

Epoch: 5| Step: 6
Training loss: 3.127848585722704
Validation loss: 2.6499357059571036

Epoch: 5| Step: 7
Training loss: 2.858332876607638
Validation loss: 2.647695586715615

Epoch: 5| Step: 8
Training loss: 3.3226436554656376
Validation loss: 2.6475935114593696

Epoch: 5| Step: 9
Training loss: 2.2862495089489228
Validation loss: 2.6456753547394527

Epoch: 5| Step: 10
Training loss: 3.4699405311083806
Validation loss: 2.6519627543106816

Epoch: 65| Step: 0
Training loss: 2.8674565742483695
Validation loss: 2.664517968131549

Epoch: 5| Step: 1
Training loss: 2.7252442416704254
Validation loss: 2.650877215291175

Epoch: 5| Step: 2
Training loss: 3.227342266598621
Validation loss: 2.6469021728676725

Epoch: 5| Step: 3
Training loss: 3.218903176820198
Validation loss: 2.6438588171030775

Epoch: 5| Step: 4
Training loss: 3.064284758261202
Validation loss: 2.6485221990332692

Epoch: 5| Step: 5
Training loss: 3.0594462525717767
Validation loss: 2.640805583831582

Epoch: 5| Step: 6
Training loss: 2.7807106609530132
Validation loss: 2.6480409131240465

Epoch: 5| Step: 7
Training loss: 3.1601346726028647
Validation loss: 2.644090411873207

Epoch: 5| Step: 8
Training loss: 2.875893329652976
Validation loss: 2.6455930026271295

Epoch: 5| Step: 9
Training loss: 3.0519915535486217
Validation loss: 2.6436979259555278

Epoch: 5| Step: 10
Training loss: 2.74501817457509
Validation loss: 2.644012416271872

Epoch: 66| Step: 0
Training loss: 3.461245279856529
Validation loss: 2.6472183709342776

Epoch: 5| Step: 1
Training loss: 3.0247011525732472
Validation loss: 2.6517443204517126

Epoch: 5| Step: 2
Training loss: 3.078119733002315
Validation loss: 2.6530982257794533

Epoch: 5| Step: 3
Training loss: 2.474239957072341
Validation loss: 2.6458021100247757

Epoch: 5| Step: 4
Training loss: 2.8405895067269995
Validation loss: 2.6430733273147755

Epoch: 5| Step: 5
Training loss: 2.860328390301752
Validation loss: 2.6505889135269793

Epoch: 5| Step: 6
Training loss: 2.637718451976437
Validation loss: 2.653026773106615

Epoch: 5| Step: 7
Training loss: 3.6732858098366195
Validation loss: 2.6488874767473867

Epoch: 5| Step: 8
Training loss: 2.60006569999435
Validation loss: 2.6540408073533857

Epoch: 5| Step: 9
Training loss: 2.9900248308968744
Validation loss: 2.647580100596798

Epoch: 5| Step: 10
Training loss: 3.013620768006787
Validation loss: 2.644316645440452

Epoch: 67| Step: 0
Training loss: 3.0202503211759577
Validation loss: 2.643082306097471

Epoch: 5| Step: 1
Training loss: 3.4534822167026635
Validation loss: 2.6497431624604717

Epoch: 5| Step: 2
Training loss: 3.0310143447984577
Validation loss: 2.6507479284548623

Epoch: 5| Step: 3
Training loss: 3.0184701730626635
Validation loss: 2.6541693020975905

Epoch: 5| Step: 4
Training loss: 3.235092894268401
Validation loss: 2.651578855581615

Epoch: 5| Step: 5
Training loss: 3.035041561439165
Validation loss: 2.653260186099148

Epoch: 5| Step: 6
Training loss: 2.818859624316898
Validation loss: 2.659189350375933

Epoch: 5| Step: 7
Training loss: 2.8154186787093995
Validation loss: 2.650899065664879

Epoch: 5| Step: 8
Training loss: 2.392447182322451
Validation loss: 2.654489309838367

Epoch: 5| Step: 9
Training loss: 3.0461063295717583
Validation loss: 2.6489904486316687

Epoch: 5| Step: 10
Training loss: 2.811006361333651
Validation loss: 2.6448980656153855

Epoch: 68| Step: 0
Training loss: 3.1445574836347485
Validation loss: 2.640985577740774

Epoch: 5| Step: 1
Training loss: 3.0172053332125266
Validation loss: 2.6380105179318556

Epoch: 5| Step: 2
Training loss: 2.929046479350958
Validation loss: 2.635719912869897

Epoch: 5| Step: 3
Training loss: 2.9002503846040595
Validation loss: 2.6321659671513036

Epoch: 5| Step: 4
Training loss: 3.3055125980809357
Validation loss: 2.633589995796806

Epoch: 5| Step: 5
Training loss: 3.346347263591291
Validation loss: 2.632059806592575

Epoch: 5| Step: 6
Training loss: 2.864017241058454
Validation loss: 2.6331217901117125

Epoch: 5| Step: 7
Training loss: 2.834855138451674
Validation loss: 2.6327288866816634

Epoch: 5| Step: 8
Training loss: 3.2356147785699543
Validation loss: 2.6343925308790404

Epoch: 5| Step: 9
Training loss: 2.5010699843445714
Validation loss: 2.6346213331358803

Epoch: 5| Step: 10
Training loss: 2.46481318386852
Validation loss: 2.642813947891355

Epoch: 69| Step: 0
Training loss: 2.4062074929978774
Validation loss: 2.6394517785763534

Epoch: 5| Step: 1
Training loss: 3.292027095585832
Validation loss: 2.646879043941157

Epoch: 5| Step: 2
Training loss: 3.106544931943617
Validation loss: 2.6314926892539487

Epoch: 5| Step: 3
Training loss: 2.952970954873608
Validation loss: 2.6268752617532223

Epoch: 5| Step: 4
Training loss: 3.1186708061850763
Validation loss: 2.6278629950945

Epoch: 5| Step: 5
Training loss: 2.9981303747024377
Validation loss: 2.6277920396343455

Epoch: 5| Step: 6
Training loss: 3.23567726345049
Validation loss: 2.6262272701855434

Epoch: 5| Step: 7
Training loss: 2.639189832381141
Validation loss: 2.626116435614625

Epoch: 5| Step: 8
Training loss: 2.9953496175504895
Validation loss: 2.6263089673946634

Epoch: 5| Step: 9
Training loss: 2.611820650979795
Validation loss: 2.6267132868996033

Epoch: 5| Step: 10
Training loss: 3.3415502995677047
Validation loss: 2.625785216999999

Epoch: 70| Step: 0
Training loss: 3.210852851587238
Validation loss: 2.624726440895173

Epoch: 5| Step: 1
Training loss: 2.3711240160045786
Validation loss: 2.626423494222108

Epoch: 5| Step: 2
Training loss: 3.3162296818794856
Validation loss: 2.6412804485416537

Epoch: 5| Step: 3
Training loss: 3.5607233804132115
Validation loss: 2.6585827500372794

Epoch: 5| Step: 4
Training loss: 2.2163362940443716
Validation loss: 2.6785837272404667

Epoch: 5| Step: 5
Training loss: 3.2223310086824726
Validation loss: 2.6896243073872683

Epoch: 5| Step: 6
Training loss: 2.8265752603338803
Validation loss: 2.6927254993766425

Epoch: 5| Step: 7
Training loss: 3.4038159834524757
Validation loss: 2.6587965955432504

Epoch: 5| Step: 8
Training loss: 2.963197354034989
Validation loss: 2.6289522778028998

Epoch: 5| Step: 9
Training loss: 2.508354722583439
Validation loss: 2.618272878887148

Epoch: 5| Step: 10
Training loss: 2.878093382157441
Validation loss: 2.621263721183358

Epoch: 71| Step: 0
Training loss: 2.806356034686335
Validation loss: 2.6229105288696752

Epoch: 5| Step: 1
Training loss: 2.6756603352869512
Validation loss: 2.6238444588328487

Epoch: 5| Step: 2
Training loss: 2.388194525394757
Validation loss: 2.6252593885516133

Epoch: 5| Step: 3
Training loss: 2.845845048097447
Validation loss: 2.625657868852108

Epoch: 5| Step: 4
Training loss: 2.852540399798557
Validation loss: 2.620547112283309

Epoch: 5| Step: 5
Training loss: 2.9796435812487467
Validation loss: 2.6221427534261097

Epoch: 5| Step: 6
Training loss: 3.3256915871861983
Validation loss: 2.6222518583372305

Epoch: 5| Step: 7
Training loss: 3.401176103366069
Validation loss: 2.6287343056239982

Epoch: 5| Step: 8
Training loss: 3.536323615960066
Validation loss: 2.6307848031906462

Epoch: 5| Step: 9
Training loss: 2.751487762978523
Validation loss: 2.628294735119966

Epoch: 5| Step: 10
Training loss: 2.9533912498300583
Validation loss: 2.62643067730042

Epoch: 72| Step: 0
Training loss: 2.389309988931023
Validation loss: 2.6281949809438245

Epoch: 5| Step: 1
Training loss: 3.215929888478202
Validation loss: 2.6365843817269186

Epoch: 5| Step: 2
Training loss: 3.1164612653097854
Validation loss: 2.6338043199673264

Epoch: 5| Step: 3
Training loss: 3.294706236923418
Validation loss: 2.623074690296149

Epoch: 5| Step: 4
Training loss: 3.2229435833080484
Validation loss: 2.6160626029730394

Epoch: 5| Step: 5
Training loss: 2.9676199920741873
Validation loss: 2.613956854035467

Epoch: 5| Step: 6
Training loss: 2.767030692759893
Validation loss: 2.612206578958313

Epoch: 5| Step: 7
Training loss: 3.1592044410738107
Validation loss: 2.617142055792204

Epoch: 5| Step: 8
Training loss: 2.975484978390652
Validation loss: 2.6120885963219354

Epoch: 5| Step: 9
Training loss: 2.4155835541677146
Validation loss: 2.611412591408966

Epoch: 5| Step: 10
Training loss: 3.015065670025758
Validation loss: 2.612798708201846

Epoch: 73| Step: 0
Training loss: 3.026230106406548
Validation loss: 2.6186920019406665

Epoch: 5| Step: 1
Training loss: 3.735649242632002
Validation loss: 2.6264167484161773

Epoch: 5| Step: 2
Training loss: 2.9566522756037794
Validation loss: 2.6139238133083103

Epoch: 5| Step: 3
Training loss: 2.247636825588968
Validation loss: 2.61497974189245

Epoch: 5| Step: 4
Training loss: 3.096714333563283
Validation loss: 2.6091562245353703

Epoch: 5| Step: 5
Training loss: 3.009300597034631
Validation loss: 2.6093630128186867

Epoch: 5| Step: 6
Training loss: 2.8815504997259205
Validation loss: 2.612331549454897

Epoch: 5| Step: 7
Training loss: 2.507479921909018
Validation loss: 2.609658504708925

Epoch: 5| Step: 8
Training loss: 2.8410457286920834
Validation loss: 2.6097970991640373

Epoch: 5| Step: 9
Training loss: 3.5008620154290884
Validation loss: 2.6076162737345205

Epoch: 5| Step: 10
Training loss: 2.3219888199726815
Validation loss: 2.6104911326565703

Epoch: 74| Step: 0
Training loss: 2.7540170633733485
Validation loss: 2.6089356580458096

Epoch: 5| Step: 1
Training loss: 2.805800108328155
Validation loss: 2.6079666554859537

Epoch: 5| Step: 2
Training loss: 2.847335064727226
Validation loss: 2.609482213117296

Epoch: 5| Step: 3
Training loss: 3.563001898496059
Validation loss: 2.612472154112011

Epoch: 5| Step: 4
Training loss: 2.912135573741977
Validation loss: 2.608846020221466

Epoch: 5| Step: 5
Training loss: 2.9518468603714796
Validation loss: 2.6134301269244555

Epoch: 5| Step: 6
Training loss: 3.1319353583279965
Validation loss: 2.613290081968484

Epoch: 5| Step: 7
Training loss: 2.401017481971022
Validation loss: 2.610416168528942

Epoch: 5| Step: 8
Training loss: 2.868717627249175
Validation loss: 2.6101801499984565

Epoch: 5| Step: 9
Training loss: 2.714684527002399
Validation loss: 2.6101573124817308

Epoch: 5| Step: 10
Training loss: 3.443045426032758
Validation loss: 2.6112830942428813

Epoch: 75| Step: 0
Training loss: 2.799484569928773
Validation loss: 2.6091903071957776

Epoch: 5| Step: 1
Training loss: 3.397825830595358
Validation loss: 2.616799148596717

Epoch: 5| Step: 2
Training loss: 2.892192353791173
Validation loss: 2.6182064878480102

Epoch: 5| Step: 3
Training loss: 2.690252447088366
Validation loss: 2.6091102793473095

Epoch: 5| Step: 4
Training loss: 3.1957874644608912
Validation loss: 2.607450132273337

Epoch: 5| Step: 5
Training loss: 3.02219461922413
Validation loss: 2.6082909026618526

Epoch: 5| Step: 6
Training loss: 2.7105092526650707
Validation loss: 2.6106301503410756

Epoch: 5| Step: 7
Training loss: 3.2214449368853533
Validation loss: 2.608919418911331

Epoch: 5| Step: 8
Training loss: 2.6762955888253206
Validation loss: 2.609560795742672

Epoch: 5| Step: 9
Training loss: 2.6631643621565755
Validation loss: 2.6075486036266367

Epoch: 5| Step: 10
Training loss: 3.197435340734619
Validation loss: 2.6060765462063022

Testing loss: 2.830909486962286

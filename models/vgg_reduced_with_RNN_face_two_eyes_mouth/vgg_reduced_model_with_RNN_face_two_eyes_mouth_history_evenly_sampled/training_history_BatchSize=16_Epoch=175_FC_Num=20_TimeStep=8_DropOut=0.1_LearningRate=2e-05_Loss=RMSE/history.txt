Epoch: 1| Step: 0
Training loss: 5.640872516643452
Validation loss: 5.755966686093283

Epoch: 6| Step: 1
Training loss: 4.494010329896321
Validation loss: 5.73422766502329

Epoch: 6| Step: 2
Training loss: 6.448859109445372
Validation loss: 5.7144873297116625

Epoch: 6| Step: 3
Training loss: 6.5048821527594995
Validation loss: 5.69403472088008

Epoch: 6| Step: 4
Training loss: 6.263351827164098
Validation loss: 5.6710939894801236

Epoch: 6| Step: 5
Training loss: 7.145975124483455
Validation loss: 5.645241860958853

Epoch: 6| Step: 6
Training loss: 5.8047670176669905
Validation loss: 5.6156828764025235

Epoch: 6| Step: 7
Training loss: 5.480403320617984
Validation loss: 5.58185698866412

Epoch: 6| Step: 8
Training loss: 4.824867705275771
Validation loss: 5.542664222145451

Epoch: 6| Step: 9
Training loss: 5.272741562874781
Validation loss: 5.499561422840711

Epoch: 6| Step: 10
Training loss: 4.76446939434174
Validation loss: 5.450536925476601

Epoch: 6| Step: 11
Training loss: 5.6421085847603605
Validation loss: 5.397261904582569

Epoch: 6| Step: 12
Training loss: 5.240398892078322
Validation loss: 5.336887019117911

Epoch: 6| Step: 13
Training loss: 3.433717206833776
Validation loss: 5.2731859212316685

Epoch: 2| Step: 0
Training loss: 4.263187368686409
Validation loss: 5.205619681300991

Epoch: 6| Step: 1
Training loss: 4.202294022903638
Validation loss: 5.134847796394976

Epoch: 6| Step: 2
Training loss: 5.154730185580309
Validation loss: 5.066000026915486

Epoch: 6| Step: 3
Training loss: 4.987472098136239
Validation loss: 4.9940370709556365

Epoch: 6| Step: 4
Training loss: 5.46831018041662
Validation loss: 4.922580332105479

Epoch: 6| Step: 5
Training loss: 4.239683082900491
Validation loss: 4.850055525199388

Epoch: 6| Step: 6
Training loss: 5.365258904544399
Validation loss: 4.779635199656129

Epoch: 6| Step: 7
Training loss: 4.731684256898639
Validation loss: 4.710029062920347

Epoch: 6| Step: 8
Training loss: 5.327420274772549
Validation loss: 4.640515188011249

Epoch: 6| Step: 9
Training loss: 5.41584587725335
Validation loss: 4.5750319440051355

Epoch: 6| Step: 10
Training loss: 4.668677623346082
Validation loss: 4.516006816361745

Epoch: 6| Step: 11
Training loss: 4.423146418915393
Validation loss: 4.4651675043898305

Epoch: 6| Step: 12
Training loss: 4.343803570094579
Validation loss: 4.418452288306598

Epoch: 6| Step: 13
Training loss: 4.943737870115887
Validation loss: 4.367955810639929

Epoch: 3| Step: 0
Training loss: 5.001743584846169
Validation loss: 4.3145458379502175

Epoch: 6| Step: 1
Training loss: 3.4186913298710584
Validation loss: 4.2640960452153

Epoch: 6| Step: 2
Training loss: 4.791188401389307
Validation loss: 4.210493110855438

Epoch: 6| Step: 3
Training loss: 3.8870828582592187
Validation loss: 4.16602398417639

Epoch: 6| Step: 4
Training loss: 3.990345270424261
Validation loss: 4.127539031345306

Epoch: 6| Step: 5
Training loss: 3.422957854314322
Validation loss: 4.091557678237409

Epoch: 6| Step: 6
Training loss: 4.202513696397455
Validation loss: 4.060565434226188

Epoch: 6| Step: 7
Training loss: 4.250198359628808
Validation loss: 4.02754095105212

Epoch: 6| Step: 8
Training loss: 4.12848908799069
Validation loss: 3.9927677817825638

Epoch: 6| Step: 9
Training loss: 3.881540746441312
Validation loss: 3.960694322756363

Epoch: 6| Step: 10
Training loss: 3.9104153816872738
Validation loss: 3.942480071921804

Epoch: 6| Step: 11
Training loss: 4.531677646845798
Validation loss: 3.9039118860955853

Epoch: 6| Step: 12
Training loss: 5.156891753978927
Validation loss: 3.8731485112226753

Epoch: 6| Step: 13
Training loss: 3.4876693411017805
Validation loss: 3.8471713828995964

Epoch: 4| Step: 0
Training loss: 3.656954966342147
Validation loss: 3.82235805340552

Epoch: 6| Step: 1
Training loss: 4.247407009999601
Validation loss: 3.8027485592403534

Epoch: 6| Step: 2
Training loss: 3.6014099750783375
Validation loss: 3.782906611698877

Epoch: 6| Step: 3
Training loss: 3.9094222958613627
Validation loss: 3.767821265686414

Epoch: 6| Step: 4
Training loss: 3.8027413970145747
Validation loss: 3.755819463497269

Epoch: 6| Step: 5
Training loss: 4.145071499602438
Validation loss: 3.74062392553415

Epoch: 6| Step: 6
Training loss: 3.3433503954963246
Validation loss: 3.7234599811417017

Epoch: 6| Step: 7
Training loss: 4.024998275093041
Validation loss: 3.713386250154563

Epoch: 6| Step: 8
Training loss: 3.1707054951212346
Validation loss: 3.697859519830054

Epoch: 6| Step: 9
Training loss: 3.617804917440173
Validation loss: 3.684293498208195

Epoch: 6| Step: 10
Training loss: 3.932277061317385
Validation loss: 3.668541615333592

Epoch: 6| Step: 11
Training loss: 4.807388563684664
Validation loss: 3.652420225818445

Epoch: 6| Step: 12
Training loss: 3.270176977565528
Validation loss: 3.6358135056276137

Epoch: 6| Step: 13
Training loss: 5.21924264375724
Validation loss: 3.617885622428218

Epoch: 5| Step: 0
Training loss: 4.290824486525861
Validation loss: 3.6010455382748217

Epoch: 6| Step: 1
Training loss: 3.56489067840954
Validation loss: 3.5881737189807854

Epoch: 6| Step: 2
Training loss: 3.3877392279691763
Validation loss: 3.5773434356563256

Epoch: 6| Step: 3
Training loss: 3.2937238196358574
Validation loss: 3.569088982167276

Epoch: 6| Step: 4
Training loss: 3.9286435752287994
Validation loss: 3.5592519305606993

Epoch: 6| Step: 5
Training loss: 3.5402470305291422
Validation loss: 3.5394818022875496

Epoch: 6| Step: 6
Training loss: 4.552787371326574
Validation loss: 3.529045184035034

Epoch: 6| Step: 7
Training loss: 3.9154033381991757
Validation loss: 3.5155423908258445

Epoch: 6| Step: 8
Training loss: 3.7238329601746645
Validation loss: 3.5034023673509207

Epoch: 6| Step: 9
Training loss: 4.136188476695192
Validation loss: 3.495072605569484

Epoch: 6| Step: 10
Training loss: 3.7163496204397632
Validation loss: 3.487388662459867

Epoch: 6| Step: 11
Training loss: 3.9495997938391354
Validation loss: 3.480432234634778

Epoch: 6| Step: 12
Training loss: 3.159510976881883
Validation loss: 3.4678359160836756

Epoch: 6| Step: 13
Training loss: 1.7269729885778724
Validation loss: 3.4594907863501194

Epoch: 6| Step: 0
Training loss: 3.8976809086634168
Validation loss: 3.453994977698332

Epoch: 6| Step: 1
Training loss: 3.5019049228940036
Validation loss: 3.449863688727748

Epoch: 6| Step: 2
Training loss: 4.195826891402016
Validation loss: 3.4417321755456767

Epoch: 6| Step: 3
Training loss: 4.784417592861315
Validation loss: 3.432149587462209

Epoch: 6| Step: 4
Training loss: 2.9489429066985635
Validation loss: 3.4248324863220003

Epoch: 6| Step: 5
Training loss: 3.1149413924727445
Validation loss: 3.4214469959279477

Epoch: 6| Step: 6
Training loss: 3.1024675946383793
Validation loss: 3.419243311944838

Epoch: 6| Step: 7
Training loss: 3.5578435615855595
Validation loss: 3.423487221051934

Epoch: 6| Step: 8
Training loss: 2.6234144235928563
Validation loss: 3.4011684753951745

Epoch: 6| Step: 9
Training loss: 4.280608414517439
Validation loss: 3.4073402539285715

Epoch: 6| Step: 10
Training loss: 3.820445052579643
Validation loss: 3.413739221835922

Epoch: 6| Step: 11
Training loss: 3.2202814403296554
Validation loss: 3.4006421674071574

Epoch: 6| Step: 12
Training loss: 4.253115241539136
Validation loss: 3.391377666752716

Epoch: 6| Step: 13
Training loss: 2.385390890945591
Validation loss: 3.380457004248667

Epoch: 7| Step: 0
Training loss: 3.260356540958354
Validation loss: 3.3703421715701856

Epoch: 6| Step: 1
Training loss: 4.067936941732758
Validation loss: 3.365635403034815

Epoch: 6| Step: 2
Training loss: 3.750651875103875
Validation loss: 3.36499094318743

Epoch: 6| Step: 3
Training loss: 3.4895676038397045
Validation loss: 3.3617759898583284

Epoch: 6| Step: 4
Training loss: 2.2732631970472132
Validation loss: 3.3538911986972155

Epoch: 6| Step: 5
Training loss: 4.062638031008435
Validation loss: 3.3439482127106395

Epoch: 6| Step: 6
Training loss: 3.7248892236405573
Validation loss: 3.3369196539418424

Epoch: 6| Step: 7
Training loss: 3.6000585498287143
Validation loss: 3.3342943395686038

Epoch: 6| Step: 8
Training loss: 3.932589178220034
Validation loss: 3.32613314159972

Epoch: 6| Step: 9
Training loss: 4.316810333531979
Validation loss: 3.3213207789476695

Epoch: 6| Step: 10
Training loss: 3.1860176080096236
Validation loss: 3.316239617216677

Epoch: 6| Step: 11
Training loss: 3.092926994826213
Validation loss: 3.3056217865550437

Epoch: 6| Step: 12
Training loss: 3.4242971548390293
Validation loss: 3.2992466565430907

Epoch: 6| Step: 13
Training loss: 3.2289385971416875
Validation loss: 3.291734512230125

Epoch: 8| Step: 0
Training loss: 3.381925470993916
Validation loss: 3.2918251615232723

Epoch: 6| Step: 1
Training loss: 3.796349634314452
Validation loss: 3.2854401710335295

Epoch: 6| Step: 2
Training loss: 3.842992072084078
Validation loss: 3.277989195387641

Epoch: 6| Step: 3
Training loss: 3.1766423820963023
Validation loss: 3.275870097114974

Epoch: 6| Step: 4
Training loss: 4.247128133408715
Validation loss: 3.270384399839687

Epoch: 6| Step: 5
Training loss: 3.483258260864021
Validation loss: 3.2660245302133375

Epoch: 6| Step: 6
Training loss: 2.792132571234597
Validation loss: 3.2603389056209444

Epoch: 6| Step: 7
Training loss: 3.198119779384602
Validation loss: 3.2602213169675687

Epoch: 6| Step: 8
Training loss: 3.5208619848285645
Validation loss: 3.2522718205469956

Epoch: 6| Step: 9
Training loss: 3.3027036226669275
Validation loss: 3.247531447165291

Epoch: 6| Step: 10
Training loss: 3.4494879024640173
Validation loss: 3.2396850678114384

Epoch: 6| Step: 11
Training loss: 3.4349409808512217
Validation loss: 3.237907317719119

Epoch: 6| Step: 12
Training loss: 3.6574073981076296
Validation loss: 3.2331588747104254

Epoch: 6| Step: 13
Training loss: 3.7139283023994647
Validation loss: 3.229859079731976

Epoch: 9| Step: 0
Training loss: 3.416273575166494
Validation loss: 3.2376886408892114

Epoch: 6| Step: 1
Training loss: 2.657357198300204
Validation loss: 3.243340935979945

Epoch: 6| Step: 2
Training loss: 3.536363258638159
Validation loss: 3.215965308110281

Epoch: 6| Step: 3
Training loss: 3.015210375175733
Validation loss: 3.211175211821012

Epoch: 6| Step: 4
Training loss: 3.7178140953171486
Validation loss: 3.2181220234027514

Epoch: 6| Step: 5
Training loss: 4.222270033939321
Validation loss: 3.2203448118221654

Epoch: 6| Step: 6
Training loss: 3.7135075393257124
Validation loss: 3.201253307028323

Epoch: 6| Step: 7
Training loss: 3.578771008141054
Validation loss: 3.1896015311526895

Epoch: 6| Step: 8
Training loss: 3.549049551086101
Validation loss: 3.203293819465704

Epoch: 6| Step: 9
Training loss: 3.6332553388524462
Validation loss: 3.205261248033606

Epoch: 6| Step: 10
Training loss: 2.882310777129106
Validation loss: 3.195399779225848

Epoch: 6| Step: 11
Training loss: 3.307319909323103
Validation loss: 3.180120378154383

Epoch: 6| Step: 12
Training loss: 3.2328497934748546
Validation loss: 3.1638098320747647

Epoch: 6| Step: 13
Training loss: 3.8858246508119625
Validation loss: 3.1643922865596936

Epoch: 10| Step: 0
Training loss: 3.6609986596853954
Validation loss: 3.167866088406283

Epoch: 6| Step: 1
Training loss: 2.838430270654396
Validation loss: 3.165059853179169

Epoch: 6| Step: 2
Training loss: 3.498890837260353
Validation loss: 3.1622960904400483

Epoch: 6| Step: 3
Training loss: 3.5637592967703897
Validation loss: 3.1481874493314583

Epoch: 6| Step: 4
Training loss: 3.2858420074707
Validation loss: 3.140358734965575

Epoch: 6| Step: 5
Training loss: 3.2333982900275564
Validation loss: 3.142878110770662

Epoch: 6| Step: 6
Training loss: 3.15510372082056
Validation loss: 3.136017365959198

Epoch: 6| Step: 7
Training loss: 3.9278049092762326
Validation loss: 3.1330057581224753

Epoch: 6| Step: 8
Training loss: 3.8162999211534463
Validation loss: 3.130738179308361

Epoch: 6| Step: 9
Training loss: 3.0692848252286566
Validation loss: 3.1264175013994677

Epoch: 6| Step: 10
Training loss: 3.1698114607426215
Validation loss: 3.123041710031025

Epoch: 6| Step: 11
Training loss: 3.623837514809437
Validation loss: 3.1196202130653092

Epoch: 6| Step: 12
Training loss: 3.2911408080790947
Validation loss: 3.118966071922211

Epoch: 6| Step: 13
Training loss: 3.467138930088419
Validation loss: 3.118542395895604

Epoch: 11| Step: 0
Training loss: 3.328033875843178
Validation loss: 3.115037089434357

Epoch: 6| Step: 1
Training loss: 3.3470761877118447
Validation loss: 3.113238832002571

Epoch: 6| Step: 2
Training loss: 2.9468133881520706
Validation loss: 3.1121187159314867

Epoch: 6| Step: 3
Training loss: 3.5393535165944767
Validation loss: 3.1093968653300443

Epoch: 6| Step: 4
Training loss: 3.5731497077828287
Validation loss: 3.1078997722805703

Epoch: 6| Step: 5
Training loss: 3.47942298051613
Validation loss: 3.1058794823813916

Epoch: 6| Step: 6
Training loss: 3.6996946260370915
Validation loss: 3.103989263997098

Epoch: 6| Step: 7
Training loss: 3.6823443562336404
Validation loss: 3.1052884237899665

Epoch: 6| Step: 8
Training loss: 3.1247547816386385
Validation loss: 3.103847977594983

Epoch: 6| Step: 9
Training loss: 3.285294574088844
Validation loss: 3.1006094440334286

Epoch: 6| Step: 10
Training loss: 3.925989310601287
Validation loss: 3.098567112278824

Epoch: 6| Step: 11
Training loss: 3.0216787974778114
Validation loss: 3.096442431439277

Epoch: 6| Step: 12
Training loss: 3.38036259483846
Validation loss: 3.0939575089259987

Epoch: 6| Step: 13
Training loss: 2.3764367024511373
Validation loss: 3.088536362096228

Epoch: 12| Step: 0
Training loss: 3.0238743350646526
Validation loss: 3.0880144221873214

Epoch: 6| Step: 1
Training loss: 3.1959860541811675
Validation loss: 3.086516973031539

Epoch: 6| Step: 2
Training loss: 3.2860358803497265
Validation loss: 3.0857861187981923

Epoch: 6| Step: 3
Training loss: 2.99718979503809
Validation loss: 3.0801201723469758

Epoch: 6| Step: 4
Training loss: 3.964087443260113
Validation loss: 3.0772778403159164

Epoch: 6| Step: 5
Training loss: 3.5392876357664074
Validation loss: 3.0771566537400377

Epoch: 6| Step: 6
Training loss: 3.368682317106923
Validation loss: 3.0747050831137446

Epoch: 6| Step: 7
Training loss: 3.5906799471562705
Validation loss: 3.075876656941016

Epoch: 6| Step: 8
Training loss: 2.8306526143316244
Validation loss: 3.0728614319678917

Epoch: 6| Step: 9
Training loss: 3.5635658142169704
Validation loss: 3.0698865352718516

Epoch: 6| Step: 10
Training loss: 3.1908391777882352
Validation loss: 3.068671516831864

Epoch: 6| Step: 11
Training loss: 3.054310338768445
Validation loss: 3.0662204326372264

Epoch: 6| Step: 12
Training loss: 4.0562936178016455
Validation loss: 3.0648501019899275

Epoch: 6| Step: 13
Training loss: 2.9193241183714886
Validation loss: 3.064786451341534

Epoch: 13| Step: 0
Training loss: 3.3040228281225423
Validation loss: 3.0625150779368013

Epoch: 6| Step: 1
Training loss: 2.9543683699244347
Validation loss: 3.0604354499184585

Epoch: 6| Step: 2
Training loss: 3.145081655536962
Validation loss: 3.0606287918778134

Epoch: 6| Step: 3
Training loss: 3.949899677016632
Validation loss: 3.0591138578420214

Epoch: 6| Step: 4
Training loss: 3.307412325042313
Validation loss: 3.0557345628430768

Epoch: 6| Step: 5
Training loss: 3.2622373979488635
Validation loss: 3.055842902605195

Epoch: 6| Step: 6
Training loss: 3.3128765539952916
Validation loss: 3.0551918095431825

Epoch: 6| Step: 7
Training loss: 2.735177320210144
Validation loss: 3.052487451418489

Epoch: 6| Step: 8
Training loss: 4.0957606379337
Validation loss: 3.0506642361859

Epoch: 6| Step: 9
Training loss: 3.386184527051814
Validation loss: 3.052917595479595

Epoch: 6| Step: 10
Training loss: 3.9550125380364336
Validation loss: 3.0479050596068586

Epoch: 6| Step: 11
Training loss: 2.5973276489764814
Validation loss: 3.0506345832788253

Epoch: 6| Step: 12
Training loss: 3.0093371997966725
Validation loss: 3.048081084888663

Epoch: 6| Step: 13
Training loss: 3.465564536692889
Validation loss: 3.0464202440522397

Epoch: 14| Step: 0
Training loss: 3.512287505431893
Validation loss: 3.0463726949599668

Epoch: 6| Step: 1
Training loss: 3.7457461071502105
Validation loss: 3.0483490404443705

Epoch: 6| Step: 2
Training loss: 3.4613937869364397
Validation loss: 3.0509781564560234

Epoch: 6| Step: 3
Training loss: 3.552307539470713
Validation loss: 3.056476001564816

Epoch: 6| Step: 4
Training loss: 3.032053890011052
Validation loss: 3.060741983932629

Epoch: 6| Step: 5
Training loss: 3.5144325777262804
Validation loss: 3.0499289580513307

Epoch: 6| Step: 6
Training loss: 3.129937810310475
Validation loss: 3.038642499771297

Epoch: 6| Step: 7
Training loss: 3.7862715992102096
Validation loss: 3.1077090429011895

Epoch: 6| Step: 8
Training loss: 3.1832983710747427
Validation loss: 3.1095117794579683

Epoch: 6| Step: 9
Training loss: 3.033933729621071
Validation loss: 3.1031266577812593

Epoch: 6| Step: 10
Training loss: 3.731439561996681
Validation loss: 3.1106270388112502

Epoch: 6| Step: 11
Training loss: 3.233006431990885
Validation loss: 3.0682506500110382

Epoch: 6| Step: 12
Training loss: 2.8337520028247325
Validation loss: 3.0455272258863832

Epoch: 6| Step: 13
Training loss: 2.448362453262418
Validation loss: 3.048322170595081

Epoch: 15| Step: 0
Training loss: 3.4162792978660033
Validation loss: 3.07977055381509

Epoch: 6| Step: 1
Training loss: 2.2450411511156396
Validation loss: 3.0404986084164016

Epoch: 6| Step: 2
Training loss: 3.7210852719598204
Validation loss: 3.037091381994509

Epoch: 6| Step: 3
Training loss: 3.3405456876127038
Validation loss: 3.02849760138863

Epoch: 6| Step: 4
Training loss: 3.421291911293913
Validation loss: 3.0328608365182634

Epoch: 6| Step: 5
Training loss: 3.447729530134849
Validation loss: 3.037541828401765

Epoch: 6| Step: 6
Training loss: 3.9163238023609543
Validation loss: 3.0489316920890777

Epoch: 6| Step: 7
Training loss: 2.8872083112022575
Validation loss: 3.0435883057721433

Epoch: 6| Step: 8
Training loss: 3.425844163654059
Validation loss: 3.1218816001620553

Epoch: 6| Step: 9
Training loss: 3.577209488797882
Validation loss: 3.116452471567549

Epoch: 6| Step: 10
Training loss: 3.8725003209369446
Validation loss: 3.023070553692058

Epoch: 6| Step: 11
Training loss: 3.556742114190725
Validation loss: 3.046028245462502

Epoch: 6| Step: 12
Training loss: 2.151727771713804
Validation loss: 3.0556578808900707

Epoch: 6| Step: 13
Training loss: 3.2151664859267495
Validation loss: 3.083188580410745

Epoch: 16| Step: 0
Training loss: 3.803797059559369
Validation loss: 3.0859970276027497

Epoch: 6| Step: 1
Training loss: 3.4887506355003506
Validation loss: 3.055924724105531

Epoch: 6| Step: 2
Training loss: 3.109112138995099
Validation loss: 3.0351602874178716

Epoch: 6| Step: 3
Training loss: 3.372778267207332
Validation loss: 3.0322182904617225

Epoch: 6| Step: 4
Training loss: 2.8366859646360028
Validation loss: 3.0307172402870286

Epoch: 6| Step: 5
Training loss: 3.5066677341048593
Validation loss: 3.024728715366861

Epoch: 6| Step: 6
Training loss: 2.9849332117631246
Validation loss: 3.0209794571739614

Epoch: 6| Step: 7
Training loss: 2.9766057529460768
Validation loss: 3.0202031486365484

Epoch: 6| Step: 8
Training loss: 3.3424374865418844
Validation loss: 3.0188918010575394

Epoch: 6| Step: 9
Training loss: 3.407270366182672
Validation loss: 3.0273291565711715

Epoch: 6| Step: 10
Training loss: 3.6622199202065295
Validation loss: 3.0201013483732386

Epoch: 6| Step: 11
Training loss: 3.1319050604314547
Validation loss: 3.0092252484308473

Epoch: 6| Step: 12
Training loss: 3.4540147282430516
Validation loss: 3.012535420633646

Epoch: 6| Step: 13
Training loss: 3.2178110771698414
Validation loss: 3.0166007041574723

Epoch: 17| Step: 0
Training loss: 3.924967363549873
Validation loss: 3.0189508588905034

Epoch: 6| Step: 1
Training loss: 3.341343379059121
Validation loss: 3.0025555952581757

Epoch: 6| Step: 2
Training loss: 3.7968039172390657
Validation loss: 3.000679586886759

Epoch: 6| Step: 3
Training loss: 3.5451072287113576
Validation loss: 2.9951116641140225

Epoch: 6| Step: 4
Training loss: 2.8962986707158214
Validation loss: 2.9922948386586183

Epoch: 6| Step: 5
Training loss: 3.5816836588867567
Validation loss: 2.991294830282385

Epoch: 6| Step: 6
Training loss: 2.771138186403654
Validation loss: 2.9862976564132295

Epoch: 6| Step: 7
Training loss: 3.2403385635374056
Validation loss: 2.9807500374132125

Epoch: 6| Step: 8
Training loss: 2.914491841695993
Validation loss: 2.9773872500456235

Epoch: 6| Step: 9
Training loss: 2.3733736794324867
Validation loss: 2.9745123180422093

Epoch: 6| Step: 10
Training loss: 2.7632991687731856
Validation loss: 2.9752720690580934

Epoch: 6| Step: 11
Training loss: 3.6208166779785116
Validation loss: 2.9700213505246245

Epoch: 6| Step: 12
Training loss: 3.158023147314568
Validation loss: 2.9731802215076892

Epoch: 6| Step: 13
Training loss: 4.033007098368804
Validation loss: 2.969785679555693

Epoch: 18| Step: 0
Training loss: 3.020438508287933
Validation loss: 2.968103447430577

Epoch: 6| Step: 1
Training loss: 3.7780471983062087
Validation loss: 2.9670787802831042

Epoch: 6| Step: 2
Training loss: 3.002756600702755
Validation loss: 2.96236952918447

Epoch: 6| Step: 3
Training loss: 2.9725237502295436
Validation loss: 2.956667945349991

Epoch: 6| Step: 4
Training loss: 2.8600191318432255
Validation loss: 2.9531216438805235

Epoch: 6| Step: 5
Training loss: 3.21487905082001
Validation loss: 2.948999995850939

Epoch: 6| Step: 6
Training loss: 2.8422336098578125
Validation loss: 2.943253187041767

Epoch: 6| Step: 7
Training loss: 3.088985897922403
Validation loss: 2.940406131514143

Epoch: 6| Step: 8
Training loss: 3.3751231983446037
Validation loss: 2.937959710539835

Epoch: 6| Step: 9
Training loss: 2.887526547528716
Validation loss: 2.9366856137537996

Epoch: 6| Step: 10
Training loss: 3.8394310730446306
Validation loss: 2.935181173128879

Epoch: 6| Step: 11
Training loss: 3.189626993835211
Validation loss: 2.9327531091332384

Epoch: 6| Step: 12
Training loss: 3.704338036010507
Validation loss: 2.929030724853422

Epoch: 6| Step: 13
Training loss: 3.7292886355324946
Validation loss: 2.9257360049320673

Epoch: 19| Step: 0
Training loss: 2.9784823256335313
Validation loss: 2.9645454636823554

Epoch: 6| Step: 1
Training loss: 2.504694631546158
Validation loss: 3.013810489360007

Epoch: 6| Step: 2
Training loss: 3.32613589165981
Validation loss: 3.0441255939643352

Epoch: 6| Step: 3
Training loss: 2.948664450186715
Validation loss: 3.0517304568666406

Epoch: 6| Step: 4
Training loss: 2.6055500069319426
Validation loss: 3.05890942022162

Epoch: 6| Step: 5
Training loss: 3.398290731834357
Validation loss: 3.1041185620722263

Epoch: 6| Step: 6
Training loss: 3.955977906908881
Validation loss: 3.0435614040008643

Epoch: 6| Step: 7
Training loss: 3.7082252629566717
Validation loss: 2.9925098231125578

Epoch: 6| Step: 8
Training loss: 3.3378329107429963
Validation loss: 2.987981330093541

Epoch: 6| Step: 9
Training loss: 3.279560780541968
Validation loss: 2.995140852435907

Epoch: 6| Step: 10
Training loss: 3.1434968941274097
Validation loss: 3.0023012666526454

Epoch: 6| Step: 11
Training loss: 3.254068322513067
Validation loss: 3.0126530610393565

Epoch: 6| Step: 12
Training loss: 3.9461272654208024
Validation loss: 3.0030626435538363

Epoch: 6| Step: 13
Training loss: 3.742848889353231
Validation loss: 2.9843885020606846

Epoch: 20| Step: 0
Training loss: 3.10863298159854
Validation loss: 2.967125027048288

Epoch: 6| Step: 1
Training loss: 2.547047708658376
Validation loss: 2.9525466928256736

Epoch: 6| Step: 2
Training loss: 3.5166787157861714
Validation loss: 2.9367536261171714

Epoch: 6| Step: 3
Training loss: 2.707946886710956
Validation loss: 2.930355405741768

Epoch: 6| Step: 4
Training loss: 3.750373948843575
Validation loss: 2.9405685609266863

Epoch: 6| Step: 5
Training loss: 3.8008221489802323
Validation loss: 2.9492043445726743

Epoch: 6| Step: 6
Training loss: 3.156880381867891
Validation loss: 2.9553956040586833

Epoch: 6| Step: 7
Training loss: 3.5333297141674387
Validation loss: 2.92951983622219

Epoch: 6| Step: 8
Training loss: 3.2780149952311017
Validation loss: 2.9113742307639616

Epoch: 6| Step: 9
Training loss: 2.6510299552453946
Validation loss: 2.907987008696403

Epoch: 6| Step: 10
Training loss: 3.6403750878212118
Validation loss: 2.9085491523336766

Epoch: 6| Step: 11
Training loss: 3.1037672839872217
Validation loss: 2.9061291215545526

Epoch: 6| Step: 12
Training loss: 3.394238212412697
Validation loss: 2.9041197775672156

Epoch: 6| Step: 13
Training loss: 2.3131384097197367
Validation loss: 2.903727473733541

Epoch: 21| Step: 0
Training loss: 3.610914818408995
Validation loss: 2.9025844251759345

Epoch: 6| Step: 1
Training loss: 2.7236392902284927
Validation loss: 2.9079649883579184

Epoch: 6| Step: 2
Training loss: 2.945798150859198
Validation loss: 2.9122392800645565

Epoch: 6| Step: 3
Training loss: 3.217245546607459
Validation loss: 2.907303314803496

Epoch: 6| Step: 4
Training loss: 3.448558981881225
Validation loss: 2.899683831277996

Epoch: 6| Step: 5
Training loss: 3.0077828542065803
Validation loss: 2.8975424590124863

Epoch: 6| Step: 6
Training loss: 3.308260814214648
Validation loss: 2.895424887434591

Epoch: 6| Step: 7
Training loss: 3.480154997344514
Validation loss: 2.8924139383268823

Epoch: 6| Step: 8
Training loss: 3.9343423443591163
Validation loss: 2.8942726044826754

Epoch: 6| Step: 9
Training loss: 3.1333529417628343
Validation loss: 2.8940609719341133

Epoch: 6| Step: 10
Training loss: 2.3815410285985914
Validation loss: 2.895893147357584

Epoch: 6| Step: 11
Training loss: 3.2891368721642715
Validation loss: 2.9104622862138645

Epoch: 6| Step: 12
Training loss: 3.2421832348898905
Validation loss: 2.9152401355197557

Epoch: 6| Step: 13
Training loss: 2.5590998724126464
Validation loss: 2.923103627403047

Epoch: 22| Step: 0
Training loss: 2.7979470867473597
Validation loss: 2.9230569130667354

Epoch: 6| Step: 1
Training loss: 2.9918891618567587
Validation loss: 2.9036901250538705

Epoch: 6| Step: 2
Training loss: 2.717029070718672
Validation loss: 2.888089685769512

Epoch: 6| Step: 3
Training loss: 2.636918214388258
Validation loss: 2.886200615581143

Epoch: 6| Step: 4
Training loss: 3.2260803896967807
Validation loss: 2.8905225439075912

Epoch: 6| Step: 5
Training loss: 3.197229085723937
Validation loss: 2.887042282051065

Epoch: 6| Step: 6
Training loss: 3.476483496293345
Validation loss: 2.8912834031816863

Epoch: 6| Step: 7
Training loss: 3.4617734291340043
Validation loss: 2.8885958080247707

Epoch: 6| Step: 8
Training loss: 2.8680579919525053
Validation loss: 2.8828222127495637

Epoch: 6| Step: 9
Training loss: 3.2328233913203137
Validation loss: 2.8873890296150955

Epoch: 6| Step: 10
Training loss: 3.484069263404812
Validation loss: 2.8801802022420926

Epoch: 6| Step: 11
Training loss: 3.0988317811060306
Validation loss: 2.880686575029868

Epoch: 6| Step: 12
Training loss: 3.736833983823238
Validation loss: 2.876107962305316

Epoch: 6| Step: 13
Training loss: 3.8086547220680367
Validation loss: 2.877201435930244

Epoch: 23| Step: 0
Training loss: 2.9236637290957517
Validation loss: 2.873701773531897

Epoch: 6| Step: 1
Training loss: 2.3000192724332664
Validation loss: 2.871716591950823

Epoch: 6| Step: 2
Training loss: 2.8526657687538886
Validation loss: 2.874409838396969

Epoch: 6| Step: 3
Training loss: 3.051735468668203
Validation loss: 2.872372103489615

Epoch: 6| Step: 4
Training loss: 3.1635654388696848
Validation loss: 2.8720065945650055

Epoch: 6| Step: 5
Training loss: 3.481740460183682
Validation loss: 2.8681663578451664

Epoch: 6| Step: 6
Training loss: 3.4534820786283555
Validation loss: 2.870611691181805

Epoch: 6| Step: 7
Training loss: 3.113428129132098
Validation loss: 2.8681859003824206

Epoch: 6| Step: 8
Training loss: 3.289433619205022
Validation loss: 2.8669762100023073

Epoch: 6| Step: 9
Training loss: 3.77757791071488
Validation loss: 2.866798259338502

Epoch: 6| Step: 10
Training loss: 3.7936398887680225
Validation loss: 2.8696625463502623

Epoch: 6| Step: 11
Training loss: 2.5484360667399497
Validation loss: 2.8715438953091876

Epoch: 6| Step: 12
Training loss: 3.1366929068634968
Validation loss: 2.8736476026867024

Epoch: 6| Step: 13
Training loss: 3.3641333658042574
Validation loss: 2.8662355764071106

Epoch: 24| Step: 0
Training loss: 3.251202874228942
Validation loss: 2.8721438528598258

Epoch: 6| Step: 1
Training loss: 2.7237111569664014
Validation loss: 2.868025539309023

Epoch: 6| Step: 2
Training loss: 3.2117826737055597
Validation loss: 2.876399364425193

Epoch: 6| Step: 3
Training loss: 3.536496206688461
Validation loss: 2.8669504149943665

Epoch: 6| Step: 4
Training loss: 3.5803136004119698
Validation loss: 2.8601079627066173

Epoch: 6| Step: 5
Training loss: 3.6499945444562716
Validation loss: 2.8630977599185634

Epoch: 6| Step: 6
Training loss: 3.323146911744001
Validation loss: 2.869628448397792

Epoch: 6| Step: 7
Training loss: 2.5445288400752806
Validation loss: 2.8781977735257973

Epoch: 6| Step: 8
Training loss: 3.8870148972445895
Validation loss: 2.896508927209029

Epoch: 6| Step: 9
Training loss: 3.159709733838192
Validation loss: 2.8926345504950874

Epoch: 6| Step: 10
Training loss: 2.5884830358198787
Validation loss: 2.8633880633363673

Epoch: 6| Step: 11
Training loss: 2.9465740547758714
Validation loss: 2.8677290864605367

Epoch: 6| Step: 12
Training loss: 3.10208855691081
Validation loss: 2.890750153949311

Epoch: 6| Step: 13
Training loss: 2.5903765330325514
Validation loss: 2.940095823201228

Epoch: 25| Step: 0
Training loss: 3.3754039098855424
Validation loss: 3.0323713245896315

Epoch: 6| Step: 1
Training loss: 3.9230873696085995
Validation loss: 2.8869963241937873

Epoch: 6| Step: 2
Training loss: 3.061126556711371
Validation loss: 2.870219593128831

Epoch: 6| Step: 3
Training loss: 1.7832491180404906
Validation loss: 2.8652565823679366

Epoch: 6| Step: 4
Training loss: 3.0963795593777617
Validation loss: 2.8910002274494375

Epoch: 6| Step: 5
Training loss: 3.779477799690652
Validation loss: 2.938312190453962

Epoch: 6| Step: 6
Training loss: 3.2789931028804853
Validation loss: 2.962631810547251

Epoch: 6| Step: 7
Training loss: 3.6115853283543466
Validation loss: 2.9644139487525183

Epoch: 6| Step: 8
Training loss: 3.340007934104039
Validation loss: 2.899544711400823

Epoch: 6| Step: 9
Training loss: 3.324413540396748
Validation loss: 2.8697278912888797

Epoch: 6| Step: 10
Training loss: 3.024693270178335
Validation loss: 2.853740203688105

Epoch: 6| Step: 11
Training loss: 3.46083554360968
Validation loss: 2.8532180203632347

Epoch: 6| Step: 12
Training loss: 2.997711421328605
Validation loss: 2.8490632031458523

Epoch: 6| Step: 13
Training loss: 2.0663281373690285
Validation loss: 2.847190964876789

Epoch: 26| Step: 0
Training loss: 3.5448887847112607
Validation loss: 2.8464378060406905

Epoch: 6| Step: 1
Training loss: 3.1767549605639673
Validation loss: 2.846534161216373

Epoch: 6| Step: 2
Training loss: 2.861292128613168
Validation loss: 2.84472730868566

Epoch: 6| Step: 3
Training loss: 3.871520541440457
Validation loss: 2.849814420413874

Epoch: 6| Step: 4
Training loss: 3.077501572733275
Validation loss: 2.85142361354258

Epoch: 6| Step: 5
Training loss: 3.109130389689428
Validation loss: 2.8562098741399895

Epoch: 6| Step: 6
Training loss: 3.236106224947739
Validation loss: 2.8583308011787634

Epoch: 6| Step: 7
Training loss: 2.9850801763198285
Validation loss: 2.86224910970849

Epoch: 6| Step: 8
Training loss: 3.0247824976889155
Validation loss: 2.8545557602244496

Epoch: 6| Step: 9
Training loss: 2.921867819384559
Validation loss: 2.8413144707096682

Epoch: 6| Step: 10
Training loss: 3.064572469941435
Validation loss: 2.8376221839062543

Epoch: 6| Step: 11
Training loss: 3.22544268741287
Validation loss: 2.8386264465863316

Epoch: 6| Step: 12
Training loss: 3.1161253991217617
Validation loss: 2.8364061559159324

Epoch: 6| Step: 13
Training loss: 2.7190197339479965
Validation loss: 2.836346423602103

Epoch: 27| Step: 0
Training loss: 3.374455302046577
Validation loss: 2.836756835430235

Epoch: 6| Step: 1
Training loss: 2.2360014431152053
Validation loss: 2.846229109069478

Epoch: 6| Step: 2
Training loss: 2.727795253895364
Validation loss: 2.8674049533043235

Epoch: 6| Step: 3
Training loss: 2.9739992013095176
Validation loss: 2.882402779267544

Epoch: 6| Step: 4
Training loss: 2.756303498663673
Validation loss: 2.847080697571513

Epoch: 6| Step: 5
Training loss: 3.8060931795133044
Validation loss: 2.8386214974451907

Epoch: 6| Step: 6
Training loss: 2.946404130842559
Validation loss: 2.8322664163649796

Epoch: 6| Step: 7
Training loss: 3.2328735404799476
Validation loss: 2.8344607061706

Epoch: 6| Step: 8
Training loss: 3.394506526808702
Validation loss: 2.8335971299289398

Epoch: 6| Step: 9
Training loss: 3.495615529007122
Validation loss: 2.8362804921719937

Epoch: 6| Step: 10
Training loss: 3.0491878241129635
Validation loss: 2.8388640439792208

Epoch: 6| Step: 11
Training loss: 2.8902660250074734
Validation loss: 2.8335592821015965

Epoch: 6| Step: 12
Training loss: 3.814627632176956
Validation loss: 2.8331734312371246

Epoch: 6| Step: 13
Training loss: 3.1382185033480154
Validation loss: 2.832231970474904

Epoch: 28| Step: 0
Training loss: 3.0237319369208278
Validation loss: 2.8320229236790144

Epoch: 6| Step: 1
Training loss: 3.3587680002119895
Validation loss: 2.83157271370749

Epoch: 6| Step: 2
Training loss: 3.118964049919451
Validation loss: 2.832737717706652

Epoch: 6| Step: 3
Training loss: 3.232981948504803
Validation loss: 2.82867168734982

Epoch: 6| Step: 4
Training loss: 2.5411342209065046
Validation loss: 2.8312304523911207

Epoch: 6| Step: 5
Training loss: 3.3249006098756935
Validation loss: 2.834975001058952

Epoch: 6| Step: 6
Training loss: 3.0070086309108044
Validation loss: 2.8369240950520553

Epoch: 6| Step: 7
Training loss: 3.490099254825386
Validation loss: 2.8427543407138387

Epoch: 6| Step: 8
Training loss: 3.1521747832175544
Validation loss: 2.8328225560914757

Epoch: 6| Step: 9
Training loss: 3.6675268522500413
Validation loss: 2.8303001830367704

Epoch: 6| Step: 10
Training loss: 3.0763471862247393
Validation loss: 2.8221635558953535

Epoch: 6| Step: 11
Training loss: 2.0415465909979456
Validation loss: 2.8203300216694553

Epoch: 6| Step: 12
Training loss: 3.597212740289538
Validation loss: 2.8189829407847125

Epoch: 6| Step: 13
Training loss: 3.2314286379990613
Validation loss: 2.820683542363825

Epoch: 29| Step: 0
Training loss: 3.1413158966329644
Validation loss: 2.8201556600301894

Epoch: 6| Step: 1
Training loss: 2.956091949560739
Validation loss: 2.8209680577032903

Epoch: 6| Step: 2
Training loss: 3.446658470567099
Validation loss: 2.821015066706993

Epoch: 6| Step: 3
Training loss: 2.9962059507287093
Validation loss: 2.821163681362646

Epoch: 6| Step: 4
Training loss: 3.3492397983068485
Validation loss: 2.8202998413532683

Epoch: 6| Step: 5
Training loss: 3.842670568230806
Validation loss: 2.82068457666011

Epoch: 6| Step: 6
Training loss: 3.4329236171902227
Validation loss: 2.8219105791240024

Epoch: 6| Step: 7
Training loss: 2.6360442930792503
Validation loss: 2.8195009036146086

Epoch: 6| Step: 8
Training loss: 3.4031003336367176
Validation loss: 2.8180520407576615

Epoch: 6| Step: 9
Training loss: 2.836246862430779
Validation loss: 2.8164989012504646

Epoch: 6| Step: 10
Training loss: 3.0070340028444464
Validation loss: 2.8151683332011883

Epoch: 6| Step: 11
Training loss: 2.5335428661561816
Validation loss: 2.8138706982659847

Epoch: 6| Step: 12
Training loss: 3.1748943521539212
Validation loss: 2.813799959874095

Epoch: 6| Step: 13
Training loss: 2.885950876813205
Validation loss: 2.8127465380465306

Epoch: 30| Step: 0
Training loss: 3.3259006284365102
Validation loss: 2.810122898454716

Epoch: 6| Step: 1
Training loss: 2.8257232077883514
Validation loss: 2.811032147131887

Epoch: 6| Step: 2
Training loss: 1.9158445198271208
Validation loss: 2.808654457723327

Epoch: 6| Step: 3
Training loss: 3.7282754412064207
Validation loss: 2.8110782671334613

Epoch: 6| Step: 4
Training loss: 3.7804075201049963
Validation loss: 2.8081741353854994

Epoch: 6| Step: 5
Training loss: 2.4830314794135795
Validation loss: 2.8086020289425466

Epoch: 6| Step: 6
Training loss: 2.629186924182779
Validation loss: 2.808642147238878

Epoch: 6| Step: 7
Training loss: 3.5040986721655627
Validation loss: 2.8067451264807293

Epoch: 6| Step: 8
Training loss: 2.5667156805172318
Validation loss: 2.8043720717996585

Epoch: 6| Step: 9
Training loss: 3.330996997952146
Validation loss: 2.8011463277631985

Epoch: 6| Step: 10
Training loss: 2.8794787806825175
Validation loss: 2.7999350464446144

Epoch: 6| Step: 11
Training loss: 3.1500670047475836
Validation loss: 2.8022293404714147

Epoch: 6| Step: 12
Training loss: 3.713604227888206
Validation loss: 2.80263184430665

Epoch: 6| Step: 13
Training loss: 3.5095758232885665
Validation loss: 2.8079005333808382

Epoch: 31| Step: 0
Training loss: 3.437015291199397
Validation loss: 2.820467573355561

Epoch: 6| Step: 1
Training loss: 3.480732883145243
Validation loss: 2.8311042836340268

Epoch: 6| Step: 2
Training loss: 3.181554297861996
Validation loss: 2.8127106199167407

Epoch: 6| Step: 3
Training loss: 2.2027295346113025
Validation loss: 2.799049103945434

Epoch: 6| Step: 4
Training loss: 2.4717788465392148
Validation loss: 2.7927629099671076

Epoch: 6| Step: 5
Training loss: 3.5189411714432812
Validation loss: 2.791990220689276

Epoch: 6| Step: 6
Training loss: 3.7349288721218734
Validation loss: 2.7905367929607996

Epoch: 6| Step: 7
Training loss: 2.910626633283023
Validation loss: 2.7907652252794017

Epoch: 6| Step: 8
Training loss: 2.9133358783259795
Validation loss: 2.790724069157194

Epoch: 6| Step: 9
Training loss: 3.6001451727948073
Validation loss: 2.790429822221995

Epoch: 6| Step: 10
Training loss: 2.6911479233564517
Validation loss: 2.7901554348644115

Epoch: 6| Step: 11
Training loss: 2.7039909078400015
Validation loss: 2.7883516893041977

Epoch: 6| Step: 12
Training loss: 3.346487618047702
Validation loss: 2.7880779865857597

Epoch: 6| Step: 13
Training loss: 3.0305834165093337
Validation loss: 2.787275900786494

Epoch: 32| Step: 0
Training loss: 3.2186212606516853
Validation loss: 2.785602753262944

Epoch: 6| Step: 1
Training loss: 3.0407926444623303
Validation loss: 2.785955909389955

Epoch: 6| Step: 2
Training loss: 2.849994886962587
Validation loss: 2.7844180583887335

Epoch: 6| Step: 3
Training loss: 3.4762868021827935
Validation loss: 2.7866837405851275

Epoch: 6| Step: 4
Training loss: 2.6954939186849827
Validation loss: 2.7869298570290644

Epoch: 6| Step: 5
Training loss: 3.222552229618572
Validation loss: 2.7823555825079684

Epoch: 6| Step: 6
Training loss: 2.741828221099665
Validation loss: 2.780611891143369

Epoch: 6| Step: 7
Training loss: 3.2309972845011203
Validation loss: 2.7818405374290114

Epoch: 6| Step: 8
Training loss: 2.538983998185493
Validation loss: 2.7804039275100907

Epoch: 6| Step: 9
Training loss: 2.9814032493245537
Validation loss: 2.7797403232940754

Epoch: 6| Step: 10
Training loss: 3.7704656994297365
Validation loss: 2.779053929341628

Epoch: 6| Step: 11
Training loss: 3.4861902546268295
Validation loss: 2.7795829611583875

Epoch: 6| Step: 12
Training loss: 2.8515611622428367
Validation loss: 2.7801026294355666

Epoch: 6| Step: 13
Training loss: 3.3487937178824856
Validation loss: 2.7795303889199667

Epoch: 33| Step: 0
Training loss: 3.0485672383976135
Validation loss: 2.7774045712646136

Epoch: 6| Step: 1
Training loss: 3.0816422713782203
Validation loss: 2.777942801533404

Epoch: 6| Step: 2
Training loss: 2.749646771026686
Validation loss: 2.7756489675651896

Epoch: 6| Step: 3
Training loss: 2.3158794144910435
Validation loss: 2.778710595436546

Epoch: 6| Step: 4
Training loss: 3.113593379016984
Validation loss: 2.77624298001735

Epoch: 6| Step: 5
Training loss: 3.8118695847897675
Validation loss: 2.7773873030788403

Epoch: 6| Step: 6
Training loss: 2.7230493160549325
Validation loss: 2.7864445257127506

Epoch: 6| Step: 7
Training loss: 2.438931362773823
Validation loss: 2.7895822660866605

Epoch: 6| Step: 8
Training loss: 2.490480035370609
Validation loss: 2.8075660807205716

Epoch: 6| Step: 9
Training loss: 3.243972470807578
Validation loss: 2.8184695007386313

Epoch: 6| Step: 10
Training loss: 4.048154415125635
Validation loss: 2.8380160160086403

Epoch: 6| Step: 11
Training loss: 3.216417523026835
Validation loss: 2.8090158719476985

Epoch: 6| Step: 12
Training loss: 3.167176473303835
Validation loss: 2.790532980391142

Epoch: 6| Step: 13
Training loss: 4.0872336204593065
Validation loss: 2.7725926024076095

Epoch: 34| Step: 0
Training loss: 2.9662677225193543
Validation loss: 2.768897403963697

Epoch: 6| Step: 1
Training loss: 3.4267327652084036
Validation loss: 2.7671657252129407

Epoch: 6| Step: 2
Training loss: 3.330668592262577
Validation loss: 2.7681397455913817

Epoch: 6| Step: 3
Training loss: 3.197445481635649
Validation loss: 2.7668789620410266

Epoch: 6| Step: 4
Training loss: 2.9932145629406195
Validation loss: 2.7670276001172107

Epoch: 6| Step: 5
Training loss: 3.0339463030313727
Validation loss: 2.7698105339855252

Epoch: 6| Step: 6
Training loss: 3.4466489245648595
Validation loss: 2.7687348348354597

Epoch: 6| Step: 7
Training loss: 2.704591915352271
Validation loss: 2.7690259719963985

Epoch: 6| Step: 8
Training loss: 3.133421118162632
Validation loss: 2.7673495128892207

Epoch: 6| Step: 9
Training loss: 3.3012319403850965
Validation loss: 2.766804014573092

Epoch: 6| Step: 10
Training loss: 3.096321963481525
Validation loss: 2.767254083969248

Epoch: 6| Step: 11
Training loss: 2.944520393527678
Validation loss: 2.766280430123619

Epoch: 6| Step: 12
Training loss: 3.0239063461272946
Validation loss: 2.7637498812767443

Epoch: 6| Step: 13
Training loss: 2.287013676544515
Validation loss: 2.762104208786296

Epoch: 35| Step: 0
Training loss: 3.142798435604157
Validation loss: 2.762232765387157

Epoch: 6| Step: 1
Training loss: 3.4206713645465148
Validation loss: 2.763568090790856

Epoch: 6| Step: 2
Training loss: 2.4736580649742415
Validation loss: 2.761107604904074

Epoch: 6| Step: 3
Training loss: 3.0431506747707893
Validation loss: 2.760572134149256

Epoch: 6| Step: 4
Training loss: 3.213721486176182
Validation loss: 2.769301066147506

Epoch: 6| Step: 5
Training loss: 2.1360422350104726
Validation loss: 2.7713408676500477

Epoch: 6| Step: 6
Training loss: 2.644792957964163
Validation loss: 2.7704379934402206

Epoch: 6| Step: 7
Training loss: 3.464445863260867
Validation loss: 2.779228894677381

Epoch: 6| Step: 8
Training loss: 2.29679403356975
Validation loss: 2.771345326407925

Epoch: 6| Step: 9
Training loss: 3.6409103871093076
Validation loss: 2.7705126815947954

Epoch: 6| Step: 10
Training loss: 3.315618234927712
Validation loss: 2.757291247056289

Epoch: 6| Step: 11
Training loss: 2.9778466846154448
Validation loss: 2.7560267574014463

Epoch: 6| Step: 12
Training loss: 3.622581727637499
Validation loss: 2.7557236852647624

Epoch: 6| Step: 13
Training loss: 3.5915905642726136
Validation loss: 2.75455705651216

Epoch: 36| Step: 0
Training loss: 3.435546319819455
Validation loss: 2.7536810777137792

Epoch: 6| Step: 1
Training loss: 2.541160116111886
Validation loss: 2.7576120540729794

Epoch: 6| Step: 2
Training loss: 3.125955969502226
Validation loss: 2.7661849884182548

Epoch: 6| Step: 3
Training loss: 3.254310097425425
Validation loss: 2.7535551197717476

Epoch: 6| Step: 4
Training loss: 3.201878300400209
Validation loss: 2.7502061397446735

Epoch: 6| Step: 5
Training loss: 3.251048579153143
Validation loss: 2.7517945129574315

Epoch: 6| Step: 6
Training loss: 2.577112449865964
Validation loss: 2.7490784209309225

Epoch: 6| Step: 7
Training loss: 2.7083546759913766
Validation loss: 2.7480499013114206

Epoch: 6| Step: 8
Training loss: 2.9411481564224786
Validation loss: 2.748018234958103

Epoch: 6| Step: 9
Training loss: 2.741574123718395
Validation loss: 2.751453640059845

Epoch: 6| Step: 10
Training loss: 3.6423418311008926
Validation loss: 2.7568833956283263

Epoch: 6| Step: 11
Training loss: 3.0646054562501233
Validation loss: 2.7650443226960273

Epoch: 6| Step: 12
Training loss: 3.0827756497655012
Validation loss: 2.7704390215102315

Epoch: 6| Step: 13
Training loss: 3.715574303001226
Validation loss: 2.78016696585748

Epoch: 37| Step: 0
Training loss: 3.0938565014319326
Validation loss: 2.759121451007121

Epoch: 6| Step: 1
Training loss: 2.8842204307726087
Validation loss: 2.7487061571449933

Epoch: 6| Step: 2
Training loss: 3.3247669453796105
Validation loss: 2.744886842923626

Epoch: 6| Step: 3
Training loss: 2.9128317189715704
Validation loss: 2.7474033797478836

Epoch: 6| Step: 4
Training loss: 3.1473353423300923
Validation loss: 2.7501158759798736

Epoch: 6| Step: 5
Training loss: 2.801329831675696
Validation loss: 2.7453981155986584

Epoch: 6| Step: 6
Training loss: 2.967032327155082
Validation loss: 2.7471457722120283

Epoch: 6| Step: 7
Training loss: 3.207547591132116
Validation loss: 2.74249969959234

Epoch: 6| Step: 8
Training loss: 2.7551699505291554
Validation loss: 2.7409358019698105

Epoch: 6| Step: 9
Training loss: 2.798017385738258
Validation loss: 2.7382026838857536

Epoch: 6| Step: 10
Training loss: 3.29698274987764
Validation loss: 2.741423357453585

Epoch: 6| Step: 11
Training loss: 3.315732566246364
Validation loss: 2.741235256903683

Epoch: 6| Step: 12
Training loss: 3.3899832263182716
Validation loss: 2.7394805955067416

Epoch: 6| Step: 13
Training loss: 3.142867165710519
Validation loss: 2.737935782686064

Epoch: 38| Step: 0
Training loss: 3.1523071438201407
Validation loss: 2.734974837600876

Epoch: 6| Step: 1
Training loss: 3.3506987910442896
Validation loss: 2.7353766008273865

Epoch: 6| Step: 2
Training loss: 1.9833187986957952
Validation loss: 2.7357786395900985

Epoch: 6| Step: 3
Training loss: 2.443881557660087
Validation loss: 2.7356346086348338

Epoch: 6| Step: 4
Training loss: 2.5133325782019225
Validation loss: 2.7363057875584653

Epoch: 6| Step: 5
Training loss: 2.8869924454254896
Validation loss: 2.7378354098077016

Epoch: 6| Step: 6
Training loss: 3.391833674437594
Validation loss: 2.7341080401961286

Epoch: 6| Step: 7
Training loss: 2.7327255941872797
Validation loss: 2.7372528804476004

Epoch: 6| Step: 8
Training loss: 3.2276712872404714
Validation loss: 2.7340916415590324

Epoch: 6| Step: 9
Training loss: 4.0869879165594
Validation loss: 2.731956872955706

Epoch: 6| Step: 10
Training loss: 3.572822473172252
Validation loss: 2.7331076520837376

Epoch: 6| Step: 11
Training loss: 3.2344808515098955
Validation loss: 2.7305377170622647

Epoch: 6| Step: 12
Training loss: 3.0805282892737123
Validation loss: 2.729405258772435

Epoch: 6| Step: 13
Training loss: 2.578065536275654
Validation loss: 2.7269653310450184

Epoch: 39| Step: 0
Training loss: 3.0160134811403627
Validation loss: 2.7266358626803653

Epoch: 6| Step: 1
Training loss: 3.459842827374781
Validation loss: 2.72729483668805

Epoch: 6| Step: 2
Training loss: 3.4405171418153113
Validation loss: 2.7270183413130495

Epoch: 6| Step: 3
Training loss: 3.014561440881106
Validation loss: 2.7280463664019368

Epoch: 6| Step: 4
Training loss: 3.455659652428585
Validation loss: 2.7362492470269975

Epoch: 6| Step: 5
Training loss: 3.3934396057375302
Validation loss: 2.7516395420868642

Epoch: 6| Step: 6
Training loss: 3.0730949393247706
Validation loss: 2.756559359433027

Epoch: 6| Step: 7
Training loss: 3.07506428473042
Validation loss: 2.756404715976448

Epoch: 6| Step: 8
Training loss: 2.988398850065492
Validation loss: 2.7515204133602453

Epoch: 6| Step: 9
Training loss: 3.3257459277050874
Validation loss: 2.730892155082074

Epoch: 6| Step: 10
Training loss: 2.292719188931971
Validation loss: 2.7253242019841255

Epoch: 6| Step: 11
Training loss: 2.1677190963377706
Validation loss: 2.72517820082114

Epoch: 6| Step: 12
Training loss: 2.441718144139975
Validation loss: 2.72528536505553

Epoch: 6| Step: 13
Training loss: 3.821257866279509
Validation loss: 2.729414103836637

Epoch: 40| Step: 0
Training loss: 2.9442076697806847
Validation loss: 2.7322628600623307

Epoch: 6| Step: 1
Training loss: 2.967385872090011
Validation loss: 2.7352620335271465

Epoch: 6| Step: 2
Training loss: 2.388523250020282
Validation loss: 2.7289805727325924

Epoch: 6| Step: 3
Training loss: 2.743364478205242
Validation loss: 2.729367718825015

Epoch: 6| Step: 4
Training loss: 3.2613807366025256
Validation loss: 2.727613229772226

Epoch: 6| Step: 5
Training loss: 3.20406391060055
Validation loss: 2.7252230099607493

Epoch: 6| Step: 6
Training loss: 2.39258529973414
Validation loss: 2.7246182450424925

Epoch: 6| Step: 7
Training loss: 3.7176875031111174
Validation loss: 2.7247767182056637

Epoch: 6| Step: 8
Training loss: 2.5839363952934047
Validation loss: 2.7311412829158384

Epoch: 6| Step: 9
Training loss: 3.0589844124114096
Validation loss: 2.7328105035807626

Epoch: 6| Step: 10
Training loss: 3.4667844960166314
Validation loss: 2.7324743816086485

Epoch: 6| Step: 11
Training loss: 3.2534328817031204
Validation loss: 2.7197735643838743

Epoch: 6| Step: 12
Training loss: 3.5492920559184147
Validation loss: 2.717162700842462

Epoch: 6| Step: 13
Training loss: 3.096213082750667
Validation loss: 2.7193434777911296

Epoch: 41| Step: 0
Training loss: 3.3004212312907995
Validation loss: 2.7187981842301108

Epoch: 6| Step: 1
Training loss: 3.5041639898760923
Validation loss: 2.718630231615033

Epoch: 6| Step: 2
Training loss: 3.1631445789466115
Validation loss: 2.7169718204904614

Epoch: 6| Step: 3
Training loss: 2.886347724908923
Validation loss: 2.7168557811688108

Epoch: 6| Step: 4
Training loss: 2.5636016524539453
Validation loss: 2.717742401290488

Epoch: 6| Step: 5
Training loss: 3.1507435723323707
Validation loss: 2.717988999296485

Epoch: 6| Step: 6
Training loss: 2.780773164827738
Validation loss: 2.716584848079918

Epoch: 6| Step: 7
Training loss: 2.4420590924010845
Validation loss: 2.716398374144796

Epoch: 6| Step: 8
Training loss: 3.1349746206616063
Validation loss: 2.7177016033511143

Epoch: 6| Step: 9
Training loss: 3.430398993237903
Validation loss: 2.72259905877715

Epoch: 6| Step: 10
Training loss: 3.148163662874309
Validation loss: 2.725169108708166

Epoch: 6| Step: 11
Training loss: 2.9712367835611513
Validation loss: 2.7215981468834802

Epoch: 6| Step: 12
Training loss: 3.0580884338226046
Validation loss: 2.7268372637700415

Epoch: 6| Step: 13
Training loss: 3.2021779218679183
Validation loss: 2.728034655399306

Epoch: 42| Step: 0
Training loss: 3.26696574600146
Validation loss: 2.7241728498643614

Epoch: 6| Step: 1
Training loss: 2.811589241914007
Validation loss: 2.7275843516766667

Epoch: 6| Step: 2
Training loss: 2.6197505414299487
Validation loss: 2.725674481633734

Epoch: 6| Step: 3
Training loss: 2.895142690143106
Validation loss: 2.7291286512512563

Epoch: 6| Step: 4
Training loss: 3.158285108078999
Validation loss: 2.715362968024956

Epoch: 6| Step: 5
Training loss: 3.0234748778878977
Validation loss: 2.7046083715242424

Epoch: 6| Step: 6
Training loss: 3.1935203395020793
Validation loss: 2.7039292904348655

Epoch: 6| Step: 7
Training loss: 3.3198945623913283
Validation loss: 2.700960777997653

Epoch: 6| Step: 8
Training loss: 3.3057311373826415
Validation loss: 2.698849677813082

Epoch: 6| Step: 9
Training loss: 3.405137186521742
Validation loss: 2.6991205760488484

Epoch: 6| Step: 10
Training loss: 2.868887000043493
Validation loss: 2.699279574571467

Epoch: 6| Step: 11
Training loss: 2.6327027430989065
Validation loss: 2.698363026132792

Epoch: 6| Step: 12
Training loss: 2.5798811970940414
Validation loss: 2.6994585756069895

Epoch: 6| Step: 13
Training loss: 3.727383249297823
Validation loss: 2.7018262882658877

Epoch: 43| Step: 0
Training loss: 2.718203347156522
Validation loss: 2.6953359846887572

Epoch: 6| Step: 1
Training loss: 3.2055645840269955
Validation loss: 2.697354638777422

Epoch: 6| Step: 2
Training loss: 2.7002114707642666
Validation loss: 2.69658185789935

Epoch: 6| Step: 3
Training loss: 1.9787175790006741
Validation loss: 2.697460080019744

Epoch: 6| Step: 4
Training loss: 3.257166370746762
Validation loss: 2.6936782026776234

Epoch: 6| Step: 5
Training loss: 3.6535693391970545
Validation loss: 2.703620145934734

Epoch: 6| Step: 6
Training loss: 2.852545414662561
Validation loss: 2.7050215534593964

Epoch: 6| Step: 7
Training loss: 3.3854493750923935
Validation loss: 2.7054104025291337

Epoch: 6| Step: 8
Training loss: 3.7649054255265404
Validation loss: 2.716719672071423

Epoch: 6| Step: 9
Training loss: 3.146653647341977
Validation loss: 2.6957474828228323

Epoch: 6| Step: 10
Training loss: 3.1377045826874417
Validation loss: 2.695375505198154

Epoch: 6| Step: 11
Training loss: 2.8568414188498554
Validation loss: 2.7002448209108985

Epoch: 6| Step: 12
Training loss: 2.549203381739034
Validation loss: 2.7075262761953676

Epoch: 6| Step: 13
Training loss: 3.049274927472593
Validation loss: 2.717016330937054

Epoch: 44| Step: 0
Training loss: 2.9018209199551337
Validation loss: 2.702749331131524

Epoch: 6| Step: 1
Training loss: 3.093805794260936
Validation loss: 2.7008965125396185

Epoch: 6| Step: 2
Training loss: 3.1744796506752517
Validation loss: 2.698523062057818

Epoch: 6| Step: 3
Training loss: 3.1998132472379863
Validation loss: 2.6961176671447005

Epoch: 6| Step: 4
Training loss: 2.7885062894594626
Validation loss: 2.6938720192406684

Epoch: 6| Step: 5
Training loss: 2.5574920845095694
Validation loss: 2.6912965393637682

Epoch: 6| Step: 6
Training loss: 2.8847692614863676
Validation loss: 2.6929483706377613

Epoch: 6| Step: 7
Training loss: 3.3306038807839875
Validation loss: 2.6960384438140728

Epoch: 6| Step: 8
Training loss: 3.1960603543002875
Validation loss: 2.6917172258584436

Epoch: 6| Step: 9
Training loss: 3.2462332978992205
Validation loss: 2.693330751188574

Epoch: 6| Step: 10
Training loss: 2.47183855225208
Validation loss: 2.703931692013512

Epoch: 6| Step: 11
Training loss: 3.07982236275614
Validation loss: 2.7194105455501454

Epoch: 6| Step: 12
Training loss: 3.313924231335888
Validation loss: 2.7219714093208096

Epoch: 6| Step: 13
Training loss: 3.3747061142300785
Validation loss: 2.736041749342567

Epoch: 45| Step: 0
Training loss: 2.7852876479731683
Validation loss: 2.7249297753240094

Epoch: 6| Step: 1
Training loss: 3.286542275752749
Validation loss: 2.712303043475511

Epoch: 6| Step: 2
Training loss: 3.2184525139378173
Validation loss: 2.6911112710623546

Epoch: 6| Step: 3
Training loss: 3.2014143679127396
Validation loss: 2.687219878448279

Epoch: 6| Step: 4
Training loss: 2.8708309961613043
Validation loss: 2.6885811171040794

Epoch: 6| Step: 5
Training loss: 3.0790199489164074
Validation loss: 2.69036726304384

Epoch: 6| Step: 6
Training loss: 2.9407025998910292
Validation loss: 2.692405222981378

Epoch: 6| Step: 7
Training loss: 3.146596517050332
Validation loss: 2.6943077341610127

Epoch: 6| Step: 8
Training loss: 3.4117898949782335
Validation loss: 2.7018780335627173

Epoch: 6| Step: 9
Training loss: 2.907646417973748
Validation loss: 2.6999230351448933

Epoch: 6| Step: 10
Training loss: 3.1827665600015136
Validation loss: 2.695730131875008

Epoch: 6| Step: 11
Training loss: 2.9215848238054685
Validation loss: 2.6967121700651644

Epoch: 6| Step: 12
Training loss: 2.619226646290827
Validation loss: 2.6949978267106824

Epoch: 6| Step: 13
Training loss: 2.903674389252567
Validation loss: 2.694775964029292

Epoch: 46| Step: 0
Training loss: 2.6867365972664348
Validation loss: 2.6936339052293414

Epoch: 6| Step: 1
Training loss: 3.100904111381614
Validation loss: 2.6902203233960775

Epoch: 6| Step: 2
Training loss: 3.275866060548763
Validation loss: 2.688115830869435

Epoch: 6| Step: 3
Training loss: 3.4860802826820683
Validation loss: 2.6889556744758436

Epoch: 6| Step: 4
Training loss: 3.224225766003543
Validation loss: 2.6821000187322634

Epoch: 6| Step: 5
Training loss: 3.5593094843996504
Validation loss: 2.6821866626697597

Epoch: 6| Step: 6
Training loss: 2.9417340221384456
Validation loss: 2.6824011782209483

Epoch: 6| Step: 7
Training loss: 2.474835584940008
Validation loss: 2.6783931211472147

Epoch: 6| Step: 8
Training loss: 3.0693988556926173
Validation loss: 2.677987371035115

Epoch: 6| Step: 9
Training loss: 3.268365759357186
Validation loss: 2.6776834920204684

Epoch: 6| Step: 10
Training loss: 2.4254841154133158
Validation loss: 2.677213780959

Epoch: 6| Step: 11
Training loss: 2.8700567124242924
Validation loss: 2.676688279602149

Epoch: 6| Step: 12
Training loss: 2.8239193832842635
Validation loss: 2.6762800275908387

Epoch: 6| Step: 13
Training loss: 3.108589878353096
Validation loss: 2.6775485704207536

Epoch: 47| Step: 0
Training loss: 3.5497881651524015
Validation loss: 2.6776809625423663

Epoch: 6| Step: 1
Training loss: 3.213756650949701
Validation loss: 2.678723703461595

Epoch: 6| Step: 2
Training loss: 3.2464803564152125
Validation loss: 2.67732546285019

Epoch: 6| Step: 3
Training loss: 2.7332938536452236
Validation loss: 2.6759355421966227

Epoch: 6| Step: 4
Training loss: 2.5036720011574767
Validation loss: 2.676527360523925

Epoch: 6| Step: 5
Training loss: 3.033034910117457
Validation loss: 2.675708064297074

Epoch: 6| Step: 6
Training loss: 2.8650852572819088
Validation loss: 2.671777755712991

Epoch: 6| Step: 7
Training loss: 3.3029990064831405
Validation loss: 2.673341830246261

Epoch: 6| Step: 8
Training loss: 2.764522440473771
Validation loss: 2.670998766945413

Epoch: 6| Step: 9
Training loss: 2.899599402810213
Validation loss: 2.6762922581827437

Epoch: 6| Step: 10
Training loss: 2.738364486408777
Validation loss: 2.669494806472868

Epoch: 6| Step: 11
Training loss: 3.03762995437991
Validation loss: 2.6736506863268965

Epoch: 6| Step: 12
Training loss: 2.831792150467132
Validation loss: 2.670875221699404

Epoch: 6| Step: 13
Training loss: 3.7066998831086257
Validation loss: 2.669706187707633

Epoch: 48| Step: 0
Training loss: 3.181017848089778
Validation loss: 2.6705179184465364

Epoch: 6| Step: 1
Training loss: 2.6906747031411617
Validation loss: 2.663668453155399

Epoch: 6| Step: 2
Training loss: 3.3467239986944697
Validation loss: 2.6632254431871383

Epoch: 6| Step: 3
Training loss: 2.3263374993514314
Validation loss: 2.6645307126140607

Epoch: 6| Step: 4
Training loss: 3.1306379195891267
Validation loss: 2.661901008820541

Epoch: 6| Step: 5
Training loss: 2.1590062092863604
Validation loss: 2.6604070766403103

Epoch: 6| Step: 6
Training loss: 2.9384647165036233
Validation loss: 2.6644361772370515

Epoch: 6| Step: 7
Training loss: 2.724067136343314
Validation loss: 2.6728683368585218

Epoch: 6| Step: 8
Training loss: 3.3705414886569507
Validation loss: 2.6805745164757195

Epoch: 6| Step: 9
Training loss: 3.5032722979841635
Validation loss: 2.681603462267593

Epoch: 6| Step: 10
Training loss: 2.587834886634419
Validation loss: 2.679468511476029

Epoch: 6| Step: 11
Training loss: 3.8786429232275714
Validation loss: 2.67129738812954

Epoch: 6| Step: 12
Training loss: 2.82913173228865
Validation loss: 2.6595663245627783

Epoch: 6| Step: 13
Training loss: 3.4059938106962226
Validation loss: 2.6625475416917457

Epoch: 49| Step: 0
Training loss: 3.8494931865869177
Validation loss: 2.6740928339158194

Epoch: 6| Step: 1
Training loss: 2.7725445236607387
Validation loss: 2.6830992089471666

Epoch: 6| Step: 2
Training loss: 3.2567398903566036
Validation loss: 2.692121826537933

Epoch: 6| Step: 3
Training loss: 3.3107240342531488
Validation loss: 2.7004407797114744

Epoch: 6| Step: 4
Training loss: 2.878891674403401
Validation loss: 2.687051883091046

Epoch: 6| Step: 5
Training loss: 3.25037293861986
Validation loss: 2.698858504280902

Epoch: 6| Step: 6
Training loss: 3.485889738500335
Validation loss: 2.6989881176175303

Epoch: 6| Step: 7
Training loss: 3.031170716182863
Validation loss: 2.704040589465062

Epoch: 6| Step: 8
Training loss: 2.359072027900951
Validation loss: 2.6772996197347356

Epoch: 6| Step: 9
Training loss: 2.3762984490911134
Validation loss: 2.675014732948192

Epoch: 6| Step: 10
Training loss: 2.890148886571045
Validation loss: 2.6649619762941286

Epoch: 6| Step: 11
Training loss: 2.9810393700877524
Validation loss: 2.6596175965665534

Epoch: 6| Step: 12
Training loss: 3.1017466761870325
Validation loss: 2.658215383792668

Epoch: 6| Step: 13
Training loss: 1.990457000110819
Validation loss: 2.6656490686503793

Epoch: 50| Step: 0
Training loss: 3.136892957588474
Validation loss: 2.6881032563812193

Epoch: 6| Step: 1
Training loss: 3.3977661872878424
Validation loss: 2.7288144277364714

Epoch: 6| Step: 2
Training loss: 2.831539558609884
Validation loss: 2.714069611672082

Epoch: 6| Step: 3
Training loss: 3.161231161101392
Validation loss: 2.6652132875417767

Epoch: 6| Step: 4
Training loss: 3.510074013910516
Validation loss: 2.654186810762349

Epoch: 6| Step: 5
Training loss: 3.1053465237346223
Validation loss: 2.6590187279187334

Epoch: 6| Step: 6
Training loss: 3.4265166551381965
Validation loss: 2.6614401022239673

Epoch: 6| Step: 7
Training loss: 3.177364466167943
Validation loss: 2.6622815981699834

Epoch: 6| Step: 8
Training loss: 2.9988544184198034
Validation loss: 2.6630581617876357

Epoch: 6| Step: 9
Training loss: 2.396778241011144
Validation loss: 2.663054194630223

Epoch: 6| Step: 10
Training loss: 3.2048013905895947
Validation loss: 2.665256357719219

Epoch: 6| Step: 11
Training loss: 2.623598360329424
Validation loss: 2.6723246766596307

Epoch: 6| Step: 12
Training loss: 2.4682614772656906
Validation loss: 2.6694713547024578

Epoch: 6| Step: 13
Training loss: 2.45505879955227
Validation loss: 2.6663829953173472

Epoch: 51| Step: 0
Training loss: 3.5163896195417395
Validation loss: 2.660163761207999

Epoch: 6| Step: 1
Training loss: 3.6877551233001182
Validation loss: 2.6631518344665177

Epoch: 6| Step: 2
Training loss: 2.8415293983873218
Validation loss: 2.661535778515857

Epoch: 6| Step: 3
Training loss: 3.1008444466545937
Validation loss: 2.659404765644866

Epoch: 6| Step: 4
Training loss: 2.998073754205646
Validation loss: 2.656726712826836

Epoch: 6| Step: 5
Training loss: 2.7979982986717697
Validation loss: 2.653014121207578

Epoch: 6| Step: 6
Training loss: 2.6299975823801045
Validation loss: 2.6543611270927925

Epoch: 6| Step: 7
Training loss: 3.1648734854772864
Validation loss: 2.6536559176046848

Epoch: 6| Step: 8
Training loss: 2.69162159184211
Validation loss: 2.6496118822800225

Epoch: 6| Step: 9
Training loss: 3.1167496682060776
Validation loss: 2.6506241966248063

Epoch: 6| Step: 10
Training loss: 2.935758845733957
Validation loss: 2.6472580355149042

Epoch: 6| Step: 11
Training loss: 2.629730322178077
Validation loss: 2.6450886051722255

Epoch: 6| Step: 12
Training loss: 2.9865871040214493
Validation loss: 2.6499361882234234

Epoch: 6| Step: 13
Training loss: 2.76196141235776
Validation loss: 2.6509595154284455

Epoch: 52| Step: 0
Training loss: 3.1657362290448656
Validation loss: 2.6595472044242254

Epoch: 6| Step: 1
Training loss: 2.544694025199251
Validation loss: 2.6726112226944756

Epoch: 6| Step: 2
Training loss: 2.558699790562601
Validation loss: 2.6658372624264364

Epoch: 6| Step: 3
Training loss: 4.063176025483329
Validation loss: 2.6765830334321867

Epoch: 6| Step: 4
Training loss: 2.2122824524214786
Validation loss: 2.6599403086381317

Epoch: 6| Step: 5
Training loss: 2.884063862626571
Validation loss: 2.6524964171040724

Epoch: 6| Step: 6
Training loss: 3.0946237890693635
Validation loss: 2.64847744252063

Epoch: 6| Step: 7
Training loss: 3.037789124594222
Validation loss: 2.6450534905388023

Epoch: 6| Step: 8
Training loss: 3.0243612450485737
Validation loss: 2.643177352066848

Epoch: 6| Step: 9
Training loss: 3.2515167218440753
Validation loss: 2.647661425541948

Epoch: 6| Step: 10
Training loss: 2.567843935921349
Validation loss: 2.645759275945574

Epoch: 6| Step: 11
Training loss: 2.446680630774844
Validation loss: 2.647073637336246

Epoch: 6| Step: 12
Training loss: 3.597537491253783
Validation loss: 2.6440632113115314

Epoch: 6| Step: 13
Training loss: 3.2115071109346753
Validation loss: 2.640911455921754

Epoch: 53| Step: 0
Training loss: 3.0457072832471295
Validation loss: 2.644175077575147

Epoch: 6| Step: 1
Training loss: 3.0150354629379463
Validation loss: 2.6388096188863903

Epoch: 6| Step: 2
Training loss: 3.172135967173898
Validation loss: 2.635855266690949

Epoch: 6| Step: 3
Training loss: 3.4013017182096474
Validation loss: 2.6349103448448883

Epoch: 6| Step: 4
Training loss: 2.5203335689602926
Validation loss: 2.635468124775083

Epoch: 6| Step: 5
Training loss: 2.6821208615532233
Validation loss: 2.6350854946626345

Epoch: 6| Step: 6
Training loss: 3.0950958715328176
Validation loss: 2.6373264685863225

Epoch: 6| Step: 7
Training loss: 2.8707697056186436
Validation loss: 2.6386003143980106

Epoch: 6| Step: 8
Training loss: 2.738023601605808
Validation loss: 2.6368311028525655

Epoch: 6| Step: 9
Training loss: 2.760885110130284
Validation loss: 2.6336087441756226

Epoch: 6| Step: 10
Training loss: 3.221436203714053
Validation loss: 2.633866812852403

Epoch: 6| Step: 11
Training loss: 3.3609439468499303
Validation loss: 2.637594150716752

Epoch: 6| Step: 12
Training loss: 3.229568817364595
Validation loss: 2.6419372686162492

Epoch: 6| Step: 13
Training loss: 2.382194163655791
Validation loss: 2.6361921898368514

Epoch: 54| Step: 0
Training loss: 2.550170451432583
Validation loss: 2.6332158754346926

Epoch: 6| Step: 1
Training loss: 2.7446443251573878
Validation loss: 2.631599425535451

Epoch: 6| Step: 2
Training loss: 2.9704453295646793
Validation loss: 2.641447778495983

Epoch: 6| Step: 3
Training loss: 2.8515835695925373
Validation loss: 2.6323716880406285

Epoch: 6| Step: 4
Training loss: 3.0657440442411046
Validation loss: 2.6311913939524354

Epoch: 6| Step: 5
Training loss: 2.7476013299585316
Validation loss: 2.6416959445808996

Epoch: 6| Step: 6
Training loss: 3.580164165888474
Validation loss: 2.6557187692253867

Epoch: 6| Step: 7
Training loss: 2.9598886046197435
Validation loss: 2.6526278502618017

Epoch: 6| Step: 8
Training loss: 3.364215007891697
Validation loss: 2.6515687327943436

Epoch: 6| Step: 9
Training loss: 3.049190326219046
Validation loss: 2.6577752431492008

Epoch: 6| Step: 10
Training loss: 2.611200847391384
Validation loss: 2.6392193173057246

Epoch: 6| Step: 11
Training loss: 3.122124837018021
Validation loss: 2.629352248886438

Epoch: 6| Step: 12
Training loss: 3.359125456302095
Validation loss: 2.6291737012286553

Epoch: 6| Step: 13
Training loss: 2.5926246499799377
Validation loss: 2.6271091060957215

Epoch: 55| Step: 0
Training loss: 3.025817722737996
Validation loss: 2.629934285955287

Epoch: 6| Step: 1
Training loss: 3.689219962689331
Validation loss: 2.6361027903031653

Epoch: 6| Step: 2
Training loss: 3.233015133908145
Validation loss: 2.6353252206469047

Epoch: 6| Step: 3
Training loss: 3.0554340357650696
Validation loss: 2.631819097407322

Epoch: 6| Step: 4
Training loss: 2.610700624634093
Validation loss: 2.6326854469819545

Epoch: 6| Step: 5
Training loss: 3.192712747509608
Validation loss: 2.647467729423574

Epoch: 6| Step: 6
Training loss: 3.036947970992527
Validation loss: 2.6416959096446067

Epoch: 6| Step: 7
Training loss: 2.725462858454718
Validation loss: 2.6299547565214567

Epoch: 6| Step: 8
Training loss: 3.299044360882157
Validation loss: 2.6264692112177443

Epoch: 6| Step: 9
Training loss: 2.5274468567465425
Validation loss: 2.628231239666273

Epoch: 6| Step: 10
Training loss: 2.58029471909748
Validation loss: 2.6269946218447937

Epoch: 6| Step: 11
Training loss: 3.0510400718475545
Validation loss: 2.625339817137148

Epoch: 6| Step: 12
Training loss: 3.0339544757201264
Validation loss: 2.6244065863578316

Epoch: 6| Step: 13
Training loss: 2.3716821081963886
Validation loss: 2.6244514417582674

Epoch: 56| Step: 0
Training loss: 2.941930149226811
Validation loss: 2.623549961804023

Epoch: 6| Step: 1
Training loss: 3.3488995125184466
Validation loss: 2.6236881624882886

Epoch: 6| Step: 2
Training loss: 2.976679762084907
Validation loss: 2.6295770196750174

Epoch: 6| Step: 3
Training loss: 3.605980007173333
Validation loss: 2.624909336381935

Epoch: 6| Step: 4
Training loss: 3.047316925082544
Validation loss: 2.6249843065658993

Epoch: 6| Step: 5
Training loss: 3.1384719376657846
Validation loss: 2.624652997826744

Epoch: 6| Step: 6
Training loss: 3.0379942754775677
Validation loss: 2.622271513886961

Epoch: 6| Step: 7
Training loss: 2.726961909991579
Validation loss: 2.622057171961276

Epoch: 6| Step: 8
Training loss: 2.9658182223918383
Validation loss: 2.620098535336036

Epoch: 6| Step: 9
Training loss: 2.7947135543760444
Validation loss: 2.6184210020422225

Epoch: 6| Step: 10
Training loss: 2.926508692110789
Validation loss: 2.6178159407070813

Epoch: 6| Step: 11
Training loss: 2.685963568524873
Validation loss: 2.618895883696747

Epoch: 6| Step: 12
Training loss: 2.9403597932057526
Validation loss: 2.616173081499878

Epoch: 6| Step: 13
Training loss: 2.2543796723065928
Validation loss: 2.616201156060631

Epoch: 57| Step: 0
Training loss: 3.6027341579390453
Validation loss: 2.616535306433655

Epoch: 6| Step: 1
Training loss: 3.1388377707791033
Validation loss: 2.61742101199086

Epoch: 6| Step: 2
Training loss: 3.3137095420321736
Validation loss: 2.620873115099768

Epoch: 6| Step: 3
Training loss: 2.6593769293126996
Validation loss: 2.6240970496174953

Epoch: 6| Step: 4
Training loss: 2.7528186571370865
Validation loss: 2.6229097019866017

Epoch: 6| Step: 5
Training loss: 2.9454688883174267
Validation loss: 2.6254227586409

Epoch: 6| Step: 6
Training loss: 2.685883501629617
Validation loss: 2.6305744265222044

Epoch: 6| Step: 7
Training loss: 2.307258852573929
Validation loss: 2.6230875989953644

Epoch: 6| Step: 8
Training loss: 2.4826843459110717
Validation loss: 2.614099972082582

Epoch: 6| Step: 9
Training loss: 3.2913334995779957
Validation loss: 2.6145558496539163

Epoch: 6| Step: 10
Training loss: 2.9775600413843595
Validation loss: 2.6115759775956953

Epoch: 6| Step: 11
Training loss: 2.925394641304494
Validation loss: 2.6113427754384184

Epoch: 6| Step: 12
Training loss: 3.2716598600322273
Validation loss: 2.61260775170055

Epoch: 6| Step: 13
Training loss: 3.1847507363697427
Validation loss: 2.6235421386110866

Epoch: 58| Step: 0
Training loss: 3.182365320896476
Validation loss: 2.6154856673785916

Epoch: 6| Step: 1
Training loss: 2.744607145898105
Validation loss: 2.6144872050518

Epoch: 6| Step: 2
Training loss: 2.627895030371155
Validation loss: 2.613116179009334

Epoch: 6| Step: 3
Training loss: 3.3558359355993783
Validation loss: 2.6132112171131876

Epoch: 6| Step: 4
Training loss: 2.9595301360864377
Validation loss: 2.618602203686813

Epoch: 6| Step: 5
Training loss: 2.484860834565385
Validation loss: 2.623610623492593

Epoch: 6| Step: 6
Training loss: 2.926532643782487
Validation loss: 2.626876945228164

Epoch: 6| Step: 7
Training loss: 2.887495336487542
Validation loss: 2.6379443952633683

Epoch: 6| Step: 8
Training loss: 3.306535497212109
Validation loss: 2.6419358256846888

Epoch: 6| Step: 9
Training loss: 2.6509260790694777
Validation loss: 2.6346805537825864

Epoch: 6| Step: 10
Training loss: 2.8885710146037074
Validation loss: 2.6400311559489986

Epoch: 6| Step: 11
Training loss: 2.954455686419266
Validation loss: 2.6515291249631887

Epoch: 6| Step: 12
Training loss: 2.876445075378197
Validation loss: 2.635835358405771

Epoch: 6| Step: 13
Training loss: 4.088674645984659
Validation loss: 2.624653032989892

Epoch: 59| Step: 0
Training loss: 2.86425764602156
Validation loss: 2.6040847466822554

Epoch: 6| Step: 1
Training loss: 3.3663348824458157
Validation loss: 2.6315128719964624

Epoch: 6| Step: 2
Training loss: 3.0937612514098594
Validation loss: 2.653291001665754

Epoch: 6| Step: 3
Training loss: 2.70556953456156
Validation loss: 2.677392661060066

Epoch: 6| Step: 4
Training loss: 2.9986413422235065
Validation loss: 2.6924786537072163

Epoch: 6| Step: 5
Training loss: 3.881464211761466
Validation loss: 2.7052460490371386

Epoch: 6| Step: 6
Training loss: 2.628552802757355
Validation loss: 2.6949838041669354

Epoch: 6| Step: 7
Training loss: 2.714148123559415
Validation loss: 2.6833111818327025

Epoch: 6| Step: 8
Training loss: 3.106324199197112
Validation loss: 2.6742255773454136

Epoch: 6| Step: 9
Training loss: 2.8516350723855903
Validation loss: 2.672962665127697

Epoch: 6| Step: 10
Training loss: 3.213168888738367
Validation loss: 2.66669527802449

Epoch: 6| Step: 11
Training loss: 3.209764400879802
Validation loss: 2.6674993792150445

Epoch: 6| Step: 12
Training loss: 2.831922479053437
Validation loss: 2.664908642616584

Epoch: 6| Step: 13
Training loss: 2.494846277030792
Validation loss: 2.6628549926017957

Epoch: 60| Step: 0
Training loss: 2.7316458024660912
Validation loss: 2.660150371313304

Epoch: 6| Step: 1
Training loss: 2.921501094945219
Validation loss: 2.6586833292056493

Epoch: 6| Step: 2
Training loss: 3.007899374760007
Validation loss: 2.6563460446679317

Epoch: 6| Step: 3
Training loss: 2.8558087231351412
Validation loss: 2.655296465308283

Epoch: 6| Step: 4
Training loss: 3.137265662450662
Validation loss: 2.651358792750552

Epoch: 6| Step: 5
Training loss: 2.96389083727179
Validation loss: 2.653616960192467

Epoch: 6| Step: 6
Training loss: 3.085855294593368
Validation loss: 2.6336677130895647

Epoch: 6| Step: 7
Training loss: 3.473515441816278
Validation loss: 2.640560645221963

Epoch: 6| Step: 8
Training loss: 3.3107730034312266
Validation loss: 2.654227578545724

Epoch: 6| Step: 9
Training loss: 2.3479319583515585
Validation loss: 2.6639478485164876

Epoch: 6| Step: 10
Training loss: 3.2793785934762987
Validation loss: 2.6648658486896677

Epoch: 6| Step: 11
Training loss: 2.9678163365297805
Validation loss: 2.6691705420276617

Epoch: 6| Step: 12
Training loss: 3.1247695837904956
Validation loss: 2.6697914699070906

Epoch: 6| Step: 13
Training loss: 2.876971688246289
Validation loss: 2.6651695096680132

Epoch: 61| Step: 0
Training loss: 2.9422878448829572
Validation loss: 2.652206476064402

Epoch: 6| Step: 1
Training loss: 3.2195608358451633
Validation loss: 2.644488977477399

Epoch: 6| Step: 2
Training loss: 2.9652210342334264
Validation loss: 2.643775849865789

Epoch: 6| Step: 3
Training loss: 2.456378310251939
Validation loss: 2.644734073288688

Epoch: 6| Step: 4
Training loss: 3.803978824608687
Validation loss: 2.64246037786769

Epoch: 6| Step: 5
Training loss: 3.6702473380912783
Validation loss: 2.643315309730949

Epoch: 6| Step: 6
Training loss: 3.052403681654641
Validation loss: 2.643298492823772

Epoch: 6| Step: 7
Training loss: 2.545788679548153
Validation loss: 2.6397614406487113

Epoch: 6| Step: 8
Training loss: 2.4755485697377
Validation loss: 2.6409305988619662

Epoch: 6| Step: 9
Training loss: 2.9470682349821193
Validation loss: 2.64147995376646

Epoch: 6| Step: 10
Training loss: 2.7369696813262334
Validation loss: 2.6388132742054937

Epoch: 6| Step: 11
Training loss: 2.8658074739987933
Validation loss: 2.638127606577225

Epoch: 6| Step: 12
Training loss: 2.933728820698327
Validation loss: 2.635260320563216

Epoch: 6| Step: 13
Training loss: 3.0838410414858988
Validation loss: 2.6393194707266843

Epoch: 62| Step: 0
Training loss: 2.8186197458017177
Validation loss: 2.635172786483031

Epoch: 6| Step: 1
Training loss: 2.886277347003944
Validation loss: 2.632302211754296

Epoch: 6| Step: 2
Training loss: 2.133236126870591
Validation loss: 2.633674251481894

Epoch: 6| Step: 3
Training loss: 2.6044021093112453
Validation loss: 2.6362409222367873

Epoch: 6| Step: 4
Training loss: 3.6687880505955737
Validation loss: 2.639296303519442

Epoch: 6| Step: 5
Training loss: 2.9148120069654078
Validation loss: 2.6394724385433146

Epoch: 6| Step: 6
Training loss: 3.1508501147011896
Validation loss: 2.635995436447824

Epoch: 6| Step: 7
Training loss: 3.3971734864204945
Validation loss: 2.6433502481514193

Epoch: 6| Step: 8
Training loss: 3.0586575127553055
Validation loss: 2.6384199517739675

Epoch: 6| Step: 9
Training loss: 3.2571568549723033
Validation loss: 2.6369682312481713

Epoch: 6| Step: 10
Training loss: 2.509246701277803
Validation loss: 2.6333815819320887

Epoch: 6| Step: 11
Training loss: 3.028296850975306
Validation loss: 2.63422044681578

Epoch: 6| Step: 12
Training loss: 3.0620225806721115
Validation loss: 2.630926553692348

Epoch: 6| Step: 13
Training loss: 3.199737245977916
Validation loss: 2.6348733120950754

Epoch: 63| Step: 0
Training loss: 3.117562560652164
Validation loss: 2.6271363202070703

Epoch: 6| Step: 1
Training loss: 2.766352153810429
Validation loss: 2.6281979794378003

Epoch: 6| Step: 2
Training loss: 2.4720780357956897
Validation loss: 2.6264809856286284

Epoch: 6| Step: 3
Training loss: 3.214488347039378
Validation loss: 2.626837915658037

Epoch: 6| Step: 4
Training loss: 3.0844038230530955
Validation loss: 2.630816580710463

Epoch: 6| Step: 5
Training loss: 2.9710315169207027
Validation loss: 2.626983834406733

Epoch: 6| Step: 6
Training loss: 3.379570303695733
Validation loss: 2.6271367856783385

Epoch: 6| Step: 7
Training loss: 2.9161488936287205
Validation loss: 2.627230966286783

Epoch: 6| Step: 8
Training loss: 2.592298906081094
Validation loss: 2.6287767622517166

Epoch: 6| Step: 9
Training loss: 3.040938791077371
Validation loss: 2.6257876070619806

Epoch: 6| Step: 10
Training loss: 2.6049192841484956
Validation loss: 2.6270468642711786

Epoch: 6| Step: 11
Training loss: 3.051282931802167
Validation loss: 2.6239167266958634

Epoch: 6| Step: 12
Training loss: 3.0302358287526805
Validation loss: 2.623914246997121

Epoch: 6| Step: 13
Training loss: 3.711373843590102
Validation loss: 2.6227246153420776

Epoch: 64| Step: 0
Training loss: 2.890382498801825
Validation loss: 2.630707545478912

Epoch: 6| Step: 1
Training loss: 3.5564312022191804
Validation loss: 2.6408146751684725

Epoch: 6| Step: 2
Training loss: 2.2584016552639663
Validation loss: 2.6439422684901785

Epoch: 6| Step: 3
Training loss: 3.675127901719087
Validation loss: 2.650864293951094

Epoch: 6| Step: 4
Training loss: 3.5489612779007005
Validation loss: 2.6340209401870105

Epoch: 6| Step: 5
Training loss: 3.246081116946687
Validation loss: 2.622697286051936

Epoch: 6| Step: 6
Training loss: 3.0028691876527516
Validation loss: 2.6209312055895104

Epoch: 6| Step: 7
Training loss: 2.960822996477041
Validation loss: 2.6182360592641527

Epoch: 6| Step: 8
Training loss: 2.288086145900012
Validation loss: 2.6162408577768517

Epoch: 6| Step: 9
Training loss: 2.1383940494487383
Validation loss: 2.621176083521757

Epoch: 6| Step: 10
Training loss: 3.0815342646273245
Validation loss: 2.6234523011605115

Epoch: 6| Step: 11
Training loss: 2.4606748410487644
Validation loss: 2.620113410699469

Epoch: 6| Step: 12
Training loss: 3.155068355732678
Validation loss: 2.619240146536866

Epoch: 6| Step: 13
Training loss: 3.0767391590024435
Validation loss: 2.6244232180822675

Epoch: 65| Step: 0
Training loss: 3.285266271102831
Validation loss: 2.6224741192630643

Epoch: 6| Step: 1
Training loss: 2.3431564596755043
Validation loss: 2.6232445389896246

Epoch: 6| Step: 2
Training loss: 2.8289286277831946
Validation loss: 2.6202541720937185

Epoch: 6| Step: 3
Training loss: 2.7488006230472397
Validation loss: 2.625345566754563

Epoch: 6| Step: 4
Training loss: 3.2124281344557684
Validation loss: 2.6215717305379242

Epoch: 6| Step: 5
Training loss: 2.898573057392718
Validation loss: 2.6221677088240374

Epoch: 6| Step: 6
Training loss: 2.698070762288838
Validation loss: 2.6199897164947137

Epoch: 6| Step: 7
Training loss: 3.222262197736161
Validation loss: 2.621710277031649

Epoch: 6| Step: 8
Training loss: 3.1477645268378094
Validation loss: 2.6383305208417926

Epoch: 6| Step: 9
Training loss: 2.7058682496222843
Validation loss: 2.6552754495809348

Epoch: 6| Step: 10
Training loss: 2.6376211924980333
Validation loss: 2.637219800782865

Epoch: 6| Step: 11
Training loss: 3.253605529884008
Validation loss: 2.623029002099417

Epoch: 6| Step: 12
Training loss: 3.1094420152659654
Validation loss: 2.6311605855545133

Epoch: 6| Step: 13
Training loss: 3.7011415988619865
Validation loss: 2.6194123246058636

Epoch: 66| Step: 0
Training loss: 3.327088817065614
Validation loss: 2.614111077474572

Epoch: 6| Step: 1
Training loss: 2.4029029257443044
Validation loss: 2.61194913213784

Epoch: 6| Step: 2
Training loss: 2.8936777851494764
Validation loss: 2.612563319988599

Epoch: 6| Step: 3
Training loss: 2.875115931288192
Validation loss: 2.6144405578163274

Epoch: 6| Step: 4
Training loss: 2.928499759235734
Validation loss: 2.6123111733743345

Epoch: 6| Step: 5
Training loss: 3.3098966336893376
Validation loss: 2.6106723023808303

Epoch: 6| Step: 6
Training loss: 3.17325896338929
Validation loss: 2.6103868787750018

Epoch: 6| Step: 7
Training loss: 3.0578484534565544
Validation loss: 2.612242527688815

Epoch: 6| Step: 8
Training loss: 2.83316854857917
Validation loss: 2.6090719877153683

Epoch: 6| Step: 9
Training loss: 2.8531178870222123
Validation loss: 2.607174595061556

Epoch: 6| Step: 10
Training loss: 3.1357332156423348
Validation loss: 2.607260414302561

Epoch: 6| Step: 11
Training loss: 3.062782897339103
Validation loss: 2.607283164118529

Epoch: 6| Step: 12
Training loss: 2.9446238717005224
Validation loss: 2.605343370612106

Epoch: 6| Step: 13
Training loss: 2.323063821507887
Validation loss: 2.6044807349702066

Epoch: 67| Step: 0
Training loss: 2.956071463550844
Validation loss: 2.609208843778676

Epoch: 6| Step: 1
Training loss: 3.072987563182865
Validation loss: 2.6079877423026314

Epoch: 6| Step: 2
Training loss: 2.6170489374176062
Validation loss: 2.607180304108147

Epoch: 6| Step: 3
Training loss: 3.1095541130886537
Validation loss: 2.614199672766518

Epoch: 6| Step: 4
Training loss: 2.8174585500814384
Validation loss: 2.6122030782733563

Epoch: 6| Step: 5
Training loss: 3.1880461281056145
Validation loss: 2.613214432930102

Epoch: 6| Step: 6
Training loss: 3.039421785980159
Validation loss: 2.61557325458822

Epoch: 6| Step: 7
Training loss: 2.646694994213542
Validation loss: 2.624054742954705

Epoch: 6| Step: 8
Training loss: 3.227097436928678
Validation loss: 2.6268527879913544

Epoch: 6| Step: 9
Training loss: 3.1763188844287367
Validation loss: 2.6472624998977503

Epoch: 6| Step: 10
Training loss: 2.411394836600468
Validation loss: 2.676836164142394

Epoch: 6| Step: 11
Training loss: 3.3924555605379134
Validation loss: 2.7097588951462

Epoch: 6| Step: 12
Training loss: 3.100467517423642
Validation loss: 2.697332864390333

Epoch: 6| Step: 13
Training loss: 2.560877961170121
Validation loss: 2.6824685981109107

Epoch: 68| Step: 0
Training loss: 3.418324758245189
Validation loss: 2.7075083729610294

Epoch: 6| Step: 1
Training loss: 2.537288856177999
Validation loss: 2.6938526396393687

Epoch: 6| Step: 2
Training loss: 3.137336945571257
Validation loss: 2.6636188135085024

Epoch: 6| Step: 3
Training loss: 2.986289164436829
Validation loss: 2.6470720025397414

Epoch: 6| Step: 4
Training loss: 1.9993056045513964
Validation loss: 2.63538114235281

Epoch: 6| Step: 5
Training loss: 3.1368166479508055
Validation loss: 2.6254362367905597

Epoch: 6| Step: 6
Training loss: 3.339187504057815
Validation loss: 2.624111733301146

Epoch: 6| Step: 7
Training loss: 2.781208423775225
Validation loss: 2.6128350805012452

Epoch: 6| Step: 8
Training loss: 3.1123387000024136
Validation loss: 2.6305260891024944

Epoch: 6| Step: 9
Training loss: 3.378294855270983
Validation loss: 2.6378787150760514

Epoch: 6| Step: 10
Training loss: 2.4555248014999256
Validation loss: 2.643155513512971

Epoch: 6| Step: 11
Training loss: 3.1925036482465265
Validation loss: 2.642155883535475

Epoch: 6| Step: 12
Training loss: 3.2341377797876674
Validation loss: 2.6364710177858535

Epoch: 6| Step: 13
Training loss: 2.523515068255614
Validation loss: 2.6292503965570337

Epoch: 69| Step: 0
Training loss: 3.0646487113432737
Validation loss: 2.628887196984228

Epoch: 6| Step: 1
Training loss: 3.1754590160744565
Validation loss: 2.6226053150681987

Epoch: 6| Step: 2
Training loss: 2.735793263605451
Validation loss: 2.618960931860478

Epoch: 6| Step: 3
Training loss: 2.2658895403882164
Validation loss: 2.6155998516634353

Epoch: 6| Step: 4
Training loss: 1.909673899029561
Validation loss: 2.6087470750398105

Epoch: 6| Step: 5
Training loss: 3.231289631288785
Validation loss: 2.6002568667061996

Epoch: 6| Step: 6
Training loss: 3.038310215273187
Validation loss: 2.5944469576557156

Epoch: 6| Step: 7
Training loss: 3.033536697389563
Validation loss: 2.5936639254802842

Epoch: 6| Step: 8
Training loss: 3.1326794679321583
Validation loss: 2.595181764446578

Epoch: 6| Step: 9
Training loss: 3.3755060452325316
Validation loss: 2.591004093469701

Epoch: 6| Step: 10
Training loss: 2.4546107755504214
Validation loss: 2.5902016093229774

Epoch: 6| Step: 11
Training loss: 3.3138076792171973
Validation loss: 2.5917892032467122

Epoch: 6| Step: 12
Training loss: 3.727269519457737
Validation loss: 2.5912067078239494

Epoch: 6| Step: 13
Training loss: 1.9571071465369807
Validation loss: 2.5994020484773284

Epoch: 70| Step: 0
Training loss: 2.6503249562954356
Validation loss: 2.61180474093727

Epoch: 6| Step: 1
Training loss: 3.3702576839285006
Validation loss: 2.607065468844382

Epoch: 6| Step: 2
Training loss: 2.4962966191767126
Validation loss: 2.605614412250547

Epoch: 6| Step: 3
Training loss: 3.0464539848344776
Validation loss: 2.61567030623988

Epoch: 6| Step: 4
Training loss: 2.169844412257935
Validation loss: 2.610134799869125

Epoch: 6| Step: 5
Training loss: 2.8924749279996265
Validation loss: 2.601112764131057

Epoch: 6| Step: 6
Training loss: 3.5941772207000073
Validation loss: 2.594554951657072

Epoch: 6| Step: 7
Training loss: 3.1862275818926182
Validation loss: 2.58588419902284

Epoch: 6| Step: 8
Training loss: 2.950478149715853
Validation loss: 2.5814613648052767

Epoch: 6| Step: 9
Training loss: 3.1198231351068837
Validation loss: 2.583654836865993

Epoch: 6| Step: 10
Training loss: 2.618122355190461
Validation loss: 2.585023105729354

Epoch: 6| Step: 11
Training loss: 2.8650839258387544
Validation loss: 2.581433144816601

Epoch: 6| Step: 12
Training loss: 3.3695179249787244
Validation loss: 2.5870181276560635

Epoch: 6| Step: 13
Training loss: 2.4175182902799084
Validation loss: 2.5923252395706298

Epoch: 71| Step: 0
Training loss: 3.2484000742831136
Validation loss: 2.591091746392734

Epoch: 6| Step: 1
Training loss: 2.8870652833487127
Validation loss: 2.5867549361537314

Epoch: 6| Step: 2
Training loss: 2.8293875726479896
Validation loss: 2.590464161666515

Epoch: 6| Step: 3
Training loss: 3.278161329971767
Validation loss: 2.588969532370484

Epoch: 6| Step: 4
Training loss: 2.634996182115022
Validation loss: 2.5892544554827936

Epoch: 6| Step: 5
Training loss: 3.400153627851713
Validation loss: 2.597469308612157

Epoch: 6| Step: 6
Training loss: 2.5746201244251967
Validation loss: 2.597027995007588

Epoch: 6| Step: 7
Training loss: 2.9497962234720156
Validation loss: 2.607508899342124

Epoch: 6| Step: 8
Training loss: 2.9028480882256704
Validation loss: 2.596338411002929

Epoch: 6| Step: 9
Training loss: 2.88175965805732
Validation loss: 2.598327870639641

Epoch: 6| Step: 10
Training loss: 2.3157686382022606
Validation loss: 2.5880946826566436

Epoch: 6| Step: 11
Training loss: 2.8102686189568
Validation loss: 2.5823684389557258

Epoch: 6| Step: 12
Training loss: 3.4703250092989872
Validation loss: 2.5792955492560203

Epoch: 6| Step: 13
Training loss: 2.5280028814709814
Validation loss: 2.5783591185587045

Epoch: 72| Step: 0
Training loss: 2.690813018574077
Validation loss: 2.576026240733051

Epoch: 6| Step: 1
Training loss: 2.7660013578553637
Validation loss: 2.576337332394356

Epoch: 6| Step: 2
Training loss: 2.853033987308819
Validation loss: 2.5814335261698553

Epoch: 6| Step: 3
Training loss: 3.1504915792391723
Validation loss: 2.581077087247288

Epoch: 6| Step: 4
Training loss: 3.358495126870354
Validation loss: 2.586714595555304

Epoch: 6| Step: 5
Training loss: 2.85788942528627
Validation loss: 2.592625616057474

Epoch: 6| Step: 6
Training loss: 3.1479213092667893
Validation loss: 2.6010716537028378

Epoch: 6| Step: 7
Training loss: 2.979962667192958
Validation loss: 2.6123968248561407

Epoch: 6| Step: 8
Training loss: 3.0244050757151757
Validation loss: 2.6078274466634563

Epoch: 6| Step: 9
Training loss: 2.512921889075198
Validation loss: 2.630763949289188

Epoch: 6| Step: 10
Training loss: 3.1087529312567854
Validation loss: 2.641655643318892

Epoch: 6| Step: 11
Training loss: 2.377107488279346
Validation loss: 2.6242481332730496

Epoch: 6| Step: 12
Training loss: 2.615568565571091
Validation loss: 2.6010225935473668

Epoch: 6| Step: 13
Training loss: 3.7473699883830505
Validation loss: 2.5758007293179292

Epoch: 73| Step: 0
Training loss: 2.6974910169563233
Validation loss: 2.5667637748777037

Epoch: 6| Step: 1
Training loss: 3.2298778284074445
Validation loss: 2.5668122378798786

Epoch: 6| Step: 2
Training loss: 3.080209868328257
Validation loss: 2.573032382692681

Epoch: 6| Step: 3
Training loss: 3.0403460069806343
Validation loss: 2.5750091109735385

Epoch: 6| Step: 4
Training loss: 3.075420605186177
Validation loss: 2.5787681277107675

Epoch: 6| Step: 5
Training loss: 3.12353145425615
Validation loss: 2.576458307896247

Epoch: 6| Step: 6
Training loss: 2.0434299043338338
Validation loss: 2.571134271633702

Epoch: 6| Step: 7
Training loss: 2.648294360588473
Validation loss: 2.57251424450104

Epoch: 6| Step: 8
Training loss: 2.9784808847871562
Validation loss: 2.5668832710172986

Epoch: 6| Step: 9
Training loss: 2.638041298809796
Validation loss: 2.570592375348382

Epoch: 6| Step: 10
Training loss: 3.060744424662972
Validation loss: 2.5738198349698234

Epoch: 6| Step: 11
Training loss: 3.374334057698908
Validation loss: 2.5870192989747

Epoch: 6| Step: 12
Training loss: 3.370205900427939
Validation loss: 2.6019644537123123

Epoch: 6| Step: 13
Training loss: 2.326705499869672
Validation loss: 2.62041601203227

Epoch: 74| Step: 0
Training loss: 3.300908662943983
Validation loss: 2.6269293617545784

Epoch: 6| Step: 1
Training loss: 3.306312972913108
Validation loss: 2.6413438380090177

Epoch: 6| Step: 2
Training loss: 3.315166335960084
Validation loss: 2.646207594197251

Epoch: 6| Step: 3
Training loss: 2.7931952964829425
Validation loss: 2.5694018063217343

Epoch: 6| Step: 4
Training loss: 2.8635160550253067
Validation loss: 2.561907376828584

Epoch: 6| Step: 5
Training loss: 3.2151354892592683
Validation loss: 2.5655829489643724

Epoch: 6| Step: 6
Training loss: 2.619019007010455
Validation loss: 2.5684401001431167

Epoch: 6| Step: 7
Training loss: 2.93466406750192
Validation loss: 2.5795045464918536

Epoch: 6| Step: 8
Training loss: 2.933222639278292
Validation loss: 2.5845524143984253

Epoch: 6| Step: 9
Training loss: 3.333397785199444
Validation loss: 2.6012628933321906

Epoch: 6| Step: 10
Training loss: 2.644484639285275
Validation loss: 2.587124813581672

Epoch: 6| Step: 11
Training loss: 2.4528925111269304
Validation loss: 2.596859380400802

Epoch: 6| Step: 12
Training loss: 2.673347468953166
Validation loss: 2.6054568028999006

Epoch: 6| Step: 13
Training loss: 2.9615416703030544
Validation loss: 2.610772532631872

Epoch: 75| Step: 0
Training loss: 2.988899675591664
Validation loss: 2.6352796349111225

Epoch: 6| Step: 1
Training loss: 3.093850952967393
Validation loss: 2.6714141276474286

Epoch: 6| Step: 2
Training loss: 2.781721696608342
Validation loss: 2.6804180906648645

Epoch: 6| Step: 3
Training loss: 2.6126987509438577
Validation loss: 2.6893379535346282

Epoch: 6| Step: 4
Training loss: 3.171234817919629
Validation loss: 2.6309582944211822

Epoch: 6| Step: 5
Training loss: 3.023367316641405
Validation loss: 2.595409732560472

Epoch: 6| Step: 6
Training loss: 2.9620159671206263
Validation loss: 2.5673328938808058

Epoch: 6| Step: 7
Training loss: 2.601387819949712
Validation loss: 2.557659017441029

Epoch: 6| Step: 8
Training loss: 2.4693828689036583
Validation loss: 2.5678700060446746

Epoch: 6| Step: 9
Training loss: 2.9727351695749378
Validation loss: 2.565861658652963

Epoch: 6| Step: 10
Training loss: 2.7483805309504934
Validation loss: 2.5709961755752304

Epoch: 6| Step: 11
Training loss: 3.2340361929090866
Validation loss: 2.5793510855213593

Epoch: 6| Step: 12
Training loss: 3.3299077233711736
Validation loss: 2.5832726064233693

Epoch: 6| Step: 13
Training loss: 3.1622619780579684
Validation loss: 2.5829079033230617

Epoch: 76| Step: 0
Training loss: 2.7228790148287034
Validation loss: 2.576337197562123

Epoch: 6| Step: 1
Training loss: 2.933404868149536
Validation loss: 2.5783396771031017

Epoch: 6| Step: 2
Training loss: 2.8337252476725534
Validation loss: 2.5693143713200244

Epoch: 6| Step: 3
Training loss: 2.7130999556455215
Validation loss: 2.574350733013217

Epoch: 6| Step: 4
Training loss: 2.7875202947998243
Validation loss: 2.5758429288061544

Epoch: 6| Step: 5
Training loss: 2.9211211022409733
Validation loss: 2.585979507303866

Epoch: 6| Step: 6
Training loss: 2.6125507331558153
Validation loss: 2.565104552779547

Epoch: 6| Step: 7
Training loss: 3.4549867608221096
Validation loss: 2.558249614423064

Epoch: 6| Step: 8
Training loss: 3.015090499709013
Validation loss: 2.553984936131106

Epoch: 6| Step: 9
Training loss: 2.8381621407449935
Validation loss: 2.5573326100711933

Epoch: 6| Step: 10
Training loss: 2.952395071414875
Validation loss: 2.5513584060838674

Epoch: 6| Step: 11
Training loss: 3.442622165880851
Validation loss: 2.5542633998136575

Epoch: 6| Step: 12
Training loss: 2.8099479116587465
Validation loss: 2.558443512834929

Epoch: 6| Step: 13
Training loss: 2.728505139340983
Validation loss: 2.551209822926196

Epoch: 77| Step: 0
Training loss: 2.6852763535623705
Validation loss: 2.5503082206138874

Epoch: 6| Step: 1
Training loss: 2.9049743806165615
Validation loss: 2.5504130164710808

Epoch: 6| Step: 2
Training loss: 3.1096606866146126
Validation loss: 2.553587471307771

Epoch: 6| Step: 3
Training loss: 2.4150568324760715
Validation loss: 2.550995667565512

Epoch: 6| Step: 4
Training loss: 2.5428899937401286
Validation loss: 2.552233407833876

Epoch: 6| Step: 5
Training loss: 2.703627964039844
Validation loss: 2.54998283125092

Epoch: 6| Step: 6
Training loss: 3.348319523201985
Validation loss: 2.550901210925

Epoch: 6| Step: 7
Training loss: 3.799419288433601
Validation loss: 2.5490271130141706

Epoch: 6| Step: 8
Training loss: 3.030655006138995
Validation loss: 2.5520022197102574

Epoch: 6| Step: 9
Training loss: 3.14232304914734
Validation loss: 2.552565085216531

Epoch: 6| Step: 10
Training loss: 3.0155916360383146
Validation loss: 2.553357809415279

Epoch: 6| Step: 11
Training loss: 2.3194946491749597
Validation loss: 2.55672320816286

Epoch: 6| Step: 12
Training loss: 2.780748214889841
Validation loss: 2.553982418648019

Epoch: 6| Step: 13
Training loss: 2.798961777301047
Validation loss: 2.5551024776617566

Epoch: 78| Step: 0
Training loss: 2.451646103901298
Validation loss: 2.5519226733515885

Epoch: 6| Step: 1
Training loss: 2.958180616135232
Validation loss: 2.5503522512204087

Epoch: 6| Step: 2
Training loss: 3.22018341442443
Validation loss: 2.5507271661414137

Epoch: 6| Step: 3
Training loss: 3.0401556015347255
Validation loss: 2.5527729287256697

Epoch: 6| Step: 4
Training loss: 3.1008069249496057
Validation loss: 2.5578481858391857

Epoch: 6| Step: 5
Training loss: 3.391675092107132
Validation loss: 2.5613587001577374

Epoch: 6| Step: 6
Training loss: 3.4202350187696404
Validation loss: 2.5741180345943553

Epoch: 6| Step: 7
Training loss: 2.93457746203372
Validation loss: 2.5698896126836974

Epoch: 6| Step: 8
Training loss: 2.810015704426713
Validation loss: 2.5854950737568623

Epoch: 6| Step: 9
Training loss: 2.6489770390346004
Validation loss: 2.565426735058679

Epoch: 6| Step: 10
Training loss: 2.7115183271252623
Validation loss: 2.5727843055060036

Epoch: 6| Step: 11
Training loss: 2.333717745949846
Validation loss: 2.5738040725078464

Epoch: 6| Step: 12
Training loss: 2.785289787954004
Validation loss: 2.5806671506319163

Epoch: 6| Step: 13
Training loss: 2.8385834763740436
Validation loss: 2.567155300506635

Epoch: 79| Step: 0
Training loss: 2.618726135727845
Validation loss: 2.5698700722398455

Epoch: 6| Step: 1
Training loss: 2.8780014293208493
Validation loss: 2.570005218283819

Epoch: 6| Step: 2
Training loss: 2.6606446656639946
Validation loss: 2.568242980597923

Epoch: 6| Step: 3
Training loss: 2.968725585837112
Validation loss: 2.562870968213914

Epoch: 6| Step: 4
Training loss: 2.546107170582294
Validation loss: 2.5480987107352293

Epoch: 6| Step: 5
Training loss: 2.9849204318944937
Validation loss: 2.5454611115956247

Epoch: 6| Step: 6
Training loss: 3.318897044342048
Validation loss: 2.5476044496598056

Epoch: 6| Step: 7
Training loss: 2.676042485537069
Validation loss: 2.5449889629738838

Epoch: 6| Step: 8
Training loss: 2.9142186018217817
Validation loss: 2.5458026910794036

Epoch: 6| Step: 9
Training loss: 3.2557717936818267
Validation loss: 2.544912830444026

Epoch: 6| Step: 10
Training loss: 3.2494685398732477
Validation loss: 2.545359455708586

Epoch: 6| Step: 11
Training loss: 3.004729040436976
Validation loss: 2.542874061733641

Epoch: 6| Step: 12
Training loss: 2.557706116903257
Validation loss: 2.5447801230605425

Epoch: 6| Step: 13
Training loss: 3.218841995850447
Validation loss: 2.5447910524439012

Epoch: 80| Step: 0
Training loss: 2.932843027174468
Validation loss: 2.543796277195302

Epoch: 6| Step: 1
Training loss: 3.6503570538768635
Validation loss: 2.5463517446271946

Epoch: 6| Step: 2
Training loss: 2.065319330680635
Validation loss: 2.5475827246727594

Epoch: 6| Step: 3
Training loss: 3.224734178573629
Validation loss: 2.5435710418228292

Epoch: 6| Step: 4
Training loss: 2.9601834023852027
Validation loss: 2.5499005631473115

Epoch: 6| Step: 5
Training loss: 2.976084272760396
Validation loss: 2.5472509883077388

Epoch: 6| Step: 6
Training loss: 2.992038812752776
Validation loss: 2.555989929700358

Epoch: 6| Step: 7
Training loss: 3.0579509035189756
Validation loss: 2.555960609030633

Epoch: 6| Step: 8
Training loss: 2.554022372859505
Validation loss: 2.5546552762608417

Epoch: 6| Step: 9
Training loss: 3.069916289489845
Validation loss: 2.551211405599919

Epoch: 6| Step: 10
Training loss: 2.0649007061230478
Validation loss: 2.5467939838926714

Epoch: 6| Step: 11
Training loss: 2.971436900871256
Validation loss: 2.5509364317046503

Epoch: 6| Step: 12
Training loss: 3.377977258943357
Validation loss: 2.5558360488534637

Epoch: 6| Step: 13
Training loss: 2.030562533905706
Validation loss: 2.572243046861382

Epoch: 81| Step: 0
Training loss: 3.6176282972508815
Validation loss: 2.5895304520916786

Epoch: 6| Step: 1
Training loss: 2.5885435497090317
Validation loss: 2.614852424373683

Epoch: 6| Step: 2
Training loss: 2.9848919964904743
Validation loss: 2.57215854701183

Epoch: 6| Step: 3
Training loss: 2.597249072293659
Validation loss: 2.554013617998081

Epoch: 6| Step: 4
Training loss: 2.7524984454324684
Validation loss: 2.5439791562835037

Epoch: 6| Step: 5
Training loss: 2.487655680783177
Validation loss: 2.536456141424907

Epoch: 6| Step: 6
Training loss: 2.5075210924868325
Validation loss: 2.54194049446131

Epoch: 6| Step: 7
Training loss: 2.811085832890111
Validation loss: 2.5397212045077437

Epoch: 6| Step: 8
Training loss: 3.1663052369369873
Validation loss: 2.5386581212474537

Epoch: 6| Step: 9
Training loss: 2.659601587847154
Validation loss: 2.5366682316272358

Epoch: 6| Step: 10
Training loss: 3.4255390494841853
Validation loss: 2.5358206237108036

Epoch: 6| Step: 11
Training loss: 3.1534102856183113
Validation loss: 2.5369326408767487

Epoch: 6| Step: 12
Training loss: 2.842799772024622
Validation loss: 2.5354736529879194

Epoch: 6| Step: 13
Training loss: 3.0039538713309137
Validation loss: 2.536367956810643

Epoch: 82| Step: 0
Training loss: 2.5389759225022965
Validation loss: 2.5371622920958474

Epoch: 6| Step: 1
Training loss: 3.5106361086633817
Validation loss: 2.543231201843078

Epoch: 6| Step: 2
Training loss: 2.9153339792712187
Validation loss: 2.540758217366312

Epoch: 6| Step: 3
Training loss: 3.0936831072116417
Validation loss: 2.5406258716654158

Epoch: 6| Step: 4
Training loss: 3.152969014720583
Validation loss: 2.557521166092929

Epoch: 6| Step: 5
Training loss: 3.187057464397539
Validation loss: 2.5636060585171583

Epoch: 6| Step: 6
Training loss: 2.4399959793995736
Validation loss: 2.560992147678765

Epoch: 6| Step: 7
Training loss: 2.795409260712193
Validation loss: 2.578906883189492

Epoch: 6| Step: 8
Training loss: 2.905003762431507
Validation loss: 2.56395319969611

Epoch: 6| Step: 9
Training loss: 2.846082464421421
Validation loss: 2.5486172478959155

Epoch: 6| Step: 10
Training loss: 2.931465768904739
Validation loss: 2.5369736356321235

Epoch: 6| Step: 11
Training loss: 2.495575613769891
Validation loss: 2.5386823017710753

Epoch: 6| Step: 12
Training loss: 2.807769357289325
Validation loss: 2.532561827816769

Epoch: 6| Step: 13
Training loss: 2.8182761324765035
Validation loss: 2.53442744167972

Epoch: 83| Step: 0
Training loss: 3.2675200444068566
Validation loss: 2.5361632670964407

Epoch: 6| Step: 1
Training loss: 1.8830933420765803
Validation loss: 2.53990966900286

Epoch: 6| Step: 2
Training loss: 2.9595383531547528
Validation loss: 2.5367391351319313

Epoch: 6| Step: 3
Training loss: 3.2564564176361057
Validation loss: 2.5363371407966264

Epoch: 6| Step: 4
Training loss: 2.8852318843427858
Validation loss: 2.5408345061660467

Epoch: 6| Step: 5
Training loss: 2.5270395936086985
Validation loss: 2.5405420336492237

Epoch: 6| Step: 6
Training loss: 3.2162012175245573
Validation loss: 2.5419642292565094

Epoch: 6| Step: 7
Training loss: 3.424549607784112
Validation loss: 2.5408101877139235

Epoch: 6| Step: 8
Training loss: 3.1468313962307723
Validation loss: 2.539749852389837

Epoch: 6| Step: 9
Training loss: 2.5606235984405488
Validation loss: 2.5379181373550947

Epoch: 6| Step: 10
Training loss: 3.115896010018862
Validation loss: 2.5403562812763916

Epoch: 6| Step: 11
Training loss: 2.071988906145774
Validation loss: 2.540134600266488

Epoch: 6| Step: 12
Training loss: 3.308633815338141
Validation loss: 2.537878768037531

Epoch: 6| Step: 13
Training loss: 2.334750891554912
Validation loss: 2.5489731287190645

Epoch: 84| Step: 0
Training loss: 2.57632531438662
Validation loss: 2.5492821159249903

Epoch: 6| Step: 1
Training loss: 3.2930712700900626
Validation loss: 2.5625428767184975

Epoch: 6| Step: 2
Training loss: 2.803389735143973
Validation loss: 2.578533598887264

Epoch: 6| Step: 3
Training loss: 2.7039290979671593
Validation loss: 2.60928106013008

Epoch: 6| Step: 4
Training loss: 2.91940823633462
Validation loss: 2.6822873517025725

Epoch: 6| Step: 5
Training loss: 3.01296800958899
Validation loss: 2.7304023500933114

Epoch: 6| Step: 6
Training loss: 2.836970033602332
Validation loss: 2.680587999449075

Epoch: 6| Step: 7
Training loss: 2.89633390276293
Validation loss: 2.6175186087362943

Epoch: 6| Step: 8
Training loss: 2.6235405634162823
Validation loss: 2.593993365181138

Epoch: 6| Step: 9
Training loss: 3.1589730479656577
Validation loss: 2.584869624272562

Epoch: 6| Step: 10
Training loss: 2.7358645496413696
Validation loss: 2.570223382379292

Epoch: 6| Step: 11
Training loss: 2.862797924013386
Validation loss: 2.5483090442416954

Epoch: 6| Step: 12
Training loss: 3.344786866472796
Validation loss: 2.5420200163473194

Epoch: 6| Step: 13
Training loss: 2.9604060116244377
Validation loss: 2.5401947712686064

Epoch: 85| Step: 0
Training loss: 2.20139217543599
Validation loss: 2.5406690761643542

Epoch: 6| Step: 1
Training loss: 3.0455635574064366
Validation loss: 2.5356898633965512

Epoch: 6| Step: 2
Training loss: 3.1151110011815977
Validation loss: 2.535385208715995

Epoch: 6| Step: 3
Training loss: 2.5331422302879893
Validation loss: 2.5402394757249858

Epoch: 6| Step: 4
Training loss: 2.783152840143333
Validation loss: 2.548762265120918

Epoch: 6| Step: 5
Training loss: 3.602308349170693
Validation loss: 2.5568662320922555

Epoch: 6| Step: 6
Training loss: 2.8391114016044203
Validation loss: 2.5486685651102525

Epoch: 6| Step: 7
Training loss: 2.8393826326758447
Validation loss: 2.555376498638865

Epoch: 6| Step: 8
Training loss: 2.7903702605749428
Validation loss: 2.562217815918171

Epoch: 6| Step: 9
Training loss: 2.4654173750299084
Validation loss: 2.5666505398248325

Epoch: 6| Step: 10
Training loss: 3.1125714152159363
Validation loss: 2.5801640237970678

Epoch: 6| Step: 11
Training loss: 3.057482599191752
Validation loss: 2.5865559584549107

Epoch: 6| Step: 12
Training loss: 3.319796605141905
Validation loss: 2.5821087478873097

Epoch: 6| Step: 13
Training loss: 2.552831132801882
Validation loss: 2.56890393382668

Epoch: 86| Step: 0
Training loss: 3.231650859236031
Validation loss: 2.5584109155450587

Epoch: 6| Step: 1
Training loss: 2.7703189133313297
Validation loss: 2.569459328409233

Epoch: 6| Step: 2
Training loss: 2.4575253036919342
Validation loss: 2.5929688573321155

Epoch: 6| Step: 3
Training loss: 2.3834917945024374
Validation loss: 2.6171655621234406

Epoch: 6| Step: 4
Training loss: 3.012205725261611
Validation loss: 2.626077268808629

Epoch: 6| Step: 5
Training loss: 2.9708918825633592
Validation loss: 2.651722390032479

Epoch: 6| Step: 6
Training loss: 3.090019674392829
Validation loss: 2.66577609410257

Epoch: 6| Step: 7
Training loss: 2.8045686574425455
Validation loss: 2.610835993140707

Epoch: 6| Step: 8
Training loss: 2.856621088341679
Validation loss: 2.598305467664299

Epoch: 6| Step: 9
Training loss: 2.745248330701031
Validation loss: 2.5774578604796283

Epoch: 6| Step: 10
Training loss: 2.7853195763164975
Validation loss: 2.5595366668596276

Epoch: 6| Step: 11
Training loss: 3.1181892945952963
Validation loss: 2.5473653227110606

Epoch: 6| Step: 12
Training loss: 3.336741723490847
Validation loss: 2.5367160341179233

Epoch: 6| Step: 13
Training loss: 3.506454646756222
Validation loss: 2.539641992212754

Epoch: 87| Step: 0
Training loss: 3.0920794051264955
Validation loss: 2.578812610839463

Epoch: 6| Step: 1
Training loss: 2.6656612845452328
Validation loss: 2.572801065673168

Epoch: 6| Step: 2
Training loss: 3.065292953256218
Validation loss: 2.559768727848159

Epoch: 6| Step: 3
Training loss: 3.0001754709425437
Validation loss: 2.546846992685449

Epoch: 6| Step: 4
Training loss: 2.452536251099617
Validation loss: 2.540376708728039

Epoch: 6| Step: 5
Training loss: 3.0552359866635674
Validation loss: 2.536334198458205

Epoch: 6| Step: 6
Training loss: 2.8375617806182976
Validation loss: 2.545220308682578

Epoch: 6| Step: 7
Training loss: 2.5376408279501823
Validation loss: 2.5578764034405017

Epoch: 6| Step: 8
Training loss: 3.0098316895835855
Validation loss: 2.5899520448252003

Epoch: 6| Step: 9
Training loss: 2.8185765215149825
Validation loss: 2.609173120503966

Epoch: 6| Step: 10
Training loss: 3.776493327872929
Validation loss: 2.630170474434324

Epoch: 6| Step: 11
Training loss: 2.9261423863402
Validation loss: 2.634921373229914

Epoch: 6| Step: 12
Training loss: 2.8143486728915095
Validation loss: 2.6464517917177917

Epoch: 6| Step: 13
Training loss: 3.0836231679699475
Validation loss: 2.6584677127579637

Epoch: 88| Step: 0
Training loss: 2.9913006855312565
Validation loss: 2.6737015405824667

Epoch: 6| Step: 1
Training loss: 3.131728291482964
Validation loss: 2.683225955762428

Epoch: 6| Step: 2
Training loss: 2.9254168091196986
Validation loss: 2.694555280912198

Epoch: 6| Step: 3
Training loss: 3.0435265555015234
Validation loss: 2.657601445937361

Epoch: 6| Step: 4
Training loss: 2.7059177679295567
Validation loss: 2.615010359564428

Epoch: 6| Step: 5
Training loss: 3.088351384820556
Validation loss: 2.574970770706919

Epoch: 6| Step: 6
Training loss: 3.3684597930750897
Validation loss: 2.5583093463360673

Epoch: 6| Step: 7
Training loss: 2.9909077190829247
Validation loss: 2.5513271752689084

Epoch: 6| Step: 8
Training loss: 2.7702073750180234
Validation loss: 2.552167631309482

Epoch: 6| Step: 9
Training loss: 2.8347484943975645
Validation loss: 2.5514576998708036

Epoch: 6| Step: 10
Training loss: 2.9522283898472885
Validation loss: 2.5468122720055426

Epoch: 6| Step: 11
Training loss: 2.577781145689215
Validation loss: 2.559355317134167

Epoch: 6| Step: 12
Training loss: 3.019236087560759
Validation loss: 2.5485482969036606

Epoch: 6| Step: 13
Training loss: 2.137301126787562
Validation loss: 2.5480261638976964

Epoch: 89| Step: 0
Training loss: 2.9031979521915785
Validation loss: 2.5416586886044876

Epoch: 6| Step: 1
Training loss: 3.05684887843884
Validation loss: 2.5356292852110904

Epoch: 6| Step: 2
Training loss: 2.5220752735808483
Validation loss: 2.5327115535575335

Epoch: 6| Step: 3
Training loss: 3.348282211341312
Validation loss: 2.538384126544684

Epoch: 6| Step: 4
Training loss: 2.6833320056919896
Validation loss: 2.5348954247251174

Epoch: 6| Step: 5
Training loss: 3.2580534116372393
Validation loss: 2.5353311139341175

Epoch: 6| Step: 6
Training loss: 2.7340261182283956
Validation loss: 2.5339436046028094

Epoch: 6| Step: 7
Training loss: 3.0424468899231307
Validation loss: 2.5360293684601003

Epoch: 6| Step: 8
Training loss: 2.7319306701699055
Validation loss: 2.5420508602860052

Epoch: 6| Step: 9
Training loss: 2.47631555523432
Validation loss: 2.5402782756003908

Epoch: 6| Step: 10
Training loss: 2.7009199553192063
Validation loss: 2.536066500111498

Epoch: 6| Step: 11
Training loss: 3.453317490127728
Validation loss: 2.530083185201841

Epoch: 6| Step: 12
Training loss: 2.6890517789282304
Validation loss: 2.520394909438831

Epoch: 6| Step: 13
Training loss: 2.824543830027745
Validation loss: 2.5237930348613373

Epoch: 90| Step: 0
Training loss: 2.508199976758848
Validation loss: 2.5237384224185413

Epoch: 6| Step: 1
Training loss: 2.7110670427048245
Validation loss: 2.5398569534868054

Epoch: 6| Step: 2
Training loss: 2.590451360573165
Validation loss: 2.5549656091701807

Epoch: 6| Step: 3
Training loss: 3.635488529460567
Validation loss: 2.5462507102724845

Epoch: 6| Step: 4
Training loss: 2.596052687323369
Validation loss: 2.5362687646596656

Epoch: 6| Step: 5
Training loss: 2.9421836362743483
Validation loss: 2.5367664864969064

Epoch: 6| Step: 6
Training loss: 3.2513581885875946
Validation loss: 2.521365849233755

Epoch: 6| Step: 7
Training loss: 2.8238755646811824
Validation loss: 2.5149911356942445

Epoch: 6| Step: 8
Training loss: 2.8900017143693213
Validation loss: 2.519023820063649

Epoch: 6| Step: 9
Training loss: 3.0850527497021543
Validation loss: 2.52133039229883

Epoch: 6| Step: 10
Training loss: 2.8418846300346616
Validation loss: 2.521146931909993

Epoch: 6| Step: 11
Training loss: 2.490132887025097
Validation loss: 2.5267216709202085

Epoch: 6| Step: 12
Training loss: 3.0663345620103493
Validation loss: 2.529017845806077

Epoch: 6| Step: 13
Training loss: 2.823301300833924
Validation loss: 2.523235939190058

Epoch: 91| Step: 0
Training loss: 2.7418635250327075
Validation loss: 2.5231273880531484

Epoch: 6| Step: 1
Training loss: 3.404080040964514
Validation loss: 2.5187354155372446

Epoch: 6| Step: 2
Training loss: 2.9381445319640846
Validation loss: 2.5165673912801316

Epoch: 6| Step: 3
Training loss: 2.7143081685621735
Validation loss: 2.5133330463894406

Epoch: 6| Step: 4
Training loss: 2.8548715282602637
Validation loss: 2.515159441053837

Epoch: 6| Step: 5
Training loss: 2.770923861599854
Validation loss: 2.5155053308672897

Epoch: 6| Step: 6
Training loss: 2.58182948445204
Validation loss: 2.511881779686461

Epoch: 6| Step: 7
Training loss: 2.632516816059247
Validation loss: 2.513915074750536

Epoch: 6| Step: 8
Training loss: 3.0534134571384817
Validation loss: 2.5152006469707726

Epoch: 6| Step: 9
Training loss: 3.716483571513414
Validation loss: 2.5160842829869647

Epoch: 6| Step: 10
Training loss: 2.7674113393639614
Validation loss: 2.5224538882585117

Epoch: 6| Step: 11
Training loss: 2.288383305007001
Validation loss: 2.532870848423915

Epoch: 6| Step: 12
Training loss: 2.6792538127895176
Validation loss: 2.551666385091233

Epoch: 6| Step: 13
Training loss: 3.147940243814277
Validation loss: 2.563932770105525

Epoch: 92| Step: 0
Training loss: 2.9211264890808755
Validation loss: 2.6039252076136785

Epoch: 6| Step: 1
Training loss: 2.905409496551453
Validation loss: 2.6220883552127723

Epoch: 6| Step: 2
Training loss: 2.983597739097883
Validation loss: 2.6437740016377105

Epoch: 6| Step: 3
Training loss: 2.4424897986114087
Validation loss: 2.606876361115586

Epoch: 6| Step: 4
Training loss: 2.4671911798876134
Validation loss: 2.5597924085040638

Epoch: 6| Step: 5
Training loss: 2.8108417284746183
Validation loss: 2.5255614439607386

Epoch: 6| Step: 6
Training loss: 2.1847837750536763
Validation loss: 2.5088145415217205

Epoch: 6| Step: 7
Training loss: 2.5905202957179965
Validation loss: 2.5022192938612084

Epoch: 6| Step: 8
Training loss: 3.0596296911629515
Validation loss: 2.505541017860323

Epoch: 6| Step: 9
Training loss: 2.8652651628479653
Validation loss: 2.508644883598534

Epoch: 6| Step: 10
Training loss: 3.438995590841623
Validation loss: 2.524228228293088

Epoch: 6| Step: 11
Training loss: 3.4604664087978745
Validation loss: 2.5339975135129875

Epoch: 6| Step: 12
Training loss: 3.6985745699921315
Validation loss: 2.5444086159925634

Epoch: 6| Step: 13
Training loss: 2.2501589401056328
Validation loss: 2.55428681836663

Epoch: 93| Step: 0
Training loss: 3.0760695667510816
Validation loss: 2.5680900236290767

Epoch: 6| Step: 1
Training loss: 2.879050842546426
Validation loss: 2.56928901830483

Epoch: 6| Step: 2
Training loss: 3.5394852745718737
Validation loss: 2.5611253698485923

Epoch: 6| Step: 3
Training loss: 2.776148457050652
Validation loss: 2.561876630857088

Epoch: 6| Step: 4
Training loss: 3.1767177350588303
Validation loss: 2.549905248253295

Epoch: 6| Step: 5
Training loss: 2.5098632793501467
Validation loss: 2.542441401493064

Epoch: 6| Step: 6
Training loss: 3.0367488732989467
Validation loss: 2.5278831522924805

Epoch: 6| Step: 7
Training loss: 3.3187717587894587
Validation loss: 2.524541058536062

Epoch: 6| Step: 8
Training loss: 2.5992713897667383
Validation loss: 2.518669319942116

Epoch: 6| Step: 9
Training loss: 2.6947634815049057
Validation loss: 2.516004966412183

Epoch: 6| Step: 10
Training loss: 2.8496317976970795
Validation loss: 2.5308482029588197

Epoch: 6| Step: 11
Training loss: 3.0036525743220444
Validation loss: 2.5255696843465647

Epoch: 6| Step: 12
Training loss: 2.717599296405107
Validation loss: 2.5315977501062124

Epoch: 6| Step: 13
Training loss: 2.303621207087
Validation loss: 2.544798053913782

Epoch: 94| Step: 0
Training loss: 2.919895909426774
Validation loss: 2.538185103077228

Epoch: 6| Step: 1
Training loss: 3.2796961420317
Validation loss: 2.5377560701174486

Epoch: 6| Step: 2
Training loss: 3.2540193792233825
Validation loss: 2.53063782733813

Epoch: 6| Step: 3
Training loss: 2.398203487349612
Validation loss: 2.511166305720675

Epoch: 6| Step: 4
Training loss: 3.0972288974600697
Validation loss: 2.507407733488001

Epoch: 6| Step: 5
Training loss: 2.8748998209873142
Validation loss: 2.5034290095873297

Epoch: 6| Step: 6
Training loss: 2.685508433673736
Validation loss: 2.504701230262503

Epoch: 6| Step: 7
Training loss: 2.504213406533362
Validation loss: 2.506941025875853

Epoch: 6| Step: 8
Training loss: 2.614045583028373
Validation loss: 2.5092134331978992

Epoch: 6| Step: 9
Training loss: 2.8701798210565475
Validation loss: 2.5066672430477426

Epoch: 6| Step: 10
Training loss: 2.301489418252424
Validation loss: 2.512966289095168

Epoch: 6| Step: 11
Training loss: 3.2008855667228846
Validation loss: 2.516712589275488

Epoch: 6| Step: 12
Training loss: 2.9231916918598406
Validation loss: 2.533750759444997

Epoch: 6| Step: 13
Training loss: 3.416684468540863
Validation loss: 2.529247189913264

Epoch: 95| Step: 0
Training loss: 2.923194628056109
Validation loss: 2.5143091847981047

Epoch: 6| Step: 1
Training loss: 2.592762770693689
Validation loss: 2.501645764729522

Epoch: 6| Step: 2
Training loss: 2.3173458773197013
Validation loss: 2.494727528712907

Epoch: 6| Step: 3
Training loss: 3.079521830460805
Validation loss: 2.4983000328415024

Epoch: 6| Step: 4
Training loss: 3.5569111670339617
Validation loss: 2.498019128934129

Epoch: 6| Step: 5
Training loss: 3.4999623977821064
Validation loss: 2.498929032767092

Epoch: 6| Step: 6
Training loss: 3.294744444967106
Validation loss: 2.499724455999616

Epoch: 6| Step: 7
Training loss: 2.5588475691134884
Validation loss: 2.499273741248727

Epoch: 6| Step: 8
Training loss: 2.448443568452814
Validation loss: 2.500212282991843

Epoch: 6| Step: 9
Training loss: 2.5072505713532642
Validation loss: 2.50026599227833

Epoch: 6| Step: 10
Training loss: 2.1514593893372203
Validation loss: 2.5051507485060034

Epoch: 6| Step: 11
Training loss: 3.112614310130301
Validation loss: 2.499972572996577

Epoch: 6| Step: 12
Training loss: 2.443428166653754
Validation loss: 2.50034285932267

Epoch: 6| Step: 13
Training loss: 3.435914384834423
Validation loss: 2.4995190393631033

Epoch: 96| Step: 0
Training loss: 2.8042938891303404
Validation loss: 2.5000647977409534

Epoch: 6| Step: 1
Training loss: 3.5495991601437016
Validation loss: 2.5035359167619644

Epoch: 6| Step: 2
Training loss: 3.426936477869418
Validation loss: 2.5056786769225994

Epoch: 6| Step: 3
Training loss: 2.671732223889938
Validation loss: 2.510320296437865

Epoch: 6| Step: 4
Training loss: 2.7016159873066887
Validation loss: 2.5153972064262833

Epoch: 6| Step: 5
Training loss: 2.729531494294346
Validation loss: 2.521079878588713

Epoch: 6| Step: 6
Training loss: 3.1213514773028748
Validation loss: 2.534060718567613

Epoch: 6| Step: 7
Training loss: 2.754741049779956
Validation loss: 2.5260584653180143

Epoch: 6| Step: 8
Training loss: 3.0165310620201353
Validation loss: 2.5319766266789916

Epoch: 6| Step: 9
Training loss: 2.914473190207868
Validation loss: 2.5317299176584864

Epoch: 6| Step: 10
Training loss: 2.327013403224008
Validation loss: 2.51876389421707

Epoch: 6| Step: 11
Training loss: 2.396811664246824
Validation loss: 2.5205171922963383

Epoch: 6| Step: 12
Training loss: 2.726299459016631
Validation loss: 2.520617831185168

Epoch: 6| Step: 13
Training loss: 2.6313527755418518
Validation loss: 2.5198819429522676

Epoch: 97| Step: 0
Training loss: 2.399541246756399
Validation loss: 2.518127388108473

Epoch: 6| Step: 1
Training loss: 3.034253235248778
Validation loss: 2.5216426253695854

Epoch: 6| Step: 2
Training loss: 1.9559004937587177
Validation loss: 2.53852298075002

Epoch: 6| Step: 3
Training loss: 2.7669402190251926
Validation loss: 2.5517405072293093

Epoch: 6| Step: 4
Training loss: 3.206964192403754
Validation loss: 2.561301650742421

Epoch: 6| Step: 5
Training loss: 2.7058439306911812
Validation loss: 2.569485460963007

Epoch: 6| Step: 6
Training loss: 2.376637145506004
Validation loss: 2.5677573883623093

Epoch: 6| Step: 7
Training loss: 2.936739234619465
Validation loss: 2.5701357848099677

Epoch: 6| Step: 8
Training loss: 3.286695629969788
Validation loss: 2.561013887045452

Epoch: 6| Step: 9
Training loss: 3.4618327961195026
Validation loss: 2.5513830952491086

Epoch: 6| Step: 10
Training loss: 2.943899448108119
Validation loss: 2.532198233204905

Epoch: 6| Step: 11
Training loss: 2.9693858218693396
Validation loss: 2.529159644927154

Epoch: 6| Step: 12
Training loss: 2.5155843411589545
Validation loss: 2.52665516936947

Epoch: 6| Step: 13
Training loss: 3.7068644125309174
Validation loss: 2.5211544322206496

Epoch: 98| Step: 0
Training loss: 2.137351101156349
Validation loss: 2.5204591492877655

Epoch: 6| Step: 1
Training loss: 3.125009002672579
Validation loss: 2.5228085171002745

Epoch: 6| Step: 2
Training loss: 2.83558505471782
Validation loss: 2.523035540946796

Epoch: 6| Step: 3
Training loss: 2.8607797976102667
Validation loss: 2.520568687997248

Epoch: 6| Step: 4
Training loss: 2.7806593878641697
Validation loss: 2.5310971218841827

Epoch: 6| Step: 5
Training loss: 3.4216121764256475
Validation loss: 2.533854234791529

Epoch: 6| Step: 6
Training loss: 2.7515732859758075
Validation loss: 2.5381146030819264

Epoch: 6| Step: 7
Training loss: 2.976956238824477
Validation loss: 2.546294003578099

Epoch: 6| Step: 8
Training loss: 2.9361262254216673
Validation loss: 2.545551962234228

Epoch: 6| Step: 9
Training loss: 2.6751740372274035
Validation loss: 2.5430213858885033

Epoch: 6| Step: 10
Training loss: 2.6615114051220456
Validation loss: 2.554135775816331

Epoch: 6| Step: 11
Training loss: 2.9814417939301214
Validation loss: 2.5529318906639213

Epoch: 6| Step: 12
Training loss: 3.043659254284054
Validation loss: 2.5517823491131186

Epoch: 6| Step: 13
Training loss: 3.200490621864418
Validation loss: 2.543713295516495

Epoch: 99| Step: 0
Training loss: 2.8959687199036104
Validation loss: 2.5184546507328007

Epoch: 6| Step: 1
Training loss: 3.5793500244164496
Validation loss: 2.5144250558552885

Epoch: 6| Step: 2
Training loss: 3.113637178720875
Validation loss: 2.5183243706434983

Epoch: 6| Step: 3
Training loss: 3.2184125112246296
Validation loss: 2.525196665753544

Epoch: 6| Step: 4
Training loss: 2.8387344901477136
Validation loss: 2.5250574675796176

Epoch: 6| Step: 5
Training loss: 2.8046559621583813
Validation loss: 2.5318103080512238

Epoch: 6| Step: 6
Training loss: 2.638727352708338
Validation loss: 2.5359016335299476

Epoch: 6| Step: 7
Training loss: 3.169440777667197
Validation loss: 2.532112064924668

Epoch: 6| Step: 8
Training loss: 2.71961277393248
Validation loss: 2.53330136145649

Epoch: 6| Step: 9
Training loss: 2.73751877033644
Validation loss: 2.5372435339076866

Epoch: 6| Step: 10
Training loss: 3.0554089096791412
Validation loss: 2.535917857019366

Epoch: 6| Step: 11
Training loss: 2.6761549194493703
Validation loss: 2.5285989624301206

Epoch: 6| Step: 12
Training loss: 2.1441709010703476
Validation loss: 2.5300359636479515

Epoch: 6| Step: 13
Training loss: 3.223675856697248
Validation loss: 2.5262665706302805

Epoch: 100| Step: 0
Training loss: 2.8073822931631205
Validation loss: 2.5223525176621355

Epoch: 6| Step: 1
Training loss: 2.808147368419476
Validation loss: 2.520797225488796

Epoch: 6| Step: 2
Training loss: 2.6071029182256504
Validation loss: 2.5209833431196134

Epoch: 6| Step: 3
Training loss: 3.73739565700463
Validation loss: 2.521016473167732

Epoch: 6| Step: 4
Training loss: 3.0288834920032635
Validation loss: 2.5193605492813256

Epoch: 6| Step: 5
Training loss: 2.7448355124130583
Validation loss: 2.516123086511062

Epoch: 6| Step: 6
Training loss: 2.102442560670029
Validation loss: 2.5154027670621186

Epoch: 6| Step: 7
Training loss: 3.0560540072077758
Validation loss: 2.515965830926905

Epoch: 6| Step: 8
Training loss: 2.751881909176559
Validation loss: 2.515871283938115

Epoch: 6| Step: 9
Training loss: 2.8022276736004845
Validation loss: 2.521515097005629

Epoch: 6| Step: 10
Training loss: 2.140694832880949
Validation loss: 2.5287381831177655

Epoch: 6| Step: 11
Training loss: 2.9525326735249555
Validation loss: 2.545801066776908

Epoch: 6| Step: 12
Training loss: 3.1673012816998667
Validation loss: 2.5502893775498

Epoch: 6| Step: 13
Training loss: 3.3929050356130626
Validation loss: 2.5554351701004663

Epoch: 101| Step: 0
Training loss: 2.892648349607554
Validation loss: 2.5765125133073568

Epoch: 6| Step: 1
Training loss: 2.8134813080270633
Validation loss: 2.570814779614909

Epoch: 6| Step: 2
Training loss: 2.7381856469023163
Validation loss: 2.5952225710674757

Epoch: 6| Step: 3
Training loss: 2.7716828289313904
Validation loss: 2.6064348806574973

Epoch: 6| Step: 4
Training loss: 3.2530283390456023
Validation loss: 2.57504399802936

Epoch: 6| Step: 5
Training loss: 3.2884557168817237
Validation loss: 2.5662151560270408

Epoch: 6| Step: 6
Training loss: 2.94866671416682
Validation loss: 2.547970745761184

Epoch: 6| Step: 7
Training loss: 3.002900311085986
Validation loss: 2.5482268775353374

Epoch: 6| Step: 8
Training loss: 2.7201970075338555
Validation loss: 2.5386103068533448

Epoch: 6| Step: 9
Training loss: 2.7214244097294147
Validation loss: 2.5368011324496194

Epoch: 6| Step: 10
Training loss: 2.6775949387588285
Validation loss: 2.52750795967669

Epoch: 6| Step: 11
Training loss: 3.3098161008982263
Validation loss: 2.517281261889629

Epoch: 6| Step: 12
Training loss: 2.56138847248566
Validation loss: 2.507031620951183

Epoch: 6| Step: 13
Training loss: 1.982544061813318
Validation loss: 2.5022306478624885

Epoch: 102| Step: 0
Training loss: 2.9455045035435794
Validation loss: 2.507508300429551

Epoch: 6| Step: 1
Training loss: 3.1126858514044895
Validation loss: 2.5083230227685465

Epoch: 6| Step: 2
Training loss: 3.491592936924724
Validation loss: 2.5060796032097072

Epoch: 6| Step: 3
Training loss: 2.251623415735105
Validation loss: 2.5023783599330898

Epoch: 6| Step: 4
Training loss: 2.955736731072828
Validation loss: 2.5028674549830607

Epoch: 6| Step: 5
Training loss: 2.9318302710769806
Validation loss: 2.5102726877989965

Epoch: 6| Step: 6
Training loss: 2.825959277378629
Validation loss: 2.516247807198031

Epoch: 6| Step: 7
Training loss: 2.7769605154898844
Validation loss: 2.5251025811485106

Epoch: 6| Step: 8
Training loss: 3.363518293403292
Validation loss: 2.5246947255964547

Epoch: 6| Step: 9
Training loss: 2.1188683400821624
Validation loss: 2.5372515595327756

Epoch: 6| Step: 10
Training loss: 2.642514427528225
Validation loss: 2.5342166651748417

Epoch: 6| Step: 11
Training loss: 2.8124442200957342
Validation loss: 2.5243254937692856

Epoch: 6| Step: 12
Training loss: 2.43040490061986
Validation loss: 2.51527758862875

Epoch: 6| Step: 13
Training loss: 3.2738625334704454
Validation loss: 2.499273515582784

Epoch: 103| Step: 0
Training loss: 2.553964495060338
Validation loss: 2.4978696585843934

Epoch: 6| Step: 1
Training loss: 2.4226201049509593
Validation loss: 2.4880427088770527

Epoch: 6| Step: 2
Training loss: 2.5495604416722006
Validation loss: 2.4824850181624276

Epoch: 6| Step: 3
Training loss: 3.2417609612120803
Validation loss: 2.482473903797627

Epoch: 6| Step: 4
Training loss: 3.4324456249356157
Validation loss: 2.4845262490325717

Epoch: 6| Step: 5
Training loss: 2.7849214302362486
Validation loss: 2.486431741707394

Epoch: 6| Step: 6
Training loss: 3.185481311116372
Validation loss: 2.4843145357837053

Epoch: 6| Step: 7
Training loss: 2.6052430840123986
Validation loss: 2.4884290236137714

Epoch: 6| Step: 8
Training loss: 3.132385985909412
Validation loss: 2.4813756438401247

Epoch: 6| Step: 9
Training loss: 2.577238173113255
Validation loss: 2.4835589650002943

Epoch: 6| Step: 10
Training loss: 3.197761772123953
Validation loss: 2.4822145658518613

Epoch: 6| Step: 11
Training loss: 2.272575293574631
Validation loss: 2.483797868615332

Epoch: 6| Step: 12
Training loss: 2.9803375575238302
Validation loss: 2.4938640347486536

Epoch: 6| Step: 13
Training loss: 3.1845408148453704
Validation loss: 2.496006158643786

Epoch: 104| Step: 0
Training loss: 2.765124604382014
Validation loss: 2.5045619360329043

Epoch: 6| Step: 1
Training loss: 3.184440191348456
Validation loss: 2.505229329131708

Epoch: 6| Step: 2
Training loss: 2.0273398691482
Validation loss: 2.500417775963404

Epoch: 6| Step: 3
Training loss: 2.904006912748658
Validation loss: 2.501431754525887

Epoch: 6| Step: 4
Training loss: 2.3599646633541513
Validation loss: 2.5189957381813786

Epoch: 6| Step: 5
Training loss: 3.109830736662907
Validation loss: 2.5160029524901866

Epoch: 6| Step: 6
Training loss: 3.077277517078084
Validation loss: 2.5251535224195623

Epoch: 6| Step: 7
Training loss: 2.9545373516371796
Validation loss: 2.5262361519913332

Epoch: 6| Step: 8
Training loss: 3.0334386101643367
Validation loss: 2.528688448021382

Epoch: 6| Step: 9
Training loss: 3.506463214014976
Validation loss: 2.520012302744651

Epoch: 6| Step: 10
Training loss: 2.4775146194473194
Validation loss: 2.509430935642521

Epoch: 6| Step: 11
Training loss: 3.1569661753706013
Validation loss: 2.491306576781283

Epoch: 6| Step: 12
Training loss: 2.45562344778958
Validation loss: 2.4798766998227646

Epoch: 6| Step: 13
Training loss: 2.1935570917327776
Validation loss: 2.4804757717977246

Epoch: 105| Step: 0
Training loss: 3.214566372925341
Validation loss: 2.4805550979210667

Epoch: 6| Step: 1
Training loss: 3.1278668795403504
Validation loss: 2.480933937827311

Epoch: 6| Step: 2
Training loss: 2.692437094942682
Validation loss: 2.486290229905741

Epoch: 6| Step: 3
Training loss: 2.807721125793488
Validation loss: 2.4828113762081094

Epoch: 6| Step: 4
Training loss: 2.2535368353840184
Validation loss: 2.482208618960458

Epoch: 6| Step: 5
Training loss: 2.556420627859232
Validation loss: 2.483227273331104

Epoch: 6| Step: 6
Training loss: 3.326034534103021
Validation loss: 2.4940072641331117

Epoch: 6| Step: 7
Training loss: 2.869997731384019
Validation loss: 2.495623071335719

Epoch: 6| Step: 8
Training loss: 3.103542512617559
Validation loss: 2.4929731891296507

Epoch: 6| Step: 9
Training loss: 2.227938899204021
Validation loss: 2.4955040384354605

Epoch: 6| Step: 10
Training loss: 2.684972062063227
Validation loss: 2.494658095337245

Epoch: 6| Step: 11
Training loss: 3.2446419491014185
Validation loss: 2.494528128901022

Epoch: 6| Step: 12
Training loss: 2.348530991978173
Validation loss: 2.4943164033039347

Epoch: 6| Step: 13
Training loss: 3.1844321053937237
Validation loss: 2.4952490948528343

Epoch: 106| Step: 0
Training loss: 3.0602199864317674
Validation loss: 2.49259464141136

Epoch: 6| Step: 1
Training loss: 3.0338397417284932
Validation loss: 2.4935182180629067

Epoch: 6| Step: 2
Training loss: 2.5611423524544925
Validation loss: 2.4933183020307053

Epoch: 6| Step: 3
Training loss: 2.5299824496176915
Validation loss: 2.490080819556053

Epoch: 6| Step: 4
Training loss: 2.342014331281875
Validation loss: 2.4898637220778617

Epoch: 6| Step: 5
Training loss: 3.0373412604905337
Validation loss: 2.4857473691847565

Epoch: 6| Step: 6
Training loss: 2.9619457772679745
Validation loss: 2.4883745097155723

Epoch: 6| Step: 7
Training loss: 2.8615449269933997
Validation loss: 2.4831073406127127

Epoch: 6| Step: 8
Training loss: 3.185226676935171
Validation loss: 2.4856007769577086

Epoch: 6| Step: 9
Training loss: 2.585920443291321
Validation loss: 2.488098999175752

Epoch: 6| Step: 10
Training loss: 2.8624930968888513
Validation loss: 2.491770461488644

Epoch: 6| Step: 11
Training loss: 3.0688280394783725
Validation loss: 2.4934874359038504

Epoch: 6| Step: 12
Training loss: 2.7344916618528345
Validation loss: 2.5100781374129455

Epoch: 6| Step: 13
Training loss: 2.7692319119076125
Validation loss: 2.5192431935871618

Epoch: 107| Step: 0
Training loss: 3.0840986350251454
Validation loss: 2.5082942640588115

Epoch: 6| Step: 1
Training loss: 2.4601642665447745
Validation loss: 2.4996999611826816

Epoch: 6| Step: 2
Training loss: 2.8784045088876353
Validation loss: 2.493128132503288

Epoch: 6| Step: 3
Training loss: 2.9730659032164364
Validation loss: 2.4876634531399455

Epoch: 6| Step: 4
Training loss: 3.0575597971498403
Validation loss: 2.484949764819253

Epoch: 6| Step: 5
Training loss: 2.80201172766726
Validation loss: 2.491682488713651

Epoch: 6| Step: 6
Training loss: 2.2039092271116014
Validation loss: 2.4927894414913756

Epoch: 6| Step: 7
Training loss: 3.0952249729311503
Validation loss: 2.499397937511949

Epoch: 6| Step: 8
Training loss: 2.567380769481405
Validation loss: 2.4907040628273234

Epoch: 6| Step: 9
Training loss: 3.463545735674693
Validation loss: 2.491166960659561

Epoch: 6| Step: 10
Training loss: 2.71617855492422
Validation loss: 2.4891686355210227

Epoch: 6| Step: 11
Training loss: 2.7814187845170855
Validation loss: 2.490088144730461

Epoch: 6| Step: 12
Training loss: 2.152915587851011
Validation loss: 2.483816557602441

Epoch: 6| Step: 13
Training loss: 3.235750946900681
Validation loss: 2.4874090694932898

Epoch: 108| Step: 0
Training loss: 3.004124666961815
Validation loss: 2.4806999632988096

Epoch: 6| Step: 1
Training loss: 2.699113636758056
Validation loss: 2.4930755116351473

Epoch: 6| Step: 2
Training loss: 3.2303778233122604
Validation loss: 2.489250628036104

Epoch: 6| Step: 3
Training loss: 2.4700678917206855
Validation loss: 2.493676078096137

Epoch: 6| Step: 4
Training loss: 2.8152156011432607
Validation loss: 2.5039588398357404

Epoch: 6| Step: 5
Training loss: 2.666107735073189
Validation loss: 2.5065945331387707

Epoch: 6| Step: 6
Training loss: 3.169022202697767
Validation loss: 2.5061796454918124

Epoch: 6| Step: 7
Training loss: 3.5155322253383736
Validation loss: 2.520644313471925

Epoch: 6| Step: 8
Training loss: 2.2223619960166507
Validation loss: 2.5007543256447984

Epoch: 6| Step: 9
Training loss: 2.76363512425874
Validation loss: 2.491604000445853

Epoch: 6| Step: 10
Training loss: 2.75368573543662
Validation loss: 2.488531115740782

Epoch: 6| Step: 11
Training loss: 2.361628182832809
Validation loss: 2.490997582781061

Epoch: 6| Step: 12
Training loss: 3.06104991603207
Validation loss: 2.490099949386418

Epoch: 6| Step: 13
Training loss: 2.319220083455149
Validation loss: 2.4882570381678204

Epoch: 109| Step: 0
Training loss: 2.177661460032132
Validation loss: 2.483794223074975

Epoch: 6| Step: 1
Training loss: 2.8640482084809484
Validation loss: 2.4843307979276346

Epoch: 6| Step: 2
Training loss: 3.245620123765714
Validation loss: 2.485862442986972

Epoch: 6| Step: 3
Training loss: 2.0756265636462325
Validation loss: 2.49832043064795

Epoch: 6| Step: 4
Training loss: 2.609000721660498
Validation loss: 2.500611017273494

Epoch: 6| Step: 5
Training loss: 2.6549816918325564
Validation loss: 2.504272755605074

Epoch: 6| Step: 6
Training loss: 2.7631417888277134
Validation loss: 2.50489493941636

Epoch: 6| Step: 7
Training loss: 3.4315308502824844
Validation loss: 2.512269694172842

Epoch: 6| Step: 8
Training loss: 3.538420023084544
Validation loss: 2.504253065554251

Epoch: 6| Step: 9
Training loss: 2.596238746232982
Validation loss: 2.5054128950192847

Epoch: 6| Step: 10
Training loss: 2.9226760174272206
Validation loss: 2.5167589810684405

Epoch: 6| Step: 11
Training loss: 2.973056600833192
Validation loss: 2.516220372885911

Epoch: 6| Step: 12
Training loss: 2.568065832500435
Validation loss: 2.508566810570897

Epoch: 6| Step: 13
Training loss: 2.6530231397865567
Validation loss: 2.500150304808521

Epoch: 110| Step: 0
Training loss: 2.4118738221587988
Validation loss: 2.4921349127626287

Epoch: 6| Step: 1
Training loss: 2.311611004658684
Validation loss: 2.49020637099147

Epoch: 6| Step: 2
Training loss: 3.364338742903567
Validation loss: 2.486033277743946

Epoch: 6| Step: 3
Training loss: 2.656629108975555
Validation loss: 2.5005786820770686

Epoch: 6| Step: 4
Training loss: 2.6572993281508333
Validation loss: 2.486481971995579

Epoch: 6| Step: 5
Training loss: 2.3006174129577572
Validation loss: 2.494670707679611

Epoch: 6| Step: 6
Training loss: 3.1570330866426235
Validation loss: 2.492804915087598

Epoch: 6| Step: 7
Training loss: 2.963516923547484
Validation loss: 2.5008289511277204

Epoch: 6| Step: 8
Training loss: 2.392995220340558
Validation loss: 2.509470264983044

Epoch: 6| Step: 9
Training loss: 2.977051061549212
Validation loss: 2.5103373847961947

Epoch: 6| Step: 10
Training loss: 3.206663531132022
Validation loss: 2.5181674174514876

Epoch: 6| Step: 11
Training loss: 2.8394123573541084
Validation loss: 2.525857725678446

Epoch: 6| Step: 12
Training loss: 3.235523996499081
Validation loss: 2.526096096706206

Epoch: 6| Step: 13
Training loss: 2.3601448874529942
Validation loss: 2.5198509843337598

Epoch: 111| Step: 0
Training loss: 2.87763822831698
Validation loss: 2.5152184594761713

Epoch: 6| Step: 1
Training loss: 2.730858421625629
Validation loss: 2.509688738065016

Epoch: 6| Step: 2
Training loss: 3.1502627248041595
Validation loss: 2.491665149978679

Epoch: 6| Step: 3
Training loss: 3.3263817461680842
Validation loss: 2.4894030874684594

Epoch: 6| Step: 4
Training loss: 3.0385349472820766
Validation loss: 2.499396011240467

Epoch: 6| Step: 5
Training loss: 2.328397594084854
Validation loss: 2.490821536301852

Epoch: 6| Step: 6
Training loss: 2.6313575777003764
Validation loss: 2.4885921429902003

Epoch: 6| Step: 7
Training loss: 3.1297121760233058
Validation loss: 2.490871825244186

Epoch: 6| Step: 8
Training loss: 2.3627458959101246
Validation loss: 2.4877089593789954

Epoch: 6| Step: 9
Training loss: 2.4264819228235663
Validation loss: 2.497161337540724

Epoch: 6| Step: 10
Training loss: 2.1598732402482397
Validation loss: 2.4967291230951596

Epoch: 6| Step: 11
Training loss: 2.7476406380060983
Validation loss: 2.5183635488793046

Epoch: 6| Step: 12
Training loss: 2.9903791655425467
Validation loss: 2.5231740124349065

Epoch: 6| Step: 13
Training loss: 3.22523408380302
Validation loss: 2.5383069025855547

Epoch: 112| Step: 0
Training loss: 1.9955070215578166
Validation loss: 2.5419235671226144

Epoch: 6| Step: 1
Training loss: 2.7835329951171777
Validation loss: 2.534099007063787

Epoch: 6| Step: 2
Training loss: 2.8518029516558894
Validation loss: 2.520833894029021

Epoch: 6| Step: 3
Training loss: 3.3802703397184195
Validation loss: 2.5121151150124907

Epoch: 6| Step: 4
Training loss: 2.6938406201053433
Validation loss: 2.5071615200258033

Epoch: 6| Step: 5
Training loss: 3.0071752888846723
Validation loss: 2.508441821424431

Epoch: 6| Step: 6
Training loss: 2.300120856385484
Validation loss: 2.5444185525155025

Epoch: 6| Step: 7
Training loss: 3.248113965266592
Validation loss: 2.593382494920456

Epoch: 6| Step: 8
Training loss: 2.5244593013052388
Validation loss: 2.5768861988211054

Epoch: 6| Step: 9
Training loss: 3.06796648702011
Validation loss: 2.549905350802729

Epoch: 6| Step: 10
Training loss: 3.4376136067430756
Validation loss: 2.5147008210351265

Epoch: 6| Step: 11
Training loss: 2.625186368593406
Validation loss: 2.4925618545826036

Epoch: 6| Step: 12
Training loss: 2.4596615344482236
Validation loss: 2.484778896321707

Epoch: 6| Step: 13
Training loss: 2.702021555511761
Validation loss: 2.4879977569169016

Epoch: 113| Step: 0
Training loss: 3.099496302137709
Validation loss: 2.4972702751525016

Epoch: 6| Step: 1
Training loss: 2.135883286519948
Validation loss: 2.512701244372833

Epoch: 6| Step: 2
Training loss: 2.51696779415849
Validation loss: 2.541183414307485

Epoch: 6| Step: 3
Training loss: 2.742039777031774
Validation loss: 2.54309134662505

Epoch: 6| Step: 4
Training loss: 3.3844329969908706
Validation loss: 2.551135163825273

Epoch: 6| Step: 5
Training loss: 3.0781166347641724
Validation loss: 2.5615680199310176

Epoch: 6| Step: 6
Training loss: 2.5051565396248
Validation loss: 2.5564837679036523

Epoch: 6| Step: 7
Training loss: 2.4109578832035408
Validation loss: 2.541052781650958

Epoch: 6| Step: 8
Training loss: 2.531070090599593
Validation loss: 2.5403698151626553

Epoch: 6| Step: 9
Training loss: 2.905154770423136
Validation loss: 2.530778614888788

Epoch: 6| Step: 10
Training loss: 2.1807532764162243
Validation loss: 2.4987466541659744

Epoch: 6| Step: 11
Training loss: 3.377770170055433
Validation loss: 2.484230705472452

Epoch: 6| Step: 12
Training loss: 3.355908117516463
Validation loss: 2.47287006131239

Epoch: 6| Step: 13
Training loss: 2.8199932731385857
Validation loss: 2.468885596380612

Epoch: 114| Step: 0
Training loss: 2.8274747828160973
Validation loss: 2.4678941216358825

Epoch: 6| Step: 1
Training loss: 2.8141814397895364
Validation loss: 2.4682102747879875

Epoch: 6| Step: 2
Training loss: 3.127793398724567
Validation loss: 2.478737604148725

Epoch: 6| Step: 3
Training loss: 2.9625352559627482
Validation loss: 2.4823088487201534

Epoch: 6| Step: 4
Training loss: 3.1263260126157992
Validation loss: 2.4708913620904

Epoch: 6| Step: 5
Training loss: 2.8221569845547556
Validation loss: 2.475514380702825

Epoch: 6| Step: 6
Training loss: 3.3004093841008775
Validation loss: 2.500365942147093

Epoch: 6| Step: 7
Training loss: 2.367901785572912
Validation loss: 2.5323592094841323

Epoch: 6| Step: 8
Training loss: 2.700526052020532
Validation loss: 2.5249580514242407

Epoch: 6| Step: 9
Training loss: 2.47156238918133
Validation loss: 2.524939778642466

Epoch: 6| Step: 10
Training loss: 2.7273927315406095
Validation loss: 2.5192031131560437

Epoch: 6| Step: 11
Training loss: 2.87931367431675
Validation loss: 2.5168749257430116

Epoch: 6| Step: 12
Training loss: 2.9133715590261366
Validation loss: 2.5299362425459613

Epoch: 6| Step: 13
Training loss: 2.043681208301469
Validation loss: 2.5283346662168302

Epoch: 115| Step: 0
Training loss: 2.591844525219002
Validation loss: 2.5178160733212915

Epoch: 6| Step: 1
Training loss: 2.2602009003728822
Validation loss: 2.5136242329505025

Epoch: 6| Step: 2
Training loss: 2.6877191143757493
Validation loss: 2.506215134694921

Epoch: 6| Step: 3
Training loss: 2.6822939283244156
Validation loss: 2.5033347930134924

Epoch: 6| Step: 4
Training loss: 3.125590764472074
Validation loss: 2.4818792403500756

Epoch: 6| Step: 5
Training loss: 2.390516091652228
Validation loss: 2.483881304818879

Epoch: 6| Step: 6
Training loss: 2.52364686268678
Validation loss: 2.4754471362642754

Epoch: 6| Step: 7
Training loss: 3.2166077234537056
Validation loss: 2.4631527968386355

Epoch: 6| Step: 8
Training loss: 2.8483085565473383
Validation loss: 2.466352860514681

Epoch: 6| Step: 9
Training loss: 3.117089138534668
Validation loss: 2.4711422236710123

Epoch: 6| Step: 10
Training loss: 3.1525080187799173
Validation loss: 2.466317999328084

Epoch: 6| Step: 11
Training loss: 2.68613984930775
Validation loss: 2.470164065245151

Epoch: 6| Step: 12
Training loss: 2.4450607413955527
Validation loss: 2.47415047711681

Epoch: 6| Step: 13
Training loss: 3.550635407225578
Validation loss: 2.4708048642487395

Epoch: 116| Step: 0
Training loss: 2.819742668260698
Validation loss: 2.468186072764389

Epoch: 6| Step: 1
Training loss: 3.0200640170152218
Validation loss: 2.4805936996825872

Epoch: 6| Step: 2
Training loss: 3.1192702599078688
Validation loss: 2.478537600599802

Epoch: 6| Step: 3
Training loss: 3.0495609280087677
Validation loss: 2.473800637228296

Epoch: 6| Step: 4
Training loss: 2.951506963624386
Validation loss: 2.4732935507846183

Epoch: 6| Step: 5
Training loss: 2.1039904781894836
Validation loss: 2.483869879326424

Epoch: 6| Step: 6
Training loss: 3.1832261698262347
Validation loss: 2.489698974653117

Epoch: 6| Step: 7
Training loss: 2.779344570369915
Validation loss: 2.5085121190684614

Epoch: 6| Step: 8
Training loss: 2.758572481819387
Validation loss: 2.487566139255859

Epoch: 6| Step: 9
Training loss: 2.539067664507969
Validation loss: 2.474879342331824

Epoch: 6| Step: 10
Training loss: 2.5900917452318675
Validation loss: 2.46728159489245

Epoch: 6| Step: 11
Training loss: 3.340527702035109
Validation loss: 2.4554539625461054

Epoch: 6| Step: 12
Training loss: 1.73169563149297
Validation loss: 2.471192047888492

Epoch: 6| Step: 13
Training loss: 2.679180486553455
Validation loss: 2.4689689550556935

Epoch: 117| Step: 0
Training loss: 2.0693631280658162
Validation loss: 2.4740896687298846

Epoch: 6| Step: 1
Training loss: 3.127566694955758
Validation loss: 2.483664078320521

Epoch: 6| Step: 2
Training loss: 2.958560200620119
Validation loss: 2.484423396936177

Epoch: 6| Step: 3
Training loss: 2.7377777725105505
Validation loss: 2.5064217791994574

Epoch: 6| Step: 4
Training loss: 2.486683474766002
Validation loss: 2.5337947105002865

Epoch: 6| Step: 5
Training loss: 3.2230370868723326
Validation loss: 2.5322754166787584

Epoch: 6| Step: 6
Training loss: 2.559334544326696
Validation loss: 2.515661876410475

Epoch: 6| Step: 7
Training loss: 3.041862394401997
Validation loss: 2.494508925119452

Epoch: 6| Step: 8
Training loss: 2.4066217308745426
Validation loss: 2.4720238934854333

Epoch: 6| Step: 9
Training loss: 3.099397071533631
Validation loss: 2.467517978912615

Epoch: 6| Step: 10
Training loss: 2.98527984483082
Validation loss: 2.4711881524089065

Epoch: 6| Step: 11
Training loss: 2.869046723222998
Validation loss: 2.4699852593104845

Epoch: 6| Step: 12
Training loss: 2.916991860562419
Validation loss: 2.4642209885477095

Epoch: 6| Step: 13
Training loss: 2.2016334625135214
Validation loss: 2.468163854389418

Epoch: 118| Step: 0
Training loss: 3.058006571291742
Validation loss: 2.4584280991334144

Epoch: 6| Step: 1
Training loss: 3.0075962533871916
Validation loss: 2.467905490202405

Epoch: 6| Step: 2
Training loss: 2.183414458692003
Validation loss: 2.4652818759404833

Epoch: 6| Step: 3
Training loss: 2.335400778052542
Validation loss: 2.464808142521391

Epoch: 6| Step: 4
Training loss: 3.2011587667516856
Validation loss: 2.4650350896551823

Epoch: 6| Step: 5
Training loss: 2.729873264850362
Validation loss: 2.4732231248881265

Epoch: 6| Step: 6
Training loss: 3.0001314452303034
Validation loss: 2.4817845630893296

Epoch: 6| Step: 7
Training loss: 2.392715836279175
Validation loss: 2.476238818198497

Epoch: 6| Step: 8
Training loss: 2.8261409143572362
Validation loss: 2.4699308272094154

Epoch: 6| Step: 9
Training loss: 2.221915015867627
Validation loss: 2.4689107115421876

Epoch: 6| Step: 10
Training loss: 3.069448257221445
Validation loss: 2.4725285009649736

Epoch: 6| Step: 11
Training loss: 3.1926352330230485
Validation loss: 2.4762581657382743

Epoch: 6| Step: 12
Training loss: 2.7482241185011596
Validation loss: 2.4944871775781143

Epoch: 6| Step: 13
Training loss: 2.4397986894954413
Validation loss: 2.506816008846955

Epoch: 119| Step: 0
Training loss: 2.680166663211512
Validation loss: 2.529690427003887

Epoch: 6| Step: 1
Training loss: 3.1450866587818465
Validation loss: 2.530537265795405

Epoch: 6| Step: 2
Training loss: 2.883165126002815
Validation loss: 2.5494022549886397

Epoch: 6| Step: 3
Training loss: 3.2305950984209404
Validation loss: 2.5574626467379766

Epoch: 6| Step: 4
Training loss: 2.548555720596732
Validation loss: 2.5368201130932606

Epoch: 6| Step: 5
Training loss: 3.0283700692260322
Validation loss: 2.528197696233363

Epoch: 6| Step: 6
Training loss: 2.8146015369006387
Validation loss: 2.501402891989873

Epoch: 6| Step: 7
Training loss: 3.17075963446394
Validation loss: 2.4907344553255584

Epoch: 6| Step: 8
Training loss: 2.6547655333242273
Validation loss: 2.479559970500085

Epoch: 6| Step: 9
Training loss: 2.29173523771405
Validation loss: 2.4750102662355946

Epoch: 6| Step: 10
Training loss: 3.1373109555379237
Validation loss: 2.4767597260088796

Epoch: 6| Step: 11
Training loss: 2.5281079417856795
Validation loss: 2.4734349572921346

Epoch: 6| Step: 12
Training loss: 1.7320874223560663
Validation loss: 2.469838707255861

Epoch: 6| Step: 13
Training loss: 2.6363067531129203
Validation loss: 2.465478351557843

Epoch: 120| Step: 0
Training loss: 2.2319792594903123
Validation loss: 2.475423770330843

Epoch: 6| Step: 1
Training loss: 2.6836660672456696
Validation loss: 2.505373276869669

Epoch: 6| Step: 2
Training loss: 3.1774018341675774
Validation loss: 2.542670598521619

Epoch: 6| Step: 3
Training loss: 3.010305981212691
Validation loss: 2.5828339123410755

Epoch: 6| Step: 4
Training loss: 3.269279224157492
Validation loss: 2.5644484344822653

Epoch: 6| Step: 5
Training loss: 3.4409164097418246
Validation loss: 2.5525021676251822

Epoch: 6| Step: 6
Training loss: 2.655012672810743
Validation loss: 2.512879121612319

Epoch: 6| Step: 7
Training loss: 2.8024731238486122
Validation loss: 2.4926655021919077

Epoch: 6| Step: 8
Training loss: 2.084998787529682
Validation loss: 2.4765404675376654

Epoch: 6| Step: 9
Training loss: 2.654975136390832
Validation loss: 2.474776942896592

Epoch: 6| Step: 10
Training loss: 2.6759849303752095
Validation loss: 2.4614993655727173

Epoch: 6| Step: 11
Training loss: 2.766887656735032
Validation loss: 2.4613058074710605

Epoch: 6| Step: 12
Training loss: 2.903303724357674
Validation loss: 2.463168616893556

Epoch: 6| Step: 13
Training loss: 2.506397168792431
Validation loss: 2.4585562674953514

Epoch: 121| Step: 0
Training loss: 3.0682487249304136
Validation loss: 2.465540562366635

Epoch: 6| Step: 1
Training loss: 2.8624439550518694
Validation loss: 2.484444948804565

Epoch: 6| Step: 2
Training loss: 2.4755683130844526
Validation loss: 2.4896992959191957

Epoch: 6| Step: 3
Training loss: 2.3956413744997067
Validation loss: 2.5019436909767983

Epoch: 6| Step: 4
Training loss: 2.688552384254783
Validation loss: 2.5104044380923267

Epoch: 6| Step: 5
Training loss: 2.619444463232514
Validation loss: 2.541724763310824

Epoch: 6| Step: 6
Training loss: 2.95523093081254
Validation loss: 2.5763259084471892

Epoch: 6| Step: 7
Training loss: 3.069099167112526
Validation loss: 2.5907807555766866

Epoch: 6| Step: 8
Training loss: 2.269132332861366
Validation loss: 2.5950424437590853

Epoch: 6| Step: 9
Training loss: 2.7760919467363863
Validation loss: 2.609903114248396

Epoch: 6| Step: 10
Training loss: 2.892959888927828
Validation loss: 2.577522458502531

Epoch: 6| Step: 11
Training loss: 2.807099052054133
Validation loss: 2.527699295274593

Epoch: 6| Step: 12
Training loss: 3.1280344154435418
Validation loss: 2.4801591644607064

Epoch: 6| Step: 13
Training loss: 2.8508115850974063
Validation loss: 2.4736454024959884

Epoch: 122| Step: 0
Training loss: 2.375596925107421
Validation loss: 2.466558359784288

Epoch: 6| Step: 1
Training loss: 3.085590429948797
Validation loss: 2.466009719674786

Epoch: 6| Step: 2
Training loss: 2.8418394944183833
Validation loss: 2.471411417280242

Epoch: 6| Step: 3
Training loss: 2.299019363992122
Validation loss: 2.4702716362901205

Epoch: 6| Step: 4
Training loss: 2.66177171229549
Validation loss: 2.4674103376891887

Epoch: 6| Step: 5
Training loss: 3.6212246407202593
Validation loss: 2.4622987075569966

Epoch: 6| Step: 6
Training loss: 2.442679355560617
Validation loss: 2.4594283354888447

Epoch: 6| Step: 7
Training loss: 2.806958142758412
Validation loss: 2.4630936643205152

Epoch: 6| Step: 8
Training loss: 3.0010792062660916
Validation loss: 2.4910498863149737

Epoch: 6| Step: 9
Training loss: 2.5993765963939066
Validation loss: 2.54733099555475

Epoch: 6| Step: 10
Training loss: 2.062150174153007
Validation loss: 2.5901723067491154

Epoch: 6| Step: 11
Training loss: 2.906502333320693
Validation loss: 2.6613119402293632

Epoch: 6| Step: 12
Training loss: 3.045923485489346
Validation loss: 2.673427241956039

Epoch: 6| Step: 13
Training loss: 3.124976196198403
Validation loss: 2.7131305914085853

Epoch: 123| Step: 0
Training loss: 3.6210412080279917
Validation loss: 2.694274318136313

Epoch: 6| Step: 1
Training loss: 3.032440580963656
Validation loss: 2.6344832933894535

Epoch: 6| Step: 2
Training loss: 2.3936037940252803
Validation loss: 2.5936784276021196

Epoch: 6| Step: 3
Training loss: 2.7510191156131567
Validation loss: 2.5073856377270505

Epoch: 6| Step: 4
Training loss: 2.7289486475243896
Validation loss: 2.459099687857522

Epoch: 6| Step: 5
Training loss: 2.900261893449994
Validation loss: 2.453451443839028

Epoch: 6| Step: 6
Training loss: 2.5900739794737
Validation loss: 2.466709083080467

Epoch: 6| Step: 7
Training loss: 2.4864632329318535
Validation loss: 2.48924470001387

Epoch: 6| Step: 8
Training loss: 2.992184950227373
Validation loss: 2.523906554830207

Epoch: 6| Step: 9
Training loss: 2.991468378011144
Validation loss: 2.512006522150915

Epoch: 6| Step: 10
Training loss: 2.602921414514222
Validation loss: 2.483927159012728

Epoch: 6| Step: 11
Training loss: 2.866851204839288
Validation loss: 2.470075133017395

Epoch: 6| Step: 12
Training loss: 2.843980549280367
Validation loss: 2.4745783306913776

Epoch: 6| Step: 13
Training loss: 2.877022736555997
Validation loss: 2.4835685725994687

Epoch: 124| Step: 0
Training loss: 2.6122773070994993
Validation loss: 2.492920245168366

Epoch: 6| Step: 1
Training loss: 2.7713816585418627
Validation loss: 2.4931027163091692

Epoch: 6| Step: 2
Training loss: 2.7856844579056927
Validation loss: 2.5057114813968466

Epoch: 6| Step: 3
Training loss: 2.3304899352207644
Validation loss: 2.5211616823641614

Epoch: 6| Step: 4
Training loss: 2.492293304780403
Validation loss: 2.546883362676391

Epoch: 6| Step: 5
Training loss: 3.0886223424932258
Validation loss: 2.586281090649147

Epoch: 6| Step: 6
Training loss: 2.8431855364627503
Validation loss: 2.5657036916416653

Epoch: 6| Step: 7
Training loss: 2.40112571541619
Validation loss: 2.5446593587622353

Epoch: 6| Step: 8
Training loss: 2.5989407912728115
Validation loss: 2.545328205675864

Epoch: 6| Step: 9
Training loss: 2.317350198457553
Validation loss: 2.511617987253334

Epoch: 6| Step: 10
Training loss: 3.2478407509385416
Validation loss: 2.496949678136663

Epoch: 6| Step: 11
Training loss: 2.9599921899125445
Validation loss: 2.4954688437884034

Epoch: 6| Step: 12
Training loss: 2.932952607618329
Validation loss: 2.4821888891837918

Epoch: 6| Step: 13
Training loss: 3.060342924312909
Validation loss: 2.4793881918576806

Epoch: 125| Step: 0
Training loss: 2.5585102562712763
Validation loss: 2.4761740740690112

Epoch: 6| Step: 1
Training loss: 3.3932678891849135
Validation loss: 2.469888560882606

Epoch: 6| Step: 2
Training loss: 2.664946557630996
Validation loss: 2.4738958252208176

Epoch: 6| Step: 3
Training loss: 2.3260556439396742
Validation loss: 2.4732192377904196

Epoch: 6| Step: 4
Training loss: 3.080469158655343
Validation loss: 2.478735790069125

Epoch: 6| Step: 5
Training loss: 2.5979287370300916
Validation loss: 2.4700597848059194

Epoch: 6| Step: 6
Training loss: 2.698058390975948
Validation loss: 2.470865421459982

Epoch: 6| Step: 7
Training loss: 3.3856136010428606
Validation loss: 2.4720896900093248

Epoch: 6| Step: 8
Training loss: 2.813606976840101
Validation loss: 2.4677581840528187

Epoch: 6| Step: 9
Training loss: 2.3027187408356564
Validation loss: 2.47978548072335

Epoch: 6| Step: 10
Training loss: 2.176936556843578
Validation loss: 2.478060166150516

Epoch: 6| Step: 11
Training loss: 2.2849028096303763
Validation loss: 2.495597407298528

Epoch: 6| Step: 12
Training loss: 2.4947861662759587
Validation loss: 2.5085159003789563

Epoch: 6| Step: 13
Training loss: 3.47319584399261
Validation loss: 2.508593221818757

Epoch: 126| Step: 0
Training loss: 2.6482271994307225
Validation loss: 2.5147991008089114

Epoch: 6| Step: 1
Training loss: 2.9346929895842995
Validation loss: 2.5096353070508495

Epoch: 6| Step: 2
Training loss: 2.6857020551756676
Validation loss: 2.5079921345902823

Epoch: 6| Step: 3
Training loss: 2.431462268339696
Validation loss: 2.4946655262785273

Epoch: 6| Step: 4
Training loss: 2.5998280101524114
Validation loss: 2.49608230235143

Epoch: 6| Step: 5
Training loss: 2.7896853480774633
Validation loss: 2.4923076345444612

Epoch: 6| Step: 6
Training loss: 2.6192234603674724
Validation loss: 2.488129387432636

Epoch: 6| Step: 7
Training loss: 2.4309275121516762
Validation loss: 2.4809774161306035

Epoch: 6| Step: 8
Training loss: 2.8774720220589347
Validation loss: 2.47176333777364

Epoch: 6| Step: 9
Training loss: 2.548575179019688
Validation loss: 2.467754658179583

Epoch: 6| Step: 10
Training loss: 2.8967033195444714
Validation loss: 2.4670994657726495

Epoch: 6| Step: 11
Training loss: 2.6960737397328183
Validation loss: 2.455273789332213

Epoch: 6| Step: 12
Training loss: 3.0067101774711498
Validation loss: 2.4634063873649286

Epoch: 6| Step: 13
Training loss: 3.0103395621557345
Validation loss: 2.4672004657231064

Epoch: 127| Step: 0
Training loss: 3.240161382164583
Validation loss: 2.460639229503024

Epoch: 6| Step: 1
Training loss: 2.9030524270438245
Validation loss: 2.4693512709418437

Epoch: 6| Step: 2
Training loss: 2.5816670448035333
Validation loss: 2.46130724380595

Epoch: 6| Step: 3
Training loss: 2.5693553313419044
Validation loss: 2.465551653245346

Epoch: 6| Step: 4
Training loss: 3.0678558227800137
Validation loss: 2.4682650886221253

Epoch: 6| Step: 5
Training loss: 2.193004222728388
Validation loss: 2.470009790366006

Epoch: 6| Step: 6
Training loss: 2.55505443119365
Validation loss: 2.4634808662898786

Epoch: 6| Step: 7
Training loss: 2.5687004994173197
Validation loss: 2.4799972849641483

Epoch: 6| Step: 8
Training loss: 2.4224311436176773
Validation loss: 2.4743390922517774

Epoch: 6| Step: 9
Training loss: 2.951423598939328
Validation loss: 2.4904027094960113

Epoch: 6| Step: 10
Training loss: 2.5804186241831952
Validation loss: 2.5115709380767215

Epoch: 6| Step: 11
Training loss: 2.68511431175425
Validation loss: 2.5323027944902226

Epoch: 6| Step: 12
Training loss: 2.3661298245937976
Validation loss: 2.544750816285275

Epoch: 6| Step: 13
Training loss: 3.562871144018458
Validation loss: 2.54129207183326

Epoch: 128| Step: 0
Training loss: 2.5801666498699407
Validation loss: 2.516325216050697

Epoch: 6| Step: 1
Training loss: 2.925913096004995
Validation loss: 2.5070953902801025

Epoch: 6| Step: 2
Training loss: 2.938159625075172
Validation loss: 2.4936984361750745

Epoch: 6| Step: 3
Training loss: 2.4293128212287174
Validation loss: 2.4827618770673188

Epoch: 6| Step: 4
Training loss: 2.699812077234334
Validation loss: 2.4938423165351535

Epoch: 6| Step: 5
Training loss: 3.1159299833195395
Validation loss: 2.4900797272126685

Epoch: 6| Step: 6
Training loss: 3.106745082154821
Validation loss: 2.4646431120167174

Epoch: 6| Step: 7
Training loss: 2.44509350463309
Validation loss: 2.457760618130181

Epoch: 6| Step: 8
Training loss: 2.9881502409497074
Validation loss: 2.4497089960699974

Epoch: 6| Step: 9
Training loss: 2.3153179629242495
Validation loss: 2.4455234657139115

Epoch: 6| Step: 10
Training loss: 2.4264361346785592
Validation loss: 2.443519713784095

Epoch: 6| Step: 11
Training loss: 2.838399695716162
Validation loss: 2.4327610068350953

Epoch: 6| Step: 12
Training loss: 2.598790246781209
Validation loss: 2.439400198600651

Epoch: 6| Step: 13
Training loss: 2.3728286454440086
Validation loss: 2.4437000030043308

Epoch: 129| Step: 0
Training loss: 2.6053862093958093
Validation loss: 2.436194355990564

Epoch: 6| Step: 1
Training loss: 2.991398879226686
Validation loss: 2.432240533674436

Epoch: 6| Step: 2
Training loss: 2.318281318616728
Validation loss: 2.4359134657021286

Epoch: 6| Step: 3
Training loss: 2.966750907416272
Validation loss: 2.429772801691114

Epoch: 6| Step: 4
Training loss: 2.307982078112433
Validation loss: 2.43557508486681

Epoch: 6| Step: 5
Training loss: 3.0688541433528296
Validation loss: 2.448826428669361

Epoch: 6| Step: 6
Training loss: 2.170595299769927
Validation loss: 2.453635075010704

Epoch: 6| Step: 7
Training loss: 2.814104258446155
Validation loss: 2.453840915334503

Epoch: 6| Step: 8
Training loss: 2.9874287105976807
Validation loss: 2.474848039332531

Epoch: 6| Step: 9
Training loss: 3.2114205472746042
Validation loss: 2.4893302718498305

Epoch: 6| Step: 10
Training loss: 2.4268391585963784
Validation loss: 2.472091780668639

Epoch: 6| Step: 11
Training loss: 2.913425570263774
Validation loss: 2.4713384584855027

Epoch: 6| Step: 12
Training loss: 2.367435052095646
Validation loss: 2.494314533747988

Epoch: 6| Step: 13
Training loss: 2.521984331486165
Validation loss: 2.497990048394654

Epoch: 130| Step: 0
Training loss: 2.9240652432866594
Validation loss: 2.4905569738696824

Epoch: 6| Step: 1
Training loss: 3.1165535265400246
Validation loss: 2.479284537217341

Epoch: 6| Step: 2
Training loss: 2.9874500988739072
Validation loss: 2.4688827283742696

Epoch: 6| Step: 3
Training loss: 2.3508135341102157
Validation loss: 2.4564356820365156

Epoch: 6| Step: 4
Training loss: 2.6871895721707926
Validation loss: 2.4446769694965496

Epoch: 6| Step: 5
Training loss: 2.2378687719848545
Validation loss: 2.4462757448449546

Epoch: 6| Step: 6
Training loss: 2.21464653415398
Validation loss: 2.4552507033687747

Epoch: 6| Step: 7
Training loss: 2.457938264901827
Validation loss: 2.453337658652728

Epoch: 6| Step: 8
Training loss: 3.483452508034582
Validation loss: 2.4572587964555304

Epoch: 6| Step: 9
Training loss: 3.1963596260447273
Validation loss: 2.456413432521648

Epoch: 6| Step: 10
Training loss: 2.5742532036057395
Validation loss: 2.4590233289319463

Epoch: 6| Step: 11
Training loss: 2.532336531670178
Validation loss: 2.446535555977525

Epoch: 6| Step: 12
Training loss: 2.1615550804700217
Validation loss: 2.453111203198432

Epoch: 6| Step: 13
Training loss: 2.4371302397578165
Validation loss: 2.448544359512856

Epoch: 131| Step: 0
Training loss: 2.850229950043933
Validation loss: 2.461186367928017

Epoch: 6| Step: 1
Training loss: 2.4699159597084392
Validation loss: 2.4619553929294415

Epoch: 6| Step: 2
Training loss: 2.7309214553684704
Validation loss: 2.465820027630267

Epoch: 6| Step: 3
Training loss: 3.061849408234516
Validation loss: 2.4755325181202266

Epoch: 6| Step: 4
Training loss: 2.5837787941415695
Validation loss: 2.4773739766673355

Epoch: 6| Step: 5
Training loss: 1.8459973620460979
Validation loss: 2.4595261445740912

Epoch: 6| Step: 6
Training loss: 2.2003292444255953
Validation loss: 2.4562048383452635

Epoch: 6| Step: 7
Training loss: 3.3500916425781364
Validation loss: 2.4598128220830873

Epoch: 6| Step: 8
Training loss: 2.8283836288640845
Validation loss: 2.4523652755723564

Epoch: 6| Step: 9
Training loss: 2.836747151124693
Validation loss: 2.4575039643142906

Epoch: 6| Step: 10
Training loss: 2.0793200047073848
Validation loss: 2.454738929130249

Epoch: 6| Step: 11
Training loss: 2.4266089656628047
Validation loss: 2.457972660863086

Epoch: 6| Step: 12
Training loss: 2.9591055565119806
Validation loss: 2.455972341435981

Epoch: 6| Step: 13
Training loss: 3.3646491234235527
Validation loss: 2.452296680260755

Epoch: 132| Step: 0
Training loss: 2.6757936352074223
Validation loss: 2.4566314678258783

Epoch: 6| Step: 1
Training loss: 2.384930879083113
Validation loss: 2.4565185719422415

Epoch: 6| Step: 2
Training loss: 2.9186321357274836
Validation loss: 2.453757131538373

Epoch: 6| Step: 3
Training loss: 2.4827022079063994
Validation loss: 2.460861929419911

Epoch: 6| Step: 4
Training loss: 2.8098647593470245
Validation loss: 2.4620182131349027

Epoch: 6| Step: 5
Training loss: 2.910139536969626
Validation loss: 2.475144932127599

Epoch: 6| Step: 6
Training loss: 2.4391369336028426
Validation loss: 2.484507992570024

Epoch: 6| Step: 7
Training loss: 2.4089946204125816
Validation loss: 2.4785506569885776

Epoch: 6| Step: 8
Training loss: 1.9848586566866482
Validation loss: 2.470940061511256

Epoch: 6| Step: 9
Training loss: 3.168412446772252
Validation loss: 2.475125388413006

Epoch: 6| Step: 10
Training loss: 2.9127691840339853
Validation loss: 2.4608581165558565

Epoch: 6| Step: 11
Training loss: 2.768453756978579
Validation loss: 2.4551628932816443

Epoch: 6| Step: 12
Training loss: 2.8380720864214513
Validation loss: 2.4458428669064296

Epoch: 6| Step: 13
Training loss: 2.6999121298443427
Validation loss: 2.456986225243267

Epoch: 133| Step: 0
Training loss: 2.3105733939266946
Validation loss: 2.451606105217255

Epoch: 6| Step: 1
Training loss: 2.1961325203354645
Validation loss: 2.4493899870309543

Epoch: 6| Step: 2
Training loss: 2.499175507963475
Validation loss: 2.4629218634684933

Epoch: 6| Step: 3
Training loss: 2.7463154818576676
Validation loss: 2.466694149392481

Epoch: 6| Step: 4
Training loss: 2.583487372779294
Validation loss: 2.4774631135800855

Epoch: 6| Step: 5
Training loss: 2.8397393082797686
Validation loss: 2.473291667410602

Epoch: 6| Step: 6
Training loss: 2.4402672147609685
Validation loss: 2.4764021556339233

Epoch: 6| Step: 7
Training loss: 3.0469130293600255
Validation loss: 2.484832064303634

Epoch: 6| Step: 8
Training loss: 2.463777774091036
Validation loss: 2.4985535159417007

Epoch: 6| Step: 9
Training loss: 3.302756175776289
Validation loss: 2.523819406631734

Epoch: 6| Step: 10
Training loss: 2.6817706049503345
Validation loss: 2.500310327896459

Epoch: 6| Step: 11
Training loss: 2.929041432669472
Validation loss: 2.484804628764181

Epoch: 6| Step: 12
Training loss: 2.7220043129757157
Validation loss: 2.484152497760215

Epoch: 6| Step: 13
Training loss: 2.523237757852757
Validation loss: 2.477746467960269

Epoch: 134| Step: 0
Training loss: 2.421732842980116
Validation loss: 2.464796652538207

Epoch: 6| Step: 1
Training loss: 2.6361439622342373
Validation loss: 2.463520940656035

Epoch: 6| Step: 2
Training loss: 2.1274079536222605
Validation loss: 2.471848826131551

Epoch: 6| Step: 3
Training loss: 2.649885203916031
Validation loss: 2.4695728447350755

Epoch: 6| Step: 4
Training loss: 2.744795729883264
Validation loss: 2.4916250868740555

Epoch: 6| Step: 5
Training loss: 2.086861052350838
Validation loss: 2.4925208555264375

Epoch: 6| Step: 6
Training loss: 2.6707052603815327
Validation loss: 2.5432474944989862

Epoch: 6| Step: 7
Training loss: 2.6852456329625567
Validation loss: 2.552243403290378

Epoch: 6| Step: 8
Training loss: 2.732979205692044
Validation loss: 2.5812248758121616

Epoch: 6| Step: 9
Training loss: 3.081537204689289
Validation loss: 2.578940075318498

Epoch: 6| Step: 10
Training loss: 2.7866511673321357
Validation loss: 2.518994159692058

Epoch: 6| Step: 11
Training loss: 3.3255687083853633
Validation loss: 2.466499558364269

Epoch: 6| Step: 12
Training loss: 2.8173017414137327
Validation loss: 2.457207819592779

Epoch: 6| Step: 13
Training loss: 2.8493894893531375
Validation loss: 2.4682055706752055

Epoch: 135| Step: 0
Training loss: 2.0654229923798106
Validation loss: 2.4617005597095902

Epoch: 6| Step: 1
Training loss: 2.746356631376213
Validation loss: 2.4569901171543806

Epoch: 6| Step: 2
Training loss: 2.7164405572637627
Validation loss: 2.4588071478788263

Epoch: 6| Step: 3
Training loss: 2.7207069807716757
Validation loss: 2.4546832654716777

Epoch: 6| Step: 4
Training loss: 2.8956094193281245
Validation loss: 2.456400286646681

Epoch: 6| Step: 5
Training loss: 2.805623952789349
Validation loss: 2.447589429470723

Epoch: 6| Step: 6
Training loss: 2.457281200024957
Validation loss: 2.4512249007285023

Epoch: 6| Step: 7
Training loss: 3.0741348352091498
Validation loss: 2.4410185060456246

Epoch: 6| Step: 8
Training loss: 2.4961705442265476
Validation loss: 2.4555621628598234

Epoch: 6| Step: 9
Training loss: 1.8402386867122063
Validation loss: 2.459465462434823

Epoch: 6| Step: 10
Training loss: 2.9905330376751014
Validation loss: 2.4696593104682925

Epoch: 6| Step: 11
Training loss: 3.1373471287419865
Validation loss: 2.501327127016973

Epoch: 6| Step: 12
Training loss: 3.244224110906506
Validation loss: 2.521128964528144

Epoch: 6| Step: 13
Training loss: 2.2267684038223363
Validation loss: 2.5684330528269115

Epoch: 136| Step: 0
Training loss: 3.105153807978538
Validation loss: 2.566096951008598

Epoch: 6| Step: 1
Training loss: 2.853967441557577
Validation loss: 2.5533094750894585

Epoch: 6| Step: 2
Training loss: 1.9243929116238465
Validation loss: 2.5429392116476794

Epoch: 6| Step: 3
Training loss: 2.445204662346135
Validation loss: 2.538888893723717

Epoch: 6| Step: 4
Training loss: 1.400913880298217
Validation loss: 2.496108087730899

Epoch: 6| Step: 5
Training loss: 2.4965962126620256
Validation loss: 2.4874723792700473

Epoch: 6| Step: 6
Training loss: 2.7682168313747737
Validation loss: 2.4678163337437513

Epoch: 6| Step: 7
Training loss: 2.9732346241507788
Validation loss: 2.464954163716046

Epoch: 6| Step: 8
Training loss: 2.8373670101509547
Validation loss: 2.4689249028921236

Epoch: 6| Step: 9
Training loss: 3.3667076813368078
Validation loss: 2.459569509335626

Epoch: 6| Step: 10
Training loss: 2.7141323996389297
Validation loss: 2.4581926238466547

Epoch: 6| Step: 11
Training loss: 2.956965940855325
Validation loss: 2.4582848976025864

Epoch: 6| Step: 12
Training loss: 2.66767463906094
Validation loss: 2.4816332368331757

Epoch: 6| Step: 13
Training loss: 2.7753428616799636
Validation loss: 2.481931541852141

Epoch: 137| Step: 0
Training loss: 2.308709930749647
Validation loss: 2.4724114907353

Epoch: 6| Step: 1
Training loss: 2.229229317138251
Validation loss: 2.481284571800163

Epoch: 6| Step: 2
Training loss: 3.294125246391827
Validation loss: 2.481572708912265

Epoch: 6| Step: 3
Training loss: 3.209437853115061
Validation loss: 2.4765693424040145

Epoch: 6| Step: 4
Training loss: 2.2886652162308194
Validation loss: 2.4836931222234613

Epoch: 6| Step: 5
Training loss: 2.732953906667516
Validation loss: 2.4956052566203906

Epoch: 6| Step: 6
Training loss: 2.7671417559952034
Validation loss: 2.4974357456965506

Epoch: 6| Step: 7
Training loss: 2.433138637940553
Validation loss: 2.522053563494345

Epoch: 6| Step: 8
Training loss: 2.3311588054748165
Validation loss: 2.539575986472919

Epoch: 6| Step: 9
Training loss: 2.8042579258149294
Validation loss: 2.5731910823935538

Epoch: 6| Step: 10
Training loss: 2.3698570382180857
Validation loss: 2.5571344738859274

Epoch: 6| Step: 11
Training loss: 3.2054161251544846
Validation loss: 2.547990726814673

Epoch: 6| Step: 12
Training loss: 2.8021727102175884
Validation loss: 2.5130749975827498

Epoch: 6| Step: 13
Training loss: 2.700641086197635
Validation loss: 2.46744855287155

Epoch: 138| Step: 0
Training loss: 2.992460153771746
Validation loss: 2.46913419220866

Epoch: 6| Step: 1
Training loss: 2.69066424723322
Validation loss: 2.4565178998590693

Epoch: 6| Step: 2
Training loss: 2.8151095681692997
Validation loss: 2.447214757389037

Epoch: 6| Step: 3
Training loss: 2.4544806166796773
Validation loss: 2.4548757573482223

Epoch: 6| Step: 4
Training loss: 2.535251985925879
Validation loss: 2.4560561269058154

Epoch: 6| Step: 5
Training loss: 3.061134812605022
Validation loss: 2.4484395645337806

Epoch: 6| Step: 6
Training loss: 2.601451699586595
Validation loss: 2.4568818049489565

Epoch: 6| Step: 7
Training loss: 2.743300252910426
Validation loss: 2.4610359230810297

Epoch: 6| Step: 8
Training loss: 2.500276550255749
Validation loss: 2.4577246117922686

Epoch: 6| Step: 9
Training loss: 2.178693428465055
Validation loss: 2.4536460467849492

Epoch: 6| Step: 10
Training loss: 2.8450502210853936
Validation loss: 2.4526298132366104

Epoch: 6| Step: 11
Training loss: 2.7764629356895902
Validation loss: 2.4766001133203788

Epoch: 6| Step: 12
Training loss: 2.2867180076522327
Validation loss: 2.484389204140868

Epoch: 6| Step: 13
Training loss: 2.9358525627806555
Validation loss: 2.4729682334023106

Epoch: 139| Step: 0
Training loss: 2.0483542186064434
Validation loss: 2.4675365958806146

Epoch: 6| Step: 1
Training loss: 2.3389748311606993
Validation loss: 2.4589240120758333

Epoch: 6| Step: 2
Training loss: 2.8715303465342124
Validation loss: 2.4573653988541535

Epoch: 6| Step: 3
Training loss: 2.3034294186032307
Validation loss: 2.4679632879159876

Epoch: 6| Step: 4
Training loss: 2.4860431181159823
Validation loss: 2.4500940630453805

Epoch: 6| Step: 5
Training loss: 2.7741954358817775
Validation loss: 2.4486097295068596

Epoch: 6| Step: 6
Training loss: 2.824424050283582
Validation loss: 2.4547720336749994

Epoch: 6| Step: 7
Training loss: 2.4642589618976176
Validation loss: 2.4688879836143416

Epoch: 6| Step: 8
Training loss: 2.74308592595753
Validation loss: 2.4718917745509494

Epoch: 6| Step: 9
Training loss: 3.0563078580345104
Validation loss: 2.470354088409209

Epoch: 6| Step: 10
Training loss: 2.5484036029968036
Validation loss: 2.5051020233709114

Epoch: 6| Step: 11
Training loss: 2.9640726285368264
Validation loss: 2.5060023013984805

Epoch: 6| Step: 12
Training loss: 3.0784213315581552
Validation loss: 2.5193507601819314

Epoch: 6| Step: 13
Training loss: 2.8923623302671366
Validation loss: 2.4913260625040277

Epoch: 140| Step: 0
Training loss: 3.1401922582579624
Validation loss: 2.4973154728828417

Epoch: 6| Step: 1
Training loss: 2.216024524732041
Validation loss: 2.504929811265864

Epoch: 6| Step: 2
Training loss: 2.854687459911227
Validation loss: 2.5170018682747726

Epoch: 6| Step: 3
Training loss: 2.614192148152729
Validation loss: 2.5232652926522676

Epoch: 6| Step: 4
Training loss: 2.5698229662707144
Validation loss: 2.5352378877702253

Epoch: 6| Step: 5
Training loss: 2.670224399352142
Validation loss: 2.5492077231843964

Epoch: 6| Step: 6
Training loss: 2.729788459434936
Validation loss: 2.5592956577147774

Epoch: 6| Step: 7
Training loss: 2.848456543708625
Validation loss: 2.5249686026085687

Epoch: 6| Step: 8
Training loss: 1.9295711096130204
Validation loss: 2.5128835584605484

Epoch: 6| Step: 9
Training loss: 2.8596431418842774
Validation loss: 2.5024642346068964

Epoch: 6| Step: 10
Training loss: 2.7827567830149165
Validation loss: 2.4976073536527585

Epoch: 6| Step: 11
Training loss: 2.730525942573186
Validation loss: 2.491106400931289

Epoch: 6| Step: 12
Training loss: 2.579543861905035
Validation loss: 2.4913348534622397

Epoch: 6| Step: 13
Training loss: 2.1075546887452874
Validation loss: 2.486567085262288

Epoch: 141| Step: 0
Training loss: 2.694804179591759
Validation loss: 2.473775127127637

Epoch: 6| Step: 1
Training loss: 2.8302759242255826
Validation loss: 2.47530216387402

Epoch: 6| Step: 2
Training loss: 2.3980062389022185
Validation loss: 2.471097384439938

Epoch: 6| Step: 3
Training loss: 3.2617778110296523
Validation loss: 2.4874100238722066

Epoch: 6| Step: 4
Training loss: 2.5230287392060884
Validation loss: 2.489704134984491

Epoch: 6| Step: 5
Training loss: 2.174443105676536
Validation loss: 2.4947901415408653

Epoch: 6| Step: 6
Training loss: 2.9697707178257176
Validation loss: 2.516173052146351

Epoch: 6| Step: 7
Training loss: 2.5001038529759687
Validation loss: 2.527226432292118

Epoch: 6| Step: 8
Training loss: 2.450199010123256
Validation loss: 2.5500375557115866

Epoch: 6| Step: 9
Training loss: 2.622477545628454
Validation loss: 2.602120630274539

Epoch: 6| Step: 10
Training loss: 2.5282830640421983
Validation loss: 2.634275236315857

Epoch: 6| Step: 11
Training loss: 2.6364820133885583
Validation loss: 2.630184106624845

Epoch: 6| Step: 12
Training loss: 2.8769687048754995
Validation loss: 2.6193931692565857

Epoch: 6| Step: 13
Training loss: 2.216342102994229
Validation loss: 2.5386854595113237

Epoch: 142| Step: 0
Training loss: 2.389600746293106
Validation loss: 2.4860599020863923

Epoch: 6| Step: 1
Training loss: 2.6533916577521515
Validation loss: 2.458866908957843

Epoch: 6| Step: 2
Training loss: 2.8742635032361172
Validation loss: 2.444656016084831

Epoch: 6| Step: 3
Training loss: 2.677057783150863
Validation loss: 2.4357340262892455

Epoch: 6| Step: 4
Training loss: 2.5702100924697953
Validation loss: 2.428281441248237

Epoch: 6| Step: 5
Training loss: 2.4812014481336866
Validation loss: 2.430424568721196

Epoch: 6| Step: 6
Training loss: 2.8912793217947974
Validation loss: 2.420833878049986

Epoch: 6| Step: 7
Training loss: 3.0833321305006276
Validation loss: 2.4282669490236466

Epoch: 6| Step: 8
Training loss: 2.5154698483975158
Validation loss: 2.420994364893425

Epoch: 6| Step: 9
Training loss: 2.8077306362948655
Validation loss: 2.422794442995757

Epoch: 6| Step: 10
Training loss: 1.701027094289386
Validation loss: 2.4202182720053536

Epoch: 6| Step: 11
Training loss: 2.718344647235664
Validation loss: 2.4266139934004083

Epoch: 6| Step: 12
Training loss: 3.343407158789273
Validation loss: 2.4363501513101227

Epoch: 6| Step: 13
Training loss: 2.131548496497305
Validation loss: 2.4478753778714775

Epoch: 143| Step: 0
Training loss: 2.1691373407719263
Validation loss: 2.464127902937388

Epoch: 6| Step: 1
Training loss: 2.530866523234474
Validation loss: 2.4829919574092494

Epoch: 6| Step: 2
Training loss: 2.3361476433682418
Validation loss: 2.4703324753556357

Epoch: 6| Step: 3
Training loss: 2.9070599155504344
Validation loss: 2.4599494884476463

Epoch: 6| Step: 4
Training loss: 2.7031644256009875
Validation loss: 2.446034245495941

Epoch: 6| Step: 5
Training loss: 2.277252666556003
Validation loss: 2.445741113372468

Epoch: 6| Step: 6
Training loss: 2.545996297895043
Validation loss: 2.4491820266806323

Epoch: 6| Step: 7
Training loss: 2.975910425724954
Validation loss: 2.4490559119466093

Epoch: 6| Step: 8
Training loss: 2.8984727060488185
Validation loss: 2.444459576325303

Epoch: 6| Step: 9
Training loss: 2.642534366986887
Validation loss: 2.444825220686297

Epoch: 6| Step: 10
Training loss: 3.0594987760448795
Validation loss: 2.4543408422637323

Epoch: 6| Step: 11
Training loss: 2.5052774040444743
Validation loss: 2.4812599135436626

Epoch: 6| Step: 12
Training loss: 2.613031805931768
Validation loss: 2.500288016638332

Epoch: 6| Step: 13
Training loss: 2.39307512376307
Validation loss: 2.5505556886468335

Epoch: 144| Step: 0
Training loss: 2.743673416493977
Validation loss: 2.571298557155834

Epoch: 6| Step: 1
Training loss: 2.510427947170719
Validation loss: 2.547030907861722

Epoch: 6| Step: 2
Training loss: 2.9122004146148344
Validation loss: 2.508178130728924

Epoch: 6| Step: 3
Training loss: 2.653255614894877
Validation loss: 2.5102860468631665

Epoch: 6| Step: 4
Training loss: 2.7818560850708387
Validation loss: 2.5016465614971155

Epoch: 6| Step: 5
Training loss: 2.7247110546103275
Validation loss: 2.5019615947900262

Epoch: 6| Step: 6
Training loss: 2.773101442420532
Validation loss: 2.5020350684517494

Epoch: 6| Step: 7
Training loss: 2.7524087933389207
Validation loss: 2.5126131101356752

Epoch: 6| Step: 8
Training loss: 2.5844350639476246
Validation loss: 2.5231247076939955

Epoch: 6| Step: 9
Training loss: 2.7188607555060282
Validation loss: 2.5411330163311545

Epoch: 6| Step: 10
Training loss: 2.1370467748382764
Validation loss: 2.5542676282769627

Epoch: 6| Step: 11
Training loss: 2.0004065815595165
Validation loss: 2.579689724912407

Epoch: 6| Step: 12
Training loss: 2.5019085274860124
Validation loss: 2.579912149822876

Epoch: 6| Step: 13
Training loss: 2.5813962585540327
Validation loss: 2.5725547041375734

Epoch: 145| Step: 0
Training loss: 1.4855590916105452
Validation loss: 2.553900825958665

Epoch: 6| Step: 1
Training loss: 2.212254970797139
Validation loss: 2.5456571170345073

Epoch: 6| Step: 2
Training loss: 2.4712880282835896
Validation loss: 2.5409278286637615

Epoch: 6| Step: 3
Training loss: 2.3498135269527656
Validation loss: 2.502640641143548

Epoch: 6| Step: 4
Training loss: 2.431690923331448
Validation loss: 2.502191415788948

Epoch: 6| Step: 5
Training loss: 2.6700256372142475
Validation loss: 2.5022955189780935

Epoch: 6| Step: 6
Training loss: 2.7258039146086785
Validation loss: 2.4858187861110053

Epoch: 6| Step: 7
Training loss: 2.888765964175078
Validation loss: 2.4938639463424312

Epoch: 6| Step: 8
Training loss: 2.4832550019406154
Validation loss: 2.468898364264142

Epoch: 6| Step: 9
Training loss: 3.142071139525234
Validation loss: 2.478637516476559

Epoch: 6| Step: 10
Training loss: 3.0899912802563936
Validation loss: 2.47661476523929

Epoch: 6| Step: 11
Training loss: 2.543114817653549
Validation loss: 2.465785474974814

Epoch: 6| Step: 12
Training loss: 2.9645883402421336
Validation loss: 2.48501205763862

Epoch: 6| Step: 13
Training loss: 2.7105683616823377
Validation loss: 2.4949357432633064

Epoch: 146| Step: 0
Training loss: 2.9166967844543406
Validation loss: 2.5402146974265354

Epoch: 6| Step: 1
Training loss: 2.220599840592306
Validation loss: 2.5700128403496847

Epoch: 6| Step: 2
Training loss: 2.706485841971646
Validation loss: 2.5739508852715196

Epoch: 6| Step: 3
Training loss: 1.8179931271782985
Validation loss: 2.557890559230083

Epoch: 6| Step: 4
Training loss: 2.803335134737734
Validation loss: 2.5615185164311827

Epoch: 6| Step: 5
Training loss: 2.352642128326846
Validation loss: 2.5882629144082756

Epoch: 6| Step: 6
Training loss: 2.1025912238072224
Validation loss: 2.6166136566643807

Epoch: 6| Step: 7
Training loss: 2.704460475656094
Validation loss: 2.5881574442186093

Epoch: 6| Step: 8
Training loss: 3.136038548505098
Validation loss: 2.5681977214493585

Epoch: 6| Step: 9
Training loss: 2.753982954106875
Validation loss: 2.5295963696804176

Epoch: 6| Step: 10
Training loss: 2.6995386577225697
Validation loss: 2.493725379100426

Epoch: 6| Step: 11
Training loss: 2.2469661179531597
Validation loss: 2.4734670721004

Epoch: 6| Step: 12
Training loss: 2.805655309794257
Validation loss: 2.471803793324855

Epoch: 6| Step: 13
Training loss: 2.7998901822489053
Validation loss: 2.4662055229576945

Epoch: 147| Step: 0
Training loss: 2.7567783394288594
Validation loss: 2.453736147007476

Epoch: 6| Step: 1
Training loss: 2.5197216346431737
Validation loss: 2.472487557501811

Epoch: 6| Step: 2
Training loss: 2.947187964931108
Validation loss: 2.485653933547909

Epoch: 6| Step: 3
Training loss: 2.520426840887494
Validation loss: 2.500865349668923

Epoch: 6| Step: 4
Training loss: 2.864929973552047
Validation loss: 2.5217592604527486

Epoch: 6| Step: 5
Training loss: 2.606477510690033
Validation loss: 2.5319968048236507

Epoch: 6| Step: 6
Training loss: 2.2723480445744118
Validation loss: 2.5578480625606392

Epoch: 6| Step: 7
Training loss: 1.9485632763289753
Validation loss: 2.5676981298361277

Epoch: 6| Step: 8
Training loss: 2.5222818190675254
Validation loss: 2.6278389581372372

Epoch: 6| Step: 9
Training loss: 2.662671394383425
Validation loss: 2.6103803606208236

Epoch: 6| Step: 10
Training loss: 2.7378411694442764
Validation loss: 2.616440346939888

Epoch: 6| Step: 11
Training loss: 2.8129270441309044
Validation loss: 2.620753905642197

Epoch: 6| Step: 12
Training loss: 1.9474514764462794
Validation loss: 2.590985918399998

Epoch: 6| Step: 13
Training loss: 2.8021534812872813
Validation loss: 2.549377693456997

Epoch: 148| Step: 0
Training loss: 2.231415823109301
Validation loss: 2.5347341058369817

Epoch: 6| Step: 1
Training loss: 2.2141706463145465
Validation loss: 2.516661937583952

Epoch: 6| Step: 2
Training loss: 2.25955513094968
Validation loss: 2.533588291053165

Epoch: 6| Step: 3
Training loss: 2.1564335399410335
Validation loss: 2.503266072545381

Epoch: 6| Step: 4
Training loss: 2.586574879259842
Validation loss: 2.5031067054453504

Epoch: 6| Step: 5
Training loss: 3.124071822607095
Validation loss: 2.5160772571457386

Epoch: 6| Step: 6
Training loss: 2.946127376602213
Validation loss: 2.5337210175066907

Epoch: 6| Step: 7
Training loss: 2.866851703822754
Validation loss: 2.529859592678012

Epoch: 6| Step: 8
Training loss: 2.785574219574407
Validation loss: 2.535219418076711

Epoch: 6| Step: 9
Training loss: 2.389166692603592
Validation loss: 2.532901038600094

Epoch: 6| Step: 10
Training loss: 2.398404198828515
Validation loss: 2.5030258878675116

Epoch: 6| Step: 11
Training loss: 2.701232335757956
Validation loss: 2.515670646529011

Epoch: 6| Step: 12
Training loss: 2.5281122799094784
Validation loss: 2.509762718927115

Epoch: 6| Step: 13
Training loss: 2.635848290200682
Validation loss: 2.5135954380018832

Epoch: 149| Step: 0
Training loss: 2.7148244294501365
Validation loss: 2.5049564471245396

Epoch: 6| Step: 1
Training loss: 3.1357149677170355
Validation loss: 2.517183185456904

Epoch: 6| Step: 2
Training loss: 1.9719071278320077
Validation loss: 2.525966506582405

Epoch: 6| Step: 3
Training loss: 1.9391640316112382
Validation loss: 2.521010764242393

Epoch: 6| Step: 4
Training loss: 2.8671003531119124
Validation loss: 2.5538532929300395

Epoch: 6| Step: 5
Training loss: 2.3609361346863813
Validation loss: 2.5653796149544226

Epoch: 6| Step: 6
Training loss: 2.5929013438024646
Validation loss: 2.5876397289982314

Epoch: 6| Step: 7
Training loss: 2.175272089283212
Validation loss: 2.5796268229243164

Epoch: 6| Step: 8
Training loss: 2.8667134820826603
Validation loss: 2.5857637948882255

Epoch: 6| Step: 9
Training loss: 2.7008834100082897
Validation loss: 2.572805821674557

Epoch: 6| Step: 10
Training loss: 2.7459202761819164
Validation loss: 2.5600436793675256

Epoch: 6| Step: 11
Training loss: 2.430352319364465
Validation loss: 2.4881588026809927

Epoch: 6| Step: 12
Training loss: 2.114659676011514
Validation loss: 2.458921634977783

Epoch: 6| Step: 13
Training loss: 2.848881043403338
Validation loss: 2.4597936119999937

Epoch: 150| Step: 0
Training loss: 2.611535554121263
Validation loss: 2.452848694911664

Epoch: 6| Step: 1
Training loss: 2.438530630689371
Validation loss: 2.4766172873542818

Epoch: 6| Step: 2
Training loss: 2.175705201276995
Validation loss: 2.4930212289090354

Epoch: 6| Step: 3
Training loss: 2.3413548756341926
Validation loss: 2.489678215833998

Epoch: 6| Step: 4
Training loss: 3.0161948184527683
Validation loss: 2.5129509151593044

Epoch: 6| Step: 5
Training loss: 2.304531079172734
Validation loss: 2.5425679246147785

Epoch: 6| Step: 6
Training loss: 2.3058994016383445
Validation loss: 2.5848365271296525

Epoch: 6| Step: 7
Training loss: 2.786989440689916
Validation loss: 2.5969170139578046

Epoch: 6| Step: 8
Training loss: 2.968684145548373
Validation loss: 2.650823448330792

Epoch: 6| Step: 9
Training loss: 2.4941917659143673
Validation loss: 2.688542334884233

Epoch: 6| Step: 10
Training loss: 2.6085484274839623
Validation loss: 2.713235781452079

Epoch: 6| Step: 11
Training loss: 2.3551992891709665
Validation loss: 2.715453522213998

Epoch: 6| Step: 12
Training loss: 2.644235163331549
Validation loss: 2.67752413214803

Epoch: 6| Step: 13
Training loss: 2.367660827652433
Validation loss: 2.649607309619333

Epoch: 151| Step: 0
Training loss: 2.7012274812945103
Validation loss: 2.60892232753779

Epoch: 6| Step: 1
Training loss: 2.892621644675993
Validation loss: 2.592200470576149

Epoch: 6| Step: 2
Training loss: 2.6232262022479875
Validation loss: 2.5462109361645204

Epoch: 6| Step: 3
Training loss: 1.9285665491839652
Validation loss: 2.518636268949435

Epoch: 6| Step: 4
Training loss: 2.1580791731232343
Validation loss: 2.5255225002116073

Epoch: 6| Step: 5
Training loss: 2.714817579403986
Validation loss: 2.5371335227931353

Epoch: 6| Step: 6
Training loss: 2.7544074718412777
Validation loss: 2.5325524663108996

Epoch: 6| Step: 7
Training loss: 2.309885247918395
Validation loss: 2.570043516916237

Epoch: 6| Step: 8
Training loss: 1.9740090745294112
Validation loss: 2.581870601400473

Epoch: 6| Step: 9
Training loss: 1.9861923666503423
Validation loss: 2.5688559848202788

Epoch: 6| Step: 10
Training loss: 2.496917922852963
Validation loss: 2.576164327879698

Epoch: 6| Step: 11
Training loss: 2.70535750616162
Validation loss: 2.512696089949559

Epoch: 6| Step: 12
Training loss: 2.907375978955562
Validation loss: 2.4831135909509965

Epoch: 6| Step: 13
Training loss: 3.044553527672215
Validation loss: 2.451597288918007

Epoch: 152| Step: 0
Training loss: 2.608746024523102
Validation loss: 2.4310311912574907

Epoch: 6| Step: 1
Training loss: 2.9311319681247703
Validation loss: 2.433579903568822

Epoch: 6| Step: 2
Training loss: 2.6060188319008133
Validation loss: 2.437646442323303

Epoch: 6| Step: 3
Training loss: 2.5397051658123924
Validation loss: 2.4428618555338515

Epoch: 6| Step: 4
Training loss: 3.051408573766813
Validation loss: 2.4682934828188836

Epoch: 6| Step: 5
Training loss: 2.344524611578406
Validation loss: 2.5006135720847604

Epoch: 6| Step: 6
Training loss: 2.122681306389943
Validation loss: 2.521244879573359

Epoch: 6| Step: 7
Training loss: 2.1727625727914655
Validation loss: 2.5637001130603836

Epoch: 6| Step: 8
Training loss: 2.2887123022303113
Validation loss: 2.594884142885124

Epoch: 6| Step: 9
Training loss: 1.9555559822404762
Validation loss: 2.6604630897728243

Epoch: 6| Step: 10
Training loss: 2.2710585059655135
Validation loss: 2.7241855345336465

Epoch: 6| Step: 11
Training loss: 3.3794945353579373
Validation loss: 2.765464893487711

Epoch: 6| Step: 12
Training loss: 2.852755529541241
Validation loss: 2.6566964832865856

Epoch: 6| Step: 13
Training loss: 2.1540910167679796
Validation loss: 2.559538297469448

Epoch: 153| Step: 0
Training loss: 2.89112198140105
Validation loss: 2.4939860518800865

Epoch: 6| Step: 1
Training loss: 2.065387207748738
Validation loss: 2.452800738968271

Epoch: 6| Step: 2
Training loss: 2.884732896416407
Validation loss: 2.4268066485848565

Epoch: 6| Step: 3
Training loss: 2.1194606838156913
Validation loss: 2.4170876144509092

Epoch: 6| Step: 4
Training loss: 2.619564696251165
Validation loss: 2.414513177615283

Epoch: 6| Step: 5
Training loss: 2.3491285534496242
Validation loss: 2.4197144884345665

Epoch: 6| Step: 6
Training loss: 2.4671332944649484
Validation loss: 2.423843815669374

Epoch: 6| Step: 7
Training loss: 2.4893614432013407
Validation loss: 2.431731423543697

Epoch: 6| Step: 8
Training loss: 2.676510097616451
Validation loss: 2.4397726053125117

Epoch: 6| Step: 9
Training loss: 2.325425189117178
Validation loss: 2.4992237511288

Epoch: 6| Step: 10
Training loss: 2.4424308396917294
Validation loss: 2.535776333703714

Epoch: 6| Step: 11
Training loss: 2.795551860832491
Validation loss: 2.5809538147144293

Epoch: 6| Step: 12
Training loss: 2.7604359368185483
Validation loss: 2.637869633057669

Epoch: 6| Step: 13
Training loss: 2.349940960730907
Validation loss: 2.709988341277456

Epoch: 154| Step: 0
Training loss: 1.9740413222234492
Validation loss: 2.7636312281951225

Epoch: 6| Step: 1
Training loss: 2.8322938994585565
Validation loss: 2.7918491374295975

Epoch: 6| Step: 2
Training loss: 3.0458137424602585
Validation loss: 2.81383471339941

Epoch: 6| Step: 3
Training loss: 2.613873196603298
Validation loss: 2.7151555132940928

Epoch: 6| Step: 4
Training loss: 2.1527499111655293
Validation loss: 2.6155234372720373

Epoch: 6| Step: 5
Training loss: 2.173929025342493
Validation loss: 2.5470869050537757

Epoch: 6| Step: 6
Training loss: 2.8330497225765066
Validation loss: 2.515074529886391

Epoch: 6| Step: 7
Training loss: 2.4397362452362716
Validation loss: 2.4966872426658338

Epoch: 6| Step: 8
Training loss: 2.557530026226963
Validation loss: 2.500007262270644

Epoch: 6| Step: 9
Training loss: 1.7842432938853665
Validation loss: 2.496937204618215

Epoch: 6| Step: 10
Training loss: 2.2666568905488664
Validation loss: 2.5011659364471797

Epoch: 6| Step: 11
Training loss: 2.919340942158017
Validation loss: 2.498551780890683

Epoch: 6| Step: 12
Training loss: 3.290393116199166
Validation loss: 2.507882170255212

Epoch: 6| Step: 13
Training loss: 2.476076096383754
Validation loss: 2.5278374159572468

Epoch: 155| Step: 0
Training loss: 2.8936319743200962
Validation loss: 2.571422584669635

Epoch: 6| Step: 1
Training loss: 2.7654370605060095
Validation loss: 2.6135092333464587

Epoch: 6| Step: 2
Training loss: 1.792549499828478
Validation loss: 2.6436518673582325

Epoch: 6| Step: 3
Training loss: 2.718066250115079
Validation loss: 2.6829876403098285

Epoch: 6| Step: 4
Training loss: 2.4334320945380883
Validation loss: 2.6984198199219516

Epoch: 6| Step: 5
Training loss: 3.1183550567386797
Validation loss: 2.743125003385384

Epoch: 6| Step: 6
Training loss: 2.752839356584914
Validation loss: 2.691957252178148

Epoch: 6| Step: 7
Training loss: 2.2674565771418056
Validation loss: 2.675475864215781

Epoch: 6| Step: 8
Training loss: 2.1240498157485606
Validation loss: 2.6428431731563684

Epoch: 6| Step: 9
Training loss: 2.202691434579351
Validation loss: 2.620272363307908

Epoch: 6| Step: 10
Training loss: 1.6824478533572318
Validation loss: 2.616075975493506

Epoch: 6| Step: 11
Training loss: 2.640147871442252
Validation loss: 2.6014630964689625

Epoch: 6| Step: 12
Training loss: 2.531697928273516
Validation loss: 2.5888753709606385

Epoch: 6| Step: 13
Training loss: 2.6754794430899165
Validation loss: 2.5652830266530624

Epoch: 156| Step: 0
Training loss: 2.343066001265308
Validation loss: 2.6038388639479506

Epoch: 6| Step: 1
Training loss: 2.4621863693116572
Validation loss: 2.6230605530083846

Epoch: 6| Step: 2
Training loss: 2.484240738221776
Validation loss: 2.6433823750493524

Epoch: 6| Step: 3
Training loss: 2.2845113773576107
Validation loss: 2.63070605058608

Epoch: 6| Step: 4
Training loss: 2.8019038335510844
Validation loss: 2.6628052684718093

Epoch: 6| Step: 5
Training loss: 2.24608961022037
Validation loss: 2.6850124588544007

Epoch: 6| Step: 6
Training loss: 1.8062017124149745
Validation loss: 2.6874472318878992

Epoch: 6| Step: 7
Training loss: 2.448356707906924
Validation loss: 2.690786410367125

Epoch: 6| Step: 8
Training loss: 2.913380560968612
Validation loss: 2.7079238815580973

Epoch: 6| Step: 9
Training loss: 2.2315701038683513
Validation loss: 2.680306501290028

Epoch: 6| Step: 10
Training loss: 2.582563983017535
Validation loss: 2.6790243621187466

Epoch: 6| Step: 11
Training loss: 2.5022314603242397
Validation loss: 2.671681938591656

Epoch: 6| Step: 12
Training loss: 2.715715666246196
Validation loss: 2.6112143567328534

Epoch: 6| Step: 13
Training loss: 2.6988666486951454
Validation loss: 2.564235220799237

Epoch: 157| Step: 0
Training loss: 2.0687256687727253
Validation loss: 2.5431732246242853

Epoch: 6| Step: 1
Training loss: 2.3465665043059953
Validation loss: 2.5245319739658933

Epoch: 6| Step: 2
Training loss: 2.4015282215856355
Validation loss: 2.517678688921515

Epoch: 6| Step: 3
Training loss: 2.4502902812653398
Validation loss: 2.527942302635007

Epoch: 6| Step: 4
Training loss: 2.5117982937808
Validation loss: 2.543439133780318

Epoch: 6| Step: 5
Training loss: 2.5458890726716907
Validation loss: 2.5581873016296215

Epoch: 6| Step: 6
Training loss: 2.702622559879917
Validation loss: 2.543408696755721

Epoch: 6| Step: 7
Training loss: 2.5767976060227906
Validation loss: 2.5273874219501655

Epoch: 6| Step: 8
Training loss: 2.5253689109299464
Validation loss: 2.526852277456786

Epoch: 6| Step: 9
Training loss: 2.87396735844031
Validation loss: 2.5463230127014613

Epoch: 6| Step: 10
Training loss: 2.243567915729915
Validation loss: 2.5545541317620604

Epoch: 6| Step: 11
Training loss: 1.845419564238038
Validation loss: 2.560293528747598

Epoch: 6| Step: 12
Training loss: 2.7985079809159896
Validation loss: 2.5883355368829775

Epoch: 6| Step: 13
Training loss: 2.1451397364345675
Validation loss: 2.5842229844136426

Epoch: 158| Step: 0
Training loss: 2.305651033357405
Validation loss: 2.6150784698786502

Epoch: 6| Step: 1
Training loss: 2.847226376000659
Validation loss: 2.6404236737825846

Epoch: 6| Step: 2
Training loss: 2.791604387717865
Validation loss: 2.6500370704099647

Epoch: 6| Step: 3
Training loss: 2.1983228793219816
Validation loss: 2.65358651070019

Epoch: 6| Step: 4
Training loss: 2.1702145603126364
Validation loss: 2.6762421495682345

Epoch: 6| Step: 5
Training loss: 2.156149709829424
Validation loss: 2.69547391066054

Epoch: 6| Step: 6
Training loss: 2.6283347428277137
Validation loss: 2.693790124153198

Epoch: 6| Step: 7
Training loss: 2.768308985873121
Validation loss: 2.695681903356505

Epoch: 6| Step: 8
Training loss: 2.4741888855702774
Validation loss: 2.684479151061937

Epoch: 6| Step: 9
Training loss: 2.308855225912537
Validation loss: 2.6592528344916273

Epoch: 6| Step: 10
Training loss: 2.3070492814467416
Validation loss: 2.666623115824834

Epoch: 6| Step: 11
Training loss: 2.29839954254114
Validation loss: 2.6469407506538394

Epoch: 6| Step: 12
Training loss: 2.4047153087096156
Validation loss: 2.60802604198986

Epoch: 6| Step: 13
Training loss: 2.3279477250368297
Validation loss: 2.580004510939962

Epoch: 159| Step: 0
Training loss: 2.4678628993603073
Validation loss: 2.5592561734910118

Epoch: 6| Step: 1
Training loss: 2.699465440885366
Validation loss: 2.5238541114697703

Epoch: 6| Step: 2
Training loss: 2.0615189270808663
Validation loss: 2.5100833748337523

Epoch: 6| Step: 3
Training loss: 2.0485219372093098
Validation loss: 2.5061441496484917

Epoch: 6| Step: 4
Training loss: 2.826701949577133
Validation loss: 2.496658029554341

Epoch: 6| Step: 5
Training loss: 2.3665422120302018
Validation loss: 2.5249487876267476

Epoch: 6| Step: 6
Training loss: 2.674925551759992
Validation loss: 2.550999143708976

Epoch: 6| Step: 7
Training loss: 2.4976826417904983
Validation loss: 2.6235255403880413

Epoch: 6| Step: 8
Training loss: 2.48403717239258
Validation loss: 2.683969054000472

Epoch: 6| Step: 9
Training loss: 2.390877105783762
Validation loss: 2.7373750735026054

Epoch: 6| Step: 10
Training loss: 2.506004180648982
Validation loss: 2.7536803645778254

Epoch: 6| Step: 11
Training loss: 2.496844207230043
Validation loss: 2.776564461451866

Epoch: 6| Step: 12
Training loss: 2.2969200493164013
Validation loss: 2.7375892266879704

Epoch: 6| Step: 13
Training loss: 2.155513029345389
Validation loss: 2.731529739864634

Epoch: 160| Step: 0
Training loss: 2.6658881859135994
Validation loss: 2.699383589181708

Epoch: 6| Step: 1
Training loss: 2.593874641089373
Validation loss: 2.682764022784491

Epoch: 6| Step: 2
Training loss: 2.5203496505733125
Validation loss: 2.6458971527621906

Epoch: 6| Step: 3
Training loss: 2.207355954750869
Validation loss: 2.6183374882213375

Epoch: 6| Step: 4
Training loss: 2.1495149996887015
Validation loss: 2.579175193385934

Epoch: 6| Step: 5
Training loss: 2.4194335444784305
Validation loss: 2.5769290033058057

Epoch: 6| Step: 6
Training loss: 2.2464021951936393
Validation loss: 2.5909596890433995

Epoch: 6| Step: 7
Training loss: 2.4361051823500266
Validation loss: 2.5875593977472335

Epoch: 6| Step: 8
Training loss: 2.798802398968037
Validation loss: 2.5772247144625458

Epoch: 6| Step: 9
Training loss: 2.607291754858361
Validation loss: 2.556689662427505

Epoch: 6| Step: 10
Training loss: 2.1988733267665803
Validation loss: 2.554957939204541

Epoch: 6| Step: 11
Training loss: 1.675595217606929
Validation loss: 2.516825320366114

Epoch: 6| Step: 12
Training loss: 2.4723387118075264
Validation loss: 2.5136527450607047

Epoch: 6| Step: 13
Training loss: 2.6496027630747796
Validation loss: 2.525161772258469

Epoch: 161| Step: 0
Training loss: 2.46412215272519
Validation loss: 2.520382564158145

Epoch: 6| Step: 1
Training loss: 3.218377397322172
Validation loss: 2.52497863898577

Epoch: 6| Step: 2
Training loss: 2.2236929622752357
Validation loss: 2.5056711926990918

Epoch: 6| Step: 3
Training loss: 2.139671726004308
Validation loss: 2.5288102903003735

Epoch: 6| Step: 4
Training loss: 2.2571604490025745
Validation loss: 2.5307281393716003

Epoch: 6| Step: 5
Training loss: 2.2677183806080414
Validation loss: 2.526124599978415

Epoch: 6| Step: 6
Training loss: 2.7524398037653888
Validation loss: 2.5353868134016997

Epoch: 6| Step: 7
Training loss: 2.0572831036997097
Validation loss: 2.5730187725166354

Epoch: 6| Step: 8
Training loss: 2.375338981931474
Validation loss: 2.5961035655660063

Epoch: 6| Step: 9
Training loss: 1.6398682074974242
Validation loss: 2.596599182745188

Epoch: 6| Step: 10
Training loss: 3.150043087619187
Validation loss: 2.618564656383746

Epoch: 6| Step: 11
Training loss: 1.4585787248782565
Validation loss: 2.6282293024728856

Epoch: 6| Step: 12
Training loss: 2.6098498066624796
Validation loss: 2.6686909097802824

Epoch: 6| Step: 13
Training loss: 2.4411276208192616
Validation loss: 2.64878539612624

Epoch: 162| Step: 0
Training loss: 2.135731582371075
Validation loss: 2.66823144412577

Epoch: 6| Step: 1
Training loss: 2.2374036842799385
Validation loss: 2.7168430594468083

Epoch: 6| Step: 2
Training loss: 2.2312202857680923
Validation loss: 2.7349361424010965

Epoch: 6| Step: 3
Training loss: 2.2869070274693764
Validation loss: 2.746254397741379

Epoch: 6| Step: 4
Training loss: 2.569846809660465
Validation loss: 2.7849940592313858

Epoch: 6| Step: 5
Training loss: 2.256365566516912
Validation loss: 2.7565808324580447

Epoch: 6| Step: 6
Training loss: 2.7268785003912432
Validation loss: 2.6765268969373417

Epoch: 6| Step: 7
Training loss: 2.5662509168650183
Validation loss: 2.600005417795271

Epoch: 6| Step: 8
Training loss: 2.673057963520951
Validation loss: 2.541642523949628

Epoch: 6| Step: 9
Training loss: 2.172093551946366
Validation loss: 2.4899436366276553

Epoch: 6| Step: 10
Training loss: 2.9780718163207656
Validation loss: 2.4724606361417263

Epoch: 6| Step: 11
Training loss: 2.49061491807455
Validation loss: 2.467962936812468

Epoch: 6| Step: 12
Training loss: 2.1942181557731475
Validation loss: 2.47663100185951

Epoch: 6| Step: 13
Training loss: 2.1672550160645567
Validation loss: 2.532407810045285

Epoch: 163| Step: 0
Training loss: 1.8158559970687251
Validation loss: 2.5682111036264903

Epoch: 6| Step: 1
Training loss: 2.378402080259623
Validation loss: 2.631527439286909

Epoch: 6| Step: 2
Training loss: 2.458291996322766
Validation loss: 2.704257549295316

Epoch: 6| Step: 3
Training loss: 2.3646585281672245
Validation loss: 2.738638601551568

Epoch: 6| Step: 4
Training loss: 2.2761121399980353
Validation loss: 2.7669629002761664

Epoch: 6| Step: 5
Training loss: 2.788397188734258
Validation loss: 2.7817493233381496

Epoch: 6| Step: 6
Training loss: 2.786255008492775
Validation loss: 2.7682769817257133

Epoch: 6| Step: 7
Training loss: 2.347685701055391
Validation loss: 2.7615255557548495

Epoch: 6| Step: 8
Training loss: 2.3817985003079785
Validation loss: 2.7521835078519175

Epoch: 6| Step: 9
Training loss: 2.0264951955049844
Validation loss: 2.7568419633838537

Epoch: 6| Step: 10
Training loss: 2.262633986747171
Validation loss: 2.7672224131802183

Epoch: 6| Step: 11
Training loss: 1.8023211665861372
Validation loss: 2.766145353527118

Epoch: 6| Step: 12
Training loss: 3.0792370641470264
Validation loss: 2.7369659121587886

Epoch: 6| Step: 13
Training loss: 2.4773945177952093
Validation loss: 2.7083899278304537

Epoch: 164| Step: 0
Training loss: 2.88639893779862
Validation loss: 2.7104656086207752

Epoch: 6| Step: 1
Training loss: 2.033042586233792
Validation loss: 2.645765028667692

Epoch: 6| Step: 2
Training loss: 2.631709289334469
Validation loss: 2.6207560479153362

Epoch: 6| Step: 3
Training loss: 2.106944626727341
Validation loss: 2.611402805740543

Epoch: 6| Step: 4
Training loss: 2.21937219196852
Validation loss: 2.6162307932586955

Epoch: 6| Step: 5
Training loss: 1.9673785322179316
Validation loss: 2.6247045211009863

Epoch: 6| Step: 6
Training loss: 2.686285764829786
Validation loss: 2.6274264030865857

Epoch: 6| Step: 7
Training loss: 2.1688699035039414
Validation loss: 2.630638242562457

Epoch: 6| Step: 8
Training loss: 2.581197122027621
Validation loss: 2.6220960077336852

Epoch: 6| Step: 9
Training loss: 2.493127723246723
Validation loss: 2.614347497174101

Epoch: 6| Step: 10
Training loss: 2.0650329064432857
Validation loss: 2.6173394252299844

Epoch: 6| Step: 11
Training loss: 2.0317115846111653
Validation loss: 2.6092791363778667

Epoch: 6| Step: 12
Training loss: 2.8622232228363584
Validation loss: 2.5906006440105376

Epoch: 6| Step: 13
Training loss: 1.9722843839433628
Validation loss: 2.6080058426304302

Epoch: 165| Step: 0
Training loss: 2.1901142303078336
Validation loss: 2.594663016254874

Epoch: 6| Step: 1
Training loss: 2.078517102415022
Validation loss: 2.6087075845580876

Epoch: 6| Step: 2
Training loss: 2.5683981772853133
Validation loss: 2.5939035005758475

Epoch: 6| Step: 3
Training loss: 2.198862267122566
Validation loss: 2.597315098858114

Epoch: 6| Step: 4
Training loss: 2.6672440042974497
Validation loss: 2.600618429049799

Epoch: 6| Step: 5
Training loss: 2.1088295690587873
Validation loss: 2.6095876132449094

Epoch: 6| Step: 6
Training loss: 2.6171741086702647
Validation loss: 2.6166450938781676

Epoch: 6| Step: 7
Training loss: 2.4045635109626806
Validation loss: 2.6537428062725215

Epoch: 6| Step: 8
Training loss: 2.201738069788071
Validation loss: 2.6999793213675685

Epoch: 6| Step: 9
Training loss: 2.1041307603649226
Validation loss: 2.7147268746266677

Epoch: 6| Step: 10
Training loss: 2.638129324657562
Validation loss: 2.724443041535839

Epoch: 6| Step: 11
Training loss: 1.7282266321355846
Validation loss: 2.709232153409445

Epoch: 6| Step: 12
Training loss: 2.3621498615421905
Validation loss: 2.6856066307959674

Epoch: 6| Step: 13
Training loss: 2.485132353882726
Validation loss: 2.6215054127757536

Epoch: 166| Step: 0
Training loss: 2.264771820431499
Validation loss: 2.6245077563306234

Epoch: 6| Step: 1
Training loss: 1.9214106673380433
Validation loss: 2.611858478780469

Epoch: 6| Step: 2
Training loss: 2.761543235974939
Validation loss: 2.594791109016003

Epoch: 6| Step: 3
Training loss: 1.8181605245687125
Validation loss: 2.597771911735467

Epoch: 6| Step: 4
Training loss: 2.456662682573598
Validation loss: 2.5793852449586154

Epoch: 6| Step: 5
Training loss: 2.4245713513826637
Validation loss: 2.58556669520031

Epoch: 6| Step: 6
Training loss: 2.2236306680929494
Validation loss: 2.604116852970941

Epoch: 6| Step: 7
Training loss: 1.9754093711646967
Validation loss: 2.5874261920950605

Epoch: 6| Step: 8
Training loss: 2.692036902255515
Validation loss: 2.5782555918403753

Epoch: 6| Step: 9
Training loss: 2.3427692650220764
Validation loss: 2.578330918308085

Epoch: 6| Step: 10
Training loss: 1.9151643034168195
Validation loss: 2.587124248755687

Epoch: 6| Step: 11
Training loss: 2.276714571741574
Validation loss: 2.5942526917172293

Epoch: 6| Step: 12
Training loss: 2.7938319866260084
Validation loss: 2.6305565891733864

Epoch: 6| Step: 13
Training loss: 2.1705436742712516
Validation loss: 2.6460560737266214

Epoch: 167| Step: 0
Training loss: 2.029886580432253
Validation loss: 2.6486905736581465

Epoch: 6| Step: 1
Training loss: 2.553763219469597
Validation loss: 2.6697582569184166

Epoch: 6| Step: 2
Training loss: 2.3710666007635357
Validation loss: 2.696639712133168

Epoch: 6| Step: 3
Training loss: 2.285091144905897
Validation loss: 2.7148807967225532

Epoch: 6| Step: 4
Training loss: 2.817870882226318
Validation loss: 2.688360879069569

Epoch: 6| Step: 5
Training loss: 2.1750843294830084
Validation loss: 2.6368657650329017

Epoch: 6| Step: 6
Training loss: 1.79317092586725
Validation loss: 2.5827785760316826

Epoch: 6| Step: 7
Training loss: 2.2984045216925835
Validation loss: 2.5608803337257773

Epoch: 6| Step: 8
Training loss: 2.2423558188605317
Validation loss: 2.5365576619242614

Epoch: 6| Step: 9
Training loss: 2.9388027143273487
Validation loss: 2.5266121047170023

Epoch: 6| Step: 10
Training loss: 2.34296404058041
Validation loss: 2.5231753475084635

Epoch: 6| Step: 11
Training loss: 2.526188723553824
Validation loss: 2.5345405364221736

Epoch: 6| Step: 12
Training loss: 1.6369391318011564
Validation loss: 2.5336580609836004

Epoch: 6| Step: 13
Training loss: 1.9705079117146433
Validation loss: 2.552550901901932

Epoch: 168| Step: 0
Training loss: 2.2867049748221175
Validation loss: 2.585510461533476

Epoch: 6| Step: 1
Training loss: 2.3117170297114145
Validation loss: 2.597581340836323

Epoch: 6| Step: 2
Training loss: 1.8125920107934281
Validation loss: 2.629758245181533

Epoch: 6| Step: 3
Training loss: 2.823353066208423
Validation loss: 2.649147445033561

Epoch: 6| Step: 4
Training loss: 2.2255375159278366
Validation loss: 2.629576549761078

Epoch: 6| Step: 5
Training loss: 2.6085076632351485
Validation loss: 2.6104690137832542

Epoch: 6| Step: 6
Training loss: 2.291861947726993
Validation loss: 2.5956946163997445

Epoch: 6| Step: 7
Training loss: 2.1177704068487375
Validation loss: 2.586988840553898

Epoch: 6| Step: 8
Training loss: 1.5977371307270194
Validation loss: 2.5906754294537366

Epoch: 6| Step: 9
Training loss: 2.5752522502415696
Validation loss: 2.5787696467468124

Epoch: 6| Step: 10
Training loss: 2.049189067571795
Validation loss: 2.587797018731491

Epoch: 6| Step: 11
Training loss: 2.236326205989897
Validation loss: 2.578413430167274

Epoch: 6| Step: 12
Training loss: 2.3163568449621756
Validation loss: 2.616918687602049

Epoch: 6| Step: 13
Training loss: 2.623543834973664
Validation loss: 2.613394847850731

Epoch: 169| Step: 0
Training loss: 1.8599685875485856
Validation loss: 2.608773499965481

Epoch: 6| Step: 1
Training loss: 1.9607774281614616
Validation loss: 2.6245255771584914

Epoch: 6| Step: 2
Training loss: 2.241391136887006
Validation loss: 2.637354945914205

Epoch: 6| Step: 3
Training loss: 2.631687184165841
Validation loss: 2.648692397161935

Epoch: 6| Step: 4
Training loss: 2.1866294218396014
Validation loss: 2.6677609788874843

Epoch: 6| Step: 5
Training loss: 2.0388962250363027
Validation loss: 2.685538105969921

Epoch: 6| Step: 6
Training loss: 2.7136897207178254
Validation loss: 2.690925841384413

Epoch: 6| Step: 7
Training loss: 1.7539063859357547
Validation loss: 2.6813715083533385

Epoch: 6| Step: 8
Training loss: 2.224720939538166
Validation loss: 2.6659405960655334

Epoch: 6| Step: 9
Training loss: 2.7194197257345833
Validation loss: 2.600180731014372

Epoch: 6| Step: 10
Training loss: 1.80997896803971
Validation loss: 2.557903799893823

Epoch: 6| Step: 11
Training loss: 2.395224540872168
Validation loss: 2.512263899041852

Epoch: 6| Step: 12
Training loss: 2.3262849230965914
Validation loss: 2.504946059316626

Epoch: 6| Step: 13
Training loss: 3.0230619606383997
Validation loss: 2.482283094536423

Epoch: 170| Step: 0
Training loss: 2.3414610812080587
Validation loss: 2.46963569202334

Epoch: 6| Step: 1
Training loss: 1.9864234263795897
Validation loss: 2.475748833394731

Epoch: 6| Step: 2
Training loss: 2.497936255277726
Validation loss: 2.475412521215363

Epoch: 6| Step: 3
Training loss: 1.9557627451960593
Validation loss: 2.4970542318824993

Epoch: 6| Step: 4
Training loss: 2.7140684489007647
Validation loss: 2.5063593572199436

Epoch: 6| Step: 5
Training loss: 1.8051776963363861
Validation loss: 2.5205250057153052

Epoch: 6| Step: 6
Training loss: 2.569285642739876
Validation loss: 2.524547951657811

Epoch: 6| Step: 7
Training loss: 1.9794962223804153
Validation loss: 2.5398773935339194

Epoch: 6| Step: 8
Training loss: 2.0140900200856935
Validation loss: 2.5744938132753923

Epoch: 6| Step: 9
Training loss: 2.651615183210789
Validation loss: 2.601139682529868

Epoch: 6| Step: 10
Training loss: 1.679821133287916
Validation loss: 2.6490177273130877

Epoch: 6| Step: 11
Training loss: 2.76213940299426
Validation loss: 2.6575754389942134

Epoch: 6| Step: 12
Training loss: 2.16873468838874
Validation loss: 2.705885231436002

Epoch: 6| Step: 13
Training loss: 2.509974798250772
Validation loss: 2.747038100109137

Epoch: 171| Step: 0
Training loss: 2.4204367027650786
Validation loss: 2.7663843368403422

Epoch: 6| Step: 1
Training loss: 2.0743200130134065
Validation loss: 2.7727413918638018

Epoch: 6| Step: 2
Training loss: 1.9801430218029819
Validation loss: 2.766681819384529

Epoch: 6| Step: 3
Training loss: 1.9335740213639998
Validation loss: 2.79026829264078

Epoch: 6| Step: 4
Training loss: 2.2622561960952257
Validation loss: 2.774765243585243

Epoch: 6| Step: 5
Training loss: 2.043443088643444
Validation loss: 2.747132282750805

Epoch: 6| Step: 6
Training loss: 1.9734171089333061
Validation loss: 2.6864960424772804

Epoch: 6| Step: 7
Training loss: 2.103064585210509
Validation loss: 2.6050933426242655

Epoch: 6| Step: 8
Training loss: 2.239502152738497
Validation loss: 2.5574094108948526

Epoch: 6| Step: 9
Training loss: 2.369391998012693
Validation loss: 2.5346488501117914

Epoch: 6| Step: 10
Training loss: 2.4220846762183963
Validation loss: 2.525975094776319

Epoch: 6| Step: 11
Training loss: 2.3028159608373624
Validation loss: 2.4938105658874905

Epoch: 6| Step: 12
Training loss: 2.859448041139336
Validation loss: 2.48770536594363

Epoch: 6| Step: 13
Training loss: 2.3440149793243896
Validation loss: 2.4896700780413603

Epoch: 172| Step: 0
Training loss: 2.2220098698668935
Validation loss: 2.4861750923800083

Epoch: 6| Step: 1
Training loss: 1.8625826055654369
Validation loss: 2.514023118133295

Epoch: 6| Step: 2
Training loss: 2.292464192650555
Validation loss: 2.5388349341808496

Epoch: 6| Step: 3
Training loss: 2.2435975641945713
Validation loss: 2.5710949871629736

Epoch: 6| Step: 4
Training loss: 2.707490780181544
Validation loss: 2.6622813766917703

Epoch: 6| Step: 5
Training loss: 2.178161524462304
Validation loss: 2.7158852272865515

Epoch: 6| Step: 6
Training loss: 1.9795021843528402
Validation loss: 2.7507404083901204

Epoch: 6| Step: 7
Training loss: 2.6504024092117815
Validation loss: 2.7652349375995176

Epoch: 6| Step: 8
Training loss: 2.3721420760938425
Validation loss: 2.751388659713987

Epoch: 6| Step: 9
Training loss: 2.1831488795903367
Validation loss: 2.7345852247486313

Epoch: 6| Step: 10
Training loss: 1.2071182565286431
Validation loss: 2.7360559746704363

Epoch: 6| Step: 11
Training loss: 2.190459184452251
Validation loss: 2.7116124931463075

Epoch: 6| Step: 12
Training loss: 2.59023662826493
Validation loss: 2.6609919186522983

Epoch: 6| Step: 13
Training loss: 2.229477216057923
Validation loss: 2.6509182370580286

Epoch: 173| Step: 0
Training loss: 2.042148520251617
Validation loss: 2.639305370915244

Epoch: 6| Step: 1
Training loss: 1.7543939876582098
Validation loss: 2.595554503445888

Epoch: 6| Step: 2
Training loss: 2.2199392221052605
Validation loss: 2.582077849290322

Epoch: 6| Step: 3
Training loss: 2.172381883928694
Validation loss: 2.586298790276119

Epoch: 6| Step: 4
Training loss: 2.470534826283746
Validation loss: 2.5452610254180175

Epoch: 6| Step: 5
Training loss: 2.352066949112093
Validation loss: 2.5522197801596787

Epoch: 6| Step: 6
Training loss: 1.9179611672059245
Validation loss: 2.51648229534709

Epoch: 6| Step: 7
Training loss: 2.541187512201327
Validation loss: 2.5276909664944953

Epoch: 6| Step: 8
Training loss: 2.421146504755847
Validation loss: 2.5184115135463654

Epoch: 6| Step: 9
Training loss: 1.5926566019715578
Validation loss: 2.5576082895782863

Epoch: 6| Step: 10
Training loss: 2.485793854155583
Validation loss: 2.584149144181154

Epoch: 6| Step: 11
Training loss: 1.9757810714076012
Validation loss: 2.6015430240184787

Epoch: 6| Step: 12
Training loss: 2.492063130590914
Validation loss: 2.6296738045579735

Epoch: 6| Step: 13
Training loss: 2.115022234788538
Validation loss: 2.6513966259609743

Epoch: 174| Step: 0
Training loss: 2.0338721150393093
Validation loss: 2.6970847617784037

Epoch: 6| Step: 1
Training loss: 1.9022489990199656
Validation loss: 2.7174727348942245

Epoch: 6| Step: 2
Training loss: 2.3384143367049863
Validation loss: 2.758695913461574

Epoch: 6| Step: 3
Training loss: 2.319149663572478
Validation loss: 2.788662570733032

Epoch: 6| Step: 4
Training loss: 2.1520974906266157
Validation loss: 2.814575438593391

Epoch: 6| Step: 5
Training loss: 2.1828170787241707
Validation loss: 2.7671611179733464

Epoch: 6| Step: 6
Training loss: 2.2496494443980963
Validation loss: 2.7147312327785493

Epoch: 6| Step: 7
Training loss: 2.2500402658886167
Validation loss: 2.617739455367779

Epoch: 6| Step: 8
Training loss: 2.054961443722672
Validation loss: 2.575832055537158

Epoch: 6| Step: 9
Training loss: 2.4902016312185515
Validation loss: 2.52062345963517

Epoch: 6| Step: 10
Training loss: 2.3474409414059116
Validation loss: 2.491278875016175

Epoch: 6| Step: 11
Training loss: 2.0492058215822397
Validation loss: 2.4924357388305807

Epoch: 6| Step: 12
Training loss: 2.3884496827234725
Validation loss: 2.4802739579856543

Epoch: 6| Step: 13
Training loss: 1.7953707825817602
Validation loss: 2.4790084083806114

Epoch: 175| Step: 0
Training loss: 1.8696997433082545
Validation loss: 2.481865635416037

Epoch: 6| Step: 1
Training loss: 2.2803835333767113
Validation loss: 2.47708821897814

Epoch: 6| Step: 2
Training loss: 1.8900592051926048
Validation loss: 2.487184223693172

Epoch: 6| Step: 3
Training loss: 2.2898496765039202
Validation loss: 2.49114935231423

Epoch: 6| Step: 4
Training loss: 2.4562261138608776
Validation loss: 2.502481547692242

Epoch: 6| Step: 5
Training loss: 2.295243183125448
Validation loss: 2.5136623503339157

Epoch: 6| Step: 6
Training loss: 2.0117630740717782
Validation loss: 2.5038460198943975

Epoch: 6| Step: 7
Training loss: 2.9214343386375643
Validation loss: 2.502213772572702

Epoch: 6| Step: 8
Training loss: 2.3820083011511377
Validation loss: 2.5250215954477944

Epoch: 6| Step: 9
Training loss: 1.5142780099400306
Validation loss: 2.515240142942905

Epoch: 6| Step: 10
Training loss: 2.3826545037590843
Validation loss: 2.543010832990284

Epoch: 6| Step: 11
Training loss: 2.006960439416607
Validation loss: 2.5751960531616302

Epoch: 6| Step: 12
Training loss: 1.2668887292977113
Validation loss: 2.631870204729773

Epoch: 6| Step: 13
Training loss: 2.2768370913738027
Validation loss: 2.698083582025727

Testing loss: 2.7446769984769475

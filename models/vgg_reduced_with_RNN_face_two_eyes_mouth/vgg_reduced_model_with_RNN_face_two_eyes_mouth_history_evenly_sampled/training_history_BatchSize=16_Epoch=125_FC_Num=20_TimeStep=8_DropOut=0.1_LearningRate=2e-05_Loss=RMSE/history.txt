Epoch: 1| Step: 0
Training loss: 5.376570716060091
Validation loss: 5.817075754486589

Epoch: 6| Step: 1
Training loss: 5.875825864492901
Validation loss: 5.791761726714142

Epoch: 6| Step: 2
Training loss: 6.202933128329795
Validation loss: 5.768976418857299

Epoch: 6| Step: 3
Training loss: 5.8716258647337325
Validation loss: 5.743237596194711

Epoch: 6| Step: 4
Training loss: 6.79869150026111
Validation loss: 5.714000709226491

Epoch: 6| Step: 5
Training loss: 6.224940606871846
Validation loss: 5.680485495016194

Epoch: 6| Step: 6
Training loss: 5.453408307692553
Validation loss: 5.643417594737115

Epoch: 6| Step: 7
Training loss: 5.5906997974183446
Validation loss: 5.601716352137592

Epoch: 6| Step: 8
Training loss: 5.944122155188332
Validation loss: 5.554939547332825

Epoch: 6| Step: 9
Training loss: 5.903379681385711
Validation loss: 5.504094744641671

Epoch: 6| Step: 10
Training loss: 5.06814034931372
Validation loss: 5.449563394492484

Epoch: 6| Step: 11
Training loss: 5.013574484104731
Validation loss: 5.390569448875906

Epoch: 6| Step: 12
Training loss: 4.904786687445318
Validation loss: 5.32945901305422

Epoch: 6| Step: 13
Training loss: 3.3154381730837654
Validation loss: 5.264594625906828

Epoch: 2| Step: 0
Training loss: 5.901391993395863
Validation loss: 5.201290113668008

Epoch: 6| Step: 1
Training loss: 4.811953823258071
Validation loss: 5.135773398900699

Epoch: 6| Step: 2
Training loss: 4.795543572858001
Validation loss: 5.06763731435167

Epoch: 6| Step: 3
Training loss: 4.872146162374346
Validation loss: 5.001213545836697

Epoch: 6| Step: 4
Training loss: 4.965795634310607
Validation loss: 4.935518522688509

Epoch: 6| Step: 5
Training loss: 4.606661757720967
Validation loss: 4.869251704247668

Epoch: 6| Step: 6
Training loss: 4.429281226202073
Validation loss: 4.803145062253799

Epoch: 6| Step: 7
Training loss: 5.101800092273173
Validation loss: 4.737361945147492

Epoch: 6| Step: 8
Training loss: 5.606686428608277
Validation loss: 4.670583287056802

Epoch: 6| Step: 9
Training loss: 3.8241307187103555
Validation loss: 4.601081019442874

Epoch: 6| Step: 10
Training loss: 4.187563539848506
Validation loss: 4.536999066879248

Epoch: 6| Step: 11
Training loss: 4.970138259898789
Validation loss: 4.466727570945169

Epoch: 6| Step: 12
Training loss: 5.031430448530362
Validation loss: 4.390307646824233

Epoch: 6| Step: 13
Training loss: 4.3575558165205175
Validation loss: 4.333364795794619

Epoch: 3| Step: 0
Training loss: 3.4676331792777377
Validation loss: 4.288070334730838

Epoch: 6| Step: 1
Training loss: 4.454513386909973
Validation loss: 4.254516571038604

Epoch: 6| Step: 2
Training loss: 5.221140787865137
Validation loss: 4.217807577310159

Epoch: 6| Step: 3
Training loss: 4.440888736202922
Validation loss: 4.176618187073223

Epoch: 6| Step: 4
Training loss: 4.63530264153413
Validation loss: 4.14143893011766

Epoch: 6| Step: 5
Training loss: 4.377454668525992
Validation loss: 4.1098564873707035

Epoch: 6| Step: 6
Training loss: 3.396278133788271
Validation loss: 4.084964468264725

Epoch: 6| Step: 7
Training loss: 4.2975425617802205
Validation loss: 4.065420325672193

Epoch: 6| Step: 8
Training loss: 3.831079705142314
Validation loss: 4.042435635806637

Epoch: 6| Step: 9
Training loss: 4.234995215907541
Validation loss: 4.017980401329123

Epoch: 6| Step: 10
Training loss: 2.958660447925406
Validation loss: 3.9899501400048503

Epoch: 6| Step: 11
Training loss: 4.810806769205128
Validation loss: 3.963583965797262

Epoch: 6| Step: 12
Training loss: 4.58018056921677
Validation loss: 3.9389998975760427

Epoch: 6| Step: 13
Training loss: 3.8530806514913456
Validation loss: 3.91694479543677

Epoch: 4| Step: 0
Training loss: 4.075201748025412
Validation loss: 3.895251487900835

Epoch: 6| Step: 1
Training loss: 3.9244856333368188
Validation loss: 3.871155948233813

Epoch: 6| Step: 2
Training loss: 3.917366262951468
Validation loss: 3.850855588671944

Epoch: 6| Step: 3
Training loss: 4.211363931837504
Validation loss: 3.8268824992097406

Epoch: 6| Step: 4
Training loss: 3.881228578665033
Validation loss: 3.800472200120878

Epoch: 6| Step: 5
Training loss: 4.079210392708191
Validation loss: 3.776502189460303

Epoch: 6| Step: 6
Training loss: 2.4543277197573348
Validation loss: 3.7528555262832786

Epoch: 6| Step: 7
Training loss: 4.302571685388886
Validation loss: 3.7458247783310012

Epoch: 6| Step: 8
Training loss: 4.49051917899793
Validation loss: 3.7158470842877853

Epoch: 6| Step: 9
Training loss: 3.918924749393161
Validation loss: 3.686713069196579

Epoch: 6| Step: 10
Training loss: 4.112694621481219
Validation loss: 3.664487590235265

Epoch: 6| Step: 11
Training loss: 3.602507692708704
Validation loss: 3.644039984084357

Epoch: 6| Step: 12
Training loss: 3.3957792562535847
Validation loss: 3.626555068595299

Epoch: 6| Step: 13
Training loss: 4.508559880702038
Validation loss: 3.6044874321803313

Epoch: 5| Step: 0
Training loss: 3.8891321711661853
Validation loss: 3.58232327801756

Epoch: 6| Step: 1
Training loss: 4.483471929022204
Validation loss: 3.5627299754291513

Epoch: 6| Step: 2
Training loss: 4.345637947193809
Validation loss: 3.5405256359965187

Epoch: 6| Step: 3
Training loss: 3.614323646092194
Validation loss: 3.522440484856359

Epoch: 6| Step: 4
Training loss: 3.891200869428073
Validation loss: 3.5134836166633248

Epoch: 6| Step: 5
Training loss: 2.8346815454493877
Validation loss: 3.4906568340123014

Epoch: 6| Step: 6
Training loss: 5.139537754276811
Validation loss: 3.481985850436018

Epoch: 6| Step: 7
Training loss: 3.2141425479310044
Validation loss: 3.4592224203348674

Epoch: 6| Step: 8
Training loss: 3.937747765874722
Validation loss: 3.44459307899897

Epoch: 6| Step: 9
Training loss: 3.5116192185126844
Validation loss: 3.435217974506841

Epoch: 6| Step: 10
Training loss: 3.4923888965538974
Validation loss: 3.42933424571813

Epoch: 6| Step: 11
Training loss: 2.973052911949018
Validation loss: 3.4055812570971042

Epoch: 6| Step: 12
Training loss: 2.3598773749870112
Validation loss: 3.3959306594386773

Epoch: 6| Step: 13
Training loss: 2.8065329937880557
Validation loss: 3.3902668573588057

Epoch: 6| Step: 0
Training loss: 2.8412630713749962
Validation loss: 3.3832871817991226

Epoch: 6| Step: 1
Training loss: 3.6417073617810227
Validation loss: 3.3745665368724604

Epoch: 6| Step: 2
Training loss: 3.9665538087919585
Validation loss: 3.360729762241181

Epoch: 6| Step: 3
Training loss: 3.3125680880476494
Validation loss: 3.3503412332184968

Epoch: 6| Step: 4
Training loss: 3.9608740566371843
Validation loss: 3.3390273027853263

Epoch: 6| Step: 5
Training loss: 3.263502681599575
Validation loss: 3.3270124823025045

Epoch: 6| Step: 6
Training loss: 3.5140192730569955
Validation loss: 3.3182560122282827

Epoch: 6| Step: 7
Training loss: 2.8080993132241043
Validation loss: 3.3105251549546

Epoch: 6| Step: 8
Training loss: 3.8281357122777413
Validation loss: 3.3030814455801867

Epoch: 6| Step: 9
Training loss: 3.9758446425820315
Validation loss: 3.2981813983273573

Epoch: 6| Step: 10
Training loss: 3.7820295050573076
Validation loss: 3.288546846338599

Epoch: 6| Step: 11
Training loss: 2.578897256497496
Validation loss: 3.280224389710262

Epoch: 6| Step: 12
Training loss: 3.421605765842363
Validation loss: 3.271097501097724

Epoch: 6| Step: 13
Training loss: 4.994849222254037
Validation loss: 3.2640787993748934

Epoch: 7| Step: 0
Training loss: 3.1823976855975866
Validation loss: 3.268529902431682

Epoch: 6| Step: 1
Training loss: 3.875722879232839
Validation loss: 3.2844497759730324

Epoch: 6| Step: 2
Training loss: 3.5614429712684794
Validation loss: 3.279893861104301

Epoch: 6| Step: 3
Training loss: 4.327537744667117
Validation loss: 3.286930428195594

Epoch: 6| Step: 4
Training loss: 3.1939327180257555
Validation loss: 3.25908461453764

Epoch: 6| Step: 5
Training loss: 2.3746098649679745
Validation loss: 3.2546790606347296

Epoch: 6| Step: 6
Training loss: 3.7610268278334065
Validation loss: 3.2571013780046765

Epoch: 6| Step: 7
Training loss: 3.4752519859229807
Validation loss: 3.2460612163920115

Epoch: 6| Step: 8
Training loss: 2.884033771470596
Validation loss: 3.221919546832581

Epoch: 6| Step: 9
Training loss: 3.612557993492652
Validation loss: 3.2120654502723087

Epoch: 6| Step: 10
Training loss: 3.799160618683445
Validation loss: 3.217814643211741

Epoch: 6| Step: 11
Training loss: 3.828381089967342
Validation loss: 3.2268976904834012

Epoch: 6| Step: 12
Training loss: 3.193240960868359
Validation loss: 3.214791770934201

Epoch: 6| Step: 13
Training loss: 3.412972352589604
Validation loss: 3.1930444757891276

Epoch: 8| Step: 0
Training loss: 3.5742726473053774
Validation loss: 3.1923492207729787

Epoch: 6| Step: 1
Training loss: 2.91599649268464
Validation loss: 3.1890861896416527

Epoch: 6| Step: 2
Training loss: 3.675823539790602
Validation loss: 3.1914275292132097

Epoch: 6| Step: 3
Training loss: 3.421319367747695
Validation loss: 3.180083438727729

Epoch: 6| Step: 4
Training loss: 3.1041567160086334
Validation loss: 3.176996192698663

Epoch: 6| Step: 5
Training loss: 3.1716732468355535
Validation loss: 3.1793827159642363

Epoch: 6| Step: 6
Training loss: 4.125105654201501
Validation loss: 3.1698802618510684

Epoch: 6| Step: 7
Training loss: 3.897154449746094
Validation loss: 3.1610825551769124

Epoch: 6| Step: 8
Training loss: 3.533827929398194
Validation loss: 3.158205271240526

Epoch: 6| Step: 9
Training loss: 3.594662890379457
Validation loss: 3.1552087972552605

Epoch: 6| Step: 10
Training loss: 2.847340256226678
Validation loss: 3.153998880785035

Epoch: 6| Step: 11
Training loss: 3.0570836340308034
Validation loss: 3.1536784759688525

Epoch: 6| Step: 12
Training loss: 3.1278376093807583
Validation loss: 3.1512111996868177

Epoch: 6| Step: 13
Training loss: 3.9929809976149966
Validation loss: 3.1482178462240182

Epoch: 9| Step: 0
Training loss: 2.8209739184223577
Validation loss: 3.1462725554315347

Epoch: 6| Step: 1
Training loss: 2.379044752677364
Validation loss: 3.142376305261584

Epoch: 6| Step: 2
Training loss: 3.4254591473966522
Validation loss: 3.140571501134432

Epoch: 6| Step: 3
Training loss: 3.733331893738969
Validation loss: 3.1383346371439984

Epoch: 6| Step: 4
Training loss: 3.972495886942785
Validation loss: 3.1357151557562335

Epoch: 6| Step: 5
Training loss: 3.6203920740067987
Validation loss: 3.1342356847273787

Epoch: 6| Step: 6
Training loss: 3.2049875194288706
Validation loss: 3.1317155016112537

Epoch: 6| Step: 7
Training loss: 3.0123016545383647
Validation loss: 3.1306973719036106

Epoch: 6| Step: 8
Training loss: 3.819548797723165
Validation loss: 3.1293330106173487

Epoch: 6| Step: 9
Training loss: 3.378373332251827
Validation loss: 3.12700789231347

Epoch: 6| Step: 10
Training loss: 3.4653019994493546
Validation loss: 3.1247132610726016

Epoch: 6| Step: 11
Training loss: 3.750015131601958
Validation loss: 3.123255589404575

Epoch: 6| Step: 12
Training loss: 3.7750650059237647
Validation loss: 3.1216009953929644

Epoch: 6| Step: 13
Training loss: 2.4944054949107155
Validation loss: 3.120035014961781

Epoch: 10| Step: 0
Training loss: 4.220331644341759
Validation loss: 3.118385287336155

Epoch: 6| Step: 1
Training loss: 2.7921309488351573
Validation loss: 3.1171014209340866

Epoch: 6| Step: 2
Training loss: 2.7426528753132633
Validation loss: 3.1149649593918762

Epoch: 6| Step: 3
Training loss: 2.7788372075188414
Validation loss: 3.1136519596233403

Epoch: 6| Step: 4
Training loss: 3.7204946064027524
Validation loss: 3.114303507704514

Epoch: 6| Step: 5
Training loss: 3.8251343547686334
Validation loss: 3.1140301233303425

Epoch: 6| Step: 6
Training loss: 3.295222876366884
Validation loss: 3.1111300215046307

Epoch: 6| Step: 7
Training loss: 3.628002501792844
Validation loss: 3.112165981305594

Epoch: 6| Step: 8
Training loss: 3.376351968750048
Validation loss: 3.1095663592955467

Epoch: 6| Step: 9
Training loss: 3.497144078869789
Validation loss: 3.106366992212292

Epoch: 6| Step: 10
Training loss: 3.396005185387273
Validation loss: 3.106740636052787

Epoch: 6| Step: 11
Training loss: 3.4642337576069764
Validation loss: 3.104855864687467

Epoch: 6| Step: 12
Training loss: 2.9761498032470053
Validation loss: 3.1049849136570202

Epoch: 6| Step: 13
Training loss: 3.3668483200418247
Validation loss: 3.1030040682745765

Epoch: 11| Step: 0
Training loss: 3.279800530858058
Validation loss: 3.100495796601871

Epoch: 6| Step: 1
Training loss: 3.4876388522163198
Validation loss: 3.0993986811510075

Epoch: 6| Step: 2
Training loss: 3.1070247182952464
Validation loss: 3.0987692570088914

Epoch: 6| Step: 3
Training loss: 3.186653024678844
Validation loss: 3.0969436335418736

Epoch: 6| Step: 4
Training loss: 2.8102085527564236
Validation loss: 3.0969384101384265

Epoch: 6| Step: 5
Training loss: 2.931497650421414
Validation loss: 3.0945278286638946

Epoch: 6| Step: 6
Training loss: 3.5102552307016035
Validation loss: 3.093257305895369

Epoch: 6| Step: 7
Training loss: 3.6666161071297045
Validation loss: 3.0918947923821114

Epoch: 6| Step: 8
Training loss: 3.6548336578799945
Validation loss: 3.0915124316780513

Epoch: 6| Step: 9
Training loss: 3.5720489753279017
Validation loss: 3.0887990054158183

Epoch: 6| Step: 10
Training loss: 3.529993964776345
Validation loss: 3.0870800282856967

Epoch: 6| Step: 11
Training loss: 3.04448759005093
Validation loss: 3.0874164842517438

Epoch: 6| Step: 12
Training loss: 3.823353810830613
Validation loss: 3.0856416044567143

Epoch: 6| Step: 13
Training loss: 3.5204176046001714
Validation loss: 3.08458493135829

Epoch: 12| Step: 0
Training loss: 2.8525387281752628
Validation loss: 3.081918585312093

Epoch: 6| Step: 1
Training loss: 3.2694370342794743
Validation loss: 3.0789347045315316

Epoch: 6| Step: 2
Training loss: 3.5107406934541325
Validation loss: 3.0779171866174493

Epoch: 6| Step: 3
Training loss: 3.8543685997652646
Validation loss: 3.0788947309463848

Epoch: 6| Step: 4
Training loss: 3.4692389341242347
Validation loss: 3.0781655049432177

Epoch: 6| Step: 5
Training loss: 3.627546402616679
Validation loss: 3.0749737116579268

Epoch: 6| Step: 6
Training loss: 3.5432928204186664
Validation loss: 3.0714939823720715

Epoch: 6| Step: 7
Training loss: 3.3179741018892996
Validation loss: 3.0740179406044317

Epoch: 6| Step: 8
Training loss: 2.696369969895987
Validation loss: 3.0736419144479648

Epoch: 6| Step: 9
Training loss: 2.8889103933495797
Validation loss: 3.073339355552627

Epoch: 6| Step: 10
Training loss: 3.6143287913524844
Validation loss: 3.068817737542593

Epoch: 6| Step: 11
Training loss: 3.46581714809485
Validation loss: 3.0660340655374068

Epoch: 6| Step: 12
Training loss: 2.960927837426516
Validation loss: 3.067811868335094

Epoch: 6| Step: 13
Training loss: 3.9828209333304327
Validation loss: 3.0676952525924306

Epoch: 13| Step: 0
Training loss: 3.297440177600424
Validation loss: 3.066521773609255

Epoch: 6| Step: 1
Training loss: 3.144623445853404
Validation loss: 3.063179856446493

Epoch: 6| Step: 2
Training loss: 4.261390904194837
Validation loss: 3.063654636939563

Epoch: 6| Step: 3
Training loss: 3.1833548426941656
Validation loss: 3.06292658855273

Epoch: 6| Step: 4
Training loss: 2.035990421430019
Validation loss: 3.0646768885610634

Epoch: 6| Step: 5
Training loss: 3.782834564358927
Validation loss: 3.061314743346816

Epoch: 6| Step: 6
Training loss: 3.488890318117706
Validation loss: 3.0912331620622986

Epoch: 6| Step: 7
Training loss: 3.024861791306563
Validation loss: 3.0597181763541235

Epoch: 6| Step: 8
Training loss: 3.4275546382719915
Validation loss: 3.0578634755221588

Epoch: 6| Step: 9
Training loss: 3.555112708514212
Validation loss: 3.063070267167022

Epoch: 6| Step: 10
Training loss: 3.251871157234615
Validation loss: 3.073279595290852

Epoch: 6| Step: 11
Training loss: 2.7822030823969732
Validation loss: 3.1262726364303215

Epoch: 6| Step: 12
Training loss: 3.706168296389427
Validation loss: 3.1140381731065947

Epoch: 6| Step: 13
Training loss: 3.634969354051698
Validation loss: 3.0606127547836284

Epoch: 14| Step: 0
Training loss: 2.645904500009202
Validation loss: 3.0548215132300762

Epoch: 6| Step: 1
Training loss: 3.699943109023403
Validation loss: 3.0682056409089746

Epoch: 6| Step: 2
Training loss: 4.1079233600781055
Validation loss: 3.1081954888100607

Epoch: 6| Step: 3
Training loss: 2.850600156000425
Validation loss: 3.0755256724410924

Epoch: 6| Step: 4
Training loss: 2.770281131951923
Validation loss: 3.045941170457767

Epoch: 6| Step: 5
Training loss: 3.2410311555668483
Validation loss: 3.039540072654221

Epoch: 6| Step: 6
Training loss: 4.099915498932712
Validation loss: 3.0361931477015194

Epoch: 6| Step: 7
Training loss: 3.5099650619811196
Validation loss: 3.0434882227021824

Epoch: 6| Step: 8
Training loss: 3.780156127087056
Validation loss: 3.0458611891766

Epoch: 6| Step: 9
Training loss: 3.014034978682591
Validation loss: 3.043399899089257

Epoch: 6| Step: 10
Training loss: 3.4087579576142732
Validation loss: 3.0402757670371305

Epoch: 6| Step: 11
Training loss: 2.291590001528943
Validation loss: 3.0379922924068596

Epoch: 6| Step: 12
Training loss: 3.709560805490174
Validation loss: 3.0356190036322763

Epoch: 6| Step: 13
Training loss: 2.8403225879404004
Validation loss: 3.034701517916055

Epoch: 15| Step: 0
Training loss: 2.510766307072498
Validation loss: 3.0341951883719998

Epoch: 6| Step: 1
Training loss: 3.7762551854592674
Validation loss: 3.030082309427606

Epoch: 6| Step: 2
Training loss: 3.443543825710636
Validation loss: 3.0288892407323638

Epoch: 6| Step: 3
Training loss: 3.7774903072735224
Validation loss: 3.028093278173241

Epoch: 6| Step: 4
Training loss: 2.725468282097587
Validation loss: 3.022040012754953

Epoch: 6| Step: 5
Training loss: 3.472600638219995
Validation loss: 3.01614254724768

Epoch: 6| Step: 6
Training loss: 3.0551551402356316
Validation loss: 3.02138533479851

Epoch: 6| Step: 7
Training loss: 2.5903957693471873
Validation loss: 3.0211808703106997

Epoch: 6| Step: 8
Training loss: 4.240626601216767
Validation loss: 3.016340436639559

Epoch: 6| Step: 9
Training loss: 3.303508142284892
Validation loss: 3.0177543051618794

Epoch: 6| Step: 10
Training loss: 2.7857675617139974
Validation loss: 3.0164828471601255

Epoch: 6| Step: 11
Training loss: 3.5092919846836126
Validation loss: 3.0140677712892074

Epoch: 6| Step: 12
Training loss: 3.0531810743635175
Validation loss: 3.012930424775577

Epoch: 6| Step: 13
Training loss: 3.907018845234694
Validation loss: 3.0094991663179

Epoch: 16| Step: 0
Training loss: 3.2949016143049277
Validation loss: 3.0105668921886717

Epoch: 6| Step: 1
Training loss: 3.2320049650890623
Validation loss: 3.0088400419617916

Epoch: 6| Step: 2
Training loss: 3.9334955591487293
Validation loss: 3.007120446552557

Epoch: 6| Step: 3
Training loss: 2.501848109927724
Validation loss: 3.00474528877072

Epoch: 6| Step: 4
Training loss: 3.998604054054804
Validation loss: 3.0074118513696453

Epoch: 6| Step: 5
Training loss: 3.4227455460133034
Validation loss: 3.0022985410285665

Epoch: 6| Step: 6
Training loss: 3.692531708805251
Validation loss: 3.0005107841314373

Epoch: 6| Step: 7
Training loss: 3.152568067016998
Validation loss: 2.996851437005638

Epoch: 6| Step: 8
Training loss: 2.834473287104258
Validation loss: 2.9991268917832437

Epoch: 6| Step: 9
Training loss: 3.677774049258255
Validation loss: 2.9974618920162053

Epoch: 6| Step: 10
Training loss: 2.420979881994019
Validation loss: 2.9980343810554633

Epoch: 6| Step: 11
Training loss: 3.5809738611941393
Validation loss: 2.9951215990115627

Epoch: 6| Step: 12
Training loss: 2.550422865269015
Validation loss: 2.9973432183237163

Epoch: 6| Step: 13
Training loss: 3.4836831539271143
Validation loss: 2.997483442188723

Epoch: 17| Step: 0
Training loss: 3.113156267096044
Validation loss: 2.994019053107613

Epoch: 6| Step: 1
Training loss: 2.7269960075212105
Validation loss: 2.990756799526

Epoch: 6| Step: 2
Training loss: 3.2542608647179416
Validation loss: 2.990629271244056

Epoch: 6| Step: 3
Training loss: 3.3513799586525996
Validation loss: 2.992478581332164

Epoch: 6| Step: 4
Training loss: 2.4149572202001277
Validation loss: 2.99289352863766

Epoch: 6| Step: 5
Training loss: 3.6449306714825935
Validation loss: 2.9910407364076925

Epoch: 6| Step: 6
Training loss: 3.3608541381656094
Validation loss: 2.9879568662911926

Epoch: 6| Step: 7
Training loss: 3.295525875933945
Validation loss: 2.990436440951831

Epoch: 6| Step: 8
Training loss: 3.4404722368661087
Validation loss: 2.9869540272013024

Epoch: 6| Step: 9
Training loss: 3.340549541652447
Validation loss: 2.990922999362077

Epoch: 6| Step: 10
Training loss: 3.216147546600421
Validation loss: 2.9893551419568287

Epoch: 6| Step: 11
Training loss: 3.3996286694324214
Validation loss: 2.9892496325304654

Epoch: 6| Step: 12
Training loss: 3.794630603930781
Validation loss: 2.9871663656626253

Epoch: 6| Step: 13
Training loss: 3.6512228811498253
Validation loss: 2.987527366221425

Epoch: 18| Step: 0
Training loss: 3.219755737801922
Validation loss: 2.9885229427046887

Epoch: 6| Step: 1
Training loss: 3.140285948106711
Validation loss: 2.98528384149986

Epoch: 6| Step: 2
Training loss: 2.6093260252234107
Validation loss: 2.990891432432544

Epoch: 6| Step: 3
Training loss: 3.156089552020978
Validation loss: 2.999758329535939

Epoch: 6| Step: 4
Training loss: 3.8749785884142565
Validation loss: 3.0069619020010223

Epoch: 6| Step: 5
Training loss: 3.767085002072796
Validation loss: 3.004158906675024

Epoch: 6| Step: 6
Training loss: 3.5820064897419868
Validation loss: 2.9835789739932013

Epoch: 6| Step: 7
Training loss: 2.323322847945617
Validation loss: 2.989857599358335

Epoch: 6| Step: 8
Training loss: 3.750708449518706
Validation loss: 3.004512754493801

Epoch: 6| Step: 9
Training loss: 3.030089480649406
Validation loss: 2.9941221682824355

Epoch: 6| Step: 10
Training loss: 3.3437564172415786
Validation loss: 2.981704779247645

Epoch: 6| Step: 11
Training loss: 2.7847851349490598
Validation loss: 2.977910397729841

Epoch: 6| Step: 12
Training loss: 3.554780325882769
Validation loss: 2.9906179550022256

Epoch: 6| Step: 13
Training loss: 3.6214619174689293
Validation loss: 2.99516382730801

Epoch: 19| Step: 0
Training loss: 2.831243829109928
Validation loss: 2.9826231053046026

Epoch: 6| Step: 1
Training loss: 3.5372363635921964
Validation loss: 2.975713631478778

Epoch: 6| Step: 2
Training loss: 4.082953039612584
Validation loss: 2.977973573160711

Epoch: 6| Step: 3
Training loss: 3.267418327916101
Validation loss: 2.97240415591386

Epoch: 6| Step: 4
Training loss: 2.978587985801352
Validation loss: 2.970798513202694

Epoch: 6| Step: 5
Training loss: 2.8236229401295994
Validation loss: 2.969253773629317

Epoch: 6| Step: 6
Training loss: 3.7397623825067154
Validation loss: 2.971183438412202

Epoch: 6| Step: 7
Training loss: 2.506171429322882
Validation loss: 2.9757992956148436

Epoch: 6| Step: 8
Training loss: 3.2507018285179106
Validation loss: 2.97654647678572

Epoch: 6| Step: 9
Training loss: 3.168243283419013
Validation loss: 2.9769875944172717

Epoch: 6| Step: 10
Training loss: 3.937367028307189
Validation loss: 2.9769681400551034

Epoch: 6| Step: 11
Training loss: 3.7089499200282208
Validation loss: 2.974014077977746

Epoch: 6| Step: 12
Training loss: 2.8240699992703453
Validation loss: 2.9707363752161076

Epoch: 6| Step: 13
Training loss: 2.1728341159763063
Validation loss: 2.9645574994829094

Epoch: 20| Step: 0
Training loss: 3.740877372112536
Validation loss: 2.964700456116644

Epoch: 6| Step: 1
Training loss: 3.8993860592924494
Validation loss: 2.9622902746054143

Epoch: 6| Step: 2
Training loss: 2.5949549805956864
Validation loss: 2.962702565631834

Epoch: 6| Step: 3
Training loss: 3.1807658548437425
Validation loss: 2.958994494442223

Epoch: 6| Step: 4
Training loss: 3.6560362402874795
Validation loss: 2.960332220887549

Epoch: 6| Step: 5
Training loss: 3.5328693559096256
Validation loss: 2.960925042547231

Epoch: 6| Step: 6
Training loss: 2.875216600302876
Validation loss: 2.958895554616175

Epoch: 6| Step: 7
Training loss: 2.6511289710837493
Validation loss: 2.9610790704025445

Epoch: 6| Step: 8
Training loss: 3.005057998326026
Validation loss: 2.9600015853189148

Epoch: 6| Step: 9
Training loss: 3.170041912295168
Validation loss: 2.9585033184773692

Epoch: 6| Step: 10
Training loss: 3.3404100796799256
Validation loss: 2.9589206589738413

Epoch: 6| Step: 11
Training loss: 3.447795362537991
Validation loss: 2.9541733740743985

Epoch: 6| Step: 12
Training loss: 3.0892254660517318
Validation loss: 2.9526853627516116

Epoch: 6| Step: 13
Training loss: 3.078076977040521
Validation loss: 2.952948960049284

Epoch: 21| Step: 0
Training loss: 3.1188486209787505
Validation loss: 2.9531565782410714

Epoch: 6| Step: 1
Training loss: 3.507811004706321
Validation loss: 2.954464889444499

Epoch: 6| Step: 2
Training loss: 2.9791444008446426
Validation loss: 2.957522836870807

Epoch: 6| Step: 3
Training loss: 3.157298452147053
Validation loss: 2.962564331702009

Epoch: 6| Step: 4
Training loss: 3.359389478075633
Validation loss: 2.9682650901392122

Epoch: 6| Step: 5
Training loss: 3.1567497235508983
Validation loss: 2.9575652786853612

Epoch: 6| Step: 6
Training loss: 2.492010797915019
Validation loss: 2.952352271328914

Epoch: 6| Step: 7
Training loss: 3.345013246153647
Validation loss: 2.9481902036717105

Epoch: 6| Step: 8
Training loss: 3.18980145114543
Validation loss: 2.949014321429604

Epoch: 6| Step: 9
Training loss: 3.2369024012397047
Validation loss: 2.9490112796731953

Epoch: 6| Step: 10
Training loss: 3.1710991873721555
Validation loss: 2.947830891764513

Epoch: 6| Step: 11
Training loss: 3.640750735489694
Validation loss: 2.953208360061364

Epoch: 6| Step: 12
Training loss: 3.2079217489882503
Validation loss: 2.9529207385441065

Epoch: 6| Step: 13
Training loss: 4.11359493439762
Validation loss: 2.9435175811059087

Epoch: 22| Step: 0
Training loss: 3.8526292913625935
Validation loss: 2.9462583886834777

Epoch: 6| Step: 1
Training loss: 3.1775391465363563
Validation loss: 2.9568599606289125

Epoch: 6| Step: 2
Training loss: 3.1906058943248365
Validation loss: 2.957460367944137

Epoch: 6| Step: 3
Training loss: 3.8893244151302073
Validation loss: 2.956290790555942

Epoch: 6| Step: 4
Training loss: 4.0331113791017765
Validation loss: 2.9552754363120672

Epoch: 6| Step: 5
Training loss: 3.319477434052032
Validation loss: 2.9710734732518236

Epoch: 6| Step: 6
Training loss: 3.115200394113258
Validation loss: 2.954836545881088

Epoch: 6| Step: 7
Training loss: 2.9551978531388197
Validation loss: 2.9691005998459095

Epoch: 6| Step: 8
Training loss: 3.3130079635676393
Validation loss: 2.965032470746119

Epoch: 6| Step: 9
Training loss: 2.8740730449868392
Validation loss: 2.953242417506974

Epoch: 6| Step: 10
Training loss: 2.421315183850185
Validation loss: 2.9519423090239703

Epoch: 6| Step: 11
Training loss: 3.045594244538849
Validation loss: 2.955628526188629

Epoch: 6| Step: 12
Training loss: 2.577447975736262
Validation loss: 2.9522081158320255

Epoch: 6| Step: 13
Training loss: 3.35395695443416
Validation loss: 2.9534221742066125

Epoch: 23| Step: 0
Training loss: 3.6094048569009973
Validation loss: 2.952287435267375

Epoch: 6| Step: 1
Training loss: 2.9747493157434484
Validation loss: 2.9431359380691524

Epoch: 6| Step: 2
Training loss: 3.666669426541301
Validation loss: 2.9412840141640877

Epoch: 6| Step: 3
Training loss: 2.836076549137191
Validation loss: 2.944934063289649

Epoch: 6| Step: 4
Training loss: 3.8497874907265834
Validation loss: 2.97414008704497

Epoch: 6| Step: 5
Training loss: 3.152531917207766
Validation loss: 2.9408076194008363

Epoch: 6| Step: 6
Training loss: 2.3103511208219314
Validation loss: 2.9310108405862865

Epoch: 6| Step: 7
Training loss: 2.366495767879349
Validation loss: 2.9382760884447072

Epoch: 6| Step: 8
Training loss: 3.0764012811285357
Validation loss: 2.949261094872985

Epoch: 6| Step: 9
Training loss: 2.951010295047208
Validation loss: 2.9570141603496167

Epoch: 6| Step: 10
Training loss: 3.9145280937992455
Validation loss: 2.942150044204576

Epoch: 6| Step: 11
Training loss: 3.330781691164153
Validation loss: 2.9287274955319162

Epoch: 6| Step: 12
Training loss: 3.5628114530710544
Validation loss: 2.9252746626262907

Epoch: 6| Step: 13
Training loss: 3.2031202362769453
Validation loss: 2.9211572407430753

Epoch: 24| Step: 0
Training loss: 3.0326248669055698
Validation loss: 2.923356652757244

Epoch: 6| Step: 1
Training loss: 3.820122150445419
Validation loss: 2.924449482890422

Epoch: 6| Step: 2
Training loss: 3.169816876245482
Validation loss: 2.922876782598671

Epoch: 6| Step: 3
Training loss: 3.698693178700225
Validation loss: 2.9234430039731425

Epoch: 6| Step: 4
Training loss: 3.392408332682594
Validation loss: 2.919310419006112

Epoch: 6| Step: 5
Training loss: 2.7646663750080402
Validation loss: 2.9184413043562016

Epoch: 6| Step: 6
Training loss: 3.3238394640774973
Validation loss: 2.918351818399648

Epoch: 6| Step: 7
Training loss: 3.129661897429836
Validation loss: 2.9183327699200876

Epoch: 6| Step: 8
Training loss: 3.1348873126692713
Validation loss: 2.915879292424844

Epoch: 6| Step: 9
Training loss: 3.5150227348710215
Validation loss: 2.9166391034442642

Epoch: 6| Step: 10
Training loss: 3.005921559623175
Validation loss: 2.915531741595555

Epoch: 6| Step: 11
Training loss: 3.1246038567272185
Validation loss: 2.9210559784396612

Epoch: 6| Step: 12
Training loss: 3.133255544332653
Validation loss: 2.9220475119644496

Epoch: 6| Step: 13
Training loss: 2.1780880764823984
Validation loss: 2.914053786083102

Epoch: 25| Step: 0
Training loss: 3.388638525367235
Validation loss: 2.916122634238904

Epoch: 6| Step: 1
Training loss: 2.214299856747234
Validation loss: 2.911929570970195

Epoch: 6| Step: 2
Training loss: 3.2164642217571378
Validation loss: 2.9103899950864527

Epoch: 6| Step: 3
Training loss: 2.1910981150343387
Validation loss: 2.9220608738818887

Epoch: 6| Step: 4
Training loss: 3.8470530390502953
Validation loss: 2.94093860172236

Epoch: 6| Step: 5
Training loss: 2.957473701624544
Validation loss: 2.9442733559640706

Epoch: 6| Step: 6
Training loss: 2.727637486196345
Validation loss: 2.9258833476268022

Epoch: 6| Step: 7
Training loss: 3.736554136484626
Validation loss: 2.9196711001053837

Epoch: 6| Step: 8
Training loss: 2.9523220686306955
Validation loss: 2.921567935721346

Epoch: 6| Step: 9
Training loss: 3.482483217701453
Validation loss: 2.9187621773532535

Epoch: 6| Step: 10
Training loss: 3.9530730112147725
Validation loss: 2.9163058173453917

Epoch: 6| Step: 11
Training loss: 3.1792032308879805
Validation loss: 2.909256916996708

Epoch: 6| Step: 12
Training loss: 3.110892687426409
Validation loss: 2.902816902382785

Epoch: 6| Step: 13
Training loss: 3.5266318041034377
Validation loss: 2.9081871729615143

Epoch: 26| Step: 0
Training loss: 3.207337823634578
Validation loss: 2.910759380635072

Epoch: 6| Step: 1
Training loss: 3.4601115668506575
Validation loss: 2.919319551045342

Epoch: 6| Step: 2
Training loss: 3.5634202270570907
Validation loss: 2.9123379475602533

Epoch: 6| Step: 3
Training loss: 2.8387971442275015
Validation loss: 2.90253208202837

Epoch: 6| Step: 4
Training loss: 3.16516787211854
Validation loss: 2.899598555807663

Epoch: 6| Step: 5
Training loss: 3.272149098746307
Validation loss: 2.8985660790800925

Epoch: 6| Step: 6
Training loss: 2.9282465531921553
Validation loss: 2.897304516795942

Epoch: 6| Step: 7
Training loss: 2.476172672240909
Validation loss: 2.8988278224034723

Epoch: 6| Step: 8
Training loss: 3.4164617213367654
Validation loss: 2.8970274417892607

Epoch: 6| Step: 9
Training loss: 3.5364616892166207
Validation loss: 2.898401876038894

Epoch: 6| Step: 10
Training loss: 3.0600015218893017
Validation loss: 2.89824191042349

Epoch: 6| Step: 11
Training loss: 3.090640113724184
Validation loss: 2.902589620311894

Epoch: 6| Step: 12
Training loss: 3.148198802598653
Validation loss: 2.964718507125891

Epoch: 6| Step: 13
Training loss: 3.578076358114722
Validation loss: 3.0166097303392987

Epoch: 27| Step: 0
Training loss: 2.7951708668734923
Validation loss: 2.9052607651907487

Epoch: 6| Step: 1
Training loss: 3.487793208003019
Validation loss: 2.9038675135407703

Epoch: 6| Step: 2
Training loss: 3.667833937177425
Validation loss: 2.945032196010998

Epoch: 6| Step: 3
Training loss: 3.2796536876867393
Validation loss: 2.986684160000943

Epoch: 6| Step: 4
Training loss: 4.002309609244636
Validation loss: 2.9686991732034675

Epoch: 6| Step: 5
Training loss: 2.569007611873363
Validation loss: 2.909169785162591

Epoch: 6| Step: 6
Training loss: 3.307271321552006
Validation loss: 2.891219810742241

Epoch: 6| Step: 7
Training loss: 3.3810035150346645
Validation loss: 2.8926079783770993

Epoch: 6| Step: 8
Training loss: 2.8662350603222526
Validation loss: 2.8941015726960932

Epoch: 6| Step: 9
Training loss: 3.528874231057696
Validation loss: 2.90483965588585

Epoch: 6| Step: 10
Training loss: 3.1730951677700414
Validation loss: 2.90894534634739

Epoch: 6| Step: 11
Training loss: 2.534200949754252
Validation loss: 2.9160567710972614

Epoch: 6| Step: 12
Training loss: 2.929868972244159
Validation loss: 2.9204683118549175

Epoch: 6| Step: 13
Training loss: 3.4789204005877297
Validation loss: 2.914052995187184

Epoch: 28| Step: 0
Training loss: 3.650399899458189
Validation loss: 2.9194324552549373

Epoch: 6| Step: 1
Training loss: 3.4514617366424383
Validation loss: 2.903466516460617

Epoch: 6| Step: 2
Training loss: 2.3347106569714637
Validation loss: 2.892792574310641

Epoch: 6| Step: 3
Training loss: 3.8168607692974796
Validation loss: 2.886162713216608

Epoch: 6| Step: 4
Training loss: 2.8352051517056576
Validation loss: 2.884544963813236

Epoch: 6| Step: 5
Training loss: 2.842054427493508
Validation loss: 2.8846448896280608

Epoch: 6| Step: 6
Training loss: 3.1645793728095244
Validation loss: 2.880896685959863

Epoch: 6| Step: 7
Training loss: 2.8257902846061422
Validation loss: 2.8831603422330834

Epoch: 6| Step: 8
Training loss: 3.4283626839125185
Validation loss: 2.8789985060799497

Epoch: 6| Step: 9
Training loss: 2.885312038319544
Validation loss: 2.8835991409421218

Epoch: 6| Step: 10
Training loss: 3.477752002001689
Validation loss: 2.8829878188978513

Epoch: 6| Step: 11
Training loss: 3.0495487317045806
Validation loss: 2.8788781566470516

Epoch: 6| Step: 12
Training loss: 3.4751747361854317
Validation loss: 2.876917568278589

Epoch: 6| Step: 13
Training loss: 3.1044935698624636
Validation loss: 2.874502506344218

Epoch: 29| Step: 0
Training loss: 3.0316596000243696
Validation loss: 2.876698195277396

Epoch: 6| Step: 1
Training loss: 3.3912868996344625
Validation loss: 2.878951506187247

Epoch: 6| Step: 2
Training loss: 3.0575702460166263
Validation loss: 2.881745292608054

Epoch: 6| Step: 3
Training loss: 3.2798046016641993
Validation loss: 2.8848041908952777

Epoch: 6| Step: 4
Training loss: 2.9427534161830535
Validation loss: 2.891218624339235

Epoch: 6| Step: 5
Training loss: 3.0461885119068457
Validation loss: 2.889484102100395

Epoch: 6| Step: 6
Training loss: 3.2576832551270027
Validation loss: 2.8884716976720792

Epoch: 6| Step: 7
Training loss: 3.4852463526516853
Validation loss: 2.8776424715923246

Epoch: 6| Step: 8
Training loss: 2.9491537529054614
Validation loss: 2.8694402627214592

Epoch: 6| Step: 9
Training loss: 3.726647845876411
Validation loss: 2.8702367761942083

Epoch: 6| Step: 10
Training loss: 2.928812369294293
Validation loss: 2.8680765698663926

Epoch: 6| Step: 11
Training loss: 3.3737816200880495
Validation loss: 2.869441413457552

Epoch: 6| Step: 12
Training loss: 2.9637951108410667
Validation loss: 2.871341808731899

Epoch: 6| Step: 13
Training loss: 2.7970298852070665
Validation loss: 2.875001267998671

Epoch: 30| Step: 0
Training loss: 2.74175917711876
Validation loss: 2.882129625549325

Epoch: 6| Step: 1
Training loss: 2.626415143710262
Validation loss: 2.8892874210775608

Epoch: 6| Step: 2
Training loss: 3.8286665552843244
Validation loss: 2.905197296041017

Epoch: 6| Step: 3
Training loss: 2.9977216016034935
Validation loss: 2.8868596219945126

Epoch: 6| Step: 4
Training loss: 3.505370379590138
Validation loss: 2.8720792100294794

Epoch: 6| Step: 5
Training loss: 3.289164851899891
Validation loss: 2.866509194733859

Epoch: 6| Step: 6
Training loss: 3.357642006217897
Validation loss: 2.864532777819309

Epoch: 6| Step: 7
Training loss: 3.033336904195284
Validation loss: 2.8655739535541653

Epoch: 6| Step: 8
Training loss: 2.8844161703327
Validation loss: 2.8732542361589797

Epoch: 6| Step: 9
Training loss: 3.17442151904734
Validation loss: 2.878661001843766

Epoch: 6| Step: 10
Training loss: 3.2193807335789377
Validation loss: 2.888670288184012

Epoch: 6| Step: 11
Training loss: 2.8217928480523398
Validation loss: 2.882162900485356

Epoch: 6| Step: 12
Training loss: 3.4464948013910632
Validation loss: 2.871784464697984

Epoch: 6| Step: 13
Training loss: 3.574950696698583
Validation loss: 2.8651596345792867

Epoch: 31| Step: 0
Training loss: 3.0452706049801472
Validation loss: 2.8602983649339158

Epoch: 6| Step: 1
Training loss: 3.225529318256861
Validation loss: 2.860046913857357

Epoch: 6| Step: 2
Training loss: 3.3989265879205512
Validation loss: 2.857941831666541

Epoch: 6| Step: 3
Training loss: 2.423878880537278
Validation loss: 2.8624503497180984

Epoch: 6| Step: 4
Training loss: 3.665223935976422
Validation loss: 2.8578812783589496

Epoch: 6| Step: 5
Training loss: 3.746743886656421
Validation loss: 2.854237808791215

Epoch: 6| Step: 6
Training loss: 3.3959318899506923
Validation loss: 2.852558600657173

Epoch: 6| Step: 7
Training loss: 3.596572165910826
Validation loss: 2.8473156005716467

Epoch: 6| Step: 8
Training loss: 3.0897881928475543
Validation loss: 2.8533124161044525

Epoch: 6| Step: 9
Training loss: 2.7944130744294036
Validation loss: 2.85254780705515

Epoch: 6| Step: 10
Training loss: 3.3541982108265764
Validation loss: 2.8541476589857693

Epoch: 6| Step: 11
Training loss: 2.278427991354102
Validation loss: 2.85532446625899

Epoch: 6| Step: 12
Training loss: 2.900909767827862
Validation loss: 2.8552690399137113

Epoch: 6| Step: 13
Training loss: 2.9269212200483192
Validation loss: 2.8566654232352358

Epoch: 32| Step: 0
Training loss: 3.28837987924519
Validation loss: 2.8594831606144373

Epoch: 6| Step: 1
Training loss: 2.91988709087768
Validation loss: 2.86009131396455

Epoch: 6| Step: 2
Training loss: 3.3726326445671777
Validation loss: 2.8521659802485813

Epoch: 6| Step: 3
Training loss: 2.994636191050266
Validation loss: 2.8489248778421947

Epoch: 6| Step: 4
Training loss: 3.1682089680425984
Validation loss: 2.8469401354796995

Epoch: 6| Step: 5
Training loss: 3.0622988264999877
Validation loss: 2.846230247572737

Epoch: 6| Step: 6
Training loss: 2.7755131221317417
Validation loss: 2.8438547505681666

Epoch: 6| Step: 7
Training loss: 2.7645968665160905
Validation loss: 2.8489534231371074

Epoch: 6| Step: 8
Training loss: 3.4091875201320794
Validation loss: 2.852701350964531

Epoch: 6| Step: 9
Training loss: 3.633275943851152
Validation loss: 2.850908881423127

Epoch: 6| Step: 10
Training loss: 2.7067127563360875
Validation loss: 2.8491172529489006

Epoch: 6| Step: 11
Training loss: 3.5486246910424293
Validation loss: 2.847295997624913

Epoch: 6| Step: 12
Training loss: 3.270163854290964
Validation loss: 2.855971118435013

Epoch: 6| Step: 13
Training loss: 3.3012995386360853
Validation loss: 2.8489527086533593

Epoch: 33| Step: 0
Training loss: 2.1664087680013764
Validation loss: 2.844837123812729

Epoch: 6| Step: 1
Training loss: 2.788011197827043
Validation loss: 2.8398100378761746

Epoch: 6| Step: 2
Training loss: 3.2912590321707746
Validation loss: 2.8348884555309133

Epoch: 6| Step: 3
Training loss: 3.2082569228812288
Validation loss: 2.8397833506331684

Epoch: 6| Step: 4
Training loss: 2.0701792548144735
Validation loss: 2.8366144955844774

Epoch: 6| Step: 5
Training loss: 3.9607051502826596
Validation loss: 2.8323765743243694

Epoch: 6| Step: 6
Training loss: 3.777838058240071
Validation loss: 2.833308235783565

Epoch: 6| Step: 7
Training loss: 2.763743735952883
Validation loss: 2.8323412599429787

Epoch: 6| Step: 8
Training loss: 3.836512877903174
Validation loss: 2.8277989473034073

Epoch: 6| Step: 9
Training loss: 3.2922708624816703
Validation loss: 2.828314163498164

Epoch: 6| Step: 10
Training loss: 2.8201900043461636
Validation loss: 2.828678838090975

Epoch: 6| Step: 11
Training loss: 3.1079989508255177
Validation loss: 2.827223043881141

Epoch: 6| Step: 12
Training loss: 3.1452778374192234
Validation loss: 2.8238695138893717

Epoch: 6| Step: 13
Training loss: 2.9668483064172686
Validation loss: 2.8262126237723644

Epoch: 34| Step: 0
Training loss: 2.634554414081135
Validation loss: 2.825195615578489

Epoch: 6| Step: 1
Training loss: 3.623545256110857
Validation loss: 2.827162347682458

Epoch: 6| Step: 2
Training loss: 3.3465604290558186
Validation loss: 2.830116614520455

Epoch: 6| Step: 3
Training loss: 3.133022386935429
Validation loss: 2.830289708531153

Epoch: 6| Step: 4
Training loss: 3.21752995451149
Validation loss: 2.8265742118675026

Epoch: 6| Step: 5
Training loss: 3.4095801077596737
Validation loss: 2.8236291566919665

Epoch: 6| Step: 6
Training loss: 3.202859709383657
Validation loss: 2.8242117186802473

Epoch: 6| Step: 7
Training loss: 2.771058687687858
Validation loss: 2.821339499077762

Epoch: 6| Step: 8
Training loss: 2.867369435538155
Validation loss: 2.8219812168629312

Epoch: 6| Step: 9
Training loss: 2.8797472032224936
Validation loss: 2.8217542504445383

Epoch: 6| Step: 10
Training loss: 3.155488177223404
Validation loss: 2.822491491231116

Epoch: 6| Step: 11
Training loss: 2.8863794439670722
Validation loss: 2.82102058563333

Epoch: 6| Step: 12
Training loss: 3.141023373365956
Validation loss: 2.8234948370387216

Epoch: 6| Step: 13
Training loss: 3.578564745937838
Validation loss: 2.8237181863143532

Epoch: 35| Step: 0
Training loss: 3.0913844457035258
Validation loss: 2.8230026977298808

Epoch: 6| Step: 1
Training loss: 3.1948743300579574
Validation loss: 2.824757025169712

Epoch: 6| Step: 2
Training loss: 2.669728002088051
Validation loss: 2.8273426959792514

Epoch: 6| Step: 3
Training loss: 3.6055139805363083
Validation loss: 2.8233947500429006

Epoch: 6| Step: 4
Training loss: 3.808850777585103
Validation loss: 2.8216239464574175

Epoch: 6| Step: 5
Training loss: 3.406389846030852
Validation loss: 2.8209749853281934

Epoch: 6| Step: 6
Training loss: 2.8011457347062447
Validation loss: 2.81905899095049

Epoch: 6| Step: 7
Training loss: 2.5416428820221553
Validation loss: 2.8172993118111993

Epoch: 6| Step: 8
Training loss: 2.932385151232849
Validation loss: 2.814779129760412

Epoch: 6| Step: 9
Training loss: 3.2421354358583234
Validation loss: 2.8144792472133378

Epoch: 6| Step: 10
Training loss: 2.7753131380478835
Validation loss: 2.812678050030518

Epoch: 6| Step: 11
Training loss: 2.9899726817946712
Validation loss: 2.812422574708296

Epoch: 6| Step: 12
Training loss: 3.516643054635807
Validation loss: 2.8163969563073463

Epoch: 6| Step: 13
Training loss: 2.9892176139693394
Validation loss: 2.821683455295902

Epoch: 36| Step: 0
Training loss: 3.4358089102031912
Validation loss: 2.82330792944089

Epoch: 6| Step: 1
Training loss: 3.699027712117692
Validation loss: 2.8173546898492297

Epoch: 6| Step: 2
Training loss: 3.5296917743496556
Validation loss: 2.8246260973179242

Epoch: 6| Step: 3
Training loss: 3.5455943749159387
Validation loss: 2.810446359733608

Epoch: 6| Step: 4
Training loss: 2.9922270211978765
Validation loss: 2.804694026350205

Epoch: 6| Step: 5
Training loss: 2.651323664467824
Validation loss: 2.8061989865728605

Epoch: 6| Step: 6
Training loss: 2.9535002293442147
Validation loss: 2.8072580780741867

Epoch: 6| Step: 7
Training loss: 2.928712077202596
Validation loss: 2.806311588309012

Epoch: 6| Step: 8
Training loss: 2.840786574062719
Validation loss: 2.8055339395990004

Epoch: 6| Step: 9
Training loss: 3.2966502325447427
Validation loss: 2.8052112287907747

Epoch: 6| Step: 10
Training loss: 2.656896714413643
Validation loss: 2.804746099499656

Epoch: 6| Step: 11
Training loss: 2.572181740843455
Validation loss: 2.8059411908978396

Epoch: 6| Step: 12
Training loss: 3.5029777393815307
Validation loss: 2.8047701751208085

Epoch: 6| Step: 13
Training loss: 2.6781044780074006
Validation loss: 2.8048347879844084

Epoch: 37| Step: 0
Training loss: 3.3581696942946033
Validation loss: 2.8036252117898015

Epoch: 6| Step: 1
Training loss: 3.7300236461391014
Validation loss: 2.803302085547895

Epoch: 6| Step: 2
Training loss: 2.6005497241247406
Validation loss: 2.803008278232139

Epoch: 6| Step: 3
Training loss: 2.6571873525623726
Validation loss: 2.800474416223809

Epoch: 6| Step: 4
Training loss: 2.7295859987792004
Validation loss: 2.7992390041639665

Epoch: 6| Step: 5
Training loss: 2.207537729668822
Validation loss: 2.799573481870003

Epoch: 6| Step: 6
Training loss: 2.5552552318558623
Validation loss: 2.7999813712067287

Epoch: 6| Step: 7
Training loss: 3.099827201703334
Validation loss: 2.7968640758332914

Epoch: 6| Step: 8
Training loss: 2.9930240268526718
Validation loss: 2.797849300181522

Epoch: 6| Step: 9
Training loss: 3.6112046661647654
Validation loss: 2.796410905303354

Epoch: 6| Step: 10
Training loss: 3.4759777477416924
Validation loss: 2.795571833952776

Epoch: 6| Step: 11
Training loss: 3.336419457097859
Validation loss: 2.7937651344636882

Epoch: 6| Step: 12
Training loss: 3.5716225789372693
Validation loss: 2.792304868912497

Epoch: 6| Step: 13
Training loss: 3.3469442636426834
Validation loss: 2.79273691234156

Epoch: 38| Step: 0
Training loss: 2.861036474577405
Validation loss: 2.7922490133477984

Epoch: 6| Step: 1
Training loss: 2.739951675852083
Validation loss: 2.791321071109022

Epoch: 6| Step: 2
Training loss: 3.4241083251359945
Validation loss: 2.7898972345839157

Epoch: 6| Step: 3
Training loss: 3.1052335062236187
Validation loss: 2.7900112559790964

Epoch: 6| Step: 4
Training loss: 3.4193691327660547
Validation loss: 2.7881057700910468

Epoch: 6| Step: 5
Training loss: 2.534271320898709
Validation loss: 2.7899088320151075

Epoch: 6| Step: 6
Training loss: 3.1605621190423068
Validation loss: 2.791711689036987

Epoch: 6| Step: 7
Training loss: 2.349625407455173
Validation loss: 2.78833875960025

Epoch: 6| Step: 8
Training loss: 3.151764051794422
Validation loss: 2.788677681362551

Epoch: 6| Step: 9
Training loss: 2.910617622823215
Validation loss: 2.7872377753197157

Epoch: 6| Step: 10
Training loss: 2.9447846685937438
Validation loss: 2.7911953633088706

Epoch: 6| Step: 11
Training loss: 3.8750649723635684
Validation loss: 2.794913845386521

Epoch: 6| Step: 12
Training loss: 3.6326678503628114
Validation loss: 2.815140499874561

Epoch: 6| Step: 13
Training loss: 2.8949188508303765
Validation loss: 2.8015353405799632

Epoch: 39| Step: 0
Training loss: 2.1747397870031064
Validation loss: 2.800224902603202

Epoch: 6| Step: 1
Training loss: 3.7933497762966164
Validation loss: 2.8005924384036662

Epoch: 6| Step: 2
Training loss: 2.8544898488273005
Validation loss: 2.799193292641663

Epoch: 6| Step: 3
Training loss: 2.49669228125124
Validation loss: 2.8248486864833544

Epoch: 6| Step: 4
Training loss: 2.9416429239527044
Validation loss: 2.8276791268403065

Epoch: 6| Step: 5
Training loss: 2.8895028900485844
Validation loss: 2.8464174206840167

Epoch: 6| Step: 6
Training loss: 3.095320177911193
Validation loss: 2.8492875011474257

Epoch: 6| Step: 7
Training loss: 2.4201688601953233
Validation loss: 2.841571043842276

Epoch: 6| Step: 8
Training loss: 3.096664443109355
Validation loss: 2.8254388627853766

Epoch: 6| Step: 9
Training loss: 3.3637337729106225
Validation loss: 2.7843520058934543

Epoch: 6| Step: 10
Training loss: 2.9140705180121764
Validation loss: 2.7772378200507

Epoch: 6| Step: 11
Training loss: 3.1801067703844352
Validation loss: 2.776217176748797

Epoch: 6| Step: 12
Training loss: 4.019314388434447
Validation loss: 2.7793046287356

Epoch: 6| Step: 13
Training loss: 3.950206297303948
Validation loss: 2.778624709200184

Epoch: 40| Step: 0
Training loss: 2.8742522220886673
Validation loss: 2.7781815700598176

Epoch: 6| Step: 1
Training loss: 2.6227209097420086
Validation loss: 2.7810028794124726

Epoch: 6| Step: 2
Training loss: 3.503156328699258
Validation loss: 2.7807389154192195

Epoch: 6| Step: 3
Training loss: 3.3444166142885368
Validation loss: 2.791539994844687

Epoch: 6| Step: 4
Training loss: 2.827915752952754
Validation loss: 2.817682123990148

Epoch: 6| Step: 5
Training loss: 2.9829280201446795
Validation loss: 2.8302242709786265

Epoch: 6| Step: 6
Training loss: 2.663772432974805
Validation loss: 2.8245631742790835

Epoch: 6| Step: 7
Training loss: 3.62585307639681
Validation loss: 2.8186772389060515

Epoch: 6| Step: 8
Training loss: 3.2134248700960715
Validation loss: 2.8055581473527367

Epoch: 6| Step: 9
Training loss: 2.4464673127735597
Validation loss: 2.7796142051986936

Epoch: 6| Step: 10
Training loss: 3.7364388991354662
Validation loss: 2.7752288297569536

Epoch: 6| Step: 11
Training loss: 3.2799386446726992
Validation loss: 2.777537750894054

Epoch: 6| Step: 12
Training loss: 3.2145696363241756
Validation loss: 2.7735615290203595

Epoch: 6| Step: 13
Training loss: 2.8790308020886126
Validation loss: 2.785606163959738

Epoch: 41| Step: 0
Training loss: 2.5216877542821265
Validation loss: 2.780985365312733

Epoch: 6| Step: 1
Training loss: 2.943944476723576
Validation loss: 2.806868466451337

Epoch: 6| Step: 2
Training loss: 3.180186389522937
Validation loss: 2.8292802178014487

Epoch: 6| Step: 3
Training loss: 3.2954931753160537
Validation loss: 2.8620232800178047

Epoch: 6| Step: 4
Training loss: 3.4056461743952995
Validation loss: 2.861237543622354

Epoch: 6| Step: 5
Training loss: 2.986720736076619
Validation loss: 2.7690729156863036

Epoch: 6| Step: 6
Training loss: 3.593164147432325
Validation loss: 2.759870070525638

Epoch: 6| Step: 7
Training loss: 2.7886830988117213
Validation loss: 2.764460852635104

Epoch: 6| Step: 8
Training loss: 2.686845389196553
Validation loss: 2.7730316396648753

Epoch: 6| Step: 9
Training loss: 2.612948865522291
Validation loss: 2.8033944291600066

Epoch: 6| Step: 10
Training loss: 3.503469382516907
Validation loss: 2.8123221884683822

Epoch: 6| Step: 11
Training loss: 2.9411205948402652
Validation loss: 2.7880940364303872

Epoch: 6| Step: 12
Training loss: 3.1507030126428854
Validation loss: 2.781126962806716

Epoch: 6| Step: 13
Training loss: 3.6293808346493375
Validation loss: 2.7716313638606263

Epoch: 42| Step: 0
Training loss: 2.5789203688481472
Validation loss: 2.7660493047255006

Epoch: 6| Step: 1
Training loss: 2.9657100170880737
Validation loss: 2.7613143386928263

Epoch: 6| Step: 2
Training loss: 2.9344882541043784
Validation loss: 2.761187505254971

Epoch: 6| Step: 3
Training loss: 2.8110261234140306
Validation loss: 2.76123220447062

Epoch: 6| Step: 4
Training loss: 2.948639061147913
Validation loss: 2.7590517637971774

Epoch: 6| Step: 5
Training loss: 2.8071633464713046
Validation loss: 2.756941650793675

Epoch: 6| Step: 6
Training loss: 2.836412458344184
Validation loss: 2.7545081836485497

Epoch: 6| Step: 7
Training loss: 3.4052349292972512
Validation loss: 2.7568130055437856

Epoch: 6| Step: 8
Training loss: 3.216269120338558
Validation loss: 2.7574253999896023

Epoch: 6| Step: 9
Training loss: 2.9881519962855427
Validation loss: 2.7605007562040056

Epoch: 6| Step: 10
Training loss: 3.2529899341932937
Validation loss: 2.761811674557206

Epoch: 6| Step: 11
Training loss: 3.174252075050423
Validation loss: 2.761841753276748

Epoch: 6| Step: 12
Training loss: 3.6467226233597247
Validation loss: 2.765063329426577

Epoch: 6| Step: 13
Training loss: 3.717291666233381
Validation loss: 2.754618650795426

Epoch: 43| Step: 0
Training loss: 2.9293880055250554
Validation loss: 2.749045256616234

Epoch: 6| Step: 1
Training loss: 3.3186079609103833
Validation loss: 2.7493927491189774

Epoch: 6| Step: 2
Training loss: 3.2737802403449066
Validation loss: 2.7518775865703264

Epoch: 6| Step: 3
Training loss: 2.762731385357904
Validation loss: 2.7508121765942293

Epoch: 6| Step: 4
Training loss: 3.258401721389507
Validation loss: 2.748809178138052

Epoch: 6| Step: 5
Training loss: 3.4913619941096874
Validation loss: 2.747187435611565

Epoch: 6| Step: 6
Training loss: 3.210364668860513
Validation loss: 2.747371692878976

Epoch: 6| Step: 7
Training loss: 3.040571529471076
Validation loss: 2.748952116868272

Epoch: 6| Step: 8
Training loss: 2.473800129433116
Validation loss: 2.7484627908552306

Epoch: 6| Step: 9
Training loss: 3.0438815548644373
Validation loss: 2.7472024925486207

Epoch: 6| Step: 10
Training loss: 2.619004987807329
Validation loss: 2.744046778777132

Epoch: 6| Step: 11
Training loss: 3.6332655757453907
Validation loss: 2.742961494664557

Epoch: 6| Step: 12
Training loss: 2.939866918289571
Validation loss: 2.7473483870553563

Epoch: 6| Step: 13
Training loss: 2.560183897267849
Validation loss: 2.742314181063038

Epoch: 44| Step: 0
Training loss: 3.5146564421015407
Validation loss: 2.7465321796737814

Epoch: 6| Step: 1
Training loss: 4.207589313468051
Validation loss: 2.748562943529254

Epoch: 6| Step: 2
Training loss: 3.231760636200086
Validation loss: 2.7529788901028796

Epoch: 6| Step: 3
Training loss: 2.2002116751707916
Validation loss: 2.7515545661971537

Epoch: 6| Step: 4
Training loss: 2.0505044223502376
Validation loss: 2.7590401230740653

Epoch: 6| Step: 5
Training loss: 3.2665920742108088
Validation loss: 2.7682880613017047

Epoch: 6| Step: 6
Training loss: 2.953681691494188
Validation loss: 2.7709760040874767

Epoch: 6| Step: 7
Training loss: 3.2232766034072773
Validation loss: 2.757451396769618

Epoch: 6| Step: 8
Training loss: 3.8427058096415765
Validation loss: 2.7409070969767804

Epoch: 6| Step: 9
Training loss: 2.8595957853200025
Validation loss: 2.7411128159978753

Epoch: 6| Step: 10
Training loss: 2.5885739442794446
Validation loss: 2.736376661134125

Epoch: 6| Step: 11
Training loss: 2.7805252845463766
Validation loss: 2.7393086700956344

Epoch: 6| Step: 12
Training loss: 2.5229027715962378
Validation loss: 2.737598615611877

Epoch: 6| Step: 13
Training loss: 2.8925136685192694
Validation loss: 2.737362850790068

Epoch: 45| Step: 0
Training loss: 3.0747710414097966
Validation loss: 2.744686462723681

Epoch: 6| Step: 1
Training loss: 2.6005696186236955
Validation loss: 2.7536087912286877

Epoch: 6| Step: 2
Training loss: 3.760799370873031
Validation loss: 2.764224735741623

Epoch: 6| Step: 3
Training loss: 3.2622173727688386
Validation loss: 2.766768307998832

Epoch: 6| Step: 4
Training loss: 2.929867507490773
Validation loss: 2.7788374621450505

Epoch: 6| Step: 5
Training loss: 2.530549882681674
Validation loss: 2.7697928695146365

Epoch: 6| Step: 6
Training loss: 2.929545895015297
Validation loss: 2.7845457855118867

Epoch: 6| Step: 7
Training loss: 3.276154257937461
Validation loss: 2.796457196770103

Epoch: 6| Step: 8
Training loss: 2.9833576805215225
Validation loss: 2.7472018962450906

Epoch: 6| Step: 9
Training loss: 2.8101149724877548
Validation loss: 2.729084702831013

Epoch: 6| Step: 10
Training loss: 3.5392065292591344
Validation loss: 2.730678235253549

Epoch: 6| Step: 11
Training loss: 3.2444604466826052
Validation loss: 2.7361253218504484

Epoch: 6| Step: 12
Training loss: 2.266966113711842
Validation loss: 2.744525412152193

Epoch: 6| Step: 13
Training loss: 3.5287250505214676
Validation loss: 2.75433449703813

Epoch: 46| Step: 0
Training loss: 2.9673786409110576
Validation loss: 2.7659443677317377

Epoch: 6| Step: 1
Training loss: 3.3588311753225777
Validation loss: 2.7592622918475556

Epoch: 6| Step: 2
Training loss: 3.240873874977356
Validation loss: 2.751343489363201

Epoch: 6| Step: 3
Training loss: 3.204332524154446
Validation loss: 2.7353822517696518

Epoch: 6| Step: 4
Training loss: 2.844086679503166
Validation loss: 2.7348245858779117

Epoch: 6| Step: 5
Training loss: 2.7371243850603486
Validation loss: 2.7314818975424617

Epoch: 6| Step: 6
Training loss: 2.683261189961929
Validation loss: 2.7329772226772238

Epoch: 6| Step: 7
Training loss: 3.252925583061806
Validation loss: 2.7345548115336453

Epoch: 6| Step: 8
Training loss: 2.8194998206936637
Validation loss: 2.7387620947459617

Epoch: 6| Step: 9
Training loss: 3.238944249218924
Validation loss: 2.7289988855939473

Epoch: 6| Step: 10
Training loss: 3.133590183146655
Validation loss: 2.7276064541384617

Epoch: 6| Step: 11
Training loss: 3.099473840914799
Validation loss: 2.728040199853937

Epoch: 6| Step: 12
Training loss: 3.4263722030741235
Validation loss: 2.7351626272912437

Epoch: 6| Step: 13
Training loss: 2.4437156069937824
Validation loss: 2.7422573399853634

Epoch: 47| Step: 0
Training loss: 3.0377633816535217
Validation loss: 2.769167674615363

Epoch: 6| Step: 1
Training loss: 3.320067632285038
Validation loss: 2.8253651148428194

Epoch: 6| Step: 2
Training loss: 3.2822912108238307
Validation loss: 2.862232768099103

Epoch: 6| Step: 3
Training loss: 3.623006601443951
Validation loss: 2.844661897739826

Epoch: 6| Step: 4
Training loss: 3.2951655724564652
Validation loss: 2.7645397686162676

Epoch: 6| Step: 5
Training loss: 2.8337181802316875
Validation loss: 2.7327575944122184

Epoch: 6| Step: 6
Training loss: 3.0484657243275515
Validation loss: 2.722456031205538

Epoch: 6| Step: 7
Training loss: 2.7759906890559085
Validation loss: 2.727368193769035

Epoch: 6| Step: 8
Training loss: 2.605631444332419
Validation loss: 2.727454427950834

Epoch: 6| Step: 9
Training loss: 2.374384348530351
Validation loss: 2.7303060767498644

Epoch: 6| Step: 10
Training loss: 3.650794238070065
Validation loss: 2.7301685902314268

Epoch: 6| Step: 11
Training loss: 3.2561926897097986
Validation loss: 2.7259290312905438

Epoch: 6| Step: 12
Training loss: 2.520578471349526
Validation loss: 2.7278400877117237

Epoch: 6| Step: 13
Training loss: 3.438279497024377
Validation loss: 2.737753600310682

Epoch: 48| Step: 0
Training loss: 3.689623334521692
Validation loss: 2.7414187733469864

Epoch: 6| Step: 1
Training loss: 2.979434572636162
Validation loss: 2.7432024023949904

Epoch: 6| Step: 2
Training loss: 2.9564666410590235
Validation loss: 2.7390209849669773

Epoch: 6| Step: 3
Training loss: 3.2116098558528146
Validation loss: 2.7242455739355544

Epoch: 6| Step: 4
Training loss: 3.136622065173987
Validation loss: 2.719615837540451

Epoch: 6| Step: 5
Training loss: 3.579835441259699
Validation loss: 2.7162572418071504

Epoch: 6| Step: 6
Training loss: 3.099258451060861
Validation loss: 2.714581533100604

Epoch: 6| Step: 7
Training loss: 2.906714945692593
Validation loss: 2.713237303628638

Epoch: 6| Step: 8
Training loss: 2.8000420056325734
Validation loss: 2.709904058262849

Epoch: 6| Step: 9
Training loss: 2.804083969083338
Validation loss: 2.714276781043551

Epoch: 6| Step: 10
Training loss: 2.6662619005412487
Validation loss: 2.7075978993109033

Epoch: 6| Step: 11
Training loss: 2.907185065678399
Validation loss: 2.7089390322719713

Epoch: 6| Step: 12
Training loss: 2.9316323297371816
Validation loss: 2.7116195588105114

Epoch: 6| Step: 13
Training loss: 2.867876598900908
Validation loss: 2.7128741122838638

Epoch: 49| Step: 0
Training loss: 2.6921883430856015
Validation loss: 2.713265374452208

Epoch: 6| Step: 1
Training loss: 3.5238285904936784
Validation loss: 2.719281020384056

Epoch: 6| Step: 2
Training loss: 3.5835535440129282
Validation loss: 2.7175972889607816

Epoch: 6| Step: 3
Training loss: 3.15498538223967
Validation loss: 2.7142942443624647

Epoch: 6| Step: 4
Training loss: 3.298041982352317
Validation loss: 2.71051108470834

Epoch: 6| Step: 5
Training loss: 2.575962153553605
Validation loss: 2.7111299626186818

Epoch: 6| Step: 6
Training loss: 2.8045140799884596
Validation loss: 2.71461394364222

Epoch: 6| Step: 7
Training loss: 3.2461248917353966
Validation loss: 2.7147716674064513

Epoch: 6| Step: 8
Training loss: 2.7055466229001714
Validation loss: 2.725224656200246

Epoch: 6| Step: 9
Training loss: 3.1108537541013943
Validation loss: 2.7182365689865353

Epoch: 6| Step: 10
Training loss: 2.9924145804111317
Validation loss: 2.7109297928135083

Epoch: 6| Step: 11
Training loss: 2.9021492195911356
Validation loss: 2.705680566196659

Epoch: 6| Step: 12
Training loss: 2.8741151857913225
Validation loss: 2.7065628745827452

Epoch: 6| Step: 13
Training loss: 2.7378923735883096
Validation loss: 2.7130331363173457

Epoch: 50| Step: 0
Training loss: 2.992730074063415
Validation loss: 2.7075942710478222

Epoch: 6| Step: 1
Training loss: 2.9346525310861673
Validation loss: 2.708716731861945

Epoch: 6| Step: 2
Training loss: 2.8086120010653084
Validation loss: 2.713707169392944

Epoch: 6| Step: 3
Training loss: 2.4276593443001375
Validation loss: 2.711786387657759

Epoch: 6| Step: 4
Training loss: 3.23700758087053
Validation loss: 2.713147092169891

Epoch: 6| Step: 5
Training loss: 3.5693718737503546
Validation loss: 2.7149277200827657

Epoch: 6| Step: 6
Training loss: 3.2507672504617746
Validation loss: 2.712108308522923

Epoch: 6| Step: 7
Training loss: 3.0220887480352023
Validation loss: 2.709805358482734

Epoch: 6| Step: 8
Training loss: 2.9042124835376857
Validation loss: 2.708841815351727

Epoch: 6| Step: 9
Training loss: 3.229891262999543
Validation loss: 2.706380859415338

Epoch: 6| Step: 10
Training loss: 2.7672827972941607
Validation loss: 2.706655831571652

Epoch: 6| Step: 11
Training loss: 3.056286015522631
Validation loss: 2.7048533595809605

Epoch: 6| Step: 12
Training loss: 2.914278324101799
Validation loss: 2.702214814442134

Epoch: 6| Step: 13
Training loss: 3.584388814033156
Validation loss: 2.7040530888928265

Epoch: 51| Step: 0
Training loss: 3.1621413438393247
Validation loss: 2.697287839988863

Epoch: 6| Step: 1
Training loss: 3.033694196202745
Validation loss: 2.699911240136991

Epoch: 6| Step: 2
Training loss: 3.1557137817538323
Validation loss: 2.702828958159824

Epoch: 6| Step: 3
Training loss: 3.7492395265690734
Validation loss: 2.7008648049500397

Epoch: 6| Step: 4
Training loss: 3.4427270162578503
Validation loss: 2.699932453453038

Epoch: 6| Step: 5
Training loss: 3.443397872285506
Validation loss: 2.7067441946293576

Epoch: 6| Step: 6
Training loss: 3.1072366540591356
Validation loss: 2.706674345677336

Epoch: 6| Step: 7
Training loss: 2.530954979552285
Validation loss: 2.6997902039707258

Epoch: 6| Step: 8
Training loss: 2.1422614086932223
Validation loss: 2.6976756713077235

Epoch: 6| Step: 9
Training loss: 3.1199553398457223
Validation loss: 2.6984046370894093

Epoch: 6| Step: 10
Training loss: 2.8605769398448233
Validation loss: 2.696041916468401

Epoch: 6| Step: 11
Training loss: 3.1298863165444506
Validation loss: 2.6964556190840936

Epoch: 6| Step: 12
Training loss: 2.220043825971325
Validation loss: 2.6968853252101397

Epoch: 6| Step: 13
Training loss: 3.0251518337512473
Validation loss: 2.6964179826454555

Epoch: 52| Step: 0
Training loss: 2.299335226102238
Validation loss: 2.69753812021289

Epoch: 6| Step: 1
Training loss: 3.5023207463554167
Validation loss: 2.697482226880389

Epoch: 6| Step: 2
Training loss: 3.3549056729787443
Validation loss: 2.697368884700987

Epoch: 6| Step: 3
Training loss: 2.7154026691130038
Validation loss: 2.700168495777157

Epoch: 6| Step: 4
Training loss: 3.833443073415743
Validation loss: 2.7003860040471883

Epoch: 6| Step: 5
Training loss: 2.6497299956477085
Validation loss: 2.69510583904164

Epoch: 6| Step: 6
Training loss: 3.140367336046861
Validation loss: 2.700710276735234

Epoch: 6| Step: 7
Training loss: 2.7943332993397676
Validation loss: 2.6954510044906206

Epoch: 6| Step: 8
Training loss: 2.9941196030328556
Validation loss: 2.6953474392379286

Epoch: 6| Step: 9
Training loss: 2.3517612107176884
Validation loss: 2.697289827381723

Epoch: 6| Step: 10
Training loss: 3.5326241039335593
Validation loss: 2.7039058419839326

Epoch: 6| Step: 11
Training loss: 3.3089327047450285
Validation loss: 2.7063209011154563

Epoch: 6| Step: 12
Training loss: 2.675344968994653
Validation loss: 2.7173848971237398

Epoch: 6| Step: 13
Training loss: 2.476735202291126
Validation loss: 2.7136074742580987

Epoch: 53| Step: 0
Training loss: 2.8066799556601816
Validation loss: 2.7259857275692596

Epoch: 6| Step: 1
Training loss: 1.989987463791816
Validation loss: 2.715218671759254

Epoch: 6| Step: 2
Training loss: 3.6574981384898395
Validation loss: 2.7236548905535285

Epoch: 6| Step: 3
Training loss: 3.0058300272182485
Validation loss: 2.717890710082336

Epoch: 6| Step: 4
Training loss: 2.9600527606592135
Validation loss: 2.6973389614265226

Epoch: 6| Step: 5
Training loss: 2.910522274066446
Validation loss: 2.6871680063945234

Epoch: 6| Step: 6
Training loss: 2.840060345183219
Validation loss: 2.690930766830425

Epoch: 6| Step: 7
Training loss: 2.746504903430961
Validation loss: 2.6859697333649537

Epoch: 6| Step: 8
Training loss: 2.942007786007244
Validation loss: 2.687602380556234

Epoch: 6| Step: 9
Training loss: 3.46388467031992
Validation loss: 2.683865679937041

Epoch: 6| Step: 10
Training loss: 3.8598975935876068
Validation loss: 2.684984296982436

Epoch: 6| Step: 11
Training loss: 2.9375696376906095
Validation loss: 2.6853141337472897

Epoch: 6| Step: 12
Training loss: 2.860330390786702
Validation loss: 2.685250191716323

Epoch: 6| Step: 13
Training loss: 2.89193943140702
Validation loss: 2.6847293992704313

Epoch: 54| Step: 0
Training loss: 3.3114734714510434
Validation loss: 2.6847402564298513

Epoch: 6| Step: 1
Training loss: 2.793490103766052
Validation loss: 2.6837122343616473

Epoch: 6| Step: 2
Training loss: 2.9145265266021476
Validation loss: 2.6844654546144247

Epoch: 6| Step: 3
Training loss: 2.1929504067605428
Validation loss: 2.683399728931784

Epoch: 6| Step: 4
Training loss: 2.9618594865459085
Validation loss: 2.693413936632223

Epoch: 6| Step: 5
Training loss: 3.288935353048272
Validation loss: 2.6961950183281505

Epoch: 6| Step: 6
Training loss: 3.630237872944438
Validation loss: 2.7043194142277596

Epoch: 6| Step: 7
Training loss: 3.0363261399630073
Validation loss: 2.7023225530133215

Epoch: 6| Step: 8
Training loss: 3.063182832537385
Validation loss: 2.7094502767067286

Epoch: 6| Step: 9
Training loss: 2.407351749407123
Validation loss: 2.696163228348603

Epoch: 6| Step: 10
Training loss: 2.793976373909828
Validation loss: 2.7016990780115218

Epoch: 6| Step: 11
Training loss: 2.941876012973356
Validation loss: 2.6911925608420404

Epoch: 6| Step: 12
Training loss: 3.534562978965081
Validation loss: 2.686893974785082

Epoch: 6| Step: 13
Training loss: 3.037897901807136
Validation loss: 2.676034231422308

Epoch: 55| Step: 0
Training loss: 2.79203684805462
Validation loss: 2.6802401241156035

Epoch: 6| Step: 1
Training loss: 3.114004704407536
Validation loss: 2.677921561509947

Epoch: 6| Step: 2
Training loss: 2.929005780059019
Validation loss: 2.677889479450173

Epoch: 6| Step: 3
Training loss: 2.966843484759775
Validation loss: 2.6801305236218163

Epoch: 6| Step: 4
Training loss: 3.4762014823087384
Validation loss: 2.6731633417646345

Epoch: 6| Step: 5
Training loss: 2.706707647448709
Validation loss: 2.6763461859202526

Epoch: 6| Step: 6
Training loss: 3.6967555125927998
Validation loss: 2.674831885980852

Epoch: 6| Step: 7
Training loss: 2.8397292333126187
Validation loss: 2.672801727004533

Epoch: 6| Step: 8
Training loss: 2.5490773981614656
Validation loss: 2.6758397695287353

Epoch: 6| Step: 9
Training loss: 3.0352611940914445
Validation loss: 2.6726764512475976

Epoch: 6| Step: 10
Training loss: 2.8976057560918838
Validation loss: 2.673147193588903

Epoch: 6| Step: 11
Training loss: 2.696440087675478
Validation loss: 2.6740787218815436

Epoch: 6| Step: 12
Training loss: 3.145896776245914
Validation loss: 2.672054428430583

Epoch: 6| Step: 13
Training loss: 3.1800314977273554
Validation loss: 2.6855353481011788

Epoch: 56| Step: 0
Training loss: 3.365500184474697
Validation loss: 2.6927245406504077

Epoch: 6| Step: 1
Training loss: 2.986094194310099
Validation loss: 2.7047546394688644

Epoch: 6| Step: 2
Training loss: 3.3638378218885308
Validation loss: 2.7358811259976297

Epoch: 6| Step: 3
Training loss: 2.656841616101132
Validation loss: 2.7239634887656567

Epoch: 6| Step: 4
Training loss: 3.2219780595578196
Validation loss: 2.6957407821082873

Epoch: 6| Step: 5
Training loss: 3.2932334422459264
Validation loss: 2.676439686635324

Epoch: 6| Step: 6
Training loss: 2.4359233354177454
Validation loss: 2.6676392066822783

Epoch: 6| Step: 7
Training loss: 2.962277071649232
Validation loss: 2.66996561623962

Epoch: 6| Step: 8
Training loss: 3.163947661399782
Validation loss: 2.667669391023913

Epoch: 6| Step: 9
Training loss: 3.0699288708526664
Validation loss: 2.6697517492828595

Epoch: 6| Step: 10
Training loss: 2.612411559583736
Validation loss: 2.67272593513943

Epoch: 6| Step: 11
Training loss: 2.627087081273244
Validation loss: 2.6728818740506846

Epoch: 6| Step: 12
Training loss: 3.3537871973432476
Validation loss: 2.6720034766250422

Epoch: 6| Step: 13
Training loss: 2.9743501546502995
Validation loss: 2.670719914349963

Epoch: 57| Step: 0
Training loss: 3.2093386045359025
Validation loss: 2.6705976329378944

Epoch: 6| Step: 1
Training loss: 2.812483723911238
Validation loss: 2.6659544011479603

Epoch: 6| Step: 2
Training loss: 3.0167106295028963
Validation loss: 2.6676210117278933

Epoch: 6| Step: 3
Training loss: 3.11781690930822
Validation loss: 2.668658616844182

Epoch: 6| Step: 4
Training loss: 2.512723588246948
Validation loss: 2.6761972004518886

Epoch: 6| Step: 5
Training loss: 3.3209282517279712
Validation loss: 2.6734645960153096

Epoch: 6| Step: 6
Training loss: 3.2309894626437337
Validation loss: 2.6993397984935865

Epoch: 6| Step: 7
Training loss: 2.49549087144867
Validation loss: 2.7026366594643623

Epoch: 6| Step: 8
Training loss: 3.762648296285222
Validation loss: 2.7257808852302845

Epoch: 6| Step: 9
Training loss: 2.5995534293261575
Validation loss: 2.696593189270005

Epoch: 6| Step: 10
Training loss: 2.5215602068501197
Validation loss: 2.675312008936229

Epoch: 6| Step: 11
Training loss: 2.8068704000030587
Validation loss: 2.665712798036829

Epoch: 6| Step: 12
Training loss: 3.4038553482322738
Validation loss: 2.6627541738621936

Epoch: 6| Step: 13
Training loss: 2.9876520030405103
Validation loss: 2.6602683839432806

Epoch: 58| Step: 0
Training loss: 2.6353404419823456
Validation loss: 2.6598127644879

Epoch: 6| Step: 1
Training loss: 2.514248868436958
Validation loss: 2.660848857251555

Epoch: 6| Step: 2
Training loss: 2.830928698365088
Validation loss: 2.660200227946683

Epoch: 6| Step: 3
Training loss: 2.825071425083573
Validation loss: 2.6568726284613127

Epoch: 6| Step: 4
Training loss: 3.282646826476758
Validation loss: 2.655958669515722

Epoch: 6| Step: 5
Training loss: 3.0161078664376886
Validation loss: 2.6554259680283367

Epoch: 6| Step: 6
Training loss: 3.033880292045954
Validation loss: 2.6567588526256913

Epoch: 6| Step: 7
Training loss: 2.9125648720519743
Validation loss: 2.656735776700548

Epoch: 6| Step: 8
Training loss: 2.9377496491530097
Validation loss: 2.6608476201593647

Epoch: 6| Step: 9
Training loss: 2.9650274126866685
Validation loss: 2.665019922465014

Epoch: 6| Step: 10
Training loss: 3.9454678042220586
Validation loss: 2.669182149202711

Epoch: 6| Step: 11
Training loss: 2.879258526350154
Validation loss: 2.6745476828019146

Epoch: 6| Step: 12
Training loss: 2.958003137634553
Validation loss: 2.695693727331017

Epoch: 6| Step: 13
Training loss: 3.010872055158063
Validation loss: 2.708080439596576

Epoch: 59| Step: 0
Training loss: 1.8333627380556123
Validation loss: 2.7067814024047934

Epoch: 6| Step: 1
Training loss: 3.3467979443652878
Validation loss: 2.7031334046975233

Epoch: 6| Step: 2
Training loss: 2.756200216453571
Validation loss: 2.695370852287719

Epoch: 6| Step: 3
Training loss: 2.924725452808212
Validation loss: 2.6921768055805915

Epoch: 6| Step: 4
Training loss: 3.372524908043096
Validation loss: 2.6533885978742213

Epoch: 6| Step: 5
Training loss: 3.3939188765910933
Validation loss: 2.6511174763346705

Epoch: 6| Step: 6
Training loss: 3.2591204681932653
Validation loss: 2.64940719041471

Epoch: 6| Step: 7
Training loss: 2.7314073425643426
Validation loss: 2.649041783069786

Epoch: 6| Step: 8
Training loss: 2.9125503011788436
Validation loss: 2.651903038919867

Epoch: 6| Step: 9
Training loss: 2.414855826676875
Validation loss: 2.6537578379233726

Epoch: 6| Step: 10
Training loss: 3.7183690236588887
Validation loss: 2.655962407892116

Epoch: 6| Step: 11
Training loss: 2.6709966275369883
Validation loss: 2.653164635566605

Epoch: 6| Step: 12
Training loss: 2.678975625195314
Validation loss: 2.653021008107632

Epoch: 6| Step: 13
Training loss: 3.9119932145057916
Validation loss: 2.652590111929372

Epoch: 60| Step: 0
Training loss: 3.5497502843578768
Validation loss: 2.650823550844561

Epoch: 6| Step: 1
Training loss: 2.8893489186081203
Validation loss: 2.651404883290987

Epoch: 6| Step: 2
Training loss: 2.873746515763914
Validation loss: 2.6542811548524394

Epoch: 6| Step: 3
Training loss: 3.1389377293870813
Validation loss: 2.643344952791147

Epoch: 6| Step: 4
Training loss: 2.79498619362794
Validation loss: 2.6477478360660727

Epoch: 6| Step: 5
Training loss: 2.5520867691535916
Validation loss: 2.6411090447719183

Epoch: 6| Step: 6
Training loss: 3.274000023918175
Validation loss: 2.6439943476704593

Epoch: 6| Step: 7
Training loss: 3.2892653160356193
Validation loss: 2.644924797189172

Epoch: 6| Step: 8
Training loss: 3.0215096253793012
Validation loss: 2.6414314956332823

Epoch: 6| Step: 9
Training loss: 2.4199142886975014
Validation loss: 2.6454528786658598

Epoch: 6| Step: 10
Training loss: 2.4864613151985213
Validation loss: 2.64837321067493

Epoch: 6| Step: 11
Training loss: 3.366778780389217
Validation loss: 2.6529772145786015

Epoch: 6| Step: 12
Training loss: 2.5397565157273814
Validation loss: 2.6571967728024513

Epoch: 6| Step: 13
Training loss: 3.6133129221584865
Validation loss: 2.6605277508656004

Epoch: 61| Step: 0
Training loss: 3.251740209809179
Validation loss: 2.67158835388919

Epoch: 6| Step: 1
Training loss: 3.000033060527469
Validation loss: 2.689601605894526

Epoch: 6| Step: 2
Training loss: 3.1078887914745823
Validation loss: 2.7105614885850264

Epoch: 6| Step: 3
Training loss: 2.766133493347933
Validation loss: 2.7350816283001764

Epoch: 6| Step: 4
Training loss: 2.7918125987460787
Validation loss: 2.711597051861831

Epoch: 6| Step: 5
Training loss: 2.9409506800879814
Validation loss: 2.6871330037360943

Epoch: 6| Step: 6
Training loss: 3.505741587070756
Validation loss: 2.650993426670332

Epoch: 6| Step: 7
Training loss: 3.6546451274214373
Validation loss: 2.647155966629354

Epoch: 6| Step: 8
Training loss: 2.468061713351318
Validation loss: 2.6326327691766918

Epoch: 6| Step: 9
Training loss: 2.123376225927067
Validation loss: 2.637172052270406

Epoch: 6| Step: 10
Training loss: 2.7606155803676033
Validation loss: 2.6347283255549137

Epoch: 6| Step: 11
Training loss: 2.764097578603503
Validation loss: 2.635504488681673

Epoch: 6| Step: 12
Training loss: 3.4241049829235712
Validation loss: 2.6323682063813423

Epoch: 6| Step: 13
Training loss: 2.6827953766869004
Validation loss: 2.631433785584728

Epoch: 62| Step: 0
Training loss: 2.5021092100390825
Validation loss: 2.6365667056078994

Epoch: 6| Step: 1
Training loss: 3.7994746346955424
Validation loss: 2.6291258987342836

Epoch: 6| Step: 2
Training loss: 3.363223120777424
Validation loss: 2.629972748067982

Epoch: 6| Step: 3
Training loss: 2.8583847582522015
Validation loss: 2.631869818995794

Epoch: 6| Step: 4
Training loss: 3.1936018638819452
Validation loss: 2.6398372544297994

Epoch: 6| Step: 5
Training loss: 2.7460857624572665
Validation loss: 2.63609184368947

Epoch: 6| Step: 6
Training loss: 3.204490407818958
Validation loss: 2.6615467360675344

Epoch: 6| Step: 7
Training loss: 3.3211273988514587
Validation loss: 2.662537520303759

Epoch: 6| Step: 8
Training loss: 2.782128270382246
Validation loss: 2.680484881383172

Epoch: 6| Step: 9
Training loss: 2.588509102097782
Validation loss: 2.664537945941883

Epoch: 6| Step: 10
Training loss: 2.8643515383254585
Validation loss: 2.654445233411969

Epoch: 6| Step: 11
Training loss: 2.301286263084331
Validation loss: 2.639849078953609

Epoch: 6| Step: 12
Training loss: 3.0876389074545534
Validation loss: 2.642853097528106

Epoch: 6| Step: 13
Training loss: 2.608335824869772
Validation loss: 2.6515541180124735

Epoch: 63| Step: 0
Training loss: 2.9366861392821426
Validation loss: 2.6408062449325223

Epoch: 6| Step: 1
Training loss: 2.906064099344595
Validation loss: 2.6503710580435826

Epoch: 6| Step: 2
Training loss: 3.3291349356185105
Validation loss: 2.651548969076849

Epoch: 6| Step: 3
Training loss: 2.7699341043835237
Validation loss: 2.649215625072786

Epoch: 6| Step: 4
Training loss: 3.3453205733934577
Validation loss: 2.6435020022677516

Epoch: 6| Step: 5
Training loss: 2.423948618485923
Validation loss: 2.6414397889677406

Epoch: 6| Step: 6
Training loss: 3.199903403254719
Validation loss: 2.631428582178795

Epoch: 6| Step: 7
Training loss: 2.827185121604616
Validation loss: 2.6307629367988303

Epoch: 6| Step: 8
Training loss: 3.3502499159781673
Validation loss: 2.6291903671518098

Epoch: 6| Step: 9
Training loss: 3.0963361315553573
Validation loss: 2.6331104494473574

Epoch: 6| Step: 10
Training loss: 2.990328776677734
Validation loss: 2.6314105479768632

Epoch: 6| Step: 11
Training loss: 3.166440319872839
Validation loss: 2.6282072909395766

Epoch: 6| Step: 12
Training loss: 2.6343239088070494
Validation loss: 2.6315115938353837

Epoch: 6| Step: 13
Training loss: 1.948517269896889
Validation loss: 2.6289296111610123

Epoch: 64| Step: 0
Training loss: 2.9913073806583395
Validation loss: 2.631577232254497

Epoch: 6| Step: 1
Training loss: 3.133267566991735
Validation loss: 2.6325358476747467

Epoch: 6| Step: 2
Training loss: 2.5215477259542673
Validation loss: 2.639550866466707

Epoch: 6| Step: 3
Training loss: 2.889259139404735
Validation loss: 2.642299060532165

Epoch: 6| Step: 4
Training loss: 3.0363514239606055
Validation loss: 2.6474030117739966

Epoch: 6| Step: 5
Training loss: 3.5023456615176443
Validation loss: 2.6693078569095094

Epoch: 6| Step: 6
Training loss: 3.232904809562204
Validation loss: 2.6557550773900083

Epoch: 6| Step: 7
Training loss: 2.2924697046971736
Validation loss: 2.635727249580564

Epoch: 6| Step: 8
Training loss: 2.906899820507629
Validation loss: 2.6284636437406084

Epoch: 6| Step: 9
Training loss: 2.961711369744003
Validation loss: 2.6209323813148964

Epoch: 6| Step: 10
Training loss: 3.3572223242308463
Validation loss: 2.6250929144584814

Epoch: 6| Step: 11
Training loss: 2.8005112181324576
Validation loss: 2.624660579381292

Epoch: 6| Step: 12
Training loss: 3.3738293913538207
Validation loss: 2.6266976163896043

Epoch: 6| Step: 13
Training loss: 2.108022298474155
Validation loss: 2.628814790754149

Epoch: 65| Step: 0
Training loss: 3.165884138762175
Validation loss: 2.6246940690394904

Epoch: 6| Step: 1
Training loss: 2.9623018608952525
Validation loss: 2.623053364610584

Epoch: 6| Step: 2
Training loss: 2.457873371435104
Validation loss: 2.623141070513777

Epoch: 6| Step: 3
Training loss: 3.2627895767492507
Validation loss: 2.6219805976816426

Epoch: 6| Step: 4
Training loss: 2.6689236943988184
Validation loss: 2.619401402198241

Epoch: 6| Step: 5
Training loss: 2.8821971205487
Validation loss: 2.6250434701956182

Epoch: 6| Step: 6
Training loss: 2.9201475534651995
Validation loss: 2.6354992349486848

Epoch: 6| Step: 7
Training loss: 2.5956092513240665
Validation loss: 2.6519085685372517

Epoch: 6| Step: 8
Training loss: 3.402719610499911
Validation loss: 2.6835247964205964

Epoch: 6| Step: 9
Training loss: 3.1604157707868565
Validation loss: 2.6814901019015744

Epoch: 6| Step: 10
Training loss: 2.600253122186084
Validation loss: 2.65845911574055

Epoch: 6| Step: 11
Training loss: 2.8914730992973046
Validation loss: 2.6359181145723904

Epoch: 6| Step: 12
Training loss: 3.4630984068109223
Validation loss: 2.6224306895715737

Epoch: 6| Step: 13
Training loss: 3.0324366498275417
Validation loss: 2.615233849574236

Epoch: 66| Step: 0
Training loss: 2.798277689653381
Validation loss: 2.6192004433019225

Epoch: 6| Step: 1
Training loss: 2.9322620523604943
Validation loss: 2.6242860427674586

Epoch: 6| Step: 2
Training loss: 3.0513126237780592
Validation loss: 2.6275178579725407

Epoch: 6| Step: 3
Training loss: 2.7854901684745204
Validation loss: 2.6302519536470124

Epoch: 6| Step: 4
Training loss: 2.932379622464346
Validation loss: 2.6253243659544556

Epoch: 6| Step: 5
Training loss: 2.665105352699046
Validation loss: 2.6208424905992977

Epoch: 6| Step: 6
Training loss: 3.068291617846719
Validation loss: 2.6193857730932826

Epoch: 6| Step: 7
Training loss: 3.3952883105550833
Validation loss: 2.6253822544464778

Epoch: 6| Step: 8
Training loss: 2.9627924521268585
Validation loss: 2.637761177055581

Epoch: 6| Step: 9
Training loss: 3.0784032085965065
Validation loss: 2.6893231817636285

Epoch: 6| Step: 10
Training loss: 3.227532414185664
Validation loss: 2.7110116590363096

Epoch: 6| Step: 11
Training loss: 2.9381680642002643
Validation loss: 2.72875906887615

Epoch: 6| Step: 12
Training loss: 3.3619004803507666
Validation loss: 2.714484968754741

Epoch: 6| Step: 13
Training loss: 2.485463893427834
Validation loss: 2.6660065897910528

Epoch: 67| Step: 0
Training loss: 2.3596559066789387
Validation loss: 2.628053525980656

Epoch: 6| Step: 1
Training loss: 3.2646642120281473
Validation loss: 2.6216678096194883

Epoch: 6| Step: 2
Training loss: 2.4464856340994983
Validation loss: 2.6182482858568457

Epoch: 6| Step: 3
Training loss: 2.7235717110056483
Validation loss: 2.6187350785557717

Epoch: 6| Step: 4
Training loss: 2.4924245500384314
Validation loss: 2.619461882990248

Epoch: 6| Step: 5
Training loss: 2.8475055419224904
Validation loss: 2.6178206237401254

Epoch: 6| Step: 6
Training loss: 2.519356749649965
Validation loss: 2.618987654087533

Epoch: 6| Step: 7
Training loss: 3.1230651969443795
Validation loss: 2.619703259837003

Epoch: 6| Step: 8
Training loss: 2.6512018141623477
Validation loss: 2.6159850525701964

Epoch: 6| Step: 9
Training loss: 3.359634460365501
Validation loss: 2.6153320337397976

Epoch: 6| Step: 10
Training loss: 3.1879759788421875
Validation loss: 2.6209168796952316

Epoch: 6| Step: 11
Training loss: 3.7827407798601778
Validation loss: 2.61914404104414

Epoch: 6| Step: 12
Training loss: 2.901560619971961
Validation loss: 2.616855663603712

Epoch: 6| Step: 13
Training loss: 3.8932134757687917
Validation loss: 2.6161458689766652

Epoch: 68| Step: 0
Training loss: 2.493244867625357
Validation loss: 2.616059817920971

Epoch: 6| Step: 1
Training loss: 2.9318179102999
Validation loss: 2.618000604704135

Epoch: 6| Step: 2
Training loss: 3.787230750859248
Validation loss: 2.6284715527450517

Epoch: 6| Step: 3
Training loss: 3.030341730015179
Validation loss: 2.6105233447781218

Epoch: 6| Step: 4
Training loss: 3.3503654849966207
Validation loss: 2.6134869987628835

Epoch: 6| Step: 5
Training loss: 3.085096336356784
Validation loss: 2.602931009479671

Epoch: 6| Step: 6
Training loss: 2.7269302601081775
Validation loss: 2.604852715149849

Epoch: 6| Step: 7
Training loss: 2.8991038746588593
Validation loss: 2.6052834742464173

Epoch: 6| Step: 8
Training loss: 2.6976178468941963
Validation loss: 2.6068079381537737

Epoch: 6| Step: 9
Training loss: 3.247223475006126
Validation loss: 2.615124822424964

Epoch: 6| Step: 10
Training loss: 2.3281789683800884
Validation loss: 2.603187846330802

Epoch: 6| Step: 11
Training loss: 2.590164924142676
Validation loss: 2.6101064066510498

Epoch: 6| Step: 12
Training loss: 3.102756837208602
Validation loss: 2.6029080797975928

Epoch: 6| Step: 13
Training loss: 2.685538441037893
Validation loss: 2.6126059030109934

Epoch: 69| Step: 0
Training loss: 2.7110484867297933
Validation loss: 2.604666186918596

Epoch: 6| Step: 1
Training loss: 3.781625902391352
Validation loss: 2.6025041815160592

Epoch: 6| Step: 2
Training loss: 2.6809458099887418
Validation loss: 2.600361108708437

Epoch: 6| Step: 3
Training loss: 2.8271369683489294
Validation loss: 2.59839117235681

Epoch: 6| Step: 4
Training loss: 2.908437582368277
Validation loss: 2.5980958820551905

Epoch: 6| Step: 5
Training loss: 2.5670151358359936
Validation loss: 2.6049581594009985

Epoch: 6| Step: 6
Training loss: 3.248332696336893
Validation loss: 2.606385560473218

Epoch: 6| Step: 7
Training loss: 2.8125932466096164
Validation loss: 2.6007311309670516

Epoch: 6| Step: 8
Training loss: 3.348984373633127
Validation loss: 2.5984219815484257

Epoch: 6| Step: 9
Training loss: 2.9659238513989807
Validation loss: 2.5975297868545337

Epoch: 6| Step: 10
Training loss: 2.098768681618551
Validation loss: 2.6078804013908976

Epoch: 6| Step: 11
Training loss: 3.3714627697159236
Validation loss: 2.629750834301234

Epoch: 6| Step: 12
Training loss: 2.9199607412756117
Validation loss: 2.652403856580755

Epoch: 6| Step: 13
Training loss: 2.6781079499849256
Validation loss: 2.656651847115997

Epoch: 70| Step: 0
Training loss: 2.9276319286091312
Validation loss: 2.6931022923441437

Epoch: 6| Step: 1
Training loss: 3.4028737545534526
Validation loss: 2.72986465417295

Epoch: 6| Step: 2
Training loss: 2.17284761235596
Validation loss: 2.6766405766909624

Epoch: 6| Step: 3
Training loss: 2.372395744002697
Validation loss: 2.6705673962564607

Epoch: 6| Step: 4
Training loss: 3.1043256851190004
Validation loss: 2.657911454954066

Epoch: 6| Step: 5
Training loss: 2.8395980875625972
Validation loss: 2.6175982654183434

Epoch: 6| Step: 6
Training loss: 3.1902029507451966
Validation loss: 2.603742104325781

Epoch: 6| Step: 7
Training loss: 3.0095205238150746
Validation loss: 2.597790915654526

Epoch: 6| Step: 8
Training loss: 2.6467401247215943
Validation loss: 2.5997188842100742

Epoch: 6| Step: 9
Training loss: 3.28706005329159
Validation loss: 2.598607190102515

Epoch: 6| Step: 10
Training loss: 3.188686673542317
Validation loss: 2.6021195741269927

Epoch: 6| Step: 11
Training loss: 3.1360719995018562
Validation loss: 2.6012684763928844

Epoch: 6| Step: 12
Training loss: 2.7415344677932
Validation loss: 2.602841632710666

Epoch: 6| Step: 13
Training loss: 3.2426100122381794
Validation loss: 2.600669005045882

Epoch: 71| Step: 0
Training loss: 2.861092640427243
Validation loss: 2.6124658315349163

Epoch: 6| Step: 1
Training loss: 2.7480290025293903
Validation loss: 2.618154429359231

Epoch: 6| Step: 2
Training loss: 3.2454285381878885
Validation loss: 2.635151692959672

Epoch: 6| Step: 3
Training loss: 3.2905432478122294
Validation loss: 2.6202389932471615

Epoch: 6| Step: 4
Training loss: 2.716794593310958
Validation loss: 2.6147888191745947

Epoch: 6| Step: 5
Training loss: 3.4742381351617238
Validation loss: 2.604266060849959

Epoch: 6| Step: 6
Training loss: 3.1266821339379884
Validation loss: 2.6248708733257784

Epoch: 6| Step: 7
Training loss: 3.0438433309733175
Validation loss: 2.6079593615889722

Epoch: 6| Step: 8
Training loss: 2.5207626290822653
Validation loss: 2.6019259500142624

Epoch: 6| Step: 9
Training loss: 2.425989998943851
Validation loss: 2.604962164351139

Epoch: 6| Step: 10
Training loss: 2.702722067376949
Validation loss: 2.597856034380352

Epoch: 6| Step: 11
Training loss: 2.9387225589744754
Validation loss: 2.599748530820357

Epoch: 6| Step: 12
Training loss: 3.194090220028107
Validation loss: 2.6002255281206406

Epoch: 6| Step: 13
Training loss: 2.4684081987785405
Validation loss: 2.591976342640878

Epoch: 72| Step: 0
Training loss: 2.2700675816495743
Validation loss: 2.5972947293391146

Epoch: 6| Step: 1
Training loss: 2.0685449504361992
Validation loss: 2.6000822182443084

Epoch: 6| Step: 2
Training loss: 3.287683773343455
Validation loss: 2.6025620190743584

Epoch: 6| Step: 3
Training loss: 2.7901434847124267
Validation loss: 2.611317543803985

Epoch: 6| Step: 4
Training loss: 3.3073932942473037
Validation loss: 2.620277774761279

Epoch: 6| Step: 5
Training loss: 2.9214757962795224
Validation loss: 2.6254424500155595

Epoch: 6| Step: 6
Training loss: 2.6063816467261876
Validation loss: 2.6404454456069986

Epoch: 6| Step: 7
Training loss: 3.600256296147431
Validation loss: 2.652725602301012

Epoch: 6| Step: 8
Training loss: 2.7232827296355047
Validation loss: 2.6324889495838626

Epoch: 6| Step: 9
Training loss: 3.0514370143889917
Validation loss: 2.608579977619949

Epoch: 6| Step: 10
Training loss: 3.0819105697618427
Validation loss: 2.6006117829047684

Epoch: 6| Step: 11
Training loss: 2.908545295256394
Validation loss: 2.597372759809594

Epoch: 6| Step: 12
Training loss: 3.283675541608981
Validation loss: 2.592735865190388

Epoch: 6| Step: 13
Training loss: 2.976597102407142
Validation loss: 2.5930849238953955

Epoch: 73| Step: 0
Training loss: 2.7205814901247085
Validation loss: 2.5914552779717375

Epoch: 6| Step: 1
Training loss: 2.8756472646791944
Validation loss: 2.5906895069611613

Epoch: 6| Step: 2
Training loss: 3.0310588658430184
Validation loss: 2.5932196163679992

Epoch: 6| Step: 3
Training loss: 3.0398785983489858
Validation loss: 2.591671849099773

Epoch: 6| Step: 4
Training loss: 2.8191288289609195
Validation loss: 2.601914847790423

Epoch: 6| Step: 5
Training loss: 2.879225403940577
Validation loss: 2.6069473007393698

Epoch: 6| Step: 6
Training loss: 3.6607569117174403
Validation loss: 2.60204571859699

Epoch: 6| Step: 7
Training loss: 2.729233272603612
Validation loss: 2.600499795468312

Epoch: 6| Step: 8
Training loss: 2.658928093301261
Validation loss: 2.5980487994120303

Epoch: 6| Step: 9
Training loss: 2.698051144894913
Validation loss: 2.606977692167688

Epoch: 6| Step: 10
Training loss: 3.1517319776955204
Validation loss: 2.6001256416228617

Epoch: 6| Step: 11
Training loss: 2.748105523650104
Validation loss: 2.595260686236726

Epoch: 6| Step: 12
Training loss: 3.2138591755825385
Validation loss: 2.5949822888850806

Epoch: 6| Step: 13
Training loss: 2.660915092871196
Validation loss: 2.590028805854638

Epoch: 74| Step: 0
Training loss: 3.2581040505782703
Validation loss: 2.581939596847633

Epoch: 6| Step: 1
Training loss: 2.9957962783693586
Validation loss: 2.5849394322563533

Epoch: 6| Step: 2
Training loss: 2.0392072487386135
Validation loss: 2.586785165422328

Epoch: 6| Step: 3
Training loss: 3.250009096573057
Validation loss: 2.5892572470897988

Epoch: 6| Step: 4
Training loss: 2.925618593713779
Validation loss: 2.588059578269911

Epoch: 6| Step: 5
Training loss: 3.2998503506569743
Validation loss: 2.5860260166124345

Epoch: 6| Step: 6
Training loss: 3.6616181962271006
Validation loss: 2.586802816012163

Epoch: 6| Step: 7
Training loss: 2.726512219858969
Validation loss: 2.582311191518199

Epoch: 6| Step: 8
Training loss: 2.8957052591301324
Validation loss: 2.5763898457785195

Epoch: 6| Step: 9
Training loss: 2.589174025809253
Validation loss: 2.57591983061919

Epoch: 6| Step: 10
Training loss: 2.3650513128476396
Validation loss: 2.5753407789857246

Epoch: 6| Step: 11
Training loss: 2.777400921583239
Validation loss: 2.5740486472360535

Epoch: 6| Step: 12
Training loss: 2.9067323345967435
Validation loss: 2.5763061093396464

Epoch: 6| Step: 13
Training loss: 3.0118955963500773
Validation loss: 2.5940530053656623

Epoch: 75| Step: 0
Training loss: 2.955561203182373
Validation loss: 2.5795321207131745

Epoch: 6| Step: 1
Training loss: 3.1553279927483433
Validation loss: 2.570783600677084

Epoch: 6| Step: 2
Training loss: 2.9504267561690236
Validation loss: 2.569315547217311

Epoch: 6| Step: 3
Training loss: 3.1117656469582275
Validation loss: 2.5706609913663305

Epoch: 6| Step: 4
Training loss: 3.115359580653967
Validation loss: 2.5748696496221735

Epoch: 6| Step: 5
Training loss: 2.629721799865899
Validation loss: 2.5752187039647714

Epoch: 6| Step: 6
Training loss: 3.2460269117761635
Validation loss: 2.5767424385565407

Epoch: 6| Step: 7
Training loss: 2.8999277895125606
Validation loss: 2.598103655567651

Epoch: 6| Step: 8
Training loss: 2.9170582281401005
Validation loss: 2.631980527197793

Epoch: 6| Step: 9
Training loss: 3.2865412601380566
Validation loss: 2.634577454644355

Epoch: 6| Step: 10
Training loss: 2.2510678618279565
Validation loss: 2.6321169352891243

Epoch: 6| Step: 11
Training loss: 3.034878476269298
Validation loss: 2.631770951369779

Epoch: 6| Step: 12
Training loss: 3.0159300022350233
Validation loss: 2.636337256321296

Epoch: 6| Step: 13
Training loss: 2.1887557648692306
Validation loss: 2.6487662673676216

Epoch: 76| Step: 0
Training loss: 3.2164418361011866
Validation loss: 2.655498212462456

Epoch: 6| Step: 1
Training loss: 2.5790118917225806
Validation loss: 2.6810861782774476

Epoch: 6| Step: 2
Training loss: 2.923490026951196
Validation loss: 2.7215381866065314

Epoch: 6| Step: 3
Training loss: 3.267407382624383
Validation loss: 2.755481151825492

Epoch: 6| Step: 4
Training loss: 3.100415687993223
Validation loss: 2.714227011928098

Epoch: 6| Step: 5
Training loss: 2.093624737536786
Validation loss: 2.667727221688571

Epoch: 6| Step: 6
Training loss: 3.150212774205278
Validation loss: 2.6520739316372905

Epoch: 6| Step: 7
Training loss: 3.146788967740785
Validation loss: 2.655447745251798

Epoch: 6| Step: 8
Training loss: 2.736337709376059
Validation loss: 2.644227098846355

Epoch: 6| Step: 9
Training loss: 3.423132539064896
Validation loss: 2.6379355486554266

Epoch: 6| Step: 10
Training loss: 3.0876838474904167
Validation loss: 2.628969372245659

Epoch: 6| Step: 11
Training loss: 2.9835060012831183
Validation loss: 2.628022672993389

Epoch: 6| Step: 12
Training loss: 2.4907446245018505
Validation loss: 2.6267809366363646

Epoch: 6| Step: 13
Training loss: 3.272703976259256
Validation loss: 2.624421557454352

Epoch: 77| Step: 0
Training loss: 2.5088895107190052
Validation loss: 2.6199689762827783

Epoch: 6| Step: 1
Training loss: 3.3205732893906803
Validation loss: 2.619030390094

Epoch: 6| Step: 2
Training loss: 2.9395864665561975
Validation loss: 2.625944545798718

Epoch: 6| Step: 3
Training loss: 2.7802166680079696
Validation loss: 2.628444441214707

Epoch: 6| Step: 4
Training loss: 2.3346658262508733
Validation loss: 2.629300951241972

Epoch: 6| Step: 5
Training loss: 3.163995285262153
Validation loss: 2.626935035675916

Epoch: 6| Step: 6
Training loss: 2.7826196962507117
Validation loss: 2.6328634351670304

Epoch: 6| Step: 7
Training loss: 2.850027010605626
Validation loss: 2.631678829898997

Epoch: 6| Step: 8
Training loss: 3.2538401584393113
Validation loss: 2.6252765919979826

Epoch: 6| Step: 9
Training loss: 2.9507074703883585
Validation loss: 2.6152949982295617

Epoch: 6| Step: 10
Training loss: 2.9565256712050765
Validation loss: 2.5984635551749125

Epoch: 6| Step: 11
Training loss: 2.9775920699303504
Validation loss: 2.6270956697073746

Epoch: 6| Step: 12
Training loss: 3.4738587570598587
Validation loss: 2.65386263978192

Epoch: 6| Step: 13
Training loss: 3.462292959034473
Validation loss: 2.6163838087541786

Epoch: 78| Step: 0
Training loss: 2.8949728769141942
Validation loss: 2.5965277965034845

Epoch: 6| Step: 1
Training loss: 2.7664971134978074
Validation loss: 2.5875663102443296

Epoch: 6| Step: 2
Training loss: 3.2751518621979936
Validation loss: 2.570292103999376

Epoch: 6| Step: 3
Training loss: 3.35644985861272
Validation loss: 2.5608204735747973

Epoch: 6| Step: 4
Training loss: 2.861557424685968
Validation loss: 2.560777516959994

Epoch: 6| Step: 5
Training loss: 2.2633008940913766
Validation loss: 2.5646656608314182

Epoch: 6| Step: 6
Training loss: 2.865969531731195
Validation loss: 2.5605304691491875

Epoch: 6| Step: 7
Training loss: 2.577017436396711
Validation loss: 2.5711488280433406

Epoch: 6| Step: 8
Training loss: 2.3429988420315078
Validation loss: 2.571428985232691

Epoch: 6| Step: 9
Training loss: 2.8047527847555034
Validation loss: 2.5707633779430843

Epoch: 6| Step: 10
Training loss: 3.239157269458243
Validation loss: 2.5739801832756806

Epoch: 6| Step: 11
Training loss: 2.8224331393571322
Validation loss: 2.5819694863258587

Epoch: 6| Step: 12
Training loss: 3.139628997315398
Validation loss: 2.5825763258797827

Epoch: 6| Step: 13
Training loss: 3.675528928100846
Validation loss: 2.5772989162125226

Epoch: 79| Step: 0
Training loss: 3.2040490283048495
Validation loss: 2.577554080080257

Epoch: 6| Step: 1
Training loss: 2.678382043955374
Validation loss: 2.565980667786476

Epoch: 6| Step: 2
Training loss: 2.494356939125675
Validation loss: 2.568019002907052

Epoch: 6| Step: 3
Training loss: 2.0757847278753916
Validation loss: 2.5794669409208195

Epoch: 6| Step: 4
Training loss: 2.9313309192672112
Validation loss: 2.5773046755222406

Epoch: 6| Step: 5
Training loss: 2.7501909016325325
Validation loss: 2.574514776442075

Epoch: 6| Step: 6
Training loss: 3.2108402283871125
Validation loss: 2.5746152134530678

Epoch: 6| Step: 7
Training loss: 2.779173429405605
Validation loss: 2.5671502993581634

Epoch: 6| Step: 8
Training loss: 2.6023628905252925
Validation loss: 2.563074227179478

Epoch: 6| Step: 9
Training loss: 3.1721759521827355
Validation loss: 2.55727767481037

Epoch: 6| Step: 10
Training loss: 3.5927430442194317
Validation loss: 2.5545919364774043

Epoch: 6| Step: 11
Training loss: 3.020610897600376
Validation loss: 2.5513798627917157

Epoch: 6| Step: 12
Training loss: 2.887843427836123
Validation loss: 2.5506736319220042

Epoch: 6| Step: 13
Training loss: 3.3182524629628447
Validation loss: 2.552806984869224

Epoch: 80| Step: 0
Training loss: 2.3859789212071987
Validation loss: 2.5542196152855494

Epoch: 6| Step: 1
Training loss: 3.0272607410494583
Validation loss: 2.5535359678225666

Epoch: 6| Step: 2
Training loss: 3.0258976195690708
Validation loss: 2.5526551959330415

Epoch: 6| Step: 3
Training loss: 2.816137039079593
Validation loss: 2.553340242895348

Epoch: 6| Step: 4
Training loss: 2.7525948506573275
Validation loss: 2.5617344751004145

Epoch: 6| Step: 5
Training loss: 3.0148279589506117
Validation loss: 2.5603006960953167

Epoch: 6| Step: 6
Training loss: 3.542448264251014
Validation loss: 2.5728896207178

Epoch: 6| Step: 7
Training loss: 2.29844393959414
Validation loss: 2.577216002609634

Epoch: 6| Step: 8
Training loss: 3.18717685632231
Validation loss: 2.6063683621718297

Epoch: 6| Step: 9
Training loss: 2.95152731975957
Validation loss: 2.6092823344432015

Epoch: 6| Step: 10
Training loss: 2.6519471266641177
Validation loss: 2.5936098847916327

Epoch: 6| Step: 11
Training loss: 2.6660592658973927
Validation loss: 2.5761055523315477

Epoch: 6| Step: 12
Training loss: 3.4162499863770783
Validation loss: 2.5709094738716933

Epoch: 6| Step: 13
Training loss: 2.6222606170756495
Validation loss: 2.5715497873891136

Epoch: 81| Step: 0
Training loss: 3.302553033164681
Validation loss: 2.563932521133884

Epoch: 6| Step: 1
Training loss: 3.066242966890725
Validation loss: 2.553659944482618

Epoch: 6| Step: 2
Training loss: 2.757734605591464
Validation loss: 2.548955711006922

Epoch: 6| Step: 3
Training loss: 2.9761222453697638
Validation loss: 2.5489941046440348

Epoch: 6| Step: 4
Training loss: 3.1154290691511672
Validation loss: 2.5569628335409074

Epoch: 6| Step: 5
Training loss: 2.7766084837875886
Validation loss: 2.5521016833256955

Epoch: 6| Step: 6
Training loss: 2.4946623087073054
Validation loss: 2.5570327797835164

Epoch: 6| Step: 7
Training loss: 2.662649277634908
Validation loss: 2.563960292821489

Epoch: 6| Step: 8
Training loss: 3.151822903891124
Validation loss: 2.5731138898004713

Epoch: 6| Step: 9
Training loss: 2.8868717055003597
Validation loss: 2.5621216177233364

Epoch: 6| Step: 10
Training loss: 2.1952302856255175
Validation loss: 2.5896031360372134

Epoch: 6| Step: 11
Training loss: 2.745600909833602
Validation loss: 2.588551045881107

Epoch: 6| Step: 12
Training loss: 3.3890170314582604
Validation loss: 2.6084751903470385

Epoch: 6| Step: 13
Training loss: 2.94486368741427
Validation loss: 2.6111824437465216

Epoch: 82| Step: 0
Training loss: 2.85732274851557
Validation loss: 2.5901505636806275

Epoch: 6| Step: 1
Training loss: 3.3658035160137683
Validation loss: 2.573714146438479

Epoch: 6| Step: 2
Training loss: 2.6622185464305406
Validation loss: 2.556055817483432

Epoch: 6| Step: 3
Training loss: 3.1952925030891697
Validation loss: 2.5454679571301684

Epoch: 6| Step: 4
Training loss: 3.064294406143041
Validation loss: 2.546159210973082

Epoch: 6| Step: 5
Training loss: 2.8804786559871705
Validation loss: 2.5435353743500513

Epoch: 6| Step: 6
Training loss: 1.9783742690357071
Validation loss: 2.5429163590105497

Epoch: 6| Step: 7
Training loss: 3.1665571762698264
Validation loss: 2.5484241359524735

Epoch: 6| Step: 8
Training loss: 3.434017289808556
Validation loss: 2.5463128862765516

Epoch: 6| Step: 9
Training loss: 2.462201281403794
Validation loss: 2.547747456211378

Epoch: 6| Step: 10
Training loss: 3.083468838025324
Validation loss: 2.546207485703176

Epoch: 6| Step: 11
Training loss: 2.9002660037410455
Validation loss: 2.54762457949773

Epoch: 6| Step: 12
Training loss: 2.188523189485441
Validation loss: 2.549670701879378

Epoch: 6| Step: 13
Training loss: 3.045398843636296
Validation loss: 2.553881823694646

Epoch: 83| Step: 0
Training loss: 2.5137759219008693
Validation loss: 2.5591219454136893

Epoch: 6| Step: 1
Training loss: 2.5714984233390754
Validation loss: 2.5669914339250686

Epoch: 6| Step: 2
Training loss: 3.3053103461974906
Validation loss: 2.5701265481948896

Epoch: 6| Step: 3
Training loss: 3.090682850171897
Validation loss: 2.5813638835767816

Epoch: 6| Step: 4
Training loss: 2.921778386604584
Validation loss: 2.585276131836529

Epoch: 6| Step: 5
Training loss: 3.0006646373747285
Validation loss: 2.5855532144616347

Epoch: 6| Step: 6
Training loss: 2.5678055895066723
Validation loss: 2.583676160306387

Epoch: 6| Step: 7
Training loss: 2.9336774588069514
Validation loss: 2.611896092060902

Epoch: 6| Step: 8
Training loss: 2.8702841519039364
Validation loss: 2.6064167866329053

Epoch: 6| Step: 9
Training loss: 3.5129509144303834
Validation loss: 2.6123552580507976

Epoch: 6| Step: 10
Training loss: 3.1027577592990765
Validation loss: 2.5973620131900033

Epoch: 6| Step: 11
Training loss: 2.320202860184524
Validation loss: 2.5785289846883876

Epoch: 6| Step: 12
Training loss: 2.7983350299453664
Validation loss: 2.5630759025490857

Epoch: 6| Step: 13
Training loss: 3.0800174375758793
Validation loss: 2.5551460895393543

Epoch: 84| Step: 0
Training loss: 2.668675162988054
Validation loss: 2.5462367455073585

Epoch: 6| Step: 1
Training loss: 3.2230131194451
Validation loss: 2.5429869789239

Epoch: 6| Step: 2
Training loss: 3.1524547760613957
Validation loss: 2.541829953394417

Epoch: 6| Step: 3
Training loss: 2.657782796775618
Validation loss: 2.5484582612763482

Epoch: 6| Step: 4
Training loss: 3.150767635483464
Validation loss: 2.5517274837426855

Epoch: 6| Step: 5
Training loss: 2.6494365596976097
Validation loss: 2.561246684003236

Epoch: 6| Step: 6
Training loss: 3.0933564831700555
Validation loss: 2.584278299714531

Epoch: 6| Step: 7
Training loss: 2.8399926819505565
Validation loss: 2.5791881160476016

Epoch: 6| Step: 8
Training loss: 3.2957229409295223
Validation loss: 2.59198251145012

Epoch: 6| Step: 9
Training loss: 3.166332494938878
Validation loss: 2.5883201787777153

Epoch: 6| Step: 10
Training loss: 2.893977020784058
Validation loss: 2.600124711854887

Epoch: 6| Step: 11
Training loss: 2.296674654780182
Validation loss: 2.5926942501493877

Epoch: 6| Step: 12
Training loss: 2.13408245801845
Validation loss: 2.593527890997156

Epoch: 6| Step: 13
Training loss: 3.14151034046337
Validation loss: 2.580415690382056

Epoch: 85| Step: 0
Training loss: 2.8536499743783454
Validation loss: 2.5661594323815646

Epoch: 6| Step: 1
Training loss: 2.592701159832968
Validation loss: 2.567015377517858

Epoch: 6| Step: 2
Training loss: 2.701004519625812
Validation loss: 2.555901249517153

Epoch: 6| Step: 3
Training loss: 2.8112624094592924
Validation loss: 2.5413182660477744

Epoch: 6| Step: 4
Training loss: 2.882734261461606
Validation loss: 2.5406516126224146

Epoch: 6| Step: 5
Training loss: 2.6386362746835124
Validation loss: 2.5409277257520033

Epoch: 6| Step: 6
Training loss: 3.137573277806974
Validation loss: 2.551979614989689

Epoch: 6| Step: 7
Training loss: 3.2322386537007968
Validation loss: 2.564556480160434

Epoch: 6| Step: 8
Training loss: 3.082444311970837
Validation loss: 2.578575844123843

Epoch: 6| Step: 9
Training loss: 2.2958869106274333
Validation loss: 2.574700952980689

Epoch: 6| Step: 10
Training loss: 2.694179219260817
Validation loss: 2.564560104862969

Epoch: 6| Step: 11
Training loss: 3.223543320147138
Validation loss: 2.550345698245318

Epoch: 6| Step: 12
Training loss: 2.980272599111472
Validation loss: 2.5518193208617497

Epoch: 6| Step: 13
Training loss: 3.466152421394276
Validation loss: 2.545485970758677

Epoch: 86| Step: 0
Training loss: 2.6694798134974556
Validation loss: 2.544996848331383

Epoch: 6| Step: 1
Training loss: 3.4623640233411264
Validation loss: 2.5413157582100707

Epoch: 6| Step: 2
Training loss: 2.5822020689170007
Validation loss: 2.53710945987841

Epoch: 6| Step: 3
Training loss: 3.0576042435748856
Validation loss: 2.5409716222832532

Epoch: 6| Step: 4
Training loss: 2.5112522576010066
Validation loss: 2.5371667895434054

Epoch: 6| Step: 5
Training loss: 2.939652324022828
Validation loss: 2.5301619702691602

Epoch: 6| Step: 6
Training loss: 2.9114083073685073
Validation loss: 2.534025041670541

Epoch: 6| Step: 7
Training loss: 3.113529975556909
Validation loss: 2.5390326294510532

Epoch: 6| Step: 8
Training loss: 3.0886703558749584
Validation loss: 2.5432114535486043

Epoch: 6| Step: 9
Training loss: 2.5778314596739764
Validation loss: 2.567403231612658

Epoch: 6| Step: 10
Training loss: 2.880010468146055
Validation loss: 2.559402206083778

Epoch: 6| Step: 11
Training loss: 2.456406846025447
Validation loss: 2.5480970174693423

Epoch: 6| Step: 12
Training loss: 2.7041311872022957
Validation loss: 2.560051052702195

Epoch: 6| Step: 13
Training loss: 3.37020094841216
Validation loss: 2.556958088175731

Epoch: 87| Step: 0
Training loss: 2.7320380987747477
Validation loss: 2.554513509459546

Epoch: 6| Step: 1
Training loss: 2.3189398304847084
Validation loss: 2.554984806058077

Epoch: 6| Step: 2
Training loss: 2.582326713427798
Validation loss: 2.556155221654629

Epoch: 6| Step: 3
Training loss: 2.7663230230636713
Validation loss: 2.561657983084652

Epoch: 6| Step: 4
Training loss: 2.5774472357217495
Validation loss: 2.560227908233174

Epoch: 6| Step: 5
Training loss: 2.9198251969781004
Validation loss: 2.5435762465631253

Epoch: 6| Step: 6
Training loss: 3.220456902210707
Validation loss: 2.546052267496626

Epoch: 6| Step: 7
Training loss: 3.388932328562446
Validation loss: 2.536485300457465

Epoch: 6| Step: 8
Training loss: 3.2950506722196424
Validation loss: 2.5388680382812865

Epoch: 6| Step: 9
Training loss: 2.6219822930990033
Validation loss: 2.5351288494080024

Epoch: 6| Step: 10
Training loss: 2.6897999879865258
Validation loss: 2.532327110614235

Epoch: 6| Step: 11
Training loss: 2.6459206293878625
Validation loss: 2.529244215000523

Epoch: 6| Step: 12
Training loss: 3.1608845139200126
Validation loss: 2.5377445507831604

Epoch: 6| Step: 13
Training loss: 3.2113954537705327
Validation loss: 2.540219573478097

Epoch: 88| Step: 0
Training loss: 2.6900879575395518
Validation loss: 2.551939777474547

Epoch: 6| Step: 1
Training loss: 3.1391352066422775
Validation loss: 2.556047115723532

Epoch: 6| Step: 2
Training loss: 2.5543199560601733
Validation loss: 2.586725186212579

Epoch: 6| Step: 3
Training loss: 2.331462950606608
Validation loss: 2.5842820217489333

Epoch: 6| Step: 4
Training loss: 3.3703912420717255
Validation loss: 2.5911496437816868

Epoch: 6| Step: 5
Training loss: 3.2100008969885026
Validation loss: 2.584772725837628

Epoch: 6| Step: 6
Training loss: 2.9837524403824593
Validation loss: 2.590865250177608

Epoch: 6| Step: 7
Training loss: 2.4690203941124036
Validation loss: 2.571815246670183

Epoch: 6| Step: 8
Training loss: 2.747345249861407
Validation loss: 2.546007125392773

Epoch: 6| Step: 9
Training loss: 2.893619944737477
Validation loss: 2.542488465240175

Epoch: 6| Step: 10
Training loss: 3.1087346783464818
Validation loss: 2.540269110058773

Epoch: 6| Step: 11
Training loss: 2.8360952118035527
Validation loss: 2.5465172222481884

Epoch: 6| Step: 12
Training loss: 3.2084887743529547
Validation loss: 2.558374490981706

Epoch: 6| Step: 13
Training loss: 2.2699469023220726
Validation loss: 2.5837501867018915

Epoch: 89| Step: 0
Training loss: 2.7711444670491434
Validation loss: 2.6206964803309627

Epoch: 6| Step: 1
Training loss: 3.2947635488227935
Validation loss: 2.62978470756645

Epoch: 6| Step: 2
Training loss: 2.3314383054878682
Validation loss: 2.6206086583736594

Epoch: 6| Step: 3
Training loss: 2.7018883141871926
Validation loss: 2.5868273204999914

Epoch: 6| Step: 4
Training loss: 2.999246184692347
Validation loss: 2.5582537661502744

Epoch: 6| Step: 5
Training loss: 2.8009135322775336
Validation loss: 2.538895482327686

Epoch: 6| Step: 6
Training loss: 2.1869421111278453
Validation loss: 2.530711493616251

Epoch: 6| Step: 7
Training loss: 3.0708498108489475
Validation loss: 2.529394317535105

Epoch: 6| Step: 8
Training loss: 2.6706159871219572
Validation loss: 2.529614189256325

Epoch: 6| Step: 9
Training loss: 2.7827919960873015
Validation loss: 2.529030840267517

Epoch: 6| Step: 10
Training loss: 3.1099625995509124
Validation loss: 2.534581728162128

Epoch: 6| Step: 11
Training loss: 2.7260107668120366
Validation loss: 2.537010592794283

Epoch: 6| Step: 12
Training loss: 3.7917833135638808
Validation loss: 2.5532864582309895

Epoch: 6| Step: 13
Training loss: 2.939133048136133
Validation loss: 2.5624170607563777

Epoch: 90| Step: 0
Training loss: 2.724060834678674
Validation loss: 2.5660166387006944

Epoch: 6| Step: 1
Training loss: 2.500322130430479
Validation loss: 2.563491739426399

Epoch: 6| Step: 2
Training loss: 2.6001364892199876
Validation loss: 2.562031615815223

Epoch: 6| Step: 3
Training loss: 2.9374183481109477
Validation loss: 2.572880968916772

Epoch: 6| Step: 4
Training loss: 2.5097289561401994
Validation loss: 2.573262298101288

Epoch: 6| Step: 5
Training loss: 3.3955993727768936
Validation loss: 2.5650502552101133

Epoch: 6| Step: 6
Training loss: 3.074836329624634
Validation loss: 2.5566490499905594

Epoch: 6| Step: 7
Training loss: 2.7678567104075573
Validation loss: 2.554194119465201

Epoch: 6| Step: 8
Training loss: 3.0796337786846393
Validation loss: 2.552714825758925

Epoch: 6| Step: 9
Training loss: 2.722230371692249
Validation loss: 2.5373054527209007

Epoch: 6| Step: 10
Training loss: 3.0670300669208452
Validation loss: 2.5245493032680146

Epoch: 6| Step: 11
Training loss: 2.890893047379851
Validation loss: 2.519471158373626

Epoch: 6| Step: 12
Training loss: 3.070510353139255
Validation loss: 2.5220525673345664

Epoch: 6| Step: 13
Training loss: 2.685896994222641
Validation loss: 2.520325301282744

Epoch: 91| Step: 0
Training loss: 2.14407894179925
Validation loss: 2.5229347851112642

Epoch: 6| Step: 1
Training loss: 2.620572124942171
Validation loss: 2.5186423232242667

Epoch: 6| Step: 2
Training loss: 2.3737712492350793
Validation loss: 2.519102252884613

Epoch: 6| Step: 3
Training loss: 2.651933011821421
Validation loss: 2.524807203858447

Epoch: 6| Step: 4
Training loss: 2.868322994724093
Validation loss: 2.522036721225753

Epoch: 6| Step: 5
Training loss: 2.8656851759055435
Validation loss: 2.5391219472433626

Epoch: 6| Step: 6
Training loss: 2.657986600579443
Validation loss: 2.5446070237378593

Epoch: 6| Step: 7
Training loss: 3.1262149737751335
Validation loss: 2.545238603066511

Epoch: 6| Step: 8
Training loss: 3.3955787298011986
Validation loss: 2.5582747872348044

Epoch: 6| Step: 9
Training loss: 3.2230193332396446
Validation loss: 2.558217749216415

Epoch: 6| Step: 10
Training loss: 1.8380178150106459
Validation loss: 2.569270627758137

Epoch: 6| Step: 11
Training loss: 3.7643335430884184
Validation loss: 2.560520565114646

Epoch: 6| Step: 12
Training loss: 2.9659912140523845
Validation loss: 2.536874509606409

Epoch: 6| Step: 13
Training loss: 3.3172494200624687
Validation loss: 2.5352616853143712

Epoch: 92| Step: 0
Training loss: 3.2644242264747865
Validation loss: 2.5200362247403283

Epoch: 6| Step: 1
Training loss: 2.779504635538846
Validation loss: 2.5193353865382497

Epoch: 6| Step: 2
Training loss: 2.8717666356476705
Validation loss: 2.519143609035544

Epoch: 6| Step: 3
Training loss: 2.301452642384092
Validation loss: 2.522682220411989

Epoch: 6| Step: 4
Training loss: 2.670743825506657
Validation loss: 2.51603112330676

Epoch: 6| Step: 5
Training loss: 2.6606986099322687
Validation loss: 2.529593402272544

Epoch: 6| Step: 6
Training loss: 2.841384238996149
Validation loss: 2.5351054348942164

Epoch: 6| Step: 7
Training loss: 2.874494757002042
Validation loss: 2.550702854564824

Epoch: 6| Step: 8
Training loss: 3.2421992657919043
Validation loss: 2.5703353444456387

Epoch: 6| Step: 9
Training loss: 2.5041257193756707
Validation loss: 2.5677584546498187

Epoch: 6| Step: 10
Training loss: 3.153488915485179
Validation loss: 2.5677889405009973

Epoch: 6| Step: 11
Training loss: 3.218281832229762
Validation loss: 2.5341628944620944

Epoch: 6| Step: 12
Training loss: 2.986350159847582
Validation loss: 2.5140837519423074

Epoch: 6| Step: 13
Training loss: 2.266298917315033
Validation loss: 2.5184518437563126

Epoch: 93| Step: 0
Training loss: 2.989977625632377
Validation loss: 2.518826823018598

Epoch: 6| Step: 1
Training loss: 2.5923756094862447
Validation loss: 2.5230646853953997

Epoch: 6| Step: 2
Training loss: 2.4881976965465604
Validation loss: 2.5261034148703367

Epoch: 6| Step: 3
Training loss: 3.0520101458179627
Validation loss: 2.527873975275656

Epoch: 6| Step: 4
Training loss: 2.332638659837954
Validation loss: 2.5280784082737795

Epoch: 6| Step: 5
Training loss: 2.840453196618916
Validation loss: 2.51956790236987

Epoch: 6| Step: 6
Training loss: 2.986176431419522
Validation loss: 2.5277046787432784

Epoch: 6| Step: 7
Training loss: 3.063368849218106
Validation loss: 2.516046228786967

Epoch: 6| Step: 8
Training loss: 2.9727651648801494
Validation loss: 2.5243048165875037

Epoch: 6| Step: 9
Training loss: 3.5060905869091976
Validation loss: 2.537505871705203

Epoch: 6| Step: 10
Training loss: 3.500198358636934
Validation loss: 2.5672084191324784

Epoch: 6| Step: 11
Training loss: 2.9938501265631627
Validation loss: 2.5396771884486506

Epoch: 6| Step: 12
Training loss: 1.9239051454879055
Validation loss: 2.5309921856489708

Epoch: 6| Step: 13
Training loss: 2.4176244043730444
Validation loss: 2.513001141775722

Epoch: 94| Step: 0
Training loss: 3.295039818715608
Validation loss: 2.527048421630982

Epoch: 6| Step: 1
Training loss: 3.3116553327279763
Validation loss: 2.5312485456240945

Epoch: 6| Step: 2
Training loss: 3.1273849541767844
Validation loss: 2.5546996122717034

Epoch: 6| Step: 3
Training loss: 2.9206644906415615
Validation loss: 2.549668742199834

Epoch: 6| Step: 4
Training loss: 2.4254307393832977
Validation loss: 2.5353500550249826

Epoch: 6| Step: 5
Training loss: 1.9263726925996063
Validation loss: 2.537088238671825

Epoch: 6| Step: 6
Training loss: 2.94528578687944
Validation loss: 2.5415497623013854

Epoch: 6| Step: 7
Training loss: 2.9172228328320213
Validation loss: 2.5670599474591542

Epoch: 6| Step: 8
Training loss: 2.4624890477280634
Validation loss: 2.5596320597933024

Epoch: 6| Step: 9
Training loss: 3.179212380043503
Validation loss: 2.5580848887329983

Epoch: 6| Step: 10
Training loss: 3.042241882559155
Validation loss: 2.5514130955139014

Epoch: 6| Step: 11
Training loss: 2.7999707356694765
Validation loss: 2.5629347025562685

Epoch: 6| Step: 12
Training loss: 2.6314151123144085
Validation loss: 2.5492561443309087

Epoch: 6| Step: 13
Training loss: 2.4037103503086956
Validation loss: 2.5481503824714378

Epoch: 95| Step: 0
Training loss: 2.2161067207611493
Validation loss: 2.5443468861621135

Epoch: 6| Step: 1
Training loss: 2.9823795058283022
Validation loss: 2.5480260597634397

Epoch: 6| Step: 2
Training loss: 2.550527749893836
Validation loss: 2.5548968264958383

Epoch: 6| Step: 3
Training loss: 3.0581523630370864
Validation loss: 2.550550150373233

Epoch: 6| Step: 4
Training loss: 3.1462776594495856
Validation loss: 2.542140808207119

Epoch: 6| Step: 5
Training loss: 2.776418024618108
Validation loss: 2.5375554132727953

Epoch: 6| Step: 6
Training loss: 3.135153335885529
Validation loss: 2.532573575202459

Epoch: 6| Step: 7
Training loss: 2.8702391306180663
Validation loss: 2.537480123084299

Epoch: 6| Step: 8
Training loss: 2.728229177262158
Validation loss: 2.541722925600072

Epoch: 6| Step: 9
Training loss: 2.9469122551578644
Validation loss: 2.5536024590374495

Epoch: 6| Step: 10
Training loss: 2.7432283778950257
Validation loss: 2.5568892157233214

Epoch: 6| Step: 11
Training loss: 3.4526927556639975
Validation loss: 2.56783632039632

Epoch: 6| Step: 12
Training loss: 2.2631385576526313
Validation loss: 2.585176955900012

Epoch: 6| Step: 13
Training loss: 2.721952634770682
Validation loss: 2.5802927339924757

Epoch: 96| Step: 0
Training loss: 2.759107680024816
Validation loss: 2.578122462336341

Epoch: 6| Step: 1
Training loss: 3.302495134510323
Validation loss: 2.572921695798141

Epoch: 6| Step: 2
Training loss: 3.0168467206243985
Validation loss: 2.5566289149991452

Epoch: 6| Step: 3
Training loss: 2.6850813694563906
Validation loss: 2.531331153560238

Epoch: 6| Step: 4
Training loss: 2.7455793742785635
Validation loss: 2.5267472805970073

Epoch: 6| Step: 5
Training loss: 2.589676841547269
Validation loss: 2.5165841642076296

Epoch: 6| Step: 6
Training loss: 2.424917856771867
Validation loss: 2.5285680740394314

Epoch: 6| Step: 7
Training loss: 2.358583229044003
Validation loss: 2.5197639919370904

Epoch: 6| Step: 8
Training loss: 2.4817038517359293
Validation loss: 2.519934780286384

Epoch: 6| Step: 9
Training loss: 2.7516935509012064
Validation loss: 2.5358050729100885

Epoch: 6| Step: 10
Training loss: 2.931307494813601
Validation loss: 2.5386810172654437

Epoch: 6| Step: 11
Training loss: 3.54718070036198
Validation loss: 2.5555310792837234

Epoch: 6| Step: 12
Training loss: 2.9178159538412842
Validation loss: 2.5488377033249017

Epoch: 6| Step: 13
Training loss: 3.3121470137323574
Validation loss: 2.5528219681430313

Epoch: 97| Step: 0
Training loss: 3.2854780295831434
Validation loss: 2.5548662520332015

Epoch: 6| Step: 1
Training loss: 2.4759708504077933
Validation loss: 2.5513502590389794

Epoch: 6| Step: 2
Training loss: 2.4118717462656614
Validation loss: 2.5281583002721613

Epoch: 6| Step: 3
Training loss: 2.5705158188011663
Validation loss: 2.520283754978737

Epoch: 6| Step: 4
Training loss: 2.78624799178108
Validation loss: 2.5206875543249843

Epoch: 6| Step: 5
Training loss: 2.6998954399490573
Validation loss: 2.516552981651868

Epoch: 6| Step: 6
Training loss: 2.6134373457152646
Validation loss: 2.507922963165016

Epoch: 6| Step: 7
Training loss: 2.8404847566839893
Validation loss: 2.510113896278474

Epoch: 6| Step: 8
Training loss: 3.087134175114702
Validation loss: 2.509964079855396

Epoch: 6| Step: 9
Training loss: 3.251780095809664
Validation loss: 2.50856851008032

Epoch: 6| Step: 10
Training loss: 2.7340187930652085
Validation loss: 2.5168633159315514

Epoch: 6| Step: 11
Training loss: 2.4872614568840814
Validation loss: 2.52609327234181

Epoch: 6| Step: 12
Training loss: 2.837002640882627
Validation loss: 2.5373334825056473

Epoch: 6| Step: 13
Training loss: 3.9660748454316175
Validation loss: 2.5350275872104335

Epoch: 98| Step: 0
Training loss: 2.890301330591018
Validation loss: 2.55439616299458

Epoch: 6| Step: 1
Training loss: 2.844651634730276
Validation loss: 2.544885003512818

Epoch: 6| Step: 2
Training loss: 2.7273250704857492
Validation loss: 2.5559384314963576

Epoch: 6| Step: 3
Training loss: 3.0665404466555013
Validation loss: 2.5474106359826116

Epoch: 6| Step: 4
Training loss: 2.4987820519533237
Validation loss: 2.5489105398617586

Epoch: 6| Step: 5
Training loss: 2.65635959174773
Validation loss: 2.5436820393699624

Epoch: 6| Step: 6
Training loss: 3.059988743879771
Validation loss: 2.536478884488355

Epoch: 6| Step: 7
Training loss: 2.6636700383057077
Validation loss: 2.5359537397985807

Epoch: 6| Step: 8
Training loss: 2.3529053969327545
Validation loss: 2.5287226338768836

Epoch: 6| Step: 9
Training loss: 3.0742494613102256
Validation loss: 2.5331027027429833

Epoch: 6| Step: 10
Training loss: 3.381791945371534
Validation loss: 2.52026253094514

Epoch: 6| Step: 11
Training loss: 2.916195495331578
Validation loss: 2.5220333179943224

Epoch: 6| Step: 12
Training loss: 2.78820129262786
Validation loss: 2.5195163007430867

Epoch: 6| Step: 13
Training loss: 2.317752029064197
Validation loss: 2.518725280998696

Epoch: 99| Step: 0
Training loss: 2.3263268407135356
Validation loss: 2.518996505544624

Epoch: 6| Step: 1
Training loss: 2.9307938991565914
Validation loss: 2.515465820714215

Epoch: 6| Step: 2
Training loss: 3.0722690395522982
Validation loss: 2.516053523707987

Epoch: 6| Step: 3
Training loss: 2.519926386166711
Validation loss: 2.526316525917298

Epoch: 6| Step: 4
Training loss: 2.768377626062051
Validation loss: 2.5202661430614417

Epoch: 6| Step: 5
Training loss: 2.620155678266242
Validation loss: 2.526354037271631

Epoch: 6| Step: 6
Training loss: 2.341459757486546
Validation loss: 2.531472629169172

Epoch: 6| Step: 7
Training loss: 2.4844542676647676
Validation loss: 2.5428465497271246

Epoch: 6| Step: 8
Training loss: 3.02404668619559
Validation loss: 2.556011692555153

Epoch: 6| Step: 9
Training loss: 3.234063764699618
Validation loss: 2.565860880327151

Epoch: 6| Step: 10
Training loss: 2.5279717586364328
Validation loss: 2.5537474948361463

Epoch: 6| Step: 11
Training loss: 3.0263207066755697
Validation loss: 2.544648363328112

Epoch: 6| Step: 12
Training loss: 3.60147498432005
Validation loss: 2.532090152342234

Epoch: 6| Step: 13
Training loss: 2.6103751270330875
Validation loss: 2.5266133811531826

Epoch: 100| Step: 0
Training loss: 2.791320485148878
Validation loss: 2.5220943365863353

Epoch: 6| Step: 1
Training loss: 3.044808493920174
Validation loss: 2.5114441538473513

Epoch: 6| Step: 2
Training loss: 3.1463540427305747
Validation loss: 2.522561429734756

Epoch: 6| Step: 3
Training loss: 3.234634610137008
Validation loss: 2.5313369906178456

Epoch: 6| Step: 4
Training loss: 2.5085387322194013
Validation loss: 2.5356396251686997

Epoch: 6| Step: 5
Training loss: 3.2987422627036307
Validation loss: 2.5512781854973285

Epoch: 6| Step: 6
Training loss: 2.742376685069449
Validation loss: 2.572784477891156

Epoch: 6| Step: 7
Training loss: 2.6603320003490727
Validation loss: 2.5891100541570142

Epoch: 6| Step: 8
Training loss: 2.2092894757456256
Validation loss: 2.5846605113656462

Epoch: 6| Step: 9
Training loss: 3.438955103069497
Validation loss: 2.562511963294891

Epoch: 6| Step: 10
Training loss: 2.1450498193677414
Validation loss: 2.5292134723412856

Epoch: 6| Step: 11
Training loss: 2.6988003043450908
Validation loss: 2.5100667351587154

Epoch: 6| Step: 12
Training loss: 2.497600739273558
Validation loss: 2.5004359326681027

Epoch: 6| Step: 13
Training loss: 3.0172276166780536
Validation loss: 2.506059129433375

Epoch: 101| Step: 0
Training loss: 3.0759742310018785
Validation loss: 2.5042739328674273

Epoch: 6| Step: 1
Training loss: 1.7836312556156082
Validation loss: 2.506601348791539

Epoch: 6| Step: 2
Training loss: 2.8557744938807583
Validation loss: 2.5081064836961904

Epoch: 6| Step: 3
Training loss: 2.788934870345497
Validation loss: 2.5116588909651214

Epoch: 6| Step: 4
Training loss: 2.2838538998843263
Validation loss: 2.5195623499089304

Epoch: 6| Step: 5
Training loss: 2.7050114041579856
Validation loss: 2.539655950843693

Epoch: 6| Step: 6
Training loss: 2.745561225232066
Validation loss: 2.569335816768211

Epoch: 6| Step: 7
Training loss: 3.2047028911784534
Validation loss: 2.629120161284447

Epoch: 6| Step: 8
Training loss: 3.391470808017988
Validation loss: 2.736056501255386

Epoch: 6| Step: 9
Training loss: 2.8408691569902813
Validation loss: 2.822639157147277

Epoch: 6| Step: 10
Training loss: 2.914846197294436
Validation loss: 2.759516606454105

Epoch: 6| Step: 11
Training loss: 3.496013960114285
Validation loss: 2.6612892351617514

Epoch: 6| Step: 12
Training loss: 2.9703019753499924
Validation loss: 2.5664819721938064

Epoch: 6| Step: 13
Training loss: 2.965550515593128
Validation loss: 2.525742218736166

Epoch: 102| Step: 0
Training loss: 3.344958505872062
Validation loss: 2.4982707062134817

Epoch: 6| Step: 1
Training loss: 2.8946093344098336
Validation loss: 2.5075956280284544

Epoch: 6| Step: 2
Training loss: 3.034321281044029
Validation loss: 2.5180887172794773

Epoch: 6| Step: 3
Training loss: 3.278645380861929
Validation loss: 2.517080505671415

Epoch: 6| Step: 4
Training loss: 3.0870201819093483
Validation loss: 2.50191198370905

Epoch: 6| Step: 5
Training loss: 3.3458698718845836
Validation loss: 2.4942874306822236

Epoch: 6| Step: 6
Training loss: 2.817922662749946
Validation loss: 2.494992391289588

Epoch: 6| Step: 7
Training loss: 2.8064046294636316
Validation loss: 2.5003943480932627

Epoch: 6| Step: 8
Training loss: 2.5543897729441336
Validation loss: 2.5092079661237143

Epoch: 6| Step: 9
Training loss: 2.184342639776172
Validation loss: 2.514637716596835

Epoch: 6| Step: 10
Training loss: 2.0423988145396716
Validation loss: 2.5303285340195614

Epoch: 6| Step: 11
Training loss: 2.3244966228655044
Validation loss: 2.5638894206788185

Epoch: 6| Step: 12
Training loss: 3.085022300503595
Validation loss: 2.5778384370418292

Epoch: 6| Step: 13
Training loss: 2.8457059735851393
Validation loss: 2.565672167816233

Epoch: 103| Step: 0
Training loss: 3.0620717897576935
Validation loss: 2.5583959609947824

Epoch: 6| Step: 1
Training loss: 2.7009766260553
Validation loss: 2.571394011245231

Epoch: 6| Step: 2
Training loss: 2.454048323139185
Validation loss: 2.557204608919477

Epoch: 6| Step: 3
Training loss: 3.0325902748350977
Validation loss: 2.5999028486680786

Epoch: 6| Step: 4
Training loss: 2.9396732488696133
Validation loss: 2.5502164678042614

Epoch: 6| Step: 5
Training loss: 3.163749019964783
Validation loss: 2.516270192934969

Epoch: 6| Step: 6
Training loss: 2.733939348665697
Validation loss: 2.5091650168404467

Epoch: 6| Step: 7
Training loss: 2.8064200912617654
Validation loss: 2.4999614743371406

Epoch: 6| Step: 8
Training loss: 2.50995570550263
Validation loss: 2.5180478498020453

Epoch: 6| Step: 9
Training loss: 3.2928917130297854
Validation loss: 2.5392764462178397

Epoch: 6| Step: 10
Training loss: 3.244316119293738
Validation loss: 2.557681228168025

Epoch: 6| Step: 11
Training loss: 2.9826955813338047
Validation loss: 2.528099302021856

Epoch: 6| Step: 12
Training loss: 2.0345011588501367
Validation loss: 2.525180954095739

Epoch: 6| Step: 13
Training loss: 2.812249236053952
Validation loss: 2.5139805753945894

Epoch: 104| Step: 0
Training loss: 2.42914214560781
Validation loss: 2.5182457435843557

Epoch: 6| Step: 1
Training loss: 3.372154590515747
Validation loss: 2.5333255809666397

Epoch: 6| Step: 2
Training loss: 2.6319585937906984
Validation loss: 2.550207624482113

Epoch: 6| Step: 3
Training loss: 2.412957095786404
Validation loss: 2.59208199568941

Epoch: 6| Step: 4
Training loss: 2.9949243842308846
Validation loss: 2.6107409029407758

Epoch: 6| Step: 5
Training loss: 2.433882155863922
Validation loss: 2.6337360300681505

Epoch: 6| Step: 6
Training loss: 2.9569669084087358
Validation loss: 2.6610963946677906

Epoch: 6| Step: 7
Training loss: 3.3132648664598765
Validation loss: 2.6669751320240023

Epoch: 6| Step: 8
Training loss: 2.74699020404548
Validation loss: 2.686787536841861

Epoch: 6| Step: 9
Training loss: 2.831571386424587
Validation loss: 2.6270999507400123

Epoch: 6| Step: 10
Training loss: 2.3142588344742028
Validation loss: 2.5790278529713193

Epoch: 6| Step: 11
Training loss: 3.1829115809588786
Validation loss: 2.5475026075904164

Epoch: 6| Step: 12
Training loss: 3.1200867748419987
Validation loss: 2.51953418550593

Epoch: 6| Step: 13
Training loss: 2.774748586314558
Validation loss: 2.511962822629346

Epoch: 105| Step: 0
Training loss: 2.105715254187985
Validation loss: 2.4995556969786765

Epoch: 6| Step: 1
Training loss: 2.6383469365295986
Validation loss: 2.5077194642827054

Epoch: 6| Step: 2
Training loss: 3.255579487304332
Validation loss: 2.506251933728452

Epoch: 6| Step: 3
Training loss: 2.8931916260479844
Validation loss: 2.511727304236249

Epoch: 6| Step: 4
Training loss: 2.817610442223335
Validation loss: 2.5235532780221424

Epoch: 6| Step: 5
Training loss: 3.063369783164573
Validation loss: 2.52739020226594

Epoch: 6| Step: 6
Training loss: 2.373776773359022
Validation loss: 2.5244684379282956

Epoch: 6| Step: 7
Training loss: 2.620826127649377
Validation loss: 2.5251451730597094

Epoch: 6| Step: 8
Training loss: 2.880266921336801
Validation loss: 2.519586315858551

Epoch: 6| Step: 9
Training loss: 3.0908173812683573
Validation loss: 2.524313177867115

Epoch: 6| Step: 10
Training loss: 2.5346365044672248
Validation loss: 2.5168142685216566

Epoch: 6| Step: 11
Training loss: 3.1803739589357662
Validation loss: 2.5159717418425758

Epoch: 6| Step: 12
Training loss: 3.409532557692167
Validation loss: 2.525182131255189

Epoch: 6| Step: 13
Training loss: 2.3316628630004717
Validation loss: 2.5394395814228936

Epoch: 106| Step: 0
Training loss: 3.2118025679474185
Validation loss: 2.5356209541693002

Epoch: 6| Step: 1
Training loss: 2.455556842560045
Validation loss: 2.5227348680040818

Epoch: 6| Step: 2
Training loss: 1.9920274857229587
Validation loss: 2.5260067740185947

Epoch: 6| Step: 3
Training loss: 2.948730751741272
Validation loss: 2.527802899678421

Epoch: 6| Step: 4
Training loss: 2.544734500009207
Validation loss: 2.5261297122862696

Epoch: 6| Step: 5
Training loss: 3.6215485551595092
Validation loss: 2.52498334292101

Epoch: 6| Step: 6
Training loss: 3.04821200257769
Validation loss: 2.5203065422612294

Epoch: 6| Step: 7
Training loss: 2.4876859662495194
Validation loss: 2.5211828500201063

Epoch: 6| Step: 8
Training loss: 3.0933393726070686
Validation loss: 2.5160088791120847

Epoch: 6| Step: 9
Training loss: 2.6556859932294348
Validation loss: 2.512184476133798

Epoch: 6| Step: 10
Training loss: 2.835332800759807
Validation loss: 2.504305141378766

Epoch: 6| Step: 11
Training loss: 2.1846548924381524
Validation loss: 2.508739441326737

Epoch: 6| Step: 12
Training loss: 2.8102737092537535
Validation loss: 2.508375782665849

Epoch: 6| Step: 13
Training loss: 3.123539239887611
Validation loss: 2.509915323550353

Epoch: 107| Step: 0
Training loss: 2.92276656476186
Validation loss: 2.5117003768187405

Epoch: 6| Step: 1
Training loss: 2.7922167639327555
Validation loss: 2.512373155267221

Epoch: 6| Step: 2
Training loss: 2.8921824615451115
Validation loss: 2.5149059639997096

Epoch: 6| Step: 3
Training loss: 2.7407027787031937
Validation loss: 2.522632253622638

Epoch: 6| Step: 4
Training loss: 2.732286451254514
Validation loss: 2.52338468622788

Epoch: 6| Step: 5
Training loss: 3.3581199963175776
Validation loss: 2.5258653256758663

Epoch: 6| Step: 6
Training loss: 3.1339803217091764
Validation loss: 2.5411781885248548

Epoch: 6| Step: 7
Training loss: 2.349994326645516
Validation loss: 2.5386797125625176

Epoch: 6| Step: 8
Training loss: 2.175654464123272
Validation loss: 2.5443830742553235

Epoch: 6| Step: 9
Training loss: 2.7021374965897045
Validation loss: 2.5602659025297663

Epoch: 6| Step: 10
Training loss: 2.233688355690901
Validation loss: 2.5846780981076036

Epoch: 6| Step: 11
Training loss: 3.365438409731675
Validation loss: 2.5980311512183145

Epoch: 6| Step: 12
Training loss: 3.388635711037216
Validation loss: 2.572890080060563

Epoch: 6| Step: 13
Training loss: 1.2953226381280163
Validation loss: 2.5532556676874907

Epoch: 108| Step: 0
Training loss: 2.4020176115637053
Validation loss: 2.548048897259927

Epoch: 6| Step: 1
Training loss: 2.702476997101148
Validation loss: 2.5293063894423815

Epoch: 6| Step: 2
Training loss: 3.1075552211037976
Validation loss: 2.523086038268473

Epoch: 6| Step: 3
Training loss: 2.963323512481218
Validation loss: 2.5169692211372223

Epoch: 6| Step: 4
Training loss: 2.9056133270193305
Validation loss: 2.5113659912420405

Epoch: 6| Step: 5
Training loss: 3.179467645865955
Validation loss: 2.5157336399672956

Epoch: 6| Step: 6
Training loss: 2.615937802879016
Validation loss: 2.5145269988832086

Epoch: 6| Step: 7
Training loss: 3.142946514493804
Validation loss: 2.509807943985894

Epoch: 6| Step: 8
Training loss: 2.7686601783528837
Validation loss: 2.5112908315757174

Epoch: 6| Step: 9
Training loss: 2.2328035390472523
Validation loss: 2.5020063050924777

Epoch: 6| Step: 10
Training loss: 2.706413077359089
Validation loss: 2.519964769375614

Epoch: 6| Step: 11
Training loss: 2.530911269894522
Validation loss: 2.52565747557349

Epoch: 6| Step: 12
Training loss: 2.7793303304710766
Validation loss: 2.526996478695465

Epoch: 6| Step: 13
Training loss: 3.126869862941294
Validation loss: 2.520848247660572

Epoch: 109| Step: 0
Training loss: 3.2494101355858493
Validation loss: 2.540729769288006

Epoch: 6| Step: 1
Training loss: 2.436050962456864
Validation loss: 2.5463559952896384

Epoch: 6| Step: 2
Training loss: 2.5438255355079953
Validation loss: 2.5827737490611575

Epoch: 6| Step: 3
Training loss: 2.3469708500576885
Validation loss: 2.5644512815835676

Epoch: 6| Step: 4
Training loss: 2.508575608560026
Validation loss: 2.569376573901406

Epoch: 6| Step: 5
Training loss: 2.522445530979131
Validation loss: 2.5504792634296334

Epoch: 6| Step: 6
Training loss: 2.9325389769062267
Validation loss: 2.5218907318883046

Epoch: 6| Step: 7
Training loss: 2.3893397248358617
Validation loss: 2.5118866755268003

Epoch: 6| Step: 8
Training loss: 3.4746858139164365
Validation loss: 2.5245816015607665

Epoch: 6| Step: 9
Training loss: 2.9825379474588796
Validation loss: 2.5201630009142906

Epoch: 6| Step: 10
Training loss: 2.9936209250639862
Validation loss: 2.507584413845157

Epoch: 6| Step: 11
Training loss: 2.9965288425727055
Validation loss: 2.5090428806194893

Epoch: 6| Step: 12
Training loss: 2.5859375
Validation loss: 2.4963175981863546

Epoch: 6| Step: 13
Training loss: 2.9843960906077407
Validation loss: 2.4927775621553003

Epoch: 110| Step: 0
Training loss: 2.9549424164868965
Validation loss: 2.491735635938048

Epoch: 6| Step: 1
Training loss: 3.3671236319802027
Validation loss: 2.48886163086414

Epoch: 6| Step: 2
Training loss: 3.3312122908241806
Validation loss: 2.4946643537306965

Epoch: 6| Step: 3
Training loss: 2.3443928154939915
Validation loss: 2.491840075121188

Epoch: 6| Step: 4
Training loss: 3.116391493809465
Validation loss: 2.4815281841886576

Epoch: 6| Step: 5
Training loss: 2.489471869321055
Validation loss: 2.49576849041658

Epoch: 6| Step: 6
Training loss: 2.2006461625082427
Validation loss: 2.487664829943121

Epoch: 6| Step: 7
Training loss: 2.860593442369834
Validation loss: 2.492305814912777

Epoch: 6| Step: 8
Training loss: 3.027510076076079
Validation loss: 2.4936461604358913

Epoch: 6| Step: 9
Training loss: 2.5792233988285025
Validation loss: 2.5073004428459034

Epoch: 6| Step: 10
Training loss: 2.3610089435787693
Validation loss: 2.509505209123349

Epoch: 6| Step: 11
Training loss: 2.9270981395704445
Validation loss: 2.501074458542189

Epoch: 6| Step: 12
Training loss: 2.6237057719653394
Validation loss: 2.5218931472218302

Epoch: 6| Step: 13
Training loss: 2.3340585353673107
Validation loss: 2.518824860715741

Epoch: 111| Step: 0
Training loss: 3.005403579548736
Validation loss: 2.509291549449663

Epoch: 6| Step: 1
Training loss: 2.4763822760725156
Validation loss: 2.5056057643889975

Epoch: 6| Step: 2
Training loss: 2.7362112800434226
Validation loss: 2.507466332167633

Epoch: 6| Step: 3
Training loss: 3.161742615002902
Validation loss: 2.502011011231843

Epoch: 6| Step: 4
Training loss: 2.690917215639638
Validation loss: 2.4907219928816375

Epoch: 6| Step: 5
Training loss: 2.8994678403673007
Validation loss: 2.4858917128367017

Epoch: 6| Step: 6
Training loss: 2.808056181712417
Validation loss: 2.4831070329478186

Epoch: 6| Step: 7
Training loss: 2.5519607410699723
Validation loss: 2.489309770510631

Epoch: 6| Step: 8
Training loss: 3.1820569407069645
Validation loss: 2.4864113175127254

Epoch: 6| Step: 9
Training loss: 2.7902921642613836
Validation loss: 2.4909433762164372

Epoch: 6| Step: 10
Training loss: 2.1972539062187497
Validation loss: 2.4870195667808273

Epoch: 6| Step: 11
Training loss: 2.053086385842825
Validation loss: 2.5044206277101275

Epoch: 6| Step: 12
Training loss: 3.0582825562075713
Validation loss: 2.522256094426824

Epoch: 6| Step: 13
Training loss: 3.394852073266998
Validation loss: 2.5462370314485225

Epoch: 112| Step: 0
Training loss: 3.139060926270838
Validation loss: 2.5459290411159006

Epoch: 6| Step: 1
Training loss: 2.6840502754596582
Validation loss: 2.559080203499286

Epoch: 6| Step: 2
Training loss: 2.8697378680845667
Validation loss: 2.579352455127799

Epoch: 6| Step: 3
Training loss: 2.995205385683793
Validation loss: 2.576340048441412

Epoch: 6| Step: 4
Training loss: 2.369961061117541
Validation loss: 2.5522635166297816

Epoch: 6| Step: 5
Training loss: 2.4545084945708897
Validation loss: 2.564108171117693

Epoch: 6| Step: 6
Training loss: 2.8561169349175555
Validation loss: 2.5733793680501598

Epoch: 6| Step: 7
Training loss: 2.821656559360681
Validation loss: 2.5824771499992827

Epoch: 6| Step: 8
Training loss: 3.243681267100179
Validation loss: 2.558395866802082

Epoch: 6| Step: 9
Training loss: 2.9314490147034222
Validation loss: 2.5218233049268113

Epoch: 6| Step: 10
Training loss: 2.657481726998983
Validation loss: 2.50536372986407

Epoch: 6| Step: 11
Training loss: 2.9324758865913036
Validation loss: 2.50204892540379

Epoch: 6| Step: 12
Training loss: 2.3016483246857162
Validation loss: 2.510423447774202

Epoch: 6| Step: 13
Training loss: 2.794858578565672
Validation loss: 2.508516846216677

Epoch: 113| Step: 0
Training loss: 2.0403293459701195
Validation loss: 2.514766403156799

Epoch: 6| Step: 1
Training loss: 2.9157444903879246
Validation loss: 2.519857993040182

Epoch: 6| Step: 2
Training loss: 2.5201906272903405
Validation loss: 2.5398411235790133

Epoch: 6| Step: 3
Training loss: 3.08808534455832
Validation loss: 2.544631198105168

Epoch: 6| Step: 4
Training loss: 2.8486102145465506
Validation loss: 2.5353990188890285

Epoch: 6| Step: 5
Training loss: 2.668225121083948
Validation loss: 2.5322290692153016

Epoch: 6| Step: 6
Training loss: 2.5054660170242538
Validation loss: 2.5322706220132716

Epoch: 6| Step: 7
Training loss: 2.5071026043888005
Validation loss: 2.5518470686606647

Epoch: 6| Step: 8
Training loss: 3.454525072548711
Validation loss: 2.588057929969612

Epoch: 6| Step: 9
Training loss: 2.973231095864239
Validation loss: 2.588686702145726

Epoch: 6| Step: 10
Training loss: 2.8797000116830915
Validation loss: 2.6090622679450735

Epoch: 6| Step: 11
Training loss: 2.520178234215659
Validation loss: 2.6058418100443497

Epoch: 6| Step: 12
Training loss: 3.241512660378265
Validation loss: 2.6088277876564945

Epoch: 6| Step: 13
Training loss: 2.479740258120132
Validation loss: 2.621635301230221

Epoch: 114| Step: 0
Training loss: 2.945211636463011
Validation loss: 2.5755449020167083

Epoch: 6| Step: 1
Training loss: 2.5046094362274434
Validation loss: 2.5533816047485174

Epoch: 6| Step: 2
Training loss: 2.756386709823532
Validation loss: 2.5231854692531974

Epoch: 6| Step: 3
Training loss: 2.8662881298165823
Validation loss: 2.507356499162204

Epoch: 6| Step: 4
Training loss: 2.646627071798695
Validation loss: 2.5030534667956514

Epoch: 6| Step: 5
Training loss: 2.717310206322652
Validation loss: 2.4952916220188652

Epoch: 6| Step: 6
Training loss: 2.9572668347061333
Validation loss: 2.4990490940513563

Epoch: 6| Step: 7
Training loss: 3.2142913455005546
Validation loss: 2.4994597989619676

Epoch: 6| Step: 8
Training loss: 2.851548119077607
Validation loss: 2.4912420505891024

Epoch: 6| Step: 9
Training loss: 2.5706636600569137
Validation loss: 2.488419135499032

Epoch: 6| Step: 10
Training loss: 2.9480107340727253
Validation loss: 2.4936122195306916

Epoch: 6| Step: 11
Training loss: 2.4557150998459347
Validation loss: 2.477463249136862

Epoch: 6| Step: 12
Training loss: 2.469248178170669
Validation loss: 2.4997227915001043

Epoch: 6| Step: 13
Training loss: 3.3298408490889218
Validation loss: 2.5028837522717042

Epoch: 115| Step: 0
Training loss: 2.6143048709343213
Validation loss: 2.5089043659356505

Epoch: 6| Step: 1
Training loss: 3.313544054858968
Validation loss: 2.512262672460203

Epoch: 6| Step: 2
Training loss: 2.369349936652341
Validation loss: 2.4978668772263006

Epoch: 6| Step: 3
Training loss: 2.8229314843571314
Validation loss: 2.507035153962925

Epoch: 6| Step: 4
Training loss: 2.824312960371091
Validation loss: 2.503413486964128

Epoch: 6| Step: 5
Training loss: 2.68364563382341
Validation loss: 2.51243629617651

Epoch: 6| Step: 6
Training loss: 2.400081009292347
Validation loss: 2.5187847888949846

Epoch: 6| Step: 7
Training loss: 2.8834462696809036
Validation loss: 2.531746935429386

Epoch: 6| Step: 8
Training loss: 3.014817836439552
Validation loss: 2.537274377308147

Epoch: 6| Step: 9
Training loss: 2.7701749282302743
Validation loss: 2.530979959922878

Epoch: 6| Step: 10
Training loss: 2.185022313333579
Validation loss: 2.5141202084109486

Epoch: 6| Step: 11
Training loss: 3.545278181344544
Validation loss: 2.504849517165373

Epoch: 6| Step: 12
Training loss: 2.2694606540225037
Validation loss: 2.4995368743894684

Epoch: 6| Step: 13
Training loss: 2.9013911823730103
Validation loss: 2.5022774065023756

Epoch: 116| Step: 0
Training loss: 2.9900602343561786
Validation loss: 2.4944375339757685

Epoch: 6| Step: 1
Training loss: 2.439431036917115
Validation loss: 2.499497263617223

Epoch: 6| Step: 2
Training loss: 2.375843400277127
Validation loss: 2.510537712178873

Epoch: 6| Step: 3
Training loss: 3.102771744304343
Validation loss: 2.5164484444337987

Epoch: 6| Step: 4
Training loss: 3.235250898062326
Validation loss: 2.5353238931873885

Epoch: 6| Step: 5
Training loss: 2.6711378363345912
Validation loss: 2.555299152138348

Epoch: 6| Step: 6
Training loss: 2.6347130500858578
Validation loss: 2.55575948361973

Epoch: 6| Step: 7
Training loss: 2.8964153958092003
Validation loss: 2.5377599310980887

Epoch: 6| Step: 8
Training loss: 2.717132876671387
Validation loss: 2.5325187411181376

Epoch: 6| Step: 9
Training loss: 2.5168871822967005
Validation loss: 2.526297617052924

Epoch: 6| Step: 10
Training loss: 2.9626820439930484
Validation loss: 2.51342039090619

Epoch: 6| Step: 11
Training loss: 2.7019029622304296
Validation loss: 2.499404658936434

Epoch: 6| Step: 12
Training loss: 2.9983140658624854
Validation loss: 2.4970881095767687

Epoch: 6| Step: 13
Training loss: 1.896684944397739
Validation loss: 2.493042347567843

Epoch: 117| Step: 0
Training loss: 2.4046798140791523
Validation loss: 2.4975170818773105

Epoch: 6| Step: 1
Training loss: 2.7447094745140705
Validation loss: 2.492483184182074

Epoch: 6| Step: 2
Training loss: 3.0316514211491183
Validation loss: 2.493991164793785

Epoch: 6| Step: 3
Training loss: 2.264116719125201
Validation loss: 2.4867046410633673

Epoch: 6| Step: 4
Training loss: 3.0077140494744112
Validation loss: 2.4810690677496794

Epoch: 6| Step: 5
Training loss: 2.744830127038839
Validation loss: 2.486412796053123

Epoch: 6| Step: 6
Training loss: 3.323569461873109
Validation loss: 2.489876346350374

Epoch: 6| Step: 7
Training loss: 3.1377246426714276
Validation loss: 2.5019017677024573

Epoch: 6| Step: 8
Training loss: 2.378277975969471
Validation loss: 2.521000978500077

Epoch: 6| Step: 9
Training loss: 2.6651762930154477
Validation loss: 2.538775738724161

Epoch: 6| Step: 10
Training loss: 2.153376447503851
Validation loss: 2.543660766668296

Epoch: 6| Step: 11
Training loss: 2.7451211826834143
Validation loss: 2.535847504271257

Epoch: 6| Step: 12
Training loss: 3.184557285650679
Validation loss: 2.5155148658804074

Epoch: 6| Step: 13
Training loss: 2.8544721416718306
Validation loss: 2.515067359539929

Epoch: 118| Step: 0
Training loss: 2.8368248089983004
Validation loss: 2.506115286144877

Epoch: 6| Step: 1
Training loss: 2.2935608289497207
Validation loss: 2.50008071851141

Epoch: 6| Step: 2
Training loss: 2.3097434239100356
Validation loss: 2.4962539376240276

Epoch: 6| Step: 3
Training loss: 2.774867245359698
Validation loss: 2.5045126610244526

Epoch: 6| Step: 4
Training loss: 2.7443753722435376
Validation loss: 2.49683077316627

Epoch: 6| Step: 5
Training loss: 3.0435818604198346
Validation loss: 2.495911474930756

Epoch: 6| Step: 6
Training loss: 2.884636692066237
Validation loss: 2.4976521552436504

Epoch: 6| Step: 7
Training loss: 2.4566503572202976
Validation loss: 2.5003596190759283

Epoch: 6| Step: 8
Training loss: 1.7602413774909536
Validation loss: 2.5027458831413893

Epoch: 6| Step: 9
Training loss: 3.051514521552231
Validation loss: 2.503424692181804

Epoch: 6| Step: 10
Training loss: 2.875558218459335
Validation loss: 2.504468484759585

Epoch: 6| Step: 11
Training loss: 2.8939524701268033
Validation loss: 2.5212107499332577

Epoch: 6| Step: 12
Training loss: 3.2807753446681804
Validation loss: 2.5361802733144576

Epoch: 6| Step: 13
Training loss: 3.1704970500206198
Validation loss: 2.539426957217905

Epoch: 119| Step: 0
Training loss: 2.4880314436578423
Validation loss: 2.5558545791991154

Epoch: 6| Step: 1
Training loss: 2.6492710658459186
Validation loss: 2.563473862325767

Epoch: 6| Step: 2
Training loss: 2.317375919349472
Validation loss: 2.5234128894378545

Epoch: 6| Step: 3
Training loss: 2.8661234281785912
Validation loss: 2.512969157797937

Epoch: 6| Step: 4
Training loss: 3.356068105759054
Validation loss: 2.493752142344249

Epoch: 6| Step: 5
Training loss: 2.4282665942919244
Validation loss: 2.4917725078565613

Epoch: 6| Step: 6
Training loss: 2.971440752235011
Validation loss: 2.493098228833491

Epoch: 6| Step: 7
Training loss: 3.0157584521556178
Validation loss: 2.4883960108797676

Epoch: 6| Step: 8
Training loss: 3.0762661195880616
Validation loss: 2.4907512992878083

Epoch: 6| Step: 9
Training loss: 2.785034690654573
Validation loss: 2.5034580689326567

Epoch: 6| Step: 10
Training loss: 2.483242136498953
Validation loss: 2.4990194858957904

Epoch: 6| Step: 11
Training loss: 2.6769655154732037
Validation loss: 2.506844470559837

Epoch: 6| Step: 12
Training loss: 2.720005403260867
Validation loss: 2.512465929887968

Epoch: 6| Step: 13
Training loss: 3.3397061146713263
Validation loss: 2.5209739111606755

Epoch: 120| Step: 0
Training loss: 3.0816162758299357
Validation loss: 2.5318624747252514

Epoch: 6| Step: 1
Training loss: 2.757567224590985
Validation loss: 2.5301906030253445

Epoch: 6| Step: 2
Training loss: 3.3183024706348117
Validation loss: 2.5439853195443525

Epoch: 6| Step: 3
Training loss: 2.5191398375715934
Validation loss: 2.560920918102208

Epoch: 6| Step: 4
Training loss: 2.720603574094
Validation loss: 2.5746908664770602

Epoch: 6| Step: 5
Training loss: 2.9613226889130293
Validation loss: 2.599574578963936

Epoch: 6| Step: 6
Training loss: 3.206472740839089
Validation loss: 2.617516593096581

Epoch: 6| Step: 7
Training loss: 2.0375849606639296
Validation loss: 2.6112899301950687

Epoch: 6| Step: 8
Training loss: 2.678823259413689
Validation loss: 2.5999633146747456

Epoch: 6| Step: 9
Training loss: 2.3997492977172703
Validation loss: 2.596748205554375

Epoch: 6| Step: 10
Training loss: 3.146244316959818
Validation loss: 2.5818246199606474

Epoch: 6| Step: 11
Training loss: 2.6144639148771636
Validation loss: 2.576301466274158

Epoch: 6| Step: 12
Training loss: 2.620224832832302
Validation loss: 2.5664188074224987

Epoch: 6| Step: 13
Training loss: 2.9679935495455583
Validation loss: 2.5393217241277526

Epoch: 121| Step: 0
Training loss: 2.8977834781084897
Validation loss: 2.523929629360438

Epoch: 6| Step: 1
Training loss: 2.636054603856248
Validation loss: 2.50490244131092

Epoch: 6| Step: 2
Training loss: 2.8703288402310303
Validation loss: 2.5104641779455856

Epoch: 6| Step: 3
Training loss: 2.925696500357997
Validation loss: 2.5090815784741243

Epoch: 6| Step: 4
Training loss: 2.2121234853605354
Validation loss: 2.5341692550912005

Epoch: 6| Step: 5
Training loss: 2.313390199620082
Validation loss: 2.548038198167071

Epoch: 6| Step: 6
Training loss: 2.984334416138485
Validation loss: 2.5777744963569296

Epoch: 6| Step: 7
Training loss: 1.9490401008398999
Validation loss: 2.580119678233215

Epoch: 6| Step: 8
Training loss: 2.8787889806085207
Validation loss: 2.597128898908303

Epoch: 6| Step: 9
Training loss: 3.341619936139869
Validation loss: 2.613989239211563

Epoch: 6| Step: 10
Training loss: 3.100897652884098
Validation loss: 2.6101863396264515

Epoch: 6| Step: 11
Training loss: 2.93296252493421
Validation loss: 2.6404617792120306

Epoch: 6| Step: 12
Training loss: 2.8645060210921267
Validation loss: 2.64112131204849

Epoch: 6| Step: 13
Training loss: 3.0752554748600542
Validation loss: 2.635358286836031

Epoch: 122| Step: 0
Training loss: 2.296345760070221
Validation loss: 2.6339915956231414

Epoch: 6| Step: 1
Training loss: 2.621897635288308
Validation loss: 2.6233623707278526

Epoch: 6| Step: 2
Training loss: 2.6578215494063495
Validation loss: 2.6160568839114675

Epoch: 6| Step: 3
Training loss: 2.8388324180339817
Validation loss: 2.5958274372593313

Epoch: 6| Step: 4
Training loss: 2.914462391923345
Validation loss: 2.5816621541875593

Epoch: 6| Step: 5
Training loss: 3.2032967870283473
Validation loss: 2.572214543347747

Epoch: 6| Step: 6
Training loss: 2.9742730414543432
Validation loss: 2.552780626345628

Epoch: 6| Step: 7
Training loss: 2.6247586638909595
Validation loss: 2.557091059338267

Epoch: 6| Step: 8
Training loss: 3.021821134574373
Validation loss: 2.5475887222327422

Epoch: 6| Step: 9
Training loss: 3.1064948923758786
Validation loss: 2.536184674453351

Epoch: 6| Step: 10
Training loss: 2.6722893644565695
Validation loss: 2.5228968169665063

Epoch: 6| Step: 11
Training loss: 2.7157985407928114
Validation loss: 2.5318355756508497

Epoch: 6| Step: 12
Training loss: 2.605412930143715
Validation loss: 2.517173532511233

Epoch: 6| Step: 13
Training loss: 2.4718627620736484
Validation loss: 2.505351624147263

Epoch: 123| Step: 0
Training loss: 2.0777572650656184
Validation loss: 2.5016472301668298

Epoch: 6| Step: 1
Training loss: 2.6100077518625717
Validation loss: 2.4978625255749525

Epoch: 6| Step: 2
Training loss: 2.487555812803798
Validation loss: 2.504860927816651

Epoch: 6| Step: 3
Training loss: 3.36250563837309
Validation loss: 2.5006653546349424

Epoch: 6| Step: 4
Training loss: 2.1301437281081954
Validation loss: 2.497592272144639

Epoch: 6| Step: 5
Training loss: 2.9806375318260976
Validation loss: 2.496124600668781

Epoch: 6| Step: 6
Training loss: 2.7047936906248493
Validation loss: 2.499832645065061

Epoch: 6| Step: 7
Training loss: 2.9711194671472776
Validation loss: 2.49124252061278

Epoch: 6| Step: 8
Training loss: 3.278216603820208
Validation loss: 2.5091847010407355

Epoch: 6| Step: 9
Training loss: 2.5776782949295436
Validation loss: 2.520951905831075

Epoch: 6| Step: 10
Training loss: 2.478669530781033
Validation loss: 2.5514580435037937

Epoch: 6| Step: 11
Training loss: 2.9250198820041597
Validation loss: 2.564717486585981

Epoch: 6| Step: 12
Training loss: 2.822811467273886
Validation loss: 2.5722407994018486

Epoch: 6| Step: 13
Training loss: 3.0891649583294365
Validation loss: 2.588024634871259

Epoch: 124| Step: 0
Training loss: 2.621858806355496
Validation loss: 2.562311446173127

Epoch: 6| Step: 1
Training loss: 3.132576417470371
Validation loss: 2.552517526309359

Epoch: 6| Step: 2
Training loss: 2.5009794224041633
Validation loss: 2.5293198912331882

Epoch: 6| Step: 3
Training loss: 2.5741817952167714
Validation loss: 2.517368586529948

Epoch: 6| Step: 4
Training loss: 3.2143741232203076
Validation loss: 2.511387514017985

Epoch: 6| Step: 5
Training loss: 2.4977110874908464
Validation loss: 2.5075179210605563

Epoch: 6| Step: 6
Training loss: 2.430693292499518
Validation loss: 2.499984616057941

Epoch: 6| Step: 7
Training loss: 2.5659055642391646
Validation loss: 2.501495104995199

Epoch: 6| Step: 8
Training loss: 2.926204309575282
Validation loss: 2.506438295786142

Epoch: 6| Step: 9
Training loss: 3.108378188258738
Validation loss: 2.500082623746889

Epoch: 6| Step: 10
Training loss: 2.825678320273829
Validation loss: 2.5108786424961256

Epoch: 6| Step: 11
Training loss: 2.423031832143878
Validation loss: 2.5078733166770815

Epoch: 6| Step: 12
Training loss: 2.7353559205940825
Validation loss: 2.515568978127805

Epoch: 6| Step: 13
Training loss: 2.8201676011838535
Validation loss: 2.5169261465977035

Epoch: 125| Step: 0
Training loss: 3.416331360033428
Validation loss: 2.5221517636040134

Epoch: 6| Step: 1
Training loss: 2.653676121133018
Validation loss: 2.528367184851008

Epoch: 6| Step: 2
Training loss: 2.5766181009838984
Validation loss: 2.548275780093041

Epoch: 6| Step: 3
Training loss: 3.187210967479358
Validation loss: 2.54594049925188

Epoch: 6| Step: 4
Training loss: 2.274352423756866
Validation loss: 2.5582489169562823

Epoch: 6| Step: 5
Training loss: 2.462168842646354
Validation loss: 2.5640793117183938

Epoch: 6| Step: 6
Training loss: 2.49847270089092
Validation loss: 2.5846365864201077

Epoch: 6| Step: 7
Training loss: 2.055671717251187
Validation loss: 2.564291431146135

Epoch: 6| Step: 8
Training loss: 3.11909109325888
Validation loss: 2.5559686260350385

Epoch: 6| Step: 9
Training loss: 2.500375814800743
Validation loss: 2.538240840917209

Epoch: 6| Step: 10
Training loss: 2.408729959670806
Validation loss: 2.5115239410977757

Epoch: 6| Step: 11
Training loss: 3.0935280460154835
Validation loss: 2.5141968415454174

Epoch: 6| Step: 12
Training loss: 2.711191654659157
Validation loss: 2.481626338144157

Epoch: 6| Step: 13
Training loss: 3.2515960588973556
Validation loss: 2.4741672599062667

Testing loss: 2.7190724304685836

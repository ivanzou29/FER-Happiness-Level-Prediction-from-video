Epoch: 1| Step: 0
Training loss: 6.344007966006498
Validation loss: 5.789133230309254

Epoch: 6| Step: 1
Training loss: 5.960771110882225
Validation loss: 5.762268793440992

Epoch: 6| Step: 2
Training loss: 4.30564313136356
Validation loss: 5.7360983015508324

Epoch: 6| Step: 3
Training loss: 5.884224325834974
Validation loss: 5.711231098282159

Epoch: 6| Step: 4
Training loss: 7.05217535126989
Validation loss: 5.684429273249457

Epoch: 6| Step: 5
Training loss: 5.764949769382718
Validation loss: 5.6559105091761985

Epoch: 6| Step: 6
Training loss: 5.437881631722504
Validation loss: 5.623268311175425

Epoch: 6| Step: 7
Training loss: 5.739291627351677
Validation loss: 5.587492282617369

Epoch: 6| Step: 8
Training loss: 5.1011317773761
Validation loss: 5.5469591748189675

Epoch: 6| Step: 9
Training loss: 5.101634096633413
Validation loss: 5.502145173609407

Epoch: 6| Step: 10
Training loss: 5.249318033157491
Validation loss: 5.453170786047559

Epoch: 6| Step: 11
Training loss: 5.1407394628332845
Validation loss: 5.401710276712497

Epoch: 6| Step: 12
Training loss: 6.175806883940317
Validation loss: 5.345411173790865

Epoch: 6| Step: 13
Training loss: 4.5316054632261205
Validation loss: 5.284899609827745

Epoch: 2| Step: 0
Training loss: 5.3249233616431555
Validation loss: 5.222625759085235

Epoch: 6| Step: 1
Training loss: 4.2654443499979795
Validation loss: 5.159784579596406

Epoch: 6| Step: 2
Training loss: 4.49937031896819
Validation loss: 5.096222445113802

Epoch: 6| Step: 3
Training loss: 5.348568890690929
Validation loss: 5.033041052646758

Epoch: 6| Step: 4
Training loss: 5.882989199392263
Validation loss: 4.968985987093856

Epoch: 6| Step: 5
Training loss: 4.223086240826836
Validation loss: 4.905578048921376

Epoch: 6| Step: 6
Training loss: 4.387316342935983
Validation loss: 4.843743301582799

Epoch: 6| Step: 7
Training loss: 5.265415085590078
Validation loss: 4.785370836065213

Epoch: 6| Step: 8
Training loss: 5.098636459878268
Validation loss: 4.725829418137346

Epoch: 6| Step: 9
Training loss: 5.4759759738526
Validation loss: 4.67070479784449

Epoch: 6| Step: 10
Training loss: 3.743923859547476
Validation loss: 4.6180402588275475

Epoch: 6| Step: 11
Training loss: 5.0363343889935255
Validation loss: 4.567510427447839

Epoch: 6| Step: 12
Training loss: 4.927861910941262
Validation loss: 4.5170581824272285

Epoch: 6| Step: 13
Training loss: 4.629076424218957
Validation loss: 4.470734925087254

Epoch: 3| Step: 0
Training loss: 4.897297655085891
Validation loss: 4.425568214689848

Epoch: 6| Step: 1
Training loss: 4.604831745333594
Validation loss: 4.381622479872285

Epoch: 6| Step: 2
Training loss: 5.102659145566649
Validation loss: 4.341369901073337

Epoch: 6| Step: 3
Training loss: 4.60655203544619
Validation loss: 4.30229796724579

Epoch: 6| Step: 4
Training loss: 4.481334120467651
Validation loss: 4.269843924161982

Epoch: 6| Step: 5
Training loss: 4.705414173735051
Validation loss: 4.2408557989937865

Epoch: 6| Step: 6
Training loss: 3.0904253425560366
Validation loss: 4.216700361581619

Epoch: 6| Step: 7
Training loss: 5.45232571315884
Validation loss: 4.208784093670083

Epoch: 6| Step: 8
Training loss: 3.561814627054586
Validation loss: 4.179718045067579

Epoch: 6| Step: 9
Training loss: 4.050164848080984
Validation loss: 4.156013334612451

Epoch: 6| Step: 10
Training loss: 4.556922743623971
Validation loss: 4.135524287235971

Epoch: 6| Step: 11
Training loss: 4.647093096029394
Validation loss: 4.118386719533696

Epoch: 6| Step: 12
Training loss: 2.8809335265880645
Validation loss: 4.099300614098873

Epoch: 6| Step: 13
Training loss: 2.829992025788736
Validation loss: 4.076721827579959

Epoch: 4| Step: 0
Training loss: 4.745459544817963
Validation loss: 4.056704222577532

Epoch: 6| Step: 1
Training loss: 3.976197951119181
Validation loss: 4.03319704579279

Epoch: 6| Step: 2
Training loss: 3.5236909694808483
Validation loss: 4.011360458636388

Epoch: 6| Step: 3
Training loss: 3.4449100043026
Validation loss: 3.9936687754898004

Epoch: 6| Step: 4
Training loss: 3.7163355065397377
Validation loss: 3.9771668152064215

Epoch: 6| Step: 5
Training loss: 3.9964459842929307
Validation loss: 3.964155981679026

Epoch: 6| Step: 6
Training loss: 5.0640267789593665
Validation loss: 3.944068291273212

Epoch: 6| Step: 7
Training loss: 3.9326821779612455
Validation loss: 3.924613587076808

Epoch: 6| Step: 8
Training loss: 5.22052940139551
Validation loss: 3.906815210284854

Epoch: 6| Step: 9
Training loss: 4.32512596696639
Validation loss: 3.89061099512517

Epoch: 6| Step: 10
Training loss: 3.610187521631697
Validation loss: 3.8727783068969677

Epoch: 6| Step: 11
Training loss: 3.629573042169674
Validation loss: 3.8568945058338358

Epoch: 6| Step: 12
Training loss: 3.6678946490618034
Validation loss: 3.8418053403901102

Epoch: 6| Step: 13
Training loss: 3.964305401413194
Validation loss: 3.8334591355798624

Epoch: 5| Step: 0
Training loss: 4.2524835398521965
Validation loss: 3.8217380867718584

Epoch: 6| Step: 1
Training loss: 4.472755184495312
Validation loss: 3.8037360568039023

Epoch: 6| Step: 2
Training loss: 4.180996527636842
Validation loss: 3.784970105026959

Epoch: 6| Step: 3
Training loss: 4.118966053229409
Validation loss: 3.7791099448479404

Epoch: 6| Step: 4
Training loss: 3.2421386715073126
Validation loss: 3.7685958396240142

Epoch: 6| Step: 5
Training loss: 3.4685231727462815
Validation loss: 3.7551382213955793

Epoch: 6| Step: 6
Training loss: 4.325023214465335
Validation loss: 3.736558878787877

Epoch: 6| Step: 7
Training loss: 4.463035718032578
Validation loss: 3.720602240010371

Epoch: 6| Step: 8
Training loss: 3.0989021018346756
Validation loss: 3.7108696152976646

Epoch: 6| Step: 9
Training loss: 4.0887000699144895
Validation loss: 3.7023217111531697

Epoch: 6| Step: 10
Training loss: 4.1212555044529084
Validation loss: 3.697375003995751

Epoch: 6| Step: 11
Training loss: 3.8008505020027057
Validation loss: 3.6866392379270296

Epoch: 6| Step: 12
Training loss: 3.7039432185045387
Validation loss: 3.673894571434815

Epoch: 6| Step: 13
Training loss: 2.6154874375788566
Validation loss: 3.6619456840656417

Epoch: 6| Step: 0
Training loss: 4.6950705873750795
Validation loss: 3.654268386694786

Epoch: 6| Step: 1
Training loss: 5.015933588820336
Validation loss: 3.6414403416543424

Epoch: 6| Step: 2
Training loss: 3.4861773973894867
Validation loss: 3.6250567065801103

Epoch: 6| Step: 3
Training loss: 3.1694314498553258
Validation loss: 3.6117415773622112

Epoch: 6| Step: 4
Training loss: 3.8198702503557924
Validation loss: 3.598317916765581

Epoch: 6| Step: 5
Training loss: 4.157769405882813
Validation loss: 3.575762311000151

Epoch: 6| Step: 6
Training loss: 3.644514764582892
Validation loss: 3.553509735292027

Epoch: 6| Step: 7
Training loss: 4.222895415217645
Validation loss: 3.5398192614025428

Epoch: 6| Step: 8
Training loss: 3.1458026471862386
Validation loss: 3.5215108595405153

Epoch: 6| Step: 9
Training loss: 3.360163964176436
Validation loss: 3.5115361880342517

Epoch: 6| Step: 10
Training loss: 3.0011349756330143
Validation loss: 3.5026487342215264

Epoch: 6| Step: 11
Training loss: 3.5620119530178984
Validation loss: 3.494713803746442

Epoch: 6| Step: 12
Training loss: 3.290641496260141
Validation loss: 3.4859254230295527

Epoch: 6| Step: 13
Training loss: 3.6142183645425576
Validation loss: 3.473927465333282

Epoch: 7| Step: 0
Training loss: 4.737416112152038
Validation loss: 3.4628339961242594

Epoch: 6| Step: 1
Training loss: 3.6437498115552587
Validation loss: 3.451559768944775

Epoch: 6| Step: 2
Training loss: 3.6611863420083117
Validation loss: 3.443002220461391

Epoch: 6| Step: 3
Training loss: 3.034188802488723
Validation loss: 3.434981036791465

Epoch: 6| Step: 4
Training loss: 4.021370069556907
Validation loss: 3.430595194677103

Epoch: 6| Step: 5
Training loss: 3.4865071568614616
Validation loss: 3.4273873476178283

Epoch: 6| Step: 6
Training loss: 4.3318313538753985
Validation loss: 3.419239674066558

Epoch: 6| Step: 7
Training loss: 3.8780715828043943
Validation loss: 3.414334090461478

Epoch: 6| Step: 8
Training loss: 2.905859480020834
Validation loss: 3.407917846809313

Epoch: 6| Step: 9
Training loss: 3.8436254465655635
Validation loss: 3.402234045745421

Epoch: 6| Step: 10
Training loss: 3.3245179594154077
Validation loss: 3.394653559026347

Epoch: 6| Step: 11
Training loss: 3.4207477541838576
Validation loss: 3.387588207887167

Epoch: 6| Step: 12
Training loss: 3.010634012967066
Validation loss: 3.385071869638536

Epoch: 6| Step: 13
Training loss: 2.986687687845276
Validation loss: 3.3803000344867264

Epoch: 8| Step: 0
Training loss: 2.7546558847847225
Validation loss: 3.3761953487740723

Epoch: 6| Step: 1
Training loss: 3.2084357175137495
Validation loss: 3.370581176636239

Epoch: 6| Step: 2
Training loss: 3.6634113570961793
Validation loss: 3.3677795664692907

Epoch: 6| Step: 3
Training loss: 3.9438215101139775
Validation loss: 3.361649958851857

Epoch: 6| Step: 4
Training loss: 2.901286819664843
Validation loss: 3.359486241178915

Epoch: 6| Step: 5
Training loss: 3.871408213017435
Validation loss: 3.35320764669579

Epoch: 6| Step: 6
Training loss: 4.619108829832432
Validation loss: 3.3490731640930234

Epoch: 6| Step: 7
Training loss: 4.436129721104402
Validation loss: 3.344876836771151

Epoch: 6| Step: 8
Training loss: 2.909646623267169
Validation loss: 3.3404994472308194

Epoch: 6| Step: 9
Training loss: 3.4284487770485508
Validation loss: 3.336230073518611

Epoch: 6| Step: 10
Training loss: 3.811128463433753
Validation loss: 3.333663873793782

Epoch: 6| Step: 11
Training loss: 3.3306073168233077
Validation loss: 3.331837136560931

Epoch: 6| Step: 12
Training loss: 3.1428116355425226
Validation loss: 3.32792864427326

Epoch: 6| Step: 13
Training loss: 3.6550708401725522
Validation loss: 3.323364090876881

Epoch: 9| Step: 0
Training loss: 4.099462818217677
Validation loss: 3.3195510274836173

Epoch: 6| Step: 1
Training loss: 2.99380234451564
Validation loss: 3.313711582911686

Epoch: 6| Step: 2
Training loss: 4.694920071215021
Validation loss: 3.3130596397832077

Epoch: 6| Step: 3
Training loss: 3.2573492146137517
Validation loss: 3.3026978335768966

Epoch: 6| Step: 4
Training loss: 2.9789974635876284
Validation loss: 3.305756254633263

Epoch: 6| Step: 5
Training loss: 3.6702157674865963
Validation loss: 3.3030210709561914

Epoch: 6| Step: 6
Training loss: 2.992111324605588
Validation loss: 3.2953250198119814

Epoch: 6| Step: 7
Training loss: 4.013747433139167
Validation loss: 3.292616306226269

Epoch: 6| Step: 8
Training loss: 3.4511903892931515
Validation loss: 3.2890333891898

Epoch: 6| Step: 9
Training loss: 3.2788902880389417
Validation loss: 3.2844350924434527

Epoch: 6| Step: 10
Training loss: 2.9397039059343086
Validation loss: 3.283692975104679

Epoch: 6| Step: 11
Training loss: 3.7570175630729064
Validation loss: 3.2775434491135447

Epoch: 6| Step: 12
Training loss: 3.7927482375878294
Validation loss: 3.2742896774379617

Epoch: 6| Step: 13
Training loss: 2.8593768343893533
Validation loss: 3.2729258200891853

Epoch: 10| Step: 0
Training loss: 4.042305859756082
Validation loss: 3.29099709609137

Epoch: 6| Step: 1
Training loss: 3.816919235690248
Validation loss: 3.2662878810639846

Epoch: 6| Step: 2
Training loss: 3.065506218680696
Validation loss: 3.2642550846362495

Epoch: 6| Step: 3
Training loss: 2.7941076130878755
Validation loss: 3.2619601134786005

Epoch: 6| Step: 4
Training loss: 4.343368403736457
Validation loss: 3.2598073521638233

Epoch: 6| Step: 5
Training loss: 3.0584213188743496
Validation loss: 3.2732926802490283

Epoch: 6| Step: 6
Training loss: 3.658011509346936
Validation loss: 3.2551933866424325

Epoch: 6| Step: 7
Training loss: 3.5297837715816383
Validation loss: 3.2605445037619587

Epoch: 6| Step: 8
Training loss: 2.9677684566735927
Validation loss: 3.262026708540429

Epoch: 6| Step: 9
Training loss: 3.374138050920549
Validation loss: 3.2615961389877337

Epoch: 6| Step: 10
Training loss: 3.4308965893568333
Validation loss: 3.2548566268754104

Epoch: 6| Step: 11
Training loss: 3.5816529052337462
Validation loss: 3.2487700764204135

Epoch: 6| Step: 12
Training loss: 3.5687319138798492
Validation loss: 3.247489375964059

Epoch: 6| Step: 13
Training loss: 3.642037702873268
Validation loss: 3.2450176428886546

Epoch: 11| Step: 0
Training loss: 3.917532169134494
Validation loss: 3.2437978114684345

Epoch: 6| Step: 1
Training loss: 3.1644112100475206
Validation loss: 3.2450960107069498

Epoch: 6| Step: 2
Training loss: 3.295373800837115
Validation loss: 3.2447237581689468

Epoch: 6| Step: 3
Training loss: 3.5832554712633686
Validation loss: 3.2397494405902005

Epoch: 6| Step: 4
Training loss: 2.5019815221480775
Validation loss: 3.23239827378101

Epoch: 6| Step: 5
Training loss: 4.023490594309871
Validation loss: 3.229707806061591

Epoch: 6| Step: 6
Training loss: 3.7055129709486447
Validation loss: 3.2275680639232296

Epoch: 6| Step: 7
Training loss: 3.725910121667082
Validation loss: 3.225322705780421

Epoch: 6| Step: 8
Training loss: 2.966717315270052
Validation loss: 3.22522994411931

Epoch: 6| Step: 9
Training loss: 4.068867549575229
Validation loss: 3.223724918714912

Epoch: 6| Step: 10
Training loss: 2.514542435569207
Validation loss: 3.222111106050967

Epoch: 6| Step: 11
Training loss: 3.907829880704363
Validation loss: 3.220315802666009

Epoch: 6| Step: 12
Training loss: 3.2932962818094995
Validation loss: 3.2197498855642586

Epoch: 6| Step: 13
Training loss: 3.735497980177758
Validation loss: 3.219547894834427

Epoch: 12| Step: 0
Training loss: 3.7965293695566547
Validation loss: 3.218762046599472

Epoch: 6| Step: 1
Training loss: 3.185410356899987
Validation loss: 3.2164946285707114

Epoch: 6| Step: 2
Training loss: 3.205034087144138
Validation loss: 3.216342553369012

Epoch: 6| Step: 3
Training loss: 3.9120725648074752
Validation loss: 3.2164141929566425

Epoch: 6| Step: 4
Training loss: 2.6275526078871105
Validation loss: 3.2140685461761156

Epoch: 6| Step: 5
Training loss: 3.281389215331689
Validation loss: 3.213721562757024

Epoch: 6| Step: 6
Training loss: 3.9017713583276525
Validation loss: 3.2147763545641435

Epoch: 6| Step: 7
Training loss: 3.796842096131218
Validation loss: 3.2137880246674366

Epoch: 6| Step: 8
Training loss: 3.462509452724292
Validation loss: 3.207883077434577

Epoch: 6| Step: 9
Training loss: 4.032745320567199
Validation loss: 3.2109895765876337

Epoch: 6| Step: 10
Training loss: 3.145260706104153
Validation loss: 3.2138289735691585

Epoch: 6| Step: 11
Training loss: 3.6263642210462095
Validation loss: 3.216331232605933

Epoch: 6| Step: 12
Training loss: 2.939781114902951
Validation loss: 3.2091501908993463

Epoch: 6| Step: 13
Training loss: 3.385997092546002
Validation loss: 3.2124573441202373

Epoch: 13| Step: 0
Training loss: 3.5578499947416287
Validation loss: 3.2053610067641864

Epoch: 6| Step: 1
Training loss: 3.732301910724186
Validation loss: 3.200149632488196

Epoch: 6| Step: 2
Training loss: 3.7651782265434286
Validation loss: 3.2009367576640124

Epoch: 6| Step: 3
Training loss: 3.4035420984494915
Validation loss: 3.202718553384856

Epoch: 6| Step: 4
Training loss: 3.4926585449407805
Validation loss: 3.2041574962529595

Epoch: 6| Step: 5
Training loss: 3.547338514248107
Validation loss: 3.205337012740811

Epoch: 6| Step: 6
Training loss: 3.395619734772831
Validation loss: 3.202796738734491

Epoch: 6| Step: 7
Training loss: 3.186123270153675
Validation loss: 3.1983758205891255

Epoch: 6| Step: 8
Training loss: 3.32948468869228
Validation loss: 3.1938560896061507

Epoch: 6| Step: 9
Training loss: 2.4171318614945383
Validation loss: 3.193637555229163

Epoch: 6| Step: 10
Training loss: 3.7003419614974833
Validation loss: 3.190807615383737

Epoch: 6| Step: 11
Training loss: 3.9189477459966278
Validation loss: 3.191332916358278

Epoch: 6| Step: 12
Training loss: 3.4986812286868636
Validation loss: 3.1909091162775134

Epoch: 6| Step: 13
Training loss: 3.1409917968331973
Validation loss: 3.190441150793407

Epoch: 14| Step: 0
Training loss: 3.9813011130221323
Validation loss: 3.1886921856251287

Epoch: 6| Step: 1
Training loss: 3.8036643029073196
Validation loss: 3.188913333204536

Epoch: 6| Step: 2
Training loss: 3.2526170024308048
Validation loss: 3.18589450891445

Epoch: 6| Step: 3
Training loss: 3.6628030593004457
Validation loss: 3.1840203804684672

Epoch: 6| Step: 4
Training loss: 3.8045150435761026
Validation loss: 3.1828625100242696

Epoch: 6| Step: 5
Training loss: 3.0106420905554874
Validation loss: 3.1813186880172624

Epoch: 6| Step: 6
Training loss: 2.7446427615538247
Validation loss: 3.178560900163356

Epoch: 6| Step: 7
Training loss: 2.931964284577839
Validation loss: 3.1799293029649336

Epoch: 6| Step: 8
Training loss: 3.1760950437877096
Validation loss: 3.1875245819012545

Epoch: 6| Step: 9
Training loss: 3.839573521773162
Validation loss: 3.226960824000681

Epoch: 6| Step: 10
Training loss: 3.270137024321292
Validation loss: 3.181102753677842

Epoch: 6| Step: 11
Training loss: 3.774688071190789
Validation loss: 3.1785267445689396

Epoch: 6| Step: 12
Training loss: 3.0575378076269732
Validation loss: 3.191785681503006

Epoch: 6| Step: 13
Training loss: 3.9295446775658167
Validation loss: 3.1947693585666763

Epoch: 15| Step: 0
Training loss: 3.2224641869850728
Validation loss: 3.18567102966826

Epoch: 6| Step: 1
Training loss: 3.0473349199654636
Validation loss: 3.18202826569884

Epoch: 6| Step: 2
Training loss: 3.8186715625255605
Validation loss: 3.1784379744401705

Epoch: 6| Step: 3
Training loss: 4.096645583327949
Validation loss: 3.179120026068923

Epoch: 6| Step: 4
Training loss: 3.1661068940229864
Validation loss: 3.1871154025525117

Epoch: 6| Step: 5
Training loss: 3.325941488874173
Validation loss: 3.1729725360802776

Epoch: 6| Step: 6
Training loss: 3.2755884646478237
Validation loss: 3.1720661530259684

Epoch: 6| Step: 7
Training loss: 3.202677179061488
Validation loss: 3.177208708991297

Epoch: 6| Step: 8
Training loss: 3.077161919053695
Validation loss: 3.184832557677042

Epoch: 6| Step: 9
Training loss: 3.79191183338601
Validation loss: 3.196045291957292

Epoch: 6| Step: 10
Training loss: 3.4994191641455528
Validation loss: 3.1673134794627402

Epoch: 6| Step: 11
Training loss: 3.3753780577100847
Validation loss: 3.1610377000237215

Epoch: 6| Step: 12
Training loss: 3.708999288246743
Validation loss: 3.1602802875654814

Epoch: 6| Step: 13
Training loss: 3.354134095470172
Validation loss: 3.16521129021922

Epoch: 16| Step: 0
Training loss: 2.9103890754704387
Validation loss: 3.1896369120094423

Epoch: 6| Step: 1
Training loss: 3.7760036422744254
Validation loss: 3.2172167087183183

Epoch: 6| Step: 2
Training loss: 3.517545861959899
Validation loss: 3.197186873800762

Epoch: 6| Step: 3
Training loss: 3.4469159259597215
Validation loss: 3.1644048325579726

Epoch: 6| Step: 4
Training loss: 3.4355107880779534
Validation loss: 3.155997595273547

Epoch: 6| Step: 5
Training loss: 3.0867195780141574
Validation loss: 3.1554936173080863

Epoch: 6| Step: 6
Training loss: 3.5070427472696646
Validation loss: 3.15984188104027

Epoch: 6| Step: 7
Training loss: 3.914340254927726
Validation loss: 3.155431920218269

Epoch: 6| Step: 8
Training loss: 4.01416559083267
Validation loss: 3.155592559242866

Epoch: 6| Step: 9
Training loss: 2.738501350819002
Validation loss: 3.15238723412807

Epoch: 6| Step: 10
Training loss: 2.3159862735848833
Validation loss: 3.1493401336450977

Epoch: 6| Step: 11
Training loss: 3.7699224871855397
Validation loss: 3.1487466113872253

Epoch: 6| Step: 12
Training loss: 3.597601642670531
Validation loss: 3.144482982410017

Epoch: 6| Step: 13
Training loss: 3.617915630177144
Validation loss: 3.1429230121995495

Epoch: 17| Step: 0
Training loss: 4.012989886554642
Validation loss: 3.13994303856799

Epoch: 6| Step: 1
Training loss: 3.550057348943272
Validation loss: 3.1383397548887078

Epoch: 6| Step: 2
Training loss: 3.1690044473970165
Validation loss: 3.1360840783957995

Epoch: 6| Step: 3
Training loss: 3.172659941264628
Validation loss: 3.1375888561685565

Epoch: 6| Step: 4
Training loss: 3.6586040515416576
Validation loss: 3.136731048708213

Epoch: 6| Step: 5
Training loss: 2.98406565396079
Validation loss: 3.1311603688790615

Epoch: 6| Step: 6
Training loss: 3.6150453622169434
Validation loss: 3.1286549284106893

Epoch: 6| Step: 7
Training loss: 2.7250814268212973
Validation loss: 3.1285842816498386

Epoch: 6| Step: 8
Training loss: 3.511352384167089
Validation loss: 3.126851825651888

Epoch: 6| Step: 9
Training loss: 3.211162030242737
Validation loss: 3.1236705125937734

Epoch: 6| Step: 10
Training loss: 3.100563945542086
Validation loss: 3.1248360211408777

Epoch: 6| Step: 11
Training loss: 3.601206598467207
Validation loss: 3.1268968629293363

Epoch: 6| Step: 12
Training loss: 3.460164485395342
Validation loss: 3.1285670327029402

Epoch: 6| Step: 13
Training loss: 3.860475082216255
Validation loss: 3.1257836489341657

Epoch: 18| Step: 0
Training loss: 2.964798234632927
Validation loss: 3.121179761672601

Epoch: 6| Step: 1
Training loss: 3.4293243285437143
Validation loss: 3.120517022213968

Epoch: 6| Step: 2
Training loss: 3.893742794317984
Validation loss: 3.118545435885845

Epoch: 6| Step: 3
Training loss: 3.6814393491897563
Validation loss: 3.1195281115650295

Epoch: 6| Step: 4
Training loss: 2.097921400876151
Validation loss: 3.117581179613871

Epoch: 6| Step: 5
Training loss: 3.3877621707603334
Validation loss: 3.11627922567256

Epoch: 6| Step: 6
Training loss: 3.16315799547438
Validation loss: 3.114960262482496

Epoch: 6| Step: 7
Training loss: 3.417022888098007
Validation loss: 3.1153188195029515

Epoch: 6| Step: 8
Training loss: 3.4193172563053547
Validation loss: 3.1143814061514488

Epoch: 6| Step: 9
Training loss: 3.734226495192146
Validation loss: 3.1126480046264025

Epoch: 6| Step: 10
Training loss: 3.6932519567271798
Validation loss: 3.112198076011341

Epoch: 6| Step: 11
Training loss: 3.646999165465058
Validation loss: 3.1120283052499684

Epoch: 6| Step: 12
Training loss: 2.50334668267251
Validation loss: 3.1105867818472266

Epoch: 6| Step: 13
Training loss: 4.237061496427886
Validation loss: 3.1101370578590624

Epoch: 19| Step: 0
Training loss: 3.258049606367102
Validation loss: 3.1098018902925513

Epoch: 6| Step: 1
Training loss: 3.8245492856628682
Validation loss: 3.108399050286566

Epoch: 6| Step: 2
Training loss: 3.079476771313366
Validation loss: 3.106871790087491

Epoch: 6| Step: 3
Training loss: 2.802133741717145
Validation loss: 3.1040843257043673

Epoch: 6| Step: 4
Training loss: 3.6271784583510067
Validation loss: 3.1073069066844425

Epoch: 6| Step: 5
Training loss: 3.1883468998807976
Validation loss: 3.13217755170141

Epoch: 6| Step: 6
Training loss: 3.437534540176256
Validation loss: 3.1197373210520487

Epoch: 6| Step: 7
Training loss: 3.2043496372743707
Validation loss: 3.1009601687892188

Epoch: 6| Step: 8
Training loss: 3.48088932600247
Validation loss: 3.1104821109727134

Epoch: 6| Step: 9
Training loss: 4.004162529907842
Validation loss: 3.1449860560021015

Epoch: 6| Step: 10
Training loss: 3.107740576493426
Validation loss: 3.1434354998400384

Epoch: 6| Step: 11
Training loss: 3.5431336151088293
Validation loss: 3.154138780657189

Epoch: 6| Step: 12
Training loss: 3.7808091087874094
Validation loss: 3.1206436441525054

Epoch: 6| Step: 13
Training loss: 2.6981086711729647
Validation loss: 3.110652639450726

Epoch: 20| Step: 0
Training loss: 3.21461087353311
Validation loss: 3.10733617553029

Epoch: 6| Step: 1
Training loss: 3.3715711058951334
Validation loss: 3.109021248381352

Epoch: 6| Step: 2
Training loss: 3.5437642038319703
Validation loss: 3.1095619205251723

Epoch: 6| Step: 3
Training loss: 3.3316080555826115
Validation loss: 3.114364672867999

Epoch: 6| Step: 4
Training loss: 3.519554823988059
Validation loss: 3.1148412547396047

Epoch: 6| Step: 5
Training loss: 3.623139792019437
Validation loss: 3.113091912646109

Epoch: 6| Step: 6
Training loss: 3.384282521739384
Validation loss: 3.103972400357364

Epoch: 6| Step: 7
Training loss: 3.404985945752417
Validation loss: 3.108720377107308

Epoch: 6| Step: 8
Training loss: 3.6627012542528328
Validation loss: 3.1367777339956513

Epoch: 6| Step: 9
Training loss: 3.205587194394228
Validation loss: 3.0985525986382534

Epoch: 6| Step: 10
Training loss: 2.9390125033127887
Validation loss: 3.097756358154793

Epoch: 6| Step: 11
Training loss: 3.2742506669640834
Validation loss: 3.0948106214084827

Epoch: 6| Step: 12
Training loss: 3.344177646563487
Validation loss: 3.0944392295341254

Epoch: 6| Step: 13
Training loss: 3.6009420327963584
Validation loss: 3.091859302125402

Epoch: 21| Step: 0
Training loss: 4.154682064447366
Validation loss: 3.0885916114123217

Epoch: 6| Step: 1
Training loss: 3.1115043671638776
Validation loss: 3.088188924987366

Epoch: 6| Step: 2
Training loss: 2.8492062383512478
Validation loss: 3.082747077471196

Epoch: 6| Step: 3
Training loss: 3.0826854712507803
Validation loss: 3.0827931665682344

Epoch: 6| Step: 4
Training loss: 3.683614461765738
Validation loss: 3.079929986719183

Epoch: 6| Step: 5
Training loss: 2.535610070188149
Validation loss: 3.0770379443524396

Epoch: 6| Step: 6
Training loss: 3.5007268968247645
Validation loss: 3.0762233645311037

Epoch: 6| Step: 7
Training loss: 3.9064782648149574
Validation loss: 3.070934327976893

Epoch: 6| Step: 8
Training loss: 2.9618487000288845
Validation loss: 3.0672334108091763

Epoch: 6| Step: 9
Training loss: 3.381623021560613
Validation loss: 3.068111081877378

Epoch: 6| Step: 10
Training loss: 3.0886845590474032
Validation loss: 3.070783439397088

Epoch: 6| Step: 11
Training loss: 3.559463947313432
Validation loss: 3.071976475755672

Epoch: 6| Step: 12
Training loss: 3.124961395025217
Validation loss: 3.0729485934382033

Epoch: 6| Step: 13
Training loss: 4.009312518161453
Validation loss: 3.0707775086237135

Epoch: 22| Step: 0
Training loss: 3.1593356015911707
Validation loss: 3.0703215018749477

Epoch: 6| Step: 1
Training loss: 3.7583381776856006
Validation loss: 3.0670060037801123

Epoch: 6| Step: 2
Training loss: 2.928595499610999
Validation loss: 3.059766567888618

Epoch: 6| Step: 3
Training loss: 2.561425332602352
Validation loss: 3.0577373157126098

Epoch: 6| Step: 4
Training loss: 3.7800137098760085
Validation loss: 3.053386248995942

Epoch: 6| Step: 5
Training loss: 3.60593451800902
Validation loss: 3.0507451806169534

Epoch: 6| Step: 6
Training loss: 4.094088241930655
Validation loss: 3.053176088445502

Epoch: 6| Step: 7
Training loss: 3.0318307222000658
Validation loss: 3.0531648369333655

Epoch: 6| Step: 8
Training loss: 3.33503142655191
Validation loss: 3.0502398728255837

Epoch: 6| Step: 9
Training loss: 3.0415803731583084
Validation loss: 3.0508838062963926

Epoch: 6| Step: 10
Training loss: 2.5299588901964363
Validation loss: 3.047412462477764

Epoch: 6| Step: 11
Training loss: 3.509076203069719
Validation loss: 3.047202019486725

Epoch: 6| Step: 12
Training loss: 3.616083293807866
Validation loss: 3.048121960474321

Epoch: 6| Step: 13
Training loss: 3.523173861568272
Validation loss: 3.0427846760090254

Epoch: 23| Step: 0
Training loss: 2.9055489955979596
Validation loss: 3.036652129164269

Epoch: 6| Step: 1
Training loss: 3.5486064163512054
Validation loss: 3.0344018596620685

Epoch: 6| Step: 2
Training loss: 3.3212409662332742
Validation loss: 3.0334910160320825

Epoch: 6| Step: 3
Training loss: 3.13273761307783
Validation loss: 3.0289991500684788

Epoch: 6| Step: 4
Training loss: 3.175270405465832
Validation loss: 3.028627423807044

Epoch: 6| Step: 5
Training loss: 2.81868589205698
Validation loss: 3.0267661952053078

Epoch: 6| Step: 6
Training loss: 3.7319110738415944
Validation loss: 3.026813038470594

Epoch: 6| Step: 7
Training loss: 3.673307488403048
Validation loss: 3.025517816715974

Epoch: 6| Step: 8
Training loss: 3.4257149945488683
Validation loss: 3.0234772045575347

Epoch: 6| Step: 9
Training loss: 3.1926789938411977
Validation loss: 3.0237719360478863

Epoch: 6| Step: 10
Training loss: 3.3203147977933227
Validation loss: 3.021699843211791

Epoch: 6| Step: 11
Training loss: 2.7562352497977716
Validation loss: 3.0206412729913557

Epoch: 6| Step: 12
Training loss: 3.4973221480200034
Validation loss: 3.0188977963982495

Epoch: 6| Step: 13
Training loss: 4.103545837436376
Validation loss: 3.0193964456323585

Epoch: 24| Step: 0
Training loss: 2.777205280012243
Validation loss: 3.0195148502965723

Epoch: 6| Step: 1
Training loss: 3.9676403987587263
Validation loss: 3.0171529299690802

Epoch: 6| Step: 2
Training loss: 3.2500026409431877
Validation loss: 3.0180354923068506

Epoch: 6| Step: 3
Training loss: 3.8441940919460262
Validation loss: 3.017224190816228

Epoch: 6| Step: 4
Training loss: 3.461226957112899
Validation loss: 3.0146373636416834

Epoch: 6| Step: 5
Training loss: 2.9270575760944597
Validation loss: 3.0142055415887894

Epoch: 6| Step: 6
Training loss: 3.6073563120057295
Validation loss: 3.0132077068277763

Epoch: 6| Step: 7
Training loss: 3.1450155513118156
Validation loss: 3.014157410305149

Epoch: 6| Step: 8
Training loss: 3.3587285329106438
Validation loss: 3.0112160871198657

Epoch: 6| Step: 9
Training loss: 3.4146099568217982
Validation loss: 3.0106666518771714

Epoch: 6| Step: 10
Training loss: 2.5837203833105704
Validation loss: 3.010431555261139

Epoch: 6| Step: 11
Training loss: 3.3344128450142603
Validation loss: 3.0090793762278705

Epoch: 6| Step: 12
Training loss: 3.2679832011319854
Validation loss: 3.0124634565682182

Epoch: 6| Step: 13
Training loss: 2.850371647745858
Validation loss: 3.01201453845302

Epoch: 25| Step: 0
Training loss: 3.423493163832821
Validation loss: 3.0172393896497534

Epoch: 6| Step: 1
Training loss: 2.689820374656773
Validation loss: 3.0182907102606116

Epoch: 6| Step: 2
Training loss: 3.620528916766996
Validation loss: 3.0192101185009976

Epoch: 6| Step: 3
Training loss: 3.3956274582565906
Validation loss: 3.019327394407681

Epoch: 6| Step: 4
Training loss: 3.055016697464934
Validation loss: 3.0131108645302445

Epoch: 6| Step: 5
Training loss: 3.023985978088971
Validation loss: 3.0147734020915284

Epoch: 6| Step: 6
Training loss: 3.339276895882989
Validation loss: 3.0112620876620997

Epoch: 6| Step: 7
Training loss: 3.162507756039001
Validation loss: 3.007752140985983

Epoch: 6| Step: 8
Training loss: 3.2340356031355983
Validation loss: 3.003173016629779

Epoch: 6| Step: 9
Training loss: 3.438868025013032
Validation loss: 2.9983327532827784

Epoch: 6| Step: 10
Training loss: 3.750290922959969
Validation loss: 2.9998945914998054

Epoch: 6| Step: 11
Training loss: 3.0027370682633947
Validation loss: 2.9992888515264426

Epoch: 6| Step: 12
Training loss: 3.547493363979765
Validation loss: 2.998517709369407

Epoch: 6| Step: 13
Training loss: 3.3041801333704197
Validation loss: 2.9975656952508167

Epoch: 26| Step: 0
Training loss: 4.097837780476144
Validation loss: 2.9980181382511466

Epoch: 6| Step: 1
Training loss: 3.0256446844988156
Validation loss: 2.994350120805165

Epoch: 6| Step: 2
Training loss: 3.492572259722035
Validation loss: 2.9948613703437528

Epoch: 6| Step: 3
Training loss: 3.241186515989647
Validation loss: 2.994749269903191

Epoch: 6| Step: 4
Training loss: 3.3231582474063264
Validation loss: 2.996948905643547

Epoch: 6| Step: 5
Training loss: 3.586717574763952
Validation loss: 2.996599930847087

Epoch: 6| Step: 6
Training loss: 2.413276914899544
Validation loss: 2.9969752686199373

Epoch: 6| Step: 7
Training loss: 3.187616682721873
Validation loss: 2.997805590972904

Epoch: 6| Step: 8
Training loss: 3.9745072785182605
Validation loss: 2.994830514335755

Epoch: 6| Step: 9
Training loss: 3.2865794180173187
Validation loss: 2.9935839246071523

Epoch: 6| Step: 10
Training loss: 3.1149738453950326
Validation loss: 2.9937958142432444

Epoch: 6| Step: 11
Training loss: 3.135369605707131
Validation loss: 2.9922401742521925

Epoch: 6| Step: 12
Training loss: 2.67862415625128
Validation loss: 2.992816788727081

Epoch: 6| Step: 13
Training loss: 2.8812339203027957
Validation loss: 2.9940985415314776

Epoch: 27| Step: 0
Training loss: 2.507686053264212
Validation loss: 2.9950543056350374

Epoch: 6| Step: 1
Training loss: 2.2623048856284447
Validation loss: 2.9891496477892145

Epoch: 6| Step: 2
Training loss: 3.5244558685436145
Validation loss: 2.9905770658172894

Epoch: 6| Step: 3
Training loss: 3.040154817303123
Validation loss: 2.993433384912713

Epoch: 6| Step: 4
Training loss: 3.120442232922107
Validation loss: 2.9933575495433216

Epoch: 6| Step: 5
Training loss: 3.0613860615111492
Validation loss: 2.9913683808718847

Epoch: 6| Step: 6
Training loss: 2.919403826326854
Validation loss: 2.993342520640806

Epoch: 6| Step: 7
Training loss: 3.758791726543288
Validation loss: 3.0132814652814655

Epoch: 6| Step: 8
Training loss: 2.903760274276916
Validation loss: 3.0137156195431074

Epoch: 6| Step: 9
Training loss: 4.227154984610337
Validation loss: 3.000665356743741

Epoch: 6| Step: 10
Training loss: 3.6751888823464367
Validation loss: 2.988408060940099

Epoch: 6| Step: 11
Training loss: 3.4589548739742804
Validation loss: 2.98411780663654

Epoch: 6| Step: 12
Training loss: 3.510604868446234
Validation loss: 2.983237895778436

Epoch: 6| Step: 13
Training loss: 3.534774236956301
Validation loss: 2.9821533221468015

Epoch: 28| Step: 0
Training loss: 3.9485605296934265
Validation loss: 2.9794031249134005

Epoch: 6| Step: 1
Training loss: 4.064010104699976
Validation loss: 2.981815796734447

Epoch: 6| Step: 2
Training loss: 2.682607322316453
Validation loss: 2.9815102780365144

Epoch: 6| Step: 3
Training loss: 2.998350007578224
Validation loss: 2.98334151162792

Epoch: 6| Step: 4
Training loss: 3.5862360146463566
Validation loss: 2.9800310717351173

Epoch: 6| Step: 5
Training loss: 3.5620063305840843
Validation loss: 2.9797944906765554

Epoch: 6| Step: 6
Training loss: 3.147148834103113
Validation loss: 2.978342400188197

Epoch: 6| Step: 7
Training loss: 2.5351412027673628
Validation loss: 2.9782438457402485

Epoch: 6| Step: 8
Training loss: 3.161742313373555
Validation loss: 2.978933608489994

Epoch: 6| Step: 9
Training loss: 2.8235449191560495
Validation loss: 2.977501808816805

Epoch: 6| Step: 10
Training loss: 3.608318223694022
Validation loss: 2.9802320746535784

Epoch: 6| Step: 11
Training loss: 3.536835700432038
Validation loss: 2.98398533296795

Epoch: 6| Step: 12
Training loss: 2.711152873429805
Validation loss: 2.987966595037337

Epoch: 6| Step: 13
Training loss: 2.406151311262538
Validation loss: 2.988666299592228

Epoch: 29| Step: 0
Training loss: 2.567973269605707
Validation loss: 2.9900855546202387

Epoch: 6| Step: 1
Training loss: 3.2769581655440136
Validation loss: 2.9998368317998967

Epoch: 6| Step: 2
Training loss: 2.4894337522721717
Validation loss: 3.0019434851353015

Epoch: 6| Step: 3
Training loss: 3.471362118474615
Validation loss: 2.999533998194238

Epoch: 6| Step: 4
Training loss: 3.403193371018525
Validation loss: 2.9908251097665426

Epoch: 6| Step: 5
Training loss: 3.415041994699556
Validation loss: 2.9725424134981515

Epoch: 6| Step: 6
Training loss: 3.8355139597595334
Validation loss: 2.967735696690633

Epoch: 6| Step: 7
Training loss: 2.8097635308136835
Validation loss: 2.9666962260102947

Epoch: 6| Step: 8
Training loss: 3.3663015948228896
Validation loss: 2.9681891092299315

Epoch: 6| Step: 9
Training loss: 3.9812425454637332
Validation loss: 2.96751022314852

Epoch: 6| Step: 10
Training loss: 3.6871552386999467
Validation loss: 2.967635811236452

Epoch: 6| Step: 11
Training loss: 3.2425033966964305
Validation loss: 2.9658168497310866

Epoch: 6| Step: 12
Training loss: 2.7577196489216123
Validation loss: 2.9647014929177073

Epoch: 6| Step: 13
Training loss: 2.5834635629950817
Validation loss: 2.9704378400137825

Epoch: 30| Step: 0
Training loss: 3.356087713029686
Validation loss: 2.9730829256056306

Epoch: 6| Step: 1
Training loss: 3.0266815051864793
Validation loss: 2.9664418451495442

Epoch: 6| Step: 2
Training loss: 3.5106839192400314
Validation loss: 2.951992965798243

Epoch: 6| Step: 3
Training loss: 3.64587913847714
Validation loss: 2.960776125693654

Epoch: 6| Step: 4
Training loss: 3.3425780051693765
Validation loss: 2.9567932066904503

Epoch: 6| Step: 5
Training loss: 3.963210196885508
Validation loss: 2.955884868855161

Epoch: 6| Step: 6
Training loss: 3.2552981107411534
Validation loss: 2.9538489055260833

Epoch: 6| Step: 7
Training loss: 2.6773599464409705
Validation loss: 2.95115198376913

Epoch: 6| Step: 8
Training loss: 2.5244204847597222
Validation loss: 2.954118136158853

Epoch: 6| Step: 9
Training loss: 3.009449857452196
Validation loss: 2.959387828886895

Epoch: 6| Step: 10
Training loss: 2.9586137092507374
Validation loss: 2.976528218450116

Epoch: 6| Step: 11
Training loss: 3.1734374759585795
Validation loss: 2.9943700298668134

Epoch: 6| Step: 12
Training loss: 2.9914612050461873
Validation loss: 2.9796535703018248

Epoch: 6| Step: 13
Training loss: 4.043743085191997
Validation loss: 2.9536242006962263

Epoch: 31| Step: 0
Training loss: 3.1918287616201875
Validation loss: 2.945221921606694

Epoch: 6| Step: 1
Training loss: 3.0630215570511354
Validation loss: 2.94119037212444

Epoch: 6| Step: 2
Training loss: 3.063952665874704
Validation loss: 2.9406498934478376

Epoch: 6| Step: 3
Training loss: 3.151937579112967
Validation loss: 2.943291407274123

Epoch: 6| Step: 4
Training loss: 3.6595738618341516
Validation loss: 2.947324345369997

Epoch: 6| Step: 5
Training loss: 3.481826876881407
Validation loss: 2.941012312453817

Epoch: 6| Step: 6
Training loss: 3.0785229420850264
Validation loss: 2.935592808967647

Epoch: 6| Step: 7
Training loss: 3.8578774948485326
Validation loss: 2.932221388957775

Epoch: 6| Step: 8
Training loss: 3.357565032942691
Validation loss: 2.929293050702875

Epoch: 6| Step: 9
Training loss: 2.2568325569262866
Validation loss: 2.926324866885537

Epoch: 6| Step: 10
Training loss: 3.1008853509468546
Validation loss: 2.9250876432585424

Epoch: 6| Step: 11
Training loss: 3.943650180623447
Validation loss: 2.9417972624832784

Epoch: 6| Step: 12
Training loss: 2.439115722388372
Validation loss: 2.923906628218886

Epoch: 6| Step: 13
Training loss: 3.068565279184986
Validation loss: 2.920562045548321

Epoch: 32| Step: 0
Training loss: 2.9889669990338597
Validation loss: 2.9213545491857746

Epoch: 6| Step: 1
Training loss: 3.176479061173585
Validation loss: 2.920256645943909

Epoch: 6| Step: 2
Training loss: 4.015576314074391
Validation loss: 2.916308605758102

Epoch: 6| Step: 3
Training loss: 3.050148012916062
Validation loss: 2.9152393757244406

Epoch: 6| Step: 4
Training loss: 3.5546691223340185
Validation loss: 2.9146722959804956

Epoch: 6| Step: 5
Training loss: 2.8216978775824875
Validation loss: 2.9113756880909327

Epoch: 6| Step: 6
Training loss: 3.46003577065031
Validation loss: 2.911096695741087

Epoch: 6| Step: 7
Training loss: 3.6071305335453494
Validation loss: 2.909225056112289

Epoch: 6| Step: 8
Training loss: 3.336224510121223
Validation loss: 2.9066787687478506

Epoch: 6| Step: 9
Training loss: 3.3445242582522985
Validation loss: 2.9029029576099847

Epoch: 6| Step: 10
Training loss: 2.7909052555373037
Validation loss: 2.9023901811836663

Epoch: 6| Step: 11
Training loss: 2.6732081606409155
Validation loss: 2.8999259303842044

Epoch: 6| Step: 12
Training loss: 2.9105393125614745
Validation loss: 2.906233694097174

Epoch: 6| Step: 13
Training loss: 2.662381175796966
Validation loss: 2.8989819129624936

Epoch: 33| Step: 0
Training loss: 3.4743413451901857
Validation loss: 2.904432411362662

Epoch: 6| Step: 1
Training loss: 3.0458990637022443
Validation loss: 2.908642117278392

Epoch: 6| Step: 2
Training loss: 3.54498253972713
Validation loss: 2.913134284896376

Epoch: 6| Step: 3
Training loss: 3.052606444506318
Validation loss: 2.9011747327097615

Epoch: 6| Step: 4
Training loss: 2.9439049552419396
Validation loss: 2.8983630567796985

Epoch: 6| Step: 5
Training loss: 2.562015952936872
Validation loss: 2.899513652714192

Epoch: 6| Step: 6
Training loss: 3.435980166057145
Validation loss: 2.902856508133853

Epoch: 6| Step: 7
Training loss: 2.2827329649180466
Validation loss: 2.910743604664097

Epoch: 6| Step: 8
Training loss: 3.3401915252046295
Validation loss: 2.9166997570733173

Epoch: 6| Step: 9
Training loss: 3.1392948503266775
Validation loss: 2.9136103571509686

Epoch: 6| Step: 10
Training loss: 3.4510596817855954
Validation loss: 2.9082018221124284

Epoch: 6| Step: 11
Training loss: 3.8847683253981313
Validation loss: 2.89865434332601

Epoch: 6| Step: 12
Training loss: 3.289292569846347
Validation loss: 2.89435399967662

Epoch: 6| Step: 13
Training loss: 3.008351146472665
Validation loss: 2.8899011505488574

Epoch: 34| Step: 0
Training loss: 3.2029852115552915
Validation loss: 2.8871833540287506

Epoch: 6| Step: 1
Training loss: 3.60762040728397
Validation loss: 2.8899015781325814

Epoch: 6| Step: 2
Training loss: 3.6401953709749133
Validation loss: 2.8886718572503853

Epoch: 6| Step: 3
Training loss: 3.236095173750577
Validation loss: 2.885471787856697

Epoch: 6| Step: 4
Training loss: 3.5803626114464104
Validation loss: 2.8872569027959645

Epoch: 6| Step: 5
Training loss: 3.4720124719633065
Validation loss: 2.8860612072895977

Epoch: 6| Step: 6
Training loss: 3.203818790383403
Validation loss: 2.885627494251106

Epoch: 6| Step: 7
Training loss: 2.3667408739016564
Validation loss: 2.8850781665801475

Epoch: 6| Step: 8
Training loss: 3.3630996282896892
Validation loss: 2.883639683637667

Epoch: 6| Step: 9
Training loss: 3.203200623154556
Validation loss: 2.8831091144972305

Epoch: 6| Step: 10
Training loss: 2.487601530217094
Validation loss: 2.8864457604706595

Epoch: 6| Step: 11
Training loss: 2.9928624758850093
Validation loss: 2.885103416535023

Epoch: 6| Step: 12
Training loss: 2.6563165095361336
Validation loss: 2.8850088107052607

Epoch: 6| Step: 13
Training loss: 3.5007025149889137
Validation loss: 2.885688075661517

Epoch: 35| Step: 0
Training loss: 2.8192407151286085
Validation loss: 2.8882204483883633

Epoch: 6| Step: 1
Training loss: 2.6967854331339156
Validation loss: 2.892776776571405

Epoch: 6| Step: 2
Training loss: 3.0968372084758964
Validation loss: 2.8934576255343196

Epoch: 6| Step: 3
Training loss: 2.611515377980281
Validation loss: 2.8866804290421064

Epoch: 6| Step: 4
Training loss: 2.9629545160897264
Validation loss: 2.8825845211108496

Epoch: 6| Step: 5
Training loss: 2.8789252975434163
Validation loss: 2.8808349902005848

Epoch: 6| Step: 6
Training loss: 3.4530511218146946
Validation loss: 2.8801417569027126

Epoch: 6| Step: 7
Training loss: 2.8760206649844298
Validation loss: 2.880788195582183

Epoch: 6| Step: 8
Training loss: 3.5834854152846245
Validation loss: 2.8825334343346474

Epoch: 6| Step: 9
Training loss: 3.8939266060690136
Validation loss: 2.8814451835281423

Epoch: 6| Step: 10
Training loss: 3.057673173243831
Validation loss: 2.880907360891816

Epoch: 6| Step: 11
Training loss: 3.8053023137265374
Validation loss: 2.8808145287723295

Epoch: 6| Step: 12
Training loss: 3.4304479220893285
Validation loss: 2.875647032889086

Epoch: 6| Step: 13
Training loss: 2.9417076007131064
Validation loss: 2.8796744241266765

Epoch: 36| Step: 0
Training loss: 3.423230464495389
Validation loss: 2.8809173488050006

Epoch: 6| Step: 1
Training loss: 3.3935337510595174
Validation loss: 2.8813002742692757

Epoch: 6| Step: 2
Training loss: 2.821327850930789
Validation loss: 2.8830699354953087

Epoch: 6| Step: 3
Training loss: 3.082608129085246
Validation loss: 2.8772305489319594

Epoch: 6| Step: 4
Training loss: 3.3726185061119134
Validation loss: 2.877688591306589

Epoch: 6| Step: 5
Training loss: 3.3446713898277984
Validation loss: 2.874836817636182

Epoch: 6| Step: 6
Training loss: 2.8325728536530956
Validation loss: 2.8730216493625202

Epoch: 6| Step: 7
Training loss: 2.1162916026388947
Validation loss: 2.8723262813066524

Epoch: 6| Step: 8
Training loss: 2.942776101381369
Validation loss: 2.8700003664858933

Epoch: 6| Step: 9
Training loss: 3.23844381000302
Validation loss: 2.8675797855675853

Epoch: 6| Step: 10
Training loss: 3.0575013140272436
Validation loss: 2.867310120263296

Epoch: 6| Step: 11
Training loss: 3.8209928129159025
Validation loss: 2.868343946564035

Epoch: 6| Step: 12
Training loss: 3.3041250052276925
Validation loss: 2.867366857027404

Epoch: 6| Step: 13
Training loss: 3.5526488777765652
Validation loss: 2.8670829724165827

Epoch: 37| Step: 0
Training loss: 2.3491501712208653
Validation loss: 2.8659723816411162

Epoch: 6| Step: 1
Training loss: 2.8219030233113314
Validation loss: 2.866524860886398

Epoch: 6| Step: 2
Training loss: 3.1335210973876397
Validation loss: 2.8827773570386914

Epoch: 6| Step: 3
Training loss: 2.606917634705436
Validation loss: 2.912278139713883

Epoch: 6| Step: 4
Training loss: 3.8657677712746508
Validation loss: 2.97283809337936

Epoch: 6| Step: 5
Training loss: 3.389497771743377
Validation loss: 2.865230014032508

Epoch: 6| Step: 6
Training loss: 3.4705552910622695
Validation loss: 2.8680198471422194

Epoch: 6| Step: 7
Training loss: 3.045555572438633
Validation loss: 2.8717942486681416

Epoch: 6| Step: 8
Training loss: 3.0035652592140263
Validation loss: 2.8988751252789555

Epoch: 6| Step: 9
Training loss: 3.6027905404060823
Validation loss: 2.923730504647785

Epoch: 6| Step: 10
Training loss: 3.6516922150319835
Validation loss: 2.9132048922319584

Epoch: 6| Step: 11
Training loss: 3.8768147556778243
Validation loss: 2.880426294732417

Epoch: 6| Step: 12
Training loss: 2.7110401321024136
Validation loss: 2.8655657264856935

Epoch: 6| Step: 13
Training loss: 2.044333474400207
Validation loss: 2.8567690819245577

Epoch: 38| Step: 0
Training loss: 3.234752983084221
Validation loss: 2.8583970276041577

Epoch: 6| Step: 1
Training loss: 2.9634342185298648
Validation loss: 2.859546166948647

Epoch: 6| Step: 2
Training loss: 3.4299230125395295
Validation loss: 2.861964827635821

Epoch: 6| Step: 3
Training loss: 3.324786593823762
Validation loss: 2.8695487765344176

Epoch: 6| Step: 4
Training loss: 2.718182383949265
Validation loss: 2.864966718835499

Epoch: 6| Step: 5
Training loss: 3.200353847251836
Validation loss: 2.8696369193218385

Epoch: 6| Step: 6
Training loss: 4.1046189318775665
Validation loss: 2.869511421657992

Epoch: 6| Step: 7
Training loss: 3.1685767854377214
Validation loss: 2.862185996276197

Epoch: 6| Step: 8
Training loss: 3.3639547668300787
Validation loss: 2.857148517875513

Epoch: 6| Step: 9
Training loss: 3.382669846696252
Validation loss: 2.859355886776402

Epoch: 6| Step: 10
Training loss: 2.3782130141143645
Validation loss: 2.8546625713560734

Epoch: 6| Step: 11
Training loss: 3.174720427451915
Validation loss: 2.8567076521154973

Epoch: 6| Step: 12
Training loss: 2.623869697999052
Validation loss: 2.8577649313875164

Epoch: 6| Step: 13
Training loss: 2.6398303399333853
Validation loss: 2.8590328720838207

Epoch: 39| Step: 0
Training loss: 3.345924454743135
Validation loss: 2.8630718341579864

Epoch: 6| Step: 1
Training loss: 2.775341658995828
Validation loss: 2.869729890581482

Epoch: 6| Step: 2
Training loss: 3.599812974310222
Validation loss: 2.8810719737283503

Epoch: 6| Step: 3
Training loss: 3.472130580116603
Validation loss: 2.862591653866412

Epoch: 6| Step: 4
Training loss: 3.5980131653592022
Validation loss: 2.8503064886524885

Epoch: 6| Step: 5
Training loss: 3.4829553012657564
Validation loss: 2.8463596926039383

Epoch: 6| Step: 6
Training loss: 3.2007374867340146
Validation loss: 2.8425568885127586

Epoch: 6| Step: 7
Training loss: 3.034818613328992
Validation loss: 2.8440145238581613

Epoch: 6| Step: 8
Training loss: 2.902035189793165
Validation loss: 2.842245458255171

Epoch: 6| Step: 9
Training loss: 2.686258960991172
Validation loss: 2.842741017256738

Epoch: 6| Step: 10
Training loss: 3.117390026119794
Validation loss: 2.842065632585461

Epoch: 6| Step: 11
Training loss: 2.337607204814411
Validation loss: 2.842458821976075

Epoch: 6| Step: 12
Training loss: 3.6476467018876693
Validation loss: 2.842265691335984

Epoch: 6| Step: 13
Training loss: 2.1446854443868895
Validation loss: 2.839681849793297

Epoch: 40| Step: 0
Training loss: 3.3239883719561076
Validation loss: 2.8398805224257226

Epoch: 6| Step: 1
Training loss: 3.4681455068279305
Validation loss: 2.8401353417256976

Epoch: 6| Step: 2
Training loss: 3.452231036847752
Validation loss: 2.8443888573519636

Epoch: 6| Step: 3
Training loss: 2.579126706735629
Validation loss: 2.8397146778108433

Epoch: 6| Step: 4
Training loss: 2.83164042966055
Validation loss: 2.838603162066444

Epoch: 6| Step: 5
Training loss: 3.0933107006406972
Validation loss: 2.8376889550655613

Epoch: 6| Step: 6
Training loss: 3.2349622994868987
Validation loss: 2.840812501057737

Epoch: 6| Step: 7
Training loss: 3.3391253853259575
Validation loss: 2.845676282215181

Epoch: 6| Step: 8
Training loss: 3.091470668643092
Validation loss: 2.8376988349099093

Epoch: 6| Step: 9
Training loss: 3.2029633271889617
Validation loss: 2.8346322343872594

Epoch: 6| Step: 10
Training loss: 3.049175469934086
Validation loss: 2.8358918882429984

Epoch: 6| Step: 11
Training loss: 3.3336267977914273
Validation loss: 2.833073506997612

Epoch: 6| Step: 12
Training loss: 3.0160825708353345
Validation loss: 2.834426356590876

Epoch: 6| Step: 13
Training loss: 2.68225908475673
Validation loss: 2.8332188062545676

Epoch: 41| Step: 0
Training loss: 3.013047455868141
Validation loss: 2.8333900368775287

Epoch: 6| Step: 1
Training loss: 2.88698930724238
Validation loss: 2.832747399429206

Epoch: 6| Step: 2
Training loss: 3.8100265233676063
Validation loss: 2.830369641325133

Epoch: 6| Step: 3
Training loss: 3.7046173565296567
Validation loss: 2.8301308833195864

Epoch: 6| Step: 4
Training loss: 2.85574076514098
Validation loss: 2.827506914789405

Epoch: 6| Step: 5
Training loss: 2.871842018245222
Validation loss: 2.8284363172761466

Epoch: 6| Step: 6
Training loss: 3.258212789922049
Validation loss: 2.8257687958837465

Epoch: 6| Step: 7
Training loss: 3.0253693473368135
Validation loss: 2.825034033835033

Epoch: 6| Step: 8
Training loss: 2.4967414600626947
Validation loss: 2.821812740822829

Epoch: 6| Step: 9
Training loss: 3.102231354744023
Validation loss: 2.823234884057032

Epoch: 6| Step: 10
Training loss: 2.9636733166102993
Validation loss: 2.8222526099501777

Epoch: 6| Step: 11
Training loss: 2.441530074984878
Validation loss: 2.823233724475949

Epoch: 6| Step: 12
Training loss: 3.289760633300554
Validation loss: 2.8250116699947627

Epoch: 6| Step: 13
Training loss: 4.294237027595231
Validation loss: 2.8214246565750773

Epoch: 42| Step: 0
Training loss: 3.6157513727442536
Validation loss: 2.8248876084484995

Epoch: 6| Step: 1
Training loss: 3.043433020533896
Validation loss: 2.821553135319056

Epoch: 6| Step: 2
Training loss: 3.0822552695042504
Validation loss: 2.821552423891422

Epoch: 6| Step: 3
Training loss: 3.0984876943149686
Validation loss: 2.8215106401507244

Epoch: 6| Step: 4
Training loss: 2.954861730706737
Validation loss: 2.819993068592295

Epoch: 6| Step: 5
Training loss: 3.3583184998392133
Validation loss: 2.820856008300538

Epoch: 6| Step: 6
Training loss: 3.4531585553640434
Validation loss: 2.8198781739596974

Epoch: 6| Step: 7
Training loss: 2.8985404847143883
Validation loss: 2.8191091927937233

Epoch: 6| Step: 8
Training loss: 2.2062716907158997
Validation loss: 2.819514833341486

Epoch: 6| Step: 9
Training loss: 3.459872320829257
Validation loss: 2.8223771188607962

Epoch: 6| Step: 10
Training loss: 2.855918420956846
Validation loss: 2.8170958219775133

Epoch: 6| Step: 11
Training loss: 3.357509929219076
Validation loss: 2.8186576405214976

Epoch: 6| Step: 12
Training loss: 3.1719824862002675
Validation loss: 2.816748846357251

Epoch: 6| Step: 13
Training loss: 2.998940916674435
Validation loss: 2.817783108990297

Epoch: 43| Step: 0
Training loss: 3.416376023995178
Validation loss: 2.817014425022428

Epoch: 6| Step: 1
Training loss: 2.965992500197947
Validation loss: 2.8163184566051895

Epoch: 6| Step: 2
Training loss: 3.568097986629221
Validation loss: 2.8147231236046766

Epoch: 6| Step: 3
Training loss: 2.811508682175157
Validation loss: 2.815404706862223

Epoch: 6| Step: 4
Training loss: 3.193406858878619
Validation loss: 2.817201845713133

Epoch: 6| Step: 5
Training loss: 3.691423817749202
Validation loss: 2.8154634293116163

Epoch: 6| Step: 6
Training loss: 2.7913615691065288
Validation loss: 2.8154354595856614

Epoch: 6| Step: 7
Training loss: 3.142681454417229
Validation loss: 2.8300922345634945

Epoch: 6| Step: 8
Training loss: 2.1717226263152054
Validation loss: 2.834667546454881

Epoch: 6| Step: 9
Training loss: 3.162976038390679
Validation loss: 2.8573784149502757

Epoch: 6| Step: 10
Training loss: 3.1324407875450073
Validation loss: 2.855096122411299

Epoch: 6| Step: 11
Training loss: 3.5960383551634485
Validation loss: 2.840640115036062

Epoch: 6| Step: 12
Training loss: 2.67603393252631
Validation loss: 2.8247509354256906

Epoch: 6| Step: 13
Training loss: 3.133036693437266
Validation loss: 2.809989630127846

Epoch: 44| Step: 0
Training loss: 2.8612109684313007
Validation loss: 2.8065004536041376

Epoch: 6| Step: 1
Training loss: 2.6849118567407007
Validation loss: 2.80888673669347

Epoch: 6| Step: 2
Training loss: 3.1050793289641017
Validation loss: 2.8069867257599226

Epoch: 6| Step: 3
Training loss: 3.5202220107637983
Validation loss: 2.8091070767835085

Epoch: 6| Step: 4
Training loss: 3.2046000736090146
Validation loss: 2.808837155326867

Epoch: 6| Step: 5
Training loss: 3.140656124738884
Validation loss: 2.8071462796120157

Epoch: 6| Step: 6
Training loss: 2.842616767196036
Validation loss: 2.8081491924520696

Epoch: 6| Step: 7
Training loss: 3.486117624291106
Validation loss: 2.8082927066900805

Epoch: 6| Step: 8
Training loss: 3.884866888645308
Validation loss: 2.8039581276382757

Epoch: 6| Step: 9
Training loss: 2.7493195125307683
Validation loss: 2.8039094520594703

Epoch: 6| Step: 10
Training loss: 2.7664036920724726
Validation loss: 2.801990077628425

Epoch: 6| Step: 11
Training loss: 3.1231250478802903
Validation loss: 2.8053620157732873

Epoch: 6| Step: 12
Training loss: 2.840541664638518
Validation loss: 2.8117154849134787

Epoch: 6| Step: 13
Training loss: 3.37392040928986
Validation loss: 2.802987682204643

Epoch: 45| Step: 0
Training loss: 2.856736263227097
Validation loss: 2.7988053703981777

Epoch: 6| Step: 1
Training loss: 3.767346507023145
Validation loss: 2.800381022412632

Epoch: 6| Step: 2
Training loss: 3.2829886053784034
Validation loss: 2.8008682087134043

Epoch: 6| Step: 3
Training loss: 2.1133480378771545
Validation loss: 2.8029037087711206

Epoch: 6| Step: 4
Training loss: 2.7884863677803984
Validation loss: 2.800733804354523

Epoch: 6| Step: 5
Training loss: 2.5270557268924776
Validation loss: 2.800738926626356

Epoch: 6| Step: 6
Training loss: 3.2349575826467087
Validation loss: 2.8002963848994185

Epoch: 6| Step: 7
Training loss: 4.038088181113343
Validation loss: 2.798067704895819

Epoch: 6| Step: 8
Training loss: 3.8436080782361293
Validation loss: 2.796187502793983

Epoch: 6| Step: 9
Training loss: 3.0369964873011286
Validation loss: 2.7966637246582597

Epoch: 6| Step: 10
Training loss: 2.8576122749812534
Validation loss: 2.795447089483297

Epoch: 6| Step: 11
Training loss: 3.144763554113297
Validation loss: 2.7946395166714795

Epoch: 6| Step: 12
Training loss: 2.7141826455904305
Validation loss: 2.7946068370834927

Epoch: 6| Step: 13
Training loss: 2.710040381014596
Validation loss: 2.7936533503012866

Epoch: 46| Step: 0
Training loss: 3.2573039803771855
Validation loss: 2.793676266162116

Epoch: 6| Step: 1
Training loss: 2.461200227791168
Validation loss: 2.7945235457047204

Epoch: 6| Step: 2
Training loss: 3.5105423871779204
Validation loss: 2.79340141944199

Epoch: 6| Step: 3
Training loss: 3.435356720416206
Validation loss: 2.793982681209346

Epoch: 6| Step: 4
Training loss: 2.5750788408079766
Validation loss: 2.7939220997155823

Epoch: 6| Step: 5
Training loss: 3.522540669798681
Validation loss: 2.7924078384631903

Epoch: 6| Step: 6
Training loss: 2.8273542839838566
Validation loss: 2.791334719904474

Epoch: 6| Step: 7
Training loss: 3.1138740846219797
Validation loss: 2.7912699635333755

Epoch: 6| Step: 8
Training loss: 3.8178695637726343
Validation loss: 2.791134939918033

Epoch: 6| Step: 9
Training loss: 2.978718935171792
Validation loss: 2.7915369238448027

Epoch: 6| Step: 10
Training loss: 2.9666351816742584
Validation loss: 2.7899048109161466

Epoch: 6| Step: 11
Training loss: 2.613985385886029
Validation loss: 2.788374022622311

Epoch: 6| Step: 12
Training loss: 3.0186857986731312
Validation loss: 2.7872960573778696

Epoch: 6| Step: 13
Training loss: 3.0974345759270587
Validation loss: 2.7860658492077937

Epoch: 47| Step: 0
Training loss: 3.0832411949893723
Validation loss: 2.787308562393187

Epoch: 6| Step: 1
Training loss: 3.59018656657627
Validation loss: 2.7870815190135505

Epoch: 6| Step: 2
Training loss: 3.2762245567873673
Validation loss: 2.786743097682972

Epoch: 6| Step: 3
Training loss: 2.879582443432198
Validation loss: 2.7875526086651057

Epoch: 6| Step: 4
Training loss: 3.3007673816276704
Validation loss: 2.787094056265522

Epoch: 6| Step: 5
Training loss: 3.339233771052032
Validation loss: 2.7828795436397487

Epoch: 6| Step: 6
Training loss: 2.0482152380343033
Validation loss: 2.78231780701553

Epoch: 6| Step: 7
Training loss: 2.8088155560722474
Validation loss: 2.7808863572744373

Epoch: 6| Step: 8
Training loss: 2.6717945109061625
Validation loss: 2.7805248392209796

Epoch: 6| Step: 9
Training loss: 3.221590880956217
Validation loss: 2.778401611241313

Epoch: 6| Step: 10
Training loss: 3.044534106763479
Validation loss: 2.778702278177831

Epoch: 6| Step: 11
Training loss: 3.294026233302278
Validation loss: 2.7790845391308765

Epoch: 6| Step: 12
Training loss: 3.066134573205863
Validation loss: 2.780670802555843

Epoch: 6| Step: 13
Training loss: 3.7028433777159706
Validation loss: 2.7782847280966654

Epoch: 48| Step: 0
Training loss: 3.2708842441621955
Validation loss: 2.7797632390310567

Epoch: 6| Step: 1
Training loss: 3.2770224812737303
Validation loss: 2.7835177018299166

Epoch: 6| Step: 2
Training loss: 2.994184261036399
Validation loss: 2.789486913990492

Epoch: 6| Step: 3
Training loss: 3.4637266334343
Validation loss: 2.79291903107828

Epoch: 6| Step: 4
Training loss: 3.0787425704538065
Validation loss: 2.793532810439157

Epoch: 6| Step: 5
Training loss: 2.8308510471883164
Validation loss: 2.7878791419904947

Epoch: 6| Step: 6
Training loss: 2.5595684495414814
Validation loss: 2.786693183969804

Epoch: 6| Step: 7
Training loss: 3.4646357976015274
Validation loss: 2.7886469516413457

Epoch: 6| Step: 8
Training loss: 3.538963063845219
Validation loss: 2.791391454314035

Epoch: 6| Step: 9
Training loss: 3.2149161310958747
Validation loss: 2.7876714639246534

Epoch: 6| Step: 10
Training loss: 2.2889440720684666
Validation loss: 2.7800616391394586

Epoch: 6| Step: 11
Training loss: 3.2473637085492943
Validation loss: 2.773560441103042

Epoch: 6| Step: 12
Training loss: 2.5794011512208948
Validation loss: 2.7740766640035766

Epoch: 6| Step: 13
Training loss: 3.2637228650733263
Validation loss: 2.774197946204787

Epoch: 49| Step: 0
Training loss: 2.710007917667098
Validation loss: 2.780069136228721

Epoch: 6| Step: 1
Training loss: 3.7407975771299795
Validation loss: 2.782388754270973

Epoch: 6| Step: 2
Training loss: 3.097599293592861
Validation loss: 2.7780695713425634

Epoch: 6| Step: 3
Training loss: 3.3545206171466315
Validation loss: 2.772579508597602

Epoch: 6| Step: 4
Training loss: 2.5226309700907477
Validation loss: 2.771969418833763

Epoch: 6| Step: 5
Training loss: 3.0916995565167897
Validation loss: 2.779660102200903

Epoch: 6| Step: 6
Training loss: 3.2395685487922954
Validation loss: 2.78263754370834

Epoch: 6| Step: 7
Training loss: 2.491747778483438
Validation loss: 2.777561659934059

Epoch: 6| Step: 8
Training loss: 3.479202605345508
Validation loss: 2.782464797584817

Epoch: 6| Step: 9
Training loss: 3.1039983375190925
Validation loss: 2.7761740337907637

Epoch: 6| Step: 10
Training loss: 2.842570972210837
Validation loss: 2.769336850226647

Epoch: 6| Step: 11
Training loss: 3.8998360428213736
Validation loss: 2.767270776161672

Epoch: 6| Step: 12
Training loss: 2.7221855293015427
Validation loss: 2.769985499182841

Epoch: 6| Step: 13
Training loss: 2.219386479598629
Validation loss: 2.7696455674130602

Epoch: 50| Step: 0
Training loss: 2.6849607847678088
Validation loss: 2.7655218953460396

Epoch: 6| Step: 1
Training loss: 3.077671075647612
Validation loss: 2.767245190319116

Epoch: 6| Step: 2
Training loss: 3.0313590118627105
Validation loss: 2.768658988506997

Epoch: 6| Step: 3
Training loss: 3.103433885212123
Validation loss: 2.7663374077635607

Epoch: 6| Step: 4
Training loss: 3.6944480997958222
Validation loss: 2.7644017960900884

Epoch: 6| Step: 5
Training loss: 3.142449148126278
Validation loss: 2.7633778887826734

Epoch: 6| Step: 6
Training loss: 3.185573818670542
Validation loss: 2.7622515761449353

Epoch: 6| Step: 7
Training loss: 2.987880066513734
Validation loss: 2.7610080428379242

Epoch: 6| Step: 8
Training loss: 2.985877812066956
Validation loss: 2.7629809278934476

Epoch: 6| Step: 9
Training loss: 2.641860943538686
Validation loss: 2.766842991438388

Epoch: 6| Step: 10
Training loss: 3.2317246344883994
Validation loss: 2.7747961150299068

Epoch: 6| Step: 11
Training loss: 3.145135932738105
Validation loss: 2.7801670078138265

Epoch: 6| Step: 12
Training loss: 2.970295714481931
Validation loss: 2.787867117711272

Epoch: 6| Step: 13
Training loss: 3.270462614460751
Validation loss: 2.798178660388218

Epoch: 51| Step: 0
Training loss: 3.7184453246382714
Validation loss: 2.7918885635202275

Epoch: 6| Step: 1
Training loss: 3.4682383116928883
Validation loss: 2.7755000162369905

Epoch: 6| Step: 2
Training loss: 3.1380568295177835
Validation loss: 2.76202546918301

Epoch: 6| Step: 3
Training loss: 3.237945796953493
Validation loss: 2.7605551683565097

Epoch: 6| Step: 4
Training loss: 2.940719139221924
Validation loss: 2.759808238597868

Epoch: 6| Step: 5
Training loss: 3.3949303079679667
Validation loss: 2.75482489441274

Epoch: 6| Step: 6
Training loss: 3.387447292016359
Validation loss: 2.7534742735822535

Epoch: 6| Step: 7
Training loss: 2.573472790455837
Validation loss: 2.7534160819243936

Epoch: 6| Step: 8
Training loss: 2.7919272875239427
Validation loss: 2.7550873725882363

Epoch: 6| Step: 9
Training loss: 2.1764146304101004
Validation loss: 2.757154505503553

Epoch: 6| Step: 10
Training loss: 3.700171214085884
Validation loss: 2.756860639820444

Epoch: 6| Step: 11
Training loss: 2.8512293790868473
Validation loss: 2.7575895357651854

Epoch: 6| Step: 12
Training loss: 2.349891246145953
Validation loss: 2.7567089333540022

Epoch: 6| Step: 13
Training loss: 3.0612575579822927
Validation loss: 2.760478878113489

Epoch: 52| Step: 0
Training loss: 3.0526115993185154
Validation loss: 2.758809567541434

Epoch: 6| Step: 1
Training loss: 2.770494473801385
Validation loss: 2.761765789119832

Epoch: 6| Step: 2
Training loss: 2.446005238649836
Validation loss: 2.786238168723064

Epoch: 6| Step: 3
Training loss: 3.151902783642246
Validation loss: 2.8305626525787786

Epoch: 6| Step: 4
Training loss: 2.974084658815262
Validation loss: 2.7982330315908843

Epoch: 6| Step: 5
Training loss: 3.5918839627152948
Validation loss: 2.7631655951238514

Epoch: 6| Step: 6
Training loss: 3.7470570778532237
Validation loss: 2.7464837156649486

Epoch: 6| Step: 7
Training loss: 2.845129160706254
Validation loss: 2.746625971777397

Epoch: 6| Step: 8
Training loss: 3.292505921841497
Validation loss: 2.745456495877822

Epoch: 6| Step: 9
Training loss: 2.941224354746855
Validation loss: 2.7447848254055476

Epoch: 6| Step: 10
Training loss: 3.1747895178524854
Validation loss: 2.7444383402716066

Epoch: 6| Step: 11
Training loss: 2.4321800277107006
Validation loss: 2.7423927340877636

Epoch: 6| Step: 12
Training loss: 3.4906097876235216
Validation loss: 2.738990557359158

Epoch: 6| Step: 13
Training loss: 2.7394875822751854
Validation loss: 2.7405089635459583

Epoch: 53| Step: 0
Training loss: 3.3606557844162745
Validation loss: 2.7407563651081537

Epoch: 6| Step: 1
Training loss: 2.872789943261684
Validation loss: 2.7391851968634304

Epoch: 6| Step: 2
Training loss: 2.7146228727875226
Validation loss: 2.7427460280143143

Epoch: 6| Step: 3
Training loss: 3.120674496637681
Validation loss: 2.7433442240418997

Epoch: 6| Step: 4
Training loss: 3.3411632764345596
Validation loss: 2.7370114527956555

Epoch: 6| Step: 5
Training loss: 3.2198217885239417
Validation loss: 2.7346690967573597

Epoch: 6| Step: 6
Training loss: 2.3554819084546432
Validation loss: 2.7351069029572894

Epoch: 6| Step: 7
Training loss: 3.145923907931829
Validation loss: 2.7425873901336506

Epoch: 6| Step: 8
Training loss: 2.5738006391170694
Validation loss: 2.7576959062385176

Epoch: 6| Step: 9
Training loss: 3.065599235714158
Validation loss: 2.8079019439805872

Epoch: 6| Step: 10
Training loss: 1.956355970151087
Validation loss: 2.8169240814131618

Epoch: 6| Step: 11
Training loss: 3.872910459177269
Validation loss: 2.7912729163471095

Epoch: 6| Step: 12
Training loss: 3.6830463993858804
Validation loss: 2.7360894313531006

Epoch: 6| Step: 13
Training loss: 3.365719645665
Validation loss: 2.7304396908503996

Epoch: 54| Step: 0
Training loss: 2.5612957613623717
Validation loss: 2.7324510275322718

Epoch: 6| Step: 1
Training loss: 2.5833512479919776
Validation loss: 2.7341218227014896

Epoch: 6| Step: 2
Training loss: 2.8871942729851927
Validation loss: 2.7394766707056153

Epoch: 6| Step: 3
Training loss: 3.295342401013435
Validation loss: 2.7459359114382873

Epoch: 6| Step: 4
Training loss: 3.549405040046242
Validation loss: 2.7623703274656948

Epoch: 6| Step: 5
Training loss: 3.550964015038197
Validation loss: 2.7560738806465115

Epoch: 6| Step: 6
Training loss: 2.8309000636888015
Validation loss: 2.739297787761399

Epoch: 6| Step: 7
Training loss: 2.693506492707475
Validation loss: 2.737595142294388

Epoch: 6| Step: 8
Training loss: 3.271474463604389
Validation loss: 2.739219241444405

Epoch: 6| Step: 9
Training loss: 2.997757232293351
Validation loss: 2.738794220073956

Epoch: 6| Step: 10
Training loss: 3.3316793311830777
Validation loss: 2.736761312762743

Epoch: 6| Step: 11
Training loss: 3.2859224022533415
Validation loss: 2.7400089212363237

Epoch: 6| Step: 12
Training loss: 2.960777258248487
Validation loss: 2.740139261061106

Epoch: 6| Step: 13
Training loss: 2.634462105791812
Validation loss: 2.7452321956532195

Epoch: 55| Step: 0
Training loss: 3.449834991047268
Validation loss: 2.7493181567282194

Epoch: 6| Step: 1
Training loss: 3.028432578890885
Validation loss: 2.7459096347831706

Epoch: 6| Step: 2
Training loss: 3.573548167541233
Validation loss: 2.755428431419027

Epoch: 6| Step: 3
Training loss: 2.873822510125173
Validation loss: 2.745587013145266

Epoch: 6| Step: 4
Training loss: 2.848160729116142
Validation loss: 2.729845385465207

Epoch: 6| Step: 5
Training loss: 3.448829706701274
Validation loss: 2.726005442994542

Epoch: 6| Step: 6
Training loss: 3.1235949600169297
Validation loss: 2.7261468831656144

Epoch: 6| Step: 7
Training loss: 3.0510622644863314
Validation loss: 2.7246331820648093

Epoch: 6| Step: 8
Training loss: 3.327008127132083
Validation loss: 2.7219380465902328

Epoch: 6| Step: 9
Training loss: 2.9213814548073294
Validation loss: 2.7205207724198255

Epoch: 6| Step: 10
Training loss: 3.1959699406744013
Validation loss: 2.7184831693178233

Epoch: 6| Step: 11
Training loss: 2.5810664187860053
Validation loss: 2.723471868661966

Epoch: 6| Step: 12
Training loss: 2.358840578702262
Validation loss: 2.718956911753996

Epoch: 6| Step: 13
Training loss: 2.501217545618584
Validation loss: 2.717728576281649

Epoch: 56| Step: 0
Training loss: 2.9764128719978897
Validation loss: 2.7172078327483713

Epoch: 6| Step: 1
Training loss: 3.068055699386198
Validation loss: 2.719139492719282

Epoch: 6| Step: 2
Training loss: 2.948871435468139
Validation loss: 2.723772519812496

Epoch: 6| Step: 3
Training loss: 3.0744818025957135
Validation loss: 2.729966648902811

Epoch: 6| Step: 4
Training loss: 2.802886724817288
Validation loss: 2.7346502973551745

Epoch: 6| Step: 5
Training loss: 3.293854256125444
Validation loss: 2.734941486328787

Epoch: 6| Step: 6
Training loss: 3.068479966599085
Validation loss: 2.737984404552844

Epoch: 6| Step: 7
Training loss: 3.482664637894597
Validation loss: 2.732035032206129

Epoch: 6| Step: 8
Training loss: 3.2272372152252706
Validation loss: 2.7252294801466

Epoch: 6| Step: 9
Training loss: 3.674643913196753
Validation loss: 2.7194385092871065

Epoch: 6| Step: 10
Training loss: 2.5959828888339485
Validation loss: 2.719248479742262

Epoch: 6| Step: 11
Training loss: 2.77712277849195
Validation loss: 2.7255272388260505

Epoch: 6| Step: 12
Training loss: 3.153768338071871
Validation loss: 2.7238171647791494

Epoch: 6| Step: 13
Training loss: 1.5789124441048947
Validation loss: 2.710195622088266

Epoch: 57| Step: 0
Training loss: 2.7436429152739983
Validation loss: 2.710926106595585

Epoch: 6| Step: 1
Training loss: 3.572402709533122
Validation loss: 2.7136336791979336

Epoch: 6| Step: 2
Training loss: 3.8069075544158744
Validation loss: 2.7150813116452603

Epoch: 6| Step: 3
Training loss: 2.87290463591434
Validation loss: 2.718850467396996

Epoch: 6| Step: 4
Training loss: 2.4245522744492156
Validation loss: 2.723990296636853

Epoch: 6| Step: 5
Training loss: 3.0886876466849444
Validation loss: 2.734371686660939

Epoch: 6| Step: 6
Training loss: 2.4075122841653336
Validation loss: 2.7343991317504535

Epoch: 6| Step: 7
Training loss: 3.3537573396799343
Validation loss: 2.724238434209542

Epoch: 6| Step: 8
Training loss: 2.456554954952993
Validation loss: 2.719579219186784

Epoch: 6| Step: 9
Training loss: 3.1834793163866055
Validation loss: 2.7200731116143806

Epoch: 6| Step: 10
Training loss: 3.269812130912105
Validation loss: 2.7123881980508155

Epoch: 6| Step: 11
Training loss: 3.1011184643577336
Validation loss: 2.7121177336687694

Epoch: 6| Step: 12
Training loss: 3.300636207057344
Validation loss: 2.7105044280578467

Epoch: 6| Step: 13
Training loss: 2.80485206907504
Validation loss: 2.71422495570813

Epoch: 58| Step: 0
Training loss: 3.005667736743563
Validation loss: 2.728508411885663

Epoch: 6| Step: 1
Training loss: 2.618464098780578
Validation loss: 2.737065484251698

Epoch: 6| Step: 2
Training loss: 2.8906025653690217
Validation loss: 2.7245599066000272

Epoch: 6| Step: 3
Training loss: 3.791739801952134
Validation loss: 2.7074270939319973

Epoch: 6| Step: 4
Training loss: 3.212194192187065
Validation loss: 2.700225431981192

Epoch: 6| Step: 5
Training loss: 3.477450345286136
Validation loss: 2.7014959489696513

Epoch: 6| Step: 6
Training loss: 2.85100577158429
Validation loss: 2.708933549983499

Epoch: 6| Step: 7
Training loss: 3.2413495187306913
Validation loss: 2.7114388805845397

Epoch: 6| Step: 8
Training loss: 2.299159775553441
Validation loss: 2.7123055746920857

Epoch: 6| Step: 9
Training loss: 2.3425882130301168
Validation loss: 2.7124693709515006

Epoch: 6| Step: 10
Training loss: 3.115687265032209
Validation loss: 2.719077098039348

Epoch: 6| Step: 11
Training loss: 3.083339399039686
Validation loss: 2.706714464979251

Epoch: 6| Step: 12
Training loss: 3.060205027873471
Validation loss: 2.708626768782633

Epoch: 6| Step: 13
Training loss: 3.6028679655955544
Validation loss: 2.704338031542295

Epoch: 59| Step: 0
Training loss: 2.848321781957189
Validation loss: 2.7068250130360965

Epoch: 6| Step: 1
Training loss: 3.000097908965457
Validation loss: 2.7106930570893573

Epoch: 6| Step: 2
Training loss: 3.414433718823556
Validation loss: 2.7050574571723165

Epoch: 6| Step: 3
Training loss: 3.1526517091287243
Validation loss: 2.703754404645725

Epoch: 6| Step: 4
Training loss: 3.1052186109459567
Validation loss: 2.7009327881093763

Epoch: 6| Step: 5
Training loss: 3.1548824558034685
Validation loss: 2.6970262146398696

Epoch: 6| Step: 6
Training loss: 3.1902718553131524
Validation loss: 2.6971885207555246

Epoch: 6| Step: 7
Training loss: 2.510083937590291
Validation loss: 2.694773507675507

Epoch: 6| Step: 8
Training loss: 2.7859903293121375
Validation loss: 2.6927372782574004

Epoch: 6| Step: 9
Training loss: 3.011739015820272
Validation loss: 2.6927645993119613

Epoch: 6| Step: 10
Training loss: 3.3531373776565583
Validation loss: 2.689679409919703

Epoch: 6| Step: 11
Training loss: 3.0938603545264525
Validation loss: 2.68948001520311

Epoch: 6| Step: 12
Training loss: 2.938052024105444
Validation loss: 2.6870719080310694

Epoch: 6| Step: 13
Training loss: 2.897553095623196
Validation loss: 2.6865058905181662

Epoch: 60| Step: 0
Training loss: 2.6817083718340706
Validation loss: 2.683915290891009

Epoch: 6| Step: 1
Training loss: 2.255854936075178
Validation loss: 2.6853855760150336

Epoch: 6| Step: 2
Training loss: 3.0081400110855787
Validation loss: 2.684196108780149

Epoch: 6| Step: 3
Training loss: 3.288580997406247
Validation loss: 2.682641507703084

Epoch: 6| Step: 4
Training loss: 3.407730200891836
Validation loss: 2.680872845600972

Epoch: 6| Step: 5
Training loss: 2.7173066966941595
Validation loss: 2.6809200142034495

Epoch: 6| Step: 6
Training loss: 3.63008812915602
Validation loss: 2.6853196900331917

Epoch: 6| Step: 7
Training loss: 2.9275027661265685
Validation loss: 2.6859133291798325

Epoch: 6| Step: 8
Training loss: 2.6808656821662384
Validation loss: 2.6842329605565785

Epoch: 6| Step: 9
Training loss: 3.291306262667279
Validation loss: 2.684514509194405

Epoch: 6| Step: 10
Training loss: 2.933042349976865
Validation loss: 2.685088819532338

Epoch: 6| Step: 11
Training loss: 3.273237490859749
Validation loss: 2.684184641026819

Epoch: 6| Step: 12
Training loss: 2.9389975768264587
Validation loss: 2.6856792317549045

Epoch: 6| Step: 13
Training loss: 3.226476044685773
Validation loss: 2.6852884343433994

Epoch: 61| Step: 0
Training loss: 2.636363456241757
Validation loss: 2.6905587073644326

Epoch: 6| Step: 1
Training loss: 3.2632211114786416
Validation loss: 2.690434401363801

Epoch: 6| Step: 2
Training loss: 2.8983792610178267
Validation loss: 2.6869625289577415

Epoch: 6| Step: 3
Training loss: 3.2047939511627233
Validation loss: 2.6850759931296113

Epoch: 6| Step: 4
Training loss: 2.196699796937162
Validation loss: 2.6842001010405703

Epoch: 6| Step: 5
Training loss: 3.768699815204063
Validation loss: 2.6855363800350895

Epoch: 6| Step: 6
Training loss: 2.679997284446593
Validation loss: 2.686499865277283

Epoch: 6| Step: 7
Training loss: 2.856665656565514
Validation loss: 2.6837679120182036

Epoch: 6| Step: 8
Training loss: 2.6366120493456906
Validation loss: 2.683249078078894

Epoch: 6| Step: 9
Training loss: 3.5906480753582937
Validation loss: 2.682576902766389

Epoch: 6| Step: 10
Training loss: 2.6995715119184056
Validation loss: 2.684598928493214

Epoch: 6| Step: 11
Training loss: 2.8563115954278837
Validation loss: 2.6852136317253343

Epoch: 6| Step: 12
Training loss: 3.5773341208547245
Validation loss: 2.7007634701045493

Epoch: 6| Step: 13
Training loss: 3.295804830846667
Validation loss: 2.707369260001094

Epoch: 62| Step: 0
Training loss: 3.29966552657694
Validation loss: 2.7226349227803706

Epoch: 6| Step: 1
Training loss: 2.6306104195322817
Validation loss: 2.731877385486007

Epoch: 6| Step: 2
Training loss: 3.136639395692211
Validation loss: 2.750152139939271

Epoch: 6| Step: 3
Training loss: 3.441495619272694
Validation loss: 2.7458452122675787

Epoch: 6| Step: 4
Training loss: 3.2658073702908736
Validation loss: 2.692156199641227

Epoch: 6| Step: 5
Training loss: 2.312243060710045
Validation loss: 2.6822055827129105

Epoch: 6| Step: 6
Training loss: 3.4840613253919335
Validation loss: 2.682039889449949

Epoch: 6| Step: 7
Training loss: 2.9842692251584038
Validation loss: 2.6809310990775423

Epoch: 6| Step: 8
Training loss: 2.9151246126998345
Validation loss: 2.6898580500367197

Epoch: 6| Step: 9
Training loss: 3.1801549019663318
Validation loss: 2.7057865546563518

Epoch: 6| Step: 10
Training loss: 2.9535438200239654
Validation loss: 2.7033193021581643

Epoch: 6| Step: 11
Training loss: 3.1292000583809103
Validation loss: 2.685542208879133

Epoch: 6| Step: 12
Training loss: 2.6823889458555676
Validation loss: 2.682934112662065

Epoch: 6| Step: 13
Training loss: 3.118478913843091
Validation loss: 2.68352598771168

Epoch: 63| Step: 0
Training loss: 2.5954801005815185
Validation loss: 2.683736489714439

Epoch: 6| Step: 1
Training loss: 3.1450208578949423
Validation loss: 2.683442293211024

Epoch: 6| Step: 2
Training loss: 3.0611712628881813
Validation loss: 2.684613523811921

Epoch: 6| Step: 3
Training loss: 3.0778298793228074
Validation loss: 2.68172451243261

Epoch: 6| Step: 4
Training loss: 3.0352872724495032
Validation loss: 2.6835290313767945

Epoch: 6| Step: 5
Training loss: 2.7161054355229735
Validation loss: 2.6839350803795425

Epoch: 6| Step: 6
Training loss: 3.1266484299746726
Validation loss: 2.6815322796610053

Epoch: 6| Step: 7
Training loss: 2.9423995044436095
Validation loss: 2.689485957510908

Epoch: 6| Step: 8
Training loss: 2.7277697319832326
Validation loss: 2.7085792824234547

Epoch: 6| Step: 9
Training loss: 2.6401648487294875
Validation loss: 2.7171625612045873

Epoch: 6| Step: 10
Training loss: 2.7478815068291667
Validation loss: 2.729036998948298

Epoch: 6| Step: 11
Training loss: 3.5721496262860666
Validation loss: 2.7097870597128835

Epoch: 6| Step: 12
Training loss: 3.753570382835053
Validation loss: 2.704905078245964

Epoch: 6| Step: 13
Training loss: 3.104242123363032
Validation loss: 2.684923004384974

Epoch: 64| Step: 0
Training loss: 2.7431712763288467
Validation loss: 2.68335891902004

Epoch: 6| Step: 1
Training loss: 3.2395536824103197
Validation loss: 2.679537688074065

Epoch: 6| Step: 2
Training loss: 2.772017510329919
Validation loss: 2.6777015563691844

Epoch: 6| Step: 3
Training loss: 3.4869740462129055
Validation loss: 2.6791817458005647

Epoch: 6| Step: 4
Training loss: 2.8802329827484634
Validation loss: 2.6777719657634154

Epoch: 6| Step: 5
Training loss: 3.2329641019914876
Validation loss: 2.680159183192445

Epoch: 6| Step: 6
Training loss: 3.237736967980587
Validation loss: 2.6786351080228274

Epoch: 6| Step: 7
Training loss: 3.29052846681842
Validation loss: 2.6820030304806894

Epoch: 6| Step: 8
Training loss: 3.0145176252696753
Validation loss: 2.6786471747310676

Epoch: 6| Step: 9
Training loss: 2.482965609401037
Validation loss: 2.679075348814669

Epoch: 6| Step: 10
Training loss: 2.8581215033894565
Validation loss: 2.6775326612492747

Epoch: 6| Step: 11
Training loss: 2.6252471262544925
Validation loss: 2.675009750403643

Epoch: 6| Step: 12
Training loss: 3.278808848252503
Validation loss: 2.6728721820265657

Epoch: 6| Step: 13
Training loss: 3.096519848642128
Validation loss: 2.670590980474973

Epoch: 65| Step: 0
Training loss: 3.1125044673290136
Validation loss: 2.667885446699055

Epoch: 6| Step: 1
Training loss: 2.8411368635941034
Validation loss: 2.668048050611641

Epoch: 6| Step: 2
Training loss: 3.3664494743413953
Validation loss: 2.66449016116402

Epoch: 6| Step: 3
Training loss: 3.5334336272586455
Validation loss: 2.6668013768113417

Epoch: 6| Step: 4
Training loss: 3.1447101802741573
Validation loss: 2.6689135384606524

Epoch: 6| Step: 5
Training loss: 3.45715331815738
Validation loss: 2.6787914465467018

Epoch: 6| Step: 6
Training loss: 3.066128352505101
Validation loss: 2.6651158829088804

Epoch: 6| Step: 7
Training loss: 2.840051446636743
Validation loss: 2.665459418405474

Epoch: 6| Step: 8
Training loss: 2.802504090740883
Validation loss: 2.667317188682012

Epoch: 6| Step: 9
Training loss: 2.9816133993418337
Validation loss: 2.6687289910420007

Epoch: 6| Step: 10
Training loss: 3.0396618092799272
Validation loss: 2.6703642070143383

Epoch: 6| Step: 11
Training loss: 2.8677003490162716
Validation loss: 2.668407151630893

Epoch: 6| Step: 12
Training loss: 2.2982937330228164
Validation loss: 2.6708290649746496

Epoch: 6| Step: 13
Training loss: 2.3705899301917372
Validation loss: 2.664859636952261

Epoch: 66| Step: 0
Training loss: 3.0379150107112425
Validation loss: 2.664255448635326

Epoch: 6| Step: 1
Training loss: 2.6074046845173022
Validation loss: 2.6648200910204296

Epoch: 6| Step: 2
Training loss: 3.1464202703839965
Validation loss: 2.662750720857334

Epoch: 6| Step: 3
Training loss: 3.092877659970316
Validation loss: 2.6591665760887024

Epoch: 6| Step: 4
Training loss: 3.0036100283683047
Validation loss: 2.6599708492400502

Epoch: 6| Step: 5
Training loss: 3.2207099354650555
Validation loss: 2.6619639486613487

Epoch: 6| Step: 6
Training loss: 3.048191666381489
Validation loss: 2.6575644052594676

Epoch: 6| Step: 7
Training loss: 3.0604138178040654
Validation loss: 2.6586378392655883

Epoch: 6| Step: 8
Training loss: 3.2627086119621937
Validation loss: 2.656430492506066

Epoch: 6| Step: 9
Training loss: 2.8206308454659648
Validation loss: 2.6574966304215426

Epoch: 6| Step: 10
Training loss: 2.9970699942978
Validation loss: 2.6574269736401783

Epoch: 6| Step: 11
Training loss: 2.723606813894156
Validation loss: 2.6635829604729446

Epoch: 6| Step: 12
Training loss: 3.463710664149274
Validation loss: 2.66402360142374

Epoch: 6| Step: 13
Training loss: 2.152757109949582
Validation loss: 2.6554059554549774

Epoch: 67| Step: 0
Training loss: 3.1314688295813595
Validation loss: 2.6533876307317734

Epoch: 6| Step: 1
Training loss: 3.1394599537610812
Validation loss: 2.6530538131617885

Epoch: 6| Step: 2
Training loss: 3.642430721297008
Validation loss: 2.653483587709117

Epoch: 6| Step: 3
Training loss: 2.94143024247353
Validation loss: 2.6529936401356635

Epoch: 6| Step: 4
Training loss: 2.824043490088482
Validation loss: 2.652056596555783

Epoch: 6| Step: 5
Training loss: 3.4134724890635564
Validation loss: 2.6512456089758585

Epoch: 6| Step: 6
Training loss: 2.6636446180402142
Validation loss: 2.6516190350279523

Epoch: 6| Step: 7
Training loss: 2.8342730889720373
Validation loss: 2.6518694820154605

Epoch: 6| Step: 8
Training loss: 2.183059108295627
Validation loss: 2.650029741392371

Epoch: 6| Step: 9
Training loss: 3.3648030275281346
Validation loss: 2.6499460086364532

Epoch: 6| Step: 10
Training loss: 3.056318311181395
Validation loss: 2.650324478452677

Epoch: 6| Step: 11
Training loss: 3.1335795312566965
Validation loss: 2.65011206204401

Epoch: 6| Step: 12
Training loss: 2.760860584960092
Validation loss: 2.6492513638538893

Epoch: 6| Step: 13
Training loss: 2.4187390112812097
Validation loss: 2.649403130244424

Epoch: 68| Step: 0
Training loss: 3.0366676917076125
Validation loss: 2.649230215015206

Epoch: 6| Step: 1
Training loss: 2.710350742657466
Validation loss: 2.6505453235513277

Epoch: 6| Step: 2
Training loss: 3.2362945315471325
Validation loss: 2.6499838591146165

Epoch: 6| Step: 3
Training loss: 2.563006095221122
Validation loss: 2.6499189562177348

Epoch: 6| Step: 4
Training loss: 3.0694050697651174
Validation loss: 2.6457506628268717

Epoch: 6| Step: 5
Training loss: 2.8515663460483482
Validation loss: 2.648744253251358

Epoch: 6| Step: 6
Training loss: 3.8682487495857703
Validation loss: 2.6496125247353146

Epoch: 6| Step: 7
Training loss: 2.9960245176669407
Validation loss: 2.650393789902212

Epoch: 6| Step: 8
Training loss: 2.4274821683658923
Validation loss: 2.6492769570614447

Epoch: 6| Step: 9
Training loss: 3.2126457329731863
Validation loss: 2.6530124572169784

Epoch: 6| Step: 10
Training loss: 3.262811059848251
Validation loss: 2.66122464459102

Epoch: 6| Step: 11
Training loss: 2.7999325880382537
Validation loss: 2.6576343729089706

Epoch: 6| Step: 12
Training loss: 2.989331348889271
Validation loss: 2.652901622656937

Epoch: 6| Step: 13
Training loss: 2.4725090091491047
Validation loss: 2.652212057729105

Epoch: 69| Step: 0
Training loss: 2.8464671139005944
Validation loss: 2.647706839734391

Epoch: 6| Step: 1
Training loss: 2.9981047683742537
Validation loss: 2.646086685444107

Epoch: 6| Step: 2
Training loss: 3.32578306227194
Validation loss: 2.643976515531763

Epoch: 6| Step: 3
Training loss: 2.757020658330551
Validation loss: 2.6410873580133387

Epoch: 6| Step: 4
Training loss: 3.241655935810702
Validation loss: 2.6427163317280327

Epoch: 6| Step: 5
Training loss: 3.221085377084831
Validation loss: 2.645089882587861

Epoch: 6| Step: 6
Training loss: 3.2082948228250494
Validation loss: 2.648780417474575

Epoch: 6| Step: 7
Training loss: 2.871538151193497
Validation loss: 2.6638962445965624

Epoch: 6| Step: 8
Training loss: 3.0537146850992354
Validation loss: 2.6786792304890894

Epoch: 6| Step: 9
Training loss: 2.4073027252467076
Validation loss: 2.6682334329840263

Epoch: 6| Step: 10
Training loss: 2.6521925510912596
Validation loss: 2.676772350968509

Epoch: 6| Step: 11
Training loss: 2.685707825432634
Validation loss: 2.6874685970073324

Epoch: 6| Step: 12
Training loss: 3.327132959220674
Validation loss: 2.689268022361301

Epoch: 6| Step: 13
Training loss: 3.502379289898171
Validation loss: 2.6853265790479948

Epoch: 70| Step: 0
Training loss: 3.412260160297027
Validation loss: 2.672127194206183

Epoch: 6| Step: 1
Training loss: 2.849366730052329
Validation loss: 2.6667724083372857

Epoch: 6| Step: 2
Training loss: 2.4095563095488592
Validation loss: 2.668803459366528

Epoch: 6| Step: 3
Training loss: 3.1211774716946894
Validation loss: 2.6682793530990696

Epoch: 6| Step: 4
Training loss: 3.185501519306559
Validation loss: 2.66509671937731

Epoch: 6| Step: 5
Training loss: 3.1640407985366887
Validation loss: 2.7033025970245164

Epoch: 6| Step: 6
Training loss: 3.021280786107272
Validation loss: 2.7346918825451407

Epoch: 6| Step: 7
Training loss: 3.2568451613906113
Validation loss: 2.723484474702193

Epoch: 6| Step: 8
Training loss: 3.393882206542349
Validation loss: 2.7094366289029748

Epoch: 6| Step: 9
Training loss: 3.050096579101341
Validation loss: 2.699148686357749

Epoch: 6| Step: 10
Training loss: 2.4619564792681747
Validation loss: 2.697494343283134

Epoch: 6| Step: 11
Training loss: 2.7883482801268964
Validation loss: 2.6929766691309514

Epoch: 6| Step: 12
Training loss: 2.996108710339931
Validation loss: 2.6905045099415594

Epoch: 6| Step: 13
Training loss: 3.1239031583402177
Validation loss: 2.6823274869087133

Epoch: 71| Step: 0
Training loss: 2.592913757090464
Validation loss: 2.681764628339552

Epoch: 6| Step: 1
Training loss: 2.9974436995028704
Validation loss: 2.6721039430296503

Epoch: 6| Step: 2
Training loss: 2.8889429095507504
Validation loss: 2.665754595516238

Epoch: 6| Step: 3
Training loss: 3.1871633912913984
Validation loss: 2.6599808022315727

Epoch: 6| Step: 4
Training loss: 2.8740693949668232
Validation loss: 2.651068795950163

Epoch: 6| Step: 5
Training loss: 2.1933306779623143
Validation loss: 2.6464231894728214

Epoch: 6| Step: 6
Training loss: 3.251907669070446
Validation loss: 2.6420425393611695

Epoch: 6| Step: 7
Training loss: 3.0304953038743023
Validation loss: 2.6378636138343565

Epoch: 6| Step: 8
Training loss: 2.5718551690598983
Validation loss: 2.6350123364301288

Epoch: 6| Step: 9
Training loss: 2.8328022926681107
Validation loss: 2.637823965921183

Epoch: 6| Step: 10
Training loss: 3.58657239558427
Validation loss: 2.6356130579734782

Epoch: 6| Step: 11
Training loss: 3.2127205383360518
Validation loss: 2.633002418602793

Epoch: 6| Step: 12
Training loss: 3.0493598391247296
Validation loss: 2.638952504819485

Epoch: 6| Step: 13
Training loss: 3.5946020028592818
Validation loss: 2.6405319777077687

Epoch: 72| Step: 0
Training loss: 2.867478192220706
Validation loss: 2.63104207801086

Epoch: 6| Step: 1
Training loss: 2.889068844480341
Validation loss: 2.6309107533433522

Epoch: 6| Step: 2
Training loss: 2.8136756135385323
Validation loss: 2.631263520437907

Epoch: 6| Step: 3
Training loss: 2.3989661811394365
Validation loss: 2.631421856023395

Epoch: 6| Step: 4
Training loss: 2.745585279205624
Validation loss: 2.629652163878096

Epoch: 6| Step: 5
Training loss: 2.9520570145867113
Validation loss: 2.6290214603654545

Epoch: 6| Step: 6
Training loss: 3.1060190157139353
Validation loss: 2.6341339346304973

Epoch: 6| Step: 7
Training loss: 3.280425921908004
Validation loss: 2.6530470683930645

Epoch: 6| Step: 8
Training loss: 2.902403552336448
Validation loss: 2.6545468501166627

Epoch: 6| Step: 9
Training loss: 3.153934649286928
Validation loss: 2.6800014149700817

Epoch: 6| Step: 10
Training loss: 3.0713860961917314
Validation loss: 2.6574069404416636

Epoch: 6| Step: 11
Training loss: 3.163668233537539
Validation loss: 2.633569701451071

Epoch: 6| Step: 12
Training loss: 3.1373392253884713
Validation loss: 2.6266659900955287

Epoch: 6| Step: 13
Training loss: 3.451785280801206
Validation loss: 2.62686005577792

Epoch: 73| Step: 0
Training loss: 2.4549385704854565
Validation loss: 2.6268492824263108

Epoch: 6| Step: 1
Training loss: 3.402632866366692
Validation loss: 2.62619880690684

Epoch: 6| Step: 2
Training loss: 2.946648980025754
Validation loss: 2.626625479695971

Epoch: 6| Step: 3
Training loss: 2.9281289799845385
Validation loss: 2.6282214610194043

Epoch: 6| Step: 4
Training loss: 2.978087187423907
Validation loss: 2.625850490880939

Epoch: 6| Step: 5
Training loss: 2.5605347703637418
Validation loss: 2.626226255946157

Epoch: 6| Step: 6
Training loss: 2.8242081131477597
Validation loss: 2.6270165849864284

Epoch: 6| Step: 7
Training loss: 3.264546485470584
Validation loss: 2.628754136031335

Epoch: 6| Step: 8
Training loss: 3.271212383749292
Validation loss: 2.6277609630915877

Epoch: 6| Step: 9
Training loss: 2.9011313369197227
Validation loss: 2.6309852212192792

Epoch: 6| Step: 10
Training loss: 1.9257888020024971
Validation loss: 2.6325540416653523

Epoch: 6| Step: 11
Training loss: 3.32014130992509
Validation loss: 2.639232915384735

Epoch: 6| Step: 12
Training loss: 3.226624273489324
Validation loss: 2.654893775725911

Epoch: 6| Step: 13
Training loss: 3.6459226906134714
Validation loss: 2.668495035630331

Epoch: 74| Step: 0
Training loss: 3.0586894715824515
Validation loss: 2.6363925915933732

Epoch: 6| Step: 1
Training loss: 2.1060301080129253
Validation loss: 2.626325601713878

Epoch: 6| Step: 2
Training loss: 2.764334857260235
Validation loss: 2.6272707257568357

Epoch: 6| Step: 3
Training loss: 3.042627121989613
Validation loss: 2.6250601564810756

Epoch: 6| Step: 4
Training loss: 3.080759692692478
Validation loss: 2.622944933229849

Epoch: 6| Step: 5
Training loss: 2.49289054879452
Validation loss: 2.6246531404328417

Epoch: 6| Step: 6
Training loss: 2.3438391096341458
Validation loss: 2.6273347186415297

Epoch: 6| Step: 7
Training loss: 2.701697352911861
Validation loss: 2.6238368397460414

Epoch: 6| Step: 8
Training loss: 3.851227472989106
Validation loss: 2.6256126322746214

Epoch: 6| Step: 9
Training loss: 3.2636281894611416
Validation loss: 2.625266249631949

Epoch: 6| Step: 10
Training loss: 2.741484723146137
Validation loss: 2.6297874225170177

Epoch: 6| Step: 11
Training loss: 3.016078302181516
Validation loss: 2.6309002830628962

Epoch: 6| Step: 12
Training loss: 3.607793816691148
Validation loss: 2.639691285760247

Epoch: 6| Step: 13
Training loss: 3.2176757797680695
Validation loss: 2.643233853492641

Epoch: 75| Step: 0
Training loss: 2.942441152860889
Validation loss: 2.642759395074068

Epoch: 6| Step: 1
Training loss: 2.7024483247204607
Validation loss: 2.626469482567607

Epoch: 6| Step: 2
Training loss: 2.151124141365096
Validation loss: 2.617294003250083

Epoch: 6| Step: 3
Training loss: 3.3252198340230197
Validation loss: 2.6168393893661777

Epoch: 6| Step: 4
Training loss: 3.2484587903079674
Validation loss: 2.620079405551273

Epoch: 6| Step: 5
Training loss: 3.017921798542285
Validation loss: 2.6176142920514165

Epoch: 6| Step: 6
Training loss: 3.372584573577454
Validation loss: 2.621761848358389

Epoch: 6| Step: 7
Training loss: 3.292276221384759
Validation loss: 2.6196345994087356

Epoch: 6| Step: 8
Training loss: 3.1184218790221383
Validation loss: 2.6200636865580758

Epoch: 6| Step: 9
Training loss: 3.064353537658435
Validation loss: 2.620888679580334

Epoch: 6| Step: 10
Training loss: 2.319794773692971
Validation loss: 2.623328306060893

Epoch: 6| Step: 11
Training loss: 2.440898384676512
Validation loss: 2.63261192796606

Epoch: 6| Step: 12
Training loss: 2.959801287295875
Validation loss: 2.632822289833127

Epoch: 6| Step: 13
Training loss: 3.3927608390276855
Validation loss: 2.6297268594437213

Epoch: 76| Step: 0
Training loss: 2.774090069574405
Validation loss: 2.635705998060678

Epoch: 6| Step: 1
Training loss: 2.30791803014104
Validation loss: 2.6514963194156103

Epoch: 6| Step: 2
Training loss: 2.9540324759483467
Validation loss: 2.662028469289679

Epoch: 6| Step: 3
Training loss: 3.0658496521618024
Validation loss: 2.636919636732424

Epoch: 6| Step: 4
Training loss: 3.118327838098389
Validation loss: 2.608112719833975

Epoch: 6| Step: 5
Training loss: 3.1263058794923504
Validation loss: 2.6135805834333032

Epoch: 6| Step: 6
Training loss: 2.9361994684839763
Validation loss: 2.61669564716705

Epoch: 6| Step: 7
Training loss: 3.16319040595275
Validation loss: 2.616073185555886

Epoch: 6| Step: 8
Training loss: 2.896454906763871
Validation loss: 2.620129050140668

Epoch: 6| Step: 9
Training loss: 3.3793081408392145
Validation loss: 2.6169580169506004

Epoch: 6| Step: 10
Training loss: 2.819558166899694
Validation loss: 2.6232231325927895

Epoch: 6| Step: 11
Training loss: 2.9470200180342068
Validation loss: 2.6297774293404355

Epoch: 6| Step: 12
Training loss: 2.9483002501838875
Validation loss: 2.61876828765377

Epoch: 6| Step: 13
Training loss: 3.340397375075769
Validation loss: 2.6123917807811123

Epoch: 77| Step: 0
Training loss: 2.9942892078776553
Validation loss: 2.6088581345770585

Epoch: 6| Step: 1
Training loss: 3.4526434515141715
Validation loss: 2.612287167020874

Epoch: 6| Step: 2
Training loss: 2.6984516825963154
Validation loss: 2.6119440499009916

Epoch: 6| Step: 3
Training loss: 3.3688707149583528
Validation loss: 2.6119839664927507

Epoch: 6| Step: 4
Training loss: 3.2653063750996782
Validation loss: 2.6137528390906692

Epoch: 6| Step: 5
Training loss: 2.360428833712683
Validation loss: 2.6094992120947986

Epoch: 6| Step: 6
Training loss: 3.165935498870407
Validation loss: 2.614931786786136

Epoch: 6| Step: 7
Training loss: 2.8692167413533443
Validation loss: 2.609614659366769

Epoch: 6| Step: 8
Training loss: 2.7459989398418028
Validation loss: 2.6135868562309716

Epoch: 6| Step: 9
Training loss: 2.660533189325302
Validation loss: 2.620827287771023

Epoch: 6| Step: 10
Training loss: 2.4432010980534913
Validation loss: 2.621723704839962

Epoch: 6| Step: 11
Training loss: 2.7440891899094195
Validation loss: 2.6311829942665566

Epoch: 6| Step: 12
Training loss: 3.1690315317142277
Validation loss: 2.6313462167670543

Epoch: 6| Step: 13
Training loss: 3.531104295813506
Validation loss: 2.6299108469613532

Epoch: 78| Step: 0
Training loss: 2.5305559125058585
Validation loss: 2.6199071362860296

Epoch: 6| Step: 1
Training loss: 2.9106933098186833
Validation loss: 2.6114967900337582

Epoch: 6| Step: 2
Training loss: 3.130143776903373
Validation loss: 2.6084363237753907

Epoch: 6| Step: 3
Training loss: 2.947370301063196
Validation loss: 2.5999186856086256

Epoch: 6| Step: 4
Training loss: 2.536177278230958
Validation loss: 2.600432061430647

Epoch: 6| Step: 5
Training loss: 2.7929155678120674
Validation loss: 2.600250217665187

Epoch: 6| Step: 6
Training loss: 2.873969349431578
Validation loss: 2.599827889850544

Epoch: 6| Step: 7
Training loss: 2.6945684760561868
Validation loss: 2.5991643107677374

Epoch: 6| Step: 8
Training loss: 3.2745263375938363
Validation loss: 2.599980400538503

Epoch: 6| Step: 9
Training loss: 3.1459049612484353
Validation loss: 2.598656345184776

Epoch: 6| Step: 10
Training loss: 2.638204153356058
Validation loss: 2.597416172225839

Epoch: 6| Step: 11
Training loss: 3.3459273049984923
Validation loss: 2.596389478161793

Epoch: 6| Step: 12
Training loss: 3.471840246594877
Validation loss: 2.599084910762573

Epoch: 6| Step: 13
Training loss: 2.9517788518447237
Validation loss: 2.597577965520782

Epoch: 79| Step: 0
Training loss: 3.145986052252425
Validation loss: 2.5952960310486892

Epoch: 6| Step: 1
Training loss: 2.4296301930038364
Validation loss: 2.5970996922618137

Epoch: 6| Step: 2
Training loss: 3.3591724423398523
Validation loss: 2.5989886799208137

Epoch: 6| Step: 3
Training loss: 2.8731527613687953
Validation loss: 2.6034399125175702

Epoch: 6| Step: 4
Training loss: 3.125741489179445
Validation loss: 2.6031817562700112

Epoch: 6| Step: 5
Training loss: 2.859626467127156
Validation loss: 2.6062305818290126

Epoch: 6| Step: 6
Training loss: 2.6894509974120124
Validation loss: 2.602331930963967

Epoch: 6| Step: 7
Training loss: 2.56256819843628
Validation loss: 2.600094968963949

Epoch: 6| Step: 8
Training loss: 2.69175569566188
Validation loss: 2.598961574019679

Epoch: 6| Step: 9
Training loss: 2.653762101061808
Validation loss: 2.595291689651838

Epoch: 6| Step: 10
Training loss: 3.527340370421623
Validation loss: 2.5968845116360018

Epoch: 6| Step: 11
Training loss: 2.8869257170076303
Validation loss: 2.598515578987539

Epoch: 6| Step: 12
Training loss: 3.143584873117344
Validation loss: 2.5969236221738115

Epoch: 6| Step: 13
Training loss: 3.3361964962764974
Validation loss: 2.596624857537652

Epoch: 80| Step: 0
Training loss: 2.598695016694903
Validation loss: 2.6225206382673956

Epoch: 6| Step: 1
Training loss: 2.6361531873193864
Validation loss: 2.6445065434473953

Epoch: 6| Step: 2
Training loss: 3.1967250993921863
Validation loss: 2.6706217688558

Epoch: 6| Step: 3
Training loss: 2.7450199116747265
Validation loss: 2.6278124585979565

Epoch: 6| Step: 4
Training loss: 2.5777866025843053
Validation loss: 2.6083584139463127

Epoch: 6| Step: 5
Training loss: 2.519728825828861
Validation loss: 2.6029200494300393

Epoch: 6| Step: 6
Training loss: 3.1448278441258837
Validation loss: 2.60281105115522

Epoch: 6| Step: 7
Training loss: 3.117381766248415
Validation loss: 2.5968634753368054

Epoch: 6| Step: 8
Training loss: 2.7620864903901015
Validation loss: 2.59776880015984

Epoch: 6| Step: 9
Training loss: 2.012912670024404
Validation loss: 2.5993037705184285

Epoch: 6| Step: 10
Training loss: 3.6820103791032053
Validation loss: 2.5985212399710877

Epoch: 6| Step: 11
Training loss: 3.057557457846414
Validation loss: 2.5969769660708177

Epoch: 6| Step: 12
Training loss: 3.4285340335486914
Validation loss: 2.598584557202096

Epoch: 6| Step: 13
Training loss: 3.823141037734441
Validation loss: 2.596748996341804

Epoch: 81| Step: 0
Training loss: 2.726207021368362
Validation loss: 2.59690630493984

Epoch: 6| Step: 1
Training loss: 3.514130134660997
Validation loss: 2.595126339751702

Epoch: 6| Step: 2
Training loss: 3.673551136305279
Validation loss: 2.5966186345954965

Epoch: 6| Step: 3
Training loss: 2.629624335289144
Validation loss: 2.5905286540674415

Epoch: 6| Step: 4
Training loss: 2.6158977917760935
Validation loss: 2.5895837017781718

Epoch: 6| Step: 5
Training loss: 2.956773713964811
Validation loss: 2.588709222033679

Epoch: 6| Step: 6
Training loss: 2.8202475754299923
Validation loss: 2.586937744399889

Epoch: 6| Step: 7
Training loss: 3.162671496901702
Validation loss: 2.586563901443517

Epoch: 6| Step: 8
Training loss: 2.8890049699137803
Validation loss: 2.591166040782116

Epoch: 6| Step: 9
Training loss: 3.3652346583901505
Validation loss: 2.5926713891302753

Epoch: 6| Step: 10
Training loss: 2.671655032377876
Validation loss: 2.5932786129931134

Epoch: 6| Step: 11
Training loss: 2.1444216285023274
Validation loss: 2.5989952892809733

Epoch: 6| Step: 12
Training loss: 2.9804793091433206
Validation loss: 2.6151257086276885

Epoch: 6| Step: 13
Training loss: 2.5877494802311376
Validation loss: 2.6336709039282837

Epoch: 82| Step: 0
Training loss: 3.19026931439022
Validation loss: 2.662929031196749

Epoch: 6| Step: 1
Training loss: 2.356391384426174
Validation loss: 2.655884397030685

Epoch: 6| Step: 2
Training loss: 2.913488091112724
Validation loss: 2.6580131667648286

Epoch: 6| Step: 3
Training loss: 3.522824659148048
Validation loss: 2.6687303051730114

Epoch: 6| Step: 4
Training loss: 2.841407230052931
Validation loss: 2.6329273018586488

Epoch: 6| Step: 5
Training loss: 3.2784062733088914
Validation loss: 2.6106505611458393

Epoch: 6| Step: 6
Training loss: 3.16589091653679
Validation loss: 2.5965283889039235

Epoch: 6| Step: 7
Training loss: 3.1591345570095606
Validation loss: 2.5899938771462976

Epoch: 6| Step: 8
Training loss: 2.6036987698148852
Validation loss: 2.590541707130853

Epoch: 6| Step: 9
Training loss: 3.021002999287505
Validation loss: 2.582428134177604

Epoch: 6| Step: 10
Training loss: 2.545230451541203
Validation loss: 2.581931113387524

Epoch: 6| Step: 11
Training loss: 3.3304415238851672
Validation loss: 2.578694698946531

Epoch: 6| Step: 12
Training loss: 2.6420703070134506
Validation loss: 2.5765179201542185

Epoch: 6| Step: 13
Training loss: 2.2185226108997047
Validation loss: 2.579459848693959

Epoch: 83| Step: 0
Training loss: 3.2290233621280824
Validation loss: 2.5754437497218796

Epoch: 6| Step: 1
Training loss: 3.0501738076637546
Validation loss: 2.5775484267558655

Epoch: 6| Step: 2
Training loss: 3.058076427455211
Validation loss: 2.5736184845453307

Epoch: 6| Step: 3
Training loss: 2.6011699575109786
Validation loss: 2.5747988885863466

Epoch: 6| Step: 4
Training loss: 3.270282107675382
Validation loss: 2.5895622359146615

Epoch: 6| Step: 5
Training loss: 2.6863503769572428
Validation loss: 2.5860459346136278

Epoch: 6| Step: 6
Training loss: 3.055933237335169
Validation loss: 2.596858187853134

Epoch: 6| Step: 7
Training loss: 3.079523224032174
Validation loss: 2.5964797005419333

Epoch: 6| Step: 8
Training loss: 3.0422127290139027
Validation loss: 2.5962756181213402

Epoch: 6| Step: 9
Training loss: 3.0897411228116916
Validation loss: 2.608018667162787

Epoch: 6| Step: 10
Training loss: 2.4164592610071916
Validation loss: 2.6174708989701325

Epoch: 6| Step: 11
Training loss: 2.7029132208800477
Validation loss: 2.6157275787416663

Epoch: 6| Step: 12
Training loss: 2.7819736696501476
Validation loss: 2.6120609830125594

Epoch: 6| Step: 13
Training loss: 3.108109413106355
Validation loss: 2.5998967252750056

Epoch: 84| Step: 0
Training loss: 3.1326870786147043
Validation loss: 2.5849399945844542

Epoch: 6| Step: 1
Training loss: 2.458656828549631
Validation loss: 2.5860540011027173

Epoch: 6| Step: 2
Training loss: 3.320780929732699
Validation loss: 2.592324949812718

Epoch: 6| Step: 3
Training loss: 1.9444587396671456
Validation loss: 2.591293764253677

Epoch: 6| Step: 4
Training loss: 2.547016163289815
Validation loss: 2.5831682226533412

Epoch: 6| Step: 5
Training loss: 2.513499149532565
Validation loss: 2.5853471485172803

Epoch: 6| Step: 6
Training loss: 3.19482686798998
Validation loss: 2.582933281465774

Epoch: 6| Step: 7
Training loss: 2.7749215467205737
Validation loss: 2.5747228892471807

Epoch: 6| Step: 8
Training loss: 2.6641270246550395
Validation loss: 2.570438662558075

Epoch: 6| Step: 9
Training loss: 3.159315527894826
Validation loss: 2.568352262082453

Epoch: 6| Step: 10
Training loss: 3.455564163895079
Validation loss: 2.5678427279017124

Epoch: 6| Step: 11
Training loss: 3.4296513514253872
Validation loss: 2.564824694311998

Epoch: 6| Step: 12
Training loss: 3.1926894485554014
Validation loss: 2.563870565439262

Epoch: 6| Step: 13
Training loss: 2.8057629746748303
Validation loss: 2.5664721960103156

Epoch: 85| Step: 0
Training loss: 3.092112714837505
Validation loss: 2.5661417996452265

Epoch: 6| Step: 1
Training loss: 3.5104731131674383
Validation loss: 2.5611594180281947

Epoch: 6| Step: 2
Training loss: 2.900579026393608
Validation loss: 2.55845908736097

Epoch: 6| Step: 3
Training loss: 2.808415477543669
Validation loss: 2.559877405682504

Epoch: 6| Step: 4
Training loss: 3.0181693772501417
Validation loss: 2.5597015344843683

Epoch: 6| Step: 5
Training loss: 2.904459411957414
Validation loss: 2.5575383660920252

Epoch: 6| Step: 6
Training loss: 2.6133039670221976
Validation loss: 2.561061066452122

Epoch: 6| Step: 7
Training loss: 3.080659548998548
Validation loss: 2.558716615934536

Epoch: 6| Step: 8
Training loss: 2.3917513917632034
Validation loss: 2.567494630838838

Epoch: 6| Step: 9
Training loss: 2.974583566194258
Validation loss: 2.5771965515360304

Epoch: 6| Step: 10
Training loss: 3.283971910939681
Validation loss: 2.5991331365508916

Epoch: 6| Step: 11
Training loss: 2.731859194208666
Validation loss: 2.6113460711130667

Epoch: 6| Step: 12
Training loss: 2.50288615521912
Validation loss: 2.606018776811466

Epoch: 6| Step: 13
Training loss: 3.056072262703505
Validation loss: 2.590218911997695

Epoch: 86| Step: 0
Training loss: 2.853037831373523
Validation loss: 2.586219617801497

Epoch: 6| Step: 1
Training loss: 3.4589920948530453
Validation loss: 2.572973153989373

Epoch: 6| Step: 2
Training loss: 2.9604527220028136
Validation loss: 2.5777566288020495

Epoch: 6| Step: 3
Training loss: 2.8229232919387677
Validation loss: 2.5890587077622143

Epoch: 6| Step: 4
Training loss: 2.985790935374683
Validation loss: 2.5792504303781554

Epoch: 6| Step: 5
Training loss: 2.509126788223322
Validation loss: 2.581834159280203

Epoch: 6| Step: 6
Training loss: 2.9372574726385987
Validation loss: 2.5603458184841674

Epoch: 6| Step: 7
Training loss: 2.495448929145053
Validation loss: 2.5539519807962723

Epoch: 6| Step: 8
Training loss: 2.725580863988918
Validation loss: 2.5541383874979755

Epoch: 6| Step: 9
Training loss: 2.217864813658632
Validation loss: 2.555245320417412

Epoch: 6| Step: 10
Training loss: 3.5380081728360504
Validation loss: 2.54815832494987

Epoch: 6| Step: 11
Training loss: 2.774882539204908
Validation loss: 2.550404811120796

Epoch: 6| Step: 12
Training loss: 2.9309927105586846
Validation loss: 2.549447472727782

Epoch: 6| Step: 13
Training loss: 3.743672946143339
Validation loss: 2.5473338346150114

Epoch: 87| Step: 0
Training loss: 2.557670881094465
Validation loss: 2.548163780390503

Epoch: 6| Step: 1
Training loss: 2.105593760783219
Validation loss: 2.561757735289693

Epoch: 6| Step: 2
Training loss: 3.1713493926426404
Validation loss: 2.576151906529394

Epoch: 6| Step: 3
Training loss: 2.8279978686679432
Validation loss: 2.6228733824053907

Epoch: 6| Step: 4
Training loss: 3.110323200665432
Validation loss: 2.6384632620816078

Epoch: 6| Step: 5
Training loss: 3.168131456065641
Validation loss: 2.6700250131128964

Epoch: 6| Step: 6
Training loss: 2.7338702798694237
Validation loss: 2.6129830694273353

Epoch: 6| Step: 7
Training loss: 2.8984213774610557
Validation loss: 2.582601983239719

Epoch: 6| Step: 8
Training loss: 3.260092836005969
Validation loss: 2.560002024584721

Epoch: 6| Step: 9
Training loss: 3.0491961123314977
Validation loss: 2.5500012236706224

Epoch: 6| Step: 10
Training loss: 3.0702224213487748
Validation loss: 2.548356161589198

Epoch: 6| Step: 11
Training loss: 3.327889160969787
Validation loss: 2.547893347506978

Epoch: 6| Step: 12
Training loss: 2.8055051499004886
Validation loss: 2.5465558912396817

Epoch: 6| Step: 13
Training loss: 2.4697708723227105
Validation loss: 2.5469056321443557

Epoch: 88| Step: 0
Training loss: 2.3752281681683964
Validation loss: 2.541364708010879

Epoch: 6| Step: 1
Training loss: 2.669839273108435
Validation loss: 2.5424151885996302

Epoch: 6| Step: 2
Training loss: 2.6009727932314806
Validation loss: 2.5451162952126705

Epoch: 6| Step: 3
Training loss: 2.8616619032603725
Validation loss: 2.5475541092743956

Epoch: 6| Step: 4
Training loss: 3.197521537368215
Validation loss: 2.5483388452906164

Epoch: 6| Step: 5
Training loss: 3.316987363048882
Validation loss: 2.550758990247015

Epoch: 6| Step: 6
Training loss: 2.5294804922491387
Validation loss: 2.545431893233186

Epoch: 6| Step: 7
Training loss: 3.060548744614936
Validation loss: 2.546889038771985

Epoch: 6| Step: 8
Training loss: 2.46680538028027
Validation loss: 2.5556430851447285

Epoch: 6| Step: 9
Training loss: 3.531264988690189
Validation loss: 2.575479896047781

Epoch: 6| Step: 10
Training loss: 2.277249944465981
Validation loss: 2.581490210129436

Epoch: 6| Step: 11
Training loss: 3.416512664177445
Validation loss: 2.5989602902072733

Epoch: 6| Step: 12
Training loss: 3.1981864260406163
Validation loss: 2.5862716658655596

Epoch: 6| Step: 13
Training loss: 3.036068419629148
Validation loss: 2.5557033474310784

Epoch: 89| Step: 0
Training loss: 2.684133417096689
Validation loss: 2.549989440955624

Epoch: 6| Step: 1
Training loss: 2.9215245980254343
Validation loss: 2.5480513572199124

Epoch: 6| Step: 2
Training loss: 2.7005131622372263
Validation loss: 2.5481791813205987

Epoch: 6| Step: 3
Training loss: 2.919614519254218
Validation loss: 2.555387153965169

Epoch: 6| Step: 4
Training loss: 2.8469401480865573
Validation loss: 2.548793593245718

Epoch: 6| Step: 5
Training loss: 3.115927993902233
Validation loss: 2.550590154329681

Epoch: 6| Step: 6
Training loss: 2.6943845174183765
Validation loss: 2.5520655996590906

Epoch: 6| Step: 7
Training loss: 2.430620707289617
Validation loss: 2.557455676932318

Epoch: 6| Step: 8
Training loss: 2.1577318047993987
Validation loss: 2.5605982945366073

Epoch: 6| Step: 9
Training loss: 3.2747580111258765
Validation loss: 2.5695403891442083

Epoch: 6| Step: 10
Training loss: 3.5365099596497886
Validation loss: 2.611152162129548

Epoch: 6| Step: 11
Training loss: 3.5796889173154387
Validation loss: 2.5794406043708413

Epoch: 6| Step: 12
Training loss: 3.1470280751156965
Validation loss: 2.559674179306475

Epoch: 6| Step: 13
Training loss: 2.0873785291952935
Validation loss: 2.5486261862607322

Epoch: 90| Step: 0
Training loss: 2.7494455992455005
Validation loss: 2.547092988326972

Epoch: 6| Step: 1
Training loss: 2.891390972205314
Validation loss: 2.546568064335344

Epoch: 6| Step: 2
Training loss: 2.1743689838445714
Validation loss: 2.5442140389226195

Epoch: 6| Step: 3
Training loss: 3.0062035951581842
Validation loss: 2.550150023013775

Epoch: 6| Step: 4
Training loss: 3.1508216633984563
Validation loss: 2.5500978665207916

Epoch: 6| Step: 5
Training loss: 2.778334300135044
Validation loss: 2.560500564695184

Epoch: 6| Step: 6
Training loss: 3.4384062526093975
Validation loss: 2.5549071637322087

Epoch: 6| Step: 7
Training loss: 2.509665496847517
Validation loss: 2.56313185626426

Epoch: 6| Step: 8
Training loss: 3.00500388857674
Validation loss: 2.5713545871747328

Epoch: 6| Step: 9
Training loss: 2.6872726166809957
Validation loss: 2.5619787926175555

Epoch: 6| Step: 10
Training loss: 2.110477865624054
Validation loss: 2.55843371896166

Epoch: 6| Step: 11
Training loss: 2.5540246132640716
Validation loss: 2.554999837766795

Epoch: 6| Step: 12
Training loss: 3.6937533873979813
Validation loss: 2.5553714092276176

Epoch: 6| Step: 13
Training loss: 3.8583497450103947
Validation loss: 2.5450761608961936

Epoch: 91| Step: 0
Training loss: 2.624558820434895
Validation loss: 2.5443837080159297

Epoch: 6| Step: 1
Training loss: 2.7114772644046456
Validation loss: 2.553142201540201

Epoch: 6| Step: 2
Training loss: 3.125931562810673
Validation loss: 2.587887386630341

Epoch: 6| Step: 3
Training loss: 2.197283094548609
Validation loss: 2.6098974366970307

Epoch: 6| Step: 4
Training loss: 3.3685659609544207
Validation loss: 2.5981427112447033

Epoch: 6| Step: 5
Training loss: 3.0506705875816134
Validation loss: 2.582632165823863

Epoch: 6| Step: 6
Training loss: 3.3374641730853707
Validation loss: 2.556363928549378

Epoch: 6| Step: 7
Training loss: 3.3224569419212058
Validation loss: 2.5516471602236543

Epoch: 6| Step: 8
Training loss: 2.685687585092134
Validation loss: 2.5468505046891288

Epoch: 6| Step: 9
Training loss: 3.0540371175632592
Validation loss: 2.5515315487518766

Epoch: 6| Step: 10
Training loss: 2.435633017722802
Validation loss: 2.549866997026798

Epoch: 6| Step: 11
Training loss: 2.8644319436499526
Validation loss: 2.5481954896249857

Epoch: 6| Step: 12
Training loss: 2.884560982606873
Validation loss: 2.5507705281963258

Epoch: 6| Step: 13
Training loss: 2.6432417144227007
Validation loss: 2.5495554482420957

Epoch: 92| Step: 0
Training loss: 2.617366226698252
Validation loss: 2.549485810692778

Epoch: 6| Step: 1
Training loss: 3.245794068973507
Validation loss: 2.5507101704751785

Epoch: 6| Step: 2
Training loss: 3.201581331777598
Validation loss: 2.547174812173266

Epoch: 6| Step: 3
Training loss: 2.76251226569167
Validation loss: 2.5459967701452766

Epoch: 6| Step: 4
Training loss: 2.662121823814207
Validation loss: 2.544361516225023

Epoch: 6| Step: 5
Training loss: 2.855143433289859
Validation loss: 2.555468026052235

Epoch: 6| Step: 6
Training loss: 2.8495631903813896
Validation loss: 2.554824841465858

Epoch: 6| Step: 7
Training loss: 2.567072069262774
Validation loss: 2.554834848357819

Epoch: 6| Step: 8
Training loss: 2.6985181238690714
Validation loss: 2.5585568991891656

Epoch: 6| Step: 9
Training loss: 3.163444552889932
Validation loss: 2.5717508840934395

Epoch: 6| Step: 10
Training loss: 3.2739241427219055
Validation loss: 2.5840201295210967

Epoch: 6| Step: 11
Training loss: 2.5957967197373093
Validation loss: 2.5644968027852086

Epoch: 6| Step: 12
Training loss: 2.948061522754123
Validation loss: 2.550932611772005

Epoch: 6| Step: 13
Training loss: 3.211160396812214
Validation loss: 2.549582760154068

Epoch: 93| Step: 0
Training loss: 3.041836058894676
Validation loss: 2.5477347011049165

Epoch: 6| Step: 1
Training loss: 2.4724319621445767
Validation loss: 2.5439653271858966

Epoch: 6| Step: 2
Training loss: 3.069380834811224
Validation loss: 2.5408404258204165

Epoch: 6| Step: 3
Training loss: 3.1761474398253737
Validation loss: 2.5414110139532102

Epoch: 6| Step: 4
Training loss: 3.1601822030377433
Validation loss: 2.537459122701174

Epoch: 6| Step: 5
Training loss: 3.190413695389595
Validation loss: 2.5440232843375834

Epoch: 6| Step: 6
Training loss: 3.1815920662730908
Validation loss: 2.542411059409019

Epoch: 6| Step: 7
Training loss: 2.388511271781749
Validation loss: 2.5504746437084695

Epoch: 6| Step: 8
Training loss: 2.459597364985875
Validation loss: 2.5558925853462084

Epoch: 6| Step: 9
Training loss: 3.0167797032319132
Validation loss: 2.575450125360232

Epoch: 6| Step: 10
Training loss: 2.986777731442535
Validation loss: 2.5749796574162698

Epoch: 6| Step: 11
Training loss: 2.905789246490449
Validation loss: 2.583178343541232

Epoch: 6| Step: 12
Training loss: 2.633802459875084
Validation loss: 2.573807772835395

Epoch: 6| Step: 13
Training loss: 2.6285020083832826
Validation loss: 2.5742375403980464

Epoch: 94| Step: 0
Training loss: 2.7279325611806478
Validation loss: 2.580574376615345

Epoch: 6| Step: 1
Training loss: 2.52992675479213
Validation loss: 2.585784113466769

Epoch: 6| Step: 2
Training loss: 2.7016353140660456
Validation loss: 2.5823199844655886

Epoch: 6| Step: 3
Training loss: 3.061491352503325
Validation loss: 2.586696883907205

Epoch: 6| Step: 4
Training loss: 2.949490525815847
Validation loss: 2.5753033034953865

Epoch: 6| Step: 5
Training loss: 2.661617286747988
Validation loss: 2.572594026109061

Epoch: 6| Step: 6
Training loss: 2.914528489886379
Validation loss: 2.571005406074671

Epoch: 6| Step: 7
Training loss: 2.800568001900733
Validation loss: 2.5922853634350354

Epoch: 6| Step: 8
Training loss: 3.5357665488824064
Validation loss: 2.5837609631473155

Epoch: 6| Step: 9
Training loss: 3.4569931804047913
Validation loss: 2.5696188394048387

Epoch: 6| Step: 10
Training loss: 3.190500081724552
Validation loss: 2.5421102295788125

Epoch: 6| Step: 11
Training loss: 2.859013602342834
Validation loss: 2.5320782457816686

Epoch: 6| Step: 12
Training loss: 2.424849424898164
Validation loss: 2.535144198063667

Epoch: 6| Step: 13
Training loss: 2.585775687234607
Validation loss: 2.5376632128748073

Epoch: 95| Step: 0
Training loss: 2.8675603390292648
Validation loss: 2.5509745282100478

Epoch: 6| Step: 1
Training loss: 3.6701102703273514
Validation loss: 2.5928939076657547

Epoch: 6| Step: 2
Training loss: 2.9352604871633523
Validation loss: 2.583056009936509

Epoch: 6| Step: 3
Training loss: 2.4334631528147654
Validation loss: 2.602722122867239

Epoch: 6| Step: 4
Training loss: 3.2799893818660157
Validation loss: 2.6586107219694446

Epoch: 6| Step: 5
Training loss: 3.141072711062907
Validation loss: 2.6677176166457195

Epoch: 6| Step: 6
Training loss: 2.306869870130798
Validation loss: 2.645827973502223

Epoch: 6| Step: 7
Training loss: 1.6729256724316706
Validation loss: 2.581098700157242

Epoch: 6| Step: 8
Training loss: 2.5179364502078196
Validation loss: 2.5503920743303605

Epoch: 6| Step: 9
Training loss: 2.9366911728262584
Validation loss: 2.5274322068760977

Epoch: 6| Step: 10
Training loss: 3.3764826025947476
Validation loss: 2.5246118543060736

Epoch: 6| Step: 11
Training loss: 3.039964556738382
Validation loss: 2.5366310683581776

Epoch: 6| Step: 12
Training loss: 2.9438957226882225
Validation loss: 2.5415824164741965

Epoch: 6| Step: 13
Training loss: 3.665003543867659
Validation loss: 2.5513924610125445

Epoch: 96| Step: 0
Training loss: 2.8964081520757126
Validation loss: 2.5756562923556574

Epoch: 6| Step: 1
Training loss: 3.1131365083099967
Validation loss: 2.5809739496569137

Epoch: 6| Step: 2
Training loss: 3.0993373777948867
Validation loss: 2.5707701680686106

Epoch: 6| Step: 3
Training loss: 3.1435138833655736
Validation loss: 2.5579749852017892

Epoch: 6| Step: 4
Training loss: 2.4085305046318064
Validation loss: 2.5469244710522743

Epoch: 6| Step: 5
Training loss: 2.5876429716407245
Validation loss: 2.5319349181903017

Epoch: 6| Step: 6
Training loss: 2.5875418949881936
Validation loss: 2.5368010192646504

Epoch: 6| Step: 7
Training loss: 3.512423536887035
Validation loss: 2.5380602594089017

Epoch: 6| Step: 8
Training loss: 3.013259196874512
Validation loss: 2.5438716464981272

Epoch: 6| Step: 9
Training loss: 2.854906770483382
Validation loss: 2.53528335308031

Epoch: 6| Step: 10
Training loss: 2.803526911846358
Validation loss: 2.5306323933787604

Epoch: 6| Step: 11
Training loss: 2.915423282624689
Validation loss: 2.5311929647465647

Epoch: 6| Step: 12
Training loss: 2.965835747099285
Validation loss: 2.533123816157766

Epoch: 6| Step: 13
Training loss: 2.3634244008491265
Validation loss: 2.5275267808789463

Epoch: 97| Step: 0
Training loss: 2.6631827146251665
Validation loss: 2.5342429800667494

Epoch: 6| Step: 1
Training loss: 3.0067441157525034
Validation loss: 2.5326524913199076

Epoch: 6| Step: 2
Training loss: 3.0342027892285164
Validation loss: 2.529956270780564

Epoch: 6| Step: 3
Training loss: 2.6359855027907213
Validation loss: 2.536109775206703

Epoch: 6| Step: 4
Training loss: 2.7650583839805787
Validation loss: 2.543739808466224

Epoch: 6| Step: 5
Training loss: 3.3375331805576467
Validation loss: 2.558981471965666

Epoch: 6| Step: 6
Training loss: 2.6932483675116194
Validation loss: 2.539889290281243

Epoch: 6| Step: 7
Training loss: 3.078015320658587
Validation loss: 2.5376675730528726

Epoch: 6| Step: 8
Training loss: 2.4964139013679922
Validation loss: 2.519307955320788

Epoch: 6| Step: 9
Training loss: 3.135309228080777
Validation loss: 2.512081415478425

Epoch: 6| Step: 10
Training loss: 2.66870482359624
Validation loss: 2.509176672488539

Epoch: 6| Step: 11
Training loss: 3.618824137453682
Validation loss: 2.5091806653120585

Epoch: 6| Step: 12
Training loss: 2.6318062237422826
Validation loss: 2.5099078642168826

Epoch: 6| Step: 13
Training loss: 2.1752908314909534
Validation loss: 2.5130107974864155

Epoch: 98| Step: 0
Training loss: 2.949396272078572
Validation loss: 2.516236830248791

Epoch: 6| Step: 1
Training loss: 3.203288450955426
Validation loss: 2.5125794769697323

Epoch: 6| Step: 2
Training loss: 2.5138362424710925
Validation loss: 2.5143478383058087

Epoch: 6| Step: 3
Training loss: 2.522379650500163
Validation loss: 2.5152353443427953

Epoch: 6| Step: 4
Training loss: 3.387425191693728
Validation loss: 2.5093965881135816

Epoch: 6| Step: 5
Training loss: 3.166290930157008
Validation loss: 2.5139555443991513

Epoch: 6| Step: 6
Training loss: 3.1204280214887956
Validation loss: 2.50773625243074

Epoch: 6| Step: 7
Training loss: 2.9028911254103775
Validation loss: 2.5127020769152675

Epoch: 6| Step: 8
Training loss: 2.536661180430966
Validation loss: 2.5162599150007385

Epoch: 6| Step: 9
Training loss: 3.1161908920441923
Validation loss: 2.5115124994734823

Epoch: 6| Step: 10
Training loss: 2.4662457089742076
Validation loss: 2.510648659935712

Epoch: 6| Step: 11
Training loss: 2.344233755414424
Validation loss: 2.5118258100718696

Epoch: 6| Step: 12
Training loss: 3.1583622577990096
Validation loss: 2.5126154032780916

Epoch: 6| Step: 13
Training loss: 2.7859497651981746
Validation loss: 2.51658398389798

Epoch: 99| Step: 0
Training loss: 2.565877781634036
Validation loss: 2.522191259992553

Epoch: 6| Step: 1
Training loss: 3.429279416078194
Validation loss: 2.53239175540134

Epoch: 6| Step: 2
Training loss: 3.5334915203696085
Validation loss: 2.530037911175764

Epoch: 6| Step: 3
Training loss: 2.5168783726182937
Validation loss: 2.5479890224094555

Epoch: 6| Step: 4
Training loss: 2.965279890104837
Validation loss: 2.5611090577924434

Epoch: 6| Step: 5
Training loss: 2.7037150009225046
Validation loss: 2.599374219525251

Epoch: 6| Step: 6
Training loss: 2.9689032264117086
Validation loss: 2.6095424119685626

Epoch: 6| Step: 7
Training loss: 2.071260861245722
Validation loss: 2.590246242505824

Epoch: 6| Step: 8
Training loss: 3.2871743624740444
Validation loss: 2.572322199040779

Epoch: 6| Step: 9
Training loss: 3.094478174691982
Validation loss: 2.5532723974027585

Epoch: 6| Step: 10
Training loss: 3.1233488680013184
Validation loss: 2.518321343125708

Epoch: 6| Step: 11
Training loss: 2.7514942617798464
Validation loss: 2.5050585073492453

Epoch: 6| Step: 12
Training loss: 2.11479090782074
Validation loss: 2.5032539090625865

Epoch: 6| Step: 13
Training loss: 2.9208297083710195
Validation loss: 2.5088453726926554

Epoch: 100| Step: 0
Training loss: 2.7365980430421506
Validation loss: 2.5072618166818335

Epoch: 6| Step: 1
Training loss: 3.2502288737769827
Validation loss: 2.5045508341681195

Epoch: 6| Step: 2
Training loss: 2.0792947789578835
Validation loss: 2.5078585248655703

Epoch: 6| Step: 3
Training loss: 3.382966141592483
Validation loss: 2.5061874401889317

Epoch: 6| Step: 4
Training loss: 2.4578755054776975
Validation loss: 2.511638019682915

Epoch: 6| Step: 5
Training loss: 3.230447199425548
Validation loss: 2.521676159512466

Epoch: 6| Step: 6
Training loss: 3.4246313411808353
Validation loss: 2.532264111337051

Epoch: 6| Step: 7
Training loss: 2.316748968441743
Validation loss: 2.531371601505328

Epoch: 6| Step: 8
Training loss: 2.4200882750560404
Validation loss: 2.531813410570425

Epoch: 6| Step: 9
Training loss: 3.0907669327933998
Validation loss: 2.5392332876868418

Epoch: 6| Step: 10
Training loss: 3.150511255100998
Validation loss: 2.54304366094907

Epoch: 6| Step: 11
Training loss: 3.1599274919134444
Validation loss: 2.5514443719994353

Epoch: 6| Step: 12
Training loss: 2.5918767208170004
Validation loss: 2.550494951924164

Epoch: 6| Step: 13
Training loss: 2.8721048456143903
Validation loss: 2.5414843659473627

Testing loss: 2.7296993289082967

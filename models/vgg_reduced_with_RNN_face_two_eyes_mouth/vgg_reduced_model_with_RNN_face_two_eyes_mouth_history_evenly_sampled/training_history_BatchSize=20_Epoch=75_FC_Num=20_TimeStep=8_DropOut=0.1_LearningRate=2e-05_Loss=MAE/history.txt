Epoch: 1| Step: 0
Training loss: 4.265685081481934
Validation loss: 5.21550130331388

Epoch: 5| Step: 1
Training loss: 4.68603515625
Validation loss: 5.185823835352416

Epoch: 5| Step: 2
Training loss: 5.748068809509277
Validation loss: 5.159549179897513

Epoch: 5| Step: 3
Training loss: 5.864508152008057
Validation loss: 5.13441328848562

Epoch: 5| Step: 4
Training loss: 5.222228527069092
Validation loss: 5.106842517852783

Epoch: 5| Step: 5
Training loss: 3.2533161640167236
Validation loss: 5.075288029127224

Epoch: 5| Step: 6
Training loss: 4.4683709144592285
Validation loss: 5.038429747345627

Epoch: 5| Step: 7
Training loss: 6.768566131591797
Validation loss: 4.996786968682402

Epoch: 5| Step: 8
Training loss: 4.675182342529297
Validation loss: 4.9491193320161555

Epoch: 5| Step: 9
Training loss: 3.269780397415161
Validation loss: 4.895349271835819

Epoch: 5| Step: 10
Training loss: 5.1224751472473145
Validation loss: 4.8329841731697

Epoch: 2| Step: 0
Training loss: 4.970284461975098
Validation loss: 4.766207753971059

Epoch: 5| Step: 1
Training loss: 3.4629950523376465
Validation loss: 4.6911140359858035

Epoch: 5| Step: 2
Training loss: 4.6351189613342285
Validation loss: 4.611736271971015

Epoch: 5| Step: 3
Training loss: 4.171763896942139
Validation loss: 4.526172955830892

Epoch: 5| Step: 4
Training loss: 3.813774824142456
Validation loss: 4.441413540993968

Epoch: 5| Step: 5
Training loss: 3.5567855834960938
Validation loss: 4.351638832399922

Epoch: 5| Step: 6
Training loss: 4.583502292633057
Validation loss: 4.262939058324342

Epoch: 5| Step: 7
Training loss: 4.229433536529541
Validation loss: 4.176792093502578

Epoch: 5| Step: 8
Training loss: 4.05392599105835
Validation loss: 4.0906494227788786

Epoch: 5| Step: 9
Training loss: 4.292670249938965
Validation loss: 4.007123395960818

Epoch: 5| Step: 10
Training loss: 3.8516993522644043
Validation loss: 3.9279820124308267

Epoch: 3| Step: 0
Training loss: 4.90657901763916
Validation loss: 3.861990036502961

Epoch: 5| Step: 1
Training loss: 3.9747352600097656
Validation loss: 3.8025443887197845

Epoch: 5| Step: 2
Training loss: 3.806910753250122
Validation loss: 3.7507872222572245

Epoch: 5| Step: 3
Training loss: 3.5879464149475098
Validation loss: 3.7074490670234925

Epoch: 5| Step: 4
Training loss: 3.4525246620178223
Validation loss: 3.6702418327331543

Epoch: 5| Step: 5
Training loss: 3.235211133956909
Validation loss: 3.63683404204666

Epoch: 5| Step: 6
Training loss: 2.8529911041259766
Validation loss: 3.608060759882773

Epoch: 5| Step: 7
Training loss: 3.257192611694336
Validation loss: 3.5809544542784333

Epoch: 5| Step: 8
Training loss: 3.063687324523926
Validation loss: 3.5553511522149526

Epoch: 5| Step: 9
Training loss: 2.6550345420837402
Validation loss: 3.528143613569198

Epoch: 5| Step: 10
Training loss: 4.803628444671631
Validation loss: 3.496322229344358

Epoch: 4| Step: 0
Training loss: 3.3464837074279785
Validation loss: 3.4563000176542547

Epoch: 5| Step: 1
Training loss: 3.086024045944214
Validation loss: 3.4189225653166413

Epoch: 5| Step: 2
Training loss: 3.5819058418273926
Validation loss: 3.389143902768371

Epoch: 5| Step: 3
Training loss: 3.2227120399475098
Validation loss: 3.36662620370106

Epoch: 5| Step: 4
Training loss: 3.6156554222106934
Validation loss: 3.3580325957267516

Epoch: 5| Step: 5
Training loss: 3.2816758155822754
Validation loss: 3.3361915003868843

Epoch: 5| Step: 6
Training loss: 2.8643689155578613
Validation loss: 3.315767272826164

Epoch: 5| Step: 7
Training loss: 2.9463939666748047
Validation loss: 3.2956955381619033

Epoch: 5| Step: 8
Training loss: 2.548409938812256
Validation loss: 3.2736674790741294

Epoch: 5| Step: 9
Training loss: 3.86588716506958
Validation loss: 3.2461087754977647

Epoch: 5| Step: 10
Training loss: 3.993373155593872
Validation loss: 3.2198329176954044

Epoch: 5| Step: 0
Training loss: 3.646544933319092
Validation loss: 3.197698490594023

Epoch: 5| Step: 1
Training loss: 3.112802028656006
Validation loss: 3.187961193823045

Epoch: 5| Step: 2
Training loss: 3.0600452423095703
Validation loss: 3.1790163055542977

Epoch: 5| Step: 3
Training loss: 3.72617769241333
Validation loss: 3.170029817088958

Epoch: 5| Step: 4
Training loss: 3.0928847789764404
Validation loss: 3.156277075890572

Epoch: 5| Step: 5
Training loss: 2.6362063884735107
Validation loss: 3.131401046629875

Epoch: 5| Step: 6
Training loss: 3.1548876762390137
Validation loss: 3.1162855548243367

Epoch: 5| Step: 7
Training loss: 3.6158173084259033
Validation loss: 3.1088370200126403

Epoch: 5| Step: 8
Training loss: 2.9014880657196045
Validation loss: 3.1053958964604202

Epoch: 5| Step: 9
Training loss: 2.8467020988464355
Validation loss: 3.091981008488645

Epoch: 5| Step: 10
Training loss: 2.824265480041504
Validation loss: 3.072351417233867

Epoch: 6| Step: 0
Training loss: 2.531252384185791
Validation loss: 3.056362908373597

Epoch: 5| Step: 1
Training loss: 2.5979793071746826
Validation loss: 3.045630908781482

Epoch: 5| Step: 2
Training loss: 3.643345594406128
Validation loss: 3.037598286905596

Epoch: 5| Step: 3
Training loss: 3.239340305328369
Validation loss: 3.0216081783335698

Epoch: 5| Step: 4
Training loss: 3.06868314743042
Validation loss: 3.005042414511404

Epoch: 5| Step: 5
Training loss: 3.021127700805664
Validation loss: 2.992400478291255

Epoch: 5| Step: 6
Training loss: 2.988929271697998
Validation loss: 2.9864698020360803

Epoch: 5| Step: 7
Training loss: 3.6173477172851562
Validation loss: 2.9647222744521273

Epoch: 5| Step: 8
Training loss: 2.4303741455078125
Validation loss: 2.9556944216451337

Epoch: 5| Step: 9
Training loss: 3.0927183628082275
Validation loss: 2.952126754227505

Epoch: 5| Step: 10
Training loss: 3.4040162563323975
Validation loss: 2.9529008660265195

Epoch: 7| Step: 0
Training loss: 2.456298589706421
Validation loss: 2.9493870453167985

Epoch: 5| Step: 1
Training loss: 3.3490169048309326
Validation loss: 2.9368352223468084

Epoch: 5| Step: 2
Training loss: 3.0622751712799072
Validation loss: 2.9241887472009145

Epoch: 5| Step: 3
Training loss: 3.414686918258667
Validation loss: 2.9166183625498125

Epoch: 5| Step: 4
Training loss: 4.105321407318115
Validation loss: 2.918456446739935

Epoch: 5| Step: 5
Training loss: 2.967796564102173
Validation loss: 2.9071633482492096

Epoch: 5| Step: 6
Training loss: 1.6712779998779297
Validation loss: 2.897383011797423

Epoch: 5| Step: 7
Training loss: 2.984025239944458
Validation loss: 2.8884474718442528

Epoch: 5| Step: 8
Training loss: 3.3891189098358154
Validation loss: 2.878232343222505

Epoch: 5| Step: 9
Training loss: 3.097729444503784
Validation loss: 2.8676104366138415

Epoch: 5| Step: 10
Training loss: 2.3440353870391846
Validation loss: 2.860497905362037

Epoch: 8| Step: 0
Training loss: 2.3857100009918213
Validation loss: 2.8541783568679646

Epoch: 5| Step: 1
Training loss: 3.628394365310669
Validation loss: 2.8481967218460573

Epoch: 5| Step: 2
Training loss: 2.413909435272217
Validation loss: 2.8411349558061167

Epoch: 5| Step: 3
Training loss: 3.341540575027466
Validation loss: 2.829371431822418

Epoch: 5| Step: 4
Training loss: 2.665180206298828
Validation loss: 2.818246067211192

Epoch: 5| Step: 5
Training loss: 3.3517050743103027
Validation loss: 2.8101884549663914

Epoch: 5| Step: 6
Training loss: 2.936281204223633
Validation loss: 2.80397020616839

Epoch: 5| Step: 7
Training loss: 3.0574984550476074
Validation loss: 2.7930911535857827

Epoch: 5| Step: 8
Training loss: 2.412832736968994
Validation loss: 2.7846187801771265

Epoch: 5| Step: 9
Training loss: 3.1476099491119385
Validation loss: 2.7773541199263705

Epoch: 5| Step: 10
Training loss: 2.9581048488616943
Validation loss: 2.770188472604239

Epoch: 9| Step: 0
Training loss: 2.8008835315704346
Validation loss: 2.7608669265624015

Epoch: 5| Step: 1
Training loss: 2.9796395301818848
Validation loss: 2.7510961255719586

Epoch: 5| Step: 2
Training loss: 2.640479326248169
Validation loss: 2.748659139038414

Epoch: 5| Step: 3
Training loss: 2.6693217754364014
Validation loss: 2.7502478220129527

Epoch: 5| Step: 4
Training loss: 2.358549118041992
Validation loss: 2.7283704562853743

Epoch: 5| Step: 5
Training loss: 2.8982255458831787
Validation loss: 2.7306474331886537

Epoch: 5| Step: 6
Training loss: 2.3342041969299316
Validation loss: 2.7168806368304836

Epoch: 5| Step: 7
Training loss: 3.3649814128875732
Validation loss: 2.7057633733236663

Epoch: 5| Step: 8
Training loss: 3.7433204650878906
Validation loss: 2.7024913346895607

Epoch: 5| Step: 9
Training loss: 3.287081241607666
Validation loss: 2.6935945608282603

Epoch: 5| Step: 10
Training loss: 2.5775699615478516
Validation loss: 2.680757155982397

Epoch: 10| Step: 0
Training loss: 2.4585041999816895
Validation loss: 2.6787767897370043

Epoch: 5| Step: 1
Training loss: 2.402709484100342
Validation loss: 2.6842934777659755

Epoch: 5| Step: 2
Training loss: 3.3200759887695312
Validation loss: 2.6828818116136777

Epoch: 5| Step: 3
Training loss: 3.4061572551727295
Validation loss: 2.6615077833975516

Epoch: 5| Step: 4
Training loss: 3.0708839893341064
Validation loss: 2.652994345593196

Epoch: 5| Step: 5
Training loss: 2.1872591972351074
Validation loss: 2.644129909494872

Epoch: 5| Step: 6
Training loss: 3.0581862926483154
Validation loss: 2.6457992830584125

Epoch: 5| Step: 7
Training loss: 2.553950071334839
Validation loss: 2.634180361224759

Epoch: 5| Step: 8
Training loss: 3.3362784385681152
Validation loss: 2.637083084352555

Epoch: 5| Step: 9
Training loss: 2.4461770057678223
Validation loss: 2.63047529292363

Epoch: 5| Step: 10
Training loss: 2.9870240688323975
Validation loss: 2.6239779790242515

Epoch: 11| Step: 0
Training loss: 2.1572344303131104
Validation loss: 2.6167410932561403

Epoch: 5| Step: 1
Training loss: 3.027759075164795
Validation loss: 2.6085267259228613

Epoch: 5| Step: 2
Training loss: 3.810105562210083
Validation loss: 2.6027652781496764

Epoch: 5| Step: 3
Training loss: 3.027473211288452
Validation loss: 2.6065682006138626

Epoch: 5| Step: 4
Training loss: 3.079136371612549
Validation loss: 2.6165552318737073

Epoch: 5| Step: 5
Training loss: 2.85937762260437
Validation loss: 2.6096076016784995

Epoch: 5| Step: 6
Training loss: 2.5523407459259033
Validation loss: 2.5880923578816075

Epoch: 5| Step: 7
Training loss: 2.866302967071533
Validation loss: 2.584703658216743

Epoch: 5| Step: 8
Training loss: 3.1387903690338135
Validation loss: 2.5989791013861216

Epoch: 5| Step: 9
Training loss: 2.2444567680358887
Validation loss: 2.582239894456761

Epoch: 5| Step: 10
Training loss: 1.9783895015716553
Validation loss: 2.5766560851886706

Epoch: 12| Step: 0
Training loss: 3.020127058029175
Validation loss: 2.5843111340717604

Epoch: 5| Step: 1
Training loss: 2.81606388092041
Validation loss: 2.575440534981348

Epoch: 5| Step: 2
Training loss: 2.929358959197998
Validation loss: 2.5726113819306895

Epoch: 5| Step: 3
Training loss: 3.184426784515381
Validation loss: 2.586129670502037

Epoch: 5| Step: 4
Training loss: 2.3116455078125
Validation loss: 2.5649209996705413

Epoch: 5| Step: 5
Training loss: 3.2796173095703125
Validation loss: 2.574248972759452

Epoch: 5| Step: 6
Training loss: 2.8788959980010986
Validation loss: 2.5667259667509343

Epoch: 5| Step: 7
Training loss: 2.2456436157226562
Validation loss: 2.5571859857087493

Epoch: 5| Step: 8
Training loss: 2.584118127822876
Validation loss: 2.5683533863354753

Epoch: 5| Step: 9
Training loss: 2.546504497528076
Validation loss: 2.605120740911012

Epoch: 5| Step: 10
Training loss: 2.8626441955566406
Validation loss: 2.575704143893334

Epoch: 13| Step: 0
Training loss: 3.7213408946990967
Validation loss: 2.5503124549824703

Epoch: 5| Step: 1
Training loss: 1.9167369604110718
Validation loss: 2.5243000189463296

Epoch: 5| Step: 2
Training loss: 3.008495330810547
Validation loss: 2.534239084489884

Epoch: 5| Step: 3
Training loss: 2.439826011657715
Validation loss: 2.5774933945748115

Epoch: 5| Step: 4
Training loss: 3.0900590419769287
Validation loss: 2.560143865564818

Epoch: 5| Step: 5
Training loss: 3.4761219024658203
Validation loss: 2.531677030747937

Epoch: 5| Step: 6
Training loss: 2.389611005783081
Validation loss: 2.5289940782772597

Epoch: 5| Step: 7
Training loss: 2.838902473449707
Validation loss: 2.5589517137055755

Epoch: 5| Step: 8
Training loss: 2.335893392562866
Validation loss: 2.55211297158272

Epoch: 5| Step: 9
Training loss: 2.2112679481506348
Validation loss: 2.5261928214821765

Epoch: 5| Step: 10
Training loss: 3.0276734828948975
Validation loss: 2.5394373888610513

Epoch: 14| Step: 0
Training loss: 2.547452211380005
Validation loss: 2.542804930799751

Epoch: 5| Step: 1
Training loss: 3.1553454399108887
Validation loss: 2.513736063434232

Epoch: 5| Step: 2
Training loss: 3.1645445823669434
Validation loss: 2.51107096415694

Epoch: 5| Step: 3
Training loss: 2.244263172149658
Validation loss: 2.507997553835633

Epoch: 5| Step: 4
Training loss: 2.9035582542419434
Validation loss: 2.507534414209345

Epoch: 5| Step: 5
Training loss: 2.3953936100006104
Validation loss: 2.4935323884410243

Epoch: 5| Step: 6
Training loss: 2.311152935028076
Validation loss: 2.4967903501244

Epoch: 5| Step: 7
Training loss: 2.4395530223846436
Validation loss: 2.490313142858526

Epoch: 5| Step: 8
Training loss: 2.9035797119140625
Validation loss: 2.4965322402215775

Epoch: 5| Step: 9
Training loss: 2.679903507232666
Validation loss: 2.4930224572458575

Epoch: 5| Step: 10
Training loss: 3.5182409286499023
Validation loss: 2.4926996513079573

Epoch: 15| Step: 0
Training loss: 3.4253158569335938
Validation loss: 2.4932493240602556

Epoch: 5| Step: 1
Training loss: 2.9070942401885986
Validation loss: 2.497886775642313

Epoch: 5| Step: 2
Training loss: 2.6839146614074707
Validation loss: 2.497837584505799

Epoch: 5| Step: 3
Training loss: 3.040904998779297
Validation loss: 2.485939415552283

Epoch: 5| Step: 4
Training loss: 2.3556697368621826
Validation loss: 2.4888958059331423

Epoch: 5| Step: 5
Training loss: 2.959897518157959
Validation loss: 2.5095971861193256

Epoch: 5| Step: 6
Training loss: 2.5326807498931885
Validation loss: 2.5087677535190376

Epoch: 5| Step: 7
Training loss: 2.2096400260925293
Validation loss: 2.505765838007773

Epoch: 5| Step: 8
Training loss: 2.528773069381714
Validation loss: 2.5211715570060154

Epoch: 5| Step: 9
Training loss: 2.5579946041107178
Validation loss: 2.5064496353108394

Epoch: 5| Step: 10
Training loss: 2.9011099338531494
Validation loss: 2.484357880007836

Epoch: 16| Step: 0
Training loss: 1.9639240503311157
Validation loss: 2.475460071717539

Epoch: 5| Step: 1
Training loss: 2.6884255409240723
Validation loss: 2.4828774185590845

Epoch: 5| Step: 2
Training loss: 2.8443443775177
Validation loss: 2.51918537129638

Epoch: 5| Step: 3
Training loss: 3.2809112071990967
Validation loss: 2.5011801719665527

Epoch: 5| Step: 4
Training loss: 2.6118247509002686
Validation loss: 2.4736558083565003

Epoch: 5| Step: 5
Training loss: 2.888930082321167
Validation loss: 2.470939725957891

Epoch: 5| Step: 6
Training loss: 2.5243678092956543
Validation loss: 2.4993693649127917

Epoch: 5| Step: 7
Training loss: 3.3905017375946045
Validation loss: 2.5066493403527046

Epoch: 5| Step: 8
Training loss: 2.7320923805236816
Validation loss: 2.5083611652415287

Epoch: 5| Step: 9
Training loss: 2.6611905097961426
Validation loss: 2.4967792290513233

Epoch: 5| Step: 10
Training loss: 2.421950340270996
Validation loss: 2.4812205709436888

Epoch: 17| Step: 0
Training loss: 3.4882774353027344
Validation loss: 2.4658223685397895

Epoch: 5| Step: 1
Training loss: 2.1321849822998047
Validation loss: 2.4611142655854583

Epoch: 5| Step: 2
Training loss: 2.726994037628174
Validation loss: 2.4677849713192193

Epoch: 5| Step: 3
Training loss: 2.205920457839966
Validation loss: 2.4828281069314606

Epoch: 5| Step: 4
Training loss: 2.993175983428955
Validation loss: 2.485903457928729

Epoch: 5| Step: 5
Training loss: 2.9901235103607178
Validation loss: 2.4947939354886293

Epoch: 5| Step: 6
Training loss: 2.2975525856018066
Validation loss: 2.505079501418657

Epoch: 5| Step: 7
Training loss: 2.67808198928833
Validation loss: 2.4858647392642115

Epoch: 5| Step: 8
Training loss: 3.1310932636260986
Validation loss: 2.4684141400039836

Epoch: 5| Step: 9
Training loss: 3.169640064239502
Validation loss: 2.4672443251455984

Epoch: 5| Step: 10
Training loss: 1.874498963356018
Validation loss: 2.467332827147617

Epoch: 18| Step: 0
Training loss: 2.6287834644317627
Validation loss: 2.4782873610014557

Epoch: 5| Step: 1
Training loss: 3.1766490936279297
Validation loss: 2.5368033506536998

Epoch: 5| Step: 2
Training loss: 2.7878215312957764
Validation loss: 2.4883417032098256

Epoch: 5| Step: 3
Training loss: 3.081601619720459
Validation loss: 2.460527455934914

Epoch: 5| Step: 4
Training loss: 2.2009758949279785
Validation loss: 2.436045979940763

Epoch: 5| Step: 5
Training loss: 3.1339104175567627
Validation loss: 2.445041564203078

Epoch: 5| Step: 6
Training loss: 2.457796096801758
Validation loss: 2.4737883280682307

Epoch: 5| Step: 7
Training loss: 3.0949485301971436
Validation loss: 2.508729939819664

Epoch: 5| Step: 8
Training loss: 2.644056797027588
Validation loss: 2.5187761783599854

Epoch: 5| Step: 9
Training loss: 2.5650858879089355
Validation loss: 2.5116930469389884

Epoch: 5| Step: 10
Training loss: 2.0046300888061523
Validation loss: 2.525210275444933

Epoch: 19| Step: 0
Training loss: 2.462167739868164
Validation loss: 2.531789782226727

Epoch: 5| Step: 1
Training loss: 2.370011568069458
Validation loss: 2.5197170242186515

Epoch: 5| Step: 2
Training loss: 2.129108428955078
Validation loss: 2.471756068609094

Epoch: 5| Step: 3
Training loss: 2.7143025398254395
Validation loss: 2.445763671269981

Epoch: 5| Step: 4
Training loss: 2.998605728149414
Validation loss: 2.4383704841777845

Epoch: 5| Step: 5
Training loss: 3.647473096847534
Validation loss: 2.4513958141367924

Epoch: 5| Step: 6
Training loss: 2.4463164806365967
Validation loss: 2.452788829803467

Epoch: 5| Step: 7
Training loss: 2.853708267211914
Validation loss: 2.4531878579047417

Epoch: 5| Step: 8
Training loss: 2.7462220191955566
Validation loss: 2.4831840299790904

Epoch: 5| Step: 9
Training loss: 2.9390273094177246
Validation loss: 2.482879953999673

Epoch: 5| Step: 10
Training loss: 2.47579288482666
Validation loss: 2.4814829134172007

Epoch: 20| Step: 0
Training loss: 2.76751971244812
Validation loss: 2.465060116142355

Epoch: 5| Step: 1
Training loss: 3.5413765907287598
Validation loss: 2.454706181762039

Epoch: 5| Step: 2
Training loss: 2.4135947227478027
Validation loss: 2.450270673280121

Epoch: 5| Step: 3
Training loss: 2.581566333770752
Validation loss: 2.4400635368080548

Epoch: 5| Step: 4
Training loss: 2.896641731262207
Validation loss: 2.4418247720246673

Epoch: 5| Step: 5
Training loss: 2.167426586151123
Validation loss: 2.4501927770594114

Epoch: 5| Step: 6
Training loss: 3.223820209503174
Validation loss: 2.4647673586363434

Epoch: 5| Step: 7
Training loss: 2.456733226776123
Validation loss: 2.451087576086803

Epoch: 5| Step: 8
Training loss: 2.689690351486206
Validation loss: 2.4515703852458666

Epoch: 5| Step: 9
Training loss: 2.7101848125457764
Validation loss: 2.440781654850129

Epoch: 5| Step: 10
Training loss: 2.013517379760742
Validation loss: 2.4287930560368363

Epoch: 21| Step: 0
Training loss: 3.1759719848632812
Validation loss: 2.424218234195504

Epoch: 5| Step: 1
Training loss: 2.6789002418518066
Validation loss: 2.4221368425635883

Epoch: 5| Step: 2
Training loss: 1.9948346614837646
Validation loss: 2.4162335293267363

Epoch: 5| Step: 3
Training loss: 2.3230533599853516
Validation loss: 2.4101996344904744

Epoch: 5| Step: 4
Training loss: 2.955833673477173
Validation loss: 2.4067398296889437

Epoch: 5| Step: 5
Training loss: 2.834737777709961
Validation loss: 2.4047686425588464

Epoch: 5| Step: 6
Training loss: 2.325671911239624
Validation loss: 2.4084900245871594

Epoch: 5| Step: 7
Training loss: 2.307213544845581
Validation loss: 2.405387055489325

Epoch: 5| Step: 8
Training loss: 2.8608577251434326
Validation loss: 2.4050876632813485

Epoch: 5| Step: 9
Training loss: 2.9820079803466797
Validation loss: 2.4040696441486316

Epoch: 5| Step: 10
Training loss: 2.851412296295166
Validation loss: 2.4021321496655865

Epoch: 22| Step: 0
Training loss: 2.6831271648406982
Validation loss: 2.402856574263624

Epoch: 5| Step: 1
Training loss: 2.2120680809020996
Validation loss: 2.403366155521844

Epoch: 5| Step: 2
Training loss: 2.486328125
Validation loss: 2.432546556636851

Epoch: 5| Step: 3
Training loss: 2.9432244300842285
Validation loss: 2.4758158242830666

Epoch: 5| Step: 4
Training loss: 2.674313545227051
Validation loss: 2.4894301814417683

Epoch: 5| Step: 5
Training loss: 2.888800859451294
Validation loss: 2.477876291480116

Epoch: 5| Step: 6
Training loss: 2.6173768043518066
Validation loss: 2.433304222681189

Epoch: 5| Step: 7
Training loss: 2.2348010540008545
Validation loss: 2.398016277179923

Epoch: 5| Step: 8
Training loss: 3.0335798263549805
Validation loss: 2.3865293815571773

Epoch: 5| Step: 9
Training loss: 2.677044630050659
Validation loss: 2.387495842031253

Epoch: 5| Step: 10
Training loss: 2.9569942951202393
Validation loss: 2.4076685905456543

Epoch: 23| Step: 0
Training loss: 2.4800469875335693
Validation loss: 2.4170225051141556

Epoch: 5| Step: 1
Training loss: 2.524247646331787
Validation loss: 2.4186555775262977

Epoch: 5| Step: 2
Training loss: 2.493018388748169
Validation loss: 2.416484676381593

Epoch: 5| Step: 3
Training loss: 2.699674129486084
Validation loss: 2.426064952727287

Epoch: 5| Step: 4
Training loss: 3.227053165435791
Validation loss: 2.4037415366018973

Epoch: 5| Step: 5
Training loss: 2.2630412578582764
Validation loss: 2.3866418382172943

Epoch: 5| Step: 6
Training loss: 3.172595977783203
Validation loss: 2.379693582493772

Epoch: 5| Step: 7
Training loss: 2.387354850769043
Validation loss: 2.3740837881642003

Epoch: 5| Step: 8
Training loss: 3.270334243774414
Validation loss: 2.3757790134799097

Epoch: 5| Step: 9
Training loss: 2.3676083087921143
Validation loss: 2.389742979439356

Epoch: 5| Step: 10
Training loss: 2.2680165767669678
Validation loss: 2.394215250527987

Epoch: 24| Step: 0
Training loss: 2.9814484119415283
Validation loss: 2.4453348703281854

Epoch: 5| Step: 1
Training loss: 2.3414337635040283
Validation loss: 2.4278172472471833

Epoch: 5| Step: 2
Training loss: 2.4630866050720215
Validation loss: 2.420215727180563

Epoch: 5| Step: 3
Training loss: 2.237302541732788
Validation loss: 2.4041162793354323

Epoch: 5| Step: 4
Training loss: 3.027965784072876
Validation loss: 2.382036716707291

Epoch: 5| Step: 5
Training loss: 3.118166446685791
Validation loss: 2.3779744179018083

Epoch: 5| Step: 6
Training loss: 2.112821578979492
Validation loss: 2.3748741406266407

Epoch: 5| Step: 7
Training loss: 3.2457518577575684
Validation loss: 2.382193365404683

Epoch: 5| Step: 8
Training loss: 2.8490397930145264
Validation loss: 2.3819019051008326

Epoch: 5| Step: 9
Training loss: 2.4404213428497314
Validation loss: 2.401582838386618

Epoch: 5| Step: 10
Training loss: 2.4633848667144775
Validation loss: 2.4115787629158265

Epoch: 25| Step: 0
Training loss: 2.865626811981201
Validation loss: 2.39635367290948

Epoch: 5| Step: 1
Training loss: 2.8041749000549316
Validation loss: 2.3864443712337042

Epoch: 5| Step: 2
Training loss: 2.1066765785217285
Validation loss: 2.3953069820198962

Epoch: 5| Step: 3
Training loss: 3.2430355548858643
Validation loss: 2.457701693298996

Epoch: 5| Step: 4
Training loss: 2.650306224822998
Validation loss: 2.5647150624182915

Epoch: 5| Step: 5
Training loss: 2.9579684734344482
Validation loss: 2.5350719395504204

Epoch: 5| Step: 6
Training loss: 2.0673987865448
Validation loss: 2.4257171102749404

Epoch: 5| Step: 7
Training loss: 2.998753547668457
Validation loss: 2.4218649659105527

Epoch: 5| Step: 8
Training loss: 2.878540515899658
Validation loss: 2.4117861281159105

Epoch: 5| Step: 9
Training loss: 2.172746181488037
Validation loss: 2.4035468075865056

Epoch: 5| Step: 10
Training loss: 2.3977813720703125
Validation loss: 2.3995464489024174

Epoch: 26| Step: 0
Training loss: 2.315422534942627
Validation loss: 2.3993434598368983

Epoch: 5| Step: 1
Training loss: 3.141881227493286
Validation loss: 2.4183702648326917

Epoch: 5| Step: 2
Training loss: 2.809451103210449
Validation loss: 2.418527844131634

Epoch: 5| Step: 3
Training loss: 2.5064644813537598
Validation loss: 2.422519899183704

Epoch: 5| Step: 4
Training loss: 2.544865131378174
Validation loss: 2.4173596136031614

Epoch: 5| Step: 5
Training loss: 3.145209789276123
Validation loss: 2.4097863628018286

Epoch: 5| Step: 6
Training loss: 2.7626945972442627
Validation loss: 2.3731236124551423

Epoch: 5| Step: 7
Training loss: 2.7006351947784424
Validation loss: 2.36360946009236

Epoch: 5| Step: 8
Training loss: 2.842341184616089
Validation loss: 2.354229957826676

Epoch: 5| Step: 9
Training loss: 2.0678842067718506
Validation loss: 2.354716213800574

Epoch: 5| Step: 10
Training loss: 2.2606403827667236
Validation loss: 2.361551200189898

Epoch: 27| Step: 0
Training loss: 2.70239520072937
Validation loss: 2.3648479189924014

Epoch: 5| Step: 1
Training loss: 2.980818271636963
Validation loss: 2.36994848456434

Epoch: 5| Step: 2
Training loss: 2.376845121383667
Validation loss: 2.378188415240216

Epoch: 5| Step: 3
Training loss: 2.478710412979126
Validation loss: 2.406797401366695

Epoch: 5| Step: 4
Training loss: 2.7592642307281494
Validation loss: 2.3877993270915043

Epoch: 5| Step: 5
Training loss: 2.3408799171447754
Validation loss: 2.4281681891410583

Epoch: 5| Step: 6
Training loss: 2.8695437908172607
Validation loss: 2.411901012543709

Epoch: 5| Step: 7
Training loss: 2.772590160369873
Validation loss: 2.391707420349121

Epoch: 5| Step: 8
Training loss: 2.410970687866211
Validation loss: 2.372003432243101

Epoch: 5| Step: 9
Training loss: 2.5684428215026855
Validation loss: 2.357352784884873

Epoch: 5| Step: 10
Training loss: 2.5290067195892334
Validation loss: 2.3484628277440227

Epoch: 28| Step: 0
Training loss: 2.3272411823272705
Validation loss: 2.3446731708383046

Epoch: 5| Step: 1
Training loss: 2.147223472595215
Validation loss: 2.342417019669728

Epoch: 5| Step: 2
Training loss: 2.983764410018921
Validation loss: 2.343358532074959

Epoch: 5| Step: 3
Training loss: 3.0605545043945312
Validation loss: 2.3456404644955873

Epoch: 5| Step: 4
Training loss: 1.9246635437011719
Validation loss: 2.3430920621400237

Epoch: 5| Step: 5
Training loss: 2.118751049041748
Validation loss: 2.344920871078327

Epoch: 5| Step: 6
Training loss: 2.9583678245544434
Validation loss: 2.3854795245714087

Epoch: 5| Step: 7
Training loss: 2.698157787322998
Validation loss: 2.4205643925615536

Epoch: 5| Step: 8
Training loss: 3.3883209228515625
Validation loss: 2.420588477965324

Epoch: 5| Step: 9
Training loss: 2.8514363765716553
Validation loss: 2.421809968127999

Epoch: 5| Step: 10
Training loss: 2.3801393508911133
Validation loss: 2.3873577848557503

Epoch: 29| Step: 0
Training loss: 2.7578349113464355
Validation loss: 2.350153543615854

Epoch: 5| Step: 1
Training loss: 2.918349266052246
Validation loss: 2.339919620944608

Epoch: 5| Step: 2
Training loss: 2.5207505226135254
Validation loss: 2.3403920640227613

Epoch: 5| Step: 3
Training loss: 2.3142294883728027
Validation loss: 2.355169201409945

Epoch: 5| Step: 4
Training loss: 2.8726277351379395
Validation loss: 2.3470670587273053

Epoch: 5| Step: 5
Training loss: 1.7889560461044312
Validation loss: 2.340358429057624

Epoch: 5| Step: 6
Training loss: 2.7327990531921387
Validation loss: 2.335292587998093

Epoch: 5| Step: 7
Training loss: 2.32611346244812
Validation loss: 2.3317805515822543

Epoch: 5| Step: 8
Training loss: 3.102961301803589
Validation loss: 2.353911663896294

Epoch: 5| Step: 9
Training loss: 2.7900454998016357
Validation loss: 2.3669862106282222

Epoch: 5| Step: 10
Training loss: 2.7131569385528564
Validation loss: 2.3754179964783373

Epoch: 30| Step: 0
Training loss: 1.9612939357757568
Validation loss: 2.397946814055084

Epoch: 5| Step: 1
Training loss: 3.0345187187194824
Validation loss: 2.4113602330607753

Epoch: 5| Step: 2
Training loss: 2.17880916595459
Validation loss: 2.408245332779423

Epoch: 5| Step: 3
Training loss: 2.795036554336548
Validation loss: 2.386040646542785

Epoch: 5| Step: 4
Training loss: 2.882107734680176
Validation loss: 2.3409691062024844

Epoch: 5| Step: 5
Training loss: 2.7807841300964355
Validation loss: 2.3252981375622492

Epoch: 5| Step: 6
Training loss: 2.793832778930664
Validation loss: 2.3232433206291607

Epoch: 5| Step: 7
Training loss: 2.7016797065734863
Validation loss: 2.321067281948623

Epoch: 5| Step: 8
Training loss: 2.455853223800659
Validation loss: 2.330533117376348

Epoch: 5| Step: 9
Training loss: 2.7355709075927734
Validation loss: 2.328800447525517

Epoch: 5| Step: 10
Training loss: 2.3707120418548584
Validation loss: 2.3237147651692873

Epoch: 31| Step: 0
Training loss: 2.7920687198638916
Validation loss: 2.3226466999259046

Epoch: 5| Step: 1
Training loss: 2.7580924034118652
Validation loss: 2.3191532422137517

Epoch: 5| Step: 2
Training loss: 2.4535508155822754
Validation loss: 2.325579217685166

Epoch: 5| Step: 3
Training loss: 2.444608211517334
Validation loss: 2.3331145804415465

Epoch: 5| Step: 4
Training loss: 3.0914626121520996
Validation loss: 2.3527399314347135

Epoch: 5| Step: 5
Training loss: 2.567251682281494
Validation loss: 2.377413216457572

Epoch: 5| Step: 6
Training loss: 2.7753262519836426
Validation loss: 2.385517351088985

Epoch: 5| Step: 7
Training loss: 2.691474676132202
Validation loss: 2.401804542028776

Epoch: 5| Step: 8
Training loss: 2.7601585388183594
Validation loss: 2.4301893249634774

Epoch: 5| Step: 9
Training loss: 2.404113292694092
Validation loss: 2.415370975771258

Epoch: 5| Step: 10
Training loss: 1.8211479187011719
Validation loss: 2.4181581184428227

Epoch: 32| Step: 0
Training loss: 2.319223165512085
Validation loss: 2.434695097707933

Epoch: 5| Step: 1
Training loss: 2.4721693992614746
Validation loss: 2.4491917151276783

Epoch: 5| Step: 2
Training loss: 3.2139229774475098
Validation loss: 2.41626759241986

Epoch: 5| Step: 3
Training loss: 2.385862350463867
Validation loss: 2.37158340279774

Epoch: 5| Step: 4
Training loss: 2.582129716873169
Validation loss: 2.341846691664829

Epoch: 5| Step: 5
Training loss: 2.129840135574341
Validation loss: 2.3212710657427387

Epoch: 5| Step: 6
Training loss: 2.5753273963928223
Validation loss: 2.3181580779373006

Epoch: 5| Step: 7
Training loss: 3.1945369243621826
Validation loss: 2.3217124426236717

Epoch: 5| Step: 8
Training loss: 2.997255563735962
Validation loss: 2.3226472716177664

Epoch: 5| Step: 9
Training loss: 2.4561893939971924
Validation loss: 2.3188899268386183

Epoch: 5| Step: 10
Training loss: 2.559053897857666
Validation loss: 2.309651428653348

Epoch: 33| Step: 0
Training loss: 2.4129109382629395
Validation loss: 2.309338303022487

Epoch: 5| Step: 1
Training loss: 2.014397144317627
Validation loss: 2.3139615648536274

Epoch: 5| Step: 2
Training loss: 2.8039557933807373
Validation loss: 2.343975213266188

Epoch: 5| Step: 3
Training loss: 2.2856526374816895
Validation loss: 2.408634476764228

Epoch: 5| Step: 4
Training loss: 2.7913360595703125
Validation loss: 2.4959384728503484

Epoch: 5| Step: 5
Training loss: 2.8680167198181152
Validation loss: 2.48944184600666

Epoch: 5| Step: 6
Training loss: 2.6024668216705322
Validation loss: 2.4273747372370895

Epoch: 5| Step: 7
Training loss: 2.5648059844970703
Validation loss: 2.360551964852118

Epoch: 5| Step: 8
Training loss: 3.0658321380615234
Validation loss: 2.3046478686794156

Epoch: 5| Step: 9
Training loss: 2.875497341156006
Validation loss: 2.2883417349989696

Epoch: 5| Step: 10
Training loss: 2.6889867782592773
Validation loss: 2.2989509951683784

Epoch: 34| Step: 0
Training loss: 2.916395902633667
Validation loss: 2.309271720147902

Epoch: 5| Step: 1
Training loss: 3.032188892364502
Validation loss: 2.311176294921547

Epoch: 5| Step: 2
Training loss: 3.297940731048584
Validation loss: 2.30836840855178

Epoch: 5| Step: 3
Training loss: 1.8806469440460205
Validation loss: 2.304090207622897

Epoch: 5| Step: 4
Training loss: 2.4851200580596924
Validation loss: 2.3267193814759612

Epoch: 5| Step: 5
Training loss: 2.3243813514709473
Validation loss: 2.352167876817847

Epoch: 5| Step: 6
Training loss: 3.0343728065490723
Validation loss: 2.366374828482187

Epoch: 5| Step: 7
Training loss: 2.5519847869873047
Validation loss: 2.3834082054835495

Epoch: 5| Step: 8
Training loss: 2.406893014907837
Validation loss: 2.3645156968024468

Epoch: 5| Step: 9
Training loss: 2.484703302383423
Validation loss: 2.351058183177825

Epoch: 5| Step: 10
Training loss: 2.2850193977355957
Validation loss: 2.293884082507062

Epoch: 35| Step: 0
Training loss: 2.738131284713745
Validation loss: 2.2859435055845525

Epoch: 5| Step: 1
Training loss: 2.8282268047332764
Validation loss: 2.2842380385245047

Epoch: 5| Step: 2
Training loss: 2.491806983947754
Validation loss: 2.2863982057058685

Epoch: 5| Step: 3
Training loss: 1.9326210021972656
Validation loss: 2.2872641983852593

Epoch: 5| Step: 4
Training loss: 2.0605826377868652
Validation loss: 2.2927502842359644

Epoch: 5| Step: 5
Training loss: 2.794153928756714
Validation loss: 2.3123509037879204

Epoch: 5| Step: 6
Training loss: 2.466866970062256
Validation loss: 2.3341404622600925

Epoch: 5| Step: 7
Training loss: 3.170611619949341
Validation loss: 2.355462189643614

Epoch: 5| Step: 8
Training loss: 2.744283676147461
Validation loss: 2.353235593406103

Epoch: 5| Step: 9
Training loss: 2.935441493988037
Validation loss: 2.3355217313253753

Epoch: 5| Step: 10
Training loss: 2.382852077484131
Validation loss: 2.322124206891624

Epoch: 36| Step: 0
Training loss: 2.3402438163757324
Validation loss: 2.314905612699447

Epoch: 5| Step: 1
Training loss: 3.0344033241271973
Validation loss: 2.312815830271731

Epoch: 5| Step: 2
Training loss: 3.115405559539795
Validation loss: 2.315121135404033

Epoch: 5| Step: 3
Training loss: 2.5489661693573
Validation loss: 2.297894257371144

Epoch: 5| Step: 4
Training loss: 2.4718258380889893
Validation loss: 2.2837265486358316

Epoch: 5| Step: 5
Training loss: 2.2877817153930664
Validation loss: 2.286231730573921

Epoch: 5| Step: 6
Training loss: 2.8583710193634033
Validation loss: 2.2819369044355167

Epoch: 5| Step: 7
Training loss: 1.7603676319122314
Validation loss: 2.2849791690867436

Epoch: 5| Step: 8
Training loss: 2.5985381603240967
Validation loss: 2.291081428527832

Epoch: 5| Step: 9
Training loss: 2.634597063064575
Validation loss: 2.315810675262123

Epoch: 5| Step: 10
Training loss: 2.8500123023986816
Validation loss: 2.3652727219366256

Epoch: 37| Step: 0
Training loss: 2.614548683166504
Validation loss: 2.413349018302015

Epoch: 5| Step: 1
Training loss: 2.5723793506622314
Validation loss: 2.4045517521519817

Epoch: 5| Step: 2
Training loss: 2.5673789978027344
Validation loss: 2.3757599194844565

Epoch: 5| Step: 3
Training loss: 2.6942801475524902
Validation loss: 2.3613221799173663

Epoch: 5| Step: 4
Training loss: 2.6624536514282227
Validation loss: 2.3245377027860252

Epoch: 5| Step: 5
Training loss: 2.65529203414917
Validation loss: 2.316222293402559

Epoch: 5| Step: 6
Training loss: 2.063913583755493
Validation loss: 2.3127920217411493

Epoch: 5| Step: 7
Training loss: 2.578014850616455
Validation loss: 2.3102640016104585

Epoch: 5| Step: 8
Training loss: 2.6773390769958496
Validation loss: 2.2970900740674747

Epoch: 5| Step: 9
Training loss: 3.2506299018859863
Validation loss: 2.2931801939523346

Epoch: 5| Step: 10
Training loss: 2.3389599323272705
Validation loss: 2.2853900130077074

Epoch: 38| Step: 0
Training loss: 2.1045081615448
Validation loss: 2.2759288331513763

Epoch: 5| Step: 1
Training loss: 2.260998487472534
Validation loss: 2.2702155433675295

Epoch: 5| Step: 2
Training loss: 2.868102550506592
Validation loss: 2.2740078895322737

Epoch: 5| Step: 3
Training loss: 2.1443870067596436
Validation loss: 2.28536041321293

Epoch: 5| Step: 4
Training loss: 2.9373087882995605
Validation loss: 2.3059413279256513

Epoch: 5| Step: 5
Training loss: 2.1953015327453613
Validation loss: 2.31126509686952

Epoch: 5| Step: 6
Training loss: 2.6507186889648438
Validation loss: 2.3077264652457288

Epoch: 5| Step: 7
Training loss: 2.9563374519348145
Validation loss: 2.2721329376261723

Epoch: 5| Step: 8
Training loss: 2.5371899604797363
Validation loss: 2.2633083276851202

Epoch: 5| Step: 9
Training loss: 2.770411968231201
Validation loss: 2.265792110914825

Epoch: 5| Step: 10
Training loss: 3.087576389312744
Validation loss: 2.264383658286064

Epoch: 39| Step: 0
Training loss: 3.0703816413879395
Validation loss: 2.2652502957210747

Epoch: 5| Step: 1
Training loss: 2.7131881713867188
Validation loss: 2.2697374513072353

Epoch: 5| Step: 2
Training loss: 2.7192795276641846
Validation loss: 2.2813905208341536

Epoch: 5| Step: 3
Training loss: 2.251168727874756
Validation loss: 2.2884245495642386

Epoch: 5| Step: 4
Training loss: 3.123959541320801
Validation loss: 2.313417498783399

Epoch: 5| Step: 5
Training loss: 2.0965232849121094
Validation loss: 2.3129136382892566

Epoch: 5| Step: 6
Training loss: 2.313478946685791
Validation loss: 2.3055167326363186

Epoch: 5| Step: 7
Training loss: 2.344872236251831
Validation loss: 2.332758803521433

Epoch: 5| Step: 8
Training loss: 2.2814295291900635
Validation loss: 2.3075949786811747

Epoch: 5| Step: 9
Training loss: 2.4452693462371826
Validation loss: 2.304557528547061

Epoch: 5| Step: 10
Training loss: 3.02133846282959
Validation loss: 2.3091440995534263

Epoch: 40| Step: 0
Training loss: 2.309499740600586
Validation loss: 2.2962277422669115

Epoch: 5| Step: 1
Training loss: 2.4820046424865723
Validation loss: 2.289743756735197

Epoch: 5| Step: 2
Training loss: 2.8944528102874756
Validation loss: 2.2767564558213755

Epoch: 5| Step: 3
Training loss: 2.8204026222229004
Validation loss: 2.2739355461571806

Epoch: 5| Step: 4
Training loss: 2.60062837600708
Validation loss: 2.2782068009017618

Epoch: 5| Step: 5
Training loss: 2.1558549404144287
Validation loss: 2.2802761344499487

Epoch: 5| Step: 6
Training loss: 2.2507872581481934
Validation loss: 2.2784130983455206

Epoch: 5| Step: 7
Training loss: 2.8889522552490234
Validation loss: 2.2772993246714273

Epoch: 5| Step: 8
Training loss: 2.5991899967193604
Validation loss: 2.2697947486754386

Epoch: 5| Step: 9
Training loss: 2.8991827964782715
Validation loss: 2.277242704104352

Epoch: 5| Step: 10
Training loss: 2.066161632537842
Validation loss: 2.2797486858983196

Epoch: 41| Step: 0
Training loss: 2.67637300491333
Validation loss: 2.283478154931017

Epoch: 5| Step: 1
Training loss: 2.7571322917938232
Validation loss: 2.300865219485375

Epoch: 5| Step: 2
Training loss: 2.520430326461792
Validation loss: 2.2980764630020305

Epoch: 5| Step: 3
Training loss: 2.5066580772399902
Validation loss: 2.317127130364859

Epoch: 5| Step: 4
Training loss: 2.4605908393859863
Validation loss: 2.2692202188635386

Epoch: 5| Step: 5
Training loss: 3.0984127521514893
Validation loss: 2.2542102657338625

Epoch: 5| Step: 6
Training loss: 2.4814841747283936
Validation loss: 2.2476477751167874

Epoch: 5| Step: 7
Training loss: 1.9676939249038696
Validation loss: 2.2473460987050045

Epoch: 5| Step: 8
Training loss: 2.819955348968506
Validation loss: 2.2428296612155054

Epoch: 5| Step: 9
Training loss: 2.024829387664795
Validation loss: 2.2499291922456477

Epoch: 5| Step: 10
Training loss: 2.776108503341675
Validation loss: 2.259676266742009

Epoch: 42| Step: 0
Training loss: 2.0297462940216064
Validation loss: 2.2689397514507337

Epoch: 5| Step: 1
Training loss: 2.762603998184204
Validation loss: 2.2680853361724527

Epoch: 5| Step: 2
Training loss: 3.0578529834747314
Validation loss: 2.2664675661312637

Epoch: 5| Step: 3
Training loss: 2.7352116107940674
Validation loss: 2.269238769367177

Epoch: 5| Step: 4
Training loss: 2.798832416534424
Validation loss: 2.257126808166504

Epoch: 5| Step: 5
Training loss: 2.7890191078186035
Validation loss: 2.2553370024568293

Epoch: 5| Step: 6
Training loss: 2.1841115951538086
Validation loss: 2.2555669123126614

Epoch: 5| Step: 7
Training loss: 3.1703734397888184
Validation loss: 2.2488546422732774

Epoch: 5| Step: 8
Training loss: 1.845754623413086
Validation loss: 2.2510091438088367

Epoch: 5| Step: 9
Training loss: 2.648968458175659
Validation loss: 2.2531218067292245

Epoch: 5| Step: 10
Training loss: 2.04172682762146
Validation loss: 2.2535489374591458

Epoch: 43| Step: 0
Training loss: 2.51908802986145
Validation loss: 2.269798514663532

Epoch: 5| Step: 1
Training loss: 2.4502577781677246
Validation loss: 2.284597278923117

Epoch: 5| Step: 2
Training loss: 2.62214732170105
Validation loss: 2.2800339216827066

Epoch: 5| Step: 3
Training loss: 1.9251638650894165
Validation loss: 2.2970572030672463

Epoch: 5| Step: 4
Training loss: 2.7035305500030518
Validation loss: 2.322310742511544

Epoch: 5| Step: 5
Training loss: 3.203064441680908
Validation loss: 2.351533261678552

Epoch: 5| Step: 6
Training loss: 2.6846466064453125
Validation loss: 2.3716023891202864

Epoch: 5| Step: 7
Training loss: 2.8127970695495605
Validation loss: 2.320952025792932

Epoch: 5| Step: 8
Training loss: 2.4695076942443848
Validation loss: 2.302994579397222

Epoch: 5| Step: 9
Training loss: 2.5611281394958496
Validation loss: 2.2578811850599063

Epoch: 5| Step: 10
Training loss: 2.03591251373291
Validation loss: 2.234933114820911

Epoch: 44| Step: 0
Training loss: 2.4121832847595215
Validation loss: 2.228757526284905

Epoch: 5| Step: 1
Training loss: 2.7684168815612793
Validation loss: 2.225656465817523

Epoch: 5| Step: 2
Training loss: 2.9171223640441895
Validation loss: 2.23186662889296

Epoch: 5| Step: 3
Training loss: 2.6690943241119385
Validation loss: 2.2299808148414857

Epoch: 5| Step: 4
Training loss: 2.7486417293548584
Validation loss: 2.2398846533990677

Epoch: 5| Step: 5
Training loss: 2.501441717147827
Validation loss: 2.232781171798706

Epoch: 5| Step: 6
Training loss: 3.0089614391326904
Validation loss: 2.2334422014092885

Epoch: 5| Step: 7
Training loss: 2.825294017791748
Validation loss: 2.2363791773396153

Epoch: 5| Step: 8
Training loss: 2.1864829063415527
Validation loss: 2.232880356491253

Epoch: 5| Step: 9
Training loss: 2.0487911701202393
Validation loss: 2.2395231544330554

Epoch: 5| Step: 10
Training loss: 1.9522532224655151
Validation loss: 2.2514662383705057

Epoch: 45| Step: 0
Training loss: 2.7425942420959473
Validation loss: 2.2690691640300136

Epoch: 5| Step: 1
Training loss: 2.371589183807373
Validation loss: 2.26858280551049

Epoch: 5| Step: 2
Training loss: 2.011495590209961
Validation loss: 2.2647710897589244

Epoch: 5| Step: 3
Training loss: 2.8599495887756348
Validation loss: 2.2523874928874354

Epoch: 5| Step: 4
Training loss: 2.681441068649292
Validation loss: 2.254153367011778

Epoch: 5| Step: 5
Training loss: 2.8239917755126953
Validation loss: 2.257051790914228

Epoch: 5| Step: 6
Training loss: 2.3370120525360107
Validation loss: 2.2576560256301716

Epoch: 5| Step: 7
Training loss: 2.986349582672119
Validation loss: 2.256430724615692

Epoch: 5| Step: 8
Training loss: 2.407369375228882
Validation loss: 2.2363157015974804

Epoch: 5| Step: 9
Training loss: 2.2636477947235107
Validation loss: 2.236100294256723

Epoch: 5| Step: 10
Training loss: 2.3859832286834717
Validation loss: 2.23213460881223

Epoch: 46| Step: 0
Training loss: 2.7115767002105713
Validation loss: 2.2321527414424445

Epoch: 5| Step: 1
Training loss: 2.5440633296966553
Validation loss: 2.2294235972947973

Epoch: 5| Step: 2
Training loss: 2.8929481506347656
Validation loss: 2.2288101001452376

Epoch: 5| Step: 3
Training loss: 2.7256827354431152
Validation loss: 2.233442114245507

Epoch: 5| Step: 4
Training loss: 2.809521436691284
Validation loss: 2.2298823479683167

Epoch: 5| Step: 5
Training loss: 2.472412109375
Validation loss: 2.23867731196906

Epoch: 5| Step: 6
Training loss: 2.378178834915161
Validation loss: 2.238039606360979

Epoch: 5| Step: 7
Training loss: 1.731610655784607
Validation loss: 2.2608031072924213

Epoch: 5| Step: 8
Training loss: 2.1127660274505615
Validation loss: 2.283464559944727

Epoch: 5| Step: 9
Training loss: 2.430612564086914
Validation loss: 2.28676095316487

Epoch: 5| Step: 10
Training loss: 2.9810853004455566
Validation loss: 2.324859711431688

Epoch: 47| Step: 0
Training loss: 1.9193851947784424
Validation loss: 2.339166734808235

Epoch: 5| Step: 1
Training loss: 2.5073204040527344
Validation loss: 2.3103281746628466

Epoch: 5| Step: 2
Training loss: 3.2452054023742676
Validation loss: 2.2499956571927635

Epoch: 5| Step: 3
Training loss: 2.5199830532073975
Validation loss: 2.2345405278667325

Epoch: 5| Step: 4
Training loss: 2.533707618713379
Validation loss: 2.2216190420171267

Epoch: 5| Step: 5
Training loss: 2.6225171089172363
Validation loss: 2.2155931572760306

Epoch: 5| Step: 6
Training loss: 2.5306332111358643
Validation loss: 2.2069447168739895

Epoch: 5| Step: 7
Training loss: 2.720916271209717
Validation loss: 2.2035861092229045

Epoch: 5| Step: 8
Training loss: 2.458395481109619
Validation loss: 2.2081585430329844

Epoch: 5| Step: 9
Training loss: 2.4652161598205566
Validation loss: 2.221620108491631

Epoch: 5| Step: 10
Training loss: 2.372081995010376
Validation loss: 2.2264660635302143

Epoch: 48| Step: 0
Training loss: 1.9942474365234375
Validation loss: 2.23205671259152

Epoch: 5| Step: 1
Training loss: 2.308314561843872
Validation loss: 2.228666111987124

Epoch: 5| Step: 2
Training loss: 2.106116771697998
Validation loss: 2.2309925633092083

Epoch: 5| Step: 3
Training loss: 2.9114537239074707
Validation loss: 2.23138807025007

Epoch: 5| Step: 4
Training loss: 2.9008498191833496
Validation loss: 2.2145288913480696

Epoch: 5| Step: 5
Training loss: 2.6255316734313965
Validation loss: 2.2126196686939528

Epoch: 5| Step: 6
Training loss: 2.373661518096924
Validation loss: 2.199559380931239

Epoch: 5| Step: 7
Training loss: 2.7160983085632324
Validation loss: 2.209769636072138

Epoch: 5| Step: 8
Training loss: 2.549461841583252
Validation loss: 2.2035996811364287

Epoch: 5| Step: 9
Training loss: 2.494921922683716
Validation loss: 2.197821009543634

Epoch: 5| Step: 10
Training loss: 2.708510160446167
Validation loss: 2.1958970715922694

Epoch: 49| Step: 0
Training loss: 2.9280056953430176
Validation loss: 2.196042492825498

Epoch: 5| Step: 1
Training loss: 2.3740897178649902
Validation loss: 2.204921143029326

Epoch: 5| Step: 2
Training loss: 1.779874563217163
Validation loss: 2.2097868868099746

Epoch: 5| Step: 3
Training loss: 1.808588981628418
Validation loss: 2.219603411612972

Epoch: 5| Step: 4
Training loss: 2.9164772033691406
Validation loss: 2.2289387051777174

Epoch: 5| Step: 5
Training loss: 1.863849401473999
Validation loss: 2.239514130418019

Epoch: 5| Step: 6
Training loss: 2.2258174419403076
Validation loss: 2.235816829948015

Epoch: 5| Step: 7
Training loss: 3.2575314044952393
Validation loss: 2.2354183171385076

Epoch: 5| Step: 8
Training loss: 2.725517988204956
Validation loss: 2.2337200513450046

Epoch: 5| Step: 9
Training loss: 2.9262709617614746
Validation loss: 2.209331540651219

Epoch: 5| Step: 10
Training loss: 2.880862236022949
Validation loss: 2.2090606048542965

Epoch: 50| Step: 0
Training loss: 2.0793488025665283
Validation loss: 2.2017164332892305

Epoch: 5| Step: 1
Training loss: 2.9990458488464355
Validation loss: 2.2019766530682965

Epoch: 5| Step: 2
Training loss: 2.666712999343872
Validation loss: 2.196801854718116

Epoch: 5| Step: 3
Training loss: 2.565983533859253
Validation loss: 2.201452539813134

Epoch: 5| Step: 4
Training loss: 2.0182793140411377
Validation loss: 2.2270549676751576

Epoch: 5| Step: 5
Training loss: 2.843132734298706
Validation loss: 2.2724916755512194

Epoch: 5| Step: 6
Training loss: 2.5460238456726074
Validation loss: 2.350320854494649

Epoch: 5| Step: 7
Training loss: 2.4460463523864746
Validation loss: 2.3399735266162502

Epoch: 5| Step: 8
Training loss: 1.7544723749160767
Validation loss: 2.288158570566485

Epoch: 5| Step: 9
Training loss: 2.978445291519165
Validation loss: 2.2627730651568343

Epoch: 5| Step: 10
Training loss: 2.7919085025787354
Validation loss: 2.2102171374905493

Epoch: 51| Step: 0
Training loss: 2.1401760578155518
Validation loss: 2.1916465810550156

Epoch: 5| Step: 1
Training loss: 2.7599399089813232
Validation loss: 2.194084458453681

Epoch: 5| Step: 2
Training loss: 2.532944917678833
Validation loss: 2.1784776051839194

Epoch: 5| Step: 3
Training loss: 2.6774768829345703
Validation loss: 2.1814442732000865

Epoch: 5| Step: 4
Training loss: 2.8153655529022217
Validation loss: 2.175615022259374

Epoch: 5| Step: 5
Training loss: 2.4502921104431152
Validation loss: 2.1879984178850727

Epoch: 5| Step: 6
Training loss: 2.6553428173065186
Validation loss: 2.182626644770304

Epoch: 5| Step: 7
Training loss: 2.5652353763580322
Validation loss: 2.1856665508721465

Epoch: 5| Step: 8
Training loss: 2.5428223609924316
Validation loss: 2.192986390923941

Epoch: 5| Step: 9
Training loss: 2.352614164352417
Validation loss: 2.218971557514642

Epoch: 5| Step: 10
Training loss: 1.9535928964614868
Validation loss: 2.265465754334645

Epoch: 52| Step: 0
Training loss: 2.6155123710632324
Validation loss: 2.3206077621829126

Epoch: 5| Step: 1
Training loss: 2.0181045532226562
Validation loss: 2.318616364591865

Epoch: 5| Step: 2
Training loss: 2.8839633464813232
Validation loss: 2.3107567089860157

Epoch: 5| Step: 3
Training loss: 2.297384738922119
Validation loss: 2.2322681834620814

Epoch: 5| Step: 4
Training loss: 2.453174114227295
Validation loss: 2.181193300472793

Epoch: 5| Step: 5
Training loss: 2.3435001373291016
Validation loss: 2.1945302935056787

Epoch: 5| Step: 6
Training loss: 2.815852165222168
Validation loss: 2.2564069917125087

Epoch: 5| Step: 7
Training loss: 2.3436596393585205
Validation loss: 2.2443316469910326

Epoch: 5| Step: 8
Training loss: 2.5625007152557373
Validation loss: 2.2502576074292584

Epoch: 5| Step: 9
Training loss: 2.7504405975341797
Validation loss: 2.2364622623689714

Epoch: 5| Step: 10
Training loss: 3.250406503677368
Validation loss: 2.230387044209306

Epoch: 53| Step: 0
Training loss: 2.5157501697540283
Validation loss: 2.2181416096225863

Epoch: 5| Step: 1
Training loss: 2.7568135261535645
Validation loss: 2.2147134914193103

Epoch: 5| Step: 2
Training loss: 2.328713893890381
Validation loss: 2.2206907631248556

Epoch: 5| Step: 3
Training loss: 2.869997501373291
Validation loss: 2.275109068039925

Epoch: 5| Step: 4
Training loss: 2.592043876647949
Validation loss: 2.336358326737599

Epoch: 5| Step: 5
Training loss: 3.268538236618042
Validation loss: 2.422348750534878

Epoch: 5| Step: 6
Training loss: 2.328352928161621
Validation loss: 2.4659519708284767

Epoch: 5| Step: 7
Training loss: 2.183333396911621
Validation loss: 2.5106967649152203

Epoch: 5| Step: 8
Training loss: 2.0620834827423096
Validation loss: 2.464662615970899

Epoch: 5| Step: 9
Training loss: 2.489250898361206
Validation loss: 2.3657694016733477

Epoch: 5| Step: 10
Training loss: 2.677393913269043
Validation loss: 2.2849657638098604

Epoch: 54| Step: 0
Training loss: 2.304945468902588
Validation loss: 2.208309062065617

Epoch: 5| Step: 1
Training loss: 2.500938653945923
Validation loss: 2.209989950221072

Epoch: 5| Step: 2
Training loss: 2.922719955444336
Validation loss: 2.2198894228986514

Epoch: 5| Step: 3
Training loss: 2.770822048187256
Validation loss: 2.234945233150195

Epoch: 5| Step: 4
Training loss: 2.265254497528076
Validation loss: 2.223584072564238

Epoch: 5| Step: 5
Training loss: 2.115513563156128
Validation loss: 2.208806017393707

Epoch: 5| Step: 6
Training loss: 2.8345656394958496
Validation loss: 2.181489452239006

Epoch: 5| Step: 7
Training loss: 2.7489280700683594
Validation loss: 2.181401373237692

Epoch: 5| Step: 8
Training loss: 2.4441254138946533
Validation loss: 2.1956262178318475

Epoch: 5| Step: 9
Training loss: 2.3506505489349365
Validation loss: 2.1998043880667737

Epoch: 5| Step: 10
Training loss: 2.6136112213134766
Validation loss: 2.219201757061866

Epoch: 55| Step: 0
Training loss: 2.4227938652038574
Validation loss: 2.217738425859841

Epoch: 5| Step: 1
Training loss: 2.5006115436553955
Validation loss: 2.2116449571424917

Epoch: 5| Step: 2
Training loss: 2.364713430404663
Validation loss: 2.193075321053946

Epoch: 5| Step: 3
Training loss: 2.5268805027008057
Validation loss: 2.192760044528592

Epoch: 5| Step: 4
Training loss: 2.5229225158691406
Validation loss: 2.181716929199875

Epoch: 5| Step: 5
Training loss: 2.256483793258667
Validation loss: 2.1910093907387025

Epoch: 5| Step: 6
Training loss: 2.567988157272339
Validation loss: 2.1826422240144465

Epoch: 5| Step: 7
Training loss: 2.779972553253174
Validation loss: 2.1825923996586956

Epoch: 5| Step: 8
Training loss: 2.2106151580810547
Validation loss: 2.1709170649128575

Epoch: 5| Step: 9
Training loss: 2.452913284301758
Validation loss: 2.160591648470971

Epoch: 5| Step: 10
Training loss: 2.6901986598968506
Validation loss: 2.147851695296585

Epoch: 56| Step: 0
Training loss: 2.0852668285369873
Validation loss: 2.1562795075037147

Epoch: 5| Step: 1
Training loss: 2.458122968673706
Validation loss: 2.15477014869772

Epoch: 5| Step: 2
Training loss: 2.5158443450927734
Validation loss: 2.160053146782742

Epoch: 5| Step: 3
Training loss: 2.2519333362579346
Validation loss: 2.1832460523933492

Epoch: 5| Step: 4
Training loss: 2.002713203430176
Validation loss: 2.1732847536763837

Epoch: 5| Step: 5
Training loss: 2.4335525035858154
Validation loss: 2.2261311469539518

Epoch: 5| Step: 6
Training loss: 3.0460362434387207
Validation loss: 2.3424091287838515

Epoch: 5| Step: 7
Training loss: 2.4122672080993652
Validation loss: 2.365123574451734

Epoch: 5| Step: 8
Training loss: 3.2057080268859863
Validation loss: 2.3078655863320954

Epoch: 5| Step: 9
Training loss: 2.682366371154785
Validation loss: 2.2367387330660256

Epoch: 5| Step: 10
Training loss: 2.4372951984405518
Validation loss: 2.2079804994726695

Epoch: 57| Step: 0
Training loss: 2.4334230422973633
Validation loss: 2.1627249922803653

Epoch: 5| Step: 1
Training loss: 2.7973387241363525
Validation loss: 2.148237802649057

Epoch: 5| Step: 2
Training loss: 2.38114595413208
Validation loss: 2.1398955891209264

Epoch: 5| Step: 3
Training loss: 2.4987263679504395
Validation loss: 2.140787806562198

Epoch: 5| Step: 4
Training loss: 2.8408703804016113
Validation loss: 2.139858122794859

Epoch: 5| Step: 5
Training loss: 2.0417001247406006
Validation loss: 2.139017733194495

Epoch: 5| Step: 6
Training loss: 2.489037036895752
Validation loss: 2.1347473539331907

Epoch: 5| Step: 7
Training loss: 2.2816174030303955
Validation loss: 2.1321123799970074

Epoch: 5| Step: 8
Training loss: 3.202178955078125
Validation loss: 2.1418441572496967

Epoch: 5| Step: 9
Training loss: 2.1525113582611084
Validation loss: 2.145414106307491

Epoch: 5| Step: 10
Training loss: 1.9995206594467163
Validation loss: 2.1597501693233365

Epoch: 58| Step: 0
Training loss: 2.6665878295898438
Validation loss: 2.173584859858277

Epoch: 5| Step: 1
Training loss: 2.501443386077881
Validation loss: 2.216091981498144

Epoch: 5| Step: 2
Training loss: 2.6070852279663086
Validation loss: 2.1879171504769275

Epoch: 5| Step: 3
Training loss: 2.165999174118042
Validation loss: 2.2209445379113637

Epoch: 5| Step: 4
Training loss: 1.8103125095367432
Validation loss: 2.2785988212913595

Epoch: 5| Step: 5
Training loss: 3.255070924758911
Validation loss: 2.3470122480905182

Epoch: 5| Step: 6
Training loss: 2.4599342346191406
Validation loss: 2.3093790008175756

Epoch: 5| Step: 7
Training loss: 2.7055320739746094
Validation loss: 2.2488849380964875

Epoch: 5| Step: 8
Training loss: 2.489264965057373
Validation loss: 2.2288001250195246

Epoch: 5| Step: 9
Training loss: 2.3700690269470215
Validation loss: 2.180782659079439

Epoch: 5| Step: 10
Training loss: 2.2549686431884766
Validation loss: 2.1524955893075592

Epoch: 59| Step: 0
Training loss: 3.1177010536193848
Validation loss: 2.1449996015076995

Epoch: 5| Step: 1
Training loss: 2.510129928588867
Validation loss: 2.129856260873938

Epoch: 5| Step: 2
Training loss: 2.690645217895508
Validation loss: 2.148635713003015

Epoch: 5| Step: 3
Training loss: 2.001579761505127
Validation loss: 2.160934345696562

Epoch: 5| Step: 4
Training loss: 2.1454434394836426
Validation loss: 2.1741607419906126

Epoch: 5| Step: 5
Training loss: 2.9003894329071045
Validation loss: 2.1716998302808372

Epoch: 5| Step: 6
Training loss: 2.220268726348877
Validation loss: 2.159516062787784

Epoch: 5| Step: 7
Training loss: 2.7505555152893066
Validation loss: 2.1521501694956133

Epoch: 5| Step: 8
Training loss: 2.396942615509033
Validation loss: 2.140453436041391

Epoch: 5| Step: 9
Training loss: 2.759352684020996
Validation loss: 2.1505331582920526

Epoch: 5| Step: 10
Training loss: 1.957189679145813
Validation loss: 2.173648544537124

Epoch: 60| Step: 0
Training loss: 3.0343949794769287
Validation loss: 2.190631366545154

Epoch: 5| Step: 1
Training loss: 2.2818048000335693
Validation loss: 2.2329874551424416

Epoch: 5| Step: 2
Training loss: 2.5613811016082764
Validation loss: 2.2565556931239303

Epoch: 5| Step: 3
Training loss: 2.8145313262939453
Validation loss: 2.2938797730271534

Epoch: 5| Step: 4
Training loss: 2.406273365020752
Validation loss: 2.316804629500194

Epoch: 5| Step: 5
Training loss: 2.96488618850708
Validation loss: 2.3426566200871624

Epoch: 5| Step: 6
Training loss: 2.2989537715911865
Validation loss: 2.3251238869082544

Epoch: 5| Step: 7
Training loss: 2.392876148223877
Validation loss: 2.288122666779385

Epoch: 5| Step: 8
Training loss: 2.187481641769409
Validation loss: 2.262076885469498

Epoch: 5| Step: 9
Training loss: 2.4061689376831055
Validation loss: 2.2143010606047926

Epoch: 5| Step: 10
Training loss: 2.003925323486328
Validation loss: 2.1760885612938994

Epoch: 61| Step: 0
Training loss: 2.8112857341766357
Validation loss: 2.1398883481179514

Epoch: 5| Step: 1
Training loss: 2.150954484939575
Validation loss: 2.1410144811035483

Epoch: 5| Step: 2
Training loss: 2.624393939971924
Validation loss: 2.1354174331952165

Epoch: 5| Step: 3
Training loss: 2.5153467655181885
Validation loss: 2.128089307456888

Epoch: 5| Step: 4
Training loss: 1.835913896560669
Validation loss: 2.1372179485136464

Epoch: 5| Step: 5
Training loss: 2.6833834648132324
Validation loss: 2.1312596182669363

Epoch: 5| Step: 6
Training loss: 2.521939516067505
Validation loss: 2.135709326754334

Epoch: 5| Step: 7
Training loss: 2.2051758766174316
Validation loss: 2.1409219926403416

Epoch: 5| Step: 8
Training loss: 2.0961787700653076
Validation loss: 2.1404489253156926

Epoch: 5| Step: 9
Training loss: 3.276984453201294
Validation loss: 2.1594610957689184

Epoch: 5| Step: 10
Training loss: 2.667426347732544
Validation loss: 2.1632875434813963

Epoch: 62| Step: 0
Training loss: 2.2256903648376465
Validation loss: 2.1578008513296805

Epoch: 5| Step: 1
Training loss: 2.715366840362549
Validation loss: 2.158063009221067

Epoch: 5| Step: 2
Training loss: 2.5149176120758057
Validation loss: 2.1527478464188112

Epoch: 5| Step: 3
Training loss: 2.879443883895874
Validation loss: 2.162136439354189

Epoch: 5| Step: 4
Training loss: 1.748978853225708
Validation loss: 2.1839366676986858

Epoch: 5| Step: 5
Training loss: 2.6014466285705566
Validation loss: 2.1994142352893786

Epoch: 5| Step: 6
Training loss: 2.184391975402832
Validation loss: 2.2285759154186455

Epoch: 5| Step: 7
Training loss: 2.4926860332489014
Validation loss: 2.2967044666249263

Epoch: 5| Step: 8
Training loss: 2.153374195098877
Validation loss: 2.3434094921235116

Epoch: 5| Step: 9
Training loss: 2.6732017993927
Validation loss: 2.3513737929764615

Epoch: 5| Step: 10
Training loss: 3.53163480758667
Validation loss: 2.3297437083336616

Epoch: 63| Step: 0
Training loss: 2.0909037590026855
Validation loss: 2.21590220415464

Epoch: 5| Step: 1
Training loss: 2.532550096511841
Validation loss: 2.1672877573197886

Epoch: 5| Step: 2
Training loss: 2.4592525959014893
Validation loss: 2.128519963192683

Epoch: 5| Step: 3
Training loss: 2.1394667625427246
Validation loss: 2.1132649837001676

Epoch: 5| Step: 4
Training loss: 2.6494083404541016
Validation loss: 2.1062872089365476

Epoch: 5| Step: 5
Training loss: 2.4887702465057373
Validation loss: 2.1185114973334858

Epoch: 5| Step: 6
Training loss: 2.5981743335723877
Validation loss: 2.1177090393599642

Epoch: 5| Step: 7
Training loss: 2.8270206451416016
Validation loss: 2.1151615265877015

Epoch: 5| Step: 8
Training loss: 2.7971277236938477
Validation loss: 2.115899734599616

Epoch: 5| Step: 9
Training loss: 2.4952759742736816
Validation loss: 2.1103611479523363

Epoch: 5| Step: 10
Training loss: 2.4366698265075684
Validation loss: 2.1108651673921974

Epoch: 64| Step: 0
Training loss: 2.119528293609619
Validation loss: 2.111795574106196

Epoch: 5| Step: 1
Training loss: 2.4813103675842285
Validation loss: 2.11778716118105

Epoch: 5| Step: 2
Training loss: 2.225773334503174
Validation loss: 2.126674311135405

Epoch: 5| Step: 3
Training loss: 2.4593722820281982
Validation loss: 2.136641849753677

Epoch: 5| Step: 4
Training loss: 1.9360157251358032
Validation loss: 2.176871591998685

Epoch: 5| Step: 5
Training loss: 2.715489387512207
Validation loss: 2.235663096110026

Epoch: 5| Step: 6
Training loss: 2.9777512550354004
Validation loss: 2.2756413349541287

Epoch: 5| Step: 7
Training loss: 2.245272159576416
Validation loss: 2.3059882528038433

Epoch: 5| Step: 8
Training loss: 2.9353229999542236
Validation loss: 2.2927933431440786

Epoch: 5| Step: 9
Training loss: 2.894557476043701
Validation loss: 2.2397887911847842

Epoch: 5| Step: 10
Training loss: 2.3455379009246826
Validation loss: 2.1805797135958107

Epoch: 65| Step: 0
Training loss: 2.516035318374634
Validation loss: 2.1304465006756526

Epoch: 5| Step: 1
Training loss: 2.674671173095703
Validation loss: 2.1190757918101486

Epoch: 5| Step: 2
Training loss: 2.637868642807007
Validation loss: 2.1108030093613492

Epoch: 5| Step: 3
Training loss: 2.404754161834717
Validation loss: 2.1006823367969965

Epoch: 5| Step: 4
Training loss: 2.228766679763794
Validation loss: 2.0980156134533625

Epoch: 5| Step: 5
Training loss: 2.22060227394104
Validation loss: 2.098650816948183

Epoch: 5| Step: 6
Training loss: 2.3980913162231445
Validation loss: 2.099909092790337

Epoch: 5| Step: 7
Training loss: 2.5419392585754395
Validation loss: 2.104724020086309

Epoch: 5| Step: 8
Training loss: 2.949258327484131
Validation loss: 2.105761548524262

Epoch: 5| Step: 9
Training loss: 2.806837558746338
Validation loss: 2.112012311976443

Epoch: 5| Step: 10
Training loss: 1.4250683784484863
Validation loss: 2.1197945789624284

Epoch: 66| Step: 0
Training loss: 2.3604724407196045
Validation loss: 2.1367696485211773

Epoch: 5| Step: 1
Training loss: 2.8573343753814697
Validation loss: 2.140343468676331

Epoch: 5| Step: 2
Training loss: 1.4807980060577393
Validation loss: 2.1560711886293147

Epoch: 5| Step: 3
Training loss: 2.3417305946350098
Validation loss: 2.1769615552758657

Epoch: 5| Step: 4
Training loss: 2.5552401542663574
Validation loss: 2.1568556165182464

Epoch: 5| Step: 5
Training loss: 2.5030505657196045
Validation loss: 2.162029799594674

Epoch: 5| Step: 6
Training loss: 2.641005277633667
Validation loss: 2.1274818374264624

Epoch: 5| Step: 7
Training loss: 3.214977979660034
Validation loss: 2.1073872453422955

Epoch: 5| Step: 8
Training loss: 1.9203674793243408
Validation loss: 2.0994672390722458

Epoch: 5| Step: 9
Training loss: 2.9514031410217285
Validation loss: 2.094737995055414

Epoch: 5| Step: 10
Training loss: 1.9602086544036865
Validation loss: 2.0964588067864858

Epoch: 67| Step: 0
Training loss: 2.7459001541137695
Validation loss: 2.0941043707632248

Epoch: 5| Step: 1
Training loss: 2.788179874420166
Validation loss: 2.100301719480945

Epoch: 5| Step: 2
Training loss: 1.5854735374450684
Validation loss: 2.0999899730887464

Epoch: 5| Step: 3
Training loss: 2.869035482406616
Validation loss: 2.111950312891314

Epoch: 5| Step: 4
Training loss: 1.8601058721542358
Validation loss: 2.116474114438539

Epoch: 5| Step: 5
Training loss: 3.0017917156219482
Validation loss: 2.1394501578423286

Epoch: 5| Step: 6
Training loss: 2.4197998046875
Validation loss: 2.15612615564818

Epoch: 5| Step: 7
Training loss: 1.9967658519744873
Validation loss: 2.1360335978128577

Epoch: 5| Step: 8
Training loss: 2.7205703258514404
Validation loss: 2.1160350999524518

Epoch: 5| Step: 9
Training loss: 2.7195849418640137
Validation loss: 2.1198646637701217

Epoch: 5| Step: 10
Training loss: 2.2220475673675537
Validation loss: 2.11239271010122

Epoch: 68| Step: 0
Training loss: 2.206364631652832
Validation loss: 2.1214109851467993

Epoch: 5| Step: 1
Training loss: 2.7212014198303223
Validation loss: 2.115152641009259

Epoch: 5| Step: 2
Training loss: 1.8938487768173218
Validation loss: 2.1400377263305006

Epoch: 5| Step: 3
Training loss: 3.0985331535339355
Validation loss: 2.1607971242679063

Epoch: 5| Step: 4
Training loss: 2.464467763900757
Validation loss: 2.1934348383257465

Epoch: 5| Step: 5
Training loss: 2.760097026824951
Validation loss: 2.2086553291607927

Epoch: 5| Step: 6
Training loss: 2.8593955039978027
Validation loss: 2.21935191974845

Epoch: 5| Step: 7
Training loss: 2.243157148361206
Validation loss: 2.2014116881996073

Epoch: 5| Step: 8
Training loss: 2.450897455215454
Validation loss: 2.169125326218144

Epoch: 5| Step: 9
Training loss: 2.2830026149749756
Validation loss: 2.1295475857232207

Epoch: 5| Step: 10
Training loss: 1.7756656408309937
Validation loss: 2.1003737218918337

Epoch: 69| Step: 0
Training loss: 2.321239948272705
Validation loss: 2.0964926878611245

Epoch: 5| Step: 1
Training loss: 3.0294620990753174
Validation loss: 2.1002076031059347

Epoch: 5| Step: 2
Training loss: 3.0064845085144043
Validation loss: 2.1101320558978665

Epoch: 5| Step: 3
Training loss: 2.728684902191162
Validation loss: 2.112175744066956

Epoch: 5| Step: 4
Training loss: 1.9205400943756104
Validation loss: 2.1222924135064565

Epoch: 5| Step: 5
Training loss: 2.517348051071167
Validation loss: 2.1415611056871313

Epoch: 5| Step: 6
Training loss: 1.7328609228134155
Validation loss: 2.158060589144307

Epoch: 5| Step: 7
Training loss: 2.597705364227295
Validation loss: 2.141157541223752

Epoch: 5| Step: 8
Training loss: 2.0831663608551025
Validation loss: 2.1210422131323043

Epoch: 5| Step: 9
Training loss: 2.319638729095459
Validation loss: 2.1018800863655667

Epoch: 5| Step: 10
Training loss: 2.3319592475891113
Validation loss: 2.0975461698347524

Epoch: 70| Step: 0
Training loss: 2.839156150817871
Validation loss: 2.100273000296726

Epoch: 5| Step: 1
Training loss: 2.171586275100708
Validation loss: 2.095053816354403

Epoch: 5| Step: 2
Training loss: 2.59879994392395
Validation loss: 2.093807174313453

Epoch: 5| Step: 3
Training loss: 1.7949044704437256
Validation loss: 2.0923903347343527

Epoch: 5| Step: 4
Training loss: 2.278992176055908
Validation loss: 2.100042607194634

Epoch: 5| Step: 5
Training loss: 3.0006556510925293
Validation loss: 2.1115577451644407

Epoch: 5| Step: 6
Training loss: 2.395267963409424
Validation loss: 2.1226989479475122

Epoch: 5| Step: 7
Training loss: 2.10518741607666
Validation loss: 2.12559034234734

Epoch: 5| Step: 8
Training loss: 2.771850109100342
Validation loss: 2.1503409993263984

Epoch: 5| Step: 9
Training loss: 2.492048740386963
Validation loss: 2.1769096543712

Epoch: 5| Step: 10
Training loss: 2.268578052520752
Validation loss: 2.1744096843145226

Epoch: 71| Step: 0
Training loss: 2.011671543121338
Validation loss: 2.1231869869334723

Epoch: 5| Step: 1
Training loss: 2.7530906200408936
Validation loss: 2.102762117180773

Epoch: 5| Step: 2
Training loss: 3.200984239578247
Validation loss: 2.1008464777341453

Epoch: 5| Step: 3
Training loss: 1.7387285232543945
Validation loss: 2.09417127537471

Epoch: 5| Step: 4
Training loss: 2.7004764080047607
Validation loss: 2.0764548752897527

Epoch: 5| Step: 5
Training loss: 2.607067346572876
Validation loss: 2.075483088852257

Epoch: 5| Step: 6
Training loss: 2.3048176765441895
Validation loss: 2.0828215870805966

Epoch: 5| Step: 7
Training loss: 2.2349698543548584
Validation loss: 2.0903866111591296

Epoch: 5| Step: 8
Training loss: 2.385439395904541
Validation loss: 2.1195612620281916

Epoch: 5| Step: 9
Training loss: 1.973773717880249
Validation loss: 2.1420558062932824

Epoch: 5| Step: 10
Training loss: 2.7107343673706055
Validation loss: 2.1610500812530518

Epoch: 72| Step: 0
Training loss: 2.8193788528442383
Validation loss: 2.1568462156480357

Epoch: 5| Step: 1
Training loss: 2.0944104194641113
Validation loss: 2.122660162628338

Epoch: 5| Step: 2
Training loss: 2.8298213481903076
Validation loss: 2.0742891552627727

Epoch: 5| Step: 3
Training loss: 2.5614476203918457
Validation loss: 2.069261686776274

Epoch: 5| Step: 4
Training loss: 2.6478686332702637
Validation loss: 2.0704594453175864

Epoch: 5| Step: 5
Training loss: 2.244894504547119
Validation loss: 2.072783403499152

Epoch: 5| Step: 6
Training loss: 2.694927930831909
Validation loss: 2.074848569849486

Epoch: 5| Step: 7
Training loss: 1.7432836294174194
Validation loss: 2.0700667750450874

Epoch: 5| Step: 8
Training loss: 2.2203052043914795
Validation loss: 2.0631672028572328

Epoch: 5| Step: 9
Training loss: 2.77095365524292
Validation loss: 2.073184972168297

Epoch: 5| Step: 10
Training loss: 2.0884039402008057
Validation loss: 2.0878400559066446

Epoch: 73| Step: 0
Training loss: 2.0993714332580566
Validation loss: 2.1124023391354467

Epoch: 5| Step: 1
Training loss: 2.303831100463867
Validation loss: 2.1319842415471233

Epoch: 5| Step: 2
Training loss: 2.0701472759246826
Validation loss: 2.1784144524605042

Epoch: 5| Step: 3
Training loss: 2.612182140350342
Validation loss: 2.2309846467869257

Epoch: 5| Step: 4
Training loss: 2.137312889099121
Validation loss: 2.2311765904067666

Epoch: 5| Step: 5
Training loss: 2.2664437294006348
Validation loss: 2.226743621210898

Epoch: 5| Step: 6
Training loss: 2.7622063159942627
Validation loss: 2.1610472151028213

Epoch: 5| Step: 7
Training loss: 3.3394370079040527
Validation loss: 2.123361764415618

Epoch: 5| Step: 8
Training loss: 2.6265578269958496
Validation loss: 2.0936051760950396

Epoch: 5| Step: 9
Training loss: 2.496289014816284
Validation loss: 2.0762268292006625

Epoch: 5| Step: 10
Training loss: 1.6785590648651123
Validation loss: 2.062133317352623

Epoch: 74| Step: 0
Training loss: 2.694889783859253
Validation loss: 2.05950786477776

Epoch: 5| Step: 1
Training loss: 3.1849708557128906
Validation loss: 2.0636455064178794

Epoch: 5| Step: 2
Training loss: 1.9298069477081299
Validation loss: 2.062267282957672

Epoch: 5| Step: 3
Training loss: 2.245175838470459
Validation loss: 2.0581309051923853

Epoch: 5| Step: 4
Training loss: 2.458719253540039
Validation loss: 2.060564958921043

Epoch: 5| Step: 5
Training loss: 2.313283920288086
Validation loss: 2.06506444305502

Epoch: 5| Step: 6
Training loss: 2.2762858867645264
Validation loss: 2.071796376218078

Epoch: 5| Step: 7
Training loss: 2.322533130645752
Validation loss: 2.1152894522554133

Epoch: 5| Step: 8
Training loss: 2.523772716522217
Validation loss: 2.185798778328844

Epoch: 5| Step: 9
Training loss: 2.0718226432800293
Validation loss: 2.2584090194394513

Epoch: 5| Step: 10
Training loss: 2.5385282039642334
Validation loss: 2.2330399533753753

Epoch: 75| Step: 0
Training loss: 2.5488784313201904
Validation loss: 2.21870094473644

Epoch: 5| Step: 1
Training loss: 2.708742618560791
Validation loss: 2.150254189327199

Epoch: 5| Step: 2
Training loss: 2.856881856918335
Validation loss: 2.0753536429456485

Epoch: 5| Step: 3
Training loss: 2.4388396739959717
Validation loss: 2.0607593033903386

Epoch: 5| Step: 4
Training loss: 2.615947961807251
Validation loss: 2.095756075715506

Epoch: 5| Step: 5
Training loss: 2.37591552734375
Validation loss: 2.1640635011016682

Epoch: 5| Step: 6
Training loss: 2.334563732147217
Validation loss: 2.170943119192636

Epoch: 5| Step: 7
Training loss: 2.8625810146331787
Validation loss: 2.12852979219088

Epoch: 5| Step: 8
Training loss: 1.9604307413101196
Validation loss: 2.094318818020564

Epoch: 5| Step: 9
Training loss: 2.527536392211914
Validation loss: 2.0690216300308064

Epoch: 5| Step: 10
Training loss: 2.055847406387329
Validation loss: 2.065730797347202

Testing loss: 2.3243823051452637

Epoch: 1| Step: 0
Training loss: 6.718205487564706
Validation loss: 6.780332069811394
Epoch: 5| Step: 1
Training loss: 7.648066417198206
Validation loss: 6.774595905969162
Epoch: 5| Step: 2
Training loss: 7.307485572886834
Validation loss: 6.768936661549896
Epoch: 5| Step: 3
Training loss: 7.469152208296023
Validation loss: 6.766301209204888
Epoch: 5| Step: 4
Training loss: 7.064094025985143
Validation loss: 6.763954145805963
Epoch: 5| Step: 5
Training loss: 6.674133347388311
Validation loss: 6.76010262066823
Epoch: 5| Step: 6
Training loss: 6.703786537231595
Validation loss: 6.751952302339539
Epoch: 5| Step: 7
Training loss: 7.531939336482805
Validation loss: 6.742227119915948
Epoch: 5| Step: 8
Training loss: 6.753828022880541
Validation loss: 6.748028302991429
Epoch: 5| Step: 9
Training loss: 6.917705166844756
Validation loss: 6.746014061450445
Epoch: 2| Step: 0
Training loss: 7.69793191855799
Validation loss: 6.731419193688327
Epoch: 5| Step: 1
Training loss: 6.6415549670518494
Validation loss: 6.7273636360243785
Epoch: 5| Step: 2
Training loss: 6.443776746485319
Validation loss: 6.739150838738679
Epoch: 5| Step: 3
Training loss: 7.101068829960179
Validation loss: 6.734598312970469
Epoch: 5| Step: 4
Training loss: 7.461515234503415
Validation loss: 6.720535129414098
Epoch: 5| Step: 5
Training loss: 6.860504961029655
Validation loss: 6.725933760012554
Epoch: 5| Step: 6
Training loss: 7.701782852459897
Validation loss: 6.713552667470714
Epoch: 5| Step: 7
Training loss: 6.542065585986406
Validation loss: 6.723619476098287
Epoch: 5| Step: 8
Training loss: 7.478185081001465
Validation loss: 6.700497605623246
Epoch: 5| Step: 9
Training loss: 6.437562812572807
Validation loss: 6.70375918540465
Epoch: 3| Step: 0
Training loss: 7.468908332199873
Validation loss: 6.704535554828097
Epoch: 5| Step: 1
Training loss: 6.850812499037808
Validation loss: 6.7108640418734575
Epoch: 5| Step: 2
Training loss: 6.624512852446326
Validation loss: 6.710786480454945
Epoch: 5| Step: 3
Training loss: 7.534499104639439
Validation loss: 6.698901367597405
Epoch: 5| Step: 4
Training loss: 7.234629154890323
Validation loss: 6.687210687499482
Epoch: 5| Step: 5
Training loss: 7.257447988104967
Validation loss: 6.6911928855093725
Epoch: 5| Step: 6
Training loss: 6.600855176604013
Validation loss: 6.694730958461811
Epoch: 5| Step: 7
Training loss: 6.613571097550363
Validation loss: 6.690254454412291
Epoch: 5| Step: 8
Training loss: 6.766589089435935
Validation loss: 6.688252487255937
Epoch: 5| Step: 9
Training loss: 7.2036852835919385
Validation loss: 6.686208668504468
Epoch: 4| Step: 0
Training loss: 6.998985216971302
Validation loss: 6.681731720959902
Epoch: 5| Step: 1
Training loss: 6.915741211284937
Validation loss: 6.68060213090482
Epoch: 5| Step: 2
Training loss: 7.113483960250025
Validation loss: 6.679972251064918
Epoch: 5| Step: 3
Training loss: 7.237650910222334
Validation loss: 6.669612759415429
Epoch: 5| Step: 4
Training loss: 7.263682641074974
Validation loss: 6.667339882005127
Epoch: 5| Step: 5
Training loss: 7.506374447669981
Validation loss: 6.665228304599279
Epoch: 5| Step: 6
Training loss: 6.418716433547822
Validation loss: 6.658596616547226
Epoch: 5| Step: 7
Training loss: 6.688570311272648
Validation loss: 6.661615269203763
Epoch: 5| Step: 8
Training loss: 6.951799656429604
Validation loss: 6.656843772737365
Epoch: 5| Step: 9
Training loss: 6.7779746869499
Validation loss: 6.64831385181143
Epoch: 5| Step: 0
Training loss: 7.065872872352162
Validation loss: 6.652067089518236
Epoch: 5| Step: 1
Training loss: 6.948801543828102
Validation loss: 6.645869435112963
Epoch: 5| Step: 2
Training loss: 6.614596565613815
Validation loss: 6.626640366159388
Epoch: 5| Step: 3
Training loss: 7.1897895151878455
Validation loss: 6.633272810289745
Epoch: 5| Step: 4
Training loss: 7.595666211902463
Validation loss: 6.637508684299534
Epoch: 5| Step: 5
Training loss: 7.205550639747989
Validation loss: 6.633657311323075
Epoch: 5| Step: 6
Training loss: 7.052892851653447
Validation loss: 6.629482147909772
Epoch: 5| Step: 7
Training loss: 5.917592938768672
Validation loss: 6.625825633368818
Epoch: 5| Step: 8
Training loss: 6.774933716847404
Validation loss: 6.622638920976303
Epoch: 5| Step: 9
Training loss: 7.156697488311839
Validation loss: 6.607906822478897
Epoch: 6| Step: 0
Training loss: 7.949847852102708
Validation loss: 6.5980329279255665
Epoch: 5| Step: 1
Training loss: 6.614991311134635
Validation loss: 6.610392888394986
Epoch: 5| Step: 2
Training loss: 6.4153470090162505
Validation loss: 6.613265687439408
Epoch: 5| Step: 3
Training loss: 6.968325751695186
Validation loss: 6.595320017501842
Epoch: 5| Step: 4
Training loss: 7.341818178008466
Validation loss: 6.599568941069551
Epoch: 5| Step: 5
Training loss: 6.313623007413123
Validation loss: 6.592410506230857
Epoch: 5| Step: 6
Training loss: 6.9980847599623415
Validation loss: 6.5937047770830794
Epoch: 5| Step: 7
Training loss: 6.722727746932416
Validation loss: 6.58538030120143
Epoch: 5| Step: 8
Training loss: 6.88111189617887
Validation loss: 6.591283146268319
Epoch: 5| Step: 9
Training loss: 6.967139361588016
Validation loss: 6.5824625174285245
Epoch: 7| Step: 0
Training loss: 6.950780306037084
Validation loss: 6.579960358595724
Epoch: 5| Step: 1
Training loss: 6.726895385019459
Validation loss: 6.580116543254236
Epoch: 5| Step: 2
Training loss: 6.9528800321138
Validation loss: 6.569935553529537
Epoch: 5| Step: 3
Training loss: 6.638692561199015
Validation loss: 6.553011850169648
Epoch: 5| Step: 4
Training loss: 6.363089711378335
Validation loss: 6.56106657851622
Epoch: 5| Step: 5
Training loss: 7.607604899916426
Validation loss: 6.556735779897646
Epoch: 5| Step: 6
Training loss: 6.835375813977644
Validation loss: 6.5408723949493845
Epoch: 5| Step: 7
Training loss: 7.068510215733006
Validation loss: 6.536563087032893
Epoch: 5| Step: 8
Training loss: 6.184087255259588
Validation loss: 6.54236166423965
Epoch: 5| Step: 9
Training loss: 7.476944390472146
Validation loss: 6.513076035630419
Epoch: 8| Step: 0
Training loss: 6.997744060483718
Validation loss: 6.530825531805785
Epoch: 5| Step: 1
Training loss: 6.594504941416246
Validation loss: 6.532819058567912
Epoch: 5| Step: 2
Training loss: 7.185876812298096
Validation loss: 6.525273619662662
Epoch: 5| Step: 3
Training loss: 6.692228552892556
Validation loss: 6.507992535612532
Epoch: 5| Step: 4
Training loss: 6.332656556493491
Validation loss: 6.515501083623955
Epoch: 5| Step: 5
Training loss: 7.075285824235577
Validation loss: 6.507279427379849
Epoch: 5| Step: 6
Training loss: 6.5005698321062475
Validation loss: 6.510820606989373
Epoch: 5| Step: 7
Training loss: 7.080357483712808
Validation loss: 6.502169596155234
Epoch: 5| Step: 8
Training loss: 6.659348890306188
Validation loss: 6.4919265348263675
Epoch: 5| Step: 9
Training loss: 7.315307860607029
Validation loss: 6.484325220671802
Epoch: 9| Step: 0
Training loss: 6.416290007926723
Validation loss: 6.4815834761496145
Epoch: 5| Step: 1
Training loss: 7.051145896041694
Validation loss: 6.484514991659041
Epoch: 5| Step: 2
Training loss: 6.948615165345139
Validation loss: 6.467166186379632
Epoch: 5| Step: 3
Training loss: 6.486098168537406
Validation loss: 6.467564568416549
Epoch: 5| Step: 4
Training loss: 6.837690902360292
Validation loss: 6.461477695417586
Epoch: 5| Step: 5
Training loss: 6.960446461356215
Validation loss: 6.461988353996043
Epoch: 5| Step: 6
Training loss: 6.80137944534924
Validation loss: 6.445753956739376
Epoch: 5| Step: 7
Training loss: 7.160057687367175
Validation loss: 6.446187419063107
Epoch: 5| Step: 8
Training loss: 6.337694707584372
Validation loss: 6.447876114914794
Epoch: 5| Step: 9
Training loss: 7.001109444028541
Validation loss: 6.4426048466632
Epoch: 10| Step: 0
Training loss: 6.994071357168864
Validation loss: 6.425211118036059
Epoch: 5| Step: 1
Training loss: 5.803264541057567
Validation loss: 6.4257575471320605
Epoch: 5| Step: 2
Training loss: 7.327588808008936
Validation loss: 6.42392070550675
Epoch: 5| Step: 3
Training loss: 6.640926291418738
Validation loss: 6.42549046037013
Epoch: 5| Step: 4
Training loss: 6.627494198549452
Validation loss: 6.417647269405714
Epoch: 5| Step: 5
Training loss: 6.326084439037625
Validation loss: 6.409954158358037
Epoch: 5| Step: 6
Training loss: 7.042151379878802
Validation loss: 6.405725948798222
Epoch: 5| Step: 7
Training loss: 7.399022692504681
Validation loss: 6.3861152862601465
Epoch: 5| Step: 8
Training loss: 6.4516706328731726
Validation loss: 6.394206907749136
Epoch: 5| Step: 9
Training loss: 6.774723692042032
Validation loss: 6.382929181679831
Epoch: 11| Step: 0
Training loss: 5.92406799454597
Validation loss: 6.381671406432659
Epoch: 5| Step: 1
Training loss: 6.946789270198916
Validation loss: 6.373208989211585
Epoch: 5| Step: 2
Training loss: 7.043867530959194
Validation loss: 6.365507889656901
Epoch: 5| Step: 3
Training loss: 6.543714976103144
Validation loss: 6.369826657930345
Epoch: 5| Step: 4
Training loss: 6.913781663611445
Validation loss: 6.359092087378974
Epoch: 5| Step: 5
Training loss: 7.179107808650226
Validation loss: 6.345144039784629
Epoch: 5| Step: 6
Training loss: 6.901822841743938
Validation loss: 6.331194063873749
Epoch: 5| Step: 7
Training loss: 6.829120484855988
Validation loss: 6.345688705306926
Epoch: 5| Step: 8
Training loss: 6.462568637623596
Validation loss: 6.32941971914177
Epoch: 5| Step: 9
Training loss: 6.169958971279743
Validation loss: 6.332209976909454
Epoch: 12| Step: 0
Training loss: 6.967301427710036
Validation loss: 6.325563047283243
Epoch: 5| Step: 1
Training loss: 6.6229887014549265
Validation loss: 6.319143211645386
Epoch: 5| Step: 2
Training loss: 6.420060618461991
Validation loss: 6.3102967758004604
Epoch: 5| Step: 3
Training loss: 6.098416307330907
Validation loss: 6.308512853227412
Epoch: 5| Step: 4
Training loss: 5.943878281549758
Validation loss: 6.293251172294624
Epoch: 5| Step: 5
Training loss: 7.489735000057996
Validation loss: 6.296854194452158
Epoch: 5| Step: 6
Training loss: 6.7917744758645995
Validation loss: 6.288351093783906
Epoch: 5| Step: 7
Training loss: 6.5000032278199615
Validation loss: 6.274973667480576
Epoch: 5| Step: 8
Training loss: 6.637070222723082
Validation loss: 6.256513884015179
Epoch: 5| Step: 9
Training loss: 6.848785359527549
Validation loss: 6.261525512936513
Epoch: 13| Step: 0
Training loss: 5.979327510925469
Validation loss: 6.243800578874243
Epoch: 5| Step: 1
Training loss: 7.002778727912897
Validation loss: 6.252226520992205
Epoch: 5| Step: 2
Training loss: 6.317254721344982
Validation loss: 6.245102424284425
Epoch: 5| Step: 3
Training loss: 6.857093379432557
Validation loss: 6.2430404776654695
Epoch: 5| Step: 4
Training loss: 6.586325854597497
Validation loss: 6.238177016051716
Epoch: 5| Step: 5
Training loss: 7.105184472821397
Validation loss: 6.2309247814533695
Epoch: 5| Step: 6
Training loss: 7.001871676217355
Validation loss: 6.210195265899181
Epoch: 5| Step: 7
Training loss: 6.507830378220491
Validation loss: 6.211536988361431
Epoch: 5| Step: 8
Training loss: 5.9453439398396695
Validation loss: 6.200283848864409
Epoch: 5| Step: 9
Training loss: 6.39453125
Validation loss: 6.192120160261299
Epoch: 14| Step: 0
Training loss: 6.992845421211665
Validation loss: 6.191748091095934
Epoch: 5| Step: 1
Training loss: 6.115162325409955
Validation loss: 6.175547184087401
Epoch: 5| Step: 2
Training loss: 6.9056926519660555
Validation loss: 6.175336668324313
Epoch: 5| Step: 3
Training loss: 6.331283271001702
Validation loss: 6.164588395404574
Epoch: 5| Step: 4
Training loss: 6.494172932250465
Validation loss: 6.144073775442882
Epoch: 5| Step: 5
Training loss: 6.215108293984669
Validation loss: 6.156575755882244
Epoch: 5| Step: 6
Training loss: 6.804359261690862
Validation loss: 6.1398541486486575
Epoch: 5| Step: 7
Training loss: 6.384850763714242
Validation loss: 6.138873525811068
Epoch: 5| Step: 8
Training loss: 6.464968843200614
Validation loss: 6.122324809050347
Epoch: 5| Step: 9
Training loss: 6.399368684148778
Validation loss: 6.1129243321294195
Epoch: 15| Step: 0
Training loss: 6.869948317154292
Validation loss: 6.1223822898386375
Epoch: 5| Step: 1
Training loss: 6.316909005675666
Validation loss: 6.100966979553495
Epoch: 5| Step: 2
Training loss: 6.983679137245081
Validation loss: 6.093127131428297
Epoch: 5| Step: 3
Training loss: 6.3975634228214915
Validation loss: 6.093716873021109
Epoch: 5| Step: 4
Training loss: 5.757233092349021
Validation loss: 6.07758864782168
Epoch: 5| Step: 5
Training loss: 6.559889501245786
Validation loss: 6.0749060922551665
Epoch: 5| Step: 6
Training loss: 6.187569665998277
Validation loss: 6.060862751389207
Epoch: 5| Step: 7
Training loss: 7.11801072352074
Validation loss: 6.049258230427971
Epoch: 5| Step: 8
Training loss: 5.678470360472583
Validation loss: 6.020027623324995
Epoch: 5| Step: 9
Training loss: 6.389755227142349
Validation loss: 6.036045386947369
Epoch: 16| Step: 0
Training loss: 6.58129951190347
Validation loss: 6.013967612097376
Epoch: 5| Step: 1
Training loss: 6.6964798291379015
Validation loss: 6.02081429641507
Epoch: 5| Step: 2
Training loss: 6.338834013475241
Validation loss: 6.0165010661610845
Epoch: 5| Step: 3
Training loss: 6.2230185423018005
Validation loss: 6.005506900441617
Epoch: 5| Step: 4
Training loss: 5.843716708001758
Validation loss: 6.0005400254304275
Epoch: 5| Step: 5
Training loss: 6.765186322022453
Validation loss: 5.991040038237667
Epoch: 5| Step: 6
Training loss: 6.093179372822104
Validation loss: 5.972278890336633
Epoch: 5| Step: 7
Training loss: 6.583817798156052
Validation loss: 5.963125467948753
Epoch: 5| Step: 8
Training loss: 6.461938192418488
Validation loss: 5.948004460276618
Epoch: 5| Step: 9
Training loss: 5.94247485099738
Validation loss: 5.955103524833703
Epoch: 17| Step: 0
Training loss: 6.36347607992866
Validation loss: 5.946076749509597
Epoch: 5| Step: 1
Training loss: 6.542906949335001
Validation loss: 5.931866092750449
Epoch: 5| Step: 2
Training loss: 6.510382063710126
Validation loss: 5.919131931802551
Epoch: 5| Step: 3
Training loss: 6.2494122038049635
Validation loss: 5.9180841218474045
Epoch: 5| Step: 4
Training loss: 6.381038124983288
Validation loss: 5.896250266965728
Epoch: 5| Step: 5
Training loss: 5.580063597887548
Validation loss: 5.887711474757855
Epoch: 5| Step: 6
Training loss: 6.093796323942516
Validation loss: 5.882081290852702
Epoch: 5| Step: 7
Training loss: 6.697050031616649
Validation loss: 5.8781785109807485
Epoch: 5| Step: 8
Training loss: 6.463487631556299
Validation loss: 5.870904953926109
Epoch: 5| Step: 9
Training loss: 5.793770753232788
Validation loss: 5.845138713948216
Epoch: 18| Step: 0
Training loss: 5.939662499392491
Validation loss: 5.839654300595466
Epoch: 5| Step: 1
Training loss: 6.444148805933008
Validation loss: 5.836192127496761
Epoch: 5| Step: 2
Training loss: 6.619813436316814
Validation loss: 5.823726809273956
Epoch: 5| Step: 3
Training loss: 5.683534014935231
Validation loss: 5.817334377392968
Epoch: 5| Step: 4
Training loss: 6.628115515227773
Validation loss: 5.791329612338055
Epoch: 5| Step: 5
Training loss: 6.259889331850962
Validation loss: 5.790799744195252
Epoch: 5| Step: 6
Training loss: 5.62420326524339
Validation loss: 5.7813048892776076
Epoch: 5| Step: 7
Training loss: 6.40587216635408
Validation loss: 5.764340089239453
Epoch: 5| Step: 8
Training loss: 6.248557572810117
Validation loss: 5.7657521958224125
Epoch: 5| Step: 9
Training loss: 5.842865086429165
Validation loss: 5.740591549445376
Epoch: 19| Step: 0
Training loss: 5.929378054137442
Validation loss: 5.737858051697359
Epoch: 5| Step: 1
Training loss: 6.83801307775603
Validation loss: 5.732297676696348
Epoch: 5| Step: 2
Training loss: 5.854805835661078
Validation loss: 5.726762843589129
Epoch: 5| Step: 3
Training loss: 6.485450306172126
Validation loss: 5.709199963017358
Epoch: 5| Step: 4
Training loss: 5.66442684711769
Validation loss: 5.696506184783348
Epoch: 5| Step: 5
Training loss: 6.360827081031614
Validation loss: 5.68233218713999
Epoch: 5| Step: 6
Training loss: 6.0598610709099985
Validation loss: 5.675820992270202
Epoch: 5| Step: 7
Training loss: 5.967168308815532
Validation loss: 5.658414917239164
Epoch: 5| Step: 8
Training loss: 5.699552404665246
Validation loss: 5.623723331688962
Epoch: 5| Step: 9
Training loss: 5.840203009434368
Validation loss: 5.6416604814183104
Epoch: 20| Step: 0
Training loss: 5.769954720093029
Validation loss: 5.621486688435513
Epoch: 5| Step: 1
Training loss: 5.594807444950124
Validation loss: 5.617753658511186
Epoch: 5| Step: 2
Training loss: 6.31110389303114
Validation loss: 5.599543700932043
Epoch: 5| Step: 3
Training loss: 5.818240704563557
Validation loss: 5.596982979520698
Epoch: 5| Step: 4
Training loss: 6.03324991382796
Validation loss: 5.5855513504246215
Epoch: 5| Step: 5
Training loss: 5.219596200283941
Validation loss: 5.575156535193582
Epoch: 5| Step: 6
Training loss: 6.430623353729349
Validation loss: 5.55880703624953
Epoch: 5| Step: 7
Training loss: 5.926843979153041
Validation loss: 5.533670627619421
Epoch: 5| Step: 8
Training loss: 6.214137528129375
Validation loss: 5.514547825848473
Epoch: 5| Step: 9
Training loss: 6.277891865071527
Validation loss: 5.514854721901134
Epoch: 21| Step: 0
Training loss: 5.947651106144334
Validation loss: 5.508843724583196
Epoch: 5| Step: 1
Training loss: 6.002587714072717
Validation loss: 5.494052788035125
Epoch: 5| Step: 2
Training loss: 6.1719606031444805
Validation loss: 5.472490865954188
Epoch: 5| Step: 3
Training loss: 5.361012022500557
Validation loss: 5.480360396624607
Epoch: 5| Step: 4
Training loss: 6.358990481887079
Validation loss: 5.440148083798371
Epoch: 5| Step: 5
Training loss: 6.017233259551562
Validation loss: 5.439992312772407
Epoch: 5| Step: 6
Training loss: 5.435791975081672
Validation loss: 5.430656655239544
Epoch: 5| Step: 7
Training loss: 6.174382031364619
Validation loss: 5.4223441992364805
Epoch: 5| Step: 8
Training loss: 5.342985935130555
Validation loss: 5.405480599450291
Epoch: 5| Step: 9
Training loss: 5.594578345997743
Validation loss: 5.386856373709846
Epoch: 22| Step: 0
Training loss: 5.807606267461778
Validation loss: 5.363296767071229
Epoch: 5| Step: 1
Training loss: 6.339858491684185
Validation loss: 5.357563506636235
Epoch: 5| Step: 2
Training loss: 5.432220000114012
Validation loss: 5.338854525026669
Epoch: 5| Step: 3
Training loss: 6.192225029653807
Validation loss: 5.3398750658763285
Epoch: 5| Step: 4
Training loss: 5.862549921079431
Validation loss: 5.31494428297199
Epoch: 5| Step: 5
Training loss: 6.021094117462199
Validation loss: 5.302174283938093
Epoch: 5| Step: 6
Training loss: 5.6477825370723655
Validation loss: 5.2836926070501375
Epoch: 5| Step: 7
Training loss: 4.656465461405434
Validation loss: 5.272490272605692
Epoch: 5| Step: 8
Training loss: 5.702335668835896
Validation loss: 5.2634721373357065
Epoch: 5| Step: 9
Training loss: 5.486000102834113
Validation loss: 5.252370906261676
Epoch: 23| Step: 0
Training loss: 5.237813520493439
Validation loss: 5.238875907126089
Epoch: 5| Step: 1
Training loss: 5.419287160188595
Validation loss: 5.231315640016179
Epoch: 5| Step: 2
Training loss: 6.211020722221528
Validation loss: 5.222918746507533
Epoch: 5| Step: 3
Training loss: 5.469648538069414
Validation loss: 5.2002806177478105
Epoch: 5| Step: 4
Training loss: 5.294282081643343
Validation loss: 5.141892262501067
Epoch: 5| Step: 5
Training loss: 5.295713584300359
Validation loss: 5.150497830457183
Epoch: 5| Step: 6
Training loss: 5.6396945175336075
Validation loss: 5.1425770517689555
Epoch: 5| Step: 7
Training loss: 5.018588797710748
Validation loss: 5.120639855157507
Epoch: 5| Step: 8
Training loss: 6.137695856329531
Validation loss: 5.115731360502965
Epoch: 5| Step: 9
Training loss: 6.058501822722966
Validation loss: 5.106182370041658
Epoch: 24| Step: 0
Training loss: 5.732572266979508
Validation loss: 5.081787427888495
Epoch: 5| Step: 1
Training loss: 5.502823018459284
Validation loss: 5.066808545720703
Epoch: 5| Step: 2
Training loss: 5.652164901930346
Validation loss: 5.055113085308933
Epoch: 5| Step: 3
Training loss: 5.562722533843063
Validation loss: 5.023926520529865
Epoch: 5| Step: 4
Training loss: 5.1700727659081505
Validation loss: 5.021823879416897
Epoch: 5| Step: 5
Training loss: 5.367076267383585
Validation loss: 5.005314269713819
Epoch: 5| Step: 6
Training loss: 5.990195847304369
Validation loss: 4.9731318422198605
Epoch: 5| Step: 7
Training loss: 5.550218890573269
Validation loss: 4.975611954875822
Epoch: 5| Step: 8
Training loss: 4.812702521484445
Validation loss: 4.952518638701996
Epoch: 5| Step: 9
Training loss: 5.086384883956791
Validation loss: 4.939764890087349
Epoch: 25| Step: 0
Training loss: 5.397412259341511
Validation loss: 4.897691936102277
Epoch: 5| Step: 1
Training loss: 5.623193238749437
Validation loss: 4.896185424329926
Epoch: 5| Step: 2
Training loss: 5.143805265071809
Validation loss: 4.895535448061173
Epoch: 5| Step: 3
Training loss: 5.18625212218121
Validation loss: 4.86283438566699
Epoch: 5| Step: 4
Training loss: 5.7710530850909265
Validation loss: 4.848309384442338
Epoch: 5| Step: 5
Training loss: 5.298307242067758
Validation loss: 4.826186904322062
Epoch: 5| Step: 6
Training loss: 5.020134440593772
Validation loss: 4.773638573947237
Epoch: 5| Step: 7
Training loss: 5.111657802166553
Validation loss: 4.779734443804599
Epoch: 5| Step: 8
Training loss: 5.188854546243824
Validation loss: 4.775825403085786
Epoch: 5| Step: 9
Training loss: 5.104947566287142
Validation loss: 4.761074932553678
Epoch: 26| Step: 0
Training loss: 5.065377253710933
Validation loss: 4.722607067463567
Epoch: 5| Step: 1
Training loss: 5.233770990395666
Validation loss: 4.718822095073022
Epoch: 5| Step: 2
Training loss: 5.133548027767302
Validation loss: 4.708395946341182
Epoch: 5| Step: 3
Training loss: 5.218901671986668
Validation loss: 4.683280303636329
Epoch: 5| Step: 4
Training loss: 5.389076076721627
Validation loss: 4.667074920117715
Epoch: 5| Step: 5
Training loss: 5.240026355415051
Validation loss: 4.625999555731534
Epoch: 5| Step: 6
Training loss: 5.351187543889937
Validation loss: 4.625260599742511
Epoch: 5| Step: 7
Training loss: 5.053853881583487
Validation loss: 4.59539826745596
Epoch: 5| Step: 8
Training loss: 4.439179653298872
Validation loss: 4.566576937592867
Epoch: 5| Step: 9
Training loss: 4.895744398675556
Validation loss: 4.536199797212718
Epoch: 27| Step: 0
Training loss: 4.698363266698761
Validation loss: 4.519095363153285
Epoch: 5| Step: 1
Training loss: 4.473185652006782
Validation loss: 4.516009595154158
Epoch: 5| Step: 2
Training loss: 4.817722805791888
Validation loss: 4.4923160529043855
Epoch: 5| Step: 3
Training loss: 5.493749708364433
Validation loss: 4.459548402157869
Epoch: 5| Step: 4
Training loss: 4.56830259290278
Validation loss: 4.4437356907345285
Epoch: 5| Step: 5
Training loss: 5.206851209202209
Validation loss: 4.398446045348155
Epoch: 5| Step: 6
Training loss: 5.033355082955432
Validation loss: 4.38479856466276
Epoch: 5| Step: 7
Training loss: 4.884173638408749
Validation loss: 4.358028422356732
Epoch: 5| Step: 8
Training loss: 4.488951364928056
Validation loss: 4.344280593237269
Epoch: 5| Step: 9
Training loss: 5.29450580196223
Validation loss: 4.343374733702872
Epoch: 28| Step: 0
Training loss: 4.578691571170332
Validation loss: 4.318731788536905
Epoch: 5| Step: 1
Training loss: 5.19409220425417
Validation loss: 4.30217908509588
Epoch: 5| Step: 2
Training loss: 5.0632790801954926
Validation loss: 4.234916801359015
Epoch: 5| Step: 3
Training loss: 4.460925955682304
Validation loss: 4.256703081850688
Epoch: 5| Step: 4
Training loss: 4.722722888410127
Validation loss: 4.189664865345031
Epoch: 5| Step: 5
Training loss: 4.909732044623209
Validation loss: 4.176655266599674
Epoch: 5| Step: 6
Training loss: 4.565217554396474
Validation loss: 4.148542916809284
Epoch: 5| Step: 7
Training loss: 3.783622312782571
Validation loss: 4.106694600352622
Epoch: 5| Step: 8
Training loss: 4.75192422540534
Validation loss: 4.098224385750593
Epoch: 5| Step: 9
Training loss: 4.897263186903657
Validation loss: 4.059012939671836
Epoch: 29| Step: 0
Training loss: 4.90300745631691
Validation loss: 4.035347347959435
Epoch: 5| Step: 1
Training loss: 4.162135037755235
Validation loss: 4.05919876667443
Epoch: 5| Step: 2
Training loss: 4.65334275391362
Validation loss: 4.0192430068445155
Epoch: 5| Step: 3
Training loss: 5.174651630857136
Validation loss: 3.987204015850979
Epoch: 5| Step: 4
Training loss: 4.036726669088178
Validation loss: 3.968046457973588
Epoch: 5| Step: 5
Training loss: 4.336958811305109
Validation loss: 3.9339854094194964
Epoch: 5| Step: 6
Training loss: 4.479724133717275
Validation loss: 3.880578318535952
Epoch: 5| Step: 7
Training loss: 4.40777604751632
Validation loss: 3.883856820024103
Epoch: 5| Step: 8
Training loss: 4.017601862472413
Validation loss: 3.8627467900185763
Epoch: 5| Step: 9
Training loss: 4.3200586392696705
Validation loss: 3.823064943161979
Epoch: 30| Step: 0
Training loss: 4.1825026114137005
Validation loss: 3.7971088527025176
Epoch: 5| Step: 1
Training loss: 4.441401526080381
Validation loss: 3.7594818227464986
Epoch: 5| Step: 2
Training loss: 4.2469457983946475
Validation loss: 3.750240999184018
Epoch: 5| Step: 3
Training loss: 3.9930581414403234
Validation loss: 3.74805194085034
Epoch: 5| Step: 4
Training loss: 4.114004102409072
Validation loss: 3.7001786395278504
Epoch: 5| Step: 5
Training loss: 4.463999964696959
Validation loss: 3.6571605475038726
Epoch: 5| Step: 6
Training loss: 4.2279328048115845
Validation loss: 3.6475601758578713
Epoch: 5| Step: 7
Training loss: 3.744396855636432
Validation loss: 3.6230603276811046
Epoch: 5| Step: 8
Training loss: 4.400076796121583
Validation loss: 3.5772487616349564
Epoch: 5| Step: 9
Training loss: 4.278953660357896
Validation loss: 3.5408702511051584
Epoch: 31| Step: 0
Training loss: 3.8958780758714284
Validation loss: 3.5290858213100145
Epoch: 5| Step: 1
Training loss: 4.277589890178193
Validation loss: 3.5000729703631532
Epoch: 5| Step: 2
Training loss: 4.076405831510625
Validation loss: 3.469046953640593
Epoch: 5| Step: 3
Training loss: 4.114982231537476
Validation loss: 3.453703345501761
Epoch: 5| Step: 4
Training loss: 4.000273695165178
Validation loss: 3.440166657931816
Epoch: 5| Step: 5
Training loss: 4.158890881870979
Validation loss: 3.380175026727812
Epoch: 5| Step: 6
Training loss: 4.023533969944805
Validation loss: 3.3338114436856428
Epoch: 5| Step: 7
Training loss: 3.5874905682483
Validation loss: 3.3196722602943614
Epoch: 5| Step: 8
Training loss: 3.3472398746037317
Validation loss: 3.296979128365562
Epoch: 5| Step: 9
Training loss: 3.9203167133453096
Validation loss: 3.2462471682899445
Epoch: 32| Step: 0
Training loss: 3.261558958239714
Validation loss: 3.2162279381948
Epoch: 5| Step: 1
Training loss: 3.6871019568834953
Validation loss: 3.1902705022047075
Epoch: 5| Step: 2
Training loss: 3.9769261761576504
Validation loss: 3.2025327836162565
Epoch: 5| Step: 3
Training loss: 3.914002317167677
Validation loss: 3.118728117732652
Epoch: 5| Step: 4
Training loss: 4.3018390626194645
Validation loss: 3.1261416420415227
Epoch: 5| Step: 5
Training loss: 3.8324332770046854
Validation loss: 3.076326336870482
Epoch: 5| Step: 6
Training loss: 3.4541698963501437
Validation loss: 3.09557348460504
Epoch: 5| Step: 7
Training loss: 3.5215723891004718
Validation loss: 3.0485880651412294
Epoch: 5| Step: 8
Training loss: 3.6615228697007747
Validation loss: 3.0228252296558824
Epoch: 5| Step: 9
Training loss: 3.007708817713796
Validation loss: 2.9960175322618676
Epoch: 33| Step: 0
Training loss: 3.5697266747055707
Validation loss: 2.897462339315426
Epoch: 5| Step: 1
Training loss: 3.9309589611813793
Validation loss: 2.94056992889619
Epoch: 5| Step: 2
Training loss: 3.42917624032286
Validation loss: 2.8622683661285535
Epoch: 5| Step: 3
Training loss: 3.76842068930436
Validation loss: 2.84451997928057
Epoch: 5| Step: 4
Training loss: 3.0386975223681425
Validation loss: 2.822753572537484
Epoch: 5| Step: 5
Training loss: 2.813166899556573
Validation loss: 2.837984215987094
Epoch: 5| Step: 6
Training loss: 3.691240514839302
Validation loss: 2.7888609368051207
Epoch: 5| Step: 7
Training loss: 3.309858024295183
Validation loss: 2.7426953078501435
Epoch: 5| Step: 8
Training loss: 3.426897378222631
Validation loss: 2.7652698485166067
Epoch: 5| Step: 9
Training loss: 3.166157346891635
Validation loss: 2.6919503826232547
Epoch: 34| Step: 0
Training loss: 3.047235086144003
Validation loss: 2.6860096412889902
Epoch: 5| Step: 1
Training loss: 3.504289859113685
Validation loss: 2.6529473666583834
Epoch: 5| Step: 2
Training loss: 3.130482251733424
Validation loss: 2.6415020604895094
Epoch: 5| Step: 3
Training loss: 3.6301285868587225
Validation loss: 2.653301780086711
Epoch: 5| Step: 4
Training loss: 2.824134160591356
Validation loss: 2.597020294353691
Epoch: 5| Step: 5
Training loss: 3.062683567073008
Validation loss: 2.5411826481465254
Epoch: 5| Step: 6
Training loss: 3.0797582640470047
Validation loss: 2.599388046426383
Epoch: 5| Step: 7
Training loss: 2.667607518954598
Validation loss: 2.527453003655089
Epoch: 5| Step: 8
Training loss: 3.3532912413102225
Validation loss: 2.484440310481121
Epoch: 5| Step: 9
Training loss: 3.5451910247771763
Validation loss: 2.4980508974294593
Epoch: 35| Step: 0
Training loss: 3.755684422521065
Validation loss: 2.4721925735110672
Epoch: 5| Step: 1
Training loss: 2.9926096167760354
Validation loss: 2.4574170642983786
Epoch: 5| Step: 2
Training loss: 2.83748901633939
Validation loss: 2.426200857266357
Epoch: 5| Step: 3
Training loss: 2.9834276862825657
Validation loss: 2.3636411120096055
Epoch: 5| Step: 4
Training loss: 2.941474336238879
Validation loss: 2.3981530230928625
Epoch: 5| Step: 5
Training loss: 2.9449993746227383
Validation loss: 2.392530314990145
Epoch: 5| Step: 6
Training loss: 2.658356493268029
Validation loss: 2.352202942557968
Epoch: 5| Step: 7
Training loss: 2.920920639438356
Validation loss: 2.3217971238888615
Epoch: 5| Step: 8
Training loss: 2.9815510275947594
Validation loss: 2.3014292372158094
Epoch: 5| Step: 9
Training loss: 2.8809808634341243
Validation loss: 2.2553754423965753
Epoch: 36| Step: 0
Training loss: 2.999310414214479
Validation loss: 2.2831131320130913
Epoch: 5| Step: 1
Training loss: 3.587347281259151
Validation loss: 2.2557615969265
Epoch: 5| Step: 2
Training loss: 2.9883864041445634
Validation loss: 2.2881759642720385
Epoch: 5| Step: 3
Training loss: 2.375117349235178
Validation loss: 2.2624388962468154
Epoch: 5| Step: 4
Training loss: 2.825107207838104
Validation loss: 2.2634710240473392
Epoch: 5| Step: 5
Training loss: 2.7727846048741065
Validation loss: 2.2515090559062227
Epoch: 5| Step: 6
Training loss: 2.857175053687567
Validation loss: 2.25027399205337
Epoch: 5| Step: 7
Training loss: 2.909214107928298
Validation loss: 2.23864223735382
Epoch: 5| Step: 8
Training loss: 2.536432118577369
Validation loss: 2.2485262235585464
Epoch: 5| Step: 9
Training loss: 2.412666089759428
Validation loss: 2.2083037229744997
Epoch: 37| Step: 0
Training loss: 2.6587288453659865
Validation loss: 2.1888676667119267
Epoch: 5| Step: 1
Training loss: 2.7613233312882777
Validation loss: 2.189708598078453
Epoch: 5| Step: 2
Training loss: 2.2832567977144143
Validation loss: 2.2378263041194315
Epoch: 5| Step: 3
Training loss: 2.7262326453634707
Validation loss: 2.190269660947235
Epoch: 5| Step: 4
Training loss: 2.931008816620221
Validation loss: 2.24664960543328
Epoch: 5| Step: 5
Training loss: 2.678920713993606
Validation loss: 2.190231770587428
Epoch: 5| Step: 6
Training loss: 3.43833358367707
Validation loss: 2.200527746611292
Epoch: 5| Step: 7
Training loss: 2.6381840907973113
Validation loss: 2.193993381278322
Epoch: 5| Step: 8
Training loss: 2.7124416784529743
Validation loss: 2.15891686551401
Epoch: 5| Step: 9
Training loss: 2.51870141372959
Validation loss: 2.1903473017713417
Epoch: 38| Step: 0
Training loss: 2.9546257928449466
Validation loss: 2.151927561628157
Epoch: 5| Step: 1
Training loss: 2.250237346422456
Validation loss: 2.217057234402008
Epoch: 5| Step: 2
Training loss: 2.421376035420196
Validation loss: 2.1488218992378907
Epoch: 5| Step: 3
Training loss: 2.581482706364084
Validation loss: 2.217047134601564
Epoch: 5| Step: 4
Training loss: 3.26728800359526
Validation loss: 2.180476980605567
Epoch: 5| Step: 5
Training loss: 2.908478733425026
Validation loss: 2.1934565631633594
Epoch: 5| Step: 6
Training loss: 2.887651223190223
Validation loss: 2.1799537558181554
Epoch: 5| Step: 7
Training loss: 2.4205698742094133
Validation loss: 2.118790097441495
Epoch: 5| Step: 8
Training loss: 2.4385315106319063
Validation loss: 2.171018868212243
Epoch: 5| Step: 9
Training loss: 2.5500130073365215
Validation loss: 2.2130082189184095
Epoch: 39| Step: 0
Training loss: 2.612968300639892
Validation loss: 2.186938344271542
Epoch: 5| Step: 1
Training loss: 2.685606533428264
Validation loss: 2.152513015820629
Epoch: 5| Step: 2
Training loss: 2.9646274251511766
Validation loss: 2.1815878782990414
Epoch: 5| Step: 3
Training loss: 2.600967018320187
Validation loss: 2.2241212377542983
Epoch: 5| Step: 4
Training loss: 2.7793248403701103
Validation loss: 2.1678319969535256
Epoch: 5| Step: 5
Training loss: 2.9603691260117633
Validation loss: 2.1957065501177455
Epoch: 5| Step: 6
Training loss: 1.8607275395173684
Validation loss: 2.1887006598413485
Epoch: 5| Step: 7
Training loss: 2.761188116178839
Validation loss: 2.157407873606278
Epoch: 5| Step: 8
Training loss: 2.667560328308167
Validation loss: 2.180657405185993
Epoch: 5| Step: 9
Training loss: 2.5550018023878334
Validation loss: 2.166847887461858
Epoch: 40| Step: 0
Training loss: 3.013937680679144
Validation loss: 2.2336222769413956
Epoch: 5| Step: 1
Training loss: 2.6985233366124866
Validation loss: 2.2229227178630113
Epoch: 5| Step: 2
Training loss: 2.477798009323218
Validation loss: 2.245417574722749
Epoch: 5| Step: 3
Training loss: 2.1802097364902004
Validation loss: 2.1881152270890363
Epoch: 5| Step: 4
Training loss: 2.6456363234175124
Validation loss: 2.255434650944489
Epoch: 5| Step: 5
Training loss: 2.3342847814322947
Validation loss: 2.1858696266753097
Epoch: 5| Step: 6
Training loss: 2.5579702767918713
Validation loss: 2.205189177138034
Epoch: 5| Step: 7
Training loss: 2.9305625320841675
Validation loss: 2.216413770750883
Epoch: 5| Step: 8
Training loss: 2.940084416222044
Validation loss: 2.167339910931674
Epoch: 5| Step: 9
Training loss: 2.820958958978699
Validation loss: 2.252223007123345
Epoch: 41| Step: 0
Training loss: 2.9414642855122914
Validation loss: 2.1554429501104932
Epoch: 5| Step: 1
Training loss: 2.7942161495454965
Validation loss: 2.2173275348549244
Epoch: 5| Step: 2
Training loss: 2.7564100638556144
Validation loss: 2.2058714360882083
Epoch: 5| Step: 3
Training loss: 2.433723066638415
Validation loss: 2.2088536990602328
Epoch: 5| Step: 4
Training loss: 1.987109665357907
Validation loss: 2.2179403984964368
Epoch: 5| Step: 5
Training loss: 2.4727322294063154
Validation loss: 2.226060467148027
Epoch: 5| Step: 6
Training loss: 2.557979970213828
Validation loss: 2.193053633625236
Epoch: 5| Step: 7
Training loss: 2.655578797751653
Validation loss: 2.1889808895475733
Epoch: 5| Step: 8
Training loss: 2.933659904536625
Validation loss: 2.1777468395828534
Epoch: 5| Step: 9
Training loss: 2.8826394791616394
Validation loss: 2.2038297418948383
Epoch: 42| Step: 0
Training loss: 2.4961871635616157
Validation loss: 2.2069319514592163
Epoch: 5| Step: 1
Training loss: 2.891695061653259
Validation loss: 2.1977047400814977
Epoch: 5| Step: 2
Training loss: 2.1969481107394726
Validation loss: 2.2158900591098
Epoch: 5| Step: 3
Training loss: 2.8172372552304323
Validation loss: 2.222780025348325
Epoch: 5| Step: 4
Training loss: 2.56229790611716
Validation loss: 2.2082624899339987
Epoch: 5| Step: 5
Training loss: 2.2666537349930427
Validation loss: 2.2248817124157463
Epoch: 5| Step: 6
Training loss: 2.893817355399883
Validation loss: 2.219212652624487
Epoch: 5| Step: 7
Training loss: 2.7439609455187637
Validation loss: 2.2266774284989124
Epoch: 5| Step: 8
Training loss: 3.0290483167389657
Validation loss: 2.291340484783967
Epoch: 5| Step: 9
Training loss: 2.426370103900787
Validation loss: 2.2038546419389986
Epoch: 43| Step: 0
Training loss: 2.3313854352048593
Validation loss: 2.200344736644325
Epoch: 5| Step: 1
Training loss: 2.4025952215308655
Validation loss: 2.254708567722773
Epoch: 5| Step: 2
Training loss: 2.5365335399802187
Validation loss: 2.199334518779944
Epoch: 5| Step: 3
Training loss: 2.9779209831622477
Validation loss: 2.22582455361273
Epoch: 5| Step: 4
Training loss: 2.4570714222545624
Validation loss: 2.199628837719742
Epoch: 5| Step: 5
Training loss: 3.1662172868667446
Validation loss: 2.2063718073640097
Epoch: 5| Step: 6
Training loss: 2.781082834084865
Validation loss: 2.232547883229562
Epoch: 5| Step: 7
Training loss: 2.6768613097352687
Validation loss: 2.2221607525689397
Epoch: 5| Step: 8
Training loss: 2.5948454600275874
Validation loss: 2.1896896497159783
Epoch: 5| Step: 9
Training loss: 2.6349257865168916
Validation loss: 2.2311826675289077
Epoch: 44| Step: 0
Training loss: 2.1041084382212105
Validation loss: 2.2235483941221577
Epoch: 5| Step: 1
Training loss: 3.075147088727557
Validation loss: 2.212193613562705
Epoch: 5| Step: 2
Training loss: 2.8370443238704377
Validation loss: 2.2588556817000747
Epoch: 5| Step: 3
Training loss: 2.6900190036485307
Validation loss: 2.2245199596256446
Epoch: 5| Step: 4
Training loss: 2.1937532691170443
Validation loss: 2.2415476411670707
Epoch: 5| Step: 5
Training loss: 3.533372089485327
Validation loss: 2.254134524326881
Epoch: 5| Step: 6
Training loss: 3.0079431282510924
Validation loss: 2.1805592178823785
Epoch: 5| Step: 7
Training loss: 2.1566075567589085
Validation loss: 2.1579510640098523
Epoch: 5| Step: 8
Training loss: 2.2924418323255114
Validation loss: 2.162910946973188
Epoch: 5| Step: 9
Training loss: 2.4725025484787446
Validation loss: 2.223101711678689
Epoch: 45| Step: 0
Training loss: 2.5173760237562663
Validation loss: 2.19529445053253
Epoch: 5| Step: 1
Training loss: 2.289056875593026
Validation loss: 2.2099813221524918
Epoch: 5| Step: 2
Training loss: 2.576763278964347
Validation loss: 2.2099322514950153
Epoch: 5| Step: 3
Training loss: 3.0288277612670345
Validation loss: 2.2445713200882658
Epoch: 5| Step: 4
Training loss: 2.4679995011710925
Validation loss: 2.2215648303227375
Epoch: 5| Step: 5
Training loss: 3.042669122363389
Validation loss: 2.214314343409892
Epoch: 5| Step: 6
Training loss: 2.7526883942785996
Validation loss: 2.2184965432855703
Epoch: 5| Step: 7
Training loss: 2.2742404633995434
Validation loss: 2.225075698976669
Epoch: 5| Step: 8
Training loss: 3.29591767934198
Validation loss: 2.2031593026462244
Epoch: 5| Step: 9
Training loss: 2.1110846478772305
Validation loss: 2.216428709416233
Epoch: 46| Step: 0
Training loss: 2.7280423321166247
Validation loss: 2.1728849564760444
Epoch: 5| Step: 1
Training loss: 3.018980383127876
Validation loss: 2.185128356023025
Epoch: 5| Step: 2
Training loss: 2.538943807321911
Validation loss: 2.22747284150017
Epoch: 5| Step: 3
Training loss: 2.7119833388252808
Validation loss: 2.2176641896888203
Epoch: 5| Step: 4
Training loss: 2.3379482431091745
Validation loss: 2.2194649697101565
Epoch: 5| Step: 5
Training loss: 2.192569743387964
Validation loss: 2.1850743479127623
Epoch: 5| Step: 6
Training loss: 2.688831620442839
Validation loss: 2.1882824853610745
Epoch: 5| Step: 7
Training loss: 2.4380604026390538
Validation loss: 2.20037680680568
Epoch: 5| Step: 8
Training loss: 3.285304879218071
Validation loss: 2.2183487798414503
Epoch: 5| Step: 9
Training loss: 2.325665397180414
Validation loss: 2.238142393940321
Epoch: 47| Step: 0
Training loss: 2.787940646526297
Validation loss: 2.174973718755475
Epoch: 5| Step: 1
Training loss: 2.634192673786991
Validation loss: 2.227886113404191
Epoch: 5| Step: 2
Training loss: 2.2297738457516814
Validation loss: 2.2113721179993493
Epoch: 5| Step: 3
Training loss: 2.9396750331519823
Validation loss: 2.1939056884341763
Epoch: 5| Step: 4
Training loss: 2.7032580039758636
Validation loss: 2.1980337888784494
Epoch: 5| Step: 5
Training loss: 2.916090844852006
Validation loss: 2.2161409058926185
Epoch: 5| Step: 6
Training loss: 2.5909789576380855
Validation loss: 2.283322022801497
Epoch: 5| Step: 7
Training loss: 2.440158470198232
Validation loss: 2.1955445805225384
Epoch: 5| Step: 8
Training loss: 2.690944327479889
Validation loss: 2.2542798438222915
Epoch: 5| Step: 9
Training loss: 2.812028548675001
Validation loss: 2.2017621433101797
Epoch: 48| Step: 0
Training loss: 2.55500021604175
Validation loss: 2.2820961711263816
Epoch: 5| Step: 1
Training loss: 2.8852879097278117
Validation loss: 2.2096353354971883
Epoch: 5| Step: 2
Training loss: 2.590216194173018
Validation loss: 2.2112608636887434
Epoch: 5| Step: 3
Training loss: 2.3059136701108103
Validation loss: 2.1830138852549994
Epoch: 5| Step: 4
Training loss: 2.374994679495223
Validation loss: 2.169480729457202
Epoch: 5| Step: 5
Training loss: 2.8547492627674824
Validation loss: 2.223187934552401
Epoch: 5| Step: 6
Training loss: 2.8335438444435246
Validation loss: 2.231064819269259
Epoch: 5| Step: 7
Training loss: 2.7136290982170777
Validation loss: 2.2242110067124603
Epoch: 5| Step: 8
Training loss: 2.706502755499677
Validation loss: 2.1913470480256154
Epoch: 5| Step: 9
Training loss: 2.6864606044996844
Validation loss: 2.2313267825485243
Epoch: 49| Step: 0
Training loss: 2.805175483776638
Validation loss: 2.1894090978503726
Epoch: 5| Step: 1
Training loss: 2.463907248299468
Validation loss: 2.2380854599986906
Epoch: 5| Step: 2
Training loss: 3.2231652060222573
Validation loss: 2.2311368873177395
Epoch: 5| Step: 3
Training loss: 2.3760090742226923
Validation loss: 2.218503971467421
Epoch: 5| Step: 4
Training loss: 2.7033742447974314
Validation loss: 2.186769770077385
Epoch: 5| Step: 5
Training loss: 2.6213969070969307
Validation loss: 2.168391961454455
Epoch: 5| Step: 6
Training loss: 2.280600468385229
Validation loss: 2.193030433810064
Epoch: 5| Step: 7
Training loss: 2.82220834852131
Validation loss: 2.180788949565819
Epoch: 5| Step: 8
Training loss: 2.8649576023553047
Validation loss: 2.2121887783110616
Epoch: 5| Step: 9
Training loss: 2.5271098811387045
Validation loss: 2.196910079743696
Epoch: 50| Step: 0
Training loss: 2.5099292983777897
Validation loss: 2.1544657874532467
Epoch: 5| Step: 1
Training loss: 2.467165281402289
Validation loss: 2.2149732974190073
Epoch: 5| Step: 2
Training loss: 2.614057895909443
Validation loss: 2.2243094784472577
Epoch: 5| Step: 3
Training loss: 2.8571959150019977
Validation loss: 2.142809623847541
Epoch: 5| Step: 4
Training loss: 2.845887774444285
Validation loss: 2.194737869172242
Epoch: 5| Step: 5
Training loss: 2.6614029213548522
Validation loss: 2.2058445348371065
Epoch: 5| Step: 6
Training loss: 2.9146655892977713
Validation loss: 2.1780525206181003
Epoch: 5| Step: 7
Training loss: 2.200103310846938
Validation loss: 2.212021068504809
Epoch: 5| Step: 8
Training loss: 2.685618429444469
Validation loss: 2.2457973352061478
Epoch: 5| Step: 9
Training loss: 2.6756412664398836
Validation loss: 2.1961674681898633
Epoch: 51| Step: 0
Training loss: 3.253239411056219
Validation loss: 2.2140412950012647
Epoch: 5| Step: 1
Training loss: 2.5999718004311334
Validation loss: 2.1820573819286158
Epoch: 5| Step: 2
Training loss: 2.3366796022819867
Validation loss: 2.2030949176173453
Epoch: 5| Step: 3
Training loss: 2.4012508232278122
Validation loss: 2.205151129375062
Epoch: 5| Step: 4
Training loss: 3.0767645758372466
Validation loss: 2.23485421208839
Epoch: 5| Step: 5
Training loss: 2.56916565513859
Validation loss: 2.2665524116967584
Epoch: 5| Step: 6
Training loss: 2.582609957249484
Validation loss: 2.227024028024209
Epoch: 5| Step: 7
Training loss: 2.8528269015594394
Validation loss: 2.1925507114602003
Epoch: 5| Step: 8
Training loss: 2.3194811837744114
Validation loss: 2.1828873639070268
Epoch: 5| Step: 9
Training loss: 2.2980903990970143
Validation loss: 2.20609573741918
Epoch: 52| Step: 0
Training loss: 2.6841022392846026
Validation loss: 2.169354900452549
Epoch: 5| Step: 1
Training loss: 2.2046499182879424
Validation loss: 2.207169084861476
Epoch: 5| Step: 2
Training loss: 2.610270089684413
Validation loss: 2.158203726701631
Epoch: 5| Step: 3
Training loss: 2.385376198326785
Validation loss: 2.228999598812907
Epoch: 5| Step: 4
Training loss: 3.364153493030663
Validation loss: 2.220950667362123
Epoch: 5| Step: 5
Training loss: 2.567856470339494
Validation loss: 2.16645587603269
Epoch: 5| Step: 6
Training loss: 2.9376564795838562
Validation loss: 2.2108838193846805
Epoch: 5| Step: 7
Training loss: 2.8362038227220974
Validation loss: 2.1752304124763793
Epoch: 5| Step: 8
Training loss: 2.0718171032487316
Validation loss: 2.200181544880584
Epoch: 5| Step: 9
Training loss: 2.5692500089560197
Validation loss: 2.2399010071854817
Epoch: 53| Step: 0
Training loss: 2.8854126821185164
Validation loss: 2.199206440532118
Epoch: 5| Step: 1
Training loss: 2.8921649851610476
Validation loss: 2.221089388365001
Epoch: 5| Step: 2
Training loss: 2.3400030065989945
Validation loss: 2.205928427020058
Epoch: 5| Step: 3
Training loss: 3.0496800739531347
Validation loss: 2.2136443298385755
Epoch: 5| Step: 4
Training loss: 2.9754998820937373
Validation loss: 2.1600977915574386
Epoch: 5| Step: 5
Training loss: 2.6331823072302365
Validation loss: 2.175786736134648
Epoch: 5| Step: 6
Training loss: 2.2987544875233397
Validation loss: 2.2340861422251335
Epoch: 5| Step: 7
Training loss: 2.4161363107425506
Validation loss: 2.210328183259376
Epoch: 5| Step: 8
Training loss: 2.4434650498917976
Validation loss: 2.1916111638848816
Epoch: 5| Step: 9
Training loss: 2.8968621674451986
Validation loss: 2.2121438943297376
Epoch: 54| Step: 0
Training loss: 2.8475422149556606
Validation loss: 2.236955046697406
Epoch: 5| Step: 1
Training loss: 2.886789282322953
Validation loss: 2.2232130455810872
Epoch: 5| Step: 2
Training loss: 2.4076121054402537
Validation loss: 2.234229150138657
Epoch: 5| Step: 3
Training loss: 2.7538657327471547
Validation loss: 2.258824756392143
Epoch: 5| Step: 4
Training loss: 2.8696797113106216
Validation loss: 2.2139826446520607
Epoch: 5| Step: 5
Training loss: 2.5369509323721973
Validation loss: 2.189199726657406
Epoch: 5| Step: 6
Training loss: 2.365728451118419
Validation loss: 2.1996324273031633
Epoch: 5| Step: 7
Training loss: 2.6176731854614297
Validation loss: 2.2227322035282158
Epoch: 5| Step: 8
Training loss: 2.771736074447573
Validation loss: 2.210521076641309
Epoch: 5| Step: 9
Training loss: 2.4086701743850627
Validation loss: 2.204027019767111
Epoch: 55| Step: 0
Training loss: 2.6129667494855795
Validation loss: 2.2751891598169722
Epoch: 5| Step: 1
Training loss: 2.876673004332008
Validation loss: 2.1955220946363054
Epoch: 5| Step: 2
Training loss: 3.0435057179770317
Validation loss: 2.180878128828399
Epoch: 5| Step: 3
Training loss: 2.776661377253385
Validation loss: 2.2257316742985847
Epoch: 5| Step: 4
Training loss: 2.1767229819302862
Validation loss: 2.1969948154377077
Epoch: 5| Step: 5
Training loss: 2.3714784313285135
Validation loss: 2.1535178078408905
Epoch: 5| Step: 6
Training loss: 2.310545533567605
Validation loss: 2.1767193481947644
Epoch: 5| Step: 7
Training loss: 2.5928769768050515
Validation loss: 2.2321197926435135
Epoch: 5| Step: 8
Training loss: 2.8838062589438174
Validation loss: 2.2017609546830927
Epoch: 5| Step: 9
Training loss: 2.665691515326335
Validation loss: 2.2210789688424253
Epoch: 56| Step: 0
Training loss: 2.898921627936746
Validation loss: 2.229154738410336
Epoch: 5| Step: 1
Training loss: 2.6401072338751437
Validation loss: 2.235404639466049
Epoch: 5| Step: 2
Training loss: 2.542635706877929
Validation loss: 2.213641098346227
Epoch: 5| Step: 3
Training loss: 2.5207420101385316
Validation loss: 2.1825063374430647
Epoch: 5| Step: 4
Training loss: 2.3047598100083735
Validation loss: 2.174947054977985
Epoch: 5| Step: 5
Training loss: 2.8041660174689786
Validation loss: 2.1888085034015163
Epoch: 5| Step: 6
Training loss: 2.6470832175333805
Validation loss: 2.21105086801935
Epoch: 5| Step: 7
Training loss: 2.7343026287855725
Validation loss: 2.1819093442374062
Epoch: 5| Step: 8
Training loss: 2.3282410253505206
Validation loss: 2.1853467939395945
Epoch: 5| Step: 9
Training loss: 2.985843636610939
Validation loss: 2.2056268558924645
Epoch: 57| Step: 0
Training loss: 2.5840179602645423
Validation loss: 2.2145773742067303
Epoch: 5| Step: 1
Training loss: 2.812405054821018
Validation loss: 2.2071698089893004
Epoch: 5| Step: 2
Training loss: 2.739152494911676
Validation loss: 2.1935022946262777
Epoch: 5| Step: 3
Training loss: 2.6159734386763853
Validation loss: 2.2459519552902636
Epoch: 5| Step: 4
Training loss: 3.035702296642062
Validation loss: 2.2094784765542332
Epoch: 5| Step: 5
Training loss: 2.850312761244643
Validation loss: 2.196497189489888
Epoch: 5| Step: 6
Training loss: 2.366487204324676
Validation loss: 2.194755576636652
Epoch: 5| Step: 7
Training loss: 2.0555728991810667
Validation loss: 2.143729093057155
Epoch: 5| Step: 8
Training loss: 2.817737196082872
Validation loss: 2.219421901060212
Epoch: 5| Step: 9
Training loss: 2.5273518628516425
Validation loss: 2.184860373704789
Epoch: 58| Step: 0
Training loss: 2.6737329815808484
Validation loss: 2.171427645704534
Epoch: 5| Step: 1
Training loss: 2.120805864836974
Validation loss: 2.1788988636762086
Epoch: 5| Step: 2
Training loss: 2.781075289950199
Validation loss: 2.194020267382459
Epoch: 5| Step: 3
Training loss: 2.460358566730871
Validation loss: 2.248936880641911
Epoch: 5| Step: 4
Training loss: 2.9552520680589334
Validation loss: 2.209219040765504
Epoch: 5| Step: 5
Training loss: 2.3422124269671936
Validation loss: 2.2090669964632297
Epoch: 5| Step: 6
Training loss: 2.6825115126107675
Validation loss: 2.2125116466470884
Epoch: 5| Step: 7
Training loss: 3.079009417973852
Validation loss: 2.1917453438557177
Epoch: 5| Step: 8
Training loss: 2.7350965147406203
Validation loss: 2.1729232207215
Epoch: 5| Step: 9
Training loss: 2.7819754693754994
Validation loss: 2.2048756475914155
Epoch: 59| Step: 0
Training loss: 2.3715192485287946
Validation loss: 2.2084170881023
Epoch: 5| Step: 1
Training loss: 2.8745874855316367
Validation loss: 2.1887658085686343
Epoch: 5| Step: 2
Training loss: 3.1012531577866453
Validation loss: 2.221596515891264
Epoch: 5| Step: 3
Training loss: 2.980092916128174
Validation loss: 2.186997754646399
Epoch: 5| Step: 4
Training loss: 2.7290276256601382
Validation loss: 2.1696791288408304
Epoch: 5| Step: 5
Training loss: 2.1651353070483026
Validation loss: 2.2351685166205586
Epoch: 5| Step: 6
Training loss: 2.5186889186675185
Validation loss: 2.2093321718900945
Epoch: 5| Step: 7
Training loss: 2.947239738537742
Validation loss: 2.2030151427596434
Epoch: 5| Step: 8
Training loss: 2.4620006384147604
Validation loss: 2.2286696494213425
Epoch: 5| Step: 9
Training loss: 2.1780485602255713
Validation loss: 2.2407410920397535
Epoch: 60| Step: 0
Training loss: 3.2907035158939193
Validation loss: 2.2052686174417473
Epoch: 5| Step: 1
Training loss: 2.564057876796611
Validation loss: 2.177935650579573
Epoch: 5| Step: 2
Training loss: 2.8409431774522007
Validation loss: 2.2078342329310985
Epoch: 5| Step: 3
Training loss: 2.327609216276712
Validation loss: 2.199543568165793
Epoch: 5| Step: 4
Training loss: 2.8269627327285956
Validation loss: 2.178673645795256
Epoch: 5| Step: 5
Training loss: 2.560974504390837
Validation loss: 2.2659624812305763
Epoch: 5| Step: 6
Training loss: 2.3724105170109926
Validation loss: 2.1964479396341687
Epoch: 5| Step: 7
Training loss: 2.592981155609494
Validation loss: 2.20697511132513
Epoch: 5| Step: 8
Training loss: 2.346732721193805
Validation loss: 2.1847477330400347
Epoch: 5| Step: 9
Training loss: 2.8469329459598276
Validation loss: 2.174221169385938
Epoch: 61| Step: 0
Training loss: 2.8239040173004786
Validation loss: 2.1814950328838196
Epoch: 5| Step: 1
Training loss: 2.183288880495057
Validation loss: 2.2184366265407838
Epoch: 5| Step: 2
Training loss: 2.565778356468837
Validation loss: 2.2079518736049013
Epoch: 5| Step: 3
Training loss: 3.145722915698952
Validation loss: 2.193187388226984
Epoch: 5| Step: 4
Training loss: 2.475499451460187
Validation loss: 2.203813962206068
Epoch: 5| Step: 5
Training loss: 3.0354874085347556
Validation loss: 2.241479199882326
Epoch: 5| Step: 6
Training loss: 2.1929498631581708
Validation loss: 2.194266277724379
Epoch: 5| Step: 7
Training loss: 3.1814039691470786
Validation loss: 2.2269554631749338
Epoch: 5| Step: 8
Training loss: 2.012218109670577
Validation loss: 2.2090098348023806
Epoch: 5| Step: 9
Training loss: 2.4429840604657946
Validation loss: 2.2037201466023233
Epoch: 62| Step: 0
Training loss: 2.6780082401208687
Validation loss: 2.2184617580442714
Epoch: 5| Step: 1
Training loss: 2.5817771244081444
Validation loss: 2.1302011348895054
Epoch: 5| Step: 2
Training loss: 2.409054199711164
Validation loss: 2.1861461966999127
Epoch: 5| Step: 3
Training loss: 3.0179562427440323
Validation loss: 2.1903612804908654
Epoch: 5| Step: 4
Training loss: 2.5087865916322007
Validation loss: 2.195092989734652
Epoch: 5| Step: 5
Training loss: 2.7369634093687902
Validation loss: 2.125887713786631
Epoch: 5| Step: 6
Training loss: 2.6139111408345457
Validation loss: 2.2207307126786318
Epoch: 5| Step: 7
Training loss: 2.9622472920858947
Validation loss: 2.1662224364403597
Epoch: 5| Step: 8
Training loss: 2.4948638608251783
Validation loss: 2.1871192077751687
Epoch: 5| Step: 9
Training loss: 2.7539921307531
Validation loss: 2.1935944245730368
Epoch: 63| Step: 0
Training loss: 3.139282243175872
Validation loss: 2.172613674960537
Epoch: 5| Step: 1
Training loss: 2.4917181164927356
Validation loss: 2.2530890364693468
Epoch: 5| Step: 2
Training loss: 2.793867572128214
Validation loss: 2.178300416687948
Epoch: 5| Step: 3
Training loss: 2.4708293415142406
Validation loss: 2.164130923682742
Epoch: 5| Step: 4
Training loss: 2.3174960835741647
Validation loss: 2.1920525763086323
Epoch: 5| Step: 5
Training loss: 2.3673196384324218
Validation loss: 2.182507488024078
Epoch: 5| Step: 6
Training loss: 2.804428386141826
Validation loss: 2.186600490007702
Epoch: 5| Step: 7
Training loss: 2.3703324479915184
Validation loss: 2.180134944350497
Epoch: 5| Step: 8
Training loss: 2.823290407183986
Validation loss: 2.179978474457985
Epoch: 5| Step: 9
Training loss: 2.838890366907434
Validation loss: 2.164045090146459
Epoch: 64| Step: 0
Training loss: 2.8405978999927743
Validation loss: 2.1966497575897215
Epoch: 5| Step: 1
Training loss: 2.481016756332499
Validation loss: 2.2140703640888915
Epoch: 5| Step: 2
Training loss: 2.7001380955737164
Validation loss: 2.1683243920493305
Epoch: 5| Step: 3
Training loss: 2.6258903083523926
Validation loss: 2.1598511260868936
Epoch: 5| Step: 4
Training loss: 2.2935002245540104
Validation loss: 2.2084608933214835
Epoch: 5| Step: 5
Training loss: 2.9451233982729295
Validation loss: 2.210712981856515
Epoch: 5| Step: 6
Training loss: 2.7397172457573866
Validation loss: 2.1811325347478383
Epoch: 5| Step: 7
Training loss: 2.617579006689618
Validation loss: 2.206501494520995
Epoch: 5| Step: 8
Training loss: 2.729146874819521
Validation loss: 2.201971083692698
Epoch: 5| Step: 9
Training loss: 2.286604358873339
Validation loss: 2.193828593897839
Epoch: 65| Step: 0
Training loss: 2.0782478697743048
Validation loss: 2.2005721260151163
Epoch: 5| Step: 1
Training loss: 2.4396964716120304
Validation loss: 2.1653966022722537
Epoch: 5| Step: 2
Training loss: 2.5201349052770765
Validation loss: 2.215523854105026
Epoch: 5| Step: 3
Training loss: 2.5018401049682684
Validation loss: 2.1996338112103317
Epoch: 5| Step: 4
Training loss: 2.7440241995987202
Validation loss: 2.2397305954934255
Epoch: 5| Step: 5
Training loss: 2.896934757213015
Validation loss: 2.1300752772281424
Epoch: 5| Step: 6
Training loss: 2.884604953844823
Validation loss: 2.220752972647364
Epoch: 5| Step: 7
Training loss: 2.643364472833168
Validation loss: 2.222901161733476
Epoch: 5| Step: 8
Training loss: 3.0943451896499785
Validation loss: 2.229019119249345
Epoch: 5| Step: 9
Training loss: 2.451010697981523
Validation loss: 2.18822731871258
Epoch: 66| Step: 0
Training loss: 3.069907125254839
Validation loss: 2.1815429973807547
Epoch: 5| Step: 1
Training loss: 2.840634662204672
Validation loss: 2.1136451559979843
Epoch: 5| Step: 2
Training loss: 2.3217185541485716
Validation loss: 2.1942638900536284
Epoch: 5| Step: 3
Training loss: 2.4263854326343526
Validation loss: 2.2209266720226655
Epoch: 5| Step: 4
Training loss: 3.021242276277719
Validation loss: 2.167745104500991
Epoch: 5| Step: 5
Training loss: 2.619404414673279
Validation loss: 2.204729897895042
Epoch: 5| Step: 6
Training loss: 2.120127533369506
Validation loss: 2.2422064558751784
Epoch: 5| Step: 7
Training loss: 2.471038723193345
Validation loss: 2.2050132862669813
Epoch: 5| Step: 8
Training loss: 2.6168305566410694
Validation loss: 2.2144994821408606
Epoch: 5| Step: 9
Training loss: 2.7868422959695764
Validation loss: 2.1716189167419224
Epoch: 67| Step: 0
Training loss: 2.6298074750453586
Validation loss: 2.1605459533511255
Epoch: 5| Step: 1
Training loss: 2.0947142700688275
Validation loss: 2.190654451051609
Epoch: 5| Step: 2
Training loss: 2.3265451280753595
Validation loss: 2.167596053552634
Epoch: 5| Step: 3
Training loss: 2.8584705027001993
Validation loss: 2.258274500519592
Epoch: 5| Step: 4
Training loss: 2.980734956984445
Validation loss: 2.221851448375831
Epoch: 5| Step: 5
Training loss: 2.5396433004591206
Validation loss: 2.176621932982441
Epoch: 5| Step: 6
Training loss: 2.743043597446135
Validation loss: 2.1810560691415755
Epoch: 5| Step: 7
Training loss: 2.902756755292373
Validation loss: 2.203427456525691
Epoch: 5| Step: 8
Training loss: 2.6752503251215556
Validation loss: 2.1825622980097577
Epoch: 5| Step: 9
Training loss: 2.371023161338582
Validation loss: 2.187783205020142
Epoch: 68| Step: 0
Training loss: 2.755868026319695
Validation loss: 2.1762285978268405
Epoch: 5| Step: 1
Training loss: 2.7935903857759383
Validation loss: 2.1789867749783842
Epoch: 5| Step: 2
Training loss: 2.8534474452840457
Validation loss: 2.1954164054175433
Epoch: 5| Step: 3
Training loss: 2.599030967008331
Validation loss: 2.2285469817147963
Epoch: 5| Step: 4
Training loss: 2.3697702148360857
Validation loss: 2.2121320680489776
Epoch: 5| Step: 5
Training loss: 2.876403300258805
Validation loss: 2.215356357781349
Epoch: 5| Step: 6
Training loss: 3.0957320828692927
Validation loss: 2.181210091261393
Epoch: 5| Step: 7
Training loss: 2.199361630502377
Validation loss: 2.225181856636764
Epoch: 5| Step: 8
Training loss: 2.3971152372959597
Validation loss: 2.2528942172851476
Epoch: 5| Step: 9
Training loss: 2.2188215647156855
Validation loss: 2.157302572029788
Epoch: 69| Step: 0
Training loss: 3.0005408435311676
Validation loss: 2.21423799300591
Epoch: 5| Step: 1
Training loss: 2.923742666378014
Validation loss: 2.179721823976612
Epoch: 5| Step: 2
Training loss: 2.7952379944938173
Validation loss: 2.2024947056476454
Epoch: 5| Step: 3
Training loss: 2.276289157520151
Validation loss: 2.175874751180845
Epoch: 5| Step: 4
Training loss: 2.30881867063432
Validation loss: 2.2261712225929804
Epoch: 5| Step: 5
Training loss: 2.0372886688535963
Validation loss: 2.159808681584877
Epoch: 5| Step: 6
Training loss: 2.665722640151047
Validation loss: 2.2273406419399713
Epoch: 5| Step: 7
Training loss: 3.2075029925290046
Validation loss: 2.2012147622074165
Epoch: 5| Step: 8
Training loss: 2.8483602859653394
Validation loss: 2.2306071407408132
Epoch: 5| Step: 9
Training loss: 2.2910204120660773
Validation loss: 2.203454513867729
Epoch: 70| Step: 0
Training loss: 2.1796936253834183
Validation loss: 2.2076082319026655
Epoch: 5| Step: 1
Training loss: 2.9909110670881653
Validation loss: 2.1856143258427028
Epoch: 5| Step: 2
Training loss: 2.8619734833753188
Validation loss: 2.2266155989049454
Epoch: 5| Step: 3
Training loss: 2.2325523785537125
Validation loss: 2.1996419715862
Epoch: 5| Step: 4
Training loss: 2.3729687085248687
Validation loss: 2.2060549398407057
Epoch: 5| Step: 5
Training loss: 2.498989091572303
Validation loss: 2.22332850961213
Epoch: 5| Step: 6
Training loss: 3.0022946165354156
Validation loss: 2.204030232402668
Epoch: 5| Step: 7
Training loss: 3.0078762295399466
Validation loss: 2.232548804781905
Epoch: 5| Step: 8
Training loss: 2.747229828120615
Validation loss: 2.2163236636243866
Epoch: 5| Step: 9
Training loss: 2.519237885658306
Validation loss: 2.1732163038152272
Epoch: 71| Step: 0
Training loss: 2.366331845987471
Validation loss: 2.1995183292276588
Epoch: 5| Step: 1
Training loss: 2.4356017913378243
Validation loss: 2.172620539785493
Epoch: 5| Step: 2
Training loss: 2.089070456924065
Validation loss: 2.177403976647517
Epoch: 5| Step: 3
Training loss: 2.7343321878623894
Validation loss: 2.236804183775806
Epoch: 5| Step: 4
Training loss: 3.3411824003118
Validation loss: 2.1771541537048646
Epoch: 5| Step: 5
Training loss: 2.581406695248859
Validation loss: 2.200488050997814
Epoch: 5| Step: 6
Training loss: 2.656342807719211
Validation loss: 2.1803641573291457
Epoch: 5| Step: 7
Training loss: 2.951529904655579
Validation loss: 2.2151246860649896
Epoch: 5| Step: 8
Training loss: 2.525061872876193
Validation loss: 2.206261077495567
Epoch: 5| Step: 9
Training loss: 2.671368824266629
Validation loss: 2.1994660176606846
Epoch: 72| Step: 0
Training loss: 1.8412822437247116
Validation loss: 2.190252873823942
Epoch: 5| Step: 1
Training loss: 2.3252014647125665
Validation loss: 2.177158693548268
Epoch: 5| Step: 2
Training loss: 3.063278722044513
Validation loss: 2.1973077342827074
Epoch: 5| Step: 3
Training loss: 2.8379469128859487
Validation loss: 2.210619168841303
Epoch: 5| Step: 4
Training loss: 3.0535985073842493
Validation loss: 2.179858192440778
Epoch: 5| Step: 5
Training loss: 2.5399148772085995
Validation loss: 2.1676765716516493
Epoch: 5| Step: 6
Training loss: 2.3918404076324773
Validation loss: 2.1631117905782498
Epoch: 5| Step: 7
Training loss: 3.2268672180377487
Validation loss: 2.134001700087969
Epoch: 5| Step: 8
Training loss: 2.3448249894740987
Validation loss: 2.2168829529045495
Epoch: 5| Step: 9
Training loss: 2.5478186234216134
Validation loss: 2.1082080786703417
Epoch: 73| Step: 0
Training loss: 3.0388189772290133
Validation loss: 2.165005299092308
Epoch: 5| Step: 1
Training loss: 2.4978340302819304
Validation loss: 2.185748783551398
Epoch: 5| Step: 2
Training loss: 2.0991422763491285
Validation loss: 2.2055448112482563
Epoch: 5| Step: 3
Training loss: 2.3003601828795817
Validation loss: 2.205288484225248
Epoch: 5| Step: 4
Training loss: 2.11265117397789
Validation loss: 2.15588654059022
Epoch: 5| Step: 5
Training loss: 3.2641675699359527
Validation loss: 2.194904422678553
Epoch: 5| Step: 6
Training loss: 2.4224300609842397
Validation loss: 2.182785907284103
Epoch: 5| Step: 7
Training loss: 2.2143886845462197
Validation loss: 2.1695650932647634
Epoch: 5| Step: 8
Training loss: 2.6214402903483833
Validation loss: 2.186524704304417
Epoch: 5| Step: 9
Training loss: 3.348007771253967
Validation loss: 2.205682738125546
Epoch: 74| Step: 0
Training loss: 2.675624959794641
Validation loss: 2.15745967814203
Epoch: 5| Step: 1
Training loss: 2.9580685850785002
Validation loss: 2.1833825599450143
Epoch: 5| Step: 2
Training loss: 2.9037752176832736
Validation loss: 2.2109465028757516
Epoch: 5| Step: 3
Training loss: 2.480019067567927
Validation loss: 2.144852749405162
Epoch: 5| Step: 4
Training loss: 2.2602231577184515
Validation loss: 2.17683233940789
Epoch: 5| Step: 5
Training loss: 2.521406838830143
Validation loss: 2.219763205416403
Epoch: 5| Step: 6
Training loss: 2.3057019086395156
Validation loss: 2.2091814597807273
Epoch: 5| Step: 7
Training loss: 2.7951710374666927
Validation loss: 2.189018322517317
Epoch: 5| Step: 8
Training loss: 3.1242699342518905
Validation loss: 2.1896936640872036
Epoch: 5| Step: 9
Training loss: 2.3305313679962247
Validation loss: 2.1658845011725307
Epoch: 75| Step: 0
Training loss: 2.767430637389792
Validation loss: 2.152352902832425
Epoch: 5| Step: 1
Training loss: 2.7345005551531365
Validation loss: 2.1753311682764953
Epoch: 5| Step: 2
Training loss: 3.137003618466276
Validation loss: 2.1754772352452103
Epoch: 5| Step: 3
Training loss: 2.4039203215674743
Validation loss: 2.2069446781150845
Epoch: 5| Step: 4
Training loss: 2.4198935001363084
Validation loss: 2.1448845756531174
Epoch: 5| Step: 5
Training loss: 2.466592932793321
Validation loss: 2.2354127364147764
Epoch: 5| Step: 6
Training loss: 2.6456934574028854
Validation loss: 2.1506315751115523
Epoch: 5| Step: 7
Training loss: 2.688378390103909
Validation loss: 2.178699275670008
Epoch: 5| Step: 8
Training loss: 2.1789786987734074
Validation loss: 2.1597246440750815
Epoch: 5| Step: 9
Training loss: 2.583746497636773
Validation loss: 2.18288992412357
Epoch: 76| Step: 0
Training loss: 2.7078289051296367
Validation loss: 2.180631466629494
Epoch: 5| Step: 1
Training loss: 2.9233156620246925
Validation loss: 2.2032394509726774
Epoch: 5| Step: 2
Training loss: 2.261831435590124
Validation loss: 2.189344341394346
Epoch: 5| Step: 3
Training loss: 2.490825986709898
Validation loss: 2.1897595090637956
Epoch: 5| Step: 4
Training loss: 3.003388398653918
Validation loss: 2.200779525107646
Epoch: 5| Step: 5
Training loss: 1.837115230628027
Validation loss: 2.200358877260872
Epoch: 5| Step: 6
Training loss: 3.0049696132754793
Validation loss: 2.1743820987309785
Epoch: 5| Step: 7
Training loss: 2.8163845786267476
Validation loss: 2.1150960940933055
Epoch: 5| Step: 8
Training loss: 2.5013300218797467
Validation loss: 2.2174483325052
Epoch: 5| Step: 9
Training loss: 2.7228301552355876
Validation loss: 2.2106313908087913
Epoch: 77| Step: 0
Training loss: 2.6339299125760336
Validation loss: 2.1212674305187194
Epoch: 5| Step: 1
Training loss: 2.8146216972742697
Validation loss: 2.2135081322700714
Epoch: 5| Step: 2
Training loss: 3.227088866814995
Validation loss: 2.191287594892151
Epoch: 5| Step: 3
Training loss: 2.336537669979588
Validation loss: 2.1974790641969433
Epoch: 5| Step: 4
Training loss: 2.597053905434374
Validation loss: 2.2114331699653507
Epoch: 5| Step: 5
Training loss: 2.2301926316681535
Validation loss: 2.2136007098442385
Epoch: 5| Step: 6
Training loss: 2.271592063023961
Validation loss: 2.190845260975653
Epoch: 5| Step: 7
Training loss: 2.613704173900901
Validation loss: 2.2039067260171334
Epoch: 5| Step: 8
Training loss: 2.5520332383409836
Validation loss: 2.1563142391062025
Epoch: 5| Step: 9
Training loss: 2.965732687471202
Validation loss: 2.2089033947215313
Epoch: 78| Step: 0
Training loss: 2.5045943006665565
Validation loss: 2.233262476063111
Epoch: 5| Step: 1
Training loss: 2.815522244321693
Validation loss: 2.1752990753265364
Epoch: 5| Step: 2
Training loss: 2.9145384698941057
Validation loss: 2.222003345900477
Epoch: 5| Step: 3
Training loss: 2.66495031514472
Validation loss: 2.173530020806203
Epoch: 5| Step: 4
Training loss: 2.5003398664245298
Validation loss: 2.1712412929858838
Epoch: 5| Step: 5
Training loss: 2.61758738636951
Validation loss: 2.222513503947834
Epoch: 5| Step: 6
Training loss: 2.737566061398563
Validation loss: 2.174928483221952
Epoch: 5| Step: 7
Training loss: 2.656197850332517
Validation loss: 2.1517176011135724
Epoch: 5| Step: 8
Training loss: 2.7470661465707837
Validation loss: 2.2025227430508796
Epoch: 5| Step: 9
Training loss: 2.033258003131871
Validation loss: 2.1958198401739284
Epoch: 79| Step: 0
Training loss: 2.73898162809452
Validation loss: 2.1803695564575407
Epoch: 5| Step: 1
Training loss: 2.3977196827661116
Validation loss: 2.219741662920599
Epoch: 5| Step: 2
Training loss: 2.8369939008236424
Validation loss: 2.138511159624938
Epoch: 5| Step: 3
Training loss: 2.6895670371493914
Validation loss: 2.201080843940637
Epoch: 5| Step: 4
Training loss: 2.281947199218144
Validation loss: 2.2285096133111377
Epoch: 5| Step: 5
Training loss: 2.3164637849555296
Validation loss: 2.158367645441698
Epoch: 5| Step: 6
Training loss: 2.54727562871828
Validation loss: 2.184318801673415
Epoch: 5| Step: 7
Training loss: 3.175153569389831
Validation loss: 2.187319290776518
Epoch: 5| Step: 8
Training loss: 2.7338629542886803
Validation loss: 2.2305907969907794
Epoch: 5| Step: 9
Training loss: 2.6567698250545364
Validation loss: 2.220017259370459
Epoch: 80| Step: 0
Training loss: 2.4364584629362698
Validation loss: 2.1940443171824255
Epoch: 5| Step: 1
Training loss: 2.4908688683100544
Validation loss: 2.202543886191359
Epoch: 5| Step: 2
Training loss: 2.8605434344254195
Validation loss: 2.1748142015719294
Epoch: 5| Step: 3
Training loss: 2.633360853735456
Validation loss: 2.2005153869362615
Epoch: 5| Step: 4
Training loss: 2.630091860367186
Validation loss: 2.165045881141988
Epoch: 5| Step: 5
Training loss: 3.0439959103610703
Validation loss: 2.163709508005856
Epoch: 5| Step: 6
Training loss: 2.2876812930782013
Validation loss: 2.192374813274351
Epoch: 5| Step: 7
Training loss: 2.561328713290595
Validation loss: 2.155089295227034
Epoch: 5| Step: 8
Training loss: 2.6356896320656515
Validation loss: 2.1896201830868565
Epoch: 5| Step: 9
Training loss: 2.7242630933342102
Validation loss: 2.198859377569762
Epoch: 81| Step: 0
Training loss: 3.0685043640652685
Validation loss: 2.243759797283559
Epoch: 5| Step: 1
Training loss: 2.5729085224552963
Validation loss: 2.1802687421668363
Epoch: 5| Step: 2
Training loss: 2.8670428081040185
Validation loss: 2.202324392560748
Epoch: 5| Step: 3
Training loss: 2.998699542150196
Validation loss: 2.1532695356383056
Epoch: 5| Step: 4
Training loss: 2.47178019692669
Validation loss: 2.1713556489620913
Epoch: 5| Step: 5
Training loss: 2.4228031471885734
Validation loss: 2.1745598094454293
Epoch: 5| Step: 6
Training loss: 2.555760709387697
Validation loss: 2.2012883051417043
Epoch: 5| Step: 7
Training loss: 2.7726557959768225
Validation loss: 2.168728780717349
Epoch: 5| Step: 8
Training loss: 2.184309021658705
Validation loss: 2.225956890610347
Epoch: 5| Step: 9
Training loss: 2.38175455590136
Validation loss: 2.197760323057996
Epoch: 82| Step: 0
Training loss: 2.854725543991673
Validation loss: 2.1803794861900303
Epoch: 5| Step: 1
Training loss: 2.7159586641090336
Validation loss: 2.183679075883195
Epoch: 5| Step: 2
Training loss: 2.404606245327571
Validation loss: 2.159390036014793
Epoch: 5| Step: 3
Training loss: 2.6444858113249214
Validation loss: 2.18468100119762
Epoch: 5| Step: 4
Training loss: 2.447404058366425
Validation loss: 2.1567549724325823
Epoch: 5| Step: 5
Training loss: 2.2298676170091323
Validation loss: 2.2130549003395212
Epoch: 5| Step: 6
Training loss: 2.361182726602963
Validation loss: 2.1728395941851177
Epoch: 5| Step: 7
Training loss: 3.234816663874006
Validation loss: 2.1887254796706164
Epoch: 5| Step: 8
Training loss: 2.83794372046855
Validation loss: 2.144602054976289
Epoch: 5| Step: 9
Training loss: 2.145846715595281
Validation loss: 2.1868857729519657
Epoch: 83| Step: 0
Training loss: 2.4431996342854205
Validation loss: 2.152319819601566
Epoch: 5| Step: 1
Training loss: 2.441190713142033
Validation loss: 2.1784371611578686
Epoch: 5| Step: 2
Training loss: 2.26817672674713
Validation loss: 2.1608988343319977
Epoch: 5| Step: 3
Training loss: 3.381439987739844
Validation loss: 2.2169259022737218
Epoch: 5| Step: 4
Training loss: 3.27461749456601
Validation loss: 2.178495503711457
Epoch: 5| Step: 5
Training loss: 2.340183443557259
Validation loss: 2.1574051803497643
Epoch: 5| Step: 6
Training loss: 2.2661940287575915
Validation loss: 2.224237186615568
Epoch: 5| Step: 7
Training loss: 2.7070622518066054
Validation loss: 2.207907985653487
Epoch: 5| Step: 8
Training loss: 2.739582617775324
Validation loss: 2.1421524413511563
Epoch: 5| Step: 9
Training loss: 2.3626469036629105
Validation loss: 2.148532995743465
Epoch: 84| Step: 0
Training loss: 3.082024597238146
Validation loss: 2.180245069348663
Epoch: 5| Step: 1
Training loss: 2.6327420460134654
Validation loss: 2.149822774512053
Epoch: 5| Step: 2
Training loss: 2.4672578575727333
Validation loss: 2.199317181857651
Epoch: 5| Step: 3
Training loss: 2.931091785837973
Validation loss: 2.1842735428317988
Epoch: 5| Step: 4
Training loss: 2.2443742356581886
Validation loss: 2.168857840701478
Epoch: 5| Step: 5
Training loss: 2.633063873153749
Validation loss: 2.1778367346787526
Epoch: 5| Step: 6
Training loss: 2.3834359776017
Validation loss: 2.1765073953615537
Epoch: 5| Step: 7
Training loss: 2.851289417307117
Validation loss: 2.1758553284242157
Epoch: 5| Step: 8
Training loss: 2.411904169535444
Validation loss: 2.1712122585991986
Epoch: 5| Step: 9
Training loss: 2.26385913301453
Validation loss: 2.1606998492017464
Epoch: 85| Step: 0
Training loss: 2.533006976672601
Validation loss: 2.2167578709061044
Epoch: 5| Step: 1
Training loss: 2.485094362152079
Validation loss: 2.2121054232402075
Epoch: 5| Step: 2
Training loss: 2.7394278788005013
Validation loss: 2.1844249082969385
Epoch: 5| Step: 3
Training loss: 3.1199495321251294
Validation loss: 2.20784038116197
Epoch: 5| Step: 4
Training loss: 2.565008099043108
Validation loss: 2.2081460527514816
Epoch: 5| Step: 5
Training loss: 2.682384235056335
Validation loss: 2.1993511183616157
Epoch: 5| Step: 6
Training loss: 2.680067029940323
Validation loss: 2.1987826154522443
Epoch: 5| Step: 7
Training loss: 1.8976781762946298
Validation loss: 2.1965685979067624
Epoch: 5| Step: 8
Training loss: 2.9727932350787993
Validation loss: 2.188618608278107
Epoch: 5| Step: 9
Training loss: 2.3938519010442776
Validation loss: 2.196330243168896
Epoch: 86| Step: 0
Training loss: 2.2939507170540847
Validation loss: 2.200836861301789
Epoch: 5| Step: 1
Training loss: 2.4035121650491105
Validation loss: 2.1584972334477763
Epoch: 5| Step: 2
Training loss: 2.5970998067673556
Validation loss: 2.1875225443149846
Epoch: 5| Step: 3
Training loss: 2.636805192448772
Validation loss: 2.2012422993168768
Epoch: 5| Step: 4
Training loss: 2.6273811076783953
Validation loss: 2.1848464848832134
Epoch: 5| Step: 5
Training loss: 2.624925339863204
Validation loss: 2.21575741933484
Epoch: 5| Step: 6
Training loss: 2.878448739829693
Validation loss: 2.1606445741202154
Epoch: 5| Step: 7
Training loss: 2.7313142048776817
Validation loss: 2.188886668285909
Epoch: 5| Step: 8
Training loss: 2.4038220331008886
Validation loss: 2.1816461986668325
Epoch: 5| Step: 9
Training loss: 3.0460329114772766
Validation loss: 2.1841211817848385
Epoch: 87| Step: 0
Training loss: 2.289327143971618
Validation loss: 2.1612713484314403
Epoch: 5| Step: 1
Training loss: 3.166140629761533
Validation loss: 2.180687104877201
Epoch: 5| Step: 2
Training loss: 2.5978156707718623
Validation loss: 2.203273369234796
Epoch: 5| Step: 3
Training loss: 2.876154294699174
Validation loss: 2.199205179292137
Epoch: 5| Step: 4
Training loss: 2.7158789547785958
Validation loss: 2.1984607696206946
Epoch: 5| Step: 5
Training loss: 2.8369858330529887
Validation loss: 2.199292271958812
Epoch: 5| Step: 6
Training loss: 2.7633371318918343
Validation loss: 2.1664061951485825
Epoch: 5| Step: 7
Training loss: 2.5137554353401987
Validation loss: 2.1838016965271536
Epoch: 5| Step: 8
Training loss: 2.285257138364531
Validation loss: 2.1305889961543736
Epoch: 5| Step: 9
Training loss: 2.108073419323854
Validation loss: 2.1785673947577893
Epoch: 88| Step: 0
Training loss: 2.9764772738372995
Validation loss: 2.1943343426117483
Epoch: 5| Step: 1
Training loss: 2.8646398550295005
Validation loss: 2.1984513735630333
Epoch: 5| Step: 2
Training loss: 2.5507988220824678
Validation loss: 2.213918149270309
Epoch: 5| Step: 3
Training loss: 2.8633625180315483
Validation loss: 2.234552386306016
Epoch: 5| Step: 4
Training loss: 2.4602316192197717
Validation loss: 2.2055269780558695
Epoch: 5| Step: 5
Training loss: 2.138642221184257
Validation loss: 2.1183377969421606
Epoch: 5| Step: 6
Training loss: 2.4557585945402294
Validation loss: 2.1922062607637782
Epoch: 5| Step: 7
Training loss: 2.609420958702443
Validation loss: 2.1658179955773145
Epoch: 5| Step: 8
Training loss: 2.2524643224331498
Validation loss: 2.20658939908257
Epoch: 5| Step: 9
Training loss: 2.883326208146246
Validation loss: 2.1709904494658963
Epoch: 89| Step: 0
Training loss: 2.1882070216235077
Validation loss: 2.217047436331581
Epoch: 5| Step: 1
Training loss: 2.1136540843783838
Validation loss: 2.224770589774444
Epoch: 5| Step: 2
Training loss: 2.8033003498122215
Validation loss: 2.176543433542988
Epoch: 5| Step: 3
Training loss: 3.292886065511817
Validation loss: 2.1956613127670566
Epoch: 5| Step: 4
Training loss: 2.4558587847442426
Validation loss: 2.197218584765289
Epoch: 5| Step: 5
Training loss: 2.924794090391904
Validation loss: 2.23575361295012
Epoch: 5| Step: 6
Training loss: 2.344163781833422
Validation loss: 2.175233195468564
Epoch: 5| Step: 7
Training loss: 2.57424672043558
Validation loss: 2.182724848526597
Epoch: 5| Step: 8
Training loss: 2.714953347607929
Validation loss: 2.1949041290591045
Epoch: 5| Step: 9
Training loss: 2.592688837485135
Validation loss: 2.194454916291247
Epoch: 90| Step: 0
Training loss: 2.606889100238985
Validation loss: 2.1869968519310072
Epoch: 5| Step: 1
Training loss: 2.366716898353735
Validation loss: 2.180558011071775
Epoch: 5| Step: 2
Training loss: 2.449214369482476
Validation loss: 2.158446472191426
Epoch: 5| Step: 3
Training loss: 2.3368497872350686
Validation loss: 2.1254834643322624
Epoch: 5| Step: 4
Training loss: 2.6325188085277182
Validation loss: 2.175230266746465
Epoch: 5| Step: 5
Training loss: 2.5327340944028136
Validation loss: 2.187907578696501
Epoch: 5| Step: 6
Training loss: 2.558523209159984
Validation loss: 2.1957897606803636
Epoch: 5| Step: 7
Training loss: 2.8228691538135453
Validation loss: 2.176179956022213
Epoch: 5| Step: 8
Training loss: 3.046903482939784
Validation loss: 2.1719555686273075
Epoch: 5| Step: 9
Training loss: 2.722056603313356
Validation loss: 2.172691752120339
Epoch: 91| Step: 0
Training loss: 2.737222289887654
Validation loss: 2.1664645450591435
Epoch: 5| Step: 1
Training loss: 2.4328088857125327
Validation loss: 2.1517103151019215
Epoch: 5| Step: 2
Training loss: 3.0599709792412786
Validation loss: 2.1467855391795108
Epoch: 5| Step: 3
Training loss: 2.6605667044069077
Validation loss: 2.1571630261793464
Epoch: 5| Step: 4
Training loss: 2.7054451924068674
Validation loss: 2.1963025878306346
Epoch: 5| Step: 5
Training loss: 2.3517466121343826
Validation loss: 2.172915236897312
Epoch: 5| Step: 6
Training loss: 3.0329036331438246
Validation loss: 2.1714525179300868
Epoch: 5| Step: 7
Training loss: 2.622211610194283
Validation loss: 2.1764781943521734
Epoch: 5| Step: 8
Training loss: 2.3224626334051606
Validation loss: 2.2025488649889784
Epoch: 5| Step: 9
Training loss: 2.3183525875927393
Validation loss: 2.1582358274454427
Epoch: 92| Step: 0
Training loss: 2.7038541482944347
Validation loss: 2.180164957357938
Epoch: 5| Step: 1
Training loss: 2.667773662712076
Validation loss: 2.2067850521906935
Epoch: 5| Step: 2
Training loss: 2.7484648494433195
Validation loss: 2.1888466000254607
Epoch: 5| Step: 3
Training loss: 2.980828699697196
Validation loss: 2.178960527562406
Epoch: 5| Step: 4
Training loss: 2.1809265552732056
Validation loss: 2.220446276861608
Epoch: 5| Step: 5
Training loss: 2.7571211426484794
Validation loss: 2.159208173267672
Epoch: 5| Step: 6
Training loss: 2.6725625386071163
Validation loss: 2.111128956513616
Epoch: 5| Step: 7
Training loss: 2.612213418382248
Validation loss: 2.1256417114418875
Epoch: 5| Step: 8
Training loss: 1.9967282475616444
Validation loss: 2.1503647153922647
Epoch: 5| Step: 9
Training loss: 2.7756704877919707
Validation loss: 2.165431672046596
Epoch: 93| Step: 0
Training loss: 2.920755916627016
Validation loss: 2.1503001641159734
Epoch: 5| Step: 1
Training loss: 2.647423834349007
Validation loss: 2.145946449285535
Epoch: 5| Step: 2
Training loss: 2.4018684227104172
Validation loss: 2.141154681402421
Epoch: 5| Step: 3
Training loss: 2.5383640678517123
Validation loss: 2.1645772714162104
Epoch: 5| Step: 4
Training loss: 2.7181337907885377
Validation loss: 2.1469920587710845
Epoch: 5| Step: 5
Training loss: 2.508709898487038
Validation loss: 2.196656519041145
Epoch: 5| Step: 6
Training loss: 2.623892414213507
Validation loss: 2.131617197829703
Epoch: 5| Step: 7
Training loss: 2.6613028543723525
Validation loss: 2.188852268843347
Epoch: 5| Step: 8
Training loss: 2.7555138190844786
Validation loss: 2.1507191296461987
Epoch: 5| Step: 9
Training loss: 2.445564135196943
Validation loss: 2.2117687973254037
Epoch: 94| Step: 0
Training loss: 2.5561782267224675
Validation loss: 2.112477631062115
Epoch: 5| Step: 1
Training loss: 2.773292215329803
Validation loss: 2.171649723696632
Epoch: 5| Step: 2
Training loss: 2.4638159977527687
Validation loss: 2.1524466808875196
Epoch: 5| Step: 3
Training loss: 2.961675949411294
Validation loss: 2.1845172841529847
Epoch: 5| Step: 4
Training loss: 2.205054337919751
Validation loss: 2.199811848898407
Epoch: 5| Step: 5
Training loss: 2.6745288897494626
Validation loss: 2.192801830901867
Epoch: 5| Step: 6
Training loss: 3.0420754214636174
Validation loss: 2.175057447403042
Epoch: 5| Step: 7
Training loss: 2.5983784754340777
Validation loss: 2.1487638042125328
Epoch: 5| Step: 8
Training loss: 2.729731338751617
Validation loss: 2.180181277292204
Epoch: 5| Step: 9
Training loss: 2.3780449872586056
Validation loss: 2.186740490305325
Epoch: 95| Step: 0
Training loss: 2.3838847765055737
Validation loss: 2.196279146635285
Epoch: 5| Step: 1
Training loss: 2.4139481304446146
Validation loss: 2.240071714837588
Epoch: 5| Step: 2
Training loss: 2.6294118953262537
Validation loss: 2.193214866528039
Epoch: 5| Step: 3
Training loss: 3.0029203188370115
Validation loss: 2.19053737428482
Epoch: 5| Step: 4
Training loss: 2.353178869190626
Validation loss: 2.167888057168042
Epoch: 5| Step: 5
Training loss: 3.375940827524568
Validation loss: 2.201936756681821
Epoch: 5| Step: 6
Training loss: 2.306959680880393
Validation loss: 2.1705743876471137
Epoch: 5| Step: 7
Training loss: 2.5650895223183388
Validation loss: 2.1250565399284755
Epoch: 5| Step: 8
Training loss: 2.41784608495904
Validation loss: 2.1630709474791248
Epoch: 5| Step: 9
Training loss: 2.6667677045595455
Validation loss: 2.1449610879911343
Epoch: 96| Step: 0
Training loss: 2.7508382386683197
Validation loss: 2.171685186251543
Epoch: 5| Step: 1
Training loss: 2.534615245866733
Validation loss: 2.185924524284445
Epoch: 5| Step: 2
Training loss: 2.4638766705500226
Validation loss: 2.1357078311106212
Epoch: 5| Step: 3
Training loss: 2.2566611974622677
Validation loss: 2.183536685213136
Epoch: 5| Step: 4
Training loss: 3.0525053771875306
Validation loss: 2.1496445034866984
Epoch: 5| Step: 5
Training loss: 2.542273454203636
Validation loss: 2.1583112211582
Epoch: 5| Step: 6
Training loss: 2.7832934127350843
Validation loss: 2.1844031009421263
Epoch: 5| Step: 7
Training loss: 2.9297171222460774
Validation loss: 2.1575623516133153
Epoch: 5| Step: 8
Training loss: 2.505670220720373
Validation loss: 2.1740341684582116
Epoch: 5| Step: 9
Training loss: 2.360109530671222
Validation loss: 2.205689558854371
Epoch: 97| Step: 0
Training loss: 2.4493725498709917
Validation loss: 2.1727742043401386
Epoch: 5| Step: 1
Training loss: 2.3446273178965265
Validation loss: 2.1971737125739472
Epoch: 5| Step: 2
Training loss: 3.029191724279169
Validation loss: 2.172666299020915
Epoch: 5| Step: 3
Training loss: 2.1444632097300578
Validation loss: 2.151299536169988
Epoch: 5| Step: 4
Training loss: 2.2770223243500225
Validation loss: 2.172873340315991
Epoch: 5| Step: 5
Training loss: 2.450086035482899
Validation loss: 2.247802348699688
Epoch: 5| Step: 6
Training loss: 2.717438304660518
Validation loss: 2.148398433083351
Epoch: 5| Step: 7
Training loss: 2.4472420486568867
Validation loss: 2.2125561266670295
Epoch: 5| Step: 8
Training loss: 2.7893709070994928
Validation loss: 2.1615966001930094
Epoch: 5| Step: 9
Training loss: 3.3183271867920916
Validation loss: 2.1781083414263844
Epoch: 98| Step: 0
Training loss: 2.780177048733092
Validation loss: 2.1545704239171277
Epoch: 5| Step: 1
Training loss: 2.615962137350756
Validation loss: 2.19649750506087
Epoch: 5| Step: 2
Training loss: 2.4737006658923053
Validation loss: 2.1600322320591863
Epoch: 5| Step: 3
Training loss: 3.0166075691911707
Validation loss: 2.2057428792727385
Epoch: 5| Step: 4
Training loss: 2.6796830513697585
Validation loss: 2.214018064195207
Epoch: 5| Step: 5
Training loss: 2.6267135113768325
Validation loss: 2.1739324766270154
Epoch: 5| Step: 6
Training loss: 2.8829795561564118
Validation loss: 2.1795945985953575
Epoch: 5| Step: 7
Training loss: 2.611764693020132
Validation loss: 2.182625744776911
Epoch: 5| Step: 8
Training loss: 2.2261718407286364
Validation loss: 2.192093828600008
Epoch: 5| Step: 9
Training loss: 2.1853826902625477
Validation loss: 2.1432761236161557
Epoch: 99| Step: 0
Training loss: 2.835450185726652
Validation loss: 2.126827182881082
Epoch: 5| Step: 1
Training loss: 3.104101568644608
Validation loss: 2.1859946966549746
Epoch: 5| Step: 2
Training loss: 2.979886979385646
Validation loss: 2.2259065287332223
Epoch: 5| Step: 3
Training loss: 2.5478974144853654
Validation loss: 2.1896577550410354
Epoch: 5| Step: 4
Training loss: 2.2399789386849047
Validation loss: 2.1335727725822267
Epoch: 5| Step: 5
Training loss: 2.746656118792431
Validation loss: 2.148856563926403
Epoch: 5| Step: 6
Training loss: 2.543256939615201
Validation loss: 2.173111813375865
Epoch: 5| Step: 7
Training loss: 2.368982220139775
Validation loss: 2.1793546264779815
Epoch: 5| Step: 8
Training loss: 2.616115794742554
Validation loss: 2.1719848224773726
Epoch: 5| Step: 9
Training loss: 2.031003966469885
Validation loss: 2.149362348297296
Epoch: 100| Step: 0
Training loss: 2.7506900268440457
Validation loss: 2.179128855616782
Epoch: 5| Step: 1
Training loss: 2.686365464730221
Validation loss: 2.138020440072371
Epoch: 5| Step: 2
Training loss: 2.5809156630821217
Validation loss: 2.144622653043407
Epoch: 5| Step: 3
Training loss: 2.9824869464802846
Validation loss: 2.169611222495018
Epoch: 5| Step: 4
Training loss: 2.373077819371289
Validation loss: 2.161641837749383
Epoch: 5| Step: 5
Training loss: 2.881600805014964
Validation loss: 2.171522256633839
Epoch: 5| Step: 6
Training loss: 2.327753434196809
Validation loss: 2.149204906263968
Epoch: 5| Step: 7
Training loss: 2.2834875551289326
Validation loss: 2.187250929915488
Epoch: 5| Step: 8
Training loss: 2.596342239173931
Validation loss: 2.1642277558538145
Epoch: 5| Step: 9
Training loss: 2.7651499539596944
Validation loss: 2.187853936401777

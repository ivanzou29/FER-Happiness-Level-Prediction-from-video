Epoch: 1| Step: 0
Training loss: 6.200685881546599
Validation loss: 5.875632734254094
Epoch: 7| Step: 1
Training loss: 5.069118553149432
Validation loss: 5.87069238504982
Epoch: 7| Step: 2
Training loss: 6.907806911436045
Validation loss: 5.857007951690746
Epoch: 7| Step: 3
Training loss: 5.231921358223269
Validation loss: 5.851255316555054
Epoch: 7| Step: 4
Training loss: 7.125756742211308
Validation loss: 5.838536348660461
Epoch: 7| Step: 5
Training loss: 6.7416451634966705
Validation loss: 5.845352371295366
Epoch: 7| Step: 6
Training loss: 6.642205119911645
Validation loss: 5.845718858977035
Epoch: 7| Step: 7
Training loss: 5.667451841522481
Validation loss: 5.827485898468123
Epoch: 7| Step: 8
Training loss: 6.045214994407832
Validation loss: 5.835344714243464
Epoch: 7| Step: 9
Training loss: 5.274829098794043
Validation loss: 5.811985432958494
Epoch: 7| Step: 10
Training loss: 6.528998904171591
Validation loss: 5.819790824992626
Epoch: 7| Step: 11
Training loss: 6.798525134192278
Validation loss: 5.807139559264827
Epoch: 7| Step: 12
Training loss: 6.136313752250976
Validation loss: 5.7910331849482155
Epoch: 7| Step: 13
Training loss: 5.4076700275019345
Validation loss: 5.8010802484608615
Epoch: 7| Step: 14
Training loss: 6.827421592952215
Validation loss: 5.804228010683099
Epoch: 7| Step: 15
Training loss: 5.865253561511242
Validation loss: 5.797614789955245
Epoch: 2| Step: 0
Training loss: 6.037564939540602
Validation loss: 5.795928228054773
Epoch: 7| Step: 1
Training loss: 5.288376914461024
Validation loss: 5.779280377992078
Epoch: 7| Step: 2
Training loss: 6.177767719440521
Validation loss: 5.772214958912682
Epoch: 7| Step: 3
Training loss: 5.800718775968293
Validation loss: 5.773761108541727
Epoch: 7| Step: 4
Training loss: 6.438634976081857
Validation loss: 5.765670500400403
Epoch: 7| Step: 5
Training loss: 6.737178351769729
Validation loss: 5.75671336267901
Epoch: 7| Step: 6
Training loss: 6.31129217395529
Validation loss: 5.7585150742600195
Epoch: 7| Step: 7
Training loss: 6.307560262675669
Validation loss: 5.749176506303693
Epoch: 7| Step: 8
Training loss: 5.79504201036157
Validation loss: 5.737114498974701
Epoch: 7| Step: 9
Training loss: 5.754832020420054
Validation loss: 5.730573905664292
Epoch: 7| Step: 10
Training loss: 5.062541631833545
Validation loss: 5.7338559450004585
Epoch: 7| Step: 11
Training loss: 5.914535281837251
Validation loss: 5.734320355223455
Epoch: 7| Step: 12
Training loss: 6.2467133081758295
Validation loss: 5.718640702821643
Epoch: 7| Step: 13
Training loss: 6.897475800922857
Validation loss: 5.707680722610946
Epoch: 7| Step: 14
Training loss: 6.113903035970718
Validation loss: 5.710051845813801
Epoch: 7| Step: 15
Training loss: 6.556333714797101
Validation loss: 5.7144551496609
Epoch: 3| Step: 0
Training loss: 6.202540449374196
Validation loss: 5.711394925768042
Epoch: 7| Step: 1
Training loss: 6.146524095240264
Validation loss: 5.698119764452654
Epoch: 7| Step: 2
Training loss: 5.8064744131946355
Validation loss: 5.689544258249279
Epoch: 7| Step: 3
Training loss: 6.261575279567391
Validation loss: 5.692028031412338
Epoch: 7| Step: 4
Training loss: 6.511418510156163
Validation loss: 5.68527295405201
Epoch: 7| Step: 5
Training loss: 6.59955388497684
Validation loss: 5.68991145720027
Epoch: 7| Step: 6
Training loss: 5.827763032283418
Validation loss: 5.652358370816775
Epoch: 7| Step: 7
Training loss: 6.242324999463404
Validation loss: 5.673951469698581
Epoch: 7| Step: 8
Training loss: 6.2167158777711276
Validation loss: 5.666178837354887
Epoch: 7| Step: 9
Training loss: 5.166864739999982
Validation loss: 5.666529296680738
Epoch: 7| Step: 10
Training loss: 5.910232089628248
Validation loss: 5.649609633634328
Epoch: 7| Step: 11
Training loss: 6.2028737822044215
Validation loss: 5.651162745146141
Epoch: 7| Step: 12
Training loss: 5.749102315074869
Validation loss: 5.6383100053309665
Epoch: 7| Step: 13
Training loss: 5.9239730138067435
Validation loss: 5.632568676303069
Epoch: 7| Step: 14
Training loss: 5.868926459798874
Validation loss: 5.641240120116171
Epoch: 7| Step: 15
Training loss: 5.71106908727507
Validation loss: 5.627443233120559
Epoch: 4| Step: 0
Training loss: 6.246304449424663
Validation loss: 5.623805536132922
Epoch: 7| Step: 1
Training loss: 6.2971539684056195
Validation loss: 5.619424155770413
Epoch: 7| Step: 2
Training loss: 6.227204829341352
Validation loss: 5.6144555971845795
Epoch: 7| Step: 3
Training loss: 6.461800052950911
Validation loss: 5.6136528328177215
Epoch: 7| Step: 4
Training loss: 6.060437352982359
Validation loss: 5.594774074083358
Epoch: 7| Step: 5
Training loss: 5.516558949013366
Validation loss: 5.59640170549878
Epoch: 7| Step: 6
Training loss: 5.574876710358119
Validation loss: 5.568180095968442
Epoch: 7| Step: 7
Training loss: 6.950065711280238
Validation loss: 5.55949080654768
Epoch: 7| Step: 8
Training loss: 5.634321669306588
Validation loss: 5.571795839790695
Epoch: 7| Step: 9
Training loss: 6.105426263447822
Validation loss: 5.577005267177072
Epoch: 7| Step: 10
Training loss: 5.7816336839529985
Validation loss: 5.57352452462897
Epoch: 7| Step: 11
Training loss: 4.959951996724812
Validation loss: 5.572691246430403
Epoch: 7| Step: 12
Training loss: 5.561282421111016
Validation loss: 5.538601931824669
Epoch: 7| Step: 13
Training loss: 5.71471725605479
Validation loss: 5.555891520697465
Epoch: 7| Step: 14
Training loss: 5.807146129295541
Validation loss: 5.539191249657387
Epoch: 7| Step: 15
Training loss: 6.09914187039257
Validation loss: 5.539399171058807
Epoch: 5| Step: 0
Training loss: 5.803653999831129
Validation loss: 5.532636752073554
Epoch: 7| Step: 1
Training loss: 5.487011398078338
Validation loss: 5.519412556167551
Epoch: 7| Step: 2
Training loss: 5.0799731838089235
Validation loss: 5.525574542937476
Epoch: 7| Step: 3
Training loss: 6.004511726341897
Validation loss: 5.511777175614132
Epoch: 7| Step: 4
Training loss: 6.699468722559333
Validation loss: 5.500075727384197
Epoch: 7| Step: 5
Training loss: 6.234486973266062
Validation loss: 5.502278936173064
Epoch: 7| Step: 6
Training loss: 6.0590086646690935
Validation loss: 5.501258968014694
Epoch: 7| Step: 7
Training loss: 6.198486130996996
Validation loss: 5.500680967920894
Epoch: 7| Step: 8
Training loss: 6.056414824172873
Validation loss: 5.4850228039750135
Epoch: 7| Step: 9
Training loss: 5.813806202464479
Validation loss: 5.456346798223834
Epoch: 7| Step: 10
Training loss: 5.236272758239552
Validation loss: 5.467279034489195
Epoch: 7| Step: 11
Training loss: 5.535337155763731
Validation loss: 5.472256447394382
Epoch: 7| Step: 12
Training loss: 5.769434386352594
Validation loss: 5.466011949647759
Epoch: 7| Step: 13
Training loss: 6.448093031862878
Validation loss: 5.442846674288401
Epoch: 7| Step: 14
Training loss: 4.861428958250165
Validation loss: 5.450112866075228
Epoch: 7| Step: 15
Training loss: 6.257162643769614
Validation loss: 5.436184005059523
Epoch: 6| Step: 0
Training loss: 6.317378510128578
Validation loss: 5.440385598052013
Epoch: 7| Step: 1
Training loss: 5.194408917546398
Validation loss: 5.42866803683695
Epoch: 7| Step: 2
Training loss: 5.7019085148457025
Validation loss: 5.427201893703534
Epoch: 7| Step: 3
Training loss: 5.951444934455945
Validation loss: 5.404727922230096
Epoch: 7| Step: 4
Training loss: 5.7145957658303095
Validation loss: 5.40906403186975
Epoch: 7| Step: 5
Training loss: 6.0764088696348075
Validation loss: 5.399059142059983
Epoch: 7| Step: 6
Training loss: 5.495072324795765
Validation loss: 5.3916718924801845
Epoch: 7| Step: 7
Training loss: 6.485247082264297
Validation loss: 5.385143866962416
Epoch: 7| Step: 8
Training loss: 5.297878474153999
Validation loss: 5.380449180468211
Epoch: 7| Step: 9
Training loss: 4.586388742838907
Validation loss: 5.376534403826587
Epoch: 7| Step: 10
Training loss: 5.69568288511852
Validation loss: 5.367463247951696
Epoch: 7| Step: 11
Training loss: 5.592692525633722
Validation loss: 5.359284055608033
Epoch: 7| Step: 12
Training loss: 5.484128767175415
Validation loss: 5.358437939450864
Epoch: 7| Step: 13
Training loss: 6.775639757869213
Validation loss: 5.353431997403975
Epoch: 7| Step: 14
Training loss: 5.543940925965985
Validation loss: 5.341279498661037
Epoch: 7| Step: 15
Training loss: 6.078154811442798
Validation loss: 5.3358214769458945
Epoch: 7| Step: 0
Training loss: 4.752911528659836
Validation loss: 5.319228566632513
Epoch: 7| Step: 1
Training loss: 5.1147080333085695
Validation loss: 5.313427576491807
Epoch: 7| Step: 2
Training loss: 6.030992887877008
Validation loss: 5.30505584396567
Epoch: 7| Step: 3
Training loss: 6.202276599432594
Validation loss: 5.306033737131344
Epoch: 7| Step: 4
Training loss: 5.733413657283708
Validation loss: 5.269545582140407
Epoch: 7| Step: 5
Training loss: 5.581174214736033
Validation loss: 5.291727542940813
Epoch: 7| Step: 6
Training loss: 5.157342413576398
Validation loss: 5.258097093071895
Epoch: 7| Step: 7
Training loss: 5.9560571693356446
Validation loss: 5.278329414996125
Epoch: 7| Step: 8
Training loss: 5.144596502053909
Validation loss: 5.245725755457155
Epoch: 7| Step: 9
Training loss: 6.466062125561515
Validation loss: 5.266333096216033
Epoch: 7| Step: 10
Training loss: 6.324391862786653
Validation loss: 5.252062154417564
Epoch: 7| Step: 11
Training loss: 4.7463573743656635
Validation loss: 5.2428110003340604
Epoch: 7| Step: 12
Training loss: 6.60193375666498
Validation loss: 5.232017347246846
Epoch: 7| Step: 13
Training loss: 5.93600348636494
Validation loss: 5.219907683524041
Epoch: 7| Step: 14
Training loss: 5.008498408198973
Validation loss: 5.214214966156513
Epoch: 7| Step: 15
Training loss: 5.337129473723409
Validation loss: 5.1953513457605
Epoch: 8| Step: 0
Training loss: 6.199529494614074
Validation loss: 5.204074208194793
Epoch: 7| Step: 1
Training loss: 5.358103253698544
Validation loss: 5.187384166341435
Epoch: 7| Step: 2
Training loss: 5.237474850250532
Validation loss: 5.184983067178841
Epoch: 7| Step: 3
Training loss: 5.467726257414995
Validation loss: 5.164511124381885
Epoch: 7| Step: 4
Training loss: 6.045318797550189
Validation loss: 5.16232009942943
Epoch: 7| Step: 5
Training loss: 5.177766976097384
Validation loss: 5.154346571870539
Epoch: 7| Step: 6
Training loss: 5.055487402897902
Validation loss: 5.139584197403847
Epoch: 7| Step: 7
Training loss: 5.830587158472878
Validation loss: 5.136016477740761
Epoch: 7| Step: 8
Training loss: 5.807151384469707
Validation loss: 5.125767098376851
Epoch: 7| Step: 9
Training loss: 5.351586735802785
Validation loss: 5.116279052008026
Epoch: 7| Step: 10
Training loss: 5.270181286352076
Validation loss: 5.102212856298013
Epoch: 7| Step: 11
Training loss: 5.632048027234774
Validation loss: 5.076742021644234
Epoch: 7| Step: 12
Training loss: 5.447063936074336
Validation loss: 5.058807959031478
Epoch: 7| Step: 13
Training loss: 5.408667404288228
Validation loss: 5.076575882217913
Epoch: 7| Step: 14
Training loss: 6.041704199663409
Validation loss: 5.034343496201246
Epoch: 7| Step: 15
Training loss: 5.052883858196607
Validation loss: 5.039987935131782
Epoch: 9| Step: 0
Training loss: 5.7696522446266885
Validation loss: 5.040424795199889
Epoch: 7| Step: 1
Training loss: 5.318594275591942
Validation loss: 5.008214028803215
Epoch: 7| Step: 2
Training loss: 5.641927890937466
Validation loss: 5.029302314911817
Epoch: 7| Step: 3
Training loss: 4.888007783000138
Validation loss: 5.009393377128126
Epoch: 7| Step: 4
Training loss: 5.060654480414722
Validation loss: 5.001601612448685
Epoch: 7| Step: 5
Training loss: 4.202410442208171
Validation loss: 4.999981763564797
Epoch: 7| Step: 6
Training loss: 5.993825596560383
Validation loss: 4.982299000644997
Epoch: 7| Step: 7
Training loss: 5.850501092821825
Validation loss: 4.963141287754051
Epoch: 7| Step: 8
Training loss: 6.0260031217256165
Validation loss: 4.940834138884949
Epoch: 7| Step: 9
Training loss: 5.770139833868203
Validation loss: 4.9368215693054545
Epoch: 7| Step: 10
Training loss: 5.0282902512028675
Validation loss: 4.922463550948416
Epoch: 7| Step: 11
Training loss: 5.43233586772176
Validation loss: 4.915721274808255
Epoch: 7| Step: 12
Training loss: 5.218796781227451
Validation loss: 4.907967226403897
Epoch: 7| Step: 13
Training loss: 5.875850210151606
Validation loss: 4.895040905007999
Epoch: 7| Step: 14
Training loss: 4.75559467307508
Validation loss: 4.891406737190412
Epoch: 7| Step: 15
Training loss: 4.7061648256008475
Validation loss: 4.8530961226610385
Epoch: 10| Step: 0
Training loss: 4.920035657365806
Validation loss: 4.851670571747183
Epoch: 7| Step: 1
Training loss: 5.152729103884424
Validation loss: 4.849603557177248
Epoch: 7| Step: 2
Training loss: 5.127124811113472
Validation loss: 4.819374402624402
Epoch: 7| Step: 3
Training loss: 4.383624703667336
Validation loss: 4.785334012912955
Epoch: 7| Step: 4
Training loss: 6.169323048357899
Validation loss: 4.799122802441616
Epoch: 7| Step: 5
Training loss: 5.288243104918566
Validation loss: 4.781161377899375
Epoch: 7| Step: 6
Training loss: 6.054829572425887
Validation loss: 4.776439708791576
Epoch: 7| Step: 7
Training loss: 4.718799110813871
Validation loss: 4.750380674742195
Epoch: 7| Step: 8
Training loss: 4.595253211880585
Validation loss: 4.744147943309522
Epoch: 7| Step: 9
Training loss: 5.929726742500253
Validation loss: 4.725235367479688
Epoch: 7| Step: 10
Training loss: 5.317906220652567
Validation loss: 4.725303970698481
Epoch: 7| Step: 11
Training loss: 4.949900446959965
Validation loss: 4.698199950670742
Epoch: 7| Step: 12
Training loss: 4.808531474896297
Validation loss: 4.692141770984467
Epoch: 7| Step: 13
Training loss: 5.956641893013341
Validation loss: 4.674325265296153
Epoch: 7| Step: 14
Training loss: 4.790105460325996
Validation loss: 4.641335904779653
Epoch: 7| Step: 15
Training loss: 4.248865088228134
Validation loss: 4.64364392543577
Epoch: 11| Step: 0
Training loss: 5.347406575161283
Validation loss: 4.617195008123431
Epoch: 7| Step: 1
Training loss: 5.397734004112317
Validation loss: 4.591679132312389
Epoch: 7| Step: 2
Training loss: 5.171356267355876
Validation loss: 4.591792921488315
Epoch: 7| Step: 3
Training loss: 4.817082192912166
Validation loss: 4.572699630108848
Epoch: 7| Step: 4
Training loss: 4.868457022435021
Validation loss: 4.5699352513633915
Epoch: 7| Step: 5
Training loss: 4.646755908089035
Validation loss: 4.5445204125217895
Epoch: 7| Step: 6
Training loss: 5.380493550404764
Validation loss: 4.5337705217105295
Epoch: 7| Step: 7
Training loss: 5.375308892888829
Validation loss: 4.5012019836696
Epoch: 7| Step: 8
Training loss: 5.264877854947316
Validation loss: 4.503331971408362
Epoch: 7| Step: 9
Training loss: 4.420622847157572
Validation loss: 4.476114281857721
Epoch: 7| Step: 10
Training loss: 4.430757369836598
Validation loss: 4.457374037315622
Epoch: 7| Step: 11
Training loss: 4.707471492962614
Validation loss: 4.447499677196713
Epoch: 7| Step: 12
Training loss: 4.537309685701425
Validation loss: 4.424511375553441
Epoch: 7| Step: 13
Training loss: 4.3101174227480135
Validation loss: 4.403044279804422
Epoch: 7| Step: 14
Training loss: 5.431161277055103
Validation loss: 4.384464370397414
Epoch: 7| Step: 15
Training loss: 4.995195560082876
Validation loss: 4.372076665137219
Epoch: 12| Step: 0
Training loss: 4.6197748309333635
Validation loss: 4.362737486717071
Epoch: 7| Step: 1
Training loss: 4.891113646839422
Validation loss: 4.321058416921225
Epoch: 7| Step: 2
Training loss: 4.801245869438859
Validation loss: 4.299936863120339
Epoch: 7| Step: 3
Training loss: 5.52399498960142
Validation loss: 4.297047890655308
Epoch: 7| Step: 4
Training loss: 4.913231420059566
Validation loss: 4.268286580359448
Epoch: 7| Step: 5
Training loss: 4.798024589637755
Validation loss: 4.263151586864813
Epoch: 7| Step: 6
Training loss: 2.8358065151934766
Validation loss: 4.231368149802104
Epoch: 7| Step: 7
Training loss: 4.929595607862339
Validation loss: 4.221559028305127
Epoch: 7| Step: 8
Training loss: 5.2974644335803545
Validation loss: 4.187001244336222
Epoch: 7| Step: 9
Training loss: 4.758154645371718
Validation loss: 4.175148138775982
Epoch: 7| Step: 10
Training loss: 3.9069284078865056
Validation loss: 4.160955171211787
Epoch: 7| Step: 11
Training loss: 4.9930866130251434
Validation loss: 4.147188791892222
Epoch: 7| Step: 12
Training loss: 4.570747759054781
Validation loss: 4.128310420624472
Epoch: 7| Step: 13
Training loss: 4.022581968734811
Validation loss: 4.104012368397282
Epoch: 7| Step: 14
Training loss: 4.380122428471985
Validation loss: 4.094462881255919
Epoch: 7| Step: 15
Training loss: 5.111280919907642
Validation loss: 4.080440842248721
Epoch: 13| Step: 0
Training loss: 4.402850198935637
Validation loss: 4.028891528956369
Epoch: 7| Step: 1
Training loss: 4.201854641462329
Validation loss: 4.031135044856982
Epoch: 7| Step: 2
Training loss: 4.342462801776712
Validation loss: 4.001798698487479
Epoch: 7| Step: 3
Training loss: 4.58510747153354
Validation loss: 3.97296881752868
Epoch: 7| Step: 4
Training loss: 4.407513809225744
Validation loss: 3.951668322075778
Epoch: 7| Step: 5
Training loss: 4.642409061985352
Validation loss: 3.949950515246838
Epoch: 7| Step: 6
Training loss: 4.744081926355679
Validation loss: 3.926132580912734
Epoch: 7| Step: 7
Training loss: 4.322323432877095
Validation loss: 3.8704888837036764
Epoch: 7| Step: 8
Training loss: 4.919670265127341
Validation loss: 3.847761468612762
Epoch: 7| Step: 9
Training loss: 3.9308230994612074
Validation loss: 3.821800836665788
Epoch: 7| Step: 10
Training loss: 4.228091374141625
Validation loss: 3.8091879041331764
Epoch: 7| Step: 11
Training loss: 4.322538330190583
Validation loss: 3.783283716533966
Epoch: 7| Step: 12
Training loss: 4.779411810275117
Validation loss: 3.772979090841421
Epoch: 7| Step: 13
Training loss: 4.330511103324366
Validation loss: 3.734316148869609
Epoch: 7| Step: 14
Training loss: 4.325439942315704
Validation loss: 3.726891540466722
Epoch: 7| Step: 15
Training loss: 3.2324475428550374
Validation loss: 3.685458848334114
Epoch: 14| Step: 0
Training loss: 4.010025811243107
Validation loss: 3.688258437349132
Epoch: 7| Step: 1
Training loss: 3.6211719689670923
Validation loss: 3.624818619224307
Epoch: 7| Step: 2
Training loss: 3.6016014245964634
Validation loss: 3.6290372332246594
Epoch: 7| Step: 3
Training loss: 4.20812758644667
Validation loss: 3.6086983982189285
Epoch: 7| Step: 4
Training loss: 3.795506108929981
Validation loss: 3.5930965702310966
Epoch: 7| Step: 5
Training loss: 3.4848100193951543
Validation loss: 3.5454971415397103
Epoch: 7| Step: 6
Training loss: 4.3002321757245525
Validation loss: 3.5321558387156786
Epoch: 7| Step: 7
Training loss: 4.487085035259245
Validation loss: 3.4965994037003907
Epoch: 7| Step: 8
Training loss: 3.774577914255597
Validation loss: 3.493744071831358
Epoch: 7| Step: 9
Training loss: 4.70380201567944
Validation loss: 3.4658314005111626
Epoch: 7| Step: 10
Training loss: 4.023721214587279
Validation loss: 3.4224660716268276
Epoch: 7| Step: 11
Training loss: 4.725506322465574
Validation loss: 3.40074092258096
Epoch: 7| Step: 12
Training loss: 4.231057081233743
Validation loss: 3.4030323860907212
Epoch: 7| Step: 13
Training loss: 3.905032036682866
Validation loss: 3.3785460551576807
Epoch: 7| Step: 14
Training loss: 4.143726957553415
Validation loss: 3.310379799366022
Epoch: 7| Step: 15
Training loss: 2.9346049225808475
Validation loss: 3.301799549340266
Epoch: 15| Step: 0
Training loss: 4.1401564998808755
Validation loss: 3.2703743230851505
Epoch: 7| Step: 1
Training loss: 4.101687126082807
Validation loss: 3.2769741036352547
Epoch: 7| Step: 2
Training loss: 3.635183565523715
Validation loss: 3.2185260956876816
Epoch: 7| Step: 3
Training loss: 3.413960540436177
Validation loss: 3.1748419651302067
Epoch: 7| Step: 4
Training loss: 3.6958571308893693
Validation loss: 3.1799789382193064
Epoch: 7| Step: 5
Training loss: 3.0504168929049715
Validation loss: 3.11351541167052
Epoch: 7| Step: 6
Training loss: 3.3975514627114127
Validation loss: 3.1101169967005338
Epoch: 7| Step: 7
Training loss: 3.6895377784011796
Validation loss: 3.0609236557111847
Epoch: 7| Step: 8
Training loss: 3.078138787098327
Validation loss: 3.0919255914826507
Epoch: 7| Step: 9
Training loss: 3.8008595347796
Validation loss: 3.0596045394331086
Epoch: 7| Step: 10
Training loss: 3.3146707871315044
Validation loss: 3.0316368851957143
Epoch: 7| Step: 11
Training loss: 3.4835369658991198
Validation loss: 2.9733578880779423
Epoch: 7| Step: 12
Training loss: 3.7988926428486565
Validation loss: 2.9362618727274206
Epoch: 7| Step: 13
Training loss: 3.852582753831781
Validation loss: 2.9522876701774305
Epoch: 7| Step: 14
Training loss: 3.8154038173793414
Validation loss: 2.9004901358638735
Epoch: 7| Step: 15
Training loss: 3.8315695699364114
Validation loss: 2.881702697372519
Epoch: 16| Step: 0
Training loss: 3.4083052567970995
Validation loss: 2.870528174969848
Epoch: 7| Step: 1
Training loss: 3.9957011487707743
Validation loss: 2.830279761122551
Epoch: 7| Step: 2
Training loss: 3.6043131951036718
Validation loss: 2.856413462071158
Epoch: 7| Step: 3
Training loss: 2.8820624474160277
Validation loss: 2.8232642007274142
Epoch: 7| Step: 4
Training loss: 3.0251215697521623
Validation loss: 2.7514979687698164
Epoch: 7| Step: 5
Training loss: 3.229451878575551
Validation loss: 2.7379020456582555
Epoch: 7| Step: 6
Training loss: 3.267927170538438
Validation loss: 2.730608590199126
Epoch: 7| Step: 7
Training loss: 3.10778614644825
Validation loss: 2.7043995951262056
Epoch: 7| Step: 8
Training loss: 2.59766992551029
Validation loss: 2.7147348633178985
Epoch: 7| Step: 9
Training loss: 2.774358891841895
Validation loss: 2.6254739137902687
Epoch: 7| Step: 10
Training loss: 3.2883611733259763
Validation loss: 2.654325604282162
Epoch: 7| Step: 11
Training loss: 3.6054038127701045
Validation loss: 2.599790838896635
Epoch: 7| Step: 12
Training loss: 3.262844380293398
Validation loss: 2.6069596456367936
Epoch: 7| Step: 13
Training loss: 3.212181574258127
Validation loss: 2.560903476972093
Epoch: 7| Step: 14
Training loss: 3.587566064100036
Validation loss: 2.5604533475478597
Epoch: 7| Step: 15
Training loss: 3.2933698344299276
Validation loss: 2.5222069314473017
Epoch: 17| Step: 0
Training loss: 3.0760129857279046
Validation loss: 2.4934474839746255
Epoch: 7| Step: 1
Training loss: 2.940606929387058
Validation loss: 2.4682954530291497
Epoch: 7| Step: 2
Training loss: 3.032388060564399
Validation loss: 2.4807100935383963
Epoch: 7| Step: 3
Training loss: 3.1451718643364117
Validation loss: 2.4705776534544532
Epoch: 7| Step: 4
Training loss: 3.1422225912250674
Validation loss: 2.3854817064035885
Epoch: 7| Step: 5
Training loss: 3.237816789953937
Validation loss: 2.403987615655584
Epoch: 7| Step: 6
Training loss: 2.9256282099180777
Validation loss: 2.4330422308752007
Epoch: 7| Step: 7
Training loss: 3.1085518365581177
Validation loss: 2.375666294454101
Epoch: 7| Step: 8
Training loss: 3.0448310451895355
Validation loss: 2.34601076098446
Epoch: 7| Step: 9
Training loss: 2.968922499614266
Validation loss: 2.371106285416275
Epoch: 7| Step: 10
Training loss: 3.052870734568508
Validation loss: 2.3566590419256546
Epoch: 7| Step: 11
Training loss: 2.028968468712017
Validation loss: 2.30088877332194
Epoch: 7| Step: 12
Training loss: 2.943278533696141
Validation loss: 2.3274969935908576
Epoch: 7| Step: 13
Training loss: 3.083030393685121
Validation loss: 2.2965116423505196
Epoch: 7| Step: 14
Training loss: 3.2215156895019956
Validation loss: 2.262501275162543
Epoch: 7| Step: 15
Training loss: 2.6075313245111524
Validation loss: 2.3053567630365466
Epoch: 18| Step: 0
Training loss: 3.212170737644299
Validation loss: 2.2782281913041476
Epoch: 7| Step: 1
Training loss: 3.3386045421247914
Validation loss: 2.2305579513560847
Epoch: 7| Step: 2
Training loss: 2.498384430532225
Validation loss: 2.217979751809107
Epoch: 7| Step: 3
Training loss: 2.437398761700937
Validation loss: 2.239824015906872
Epoch: 7| Step: 4
Training loss: 3.037712523007489
Validation loss: 2.166278813652076
Epoch: 7| Step: 5
Training loss: 2.686952757010849
Validation loss: 2.1855147409765907
Epoch: 7| Step: 6
Training loss: 2.5272551672857473
Validation loss: 2.155671991373194
Epoch: 7| Step: 7
Training loss: 3.0536664344074897
Validation loss: 2.1688658690389206
Epoch: 7| Step: 8
Training loss: 2.5591125428233785
Validation loss: 2.2224612916525284
Epoch: 7| Step: 9
Training loss: 2.3466699340465227
Validation loss: 2.189269068883066
Epoch: 7| Step: 10
Training loss: 2.8016866712822344
Validation loss: 2.1427647311926914
Epoch: 7| Step: 11
Training loss: 2.6859263758847867
Validation loss: 2.1328219355358153
Epoch: 7| Step: 12
Training loss: 2.5164245851139504
Validation loss: 2.141790622281168
Epoch: 7| Step: 13
Training loss: 2.9266533767757617
Validation loss: 2.156948622031016
Epoch: 7| Step: 14
Training loss: 2.882284141855145
Validation loss: 2.1366906275154456
Epoch: 7| Step: 15
Training loss: 2.1744137204004717
Validation loss: 2.175529026241755
Epoch: 19| Step: 0
Training loss: 2.083389383197842
Validation loss: 2.1536880547358654
Epoch: 7| Step: 1
Training loss: 2.2262983198881865
Validation loss: 2.1847683121306916
Epoch: 7| Step: 2
Training loss: 3.276791113362332
Validation loss: 2.137798252004981
Epoch: 7| Step: 3
Training loss: 2.3978597833651434
Validation loss: 2.1575515893788437
Epoch: 7| Step: 4
Training loss: 3.0753412196947694
Validation loss: 2.1014525905099815
Epoch: 7| Step: 5
Training loss: 1.9140824297431818
Validation loss: 2.114271324125236
Epoch: 7| Step: 6
Training loss: 2.8896033878817717
Validation loss: 2.164524653335263
Epoch: 7| Step: 7
Training loss: 2.7406752021741387
Validation loss: 2.132519958346588
Epoch: 7| Step: 8
Training loss: 2.3286560272241674
Validation loss: 2.130363486460291
Epoch: 7| Step: 9
Training loss: 3.259437942262882
Validation loss: 2.1232316003742215
Epoch: 7| Step: 10
Training loss: 3.025695588396185
Validation loss: 2.154525528466668
Epoch: 7| Step: 11
Training loss: 3.4693646960276414
Validation loss: 2.1588212031819194
Epoch: 7| Step: 12
Training loss: 1.7521481952915825
Validation loss: 2.1638270508202235
Epoch: 7| Step: 13
Training loss: 1.721739025251036
Validation loss: 2.127260114322907
Epoch: 7| Step: 14
Training loss: 2.8179501601902333
Validation loss: 2.13721131391105
Epoch: 7| Step: 15
Training loss: 2.868214103825888
Validation loss: 2.1384377747310346
Epoch: 20| Step: 0
Training loss: 2.5116175605953464
Validation loss: 2.1429431334244384
Epoch: 7| Step: 1
Training loss: 2.428481609222225
Validation loss: 2.1472442124717475
Epoch: 7| Step: 2
Training loss: 2.58753194374601
Validation loss: 2.127076408329802
Epoch: 7| Step: 3
Training loss: 3.5291269047166223
Validation loss: 2.1575051439853636
Epoch: 7| Step: 4
Training loss: 2.371301079472887
Validation loss: 2.13349226765241
Epoch: 7| Step: 5
Training loss: 2.550960516946704
Validation loss: 2.147213687755081
Epoch: 7| Step: 6
Training loss: 2.4950781055831106
Validation loss: 2.1577993562770907
Epoch: 7| Step: 7
Training loss: 2.7280600733421227
Validation loss: 2.120604436711497
Epoch: 7| Step: 8
Training loss: 2.754861868772303
Validation loss: 2.1189457208074183
Epoch: 7| Step: 9
Training loss: 2.878270486639971
Validation loss: 2.1763828052808827
Epoch: 7| Step: 10
Training loss: 2.648548945329896
Validation loss: 2.1669266939860474
Epoch: 7| Step: 11
Training loss: 2.334942762175707
Validation loss: 2.1536302115376675
Epoch: 7| Step: 12
Training loss: 2.3632566340401886
Validation loss: 2.156007463470987
Epoch: 7| Step: 13
Training loss: 2.664570242636117
Validation loss: 2.1287687880552872
Epoch: 7| Step: 14
Training loss: 2.591907812186336
Validation loss: 2.0881697476692307
Epoch: 7| Step: 15
Training loss: 2.1154523545203294
Validation loss: 2.155856617766196
Epoch: 21| Step: 0
Training loss: 2.407051151221841
Validation loss: 2.12895246153204
Epoch: 7| Step: 1
Training loss: 2.7367681003904845
Validation loss: 2.185768386185418
Epoch: 7| Step: 2
Training loss: 3.4697442906509526
Validation loss: 2.2076953752112143
Epoch: 7| Step: 3
Training loss: 2.681292973242912
Validation loss: 2.1549887800154655
Epoch: 7| Step: 4
Training loss: 2.671091600647625
Validation loss: 2.12954697446888
Epoch: 7| Step: 5
Training loss: 2.9364068452494174
Validation loss: 2.1800768252700355
Epoch: 7| Step: 6
Training loss: 3.149322763938298
Validation loss: 2.1604845459767086
Epoch: 7| Step: 7
Training loss: 2.3186600577706464
Validation loss: 2.1965766078016564
Epoch: 7| Step: 8
Training loss: 2.636460309961714
Validation loss: 2.1819100852435467
Epoch: 7| Step: 9
Training loss: 2.15136009488731
Validation loss: 2.1598816268535495
Epoch: 7| Step: 10
Training loss: 2.1978725290311147
Validation loss: 2.0907157279227353
Epoch: 7| Step: 11
Training loss: 1.9563356179705926
Validation loss: 2.1589534611234846
Epoch: 7| Step: 12
Training loss: 2.480510753366068
Validation loss: 2.1501010623265335
Epoch: 7| Step: 13
Training loss: 2.4993985406253985
Validation loss: 2.119345243109413
Epoch: 7| Step: 14
Training loss: 2.1436526751877505
Validation loss: 2.2044533990843944
Epoch: 7| Step: 15
Training loss: 2.761791007082719
Validation loss: 2.1716722775756376
Epoch: 22| Step: 0
Training loss: 3.1503701915894227
Validation loss: 2.1642034956306024
Epoch: 7| Step: 1
Training loss: 2.348123564203152
Validation loss: 2.1615461904290045
Epoch: 7| Step: 2
Training loss: 2.766319920362508
Validation loss: 2.1397620482847457
Epoch: 7| Step: 3
Training loss: 2.2658490300403544
Validation loss: 2.143584309910494
Epoch: 7| Step: 4
Training loss: 3.302679655861392
Validation loss: 2.168186288674676
Epoch: 7| Step: 5
Training loss: 2.4015369580212975
Validation loss: 2.1773754754612167
Epoch: 7| Step: 6
Training loss: 2.7507617068930434
Validation loss: 2.1724604024280256
Epoch: 7| Step: 7
Training loss: 1.8274189164194528
Validation loss: 2.182473511860042
Epoch: 7| Step: 8
Training loss: 2.276025406861585
Validation loss: 2.2029956556726193
Epoch: 7| Step: 9
Training loss: 2.2883134990072427
Validation loss: 2.1948052850596493
Epoch: 7| Step: 10
Training loss: 3.0592740253622033
Validation loss: 2.1399602207800323
Epoch: 7| Step: 11
Training loss: 2.223051173518182
Validation loss: 2.1763419855600494
Epoch: 7| Step: 12
Training loss: 2.445291147352111
Validation loss: 2.1912940677397947
Epoch: 7| Step: 13
Training loss: 2.721889218141054
Validation loss: 2.2058862556467487
Epoch: 7| Step: 14
Training loss: 2.8819744266855207
Validation loss: 2.2011527010473455
Epoch: 7| Step: 15
Training loss: 2.3653370892424297
Validation loss: 2.2190097590752047
Epoch: 23| Step: 0
Training loss: 2.5870065006547494
Validation loss: 2.161737546012019
Epoch: 7| Step: 1
Training loss: 2.5572103473193226
Validation loss: 2.1767249306056633
Epoch: 7| Step: 2
Training loss: 2.4453468198470207
Validation loss: 2.120225711157023
Epoch: 7| Step: 3
Training loss: 2.9356998852962337
Validation loss: 2.19215212610046
Epoch: 7| Step: 4
Training loss: 2.596920419747957
Validation loss: 2.212114524092964
Epoch: 7| Step: 5
Training loss: 2.368189231502849
Validation loss: 2.220759397689204
Epoch: 7| Step: 6
Training loss: 2.754980432402808
Validation loss: 2.124217420893365
Epoch: 7| Step: 7
Training loss: 2.7756107036357256
Validation loss: 2.189442062730504
Epoch: 7| Step: 8
Training loss: 2.448108573587726
Validation loss: 2.149078632970421
Epoch: 7| Step: 9
Training loss: 3.1691299361184218
Validation loss: 2.175737613909908
Epoch: 7| Step: 10
Training loss: 2.4234930724431987
Validation loss: 2.2192550150903596
Epoch: 7| Step: 11
Training loss: 2.3286077011787043
Validation loss: 2.1693711185309437
Epoch: 7| Step: 12
Training loss: 1.583466055811885
Validation loss: 2.139846990487801
Epoch: 7| Step: 13
Training loss: 2.6727009885199737
Validation loss: 2.138735572758774
Epoch: 7| Step: 14
Training loss: 2.8247161891462396
Validation loss: 2.1574364616463857
Epoch: 7| Step: 15
Training loss: 3.046496088969257
Validation loss: 2.216790277455924
Epoch: 24| Step: 0
Training loss: 2.8880262085314516
Validation loss: 2.2040756626062876
Epoch: 7| Step: 1
Training loss: 2.2229267619462973
Validation loss: 2.17408693264239
Epoch: 7| Step: 2
Training loss: 3.2755791479641636
Validation loss: 2.147566531261719
Epoch: 7| Step: 3
Training loss: 2.3657612044556755
Validation loss: 2.1609139771155697
Epoch: 7| Step: 4
Training loss: 2.9555573311261014
Validation loss: 2.217533174331376
Epoch: 7| Step: 5
Training loss: 2.5362881101374333
Validation loss: 2.225512218100154
Epoch: 7| Step: 6
Training loss: 2.76351451627985
Validation loss: 2.1696615508715498
Epoch: 7| Step: 7
Training loss: 2.4712922731989337
Validation loss: 2.141108430033907
Epoch: 7| Step: 8
Training loss: 2.6412328438082056
Validation loss: 2.1425160942768486
Epoch: 7| Step: 9
Training loss: 2.279898151498719
Validation loss: 2.198860820346559
Epoch: 7| Step: 10
Training loss: 2.5824351030967927
Validation loss: 2.1863608985357117
Epoch: 7| Step: 11
Training loss: 2.6530025602199654
Validation loss: 2.1758946316076666
Epoch: 7| Step: 12
Training loss: 2.5282428916726363
Validation loss: 2.13525484004968
Epoch: 7| Step: 13
Training loss: 2.3881186517602626
Validation loss: 2.170217677229473
Epoch: 7| Step: 14
Training loss: 2.8321423738974336
Validation loss: 2.207838359304418
Epoch: 7| Step: 15
Training loss: 2.0138029394407355
Validation loss: 2.1777987206488816
Epoch: 25| Step: 0
Training loss: 2.9981400764218273
Validation loss: 2.122118264796422
Epoch: 7| Step: 1
Training loss: 2.4302471534407033
Validation loss: 2.148937752847524
Epoch: 7| Step: 2
Training loss: 2.3631556453902056
Validation loss: 2.170395838822562
Epoch: 7| Step: 3
Training loss: 2.5850183057598026
Validation loss: 2.1586285987725327
Epoch: 7| Step: 4
Training loss: 2.719920202515239
Validation loss: 2.1277649620960455
Epoch: 7| Step: 5
Training loss: 2.4430433963976426
Validation loss: 2.1795804243750996
Epoch: 7| Step: 6
Training loss: 2.3116971246005025
Validation loss: 2.2209196347710005
Epoch: 7| Step: 7
Training loss: 2.4011776458785072
Validation loss: 2.142760883331549
Epoch: 7| Step: 8
Training loss: 2.7547266826991224
Validation loss: 2.086305582656542
Epoch: 7| Step: 9
Training loss: 2.977447138011985
Validation loss: 2.1380113896515627
Epoch: 7| Step: 10
Training loss: 2.663075731381708
Validation loss: 2.1677285594605022
Epoch: 7| Step: 11
Training loss: 2.014821210175277
Validation loss: 2.1960814977773393
Epoch: 7| Step: 12
Training loss: 2.8967609337794347
Validation loss: 2.1749036647181663
Epoch: 7| Step: 13
Training loss: 2.5182460612085706
Validation loss: 2.1563205349687005
Epoch: 7| Step: 14
Training loss: 2.5956227539111567
Validation loss: 2.1428623667724116
Epoch: 7| Step: 15
Training loss: 2.7501607327738697
Validation loss: 2.176211110404768
Epoch: 26| Step: 0
Training loss: 2.840435066210183
Validation loss: 2.172866798845418
Epoch: 7| Step: 1
Training loss: 2.488535821608945
Validation loss: 2.1560144182085073
Epoch: 7| Step: 2
Training loss: 2.504214549016782
Validation loss: 2.172160220861077
Epoch: 7| Step: 3
Training loss: 2.3275583076753765
Validation loss: 2.1516227562794823
Epoch: 7| Step: 4
Training loss: 2.3990688424960123
Validation loss: 2.1396337901167723
Epoch: 7| Step: 5
Training loss: 3.272918587207939
Validation loss: 2.187304247094052
Epoch: 7| Step: 6
Training loss: 2.1978284869350926
Validation loss: 2.1443859061876243
Epoch: 7| Step: 7
Training loss: 2.906867341134204
Validation loss: 2.141522811136443
Epoch: 7| Step: 8
Training loss: 2.0486621771300944
Validation loss: 2.1778579391864463
Epoch: 7| Step: 9
Training loss: 3.00450416041299
Validation loss: 2.146137167968284
Epoch: 7| Step: 10
Training loss: 2.5801103750172643
Validation loss: 2.143863622155573
Epoch: 7| Step: 11
Training loss: 2.656380953085197
Validation loss: 2.136513796425567
Epoch: 7| Step: 12
Training loss: 2.4584218564232225
Validation loss: 2.2239162823617065
Epoch: 7| Step: 13
Training loss: 2.744760029349265
Validation loss: 2.1804414910910044
Epoch: 7| Step: 14
Training loss: 2.443252231801024
Validation loss: 2.2131971080352457
Epoch: 7| Step: 15
Training loss: 2.453595936272648
Validation loss: 2.1680252244610903
Epoch: 27| Step: 0
Training loss: 2.1067794089187974
Validation loss: 2.1648812277233085
Epoch: 7| Step: 1
Training loss: 2.6962214167480947
Validation loss: 2.2031393649937923
Epoch: 7| Step: 2
Training loss: 2.7126555261983962
Validation loss: 2.1632541463293484
Epoch: 7| Step: 3
Training loss: 2.7434202721664587
Validation loss: 2.1890960412107523
Epoch: 7| Step: 4
Training loss: 2.8536347684850525
Validation loss: 2.1861201707392612
Epoch: 7| Step: 5
Training loss: 2.694001340535828
Validation loss: 2.163690374286861
Epoch: 7| Step: 6
Training loss: 1.9907013978775105
Validation loss: 2.176673900785422
Epoch: 7| Step: 7
Training loss: 2.705249282434938
Validation loss: 2.140408062544485
Epoch: 7| Step: 8
Training loss: 2.0230333079199694
Validation loss: 2.204719038692954
Epoch: 7| Step: 9
Training loss: 2.776981635972907
Validation loss: 2.16042181808818
Epoch: 7| Step: 10
Training loss: 2.2950019725051414
Validation loss: 2.140784246698494
Epoch: 7| Step: 11
Training loss: 2.7450596041018893
Validation loss: 2.2279276531424204
Epoch: 7| Step: 12
Training loss: 3.1591442171018786
Validation loss: 2.1514182788620713
Epoch: 7| Step: 13
Training loss: 2.4902681714924775
Validation loss: 2.1817435242702325
Epoch: 7| Step: 14
Training loss: 2.6223103503955256
Validation loss: 2.1568522916773554
Epoch: 7| Step: 15
Training loss: 2.999510725177375
Validation loss: 2.1778081420760382
Epoch: 28| Step: 0
Training loss: 2.7742252574297535
Validation loss: 2.199978472803043
Epoch: 7| Step: 1
Training loss: 2.994295577825485
Validation loss: 2.1640382800381612
Epoch: 7| Step: 2
Training loss: 2.6938073419897823
Validation loss: 2.1509003186873943
Epoch: 7| Step: 3
Training loss: 2.412885162757396
Validation loss: 2.211978504887838
Epoch: 7| Step: 4
Training loss: 2.439769959471417
Validation loss: 2.143062630467238
Epoch: 7| Step: 5
Training loss: 2.122951417687836
Validation loss: 2.109330793047542
Epoch: 7| Step: 6
Training loss: 2.699221576518057
Validation loss: 2.2045777588038957
Epoch: 7| Step: 7
Training loss: 2.327935947192482
Validation loss: 2.183222013019619
Epoch: 7| Step: 8
Training loss: 1.9594787399840143
Validation loss: 2.160409331822976
Epoch: 7| Step: 9
Training loss: 2.6159995044509463
Validation loss: 2.1258200476652944
Epoch: 7| Step: 10
Training loss: 2.6345124232603245
Validation loss: 2.172292769351411
Epoch: 7| Step: 11
Training loss: 3.073706694519356
Validation loss: 2.1240474474418742
Epoch: 7| Step: 12
Training loss: 2.666787730886211
Validation loss: 2.132239052644637
Epoch: 7| Step: 13
Training loss: 2.4303807683192593
Validation loss: 2.1755355683594217
Epoch: 7| Step: 14
Training loss: 2.6534133125436448
Validation loss: 2.2035302383866187
Epoch: 7| Step: 15
Training loss: 2.191419088016651
Validation loss: 2.1713398322841266
Epoch: 29| Step: 0
Training loss: 2.854252296518333
Validation loss: 2.1650766365000456
Epoch: 7| Step: 1
Training loss: 2.4048281345302867
Validation loss: 2.205809100013072
Epoch: 7| Step: 2
Training loss: 2.9670913078059162
Validation loss: 2.2045158423792666
Epoch: 7| Step: 3
Training loss: 2.1707208434653977
Validation loss: 2.221029871411756
Epoch: 7| Step: 4
Training loss: 2.5568366387211627
Validation loss: 2.1096045719621057
Epoch: 7| Step: 5
Training loss: 2.9854625697487975
Validation loss: 2.156374744388836
Epoch: 7| Step: 6
Training loss: 2.9827988541831982
Validation loss: 2.164847339172309
Epoch: 7| Step: 7
Training loss: 2.4491192616481294
Validation loss: 2.1657831512710044
Epoch: 7| Step: 8
Training loss: 1.7609224063256885
Validation loss: 2.179452426741512
Epoch: 7| Step: 9
Training loss: 2.4790111674633906
Validation loss: 2.2431636035016824
Epoch: 7| Step: 10
Training loss: 2.7269673306529967
Validation loss: 2.150035468809559
Epoch: 7| Step: 11
Training loss: 2.35924949059537
Validation loss: 2.12558862733192
Epoch: 7| Step: 12
Training loss: 2.5748431037583956
Validation loss: 2.158337358361762
Epoch: 7| Step: 13
Training loss: 3.6064681861497077
Validation loss: 2.107831371511433
Epoch: 7| Step: 14
Training loss: 2.0505559307024437
Validation loss: 2.1761956669894884
Epoch: 7| Step: 15
Training loss: 2.021239747804062
Validation loss: 2.114727857823602
Epoch: 30| Step: 0
Training loss: 3.139184877811837
Validation loss: 2.141147580253914
Epoch: 7| Step: 1
Training loss: 2.783364681440457
Validation loss: 2.124612873806781
Epoch: 7| Step: 2
Training loss: 2.3095231355844974
Validation loss: 2.1449983519032845
Epoch: 7| Step: 3
Training loss: 2.506695364862208
Validation loss: 2.2057072741297756
Epoch: 7| Step: 4
Training loss: 2.3801213310473464
Validation loss: 2.1799461759861924
Epoch: 7| Step: 5
Training loss: 3.100910877412203
Validation loss: 2.1675254907007284
Epoch: 7| Step: 6
Training loss: 2.6035901664943193
Validation loss: 2.1950195789682105
Epoch: 7| Step: 7
Training loss: 3.0851818076896254
Validation loss: 2.141313803357317
Epoch: 7| Step: 8
Training loss: 2.0986017976637377
Validation loss: 2.1739699794960683
Epoch: 7| Step: 9
Training loss: 2.409222636866352
Validation loss: 2.146859073475994
Epoch: 7| Step: 10
Training loss: 2.718478923744673
Validation loss: 2.1603058990236703
Epoch: 7| Step: 11
Training loss: 2.868980740843864
Validation loss: 2.1902982486746962
Epoch: 7| Step: 12
Training loss: 2.4359695324801143
Validation loss: 2.124373280816057
Epoch: 7| Step: 13
Training loss: 2.3997192933909486
Validation loss: 2.182145085160218
Epoch: 7| Step: 14
Training loss: 2.4935690180755965
Validation loss: 2.156621469945827
Epoch: 7| Step: 15
Training loss: 1.8287912442607654
Validation loss: 2.159742272811691
Epoch: 31| Step: 0
Training loss: 2.2784638831746165
Validation loss: 2.1737139200946705
Epoch: 7| Step: 1
Training loss: 2.8233028208747597
Validation loss: 2.192493163046977
Epoch: 7| Step: 2
Training loss: 2.4961425585444643
Validation loss: 2.1905509588134326
Epoch: 7| Step: 3
Training loss: 2.7479214182116825
Validation loss: 2.1288027707443784
Epoch: 7| Step: 4
Training loss: 2.474983747024345
Validation loss: 2.183019399543685
Epoch: 7| Step: 5
Training loss: 2.6935654437575276
Validation loss: 2.1190747303007473
Epoch: 7| Step: 6
Training loss: 2.2605324172049865
Validation loss: 2.2004145517121474
Epoch: 7| Step: 7
Training loss: 2.9611558657523074
Validation loss: 2.1786285631153826
Epoch: 7| Step: 8
Training loss: 2.553903161832075
Validation loss: 2.1369085523452003
Epoch: 7| Step: 9
Training loss: 2.6246388504819067
Validation loss: 2.16249561227021
Epoch: 7| Step: 10
Training loss: 2.948549631773268
Validation loss: 2.1402306632055663
Epoch: 7| Step: 11
Training loss: 2.9747456289583543
Validation loss: 2.157476277776786
Epoch: 7| Step: 12
Training loss: 2.1233867804818667
Validation loss: 2.115983804129813
Epoch: 7| Step: 13
Training loss: 2.384338290630648
Validation loss: 2.2444089580512503
Epoch: 7| Step: 14
Training loss: 2.410681372120124
Validation loss: 2.1173135914729975
Epoch: 7| Step: 15
Training loss: 2.3917847855627015
Validation loss: 2.1932721975967806
Epoch: 32| Step: 0
Training loss: 1.8287957420025882
Validation loss: 2.163331063712885
Epoch: 7| Step: 1
Training loss: 2.457075691730928
Validation loss: 2.1388176195953665
Epoch: 7| Step: 2
Training loss: 2.435418634166418
Validation loss: 2.121848160264669
Epoch: 7| Step: 3
Training loss: 3.587225256843367
Validation loss: 2.1346619779779106
Epoch: 7| Step: 4
Training loss: 2.81099592893476
Validation loss: 2.1757086485918578
Epoch: 7| Step: 5
Training loss: 2.648308854931233
Validation loss: 2.1897223916682247
Epoch: 7| Step: 6
Training loss: 2.4302754074195385
Validation loss: 2.151777943155103
Epoch: 7| Step: 7
Training loss: 2.916565666266496
Validation loss: 2.184966890574569
Epoch: 7| Step: 8
Training loss: 2.7750877950715767
Validation loss: 2.1524552891589277
Epoch: 7| Step: 9
Training loss: 2.157860196171361
Validation loss: 2.1914541483024714
Epoch: 7| Step: 10
Training loss: 2.0863506465196964
Validation loss: 2.0915149841710523
Epoch: 7| Step: 11
Training loss: 2.2585328744755233
Validation loss: 2.1481676547195225
Epoch: 7| Step: 12
Training loss: 1.8028946595523496
Validation loss: 2.1721538468508808
Epoch: 7| Step: 13
Training loss: 3.1753513472187125
Validation loss: 2.1718088045957393
Epoch: 7| Step: 14
Training loss: 2.8691595711806377
Validation loss: 2.1529925095940206
Epoch: 7| Step: 15
Training loss: 2.6953300143792362
Validation loss: 2.177528013165033
Epoch: 33| Step: 0
Training loss: 3.123550994627755
Validation loss: 2.127091804213039
Epoch: 7| Step: 1
Training loss: 2.7045571827470267
Validation loss: 2.1727921212015366
Epoch: 7| Step: 2
Training loss: 2.768112873973285
Validation loss: 2.185744145316585
Epoch: 7| Step: 3
Training loss: 2.342088746374128
Validation loss: 2.2040236486116656
Epoch: 7| Step: 4
Training loss: 1.678889595186994
Validation loss: 2.14401922631977
Epoch: 7| Step: 5
Training loss: 3.064831216219114
Validation loss: 2.153998087381287
Epoch: 7| Step: 6
Training loss: 2.073991838532946
Validation loss: 2.162314189125903
Epoch: 7| Step: 7
Training loss: 2.0547652429297725
Validation loss: 2.1660204580797906
Epoch: 7| Step: 8
Training loss: 2.5336776206655465
Validation loss: 2.1402854420822273
Epoch: 7| Step: 9
Training loss: 2.900018389413341
Validation loss: 2.210153594187484
Epoch: 7| Step: 10
Training loss: 2.5606588867269267
Validation loss: 2.1818378669642504
Epoch: 7| Step: 11
Training loss: 2.529968690942328
Validation loss: 2.2062840359672418
Epoch: 7| Step: 12
Training loss: 2.9716721458477258
Validation loss: 2.128546227114023
Epoch: 7| Step: 13
Training loss: 2.640627889236585
Validation loss: 2.125797223905223
Epoch: 7| Step: 14
Training loss: 2.30440403116354
Validation loss: 2.168837904134057
Epoch: 7| Step: 15
Training loss: 2.8860498461208937
Validation loss: 2.162948332722953
Epoch: 34| Step: 0
Training loss: 2.413506798841511
Validation loss: 2.188784058092306
Epoch: 7| Step: 1
Training loss: 1.468736526752116
Validation loss: 2.180119960316652
Epoch: 7| Step: 2
Training loss: 2.821879957834749
Validation loss: 2.1631728938010135
Epoch: 7| Step: 3
Training loss: 3.4601773014836446
Validation loss: 2.1468676083152305
Epoch: 7| Step: 4
Training loss: 2.2492331151579736
Validation loss: 2.150113360724132
Epoch: 7| Step: 5
Training loss: 2.880145237227528
Validation loss: 2.1877765719256406
Epoch: 7| Step: 6
Training loss: 2.2173353171455346
Validation loss: 2.1648703288595503
Epoch: 7| Step: 7
Training loss: 3.0515746820053344
Validation loss: 2.1533711375670075
Epoch: 7| Step: 8
Training loss: 2.7981399147875097
Validation loss: 2.1451390273222475
Epoch: 7| Step: 9
Training loss: 2.852120457238806
Validation loss: 2.1502623841280064
Epoch: 7| Step: 10
Training loss: 2.157543402596605
Validation loss: 2.161000547581686
Epoch: 7| Step: 11
Training loss: 2.8243533111637675
Validation loss: 2.2087535989246025
Epoch: 7| Step: 12
Training loss: 2.6379075372787297
Validation loss: 2.1817175885444455
Epoch: 7| Step: 13
Training loss: 2.162730002988449
Validation loss: 2.151804832312696
Epoch: 7| Step: 14
Training loss: 2.8084420493464615
Validation loss: 2.171098782046603
Epoch: 7| Step: 15
Training loss: 2.116249017236417
Validation loss: 2.198987153205829
Epoch: 35| Step: 0
Training loss: 2.0143123166956745
Validation loss: 2.098046245212743
Epoch: 7| Step: 1
Training loss: 2.0759286388120235
Validation loss: 2.1849249078677433
Epoch: 7| Step: 2
Training loss: 2.6350336412167943
Validation loss: 2.159551179560463
Epoch: 7| Step: 3
Training loss: 2.767842583683942
Validation loss: 2.2533883907304184
Epoch: 7| Step: 4
Training loss: 3.1602318451731644
Validation loss: 2.222883902464019
Epoch: 7| Step: 5
Training loss: 2.3147171320614404
Validation loss: 2.10377716805221
Epoch: 7| Step: 6
Training loss: 2.87674245691044
Validation loss: 2.1747052377344347
Epoch: 7| Step: 7
Training loss: 2.1829605958871086
Validation loss: 2.1593186319628184
Epoch: 7| Step: 8
Training loss: 2.6417469598097973
Validation loss: 2.139698740067057
Epoch: 7| Step: 9
Training loss: 2.946202313211626
Validation loss: 2.193046186309626
Epoch: 7| Step: 10
Training loss: 2.270911002507812
Validation loss: 2.2030764239538025
Epoch: 7| Step: 11
Training loss: 2.359166723494112
Validation loss: 2.1194446151430038
Epoch: 7| Step: 12
Training loss: 2.7278136959503767
Validation loss: 2.152137124013999
Epoch: 7| Step: 13
Training loss: 2.6256609493301686
Validation loss: 2.1739479106582893
Epoch: 7| Step: 14
Training loss: 2.6087618352953297
Validation loss: 2.170834768962545
Epoch: 7| Step: 15
Training loss: 2.8040903459869178
Validation loss: 2.1161228820973186
Epoch: 36| Step: 0
Training loss: 3.1884750669981177
Validation loss: 2.149410704407029
Epoch: 7| Step: 1
Training loss: 2.034590688308441
Validation loss: 2.173528148503357
Epoch: 7| Step: 2
Training loss: 2.3622436263230258
Validation loss: 2.1534736221790745
Epoch: 7| Step: 3
Training loss: 2.585028635600581
Validation loss: 2.1260758391478576
Epoch: 7| Step: 4
Training loss: 2.4091425762557663
Validation loss: 2.1588985942818457
Epoch: 7| Step: 5
Training loss: 3.284114785961076
Validation loss: 2.1281237669745052
Epoch: 7| Step: 6
Training loss: 2.6679353279042193
Validation loss: 2.14839698577058
Epoch: 7| Step: 7
Training loss: 2.7584600363805074
Validation loss: 2.112336530832033
Epoch: 7| Step: 8
Training loss: 2.6601529338440693
Validation loss: 2.172041644798889
Epoch: 7| Step: 9
Training loss: 2.2876424192692224
Validation loss: 2.1086864488408414
Epoch: 7| Step: 10
Training loss: 2.685194756745371
Validation loss: 2.1209258687618933
Epoch: 7| Step: 11
Training loss: 2.285012682455915
Validation loss: 2.1882273903032976
Epoch: 7| Step: 12
Training loss: 2.2557523099254784
Validation loss: 2.1605727416804363
Epoch: 7| Step: 13
Training loss: 2.1057626947235195
Validation loss: 2.124879133279334
Epoch: 7| Step: 14
Training loss: 2.4548496088396665
Validation loss: 2.1564568182120736
Epoch: 7| Step: 15
Training loss: 2.4670081452110364
Validation loss: 2.1559363353207175
Epoch: 37| Step: 0
Training loss: 2.1402262295371095
Validation loss: 2.1444867108408565
Epoch: 7| Step: 1
Training loss: 2.2623119465776442
Validation loss: 2.1470563290620577
Epoch: 7| Step: 2
Training loss: 3.2135972935804538
Validation loss: 2.1753268401312793
Epoch: 7| Step: 3
Training loss: 3.0152272965266214
Validation loss: 2.129500685320684
Epoch: 7| Step: 4
Training loss: 2.406846207907366
Validation loss: 2.1549550749699087
Epoch: 7| Step: 5
Training loss: 2.7612159195709753
Validation loss: 2.141736061051814
Epoch: 7| Step: 6
Training loss: 3.1136112971521377
Validation loss: 2.136211989353222
Epoch: 7| Step: 7
Training loss: 2.8051303524525357
Validation loss: 2.1289690450403542
Epoch: 7| Step: 8
Training loss: 2.3894654497393444
Validation loss: 2.0762040902850614
Epoch: 7| Step: 9
Training loss: 2.8655528884184363
Validation loss: 2.1388976109856017
Epoch: 7| Step: 10
Training loss: 2.5741596591401685
Validation loss: 2.1122557386064065
Epoch: 7| Step: 11
Training loss: 2.0040858970857385
Validation loss: 2.1217441041250695
Epoch: 7| Step: 12
Training loss: 2.284718215562682
Validation loss: 2.1523816200477235
Epoch: 7| Step: 13
Training loss: 2.3487611224046665
Validation loss: 2.1827961142997587
Epoch: 7| Step: 14
Training loss: 2.2798762953812233
Validation loss: 2.143600995822434
Epoch: 7| Step: 15
Training loss: 2.6297265143398785
Validation loss: 2.1722500445256276
Epoch: 38| Step: 0
Training loss: 1.948700677371097
Validation loss: 2.162285273520236
Epoch: 7| Step: 1
Training loss: 2.5071437811626613
Validation loss: 2.133884299455627
Epoch: 7| Step: 2
Training loss: 2.948858337611404
Validation loss: 2.136561969119883
Epoch: 7| Step: 3
Training loss: 2.292007166679855
Validation loss: 2.1834105546607625
Epoch: 7| Step: 4
Training loss: 2.3993831596822033
Validation loss: 2.1861063032978985
Epoch: 7| Step: 5
Training loss: 2.37768794090087
Validation loss: 2.1616487554187227
Epoch: 7| Step: 6
Training loss: 2.3326484719601006
Validation loss: 2.1210894026383325
Epoch: 7| Step: 7
Training loss: 2.756470869907159
Validation loss: 2.1544458212975424
Epoch: 7| Step: 8
Training loss: 3.3217198878451
Validation loss: 2.155946692144831
Epoch: 7| Step: 9
Training loss: 2.344568236890909
Validation loss: 2.1509585611776267
Epoch: 7| Step: 10
Training loss: 3.0171443294081874
Validation loss: 2.14534268273795
Epoch: 7| Step: 11
Training loss: 2.0263720114776915
Validation loss: 2.158325120736189
Epoch: 7| Step: 12
Training loss: 2.883387893278153
Validation loss: 2.164346208669784
Epoch: 7| Step: 13
Training loss: 2.1744413513428427
Validation loss: 2.1593851702200855
Epoch: 7| Step: 14
Training loss: 2.6986833364319893
Validation loss: 2.1860071579528446
Epoch: 7| Step: 15
Training loss: 2.3875007909004538
Validation loss: 2.1921652946464962
Epoch: 39| Step: 0
Training loss: 2.3577152502633694
Validation loss: 2.1331962842066265
Epoch: 7| Step: 1
Training loss: 2.9666649171916935
Validation loss: 2.135184015660162
Epoch: 7| Step: 2
Training loss: 2.2932659002119022
Validation loss: 2.1114333943681816
Epoch: 7| Step: 3
Training loss: 2.739653805211912
Validation loss: 2.1460728347583573
Epoch: 7| Step: 4
Training loss: 2.6677790248987447
Validation loss: 2.156603460984108
Epoch: 7| Step: 5
Training loss: 2.4819306638686474
Validation loss: 2.110134962388058
Epoch: 7| Step: 6
Training loss: 2.472452598248536
Validation loss: 2.1273069248971916
Epoch: 7| Step: 7
Training loss: 2.997838513219612
Validation loss: 2.172777890197926
Epoch: 7| Step: 8
Training loss: 2.608049615739075
Validation loss: 2.1777437675006674
Epoch: 7| Step: 9
Training loss: 2.525792490411744
Validation loss: 2.1450495182461977
Epoch: 7| Step: 10
Training loss: 1.982967207744739
Validation loss: 2.158406787987608
Epoch: 7| Step: 11
Training loss: 2.0181816505663255
Validation loss: 2.1095439602279367
Epoch: 7| Step: 12
Training loss: 3.2062031168047502
Validation loss: 2.130132280391339
Epoch: 7| Step: 13
Training loss: 2.8022811044625295
Validation loss: 2.1822117713843783
Epoch: 7| Step: 14
Training loss: 2.5834792516372445
Validation loss: 2.139614011364339
Epoch: 7| Step: 15
Training loss: 2.121334504853474
Validation loss: 2.1971962108675394
Epoch: 40| Step: 0
Training loss: 2.320771451315692
Validation loss: 2.178307425603369
Epoch: 7| Step: 1
Training loss: 2.5675391914622887
Validation loss: 2.1644608729126302
Epoch: 7| Step: 2
Training loss: 2.4618239970757707
Validation loss: 2.164990420746036
Epoch: 7| Step: 3
Training loss: 2.214260340690263
Validation loss: 2.148456297362881
Epoch: 7| Step: 4
Training loss: 2.1277226228706336
Validation loss: 2.146800560152705
Epoch: 7| Step: 5
Training loss: 2.4157114662398236
Validation loss: 2.1586693037803477
Epoch: 7| Step: 6
Training loss: 2.922199741207751
Validation loss: 2.148764866247786
Epoch: 7| Step: 7
Training loss: 2.76151595389506
Validation loss: 2.1269293853988813
Epoch: 7| Step: 8
Training loss: 2.863862898427362
Validation loss: 2.174644641380049
Epoch: 7| Step: 9
Training loss: 2.55174211569512
Validation loss: 2.2036657606067536
Epoch: 7| Step: 10
Training loss: 3.2498471150751547
Validation loss: 2.1095146119510853
Epoch: 7| Step: 11
Training loss: 2.294134880018454
Validation loss: 2.162463859120195
Epoch: 7| Step: 12
Training loss: 2.7386383825040705
Validation loss: 2.1264039023343173
Epoch: 7| Step: 13
Training loss: 2.021435428340091
Validation loss: 2.1660816134594962
Epoch: 7| Step: 14
Training loss: 2.4600207362037603
Validation loss: 2.1255446752437117
Epoch: 7| Step: 15
Training loss: 2.622845719407865
Validation loss: 2.1413392801159588
Epoch: 41| Step: 0
Training loss: 1.7914490087025223
Validation loss: 2.1906080117467335
Epoch: 7| Step: 1
Training loss: 2.4967992319921737
Validation loss: 2.2127008475077243
Epoch: 7| Step: 2
Training loss: 2.4765397801846256
Validation loss: 2.1985462230244646
Epoch: 7| Step: 3
Training loss: 1.9174482645468547
Validation loss: 2.1388518117200657
Epoch: 7| Step: 4
Training loss: 3.335584849672944
Validation loss: 2.15031299626988
Epoch: 7| Step: 5
Training loss: 2.54821339644553
Validation loss: 2.122743391845789
Epoch: 7| Step: 6
Training loss: 2.9535262223825387
Validation loss: 2.09964895382115
Epoch: 7| Step: 7
Training loss: 2.1169278031310923
Validation loss: 2.1484167766486246
Epoch: 7| Step: 8
Training loss: 2.2463705671811267
Validation loss: 2.1631523288444536
Epoch: 7| Step: 9
Training loss: 3.0433018787286357
Validation loss: 2.1715077865585393
Epoch: 7| Step: 10
Training loss: 3.0321902360444795
Validation loss: 2.147763778455443
Epoch: 7| Step: 11
Training loss: 2.1059342190068366
Validation loss: 2.077496614023187
Epoch: 7| Step: 12
Training loss: 3.5198436602639323
Validation loss: 2.122323892866893
Epoch: 7| Step: 13
Training loss: 2.2626607511117487
Validation loss: 2.201069381241009
Epoch: 7| Step: 14
Training loss: 2.4846877375081933
Validation loss: 2.1379521665095416
Epoch: 7| Step: 15
Training loss: 2.5746629994427352
Validation loss: 2.1432765281308512
Epoch: 42| Step: 0
Training loss: 2.524646009049249
Validation loss: 2.112225678230084
Epoch: 7| Step: 1
Training loss: 2.041850672104915
Validation loss: 2.1414755547804085
Epoch: 7| Step: 2
Training loss: 2.3335429392628595
Validation loss: 2.101726233546308
Epoch: 7| Step: 3
Training loss: 2.985661733559857
Validation loss: 2.1524637372500517
Epoch: 7| Step: 4
Training loss: 2.794012469564914
Validation loss: 2.1492104618051573
Epoch: 7| Step: 5
Training loss: 2.5346312368660207
Validation loss: 2.1297467323130355
Epoch: 7| Step: 6
Training loss: 2.3940889282918927
Validation loss: 2.1903716729600147
Epoch: 7| Step: 7
Training loss: 2.3513653948229107
Validation loss: 2.1711419981158704
Epoch: 7| Step: 8
Training loss: 2.426079036107628
Validation loss: 2.1263401792409513
Epoch: 7| Step: 9
Training loss: 2.9228065352153165
Validation loss: 2.1320354016846332
Epoch: 7| Step: 10
Training loss: 1.9269868431335018
Validation loss: 2.136560968776408
Epoch: 7| Step: 11
Training loss: 2.9370631440972716
Validation loss: 2.100041387913492
Epoch: 7| Step: 12
Training loss: 3.2568986008776704
Validation loss: 2.1422443904495467
Epoch: 7| Step: 13
Training loss: 2.472751223893114
Validation loss: 2.1939056345667614
Epoch: 7| Step: 14
Training loss: 2.404954735058322
Validation loss: 2.1604398667119957
Epoch: 7| Step: 15
Training loss: 2.1824358495698775
Validation loss: 2.1387547540126617
Epoch: 43| Step: 0
Training loss: 3.0263408747141853
Validation loss: 2.1402556442809013
Epoch: 7| Step: 1
Training loss: 1.880139237787137
Validation loss: 2.153127068919687
Epoch: 7| Step: 2
Training loss: 2.888746981518108
Validation loss: 2.1131839422465215
Epoch: 7| Step: 3
Training loss: 2.7211346746144183
Validation loss: 2.153942184832744
Epoch: 7| Step: 4
Training loss: 1.938480129286706
Validation loss: 2.117154789362345
Epoch: 7| Step: 5
Training loss: 1.7327663074937423
Validation loss: 2.162026588331656
Epoch: 7| Step: 6
Training loss: 2.6340437820028377
Validation loss: 2.1304062175700587
Epoch: 7| Step: 7
Training loss: 2.57853260863972
Validation loss: 2.150845739530963
Epoch: 7| Step: 8
Training loss: 2.9174622858350747
Validation loss: 2.165747314599909
Epoch: 7| Step: 9
Training loss: 2.704398147557999
Validation loss: 2.130962731807018
Epoch: 7| Step: 10
Training loss: 2.7821130163683048
Validation loss: 2.1613627095164927
Epoch: 7| Step: 11
Training loss: 2.379201585699347
Validation loss: 2.140585443866427
Epoch: 7| Step: 12
Training loss: 2.649884394157353
Validation loss: 2.1120026375766052
Epoch: 7| Step: 13
Training loss: 2.66485136551609
Validation loss: 2.154363725835979
Epoch: 7| Step: 14
Training loss: 3.0529758506741915
Validation loss: 2.1422759799355786
Epoch: 7| Step: 15
Training loss: 2.275096065463211
Validation loss: 2.1182393305861322
Epoch: 44| Step: 0
Training loss: 2.3131311947054964
Validation loss: 2.1100019746418126
Epoch: 7| Step: 1
Training loss: 2.626156960699925
Validation loss: 2.1237468289110795
Epoch: 7| Step: 2
Training loss: 2.21994319585348
Validation loss: 2.142193423927847
Epoch: 7| Step: 3
Training loss: 2.7264346554577386
Validation loss: 2.13004985271558
Epoch: 7| Step: 4
Training loss: 2.3685015062420374
Validation loss: 2.1366858264702393
Epoch: 7| Step: 5
Training loss: 2.754052124488684
Validation loss: 2.1900656466674597
Epoch: 7| Step: 6
Training loss: 2.3795502891733205
Validation loss: 2.1790704627631943
Epoch: 7| Step: 7
Training loss: 3.2891518043844004
Validation loss: 2.1558051592896135
Epoch: 7| Step: 8
Training loss: 2.208258201712587
Validation loss: 2.163010923254575
Epoch: 7| Step: 9
Training loss: 3.138099983873851
Validation loss: 2.187260160192168
Epoch: 7| Step: 10
Training loss: 2.8650143570731634
Validation loss: 2.1698405951392377
Epoch: 7| Step: 11
Training loss: 2.376115837985367
Validation loss: 2.126524851136152
Epoch: 7| Step: 12
Training loss: 1.7465286566662794
Validation loss: 2.162646723564445
Epoch: 7| Step: 13
Training loss: 2.709652290982166
Validation loss: 2.156013859641813
Epoch: 7| Step: 14
Training loss: 2.7936172692773416
Validation loss: 2.164012888295122
Epoch: 7| Step: 15
Training loss: 2.4737942504009203
Validation loss: 2.1822802159949344
Epoch: 45| Step: 0
Training loss: 3.014256932941863
Validation loss: 2.1306149907501255
Epoch: 7| Step: 1
Training loss: 2.676149574045437
Validation loss: 2.1756274271034184
Epoch: 7| Step: 2
Training loss: 1.9973902961292795
Validation loss: 2.1654153860660608
Epoch: 7| Step: 3
Training loss: 2.8933521499842287
Validation loss: 2.1040341420816686
Epoch: 7| Step: 4
Training loss: 2.7129880861554363
Validation loss: 2.179495453158764
Epoch: 7| Step: 5
Training loss: 2.7809737850573617
Validation loss: 2.1589386582335903
Epoch: 7| Step: 6
Training loss: 3.110715337587003
Validation loss: 2.124007672200085
Epoch: 7| Step: 7
Training loss: 2.8946243250576287
Validation loss: 2.1080304940612486
Epoch: 7| Step: 8
Training loss: 2.3827783801027955
Validation loss: 2.173397615883914
Epoch: 7| Step: 9
Training loss: 2.2509841885716804
Validation loss: 2.1128331344063094
Epoch: 7| Step: 10
Training loss: 2.7757625666171286
Validation loss: 2.1661383850694222
Epoch: 7| Step: 11
Training loss: 1.961978286595793
Validation loss: 2.197123387591603
Epoch: 7| Step: 12
Training loss: 2.15639384798854
Validation loss: 2.166565124862226
Epoch: 7| Step: 13
Training loss: 2.756968858185319
Validation loss: 2.165647481323997
Epoch: 7| Step: 14
Training loss: 2.0197083268293636
Validation loss: 2.142504551962949
Epoch: 7| Step: 15
Training loss: 2.4253399089635312
Validation loss: 2.184213340719834
Epoch: 46| Step: 0
Training loss: 2.285514503501418
Validation loss: 2.1465848442559046
Epoch: 7| Step: 1
Training loss: 2.300515747294994
Validation loss: 2.1162650821798916
Epoch: 7| Step: 2
Training loss: 2.5990954549593748
Validation loss: 2.1528766347814137
Epoch: 7| Step: 3
Training loss: 3.021180722661982
Validation loss: 2.1853471645849685
Epoch: 7| Step: 4
Training loss: 2.9268022902338595
Validation loss: 2.1072470012039757
Epoch: 7| Step: 5
Training loss: 3.510348416503556
Validation loss: 2.1538736933838862
Epoch: 7| Step: 6
Training loss: 1.8056797751057163
Validation loss: 2.122465485238118
Epoch: 7| Step: 7
Training loss: 2.5096205613779676
Validation loss: 2.1769908007476215
Epoch: 7| Step: 8
Training loss: 2.6178292925194624
Validation loss: 2.14124578068211
Epoch: 7| Step: 9
Training loss: 2.7459317372527408
Validation loss: 2.1648847198916474
Epoch: 7| Step: 10
Training loss: 2.4757072825289925
Validation loss: 2.1867651148910974
Epoch: 7| Step: 11
Training loss: 1.9083058886935258
Validation loss: 2.14396652720736
Epoch: 7| Step: 12
Training loss: 3.143534361343108
Validation loss: 2.1167828656639784
Epoch: 7| Step: 13
Training loss: 2.0485495204251905
Validation loss: 2.1594885395671524
Epoch: 7| Step: 14
Training loss: 2.6911170925945767
Validation loss: 2.152099165523625
Epoch: 7| Step: 15
Training loss: 1.8242304472783097
Validation loss: 2.1472972066794185
Epoch: 47| Step: 0
Training loss: 2.5495950414699746
Validation loss: 2.147981625968868
Epoch: 7| Step: 1
Training loss: 2.3041409119126075
Validation loss: 2.130798013828761
Epoch: 7| Step: 2
Training loss: 2.352766976850274
Validation loss: 2.1380181023644926
Epoch: 7| Step: 3
Training loss: 2.757155818400494
Validation loss: 2.1769775333705073
Epoch: 7| Step: 4
Training loss: 2.4144588009369965
Validation loss: 2.137505588697805
Epoch: 7| Step: 5
Training loss: 2.765614655038915
Validation loss: 2.16521567296301
Epoch: 7| Step: 6
Training loss: 2.409539092694118
Validation loss: 2.1539298079638924
Epoch: 7| Step: 7
Training loss: 2.0868626518146955
Validation loss: 2.105800482571958
Epoch: 7| Step: 8
Training loss: 2.333978858845469
Validation loss: 2.1762800041284023
Epoch: 7| Step: 9
Training loss: 2.2784877409815536
Validation loss: 2.17089837728666
Epoch: 7| Step: 10
Training loss: 2.8707379801896447
Validation loss: 2.1536515998098755
Epoch: 7| Step: 11
Training loss: 2.98158509232602
Validation loss: 2.118154752734849
Epoch: 7| Step: 12
Training loss: 3.307113730782859
Validation loss: 2.1162295086038467
Epoch: 7| Step: 13
Training loss: 2.4552338883811604
Validation loss: 2.1605998547077103
Epoch: 7| Step: 14
Training loss: 2.320764054566201
Validation loss: 2.2123648455641463
Epoch: 7| Step: 15
Training loss: 2.5846325673328647
Validation loss: 2.138342584070059
Epoch: 48| Step: 0
Training loss: 2.2301256012719843
Validation loss: 2.121074649226755
Epoch: 7| Step: 1
Training loss: 3.2733479125662246
Validation loss: 2.1282993553059764
Epoch: 7| Step: 2
Training loss: 2.287888365749802
Validation loss: 2.112742883261414
Epoch: 7| Step: 3
Training loss: 2.418127791146023
Validation loss: 2.088557444657831
Epoch: 7| Step: 4
Training loss: 2.9713651683089948
Validation loss: 2.1496033629019795
Epoch: 7| Step: 5
Training loss: 2.9838575943212295
Validation loss: 2.2214958749827693
Epoch: 7| Step: 6
Training loss: 2.229103432111993
Validation loss: 2.108477892872483
Epoch: 7| Step: 7
Training loss: 2.1008145160530893
Validation loss: 2.10547769898477
Epoch: 7| Step: 8
Training loss: 2.2810763985408786
Validation loss: 2.1958397475989773
Epoch: 7| Step: 9
Training loss: 2.4895505916299254
Validation loss: 2.197777618687111
Epoch: 7| Step: 10
Training loss: 3.118607200151287
Validation loss: 2.147710908372763
Epoch: 7| Step: 11
Training loss: 2.4367514218864077
Validation loss: 2.15239244057804
Epoch: 7| Step: 12
Training loss: 2.6892626105531767
Validation loss: 2.1326345616975804
Epoch: 7| Step: 13
Training loss: 2.136364560523654
Validation loss: 2.170198929053965
Epoch: 7| Step: 14
Training loss: 2.2878283405732556
Validation loss: 2.174316170765997
Epoch: 7| Step: 15
Training loss: 2.650222042066177
Validation loss: 2.1822906074975084
Epoch: 49| Step: 0
Training loss: 3.1240287797410558
Validation loss: 2.0851229328899707
Epoch: 7| Step: 1
Training loss: 2.729758414453876
Validation loss: 2.1528014148822248
Epoch: 7| Step: 2
Training loss: 2.5252125641136667
Validation loss: 2.129933719004203
Epoch: 7| Step: 3
Training loss: 2.364059950479871
Validation loss: 2.1542263313751087
Epoch: 7| Step: 4
Training loss: 2.2335746772328062
Validation loss: 2.1445115223207165
Epoch: 7| Step: 5
Training loss: 2.6474667010496096
Validation loss: 2.154910155338976
Epoch: 7| Step: 6
Training loss: 2.877772777099716
Validation loss: 2.1617513004820044
Epoch: 7| Step: 7
Training loss: 2.8049899392449054
Validation loss: 2.1793113430767694
Epoch: 7| Step: 8
Training loss: 2.8173873821995
Validation loss: 2.124581419218867
Epoch: 7| Step: 9
Training loss: 3.2078625882749257
Validation loss: 2.1184182546181236
Epoch: 7| Step: 10
Training loss: 2.170941488411412
Validation loss: 2.110304935499683
Epoch: 7| Step: 11
Training loss: 2.6140620913964114
Validation loss: 2.156860118831822
Epoch: 7| Step: 12
Training loss: 2.743554798733713
Validation loss: 2.153395208861249
Epoch: 7| Step: 13
Training loss: 1.9550207474527173
Validation loss: 2.131056713532451
Epoch: 7| Step: 14
Training loss: 2.3086654213044504
Validation loss: 2.170611637524848
Epoch: 7| Step: 15
Training loss: 2.128212015890926
Validation loss: 2.1799881140908557
Epoch: 50| Step: 0
Training loss: 2.91580483555575
Validation loss: 2.1457623595157784
Epoch: 7| Step: 1
Training loss: 1.432641651280133
Validation loss: 2.188031213464281
Epoch: 7| Step: 2
Training loss: 2.0282549565180745
Validation loss: 2.1470603839196847
Epoch: 7| Step: 3
Training loss: 2.3313055650567978
Validation loss: 2.1341488054311073
Epoch: 7| Step: 4
Training loss: 2.465059539973429
Validation loss: 2.184685738484965
Epoch: 7| Step: 5
Training loss: 2.8387454085184824
Validation loss: 2.147973424656782
Epoch: 7| Step: 6
Training loss: 2.9317440698355006
Validation loss: 2.113919286920645
Epoch: 7| Step: 7
Training loss: 2.5534491381101616
Validation loss: 2.1485542742719725
Epoch: 7| Step: 8
Training loss: 2.319054362019719
Validation loss: 2.1772727950446256
Epoch: 7| Step: 9
Training loss: 2.1482149875894763
Validation loss: 2.178648025483324
Epoch: 7| Step: 10
Training loss: 2.63793465165068
Validation loss: 2.149575969951978
Epoch: 7| Step: 11
Training loss: 2.5602256171822955
Validation loss: 2.104988781835963
Epoch: 7| Step: 12
Training loss: 2.943816515906059
Validation loss: 2.1255441680479543
Epoch: 7| Step: 13
Training loss: 3.3875438560659283
Validation loss: 2.1384415803085037
Epoch: 7| Step: 14
Training loss: 2.7247412427486215
Validation loss: 2.1499977000133637
Epoch: 7| Step: 15
Training loss: 2.607501425226102
Validation loss: 2.1828221314630682
Epoch: 51| Step: 0
Training loss: 2.3256606814264753
Validation loss: 2.138981837010538
Epoch: 7| Step: 1
Training loss: 2.761816991545167
Validation loss: 2.1455592607798835
Epoch: 7| Step: 2
Training loss: 2.899654163917605
Validation loss: 2.1142594335946407
Epoch: 7| Step: 3
Training loss: 2.5268240958952166
Validation loss: 2.1462570054005212
Epoch: 7| Step: 4
Training loss: 2.002019577784279
Validation loss: 2.1905021088569807
Epoch: 7| Step: 5
Training loss: 2.4386799230630656
Validation loss: 2.1262805524410435
Epoch: 7| Step: 6
Training loss: 2.3923872891048714
Validation loss: 2.177490087338187
Epoch: 7| Step: 7
Training loss: 2.8115316631359786
Validation loss: 2.136949450491497
Epoch: 7| Step: 8
Training loss: 2.5275867469156412
Validation loss: 2.087692952658467
Epoch: 7| Step: 9
Training loss: 2.5847910388183157
Validation loss: 2.1087414303074543
Epoch: 7| Step: 10
Training loss: 2.5751682782047034
Validation loss: 2.154661762367125
Epoch: 7| Step: 11
Training loss: 2.5723924192838585
Validation loss: 2.1924246091009065
Epoch: 7| Step: 12
Training loss: 2.969196326184123
Validation loss: 2.088881344963478
Epoch: 7| Step: 13
Training loss: 2.452295831392601
Validation loss: 2.1555380738247716
Epoch: 7| Step: 14
Training loss: 1.8500988160625818
Validation loss: 2.1578504280498882
Epoch: 7| Step: 15
Training loss: 2.917963847573693
Validation loss: 2.149623629002305
Epoch: 52| Step: 0
Training loss: 3.559086419012678
Validation loss: 2.15465134730055
Epoch: 7| Step: 1
Training loss: 2.8391540613352744
Validation loss: 2.1185537952753193
Epoch: 7| Step: 2
Training loss: 2.738367794916676
Validation loss: 2.1031908218316575
Epoch: 7| Step: 3
Training loss: 2.320128770740405
Validation loss: 2.161663265909704
Epoch: 7| Step: 4
Training loss: 2.6251900921975384
Validation loss: 2.1409450714990497
Epoch: 7| Step: 5
Training loss: 2.3230088105907027
Validation loss: 2.1244244703738047
Epoch: 7| Step: 6
Training loss: 2.440340587734234
Validation loss: 2.134783397727564
Epoch: 7| Step: 7
Training loss: 2.5148209892254996
Validation loss: 2.1358373703436353
Epoch: 7| Step: 8
Training loss: 2.2567292356996926
Validation loss: 2.115813088057156
Epoch: 7| Step: 9
Training loss: 2.851681390661519
Validation loss: 2.110029633345228
Epoch: 7| Step: 10
Training loss: 2.411790290762489
Validation loss: 2.100397510260767
Epoch: 7| Step: 11
Training loss: 2.3865051673435205
Validation loss: 2.1134360656749243
Epoch: 7| Step: 12
Training loss: 2.3882819767515047
Validation loss: 2.1267264035533526
Epoch: 7| Step: 13
Training loss: 1.942343656318815
Validation loss: 2.1210975520377504
Epoch: 7| Step: 14
Training loss: 2.2928584265609837
Validation loss: 2.1370976362397127
Epoch: 7| Step: 15
Training loss: 2.8614309455182103
Validation loss: 2.1166677222540113
Epoch: 53| Step: 0
Training loss: 2.705667700021918
Validation loss: 2.1053313689378506
Epoch: 7| Step: 1
Training loss: 2.248847878379134
Validation loss: 2.169620695392031
Epoch: 7| Step: 2
Training loss: 2.293479121789902
Validation loss: 2.1629095744799676
Epoch: 7| Step: 3
Training loss: 2.7109466464633165
Validation loss: 2.1668908384457564
Epoch: 7| Step: 4
Training loss: 2.7330680121380384
Validation loss: 2.149503804598403
Epoch: 7| Step: 5
Training loss: 2.266662149798811
Validation loss: 2.164700630778039
Epoch: 7| Step: 6
Training loss: 1.8040185654651248
Validation loss: 2.145236781690982
Epoch: 7| Step: 7
Training loss: 2.9037085465072217
Validation loss: 2.130337313102045
Epoch: 7| Step: 8
Training loss: 2.6885875009745783
Validation loss: 2.1429901234183424
Epoch: 7| Step: 9
Training loss: 2.3063367212216157
Validation loss: 2.1736754459369036
Epoch: 7| Step: 10
Training loss: 3.2431486244833554
Validation loss: 2.141881175567335
Epoch: 7| Step: 11
Training loss: 2.7979127461133024
Validation loss: 2.164963290496334
Epoch: 7| Step: 12
Training loss: 2.154483790515717
Validation loss: 2.126260045124398
Epoch: 7| Step: 13
Training loss: 2.7057195130299294
Validation loss: 2.1186389323010806
Epoch: 7| Step: 14
Training loss: 2.7602961004066513
Validation loss: 2.1393161164366874
Epoch: 7| Step: 15
Training loss: 2.2987060514462936
Validation loss: 2.097687941358752
Epoch: 54| Step: 0
Training loss: 2.9755546886179145
Validation loss: 2.1345720934768866
Epoch: 7| Step: 1
Training loss: 2.511373967879529
Validation loss: 2.1586879435789657
Epoch: 7| Step: 2
Training loss: 2.267180336455182
Validation loss: 2.1472819618831624
Epoch: 7| Step: 3
Training loss: 2.5761227317443574
Validation loss: 2.1349701661382414
Epoch: 7| Step: 4
Training loss: 2.7768737137692994
Validation loss: 2.1920908692363823
Epoch: 7| Step: 5
Training loss: 2.280491742093006
Validation loss: 2.160164224990697
Epoch: 7| Step: 6
Training loss: 2.61869108365958
Validation loss: 2.1506599041471492
Epoch: 7| Step: 7
Training loss: 2.377555375923403
Validation loss: 2.1612600845577963
Epoch: 7| Step: 8
Training loss: 2.670154486055186
Validation loss: 2.162581599816299
Epoch: 7| Step: 9
Training loss: 2.2266175380396085
Validation loss: 2.1142298617737882
Epoch: 7| Step: 10
Training loss: 2.4584178802262073
Validation loss: 2.162436269853619
Epoch: 7| Step: 11
Training loss: 3.094002453300623
Validation loss: 2.1494538223256527
Epoch: 7| Step: 12
Training loss: 1.9473340664298142
Validation loss: 2.1464215602516123
Epoch: 7| Step: 13
Training loss: 2.740112824031454
Validation loss: 2.146301834637036
Epoch: 7| Step: 14
Training loss: 2.1878999616917145
Validation loss: 2.153837859986909
Epoch: 7| Step: 15
Training loss: 2.8191970775143114
Validation loss: 2.144419968394129
Epoch: 55| Step: 0
Training loss: 2.534251564491273
Validation loss: 2.115557975572851
Epoch: 7| Step: 1
Training loss: 2.061254645551139
Validation loss: 2.0877243913468013
Epoch: 7| Step: 2
Training loss: 2.7093910401605372
Validation loss: 2.1549625229880003
Epoch: 7| Step: 3
Training loss: 2.6316641728504324
Validation loss: 2.126289346899508
Epoch: 7| Step: 4
Training loss: 2.1944923905340423
Validation loss: 2.127627008060345
Epoch: 7| Step: 5
Training loss: 2.087309996586656
Validation loss: 2.1878888112849353
Epoch: 7| Step: 6
Training loss: 2.5128968414130126
Validation loss: 2.1561003438197486
Epoch: 7| Step: 7
Training loss: 2.279598317483742
Validation loss: 2.1318409703934447
Epoch: 7| Step: 8
Training loss: 2.404618242541735
Validation loss: 2.130741157873973
Epoch: 7| Step: 9
Training loss: 2.680401320514089
Validation loss: 2.1384294564587267
Epoch: 7| Step: 10
Training loss: 2.8218251237654273
Validation loss: 2.1816695058343463
Epoch: 7| Step: 11
Training loss: 3.0641697593952086
Validation loss: 2.138112881397318
Epoch: 7| Step: 12
Training loss: 2.8718499880958226
Validation loss: 2.107366210977836
Epoch: 7| Step: 13
Training loss: 2.597035728282268
Validation loss: 2.1729752151195822
Epoch: 7| Step: 14
Training loss: 2.7304612406571453
Validation loss: 2.09933770290223
Epoch: 7| Step: 15
Training loss: 2.4730208912980065
Validation loss: 2.1929933411192004
Epoch: 56| Step: 0
Training loss: 2.374540585705804
Validation loss: 2.138839836586927
Epoch: 7| Step: 1
Training loss: 2.0859503013418053
Validation loss: 2.1353793356370834
Epoch: 7| Step: 2
Training loss: 2.7212290368523377
Validation loss: 2.1394835332512567
Epoch: 7| Step: 3
Training loss: 2.872210932729322
Validation loss: 2.1283002345135698
Epoch: 7| Step: 4
Training loss: 2.4708080163937733
Validation loss: 2.151712136944749
Epoch: 7| Step: 5
Training loss: 3.184311712083463
Validation loss: 2.1190182392345753
Epoch: 7| Step: 6
Training loss: 2.484142940407509
Validation loss: 2.157945619068143
Epoch: 7| Step: 7
Training loss: 2.086396813275238
Validation loss: 2.1500490086539954
Epoch: 7| Step: 8
Training loss: 3.0543841823613924
Validation loss: 2.1519757998407436
Epoch: 7| Step: 9
Training loss: 2.0194114192038697
Validation loss: 2.166194495341047
Epoch: 7| Step: 10
Training loss: 2.575960580115472
Validation loss: 2.095304560388814
Epoch: 7| Step: 11
Training loss: 2.748652474831746
Validation loss: 2.113653189825016
Epoch: 7| Step: 12
Training loss: 2.53499251249814
Validation loss: 2.1268898479695375
Epoch: 7| Step: 13
Training loss: 2.4315706174033758
Validation loss: 2.142272921302666
Epoch: 7| Step: 14
Training loss: 2.173818692772102
Validation loss: 2.1297378942560385
Epoch: 7| Step: 15
Training loss: 2.8731255433017964
Validation loss: 2.0903181633210766
Epoch: 57| Step: 0
Training loss: 2.86951836163259
Validation loss: 2.156274385335298
Epoch: 7| Step: 1
Training loss: 2.5640596435084637
Validation loss: 2.148784609090134
Epoch: 7| Step: 2
Training loss: 1.919141236221089
Validation loss: 2.1180914874501493
Epoch: 7| Step: 3
Training loss: 2.295781816111078
Validation loss: 2.1154728375242144
Epoch: 7| Step: 4
Training loss: 2.43401606096346
Validation loss: 2.125452015956504
Epoch: 7| Step: 5
Training loss: 2.5074170712937414
Validation loss: 2.1220387810536567
Epoch: 7| Step: 6
Training loss: 2.77987645553948
Validation loss: 2.1789871946275765
Epoch: 7| Step: 7
Training loss: 2.626527432690413
Validation loss: 2.1015737518291564
Epoch: 7| Step: 8
Training loss: 2.9456968183238748
Validation loss: 2.1493002387687232
Epoch: 7| Step: 9
Training loss: 3.017835370305283
Validation loss: 2.1078129590008117
Epoch: 7| Step: 10
Training loss: 2.3628725313215018
Validation loss: 2.1326250555559163
Epoch: 7| Step: 11
Training loss: 2.0265139018800093
Validation loss: 2.1204851934234172
Epoch: 7| Step: 12
Training loss: 2.4735011484773066
Validation loss: 2.1099915732274184
Epoch: 7| Step: 13
Training loss: 2.2392904992094826
Validation loss: 2.1774470936412316
Epoch: 7| Step: 14
Training loss: 2.362827124982378
Validation loss: 2.169437600610182
Epoch: 7| Step: 15
Training loss: 3.2345212451840344
Validation loss: 2.0788339492668064
Epoch: 58| Step: 0
Training loss: 2.0642038878790205
Validation loss: 2.1642873154267206
Epoch: 7| Step: 1
Training loss: 2.079657656300004
Validation loss: 2.1423114033745554
Epoch: 7| Step: 2
Training loss: 3.122118727875695
Validation loss: 2.159020947531213
Epoch: 7| Step: 3
Training loss: 3.1174177118253565
Validation loss: 2.1506603915980502
Epoch: 7| Step: 4
Training loss: 2.246477336471216
Validation loss: 2.1449914923405893
Epoch: 7| Step: 5
Training loss: 2.2130953230291452
Validation loss: 2.155342511760297
Epoch: 7| Step: 6
Training loss: 2.3551806626401968
Validation loss: 2.1406487970966674
Epoch: 7| Step: 7
Training loss: 2.207409527524195
Validation loss: 2.1122613481138575
Epoch: 7| Step: 8
Training loss: 2.9332364572075673
Validation loss: 2.1274280105512275
Epoch: 7| Step: 9
Training loss: 2.8285338385535157
Validation loss: 2.105680383208339
Epoch: 7| Step: 10
Training loss: 2.3887907483644177
Validation loss: 2.1491751459645987
Epoch: 7| Step: 11
Training loss: 2.3507030855662054
Validation loss: 2.1410335930009947
Epoch: 7| Step: 12
Training loss: 2.2510374644350732
Validation loss: 2.1467334380503167
Epoch: 7| Step: 13
Training loss: 2.7824993970515584
Validation loss: 2.1587280893782714
Epoch: 7| Step: 14
Training loss: 2.4222003349089154
Validation loss: 2.1178725416436994
Epoch: 7| Step: 15
Training loss: 2.941076495771487
Validation loss: 2.1866364983763753
Epoch: 59| Step: 0
Training loss: 2.1933362217374914
Validation loss: 2.1793732177846863
Epoch: 7| Step: 1
Training loss: 2.872164613485022
Validation loss: 2.1262177573312093
Epoch: 7| Step: 2
Training loss: 2.3680404284737686
Validation loss: 2.1069381187640825
Epoch: 7| Step: 3
Training loss: 2.1900988808173865
Validation loss: 2.1005365582256896
Epoch: 7| Step: 4
Training loss: 2.6560982155909594
Validation loss: 2.087739427967511
Epoch: 7| Step: 5
Training loss: 2.516997821653846
Validation loss: 2.1863432661929743
Epoch: 7| Step: 6
Training loss: 2.353803510984456
Validation loss: 2.1850146190400896
Epoch: 7| Step: 7
Training loss: 2.3960512697915046
Validation loss: 2.1615638403463926
Epoch: 7| Step: 8
Training loss: 2.7296256536018397
Validation loss: 2.1434892644079397
Epoch: 7| Step: 9
Training loss: 2.1245742820202302
Validation loss: 2.1594749079591926
Epoch: 7| Step: 10
Training loss: 2.363661150702307
Validation loss: 2.1234561981317026
Epoch: 7| Step: 11
Training loss: 2.595587022422249
Validation loss: 2.1065505826490174
Epoch: 7| Step: 12
Training loss: 2.1855610564916352
Validation loss: 2.1077508798270346
Epoch: 7| Step: 13
Training loss: 2.745177027790724
Validation loss: 2.1228001861078543
Epoch: 7| Step: 14
Training loss: 2.6607828396590767
Validation loss: 2.1714254437758065
Epoch: 7| Step: 15
Training loss: 3.286434328663613
Validation loss: 2.155961861911951
Epoch: 60| Step: 0
Training loss: 3.0451580198279786
Validation loss: 2.1353213987454778
Epoch: 7| Step: 1
Training loss: 3.155733576157297
Validation loss: 2.1683547395698
Epoch: 7| Step: 2
Training loss: 2.581795039604399
Validation loss: 2.1044628980717297
Epoch: 7| Step: 3
Training loss: 2.123317501222682
Validation loss: 2.1299539790639934
Epoch: 7| Step: 4
Training loss: 2.4169362564551293
Validation loss: 2.1131569906864938
Epoch: 7| Step: 5
Training loss: 2.150468455888844
Validation loss: 2.158304415198508
Epoch: 7| Step: 6
Training loss: 2.5492578136971162
Validation loss: 2.16460827819312
Epoch: 7| Step: 7
Training loss: 2.2719249609657615
Validation loss: 2.1337766146367345
Epoch: 7| Step: 8
Training loss: 2.5081080562609546
Validation loss: 2.1221085137443727
Epoch: 7| Step: 9
Training loss: 2.6952875550815794
Validation loss: 2.155687089370059
Epoch: 7| Step: 10
Training loss: 2.5639723292924197
Validation loss: 2.1217869854116738
Epoch: 7| Step: 11
Training loss: 2.6815168625847146
Validation loss: 2.1372807042780417
Epoch: 7| Step: 12
Training loss: 2.3274837353196456
Validation loss: 2.152942439704517
Epoch: 7| Step: 13
Training loss: 2.693282449244771
Validation loss: 2.1793895175694042
Epoch: 7| Step: 14
Training loss: 2.2750765735189566
Validation loss: 2.140788125289916
Epoch: 7| Step: 15
Training loss: 2.569289540155002
Validation loss: 2.1213504948506032
Epoch: 61| Step: 0
Training loss: 2.2733891537693935
Validation loss: 2.1691907501228624
Epoch: 7| Step: 1
Training loss: 2.2806046500621835
Validation loss: 2.1473650345070223
Epoch: 7| Step: 2
Training loss: 2.5625930397076715
Validation loss: 2.1477856996580496
Epoch: 7| Step: 3
Training loss: 1.384205348381311
Validation loss: 2.1354838018771116
Epoch: 7| Step: 4
Training loss: 2.6531696185526457
Validation loss: 2.125504452066046
Epoch: 7| Step: 5
Training loss: 2.305305630825048
Validation loss: 2.1539545409148206
Epoch: 7| Step: 6
Training loss: 2.903224231350379
Validation loss: 2.1587377920853292
Epoch: 7| Step: 7
Training loss: 2.6742123201712524
Validation loss: 2.1274010404968937
Epoch: 7| Step: 8
Training loss: 1.9560627320572221
Validation loss: 2.108176552724248
Epoch: 7| Step: 9
Training loss: 2.340527671756324
Validation loss: 2.1781092161352906
Epoch: 7| Step: 10
Training loss: 2.678940471476202
Validation loss: 2.117284424018242
Epoch: 7| Step: 11
Training loss: 2.6713264303737807
Validation loss: 2.132306837333579
Epoch: 7| Step: 12
Training loss: 2.428614830382588
Validation loss: 2.129614034244943
Epoch: 7| Step: 13
Training loss: 3.083608013668081
Validation loss: 2.1181887694040924
Epoch: 7| Step: 14
Training loss: 2.655280441157093
Validation loss: 2.1470542673757773
Epoch: 7| Step: 15
Training loss: 3.4012260133906183
Validation loss: 2.121689356786849
Epoch: 62| Step: 0
Training loss: 2.8809245887745925
Validation loss: 2.1067524850179873
Epoch: 7| Step: 1
Training loss: 2.3055873342670448
Validation loss: 2.1337991670018126
Epoch: 7| Step: 2
Training loss: 2.314548719017398
Validation loss: 2.1349134471753715
Epoch: 7| Step: 3
Training loss: 3.067163769887189
Validation loss: 2.146466482962068
Epoch: 7| Step: 4
Training loss: 2.933734834531864
Validation loss: 2.135502598737192
Epoch: 7| Step: 5
Training loss: 2.5870382956673663
Validation loss: 2.084786145013019
Epoch: 7| Step: 6
Training loss: 2.760393787984985
Validation loss: 2.1519705601874124
Epoch: 7| Step: 7
Training loss: 3.0718965205605846
Validation loss: 2.0653498078463985
Epoch: 7| Step: 8
Training loss: 2.515952900432038
Validation loss: 2.157353154377458
Epoch: 7| Step: 9
Training loss: 2.310379499498223
Validation loss: 2.078598370363335
Epoch: 7| Step: 10
Training loss: 2.480161056057278
Validation loss: 2.1298638212182754
Epoch: 7| Step: 11
Training loss: 2.5524215681708653
Validation loss: 2.094350213528344
Epoch: 7| Step: 12
Training loss: 2.1693714449761887
Validation loss: 2.086548774951976
Epoch: 7| Step: 13
Training loss: 2.6669347946154343
Validation loss: 2.10471118358007
Epoch: 7| Step: 14
Training loss: 2.541397851788787
Validation loss: 2.103676804512061
Epoch: 7| Step: 15
Training loss: 2.047074169922899
Validation loss: 2.1005858367212813
Epoch: 63| Step: 0
Training loss: 3.4197743556668705
Validation loss: 2.056371797522903
Epoch: 7| Step: 1
Training loss: 2.200082179181907
Validation loss: 2.1294196581852916
Epoch: 7| Step: 2
Training loss: 1.5942250460065082
Validation loss: 2.120723446454867
Epoch: 7| Step: 3
Training loss: 2.78890999342324
Validation loss: 2.171549091535156
Epoch: 7| Step: 4
Training loss: 3.0926066125673364
Validation loss: 2.151042434510423
Epoch: 7| Step: 5
Training loss: 2.9864127507017026
Validation loss: 2.1180630746903097
Epoch: 7| Step: 6
Training loss: 1.8154658682810405
Validation loss: 2.151507554400805
Epoch: 7| Step: 7
Training loss: 2.648700622271169
Validation loss: 2.098137908231892
Epoch: 7| Step: 8
Training loss: 2.632405597700408
Validation loss: 2.1381103814464484
Epoch: 7| Step: 9
Training loss: 2.0405511208869838
Validation loss: 2.1289895803260275
Epoch: 7| Step: 10
Training loss: 2.4706932821567773
Validation loss: 2.1200561680258296
Epoch: 7| Step: 11
Training loss: 3.0665718568437352
Validation loss: 2.132361369109298
Epoch: 7| Step: 12
Training loss: 2.2786074448031206
Validation loss: 2.1564923872930035
Epoch: 7| Step: 13
Training loss: 2.7433430120020073
Validation loss: 2.1116227444905133
Epoch: 7| Step: 14
Training loss: 2.095429387484133
Validation loss: 2.1324996055298198
Epoch: 7| Step: 15
Training loss: 2.47532396604514
Validation loss: 2.093044812453239
Epoch: 64| Step: 0
Training loss: 2.7554817184259877
Validation loss: 2.1395014073063745
Epoch: 7| Step: 1
Training loss: 1.716346238918586
Validation loss: 2.152975901841684
Epoch: 7| Step: 2
Training loss: 3.273607636539207
Validation loss: 2.144890611559078
Epoch: 7| Step: 3
Training loss: 2.1909161731975315
Validation loss: 2.1565257124726807
Epoch: 7| Step: 4
Training loss: 2.5141953854621755
Validation loss: 2.0877306173216583
Epoch: 7| Step: 5
Training loss: 2.7468984626748774
Validation loss: 2.1804279641138873
Epoch: 7| Step: 6
Training loss: 2.4046373784701998
Validation loss: 2.094608937727246
Epoch: 7| Step: 7
Training loss: 2.4071142452879473
Validation loss: 2.1875024021150526
Epoch: 7| Step: 8
Training loss: 2.6914966939005494
Validation loss: 2.114564904616853
Epoch: 7| Step: 9
Training loss: 2.466091607909876
Validation loss: 2.114377160584439
Epoch: 7| Step: 10
Training loss: 2.3804185191589693
Validation loss: 2.1524585051237497
Epoch: 7| Step: 11
Training loss: 2.2626602242572327
Validation loss: 2.150075578718813
Epoch: 7| Step: 12
Training loss: 2.7449131648602823
Validation loss: 2.1687614438123233
Epoch: 7| Step: 13
Training loss: 2.894564197265254
Validation loss: 2.1223714270819034
Epoch: 7| Step: 14
Training loss: 2.113558992478104
Validation loss: 2.1687030169926476
Epoch: 7| Step: 15
Training loss: 2.6922031324644538
Validation loss: 2.13650743495151
Epoch: 65| Step: 0
Training loss: 2.704324004342876
Validation loss: 2.1836415322076976
Epoch: 7| Step: 1
Training loss: 2.7182115920556646
Validation loss: 2.1147849468067434
Epoch: 7| Step: 2
Training loss: 2.803701243289144
Validation loss: 2.1555921326474383
Epoch: 7| Step: 3
Training loss: 2.0848827195515502
Validation loss: 2.139279171369733
Epoch: 7| Step: 4
Training loss: 1.5781719842847297
Validation loss: 2.164878594096154
Epoch: 7| Step: 5
Training loss: 2.7940431889040624
Validation loss: 2.2281443622065153
Epoch: 7| Step: 6
Training loss: 2.33736077776325
Validation loss: 2.1768089308430327
Epoch: 7| Step: 7
Training loss: 2.5759653004269887
Validation loss: 2.119847400950152
Epoch: 7| Step: 8
Training loss: 2.1059390871489465
Validation loss: 2.1163472369120453
Epoch: 7| Step: 9
Training loss: 3.0227519990609797
Validation loss: 2.1652758097150717
Epoch: 7| Step: 10
Training loss: 2.9533726825489475
Validation loss: 2.1009188140270174
Epoch: 7| Step: 11
Training loss: 2.2921126422997267
Validation loss: 2.1238538229651636
Epoch: 7| Step: 12
Training loss: 2.92104340005124
Validation loss: 2.1380202406910462
Epoch: 7| Step: 13
Training loss: 2.3022809411300935
Validation loss: 2.1336242136567893
Epoch: 7| Step: 14
Training loss: 2.613517442643946
Validation loss: 2.1100906182418386
Epoch: 7| Step: 15
Training loss: 2.4288379619099425
Validation loss: 2.114225405637276
Epoch: 66| Step: 0
Training loss: 2.903744509720918
Validation loss: 2.1827013562278377
Epoch: 7| Step: 1
Training loss: 1.8113132406268568
Validation loss: 2.1291996127137063
Epoch: 7| Step: 2
Training loss: 2.5940748264105817
Validation loss: 2.161128583516476
Epoch: 7| Step: 3
Training loss: 2.88947747624971
Validation loss: 2.144815026698751
Epoch: 7| Step: 4
Training loss: 2.1674172739611843
Validation loss: 2.1522694327561624
Epoch: 7| Step: 5
Training loss: 2.145177969496468
Validation loss: 2.1407536826611784
Epoch: 7| Step: 6
Training loss: 2.3071021927190234
Validation loss: 2.168847515872567
Epoch: 7| Step: 7
Training loss: 2.4094866497923046
Validation loss: 2.131074362382375
Epoch: 7| Step: 8
Training loss: 2.9009597374095506
Validation loss: 2.0894152187646626
Epoch: 7| Step: 9
Training loss: 2.9570207683825958
Validation loss: 2.1284835384679943
Epoch: 7| Step: 10
Training loss: 3.1042822148538636
Validation loss: 2.0944375575997856
Epoch: 7| Step: 11
Training loss: 2.864456414387686
Validation loss: 2.09514270072142
Epoch: 7| Step: 12
Training loss: 2.6814805863115296
Validation loss: 2.0940149107421178
Epoch: 7| Step: 13
Training loss: 2.6522957484265555
Validation loss: 2.140273424276768
Epoch: 7| Step: 14
Training loss: 2.132349096685507
Validation loss: 2.083486624901664
Epoch: 7| Step: 15
Training loss: 2.2151246876636477
Validation loss: 2.119493891149535
Epoch: 67| Step: 0
Training loss: 2.7969445587544444
Validation loss: 2.1283225686180165
Epoch: 7| Step: 1
Training loss: 2.8509795128585647
Validation loss: 2.1653923115451232
Epoch: 7| Step: 2
Training loss: 3.1446249622099773
Validation loss: 2.1141273729959855
Epoch: 7| Step: 3
Training loss: 2.4123689210691492
Validation loss: 2.124827332681575
Epoch: 7| Step: 4
Training loss: 2.214851177546243
Validation loss: 2.1262260444899965
Epoch: 7| Step: 5
Training loss: 2.186589078612178
Validation loss: 2.1079839608862705
Epoch: 7| Step: 6
Training loss: 2.1298792304245437
Validation loss: 2.128356890159002
Epoch: 7| Step: 7
Training loss: 3.4437181585539487
Validation loss: 2.133628526183542
Epoch: 7| Step: 8
Training loss: 2.216761060248958
Validation loss: 2.049461730274227
Epoch: 7| Step: 9
Training loss: 2.8626333547121856
Validation loss: 2.12806036284149
Epoch: 7| Step: 10
Training loss: 2.6089866486248328
Validation loss: 2.0952554997037667
Epoch: 7| Step: 11
Training loss: 2.765895334598387
Validation loss: 2.142442751292638
Epoch: 7| Step: 12
Training loss: 2.758986355638934
Validation loss: 2.157758173722179
Epoch: 7| Step: 13
Training loss: 1.663691591287953
Validation loss: 2.116914928288613
Epoch: 7| Step: 14
Training loss: 1.9358469156004172
Validation loss: 2.076715134194221
Epoch: 7| Step: 15
Training loss: 2.293702925944812
Validation loss: 2.1173632323077904
Epoch: 68| Step: 0
Training loss: 2.76707222352174
Validation loss: 2.106030301797701
Epoch: 7| Step: 1
Training loss: 2.246301790637033
Validation loss: 2.0971799985748683
Epoch: 7| Step: 2
Training loss: 2.4035764431400093
Validation loss: 2.1126255134559817
Epoch: 7| Step: 3
Training loss: 2.262654639591824
Validation loss: 2.114957939153838
Epoch: 7| Step: 4
Training loss: 2.5186627924283336
Validation loss: 2.0768648041640136
Epoch: 7| Step: 5
Training loss: 2.8502287789591954
Validation loss: 2.1599550387320767
Epoch: 7| Step: 6
Training loss: 2.6657959788182577
Validation loss: 2.063116586688191
Epoch: 7| Step: 7
Training loss: 2.4558742206622926
Validation loss: 2.1474619992200825
Epoch: 7| Step: 8
Training loss: 2.246179197593419
Validation loss: 2.14792450740902
Epoch: 7| Step: 9
Training loss: 2.0885343353320387
Validation loss: 2.1426267610857774
Epoch: 7| Step: 10
Training loss: 2.9397306697770182
Validation loss: 2.1374069880585314
Epoch: 7| Step: 11
Training loss: 2.4655387370173094
Validation loss: 2.1512061280032455
Epoch: 7| Step: 12
Training loss: 2.1288033276798197
Validation loss: 2.1550956352161013
Epoch: 7| Step: 13
Training loss: 2.992608660746425
Validation loss: 2.120855138694007
Epoch: 7| Step: 14
Training loss: 2.462228975049585
Validation loss: 2.1455078084126473
Epoch: 7| Step: 15
Training loss: 3.285739987432762
Validation loss: 2.1238176354543112
Epoch: 69| Step: 0
Training loss: 2.7212653089310854
Validation loss: 2.123227555233655
Epoch: 7| Step: 1
Training loss: 2.8883081997766396
Validation loss: 2.1548836203890103
Epoch: 7| Step: 2
Training loss: 2.7854484843507996
Validation loss: 2.127437555809852
Epoch: 7| Step: 3
Training loss: 2.7333405199964376
Validation loss: 2.163127061004953
Epoch: 7| Step: 4
Training loss: 2.410307893356695
Validation loss: 2.1030851599807887
Epoch: 7| Step: 5
Training loss: 2.2033247620400953
Validation loss: 2.13447999672193
Epoch: 7| Step: 6
Training loss: 2.662672021171192
Validation loss: 2.1536107655791437
Epoch: 7| Step: 7
Training loss: 2.690741602268457
Validation loss: 2.1365239828441522
Epoch: 7| Step: 8
Training loss: 2.4171772340779696
Validation loss: 2.165199847459991
Epoch: 7| Step: 9
Training loss: 2.624855854981361
Validation loss: 2.116867958332483
Epoch: 7| Step: 10
Training loss: 2.209165692160713
Validation loss: 2.1260044558497326
Epoch: 7| Step: 11
Training loss: 3.0685029654896456
Validation loss: 2.1196498753151
Epoch: 7| Step: 12
Training loss: 1.738934683408322
Validation loss: 2.1523712121155736
Epoch: 7| Step: 13
Training loss: 2.4865558576908637
Validation loss: 2.1411967852270335
Epoch: 7| Step: 14
Training loss: 2.3052973570758755
Validation loss: 2.0879756656277824
Epoch: 7| Step: 15
Training loss: 2.35937742523675
Validation loss: 2.1663154801410878
Epoch: 70| Step: 0
Training loss: 3.144982498678202
Validation loss: 2.138998772928485
Epoch: 7| Step: 1
Training loss: 2.569540910943329
Validation loss: 2.115452985432413
Epoch: 7| Step: 2
Training loss: 2.0398905158115235
Validation loss: 2.1114920705733016
Epoch: 7| Step: 3
Training loss: 2.4653023899145237
Validation loss: 2.1765050950921854
Epoch: 7| Step: 4
Training loss: 2.3655338365533183
Validation loss: 2.1097384409132736
Epoch: 7| Step: 5
Training loss: 2.9470438030030786
Validation loss: 2.13763220567047
Epoch: 7| Step: 6
Training loss: 2.7230048373792637
Validation loss: 2.1430594298753265
Epoch: 7| Step: 7
Training loss: 2.093007767482001
Validation loss: 2.1779011141083173
Epoch: 7| Step: 8
Training loss: 2.693726357624007
Validation loss: 2.1782022308848488
Epoch: 7| Step: 9
Training loss: 2.011648825013507
Validation loss: 2.145036050600458
Epoch: 7| Step: 10
Training loss: 2.6907571970577475
Validation loss: 2.1561651038278375
Epoch: 7| Step: 11
Training loss: 3.026216555524435
Validation loss: 2.158341815289744
Epoch: 7| Step: 12
Training loss: 2.657513755411056
Validation loss: 2.125847254343746
Epoch: 7| Step: 13
Training loss: 2.0636749388863187
Validation loss: 2.1427900126631134
Epoch: 7| Step: 14
Training loss: 2.179840301298758
Validation loss: 2.1138028761450087
Epoch: 7| Step: 15
Training loss: 3.1480470325296213
Validation loss: 2.1476884196464154
Epoch: 71| Step: 0
Training loss: 3.1898264155499643
Validation loss: 2.12380730199182
Epoch: 7| Step: 1
Training loss: 2.3099562598707077
Validation loss: 2.1338812198000583
Epoch: 7| Step: 2
Training loss: 2.148232634053446
Validation loss: 2.1356709958867968
Epoch: 7| Step: 3
Training loss: 2.45183242469169
Validation loss: 2.1588178472186104
Epoch: 7| Step: 4
Training loss: 2.7263493058327195
Validation loss: 2.1490709748535375
Epoch: 7| Step: 5
Training loss: 2.639688720847356
Validation loss: 2.0867471883096376
Epoch: 7| Step: 6
Training loss: 2.654676442519
Validation loss: 2.1508019813517203
Epoch: 7| Step: 7
Training loss: 2.910818467544992
Validation loss: 2.1167650632216715
Epoch: 7| Step: 8
Training loss: 2.491520520350257
Validation loss: 2.0983060860331664
Epoch: 7| Step: 9
Training loss: 2.418776369567175
Validation loss: 2.0713673936083277
Epoch: 7| Step: 10
Training loss: 1.954174522704036
Validation loss: 2.0898396858522172
Epoch: 7| Step: 11
Training loss: 2.37726234299162
Validation loss: 2.1246383246808214
Epoch: 7| Step: 12
Training loss: 2.4720702237706886
Validation loss: 2.1016036936511755
Epoch: 7| Step: 13
Training loss: 3.054481596970738
Validation loss: 2.133038719545956
Epoch: 7| Step: 14
Training loss: 2.284637444445721
Validation loss: 2.1343633955562518
Epoch: 7| Step: 15
Training loss: 2.405652355931521
Validation loss: 2.098971415125383
Epoch: 72| Step: 0
Training loss: 1.7251071924269399
Validation loss: 2.1039983731740155
Epoch: 7| Step: 1
Training loss: 2.843352572997256
Validation loss: 2.124983590850736
Epoch: 7| Step: 2
Training loss: 2.853596836947524
Validation loss: 2.1034038891991407
Epoch: 7| Step: 3
Training loss: 2.9083346202013143
Validation loss: 2.127465896080471
Epoch: 7| Step: 4
Training loss: 3.2865734694797384
Validation loss: 2.1411351164566046
Epoch: 7| Step: 5
Training loss: 2.0414337751983305
Validation loss: 2.118423399616547
Epoch: 7| Step: 6
Training loss: 2.6454966373425837
Validation loss: 2.113962497440088
Epoch: 7| Step: 7
Training loss: 1.7489231747548142
Validation loss: 2.0871001577761255
Epoch: 7| Step: 8
Training loss: 2.4466574385812856
Validation loss: 2.124877393286074
Epoch: 7| Step: 9
Training loss: 2.60001095622762
Validation loss: 2.098304897898037
Epoch: 7| Step: 10
Training loss: 3.2134270959314915
Validation loss: 2.1179786683317023
Epoch: 7| Step: 11
Training loss: 2.6758459375734653
Validation loss: 2.124517529849099
Epoch: 7| Step: 12
Training loss: 2.293658437122686
Validation loss: 2.1207506238145424
Epoch: 7| Step: 13
Training loss: 2.8266079875265557
Validation loss: 2.1165211507814905
Epoch: 7| Step: 14
Training loss: 1.9466338668089258
Validation loss: 2.133571436675625
Epoch: 7| Step: 15
Training loss: 2.020251147643392
Validation loss: 2.1359676548589985
Epoch: 73| Step: 0
Training loss: 2.8707628954775735
Validation loss: 2.148384102888449
Epoch: 7| Step: 1
Training loss: 2.9483110862684714
Validation loss: 2.1490659310985945
Epoch: 7| Step: 2
Training loss: 2.994727110236042
Validation loss: 2.084109733819502
Epoch: 7| Step: 3
Training loss: 2.983601574763895
Validation loss: 2.104022784558815
Epoch: 7| Step: 4
Training loss: 2.3197540741506577
Validation loss: 2.14029228798941
Epoch: 7| Step: 5
Training loss: 2.0706200713085465
Validation loss: 2.0901281782705494
Epoch: 7| Step: 6
Training loss: 2.4375264092995415
Validation loss: 2.115527307618991
Epoch: 7| Step: 7
Training loss: 2.406323518496918
Validation loss: 2.128217590072569
Epoch: 7| Step: 8
Training loss: 2.924548389751242
Validation loss: 2.149407571759526
Epoch: 7| Step: 9
Training loss: 2.3818695704113733
Validation loss: 2.1498700997506033
Epoch: 7| Step: 10
Training loss: 2.4248978976748585
Validation loss: 2.128936174064511
Epoch: 7| Step: 11
Training loss: 1.7997499769136365
Validation loss: 2.1556352845072677
Epoch: 7| Step: 12
Training loss: 2.5761429999803305
Validation loss: 2.1122251143993087
Epoch: 7| Step: 13
Training loss: 1.9403629685072954
Validation loss: 2.142776026202713
Epoch: 7| Step: 14
Training loss: 2.576873383004558
Validation loss: 2.1353446272865093
Epoch: 7| Step: 15
Training loss: 2.3060986352885973
Validation loss: 2.12935715162739
Epoch: 74| Step: 0
Training loss: 2.9652689552225513
Validation loss: 2.090262588651262
Epoch: 7| Step: 1
Training loss: 2.5650290128022597
Validation loss: 2.0955476335036556
Epoch: 7| Step: 2
Training loss: 2.9349675726845708
Validation loss: 2.104022206171795
Epoch: 7| Step: 3
Training loss: 2.090258405672402
Validation loss: 2.1768329662522063
Epoch: 7| Step: 4
Training loss: 2.491957316037659
Validation loss: 2.080143057447628
Epoch: 7| Step: 5
Training loss: 2.4893419050257415
Validation loss: 2.149501139412453
Epoch: 7| Step: 6
Training loss: 2.3702970420016514
Validation loss: 2.1356173524684863
Epoch: 7| Step: 7
Training loss: 2.1516362462255967
Validation loss: 2.104967882273679
Epoch: 7| Step: 8
Training loss: 2.9878703314797344
Validation loss: 2.114471638472191
Epoch: 7| Step: 9
Training loss: 2.7153200458988542
Validation loss: 2.1677962641016912
Epoch: 7| Step: 10
Training loss: 1.7140786934351082
Validation loss: 2.1437045190108437
Epoch: 7| Step: 11
Training loss: 2.4936991444997547
Validation loss: 2.1154516043989804
Epoch: 7| Step: 12
Training loss: 2.443513738816434
Validation loss: 2.130246954720258
Epoch: 7| Step: 13
Training loss: 2.7145808030822
Validation loss: 2.1630700342265166
Epoch: 7| Step: 14
Training loss: 2.453151435466639
Validation loss: 2.1295946579381297
Epoch: 7| Step: 15
Training loss: 2.187041098279583
Validation loss: 2.145294952919801
Epoch: 75| Step: 0
Training loss: 2.646880555850777
Validation loss: 2.135729316864494
Epoch: 7| Step: 1
Training loss: 2.5247390272281867
Validation loss: 2.131081365013853
Epoch: 7| Step: 2
Training loss: 2.9085392293397727
Validation loss: 2.131915776670013
Epoch: 7| Step: 3
Training loss: 2.8829588814144174
Validation loss: 2.1815234175076843
Epoch: 7| Step: 4
Training loss: 2.1040896285152493
Validation loss: 2.07108138085428
Epoch: 7| Step: 5
Training loss: 1.8584612997611656
Validation loss: 2.1232910098648143
Epoch: 7| Step: 6
Training loss: 2.8801490451075082
Validation loss: 2.072318451385975
Epoch: 7| Step: 7
Training loss: 2.724279896493247
Validation loss: 2.109107868098174
Epoch: 7| Step: 8
Training loss: 3.0973973208104684
Validation loss: 2.0976781805783524
Epoch: 7| Step: 9
Training loss: 2.6143838469455707
Validation loss: 2.131971829735418
Epoch: 7| Step: 10
Training loss: 2.4250902060808777
Validation loss: 2.1089951593902168
Epoch: 7| Step: 11
Training loss: 2.50497799222191
Validation loss: 2.102675518275334
Epoch: 7| Step: 12
Training loss: 2.3494795892628115
Validation loss: 2.101312676685306
Epoch: 7| Step: 13
Training loss: 1.7328990808016387
Validation loss: 2.149384369563805
Epoch: 7| Step: 14
Training loss: 3.096838132328953
Validation loss: 2.1448659273093815
Epoch: 7| Step: 15
Training loss: 2.0508467018945202
Validation loss: 2.1324617422694696
Epoch: 76| Step: 0
Training loss: 2.8705381517584265
Validation loss: 2.1094442918188925
Epoch: 7| Step: 1
Training loss: 2.7033520200799255
Validation loss: 2.103439379956288
Epoch: 7| Step: 2
Training loss: 2.2670790643084513
Validation loss: 2.166012615368677
Epoch: 7| Step: 3
Training loss: 2.5025860762297887
Validation loss: 2.130368269647867
Epoch: 7| Step: 4
Training loss: 2.332325979039736
Validation loss: 2.1392014281580614
Epoch: 7| Step: 5
Training loss: 2.1273130282265127
Validation loss: 2.157722934153737
Epoch: 7| Step: 6
Training loss: 2.495695604739022
Validation loss: 2.170186489092212
Epoch: 7| Step: 7
Training loss: 2.8133914806208504
Validation loss: 2.1176487953057714
Epoch: 7| Step: 8
Training loss: 2.2537052587213964
Validation loss: 2.110475617144626
Epoch: 7| Step: 9
Training loss: 1.8641338836738595
Validation loss: 2.0870973104867065
Epoch: 7| Step: 10
Training loss: 3.121013998413961
Validation loss: 2.122466410297247
Epoch: 7| Step: 11
Training loss: 3.13264126202639
Validation loss: 2.1302443936316395
Epoch: 7| Step: 12
Training loss: 2.554601358717095
Validation loss: 2.1290274962684648
Epoch: 7| Step: 13
Training loss: 2.639969603334622
Validation loss: 2.095092179974433
Epoch: 7| Step: 14
Training loss: 2.316557443454503
Validation loss: 2.186335851030099
Epoch: 7| Step: 15
Training loss: 2.288458526402213
Validation loss: 2.11141656160047
Epoch: 77| Step: 0
Training loss: 2.440706051934953
Validation loss: 2.087166639264872
Epoch: 7| Step: 1
Training loss: 2.488596753968695
Validation loss: 2.1157625052440587
Epoch: 7| Step: 2
Training loss: 3.3519074944907916
Validation loss: 2.1465046830489714
Epoch: 7| Step: 3
Training loss: 2.480987446536033
Validation loss: 2.1701316950313103
Epoch: 7| Step: 4
Training loss: 2.8008311332399654
Validation loss: 2.13669378795416
Epoch: 7| Step: 5
Training loss: 2.4728477367723745
Validation loss: 2.124904392279142
Epoch: 7| Step: 6
Training loss: 2.4036505393619008
Validation loss: 2.08840092138777
Epoch: 7| Step: 7
Training loss: 2.660596276187133
Validation loss: 2.132379953956901
Epoch: 7| Step: 8
Training loss: 2.444051008881544
Validation loss: 2.082566470606412
Epoch: 7| Step: 9
Training loss: 1.9483641928041877
Validation loss: 2.1277188179253086
Epoch: 7| Step: 10
Training loss: 2.106072220812901
Validation loss: 2.0698453993804384
Epoch: 7| Step: 11
Training loss: 2.6714578046284765
Validation loss: 2.1305300348724434
Epoch: 7| Step: 12
Training loss: 2.269499944316681
Validation loss: 2.1868296191017307
Epoch: 7| Step: 13
Training loss: 2.9001867563030395
Validation loss: 2.142535326132492
Epoch: 7| Step: 14
Training loss: 2.157854671742931
Validation loss: 2.116607931040296
Epoch: 7| Step: 15
Training loss: 2.4546622543930745
Validation loss: 2.1039121582968114
Epoch: 78| Step: 0
Training loss: 2.2007429082345196
Validation loss: 2.0689908622508737
Epoch: 7| Step: 1
Training loss: 2.70823741645182
Validation loss: 2.1483971805589213
Epoch: 7| Step: 2
Training loss: 2.289625703965129
Validation loss: 2.1084309305606643
Epoch: 7| Step: 3
Training loss: 2.4085513912686864
Validation loss: 2.095092325175457
Epoch: 7| Step: 4
Training loss: 2.4344914772070974
Validation loss: 2.129273144897855
Epoch: 7| Step: 5
Training loss: 2.3641445632276006
Validation loss: 2.0818018437057004
Epoch: 7| Step: 6
Training loss: 2.195471163951917
Validation loss: 2.1043443246347384
Epoch: 7| Step: 7
Training loss: 2.9246449116417352
Validation loss: 2.1361592828145146
Epoch: 7| Step: 8
Training loss: 2.73601792161214
Validation loss: 2.1063462491484284
Epoch: 7| Step: 9
Training loss: 2.4383333200005795
Validation loss: 2.122544947579986
Epoch: 7| Step: 10
Training loss: 2.6518620768566397
Validation loss: 2.1346077883639722
Epoch: 7| Step: 11
Training loss: 2.828054690672558
Validation loss: 2.1039935254999897
Epoch: 7| Step: 12
Training loss: 2.6592180611443488
Validation loss: 2.119243251758264
Epoch: 7| Step: 13
Training loss: 2.1614816197132924
Validation loss: 2.105336348011009
Epoch: 7| Step: 14
Training loss: 2.1249827215950807
Validation loss: 2.0831378606504853
Epoch: 7| Step: 15
Training loss: 3.098246841605122
Validation loss: 2.101057762702075
Epoch: 79| Step: 0
Training loss: 2.355988351880539
Validation loss: 2.141153587199735
Epoch: 7| Step: 1
Training loss: 2.314651519384498
Validation loss: 2.142635111702286
Epoch: 7| Step: 2
Training loss: 2.5359348224242644
Validation loss: 2.0912783990439427
Epoch: 7| Step: 3
Training loss: 2.385178089010627
Validation loss: 2.1392937420675904
Epoch: 7| Step: 4
Training loss: 2.6335861020357654
Validation loss: 2.2002908331466946
Epoch: 7| Step: 5
Training loss: 2.5712473366463477
Validation loss: 2.135546000557653
Epoch: 7| Step: 6
Training loss: 2.0328911826925085
Validation loss: 2.148398963971994
Epoch: 7| Step: 7
Training loss: 2.4349652339952166
Validation loss: 2.139516668276218
Epoch: 7| Step: 8
Training loss: 2.5343762154346945
Validation loss: 2.150125357241889
Epoch: 7| Step: 9
Training loss: 2.3054698586016906
Validation loss: 2.1228533223258976
Epoch: 7| Step: 10
Training loss: 2.3226657840290574
Validation loss: 2.138809780743646
Epoch: 7| Step: 11
Training loss: 2.632954036179614
Validation loss: 2.1499082202834785
Epoch: 7| Step: 12
Training loss: 3.1409309200255517
Validation loss: 2.104227547587145
Epoch: 7| Step: 13
Training loss: 2.8048314134658945
Validation loss: 2.1520840211495784
Epoch: 7| Step: 14
Training loss: 2.4618650595703255
Validation loss: 2.120492638820204
Epoch: 7| Step: 15
Training loss: 2.6111773816169253
Validation loss: 2.1608214967622565
Epoch: 80| Step: 0
Training loss: 2.4535943815347965
Validation loss: 2.1595852737357166
Epoch: 7| Step: 1
Training loss: 2.8024299908212855
Validation loss: 2.1057035737787957
Epoch: 7| Step: 2
Training loss: 2.318805243561511
Validation loss: 2.0993504106381757
Epoch: 7| Step: 3
Training loss: 2.73637256136965
Validation loss: 2.141184121227973
Epoch: 7| Step: 4
Training loss: 2.239586042062467
Validation loss: 2.1044852528654068
Epoch: 7| Step: 5
Training loss: 2.7902275665341123
Validation loss: 2.119635818416924
Epoch: 7| Step: 6
Training loss: 1.9157468274754945
Validation loss: 2.1338297075018025
Epoch: 7| Step: 7
Training loss: 2.65325543517711
Validation loss: 2.1154487556364505
Epoch: 7| Step: 8
Training loss: 2.697427378782634
Validation loss: 2.1381660575621053
Epoch: 7| Step: 9
Training loss: 2.0870201929977164
Validation loss: 2.0839065740330898
Epoch: 7| Step: 10
Training loss: 2.8977559977312954
Validation loss: 2.0880992067830886
Epoch: 7| Step: 11
Training loss: 2.5993530238755014
Validation loss: 2.158872179572333
Epoch: 7| Step: 12
Training loss: 2.907557039759173
Validation loss: 2.159820350027669
Epoch: 7| Step: 13
Training loss: 2.1881119008504366
Validation loss: 2.128941840938647
Epoch: 7| Step: 14
Training loss: 2.6526251799503067
Validation loss: 2.16657726280113
Epoch: 7| Step: 15
Training loss: 2.1984662518386844
Validation loss: 2.143832983961413
Epoch: 81| Step: 0
Training loss: 1.813046603159856
Validation loss: 2.1081721776836972
Epoch: 7| Step: 1
Training loss: 2.6701947556234518
Validation loss: 2.112052348816698
Epoch: 7| Step: 2
Training loss: 2.040823223773823
Validation loss: 2.093372812477804
Epoch: 7| Step: 3
Training loss: 2.417958298076663
Validation loss: 2.146251557378703
Epoch: 7| Step: 4
Training loss: 2.887154304981624
Validation loss: 2.1233524298607596
Epoch: 7| Step: 5
Training loss: 2.816692533505939
Validation loss: 2.125092217940947
Epoch: 7| Step: 6
Training loss: 1.6663527510970981
Validation loss: 2.139161599858814
Epoch: 7| Step: 7
Training loss: 2.640204130863865
Validation loss: 2.0970605369496345
Epoch: 7| Step: 8
Training loss: 2.585902833274051
Validation loss: 2.150693469345703
Epoch: 7| Step: 9
Training loss: 2.488305587290224
Validation loss: 2.094225089205932
Epoch: 7| Step: 10
Training loss: 3.2372765541292634
Validation loss: 2.1459963530347292
Epoch: 7| Step: 11
Training loss: 2.221219045651979
Validation loss: 2.1358097978423807
Epoch: 7| Step: 12
Training loss: 2.6405161095914957
Validation loss: 2.073785451213052
Epoch: 7| Step: 13
Training loss: 2.9070167760532244
Validation loss: 2.149784872843161
Epoch: 7| Step: 14
Training loss: 2.7265736926704367
Validation loss: 2.088580868432699
Epoch: 7| Step: 15
Training loss: 2.151091777473366
Validation loss: 2.1258564183317183
Epoch: 82| Step: 0
Training loss: 1.9110593318701485
Validation loss: 2.103347216002963
Epoch: 7| Step: 1
Training loss: 2.5544496011352407
Validation loss: 2.090840285580956
Epoch: 7| Step: 2
Training loss: 2.3651085716956426
Validation loss: 2.105111078862361
Epoch: 7| Step: 3
Training loss: 3.0170137834445883
Validation loss: 2.122848629086253
Epoch: 7| Step: 4
Training loss: 2.280162499810751
Validation loss: 2.1390007372704467
Epoch: 7| Step: 5
Training loss: 2.878439794300736
Validation loss: 2.1280985660649856
Epoch: 7| Step: 6
Training loss: 2.6495184478837412
Validation loss: 2.1188726776445903
Epoch: 7| Step: 7
Training loss: 2.9291226669052834
Validation loss: 2.080323831067495
Epoch: 7| Step: 8
Training loss: 2.059574016858926
Validation loss: 2.12527650912895
Epoch: 7| Step: 9
Training loss: 2.48598576756007
Validation loss: 2.1051822164479934
Epoch: 7| Step: 10
Training loss: 2.468358841823708
Validation loss: 2.070681218668151
Epoch: 7| Step: 11
Training loss: 2.9972997274935826
Validation loss: 2.104329216588248
Epoch: 7| Step: 12
Training loss: 2.088408760034016
Validation loss: 2.1073879794972
Epoch: 7| Step: 13
Training loss: 1.987797346604581
Validation loss: 2.0994259135497613
Epoch: 7| Step: 14
Training loss: 2.5692672691317915
Validation loss: 2.1107544665120432
Epoch: 7| Step: 15
Training loss: 2.899900329455543
Validation loss: 2.1150078121684013
Epoch: 83| Step: 0
Training loss: 2.1673179405688208
Validation loss: 2.0942442073101817
Epoch: 7| Step: 1
Training loss: 2.8065045349639024
Validation loss: 2.1530427593239465
Epoch: 7| Step: 2
Training loss: 2.7399454977364797
Validation loss: 2.158365991209155
Epoch: 7| Step: 3
Training loss: 2.3534996190456767
Validation loss: 2.138089532953008
Epoch: 7| Step: 4
Training loss: 2.8164826064003594
Validation loss: 2.1027964021445915
Epoch: 7| Step: 5
Training loss: 2.2191578664783043
Validation loss: 2.1110029814911013
Epoch: 7| Step: 6
Training loss: 2.31827411960843
Validation loss: 2.109349696535454
Epoch: 7| Step: 7
Training loss: 1.7675321288473282
Validation loss: 2.1426684848368414
Epoch: 7| Step: 8
Training loss: 2.8805518241554875
Validation loss: 2.1697726448795347
Epoch: 7| Step: 9
Training loss: 3.2018888740008133
Validation loss: 2.1047561882457506
Epoch: 7| Step: 10
Training loss: 2.339536719963249
Validation loss: 2.142520763793382
Epoch: 7| Step: 11
Training loss: 2.201858047996144
Validation loss: 2.148707476543381
Epoch: 7| Step: 12
Training loss: 2.9643962859182342
Validation loss: 2.118176050815839
Epoch: 7| Step: 13
Training loss: 2.6106484783378074
Validation loss: 2.1129453106083163
Epoch: 7| Step: 14
Training loss: 2.4175736161456682
Validation loss: 2.115953231598272
Epoch: 7| Step: 15
Training loss: 2.505031100934755
Validation loss: 2.0791840026235042
Epoch: 84| Step: 0
Training loss: 2.1209776339001407
Validation loss: 2.099500954017449
Epoch: 7| Step: 1
Training loss: 2.3167522615862364
Validation loss: 2.150936968674462
Epoch: 7| Step: 2
Training loss: 2.667667936063642
Validation loss: 2.168080374860346
Epoch: 7| Step: 3
Training loss: 2.3400832930844575
Validation loss: 2.108826712092205
Epoch: 7| Step: 4
Training loss: 2.09898087431839
Validation loss: 2.1666757493687627
Epoch: 7| Step: 5
Training loss: 1.3059865914104822
Validation loss: 2.156648952436843
Epoch: 7| Step: 6
Training loss: 2.65984289981739
Validation loss: 2.109839752663868
Epoch: 7| Step: 7
Training loss: 2.388690140561552
Validation loss: 2.141095743746664
Epoch: 7| Step: 8
Training loss: 2.7681898734508934
Validation loss: 2.1392321511737933
Epoch: 7| Step: 9
Training loss: 2.9936266592974397
Validation loss: 2.1104957250501
Epoch: 7| Step: 10
Training loss: 2.8242484654384166
Validation loss: 2.170131063633628
Epoch: 7| Step: 11
Training loss: 2.312585158970946
Validation loss: 2.1495157656832324
Epoch: 7| Step: 12
Training loss: 2.570851649173263
Validation loss: 2.1650310803251744
Epoch: 7| Step: 13
Training loss: 2.7020517324356135
Validation loss: 2.1227104239685306
Epoch: 7| Step: 14
Training loss: 2.3736461997174527
Validation loss: 2.0932854979642572
Epoch: 7| Step: 15
Training loss: 3.4786894389126406
Validation loss: 2.128971917991467
Epoch: 85| Step: 0
Training loss: 2.553328032807641
Validation loss: 2.106597873073205
Epoch: 7| Step: 1
Training loss: 2.7545747851822546
Validation loss: 2.1165810211815197
Epoch: 7| Step: 2
Training loss: 2.4681186110065347
Validation loss: 2.1354623659081113
Epoch: 7| Step: 3
Training loss: 2.5051501631509643
Validation loss: 2.0825399127355886
Epoch: 7| Step: 4
Training loss: 2.7573558223933707
Validation loss: 2.0861596316236497
Epoch: 7| Step: 5
Training loss: 2.376507832577457
Validation loss: 2.141622424503042
Epoch: 7| Step: 6
Training loss: 1.9900255627164423
Validation loss: 2.117998944921511
Epoch: 7| Step: 7
Training loss: 2.0653782037761967
Validation loss: 2.1279039822160883
Epoch: 7| Step: 8
Training loss: 2.6560812503842155
Validation loss: 2.1375157622105347
Epoch: 7| Step: 9
Training loss: 1.8480853173307954
Validation loss: 2.104599524953015
Epoch: 7| Step: 10
Training loss: 2.8011775674098485
Validation loss: 2.1258333298675556
Epoch: 7| Step: 11
Training loss: 3.188550289873096
Validation loss: 2.1028044410136846
Epoch: 7| Step: 12
Training loss: 2.8277250397000206
Validation loss: 2.1066198242555743
Epoch: 7| Step: 13
Training loss: 2.03453912732142
Validation loss: 2.1121717396812043
Epoch: 7| Step: 14
Training loss: 2.4747081276233462
Validation loss: 2.1284884348164637
Epoch: 7| Step: 15
Training loss: 2.5126426504452777
Validation loss: 2.1490619548530545
Epoch: 86| Step: 0
Training loss: 2.352428897670961
Validation loss: 2.086412141933569
Epoch: 7| Step: 1
Training loss: 2.7492856918644746
Validation loss: 2.1116785839395162
Epoch: 7| Step: 2
Training loss: 2.8540777113665863
Validation loss: 2.1294599770436355
Epoch: 7| Step: 3
Training loss: 2.5698737143871386
Validation loss: 2.1193144384794014
Epoch: 7| Step: 4
Training loss: 2.18453888066905
Validation loss: 2.092211780744094
Epoch: 7| Step: 5
Training loss: 3.3146269105883523
Validation loss: 2.0994087326474102
Epoch: 7| Step: 6
Training loss: 2.5742658920486607
Validation loss: 2.1215071997636046
Epoch: 7| Step: 7
Training loss: 2.7800637619335746
Validation loss: 2.097154400591406
Epoch: 7| Step: 8
Training loss: 2.1234360718038103
Validation loss: 2.07543423753161
Epoch: 7| Step: 9
Training loss: 1.9036593734600613
Validation loss: 2.1307742168589567
Epoch: 7| Step: 10
Training loss: 2.7742356562226873
Validation loss: 2.1580052554296456
Epoch: 7| Step: 11
Training loss: 2.2569615433526127
Validation loss: 2.1351653690035426
Epoch: 7| Step: 12
Training loss: 1.580419175883295
Validation loss: 2.048151436905901
Epoch: 7| Step: 13
Training loss: 2.6635535864376196
Validation loss: 2.0908329743587544
Epoch: 7| Step: 14
Training loss: 2.7663095780001647
Validation loss: 2.0882845956711735
Epoch: 7| Step: 15
Training loss: 2.4800076273831624
Validation loss: 2.0905260985520133
Epoch: 87| Step: 0
Training loss: 2.292153312568575
Validation loss: 2.1188299228786063
Epoch: 7| Step: 1
Training loss: 2.0156273657023425
Validation loss: 2.096342074748263
Epoch: 7| Step: 2
Training loss: 2.415373807137178
Validation loss: 2.1121629506811144
Epoch: 7| Step: 3
Training loss: 2.0613088925324488
Validation loss: 2.107306905875341
Epoch: 7| Step: 4
Training loss: 2.668093408936849
Validation loss: 2.1311003579438137
Epoch: 7| Step: 5
Training loss: 2.514242420195349
Validation loss: 2.0822327147917616
Epoch: 7| Step: 6
Training loss: 2.798113585979877
Validation loss: 2.0995267536358413
Epoch: 7| Step: 7
Training loss: 2.4559163534290582
Validation loss: 2.0507277127982824
Epoch: 7| Step: 8
Training loss: 3.6161260180246573
Validation loss: 2.095103204996485
Epoch: 7| Step: 9
Training loss: 2.8080927756163088
Validation loss: 2.1342621094536014
Epoch: 7| Step: 10
Training loss: 2.747125684042465
Validation loss: 2.1252591354457238
Epoch: 7| Step: 11
Training loss: 1.825632009417677
Validation loss: 2.12130878263781
Epoch: 7| Step: 12
Training loss: 2.1931682717421093
Validation loss: 2.1187551857665374
Epoch: 7| Step: 13
Training loss: 2.7308694220789023
Validation loss: 2.135537591510072
Epoch: 7| Step: 14
Training loss: 2.466393033736268
Validation loss: 2.155605107029109
Epoch: 7| Step: 15
Training loss: 2.48578234463492
Validation loss: 2.1388469227596083
Epoch: 88| Step: 0
Training loss: 2.796602374380932
Validation loss: 2.129545220543188
Epoch: 7| Step: 1
Training loss: 2.623542926208134
Validation loss: 2.116793765984258
Epoch: 7| Step: 2
Training loss: 2.3574480688983352
Validation loss: 2.1032520060009174
Epoch: 7| Step: 3
Training loss: 2.231908223490544
Validation loss: 2.1179125770183425
Epoch: 7| Step: 4
Training loss: 2.646150845276866
Validation loss: 2.0968868165465677
Epoch: 7| Step: 5
Training loss: 2.4356226416016735
Validation loss: 2.102911048215295
Epoch: 7| Step: 6
Training loss: 2.1451674110155037
Validation loss: 2.1214884900099165
Epoch: 7| Step: 7
Training loss: 2.152260669690224
Validation loss: 2.0584072783150433
Epoch: 7| Step: 8
Training loss: 2.573003594993099
Validation loss: 2.1205773980255835
Epoch: 7| Step: 9
Training loss: 2.650896669230987
Validation loss: 2.123874891674208
Epoch: 7| Step: 10
Training loss: 2.058390134987763
Validation loss: 2.140122139711615
Epoch: 7| Step: 11
Training loss: 2.843597743391407
Validation loss: 2.1010817458328535
Epoch: 7| Step: 12
Training loss: 2.960945713162669
Validation loss: 2.099797261095921
Epoch: 7| Step: 13
Training loss: 2.9494417017859305
Validation loss: 2.109193459997034
Epoch: 7| Step: 14
Training loss: 2.4000073989118333
Validation loss: 2.119465986859835
Epoch: 7| Step: 15
Training loss: 2.0702683929927197
Validation loss: 2.120011552325493
Epoch: 89| Step: 0
Training loss: 2.909914065530873
Validation loss: 2.1159184822880897
Epoch: 7| Step: 1
Training loss: 1.8263558198217904
Validation loss: 2.0773657607273908
Epoch: 7| Step: 2
Training loss: 2.375012548313119
Validation loss: 2.1097891947625045
Epoch: 7| Step: 3
Training loss: 2.358189973641273
Validation loss: 2.088803989914014
Epoch: 7| Step: 4
Training loss: 1.9780474124776488
Validation loss: 2.100419103677514
Epoch: 7| Step: 5
Training loss: 2.7386590150164745
Validation loss: 2.125635347953694
Epoch: 7| Step: 6
Training loss: 2.8618261951851225
Validation loss: 2.094710629203155
Epoch: 7| Step: 7
Training loss: 2.634134566262227
Validation loss: 2.125901342252546
Epoch: 7| Step: 8
Training loss: 2.0056898719612146
Validation loss: 2.103006042544422
Epoch: 7| Step: 9
Training loss: 2.5504797016795093
Validation loss: 2.1068859892818357
Epoch: 7| Step: 10
Training loss: 2.354629189845735
Validation loss: 2.1287906673987145
Epoch: 7| Step: 11
Training loss: 2.853188080106004
Validation loss: 2.1422119316745896
Epoch: 7| Step: 12
Training loss: 2.5672885533583547
Validation loss: 2.1570511242476265
Epoch: 7| Step: 13
Training loss: 2.3953144230246988
Validation loss: 2.1380864912427175
Epoch: 7| Step: 14
Training loss: 2.8007354519769354
Validation loss: 2.142919606774153
Epoch: 7| Step: 15
Training loss: 2.8673142241451934
Validation loss: 2.125298574810979
Epoch: 90| Step: 0
Training loss: 2.257409188439055
Validation loss: 2.1105912715661903
Epoch: 7| Step: 1
Training loss: 2.87179386659402
Validation loss: 2.1080419848964618
Epoch: 7| Step: 2
Training loss: 2.303883557807775
Validation loss: 2.1404496980049
Epoch: 7| Step: 3
Training loss: 2.9796434212171325
Validation loss: 2.113003043539268
Epoch: 7| Step: 4
Training loss: 2.468239550391918
Validation loss: 2.1484463783821357
Epoch: 7| Step: 5
Training loss: 2.3900044951137263
Validation loss: 2.1267637684702665
Epoch: 7| Step: 6
Training loss: 2.336012823108855
Validation loss: 2.136632560766281
Epoch: 7| Step: 7
Training loss: 2.727243507835612
Validation loss: 2.1673535545602034
Epoch: 7| Step: 8
Training loss: 2.5844080340524034
Validation loss: 2.1229227137709263
Epoch: 7| Step: 9
Training loss: 2.608551809242804
Validation loss: 2.1142156048862395
Epoch: 7| Step: 10
Training loss: 2.325548115591967
Validation loss: 2.079354705757495
Epoch: 7| Step: 11
Training loss: 2.281070231840285
Validation loss: 2.0911340759039483
Epoch: 7| Step: 12
Training loss: 2.508614765318
Validation loss: 2.134775268926044
Epoch: 7| Step: 13
Training loss: 2.496479034537493
Validation loss: 2.121573309474608
Epoch: 7| Step: 14
Training loss: 2.2804305812755725
Validation loss: 2.1152168424679902
Epoch: 7| Step: 15
Training loss: 2.6963275269745974
Validation loss: 2.1075763286618163
Epoch: 91| Step: 0
Training loss: 3.034738951364629
Validation loss: 2.153186360104589
Epoch: 7| Step: 1
Training loss: 2.5464158843757576
Validation loss: 2.1138774453868012
Epoch: 7| Step: 2
Training loss: 1.7413961897123462
Validation loss: 2.1110430406289593
Epoch: 7| Step: 3
Training loss: 2.4462689853841786
Validation loss: 2.1381367046589745
Epoch: 7| Step: 4
Training loss: 2.712888515792917
Validation loss: 2.1272297587399533
Epoch: 7| Step: 5
Training loss: 1.9590564231516654
Validation loss: 2.104429042812753
Epoch: 7| Step: 6
Training loss: 2.4844238468382414
Validation loss: 2.125031315178641
Epoch: 7| Step: 7
Training loss: 2.8171215656877564
Validation loss: 2.1146629367976955
Epoch: 7| Step: 8
Training loss: 2.1075777662597694
Validation loss: 2.122993029912154
Epoch: 7| Step: 9
Training loss: 2.264700865539636
Validation loss: 2.113981197045175
Epoch: 7| Step: 10
Training loss: 2.2918436386671583
Validation loss: 2.0562414792872548
Epoch: 7| Step: 11
Training loss: 3.2599041488223666
Validation loss: 2.131607644018196
Epoch: 7| Step: 12
Training loss: 2.772420691412712
Validation loss: 2.106656988688576
Epoch: 7| Step: 13
Training loss: 2.49928712217696
Validation loss: 2.133362848767205
Epoch: 7| Step: 14
Training loss: 2.2049078254723815
Validation loss: 2.1308810475613718
Epoch: 7| Step: 15
Training loss: 2.8690154773484817
Validation loss: 2.1065139580641303
Epoch: 92| Step: 0
Training loss: 2.4894911191666598
Validation loss: 2.093073999786611
Epoch: 7| Step: 1
Training loss: 2.3979976884514214
Validation loss: 2.1332076521008423
Epoch: 7| Step: 2
Training loss: 2.3436768583964924
Validation loss: 2.085983039053124
Epoch: 7| Step: 3
Training loss: 2.1119158100701987
Validation loss: 2.036982474776669
Epoch: 7| Step: 4
Training loss: 2.1118379130949663
Validation loss: 2.0901541176770824
Epoch: 7| Step: 5
Training loss: 2.8977204538628896
Validation loss: 2.1072132587137324
Epoch: 7| Step: 6
Training loss: 2.3924490757607866
Validation loss: 2.117577604487872
Epoch: 7| Step: 7
Training loss: 3.018794474461706
Validation loss: 2.0978063222981524
Epoch: 7| Step: 8
Training loss: 2.683180331573326
Validation loss: 2.1022466369132182
Epoch: 7| Step: 9
Training loss: 2.5448830895092254
Validation loss: 2.1118783296001973
Epoch: 7| Step: 10
Training loss: 3.0752467917055077
Validation loss: 2.1437301294551
Epoch: 7| Step: 11
Training loss: 2.5994442859394247
Validation loss: 2.152716115579147
Epoch: 7| Step: 12
Training loss: 3.0791475562989694
Validation loss: 2.122148061286904
Epoch: 7| Step: 13
Training loss: 2.6126143398542876
Validation loss: 2.1062574194099657
Epoch: 7| Step: 14
Training loss: 1.5435844641517291
Validation loss: 2.0985966858618754
Epoch: 7| Step: 15
Training loss: 2.02631694685459
Validation loss: 2.1359176609840254
Epoch: 93| Step: 0
Training loss: 2.788786460402161
Validation loss: 2.099897569248423
Epoch: 7| Step: 1
Training loss: 2.6373405119056064
Validation loss: 2.1200792197624905
Epoch: 7| Step: 2
Training loss: 2.730551438723574
Validation loss: 2.1300981866795765
Epoch: 7| Step: 3
Training loss: 2.0549629519944697
Validation loss: 2.0898911126222544
Epoch: 7| Step: 4
Training loss: 2.081479341168483
Validation loss: 2.0765053225084333
Epoch: 7| Step: 5
Training loss: 2.0612969791386333
Validation loss: 2.09870103085464
Epoch: 7| Step: 6
Training loss: 2.656433278660891
Validation loss: 2.0820865771749224
Epoch: 7| Step: 7
Training loss: 2.87405396528554
Validation loss: 2.1580038859579798
Epoch: 7| Step: 8
Training loss: 2.2015551662390602
Validation loss: 2.1084051391721217
Epoch: 7| Step: 9
Training loss: 2.682449474445489
Validation loss: 2.149072663381034
Epoch: 7| Step: 10
Training loss: 2.9959766111390436
Validation loss: 2.1609416833630206
Epoch: 7| Step: 11
Training loss: 1.9968631702245814
Validation loss: 2.118700100386987
Epoch: 7| Step: 12
Training loss: 2.5260091143735877
Validation loss: 2.105636635677725
Epoch: 7| Step: 13
Training loss: 2.1803829491961415
Validation loss: 2.114363034200136
Epoch: 7| Step: 14
Training loss: 2.877795974529902
Validation loss: 2.1186190290054414
Epoch: 7| Step: 15
Training loss: 2.6977646439972958
Validation loss: 2.097257112224257
Epoch: 94| Step: 0
Training loss: 2.7645555572915885
Validation loss: 2.0956933641684303
Epoch: 7| Step: 1
Training loss: 2.294302193596561
Validation loss: 2.087582287603878
Epoch: 7| Step: 2
Training loss: 3.127253063525182
Validation loss: 2.1268822408865145
Epoch: 7| Step: 3
Training loss: 3.0928013435648976
Validation loss: 2.139146061681092
Epoch: 7| Step: 4
Training loss: 2.3520742474209566
Validation loss: 2.1045881717495654
Epoch: 7| Step: 5
Training loss: 1.983998781119249
Validation loss: 2.1363660173612575
Epoch: 7| Step: 6
Training loss: 2.481307240938108
Validation loss: 2.1085454405556883
Epoch: 7| Step: 7
Training loss: 2.887871497890804
Validation loss: 2.1286532160218874
Epoch: 7| Step: 8
Training loss: 2.470082370141369
Validation loss: 2.121437378873184
Epoch: 7| Step: 9
Training loss: 2.375382743913963
Validation loss: 2.1255990398566897
Epoch: 7| Step: 10
Training loss: 2.1488349685601578
Validation loss: 2.078286290752862
Epoch: 7| Step: 11
Training loss: 2.1816927528813155
Validation loss: 2.119289360413744
Epoch: 7| Step: 12
Training loss: 2.506231267007657
Validation loss: 2.069112396791502
Epoch: 7| Step: 13
Training loss: 2.063340218223872
Validation loss: 2.1202547280833235
Epoch: 7| Step: 14
Training loss: 2.458658767970231
Validation loss: 2.124414292742338
Epoch: 7| Step: 15
Training loss: 2.9372477321656585
Validation loss: 2.1331744012864102
Epoch: 95| Step: 0
Training loss: 2.0321281295614293
Validation loss: 2.085130491884564
Epoch: 7| Step: 1
Training loss: 2.699530179155511
Validation loss: 2.0817176930104635
Epoch: 7| Step: 2
Training loss: 2.1667478130501636
Validation loss: 2.106044892587063
Epoch: 7| Step: 3
Training loss: 3.1389699342180397
Validation loss: 2.094449144318475
Epoch: 7| Step: 4
Training loss: 2.036670670492396
Validation loss: 2.1531034085289966
Epoch: 7| Step: 5
Training loss: 2.4602979040809165
Validation loss: 2.1074781033195027
Epoch: 7| Step: 6
Training loss: 2.6229572522268216
Validation loss: 2.129832423501567
Epoch: 7| Step: 7
Training loss: 2.9023867946631374
Validation loss: 2.1050251310830914
Epoch: 7| Step: 8
Training loss: 2.6508229183538865
Validation loss: 2.102356954759195
Epoch: 7| Step: 9
Training loss: 2.758673428162883
Validation loss: 2.118627111410418
Epoch: 7| Step: 10
Training loss: 2.855629390680616
Validation loss: 2.1213791631482004
Epoch: 7| Step: 11
Training loss: 2.3455581810833785
Validation loss: 2.103141720348005
Epoch: 7| Step: 12
Training loss: 2.658719698612664
Validation loss: 2.1391776591678084
Epoch: 7| Step: 13
Training loss: 1.8590724923382624
Validation loss: 2.1267972006513465
Epoch: 7| Step: 14
Training loss: 2.609533430760626
Validation loss: 2.1181173887407074
Epoch: 7| Step: 15
Training loss: 2.4358316238228532
Validation loss: 2.1373620872691537
Epoch: 96| Step: 0
Training loss: 2.750822984486639
Validation loss: 2.1429660209946215
Epoch: 7| Step: 1
Training loss: 2.15003125811188
Validation loss: 2.1320212326479293
Epoch: 7| Step: 2
Training loss: 2.7003900881942906
Validation loss: 2.136820642260089
Epoch: 7| Step: 3
Training loss: 2.568586332207309
Validation loss: 2.142332613000858
Epoch: 7| Step: 4
Training loss: 2.996901660597734
Validation loss: 2.1522586026328683
Epoch: 7| Step: 5
Training loss: 2.3190675214747274
Validation loss: 2.109721245189387
Epoch: 7| Step: 6
Training loss: 2.595547891744131
Validation loss: 2.1362849385655562
Epoch: 7| Step: 7
Training loss: 2.688016087858558
Validation loss: 2.0528739971595176
Epoch: 7| Step: 8
Training loss: 1.7523988903008183
Validation loss: 2.102188367720183
Epoch: 7| Step: 9
Training loss: 2.4824064120719727
Validation loss: 2.1465444672124927
Epoch: 7| Step: 10
Training loss: 2.6925308690475447
Validation loss: 2.12368721956264
Epoch: 7| Step: 11
Training loss: 2.6989394400870363
Validation loss: 2.1633499990219955
Epoch: 7| Step: 12
Training loss: 2.2669132121956492
Validation loss: 2.0893965268686503
Epoch: 7| Step: 13
Training loss: 2.896480094716156
Validation loss: 2.1103690479825317
Epoch: 7| Step: 14
Training loss: 2.522761865639541
Validation loss: 2.11769383786162
Epoch: 7| Step: 15
Training loss: 1.7761451116810285
Validation loss: 2.1095144940983337
Epoch: 97| Step: 0
Training loss: 2.486864486100721
Validation loss: 2.086369175377764
Epoch: 7| Step: 1
Training loss: 3.045578587877273
Validation loss: 2.0943177243352857
Epoch: 7| Step: 2
Training loss: 2.390270032272286
Validation loss: 2.126804445784386
Epoch: 7| Step: 3
Training loss: 2.644141028955502
Validation loss: 2.071124757215591
Epoch: 7| Step: 4
Training loss: 2.595102256421307
Validation loss: 2.148711565299966
Epoch: 7| Step: 5
Training loss: 2.7917970560473306
Validation loss: 2.1339293562974766
Epoch: 7| Step: 6
Training loss: 2.88772520037585
Validation loss: 2.1675806945128837
Epoch: 7| Step: 7
Training loss: 2.268516011427736
Validation loss: 2.1218879508673565
Epoch: 7| Step: 8
Training loss: 2.3124010219527507
Validation loss: 2.1268296162839904
Epoch: 7| Step: 9
Training loss: 2.238843384567701
Validation loss: 2.130167933657157
Epoch: 7| Step: 10
Training loss: 2.9111229846152455
Validation loss: 2.12827178905583
Epoch: 7| Step: 11
Training loss: 2.8367844675111122
Validation loss: 2.1205669454483473
Epoch: 7| Step: 12
Training loss: 2.106232173855562
Validation loss: 2.131105624248931
Epoch: 7| Step: 13
Training loss: 2.4472296758570216
Validation loss: 2.094558621491302
Epoch: 7| Step: 14
Training loss: 2.5469089810029386
Validation loss: 2.088045723087039
Epoch: 7| Step: 15
Training loss: 2.142212884420055
Validation loss: 2.0845263614393614
Epoch: 98| Step: 0
Training loss: 2.76475450857319
Validation loss: 2.111430154288064
Epoch: 7| Step: 1
Training loss: 2.123147044411287
Validation loss: 2.1002053514996355
Epoch: 7| Step: 2
Training loss: 2.230025212269399
Validation loss: 2.156604756037648
Epoch: 7| Step: 3
Training loss: 2.7361798242847186
Validation loss: 2.115461385220622
Epoch: 7| Step: 4
Training loss: 2.3139409911034545
Validation loss: 2.1424632715265695
Epoch: 7| Step: 5
Training loss: 2.3263519499267473
Validation loss: 2.110874839205703
Epoch: 7| Step: 6
Training loss: 2.6791206850305374
Validation loss: 2.0714138805274938
Epoch: 7| Step: 7
Training loss: 2.724847642331526
Validation loss: 2.11340613879187
Epoch: 7| Step: 8
Training loss: 2.53851152750959
Validation loss: 2.1317290996880542
Epoch: 7| Step: 9
Training loss: 2.650719663678639
Validation loss: 2.0759469819801963
Epoch: 7| Step: 10
Training loss: 2.1198125239915293
Validation loss: 2.1452936904096434
Epoch: 7| Step: 11
Training loss: 2.494294809282089
Validation loss: 2.0777736845496855
Epoch: 7| Step: 12
Training loss: 2.5836046650635494
Validation loss: 2.100575095232535
Epoch: 7| Step: 13
Training loss: 2.9188866658858363
Validation loss: 2.136798940352079
Epoch: 7| Step: 14
Training loss: 1.9725751501352005
Validation loss: 2.0660483084246337
Epoch: 7| Step: 15
Training loss: 2.612440033730935
Validation loss: 2.063301906939009
Epoch: 99| Step: 0
Training loss: 3.0061918574240365
Validation loss: 2.1137646385063076
Epoch: 7| Step: 1
Training loss: 2.839658539621105
Validation loss: 2.089329585075713
Epoch: 7| Step: 2
Training loss: 2.042314997387393
Validation loss: 2.1346168786104007
Epoch: 7| Step: 3
Training loss: 2.523343961416998
Validation loss: 2.0904878955512882
Epoch: 7| Step: 4
Training loss: 2.488029814612043
Validation loss: 2.138252015159872
Epoch: 7| Step: 5
Training loss: 2.4709049911357837
Validation loss: 2.117435564190355
Epoch: 7| Step: 6
Training loss: 2.8318364359179506
Validation loss: 2.1099974686781073
Epoch: 7| Step: 7
Training loss: 2.8878198157553117
Validation loss: 2.084574975894784
Epoch: 7| Step: 8
Training loss: 2.8796764984169085
Validation loss: 2.1288402336549375
Epoch: 7| Step: 9
Training loss: 2.336096512603606
Validation loss: 2.1251061043127732
Epoch: 7| Step: 10
Training loss: 2.7945313250781894
Validation loss: 2.0653578316669945
Epoch: 7| Step: 11
Training loss: 1.761156893368067
Validation loss: 2.128028613297866
Epoch: 7| Step: 12
Training loss: 2.15744096235389
Validation loss: 2.123663551708645
Epoch: 7| Step: 13
Training loss: 2.375686044488269
Validation loss: 2.0884209997902112
Epoch: 7| Step: 14
Training loss: 2.536304372596649
Validation loss: 2.111143910054941
Epoch: 7| Step: 15
Training loss: 2.377334150432219
Validation loss: 2.106571056783516
Epoch: 100| Step: 0
Training loss: 2.5417230839539053
Validation loss: 2.1670839186134114
Epoch: 7| Step: 1
Training loss: 1.367179914180964
Validation loss: 2.1225549262622243
Epoch: 7| Step: 2
Training loss: 2.8350692554875345
Validation loss: 2.100305982887175
Epoch: 7| Step: 3
Training loss: 2.8824793511980284
Validation loss: 2.091222081756008
Epoch: 7| Step: 4
Training loss: 2.8993681778429723
Validation loss: 2.1189551969095497
Epoch: 7| Step: 5
Training loss: 2.850060472348775
Validation loss: 2.1362913861218313
Epoch: 7| Step: 6
Training loss: 2.504763927947565
Validation loss: 2.111392792800305
Epoch: 7| Step: 7
Training loss: 1.9951918145038607
Validation loss: 2.106349372559633
Epoch: 7| Step: 8
Training loss: 2.740123700322014
Validation loss: 2.1109901675155456
Epoch: 7| Step: 9
Training loss: 2.117280514634026
Validation loss: 2.0934013214713243
Epoch: 7| Step: 10
Training loss: 2.1032753243081967
Validation loss: 2.13577120441948
Epoch: 7| Step: 11
Training loss: 2.466447939926698
Validation loss: 2.0995315822668648
Epoch: 7| Step: 12
Training loss: 2.2780201661487482
Validation loss: 2.0966234134097297
Epoch: 7| Step: 13
Training loss: 2.5155430182135934
Validation loss: 2.089511055641442
Epoch: 7| Step: 14
Training loss: 2.491517553896523
Validation loss: 2.113317688363573
Epoch: 7| Step: 15
Training loss: 3.0677286783179527
Validation loss: 2.1112570405874953

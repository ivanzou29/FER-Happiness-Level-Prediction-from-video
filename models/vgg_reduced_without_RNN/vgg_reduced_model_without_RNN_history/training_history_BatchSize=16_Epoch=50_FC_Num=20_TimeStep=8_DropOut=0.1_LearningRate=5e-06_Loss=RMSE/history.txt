Epoch: 1| Step: 0
Training loss: 5.415738637604729
Validation loss: 6.755422321617517

Epoch: 6| Step: 1
Training loss: 8.270796486670784
Validation loss: 6.734924860409818

Epoch: 6| Step: 2
Training loss: 6.78296692379571
Validation loss: 6.7133659614040315

Epoch: 6| Step: 3
Training loss: 6.105252565350491
Validation loss: 6.695766580466919

Epoch: 6| Step: 4
Training loss: 6.966272572347862
Validation loss: 6.672010457601669

Epoch: 6| Step: 5
Training loss: 7.503418715454549
Validation loss: 6.650754854196269

Epoch: 6| Step: 6
Training loss: 5.742065594475543
Validation loss: 6.628688049621581

Epoch: 6| Step: 7
Training loss: 6.790417078436427
Validation loss: 6.603073591232872

Epoch: 6| Step: 8
Training loss: 7.392160139273107
Validation loss: 6.576902830224585

Epoch: 6| Step: 9
Training loss: 6.640431408023726
Validation loss: 6.546699903964467

Epoch: 6| Step: 10
Training loss: 5.948970596638377
Validation loss: 6.517217866785223

Epoch: 6| Step: 11
Training loss: 6.703842018032225
Validation loss: 6.485247425387694

Epoch: 6| Step: 12
Training loss: 6.697266764180422
Validation loss: 6.451910488074544

Epoch: 6| Step: 13
Training loss: 6.416998652337521
Validation loss: 6.41530816035502

Epoch: 2| Step: 0
Training loss: 5.209069324626019
Validation loss: 6.382448868527414

Epoch: 6| Step: 1
Training loss: 6.684352811586596
Validation loss: 6.346523536756495

Epoch: 6| Step: 2
Training loss: 6.231490842398509
Validation loss: 6.308981390067375

Epoch: 6| Step: 3
Training loss: 5.92137896792934
Validation loss: 6.2687239874886105

Epoch: 6| Step: 4
Training loss: 6.637863051548539
Validation loss: 6.231168503963309

Epoch: 6| Step: 5
Training loss: 5.799593141689412
Validation loss: 6.181225919146249

Epoch: 6| Step: 6
Training loss: 6.196283136940626
Validation loss: 6.141716029444561

Epoch: 6| Step: 7
Training loss: 6.010225166253945
Validation loss: 6.0961891704937665

Epoch: 6| Step: 8
Training loss: 7.192295232940138
Validation loss: 6.045759020856271

Epoch: 6| Step: 9
Training loss: 6.442496721579946
Validation loss: 5.999032949292815

Epoch: 6| Step: 10
Training loss: 5.40312766168285
Validation loss: 5.940408308935653

Epoch: 6| Step: 11
Training loss: 7.0148536580945455
Validation loss: 5.883156221369673

Epoch: 6| Step: 12
Training loss: 4.893610131614415
Validation loss: 5.832867612867053

Epoch: 6| Step: 13
Training loss: 6.598233836285342
Validation loss: 5.773777216585437

Epoch: 3| Step: 0
Training loss: 5.812234523822323
Validation loss: 5.712829777320188

Epoch: 6| Step: 1
Training loss: 6.478623378663739
Validation loss: 5.644138612037743

Epoch: 6| Step: 2
Training loss: 5.2947374379995
Validation loss: 5.5742762918404996

Epoch: 6| Step: 3
Training loss: 5.369856924356693
Validation loss: 5.502768080063774

Epoch: 6| Step: 4
Training loss: 5.431723496985564
Validation loss: 5.437202123908822

Epoch: 6| Step: 5
Training loss: 5.380444452865951
Validation loss: 5.358378622635892

Epoch: 6| Step: 6
Training loss: 5.826328031242038
Validation loss: 5.278062479248524

Epoch: 6| Step: 7
Training loss: 5.512107354191395
Validation loss: 5.198058111780129

Epoch: 6| Step: 8
Training loss: 4.997093882010323
Validation loss: 5.116225383670433

Epoch: 6| Step: 9
Training loss: 4.694655386695503
Validation loss: 5.020291416580249

Epoch: 6| Step: 10
Training loss: 5.691069771711148
Validation loss: 4.924222775755765

Epoch: 6| Step: 11
Training loss: 3.6700801277248565
Validation loss: 4.833552530952607

Epoch: 6| Step: 12
Training loss: 4.855978806237315
Validation loss: 4.731112422983746

Epoch: 6| Step: 13
Training loss: 4.818517712487591
Validation loss: 4.638083090374138

Epoch: 4| Step: 0
Training loss: 4.34854388365589
Validation loss: 4.522198390403226

Epoch: 6| Step: 1
Training loss: 4.406726135068094
Validation loss: 4.409948019152424

Epoch: 6| Step: 2
Training loss: 4.088979023018288
Validation loss: 4.309440504225866

Epoch: 6| Step: 3
Training loss: 4.751908370691942
Validation loss: 4.1920233251818155

Epoch: 6| Step: 4
Training loss: 3.621021455209753
Validation loss: 4.080816420949226

Epoch: 6| Step: 5
Training loss: 3.005914421147612
Validation loss: 3.9487322297903464

Epoch: 6| Step: 6
Training loss: 4.82948693877485
Validation loss: 3.8120452109314686

Epoch: 6| Step: 7
Training loss: 3.9284479146589786
Validation loss: 3.7110924618138843

Epoch: 6| Step: 8
Training loss: 4.221871132781815
Validation loss: 3.586019001175168

Epoch: 6| Step: 9
Training loss: 3.3161569238765547
Validation loss: 3.4557939114670475

Epoch: 6| Step: 10
Training loss: 3.1288879622592902
Validation loss: 3.318536236925577

Epoch: 6| Step: 11
Training loss: 2.9979710075396557
Validation loss: 3.1929292752773937

Epoch: 6| Step: 12
Training loss: 3.028856099057667
Validation loss: 3.096330189698953

Epoch: 6| Step: 13
Training loss: 2.9806767262381695
Validation loss: 3.005298729941323

Epoch: 5| Step: 0
Training loss: 2.8109086514954558
Validation loss: 2.9387230457544713

Epoch: 6| Step: 1
Training loss: 2.976687771621619
Validation loss: 2.8463253893477987

Epoch: 6| Step: 2
Training loss: 3.6036924075877868
Validation loss: 2.7805396040709667

Epoch: 6| Step: 3
Training loss: 2.543638079756264
Validation loss: 2.7387649899723523

Epoch: 6| Step: 4
Training loss: 2.8058015528768205
Validation loss: 2.7187220107936145

Epoch: 6| Step: 5
Training loss: 2.5400966944274024
Validation loss: 2.7101211711262443

Epoch: 6| Step: 6
Training loss: 3.0311276126025835
Validation loss: 2.751668618889094

Epoch: 6| Step: 7
Training loss: 2.1825685773040466
Validation loss: 2.7693676382357393

Epoch: 6| Step: 8
Training loss: 2.8505625186388137
Validation loss: 2.7893104621087517

Epoch: 6| Step: 9
Training loss: 2.4387157049461057
Validation loss: 2.811168270593787

Epoch: 6| Step: 10
Training loss: 2.4956897772854343
Validation loss: 2.8525684272032157

Epoch: 6| Step: 11
Training loss: 2.8330502275134073
Validation loss: 2.903914808921458

Epoch: 6| Step: 12
Training loss: 3.970308973843008
Validation loss: 2.8927859090683405

Epoch: 6| Step: 13
Training loss: 2.563044420420724
Validation loss: 2.9006944970280664

Epoch: 6| Step: 0
Training loss: 3.443739759149573
Validation loss: 2.8650358409093295

Epoch: 6| Step: 1
Training loss: 2.2529430215388895
Validation loss: 2.8424108796699983

Epoch: 6| Step: 2
Training loss: 2.4275114366790365
Validation loss: 2.784329028023

Epoch: 6| Step: 3
Training loss: 3.299774774899565
Validation loss: 2.8040080544750134

Epoch: 6| Step: 4
Training loss: 2.7487202180686268
Validation loss: 2.7635591481721473

Epoch: 6| Step: 5
Training loss: 2.4995875972103736
Validation loss: 2.727340317680904

Epoch: 6| Step: 6
Training loss: 2.8678662902436405
Validation loss: 2.7193821139320393

Epoch: 6| Step: 7
Training loss: 2.8489908405985536
Validation loss: 2.7172714685470623

Epoch: 6| Step: 8
Training loss: 2.8746700926862316
Validation loss: 2.7132616588098695

Epoch: 6| Step: 9
Training loss: 2.644515562926608
Validation loss: 2.7101240889091716

Epoch: 6| Step: 10
Training loss: 2.0524934723502843
Validation loss: 2.717494148381451

Epoch: 6| Step: 11
Training loss: 2.5072256572187155
Validation loss: 2.6974732661815426

Epoch: 6| Step: 12
Training loss: 2.49637245210259
Validation loss: 2.6936195179668414

Epoch: 6| Step: 13
Training loss: 2.9016858429542594
Validation loss: 2.7097171378227833

Epoch: 7| Step: 0
Training loss: 2.454864079894615
Validation loss: 2.7388656797923807

Epoch: 6| Step: 1
Training loss: 2.8385717174560874
Validation loss: 2.7124180630498227

Epoch: 6| Step: 2
Training loss: 2.55501878556066
Validation loss: 2.732692978638821

Epoch: 6| Step: 3
Training loss: 2.7371919780911895
Validation loss: 2.7109799193948714

Epoch: 6| Step: 4
Training loss: 3.0447023127823045
Validation loss: 2.7111992173739394

Epoch: 6| Step: 5
Training loss: 2.346157007828611
Validation loss: 2.7254749012600126

Epoch: 6| Step: 6
Training loss: 2.3887338575679085
Validation loss: 2.6976926457740955

Epoch: 6| Step: 7
Training loss: 3.4260721463759123
Validation loss: 2.7483147023180403

Epoch: 6| Step: 8
Training loss: 2.62404288145243
Validation loss: 2.6946757720889063

Epoch: 6| Step: 9
Training loss: 2.198002480613968
Validation loss: 2.698530021867552

Epoch: 6| Step: 10
Training loss: 3.2491236752365125
Validation loss: 2.704703867668286

Epoch: 6| Step: 11
Training loss: 2.337587418171041
Validation loss: 2.73024172878302

Epoch: 6| Step: 12
Training loss: 2.568093962789346
Validation loss: 2.6960969087576125

Epoch: 6| Step: 13
Training loss: 2.3784989131967507
Validation loss: 2.686535359038056

Epoch: 8| Step: 0
Training loss: 2.2173433814923107
Validation loss: 2.7197738415063224

Epoch: 6| Step: 1
Training loss: 2.704910041737585
Validation loss: 2.6839487728473808

Epoch: 6| Step: 2
Training loss: 2.814871911070275
Validation loss: 2.683259486925751

Epoch: 6| Step: 3
Training loss: 3.4227291068947556
Validation loss: 2.6643714912040073

Epoch: 6| Step: 4
Training loss: 2.852046392483462
Validation loss: 2.697540055626608

Epoch: 6| Step: 5
Training loss: 2.594247977010514
Validation loss: 2.6730082675840707

Epoch: 6| Step: 6
Training loss: 2.5044076212278545
Validation loss: 2.7153221532171745

Epoch: 6| Step: 7
Training loss: 2.6418965004018324
Validation loss: 2.7245625736861157

Epoch: 6| Step: 8
Training loss: 2.555130759698555
Validation loss: 2.7018621798806874

Epoch: 6| Step: 9
Training loss: 2.315201701858412
Validation loss: 2.71533203125

Epoch: 6| Step: 10
Training loss: 2.5801644321649886
Validation loss: 2.7128745861966266

Epoch: 6| Step: 11
Training loss: 3.0843826432988504
Validation loss: 2.731832837521477

Epoch: 6| Step: 12
Training loss: 2.6548083712860957
Validation loss: 2.730165682132892

Epoch: 6| Step: 13
Training loss: 2.163094534760003
Validation loss: 2.697232757449132

Epoch: 9| Step: 0
Training loss: 3.22515409825942
Validation loss: 2.724742774022086

Epoch: 6| Step: 1
Training loss: 2.776240090632026
Validation loss: 2.6902766558631086

Epoch: 6| Step: 2
Training loss: 3.581381436211198
Validation loss: 2.7028371992131976

Epoch: 6| Step: 3
Training loss: 1.8665561007497453
Validation loss: 2.6699033828165653

Epoch: 6| Step: 4
Training loss: 2.182782235615009
Validation loss: 2.7116326605179335

Epoch: 6| Step: 5
Training loss: 2.3403034554672337
Validation loss: 2.7021663928495196

Epoch: 6| Step: 6
Training loss: 2.7029882848487867
Validation loss: 2.6838079271726265

Epoch: 6| Step: 7
Training loss: 2.2770416949328456
Validation loss: 2.6833514493326778

Epoch: 6| Step: 8
Training loss: 2.465515915617576
Validation loss: 2.6858454349902865

Epoch: 6| Step: 9
Training loss: 2.1816567989832416
Validation loss: 2.6905016290054444

Epoch: 6| Step: 10
Training loss: 1.490921442076913
Validation loss: 2.6832425453591675

Epoch: 6| Step: 11
Training loss: 3.242389571545497
Validation loss: 2.692573120933069

Epoch: 6| Step: 12
Training loss: 3.424165002991591
Validation loss: 2.706275280253656

Epoch: 6| Step: 13
Training loss: 2.452808529795567
Validation loss: 2.6952757459705334

Epoch: 10| Step: 0
Training loss: 1.892016820667491
Validation loss: 2.6737299349143275

Epoch: 6| Step: 1
Training loss: 2.799694664519432
Validation loss: 2.681999832622685

Epoch: 6| Step: 2
Training loss: 2.5000326154488675
Validation loss: 2.680859293773942

Epoch: 6| Step: 3
Training loss: 2.8073614863180802
Validation loss: 2.677240052936183

Epoch: 6| Step: 4
Training loss: 2.8254712547253593
Validation loss: 2.698341341114787

Epoch: 6| Step: 5
Training loss: 2.266629752625214
Validation loss: 2.68824363001492

Epoch: 6| Step: 6
Training loss: 2.7055966758170706
Validation loss: 2.6902962856370545

Epoch: 6| Step: 7
Training loss: 2.3951551610901998
Validation loss: 2.690535317232228

Epoch: 6| Step: 8
Training loss: 2.687691260340943
Validation loss: 2.6728437128812392

Epoch: 6| Step: 9
Training loss: 3.5564463529365824
Validation loss: 2.6768778908911597

Epoch: 6| Step: 10
Training loss: 2.419909855133082
Validation loss: 2.7058311249826072

Epoch: 6| Step: 11
Training loss: 2.238988208802743
Validation loss: 2.685483397687305

Epoch: 6| Step: 12
Training loss: 2.8250989373360547
Validation loss: 2.6844152741377116

Epoch: 6| Step: 13
Training loss: 2.5390788385892584
Validation loss: 2.635234725964339

Epoch: 11| Step: 0
Training loss: 2.6961223767688876
Validation loss: 2.680462916875689

Epoch: 6| Step: 1
Training loss: 3.429670398991036
Validation loss: 2.6726121982280766

Epoch: 6| Step: 2
Training loss: 2.400826224524334
Validation loss: 2.684501172476683

Epoch: 6| Step: 3
Training loss: 2.8035139853759836
Validation loss: 2.6598993252941123

Epoch: 6| Step: 4
Training loss: 2.48844614485643
Validation loss: 2.692574936140681

Epoch: 6| Step: 5
Training loss: 2.8710964073116956
Validation loss: 2.653275623396814

Epoch: 6| Step: 6
Training loss: 2.277255388642771
Validation loss: 2.6943099216644497

Epoch: 6| Step: 7
Training loss: 2.407772127737612
Validation loss: 2.713343949293259

Epoch: 6| Step: 8
Training loss: 2.5438349079329736
Validation loss: 2.693927781448031

Epoch: 6| Step: 9
Training loss: 2.895549806029801
Validation loss: 2.668762863330677

Epoch: 6| Step: 10
Training loss: 1.9633589304543686
Validation loss: 2.6674007706685776

Epoch: 6| Step: 11
Training loss: 2.9311807717808724
Validation loss: 2.7042234390325994

Epoch: 6| Step: 12
Training loss: 2.6048716900466413
Validation loss: 2.647039233760609

Epoch: 6| Step: 13
Training loss: 2.448208200396072
Validation loss: 2.658403264344651

Epoch: 12| Step: 0
Training loss: 2.910354341255133
Validation loss: 2.695577591838043

Epoch: 6| Step: 1
Training loss: 2.6664693481240063
Validation loss: 2.6571362980235724

Epoch: 6| Step: 2
Training loss: 1.713464412474377
Validation loss: 2.7092124734755196

Epoch: 6| Step: 3
Training loss: 3.123630834094759
Validation loss: 2.6876895601118482

Epoch: 6| Step: 4
Training loss: 2.624776739889535
Validation loss: 2.651704961291216

Epoch: 6| Step: 5
Training loss: 2.6951327498939235
Validation loss: 2.718426191843844

Epoch: 6| Step: 6
Training loss: 2.1039479837623567
Validation loss: 2.699956150052067

Epoch: 6| Step: 7
Training loss: 2.947223397591377
Validation loss: 2.7025018462516788

Epoch: 6| Step: 8
Training loss: 2.8538723721961827
Validation loss: 2.7102079115124496

Epoch: 6| Step: 9
Training loss: 3.4034387028816107
Validation loss: 2.678003210015969

Epoch: 6| Step: 10
Training loss: 2.464627070507765
Validation loss: 2.694588841395846

Epoch: 6| Step: 11
Training loss: 2.4363816458930607
Validation loss: 2.671550455887643

Epoch: 6| Step: 12
Training loss: 2.66775909538388
Validation loss: 2.6523354052195605

Epoch: 6| Step: 13
Training loss: 2.4214792235840106
Validation loss: 2.6947085528587422

Epoch: 13| Step: 0
Training loss: 3.28293283088566
Validation loss: 2.6305116283433088

Epoch: 6| Step: 1
Training loss: 2.5167965263900505
Validation loss: 2.684229923897641

Epoch: 6| Step: 2
Training loss: 2.661300166756032
Validation loss: 2.66769830795444

Epoch: 6| Step: 3
Training loss: 2.853819573081081
Validation loss: 2.66029981168827

Epoch: 6| Step: 4
Training loss: 2.3808535668894497
Validation loss: 2.658309736316509

Epoch: 6| Step: 5
Training loss: 2.7659386925678056
Validation loss: 2.6763972035214914

Epoch: 6| Step: 6
Training loss: 2.4335309505779157
Validation loss: 2.6552145042861537

Epoch: 6| Step: 7
Training loss: 2.6892231584574544
Validation loss: 2.627537560477291

Epoch: 6| Step: 8
Training loss: 2.198305092669474
Validation loss: 2.6665056199875847

Epoch: 6| Step: 9
Training loss: 2.3219099614628957
Validation loss: 2.665591996925158

Epoch: 6| Step: 10
Training loss: 2.6840013308344126
Validation loss: 2.6443876587178488

Epoch: 6| Step: 11
Training loss: 2.3675427060167746
Validation loss: 2.6334214378860867

Epoch: 6| Step: 12
Training loss: 2.6350349984206303
Validation loss: 2.6618670443991963

Epoch: 6| Step: 13
Training loss: 2.7009141292882792
Validation loss: 2.6476577165962962

Epoch: 14| Step: 0
Training loss: 2.3203944952762874
Validation loss: 2.656110662657855

Epoch: 6| Step: 1
Training loss: 2.082762169426584
Validation loss: 2.6799059334895503

Epoch: 6| Step: 2
Training loss: 2.2188806629659865
Validation loss: 2.647807943861774

Epoch: 6| Step: 3
Training loss: 2.2816239142005963
Validation loss: 2.6738924288581507

Epoch: 6| Step: 4
Training loss: 3.12362335400189
Validation loss: 2.6445411670673544

Epoch: 6| Step: 5
Training loss: 3.5294084726580057
Validation loss: 2.6479105767500926

Epoch: 6| Step: 6
Training loss: 2.446212651625998
Validation loss: 2.6694350525497508

Epoch: 6| Step: 7
Training loss: 2.311495176012805
Validation loss: 2.6531079726464704

Epoch: 6| Step: 8
Training loss: 2.89976746514763
Validation loss: 2.674656541112342

Epoch: 6| Step: 9
Training loss: 2.6102470722399453
Validation loss: 2.641182639326247

Epoch: 6| Step: 10
Training loss: 3.464239814016009
Validation loss: 2.6381628908932075

Epoch: 6| Step: 11
Training loss: 2.4082336176344046
Validation loss: 2.6632304155312463

Epoch: 6| Step: 12
Training loss: 2.0452457379197746
Validation loss: 2.629577203448392

Epoch: 6| Step: 13
Training loss: 2.439929631578678
Validation loss: 2.6692697714206477

Epoch: 15| Step: 0
Training loss: 3.30401561210583
Validation loss: 2.661419001597839

Epoch: 6| Step: 1
Training loss: 1.9635761639088378
Validation loss: 2.6367094967526676

Epoch: 6| Step: 2
Training loss: 2.7667827015025925
Validation loss: 2.6618121385403093

Epoch: 6| Step: 3
Training loss: 3.0678558227800137
Validation loss: 2.6451362732923047

Epoch: 6| Step: 4
Training loss: 2.914455847489011
Validation loss: 2.6645703022876566

Epoch: 6| Step: 5
Training loss: 2.716539997431973
Validation loss: 2.6665990840772102

Epoch: 6| Step: 6
Training loss: 2.5852730349702404
Validation loss: 2.6338040138454857

Epoch: 6| Step: 7
Training loss: 2.0449071845371325
Validation loss: 2.702732211995765

Epoch: 6| Step: 8
Training loss: 3.1380411783194644
Validation loss: 2.6403204838255827

Epoch: 6| Step: 9
Training loss: 2.696903544216337
Validation loss: 2.6253265450229843

Epoch: 6| Step: 10
Training loss: 1.7441012558546674
Validation loss: 2.6595700178650223

Epoch: 6| Step: 11
Training loss: 3.0371784558059987
Validation loss: 2.6542815300856035

Epoch: 6| Step: 12
Training loss: 2.2141820602063746
Validation loss: 2.6448339291882257

Epoch: 6| Step: 13
Training loss: 1.9122756402551788
Validation loss: 2.6080458371906605

Epoch: 16| Step: 0
Training loss: 2.993316994971507
Validation loss: 2.6722680856765595

Epoch: 6| Step: 1
Training loss: 2.5826965490623506
Validation loss: 2.656548157956025

Epoch: 6| Step: 2
Training loss: 2.955834815648104
Validation loss: 2.624967272115469

Epoch: 6| Step: 3
Training loss: 2.2147274895081797
Validation loss: 2.6244213283853006

Epoch: 6| Step: 4
Training loss: 2.2654239960000493
Validation loss: 2.6390931842315792

Epoch: 6| Step: 5
Training loss: 2.1236274999143174
Validation loss: 2.6917576442810844

Epoch: 6| Step: 6
Training loss: 3.090519615359191
Validation loss: 2.6362582937308803

Epoch: 6| Step: 7
Training loss: 3.169458229628583
Validation loss: 2.6326657262805115

Epoch: 6| Step: 8
Training loss: 2.647306757890656
Validation loss: 2.6162975411333935

Epoch: 6| Step: 9
Training loss: 2.3143033393646215
Validation loss: 2.6784014938581624

Epoch: 6| Step: 10
Training loss: 2.349344925176721
Validation loss: 2.6278757812535614

Epoch: 6| Step: 11
Training loss: 2.7792194969573862
Validation loss: 2.6659584793841122

Epoch: 6| Step: 12
Training loss: 2.4658077428516263
Validation loss: 2.6266186128939064

Epoch: 6| Step: 13
Training loss: 2.347367305397469
Validation loss: 2.646966095985268

Epoch: 17| Step: 0
Training loss: 2.6523567839851414
Validation loss: 2.6634393822806817

Epoch: 6| Step: 1
Training loss: 2.5941262948872947
Validation loss: 2.6298914400571363

Epoch: 6| Step: 2
Training loss: 2.8887349315928734
Validation loss: 2.657769430575168

Epoch: 6| Step: 3
Training loss: 2.526817302323476
Validation loss: 2.656480203264826

Epoch: 6| Step: 4
Training loss: 2.7807465858483096
Validation loss: 2.645464974140211

Epoch: 6| Step: 5
Training loss: 2.786727825692223
Validation loss: 2.6336688149903766

Epoch: 6| Step: 6
Training loss: 2.294787957362639
Validation loss: 2.703293194251355

Epoch: 6| Step: 7
Training loss: 2.6627931675222034
Validation loss: 2.6784583962524073

Epoch: 6| Step: 8
Training loss: 2.666730870030309
Validation loss: 2.689870542935151

Epoch: 6| Step: 9
Training loss: 2.2872023592061126
Validation loss: 2.663842991113196

Epoch: 6| Step: 10
Training loss: 2.6800981657354486
Validation loss: 2.6773730961074684

Epoch: 6| Step: 11
Training loss: 2.094782218994
Validation loss: 2.6711216212073103

Epoch: 6| Step: 12
Training loss: 3.3550060161872057
Validation loss: 2.660192032852041

Epoch: 6| Step: 13
Training loss: 2.172150299407887
Validation loss: 2.663404068260163

Epoch: 18| Step: 0
Training loss: 2.6722642046229415
Validation loss: 2.641751652823659

Epoch: 6| Step: 1
Training loss: 2.291128742778351
Validation loss: 2.690841165174623

Epoch: 6| Step: 2
Training loss: 2.1063035997764312
Validation loss: 2.648494626069602

Epoch: 6| Step: 3
Training loss: 2.4787366691842623
Validation loss: 2.6256518387204086

Epoch: 6| Step: 4
Training loss: 3.2851018186725067
Validation loss: 2.640282422478091

Epoch: 6| Step: 5
Training loss: 2.550330409915066
Validation loss: 2.6364920662347546

Epoch: 6| Step: 6
Training loss: 2.6787080893395356
Validation loss: 2.6373323456569837

Epoch: 6| Step: 7
Training loss: 1.968523527168055
Validation loss: 2.6548854988298025

Epoch: 6| Step: 8
Training loss: 2.359843630701989
Validation loss: 2.614457075457151

Epoch: 6| Step: 9
Training loss: 2.80082917538211
Validation loss: 2.684084355551833

Epoch: 6| Step: 10
Training loss: 1.921260518409463
Validation loss: 2.6577656928113376

Epoch: 6| Step: 11
Training loss: 3.4418406044616585
Validation loss: 2.6371066940686516

Epoch: 6| Step: 12
Training loss: 2.778908075734746
Validation loss: 2.6196537373347457

Epoch: 6| Step: 13
Training loss: 2.770356349953034
Validation loss: 2.641690605388042

Epoch: 19| Step: 0
Training loss: 2.3496123176740196
Validation loss: 2.647545986106355

Epoch: 6| Step: 1
Training loss: 2.6768474153303803
Validation loss: 2.62902875579879

Epoch: 6| Step: 2
Training loss: 2.6826194982584695
Validation loss: 2.641465950409401

Epoch: 6| Step: 3
Training loss: 2.8433036034637116
Validation loss: 2.6529846091357823

Epoch: 6| Step: 4
Training loss: 2.3366171572125123
Validation loss: 2.5916532597905437

Epoch: 6| Step: 5
Training loss: 2.9730217968308192
Validation loss: 2.5901397796725147

Epoch: 6| Step: 6
Training loss: 2.7543868707375525
Validation loss: 2.667580914734487

Epoch: 6| Step: 7
Training loss: 2.5291458615029154
Validation loss: 2.6538034578827565

Epoch: 6| Step: 8
Training loss: 2.793188809348629
Validation loss: 2.6696570338786976

Epoch: 6| Step: 9
Training loss: 2.1705153346456987
Validation loss: 2.630499845660288

Epoch: 6| Step: 10
Training loss: 2.940510282802392
Validation loss: 2.6249823645347505

Epoch: 6| Step: 11
Training loss: 1.9468775815012855
Validation loss: 2.6345253192193905

Epoch: 6| Step: 12
Training loss: 1.9403260447637154
Validation loss: 2.626616086456705

Epoch: 6| Step: 13
Training loss: 3.069806161532804
Validation loss: 2.615271372648546

Epoch: 20| Step: 0
Training loss: 2.3838902771937156
Validation loss: 2.6481356603512753

Epoch: 6| Step: 1
Training loss: 2.615219970845788
Validation loss: 2.6503519586241224

Epoch: 6| Step: 2
Training loss: 2.751911885891327
Validation loss: 2.657709962129125

Epoch: 6| Step: 3
Training loss: 2.3996170572899675
Validation loss: 2.681198909838898

Epoch: 6| Step: 4
Training loss: 2.131801826878674
Validation loss: 2.613254761763568

Epoch: 6| Step: 5
Training loss: 2.4326805005435075
Validation loss: 2.600106138117034

Epoch: 6| Step: 6
Training loss: 2.5010130736975182
Validation loss: 2.6151911319104255

Epoch: 6| Step: 7
Training loss: 3.532444979596216
Validation loss: 2.6121190123363385

Epoch: 6| Step: 8
Training loss: 2.2846624900587496
Validation loss: 2.6334369420742485

Epoch: 6| Step: 9
Training loss: 2.310271142676882
Validation loss: 2.6021311828595532

Epoch: 6| Step: 10
Training loss: 2.7731794210359566
Validation loss: 2.612695176835898

Epoch: 6| Step: 11
Training loss: 2.843471471539753
Validation loss: 2.648709953645059

Epoch: 6| Step: 12
Training loss: 2.713801912701186
Validation loss: 2.614855449442529

Epoch: 6| Step: 13
Training loss: 2.1305711308252206
Validation loss: 2.624305981519394

Epoch: 21| Step: 0
Training loss: 2.949180916044576
Validation loss: 2.639164582847814

Epoch: 6| Step: 1
Training loss: 2.7952154766407635
Validation loss: 2.634599662061296

Epoch: 6| Step: 2
Training loss: 2.9495291641275667
Validation loss: 2.6090216026157487

Epoch: 6| Step: 3
Training loss: 2.4548249398647717
Validation loss: 2.6105743362616414

Epoch: 6| Step: 4
Training loss: 2.421282492731266
Validation loss: 2.585109074749035

Epoch: 6| Step: 5
Training loss: 2.159760312980954
Validation loss: 2.6256561064850623

Epoch: 6| Step: 6
Training loss: 2.543015252551001
Validation loss: 2.6129435276829986

Epoch: 6| Step: 7
Training loss: 2.2223415051341444
Validation loss: 2.637533179823218

Epoch: 6| Step: 8
Training loss: 3.084626897325368
Validation loss: 2.6472624175827337

Epoch: 6| Step: 9
Training loss: 2.675676017975219
Validation loss: 2.622122777204403

Epoch: 6| Step: 10
Training loss: 1.935855536769395
Validation loss: 2.6541722268166583

Epoch: 6| Step: 11
Training loss: 3.334249243469679
Validation loss: 2.6482759349533413

Epoch: 6| Step: 12
Training loss: 1.7254020277721323
Validation loss: 2.6479379863666086

Epoch: 6| Step: 13
Training loss: 2.8303914972724233
Validation loss: 2.66627889038973

Epoch: 22| Step: 0
Training loss: 2.857726742483366
Validation loss: 2.6522965874128266

Epoch: 6| Step: 1
Training loss: 2.2048585172489426
Validation loss: 2.6602467406235326

Epoch: 6| Step: 2
Training loss: 2.2452196626311594
Validation loss: 2.62607142365869

Epoch: 6| Step: 3
Training loss: 2.132704959156343
Validation loss: 2.6650712386505004

Epoch: 6| Step: 4
Training loss: 3.417110104201121
Validation loss: 2.6082427257932577

Epoch: 6| Step: 5
Training loss: 2.7636315009197547
Validation loss: 2.6435398664600385

Epoch: 6| Step: 6
Training loss: 2.2628786476222595
Validation loss: 2.646364629272208

Epoch: 6| Step: 7
Training loss: 2.4544287454749565
Validation loss: 2.6668192005242712

Epoch: 6| Step: 8
Training loss: 3.059327331109254
Validation loss: 2.630669700017412

Epoch: 6| Step: 9
Training loss: 2.393588454569802
Validation loss: 2.6571676127867105

Epoch: 6| Step: 10
Training loss: 2.0710239555487493
Validation loss: 2.6335173435947374

Epoch: 6| Step: 11
Training loss: 2.4237959596035066
Validation loss: 2.6075752127489675

Epoch: 6| Step: 12
Training loss: 2.8912057654018795
Validation loss: 2.6129230278455577

Epoch: 6| Step: 13
Training loss: 2.4822914455433462
Validation loss: 2.6075940174136947

Epoch: 23| Step: 0
Training loss: 2.2877717528969694
Validation loss: 2.6454635622049034

Epoch: 6| Step: 1
Training loss: 2.169051166477885
Validation loss: 2.6091675970984958

Epoch: 6| Step: 2
Training loss: 2.439890056540784
Validation loss: 2.618612290663547

Epoch: 6| Step: 3
Training loss: 1.9622168755857483
Validation loss: 2.6384902993277435

Epoch: 6| Step: 4
Training loss: 2.6678835853846388
Validation loss: 2.626190203337418

Epoch: 6| Step: 5
Training loss: 3.390179442211079
Validation loss: 2.5986735481587337

Epoch: 6| Step: 6
Training loss: 2.1159824446903603
Validation loss: 2.640301129646273

Epoch: 6| Step: 7
Training loss: 2.1304885443679593
Validation loss: 2.6336961691409

Epoch: 6| Step: 8
Training loss: 2.4086860116913287
Validation loss: 2.618624900737141

Epoch: 6| Step: 9
Training loss: 2.118526921864471
Validation loss: 2.6024254178253847

Epoch: 6| Step: 10
Training loss: 2.190974718331918
Validation loss: 2.66382646307419

Epoch: 6| Step: 11
Training loss: 3.347675906538703
Validation loss: 2.6066624901226696

Epoch: 6| Step: 12
Training loss: 3.3315489443395956
Validation loss: 2.615553798640736

Epoch: 6| Step: 13
Training loss: 2.991728824688767
Validation loss: 2.637757348737886

Epoch: 24| Step: 0
Training loss: 2.8206698965247834
Validation loss: 2.6098362254259038

Epoch: 6| Step: 1
Training loss: 3.189692472504651
Validation loss: 2.674126670857684

Epoch: 6| Step: 2
Training loss: 2.3111374423572952
Validation loss: 2.6721316506186055

Epoch: 6| Step: 3
Training loss: 2.4464069878637433
Validation loss: 2.6000306867353578

Epoch: 6| Step: 4
Training loss: 2.287893993029363
Validation loss: 2.6114906825367377

Epoch: 6| Step: 5
Training loss: 2.4543986324749136
Validation loss: 2.6389156278850576

Epoch: 6| Step: 6
Training loss: 2.2830368777691494
Validation loss: 2.577000135611966

Epoch: 6| Step: 7
Training loss: 2.3241383258144364
Validation loss: 2.635283430347707

Epoch: 6| Step: 8
Training loss: 2.307615741046415
Validation loss: 2.684850451248989

Epoch: 6| Step: 9
Training loss: 3.2191267070879053
Validation loss: 2.5984152237240647

Epoch: 6| Step: 10
Training loss: 2.3929876483067627
Validation loss: 2.619651628901141

Epoch: 6| Step: 11
Training loss: 2.7614028512587034
Validation loss: 2.5642314271762854

Epoch: 6| Step: 12
Training loss: 3.252979673278138
Validation loss: 2.6372131034933255

Epoch: 6| Step: 13
Training loss: 1.888961564641825
Validation loss: 2.580228005617744

Epoch: 25| Step: 0
Training loss: 3.149496879967311
Validation loss: 2.5805785863054167

Epoch: 6| Step: 1
Training loss: 2.376835766091584
Validation loss: 2.622226430553716

Epoch: 6| Step: 2
Training loss: 2.971535430024291
Validation loss: 2.6176339750788653

Epoch: 6| Step: 3
Training loss: 2.757241684408132
Validation loss: 2.6160087550043016

Epoch: 6| Step: 4
Training loss: 2.5526309820599824
Validation loss: 2.613213310632031

Epoch: 6| Step: 5
Training loss: 1.6763520946887713
Validation loss: 2.6125653345434827

Epoch: 6| Step: 6
Training loss: 2.5134057153790548
Validation loss: 2.644418974182964

Epoch: 6| Step: 7
Training loss: 2.695225457154295
Validation loss: 2.56320954400503

Epoch: 6| Step: 8
Training loss: 2.413552733489112
Validation loss: 2.6132595059450545

Epoch: 6| Step: 9
Training loss: 2.4960895472626983
Validation loss: 2.6194942955279723

Epoch: 6| Step: 10
Training loss: 1.528094407225882
Validation loss: 2.5915122126176766

Epoch: 6| Step: 11
Training loss: 2.7703188272695476
Validation loss: 2.587713517180583

Epoch: 6| Step: 12
Training loss: 2.784501136465829
Validation loss: 2.5920721241861364

Epoch: 6| Step: 13
Training loss: 2.9363358808473135
Validation loss: 2.6107312026403604

Epoch: 26| Step: 0
Training loss: 2.162231221336586
Validation loss: 2.6272854468343834

Epoch: 6| Step: 1
Training loss: 2.5900221543119395
Validation loss: 2.6137330064383804

Epoch: 6| Step: 2
Training loss: 2.248702734916457
Validation loss: 2.6164553249157665

Epoch: 6| Step: 3
Training loss: 2.704471671640337
Validation loss: 2.6479107718373336

Epoch: 6| Step: 4
Training loss: 3.0067536310897323
Validation loss: 2.656163861243098

Epoch: 6| Step: 5
Training loss: 2.6038005927596433
Validation loss: 2.6226792445958336

Epoch: 6| Step: 6
Training loss: 2.977131946790581
Validation loss: 2.6575601824659296

Epoch: 6| Step: 7
Training loss: 2.7616501169700682
Validation loss: 2.6423598841027847

Epoch: 6| Step: 8
Training loss: 2.1981866559234193
Validation loss: 2.614672083063982

Epoch: 6| Step: 9
Training loss: 2.91630030556337
Validation loss: 2.6457555013507683

Epoch: 6| Step: 10
Training loss: 2.459919843220669
Validation loss: 2.630321459965497

Epoch: 6| Step: 11
Training loss: 2.6975172672655954
Validation loss: 2.608684288480444

Epoch: 6| Step: 12
Training loss: 1.9058937615390417
Validation loss: 2.6089890855195597

Epoch: 6| Step: 13
Training loss: 2.63226457548422
Validation loss: 2.605164486773992

Epoch: 27| Step: 0
Training loss: 2.3229296787963807
Validation loss: 2.5974851776620462

Epoch: 6| Step: 1
Training loss: 2.533735773679883
Validation loss: 2.67451923245014

Epoch: 6| Step: 2
Training loss: 2.16327484890372
Validation loss: 2.583265078063354

Epoch: 6| Step: 3
Training loss: 3.056718624204491
Validation loss: 2.6289521091007715

Epoch: 6| Step: 4
Training loss: 2.3036552574476103
Validation loss: 2.602129915389065

Epoch: 6| Step: 5
Training loss: 2.3715004485755413
Validation loss: 2.609712163953046

Epoch: 6| Step: 6
Training loss: 2.731360119448151
Validation loss: 2.6409972566130318

Epoch: 6| Step: 7
Training loss: 2.656349180277287
Validation loss: 2.6140112586660647

Epoch: 6| Step: 8
Training loss: 2.019659219077826
Validation loss: 2.5908945290827043

Epoch: 6| Step: 9
Training loss: 2.447142090233021
Validation loss: 2.623723181262929

Epoch: 6| Step: 10
Training loss: 2.3193162005739167
Validation loss: 2.6442216535113223

Epoch: 6| Step: 11
Training loss: 3.14747623886096
Validation loss: 2.62354266872451

Epoch: 6| Step: 12
Training loss: 2.3678028074974637
Validation loss: 2.6102868350521837

Epoch: 6| Step: 13
Training loss: 3.0401905780584757
Validation loss: 2.633885784586752

Epoch: 28| Step: 0
Training loss: 3.2157700456802467
Validation loss: 2.595230124504245

Epoch: 6| Step: 1
Training loss: 3.0100415974020724
Validation loss: 2.6212601284352295

Epoch: 6| Step: 2
Training loss: 2.815662216973615
Validation loss: 2.6310310562632298

Epoch: 6| Step: 3
Training loss: 2.9812522648256925
Validation loss: 2.6307064428249918

Epoch: 6| Step: 4
Training loss: 2.5187425901999703
Validation loss: 2.6352236429526177

Epoch: 6| Step: 5
Training loss: 2.372671893598618
Validation loss: 2.600314844459956

Epoch: 6| Step: 6
Training loss: 2.6198445511340807
Validation loss: 2.5732256488883842

Epoch: 6| Step: 7
Training loss: 2.5602966698467142
Validation loss: 2.5982943102573577

Epoch: 6| Step: 8
Training loss: 2.5398907528309005
Validation loss: 2.604334823265546

Epoch: 6| Step: 9
Training loss: 2.056598079636206
Validation loss: 2.599778351139166

Epoch: 6| Step: 10
Training loss: 2.1651925672210806
Validation loss: 2.613721125312637

Epoch: 6| Step: 11
Training loss: 1.5947058560112757
Validation loss: 2.602555201076642

Epoch: 6| Step: 12
Training loss: 2.3340399444221607
Validation loss: 2.595784641705215

Epoch: 6| Step: 13
Training loss: 3.0234665191598316
Validation loss: 2.6571798305287886

Epoch: 29| Step: 0
Training loss: 2.96859612317083
Validation loss: 2.624091187844587

Epoch: 6| Step: 1
Training loss: 3.4180471531186316
Validation loss: 2.6528640558837613

Epoch: 6| Step: 2
Training loss: 2.62332081447201
Validation loss: 2.633816060856768

Epoch: 6| Step: 3
Training loss: 2.620560115608692
Validation loss: 2.5909167982440064

Epoch: 6| Step: 4
Training loss: 1.9409214082123614
Validation loss: 2.6047835471345975

Epoch: 6| Step: 5
Training loss: 2.2351977560808938
Validation loss: 2.6283443279274263

Epoch: 6| Step: 6
Training loss: 3.0626201995305045
Validation loss: 2.6208328139926738

Epoch: 6| Step: 7
Training loss: 2.6869884270211886
Validation loss: 2.6323985935639653

Epoch: 6| Step: 8
Training loss: 2.1338783367694685
Validation loss: 2.6078681935282475

Epoch: 6| Step: 9
Training loss: 2.6798675743522202
Validation loss: 2.6331523219400217

Epoch: 6| Step: 10
Training loss: 1.8790768488404797
Validation loss: 2.634998700517386

Epoch: 6| Step: 11
Training loss: 2.1677894861837235
Validation loss: 2.619009691227998

Epoch: 6| Step: 12
Training loss: 2.789092333551269
Validation loss: 2.5797968366262496

Epoch: 6| Step: 13
Training loss: 1.98951979397545
Validation loss: 2.626506237037874

Epoch: 30| Step: 0
Training loss: 2.5349696580022782
Validation loss: 2.599334801652521

Epoch: 6| Step: 1
Training loss: 3.05693919518233
Validation loss: 2.580097130073282

Epoch: 6| Step: 2
Training loss: 2.7045064934997973
Validation loss: 2.65150451117193

Epoch: 6| Step: 3
Training loss: 2.158845307102285
Validation loss: 2.622202972522849

Epoch: 6| Step: 4
Training loss: 2.4221073162265867
Validation loss: 2.6416305493393817

Epoch: 6| Step: 5
Training loss: 2.454270016664276
Validation loss: 2.6228497038866454

Epoch: 6| Step: 6
Training loss: 2.198254117929087
Validation loss: 2.627603556718305

Epoch: 6| Step: 7
Training loss: 2.083360430223359
Validation loss: 2.629569043313225

Epoch: 6| Step: 8
Training loss: 2.642855994029145
Validation loss: 2.641027732098791

Epoch: 6| Step: 9
Training loss: 2.738007492333827
Validation loss: 2.6736931071292824

Epoch: 6| Step: 10
Training loss: 2.7375615326397713
Validation loss: 2.5747064679315987

Epoch: 6| Step: 11
Training loss: 2.817464896713687
Validation loss: 2.61086411919957

Epoch: 6| Step: 12
Training loss: 2.3526669566859715
Validation loss: 2.615207329159846

Epoch: 6| Step: 13
Training loss: 2.4505947208649976
Validation loss: 2.624193945757353

Epoch: 31| Step: 0
Training loss: 2.539679725159115
Validation loss: 2.6292664128610657

Epoch: 6| Step: 1
Training loss: 3.3321132493299
Validation loss: 2.6311541880412

Epoch: 6| Step: 2
Training loss: 2.6196783709944347
Validation loss: 2.5780964705304754

Epoch: 6| Step: 3
Training loss: 2.2603461493349273
Validation loss: 2.590775634288367

Epoch: 6| Step: 4
Training loss: 2.2929371404337426
Validation loss: 2.590920540425881

Epoch: 6| Step: 5
Training loss: 2.4389898197701405
Validation loss: 2.6072320418957893

Epoch: 6| Step: 6
Training loss: 1.6448657495276373
Validation loss: 2.637058045962233

Epoch: 6| Step: 7
Training loss: 2.9465734074651957
Validation loss: 2.624548661319456

Epoch: 6| Step: 8
Training loss: 2.4147697326315196
Validation loss: 2.585463419903161

Epoch: 6| Step: 9
Training loss: 3.0230982390507357
Validation loss: 2.607124088727429

Epoch: 6| Step: 10
Training loss: 2.182594029519203
Validation loss: 2.59834230011165

Epoch: 6| Step: 11
Training loss: 2.186656680355684
Validation loss: 2.6209644753819914

Epoch: 6| Step: 12
Training loss: 2.1664106388921125
Validation loss: 2.615875978327817

Epoch: 6| Step: 13
Training loss: 2.785172172169162
Validation loss: 2.6419638074513077

Epoch: 32| Step: 0
Training loss: 3.473372395223184
Validation loss: 2.625867791013818

Epoch: 6| Step: 1
Training loss: 2.4412479440863057
Validation loss: 2.6316340343665927

Epoch: 6| Step: 2
Training loss: 2.457558191799894
Validation loss: 2.646738383172684

Epoch: 6| Step: 3
Training loss: 2.863050422887977
Validation loss: 2.6026040437087743

Epoch: 6| Step: 4
Training loss: 1.9827453048779369
Validation loss: 2.615393514351931

Epoch: 6| Step: 5
Training loss: 2.4089195998643955
Validation loss: 2.618884729092459

Epoch: 6| Step: 6
Training loss: 2.783257520712955
Validation loss: 2.6396941400848397

Epoch: 6| Step: 7
Training loss: 2.358499427670799
Validation loss: 2.610120183411408

Epoch: 6| Step: 8
Training loss: 2.8596873295204195
Validation loss: 2.6185311583347257

Epoch: 6| Step: 9
Training loss: 2.9337543387720233
Validation loss: 2.6277611455285705

Epoch: 6| Step: 10
Training loss: 2.7024042127715315
Validation loss: 2.627965379933528

Epoch: 6| Step: 11
Training loss: 1.935084406073898
Validation loss: 2.5944006919461744

Epoch: 6| Step: 12
Training loss: 1.9129714035792238
Validation loss: 2.6176275234497766

Epoch: 6| Step: 13
Training loss: 2.196842624214116
Validation loss: 2.5826231354552127

Epoch: 33| Step: 0
Training loss: 2.616155529111838
Validation loss: 2.5682947264296074

Epoch: 6| Step: 1
Training loss: 2.3115152891504565
Validation loss: 2.6186499006734283

Epoch: 6| Step: 2
Training loss: 2.452770426206227
Validation loss: 2.611388200797827

Epoch: 6| Step: 3
Training loss: 2.095918130644365
Validation loss: 2.595587053040702

Epoch: 6| Step: 4
Training loss: 2.488449785642226
Validation loss: 2.638731960739139

Epoch: 6| Step: 5
Training loss: 2.7423513858000517
Validation loss: 2.637516019871951

Epoch: 6| Step: 6
Training loss: 3.219908718687107
Validation loss: 2.624006779793765

Epoch: 6| Step: 7
Training loss: 2.912909476510928
Validation loss: 2.6251291515731117

Epoch: 6| Step: 8
Training loss: 2.348183063421764
Validation loss: 2.629289180684094

Epoch: 6| Step: 9
Training loss: 1.8113970853846324
Validation loss: 2.605450540047168

Epoch: 6| Step: 10
Training loss: 2.0726510138073464
Validation loss: 2.6185851507263638

Epoch: 6| Step: 11
Training loss: 2.6811109493452476
Validation loss: 2.6251327768684125

Epoch: 6| Step: 12
Training loss: 2.836324029648176
Validation loss: 2.6166743140706203

Epoch: 6| Step: 13
Training loss: 2.717566133763028
Validation loss: 2.5813513710492555

Epoch: 34| Step: 0
Training loss: 2.7302161278601886
Validation loss: 2.6461879212274537

Epoch: 6| Step: 1
Training loss: 1.924428592442356
Validation loss: 2.614799336076259

Epoch: 6| Step: 2
Training loss: 2.2603428794900404
Validation loss: 2.664659681163422

Epoch: 6| Step: 3
Training loss: 2.7198124705911444
Validation loss: 2.59379629683636

Epoch: 6| Step: 4
Training loss: 2.1615385354723684
Validation loss: 2.6084602564764343

Epoch: 6| Step: 5
Training loss: 2.0348970976735203
Validation loss: 2.6253518216951455

Epoch: 6| Step: 6
Training loss: 2.4918252807280235
Validation loss: 2.615619292076382

Epoch: 6| Step: 7
Training loss: 2.9950850279492895
Validation loss: 2.5908113937905015

Epoch: 6| Step: 8
Training loss: 3.1006142346149694
Validation loss: 2.6097707086696404

Epoch: 6| Step: 9
Training loss: 2.7767671611623417
Validation loss: 2.5802232161025827

Epoch: 6| Step: 10
Training loss: 2.263238215276
Validation loss: 2.577071743453127

Epoch: 6| Step: 11
Training loss: 2.8001027156518177
Validation loss: 2.5904168462934756

Epoch: 6| Step: 12
Training loss: 2.7211093530856485
Validation loss: 2.5733265312216926

Epoch: 6| Step: 13
Training loss: 2.0956152962432375
Validation loss: 2.6044603614497954

Epoch: 35| Step: 0
Training loss: 2.5719309989608057
Validation loss: 2.6180521888834933

Epoch: 6| Step: 1
Training loss: 3.1565663396352934
Validation loss: 2.5953502544341736

Epoch: 6| Step: 2
Training loss: 2.3513615417792924
Validation loss: 2.5880922788350817

Epoch: 6| Step: 3
Training loss: 1.9881683497423235
Validation loss: 2.5961698557875397

Epoch: 6| Step: 4
Training loss: 1.9994186510124892
Validation loss: 2.627485922253344

Epoch: 6| Step: 5
Training loss: 2.472650657514904
Validation loss: 2.609564433578844

Epoch: 6| Step: 6
Training loss: 2.537336026611075
Validation loss: 2.6559304437836464

Epoch: 6| Step: 7
Training loss: 2.0888012148447057
Validation loss: 2.610718295690854

Epoch: 6| Step: 8
Training loss: 2.2937545859475508
Validation loss: 2.5860429878654556

Epoch: 6| Step: 9
Training loss: 1.900066540205435
Validation loss: 2.6055469872939976

Epoch: 6| Step: 10
Training loss: 2.872768033290489
Validation loss: 2.6335783013549214

Epoch: 6| Step: 11
Training loss: 2.638757169235805
Validation loss: 2.6291307766294003

Epoch: 6| Step: 12
Training loss: 3.1340664377966587
Validation loss: 2.6019060062757404

Epoch: 6| Step: 13
Training loss: 3.1813336734222633
Validation loss: 2.5999589012027715

Epoch: 36| Step: 0
Training loss: 2.6394876592730245
Validation loss: 2.6625562718613187

Epoch: 6| Step: 1
Training loss: 2.0198712238017027
Validation loss: 2.584415644887672

Epoch: 6| Step: 2
Training loss: 2.2646715986310237
Validation loss: 2.6472638285586103

Epoch: 6| Step: 3
Training loss: 2.0493492721892497
Validation loss: 2.6337723986652852

Epoch: 6| Step: 4
Training loss: 2.7054921628647817
Validation loss: 2.6489748789372696

Epoch: 6| Step: 5
Training loss: 2.540037466809139
Validation loss: 2.610101396946627

Epoch: 6| Step: 6
Training loss: 2.1659200800331533
Validation loss: 2.604538293389166

Epoch: 6| Step: 7
Training loss: 3.2529103379603135
Validation loss: 2.5999406985097986

Epoch: 6| Step: 8
Training loss: 2.46242049810195
Validation loss: 2.608237347852754

Epoch: 6| Step: 9
Training loss: 2.648861921393257
Validation loss: 2.6214683935809147

Epoch: 6| Step: 10
Training loss: 2.8700917681866027
Validation loss: 2.5893983069495867

Epoch: 6| Step: 11
Training loss: 2.7048616508599785
Validation loss: 2.595824748515409

Epoch: 6| Step: 12
Training loss: 2.9519848110976405
Validation loss: 2.6179624254800493

Epoch: 6| Step: 13
Training loss: 1.8922814293124972
Validation loss: 2.583234444130376

Epoch: 37| Step: 0
Training loss: 2.5545252943894083
Validation loss: 2.593924796101302

Epoch: 6| Step: 1
Training loss: 3.0503198174567747
Validation loss: 2.6382622917412135

Epoch: 6| Step: 2
Training loss: 2.2337021248014537
Validation loss: 2.6008792472566706

Epoch: 6| Step: 3
Training loss: 2.685824914584299
Validation loss: 2.5821386517893288

Epoch: 6| Step: 4
Training loss: 1.8635196813143333
Validation loss: 2.6012255781726883

Epoch: 6| Step: 5
Training loss: 2.3280648857394706
Validation loss: 2.6231467956311274

Epoch: 6| Step: 6
Training loss: 2.336714293266186
Validation loss: 2.6138897364641256

Epoch: 6| Step: 7
Training loss: 2.2801142961731724
Validation loss: 2.5696138555220087

Epoch: 6| Step: 8
Training loss: 2.555380537655835
Validation loss: 2.5805377343881792

Epoch: 6| Step: 9
Training loss: 2.453524028616101
Validation loss: 2.592128936492334

Epoch: 6| Step: 10
Training loss: 2.7799671943704882
Validation loss: 2.5956346796070986

Epoch: 6| Step: 11
Training loss: 2.9472850396974857
Validation loss: 2.631211651592837

Epoch: 6| Step: 12
Training loss: 3.118560259289583
Validation loss: 2.5925038726198952

Epoch: 6| Step: 13
Training loss: 1.827635544683032
Validation loss: 2.620000270984238

Epoch: 38| Step: 0
Training loss: 2.386288568215994
Validation loss: 2.5985258248358365

Epoch: 6| Step: 1
Training loss: 2.3676572025236484
Validation loss: 2.563196924851986

Epoch: 6| Step: 2
Training loss: 2.318239255523383
Validation loss: 2.628929670889879

Epoch: 6| Step: 3
Training loss: 3.122650789356778
Validation loss: 2.599766582012683

Epoch: 6| Step: 4
Training loss: 3.345199983603051
Validation loss: 2.5812146563877385

Epoch: 6| Step: 5
Training loss: 2.528973155864549
Validation loss: 2.6748474719332895

Epoch: 6| Step: 6
Training loss: 2.359369644260012
Validation loss: 2.5687542707248405

Epoch: 6| Step: 7
Training loss: 2.798221796648022
Validation loss: 2.5984563603942967

Epoch: 6| Step: 8
Training loss: 2.229253808810899
Validation loss: 2.6276658614558763

Epoch: 6| Step: 9
Training loss: 2.761148223866324
Validation loss: 2.6084163069603883

Epoch: 6| Step: 10
Training loss: 2.2182931564021198
Validation loss: 2.584270558520533

Epoch: 6| Step: 11
Training loss: 1.9027007784272816
Validation loss: 2.5885342010042436

Epoch: 6| Step: 12
Training loss: 2.711810144693809
Validation loss: 2.637971948520305

Epoch: 6| Step: 13
Training loss: 2.062419196193251
Validation loss: 2.630834770512382

Epoch: 39| Step: 0
Training loss: 2.315996876867689
Validation loss: 2.6057370194057485

Epoch: 6| Step: 1
Training loss: 2.6954506658993713
Validation loss: 2.627181433455174

Epoch: 6| Step: 2
Training loss: 2.9577950180351205
Validation loss: 2.6347209756026344

Epoch: 6| Step: 3
Training loss: 2.3043596454767874
Validation loss: 2.6027061690263467

Epoch: 6| Step: 4
Training loss: 2.310970625469092
Validation loss: 2.606959597485343

Epoch: 6| Step: 5
Training loss: 1.6318981768314769
Validation loss: 2.6071092892082715

Epoch: 6| Step: 6
Training loss: 3.1387326437023235
Validation loss: 2.573871980888737

Epoch: 6| Step: 7
Training loss: 3.0281722971526364
Validation loss: 2.609007514462238

Epoch: 6| Step: 8
Training loss: 2.84438140901947
Validation loss: 2.573573462379071

Epoch: 6| Step: 9
Training loss: 2.4079334265799
Validation loss: 2.5967299413582374

Epoch: 6| Step: 10
Training loss: 2.5845245465666253
Validation loss: 2.6056106124606373

Epoch: 6| Step: 11
Training loss: 2.448460511742731
Validation loss: 2.6201856606344633

Epoch: 6| Step: 12
Training loss: 2.074444487333964
Validation loss: 2.612629108189493

Epoch: 6| Step: 13
Training loss: 2.3914400593692813
Validation loss: 2.5763070526789758

Epoch: 40| Step: 0
Training loss: 2.5889021829019105
Validation loss: 2.617378812411634

Epoch: 6| Step: 1
Training loss: 2.3879960508457674
Validation loss: 2.606894724846029

Epoch: 6| Step: 2
Training loss: 2.9859745871707437
Validation loss: 2.592127970724192

Epoch: 6| Step: 3
Training loss: 2.2134094438144056
Validation loss: 2.608316598986812

Epoch: 6| Step: 4
Training loss: 3.01418669401741
Validation loss: 2.6295662703726395

Epoch: 6| Step: 5
Training loss: 2.778508410262758
Validation loss: 2.616490892916979

Epoch: 6| Step: 6
Training loss: 2.3626017957282226
Validation loss: 2.6349937692721763

Epoch: 6| Step: 7
Training loss: 2.1779820049092455
Validation loss: 2.593673229996416

Epoch: 6| Step: 8
Training loss: 3.562258795888058
Validation loss: 2.656312036724823

Epoch: 6| Step: 9
Training loss: 1.5094505464906842
Validation loss: 2.628950393555538

Epoch: 6| Step: 10
Training loss: 2.3655742523918164
Validation loss: 2.597019394747925

Epoch: 6| Step: 11
Training loss: 2.3490826784591503
Validation loss: 2.631804985661764

Epoch: 6| Step: 12
Training loss: 2.3624249885397526
Validation loss: 2.5963961267221354

Epoch: 6| Step: 13
Training loss: 2.1460350336875234
Validation loss: 2.57616566658959

Epoch: 41| Step: 0
Training loss: 2.3410299157174776
Validation loss: 2.5834920024340753

Epoch: 6| Step: 1
Training loss: 3.2969415305416847
Validation loss: 2.6030106432747577

Epoch: 6| Step: 2
Training loss: 2.581915271364624
Validation loss: 2.6239574420761365

Epoch: 6| Step: 3
Training loss: 2.754120600589737
Validation loss: 2.596197559119096

Epoch: 6| Step: 4
Training loss: 2.7331724301716744
Validation loss: 2.632778963654563

Epoch: 6| Step: 5
Training loss: 2.23647694987873
Validation loss: 2.6146437396079034

Epoch: 6| Step: 6
Training loss: 2.902453167625412
Validation loss: 2.6341436777033915

Epoch: 6| Step: 7
Training loss: 2.2133530001551716
Validation loss: 2.6350975798382437

Epoch: 6| Step: 8
Training loss: 2.458133904940229
Validation loss: 2.6295452276628257

Epoch: 6| Step: 9
Training loss: 2.039345907926788
Validation loss: 2.582421623879696

Epoch: 6| Step: 10
Training loss: 1.9229552171782236
Validation loss: 2.621456024557762

Epoch: 6| Step: 11
Training loss: 2.4287032804840094
Validation loss: 2.5690470849327065

Epoch: 6| Step: 12
Training loss: 2.499051486324127
Validation loss: 2.6391773055110797

Epoch: 6| Step: 13
Training loss: 2.542909495517404
Validation loss: 2.6126114652664625

Epoch: 42| Step: 0
Training loss: 1.9631614080755317
Validation loss: 2.5985270405426055

Epoch: 6| Step: 1
Training loss: 3.0489196177115123
Validation loss: 2.5905274974480554

Epoch: 6| Step: 2
Training loss: 2.543448641685026
Validation loss: 2.58225175817216

Epoch: 6| Step: 3
Training loss: 2.6263980775436258
Validation loss: 2.623820312270288

Epoch: 6| Step: 4
Training loss: 1.9104784984282193
Validation loss: 2.5890308329844753

Epoch: 6| Step: 5
Training loss: 2.788142632277112
Validation loss: 2.616600509787233

Epoch: 6| Step: 6
Training loss: 2.7547156044169534
Validation loss: 2.5760937173482086

Epoch: 6| Step: 7
Training loss: 1.8328986808632666
Validation loss: 2.6524786411567587

Epoch: 6| Step: 8
Training loss: 2.3044724186252146
Validation loss: 2.627708158272002

Epoch: 6| Step: 9
Training loss: 2.9747935568081765
Validation loss: 2.6386846150980836

Epoch: 6| Step: 10
Training loss: 2.5095362933282592
Validation loss: 2.6456220998071505

Epoch: 6| Step: 11
Training loss: 2.74818568634843
Validation loss: 2.6518345504625924

Epoch: 6| Step: 12
Training loss: 2.6393374402036014
Validation loss: 2.615349103990754

Epoch: 6| Step: 13
Training loss: 2.578390766663904
Validation loss: 2.559833612523425

Epoch: 43| Step: 0
Training loss: 1.808425005626875
Validation loss: 2.5867945700006105

Epoch: 6| Step: 1
Training loss: 2.577433915424197
Validation loss: 2.6152201379830537

Epoch: 6| Step: 2
Training loss: 2.844016261787976
Validation loss: 2.583159138333701

Epoch: 6| Step: 3
Training loss: 2.927064255259687
Validation loss: 2.587492883214823

Epoch: 6| Step: 4
Training loss: 2.575533587530562
Validation loss: 2.6262592897851538

Epoch: 6| Step: 5
Training loss: 2.30874039495646
Validation loss: 2.61735871926863

Epoch: 6| Step: 6
Training loss: 2.528421776401699
Validation loss: 2.615280185152948

Epoch: 6| Step: 7
Training loss: 2.5183418719460198
Validation loss: 2.61233256957959

Epoch: 6| Step: 8
Training loss: 2.8382614324052673
Validation loss: 2.6091766738902753

Epoch: 6| Step: 9
Training loss: 2.31389452152653
Validation loss: 2.621498315461742

Epoch: 6| Step: 10
Training loss: 3.1107795648418417
Validation loss: 2.6089470183048364

Epoch: 6| Step: 11
Training loss: 2.5901879359514792
Validation loss: 2.6011010601782503

Epoch: 6| Step: 12
Training loss: 2.0268231791159916
Validation loss: 2.5695199797814126

Epoch: 6| Step: 13
Training loss: 2.1380691306033044
Validation loss: 2.6057275798903845

Epoch: 44| Step: 0
Training loss: 3.296306253704727
Validation loss: 2.616307960107601

Epoch: 6| Step: 1
Training loss: 2.523476615106577
Validation loss: 2.6128963230683233

Epoch: 6| Step: 2
Training loss: 2.349335791688658
Validation loss: 2.637549044015986

Epoch: 6| Step: 3
Training loss: 2.2508346281173846
Validation loss: 2.6189593338432786

Epoch: 6| Step: 4
Training loss: 1.8810176921379518
Validation loss: 2.6469510088158965

Epoch: 6| Step: 5
Training loss: 2.497573151457451
Validation loss: 2.6294863676269187

Epoch: 6| Step: 6
Training loss: 2.378611378802689
Validation loss: 2.6190482134426154

Epoch: 6| Step: 7
Training loss: 2.9632288941500917
Validation loss: 2.612298283554261

Epoch: 6| Step: 8
Training loss: 2.448314445047076
Validation loss: 2.663801611154646

Epoch: 6| Step: 9
Training loss: 1.991962016191623
Validation loss: 2.6580871063408793

Epoch: 6| Step: 10
Training loss: 2.4157271586896383
Validation loss: 2.609414760872958

Epoch: 6| Step: 11
Training loss: 2.7427870048910505
Validation loss: 2.590208232197166

Epoch: 6| Step: 12
Training loss: 2.8261015170816273
Validation loss: 2.6320441961606975

Epoch: 6| Step: 13
Training loss: 2.4704979611581197
Validation loss: 2.6083209103500713

Epoch: 45| Step: 0
Training loss: 2.309937062087286
Validation loss: 2.6223977981352937

Epoch: 6| Step: 1
Training loss: 2.274077335260524
Validation loss: 2.5970054633477617

Epoch: 6| Step: 2
Training loss: 1.8227742457385485
Validation loss: 2.6362988248270693

Epoch: 6| Step: 3
Training loss: 2.83841649517354
Validation loss: 2.648204031737228

Epoch: 6| Step: 4
Training loss: 2.2794120386836485
Validation loss: 2.604964459604529

Epoch: 6| Step: 5
Training loss: 3.226880517393719
Validation loss: 2.6113091494037945

Epoch: 6| Step: 6
Training loss: 2.5218581226261554
Validation loss: 2.597305702515677

Epoch: 6| Step: 7
Training loss: 2.898706305447183
Validation loss: 2.578490630145577

Epoch: 6| Step: 8
Training loss: 2.2624932051920155
Validation loss: 2.6355203500086377

Epoch: 6| Step: 9
Training loss: 2.3453659590309153
Validation loss: 2.605293798029876

Epoch: 6| Step: 10
Training loss: 2.5363192249249455
Validation loss: 2.6079904991125065

Epoch: 6| Step: 11
Training loss: 3.40705694037157
Validation loss: 2.590954127799928

Epoch: 6| Step: 12
Training loss: 1.6305094269237548
Validation loss: 2.592331724502452

Epoch: 6| Step: 13
Training loss: 2.1237142824578887
Validation loss: 2.5826903640291405

Epoch: 46| Step: 0
Training loss: 2.8193640976739216
Validation loss: 2.5900848644669323

Epoch: 6| Step: 1
Training loss: 2.691766235903482
Validation loss: 2.6260218069638763

Epoch: 6| Step: 2
Training loss: 2.8553977780060853
Validation loss: 2.6162036469896552

Epoch: 6| Step: 3
Training loss: 2.3246891347143777
Validation loss: 2.605732231029046

Epoch: 6| Step: 4
Training loss: 2.4838362775464904
Validation loss: 2.6542053581144427

Epoch: 6| Step: 5
Training loss: 2.490028905432653
Validation loss: 2.6037539701878676

Epoch: 6| Step: 6
Training loss: 2.3193939137490704
Validation loss: 2.6000264227575993

Epoch: 6| Step: 7
Training loss: 2.599599286524846
Validation loss: 2.623205101105676

Epoch: 6| Step: 8
Training loss: 1.6304491817833673
Validation loss: 2.6229487230575566

Epoch: 6| Step: 9
Training loss: 2.218052714210728
Validation loss: 2.6182989996106443

Epoch: 6| Step: 10
Training loss: 2.2547381632631955
Validation loss: 2.6053222127004867

Epoch: 6| Step: 11
Training loss: 2.7321552094679844
Validation loss: 2.611281804216291

Epoch: 6| Step: 12
Training loss: 2.342348417183684
Validation loss: 2.600976230672592

Epoch: 6| Step: 13
Training loss: 2.8009182139682536
Validation loss: 2.548408460110523

Epoch: 47| Step: 0
Training loss: 2.367985254203145
Validation loss: 2.603686255329277

Epoch: 6| Step: 1
Training loss: 2.6007167598466983
Validation loss: 2.5995907265758307

Epoch: 6| Step: 2
Training loss: 2.4890854043619153
Validation loss: 2.612663724475024

Epoch: 6| Step: 3
Training loss: 1.7229299176531845
Validation loss: 2.5963157005610817

Epoch: 6| Step: 4
Training loss: 3.029840830030889
Validation loss: 2.627725805687592

Epoch: 6| Step: 5
Training loss: 3.461071692076087
Validation loss: 2.599867412658837

Epoch: 6| Step: 6
Training loss: 2.2320505068612233
Validation loss: 2.5944356816307423

Epoch: 6| Step: 7
Training loss: 2.7113327046113715
Validation loss: 2.5946831920302706

Epoch: 6| Step: 8
Training loss: 2.5981397135861415
Validation loss: 2.585285684698651

Epoch: 6| Step: 9
Training loss: 2.4215596639784196
Validation loss: 2.6266429921316137

Epoch: 6| Step: 10
Training loss: 1.949350784508484
Validation loss: 2.6032375560177208

Epoch: 6| Step: 11
Training loss: 2.8442457835872084
Validation loss: 2.6069344168499358

Epoch: 6| Step: 12
Training loss: 1.7927413497033071
Validation loss: 2.5898754641517154

Epoch: 6| Step: 13
Training loss: 2.6066977649144256
Validation loss: 2.6786557390174615

Epoch: 48| Step: 0
Training loss: 3.1264187453286834
Validation loss: 2.6209222819927054

Epoch: 6| Step: 1
Training loss: 2.241440066923137
Validation loss: 2.6025226795284455

Epoch: 6| Step: 2
Training loss: 2.28342323775932
Validation loss: 2.598538157734097

Epoch: 6| Step: 3
Training loss: 2.14281713357541
Validation loss: 2.632998119417807

Epoch: 6| Step: 4
Training loss: 2.381297746314812
Validation loss: 2.6372870315578125

Epoch: 6| Step: 5
Training loss: 2.767843272794084
Validation loss: 2.6332247267027875

Epoch: 6| Step: 6
Training loss: 2.4645744454703786
Validation loss: 2.613801418779565

Epoch: 6| Step: 7
Training loss: 1.8719251852042793
Validation loss: 2.6473114110281855

Epoch: 6| Step: 8
Training loss: 1.87130946307603
Validation loss: 2.605755791587509

Epoch: 6| Step: 9
Training loss: 1.880011473377199
Validation loss: 2.6041917163915667

Epoch: 6| Step: 10
Training loss: 2.821564964253643
Validation loss: 2.568067766661078

Epoch: 6| Step: 11
Training loss: 2.8031132352278023
Validation loss: 2.604675263648991

Epoch: 6| Step: 12
Training loss: 2.5711384753426265
Validation loss: 2.5781328451634504

Epoch: 6| Step: 13
Training loss: 3.3447228558373623
Validation loss: 2.6075013185511784

Epoch: 49| Step: 0
Training loss: 2.2644512424589487
Validation loss: 2.630110322743949

Epoch: 6| Step: 1
Training loss: 2.096148355620845
Validation loss: 2.6187450879277763

Epoch: 6| Step: 2
Training loss: 2.8918522059768854
Validation loss: 2.585203552581878

Epoch: 6| Step: 3
Training loss: 1.9328349156109113
Validation loss: 2.6082032365214753

Epoch: 6| Step: 4
Training loss: 2.941283528679536
Validation loss: 2.60750493025687

Epoch: 6| Step: 5
Training loss: 3.0913360117273476
Validation loss: 2.631722621802011

Epoch: 6| Step: 6
Training loss: 1.9631172011450357
Validation loss: 2.5837862684180126

Epoch: 6| Step: 7
Training loss: 3.102932336824969
Validation loss: 2.5922148424285054

Epoch: 6| Step: 8
Training loss: 2.569744940246476
Validation loss: 2.5738060272575845

Epoch: 6| Step: 9
Training loss: 2.9289012616863697
Validation loss: 2.631602763059833

Epoch: 6| Step: 10
Training loss: 2.5015181700134073
Validation loss: 2.637162732079103

Epoch: 6| Step: 11
Training loss: 1.941186596976606
Validation loss: 2.6234198310079826

Epoch: 6| Step: 12
Training loss: 2.1869196530667803
Validation loss: 2.6128945437539914

Epoch: 6| Step: 13
Training loss: 2.159409129953688
Validation loss: 2.642436156931721

Epoch: 50| Step: 0
Training loss: 1.9778203768851321
Validation loss: 2.5998862730731562

Epoch: 6| Step: 1
Training loss: 2.2539981710905495
Validation loss: 2.6117694094795296

Epoch: 6| Step: 2
Training loss: 2.1291483610178736
Validation loss: 2.621606828678043

Epoch: 6| Step: 3
Training loss: 3.3670783146903935
Validation loss: 2.594597781326779

Epoch: 6| Step: 4
Training loss: 2.5902065293436625
Validation loss: 2.599688598740187

Epoch: 6| Step: 5
Training loss: 2.573826020402463
Validation loss: 2.5772244046043866

Epoch: 6| Step: 6
Training loss: 2.483661573167342
Validation loss: 2.5933266566157096

Epoch: 6| Step: 7
Training loss: 3.0434660792847414
Validation loss: 2.6035194253858

Epoch: 6| Step: 8
Training loss: 1.46319519865313
Validation loss: 2.618202764103411

Epoch: 6| Step: 9
Training loss: 3.2534695225752723
Validation loss: 2.6259658338489076

Epoch: 6| Step: 10
Training loss: 1.8748154867302136
Validation loss: 2.6038401488009613

Epoch: 6| Step: 11
Training loss: 2.5434604526847546
Validation loss: 2.6255780068072276

Epoch: 6| Step: 12
Training loss: 2.4002649757022168
Validation loss: 2.5982065483156584

Epoch: 6| Step: 13
Training loss: 2.2205751461034113
Validation loss: 2.604370404221029

Testing loss: 2.253520667335778

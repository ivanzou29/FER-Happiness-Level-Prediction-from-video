Epoch: 1| Step: 0
Training loss: 6.145551496174991
Validation loss: 6.1454906648269025

Epoch: 6| Step: 1
Training loss: 6.755066841975051
Validation loss: 6.1377197588846215

Epoch: 6| Step: 2
Training loss: 6.195966996284193
Validation loss: 6.122162609344739

Epoch: 6| Step: 3
Training loss: 6.764372042250231
Validation loss: 6.112218144841211

Epoch: 6| Step: 4
Training loss: 5.832656530445491
Validation loss: 6.100900689840388

Epoch: 6| Step: 5
Training loss: 5.85772209759113
Validation loss: 6.087988061158834

Epoch: 6| Step: 6
Training loss: 6.738031585048654
Validation loss: 6.07594847424282

Epoch: 6| Step: 7
Training loss: 5.572011979965837
Validation loss: 6.065809834235578

Epoch: 6| Step: 8
Training loss: 5.364114606311423
Validation loss: 6.055769392587362

Epoch: 6| Step: 9
Training loss: 5.543574337073876
Validation loss: 6.040407229288945

Epoch: 6| Step: 10
Training loss: 5.4875340071821395
Validation loss: 6.030177572829439

Epoch: 6| Step: 11
Training loss: 6.643391542283356
Validation loss: 6.015913839960973

Epoch: 6| Step: 12
Training loss: 6.297590721479419
Validation loss: 6.001510641874907

Epoch: 6| Step: 13
Training loss: 7.065209333230211
Validation loss: 5.987446208544986

Epoch: 2| Step: 0
Training loss: 6.6256365200374745
Validation loss: 5.9700202588835065

Epoch: 6| Step: 1
Training loss: 5.707603723533421
Validation loss: 5.9550131859621365

Epoch: 6| Step: 2
Training loss: 5.918251720751305
Validation loss: 5.943490122112149

Epoch: 6| Step: 3
Training loss: 6.1087611833988715
Validation loss: 5.929265519491147

Epoch: 6| Step: 4
Training loss: 5.500734973697105
Validation loss: 5.910511128053182

Epoch: 6| Step: 5
Training loss: 5.996380031846558
Validation loss: 5.897506647153405

Epoch: 6| Step: 6
Training loss: 6.425301261993328
Validation loss: 5.875680329451475

Epoch: 6| Step: 7
Training loss: 6.910120367169412
Validation loss: 5.85714057873583

Epoch: 6| Step: 8
Training loss: 5.426991272168077
Validation loss: 5.835958634848499

Epoch: 6| Step: 9
Training loss: 6.059517982373758
Validation loss: 5.823980744869764

Epoch: 6| Step: 10
Training loss: 4.612130275887997
Validation loss: 5.802319488193885

Epoch: 6| Step: 11
Training loss: 5.688553597251993
Validation loss: 5.7765030076135675

Epoch: 6| Step: 12
Training loss: 6.467695214709863
Validation loss: 5.755621774129702

Epoch: 6| Step: 13
Training loss: 5.9092983876330045
Validation loss: 5.730183662546826

Epoch: 3| Step: 0
Training loss: 5.497601419565464
Validation loss: 5.707016099112902

Epoch: 6| Step: 1
Training loss: 5.801604529119913
Validation loss: 5.679083440912024

Epoch: 6| Step: 2
Training loss: 5.319301962984719
Validation loss: 5.653700139648663

Epoch: 6| Step: 3
Training loss: 6.1762046596435525
Validation loss: 5.626370157155029

Epoch: 6| Step: 4
Training loss: 5.737817214474399
Validation loss: 5.601839492312489

Epoch: 6| Step: 5
Training loss: 4.843344985272297
Validation loss: 5.5715720672899

Epoch: 6| Step: 6
Training loss: 6.826227478761375
Validation loss: 5.539615633974259

Epoch: 6| Step: 7
Training loss: 6.172056712119323
Validation loss: 5.504738067299406

Epoch: 6| Step: 8
Training loss: 4.723939580855604
Validation loss: 5.472080430045762

Epoch: 6| Step: 9
Training loss: 6.248672344337998
Validation loss: 5.4341049118444635

Epoch: 6| Step: 10
Training loss: 5.585172587488344
Validation loss: 5.401034387959426

Epoch: 6| Step: 11
Training loss: 5.455943606099185
Validation loss: 5.355460291437749

Epoch: 6| Step: 12
Training loss: 4.853695607829852
Validation loss: 5.307508449850186

Epoch: 6| Step: 13
Training loss: 4.941304732776375
Validation loss: 5.262425296642849

Epoch: 4| Step: 0
Training loss: 5.429987134830416
Validation loss: 5.221761295367448

Epoch: 6| Step: 1
Training loss: 5.319137555536311
Validation loss: 5.172906993539927

Epoch: 6| Step: 2
Training loss: 5.210879301235159
Validation loss: 5.117626530496703

Epoch: 6| Step: 3
Training loss: 5.102990877873308
Validation loss: 5.0624026065453185

Epoch: 6| Step: 4
Training loss: 5.755585984027278
Validation loss: 5.010421604886334

Epoch: 6| Step: 5
Training loss: 5.062001521682661
Validation loss: 4.944721333231409

Epoch: 6| Step: 6
Training loss: 5.193329626301596
Validation loss: 4.879368870316272

Epoch: 6| Step: 7
Training loss: 4.550723410308399
Validation loss: 4.821212247674823

Epoch: 6| Step: 8
Training loss: 4.462417918072791
Validation loss: 4.752027647680821

Epoch: 6| Step: 9
Training loss: 3.963378755861538
Validation loss: 4.6827343260135645

Epoch: 6| Step: 10
Training loss: 5.050215144342901
Validation loss: 4.599320595781462

Epoch: 6| Step: 11
Training loss: 4.038092432166785
Validation loss: 4.525966343693737

Epoch: 6| Step: 12
Training loss: 4.35110049303487
Validation loss: 4.432175840604404

Epoch: 6| Step: 13
Training loss: 5.111115791945226
Validation loss: 4.361797029099255

Epoch: 5| Step: 0
Training loss: 3.8057245803261734
Validation loss: 4.26529003872909

Epoch: 6| Step: 1
Training loss: 5.0135628807664725
Validation loss: 4.170461141243904

Epoch: 6| Step: 2
Training loss: 3.403957050146662
Validation loss: 4.070779198892125

Epoch: 6| Step: 3
Training loss: 5.148906124940254
Validation loss: 3.9824405536896434

Epoch: 6| Step: 4
Training loss: 3.8670883628391346
Validation loss: 3.8843751726792393

Epoch: 6| Step: 5
Training loss: 3.0790261435716735
Validation loss: 3.7846511593056857

Epoch: 6| Step: 6
Training loss: 3.82668267930122
Validation loss: 3.6805595773798885

Epoch: 6| Step: 7
Training loss: 4.138131711741508
Validation loss: 3.579804194421668

Epoch: 6| Step: 8
Training loss: 3.8509043040226585
Validation loss: 3.4618420018137823

Epoch: 6| Step: 9
Training loss: 3.2248376849823654
Validation loss: 3.375123480904445

Epoch: 6| Step: 10
Training loss: 3.064533103740421
Validation loss: 3.231611167453452

Epoch: 6| Step: 11
Training loss: 3.3325635657003745
Validation loss: 3.135335082622313

Epoch: 6| Step: 12
Training loss: 3.0562340609207004
Validation loss: 3.0575571719314283

Epoch: 6| Step: 13
Training loss: 1.9454800330642084
Validation loss: 2.952636005478698

Epoch: 6| Step: 0
Training loss: 2.869237515116491
Validation loss: 2.883244593215751

Epoch: 6| Step: 1
Training loss: 2.081250128302126
Validation loss: 2.8186276969681914

Epoch: 6| Step: 2
Training loss: 2.7958189606237913
Validation loss: 2.773501700791

Epoch: 6| Step: 3
Training loss: 2.482785274099465
Validation loss: 2.7174798768300676

Epoch: 6| Step: 4
Training loss: 3.0274976334377524
Validation loss: 2.674274965468624

Epoch: 6| Step: 5
Training loss: 2.604963399443256
Validation loss: 2.664195097516131

Epoch: 6| Step: 6
Training loss: 2.3425823100314878
Validation loss: 2.7186683993316088

Epoch: 6| Step: 7
Training loss: 2.9377587387088737
Validation loss: 2.662666589005644

Epoch: 6| Step: 8
Training loss: 2.823075312324341
Validation loss: 2.6885232308213967

Epoch: 6| Step: 9
Training loss: 3.1366381795186173
Validation loss: 2.7415975169666758

Epoch: 6| Step: 10
Training loss: 2.182281154221833
Validation loss: 2.7305623530987453

Epoch: 6| Step: 11
Training loss: 2.9480246444355274
Validation loss: 2.7507836352410844

Epoch: 6| Step: 12
Training loss: 2.3787994110219586
Validation loss: 2.7931565299409464

Epoch: 6| Step: 13
Training loss: 2.9154414373674595
Validation loss: 2.8297135965375118

Epoch: 7| Step: 0
Training loss: 3.317883417529081
Validation loss: 2.810787216277172

Epoch: 6| Step: 1
Training loss: 3.425224998490348
Validation loss: 2.8424289555367896

Epoch: 6| Step: 2
Training loss: 3.80793363670416
Validation loss: 2.7912942628472908

Epoch: 6| Step: 3
Training loss: 1.870737953841574
Validation loss: 2.788972755220638

Epoch: 6| Step: 4
Training loss: 1.813661926360556
Validation loss: 2.808142040785963

Epoch: 6| Step: 5
Training loss: 1.9359648375470635
Validation loss: 2.7264222088038332

Epoch: 6| Step: 6
Training loss: 2.293688165730322
Validation loss: 2.7251715477774563

Epoch: 6| Step: 7
Training loss: 2.379473939506437
Validation loss: 2.7616464190833425

Epoch: 6| Step: 8
Training loss: 2.026370717240902
Validation loss: 2.7107468243912534

Epoch: 6| Step: 9
Training loss: 2.954890293724831
Validation loss: 2.7041455292034793

Epoch: 6| Step: 10
Training loss: 2.4793844902024267
Validation loss: 2.700116815512494

Epoch: 6| Step: 11
Training loss: 3.1288696744116917
Validation loss: 2.6635159764389256

Epoch: 6| Step: 12
Training loss: 2.4338404253367596
Validation loss: 2.717842813022829

Epoch: 6| Step: 13
Training loss: 3.000155444886389
Validation loss: 2.670307301828181

Epoch: 8| Step: 0
Training loss: 1.8868476397162888
Validation loss: 2.675128480315453

Epoch: 6| Step: 1
Training loss: 2.8978372862360646
Validation loss: 2.674385423264796

Epoch: 6| Step: 2
Training loss: 2.238864256873464
Validation loss: 2.667770534764875

Epoch: 6| Step: 3
Training loss: 2.8398927791933364
Validation loss: 2.6601659295666407

Epoch: 6| Step: 4
Training loss: 2.4143933313332004
Validation loss: 2.657944950045568

Epoch: 6| Step: 5
Training loss: 1.9476646695526938
Validation loss: 2.6579166643862604

Epoch: 6| Step: 6
Training loss: 2.8168042625210816
Validation loss: 2.6845498563566

Epoch: 6| Step: 7
Training loss: 3.142861552049591
Validation loss: 2.684515293705149

Epoch: 6| Step: 8
Training loss: 3.4423649434376644
Validation loss: 2.684568995140902

Epoch: 6| Step: 9
Training loss: 2.33007607855631
Validation loss: 2.68333999492774

Epoch: 6| Step: 10
Training loss: 3.1293496949925244
Validation loss: 2.690355410418838

Epoch: 6| Step: 11
Training loss: 2.1639736997377623
Validation loss: 2.6792345322180284

Epoch: 6| Step: 12
Training loss: 2.322048988857219
Validation loss: 2.666906604303929

Epoch: 6| Step: 13
Training loss: 2.5236723704913726
Validation loss: 2.6866179061027124

Epoch: 9| Step: 0
Training loss: 2.200038181320401
Validation loss: 2.681783732989925

Epoch: 6| Step: 1
Training loss: 2.5151822666079355
Validation loss: 2.6576230558157388

Epoch: 6| Step: 2
Training loss: 2.352673746436516
Validation loss: 2.644687018791274

Epoch: 6| Step: 3
Training loss: 2.6193916718214774
Validation loss: 2.630777615921922

Epoch: 6| Step: 4
Training loss: 3.013451618247351
Validation loss: 2.638179391437172

Epoch: 6| Step: 5
Training loss: 2.2169249644643334
Validation loss: 2.6855700830554965

Epoch: 6| Step: 6
Training loss: 3.25992506584684
Validation loss: 2.6907192141364207

Epoch: 6| Step: 7
Training loss: 2.6273937210380827
Validation loss: 2.670158098584675

Epoch: 6| Step: 8
Training loss: 3.1046127579115104
Validation loss: 2.677445432959983

Epoch: 6| Step: 9
Training loss: 2.5543252763979107
Validation loss: 2.6726980596173378

Epoch: 6| Step: 10
Training loss: 2.8022477528337353
Validation loss: 2.657712675803092

Epoch: 6| Step: 11
Training loss: 2.015770603805589
Validation loss: 2.666837706643706

Epoch: 6| Step: 12
Training loss: 2.048909581167934
Validation loss: 2.649878215990404

Epoch: 6| Step: 13
Training loss: 2.7345986411054533
Validation loss: 2.705080725052338

Epoch: 10| Step: 0
Training loss: 2.6147920036381556
Validation loss: 2.6586912717372555

Epoch: 6| Step: 1
Training loss: 2.145567374429275
Validation loss: 2.6431325559702774

Epoch: 6| Step: 2
Training loss: 2.7231360824040753
Validation loss: 2.6480653514481016

Epoch: 6| Step: 3
Training loss: 2.112029940884221
Validation loss: 2.702697308466228

Epoch: 6| Step: 4
Training loss: 2.4836812520219875
Validation loss: 2.6715141484002936

Epoch: 6| Step: 5
Training loss: 2.4243886393286074
Validation loss: 2.689171308362336

Epoch: 6| Step: 6
Training loss: 3.053621774511778
Validation loss: 2.6348860486682057

Epoch: 6| Step: 7
Training loss: 2.9851456690632943
Validation loss: 2.6711318560880972

Epoch: 6| Step: 8
Training loss: 2.677054042631185
Validation loss: 2.65781962075869

Epoch: 6| Step: 9
Training loss: 2.566114899748438
Validation loss: 2.6536954526345813

Epoch: 6| Step: 10
Training loss: 3.4746203537628686
Validation loss: 2.6941575676524168

Epoch: 6| Step: 11
Training loss: 1.92346572173523
Validation loss: 2.654938677063424

Epoch: 6| Step: 12
Training loss: 2.480706919852603
Validation loss: 2.6762387072606875

Epoch: 6| Step: 13
Training loss: 1.9841257975135767
Validation loss: 2.657153465857279

Epoch: 11| Step: 0
Training loss: 2.89550171927057
Validation loss: 2.6523800727106224

Epoch: 6| Step: 1
Training loss: 2.644384112413394
Validation loss: 2.6464568827609924

Epoch: 6| Step: 2
Training loss: 2.75562525997736
Validation loss: 2.687635980167453

Epoch: 6| Step: 3
Training loss: 2.25767712038711
Validation loss: 2.6341914971675844

Epoch: 6| Step: 4
Training loss: 2.739793650938943
Validation loss: 2.661231631622783

Epoch: 6| Step: 5
Training loss: 2.396731487488767
Validation loss: 2.64640230283833

Epoch: 6| Step: 6
Training loss: 2.2869177655779653
Validation loss: 2.6569193725900786

Epoch: 6| Step: 7
Training loss: 2.7633543876824462
Validation loss: 2.636643720901545

Epoch: 6| Step: 8
Training loss: 2.547706234326674
Validation loss: 2.6459467605747746

Epoch: 6| Step: 9
Training loss: 2.779063619126344
Validation loss: 2.645974843804034

Epoch: 6| Step: 10
Training loss: 3.2901458768186966
Validation loss: 2.639213877180274

Epoch: 6| Step: 11
Training loss: 2.6347647201050153
Validation loss: 2.6548383215260007

Epoch: 6| Step: 12
Training loss: 1.988671165260873
Validation loss: 2.631553280855797

Epoch: 6| Step: 13
Training loss: 2.4737777697607757
Validation loss: 2.6440124303311063

Epoch: 12| Step: 0
Training loss: 2.5766503943077113
Validation loss: 2.649450582854137

Epoch: 6| Step: 1
Training loss: 2.1000160852452
Validation loss: 2.6577207495274346

Epoch: 6| Step: 2
Training loss: 2.6747781251533422
Validation loss: 2.6369160293433507

Epoch: 6| Step: 3
Training loss: 2.4252363935261325
Validation loss: 2.6377673590625825

Epoch: 6| Step: 4
Training loss: 2.7999630721245645
Validation loss: 2.6543370557994708

Epoch: 6| Step: 5
Training loss: 2.8073099356335827
Validation loss: 2.6427466542481968

Epoch: 6| Step: 6
Training loss: 2.810082647119579
Validation loss: 2.662229375263982

Epoch: 6| Step: 7
Training loss: 2.6015202515983455
Validation loss: 2.6671478363884007

Epoch: 6| Step: 8
Training loss: 2.5373091527268077
Validation loss: 2.6255215172191946

Epoch: 6| Step: 9
Training loss: 2.587932358891081
Validation loss: 2.630749589479139

Epoch: 6| Step: 10
Training loss: 2.4825309771461535
Validation loss: 2.6913387177534513

Epoch: 6| Step: 11
Training loss: 2.8429549227108613
Validation loss: 2.666333838395261

Epoch: 6| Step: 12
Training loss: 2.4754407008678645
Validation loss: 2.659594371457397

Epoch: 6| Step: 13
Training loss: 2.2578636084451205
Validation loss: 2.6457375686816285

Epoch: 13| Step: 0
Training loss: 2.4804306868388633
Validation loss: 2.6463568662586994

Epoch: 6| Step: 1
Training loss: 2.056612570662765
Validation loss: 2.647106139844724

Epoch: 6| Step: 2
Training loss: 2.356671533898809
Validation loss: 2.6399845723582804

Epoch: 6| Step: 3
Training loss: 2.701059423217391
Validation loss: 2.6184886525799147

Epoch: 6| Step: 4
Training loss: 3.3545585703493535
Validation loss: 2.634401756745042

Epoch: 6| Step: 5
Training loss: 2.630416912205942
Validation loss: 2.649543898696374

Epoch: 6| Step: 6
Training loss: 2.4003570529461693
Validation loss: 2.641590024837134

Epoch: 6| Step: 7
Training loss: 2.5667466122285463
Validation loss: 2.638801772927211

Epoch: 6| Step: 8
Training loss: 2.945657320353166
Validation loss: 2.651736939596206

Epoch: 6| Step: 9
Training loss: 1.9618244369033329
Validation loss: 2.624220444713824

Epoch: 6| Step: 10
Training loss: 2.39595285753289
Validation loss: 2.6483585792268056

Epoch: 6| Step: 11
Training loss: 3.0909369424085313
Validation loss: 2.6557897879656727

Epoch: 6| Step: 12
Training loss: 2.0648591392079285
Validation loss: 2.667931798006022

Epoch: 6| Step: 13
Training loss: 2.9292825240930638
Validation loss: 2.6272078417789144

Epoch: 14| Step: 0
Training loss: 2.318308366119555
Validation loss: 2.615607700571743

Epoch: 6| Step: 1
Training loss: 2.062529014614512
Validation loss: 2.6378517485817947

Epoch: 6| Step: 2
Training loss: 2.4157857824117364
Validation loss: 2.6161354341679366

Epoch: 6| Step: 3
Training loss: 3.1539824243417467
Validation loss: 2.6235252203635757

Epoch: 6| Step: 4
Training loss: 2.0685463335446457
Validation loss: 2.6156702910481986

Epoch: 6| Step: 5
Training loss: 3.264738263655416
Validation loss: 2.623627379473394

Epoch: 6| Step: 6
Training loss: 2.6997277405127886
Validation loss: 2.6501044894557206

Epoch: 6| Step: 7
Training loss: 2.3041753684828645
Validation loss: 2.578131334701616

Epoch: 6| Step: 8
Training loss: 2.4805481424917435
Validation loss: 2.6151055096990867

Epoch: 6| Step: 9
Training loss: 2.3462366772589958
Validation loss: 2.6060750352172395

Epoch: 6| Step: 10
Training loss: 3.099159674421606
Validation loss: 2.6021573567506606

Epoch: 6| Step: 11
Training loss: 2.659186860185883
Validation loss: 2.6362703370632823

Epoch: 6| Step: 12
Training loss: 2.156181444930346
Validation loss: 2.6297623410438664

Epoch: 6| Step: 13
Training loss: 2.932415234055272
Validation loss: 2.6343133499093576

Epoch: 15| Step: 0
Training loss: 2.320859902285176
Validation loss: 2.6247338962401856

Epoch: 6| Step: 1
Training loss: 2.38462585489516
Validation loss: 2.648410103055121

Epoch: 6| Step: 2
Training loss: 3.5154616932556735
Validation loss: 2.633617334704402

Epoch: 6| Step: 3
Training loss: 1.9033617746173381
Validation loss: 2.639728837994511

Epoch: 6| Step: 4
Training loss: 2.533108441091422
Validation loss: 2.60283747257059

Epoch: 6| Step: 5
Training loss: 1.935029577576263
Validation loss: 2.624422463961956

Epoch: 6| Step: 6
Training loss: 2.8825501526756527
Validation loss: 2.640278509457526

Epoch: 6| Step: 7
Training loss: 2.416734716400028
Validation loss: 2.6283449175459834

Epoch: 6| Step: 8
Training loss: 2.219666197829114
Validation loss: 2.615761880092166

Epoch: 6| Step: 9
Training loss: 2.867798617976695
Validation loss: 2.6118516038277484

Epoch: 6| Step: 10
Training loss: 3.031108420300276
Validation loss: 2.626573098830927

Epoch: 6| Step: 11
Training loss: 2.5432920938883927
Validation loss: 2.6126311310342616

Epoch: 6| Step: 12
Training loss: 2.2638712442200624
Validation loss: 2.6261203433796005

Epoch: 6| Step: 13
Training loss: 2.5400290190172927
Validation loss: 2.6145457433503534

Epoch: 16| Step: 0
Training loss: 2.8962051555403523
Validation loss: 2.6393082023452585

Epoch: 6| Step: 1
Training loss: 2.410521048415366
Validation loss: 2.6391274232860633

Epoch: 6| Step: 2
Training loss: 3.0569463704914006
Validation loss: 2.6144353259825968

Epoch: 6| Step: 3
Training loss: 2.678015807517521
Validation loss: 2.6328351088702995

Epoch: 6| Step: 4
Training loss: 1.9660280575540114
Validation loss: 2.6519617358967076

Epoch: 6| Step: 5
Training loss: 2.861009474577644
Validation loss: 2.63682817396884

Epoch: 6| Step: 6
Training loss: 2.389531103448571
Validation loss: 2.5923489842808354

Epoch: 6| Step: 7
Training loss: 2.570202857000835
Validation loss: 2.5991663317625613

Epoch: 6| Step: 8
Training loss: 2.3579148580631055
Validation loss: 2.626275812161703

Epoch: 6| Step: 9
Training loss: 2.4647944183974815
Validation loss: 2.6497993381270324

Epoch: 6| Step: 10
Training loss: 1.6467811977592703
Validation loss: 2.62756633951856

Epoch: 6| Step: 11
Training loss: 2.675788289081754
Validation loss: 2.6239554885378373

Epoch: 6| Step: 12
Training loss: 3.176089338722831
Validation loss: 2.651219080376441

Epoch: 6| Step: 13
Training loss: 2.3050174800185177
Validation loss: 2.617769713668226

Epoch: 17| Step: 0
Training loss: 3.2850505798254797
Validation loss: 2.6111337061055457

Epoch: 6| Step: 1
Training loss: 1.877811993557447
Validation loss: 2.632318255993795

Epoch: 6| Step: 2
Training loss: 2.336822648259064
Validation loss: 2.632510189581288

Epoch: 6| Step: 3
Training loss: 2.629589791162796
Validation loss: 2.6108266785998806

Epoch: 6| Step: 4
Training loss: 2.583851136847418
Validation loss: 2.64085893516453

Epoch: 6| Step: 5
Training loss: 2.7945396860457654
Validation loss: 2.6385157661557304

Epoch: 6| Step: 6
Training loss: 3.2472060004544225
Validation loss: 2.59691794092548

Epoch: 6| Step: 7
Training loss: 2.4295056633813594
Validation loss: 2.600953742086607

Epoch: 6| Step: 8
Training loss: 2.837530692182403
Validation loss: 2.574816976295732

Epoch: 6| Step: 9
Training loss: 2.2119070562458023
Validation loss: 2.613244649937748

Epoch: 6| Step: 10
Training loss: 2.1235717012440873
Validation loss: 2.6480683526132585

Epoch: 6| Step: 11
Training loss: 2.613962674785801
Validation loss: 2.61271327545994

Epoch: 6| Step: 12
Training loss: 1.9681023031835791
Validation loss: 2.6396995894179804

Epoch: 6| Step: 13
Training loss: 2.6901232314688266
Validation loss: 2.599511652713991

Epoch: 18| Step: 0
Training loss: 2.662343743265679
Validation loss: 2.624220520424745

Epoch: 6| Step: 1
Training loss: 2.127457263844532
Validation loss: 2.5895213544512226

Epoch: 6| Step: 2
Training loss: 2.7256809328774274
Validation loss: 2.6290716503725733

Epoch: 6| Step: 3
Training loss: 2.270822810810686
Validation loss: 2.6361837263415295

Epoch: 6| Step: 4
Training loss: 2.761075345512801
Validation loss: 2.6359672323109207

Epoch: 6| Step: 5
Training loss: 2.958287646160244
Validation loss: 2.617606134288062

Epoch: 6| Step: 6
Training loss: 2.811619854009552
Validation loss: 2.5912152356129634

Epoch: 6| Step: 7
Training loss: 2.5005337145446496
Validation loss: 2.645211985361399

Epoch: 6| Step: 8
Training loss: 2.83159193117318
Validation loss: 2.5953861116660364

Epoch: 6| Step: 9
Training loss: 2.3328337702257347
Validation loss: 2.6626392041561693

Epoch: 6| Step: 10
Training loss: 2.9534782723137303
Validation loss: 2.604721823963693

Epoch: 6| Step: 11
Training loss: 1.8279444939598266
Validation loss: 2.6213514766961015

Epoch: 6| Step: 12
Training loss: 2.371760518121985
Validation loss: 2.616365856042227

Epoch: 6| Step: 13
Training loss: 2.4230830963344907
Validation loss: 2.6075267832554845

Epoch: 19| Step: 0
Training loss: 2.7507956827571083
Validation loss: 2.654040505980505

Epoch: 6| Step: 1
Training loss: 2.4965104544764682
Validation loss: 2.603420918815223

Epoch: 6| Step: 2
Training loss: 3.274892115089979
Validation loss: 2.6197767819739513

Epoch: 6| Step: 3
Training loss: 2.5508580802341014
Validation loss: 2.582377123499043

Epoch: 6| Step: 4
Training loss: 2.510203519982186
Validation loss: 2.6171042054239897

Epoch: 6| Step: 5
Training loss: 2.575404263215396
Validation loss: 2.618543207323569

Epoch: 6| Step: 6
Training loss: 2.421999626644745
Validation loss: 2.6406333968350686

Epoch: 6| Step: 7
Training loss: 2.932793600827326
Validation loss: 2.6064209654037627

Epoch: 6| Step: 8
Training loss: 2.832709037966027
Validation loss: 2.6399651630347605

Epoch: 6| Step: 9
Training loss: 1.7356220607712076
Validation loss: 2.6119255042656895

Epoch: 6| Step: 10
Training loss: 2.693772647346476
Validation loss: 2.6198034317834393

Epoch: 6| Step: 11
Training loss: 2.1122360606339163
Validation loss: 2.5987866076737673

Epoch: 6| Step: 12
Training loss: 2.6215034085138265
Validation loss: 2.5976626517998533

Epoch: 6| Step: 13
Training loss: 1.7319839078759114
Validation loss: 2.5908432111268582

Epoch: 20| Step: 0
Training loss: 2.093819346631735
Validation loss: 2.627889896782882

Epoch: 6| Step: 1
Training loss: 2.9873220861601046
Validation loss: 2.594416260844176

Epoch: 6| Step: 2
Training loss: 2.74825318083314
Validation loss: 2.6273616581825534

Epoch: 6| Step: 3
Training loss: 2.099271184609259
Validation loss: 2.62774768710858

Epoch: 6| Step: 4
Training loss: 3.3877924324864996
Validation loss: 2.6184520342521687

Epoch: 6| Step: 5
Training loss: 2.410128353705555
Validation loss: 2.566524632557485

Epoch: 6| Step: 6
Training loss: 2.5512245342174253
Validation loss: 2.6281424891872343

Epoch: 6| Step: 7
Training loss: 2.4334410103615465
Validation loss: 2.589976833293901

Epoch: 6| Step: 8
Training loss: 2.8892305877131115
Validation loss: 2.6107699992070077

Epoch: 6| Step: 9
Training loss: 2.0258232986199576
Validation loss: 2.589626435473295

Epoch: 6| Step: 10
Training loss: 2.5272337522551407
Validation loss: 2.632746090980588

Epoch: 6| Step: 11
Training loss: 2.316703687230257
Validation loss: 2.647063605064237

Epoch: 6| Step: 12
Training loss: 2.3674702994750505
Validation loss: 2.593698095085728

Epoch: 6| Step: 13
Training loss: 2.3464266941886702
Validation loss: 2.6409462125838585

Epoch: 21| Step: 0
Training loss: 3.231180133470067
Validation loss: 2.6150119526421567

Epoch: 6| Step: 1
Training loss: 2.9179870523187725
Validation loss: 2.5969532103683046

Epoch: 6| Step: 2
Training loss: 2.1446305271092356
Validation loss: 2.5690087719444943

Epoch: 6| Step: 3
Training loss: 2.1048685042748123
Validation loss: 2.557338431889593

Epoch: 6| Step: 4
Training loss: 2.9630950072676923
Validation loss: 2.618674437556355

Epoch: 6| Step: 5
Training loss: 2.7136385870457245
Validation loss: 2.562584061523141

Epoch: 6| Step: 6
Training loss: 2.660306996290872
Validation loss: 2.5959997034004134

Epoch: 6| Step: 7
Training loss: 2.271422026968385
Validation loss: 2.6310144127180712

Epoch: 6| Step: 8
Training loss: 2.3636173734368886
Validation loss: 2.5801098359801737

Epoch: 6| Step: 9
Training loss: 2.862374655298877
Validation loss: 2.600710969087815

Epoch: 6| Step: 10
Training loss: 1.6736181565906316
Validation loss: 2.5929267066917707

Epoch: 6| Step: 11
Training loss: 2.4749035980253944
Validation loss: 2.6154049244871778

Epoch: 6| Step: 12
Training loss: 2.4965688047426142
Validation loss: 2.597598028845118

Epoch: 6| Step: 13
Training loss: 2.3398942894166246
Validation loss: 2.6484718808143413

Epoch: 22| Step: 0
Training loss: 2.7598549517235904
Validation loss: 2.5837130780189512

Epoch: 6| Step: 1
Training loss: 2.069675563505398
Validation loss: 2.6061204117256374

Epoch: 6| Step: 2
Training loss: 2.5947619382997305
Validation loss: 2.6338257467044

Epoch: 6| Step: 3
Training loss: 3.037491968717279
Validation loss: 2.5728570774194655

Epoch: 6| Step: 4
Training loss: 2.8671228053306623
Validation loss: 2.6057939150172826

Epoch: 6| Step: 5
Training loss: 2.304494248386124
Validation loss: 2.605417642849934

Epoch: 6| Step: 6
Training loss: 2.2309443673208365
Validation loss: 2.620483024081691

Epoch: 6| Step: 7
Training loss: 3.3803081448733154
Validation loss: 2.6571291796209677

Epoch: 6| Step: 8
Training loss: 2.422385672596698
Validation loss: 2.5993516633280307

Epoch: 6| Step: 9
Training loss: 2.57358349847457
Validation loss: 2.589186057943171

Epoch: 6| Step: 10
Training loss: 2.2066019096361407
Validation loss: 2.6505491827264938

Epoch: 6| Step: 11
Training loss: 3.036785616284505
Validation loss: 2.6165177356549694

Epoch: 6| Step: 12
Training loss: 1.790620084159987
Validation loss: 2.5937272419371538

Epoch: 6| Step: 13
Training loss: 1.8503640718751346
Validation loss: 2.5877098624958896

Epoch: 23| Step: 0
Training loss: 2.581833639965598
Validation loss: 2.614707082628957

Epoch: 6| Step: 1
Training loss: 2.284856271209248
Validation loss: 2.557208863346698

Epoch: 6| Step: 2
Training loss: 2.5141778420400236
Validation loss: 2.6139428062116643

Epoch: 6| Step: 3
Training loss: 2.922944877707699
Validation loss: 2.6161173440297274

Epoch: 6| Step: 4
Training loss: 2.9891081818969845
Validation loss: 2.5621965275147263

Epoch: 6| Step: 5
Training loss: 3.1221097168689442
Validation loss: 2.5712996947574203

Epoch: 6| Step: 6
Training loss: 1.8221409336944476
Validation loss: 2.5897452909646086

Epoch: 6| Step: 7
Training loss: 2.412225511975151
Validation loss: 2.585445422541083

Epoch: 6| Step: 8
Training loss: 2.2909192571510775
Validation loss: 2.579600796088689

Epoch: 6| Step: 9
Training loss: 2.5372725060622736
Validation loss: 2.597554255039153

Epoch: 6| Step: 10
Training loss: 2.2819579606659772
Validation loss: 2.5822741788082446

Epoch: 6| Step: 11
Training loss: 2.4170464403633303
Validation loss: 2.601487640739594

Epoch: 6| Step: 12
Training loss: 3.0265296284353265
Validation loss: 2.5709453990093603

Epoch: 6| Step: 13
Training loss: 1.7637389921173323
Validation loss: 2.575045733061156

Epoch: 24| Step: 0
Training loss: 2.5066814783686784
Validation loss: 2.5790184245443064

Epoch: 6| Step: 1
Training loss: 2.0974797293555363
Validation loss: 2.5837292419101714

Epoch: 6| Step: 2
Training loss: 1.7476626181625008
Validation loss: 2.5769387030100357

Epoch: 6| Step: 3
Training loss: 1.9536593507322646
Validation loss: 2.5961820391675574

Epoch: 6| Step: 4
Training loss: 2.3820621497232275
Validation loss: 2.5962679487405986

Epoch: 6| Step: 5
Training loss: 3.5941976517219465
Validation loss: 2.5861558005704173

Epoch: 6| Step: 6
Training loss: 1.7152366101279277
Validation loss: 2.6063983865846496

Epoch: 6| Step: 7
Training loss: 2.831345552764649
Validation loss: 2.583674244279921

Epoch: 6| Step: 8
Training loss: 2.958863349680502
Validation loss: 2.5931327039301237

Epoch: 6| Step: 9
Training loss: 2.6942136431900434
Validation loss: 2.593392711374771

Epoch: 6| Step: 10
Training loss: 2.8050785057791137
Validation loss: 2.5940271252498466

Epoch: 6| Step: 11
Training loss: 2.69129959138714
Validation loss: 2.584136012696245

Epoch: 6| Step: 12
Training loss: 2.555408527687439
Validation loss: 2.5961165375215973

Epoch: 6| Step: 13
Training loss: 1.8095060152837692
Validation loss: 2.6060555639396292

Epoch: 25| Step: 0
Training loss: 2.916187483161994
Validation loss: 2.6011569572892097

Epoch: 6| Step: 1
Training loss: 3.166622830806961
Validation loss: 2.6165882772095266

Epoch: 6| Step: 2
Training loss: 2.6013474934028142
Validation loss: 2.6146291954202736

Epoch: 6| Step: 3
Training loss: 2.007812856236289
Validation loss: 2.600255490856952

Epoch: 6| Step: 4
Training loss: 2.156693786065381
Validation loss: 2.6224018211763593

Epoch: 6| Step: 5
Training loss: 2.7997605766384783
Validation loss: 2.630028132444121

Epoch: 6| Step: 6
Training loss: 2.6282189024706826
Validation loss: 2.6446144169812364

Epoch: 6| Step: 7
Training loss: 2.0729578931021986
Validation loss: 2.6422616525234814

Epoch: 6| Step: 8
Training loss: 2.304401341146255
Validation loss: 2.649922752454078

Epoch: 6| Step: 9
Training loss: 3.045042455217521
Validation loss: 2.6421804117069705

Epoch: 6| Step: 10
Training loss: 2.607384750730564
Validation loss: 2.654380641698025

Epoch: 6| Step: 11
Training loss: 2.5456992401753675
Validation loss: 2.617066451732223

Epoch: 6| Step: 12
Training loss: 2.207936437196384
Validation loss: 2.6067973061562935

Epoch: 6| Step: 13
Training loss: 2.2193643498467317
Validation loss: 2.6093575330442786

Epoch: 26| Step: 0
Training loss: 1.7740353824222157
Validation loss: 2.5552387323345376

Epoch: 6| Step: 1
Training loss: 2.4997426854272757
Validation loss: 2.604269847097368

Epoch: 6| Step: 2
Training loss: 2.5381809053931117
Validation loss: 2.5920528926471262

Epoch: 6| Step: 3
Training loss: 2.911741585279889
Validation loss: 2.5945916323214866

Epoch: 6| Step: 4
Training loss: 2.756794252529784
Validation loss: 2.5890821254725402

Epoch: 6| Step: 5
Training loss: 2.2921858690668193
Validation loss: 2.6138599555179582

Epoch: 6| Step: 6
Training loss: 3.387886030998269
Validation loss: 2.5993696255546936

Epoch: 6| Step: 7
Training loss: 2.368049389137252
Validation loss: 2.56232389759867

Epoch: 6| Step: 8
Training loss: 2.7153885329071334
Validation loss: 2.571077165421679

Epoch: 6| Step: 9
Training loss: 1.685447292104648
Validation loss: 2.5678628613217374

Epoch: 6| Step: 10
Training loss: 2.1300999645720444
Validation loss: 2.576225444091172

Epoch: 6| Step: 11
Training loss: 3.0922333053272673
Validation loss: 2.574431530692268

Epoch: 6| Step: 12
Training loss: 2.431446677443588
Validation loss: 2.585403079833039

Epoch: 6| Step: 13
Training loss: 2.196804639142633
Validation loss: 2.6084192775749284

Epoch: 27| Step: 0
Training loss: 2.0545842249918715
Validation loss: 2.568145673440704

Epoch: 6| Step: 1
Training loss: 2.0585639850122233
Validation loss: 2.5844746703824653

Epoch: 6| Step: 2
Training loss: 1.864057271331335
Validation loss: 2.643993471386208

Epoch: 6| Step: 3
Training loss: 2.7412859094775213
Validation loss: 2.6197941187903355

Epoch: 6| Step: 4
Training loss: 2.252395731960908
Validation loss: 2.602093356998665

Epoch: 6| Step: 5
Training loss: 2.3689913785237326
Validation loss: 2.5719045483609455

Epoch: 6| Step: 6
Training loss: 3.560315851280117
Validation loss: 2.5864929263479524

Epoch: 6| Step: 7
Training loss: 2.358796711971119
Validation loss: 2.582803238553833

Epoch: 6| Step: 8
Training loss: 2.578108353994557
Validation loss: 2.5612946288262295

Epoch: 6| Step: 9
Training loss: 2.676898717389969
Validation loss: 2.5871495904935418

Epoch: 6| Step: 10
Training loss: 2.6662773702498024
Validation loss: 2.599706925478043

Epoch: 6| Step: 11
Training loss: 2.2200934412768505
Validation loss: 2.6106519487016113

Epoch: 6| Step: 12
Training loss: 2.542445443917456
Validation loss: 2.6093587437045276

Epoch: 6| Step: 13
Training loss: 2.700285020548703
Validation loss: 2.5718866106206173

Epoch: 28| Step: 0
Training loss: 3.091551028108612
Validation loss: 2.6001391025159117

Epoch: 6| Step: 1
Training loss: 3.1736759278411677
Validation loss: 2.5809492266948557

Epoch: 6| Step: 2
Training loss: 3.0315936965367487
Validation loss: 2.5556634094680577

Epoch: 6| Step: 3
Training loss: 2.012965377168762
Validation loss: 2.588156647834528

Epoch: 6| Step: 4
Training loss: 1.4605970342959889
Validation loss: 2.5631881968264967

Epoch: 6| Step: 5
Training loss: 2.809845243652028
Validation loss: 2.5887237167559114

Epoch: 6| Step: 6
Training loss: 2.2623064664398815
Validation loss: 2.5751600382431956

Epoch: 6| Step: 7
Training loss: 2.5717147202451205
Validation loss: 2.6107018727229923

Epoch: 6| Step: 8
Training loss: 2.175201283929915
Validation loss: 2.5858429798319604

Epoch: 6| Step: 9
Training loss: 1.9339528376520274
Validation loss: 2.5934506592120026

Epoch: 6| Step: 10
Training loss: 2.400885410699573
Validation loss: 2.5913083790669464

Epoch: 6| Step: 11
Training loss: 1.9321083605688323
Validation loss: 2.5874312236570964

Epoch: 6| Step: 12
Training loss: 2.808784404114418
Validation loss: 2.557461258391895

Epoch: 6| Step: 13
Training loss: 2.9052929686843494
Validation loss: 2.5518442210592056

Epoch: 29| Step: 0
Training loss: 3.0775441817482965
Validation loss: 2.6004140035204784

Epoch: 6| Step: 1
Training loss: 2.2552175426221703
Validation loss: 2.5738369972677035

Epoch: 6| Step: 2
Training loss: 2.76233783781061
Validation loss: 2.5398570377687135

Epoch: 6| Step: 3
Training loss: 2.5663776367078825
Validation loss: 2.5925637638732115

Epoch: 6| Step: 4
Training loss: 3.4541718290050354
Validation loss: 2.5977361145744884

Epoch: 6| Step: 5
Training loss: 2.5913446142475993
Validation loss: 2.594244875290121

Epoch: 6| Step: 6
Training loss: 2.8691161942139742
Validation loss: 2.620343589408721

Epoch: 6| Step: 7
Training loss: 1.9746442088026979
Validation loss: 2.563301565827489

Epoch: 6| Step: 8
Training loss: 1.6566589138583885
Validation loss: 2.6328118359180976

Epoch: 6| Step: 9
Training loss: 1.6606029672420897
Validation loss: 2.5708205350024547

Epoch: 6| Step: 10
Training loss: 2.1246825710395996
Validation loss: 2.603589929930709

Epoch: 6| Step: 11
Training loss: 2.948398258694052
Validation loss: 2.60174439245911

Epoch: 6| Step: 12
Training loss: 2.363667101926938
Validation loss: 2.547285893221213

Epoch: 6| Step: 13
Training loss: 2.2828556837400877
Validation loss: 2.6144057336396527

Epoch: 30| Step: 0
Training loss: 2.583714569838076
Validation loss: 2.559752580910329

Epoch: 6| Step: 1
Training loss: 2.2431593858788865
Validation loss: 2.5850816368724696

Epoch: 6| Step: 2
Training loss: 2.757939495156058
Validation loss: 2.625600700674597

Epoch: 6| Step: 3
Training loss: 2.7481159345222674
Validation loss: 2.5988101471042544

Epoch: 6| Step: 4
Training loss: 2.2399503067158424
Validation loss: 2.605798718529868

Epoch: 6| Step: 5
Training loss: 2.486647040806998
Validation loss: 2.610688174153629

Epoch: 6| Step: 6
Training loss: 2.832232616763493
Validation loss: 2.6029230174530684

Epoch: 6| Step: 7
Training loss: 2.2361418667317676
Validation loss: 2.6262677779421337

Epoch: 6| Step: 8
Training loss: 2.0016272600642764
Validation loss: 2.56859638778867

Epoch: 6| Step: 9
Training loss: 2.9553682395387573
Validation loss: 2.5926299683460248

Epoch: 6| Step: 10
Training loss: 1.9973578882571243
Validation loss: 2.608136094245927

Epoch: 6| Step: 11
Training loss: 2.539908024782202
Validation loss: 2.582664639055477

Epoch: 6| Step: 12
Training loss: 2.9040377821342953
Validation loss: 2.579800872190729

Epoch: 6| Step: 13
Training loss: 2.173543385299295
Validation loss: 2.569978377279712

Epoch: 31| Step: 0
Training loss: 3.297470834413641
Validation loss: 2.571988889599025

Epoch: 6| Step: 1
Training loss: 2.035909033903488
Validation loss: 2.5427110954581176

Epoch: 6| Step: 2
Training loss: 2.1960890947344827
Validation loss: 2.5644068949847543

Epoch: 6| Step: 3
Training loss: 2.3820563445453735
Validation loss: 2.6281178743838107

Epoch: 6| Step: 4
Training loss: 2.0776340220127913
Validation loss: 2.555472522632905

Epoch: 6| Step: 5
Training loss: 2.253614912591628
Validation loss: 2.561418312776743

Epoch: 6| Step: 6
Training loss: 2.710702319758703
Validation loss: 2.566791406816351

Epoch: 6| Step: 7
Training loss: 1.9697373956808877
Validation loss: 2.5638592844262225

Epoch: 6| Step: 8
Training loss: 2.818055240243997
Validation loss: 2.5709706152848186

Epoch: 6| Step: 9
Training loss: 2.668701607401649
Validation loss: 2.5715495246995306

Epoch: 6| Step: 10
Training loss: 3.010015933986324
Validation loss: 2.587976303121808

Epoch: 6| Step: 11
Training loss: 2.7469243276287663
Validation loss: 2.576334715105416

Epoch: 6| Step: 12
Training loss: 2.744595679295961
Validation loss: 2.5912639852963135

Epoch: 6| Step: 13
Training loss: 1.658667761022405
Validation loss: 2.5415841751053043

Epoch: 32| Step: 0
Training loss: 2.7070548536804644
Validation loss: 2.553810272327108

Epoch: 6| Step: 1
Training loss: 2.2507210741668393
Validation loss: 2.560194447596895

Epoch: 6| Step: 2
Training loss: 2.999058575736433
Validation loss: 2.558620307143901

Epoch: 6| Step: 3
Training loss: 2.8102780359989152
Validation loss: 2.583781285569453

Epoch: 6| Step: 4
Training loss: 2.6758943185793647
Validation loss: 2.579489884180892

Epoch: 6| Step: 5
Training loss: 2.4924253152966376
Validation loss: 2.5517543866182835

Epoch: 6| Step: 6
Training loss: 2.7787012144157135
Validation loss: 2.564963250073557

Epoch: 6| Step: 7
Training loss: 2.8768109133003144
Validation loss: 2.5671394961251948

Epoch: 6| Step: 8
Training loss: 2.247566390536797
Validation loss: 2.531352225543172

Epoch: 6| Step: 9
Training loss: 2.1436203097159168
Validation loss: 2.568295043603783

Epoch: 6| Step: 10
Training loss: 2.6532010699937842
Validation loss: 2.5828059847750127

Epoch: 6| Step: 11
Training loss: 2.1777461988187987
Validation loss: 2.548566931036431

Epoch: 6| Step: 12
Training loss: 2.285512417155542
Validation loss: 2.5334217346812613

Epoch: 6| Step: 13
Training loss: 1.4871234545430732
Validation loss: 2.5651369560714934

Epoch: 33| Step: 0
Training loss: 2.263581084042638
Validation loss: 2.559876316082823

Epoch: 6| Step: 1
Training loss: 1.9652853255319807
Validation loss: 2.549544341696311

Epoch: 6| Step: 2
Training loss: 2.8429668312244485
Validation loss: 2.53800610598628

Epoch: 6| Step: 3
Training loss: 2.6735962794389296
Validation loss: 2.5868963210007663

Epoch: 6| Step: 4
Training loss: 2.8128387247199544
Validation loss: 2.629644780483487

Epoch: 6| Step: 5
Training loss: 2.6558039571212495
Validation loss: 2.6313364360563267

Epoch: 6| Step: 6
Training loss: 3.071515262685688
Validation loss: 2.6057606637929203

Epoch: 6| Step: 7
Training loss: 2.5049083210202143
Validation loss: 2.61397775473514

Epoch: 6| Step: 8
Training loss: 2.719061822657692
Validation loss: 2.594556032127713

Epoch: 6| Step: 9
Training loss: 2.3344397192464426
Validation loss: 2.600210692048534

Epoch: 6| Step: 10
Training loss: 2.696109996515678
Validation loss: 2.5847479935862814

Epoch: 6| Step: 11
Training loss: 2.7908635668646613
Validation loss: 2.6179715324925144

Epoch: 6| Step: 12
Training loss: 1.5788376209532766
Validation loss: 2.604521007590553

Epoch: 6| Step: 13
Training loss: 2.0711157050420614
Validation loss: 2.5375334219148726

Epoch: 34| Step: 0
Training loss: 2.1309953151456265
Validation loss: 2.5856077784978364

Epoch: 6| Step: 1
Training loss: 2.551428906879028
Validation loss: 2.57291777349492

Epoch: 6| Step: 2
Training loss: 2.495284210407794
Validation loss: 2.55829162366915

Epoch: 6| Step: 3
Training loss: 2.5194557356052747
Validation loss: 2.5695731153718513

Epoch: 6| Step: 4
Training loss: 2.4640696136623497
Validation loss: 2.528112358498609

Epoch: 6| Step: 5
Training loss: 2.3036859954867985
Validation loss: 2.552052530115777

Epoch: 6| Step: 6
Training loss: 2.2433892727811946
Validation loss: 2.5501336467850564

Epoch: 6| Step: 7
Training loss: 2.282887955083716
Validation loss: 2.6114827093430493

Epoch: 6| Step: 8
Training loss: 2.4451416735958427
Validation loss: 2.6027237416885636

Epoch: 6| Step: 9
Training loss: 1.9551917775719498
Validation loss: 2.639731622839507

Epoch: 6| Step: 10
Training loss: 2.2498170990249275
Validation loss: 2.568435834126099

Epoch: 6| Step: 11
Training loss: 2.2812750828030293
Validation loss: 2.553176650883176

Epoch: 6| Step: 12
Training loss: 3.0170487121553995
Validation loss: 2.6223354136916672

Epoch: 6| Step: 13
Training loss: 3.38093158682269
Validation loss: 2.5800149328948008

Epoch: 35| Step: 0
Training loss: 2.427531177858756
Validation loss: 2.6056963026734627

Epoch: 6| Step: 1
Training loss: 1.7955326539620748
Validation loss: 2.5398944763230866

Epoch: 6| Step: 2
Training loss: 1.904537056668307
Validation loss: 2.5487830380949967

Epoch: 6| Step: 3
Training loss: 3.419155067562394
Validation loss: 2.550486401063309

Epoch: 6| Step: 4
Training loss: 2.1479851263799326
Validation loss: 2.5671837034098712

Epoch: 6| Step: 5
Training loss: 1.7877080862930295
Validation loss: 2.6414338778465303

Epoch: 6| Step: 6
Training loss: 2.5380819921694036
Validation loss: 2.5693904301118113

Epoch: 6| Step: 7
Training loss: 2.7825012821218547
Validation loss: 2.55368678784789

Epoch: 6| Step: 8
Training loss: 2.5472945353390264
Validation loss: 2.6038569355195924

Epoch: 6| Step: 9
Training loss: 2.5326284728832484
Validation loss: 2.6124391819447097

Epoch: 6| Step: 10
Training loss: 2.276119681853708
Validation loss: 2.5725901057975618

Epoch: 6| Step: 11
Training loss: 2.0658990998421873
Validation loss: 2.5991192132748457

Epoch: 6| Step: 12
Training loss: 3.1714107380369287
Validation loss: 2.603341991818821

Epoch: 6| Step: 13
Training loss: 2.6945305173656675
Validation loss: 2.5830862301399

Epoch: 36| Step: 0
Training loss: 3.0363760795815913
Validation loss: 2.57138778394627

Epoch: 6| Step: 1
Training loss: 2.6488715522242874
Validation loss: 2.585134052954861

Epoch: 6| Step: 2
Training loss: 2.239279852125239
Validation loss: 2.5865974928595956

Epoch: 6| Step: 3
Training loss: 2.08760752600859
Validation loss: 2.5398066599044555

Epoch: 6| Step: 4
Training loss: 1.8680431368705346
Validation loss: 2.56921930844644

Epoch: 6| Step: 5
Training loss: 2.183131842986095
Validation loss: 2.5629128146857116

Epoch: 6| Step: 6
Training loss: 2.486419220579327
Validation loss: 2.5657006565822926

Epoch: 6| Step: 7
Training loss: 2.9173977253564587
Validation loss: 2.5560111797791705

Epoch: 6| Step: 8
Training loss: 2.326994551078079
Validation loss: 2.5530848711238288

Epoch: 6| Step: 9
Training loss: 2.7354202452551704
Validation loss: 2.572557931413937

Epoch: 6| Step: 10
Training loss: 2.2268905665656282
Validation loss: 2.5707412872241

Epoch: 6| Step: 11
Training loss: 3.1139324277824207
Validation loss: 2.5309932562834807

Epoch: 6| Step: 12
Training loss: 2.33430806872154
Validation loss: 2.592550390962217

Epoch: 6| Step: 13
Training loss: 2.284416405006408
Validation loss: 2.5765094203351095

Epoch: 37| Step: 0
Training loss: 1.8674653297484534
Validation loss: 2.5893356415518634

Epoch: 6| Step: 1
Training loss: 2.2802441613484663
Validation loss: 2.547487228231762

Epoch: 6| Step: 2
Training loss: 2.757008983925346
Validation loss: 2.5828203543104937

Epoch: 6| Step: 3
Training loss: 2.547240810217844
Validation loss: 2.565202931214837

Epoch: 6| Step: 4
Training loss: 2.0112109682268096
Validation loss: 2.5518796307498954

Epoch: 6| Step: 5
Training loss: 3.0810859497225382
Validation loss: 2.549473254333644

Epoch: 6| Step: 6
Training loss: 2.2537830127365313
Validation loss: 2.5562548481118785

Epoch: 6| Step: 7
Training loss: 2.8089916812152933
Validation loss: 2.586213173054792

Epoch: 6| Step: 8
Training loss: 2.6243713171020744
Validation loss: 2.565718776957572

Epoch: 6| Step: 9
Training loss: 2.5602030810629417
Validation loss: 2.611613716176214

Epoch: 6| Step: 10
Training loss: 2.2107740146325257
Validation loss: 2.606128599539682

Epoch: 6| Step: 11
Training loss: 2.6244771527520263
Validation loss: 2.595402983684872

Epoch: 6| Step: 12
Training loss: 2.3276403549930955
Validation loss: 2.5626211525414444

Epoch: 6| Step: 13
Training loss: 2.6158142130787825
Validation loss: 2.581807567855063

Epoch: 38| Step: 0
Training loss: 2.5153433126694296
Validation loss: 2.5991274842976733

Epoch: 6| Step: 1
Training loss: 1.9955186108605687
Validation loss: 2.59660643203696

Epoch: 6| Step: 2
Training loss: 2.8665062201495832
Validation loss: 2.5541211199368394

Epoch: 6| Step: 3
Training loss: 2.7153653528823316
Validation loss: 2.5611284508696244

Epoch: 6| Step: 4
Training loss: 2.4396659813185146
Validation loss: 2.5817609175126814

Epoch: 6| Step: 5
Training loss: 2.0544073689890103
Validation loss: 2.5646861915403765

Epoch: 6| Step: 6
Training loss: 2.2554368981760615
Validation loss: 2.56823956073511

Epoch: 6| Step: 7
Training loss: 2.001716235031729
Validation loss: 2.589901899986238

Epoch: 6| Step: 8
Training loss: 2.7032870205519464
Validation loss: 2.5254717914846436

Epoch: 6| Step: 9
Training loss: 2.5025409183720093
Validation loss: 2.5987323721366864

Epoch: 6| Step: 10
Training loss: 2.4374098149757533
Validation loss: 2.5307625195198185

Epoch: 6| Step: 11
Training loss: 2.77463662457582
Validation loss: 2.5691167490718145

Epoch: 6| Step: 12
Training loss: 2.7847093648975725
Validation loss: 2.6000509519964217

Epoch: 6| Step: 13
Training loss: 2.50662098562267
Validation loss: 2.5505096773902394

Epoch: 39| Step: 0
Training loss: 2.019342350933451
Validation loss: 2.5309466270467156

Epoch: 6| Step: 1
Training loss: 2.0735870376396637
Validation loss: 2.5599755048076633

Epoch: 6| Step: 2
Training loss: 2.7759338319791356
Validation loss: 2.6208495828420957

Epoch: 6| Step: 3
Training loss: 1.6374872629383719
Validation loss: 2.6194616202120224

Epoch: 6| Step: 4
Training loss: 1.7485522685083126
Validation loss: 2.586985948887943

Epoch: 6| Step: 5
Training loss: 2.7369031281560106
Validation loss: 2.538094141263034

Epoch: 6| Step: 6
Training loss: 2.994759432854156
Validation loss: 2.576798778007728

Epoch: 6| Step: 7
Training loss: 2.8432408809799057
Validation loss: 2.618307240398827

Epoch: 6| Step: 8
Training loss: 2.2881797155878374
Validation loss: 2.603741349138603

Epoch: 6| Step: 9
Training loss: 2.4627016556172774
Validation loss: 2.565331220824561

Epoch: 6| Step: 10
Training loss: 1.932881542011936
Validation loss: 2.53955785760992

Epoch: 6| Step: 11
Training loss: 2.0785001258437115
Validation loss: 2.608088025591936

Epoch: 6| Step: 12
Training loss: 3.0801624971104946
Validation loss: 2.618057182394703

Epoch: 6| Step: 13
Training loss: 3.404278806024552
Validation loss: 2.5987440083312174

Epoch: 40| Step: 0
Training loss: 2.6577625231834903
Validation loss: 2.5985295484138913

Epoch: 6| Step: 1
Training loss: 2.565829184581363
Validation loss: 2.573323381121403

Epoch: 6| Step: 2
Training loss: 2.788370511418408
Validation loss: 2.5656825902858613

Epoch: 6| Step: 3
Training loss: 2.4231814890552763
Validation loss: 2.5696030307206774

Epoch: 6| Step: 4
Training loss: 3.195656606337715
Validation loss: 2.6180273729646224

Epoch: 6| Step: 5
Training loss: 3.1784503601432146
Validation loss: 2.5291280289976554

Epoch: 6| Step: 6
Training loss: 2.5676077203337697
Validation loss: 2.6036531984840994

Epoch: 6| Step: 7
Training loss: 1.9962177872611637
Validation loss: 2.595023520478837

Epoch: 6| Step: 8
Training loss: 2.024231037086406
Validation loss: 2.588324767350537

Epoch: 6| Step: 9
Training loss: 2.0791805712808937
Validation loss: 2.560672728789143

Epoch: 6| Step: 10
Training loss: 2.629971927304439
Validation loss: 2.5574420695675055

Epoch: 6| Step: 11
Training loss: 1.6444294732646652
Validation loss: 2.5264963953307156

Epoch: 6| Step: 12
Training loss: 2.0346892364686715
Validation loss: 2.5949112617397754

Epoch: 6| Step: 13
Training loss: 2.4628029186777325
Validation loss: 2.533141681256364

Epoch: 41| Step: 0
Training loss: 1.4028971650630286
Validation loss: 2.5796305104273674

Epoch: 6| Step: 1
Training loss: 2.451789249317993
Validation loss: 2.575355614485386

Epoch: 6| Step: 2
Training loss: 2.0448132097142935
Validation loss: 2.528046955779142

Epoch: 6| Step: 3
Training loss: 2.1830623846851576
Validation loss: 2.585679555207827

Epoch: 6| Step: 4
Training loss: 2.0748417667586563
Validation loss: 2.5895718392945404

Epoch: 6| Step: 5
Training loss: 2.5733977471658767
Validation loss: 2.6029141860110663

Epoch: 6| Step: 6
Training loss: 2.9386067030420344
Validation loss: 2.5650322970215385

Epoch: 6| Step: 7
Training loss: 2.1989013009119676
Validation loss: 2.5656837905795395

Epoch: 6| Step: 8
Training loss: 1.8063275040098774
Validation loss: 2.5897167053822203

Epoch: 6| Step: 9
Training loss: 3.1188287453657857
Validation loss: 2.563466463583778

Epoch: 6| Step: 10
Training loss: 2.966200687659251
Validation loss: 2.601707156630137

Epoch: 6| Step: 11
Training loss: 2.8296504674939054
Validation loss: 2.528455368919913

Epoch: 6| Step: 12
Training loss: 3.025232378648658
Validation loss: 2.5957888973427075

Epoch: 6| Step: 13
Training loss: 2.2551007204759097
Validation loss: 2.568287771795773

Epoch: 42| Step: 0
Training loss: 1.9047491700450623
Validation loss: 2.5710471513099944

Epoch: 6| Step: 1
Training loss: 2.6933487524718007
Validation loss: 2.5149050169970653

Epoch: 6| Step: 2
Training loss: 2.863025107382562
Validation loss: 2.5660836816880663

Epoch: 6| Step: 3
Training loss: 2.03408720966427
Validation loss: 2.584486609081987

Epoch: 6| Step: 4
Training loss: 3.073292613335511
Validation loss: 2.5927555215315956

Epoch: 6| Step: 5
Training loss: 3.246795688574686
Validation loss: 2.566733886614669

Epoch: 6| Step: 6
Training loss: 1.9531130370727385
Validation loss: 2.5583822226020825

Epoch: 6| Step: 7
Training loss: 2.9562835755971895
Validation loss: 2.610396666778726

Epoch: 6| Step: 8
Training loss: 2.359949307301372
Validation loss: 2.576043060915259

Epoch: 6| Step: 9
Training loss: 2.465358674299611
Validation loss: 2.577728456768933

Epoch: 6| Step: 10
Training loss: 2.258262087928653
Validation loss: 2.5709116197863593

Epoch: 6| Step: 11
Training loss: 1.6828345337171726
Validation loss: 2.5645278877154603

Epoch: 6| Step: 12
Training loss: 1.9900668956305143
Validation loss: 2.6182753090972133

Epoch: 6| Step: 13
Training loss: 2.4227980300666063
Validation loss: 2.561020354163611

Epoch: 43| Step: 0
Training loss: 1.8562326244221758
Validation loss: 2.5849320761108583

Epoch: 6| Step: 1
Training loss: 2.22183700507576
Validation loss: 2.591335091636002

Epoch: 6| Step: 2
Training loss: 2.634117369085408
Validation loss: 2.563838469616985

Epoch: 6| Step: 3
Training loss: 2.556205275314543
Validation loss: 2.5633301204451264

Epoch: 6| Step: 4
Training loss: 2.2993414475071163
Validation loss: 2.5858649621107506

Epoch: 6| Step: 5
Training loss: 2.513181838707008
Validation loss: 2.569494772466867

Epoch: 6| Step: 6
Training loss: 1.8282085301805233
Validation loss: 2.6240432751758354

Epoch: 6| Step: 7
Training loss: 2.5837927276518435
Validation loss: 2.556893837641106

Epoch: 6| Step: 8
Training loss: 2.302201511117245
Validation loss: 2.5663511443548384

Epoch: 6| Step: 9
Training loss: 2.5305658993705595
Validation loss: 2.5702260475344976

Epoch: 6| Step: 10
Training loss: 2.718853652556716
Validation loss: 2.5558888976984373

Epoch: 6| Step: 11
Training loss: 2.6633320363518616
Validation loss: 2.525965098895271

Epoch: 6| Step: 12
Training loss: 2.748749708840917
Validation loss: 2.5708747102034275

Epoch: 6| Step: 13
Training loss: 2.7060654363621777
Validation loss: 2.581846352717396

Epoch: 44| Step: 0
Training loss: 2.5995172932837076
Validation loss: 2.547200156832375

Epoch: 6| Step: 1
Training loss: 2.1374500424977385
Validation loss: 2.556426798730895

Epoch: 6| Step: 2
Training loss: 2.3613210572293934
Validation loss: 2.5223211568968087

Epoch: 6| Step: 3
Training loss: 2.184166684675259
Validation loss: 2.583074146517491

Epoch: 6| Step: 4
Training loss: 2.870094094146206
Validation loss: 2.5501315665742585

Epoch: 6| Step: 5
Training loss: 2.718597407827192
Validation loss: 2.5858827644694173

Epoch: 6| Step: 6
Training loss: 2.752287173657447
Validation loss: 2.5873539820457108

Epoch: 6| Step: 7
Training loss: 2.563881106472178
Validation loss: 2.5329528857544554

Epoch: 6| Step: 8
Training loss: 2.0405973891161806
Validation loss: 2.526708146621233

Epoch: 6| Step: 9
Training loss: 2.50006103441121
Validation loss: 2.614334464418957

Epoch: 6| Step: 10
Training loss: 1.7483235230850955
Validation loss: 2.549721622855085

Epoch: 6| Step: 11
Training loss: 2.6992087688344313
Validation loss: 2.5438626344880197

Epoch: 6| Step: 12
Training loss: 2.688832329802386
Validation loss: 2.5768719334861294

Epoch: 6| Step: 13
Training loss: 1.9389669955800513
Validation loss: 2.558325150152615

Epoch: 45| Step: 0
Training loss: 1.575240567923603
Validation loss: 2.587148369440484

Epoch: 6| Step: 1
Training loss: 2.602236640349826
Validation loss: 2.5585274180581457

Epoch: 6| Step: 2
Training loss: 2.229820678411951
Validation loss: 2.5651218213604983

Epoch: 6| Step: 3
Training loss: 3.125609986853285
Validation loss: 2.6012215452946092

Epoch: 6| Step: 4
Training loss: 1.7707109371596157
Validation loss: 2.5433250290954663

Epoch: 6| Step: 5
Training loss: 1.7508645646827439
Validation loss: 2.615342069368976

Epoch: 6| Step: 6
Training loss: 2.3164991903717387
Validation loss: 2.593595561443185

Epoch: 6| Step: 7
Training loss: 2.5289339372205206
Validation loss: 2.622601988087815

Epoch: 6| Step: 8
Training loss: 2.3207900458179274
Validation loss: 2.6108531914702677

Epoch: 6| Step: 9
Training loss: 2.6302494964903693
Validation loss: 2.55422860631054

Epoch: 6| Step: 10
Training loss: 3.3262666340454667
Validation loss: 2.6262151539527516

Epoch: 6| Step: 11
Training loss: 3.3857195126962814
Validation loss: 2.6613772554732478

Epoch: 6| Step: 12
Training loss: 2.55859524093468
Validation loss: 2.5540362586686545

Epoch: 6| Step: 13
Training loss: 1.5423245787450746
Validation loss: 2.551674663494626

Epoch: 46| Step: 0
Training loss: 2.404721059187023
Validation loss: 2.562466055172266

Epoch: 6| Step: 1
Training loss: 2.8642230182957396
Validation loss: 2.5458841405149326

Epoch: 6| Step: 2
Training loss: 2.1706521962376306
Validation loss: 2.540602732330497

Epoch: 6| Step: 3
Training loss: 2.382264921713486
Validation loss: 2.5671697261889235

Epoch: 6| Step: 4
Training loss: 2.5349012813871417
Validation loss: 2.5622009009745814

Epoch: 6| Step: 5
Training loss: 2.128389236245757
Validation loss: 2.587771653613574

Epoch: 6| Step: 6
Training loss: 2.9209049674993874
Validation loss: 2.5475126221119178

Epoch: 6| Step: 7
Training loss: 3.3047688792063363
Validation loss: 2.5600032428134805

Epoch: 6| Step: 8
Training loss: 2.0653322598179753
Validation loss: 2.5885629378019837

Epoch: 6| Step: 9
Training loss: 1.5261776270097727
Validation loss: 2.542361935643163

Epoch: 6| Step: 10
Training loss: 2.2879148346849547
Validation loss: 2.5678636659963403

Epoch: 6| Step: 11
Training loss: 2.259425764977701
Validation loss: 2.5371668542112067

Epoch: 6| Step: 12
Training loss: 2.4008969299868768
Validation loss: 2.5451027256257115

Epoch: 6| Step: 13
Training loss: 2.7519497461887084
Validation loss: 2.5648405891714257

Epoch: 47| Step: 0
Training loss: 3.023192087706291
Validation loss: 2.563710353302354

Epoch: 6| Step: 1
Training loss: 2.2648714061889397
Validation loss: 2.60066351188143

Epoch: 6| Step: 2
Training loss: 2.8512544648479605
Validation loss: 2.565378278360718

Epoch: 6| Step: 3
Training loss: 2.4682795402268343
Validation loss: 2.6162674534545536

Epoch: 6| Step: 4
Training loss: 3.0518256242465926
Validation loss: 2.60545876046877

Epoch: 6| Step: 5
Training loss: 2.207612681812453
Validation loss: 2.5332294934306527

Epoch: 6| Step: 6
Training loss: 2.682673711636029
Validation loss: 2.586662935994922

Epoch: 6| Step: 7
Training loss: 2.3501449824310936
Validation loss: 2.577454867111205

Epoch: 6| Step: 8
Training loss: 1.8169027326768334
Validation loss: 2.5872015730291595

Epoch: 6| Step: 9
Training loss: 2.208067200177125
Validation loss: 2.5271901118862226

Epoch: 6| Step: 10
Training loss: 2.3660736988807334
Validation loss: 2.6004993301583395

Epoch: 6| Step: 11
Training loss: 1.832968596098372
Validation loss: 2.5673535290781158

Epoch: 6| Step: 12
Training loss: 2.7349573877561233
Validation loss: 2.5887157194900015

Epoch: 6| Step: 13
Training loss: 2.126764910928954
Validation loss: 2.5853245249724908

Epoch: 48| Step: 0
Training loss: 2.4840624150775144
Validation loss: 2.6479404249308462

Epoch: 6| Step: 1
Training loss: 3.2388205821135814
Validation loss: 2.6090645824367256

Epoch: 6| Step: 2
Training loss: 2.0463084827377953
Validation loss: 2.5704991544375724

Epoch: 6| Step: 3
Training loss: 2.2408419835198066
Validation loss: 2.581243520856335

Epoch: 6| Step: 4
Training loss: 1.997958631608466
Validation loss: 2.5664428058035536

Epoch: 6| Step: 5
Training loss: 1.6150325109321477
Validation loss: 2.6115475136606605

Epoch: 6| Step: 6
Training loss: 2.605398746219192
Validation loss: 2.5746965365307797

Epoch: 6| Step: 7
Training loss: 2.3399873157711713
Validation loss: 2.5437978944635713

Epoch: 6| Step: 8
Training loss: 2.3791326405708118
Validation loss: 2.5817026457260908

Epoch: 6| Step: 9
Training loss: 2.6474125772222252
Validation loss: 2.536337680545042

Epoch: 6| Step: 10
Training loss: 2.2664732298870054
Validation loss: 2.5437022067992165

Epoch: 6| Step: 11
Training loss: 2.5948980157763955
Validation loss: 2.5565501197782834

Epoch: 6| Step: 12
Training loss: 3.2527253388230886
Validation loss: 2.5372543078210668

Epoch: 6| Step: 13
Training loss: 2.476117115104292
Validation loss: 2.5529815624197356

Epoch: 49| Step: 0
Training loss: 2.370584197494037
Validation loss: 2.565587068340064

Epoch: 6| Step: 1
Training loss: 2.1532142388131144
Validation loss: 2.5780065335885674

Epoch: 6| Step: 2
Training loss: 2.044858215565357
Validation loss: 2.527405499780464

Epoch: 6| Step: 3
Training loss: 2.4863760704577893
Validation loss: 2.6551808318525576

Epoch: 6| Step: 4
Training loss: 2.5803912750264573
Validation loss: 2.592737513483081

Epoch: 6| Step: 5
Training loss: 2.7608687024701215
Validation loss: 2.6167554508219393

Epoch: 6| Step: 6
Training loss: 2.5465951777538436
Validation loss: 2.611796559384208

Epoch: 6| Step: 7
Training loss: 2.657160165469612
Validation loss: 2.5882474833184865

Epoch: 6| Step: 8
Training loss: 2.736548905651953
Validation loss: 2.646713325760218

Epoch: 6| Step: 9
Training loss: 2.529342874823018
Validation loss: 2.6940222854070104

Epoch: 6| Step: 10
Training loss: 2.39516242764652
Validation loss: 2.6453629373427616

Epoch: 6| Step: 11
Training loss: 2.7985673611242157
Validation loss: 2.6553387892830886

Epoch: 6| Step: 12
Training loss: 2.239637034164198
Validation loss: 2.5969973538766804

Epoch: 6| Step: 13
Training loss: 2.006822512690803
Validation loss: 2.598612497951079

Epoch: 50| Step: 0
Training loss: 2.5624852994171508
Validation loss: 2.5807935679099177

Epoch: 6| Step: 1
Training loss: 2.6179404015572922
Validation loss: 2.5413642114460915

Epoch: 6| Step: 2
Training loss: 2.054507635678445
Validation loss: 2.5932896523445734

Epoch: 6| Step: 3
Training loss: 2.1554607107766786
Validation loss: 2.585619200971305

Epoch: 6| Step: 4
Training loss: 2.6400689436840064
Validation loss: 2.5822595292451456

Epoch: 6| Step: 5
Training loss: 2.0774521276143276
Validation loss: 2.566193005176202

Epoch: 6| Step: 6
Training loss: 3.0377547483013894
Validation loss: 2.4977848570733907

Epoch: 6| Step: 7
Training loss: 1.6347094746087858
Validation loss: 2.5676408698225415

Epoch: 6| Step: 8
Training loss: 3.0328581958376444
Validation loss: 2.536622816910708

Epoch: 6| Step: 9
Training loss: 2.4457532589058295
Validation loss: 2.5532185008601918

Epoch: 6| Step: 10
Training loss: 2.1462822108220556
Validation loss: 2.5386999252842837

Epoch: 6| Step: 11
Training loss: 2.6830531746069726
Validation loss: 2.550640318454619

Epoch: 6| Step: 12
Training loss: 2.0300768472191475
Validation loss: 2.5784731697263195

Epoch: 6| Step: 13
Training loss: 2.919988502584318
Validation loss: 2.5515476417147824

Epoch: 51| Step: 0
Training loss: 3.3315108721362607
Validation loss: 2.6021111628619926

Epoch: 6| Step: 1
Training loss: 2.291726290765254
Validation loss: 2.5545268654751165

Epoch: 6| Step: 2
Training loss: 2.1825018321026324
Validation loss: 2.5752805026210988

Epoch: 6| Step: 3
Training loss: 2.8865165588020245
Validation loss: 2.5377580627357417

Epoch: 6| Step: 4
Training loss: 1.8783379407057263
Validation loss: 2.5765642316113406

Epoch: 6| Step: 5
Training loss: 2.5775282660487497
Validation loss: 2.543646648347541

Epoch: 6| Step: 6
Training loss: 1.9138200255974456
Validation loss: 2.6234120606852684

Epoch: 6| Step: 7
Training loss: 2.7997509539064627
Validation loss: 2.5730560872428594

Epoch: 6| Step: 8
Training loss: 2.5822374316376933
Validation loss: 2.602250681174595

Epoch: 6| Step: 9
Training loss: 1.9317033871289186
Validation loss: 2.653544405620086

Epoch: 6| Step: 10
Training loss: 3.1586747628276353
Validation loss: 2.7144106951779152

Epoch: 6| Step: 11
Training loss: 2.3810486315391364
Validation loss: 2.5964388258028093

Epoch: 6| Step: 12
Training loss: 2.07813211310754
Validation loss: 2.6006276815752494

Epoch: 6| Step: 13
Training loss: 1.7183804201370974
Validation loss: 2.612118388630523

Epoch: 52| Step: 0
Training loss: 2.0431721514946584
Validation loss: 2.559419532671019

Epoch: 6| Step: 1
Training loss: 2.0296569446028703
Validation loss: 2.649690239874114

Epoch: 6| Step: 2
Training loss: 2.5436980670998244
Validation loss: 2.578745310747226

Epoch: 6| Step: 3
Training loss: 3.4916210696597356
Validation loss: 2.5678893612902596

Epoch: 6| Step: 4
Training loss: 2.5004599148184745
Validation loss: 2.52801732673723

Epoch: 6| Step: 5
Training loss: 2.528226954569838
Validation loss: 2.557864244823559

Epoch: 6| Step: 6
Training loss: 2.4988721210661033
Validation loss: 2.597641029408925

Epoch: 6| Step: 7
Training loss: 2.242467244737252
Validation loss: 2.5327482302820843

Epoch: 6| Step: 8
Training loss: 2.0806864390722737
Validation loss: 2.6047325943504633

Epoch: 6| Step: 9
Training loss: 2.3058065510159365
Validation loss: 2.583889152865671

Epoch: 6| Step: 10
Training loss: 2.2399606312970644
Validation loss: 2.5480799252806583

Epoch: 6| Step: 11
Training loss: 1.758550531372292
Validation loss: 2.5491914570569345

Epoch: 6| Step: 12
Training loss: 2.534984800304463
Validation loss: 2.5770747347795955

Epoch: 6| Step: 13
Training loss: 2.620256224763266
Validation loss: 2.554922717170424

Epoch: 53| Step: 0
Training loss: 2.137083702372475
Validation loss: 2.5805197950644994

Epoch: 6| Step: 1
Training loss: 2.8217536435482686
Validation loss: 2.556370436513499

Epoch: 6| Step: 2
Training loss: 2.304265077316259
Validation loss: 2.4930844979715148

Epoch: 6| Step: 3
Training loss: 2.3316043737823673
Validation loss: 2.5886556547819617

Epoch: 6| Step: 4
Training loss: 2.220158304635965
Validation loss: 2.5119514970536168

Epoch: 6| Step: 5
Training loss: 1.8390791614331945
Validation loss: 2.536743419587178

Epoch: 6| Step: 6
Training loss: 2.7841238540087345
Validation loss: 2.534023633401602

Epoch: 6| Step: 7
Training loss: 2.607733112861636
Validation loss: 2.5458232213478698

Epoch: 6| Step: 8
Training loss: 2.853460981092553
Validation loss: 2.5693581537987193

Epoch: 6| Step: 9
Training loss: 2.0758622549587677
Validation loss: 2.5581845643053693

Epoch: 6| Step: 10
Training loss: 2.1007222477637915
Validation loss: 2.5481480028419283

Epoch: 6| Step: 11
Training loss: 2.798032467837097
Validation loss: 2.553263532785895

Epoch: 6| Step: 12
Training loss: 1.9576644025534362
Validation loss: 2.5963517204637907

Epoch: 6| Step: 13
Training loss: 2.9442405470140582
Validation loss: 2.5432549240888096

Epoch: 54| Step: 0
Training loss: 2.028624143145805
Validation loss: 2.5918636586511687

Epoch: 6| Step: 1
Training loss: 2.6499760752623582
Validation loss: 2.623238744698666

Epoch: 6| Step: 2
Training loss: 2.7983986737376276
Validation loss: 2.580664845937594

Epoch: 6| Step: 3
Training loss: 2.9304946804704533
Validation loss: 2.5882641024970976

Epoch: 6| Step: 4
Training loss: 2.1646954543808894
Validation loss: 2.557424776195945

Epoch: 6| Step: 5
Training loss: 2.697341199695898
Validation loss: 2.581145234094752

Epoch: 6| Step: 6
Training loss: 2.507578045589832
Validation loss: 2.5804235673275535

Epoch: 6| Step: 7
Training loss: 2.2075920540267293
Validation loss: 2.650172772563412

Epoch: 6| Step: 8
Training loss: 1.8805914477077492
Validation loss: 2.547666742559992

Epoch: 6| Step: 9
Training loss: 2.2886001067816513
Validation loss: 2.5385709003011745

Epoch: 6| Step: 10
Training loss: 2.7417565683700285
Validation loss: 2.559672291885569

Epoch: 6| Step: 11
Training loss: 1.8860051481918991
Validation loss: 2.6312367056105286

Epoch: 6| Step: 12
Training loss: 2.152883361633609
Validation loss: 2.5749415461994856

Epoch: 6| Step: 13
Training loss: 2.4427596834447782
Validation loss: 2.5853313185021802

Epoch: 55| Step: 0
Training loss: 2.951040995919021
Validation loss: 2.5592568661755806

Epoch: 6| Step: 1
Training loss: 2.5638716213443415
Validation loss: 2.5371868385038123

Epoch: 6| Step: 2
Training loss: 2.003931353971349
Validation loss: 2.5021649205746272

Epoch: 6| Step: 3
Training loss: 2.1085345819290984
Validation loss: 2.5788349445681247

Epoch: 6| Step: 4
Training loss: 2.932221235081003
Validation loss: 2.566343030920244

Epoch: 6| Step: 5
Training loss: 1.8418250946439658
Validation loss: 2.5528249843698565

Epoch: 6| Step: 6
Training loss: 2.6281159390577957
Validation loss: 2.57971955896948

Epoch: 6| Step: 7
Training loss: 3.2483172828847993
Validation loss: 2.53918813541335

Epoch: 6| Step: 8
Training loss: 2.4342239078354693
Validation loss: 2.584656858315239

Epoch: 6| Step: 9
Training loss: 2.0690849839720062
Validation loss: 2.6003763054912885

Epoch: 6| Step: 10
Training loss: 1.8858162753280057
Validation loss: 2.5642519443659926

Epoch: 6| Step: 11
Training loss: 2.5658339235313985
Validation loss: 2.603790062693664

Epoch: 6| Step: 12
Training loss: 1.8765020076457382
Validation loss: 2.565563820385561

Epoch: 6| Step: 13
Training loss: 2.6927993336001843
Validation loss: 2.5447295656146736

Epoch: 56| Step: 0
Training loss: 2.1319742756377718
Validation loss: 2.6078402027679943

Epoch: 6| Step: 1
Training loss: 2.2169779833783148
Validation loss: 2.5427926310675253

Epoch: 6| Step: 2
Training loss: 2.7206640411877774
Validation loss: 2.6121754494900333

Epoch: 6| Step: 3
Training loss: 2.351084106044241
Validation loss: 2.60661354060398

Epoch: 6| Step: 4
Training loss: 2.940886797384302
Validation loss: 2.599133416179538

Epoch: 6| Step: 5
Training loss: 2.694255057514304
Validation loss: 2.5953118015191143

Epoch: 6| Step: 6
Training loss: 2.50178197294671
Validation loss: 2.596334265374703

Epoch: 6| Step: 7
Training loss: 2.3294665676386286
Validation loss: 2.546181878945407

Epoch: 6| Step: 8
Training loss: 1.6994765513621424
Validation loss: 2.640327060602275

Epoch: 6| Step: 9
Training loss: 2.3454239017630014
Validation loss: 2.588232706361045

Epoch: 6| Step: 10
Training loss: 2.44713302947231
Validation loss: 2.575351973122608

Epoch: 6| Step: 11
Training loss: 1.9939840556074473
Validation loss: 2.553321091877383

Epoch: 6| Step: 12
Training loss: 2.3662582940602626
Validation loss: 2.547764347874867

Epoch: 6| Step: 13
Training loss: 2.9282082854225693
Validation loss: 2.580483130702617

Epoch: 57| Step: 0
Training loss: 2.72305211783695
Validation loss: 2.5905289316560176

Epoch: 6| Step: 1
Training loss: 2.3238668736394787
Validation loss: 2.5626911231617906

Epoch: 6| Step: 2
Training loss: 2.4782881155123975
Validation loss: 2.555604099478354

Epoch: 6| Step: 3
Training loss: 2.1798913785138794
Validation loss: 2.576599648575859

Epoch: 6| Step: 4
Training loss: 3.1086885087420333
Validation loss: 2.570360610341599

Epoch: 6| Step: 5
Training loss: 2.40195825476664
Validation loss: 2.606762139416709

Epoch: 6| Step: 6
Training loss: 3.003636540283376
Validation loss: 2.604456089467143

Epoch: 6| Step: 7
Training loss: 2.3188048322831585
Validation loss: 2.5996791525479495

Epoch: 6| Step: 8
Training loss: 1.9737633950260294
Validation loss: 2.623350351691231

Epoch: 6| Step: 9
Training loss: 1.9658332289883966
Validation loss: 2.6030798106337367

Epoch: 6| Step: 10
Training loss: 2.007035992157569
Validation loss: 2.534716928090412

Epoch: 6| Step: 11
Training loss: 2.5633785323518006
Validation loss: 2.5859086725600835

Epoch: 6| Step: 12
Training loss: 2.2956564648780375
Validation loss: 2.566495107102463

Epoch: 6| Step: 13
Training loss: 1.9010158031916449
Validation loss: 2.5789603989494227

Epoch: 58| Step: 0
Training loss: 2.4567356629260453
Validation loss: 2.5626217262695348

Epoch: 6| Step: 1
Training loss: 2.8660417393096083
Validation loss: 2.525031272455321

Epoch: 6| Step: 2
Training loss: 1.8055358478090067
Validation loss: 2.5169975216964104

Epoch: 6| Step: 3
Training loss: 2.43768691299956
Validation loss: 2.5760385952653686

Epoch: 6| Step: 4
Training loss: 2.593983467349365
Validation loss: 2.594785567840143

Epoch: 6| Step: 5
Training loss: 1.953332508507946
Validation loss: 2.5448355593014433

Epoch: 6| Step: 6
Training loss: 2.315970625933228
Validation loss: 2.541905490932174

Epoch: 6| Step: 7
Training loss: 2.3548842380200132
Validation loss: 2.5263620206221926

Epoch: 6| Step: 8
Training loss: 2.9827010168360757
Validation loss: 2.558277038696255

Epoch: 6| Step: 9
Training loss: 2.5025972226642303
Validation loss: 2.5923244128210072

Epoch: 6| Step: 10
Training loss: 2.900868673776874
Validation loss: 2.556954112308903

Epoch: 6| Step: 11
Training loss: 2.3972195692208547
Validation loss: 2.596890597214359

Epoch: 6| Step: 12
Training loss: 1.9080941706996049
Validation loss: 2.544996399567761

Epoch: 6| Step: 13
Training loss: 2.0988063962355827
Validation loss: 2.565706665746568

Epoch: 59| Step: 0
Training loss: 2.7234888103683144
Validation loss: 2.5930811008020775

Epoch: 6| Step: 1
Training loss: 1.8791783506561661
Validation loss: 2.5851789516337185

Epoch: 6| Step: 2
Training loss: 1.9589698311749815
Validation loss: 2.5666260879165765

Epoch: 6| Step: 3
Training loss: 2.6439054078222
Validation loss: 2.5480056312564683

Epoch: 6| Step: 4
Training loss: 2.0923678262729912
Validation loss: 2.577140997938197

Epoch: 6| Step: 5
Training loss: 2.0992879932086916
Validation loss: 2.5677604334705957

Epoch: 6| Step: 6
Training loss: 2.064458783880903
Validation loss: 2.5711118465850418

Epoch: 6| Step: 7
Training loss: 2.6744140696318373
Validation loss: 2.5572039297142406

Epoch: 6| Step: 8
Training loss: 1.5529123264283138
Validation loss: 2.5832160928230437

Epoch: 6| Step: 9
Training loss: 3.3727980600905223
Validation loss: 2.594184670545642

Epoch: 6| Step: 10
Training loss: 2.6695016950311476
Validation loss: 2.5983396620690686

Epoch: 6| Step: 11
Training loss: 2.815096864270324
Validation loss: 2.6105408186639885

Epoch: 6| Step: 12
Training loss: 2.3872854805484005
Validation loss: 2.598485278013187

Epoch: 6| Step: 13
Training loss: 2.6851553336733778
Validation loss: 2.636268588597912

Epoch: 60| Step: 0
Training loss: 2.4222744520002246
Validation loss: 2.5522822062538872

Epoch: 6| Step: 1
Training loss: 2.2922520207826684
Validation loss: 2.571715655050817

Epoch: 6| Step: 2
Training loss: 2.140156158560894
Validation loss: 2.533740949045284

Epoch: 6| Step: 3
Training loss: 2.9299734560965183
Validation loss: 2.527868799054322

Epoch: 6| Step: 4
Training loss: 3.1643666062384046
Validation loss: 2.609421872386134

Epoch: 6| Step: 5
Training loss: 2.0918540911146706
Validation loss: 2.588924530595282

Epoch: 6| Step: 6
Training loss: 1.959200998273465
Validation loss: 2.581037798665225

Epoch: 6| Step: 7
Training loss: 2.693461703209324
Validation loss: 2.5403422255764823

Epoch: 6| Step: 8
Training loss: 1.8669268693541152
Validation loss: 2.585250386729585

Epoch: 6| Step: 9
Training loss: 2.7822803774630476
Validation loss: 2.5692367389570943

Epoch: 6| Step: 10
Training loss: 2.3733538896049056
Validation loss: 2.580770564717602

Epoch: 6| Step: 11
Training loss: 1.9806881995574082
Validation loss: 2.5136232329401946

Epoch: 6| Step: 12
Training loss: 2.358222932781335
Validation loss: 2.5584072986642212

Epoch: 6| Step: 13
Training loss: 2.3947138506666525
Validation loss: 2.554390239628114

Epoch: 61| Step: 0
Training loss: 2.008645682626112
Validation loss: 2.6106800691329104

Epoch: 6| Step: 1
Training loss: 2.327575311476929
Validation loss: 2.567662737108169

Epoch: 6| Step: 2
Training loss: 2.1724198570438267
Validation loss: 2.5737291255361936

Epoch: 6| Step: 3
Training loss: 1.839032231084206
Validation loss: 2.5287569115255617

Epoch: 6| Step: 4
Training loss: 3.141309369425966
Validation loss: 2.5238472966985968

Epoch: 6| Step: 5
Training loss: 2.156937199427788
Validation loss: 2.58033070854793

Epoch: 6| Step: 6
Training loss: 2.1372286172551944
Validation loss: 2.57573644791742

Epoch: 6| Step: 7
Training loss: 2.564915425800627
Validation loss: 2.5757385151628744

Epoch: 6| Step: 8
Training loss: 2.869581839107477
Validation loss: 2.5530708634089354

Epoch: 6| Step: 9
Training loss: 2.4067199050037704
Validation loss: 2.5415647726010886

Epoch: 6| Step: 10
Training loss: 2.6184815808665065
Validation loss: 2.6181748838294725

Epoch: 6| Step: 11
Training loss: 2.561511151323947
Validation loss: 2.579836021382344

Epoch: 6| Step: 12
Training loss: 2.082270338347759
Validation loss: 2.535173193704012

Epoch: 6| Step: 13
Training loss: 2.366576163034504
Validation loss: 2.542828768438678

Epoch: 62| Step: 0
Training loss: 2.067228618740533
Validation loss: 2.541721207914233

Epoch: 6| Step: 1
Training loss: 2.313784474549332
Validation loss: 2.568947782428043

Epoch: 6| Step: 2
Training loss: 2.19369251578025
Validation loss: 2.547650973757018

Epoch: 6| Step: 3
Training loss: 2.1045175643771596
Validation loss: 2.5584975518155333

Epoch: 6| Step: 4
Training loss: 2.656323241177755
Validation loss: 2.544650567153812

Epoch: 6| Step: 5
Training loss: 2.628086273960371
Validation loss: 2.568096453960696

Epoch: 6| Step: 6
Training loss: 1.9710214015702614
Validation loss: 2.595383784482417

Epoch: 6| Step: 7
Training loss: 2.5775086562032596
Validation loss: 2.5725297418128714

Epoch: 6| Step: 8
Training loss: 2.885242130954669
Validation loss: 2.5656964904266344

Epoch: 6| Step: 9
Training loss: 2.1133967736446375
Validation loss: 2.5185760658858642

Epoch: 6| Step: 10
Training loss: 1.9028617889841801
Validation loss: 2.6122659593605078

Epoch: 6| Step: 11
Training loss: 2.239850677383722
Validation loss: 2.5198388988730214

Epoch: 6| Step: 12
Training loss: 2.941528641990071
Validation loss: 2.5683306210144825

Epoch: 6| Step: 13
Training loss: 2.8029198987214534
Validation loss: 2.5550261106855228

Epoch: 63| Step: 0
Training loss: 2.5535154307929786
Validation loss: 2.561455645617336

Epoch: 6| Step: 1
Training loss: 2.355118708119295
Validation loss: 2.518384900384567

Epoch: 6| Step: 2
Training loss: 2.3968180305245856
Validation loss: 2.5186887135712857

Epoch: 6| Step: 3
Training loss: 2.5286795669227944
Validation loss: 2.528364491288532

Epoch: 6| Step: 4
Training loss: 2.0008130804983413
Validation loss: 2.5542274861979672

Epoch: 6| Step: 5
Training loss: 2.476843399820402
Validation loss: 2.528678781207662

Epoch: 6| Step: 6
Training loss: 2.373065964107893
Validation loss: 2.5718206215258923

Epoch: 6| Step: 7
Training loss: 2.008908458790689
Validation loss: 2.587433028160666

Epoch: 6| Step: 8
Training loss: 2.956431964330787
Validation loss: 2.5754666272904694

Epoch: 6| Step: 9
Training loss: 1.927610816590215
Validation loss: 2.518573888612061

Epoch: 6| Step: 10
Training loss: 2.4481077944764738
Validation loss: 2.5868261986462806

Epoch: 6| Step: 11
Training loss: 2.321272012650363
Validation loss: 2.5960904481361524

Epoch: 6| Step: 12
Training loss: 2.4338180903944626
Validation loss: 2.554013053377749

Epoch: 6| Step: 13
Training loss: 2.600418251581926
Validation loss: 2.5663711878347355

Epoch: 64| Step: 0
Training loss: 2.374534059294324
Validation loss: 2.564738211046953

Epoch: 6| Step: 1
Training loss: 2.211628189307981
Validation loss: 2.5671160610023716

Epoch: 6| Step: 2
Training loss: 2.419062205429069
Validation loss: 2.582197483115311

Epoch: 6| Step: 3
Training loss: 1.8383760530771351
Validation loss: 2.5991267963208076

Epoch: 6| Step: 4
Training loss: 2.435132662157425
Validation loss: 2.5927674297639123

Epoch: 6| Step: 5
Training loss: 2.2967805388872127
Validation loss: 2.6051007210582893

Epoch: 6| Step: 6
Training loss: 1.642620217671874
Validation loss: 2.6369876677343256

Epoch: 6| Step: 7
Training loss: 2.5828795085690954
Validation loss: 2.6278390215492813

Epoch: 6| Step: 8
Training loss: 2.424021403485721
Validation loss: 2.609445734645254

Epoch: 6| Step: 9
Training loss: 1.9898184659000777
Validation loss: 2.6222072610545033

Epoch: 6| Step: 10
Training loss: 2.9342908170769526
Validation loss: 2.6734282531518914

Epoch: 6| Step: 11
Training loss: 2.3712767478548327
Validation loss: 2.5735729991737184

Epoch: 6| Step: 12
Training loss: 3.5234172000004187
Validation loss: 2.6196049319708146

Epoch: 6| Step: 13
Training loss: 2.258988018319894
Validation loss: 2.5792437505424175

Epoch: 65| Step: 0
Training loss: 1.3507509827187518
Validation loss: 2.591545684991737

Epoch: 6| Step: 1
Training loss: 2.567437694800189
Validation loss: 2.559883340136964

Epoch: 6| Step: 2
Training loss: 2.0219223653678466
Validation loss: 2.5450713435067844

Epoch: 6| Step: 3
Training loss: 2.2531505355763555
Validation loss: 2.5944034182331426

Epoch: 6| Step: 4
Training loss: 2.595565344466878
Validation loss: 2.5534299502589684

Epoch: 6| Step: 5
Training loss: 2.8329891294950134
Validation loss: 2.5905667269678982

Epoch: 6| Step: 6
Training loss: 2.7483932829873474
Validation loss: 2.552678935298526

Epoch: 6| Step: 7
Training loss: 2.9784962537792214
Validation loss: 2.5751365526651657

Epoch: 6| Step: 8
Training loss: 2.288949592591929
Validation loss: 2.566454340669495

Epoch: 6| Step: 9
Training loss: 2.232861413012836
Validation loss: 2.581885460196989

Epoch: 6| Step: 10
Training loss: 1.6558842525066735
Validation loss: 2.567470722669201

Epoch: 6| Step: 11
Training loss: 3.164169498094389
Validation loss: 2.554257557938703

Epoch: 6| Step: 12
Training loss: 2.400390227700333
Validation loss: 2.580367582969143

Epoch: 6| Step: 13
Training loss: 1.877753778670451
Validation loss: 2.569051539534548

Epoch: 66| Step: 0
Training loss: 2.7415083780558986
Validation loss: 2.5324748258729675

Epoch: 6| Step: 1
Training loss: 2.595666384239601
Validation loss: 2.600898543394614

Epoch: 6| Step: 2
Training loss: 2.415614052336882
Validation loss: 2.535547932528754

Epoch: 6| Step: 3
Training loss: 3.2152384148411173
Validation loss: 2.5292892004262386

Epoch: 6| Step: 4
Training loss: 2.1470662127635856
Validation loss: 2.5612057076616708

Epoch: 6| Step: 5
Training loss: 1.894451210454876
Validation loss: 2.5820817273967878

Epoch: 6| Step: 6
Training loss: 1.8949238478525083
Validation loss: 2.5471318355314514

Epoch: 6| Step: 7
Training loss: 2.160877845365525
Validation loss: 2.529538364738531

Epoch: 6| Step: 8
Training loss: 2.6625283038287058
Validation loss: 2.6018747900251746

Epoch: 6| Step: 9
Training loss: 2.0665340852766865
Validation loss: 2.557011425152415

Epoch: 6| Step: 10
Training loss: 2.407222600782744
Validation loss: 2.5676261599831687

Epoch: 6| Step: 11
Training loss: 2.5722439358780926
Validation loss: 2.6306345427498568

Epoch: 6| Step: 12
Training loss: 2.0084889975510674
Validation loss: 2.61556633230611

Epoch: 6| Step: 13
Training loss: 1.9500257392553937
Validation loss: 2.5873074317077287

Epoch: 67| Step: 0
Training loss: 2.920694204400748
Validation loss: 2.629489389997181

Epoch: 6| Step: 1
Training loss: 2.070365703546922
Validation loss: 2.5731950421390755

Epoch: 6| Step: 2
Training loss: 2.0386946189183273
Validation loss: 2.5925513259186954

Epoch: 6| Step: 3
Training loss: 3.0297297175251265
Validation loss: 2.546356054690129

Epoch: 6| Step: 4
Training loss: 2.068440868872311
Validation loss: 2.569777752906007

Epoch: 6| Step: 5
Training loss: 2.886065872550924
Validation loss: 2.550407051185049

Epoch: 6| Step: 6
Training loss: 1.9473020498528744
Validation loss: 2.527465565815096

Epoch: 6| Step: 7
Training loss: 2.4930470099578677
Validation loss: 2.5910658678658214

Epoch: 6| Step: 8
Training loss: 2.1697372783767475
Validation loss: 2.569029931550006

Epoch: 6| Step: 9
Training loss: 2.8016555251566215
Validation loss: 2.5851126869968053

Epoch: 6| Step: 10
Training loss: 1.5670266765862848
Validation loss: 2.5227828933526557

Epoch: 6| Step: 11
Training loss: 2.5162983816453863
Validation loss: 2.534802365596471

Epoch: 6| Step: 12
Training loss: 1.9876759625160514
Validation loss: 2.602924192940929

Epoch: 6| Step: 13
Training loss: 2.4892041756922416
Validation loss: 2.556026640494007

Epoch: 68| Step: 0
Training loss: 2.647332875395426
Validation loss: 2.5578399557303535

Epoch: 6| Step: 1
Training loss: 2.6832433302418006
Validation loss: 2.571408827393032

Epoch: 6| Step: 2
Training loss: 2.46657611405779
Validation loss: 2.5925466511329316

Epoch: 6| Step: 3
Training loss: 1.6447875486889685
Validation loss: 2.5491771473647824

Epoch: 6| Step: 4
Training loss: 2.8886796333461566
Validation loss: 2.5792823582730384

Epoch: 6| Step: 5
Training loss: 1.798388034859491
Validation loss: 2.618476481936834

Epoch: 6| Step: 6
Training loss: 2.165058003412862
Validation loss: 2.556844215067871

Epoch: 6| Step: 7
Training loss: 2.6746115250911013
Validation loss: 2.5953028216900194

Epoch: 6| Step: 8
Training loss: 2.383847071446989
Validation loss: 2.6210659459586996

Epoch: 6| Step: 9
Training loss: 1.9653888651526112
Validation loss: 2.568160976004123

Epoch: 6| Step: 10
Training loss: 2.915192004206021
Validation loss: 2.5865681504467495

Epoch: 6| Step: 11
Training loss: 2.113059548755239
Validation loss: 2.5476698463979432

Epoch: 6| Step: 12
Training loss: 2.0043261944996678
Validation loss: 2.5317733443409334

Epoch: 6| Step: 13
Training loss: 2.511230801043265
Validation loss: 2.5903395786588916

Epoch: 69| Step: 0
Training loss: 2.5803117205949078
Validation loss: 2.582059782161995

Epoch: 6| Step: 1
Training loss: 2.4490879151657534
Validation loss: 2.576594181455915

Epoch: 6| Step: 2
Training loss: 2.4201055153787006
Validation loss: 2.6068160859460066

Epoch: 6| Step: 3
Training loss: 1.5892547341108658
Validation loss: 2.5920821959674916

Epoch: 6| Step: 4
Training loss: 2.0063407045579553
Validation loss: 2.5739556092685647

Epoch: 6| Step: 5
Training loss: 2.3943175676348907
Validation loss: 2.517306885116994

Epoch: 6| Step: 6
Training loss: 2.7310998975766547
Validation loss: 2.558532186063566

Epoch: 6| Step: 7
Training loss: 2.4166293525007436
Validation loss: 2.5609176060864143

Epoch: 6| Step: 8
Training loss: 2.6110280905256764
Validation loss: 2.600186019136233

Epoch: 6| Step: 9
Training loss: 2.2801709693251135
Validation loss: 2.5766917242367353

Epoch: 6| Step: 10
Training loss: 1.92990024644868
Validation loss: 2.591330230643644

Epoch: 6| Step: 11
Training loss: 3.0018258896161556
Validation loss: 2.581220744893176

Epoch: 6| Step: 12
Training loss: 2.170552571519234
Validation loss: 2.5544212116976786

Epoch: 6| Step: 13
Training loss: 1.9283974230948675
Validation loss: 2.61071977207864

Epoch: 70| Step: 0
Training loss: 1.8252422067527827
Validation loss: 2.5604717166984554

Epoch: 6| Step: 1
Training loss: 3.1867266446086173
Validation loss: 2.553204369371586

Epoch: 6| Step: 2
Training loss: 2.659667117148638
Validation loss: 2.5650132887696193

Epoch: 6| Step: 3
Training loss: 2.293777141358495
Validation loss: 2.5450456442296594

Epoch: 6| Step: 4
Training loss: 2.3225171439828465
Validation loss: 2.5999927615407294

Epoch: 6| Step: 5
Training loss: 2.034815783586397
Validation loss: 2.5304728441322077

Epoch: 6| Step: 6
Training loss: 2.5119433265537405
Validation loss: 2.5353511192690292

Epoch: 6| Step: 7
Training loss: 2.578042323780889
Validation loss: 2.601979173622379

Epoch: 6| Step: 8
Training loss: 2.0832106236241144
Validation loss: 2.55643166391644

Epoch: 6| Step: 9
Training loss: 2.649713349583486
Validation loss: 2.6013449118715144

Epoch: 6| Step: 10
Training loss: 1.7556728650117592
Validation loss: 2.5988135338901213

Epoch: 6| Step: 11
Training loss: 2.855080803898473
Validation loss: 2.581233360600201

Epoch: 6| Step: 12
Training loss: 2.1273321929628923
Validation loss: 2.59843956937113

Epoch: 6| Step: 13
Training loss: 2.0532826309425882
Validation loss: 2.577104478244079

Epoch: 71| Step: 0
Training loss: 2.1085504121061622
Validation loss: 2.606409272008694

Epoch: 6| Step: 1
Training loss: 1.9501482442601805
Validation loss: 2.5671556250616607

Epoch: 6| Step: 2
Training loss: 2.6594708829957963
Validation loss: 2.568827214437947

Epoch: 6| Step: 3
Training loss: 2.3480781772667614
Validation loss: 2.559746185202528

Epoch: 6| Step: 4
Training loss: 2.324580932094466
Validation loss: 2.5971364660492675

Epoch: 6| Step: 5
Training loss: 2.151361203109584
Validation loss: 2.562621214566109

Epoch: 6| Step: 6
Training loss: 2.7589341603159454
Validation loss: 2.5609737208258263

Epoch: 6| Step: 7
Training loss: 1.7844240786455308
Validation loss: 2.611101283794424

Epoch: 6| Step: 8
Training loss: 2.4220733561349417
Validation loss: 2.5525523526772496

Epoch: 6| Step: 9
Training loss: 2.780017536849001
Validation loss: 2.5826106034677947

Epoch: 6| Step: 10
Training loss: 2.7923461220059713
Validation loss: 2.653329178345205

Epoch: 6| Step: 11
Training loss: 2.671870806060533
Validation loss: 2.6105399967011467

Epoch: 6| Step: 12
Training loss: 1.8037018829928055
Validation loss: 2.5708132085110362

Epoch: 6| Step: 13
Training loss: 2.093828569902312
Validation loss: 2.5845584476774865

Epoch: 72| Step: 0
Training loss: 2.157108301916315
Validation loss: 2.5331783092482296

Epoch: 6| Step: 1
Training loss: 3.043624474288956
Validation loss: 2.5563368533052984

Epoch: 6| Step: 2
Training loss: 2.4629973011071753
Validation loss: 2.5494355356121687

Epoch: 6| Step: 3
Training loss: 2.1164283658347527
Validation loss: 2.5536355313217705

Epoch: 6| Step: 4
Training loss: 2.5355615512280347
Validation loss: 2.5591531389558586

Epoch: 6| Step: 5
Training loss: 1.838561046235246
Validation loss: 2.613958357526222

Epoch: 6| Step: 6
Training loss: 2.3143204405532307
Validation loss: 2.6062727892860433

Epoch: 6| Step: 7
Training loss: 2.7872124527901576
Validation loss: 2.544822724119387

Epoch: 6| Step: 8
Training loss: 2.104879378172363
Validation loss: 2.562355805038417

Epoch: 6| Step: 9
Training loss: 2.1098568966324653
Validation loss: 2.5708618118176125

Epoch: 6| Step: 10
Training loss: 2.471357586090863
Validation loss: 2.5910000988538067

Epoch: 6| Step: 11
Training loss: 2.4173380039855195
Validation loss: 2.5777559266680643

Epoch: 6| Step: 12
Training loss: 2.4117521322625888
Validation loss: 2.6043396447270943

Epoch: 6| Step: 13
Training loss: 2.2535459339512522
Validation loss: 2.5565275668020733

Epoch: 73| Step: 0
Training loss: 1.8220575179056555
Validation loss: 2.5433751185406477

Epoch: 6| Step: 1
Training loss: 2.329093885109202
Validation loss: 2.5727498132964297

Epoch: 6| Step: 2
Training loss: 2.1903029875862376
Validation loss: 2.5727315879901593

Epoch: 6| Step: 3
Training loss: 1.8177935153126563
Validation loss: 2.565001716439687

Epoch: 6| Step: 4
Training loss: 2.3157932441740052
Validation loss: 2.5687690901220956

Epoch: 6| Step: 5
Training loss: 1.8979161178414996
Validation loss: 2.5473812980811728

Epoch: 6| Step: 6
Training loss: 3.1232936014954755
Validation loss: 2.545957474145387

Epoch: 6| Step: 7
Training loss: 2.0561079909614683
Validation loss: 2.6107063323528426

Epoch: 6| Step: 8
Training loss: 2.21268709329201
Validation loss: 2.5812671663884985

Epoch: 6| Step: 9
Training loss: 2.9097111918488925
Validation loss: 2.5708305818449246

Epoch: 6| Step: 10
Training loss: 1.9719338482329067
Validation loss: 2.5372219750817444

Epoch: 6| Step: 11
Training loss: 2.7058310662406853
Validation loss: 2.563027521434901

Epoch: 6| Step: 12
Training loss: 1.9111961857147286
Validation loss: 2.5410607793550564

Epoch: 6| Step: 13
Training loss: 2.8239085764472485
Validation loss: 2.585972342971908

Epoch: 74| Step: 0
Training loss: 2.3790868430656635
Validation loss: 2.595592947086176

Epoch: 6| Step: 1
Training loss: 2.4565547608449085
Validation loss: 2.582333453292847

Epoch: 6| Step: 2
Training loss: 2.4789625025010267
Validation loss: 2.5806911605566705

Epoch: 6| Step: 3
Training loss: 2.751168262978159
Validation loss: 2.615368946688021

Epoch: 6| Step: 4
Training loss: 2.4663980604101643
Validation loss: 2.593002594652899

Epoch: 6| Step: 5
Training loss: 2.0898349654824138
Validation loss: 2.5227977780324

Epoch: 6| Step: 6
Training loss: 2.04927702459852
Validation loss: 2.5413022379362107

Epoch: 6| Step: 7
Training loss: 2.149551158451525
Validation loss: 2.5955869152576607

Epoch: 6| Step: 8
Training loss: 2.9596996286421815
Validation loss: 2.55708365491078

Epoch: 6| Step: 9
Training loss: 2.125339256581501
Validation loss: 2.5619168277008293

Epoch: 6| Step: 10
Training loss: 2.1099652630084447
Validation loss: 2.5219354479673934

Epoch: 6| Step: 11
Training loss: 2.236453070336024
Validation loss: 2.5566358534669074

Epoch: 6| Step: 12
Training loss: 1.967423612864
Validation loss: 2.5978930984402853

Epoch: 6| Step: 13
Training loss: 2.7253289260430478
Validation loss: 2.6185918579665297

Epoch: 75| Step: 0
Training loss: 2.4462436450998513
Validation loss: 2.615789345546319

Epoch: 6| Step: 1
Training loss: 2.8527040469769505
Validation loss: 2.6357809777436545

Epoch: 6| Step: 2
Training loss: 1.9180665156353387
Validation loss: 2.5666980935521844

Epoch: 6| Step: 3
Training loss: 1.8274848012644425
Validation loss: 2.60733834465044

Epoch: 6| Step: 4
Training loss: 2.016366038758415
Validation loss: 2.6057410300457056

Epoch: 6| Step: 5
Training loss: 1.962014195273218
Validation loss: 2.649757543904437

Epoch: 6| Step: 6
Training loss: 2.2811592815598565
Validation loss: 2.6627218801730197

Epoch: 6| Step: 7
Training loss: 3.40538419866648
Validation loss: 2.6420299847778583

Epoch: 6| Step: 8
Training loss: 2.247591637089411
Validation loss: 2.6076495315282653

Epoch: 6| Step: 9
Training loss: 1.375366552218619
Validation loss: 2.6260397078191424

Epoch: 6| Step: 10
Training loss: 3.125898766018815
Validation loss: 2.606336579687635

Epoch: 6| Step: 11
Training loss: 2.1073069289203654
Validation loss: 2.6734719959963806

Epoch: 6| Step: 12
Training loss: 2.4404418016906213
Validation loss: 2.561261955718619

Epoch: 6| Step: 13
Training loss: 1.819101151588356
Validation loss: 2.585462451645196

Epoch: 76| Step: 0
Training loss: 2.773963126220077
Validation loss: 2.518540724397243

Epoch: 6| Step: 1
Training loss: 1.4431167006222991
Validation loss: 2.5921731161676944

Epoch: 6| Step: 2
Training loss: 2.2234480642468526
Validation loss: 2.529545119584498

Epoch: 6| Step: 3
Training loss: 2.5350205395410064
Validation loss: 2.5815315474811973

Epoch: 6| Step: 4
Training loss: 2.346075608078218
Validation loss: 2.516759486307773

Epoch: 6| Step: 5
Training loss: 2.637186011838216
Validation loss: 2.5453915946884216

Epoch: 6| Step: 6
Training loss: 2.566177148913717
Validation loss: 2.562642140051639

Epoch: 6| Step: 7
Training loss: 2.457680523937002
Validation loss: 2.601813257483483

Epoch: 6| Step: 8
Training loss: 2.4142238723496114
Validation loss: 2.5821986680380045

Epoch: 6| Step: 9
Training loss: 2.4678586485461644
Validation loss: 2.5594605508804515

Epoch: 6| Step: 10
Training loss: 2.134587705410256
Validation loss: 2.549770651582488

Epoch: 6| Step: 11
Training loss: 2.364615374332636
Validation loss: 2.5757884062402563

Epoch: 6| Step: 12
Training loss: 2.1074671276420416
Validation loss: 2.584980844368908

Epoch: 6| Step: 13
Training loss: 2.296289382289518
Validation loss: 2.5945348509675745

Epoch: 77| Step: 0
Training loss: 2.634153483027053
Validation loss: 2.5640615032038356

Epoch: 6| Step: 1
Training loss: 1.7462998509647105
Validation loss: 2.5598171269892944

Epoch: 6| Step: 2
Training loss: 2.640210000557891
Validation loss: 2.604818359212199

Epoch: 6| Step: 3
Training loss: 1.8961489348363578
Validation loss: 2.6312180850161657

Epoch: 6| Step: 4
Training loss: 2.2074350173388857
Validation loss: 2.587009265453968

Epoch: 6| Step: 5
Training loss: 2.0989381648783536
Validation loss: 2.6385913670002137

Epoch: 6| Step: 6
Training loss: 1.9051230804852177
Validation loss: 2.6507988664300477

Epoch: 6| Step: 7
Training loss: 2.1670870250906877
Validation loss: 2.583139755830816

Epoch: 6| Step: 8
Training loss: 2.3974983290108596
Validation loss: 2.663742061073888

Epoch: 6| Step: 9
Training loss: 2.696295959618578
Validation loss: 2.6721608416151343

Epoch: 6| Step: 10
Training loss: 2.44928864253096
Validation loss: 2.660459944557988

Epoch: 6| Step: 11
Training loss: 2.404215361191822
Validation loss: 2.62294711720731

Epoch: 6| Step: 12
Training loss: 2.228774086677104
Validation loss: 2.6133380269799935

Epoch: 6| Step: 13
Training loss: 2.8762041970078798
Validation loss: 2.604219387792342

Epoch: 78| Step: 0
Training loss: 2.1049395234035297
Validation loss: 2.613014135313482

Epoch: 6| Step: 1
Training loss: 2.1107913313264945
Validation loss: 2.6237165704384315

Epoch: 6| Step: 2
Training loss: 1.8538145381010487
Validation loss: 2.553469119466179

Epoch: 6| Step: 3
Training loss: 1.8464263629294029
Validation loss: 2.557135168649045

Epoch: 6| Step: 4
Training loss: 3.241686090510866
Validation loss: 2.59094307008696

Epoch: 6| Step: 5
Training loss: 2.496831220832659
Validation loss: 2.551091251745508

Epoch: 6| Step: 6
Training loss: 2.7363926010649653
Validation loss: 2.563426951094832

Epoch: 6| Step: 7
Training loss: 2.349408858598963
Validation loss: 2.6009991697466273

Epoch: 6| Step: 8
Training loss: 2.34533566559413
Validation loss: 2.5887189736587652

Epoch: 6| Step: 9
Training loss: 1.8159073995768316
Validation loss: 2.549329405851744

Epoch: 6| Step: 10
Training loss: 2.629442270849695
Validation loss: 2.543348980285123

Epoch: 6| Step: 11
Training loss: 2.2253382479272648
Validation loss: 2.58831636201789

Epoch: 6| Step: 12
Training loss: 2.3313079172257436
Validation loss: 2.563216241122443

Epoch: 6| Step: 13
Training loss: 2.2630418455306573
Validation loss: 2.6134328907456865

Epoch: 79| Step: 0
Training loss: 2.275236905517051
Validation loss: 2.602615143485157

Epoch: 6| Step: 1
Training loss: 2.170228292700504
Validation loss: 2.572066507668097

Epoch: 6| Step: 2
Training loss: 2.6917935163371514
Validation loss: 2.5895476097615666

Epoch: 6| Step: 3
Training loss: 2.1382669422589458
Validation loss: 2.581281725370629

Epoch: 6| Step: 4
Training loss: 1.8271132057499821
Validation loss: 2.5059875550642263

Epoch: 6| Step: 5
Training loss: 1.9313756068594552
Validation loss: 2.5889312686325625

Epoch: 6| Step: 6
Training loss: 1.8471438965727591
Validation loss: 2.7005838787159018

Epoch: 6| Step: 7
Training loss: 2.044861480199959
Validation loss: 2.5878392321203942

Epoch: 6| Step: 8
Training loss: 1.9794523200305103
Validation loss: 2.638310631477425

Epoch: 6| Step: 9
Training loss: 2.6006171081027176
Validation loss: 2.6508814995027454

Epoch: 6| Step: 10
Training loss: 2.5807256355351305
Validation loss: 2.650747021762271

Epoch: 6| Step: 11
Training loss: 2.4110250282839703
Validation loss: 2.607970645986053

Epoch: 6| Step: 12
Training loss: 2.945016051736036
Validation loss: 2.650727833656472

Epoch: 6| Step: 13
Training loss: 2.663807428851675
Validation loss: 2.5789183503777786

Epoch: 80| Step: 0
Training loss: 1.8509839920678952
Validation loss: 2.6224574385184245

Epoch: 6| Step: 1
Training loss: 2.5570246187351953
Validation loss: 2.563000870422581

Epoch: 6| Step: 2
Training loss: 2.810422681194531
Validation loss: 2.5664330514469307

Epoch: 6| Step: 3
Training loss: 1.5549914192379997
Validation loss: 2.6323526737423184

Epoch: 6| Step: 4
Training loss: 1.8515210368346637
Validation loss: 2.5592622228297457

Epoch: 6| Step: 5
Training loss: 2.2266530871786223
Validation loss: 2.6143476727022814

Epoch: 6| Step: 6
Training loss: 2.5432695014646165
Validation loss: 2.6021521036687782

Epoch: 6| Step: 7
Training loss: 2.268546595006821
Validation loss: 2.6135126381137868

Epoch: 6| Step: 8
Training loss: 1.788571219476908
Validation loss: 2.586262378088373

Epoch: 6| Step: 9
Training loss: 2.8187572809028056
Validation loss: 2.5795421982249285

Epoch: 6| Step: 10
Training loss: 2.702965880545221
Validation loss: 2.558140667506136

Epoch: 6| Step: 11
Training loss: 2.618052416551385
Validation loss: 2.5885936084553167

Epoch: 6| Step: 12
Training loss: 2.565776869708158
Validation loss: 2.60050574787964

Epoch: 6| Step: 13
Training loss: 1.9577305319174563
Validation loss: 2.584829117877804

Epoch: 81| Step: 0
Training loss: 1.8494366380862846
Validation loss: 2.6078705705196334

Epoch: 6| Step: 1
Training loss: 2.2172449944577615
Validation loss: 2.6012200635158655

Epoch: 6| Step: 2
Training loss: 2.2998407640482474
Validation loss: 2.6100662442761267

Epoch: 6| Step: 3
Training loss: 2.9470931521560955
Validation loss: 2.6005068327785805

Epoch: 6| Step: 4
Training loss: 2.40615943639182
Validation loss: 2.5867314651654496

Epoch: 6| Step: 5
Training loss: 2.401031085892297
Validation loss: 2.650439493154689

Epoch: 6| Step: 6
Training loss: 2.8058051217585693
Validation loss: 2.608347220175667

Epoch: 6| Step: 7
Training loss: 2.3364050151735203
Validation loss: 2.6604564644852173

Epoch: 6| Step: 8
Training loss: 1.6504004483805281
Validation loss: 2.626464639505186

Epoch: 6| Step: 9
Training loss: 1.8644275777821995
Validation loss: 2.603579269283747

Epoch: 6| Step: 10
Training loss: 1.462577022252335
Validation loss: 2.5457756696706344

Epoch: 6| Step: 11
Training loss: 2.5494510284107537
Validation loss: 2.647177577523456

Epoch: 6| Step: 12
Training loss: 2.785955327822585
Validation loss: 2.608800821081951

Epoch: 6| Step: 13
Training loss: 1.879073105856209
Validation loss: 2.5939976907144824

Epoch: 82| Step: 0
Training loss: 2.0890454630417388
Validation loss: 2.580992443052469

Epoch: 6| Step: 1
Training loss: 2.0801669536124074
Validation loss: 2.6243718243360403

Epoch: 6| Step: 2
Training loss: 2.3374210609529733
Validation loss: 2.5909601397405724

Epoch: 6| Step: 3
Training loss: 1.6960315497900682
Validation loss: 2.5172450454672752

Epoch: 6| Step: 4
Training loss: 2.3336439834158527
Validation loss: 2.555116281096681

Epoch: 6| Step: 5
Training loss: 2.1651481907192074
Validation loss: 2.576619535223201

Epoch: 6| Step: 6
Training loss: 1.9449949196795722
Validation loss: 2.5666870706746296

Epoch: 6| Step: 7
Training loss: 2.470663174392914
Validation loss: 2.587566957206349

Epoch: 6| Step: 8
Training loss: 3.0585638169033373
Validation loss: 2.5774721956772964

Epoch: 6| Step: 9
Training loss: 2.343645627558843
Validation loss: 2.6022048019836324

Epoch: 6| Step: 10
Training loss: 2.790610773236106
Validation loss: 2.6295503202407176

Epoch: 6| Step: 11
Training loss: 1.8133023557868657
Validation loss: 2.5622208606061636

Epoch: 6| Step: 12
Training loss: 2.6110868948857378
Validation loss: 2.585653898394075

Epoch: 6| Step: 13
Training loss: 2.022732762922526
Validation loss: 2.643211151713099

Epoch: 83| Step: 0
Training loss: 2.4930693880746446
Validation loss: 2.586197201383622

Epoch: 6| Step: 1
Training loss: 2.417931773653309
Validation loss: 2.6052819470531303

Epoch: 6| Step: 2
Training loss: 1.8920910408262341
Validation loss: 2.6155207550532142

Epoch: 6| Step: 3
Training loss: 2.3127944346447715
Validation loss: 2.5999827586115862

Epoch: 6| Step: 4
Training loss: 2.299636115438209
Validation loss: 2.571793659912406

Epoch: 6| Step: 5
Training loss: 2.7129808799500164
Validation loss: 2.6122100869991285

Epoch: 6| Step: 6
Training loss: 2.409944843436667
Validation loss: 2.6009966871726196

Epoch: 6| Step: 7
Training loss: 1.4043293446486376
Validation loss: 2.5381818212382834

Epoch: 6| Step: 8
Training loss: 1.5379923086581737
Validation loss: 2.637429939713523

Epoch: 6| Step: 9
Training loss: 2.3225035934502567
Validation loss: 2.6085658693913425

Epoch: 6| Step: 10
Training loss: 2.4343337010221355
Validation loss: 2.622965872264915

Epoch: 6| Step: 11
Training loss: 2.4358799759371625
Validation loss: 2.639214645044389

Epoch: 6| Step: 12
Training loss: 2.889461468742178
Validation loss: 2.6257721204371034

Epoch: 6| Step: 13
Training loss: 2.254760157312612
Validation loss: 2.65691778727181

Epoch: 84| Step: 0
Training loss: 2.5621146866071163
Validation loss: 2.6524837196658235

Epoch: 6| Step: 1
Training loss: 2.4387491400016703
Validation loss: 2.6224127689687258

Epoch: 6| Step: 2
Training loss: 1.7884467118802705
Validation loss: 2.573959453298978

Epoch: 6| Step: 3
Training loss: 1.9257250423624184
Validation loss: 2.585845161933001

Epoch: 6| Step: 4
Training loss: 2.392191654054709
Validation loss: 2.53663404875682

Epoch: 6| Step: 5
Training loss: 2.692711943888995
Validation loss: 2.6007123900403974

Epoch: 6| Step: 6
Training loss: 2.156901164486175
Validation loss: 2.5550940576050856

Epoch: 6| Step: 7
Training loss: 2.5124260127622646
Validation loss: 2.606458050041762

Epoch: 6| Step: 8
Training loss: 2.0348448414430913
Validation loss: 2.5944438450567775

Epoch: 6| Step: 9
Training loss: 3.0383566696211055
Validation loss: 2.5691633892708645

Epoch: 6| Step: 10
Training loss: 2.4900951630818478
Validation loss: 2.6137279970643736

Epoch: 6| Step: 11
Training loss: 1.5453534590458617
Validation loss: 2.6679593742805343

Epoch: 6| Step: 12
Training loss: 1.771973665373266
Validation loss: 2.5921043782763915

Epoch: 6| Step: 13
Training loss: 1.995040705368354
Validation loss: 2.6097822651958054

Epoch: 85| Step: 0
Training loss: 2.0459746499080596
Validation loss: 2.6086968856154398

Epoch: 6| Step: 1
Training loss: 2.9308666246953186
Validation loss: 2.5675995334893633

Epoch: 6| Step: 2
Training loss: 2.601447942004317
Validation loss: 2.5755140242118557

Epoch: 6| Step: 3
Training loss: 1.747113299280754
Validation loss: 2.6164344425377797

Epoch: 6| Step: 4
Training loss: 3.0751751547449846
Validation loss: 2.6252472473445905

Epoch: 6| Step: 5
Training loss: 1.6455701142693868
Validation loss: 2.5684164410767574

Epoch: 6| Step: 6
Training loss: 2.0383935759223064
Validation loss: 2.5763839391459182

Epoch: 6| Step: 7
Training loss: 2.073249317359973
Validation loss: 2.56058821655699

Epoch: 6| Step: 8
Training loss: 1.700262200947676
Validation loss: 2.5609500818756157

Epoch: 6| Step: 9
Training loss: 2.06449943497757
Validation loss: 2.55457436313982

Epoch: 6| Step: 10
Training loss: 2.2719953754337685
Validation loss: 2.582442904392353

Epoch: 6| Step: 11
Training loss: 2.9648900688081485
Validation loss: 2.5477427465094586

Epoch: 6| Step: 12
Training loss: 2.287048703894785
Validation loss: 2.5413376850612615

Epoch: 6| Step: 13
Training loss: 2.0790319543082814
Validation loss: 2.5922173410774745

Epoch: 86| Step: 0
Training loss: 2.390476995137629
Validation loss: 2.6027465050563663

Epoch: 6| Step: 1
Training loss: 2.120209398904419
Validation loss: 2.585399844544978

Epoch: 6| Step: 2
Training loss: 3.186562493774977
Validation loss: 2.6287454949892255

Epoch: 6| Step: 3
Training loss: 3.206761970319835
Validation loss: 2.638788551519992

Epoch: 6| Step: 4
Training loss: 2.2764372550952285
Validation loss: 2.5961527974288723

Epoch: 6| Step: 5
Training loss: 2.199568563118932
Validation loss: 2.5963472285252918

Epoch: 6| Step: 6
Training loss: 2.0280461794168216
Validation loss: 2.5618196064852183

Epoch: 6| Step: 7
Training loss: 1.7386275385849683
Validation loss: 2.559118225854872

Epoch: 6| Step: 8
Training loss: 2.1901871661328105
Validation loss: 2.6332978990641482

Epoch: 6| Step: 9
Training loss: 2.5201593133734055
Validation loss: 2.574876468744635

Epoch: 6| Step: 10
Training loss: 1.742529181259322
Validation loss: 2.5388879647544256

Epoch: 6| Step: 11
Training loss: 1.584434059742737
Validation loss: 2.6165868420988967

Epoch: 6| Step: 12
Training loss: 2.1231070670515093
Validation loss: 2.5852164639511543

Epoch: 6| Step: 13
Training loss: 2.3218596466912262
Validation loss: 2.6124375392133485

Epoch: 87| Step: 0
Training loss: 1.8418142210708888
Validation loss: 2.5937769325420708

Epoch: 6| Step: 1
Training loss: 3.4206960380283014
Validation loss: 2.6069457496902197

Epoch: 6| Step: 2
Training loss: 2.304407652335695
Validation loss: 2.595073852345674

Epoch: 6| Step: 3
Training loss: 1.6102243052782168
Validation loss: 2.5714455747799088

Epoch: 6| Step: 4
Training loss: 2.3150348688716975
Validation loss: 2.6139000130161167

Epoch: 6| Step: 5
Training loss: 2.040315323563461
Validation loss: 2.547822694041009

Epoch: 6| Step: 6
Training loss: 2.167085484838148
Validation loss: 2.5581434091364925

Epoch: 6| Step: 7
Training loss: 1.7308295500261996
Validation loss: 2.5957301903608387

Epoch: 6| Step: 8
Training loss: 2.105628409197508
Validation loss: 2.6164055562390995

Epoch: 6| Step: 9
Training loss: 2.156601808015889
Validation loss: 2.6078992770906764

Epoch: 6| Step: 10
Training loss: 2.3355994346715967
Validation loss: 2.5435301927079697

Epoch: 6| Step: 11
Training loss: 2.174791751406857
Validation loss: 2.582418284839017

Epoch: 6| Step: 12
Training loss: 1.9147296599161836
Validation loss: 2.608811536549672

Epoch: 6| Step: 13
Training loss: 2.8395856611466654
Validation loss: 2.5864213258449134

Epoch: 88| Step: 0
Training loss: 2.218787394463709
Validation loss: 2.5498107964627446

Epoch: 6| Step: 1
Training loss: 2.4791052247028413
Validation loss: 2.5375943991604877

Epoch: 6| Step: 2
Training loss: 1.7786932548430656
Validation loss: 2.579048854333106

Epoch: 6| Step: 3
Training loss: 2.223294077769267
Validation loss: 2.6558021467053727

Epoch: 6| Step: 4
Training loss: 2.0432177769322997
Validation loss: 2.5855485944467973

Epoch: 6| Step: 5
Training loss: 2.636470528680777
Validation loss: 2.516405383392851

Epoch: 6| Step: 6
Training loss: 2.5597793278355763
Validation loss: 2.602781779486223

Epoch: 6| Step: 7
Training loss: 2.086136597693411
Validation loss: 2.5713889699868617

Epoch: 6| Step: 8
Training loss: 1.968277768614015
Validation loss: 2.5933004242427846

Epoch: 6| Step: 9
Training loss: 1.7180738332823469
Validation loss: 2.582401158723804

Epoch: 6| Step: 10
Training loss: 1.999117835517715
Validation loss: 2.520134227271026

Epoch: 6| Step: 11
Training loss: 2.4844358424767785
Validation loss: 2.628969090670955

Epoch: 6| Step: 12
Training loss: 1.9484911460802559
Validation loss: 2.5150512056718424

Epoch: 6| Step: 13
Training loss: 3.27510731062635
Validation loss: 2.6185140559593942

Epoch: 89| Step: 0
Training loss: 1.7664663960046059
Validation loss: 2.619519825679215

Epoch: 6| Step: 1
Training loss: 2.292743210366224
Validation loss: 2.6183186909537124

Epoch: 6| Step: 2
Training loss: 1.968978989620296
Validation loss: 2.6078959097226404

Epoch: 6| Step: 3
Training loss: 2.1984328496974626
Validation loss: 2.620070127296073

Epoch: 6| Step: 4
Training loss: 2.151104523584737
Validation loss: 2.6385487476670018

Epoch: 6| Step: 5
Training loss: 2.654565074807766
Validation loss: 2.591720859741915

Epoch: 6| Step: 6
Training loss: 2.4264699354527863
Validation loss: 2.6056909042274796

Epoch: 6| Step: 7
Training loss: 2.29429990740629
Validation loss: 2.5565236343881947

Epoch: 6| Step: 8
Training loss: 1.8662161115877058
Validation loss: 2.6077617142815592

Epoch: 6| Step: 9
Training loss: 1.929892525225249
Validation loss: 2.567657846775958

Epoch: 6| Step: 10
Training loss: 2.2551596080517418
Validation loss: 2.5246605836976155

Epoch: 6| Step: 11
Training loss: 2.310608580048311
Validation loss: 2.5681826377175807

Epoch: 6| Step: 12
Training loss: 2.949481957418738
Validation loss: 2.5897571670075115

Epoch: 6| Step: 13
Training loss: 2.189432979494462
Validation loss: 2.5918631373896637

Epoch: 90| Step: 0
Training loss: 2.2367178631459996
Validation loss: 2.588411075872117

Epoch: 6| Step: 1
Training loss: 1.9198989013042453
Validation loss: 2.637145027270357

Epoch: 6| Step: 2
Training loss: 2.6878693792531583
Validation loss: 2.574692292330712

Epoch: 6| Step: 3
Training loss: 1.8662302923471414
Validation loss: 2.5772444174855296

Epoch: 6| Step: 4
Training loss: 2.4890486224135455
Validation loss: 2.5642969762324848

Epoch: 6| Step: 5
Training loss: 1.9466739164940294
Validation loss: 2.6287662795707485

Epoch: 6| Step: 6
Training loss: 2.193838473109119
Validation loss: 2.6124658001329744

Epoch: 6| Step: 7
Training loss: 1.9656198838111267
Validation loss: 2.6082109455080715

Epoch: 6| Step: 8
Training loss: 2.0247618123068025
Validation loss: 2.6728267796289327

Epoch: 6| Step: 9
Training loss: 2.0898189935376537
Validation loss: 2.6217567677868083

Epoch: 6| Step: 10
Training loss: 2.245106143268834
Validation loss: 2.5918376107788834

Epoch: 6| Step: 11
Training loss: 2.5437288100651125
Validation loss: 2.553044878894293

Epoch: 6| Step: 12
Training loss: 2.282808268058139
Validation loss: 2.6012170235753884

Epoch: 6| Step: 13
Training loss: 2.423693952143551
Validation loss: 2.6338203455663445

Epoch: 91| Step: 0
Training loss: 2.3887844605008777
Validation loss: 2.638203799400826

Epoch: 6| Step: 1
Training loss: 1.7967976429085628
Validation loss: 2.6432934883196504

Epoch: 6| Step: 2
Training loss: 1.7522743976694355
Validation loss: 2.6075245583421025

Epoch: 6| Step: 3
Training loss: 1.8314174119895432
Validation loss: 2.592914523340862

Epoch: 6| Step: 4
Training loss: 2.672964665811221
Validation loss: 2.652425548432609

Epoch: 6| Step: 5
Training loss: 2.924276415668809
Validation loss: 2.6589343550518305

Epoch: 6| Step: 6
Training loss: 2.3868883637690708
Validation loss: 2.562323052414427

Epoch: 6| Step: 7
Training loss: 1.902720451264878
Validation loss: 2.5599006246610543

Epoch: 6| Step: 8
Training loss: 1.932520836524112
Validation loss: 2.6312815047406937

Epoch: 6| Step: 9
Training loss: 1.6543888215113456
Validation loss: 2.6560156251316562

Epoch: 6| Step: 10
Training loss: 2.475744166386883
Validation loss: 2.6273576956704114

Epoch: 6| Step: 11
Training loss: 1.937302610125285
Validation loss: 2.555790825278771

Epoch: 6| Step: 12
Training loss: 2.686729498129308
Validation loss: 2.6670823120995655

Epoch: 6| Step: 13
Training loss: 2.6306773054053134
Validation loss: 2.6357554995678942

Epoch: 92| Step: 0
Training loss: 2.784454728172061
Validation loss: 2.6045510783985364

Epoch: 6| Step: 1
Training loss: 2.4473408339650553
Validation loss: 2.629863109531342

Epoch: 6| Step: 2
Training loss: 1.8823577204987214
Validation loss: 2.5881573540806224

Epoch: 6| Step: 3
Training loss: 2.145337118409024
Validation loss: 2.5506284705963114

Epoch: 6| Step: 4
Training loss: 1.7809291349831378
Validation loss: 2.6064376135367358

Epoch: 6| Step: 5
Training loss: 1.840570585680316
Validation loss: 2.5621226660943064

Epoch: 6| Step: 6
Training loss: 2.3747234183435237
Validation loss: 2.608623815292598

Epoch: 6| Step: 7
Training loss: 2.161447756331582
Validation loss: 2.5572200902386255

Epoch: 6| Step: 8
Training loss: 2.5426287680156765
Validation loss: 2.580846994881402

Epoch: 6| Step: 9
Training loss: 1.8473026509888915
Validation loss: 2.55927770272094

Epoch: 6| Step: 10
Training loss: 2.1249831703865962
Validation loss: 2.565424885846366

Epoch: 6| Step: 11
Training loss: 2.238871391749684
Validation loss: 2.638283001348735

Epoch: 6| Step: 12
Training loss: 2.175747390017894
Validation loss: 2.575752538448311

Epoch: 6| Step: 13
Training loss: 2.451238113055472
Validation loss: 2.56180705804098

Epoch: 93| Step: 0
Training loss: 1.7791002920784655
Validation loss: 2.5967009123754514

Epoch: 6| Step: 1
Training loss: 2.330956906464056
Validation loss: 2.6035052921961044

Epoch: 6| Step: 2
Training loss: 1.6398834005635423
Validation loss: 2.613129448390028

Epoch: 6| Step: 3
Training loss: 2.5130402928259428
Validation loss: 2.6525389834028226

Epoch: 6| Step: 4
Training loss: 2.1871452043817414
Validation loss: 2.612804177325013

Epoch: 6| Step: 5
Training loss: 2.647644374322133
Validation loss: 2.684147599476366

Epoch: 6| Step: 6
Training loss: 2.739634311525659
Validation loss: 2.6646036622020324

Epoch: 6| Step: 7
Training loss: 2.132672092161743
Validation loss: 2.6516701353695007

Epoch: 6| Step: 8
Training loss: 1.8312826680989847
Validation loss: 2.6994936076700498

Epoch: 6| Step: 9
Training loss: 2.5884484952946174
Validation loss: 2.5945463222030263

Epoch: 6| Step: 10
Training loss: 2.1101237910689634
Validation loss: 2.613028521213142

Epoch: 6| Step: 11
Training loss: 1.6086449309734367
Validation loss: 2.6136238926500774

Epoch: 6| Step: 12
Training loss: 2.376092960440065
Validation loss: 2.584672201488998

Epoch: 6| Step: 13
Training loss: 2.3412718832660318
Validation loss: 2.616316472955873

Epoch: 94| Step: 0
Training loss: 1.7783644208303164
Validation loss: 2.61297046008848

Epoch: 6| Step: 1
Training loss: 1.7246016332640508
Validation loss: 2.525684014134137

Epoch: 6| Step: 2
Training loss: 2.7101502608416994
Validation loss: 2.528907979674519

Epoch: 6| Step: 3
Training loss: 2.3987516017349884
Validation loss: 2.6137686494915693

Epoch: 6| Step: 4
Training loss: 1.6691883640082101
Validation loss: 2.566390596339128

Epoch: 6| Step: 5
Training loss: 2.2544853220258996
Validation loss: 2.6064533773301997

Epoch: 6| Step: 6
Training loss: 2.488410024140352
Validation loss: 2.601447980191156

Epoch: 6| Step: 7
Training loss: 3.034435211118549
Validation loss: 2.608234636025858

Epoch: 6| Step: 8
Training loss: 2.134989650799506
Validation loss: 2.6162306825301624

Epoch: 6| Step: 9
Training loss: 2.0361417116112484
Validation loss: 2.5353185349978022

Epoch: 6| Step: 10
Training loss: 2.0353859682088062
Validation loss: 2.579417881314564

Epoch: 6| Step: 11
Training loss: 2.551808452724172
Validation loss: 2.5768110915277807

Epoch: 6| Step: 12
Training loss: 1.8084284334036214
Validation loss: 2.658424564418439

Epoch: 6| Step: 13
Training loss: 1.2995880427901507
Validation loss: 2.621926688411113

Epoch: 95| Step: 0
Training loss: 2.492323151247964
Validation loss: 2.5797282156608743

Epoch: 6| Step: 1
Training loss: 1.7054676428112816
Validation loss: 2.6438348586679994

Epoch: 6| Step: 2
Training loss: 1.6595630977366491
Validation loss: 2.6415530498231354

Epoch: 6| Step: 3
Training loss: 2.184982704251271
Validation loss: 2.6061109354804337

Epoch: 6| Step: 4
Training loss: 2.1517839482633154
Validation loss: 2.631705967531366

Epoch: 6| Step: 5
Training loss: 1.545932829637569
Validation loss: 2.618951793048806

Epoch: 6| Step: 6
Training loss: 1.4752124083861846
Validation loss: 2.6691221664090437

Epoch: 6| Step: 7
Training loss: 2.8669097516398825
Validation loss: 2.693355081733231

Epoch: 6| Step: 8
Training loss: 1.9075122468887689
Validation loss: 2.6740357657617784

Epoch: 6| Step: 9
Training loss: 2.5715595223370986
Validation loss: 2.631187110807761

Epoch: 6| Step: 10
Training loss: 2.24087773258385
Validation loss: 2.707525709974728

Epoch: 6| Step: 11
Training loss: 2.5585881589872117
Validation loss: 2.6315121632601177

Epoch: 6| Step: 12
Training loss: 2.3979235169826287
Validation loss: 2.5527089552329993

Epoch: 6| Step: 13
Training loss: 2.48074536320596
Validation loss: 2.6125018483327263

Epoch: 96| Step: 0
Training loss: 2.0959492989443036
Validation loss: 2.6101298810716744

Epoch: 6| Step: 1
Training loss: 2.2769161496879855
Validation loss: 2.5729966608088333

Epoch: 6| Step: 2
Training loss: 2.0703129606426374
Validation loss: 2.6280507268098443

Epoch: 6| Step: 3
Training loss: 1.7602730716926478
Validation loss: 2.6064181907045954

Epoch: 6| Step: 4
Training loss: 2.7965428671945367
Validation loss: 2.6070036477698824

Epoch: 6| Step: 5
Training loss: 2.062611316798041
Validation loss: 2.5791881488486754

Epoch: 6| Step: 6
Training loss: 2.1131652687385647
Validation loss: 2.598773519077148

Epoch: 6| Step: 7
Training loss: 2.0880958163686314
Validation loss: 2.571310612865544

Epoch: 6| Step: 8
Training loss: 1.94831769218508
Validation loss: 2.6098976601649

Epoch: 6| Step: 9
Training loss: 2.5842816447840726
Validation loss: 2.5817357527582705

Epoch: 6| Step: 10
Training loss: 1.8597548922169167
Validation loss: 2.6415706987628687

Epoch: 6| Step: 11
Training loss: 1.2836282568743622
Validation loss: 2.639182183770432

Epoch: 6| Step: 12
Training loss: 2.764265340432147
Validation loss: 2.6393505534735806

Epoch: 6| Step: 13
Training loss: 2.5487989402064763
Validation loss: 2.5997776327654396

Epoch: 97| Step: 0
Training loss: 2.0891051511326597
Validation loss: 2.646084072717421

Epoch: 6| Step: 1
Training loss: 2.6759170386394016
Validation loss: 2.5961865849617873

Epoch: 6| Step: 2
Training loss: 1.9453942461724234
Validation loss: 2.619331127258379

Epoch: 6| Step: 3
Training loss: 2.484152154104827
Validation loss: 2.6442395213233496

Epoch: 6| Step: 4
Training loss: 2.189308726630506
Validation loss: 2.702887302304057

Epoch: 6| Step: 5
Training loss: 1.8984680173328423
Validation loss: 2.6231642010643648

Epoch: 6| Step: 6
Training loss: 2.1517348631145468
Validation loss: 2.67969489953861

Epoch: 6| Step: 7
Training loss: 2.629245775808982
Validation loss: 2.617576198276406

Epoch: 6| Step: 8
Training loss: 2.2528931772277327
Validation loss: 2.649187212581665

Epoch: 6| Step: 9
Training loss: 1.7377828162020799
Validation loss: 2.5921223294046807

Epoch: 6| Step: 10
Training loss: 1.8439151237383529
Validation loss: 2.6009727321213756

Epoch: 6| Step: 11
Training loss: 2.148231080282564
Validation loss: 2.6220705473963077

Epoch: 6| Step: 12
Training loss: 2.3790376373238544
Validation loss: 2.6355012470489196

Epoch: 6| Step: 13
Training loss: 1.9499197943405164
Validation loss: 2.563425354461252

Epoch: 98| Step: 0
Training loss: 2.1991857929368925
Validation loss: 2.5995118667195696

Epoch: 6| Step: 1
Training loss: 2.9158645253467763
Validation loss: 2.6233079921858877

Epoch: 6| Step: 2
Training loss: 1.6997012100185793
Validation loss: 2.6129518766063122

Epoch: 6| Step: 3
Training loss: 2.087071942510102
Validation loss: 2.6148512704249303

Epoch: 6| Step: 4
Training loss: 2.4373286260368805
Validation loss: 2.577965055185948

Epoch: 6| Step: 5
Training loss: 1.0990564504396723
Validation loss: 2.613640882591191

Epoch: 6| Step: 6
Training loss: 1.704727678673359
Validation loss: 2.632118292048939

Epoch: 6| Step: 7
Training loss: 2.2907345234290757
Validation loss: 2.58469186460157

Epoch: 6| Step: 8
Training loss: 2.3094947463853526
Validation loss: 2.593723205060321

Epoch: 6| Step: 9
Training loss: 2.100416246669036
Validation loss: 2.680632799866087

Epoch: 6| Step: 10
Training loss: 1.5001453488182992
Validation loss: 2.6120086901521633

Epoch: 6| Step: 11
Training loss: 2.1668682982696246
Validation loss: 2.6571010348255935

Epoch: 6| Step: 12
Training loss: 2.386746720120478
Validation loss: 2.723458943927301

Epoch: 6| Step: 13
Training loss: 2.6816630296542217
Validation loss: 2.630296570962801

Epoch: 99| Step: 0
Training loss: 2.5015644900274134
Validation loss: 2.6683718822187803

Epoch: 6| Step: 1
Training loss: 1.9069898451608254
Validation loss: 2.6461887471337633

Epoch: 6| Step: 2
Training loss: 2.237404217081758
Validation loss: 2.6156308531458308

Epoch: 6| Step: 3
Training loss: 2.4627481248426553
Validation loss: 2.586315984005594

Epoch: 6| Step: 4
Training loss: 1.9778945113615995
Validation loss: 2.566959973602148

Epoch: 6| Step: 5
Training loss: 1.6703447843737809
Validation loss: 2.5988996622703913

Epoch: 6| Step: 6
Training loss: 2.120242234192318
Validation loss: 2.563560219989111

Epoch: 6| Step: 7
Training loss: 1.8243640784589803
Validation loss: 2.613023122708283

Epoch: 6| Step: 8
Training loss: 2.5343815776396226
Validation loss: 2.572608957667833

Epoch: 6| Step: 9
Training loss: 2.622142053400553
Validation loss: 2.6242690203917625

Epoch: 6| Step: 10
Training loss: 2.5088430408508
Validation loss: 2.613473471740989

Epoch: 6| Step: 11
Training loss: 2.1184654742469067
Validation loss: 2.584320284884267

Epoch: 6| Step: 12
Training loss: 1.607250552126454
Validation loss: 2.5536010173921464

Epoch: 6| Step: 13
Training loss: 1.9561103892858454
Validation loss: 2.6386149353464763

Epoch: 100| Step: 0
Training loss: 1.653719858830533
Validation loss: 2.577607305163445

Epoch: 6| Step: 1
Training loss: 1.8683581652708163
Validation loss: 2.622077238143593

Epoch: 6| Step: 2
Training loss: 1.9044416010709408
Validation loss: 2.588385077690247

Epoch: 6| Step: 3
Training loss: 1.9048531213842292
Validation loss: 2.6730333758172247

Epoch: 6| Step: 4
Training loss: 2.534641489865424
Validation loss: 2.639282186100114

Epoch: 6| Step: 5
Training loss: 2.36864866981072
Validation loss: 2.7012239066385773

Epoch: 6| Step: 6
Training loss: 1.9596394045631833
Validation loss: 2.6377876582876105

Epoch: 6| Step: 7
Training loss: 2.3030153577112777
Validation loss: 2.6244023263297005

Epoch: 6| Step: 8
Training loss: 2.088533193772428
Validation loss: 2.6289776380953542

Epoch: 6| Step: 9
Training loss: 1.9613422112945789
Validation loss: 2.6359178697254215

Epoch: 6| Step: 10
Training loss: 2.231729074545208
Validation loss: 2.628016146878785

Epoch: 6| Step: 11
Training loss: 2.250295195818263
Validation loss: 2.7255155271107574

Epoch: 6| Step: 12
Training loss: 1.844018625636999
Validation loss: 2.5377627288405935

Epoch: 6| Step: 13
Training loss: 3.0653550210920875
Validation loss: 2.624901421527603

Testing loss: 2.2671309114584455

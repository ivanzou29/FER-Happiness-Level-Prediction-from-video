Epoch: 1| Step: 0
Training loss: 5.543345185200463
Validation loss: 4.318057371570804

Epoch: 6| Step: 1
Training loss: 4.03757589640897
Validation loss: 4.317126068066162

Epoch: 6| Step: 2
Training loss: 4.708926048218202
Validation loss: 4.315114859024514

Epoch: 6| Step: 3
Training loss: 4.672192616973069
Validation loss: 4.308897416259179

Epoch: 6| Step: 4
Training loss: 4.645129961673822
Validation loss: 4.303399004769014

Epoch: 6| Step: 5
Training loss: 4.787564365362646
Validation loss: 4.297395868585862

Epoch: 6| Step: 6
Training loss: 4.128988476298081
Validation loss: 4.295609021925101

Epoch: 6| Step: 7
Training loss: 3.904696468420823
Validation loss: 4.285589926390579

Epoch: 6| Step: 8
Training loss: 3.8172882528979772
Validation loss: 4.2873331865573645

Epoch: 6| Step: 9
Training loss: 3.936333862070345
Validation loss: 4.280244845054399

Epoch: 6| Step: 10
Training loss: 4.297711655052105
Validation loss: 4.277974839384437

Epoch: 6| Step: 11
Training loss: 4.015005576009904
Validation loss: 4.276325749330919

Epoch: 6| Step: 12
Training loss: 4.676155298452366
Validation loss: 4.270219011441575

Epoch: 6| Step: 13
Training loss: 4.389892517025349
Validation loss: 4.266258786750565

Epoch: 2| Step: 0
Training loss: 4.690719922736241
Validation loss: 4.259498749835736

Epoch: 6| Step: 1
Training loss: 3.5045087927785414
Validation loss: 4.262399882152817

Epoch: 6| Step: 2
Training loss: 4.563495475130669
Validation loss: 4.253017529355992

Epoch: 6| Step: 3
Training loss: 4.443011018281334
Validation loss: 4.253021326878264

Epoch: 6| Step: 4
Training loss: 3.034992385427359
Validation loss: 4.243781047524249

Epoch: 6| Step: 5
Training loss: 3.733396138503085
Validation loss: 4.241819450466166

Epoch: 6| Step: 6
Training loss: 3.380385728755063
Validation loss: 4.23660469902577

Epoch: 6| Step: 7
Training loss: 5.336249368917321
Validation loss: 4.231483489649964

Epoch: 6| Step: 8
Training loss: 3.4616793489721283
Validation loss: 4.227314361794499

Epoch: 6| Step: 9
Training loss: 5.229990292717001
Validation loss: 4.225162388762742

Epoch: 6| Step: 10
Training loss: 4.657125301144113
Validation loss: 4.215712737008559

Epoch: 6| Step: 11
Training loss: 4.186511478180084
Validation loss: 4.214943994218061

Epoch: 6| Step: 12
Training loss: 5.206381388788891
Validation loss: 4.213261146850589

Epoch: 6| Step: 13
Training loss: 5.087093755075171
Validation loss: 4.202856641196305

Epoch: 3| Step: 0
Training loss: 3.7731914558516917
Validation loss: 4.199728018806898

Epoch: 6| Step: 1
Training loss: 4.013741255483068
Validation loss: 4.19670268477376

Epoch: 6| Step: 2
Training loss: 3.1179400231231145
Validation loss: 4.194001941709467

Epoch: 6| Step: 3
Training loss: 4.160804031872973
Validation loss: 4.187850037565779

Epoch: 6| Step: 4
Training loss: 5.580920632822445
Validation loss: 4.185317049502091

Epoch: 6| Step: 5
Training loss: 3.0999653352829695
Validation loss: 4.1787131129716855

Epoch: 6| Step: 6
Training loss: 4.731190431892594
Validation loss: 4.1777176973565675

Epoch: 6| Step: 7
Training loss: 4.538692281992164
Validation loss: 4.170838613235213

Epoch: 6| Step: 8
Training loss: 4.510464897808246
Validation loss: 4.16796563522592

Epoch: 6| Step: 9
Training loss: 5.418546066934578
Validation loss: 4.163530943429356

Epoch: 6| Step: 10
Training loss: 4.104001088683224
Validation loss: 4.154611590732239

Epoch: 6| Step: 11
Training loss: 4.405057705210054
Validation loss: 4.149939914154233

Epoch: 6| Step: 12
Training loss: 3.3994863009866214
Validation loss: 4.144298768893469

Epoch: 6| Step: 13
Training loss: 4.785546659775218
Validation loss: 4.141813210876057

Epoch: 4| Step: 0
Training loss: 4.985496466582896
Validation loss: 4.138351904329372

Epoch: 6| Step: 1
Training loss: 4.090381896603127
Validation loss: 4.127757068691522

Epoch: 6| Step: 2
Training loss: 3.4104990923379366
Validation loss: 4.129308472211605

Epoch: 6| Step: 3
Training loss: 4.2299373782597565
Validation loss: 4.123230555568196

Epoch: 6| Step: 4
Training loss: 4.358940075470523
Validation loss: 4.115703312680996

Epoch: 6| Step: 5
Training loss: 4.144540224056613
Validation loss: 4.111185650111424

Epoch: 6| Step: 6
Training loss: 3.4307157673836386
Validation loss: 4.107241998993191

Epoch: 6| Step: 7
Training loss: 4.2254993843328235
Validation loss: 4.1029309693045946

Epoch: 6| Step: 8
Training loss: 4.602287818207494
Validation loss: 4.096930953742444

Epoch: 6| Step: 9
Training loss: 3.8740354998866913
Validation loss: 4.088987179805401

Epoch: 6| Step: 10
Training loss: 4.696772242149805
Validation loss: 4.0850133438767

Epoch: 6| Step: 11
Training loss: 5.143908347824668
Validation loss: 4.0800447705269764

Epoch: 6| Step: 12
Training loss: 3.966266966698415
Validation loss: 4.076750110574515

Epoch: 6| Step: 13
Training loss: 3.395445179354751
Validation loss: 4.069731379340917

Epoch: 5| Step: 0
Training loss: 4.044364002904324
Validation loss: 4.068698812833512

Epoch: 6| Step: 1
Training loss: 4.162311006671626
Validation loss: 4.060428656117228

Epoch: 6| Step: 2
Training loss: 3.8034006562711773
Validation loss: 4.052187771833537

Epoch: 6| Step: 3
Training loss: 3.727346149956325
Validation loss: 4.049321440377689

Epoch: 6| Step: 4
Training loss: 3.2083771607679332
Validation loss: 4.049046860297867

Epoch: 6| Step: 5
Training loss: 5.065759433701596
Validation loss: 4.041162357681636

Epoch: 6| Step: 6
Training loss: 4.226535874598437
Validation loss: 4.036050321802305

Epoch: 6| Step: 7
Training loss: 4.611907780369615
Validation loss: 4.030442718161705

Epoch: 6| Step: 8
Training loss: 4.250378086999589
Validation loss: 4.0261214686060445

Epoch: 6| Step: 9
Training loss: 4.6360699939726615
Validation loss: 4.0183818922281205

Epoch: 6| Step: 10
Training loss: 4.520633441121318
Validation loss: 4.0091640916159745

Epoch: 6| Step: 11
Training loss: 3.797674903282337
Validation loss: 4.0076071329109535

Epoch: 6| Step: 12
Training loss: 4.478019650292227
Validation loss: 3.9962075390202974

Epoch: 6| Step: 13
Training loss: 2.6713219678196065
Validation loss: 3.9919924082612237

Epoch: 6| Step: 0
Training loss: 4.785614016688311
Validation loss: 3.9896029483622804

Epoch: 6| Step: 1
Training loss: 4.135026940623558
Validation loss: 3.980904502750155

Epoch: 6| Step: 2
Training loss: 4.827585332146927
Validation loss: 3.9712080316337075

Epoch: 6| Step: 3
Training loss: 4.478444075148032
Validation loss: 3.967450221765115

Epoch: 6| Step: 4
Training loss: 3.967672847643532
Validation loss: 3.9619597519366345

Epoch: 6| Step: 5
Training loss: 4.162535310429365
Validation loss: 3.957355665226515

Epoch: 6| Step: 6
Training loss: 4.033628723004737
Validation loss: 3.9466878002442622

Epoch: 6| Step: 7
Training loss: 4.448538362292835
Validation loss: 3.940568290029737

Epoch: 6| Step: 8
Training loss: 3.5323688456275426
Validation loss: 3.937113128110719

Epoch: 6| Step: 9
Training loss: 2.052323522566088
Validation loss: 3.9315636167092967

Epoch: 6| Step: 10
Training loss: 3.991252393999585
Validation loss: 3.9245542251550543

Epoch: 6| Step: 11
Training loss: 3.916536504868776
Validation loss: 3.9170231068960826

Epoch: 6| Step: 12
Training loss: 4.033409545640786
Validation loss: 3.9134282163987515

Epoch: 6| Step: 13
Training loss: 4.4328860083513435
Validation loss: 3.90479413341482

Epoch: 7| Step: 0
Training loss: 4.012881755791207
Validation loss: 3.897483022789751

Epoch: 6| Step: 1
Training loss: 3.508205740111833
Validation loss: 3.894849790710481

Epoch: 6| Step: 2
Training loss: 4.245323076119164
Validation loss: 3.8897210912955984

Epoch: 6| Step: 3
Training loss: 4.432457221137268
Validation loss: 3.882632589951762

Epoch: 6| Step: 4
Training loss: 4.356492706499372
Validation loss: 3.8713203846367295

Epoch: 6| Step: 5
Training loss: 2.7453524591515537
Validation loss: 3.86619769294439

Epoch: 6| Step: 6
Training loss: 3.523064502672387
Validation loss: 3.859157875492772

Epoch: 6| Step: 7
Training loss: 3.370776889801565
Validation loss: 3.851369433489734

Epoch: 6| Step: 8
Training loss: 4.212907376386492
Validation loss: 3.845787725408264

Epoch: 6| Step: 9
Training loss: 4.075878942516907
Validation loss: 3.839729407943168

Epoch: 6| Step: 10
Training loss: 4.307883182425881
Validation loss: 3.8273463870631486

Epoch: 6| Step: 11
Training loss: 4.663583395422405
Validation loss: 3.821910506573973

Epoch: 6| Step: 12
Training loss: 4.405888752298659
Validation loss: 3.819757094328953

Epoch: 6| Step: 13
Training loss: 3.5838579784421642
Validation loss: 3.8052550842126625

Epoch: 8| Step: 0
Training loss: 4.545639866605691
Validation loss: 3.80158053433815

Epoch: 6| Step: 1
Training loss: 3.0226450431105247
Validation loss: 3.7899498896140993

Epoch: 6| Step: 2
Training loss: 4.46875128045764
Validation loss: 3.782570128722305

Epoch: 6| Step: 3
Training loss: 4.628309560697992
Validation loss: 3.779455361277607

Epoch: 6| Step: 4
Training loss: 3.1631910089352475
Validation loss: 3.7724678418962156

Epoch: 6| Step: 5
Training loss: 4.139806817749018
Validation loss: 3.7586863132882895

Epoch: 6| Step: 6
Training loss: 4.222724906934909
Validation loss: 3.755696677951419

Epoch: 6| Step: 7
Training loss: 4.295466299623496
Validation loss: 3.7435496676965756

Epoch: 6| Step: 8
Training loss: 3.3477237654546768
Validation loss: 3.7368093821266655

Epoch: 6| Step: 9
Training loss: 4.097736543317833
Validation loss: 3.7229571735452605

Epoch: 6| Step: 10
Training loss: 3.5056984697145706
Validation loss: 3.7224262941567323

Epoch: 6| Step: 11
Training loss: 3.4944261036607016
Validation loss: 3.710192544856977

Epoch: 6| Step: 12
Training loss: 3.858940563383753
Validation loss: 3.703242071504473

Epoch: 6| Step: 13
Training loss: 2.9712812374079403
Validation loss: 3.698087367974073

Epoch: 9| Step: 0
Training loss: 4.058356416755424
Validation loss: 3.6905429785370707

Epoch: 6| Step: 1
Training loss: 3.616207772834829
Validation loss: 3.674031777126393

Epoch: 6| Step: 2
Training loss: 3.6689433630135224
Validation loss: 3.6656420712734006

Epoch: 6| Step: 3
Training loss: 3.929594186694974
Validation loss: 3.6562043313272654

Epoch: 6| Step: 4
Training loss: 3.752993723838919
Validation loss: 3.641814874912888

Epoch: 6| Step: 5
Training loss: 4.0816733171422594
Validation loss: 3.641985112505551

Epoch: 6| Step: 6
Training loss: 4.479467171931186
Validation loss: 3.630092276077163

Epoch: 6| Step: 7
Training loss: 3.464213110678425
Validation loss: 3.6184017666664205

Epoch: 6| Step: 8
Training loss: 4.277785186981347
Validation loss: 3.612927809892022

Epoch: 6| Step: 9
Training loss: 3.563004575098501
Validation loss: 3.5999908787259733

Epoch: 6| Step: 10
Training loss: 3.3772527452571093
Validation loss: 3.59727062892194

Epoch: 6| Step: 11
Training loss: 4.209635743899165
Validation loss: 3.582865483924867

Epoch: 6| Step: 12
Training loss: 2.8389479786727954
Validation loss: 3.5739748951325696

Epoch: 6| Step: 13
Training loss: 3.296906529858858
Validation loss: 3.556539497650377

Epoch: 10| Step: 0
Training loss: 2.809169153261249
Validation loss: 3.551450874727624

Epoch: 6| Step: 1
Training loss: 4.252016374558278
Validation loss: 3.5396902477504937

Epoch: 6| Step: 2
Training loss: 3.1394447652153223
Validation loss: 3.531148846820325

Epoch: 6| Step: 3
Training loss: 4.263813682221494
Validation loss: 3.5147989390475134

Epoch: 6| Step: 4
Training loss: 3.9635195169652877
Validation loss: 3.5038939099326547

Epoch: 6| Step: 5
Training loss: 3.639908748782432
Validation loss: 3.496100535832736

Epoch: 6| Step: 6
Training loss: 3.8621682626427405
Validation loss: 3.486086190828107

Epoch: 6| Step: 7
Training loss: 3.921176802657615
Validation loss: 3.4760457164402747

Epoch: 6| Step: 8
Training loss: 3.3771839846171527
Validation loss: 3.468777339479831

Epoch: 6| Step: 9
Training loss: 3.188872696247998
Validation loss: 3.453513716801339

Epoch: 6| Step: 10
Training loss: 3.5303131481169046
Validation loss: 3.4427260616112183

Epoch: 6| Step: 11
Training loss: 4.286943200438301
Validation loss: 3.427786988647266

Epoch: 6| Step: 12
Training loss: 3.255150601701602
Validation loss: 3.4148842934402515

Epoch: 6| Step: 13
Training loss: 3.1760671188987804
Validation loss: 3.408481148288188

Epoch: 11| Step: 0
Training loss: 4.358476662451994
Validation loss: 3.3887587117216573

Epoch: 6| Step: 1
Training loss: 3.45590926244617
Validation loss: 3.3765726424750477

Epoch: 6| Step: 2
Training loss: 3.3738203459589604
Validation loss: 3.369163167889072

Epoch: 6| Step: 3
Training loss: 2.7065717298867615
Validation loss: 3.358892211410603

Epoch: 6| Step: 4
Training loss: 2.9903385037082764
Validation loss: 3.3407411252600343

Epoch: 6| Step: 5
Training loss: 3.115525646404754
Validation loss: 3.3344168968154526

Epoch: 6| Step: 6
Training loss: 4.02091257811152
Validation loss: 3.322444642436793

Epoch: 6| Step: 7
Training loss: 4.0481285009682635
Validation loss: 3.315283944556929

Epoch: 6| Step: 8
Training loss: 3.437279156613306
Validation loss: 3.28985849211505

Epoch: 6| Step: 9
Training loss: 3.260197706253764
Validation loss: 3.287726195734086

Epoch: 6| Step: 10
Training loss: 3.164208980984315
Validation loss: 3.2767128667241927

Epoch: 6| Step: 11
Training loss: 3.7940346725721534
Validation loss: 3.263969748065667

Epoch: 6| Step: 12
Training loss: 3.5478920171706085
Validation loss: 3.2493592546456904

Epoch: 6| Step: 13
Training loss: 3.6242607941504588
Validation loss: 3.2448311082068457

Epoch: 12| Step: 0
Training loss: 3.7063289897664644
Validation loss: 3.224679643192531

Epoch: 6| Step: 1
Training loss: 3.0845526956158893
Validation loss: 3.2115670251151633

Epoch: 6| Step: 2
Training loss: 3.8792650684915375
Validation loss: 3.201409789027924

Epoch: 6| Step: 3
Training loss: 2.411776549817835
Validation loss: 3.1871477367138143

Epoch: 6| Step: 4
Training loss: 3.262201147897797
Validation loss: 3.17704122600494

Epoch: 6| Step: 5
Training loss: 3.2745784692080564
Validation loss: 3.1679825830838477

Epoch: 6| Step: 6
Training loss: 3.873708448208723
Validation loss: 3.1478665653881994

Epoch: 6| Step: 7
Training loss: 3.1523056311589346
Validation loss: 3.1218073453171367

Epoch: 6| Step: 8
Training loss: 2.8578363666820517
Validation loss: 3.1182832303536654

Epoch: 6| Step: 9
Training loss: 3.3443052597345364
Validation loss: 3.123425201346003

Epoch: 6| Step: 10
Training loss: 3.533406097334644
Validation loss: 3.095293415990114

Epoch: 6| Step: 11
Training loss: 3.0386661378938964
Validation loss: 3.081236926182955

Epoch: 6| Step: 12
Training loss: 3.8884586293002763
Validation loss: 3.063802010814879

Epoch: 6| Step: 13
Training loss: 3.247586160857381
Validation loss: 3.0610225429386957

Epoch: 13| Step: 0
Training loss: 3.2771890851560155
Validation loss: 3.050237422008793

Epoch: 6| Step: 1
Training loss: 3.4999465938308214
Validation loss: 3.043157597850528

Epoch: 6| Step: 2
Training loss: 3.4333390454982413
Validation loss: 3.0163425546333182

Epoch: 6| Step: 3
Training loss: 3.9064830252760627
Validation loss: 2.996859470475913

Epoch: 6| Step: 4
Training loss: 3.8270772940488182
Validation loss: 2.9826235161577666

Epoch: 6| Step: 5
Training loss: 3.310394625693412
Validation loss: 2.9642983099328295

Epoch: 6| Step: 6
Training loss: 3.250731019189169
Validation loss: 2.94913306048964

Epoch: 6| Step: 7
Training loss: 3.2093050257289266
Validation loss: 2.9368527959877637

Epoch: 6| Step: 8
Training loss: 2.550189523572736
Validation loss: 2.9285376756629646

Epoch: 6| Step: 9
Training loss: 2.9719258236876414
Validation loss: 2.9104720423396255

Epoch: 6| Step: 10
Training loss: 3.0318936323365198
Validation loss: 2.8996381171052428

Epoch: 6| Step: 11
Training loss: 2.9071686636092986
Validation loss: 2.884430718901718

Epoch: 6| Step: 12
Training loss: 2.8138061881164145
Validation loss: 2.8870888376668247

Epoch: 6| Step: 13
Training loss: 1.965692537781896
Validation loss: 2.8559072396954934

Epoch: 14| Step: 0
Training loss: 3.2874322688446695
Validation loss: 2.853689453982733

Epoch: 6| Step: 1
Training loss: 3.3008595474256484
Validation loss: 2.837261964940792

Epoch: 6| Step: 2
Training loss: 3.4540604234898167
Validation loss: 2.8345089005213837

Epoch: 6| Step: 3
Training loss: 2.824656430172853
Validation loss: 2.8168680560172397

Epoch: 6| Step: 4
Training loss: 3.3653871188311646
Validation loss: 2.8019303124789245

Epoch: 6| Step: 5
Training loss: 3.1995317354913073
Validation loss: 2.793053240580558

Epoch: 6| Step: 6
Training loss: 3.5112187556842076
Validation loss: 2.791749223551502

Epoch: 6| Step: 7
Training loss: 2.669082163524025
Validation loss: 2.772499688740629

Epoch: 6| Step: 8
Training loss: 2.1558983557526346
Validation loss: 2.7598607517948217

Epoch: 6| Step: 9
Training loss: 4.021399950546558
Validation loss: 2.7440356209417063

Epoch: 6| Step: 10
Training loss: 2.7295245938091908
Validation loss: 2.73990006387869

Epoch: 6| Step: 11
Training loss: 2.6686637274848
Validation loss: 2.729704226153391

Epoch: 6| Step: 12
Training loss: 2.329310377824878
Validation loss: 2.717427523424657

Epoch: 6| Step: 13
Training loss: 2.516137775736463
Validation loss: 2.7025873713065867

Epoch: 15| Step: 0
Training loss: 3.165842116235685
Validation loss: 2.697751615582063

Epoch: 6| Step: 1
Training loss: 3.1240045106295993
Validation loss: 2.677861078005439

Epoch: 6| Step: 2
Training loss: 2.884019056450347
Validation loss: 2.671067904715826

Epoch: 6| Step: 3
Training loss: 2.4147906640350505
Validation loss: 2.671904534895864

Epoch: 6| Step: 4
Training loss: 3.184866172338418
Validation loss: 2.6699872230241586

Epoch: 6| Step: 5
Training loss: 3.5674043326325307
Validation loss: 2.657608275611692

Epoch: 6| Step: 6
Training loss: 2.328570432102749
Validation loss: 2.6369404545910613

Epoch: 6| Step: 7
Training loss: 2.5401761943050354
Validation loss: 2.6315859979482177

Epoch: 6| Step: 8
Training loss: 2.204106321993089
Validation loss: 2.6301918311115644

Epoch: 6| Step: 9
Training loss: 2.5340503225952458
Validation loss: 2.6286670411606528

Epoch: 6| Step: 10
Training loss: 3.100453522024855
Validation loss: 2.609418061438816

Epoch: 6| Step: 11
Training loss: 3.4908575815497094
Validation loss: 2.608946515196064

Epoch: 6| Step: 12
Training loss: 3.0010278848218848
Validation loss: 2.6019202373210466

Epoch: 6| Step: 13
Training loss: 3.7569615913480865
Validation loss: 2.594065428954999

Epoch: 16| Step: 0
Training loss: 3.142211513342168
Validation loss: 2.5975341995190595

Epoch: 6| Step: 1
Training loss: 2.76674444096952
Validation loss: 2.5840696270657566

Epoch: 6| Step: 2
Training loss: 3.374697212482913
Validation loss: 2.587156095830409

Epoch: 6| Step: 3
Training loss: 2.824904320370955
Validation loss: 2.5683682746035865

Epoch: 6| Step: 4
Training loss: 2.7251441567463597
Validation loss: 2.570139872450407

Epoch: 6| Step: 5
Training loss: 3.1102237022385237
Validation loss: 2.5554014670051552

Epoch: 6| Step: 6
Training loss: 2.422978402092271
Validation loss: 2.55921382170912

Epoch: 6| Step: 7
Training loss: 2.8005777920892427
Validation loss: 2.5609293810664364

Epoch: 6| Step: 8
Training loss: 2.679509371072534
Validation loss: 2.569246091524562

Epoch: 6| Step: 9
Training loss: 2.559318614520256
Validation loss: 2.5427358101091855

Epoch: 6| Step: 10
Training loss: 3.2102185114942765
Validation loss: 2.539046796398334

Epoch: 6| Step: 11
Training loss: 2.6811577236418205
Validation loss: 2.5420402418404624

Epoch: 6| Step: 12
Training loss: 3.2555611787934793
Validation loss: 2.5601939828471107

Epoch: 6| Step: 13
Training loss: 2.0653589259081264
Validation loss: 2.524525602764495

Epoch: 17| Step: 0
Training loss: 2.7536615790579604
Validation loss: 2.5173442022943617

Epoch: 6| Step: 1
Training loss: 3.3476146575231254
Validation loss: 2.5157734704743384

Epoch: 6| Step: 2
Training loss: 3.0954769978532033
Validation loss: 2.5148328245799316

Epoch: 6| Step: 3
Training loss: 2.8726390598458162
Validation loss: 2.5216673645315044

Epoch: 6| Step: 4
Training loss: 3.3313737513474817
Validation loss: 2.5170261763822617

Epoch: 6| Step: 5
Training loss: 3.217037004357013
Validation loss: 2.5034899644963176

Epoch: 6| Step: 6
Training loss: 2.854910946073409
Validation loss: 2.504098340631951

Epoch: 6| Step: 7
Training loss: 2.4867128133028804
Validation loss: 2.487019150334203

Epoch: 6| Step: 8
Training loss: 2.3684535905487984
Validation loss: 2.4879183131140903

Epoch: 6| Step: 9
Training loss: 3.5475968622018064
Validation loss: 2.4974699302701873

Epoch: 6| Step: 10
Training loss: 2.613570170261666
Validation loss: 2.493333566705231

Epoch: 6| Step: 11
Training loss: 2.060524890655827
Validation loss: 2.49407544515306

Epoch: 6| Step: 12
Training loss: 2.467227901154523
Validation loss: 2.5029803098143595

Epoch: 6| Step: 13
Training loss: 2.3063478857423827
Validation loss: 2.478162414403242

Epoch: 18| Step: 0
Training loss: 3.33693078148248
Validation loss: 2.4846526248984286

Epoch: 6| Step: 1
Training loss: 2.2223710076538525
Validation loss: 2.4921183445852333

Epoch: 6| Step: 2
Training loss: 2.8641148039541804
Validation loss: 2.493637617166797

Epoch: 6| Step: 3
Training loss: 3.0369960162727323
Validation loss: 2.4841816907506806

Epoch: 6| Step: 4
Training loss: 3.3207489085903212
Validation loss: 2.478144757620945

Epoch: 6| Step: 5
Training loss: 2.7692146927415813
Validation loss: 2.4666673321322556

Epoch: 6| Step: 6
Training loss: 2.6454156160395086
Validation loss: 2.4716298938571124

Epoch: 6| Step: 7
Training loss: 2.826162004750171
Validation loss: 2.4797166100806574

Epoch: 6| Step: 8
Training loss: 2.73140262901826
Validation loss: 2.477386569382

Epoch: 6| Step: 9
Training loss: 2.3281480416815414
Validation loss: 2.467672238158892

Epoch: 6| Step: 10
Training loss: 2.2939176659280167
Validation loss: 2.4599208030538

Epoch: 6| Step: 11
Training loss: 2.9638261619380577
Validation loss: 2.4776401909722723

Epoch: 6| Step: 12
Training loss: 3.5297291949521092
Validation loss: 2.479577384576821

Epoch: 6| Step: 13
Training loss: 2.4243558913105008
Validation loss: 2.4792526125433536

Epoch: 19| Step: 0
Training loss: 2.6388160784754415
Validation loss: 2.4739785933299157

Epoch: 6| Step: 1
Training loss: 2.5760609080121397
Validation loss: 2.4694384664648057

Epoch: 6| Step: 2
Training loss: 2.1473043886918575
Validation loss: 2.475365494346612

Epoch: 6| Step: 3
Training loss: 2.809961572160342
Validation loss: 2.470100689632898

Epoch: 6| Step: 4
Training loss: 2.4557914092238526
Validation loss: 2.475697420231371

Epoch: 6| Step: 5
Training loss: 3.0605901874527772
Validation loss: 2.477242679135411

Epoch: 6| Step: 6
Training loss: 2.7285341495657844
Validation loss: 2.4463501008246467

Epoch: 6| Step: 7
Training loss: 3.459743733039085
Validation loss: 2.462021165671413

Epoch: 6| Step: 8
Training loss: 2.471975030816152
Validation loss: 2.4713441836083527

Epoch: 6| Step: 9
Training loss: 3.4448580972045115
Validation loss: 2.4781696672201234

Epoch: 6| Step: 10
Training loss: 3.0891665019085117
Validation loss: 2.462163937490032

Epoch: 6| Step: 11
Training loss: 3.054280207571785
Validation loss: 2.4492981663250877

Epoch: 6| Step: 12
Training loss: 2.636977978431775
Validation loss: 2.456769505973008

Epoch: 6| Step: 13
Training loss: 2.5920270229744116
Validation loss: 2.4607851433457752

Epoch: 20| Step: 0
Training loss: 2.9789944223260156
Validation loss: 2.458689502369024

Epoch: 6| Step: 1
Training loss: 3.066829501541995
Validation loss: 2.479265560729451

Epoch: 6| Step: 2
Training loss: 2.8936659205438344
Validation loss: 2.467694569003837

Epoch: 6| Step: 3
Training loss: 3.0514767058032035
Validation loss: 2.4756611777105246

Epoch: 6| Step: 4
Training loss: 2.4162488992829636
Validation loss: 2.4819769114846926

Epoch: 6| Step: 5
Training loss: 2.4586280279736794
Validation loss: 2.4453212854905795

Epoch: 6| Step: 6
Training loss: 3.067802665222948
Validation loss: 2.44700793956538

Epoch: 6| Step: 7
Training loss: 2.4334811801500105
Validation loss: 2.4745763001483776

Epoch: 6| Step: 8
Training loss: 2.7067234145011274
Validation loss: 2.469733915865956

Epoch: 6| Step: 9
Training loss: 3.027297126657112
Validation loss: 2.4616318313896315

Epoch: 6| Step: 10
Training loss: 3.0864129195196535
Validation loss: 2.458144105734001

Epoch: 6| Step: 11
Training loss: 2.981013936867266
Validation loss: 2.471421741689108

Epoch: 6| Step: 12
Training loss: 2.817094398690831
Validation loss: 2.4731521900945466

Epoch: 6| Step: 13
Training loss: 2.415194051777053
Validation loss: 2.4569508784782044

Epoch: 21| Step: 0
Training loss: 2.974785381878381
Validation loss: 2.4596320535587104

Epoch: 6| Step: 1
Training loss: 2.4979692794523407
Validation loss: 2.4827756897997046

Epoch: 6| Step: 2
Training loss: 2.020437955685816
Validation loss: 2.460893166034145

Epoch: 6| Step: 3
Training loss: 2.30615818492207
Validation loss: 2.4684391513992883

Epoch: 6| Step: 4
Training loss: 3.4844491719789072
Validation loss: 2.4784245740607034

Epoch: 6| Step: 5
Training loss: 2.837861724517329
Validation loss: 2.4622843562009273

Epoch: 6| Step: 6
Training loss: 2.4311185590423428
Validation loss: 2.4571234176488894

Epoch: 6| Step: 7
Training loss: 2.718699136894011
Validation loss: 2.452379217695214

Epoch: 6| Step: 8
Training loss: 2.720974067292434
Validation loss: 2.4554055349234143

Epoch: 6| Step: 9
Training loss: 3.2993921008988356
Validation loss: 2.4790978412238296

Epoch: 6| Step: 10
Training loss: 3.0300202382233303
Validation loss: 2.4585125277160658

Epoch: 6| Step: 11
Training loss: 3.259359527580426
Validation loss: 2.4630288317420836

Epoch: 6| Step: 12
Training loss: 2.6377811806298044
Validation loss: 2.4604951642313893

Epoch: 6| Step: 13
Training loss: 2.9593711071532605
Validation loss: 2.460045377044526

Epoch: 22| Step: 0
Training loss: 2.8339275036290528
Validation loss: 2.464089713245237

Epoch: 6| Step: 1
Training loss: 2.8018732003949265
Validation loss: 2.4482795835547084

Epoch: 6| Step: 2
Training loss: 1.9901339250096932
Validation loss: 2.469698821096633

Epoch: 6| Step: 3
Training loss: 3.40476103270396
Validation loss: 2.4654392926888296

Epoch: 6| Step: 4
Training loss: 3.0278146685142597
Validation loss: 2.4601911390607856

Epoch: 6| Step: 5
Training loss: 2.409447663197317
Validation loss: 2.468705471432163

Epoch: 6| Step: 6
Training loss: 2.883656944093007
Validation loss: 2.4629969555409215

Epoch: 6| Step: 7
Training loss: 2.937982722450388
Validation loss: 2.4611055737676515

Epoch: 6| Step: 8
Training loss: 2.681335743064436
Validation loss: 2.479831345523305

Epoch: 6| Step: 9
Training loss: 2.5349502832477007
Validation loss: 2.448968640780922

Epoch: 6| Step: 10
Training loss: 2.7332163944661008
Validation loss: 2.464399620798822

Epoch: 6| Step: 11
Training loss: 3.090478110993315
Validation loss: 2.4496550490230327

Epoch: 6| Step: 12
Training loss: 2.922149645295311
Validation loss: 2.4599516998926405

Epoch: 6| Step: 13
Training loss: 3.339385562275299
Validation loss: 2.4581602901904533

Epoch: 23| Step: 0
Training loss: 2.617527817193028
Validation loss: 2.4613953408820994

Epoch: 6| Step: 1
Training loss: 2.91388708000826
Validation loss: 2.467301749354919

Epoch: 6| Step: 2
Training loss: 3.2783734019290094
Validation loss: 2.474846154033854

Epoch: 6| Step: 3
Training loss: 2.6473644862375756
Validation loss: 2.455518907423609

Epoch: 6| Step: 4
Training loss: 2.6944825594676143
Validation loss: 2.4658158377541106

Epoch: 6| Step: 5
Training loss: 3.1744452525020512
Validation loss: 2.45887077182698

Epoch: 6| Step: 6
Training loss: 2.308780875585582
Validation loss: 2.4679307815431692

Epoch: 6| Step: 7
Training loss: 2.965155423032536
Validation loss: 2.47650001068949

Epoch: 6| Step: 8
Training loss: 2.9484867223240454
Validation loss: 2.473134571140376

Epoch: 6| Step: 9
Training loss: 2.949733825572001
Validation loss: 2.4594296426237

Epoch: 6| Step: 10
Training loss: 2.484157720697063
Validation loss: 2.4942600374778277

Epoch: 6| Step: 11
Training loss: 2.8434213301654476
Validation loss: 2.455020218164794

Epoch: 6| Step: 12
Training loss: 3.0272725546063817
Validation loss: 2.4694946690384856

Epoch: 6| Step: 13
Training loss: 2.148449374079403
Validation loss: 2.4679533759743526

Epoch: 24| Step: 0
Training loss: 2.9635011550646198
Validation loss: 2.456978633390333

Epoch: 6| Step: 1
Training loss: 2.4980199602122184
Validation loss: 2.4618068312671597

Epoch: 6| Step: 2
Training loss: 2.7042172380684404
Validation loss: 2.4560520591991697

Epoch: 6| Step: 3
Training loss: 2.7446895824078315
Validation loss: 2.465597309728713

Epoch: 6| Step: 4
Training loss: 2.6104076420860647
Validation loss: 2.4812279398943766

Epoch: 6| Step: 5
Training loss: 3.209795003744702
Validation loss: 2.4555738818860817

Epoch: 6| Step: 6
Training loss: 2.9892455296729845
Validation loss: 2.4724614511278897

Epoch: 6| Step: 7
Training loss: 3.3657978491641454
Validation loss: 2.4652554176845958

Epoch: 6| Step: 8
Training loss: 2.5036173399405657
Validation loss: 2.475713246072721

Epoch: 6| Step: 9
Training loss: 2.9498075390119802
Validation loss: 2.4693283799198023

Epoch: 6| Step: 10
Training loss: 2.218289717090943
Validation loss: 2.473181809391394

Epoch: 6| Step: 11
Training loss: 2.3904887640536807
Validation loss: 2.466253114298179

Epoch: 6| Step: 12
Training loss: 3.0826692295569798
Validation loss: 2.4672887591644788

Epoch: 6| Step: 13
Training loss: 3.169946394265539
Validation loss: 2.4692449254026925

Epoch: 25| Step: 0
Training loss: 2.7847043991046996
Validation loss: 2.4644667776250646

Epoch: 6| Step: 1
Training loss: 3.0674132813474
Validation loss: 2.4821409847201577

Epoch: 6| Step: 2
Training loss: 2.6485186989188785
Validation loss: 2.4294732671383312

Epoch: 6| Step: 3
Training loss: 3.5489791476877763
Validation loss: 2.448750389061527

Epoch: 6| Step: 4
Training loss: 2.683027138230473
Validation loss: 2.4646920035453337

Epoch: 6| Step: 5
Training loss: 3.1060620011173565
Validation loss: 2.469807569766466

Epoch: 6| Step: 6
Training loss: 2.8458626413767734
Validation loss: 2.492277216999374

Epoch: 6| Step: 7
Training loss: 2.7435984228578723
Validation loss: 2.461775007973981

Epoch: 6| Step: 8
Training loss: 2.6156384794737892
Validation loss: 2.458503845152441

Epoch: 6| Step: 9
Training loss: 2.6614598064580277
Validation loss: 2.4720555257308052

Epoch: 6| Step: 10
Training loss: 2.792781796900735
Validation loss: 2.458763781140861

Epoch: 6| Step: 11
Training loss: 2.7532002294786104
Validation loss: 2.4595364667574904

Epoch: 6| Step: 12
Training loss: 2.6178124436060504
Validation loss: 2.4710495772318395

Epoch: 6| Step: 13
Training loss: 2.7109753169079718
Validation loss: 2.4666252198294414

Epoch: 26| Step: 0
Training loss: 2.4053926798434357
Validation loss: 2.4610796393673238

Epoch: 6| Step: 1
Training loss: 3.274385374212605
Validation loss: 2.466232668523014

Epoch: 6| Step: 2
Training loss: 3.0756444860413565
Validation loss: 2.4491987743578076

Epoch: 6| Step: 3
Training loss: 2.141280359341351
Validation loss: 2.4517586072233217

Epoch: 6| Step: 4
Training loss: 2.2257136282752845
Validation loss: 2.4510557456934623

Epoch: 6| Step: 5
Training loss: 2.6191500920317345
Validation loss: 2.4868164134662485

Epoch: 6| Step: 6
Training loss: 2.977926107131628
Validation loss: 2.4694125411977055

Epoch: 6| Step: 7
Training loss: 3.156521624972796
Validation loss: 2.46514044373403

Epoch: 6| Step: 8
Training loss: 2.0108124997192305
Validation loss: 2.4836816153542136

Epoch: 6| Step: 9
Training loss: 2.929481438065734
Validation loss: 2.449144105329363

Epoch: 6| Step: 10
Training loss: 3.130802413412743
Validation loss: 2.4759629792217956

Epoch: 6| Step: 11
Training loss: 3.1881728116705337
Validation loss: 2.459577380347691

Epoch: 6| Step: 12
Training loss: 3.448421260793775
Validation loss: 2.4516834271220294

Epoch: 6| Step: 13
Training loss: 2.2694389074977326
Validation loss: 2.4558164599550873

Epoch: 27| Step: 0
Training loss: 2.9021855307332376
Validation loss: 2.4597919684220066

Epoch: 6| Step: 1
Training loss: 2.3257290589227235
Validation loss: 2.4605271520282708

Epoch: 6| Step: 2
Training loss: 3.1378611079465695
Validation loss: 2.4401778768810716

Epoch: 6| Step: 3
Training loss: 1.7418177601498281
Validation loss: 2.4429784640677705

Epoch: 6| Step: 4
Training loss: 2.8438616720218173
Validation loss: 2.4630006505992563

Epoch: 6| Step: 5
Training loss: 2.913406911949432
Validation loss: 2.451163459805549

Epoch: 6| Step: 6
Training loss: 2.8339463487068053
Validation loss: 2.4587798994808936

Epoch: 6| Step: 7
Training loss: 2.786488261314649
Validation loss: 2.456568431860679

Epoch: 6| Step: 8
Training loss: 1.5717272103151423
Validation loss: 2.4649096383402336

Epoch: 6| Step: 9
Training loss: 3.293851360809922
Validation loss: 2.476732387883336

Epoch: 6| Step: 10
Training loss: 3.4161536754284145
Validation loss: 2.46717189840594

Epoch: 6| Step: 11
Training loss: 2.825068049328618
Validation loss: 2.4846356967721603

Epoch: 6| Step: 12
Training loss: 3.1587473743036827
Validation loss: 2.4550308161763534

Epoch: 6| Step: 13
Training loss: 3.3196432000053555
Validation loss: 2.4599230030607897

Epoch: 28| Step: 0
Training loss: 2.626395989653924
Validation loss: 2.472736990223652

Epoch: 6| Step: 1
Training loss: 2.8619319968578343
Validation loss: 2.455022978100676

Epoch: 6| Step: 2
Training loss: 2.4139246237679832
Validation loss: 2.45537368713439

Epoch: 6| Step: 3
Training loss: 2.7462188561973777
Validation loss: 2.462285507727506

Epoch: 6| Step: 4
Training loss: 2.9888058669398228
Validation loss: 2.4695947898752464

Epoch: 6| Step: 5
Training loss: 2.437056085316067
Validation loss: 2.474281043532618

Epoch: 6| Step: 6
Training loss: 3.3727484362853586
Validation loss: 2.4655951095859794

Epoch: 6| Step: 7
Training loss: 2.5524373542054963
Validation loss: 2.4459127196468167

Epoch: 6| Step: 8
Training loss: 3.0701609178548668
Validation loss: 2.4658564075162133

Epoch: 6| Step: 9
Training loss: 3.128742266107039
Validation loss: 2.471016790876769

Epoch: 6| Step: 10
Training loss: 2.983445587035363
Validation loss: 2.466424160283007

Epoch: 6| Step: 11
Training loss: 2.80681136533761
Validation loss: 2.451071518315714

Epoch: 6| Step: 12
Training loss: 1.7381447727761776
Validation loss: 2.4576365250002383

Epoch: 6| Step: 13
Training loss: 3.647675722633556
Validation loss: 2.453751549263208

Epoch: 29| Step: 0
Training loss: 2.99001111593062
Validation loss: 2.4522225453644837

Epoch: 6| Step: 1
Training loss: 3.1736643587556848
Validation loss: 2.463913116586911

Epoch: 6| Step: 2
Training loss: 3.1677468783086473
Validation loss: 2.4552553874040566

Epoch: 6| Step: 3
Training loss: 3.1256173096340794
Validation loss: 2.4551365577878066

Epoch: 6| Step: 4
Training loss: 3.0285952239374185
Validation loss: 2.442461499095364

Epoch: 6| Step: 5
Training loss: 2.738940803013477
Validation loss: 2.455667658182037

Epoch: 6| Step: 6
Training loss: 2.3768502105776355
Validation loss: 2.458584905203296

Epoch: 6| Step: 7
Training loss: 2.2392860274402664
Validation loss: 2.4562780577423244

Epoch: 6| Step: 8
Training loss: 3.002131023112023
Validation loss: 2.468147219819935

Epoch: 6| Step: 9
Training loss: 2.167435534089444
Validation loss: 2.447801003290498

Epoch: 6| Step: 10
Training loss: 2.710228027378542
Validation loss: 2.468019199994762

Epoch: 6| Step: 11
Training loss: 2.913705430641171
Validation loss: 2.442303433728875

Epoch: 6| Step: 12
Training loss: 3.021466068343055
Validation loss: 2.4641513863858813

Epoch: 6| Step: 13
Training loss: 2.313875356418116
Validation loss: 2.462706985465422

Epoch: 30| Step: 0
Training loss: 2.3550236472787156
Validation loss: 2.4686306381602394

Epoch: 6| Step: 1
Training loss: 2.4036449847042505
Validation loss: 2.4452464935369416

Epoch: 6| Step: 2
Training loss: 2.208436663627331
Validation loss: 2.4620940173860437

Epoch: 6| Step: 3
Training loss: 3.7451612407774095
Validation loss: 2.4579598310109634

Epoch: 6| Step: 4
Training loss: 3.0689725402816705
Validation loss: 2.461948371676094

Epoch: 6| Step: 5
Training loss: 2.8243647916186845
Validation loss: 2.478663639509836

Epoch: 6| Step: 6
Training loss: 3.68657944390404
Validation loss: 2.4638311684333

Epoch: 6| Step: 7
Training loss: 2.9777390765368565
Validation loss: 2.446871497510403

Epoch: 6| Step: 8
Training loss: 2.745533958018026
Validation loss: 2.4590993156807532

Epoch: 6| Step: 9
Training loss: 2.3328474651560085
Validation loss: 2.4774010836764853

Epoch: 6| Step: 10
Training loss: 2.647204987224217
Validation loss: 2.4682868573970382

Epoch: 6| Step: 11
Training loss: 3.1450467842151957
Validation loss: 2.4590943908489615

Epoch: 6| Step: 12
Training loss: 2.205974278498056
Validation loss: 2.4535177070779195

Epoch: 6| Step: 13
Training loss: 2.2381933704320645
Validation loss: 2.464500279415339

Epoch: 31| Step: 0
Training loss: 2.4822467829245163
Validation loss: 2.4552374541621864

Epoch: 6| Step: 1
Training loss: 3.071000427365312
Validation loss: 2.452196286496361

Epoch: 6| Step: 2
Training loss: 2.71026427074877
Validation loss: 2.4685771819863453

Epoch: 6| Step: 3
Training loss: 2.9822898091052727
Validation loss: 2.4483751124727053

Epoch: 6| Step: 4
Training loss: 2.787398325344496
Validation loss: 2.4519239842490776

Epoch: 6| Step: 5
Training loss: 2.428222999902575
Validation loss: 2.4517921812355903

Epoch: 6| Step: 6
Training loss: 2.2758040547824323
Validation loss: 2.446053976054589

Epoch: 6| Step: 7
Training loss: 3.134830120053046
Validation loss: 2.455900448034398

Epoch: 6| Step: 8
Training loss: 2.740911029082875
Validation loss: 2.457589228915691

Epoch: 6| Step: 9
Training loss: 2.9002096100395103
Validation loss: 2.4573796474890144

Epoch: 6| Step: 10
Training loss: 3.0126413399237517
Validation loss: 2.468552898920606

Epoch: 6| Step: 11
Training loss: 3.054588374800332
Validation loss: 2.438051757122866

Epoch: 6| Step: 12
Training loss: 3.1070647738813295
Validation loss: 2.456098310828962

Epoch: 6| Step: 13
Training loss: 2.4358706775281815
Validation loss: 2.438864726353818

Epoch: 32| Step: 0
Training loss: 2.7226791928171616
Validation loss: 2.447158097599716

Epoch: 6| Step: 1
Training loss: 3.0328847665249405
Validation loss: 2.444615592617499

Epoch: 6| Step: 2
Training loss: 2.6157278061218436
Validation loss: 2.4494656174438174

Epoch: 6| Step: 3
Training loss: 2.592063079478508
Validation loss: 2.464851290585961

Epoch: 6| Step: 4
Training loss: 2.9378108407989645
Validation loss: 2.4682209231461805

Epoch: 6| Step: 5
Training loss: 3.309581262430248
Validation loss: 2.4540517443909793

Epoch: 6| Step: 6
Training loss: 3.1081997744695715
Validation loss: 2.460923694911089

Epoch: 6| Step: 7
Training loss: 2.403566127019002
Validation loss: 2.449288802674194

Epoch: 6| Step: 8
Training loss: 2.3696391185805337
Validation loss: 2.4669381332623255

Epoch: 6| Step: 9
Training loss: 3.131923482805485
Validation loss: 2.4790750515738726

Epoch: 6| Step: 10
Training loss: 2.086149740672762
Validation loss: 2.473031950184426

Epoch: 6| Step: 11
Training loss: 3.1101839939653666
Validation loss: 2.440440883569019

Epoch: 6| Step: 12
Training loss: 3.0527481205704614
Validation loss: 2.454896209467125

Epoch: 6| Step: 13
Training loss: 2.2837704882775416
Validation loss: 2.472721204429584

Epoch: 33| Step: 0
Training loss: 2.604930633382841
Validation loss: 2.455089199967829

Epoch: 6| Step: 1
Training loss: 2.7894216781700503
Validation loss: 2.452790495048356

Epoch: 6| Step: 2
Training loss: 3.201056133270263
Validation loss: 2.4505167557409995

Epoch: 6| Step: 3
Training loss: 2.5555256159959012
Validation loss: 2.4415509197728102

Epoch: 6| Step: 4
Training loss: 2.3042693195160138
Validation loss: 2.4576590106624483

Epoch: 6| Step: 5
Training loss: 2.9964960616461584
Validation loss: 2.4640926919106096

Epoch: 6| Step: 6
Training loss: 3.094168894758654
Validation loss: 2.475850452280481

Epoch: 6| Step: 7
Training loss: 2.6635652229024305
Validation loss: 2.437935219470721

Epoch: 6| Step: 8
Training loss: 2.46124168816941
Validation loss: 2.4552339792224984

Epoch: 6| Step: 9
Training loss: 3.1132723665768807
Validation loss: 2.4696654012447876

Epoch: 6| Step: 10
Training loss: 3.1051303127297576
Validation loss: 2.4456186274282183

Epoch: 6| Step: 11
Training loss: 2.801995816098996
Validation loss: 2.451030442346326

Epoch: 6| Step: 12
Training loss: 2.4438046812686216
Validation loss: 2.462447756187102

Epoch: 6| Step: 13
Training loss: 3.1710760303574586
Validation loss: 2.4630219017609387

Epoch: 34| Step: 0
Training loss: 2.9985877527675195
Validation loss: 2.4530423329861026

Epoch: 6| Step: 1
Training loss: 2.6398574345559407
Validation loss: 2.453718837899379

Epoch: 6| Step: 2
Training loss: 2.811301166616655
Validation loss: 2.472149814425479

Epoch: 6| Step: 3
Training loss: 3.282067914475118
Validation loss: 2.4516925630839776

Epoch: 6| Step: 4
Training loss: 2.69918350660444
Validation loss: 2.4565550040018076

Epoch: 6| Step: 5
Training loss: 2.6982869860050087
Validation loss: 2.4531948878324528

Epoch: 6| Step: 6
Training loss: 2.4613189885984412
Validation loss: 2.4618162785166047

Epoch: 6| Step: 7
Training loss: 2.20386097836844
Validation loss: 2.4665973863819373

Epoch: 6| Step: 8
Training loss: 3.204196359987086
Validation loss: 2.4653166873058026

Epoch: 6| Step: 9
Training loss: 2.281566650152775
Validation loss: 2.437423582853824

Epoch: 6| Step: 10
Training loss: 2.279927954957648
Validation loss: 2.4700264445667774

Epoch: 6| Step: 11
Training loss: 3.4669727893308377
Validation loss: 2.462883889280164

Epoch: 6| Step: 12
Training loss: 2.987992416086237
Validation loss: 2.4703649698430916

Epoch: 6| Step: 13
Training loss: 2.8587347262249283
Validation loss: 2.461278604442377

Epoch: 35| Step: 0
Training loss: 3.075381377845951
Validation loss: 2.450760502800178

Epoch: 6| Step: 1
Training loss: 2.7528056657723408
Validation loss: 2.4556429097308587

Epoch: 6| Step: 2
Training loss: 2.860209692357001
Validation loss: 2.4449955082391246

Epoch: 6| Step: 3
Training loss: 2.9246921931621546
Validation loss: 2.456438862013104

Epoch: 6| Step: 4
Training loss: 3.2820625389040567
Validation loss: 2.4522895349143488

Epoch: 6| Step: 5
Training loss: 3.039105491039912
Validation loss: 2.442027493623393

Epoch: 6| Step: 6
Training loss: 2.5817084175544465
Validation loss: 2.449465070065568

Epoch: 6| Step: 7
Training loss: 1.9667248679563172
Validation loss: 2.4703033833359074

Epoch: 6| Step: 8
Training loss: 2.526486093548903
Validation loss: 2.4597344164445123

Epoch: 6| Step: 9
Training loss: 2.5936709472375146
Validation loss: 2.460230694937012

Epoch: 6| Step: 10
Training loss: 3.051352004268776
Validation loss: 2.465430117201471

Epoch: 6| Step: 11
Training loss: 2.8415473540047276
Validation loss: 2.4610904019110085

Epoch: 6| Step: 12
Training loss: 2.6891110936173535
Validation loss: 2.469990856792587

Epoch: 6| Step: 13
Training loss: 3.1129111880124567
Validation loss: 2.4702261222430733

Epoch: 36| Step: 0
Training loss: 3.2753817445001108
Validation loss: 2.4614273117729883

Epoch: 6| Step: 1
Training loss: 3.468227312727501
Validation loss: 2.4619210997312817

Epoch: 6| Step: 2
Training loss: 2.1805006035145897
Validation loss: 2.4715331477504834

Epoch: 6| Step: 3
Training loss: 2.9552498091237855
Validation loss: 2.4788413530199267

Epoch: 6| Step: 4
Training loss: 2.830581188574584
Validation loss: 2.455070951201041

Epoch: 6| Step: 5
Training loss: 2.032209199229044
Validation loss: 2.4643701612207254

Epoch: 6| Step: 6
Training loss: 2.8625329094873213
Validation loss: 2.4461821843880776

Epoch: 6| Step: 7
Training loss: 2.5230960201539108
Validation loss: 2.4549009542257534

Epoch: 6| Step: 8
Training loss: 2.5691951653698886
Validation loss: 2.464745820135065

Epoch: 6| Step: 9
Training loss: 3.029927702692743
Validation loss: 2.470505226099027

Epoch: 6| Step: 10
Training loss: 2.6928712265554138
Validation loss: 2.4423865744331885

Epoch: 6| Step: 11
Training loss: 3.1560590327438525
Validation loss: 2.463163925008919

Epoch: 6| Step: 12
Training loss: 2.340708271984908
Validation loss: 2.4720145889527667

Epoch: 6| Step: 13
Training loss: 3.0110405580138564
Validation loss: 2.4679414934338992

Epoch: 37| Step: 0
Training loss: 2.783109065022436
Validation loss: 2.4518537601055788

Epoch: 6| Step: 1
Training loss: 2.9554947321791114
Validation loss: 2.456776666464198

Epoch: 6| Step: 2
Training loss: 2.795638253180347
Validation loss: 2.471822122875561

Epoch: 6| Step: 3
Training loss: 2.465773321025011
Validation loss: 2.4599713954217908

Epoch: 6| Step: 4
Training loss: 2.632176534663157
Validation loss: 2.4469308680419615

Epoch: 6| Step: 5
Training loss: 3.078007574794637
Validation loss: 2.463164930413522

Epoch: 6| Step: 6
Training loss: 2.8619583217057274
Validation loss: 2.445717650215664

Epoch: 6| Step: 7
Training loss: 2.444398069182632
Validation loss: 2.454000937005484

Epoch: 6| Step: 8
Training loss: 2.857846711518672
Validation loss: 2.452100633048647

Epoch: 6| Step: 9
Training loss: 3.0180900657971876
Validation loss: 2.4745582997262257

Epoch: 6| Step: 10
Training loss: 2.9736980763055683
Validation loss: 2.453467434596339

Epoch: 6| Step: 11
Training loss: 3.052869485023939
Validation loss: 2.461030931289456

Epoch: 6| Step: 12
Training loss: 2.8843082176404353
Validation loss: 2.4513497273410083

Epoch: 6| Step: 13
Training loss: 1.7656177081742404
Validation loss: 2.463814698147603

Epoch: 38| Step: 0
Training loss: 2.632401159736808
Validation loss: 2.44863626502171

Epoch: 6| Step: 1
Training loss: 2.1370030411023433
Validation loss: 2.4492787418906836

Epoch: 6| Step: 2
Training loss: 2.8144709145104856
Validation loss: 2.474890440563448

Epoch: 6| Step: 3
Training loss: 2.9914519598659837
Validation loss: 2.447649960900462

Epoch: 6| Step: 4
Training loss: 2.993302817185574
Validation loss: 2.4576418022021023

Epoch: 6| Step: 5
Training loss: 2.4464401228933323
Validation loss: 2.454600388787552

Epoch: 6| Step: 6
Training loss: 2.5566992814902774
Validation loss: 2.4420652388932305

Epoch: 6| Step: 7
Training loss: 2.70258833127698
Validation loss: 2.4513754655813536

Epoch: 6| Step: 8
Training loss: 1.9074372518475458
Validation loss: 2.47493069376549

Epoch: 6| Step: 9
Training loss: 3.2783303486497406
Validation loss: 2.458609080783232

Epoch: 6| Step: 10
Training loss: 2.905874576726267
Validation loss: 2.44832303861716

Epoch: 6| Step: 11
Training loss: 3.087331568027234
Validation loss: 2.4621764861744366

Epoch: 6| Step: 12
Training loss: 2.80086858764846
Validation loss: 2.4319013935209584

Epoch: 6| Step: 13
Training loss: 3.9174420962165444
Validation loss: 2.4570680980676807

Epoch: 39| Step: 0
Training loss: 3.1230490125657164
Validation loss: 2.4423416670271316

Epoch: 6| Step: 1
Training loss: 3.0232499727353086
Validation loss: 2.465115292240035

Epoch: 6| Step: 2
Training loss: 2.7913528569664434
Validation loss: 2.4603641616278615

Epoch: 6| Step: 3
Training loss: 2.191539195986781
Validation loss: 2.4603853902362016

Epoch: 6| Step: 4
Training loss: 2.7394346673077923
Validation loss: 2.44421119688423

Epoch: 6| Step: 5
Training loss: 2.8264273084507328
Validation loss: 2.4630914775537494

Epoch: 6| Step: 6
Training loss: 2.6416111295459843
Validation loss: 2.4613121079650133

Epoch: 6| Step: 7
Training loss: 2.5194218574847045
Validation loss: 2.4541415199896677

Epoch: 6| Step: 8
Training loss: 2.74271137852155
Validation loss: 2.474558164010315

Epoch: 6| Step: 9
Training loss: 2.6403061713784983
Validation loss: 2.4655700812645387

Epoch: 6| Step: 10
Training loss: 2.87157468335918
Validation loss: 2.4596092669914653

Epoch: 6| Step: 11
Training loss: 3.113835341709372
Validation loss: 2.446449619004083

Epoch: 6| Step: 12
Training loss: 2.8338474106505074
Validation loss: 2.471421919588471

Epoch: 6| Step: 13
Training loss: 3.131373505431375
Validation loss: 2.4604150340216804

Epoch: 40| Step: 0
Training loss: 3.1630278979797883
Validation loss: 2.4642786583722684

Epoch: 6| Step: 1
Training loss: 2.8531402821460587
Validation loss: 2.4587653013303643

Epoch: 6| Step: 2
Training loss: 2.8660114589336967
Validation loss: 2.4555102153187254

Epoch: 6| Step: 3
Training loss: 2.8848671143073323
Validation loss: 2.471825046588643

Epoch: 6| Step: 4
Training loss: 2.697082999508505
Validation loss: 2.4761631451724724

Epoch: 6| Step: 5
Training loss: 2.8078812715475387
Validation loss: 2.482587512205356

Epoch: 6| Step: 6
Training loss: 2.3986038001245484
Validation loss: 2.4682563058605482

Epoch: 6| Step: 7
Training loss: 2.5117785504520955
Validation loss: 2.4680696891983995

Epoch: 6| Step: 8
Training loss: 3.3300009441660783
Validation loss: 2.4695889070604444

Epoch: 6| Step: 9
Training loss: 2.207848645648031
Validation loss: 2.4693539027365277

Epoch: 6| Step: 10
Training loss: 2.7622469514278785
Validation loss: 2.4660698374294143

Epoch: 6| Step: 11
Training loss: 2.6294144341894135
Validation loss: 2.461442598203179

Epoch: 6| Step: 12
Training loss: 3.0013602669703676
Validation loss: 2.4758413806110937

Epoch: 6| Step: 13
Training loss: 2.8048349835820523
Validation loss: 2.466625031710481

Epoch: 41| Step: 0
Training loss: 3.32930967352418
Validation loss: 2.4589753974928494

Epoch: 6| Step: 1
Training loss: 3.469433691332031
Validation loss: 2.46287785511711

Epoch: 6| Step: 2
Training loss: 2.746229187405133
Validation loss: 2.467858264185776

Epoch: 6| Step: 3
Training loss: 2.527086955353819
Validation loss: 2.4715962179868893

Epoch: 6| Step: 4
Training loss: 2.693272711650736
Validation loss: 2.4673226216277464

Epoch: 6| Step: 5
Training loss: 2.3836108261792046
Validation loss: 2.4615830670639736

Epoch: 6| Step: 6
Training loss: 2.9770148626897686
Validation loss: 2.456261949077758

Epoch: 6| Step: 7
Training loss: 2.3608745331765024
Validation loss: 2.471258969297801

Epoch: 6| Step: 8
Training loss: 2.510458622709781
Validation loss: 2.463910854592662

Epoch: 6| Step: 9
Training loss: 2.348726406334191
Validation loss: 2.46081075789584

Epoch: 6| Step: 10
Training loss: 3.1725357945845025
Validation loss: 2.43231207452193

Epoch: 6| Step: 11
Training loss: 2.7239747104675187
Validation loss: 2.4601164104811586

Epoch: 6| Step: 12
Training loss: 3.0079375798363728
Validation loss: 2.4556472652090937

Epoch: 6| Step: 13
Training loss: 2.503920913629547
Validation loss: 2.4545148459355595

Epoch: 42| Step: 0
Training loss: 2.0147067795190905
Validation loss: 2.4566485852695776

Epoch: 6| Step: 1
Training loss: 2.5137059254626046
Validation loss: 2.471851420004525

Epoch: 6| Step: 2
Training loss: 2.850313765002015
Validation loss: 2.468562510401041

Epoch: 6| Step: 3
Training loss: 2.5024047729788137
Validation loss: 2.458659186092429

Epoch: 6| Step: 4
Training loss: 2.95151972661445
Validation loss: 2.4500181967164383

Epoch: 6| Step: 5
Training loss: 3.020973009410045
Validation loss: 2.462640504153291

Epoch: 6| Step: 6
Training loss: 3.0965649677682165
Validation loss: 2.4741466422484653

Epoch: 6| Step: 7
Training loss: 2.5708968127392686
Validation loss: 2.458360812278857

Epoch: 6| Step: 8
Training loss: 2.8402446899310703
Validation loss: 2.4681995484837334

Epoch: 6| Step: 9
Training loss: 2.5142913505565336
Validation loss: 2.4585928695633705

Epoch: 6| Step: 10
Training loss: 2.888920131742636
Validation loss: 2.4753798030039524

Epoch: 6| Step: 11
Training loss: 3.1208489886622486
Validation loss: 2.459624580355044

Epoch: 6| Step: 12
Training loss: 3.178114593398799
Validation loss: 2.4543602015667796

Epoch: 6| Step: 13
Training loss: 3.0183407903024815
Validation loss: 2.463690740341594

Epoch: 43| Step: 0
Training loss: 2.6534699196699982
Validation loss: 2.464288058694152

Epoch: 6| Step: 1
Training loss: 2.7076760497055266
Validation loss: 2.461178035931537

Epoch: 6| Step: 2
Training loss: 2.951107244080508
Validation loss: 2.472871183028643

Epoch: 6| Step: 3
Training loss: 2.891761680016815
Validation loss: 2.464081859247919

Epoch: 6| Step: 4
Training loss: 2.9425612331296986
Validation loss: 2.4818016857522767

Epoch: 6| Step: 5
Training loss: 2.6053999358416347
Validation loss: 2.463449251943015

Epoch: 6| Step: 6
Training loss: 2.723784859982605
Validation loss: 2.466368091470009

Epoch: 6| Step: 7
Training loss: 2.953924000345455
Validation loss: 2.4698996587048025

Epoch: 6| Step: 8
Training loss: 2.9221875028719353
Validation loss: 2.476820633934224

Epoch: 6| Step: 9
Training loss: 2.736392078292169
Validation loss: 2.4655228385886585

Epoch: 6| Step: 10
Training loss: 2.8094786628039174
Validation loss: 2.47783668714503

Epoch: 6| Step: 11
Training loss: 2.853787492129001
Validation loss: 2.457809707064977

Epoch: 6| Step: 12
Training loss: 2.8212010893767285
Validation loss: 2.4416942685451555

Epoch: 6| Step: 13
Training loss: 2.2097188334023112
Validation loss: 2.4582309771114113

Epoch: 44| Step: 0
Training loss: 2.4704462332480803
Validation loss: 2.4667354789764953

Epoch: 6| Step: 1
Training loss: 3.06654277910628
Validation loss: 2.456181998064196

Epoch: 6| Step: 2
Training loss: 2.2280028921002395
Validation loss: 2.464716041212554

Epoch: 6| Step: 3
Training loss: 3.15144026996332
Validation loss: 2.4715362751095613

Epoch: 6| Step: 4
Training loss: 2.27773128503424
Validation loss: 2.45145676761432

Epoch: 6| Step: 5
Training loss: 2.817310881068317
Validation loss: 2.4602580611664773

Epoch: 6| Step: 6
Training loss: 3.1799799154355113
Validation loss: 2.48852692598596

Epoch: 6| Step: 7
Training loss: 2.6805187304069524
Validation loss: 2.466118947044863

Epoch: 6| Step: 8
Training loss: 2.607945398929277
Validation loss: 2.4755062750548587

Epoch: 6| Step: 9
Training loss: 3.1136990485814007
Validation loss: 2.4809761968141064

Epoch: 6| Step: 10
Training loss: 2.715420141712974
Validation loss: 2.466529283533484

Epoch: 6| Step: 11
Training loss: 3.1202768872959905
Validation loss: 2.477586148481545

Epoch: 6| Step: 12
Training loss: 2.8277064904214653
Validation loss: 2.474881374697918

Epoch: 6| Step: 13
Training loss: 2.542425938580655
Validation loss: 2.4578016880139897

Epoch: 45| Step: 0
Training loss: 2.922037701488202
Validation loss: 2.469055485026775

Epoch: 6| Step: 1
Training loss: 2.6724760037692867
Validation loss: 2.4684714776390835

Epoch: 6| Step: 2
Training loss: 3.0184234126562313
Validation loss: 2.4566651912570183

Epoch: 6| Step: 3
Training loss: 3.0454957627984225
Validation loss: 2.458437968662688

Epoch: 6| Step: 4
Training loss: 2.7000686919339865
Validation loss: 2.4665339866857607

Epoch: 6| Step: 5
Training loss: 3.18318602402527
Validation loss: 2.469173721622296

Epoch: 6| Step: 6
Training loss: 3.2062344972707875
Validation loss: 2.464580397465065

Epoch: 6| Step: 7
Training loss: 2.900401146866437
Validation loss: 2.4667462002149696

Epoch: 6| Step: 8
Training loss: 2.757376055452292
Validation loss: 2.4622941243776504

Epoch: 6| Step: 9
Training loss: 2.6358883602715313
Validation loss: 2.4679167475663166

Epoch: 6| Step: 10
Training loss: 2.5834762984883515
Validation loss: 2.4469582001927175

Epoch: 6| Step: 11
Training loss: 2.5288329652686135
Validation loss: 2.469361671445856

Epoch: 6| Step: 12
Training loss: 2.514747324360282
Validation loss: 2.468292486773688

Epoch: 6| Step: 13
Training loss: 2.2187950237165412
Validation loss: 2.4589047335358747

Epoch: 46| Step: 0
Training loss: 2.7133435392383083
Validation loss: 2.458096600920427

Epoch: 6| Step: 1
Training loss: 2.3508851352606435
Validation loss: 2.452220636398419

Epoch: 6| Step: 2
Training loss: 3.0221386073582
Validation loss: 2.4580722346554875

Epoch: 6| Step: 3
Training loss: 1.9146590568281454
Validation loss: 2.4715380260140103

Epoch: 6| Step: 4
Training loss: 2.8327098796286916
Validation loss: 2.469657277957003

Epoch: 6| Step: 5
Training loss: 3.2138387005916167
Validation loss: 2.436947069645609

Epoch: 6| Step: 6
Training loss: 3.2038018232627468
Validation loss: 2.4479826685050448

Epoch: 6| Step: 7
Training loss: 2.4228289294463825
Validation loss: 2.4460548213244957

Epoch: 6| Step: 8
Training loss: 2.7577380637694793
Validation loss: 2.4748318122504736

Epoch: 6| Step: 9
Training loss: 2.9734447100296846
Validation loss: 2.454186287212058

Epoch: 6| Step: 10
Training loss: 2.8055501052333485
Validation loss: 2.4669148789876814

Epoch: 6| Step: 11
Training loss: 3.1598953497736275
Validation loss: 2.481404561623471

Epoch: 6| Step: 12
Training loss: 2.8347612784390335
Validation loss: 2.4435262993460993

Epoch: 6| Step: 13
Training loss: 2.2466324931336903
Validation loss: 2.454600909954479

Epoch: 47| Step: 0
Training loss: 2.453505759848719
Validation loss: 2.4609260763257055

Epoch: 6| Step: 1
Training loss: 2.827676136794086
Validation loss: 2.4549509002558776

Epoch: 6| Step: 2
Training loss: 2.1665333437859498
Validation loss: 2.4719073390332245

Epoch: 6| Step: 3
Training loss: 2.825805134101484
Validation loss: 2.4557405856337917

Epoch: 6| Step: 4
Training loss: 2.5367640964413014
Validation loss: 2.4692738264023473

Epoch: 6| Step: 5
Training loss: 2.886670846502906
Validation loss: 2.4542665048396093

Epoch: 6| Step: 6
Training loss: 3.7171364497318757
Validation loss: 2.473301443940804

Epoch: 6| Step: 7
Training loss: 3.0693662316055903
Validation loss: 2.4637467826786916

Epoch: 6| Step: 8
Training loss: 2.703730697245482
Validation loss: 2.459532804018587

Epoch: 6| Step: 9
Training loss: 2.42925050000834
Validation loss: 2.4684074281516852

Epoch: 6| Step: 10
Training loss: 2.7493210734745532
Validation loss: 2.4714560537269334

Epoch: 6| Step: 11
Training loss: 2.6508865960614427
Validation loss: 2.4734733151873023

Epoch: 6| Step: 12
Training loss: 2.8902310489574052
Validation loss: 2.475028614248773

Epoch: 6| Step: 13
Training loss: 2.58773225119755
Validation loss: 2.4849313381468874

Epoch: 48| Step: 0
Training loss: 2.894321861364145
Validation loss: 2.459965776183242

Epoch: 6| Step: 1
Training loss: 3.3513018455607058
Validation loss: 2.4643132800092054

Epoch: 6| Step: 2
Training loss: 2.444810907271655
Validation loss: 2.456878826932353

Epoch: 6| Step: 3
Training loss: 2.7536492843322504
Validation loss: 2.469781241995651

Epoch: 6| Step: 4
Training loss: 3.0020716190219887
Validation loss: 2.466528740981418

Epoch: 6| Step: 5
Training loss: 2.241617269810787
Validation loss: 2.474685967805083

Epoch: 6| Step: 6
Training loss: 3.100296030984191
Validation loss: 2.445841055681057

Epoch: 6| Step: 7
Training loss: 3.2293858771623625
Validation loss: 2.4698266718603894

Epoch: 6| Step: 8
Training loss: 2.5594894588526884
Validation loss: 2.4469254252531654

Epoch: 6| Step: 9
Training loss: 2.748988572436546
Validation loss: 2.4455498669902833

Epoch: 6| Step: 10
Training loss: 2.522745610388389
Validation loss: 2.4604720511978218

Epoch: 6| Step: 11
Training loss: 2.7776637583749846
Validation loss: 2.4760307490857016

Epoch: 6| Step: 12
Training loss: 2.66354606746409
Validation loss: 2.4499498088015845

Epoch: 6| Step: 13
Training loss: 2.2004460489480433
Validation loss: 2.4529557638352104

Epoch: 49| Step: 0
Training loss: 2.362113121689603
Validation loss: 2.4539655242572724

Epoch: 6| Step: 1
Training loss: 2.6685224771108724
Validation loss: 2.4728180180950114

Epoch: 6| Step: 2
Training loss: 2.678626381446624
Validation loss: 2.4599435346275604

Epoch: 6| Step: 3
Training loss: 2.5342030195216863
Validation loss: 2.4600276590075105

Epoch: 6| Step: 4
Training loss: 2.317312851214275
Validation loss: 2.461229920092637

Epoch: 6| Step: 5
Training loss: 2.302334997537815
Validation loss: 2.4664325795379103

Epoch: 6| Step: 6
Training loss: 2.3438981072995353
Validation loss: 2.4583575795185744

Epoch: 6| Step: 7
Training loss: 3.0481447361801304
Validation loss: 2.4760154833487693

Epoch: 6| Step: 8
Training loss: 2.9272709760332174
Validation loss: 2.481936392963574

Epoch: 6| Step: 9
Training loss: 3.328744750009795
Validation loss: 2.479440127152551

Epoch: 6| Step: 10
Training loss: 3.2023356139541987
Validation loss: 2.4827324412452594

Epoch: 6| Step: 11
Training loss: 3.406855398152152
Validation loss: 2.4630807976525637

Epoch: 6| Step: 12
Training loss: 2.8415950113549133
Validation loss: 2.4703985400436292

Epoch: 6| Step: 13
Training loss: 2.30643409883272
Validation loss: 2.4531562708605565

Epoch: 50| Step: 0
Training loss: 2.9306049995610373
Validation loss: 2.45728705544595

Epoch: 6| Step: 1
Training loss: 2.5770441737446794
Validation loss: 2.4610099764879863

Epoch: 6| Step: 2
Training loss: 3.4106120605670687
Validation loss: 2.4539338698548416

Epoch: 6| Step: 3
Training loss: 2.3252290469321557
Validation loss: 2.45501750103987

Epoch: 6| Step: 4
Training loss: 3.219511812295348
Validation loss: 2.4618820666902224

Epoch: 6| Step: 5
Training loss: 2.528252133260193
Validation loss: 2.4734554036473706

Epoch: 6| Step: 6
Training loss: 3.2225100582371273
Validation loss: 2.4768447914359824

Epoch: 6| Step: 7
Training loss: 1.9502755703980228
Validation loss: 2.4645450368718222

Epoch: 6| Step: 8
Training loss: 2.8805981740697715
Validation loss: 2.4604973157965193

Epoch: 6| Step: 9
Training loss: 1.6710177211221309
Validation loss: 2.455271599777291

Epoch: 6| Step: 10
Training loss: 3.0915514908250135
Validation loss: 2.478875969829948

Epoch: 6| Step: 11
Training loss: 2.5295146126718095
Validation loss: 2.486749604027604

Epoch: 6| Step: 12
Training loss: 3.267377611245412
Validation loss: 2.4765183479242547

Epoch: 6| Step: 13
Training loss: 2.3355868787668386
Validation loss: 2.4644557739258346

Epoch: 51| Step: 0
Training loss: 2.832102134080579
Validation loss: 2.469506870074728

Epoch: 6| Step: 1
Training loss: 2.4383807425106476
Validation loss: 2.486033250416686

Epoch: 6| Step: 2
Training loss: 2.2839673723606064
Validation loss: 2.4691958333148323

Epoch: 6| Step: 3
Training loss: 2.3581127301707525
Validation loss: 2.4717098067372163

Epoch: 6| Step: 4
Training loss: 2.982448415382073
Validation loss: 2.471915972443088

Epoch: 6| Step: 5
Training loss: 3.3606864321121823
Validation loss: 2.4784283847253246

Epoch: 6| Step: 6
Training loss: 3.0420867072494544
Validation loss: 2.4842213155996937

Epoch: 6| Step: 7
Training loss: 2.1797631910301005
Validation loss: 2.4764409101228324

Epoch: 6| Step: 8
Training loss: 2.3493125518755793
Validation loss: 2.4527833365077587

Epoch: 6| Step: 9
Training loss: 2.600382035871443
Validation loss: 2.468813832917948

Epoch: 6| Step: 10
Training loss: 3.149545025135606
Validation loss: 2.4808176909003268

Epoch: 6| Step: 11
Training loss: 3.334639928959429
Validation loss: 2.4683209949031673

Epoch: 6| Step: 12
Training loss: 2.998308022522837
Validation loss: 2.4646810185342054

Epoch: 6| Step: 13
Training loss: 2.501975899919517
Validation loss: 2.503529450655531

Epoch: 52| Step: 0
Training loss: 2.589407537441431
Validation loss: 2.471896346157841

Epoch: 6| Step: 1
Training loss: 2.4079062966526483
Validation loss: 2.4741403153762143

Epoch: 6| Step: 2
Training loss: 2.8734317316906997
Validation loss: 2.4662908983597993

Epoch: 6| Step: 3
Training loss: 3.389191917948798
Validation loss: 2.45785947768777

Epoch: 6| Step: 4
Training loss: 2.907143896309589
Validation loss: 2.4840143155645893

Epoch: 6| Step: 5
Training loss: 3.2335333719674137
Validation loss: 2.4646413832584195

Epoch: 6| Step: 6
Training loss: 2.9372564985927583
Validation loss: 2.444172583243231

Epoch: 6| Step: 7
Training loss: 2.3917784058877807
Validation loss: 2.460373596703289

Epoch: 6| Step: 8
Training loss: 2.8828480622700665
Validation loss: 2.4807508950728034

Epoch: 6| Step: 9
Training loss: 2.8189787947339924
Validation loss: 2.458643031509038

Epoch: 6| Step: 10
Training loss: 2.9363311714784817
Validation loss: 2.458085217754894

Epoch: 6| Step: 11
Training loss: 2.453057355008825
Validation loss: 2.4542825263236843

Epoch: 6| Step: 12
Training loss: 2.225887263120884
Validation loss: 2.4623800945307144

Epoch: 6| Step: 13
Training loss: 2.4911782065543853
Validation loss: 2.4577661172411243

Epoch: 53| Step: 0
Training loss: 2.334670932298936
Validation loss: 2.461400157997879

Epoch: 6| Step: 1
Training loss: 2.483250393431861
Validation loss: 2.4677385018451012

Epoch: 6| Step: 2
Training loss: 2.8600463079158094
Validation loss: 2.471836547461211

Epoch: 6| Step: 3
Training loss: 3.3197961742385282
Validation loss: 2.4557327247689287

Epoch: 6| Step: 4
Training loss: 3.311264959454901
Validation loss: 2.4533870502975037

Epoch: 6| Step: 5
Training loss: 2.4612213455477314
Validation loss: 2.449255716650307

Epoch: 6| Step: 6
Training loss: 2.612985910738988
Validation loss: 2.438871454829118

Epoch: 6| Step: 7
Training loss: 3.1214164022836353
Validation loss: 2.4745902549179233

Epoch: 6| Step: 8
Training loss: 3.013532950560888
Validation loss: 2.4484346590958097

Epoch: 6| Step: 9
Training loss: 2.619357902964393
Validation loss: 2.48896221998141

Epoch: 6| Step: 10
Training loss: 2.5429618120465074
Validation loss: 2.4868582348809865

Epoch: 6| Step: 11
Training loss: 3.1427665734555426
Validation loss: 2.466312566580749

Epoch: 6| Step: 12
Training loss: 2.4886621875301955
Validation loss: 2.484596419546236

Epoch: 6| Step: 13
Training loss: 1.5411252368385664
Validation loss: 2.468928667994029

Epoch: 54| Step: 0
Training loss: 2.144472326362062
Validation loss: 2.46213828703879

Epoch: 6| Step: 1
Training loss: 2.71514302532099
Validation loss: 2.4408587654783105

Epoch: 6| Step: 2
Training loss: 2.0490897041668665
Validation loss: 2.4468018869308645

Epoch: 6| Step: 3
Training loss: 3.4182580792720714
Validation loss: 2.4763701850015254

Epoch: 6| Step: 4
Training loss: 2.8699806183482885
Validation loss: 2.465136747723252

Epoch: 6| Step: 5
Training loss: 2.949747081194857
Validation loss: 2.4545733997093424

Epoch: 6| Step: 6
Training loss: 2.8987964500400247
Validation loss: 2.4476293398270847

Epoch: 6| Step: 7
Training loss: 2.267919391511533
Validation loss: 2.4484236325567825

Epoch: 6| Step: 8
Training loss: 2.626162317075051
Validation loss: 2.4762841202751646

Epoch: 6| Step: 9
Training loss: 3.331446033886303
Validation loss: 2.4610645746437974

Epoch: 6| Step: 10
Training loss: 2.654021551296235
Validation loss: 2.4467543845546604

Epoch: 6| Step: 11
Training loss: 1.932503687767468
Validation loss: 2.466047142574876

Epoch: 6| Step: 12
Training loss: 2.981070401522005
Validation loss: 2.4723597665688355

Epoch: 6| Step: 13
Training loss: 3.808558318144481
Validation loss: 2.4724055964767517

Epoch: 55| Step: 0
Training loss: 2.697449917474186
Validation loss: 2.4442525384855016

Epoch: 6| Step: 1
Training loss: 2.6159851956488134
Validation loss: 2.4678035950629993

Epoch: 6| Step: 2
Training loss: 1.9413734602125945
Validation loss: 2.4725512286077227

Epoch: 6| Step: 3
Training loss: 2.449526924080328
Validation loss: 2.4550158887245037

Epoch: 6| Step: 4
Training loss: 2.388661993572417
Validation loss: 2.4789518878734187

Epoch: 6| Step: 5
Training loss: 2.247713940953712
Validation loss: 2.4609155349545526

Epoch: 6| Step: 6
Training loss: 3.543695444699665
Validation loss: 2.4561108539713583

Epoch: 6| Step: 7
Training loss: 3.1902567592119078
Validation loss: 2.4582265031508936

Epoch: 6| Step: 8
Training loss: 3.0702617146026467
Validation loss: 2.478724234330227

Epoch: 6| Step: 9
Training loss: 2.792890214149317
Validation loss: 2.4538290971385788

Epoch: 6| Step: 10
Training loss: 2.6968078003751597
Validation loss: 2.4613305843908178

Epoch: 6| Step: 11
Training loss: 3.1287291592167894
Validation loss: 2.437519752836795

Epoch: 6| Step: 12
Training loss: 2.6449035652911697
Validation loss: 2.461764079719183

Epoch: 6| Step: 13
Training loss: 2.903465824221018
Validation loss: 2.472911775959451

Epoch: 56| Step: 0
Training loss: 2.8825638826674234
Validation loss: 2.4710759361000836

Epoch: 6| Step: 1
Training loss: 2.5543666253117125
Validation loss: 2.454974757616677

Epoch: 6| Step: 2
Training loss: 2.5695898089371867
Validation loss: 2.4642578341804713

Epoch: 6| Step: 3
Training loss: 2.198082747369222
Validation loss: 2.44838589735622

Epoch: 6| Step: 4
Training loss: 3.3525980881146022
Validation loss: 2.4621055923478177

Epoch: 6| Step: 5
Training loss: 2.987948370450457
Validation loss: 2.450828380120366

Epoch: 6| Step: 6
Training loss: 2.8900799211585126
Validation loss: 2.472280629600181

Epoch: 6| Step: 7
Training loss: 2.203921018684177
Validation loss: 2.4666301566478666

Epoch: 6| Step: 8
Training loss: 2.9703515802227582
Validation loss: 2.478638042932298

Epoch: 6| Step: 9
Training loss: 3.3510807286860307
Validation loss: 2.4591970513269708

Epoch: 6| Step: 10
Training loss: 2.378942930944113
Validation loss: 2.432978818350728

Epoch: 6| Step: 11
Training loss: 2.8056253124499473
Validation loss: 2.457635441185288

Epoch: 6| Step: 12
Training loss: 2.345702515156561
Validation loss: 2.456304860054925

Epoch: 6| Step: 13
Training loss: 3.0470593518948275
Validation loss: 2.458637162634072

Epoch: 57| Step: 0
Training loss: 3.4181362961167254
Validation loss: 2.455671804822961

Epoch: 6| Step: 1
Training loss: 2.647154550753728
Validation loss: 2.4726817794197538

Epoch: 6| Step: 2
Training loss: 2.34033574964171
Validation loss: 2.451118193386949

Epoch: 6| Step: 3
Training loss: 2.7388203263056807
Validation loss: 2.441025066848621

Epoch: 6| Step: 4
Training loss: 2.641633061416868
Validation loss: 2.448150633184637

Epoch: 6| Step: 5
Training loss: 2.311358608336882
Validation loss: 2.475065356995392

Epoch: 6| Step: 6
Training loss: 2.060278765695727
Validation loss: 2.469402901361839

Epoch: 6| Step: 7
Training loss: 2.9853787157335634
Validation loss: 2.4765700618387387

Epoch: 6| Step: 8
Training loss: 3.0552025870472796
Validation loss: 2.4660063212506254

Epoch: 6| Step: 9
Training loss: 2.7107652944787546
Validation loss: 2.447885841344855

Epoch: 6| Step: 10
Training loss: 2.3124593782723335
Validation loss: 2.432666472922996

Epoch: 6| Step: 11
Training loss: 2.730922241098466
Validation loss: 2.443975411262088

Epoch: 6| Step: 12
Training loss: 3.4192744436872045
Validation loss: 2.4662965530816643

Epoch: 6| Step: 13
Training loss: 2.9212210020193883
Validation loss: 2.464104554516372

Epoch: 58| Step: 0
Training loss: 2.9366660050194033
Validation loss: 2.4608766776474686

Epoch: 6| Step: 1
Training loss: 2.620684755254966
Validation loss: 2.467541156855316

Epoch: 6| Step: 2
Training loss: 2.3584809282933654
Validation loss: 2.4673054836843233

Epoch: 6| Step: 3
Training loss: 2.894114434711602
Validation loss: 2.4605807117201763

Epoch: 6| Step: 4
Training loss: 3.375242153946635
Validation loss: 2.4569070563812545

Epoch: 6| Step: 5
Training loss: 2.42940850817961
Validation loss: 2.4642056215598966

Epoch: 6| Step: 6
Training loss: 2.508069747137497
Validation loss: 2.4692062770415992

Epoch: 6| Step: 7
Training loss: 2.9921538746988747
Validation loss: 2.4507438473885808

Epoch: 6| Step: 8
Training loss: 2.8876923402081425
Validation loss: 2.4702540725502793

Epoch: 6| Step: 9
Training loss: 2.5373738938739856
Validation loss: 2.464796643177286

Epoch: 6| Step: 10
Training loss: 2.0999501222409207
Validation loss: 2.4733665223903927

Epoch: 6| Step: 11
Training loss: 3.1201832079282776
Validation loss: 2.46999363373269

Epoch: 6| Step: 12
Training loss: 2.9068871896833084
Validation loss: 2.457651856914516

Epoch: 6| Step: 13
Training loss: 2.695711410414651
Validation loss: 2.4419622749729104

Epoch: 59| Step: 0
Training loss: 3.11545080312064
Validation loss: 2.4413316499859268

Epoch: 6| Step: 1
Training loss: 2.368315877865762
Validation loss: 2.4673035562556898

Epoch: 6| Step: 2
Training loss: 2.854501375124198
Validation loss: 2.4701122836389433

Epoch: 6| Step: 3
Training loss: 2.6337284113683084
Validation loss: 2.4469326999384124

Epoch: 6| Step: 4
Training loss: 2.748988572436546
Validation loss: 2.486033376740809

Epoch: 6| Step: 5
Training loss: 2.9782149566481815
Validation loss: 2.4682431763215336

Epoch: 6| Step: 6
Training loss: 2.548887896659072
Validation loss: 2.4478345017180843

Epoch: 6| Step: 7
Training loss: 3.2070050813487767
Validation loss: 2.4850929736087255

Epoch: 6| Step: 8
Training loss: 2.0134367190378306
Validation loss: 2.45311349396195

Epoch: 6| Step: 9
Training loss: 2.5698794663956046
Validation loss: 2.4604117872891655

Epoch: 6| Step: 10
Training loss: 2.556016302268704
Validation loss: 2.455128366066546

Epoch: 6| Step: 11
Training loss: 3.183805231664435
Validation loss: 2.4458154709312154

Epoch: 6| Step: 12
Training loss: 2.812447017594487
Validation loss: 2.47189756424904

Epoch: 6| Step: 13
Training loss: 2.7011675500180035
Validation loss: 2.4797233982923843

Epoch: 60| Step: 0
Training loss: 2.4193565810617716
Validation loss: 2.4490049485764542

Epoch: 6| Step: 1
Training loss: 2.8228928024202937
Validation loss: 2.470893708997452

Epoch: 6| Step: 2
Training loss: 3.4286735269694337
Validation loss: 2.4455740770823415

Epoch: 6| Step: 3
Training loss: 2.978782006530797
Validation loss: 2.4667639884717847

Epoch: 6| Step: 4
Training loss: 2.3163620942974767
Validation loss: 2.4694370275923636

Epoch: 6| Step: 5
Training loss: 2.745389020414609
Validation loss: 2.4794655686290974

Epoch: 6| Step: 6
Training loss: 2.5327200682692643
Validation loss: 2.473228611386573

Epoch: 6| Step: 7
Training loss: 2.798523656731813
Validation loss: 2.4701547775773802

Epoch: 6| Step: 8
Training loss: 2.793470985765814
Validation loss: 2.4776379849682337

Epoch: 6| Step: 9
Training loss: 2.581456107346254
Validation loss: 2.4913384313723217

Epoch: 6| Step: 10
Training loss: 3.098625272061815
Validation loss: 2.459027858782546

Epoch: 6| Step: 11
Training loss: 3.352084317077783
Validation loss: 2.480610939080094

Epoch: 6| Step: 12
Training loss: 2.000933667640525
Validation loss: 2.4656323193917506

Epoch: 6| Step: 13
Training loss: 2.3801498795447062
Validation loss: 2.465097506645807

Epoch: 61| Step: 0
Training loss: 1.9295788321225984
Validation loss: 2.487000172045346

Epoch: 6| Step: 1
Training loss: 2.368835278588911
Validation loss: 2.450196576430705

Epoch: 6| Step: 2
Training loss: 3.1364718633381234
Validation loss: 2.4860339134893574

Epoch: 6| Step: 3
Training loss: 3.380787443721362
Validation loss: 2.4774248345330587

Epoch: 6| Step: 4
Training loss: 2.6962592632079754
Validation loss: 2.445856773886191

Epoch: 6| Step: 5
Training loss: 2.5540671805774475
Validation loss: 2.4739112428977195

Epoch: 6| Step: 6
Training loss: 3.20309180265762
Validation loss: 2.4561879401273234

Epoch: 6| Step: 7
Training loss: 3.2205352276772197
Validation loss: 2.465248811135569

Epoch: 6| Step: 8
Training loss: 2.471130382400186
Validation loss: 2.46911163090605

Epoch: 6| Step: 9
Training loss: 3.3621740700830856
Validation loss: 2.478070111118082

Epoch: 6| Step: 10
Training loss: 1.7945987381490567
Validation loss: 2.464673324524621

Epoch: 6| Step: 11
Training loss: 2.6916363843350153
Validation loss: 2.453968464021648

Epoch: 6| Step: 12
Training loss: 2.562714265378671
Validation loss: 2.4727777419059

Epoch: 6| Step: 13
Training loss: 2.6560362785975493
Validation loss: 2.4536577509319035

Epoch: 62| Step: 0
Training loss: 2.394164015222664
Validation loss: 2.463616153295389

Epoch: 6| Step: 1
Training loss: 2.6780787496234257
Validation loss: 2.4352816522766045

Epoch: 6| Step: 2
Training loss: 3.0253669831427774
Validation loss: 2.4685555305279374

Epoch: 6| Step: 3
Training loss: 1.8625541883944774
Validation loss: 2.4706250649667645

Epoch: 6| Step: 4
Training loss: 2.1579756534889416
Validation loss: 2.4662148909869654

Epoch: 6| Step: 5
Training loss: 3.6183763696567324
Validation loss: 2.4680966828635205

Epoch: 6| Step: 6
Training loss: 3.244084770553192
Validation loss: 2.447830340753405

Epoch: 6| Step: 7
Training loss: 3.487021497488967
Validation loss: 2.4599919870284266

Epoch: 6| Step: 8
Training loss: 2.265757484508697
Validation loss: 2.4606180004799727

Epoch: 6| Step: 9
Training loss: 2.954613527434101
Validation loss: 2.4513873604453815

Epoch: 6| Step: 10
Training loss: 2.4460925725728764
Validation loss: 2.4599996039720975

Epoch: 6| Step: 11
Training loss: 2.611795730209453
Validation loss: 2.46418615286048

Epoch: 6| Step: 12
Training loss: 2.781485986877614
Validation loss: 2.4702092908829396

Epoch: 6| Step: 13
Training loss: 2.0721512600245635
Validation loss: 2.462409498793007

Epoch: 63| Step: 0
Training loss: 3.170995129848892
Validation loss: 2.4689942666527895

Epoch: 6| Step: 1
Training loss: 2.0528017390528484
Validation loss: 2.4578507865589407

Epoch: 6| Step: 2
Training loss: 3.258078584850813
Validation loss: 2.4691077861351025

Epoch: 6| Step: 3
Training loss: 2.7202847413077005
Validation loss: 2.472819833405751

Epoch: 6| Step: 4
Training loss: 2.7249242894565464
Validation loss: 2.473678480467069

Epoch: 6| Step: 5
Training loss: 2.3437470499655912
Validation loss: 2.465176891885691

Epoch: 6| Step: 6
Training loss: 3.0087529285196366
Validation loss: 2.4563646736629083

Epoch: 6| Step: 7
Training loss: 2.699814903131219
Validation loss: 2.448721234318056

Epoch: 6| Step: 8
Training loss: 3.0256850294538156
Validation loss: 2.4709262127046294

Epoch: 6| Step: 9
Training loss: 2.9689163964469927
Validation loss: 2.4317625256800697

Epoch: 6| Step: 10
Training loss: 2.093160290014796
Validation loss: 2.4603876377582146

Epoch: 6| Step: 11
Training loss: 2.7524805152473664
Validation loss: 2.4645394811120926

Epoch: 6| Step: 12
Training loss: 2.76990156831992
Validation loss: 2.460397343653762

Epoch: 6| Step: 13
Training loss: 2.67791422456288
Validation loss: 2.450244003703607

Epoch: 64| Step: 0
Training loss: 2.1023200842844765
Validation loss: 2.4478657909404826

Epoch: 6| Step: 1
Training loss: 2.475925207120785
Validation loss: 2.4567883671752906

Epoch: 6| Step: 2
Training loss: 2.8754534156513567
Validation loss: 2.4618849261907614

Epoch: 6| Step: 3
Training loss: 2.2163362940443716
Validation loss: 2.4638377382038263

Epoch: 6| Step: 4
Training loss: 3.207994286263341
Validation loss: 2.4588386749014424

Epoch: 6| Step: 5
Training loss: 3.292887948018883
Validation loss: 2.461113373721812

Epoch: 6| Step: 6
Training loss: 3.249476757477686
Validation loss: 2.4658851631739545

Epoch: 6| Step: 7
Training loss: 2.3674219600782087
Validation loss: 2.4646610007452843

Epoch: 6| Step: 8
Training loss: 3.2013631301546037
Validation loss: 2.464276962648589

Epoch: 6| Step: 9
Training loss: 2.1356390364322118
Validation loss: 2.452619991935609

Epoch: 6| Step: 10
Training loss: 2.1258867321039463
Validation loss: 2.4739506477966273

Epoch: 6| Step: 11
Training loss: 2.85557495420656
Validation loss: 2.466389113006672

Epoch: 6| Step: 12
Training loss: 3.181209265667346
Validation loss: 2.4644672488544224

Epoch: 6| Step: 13
Training loss: 2.612153726654464
Validation loss: 2.4523259095809955

Epoch: 65| Step: 0
Training loss: 3.156010986947201
Validation loss: 2.486728169040448

Epoch: 6| Step: 1
Training loss: 2.280776823953807
Validation loss: 2.453125623895344

Epoch: 6| Step: 2
Training loss: 2.421991160898751
Validation loss: 2.4451046919283614

Epoch: 6| Step: 3
Training loss: 2.778122933454571
Validation loss: 2.4837808598324456

Epoch: 6| Step: 4
Training loss: 3.0254710059326726
Validation loss: 2.450569114607406

Epoch: 6| Step: 5
Training loss: 2.402372431390017
Validation loss: 2.463076223214297

Epoch: 6| Step: 6
Training loss: 2.2401956356992696
Validation loss: 2.4713150080613135

Epoch: 6| Step: 7
Training loss: 2.917125901898452
Validation loss: 2.456496243863271

Epoch: 6| Step: 8
Training loss: 3.480258853975352
Validation loss: 2.4781702517066533

Epoch: 6| Step: 9
Training loss: 2.2423876098230626
Validation loss: 2.4714749210992504

Epoch: 6| Step: 10
Training loss: 2.8712536825672097
Validation loss: 2.466065445262102

Epoch: 6| Step: 11
Training loss: 2.6775155120599727
Validation loss: 2.47342606021937

Epoch: 6| Step: 12
Training loss: 2.983162039273676
Validation loss: 2.4454628124585343

Epoch: 6| Step: 13
Training loss: 2.6404122712713396
Validation loss: 2.4716346754677927

Epoch: 66| Step: 0
Training loss: 3.0028749042258966
Validation loss: 2.480033036161475

Epoch: 6| Step: 1
Training loss: 1.8421810392698412
Validation loss: 2.4799324554663897

Epoch: 6| Step: 2
Training loss: 2.5969643036948984
Validation loss: 2.472004143608654

Epoch: 6| Step: 3
Training loss: 2.935962680548284
Validation loss: 2.463536171429086

Epoch: 6| Step: 4
Training loss: 3.43525872423627
Validation loss: 2.473387275075853

Epoch: 6| Step: 5
Training loss: 2.701717826300826
Validation loss: 2.4630893948673664

Epoch: 6| Step: 6
Training loss: 3.113214776874259
Validation loss: 2.470055158943387

Epoch: 6| Step: 7
Training loss: 1.8786547327919263
Validation loss: 2.4745602733040815

Epoch: 6| Step: 8
Training loss: 2.4169798571339522
Validation loss: 2.4858285092480163

Epoch: 6| Step: 9
Training loss: 2.8088933071354565
Validation loss: 2.4830122300799835

Epoch: 6| Step: 10
Training loss: 2.6921693913077127
Validation loss: 2.4742356104923413

Epoch: 6| Step: 11
Training loss: 2.3113989399629533
Validation loss: 2.4754240675586656

Epoch: 6| Step: 12
Training loss: 3.2819521198208963
Validation loss: 2.469303277311943

Epoch: 6| Step: 13
Training loss: 3.2332949101894997
Validation loss: 2.4559487474212416

Epoch: 67| Step: 0
Training loss: 2.1522716364748367
Validation loss: 2.4756159677901035

Epoch: 6| Step: 1
Training loss: 2.9149989855389933
Validation loss: 2.455027501757318

Epoch: 6| Step: 2
Training loss: 2.5787359265208307
Validation loss: 2.4504362835405513

Epoch: 6| Step: 3
Training loss: 3.093035682904058
Validation loss: 2.4805241569626975

Epoch: 6| Step: 4
Training loss: 2.702472585985458
Validation loss: 2.4599750179063062

Epoch: 6| Step: 5
Training loss: 3.02613824284267
Validation loss: 2.465096878500617

Epoch: 6| Step: 6
Training loss: 2.2685504836110426
Validation loss: 2.46278968615837

Epoch: 6| Step: 7
Training loss: 2.367223556797897
Validation loss: 2.4622069371883324

Epoch: 6| Step: 8
Training loss: 2.2702800414987117
Validation loss: 2.4643994928456263

Epoch: 6| Step: 9
Training loss: 3.353381110344576
Validation loss: 2.452412172724124

Epoch: 6| Step: 10
Training loss: 2.6271389920305444
Validation loss: 2.4462271150863977

Epoch: 6| Step: 11
Training loss: 3.1303596311914803
Validation loss: 2.4563017727941014

Epoch: 6| Step: 12
Training loss: 2.771511903052153
Validation loss: 2.448628359363379

Epoch: 6| Step: 13
Training loss: 2.733820744607711
Validation loss: 2.4353857623524715

Epoch: 68| Step: 0
Training loss: 2.539121844258411
Validation loss: 2.4752060335941644

Epoch: 6| Step: 1
Training loss: 2.909557798156027
Validation loss: 2.4551129693157194

Epoch: 6| Step: 2
Training loss: 2.3966403652369714
Validation loss: 2.473582527533026

Epoch: 6| Step: 3
Training loss: 3.1812047689104017
Validation loss: 2.4561335195605136

Epoch: 6| Step: 4
Training loss: 3.336953787808608
Validation loss: 2.465036925257465

Epoch: 6| Step: 5
Training loss: 3.349285214589739
Validation loss: 2.4576317641387058

Epoch: 6| Step: 6
Training loss: 2.6452123759339146
Validation loss: 2.463576531065413

Epoch: 6| Step: 7
Training loss: 2.808926919374196
Validation loss: 2.4557902713548425

Epoch: 6| Step: 8
Training loss: 2.146604664758156
Validation loss: 2.4893705881517816

Epoch: 6| Step: 9
Training loss: 2.4264524455760696
Validation loss: 2.4737248035996484

Epoch: 6| Step: 10
Training loss: 2.6024993034603714
Validation loss: 2.449165673510094

Epoch: 6| Step: 11
Training loss: 2.1943463680493087
Validation loss: 2.465691955078202

Epoch: 6| Step: 12
Training loss: 3.052441954564628
Validation loss: 2.461303058746626

Epoch: 6| Step: 13
Training loss: 2.486998606234698
Validation loss: 2.4781921651968353

Epoch: 69| Step: 0
Training loss: 2.5344693468550186
Validation loss: 2.475757357625095

Epoch: 6| Step: 1
Training loss: 2.870681172524582
Validation loss: 2.4671115560374

Epoch: 6| Step: 2
Training loss: 2.940199889569585
Validation loss: 2.469024883816926

Epoch: 6| Step: 3
Training loss: 2.397428418344883
Validation loss: 2.474671937950792

Epoch: 6| Step: 4
Training loss: 2.173124873084788
Validation loss: 2.484973872696452

Epoch: 6| Step: 5
Training loss: 2.631235165225727
Validation loss: 2.452710855238162

Epoch: 6| Step: 6
Training loss: 2.4951751881406925
Validation loss: 2.4624147152721525

Epoch: 6| Step: 7
Training loss: 3.4607537007663756
Validation loss: 2.460416493799569

Epoch: 6| Step: 8
Training loss: 3.296215696536078
Validation loss: 2.479456247562468

Epoch: 6| Step: 9
Training loss: 2.6438702386879305
Validation loss: 2.4578505935962363

Epoch: 6| Step: 10
Training loss: 2.5601584738616086
Validation loss: 2.461807138469955

Epoch: 6| Step: 11
Training loss: 1.7183596774701726
Validation loss: 2.4470484052292667

Epoch: 6| Step: 12
Training loss: 2.9960934316940833
Validation loss: 2.457242251861156

Epoch: 6| Step: 13
Training loss: 3.206131282638575
Validation loss: 2.4467620678543747

Epoch: 70| Step: 0
Training loss: 2.803811449184507
Validation loss: 2.4740146423736387

Epoch: 6| Step: 1
Training loss: 2.9489205923374926
Validation loss: 2.4655086526118493

Epoch: 6| Step: 2
Training loss: 2.844897363618147
Validation loss: 2.4656386587379453

Epoch: 6| Step: 3
Training loss: 2.3580117234382687
Validation loss: 2.4861856833866365

Epoch: 6| Step: 4
Training loss: 2.9500141208116752
Validation loss: 2.481711289442448

Epoch: 6| Step: 5
Training loss: 2.7118312450781716
Validation loss: 2.450426177250592

Epoch: 6| Step: 6
Training loss: 2.344094924977115
Validation loss: 2.453987393763341

Epoch: 6| Step: 7
Training loss: 2.5990008781992304
Validation loss: 2.4641669076863972

Epoch: 6| Step: 8
Training loss: 3.105036329956795
Validation loss: 2.4673590562463876

Epoch: 6| Step: 9
Training loss: 2.982655294361128
Validation loss: 2.466752086691346

Epoch: 6| Step: 10
Training loss: 2.8893023790016663
Validation loss: 2.4752509723961373

Epoch: 6| Step: 11
Training loss: 2.4452334260190676
Validation loss: 2.4594609448561506

Epoch: 6| Step: 12
Training loss: 2.449889752768523
Validation loss: 2.4695173311721557

Epoch: 6| Step: 13
Training loss: 2.87614816046858
Validation loss: 2.486662837169091

Epoch: 71| Step: 0
Training loss: 3.407240697296204
Validation loss: 2.4459163000648467

Epoch: 6| Step: 1
Training loss: 3.137970822939582
Validation loss: 2.465373319717271

Epoch: 6| Step: 2
Training loss: 2.6139180728942097
Validation loss: 2.4869172363267182

Epoch: 6| Step: 3
Training loss: 2.676157413967552
Validation loss: 2.4574297622172088

Epoch: 6| Step: 4
Training loss: 3.219787282374069
Validation loss: 2.456256473735941

Epoch: 6| Step: 5
Training loss: 2.35328707396489
Validation loss: 2.458702644345749

Epoch: 6| Step: 6
Training loss: 2.4210191752743095
Validation loss: 2.453114164887721

Epoch: 6| Step: 7
Training loss: 3.1433867652318828
Validation loss: 2.4843239428468813

Epoch: 6| Step: 8
Training loss: 2.334153122168615
Validation loss: 2.4564363416184767

Epoch: 6| Step: 9
Training loss: 2.4703245332775885
Validation loss: 2.445293656169476

Epoch: 6| Step: 10
Training loss: 2.6527593677496633
Validation loss: 2.4506942905409583

Epoch: 6| Step: 11
Training loss: 2.7206297766663052
Validation loss: 2.459029097322523

Epoch: 6| Step: 12
Training loss: 2.2292032209114696
Validation loss: 2.4572181233455304

Epoch: 6| Step: 13
Training loss: 2.816093692022866
Validation loss: 2.470847089999929

Epoch: 72| Step: 0
Training loss: 3.2206507135479248
Validation loss: 2.4882483187596645

Epoch: 6| Step: 1
Training loss: 2.4338458131211023
Validation loss: 2.4781106168183187

Epoch: 6| Step: 2
Training loss: 3.046466193528257
Validation loss: 2.469486236346284

Epoch: 6| Step: 3
Training loss: 2.068827085412983
Validation loss: 2.488525897862111

Epoch: 6| Step: 4
Training loss: 2.962424516319796
Validation loss: 2.4745016174840235

Epoch: 6| Step: 5
Training loss: 2.668589752889356
Validation loss: 2.477031652586447

Epoch: 6| Step: 6
Training loss: 2.8283418182383646
Validation loss: 2.4560088442276795

Epoch: 6| Step: 7
Training loss: 2.3164581241556683
Validation loss: 2.46641776059064

Epoch: 6| Step: 8
Training loss: 2.394623049894581
Validation loss: 2.4515394875713317

Epoch: 6| Step: 9
Training loss: 2.7555138190844786
Validation loss: 2.457569090771912

Epoch: 6| Step: 10
Training loss: 2.963074730601788
Validation loss: 2.425623710496635

Epoch: 6| Step: 11
Training loss: 2.9276200387322335
Validation loss: 2.4761601396139343

Epoch: 6| Step: 12
Training loss: 2.790495346833727
Validation loss: 2.4534999941456666

Epoch: 6| Step: 13
Training loss: 2.435888001903742
Validation loss: 2.4485412478090467

Epoch: 73| Step: 0
Training loss: 2.5608202383159875
Validation loss: 2.4561776247466516

Epoch: 6| Step: 1
Training loss: 3.2309322001676355
Validation loss: 2.470591837159908

Epoch: 6| Step: 2
Training loss: 2.8139647166596147
Validation loss: 2.467820719158978

Epoch: 6| Step: 3
Training loss: 2.3457948029935323
Validation loss: 2.46667231563541

Epoch: 6| Step: 4
Training loss: 2.911389636126069
Validation loss: 2.4472457727411183

Epoch: 6| Step: 5
Training loss: 2.9938582494354007
Validation loss: 2.457408804875061

Epoch: 6| Step: 6
Training loss: 3.0088152118203997
Validation loss: 2.4685231139980184

Epoch: 6| Step: 7
Training loss: 2.47774980165267
Validation loss: 2.4748771856601683

Epoch: 6| Step: 8
Training loss: 2.353190824644262
Validation loss: 2.4747860992683295

Epoch: 6| Step: 9
Training loss: 2.5143975527152795
Validation loss: 2.490785613683803

Epoch: 6| Step: 10
Training loss: 2.6001053128555105
Validation loss: 2.4781254475695875

Epoch: 6| Step: 11
Training loss: 2.7710841550572187
Validation loss: 2.469709241935734

Epoch: 6| Step: 12
Training loss: 2.942243925375027
Validation loss: 2.4487636639446233

Epoch: 6| Step: 13
Training loss: 2.2420169353406973
Validation loss: 2.489152471928814

Epoch: 74| Step: 0
Training loss: 2.8311095954489067
Validation loss: 2.487502516499293

Epoch: 6| Step: 1
Training loss: 2.706517378568925
Validation loss: 2.472851973826741

Epoch: 6| Step: 2
Training loss: 2.450861378491658
Validation loss: 2.4423382430174083

Epoch: 6| Step: 3
Training loss: 3.099370455652972
Validation loss: 2.476784631423952

Epoch: 6| Step: 4
Training loss: 3.0249506985768875
Validation loss: 2.4706423853364545

Epoch: 6| Step: 5
Training loss: 2.3528128813379623
Validation loss: 2.4842230266045395

Epoch: 6| Step: 6
Training loss: 3.277842905363778
Validation loss: 2.458572000340948

Epoch: 6| Step: 7
Training loss: 2.3138961701305916
Validation loss: 2.465874481857027

Epoch: 6| Step: 8
Training loss: 2.643408938653943
Validation loss: 2.4740813045454932

Epoch: 6| Step: 9
Training loss: 3.0948396555542756
Validation loss: 2.4693034942965775

Epoch: 6| Step: 10
Training loss: 3.3802435373002284
Validation loss: 2.4945726672105897

Epoch: 6| Step: 11
Training loss: 2.1529936596767763
Validation loss: 2.459143204003333

Epoch: 6| Step: 12
Training loss: 2.300277266168411
Validation loss: 2.4687760843539954

Epoch: 6| Step: 13
Training loss: 2.336919979816474
Validation loss: 2.436705837236518

Epoch: 75| Step: 0
Training loss: 2.705502297096779
Validation loss: 2.4623897196943547

Epoch: 6| Step: 1
Training loss: 2.7602034192518157
Validation loss: 2.472635785631668

Epoch: 6| Step: 2
Training loss: 3.2538004442394155
Validation loss: 2.456827150360975

Epoch: 6| Step: 3
Training loss: 3.1610822794366116
Validation loss: 2.4678421469658693

Epoch: 6| Step: 4
Training loss: 1.7558346620375729
Validation loss: 2.4762220453079764

Epoch: 6| Step: 5
Training loss: 2.17754956474786
Validation loss: 2.4482918725151652

Epoch: 6| Step: 6
Training loss: 2.4242014877880003
Validation loss: 2.452931426978892

Epoch: 6| Step: 7
Training loss: 2.6019000043544933
Validation loss: 2.4734062738605744

Epoch: 6| Step: 8
Training loss: 3.1439711930894143
Validation loss: 2.4829019193522446

Epoch: 6| Step: 9
Training loss: 2.4901654402223428
Validation loss: 2.482153018244774

Epoch: 6| Step: 10
Training loss: 3.133913679013793
Validation loss: 2.4744255934650825

Epoch: 6| Step: 11
Training loss: 3.012653369086066
Validation loss: 2.464301241555273

Epoch: 6| Step: 12
Training loss: 2.365154337454118
Validation loss: 2.4806173610635005

Epoch: 6| Step: 13
Training loss: 2.746809842982779
Validation loss: 2.4863230643803185

Epoch: 76| Step: 0
Training loss: 2.309307678645459
Validation loss: 2.4624400416653143

Epoch: 6| Step: 1
Training loss: 1.9462404910169506
Validation loss: 2.4968893613362124

Epoch: 6| Step: 2
Training loss: 2.8833984771895955
Validation loss: 2.47835027941128

Epoch: 6| Step: 3
Training loss: 2.5424950505661403
Validation loss: 2.429201303581027

Epoch: 6| Step: 4
Training loss: 2.325203617983119
Validation loss: 2.468567415305268

Epoch: 6| Step: 5
Training loss: 2.63955233304056
Validation loss: 2.471917223193461

Epoch: 6| Step: 6
Training loss: 2.254155983138607
Validation loss: 2.473192832314845

Epoch: 6| Step: 7
Training loss: 2.7665021119722417
Validation loss: 2.466462842884019

Epoch: 6| Step: 8
Training loss: 3.437895596022426
Validation loss: 2.456815457138958

Epoch: 6| Step: 9
Training loss: 3.062913360725383
Validation loss: 2.464548970937136

Epoch: 6| Step: 10
Training loss: 2.9490189036180294
Validation loss: 2.479872120184389

Epoch: 6| Step: 11
Training loss: 2.944231477469107
Validation loss: 2.457483666905797

Epoch: 6| Step: 12
Training loss: 2.76501906486685
Validation loss: 2.461687909664609

Epoch: 6| Step: 13
Training loss: 3.192358017454948
Validation loss: 2.4705098168748383

Epoch: 77| Step: 0
Training loss: 2.8484326051302737
Validation loss: 2.465759964087385

Epoch: 6| Step: 1
Training loss: 2.888395532230714
Validation loss: 2.455622758757612

Epoch: 6| Step: 2
Training loss: 2.5252379616885765
Validation loss: 2.4602864305464265

Epoch: 6| Step: 3
Training loss: 2.7286403138731745
Validation loss: 2.463119755493582

Epoch: 6| Step: 4
Training loss: 2.626073799304915
Validation loss: 2.4676125095576253

Epoch: 6| Step: 5
Training loss: 2.8494664679939086
Validation loss: 2.44657576120531

Epoch: 6| Step: 6
Training loss: 2.316709656167869
Validation loss: 2.47609022337077

Epoch: 6| Step: 7
Training loss: 3.232121959037632
Validation loss: 2.4641456663994177

Epoch: 6| Step: 8
Training loss: 2.685058726941827
Validation loss: 2.4569531594012286

Epoch: 6| Step: 9
Training loss: 2.1126062580900378
Validation loss: 2.4651338524762734

Epoch: 6| Step: 10
Training loss: 2.2912062673770706
Validation loss: 2.475738656469022

Epoch: 6| Step: 11
Training loss: 3.148936494799285
Validation loss: 2.470483696865469

Epoch: 6| Step: 12
Training loss: 3.3563174505577473
Validation loss: 2.4406843323194796

Epoch: 6| Step: 13
Training loss: 2.1193273787237326
Validation loss: 2.452371824836161

Epoch: 78| Step: 0
Training loss: 1.9901562676344016
Validation loss: 2.4558359850884237

Epoch: 6| Step: 1
Training loss: 2.951485314882126
Validation loss: 2.4554901295533402

Epoch: 6| Step: 2
Training loss: 3.0711193626110167
Validation loss: 2.478026026251537

Epoch: 6| Step: 3
Training loss: 3.14132454895622
Validation loss: 2.4661608569688602

Epoch: 6| Step: 4
Training loss: 2.3711898759087817
Validation loss: 2.451415380183144

Epoch: 6| Step: 5
Training loss: 2.480321107810753
Validation loss: 2.4698597957845476

Epoch: 6| Step: 6
Training loss: 2.639322173926624
Validation loss: 2.45624765638515

Epoch: 6| Step: 7
Training loss: 3.1957843310906835
Validation loss: 2.465852525440339

Epoch: 6| Step: 8
Training loss: 2.8400015806811596
Validation loss: 2.466800142427854

Epoch: 6| Step: 9
Training loss: 2.4263209726918684
Validation loss: 2.4634293968975913

Epoch: 6| Step: 10
Training loss: 3.007764147084965
Validation loss: 2.4602604281161047

Epoch: 6| Step: 11
Training loss: 2.918920971847744
Validation loss: 2.438126813944324

Epoch: 6| Step: 12
Training loss: 2.2758838822924843
Validation loss: 2.465220128129194

Epoch: 6| Step: 13
Training loss: 2.5350904176978224
Validation loss: 2.461894706365605

Epoch: 79| Step: 0
Training loss: 2.383913479956736
Validation loss: 2.4592232891296346

Epoch: 6| Step: 1
Training loss: 2.1416945187233263
Validation loss: 2.4789765308240805

Epoch: 6| Step: 2
Training loss: 2.662818774988297
Validation loss: 2.468474545525344

Epoch: 6| Step: 3
Training loss: 2.4788270819255227
Validation loss: 2.4630249951703713

Epoch: 6| Step: 4
Training loss: 3.228264532923417
Validation loss: 2.454615458718017

Epoch: 6| Step: 5
Training loss: 2.842287463023566
Validation loss: 2.4709821075398417

Epoch: 6| Step: 6
Training loss: 2.544921125527518
Validation loss: 2.467842129305951

Epoch: 6| Step: 7
Training loss: 3.3492033509384735
Validation loss: 2.4736662036240724

Epoch: 6| Step: 8
Training loss: 2.3944729024676508
Validation loss: 2.470846494443082

Epoch: 6| Step: 9
Training loss: 2.607838252393536
Validation loss: 2.4617707018683195

Epoch: 6| Step: 10
Training loss: 2.430100776874614
Validation loss: 2.487838677168123

Epoch: 6| Step: 11
Training loss: 3.084908847544162
Validation loss: 2.448153936494008

Epoch: 6| Step: 12
Training loss: 2.778950801703509
Validation loss: 2.4569609464523046

Epoch: 6| Step: 13
Training loss: 3.069739212987708
Validation loss: 2.4543502681137497

Epoch: 80| Step: 0
Training loss: 2.2950284632827174
Validation loss: 2.472577260418859

Epoch: 6| Step: 1
Training loss: 3.1660864114349203
Validation loss: 2.4764024475681774

Epoch: 6| Step: 2
Training loss: 2.709364112911328
Validation loss: 2.466308858813475

Epoch: 6| Step: 3
Training loss: 2.782807332054964
Validation loss: 2.47688671394162

Epoch: 6| Step: 4
Training loss: 2.757240819708393
Validation loss: 2.470603153881803

Epoch: 6| Step: 5
Training loss: 2.291462548153171
Validation loss: 2.4531875951412543

Epoch: 6| Step: 6
Training loss: 3.027103695119455
Validation loss: 2.471609157532993

Epoch: 6| Step: 7
Training loss: 2.31179345136887
Validation loss: 2.479659823470053

Epoch: 6| Step: 8
Training loss: 2.596725962708201
Validation loss: 2.4700326107234347

Epoch: 6| Step: 9
Training loss: 2.8455710815710504
Validation loss: 2.4783843310572578

Epoch: 6| Step: 10
Training loss: 1.9537806516696974
Validation loss: 2.4830858752165286

Epoch: 6| Step: 11
Training loss: 3.5099937267193067
Validation loss: 2.4722573685445446

Epoch: 6| Step: 12
Training loss: 2.9271067734907668
Validation loss: 2.481010855135321

Epoch: 6| Step: 13
Training loss: 2.5987526322315238
Validation loss: 2.4716726614575744

Epoch: 81| Step: 0
Training loss: 2.968790475669803
Validation loss: 2.4631139154995556

Epoch: 6| Step: 1
Training loss: 3.270322788184068
Validation loss: 2.4938349622962637

Epoch: 6| Step: 2
Training loss: 2.256893089557875
Validation loss: 2.478924687165626

Epoch: 6| Step: 3
Training loss: 1.5585330960893435
Validation loss: 2.4602844163421795

Epoch: 6| Step: 4
Training loss: 2.6607492376941297
Validation loss: 2.473734925593341

Epoch: 6| Step: 5
Training loss: 2.9656976367347667
Validation loss: 2.491753689233358

Epoch: 6| Step: 6
Training loss: 2.794575859739693
Validation loss: 2.474827775404149

Epoch: 6| Step: 7
Training loss: 2.1567064990689437
Validation loss: 2.449372433692619

Epoch: 6| Step: 8
Training loss: 2.767517390691878
Validation loss: 2.4693565833229436

Epoch: 6| Step: 9
Training loss: 3.2970705381142182
Validation loss: 2.4668593814745687

Epoch: 6| Step: 10
Training loss: 2.52203509686437
Validation loss: 2.4726096460894453

Epoch: 6| Step: 11
Training loss: 3.339017138889285
Validation loss: 2.476626637738075

Epoch: 6| Step: 12
Training loss: 2.377121429628434
Validation loss: 2.479435096934557

Epoch: 6| Step: 13
Training loss: 2.3664288705806946
Validation loss: 2.4792308511247385

Epoch: 82| Step: 0
Training loss: 3.1305681594133072
Validation loss: 2.4723932339898136

Epoch: 6| Step: 1
Training loss: 1.8861735249178624
Validation loss: 2.480353117955693

Epoch: 6| Step: 2
Training loss: 2.008136055254313
Validation loss: 2.4853037355015144

Epoch: 6| Step: 3
Training loss: 2.827120270531102
Validation loss: 2.469100347841286

Epoch: 6| Step: 4
Training loss: 3.1699328560442117
Validation loss: 2.4680321832243988

Epoch: 6| Step: 5
Training loss: 3.0356727661370893
Validation loss: 2.4563972892604498

Epoch: 6| Step: 6
Training loss: 2.4192344791458082
Validation loss: 2.4676758108998853

Epoch: 6| Step: 7
Training loss: 2.634521292067022
Validation loss: 2.4581181870204416

Epoch: 6| Step: 8
Training loss: 2.990914733946748
Validation loss: 2.4724843515071195

Epoch: 6| Step: 9
Training loss: 3.016120197966915
Validation loss: 2.479929326807276

Epoch: 6| Step: 10
Training loss: 2.3435831646349885
Validation loss: 2.476271861526803

Epoch: 6| Step: 11
Training loss: 3.1400850505608697
Validation loss: 2.469353997730248

Epoch: 6| Step: 12
Training loss: 2.6012312761299223
Validation loss: 2.460828918780551

Epoch: 6| Step: 13
Training loss: 2.282066617101998
Validation loss: 2.480200337886699

Epoch: 83| Step: 0
Training loss: 2.7230366204440397
Validation loss: 2.484382947724716

Epoch: 6| Step: 1
Training loss: 2.4790520414465766
Validation loss: 2.483996338124788

Epoch: 6| Step: 2
Training loss: 2.4254737941731515
Validation loss: 2.4728897114889685

Epoch: 6| Step: 3
Training loss: 2.9209815306531794
Validation loss: 2.478781946295029

Epoch: 6| Step: 4
Training loss: 2.1761909249847733
Validation loss: 2.4886586881828734

Epoch: 6| Step: 5
Training loss: 3.1543032241615667
Validation loss: 2.4704978470108987

Epoch: 6| Step: 6
Training loss: 3.0030495720480097
Validation loss: 2.472957336988147

Epoch: 6| Step: 7
Training loss: 2.8006557718470813
Validation loss: 2.4702173630589925

Epoch: 6| Step: 8
Training loss: 2.179535891274787
Validation loss: 2.473507201291554

Epoch: 6| Step: 9
Training loss: 3.0660110899344564
Validation loss: 2.465434933712692

Epoch: 6| Step: 10
Training loss: 2.413578910907967
Validation loss: 2.4583221857295445

Epoch: 6| Step: 11
Training loss: 2.6957605846402064
Validation loss: 2.471307381386861

Epoch: 6| Step: 12
Training loss: 2.338452672362412
Validation loss: 2.479476967889241

Epoch: 6| Step: 13
Training loss: 3.465509911871525
Validation loss: 2.460712090876323

Epoch: 84| Step: 0
Training loss: 2.545307730335991
Validation loss: 2.4880317610176843

Epoch: 6| Step: 1
Training loss: 2.9680386945973183
Validation loss: 2.4797531122660947

Epoch: 6| Step: 2
Training loss: 2.930945205296024
Validation loss: 2.4647655782463973

Epoch: 6| Step: 3
Training loss: 2.4169438521050743
Validation loss: 2.455883036222197

Epoch: 6| Step: 4
Training loss: 3.069627680614575
Validation loss: 2.4762074889020402

Epoch: 6| Step: 5
Training loss: 2.1215048825335683
Validation loss: 2.4700821999296045

Epoch: 6| Step: 6
Training loss: 2.7554553281166765
Validation loss: 2.4720116758345267

Epoch: 6| Step: 7
Training loss: 2.6839164084594094
Validation loss: 2.4621870804545356

Epoch: 6| Step: 8
Training loss: 2.604753158647969
Validation loss: 2.495061916055964

Epoch: 6| Step: 9
Training loss: 2.684709475266039
Validation loss: 2.4715578553352313

Epoch: 6| Step: 10
Training loss: 2.860933473210233
Validation loss: 2.487582965509615

Epoch: 6| Step: 11
Training loss: 3.2344994267657627
Validation loss: 2.4850921080898534

Epoch: 6| Step: 12
Training loss: 2.29866331898489
Validation loss: 2.495764450455815

Epoch: 6| Step: 13
Training loss: 2.8450808921536748
Validation loss: 2.472064956628787

Epoch: 85| Step: 0
Training loss: 3.0472480741146284
Validation loss: 2.4501533753698665

Epoch: 6| Step: 1
Training loss: 2.979371515091558
Validation loss: 2.4715689456619727

Epoch: 6| Step: 2
Training loss: 2.33632888832201
Validation loss: 2.462110141517286

Epoch: 6| Step: 3
Training loss: 2.5794347037302776
Validation loss: 2.4745356713263256

Epoch: 6| Step: 4
Training loss: 2.0764906343377008
Validation loss: 2.472286160705222

Epoch: 6| Step: 5
Training loss: 3.2301661431539292
Validation loss: 2.468506733175947

Epoch: 6| Step: 6
Training loss: 2.801281489306336
Validation loss: 2.4779476734049335

Epoch: 6| Step: 7
Training loss: 2.435161740612293
Validation loss: 2.4731733312226574

Epoch: 6| Step: 8
Training loss: 2.696674213029498
Validation loss: 2.469977934716655

Epoch: 6| Step: 9
Training loss: 2.6026239835758127
Validation loss: 2.470238533455579

Epoch: 6| Step: 10
Training loss: 3.535100137676221
Validation loss: 2.462367244956699

Epoch: 6| Step: 11
Training loss: 2.6123636456547685
Validation loss: 2.4560096427528038

Epoch: 6| Step: 12
Training loss: 2.4386415987153307
Validation loss: 2.4612866788267933

Epoch: 6| Step: 13
Training loss: 1.9649647851753909
Validation loss: 2.4822125116042533

Epoch: 86| Step: 0
Training loss: 3.0137664918515545
Validation loss: 2.463335857801634

Epoch: 6| Step: 1
Training loss: 2.254934621738606
Validation loss: 2.46205587163704

Epoch: 6| Step: 2
Training loss: 2.7546093198833095
Validation loss: 2.445183754345708

Epoch: 6| Step: 3
Training loss: 3.169980540411394
Validation loss: 2.4622911331249213

Epoch: 6| Step: 4
Training loss: 2.7413320918909125
Validation loss: 2.4703985431568607

Epoch: 6| Step: 5
Training loss: 2.6208267644447116
Validation loss: 2.4873303072350534

Epoch: 6| Step: 6
Training loss: 2.5801785700014026
Validation loss: 2.480844098922161

Epoch: 6| Step: 7
Training loss: 2.541990405451018
Validation loss: 2.4591136533069937

Epoch: 6| Step: 8
Training loss: 2.4210592557327644
Validation loss: 2.463870545173959

Epoch: 6| Step: 9
Training loss: 2.4310288237946422
Validation loss: 2.4498604315958494

Epoch: 6| Step: 10
Training loss: 2.578705508448803
Validation loss: 2.466028967620464

Epoch: 6| Step: 11
Training loss: 2.691049051042583
Validation loss: 2.467690351143975

Epoch: 6| Step: 12
Training loss: 2.930317152389213
Validation loss: 2.469354169549481

Epoch: 6| Step: 13
Training loss: 3.1216632576465746
Validation loss: 2.4691066782834374

Epoch: 87| Step: 0
Training loss: 2.3011099251920544
Validation loss: 2.4552223139142377

Epoch: 6| Step: 1
Training loss: 3.230581519154763
Validation loss: 2.4607675197095857

Epoch: 6| Step: 2
Training loss: 2.5997740867475434
Validation loss: 2.4515880323448127

Epoch: 6| Step: 3
Training loss: 2.768375386885753
Validation loss: 2.475444400135435

Epoch: 6| Step: 4
Training loss: 2.672600006430547
Validation loss: 2.4632596260306734

Epoch: 6| Step: 5
Training loss: 2.735568412838692
Validation loss: 2.4507152843808435

Epoch: 6| Step: 6
Training loss: 2.7847490909217743
Validation loss: 2.460363858933788

Epoch: 6| Step: 7
Training loss: 2.571597627350482
Validation loss: 2.461242990174801

Epoch: 6| Step: 8
Training loss: 2.877731518351117
Validation loss: 2.469392697244943

Epoch: 6| Step: 9
Training loss: 2.9409472752073125
Validation loss: 2.4729529497968956

Epoch: 6| Step: 10
Training loss: 1.979939286524555
Validation loss: 2.458797561865556

Epoch: 6| Step: 11
Training loss: 2.0308431364607333
Validation loss: 2.4829442635821017

Epoch: 6| Step: 12
Training loss: 3.146346919764914
Validation loss: 2.476200732464348

Epoch: 6| Step: 13
Training loss: 3.0890393084059244
Validation loss: 2.4795200416272047

Epoch: 88| Step: 0
Training loss: 2.208790629800653
Validation loss: 2.489202844026378

Epoch: 6| Step: 1
Training loss: 3.019603891226813
Validation loss: 2.4885541474192276

Epoch: 6| Step: 2
Training loss: 2.76886004020449
Validation loss: 2.467680731072094

Epoch: 6| Step: 3
Training loss: 2.2012798617935303
Validation loss: 2.4543961569881327

Epoch: 6| Step: 4
Training loss: 2.996398830672181
Validation loss: 2.46056256515283

Epoch: 6| Step: 5
Training loss: 2.969807004983756
Validation loss: 2.477395045549903

Epoch: 6| Step: 6
Training loss: 3.0285905005839022
Validation loss: 2.4671482213383387

Epoch: 6| Step: 7
Training loss: 3.424326675945762
Validation loss: 2.467822521524459

Epoch: 6| Step: 8
Training loss: 2.8149244243443743
Validation loss: 2.480985293107401

Epoch: 6| Step: 9
Training loss: 2.0029419480594397
Validation loss: 2.4853573915007985

Epoch: 6| Step: 10
Training loss: 2.4508063176739383
Validation loss: 2.467095463035072

Epoch: 6| Step: 11
Training loss: 2.6883391910906185
Validation loss: 2.4717097839189655

Epoch: 6| Step: 12
Training loss: 2.2586754862280243
Validation loss: 2.4628260858248856

Epoch: 6| Step: 13
Training loss: 2.818754658832633
Validation loss: 2.4719312889060463

Epoch: 89| Step: 0
Training loss: 2.9963482247597653
Validation loss: 2.479738871747893

Epoch: 6| Step: 1
Training loss: 2.9111565630845138
Validation loss: 2.4662715495498073

Epoch: 6| Step: 2
Training loss: 2.076697066907602
Validation loss: 2.464270806017471

Epoch: 6| Step: 3
Training loss: 2.9170214482651087
Validation loss: 2.4795222718049295

Epoch: 6| Step: 4
Training loss: 3.1563729743795417
Validation loss: 2.475443091100734

Epoch: 6| Step: 5
Training loss: 3.2781955125932654
Validation loss: 2.4616740192217406

Epoch: 6| Step: 6
Training loss: 2.2466805661768583
Validation loss: 2.4775434172535924

Epoch: 6| Step: 7
Training loss: 2.501004398761528
Validation loss: 2.4640156605730517

Epoch: 6| Step: 8
Training loss: 2.536629411905875
Validation loss: 2.4873096141944107

Epoch: 6| Step: 9
Training loss: 2.856080205031582
Validation loss: 2.4796642530730124

Epoch: 6| Step: 10
Training loss: 2.75588091673566
Validation loss: 2.4484304358042537

Epoch: 6| Step: 11
Training loss: 2.433710331213911
Validation loss: 2.4762633639144167

Epoch: 6| Step: 12
Training loss: 2.286398942620665
Validation loss: 2.4847722230318134

Epoch: 6| Step: 13
Training loss: 2.7939527365496875
Validation loss: 2.48131775147083

Epoch: 90| Step: 0
Training loss: 2.4294441322047122
Validation loss: 2.497708593347354

Epoch: 6| Step: 1
Training loss: 2.558796229613417
Validation loss: 2.4548849931700607

Epoch: 6| Step: 2
Training loss: 2.504959717995828
Validation loss: 2.476701721603091

Epoch: 6| Step: 3
Training loss: 3.229415555817077
Validation loss: 2.4658967977930755

Epoch: 6| Step: 4
Training loss: 1.7115240615726923
Validation loss: 2.489816152675676

Epoch: 6| Step: 5
Training loss: 2.7324560789049346
Validation loss: 2.4643951809230824

Epoch: 6| Step: 6
Training loss: 2.730963447954758
Validation loss: 2.4778566843653937

Epoch: 6| Step: 7
Training loss: 2.597632478304059
Validation loss: 2.469038454636419

Epoch: 6| Step: 8
Training loss: 3.38303295236495
Validation loss: 2.4609893048536104

Epoch: 6| Step: 9
Training loss: 2.6044026585779343
Validation loss: 2.455448046374439

Epoch: 6| Step: 10
Training loss: 2.407497924613652
Validation loss: 2.4740587474260876

Epoch: 6| Step: 11
Training loss: 3.2988128029358497
Validation loss: 2.492183945939677

Epoch: 6| Step: 12
Training loss: 2.5828548624080474
Validation loss: 2.4540651039418133

Epoch: 6| Step: 13
Training loss: 2.219883911087432
Validation loss: 2.487834443997867

Epoch: 91| Step: 0
Training loss: 3.048849708143496
Validation loss: 2.4557630145368474

Epoch: 6| Step: 1
Training loss: 2.4505253520495556
Validation loss: 2.462258455039933

Epoch: 6| Step: 2
Training loss: 2.7156570204415247
Validation loss: 2.4712358158508754

Epoch: 6| Step: 3
Training loss: 2.5553729802947376
Validation loss: 2.477803878855781

Epoch: 6| Step: 4
Training loss: 2.584550560519959
Validation loss: 2.4621188379149057

Epoch: 6| Step: 5
Training loss: 2.6308301939616796
Validation loss: 2.485686986825245

Epoch: 6| Step: 6
Training loss: 2.8141698330707112
Validation loss: 2.4719231813613005

Epoch: 6| Step: 7
Training loss: 1.86330108312132
Validation loss: 2.4703765076079054

Epoch: 6| Step: 8
Training loss: 2.8533037278015003
Validation loss: 2.4604035349844136

Epoch: 6| Step: 9
Training loss: 3.0936072730847735
Validation loss: 2.4750701961877417

Epoch: 6| Step: 10
Training loss: 2.52926284590405
Validation loss: 2.4733133608486484

Epoch: 6| Step: 11
Training loss: 2.639554500848895
Validation loss: 2.4582965838350104

Epoch: 6| Step: 12
Training loss: 3.081910105597958
Validation loss: 2.4535048267628627

Epoch: 6| Step: 13
Training loss: 2.804389789050057
Validation loss: 2.465329257378101

Epoch: 92| Step: 0
Training loss: 3.1672216397568764
Validation loss: 2.481254252628017

Epoch: 6| Step: 1
Training loss: 3.005253959498928
Validation loss: 2.4548236772727736

Epoch: 6| Step: 2
Training loss: 2.4396798583583252
Validation loss: 2.4854264400346304

Epoch: 6| Step: 3
Training loss: 2.847635821150145
Validation loss: 2.487548260656828

Epoch: 6| Step: 4
Training loss: 2.625080470713644
Validation loss: 2.4766234640149527

Epoch: 6| Step: 5
Training loss: 2.8861478207403137
Validation loss: 2.4727996658610016

Epoch: 6| Step: 6
Training loss: 2.5676216487425303
Validation loss: 2.4817938785102593

Epoch: 6| Step: 7
Training loss: 2.4080111511390494
Validation loss: 2.46556067023386

Epoch: 6| Step: 8
Training loss: 2.153328616586879
Validation loss: 2.446117107427759

Epoch: 6| Step: 9
Training loss: 2.785839451783042
Validation loss: 2.465408630996322

Epoch: 6| Step: 10
Training loss: 3.079437131105877
Validation loss: 2.4708230933030966

Epoch: 6| Step: 11
Training loss: 2.2011387305484873
Validation loss: 2.496494672160237

Epoch: 6| Step: 12
Training loss: 2.4749077404050532
Validation loss: 2.498250630219799

Epoch: 6| Step: 13
Training loss: 3.003124676088123
Validation loss: 2.4803003841998814

Epoch: 93| Step: 0
Training loss: 2.3648378904564322
Validation loss: 2.458676685641741

Epoch: 6| Step: 1
Training loss: 3.026176375226582
Validation loss: 2.4814822350429124

Epoch: 6| Step: 2
Training loss: 3.052545523339186
Validation loss: 2.4658262053453486

Epoch: 6| Step: 3
Training loss: 2.80148157703906
Validation loss: 2.4764075502007024

Epoch: 6| Step: 4
Training loss: 2.7430903586796678
Validation loss: 2.4715645653415224

Epoch: 6| Step: 5
Training loss: 2.8947598962915273
Validation loss: 2.474755925274833

Epoch: 6| Step: 6
Training loss: 2.517000473907511
Validation loss: 2.4924479705359586

Epoch: 6| Step: 7
Training loss: 2.7864732022943435
Validation loss: 2.4928360830985996

Epoch: 6| Step: 8
Training loss: 2.5999035744025982
Validation loss: 2.4787687535706677

Epoch: 6| Step: 9
Training loss: 2.618258766587259
Validation loss: 2.475409920717482

Epoch: 6| Step: 10
Training loss: 3.210159690188516
Validation loss: 2.477293824281844

Epoch: 6| Step: 11
Training loss: 2.459328358461197
Validation loss: 2.4783165903817728

Epoch: 6| Step: 12
Training loss: 2.2971072955317213
Validation loss: 2.4682641091837296

Epoch: 6| Step: 13
Training loss: 2.0347704384406855
Validation loss: 2.4686638560179563

Epoch: 94| Step: 0
Training loss: 2.4129675693727686
Validation loss: 2.476604163840101

Epoch: 6| Step: 1
Training loss: 2.8947559429067833
Validation loss: 2.467155385478112

Epoch: 6| Step: 2
Training loss: 2.8051872127024473
Validation loss: 2.4732580297152396

Epoch: 6| Step: 3
Training loss: 2.1888490059516927
Validation loss: 2.4906614048359805

Epoch: 6| Step: 4
Training loss: 2.6826896199438255
Validation loss: 2.4793646200775235

Epoch: 6| Step: 5
Training loss: 2.5357166958035657
Validation loss: 2.47689568759549

Epoch: 6| Step: 6
Training loss: 3.2950353325901647
Validation loss: 2.4602375223252615

Epoch: 6| Step: 7
Training loss: 2.709896360435535
Validation loss: 2.4775021505209396

Epoch: 6| Step: 8
Training loss: 3.1028120084730104
Validation loss: 2.478470042024073

Epoch: 6| Step: 9
Training loss: 2.586475696625785
Validation loss: 2.449529580314208

Epoch: 6| Step: 10
Training loss: 2.092586706591455
Validation loss: 2.4700676488561313

Epoch: 6| Step: 11
Training loss: 2.756734750899405
Validation loss: 2.4794661517761942

Epoch: 6| Step: 12
Training loss: 2.988926637057428
Validation loss: 2.469282760238492

Epoch: 6| Step: 13
Training loss: 2.2041569450563046
Validation loss: 2.4891372887265235

Epoch: 95| Step: 0
Training loss: 2.2197730701036917
Validation loss: 2.489200095210366

Epoch: 6| Step: 1
Training loss: 3.0948324140247783
Validation loss: 2.479362059914553

Epoch: 6| Step: 2
Training loss: 2.2977117973391685
Validation loss: 2.472935127270741

Epoch: 6| Step: 3
Training loss: 2.190482694653713
Validation loss: 2.4848410819996936

Epoch: 6| Step: 4
Training loss: 2.497920888388669
Validation loss: 2.4711430437604296

Epoch: 6| Step: 5
Training loss: 2.4593128472779004
Validation loss: 2.4725686873806443

Epoch: 6| Step: 6
Training loss: 2.8912318237102776
Validation loss: 2.4768021240122033

Epoch: 6| Step: 7
Training loss: 2.637039458939079
Validation loss: 2.4835252929491722

Epoch: 6| Step: 8
Training loss: 2.708094884694866
Validation loss: 2.499008723590025

Epoch: 6| Step: 9
Training loss: 2.969849232401504
Validation loss: 2.4813503665906227

Epoch: 6| Step: 10
Training loss: 2.685299171815103
Validation loss: 2.4895333935419313

Epoch: 6| Step: 11
Training loss: 2.8239278260969054
Validation loss: 2.4903960440698674

Epoch: 6| Step: 12
Training loss: 2.972423970474923
Validation loss: 2.4807630448816385

Epoch: 6| Step: 13
Training loss: 3.16879694375322
Validation loss: 2.4849057813713653

Epoch: 96| Step: 0
Training loss: 2.505279307374526
Validation loss: 2.4590651888124366

Epoch: 6| Step: 1
Training loss: 2.407422263216442
Validation loss: 2.4693228131200624

Epoch: 6| Step: 2
Training loss: 2.511624395280136
Validation loss: 2.4708880596086633

Epoch: 6| Step: 3
Training loss: 2.7223272357416355
Validation loss: 2.4638814672072447

Epoch: 6| Step: 4
Training loss: 1.9262885301859323
Validation loss: 2.481161849449969

Epoch: 6| Step: 5
Training loss: 2.7007652964188376
Validation loss: 2.479899825316716

Epoch: 6| Step: 6
Training loss: 2.624297638434477
Validation loss: 2.4834486537826526

Epoch: 6| Step: 7
Training loss: 3.3134771381336074
Validation loss: 2.4732613585738794

Epoch: 6| Step: 8
Training loss: 3.336046132232994
Validation loss: 2.4721026228243703

Epoch: 6| Step: 9
Training loss: 2.4743797719610248
Validation loss: 2.475916463956943

Epoch: 6| Step: 10
Training loss: 3.0179592447429915
Validation loss: 2.4765783223836224

Epoch: 6| Step: 11
Training loss: 2.0038762675936845
Validation loss: 2.487495205353566

Epoch: 6| Step: 12
Training loss: 2.9495293257930797
Validation loss: 2.467496691667698

Epoch: 6| Step: 13
Training loss: 2.8865717332953915
Validation loss: 2.4774850545378393

Epoch: 97| Step: 0
Training loss: 2.9326375121569255
Validation loss: 2.4773993100106657

Epoch: 6| Step: 1
Training loss: 3.004677622635546
Validation loss: 2.474832657532319

Epoch: 6| Step: 2
Training loss: 2.5457261191443687
Validation loss: 2.479959667346244

Epoch: 6| Step: 3
Training loss: 2.8636347824356534
Validation loss: 2.458318526395655

Epoch: 6| Step: 4
Training loss: 2.733593463924946
Validation loss: 2.469368963607812

Epoch: 6| Step: 5
Training loss: 2.7371123644754083
Validation loss: 2.4883682344725413

Epoch: 6| Step: 6
Training loss: 2.731413714567413
Validation loss: 2.4811780165348956

Epoch: 6| Step: 7
Training loss: 2.531112102001883
Validation loss: 2.4702495970145932

Epoch: 6| Step: 8
Training loss: 2.775751486397783
Validation loss: 2.4637648558472462

Epoch: 6| Step: 9
Training loss: 2.8414176347036153
Validation loss: 2.4713112497071354

Epoch: 6| Step: 10
Training loss: 2.0447259934836586
Validation loss: 2.4761815025023197

Epoch: 6| Step: 11
Training loss: 2.613695416895359
Validation loss: 2.4749577705639654

Epoch: 6| Step: 12
Training loss: 2.4417513427980646
Validation loss: 2.49185093217456

Epoch: 6| Step: 13
Training loss: 2.9307857641966386
Validation loss: 2.4659714801156007

Epoch: 98| Step: 0
Training loss: 2.782164948230751
Validation loss: 2.4983513918356457

Epoch: 6| Step: 1
Training loss: 2.567615334539921
Validation loss: 2.487232476809771

Epoch: 6| Step: 2
Training loss: 3.6513331029851384
Validation loss: 2.469925604288524

Epoch: 6| Step: 3
Training loss: 2.8671663787645922
Validation loss: 2.4583764556500918

Epoch: 6| Step: 4
Training loss: 2.7255824385278506
Validation loss: 2.480631015215644

Epoch: 6| Step: 5
Training loss: 2.1410606664893668
Validation loss: 2.492750672748643

Epoch: 6| Step: 6
Training loss: 2.6612170284839363
Validation loss: 2.46414535012477

Epoch: 6| Step: 7
Training loss: 2.3595574921015836
Validation loss: 2.502767959429544

Epoch: 6| Step: 8
Training loss: 2.5853584309632454
Validation loss: 2.4723914940600804

Epoch: 6| Step: 9
Training loss: 2.501344128714869
Validation loss: 2.4648294197028546

Epoch: 6| Step: 10
Training loss: 2.9481692436218934
Validation loss: 2.467484494198274

Epoch: 6| Step: 11
Training loss: 2.7899147248994356
Validation loss: 2.473895862526779

Epoch: 6| Step: 12
Training loss: 2.4186875564439947
Validation loss: 2.4871952381580074

Epoch: 6| Step: 13
Training loss: 2.273260365300474
Validation loss: 2.457654020356959

Epoch: 99| Step: 0
Training loss: 2.8237735719275294
Validation loss: 2.4731467200120147

Epoch: 6| Step: 1
Training loss: 2.421793880949954
Validation loss: 2.4949856728775712

Epoch: 6| Step: 2
Training loss: 3.0196932691210865
Validation loss: 2.4935414936054916

Epoch: 6| Step: 3
Training loss: 2.9456602341559845
Validation loss: 2.4685807020114847

Epoch: 6| Step: 4
Training loss: 2.888138975313109
Validation loss: 2.4789767081812872

Epoch: 6| Step: 5
Training loss: 2.538483820775189
Validation loss: 2.4849382400406217

Epoch: 6| Step: 6
Training loss: 2.6857116426727368
Validation loss: 2.475637705064796

Epoch: 6| Step: 7
Training loss: 2.5654482744615974
Validation loss: 2.47495854329457

Epoch: 6| Step: 8
Training loss: 3.220556548457351
Validation loss: 2.464510863706065

Epoch: 6| Step: 9
Training loss: 2.9130009821986054
Validation loss: 2.484672630201253

Epoch: 6| Step: 10
Training loss: 2.4449879976379743
Validation loss: 2.4803579406301295

Epoch: 6| Step: 11
Training loss: 2.097020683401227
Validation loss: 2.4791247546226893

Epoch: 6| Step: 12
Training loss: 2.1503837509114696
Validation loss: 2.468795469609626

Epoch: 6| Step: 13
Training loss: 2.4521086707436086
Validation loss: 2.472046760584214

Epoch: 100| Step: 0
Training loss: 2.6238720604945183
Validation loss: 2.4769540290081724

Epoch: 6| Step: 1
Training loss: 2.725658890046166
Validation loss: 2.4740890801707582

Epoch: 6| Step: 2
Training loss: 2.320804530934668
Validation loss: 2.4786930977943413

Epoch: 6| Step: 3
Training loss: 2.60511047553692
Validation loss: 2.4666455646544603

Epoch: 6| Step: 4
Training loss: 3.0950542744698457
Validation loss: 2.4688926708512358

Epoch: 6| Step: 5
Training loss: 1.7188287890321885
Validation loss: 2.502573749635298

Epoch: 6| Step: 6
Training loss: 2.8300548734163886
Validation loss: 2.5064234781176875

Epoch: 6| Step: 7
Training loss: 2.527322430056424
Validation loss: 2.4912529532118257

Epoch: 6| Step: 8
Training loss: 2.3232180709196473
Validation loss: 2.470808731280616

Epoch: 6| Step: 9
Training loss: 2.9540704091849057
Validation loss: 2.454733798687401

Epoch: 6| Step: 10
Training loss: 3.1421230406293152
Validation loss: 2.479081563366999

Epoch: 6| Step: 11
Training loss: 2.6444077343178045
Validation loss: 2.464753743784964

Epoch: 6| Step: 12
Training loss: 2.8152237313022916
Validation loss: 2.48506989953588

Epoch: 6| Step: 13
Training loss: 3.30817303465364
Validation loss: 2.4689315982486266

Epoch: 101| Step: 0
Training loss: 2.4862694385012247
Validation loss: 2.470673998952409

Epoch: 6| Step: 1
Training loss: 1.7407032070705193
Validation loss: 2.488326831483267

Epoch: 6| Step: 2
Training loss: 2.771192388765926
Validation loss: 2.474687374619135

Epoch: 6| Step: 3
Training loss: 2.4717151845781737
Validation loss: 2.4644296584078185

Epoch: 6| Step: 4
Training loss: 2.6999749147168197
Validation loss: 2.449750226054442

Epoch: 6| Step: 5
Training loss: 2.1694785969230774
Validation loss: 2.494867725502972

Epoch: 6| Step: 6
Training loss: 3.4443560113800493
Validation loss: 2.4810607177947723

Epoch: 6| Step: 7
Training loss: 2.458508264384597
Validation loss: 2.467989050260129

Epoch: 6| Step: 8
Training loss: 3.0984547609280306
Validation loss: 2.474872062262619

Epoch: 6| Step: 9
Training loss: 2.303985178343465
Validation loss: 2.475547002897301

Epoch: 6| Step: 10
Training loss: 2.8795417073973963
Validation loss: 2.4733041420158606

Epoch: 6| Step: 11
Training loss: 2.9397768976587004
Validation loss: 2.4920291002285953

Epoch: 6| Step: 12
Training loss: 3.1860527792256863
Validation loss: 2.4473124962660227

Epoch: 6| Step: 13
Training loss: 2.19189197508692
Validation loss: 2.4968187600703424

Epoch: 102| Step: 0
Training loss: 2.7116959360141677
Validation loss: 2.470809344485276

Epoch: 6| Step: 1
Training loss: 1.3126600258773697
Validation loss: 2.4896314465893683

Epoch: 6| Step: 2
Training loss: 3.1439558746697
Validation loss: 2.483557294827771

Epoch: 6| Step: 3
Training loss: 2.6358413253632467
Validation loss: 2.4699242020278747

Epoch: 6| Step: 4
Training loss: 2.937617198148262
Validation loss: 2.4802654854545803

Epoch: 6| Step: 5
Training loss: 1.9812549482698953
Validation loss: 2.4732549014236347

Epoch: 6| Step: 6
Training loss: 2.7699246362562917
Validation loss: 2.458290205741102

Epoch: 6| Step: 7
Training loss: 3.1265920017114506
Validation loss: 2.4724325739097583

Epoch: 6| Step: 8
Training loss: 2.8431167735921874
Validation loss: 2.488231729854467

Epoch: 6| Step: 9
Training loss: 2.5181334884019506
Validation loss: 2.4738034218094005

Epoch: 6| Step: 10
Training loss: 2.9557060789755414
Validation loss: 2.4776598027998777

Epoch: 6| Step: 11
Training loss: 1.941220740928655
Validation loss: 2.4955674489805775

Epoch: 6| Step: 12
Training loss: 3.461630723815931
Validation loss: 2.458945313065494

Epoch: 6| Step: 13
Training loss: 2.0637053233347196
Validation loss: 2.502496043467431

Epoch: 103| Step: 0
Training loss: 2.6115546345817693
Validation loss: 2.4702352176473212

Epoch: 6| Step: 1
Training loss: 2.26760167677261
Validation loss: 2.4960244614466336

Epoch: 6| Step: 2
Training loss: 2.738183557179843
Validation loss: 2.478688665938718

Epoch: 6| Step: 3
Training loss: 2.133172197018175
Validation loss: 2.4883700858305695

Epoch: 6| Step: 4
Training loss: 2.69601051027417
Validation loss: 2.4669349377220917

Epoch: 6| Step: 5
Training loss: 2.576826658755389
Validation loss: 2.492142320366722

Epoch: 6| Step: 6
Training loss: 2.9613204346086333
Validation loss: 2.487537036483374

Epoch: 6| Step: 7
Training loss: 2.232401583254782
Validation loss: 2.487526783082973

Epoch: 6| Step: 8
Training loss: 2.9712128712890418
Validation loss: 2.503259254468405

Epoch: 6| Step: 9
Training loss: 2.759011502370884
Validation loss: 2.467646706259954

Epoch: 6| Step: 10
Training loss: 3.0350145382980727
Validation loss: 2.460313269426187

Epoch: 6| Step: 11
Training loss: 2.868140953420268
Validation loss: 2.481490271581691

Epoch: 6| Step: 12
Training loss: 3.0927066777705514
Validation loss: 2.504823409309658

Epoch: 6| Step: 13
Training loss: 2.215524827766814
Validation loss: 2.4804740266854424

Epoch: 104| Step: 0
Training loss: 2.3405908273970413
Validation loss: 2.494443644927973

Epoch: 6| Step: 1
Training loss: 2.7380338766595105
Validation loss: 2.5075279659414256

Epoch: 6| Step: 2
Training loss: 2.52208217445833
Validation loss: 2.4877003771708717

Epoch: 6| Step: 3
Training loss: 2.1718855384186773
Validation loss: 2.4972045548708746

Epoch: 6| Step: 4
Training loss: 2.5374387273296657
Validation loss: 2.479905184363303

Epoch: 6| Step: 5
Training loss: 2.6340722938470473
Validation loss: 2.493083003850091

Epoch: 6| Step: 6
Training loss: 2.719326177468704
Validation loss: 2.4730946567012655

Epoch: 6| Step: 7
Training loss: 2.766021786259864
Validation loss: 2.4947992496365385

Epoch: 6| Step: 8
Training loss: 2.7495260697242463
Validation loss: 2.473917006626166

Epoch: 6| Step: 9
Training loss: 3.098576181821777
Validation loss: 2.4802427892234857

Epoch: 6| Step: 10
Training loss: 3.48897409766989
Validation loss: 2.4719995939796227

Epoch: 6| Step: 11
Training loss: 2.28653866938849
Validation loss: 2.456172890282412

Epoch: 6| Step: 12
Training loss: 2.9147140142789616
Validation loss: 2.485476808224579

Epoch: 6| Step: 13
Training loss: 2.295953267236612
Validation loss: 2.472385956969229

Epoch: 105| Step: 0
Training loss: 2.599604972761111
Validation loss: 2.473018764104959

Epoch: 6| Step: 1
Training loss: 3.1800367458787577
Validation loss: 2.485929490254643

Epoch: 6| Step: 2
Training loss: 2.411918601704369
Validation loss: 2.4925935244570394

Epoch: 6| Step: 3
Training loss: 2.267500318297981
Validation loss: 2.493652325758835

Epoch: 6| Step: 4
Training loss: 2.880177024592961
Validation loss: 2.459563007380157

Epoch: 6| Step: 5
Training loss: 2.904854059804838
Validation loss: 2.4752645417053096

Epoch: 6| Step: 6
Training loss: 2.7482436380117523
Validation loss: 2.4732434392796314

Epoch: 6| Step: 7
Training loss: 2.4929307170227353
Validation loss: 2.4849170236200733

Epoch: 6| Step: 8
Training loss: 1.87657163238209
Validation loss: 2.4761266816928704

Epoch: 6| Step: 9
Training loss: 2.9374725665170947
Validation loss: 2.482318103306231

Epoch: 6| Step: 10
Training loss: 3.100218820539849
Validation loss: 2.480589185443723

Epoch: 6| Step: 11
Training loss: 3.0526803293160527
Validation loss: 2.503944308503779

Epoch: 6| Step: 12
Training loss: 2.436291517331593
Validation loss: 2.4706179721095896

Epoch: 6| Step: 13
Training loss: 2.3363990965528134
Validation loss: 2.4806874654286184

Epoch: 106| Step: 0
Training loss: 2.361728530141737
Validation loss: 2.459445539787943

Epoch: 6| Step: 1
Training loss: 2.6560602456923976
Validation loss: 2.4712535863145075

Epoch: 6| Step: 2
Training loss: 2.153138057408929
Validation loss: 2.4596043546460193

Epoch: 6| Step: 3
Training loss: 3.4015680736238503
Validation loss: 2.5035567700846357

Epoch: 6| Step: 4
Training loss: 2.0504191923704083
Validation loss: 2.484047206955601

Epoch: 6| Step: 5
Training loss: 2.399232415794176
Validation loss: 2.4705366541678386

Epoch: 6| Step: 6
Training loss: 2.142371919736245
Validation loss: 2.4952634045602142

Epoch: 6| Step: 7
Training loss: 2.8550285281812013
Validation loss: 2.491470727086945

Epoch: 6| Step: 8
Training loss: 3.173121165253909
Validation loss: 2.5026952591583096

Epoch: 6| Step: 9
Training loss: 2.6007636966038477
Validation loss: 2.44740532164365

Epoch: 6| Step: 10
Training loss: 2.675818138146735
Validation loss: 2.4817026296772955

Epoch: 6| Step: 11
Training loss: 2.6081349058715007
Validation loss: 2.4822521307074212

Epoch: 6| Step: 12
Training loss: 3.235299535736606
Validation loss: 2.507759339784341

Epoch: 6| Step: 13
Training loss: 2.9357528360465683
Validation loss: 2.472366714963641

Epoch: 107| Step: 0
Training loss: 2.6780954864549384
Validation loss: 2.48995372872541

Epoch: 6| Step: 1
Training loss: 2.1955010275476736
Validation loss: 2.48157421822844

Epoch: 6| Step: 2
Training loss: 1.6600060249127357
Validation loss: 2.4914631266121603

Epoch: 6| Step: 3
Training loss: 2.858020565911971
Validation loss: 2.494289533569992

Epoch: 6| Step: 4
Training loss: 2.4870023450054286
Validation loss: 2.486658559727861

Epoch: 6| Step: 5
Training loss: 2.743161628914905
Validation loss: 2.4528154666838855

Epoch: 6| Step: 6
Training loss: 2.6044590493415836
Validation loss: 2.4873215000621784

Epoch: 6| Step: 7
Training loss: 2.5502280413816556
Validation loss: 2.5078576733370688

Epoch: 6| Step: 8
Training loss: 3.2404041947097855
Validation loss: 2.4752328329156925

Epoch: 6| Step: 9
Training loss: 2.341499977921513
Validation loss: 2.4937219290076467

Epoch: 6| Step: 10
Training loss: 3.156656673406179
Validation loss: 2.4747032576807655

Epoch: 6| Step: 11
Training loss: 2.9483974500563073
Validation loss: 2.4824970019994823

Epoch: 6| Step: 12
Training loss: 3.069553582354654
Validation loss: 2.4762890523311127

Epoch: 6| Step: 13
Training loss: 2.8256910609754238
Validation loss: 2.48213315788861

Epoch: 108| Step: 0
Training loss: 2.7919848436310377
Validation loss: 2.4905975914647196

Epoch: 6| Step: 1
Training loss: 2.3934803783472307
Validation loss: 2.484532699073829

Epoch: 6| Step: 2
Training loss: 2.9870141627892584
Validation loss: 2.520424106799103

Epoch: 6| Step: 3
Training loss: 2.8535060999905197
Validation loss: 2.486298247810167

Epoch: 6| Step: 4
Training loss: 2.908137211203173
Validation loss: 2.4872002042370918

Epoch: 6| Step: 5
Training loss: 2.848338690303127
Validation loss: 2.485265014000203

Epoch: 6| Step: 6
Training loss: 2.22801894354172
Validation loss: 2.4880981800384743

Epoch: 6| Step: 7
Training loss: 2.099719065085307
Validation loss: 2.48686831991323

Epoch: 6| Step: 8
Training loss: 2.703782194653987
Validation loss: 2.479044797933022

Epoch: 6| Step: 9
Training loss: 2.242028739173308
Validation loss: 2.4756763441501306

Epoch: 6| Step: 10
Training loss: 2.527972890382584
Validation loss: 2.4776584721721866

Epoch: 6| Step: 11
Training loss: 2.4598719636425486
Validation loss: 2.483516360798421

Epoch: 6| Step: 12
Training loss: 2.848511953212627
Validation loss: 2.4752256139579245

Epoch: 6| Step: 13
Training loss: 3.686550729410566
Validation loss: 2.482892598795813

Epoch: 109| Step: 0
Training loss: 2.1573016048726212
Validation loss: 2.47988262232676

Epoch: 6| Step: 1
Training loss: 2.701464721957689
Validation loss: 2.4679177167547004

Epoch: 6| Step: 2
Training loss: 2.888162089483025
Validation loss: 2.480626808500501

Epoch: 6| Step: 3
Training loss: 2.9492688711593673
Validation loss: 2.4730725736483015

Epoch: 6| Step: 4
Training loss: 2.4432503777334564
Validation loss: 2.4668365806064263

Epoch: 6| Step: 5
Training loss: 2.710203923509601
Validation loss: 2.4794219159572726

Epoch: 6| Step: 6
Training loss: 2.545041694074747
Validation loss: 2.482741576537729

Epoch: 6| Step: 7
Training loss: 2.310060090278394
Validation loss: 2.47729211056088

Epoch: 6| Step: 8
Training loss: 2.4359081645811678
Validation loss: 2.4845314257827362

Epoch: 6| Step: 9
Training loss: 2.680544613259689
Validation loss: 2.490191831472651

Epoch: 6| Step: 10
Training loss: 3.3548761095347692
Validation loss: 2.4715596041498795

Epoch: 6| Step: 11
Training loss: 2.574277098577624
Validation loss: 2.4878831590258135

Epoch: 6| Step: 12
Training loss: 2.822469715804026
Validation loss: 2.4778201133744915

Epoch: 6| Step: 13
Training loss: 2.479793330471922
Validation loss: 2.4721352620408648

Epoch: 110| Step: 0
Training loss: 2.8006329570334585
Validation loss: 2.484676611845018

Epoch: 6| Step: 1
Training loss: 2.276031377726911
Validation loss: 2.4616662585353213

Epoch: 6| Step: 2
Training loss: 2.9479673850792874
Validation loss: 2.4625541932987947

Epoch: 6| Step: 3
Training loss: 2.985321693657825
Validation loss: 2.4669217554253673

Epoch: 6| Step: 4
Training loss: 2.7456288710683463
Validation loss: 2.4963476892218783

Epoch: 6| Step: 5
Training loss: 2.5927498969034914
Validation loss: 2.4759274736701813

Epoch: 6| Step: 6
Training loss: 2.806418731986224
Validation loss: 2.485079980477862

Epoch: 6| Step: 7
Training loss: 2.5358270777402554
Validation loss: 2.486915188025325

Epoch: 6| Step: 8
Training loss: 2.092804538711094
Validation loss: 2.471792070347936

Epoch: 6| Step: 9
Training loss: 2.693940009476492
Validation loss: 2.4897665461908813

Epoch: 6| Step: 10
Training loss: 2.64321059547753
Validation loss: 2.4753900497579733

Epoch: 6| Step: 11
Training loss: 2.5762660867558047
Validation loss: 2.4989654015321245

Epoch: 6| Step: 12
Training loss: 2.8242080287281373
Validation loss: 2.488664830322211

Epoch: 6| Step: 13
Training loss: 2.8032687113144314
Validation loss: 2.4940645833927886

Epoch: 111| Step: 0
Training loss: 3.1725418066422035
Validation loss: 2.5041000564831872

Epoch: 6| Step: 1
Training loss: 2.3364214443696687
Validation loss: 2.500867140520472

Epoch: 6| Step: 2
Training loss: 2.5454138560572814
Validation loss: 2.475845221138123

Epoch: 6| Step: 3
Training loss: 3.10578841957293
Validation loss: 2.495136844845025

Epoch: 6| Step: 4
Training loss: 2.2303634591980144
Validation loss: 2.500248773051497

Epoch: 6| Step: 5
Training loss: 3.116404040554102
Validation loss: 2.477949758087255

Epoch: 6| Step: 6
Training loss: 2.935141082960048
Validation loss: 2.483329447167673

Epoch: 6| Step: 7
Training loss: 2.7099699113027094
Validation loss: 2.459779228310369

Epoch: 6| Step: 8
Training loss: 2.5532093496408126
Validation loss: 2.4740959190533776

Epoch: 6| Step: 9
Training loss: 2.120362550936068
Validation loss: 2.469679896557699

Epoch: 6| Step: 10
Training loss: 2.0370289685189076
Validation loss: 2.486920024776583

Epoch: 6| Step: 11
Training loss: 2.7818971373946293
Validation loss: 2.4917340916226194

Epoch: 6| Step: 12
Training loss: 2.6766837053085224
Validation loss: 2.500970711474411

Epoch: 6| Step: 13
Training loss: 2.9304238983358686
Validation loss: 2.4849482709650075

Epoch: 112| Step: 0
Training loss: 1.8182822107645413
Validation loss: 2.4669934710490766

Epoch: 6| Step: 1
Training loss: 2.316599022497501
Validation loss: 2.468295605255872

Epoch: 6| Step: 2
Training loss: 2.6767313586058274
Validation loss: 2.477948246563424

Epoch: 6| Step: 3
Training loss: 2.692507846434768
Validation loss: 2.499970169299591

Epoch: 6| Step: 4
Training loss: 2.365748506292593
Validation loss: 2.4739587005114108

Epoch: 6| Step: 5
Training loss: 3.2373510849333504
Validation loss: 2.47083751542054

Epoch: 6| Step: 6
Training loss: 2.6834788735135415
Validation loss: 2.465017250441808

Epoch: 6| Step: 7
Training loss: 2.4631455945168157
Validation loss: 2.486326838193038

Epoch: 6| Step: 8
Training loss: 2.627593621093236
Validation loss: 2.4761031223489107

Epoch: 6| Step: 9
Training loss: 2.7938660360727305
Validation loss: 2.4827222093060533

Epoch: 6| Step: 10
Training loss: 2.725145206607297
Validation loss: 2.4721946998985467

Epoch: 6| Step: 11
Training loss: 2.923567501013684
Validation loss: 2.486544998653103

Epoch: 6| Step: 12
Training loss: 2.6029259027405045
Validation loss: 2.485424657655343

Epoch: 6| Step: 13
Training loss: 3.262079678056434
Validation loss: 2.477737973335832

Epoch: 113| Step: 0
Training loss: 3.16699354258181
Validation loss: 2.480858218908972

Epoch: 6| Step: 1
Training loss: 2.049107971571165
Validation loss: 2.502535543772562

Epoch: 6| Step: 2
Training loss: 2.4272034141949033
Validation loss: 2.484911395796138

Epoch: 6| Step: 3
Training loss: 3.1768941020372528
Validation loss: 2.4780452087944864

Epoch: 6| Step: 4
Training loss: 2.832727386155417
Validation loss: 2.481534605864492

Epoch: 6| Step: 5
Training loss: 2.429257566431802
Validation loss: 2.499520735793539

Epoch: 6| Step: 6
Training loss: 2.856950576306665
Validation loss: 2.4740361320728126

Epoch: 6| Step: 7
Training loss: 2.7676845145118865
Validation loss: 2.4810461319132884

Epoch: 6| Step: 8
Training loss: 2.465878615492659
Validation loss: 2.4691697648159345

Epoch: 6| Step: 9
Training loss: 2.3888224868515753
Validation loss: 2.49476804655087

Epoch: 6| Step: 10
Training loss: 2.8317577992982943
Validation loss: 2.4900762380937835

Epoch: 6| Step: 11
Training loss: 3.0079120569968385
Validation loss: 2.5044726161132567

Epoch: 6| Step: 12
Training loss: 2.149279509184381
Validation loss: 2.501410187083156

Epoch: 6| Step: 13
Training loss: 2.592162140301989
Validation loss: 2.503856565847594

Epoch: 114| Step: 0
Training loss: 1.9402854339781086
Validation loss: 2.4864090873277425

Epoch: 6| Step: 1
Training loss: 2.938904771319209
Validation loss: 2.4831591196583283

Epoch: 6| Step: 2
Training loss: 3.084788589034601
Validation loss: 2.460278384139669

Epoch: 6| Step: 3
Training loss: 2.5531088708300387
Validation loss: 2.482364465567589

Epoch: 6| Step: 4
Training loss: 3.7533464123468088
Validation loss: 2.483863813576146

Epoch: 6| Step: 5
Training loss: 2.182310215060146
Validation loss: 2.492516420487015

Epoch: 6| Step: 6
Training loss: 2.742858283008611
Validation loss: 2.474132776160444

Epoch: 6| Step: 7
Training loss: 2.6155317392438002
Validation loss: 2.485434562840717

Epoch: 6| Step: 8
Training loss: 2.2688930750098466
Validation loss: 2.4576898368399633

Epoch: 6| Step: 9
Training loss: 2.258720558536172
Validation loss: 2.455136021071661

Epoch: 6| Step: 10
Training loss: 2.531028266896597
Validation loss: 2.486730757702275

Epoch: 6| Step: 11
Training loss: 2.593688964125605
Validation loss: 2.4998567324779777

Epoch: 6| Step: 12
Training loss: 2.9439211526346436
Validation loss: 2.4784206640892856

Epoch: 6| Step: 13
Training loss: 2.0098474780616566
Validation loss: 2.4739231490961684

Epoch: 115| Step: 0
Training loss: 2.64167331450034
Validation loss: 2.4770655184740056

Epoch: 6| Step: 1
Training loss: 2.0177371764661167
Validation loss: 2.4743449585924564

Epoch: 6| Step: 2
Training loss: 2.7344913130953703
Validation loss: 2.474745799169779

Epoch: 6| Step: 3
Training loss: 2.5583284429778868
Validation loss: 2.4654181944240223

Epoch: 6| Step: 4
Training loss: 2.155583485960938
Validation loss: 2.4965165983494177

Epoch: 6| Step: 5
Training loss: 2.313037912852166
Validation loss: 2.456482353285006

Epoch: 6| Step: 6
Training loss: 2.306420247066675
Validation loss: 2.479335223539896

Epoch: 6| Step: 7
Training loss: 3.6239284378548193
Validation loss: 2.482291889635186

Epoch: 6| Step: 8
Training loss: 3.0852136463149846
Validation loss: 2.488205248783011

Epoch: 6| Step: 9
Training loss: 2.085185626249315
Validation loss: 2.469578680874169

Epoch: 6| Step: 10
Training loss: 3.2800351755070367
Validation loss: 2.489815247612655

Epoch: 6| Step: 11
Training loss: 2.8772458136714767
Validation loss: 2.469804000109208

Epoch: 6| Step: 12
Training loss: 2.750626752697248
Validation loss: 2.4616532209019724

Epoch: 6| Step: 13
Training loss: 1.6804585660930016
Validation loss: 2.465848716134451

Epoch: 116| Step: 0
Training loss: 2.213489367297296
Validation loss: 2.4973280163706773

Epoch: 6| Step: 1
Training loss: 3.1482922541811167
Validation loss: 2.4880782816171325

Epoch: 6| Step: 2
Training loss: 2.7284549823813014
Validation loss: 2.4730060734737185

Epoch: 6| Step: 3
Training loss: 3.2113663509995294
Validation loss: 2.489836359486231

Epoch: 6| Step: 4
Training loss: 2.317440426090605
Validation loss: 2.4796115878951355

Epoch: 6| Step: 5
Training loss: 2.899045155621253
Validation loss: 2.5088332996142113

Epoch: 6| Step: 6
Training loss: 2.4293110546649603
Validation loss: 2.477976787412499

Epoch: 6| Step: 7
Training loss: 2.9451959318776124
Validation loss: 2.48396118477926

Epoch: 6| Step: 8
Training loss: 2.0224899370959295
Validation loss: 2.499803241042058

Epoch: 6| Step: 9
Training loss: 2.4498454726396997
Validation loss: 2.4871045420803752

Epoch: 6| Step: 10
Training loss: 1.918620944575617
Validation loss: 2.482731064804585

Epoch: 6| Step: 11
Training loss: 3.276148727115076
Validation loss: 2.493907216701773

Epoch: 6| Step: 12
Training loss: 2.807434606835086
Validation loss: 2.465363886098372

Epoch: 6| Step: 13
Training loss: 2.3078769145388
Validation loss: 2.5002476405475025

Epoch: 117| Step: 0
Training loss: 2.7070604903498343
Validation loss: 2.5036649297330955

Epoch: 6| Step: 1
Training loss: 2.4669316029246335
Validation loss: 2.500420092080962

Epoch: 6| Step: 2
Training loss: 2.4760246775911843
Validation loss: 2.4994865643958577

Epoch: 6| Step: 3
Training loss: 2.9617748032812288
Validation loss: 2.478118277384169

Epoch: 6| Step: 4
Training loss: 2.473845908304203
Validation loss: 2.477395971707378

Epoch: 6| Step: 5
Training loss: 2.296777839941191
Validation loss: 2.4754261906135064

Epoch: 6| Step: 6
Training loss: 2.6068510537978598
Validation loss: 2.473742393477771

Epoch: 6| Step: 7
Training loss: 3.6189184805170633
Validation loss: 2.454762470545428

Epoch: 6| Step: 8
Training loss: 2.348694633574362
Validation loss: 2.4741873043988147

Epoch: 6| Step: 9
Training loss: 3.0269011143139384
Validation loss: 2.467501702586769

Epoch: 6| Step: 10
Training loss: 2.647140320290025
Validation loss: 2.4741091760698755

Epoch: 6| Step: 11
Training loss: 2.5484763885655624
Validation loss: 2.496097974298471

Epoch: 6| Step: 12
Training loss: 2.0774580953803916
Validation loss: 2.4846916716661385

Epoch: 6| Step: 13
Training loss: 2.407923426167834
Validation loss: 2.4757838993555823

Epoch: 118| Step: 0
Training loss: 2.989674920109156
Validation loss: 2.487104954389609

Epoch: 6| Step: 1
Training loss: 3.0252159861231913
Validation loss: 2.4908115794628274

Epoch: 6| Step: 2
Training loss: 2.469328414180187
Validation loss: 2.498625296336982

Epoch: 6| Step: 3
Training loss: 2.1183775762964485
Validation loss: 2.470458322699745

Epoch: 6| Step: 4
Training loss: 2.587759614903194
Validation loss: 2.506632729806024

Epoch: 6| Step: 5
Training loss: 2.626962019327415
Validation loss: 2.502493619656733

Epoch: 6| Step: 6
Training loss: 2.459894449714327
Validation loss: 2.4602886500246233

Epoch: 6| Step: 7
Training loss: 2.510280832332856
Validation loss: 2.4710801969416303

Epoch: 6| Step: 8
Training loss: 2.814897320841363
Validation loss: 2.457481473060631

Epoch: 6| Step: 9
Training loss: 2.6598968603459463
Validation loss: 2.468700071457951

Epoch: 6| Step: 10
Training loss: 2.82382651067918
Validation loss: 2.50664674645723

Epoch: 6| Step: 11
Training loss: 3.1170120381088253
Validation loss: 2.4841700406338214

Epoch: 6| Step: 12
Training loss: 2.3823910199725606
Validation loss: 2.4922530995341274

Epoch: 6| Step: 13
Training loss: 2.22915958391532
Validation loss: 2.474098378974109

Epoch: 119| Step: 0
Training loss: 2.966783374105226
Validation loss: 2.472759086641085

Epoch: 6| Step: 1
Training loss: 2.4034706016029768
Validation loss: 2.4922069726631273

Epoch: 6| Step: 2
Training loss: 2.6953165690073453
Validation loss: 2.4963055251103117

Epoch: 6| Step: 3
Training loss: 2.4626434710383878
Validation loss: 2.4906639276518088

Epoch: 6| Step: 4
Training loss: 2.6570451331736313
Validation loss: 2.494448775403026

Epoch: 6| Step: 5
Training loss: 2.7112382617518707
Validation loss: 2.494096751211071

Epoch: 6| Step: 6
Training loss: 3.33767590577116
Validation loss: 2.478150518744487

Epoch: 6| Step: 7
Training loss: 1.9800587969055063
Validation loss: 2.5074185630083097

Epoch: 6| Step: 8
Training loss: 3.3388439087280775
Validation loss: 2.493359228339348

Epoch: 6| Step: 9
Training loss: 2.7608857146211645
Validation loss: 2.502860858602012

Epoch: 6| Step: 10
Training loss: 2.892286987901744
Validation loss: 2.500109428143807

Epoch: 6| Step: 11
Training loss: 2.2739306567178517
Validation loss: 2.481396691143326

Epoch: 6| Step: 12
Training loss: 1.8864365518689716
Validation loss: 2.508813484925096

Epoch: 6| Step: 13
Training loss: 2.2472451617381592
Validation loss: 2.4675323538548914

Epoch: 120| Step: 0
Training loss: 2.145546594615635
Validation loss: 2.4768511678096092

Epoch: 6| Step: 1
Training loss: 2.127799041925102
Validation loss: 2.4775014323916498

Epoch: 6| Step: 2
Training loss: 2.225655139953617
Validation loss: 2.469967178723503

Epoch: 6| Step: 3
Training loss: 2.6298818152674026
Validation loss: 2.4848051900237396

Epoch: 6| Step: 4
Training loss: 3.151114033690357
Validation loss: 2.4677296341000923

Epoch: 6| Step: 5
Training loss: 2.7595156848680817
Validation loss: 2.470564910707563

Epoch: 6| Step: 6
Training loss: 3.041547293911616
Validation loss: 2.4820036087770108

Epoch: 6| Step: 7
Training loss: 2.4339790345404118
Validation loss: 2.4760139209453977

Epoch: 6| Step: 8
Training loss: 2.6893792789823237
Validation loss: 2.4977716502781804

Epoch: 6| Step: 9
Training loss: 2.700532585174878
Validation loss: 2.510778222292694

Epoch: 6| Step: 10
Training loss: 2.947354769764743
Validation loss: 2.4728737462831423

Epoch: 6| Step: 11
Training loss: 2.6603147036600325
Validation loss: 2.5037627626418093

Epoch: 6| Step: 12
Training loss: 2.535541052628695
Validation loss: 2.48127717827508

Epoch: 6| Step: 13
Training loss: 3.0283412545128034
Validation loss: 2.47823536160854

Epoch: 121| Step: 0
Training loss: 2.563501487974059
Validation loss: 2.4815409386784717

Epoch: 6| Step: 1
Training loss: 2.720692609231293
Validation loss: 2.4741675241278247

Epoch: 6| Step: 2
Training loss: 2.334445438573425
Validation loss: 2.4804750896692913

Epoch: 6| Step: 3
Training loss: 3.6591935006077687
Validation loss: 2.4982713557765206

Epoch: 6| Step: 4
Training loss: 2.152876384760133
Validation loss: 2.4623319910533037

Epoch: 6| Step: 5
Training loss: 3.1524027425358363
Validation loss: 2.47591393853947

Epoch: 6| Step: 6
Training loss: 2.6650014883756508
Validation loss: 2.521225021090275

Epoch: 6| Step: 7
Training loss: 2.5276254205119986
Validation loss: 2.495026531763558

Epoch: 6| Step: 8
Training loss: 2.331532998587976
Validation loss: 2.479711790811235

Epoch: 6| Step: 9
Training loss: 2.283454143734462
Validation loss: 2.4923224939629804

Epoch: 6| Step: 10
Training loss: 2.752403249538717
Validation loss: 2.493259564653274

Epoch: 6| Step: 11
Training loss: 2.08684334391907
Validation loss: 2.4588610609481885

Epoch: 6| Step: 12
Training loss: 2.479813520742286
Validation loss: 2.4806033812917154

Epoch: 6| Step: 13
Training loss: 3.113968566304675
Validation loss: 2.4779059757691004

Epoch: 122| Step: 0
Training loss: 2.5480813755832523
Validation loss: 2.4870262206400837

Epoch: 6| Step: 1
Training loss: 2.5515397303838987
Validation loss: 2.499927434585936

Epoch: 6| Step: 2
Training loss: 2.629204425651462
Validation loss: 2.4926908930185574

Epoch: 6| Step: 3
Training loss: 3.27120771917905
Validation loss: 2.4865792478919593

Epoch: 6| Step: 4
Training loss: 2.2856274762869093
Validation loss: 2.480039140225051

Epoch: 6| Step: 5
Training loss: 2.8893974378252185
Validation loss: 2.460819947481731

Epoch: 6| Step: 6
Training loss: 2.354049533416837
Validation loss: 2.507822661260577

Epoch: 6| Step: 7
Training loss: 2.33679724344328
Validation loss: 2.4963018526489225

Epoch: 6| Step: 8
Training loss: 3.0852601672107722
Validation loss: 2.506254808070795

Epoch: 6| Step: 9
Training loss: 2.9008834677022923
Validation loss: 2.491071905710455

Epoch: 6| Step: 10
Training loss: 2.44786684242568
Validation loss: 2.4765686716215214

Epoch: 6| Step: 11
Training loss: 2.432396264995862
Validation loss: 2.495990607318462

Epoch: 6| Step: 12
Training loss: 2.9397736536205445
Validation loss: 2.4680684712954366

Epoch: 6| Step: 13
Training loss: 2.231085109227777
Validation loss: 2.459290319403259

Epoch: 123| Step: 0
Training loss: 3.2581315650485965
Validation loss: 2.4780680503312436

Epoch: 6| Step: 1
Training loss: 3.2767352333798425
Validation loss: 2.4900142526546083

Epoch: 6| Step: 2
Training loss: 3.4564976856958407
Validation loss: 2.4953845837792907

Epoch: 6| Step: 3
Training loss: 2.905862597825122
Validation loss: 2.4993968861658766

Epoch: 6| Step: 4
Training loss: 2.6871218748005714
Validation loss: 2.495480329179783

Epoch: 6| Step: 5
Training loss: 2.6637002916857973
Validation loss: 2.5105267827651003

Epoch: 6| Step: 6
Training loss: 2.868358071675109
Validation loss: 2.4779964214056345

Epoch: 6| Step: 7
Training loss: 3.110544569413939
Validation loss: 2.4968196882637463

Epoch: 6| Step: 8
Training loss: 1.8655105785764168
Validation loss: 2.4803672934497336

Epoch: 6| Step: 9
Training loss: 1.7822349903482237
Validation loss: 2.49463399883438

Epoch: 6| Step: 10
Training loss: 2.1392526055868797
Validation loss: 2.482406993495507

Epoch: 6| Step: 11
Training loss: 1.7751024136078422
Validation loss: 2.4696371110568247

Epoch: 6| Step: 12
Training loss: 1.8947656860024393
Validation loss: 2.4602952021626625

Epoch: 6| Step: 13
Training loss: 2.2697552096091447
Validation loss: 2.46700468892949

Epoch: 124| Step: 0
Training loss: 1.8921924745350975
Validation loss: 2.475987525601444

Epoch: 6| Step: 1
Training loss: 1.2541746523698984
Validation loss: 2.484195687536871

Epoch: 6| Step: 2
Training loss: 2.8697940297933404
Validation loss: 2.493765661356112

Epoch: 6| Step: 3
Training loss: 2.951567546947197
Validation loss: 2.47321863969511

Epoch: 6| Step: 4
Training loss: 2.717078034629232
Validation loss: 2.4797398032329805

Epoch: 6| Step: 5
Training loss: 3.0526337805322923
Validation loss: 2.4670869057588893

Epoch: 6| Step: 6
Training loss: 3.307718244461159
Validation loss: 2.4841313644134138

Epoch: 6| Step: 7
Training loss: 2.4364844921110933
Validation loss: 2.4794391438568866

Epoch: 6| Step: 8
Training loss: 2.534525882542884
Validation loss: 2.4703985753269166

Epoch: 6| Step: 9
Training loss: 2.41307329076512
Validation loss: 2.508531529394543

Epoch: 6| Step: 10
Training loss: 2.8735327915904674
Validation loss: 2.4907121457776604

Epoch: 6| Step: 11
Training loss: 3.45568090240803
Validation loss: 2.4901855577207597

Epoch: 6| Step: 12
Training loss: 2.4881318674804844
Validation loss: 2.471951094849326

Epoch: 6| Step: 13
Training loss: 1.6317617877799184
Validation loss: 2.47958020195885

Epoch: 125| Step: 0
Training loss: 2.5540464571056085
Validation loss: 2.5014313794236913

Epoch: 6| Step: 1
Training loss: 2.314433140406863
Validation loss: 2.493641659545184

Epoch: 6| Step: 2
Training loss: 2.192963561896862
Validation loss: 2.505423385261761

Epoch: 6| Step: 3
Training loss: 2.839248615855385
Validation loss: 2.481221063325876

Epoch: 6| Step: 4
Training loss: 2.3107846057962145
Validation loss: 2.5197959161198877

Epoch: 6| Step: 5
Training loss: 2.5755190539120867
Validation loss: 2.4893239804787695

Epoch: 6| Step: 6
Training loss: 3.1405422522428443
Validation loss: 2.4897917508671004

Epoch: 6| Step: 7
Training loss: 2.199634898841602
Validation loss: 2.486356832582798

Epoch: 6| Step: 8
Training loss: 2.7630333260491367
Validation loss: 2.484081210437219

Epoch: 6| Step: 9
Training loss: 3.019352481973642
Validation loss: 2.486554958658159

Epoch: 6| Step: 10
Training loss: 2.3660943557163114
Validation loss: 2.483566952497115

Epoch: 6| Step: 11
Training loss: 3.0077942686643087
Validation loss: 2.4985951845075527

Epoch: 6| Step: 12
Training loss: 2.438429337404388
Validation loss: 2.4787104755564378

Epoch: 6| Step: 13
Training loss: 3.2865216732218774
Validation loss: 2.483677833393441

Epoch: 126| Step: 0
Training loss: 2.9105678190512716
Validation loss: 2.4865504500867224

Epoch: 6| Step: 1
Training loss: 2.746264782024982
Validation loss: 2.483229004636125

Epoch: 6| Step: 2
Training loss: 2.7907951380007847
Validation loss: 2.4816863875001975

Epoch: 6| Step: 3
Training loss: 2.962210751347782
Validation loss: 2.513925900713059

Epoch: 6| Step: 4
Training loss: 2.2947735158162343
Validation loss: 2.458148557943266

Epoch: 6| Step: 5
Training loss: 2.275351645991239
Validation loss: 2.4617133898526387

Epoch: 6| Step: 6
Training loss: 2.8384939393863617
Validation loss: 2.4951531767013084

Epoch: 6| Step: 7
Training loss: 3.1028707133094
Validation loss: 2.50035619505496

Epoch: 6| Step: 8
Training loss: 2.061164307857478
Validation loss: 2.4867936332127054

Epoch: 6| Step: 9
Training loss: 2.459810513587652
Validation loss: 2.4803111398272164

Epoch: 6| Step: 10
Training loss: 3.0504897365425547
Validation loss: 2.5027675517492987

Epoch: 6| Step: 11
Training loss: 2.5732440405591053
Validation loss: 2.482662433864764

Epoch: 6| Step: 12
Training loss: 2.384366288675416
Validation loss: 2.48505979173085

Epoch: 6| Step: 13
Training loss: 1.9235506273929468
Validation loss: 2.479300049602434

Epoch: 127| Step: 0
Training loss: 2.328434968321615
Validation loss: 2.478705858598319

Epoch: 6| Step: 1
Training loss: 2.8544176831279295
Validation loss: 2.4679825502303174

Epoch: 6| Step: 2
Training loss: 2.987949647144689
Validation loss: 2.5071205991792485

Epoch: 6| Step: 3
Training loss: 2.1696117876576597
Validation loss: 2.4719334196276566

Epoch: 6| Step: 4
Training loss: 2.834033169053584
Validation loss: 2.512375107303136

Epoch: 6| Step: 5
Training loss: 2.8474556390456742
Validation loss: 2.473634717378384

Epoch: 6| Step: 6
Training loss: 2.559542367979848
Validation loss: 2.488508405268996

Epoch: 6| Step: 7
Training loss: 2.3244267731224397
Validation loss: 2.4617394498334497

Epoch: 6| Step: 8
Training loss: 2.689704877695983
Validation loss: 2.507115317734936

Epoch: 6| Step: 9
Training loss: 2.1869715461013732
Validation loss: 2.4984575917714618

Epoch: 6| Step: 10
Training loss: 2.2162774506755247
Validation loss: 2.4944120365701536

Epoch: 6| Step: 11
Training loss: 3.0162237492038337
Validation loss: 2.4622276152590747

Epoch: 6| Step: 12
Training loss: 2.920504978096665
Validation loss: 2.500222526409036

Epoch: 6| Step: 13
Training loss: 2.6133589797156365
Validation loss: 2.4784634365656095

Epoch: 128| Step: 0
Training loss: 2.058573366241587
Validation loss: 2.4816667641060826

Epoch: 6| Step: 1
Training loss: 2.9897377603412565
Validation loss: 2.4614025254115415

Epoch: 6| Step: 2
Training loss: 2.259869018404978
Validation loss: 2.4871034669837253

Epoch: 6| Step: 3
Training loss: 2.1007238366739145
Validation loss: 2.4878105905464976

Epoch: 6| Step: 4
Training loss: 2.633021134166748
Validation loss: 2.4706279963223157

Epoch: 6| Step: 5
Training loss: 3.169956171833872
Validation loss: 2.4693758103718237

Epoch: 6| Step: 6
Training loss: 2.4895700324007812
Validation loss: 2.4843077807556386

Epoch: 6| Step: 7
Training loss: 3.4277353487801325
Validation loss: 2.4813156995802954

Epoch: 6| Step: 8
Training loss: 2.5838421863928436
Validation loss: 2.482742791889467

Epoch: 6| Step: 9
Training loss: 3.026496227131598
Validation loss: 2.491508088590944

Epoch: 6| Step: 10
Training loss: 1.9567126473521506
Validation loss: 2.486173267744333

Epoch: 6| Step: 11
Training loss: 2.8894114653381977
Validation loss: 2.4950592584625824

Epoch: 6| Step: 12
Training loss: 2.8131599499697435
Validation loss: 2.4942027823245834

Epoch: 6| Step: 13
Training loss: 2.197303602102361
Validation loss: 2.474606487727895

Epoch: 129| Step: 0
Training loss: 2.9727351695749378
Validation loss: 2.5018225599585824

Epoch: 6| Step: 1
Training loss: 2.510105213984509
Validation loss: 2.481274028064645

Epoch: 6| Step: 2
Training loss: 2.46865400296695
Validation loss: 2.481528774082675

Epoch: 6| Step: 3
Training loss: 3.037709697502187
Validation loss: 2.498174076442794

Epoch: 6| Step: 4
Training loss: 2.6517552662578923
Validation loss: 2.4828798502169156

Epoch: 6| Step: 5
Training loss: 2.6621657972171002
Validation loss: 2.474310027136352

Epoch: 6| Step: 6
Training loss: 2.223464899169068
Validation loss: 2.4922206301818757

Epoch: 6| Step: 7
Training loss: 2.4633803096347586
Validation loss: 2.4937760258308406

Epoch: 6| Step: 8
Training loss: 3.35246695027855
Validation loss: 2.4878014017337113

Epoch: 6| Step: 9
Training loss: 2.9220225251026095
Validation loss: 2.51347357622427

Epoch: 6| Step: 10
Training loss: 2.2304289859360487
Validation loss: 2.492600088356381

Epoch: 6| Step: 11
Training loss: 2.0442107821510955
Validation loss: 2.4666460843157365

Epoch: 6| Step: 12
Training loss: 2.5078864636313445
Validation loss: 2.487335669855293

Epoch: 6| Step: 13
Training loss: 2.510168947104033
Validation loss: 2.4803047656301

Epoch: 130| Step: 0
Training loss: 2.341543761407092
Validation loss: 2.493503569339347

Epoch: 6| Step: 1
Training loss: 2.242792453301559
Validation loss: 2.4730746603668545

Epoch: 6| Step: 2
Training loss: 2.700276985795774
Validation loss: 2.5013874715807822

Epoch: 6| Step: 3
Training loss: 2.9007263194831037
Validation loss: 2.468457472700257

Epoch: 6| Step: 4
Training loss: 2.766325781017339
Validation loss: 2.467781712946502

Epoch: 6| Step: 5
Training loss: 2.730904605770826
Validation loss: 2.4690871231142113

Epoch: 6| Step: 6
Training loss: 2.6932347346975978
Validation loss: 2.5254450413244838

Epoch: 6| Step: 7
Training loss: 2.654984745046465
Validation loss: 2.4952939706340262

Epoch: 6| Step: 8
Training loss: 2.7514741587713885
Validation loss: 2.4894302025189146

Epoch: 6| Step: 9
Training loss: 3.1508143991948856
Validation loss: 2.48870818850162

Epoch: 6| Step: 10
Training loss: 2.7563683724448476
Validation loss: 2.501416287150444

Epoch: 6| Step: 11
Training loss: 2.2261057602743213
Validation loss: 2.472943702666971

Epoch: 6| Step: 12
Training loss: 2.6035462110801837
Validation loss: 2.5054399636845535

Epoch: 6| Step: 13
Training loss: 2.159492045651744
Validation loss: 2.5094584497979895

Epoch: 131| Step: 0
Training loss: 2.5824146072730887
Validation loss: 2.4859319735284413

Epoch: 6| Step: 1
Training loss: 2.3753414912412505
Validation loss: 2.4918272303418836

Epoch: 6| Step: 2
Training loss: 2.9202674074401185
Validation loss: 2.4938535246660174

Epoch: 6| Step: 3
Training loss: 2.0267437761888076
Validation loss: 2.4938656825989467

Epoch: 6| Step: 4
Training loss: 3.0266145480145665
Validation loss: 2.4782370477812417

Epoch: 6| Step: 5
Training loss: 2.6627612921504267
Validation loss: 2.4839880650942523

Epoch: 6| Step: 6
Training loss: 2.11317903338459
Validation loss: 2.4999613892229586

Epoch: 6| Step: 7
Training loss: 2.7428477652565415
Validation loss: 2.4798092500951197

Epoch: 6| Step: 8
Training loss: 2.6998742462825462
Validation loss: 2.5028019072837164

Epoch: 6| Step: 9
Training loss: 2.737616486879753
Validation loss: 2.4818225073545093

Epoch: 6| Step: 10
Training loss: 2.8448512701166773
Validation loss: 2.496036695080258

Epoch: 6| Step: 11
Training loss: 2.804769955753895
Validation loss: 2.5105891072547055

Epoch: 6| Step: 12
Training loss: 2.9524451386525254
Validation loss: 2.4766669222409914

Epoch: 6| Step: 13
Training loss: 1.7052577252964514
Validation loss: 2.480226174681645

Epoch: 132| Step: 0
Training loss: 1.0821695067286525
Validation loss: 2.5134241117935314

Epoch: 6| Step: 1
Training loss: 2.7468898699014024
Validation loss: 2.4929170942453123

Epoch: 6| Step: 2
Training loss: 2.937227926437733
Validation loss: 2.5011583843961764

Epoch: 6| Step: 3
Training loss: 2.35928920564599
Validation loss: 2.4793147750076456

Epoch: 6| Step: 4
Training loss: 3.424140911536157
Validation loss: 2.4930108870045826

Epoch: 6| Step: 5
Training loss: 2.88355805802044
Validation loss: 2.4829886307550346

Epoch: 6| Step: 6
Training loss: 2.693777957780027
Validation loss: 2.493851016384663

Epoch: 6| Step: 7
Training loss: 2.6796102707302287
Validation loss: 2.497745635740815

Epoch: 6| Step: 8
Training loss: 3.210433289230918
Validation loss: 2.497356997858885

Epoch: 6| Step: 9
Training loss: 2.518342061291694
Validation loss: 2.481929091760957

Epoch: 6| Step: 10
Training loss: 2.2425838744809594
Validation loss: 2.4741217253030685

Epoch: 6| Step: 11
Training loss: 2.272687545775947
Validation loss: 2.4910326944615178

Epoch: 6| Step: 12
Training loss: 2.77779164840626
Validation loss: 2.463755342191132

Epoch: 6| Step: 13
Training loss: 2.184621060855844
Validation loss: 2.5018547632878727

Epoch: 133| Step: 0
Training loss: 2.5021469434221983
Validation loss: 2.473793267970063

Epoch: 6| Step: 1
Training loss: 2.4171834480828696
Validation loss: 2.507034244890336

Epoch: 6| Step: 2
Training loss: 2.3278564707894236
Validation loss: 2.496462571181855

Epoch: 6| Step: 3
Training loss: 2.529808293192426
Validation loss: 2.4656725532205623

Epoch: 6| Step: 4
Training loss: 2.5516166312180872
Validation loss: 2.510007274893158

Epoch: 6| Step: 5
Training loss: 3.1326027512195416
Validation loss: 2.4920744187339134

Epoch: 6| Step: 6
Training loss: 1.9673235130405367
Validation loss: 2.5059189684697922

Epoch: 6| Step: 7
Training loss: 2.897108570217373
Validation loss: 2.4907880200687114

Epoch: 6| Step: 8
Training loss: 2.8613701202122996
Validation loss: 2.52290682703653

Epoch: 6| Step: 9
Training loss: 2.829891769981596
Validation loss: 2.5093290123888528

Epoch: 6| Step: 10
Training loss: 2.1916301429325715
Validation loss: 2.512620261978268

Epoch: 6| Step: 11
Training loss: 2.732465851370562
Validation loss: 2.4995422292828318

Epoch: 6| Step: 12
Training loss: 2.72433013031939
Validation loss: 2.488301470305354

Epoch: 6| Step: 13
Training loss: 3.0388727986965356
Validation loss: 2.4827219656141826

Epoch: 134| Step: 0
Training loss: 2.0099780801745033
Validation loss: 2.4826321366829243

Epoch: 6| Step: 1
Training loss: 2.455055983268293
Validation loss: 2.486900883886634

Epoch: 6| Step: 2
Training loss: 2.8868647681693136
Validation loss: 2.4767353560017944

Epoch: 6| Step: 3
Training loss: 2.6111070743376277
Validation loss: 2.481214091682057

Epoch: 6| Step: 4
Training loss: 2.5800291485581455
Validation loss: 2.498141504459518

Epoch: 6| Step: 5
Training loss: 2.8916104673115166
Validation loss: 2.4867474267229874

Epoch: 6| Step: 6
Training loss: 2.5747232825469717
Validation loss: 2.509122574109978

Epoch: 6| Step: 7
Training loss: 2.820693901669244
Validation loss: 2.4817965518581344

Epoch: 6| Step: 8
Training loss: 3.204890662365074
Validation loss: 2.4959390800863837

Epoch: 6| Step: 9
Training loss: 2.551542813937042
Validation loss: 2.5137858693671746

Epoch: 6| Step: 10
Training loss: 2.259296708181682
Validation loss: 2.482500057716354

Epoch: 6| Step: 11
Training loss: 1.9139528204186362
Validation loss: 2.4891120769082686

Epoch: 6| Step: 12
Training loss: 2.6582372076020384
Validation loss: 2.484768343683749

Epoch: 6| Step: 13
Training loss: 3.1979814123170556
Validation loss: 2.471174338189991

Epoch: 135| Step: 0
Training loss: 2.9634792721330623
Validation loss: 2.4703008262354973

Epoch: 6| Step: 1
Training loss: 2.7606443395233007
Validation loss: 2.4871903287595774

Epoch: 6| Step: 2
Training loss: 2.4259691641688375
Validation loss: 2.496276919615239

Epoch: 6| Step: 3
Training loss: 2.0242747338465694
Validation loss: 2.469974064308225

Epoch: 6| Step: 4
Training loss: 2.783195072630895
Validation loss: 2.497100434504255

Epoch: 6| Step: 5
Training loss: 2.8099474874185035
Validation loss: 2.486530713520569

Epoch: 6| Step: 6
Training loss: 2.3391913267040056
Validation loss: 2.495647896877603

Epoch: 6| Step: 7
Training loss: 3.0392375983860425
Validation loss: 2.474264171372298

Epoch: 6| Step: 8
Training loss: 2.733061643991884
Validation loss: 2.4572113491233183

Epoch: 6| Step: 9
Training loss: 2.116686772328603
Validation loss: 2.5083340588792997

Epoch: 6| Step: 10
Training loss: 2.6564758204790873
Validation loss: 2.501647151258689

Epoch: 6| Step: 11
Training loss: 2.9159400761521077
Validation loss: 2.4879596873522076

Epoch: 6| Step: 12
Training loss: 2.711039868271656
Validation loss: 2.4943626813045583

Epoch: 6| Step: 13
Training loss: 1.7558873463949731
Validation loss: 2.48649050891974

Epoch: 136| Step: 0
Training loss: 3.0122144318401753
Validation loss: 2.4829352338345254

Epoch: 6| Step: 1
Training loss: 2.556704783381381
Validation loss: 2.4921370555283033

Epoch: 6| Step: 2
Training loss: 3.450196418146582
Validation loss: 2.4850912848666176

Epoch: 6| Step: 3
Training loss: 2.69243922017103
Validation loss: 2.466189102832968

Epoch: 6| Step: 4
Training loss: 2.711317579905387
Validation loss: 2.470434494489561

Epoch: 6| Step: 5
Training loss: 2.2135934143001528
Validation loss: 2.504268966863952

Epoch: 6| Step: 6
Training loss: 2.7497724092113
Validation loss: 2.484690287024845

Epoch: 6| Step: 7
Training loss: 2.148175365755453
Validation loss: 2.4918270338373367

Epoch: 6| Step: 8
Training loss: 2.5847847665576937
Validation loss: 2.474483315033689

Epoch: 6| Step: 9
Training loss: 2.9864671972910743
Validation loss: 2.4808170378007053

Epoch: 6| Step: 10
Training loss: 2.036215596151353
Validation loss: 2.4963787626800764

Epoch: 6| Step: 11
Training loss: 2.5865899038067193
Validation loss: 2.4785104201697004

Epoch: 6| Step: 12
Training loss: 2.185180960697226
Validation loss: 2.4843414215627533

Epoch: 6| Step: 13
Training loss: 2.258999311307724
Validation loss: 2.5067674684607524

Epoch: 137| Step: 0
Training loss: 2.971391326021114
Validation loss: 2.479848532328347

Epoch: 6| Step: 1
Training loss: 2.739577222075131
Validation loss: 2.496664888762017

Epoch: 6| Step: 2
Training loss: 2.6240946707162354
Validation loss: 2.52050954464242

Epoch: 6| Step: 3
Training loss: 2.210368484226006
Validation loss: 2.4788131230837243

Epoch: 6| Step: 4
Training loss: 2.7859635433706296
Validation loss: 2.5268622809883956

Epoch: 6| Step: 5
Training loss: 2.8538339425572565
Validation loss: 2.4739081858985776

Epoch: 6| Step: 6
Training loss: 3.1658005450154802
Validation loss: 2.492022903113807

Epoch: 6| Step: 7
Training loss: 2.9136026546317444
Validation loss: 2.4898287360035662

Epoch: 6| Step: 8
Training loss: 2.9879593819202714
Validation loss: 2.481041400472985

Epoch: 6| Step: 9
Training loss: 2.2273994880373573
Validation loss: 2.488200114192006

Epoch: 6| Step: 10
Training loss: 2.152929098355347
Validation loss: 2.501917421640582

Epoch: 6| Step: 11
Training loss: 2.581774908086565
Validation loss: 2.5041634474554093

Epoch: 6| Step: 12
Training loss: 2.0003492527240176
Validation loss: 2.5157521686475417

Epoch: 6| Step: 13
Training loss: 1.856222092132907
Validation loss: 2.464836650387223

Epoch: 138| Step: 0
Training loss: 2.271505996981679
Validation loss: 2.503242764528109

Epoch: 6| Step: 1
Training loss: 1.897313040065654
Validation loss: 2.5004497267131813

Epoch: 6| Step: 2
Training loss: 2.7582959840057195
Validation loss: 2.4885878595936757

Epoch: 6| Step: 3
Training loss: 3.026295811567555
Validation loss: 2.5013272115722263

Epoch: 6| Step: 4
Training loss: 3.21593048157212
Validation loss: 2.4855006722118116

Epoch: 6| Step: 5
Training loss: 2.3177624185373658
Validation loss: 2.4929819742796298

Epoch: 6| Step: 6
Training loss: 2.7358007583127413
Validation loss: 2.496080508066924

Epoch: 6| Step: 7
Training loss: 2.6018176254408547
Validation loss: 2.4688660674967333

Epoch: 6| Step: 8
Training loss: 2.194140138196492
Validation loss: 2.4765520075870175

Epoch: 6| Step: 9
Training loss: 2.5517722011529846
Validation loss: 2.517530629173253

Epoch: 6| Step: 10
Training loss: 2.3674549921061727
Validation loss: 2.499992242411401

Epoch: 6| Step: 11
Training loss: 2.627598793067138
Validation loss: 2.509746658344822

Epoch: 6| Step: 12
Training loss: 3.408095813362512
Validation loss: 2.4914331211538747

Epoch: 6| Step: 13
Training loss: 2.3558719726387602
Validation loss: 2.4965319276726587

Epoch: 139| Step: 0
Training loss: 2.3907548457767436
Validation loss: 2.496762402477416

Epoch: 6| Step: 1
Training loss: 2.0806786471756036
Validation loss: 2.494525113610842

Epoch: 6| Step: 2
Training loss: 1.863910240996027
Validation loss: 2.490780707245096

Epoch: 6| Step: 3
Training loss: 2.711910018395886
Validation loss: 2.473882704886555

Epoch: 6| Step: 4
Training loss: 3.202031688650134
Validation loss: 2.4992700310928893

Epoch: 6| Step: 5
Training loss: 2.4905405372455944
Validation loss: 2.499570106132879

Epoch: 6| Step: 6
Training loss: 2.029151654302168
Validation loss: 2.4927107381637885

Epoch: 6| Step: 7
Training loss: 3.219491225144013
Validation loss: 2.4818592724420174

Epoch: 6| Step: 8
Training loss: 1.9437160452179907
Validation loss: 2.488550628346672

Epoch: 6| Step: 9
Training loss: 3.0806620255433734
Validation loss: 2.49522951853062

Epoch: 6| Step: 10
Training loss: 2.0542671730982898
Validation loss: 2.508591918840406

Epoch: 6| Step: 11
Training loss: 3.3515845522566328
Validation loss: 2.5108158595997896

Epoch: 6| Step: 12
Training loss: 2.9258793609426275
Validation loss: 2.458497547889308

Epoch: 6| Step: 13
Training loss: 2.9379780157213253
Validation loss: 2.478034922326889

Epoch: 140| Step: 0
Training loss: 2.941414517698789
Validation loss: 2.4876304343684645

Epoch: 6| Step: 1
Training loss: 2.8230321562581135
Validation loss: 2.4669837079703685

Epoch: 6| Step: 2
Training loss: 2.460135192877889
Validation loss: 2.4784889118579967

Epoch: 6| Step: 3
Training loss: 2.896684882747263
Validation loss: 2.491487965307937

Epoch: 6| Step: 4
Training loss: 2.7933679682178427
Validation loss: 2.503630813362473

Epoch: 6| Step: 5
Training loss: 2.36292812767292
Validation loss: 2.4771195495547262

Epoch: 6| Step: 6
Training loss: 2.769273151374972
Validation loss: 2.5003414587411155

Epoch: 6| Step: 7
Training loss: 2.6046882615663254
Validation loss: 2.495453387737769

Epoch: 6| Step: 8
Training loss: 2.314111405911242
Validation loss: 2.497228022932167

Epoch: 6| Step: 9
Training loss: 2.4695473843910025
Validation loss: 2.4871267613275987

Epoch: 6| Step: 10
Training loss: 1.6648119461122235
Validation loss: 2.505114610251001

Epoch: 6| Step: 11
Training loss: 2.4001909736310365
Validation loss: 2.494150733007011

Epoch: 6| Step: 12
Training loss: 2.803383781880305
Validation loss: 2.5028904602380684

Epoch: 6| Step: 13
Training loss: 3.300253199487139
Validation loss: 2.506464265106289

Epoch: 141| Step: 0
Training loss: 2.228600997693707
Validation loss: 2.498826366707235

Epoch: 6| Step: 1
Training loss: 2.9130301193634027
Validation loss: 2.5090167050252465

Epoch: 6| Step: 2
Training loss: 3.29311442024702
Validation loss: 2.4872384730170247

Epoch: 6| Step: 3
Training loss: 2.544060960363164
Validation loss: 2.5121080183702813

Epoch: 6| Step: 4
Training loss: 2.720759821819288
Validation loss: 2.4673821307417163

Epoch: 6| Step: 5
Training loss: 2.8098686624697584
Validation loss: 2.492202080337986

Epoch: 6| Step: 6
Training loss: 3.1066793901021943
Validation loss: 2.4913256426611645

Epoch: 6| Step: 7
Training loss: 2.0890087135027304
Validation loss: 2.5099661675662612

Epoch: 6| Step: 8
Training loss: 2.3331424998401933
Validation loss: 2.4929221620465

Epoch: 6| Step: 9
Training loss: 2.689723049073825
Validation loss: 2.495711426006582

Epoch: 6| Step: 10
Training loss: 2.7271585332198276
Validation loss: 2.4974425442573738

Epoch: 6| Step: 11
Training loss: 2.2998767363647827
Validation loss: 2.4760080803021935

Epoch: 6| Step: 12
Training loss: 2.307318062268922
Validation loss: 2.479996424902973

Epoch: 6| Step: 13
Training loss: 2.1040071357707575
Validation loss: 2.491055040245889

Epoch: 142| Step: 0
Training loss: 2.5676031703705355
Validation loss: 2.494820461140848

Epoch: 6| Step: 1
Training loss: 2.0566339012680928
Validation loss: 2.5100029514394913

Epoch: 6| Step: 2
Training loss: 2.735891564646417
Validation loss: 2.506918588534759

Epoch: 6| Step: 3
Training loss: 2.08904249571165
Validation loss: 2.492377284603058

Epoch: 6| Step: 4
Training loss: 2.9211792142924335
Validation loss: 2.4776225283989133

Epoch: 6| Step: 5
Training loss: 2.8152494766509935
Validation loss: 2.4824609527244954

Epoch: 6| Step: 6
Training loss: 2.632617796085507
Validation loss: 2.5079269922287457

Epoch: 6| Step: 7
Training loss: 3.2558149988071374
Validation loss: 2.5099085424318783

Epoch: 6| Step: 8
Training loss: 2.0129743787003846
Validation loss: 2.4739248656689896

Epoch: 6| Step: 9
Training loss: 3.088906552319905
Validation loss: 2.5213783676333197

Epoch: 6| Step: 10
Training loss: 2.9039729232051212
Validation loss: 2.4813887958426775

Epoch: 6| Step: 11
Training loss: 2.1168066153730734
Validation loss: 2.480978222119146

Epoch: 6| Step: 12
Training loss: 2.5722801770006467
Validation loss: 2.4549607498255637

Epoch: 6| Step: 13
Training loss: 2.761522774440298
Validation loss: 2.468972868573471

Epoch: 143| Step: 0
Training loss: 2.806160967461627
Validation loss: 2.486209651417704

Epoch: 6| Step: 1
Training loss: 2.8684772636067057
Validation loss: 2.477002420867393

Epoch: 6| Step: 2
Training loss: 2.6786699058958714
Validation loss: 2.5131915600282544

Epoch: 6| Step: 3
Training loss: 2.151105410267936
Validation loss: 2.4763761743826973

Epoch: 6| Step: 4
Training loss: 2.5873236038600407
Validation loss: 2.5027332940795652

Epoch: 6| Step: 5
Training loss: 3.366244934282199
Validation loss: 2.4947274125914487

Epoch: 6| Step: 6
Training loss: 3.005860009543468
Validation loss: 2.4792499953925877

Epoch: 6| Step: 7
Training loss: 2.2915677367155367
Validation loss: 2.4986186554108345

Epoch: 6| Step: 8
Training loss: 2.664938416333087
Validation loss: 2.514696559682505

Epoch: 6| Step: 9
Training loss: 2.028822754444413
Validation loss: 2.5011139848860298

Epoch: 6| Step: 10
Training loss: 2.467509573750731
Validation loss: 2.506180336990857

Epoch: 6| Step: 11
Training loss: 2.3242585507359
Validation loss: 2.486026888828677

Epoch: 6| Step: 12
Training loss: 2.461853728714542
Validation loss: 2.491962193411874

Epoch: 6| Step: 13
Training loss: 2.569235903780044
Validation loss: 2.4829053658945486

Epoch: 144| Step: 0
Training loss: 2.0968302373755865
Validation loss: 2.5189785670954694

Epoch: 6| Step: 1
Training loss: 2.3418356643276748
Validation loss: 2.525121115703117

Epoch: 6| Step: 2
Training loss: 2.481812793314264
Validation loss: 2.475096565062346

Epoch: 6| Step: 3
Training loss: 2.2680081165982555
Validation loss: 2.4936775800866613

Epoch: 6| Step: 4
Training loss: 2.787019039793921
Validation loss: 2.497220808015368

Epoch: 6| Step: 5
Training loss: 2.6204500274871902
Validation loss: 2.499583696753773

Epoch: 6| Step: 6
Training loss: 3.2621513034198473
Validation loss: 2.4909319573868016

Epoch: 6| Step: 7
Training loss: 2.2940169218082773
Validation loss: 2.4970110175973756

Epoch: 6| Step: 8
Training loss: 2.3742493898925767
Validation loss: 2.48542875980761

Epoch: 6| Step: 9
Training loss: 3.4192561749475563
Validation loss: 2.4861798295177486

Epoch: 6| Step: 10
Training loss: 2.636144595329348
Validation loss: 2.504940970810174

Epoch: 6| Step: 11
Training loss: 1.9137229774007436
Validation loss: 2.4835023302478296

Epoch: 6| Step: 12
Training loss: 2.4207129859446463
Validation loss: 2.4698418284566497

Epoch: 6| Step: 13
Training loss: 3.6044792230869533
Validation loss: 2.498600609137125

Epoch: 145| Step: 0
Training loss: 2.129808483199807
Validation loss: 2.502644091745838

Epoch: 6| Step: 1
Training loss: 2.5904815486861223
Validation loss: 2.5015285997263197

Epoch: 6| Step: 2
Training loss: 2.584346039504829
Validation loss: 2.4936165683185054

Epoch: 6| Step: 3
Training loss: 2.41607917572542
Validation loss: 2.486448205492606

Epoch: 6| Step: 4
Training loss: 2.61381965416692
Validation loss: 2.5002923866256164

Epoch: 6| Step: 5
Training loss: 2.4859330192252203
Validation loss: 2.4716425552515875

Epoch: 6| Step: 6
Training loss: 3.0417553266032007
Validation loss: 2.5040599609504968

Epoch: 6| Step: 7
Training loss: 2.3194401703552137
Validation loss: 2.473863120111555

Epoch: 6| Step: 8
Training loss: 2.6059053847262965
Validation loss: 2.510468667057303

Epoch: 6| Step: 9
Training loss: 2.590223741919436
Validation loss: 2.4937998283902436

Epoch: 6| Step: 10
Training loss: 3.265686180600269
Validation loss: 2.4969156702238156

Epoch: 6| Step: 11
Training loss: 2.4255737609090753
Validation loss: 2.485570356754956

Epoch: 6| Step: 12
Training loss: 2.097996746253444
Validation loss: 2.503333111456815

Epoch: 6| Step: 13
Training loss: 3.3476364509059344
Validation loss: 2.4889845864142246

Epoch: 146| Step: 0
Training loss: 2.7488721788981767
Validation loss: 2.519199470003944

Epoch: 6| Step: 1
Training loss: 2.491662427543513
Validation loss: 2.50243914783615

Epoch: 6| Step: 2
Training loss: 2.7770824282607265
Validation loss: 2.5075174395186917

Epoch: 6| Step: 3
Training loss: 2.761284649736444
Validation loss: 2.516646899975777

Epoch: 6| Step: 4
Training loss: 2.379411214718727
Validation loss: 2.496454408266471

Epoch: 6| Step: 5
Training loss: 2.2772287958095814
Validation loss: 2.4780412475318734

Epoch: 6| Step: 6
Training loss: 3.1416560516043175
Validation loss: 2.504517656229316

Epoch: 6| Step: 7
Training loss: 2.8768321294778896
Validation loss: 2.5058109869367873

Epoch: 6| Step: 8
Training loss: 2.3523414310277926
Validation loss: 2.4971258972729875

Epoch: 6| Step: 9
Training loss: 2.7540915136043713
Validation loss: 2.506062642333637

Epoch: 6| Step: 10
Training loss: 2.01520717341474
Validation loss: 2.497471925775856

Epoch: 6| Step: 11
Training loss: 2.600536247119622
Validation loss: 2.4989028348433435

Epoch: 6| Step: 12
Training loss: 2.795090175122558
Validation loss: 2.5256832442350885

Epoch: 6| Step: 13
Training loss: 2.1973327897720836
Validation loss: 2.5172791293246886

Epoch: 147| Step: 0
Training loss: 2.7579207358585056
Validation loss: 2.5224586040162444

Epoch: 6| Step: 1
Training loss: 2.9300009597932983
Validation loss: 2.4954502451498604

Epoch: 6| Step: 2
Training loss: 3.1556373228156263
Validation loss: 2.491561660367544

Epoch: 6| Step: 3
Training loss: 2.4823625198819665
Validation loss: 2.475378686049275

Epoch: 6| Step: 4
Training loss: 2.7603867055358604
Validation loss: 2.480032528608687

Epoch: 6| Step: 5
Training loss: 2.9466714734114485
Validation loss: 2.4886613356141827

Epoch: 6| Step: 6
Training loss: 2.38998623959718
Validation loss: 2.511672479456608

Epoch: 6| Step: 7
Training loss: 2.423523569454033
Validation loss: 2.5111802919623742

Epoch: 6| Step: 8
Training loss: 2.67523392697272
Validation loss: 2.495054115885483

Epoch: 6| Step: 9
Training loss: 2.6407741527079427
Validation loss: 2.4942024852788927

Epoch: 6| Step: 10
Training loss: 2.383344947235336
Validation loss: 2.5029480441478977

Epoch: 6| Step: 11
Training loss: 1.7254010605016092
Validation loss: 2.481285343593644

Epoch: 6| Step: 12
Training loss: 2.1928988726565213
Validation loss: 2.4978344408200837

Epoch: 6| Step: 13
Training loss: 2.356615182924833
Validation loss: 2.508304513309124

Epoch: 148| Step: 0
Training loss: 2.508626550242005
Validation loss: 2.490495840361157

Epoch: 6| Step: 1
Training loss: 2.546927235073526
Validation loss: 2.496656856915454

Epoch: 6| Step: 2
Training loss: 1.9429619617596663
Validation loss: 2.4778649509671866

Epoch: 6| Step: 3
Training loss: 3.243630550000641
Validation loss: 2.4886083668800305

Epoch: 6| Step: 4
Training loss: 2.02795107056018
Validation loss: 2.502658173247466

Epoch: 6| Step: 5
Training loss: 2.2943022975142466
Validation loss: 2.5079290146803435

Epoch: 6| Step: 6
Training loss: 2.930046201999144
Validation loss: 2.5080746217987073

Epoch: 6| Step: 7
Training loss: 2.6769110974272294
Validation loss: 2.4836763687078505

Epoch: 6| Step: 8
Training loss: 2.474740594698233
Validation loss: 2.499050111690825

Epoch: 6| Step: 9
Training loss: 1.9986743706093884
Validation loss: 2.504252532198844

Epoch: 6| Step: 10
Training loss: 3.2622382749612373
Validation loss: 2.4972416129536135

Epoch: 6| Step: 11
Training loss: 2.7614870311426314
Validation loss: 2.4932926749346698

Epoch: 6| Step: 12
Training loss: 2.810263528650626
Validation loss: 2.4849805815527226

Epoch: 6| Step: 13
Training loss: 2.611206508363766
Validation loss: 2.494044072658653

Epoch: 149| Step: 0
Training loss: 2.903237370840578
Validation loss: 2.4742253258008886

Epoch: 6| Step: 1
Training loss: 2.121462514164492
Validation loss: 2.479137531818353

Epoch: 6| Step: 2
Training loss: 3.280148566533271
Validation loss: 2.507164417354709

Epoch: 6| Step: 3
Training loss: 2.3641902468014897
Validation loss: 2.4968662237834285

Epoch: 6| Step: 4
Training loss: 2.6681480961807718
Validation loss: 2.500116614216365

Epoch: 6| Step: 5
Training loss: 2.504198554201473
Validation loss: 2.4837097889227735

Epoch: 6| Step: 6
Training loss: 2.3238625646191475
Validation loss: 2.4858897193792595

Epoch: 6| Step: 7
Training loss: 3.295744932767892
Validation loss: 2.489085593873235

Epoch: 6| Step: 8
Training loss: 2.9052378214728662
Validation loss: 2.497316497388098

Epoch: 6| Step: 9
Training loss: 1.974810883364514
Validation loss: 2.5081487606605712

Epoch: 6| Step: 10
Training loss: 2.065309633774514
Validation loss: 2.480436618355552

Epoch: 6| Step: 11
Training loss: 2.433014875850207
Validation loss: 2.5337098875879924

Epoch: 6| Step: 12
Training loss: 2.699833889549216
Validation loss: 2.504182349672509

Epoch: 6| Step: 13
Training loss: 2.366105842854166
Validation loss: 2.486445936676477

Epoch: 150| Step: 0
Training loss: 2.190669352345011
Validation loss: 2.499819820843231

Epoch: 6| Step: 1
Training loss: 2.257947026769185
Validation loss: 2.5135983722806823

Epoch: 6| Step: 2
Training loss: 3.2935416921221115
Validation loss: 2.5143571187298

Epoch: 6| Step: 3
Training loss: 3.224028028352126
Validation loss: 2.468424562598749

Epoch: 6| Step: 4
Training loss: 2.5309546027481162
Validation loss: 2.502315826842647

Epoch: 6| Step: 5
Training loss: 2.747089666629964
Validation loss: 2.534033683993495

Epoch: 6| Step: 6
Training loss: 2.1404588175400527
Validation loss: 2.4892380768020743

Epoch: 6| Step: 7
Training loss: 2.2831202117510485
Validation loss: 2.488060563315557

Epoch: 6| Step: 8
Training loss: 2.843142434077417
Validation loss: 2.4848448647692654

Epoch: 6| Step: 9
Training loss: 3.058163901338306
Validation loss: 2.479816080439168

Epoch: 6| Step: 10
Training loss: 2.4238654048490345
Validation loss: 2.4860136891777014

Epoch: 6| Step: 11
Training loss: 2.1901891255692325
Validation loss: 2.481897343649053

Epoch: 6| Step: 12
Training loss: 2.5417896824651867
Validation loss: 2.529858988719593

Epoch: 6| Step: 13
Training loss: 1.877382671890925
Validation loss: 2.4700048291652075

Epoch: 151| Step: 0
Training loss: 2.7450299868309407
Validation loss: 2.490926871385152

Epoch: 6| Step: 1
Training loss: 2.347609127542883
Validation loss: 2.5041461598190664

Epoch: 6| Step: 2
Training loss: 2.2959659360425566
Validation loss: 2.5128901744406558

Epoch: 6| Step: 3
Training loss: 2.5386943843728202
Validation loss: 2.4904966195938005

Epoch: 6| Step: 4
Training loss: 2.4709285346780727
Validation loss: 2.50663793556159

Epoch: 6| Step: 5
Training loss: 2.929005128865751
Validation loss: 2.4847528602786326

Epoch: 6| Step: 6
Training loss: 2.7836906782389197
Validation loss: 2.509352050352655

Epoch: 6| Step: 7
Training loss: 2.2985559657288945
Validation loss: 2.4967072110940847

Epoch: 6| Step: 8
Training loss: 2.818779949020671
Validation loss: 2.4852527706678096

Epoch: 6| Step: 9
Training loss: 3.2070452263512443
Validation loss: 2.499971859271092

Epoch: 6| Step: 10
Training loss: 2.1476554418224425
Validation loss: 2.5066044947858748

Epoch: 6| Step: 11
Training loss: 2.569521704076408
Validation loss: 2.496915165076071

Epoch: 6| Step: 12
Training loss: 2.2798323733631745
Validation loss: 2.5050369506636296

Epoch: 6| Step: 13
Training loss: 2.5010749413245463
Validation loss: 2.492469230849739

Epoch: 152| Step: 0
Training loss: 2.7480427105512155
Validation loss: 2.507862381785163

Epoch: 6| Step: 1
Training loss: 2.3067442945784147
Validation loss: 2.4796630625759937

Epoch: 6| Step: 2
Training loss: 2.040443858684158
Validation loss: 2.48460790876527

Epoch: 6| Step: 3
Training loss: 2.249368261023515
Validation loss: 2.506351658183233

Epoch: 6| Step: 4
Training loss: 2.4900369483627
Validation loss: 2.4869471730847668

Epoch: 6| Step: 5
Training loss: 2.642351207026733
Validation loss: 2.5167643400635313

Epoch: 6| Step: 6
Training loss: 1.842080669848399
Validation loss: 2.5036851870681445

Epoch: 6| Step: 7
Training loss: 2.506009223007681
Validation loss: 2.476384204716849

Epoch: 6| Step: 8
Training loss: 3.0157753704312764
Validation loss: 2.485458138941639

Epoch: 6| Step: 9
Training loss: 2.1863358806058844
Validation loss: 2.4945868204798605

Epoch: 6| Step: 10
Training loss: 2.4273894503060345
Validation loss: 2.4901107759342933

Epoch: 6| Step: 11
Training loss: 3.062467925234617
Validation loss: 2.4747123164292995

Epoch: 6| Step: 12
Training loss: 2.8265622705698843
Validation loss: 2.494322445177052

Epoch: 6| Step: 13
Training loss: 3.656353158392154
Validation loss: 2.4985990341796858

Epoch: 153| Step: 0
Training loss: 2.727021973815504
Validation loss: 2.499157502126735

Epoch: 6| Step: 1
Training loss: 2.435816354534717
Validation loss: 2.482538238886187

Epoch: 6| Step: 2
Training loss: 2.275148986297617
Validation loss: 2.502251121203459

Epoch: 6| Step: 3
Training loss: 3.3380294779852435
Validation loss: 2.502557154273821

Epoch: 6| Step: 4
Training loss: 2.899778811471065
Validation loss: 2.49395223073018

Epoch: 6| Step: 5
Training loss: 2.713066562250844
Validation loss: 2.488522761978728

Epoch: 6| Step: 6
Training loss: 2.283606788057738
Validation loss: 2.4930854275552106

Epoch: 6| Step: 7
Training loss: 2.9007159631822037
Validation loss: 2.49368066630939

Epoch: 6| Step: 8
Training loss: 2.57384751093018
Validation loss: 2.509192147709384

Epoch: 6| Step: 9
Training loss: 2.3529370115046944
Validation loss: 2.4981876284793483

Epoch: 6| Step: 10
Training loss: 2.1582538305341616
Validation loss: 2.4878087511378855

Epoch: 6| Step: 11
Training loss: 1.4890183753473938
Validation loss: 2.520519360771721

Epoch: 6| Step: 12
Training loss: 2.5106525442233023
Validation loss: 2.482896662798134

Epoch: 6| Step: 13
Training loss: 2.959153093116917
Validation loss: 2.5020697322047263

Epoch: 154| Step: 0
Training loss: 2.975960578031825
Validation loss: 2.4909439155091246

Epoch: 6| Step: 1
Training loss: 2.555339018544276
Validation loss: 2.5002303868446614

Epoch: 6| Step: 2
Training loss: 2.1748046875
Validation loss: 2.4934189354971377

Epoch: 6| Step: 3
Training loss: 2.4459949065285627
Validation loss: 2.489723891185723

Epoch: 6| Step: 4
Training loss: 2.475346215433507
Validation loss: 2.478940605128029

Epoch: 6| Step: 5
Training loss: 2.910918720899403
Validation loss: 2.491424095419098

Epoch: 6| Step: 6
Training loss: 2.00013064911405
Validation loss: 2.4916883965318197

Epoch: 6| Step: 7
Training loss: 2.483364931541321
Validation loss: 2.4863935656797627

Epoch: 6| Step: 8
Training loss: 2.7562342982807255
Validation loss: 2.507248767678849

Epoch: 6| Step: 9
Training loss: 2.450827524990483
Validation loss: 2.5058250173921697

Epoch: 6| Step: 10
Training loss: 2.7485429198154923
Validation loss: 2.5171591395929176

Epoch: 6| Step: 11
Training loss: 2.2481851676077667
Validation loss: 2.5055024014073695

Epoch: 6| Step: 12
Training loss: 3.0284927255115806
Validation loss: 2.49202921030337

Epoch: 6| Step: 13
Training loss: 2.951023868103609
Validation loss: 2.495108260885332

Epoch: 155| Step: 0
Training loss: 2.9750804665846955
Validation loss: 2.521458493272714

Epoch: 6| Step: 1
Training loss: 2.8028802601133282
Validation loss: 2.515966778549211

Epoch: 6| Step: 2
Training loss: 2.644608962749868
Validation loss: 2.509934295059375

Epoch: 6| Step: 3
Training loss: 2.377953951044076
Validation loss: 2.4920509268297204

Epoch: 6| Step: 4
Training loss: 2.317812205069416
Validation loss: 2.509284423367073

Epoch: 6| Step: 5
Training loss: 2.4147877020550053
Validation loss: 2.4939516694732613

Epoch: 6| Step: 6
Training loss: 2.513013348872976
Validation loss: 2.4921259291743376

Epoch: 6| Step: 7
Training loss: 2.431065993222586
Validation loss: 2.509221090759823

Epoch: 6| Step: 8
Training loss: 2.7103210100031334
Validation loss: 2.4975793501974466

Epoch: 6| Step: 9
Training loss: 2.1801317644107825
Validation loss: 2.4772088901819505

Epoch: 6| Step: 10
Training loss: 3.3636507015250734
Validation loss: 2.4847141157478374

Epoch: 6| Step: 11
Training loss: 2.1298290807355817
Validation loss: 2.51840919870566

Epoch: 6| Step: 12
Training loss: 2.76089521374619
Validation loss: 2.473342702504175

Epoch: 6| Step: 13
Training loss: 2.258745997076493
Validation loss: 2.515536656857431

Epoch: 156| Step: 0
Training loss: 2.195416213874338
Validation loss: 2.5003268499930367

Epoch: 6| Step: 1
Training loss: 2.636497567401229
Validation loss: 2.495960707130003

Epoch: 6| Step: 2
Training loss: 2.0883726842795145
Validation loss: 2.4868954553846745

Epoch: 6| Step: 3
Training loss: 2.416644052421609
Validation loss: 2.4925876228981863

Epoch: 6| Step: 4
Training loss: 2.781353209220015
Validation loss: 2.504851381928692

Epoch: 6| Step: 5
Training loss: 2.5201568536534786
Validation loss: 2.5068015656644995

Epoch: 6| Step: 6
Training loss: 2.80372343788031
Validation loss: 2.5016757404375194

Epoch: 6| Step: 7
Training loss: 3.2142524778070665
Validation loss: 2.5199022688131

Epoch: 6| Step: 8
Training loss: 2.5465099798636497
Validation loss: 2.4739772296350093

Epoch: 6| Step: 9
Training loss: 2.701994025392513
Validation loss: 2.464843374010179

Epoch: 6| Step: 10
Training loss: 2.5375233841744067
Validation loss: 2.497443122180383

Epoch: 6| Step: 11
Training loss: 2.6087018818020726
Validation loss: 2.5200048203982965

Epoch: 6| Step: 12
Training loss: 2.517037699887272
Validation loss: 2.4784774097891837

Epoch: 6| Step: 13
Training loss: 2.2877500762155742
Validation loss: 2.467382005021212

Epoch: 157| Step: 0
Training loss: 2.396312455630477
Validation loss: 2.4984415447091526

Epoch: 6| Step: 1
Training loss: 2.36652750313528
Validation loss: 2.5233165727611597

Epoch: 6| Step: 2
Training loss: 2.572864598868372
Validation loss: 2.5197813591171037

Epoch: 6| Step: 3
Training loss: 2.5731351710728414
Validation loss: 2.472128225886034

Epoch: 6| Step: 4
Training loss: 2.3816122064933074
Validation loss: 2.5094963622929236

Epoch: 6| Step: 5
Training loss: 2.585112287344109
Validation loss: 2.4814922458450446

Epoch: 6| Step: 6
Training loss: 2.6543427744778194
Validation loss: 2.4987968061841324

Epoch: 6| Step: 7
Training loss: 2.6313317546693162
Validation loss: 2.5172573727817085

Epoch: 6| Step: 8
Training loss: 2.932313113814541
Validation loss: 2.4915300936475484

Epoch: 6| Step: 9
Training loss: 2.651193810514547
Validation loss: 2.499660457605533

Epoch: 6| Step: 10
Training loss: 3.2838708492443187
Validation loss: 2.494122237449627

Epoch: 6| Step: 11
Training loss: 1.8156535239941856
Validation loss: 2.508091158165335

Epoch: 6| Step: 12
Training loss: 2.5832549872874924
Validation loss: 2.491929707923846

Epoch: 6| Step: 13
Training loss: 2.4757343435854633
Validation loss: 2.476608743310592

Epoch: 158| Step: 0
Training loss: 2.2095645375405173
Validation loss: 2.5135189425825444

Epoch: 6| Step: 1
Training loss: 3.2851653943745354
Validation loss: 2.5016539096806927

Epoch: 6| Step: 2
Training loss: 2.322725216854959
Validation loss: 2.482578836908915

Epoch: 6| Step: 3
Training loss: 2.0769949687296245
Validation loss: 2.5023484858707343

Epoch: 6| Step: 4
Training loss: 2.219571351088868
Validation loss: 2.4678881069948013

Epoch: 6| Step: 5
Training loss: 2.212794841366738
Validation loss: 2.48569334206365

Epoch: 6| Step: 6
Training loss: 2.55864295038573
Validation loss: 2.518733393111246

Epoch: 6| Step: 7
Training loss: 2.4908770999613403
Validation loss: 2.497897524337278

Epoch: 6| Step: 8
Training loss: 2.4458232504676367
Validation loss: 2.4878601778109144

Epoch: 6| Step: 9
Training loss: 2.767416422339177
Validation loss: 2.5061264813283564

Epoch: 6| Step: 10
Training loss: 2.403206225387901
Validation loss: 2.4895928825009586

Epoch: 6| Step: 11
Training loss: 2.4333446981434648
Validation loss: 2.5047706922888637

Epoch: 6| Step: 12
Training loss: 3.4409711478281166
Validation loss: 2.529402795777436

Epoch: 6| Step: 13
Training loss: 3.0901742944047355
Validation loss: 2.5076470803363113

Epoch: 159| Step: 0
Training loss: 2.4334346419195523
Validation loss: 2.4979396123141058

Epoch: 6| Step: 1
Training loss: 2.2988582016007197
Validation loss: 2.4964587983142295

Epoch: 6| Step: 2
Training loss: 2.6172671177838382
Validation loss: 2.4923329683288773

Epoch: 6| Step: 3
Training loss: 2.9160179143284717
Validation loss: 2.5113666292514742

Epoch: 6| Step: 4
Training loss: 2.2268033081471406
Validation loss: 2.475406675010598

Epoch: 6| Step: 5
Training loss: 2.3192377651920295
Validation loss: 2.4993841417649048

Epoch: 6| Step: 6
Training loss: 2.0342467283719765
Validation loss: 2.4981296895705776

Epoch: 6| Step: 7
Training loss: 2.8558499646674225
Validation loss: 2.480963013691202

Epoch: 6| Step: 8
Training loss: 2.899666826248676
Validation loss: 2.493729303112544

Epoch: 6| Step: 9
Training loss: 2.795779562450589
Validation loss: 2.4795677527247584

Epoch: 6| Step: 10
Training loss: 2.7034523826315584
Validation loss: 2.4994289689460185

Epoch: 6| Step: 11
Training loss: 3.0205182316745613
Validation loss: 2.4949608469115416

Epoch: 6| Step: 12
Training loss: 2.6693996987599107
Validation loss: 2.4973842595730953

Epoch: 6| Step: 13
Training loss: 2.1043307426110793
Validation loss: 2.5012616214596086

Epoch: 160| Step: 0
Training loss: 2.4957009545204993
Validation loss: 2.4860456239629483

Epoch: 6| Step: 1
Training loss: 2.926779481224007
Validation loss: 2.536769468757634

Epoch: 6| Step: 2
Training loss: 2.7593903175006624
Validation loss: 2.4981661802900677

Epoch: 6| Step: 3
Training loss: 2.3146922056434316
Validation loss: 2.4791213214381402

Epoch: 6| Step: 4
Training loss: 2.3108309315631033
Validation loss: 2.5024004640508926

Epoch: 6| Step: 5
Training loss: 2.4196315095481564
Validation loss: 2.5354523286060613

Epoch: 6| Step: 6
Training loss: 2.392909734672626
Validation loss: 2.4893964657078884

Epoch: 6| Step: 7
Training loss: 2.678291691296337
Validation loss: 2.498029469603373

Epoch: 6| Step: 8
Training loss: 2.2814443845104484
Validation loss: 2.5022872992764458

Epoch: 6| Step: 9
Training loss: 2.3062203175358458
Validation loss: 2.489778058401897

Epoch: 6| Step: 10
Training loss: 2.2170131493825003
Validation loss: 2.5072231132350424

Epoch: 6| Step: 11
Training loss: 2.5147627780436834
Validation loss: 2.465916688071886

Epoch: 6| Step: 12
Training loss: 3.2124788988659803
Validation loss: 2.4970104062071274

Epoch: 6| Step: 13
Training loss: 3.2472017419359753
Validation loss: 2.5257358302874766

Epoch: 161| Step: 0
Training loss: 2.0794080631068588
Validation loss: 2.486844602549152

Epoch: 6| Step: 1
Training loss: 2.379856364620173
Validation loss: 2.486421775537951

Epoch: 6| Step: 2
Training loss: 2.732881846783092
Validation loss: 2.5160838438406223

Epoch: 6| Step: 3
Training loss: 2.6919451479942103
Validation loss: 2.509008418448339

Epoch: 6| Step: 4
Training loss: 2.599890735971275
Validation loss: 2.4804382513546948

Epoch: 6| Step: 5
Training loss: 2.8688604063509135
Validation loss: 2.5040971602157103

Epoch: 6| Step: 6
Training loss: 2.3040377460672676
Validation loss: 2.488034766660672

Epoch: 6| Step: 7
Training loss: 2.5603393191228836
Validation loss: 2.514727792783013

Epoch: 6| Step: 8
Training loss: 2.049958563595195
Validation loss: 2.501095427200862

Epoch: 6| Step: 9
Training loss: 3.0935702801531875
Validation loss: 2.525411431857879

Epoch: 6| Step: 10
Training loss: 2.2183203482683704
Validation loss: 2.512088242778066

Epoch: 6| Step: 11
Training loss: 2.34268652630318
Validation loss: 2.507294112212062

Epoch: 6| Step: 12
Training loss: 2.9257700045156882
Validation loss: 2.510789779067998

Epoch: 6| Step: 13
Training loss: 3.0364164389876747
Validation loss: 2.4945228259342658

Epoch: 162| Step: 0
Training loss: 2.7835274276519573
Validation loss: 2.5223102516076126

Epoch: 6| Step: 1
Training loss: 1.9632158759602976
Validation loss: 2.496568956718719

Epoch: 6| Step: 2
Training loss: 2.426407541214128
Validation loss: 2.4955810891357837

Epoch: 6| Step: 3
Training loss: 2.775523516099554
Validation loss: 2.4867291515157635

Epoch: 6| Step: 4
Training loss: 3.0756260366424506
Validation loss: 2.50087234392617

Epoch: 6| Step: 5
Training loss: 2.1121968926566455
Validation loss: 2.499899719647128

Epoch: 6| Step: 6
Training loss: 2.206600396965805
Validation loss: 2.4979333477583157

Epoch: 6| Step: 7
Training loss: 2.634957817652052
Validation loss: 2.4898728934982493

Epoch: 6| Step: 8
Training loss: 1.7410087532248486
Validation loss: 2.48930070566614

Epoch: 6| Step: 9
Training loss: 2.561556014196681
Validation loss: 2.4791463711670394

Epoch: 6| Step: 10
Training loss: 2.9340056068039195
Validation loss: 2.487259902575717

Epoch: 6| Step: 11
Training loss: 3.4050103127747704
Validation loss: 2.5094855100702365

Epoch: 6| Step: 12
Training loss: 2.39044388228331
Validation loss: 2.4598514189720797

Epoch: 6| Step: 13
Training loss: 2.1959392698190445
Validation loss: 2.4929434131744865

Epoch: 163| Step: 0
Training loss: 2.6195192795823274
Validation loss: 2.4849140069842273

Epoch: 6| Step: 1
Training loss: 1.997275583028636
Validation loss: 2.4909954524143267

Epoch: 6| Step: 2
Training loss: 2.180233029151097
Validation loss: 2.4750746220931803

Epoch: 6| Step: 3
Training loss: 2.424594853171632
Validation loss: 2.4847515159096596

Epoch: 6| Step: 4
Training loss: 2.675928977730886
Validation loss: 2.513905989512963

Epoch: 6| Step: 5
Training loss: 2.7260627179225203
Validation loss: 2.521544705356715

Epoch: 6| Step: 6
Training loss: 2.9019086673384846
Validation loss: 2.4962639405365454

Epoch: 6| Step: 7
Training loss: 2.9869610032868272
Validation loss: 2.513556146179463

Epoch: 6| Step: 8
Training loss: 2.3858423200307697
Validation loss: 2.5278508464898164

Epoch: 6| Step: 9
Training loss: 3.0683080910537615
Validation loss: 2.4890246405211935

Epoch: 6| Step: 10
Training loss: 2.7965551438524185
Validation loss: 2.5142968646960746

Epoch: 6| Step: 11
Training loss: 2.1278250372241807
Validation loss: 2.4880461338727637

Epoch: 6| Step: 12
Training loss: 2.437535407958471
Validation loss: 2.5192964483057563

Epoch: 6| Step: 13
Training loss: 2.5199151271575384
Validation loss: 2.5084393328404864

Epoch: 164| Step: 0
Training loss: 2.5424456314680453
Validation loss: 2.511772406146301

Epoch: 6| Step: 1
Training loss: 2.7996077160559563
Validation loss: 2.5007530021797533

Epoch: 6| Step: 2
Training loss: 2.8616249112826675
Validation loss: 2.499151225236729

Epoch: 6| Step: 3
Training loss: 3.068703421493663
Validation loss: 2.494455916118769

Epoch: 6| Step: 4
Training loss: 2.348267639067916
Validation loss: 2.5099878586159767

Epoch: 6| Step: 5
Training loss: 2.111460805953287
Validation loss: 2.489708662042806

Epoch: 6| Step: 6
Training loss: 2.905252264891338
Validation loss: 2.5074178902543514

Epoch: 6| Step: 7
Training loss: 1.8567856141117802
Validation loss: 2.520064624630632

Epoch: 6| Step: 8
Training loss: 1.9624771821463165
Validation loss: 2.495232822700743

Epoch: 6| Step: 9
Training loss: 2.408097981851601
Validation loss: 2.507814127951207

Epoch: 6| Step: 10
Training loss: 2.220059290601082
Validation loss: 2.530936049125073

Epoch: 6| Step: 11
Training loss: 2.5764301624630854
Validation loss: 2.522803045953315

Epoch: 6| Step: 12
Training loss: 3.0418512645427924
Validation loss: 2.4999370464479247

Epoch: 6| Step: 13
Training loss: 3.044398000305666
Validation loss: 2.511010454923897

Epoch: 165| Step: 0
Training loss: 2.822318507560349
Validation loss: 2.5215099504323817

Epoch: 6| Step: 1
Training loss: 2.493396238699662
Validation loss: 2.5086302414503123

Epoch: 6| Step: 2
Training loss: 1.672708820370345
Validation loss: 2.5142975407075423

Epoch: 6| Step: 3
Training loss: 2.746576085063008
Validation loss: 2.4924471363709415

Epoch: 6| Step: 4
Training loss: 2.5684858051700155
Validation loss: 2.5036895639252568

Epoch: 6| Step: 5
Training loss: 2.3177547035864934
Validation loss: 2.497725475468344

Epoch: 6| Step: 6
Training loss: 2.9372422125499877
Validation loss: 2.513895133890835

Epoch: 6| Step: 7
Training loss: 2.5038921575997732
Validation loss: 2.499943703868706

Epoch: 6| Step: 8
Training loss: 2.5204637324821424
Validation loss: 2.514378616921648

Epoch: 6| Step: 9
Training loss: 2.3437400309032776
Validation loss: 2.492324829432547

Epoch: 6| Step: 10
Training loss: 2.7000565063780195
Validation loss: 2.5178749714367172

Epoch: 6| Step: 11
Training loss: 2.8006349150284855
Validation loss: 2.516583605960763

Epoch: 6| Step: 12
Training loss: 2.4738863857582247
Validation loss: 2.4903173639285456

Epoch: 6| Step: 13
Training loss: 3.045254163736244
Validation loss: 2.5130900494233575

Epoch: 166| Step: 0
Training loss: 2.432214434838621
Validation loss: 2.514065399104008

Epoch: 6| Step: 1
Training loss: 2.639416299506315
Validation loss: 2.513196215640349

Epoch: 6| Step: 2
Training loss: 2.8608623034325964
Validation loss: 2.5219596554086396

Epoch: 6| Step: 3
Training loss: 2.8578707380914987
Validation loss: 2.506220658420313

Epoch: 6| Step: 4
Training loss: 2.6892267933938507
Validation loss: 2.5078105720137183

Epoch: 6| Step: 5
Training loss: 2.6610695591679803
Validation loss: 2.485168535648638

Epoch: 6| Step: 6
Training loss: 2.299070697130485
Validation loss: 2.487543669377233

Epoch: 6| Step: 7
Training loss: 2.2386608500917236
Validation loss: 2.493610347390624

Epoch: 6| Step: 8
Training loss: 3.2771016373621316
Validation loss: 2.495850773533897

Epoch: 6| Step: 9
Training loss: 2.590163727523026
Validation loss: 2.5036592631588013

Epoch: 6| Step: 10
Training loss: 2.2639610757480884
Validation loss: 2.499677711139883

Epoch: 6| Step: 11
Training loss: 2.446400945545261
Validation loss: 2.5091552477349914

Epoch: 6| Step: 12
Training loss: 2.094827630865974
Validation loss: 2.5182922913930303

Epoch: 6| Step: 13
Training loss: 1.9417153924004513
Validation loss: 2.50298449739663

Epoch: 167| Step: 0
Training loss: 2.3372271495831933
Validation loss: 2.5115201326728167

Epoch: 6| Step: 1
Training loss: 2.5543483310667203
Validation loss: 2.5146204999500186

Epoch: 6| Step: 2
Training loss: 2.632999945513431
Validation loss: 2.4926093196462387

Epoch: 6| Step: 3
Training loss: 2.4538359371144627
Validation loss: 2.4780255865679517

Epoch: 6| Step: 4
Training loss: 2.106009504131959
Validation loss: 2.5037010638261505

Epoch: 6| Step: 5
Training loss: 2.367977501511016
Validation loss: 2.4936523699656976

Epoch: 6| Step: 6
Training loss: 2.7907234610245935
Validation loss: 2.4927843693066705

Epoch: 6| Step: 7
Training loss: 2.9027386854866393
Validation loss: 2.5240136687602543

Epoch: 6| Step: 8
Training loss: 1.8234614693472664
Validation loss: 2.489404497293591

Epoch: 6| Step: 9
Training loss: 2.0784958816792183
Validation loss: 2.5041684479637403

Epoch: 6| Step: 10
Training loss: 2.680648053230884
Validation loss: 2.508865923871236

Epoch: 6| Step: 11
Training loss: 2.898204207992526
Validation loss: 2.49707193359462

Epoch: 6| Step: 12
Training loss: 2.9053215266503245
Validation loss: 2.498609051302486

Epoch: 6| Step: 13
Training loss: 2.97770801038604
Validation loss: 2.5230733698333885

Epoch: 168| Step: 0
Training loss: 2.862070949333464
Validation loss: 2.5091137121370517

Epoch: 6| Step: 1
Training loss: 1.9891188380555216
Validation loss: 2.493461274882451

Epoch: 6| Step: 2
Training loss: 2.135399447736091
Validation loss: 2.515741597151604

Epoch: 6| Step: 3
Training loss: 2.3386835906214922
Validation loss: 2.4692047435535187

Epoch: 6| Step: 4
Training loss: 2.6348595514091175
Validation loss: 2.5051203774043733

Epoch: 6| Step: 5
Training loss: 2.823079703902708
Validation loss: 2.486098991716328

Epoch: 6| Step: 6
Training loss: 2.7131000435223336
Validation loss: 2.5056554281808716

Epoch: 6| Step: 7
Training loss: 3.137709749665276
Validation loss: 2.494843469695438

Epoch: 6| Step: 8
Training loss: 2.4062580257133304
Validation loss: 2.5041071123459595

Epoch: 6| Step: 9
Training loss: 2.619914350929678
Validation loss: 2.520552389106364

Epoch: 6| Step: 10
Training loss: 2.6296327672510826
Validation loss: 2.4982943612817663

Epoch: 6| Step: 11
Training loss: 2.5194604671501053
Validation loss: 2.513975721362969

Epoch: 6| Step: 12
Training loss: 2.6412443078084
Validation loss: 2.5017210040904545

Epoch: 6| Step: 13
Training loss: 2.0806653550492324
Validation loss: 2.4992195941731405

Epoch: 169| Step: 0
Training loss: 2.1505700087674113
Validation loss: 2.500585301907027

Epoch: 6| Step: 1
Training loss: 2.3864022652278
Validation loss: 2.4894913240938683

Epoch: 6| Step: 2
Training loss: 2.5407088825412436
Validation loss: 2.4899783235378923

Epoch: 6| Step: 3
Training loss: 2.6611082639349366
Validation loss: 2.497132507776775

Epoch: 6| Step: 4
Training loss: 3.208537074634133
Validation loss: 2.4643729252492204

Epoch: 6| Step: 5
Training loss: 2.0638813825869837
Validation loss: 2.509941847261191

Epoch: 6| Step: 6
Training loss: 2.6198038716471155
Validation loss: 2.512403321348622

Epoch: 6| Step: 7
Training loss: 2.0256516997388236
Validation loss: 2.5044562851476933

Epoch: 6| Step: 8
Training loss: 3.127742479470345
Validation loss: 2.5197212378454035

Epoch: 6| Step: 9
Training loss: 2.3336074077767175
Validation loss: 2.470155657151839

Epoch: 6| Step: 10
Training loss: 2.4978872908857896
Validation loss: 2.4957200320272896

Epoch: 6| Step: 11
Training loss: 2.5475266447910627
Validation loss: 2.5070152320064163

Epoch: 6| Step: 12
Training loss: 2.1242198634030736
Validation loss: 2.498990230288941

Epoch: 6| Step: 13
Training loss: 3.4534019945994863
Validation loss: 2.4994587281540284

Epoch: 170| Step: 0
Training loss: 1.8499594400444488
Validation loss: 2.5303213101418804

Epoch: 6| Step: 1
Training loss: 1.9661154906771534
Validation loss: 2.4713611804749345

Epoch: 6| Step: 2
Training loss: 2.876324390044547
Validation loss: 2.484800397647145

Epoch: 6| Step: 3
Training loss: 2.799948937086705
Validation loss: 2.4932210032143494

Epoch: 6| Step: 4
Training loss: 1.8278306251288536
Validation loss: 2.509725756859429

Epoch: 6| Step: 5
Training loss: 2.498939861586055
Validation loss: 2.4883837335008105

Epoch: 6| Step: 6
Training loss: 2.664775495681168
Validation loss: 2.5178883818011215

Epoch: 6| Step: 7
Training loss: 3.2115489813703935
Validation loss: 2.507197164045126

Epoch: 6| Step: 8
Training loss: 3.572238928276405
Validation loss: 2.5175570237744402

Epoch: 6| Step: 9
Training loss: 2.233987094312528
Validation loss: 2.489673867374912

Epoch: 6| Step: 10
Training loss: 2.401573690414417
Validation loss: 2.516699930522782

Epoch: 6| Step: 11
Training loss: 2.174260428080926
Validation loss: 2.520738435316838

Epoch: 6| Step: 12
Training loss: 2.5908120302944324
Validation loss: 2.50968656431751

Epoch: 6| Step: 13
Training loss: 1.993417278019186
Validation loss: 2.504777959150523

Epoch: 171| Step: 0
Training loss: 2.562573687726715
Validation loss: 2.5067031840348863

Epoch: 6| Step: 1
Training loss: 2.4094690366371556
Validation loss: 2.489844981695248

Epoch: 6| Step: 2
Training loss: 2.331208817269087
Validation loss: 2.512099453184239

Epoch: 6| Step: 3
Training loss: 2.4347604860165566
Validation loss: 2.513540188880502

Epoch: 6| Step: 4
Training loss: 2.2677668477331374
Validation loss: 2.4975990312745937

Epoch: 6| Step: 5
Training loss: 2.524861314949596
Validation loss: 2.4954875234458656

Epoch: 6| Step: 6
Training loss: 2.5233227021645552
Validation loss: 2.5163934240125654

Epoch: 6| Step: 7
Training loss: 3.1488625970677733
Validation loss: 2.499216637883626

Epoch: 6| Step: 8
Training loss: 2.8342872210687595
Validation loss: 2.517048620381338

Epoch: 6| Step: 9
Training loss: 2.733643526603641
Validation loss: 2.506500883538419

Epoch: 6| Step: 10
Training loss: 2.579008563674962
Validation loss: 2.525007453370564

Epoch: 6| Step: 11
Training loss: 2.043201440575554
Validation loss: 2.5087468090993865

Epoch: 6| Step: 12
Training loss: 2.811497064422372
Validation loss: 2.495259952484382

Epoch: 6| Step: 13
Training loss: 2.330954656224569
Validation loss: 2.509486745160359

Epoch: 172| Step: 0
Training loss: 2.4188049546733694
Validation loss: 2.49775744115626

Epoch: 6| Step: 1
Training loss: 2.2226015429476735
Validation loss: 2.497071913061446

Epoch: 6| Step: 2
Training loss: 2.627915897283587
Validation loss: 2.5170785786730816

Epoch: 6| Step: 3
Training loss: 2.5479617930232106
Validation loss: 2.494613495864995

Epoch: 6| Step: 4
Training loss: 2.8172594278116723
Validation loss: 2.521902561002831

Epoch: 6| Step: 5
Training loss: 2.6099635391319445
Validation loss: 2.484326455587544

Epoch: 6| Step: 6
Training loss: 2.359489387619297
Validation loss: 2.4890033652045798

Epoch: 6| Step: 7
Training loss: 2.35409095664924
Validation loss: 2.4984241051223015

Epoch: 6| Step: 8
Training loss: 2.9591882213715968
Validation loss: 2.4917480007155426

Epoch: 6| Step: 9
Training loss: 2.2850792505056163
Validation loss: 2.495457782628507

Epoch: 6| Step: 10
Training loss: 2.1152295282507376
Validation loss: 2.494609340989806

Epoch: 6| Step: 11
Training loss: 2.1951374242077026
Validation loss: 2.4812047255228444

Epoch: 6| Step: 12
Training loss: 3.2638524551702845
Validation loss: 2.490871114057077

Epoch: 6| Step: 13
Training loss: 3.10214358634836
Validation loss: 2.508298145859978

Epoch: 173| Step: 0
Training loss: 2.4291796383475943
Validation loss: 2.506407254986972

Epoch: 6| Step: 1
Training loss: 2.2970580981708038
Validation loss: 2.4977538180430394

Epoch: 6| Step: 2
Training loss: 2.8196752784664536
Validation loss: 2.5004832518515903

Epoch: 6| Step: 3
Training loss: 2.2878047885981054
Validation loss: 2.4975418639575757

Epoch: 6| Step: 4
Training loss: 1.877260054025564
Validation loss: 2.5360422946468675

Epoch: 6| Step: 5
Training loss: 2.7235018540360842
Validation loss: 2.4975414687683153

Epoch: 6| Step: 6
Training loss: 3.046760439553033
Validation loss: 2.500869817056946

Epoch: 6| Step: 7
Training loss: 2.9614544017191813
Validation loss: 2.4928431965534696

Epoch: 6| Step: 8
Training loss: 2.7892063648721845
Validation loss: 2.491146404971423

Epoch: 6| Step: 9
Training loss: 2.516469872803475
Validation loss: 2.520698929894224

Epoch: 6| Step: 10
Training loss: 2.3755699276982516
Validation loss: 2.5097700127040206

Epoch: 6| Step: 11
Training loss: 2.255319770250499
Validation loss: 2.4843174251882076

Epoch: 6| Step: 12
Training loss: 2.471690491012541
Validation loss: 2.500960731487475

Epoch: 6| Step: 13
Training loss: 2.4678076382053304
Validation loss: 2.5206407537675624

Epoch: 174| Step: 0
Training loss: 3.1229448808782885
Validation loss: 2.501027734814216

Epoch: 6| Step: 1
Training loss: 2.619941105508249
Validation loss: 2.518776293232883

Epoch: 6| Step: 2
Training loss: 2.0770118428141977
Validation loss: 2.4981061786681673

Epoch: 6| Step: 3
Training loss: 1.5824769699084178
Validation loss: 2.491759717257869

Epoch: 6| Step: 4
Training loss: 2.2136390814195717
Validation loss: 2.5195313202079808

Epoch: 6| Step: 5
Training loss: 2.4290415405013057
Validation loss: 2.5150111708169427

Epoch: 6| Step: 6
Training loss: 2.7331456499870197
Validation loss: 2.5103779417358245

Epoch: 6| Step: 7
Training loss: 2.517063653552519
Validation loss: 2.498611638941529

Epoch: 6| Step: 8
Training loss: 2.771497967009923
Validation loss: 2.4984345467252864

Epoch: 6| Step: 9
Training loss: 2.624298910340243
Validation loss: 2.5131762547957237

Epoch: 6| Step: 10
Training loss: 2.441994265125486
Validation loss: 2.4993770387204663

Epoch: 6| Step: 11
Training loss: 2.819374583672823
Validation loss: 2.484394758851092

Epoch: 6| Step: 12
Training loss: 2.7828337200026154
Validation loss: 2.5308648711128776

Epoch: 6| Step: 13
Training loss: 2.184272783368205
Validation loss: 2.5172305320184667

Epoch: 175| Step: 0
Training loss: 2.653421039936829
Validation loss: 2.5105015064626484

Epoch: 6| Step: 1
Training loss: 2.226410656486336
Validation loss: 2.504089163469699

Epoch: 6| Step: 2
Training loss: 2.944425494854807
Validation loss: 2.50212753377214

Epoch: 6| Step: 3
Training loss: 2.359144389016912
Validation loss: 2.5171150039881525

Epoch: 6| Step: 4
Training loss: 2.4258991894638586
Validation loss: 2.522360750236732

Epoch: 6| Step: 5
Training loss: 2.928725916413012
Validation loss: 2.513259322101172

Epoch: 6| Step: 6
Training loss: 2.6084902057050696
Validation loss: 2.513241557423693

Epoch: 6| Step: 7
Training loss: 2.4670120109200777
Validation loss: 2.4947916495400486

Epoch: 6| Step: 8
Training loss: 3.158680801267655
Validation loss: 2.5201696567847724

Epoch: 6| Step: 9
Training loss: 1.996107306686321
Validation loss: 2.470310789987216

Epoch: 6| Step: 10
Training loss: 1.6796624470106567
Validation loss: 2.5007985080623087

Epoch: 6| Step: 11
Training loss: 2.525642872166334
Validation loss: 2.516748905800816

Epoch: 6| Step: 12
Training loss: 2.8045536104934916
Validation loss: 2.499667451131788

Epoch: 6| Step: 13
Training loss: 1.8508957575828608
Validation loss: 2.49393015658539

Epoch: 176| Step: 0
Training loss: 2.4844994663753077
Validation loss: 2.5022152468982886

Epoch: 6| Step: 1
Training loss: 1.9187337744210065
Validation loss: 2.4664179175428527

Epoch: 6| Step: 2
Training loss: 1.9645396427650945
Validation loss: 2.4714180032062414

Epoch: 6| Step: 3
Training loss: 2.78846995152588
Validation loss: 2.5174684811161283

Epoch: 6| Step: 4
Training loss: 2.712572819308051
Validation loss: 2.532330651866325

Epoch: 6| Step: 5
Training loss: 2.661303033546677
Validation loss: 2.514877594540843

Epoch: 6| Step: 6
Training loss: 2.876316266811359
Validation loss: 2.5072980257245723

Epoch: 6| Step: 7
Training loss: 2.9855576014522573
Validation loss: 2.509688284520109

Epoch: 6| Step: 8
Training loss: 1.971665962855433
Validation loss: 2.5076966852992926

Epoch: 6| Step: 9
Training loss: 2.6554785112974275
Validation loss: 2.516569090480062

Epoch: 6| Step: 10
Training loss: 2.8768745613636493
Validation loss: 2.507673874417098

Epoch: 6| Step: 11
Training loss: 1.853937870725317
Validation loss: 2.513457872372547

Epoch: 6| Step: 12
Training loss: 3.035880730177213
Validation loss: 2.5126303303341944

Epoch: 6| Step: 13
Training loss: 1.7144938396522293
Validation loss: 2.4866282978323664

Epoch: 177| Step: 0
Training loss: 2.3776824258638323
Validation loss: 2.4964297786717515

Epoch: 6| Step: 1
Training loss: 2.7496268279297635
Validation loss: 2.502293347004214

Epoch: 6| Step: 2
Training loss: 2.7451696455322145
Validation loss: 2.503990038964556

Epoch: 6| Step: 3
Training loss: 2.1369098807983375
Validation loss: 2.494703775082611

Epoch: 6| Step: 4
Training loss: 2.703923102069794
Validation loss: 2.5094016634985796

Epoch: 6| Step: 5
Training loss: 1.9886259668251836
Validation loss: 2.50654315741189

Epoch: 6| Step: 6
Training loss: 2.421334187821869
Validation loss: 2.513821040086043

Epoch: 6| Step: 7
Training loss: 2.9546052966693277
Validation loss: 2.5282580205212

Epoch: 6| Step: 8
Training loss: 2.393805986750674
Validation loss: 2.5063324345253233

Epoch: 6| Step: 9
Training loss: 2.1172755599623696
Validation loss: 2.5537976097407378

Epoch: 6| Step: 10
Training loss: 2.9960471178228083
Validation loss: 2.5063594410940007

Epoch: 6| Step: 11
Training loss: 2.1950503155409327
Validation loss: 2.4971745234382046

Epoch: 6| Step: 12
Training loss: 2.8269854193879183
Validation loss: 2.5318678913626265

Epoch: 6| Step: 13
Training loss: 2.4548524253604027
Validation loss: 2.5122797833221586

Epoch: 178| Step: 0
Training loss: 2.3838148666541716
Validation loss: 2.506898092976632

Epoch: 6| Step: 1
Training loss: 2.3596781352759977
Validation loss: 2.514351240724075

Epoch: 6| Step: 2
Training loss: 2.8705446302050457
Validation loss: 2.5140070766103624

Epoch: 6| Step: 3
Training loss: 2.4486089065819754
Validation loss: 2.528425376861678

Epoch: 6| Step: 4
Training loss: 2.6245240279507853
Validation loss: 2.5042344871112525

Epoch: 6| Step: 5
Training loss: 2.8387517915465574
Validation loss: 2.5032208419513133

Epoch: 6| Step: 6
Training loss: 2.409227782822015
Validation loss: 2.476913760559913

Epoch: 6| Step: 7
Training loss: 2.1456196008798223
Validation loss: 2.468213397009362

Epoch: 6| Step: 8
Training loss: 2.314655845552338
Validation loss: 2.5368664049758247

Epoch: 6| Step: 9
Training loss: 2.74299162026967
Validation loss: 2.4947234798730302

Epoch: 6| Step: 10
Training loss: 2.8069736015076
Validation loss: 2.5268329500345548

Epoch: 6| Step: 11
Training loss: 2.364822566048113
Validation loss: 2.5341725727283095

Epoch: 6| Step: 12
Training loss: 2.7994175441376696
Validation loss: 2.491496736144729

Epoch: 6| Step: 13
Training loss: 1.6178970623113944
Validation loss: 2.5093795761001085

Epoch: 179| Step: 0
Training loss: 2.9039261253565525
Validation loss: 2.5073088546962614

Epoch: 6| Step: 1
Training loss: 1.9118263105245124
Validation loss: 2.5016443208117822

Epoch: 6| Step: 2
Training loss: 2.4063300577692637
Validation loss: 2.46869908699984

Epoch: 6| Step: 3
Training loss: 2.602677573119956
Validation loss: 2.5060684589654687

Epoch: 6| Step: 4
Training loss: 2.8411056464430335
Validation loss: 2.4859877392845324

Epoch: 6| Step: 5
Training loss: 2.3161807280144266
Validation loss: 2.49258354229839

Epoch: 6| Step: 6
Training loss: 3.1588965169652234
Validation loss: 2.5381918854130974

Epoch: 6| Step: 7
Training loss: 2.0954767195533197
Validation loss: 2.5073282958089806

Epoch: 6| Step: 8
Training loss: 2.5663319290989035
Validation loss: 2.5017166314749097

Epoch: 6| Step: 9
Training loss: 2.8846679340368873
Validation loss: 2.4815185506106547

Epoch: 6| Step: 10
Training loss: 2.72175484708521
Validation loss: 2.511487489303794

Epoch: 6| Step: 11
Training loss: 2.0071375562193494
Validation loss: 2.5140723678550305

Epoch: 6| Step: 12
Training loss: 2.368655917027943
Validation loss: 2.5079081573352053

Epoch: 6| Step: 13
Training loss: 2.1795529559869977
Validation loss: 2.5281793535761605

Epoch: 180| Step: 0
Training loss: 2.5125165416875204
Validation loss: 2.503413178722648

Epoch: 6| Step: 1
Training loss: 2.3036618811674905
Validation loss: 2.530906197126732

Epoch: 6| Step: 2
Training loss: 2.695004476002106
Validation loss: 2.4985786150487814

Epoch: 6| Step: 3
Training loss: 3.0947011391054966
Validation loss: 2.5217675315618635

Epoch: 6| Step: 4
Training loss: 2.7355766054003445
Validation loss: 2.4999013383942326

Epoch: 6| Step: 5
Training loss: 2.3890140066969754
Validation loss: 2.5086458472712607

Epoch: 6| Step: 6
Training loss: 2.4380628473945167
Validation loss: 2.486954593048474

Epoch: 6| Step: 7
Training loss: 2.057369903493245
Validation loss: 2.5165309773943685

Epoch: 6| Step: 8
Training loss: 2.0731168359567542
Validation loss: 2.497838671411823

Epoch: 6| Step: 9
Training loss: 1.8811315099971222
Validation loss: 2.5048999901486613

Epoch: 6| Step: 10
Training loss: 3.172172494852253
Validation loss: 2.5160212652125935

Epoch: 6| Step: 11
Training loss: 2.4344186134985684
Validation loss: 2.5171940330097287

Epoch: 6| Step: 12
Training loss: 2.6659744874344478
Validation loss: 2.520887337857619

Epoch: 6| Step: 13
Training loss: 2.3938807837712344
Validation loss: 2.497634710144934

Epoch: 181| Step: 0
Training loss: 2.3884548734347235
Validation loss: 2.503795266617399

Epoch: 6| Step: 1
Training loss: 2.881475702080392
Validation loss: 2.499847567433413

Epoch: 6| Step: 2
Training loss: 2.7776635867065558
Validation loss: 2.5053452343558225

Epoch: 6| Step: 3
Training loss: 1.6991898457491361
Validation loss: 2.5427076916673395

Epoch: 6| Step: 4
Training loss: 2.8601956883420323
Validation loss: 2.505529538682784

Epoch: 6| Step: 5
Training loss: 2.662816536583251
Validation loss: 2.5074664088478427

Epoch: 6| Step: 6
Training loss: 2.4681474940341652
Validation loss: 2.5020306010950404

Epoch: 6| Step: 7
Training loss: 2.780051669731509
Validation loss: 2.5038620640599456

Epoch: 6| Step: 8
Training loss: 2.459948822512595
Validation loss: 2.49945368899135

Epoch: 6| Step: 9
Training loss: 2.414087091709544
Validation loss: 2.53864421570108

Epoch: 6| Step: 10
Training loss: 1.8576628551739527
Validation loss: 2.50796079205166

Epoch: 6| Step: 11
Training loss: 2.4564040312870143
Validation loss: 2.521205056700304

Epoch: 6| Step: 12
Training loss: 2.8366799131533154
Validation loss: 2.537699190181136

Epoch: 6| Step: 13
Training loss: 2.424690726310392
Validation loss: 2.5299688550984185

Epoch: 182| Step: 0
Training loss: 2.33587278959021
Validation loss: 2.479253377731123

Epoch: 6| Step: 1
Training loss: 2.282875004825865
Validation loss: 2.498108511296177

Epoch: 6| Step: 2
Training loss: 3.167517765179007
Validation loss: 2.5090551754321546

Epoch: 6| Step: 3
Training loss: 2.6020890353170634
Validation loss: 2.479915275962418

Epoch: 6| Step: 4
Training loss: 1.941926759755034
Validation loss: 2.497122896412643

Epoch: 6| Step: 5
Training loss: 1.9011128730153883
Validation loss: 2.5188912148130047

Epoch: 6| Step: 6
Training loss: 2.923528682687429
Validation loss: 2.5343774019790373

Epoch: 6| Step: 7
Training loss: 2.5630150254159285
Validation loss: 2.5158008739942908

Epoch: 6| Step: 8
Training loss: 2.5655794016495013
Validation loss: 2.490090792701065

Epoch: 6| Step: 9
Training loss: 2.903133238748906
Validation loss: 2.504387281189813

Epoch: 6| Step: 10
Training loss: 2.0323622959311676
Validation loss: 2.520334162995135

Epoch: 6| Step: 11
Training loss: 2.5300558593674913
Validation loss: 2.5177572023383274

Epoch: 6| Step: 12
Training loss: 2.662623578995967
Validation loss: 2.504800123971864

Epoch: 6| Step: 13
Training loss: 2.4683690803338236
Validation loss: 2.494496713797102

Epoch: 183| Step: 0
Training loss: 2.1872390591435726
Validation loss: 2.490216828521094

Epoch: 6| Step: 1
Training loss: 2.1884420273867584
Validation loss: 2.4952995442197756

Epoch: 6| Step: 2
Training loss: 3.6417292283283413
Validation loss: 2.529666597317382

Epoch: 6| Step: 3
Training loss: 2.092626355548053
Validation loss: 2.517848970134157

Epoch: 6| Step: 4
Training loss: 2.1663622153174944
Validation loss: 2.512229816788623

Epoch: 6| Step: 5
Training loss: 2.5146099435767293
Validation loss: 2.524453290431156

Epoch: 6| Step: 6
Training loss: 3.3197124342873896
Validation loss: 2.503993738031497

Epoch: 6| Step: 7
Training loss: 2.562410678702406
Validation loss: 2.5112653776781633

Epoch: 6| Step: 8
Training loss: 2.5351632093148893
Validation loss: 2.5402825162416867

Epoch: 6| Step: 9
Training loss: 2.6516115866294703
Validation loss: 2.497152938732448

Epoch: 6| Step: 10
Training loss: 1.9473359029287807
Validation loss: 2.5147286817437373

Epoch: 6| Step: 11
Training loss: 2.0551344236788136
Validation loss: 2.5093376748923832

Epoch: 6| Step: 12
Training loss: 2.146790473084681
Validation loss: 2.5196678659409733

Epoch: 6| Step: 13
Training loss: 3.2682960209588168
Validation loss: 2.5356787744726343

Epoch: 184| Step: 0
Training loss: 2.0066606950935064
Validation loss: 2.5251002998538206

Epoch: 6| Step: 1
Training loss: 2.628990228680502
Validation loss: 2.498178566088975

Epoch: 6| Step: 2
Training loss: 1.8580801165485141
Validation loss: 2.518080960440748

Epoch: 6| Step: 3
Training loss: 2.6876000674048313
Validation loss: 2.5315442204818095

Epoch: 6| Step: 4
Training loss: 2.80690123344263
Validation loss: 2.518506415093777

Epoch: 6| Step: 5
Training loss: 2.414245796034835
Validation loss: 2.4907720460757714

Epoch: 6| Step: 6
Training loss: 2.1178041805775454
Validation loss: 2.5219918028770554

Epoch: 6| Step: 7
Training loss: 2.611665919274777
Validation loss: 2.515168895836484

Epoch: 6| Step: 8
Training loss: 2.3098009183894557
Validation loss: 2.499769179650698

Epoch: 6| Step: 9
Training loss: 1.9575077169760722
Validation loss: 2.508616072372221

Epoch: 6| Step: 10
Training loss: 2.5882417951594014
Validation loss: 2.4951326358723818

Epoch: 6| Step: 11
Training loss: 3.031725973926642
Validation loss: 2.498505867781177

Epoch: 6| Step: 12
Training loss: 2.938361832827965
Validation loss: 2.5001563177152573

Epoch: 6| Step: 13
Training loss: 3.1214023480493833
Validation loss: 2.503948385946909

Epoch: 185| Step: 0
Training loss: 2.177540148639742
Validation loss: 2.523346873184056

Epoch: 6| Step: 1
Training loss: 2.6530355413696878
Validation loss: 2.5353981624542152

Epoch: 6| Step: 2
Training loss: 2.12464857000411
Validation loss: 2.5052215519199192

Epoch: 6| Step: 3
Training loss: 1.9354581841435787
Validation loss: 2.519033520875245

Epoch: 6| Step: 4
Training loss: 2.363801555262957
Validation loss: 2.496991398107177

Epoch: 6| Step: 5
Training loss: 2.440813795302174
Validation loss: 2.507721855440358

Epoch: 6| Step: 6
Training loss: 2.0479493561234814
Validation loss: 2.502377024008701

Epoch: 6| Step: 7
Training loss: 2.727268597570818
Validation loss: 2.485673778149725

Epoch: 6| Step: 8
Training loss: 3.166410352138118
Validation loss: 2.507294579993435

Epoch: 6| Step: 9
Training loss: 2.8917356264831127
Validation loss: 2.5135451590053166

Epoch: 6| Step: 10
Training loss: 2.811336276578037
Validation loss: 2.5144381063595778

Epoch: 6| Step: 11
Training loss: 2.1706062837601667
Validation loss: 2.483898740204851

Epoch: 6| Step: 12
Training loss: 2.766927466298993
Validation loss: 2.5041121006815197

Epoch: 6| Step: 13
Training loss: 2.4478038247796152
Validation loss: 2.516519637995624

Epoch: 186| Step: 0
Training loss: 2.458493523847341
Validation loss: 2.4960421848409475

Epoch: 6| Step: 1
Training loss: 2.8240375803643367
Validation loss: 2.5188505557699608

Epoch: 6| Step: 2
Training loss: 2.3049314208182623
Validation loss: 2.4799455432771276

Epoch: 6| Step: 3
Training loss: 2.30901631007458
Validation loss: 2.503679938822911

Epoch: 6| Step: 4
Training loss: 2.0787859561125015
Validation loss: 2.5131846122966204

Epoch: 6| Step: 5
Training loss: 2.246728532073307
Validation loss: 2.537008727416832

Epoch: 6| Step: 6
Training loss: 3.0184796514351278
Validation loss: 2.525135729234444

Epoch: 6| Step: 7
Training loss: 2.5217265183463686
Validation loss: 2.517240136879205

Epoch: 6| Step: 8
Training loss: 2.2869198506417483
Validation loss: 2.518690816951959

Epoch: 6| Step: 9
Training loss: 2.095072769685655
Validation loss: 2.539889037943398

Epoch: 6| Step: 10
Training loss: 3.2838088457739887
Validation loss: 2.5035701813688824

Epoch: 6| Step: 11
Training loss: 2.109443267847835
Validation loss: 2.5044031232990935

Epoch: 6| Step: 12
Training loss: 2.673283345222042
Validation loss: 2.4962634963628925

Epoch: 6| Step: 13
Training loss: 2.3009643481831934
Validation loss: 2.509719082484693

Epoch: 187| Step: 0
Training loss: 2.611964876409038
Validation loss: 2.5149790329961195

Epoch: 6| Step: 1
Training loss: 2.597297173255531
Validation loss: 2.5289373352205673

Epoch: 6| Step: 2
Training loss: 2.406731792618434
Validation loss: 2.5066503996715057

Epoch: 6| Step: 3
Training loss: 2.615705657043535
Validation loss: 2.5145978176343675

Epoch: 6| Step: 4
Training loss: 2.770369259015633
Validation loss: 2.5088740545789348

Epoch: 6| Step: 5
Training loss: 2.8594984017514578
Validation loss: 2.5314900669149156

Epoch: 6| Step: 6
Training loss: 2.2262389901695907
Validation loss: 2.5196823289769696

Epoch: 6| Step: 7
Training loss: 1.9782239241369288
Validation loss: 2.520786504294414

Epoch: 6| Step: 8
Training loss: 2.456868225414375
Validation loss: 2.5118835892195555

Epoch: 6| Step: 9
Training loss: 2.866922724916828
Validation loss: 2.54196734257876

Epoch: 6| Step: 10
Training loss: 1.7557846919079738
Validation loss: 2.5093169528754817

Epoch: 6| Step: 11
Training loss: 2.8159888774370403
Validation loss: 2.516373332655498

Epoch: 6| Step: 12
Training loss: 2.5570988371973034
Validation loss: 2.517678777509758

Epoch: 6| Step: 13
Training loss: 2.2966991538899917
Validation loss: 2.5172543898098634

Epoch: 188| Step: 0
Training loss: 2.216924319195907
Validation loss: 2.500310154615856

Epoch: 6| Step: 1
Training loss: 2.418751726963107
Validation loss: 2.5138290507458256

Epoch: 6| Step: 2
Training loss: 1.8376779950145226
Validation loss: 2.501849496344751

Epoch: 6| Step: 3
Training loss: 2.4556677208201694
Validation loss: 2.488775838958826

Epoch: 6| Step: 4
Training loss: 3.0011804165731633
Validation loss: 2.4839861691875407

Epoch: 6| Step: 5
Training loss: 2.315486937820274
Validation loss: 2.5271096072356922

Epoch: 6| Step: 6
Training loss: 2.7803588468177334
Validation loss: 2.516418445020958

Epoch: 6| Step: 7
Training loss: 3.0613566229972204
Validation loss: 2.520069295012152

Epoch: 6| Step: 8
Training loss: 2.6918703075019956
Validation loss: 2.5040709441844653

Epoch: 6| Step: 9
Training loss: 2.8385628142429455
Validation loss: 2.5305658264295117

Epoch: 6| Step: 10
Training loss: 2.3363410320477156
Validation loss: 2.509776962227215

Epoch: 6| Step: 11
Training loss: 2.3776208571272632
Validation loss: 2.4966064729694244

Epoch: 6| Step: 12
Training loss: 2.2404844615068007
Validation loss: 2.4942968093835436

Epoch: 6| Step: 13
Training loss: 2.375847313968124
Validation loss: 2.5218398578801393

Epoch: 189| Step: 0
Training loss: 2.147044892231318
Validation loss: 2.49720804942906

Epoch: 6| Step: 1
Training loss: 2.7157737840427267
Validation loss: 2.50549473554961

Epoch: 6| Step: 2
Training loss: 2.6597310313141715
Validation loss: 2.464629592928067

Epoch: 6| Step: 3
Training loss: 2.535300510810256
Validation loss: 2.54853189831151

Epoch: 6| Step: 4
Training loss: 2.4565250621272674
Validation loss: 2.4929318297125223

Epoch: 6| Step: 5
Training loss: 2.2117183101846303
Validation loss: 2.5229869732304406

Epoch: 6| Step: 6
Training loss: 2.7182107149399233
Validation loss: 2.486866498363361

Epoch: 6| Step: 7
Training loss: 2.3919799555190635
Validation loss: 2.5237317018056293

Epoch: 6| Step: 8
Training loss: 1.9775756531144122
Validation loss: 2.5161228817151233

Epoch: 6| Step: 9
Training loss: 2.394873739271192
Validation loss: 2.4931356379277343

Epoch: 6| Step: 10
Training loss: 2.3602937842018097
Validation loss: 2.501917537428331

Epoch: 6| Step: 11
Training loss: 3.083331511900768
Validation loss: 2.4843568435534853

Epoch: 6| Step: 12
Training loss: 2.5294778530812474
Validation loss: 2.5126447706206205

Epoch: 6| Step: 13
Training loss: 2.5065147393259535
Validation loss: 2.5169100777417204

Epoch: 190| Step: 0
Training loss: 2.3002164241304244
Validation loss: 2.525815036165403

Epoch: 6| Step: 1
Training loss: 2.4540815492991164
Validation loss: 2.5189261972066554

Epoch: 6| Step: 2
Training loss: 3.035711721210839
Validation loss: 2.561761564098954

Epoch: 6| Step: 3
Training loss: 2.7014906688912412
Validation loss: 2.5034893295996663

Epoch: 6| Step: 4
Training loss: 2.7691072427268706
Validation loss: 2.4709765818157785

Epoch: 6| Step: 5
Training loss: 2.1492353587755084
Validation loss: 2.504610592860173

Epoch: 6| Step: 6
Training loss: 2.753025558009402
Validation loss: 2.517379938398675

Epoch: 6| Step: 7
Training loss: 2.679240642691088
Validation loss: 2.5021019497708954

Epoch: 6| Step: 8
Training loss: 2.7116789669546466
Validation loss: 2.4896101656460763

Epoch: 6| Step: 9
Training loss: 2.586032738905421
Validation loss: 2.5236689207053975

Epoch: 6| Step: 10
Training loss: 2.0544503078694514
Validation loss: 2.5199598831255923

Epoch: 6| Step: 11
Training loss: 2.2726457928010104
Validation loss: 2.517355456507952

Epoch: 6| Step: 12
Training loss: 2.0978043431123887
Validation loss: 2.4839209478770363

Epoch: 6| Step: 13
Training loss: 2.3605982154819407
Validation loss: 2.51394720781699

Epoch: 191| Step: 0
Training loss: 2.911043541328731
Validation loss: 2.4842673224380456

Epoch: 6| Step: 1
Training loss: 2.193701862556963
Validation loss: 2.5322632923128428

Epoch: 6| Step: 2
Training loss: 2.8615670895307943
Validation loss: 2.525806567194786

Epoch: 6| Step: 3
Training loss: 2.774032829755979
Validation loss: 2.5275014063032155

Epoch: 6| Step: 4
Training loss: 2.5321150820283136
Validation loss: 2.492260814350496

Epoch: 6| Step: 5
Training loss: 1.9512723146539692
Validation loss: 2.49588197438213

Epoch: 6| Step: 6
Training loss: 2.043636293193738
Validation loss: 2.5017992815310106

Epoch: 6| Step: 7
Training loss: 2.5404986736051645
Validation loss: 2.5161276022063244

Epoch: 6| Step: 8
Training loss: 2.3856714326388864
Validation loss: 2.516464365462978

Epoch: 6| Step: 9
Training loss: 2.2848110883823036
Validation loss: 2.5177016616831933

Epoch: 6| Step: 10
Training loss: 2.470417859643408
Validation loss: 2.4926652697568654

Epoch: 6| Step: 11
Training loss: 3.003590977745918
Validation loss: 2.5220869935751553

Epoch: 6| Step: 12
Training loss: 2.339319950631144
Validation loss: 2.5122795690289257

Epoch: 6| Step: 13
Training loss: 2.478801497377981
Validation loss: 2.5073368640016076

Epoch: 192| Step: 0
Training loss: 2.4914662623960444
Validation loss: 2.491639423567819

Epoch: 6| Step: 1
Training loss: 2.5128571821039425
Validation loss: 2.520639763151797

Epoch: 6| Step: 2
Training loss: 2.024297465211359
Validation loss: 2.516681186243245

Epoch: 6| Step: 3
Training loss: 2.7552890200900007
Validation loss: 2.5243999149881873

Epoch: 6| Step: 4
Training loss: 2.7546716370453113
Validation loss: 2.522956391082907

Epoch: 6| Step: 5
Training loss: 1.9888453554590182
Validation loss: 2.5073274144490805

Epoch: 6| Step: 6
Training loss: 2.1830715585496816
Validation loss: 2.476728100537333

Epoch: 6| Step: 7
Training loss: 2.687833011310765
Validation loss: 2.511766804812573

Epoch: 6| Step: 8
Training loss: 2.6495612806960276
Validation loss: 2.5446111690108637

Epoch: 6| Step: 9
Training loss: 2.8785478014214667
Validation loss: 2.505269505209308

Epoch: 6| Step: 10
Training loss: 2.673073304706398
Validation loss: 2.50652948281439

Epoch: 6| Step: 11
Training loss: 2.4983504575414375
Validation loss: 2.5151489771488134

Epoch: 6| Step: 12
Training loss: 2.1713458178103417
Validation loss: 2.497055718494399

Epoch: 6| Step: 13
Training loss: 2.787822458407427
Validation loss: 2.500704924351132

Epoch: 193| Step: 0
Training loss: 2.330117621011133
Validation loss: 2.5113653736487542

Epoch: 6| Step: 1
Training loss: 1.9300930199417985
Validation loss: 2.5080144539761475

Epoch: 6| Step: 2
Training loss: 2.2122801892417856
Validation loss: 2.5216851933726607

Epoch: 6| Step: 3
Training loss: 2.72285309658742
Validation loss: 2.500975988468663

Epoch: 6| Step: 4
Training loss: 2.3204521368376136
Validation loss: 2.5196781900033574

Epoch: 6| Step: 5
Training loss: 2.721567907867858
Validation loss: 2.487777595368263

Epoch: 6| Step: 6
Training loss: 2.439906863795789
Validation loss: 2.5125221189022087

Epoch: 6| Step: 7
Training loss: 2.151078587939478
Validation loss: 2.5311398266335705

Epoch: 6| Step: 8
Training loss: 2.788802789302182
Validation loss: 2.5005389381236474

Epoch: 6| Step: 9
Training loss: 2.4971873674045377
Validation loss: 2.4931748418093482

Epoch: 6| Step: 10
Training loss: 2.0213285671763397
Validation loss: 2.5047800000090055

Epoch: 6| Step: 11
Training loss: 2.6414270932160204
Validation loss: 2.4843168215093305

Epoch: 6| Step: 12
Training loss: 3.032285375838643
Validation loss: 2.5074826200139713

Epoch: 6| Step: 13
Training loss: 2.8419359730240346
Validation loss: 2.524449892488571

Epoch: 194| Step: 0
Training loss: 2.7416127355154534
Validation loss: 2.5074219963000175

Epoch: 6| Step: 1
Training loss: 2.5948689816131916
Validation loss: 2.5282328540575354

Epoch: 6| Step: 2
Training loss: 2.6453008841875247
Validation loss: 2.5277798757176657

Epoch: 6| Step: 3
Training loss: 2.492079585976699
Validation loss: 2.5118295312800822

Epoch: 6| Step: 4
Training loss: 2.442727376921925
Validation loss: 2.5150769099741304

Epoch: 6| Step: 5
Training loss: 2.4047190762653328
Validation loss: 2.5323578498941166

Epoch: 6| Step: 6
Training loss: 2.602011060563666
Validation loss: 2.535049384398353

Epoch: 6| Step: 7
Training loss: 2.5035537257325626
Validation loss: 2.488237279077596

Epoch: 6| Step: 8
Training loss: 2.1136990907968776
Validation loss: 2.5263515845978044

Epoch: 6| Step: 9
Training loss: 2.2094465964450745
Validation loss: 2.515732838489886

Epoch: 6| Step: 10
Training loss: 2.809195887676642
Validation loss: 2.534151871694309

Epoch: 6| Step: 11
Training loss: 1.944638913133213
Validation loss: 2.4879124081934916

Epoch: 6| Step: 12
Training loss: 2.662636473117589
Validation loss: 2.5467764314891053

Epoch: 6| Step: 13
Training loss: 2.680297159402557
Validation loss: 2.507540213484986

Epoch: 195| Step: 0
Training loss: 2.9956000168680013
Validation loss: 2.499370560316053

Epoch: 6| Step: 1
Training loss: 2.5240070656860407
Validation loss: 2.510593563466904

Epoch: 6| Step: 2
Training loss: 2.582394111286715
Validation loss: 2.501341266665426

Epoch: 6| Step: 3
Training loss: 2.0822135459007134
Validation loss: 2.505165025191195

Epoch: 6| Step: 4
Training loss: 2.3587097847021594
Validation loss: 2.5200811779455776

Epoch: 6| Step: 5
Training loss: 2.2705139028816164
Validation loss: 2.52353035243483

Epoch: 6| Step: 6
Training loss: 2.0938133116302327
Validation loss: 2.5212215201648274

Epoch: 6| Step: 7
Training loss: 1.3914641944947244
Validation loss: 2.4786986332070846

Epoch: 6| Step: 8
Training loss: 3.0531015791526674
Validation loss: 2.4911078684520933

Epoch: 6| Step: 9
Training loss: 2.80709607935722
Validation loss: 2.503348651987842

Epoch: 6| Step: 10
Training loss: 2.736062711577008
Validation loss: 2.521545908106022

Epoch: 6| Step: 11
Training loss: 2.3795852569036158
Validation loss: 2.512713303967925

Epoch: 6| Step: 12
Training loss: 2.8903424099094206
Validation loss: 2.520658862439925

Epoch: 6| Step: 13
Training loss: 1.6138272227537627
Validation loss: 2.5067296440652154

Epoch: 196| Step: 0
Training loss: 2.9268915695057025
Validation loss: 2.5161812509395802

Epoch: 6| Step: 1
Training loss: 2.4299956000645366
Validation loss: 2.4869683732168637

Epoch: 6| Step: 2
Training loss: 2.6141486446143256
Validation loss: 2.51755799727373

Epoch: 6| Step: 3
Training loss: 2.3814065756596396
Validation loss: 2.5119508176041982

Epoch: 6| Step: 4
Training loss: 2.1782947315433425
Validation loss: 2.516011015810478

Epoch: 6| Step: 5
Training loss: 2.629726151688334
Validation loss: 2.5060640438096535

Epoch: 6| Step: 6
Training loss: 3.123612362812523
Validation loss: 2.5102813296847253

Epoch: 6| Step: 7
Training loss: 1.9618523276125772
Validation loss: 2.500945162792344

Epoch: 6| Step: 8
Training loss: 1.775626423938238
Validation loss: 2.5202934483977444

Epoch: 6| Step: 9
Training loss: 2.7994873803779696
Validation loss: 2.493341241163438

Epoch: 6| Step: 10
Training loss: 2.5967421221104487
Validation loss: 2.488388826503285

Epoch: 6| Step: 11
Training loss: 2.615024321754798
Validation loss: 2.5126964103158422

Epoch: 6| Step: 12
Training loss: 1.9161666826125014
Validation loss: 2.4951515045288057

Epoch: 6| Step: 13
Training loss: 2.415199876020946
Validation loss: 2.4995104546279485

Epoch: 197| Step: 0
Training loss: 2.782381491935029
Validation loss: 2.5120861200919737

Epoch: 6| Step: 1
Training loss: 2.6894464762819505
Validation loss: 2.5323772201866355

Epoch: 6| Step: 2
Training loss: 2.536827817686276
Validation loss: 2.5290346811170776

Epoch: 6| Step: 3
Training loss: 3.5545391345239548
Validation loss: 2.4952729460318754

Epoch: 6| Step: 4
Training loss: 2.4630383440410157
Validation loss: 2.4822369362813603

Epoch: 6| Step: 5
Training loss: 2.83674799158925
Validation loss: 2.5011624453688905

Epoch: 6| Step: 6
Training loss: 2.5992776270700038
Validation loss: 2.4997562371643127

Epoch: 6| Step: 7
Training loss: 1.9910745541281059
Validation loss: 2.53101377044457

Epoch: 6| Step: 8
Training loss: 1.9676124071440784
Validation loss: 2.4969118292502865

Epoch: 6| Step: 9
Training loss: 2.2857112373604203
Validation loss: 2.510106989051874

Epoch: 6| Step: 10
Training loss: 1.6467638966327418
Validation loss: 2.5099546340655547

Epoch: 6| Step: 11
Training loss: 1.6577518060128136
Validation loss: 2.5353331676114435

Epoch: 6| Step: 12
Training loss: 2.5423654523299133
Validation loss: 2.521976761978931

Epoch: 6| Step: 13
Training loss: 3.1688470528391095
Validation loss: 2.5288622284202074

Epoch: 198| Step: 0
Training loss: 2.3210367941119725
Validation loss: 2.5017459361535246

Epoch: 6| Step: 1
Training loss: 2.8863107189005204
Validation loss: 2.518279653815251

Epoch: 6| Step: 2
Training loss: 2.593743933245446
Validation loss: 2.504048243604994

Epoch: 6| Step: 3
Training loss: 2.4311167937907197
Validation loss: 2.5038502935704923

Epoch: 6| Step: 4
Training loss: 2.606360150028951
Validation loss: 2.538611219766489

Epoch: 6| Step: 5
Training loss: 2.771802135314053
Validation loss: 2.4930070729216034

Epoch: 6| Step: 6
Training loss: 1.9833279948870621
Validation loss: 2.5204237813121924

Epoch: 6| Step: 7
Training loss: 2.720481496575867
Validation loss: 2.4901187269721388

Epoch: 6| Step: 8
Training loss: 2.1739053361175573
Validation loss: 2.4814678267520054

Epoch: 6| Step: 9
Training loss: 1.9147133479701393
Validation loss: 2.5073391062457517

Epoch: 6| Step: 10
Training loss: 1.8731438668350642
Validation loss: 2.5319460103480984

Epoch: 6| Step: 11
Training loss: 3.0833602766629666
Validation loss: 2.499893242597228

Epoch: 6| Step: 12
Training loss: 2.602383320855987
Validation loss: 2.5097566636614492

Epoch: 6| Step: 13
Training loss: 2.047520659439137
Validation loss: 2.501375361481319

Epoch: 199| Step: 0
Training loss: 2.276191642469832
Validation loss: 2.517301222772824

Epoch: 6| Step: 1
Training loss: 2.5215324084067428
Validation loss: 2.521116779947911

Epoch: 6| Step: 2
Training loss: 2.450465418925177
Validation loss: 2.52652276272876

Epoch: 6| Step: 3
Training loss: 2.207745947783031
Validation loss: 2.5349217234688934

Epoch: 6| Step: 4
Training loss: 2.7909049138294035
Validation loss: 2.512202503915355

Epoch: 6| Step: 5
Training loss: 2.3288588423384193
Validation loss: 2.5268388030632236

Epoch: 6| Step: 6
Training loss: 2.834061940325935
Validation loss: 2.5525589788281775

Epoch: 6| Step: 7
Training loss: 2.520531460260531
Validation loss: 2.502703390450456

Epoch: 6| Step: 8
Training loss: 2.227459857206479
Validation loss: 2.5162904343723196

Epoch: 6| Step: 9
Training loss: 1.9937062177963027
Validation loss: 2.5005837097457153

Epoch: 6| Step: 10
Training loss: 2.2255370874136458
Validation loss: 2.5493170020791

Epoch: 6| Step: 11
Training loss: 2.4932922019557786
Validation loss: 2.4896669302128807

Epoch: 6| Step: 12
Training loss: 2.3739305397226724
Validation loss: 2.5304264099570353

Epoch: 6| Step: 13
Training loss: 3.387140126837177
Validation loss: 2.468830624986745

Epoch: 200| Step: 0
Training loss: 2.3256212122413977
Validation loss: 2.527961152041021

Epoch: 6| Step: 1
Training loss: 2.4391618589878
Validation loss: 2.5357151701856253

Epoch: 6| Step: 2
Training loss: 2.4187497555439355
Validation loss: 2.5346463736041827

Epoch: 6| Step: 3
Training loss: 2.1339794501733627
Validation loss: 2.512106465147957

Epoch: 6| Step: 4
Training loss: 2.3766883069409017
Validation loss: 2.5255741922870794

Epoch: 6| Step: 5
Training loss: 2.777828126027203
Validation loss: 2.5196553136067044

Epoch: 6| Step: 6
Training loss: 3.3897885464237048
Validation loss: 2.5214944872651737

Epoch: 6| Step: 7
Training loss: 2.9755586949031883
Validation loss: 2.528920985344644

Epoch: 6| Step: 8
Training loss: 2.409394129814469
Validation loss: 2.5288411797875696

Epoch: 6| Step: 9
Training loss: 2.1026118611845472
Validation loss: 2.5157507888731683

Epoch: 6| Step: 10
Training loss: 1.8721667181831343
Validation loss: 2.5155816649919185

Epoch: 6| Step: 11
Training loss: 2.760123951105071
Validation loss: 2.5060424180267824

Epoch: 6| Step: 12
Training loss: 2.0920968432795255
Validation loss: 2.462640987703755

Epoch: 6| Step: 13
Training loss: 1.8965787854252065
Validation loss: 2.5127898453378306

Epoch: 201| Step: 0
Training loss: 2.4872425732085235
Validation loss: 2.5367902232144366

Epoch: 6| Step: 1
Training loss: 3.1355069338624983
Validation loss: 2.517333424639522

Epoch: 6| Step: 2
Training loss: 2.3290782231535014
Validation loss: 2.500488911269738

Epoch: 6| Step: 3
Training loss: 3.173686895896253
Validation loss: 2.530605173767224

Epoch: 6| Step: 4
Training loss: 1.8802042896794366
Validation loss: 2.50629912264767

Epoch: 6| Step: 5
Training loss: 2.1606249447382004
Validation loss: 2.507683243945875

Epoch: 6| Step: 6
Training loss: 2.317321905146705
Validation loss: 2.5145810447110377

Epoch: 6| Step: 7
Training loss: 2.775674267206849
Validation loss: 2.484440679927355

Epoch: 6| Step: 8
Training loss: 2.4084741791015585
Validation loss: 2.5231556363050216

Epoch: 6| Step: 9
Training loss: 2.4173348478693706
Validation loss: 2.519294059993167

Epoch: 6| Step: 10
Training loss: 2.5779742572526065
Validation loss: 2.510233221924242

Epoch: 6| Step: 11
Training loss: 2.0455049778279317
Validation loss: 2.5266909919287754

Epoch: 6| Step: 12
Training loss: 2.2135100478363676
Validation loss: 2.5280412221670514

Epoch: 6| Step: 13
Training loss: 2.3091633411917205
Validation loss: 2.5138430517473096

Epoch: 202| Step: 0
Training loss: 2.8419712078721897
Validation loss: 2.4983795000238302

Epoch: 6| Step: 1
Training loss: 2.2186651079297706
Validation loss: 2.5110374101588437

Epoch: 6| Step: 2
Training loss: 2.3084163178034567
Validation loss: 2.514529761813298

Epoch: 6| Step: 3
Training loss: 2.5850943029088205
Validation loss: 2.5169584255799546

Epoch: 6| Step: 4
Training loss: 2.935009489027862
Validation loss: 2.5138171637537927

Epoch: 6| Step: 5
Training loss: 2.278553244026625
Validation loss: 2.5094992543722134

Epoch: 6| Step: 6
Training loss: 1.079977958683871
Validation loss: 2.5333612029491537

Epoch: 6| Step: 7
Training loss: 2.650485077132864
Validation loss: 2.4947769610240247

Epoch: 6| Step: 8
Training loss: 2.466837564768578
Validation loss: 2.5022009646527885

Epoch: 6| Step: 9
Training loss: 1.9526994165235734
Validation loss: 2.508646778242122

Epoch: 6| Step: 10
Training loss: 2.096704704089229
Validation loss: 2.4998489026585125

Epoch: 6| Step: 11
Training loss: 2.623896775704178
Validation loss: 2.5368886997214455

Epoch: 6| Step: 12
Training loss: 3.104993791067209
Validation loss: 2.5279341237318924

Epoch: 6| Step: 13
Training loss: 3.1047184259633918
Validation loss: 2.5235790189000986

Epoch: 203| Step: 0
Training loss: 2.2471694095516273
Validation loss: 2.5050382032979877

Epoch: 6| Step: 1
Training loss: 2.8445416700449426
Validation loss: 2.533070359265401

Epoch: 6| Step: 2
Training loss: 2.727204517754203
Validation loss: 2.523868343301756

Epoch: 6| Step: 3
Training loss: 2.8196950643447054
Validation loss: 2.521484393300939

Epoch: 6| Step: 4
Training loss: 1.9757049268263067
Validation loss: 2.50062083462388

Epoch: 6| Step: 5
Training loss: 2.8456010767456625
Validation loss: 2.5122967114093506

Epoch: 6| Step: 6
Training loss: 3.0291975485912292
Validation loss: 2.528525604674313

Epoch: 6| Step: 7
Training loss: 2.147491579760918
Validation loss: 2.485373275484378

Epoch: 6| Step: 8
Training loss: 2.014584768216537
Validation loss: 2.5186067628032065

Epoch: 6| Step: 9
Training loss: 2.4125869340007893
Validation loss: 2.5818959093118132

Epoch: 6| Step: 10
Training loss: 2.1755127664102467
Validation loss: 2.513980567236561

Epoch: 6| Step: 11
Training loss: 2.192893436498986
Validation loss: 2.527340029318643

Epoch: 6| Step: 12
Training loss: 2.173478447139668
Validation loss: 2.4998431864133326

Epoch: 6| Step: 13
Training loss: 2.787671936617075
Validation loss: 2.5211470579999986

Epoch: 204| Step: 0
Training loss: 2.322451854325859
Validation loss: 2.5041656387941513

Epoch: 6| Step: 1
Training loss: 2.2508674644792763
Validation loss: 2.543039596289284

Epoch: 6| Step: 2
Training loss: 2.7147896521138812
Validation loss: 2.5183299370318735

Epoch: 6| Step: 3
Training loss: 2.7188001606139593
Validation loss: 2.4996564870092173

Epoch: 6| Step: 4
Training loss: 2.9036108359835033
Validation loss: 2.497303025828992

Epoch: 6| Step: 5
Training loss: 2.910240141408276
Validation loss: 2.495919387974286

Epoch: 6| Step: 6
Training loss: 2.409346433635929
Validation loss: 2.510421115866366

Epoch: 6| Step: 7
Training loss: 1.6572979904364256
Validation loss: 2.5000092085802033

Epoch: 6| Step: 8
Training loss: 2.651763807667737
Validation loss: 2.5534368733233883

Epoch: 6| Step: 9
Training loss: 2.6116737701897175
Validation loss: 2.5053112749169295

Epoch: 6| Step: 10
Training loss: 2.571497867044109
Validation loss: 2.519854606196386

Epoch: 6| Step: 11
Training loss: 2.2767896550676023
Validation loss: 2.5295077857863126

Epoch: 6| Step: 12
Training loss: 2.1247443999111075
Validation loss: 2.51709497440776

Epoch: 6| Step: 13
Training loss: 1.843143945430378
Validation loss: 2.502261003836152

Epoch: 205| Step: 0
Training loss: 2.301460515577172
Validation loss: 2.545399825012014

Epoch: 6| Step: 1
Training loss: 2.102028947422176
Validation loss: 2.484691809923871

Epoch: 6| Step: 2
Training loss: 2.0426901632846652
Validation loss: 2.5195896918706673

Epoch: 6| Step: 3
Training loss: 1.9610111177159455
Validation loss: 2.528785164815319

Epoch: 6| Step: 4
Training loss: 2.0181739717545
Validation loss: 2.492768025530193

Epoch: 6| Step: 5
Training loss: 3.204236540179255
Validation loss: 2.5155235058083076

Epoch: 6| Step: 6
Training loss: 2.6652152065106076
Validation loss: 2.522095241247086

Epoch: 6| Step: 7
Training loss: 2.724724792442268
Validation loss: 2.4989932863556237

Epoch: 6| Step: 8
Training loss: 3.188053157904063
Validation loss: 2.510262290331227

Epoch: 6| Step: 9
Training loss: 2.4476793431161545
Validation loss: 2.4957842054216988

Epoch: 6| Step: 10
Training loss: 2.760901949469586
Validation loss: 2.5064625836035135

Epoch: 6| Step: 11
Training loss: 1.9235932647330154
Validation loss: 2.5234534338254995

Epoch: 6| Step: 12
Training loss: 2.5187895400207068
Validation loss: 2.5098157938086345

Epoch: 6| Step: 13
Training loss: 1.8064473476639729
Validation loss: 2.5295482076528577

Epoch: 206| Step: 0
Training loss: 1.914941951579076
Validation loss: 2.530085107361739

Epoch: 6| Step: 1
Training loss: 2.306923405613033
Validation loss: 2.508285307684227

Epoch: 6| Step: 2
Training loss: 2.1547789047361445
Validation loss: 2.5216220431046925

Epoch: 6| Step: 3
Training loss: 2.7638555351131924
Validation loss: 2.5205988180375987

Epoch: 6| Step: 4
Training loss: 2.785361604757377
Validation loss: 2.5436850084835494

Epoch: 6| Step: 5
Training loss: 2.2281151427576598
Validation loss: 2.4899652539704484

Epoch: 6| Step: 6
Training loss: 2.6713462440242832
Validation loss: 2.542872111938736

Epoch: 6| Step: 7
Training loss: 2.4225070251740908
Validation loss: 2.5161060455979785

Epoch: 6| Step: 8
Training loss: 3.1622830885730946
Validation loss: 2.5031472074791408

Epoch: 6| Step: 9
Training loss: 2.6344267200900027
Validation loss: 2.5084307612595573

Epoch: 6| Step: 10
Training loss: 2.6791179262971223
Validation loss: 2.511779979358266

Epoch: 6| Step: 11
Training loss: 2.3373948466266596
Validation loss: 2.5160014093143612

Epoch: 6| Step: 12
Training loss: 1.9952335183710754
Validation loss: 2.5441977887314287

Epoch: 6| Step: 13
Training loss: 2.237770328494598
Validation loss: 2.516187442554341

Epoch: 207| Step: 0
Training loss: 2.5335039064638094
Validation loss: 2.5243105018137975

Epoch: 6| Step: 1
Training loss: 3.1582363412368895
Validation loss: 2.527987715601913

Epoch: 6| Step: 2
Training loss: 2.1782905723657415
Validation loss: 2.511095121689523

Epoch: 6| Step: 3
Training loss: 2.4812137476196914
Validation loss: 2.5206864701598723

Epoch: 6| Step: 4
Training loss: 2.4525345984768645
Validation loss: 2.538487865462211

Epoch: 6| Step: 5
Training loss: 2.3379457956431495
Validation loss: 2.497803014955692

Epoch: 6| Step: 6
Training loss: 2.544387913602978
Validation loss: 2.5247778368455687

Epoch: 6| Step: 7
Training loss: 2.199660803915621
Validation loss: 2.528706276022966

Epoch: 6| Step: 8
Training loss: 2.519492452160097
Validation loss: 2.5483490129589628

Epoch: 6| Step: 9
Training loss: 2.0565737344817245
Validation loss: 2.5163977059234623

Epoch: 6| Step: 10
Training loss: 2.7011014387169485
Validation loss: 2.4968861322564075

Epoch: 6| Step: 11
Training loss: 2.355174791203222
Validation loss: 2.535465237512415

Epoch: 6| Step: 12
Training loss: 2.2243294208998505
Validation loss: 2.522668124162244

Epoch: 6| Step: 13
Training loss: 2.573209480787163
Validation loss: 2.494304356000618

Epoch: 208| Step: 0
Training loss: 1.754058355519876
Validation loss: 2.521191890732097

Epoch: 6| Step: 1
Training loss: 2.2434424101832517
Validation loss: 2.497443450662318

Epoch: 6| Step: 2
Training loss: 2.714096295767295
Validation loss: 2.514423817073432

Epoch: 6| Step: 3
Training loss: 2.956289059661193
Validation loss: 2.4996294793000646

Epoch: 6| Step: 4
Training loss: 2.791977926713536
Validation loss: 2.5197497216619817

Epoch: 6| Step: 5
Training loss: 1.9993787038899369
Validation loss: 2.5053980743560462

Epoch: 6| Step: 6
Training loss: 2.3526578360946995
Validation loss: 2.511663980151119

Epoch: 6| Step: 7
Training loss: 2.190201317578692
Validation loss: 2.543394245668778

Epoch: 6| Step: 8
Training loss: 1.5371392330774218
Validation loss: 2.5275453706850497

Epoch: 6| Step: 9
Training loss: 2.789612189654801
Validation loss: 2.5236222181034136

Epoch: 6| Step: 10
Training loss: 3.0024599480161127
Validation loss: 2.5363547108625677

Epoch: 6| Step: 11
Training loss: 2.1022273150138826
Validation loss: 2.5145544893926615

Epoch: 6| Step: 12
Training loss: 2.452393246503922
Validation loss: 2.5021862653297715

Epoch: 6| Step: 13
Training loss: 3.346254783173806
Validation loss: 2.505652551104668

Epoch: 209| Step: 0
Training loss: 2.4929404720661954
Validation loss: 2.53287477656798

Epoch: 6| Step: 1
Training loss: 2.529638554533681
Validation loss: 2.4878623438370915

Epoch: 6| Step: 2
Training loss: 2.309504037434397
Validation loss: 2.486672790000973

Epoch: 6| Step: 3
Training loss: 2.5366866513574933
Validation loss: 2.5093884846394436

Epoch: 6| Step: 4
Training loss: 2.9517786903024144
Validation loss: 2.521214588460454

Epoch: 6| Step: 5
Training loss: 2.4904879811120795
Validation loss: 2.5193897392951414

Epoch: 6| Step: 6
Training loss: 2.8641306201515238
Validation loss: 2.5365417214756136

Epoch: 6| Step: 7
Training loss: 2.324411797720727
Validation loss: 2.5261310473199967

Epoch: 6| Step: 8
Training loss: 2.106847647707502
Validation loss: 2.511064906711904

Epoch: 6| Step: 9
Training loss: 2.5815996279630817
Validation loss: 2.507377997053286

Epoch: 6| Step: 10
Training loss: 2.2601298019807974
Validation loss: 2.510237116046141

Epoch: 6| Step: 11
Training loss: 1.8243719849360645
Validation loss: 2.5197379826569923

Epoch: 6| Step: 12
Training loss: 2.099691586326532
Validation loss: 2.527555336988133

Epoch: 6| Step: 13
Training loss: 2.849184649101011
Validation loss: 2.533126087190461

Epoch: 210| Step: 0
Training loss: 2.04244375686063
Validation loss: 2.5092030098852627

Epoch: 6| Step: 1
Training loss: 2.858031744287488
Validation loss: 2.531225895333277

Epoch: 6| Step: 2
Training loss: 2.2502504845110005
Validation loss: 2.4992552119554596

Epoch: 6| Step: 3
Training loss: 2.7178619512906086
Validation loss: 2.528366090797557

Epoch: 6| Step: 4
Training loss: 2.9203763167999557
Validation loss: 2.5196948353918156

Epoch: 6| Step: 5
Training loss: 2.1194165871943302
Validation loss: 2.5440495421315727

Epoch: 6| Step: 6
Training loss: 2.4386045447554436
Validation loss: 2.5153370634302092

Epoch: 6| Step: 7
Training loss: 2.4377124400362766
Validation loss: 2.4762874010666236

Epoch: 6| Step: 8
Training loss: 1.8368676609179542
Validation loss: 2.5270635495147227

Epoch: 6| Step: 9
Training loss: 2.3309974104033513
Validation loss: 2.5351703142278192

Epoch: 6| Step: 10
Training loss: 1.8448879803401783
Validation loss: 2.4907939176574794

Epoch: 6| Step: 11
Training loss: 2.7960520977574834
Validation loss: 2.5042129479018786

Epoch: 6| Step: 12
Training loss: 2.742379119351103
Validation loss: 2.487163257319445

Epoch: 6| Step: 13
Training loss: 2.8408887952599784
Validation loss: 2.4836581008371263

Epoch: 211| Step: 0
Training loss: 2.4772536218970695
Validation loss: 2.5178518638194354

Epoch: 6| Step: 1
Training loss: 2.4963092263215247
Validation loss: 2.5306234876978957

Epoch: 6| Step: 2
Training loss: 2.420620501076014
Validation loss: 2.5246213752347

Epoch: 6| Step: 3
Training loss: 2.162252171598577
Validation loss: 2.5196486899537054

Epoch: 6| Step: 4
Training loss: 2.3970238312390797
Validation loss: 2.53837383513498

Epoch: 6| Step: 5
Training loss: 1.7203817771107937
Validation loss: 2.534519237062886

Epoch: 6| Step: 6
Training loss: 2.9417619021581705
Validation loss: 2.537940289524997

Epoch: 6| Step: 7
Training loss: 2.3484960694664916
Validation loss: 2.507590796397238

Epoch: 6| Step: 8
Training loss: 2.3472869633380653
Validation loss: 2.5145773703941487

Epoch: 6| Step: 9
Training loss: 2.7092165362734444
Validation loss: 2.5183566805671576

Epoch: 6| Step: 10
Training loss: 2.7718995034082003
Validation loss: 2.5385606909439664

Epoch: 6| Step: 11
Training loss: 2.247806433472335
Validation loss: 2.536255778979277

Epoch: 6| Step: 12
Training loss: 2.551451894274871
Validation loss: 2.5403664960319094

Epoch: 6| Step: 13
Training loss: 2.736711386742298
Validation loss: 2.5319413902247447

Epoch: 212| Step: 0
Training loss: 1.0367464914182043
Validation loss: 2.5268266627567266

Epoch: 6| Step: 1
Training loss: 1.983603199729331
Validation loss: 2.538838605703754

Epoch: 6| Step: 2
Training loss: 1.8931434188657053
Validation loss: 2.512112084093551

Epoch: 6| Step: 3
Training loss: 2.7035290190132466
Validation loss: 2.5602126508771272

Epoch: 6| Step: 4
Training loss: 2.0143846347447045
Validation loss: 2.5239486153852333

Epoch: 6| Step: 5
Training loss: 2.414070104691956
Validation loss: 2.53071937534238

Epoch: 6| Step: 6
Training loss: 3.0365397124511593
Validation loss: 2.500420913333286

Epoch: 6| Step: 7
Training loss: 2.4332388776797176
Validation loss: 2.5077822464940365

Epoch: 6| Step: 8
Training loss: 2.8522501913173097
Validation loss: 2.5088953085351693

Epoch: 6| Step: 9
Training loss: 2.6355894934234803
Validation loss: 2.5052190857200864

Epoch: 6| Step: 10
Training loss: 2.853416028592144
Validation loss: 2.5343749985433717

Epoch: 6| Step: 11
Training loss: 2.4757745012628907
Validation loss: 2.504087814125068

Epoch: 6| Step: 12
Training loss: 2.395721985814086
Validation loss: 2.518509942187664

Epoch: 6| Step: 13
Training loss: 2.64871133384548
Validation loss: 2.485350452605536

Epoch: 213| Step: 0
Training loss: 2.581703430695505
Validation loss: 2.516662649122798

Epoch: 6| Step: 1
Training loss: 2.496604711920862
Validation loss: 2.520445863469486

Epoch: 6| Step: 2
Training loss: 1.975227115948041
Validation loss: 2.4816638974412366

Epoch: 6| Step: 3
Training loss: 2.178309507504808
Validation loss: 2.510184655701369

Epoch: 6| Step: 4
Training loss: 2.6850697374433468
Validation loss: 2.5612371410741988

Epoch: 6| Step: 5
Training loss: 2.536805073708784
Validation loss: 2.5122546649694466

Epoch: 6| Step: 6
Training loss: 2.3694605222183633
Validation loss: 2.515327357025825

Epoch: 6| Step: 7
Training loss: 2.4736804257285967
Validation loss: 2.5131327398497283

Epoch: 6| Step: 8
Training loss: 2.211561350879422
Validation loss: 2.5012836042551134

Epoch: 6| Step: 9
Training loss: 2.55268057757008
Validation loss: 2.545718991325832

Epoch: 6| Step: 10
Training loss: 2.904460397002153
Validation loss: 2.5336063314689357

Epoch: 6| Step: 11
Training loss: 2.899547436358536
Validation loss: 2.5295892957310433

Epoch: 6| Step: 12
Training loss: 2.585162919774563
Validation loss: 2.4983034201736443

Epoch: 6| Step: 13
Training loss: 1.6259099906604584
Validation loss: 2.5309303544715585

Epoch: 214| Step: 0
Training loss: 2.0657091316945757
Validation loss: 2.5017328056053687

Epoch: 6| Step: 1
Training loss: 2.636671368208534
Validation loss: 2.511312031390447

Epoch: 6| Step: 2
Training loss: 2.094709489659446
Validation loss: 2.4964805954288503

Epoch: 6| Step: 3
Training loss: 2.393420012913599
Validation loss: 2.532171331116461

Epoch: 6| Step: 4
Training loss: 3.0961041981907984
Validation loss: 2.5184060430266886

Epoch: 6| Step: 5
Training loss: 1.9575799412682202
Validation loss: 2.538201688709727

Epoch: 6| Step: 6
Training loss: 2.5996529384204417
Validation loss: 2.4988765345661705

Epoch: 6| Step: 7
Training loss: 2.353698672698783
Validation loss: 2.519253560094646

Epoch: 6| Step: 8
Training loss: 2.9308991635019126
Validation loss: 2.519565806335109

Epoch: 6| Step: 9
Training loss: 2.2672489002833767
Validation loss: 2.499904220038369

Epoch: 6| Step: 10
Training loss: 2.5846712175607487
Validation loss: 2.4942446535928404

Epoch: 6| Step: 11
Training loss: 2.1925039551023957
Validation loss: 2.541088182316503

Epoch: 6| Step: 12
Training loss: 2.553759951877855
Validation loss: 2.5328335707318934

Epoch: 6| Step: 13
Training loss: 2.037209673832684
Validation loss: 2.535337111152604

Epoch: 215| Step: 0
Training loss: 1.7389802020428307
Validation loss: 2.5508465929315856

Epoch: 6| Step: 1
Training loss: 2.151414175468484
Validation loss: 2.5112475504044225

Epoch: 6| Step: 2
Training loss: 2.461940113067499
Validation loss: 2.478653315788664

Epoch: 6| Step: 3
Training loss: 2.69236155969577
Validation loss: 2.5241520662767423

Epoch: 6| Step: 4
Training loss: 2.021764114853263
Validation loss: 2.540766786840987

Epoch: 6| Step: 5
Training loss: 2.697732916702849
Validation loss: 2.5128768608469065

Epoch: 6| Step: 6
Training loss: 2.8655054631152015
Validation loss: 2.5269703237079835

Epoch: 6| Step: 7
Training loss: 1.8836953400374965
Validation loss: 2.497278414869565

Epoch: 6| Step: 8
Training loss: 2.0962808600140583
Validation loss: 2.4965689238590216

Epoch: 6| Step: 9
Training loss: 2.8860181234402678
Validation loss: 2.5206908393627683

Epoch: 6| Step: 10
Training loss: 2.096063047931364
Validation loss: 2.496820282759119

Epoch: 6| Step: 11
Training loss: 2.844365483005925
Validation loss: 2.5042114747524153

Epoch: 6| Step: 12
Training loss: 2.740411601789574
Validation loss: 2.5344490437485687

Epoch: 6| Step: 13
Training loss: 2.532956807698613
Validation loss: 2.48123271230747

Epoch: 216| Step: 0
Training loss: 2.3533331709295258
Validation loss: 2.498726577892375

Epoch: 6| Step: 1
Training loss: 2.0914742927184284
Validation loss: 2.520604261422626

Epoch: 6| Step: 2
Training loss: 2.346348657012353
Validation loss: 2.528932893084744

Epoch: 6| Step: 3
Training loss: 2.2436721618241986
Validation loss: 2.5302192942236266

Epoch: 6| Step: 4
Training loss: 2.0642212130265
Validation loss: 2.533552897911824

Epoch: 6| Step: 5
Training loss: 2.805044167454778
Validation loss: 2.555368476767056

Epoch: 6| Step: 6
Training loss: 2.614399897205813
Validation loss: 2.518584296045267

Epoch: 6| Step: 7
Training loss: 2.6565878260587352
Validation loss: 2.53003914332578

Epoch: 6| Step: 8
Training loss: 2.5662663391157996
Validation loss: 2.526038737031595

Epoch: 6| Step: 9
Training loss: 2.5829862135793555
Validation loss: 2.521918346453718

Epoch: 6| Step: 10
Training loss: 2.536217888957361
Validation loss: 2.4897059992500346

Epoch: 6| Step: 11
Training loss: 2.496357839639881
Validation loss: 2.505384177602318

Epoch: 6| Step: 12
Training loss: 2.5122576620414234
Validation loss: 2.50707291648567

Epoch: 6| Step: 13
Training loss: 1.9267273101480364
Validation loss: 2.485439222998625

Epoch: 217| Step: 0
Training loss: 3.238691021033671
Validation loss: 2.498826633451071

Epoch: 6| Step: 1
Training loss: 3.461368714759611
Validation loss: 2.5169130519545457

Epoch: 6| Step: 2
Training loss: 1.7802520097791643
Validation loss: 2.5135424296705997

Epoch: 6| Step: 3
Training loss: 2.5472691704839896
Validation loss: 2.502422754425802

Epoch: 6| Step: 4
Training loss: 2.800637724323307
Validation loss: 2.526762832355204

Epoch: 6| Step: 5
Training loss: 1.8796765816261243
Validation loss: 2.514880952408691

Epoch: 6| Step: 6
Training loss: 2.2556995682032186
Validation loss: 2.5116668534034003

Epoch: 6| Step: 7
Training loss: 2.3625019901004864
Validation loss: 2.5159993928421795

Epoch: 6| Step: 8
Training loss: 1.8104471880846196
Validation loss: 2.510529751265167

Epoch: 6| Step: 9
Training loss: 2.3157809926977415
Validation loss: 2.536369267756492

Epoch: 6| Step: 10
Training loss: 2.4631140393562827
Validation loss: 2.5113681186200654

Epoch: 6| Step: 11
Training loss: 2.6167870058544676
Validation loss: 2.4715967770596605

Epoch: 6| Step: 12
Training loss: 2.380994960313356
Validation loss: 2.5420116376715565

Epoch: 6| Step: 13
Training loss: 1.2270822092168783
Validation loss: 2.5255216505784825

Epoch: 218| Step: 0
Training loss: 2.404108853584382
Validation loss: 2.506547059312156

Epoch: 6| Step: 1
Training loss: 2.3117569425521336
Validation loss: 2.492609264621661

Epoch: 6| Step: 2
Training loss: 2.9569306249391656
Validation loss: 2.538914970358886

Epoch: 6| Step: 3
Training loss: 2.175483614769342
Validation loss: 2.523889088051642

Epoch: 6| Step: 4
Training loss: 2.5113063728837086
Validation loss: 2.530943969149276

Epoch: 6| Step: 5
Training loss: 2.584757463598835
Validation loss: 2.53155196543583

Epoch: 6| Step: 6
Training loss: 2.3337336832547777
Validation loss: 2.5380630613657

Epoch: 6| Step: 7
Training loss: 2.208331509955271
Validation loss: 2.5258415654583515

Epoch: 6| Step: 8
Training loss: 2.343889766340425
Validation loss: 2.5075913924284223

Epoch: 6| Step: 9
Training loss: 2.582014906192364
Validation loss: 2.512368761395154

Epoch: 6| Step: 10
Training loss: 2.1975881924687295
Validation loss: 2.497124451768352

Epoch: 6| Step: 11
Training loss: 1.818764613771226
Validation loss: 2.486153552416317

Epoch: 6| Step: 12
Training loss: 2.770339309898297
Validation loss: 2.5299659641257106

Epoch: 6| Step: 13
Training loss: 2.923959406757509
Validation loss: 2.540670594771417

Epoch: 219| Step: 0
Training loss: 1.6548666305047972
Validation loss: 2.5068494457784682

Epoch: 6| Step: 1
Training loss: 2.269290983577415
Validation loss: 2.536476238452259

Epoch: 6| Step: 2
Training loss: 2.2943662059992613
Validation loss: 2.514972232911675

Epoch: 6| Step: 3
Training loss: 2.4255479095326047
Validation loss: 2.479210815353706

Epoch: 6| Step: 4
Training loss: 2.8465775067424546
Validation loss: 2.541148043216452

Epoch: 6| Step: 5
Training loss: 2.462220647622411
Validation loss: 2.5386597930384047

Epoch: 6| Step: 6
Training loss: 2.1323536809026273
Validation loss: 2.506607607536788

Epoch: 6| Step: 7
Training loss: 2.6191212356839637
Validation loss: 2.497530122190766

Epoch: 6| Step: 8
Training loss: 2.875558881756082
Validation loss: 2.501365557341262

Epoch: 6| Step: 9
Training loss: 2.1586014462035936
Validation loss: 2.5115795693710385

Epoch: 6| Step: 10
Training loss: 2.356570971253298
Validation loss: 2.5186821662596897

Epoch: 6| Step: 11
Training loss: 2.2858635223579467
Validation loss: 2.501292914702018

Epoch: 6| Step: 12
Training loss: 2.842123048291692
Validation loss: 2.533355074558757

Epoch: 6| Step: 13
Training loss: 2.6074907272334915
Validation loss: 2.5190967691085637

Epoch: 220| Step: 0
Training loss: 2.6849615839477834
Validation loss: 2.507979327558279

Epoch: 6| Step: 1
Training loss: 2.56960057193802
Validation loss: 2.55117352868247

Epoch: 6| Step: 2
Training loss: 2.8380895598718365
Validation loss: 2.515041331759686

Epoch: 6| Step: 3
Training loss: 2.6712917115056873
Validation loss: 2.5397769167116735

Epoch: 6| Step: 4
Training loss: 1.9820867847185102
Validation loss: 2.544529355417073

Epoch: 6| Step: 5
Training loss: 1.759502700317947
Validation loss: 2.501359750271336

Epoch: 6| Step: 6
Training loss: 1.9635034316275548
Validation loss: 2.5191047248287517

Epoch: 6| Step: 7
Training loss: 2.500135608809843
Validation loss: 2.5100991278838216

Epoch: 6| Step: 8
Training loss: 2.6631312378687433
Validation loss: 2.4856423212727856

Epoch: 6| Step: 9
Training loss: 2.2206595357007197
Validation loss: 2.5607875451400264

Epoch: 6| Step: 10
Training loss: 2.9588378869989698
Validation loss: 2.5509937485996366

Epoch: 6| Step: 11
Training loss: 2.0692526353646676
Validation loss: 2.5502740773216437

Epoch: 6| Step: 12
Training loss: 2.2816948652881854
Validation loss: 2.4898931806733935

Epoch: 6| Step: 13
Training loss: 2.5343300247435034
Validation loss: 2.5198203142737428

Epoch: 221| Step: 0
Training loss: 3.206746951847556
Validation loss: 2.500207463752021

Epoch: 6| Step: 1
Training loss: 2.1751111846216755
Validation loss: 2.5304328108786294

Epoch: 6| Step: 2
Training loss: 2.2998876212488155
Validation loss: 2.5291064346968493

Epoch: 6| Step: 3
Training loss: 2.2697012176162166
Validation loss: 2.5215859828155938

Epoch: 6| Step: 4
Training loss: 2.1376179088173544
Validation loss: 2.5086506554101393

Epoch: 6| Step: 5
Training loss: 2.574684668186472
Validation loss: 2.501608396812916

Epoch: 6| Step: 6
Training loss: 1.7299369873301909
Validation loss: 2.5545887191193066

Epoch: 6| Step: 7
Training loss: 2.4576174668948023
Validation loss: 2.5074356353576297

Epoch: 6| Step: 8
Training loss: 2.199648014000887
Validation loss: 2.544412846727364

Epoch: 6| Step: 9
Training loss: 2.8390322946590776
Validation loss: 2.516643487418397

Epoch: 6| Step: 10
Training loss: 2.4424749614013965
Validation loss: 2.524386846926788

Epoch: 6| Step: 11
Training loss: 2.4199206927206554
Validation loss: 2.5418209886090497

Epoch: 6| Step: 12
Training loss: 2.5033023004686705
Validation loss: 2.526774147091985

Epoch: 6| Step: 13
Training loss: 2.1610739002222203
Validation loss: 2.5494512737687844

Epoch: 222| Step: 0
Training loss: 2.321795570791362
Validation loss: 2.5391398191077825

Epoch: 6| Step: 1
Training loss: 2.2558998533638372
Validation loss: 2.5284676441142095

Epoch: 6| Step: 2
Training loss: 2.623763656108151
Validation loss: 2.530590124799779

Epoch: 6| Step: 3
Training loss: 2.559501382131738
Validation loss: 2.500861181603052

Epoch: 6| Step: 4
Training loss: 2.418059660106927
Validation loss: 2.518933405929427

Epoch: 6| Step: 5
Training loss: 2.4537807487158574
Validation loss: 2.5540952070842

Epoch: 6| Step: 6
Training loss: 2.3962501716116225
Validation loss: 2.5085236893496248

Epoch: 6| Step: 7
Training loss: 2.420196246709988
Validation loss: 2.5232989413725733

Epoch: 6| Step: 8
Training loss: 1.7819773209709584
Validation loss: 2.552344066836144

Epoch: 6| Step: 9
Training loss: 2.704829918666979
Validation loss: 2.4863193689253724

Epoch: 6| Step: 10
Training loss: 2.437598788876009
Validation loss: 2.5449333487514347

Epoch: 6| Step: 11
Training loss: 3.147758013002723
Validation loss: 2.5312368214602325

Epoch: 6| Step: 12
Training loss: 2.0878076064017193
Validation loss: 2.5222364853041306

Epoch: 6| Step: 13
Training loss: 2.4181820185316396
Validation loss: 2.524176951470489

Epoch: 223| Step: 0
Training loss: 2.2343745731806846
Validation loss: 2.5054567374332235

Epoch: 6| Step: 1
Training loss: 1.9231358555787086
Validation loss: 2.557135690974439

Epoch: 6| Step: 2
Training loss: 2.3247145692911024
Validation loss: 2.5013337771508044

Epoch: 6| Step: 3
Training loss: 1.884547277264909
Validation loss: 2.5276287081968145

Epoch: 6| Step: 4
Training loss: 2.3766337347045186
Validation loss: 2.5041202463126737

Epoch: 6| Step: 5
Training loss: 2.314536151925862
Validation loss: 2.5348826777662103

Epoch: 6| Step: 6
Training loss: 2.989204373858751
Validation loss: 2.510579317651692

Epoch: 6| Step: 7
Training loss: 1.6391081564538281
Validation loss: 2.519221696711513

Epoch: 6| Step: 8
Training loss: 2.6523520198462753
Validation loss: 2.499851667454146

Epoch: 6| Step: 9
Training loss: 2.5510823888450354
Validation loss: 2.513550094442745

Epoch: 6| Step: 10
Training loss: 2.6647815796626877
Validation loss: 2.521467367270106

Epoch: 6| Step: 11
Training loss: 2.7052309509514783
Validation loss: 2.4989386884790905

Epoch: 6| Step: 12
Training loss: 2.9584467333339988
Validation loss: 2.5203544435047958

Epoch: 6| Step: 13
Training loss: 2.697882447093227
Validation loss: 2.5020242576397336

Epoch: 224| Step: 0
Training loss: 2.3294065903511534
Validation loss: 2.532292375102873

Epoch: 6| Step: 1
Training loss: 2.725846598293675
Validation loss: 2.468794920286661

Epoch: 6| Step: 2
Training loss: 1.8756164173309926
Validation loss: 2.5381634530126536

Epoch: 6| Step: 3
Training loss: 2.609065054571153
Validation loss: 2.5233887347979587

Epoch: 6| Step: 4
Training loss: 2.3251122560370776
Validation loss: 2.533051890972689

Epoch: 6| Step: 5
Training loss: 3.3952447736141065
Validation loss: 2.51218297092146

Epoch: 6| Step: 6
Training loss: 2.1898676456885573
Validation loss: 2.5231870989701966

Epoch: 6| Step: 7
Training loss: 3.1646394933489295
Validation loss: 2.539257580856828

Epoch: 6| Step: 8
Training loss: 1.9102514824404067
Validation loss: 2.509237389692739

Epoch: 6| Step: 9
Training loss: 2.029054247217919
Validation loss: 2.5165444896612166

Epoch: 6| Step: 10
Training loss: 2.1025843068387298
Validation loss: 2.509985705555053

Epoch: 6| Step: 11
Training loss: 2.0566392338848574
Validation loss: 2.4873830218158828

Epoch: 6| Step: 12
Training loss: 2.4674000972787673
Validation loss: 2.509863530621031

Epoch: 6| Step: 13
Training loss: 1.9711690300934583
Validation loss: 2.528032202907284

Epoch: 225| Step: 0
Training loss: 2.619744716903612
Validation loss: 2.519290321317784

Epoch: 6| Step: 1
Training loss: 1.7651579922862242
Validation loss: 2.5320227296914455

Epoch: 6| Step: 2
Training loss: 2.646334913474423
Validation loss: 2.525556969495087

Epoch: 6| Step: 3
Training loss: 1.9052290136495151
Validation loss: 2.5072429578752287

Epoch: 6| Step: 4
Training loss: 3.036735997446679
Validation loss: 2.4984366143134684

Epoch: 6| Step: 5
Training loss: 2.801421918176796
Validation loss: 2.524940901594362

Epoch: 6| Step: 6
Training loss: 1.9184981662362695
Validation loss: 2.5063467085708173

Epoch: 6| Step: 7
Training loss: 2.3632237451035794
Validation loss: 2.5396674453572885

Epoch: 6| Step: 8
Training loss: 2.635347589082549
Validation loss: 2.5273090743154967

Epoch: 6| Step: 9
Training loss: 2.4045732278919427
Validation loss: 2.5088501078985885

Epoch: 6| Step: 10
Training loss: 2.3805385057315305
Validation loss: 2.520804969138515

Epoch: 6| Step: 11
Training loss: 2.5681401032230924
Validation loss: 2.51991086139709

Epoch: 6| Step: 12
Training loss: 2.191620787320434
Validation loss: 2.5277335168202204

Epoch: 6| Step: 13
Training loss: 2.2303214483841916
Validation loss: 2.5038950172453793

Epoch: 226| Step: 0
Training loss: 2.5837259199388805
Validation loss: 2.4761761913041904

Epoch: 6| Step: 1
Training loss: 2.6142844425707543
Validation loss: 2.5009799164804654

Epoch: 6| Step: 2
Training loss: 1.8326015601436574
Validation loss: 2.543997640982649

Epoch: 6| Step: 3
Training loss: 2.248408708527223
Validation loss: 2.530766578573052

Epoch: 6| Step: 4
Training loss: 2.3656427864000684
Validation loss: 2.539799672432932

Epoch: 6| Step: 5
Training loss: 2.580589087899966
Validation loss: 2.487893192502845

Epoch: 6| Step: 6
Training loss: 2.7610024652356087
Validation loss: 2.5597519915163782

Epoch: 6| Step: 7
Training loss: 2.761616792644795
Validation loss: 2.5295780128421983

Epoch: 6| Step: 8
Training loss: 2.6314586928146424
Validation loss: 2.496900474677843

Epoch: 6| Step: 9
Training loss: 1.8040574200028552
Validation loss: 2.5169925914910594

Epoch: 6| Step: 10
Training loss: 2.9739658513728493
Validation loss: 2.5226805608979603

Epoch: 6| Step: 11
Training loss: 2.446822117883215
Validation loss: 2.519188443839718

Epoch: 6| Step: 12
Training loss: 2.295877979857548
Validation loss: 2.513326240831577

Epoch: 6| Step: 13
Training loss: 1.5773528069986695
Validation loss: 2.520752726437085

Epoch: 227| Step: 0
Training loss: 2.373583923604771
Validation loss: 2.5507096639201716

Epoch: 6| Step: 1
Training loss: 2.3678326121142304
Validation loss: 2.5004667082326284

Epoch: 6| Step: 2
Training loss: 2.0175627627531068
Validation loss: 2.5470493140486132

Epoch: 6| Step: 3
Training loss: 2.7010710746399007
Validation loss: 2.4960848576897017

Epoch: 6| Step: 4
Training loss: 1.9637746765412292
Validation loss: 2.52221030433395

Epoch: 6| Step: 5
Training loss: 2.505369333234253
Validation loss: 2.502306861363889

Epoch: 6| Step: 6
Training loss: 2.02733316584123
Validation loss: 2.5466469031503753

Epoch: 6| Step: 7
Training loss: 2.4648745092053614
Validation loss: 2.533032187803489

Epoch: 6| Step: 8
Training loss: 2.691338570107855
Validation loss: 2.4970308638287375

Epoch: 6| Step: 9
Training loss: 2.127611462982504
Validation loss: 2.481866931766698

Epoch: 6| Step: 10
Training loss: 2.3132470444994624
Validation loss: 2.494051111742882

Epoch: 6| Step: 11
Training loss: 2.571588726959939
Validation loss: 2.5253996536939485

Epoch: 6| Step: 12
Training loss: 2.6715253634654417
Validation loss: 2.541266038757474

Epoch: 6| Step: 13
Training loss: 2.9109242904289654
Validation loss: 2.5103630605175646

Epoch: 228| Step: 0
Training loss: 2.141786023783325
Validation loss: 2.505974095080649

Epoch: 6| Step: 1
Training loss: 2.221633327126183
Validation loss: 2.5193725078051874

Epoch: 6| Step: 2
Training loss: 2.4010024877787153
Validation loss: 2.5356749103230922

Epoch: 6| Step: 3
Training loss: 2.4869779949618205
Validation loss: 2.5017287604173424

Epoch: 6| Step: 4
Training loss: 2.4445644611700614
Validation loss: 2.492275356714415

Epoch: 6| Step: 5
Training loss: 2.571273855822865
Validation loss: 2.5023712059677123

Epoch: 6| Step: 6
Training loss: 1.4846098061613
Validation loss: 2.5277303342421025

Epoch: 6| Step: 7
Training loss: 2.5698470879866657
Validation loss: 2.5392185553844357

Epoch: 6| Step: 8
Training loss: 2.56906412992993
Validation loss: 2.516418867808661

Epoch: 6| Step: 9
Training loss: 2.923625890634551
Validation loss: 2.511885836590484

Epoch: 6| Step: 10
Training loss: 1.654592584439867
Validation loss: 2.494028219227826

Epoch: 6| Step: 11
Training loss: 2.5179663714647083
Validation loss: 2.535571554775533

Epoch: 6| Step: 12
Training loss: 2.9819786476890537
Validation loss: 2.535073690863336

Epoch: 6| Step: 13
Training loss: 2.4389788714092346
Validation loss: 2.5225556409703507

Epoch: 229| Step: 0
Training loss: 2.6655473346732728
Validation loss: 2.4758965825666803

Epoch: 6| Step: 1
Training loss: 2.534449027564282
Validation loss: 2.4894711680319426

Epoch: 6| Step: 2
Training loss: 2.6052472936947293
Validation loss: 2.5032363248038925

Epoch: 6| Step: 3
Training loss: 1.9977683491260991
Validation loss: 2.5514970309784264

Epoch: 6| Step: 4
Training loss: 2.299075985934158
Validation loss: 2.5091099429665427

Epoch: 6| Step: 5
Training loss: 2.607885243822337
Validation loss: 2.526065506530316

Epoch: 6| Step: 6
Training loss: 2.4568951058080613
Validation loss: 2.511097649499387

Epoch: 6| Step: 7
Training loss: 3.1379936164319355
Validation loss: 2.5127321400826292

Epoch: 6| Step: 8
Training loss: 2.3148152900977883
Validation loss: 2.5446214659224005

Epoch: 6| Step: 9
Training loss: 2.621118400446446
Validation loss: 2.509513217214667

Epoch: 6| Step: 10
Training loss: 2.1825052185695344
Validation loss: 2.5342206610338645

Epoch: 6| Step: 11
Training loss: 1.9830754147115408
Validation loss: 2.499880140774306

Epoch: 6| Step: 12
Training loss: 2.088193209555124
Validation loss: 2.540112089768781

Epoch: 6| Step: 13
Training loss: 1.8085310000250916
Validation loss: 2.515480140770461

Epoch: 230| Step: 0
Training loss: 2.752170053329083
Validation loss: 2.5335368140963723

Epoch: 6| Step: 1
Training loss: 1.8765727758324764
Validation loss: 2.502520476085752

Epoch: 6| Step: 2
Training loss: 3.286080283755499
Validation loss: 2.485208744099251

Epoch: 6| Step: 3
Training loss: 1.8788572689658989
Validation loss: 2.5157057379070107

Epoch: 6| Step: 4
Training loss: 2.47330759156739
Validation loss: 2.505348227414135

Epoch: 6| Step: 5
Training loss: 2.7589482462371486
Validation loss: 2.5526223780954873

Epoch: 6| Step: 6
Training loss: 2.875369670155695
Validation loss: 2.526004165727951

Epoch: 6| Step: 7
Training loss: 2.2488169208808744
Validation loss: 2.511019605769566

Epoch: 6| Step: 8
Training loss: 2.3151652467878145
Validation loss: 2.5264420884099277

Epoch: 6| Step: 9
Training loss: 1.9605623775522147
Validation loss: 2.5135693444939693

Epoch: 6| Step: 10
Training loss: 2.342017181698315
Validation loss: 2.510610485564551

Epoch: 6| Step: 11
Training loss: 2.043193389037392
Validation loss: 2.520241948598738

Epoch: 6| Step: 12
Training loss: 2.2319766958243816
Validation loss: 2.508328894459744

Epoch: 6| Step: 13
Training loss: 2.2700414297549627
Validation loss: 2.540107223098906

Epoch: 231| Step: 0
Training loss: 2.5742833964402574
Validation loss: 2.5108379486579473

Epoch: 6| Step: 1
Training loss: 2.1380648931748443
Validation loss: 2.529550643038538

Epoch: 6| Step: 2
Training loss: 2.8151055029278647
Validation loss: 2.5143669486823033

Epoch: 6| Step: 3
Training loss: 2.7041851456475605
Validation loss: 2.508295890164244

Epoch: 6| Step: 4
Training loss: 2.582809631013748
Validation loss: 2.5263233619339247

Epoch: 6| Step: 5
Training loss: 1.8766221341099991
Validation loss: 2.545199676351677

Epoch: 6| Step: 6
Training loss: 2.055931382002732
Validation loss: 2.5090921473783445

Epoch: 6| Step: 7
Training loss: 2.506197685780186
Validation loss: 2.539551569153601

Epoch: 6| Step: 8
Training loss: 2.5702471971187664
Validation loss: 2.5383933049522107

Epoch: 6| Step: 9
Training loss: 2.3654385895144605
Validation loss: 2.489774495753847

Epoch: 6| Step: 10
Training loss: 2.368996410587759
Validation loss: 2.507178599238029

Epoch: 6| Step: 11
Training loss: 2.4203244075396038
Validation loss: 2.517680089022691

Epoch: 6| Step: 12
Training loss: 2.0285325873664966
Validation loss: 2.465640660250293

Epoch: 6| Step: 13
Training loss: 2.4906886266145984
Validation loss: 2.484972436629524

Epoch: 232| Step: 0
Training loss: 2.1840482453756973
Validation loss: 2.5563862026760975

Epoch: 6| Step: 1
Training loss: 1.964598380631567
Validation loss: 2.543564213356734

Epoch: 6| Step: 2
Training loss: 2.143829508827869
Validation loss: 2.5212593905042575

Epoch: 6| Step: 3
Training loss: 2.2050108717739043
Validation loss: 2.531603214398452

Epoch: 6| Step: 4
Training loss: 1.9804824136416912
Validation loss: 2.5026657832589043

Epoch: 6| Step: 5
Training loss: 2.353260124551927
Validation loss: 2.5389166869140514

Epoch: 6| Step: 6
Training loss: 2.0761796838105737
Validation loss: 2.4854141201368667

Epoch: 6| Step: 7
Training loss: 2.631343443020031
Validation loss: 2.513936402341653

Epoch: 6| Step: 8
Training loss: 2.396641360040084
Validation loss: 2.515495747918482

Epoch: 6| Step: 9
Training loss: 2.9321343948351
Validation loss: 2.5367139017215727

Epoch: 6| Step: 10
Training loss: 3.0317879425616345
Validation loss: 2.492081205174205

Epoch: 6| Step: 11
Training loss: 2.1646055786438767
Validation loss: 2.527449275899283

Epoch: 6| Step: 12
Training loss: 2.4211231664425834
Validation loss: 2.5117476246408037

Epoch: 6| Step: 13
Training loss: 3.258114002647551
Validation loss: 2.498522101173804

Epoch: 233| Step: 0
Training loss: 1.805613622791313
Validation loss: 2.5137211611793218

Epoch: 6| Step: 1
Training loss: 2.7888995638571705
Validation loss: 2.5135838721980743

Epoch: 6| Step: 2
Training loss: 2.689382913707709
Validation loss: 2.502781086083362

Epoch: 6| Step: 3
Training loss: 1.8482357350349365
Validation loss: 2.549655335095788

Epoch: 6| Step: 4
Training loss: 2.474666122111036
Validation loss: 2.478500711766647

Epoch: 6| Step: 5
Training loss: 2.6089940506855225
Validation loss: 2.5498219204481627

Epoch: 6| Step: 6
Training loss: 2.3740160058115887
Validation loss: 2.5068372833200914

Epoch: 6| Step: 7
Training loss: 2.4441094409769493
Validation loss: 2.534291095360589

Epoch: 6| Step: 8
Training loss: 2.532224679424676
Validation loss: 2.5425236654293917

Epoch: 6| Step: 9
Training loss: 2.459882819013208
Validation loss: 2.505497343703795

Epoch: 6| Step: 10
Training loss: 2.7299706437205824
Validation loss: 2.515803385866783

Epoch: 6| Step: 11
Training loss: 1.8403472535396248
Validation loss: 2.522598554342384

Epoch: 6| Step: 12
Training loss: 2.2667107446910064
Validation loss: 2.5312151332344244

Epoch: 6| Step: 13
Training loss: 2.6077506668869654
Validation loss: 2.5094318336308588

Epoch: 234| Step: 0
Training loss: 2.370300562507277
Validation loss: 2.5194870694595313

Epoch: 6| Step: 1
Training loss: 2.213876772179572
Validation loss: 2.527563335561999

Epoch: 6| Step: 2
Training loss: 1.993197555297969
Validation loss: 2.4975920565912184

Epoch: 6| Step: 3
Training loss: 2.3551145575130756
Validation loss: 2.5066056198154008

Epoch: 6| Step: 4
Training loss: 1.923235961866796
Validation loss: 2.500453880085864

Epoch: 6| Step: 5
Training loss: 2.324220904180987
Validation loss: 2.524449137953778

Epoch: 6| Step: 6
Training loss: 1.934549361399045
Validation loss: 2.5241773226847903

Epoch: 6| Step: 7
Training loss: 2.5259950508981337
Validation loss: 2.4837324792769024

Epoch: 6| Step: 8
Training loss: 2.008300603175138
Validation loss: 2.5337930491583927

Epoch: 6| Step: 9
Training loss: 2.4745511813648826
Validation loss: 2.5099921637109097

Epoch: 6| Step: 10
Training loss: 2.9059887840437857
Validation loss: 2.5178674776479593

Epoch: 6| Step: 11
Training loss: 3.5878240405753417
Validation loss: 2.5083541022046525

Epoch: 6| Step: 12
Training loss: 2.001865470642028
Validation loss: 2.507852079104519

Epoch: 6| Step: 13
Training loss: 2.2010595501162524
Validation loss: 2.5299430692886156

Epoch: 235| Step: 0
Training loss: 3.1876433097188146
Validation loss: 2.5145544720608073

Epoch: 6| Step: 1
Training loss: 2.190784060027016
Validation loss: 2.489305262809197

Epoch: 6| Step: 2
Training loss: 2.00475390020282
Validation loss: 2.5123160884136957

Epoch: 6| Step: 3
Training loss: 2.4438466318835266
Validation loss: 2.5326109356770368

Epoch: 6| Step: 4
Training loss: 2.4963275638730473
Validation loss: 2.5069923586809586

Epoch: 6| Step: 5
Training loss: 1.4503258437677327
Validation loss: 2.5440680646116576

Epoch: 6| Step: 6
Training loss: 2.4725424693657705
Validation loss: 2.5086496171390595

Epoch: 6| Step: 7
Training loss: 2.671828241664142
Validation loss: 2.5136094821206774

Epoch: 6| Step: 8
Training loss: 2.2314980932625454
Validation loss: 2.5577766802778337

Epoch: 6| Step: 9
Training loss: 2.178901339170134
Validation loss: 2.5293860490676923

Epoch: 6| Step: 10
Training loss: 2.481341351189523
Validation loss: 2.5199461033001347

Epoch: 6| Step: 11
Training loss: 2.2068890821190736
Validation loss: 2.55080642817168

Epoch: 6| Step: 12
Training loss: 2.4032131699756136
Validation loss: 2.523893935206544

Epoch: 6| Step: 13
Training loss: 3.015651406286184
Validation loss: 2.530474335425786

Epoch: 236| Step: 0
Training loss: 1.7248975916884874
Validation loss: 2.5098259867949153

Epoch: 6| Step: 1
Training loss: 2.913713285983204
Validation loss: 2.521780951724331

Epoch: 6| Step: 2
Training loss: 2.663612842367193
Validation loss: 2.495405920780838

Epoch: 6| Step: 3
Training loss: 3.34072996283302
Validation loss: 2.503620178396627

Epoch: 6| Step: 4
Training loss: 1.942715485451413
Validation loss: 2.5340097818410916

Epoch: 6| Step: 5
Training loss: 2.246530825340828
Validation loss: 2.5174177107190094

Epoch: 6| Step: 6
Training loss: 3.1481783549619418
Validation loss: 2.511553300277512

Epoch: 6| Step: 7
Training loss: 2.011942256622176
Validation loss: 2.521874831901411

Epoch: 6| Step: 8
Training loss: 1.7378554604921836
Validation loss: 2.4901310297710304

Epoch: 6| Step: 9
Training loss: 2.3512923887123858
Validation loss: 2.5491660864376398

Epoch: 6| Step: 10
Training loss: 1.9622561212124159
Validation loss: 2.5055760474952935

Epoch: 6| Step: 11
Training loss: 2.3604700439918243
Validation loss: 2.5093399582575397

Epoch: 6| Step: 12
Training loss: 2.3348616862531935
Validation loss: 2.5095391383698478

Epoch: 6| Step: 13
Training loss: 1.713820028063778
Validation loss: 2.5210577043632862

Epoch: 237| Step: 0
Training loss: 2.5902310135079887
Validation loss: 2.513637576242222

Epoch: 6| Step: 1
Training loss: 2.4077759895304376
Validation loss: 2.468298847330675

Epoch: 6| Step: 2
Training loss: 3.2977285139077823
Validation loss: 2.5406248454525118

Epoch: 6| Step: 3
Training loss: 2.857125653487591
Validation loss: 2.495538208378571

Epoch: 6| Step: 4
Training loss: 2.4733478850410453
Validation loss: 2.48076661220385

Epoch: 6| Step: 5
Training loss: 1.8675968248220625
Validation loss: 2.5345087934080466

Epoch: 6| Step: 6
Training loss: 2.6516238149860496
Validation loss: 2.5390660823421274

Epoch: 6| Step: 7
Training loss: 2.192312777277806
Validation loss: 2.5045873034966513

Epoch: 6| Step: 8
Training loss: 2.35251321932801
Validation loss: 2.511584891443361

Epoch: 6| Step: 9
Training loss: 2.0880491162658212
Validation loss: 2.537719673368405

Epoch: 6| Step: 10
Training loss: 2.5849899907167364
Validation loss: 2.486237115791491

Epoch: 6| Step: 11
Training loss: 1.7303320699408034
Validation loss: 2.52925417460946

Epoch: 6| Step: 12
Training loss: 1.7695893873891557
Validation loss: 2.5196098522056998

Epoch: 6| Step: 13
Training loss: 2.4027595470327308
Validation loss: 2.5416120976871146

Epoch: 238| Step: 0
Training loss: 2.2326969701457577
Validation loss: 2.540754167212447

Epoch: 6| Step: 1
Training loss: 2.5366731170224734
Validation loss: 2.532823746657958

Epoch: 6| Step: 2
Training loss: 2.0299098362184145
Validation loss: 2.5448188275141437

Epoch: 6| Step: 3
Training loss: 2.1330940703489296
Validation loss: 2.53137536487279

Epoch: 6| Step: 4
Training loss: 2.112653882443112
Validation loss: 2.5419713030580176

Epoch: 6| Step: 5
Training loss: 2.5611256891618606
Validation loss: 2.534991077461518

Epoch: 6| Step: 6
Training loss: 2.4366655143947287
Validation loss: 2.5433004512098494

Epoch: 6| Step: 7
Training loss: 2.8461813122381696
Validation loss: 2.530089867664995

Epoch: 6| Step: 8
Training loss: 2.90009977892523
Validation loss: 2.511051783034943

Epoch: 6| Step: 9
Training loss: 2.08509266635973
Validation loss: 2.5161231206437167

Epoch: 6| Step: 10
Training loss: 2.6624154735633634
Validation loss: 2.527911484308942

Epoch: 6| Step: 11
Training loss: 2.1770999548854
Validation loss: 2.544450940152225

Epoch: 6| Step: 12
Training loss: 2.365986132048258
Validation loss: 2.530506324069297

Epoch: 6| Step: 13
Training loss: 2.250295831517648
Validation loss: 2.514365112387695

Epoch: 239| Step: 0
Training loss: 2.7542272502412866
Validation loss: 2.5378928257642213

Epoch: 6| Step: 1
Training loss: 2.4979214610694602
Validation loss: 2.530502188616433

Epoch: 6| Step: 2
Training loss: 2.7159421606071037
Validation loss: 2.511645623921134

Epoch: 6| Step: 3
Training loss: 2.022817155752212
Validation loss: 2.5471671539745326

Epoch: 6| Step: 4
Training loss: 2.2383328044073636
Validation loss: 2.5073915576228707

Epoch: 6| Step: 5
Training loss: 2.7547732456167107
Validation loss: 2.537397693612661

Epoch: 6| Step: 6
Training loss: 2.4955893709890957
Validation loss: 2.519253735125132

Epoch: 6| Step: 7
Training loss: 2.2258083203946795
Validation loss: 2.5095885210356697

Epoch: 6| Step: 8
Training loss: 2.154886007812255
Validation loss: 2.502001221878224

Epoch: 6| Step: 9
Training loss: 2.204900148167965
Validation loss: 2.5144086233388214

Epoch: 6| Step: 10
Training loss: 2.066717286669267
Validation loss: 2.5458735300015793

Epoch: 6| Step: 11
Training loss: 2.2946827086147077
Validation loss: 2.522920285848914

Epoch: 6| Step: 12
Training loss: 2.6391485476878134
Validation loss: 2.5043742110671157

Epoch: 6| Step: 13
Training loss: 2.3611298155199565
Validation loss: 2.5286510671045623

Epoch: 240| Step: 0
Training loss: 2.453706222975402
Validation loss: 2.5097346478263027

Epoch: 6| Step: 1
Training loss: 2.885900482122071
Validation loss: 2.530540475235745

Epoch: 6| Step: 2
Training loss: 1.9407793410595635
Validation loss: 2.495079626765472

Epoch: 6| Step: 3
Training loss: 2.2487895676681977
Validation loss: 2.519868496398242

Epoch: 6| Step: 4
Training loss: 2.483155052988314
Validation loss: 2.5109333411594896

Epoch: 6| Step: 5
Training loss: 2.424997888151213
Validation loss: 2.5094748999161545

Epoch: 6| Step: 6
Training loss: 2.3832394076781926
Validation loss: 2.508673963256475

Epoch: 6| Step: 7
Training loss: 1.4682726794621233
Validation loss: 2.551329788820766

Epoch: 6| Step: 8
Training loss: 2.212245810179828
Validation loss: 2.531878189978902

Epoch: 6| Step: 9
Training loss: 2.6143682525642538
Validation loss: 2.5285695441512623

Epoch: 6| Step: 10
Training loss: 3.024651020191472
Validation loss: 2.5102616346796207

Epoch: 6| Step: 11
Training loss: 1.9926543044166996
Validation loss: 2.5505980846877336

Epoch: 6| Step: 12
Training loss: 2.7226140417168585
Validation loss: 2.5122938327547937

Epoch: 6| Step: 13
Training loss: 1.920858160701124
Validation loss: 2.5118174562149216

Epoch: 241| Step: 0
Training loss: 2.871907768851236
Validation loss: 2.533721961523909

Epoch: 6| Step: 1
Training loss: 2.0272614273086047
Validation loss: 2.4962299982527725

Epoch: 6| Step: 2
Training loss: 2.8254515936786877
Validation loss: 2.5622849553013443

Epoch: 6| Step: 3
Training loss: 2.0871973697790533
Validation loss: 2.504793605348562

Epoch: 6| Step: 4
Training loss: 2.089783398478745
Validation loss: 2.5040877997921167

Epoch: 6| Step: 5
Training loss: 2.8362826723156194
Validation loss: 2.537884717819622

Epoch: 6| Step: 6
Training loss: 1.562692096345975
Validation loss: 2.489195152689414

Epoch: 6| Step: 7
Training loss: 2.203657694596607
Validation loss: 2.5257763602240013

Epoch: 6| Step: 8
Training loss: 2.0185439391419813
Validation loss: 2.519504877098534

Epoch: 6| Step: 9
Training loss: 2.231012976102141
Validation loss: 2.489989065674328

Epoch: 6| Step: 10
Training loss: 2.907384507434864
Validation loss: 2.507920231800613

Epoch: 6| Step: 11
Training loss: 2.3061677995624623
Validation loss: 2.5481189926127232

Epoch: 6| Step: 12
Training loss: 2.4316380757391256
Validation loss: 2.5147491624146787

Epoch: 6| Step: 13
Training loss: 2.666814998633853
Validation loss: 2.5368025199756596

Epoch: 242| Step: 0
Training loss: 2.769561896537949
Validation loss: 2.519163781081726

Epoch: 6| Step: 1
Training loss: 2.5037630370510553
Validation loss: 2.502139376401967

Epoch: 6| Step: 2
Training loss: 2.3138325949857323
Validation loss: 2.5284497941516824

Epoch: 6| Step: 3
Training loss: 2.0301232367254762
Validation loss: 2.530409999289843

Epoch: 6| Step: 4
Training loss: 2.388118452089623
Validation loss: 2.497636376036006

Epoch: 6| Step: 5
Training loss: 2.2213102800715534
Validation loss: 2.5347108970393206

Epoch: 6| Step: 6
Training loss: 2.877189300228288
Validation loss: 2.493719049471848

Epoch: 6| Step: 7
Training loss: 1.9885993508201383
Validation loss: 2.5212506693136323

Epoch: 6| Step: 8
Training loss: 2.110849387972148
Validation loss: 2.5682000103795715

Epoch: 6| Step: 9
Training loss: 2.4954733876127975
Validation loss: 2.5341365767152793

Epoch: 6| Step: 10
Training loss: 2.6057438969632605
Validation loss: 2.5191603729514727

Epoch: 6| Step: 11
Training loss: 2.330327163779065
Validation loss: 2.5160948234950875

Epoch: 6| Step: 12
Training loss: 2.5305833291815145
Validation loss: 2.524155314305095

Epoch: 6| Step: 13
Training loss: 2.4930150681953416
Validation loss: 2.5339224535027163

Epoch: 243| Step: 0
Training loss: 2.7390107884941424
Validation loss: 2.526753293116125

Epoch: 6| Step: 1
Training loss: 2.362546898108937
Validation loss: 2.509459513784985

Epoch: 6| Step: 2
Training loss: 2.662938571681264
Validation loss: 2.5040499287766265

Epoch: 6| Step: 3
Training loss: 2.2361974153258606
Validation loss: 2.525044725782029

Epoch: 6| Step: 4
Training loss: 2.363383141204859
Validation loss: 2.5236934642656457

Epoch: 6| Step: 5
Training loss: 2.2563395728086375
Validation loss: 2.5466163021211736

Epoch: 6| Step: 6
Training loss: 1.4737999317617991
Validation loss: 2.4951258356412342

Epoch: 6| Step: 7
Training loss: 1.9216644668015093
Validation loss: 2.5307371439583486

Epoch: 6| Step: 8
Training loss: 2.329954311921607
Validation loss: 2.4666862995305374

Epoch: 6| Step: 9
Training loss: 2.3764196469563075
Validation loss: 2.5289956276464203

Epoch: 6| Step: 10
Training loss: 2.97669674227715
Validation loss: 2.510040330223328

Epoch: 6| Step: 11
Training loss: 2.3600517464476587
Validation loss: 2.5121275815416353

Epoch: 6| Step: 12
Training loss: 2.36383362927256
Validation loss: 2.536124888462963

Epoch: 6| Step: 13
Training loss: 2.7663507748470857
Validation loss: 2.506488201851027

Epoch: 244| Step: 0
Training loss: 2.4000981946566564
Validation loss: 2.5038207421900225

Epoch: 6| Step: 1
Training loss: 2.163860655918588
Validation loss: 2.5180561219029713

Epoch: 6| Step: 2
Training loss: 1.4291812021092791
Validation loss: 2.5230608964227415

Epoch: 6| Step: 3
Training loss: 1.7812504684715325
Validation loss: 2.528660875960899

Epoch: 6| Step: 4
Training loss: 2.625074294719182
Validation loss: 2.5269645227241564

Epoch: 6| Step: 5
Training loss: 2.406197386327438
Validation loss: 2.537992625657419

Epoch: 6| Step: 6
Training loss: 2.5201995199933975
Validation loss: 2.517083035618361

Epoch: 6| Step: 7
Training loss: 2.5541778896099743
Validation loss: 2.5181074001775237

Epoch: 6| Step: 8
Training loss: 2.522207710412953
Validation loss: 2.542385994725179

Epoch: 6| Step: 9
Training loss: 2.0050481030408753
Validation loss: 2.502904950572209

Epoch: 6| Step: 10
Training loss: 2.624321804538623
Validation loss: 2.4834978151023446

Epoch: 6| Step: 11
Training loss: 2.6693947864067993
Validation loss: 2.524299940769616

Epoch: 6| Step: 12
Training loss: 3.029930220703768
Validation loss: 2.5173903492403693

Epoch: 6| Step: 13
Training loss: 1.9582804273668906
Validation loss: 2.518237860993644

Epoch: 245| Step: 0
Training loss: 2.27427851795953
Validation loss: 2.5729771205466947

Epoch: 6| Step: 1
Training loss: 1.959551621648969
Validation loss: 2.5400072040040675

Epoch: 6| Step: 2
Training loss: 2.2166698537053118
Validation loss: 2.5333811636291634

Epoch: 6| Step: 3
Training loss: 2.5979703096685753
Validation loss: 2.4885532594101702

Epoch: 6| Step: 4
Training loss: 2.8230187279150627
Validation loss: 2.529125886655822

Epoch: 6| Step: 5
Training loss: 1.9102858048952458
Validation loss: 2.5321314674707045

Epoch: 6| Step: 6
Training loss: 2.74876636231372
Validation loss: 2.497974534051089

Epoch: 6| Step: 7
Training loss: 2.498359714272918
Validation loss: 2.559162925317609

Epoch: 6| Step: 8
Training loss: 2.421013561987599
Validation loss: 2.5148775038151467

Epoch: 6| Step: 9
Training loss: 1.9877127263649295
Validation loss: 2.55023817638649

Epoch: 6| Step: 10
Training loss: 1.8485226686515248
Validation loss: 2.4952275756847646

Epoch: 6| Step: 11
Training loss: 2.438669168842604
Validation loss: 2.53722522128222

Epoch: 6| Step: 12
Training loss: 2.9547850771748054
Validation loss: 2.5324727291323366

Epoch: 6| Step: 13
Training loss: 2.159298386712144
Validation loss: 2.5019747697342662

Epoch: 246| Step: 0
Training loss: 2.560887084986048
Validation loss: 2.507590159471819

Epoch: 6| Step: 1
Training loss: 1.2003445388979253
Validation loss: 2.5505427143927775

Epoch: 6| Step: 2
Training loss: 2.8163320079229504
Validation loss: 2.539985844003667

Epoch: 6| Step: 3
Training loss: 2.0789689953039363
Validation loss: 2.5079985140603918

Epoch: 6| Step: 4
Training loss: 2.611176468547509
Validation loss: 2.576065622168758

Epoch: 6| Step: 5
Training loss: 1.9574357945227616
Validation loss: 2.520498121431076

Epoch: 6| Step: 6
Training loss: 2.220945749306431
Validation loss: 2.5142834851375997

Epoch: 6| Step: 7
Training loss: 2.669653431834544
Validation loss: 2.5317044677463567

Epoch: 6| Step: 8
Training loss: 2.883615273452422
Validation loss: 2.5178322687603605

Epoch: 6| Step: 9
Training loss: 2.8114639917245814
Validation loss: 2.4802089739552824

Epoch: 6| Step: 10
Training loss: 1.5886360849138885
Validation loss: 2.515993533957293

Epoch: 6| Step: 11
Training loss: 2.7043039033350187
Validation loss: 2.529473216287672

Epoch: 6| Step: 12
Training loss: 2.6857041857335298
Validation loss: 2.5180401254169285

Epoch: 6| Step: 13
Training loss: 1.5644489340660548
Validation loss: 2.5564370540679087

Epoch: 247| Step: 0
Training loss: 2.289777624502351
Validation loss: 2.498220080823582

Epoch: 6| Step: 1
Training loss: 2.8707155562456212
Validation loss: 2.5318993500561673

Epoch: 6| Step: 2
Training loss: 1.9121370557080921
Validation loss: 2.5213495971837023

Epoch: 6| Step: 3
Training loss: 3.0037643339566764
Validation loss: 2.5183335264703763

Epoch: 6| Step: 4
Training loss: 2.240948058691167
Validation loss: 2.5125545932285447

Epoch: 6| Step: 5
Training loss: 1.723045668410227
Validation loss: 2.5291970669809998

Epoch: 6| Step: 6
Training loss: 2.4964144743944883
Validation loss: 2.5133668795918642

Epoch: 6| Step: 7
Training loss: 2.1866226616750275
Validation loss: 2.5130025730471712

Epoch: 6| Step: 8
Training loss: 2.1290863240929085
Validation loss: 2.5217574397071654

Epoch: 6| Step: 9
Training loss: 2.489596655476145
Validation loss: 2.5233326028518355

Epoch: 6| Step: 10
Training loss: 2.4628060165259913
Validation loss: 2.5480143579636234

Epoch: 6| Step: 11
Training loss: 2.2426694558035933
Validation loss: 2.529845572901353

Epoch: 6| Step: 12
Training loss: 2.5927513681970353
Validation loss: 2.5316325771723487

Epoch: 6| Step: 13
Training loss: 2.0355656481140523
Validation loss: 2.542403205357378

Epoch: 248| Step: 0
Training loss: 2.785572079812083
Validation loss: 2.5333685375686334

Epoch: 6| Step: 1
Training loss: 1.7479482612285424
Validation loss: 2.523947004443029

Epoch: 6| Step: 2
Training loss: 2.3640815325649718
Validation loss: 2.5085936377497426

Epoch: 6| Step: 3
Training loss: 2.8232952206623976
Validation loss: 2.5526330409009272

Epoch: 6| Step: 4
Training loss: 2.2704553085327253
Validation loss: 2.535176531263106

Epoch: 6| Step: 5
Training loss: 2.056011745018568
Validation loss: 2.5228589000747514

Epoch: 6| Step: 6
Training loss: 2.371181027661613
Validation loss: 2.515919037301062

Epoch: 6| Step: 7
Training loss: 2.3002096868381527
Validation loss: 2.540567775455667

Epoch: 6| Step: 8
Training loss: 1.6768057299228494
Validation loss: 2.5230353219785844

Epoch: 6| Step: 9
Training loss: 2.0110000659898986
Validation loss: 2.495356743387242

Epoch: 6| Step: 10
Training loss: 3.10975437629466
Validation loss: 2.5505279117215713

Epoch: 6| Step: 11
Training loss: 2.502016779903553
Validation loss: 2.518869250319481

Epoch: 6| Step: 12
Training loss: 2.694012048990312
Validation loss: 2.5231522630320944

Epoch: 6| Step: 13
Training loss: 1.6720247290758332
Validation loss: 2.5435529864115245

Epoch: 249| Step: 0
Training loss: 2.316077686700098
Validation loss: 2.5220301139920758

Epoch: 6| Step: 1
Training loss: 2.253444578069464
Validation loss: 2.552094106209883

Epoch: 6| Step: 2
Training loss: 2.4366492718633648
Validation loss: 2.534368242408768

Epoch: 6| Step: 3
Training loss: 1.9681376155716666
Validation loss: 2.522269167937594

Epoch: 6| Step: 4
Training loss: 2.8544966977919377
Validation loss: 2.5085483754695654

Epoch: 6| Step: 5
Training loss: 2.1403204708254493
Validation loss: 2.5381332668481082

Epoch: 6| Step: 6
Training loss: 2.232370611288049
Validation loss: 2.523601317764025

Epoch: 6| Step: 7
Training loss: 2.165691327377028
Validation loss: 2.493221303461708

Epoch: 6| Step: 8
Training loss: 2.5185910385366204
Validation loss: 2.517457358265173

Epoch: 6| Step: 9
Training loss: 2.2079565218570063
Validation loss: 2.5101048769462597

Epoch: 6| Step: 10
Training loss: 2.436682735273587
Validation loss: 2.5196298974056384

Epoch: 6| Step: 11
Training loss: 2.1084970413198105
Validation loss: 2.495839302173222

Epoch: 6| Step: 12
Training loss: 3.142611809852874
Validation loss: 2.5112987410763274

Epoch: 6| Step: 13
Training loss: 1.6711779415827812
Validation loss: 2.5054815883238417

Epoch: 250| Step: 0
Training loss: 2.1449536739584225
Validation loss: 2.533611368474977

Epoch: 6| Step: 1
Training loss: 2.1436575688935395
Validation loss: 2.5127304913415798

Epoch: 6| Step: 2
Training loss: 2.2063814810243327
Validation loss: 2.5250832982041778

Epoch: 6| Step: 3
Training loss: 1.6872159930820985
Validation loss: 2.5142324174348913

Epoch: 6| Step: 4
Training loss: 1.7975863168335715
Validation loss: 2.4907717167140144

Epoch: 6| Step: 5
Training loss: 1.9588231089762858
Validation loss: 2.521917622674379

Epoch: 6| Step: 6
Training loss: 2.6003902875966807
Validation loss: 2.5102052919161735

Epoch: 6| Step: 7
Training loss: 2.6205582050278373
Validation loss: 2.5235723257993086

Epoch: 6| Step: 8
Training loss: 2.6215567030724927
Validation loss: 2.537796839432876

Epoch: 6| Step: 9
Training loss: 2.2956873100168314
Validation loss: 2.497773032799669

Epoch: 6| Step: 10
Training loss: 2.321765791325755
Validation loss: 2.4982006098781078

Epoch: 6| Step: 11
Training loss: 2.335227016220298
Validation loss: 2.5054198960228944

Epoch: 6| Step: 12
Training loss: 2.937018821752113
Validation loss: 2.5311577923325537

Epoch: 6| Step: 13
Training loss: 3.0414797233370967
Validation loss: 2.526354572049596

Epoch: 251| Step: 0
Training loss: 3.135034092135219
Validation loss: 2.5119452299358445

Epoch: 6| Step: 1
Training loss: 3.175843710947878
Validation loss: 2.5122565028066535

Epoch: 6| Step: 2
Training loss: 2.7984976723153347
Validation loss: 2.5027823961841666

Epoch: 6| Step: 3
Training loss: 1.9134526740990605
Validation loss: 2.536863116630795

Epoch: 6| Step: 4
Training loss: 3.0336699903301105
Validation loss: 2.4901150803902783

Epoch: 6| Step: 5
Training loss: 2.1240627241337973
Validation loss: 2.5234542364064994

Epoch: 6| Step: 6
Training loss: 2.065527803151248
Validation loss: 2.525066916773966

Epoch: 6| Step: 7
Training loss: 2.1571799221842403
Validation loss: 2.518945884512585

Epoch: 6| Step: 8
Training loss: 2.1403957717565736
Validation loss: 2.5236539725941913

Epoch: 6| Step: 9
Training loss: 1.9098717101187552
Validation loss: 2.497806792977412

Epoch: 6| Step: 10
Training loss: 2.287160662720205
Validation loss: 2.5343865584815743

Epoch: 6| Step: 11
Training loss: 2.2162254907895518
Validation loss: 2.526163122422199

Epoch: 6| Step: 12
Training loss: 1.5428155364249494
Validation loss: 2.484583273173049

Epoch: 6| Step: 13
Training loss: 1.9566837085954358
Validation loss: 2.5077866923598036

Epoch: 252| Step: 0
Training loss: 2.7928128712510767
Validation loss: 2.556898034933791

Epoch: 6| Step: 1
Training loss: 2.177634636344104
Validation loss: 2.5116313442655556

Epoch: 6| Step: 2
Training loss: 2.0837436526269926
Validation loss: 2.5325533024502667

Epoch: 6| Step: 3
Training loss: 2.9638647742473703
Validation loss: 2.489384408497371

Epoch: 6| Step: 4
Training loss: 2.059347922996607
Validation loss: 2.5580444618037674

Epoch: 6| Step: 5
Training loss: 1.845861099183589
Validation loss: 2.505745228744675

Epoch: 6| Step: 6
Training loss: 2.942663645897326
Validation loss: 2.509024906768067

Epoch: 6| Step: 7
Training loss: 2.1236538830808827
Validation loss: 2.514958822288018

Epoch: 6| Step: 8
Training loss: 2.6234167864983164
Validation loss: 2.5200056261122548

Epoch: 6| Step: 9
Training loss: 1.9422808083840357
Validation loss: 2.5144908939387602

Epoch: 6| Step: 10
Training loss: 2.7207241564154816
Validation loss: 2.540065588568973

Epoch: 6| Step: 11
Training loss: 1.995311964687673
Validation loss: 2.5004113679244524

Epoch: 6| Step: 12
Training loss: 2.404018903540553
Validation loss: 2.5361796203198415

Epoch: 6| Step: 13
Training loss: 1.8799299278795427
Validation loss: 2.534053779999314

Epoch: 253| Step: 0
Training loss: 1.8300225606684586
Validation loss: 2.5207828013935956

Epoch: 6| Step: 1
Training loss: 2.1190335154382955
Validation loss: 2.5251121179938703

Epoch: 6| Step: 2
Training loss: 2.5322521721853843
Validation loss: 2.556424329280912

Epoch: 6| Step: 3
Training loss: 2.647670578544974
Validation loss: 2.498551378678803

Epoch: 6| Step: 4
Training loss: 2.368861446929418
Validation loss: 2.5561441131956677

Epoch: 6| Step: 5
Training loss: 2.140787382997588
Validation loss: 2.490187814378987

Epoch: 6| Step: 6
Training loss: 2.658367524679526
Validation loss: 2.5119531496228396

Epoch: 6| Step: 7
Training loss: 1.7494138008331277
Validation loss: 2.514719912417902

Epoch: 6| Step: 8
Training loss: 1.7375718630193382
Validation loss: 2.515725182406097

Epoch: 6| Step: 9
Training loss: 2.884375667881914
Validation loss: 2.498218954070149

Epoch: 6| Step: 10
Training loss: 2.5055703095281063
Validation loss: 2.492902855396498

Epoch: 6| Step: 11
Training loss: 2.697629248039005
Validation loss: 2.551720577640961

Epoch: 6| Step: 12
Training loss: 2.7503302982683286
Validation loss: 2.5445822506920215

Epoch: 6| Step: 13
Training loss: 2.4047182830961984
Validation loss: 2.514751830292686

Epoch: 254| Step: 0
Training loss: 2.02672601303412
Validation loss: 2.5184196195439026

Epoch: 6| Step: 1
Training loss: 2.2595797159779263
Validation loss: 2.5288470839249078

Epoch: 6| Step: 2
Training loss: 2.608597142667755
Validation loss: 2.531006804784756

Epoch: 6| Step: 3
Training loss: 2.4473110234341617
Validation loss: 2.536309034300454

Epoch: 6| Step: 4
Training loss: 1.6774590191902221
Validation loss: 2.574177109479272

Epoch: 6| Step: 5
Training loss: 2.249449980583904
Validation loss: 2.5360153140508883

Epoch: 6| Step: 6
Training loss: 2.2493697449330483
Validation loss: 2.5311748564887138

Epoch: 6| Step: 7
Training loss: 2.7351278086199344
Validation loss: 2.5111585713976554

Epoch: 6| Step: 8
Training loss: 2.0937021733984884
Validation loss: 2.498122238147021

Epoch: 6| Step: 9
Training loss: 3.0954956370233453
Validation loss: 2.524400795465179

Epoch: 6| Step: 10
Training loss: 2.931443809495167
Validation loss: 2.542213710562034

Epoch: 6| Step: 11
Training loss: 1.9097205915347757
Validation loss: 2.535767218615287

Epoch: 6| Step: 12
Training loss: 2.0419078866057627
Validation loss: 2.5486782667510255

Epoch: 6| Step: 13
Training loss: 2.4705705328181575
Validation loss: 2.509211805643701

Epoch: 255| Step: 0
Training loss: 2.5080402781828095
Validation loss: 2.5449656704038373

Epoch: 6| Step: 1
Training loss: 1.7625102022565633
Validation loss: 2.5314797606434443

Epoch: 6| Step: 2
Training loss: 2.6827679158829083
Validation loss: 2.550184335345486

Epoch: 6| Step: 3
Training loss: 2.0289126519831853
Validation loss: 2.56612260330188

Epoch: 6| Step: 4
Training loss: 1.9992914732943212
Validation loss: 2.5400751585559935

Epoch: 6| Step: 5
Training loss: 2.415283190785941
Validation loss: 2.511486494567287

Epoch: 6| Step: 6
Training loss: 1.9618918235813367
Validation loss: 2.5495798049473137

Epoch: 6| Step: 7
Training loss: 2.4272069503890865
Validation loss: 2.529631054042183

Epoch: 6| Step: 8
Training loss: 2.494264890820751
Validation loss: 2.5596660146611616

Epoch: 6| Step: 9
Training loss: 2.839645273862929
Validation loss: 2.560519414714297

Epoch: 6| Step: 10
Training loss: 1.8393084806390372
Validation loss: 2.5100061166617933

Epoch: 6| Step: 11
Training loss: 2.5862288296525837
Validation loss: 2.530894934289854

Epoch: 6| Step: 12
Training loss: 2.387248728047203
Validation loss: 2.5469938243307624

Epoch: 6| Step: 13
Training loss: 3.0955705006586975
Validation loss: 2.531481522748448

Epoch: 256| Step: 0
Training loss: 2.6985000117162166
Validation loss: 2.510229356392071

Epoch: 6| Step: 1
Training loss: 1.8267390548442894
Validation loss: 2.5743531100814594

Epoch: 6| Step: 2
Training loss: 2.6123504121290337
Validation loss: 2.5667532292032162

Epoch: 6| Step: 3
Training loss: 2.3463976337726575
Validation loss: 2.5236252067544998

Epoch: 6| Step: 4
Training loss: 2.3385395370054565
Validation loss: 2.545271560425416

Epoch: 6| Step: 5
Training loss: 2.2588344491900365
Validation loss: 2.5005768151526753

Epoch: 6| Step: 6
Training loss: 2.3639098789445177
Validation loss: 2.545058756806276

Epoch: 6| Step: 7
Training loss: 1.9097549859521294
Validation loss: 2.505522098011223

Epoch: 6| Step: 8
Training loss: 2.3314451570615473
Validation loss: 2.522644408017355

Epoch: 6| Step: 9
Training loss: 2.3968830849562144
Validation loss: 2.5247058627641237

Epoch: 6| Step: 10
Training loss: 1.8434375562860221
Validation loss: 2.5109978848358487

Epoch: 6| Step: 11
Training loss: 2.6303668789558072
Validation loss: 2.5325196764729174

Epoch: 6| Step: 12
Training loss: 2.1773404299652883
Validation loss: 2.5145408298157177

Epoch: 6| Step: 13
Training loss: 3.110776499131892
Validation loss: 2.521264350496381

Epoch: 257| Step: 0
Training loss: 2.161553425975955
Validation loss: 2.4979205148118813

Epoch: 6| Step: 1
Training loss: 2.7227660585258606
Validation loss: 2.5290245767001256

Epoch: 6| Step: 2
Training loss: 2.211407830453336
Validation loss: 2.510756426264724

Epoch: 6| Step: 3
Training loss: 2.212385478531089
Validation loss: 2.5267363949180797

Epoch: 6| Step: 4
Training loss: 2.4110411467804744
Validation loss: 2.474310038015436

Epoch: 6| Step: 5
Training loss: 3.274612397997158
Validation loss: 2.508750370352057

Epoch: 6| Step: 6
Training loss: 2.3225716532811735
Validation loss: 2.5122759270616175

Epoch: 6| Step: 7
Training loss: 1.7545221347624715
Validation loss: 2.518677292280263

Epoch: 6| Step: 8
Training loss: 1.7541804110002934
Validation loss: 2.5401909422473

Epoch: 6| Step: 9
Training loss: 2.0835733911172594
Validation loss: 2.496283001416993

Epoch: 6| Step: 10
Training loss: 1.964030649295647
Validation loss: 2.5209567972768285

Epoch: 6| Step: 11
Training loss: 2.334247500858397
Validation loss: 2.534250075420426

Epoch: 6| Step: 12
Training loss: 1.867953155465247
Validation loss: 2.530295749809934

Epoch: 6| Step: 13
Training loss: 3.5594597944520054
Validation loss: 2.5164356376510857

Epoch: 258| Step: 0
Training loss: 2.0371656693519027
Validation loss: 2.508688039014253

Epoch: 6| Step: 1
Training loss: 2.993320180981577
Validation loss: 2.5292691250988155

Epoch: 6| Step: 2
Training loss: 2.7431577177914788
Validation loss: 2.5492951146959633

Epoch: 6| Step: 3
Training loss: 2.3285147321520476
Validation loss: 2.529930885105658

Epoch: 6| Step: 4
Training loss: 2.5889124051537236
Validation loss: 2.5202624007420336

Epoch: 6| Step: 5
Training loss: 2.3471390698794337
Validation loss: 2.536865990648628

Epoch: 6| Step: 6
Training loss: 2.130539909492127
Validation loss: 2.4807153495389023

Epoch: 6| Step: 7
Training loss: 2.6894155373644266
Validation loss: 2.5324245904256864

Epoch: 6| Step: 8
Training loss: 2.5025913160128463
Validation loss: 2.5456000495805102

Epoch: 6| Step: 9
Training loss: 2.1880728925108217
Validation loss: 2.537783126154053

Epoch: 6| Step: 10
Training loss: 2.248442004466366
Validation loss: 2.5234921504484498

Epoch: 6| Step: 11
Training loss: 1.9878382460952804
Validation loss: 2.494105564267727

Epoch: 6| Step: 12
Training loss: 1.989857646412846
Validation loss: 2.528083773696056

Epoch: 6| Step: 13
Training loss: 1.679383148298524
Validation loss: 2.526289149700081

Epoch: 259| Step: 0
Training loss: 2.44555253383356
Validation loss: 2.5693181709081285

Epoch: 6| Step: 1
Training loss: 2.251096458303687
Validation loss: 2.5527192365532017

Epoch: 6| Step: 2
Training loss: 1.8087883138124732
Validation loss: 2.5281248703640733

Epoch: 6| Step: 3
Training loss: 2.093998566861104
Validation loss: 2.54945360818838

Epoch: 6| Step: 4
Training loss: 2.559825711277848
Validation loss: 2.545363156093153

Epoch: 6| Step: 5
Training loss: 2.5201996145964527
Validation loss: 2.485640096586121

Epoch: 6| Step: 6
Training loss: 2.320648682769085
Validation loss: 2.512987423767741

Epoch: 6| Step: 7
Training loss: 2.2923003852158623
Validation loss: 2.5183553806043832

Epoch: 6| Step: 8
Training loss: 2.6035491414642133
Validation loss: 2.514572325846012

Epoch: 6| Step: 9
Training loss: 2.8427143936027397
Validation loss: 2.5416999450258704

Epoch: 6| Step: 10
Training loss: 1.795009540330616
Validation loss: 2.539271930295685

Epoch: 6| Step: 11
Training loss: 2.215611131460584
Validation loss: 2.556635876028554

Epoch: 6| Step: 12
Training loss: 2.1367649442989882
Validation loss: 2.517090601000096

Epoch: 6| Step: 13
Training loss: 2.7431350331656414
Validation loss: 2.522838495401254

Epoch: 260| Step: 0
Training loss: 2.843767438562603
Validation loss: 2.501995314844454

Epoch: 6| Step: 1
Training loss: 2.281629974909873
Validation loss: 2.5283687290980303

Epoch: 6| Step: 2
Training loss: 2.1978438909212508
Validation loss: 2.5254286114842075

Epoch: 6| Step: 3
Training loss: 3.111613172559297
Validation loss: 2.532506054069452

Epoch: 6| Step: 4
Training loss: 2.409617062294724
Validation loss: 2.5270263078944173

Epoch: 6| Step: 5
Training loss: 2.3435801126579396
Validation loss: 2.5590220153286296

Epoch: 6| Step: 6
Training loss: 2.1901582098124304
Validation loss: 2.5027349053600925

Epoch: 6| Step: 7
Training loss: 2.3624067217461553
Validation loss: 2.5123912755715785

Epoch: 6| Step: 8
Training loss: 2.0363549273710295
Validation loss: 2.499239818300056

Epoch: 6| Step: 9
Training loss: 2.905561304015808
Validation loss: 2.5278736304650034

Epoch: 6| Step: 10
Training loss: 1.7385189279661952
Validation loss: 2.507541191384274

Epoch: 6| Step: 11
Training loss: 2.213819694262022
Validation loss: 2.5622982422932132

Epoch: 6| Step: 12
Training loss: 1.8417817294164518
Validation loss: 2.5427044401153176

Epoch: 6| Step: 13
Training loss: 1.8423962553260371
Validation loss: 2.5507192180935

Epoch: 261| Step: 0
Training loss: 3.047994398237072
Validation loss: 2.5193022949301938

Epoch: 6| Step: 1
Training loss: 2.1877406396657975
Validation loss: 2.5308458275733248

Epoch: 6| Step: 2
Training loss: 2.3380291100248796
Validation loss: 2.559600817752061

Epoch: 6| Step: 3
Training loss: 1.85400718546013
Validation loss: 2.5034254520293318

Epoch: 6| Step: 4
Training loss: 2.1894831795718206
Validation loss: 2.5179532659409363

Epoch: 6| Step: 5
Training loss: 2.0352647280200573
Validation loss: 2.5204225556501667

Epoch: 6| Step: 6
Training loss: 2.089386107346488
Validation loss: 2.525092972690891

Epoch: 6| Step: 7
Training loss: 2.3468482329649176
Validation loss: 2.562603045776984

Epoch: 6| Step: 8
Training loss: 1.6075467586094416
Validation loss: 2.507597169731928

Epoch: 6| Step: 9
Training loss: 3.0491820379847843
Validation loss: 2.5652312842795464

Epoch: 6| Step: 10
Training loss: 2.8844771709097494
Validation loss: 2.5173118824306653

Epoch: 6| Step: 11
Training loss: 2.1785076051605117
Validation loss: 2.526884058316558

Epoch: 6| Step: 12
Training loss: 2.0514517133191505
Validation loss: 2.506768940108138

Epoch: 6| Step: 13
Training loss: 2.5490290420092974
Validation loss: 2.5362896445077956

Epoch: 262| Step: 0
Training loss: 1.4626184268833384
Validation loss: 2.4939502987106406

Epoch: 6| Step: 1
Training loss: 2.9856970290482554
Validation loss: 2.5385522049115687

Epoch: 6| Step: 2
Training loss: 2.45894326440019
Validation loss: 2.545496066734409

Epoch: 6| Step: 3
Training loss: 2.0196719683203854
Validation loss: 2.513579337654122

Epoch: 6| Step: 4
Training loss: 2.276405730180997
Validation loss: 2.511289589206102

Epoch: 6| Step: 5
Training loss: 2.6549075156158244
Validation loss: 2.5246147371934353

Epoch: 6| Step: 6
Training loss: 1.9467362551901397
Validation loss: 2.498400028523941

Epoch: 6| Step: 7
Training loss: 2.568868399534312
Validation loss: 2.541287227599021

Epoch: 6| Step: 8
Training loss: 2.394638283159198
Validation loss: 2.550932485144306

Epoch: 6| Step: 9
Training loss: 2.267186330614899
Validation loss: 2.4959734371705515

Epoch: 6| Step: 10
Training loss: 2.4054924899618157
Validation loss: 2.531325514991331

Epoch: 6| Step: 11
Training loss: 2.4486996528002414
Validation loss: 2.5069075114232637

Epoch: 6| Step: 12
Training loss: 2.6875262591831226
Validation loss: 2.533661005417985

Epoch: 6| Step: 13
Training loss: 1.312390459121676
Validation loss: 2.5283925061245127

Epoch: 263| Step: 0
Training loss: 1.788529095821234
Validation loss: 2.5083502031153584

Epoch: 6| Step: 1
Training loss: 2.324694467793845
Validation loss: 2.5490644023031357

Epoch: 6| Step: 2
Training loss: 2.1889237266625443
Validation loss: 2.541794691151366

Epoch: 6| Step: 3
Training loss: 3.0526306564274277
Validation loss: 2.5510854669214487

Epoch: 6| Step: 4
Training loss: 2.226935104514004
Validation loss: 2.5239920169908907

Epoch: 6| Step: 5
Training loss: 1.7951451475065896
Validation loss: 2.5249188018904696

Epoch: 6| Step: 6
Training loss: 2.179644403065766
Validation loss: 2.5310896814222477

Epoch: 6| Step: 7
Training loss: 2.617109792893738
Validation loss: 2.5119732202023135

Epoch: 6| Step: 8
Training loss: 2.774728565859511
Validation loss: 2.51915378157988

Epoch: 6| Step: 9
Training loss: 1.8940245904544681
Validation loss: 2.5371115323253735

Epoch: 6| Step: 10
Training loss: 2.883140483279593
Validation loss: 2.5502088398501326

Epoch: 6| Step: 11
Training loss: 2.254255826342016
Validation loss: 2.5525096762374937

Epoch: 6| Step: 12
Training loss: 1.8734218313597533
Validation loss: 2.5547653818076865

Epoch: 6| Step: 13
Training loss: 2.134305661863867
Validation loss: 2.54216536957196

Epoch: 264| Step: 0
Training loss: 2.6879800766933144
Validation loss: 2.541186293527532

Epoch: 6| Step: 1
Training loss: 2.739094350904908
Validation loss: 2.5639854365630432

Epoch: 6| Step: 2
Training loss: 2.588501549351689
Validation loss: 2.5227759806732646

Epoch: 6| Step: 3
Training loss: 2.2360737351899966
Validation loss: 2.504700118707035

Epoch: 6| Step: 4
Training loss: 2.001380801384909
Validation loss: 2.5101511779178116

Epoch: 6| Step: 5
Training loss: 2.58491150016477
Validation loss: 2.5500052028380145

Epoch: 6| Step: 6
Training loss: 2.3485690609850733
Validation loss: 2.5362872853371705

Epoch: 6| Step: 7
Training loss: 2.6501899255313646
Validation loss: 2.527828587647894

Epoch: 6| Step: 8
Training loss: 2.0581151416772987
Validation loss: 2.5326454967632857

Epoch: 6| Step: 9
Training loss: 1.544902121397102
Validation loss: 2.5174917225988236

Epoch: 6| Step: 10
Training loss: 2.929950834519275
Validation loss: 2.5240795258878457

Epoch: 6| Step: 11
Training loss: 1.9256536042968515
Validation loss: 2.524721215890649

Epoch: 6| Step: 12
Training loss: 1.9110998775585735
Validation loss: 2.5305109640353125

Epoch: 6| Step: 13
Training loss: 1.8932747046141924
Validation loss: 2.5579966439762276

Epoch: 265| Step: 0
Training loss: 2.451444986029923
Validation loss: 2.5356413378749245

Epoch: 6| Step: 1
Training loss: 2.494475459458476
Validation loss: 2.5339824583878654

Epoch: 6| Step: 2
Training loss: 2.6033529816039493
Validation loss: 2.5133177103978155

Epoch: 6| Step: 3
Training loss: 2.7012448690596065
Validation loss: 2.5113425960897784

Epoch: 6| Step: 4
Training loss: 2.244196081845785
Validation loss: 2.546220439760022

Epoch: 6| Step: 5
Training loss: 2.5991630600976294
Validation loss: 2.564287522135481

Epoch: 6| Step: 6
Training loss: 2.7822183359171113
Validation loss: 2.5276327114278043

Epoch: 6| Step: 7
Training loss: 1.8296535654936184
Validation loss: 2.5317760676920567

Epoch: 6| Step: 8
Training loss: 1.9508618479842836
Validation loss: 2.5168740528190603

Epoch: 6| Step: 9
Training loss: 2.299346839377728
Validation loss: 2.501355175118857

Epoch: 6| Step: 10
Training loss: 2.005728029224196
Validation loss: 2.529614257157557

Epoch: 6| Step: 11
Training loss: 1.754253803302871
Validation loss: 2.5613953945573673

Epoch: 6| Step: 12
Training loss: 2.220887457478519
Validation loss: 2.5083538242094963

Epoch: 6| Step: 13
Training loss: 2.616870006832573
Validation loss: 2.519199179976246

Epoch: 266| Step: 0
Training loss: 2.056842094906983
Validation loss: 2.5055159496178434

Epoch: 6| Step: 1
Training loss: 2.2905308307270675
Validation loss: 2.520390151167749

Epoch: 6| Step: 2
Training loss: 2.5254323532539567
Validation loss: 2.5436626377499874

Epoch: 6| Step: 3
Training loss: 2.362056799597318
Validation loss: 2.507686535796011

Epoch: 6| Step: 4
Training loss: 2.220259032387536
Validation loss: 2.516756731427964

Epoch: 6| Step: 5
Training loss: 2.507781697021691
Validation loss: 2.5366442673842444

Epoch: 6| Step: 6
Training loss: 1.787770833658615
Validation loss: 2.5729012776360025

Epoch: 6| Step: 7
Training loss: 1.7529912316189533
Validation loss: 2.554397892229608

Epoch: 6| Step: 8
Training loss: 2.3542951250615696
Validation loss: 2.5680725788464356

Epoch: 6| Step: 9
Training loss: 2.4535902031719394
Validation loss: 2.5388065433145677

Epoch: 6| Step: 10
Training loss: 2.2624783467473746
Validation loss: 2.5246453429168594

Epoch: 6| Step: 11
Training loss: 2.7483892058119306
Validation loss: 2.5061201328962026

Epoch: 6| Step: 12
Training loss: 2.5261235069834136
Validation loss: 2.542481318261688

Epoch: 6| Step: 13
Training loss: 2.0812203436677446
Validation loss: 2.5176316633983418

Epoch: 267| Step: 0
Training loss: 1.8457639008057614
Validation loss: 2.5394593408782113

Epoch: 6| Step: 1
Training loss: 2.752603772084266
Validation loss: 2.496039090240195

Epoch: 6| Step: 2
Training loss: 2.674823673176733
Validation loss: 2.485710738955769

Epoch: 6| Step: 3
Training loss: 1.6121952567345565
Validation loss: 2.4881029794578295

Epoch: 6| Step: 4
Training loss: 2.4684900074478966
Validation loss: 2.5356384078726633

Epoch: 6| Step: 5
Training loss: 2.18948285289391
Validation loss: 2.5275811907704946

Epoch: 6| Step: 6
Training loss: 2.43491020538386
Validation loss: 2.5238614574657254

Epoch: 6| Step: 7
Training loss: 3.2521902186936873
Validation loss: 2.5365445695798625

Epoch: 6| Step: 8
Training loss: 2.3482019485240713
Validation loss: 2.521250776079002

Epoch: 6| Step: 9
Training loss: 1.477640232976746
Validation loss: 2.5194208490913024

Epoch: 6| Step: 10
Training loss: 2.797497131373458
Validation loss: 2.543075178979612

Epoch: 6| Step: 11
Training loss: 1.9544640185385769
Validation loss: 2.528367284218212

Epoch: 6| Step: 12
Training loss: 2.1033700876707377
Validation loss: 2.522577285759119

Epoch: 6| Step: 13
Training loss: 1.9233922784170145
Validation loss: 2.513978301343127

Epoch: 268| Step: 0
Training loss: 2.068295860576098
Validation loss: 2.5340230486460227

Epoch: 6| Step: 1
Training loss: 2.0072373334508984
Validation loss: 2.5267965237828034

Epoch: 6| Step: 2
Training loss: 2.942297730733465
Validation loss: 2.5024928810393967

Epoch: 6| Step: 3
Training loss: 2.7874884771955646
Validation loss: 2.5491766857604725

Epoch: 6| Step: 4
Training loss: 1.9478291238570824
Validation loss: 2.5203499709839146

Epoch: 6| Step: 5
Training loss: 2.617597223350806
Validation loss: 2.5441523257838408

Epoch: 6| Step: 6
Training loss: 2.3780396735665277
Validation loss: 2.517753465453728

Epoch: 6| Step: 7
Training loss: 2.2485147448347447
Validation loss: 2.5129002937449223

Epoch: 6| Step: 8
Training loss: 2.282127002769302
Validation loss: 2.528012991999815

Epoch: 6| Step: 9
Training loss: 2.4667361773757674
Validation loss: 2.538093667542398

Epoch: 6| Step: 10
Training loss: 1.8351024270639942
Validation loss: 2.5500658758261383

Epoch: 6| Step: 11
Training loss: 2.2707283159056617
Validation loss: 2.5283177248475504

Epoch: 6| Step: 12
Training loss: 2.3779754070296835
Validation loss: 2.5225143123025426

Epoch: 6| Step: 13
Training loss: 1.8479155344423812
Validation loss: 2.5361295009651452

Epoch: 269| Step: 0
Training loss: 2.22210804592973
Validation loss: 2.546993826343834

Epoch: 6| Step: 1
Training loss: 2.2642249681559687
Validation loss: 2.529393437782809

Epoch: 6| Step: 2
Training loss: 1.8578129465986506
Validation loss: 2.51499179215157

Epoch: 6| Step: 3
Training loss: 1.90566319673516
Validation loss: 2.5122325944906887

Epoch: 6| Step: 4
Training loss: 1.825867196028328
Validation loss: 2.501312865323348

Epoch: 6| Step: 5
Training loss: 2.671253868318005
Validation loss: 2.5272032184554014

Epoch: 6| Step: 6
Training loss: 3.0123475602072096
Validation loss: 2.524128127427289

Epoch: 6| Step: 7
Training loss: 2.283494863806307
Validation loss: 2.550834098546967

Epoch: 6| Step: 8
Training loss: 1.664327609296771
Validation loss: 2.546777175885806

Epoch: 6| Step: 9
Training loss: 2.367771995573573
Validation loss: 2.5182655614262024

Epoch: 6| Step: 10
Training loss: 2.4329696026340546
Validation loss: 2.506674394979631

Epoch: 6| Step: 11
Training loss: 2.3104655621522694
Validation loss: 2.5401543563338573

Epoch: 6| Step: 12
Training loss: 3.0368116815517197
Validation loss: 2.539044797218774

Epoch: 6| Step: 13
Training loss: 2.3161877276577125
Validation loss: 2.5306881283909486

Epoch: 270| Step: 0
Training loss: 1.6620517097627536
Validation loss: 2.5355464462413955

Epoch: 6| Step: 1
Training loss: 2.7585498375261817
Validation loss: 2.5133338195616655

Epoch: 6| Step: 2
Training loss: 1.9339954922178768
Validation loss: 2.5318949799446413

Epoch: 6| Step: 3
Training loss: 2.3718519930923665
Validation loss: 2.54286235084373

Epoch: 6| Step: 4
Training loss: 2.5184308628440237
Validation loss: 2.49382774270294

Epoch: 6| Step: 5
Training loss: 2.5726189280194047
Validation loss: 2.538755590223527

Epoch: 6| Step: 6
Training loss: 2.7710881128030658
Validation loss: 2.482523568744997

Epoch: 6| Step: 7
Training loss: 2.052235115234286
Validation loss: 2.5115354122715905

Epoch: 6| Step: 8
Training loss: 2.232529311377494
Validation loss: 2.5457085039905603

Epoch: 6| Step: 9
Training loss: 2.9123189583367908
Validation loss: 2.547541272694314

Epoch: 6| Step: 10
Training loss: 2.2652806513638346
Validation loss: 2.5206704359543233

Epoch: 6| Step: 11
Training loss: 1.8360383351007665
Validation loss: 2.5125712062243157

Epoch: 6| Step: 12
Training loss: 2.492306984455736
Validation loss: 2.5533142252275374

Epoch: 6| Step: 13
Training loss: 1.5473353924198632
Validation loss: 2.521527832238325

Epoch: 271| Step: 0
Training loss: 2.4018522426570375
Validation loss: 2.5739230032536207

Epoch: 6| Step: 1
Training loss: 2.5556808546926355
Validation loss: 2.5418770314883905

Epoch: 6| Step: 2
Training loss: 2.4179943866017752
Validation loss: 2.5116548306324353

Epoch: 6| Step: 3
Training loss: 2.062298389175556
Validation loss: 2.513898961662992

Epoch: 6| Step: 4
Training loss: 2.7143213441847798
Validation loss: 2.5129520087827997

Epoch: 6| Step: 5
Training loss: 1.9986604734676434
Validation loss: 2.511282542294343

Epoch: 6| Step: 6
Training loss: 2.4616310716593954
Validation loss: 2.515657786871126

Epoch: 6| Step: 7
Training loss: 2.479796599383991
Validation loss: 2.527891575768709

Epoch: 6| Step: 8
Training loss: 1.8120560595659405
Validation loss: 2.496485163854442

Epoch: 6| Step: 9
Training loss: 2.586468137941944
Validation loss: 2.544037396285421

Epoch: 6| Step: 10
Training loss: 1.9233924023743445
Validation loss: 2.5141199157579153

Epoch: 6| Step: 11
Training loss: 2.511093512710675
Validation loss: 2.527480827119174

Epoch: 6| Step: 12
Training loss: 1.7451134304354143
Validation loss: 2.5481017667603854

Epoch: 6| Step: 13
Training loss: 2.014282370818667
Validation loss: 2.5148147728387333

Epoch: 272| Step: 0
Training loss: 2.0614202736503535
Validation loss: 2.5419642453929483

Epoch: 6| Step: 1
Training loss: 2.434534077961903
Validation loss: 2.5471232545088043

Epoch: 6| Step: 2
Training loss: 2.160445181922985
Validation loss: 2.51329677434863

Epoch: 6| Step: 3
Training loss: 2.928928612648858
Validation loss: 2.5313195700474225

Epoch: 6| Step: 4
Training loss: 1.937767379523346
Validation loss: 2.5260001649905313

Epoch: 6| Step: 5
Training loss: 2.2912774420144277
Validation loss: 2.5363255743344544

Epoch: 6| Step: 6
Training loss: 2.304550942723424
Validation loss: 2.5093878052622713

Epoch: 6| Step: 7
Training loss: 1.6104260318116965
Validation loss: 2.539443106699475

Epoch: 6| Step: 8
Training loss: 1.839658367431098
Validation loss: 2.5322513622688767

Epoch: 6| Step: 9
Training loss: 2.0187790906953658
Validation loss: 2.499500016493882

Epoch: 6| Step: 10
Training loss: 3.251965295350725
Validation loss: 2.5020975747472938

Epoch: 6| Step: 11
Training loss: 1.9317401054230618
Validation loss: 2.5414966924495492

Epoch: 6| Step: 12
Training loss: 3.0554295099636066
Validation loss: 2.547587808511428

Epoch: 6| Step: 13
Training loss: 1.9240738610109884
Validation loss: 2.5318934368358037

Epoch: 273| Step: 0
Training loss: 1.619790603620992
Validation loss: 2.5327060896320983

Epoch: 6| Step: 1
Training loss: 2.2571715398797485
Validation loss: 2.519965780603355

Epoch: 6| Step: 2
Training loss: 2.9046209548296233
Validation loss: 2.5302879513648784

Epoch: 6| Step: 3
Training loss: 2.9667558899510222
Validation loss: 2.5262198998144294

Epoch: 6| Step: 4
Training loss: 1.911453898439451
Validation loss: 2.520423080498045

Epoch: 6| Step: 5
Training loss: 2.423277811658834
Validation loss: 2.5055875919426

Epoch: 6| Step: 6
Training loss: 2.241396455420936
Validation loss: 2.509314387514052

Epoch: 6| Step: 7
Training loss: 2.4907410827924306
Validation loss: 2.528129129363194

Epoch: 6| Step: 8
Training loss: 1.73833069463123
Validation loss: 2.4948818894291196

Epoch: 6| Step: 9
Training loss: 2.4084126055737465
Validation loss: 2.5287907553181634

Epoch: 6| Step: 10
Training loss: 2.0298184558888974
Validation loss: 2.5655431278251384

Epoch: 6| Step: 11
Training loss: 2.5325815913320837
Validation loss: 2.531269381808598

Epoch: 6| Step: 12
Training loss: 2.4630495726490484
Validation loss: 2.5407110882705326

Epoch: 6| Step: 13
Training loss: 1.695493415988478
Validation loss: 2.5236445181094056

Epoch: 274| Step: 0
Training loss: 1.9932483915051924
Validation loss: 2.532421159648702

Epoch: 6| Step: 1
Training loss: 2.1520915082669774
Validation loss: 2.5560884247483098

Epoch: 6| Step: 2
Training loss: 3.020240216838261
Validation loss: 2.515439035233228

Epoch: 6| Step: 3
Training loss: 2.086233282370809
Validation loss: 2.541863563115483

Epoch: 6| Step: 4
Training loss: 1.7394994688852874
Validation loss: 2.5340846318736974

Epoch: 6| Step: 5
Training loss: 2.4369310546439125
Validation loss: 2.5407604391968555

Epoch: 6| Step: 6
Training loss: 2.444207767626992
Validation loss: 2.5048721040432325

Epoch: 6| Step: 7
Training loss: 2.3724828732739613
Validation loss: 2.534750409619711

Epoch: 6| Step: 8
Training loss: 2.5823113870938474
Validation loss: 2.539301907064828

Epoch: 6| Step: 9
Training loss: 2.3493778054397447
Validation loss: 2.525159276294176

Epoch: 6| Step: 10
Training loss: 2.772773942682533
Validation loss: 2.50720639118165

Epoch: 6| Step: 11
Training loss: 1.9776426236386278
Validation loss: 2.552786305396745

Epoch: 6| Step: 12
Training loss: 2.145206643845717
Validation loss: 2.5325256783245353

Epoch: 6| Step: 13
Training loss: 1.759435760370351
Validation loss: 2.5226693009699006

Epoch: 275| Step: 0
Training loss: 1.7107881803108285
Validation loss: 2.542907590108413

Epoch: 6| Step: 1
Training loss: 2.2574761480040877
Validation loss: 2.5273939583753884

Epoch: 6| Step: 2
Training loss: 2.166350439420962
Validation loss: 2.548280825577548

Epoch: 6| Step: 3
Training loss: 2.2815691580973723
Validation loss: 2.5205648576402275

Epoch: 6| Step: 4
Training loss: 2.4405252316606094
Validation loss: 2.56034485624731

Epoch: 6| Step: 5
Training loss: 1.9238931247942266
Validation loss: 2.508705533965383

Epoch: 6| Step: 6
Training loss: 2.8164654221581786
Validation loss: 2.52506023929281

Epoch: 6| Step: 7
Training loss: 2.147918083518517
Validation loss: 2.503473787893841

Epoch: 6| Step: 8
Training loss: 2.3584581829582945
Validation loss: 2.564614146910766

Epoch: 6| Step: 9
Training loss: 3.0534817006018957
Validation loss: 2.5517994200188667

Epoch: 6| Step: 10
Training loss: 2.367708557997118
Validation loss: 2.5276211850061454

Epoch: 6| Step: 11
Training loss: 2.045147348705584
Validation loss: 2.507668333446345

Epoch: 6| Step: 12
Training loss: 2.330890830345403
Validation loss: 2.5231155550283897

Epoch: 6| Step: 13
Training loss: 1.9361762169804388
Validation loss: 2.5139691653461402

Epoch: 276| Step: 0
Training loss: 2.4350975129891848
Validation loss: 2.5483616372292195

Epoch: 6| Step: 1
Training loss: 2.4768181798223377
Validation loss: 2.5446439586933813

Epoch: 6| Step: 2
Training loss: 1.9026397537452375
Validation loss: 2.5275315389005004

Epoch: 6| Step: 3
Training loss: 2.4156394178347624
Validation loss: 2.536485616808389

Epoch: 6| Step: 4
Training loss: 2.1386142391991187
Validation loss: 2.5302978703911356

Epoch: 6| Step: 5
Training loss: 2.128833230542476
Validation loss: 2.5218569159594777

Epoch: 6| Step: 6
Training loss: 2.3226378634200135
Validation loss: 2.537052589696813

Epoch: 6| Step: 7
Training loss: 2.4576734422267554
Validation loss: 2.4872192387202747

Epoch: 6| Step: 8
Training loss: 2.622412723510859
Validation loss: 2.5297719847734337

Epoch: 6| Step: 9
Training loss: 2.2212145375011496
Validation loss: 2.5377814785352357

Epoch: 6| Step: 10
Training loss: 1.7791894740028298
Validation loss: 2.5267669095014442

Epoch: 6| Step: 11
Training loss: 2.6358017973008785
Validation loss: 2.522738015219509

Epoch: 6| Step: 12
Training loss: 2.4432338862284433
Validation loss: 2.5461596721175774

Epoch: 6| Step: 13
Training loss: 1.4344808136383893
Validation loss: 2.553602006264535

Epoch: 277| Step: 0
Training loss: 2.4077476695725473
Validation loss: 2.528820610492646

Epoch: 6| Step: 1
Training loss: 2.2298446289813496
Validation loss: 2.5015847761566814

Epoch: 6| Step: 2
Training loss: 1.8820642907839693
Validation loss: 2.513864056672629

Epoch: 6| Step: 3
Training loss: 2.034721108336102
Validation loss: 2.5137964643851354

Epoch: 6| Step: 4
Training loss: 1.996832843269857
Validation loss: 2.526329112129386

Epoch: 6| Step: 5
Training loss: 1.929480043448915
Validation loss: 2.5242526333710344

Epoch: 6| Step: 6
Training loss: 2.7608100658771573
Validation loss: 2.4926768667991603

Epoch: 6| Step: 7
Training loss: 1.65437008674333
Validation loss: 2.51156440028024

Epoch: 6| Step: 8
Training loss: 1.5806613595445196
Validation loss: 2.5045091193362796

Epoch: 6| Step: 9
Training loss: 2.2513332655280394
Validation loss: 2.5293589163854633

Epoch: 6| Step: 10
Training loss: 2.9885375068842137
Validation loss: 2.5278778594629956

Epoch: 6| Step: 11
Training loss: 2.8386355509966115
Validation loss: 2.527257830077067

Epoch: 6| Step: 12
Training loss: 1.903650105511159
Validation loss: 2.5741758287427348

Epoch: 6| Step: 13
Training loss: 3.0659676985361077
Validation loss: 2.5265097320296404

Epoch: 278| Step: 0
Training loss: 2.4253015704277954
Validation loss: 2.521998354302963

Epoch: 6| Step: 1
Training loss: 2.430900246535418
Validation loss: 2.526174284575903

Epoch: 6| Step: 2
Training loss: 1.9547545692165047
Validation loss: 2.4998645520178018

Epoch: 6| Step: 3
Training loss: 2.0452774452836553
Validation loss: 2.5102382833623866

Epoch: 6| Step: 4
Training loss: 2.272702652190972
Validation loss: 2.5204485914314563

Epoch: 6| Step: 5
Training loss: 2.23409306808222
Validation loss: 2.549869075191283

Epoch: 6| Step: 6
Training loss: 1.6635507862443835
Validation loss: 2.5280727700555263

Epoch: 6| Step: 7
Training loss: 2.00826653122767
Validation loss: 2.5295163984425586

Epoch: 6| Step: 8
Training loss: 2.2472165162535744
Validation loss: 2.5227883945210223

Epoch: 6| Step: 9
Training loss: 2.383040220291785
Validation loss: 2.4983555892288725

Epoch: 6| Step: 10
Training loss: 2.0013524489475727
Validation loss: 2.5381347506106207

Epoch: 6| Step: 11
Training loss: 3.160335804443981
Validation loss: 2.4943328469079926

Epoch: 6| Step: 12
Training loss: 2.5362964763867004
Validation loss: 2.5419059578911183

Epoch: 6| Step: 13
Training loss: 2.5975903495517674
Validation loss: 2.5222503654380493

Epoch: 279| Step: 0
Training loss: 2.3946896573763508
Validation loss: 2.534673679786206

Epoch: 6| Step: 1
Training loss: 1.433870626628313
Validation loss: 2.518651400537777

Epoch: 6| Step: 2
Training loss: 1.7247196909614433
Validation loss: 2.537478748052503

Epoch: 6| Step: 3
Training loss: 2.536466333479801
Validation loss: 2.5184316293614346

Epoch: 6| Step: 4
Training loss: 1.6171703982715917
Validation loss: 2.5313539563415586

Epoch: 6| Step: 5
Training loss: 2.1099009670501783
Validation loss: 2.515939694751667

Epoch: 6| Step: 6
Training loss: 2.6524557502931474
Validation loss: 2.537473313588173

Epoch: 6| Step: 7
Training loss: 2.005248811651776
Validation loss: 2.547216136257702

Epoch: 6| Step: 8
Training loss: 2.6095749098899517
Validation loss: 2.5357898091332234

Epoch: 6| Step: 9
Training loss: 2.5992353415296465
Validation loss: 2.5582856048538845

Epoch: 6| Step: 10
Training loss: 2.27289860686468
Validation loss: 2.530594403963277

Epoch: 6| Step: 11
Training loss: 2.436795059383942
Validation loss: 2.5113124509544837

Epoch: 6| Step: 12
Training loss: 1.7312169922253888
Validation loss: 2.52886327258516

Epoch: 6| Step: 13
Training loss: 3.5071682412697
Validation loss: 2.531070057174939

Epoch: 280| Step: 0
Training loss: 2.5510361268084742
Validation loss: 2.559717238566002

Epoch: 6| Step: 1
Training loss: 2.0579404427647554
Validation loss: 2.5139255172773534

Epoch: 6| Step: 2
Training loss: 2.126433393944385
Validation loss: 2.5209775070038463

Epoch: 6| Step: 3
Training loss: 1.8496575193171516
Validation loss: 2.5534480527953716

Epoch: 6| Step: 4
Training loss: 2.3623142754753235
Validation loss: 2.5079662046013937

Epoch: 6| Step: 5
Training loss: 1.956204664296119
Validation loss: 2.569805986162749

Epoch: 6| Step: 6
Training loss: 2.142593703652343
Validation loss: 2.5442003642631645

Epoch: 6| Step: 7
Training loss: 3.1699342098689463
Validation loss: 2.51187181447352

Epoch: 6| Step: 8
Training loss: 2.065294626568144
Validation loss: 2.5018836923388466

Epoch: 6| Step: 9
Training loss: 2.547887963429364
Validation loss: 2.529999291682814

Epoch: 6| Step: 10
Training loss: 1.873770819529887
Validation loss: 2.5533541959189088

Epoch: 6| Step: 11
Training loss: 2.4634754475786345
Validation loss: 2.5551855449701177

Epoch: 6| Step: 12
Training loss: 2.1862569274269132
Validation loss: 2.565469337523374

Epoch: 6| Step: 13
Training loss: 2.336033133395904
Validation loss: 2.5241159893180667

Epoch: 281| Step: 0
Training loss: 2.4749457921728983
Validation loss: 2.519717063325586

Epoch: 6| Step: 1
Training loss: 1.48213363880961
Validation loss: 2.5439751445126557

Epoch: 6| Step: 2
Training loss: 2.787146842720739
Validation loss: 2.5487305579085455

Epoch: 6| Step: 3
Training loss: 2.6981464027967967
Validation loss: 2.5409870476356504

Epoch: 6| Step: 4
Training loss: 2.050848678206981
Validation loss: 2.54085297540171

Epoch: 6| Step: 5
Training loss: 1.9642760288941032
Validation loss: 2.5111813026439753

Epoch: 6| Step: 6
Training loss: 1.8492056094735378
Validation loss: 2.56303143536796

Epoch: 6| Step: 7
Training loss: 2.428838649040926
Validation loss: 2.5527714494543616

Epoch: 6| Step: 8
Training loss: 2.0841578060156523
Validation loss: 2.5324243069742383

Epoch: 6| Step: 9
Training loss: 2.0579658144158866
Validation loss: 2.5337826014985936

Epoch: 6| Step: 10
Training loss: 1.9782307335917992
Validation loss: 2.5438188976938685

Epoch: 6| Step: 11
Training loss: 2.9559883889059306
Validation loss: 2.5214591907488924

Epoch: 6| Step: 12
Training loss: 2.292051063434796
Validation loss: 2.5345784510135014

Epoch: 6| Step: 13
Training loss: 1.7306386894326633
Validation loss: 2.5177569416729875

Epoch: 282| Step: 0
Training loss: 2.2726982461636287
Validation loss: 2.5314261616841285

Epoch: 6| Step: 1
Training loss: 2.3125418581911616
Validation loss: 2.4916072065328065

Epoch: 6| Step: 2
Training loss: 2.0350348789684576
Validation loss: 2.5211366433510243

Epoch: 6| Step: 3
Training loss: 2.5877048872025483
Validation loss: 2.5385978724185017

Epoch: 6| Step: 4
Training loss: 2.291706003949638
Validation loss: 2.508241210129943

Epoch: 6| Step: 5
Training loss: 2.031264906608464
Validation loss: 2.563246787497217

Epoch: 6| Step: 6
Training loss: 3.11998514245237
Validation loss: 2.5326160374222777

Epoch: 6| Step: 7
Training loss: 1.46463287870224
Validation loss: 2.5229516041105544

Epoch: 6| Step: 8
Training loss: 1.770411055835725
Validation loss: 2.5349830831094438

Epoch: 6| Step: 9
Training loss: 2.4054486810652955
Validation loss: 2.5626879492342773

Epoch: 6| Step: 10
Training loss: 2.4500351416733945
Validation loss: 2.532005883873166

Epoch: 6| Step: 11
Training loss: 2.221482005284915
Validation loss: 2.5298649401357824

Epoch: 6| Step: 12
Training loss: 2.588119922772061
Validation loss: 2.5771205383485976

Epoch: 6| Step: 13
Training loss: 2.408062339064502
Validation loss: 2.5613733811835044

Epoch: 283| Step: 0
Training loss: 2.5339426859611653
Validation loss: 2.5203043186742144

Epoch: 6| Step: 1
Training loss: 1.7375425676579028
Validation loss: 2.5042152082990916

Epoch: 6| Step: 2
Training loss: 2.5950147004345063
Validation loss: 2.5061946303152123

Epoch: 6| Step: 3
Training loss: 1.979881425254767
Validation loss: 2.522464884904017

Epoch: 6| Step: 4
Training loss: 2.422620498605019
Validation loss: 2.5055907218159406

Epoch: 6| Step: 5
Training loss: 2.3922202578344263
Validation loss: 2.5381227299882156

Epoch: 6| Step: 6
Training loss: 2.214918131983749
Validation loss: 2.5114623155608786

Epoch: 6| Step: 7
Training loss: 2.355785645887354
Validation loss: 2.5318339828902348

Epoch: 6| Step: 8
Training loss: 2.2354063841437215
Validation loss: 2.546734668422097

Epoch: 6| Step: 9
Training loss: 2.7429383382955677
Validation loss: 2.538159323975404

Epoch: 6| Step: 10
Training loss: 1.664329829704274
Validation loss: 2.5193396726071984

Epoch: 6| Step: 11
Training loss: 2.0049810371548618
Validation loss: 2.5350137102911994

Epoch: 6| Step: 12
Training loss: 2.776664468398794
Validation loss: 2.526992605327405

Epoch: 6| Step: 13
Training loss: 2.170205771538799
Validation loss: 2.4993532923527204

Epoch: 284| Step: 0
Training loss: 2.7530039939310997
Validation loss: 2.555459696484942

Epoch: 6| Step: 1
Training loss: 1.8411825375133481
Validation loss: 2.524242097474943

Epoch: 6| Step: 2
Training loss: 2.086974040032197
Validation loss: 2.520201651104374

Epoch: 6| Step: 3
Training loss: 2.0376565697547386
Validation loss: 2.542723561198003

Epoch: 6| Step: 4
Training loss: 2.334189076405161
Validation loss: 2.509823206429379

Epoch: 6| Step: 5
Training loss: 1.936547198897654
Validation loss: 2.518919617435333

Epoch: 6| Step: 6
Training loss: 2.576422666846367
Validation loss: 2.515057371782524

Epoch: 6| Step: 7
Training loss: 2.7614780520772735
Validation loss: 2.5034410851753925

Epoch: 6| Step: 8
Training loss: 1.5619895864327615
Validation loss: 2.5313419733994977

Epoch: 6| Step: 9
Training loss: 2.9340969421139884
Validation loss: 2.5354016559379704

Epoch: 6| Step: 10
Training loss: 1.978406445577806
Validation loss: 2.5358108718675982

Epoch: 6| Step: 11
Training loss: 1.9461225179033697
Validation loss: 2.5478857045483374

Epoch: 6| Step: 12
Training loss: 2.3502275600232463
Validation loss: 2.546659168424468

Epoch: 6| Step: 13
Training loss: 2.37423995053415
Validation loss: 2.534103425980531

Epoch: 285| Step: 0
Training loss: 2.067840597565121
Validation loss: 2.5099449298277037

Epoch: 6| Step: 1
Training loss: 3.239837898155283
Validation loss: 2.5186636769466295

Epoch: 6| Step: 2
Training loss: 1.9108883443578437
Validation loss: 2.494714288776595

Epoch: 6| Step: 3
Training loss: 1.8686677181029085
Validation loss: 2.513232182095261

Epoch: 6| Step: 4
Training loss: 2.217140473612402
Validation loss: 2.5647552454473947

Epoch: 6| Step: 5
Training loss: 2.227774949149949
Validation loss: 2.5510567049010433

Epoch: 6| Step: 6
Training loss: 2.203868442929163
Validation loss: 2.5380261781142877

Epoch: 6| Step: 7
Training loss: 2.0201023260958713
Validation loss: 2.5154913380987622

Epoch: 6| Step: 8
Training loss: 2.933611142123461
Validation loss: 2.5085128640892775

Epoch: 6| Step: 9
Training loss: 1.7765523301288937
Validation loss: 2.5595474120383797

Epoch: 6| Step: 10
Training loss: 1.7512466213893496
Validation loss: 2.479016945187883

Epoch: 6| Step: 11
Training loss: 2.245116125557056
Validation loss: 2.534070561114448

Epoch: 6| Step: 12
Training loss: 2.398444856017323
Validation loss: 2.5232561811368055

Epoch: 6| Step: 13
Training loss: 2.7945855856058297
Validation loss: 2.528082603465066

Epoch: 286| Step: 0
Training loss: 2.447215646779924
Validation loss: 2.52014200627322

Epoch: 6| Step: 1
Training loss: 2.679160107910608
Validation loss: 2.536354514775522

Epoch: 6| Step: 2
Training loss: 2.1550552817774977
Validation loss: 2.5547461973197327

Epoch: 6| Step: 3
Training loss: 2.163864291922819
Validation loss: 2.530674940847065

Epoch: 6| Step: 4
Training loss: 2.2526786541980615
Validation loss: 2.5146811178257997

Epoch: 6| Step: 5
Training loss: 1.3760827743007666
Validation loss: 2.5382580432955804

Epoch: 6| Step: 6
Training loss: 1.8432824786703463
Validation loss: 2.527181169982262

Epoch: 6| Step: 7
Training loss: 2.2705008820458743
Validation loss: 2.496679753052188

Epoch: 6| Step: 8
Training loss: 2.412728641724177
Validation loss: 2.5434121288390057

Epoch: 6| Step: 9
Training loss: 2.21416181665975
Validation loss: 2.5074810383665924

Epoch: 6| Step: 10
Training loss: 1.978789631584645
Validation loss: 2.570252138380259

Epoch: 6| Step: 11
Training loss: 2.878017997616501
Validation loss: 2.5367806615520796

Epoch: 6| Step: 12
Training loss: 2.40142338187859
Validation loss: 2.5319023248868064

Epoch: 6| Step: 13
Training loss: 2.6919778291659364
Validation loss: 2.5123889970203868

Epoch: 287| Step: 0
Training loss: 2.343025502442608
Validation loss: 2.5623014129514967

Epoch: 6| Step: 1
Training loss: 1.8127045351576208
Validation loss: 2.540550714348302

Epoch: 6| Step: 2
Training loss: 2.462415850600597
Validation loss: 2.515273573887567

Epoch: 6| Step: 3
Training loss: 2.5024519340479765
Validation loss: 2.526736265048573

Epoch: 6| Step: 4
Training loss: 2.6985811178735486
Validation loss: 2.517943016257928

Epoch: 6| Step: 5
Training loss: 2.385217672207515
Validation loss: 2.5499342675677554

Epoch: 6| Step: 6
Training loss: 2.6820917937737088
Validation loss: 2.5288070766366166

Epoch: 6| Step: 7
Training loss: 2.400101274102
Validation loss: 2.540864601719228

Epoch: 6| Step: 8
Training loss: 1.4639526912257843
Validation loss: 2.5431102581437273

Epoch: 6| Step: 9
Training loss: 1.7870924318271282
Validation loss: 2.4784805687259897

Epoch: 6| Step: 10
Training loss: 2.5733100085939102
Validation loss: 2.5101378380514996

Epoch: 6| Step: 11
Training loss: 2.1359397681670016
Validation loss: 2.523065185307699

Epoch: 6| Step: 12
Training loss: 2.33701965374042
Validation loss: 2.504414476612614

Epoch: 6| Step: 13
Training loss: 2.1837891847777446
Validation loss: 2.501126047051302

Epoch: 288| Step: 0
Training loss: 2.0891230686397706
Validation loss: 2.5272946179004183

Epoch: 6| Step: 1
Training loss: 1.6397449131040656
Validation loss: 2.5017315103261146

Epoch: 6| Step: 2
Training loss: 2.2745638544558
Validation loss: 2.523086517855077

Epoch: 6| Step: 3
Training loss: 2.384209495389974
Validation loss: 2.507414462069141

Epoch: 6| Step: 4
Training loss: 2.7078121759818403
Validation loss: 2.530926068785588

Epoch: 6| Step: 5
Training loss: 2.322194168107941
Validation loss: 2.5296767893648084

Epoch: 6| Step: 6
Training loss: 2.413219514590031
Validation loss: 2.5511159600772193

Epoch: 6| Step: 7
Training loss: 2.2754134672270836
Validation loss: 2.5043736603351974

Epoch: 6| Step: 8
Training loss: 2.2210126273643436
Validation loss: 2.5191894818378544

Epoch: 6| Step: 9
Training loss: 2.122993644040009
Validation loss: 2.5306821697934425

Epoch: 6| Step: 10
Training loss: 2.0170448684203093
Validation loss: 2.5202667523703273

Epoch: 6| Step: 11
Training loss: 2.5152252069132297
Validation loss: 2.5213072212803675

Epoch: 6| Step: 12
Training loss: 2.4019939881312267
Validation loss: 2.5416313207929977

Epoch: 6| Step: 13
Training loss: 2.497157579074946
Validation loss: 2.5388590529397015

Epoch: 289| Step: 0
Training loss: 2.785835514993942
Validation loss: 2.5318979385798217

Epoch: 6| Step: 1
Training loss: 2.523274797325853
Validation loss: 2.518608182746062

Epoch: 6| Step: 2
Training loss: 2.4260524039210374
Validation loss: 2.5407663519607713

Epoch: 6| Step: 3
Training loss: 2.1307723239981105
Validation loss: 2.5899506927020868

Epoch: 6| Step: 4
Training loss: 2.077169098098941
Validation loss: 2.538435367852383

Epoch: 6| Step: 5
Training loss: 2.395743879776758
Validation loss: 2.5018460595046816

Epoch: 6| Step: 6
Training loss: 2.0937207348045113
Validation loss: 2.5375139894478207

Epoch: 6| Step: 7
Training loss: 2.2417820155117716
Validation loss: 2.5271567313049297

Epoch: 6| Step: 8
Training loss: 2.5436348928890626
Validation loss: 2.4792241877184455

Epoch: 6| Step: 9
Training loss: 2.160148524017038
Validation loss: 2.552595454238205

Epoch: 6| Step: 10
Training loss: 2.48454927037165
Validation loss: 2.537606238418382

Epoch: 6| Step: 11
Training loss: 1.7003425477423912
Validation loss: 2.537406480545848

Epoch: 6| Step: 12
Training loss: 2.2532989900368254
Validation loss: 2.533134475521517

Epoch: 6| Step: 13
Training loss: 1.7843511923890294
Validation loss: 2.5017683788967737

Epoch: 290| Step: 0
Training loss: 2.3095184901030623
Validation loss: 2.531695811902875

Epoch: 6| Step: 1
Training loss: 2.053240596588536
Validation loss: 2.509744105680319

Epoch: 6| Step: 2
Training loss: 2.7240700246014025
Validation loss: 2.506377166074208

Epoch: 6| Step: 3
Training loss: 2.3930452350146187
Validation loss: 2.5089798699084707

Epoch: 6| Step: 4
Training loss: 2.707250544925698
Validation loss: 2.539730450761007

Epoch: 6| Step: 5
Training loss: 2.4918944566214423
Validation loss: 2.4921634547028693

Epoch: 6| Step: 6
Training loss: 2.6258238453291605
Validation loss: 2.5003054150278303

Epoch: 6| Step: 7
Training loss: 1.834936185449576
Validation loss: 2.546851370358538

Epoch: 6| Step: 8
Training loss: 2.055248111495225
Validation loss: 2.495266798583091

Epoch: 6| Step: 9
Training loss: 2.2389761759694626
Validation loss: 2.55005587183438

Epoch: 6| Step: 10
Training loss: 2.1540963294857782
Validation loss: 2.535357679148243

Epoch: 6| Step: 11
Training loss: 1.7583076288354236
Validation loss: 2.5383387834318087

Epoch: 6| Step: 12
Training loss: 2.12470636021945
Validation loss: 2.5315486155030813

Epoch: 6| Step: 13
Training loss: 1.8677679460511203
Validation loss: 2.500545094653678

Epoch: 291| Step: 0
Training loss: 2.4169892282178855
Validation loss: 2.513417135125248

Epoch: 6| Step: 1
Training loss: 2.3775034308985394
Validation loss: 2.5421008608798816

Epoch: 6| Step: 2
Training loss: 2.453154934257053
Validation loss: 2.5348744676551216

Epoch: 6| Step: 3
Training loss: 2.5005013916768437
Validation loss: 2.5276329959237103

Epoch: 6| Step: 4
Training loss: 1.813386108574777
Validation loss: 2.5646529878671505

Epoch: 6| Step: 5
Training loss: 1.9099350627625344
Validation loss: 2.4993957886626763

Epoch: 6| Step: 6
Training loss: 1.881834607205517
Validation loss: 2.499563517460248

Epoch: 6| Step: 7
Training loss: 2.337124220073925
Validation loss: 2.5341885255502246

Epoch: 6| Step: 8
Training loss: 2.724965587013703
Validation loss: 2.498689857839314

Epoch: 6| Step: 9
Training loss: 1.8865278001279275
Validation loss: 2.5183719329385235

Epoch: 6| Step: 10
Training loss: 1.8418162275088459
Validation loss: 2.497976417288379

Epoch: 6| Step: 11
Training loss: 1.8949576301166033
Validation loss: 2.530571154159685

Epoch: 6| Step: 12
Training loss: 2.573986083472511
Validation loss: 2.5042196123823666

Epoch: 6| Step: 13
Training loss: 3.053943748370767
Validation loss: 2.539396103618887

Epoch: 292| Step: 0
Training loss: 2.1125303051823234
Validation loss: 2.5338090716527146

Epoch: 6| Step: 1
Training loss: 1.8779781849074502
Validation loss: 2.543539580331865

Epoch: 6| Step: 2
Training loss: 3.0717079155964346
Validation loss: 2.5265751182273584

Epoch: 6| Step: 3
Training loss: 2.353359815576408
Validation loss: 2.5366594451686804

Epoch: 6| Step: 4
Training loss: 2.752972470235582
Validation loss: 2.5075748149387898

Epoch: 6| Step: 5
Training loss: 1.8854930363427613
Validation loss: 2.5072564527134316

Epoch: 6| Step: 6
Training loss: 2.6097877769860447
Validation loss: 2.5262437904276194

Epoch: 6| Step: 7
Training loss: 2.497651332054511
Validation loss: 2.566973269602339

Epoch: 6| Step: 8
Training loss: 1.8146817460583933
Validation loss: 2.5608205546639993

Epoch: 6| Step: 9
Training loss: 1.6343496287840142
Validation loss: 2.4966169745434295

Epoch: 6| Step: 10
Training loss: 2.1919834512378253
Validation loss: 2.515542326230901

Epoch: 6| Step: 11
Training loss: 2.620135568503826
Validation loss: 2.536846321145013

Epoch: 6| Step: 12
Training loss: 2.0760395800983096
Validation loss: 2.518720709908459

Epoch: 6| Step: 13
Training loss: 1.3397489019248123
Validation loss: 2.570630560518532

Epoch: 293| Step: 0
Training loss: 2.479666993333409
Validation loss: 2.52398960366588

Epoch: 6| Step: 1
Training loss: 1.8076839259526154
Validation loss: 2.533260770766706

Epoch: 6| Step: 2
Training loss: 2.500790852388527
Validation loss: 2.578025249640923

Epoch: 6| Step: 3
Training loss: 1.9514202765508106
Validation loss: 2.517571257599996

Epoch: 6| Step: 4
Training loss: 2.4712079523780437
Validation loss: 2.572124295586263

Epoch: 6| Step: 5
Training loss: 1.9772297935249175
Validation loss: 2.522326779004974

Epoch: 6| Step: 6
Training loss: 2.13142746888435
Validation loss: 2.543296510944356

Epoch: 6| Step: 7
Training loss: 2.325115639876875
Validation loss: 2.531769975968221

Epoch: 6| Step: 8
Training loss: 2.032356664992396
Validation loss: 2.5167408911925286

Epoch: 6| Step: 9
Training loss: 2.5397536994883856
Validation loss: 2.5033064337469653

Epoch: 6| Step: 10
Training loss: 2.3611560692589633
Validation loss: 2.530363303581571

Epoch: 6| Step: 11
Training loss: 2.350120939024107
Validation loss: 2.5407706704895436

Epoch: 6| Step: 12
Training loss: 2.415977730604194
Validation loss: 2.4960061545353978

Epoch: 6| Step: 13
Training loss: 2.418583263396909
Validation loss: 2.519698728117775

Epoch: 294| Step: 0
Training loss: 1.6946620836133435
Validation loss: 2.518921918576183

Epoch: 6| Step: 1
Training loss: 2.059882728608922
Validation loss: 2.521246360056155

Epoch: 6| Step: 2
Training loss: 2.1609598220698363
Validation loss: 2.5565630279471923

Epoch: 6| Step: 3
Training loss: 2.287740071524118
Validation loss: 2.5471563938091144

Epoch: 6| Step: 4
Training loss: 1.9443253919697696
Validation loss: 2.5436798130364213

Epoch: 6| Step: 5
Training loss: 2.832306694579209
Validation loss: 2.517296747882468

Epoch: 6| Step: 6
Training loss: 2.712200651043539
Validation loss: 2.532217718108256

Epoch: 6| Step: 7
Training loss: 2.0637836507621916
Validation loss: 2.5277250481920905

Epoch: 6| Step: 8
Training loss: 3.2555812449159576
Validation loss: 2.535529505031648

Epoch: 6| Step: 9
Training loss: 1.7508563262625585
Validation loss: 2.5410517717526964

Epoch: 6| Step: 10
Training loss: 2.3261688001790133
Validation loss: 2.504363057186659

Epoch: 6| Step: 11
Training loss: 1.7535458472713699
Validation loss: 2.549105480534801

Epoch: 6| Step: 12
Training loss: 1.9857859363841288
Validation loss: 2.526295804652126

Epoch: 6| Step: 13
Training loss: 1.9149586973392556
Validation loss: 2.4964264740320234

Epoch: 295| Step: 0
Training loss: 1.9853971957372432
Validation loss: 2.4978832769310295

Epoch: 6| Step: 1
Training loss: 2.746871208735118
Validation loss: 2.5056615752088596

Epoch: 6| Step: 2
Training loss: 2.7627541679728522
Validation loss: 2.526662651299844

Epoch: 6| Step: 3
Training loss: 2.1537018245505264
Validation loss: 2.5273617152957457

Epoch: 6| Step: 4
Training loss: 2.194816451859867
Validation loss: 2.518588245453624

Epoch: 6| Step: 5
Training loss: 1.8834541462095709
Validation loss: 2.5166602761374164

Epoch: 6| Step: 6
Training loss: 1.995105476363695
Validation loss: 2.53270965869625

Epoch: 6| Step: 7
Training loss: 2.6802960030224945
Validation loss: 2.50406463865922

Epoch: 6| Step: 8
Training loss: 2.4038507960348054
Validation loss: 2.5334513764116418

Epoch: 6| Step: 9
Training loss: 1.838383380534894
Validation loss: 2.514747566987617

Epoch: 6| Step: 10
Training loss: 1.853892988373738
Validation loss: 2.5163343536816223

Epoch: 6| Step: 11
Training loss: 2.4243165537669893
Validation loss: 2.5106311754616173

Epoch: 6| Step: 12
Training loss: 2.2149177014150543
Validation loss: 2.4668836172984756

Epoch: 6| Step: 13
Training loss: 1.5193478196544143
Validation loss: 2.4887433798656455

Epoch: 296| Step: 0
Training loss: 2.5901782710166827
Validation loss: 2.4956955698133756

Epoch: 6| Step: 1
Training loss: 1.5632309539039746
Validation loss: 2.508159084098135

Epoch: 6| Step: 2
Training loss: 2.4408316706682895
Validation loss: 2.4884530668778324

Epoch: 6| Step: 3
Training loss: 2.4179054461498275
Validation loss: 2.539995961336648

Epoch: 6| Step: 4
Training loss: 2.7641704635279694
Validation loss: 2.5153367918124303

Epoch: 6| Step: 5
Training loss: 2.2993883148828145
Validation loss: 2.522133110664905

Epoch: 6| Step: 6
Training loss: 2.210198700201536
Validation loss: 2.5213622214078404

Epoch: 6| Step: 7
Training loss: 1.976393379545289
Validation loss: 2.5210368020842058

Epoch: 6| Step: 8
Training loss: 2.162940881140781
Validation loss: 2.527766012762163

Epoch: 6| Step: 9
Training loss: 2.4878904793359395
Validation loss: 2.4997238970642903

Epoch: 6| Step: 10
Training loss: 2.160935108020752
Validation loss: 2.5337936299199337

Epoch: 6| Step: 11
Training loss: 2.216799133568508
Validation loss: 2.5162800225457196

Epoch: 6| Step: 12
Training loss: 2.092277122669087
Validation loss: 2.5211056777687855

Epoch: 6| Step: 13
Training loss: 1.4685866894122552
Validation loss: 2.515581409196672

Epoch: 297| Step: 0
Training loss: 2.1144726227841453
Validation loss: 2.5158226584180072

Epoch: 6| Step: 1
Training loss: 2.334003170687917
Validation loss: 2.5138028785866964

Epoch: 6| Step: 2
Training loss: 1.8378748634627355
Validation loss: 2.517957256046641

Epoch: 6| Step: 3
Training loss: 2.23819603349997
Validation loss: 2.5392190839193782

Epoch: 6| Step: 4
Training loss: 1.638660235357946
Validation loss: 2.4724548607231784

Epoch: 6| Step: 5
Training loss: 1.967805363322743
Validation loss: 2.4932805512235108

Epoch: 6| Step: 6
Training loss: 2.244211911230431
Validation loss: 2.5218164460375085

Epoch: 6| Step: 7
Training loss: 1.60752910937684
Validation loss: 2.5150409770350968

Epoch: 6| Step: 8
Training loss: 2.7809797862942736
Validation loss: 2.5294136182910303

Epoch: 6| Step: 9
Training loss: 2.611528615737425
Validation loss: 2.510728682807084

Epoch: 6| Step: 10
Training loss: 3.2266844202241542
Validation loss: 2.5287126417600394

Epoch: 6| Step: 11
Training loss: 1.593281995087229
Validation loss: 2.5349581897188314

Epoch: 6| Step: 12
Training loss: 2.288188468013272
Validation loss: 2.5303436888981774

Epoch: 6| Step: 13
Training loss: 2.5253651345508783
Validation loss: 2.496677140820811

Epoch: 298| Step: 0
Training loss: 2.236410640876685
Validation loss: 2.5385053055132976

Epoch: 6| Step: 1
Training loss: 1.544111615658048
Validation loss: 2.506343615441667

Epoch: 6| Step: 2
Training loss: 2.1758583918102157
Validation loss: 2.525623626843816

Epoch: 6| Step: 3
Training loss: 2.2011316899961533
Validation loss: 2.503049627047024

Epoch: 6| Step: 4
Training loss: 1.759282290250297
Validation loss: 2.538901279780163

Epoch: 6| Step: 5
Training loss: 1.9491755723471185
Validation loss: 2.509693575872256

Epoch: 6| Step: 6
Training loss: 2.2572531881839057
Validation loss: 2.518313770243314

Epoch: 6| Step: 7
Training loss: 2.58101284239987
Validation loss: 2.500591243028021

Epoch: 6| Step: 8
Training loss: 2.799706416406264
Validation loss: 2.530540299972852

Epoch: 6| Step: 9
Training loss: 2.5021071137233357
Validation loss: 2.514555915701873

Epoch: 6| Step: 10
Training loss: 2.6069426935626203
Validation loss: 2.5252764600956934

Epoch: 6| Step: 11
Training loss: 2.3022831158376658
Validation loss: 2.524524053120155

Epoch: 6| Step: 12
Training loss: 2.111740142680036
Validation loss: 2.519826508138011

Epoch: 6| Step: 13
Training loss: 1.5498463000724512
Validation loss: 2.5581236023381697

Epoch: 299| Step: 0
Training loss: 2.0802726260663498
Validation loss: 2.5306434679458043

Epoch: 6| Step: 1
Training loss: 2.3877316587213184
Validation loss: 2.531013931494207

Epoch: 6| Step: 2
Training loss: 2.7303300857807167
Validation loss: 2.4961804570950163

Epoch: 6| Step: 3
Training loss: 1.9708046863347664
Validation loss: 2.517695825088104

Epoch: 6| Step: 4
Training loss: 1.884577893014472
Validation loss: 2.5130506451357157

Epoch: 6| Step: 5
Training loss: 1.8038856739772267
Validation loss: 2.524410894016699

Epoch: 6| Step: 6
Training loss: 2.2506102688007203
Validation loss: 2.513899161031277

Epoch: 6| Step: 7
Training loss: 2.6888918266683177
Validation loss: 2.533621238063646

Epoch: 6| Step: 8
Training loss: 2.589071443458263
Validation loss: 2.5331596428781027

Epoch: 6| Step: 9
Training loss: 1.635853249494537
Validation loss: 2.512575869107821

Epoch: 6| Step: 10
Training loss: 1.8782610191815081
Validation loss: 2.511848902702796

Epoch: 6| Step: 11
Training loss: 2.805844294186075
Validation loss: 2.545557690646668

Epoch: 6| Step: 12
Training loss: 1.7054718367041466
Validation loss: 2.535331400599664

Epoch: 6| Step: 13
Training loss: 2.8258684966962133
Validation loss: 2.4958923762901057

Epoch: 300| Step: 0
Training loss: 2.6343898858750543
Validation loss: 2.55569548908882

Epoch: 6| Step: 1
Training loss: 1.482387376466786
Validation loss: 2.510046435872267

Epoch: 6| Step: 2
Training loss: 1.8928465598702517
Validation loss: 2.51690729399756

Epoch: 6| Step: 3
Training loss: 2.4022063921605716
Validation loss: 2.520672648539386

Epoch: 6| Step: 4
Training loss: 2.1331634791625813
Validation loss: 2.516315700918539

Epoch: 6| Step: 5
Training loss: 1.922485130659325
Validation loss: 2.525420814251678

Epoch: 6| Step: 6
Training loss: 2.054987316231671
Validation loss: 2.514890779294301

Epoch: 6| Step: 7
Training loss: 2.7966235170305462
Validation loss: 2.5397354332258826

Epoch: 6| Step: 8
Training loss: 2.4615359592883523
Validation loss: 2.4957546633078875

Epoch: 6| Step: 9
Training loss: 2.307097749045567
Validation loss: 2.510396770864293

Epoch: 6| Step: 10
Training loss: 2.552592967523244
Validation loss: 2.526020551228964

Epoch: 6| Step: 11
Training loss: 2.412776468672712
Validation loss: 2.5138616631989565

Epoch: 6| Step: 12
Training loss: 2.2558633911625505
Validation loss: 2.5241628686463056

Epoch: 6| Step: 13
Training loss: 1.9172560228011186
Validation loss: 2.5185888134350383

Epoch: 301| Step: 0
Training loss: 1.7050370149146334
Validation loss: 2.4955575100496343

Epoch: 6| Step: 1
Training loss: 2.322505646566337
Validation loss: 2.544078517393976

Epoch: 6| Step: 2
Training loss: 2.874115849421097
Validation loss: 2.5124564986059013

Epoch: 6| Step: 3
Training loss: 2.4515568282752866
Validation loss: 2.5249930401274336

Epoch: 6| Step: 4
Training loss: 1.6759127051805958
Validation loss: 2.515047400804252

Epoch: 6| Step: 5
Training loss: 2.001043524303564
Validation loss: 2.5384924635717394

Epoch: 6| Step: 6
Training loss: 2.2594598482485075
Validation loss: 2.5007207733856354

Epoch: 6| Step: 7
Training loss: 2.092053765373719
Validation loss: 2.5237506507188003

Epoch: 6| Step: 8
Training loss: 2.753677510170189
Validation loss: 2.50776517292639

Epoch: 6| Step: 9
Training loss: 2.238319489849775
Validation loss: 2.4772562877261413

Epoch: 6| Step: 10
Training loss: 1.9201273221278226
Validation loss: 2.5274547917617363

Epoch: 6| Step: 11
Training loss: 2.577760890333538
Validation loss: 2.519366837892679

Epoch: 6| Step: 12
Training loss: 1.6999005933756859
Validation loss: 2.5051206383612254

Epoch: 6| Step: 13
Training loss: 1.9219563784854174
Validation loss: 2.4955337869198924

Epoch: 302| Step: 0
Training loss: 2.071448018306906
Validation loss: 2.514885886244025

Epoch: 6| Step: 1
Training loss: 2.119919931591149
Validation loss: 2.536982336119867

Epoch: 6| Step: 2
Training loss: 2.209855856358914
Validation loss: 2.5159746753878793

Epoch: 6| Step: 3
Training loss: 2.2931576704607597
Validation loss: 2.5354347694759074

Epoch: 6| Step: 4
Training loss: 2.3611537468245896
Validation loss: 2.483974705990767

Epoch: 6| Step: 5
Training loss: 2.1422334739821336
Validation loss: 2.519031404041659

Epoch: 6| Step: 6
Training loss: 2.2113918740673073
Validation loss: 2.5312804282691608

Epoch: 6| Step: 7
Training loss: 2.4442505892073703
Validation loss: 2.5343813813999136

Epoch: 6| Step: 8
Training loss: 2.2340271152090474
Validation loss: 2.5203500604954407

Epoch: 6| Step: 9
Training loss: 2.2752086124560296
Validation loss: 2.522687550564483

Epoch: 6| Step: 10
Training loss: 2.0726683833762483
Validation loss: 2.5485953917217015

Epoch: 6| Step: 11
Training loss: 3.007745281310586
Validation loss: 2.541538454840135

Epoch: 6| Step: 12
Training loss: 1.5505827577334665
Validation loss: 2.530637086804831

Epoch: 6| Step: 13
Training loss: 1.9107801046609
Validation loss: 2.534993379183007

Epoch: 303| Step: 0
Training loss: 1.475721170021346
Validation loss: 2.504320007410143

Epoch: 6| Step: 1
Training loss: 2.367272504532964
Validation loss: 2.5189395816196103

Epoch: 6| Step: 2
Training loss: 2.05212358412665
Validation loss: 2.5391055715739723

Epoch: 6| Step: 3
Training loss: 1.965689505532976
Validation loss: 2.5284450921028108

Epoch: 6| Step: 4
Training loss: 2.5375782545829644
Validation loss: 2.4924087120208167

Epoch: 6| Step: 5
Training loss: 2.429328720244722
Validation loss: 2.563492229454813

Epoch: 6| Step: 6
Training loss: 2.1852825369704347
Validation loss: 2.543769365740711

Epoch: 6| Step: 7
Training loss: 2.4145326619485714
Validation loss: 2.5142372674021565

Epoch: 6| Step: 8
Training loss: 2.400391121624491
Validation loss: 2.52087887673936

Epoch: 6| Step: 9
Training loss: 2.653243663636861
Validation loss: 2.5042421527319427

Epoch: 6| Step: 10
Training loss: 2.1020089848668433
Validation loss: 2.5083407706669383

Epoch: 6| Step: 11
Training loss: 1.899127348219023
Validation loss: 2.514738925158396

Epoch: 6| Step: 12
Training loss: 2.5312260756127287
Validation loss: 2.5067096726359557

Epoch: 6| Step: 13
Training loss: 2.4979311011764045
Validation loss: 2.506451336734776

Epoch: 304| Step: 0
Training loss: 1.4030587331505564
Validation loss: 2.511266860981671

Epoch: 6| Step: 1
Training loss: 2.5651685264504156
Validation loss: 2.5234884880906354

Epoch: 6| Step: 2
Training loss: 2.3677385651895646
Validation loss: 2.5526342711831385

Epoch: 6| Step: 3
Training loss: 2.635554665632707
Validation loss: 2.4968161623590928

Epoch: 6| Step: 4
Training loss: 2.2945403605752053
Validation loss: 2.503585678444853

Epoch: 6| Step: 5
Training loss: 2.438625076042877
Validation loss: 2.538724674945099

Epoch: 6| Step: 6
Training loss: 2.1279821227477185
Validation loss: 2.5484161726740338

Epoch: 6| Step: 7
Training loss: 1.6901191589286133
Validation loss: 2.53233006469508

Epoch: 6| Step: 8
Training loss: 1.815378468464537
Validation loss: 2.494233482679594

Epoch: 6| Step: 9
Training loss: 2.172896330297246
Validation loss: 2.489178769897674

Epoch: 6| Step: 10
Training loss: 2.3162050208033778
Validation loss: 2.5411670297279403

Epoch: 6| Step: 11
Training loss: 2.715953221475863
Validation loss: 2.536905680865736

Epoch: 6| Step: 12
Training loss: 2.085405603380103
Validation loss: 2.4919035933048534

Epoch: 6| Step: 13
Training loss: 1.8667755949504123
Validation loss: 2.5066288505394927

Epoch: 305| Step: 0
Training loss: 1.8243872097086424
Validation loss: 2.523266265981256

Epoch: 6| Step: 1
Training loss: 1.1808478099623883
Validation loss: 2.514017441254897

Epoch: 6| Step: 2
Training loss: 2.4626262380706168
Validation loss: 2.549660563613326

Epoch: 6| Step: 3
Training loss: 3.0815011500514737
Validation loss: 2.4813786503134687

Epoch: 6| Step: 4
Training loss: 2.2976041920876047
Validation loss: 2.508177429048481

Epoch: 6| Step: 5
Training loss: 2.0177683708253586
Validation loss: 2.536579957362179

Epoch: 6| Step: 6
Training loss: 3.0216847940761418
Validation loss: 2.4990083635120794

Epoch: 6| Step: 7
Training loss: 1.714539798571684
Validation loss: 2.523298993695938

Epoch: 6| Step: 8
Training loss: 2.2731436313372204
Validation loss: 2.4853919846011494

Epoch: 6| Step: 9
Training loss: 1.938665593152665
Validation loss: 2.5116217393907947

Epoch: 6| Step: 10
Training loss: 2.2540114988086186
Validation loss: 2.5294271681587186

Epoch: 6| Step: 11
Training loss: 2.2963797106626798
Validation loss: 2.5406573843731364

Epoch: 6| Step: 12
Training loss: 2.128468767764545
Validation loss: 2.5108363895447963

Epoch: 6| Step: 13
Training loss: 2.43935108811321
Validation loss: 2.4973587717219927

Epoch: 306| Step: 0
Training loss: 2.3521581763452586
Validation loss: 2.4974030932536575

Epoch: 6| Step: 1
Training loss: 1.8545285703958045
Validation loss: 2.5318052613865425

Epoch: 6| Step: 2
Training loss: 1.730186835547213
Validation loss: 2.4989164998804236

Epoch: 6| Step: 3
Training loss: 2.253080378710917
Validation loss: 2.5171755134152147

Epoch: 6| Step: 4
Training loss: 2.0546821623178055
Validation loss: 2.51174732456671

Epoch: 6| Step: 5
Training loss: 2.310212834284141
Validation loss: 2.4843613726164184

Epoch: 6| Step: 6
Training loss: 1.8502928167016814
Validation loss: 2.5282217141745833

Epoch: 6| Step: 7
Training loss: 1.8400616364936206
Validation loss: 2.5143394724361037

Epoch: 6| Step: 8
Training loss: 2.437015974453432
Validation loss: 2.5355393388436465

Epoch: 6| Step: 9
Training loss: 2.9312417752083437
Validation loss: 2.542571860967942

Epoch: 6| Step: 10
Training loss: 1.9154172501539972
Validation loss: 2.513145402289135

Epoch: 6| Step: 11
Training loss: 2.7882662793305637
Validation loss: 2.548907215760072

Epoch: 6| Step: 12
Training loss: 2.3601176122680396
Validation loss: 2.509483894951467

Epoch: 6| Step: 13
Training loss: 2.072339372104639
Validation loss: 2.514322666181307

Epoch: 307| Step: 0
Training loss: 2.185634689680464
Validation loss: 2.515377737988931

Epoch: 6| Step: 1
Training loss: 1.74503480346425
Validation loss: 2.51972797780315

Epoch: 6| Step: 2
Training loss: 2.5290103941583526
Validation loss: 2.549359689226344

Epoch: 6| Step: 3
Training loss: 1.7505717024883551
Validation loss: 2.5273892599410894

Epoch: 6| Step: 4
Training loss: 2.4282182869487383
Validation loss: 2.5196572462653073

Epoch: 6| Step: 5
Training loss: 2.522894738939683
Validation loss: 2.5296372046292723

Epoch: 6| Step: 6
Training loss: 2.2001185905398293
Validation loss: 2.5144130258812205

Epoch: 6| Step: 7
Training loss: 2.4337165030048777
Validation loss: 2.5491249498466493

Epoch: 6| Step: 8
Training loss: 1.9655816150723058
Validation loss: 2.5412442989273414

Epoch: 6| Step: 9
Training loss: 2.248849574668093
Validation loss: 2.532150214821909

Epoch: 6| Step: 10
Training loss: 2.177075752597977
Validation loss: 2.5261910637402525

Epoch: 6| Step: 11
Training loss: 2.041644686626011
Validation loss: 2.504019670251866

Epoch: 6| Step: 12
Training loss: 2.118641934409926
Validation loss: 2.5402361150461816

Epoch: 6| Step: 13
Training loss: 2.431814066511511
Validation loss: 2.5010614859096

Epoch: 308| Step: 0
Training loss: 2.1807807177079512
Validation loss: 2.5378577598413794

Epoch: 6| Step: 1
Training loss: 1.9108947075382592
Validation loss: 2.5560801282944206

Epoch: 6| Step: 2
Training loss: 2.03978029676179
Validation loss: 2.499697872076694

Epoch: 6| Step: 3
Training loss: 2.0066691308413023
Validation loss: 2.5120458744407004

Epoch: 6| Step: 4
Training loss: 2.255287104428365
Validation loss: 2.5452151339960847

Epoch: 6| Step: 5
Training loss: 2.433467757635924
Validation loss: 2.5776961680001387

Epoch: 6| Step: 6
Training loss: 1.5150633095584536
Validation loss: 2.5249548876848587

Epoch: 6| Step: 7
Training loss: 2.8992892347981774
Validation loss: 2.5331755742482764

Epoch: 6| Step: 8
Training loss: 2.2723638876931753
Validation loss: 2.542411801554639

Epoch: 6| Step: 9
Training loss: 2.5364226248001143
Validation loss: 2.517646696142909

Epoch: 6| Step: 10
Training loss: 1.90225288440427
Validation loss: 2.5618564649018674

Epoch: 6| Step: 11
Training loss: 2.655295525894243
Validation loss: 2.5338079121601256

Epoch: 6| Step: 12
Training loss: 2.310277850627675
Validation loss: 2.5338952793510567

Epoch: 6| Step: 13
Training loss: 1.8091630142763093
Validation loss: 2.512647718253146

Epoch: 309| Step: 0
Training loss: 2.433841013095631
Validation loss: 2.5224158173388767

Epoch: 6| Step: 1
Training loss: 2.9596459785314364
Validation loss: 2.504346367219891

Epoch: 6| Step: 2
Training loss: 2.428909324332396
Validation loss: 2.548682530130161

Epoch: 6| Step: 3
Training loss: 2.5609016085443463
Validation loss: 2.5590404655018197

Epoch: 6| Step: 4
Training loss: 2.294690189435324
Validation loss: 2.5150300192824715

Epoch: 6| Step: 5
Training loss: 1.9177309964933817
Validation loss: 2.5159365257856483

Epoch: 6| Step: 6
Training loss: 1.7499840599424015
Validation loss: 2.5586334959373374

Epoch: 6| Step: 7
Training loss: 2.040527986378967
Validation loss: 2.5017046756576966

Epoch: 6| Step: 8
Training loss: 1.878581250924991
Validation loss: 2.5118492109297055

Epoch: 6| Step: 9
Training loss: 1.9732410130728095
Validation loss: 2.479646107107002

Epoch: 6| Step: 10
Training loss: 1.6591105679669158
Validation loss: 2.5259896221747127

Epoch: 6| Step: 11
Training loss: 2.2732065614422035
Validation loss: 2.5727004746327546

Epoch: 6| Step: 12
Training loss: 2.143633433918036
Validation loss: 2.5145759950727116

Epoch: 6| Step: 13
Training loss: 2.5306111224155057
Validation loss: 2.53418309869868

Epoch: 310| Step: 0
Training loss: 2.154938893524979
Validation loss: 2.5284279958356812

Epoch: 6| Step: 1
Training loss: 2.0770406547097577
Validation loss: 2.5282365065007366

Epoch: 6| Step: 2
Training loss: 2.377820598755449
Validation loss: 2.5266257741176794

Epoch: 6| Step: 3
Training loss: 2.098390475589139
Validation loss: 2.5119532037133996

Epoch: 6| Step: 4
Training loss: 2.0189831578391595
Validation loss: 2.49421211098024

Epoch: 6| Step: 5
Training loss: 2.560148136800064
Validation loss: 2.5309130749384354

Epoch: 6| Step: 6
Training loss: 1.7493785027035846
Validation loss: 2.5369662467624243

Epoch: 6| Step: 7
Training loss: 1.5206119254256103
Validation loss: 2.5254295190110745

Epoch: 6| Step: 8
Training loss: 2.069826119348473
Validation loss: 2.548399311486563

Epoch: 6| Step: 9
Training loss: 2.641851918877986
Validation loss: 2.509699982683708

Epoch: 6| Step: 10
Training loss: 2.3790649962427795
Validation loss: 2.546097604136813

Epoch: 6| Step: 11
Training loss: 2.763639782830455
Validation loss: 2.521828859525731

Epoch: 6| Step: 12
Training loss: 2.5587851416369474
Validation loss: 2.548660264117692

Epoch: 6| Step: 13
Training loss: 1.986023586037098
Validation loss: 2.508280062420677

Epoch: 311| Step: 0
Training loss: 2.4705724628863974
Validation loss: 2.532890090299997

Epoch: 6| Step: 1
Training loss: 2.631919098002014
Validation loss: 2.4738896821648324

Epoch: 6| Step: 2
Training loss: 2.514907260648608
Validation loss: 2.469508868451929

Epoch: 6| Step: 3
Training loss: 1.8066525680559635
Validation loss: 2.512211929007662

Epoch: 6| Step: 4
Training loss: 2.202272723765453
Validation loss: 2.520425802381799

Epoch: 6| Step: 5
Training loss: 2.0442947548278596
Validation loss: 2.513831667589817

Epoch: 6| Step: 6
Training loss: 1.5755345451662064
Validation loss: 2.5497047391311147

Epoch: 6| Step: 7
Training loss: 2.0918773418375793
Validation loss: 2.4966225287461423

Epoch: 6| Step: 8
Training loss: 2.3300750553337912
Validation loss: 2.5085645040189846

Epoch: 6| Step: 9
Training loss: 1.8658997632096492
Validation loss: 2.526302124709293

Epoch: 6| Step: 10
Training loss: 2.168404090405174
Validation loss: 2.505540926796549

Epoch: 6| Step: 11
Training loss: 2.508029441125297
Validation loss: 2.5133670473822787

Epoch: 6| Step: 12
Training loss: 2.0293367031326754
Validation loss: 2.539767976475654

Epoch: 6| Step: 13
Training loss: 2.579213877683888
Validation loss: 2.522957851255005

Epoch: 312| Step: 0
Training loss: 2.1618712861866523
Validation loss: 2.5226674524263664

Epoch: 6| Step: 1
Training loss: 2.0307003744829744
Validation loss: 2.4997884948025066

Epoch: 6| Step: 2
Training loss: 2.87039711733517
Validation loss: 2.502765262387893

Epoch: 6| Step: 3
Training loss: 2.447893042484215
Validation loss: 2.528078268839577

Epoch: 6| Step: 4
Training loss: 2.234603656726407
Validation loss: 2.4736731742584266

Epoch: 6| Step: 5
Training loss: 2.2139432176374467
Validation loss: 2.5107330713807667

Epoch: 6| Step: 6
Training loss: 2.0641021285775776
Validation loss: 2.5051547139757884

Epoch: 6| Step: 7
Training loss: 2.3426157940852166
Validation loss: 2.5061877327453335

Epoch: 6| Step: 8
Training loss: 2.5021261710746248
Validation loss: 2.5322834459008217

Epoch: 6| Step: 9
Training loss: 2.1106626748203707
Validation loss: 2.495836895522722

Epoch: 6| Step: 10
Training loss: 1.7856797800817474
Validation loss: 2.494384835898411

Epoch: 6| Step: 11
Training loss: 1.9650558448213633
Validation loss: 2.5659007534848626

Epoch: 6| Step: 12
Training loss: 1.6076334446740936
Validation loss: 2.543016091298883

Epoch: 6| Step: 13
Training loss: 2.545730520899705
Validation loss: 2.5650467761207327

Epoch: 313| Step: 0
Training loss: 2.699933588129541
Validation loss: 2.478943213297934

Epoch: 6| Step: 1
Training loss: 2.0366778113168995
Validation loss: 2.5053508582326938

Epoch: 6| Step: 2
Training loss: 2.248701356591776
Validation loss: 2.5290531604991084

Epoch: 6| Step: 3
Training loss: 1.7402445132455806
Validation loss: 2.51067244855014

Epoch: 6| Step: 4
Training loss: 1.6271965147046747
Validation loss: 2.510063233990358

Epoch: 6| Step: 5
Training loss: 2.1574922383368813
Validation loss: 2.5052179856509134

Epoch: 6| Step: 6
Training loss: 2.312263786005082
Validation loss: 2.5213925849703127

Epoch: 6| Step: 7
Training loss: 2.4812539127040347
Validation loss: 2.563334939524297

Epoch: 6| Step: 8
Training loss: 1.8975303586157155
Validation loss: 2.4949158233371533

Epoch: 6| Step: 9
Training loss: 2.8396865822223947
Validation loss: 2.538370898187247

Epoch: 6| Step: 10
Training loss: 2.1070357173070344
Validation loss: 2.542872641226539

Epoch: 6| Step: 11
Training loss: 2.3151395013356173
Validation loss: 2.4973359890851476

Epoch: 6| Step: 12
Training loss: 2.0828607277462754
Validation loss: 2.5306596937401293

Epoch: 6| Step: 13
Training loss: 2.3356938569961643
Validation loss: 2.494119963793755

Epoch: 314| Step: 0
Training loss: 2.117479817948437
Validation loss: 2.5263751355464246

Epoch: 6| Step: 1
Training loss: 2.1170059633022356
Validation loss: 2.5289247660442618

Epoch: 6| Step: 2
Training loss: 2.2777336925282485
Validation loss: 2.5220458005530864

Epoch: 6| Step: 3
Training loss: 2.605251503370258
Validation loss: 2.5376125924371205

Epoch: 6| Step: 4
Training loss: 2.36158820424636
Validation loss: 2.5509670010095693

Epoch: 6| Step: 5
Training loss: 1.7892374677764187
Validation loss: 2.5037349294014644

Epoch: 6| Step: 6
Training loss: 1.7859206284788236
Validation loss: 2.5159590722297214

Epoch: 6| Step: 7
Training loss: 1.6050329057641826
Validation loss: 2.5434180959155115

Epoch: 6| Step: 8
Training loss: 1.6076289213936676
Validation loss: 2.499487200822088

Epoch: 6| Step: 9
Training loss: 2.7124778043480484
Validation loss: 2.547120677906618

Epoch: 6| Step: 10
Training loss: 2.5551593123185343
Validation loss: 2.4913369506100436

Epoch: 6| Step: 11
Training loss: 2.4395385434029393
Validation loss: 2.532128683248908

Epoch: 6| Step: 12
Training loss: 2.1052949228525493
Validation loss: 2.4932746225006195

Epoch: 6| Step: 13
Training loss: 2.5082584353023476
Validation loss: 2.526358331728302

Epoch: 315| Step: 0
Training loss: 1.918402846054912
Validation loss: 2.516673589070897

Epoch: 6| Step: 1
Training loss: 2.310318820304383
Validation loss: 2.497371377097757

Epoch: 6| Step: 2
Training loss: 2.387734055156029
Validation loss: 2.5015845834930297

Epoch: 6| Step: 3
Training loss: 1.7491580436364353
Validation loss: 2.5012502722840906

Epoch: 6| Step: 4
Training loss: 2.067019279711187
Validation loss: 2.535709410438845

Epoch: 6| Step: 5
Training loss: 2.2972574758944586
Validation loss: 2.511829216926996

Epoch: 6| Step: 6
Training loss: 2.5592986788333296
Validation loss: 2.503261859319082

Epoch: 6| Step: 7
Training loss: 1.5751512666753424
Validation loss: 2.517842146236844

Epoch: 6| Step: 8
Training loss: 2.164659328307666
Validation loss: 2.5204247618413835

Epoch: 6| Step: 9
Training loss: 2.3369180413868658
Validation loss: 2.5223762578950115

Epoch: 6| Step: 10
Training loss: 2.732224321620087
Validation loss: 2.525384119934927

Epoch: 6| Step: 11
Training loss: 2.221668848702956
Validation loss: 2.5342064691303205

Epoch: 6| Step: 12
Training loss: 1.7605027021643571
Validation loss: 2.5234549678725045

Epoch: 6| Step: 13
Training loss: 2.845972052377289
Validation loss: 2.5167051266596845

Epoch: 316| Step: 0
Training loss: 1.836321851390581
Validation loss: 2.5206627150292022

Epoch: 6| Step: 1
Training loss: 1.971339868638979
Validation loss: 2.497353363899279

Epoch: 6| Step: 2
Training loss: 1.8274024774541904
Validation loss: 2.491775154037219

Epoch: 6| Step: 3
Training loss: 1.8970975189158978
Validation loss: 2.5466689501999498

Epoch: 6| Step: 4
Training loss: 2.0785792721884544
Validation loss: 2.517961820377717

Epoch: 6| Step: 5
Training loss: 2.18580343668372
Validation loss: 2.5212642589836998

Epoch: 6| Step: 6
Training loss: 3.063368070929166
Validation loss: 2.5117431970132946

Epoch: 6| Step: 7
Training loss: 2.0102217057590765
Validation loss: 2.51243780327784

Epoch: 6| Step: 8
Training loss: 2.474952535463137
Validation loss: 2.494432547361902

Epoch: 6| Step: 9
Training loss: 2.872005727820675
Validation loss: 2.5194856245738673

Epoch: 6| Step: 10
Training loss: 1.569875627924741
Validation loss: 2.4957332532743832

Epoch: 6| Step: 11
Training loss: 2.5952707451499357
Validation loss: 2.542667630239986

Epoch: 6| Step: 12
Training loss: 2.000727282849753
Validation loss: 2.5190073951569945

Epoch: 6| Step: 13
Training loss: 1.5091211836118967
Validation loss: 2.537607265346886

Epoch: 317| Step: 0
Training loss: 2.155830010685709
Validation loss: 2.5122160904869273

Epoch: 6| Step: 1
Training loss: 2.2301957319088634
Validation loss: 2.483314682595293

Epoch: 6| Step: 2
Training loss: 2.061288188629147
Validation loss: 2.508012125450461

Epoch: 6| Step: 3
Training loss: 1.938561548632938
Validation loss: 2.506589826400156

Epoch: 6| Step: 4
Training loss: 2.6968079771905566
Validation loss: 2.508290660253826

Epoch: 6| Step: 5
Training loss: 2.297213574841281
Validation loss: 2.522247267418295

Epoch: 6| Step: 6
Training loss: 1.836236742671633
Validation loss: 2.5574485577469104

Epoch: 6| Step: 7
Training loss: 2.181955668242939
Validation loss: 2.472106143533292

Epoch: 6| Step: 8
Training loss: 2.583607064378066
Validation loss: 2.5215870970942795

Epoch: 6| Step: 9
Training loss: 2.059792214997334
Validation loss: 2.5181005841071356

Epoch: 6| Step: 10
Training loss: 2.347015242570581
Validation loss: 2.5502645013757643

Epoch: 6| Step: 11
Training loss: 1.610487618143828
Validation loss: 2.543331846107914

Epoch: 6| Step: 12
Training loss: 2.493104006816834
Validation loss: 2.513769136918955

Epoch: 6| Step: 13
Training loss: 2.164945016211214
Validation loss: 2.496062968790926

Epoch: 318| Step: 0
Training loss: 2.7024781439900485
Validation loss: 2.504395607624954

Epoch: 6| Step: 1
Training loss: 2.3293830493555583
Validation loss: 2.4931105652590095

Epoch: 6| Step: 2
Training loss: 1.6837191435458145
Validation loss: 2.527699887578416

Epoch: 6| Step: 3
Training loss: 2.0995808728079948
Validation loss: 2.524968183283089

Epoch: 6| Step: 4
Training loss: 1.9337863402351811
Validation loss: 2.479253332233479

Epoch: 6| Step: 5
Training loss: 2.2094836088298977
Validation loss: 2.5304023501278263

Epoch: 6| Step: 6
Training loss: 2.308714577858025
Validation loss: 2.5385806390171757

Epoch: 6| Step: 7
Training loss: 1.9095035366116495
Validation loss: 2.5072629925407006

Epoch: 6| Step: 8
Training loss: 2.474431899404888
Validation loss: 2.529638813974686

Epoch: 6| Step: 9
Training loss: 1.3675312155136927
Validation loss: 2.5106640643069396

Epoch: 6| Step: 10
Training loss: 2.205951365718991
Validation loss: 2.4930972303587713

Epoch: 6| Step: 11
Training loss: 2.131327129321972
Validation loss: 2.543518109896048

Epoch: 6| Step: 12
Training loss: 2.2681327884022124
Validation loss: 2.5426509628306313

Epoch: 6| Step: 13
Training loss: 3.068446400341316
Validation loss: 2.5217009146115026

Epoch: 319| Step: 0
Training loss: 2.2382625026483645
Validation loss: 2.5400816502204

Epoch: 6| Step: 1
Training loss: 2.6050825619198608
Validation loss: 2.5239784389605457

Epoch: 6| Step: 2
Training loss: 1.7891980248972283
Validation loss: 2.529549273831147

Epoch: 6| Step: 3
Training loss: 2.076435520948399
Validation loss: 2.543145276340431

Epoch: 6| Step: 4
Training loss: 2.0118768426199964
Validation loss: 2.490412240767051

Epoch: 6| Step: 5
Training loss: 1.8916768072363441
Validation loss: 2.549365191873333

Epoch: 6| Step: 6
Training loss: 2.0196334842504386
Validation loss: 2.5124513089877945

Epoch: 6| Step: 7
Training loss: 2.59650247520771
Validation loss: 2.5193502330755

Epoch: 6| Step: 8
Training loss: 1.8591564915593906
Validation loss: 2.4911473388801793

Epoch: 6| Step: 9
Training loss: 2.7952833708284355
Validation loss: 2.525458740389788

Epoch: 6| Step: 10
Training loss: 2.4035922148163427
Validation loss: 2.5593937170349035

Epoch: 6| Step: 11
Training loss: 1.8928144403869347
Validation loss: 2.4893680691744158

Epoch: 6| Step: 12
Training loss: 2.009585655169405
Validation loss: 2.51081703838891

Epoch: 6| Step: 13
Training loss: 2.2680900055116435
Validation loss: 2.4857419577591133

Epoch: 320| Step: 0
Training loss: 2.409737375915938
Validation loss: 2.5101205083241025

Epoch: 6| Step: 1
Training loss: 2.6147716702618147
Validation loss: 2.550493401975963

Epoch: 6| Step: 2
Training loss: 1.906497001279428
Validation loss: 2.535337078795368

Epoch: 6| Step: 3
Training loss: 2.053263007276657
Validation loss: 2.539307066037971

Epoch: 6| Step: 4
Training loss: 2.393995813243395
Validation loss: 2.5210693741762342

Epoch: 6| Step: 5
Training loss: 2.656699108977087
Validation loss: 2.520469837292246

Epoch: 6| Step: 6
Training loss: 2.4912885521462447
Validation loss: 2.5289951299201046

Epoch: 6| Step: 7
Training loss: 1.9350372166960457
Validation loss: 2.4947680136674517

Epoch: 6| Step: 8
Training loss: 2.5236700086684447
Validation loss: 2.545426340789895

Epoch: 6| Step: 9
Training loss: 2.160462176715112
Validation loss: 2.5476828559106868

Epoch: 6| Step: 10
Training loss: 2.232200898642009
Validation loss: 2.5037086419974837

Epoch: 6| Step: 11
Training loss: 1.7454929897109661
Validation loss: 2.520505957285659

Epoch: 6| Step: 12
Training loss: 1.4296858323718076
Validation loss: 2.5346181487311505

Epoch: 6| Step: 13
Training loss: 1.6518730052137798
Validation loss: 2.4934442415401237

Epoch: 321| Step: 0
Training loss: 2.1741407911584414
Validation loss: 2.52386213599414

Epoch: 6| Step: 1
Training loss: 2.1626669449945184
Validation loss: 2.505579131340275

Epoch: 6| Step: 2
Training loss: 2.7209867725268104
Validation loss: 2.535171958487496

Epoch: 6| Step: 3
Training loss: 1.9259020784863308
Validation loss: 2.4678494685493906

Epoch: 6| Step: 4
Training loss: 2.2217408559573895
Validation loss: 2.51481913083533

Epoch: 6| Step: 5
Training loss: 1.7854392330326716
Validation loss: 2.5051628690086325

Epoch: 6| Step: 6
Training loss: 1.9382372038148357
Validation loss: 2.5002272123226303

Epoch: 6| Step: 7
Training loss: 2.0358440386500707
Validation loss: 2.504192631876109

Epoch: 6| Step: 8
Training loss: 2.417888190139964
Validation loss: 2.508563697696172

Epoch: 6| Step: 9
Training loss: 2.40840765586489
Validation loss: 2.5711404086874743

Epoch: 6| Step: 10
Training loss: 2.185303375342919
Validation loss: 2.5146888688402624

Epoch: 6| Step: 11
Training loss: 2.1844894401031376
Validation loss: 2.525395082502924

Epoch: 6| Step: 12
Training loss: 1.7035281202964114
Validation loss: 2.536833899285801

Epoch: 6| Step: 13
Training loss: 2.8337501518474952
Validation loss: 2.520609676317892

Epoch: 322| Step: 0
Training loss: 2.3352264036410273
Validation loss: 2.547655897965026

Epoch: 6| Step: 1
Training loss: 1.6184331547011641
Validation loss: 2.504725473512553

Epoch: 6| Step: 2
Training loss: 2.1552267549175204
Validation loss: 2.507814492898366

Epoch: 6| Step: 3
Training loss: 2.0330201871951417
Validation loss: 2.5011358736550116

Epoch: 6| Step: 4
Training loss: 2.4171781217939334
Validation loss: 2.5037518087735267

Epoch: 6| Step: 5
Training loss: 2.3076981874537754
Validation loss: 2.514753293189819

Epoch: 6| Step: 6
Training loss: 2.53838670393536
Validation loss: 2.5257056671315525

Epoch: 6| Step: 7
Training loss: 1.6017535584161515
Validation loss: 2.4861608056376268

Epoch: 6| Step: 8
Training loss: 2.5429353726448185
Validation loss: 2.545197659845612

Epoch: 6| Step: 9
Training loss: 1.2216746134237153
Validation loss: 2.5432698280597776

Epoch: 6| Step: 10
Training loss: 2.5725990953997973
Validation loss: 2.505238882789355

Epoch: 6| Step: 11
Training loss: 2.154391164782653
Validation loss: 2.505497573413596

Epoch: 6| Step: 12
Training loss: 2.637185831025345
Validation loss: 2.523927118465233

Epoch: 6| Step: 13
Training loss: 2.1462686584763593
Validation loss: 2.521150001793096

Epoch: 323| Step: 0
Training loss: 1.2990115846406605
Validation loss: 2.5093918151173575

Epoch: 6| Step: 1
Training loss: 2.336764900366522
Validation loss: 2.495907201014023

Epoch: 6| Step: 2
Training loss: 2.6671345916130766
Validation loss: 2.519269114355149

Epoch: 6| Step: 3
Training loss: 2.165190144705987
Validation loss: 2.4938844780894462

Epoch: 6| Step: 4
Training loss: 2.553529996278589
Validation loss: 2.4759667853916825

Epoch: 6| Step: 5
Training loss: 2.019764161836648
Validation loss: 2.5538282592353916

Epoch: 6| Step: 6
Training loss: 1.9380543438759101
Validation loss: 2.5041390211416528

Epoch: 6| Step: 7
Training loss: 2.01556147985395
Validation loss: 2.4836693022844716

Epoch: 6| Step: 8
Training loss: 2.4197498472828327
Validation loss: 2.5232098417439652

Epoch: 6| Step: 9
Training loss: 2.537192257414961
Validation loss: 2.489777038002355

Epoch: 6| Step: 10
Training loss: 2.654694674046047
Validation loss: 2.5105682653717265

Epoch: 6| Step: 11
Training loss: 1.723233980282295
Validation loss: 2.471244607733705

Epoch: 6| Step: 12
Training loss: 1.8354232392498318
Validation loss: 2.548006748570444

Epoch: 6| Step: 13
Training loss: 1.85090876760342
Validation loss: 2.5382953655513605

Epoch: 324| Step: 0
Training loss: 1.9317866348301886
Validation loss: 2.5155885714537436

Epoch: 6| Step: 1
Training loss: 1.6948960315697275
Validation loss: 2.505581193544382

Epoch: 6| Step: 2
Training loss: 1.893807499078746
Validation loss: 2.503420131043685

Epoch: 6| Step: 3
Training loss: 1.7269596661450497
Validation loss: 2.5024842040621347

Epoch: 6| Step: 4
Training loss: 2.081157107184644
Validation loss: 2.521943055464518

Epoch: 6| Step: 5
Training loss: 2.22644085468142
Validation loss: 2.507796232697024

Epoch: 6| Step: 6
Training loss: 2.409902500538823
Validation loss: 2.4971607780317

Epoch: 6| Step: 7
Training loss: 2.356184058442961
Validation loss: 2.4926439350074414

Epoch: 6| Step: 8
Training loss: 2.8273034351401813
Validation loss: 2.49818449959299

Epoch: 6| Step: 9
Training loss: 2.2616090105541837
Validation loss: 2.5145611213667993

Epoch: 6| Step: 10
Training loss: 2.466733084463204
Validation loss: 2.5457123931917613

Epoch: 6| Step: 11
Training loss: 2.327146593382085
Validation loss: 2.512872268915952

Epoch: 6| Step: 12
Training loss: 2.061527485303641
Validation loss: 2.508492971242186

Epoch: 6| Step: 13
Training loss: 2.148895769661625
Validation loss: 2.5213428905389197

Epoch: 325| Step: 0
Training loss: 2.074956787762469
Validation loss: 2.50983491213562

Epoch: 6| Step: 1
Training loss: 1.7916411760276374
Validation loss: 2.5638723452790124

Epoch: 6| Step: 2
Training loss: 2.113269968055606
Validation loss: 2.547605273814961

Epoch: 6| Step: 3
Training loss: 2.442407997608554
Validation loss: 2.5039079639351582

Epoch: 6| Step: 4
Training loss: 1.8495806296027246
Validation loss: 2.541999936423422

Epoch: 6| Step: 5
Training loss: 2.5878155391524635
Validation loss: 2.478984208362923

Epoch: 6| Step: 6
Training loss: 2.2415265428314735
Validation loss: 2.485230849289002

Epoch: 6| Step: 7
Training loss: 2.1971848943502543
Validation loss: 2.5249888266039324

Epoch: 6| Step: 8
Training loss: 2.244707240193528
Validation loss: 2.519814050194148

Epoch: 6| Step: 9
Training loss: 2.175592328675301
Validation loss: 2.5077903045560435

Epoch: 6| Step: 10
Training loss: 2.3208245633939533
Validation loss: 2.4963580060061616

Epoch: 6| Step: 11
Training loss: 1.935318980576481
Validation loss: 2.504500040922538

Epoch: 6| Step: 12
Training loss: 2.168376052674108
Validation loss: 2.514746740219585

Epoch: 6| Step: 13
Training loss: 1.8788463559190758
Validation loss: 2.4674155804802664

Epoch: 326| Step: 0
Training loss: 1.95810704919189
Validation loss: 2.529641295891394

Epoch: 6| Step: 1
Training loss: 2.7023371612300875
Validation loss: 2.5007061176454544

Epoch: 6| Step: 2
Training loss: 2.627967065880588
Validation loss: 2.5341927617134306

Epoch: 6| Step: 3
Training loss: 1.6507125269697769
Validation loss: 2.540854185154663

Epoch: 6| Step: 4
Training loss: 2.2041925319411164
Validation loss: 2.501319714841114

Epoch: 6| Step: 5
Training loss: 2.957072853591909
Validation loss: 2.5248295827510265

Epoch: 6| Step: 6
Training loss: 1.5691472375078495
Validation loss: 2.5477575175665717

Epoch: 6| Step: 7
Training loss: 2.406879293219522
Validation loss: 2.550342091538545

Epoch: 6| Step: 8
Training loss: 1.9590076205546922
Validation loss: 2.5104091182760864

Epoch: 6| Step: 9
Training loss: 1.895401776279681
Validation loss: 2.52250523364523

Epoch: 6| Step: 10
Training loss: 2.058121860570103
Validation loss: 2.5509090297670203

Epoch: 6| Step: 11
Training loss: 1.615622385048733
Validation loss: 2.5260035821606093

Epoch: 6| Step: 12
Training loss: 2.3395182744596723
Validation loss: 2.495677516270436

Epoch: 6| Step: 13
Training loss: 1.7334110227316015
Validation loss: 2.5368268707825457

Epoch: 327| Step: 0
Training loss: 2.177019133526221
Validation loss: 2.508348043538523

Epoch: 6| Step: 1
Training loss: 2.7392861866288922
Validation loss: 2.4973685146015843

Epoch: 6| Step: 2
Training loss: 2.125537692090937
Validation loss: 2.489339789719647

Epoch: 6| Step: 3
Training loss: 1.7677018095970225
Validation loss: 2.503595154407968

Epoch: 6| Step: 4
Training loss: 2.7057725586531025
Validation loss: 2.5360403299960743

Epoch: 6| Step: 5
Training loss: 2.6629838745568666
Validation loss: 2.521292286112338

Epoch: 6| Step: 6
Training loss: 1.7260936255760144
Validation loss: 2.521582360389784

Epoch: 6| Step: 7
Training loss: 1.560794662645561
Validation loss: 2.514723529437193

Epoch: 6| Step: 8
Training loss: 2.0872127906442492
Validation loss: 2.511261284041776

Epoch: 6| Step: 9
Training loss: 1.9997660976963294
Validation loss: 2.5128685665953703

Epoch: 6| Step: 10
Training loss: 1.946986937022067
Validation loss: 2.514974726244128

Epoch: 6| Step: 11
Training loss: 2.324273629702665
Validation loss: 2.507034479060922

Epoch: 6| Step: 12
Training loss: 2.1190686191683694
Validation loss: 2.5132318832184772

Epoch: 6| Step: 13
Training loss: 2.149686914758038
Validation loss: 2.4955756158244395

Epoch: 328| Step: 0
Training loss: 2.112345997992368
Validation loss: 2.5443946693290016

Epoch: 6| Step: 1
Training loss: 2.046380252595815
Validation loss: 2.5539409290048996

Epoch: 6| Step: 2
Training loss: 1.905667700709797
Validation loss: 2.511197377531202

Epoch: 6| Step: 3
Training loss: 2.3408791825487363
Validation loss: 2.5093902387598037

Epoch: 6| Step: 4
Training loss: 2.499454152598844
Validation loss: 2.5281827880812804

Epoch: 6| Step: 5
Training loss: 1.9215884111301127
Validation loss: 2.493347881236969

Epoch: 6| Step: 6
Training loss: 2.154470179098451
Validation loss: 2.5246057158470245

Epoch: 6| Step: 7
Training loss: 2.086445835766157
Validation loss: 2.4920461843926986

Epoch: 6| Step: 8
Training loss: 2.352049514171472
Validation loss: 2.4848939664838254

Epoch: 6| Step: 9
Training loss: 2.282617446535131
Validation loss: 2.5031602502112418

Epoch: 6| Step: 10
Training loss: 2.136539542373482
Validation loss: 2.483876364099729

Epoch: 6| Step: 11
Training loss: 2.32713337716093
Validation loss: 2.490329575155557

Epoch: 6| Step: 12
Training loss: 2.2638663997456248
Validation loss: 2.503991990368943

Epoch: 6| Step: 13
Training loss: 1.8924588853812059
Validation loss: 2.4987541581037673

Epoch: 329| Step: 0
Training loss: 2.1571299650877083
Validation loss: 2.5131323797552554

Epoch: 6| Step: 1
Training loss: 2.0227883965800446
Validation loss: 2.517802329618521

Epoch: 6| Step: 2
Training loss: 1.5367574713075276
Validation loss: 2.5256318386053804

Epoch: 6| Step: 3
Training loss: 2.3297102478676988
Validation loss: 2.4757472594325614

Epoch: 6| Step: 4
Training loss: 2.4071382146566593
Validation loss: 2.47436059933672

Epoch: 6| Step: 5
Training loss: 2.266302073364898
Validation loss: 2.495530883792438

Epoch: 6| Step: 6
Training loss: 2.719271642692975
Validation loss: 2.52189640019262

Epoch: 6| Step: 7
Training loss: 1.1588472630441604
Validation loss: 2.4945923653290367

Epoch: 6| Step: 8
Training loss: 2.1696303589834844
Validation loss: 2.516388263908025

Epoch: 6| Step: 9
Training loss: 2.45522009925359
Validation loss: 2.5075388511730052

Epoch: 6| Step: 10
Training loss: 2.770288361230671
Validation loss: 2.543668085699102

Epoch: 6| Step: 11
Training loss: 2.368407888543203
Validation loss: 2.542559093005165

Epoch: 6| Step: 12
Training loss: 1.776084772595554
Validation loss: 2.494728753639819

Epoch: 6| Step: 13
Training loss: 2.311928446477546
Validation loss: 2.5172648215392606

Epoch: 330| Step: 0
Training loss: 2.6005290960273197
Validation loss: 2.5284944821904802

Epoch: 6| Step: 1
Training loss: 2.1117167719451753
Validation loss: 2.532857407060335

Epoch: 6| Step: 2
Training loss: 2.3911020638169846
Validation loss: 2.4930708400436674

Epoch: 6| Step: 3
Training loss: 1.957570319636102
Validation loss: 2.5508631635750816

Epoch: 6| Step: 4
Training loss: 2.4334690313083236
Validation loss: 2.4637170499520775

Epoch: 6| Step: 5
Training loss: 2.2884178946537035
Validation loss: 2.512273001442699

Epoch: 6| Step: 6
Training loss: 1.996588658680765
Validation loss: 2.469522454259673

Epoch: 6| Step: 7
Training loss: 1.9910304879690357
Validation loss: 2.5291987333690686

Epoch: 6| Step: 8
Training loss: 1.945060866906344
Validation loss: 2.512311623011654

Epoch: 6| Step: 9
Training loss: 2.2259795914954275
Validation loss: 2.507615051582736

Epoch: 6| Step: 10
Training loss: 2.0800558885988676
Validation loss: 2.4831623727830623

Epoch: 6| Step: 11
Training loss: 2.3491703679259843
Validation loss: 2.470541548407744

Epoch: 6| Step: 12
Training loss: 2.1275352613169476
Validation loss: 2.5152354095744642

Epoch: 6| Step: 13
Training loss: 1.6786904147914732
Validation loss: 2.5412172262737642

Epoch: 331| Step: 0
Training loss: 2.1929432311983796
Validation loss: 2.5392845118589555

Epoch: 6| Step: 1
Training loss: 2.8209294624421175
Validation loss: 2.4908167163898125

Epoch: 6| Step: 2
Training loss: 2.202193259313811
Validation loss: 2.4994222404070543

Epoch: 6| Step: 3
Training loss: 2.5693533826849793
Validation loss: 2.502755076496815

Epoch: 6| Step: 4
Training loss: 1.8431107657559762
Validation loss: 2.4820655086812344

Epoch: 6| Step: 5
Training loss: 2.0852107807345406
Validation loss: 2.47978659621001

Epoch: 6| Step: 6
Training loss: 2.0233592603720187
Validation loss: 2.513746989990478

Epoch: 6| Step: 7
Training loss: 2.712845803996646
Validation loss: 2.51648123993245

Epoch: 6| Step: 8
Training loss: 1.6689746691162612
Validation loss: 2.4950711402924903

Epoch: 6| Step: 9
Training loss: 2.215009195381866
Validation loss: 2.520131231428155

Epoch: 6| Step: 10
Training loss: 1.8176006369033892
Validation loss: 2.4935703453535485

Epoch: 6| Step: 11
Training loss: 2.051454735022489
Validation loss: 2.5367298400776557

Epoch: 6| Step: 12
Training loss: 1.7117062587795884
Validation loss: 2.5145489544080797

Epoch: 6| Step: 13
Training loss: 2.2630060251773556
Validation loss: 2.518412395099038

Epoch: 332| Step: 0
Training loss: 1.8138149358691562
Validation loss: 2.4970866024510996

Epoch: 6| Step: 1
Training loss: 2.340084821353784
Validation loss: 2.5172194188097197

Epoch: 6| Step: 2
Training loss: 1.9993381597255664
Validation loss: 2.52159270711398

Epoch: 6| Step: 3
Training loss: 1.7747032589085945
Validation loss: 2.5028574723164154

Epoch: 6| Step: 4
Training loss: 2.155661903567597
Validation loss: 2.509546094150943

Epoch: 6| Step: 5
Training loss: 1.6244510310228382
Validation loss: 2.501094869596924

Epoch: 6| Step: 6
Training loss: 2.0968418351803972
Validation loss: 2.506842094927807

Epoch: 6| Step: 7
Training loss: 2.3219651010931623
Validation loss: 2.511637150042392

Epoch: 6| Step: 8
Training loss: 1.9548782414133163
Validation loss: 2.482913553736501

Epoch: 6| Step: 9
Training loss: 2.8189853070956388
Validation loss: 2.5456204384780676

Epoch: 6| Step: 10
Training loss: 2.1843435129671342
Validation loss: 2.5221594957257287

Epoch: 6| Step: 11
Training loss: 2.8990344643519657
Validation loss: 2.533974758295681

Epoch: 6| Step: 12
Training loss: 1.929464103339116
Validation loss: 2.5369154487112118

Epoch: 6| Step: 13
Training loss: 1.9733954829124774
Validation loss: 2.514165346929341

Epoch: 333| Step: 0
Training loss: 2.072226506905761
Validation loss: 2.5364568125412448

Epoch: 6| Step: 1
Training loss: 2.777680238494744
Validation loss: 2.516414444325129

Epoch: 6| Step: 2
Training loss: 1.6874735794824807
Validation loss: 2.4796596756269422

Epoch: 6| Step: 3
Training loss: 1.9836447265613366
Validation loss: 2.5168204942302363

Epoch: 6| Step: 4
Training loss: 2.033867308850552
Validation loss: 2.537676467146347

Epoch: 6| Step: 5
Training loss: 2.1719584277479242
Validation loss: 2.5186367310617124

Epoch: 6| Step: 6
Training loss: 1.9563710818128084
Validation loss: 2.5420286451039416

Epoch: 6| Step: 7
Training loss: 2.2851536416585008
Validation loss: 2.513447455951994

Epoch: 6| Step: 8
Training loss: 2.676377813288492
Validation loss: 2.4998735390744433

Epoch: 6| Step: 9
Training loss: 2.035957164186382
Validation loss: 2.5287458905381537

Epoch: 6| Step: 10
Training loss: 2.230073643310876
Validation loss: 2.506711443971528

Epoch: 6| Step: 11
Training loss: 1.894991160148193
Validation loss: 2.4807787619357087

Epoch: 6| Step: 12
Training loss: 2.011736764179054
Validation loss: 2.5109638936222782

Epoch: 6| Step: 13
Training loss: 2.553935742375693
Validation loss: 2.452179451086848

Epoch: 334| Step: 0
Training loss: 1.9650236923303692
Validation loss: 2.514882523285826

Epoch: 6| Step: 1
Training loss: 2.046638882828459
Validation loss: 2.502002420702109

Epoch: 6| Step: 2
Training loss: 2.698417489442036
Validation loss: 2.50942740089577

Epoch: 6| Step: 3
Training loss: 1.5003955637386988
Validation loss: 2.4964073752236198

Epoch: 6| Step: 4
Training loss: 2.756703096892464
Validation loss: 2.4908795062578655

Epoch: 6| Step: 5
Training loss: 2.185589637372094
Validation loss: 2.5475820101973907

Epoch: 6| Step: 6
Training loss: 1.8857818868005285
Validation loss: 2.5179871689112456

Epoch: 6| Step: 7
Training loss: 2.4237449072262485
Validation loss: 2.504059870856656

Epoch: 6| Step: 8
Training loss: 2.246660827700822
Validation loss: 2.5167819072196584

Epoch: 6| Step: 9
Training loss: 1.4979341904734191
Validation loss: 2.4889714168686745

Epoch: 6| Step: 10
Training loss: 2.4159698358635886
Validation loss: 2.4853649822813804

Epoch: 6| Step: 11
Training loss: 2.122464293953532
Validation loss: 2.491669226423658

Epoch: 6| Step: 12
Training loss: 2.251286668579445
Validation loss: 2.5252400723041837

Epoch: 6| Step: 13
Training loss: 1.8343137662632931
Validation loss: 2.482108340740932

Epoch: 335| Step: 0
Training loss: 1.439302226921484
Validation loss: 2.4885797202958377

Epoch: 6| Step: 1
Training loss: 2.252873810669234
Validation loss: 2.535768158837769

Epoch: 6| Step: 2
Training loss: 1.93498417347589
Validation loss: 2.4757858626370783

Epoch: 6| Step: 3
Training loss: 2.165297503568778
Validation loss: 2.4754188179093912

Epoch: 6| Step: 4
Training loss: 2.1167522137793537
Validation loss: 2.5007281688839234

Epoch: 6| Step: 5
Training loss: 1.9190071231894708
Validation loss: 2.528166651849433

Epoch: 6| Step: 6
Training loss: 2.0959456588747845
Validation loss: 2.5202943318351485

Epoch: 6| Step: 7
Training loss: 2.410066031030225
Validation loss: 2.499733059485584

Epoch: 6| Step: 8
Training loss: 2.6593431302770996
Validation loss: 2.513671391577406

Epoch: 6| Step: 9
Training loss: 2.1752000782460477
Validation loss: 2.492023071826963

Epoch: 6| Step: 10
Training loss: 1.955397969396379
Validation loss: 2.5172383607312803

Epoch: 6| Step: 11
Training loss: 2.6837541068640434
Validation loss: 2.5054380983360773

Epoch: 6| Step: 12
Training loss: 2.0770083991346024
Validation loss: 2.469532405580251

Epoch: 6| Step: 13
Training loss: 2.147326816952014
Validation loss: 2.5117209628156068

Epoch: 336| Step: 0
Training loss: 2.4246101931285877
Validation loss: 2.517676584186136

Epoch: 6| Step: 1
Training loss: 1.8392347230866608
Validation loss: 2.5395194208730048

Epoch: 6| Step: 2
Training loss: 2.2930163714842986
Validation loss: 2.486606412292438

Epoch: 6| Step: 3
Training loss: 2.4721813258051912
Validation loss: 2.5255517032581163

Epoch: 6| Step: 4
Training loss: 1.4998389793436961
Validation loss: 2.491936274600492

Epoch: 6| Step: 5
Training loss: 1.9669123353375346
Validation loss: 2.4753722856873024

Epoch: 6| Step: 6
Training loss: 2.8335246601206276
Validation loss: 2.5078147101287924

Epoch: 6| Step: 7
Training loss: 2.1596997073500135
Validation loss: 2.516461306163743

Epoch: 6| Step: 8
Training loss: 2.444302969027066
Validation loss: 2.510667074509766

Epoch: 6| Step: 9
Training loss: 2.1939034606642007
Validation loss: 2.5135109248147702

Epoch: 6| Step: 10
Training loss: 2.7794711821658153
Validation loss: 2.499729013625386

Epoch: 6| Step: 11
Training loss: 1.7460534690631306
Validation loss: 2.512256557911135

Epoch: 6| Step: 12
Training loss: 1.4536888095248393
Validation loss: 2.514377179296342

Epoch: 6| Step: 13
Training loss: 1.55907430864925
Validation loss: 2.478787222951421

Epoch: 337| Step: 0
Training loss: 2.399520182329733
Validation loss: 2.554751890064138

Epoch: 6| Step: 1
Training loss: 2.4546043649019693
Validation loss: 2.4996339119750246

Epoch: 6| Step: 2
Training loss: 2.446242378078744
Validation loss: 2.498803239908431

Epoch: 6| Step: 3
Training loss: 2.033466245132378
Validation loss: 2.50061663949516

Epoch: 6| Step: 4
Training loss: 1.9681606318814995
Validation loss: 2.5269972710206776

Epoch: 6| Step: 5
Training loss: 2.0443906194171264
Validation loss: 2.5109268695989577

Epoch: 6| Step: 6
Training loss: 2.6088737874439722
Validation loss: 2.464945404518771

Epoch: 6| Step: 7
Training loss: 1.6689510267236876
Validation loss: 2.546069328490206

Epoch: 6| Step: 8
Training loss: 2.1699592319297856
Validation loss: 2.4893955306273714

Epoch: 6| Step: 9
Training loss: 2.328249319968774
Validation loss: 2.5247775484740647

Epoch: 6| Step: 10
Training loss: 1.9137295803224008
Validation loss: 2.512451487553283

Epoch: 6| Step: 11
Training loss: 1.6249904632288512
Validation loss: 2.5322080202215

Epoch: 6| Step: 12
Training loss: 2.2296403993525065
Validation loss: 2.509867863507682

Epoch: 6| Step: 13
Training loss: 2.223885194765474
Validation loss: 2.495489200016104

Epoch: 338| Step: 0
Training loss: 1.8564624574883928
Validation loss: 2.5510607175946802

Epoch: 6| Step: 1
Training loss: 2.1097611815545805
Validation loss: 2.5542506049810347

Epoch: 6| Step: 2
Training loss: 2.402480306078231
Validation loss: 2.4671172597953626

Epoch: 6| Step: 3
Training loss: 1.8243460437218737
Validation loss: 2.4982361785650857

Epoch: 6| Step: 4
Training loss: 2.044361580618348
Validation loss: 2.4811008006284987

Epoch: 6| Step: 5
Training loss: 2.473291396876297
Validation loss: 2.5140907359380917

Epoch: 6| Step: 6
Training loss: 2.562792040302841
Validation loss: 2.491238943594014

Epoch: 6| Step: 7
Training loss: 1.2461397646835648
Validation loss: 2.556586775398319

Epoch: 6| Step: 8
Training loss: 1.9203590679186924
Validation loss: 2.505943974368305

Epoch: 6| Step: 9
Training loss: 2.762241945253426
Validation loss: 2.4628757378999357

Epoch: 6| Step: 10
Training loss: 2.1140322664048625
Validation loss: 2.49036281137861

Epoch: 6| Step: 11
Training loss: 2.7908136763231166
Validation loss: 2.541414315579867

Epoch: 6| Step: 12
Training loss: 1.9116782773016443
Validation loss: 2.506363819929611

Epoch: 6| Step: 13
Training loss: 1.7169259536008872
Validation loss: 2.488157707433136

Epoch: 339| Step: 0
Training loss: 2.2268798601952384
Validation loss: 2.5243586012813197

Epoch: 6| Step: 1
Training loss: 2.7476166887990208
Validation loss: 2.5435614567678106

Epoch: 6| Step: 2
Training loss: 1.9698131657608446
Validation loss: 2.5065119982446613

Epoch: 6| Step: 3
Training loss: 2.5021964914449297
Validation loss: 2.536164491216394

Epoch: 6| Step: 4
Training loss: 1.6056471676779254
Validation loss: 2.5027430303774363

Epoch: 6| Step: 5
Training loss: 1.7482749065652636
Validation loss: 2.499317661607838

Epoch: 6| Step: 6
Training loss: 1.8248547378628626
Validation loss: 2.4821561384204585

Epoch: 6| Step: 7
Training loss: 2.3831122538147937
Validation loss: 2.5283795458983978

Epoch: 6| Step: 8
Training loss: 2.0830462703025083
Validation loss: 2.4914764718019082

Epoch: 6| Step: 9
Training loss: 2.801819761754128
Validation loss: 2.483429654401968

Epoch: 6| Step: 10
Training loss: 1.6627064225390151
Validation loss: 2.5087192636355415

Epoch: 6| Step: 11
Training loss: 1.9618634473272758
Validation loss: 2.5167933167397236

Epoch: 6| Step: 12
Training loss: 2.1695293686130213
Validation loss: 2.5039030611969673

Epoch: 6| Step: 13
Training loss: 2.5064406400934396
Validation loss: 2.4973386442557004

Epoch: 340| Step: 0
Training loss: 2.1556780512678078
Validation loss: 2.4900942673860578

Epoch: 6| Step: 1
Training loss: 1.944716212793057
Validation loss: 2.523602235090806

Epoch: 6| Step: 2
Training loss: 2.0065317781975205
Validation loss: 2.546766699451101

Epoch: 6| Step: 3
Training loss: 2.4223502400406107
Validation loss: 2.504234539321088

Epoch: 6| Step: 4
Training loss: 2.1413903643225187
Validation loss: 2.5345655143267742

Epoch: 6| Step: 5
Training loss: 1.7181898418026325
Validation loss: 2.532337918605392

Epoch: 6| Step: 6
Training loss: 1.8620350506983743
Validation loss: 2.518149273555379

Epoch: 6| Step: 7
Training loss: 2.17918844218221
Validation loss: 2.490458164635187

Epoch: 6| Step: 8
Training loss: 1.5965913613996603
Validation loss: 2.5494474174215878

Epoch: 6| Step: 9
Training loss: 2.417220830410153
Validation loss: 2.50362462755739

Epoch: 6| Step: 10
Training loss: 2.281194921377309
Validation loss: 2.5226707877304295

Epoch: 6| Step: 11
Training loss: 1.988741658383276
Validation loss: 2.557335592407716

Epoch: 6| Step: 12
Training loss: 2.51285556915229
Validation loss: 2.4868921385909477

Epoch: 6| Step: 13
Training loss: 2.7950076044078145
Validation loss: 2.504916189267561

Epoch: 341| Step: 0
Training loss: 2.1687421639160713
Validation loss: 2.5045592644656605

Epoch: 6| Step: 1
Training loss: 2.547896104440092
Validation loss: 2.4978030693526767

Epoch: 6| Step: 2
Training loss: 2.5507606867710577
Validation loss: 2.4944561658584576

Epoch: 6| Step: 3
Training loss: 1.695833539220749
Validation loss: 2.4737004088753918

Epoch: 6| Step: 4
Training loss: 2.250656244155893
Validation loss: 2.475495805090837

Epoch: 6| Step: 5
Training loss: 1.7522021470700169
Validation loss: 2.4882935144909677

Epoch: 6| Step: 6
Training loss: 1.647438940094695
Validation loss: 2.4829767912577085

Epoch: 6| Step: 7
Training loss: 2.159329413058397
Validation loss: 2.503530297512134

Epoch: 6| Step: 8
Training loss: 2.119997194216329
Validation loss: 2.5107790478148746

Epoch: 6| Step: 9
Training loss: 2.4415619091002463
Validation loss: 2.4868910030970675

Epoch: 6| Step: 10
Training loss: 2.1601786551943287
Validation loss: 2.4952887894997393

Epoch: 6| Step: 11
Training loss: 1.885251313716328
Validation loss: 2.531778695350988

Epoch: 6| Step: 12
Training loss: 2.1137509767066507
Validation loss: 2.5053184982561363

Epoch: 6| Step: 13
Training loss: 2.364123889355461
Validation loss: 2.4823590230142702

Epoch: 342| Step: 0
Training loss: 1.4511779538588192
Validation loss: 2.4463103655670526

Epoch: 6| Step: 1
Training loss: 2.0589180099222095
Validation loss: 2.514607310211167

Epoch: 6| Step: 2
Training loss: 1.7658747943633137
Validation loss: 2.5361296920153507

Epoch: 6| Step: 3
Training loss: 2.8889518225572126
Validation loss: 2.5534764545584414

Epoch: 6| Step: 4
Training loss: 2.0298923356768697
Validation loss: 2.512981938379867

Epoch: 6| Step: 5
Training loss: 1.5008582203164478
Validation loss: 2.4930458932062205

Epoch: 6| Step: 6
Training loss: 2.0929312172571937
Validation loss: 2.529822423632777

Epoch: 6| Step: 7
Training loss: 2.6186323590132203
Validation loss: 2.5235108370257975

Epoch: 6| Step: 8
Training loss: 2.186703891301619
Validation loss: 2.497802102523075

Epoch: 6| Step: 9
Training loss: 1.806484896159863
Validation loss: 2.4704641183515164

Epoch: 6| Step: 10
Training loss: 2.721696857005406
Validation loss: 2.499843972987494

Epoch: 6| Step: 11
Training loss: 2.344699718062699
Validation loss: 2.541424470602036

Epoch: 6| Step: 12
Training loss: 2.0377549696454422
Validation loss: 2.5275894215288024

Epoch: 6| Step: 13
Training loss: 1.8476912190715027
Validation loss: 2.505372127751843

Epoch: 343| Step: 0
Training loss: 1.8878798298415627
Validation loss: 2.5108601814857243

Epoch: 6| Step: 1
Training loss: 2.1602972995174174
Validation loss: 2.534535198323411

Epoch: 6| Step: 2
Training loss: 1.6065560253452857
Validation loss: 2.4897264612874705

Epoch: 6| Step: 3
Training loss: 2.723562519401496
Validation loss: 2.519411854428161

Epoch: 6| Step: 4
Training loss: 2.183010507940147
Validation loss: 2.523903137868149

Epoch: 6| Step: 5
Training loss: 2.057296199230239
Validation loss: 2.5096147100748243

Epoch: 6| Step: 6
Training loss: 1.7931083674704873
Validation loss: 2.4799237135084393

Epoch: 6| Step: 7
Training loss: 2.0634063116591044
Validation loss: 2.4975690671989286

Epoch: 6| Step: 8
Training loss: 1.3985893550663537
Validation loss: 2.521789488093823

Epoch: 6| Step: 9
Training loss: 2.3657258308318485
Validation loss: 2.4799958532515705

Epoch: 6| Step: 10
Training loss: 2.182941482654337
Validation loss: 2.525647189148131

Epoch: 6| Step: 11
Training loss: 2.5215510352887316
Validation loss: 2.5533472239348005

Epoch: 6| Step: 12
Training loss: 1.9516566135507
Validation loss: 2.5110531000510137

Epoch: 6| Step: 13
Training loss: 2.8132722430202857
Validation loss: 2.5240047148411895

Epoch: 344| Step: 0
Training loss: 2.538986064053813
Validation loss: 2.5038134162360226

Epoch: 6| Step: 1
Training loss: 2.0462654895231167
Validation loss: 2.4322926946698757

Epoch: 6| Step: 2
Training loss: 2.232279401690002
Validation loss: 2.5157125242896656

Epoch: 6| Step: 3
Training loss: 2.3614747259858038
Validation loss: 2.48752040573311

Epoch: 6| Step: 4
Training loss: 1.602568817327362
Validation loss: 2.508537350521344

Epoch: 6| Step: 5
Training loss: 2.4729151296463527
Validation loss: 2.495160702234287

Epoch: 6| Step: 6
Training loss: 2.299911049491108
Validation loss: 2.5274670771406926

Epoch: 6| Step: 7
Training loss: 2.119988759568843
Validation loss: 2.52531081903754

Epoch: 6| Step: 8
Training loss: 1.120014801268186
Validation loss: 2.4811630480104014

Epoch: 6| Step: 9
Training loss: 2.646918207061383
Validation loss: 2.541100252469728

Epoch: 6| Step: 10
Training loss: 2.268453056302078
Validation loss: 2.5396717167969394

Epoch: 6| Step: 11
Training loss: 1.3615444892565853
Validation loss: 2.518038332525417

Epoch: 6| Step: 12
Training loss: 1.9377008149459471
Validation loss: 2.4888178741481495

Epoch: 6| Step: 13
Training loss: 1.939510409919985
Validation loss: 2.512006366006006

Epoch: 345| Step: 0
Training loss: 1.9381737306749156
Validation loss: 2.5091293915803865

Epoch: 6| Step: 1
Training loss: 2.2556477765344187
Validation loss: 2.4808238788040313

Epoch: 6| Step: 2
Training loss: 1.8487519126608796
Validation loss: 2.510777737802432

Epoch: 6| Step: 3
Training loss: 1.8374803918324278
Validation loss: 2.526723364306083

Epoch: 6| Step: 4
Training loss: 2.0825998413771667
Validation loss: 2.523445063601606

Epoch: 6| Step: 5
Training loss: 1.4118060660790024
Validation loss: 2.5076572238653347

Epoch: 6| Step: 6
Training loss: 2.5028629122908326
Validation loss: 2.524851039495705

Epoch: 6| Step: 7
Training loss: 2.2215916878176265
Validation loss: 2.518000342472978

Epoch: 6| Step: 8
Training loss: 2.7841848254950836
Validation loss: 2.522813751464724

Epoch: 6| Step: 9
Training loss: 1.9806918107016833
Validation loss: 2.466233360827696

Epoch: 6| Step: 10
Training loss: 2.163425613496299
Validation loss: 2.512804632606296

Epoch: 6| Step: 11
Training loss: 2.3676417956643916
Validation loss: 2.502038319575826

Epoch: 6| Step: 12
Training loss: 2.351982510086477
Validation loss: 2.495086166143722

Epoch: 6| Step: 13
Training loss: 1.7397822039869157
Validation loss: 2.506674478843147

Epoch: 346| Step: 0
Training loss: 2.0283762630703905
Validation loss: 2.50612952971578

Epoch: 6| Step: 1
Training loss: 1.9100978975022727
Validation loss: 2.5135861925024887

Epoch: 6| Step: 2
Training loss: 1.4986544773181922
Validation loss: 2.5006641720855995

Epoch: 6| Step: 3
Training loss: 2.1909393520344675
Validation loss: 2.500302331342965

Epoch: 6| Step: 4
Training loss: 2.2425073269285263
Validation loss: 2.5303952333469844

Epoch: 6| Step: 5
Training loss: 1.6222519747010287
Validation loss: 2.5128183210054154

Epoch: 6| Step: 6
Training loss: 2.469596427967292
Validation loss: 2.5090995549847928

Epoch: 6| Step: 7
Training loss: 1.9431387157821138
Validation loss: 2.486779879910526

Epoch: 6| Step: 8
Training loss: 2.590464429860373
Validation loss: 2.5347849703592695

Epoch: 6| Step: 9
Training loss: 1.836210839235045
Validation loss: 2.523878720268907

Epoch: 6| Step: 10
Training loss: 2.070053026745648
Validation loss: 2.508142713763944

Epoch: 6| Step: 11
Training loss: 2.9132751546618487
Validation loss: 2.5047576180258

Epoch: 6| Step: 12
Training loss: 1.9862347396158726
Validation loss: 2.492042439815659

Epoch: 6| Step: 13
Training loss: 2.378243390017486
Validation loss: 2.509147160339773

Epoch: 347| Step: 0
Training loss: 2.2625794033216713
Validation loss: 2.4999202613009337

Epoch: 6| Step: 1
Training loss: 2.1272555329814278
Validation loss: 2.493384771469018

Epoch: 6| Step: 2
Training loss: 2.6054692990418276
Validation loss: 2.5004022438739333

Epoch: 6| Step: 3
Training loss: 2.025715256574876
Validation loss: 2.507043843821274

Epoch: 6| Step: 4
Training loss: 1.804685386227117
Validation loss: 2.5196694114498017

Epoch: 6| Step: 5
Training loss: 2.19881922072361
Validation loss: 2.4948009482487734

Epoch: 6| Step: 6
Training loss: 1.4299161134137017
Validation loss: 2.5572778156599547

Epoch: 6| Step: 7
Training loss: 2.2530596381030663
Validation loss: 2.4962035866260166

Epoch: 6| Step: 8
Training loss: 1.982279715115281
Validation loss: 2.493430789158073

Epoch: 6| Step: 9
Training loss: 2.0452614750919653
Validation loss: 2.523524117882509

Epoch: 6| Step: 10
Training loss: 2.3336439834158527
Validation loss: 2.5282561152241416

Epoch: 6| Step: 11
Training loss: 2.1246508143136538
Validation loss: 2.525448678512239

Epoch: 6| Step: 12
Training loss: 2.320886508827851
Validation loss: 2.4790471241946967

Epoch: 6| Step: 13
Training loss: 2.2416024857117227
Validation loss: 2.4946358671232485

Epoch: 348| Step: 0
Training loss: 1.8574931134061623
Validation loss: 2.521268996285812

Epoch: 6| Step: 1
Training loss: 2.607225274950053
Validation loss: 2.4887216169659605

Epoch: 6| Step: 2
Training loss: 2.3216175044382448
Validation loss: 2.4970845501715626

Epoch: 6| Step: 3
Training loss: 1.7502413991916475
Validation loss: 2.5060646586174977

Epoch: 6| Step: 4
Training loss: 2.3917379344286163
Validation loss: 2.5212238354738945

Epoch: 6| Step: 5
Training loss: 2.0393945416313017
Validation loss: 2.534003237694742

Epoch: 6| Step: 6
Training loss: 1.9060538769499507
Validation loss: 2.477270677565287

Epoch: 6| Step: 7
Training loss: 1.7827928119077157
Validation loss: 2.5438837810424593

Epoch: 6| Step: 8
Training loss: 1.5111265772296072
Validation loss: 2.5090571055273334

Epoch: 6| Step: 9
Training loss: 2.3570947807330294
Validation loss: 2.495619163655619

Epoch: 6| Step: 10
Training loss: 2.6760260922424326
Validation loss: 2.498545788747803

Epoch: 6| Step: 11
Training loss: 1.7153059699263826
Validation loss: 2.471233200583882

Epoch: 6| Step: 12
Training loss: 2.6326855365691606
Validation loss: 2.52881858852902

Epoch: 6| Step: 13
Training loss: 1.916871108333193
Validation loss: 2.5255717510367974

Epoch: 349| Step: 0
Training loss: 2.2138168941753693
Validation loss: 2.5371506912512665

Epoch: 6| Step: 1
Training loss: 1.882267156742907
Validation loss: 2.5041647235611046

Epoch: 6| Step: 2
Training loss: 2.0792281585753396
Validation loss: 2.4686017047065203

Epoch: 6| Step: 3
Training loss: 2.136233984372107
Validation loss: 2.516977865494304

Epoch: 6| Step: 4
Training loss: 2.2000516191842396
Validation loss: 2.4591552197532605

Epoch: 6| Step: 5
Training loss: 2.525794661463002
Validation loss: 2.497750350934306

Epoch: 6| Step: 6
Training loss: 1.8792502233137958
Validation loss: 2.5065173530669083

Epoch: 6| Step: 7
Training loss: 2.2926372437906983
Validation loss: 2.4889279669681326

Epoch: 6| Step: 8
Training loss: 2.225025061134045
Validation loss: 2.528509963889617

Epoch: 6| Step: 9
Training loss: 1.901795607911439
Validation loss: 2.4878644057850923

Epoch: 6| Step: 10
Training loss: 1.540485634216462
Validation loss: 2.5205436807259574

Epoch: 6| Step: 11
Training loss: 2.676956787284997
Validation loss: 2.489016840485534

Epoch: 6| Step: 12
Training loss: 2.14522964973253
Validation loss: 2.5044686546815997

Epoch: 6| Step: 13
Training loss: 1.7027341061596484
Validation loss: 2.4815171631657233

Epoch: 350| Step: 0
Training loss: 1.9705065202893102
Validation loss: 2.478164576491796

Epoch: 6| Step: 1
Training loss: 2.3756319510286197
Validation loss: 2.516194696821135

Epoch: 6| Step: 2
Training loss: 2.056077958072512
Validation loss: 2.4949592902070434

Epoch: 6| Step: 3
Training loss: 2.168435536213409
Validation loss: 2.4943915533593217

Epoch: 6| Step: 4
Training loss: 1.7828812742215816
Validation loss: 2.48462719216759

Epoch: 6| Step: 5
Training loss: 2.200977983710385
Validation loss: 2.522907122226805

Epoch: 6| Step: 6
Training loss: 1.668035930200821
Validation loss: 2.51207873864123

Epoch: 6| Step: 7
Training loss: 1.967890657857069
Validation loss: 2.5244486738589145

Epoch: 6| Step: 8
Training loss: 1.4420432567272248
Validation loss: 2.5136017797930568

Epoch: 6| Step: 9
Training loss: 1.730309334811126
Validation loss: 2.484969351971463

Epoch: 6| Step: 10
Training loss: 2.4137046573435783
Validation loss: 2.531069435273724

Epoch: 6| Step: 11
Training loss: 2.8722668800678104
Validation loss: 2.47690447077355

Epoch: 6| Step: 12
Training loss: 2.420838755756374
Validation loss: 2.508494245656385

Epoch: 6| Step: 13
Training loss: 2.4981238000641155
Validation loss: 2.4987268051466813

Epoch: 351| Step: 0
Training loss: 2.3400135010549894
Validation loss: 2.5276219791640333

Epoch: 6| Step: 1
Training loss: 2.397478638873043
Validation loss: 2.505529571424964

Epoch: 6| Step: 2
Training loss: 1.7726247664472226
Validation loss: 2.496514458315829

Epoch: 6| Step: 3
Training loss: 1.7473624653100914
Validation loss: 2.5029170952656226

Epoch: 6| Step: 4
Training loss: 1.7385425842660625
Validation loss: 2.483614277217829

Epoch: 6| Step: 5
Training loss: 1.855047623879002
Validation loss: 2.5063330257414336

Epoch: 6| Step: 6
Training loss: 1.962661107247623
Validation loss: 2.493402654485784

Epoch: 6| Step: 7
Training loss: 1.7234183980483222
Validation loss: 2.479642266263622

Epoch: 6| Step: 8
Training loss: 2.170170396364159
Validation loss: 2.5030433896193856

Epoch: 6| Step: 9
Training loss: 2.0274430032002346
Validation loss: 2.5005157943562617

Epoch: 6| Step: 10
Training loss: 2.3672157008943118
Validation loss: 2.515892180221848

Epoch: 6| Step: 11
Training loss: 2.4889576230700428
Validation loss: 2.544965007575345

Epoch: 6| Step: 12
Training loss: 3.095051193183682
Validation loss: 2.4919817799699575

Epoch: 6| Step: 13
Training loss: 1.255071413584389
Validation loss: 2.5364282020068996

Epoch: 352| Step: 0
Training loss: 2.064863526866248
Validation loss: 2.531907023046524

Epoch: 6| Step: 1
Training loss: 1.8564171866704897
Validation loss: 2.535611706074226

Epoch: 6| Step: 2
Training loss: 1.497270246021396
Validation loss: 2.5132904807494474

Epoch: 6| Step: 3
Training loss: 2.121271902228664
Validation loss: 2.4966909864374163

Epoch: 6| Step: 4
Training loss: 2.4179803851068002
Validation loss: 2.4828023444283938

Epoch: 6| Step: 5
Training loss: 2.961970891259559
Validation loss: 2.5191546272547356

Epoch: 6| Step: 6
Training loss: 1.8714368660887557
Validation loss: 2.4978589395622244

Epoch: 6| Step: 7
Training loss: 1.7870027770889128
Validation loss: 2.4932974407060966

Epoch: 6| Step: 8
Training loss: 1.7875918544830391
Validation loss: 2.4472423011195974

Epoch: 6| Step: 9
Training loss: 2.4569147079313844
Validation loss: 2.5319021466805767

Epoch: 6| Step: 10
Training loss: 2.4135778243037755
Validation loss: 2.516653339000547

Epoch: 6| Step: 11
Training loss: 2.436907965336105
Validation loss: 2.494504855370454

Epoch: 6| Step: 12
Training loss: 2.121262236308658
Validation loss: 2.521668333394048

Epoch: 6| Step: 13
Training loss: 0.7735231140224195
Validation loss: 2.5204911419668794

Epoch: 353| Step: 0
Training loss: 2.820655696202313
Validation loss: 2.5001041236851593

Epoch: 6| Step: 1
Training loss: 2.520940342252866
Validation loss: 2.5091902601186464

Epoch: 6| Step: 2
Training loss: 2.4734045647014633
Validation loss: 2.515770287032544

Epoch: 6| Step: 3
Training loss: 1.9424594654974676
Validation loss: 2.4953262695189022

Epoch: 6| Step: 4
Training loss: 1.2019697811149164
Validation loss: 2.534987777582117

Epoch: 6| Step: 5
Training loss: 1.8316062320369109
Validation loss: 2.4866849840729923

Epoch: 6| Step: 6
Training loss: 1.4614653424262367
Validation loss: 2.49360025158635

Epoch: 6| Step: 7
Training loss: 2.3777975370748736
Validation loss: 2.483024976946737

Epoch: 6| Step: 8
Training loss: 1.4059011662475733
Validation loss: 2.529692572414593

Epoch: 6| Step: 9
Training loss: 2.661520004802127
Validation loss: 2.5048541903292265

Epoch: 6| Step: 10
Training loss: 1.9409961535942883
Validation loss: 2.5092849485013047

Epoch: 6| Step: 11
Training loss: 1.8844043128420749
Validation loss: 2.54332121385873

Epoch: 6| Step: 12
Training loss: 1.892398412338781
Validation loss: 2.4948782888565604

Epoch: 6| Step: 13
Training loss: 2.1421849490761957
Validation loss: 2.4582154204168933

Epoch: 354| Step: 0
Training loss: 2.324247061933655
Validation loss: 2.5048085769706

Epoch: 6| Step: 1
Training loss: 2.136392906770571
Validation loss: 2.5563441382998113

Epoch: 6| Step: 2
Training loss: 2.0486201643519246
Validation loss: 2.502650877694516

Epoch: 6| Step: 3
Training loss: 1.635156290826191
Validation loss: 2.522727707744087

Epoch: 6| Step: 4
Training loss: 2.986992611753651
Validation loss: 2.5228172115657213

Epoch: 6| Step: 5
Training loss: 1.4769733805908205
Validation loss: 2.4868513682113838

Epoch: 6| Step: 6
Training loss: 2.285497395409014
Validation loss: 2.493807316373097

Epoch: 6| Step: 7
Training loss: 1.798902942870132
Validation loss: 2.4759965242575537

Epoch: 6| Step: 8
Training loss: 2.82345954958186
Validation loss: 2.5090044457862275

Epoch: 6| Step: 9
Training loss: 2.2504874866936926
Validation loss: 2.5016943594026477

Epoch: 6| Step: 10
Training loss: 1.8988928856995788
Validation loss: 2.493391974866004

Epoch: 6| Step: 11
Training loss: 1.1987358388372586
Validation loss: 2.4963213240268853

Epoch: 6| Step: 12
Training loss: 1.7702175939831826
Validation loss: 2.4888402362128565

Epoch: 6| Step: 13
Training loss: 2.2029193755161387
Validation loss: 2.5570264835470584

Epoch: 355| Step: 0
Training loss: 2.224172494003344
Validation loss: 2.4893081237638977

Epoch: 6| Step: 1
Training loss: 1.5370619110961277
Validation loss: 2.511962056179145

Epoch: 6| Step: 2
Training loss: 2.7694270836700388
Validation loss: 2.5141499680894546

Epoch: 6| Step: 3
Training loss: 1.9423041310971363
Validation loss: 2.488794868601952

Epoch: 6| Step: 4
Training loss: 1.9156714702900208
Validation loss: 2.4560428726498276

Epoch: 6| Step: 5
Training loss: 2.35863801675191
Validation loss: 2.5319177477784574

Epoch: 6| Step: 6
Training loss: 2.0504695401128052
Validation loss: 2.5478367190384046

Epoch: 6| Step: 7
Training loss: 2.148960895940523
Validation loss: 2.453286468644872

Epoch: 6| Step: 8
Training loss: 1.584801929745373
Validation loss: 2.51855050290339

Epoch: 6| Step: 9
Training loss: 1.7482882711290164
Validation loss: 2.513786068234513

Epoch: 6| Step: 10
Training loss: 1.6328244733599575
Validation loss: 2.523461681095051

Epoch: 6| Step: 11
Training loss: 2.677554335269922
Validation loss: 2.5061491661520696

Epoch: 6| Step: 12
Training loss: 2.3007775194950657
Validation loss: 2.535665742296381

Epoch: 6| Step: 13
Training loss: 2.0185249226760504
Validation loss: 2.5414974560451404

Epoch: 356| Step: 0
Training loss: 1.695793470371874
Validation loss: 2.4530707790218402

Epoch: 6| Step: 1
Training loss: 1.6960382973467594
Validation loss: 2.5066838561983715

Epoch: 6| Step: 2
Training loss: 2.667347960067393
Validation loss: 2.49733773165342

Epoch: 6| Step: 3
Training loss: 2.192019997122664
Validation loss: 2.5063300451106962

Epoch: 6| Step: 4
Training loss: 2.0821260641814066
Validation loss: 2.4681940580769797

Epoch: 6| Step: 5
Training loss: 1.7874096560822328
Validation loss: 2.551362751900699

Epoch: 6| Step: 6
Training loss: 2.052859925877715
Validation loss: 2.494503163750609

Epoch: 6| Step: 7
Training loss: 2.013460875296587
Validation loss: 2.5206565933963354

Epoch: 6| Step: 8
Training loss: 1.6405721020117994
Validation loss: 2.53158774908416

Epoch: 6| Step: 9
Training loss: 2.164364239420567
Validation loss: 2.477978520316772

Epoch: 6| Step: 10
Training loss: 2.2484659687543602
Validation loss: 2.5077099834581813

Epoch: 6| Step: 11
Training loss: 2.848766220468404
Validation loss: 2.4786118741198297

Epoch: 6| Step: 12
Training loss: 2.0586426238713273
Validation loss: 2.5090946394017686

Epoch: 6| Step: 13
Training loss: 2.2071586099521903
Validation loss: 2.496959170043062

Epoch: 357| Step: 0
Training loss: 1.5513362323852475
Validation loss: 2.498731166065606

Epoch: 6| Step: 1
Training loss: 2.124800953238055
Validation loss: 2.5134414483223724

Epoch: 6| Step: 2
Training loss: 1.8932488889510872
Validation loss: 2.5215948533143013

Epoch: 6| Step: 3
Training loss: 1.9844323097423764
Validation loss: 2.510675258608093

Epoch: 6| Step: 4
Training loss: 2.184941021190907
Validation loss: 2.489370900191728

Epoch: 6| Step: 5
Training loss: 1.8582932988232224
Validation loss: 2.5031163450420757

Epoch: 6| Step: 6
Training loss: 2.872262729709203
Validation loss: 2.479558234565409

Epoch: 6| Step: 7
Training loss: 2.058988066517809
Validation loss: 2.495092098781894

Epoch: 6| Step: 8
Training loss: 1.8005219974206927
Validation loss: 2.498575848844453

Epoch: 6| Step: 9
Training loss: 2.1333116778625634
Validation loss: 2.525306691319374

Epoch: 6| Step: 10
Training loss: 2.3254027356230185
Validation loss: 2.549337186776053

Epoch: 6| Step: 11
Training loss: 2.167901115393474
Validation loss: 2.485465090945154

Epoch: 6| Step: 12
Training loss: 1.5026300737691833
Validation loss: 2.464933748768312

Epoch: 6| Step: 13
Training loss: 2.541238269244062
Validation loss: 2.516761269924041

Epoch: 358| Step: 0
Training loss: 2.5617110735937327
Validation loss: 2.490559647075406

Epoch: 6| Step: 1
Training loss: 2.1642376660126295
Validation loss: 2.5072537635675007

Epoch: 6| Step: 2
Training loss: 1.5999907433718898
Validation loss: 2.5145195369177338

Epoch: 6| Step: 3
Training loss: 2.303790729429673
Validation loss: 2.493144357723898

Epoch: 6| Step: 4
Training loss: 2.087807035423314
Validation loss: 2.4855937314763255

Epoch: 6| Step: 5
Training loss: 2.1983083463361752
Validation loss: 2.500786504800093

Epoch: 6| Step: 6
Training loss: 2.2034963741181772
Validation loss: 2.482148563647535

Epoch: 6| Step: 7
Training loss: 2.2090623000319307
Validation loss: 2.4929983392768853

Epoch: 6| Step: 8
Training loss: 2.5391866330833963
Validation loss: 2.5021198135173854

Epoch: 6| Step: 9
Training loss: 1.3482955521963231
Validation loss: 2.4385672946926498

Epoch: 6| Step: 10
Training loss: 1.5208737442012672
Validation loss: 2.486764175012365

Epoch: 6| Step: 11
Training loss: 1.6007642291734043
Validation loss: 2.4918954092824865

Epoch: 6| Step: 12
Training loss: 1.7320846693914584
Validation loss: 2.49196956243648

Epoch: 6| Step: 13
Training loss: 3.35588751456854
Validation loss: 2.450671996222996

Epoch: 359| Step: 0
Training loss: 1.6677961416516205
Validation loss: 2.499798785079962

Epoch: 6| Step: 1
Training loss: 1.7634014205310797
Validation loss: 2.51480648803701

Epoch: 6| Step: 2
Training loss: 2.6884863174655025
Validation loss: 2.5250707382711255

Epoch: 6| Step: 3
Training loss: 1.7767397346763627
Validation loss: 2.463573101708343

Epoch: 6| Step: 4
Training loss: 2.0333013886407345
Validation loss: 2.5435464209588914

Epoch: 6| Step: 5
Training loss: 2.5314703951360356
Validation loss: 2.4889016024874726

Epoch: 6| Step: 6
Training loss: 1.130868282513629
Validation loss: 2.5432263602943883

Epoch: 6| Step: 7
Training loss: 1.85985565384282
Validation loss: 2.5152635828720764

Epoch: 6| Step: 8
Training loss: 1.9510338536839449
Validation loss: 2.5072271991496855

Epoch: 6| Step: 9
Training loss: 2.6849733052267526
Validation loss: 2.500554301226507

Epoch: 6| Step: 10
Training loss: 2.2000178076283463
Validation loss: 2.503894821687907

Epoch: 6| Step: 11
Training loss: 2.307053621870235
Validation loss: 2.5170717791682335

Epoch: 6| Step: 12
Training loss: 2.1498945254691195
Validation loss: 2.5103876693195297

Epoch: 6| Step: 13
Training loss: 2.4053481752878003
Validation loss: 2.4762754850124944

Epoch: 360| Step: 0
Training loss: 2.359710164295344
Validation loss: 2.5562812926616036

Epoch: 6| Step: 1
Training loss: 2.2036714349634625
Validation loss: 2.5205689748161

Epoch: 6| Step: 2
Training loss: 2.668281503333236
Validation loss: 2.529307202329056

Epoch: 6| Step: 3
Training loss: 1.5647714507702881
Validation loss: 2.5176249112097286

Epoch: 6| Step: 4
Training loss: 1.8200248523734053
Validation loss: 2.524996439369702

Epoch: 6| Step: 5
Training loss: 2.0235745298904213
Validation loss: 2.5024822412376038

Epoch: 6| Step: 6
Training loss: 1.9813424916143443
Validation loss: 2.5248676233671223

Epoch: 6| Step: 7
Training loss: 2.398835785938498
Validation loss: 2.5171344640812343

Epoch: 6| Step: 8
Training loss: 2.1764643639473995
Validation loss: 2.50313593342399

Epoch: 6| Step: 9
Training loss: 2.221962550674318
Validation loss: 2.532098862522807

Epoch: 6| Step: 10
Training loss: 1.9433889415542935
Validation loss: 2.518728696842498

Epoch: 6| Step: 11
Training loss: 2.2237534320669723
Validation loss: 2.5109834044493087

Epoch: 6| Step: 12
Training loss: 2.140610186671772
Validation loss: 2.529771097045368

Epoch: 6| Step: 13
Training loss: 1.0735775230179618
Validation loss: 2.517962303994734

Epoch: 361| Step: 0
Training loss: 2.457545870937849
Validation loss: 2.4840065524309907

Epoch: 6| Step: 1
Training loss: 2.4856645608103296
Validation loss: 2.4918360236472874

Epoch: 6| Step: 2
Training loss: 2.1175439963786165
Validation loss: 2.5100375419243757

Epoch: 6| Step: 3
Training loss: 1.5135938267428704
Validation loss: 2.490337637703244

Epoch: 6| Step: 4
Training loss: 2.211016434517404
Validation loss: 2.488483030488546

Epoch: 6| Step: 5
Training loss: 2.256410896291731
Validation loss: 2.506570714005023

Epoch: 6| Step: 6
Training loss: 1.499789302651143
Validation loss: 2.500190157477631

Epoch: 6| Step: 7
Training loss: 2.3895273119460536
Validation loss: 2.4702738592504176

Epoch: 6| Step: 8
Training loss: 1.8543067907755735
Validation loss: 2.474742113360846

Epoch: 6| Step: 9
Training loss: 2.2022164277198195
Validation loss: 2.514179564268053

Epoch: 6| Step: 10
Training loss: 1.6351430222604597
Validation loss: 2.490530773831297

Epoch: 6| Step: 11
Training loss: 1.8638416143424024
Validation loss: 2.466501605950265

Epoch: 6| Step: 12
Training loss: 2.192570504563187
Validation loss: 2.494607476791763

Epoch: 6| Step: 13
Training loss: 1.9855931065213026
Validation loss: 2.4920329950209394

Epoch: 362| Step: 0
Training loss: 1.7234427458059896
Validation loss: 2.4789436652287606

Epoch: 6| Step: 1
Training loss: 2.0526674733389028
Validation loss: 2.44149205788386

Epoch: 6| Step: 2
Training loss: 2.0245758742766657
Validation loss: 2.4859395893672067

Epoch: 6| Step: 3
Training loss: 2.3445094594734104
Validation loss: 2.503999971044195

Epoch: 6| Step: 4
Training loss: 2.3054224943602946
Validation loss: 2.481239505670309

Epoch: 6| Step: 5
Training loss: 2.6577934717464884
Validation loss: 2.4948808772823927

Epoch: 6| Step: 6
Training loss: 2.205636074679366
Validation loss: 2.4433761353077452

Epoch: 6| Step: 7
Training loss: 2.3267994633826636
Validation loss: 2.457448165597767

Epoch: 6| Step: 8
Training loss: 1.5319526772822094
Validation loss: 2.5155997173445437

Epoch: 6| Step: 9
Training loss: 2.05392407060962
Validation loss: 2.493487370103237

Epoch: 6| Step: 10
Training loss: 2.2996699137792165
Validation loss: 2.5049424189682314

Epoch: 6| Step: 11
Training loss: 1.7058759398078691
Validation loss: 2.4969026010331175

Epoch: 6| Step: 12
Training loss: 1.8295687981467381
Validation loss: 2.5103026442345793

Epoch: 6| Step: 13
Training loss: 2.4545818303684843
Validation loss: 2.5202598658489705

Epoch: 363| Step: 0
Training loss: 2.101810142929805
Validation loss: 2.4806708466309155

Epoch: 6| Step: 1
Training loss: 2.551740620756286
Validation loss: 2.4891957160487266

Epoch: 6| Step: 2
Training loss: 1.6398017634395678
Validation loss: 2.52299597140578

Epoch: 6| Step: 3
Training loss: 1.7113556372706455
Validation loss: 2.4800242284002456

Epoch: 6| Step: 4
Training loss: 2.2888465753469136
Validation loss: 2.4665499019826087

Epoch: 6| Step: 5
Training loss: 2.107990064557746
Validation loss: 2.497280998754527

Epoch: 6| Step: 6
Training loss: 2.1454742526249895
Validation loss: 2.487556420849568

Epoch: 6| Step: 7
Training loss: 2.0727574145104293
Validation loss: 2.481487517321557

Epoch: 6| Step: 8
Training loss: 2.327910138142598
Validation loss: 2.525900426141831

Epoch: 6| Step: 9
Training loss: 1.432818793196674
Validation loss: 2.519437680355568

Epoch: 6| Step: 10
Training loss: 2.126101488850792
Validation loss: 2.488515293120985

Epoch: 6| Step: 11
Training loss: 2.286013294144603
Validation loss: 2.5092402779837935

Epoch: 6| Step: 12
Training loss: 2.0862320252701854
Validation loss: 2.5394470983727198

Epoch: 6| Step: 13
Training loss: 2.0712248321459095
Validation loss: 2.4827115673836846

Epoch: 364| Step: 0
Training loss: 2.3062974382189205
Validation loss: 2.5053597800740515

Epoch: 6| Step: 1
Training loss: 1.2442029042128968
Validation loss: 2.4921281840755305

Epoch: 6| Step: 2
Training loss: 2.218924287210045
Validation loss: 2.477604102072191

Epoch: 6| Step: 3
Training loss: 2.3143289911001483
Validation loss: 2.4700329615327603

Epoch: 6| Step: 4
Training loss: 2.437803983073228
Validation loss: 2.504431289986153

Epoch: 6| Step: 5
Training loss: 2.115844751211787
Validation loss: 2.4981777861738745

Epoch: 6| Step: 6
Training loss: 2.1846469256933503
Validation loss: 2.4688384441474525

Epoch: 6| Step: 7
Training loss: 2.34413387969524
Validation loss: 2.471994536176134

Epoch: 6| Step: 8
Training loss: 1.98725927802466
Validation loss: 2.4907787681338673

Epoch: 6| Step: 9
Training loss: 2.2093919939523348
Validation loss: 2.4738575593709466

Epoch: 6| Step: 10
Training loss: 1.5753473443807202
Validation loss: 2.4674794697376283

Epoch: 6| Step: 11
Training loss: 2.2813127064904806
Validation loss: 2.433887244415532

Epoch: 6| Step: 12
Training loss: 1.8874745247239586
Validation loss: 2.4859997913131924

Epoch: 6| Step: 13
Training loss: 1.8804919873376005
Validation loss: 2.4773030003267653

Epoch: 365| Step: 0
Training loss: 2.825588711862276
Validation loss: 2.490077200716331

Epoch: 6| Step: 1
Training loss: 1.9283644739250578
Validation loss: 2.492252453545753

Epoch: 6| Step: 2
Training loss: 2.403215650180648
Validation loss: 2.503471371173982

Epoch: 6| Step: 3
Training loss: 2.1016070846876738
Validation loss: 2.4598726108390894

Epoch: 6| Step: 4
Training loss: 1.921422765604527
Validation loss: 2.497283247974421

Epoch: 6| Step: 5
Training loss: 1.967816146500692
Validation loss: 2.5021759305689857

Epoch: 6| Step: 6
Training loss: 1.3350945205659466
Validation loss: 2.464067856410625

Epoch: 6| Step: 7
Training loss: 2.04610550954614
Validation loss: 2.514161481324745

Epoch: 6| Step: 8
Training loss: 1.6479270108457325
Validation loss: 2.49739490261666

Epoch: 6| Step: 9
Training loss: 2.164439474936091
Validation loss: 2.4844788457200173

Epoch: 6| Step: 10
Training loss: 1.556336415047916
Validation loss: 2.4895583796957172

Epoch: 6| Step: 11
Training loss: 2.336506241675574
Validation loss: 2.4671533483062458

Epoch: 6| Step: 12
Training loss: 2.2505282735550782
Validation loss: 2.4910936305822684

Epoch: 6| Step: 13
Training loss: 2.2442226411525046
Validation loss: 2.486342029277942

Epoch: 366| Step: 0
Training loss: 2.0996331076056967
Validation loss: 2.4971768815752524

Epoch: 6| Step: 1
Training loss: 2.228380605366931
Validation loss: 2.4739537803897296

Epoch: 6| Step: 2
Training loss: 2.1803452241160537
Validation loss: 2.4689589246262735

Epoch: 6| Step: 3
Training loss: 1.930299237233355
Validation loss: 2.51679261389604

Epoch: 6| Step: 4
Training loss: 2.202189470065337
Validation loss: 2.4851396750745978

Epoch: 6| Step: 5
Training loss: 2.2925294003530587
Validation loss: 2.5111510943182775

Epoch: 6| Step: 6
Training loss: 1.6579415662106411
Validation loss: 2.451424113997413

Epoch: 6| Step: 7
Training loss: 2.5616152096479685
Validation loss: 2.476629133445084

Epoch: 6| Step: 8
Training loss: 1.3816664952430266
Validation loss: 2.483341857929661

Epoch: 6| Step: 9
Training loss: 2.322841819846034
Validation loss: 2.471787773395732

Epoch: 6| Step: 10
Training loss: 2.2612385347074615
Validation loss: 2.480573947759932

Epoch: 6| Step: 11
Training loss: 1.8960679576299928
Validation loss: 2.5091441268574135

Epoch: 6| Step: 12
Training loss: 2.3100319140983383
Validation loss: 2.506929408919253

Epoch: 6| Step: 13
Training loss: 2.234739367246271
Validation loss: 2.514169676481007

Epoch: 367| Step: 0
Training loss: 1.916553666750501
Validation loss: 2.5249024589913702

Epoch: 6| Step: 1
Training loss: 2.3738225728733204
Validation loss: 2.4781007258082624

Epoch: 6| Step: 2
Training loss: 2.4289915799292987
Validation loss: 2.525072697750688

Epoch: 6| Step: 3
Training loss: 1.5976892295362153
Validation loss: 2.477879710766088

Epoch: 6| Step: 4
Training loss: 1.9214432394204242
Validation loss: 2.5520589827748066

Epoch: 6| Step: 5
Training loss: 1.4703102752311974
Validation loss: 2.5085997448758866

Epoch: 6| Step: 6
Training loss: 2.6347498797829205
Validation loss: 2.509858407652163

Epoch: 6| Step: 7
Training loss: 1.580388928602518
Validation loss: 2.505502021798574

Epoch: 6| Step: 8
Training loss: 2.3344574899665527
Validation loss: 2.484240049903404

Epoch: 6| Step: 9
Training loss: 1.893989532731809
Validation loss: 2.5355412224905125

Epoch: 6| Step: 10
Training loss: 1.929638634190397
Validation loss: 2.489774198179834

Epoch: 6| Step: 11
Training loss: 2.1152242306283755
Validation loss: 2.5103708953286215

Epoch: 6| Step: 12
Training loss: 2.22737497597505
Validation loss: 2.471242344151051

Epoch: 6| Step: 13
Training loss: 2.2107281804831134
Validation loss: 2.4508696901220617

Epoch: 368| Step: 0
Training loss: 1.8546311586044684
Validation loss: 2.490942427308028

Epoch: 6| Step: 1
Training loss: 2.338733033730863
Validation loss: 2.5131140332306163

Epoch: 6| Step: 2
Training loss: 2.514059397029918
Validation loss: 2.5042060069955103

Epoch: 6| Step: 3
Training loss: 2.1023187233956677
Validation loss: 2.500359426317873

Epoch: 6| Step: 4
Training loss: 2.1610695975772067
Validation loss: 2.5188361205271717

Epoch: 6| Step: 5
Training loss: 1.863226995737901
Validation loss: 2.533140965237983

Epoch: 6| Step: 6
Training loss: 2.20977148574826
Validation loss: 2.475269946519149

Epoch: 6| Step: 7
Training loss: 1.5219091329330503
Validation loss: 2.475378137669395

Epoch: 6| Step: 8
Training loss: 1.6213864717125481
Validation loss: 2.4914286924100293

Epoch: 6| Step: 9
Training loss: 2.7898377268097785
Validation loss: 2.4873922874119674

Epoch: 6| Step: 10
Training loss: 2.2610027649165327
Validation loss: 2.49087083616915

Epoch: 6| Step: 11
Training loss: 1.8469413680852935
Validation loss: 2.4830236171877043

Epoch: 6| Step: 12
Training loss: 1.9497461006956096
Validation loss: 2.494335381427234

Epoch: 6| Step: 13
Training loss: 1.3417042973771611
Validation loss: 2.532353006791658

Epoch: 369| Step: 0
Training loss: 2.2094354818177946
Validation loss: 2.5518533133807684

Epoch: 6| Step: 1
Training loss: 1.619684181040698
Validation loss: 2.4981857577156896

Epoch: 6| Step: 2
Training loss: 1.8402539745526794
Validation loss: 2.481665652047122

Epoch: 6| Step: 3
Training loss: 1.8425727577284956
Validation loss: 2.492781666605496

Epoch: 6| Step: 4
Training loss: 2.005000895549926
Validation loss: 2.523736129732623

Epoch: 6| Step: 5
Training loss: 2.4337932082437006
Validation loss: 2.505177215211671

Epoch: 6| Step: 6
Training loss: 2.014299651892927
Validation loss: 2.486572153117797

Epoch: 6| Step: 7
Training loss: 1.9426274287036382
Validation loss: 2.494935835741701

Epoch: 6| Step: 8
Training loss: 2.9330540552994577
Validation loss: 2.4736147710110994

Epoch: 6| Step: 9
Training loss: 2.8248400077413125
Validation loss: 2.478619787574693

Epoch: 6| Step: 10
Training loss: 1.7190069526722604
Validation loss: 2.50374791378021

Epoch: 6| Step: 11
Training loss: 1.5625801828791492
Validation loss: 2.5042930812178246

Epoch: 6| Step: 12
Training loss: 2.059142645779778
Validation loss: 2.4948503297816758

Epoch: 6| Step: 13
Training loss: 1.891237144048328
Validation loss: 2.4838450222480817

Epoch: 370| Step: 0
Training loss: 2.079797516283793
Validation loss: 2.4461773147720103

Epoch: 6| Step: 1
Training loss: 2.0508429817717806
Validation loss: 2.479998076282189

Epoch: 6| Step: 2
Training loss: 1.6640025383260513
Validation loss: 2.5220920251258194

Epoch: 6| Step: 3
Training loss: 1.6845318864503265
Validation loss: 2.488406150463507

Epoch: 6| Step: 4
Training loss: 1.9832788278265436
Validation loss: 2.4473492046901297

Epoch: 6| Step: 5
Training loss: 2.5889705146651756
Validation loss: 2.492436081343834

Epoch: 6| Step: 6
Training loss: 2.2872221647706343
Validation loss: 2.526265113385405

Epoch: 6| Step: 7
Training loss: 2.255461950922814
Validation loss: 2.5531761473277697

Epoch: 6| Step: 8
Training loss: 2.032150069139437
Validation loss: 2.4935350771484837

Epoch: 6| Step: 9
Training loss: 2.113538574798062
Validation loss: 2.4622277651900717

Epoch: 6| Step: 10
Training loss: 2.130053401868604
Validation loss: 2.4977436024776627

Epoch: 6| Step: 11
Training loss: 2.0430165972677328
Validation loss: 2.4908098976855917

Epoch: 6| Step: 12
Training loss: 2.4002674589536306
Validation loss: 2.4900733183080384

Epoch: 6| Step: 13
Training loss: 1.2764866837921267
Validation loss: 2.4752021889681814

Epoch: 371| Step: 0
Training loss: 2.1686489378498326
Validation loss: 2.4552554103752287

Epoch: 6| Step: 1
Training loss: 2.317639079112944
Validation loss: 2.4646696246928848

Epoch: 6| Step: 2
Training loss: 1.746497600371391
Validation loss: 2.508889016156696

Epoch: 6| Step: 3
Training loss: 1.9111187154165183
Validation loss: 2.5208976976040063

Epoch: 6| Step: 4
Training loss: 1.401814650496389
Validation loss: 2.4670927612874496

Epoch: 6| Step: 5
Training loss: 1.9264135347987108
Validation loss: 2.527008295576317

Epoch: 6| Step: 6
Training loss: 1.9535852729145415
Validation loss: 2.4462036157188507

Epoch: 6| Step: 7
Training loss: 2.4268598876608345
Validation loss: 2.4737446827539538

Epoch: 6| Step: 8
Training loss: 2.591672869937583
Validation loss: 2.4813754516736632

Epoch: 6| Step: 9
Training loss: 1.684593806448489
Validation loss: 2.488154410352953

Epoch: 6| Step: 10
Training loss: 2.691481723474609
Validation loss: 2.5158552806932395

Epoch: 6| Step: 11
Training loss: 1.7031135208722956
Validation loss: 2.4899124766831

Epoch: 6| Step: 12
Training loss: 2.1257234351705425
Validation loss: 2.4931169848651455

Epoch: 6| Step: 13
Training loss: 1.7125529148809067
Validation loss: 2.5045930815881854

Epoch: 372| Step: 0
Training loss: 2.4982075941505903
Validation loss: 2.4846802571225073

Epoch: 6| Step: 1
Training loss: 2.061332025139325
Validation loss: 2.549993287434953

Epoch: 6| Step: 2
Training loss: 1.9152505729623646
Validation loss: 2.5109346464962927

Epoch: 6| Step: 3
Training loss: 2.1984377299120905
Validation loss: 2.521988787893953

Epoch: 6| Step: 4
Training loss: 1.8535531847155982
Validation loss: 2.478182457615971

Epoch: 6| Step: 5
Training loss: 1.728448657446565
Validation loss: 2.501797600991694

Epoch: 6| Step: 6
Training loss: 1.9803771349541406
Validation loss: 2.522730746233283

Epoch: 6| Step: 7
Training loss: 2.5101940219861762
Validation loss: 2.5221424102156837

Epoch: 6| Step: 8
Training loss: 2.5896949782872896
Validation loss: 2.490531791863315

Epoch: 6| Step: 9
Training loss: 1.6095163227733746
Validation loss: 2.5163185347292605

Epoch: 6| Step: 10
Training loss: 1.6769341191161795
Validation loss: 2.5013929332000586

Epoch: 6| Step: 11
Training loss: 1.7053861394455618
Validation loss: 2.486454182454792

Epoch: 6| Step: 12
Training loss: 2.489385578383013
Validation loss: 2.484770018200957

Epoch: 6| Step: 13
Training loss: 2.036634965994322
Validation loss: 2.5167339196441776

Epoch: 373| Step: 0
Training loss: 2.495368290936163
Validation loss: 2.489468651215937

Epoch: 6| Step: 1
Training loss: 2.2974586004900233
Validation loss: 2.47261396805214

Epoch: 6| Step: 2
Training loss: 2.0634377399625268
Validation loss: 2.491177686865086

Epoch: 6| Step: 3
Training loss: 2.0152637246633125
Validation loss: 2.47455029351078

Epoch: 6| Step: 4
Training loss: 1.7770095678704092
Validation loss: 2.5208883385456886

Epoch: 6| Step: 5
Training loss: 1.643961538185119
Validation loss: 2.5054210880947836

Epoch: 6| Step: 6
Training loss: 2.4887443842074894
Validation loss: 2.4823778519259503

Epoch: 6| Step: 7
Training loss: 1.3767111274768853
Validation loss: 2.4934884188003066

Epoch: 6| Step: 8
Training loss: 3.0511149322857523
Validation loss: 2.5241479366636774

Epoch: 6| Step: 9
Training loss: 1.1167609193984096
Validation loss: 2.4996834503263643

Epoch: 6| Step: 10
Training loss: 2.1125077332123734
Validation loss: 2.504752525042479

Epoch: 6| Step: 11
Training loss: 2.1739088456486515
Validation loss: 2.4838782941499504

Epoch: 6| Step: 12
Training loss: 1.8704397057893003
Validation loss: 2.501629656135147

Epoch: 6| Step: 13
Training loss: 1.8469859030226397
Validation loss: 2.5103880129576956

Epoch: 374| Step: 0
Training loss: 2.2449557664077573
Validation loss: 2.5100484377211454

Epoch: 6| Step: 1
Training loss: 2.183880017782083
Validation loss: 2.4966327571133733

Epoch: 6| Step: 2
Training loss: 2.2862672371107635
Validation loss: 2.4868651726618256

Epoch: 6| Step: 3
Training loss: 1.6508430176935003
Validation loss: 2.489739079048189

Epoch: 6| Step: 4
Training loss: 2.9304491197523004
Validation loss: 2.499951887180497

Epoch: 6| Step: 5
Training loss: 2.171621403879956
Validation loss: 2.5206577406315387

Epoch: 6| Step: 6
Training loss: 2.0308996045060335
Validation loss: 2.521073354265648

Epoch: 6| Step: 7
Training loss: 2.1702594923627623
Validation loss: 2.4970819106439754

Epoch: 6| Step: 8
Training loss: 1.8288432609469194
Validation loss: 2.4860707235417623

Epoch: 6| Step: 9
Training loss: 2.34150975291021
Validation loss: 2.513330229102884

Epoch: 6| Step: 10
Training loss: 1.7316427618487267
Validation loss: 2.4947177159168032

Epoch: 6| Step: 11
Training loss: 1.6895392070735646
Validation loss: 2.4706020477383395

Epoch: 6| Step: 12
Training loss: 1.8455592185135994
Validation loss: 2.4988827075028635

Epoch: 6| Step: 13
Training loss: 1.6736692266368047
Validation loss: 2.4918146015495815

Epoch: 375| Step: 0
Training loss: 2.7071986731516966
Validation loss: 2.4716523258581837

Epoch: 6| Step: 1
Training loss: 1.4082063418284168
Validation loss: 2.4982403458796205

Epoch: 6| Step: 2
Training loss: 2.430258533555019
Validation loss: 2.491813208520426

Epoch: 6| Step: 3
Training loss: 2.009611874585391
Validation loss: 2.461965852531856

Epoch: 6| Step: 4
Training loss: 2.190649544528042
Validation loss: 2.504505458904165

Epoch: 6| Step: 5
Training loss: 1.85230520297599
Validation loss: 2.4077119593109813

Epoch: 6| Step: 6
Training loss: 2.162066589318078
Validation loss: 2.47947430961748

Epoch: 6| Step: 7
Training loss: 1.7754648191016904
Validation loss: 2.5013698434544693

Epoch: 6| Step: 8
Training loss: 2.2538748330714986
Validation loss: 2.481726655466916

Epoch: 6| Step: 9
Training loss: 2.144005771871171
Validation loss: 2.4486443842350254

Epoch: 6| Step: 10
Training loss: 2.098304463683584
Validation loss: 2.4994338184061617

Epoch: 6| Step: 11
Training loss: 1.687190839916413
Validation loss: 2.476238942434038

Epoch: 6| Step: 12
Training loss: 2.2512857154499764
Validation loss: 2.508776136911947

Epoch: 6| Step: 13
Training loss: 1.8979696945448108
Validation loss: 2.4796184053431563

Epoch: 376| Step: 0
Training loss: 2.391617313456197
Validation loss: 2.476681481639773

Epoch: 6| Step: 1
Training loss: 1.9357044144309863
Validation loss: 2.4964203237757117

Epoch: 6| Step: 2
Training loss: 2.2359176325631798
Validation loss: 2.4960830367033315

Epoch: 6| Step: 3
Training loss: 1.2206360333125197
Validation loss: 2.49477082365673

Epoch: 6| Step: 4
Training loss: 2.005739559469255
Validation loss: 2.4861208497565124

Epoch: 6| Step: 5
Training loss: 1.922220338637961
Validation loss: 2.474520060697071

Epoch: 6| Step: 6
Training loss: 1.646838239516495
Validation loss: 2.487923822334152

Epoch: 6| Step: 7
Training loss: 2.663404993263715
Validation loss: 2.4721864558160576

Epoch: 6| Step: 8
Training loss: 1.9574795207661178
Validation loss: 2.505581007327063

Epoch: 6| Step: 9
Training loss: 1.8719377306583032
Validation loss: 2.424523966464897

Epoch: 6| Step: 10
Training loss: 1.276406413776538
Validation loss: 2.5306717812156148

Epoch: 6| Step: 11
Training loss: 1.9235958675632838
Validation loss: 2.4908190105579493

Epoch: 6| Step: 12
Training loss: 2.2135053085632292
Validation loss: 2.46691332225181

Epoch: 6| Step: 13
Training loss: 2.863907520452706
Validation loss: 2.4904114046334094

Epoch: 377| Step: 0
Training loss: 1.8017506669065624
Validation loss: 2.4375531590725488

Epoch: 6| Step: 1
Training loss: 1.6543935772263865
Validation loss: 2.527571389895421

Epoch: 6| Step: 2
Training loss: 1.6549674772541734
Validation loss: 2.486402423568462

Epoch: 6| Step: 3
Training loss: 2.8002033568513696
Validation loss: 2.474582588610319

Epoch: 6| Step: 4
Training loss: 1.8658064199701312
Validation loss: 2.5053962785572175

Epoch: 6| Step: 5
Training loss: 1.5571147431201682
Validation loss: 2.4913352239104594

Epoch: 6| Step: 6
Training loss: 2.410686514961908
Validation loss: 2.4659832463569074

Epoch: 6| Step: 7
Training loss: 2.3681188582992347
Validation loss: 2.453582022483171

Epoch: 6| Step: 8
Training loss: 2.097932424426129
Validation loss: 2.4423709724492455

Epoch: 6| Step: 9
Training loss: 2.04444519724809
Validation loss: 2.4715480210822443

Epoch: 6| Step: 10
Training loss: 1.7414094701560066
Validation loss: 2.4807231498409794

Epoch: 6| Step: 11
Training loss: 2.3089138783699776
Validation loss: 2.5097270030676215

Epoch: 6| Step: 12
Training loss: 1.751894742573422
Validation loss: 2.451100829707436

Epoch: 6| Step: 13
Training loss: 2.767041204754384
Validation loss: 2.4821214573594506

Epoch: 378| Step: 0
Training loss: 2.396612908508076
Validation loss: 2.4844199700418588

Epoch: 6| Step: 1
Training loss: 1.8647984499597183
Validation loss: 2.5192571319352877

Epoch: 6| Step: 2
Training loss: 1.803560573917514
Validation loss: 2.477977336769149

Epoch: 6| Step: 3
Training loss: 1.8208441224058625
Validation loss: 2.520949560781159

Epoch: 6| Step: 4
Training loss: 2.305744200448439
Validation loss: 2.4909422996890824

Epoch: 6| Step: 5
Training loss: 1.854391005870938
Validation loss: 2.4918364804415254

Epoch: 6| Step: 6
Training loss: 2.0705795403765883
Validation loss: 2.5001768716162047

Epoch: 6| Step: 7
Training loss: 1.3954496378215042
Validation loss: 2.491882939270391

Epoch: 6| Step: 8
Training loss: 2.4135881964147403
Validation loss: 2.5183395305736

Epoch: 6| Step: 9
Training loss: 2.1413460512547626
Validation loss: 2.45247831037997

Epoch: 6| Step: 10
Training loss: 1.689199580753427
Validation loss: 2.500300312462934

Epoch: 6| Step: 11
Training loss: 2.0530138052407922
Validation loss: 2.4782175905198596

Epoch: 6| Step: 12
Training loss: 1.619175596515234
Validation loss: 2.480390476900302

Epoch: 6| Step: 13
Training loss: 3.0024015033456934
Validation loss: 2.537284192232851

Epoch: 379| Step: 0
Training loss: 1.37088189295106
Validation loss: 2.458708050636791

Epoch: 6| Step: 1
Training loss: 1.4185093881526987
Validation loss: 2.516117782188993

Epoch: 6| Step: 2
Training loss: 1.7256463153079453
Validation loss: 2.4796862624323017

Epoch: 6| Step: 3
Training loss: 2.784345126179089
Validation loss: 2.4772105853324646

Epoch: 6| Step: 4
Training loss: 1.9445572578159174
Validation loss: 2.517611190902191

Epoch: 6| Step: 5
Training loss: 2.080433187492362
Validation loss: 2.5044613439300325

Epoch: 6| Step: 6
Training loss: 1.8173445052384027
Validation loss: 2.5170454349875016

Epoch: 6| Step: 7
Training loss: 1.5980931363231874
Validation loss: 2.514128955355921

Epoch: 6| Step: 8
Training loss: 1.8117015000509509
Validation loss: 2.4450937457843245

Epoch: 6| Step: 9
Training loss: 2.837577408730396
Validation loss: 2.5135712333858287

Epoch: 6| Step: 10
Training loss: 2.4579529117701773
Validation loss: 2.505868677653318

Epoch: 6| Step: 11
Training loss: 2.5898759704702914
Validation loss: 2.4515460625700056

Epoch: 6| Step: 12
Training loss: 1.843684567082436
Validation loss: 2.5245807191158893

Epoch: 6| Step: 13
Training loss: 1.3132305836704579
Validation loss: 2.4687427755852163

Epoch: 380| Step: 0
Training loss: 2.435937135903146
Validation loss: 2.4956811927322446

Epoch: 6| Step: 1
Training loss: 2.0273390459362575
Validation loss: 2.5018634854852486

Epoch: 6| Step: 2
Training loss: 1.8649335209143005
Validation loss: 2.49260131227177

Epoch: 6| Step: 3
Training loss: 2.148562673910329
Validation loss: 2.515737223429285

Epoch: 6| Step: 4
Training loss: 1.3768367637003862
Validation loss: 2.4686972665820686

Epoch: 6| Step: 5
Training loss: 2.0306167789291636
Validation loss: 2.49736267462701

Epoch: 6| Step: 6
Training loss: 2.328665446590814
Validation loss: 2.5068702618225145

Epoch: 6| Step: 7
Training loss: 2.0848073576705866
Validation loss: 2.509095334184406

Epoch: 6| Step: 8
Training loss: 1.7930279225410748
Validation loss: 2.470102158216926

Epoch: 6| Step: 9
Training loss: 1.7230211766691215
Validation loss: 2.4573083471518276

Epoch: 6| Step: 10
Training loss: 2.1617000092361662
Validation loss: 2.4955193205591257

Epoch: 6| Step: 11
Training loss: 1.9213378900045506
Validation loss: 2.4754517489439745

Epoch: 6| Step: 12
Training loss: 2.739828197912626
Validation loss: 2.4851671595256484

Epoch: 6| Step: 13
Training loss: 1.9632767786171663
Validation loss: 2.483405915552866

Epoch: 381| Step: 0
Training loss: 2.7454390416864496
Validation loss: 2.4829952489547193

Epoch: 6| Step: 1
Training loss: 2.537699829651852
Validation loss: 2.4620855406040065

Epoch: 6| Step: 2
Training loss: 2.08290926114001
Validation loss: 2.485255621846062

Epoch: 6| Step: 3
Training loss: 1.9018895039677606
Validation loss: 2.4833028219407396

Epoch: 6| Step: 4
Training loss: 1.2125400398958068
Validation loss: 2.5073877224726133

Epoch: 6| Step: 5
Training loss: 2.4555277143408496
Validation loss: 2.5178909496267776

Epoch: 6| Step: 6
Training loss: 1.7083948480959348
Validation loss: 2.500805455350704

Epoch: 6| Step: 7
Training loss: 2.1578131275880468
Validation loss: 2.4765636159031854

Epoch: 6| Step: 8
Training loss: 1.9351815534284151
Validation loss: 2.4592245942885596

Epoch: 6| Step: 9
Training loss: 1.4286617080227422
Validation loss: 2.476310602525834

Epoch: 6| Step: 10
Training loss: 1.881002989105981
Validation loss: 2.509961175033672

Epoch: 6| Step: 11
Training loss: 1.94765763081893
Validation loss: 2.4925916268677506

Epoch: 6| Step: 12
Training loss: 1.8665367492810383
Validation loss: 2.4705716955288017

Epoch: 6| Step: 13
Training loss: 1.8661088581203469
Validation loss: 2.4690144476294846

Epoch: 382| Step: 0
Training loss: 1.5415510787246918
Validation loss: 2.508715589419463

Epoch: 6| Step: 1
Training loss: 1.9230365499513342
Validation loss: 2.4729132729380305

Epoch: 6| Step: 2
Training loss: 2.773821393160487
Validation loss: 2.4883606868517676

Epoch: 6| Step: 3
Training loss: 1.9859111577049455
Validation loss: 2.50091954824421

Epoch: 6| Step: 4
Training loss: 2.383908079333784
Validation loss: 2.464515830765403

Epoch: 6| Step: 5
Training loss: 2.122602007316403
Validation loss: 2.4773407752217356

Epoch: 6| Step: 6
Training loss: 1.9929668024248741
Validation loss: 2.501875487679178

Epoch: 6| Step: 7
Training loss: 1.9515364633721537
Validation loss: 2.5051037323944803

Epoch: 6| Step: 8
Training loss: 2.4788944084204445
Validation loss: 2.4526317647399054

Epoch: 6| Step: 9
Training loss: 2.104631303406563
Validation loss: 2.4794601166106847

Epoch: 6| Step: 10
Training loss: 1.9571058064952398
Validation loss: 2.479446447738823

Epoch: 6| Step: 11
Training loss: 1.3029745129385657
Validation loss: 2.493487438988254

Epoch: 6| Step: 12
Training loss: 1.8827180324297716
Validation loss: 2.521288760877146

Epoch: 6| Step: 13
Training loss: 2.0725856753173706
Validation loss: 2.5217979959698322

Epoch: 383| Step: 0
Training loss: 2.482803807564482
Validation loss: 2.501814861306393

Epoch: 6| Step: 1
Training loss: 2.738331401109933
Validation loss: 2.493177767217228

Epoch: 6| Step: 2
Training loss: 2.2980690272416284
Validation loss: 2.5402386855117207

Epoch: 6| Step: 3
Training loss: 1.6210449211396145
Validation loss: 2.453195986149475

Epoch: 6| Step: 4
Training loss: 1.8351052853266057
Validation loss: 2.490110449573658

Epoch: 6| Step: 5
Training loss: 1.8513125480269876
Validation loss: 2.474158534362037

Epoch: 6| Step: 6
Training loss: 1.849749937426384
Validation loss: 2.4930003249934485

Epoch: 6| Step: 7
Training loss: 1.434158588115512
Validation loss: 2.446177330492297

Epoch: 6| Step: 8
Training loss: 1.6470484372620922
Validation loss: 2.4820487705176943

Epoch: 6| Step: 9
Training loss: 2.2335530082880326
Validation loss: 2.5149244574933314

Epoch: 6| Step: 10
Training loss: 2.1650305830841505
Validation loss: 2.5177568500328227

Epoch: 6| Step: 11
Training loss: 1.2320860391552062
Validation loss: 2.468804281605547

Epoch: 6| Step: 12
Training loss: 2.089881055267212
Validation loss: 2.4528243852128453

Epoch: 6| Step: 13
Training loss: 2.2753940827830488
Validation loss: 2.483537989698951

Epoch: 384| Step: 0
Training loss: 1.468720293759179
Validation loss: 2.471084994140364

Epoch: 6| Step: 1
Training loss: 2.5734619510047243
Validation loss: 2.4600969995612907

Epoch: 6| Step: 2
Training loss: 2.5048028587385915
Validation loss: 2.5082121734934013

Epoch: 6| Step: 3
Training loss: 2.061246201862748
Validation loss: 2.460789298552933

Epoch: 6| Step: 4
Training loss: 2.4378673570082134
Validation loss: 2.4797863749735853

Epoch: 6| Step: 5
Training loss: 1.7243686045700553
Validation loss: 2.5231388318744683

Epoch: 6| Step: 6
Training loss: 2.1464012900491847
Validation loss: 2.467849224427702

Epoch: 6| Step: 7
Training loss: 2.0447001077367344
Validation loss: 2.474944757371434

Epoch: 6| Step: 8
Training loss: 1.7393236099640064
Validation loss: 2.4601032859566554

Epoch: 6| Step: 9
Training loss: 2.1873842481233123
Validation loss: 2.501795168306562

Epoch: 6| Step: 10
Training loss: 1.6938868562369145
Validation loss: 2.5093631309760136

Epoch: 6| Step: 11
Training loss: 1.8857456011669647
Validation loss: 2.5043687446891782

Epoch: 6| Step: 12
Training loss: 2.011829202823393
Validation loss: 2.4831310131438897

Epoch: 6| Step: 13
Training loss: 1.6689432410890817
Validation loss: 2.5007543974050828

Epoch: 385| Step: 0
Training loss: 1.4688594046894443
Validation loss: 2.482142026849865

Epoch: 6| Step: 1
Training loss: 2.0985320410092227
Validation loss: 2.5251983144770236

Epoch: 6| Step: 2
Training loss: 2.3514955834280946
Validation loss: 2.4713202804323338

Epoch: 6| Step: 3
Training loss: 2.223655757525997
Validation loss: 2.4906181583729246

Epoch: 6| Step: 4
Training loss: 2.197031020461615
Validation loss: 2.4772670327681956

Epoch: 6| Step: 5
Training loss: 1.676116483009122
Validation loss: 2.52312022890707

Epoch: 6| Step: 6
Training loss: 2.405162515998251
Validation loss: 2.472725785907452

Epoch: 6| Step: 7
Training loss: 2.0151051878628707
Validation loss: 2.5210618187005065

Epoch: 6| Step: 8
Training loss: 2.7268248161333832
Validation loss: 2.5019737584075936

Epoch: 6| Step: 9
Training loss: 2.205917860755118
Validation loss: 2.4839429664790997

Epoch: 6| Step: 10
Training loss: 1.7763048426344716
Validation loss: 2.5075704719557663

Epoch: 6| Step: 11
Training loss: 1.950284372281828
Validation loss: 2.4825598494592813

Epoch: 6| Step: 12
Training loss: 1.7108243443246267
Validation loss: 2.4877935092408627

Epoch: 6| Step: 13
Training loss: 1.6261542695682532
Validation loss: 2.522093835465128

Epoch: 386| Step: 0
Training loss: 2.1854755297922694
Validation loss: 2.5096514715185143

Epoch: 6| Step: 1
Training loss: 1.8079342391452546
Validation loss: 2.456836846322604

Epoch: 6| Step: 2
Training loss: 1.5226131458552274
Validation loss: 2.4747388719577286

Epoch: 6| Step: 3
Training loss: 1.5123664986197827
Validation loss: 2.475736070810801

Epoch: 6| Step: 4
Training loss: 2.3767522070294547
Validation loss: 2.4590161437064686

Epoch: 6| Step: 5
Training loss: 1.8145117283155885
Validation loss: 2.449627838012074

Epoch: 6| Step: 6
Training loss: 2.843800470931327
Validation loss: 2.4959384424986553

Epoch: 6| Step: 7
Training loss: 2.180039353496672
Validation loss: 2.5220910757390484

Epoch: 6| Step: 8
Training loss: 2.2721789045280145
Validation loss: 2.4693811175092755

Epoch: 6| Step: 9
Training loss: 1.9375744466937814
Validation loss: 2.462483882939801

Epoch: 6| Step: 10
Training loss: 1.844576763413957
Validation loss: 2.500321888453971

Epoch: 6| Step: 11
Training loss: 1.4910363355541913
Validation loss: 2.4848478732353767

Epoch: 6| Step: 12
Training loss: 1.7885136324581845
Validation loss: 2.462156493836777

Epoch: 6| Step: 13
Training loss: 2.475976435393574
Validation loss: 2.4764871505279036

Epoch: 387| Step: 0
Training loss: 1.8654655912401146
Validation loss: 2.470943450037228

Epoch: 6| Step: 1
Training loss: 2.401021453926807
Validation loss: 2.489810921015603

Epoch: 6| Step: 2
Training loss: 2.1243657680309753
Validation loss: 2.499859533157938

Epoch: 6| Step: 3
Training loss: 1.7257054475893618
Validation loss: 2.4981036654206807

Epoch: 6| Step: 4
Training loss: 1.8837376770953005
Validation loss: 2.5131352329669863

Epoch: 6| Step: 5
Training loss: 2.1899596418729335
Validation loss: 2.4954321323014197

Epoch: 6| Step: 6
Training loss: 1.623843294889073
Validation loss: 2.4662397817884383

Epoch: 6| Step: 7
Training loss: 2.1097927103774246
Validation loss: 2.4742472721575575

Epoch: 6| Step: 8
Training loss: 1.8447260455266612
Validation loss: 2.4901379069576492

Epoch: 6| Step: 9
Training loss: 2.2257510128625224
Validation loss: 2.5028741937214636

Epoch: 6| Step: 10
Training loss: 2.090077609943798
Validation loss: 2.481125175287325

Epoch: 6| Step: 11
Training loss: 2.109729878278676
Validation loss: 2.4583269092760225

Epoch: 6| Step: 12
Training loss: 2.120176563107998
Validation loss: 2.4737006037107965

Epoch: 6| Step: 13
Training loss: 1.807129433978915
Validation loss: 2.465542463619109

Epoch: 388| Step: 0
Training loss: 1.6366882321373606
Validation loss: 2.482501640821993

Epoch: 6| Step: 1
Training loss: 1.8454408166573737
Validation loss: 2.4586850573954324

Epoch: 6| Step: 2
Training loss: 2.403761133891251
Validation loss: 2.4995371400318342

Epoch: 6| Step: 3
Training loss: 2.042449359989755
Validation loss: 2.44984450990557

Epoch: 6| Step: 4
Training loss: 2.162093165060339
Validation loss: 2.4912781557128962

Epoch: 6| Step: 5
Training loss: 1.6861393706027261
Validation loss: 2.4828539114528514

Epoch: 6| Step: 6
Training loss: 2.0437335885698467
Validation loss: 2.4949399150630436

Epoch: 6| Step: 7
Training loss: 2.0482004547887462
Validation loss: 2.503139064826578

Epoch: 6| Step: 8
Training loss: 2.371243869610686
Validation loss: 2.4748561077731153

Epoch: 6| Step: 9
Training loss: 2.2546094726257926
Validation loss: 2.4652572697613517

Epoch: 6| Step: 10
Training loss: 1.7870217223637632
Validation loss: 2.468594355237487

Epoch: 6| Step: 11
Training loss: 2.016373015007549
Validation loss: 2.469214467741428

Epoch: 6| Step: 12
Training loss: 2.065493636347494
Validation loss: 2.508779753303835

Epoch: 6| Step: 13
Training loss: 1.7407565548285926
Validation loss: 2.480433748718695

Epoch: 389| Step: 0
Training loss: 2.457451958715476
Validation loss: 2.538765533724981

Epoch: 6| Step: 1
Training loss: 2.164308389403728
Validation loss: 2.50735319767673

Epoch: 6| Step: 2
Training loss: 1.963534880473774
Validation loss: 2.4830258720952156

Epoch: 6| Step: 3
Training loss: 2.5314509935994427
Validation loss: 2.4878623896925256

Epoch: 6| Step: 4
Training loss: 1.7893278100136465
Validation loss: 2.4822428810393373

Epoch: 6| Step: 5
Training loss: 2.289903609900001
Validation loss: 2.478300116023081

Epoch: 6| Step: 6
Training loss: 1.9817322682568745
Validation loss: 2.5059093682991134

Epoch: 6| Step: 7
Training loss: 1.6719865137861354
Validation loss: 2.4520689232068142

Epoch: 6| Step: 8
Training loss: 1.9888261749045832
Validation loss: 2.498703321921426

Epoch: 6| Step: 9
Training loss: 2.1835524771787056
Validation loss: 2.4969657799615437

Epoch: 6| Step: 10
Training loss: 1.7553877319056754
Validation loss: 2.5087388578309193

Epoch: 6| Step: 11
Training loss: 1.9662064970312492
Validation loss: 2.5056494213085605

Epoch: 6| Step: 12
Training loss: 1.5975137287917054
Validation loss: 2.463305015898528

Epoch: 6| Step: 13
Training loss: 2.043593827093108
Validation loss: 2.4892198229702496

Epoch: 390| Step: 0
Training loss: 2.241752130328098
Validation loss: 2.490234642664175

Epoch: 6| Step: 1
Training loss: 1.838021382171154
Validation loss: 2.495555906462339

Epoch: 6| Step: 2
Training loss: 2.0440162321965327
Validation loss: 2.440447408107601

Epoch: 6| Step: 3
Training loss: 1.8551510830041538
Validation loss: 2.4823369376920574

Epoch: 6| Step: 4
Training loss: 1.3331627836028195
Validation loss: 2.480948874667533

Epoch: 6| Step: 5
Training loss: 2.751660105995692
Validation loss: 2.4738335431700578

Epoch: 6| Step: 6
Training loss: 2.524721745937397
Validation loss: 2.4799360854916683

Epoch: 6| Step: 7
Training loss: 2.088474060131298
Validation loss: 2.5007198702189037

Epoch: 6| Step: 8
Training loss: 2.127078218170601
Validation loss: 2.47377360579873

Epoch: 6| Step: 9
Training loss: 1.376406037426639
Validation loss: 2.505577679457527

Epoch: 6| Step: 10
Training loss: 2.63377141044905
Validation loss: 2.5075943449787537

Epoch: 6| Step: 11
Training loss: 1.7921350739185218
Validation loss: 2.4866066885949722

Epoch: 6| Step: 12
Training loss: 1.2501681691533526
Validation loss: 2.496379501052543

Epoch: 6| Step: 13
Training loss: 1.6435076864937277
Validation loss: 2.506025451784088

Epoch: 391| Step: 0
Training loss: 1.581950545901592
Validation loss: 2.4908792571887863

Epoch: 6| Step: 1
Training loss: 1.6136292458644677
Validation loss: 2.486262813550295

Epoch: 6| Step: 2
Training loss: 1.8437690087324539
Validation loss: 2.4649740158399185

Epoch: 6| Step: 3
Training loss: 2.0235741764288666
Validation loss: 2.4936691880499815

Epoch: 6| Step: 4
Training loss: 2.5118210743518548
Validation loss: 2.5086422286424903

Epoch: 6| Step: 5
Training loss: 1.6162416207434875
Validation loss: 2.508927308683403

Epoch: 6| Step: 6
Training loss: 2.475947643694019
Validation loss: 2.526368692379397

Epoch: 6| Step: 7
Training loss: 1.432838178464862
Validation loss: 2.4778892033002387

Epoch: 6| Step: 8
Training loss: 1.730398826822526
Validation loss: 2.451694146215047

Epoch: 6| Step: 9
Training loss: 2.4542783710681375
Validation loss: 2.49538160239934

Epoch: 6| Step: 10
Training loss: 2.2469897796642893
Validation loss: 2.5065150492315973

Epoch: 6| Step: 11
Training loss: 1.9023567841570928
Validation loss: 2.4672298785179323

Epoch: 6| Step: 12
Training loss: 2.319035136744181
Validation loss: 2.472357846191241

Epoch: 6| Step: 13
Training loss: 2.112638083013701
Validation loss: 2.473728417343729

Epoch: 392| Step: 0
Training loss: 2.2729276629449133
Validation loss: 2.5190172527598964

Epoch: 6| Step: 1
Training loss: 1.8169563362909058
Validation loss: 2.4730664430601124

Epoch: 6| Step: 2
Training loss: 1.8601782209884412
Validation loss: 2.505103500601831

Epoch: 6| Step: 3
Training loss: 2.331800184192253
Validation loss: 2.51497510850052

Epoch: 6| Step: 4
Training loss: 1.6740575074603328
Validation loss: 2.4482681071010353

Epoch: 6| Step: 5
Training loss: 2.137314512893778
Validation loss: 2.505845654725419

Epoch: 6| Step: 6
Training loss: 1.8723660087774492
Validation loss: 2.5106477919947086

Epoch: 6| Step: 7
Training loss: 1.514948900575924
Validation loss: 2.5000358958384763

Epoch: 6| Step: 8
Training loss: 1.8635331789281522
Validation loss: 2.50260265603363

Epoch: 6| Step: 9
Training loss: 2.6869903790976406
Validation loss: 2.491165772057718

Epoch: 6| Step: 10
Training loss: 1.4352459858192186
Validation loss: 2.524833571128116

Epoch: 6| Step: 11
Training loss: 1.87777936297067
Validation loss: 2.504295487931433

Epoch: 6| Step: 12
Training loss: 1.9241157432922804
Validation loss: 2.5177989165907655

Epoch: 6| Step: 13
Training loss: 3.2754884544871343
Validation loss: 2.4931311227477124

Epoch: 393| Step: 0
Training loss: 2.3368409109881276
Validation loss: 2.4621253664403477

Epoch: 6| Step: 1
Training loss: 2.2674618345369217
Validation loss: 2.531278187483291

Epoch: 6| Step: 2
Training loss: 2.1690956829628667
Validation loss: 2.461969852155032

Epoch: 6| Step: 3
Training loss: 1.7032760413152945
Validation loss: 2.5072450151321157

Epoch: 6| Step: 4
Training loss: 1.421758626011183
Validation loss: 2.5076553243847917

Epoch: 6| Step: 5
Training loss: 2.165527178604137
Validation loss: 2.434966530046897

Epoch: 6| Step: 6
Training loss: 2.201207835082262
Validation loss: 2.5174961996884324

Epoch: 6| Step: 7
Training loss: 2.1362782918754135
Validation loss: 2.4615431642369106

Epoch: 6| Step: 8
Training loss: 1.6102597665619376
Validation loss: 2.4872444656023203

Epoch: 6| Step: 9
Training loss: 2.245769019494685
Validation loss: 2.5117071857612627

Epoch: 6| Step: 10
Training loss: 1.6224520588332587
Validation loss: 2.4737700294285205

Epoch: 6| Step: 11
Training loss: 1.709889966536547
Validation loss: 2.481099768394126

Epoch: 6| Step: 12
Training loss: 2.0987681136221505
Validation loss: 2.5197308098111133

Epoch: 6| Step: 13
Training loss: 1.7057271554561337
Validation loss: 2.4705736178136064

Epoch: 394| Step: 0
Training loss: 2.628772386427466
Validation loss: 2.4821261397466983

Epoch: 6| Step: 1
Training loss: 2.658267971259697
Validation loss: 2.5160068728352125

Epoch: 6| Step: 2
Training loss: 1.8572876056392116
Validation loss: 2.492304155230599

Epoch: 6| Step: 3
Training loss: 1.8820347109359392
Validation loss: 2.5094681870754934

Epoch: 6| Step: 4
Training loss: 1.6962329106323892
Validation loss: 2.481905705250325

Epoch: 6| Step: 5
Training loss: 1.6694243108106928
Validation loss: 2.474185198931707

Epoch: 6| Step: 6
Training loss: 2.163410625673804
Validation loss: 2.514411779956386

Epoch: 6| Step: 7
Training loss: 2.1613794764339516
Validation loss: 2.511143528397846

Epoch: 6| Step: 8
Training loss: 2.071674402805082
Validation loss: 2.5036859872825032

Epoch: 6| Step: 9
Training loss: 1.3889875737204964
Validation loss: 2.4975499052756

Epoch: 6| Step: 10
Training loss: 2.2831371288045275
Validation loss: 2.464044737353389

Epoch: 6| Step: 11
Training loss: 1.9717703765826156
Validation loss: 2.496775512446417

Epoch: 6| Step: 12
Training loss: 1.83242370696455
Validation loss: 2.519250225354525

Epoch: 6| Step: 13
Training loss: 1.2752947223586133
Validation loss: 2.495226585254037

Epoch: 395| Step: 0
Training loss: 2.6703565420912705
Validation loss: 2.4701177448733174

Epoch: 6| Step: 1
Training loss: 2.34055548082101
Validation loss: 2.5231391559950835

Epoch: 6| Step: 2
Training loss: 1.5454967793900383
Validation loss: 2.4743827113013173

Epoch: 6| Step: 3
Training loss: 2.1768214480449934
Validation loss: 2.473088332320808

Epoch: 6| Step: 4
Training loss: 1.2498349080736924
Validation loss: 2.5059315415277266

Epoch: 6| Step: 5
Training loss: 1.5105786028465409
Validation loss: 2.4973452049221194

Epoch: 6| Step: 6
Training loss: 1.859694637745778
Validation loss: 2.50784344673709

Epoch: 6| Step: 7
Training loss: 1.419177168342291
Validation loss: 2.499365178386254

Epoch: 6| Step: 8
Training loss: 2.1862176678787533
Validation loss: 2.540752272295194

Epoch: 6| Step: 9
Training loss: 1.4898592375503914
Validation loss: 2.5044206164500196

Epoch: 6| Step: 10
Training loss: 2.281029468484831
Validation loss: 2.490577002770619

Epoch: 6| Step: 11
Training loss: 2.35622028362447
Validation loss: 2.473047955834219

Epoch: 6| Step: 12
Training loss: 2.238469140919444
Validation loss: 2.4761477466936594

Epoch: 6| Step: 13
Training loss: 1.622918997270366
Validation loss: 2.4635362161763674

Epoch: 396| Step: 0
Training loss: 1.6465331734409285
Validation loss: 2.492343707002608

Epoch: 6| Step: 1
Training loss: 2.3152059240187626
Validation loss: 2.4886025042878144

Epoch: 6| Step: 2
Training loss: 1.772917683918979
Validation loss: 2.489645769535399

Epoch: 6| Step: 3
Training loss: 1.94588416099193
Validation loss: 2.475813820590704

Epoch: 6| Step: 4
Training loss: 2.0804961021880053
Validation loss: 2.491482488149591

Epoch: 6| Step: 5
Training loss: 2.2081697751301506
Validation loss: 2.4905534463042533

Epoch: 6| Step: 6
Training loss: 2.301043406448869
Validation loss: 2.4991863269952597

Epoch: 6| Step: 7
Training loss: 1.7764257051246575
Validation loss: 2.4743016611065984

Epoch: 6| Step: 8
Training loss: 1.8720182233477702
Validation loss: 2.478154580178533

Epoch: 6| Step: 9
Training loss: 2.131644910990171
Validation loss: 2.449197166585768

Epoch: 6| Step: 10
Training loss: 1.9641166539946826
Validation loss: 2.444609844223883

Epoch: 6| Step: 11
Training loss: 1.9602006842577873
Validation loss: 2.4852506376975696

Epoch: 6| Step: 12
Training loss: 2.4555940290844185
Validation loss: 2.494240677458547

Epoch: 6| Step: 13
Training loss: 1.614195926908502
Validation loss: 2.428420572549582

Epoch: 397| Step: 0
Training loss: 1.7290312779136363
Validation loss: 2.508972010317849

Epoch: 6| Step: 1
Training loss: 2.837868109533016
Validation loss: 2.48175865109834

Epoch: 6| Step: 2
Training loss: 2.745773361855507
Validation loss: 2.517428681500803

Epoch: 6| Step: 3
Training loss: 2.3074415400777686
Validation loss: 2.5101710274954017

Epoch: 6| Step: 4
Training loss: 1.6019114788663127
Validation loss: 2.5004703540717923

Epoch: 6| Step: 5
Training loss: 1.614313345044502
Validation loss: 2.470160259474682

Epoch: 6| Step: 6
Training loss: 1.5689418751372544
Validation loss: 2.457089324373883

Epoch: 6| Step: 7
Training loss: 1.7121383424328012
Validation loss: 2.509034234468161

Epoch: 6| Step: 8
Training loss: 2.425873734681472
Validation loss: 2.4598787868155476

Epoch: 6| Step: 9
Training loss: 2.190607751623318
Validation loss: 2.4578297681376373

Epoch: 6| Step: 10
Training loss: 1.5875701768637773
Validation loss: 2.505259203633655

Epoch: 6| Step: 11
Training loss: 1.5091215785744592
Validation loss: 2.4956598036404753

Epoch: 6| Step: 12
Training loss: 2.130976407117843
Validation loss: 2.5151776218069135

Epoch: 6| Step: 13
Training loss: 1.6552329629546445
Validation loss: 2.452109443878981

Epoch: 398| Step: 0
Training loss: 2.394317766788564
Validation loss: 2.529530276128803

Epoch: 6| Step: 1
Training loss: 2.0585707024400643
Validation loss: 2.534724491410741

Epoch: 6| Step: 2
Training loss: 1.8890346427232856
Validation loss: 2.4486120024951474

Epoch: 6| Step: 3
Training loss: 2.168988841863962
Validation loss: 2.489623451261504

Epoch: 6| Step: 4
Training loss: 1.9719420093691444
Validation loss: 2.5002209493971903

Epoch: 6| Step: 5
Training loss: 1.5836827076719318
Validation loss: 2.524361628667424

Epoch: 6| Step: 6
Training loss: 1.83246989580644
Validation loss: 2.4810373737301576

Epoch: 6| Step: 7
Training loss: 2.0302331580074253
Validation loss: 2.529081778439632

Epoch: 6| Step: 8
Training loss: 2.184509958619521
Validation loss: 2.4639338979115997

Epoch: 6| Step: 9
Training loss: 1.8817684080303776
Validation loss: 2.46673782048399

Epoch: 6| Step: 10
Training loss: 1.5458952758525393
Validation loss: 2.4807461661696504

Epoch: 6| Step: 11
Training loss: 2.6708596960741464
Validation loss: 2.45422052475132

Epoch: 6| Step: 12
Training loss: 1.315134039309803
Validation loss: 2.4493877843665475

Epoch: 6| Step: 13
Training loss: 2.0827851782321023
Validation loss: 2.509014255838842

Epoch: 399| Step: 0
Training loss: 2.295883483708213
Validation loss: 2.485330977280995

Epoch: 6| Step: 1
Training loss: 1.5406714999162854
Validation loss: 2.4756180150848

Epoch: 6| Step: 2
Training loss: 2.274942430878693
Validation loss: 2.4586893647344943

Epoch: 6| Step: 3
Training loss: 2.7434538175221674
Validation loss: 2.4861124327271824

Epoch: 6| Step: 4
Training loss: 1.690958999105142
Validation loss: 2.4766961812097

Epoch: 6| Step: 5
Training loss: 2.2877722739685136
Validation loss: 2.5125070483539886

Epoch: 6| Step: 6
Training loss: 2.1851962356264027
Validation loss: 2.4407455192104037

Epoch: 6| Step: 7
Training loss: 1.42359652421296
Validation loss: 2.4992900722286167

Epoch: 6| Step: 8
Training loss: 1.410666418384417
Validation loss: 2.48321479592362

Epoch: 6| Step: 9
Training loss: 2.005036925548989
Validation loss: 2.4572195412036297

Epoch: 6| Step: 10
Training loss: 1.542425827889236
Validation loss: 2.472979344357249

Epoch: 6| Step: 11
Training loss: 2.6033157077278934
Validation loss: 2.470738226256048

Epoch: 6| Step: 12
Training loss: 1.2735227310584298
Validation loss: 2.5862014507415054

Epoch: 6| Step: 13
Training loss: 1.768314979354905
Validation loss: 2.494152056891818

Epoch: 400| Step: 0
Training loss: 1.7568948342510897
Validation loss: 2.4401564624928596

Epoch: 6| Step: 1
Training loss: 1.7373210183588117
Validation loss: 2.503520055340864

Epoch: 6| Step: 2
Training loss: 1.760287225542838
Validation loss: 2.482811024106646

Epoch: 6| Step: 3
Training loss: 1.4048688251717065
Validation loss: 2.547012114028321

Epoch: 6| Step: 4
Training loss: 1.9916005186112054
Validation loss: 2.455774904851986

Epoch: 6| Step: 5
Training loss: 2.4484822262474677
Validation loss: 2.5032443575584757

Epoch: 6| Step: 6
Training loss: 1.8503421673218803
Validation loss: 2.4493989483850402

Epoch: 6| Step: 7
Training loss: 1.985043030279638
Validation loss: 2.474748855652197

Epoch: 6| Step: 8
Training loss: 2.1777720358290633
Validation loss: 2.4546310183841964

Epoch: 6| Step: 9
Training loss: 1.5941160193116821
Validation loss: 2.4620460816897634

Epoch: 6| Step: 10
Training loss: 2.582283227026288
Validation loss: 2.5191689853660866

Epoch: 6| Step: 11
Training loss: 1.7594474818204093
Validation loss: 2.5034756506098015

Epoch: 6| Step: 12
Training loss: 1.9810933890554119
Validation loss: 2.430258692842485

Epoch: 6| Step: 13
Training loss: 2.464146535114114
Validation loss: 2.5067216475354943

Epoch: 401| Step: 0
Training loss: 2.043722505996975
Validation loss: 2.4844452088374576

Epoch: 6| Step: 1
Training loss: 2.2088450312828423
Validation loss: 2.4459359629228956

Epoch: 6| Step: 2
Training loss: 1.3124636690470177
Validation loss: 2.4596492022343734

Epoch: 6| Step: 3
Training loss: 2.296961568641265
Validation loss: 2.450407051558176

Epoch: 6| Step: 4
Training loss: 1.3852785180224614
Validation loss: 2.4942143054088235

Epoch: 6| Step: 5
Training loss: 1.9846011055516266
Validation loss: 2.45127206547028

Epoch: 6| Step: 6
Training loss: 2.0699762035236775
Validation loss: 2.4853398358623036

Epoch: 6| Step: 7
Training loss: 1.484932242751825
Validation loss: 2.4583546762876516

Epoch: 6| Step: 8
Training loss: 2.437019692073337
Validation loss: 2.4830059836164478

Epoch: 6| Step: 9
Training loss: 1.7942086381103235
Validation loss: 2.4538716891815655

Epoch: 6| Step: 10
Training loss: 2.4289585013427293
Validation loss: 2.4786826909366915

Epoch: 6| Step: 11
Training loss: 1.5171461042734269
Validation loss: 2.5325878865927582

Epoch: 6| Step: 12
Training loss: 2.470799042419278
Validation loss: 2.5121000950894596

Epoch: 6| Step: 13
Training loss: 1.6760196827390514
Validation loss: 2.500794334759381

Epoch: 402| Step: 0
Training loss: 1.689017920033694
Validation loss: 2.5017741988520408

Epoch: 6| Step: 1
Training loss: 1.7363587452920826
Validation loss: 2.471085579265178

Epoch: 6| Step: 2
Training loss: 2.73791153135521
Validation loss: 2.538890794070708

Epoch: 6| Step: 3
Training loss: 2.9217013547165194
Validation loss: 2.4620235632055087

Epoch: 6| Step: 4
Training loss: 1.7229775196231758
Validation loss: 2.479066937919819

Epoch: 6| Step: 5
Training loss: 2.054701192251788
Validation loss: 2.4966006979577173

Epoch: 6| Step: 6
Training loss: 1.2745574052078734
Validation loss: 2.504053768015175

Epoch: 6| Step: 7
Training loss: 2.1462186695760335
Validation loss: 2.5195111755248014

Epoch: 6| Step: 8
Training loss: 1.7954934821609412
Validation loss: 2.46180482038961

Epoch: 6| Step: 9
Training loss: 2.0851785372032148
Validation loss: 2.467455399769463

Epoch: 6| Step: 10
Training loss: 1.1211434064228818
Validation loss: 2.5098192820433916

Epoch: 6| Step: 11
Training loss: 2.114428083862436
Validation loss: 2.436645102319291

Epoch: 6| Step: 12
Training loss: 1.8495492411811016
Validation loss: 2.512683255369258

Epoch: 6| Step: 13
Training loss: 1.9762962674655071
Validation loss: 2.505662994302559

Epoch: 403| Step: 0
Training loss: 1.7773622616652731
Validation loss: 2.457778592416842

Epoch: 6| Step: 1
Training loss: 2.1236775715171294
Validation loss: 2.4930650198227973

Epoch: 6| Step: 2
Training loss: 1.9894208658667527
Validation loss: 2.4661809831677135

Epoch: 6| Step: 3
Training loss: 2.1014867152053496
Validation loss: 2.459496659455861

Epoch: 6| Step: 4
Training loss: 2.0233931960038727
Validation loss: 2.511775348678676

Epoch: 6| Step: 5
Training loss: 2.130090562569708
Validation loss: 2.4734360673510416

Epoch: 6| Step: 6
Training loss: 2.035583685507512
Validation loss: 2.4910918532869863

Epoch: 6| Step: 7
Training loss: 2.152484093052375
Validation loss: 2.48852227985176

Epoch: 6| Step: 8
Training loss: 2.0873767016882643
Validation loss: 2.46624078386013

Epoch: 6| Step: 9
Training loss: 2.1449198831032454
Validation loss: 2.4931438209637413

Epoch: 6| Step: 10
Training loss: 1.7190407247018884
Validation loss: 2.49424180189784

Epoch: 6| Step: 11
Training loss: 1.3852758933612626
Validation loss: 2.4762355249190993

Epoch: 6| Step: 12
Training loss: 2.085682942720425
Validation loss: 2.5090980969652246

Epoch: 6| Step: 13
Training loss: 2.565911975566859
Validation loss: 2.4591295139414937

Epoch: 404| Step: 0
Training loss: 1.6235152577492997
Validation loss: 2.519608219157413

Epoch: 6| Step: 1
Training loss: 1.600826187210509
Validation loss: 2.4687447475815176

Epoch: 6| Step: 2
Training loss: 2.1639963959330726
Validation loss: 2.4772328369213623

Epoch: 6| Step: 3
Training loss: 1.7112852814341404
Validation loss: 2.50497559332797

Epoch: 6| Step: 4
Training loss: 2.2384194003572593
Validation loss: 2.4691106507651726

Epoch: 6| Step: 5
Training loss: 1.9281235507854537
Validation loss: 2.461069811161318

Epoch: 6| Step: 6
Training loss: 2.704946797071579
Validation loss: 2.46201052017532

Epoch: 6| Step: 7
Training loss: 1.5983462907082282
Validation loss: 2.488292218397585

Epoch: 6| Step: 8
Training loss: 2.1441001806268773
Validation loss: 2.4479222039496307

Epoch: 6| Step: 9
Training loss: 1.6122718590502507
Validation loss: 2.4344747568257867

Epoch: 6| Step: 10
Training loss: 1.8199941987165278
Validation loss: 2.4887856906337444

Epoch: 6| Step: 11
Training loss: 1.8851369857594236
Validation loss: 2.470405064329946

Epoch: 6| Step: 12
Training loss: 2.511305043750383
Validation loss: 2.5040725904368593

Epoch: 6| Step: 13
Training loss: 1.0344703561214752
Validation loss: 2.5162484633275066

Epoch: 405| Step: 0
Training loss: 2.396232262169144
Validation loss: 2.4962008106006444

Epoch: 6| Step: 1
Training loss: 1.8309351523482453
Validation loss: 2.4522620400335207

Epoch: 6| Step: 2
Training loss: 1.8972927456779367
Validation loss: 2.4886341699507724

Epoch: 6| Step: 3
Training loss: 1.858310106015414
Validation loss: 2.505944985115737

Epoch: 6| Step: 4
Training loss: 2.6665331886582284
Validation loss: 2.497714481780067

Epoch: 6| Step: 5
Training loss: 1.9940751052838244
Validation loss: 2.4693805267897284

Epoch: 6| Step: 6
Training loss: 2.2552358318763805
Validation loss: 2.498831169117856

Epoch: 6| Step: 7
Training loss: 2.0802911926892866
Validation loss: 2.4666738797985572

Epoch: 6| Step: 8
Training loss: 1.6112517737592666
Validation loss: 2.5042994158550553

Epoch: 6| Step: 9
Training loss: 1.8855268611284395
Validation loss: 2.454347518911234

Epoch: 6| Step: 10
Training loss: 1.694520756648116
Validation loss: 2.5100496071678378

Epoch: 6| Step: 11
Training loss: 1.719924664067336
Validation loss: 2.4743692858334927

Epoch: 6| Step: 12
Training loss: 1.6943706937627552
Validation loss: 2.496181378850108

Epoch: 6| Step: 13
Training loss: 2.127206778485881
Validation loss: 2.5273066281475756

Epoch: 406| Step: 0
Training loss: 2.5477379583119677
Validation loss: 2.469525253007757

Epoch: 6| Step: 1
Training loss: 1.5352614660339603
Validation loss: 2.429148358555516

Epoch: 6| Step: 2
Training loss: 1.8033076043447083
Validation loss: 2.488584567204034

Epoch: 6| Step: 3
Training loss: 1.8743282386312092
Validation loss: 2.484123881320324

Epoch: 6| Step: 4
Training loss: 2.0831895651484196
Validation loss: 2.4688760297700854

Epoch: 6| Step: 5
Training loss: 2.1710398872889707
Validation loss: 2.491247087066989

Epoch: 6| Step: 6
Training loss: 1.8331361866924076
Validation loss: 2.4792015212488727

Epoch: 6| Step: 7
Training loss: 2.402125998451989
Validation loss: 2.4844785794994695

Epoch: 6| Step: 8
Training loss: 1.8336874302003612
Validation loss: 2.4618032666705645

Epoch: 6| Step: 9
Training loss: 1.9600537500991109
Validation loss: 2.49056279995163

Epoch: 6| Step: 10
Training loss: 1.9912216537958018
Validation loss: 2.4827584545913974

Epoch: 6| Step: 11
Training loss: 1.4469855445503577
Validation loss: 2.454143316209822

Epoch: 6| Step: 12
Training loss: 1.2923771637148445
Validation loss: 2.5230360896373596

Epoch: 6| Step: 13
Training loss: 2.763049548278839
Validation loss: 2.51668841973237

Epoch: 407| Step: 0
Training loss: 1.9664914936780815
Validation loss: 2.4468786293423737

Epoch: 6| Step: 1
Training loss: 2.459604247289326
Validation loss: 2.443311826155514

Epoch: 6| Step: 2
Training loss: 2.2454992419734023
Validation loss: 2.475063688341898

Epoch: 6| Step: 3
Training loss: 2.1373867964186744
Validation loss: 2.4849171236930876

Epoch: 6| Step: 4
Training loss: 1.530713279237576
Validation loss: 2.502608432048855

Epoch: 6| Step: 5
Training loss: 1.9189921521398583
Validation loss: 2.4479770610186518

Epoch: 6| Step: 6
Training loss: 1.9878308698563696
Validation loss: 2.5098504318158974

Epoch: 6| Step: 7
Training loss: 2.3542479329443933
Validation loss: 2.4717685568113454

Epoch: 6| Step: 8
Training loss: 1.804787637153515
Validation loss: 2.458467804860183

Epoch: 6| Step: 9
Training loss: 1.5898513981215763
Validation loss: 2.468203364548127

Epoch: 6| Step: 10
Training loss: 2.1480828009759385
Validation loss: 2.490359349936129

Epoch: 6| Step: 11
Training loss: 1.9353652391557763
Validation loss: 2.446967917448893

Epoch: 6| Step: 12
Training loss: 1.7921787092131287
Validation loss: 2.4779133208983724

Epoch: 6| Step: 13
Training loss: 2.123408001715178
Validation loss: 2.4549636278306193

Epoch: 408| Step: 0
Training loss: 1.6668063899918097
Validation loss: 2.4690278347237316

Epoch: 6| Step: 1
Training loss: 2.1389960335101303
Validation loss: 2.4871640355346956

Epoch: 6| Step: 2
Training loss: 1.7822927802338977
Validation loss: 2.4769655588508246

Epoch: 6| Step: 3
Training loss: 2.640756908485356
Validation loss: 2.5151802148239466

Epoch: 6| Step: 4
Training loss: 2.341390413797797
Validation loss: 2.4800710878527594

Epoch: 6| Step: 5
Training loss: 1.1347277006289151
Validation loss: 2.4529027222212862

Epoch: 6| Step: 6
Training loss: 1.9321668504021512
Validation loss: 2.498178277725657

Epoch: 6| Step: 7
Training loss: 1.98403026560033
Validation loss: 2.477844029893132

Epoch: 6| Step: 8
Training loss: 1.6170408025723526
Validation loss: 2.4670976597615355

Epoch: 6| Step: 9
Training loss: 2.8338208246656302
Validation loss: 2.474444671823557

Epoch: 6| Step: 10
Training loss: 1.5803453292589862
Validation loss: 2.4812855006386068

Epoch: 6| Step: 11
Training loss: 1.5386317081055707
Validation loss: 2.534458981892688

Epoch: 6| Step: 12
Training loss: 1.710215655776755
Validation loss: 2.499477951343227

Epoch: 6| Step: 13
Training loss: 1.8758152778713464
Validation loss: 2.5104667962546694

Epoch: 409| Step: 0
Training loss: 1.502507339434224
Validation loss: 2.480298391416603

Epoch: 6| Step: 1
Training loss: 2.1097711261803975
Validation loss: 2.478451858352247

Epoch: 6| Step: 2
Training loss: 2.403567416536549
Validation loss: 2.4912634855852667

Epoch: 6| Step: 3
Training loss: 2.148263265307088
Validation loss: 2.4664614739942894

Epoch: 6| Step: 4
Training loss: 1.495370236740045
Validation loss: 2.4704299056492394

Epoch: 6| Step: 5
Training loss: 1.945376965783615
Validation loss: 2.4840163786426768

Epoch: 6| Step: 6
Training loss: 1.8632870080247117
Validation loss: 2.4926419274088167

Epoch: 6| Step: 7
Training loss: 1.3311563342459078
Validation loss: 2.4437636289732385

Epoch: 6| Step: 8
Training loss: 1.632112343810874
Validation loss: 2.4738677937769045

Epoch: 6| Step: 9
Training loss: 2.267725845243598
Validation loss: 2.4719341943407773

Epoch: 6| Step: 10
Training loss: 1.601045255175085
Validation loss: 2.546061402150467

Epoch: 6| Step: 11
Training loss: 1.774931694113413
Validation loss: 2.4963037833619595

Epoch: 6| Step: 12
Training loss: 2.5653386094415116
Validation loss: 2.4640416348199516

Epoch: 6| Step: 13
Training loss: 2.694439732874086
Validation loss: 2.497000383697791

Epoch: 410| Step: 0
Training loss: 1.6873758058411112
Validation loss: 2.5091048046656113

Epoch: 6| Step: 1
Training loss: 2.182642857616103
Validation loss: 2.4711890165730837

Epoch: 6| Step: 2
Training loss: 2.5917196945047962
Validation loss: 2.477064278602784

Epoch: 6| Step: 3
Training loss: 1.8511375229258846
Validation loss: 2.47187816961454

Epoch: 6| Step: 4
Training loss: 1.8271627257383276
Validation loss: 2.498281143326791

Epoch: 6| Step: 5
Training loss: 2.2274707748602327
Validation loss: 2.4738733690025136

Epoch: 6| Step: 6
Training loss: 1.6119413191666647
Validation loss: 2.4904686360335466

Epoch: 6| Step: 7
Training loss: 2.5333943644082013
Validation loss: 2.471840608898239

Epoch: 6| Step: 8
Training loss: 1.8890859471483823
Validation loss: 2.495331351947386

Epoch: 6| Step: 9
Training loss: 2.0185261038282207
Validation loss: 2.4785989866461207

Epoch: 6| Step: 10
Training loss: 1.693321429996755
Validation loss: 2.4669604821834543

Epoch: 6| Step: 11
Training loss: 1.6277822005494276
Validation loss: 2.5400543168906857

Epoch: 6| Step: 12
Training loss: 1.5026309464402599
Validation loss: 2.4589607942475116

Epoch: 6| Step: 13
Training loss: 2.232547893288114
Validation loss: 2.471373743662001

Epoch: 411| Step: 0
Training loss: 1.3597257315140456
Validation loss: 2.4918574126390034

Epoch: 6| Step: 1
Training loss: 1.7794267123405405
Validation loss: 2.4404566733215978

Epoch: 6| Step: 2
Training loss: 2.1682906178228563
Validation loss: 2.4548139462069503

Epoch: 6| Step: 3
Training loss: 1.8804704337276945
Validation loss: 2.494580629205198

Epoch: 6| Step: 4
Training loss: 1.9781098472171537
Validation loss: 2.5043418251629443

Epoch: 6| Step: 5
Training loss: 2.284157767971463
Validation loss: 2.4612062400267867

Epoch: 6| Step: 6
Training loss: 1.7414674511128603
Validation loss: 2.472971156797048

Epoch: 6| Step: 7
Training loss: 2.3375644695346813
Validation loss: 2.460284426762278

Epoch: 6| Step: 8
Training loss: 2.670402433377592
Validation loss: 2.5181375138545397

Epoch: 6| Step: 9
Training loss: 1.915699846258594
Validation loss: 2.4913838427031814

Epoch: 6| Step: 10
Training loss: 1.9487342614596017
Validation loss: 2.4942175122518058

Epoch: 6| Step: 11
Training loss: 1.8249199314058906
Validation loss: 2.470723313808419

Epoch: 6| Step: 12
Training loss: 1.7472936275360953
Validation loss: 2.4855224586722233

Epoch: 6| Step: 13
Training loss: 1.2089873823827952
Validation loss: 2.467830585925186

Epoch: 412| Step: 0
Training loss: 1.6783525706870956
Validation loss: 2.508813572804523

Epoch: 6| Step: 1
Training loss: 1.8148683005227093
Validation loss: 2.498995511465268

Epoch: 6| Step: 2
Training loss: 2.0149997658173304
Validation loss: 2.4537597852825472

Epoch: 6| Step: 3
Training loss: 2.041486096344372
Validation loss: 2.4361169171010877

Epoch: 6| Step: 4
Training loss: 1.7084459252199453
Validation loss: 2.482213990063676

Epoch: 6| Step: 5
Training loss: 1.2806384790473553
Validation loss: 2.473557647398273

Epoch: 6| Step: 6
Training loss: 2.5768773614658413
Validation loss: 2.4505544706830116

Epoch: 6| Step: 7
Training loss: 2.384925980608916
Validation loss: 2.4730119782225115

Epoch: 6| Step: 8
Training loss: 1.8888359748060919
Validation loss: 2.4752228392625297

Epoch: 6| Step: 9
Training loss: 2.5211922781468794
Validation loss: 2.5163753865230003

Epoch: 6| Step: 10
Training loss: 1.7885548900129493
Validation loss: 2.513007188203443

Epoch: 6| Step: 11
Training loss: 1.9194466981753442
Validation loss: 2.474568916624149

Epoch: 6| Step: 12
Training loss: 1.4888560068870913
Validation loss: 2.470629888990388

Epoch: 6| Step: 13
Training loss: 1.7679910925558797
Validation loss: 2.4493158489368474

Epoch: 413| Step: 0
Training loss: 1.8705386809810989
Validation loss: 2.5123367754892056

Epoch: 6| Step: 1
Training loss: 1.2038639326041367
Validation loss: 2.498106867270855

Epoch: 6| Step: 2
Training loss: 2.3685962274808827
Validation loss: 2.5004221200918297

Epoch: 6| Step: 3
Training loss: 2.4883291578247166
Validation loss: 2.4810946413137325

Epoch: 6| Step: 4
Training loss: 1.545942545663385
Validation loss: 2.45626640574232

Epoch: 6| Step: 5
Training loss: 1.9924775753889263
Validation loss: 2.455496772276634

Epoch: 6| Step: 6
Training loss: 1.6541204617418663
Validation loss: 2.465807206378855

Epoch: 6| Step: 7
Training loss: 1.3549435172326363
Validation loss: 2.4804905904748726

Epoch: 6| Step: 8
Training loss: 1.582338053776962
Validation loss: 2.4462251962016723

Epoch: 6| Step: 9
Training loss: 2.2842512898273966
Validation loss: 2.4876812093080694

Epoch: 6| Step: 10
Training loss: 2.1543433563945213
Validation loss: 2.5105441423648567

Epoch: 6| Step: 11
Training loss: 2.112604452406238
Validation loss: 2.4798167389714196

Epoch: 6| Step: 12
Training loss: 2.1783813063055835
Validation loss: 2.45984058326461

Epoch: 6| Step: 13
Training loss: 1.8397258226703503
Validation loss: 2.456547607012059

Epoch: 414| Step: 0
Training loss: 2.186683393371775
Validation loss: 2.4585933231494796

Epoch: 6| Step: 1
Training loss: 1.8934106403113504
Validation loss: 2.4968699477785554

Epoch: 6| Step: 2
Training loss: 1.2456312128305413
Validation loss: 2.4772741236632267

Epoch: 6| Step: 3
Training loss: 1.791039053827131
Validation loss: 2.5007592812001787

Epoch: 6| Step: 4
Training loss: 1.6074969249832383
Validation loss: 2.518308233341237

Epoch: 6| Step: 5
Training loss: 1.9270302705074844
Validation loss: 2.470048948213722

Epoch: 6| Step: 6
Training loss: 1.8339611047927986
Validation loss: 2.4860635360547407

Epoch: 6| Step: 7
Training loss: 1.7212765936365797
Validation loss: 2.534883200631463

Epoch: 6| Step: 8
Training loss: 2.1512929355519357
Validation loss: 2.4544036017198505

Epoch: 6| Step: 9
Training loss: 1.5271561061004544
Validation loss: 2.505616837023827

Epoch: 6| Step: 10
Training loss: 2.580983190163251
Validation loss: 2.481916725580855

Epoch: 6| Step: 11
Training loss: 1.9730000111405401
Validation loss: 2.529128601200785

Epoch: 6| Step: 12
Training loss: 1.980912078054002
Validation loss: 2.498791595378543

Epoch: 6| Step: 13
Training loss: 2.871391685753216
Validation loss: 2.4790485047490485

Epoch: 415| Step: 0
Training loss: 1.9837289794885036
Validation loss: 2.502834556896145

Epoch: 6| Step: 1
Training loss: 2.6108527653183375
Validation loss: 2.4975427795646614

Epoch: 6| Step: 2
Training loss: 2.080332107440082
Validation loss: 2.440783275902535

Epoch: 6| Step: 3
Training loss: 2.1448882036930392
Validation loss: 2.418835317367152

Epoch: 6| Step: 4
Training loss: 2.206875469820269
Validation loss: 2.504202615362732

Epoch: 6| Step: 5
Training loss: 1.4766580863571317
Validation loss: 2.4637945432543567

Epoch: 6| Step: 6
Training loss: 1.2717931697680647
Validation loss: 2.489082071432223

Epoch: 6| Step: 7
Training loss: 1.736451083486427
Validation loss: 2.4585700869203135

Epoch: 6| Step: 8
Training loss: 2.2732161056893125
Validation loss: 2.4786780728925413

Epoch: 6| Step: 9
Training loss: 2.1590850547090583
Validation loss: 2.48474045965957

Epoch: 6| Step: 10
Training loss: 1.7715730655427104
Validation loss: 2.44878612220286

Epoch: 6| Step: 11
Training loss: 1.6493279099762015
Validation loss: 2.5163241840026616

Epoch: 6| Step: 12
Training loss: 2.1366341694439983
Validation loss: 2.4417591058853185

Epoch: 6| Step: 13
Training loss: 1.4216230399774759
Validation loss: 2.4410548953323383

Epoch: 416| Step: 0
Training loss: 2.7401773850597735
Validation loss: 2.4909872273291445

Epoch: 6| Step: 1
Training loss: 2.029163168943969
Validation loss: 2.47831987675807

Epoch: 6| Step: 2
Training loss: 1.6787018478884224
Validation loss: 2.459920917691825

Epoch: 6| Step: 3
Training loss: 1.9489665814348707
Validation loss: 2.464860054277984

Epoch: 6| Step: 4
Training loss: 1.2954024721932156
Validation loss: 2.493133475454697

Epoch: 6| Step: 5
Training loss: 1.8011877962234821
Validation loss: 2.4731762351982636

Epoch: 6| Step: 6
Training loss: 2.535692437459665
Validation loss: 2.480861493651458

Epoch: 6| Step: 7
Training loss: 1.8599046866288917
Validation loss: 2.4648931211776133

Epoch: 6| Step: 8
Training loss: 2.013877643539193
Validation loss: 2.46676947582589

Epoch: 6| Step: 9
Training loss: 1.5248816272257926
Validation loss: 2.470468588310001

Epoch: 6| Step: 10
Training loss: 1.9956545710560554
Validation loss: 2.476627244326571

Epoch: 6| Step: 11
Training loss: 1.6931585877031108
Validation loss: 2.4627105747785323

Epoch: 6| Step: 12
Training loss: 1.6679878323108903
Validation loss: 2.4999528972741425

Epoch: 6| Step: 13
Training loss: 1.9374681900858668
Validation loss: 2.4789765804634167

Epoch: 417| Step: 0
Training loss: 2.0905022555186035
Validation loss: 2.4578678412730954

Epoch: 6| Step: 1
Training loss: 2.4731276124637236
Validation loss: 2.4665731810049105

Epoch: 6| Step: 2
Training loss: 2.0237078983105063
Validation loss: 2.4672301133492143

Epoch: 6| Step: 3
Training loss: 2.1499719307420073
Validation loss: 2.469405833130022

Epoch: 6| Step: 4
Training loss: 1.9106941323942768
Validation loss: 2.428209476529992

Epoch: 6| Step: 5
Training loss: 1.9229249024826134
Validation loss: 2.5181694800362724

Epoch: 6| Step: 6
Training loss: 1.9054733945720352
Validation loss: 2.4526358736579423

Epoch: 6| Step: 7
Training loss: 1.4907652782915615
Validation loss: 2.4985138249373824

Epoch: 6| Step: 8
Training loss: 2.1835511669176344
Validation loss: 2.4399369213430746

Epoch: 6| Step: 9
Training loss: 2.474585673783992
Validation loss: 2.5144735758606167

Epoch: 6| Step: 10
Training loss: 1.018175590386944
Validation loss: 2.4931687637365174

Epoch: 6| Step: 11
Training loss: 1.6457258583742405
Validation loss: 2.489375052481939

Epoch: 6| Step: 12
Training loss: 1.3491681891831935
Validation loss: 2.458629616023349

Epoch: 6| Step: 13
Training loss: 2.109360758415465
Validation loss: 2.509077053673939

Epoch: 418| Step: 0
Training loss: 1.3690312623494174
Validation loss: 2.4930870008542607

Epoch: 6| Step: 1
Training loss: 1.7941086413894098
Validation loss: 2.5169653190970998

Epoch: 6| Step: 2
Training loss: 1.640269649714093
Validation loss: 2.4668330097676976

Epoch: 6| Step: 3
Training loss: 1.5063021512322436
Validation loss: 2.4776211687790837

Epoch: 6| Step: 4
Training loss: 2.1790157910177896
Validation loss: 2.5140641672829713

Epoch: 6| Step: 5
Training loss: 1.4792310226123122
Validation loss: 2.465521584074539

Epoch: 6| Step: 6
Training loss: 1.996612003779674
Validation loss: 2.461303690984765

Epoch: 6| Step: 7
Training loss: 2.8540319332055195
Validation loss: 2.473064161449096

Epoch: 6| Step: 8
Training loss: 1.5155165328706972
Validation loss: 2.4827267847197447

Epoch: 6| Step: 9
Training loss: 2.186872337756154
Validation loss: 2.468582719831568

Epoch: 6| Step: 10
Training loss: 2.4763926739633155
Validation loss: 2.47964120860907

Epoch: 6| Step: 11
Training loss: 1.7769722015705307
Validation loss: 2.520986657262629

Epoch: 6| Step: 12
Training loss: 1.6891704344868177
Validation loss: 2.469986617943965

Epoch: 6| Step: 13
Training loss: 1.637756892430129
Validation loss: 2.458394318024592

Epoch: 419| Step: 0
Training loss: 2.4065738806797863
Validation loss: 2.474209270261426

Epoch: 6| Step: 1
Training loss: 2.1190401536882484
Validation loss: 2.4706932115986198

Epoch: 6| Step: 2
Training loss: 2.106493415690032
Validation loss: 2.5287784991801243

Epoch: 6| Step: 3
Training loss: 1.2204952459715475
Validation loss: 2.49070545853538

Epoch: 6| Step: 4
Training loss: 1.4478998892198593
Validation loss: 2.489793581603094

Epoch: 6| Step: 5
Training loss: 1.8522398792041315
Validation loss: 2.4341655438633563

Epoch: 6| Step: 6
Training loss: 1.9056513111954243
Validation loss: 2.460178914749213

Epoch: 6| Step: 7
Training loss: 1.9191846547820772
Validation loss: 2.4651045119097925

Epoch: 6| Step: 8
Training loss: 2.130301761832362
Validation loss: 2.47285956308104

Epoch: 6| Step: 9
Training loss: 1.7074313535058487
Validation loss: 2.480647869908191

Epoch: 6| Step: 10
Training loss: 1.6633525641380158
Validation loss: 2.4912426965822125

Epoch: 6| Step: 11
Training loss: 1.8752371320182546
Validation loss: 2.481634914498885

Epoch: 6| Step: 12
Training loss: 2.4887020408036777
Validation loss: 2.5280551809647225

Epoch: 6| Step: 13
Training loss: 2.032799232668321
Validation loss: 2.4242702086689243

Epoch: 420| Step: 0
Training loss: 1.7511453286583354
Validation loss: 2.5051543542695622

Epoch: 6| Step: 1
Training loss: 1.6519688390605467
Validation loss: 2.458239738333119

Epoch: 6| Step: 2
Training loss: 1.9363206535042836
Validation loss: 2.4714764106498066

Epoch: 6| Step: 3
Training loss: 1.9078496880933224
Validation loss: 2.4825805201315205

Epoch: 6| Step: 4
Training loss: 1.702519720470247
Validation loss: 2.4516485737994294

Epoch: 6| Step: 5
Training loss: 2.1709335811651553
Validation loss: 2.4560960927830533

Epoch: 6| Step: 6
Training loss: 2.4170544302275987
Validation loss: 2.5036915698328843

Epoch: 6| Step: 7
Training loss: 2.6979689623699565
Validation loss: 2.457539551399678

Epoch: 6| Step: 8
Training loss: 1.9133751705571083
Validation loss: 2.44231537172695

Epoch: 6| Step: 9
Training loss: 2.12875931258973
Validation loss: 2.4452279008200133

Epoch: 6| Step: 10
Training loss: 1.430957501103579
Validation loss: 2.457612327335539

Epoch: 6| Step: 11
Training loss: 2.0949207278164397
Validation loss: 2.4900950992506643

Epoch: 6| Step: 12
Training loss: 1.7223816391096916
Validation loss: 2.5275990062967497

Epoch: 6| Step: 13
Training loss: 1.6895916303187117
Validation loss: 2.468906263164883

Epoch: 421| Step: 0
Training loss: 1.3687845547986888
Validation loss: 2.497056670213017

Epoch: 6| Step: 1
Training loss: 1.2736469868791271
Validation loss: 2.4677889193979596

Epoch: 6| Step: 2
Training loss: 1.937474035273665
Validation loss: 2.5079051090652764

Epoch: 6| Step: 3
Training loss: 1.7247942677423842
Validation loss: 2.451119145161748

Epoch: 6| Step: 4
Training loss: 2.0176455993300335
Validation loss: 2.444493465578395

Epoch: 6| Step: 5
Training loss: 1.793802570277123
Validation loss: 2.4649612317893745

Epoch: 6| Step: 6
Training loss: 2.0192310102693183
Validation loss: 2.4732857446719603

Epoch: 6| Step: 7
Training loss: 1.6444335328562032
Validation loss: 2.453004602994755

Epoch: 6| Step: 8
Training loss: 2.492319324797738
Validation loss: 2.430455568388956

Epoch: 6| Step: 9
Training loss: 1.663622229274925
Validation loss: 2.49423095216812

Epoch: 6| Step: 10
Training loss: 2.0584436466860176
Validation loss: 2.445738461933823

Epoch: 6| Step: 11
Training loss: 2.3530052043113105
Validation loss: 2.4020887728950053

Epoch: 6| Step: 12
Training loss: 1.895702071064802
Validation loss: 2.446672544861535

Epoch: 6| Step: 13
Training loss: 2.7926768757292106
Validation loss: 2.4614919928146777

Epoch: 422| Step: 0
Training loss: 2.3183210156038316
Validation loss: 2.5306306327085872

Epoch: 6| Step: 1
Training loss: 1.8447611347632207
Validation loss: 2.50333093731091

Epoch: 6| Step: 2
Training loss: 1.8577407579229324
Validation loss: 2.4517190558217647

Epoch: 6| Step: 3
Training loss: 1.469295887247227
Validation loss: 2.4600903604017246

Epoch: 6| Step: 4
Training loss: 2.370206211148709
Validation loss: 2.4216572012934154

Epoch: 6| Step: 5
Training loss: 1.9873133853656404
Validation loss: 2.478697545156079

Epoch: 6| Step: 6
Training loss: 1.5882113087158183
Validation loss: 2.4641537646811824

Epoch: 6| Step: 7
Training loss: 1.7112836095770994
Validation loss: 2.4640023232396997

Epoch: 6| Step: 8
Training loss: 1.7535879275311888
Validation loss: 2.5139477075028163

Epoch: 6| Step: 9
Training loss: 1.9340872086761678
Validation loss: 2.4235507382333434

Epoch: 6| Step: 10
Training loss: 1.7183614118164645
Validation loss: 2.4965850918048447

Epoch: 6| Step: 11
Training loss: 1.3692274297386664
Validation loss: 2.447073662797927

Epoch: 6| Step: 12
Training loss: 2.7658923176169727
Validation loss: 2.4654654141493864

Epoch: 6| Step: 13
Training loss: 1.6982981325469122
Validation loss: 2.436512301290893

Epoch: 423| Step: 0
Training loss: 1.7354628441915245
Validation loss: 2.4820990822741926

Epoch: 6| Step: 1
Training loss: 1.6079935607686333
Validation loss: 2.4587515664153656

Epoch: 6| Step: 2
Training loss: 2.2228070628618437
Validation loss: 2.4521270837405087

Epoch: 6| Step: 3
Training loss: 2.4429902088282933
Validation loss: 2.534988724161508

Epoch: 6| Step: 4
Training loss: 1.8262691369904593
Validation loss: 2.530257785735871

Epoch: 6| Step: 5
Training loss: 2.0402685815119024
Validation loss: 2.495167901531712

Epoch: 6| Step: 6
Training loss: 1.7494409894473353
Validation loss: 2.486007589471562

Epoch: 6| Step: 7
Training loss: 1.7311926849462769
Validation loss: 2.4872949825018793

Epoch: 6| Step: 8
Training loss: 2.1919834512378253
Validation loss: 2.5059900918623983

Epoch: 6| Step: 9
Training loss: 1.9452016408194435
Validation loss: 2.4492850911165487

Epoch: 6| Step: 10
Training loss: 1.5517190936799476
Validation loss: 2.497011150552918

Epoch: 6| Step: 11
Training loss: 2.2371879956814626
Validation loss: 2.471096203302529

Epoch: 6| Step: 12
Training loss: 1.7712960779284797
Validation loss: 2.498593731644673

Epoch: 6| Step: 13
Training loss: 1.7959262499333275
Validation loss: 2.455894328861229

Epoch: 424| Step: 0
Training loss: 2.398272977771593
Validation loss: 2.4611237038313867

Epoch: 6| Step: 1
Training loss: 1.8079281070180513
Validation loss: 2.5215712551771348

Epoch: 6| Step: 2
Training loss: 1.5832432001793997
Validation loss: 2.47193830643514

Epoch: 6| Step: 3
Training loss: 1.5105153895594534
Validation loss: 2.4396660490961612

Epoch: 6| Step: 4
Training loss: 2.2834113346708302
Validation loss: 2.4179608797843875

Epoch: 6| Step: 5
Training loss: 1.7587969926467875
Validation loss: 2.498733749474286

Epoch: 6| Step: 6
Training loss: 1.7364502596729448
Validation loss: 2.490752953314501

Epoch: 6| Step: 7
Training loss: 1.479609418700127
Validation loss: 2.455443330340684

Epoch: 6| Step: 8
Training loss: 1.7131085143049338
Validation loss: 2.4565750215761826

Epoch: 6| Step: 9
Training loss: 2.0945544618987264
Validation loss: 2.465470261786195

Epoch: 6| Step: 10
Training loss: 2.2067834225486753
Validation loss: 2.4486290482695963

Epoch: 6| Step: 11
Training loss: 2.585353266706868
Validation loss: 2.4960004551671884

Epoch: 6| Step: 12
Training loss: 1.5644541917813934
Validation loss: 2.496125148085488

Epoch: 6| Step: 13
Training loss: 2.2909852373037993
Validation loss: 2.4940886391421273

Epoch: 425| Step: 0
Training loss: 1.3358026626089752
Validation loss: 2.476205723699059

Epoch: 6| Step: 1
Training loss: 1.9853663333731764
Validation loss: 2.486808088479131

Epoch: 6| Step: 2
Training loss: 2.1489868571437363
Validation loss: 2.4464814341645615

Epoch: 6| Step: 3
Training loss: 2.120885905555399
Validation loss: 2.4765563149059737

Epoch: 6| Step: 4
Training loss: 1.7844821984580763
Validation loss: 2.446695838548216

Epoch: 6| Step: 5
Training loss: 2.4378224673077806
Validation loss: 2.470830080258851

Epoch: 6| Step: 6
Training loss: 1.8891655198901307
Validation loss: 2.508888130235952

Epoch: 6| Step: 7
Training loss: 1.649955159358298
Validation loss: 2.4530248883455963

Epoch: 6| Step: 8
Training loss: 1.7186893799235117
Validation loss: 2.4308180913830113

Epoch: 6| Step: 9
Training loss: 2.387522161091316
Validation loss: 2.448031777053836

Epoch: 6| Step: 10
Training loss: 1.7978624367372116
Validation loss: 2.491138036824614

Epoch: 6| Step: 11
Training loss: 1.4312928572322516
Validation loss: 2.450856481039051

Epoch: 6| Step: 12
Training loss: 2.039675448723818
Validation loss: 2.526529580430324

Epoch: 6| Step: 13
Training loss: 1.999644903607711
Validation loss: 2.4583338118216034

Epoch: 426| Step: 0
Training loss: 1.8050549946276904
Validation loss: 2.444154407156132

Epoch: 6| Step: 1
Training loss: 2.3201278458918937
Validation loss: 2.4706292591388386

Epoch: 6| Step: 2
Training loss: 2.3868793739424605
Validation loss: 2.4948289931663505

Epoch: 6| Step: 3
Training loss: 1.7306040416039477
Validation loss: 2.4829687997974097

Epoch: 6| Step: 4
Training loss: 1.9500814298598166
Validation loss: 2.499982942502476

Epoch: 6| Step: 5
Training loss: 1.4296211268959638
Validation loss: 2.5325557835353094

Epoch: 6| Step: 6
Training loss: 1.9165843310155406
Validation loss: 2.5199715661611073

Epoch: 6| Step: 7
Training loss: 1.407094574667718
Validation loss: 2.487514205630692

Epoch: 6| Step: 8
Training loss: 1.6324367547161494
Validation loss: 2.5227998429286376

Epoch: 6| Step: 9
Training loss: 1.7056981518282026
Validation loss: 2.473972028209732

Epoch: 6| Step: 10
Training loss: 1.9945978878975876
Validation loss: 2.4574298957494047

Epoch: 6| Step: 11
Training loss: 1.7038008722447477
Validation loss: 2.4379287555090245

Epoch: 6| Step: 12
Training loss: 2.604384349625871
Validation loss: 2.4568119442571317

Epoch: 6| Step: 13
Training loss: 2.6098553792121955
Validation loss: 2.452393172283163

Epoch: 427| Step: 0
Training loss: 1.6638250047154983
Validation loss: 2.5067644822104485

Epoch: 6| Step: 1
Training loss: 2.042396479846627
Validation loss: 2.4999446267999477

Epoch: 6| Step: 2
Training loss: 2.0084870982630583
Validation loss: 2.4303919547747834

Epoch: 6| Step: 3
Training loss: 2.1555327176111
Validation loss: 2.4789800779658058

Epoch: 6| Step: 4
Training loss: 2.20155549112562
Validation loss: 2.487569586549922

Epoch: 6| Step: 5
Training loss: 2.5929577088472935
Validation loss: 2.472827588116234

Epoch: 6| Step: 6
Training loss: 1.512810995112606
Validation loss: 2.496442941733382

Epoch: 6| Step: 7
Training loss: 1.6290312428625082
Validation loss: 2.4720946885073247

Epoch: 6| Step: 8
Training loss: 1.5191731558092854
Validation loss: 2.465507805173137

Epoch: 6| Step: 9
Training loss: 1.7388428198846508
Validation loss: 2.528563120263204

Epoch: 6| Step: 10
Training loss: 1.3936235053988253
Validation loss: 2.5050112684510304

Epoch: 6| Step: 11
Training loss: 1.9673134543020914
Validation loss: 2.439531006543524

Epoch: 6| Step: 12
Training loss: 2.155599744852872
Validation loss: 2.4645538869496413

Epoch: 6| Step: 13
Training loss: 2.2417327738755515
Validation loss: 2.429647216842784

Epoch: 428| Step: 0
Training loss: 1.6517358116948158
Validation loss: 2.4536333646173265

Epoch: 6| Step: 1
Training loss: 1.6243108608591763
Validation loss: 2.484046197618482

Epoch: 6| Step: 2
Training loss: 2.0415197306587425
Validation loss: 2.441330784178908

Epoch: 6| Step: 3
Training loss: 1.4177856327073142
Validation loss: 2.446316400773548

Epoch: 6| Step: 4
Training loss: 1.422046525271885
Validation loss: 2.4504254626945587

Epoch: 6| Step: 5
Training loss: 2.1659120443772113
Validation loss: 2.4516345245213977

Epoch: 6| Step: 6
Training loss: 2.005582647354901
Validation loss: 2.4266319554070916

Epoch: 6| Step: 7
Training loss: 1.931922082095878
Validation loss: 2.5029714440184296

Epoch: 6| Step: 8
Training loss: 2.128845437957429
Validation loss: 2.492681192535601

Epoch: 6| Step: 9
Training loss: 1.7994722917336186
Validation loss: 2.4191931402526565

Epoch: 6| Step: 10
Training loss: 2.917059208930419
Validation loss: 2.4427705382482037

Epoch: 6| Step: 11
Training loss: 1.7342770523044408
Validation loss: 2.4480299742549425

Epoch: 6| Step: 12
Training loss: 1.9598394108203951
Validation loss: 2.4604558762170816

Epoch: 6| Step: 13
Training loss: 1.5229531325093988
Validation loss: 2.4738450005073185

Epoch: 429| Step: 0
Training loss: 2.758357439963422
Validation loss: 2.4225347384460614

Epoch: 6| Step: 1
Training loss: 1.0500085535155175
Validation loss: 2.4022715045034557

Epoch: 6| Step: 2
Training loss: 2.1711681505587004
Validation loss: 2.4471714869272616

Epoch: 6| Step: 3
Training loss: 1.8896413088326454
Validation loss: 2.46310306708324

Epoch: 6| Step: 4
Training loss: 2.1034379836747674
Validation loss: 2.4782048830633285

Epoch: 6| Step: 5
Training loss: 1.408684890176654
Validation loss: 2.5005755510564542

Epoch: 6| Step: 6
Training loss: 1.9158768754313895
Validation loss: 2.4516450059356227

Epoch: 6| Step: 7
Training loss: 1.2067345313137192
Validation loss: 2.4627032639418096

Epoch: 6| Step: 8
Training loss: 1.8856665795141727
Validation loss: 2.4670334124980355

Epoch: 6| Step: 9
Training loss: 1.7714044285812227
Validation loss: 2.478529049212278

Epoch: 6| Step: 10
Training loss: 1.3972555525419077
Validation loss: 2.519697150068849

Epoch: 6| Step: 11
Training loss: 1.9296608741560413
Validation loss: 2.471431319191695

Epoch: 6| Step: 12
Training loss: 2.273558098914163
Validation loss: 2.4913983937874975

Epoch: 6| Step: 13
Training loss: 2.351146065474774
Validation loss: 2.4387087931273026

Epoch: 430| Step: 0
Training loss: 2.4847360024787664
Validation loss: 2.401467472375376

Epoch: 6| Step: 1
Training loss: 1.7944978995705485
Validation loss: 2.4299172514807945

Epoch: 6| Step: 2
Training loss: 2.067464460185088
Validation loss: 2.448066078152253

Epoch: 6| Step: 3
Training loss: 1.5919035144494174
Validation loss: 2.4756823688645904

Epoch: 6| Step: 4
Training loss: 1.9511432817443204
Validation loss: 2.4763988294444044

Epoch: 6| Step: 5
Training loss: 1.6568350568321808
Validation loss: 2.475883602265533

Epoch: 6| Step: 6
Training loss: 2.4472678657150055
Validation loss: 2.5099837266371336

Epoch: 6| Step: 7
Training loss: 1.9200209691968892
Validation loss: 2.469779552125993

Epoch: 6| Step: 8
Training loss: 1.8054635497232483
Validation loss: 2.4787830736098626

Epoch: 6| Step: 9
Training loss: 1.6811406436793344
Validation loss: 2.4956708413058144

Epoch: 6| Step: 10
Training loss: 1.6269453582028748
Validation loss: 2.4673379255630667

Epoch: 6| Step: 11
Training loss: 2.181027345852342
Validation loss: 2.500076672188402

Epoch: 6| Step: 12
Training loss: 1.6136586484457032
Validation loss: 2.479568882784049

Epoch: 6| Step: 13
Training loss: 1.7370184608449157
Validation loss: 2.4511691332144268

Epoch: 431| Step: 0
Training loss: 2.209297137797135
Validation loss: 2.470644407698567

Epoch: 6| Step: 1
Training loss: 1.6903528893543815
Validation loss: 2.437801616929654

Epoch: 6| Step: 2
Training loss: 2.137096420469156
Validation loss: 2.4887599385781485

Epoch: 6| Step: 3
Training loss: 1.6261322771722257
Validation loss: 2.4653621552458356

Epoch: 6| Step: 4
Training loss: 1.6290122164648126
Validation loss: 2.444406227119614

Epoch: 6| Step: 5
Training loss: 1.8253216891064803
Validation loss: 2.500152097710346

Epoch: 6| Step: 6
Training loss: 1.6725068369042853
Validation loss: 2.4228212125742985

Epoch: 6| Step: 7
Training loss: 2.2674765551783973
Validation loss: 2.468196831325958

Epoch: 6| Step: 8
Training loss: 2.221852886449379
Validation loss: 2.4625740345799194

Epoch: 6| Step: 9
Training loss: 1.0535390747312003
Validation loss: 2.438577029615673

Epoch: 6| Step: 10
Training loss: 1.4660756538719195
Validation loss: 2.5159984533836433

Epoch: 6| Step: 11
Training loss: 1.9886499449030834
Validation loss: 2.472881314736585

Epoch: 6| Step: 12
Training loss: 2.7530666072338255
Validation loss: 2.494526189619719

Epoch: 6| Step: 13
Training loss: 1.5200310598512774
Validation loss: 2.4733804331973444

Epoch: 432| Step: 0
Training loss: 1.867600909957615
Validation loss: 2.4449749339812046

Epoch: 6| Step: 1
Training loss: 2.116090046695527
Validation loss: 2.429884248798395

Epoch: 6| Step: 2
Training loss: 1.9981832000971564
Validation loss: 2.4935626469259717

Epoch: 6| Step: 3
Training loss: 1.3548940710067676
Validation loss: 2.4707374304158964

Epoch: 6| Step: 4
Training loss: 1.8427610169493978
Validation loss: 2.4800031374037537

Epoch: 6| Step: 5
Training loss: 2.0495625098868278
Validation loss: 2.486222351988244

Epoch: 6| Step: 6
Training loss: 1.9117306576799535
Validation loss: 2.494622993577442

Epoch: 6| Step: 7
Training loss: 2.1148383702412987
Validation loss: 2.415186182072909

Epoch: 6| Step: 8
Training loss: 1.6014792816127266
Validation loss: 2.48037396308726

Epoch: 6| Step: 9
Training loss: 2.110286600436117
Validation loss: 2.4758728895311495

Epoch: 6| Step: 10
Training loss: 2.065964188190586
Validation loss: 2.5031283115276315

Epoch: 6| Step: 11
Training loss: 1.8754213495484915
Validation loss: 2.4843954522860874

Epoch: 6| Step: 12
Training loss: 1.889711837296414
Validation loss: 2.4812458072080465

Epoch: 6| Step: 13
Training loss: 1.5407693760445982
Validation loss: 2.4323842582901207

Epoch: 433| Step: 0
Training loss: 2.0538022995533516
Validation loss: 2.4650312062858486

Epoch: 6| Step: 1
Training loss: 1.4326816744550108
Validation loss: 2.4884025209464586

Epoch: 6| Step: 2
Training loss: 1.5622010517238427
Validation loss: 2.502553574476016

Epoch: 6| Step: 3
Training loss: 2.6798232686428536
Validation loss: 2.486507821876909

Epoch: 6| Step: 4
Training loss: 2.3505110813927255
Validation loss: 2.473027440801953

Epoch: 6| Step: 5
Training loss: 2.0522818170851393
Validation loss: 2.444397707352824

Epoch: 6| Step: 6
Training loss: 1.798066117612382
Validation loss: 2.4689323531364726

Epoch: 6| Step: 7
Training loss: 1.4086838746819892
Validation loss: 2.465693475674992

Epoch: 6| Step: 8
Training loss: 1.9312759224840141
Validation loss: 2.4483863758688833

Epoch: 6| Step: 9
Training loss: 1.8557826368958334
Validation loss: 2.484551900513301

Epoch: 6| Step: 10
Training loss: 1.6254907013865658
Validation loss: 2.460581137850855

Epoch: 6| Step: 11
Training loss: 1.930319184607748
Validation loss: 2.5289527741668607

Epoch: 6| Step: 12
Training loss: 1.7659840176594641
Validation loss: 2.4402215518589316

Epoch: 6| Step: 13
Training loss: 1.9858852856822833
Validation loss: 2.493179260767731

Epoch: 434| Step: 0
Training loss: 1.8601114433496446
Validation loss: 2.4783827758364407

Epoch: 6| Step: 1
Training loss: 2.658569669432295
Validation loss: 2.4527698722490867

Epoch: 6| Step: 2
Training loss: 1.8319916873393474
Validation loss: 2.5093078663146673

Epoch: 6| Step: 3
Training loss: 1.2852725640159972
Validation loss: 2.5000255009160703

Epoch: 6| Step: 4
Training loss: 1.8873224339109247
Validation loss: 2.5049272286184583

Epoch: 6| Step: 5
Training loss: 1.9814590295699424
Validation loss: 2.451030737302522

Epoch: 6| Step: 6
Training loss: 1.424201738399285
Validation loss: 2.4654943615328158

Epoch: 6| Step: 7
Training loss: 1.4830900553538306
Validation loss: 2.493346584686373

Epoch: 6| Step: 8
Training loss: 1.8259811220143765
Validation loss: 2.476522521763847

Epoch: 6| Step: 9
Training loss: 1.2407875089123637
Validation loss: 2.436808729738718

Epoch: 6| Step: 10
Training loss: 2.0590199096369246
Validation loss: 2.4820074924463227

Epoch: 6| Step: 11
Training loss: 2.116420480223362
Validation loss: 2.4933988142696317

Epoch: 6| Step: 12
Training loss: 2.1311856167820555
Validation loss: 2.4445294287418213

Epoch: 6| Step: 13
Training loss: 2.7757101713915318
Validation loss: 2.461134533889624

Epoch: 435| Step: 0
Training loss: 2.1102449106842665
Validation loss: 2.441059750486925

Epoch: 6| Step: 1
Training loss: 2.1800219644962375
Validation loss: 2.5012525497078903

Epoch: 6| Step: 2
Training loss: 1.797655151844311
Validation loss: 2.4955880971756885

Epoch: 6| Step: 3
Training loss: 1.3438182636043532
Validation loss: 2.4696062990578915

Epoch: 6| Step: 4
Training loss: 1.1256493707588933
Validation loss: 2.47929777165995

Epoch: 6| Step: 5
Training loss: 2.5363013645195673
Validation loss: 2.4344361156462937

Epoch: 6| Step: 6
Training loss: 1.3437541695463564
Validation loss: 2.441418567782266

Epoch: 6| Step: 7
Training loss: 1.736507651077405
Validation loss: 2.4231097040246916

Epoch: 6| Step: 8
Training loss: 2.3269474200450464
Validation loss: 2.466023401689312

Epoch: 6| Step: 9
Training loss: 2.7026634042692264
Validation loss: 2.4581301691926702

Epoch: 6| Step: 10
Training loss: 1.6298758642049693
Validation loss: 2.4473141817460617

Epoch: 6| Step: 11
Training loss: 1.6118330470731026
Validation loss: 2.4571437075998293

Epoch: 6| Step: 12
Training loss: 1.9647514063709612
Validation loss: 2.457966706426591

Epoch: 6| Step: 13
Training loss: 1.2761885993078836
Validation loss: 2.464139705031626

Epoch: 436| Step: 0
Training loss: 1.5996664593656649
Validation loss: 2.479418170925685

Epoch: 6| Step: 1
Training loss: 1.3702185464835759
Validation loss: 2.4547982362711567

Epoch: 6| Step: 2
Training loss: 1.4370718608666297
Validation loss: 2.4460019748864807

Epoch: 6| Step: 3
Training loss: 1.7335226352953395
Validation loss: 2.460970497723975

Epoch: 6| Step: 4
Training loss: 1.6625502614619527
Validation loss: 2.4431018777764146

Epoch: 6| Step: 5
Training loss: 1.8748523654035605
Validation loss: 2.4461331361744203

Epoch: 6| Step: 6
Training loss: 2.1063303131276654
Validation loss: 2.476467143283894

Epoch: 6| Step: 7
Training loss: 1.8838844888187583
Validation loss: 2.456958075482239

Epoch: 6| Step: 8
Training loss: 2.2341137713334382
Validation loss: 2.4377224865077647

Epoch: 6| Step: 9
Training loss: 1.9329425370209976
Validation loss: 2.474467359073718

Epoch: 6| Step: 10
Training loss: 2.544308357912146
Validation loss: 2.4700883607577078

Epoch: 6| Step: 11
Training loss: 1.7090829352159629
Validation loss: 2.476533894198827

Epoch: 6| Step: 12
Training loss: 2.011826240112215
Validation loss: 2.477241965070411

Epoch: 6| Step: 13
Training loss: 2.290638664173625
Validation loss: 2.452577173279618

Epoch: 437| Step: 0
Training loss: 2.147520778320119
Validation loss: 2.4784555691581933

Epoch: 6| Step: 1
Training loss: 2.0657944233215146
Validation loss: 2.4555432911949566

Epoch: 6| Step: 2
Training loss: 2.216885172560061
Validation loss: 2.460144940410084

Epoch: 6| Step: 3
Training loss: 2.650615325600658
Validation loss: 2.5025286663377546

Epoch: 6| Step: 4
Training loss: 1.8282320692220353
Validation loss: 2.4691762144797265

Epoch: 6| Step: 5
Training loss: 1.8208255945153258
Validation loss: 2.474499915297619

Epoch: 6| Step: 6
Training loss: 1.7627254747679095
Validation loss: 2.452848522458826

Epoch: 6| Step: 7
Training loss: 1.6972925954416391
Validation loss: 2.4363309803578375

Epoch: 6| Step: 8
Training loss: 1.5345707910242508
Validation loss: 2.4395091398514115

Epoch: 6| Step: 9
Training loss: 1.239761094028492
Validation loss: 2.4763125886832174

Epoch: 6| Step: 10
Training loss: 1.9730046634995981
Validation loss: 2.454058918035363

Epoch: 6| Step: 11
Training loss: 1.8312958825518322
Validation loss: 2.460962006654745

Epoch: 6| Step: 12
Training loss: 1.8126216716047885
Validation loss: 2.4774497079095403

Epoch: 6| Step: 13
Training loss: 1.955970278597982
Validation loss: 2.443929974992213

Epoch: 438| Step: 0
Training loss: 2.3757244812073455
Validation loss: 2.4578678590046796

Epoch: 6| Step: 1
Training loss: 1.7538456216031397
Validation loss: 2.4472286445256506

Epoch: 6| Step: 2
Training loss: 1.7052702385865082
Validation loss: 2.458650793396601

Epoch: 6| Step: 3
Training loss: 1.6688818833031536
Validation loss: 2.479994091778789

Epoch: 6| Step: 4
Training loss: 1.5183336103141076
Validation loss: 2.4584038044579497

Epoch: 6| Step: 5
Training loss: 1.4808694503550106
Validation loss: 2.4794513729873797

Epoch: 6| Step: 6
Training loss: 2.2410979597749887
Validation loss: 2.4717132419187497

Epoch: 6| Step: 7
Training loss: 1.501882008231802
Validation loss: 2.4384177536118297

Epoch: 6| Step: 8
Training loss: 2.495066543333877
Validation loss: 2.4558519420512175

Epoch: 6| Step: 9
Training loss: 1.7761587363521232
Validation loss: 2.4861972632205496

Epoch: 6| Step: 10
Training loss: 1.6733037245107865
Validation loss: 2.464781715584002

Epoch: 6| Step: 11
Training loss: 1.4392620565450471
Validation loss: 2.4775737492426226

Epoch: 6| Step: 12
Training loss: 2.1463125366626827
Validation loss: 2.4869034208258527

Epoch: 6| Step: 13
Training loss: 2.225782933875559
Validation loss: 2.4780610568852213

Epoch: 439| Step: 0
Training loss: 1.9019289289006014
Validation loss: 2.4383755603018566

Epoch: 6| Step: 1
Training loss: 1.704293016756785
Validation loss: 2.4642911780825503

Epoch: 6| Step: 2
Training loss: 2.459624894084134
Validation loss: 2.434799019915248

Epoch: 6| Step: 3
Training loss: 2.019212826799317
Validation loss: 2.4757807680426325

Epoch: 6| Step: 4
Training loss: 1.7772285172552138
Validation loss: 2.476813238469475

Epoch: 6| Step: 5
Training loss: 1.8912974020998647
Validation loss: 2.4514653146132575

Epoch: 6| Step: 6
Training loss: 1.8067692891262124
Validation loss: 2.468711514196994

Epoch: 6| Step: 7
Training loss: 2.304514836454565
Validation loss: 2.4522255687622425

Epoch: 6| Step: 8
Training loss: 1.599998641013522
Validation loss: 2.4928075653144033

Epoch: 6| Step: 9
Training loss: 1.4229477409369256
Validation loss: 2.483020175455647

Epoch: 6| Step: 10
Training loss: 1.436506923474292
Validation loss: 2.416339772243753

Epoch: 6| Step: 11
Training loss: 1.5727585298315692
Validation loss: 2.468542451391051

Epoch: 6| Step: 12
Training loss: 2.26427982779963
Validation loss: 2.4630491917015616

Epoch: 6| Step: 13
Training loss: 2.059656552878224
Validation loss: 2.5004382774765848

Epoch: 440| Step: 0
Training loss: 1.5441034321705016
Validation loss: 2.44517474189604

Epoch: 6| Step: 1
Training loss: 1.6851115808415995
Validation loss: 2.48411559220234

Epoch: 6| Step: 2
Training loss: 1.95281955619437
Validation loss: 2.4619553848593463

Epoch: 6| Step: 3
Training loss: 2.140851085446195
Validation loss: 2.4600292555327226

Epoch: 6| Step: 4
Training loss: 2.212044267001084
Validation loss: 2.4266961503759137

Epoch: 6| Step: 5
Training loss: 1.7926517114853644
Validation loss: 2.524587829439721

Epoch: 6| Step: 6
Training loss: 1.5194879442334837
Validation loss: 2.4553012687591425

Epoch: 6| Step: 7
Training loss: 2.2117887010901978
Validation loss: 2.4203856007158864

Epoch: 6| Step: 8
Training loss: 1.6333827432957015
Validation loss: 2.4975071301754146

Epoch: 6| Step: 9
Training loss: 2.1583992018891167
Validation loss: 2.461999465929263

Epoch: 6| Step: 10
Training loss: 1.6326412129337167
Validation loss: 2.4611862814728918

Epoch: 6| Step: 11
Training loss: 1.6734422842147925
Validation loss: 2.43179493570285

Epoch: 6| Step: 12
Training loss: 1.9962706366172802
Validation loss: 2.4587911642183284

Epoch: 6| Step: 13
Training loss: 2.145390572901967
Validation loss: 2.4909543925843503

Epoch: 441| Step: 0
Training loss: 1.833922883811724
Validation loss: 2.457112021628008

Epoch: 6| Step: 1
Training loss: 1.6890257542906877
Validation loss: 2.466417395754665

Epoch: 6| Step: 2
Training loss: 2.1076977878854866
Validation loss: 2.45553888959526

Epoch: 6| Step: 3
Training loss: 1.741460537310287
Validation loss: 2.458003402797852

Epoch: 6| Step: 4
Training loss: 2.3236678295987168
Validation loss: 2.4420020610344455

Epoch: 6| Step: 5
Training loss: 1.789158914291911
Validation loss: 2.5100242928659307

Epoch: 6| Step: 6
Training loss: 1.404103187499603
Validation loss: 2.4773956498806298

Epoch: 6| Step: 7
Training loss: 1.9979021275388755
Validation loss: 2.470937709460643

Epoch: 6| Step: 8
Training loss: 2.034860541886534
Validation loss: 2.4461818652668654

Epoch: 6| Step: 9
Training loss: 2.223490633835655
Validation loss: 2.4870474850385627

Epoch: 6| Step: 10
Training loss: 1.6992814677272507
Validation loss: 2.4743299642726213

Epoch: 6| Step: 11
Training loss: 1.5292198580742447
Validation loss: 2.518125601387542

Epoch: 6| Step: 12
Training loss: 1.8467769022286435
Validation loss: 2.4162339242793265

Epoch: 6| Step: 13
Training loss: 2.197371959232592
Validation loss: 2.4592366763578615

Epoch: 442| Step: 0
Training loss: 2.011023184515093
Validation loss: 2.4912699192124803

Epoch: 6| Step: 1
Training loss: 1.7786742208588266
Validation loss: 2.4852273961719638

Epoch: 6| Step: 2
Training loss: 1.7618874344348059
Validation loss: 2.450671017076004

Epoch: 6| Step: 3
Training loss: 1.63943932695379
Validation loss: 2.458779497018938

Epoch: 6| Step: 4
Training loss: 2.213902941374279
Validation loss: 2.4928484640141266

Epoch: 6| Step: 5
Training loss: 1.5502841258360003
Validation loss: 2.4642914771732767

Epoch: 6| Step: 6
Training loss: 2.1433128689767367
Validation loss: 2.459911050464776

Epoch: 6| Step: 7
Training loss: 1.7310703859216743
Validation loss: 2.4894106700320653

Epoch: 6| Step: 8
Training loss: 2.197972976421258
Validation loss: 2.4579117109071933

Epoch: 6| Step: 9
Training loss: 1.7748212751216015
Validation loss: 2.4652575120598925

Epoch: 6| Step: 10
Training loss: 1.688842592559904
Validation loss: 2.4662620434843463

Epoch: 6| Step: 11
Training loss: 2.00536913206396
Validation loss: 2.486031632436128

Epoch: 6| Step: 12
Training loss: 1.9574485836365068
Validation loss: 2.5305637617917567

Epoch: 6| Step: 13
Training loss: 1.4936476349171852
Validation loss: 2.4742908120091065

Epoch: 443| Step: 0
Training loss: 1.6069591871395117
Validation loss: 2.43635099942056

Epoch: 6| Step: 1
Training loss: 1.8463552140491637
Validation loss: 2.4555129412947534

Epoch: 6| Step: 2
Training loss: 1.744751689699631
Validation loss: 2.457812428404508

Epoch: 6| Step: 3
Training loss: 1.925032030805164
Validation loss: 2.430213938983262

Epoch: 6| Step: 4
Training loss: 1.7591395815172661
Validation loss: 2.4855105930814845

Epoch: 6| Step: 5
Training loss: 2.620888168982325
Validation loss: 2.458073994629

Epoch: 6| Step: 6
Training loss: 1.7983013297065418
Validation loss: 2.4545306652637677

Epoch: 6| Step: 7
Training loss: 1.4603661220508917
Validation loss: 2.47376901693448

Epoch: 6| Step: 8
Training loss: 1.7258337908267907
Validation loss: 2.4799203362192053

Epoch: 6| Step: 9
Training loss: 2.0981976540320844
Validation loss: 2.477218030320439

Epoch: 6| Step: 10
Training loss: 2.085675969677045
Validation loss: 2.3758423223114766

Epoch: 6| Step: 11
Training loss: 1.6022786097907393
Validation loss: 2.4455304672951823

Epoch: 6| Step: 12
Training loss: 2.0390552506007387
Validation loss: 2.5027801191281456

Epoch: 6| Step: 13
Training loss: 1.7716692877409461
Validation loss: 2.432394290406956

Epoch: 444| Step: 0
Training loss: 2.334685024933645
Validation loss: 2.485678751393956

Epoch: 6| Step: 1
Training loss: 1.6091360266580381
Validation loss: 2.432780318675297

Epoch: 6| Step: 2
Training loss: 1.858401259932828
Validation loss: 2.465105809794996

Epoch: 6| Step: 3
Training loss: 2.427650309033247
Validation loss: 2.4270183915962473

Epoch: 6| Step: 4
Training loss: 2.470316522680647
Validation loss: 2.4661191767842086

Epoch: 6| Step: 5
Training loss: 1.379243458264761
Validation loss: 2.4973026562660325

Epoch: 6| Step: 6
Training loss: 1.2723660320217407
Validation loss: 2.457354699283137

Epoch: 6| Step: 7
Training loss: 1.469382210879722
Validation loss: 2.425778307780928

Epoch: 6| Step: 8
Training loss: 2.098391384546858
Validation loss: 2.405736115366666

Epoch: 6| Step: 9
Training loss: 1.743418852834062
Validation loss: 2.4684114391475918

Epoch: 6| Step: 10
Training loss: 2.423680081945206
Validation loss: 2.4548940660466902

Epoch: 6| Step: 11
Training loss: 1.5518185775891635
Validation loss: 2.494121742014844

Epoch: 6| Step: 12
Training loss: 1.3502812127989101
Validation loss: 2.4418470305224265

Epoch: 6| Step: 13
Training loss: 1.8692666452750113
Validation loss: 2.468815655326586

Epoch: 445| Step: 0
Training loss: 1.5121910284055982
Validation loss: 2.4695625987584826

Epoch: 6| Step: 1
Training loss: 2.541268385235676
Validation loss: 2.4273194510630347

Epoch: 6| Step: 2
Training loss: 2.3911457367264624
Validation loss: 2.4562200414233404

Epoch: 6| Step: 3
Training loss: 1.5212082422523179
Validation loss: 2.4421884877549553

Epoch: 6| Step: 4
Training loss: 1.8137730041349067
Validation loss: 2.448815605944179

Epoch: 6| Step: 5
Training loss: 1.8053837213504331
Validation loss: 2.4532755469893353

Epoch: 6| Step: 6
Training loss: 1.2174757140007018
Validation loss: 2.4390285674914014

Epoch: 6| Step: 7
Training loss: 1.5408949423989755
Validation loss: 2.468818042627946

Epoch: 6| Step: 8
Training loss: 1.6454936131620297
Validation loss: 2.4703389562301443

Epoch: 6| Step: 9
Training loss: 2.139378985558112
Validation loss: 2.498912389092218

Epoch: 6| Step: 10
Training loss: 2.169836061499338
Validation loss: 2.4345123064419973

Epoch: 6| Step: 11
Training loss: 1.4190521725075298
Validation loss: 2.470687206884511

Epoch: 6| Step: 12
Training loss: 2.217814718489648
Validation loss: 2.421251941022631

Epoch: 6| Step: 13
Training loss: 1.3457834250267466
Validation loss: 2.4586982046093264

Epoch: 446| Step: 0
Training loss: 1.8929795665172164
Validation loss: 2.467848710733262

Epoch: 6| Step: 1
Training loss: 1.743717360105586
Validation loss: 2.4706044229369675

Epoch: 6| Step: 2
Training loss: 2.470810525241028
Validation loss: 2.471080597400185

Epoch: 6| Step: 3
Training loss: 1.447484050129195
Validation loss: 2.486255740042345

Epoch: 6| Step: 4
Training loss: 2.084380522438981
Validation loss: 2.5099682001206696

Epoch: 6| Step: 5
Training loss: 1.7828088598189105
Validation loss: 2.434583595203151

Epoch: 6| Step: 6
Training loss: 1.8375991119850945
Validation loss: 2.4669221170691342

Epoch: 6| Step: 7
Training loss: 1.7875256996708166
Validation loss: 2.457358118018137

Epoch: 6| Step: 8
Training loss: 2.0214403820318645
Validation loss: 2.44851160056999

Epoch: 6| Step: 9
Training loss: 2.096296100325638
Validation loss: 2.4094638358693685

Epoch: 6| Step: 10
Training loss: 1.6955190787607346
Validation loss: 2.4597168697022407

Epoch: 6| Step: 11
Training loss: 1.709096466748076
Validation loss: 2.4634308163841103

Epoch: 6| Step: 12
Training loss: 1.4032912174996874
Validation loss: 2.4626997339543553

Epoch: 6| Step: 13
Training loss: 1.8166359376787409
Validation loss: 2.478073803874146

Epoch: 447| Step: 0
Training loss: 1.255434335119425
Validation loss: 2.44497111940962

Epoch: 6| Step: 1
Training loss: 1.183576426363527
Validation loss: 2.510760886264202

Epoch: 6| Step: 2
Training loss: 2.2873516263942792
Validation loss: 2.4223193218545225

Epoch: 6| Step: 3
Training loss: 2.2110597826028253
Validation loss: 2.4433077167713226

Epoch: 6| Step: 4
Training loss: 1.2807361223554996
Validation loss: 2.4588703881468623

Epoch: 6| Step: 5
Training loss: 1.3609918209057192
Validation loss: 2.428274023588031

Epoch: 6| Step: 6
Training loss: 2.8026556874570416
Validation loss: 2.4906253749181007

Epoch: 6| Step: 7
Training loss: 1.3989831296872948
Validation loss: 2.4749379239418916

Epoch: 6| Step: 8
Training loss: 1.84718532891905
Validation loss: 2.5055527937085444

Epoch: 6| Step: 9
Training loss: 1.9571578238057183
Validation loss: 2.4729510589067725

Epoch: 6| Step: 10
Training loss: 2.012796590997147
Validation loss: 2.4212666986418934

Epoch: 6| Step: 11
Training loss: 1.7975991821514692
Validation loss: 2.475881516878963

Epoch: 6| Step: 12
Training loss: 1.55362960631332
Validation loss: 2.448072692325181

Epoch: 6| Step: 13
Training loss: 2.0768091151497394
Validation loss: 2.4474033324530975

Epoch: 448| Step: 0
Training loss: 1.4884696300561895
Validation loss: 2.456100964653024

Epoch: 6| Step: 1
Training loss: 1.1283143283552581
Validation loss: 2.4597038649897938

Epoch: 6| Step: 2
Training loss: 1.5107529189179292
Validation loss: 2.462115147780587

Epoch: 6| Step: 3
Training loss: 2.291907719736677
Validation loss: 2.4799751972168247

Epoch: 6| Step: 4
Training loss: 1.9880424910749719
Validation loss: 2.4416910872147626

Epoch: 6| Step: 5
Training loss: 1.5889222569878052
Validation loss: 2.452833904685066

Epoch: 6| Step: 6
Training loss: 1.2079789475097022
Validation loss: 2.4526496114634075

Epoch: 6| Step: 7
Training loss: 1.9117386393258762
Validation loss: 2.4951811811859663

Epoch: 6| Step: 8
Training loss: 2.0068814386189073
Validation loss: 2.421148047505618

Epoch: 6| Step: 9
Training loss: 2.355581707879206
Validation loss: 2.4573617537442205

Epoch: 6| Step: 10
Training loss: 2.216601769015983
Validation loss: 2.4591105299571594

Epoch: 6| Step: 11
Training loss: 2.523562212717388
Validation loss: 2.474879118584996

Epoch: 6| Step: 12
Training loss: 1.8219294754180846
Validation loss: 2.4695897115753835

Epoch: 6| Step: 13
Training loss: 1.7295717967411464
Validation loss: 2.4638079587109405

Epoch: 449| Step: 0
Training loss: 1.7142287696167946
Validation loss: 2.4768242183201803

Epoch: 6| Step: 1
Training loss: 1.8319866118046002
Validation loss: 2.473159982116557

Epoch: 6| Step: 2
Training loss: 1.6703958117670046
Validation loss: 2.4756084900240736

Epoch: 6| Step: 3
Training loss: 1.0859257539621603
Validation loss: 2.439201342840793

Epoch: 6| Step: 4
Training loss: 1.5759386081453384
Validation loss: 2.5073569674431613

Epoch: 6| Step: 5
Training loss: 2.3706490666852167
Validation loss: 2.467229289361475

Epoch: 6| Step: 6
Training loss: 1.535831215674682
Validation loss: 2.4719783743660324

Epoch: 6| Step: 7
Training loss: 1.6623063828006204
Validation loss: 2.454546769623694

Epoch: 6| Step: 8
Training loss: 1.8784919646849991
Validation loss: 2.4276106045714974

Epoch: 6| Step: 9
Training loss: 2.4112806371440207
Validation loss: 2.4541984316904295

Epoch: 6| Step: 10
Training loss: 1.594519635036243
Validation loss: 2.443037276501887

Epoch: 6| Step: 11
Training loss: 2.0733226844947525
Validation loss: 2.436733443994864

Epoch: 6| Step: 12
Training loss: 2.562074579420493
Validation loss: 2.4824637327548196

Epoch: 6| Step: 13
Training loss: 1.5352362303732079
Validation loss: 2.4284015311331353

Epoch: 450| Step: 0
Training loss: 1.4104930866373582
Validation loss: 2.448514796072969

Epoch: 6| Step: 1
Training loss: 2.47664875638432
Validation loss: 2.4631929879056482

Epoch: 6| Step: 2
Training loss: 1.6922302353412422
Validation loss: 2.4774799525723825

Epoch: 6| Step: 3
Training loss: 1.2831299643367087
Validation loss: 2.442097216149563

Epoch: 6| Step: 4
Training loss: 1.8336035644892719
Validation loss: 2.443168764838954

Epoch: 6| Step: 5
Training loss: 1.9324803701491933
Validation loss: 2.4555428798503476

Epoch: 6| Step: 6
Training loss: 2.089436771302437
Validation loss: 2.441943050444919

Epoch: 6| Step: 7
Training loss: 1.2871492926698453
Validation loss: 2.480549509808736

Epoch: 6| Step: 8
Training loss: 1.9934884405018634
Validation loss: 2.449665314426855

Epoch: 6| Step: 9
Training loss: 2.1663031150790895
Validation loss: 2.454011842374234

Epoch: 6| Step: 10
Training loss: 2.0088276356362793
Validation loss: 2.443737017469034

Epoch: 6| Step: 11
Training loss: 1.7869291287603317
Validation loss: 2.4821172521374786

Epoch: 6| Step: 12
Training loss: 2.004121348713968
Validation loss: 2.4552998195136677

Epoch: 6| Step: 13
Training loss: 1.4564415605150647
Validation loss: 2.402520117999916

Testing loss: 2.5020418898987926

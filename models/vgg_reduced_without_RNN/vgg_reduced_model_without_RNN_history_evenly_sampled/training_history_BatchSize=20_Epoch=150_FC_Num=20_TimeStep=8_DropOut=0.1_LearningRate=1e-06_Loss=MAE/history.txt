Epoch: 1| Step: 0
Training loss: 4.3325605392456055
Validation loss: 5.289015195702993

Epoch: 5| Step: 1
Training loss: 4.743264675140381
Validation loss: 5.283713466377669

Epoch: 5| Step: 2
Training loss: 5.711305141448975
Validation loss: 5.2777234687600085

Epoch: 5| Step: 3
Training loss: 4.4423723220825195
Validation loss: 5.2707021518420145

Epoch: 5| Step: 4
Training loss: 4.779819011688232
Validation loss: 5.26480899318572

Epoch: 5| Step: 5
Training loss: 6.821353912353516
Validation loss: 5.26123579086796

Epoch: 5| Step: 6
Training loss: 5.116075038909912
Validation loss: 5.2578649931056525

Epoch: 5| Step: 7
Training loss: 4.599234580993652
Validation loss: 5.248425622140208

Epoch: 5| Step: 8
Training loss: 5.149570941925049
Validation loss: 5.2441897494818575

Epoch: 5| Step: 9
Training loss: 4.629575252532959
Validation loss: 5.236699483727896

Epoch: 5| Step: 10
Training loss: 5.392748832702637
Validation loss: 5.231123862727996

Epoch: 2| Step: 0
Training loss: 4.626791954040527
Validation loss: 5.228049719205466

Epoch: 5| Step: 1
Training loss: 4.576845645904541
Validation loss: 5.223219656175183

Epoch: 5| Step: 2
Training loss: 3.8863627910614014
Validation loss: 5.21524663125315

Epoch: 5| Step: 3
Training loss: 6.31801176071167
Validation loss: 5.212143241718251

Epoch: 5| Step: 4
Training loss: 3.8741207122802734
Validation loss: 5.209488314967001

Epoch: 5| Step: 5
Training loss: 4.47154426574707
Validation loss: 5.199188555440595

Epoch: 5| Step: 6
Training loss: 5.488504886627197
Validation loss: 5.193709717001966

Epoch: 5| Step: 7
Training loss: 5.03073787689209
Validation loss: 5.188471142963697

Epoch: 5| Step: 8
Training loss: 5.039565086364746
Validation loss: 5.184531675871982

Epoch: 5| Step: 9
Training loss: 6.634571075439453
Validation loss: 5.181971155187135

Epoch: 5| Step: 10
Training loss: 5.020327568054199
Validation loss: 5.175682037107406

Epoch: 3| Step: 0
Training loss: 4.932727813720703
Validation loss: 5.170266894884007

Epoch: 5| Step: 1
Training loss: 5.39263916015625
Validation loss: 5.165077999073972

Epoch: 5| Step: 2
Training loss: 5.465465545654297
Validation loss: 5.158952497666882

Epoch: 5| Step: 3
Training loss: 3.5954642295837402
Validation loss: 5.15396692932293

Epoch: 5| Step: 4
Training loss: 4.713826656341553
Validation loss: 5.150132389478786

Epoch: 5| Step: 5
Training loss: 5.1357550621032715
Validation loss: 5.14603195395521

Epoch: 5| Step: 6
Training loss: 6.081014156341553
Validation loss: 5.139402625381305

Epoch: 5| Step: 7
Training loss: 4.737080097198486
Validation loss: 5.137123000237249

Epoch: 5| Step: 8
Training loss: 4.218127727508545
Validation loss: 5.128883679707845

Epoch: 5| Step: 9
Training loss: 3.912959337234497
Validation loss: 5.124240372770576

Epoch: 5| Step: 10
Training loss: 6.361501216888428
Validation loss: 5.120653449848134

Epoch: 4| Step: 0
Training loss: 4.038909912109375
Validation loss: 5.118642504497241

Epoch: 5| Step: 1
Training loss: 5.201840877532959
Validation loss: 5.110754618080714

Epoch: 5| Step: 2
Training loss: 5.446953296661377
Validation loss: 5.107017819599439

Epoch: 5| Step: 3
Training loss: 4.381169319152832
Validation loss: 5.102959586728003

Epoch: 5| Step: 4
Training loss: 4.701916694641113
Validation loss: 5.09713061137866

Epoch: 5| Step: 5
Training loss: 5.425009250640869
Validation loss: 5.0923095364724436

Epoch: 5| Step: 6
Training loss: 5.790192127227783
Validation loss: 5.0862198491250314

Epoch: 5| Step: 7
Training loss: 5.021386623382568
Validation loss: 5.084580011265253

Epoch: 5| Step: 8
Training loss: 4.646396636962891
Validation loss: 5.078336136315459

Epoch: 5| Step: 9
Training loss: 5.339301109313965
Validation loss: 5.074417309094501

Epoch: 5| Step: 10
Training loss: 3.5353200435638428
Validation loss: 5.067837838203676

Epoch: 5| Step: 0
Training loss: 5.180859088897705
Validation loss: 5.063411435773296

Epoch: 5| Step: 1
Training loss: 4.103668212890625
Validation loss: 5.058210137069866

Epoch: 5| Step: 2
Training loss: 4.584725379943848
Validation loss: 5.051827753743818

Epoch: 5| Step: 3
Training loss: 4.9059929847717285
Validation loss: 5.048049301229497

Epoch: 5| Step: 4
Training loss: 5.1468400955200195
Validation loss: 5.042514021678637

Epoch: 5| Step: 5
Training loss: 5.1430768966674805
Validation loss: 5.038976992330244

Epoch: 5| Step: 6
Training loss: 5.59973669052124
Validation loss: 5.034794207542173

Epoch: 5| Step: 7
Training loss: 5.439652442932129
Validation loss: 5.027874915830551

Epoch: 5| Step: 8
Training loss: 4.22336483001709
Validation loss: 5.020457785616639

Epoch: 5| Step: 9
Training loss: 5.303689002990723
Validation loss: 5.018002956144271

Epoch: 5| Step: 10
Training loss: 3.273099184036255
Validation loss: 5.011483233462098

Epoch: 6| Step: 0
Training loss: 4.840789794921875
Validation loss: 5.004680854018017

Epoch: 5| Step: 1
Training loss: 4.371035099029541
Validation loss: 5.000395569750058

Epoch: 5| Step: 2
Training loss: 4.02462911605835
Validation loss: 4.992107401611984

Epoch: 5| Step: 3
Training loss: 6.000654697418213
Validation loss: 4.990015024779945

Epoch: 5| Step: 4
Training loss: 4.327765464782715
Validation loss: 4.982216952949442

Epoch: 5| Step: 5
Training loss: 3.199873447418213
Validation loss: 4.977988191830215

Epoch: 5| Step: 6
Training loss: 4.583786964416504
Validation loss: 4.970437054993004

Epoch: 5| Step: 7
Training loss: 5.691378593444824
Validation loss: 4.964194436227122

Epoch: 5| Step: 8
Training loss: 6.139307498931885
Validation loss: 4.960355820194367

Epoch: 5| Step: 9
Training loss: 4.417624473571777
Validation loss: 4.951017338742492

Epoch: 5| Step: 10
Training loss: 4.8498854637146
Validation loss: 4.945291801165509

Epoch: 7| Step: 0
Training loss: 4.951960563659668
Validation loss: 4.938239876941968

Epoch: 5| Step: 1
Training loss: 4.7623820304870605
Validation loss: 4.933490578846265

Epoch: 5| Step: 2
Training loss: 5.507849216461182
Validation loss: 4.92815444289997

Epoch: 5| Step: 3
Training loss: 4.23084020614624
Validation loss: 4.918238255285448

Epoch: 5| Step: 4
Training loss: 5.359371185302734
Validation loss: 4.911268787999307

Epoch: 5| Step: 5
Training loss: 4.8340744972229
Validation loss: 4.905998506853657

Epoch: 5| Step: 6
Training loss: 4.734803199768066
Validation loss: 4.899299196017686

Epoch: 5| Step: 7
Training loss: 4.528424263000488
Validation loss: 4.886646711698142

Epoch: 5| Step: 8
Training loss: 4.0887861251831055
Validation loss: 4.879501973429034

Epoch: 5| Step: 9
Training loss: 3.473787307739258
Validation loss: 4.874524721535304

Epoch: 5| Step: 10
Training loss: 5.210361957550049
Validation loss: 4.8676720947347665

Epoch: 8| Step: 0
Training loss: 3.9217467308044434
Validation loss: 4.859014162453272

Epoch: 5| Step: 1
Training loss: 5.29617166519165
Validation loss: 4.848699313338085

Epoch: 5| Step: 2
Training loss: 5.409689426422119
Validation loss: 4.845716573858774

Epoch: 5| Step: 3
Training loss: 4.237933158874512
Validation loss: 4.8358207133508495

Epoch: 5| Step: 4
Training loss: 3.9765117168426514
Validation loss: 4.829606661232569

Epoch: 5| Step: 5
Training loss: 5.6058807373046875
Validation loss: 4.820955727690009

Epoch: 5| Step: 6
Training loss: 4.653023719787598
Validation loss: 4.810600655053252

Epoch: 5| Step: 7
Training loss: 4.505461692810059
Validation loss: 4.802764205522434

Epoch: 5| Step: 8
Training loss: 3.9167823791503906
Validation loss: 4.793717732993505

Epoch: 5| Step: 9
Training loss: 5.165010452270508
Validation loss: 4.788015960365214

Epoch: 5| Step: 10
Training loss: 3.8413610458374023
Validation loss: 4.779865593038579

Epoch: 9| Step: 0
Training loss: 5.2783403396606445
Validation loss: 4.771291440533053

Epoch: 5| Step: 1
Training loss: 4.786167621612549
Validation loss: 4.760427377557241

Epoch: 5| Step: 2
Training loss: 4.745443820953369
Validation loss: 4.753168136842789

Epoch: 5| Step: 3
Training loss: 3.734525680541992
Validation loss: 4.74563548385456

Epoch: 5| Step: 4
Training loss: 5.186358451843262
Validation loss: 4.73451917914934

Epoch: 5| Step: 5
Training loss: 4.385076999664307
Validation loss: 4.725227922521611

Epoch: 5| Step: 6
Training loss: 5.093255519866943
Validation loss: 4.717138290405273

Epoch: 5| Step: 7
Training loss: 4.415674209594727
Validation loss: 4.709805401422644

Epoch: 5| Step: 8
Training loss: 3.1954874992370605
Validation loss: 4.699653128141998

Epoch: 5| Step: 9
Training loss: 4.3449578285217285
Validation loss: 4.691507113877163

Epoch: 5| Step: 10
Training loss: 4.451686382293701
Validation loss: 4.680098846394529

Epoch: 10| Step: 0
Training loss: 4.915760040283203
Validation loss: 4.672619173603673

Epoch: 5| Step: 1
Training loss: 4.605668067932129
Validation loss: 4.666645542267831

Epoch: 5| Step: 2
Training loss: 3.5815682411193848
Validation loss: 4.654080293511831

Epoch: 5| Step: 3
Training loss: 4.860759735107422
Validation loss: 4.646531474205755

Epoch: 5| Step: 4
Training loss: 3.922524929046631
Validation loss: 4.6366483421735865

Epoch: 5| Step: 5
Training loss: 4.650631904602051
Validation loss: 4.621338444371378

Epoch: 5| Step: 6
Training loss: 4.895634174346924
Validation loss: 4.612943887710571

Epoch: 5| Step: 7
Training loss: 4.500992774963379
Validation loss: 4.603332688731532

Epoch: 5| Step: 8
Training loss: 3.953169345855713
Validation loss: 4.5951648681394515

Epoch: 5| Step: 9
Training loss: 4.058636665344238
Validation loss: 4.583098257741621

Epoch: 5| Step: 10
Training loss: 4.448522090911865
Validation loss: 4.571825163338774

Epoch: 11| Step: 0
Training loss: 4.421902656555176
Validation loss: 4.565907555241739

Epoch: 5| Step: 1
Training loss: 3.5146148204803467
Validation loss: 4.552451687474405

Epoch: 5| Step: 2
Training loss: 3.2472050189971924
Validation loss: 4.543243100566249

Epoch: 5| Step: 3
Training loss: 4.002010822296143
Validation loss: 4.528260533527662

Epoch: 5| Step: 4
Training loss: 4.838312149047852
Validation loss: 4.519570483956286

Epoch: 5| Step: 5
Training loss: 4.151636123657227
Validation loss: 4.505766114880962

Epoch: 5| Step: 6
Training loss: 4.352118492126465
Validation loss: 4.498151809938492

Epoch: 5| Step: 7
Training loss: 5.025746822357178
Validation loss: 4.479475862236433

Epoch: 5| Step: 8
Training loss: 3.490588665008545
Validation loss: 4.4760370203243784

Epoch: 5| Step: 9
Training loss: 5.0704345703125
Validation loss: 4.459652746877363

Epoch: 5| Step: 10
Training loss: 5.087795257568359
Validation loss: 4.450969096153013

Epoch: 12| Step: 0
Training loss: 5.245223045349121
Validation loss: 4.430535849704538

Epoch: 5| Step: 1
Training loss: 3.6784729957580566
Validation loss: 4.42392627141809

Epoch: 5| Step: 2
Training loss: 3.626293897628784
Validation loss: 4.414243651974585

Epoch: 5| Step: 3
Training loss: 3.5254263877868652
Validation loss: 4.396234937893447

Epoch: 5| Step: 4
Training loss: 4.9029130935668945
Validation loss: 4.383025705173451

Epoch: 5| Step: 5
Training loss: 3.5157828330993652
Validation loss: 4.370801330894552

Epoch: 5| Step: 6
Training loss: 4.386172771453857
Validation loss: 4.358625924715432

Epoch: 5| Step: 7
Training loss: 3.7220993041992188
Validation loss: 4.349745914500247

Epoch: 5| Step: 8
Training loss: 4.031744956970215
Validation loss: 4.332709650839528

Epoch: 5| Step: 9
Training loss: 4.56511926651001
Validation loss: 4.322538560436618

Epoch: 5| Step: 10
Training loss: 4.549196720123291
Validation loss: 4.307698824072397

Epoch: 13| Step: 0
Training loss: 4.160788536071777
Validation loss: 4.29112615892964

Epoch: 5| Step: 1
Training loss: 4.618509292602539
Validation loss: 4.277631859625539

Epoch: 5| Step: 2
Training loss: 4.478708267211914
Validation loss: 4.268143130886939

Epoch: 5| Step: 3
Training loss: 4.455199241638184
Validation loss: 4.247773826763194

Epoch: 5| Step: 4
Training loss: 4.191598892211914
Validation loss: 4.240996965797999

Epoch: 5| Step: 5
Training loss: 3.5434792041778564
Validation loss: 4.227740651817732

Epoch: 5| Step: 6
Training loss: 3.3277735710144043
Validation loss: 4.208163938214702

Epoch: 5| Step: 7
Training loss: 3.4030139446258545
Validation loss: 4.204881873182071

Epoch: 5| Step: 8
Training loss: 4.5000691413879395
Validation loss: 4.184160742708432

Epoch: 5| Step: 9
Training loss: 4.073327541351318
Validation loss: 4.16887504054654

Epoch: 5| Step: 10
Training loss: 3.3939733505249023
Validation loss: 4.15880964648339

Epoch: 14| Step: 0
Training loss: 3.6306464672088623
Validation loss: 4.138998759690152

Epoch: 5| Step: 1
Training loss: 3.9052093029022217
Validation loss: 4.130164900133686

Epoch: 5| Step: 2
Training loss: 4.008682727813721
Validation loss: 4.116021863875851

Epoch: 5| Step: 3
Training loss: 3.2415518760681152
Validation loss: 4.08709894200807

Epoch: 5| Step: 4
Training loss: 2.9068782329559326
Validation loss: 4.082970670474473

Epoch: 5| Step: 5
Training loss: 4.450737953186035
Validation loss: 4.068609806799119

Epoch: 5| Step: 6
Training loss: 3.4684855937957764
Validation loss: 4.049006380060668

Epoch: 5| Step: 7
Training loss: 4.217312812805176
Validation loss: 4.039773392420943

Epoch: 5| Step: 8
Training loss: 3.8353309631347656
Validation loss: 4.027507212854201

Epoch: 5| Step: 9
Training loss: 5.203274726867676
Validation loss: 4.00991137309741

Epoch: 5| Step: 10
Training loss: 3.8327524662017822
Validation loss: 3.997100184040685

Epoch: 15| Step: 0
Training loss: 2.4322562217712402
Validation loss: 3.977214720941359

Epoch: 5| Step: 1
Training loss: 4.517955303192139
Validation loss: 3.961616510986

Epoch: 5| Step: 2
Training loss: 4.8649725914001465
Validation loss: 3.9501429578309417

Epoch: 5| Step: 3
Training loss: 3.828336715698242
Validation loss: 3.9312839200419765

Epoch: 5| Step: 4
Training loss: 3.8855392932891846
Validation loss: 3.919048391362672

Epoch: 5| Step: 5
Training loss: 3.6077194213867188
Validation loss: 3.8973789112542265

Epoch: 5| Step: 6
Training loss: 3.28745698928833
Validation loss: 3.8800048725579375

Epoch: 5| Step: 7
Training loss: 4.036412715911865
Validation loss: 3.8705764662834907

Epoch: 5| Step: 8
Training loss: 4.348322868347168
Validation loss: 3.8479586314129572

Epoch: 5| Step: 9
Training loss: 3.6500792503356934
Validation loss: 3.832680671445785

Epoch: 5| Step: 10
Training loss: 2.5771520137786865
Validation loss: 3.8183003266652427

Epoch: 16| Step: 0
Training loss: 3.3565585613250732
Validation loss: 3.792546077441144

Epoch: 5| Step: 1
Training loss: 4.707778453826904
Validation loss: 3.7908961644736667

Epoch: 5| Step: 2
Training loss: 2.7878470420837402
Validation loss: 3.7705283728978967

Epoch: 5| Step: 3
Training loss: 4.038153171539307
Validation loss: 3.749376045760288

Epoch: 5| Step: 4
Training loss: 3.4116902351379395
Validation loss: 3.7358818361836095

Epoch: 5| Step: 5
Training loss: 4.3230485916137695
Validation loss: 3.7145495978734826

Epoch: 5| Step: 6
Training loss: 4.581806659698486
Validation loss: 3.6971826912254415

Epoch: 5| Step: 7
Training loss: 3.0923972129821777
Validation loss: 3.6838892941833823

Epoch: 5| Step: 8
Training loss: 3.073050022125244
Validation loss: 3.6725224961516676

Epoch: 5| Step: 9
Training loss: 2.922034740447998
Validation loss: 3.651465062172182

Epoch: 5| Step: 10
Training loss: 3.2775180339813232
Validation loss: 3.6261898163826234

Epoch: 17| Step: 0
Training loss: 4.238204479217529
Validation loss: 3.623870131789997

Epoch: 5| Step: 1
Training loss: 3.2281486988067627
Validation loss: 3.607649016123946

Epoch: 5| Step: 2
Training loss: 3.781853437423706
Validation loss: 3.5845298792726252

Epoch: 5| Step: 3
Training loss: 3.6527976989746094
Validation loss: 3.571648772044848

Epoch: 5| Step: 4
Training loss: 3.0424790382385254
Validation loss: 3.5544617201692317

Epoch: 5| Step: 5
Training loss: 3.7105374336242676
Validation loss: 3.545415775750273

Epoch: 5| Step: 6
Training loss: 3.09425687789917
Validation loss: 3.51549692051385

Epoch: 5| Step: 7
Training loss: 3.3177504539489746
Validation loss: 3.505462246556436

Epoch: 5| Step: 8
Training loss: 2.4695935249328613
Validation loss: 3.4909733469768236

Epoch: 5| Step: 9
Training loss: 3.681619644165039
Validation loss: 3.4715496724651707

Epoch: 5| Step: 10
Training loss: 3.5489754676818848
Validation loss: 3.4398902334192747

Epoch: 18| Step: 0
Training loss: 2.639988422393799
Validation loss: 3.4313105126862884

Epoch: 5| Step: 1
Training loss: 3.032742977142334
Validation loss: 3.402849945970761

Epoch: 5| Step: 2
Training loss: 3.374743938446045
Validation loss: 3.393374176435573

Epoch: 5| Step: 3
Training loss: 3.0364327430725098
Validation loss: 3.377097242621965

Epoch: 5| Step: 4
Training loss: 3.304737091064453
Validation loss: 3.350170691808065

Epoch: 5| Step: 5
Training loss: 2.4400389194488525
Validation loss: 3.3410478099699943

Epoch: 5| Step: 6
Training loss: 3.6319820880889893
Validation loss: 3.3165891837048274

Epoch: 5| Step: 7
Training loss: 3.577270030975342
Validation loss: 3.29655392964681

Epoch: 5| Step: 8
Training loss: 3.915213108062744
Validation loss: 3.2819763281012095

Epoch: 5| Step: 9
Training loss: 3.3880271911621094
Validation loss: 3.263472967250373

Epoch: 5| Step: 10
Training loss: 3.5381112098693848
Validation loss: 3.2528154696187666

Epoch: 19| Step: 0
Training loss: 3.645444393157959
Validation loss: 3.2178504877193

Epoch: 5| Step: 1
Training loss: 3.249340772628784
Validation loss: 3.198025903394145

Epoch: 5| Step: 2
Training loss: 2.604710817337036
Validation loss: 3.18693954970247

Epoch: 5| Step: 3
Training loss: 3.2167677879333496
Validation loss: 3.1584300738508984

Epoch: 5| Step: 4
Training loss: 3.1025424003601074
Validation loss: 3.125812171607889

Epoch: 5| Step: 5
Training loss: 2.7338509559631348
Validation loss: 3.117497823571646

Epoch: 5| Step: 6
Training loss: 4.051299095153809
Validation loss: 3.0892797234237834

Epoch: 5| Step: 7
Training loss: 2.644345998764038
Validation loss: 3.0878623070255404

Epoch: 5| Step: 8
Training loss: 3.5232956409454346
Validation loss: 3.0491474136229484

Epoch: 5| Step: 9
Training loss: 2.7994179725646973
Validation loss: 3.032295952561081

Epoch: 5| Step: 10
Training loss: 2.3030083179473877
Validation loss: 3.0026099861309095

Epoch: 20| Step: 0
Training loss: 3.0758204460144043
Validation loss: 2.9893635678034958

Epoch: 5| Step: 1
Training loss: 2.8755173683166504
Validation loss: 2.9777561028798423

Epoch: 5| Step: 2
Training loss: 3.3247528076171875
Validation loss: 2.950920371599095

Epoch: 5| Step: 3
Training loss: 2.956693649291992
Validation loss: 2.930543299644224

Epoch: 5| Step: 4
Training loss: 3.1008048057556152
Validation loss: 2.9254396038670696

Epoch: 5| Step: 5
Training loss: 2.7382500171661377
Validation loss: 2.8917343975395284

Epoch: 5| Step: 6
Training loss: 2.476487636566162
Validation loss: 2.874068711393623

Epoch: 5| Step: 7
Training loss: 2.979494571685791
Validation loss: 2.847983673054685

Epoch: 5| Step: 8
Training loss: 2.811662197113037
Validation loss: 2.843860964621267

Epoch: 5| Step: 9
Training loss: 2.2781105041503906
Validation loss: 2.8147369661638812

Epoch: 5| Step: 10
Training loss: 3.8569321632385254
Validation loss: 2.8029633978361725

Epoch: 21| Step: 0
Training loss: 2.974622964859009
Validation loss: 2.78430143479378

Epoch: 5| Step: 1
Training loss: 1.9287865161895752
Validation loss: 2.7511486494412987

Epoch: 5| Step: 2
Training loss: 2.5588862895965576
Validation loss: 2.75698547978555

Epoch: 5| Step: 3
Training loss: 2.9264724254608154
Validation loss: 2.7262179979714016

Epoch: 5| Step: 4
Training loss: 2.3210482597351074
Validation loss: 2.719797698400354

Epoch: 5| Step: 5
Training loss: 3.402142286300659
Validation loss: 2.7111458445108063

Epoch: 5| Step: 6
Training loss: 3.138593912124634
Validation loss: 2.68363969813111

Epoch: 5| Step: 7
Training loss: 2.924724578857422
Validation loss: 2.670245098811324

Epoch: 5| Step: 8
Training loss: 3.0610015392303467
Validation loss: 2.6630340289044123

Epoch: 5| Step: 9
Training loss: 3.0845935344696045
Validation loss: 2.646420874903279

Epoch: 5| Step: 10
Training loss: 2.6351053714752197
Validation loss: 2.620209017107564

Epoch: 22| Step: 0
Training loss: 2.5843963623046875
Validation loss: 2.6071654853000434

Epoch: 5| Step: 1
Training loss: 3.0546581745147705
Validation loss: 2.592082420984904

Epoch: 5| Step: 2
Training loss: 3.3156580924987793
Validation loss: 2.5734999513113372

Epoch: 5| Step: 3
Training loss: 2.815580129623413
Validation loss: 2.560812945006996

Epoch: 5| Step: 4
Training loss: 2.677511215209961
Validation loss: 2.5348203130947646

Epoch: 5| Step: 5
Training loss: 1.8607860803604126
Validation loss: 2.52822527065072

Epoch: 5| Step: 6
Training loss: 2.591632843017578
Validation loss: 2.5158944027398222

Epoch: 5| Step: 7
Training loss: 2.5249218940734863
Validation loss: 2.4907465878353325

Epoch: 5| Step: 8
Training loss: 2.566392183303833
Validation loss: 2.4705499987448416

Epoch: 5| Step: 9
Training loss: 2.834697723388672
Validation loss: 2.477925585162255

Epoch: 5| Step: 10
Training loss: 2.8942439556121826
Validation loss: 2.474100051387664

Epoch: 23| Step: 0
Training loss: 2.4561572074890137
Validation loss: 2.4373901095441592

Epoch: 5| Step: 1
Training loss: 3.1657795906066895
Validation loss: 2.4315338211674846

Epoch: 5| Step: 2
Training loss: 2.5054187774658203
Validation loss: 2.4073840495078795

Epoch: 5| Step: 3
Training loss: 2.413538932800293
Validation loss: 2.3776430596587477

Epoch: 5| Step: 4
Training loss: 2.632207155227661
Validation loss: 2.3728782387189966

Epoch: 5| Step: 5
Training loss: 2.8652138710021973
Validation loss: 2.3900747376103557

Epoch: 5| Step: 6
Training loss: 2.489492177963257
Validation loss: 2.356294524285101

Epoch: 5| Step: 7
Training loss: 2.970341920852661
Validation loss: 2.3168255795714674

Epoch: 5| Step: 8
Training loss: 2.0436863899230957
Validation loss: 2.3298420188247517

Epoch: 5| Step: 9
Training loss: 2.195924758911133
Validation loss: 2.314563653802359

Epoch: 5| Step: 10
Training loss: 2.838780403137207
Validation loss: 2.312103312502625

Epoch: 24| Step: 0
Training loss: 2.7458176612854004
Validation loss: 2.2834239672589045

Epoch: 5| Step: 1
Training loss: 2.8116049766540527
Validation loss: 2.289915787276401

Epoch: 5| Step: 2
Training loss: 2.4850494861602783
Validation loss: 2.2691756832984185

Epoch: 5| Step: 3
Training loss: 2.1496100425720215
Validation loss: 2.263230559646442

Epoch: 5| Step: 4
Training loss: 2.5434041023254395
Validation loss: 2.246444517566312

Epoch: 5| Step: 5
Training loss: 3.342662811279297
Validation loss: 2.2446201744899956

Epoch: 5| Step: 6
Training loss: 2.249274492263794
Validation loss: 2.233196487990759

Epoch: 5| Step: 7
Training loss: 1.917433500289917
Validation loss: 2.2382822113652385

Epoch: 5| Step: 8
Training loss: 2.189040184020996
Validation loss: 2.2208725867732877

Epoch: 5| Step: 9
Training loss: 3.065507411956787
Validation loss: 2.2324712327731553

Epoch: 5| Step: 10
Training loss: 1.877739667892456
Validation loss: 2.211371109049807

Epoch: 25| Step: 0
Training loss: 2.8771259784698486
Validation loss: 2.2136141125873854

Epoch: 5| Step: 1
Training loss: 2.227604627609253
Validation loss: 2.2105941490460466

Epoch: 5| Step: 2
Training loss: 2.12664794921875
Validation loss: 2.191654069449312

Epoch: 5| Step: 3
Training loss: 2.7034759521484375
Validation loss: 2.202358394540766

Epoch: 5| Step: 4
Training loss: 2.923384189605713
Validation loss: 2.1953498932623092

Epoch: 5| Step: 5
Training loss: 2.1606156826019287
Validation loss: 2.172701399813416

Epoch: 5| Step: 6
Training loss: 2.594604730606079
Validation loss: 2.1944280209079867

Epoch: 5| Step: 7
Training loss: 2.56901478767395
Validation loss: 2.1895497152882237

Epoch: 5| Step: 8
Training loss: 2.43481183052063
Validation loss: 2.1544058579270557

Epoch: 5| Step: 9
Training loss: 2.3186566829681396
Validation loss: 2.177822051509734

Epoch: 5| Step: 10
Training loss: 2.015629291534424
Validation loss: 2.166681064072476

Epoch: 26| Step: 0
Training loss: 2.769012928009033
Validation loss: 2.1611391113650416

Epoch: 5| Step: 1
Training loss: 2.8034348487854004
Validation loss: 2.1702820011364516

Epoch: 5| Step: 2
Training loss: 2.4021828174591064
Validation loss: 2.164527085519606

Epoch: 5| Step: 3
Training loss: 2.3361079692840576
Validation loss: 2.149680895190085

Epoch: 5| Step: 4
Training loss: 2.484434127807617
Validation loss: 2.154436893360589

Epoch: 5| Step: 5
Training loss: 2.2388858795166016
Validation loss: 2.1337896688010103

Epoch: 5| Step: 6
Training loss: 2.3688318729400635
Validation loss: 2.1448978506108767

Epoch: 5| Step: 7
Training loss: 2.4838643074035645
Validation loss: 2.1284613968223653

Epoch: 5| Step: 8
Training loss: 2.1344046592712402
Validation loss: 2.118969198196165

Epoch: 5| Step: 9
Training loss: 2.5265676975250244
Validation loss: 2.163701629125944

Epoch: 5| Step: 10
Training loss: 2.390065908432007
Validation loss: 2.1348664042770222

Epoch: 27| Step: 0
Training loss: 2.712938070297241
Validation loss: 2.1414004346375823

Epoch: 5| Step: 1
Training loss: 2.7417304515838623
Validation loss: 2.1405517772961686

Epoch: 5| Step: 2
Training loss: 2.6097521781921387
Validation loss: 2.0943560702826387

Epoch: 5| Step: 3
Training loss: 2.499875783920288
Validation loss: 2.107137854381274

Epoch: 5| Step: 4
Training loss: 2.256804943084717
Validation loss: 2.1306512483986477

Epoch: 5| Step: 5
Training loss: 2.4298338890075684
Validation loss: 2.131170465100196

Epoch: 5| Step: 6
Training loss: 2.768420696258545
Validation loss: 2.1392113854808192

Epoch: 5| Step: 7
Training loss: 1.734407663345337
Validation loss: 2.114423874885805

Epoch: 5| Step: 8
Training loss: 2.321906566619873
Validation loss: 2.0763957321002917

Epoch: 5| Step: 9
Training loss: 2.468999147415161
Validation loss: 2.096918236824774

Epoch: 5| Step: 10
Training loss: 2.3623619079589844
Validation loss: 2.1212774015242055

Epoch: 28| Step: 0
Training loss: 1.816593885421753
Validation loss: 2.1015680067000853

Epoch: 5| Step: 1
Training loss: 2.5892493724823
Validation loss: 2.124781652163434

Epoch: 5| Step: 2
Training loss: 2.078040599822998
Validation loss: 2.0938952007601337

Epoch: 5| Step: 3
Training loss: 2.462359666824341
Validation loss: 2.124332394651187

Epoch: 5| Step: 4
Training loss: 2.6273248195648193
Validation loss: 2.1188943039986396

Epoch: 5| Step: 5
Training loss: 2.2337253093719482
Validation loss: 2.089620133881928

Epoch: 5| Step: 6
Training loss: 2.9099009037017822
Validation loss: 2.1036079980993785

Epoch: 5| Step: 7
Training loss: 2.254699230194092
Validation loss: 2.10844628144336

Epoch: 5| Step: 8
Training loss: 2.5653347969055176
Validation loss: 2.115666781702349

Epoch: 5| Step: 9
Training loss: 2.9772777557373047
Validation loss: 2.0915487838047806

Epoch: 5| Step: 10
Training loss: 2.140723705291748
Validation loss: 2.122598904435353

Epoch: 29| Step: 0
Training loss: 2.1493172645568848
Validation loss: 2.1048244583991265

Epoch: 5| Step: 1
Training loss: 1.9582325220108032
Validation loss: 2.099049296430362

Epoch: 5| Step: 2
Training loss: 2.7550408840179443
Validation loss: 2.116798008641889

Epoch: 5| Step: 3
Training loss: 2.9883530139923096
Validation loss: 2.1006084898466706

Epoch: 5| Step: 4
Training loss: 2.761427402496338
Validation loss: 2.106338131812311

Epoch: 5| Step: 5
Training loss: 2.510521411895752
Validation loss: 2.098134010068832

Epoch: 5| Step: 6
Training loss: 2.473543882369995
Validation loss: 2.089079978645489

Epoch: 5| Step: 7
Training loss: 2.5261802673339844
Validation loss: 2.1134057173164944

Epoch: 5| Step: 8
Training loss: 1.932381272315979
Validation loss: 2.100729368066275

Epoch: 5| Step: 9
Training loss: 1.734317421913147
Validation loss: 2.1152849787025043

Epoch: 5| Step: 10
Training loss: 3.0078487396240234
Validation loss: 2.0897628120196763

Epoch: 30| Step: 0
Training loss: 2.473799228668213
Validation loss: 2.0923355830613004

Epoch: 5| Step: 1
Training loss: 2.8318018913269043
Validation loss: 2.109515843852874

Epoch: 5| Step: 2
Training loss: 2.613285541534424
Validation loss: 2.0958178222820325

Epoch: 5| Step: 3
Training loss: 2.7993974685668945
Validation loss: 2.087186646717851

Epoch: 5| Step: 4
Training loss: 1.7305023670196533
Validation loss: 2.0704050025632306

Epoch: 5| Step: 5
Training loss: 2.578946113586426
Validation loss: 2.102408173263714

Epoch: 5| Step: 6
Training loss: 2.633603572845459
Validation loss: 2.0874284057207007

Epoch: 5| Step: 7
Training loss: 1.9228897094726562
Validation loss: 2.088767987425609

Epoch: 5| Step: 8
Training loss: 2.4274277687072754
Validation loss: 2.0743202394054783

Epoch: 5| Step: 9
Training loss: 2.3256542682647705
Validation loss: 2.0820760906383557

Epoch: 5| Step: 10
Training loss: 2.4020328521728516
Validation loss: 2.0739229917526245

Epoch: 31| Step: 0
Training loss: 2.4042582511901855
Validation loss: 2.074693191436029

Epoch: 5| Step: 1
Training loss: 2.122885227203369
Validation loss: 2.067941860486102

Epoch: 5| Step: 2
Training loss: 1.8788305521011353
Validation loss: 2.092766281097166

Epoch: 5| Step: 3
Training loss: 2.2170815467834473
Validation loss: 2.075156145198371

Epoch: 5| Step: 4
Training loss: 2.420790195465088
Validation loss: 2.1114274442836805

Epoch: 5| Step: 5
Training loss: 3.359086513519287
Validation loss: 2.070423423603017

Epoch: 5| Step: 6
Training loss: 2.0979702472686768
Validation loss: 2.111561757262035

Epoch: 5| Step: 7
Training loss: 2.3181238174438477
Validation loss: 2.0905734531341063

Epoch: 5| Step: 8
Training loss: 3.069063425064087
Validation loss: 2.101134261777324

Epoch: 5| Step: 9
Training loss: 2.3803820610046387
Validation loss: 2.098189546215919

Epoch: 5| Step: 10
Training loss: 2.448324203491211
Validation loss: 2.1004749908242175

Epoch: 32| Step: 0
Training loss: 2.587872266769409
Validation loss: 2.078767248379287

Epoch: 5| Step: 1
Training loss: 2.4767603874206543
Validation loss: 2.107698009860131

Epoch: 5| Step: 2
Training loss: 2.724618434906006
Validation loss: 2.089262516267838

Epoch: 5| Step: 3
Training loss: 2.0542922019958496
Validation loss: 2.0924104695679038

Epoch: 5| Step: 4
Training loss: 2.5014281272888184
Validation loss: 2.079656893207181

Epoch: 5| Step: 5
Training loss: 2.387439250946045
Validation loss: 2.0882710179974957

Epoch: 5| Step: 6
Training loss: 2.812610149383545
Validation loss: 2.0766549700049945

Epoch: 5| Step: 7
Training loss: 1.9896739721298218
Validation loss: 2.1048510202797512

Epoch: 5| Step: 8
Training loss: 2.586782693862915
Validation loss: 2.0900997166992514

Epoch: 5| Step: 9
Training loss: 1.9209153652191162
Validation loss: 2.088176709349437

Epoch: 5| Step: 10
Training loss: 2.4933969974517822
Validation loss: 2.0945288429978075

Epoch: 33| Step: 0
Training loss: 2.1083621978759766
Validation loss: 2.0929482752277004

Epoch: 5| Step: 1
Training loss: 2.6218886375427246
Validation loss: 2.1039005517959595

Epoch: 5| Step: 2
Training loss: 2.3299965858459473
Validation loss: 2.087639780454738

Epoch: 5| Step: 3
Training loss: 2.1980814933776855
Validation loss: 2.06740834636073

Epoch: 5| Step: 4
Training loss: 2.4742326736450195
Validation loss: 2.077003922513736

Epoch: 5| Step: 5
Training loss: 2.0865249633789062
Validation loss: 2.0792066922751804

Epoch: 5| Step: 6
Training loss: 2.5662660598754883
Validation loss: 2.0700876840981106

Epoch: 5| Step: 7
Training loss: 2.6166892051696777
Validation loss: 2.0922603966087423

Epoch: 5| Step: 8
Training loss: 2.4292898178100586
Validation loss: 2.0846556361003588

Epoch: 5| Step: 9
Training loss: 2.6635653972625732
Validation loss: 2.0835167220843736

Epoch: 5| Step: 10
Training loss: 2.3428003787994385
Validation loss: 2.0721723392445552

Epoch: 34| Step: 0
Training loss: 2.723191738128662
Validation loss: 2.0994308738298315

Epoch: 5| Step: 1
Training loss: 2.430419683456421
Validation loss: 2.0956214525366343

Epoch: 5| Step: 2
Training loss: 2.01175594329834
Validation loss: 2.0919773488916378

Epoch: 5| Step: 3
Training loss: 2.4549763202667236
Validation loss: 2.0750531419630973

Epoch: 5| Step: 4
Training loss: 2.5885443687438965
Validation loss: 2.0876994132995605

Epoch: 5| Step: 5
Training loss: 1.8752925395965576
Validation loss: 2.0670831972552883

Epoch: 5| Step: 6
Training loss: 2.5592424869537354
Validation loss: 2.100138202790291

Epoch: 5| Step: 7
Training loss: 2.589752435684204
Validation loss: 2.065969592781477

Epoch: 5| Step: 8
Training loss: 2.078655242919922
Validation loss: 2.095802293028883

Epoch: 5| Step: 9
Training loss: 2.8617630004882812
Validation loss: 2.0797045910230247

Epoch: 5| Step: 10
Training loss: 2.2567474842071533
Validation loss: 2.0870959258848623

Epoch: 35| Step: 0
Training loss: 3.2583987712860107
Validation loss: 2.0853891372680664

Epoch: 5| Step: 1
Training loss: 2.8198134899139404
Validation loss: 2.069938762213594

Epoch: 5| Step: 2
Training loss: 2.0246968269348145
Validation loss: 2.096130801785377

Epoch: 5| Step: 3
Training loss: 2.5260391235351562
Validation loss: 2.0937897941117645

Epoch: 5| Step: 4
Training loss: 2.461866617202759
Validation loss: 2.0792484796175392

Epoch: 5| Step: 5
Training loss: 2.228997230529785
Validation loss: 2.1065353167954313

Epoch: 5| Step: 6
Training loss: 2.68373441696167
Validation loss: 2.084831383920485

Epoch: 5| Step: 7
Training loss: 2.3090317249298096
Validation loss: 2.0856611292849303

Epoch: 5| Step: 8
Training loss: 1.9803149700164795
Validation loss: 2.094293196996053

Epoch: 5| Step: 9
Training loss: 2.1801564693450928
Validation loss: 2.073372144852915

Epoch: 5| Step: 10
Training loss: 1.8793882131576538
Validation loss: 2.069232815055437

Epoch: 36| Step: 0
Training loss: 2.7193667888641357
Validation loss: 2.0983703879899878

Epoch: 5| Step: 1
Training loss: 2.546952486038208
Validation loss: 2.0811350191793134

Epoch: 5| Step: 2
Training loss: 2.402367115020752
Validation loss: 2.0808290537967475

Epoch: 5| Step: 3
Training loss: 2.6206796169281006
Validation loss: 2.082587311344762

Epoch: 5| Step: 4
Training loss: 2.378243923187256
Validation loss: 2.074659707725689

Epoch: 5| Step: 5
Training loss: 2.0522027015686035
Validation loss: 2.063364441676806

Epoch: 5| Step: 6
Training loss: 2.5078938007354736
Validation loss: 2.077606593408892

Epoch: 5| Step: 7
Training loss: 2.4028594493865967
Validation loss: 2.0746245127852245

Epoch: 5| Step: 8
Training loss: 2.3396267890930176
Validation loss: 2.063881843320785

Epoch: 5| Step: 9
Training loss: 2.271775007247925
Validation loss: 2.0591151175960416

Epoch: 5| Step: 10
Training loss: 2.0455265045166016
Validation loss: 2.053487423927553

Epoch: 37| Step: 0
Training loss: 2.1079251766204834
Validation loss: 2.066196980014924

Epoch: 5| Step: 1
Training loss: 2.4133026599884033
Validation loss: 2.0680911643530733

Epoch: 5| Step: 2
Training loss: 1.8149162530899048
Validation loss: 2.0874419596887406

Epoch: 5| Step: 3
Training loss: 2.102865695953369
Validation loss: 2.06684555802294

Epoch: 5| Step: 4
Training loss: 3.001769542694092
Validation loss: 2.0542165079424457

Epoch: 5| Step: 5
Training loss: 2.0443127155303955
Validation loss: 2.056525679044826

Epoch: 5| Step: 6
Training loss: 2.7854177951812744
Validation loss: 2.053757172758861

Epoch: 5| Step: 7
Training loss: 2.9722166061401367
Validation loss: 2.0499074497530536

Epoch: 5| Step: 8
Training loss: 2.6698708534240723
Validation loss: 2.0561784775026384

Epoch: 5| Step: 9
Training loss: 2.12402081489563
Validation loss: 2.059378582944152

Epoch: 5| Step: 10
Training loss: 2.2252037525177
Validation loss: 2.0671221504929247

Epoch: 38| Step: 0
Training loss: 2.013087272644043
Validation loss: 2.0788098176320395

Epoch: 5| Step: 1
Training loss: 2.3989949226379395
Validation loss: 2.052595958914808

Epoch: 5| Step: 2
Training loss: 2.1955485343933105
Validation loss: 2.060716518791773

Epoch: 5| Step: 3
Training loss: 2.5030198097229004
Validation loss: 2.0467744988779866

Epoch: 5| Step: 4
Training loss: 1.6675035953521729
Validation loss: 2.0296385929148686

Epoch: 5| Step: 5
Training loss: 2.409726619720459
Validation loss: 2.0378310911117063

Epoch: 5| Step: 6
Training loss: 2.469292163848877
Validation loss: 2.0323436798587924

Epoch: 5| Step: 7
Training loss: 2.8397231101989746
Validation loss: 2.0693104677302863

Epoch: 5| Step: 8
Training loss: 2.589046001434326
Validation loss: 2.050497692118409

Epoch: 5| Step: 9
Training loss: 2.6700150966644287
Validation loss: 2.0458217397812875

Epoch: 5| Step: 10
Training loss: 2.4584572315216064
Validation loss: 2.058578005401037

Epoch: 39| Step: 0
Training loss: 2.05267071723938
Validation loss: 2.049601501034152

Epoch: 5| Step: 1
Training loss: 2.280548572540283
Validation loss: 2.0337847766055854

Epoch: 5| Step: 2
Training loss: 2.604841470718384
Validation loss: 2.0752543454529135

Epoch: 5| Step: 3
Training loss: 2.254800319671631
Validation loss: 2.082636825499996

Epoch: 5| Step: 4
Training loss: 2.2555091381073
Validation loss: 2.022191580905709

Epoch: 5| Step: 5
Training loss: 2.492990255355835
Validation loss: 2.020141370834843

Epoch: 5| Step: 6
Training loss: 2.6354708671569824
Validation loss: 2.0651225261790778

Epoch: 5| Step: 7
Training loss: 2.463089942932129
Validation loss: 2.0421303395302064

Epoch: 5| Step: 8
Training loss: 2.178685426712036
Validation loss: 2.049260998284945

Epoch: 5| Step: 9
Training loss: 1.7729051113128662
Validation loss: 2.0326779670612787

Epoch: 5| Step: 10
Training loss: 3.184664487838745
Validation loss: 2.0482206011331208

Epoch: 40| Step: 0
Training loss: 2.066646099090576
Validation loss: 2.047525403320148

Epoch: 5| Step: 1
Training loss: 2.711714267730713
Validation loss: 2.036765370317685

Epoch: 5| Step: 2
Training loss: 2.034036636352539
Validation loss: 2.043160838465537

Epoch: 5| Step: 3
Training loss: 2.199472665786743
Validation loss: 2.0489482084910073

Epoch: 5| Step: 4
Training loss: 2.809016227722168
Validation loss: 2.058171410714426

Epoch: 5| Step: 5
Training loss: 2.813424587249756
Validation loss: 2.055523951848348

Epoch: 5| Step: 6
Training loss: 2.6117796897888184
Validation loss: 2.0584580334283973

Epoch: 5| Step: 7
Training loss: 2.3230843544006348
Validation loss: 2.062034986352408

Epoch: 5| Step: 8
Training loss: 2.3846182823181152
Validation loss: 2.0497687837128997

Epoch: 5| Step: 9
Training loss: 2.1393513679504395
Validation loss: 2.0703821951343166

Epoch: 5| Step: 10
Training loss: 2.191877841949463
Validation loss: 2.0672230553883377

Epoch: 41| Step: 0
Training loss: 2.619572162628174
Validation loss: 2.0380588141820764

Epoch: 5| Step: 1
Training loss: 2.23229718208313
Validation loss: 2.080829685734164

Epoch: 5| Step: 2
Training loss: 2.4527058601379395
Validation loss: 2.0293216615594845

Epoch: 5| Step: 3
Training loss: 2.4720654487609863
Validation loss: 2.0969154065655125

Epoch: 5| Step: 4
Training loss: 2.4108519554138184
Validation loss: 2.075814515031794

Epoch: 5| Step: 5
Training loss: 2.3112378120422363
Validation loss: 2.0949854414950133

Epoch: 5| Step: 6
Training loss: 2.0082221031188965
Validation loss: 2.057817448851883

Epoch: 5| Step: 7
Training loss: 2.8224472999572754
Validation loss: 2.0684697294747956

Epoch: 5| Step: 8
Training loss: 1.8119665384292603
Validation loss: 2.0866242659989225

Epoch: 5| Step: 9
Training loss: 2.6768760681152344
Validation loss: 2.080947337612029

Epoch: 5| Step: 10
Training loss: 2.246941328048706
Validation loss: 2.048432042521815

Epoch: 42| Step: 0
Training loss: 2.187762498855591
Validation loss: 2.0942207356934905

Epoch: 5| Step: 1
Training loss: 2.6853814125061035
Validation loss: 2.068529599456377

Epoch: 5| Step: 2
Training loss: 2.3442835807800293
Validation loss: 2.0632828525317612

Epoch: 5| Step: 3
Training loss: 2.4225494861602783
Validation loss: 2.064753432427683

Epoch: 5| Step: 4
Training loss: 2.0529747009277344
Validation loss: 2.0644012292226157

Epoch: 5| Step: 5
Training loss: 3.0908031463623047
Validation loss: 2.0762125112677134

Epoch: 5| Step: 6
Training loss: 2.312640428543091
Validation loss: 2.0337133651138632

Epoch: 5| Step: 7
Training loss: 1.9766496419906616
Validation loss: 2.071290874993929

Epoch: 5| Step: 8
Training loss: 2.750523567199707
Validation loss: 2.0592941430307206

Epoch: 5| Step: 9
Training loss: 1.751856803894043
Validation loss: 2.0581389768149263

Epoch: 5| Step: 10
Training loss: 2.458834171295166
Validation loss: 2.038586308879237

Epoch: 43| Step: 0
Training loss: 2.6036343574523926
Validation loss: 2.0628320606805945

Epoch: 5| Step: 1
Training loss: 2.006770610809326
Validation loss: 2.046319043764504

Epoch: 5| Step: 2
Training loss: 1.7198588848114014
Validation loss: 2.0479389980275142

Epoch: 5| Step: 3
Training loss: 2.8445372581481934
Validation loss: 2.0393489432591263

Epoch: 5| Step: 4
Training loss: 1.5473730564117432
Validation loss: 2.053881511893324

Epoch: 5| Step: 5
Training loss: 2.8295998573303223
Validation loss: 2.0519161224365234

Epoch: 5| Step: 6
Training loss: 2.652346134185791
Validation loss: 2.071064467071205

Epoch: 5| Step: 7
Training loss: 2.5788447856903076
Validation loss: 2.0501487870370187

Epoch: 5| Step: 8
Training loss: 1.9461698532104492
Validation loss: 2.0528167396463375

Epoch: 5| Step: 9
Training loss: 2.4505553245544434
Validation loss: 2.0631068034838607

Epoch: 5| Step: 10
Training loss: 2.8279876708984375
Validation loss: 2.0604390431475896

Epoch: 44| Step: 0
Training loss: 2.1630990505218506
Validation loss: 2.0466083031828686

Epoch: 5| Step: 1
Training loss: 2.35823392868042
Validation loss: 2.0558205932699223

Epoch: 5| Step: 2
Training loss: 2.8982763290405273
Validation loss: 2.053878415015436

Epoch: 5| Step: 3
Training loss: 2.556591510772705
Validation loss: 2.060918402928178

Epoch: 5| Step: 4
Training loss: 1.5151093006134033
Validation loss: 2.047493907713121

Epoch: 5| Step: 5
Training loss: 2.0239531993865967
Validation loss: 2.0498059065111223

Epoch: 5| Step: 6
Training loss: 2.9591734409332275
Validation loss: 2.034193314531798

Epoch: 5| Step: 7
Training loss: 2.1366682052612305
Validation loss: 2.0237910055345103

Epoch: 5| Step: 8
Training loss: 2.8420395851135254
Validation loss: 2.0548576334471345

Epoch: 5| Step: 9
Training loss: 2.501020908355713
Validation loss: 2.020237215103642

Epoch: 5| Step: 10
Training loss: 2.0977115631103516
Validation loss: 2.0132009624153056

Epoch: 45| Step: 0
Training loss: 2.201900005340576
Validation loss: 2.032435369747941

Epoch: 5| Step: 1
Training loss: 2.438567876815796
Validation loss: 2.029006024842621

Epoch: 5| Step: 2
Training loss: 2.045271635055542
Validation loss: 2.055307138350702

Epoch: 5| Step: 3
Training loss: 1.60626220703125
Validation loss: 2.0384450112619708

Epoch: 5| Step: 4
Training loss: 2.051151752471924
Validation loss: 2.0498547489925096

Epoch: 5| Step: 5
Training loss: 2.3256683349609375
Validation loss: 2.0603046430054532

Epoch: 5| Step: 6
Training loss: 2.599534034729004
Validation loss: 2.0465866237558346

Epoch: 5| Step: 7
Training loss: 2.3937759399414062
Validation loss: 2.020840388472362

Epoch: 5| Step: 8
Training loss: 2.3016598224639893
Validation loss: 2.022164229423769

Epoch: 5| Step: 9
Training loss: 2.688602924346924
Validation loss: 2.0430141161846858

Epoch: 5| Step: 10
Training loss: 3.106494188308716
Validation loss: 2.0697481016958914

Epoch: 46| Step: 0
Training loss: 1.7591949701309204
Validation loss: 2.048411432132926

Epoch: 5| Step: 1
Training loss: 2.4753899574279785
Validation loss: 2.0555200140963317

Epoch: 5| Step: 2
Training loss: 1.8313995599746704
Validation loss: 2.076240663887352

Epoch: 5| Step: 3
Training loss: 2.5990421772003174
Validation loss: 2.0323606075779086

Epoch: 5| Step: 4
Training loss: 1.7977969646453857
Validation loss: 2.023644019198674

Epoch: 5| Step: 5
Training loss: 2.2261836528778076
Validation loss: 2.010671651491555

Epoch: 5| Step: 6
Training loss: 2.4958596229553223
Validation loss: 2.022392139639906

Epoch: 5| Step: 7
Training loss: 3.389827013015747
Validation loss: 2.0137306926071004

Epoch: 5| Step: 8
Training loss: 2.5823347568511963
Validation loss: 2.0353220457671792

Epoch: 5| Step: 9
Training loss: 2.8156185150146484
Validation loss: 2.019932343113807

Epoch: 5| Step: 10
Training loss: 2.0139031410217285
Validation loss: 2.0091535814346804

Epoch: 47| Step: 0
Training loss: 2.4870104789733887
Validation loss: 2.0193964050662134

Epoch: 5| Step: 1
Training loss: 2.375206708908081
Validation loss: 2.0578026822818223

Epoch: 5| Step: 2
Training loss: 2.521557569503784
Validation loss: 2.0303501659824

Epoch: 5| Step: 3
Training loss: 2.3092331886291504
Validation loss: 2.0255474505885953

Epoch: 5| Step: 4
Training loss: 2.1904590129852295
Validation loss: 2.041440902217742

Epoch: 5| Step: 5
Training loss: 2.3988630771636963
Validation loss: 2.0377305835805912

Epoch: 5| Step: 6
Training loss: 2.346656084060669
Validation loss: 2.0428166735556816

Epoch: 5| Step: 7
Training loss: 1.8801826238632202
Validation loss: 2.043781759918377

Epoch: 5| Step: 8
Training loss: 2.305839776992798
Validation loss: 2.011754988342203

Epoch: 5| Step: 9
Training loss: 2.0817298889160156
Validation loss: 2.04420320962065

Epoch: 5| Step: 10
Training loss: 2.9157209396362305
Validation loss: 2.0334389055928876

Epoch: 48| Step: 0
Training loss: 2.63777494430542
Validation loss: 2.045938120093397

Epoch: 5| Step: 1
Training loss: 2.813549757003784
Validation loss: 2.0382350260211575

Epoch: 5| Step: 2
Training loss: 2.2940473556518555
Validation loss: 2.0482994125735376

Epoch: 5| Step: 3
Training loss: 2.1269092559814453
Validation loss: 2.01184219186024

Epoch: 5| Step: 4
Training loss: 2.2544004917144775
Validation loss: 2.0514607032140098

Epoch: 5| Step: 5
Training loss: 2.380077362060547
Validation loss: 2.035386327774294

Epoch: 5| Step: 6
Training loss: 2.410547971725464
Validation loss: 2.024956031512189

Epoch: 5| Step: 7
Training loss: 1.4201700687408447
Validation loss: 2.022598813938838

Epoch: 5| Step: 8
Training loss: 2.342336654663086
Validation loss: 2.036554821075932

Epoch: 5| Step: 9
Training loss: 2.582486629486084
Validation loss: 2.062435596219955

Epoch: 5| Step: 10
Training loss: 2.483961582183838
Validation loss: 2.022603473355693

Epoch: 49| Step: 0
Training loss: 1.6435728073120117
Validation loss: 2.0390584955933275

Epoch: 5| Step: 1
Training loss: 2.61350154876709
Validation loss: 2.0497747493046585

Epoch: 5| Step: 2
Training loss: 2.650097370147705
Validation loss: 2.038994026440446

Epoch: 5| Step: 3
Training loss: 2.7970452308654785
Validation loss: 2.0360034960572437

Epoch: 5| Step: 4
Training loss: 2.0135257244110107
Validation loss: 2.0340511081039265

Epoch: 5| Step: 5
Training loss: 2.395455837249756
Validation loss: 2.025059420575378

Epoch: 5| Step: 6
Training loss: 1.6918962001800537
Validation loss: 2.0590649368942424

Epoch: 5| Step: 7
Training loss: 2.6462912559509277
Validation loss: 2.05981614769146

Epoch: 5| Step: 8
Training loss: 2.6415927410125732
Validation loss: 2.046636296856788

Epoch: 5| Step: 9
Training loss: 2.004138708114624
Validation loss: 2.019281546274821

Epoch: 5| Step: 10
Training loss: 2.762077569961548
Validation loss: 2.037964169697095

Epoch: 50| Step: 0
Training loss: 3.1447572708129883
Validation loss: 2.0464490305992866

Epoch: 5| Step: 1
Training loss: 2.6942126750946045
Validation loss: 2.0214296553724553

Epoch: 5| Step: 2
Training loss: 2.232729434967041
Validation loss: 2.0307613418948267

Epoch: 5| Step: 3
Training loss: 2.4083428382873535
Validation loss: 2.0404433204281713

Epoch: 5| Step: 4
Training loss: 1.9305579662322998
Validation loss: 2.0349649972813104

Epoch: 5| Step: 5
Training loss: 1.8889089822769165
Validation loss: 2.037768863862561

Epoch: 5| Step: 6
Training loss: 2.010132074356079
Validation loss: 2.0333946430554954

Epoch: 5| Step: 7
Training loss: 2.7449593544006348
Validation loss: 2.0542816321055093

Epoch: 5| Step: 8
Training loss: 2.1643879413604736
Validation loss: 2.0356328436123428

Epoch: 5| Step: 9
Training loss: 1.897146463394165
Validation loss: 2.03826355677779

Epoch: 5| Step: 10
Training loss: 2.7793447971343994
Validation loss: 2.0240244493689588

Epoch: 51| Step: 0
Training loss: 2.594968795776367
Validation loss: 2.0575304979919107

Epoch: 5| Step: 1
Training loss: 2.314879894256592
Validation loss: 2.039862012350431

Epoch: 5| Step: 2
Training loss: 1.9964501857757568
Validation loss: 2.0491541226704917

Epoch: 5| Step: 3
Training loss: 2.617658853530884
Validation loss: 2.024504493641597

Epoch: 5| Step: 4
Training loss: 1.3729076385498047
Validation loss: 2.037831901222147

Epoch: 5| Step: 5
Training loss: 2.396104335784912
Validation loss: 2.043944663898919

Epoch: 5| Step: 6
Training loss: 2.751349925994873
Validation loss: 2.056339058824765

Epoch: 5| Step: 7
Training loss: 2.6590800285339355
Validation loss: 2.028309552900253

Epoch: 5| Step: 8
Training loss: 2.445387840270996
Validation loss: 2.0318751770962953

Epoch: 5| Step: 9
Training loss: 2.3595564365386963
Validation loss: 2.027965218790116

Epoch: 5| Step: 10
Training loss: 2.087453603744507
Validation loss: 2.033468238769039

Epoch: 52| Step: 0
Training loss: 2.7384963035583496
Validation loss: 2.0356124883056967

Epoch: 5| Step: 1
Training loss: 1.9274364709854126
Validation loss: 2.015794771973805

Epoch: 5| Step: 2
Training loss: 2.8256752490997314
Validation loss: 2.048533369136113

Epoch: 5| Step: 3
Training loss: 2.241575241088867
Validation loss: 2.036866769995741

Epoch: 5| Step: 4
Training loss: 2.693427562713623
Validation loss: 2.053400524200932

Epoch: 5| Step: 5
Training loss: 1.9778425693511963
Validation loss: 2.0445305352569907

Epoch: 5| Step: 6
Training loss: 2.2408154010772705
Validation loss: 2.0278128193270777

Epoch: 5| Step: 7
Training loss: 2.232917070388794
Validation loss: 2.040103498325553

Epoch: 5| Step: 8
Training loss: 2.4381701946258545
Validation loss: 2.044683873012502

Epoch: 5| Step: 9
Training loss: 2.467977523803711
Validation loss: 2.0444020994247927

Epoch: 5| Step: 10
Training loss: 1.8487975597381592
Validation loss: 2.0685644995781685

Epoch: 53| Step: 0
Training loss: 2.0631980895996094
Validation loss: 2.0610716573653685

Epoch: 5| Step: 1
Training loss: 2.2779977321624756
Validation loss: 2.0579900574940506

Epoch: 5| Step: 2
Training loss: 2.621886730194092
Validation loss: 2.0517159892666723

Epoch: 5| Step: 3
Training loss: 2.636509656906128
Validation loss: 2.0606684633480605

Epoch: 5| Step: 4
Training loss: 2.8095855712890625
Validation loss: 2.0467354200219594

Epoch: 5| Step: 5
Training loss: 1.7877922058105469
Validation loss: 2.050477995667406

Epoch: 5| Step: 6
Training loss: 1.9245727062225342
Validation loss: 2.0843375728976343

Epoch: 5| Step: 7
Training loss: 2.398329019546509
Validation loss: 2.0690838854799987

Epoch: 5| Step: 8
Training loss: 2.234469175338745
Validation loss: 2.05298073445597

Epoch: 5| Step: 9
Training loss: 2.2638721466064453
Validation loss: 2.052803393333189

Epoch: 5| Step: 10
Training loss: 2.8437952995300293
Validation loss: 2.0701463542958742

Epoch: 54| Step: 0
Training loss: 2.1319844722747803
Validation loss: 2.0656731718329975

Epoch: 5| Step: 1
Training loss: 2.698434829711914
Validation loss: 2.054099034237605

Epoch: 5| Step: 2
Training loss: 2.2464852333068848
Validation loss: 2.026771324937062

Epoch: 5| Step: 3
Training loss: 2.953343391418457
Validation loss: 2.0192825691674345

Epoch: 5| Step: 4
Training loss: 1.8076730966567993
Validation loss: 2.036611605716008

Epoch: 5| Step: 5
Training loss: 2.041750431060791
Validation loss: 2.018704006748815

Epoch: 5| Step: 6
Training loss: 2.4047954082489014
Validation loss: 2.0163421425768124

Epoch: 5| Step: 7
Training loss: 2.5152955055236816
Validation loss: 2.0228603257927844

Epoch: 5| Step: 8
Training loss: 2.272521734237671
Validation loss: 2.040957981540311

Epoch: 5| Step: 9
Training loss: 2.1261062622070312
Validation loss: 2.056920334856997

Epoch: 5| Step: 10
Training loss: 2.538703680038452
Validation loss: 2.0527650720329693

Epoch: 55| Step: 0
Training loss: 2.1825966835021973
Validation loss: 2.0144932295686457

Epoch: 5| Step: 1
Training loss: 1.4858310222625732
Validation loss: 2.018318501851892

Epoch: 5| Step: 2
Training loss: 2.864041805267334
Validation loss: 2.0241909719282583

Epoch: 5| Step: 3
Training loss: 2.3986003398895264
Validation loss: 2.012031593630391

Epoch: 5| Step: 4
Training loss: 2.053008556365967
Validation loss: 2.0243312825438795

Epoch: 5| Step: 5
Training loss: 1.9383609294891357
Validation loss: 1.9983183030159242

Epoch: 5| Step: 6
Training loss: 2.471190929412842
Validation loss: 2.0381699941491567

Epoch: 5| Step: 7
Training loss: 2.9650635719299316
Validation loss: 1.9999876329975743

Epoch: 5| Step: 8
Training loss: 2.1044700145721436
Validation loss: 2.0036447842915854

Epoch: 5| Step: 9
Training loss: 2.130067825317383
Validation loss: 2.0206749208511843

Epoch: 5| Step: 10
Training loss: 3.0313098430633545
Validation loss: 2.0380942885593702

Epoch: 56| Step: 0
Training loss: 2.639533281326294
Validation loss: 2.0500017699374946

Epoch: 5| Step: 1
Training loss: 2.562797784805298
Validation loss: 2.0276031391595

Epoch: 5| Step: 2
Training loss: 1.8473418951034546
Validation loss: 2.0517815505304644

Epoch: 5| Step: 3
Training loss: 2.0447895526885986
Validation loss: 2.010546412519229

Epoch: 5| Step: 4
Training loss: 2.4792299270629883
Validation loss: 2.033336090785201

Epoch: 5| Step: 5
Training loss: 2.6736018657684326
Validation loss: 2.045787554915233

Epoch: 5| Step: 6
Training loss: 2.5945780277252197
Validation loss: 2.025561632648591

Epoch: 5| Step: 7
Training loss: 2.329921245574951
Validation loss: 2.0336621628012708

Epoch: 5| Step: 8
Training loss: 2.4170875549316406
Validation loss: 2.0214142376376736

Epoch: 5| Step: 9
Training loss: 1.8285757303237915
Validation loss: 2.0367268528989566

Epoch: 5| Step: 10
Training loss: 2.0949957370758057
Validation loss: 2.03626153802359

Epoch: 57| Step: 0
Training loss: 2.193497896194458
Validation loss: 2.0068445974780666

Epoch: 5| Step: 1
Training loss: 2.1621856689453125
Validation loss: 2.018181603441956

Epoch: 5| Step: 2
Training loss: 2.119081497192383
Validation loss: 2.032300482514084

Epoch: 5| Step: 3
Training loss: 2.265434741973877
Validation loss: 2.035291266697709

Epoch: 5| Step: 4
Training loss: 1.6304649114608765
Validation loss: 2.0405539133215465

Epoch: 5| Step: 5
Training loss: 3.0075340270996094
Validation loss: 2.020972398019606

Epoch: 5| Step: 6
Training loss: 1.8458877801895142
Validation loss: 2.044603263178179

Epoch: 5| Step: 7
Training loss: 2.602560520172119
Validation loss: 2.024750037859845

Epoch: 5| Step: 8
Training loss: 2.8340096473693848
Validation loss: 2.034502355001306

Epoch: 5| Step: 9
Training loss: 2.4680213928222656
Validation loss: 2.0076220022734774

Epoch: 5| Step: 10
Training loss: 2.2291767597198486
Validation loss: 2.03614051624011

Epoch: 58| Step: 0
Training loss: 2.2792763710021973
Validation loss: 2.0098364404452744

Epoch: 5| Step: 1
Training loss: 2.0777783393859863
Validation loss: 2.0249089887065272

Epoch: 5| Step: 2
Training loss: 2.3783698081970215
Validation loss: 2.011057840880527

Epoch: 5| Step: 3
Training loss: 2.8989644050598145
Validation loss: 2.0371039093181653

Epoch: 5| Step: 4
Training loss: 2.0263442993164062
Validation loss: 2.036634611827071

Epoch: 5| Step: 5
Training loss: 1.4759794473648071
Validation loss: 2.0330535006779495

Epoch: 5| Step: 6
Training loss: 2.108811616897583
Validation loss: 2.030508279800415

Epoch: 5| Step: 7
Training loss: 2.596210479736328
Validation loss: 2.0252348120494554

Epoch: 5| Step: 8
Training loss: 1.8101320266723633
Validation loss: 2.0264279432194208

Epoch: 5| Step: 9
Training loss: 3.2882816791534424
Validation loss: 2.0372977154229277

Epoch: 5| Step: 10
Training loss: 2.6173596382141113
Validation loss: 2.027400355185232

Epoch: 59| Step: 0
Training loss: 2.0610690116882324
Validation loss: 2.026105446200217

Epoch: 5| Step: 1
Training loss: 1.6856157779693604
Validation loss: 2.0277628449983496

Epoch: 5| Step: 2
Training loss: 2.392314910888672
Validation loss: 2.051860493998374

Epoch: 5| Step: 3
Training loss: 2.0199785232543945
Validation loss: 2.027774777463687

Epoch: 5| Step: 4
Training loss: 2.217374086380005
Validation loss: 2.031183931135362

Epoch: 5| Step: 5
Training loss: 2.9239566326141357
Validation loss: 2.0510537931996007

Epoch: 5| Step: 6
Training loss: 1.855897307395935
Validation loss: 2.0318344587920816

Epoch: 5| Step: 7
Training loss: 2.5043792724609375
Validation loss: 2.0461687118776384

Epoch: 5| Step: 8
Training loss: 2.546971559524536
Validation loss: 2.038373995852727

Epoch: 5| Step: 9
Training loss: 2.3610763549804688
Validation loss: 2.0559446375857116

Epoch: 5| Step: 10
Training loss: 2.9857492446899414
Validation loss: 2.0449345240028958

Epoch: 60| Step: 0
Training loss: 2.0349066257476807
Validation loss: 2.0613324719090618

Epoch: 5| Step: 1
Training loss: 2.3488218784332275
Validation loss: 2.039078366371893

Epoch: 5| Step: 2
Training loss: 2.3565115928649902
Validation loss: 2.056458712905966

Epoch: 5| Step: 3
Training loss: 2.690955400466919
Validation loss: 2.037653261615384

Epoch: 5| Step: 4
Training loss: 1.86794912815094
Validation loss: 2.031945477249802

Epoch: 5| Step: 5
Training loss: 2.362272024154663
Validation loss: 2.0277439176395373

Epoch: 5| Step: 6
Training loss: 2.5142078399658203
Validation loss: 2.029254480074811

Epoch: 5| Step: 7
Training loss: 2.6972460746765137
Validation loss: 2.021243279980075

Epoch: 5| Step: 8
Training loss: 2.6462488174438477
Validation loss: 2.044166329086468

Epoch: 5| Step: 9
Training loss: 1.8628736734390259
Validation loss: 2.015823427067008

Epoch: 5| Step: 10
Training loss: 2.0551273822784424
Validation loss: 2.0230167091533704

Epoch: 61| Step: 0
Training loss: 1.8898998498916626
Validation loss: 2.0215070286104755

Epoch: 5| Step: 1
Training loss: 1.9818185567855835
Validation loss: 2.02710235247048

Epoch: 5| Step: 2
Training loss: 2.0950188636779785
Validation loss: 2.029402249602861

Epoch: 5| Step: 3
Training loss: 2.2744321823120117
Validation loss: 2.0110658663575367

Epoch: 5| Step: 4
Training loss: 3.108943462371826
Validation loss: 2.028795269227797

Epoch: 5| Step: 5
Training loss: 2.130842924118042
Validation loss: 2.0199663613432195

Epoch: 5| Step: 6
Training loss: 2.2804617881774902
Validation loss: 1.9989090555457658

Epoch: 5| Step: 7
Training loss: 2.643284320831299
Validation loss: 2.0275449086261053

Epoch: 5| Step: 8
Training loss: 1.876199722290039
Validation loss: 2.011189071081018

Epoch: 5| Step: 9
Training loss: 2.5049824714660645
Validation loss: 2.0230583016590407

Epoch: 5| Step: 10
Training loss: 2.5020556449890137
Validation loss: 2.0175426339590423

Epoch: 62| Step: 0
Training loss: 2.3796803951263428
Validation loss: 2.0056476926290863

Epoch: 5| Step: 1
Training loss: 2.0774970054626465
Validation loss: 2.0241746569192536

Epoch: 5| Step: 2
Training loss: 2.112173557281494
Validation loss: 2.0027660733910015

Epoch: 5| Step: 3
Training loss: 2.467320203781128
Validation loss: 2.015743233824289

Epoch: 5| Step: 4
Training loss: 2.349473237991333
Validation loss: 2.0239723972094956

Epoch: 5| Step: 5
Training loss: 1.8096119165420532
Validation loss: 2.0015879587460588

Epoch: 5| Step: 6
Training loss: 2.440221071243286
Validation loss: 2.0106269800534813

Epoch: 5| Step: 7
Training loss: 2.3957104682922363
Validation loss: 2.0196721220529206

Epoch: 5| Step: 8
Training loss: 2.5724310874938965
Validation loss: 2.0281794199379544

Epoch: 5| Step: 9
Training loss: 2.489872694015503
Validation loss: 2.018290520996176

Epoch: 5| Step: 10
Training loss: 2.198493242263794
Validation loss: 2.0198667921045774

Epoch: 63| Step: 0
Training loss: 2.7177722454071045
Validation loss: 2.02851838322096

Epoch: 5| Step: 1
Training loss: 2.1475443840026855
Validation loss: 2.0130688951861475

Epoch: 5| Step: 2
Training loss: 2.61482572555542
Validation loss: 2.0420440268772904

Epoch: 5| Step: 3
Training loss: 2.8380722999572754
Validation loss: 2.011101843208395

Epoch: 5| Step: 4
Training loss: 1.71014404296875
Validation loss: 2.0250726784429243

Epoch: 5| Step: 5
Training loss: 2.1237878799438477
Validation loss: 2.016758088142641

Epoch: 5| Step: 6
Training loss: 2.129838466644287
Validation loss: 2.0188967745791198

Epoch: 5| Step: 7
Training loss: 2.4208762645721436
Validation loss: 2.013603279667516

Epoch: 5| Step: 8
Training loss: 2.003696918487549
Validation loss: 2.0264920957626833

Epoch: 5| Step: 9
Training loss: 2.3245487213134766
Validation loss: 2.0430287225272066

Epoch: 5| Step: 10
Training loss: 2.2338740825653076
Validation loss: 2.027729824025144

Epoch: 64| Step: 0
Training loss: 2.1899569034576416
Validation loss: 2.0178524217297955

Epoch: 5| Step: 1
Training loss: 1.9049373865127563
Validation loss: 2.009622553343414

Epoch: 5| Step: 2
Training loss: 2.1980502605438232
Validation loss: 2.028567998639999

Epoch: 5| Step: 3
Training loss: 1.856410026550293
Validation loss: 2.0331229420118433

Epoch: 5| Step: 4
Training loss: 2.7310616970062256
Validation loss: 2.0403463225210867

Epoch: 5| Step: 5
Training loss: 2.680647611618042
Validation loss: 2.018274190605328

Epoch: 5| Step: 6
Training loss: 1.8775272369384766
Validation loss: 2.0218692543686076

Epoch: 5| Step: 7
Training loss: 2.2314183712005615
Validation loss: 2.0258840745495212

Epoch: 5| Step: 8
Training loss: 3.313978672027588
Validation loss: 2.032047379401422

Epoch: 5| Step: 9
Training loss: 2.4957213401794434
Validation loss: 2.03870411585736

Epoch: 5| Step: 10
Training loss: 1.8739047050476074
Validation loss: 2.0313219742108415

Epoch: 65| Step: 0
Training loss: 1.6149450540542603
Validation loss: 2.0270403790217575

Epoch: 5| Step: 1
Training loss: 2.76060152053833
Validation loss: 2.0315435317254837

Epoch: 5| Step: 2
Training loss: 1.7236238718032837
Validation loss: 2.039189551466255

Epoch: 5| Step: 3
Training loss: 2.88789439201355
Validation loss: 2.036740010784518

Epoch: 5| Step: 4
Training loss: 1.9630000591278076
Validation loss: 2.051203763613137

Epoch: 5| Step: 5
Training loss: 1.9082549810409546
Validation loss: 2.0292998539504183

Epoch: 5| Step: 6
Training loss: 2.991868734359741
Validation loss: 2.0263445300440632

Epoch: 5| Step: 7
Training loss: 2.246951103210449
Validation loss: 2.0316017866134644

Epoch: 5| Step: 8
Training loss: 2.467484712600708
Validation loss: 2.037944168172857

Epoch: 5| Step: 9
Training loss: 2.328718423843384
Validation loss: 2.019869282681455

Epoch: 5| Step: 10
Training loss: 2.4585700035095215
Validation loss: 2.036078965792092

Epoch: 66| Step: 0
Training loss: 2.025383710861206
Validation loss: 2.032946220008276

Epoch: 5| Step: 1
Training loss: 2.1770317554473877
Validation loss: 2.009549581876365

Epoch: 5| Step: 2
Training loss: 2.7580342292785645
Validation loss: 2.0178300821652977

Epoch: 5| Step: 3
Training loss: 2.730447292327881
Validation loss: 2.05065296798624

Epoch: 5| Step: 4
Training loss: 3.059277057647705
Validation loss: 2.023817453333127

Epoch: 5| Step: 5
Training loss: 2.3897385597229004
Validation loss: 2.023137100281254

Epoch: 5| Step: 6
Training loss: 1.4864091873168945
Validation loss: 2.021966463776045

Epoch: 5| Step: 7
Training loss: 2.4185023307800293
Validation loss: 2.061104695002238

Epoch: 5| Step: 8
Training loss: 2.102900743484497
Validation loss: 2.032802233131983

Epoch: 5| Step: 9
Training loss: 1.991804838180542
Validation loss: 2.048960666502676

Epoch: 5| Step: 10
Training loss: 1.9161018133163452
Validation loss: 2.051645863440729

Epoch: 67| Step: 0
Training loss: 2.15348744392395
Validation loss: 2.033358709786528

Epoch: 5| Step: 1
Training loss: 2.4368414878845215
Validation loss: 2.031115581912379

Epoch: 5| Step: 2
Training loss: 2.435342788696289
Validation loss: 2.0283976062651603

Epoch: 5| Step: 3
Training loss: 2.266120433807373
Validation loss: 2.0185420423425655

Epoch: 5| Step: 4
Training loss: 1.839489221572876
Validation loss: 2.0162909671824467

Epoch: 5| Step: 5
Training loss: 2.9512181282043457
Validation loss: 2.0366651473506803

Epoch: 5| Step: 6
Training loss: 2.4893579483032227
Validation loss: 2.0245555318811888

Epoch: 5| Step: 7
Training loss: 2.455599546432495
Validation loss: 2.025083818743306

Epoch: 5| Step: 8
Training loss: 2.0798511505126953
Validation loss: 2.0304109845110165

Epoch: 5| Step: 9
Training loss: 1.8854433298110962
Validation loss: 2.039576466365527

Epoch: 5| Step: 10
Training loss: 2.2362279891967773
Validation loss: 2.037155935841222

Epoch: 68| Step: 0
Training loss: 2.7970213890075684
Validation loss: 2.0498579753342496

Epoch: 5| Step: 1
Training loss: 1.8839880228042603
Validation loss: 2.0226010545607536

Epoch: 5| Step: 2
Training loss: 1.9671653509140015
Validation loss: 2.0283551011034238

Epoch: 5| Step: 3
Training loss: 2.6142067909240723
Validation loss: 2.0194527461964595

Epoch: 5| Step: 4
Training loss: 2.413316249847412
Validation loss: 2.054586100321944

Epoch: 5| Step: 5
Training loss: 2.0656495094299316
Validation loss: 2.0328826468477965

Epoch: 5| Step: 6
Training loss: 2.9991469383239746
Validation loss: 2.025720630922625

Epoch: 5| Step: 7
Training loss: 2.0757265090942383
Validation loss: 2.0080601399944675

Epoch: 5| Step: 8
Training loss: 1.9460890293121338
Validation loss: 2.0118174975918186

Epoch: 5| Step: 9
Training loss: 2.4295246601104736
Validation loss: 2.029213454133721

Epoch: 5| Step: 10
Training loss: 2.103675365447998
Validation loss: 2.018288735420473

Epoch: 69| Step: 0
Training loss: 3.0806403160095215
Validation loss: 2.032630670455194

Epoch: 5| Step: 1
Training loss: 2.2663159370422363
Validation loss: 2.009530646826631

Epoch: 5| Step: 2
Training loss: 2.485729217529297
Validation loss: 2.012821764074346

Epoch: 5| Step: 3
Training loss: 2.295228958129883
Validation loss: 2.021321496655864

Epoch: 5| Step: 4
Training loss: 2.4775326251983643
Validation loss: 2.01780108738971

Epoch: 5| Step: 5
Training loss: 2.0003981590270996
Validation loss: 2.028733407297442

Epoch: 5| Step: 6
Training loss: 1.8047195672988892
Validation loss: 2.00756097737179

Epoch: 5| Step: 7
Training loss: 2.6794896125793457
Validation loss: 2.013629606975022

Epoch: 5| Step: 8
Training loss: 1.8437297344207764
Validation loss: 1.988276154764237

Epoch: 5| Step: 9
Training loss: 2.1663525104522705
Validation loss: 2.0208367763027066

Epoch: 5| Step: 10
Training loss: 2.103043794631958
Validation loss: 2.0020483988587574

Epoch: 70| Step: 0
Training loss: 2.554300308227539
Validation loss: 2.0401944755226054

Epoch: 5| Step: 1
Training loss: 2.328881025314331
Validation loss: 2.0159003708952214

Epoch: 5| Step: 2
Training loss: 2.097546339035034
Validation loss: 1.9995535804379372

Epoch: 5| Step: 3
Training loss: 2.139138698577881
Validation loss: 2.0065683933996383

Epoch: 5| Step: 4
Training loss: 2.2179839611053467
Validation loss: 1.98111431829391

Epoch: 5| Step: 5
Training loss: 2.5875096321105957
Validation loss: 2.0130680773847844

Epoch: 5| Step: 6
Training loss: 2.0402815341949463
Validation loss: 2.0029333958061795

Epoch: 5| Step: 7
Training loss: 1.4555962085723877
Validation loss: 1.9922841415610364

Epoch: 5| Step: 8
Training loss: 2.4897003173828125
Validation loss: 2.0134038873898086

Epoch: 5| Step: 9
Training loss: 2.9374146461486816
Validation loss: 2.010976181235365

Epoch: 5| Step: 10
Training loss: 2.1630699634552
Validation loss: 2.003474394480387

Epoch: 71| Step: 0
Training loss: 2.5926589965820312
Validation loss: 1.998111085225177

Epoch: 5| Step: 1
Training loss: 2.7117671966552734
Validation loss: 2.0256529456825665

Epoch: 5| Step: 2
Training loss: 2.5580899715423584
Validation loss: 2.0051791808938466

Epoch: 5| Step: 3
Training loss: 2.3848323822021484
Validation loss: 1.995109986233455

Epoch: 5| Step: 4
Training loss: 2.234463691711426
Validation loss: 1.9947005125784105

Epoch: 5| Step: 5
Training loss: 2.6783287525177
Validation loss: 2.022795581048535

Epoch: 5| Step: 6
Training loss: 2.280007839202881
Validation loss: 2.0255002206371677

Epoch: 5| Step: 7
Training loss: 1.757629632949829
Validation loss: 2.0170508110395042

Epoch: 5| Step: 8
Training loss: 1.8944733142852783
Validation loss: 2.0125302050703313

Epoch: 5| Step: 9
Training loss: 1.9119031429290771
Validation loss: 2.008477346871489

Epoch: 5| Step: 10
Training loss: 2.195615768432617
Validation loss: 2.012202621788107

Epoch: 72| Step: 0
Training loss: 2.7270967960357666
Validation loss: 1.9985532709347305

Epoch: 5| Step: 1
Training loss: 2.6308460235595703
Validation loss: 2.019720967097949

Epoch: 5| Step: 2
Training loss: 2.3723349571228027
Validation loss: 2.015591747017317

Epoch: 5| Step: 3
Training loss: 2.2308928966522217
Validation loss: 2.015726345841603

Epoch: 5| Step: 4
Training loss: 2.084395170211792
Validation loss: 2.017026857663226

Epoch: 5| Step: 5
Training loss: 2.2181472778320312
Validation loss: 2.0041813952948457

Epoch: 5| Step: 6
Training loss: 2.6530778408050537
Validation loss: 2.0300333217907975

Epoch: 5| Step: 7
Training loss: 1.9127485752105713
Validation loss: 2.0222349884689494

Epoch: 5| Step: 8
Training loss: 2.6899046897888184
Validation loss: 2.037890790611185

Epoch: 5| Step: 9
Training loss: 2.1387219429016113
Validation loss: 2.044155281077149

Epoch: 5| Step: 10
Training loss: 1.399118423461914
Validation loss: 2.0499258131109257

Epoch: 73| Step: 0
Training loss: 1.9789886474609375
Validation loss: 2.001459421650056

Epoch: 5| Step: 1
Training loss: 2.3902220726013184
Validation loss: 2.0097280574101273

Epoch: 5| Step: 2
Training loss: 2.6002233028411865
Validation loss: 2.0043413895432667

Epoch: 5| Step: 3
Training loss: 2.6069462299346924
Validation loss: 2.0051738318576606

Epoch: 5| Step: 4
Training loss: 2.3519339561462402
Validation loss: 2.005717387763403

Epoch: 5| Step: 5
Training loss: 2.202322006225586
Validation loss: 2.027176319911916

Epoch: 5| Step: 6
Training loss: 1.8520965576171875
Validation loss: 2.044690970451601

Epoch: 5| Step: 7
Training loss: 2.1817359924316406
Validation loss: 2.036671905107396

Epoch: 5| Step: 8
Training loss: 2.811856746673584
Validation loss: 2.020575600285684

Epoch: 5| Step: 9
Training loss: 2.1310646533966064
Validation loss: 1.9992710903126707

Epoch: 5| Step: 10
Training loss: 2.118124485015869
Validation loss: 2.0282449183925504

Epoch: 74| Step: 0
Training loss: 1.9849601984024048
Validation loss: 2.011524695222096

Epoch: 5| Step: 1
Training loss: 2.1622328758239746
Validation loss: 2.0173871376181163

Epoch: 5| Step: 2
Training loss: 2.6248011589050293
Validation loss: 2.033997460078168

Epoch: 5| Step: 3
Training loss: 2.978466033935547
Validation loss: 1.9818795880963724

Epoch: 5| Step: 4
Training loss: 2.579497814178467
Validation loss: 2.020656729257235

Epoch: 5| Step: 5
Training loss: 1.5002069473266602
Validation loss: 2.018369261936475

Epoch: 5| Step: 6
Training loss: 2.405768632888794
Validation loss: 2.0172201741126274

Epoch: 5| Step: 7
Training loss: 2.8049087524414062
Validation loss: 2.012847178725786

Epoch: 5| Step: 8
Training loss: 1.7848026752471924
Validation loss: 2.009078679546233

Epoch: 5| Step: 9
Training loss: 2.162240505218506
Validation loss: 1.9942048083069503

Epoch: 5| Step: 10
Training loss: 2.3886098861694336
Validation loss: 2.0215799962320635

Epoch: 75| Step: 0
Training loss: 2.2993850708007812
Validation loss: 2.0051958791671263

Epoch: 5| Step: 1
Training loss: 2.414565324783325
Validation loss: 2.037210523441274

Epoch: 5| Step: 2
Training loss: 2.225935459136963
Validation loss: 2.0364147104242796

Epoch: 5| Step: 3
Training loss: 2.251945972442627
Validation loss: 2.0140717914027553

Epoch: 5| Step: 4
Training loss: 2.8455138206481934
Validation loss: 2.025709127867094

Epoch: 5| Step: 5
Training loss: 1.9380552768707275
Validation loss: 2.023570550385342

Epoch: 5| Step: 6
Training loss: 2.935250759124756
Validation loss: 2.010583625044874

Epoch: 5| Step: 7
Training loss: 2.119258165359497
Validation loss: 1.9944104045949957

Epoch: 5| Step: 8
Training loss: 1.6884479522705078
Validation loss: 2.01788584134912

Epoch: 5| Step: 9
Training loss: 1.9003089666366577
Validation loss: 2.0040358548523276

Epoch: 5| Step: 10
Training loss: 2.5602529048919678
Validation loss: 2.025704054422276

Epoch: 76| Step: 0
Training loss: 1.9321911334991455
Validation loss: 2.017286491650407

Epoch: 5| Step: 1
Training loss: 1.657743215560913
Validation loss: 1.9987066740630774

Epoch: 5| Step: 2
Training loss: 2.4286954402923584
Validation loss: 2.0089459214159238

Epoch: 5| Step: 3
Training loss: 2.8784992694854736
Validation loss: 2.010378199238931

Epoch: 5| Step: 4
Training loss: 1.923177719116211
Validation loss: 2.0336666542996644

Epoch: 5| Step: 5
Training loss: 2.6664111614227295
Validation loss: 2.0360384705246135

Epoch: 5| Step: 6
Training loss: 2.3203482627868652
Validation loss: 2.013920321259447

Epoch: 5| Step: 7
Training loss: 2.867426872253418
Validation loss: 2.036389099654331

Epoch: 5| Step: 8
Training loss: 1.8844947814941406
Validation loss: 2.0180183226062405

Epoch: 5| Step: 9
Training loss: 2.3528246879577637
Validation loss: 2.0409569996659473

Epoch: 5| Step: 10
Training loss: 2.1435487270355225
Validation loss: 2.017572364499492

Epoch: 77| Step: 0
Training loss: 2.2508747577667236
Validation loss: 2.018810752899416

Epoch: 5| Step: 1
Training loss: 2.309722423553467
Validation loss: 2.030540133035311

Epoch: 5| Step: 2
Training loss: 2.149193525314331
Validation loss: 2.0185596353264263

Epoch: 5| Step: 3
Training loss: 1.9048759937286377
Validation loss: 2.0018167623909573

Epoch: 5| Step: 4
Training loss: 2.7563838958740234
Validation loss: 2.007449298776606

Epoch: 5| Step: 5
Training loss: 2.080319881439209
Validation loss: 2.0255444037017

Epoch: 5| Step: 6
Training loss: 1.9212448596954346
Validation loss: 1.9842176975742463

Epoch: 5| Step: 7
Training loss: 3.200566530227661
Validation loss: 2.024136422782816

Epoch: 5| Step: 8
Training loss: 2.0641086101531982
Validation loss: 2.015164641923802

Epoch: 5| Step: 9
Training loss: 2.3423571586608887
Validation loss: 2.0211448284887497

Epoch: 5| Step: 10
Training loss: 2.078634262084961
Validation loss: 2.016203470127557

Epoch: 78| Step: 0
Training loss: 2.044839382171631
Validation loss: 1.9928795752986785

Epoch: 5| Step: 1
Training loss: 2.5880889892578125
Validation loss: 2.0022642022819928

Epoch: 5| Step: 2
Training loss: 2.277876615524292
Validation loss: 2.0158118201840307

Epoch: 5| Step: 3
Training loss: 2.217480421066284
Validation loss: 2.0139595436793503

Epoch: 5| Step: 4
Training loss: 2.0937392711639404
Validation loss: 2.0095885851049937

Epoch: 5| Step: 5
Training loss: 2.5448620319366455
Validation loss: 2.0260011406355005

Epoch: 5| Step: 6
Training loss: 2.114313840866089
Validation loss: 1.988816959883577

Epoch: 5| Step: 7
Training loss: 2.3106396198272705
Validation loss: 2.001056135341685

Epoch: 5| Step: 8
Training loss: 2.2773711681365967
Validation loss: 1.992869064372073

Epoch: 5| Step: 9
Training loss: 2.6381897926330566
Validation loss: 1.994969734581568

Epoch: 5| Step: 10
Training loss: 2.01483416557312
Validation loss: 2.0035500731519473

Epoch: 79| Step: 0
Training loss: 2.160287857055664
Validation loss: 1.9859992163155669

Epoch: 5| Step: 1
Training loss: 2.392610549926758
Validation loss: 2.0045201727139053

Epoch: 5| Step: 2
Training loss: 2.2521109580993652
Validation loss: 1.9952320950005644

Epoch: 5| Step: 3
Training loss: 2.84797739982605
Validation loss: 1.9886344684067594

Epoch: 5| Step: 4
Training loss: 2.2802581787109375
Validation loss: 2.002631505330404

Epoch: 5| Step: 5
Training loss: 1.9542758464813232
Validation loss: 1.9900242884953816

Epoch: 5| Step: 6
Training loss: 1.9371410608291626
Validation loss: 2.0187572022920013

Epoch: 5| Step: 7
Training loss: 2.2861504554748535
Validation loss: 2.0149659572109098

Epoch: 5| Step: 8
Training loss: 2.688129425048828
Validation loss: 2.01957510107307

Epoch: 5| Step: 9
Training loss: 2.6669347286224365
Validation loss: 2.0254989695805374

Epoch: 5| Step: 10
Training loss: 1.7570346593856812
Validation loss: 2.0128414412980438

Epoch: 80| Step: 0
Training loss: 2.44661545753479
Validation loss: 2.0383673457689184

Epoch: 5| Step: 1
Training loss: 2.4650959968566895
Validation loss: 2.0113227739129016

Epoch: 5| Step: 2
Training loss: 2.210785388946533
Validation loss: 2.0300585608328543

Epoch: 5| Step: 3
Training loss: 1.9414377212524414
Validation loss: 2.036126982781195

Epoch: 5| Step: 4
Training loss: 2.3578433990478516
Validation loss: 2.0397450513737176

Epoch: 5| Step: 5
Training loss: 1.8583288192749023
Validation loss: 2.028517787174512

Epoch: 5| Step: 6
Training loss: 2.4583325386047363
Validation loss: 2.041587555280296

Epoch: 5| Step: 7
Training loss: 2.7158424854278564
Validation loss: 2.0327094729228685

Epoch: 5| Step: 8
Training loss: 2.3770599365234375
Validation loss: 2.014442059301561

Epoch: 5| Step: 9
Training loss: 1.9657539129257202
Validation loss: 2.0123007169333835

Epoch: 5| Step: 10
Training loss: 2.479310989379883
Validation loss: 2.030027340817195

Epoch: 81| Step: 0
Training loss: 2.373197078704834
Validation loss: 2.0073196426514657

Epoch: 5| Step: 1
Training loss: 2.1435043811798096
Validation loss: 2.023431484417249

Epoch: 5| Step: 2
Training loss: 2.7774269580841064
Validation loss: 2.005164191287051

Epoch: 5| Step: 3
Training loss: 2.188683032989502
Validation loss: 2.0229505967068415

Epoch: 5| Step: 4
Training loss: 2.577516555786133
Validation loss: 2.0426514956258957

Epoch: 5| Step: 5
Training loss: 2.4211251735687256
Validation loss: 2.01802812340439

Epoch: 5| Step: 6
Training loss: 2.5525553226470947
Validation loss: 2.018257669223252

Epoch: 5| Step: 7
Training loss: 1.8766931295394897
Validation loss: 2.021068328170366

Epoch: 5| Step: 8
Training loss: 1.8569743633270264
Validation loss: 2.019087650442636

Epoch: 5| Step: 9
Training loss: 1.8735498189926147
Validation loss: 2.028263932915144

Epoch: 5| Step: 10
Training loss: 2.5110056400299072
Validation loss: 2.0265276688401417

Epoch: 82| Step: 0
Training loss: 2.509009838104248
Validation loss: 2.0292109392022573

Epoch: 5| Step: 1
Training loss: 1.9460633993148804
Validation loss: 2.0234549737745717

Epoch: 5| Step: 2
Training loss: 2.39215350151062
Validation loss: 2.020516631423786

Epoch: 5| Step: 3
Training loss: 1.8335769176483154
Validation loss: 2.04548051023996

Epoch: 5| Step: 4
Training loss: 2.970898389816284
Validation loss: 2.003621962762648

Epoch: 5| Step: 5
Training loss: 2.4134278297424316
Validation loss: 2.0125725987137004

Epoch: 5| Step: 6
Training loss: 2.202782154083252
Validation loss: 2.0217637464564335

Epoch: 5| Step: 7
Training loss: 2.6096179485321045
Validation loss: 2.0151196474670083

Epoch: 5| Step: 8
Training loss: 1.9619472026824951
Validation loss: 2.022985409664851

Epoch: 5| Step: 9
Training loss: 2.143906354904175
Validation loss: 2.0185192413227533

Epoch: 5| Step: 10
Training loss: 2.136481523513794
Validation loss: 2.0154081313840804

Epoch: 83| Step: 0
Training loss: 2.281038284301758
Validation loss: 1.9975901457571215

Epoch: 5| Step: 1
Training loss: 2.1070785522460938
Validation loss: 2.0006758115624868

Epoch: 5| Step: 2
Training loss: 2.6348071098327637
Validation loss: 2.046969567575762

Epoch: 5| Step: 3
Training loss: 2.222085475921631
Validation loss: 2.011261563147268

Epoch: 5| Step: 4
Training loss: 3.0893235206604004
Validation loss: 2.0066623905653596

Epoch: 5| Step: 5
Training loss: 2.0649704933166504
Validation loss: 2.0171328770217074

Epoch: 5| Step: 6
Training loss: 1.8639205694198608
Validation loss: 2.0104270904294905

Epoch: 5| Step: 7
Training loss: 1.649643898010254
Validation loss: 1.9914354085922241

Epoch: 5| Step: 8
Training loss: 2.4015636444091797
Validation loss: 1.9926904324562318

Epoch: 5| Step: 9
Training loss: 2.5434491634368896
Validation loss: 1.9908999396908669

Epoch: 5| Step: 10
Training loss: 2.4259235858917236
Validation loss: 2.004781466658397

Epoch: 84| Step: 0
Training loss: 2.3481173515319824
Validation loss: 2.0263702536142

Epoch: 5| Step: 1
Training loss: 1.7544314861297607
Validation loss: 1.99444511500738

Epoch: 5| Step: 2
Training loss: 2.686833143234253
Validation loss: 2.0144219193407285

Epoch: 5| Step: 3
Training loss: 2.7202236652374268
Validation loss: 2.022743325079641

Epoch: 5| Step: 4
Training loss: 2.083045482635498
Validation loss: 2.0106797449050413

Epoch: 5| Step: 5
Training loss: 2.8563485145568848
Validation loss: 2.015506411111483

Epoch: 5| Step: 6
Training loss: 2.254432201385498
Validation loss: 2.0150585815470707

Epoch: 5| Step: 7
Training loss: 2.5900986194610596
Validation loss: 2.0165833632151284

Epoch: 5| Step: 8
Training loss: 2.051039695739746
Validation loss: 2.013615987634146

Epoch: 5| Step: 9
Training loss: 1.845808982849121
Validation loss: 2.0206340243739467

Epoch: 5| Step: 10
Training loss: 1.7952882051467896
Validation loss: 2.018398064439015

Epoch: 85| Step: 0
Training loss: 1.62839674949646
Validation loss: 2.002272243140846

Epoch: 5| Step: 1
Training loss: 2.3145458698272705
Validation loss: 2.0113428997737106

Epoch: 5| Step: 2
Training loss: 2.275373935699463
Validation loss: 2.0176774135199924

Epoch: 5| Step: 3
Training loss: 2.2465577125549316
Validation loss: 2.0244824373593895

Epoch: 5| Step: 4
Training loss: 2.4330785274505615
Validation loss: 2.0148978566610687

Epoch: 5| Step: 5
Training loss: 2.267488956451416
Validation loss: 2.032466785882109

Epoch: 5| Step: 6
Training loss: 2.032658100128174
Validation loss: 2.0279676298941336

Epoch: 5| Step: 7
Training loss: 2.7494118213653564
Validation loss: 1.9967846101330173

Epoch: 5| Step: 8
Training loss: 2.25901198387146
Validation loss: 2.0153105451214697

Epoch: 5| Step: 9
Training loss: 2.525113344192505
Validation loss: 2.013716161891978

Epoch: 5| Step: 10
Training loss: 2.57409930229187
Validation loss: 2.019205102356531

Epoch: 86| Step: 0
Training loss: 2.1449882984161377
Validation loss: 2.014579830631133

Epoch: 5| Step: 1
Training loss: 2.5544393062591553
Validation loss: 2.0275544069146596

Epoch: 5| Step: 2
Training loss: 2.0348434448242188
Validation loss: 2.0226729351987123

Epoch: 5| Step: 3
Training loss: 2.0680317878723145
Validation loss: 2.014930922497985

Epoch: 5| Step: 4
Training loss: 2.6018073558807373
Validation loss: 2.00581532396296

Epoch: 5| Step: 5
Training loss: 1.7046616077423096
Validation loss: 2.005287625456369

Epoch: 5| Step: 6
Training loss: 2.3431122303009033
Validation loss: 2.0153196960367183

Epoch: 5| Step: 7
Training loss: 2.5771420001983643
Validation loss: 2.0427607079987884

Epoch: 5| Step: 8
Training loss: 1.984657645225525
Validation loss: 1.9998707643119238

Epoch: 5| Step: 9
Training loss: 2.9475414752960205
Validation loss: 2.0518995151724866

Epoch: 5| Step: 10
Training loss: 1.8596973419189453
Validation loss: 2.0129335131696475

Epoch: 87| Step: 0
Training loss: 2.351166248321533
Validation loss: 2.013719633061399

Epoch: 5| Step: 1
Training loss: 2.211397647857666
Validation loss: 2.0091071282663653

Epoch: 5| Step: 2
Training loss: 2.951019763946533
Validation loss: 2.0176463883410216

Epoch: 5| Step: 3
Training loss: 1.5876753330230713
Validation loss: 2.0252207248441634

Epoch: 5| Step: 4
Training loss: 2.0029654502868652
Validation loss: 2.0499991114421556

Epoch: 5| Step: 5
Training loss: 2.707001209259033
Validation loss: 2.0297740082586966

Epoch: 5| Step: 6
Training loss: 2.508183479309082
Validation loss: 2.028853298515402

Epoch: 5| Step: 7
Training loss: 1.8646824359893799
Validation loss: 2.038942680563978

Epoch: 5| Step: 8
Training loss: 2.64199161529541
Validation loss: 2.0157185549377115

Epoch: 5| Step: 9
Training loss: 1.7897007465362549
Validation loss: 2.033407585595244

Epoch: 5| Step: 10
Training loss: 2.489121437072754
Validation loss: 2.031333783621429

Epoch: 88| Step: 0
Training loss: 2.5033042430877686
Validation loss: 2.0239037903406287

Epoch: 5| Step: 1
Training loss: 2.0606398582458496
Validation loss: 2.0207001791205457

Epoch: 5| Step: 2
Training loss: 2.1798787117004395
Validation loss: 2.041539789527975

Epoch: 5| Step: 3
Training loss: 2.3388373851776123
Validation loss: 2.0130768706721645

Epoch: 5| Step: 4
Training loss: 2.0384480953216553
Validation loss: 2.028056057550574

Epoch: 5| Step: 5
Training loss: 2.9912097454071045
Validation loss: 2.005312624798026

Epoch: 5| Step: 6
Training loss: 2.016411304473877
Validation loss: 2.006341495821553

Epoch: 5| Step: 7
Training loss: 1.9954652786254883
Validation loss: 2.0107719488041376

Epoch: 5| Step: 8
Training loss: 2.1001243591308594
Validation loss: 2.0063211712785947

Epoch: 5| Step: 9
Training loss: 2.5306639671325684
Validation loss: 2.018949031829834

Epoch: 5| Step: 10
Training loss: 2.2869298458099365
Validation loss: 2.003167256232231

Epoch: 89| Step: 0
Training loss: 1.8768974542617798
Validation loss: 2.006392204633323

Epoch: 5| Step: 1
Training loss: 1.9392976760864258
Validation loss: 2.015642435319962

Epoch: 5| Step: 2
Training loss: 1.7092349529266357
Validation loss: 2.0166241122830297

Epoch: 5| Step: 3
Training loss: 2.5315067768096924
Validation loss: 1.9984488410334433

Epoch: 5| Step: 4
Training loss: 2.4012250900268555
Validation loss: 2.014072233630765

Epoch: 5| Step: 5
Training loss: 2.2777388095855713
Validation loss: 2.008142744341204

Epoch: 5| Step: 6
Training loss: 3.1824426651000977
Validation loss: 1.9950313324569373

Epoch: 5| Step: 7
Training loss: 2.0531163215637207
Validation loss: 2.0130144473045104

Epoch: 5| Step: 8
Training loss: 2.29582142829895
Validation loss: 2.0027146570144163

Epoch: 5| Step: 9
Training loss: 2.141918659210205
Validation loss: 1.9984224150257726

Epoch: 5| Step: 10
Training loss: 2.601228713989258
Validation loss: 2.011110846714307

Epoch: 90| Step: 0
Training loss: 2.03053617477417
Validation loss: 1.981220934980659

Epoch: 5| Step: 1
Training loss: 2.250272512435913
Validation loss: 1.9752522437803206

Epoch: 5| Step: 2
Training loss: 2.01166033744812
Validation loss: 2.0316594659641223

Epoch: 5| Step: 3
Training loss: 2.1335856914520264
Validation loss: 1.9986491126398886

Epoch: 5| Step: 4
Training loss: 2.2224831581115723
Validation loss: 1.9976787003137733

Epoch: 5| Step: 5
Training loss: 2.47027325630188
Validation loss: 2.022596666889806

Epoch: 5| Step: 6
Training loss: 2.8299331665039062
Validation loss: 2.0468822486938967

Epoch: 5| Step: 7
Training loss: 1.9366333484649658
Validation loss: 2.0032960343104538

Epoch: 5| Step: 8
Training loss: 2.282482862472534
Validation loss: 2.0099794941563762

Epoch: 5| Step: 9
Training loss: 2.1219537258148193
Validation loss: 2.0174425519922727

Epoch: 5| Step: 10
Training loss: 2.7912113666534424
Validation loss: 2.040233132659748

Epoch: 91| Step: 0
Training loss: 2.169363498687744
Validation loss: 2.030599473625101

Epoch: 5| Step: 1
Training loss: 2.1438586711883545
Validation loss: 2.0400445025454284

Epoch: 5| Step: 2
Training loss: 2.4676690101623535
Validation loss: 2.0209349240026167

Epoch: 5| Step: 3
Training loss: 1.9795887470245361
Validation loss: 2.0323019155891995

Epoch: 5| Step: 4
Training loss: 2.011103868484497
Validation loss: 2.0121739038857083

Epoch: 5| Step: 5
Training loss: 2.1867403984069824
Validation loss: 2.017244969644854

Epoch: 5| Step: 6
Training loss: 2.4810526371002197
Validation loss: 2.013091592378514

Epoch: 5| Step: 7
Training loss: 2.0784125328063965
Validation loss: 2.0332778974245955

Epoch: 5| Step: 8
Training loss: 2.706923246383667
Validation loss: 2.0097877581914267

Epoch: 5| Step: 9
Training loss: 2.6366970539093018
Validation loss: 2.024107366479853

Epoch: 5| Step: 10
Training loss: 1.9508882761001587
Validation loss: 2.012364290093863

Epoch: 92| Step: 0
Training loss: 2.541374444961548
Validation loss: 2.026601706781695

Epoch: 5| Step: 1
Training loss: 1.7336419820785522
Validation loss: 2.034835130937638

Epoch: 5| Step: 2
Training loss: 2.3481757640838623
Validation loss: 2.0132780946711057

Epoch: 5| Step: 3
Training loss: 2.5754005908966064
Validation loss: 2.0229106756948654

Epoch: 5| Step: 4
Training loss: 1.948657751083374
Validation loss: 2.027427086266138

Epoch: 5| Step: 5
Training loss: 2.142615556716919
Validation loss: 2.0308015487527333

Epoch: 5| Step: 6
Training loss: 2.3487651348114014
Validation loss: 2.033614331676114

Epoch: 5| Step: 7
Training loss: 2.051825761795044
Validation loss: 2.057394112310102

Epoch: 5| Step: 8
Training loss: 2.5546464920043945
Validation loss: 2.0389845473791963

Epoch: 5| Step: 9
Training loss: 2.0060532093048096
Validation loss: 2.0331789421778854

Epoch: 5| Step: 10
Training loss: 2.8564772605895996
Validation loss: 2.030164610955023

Epoch: 93| Step: 0
Training loss: 1.567988395690918
Validation loss: 2.0432422648194017

Epoch: 5| Step: 1
Training loss: 2.6801159381866455
Validation loss: 2.053407685731047

Epoch: 5| Step: 2
Training loss: 2.196622133255005
Validation loss: 2.027414127062726

Epoch: 5| Step: 3
Training loss: 1.734971284866333
Validation loss: 2.0423003870953798

Epoch: 5| Step: 4
Training loss: 2.0489916801452637
Validation loss: 2.0461015547475507

Epoch: 5| Step: 5
Training loss: 2.705950975418091
Validation loss: 2.048804072923558

Epoch: 5| Step: 6
Training loss: 2.3699564933776855
Validation loss: 2.037467561742311

Epoch: 5| Step: 7
Training loss: 1.9635288715362549
Validation loss: 2.028672905378444

Epoch: 5| Step: 8
Training loss: 2.955167293548584
Validation loss: 2.049142861879

Epoch: 5| Step: 9
Training loss: 2.2106151580810547
Validation loss: 2.0555377557713497

Epoch: 5| Step: 10
Training loss: 2.567476511001587
Validation loss: 2.052050188023557

Epoch: 94| Step: 0
Training loss: 1.9649791717529297
Validation loss: 2.0520778112514044

Epoch: 5| Step: 1
Training loss: 1.9340614080429077
Validation loss: 2.0289429169829174

Epoch: 5| Step: 2
Training loss: 2.5488791465759277
Validation loss: 2.058111918869839

Epoch: 5| Step: 3
Training loss: 2.196566343307495
Validation loss: 2.024452047963296

Epoch: 5| Step: 4
Training loss: 2.29914927482605
Validation loss: 2.0467094529059624

Epoch: 5| Step: 5
Training loss: 2.166456460952759
Validation loss: 2.023690403148692

Epoch: 5| Step: 6
Training loss: 2.499330520629883
Validation loss: 2.0227657761625064

Epoch: 5| Step: 7
Training loss: 2.27475643157959
Validation loss: 2.032198111216227

Epoch: 5| Step: 8
Training loss: 1.9115161895751953
Validation loss: 2.0159610009962514

Epoch: 5| Step: 9
Training loss: 2.313950777053833
Validation loss: 2.053605998716047

Epoch: 5| Step: 10
Training loss: 2.7922983169555664
Validation loss: 2.02441692608659

Epoch: 95| Step: 0
Training loss: 1.7018744945526123
Validation loss: 2.0140673729681198

Epoch: 5| Step: 1
Training loss: 2.6148500442504883
Validation loss: 2.025328532341988

Epoch: 5| Step: 2
Training loss: 2.514308452606201
Validation loss: 2.043819458253922

Epoch: 5| Step: 3
Training loss: 2.6381001472473145
Validation loss: 2.0623541621751684

Epoch: 5| Step: 4
Training loss: 2.0398917198181152
Validation loss: 2.0259221856312086

Epoch: 5| Step: 5
Training loss: 2.240400791168213
Validation loss: 2.040954807753204

Epoch: 5| Step: 6
Training loss: 1.9129657745361328
Validation loss: 2.0347195902178363

Epoch: 5| Step: 7
Training loss: 2.19671368598938
Validation loss: 2.059849351964971

Epoch: 5| Step: 8
Training loss: 2.327235698699951
Validation loss: 2.0469658246604343

Epoch: 5| Step: 9
Training loss: 2.1369471549987793
Validation loss: 2.035843010871641

Epoch: 5| Step: 10
Training loss: 2.4537429809570312
Validation loss: 2.0643950918669343

Epoch: 96| Step: 0
Training loss: 2.994213104248047
Validation loss: 2.050332787216351

Epoch: 5| Step: 1
Training loss: 2.2413229942321777
Validation loss: 2.070731819316905

Epoch: 5| Step: 2
Training loss: 2.4499943256378174
Validation loss: 2.0740080495034494

Epoch: 5| Step: 3
Training loss: 2.2639050483703613
Validation loss: 2.049279192442535

Epoch: 5| Step: 4
Training loss: 1.7995212078094482
Validation loss: 2.0477474889447613

Epoch: 5| Step: 5
Training loss: 2.3356785774230957
Validation loss: 2.045048736756848

Epoch: 5| Step: 6
Training loss: 2.2113070487976074
Validation loss: 2.059093008759201

Epoch: 5| Step: 7
Training loss: 2.4074747562408447
Validation loss: 2.0481511931265555

Epoch: 5| Step: 8
Training loss: 2.1709811687469482
Validation loss: 2.0604530483163814

Epoch: 5| Step: 9
Training loss: 2.2585768699645996
Validation loss: 2.0348544697607718

Epoch: 5| Step: 10
Training loss: 1.9035121202468872
Validation loss: 2.0122742293983378

Epoch: 97| Step: 0
Training loss: 2.3322854042053223
Validation loss: 2.0450447092774096

Epoch: 5| Step: 1
Training loss: 2.106843948364258
Validation loss: 2.0313143089253414

Epoch: 5| Step: 2
Training loss: 2.285273551940918
Validation loss: 2.0467931224453833

Epoch: 5| Step: 3
Training loss: 2.038320541381836
Validation loss: 2.035445172299621

Epoch: 5| Step: 4
Training loss: 2.7220137119293213
Validation loss: 2.028138736242889

Epoch: 5| Step: 5
Training loss: 2.826639175415039
Validation loss: 2.0296193835555867

Epoch: 5| Step: 6
Training loss: 2.2020230293273926
Validation loss: 2.0321440389079433

Epoch: 5| Step: 7
Training loss: 2.3584625720977783
Validation loss: 2.0434002132825952

Epoch: 5| Step: 8
Training loss: 1.9827617406845093
Validation loss: 2.048952405170728

Epoch: 5| Step: 9
Training loss: 2.2056775093078613
Validation loss: 2.0489602576019945

Epoch: 5| Step: 10
Training loss: 1.843762993812561
Validation loss: 2.036048634077913

Epoch: 98| Step: 0
Training loss: 2.2262368202209473
Validation loss: 2.042445541709982

Epoch: 5| Step: 1
Training loss: 2.7058539390563965
Validation loss: 2.032471702944848

Epoch: 5| Step: 2
Training loss: 2.296044111251831
Validation loss: 2.005132687989102

Epoch: 5| Step: 3
Training loss: 1.4926588535308838
Validation loss: 2.036614585948247

Epoch: 5| Step: 4
Training loss: 2.2283968925476074
Validation loss: 2.039118892403059

Epoch: 5| Step: 5
Training loss: 2.169570207595825
Validation loss: 2.035107663882676

Epoch: 5| Step: 6
Training loss: 1.8895317316055298
Validation loss: 2.045552738251225

Epoch: 5| Step: 7
Training loss: 2.9612033367156982
Validation loss: 2.032725752040904

Epoch: 5| Step: 8
Training loss: 2.1455864906311035
Validation loss: 2.0208012724435456

Epoch: 5| Step: 9
Training loss: 2.7684943675994873
Validation loss: 2.038256182465502

Epoch: 5| Step: 10
Training loss: 2.088911533355713
Validation loss: 2.016649159052039

Epoch: 99| Step: 0
Training loss: 2.2030417919158936
Validation loss: 2.0225740581430416

Epoch: 5| Step: 1
Training loss: 2.1568331718444824
Validation loss: 2.0319557959033596

Epoch: 5| Step: 2
Training loss: 1.9195009469985962
Validation loss: 2.0399818189682497

Epoch: 5| Step: 3
Training loss: 2.03562593460083
Validation loss: 2.0131207858362505

Epoch: 5| Step: 4
Training loss: 2.2765135765075684
Validation loss: 2.0381602459056403

Epoch: 5| Step: 5
Training loss: 2.4146854877471924
Validation loss: 2.0299607630698913

Epoch: 5| Step: 6
Training loss: 2.4347267150878906
Validation loss: 2.0362624109432264

Epoch: 5| Step: 7
Training loss: 2.4854087829589844
Validation loss: 2.0488528205502416

Epoch: 5| Step: 8
Training loss: 2.5455679893493652
Validation loss: 2.027413962989725

Epoch: 5| Step: 9
Training loss: 2.3628904819488525
Validation loss: 2.0270251074144916

Epoch: 5| Step: 10
Training loss: 1.988646149635315
Validation loss: 2.0421452163368143

Epoch: 100| Step: 0
Training loss: 2.256399631500244
Validation loss: 2.0598879552656606

Epoch: 5| Step: 1
Training loss: 2.1256823539733887
Validation loss: 2.012024765373558

Epoch: 5| Step: 2
Training loss: 2.2006382942199707
Validation loss: 2.002799880120062

Epoch: 5| Step: 3
Training loss: 2.2792654037475586
Validation loss: 2.0125698274181736

Epoch: 5| Step: 4
Training loss: 2.2978968620300293
Validation loss: 2.0057311878409436

Epoch: 5| Step: 5
Training loss: 2.7430779933929443
Validation loss: 2.0250554341141895

Epoch: 5| Step: 6
Training loss: 2.3629977703094482
Validation loss: 2.0076855664612143

Epoch: 5| Step: 7
Training loss: 1.9625270366668701
Validation loss: 2.035465343024141

Epoch: 5| Step: 8
Training loss: 2.0576350688934326
Validation loss: 2.0268881218407744

Epoch: 5| Step: 9
Training loss: 2.0353891849517822
Validation loss: 1.9922061325401388

Epoch: 5| Step: 10
Training loss: 2.4424381256103516
Validation loss: 2.035049420531078

Epoch: 101| Step: 0
Training loss: 2.6193881034851074
Validation loss: 2.0388044439336306

Epoch: 5| Step: 1
Training loss: 2.257026195526123
Validation loss: 2.0121649901072183

Epoch: 5| Step: 2
Training loss: 1.9323616027832031
Validation loss: 2.012834741223243

Epoch: 5| Step: 3
Training loss: 2.123622179031372
Validation loss: 2.040769107880131

Epoch: 5| Step: 4
Training loss: 1.6289809942245483
Validation loss: 2.052356316197303

Epoch: 5| Step: 5
Training loss: 1.779018759727478
Validation loss: 2.0264801235609156

Epoch: 5| Step: 6
Training loss: 2.4075000286102295
Validation loss: 2.012444265427128

Epoch: 5| Step: 7
Training loss: 2.374044418334961
Validation loss: 2.0422900645963606

Epoch: 5| Step: 8
Training loss: 2.7430689334869385
Validation loss: 2.018414012847408

Epoch: 5| Step: 9
Training loss: 2.1051459312438965
Validation loss: 2.0256786987345707

Epoch: 5| Step: 10
Training loss: 2.8926193714141846
Validation loss: 2.020835212481919

Epoch: 102| Step: 0
Training loss: 1.6691551208496094
Validation loss: 2.0180360886358444

Epoch: 5| Step: 1
Training loss: 2.479907274246216
Validation loss: 2.013453427181449

Epoch: 5| Step: 2
Training loss: 1.9418582916259766
Validation loss: 1.998494243109098

Epoch: 5| Step: 3
Training loss: 2.454061985015869
Validation loss: 2.012357037554505

Epoch: 5| Step: 4
Training loss: 2.742741107940674
Validation loss: 2.0198658871394333

Epoch: 5| Step: 5
Training loss: 2.2432329654693604
Validation loss: 2.0312135193937566

Epoch: 5| Step: 6
Training loss: 3.0493736267089844
Validation loss: 2.0161120686479794

Epoch: 5| Step: 7
Training loss: 1.8168847560882568
Validation loss: 2.014884493684256

Epoch: 5| Step: 8
Training loss: 1.6945642232894897
Validation loss: 2.0270059288188977

Epoch: 5| Step: 9
Training loss: 2.1786766052246094
Validation loss: 2.0281416664841356

Epoch: 5| Step: 10
Training loss: 2.6341636180877686
Validation loss: 1.9987020979645431

Epoch: 103| Step: 0
Training loss: 1.8068794012069702
Validation loss: 2.013130123897265

Epoch: 5| Step: 1
Training loss: 2.348644256591797
Validation loss: 2.02509170193826

Epoch: 5| Step: 2
Training loss: 2.2119884490966797
Validation loss: 2.0094063538377003

Epoch: 5| Step: 3
Training loss: 2.5011584758758545
Validation loss: 2.0494970121691303

Epoch: 5| Step: 4
Training loss: 2.0427706241607666
Validation loss: 2.029070861877934

Epoch: 5| Step: 5
Training loss: 2.897704601287842
Validation loss: 2.033926558750932

Epoch: 5| Step: 6
Training loss: 2.6659951210021973
Validation loss: 2.037320078060191

Epoch: 5| Step: 7
Training loss: 2.0486724376678467
Validation loss: 2.0271153321830173

Epoch: 5| Step: 8
Training loss: 2.25254487991333
Validation loss: 2.0244747002919516

Epoch: 5| Step: 9
Training loss: 2.241586685180664
Validation loss: 2.0451952770192134

Epoch: 5| Step: 10
Training loss: 1.8697099685668945
Validation loss: 2.0502854483101958

Epoch: 104| Step: 0
Training loss: 2.6662561893463135
Validation loss: 2.0299126307169595

Epoch: 5| Step: 1
Training loss: 2.2720704078674316
Validation loss: 2.0211150120663386

Epoch: 5| Step: 2
Training loss: 2.4436492919921875
Validation loss: 2.0352675273854244

Epoch: 5| Step: 3
Training loss: 2.090968608856201
Validation loss: 2.0432198175819973

Epoch: 5| Step: 4
Training loss: 2.276073932647705
Validation loss: 2.0310261134178407

Epoch: 5| Step: 5
Training loss: 1.8878875970840454
Validation loss: 2.0467672514659103

Epoch: 5| Step: 6
Training loss: 1.8125207424163818
Validation loss: 2.0560253256110737

Epoch: 5| Step: 7
Training loss: 2.9829354286193848
Validation loss: 2.0505421661561534

Epoch: 5| Step: 8
Training loss: 1.9190938472747803
Validation loss: 2.0465468193895076

Epoch: 5| Step: 9
Training loss: 2.0935652256011963
Validation loss: 2.0428411101782196

Epoch: 5| Step: 10
Training loss: 2.4568982124328613
Validation loss: 2.03055114002638

Epoch: 105| Step: 0
Training loss: 2.298229932785034
Validation loss: 2.038586370406612

Epoch: 5| Step: 1
Training loss: 2.475916862487793
Validation loss: 2.0394462603394703

Epoch: 5| Step: 2
Training loss: 2.070011854171753
Validation loss: 2.0410478589355305

Epoch: 5| Step: 3
Training loss: 2.8432090282440186
Validation loss: 2.034564853996359

Epoch: 5| Step: 4
Training loss: 2.4081320762634277
Validation loss: 2.0511152385383524

Epoch: 5| Step: 5
Training loss: 2.4658257961273193
Validation loss: 2.037559317004296

Epoch: 5| Step: 6
Training loss: 1.970001220703125
Validation loss: 2.0132471643468386

Epoch: 5| Step: 7
Training loss: 2.3400566577911377
Validation loss: 2.0469640429301927

Epoch: 5| Step: 8
Training loss: 1.8893489837646484
Validation loss: 2.0444316441012966

Epoch: 5| Step: 9
Training loss: 2.242913246154785
Validation loss: 2.0467351482760523

Epoch: 5| Step: 10
Training loss: 1.8725659847259521
Validation loss: 2.0502835589070476

Epoch: 106| Step: 0
Training loss: 1.7999188899993896
Validation loss: 2.0324896163837884

Epoch: 5| Step: 1
Training loss: 2.3934826850891113
Validation loss: 2.0159793438449984

Epoch: 5| Step: 2
Training loss: 2.2426934242248535
Validation loss: 2.0256064245777745

Epoch: 5| Step: 3
Training loss: 2.610408067703247
Validation loss: 2.0396781582986154

Epoch: 5| Step: 4
Training loss: 2.422268867492676
Validation loss: 2.0137638174077517

Epoch: 5| Step: 5
Training loss: 2.9321656227111816
Validation loss: 2.0411108386132026

Epoch: 5| Step: 6
Training loss: 1.686449408531189
Validation loss: 2.042867160612537

Epoch: 5| Step: 7
Training loss: 2.1418113708496094
Validation loss: 2.040047077722447

Epoch: 5| Step: 8
Training loss: 1.7791411876678467
Validation loss: 2.0162421170101372

Epoch: 5| Step: 9
Training loss: 2.7268052101135254
Validation loss: 2.027034731321437

Epoch: 5| Step: 10
Training loss: 2.0014452934265137
Validation loss: 2.0216291386594056

Epoch: 107| Step: 0
Training loss: 2.164881706237793
Validation loss: 2.0097294699761177

Epoch: 5| Step: 1
Training loss: 2.23728346824646
Validation loss: 2.02554234638009

Epoch: 5| Step: 2
Training loss: 1.6787731647491455
Validation loss: 2.0229106539039203

Epoch: 5| Step: 3
Training loss: 2.3883063793182373
Validation loss: 2.003072818120321

Epoch: 5| Step: 4
Training loss: 1.5154590606689453
Validation loss: 2.033653146477156

Epoch: 5| Step: 5
Training loss: 2.3736045360565186
Validation loss: 2.004907738777899

Epoch: 5| Step: 6
Training loss: 2.7282350063323975
Validation loss: 2.011407393281178

Epoch: 5| Step: 7
Training loss: 2.406275510787964
Validation loss: 2.036729153766427

Epoch: 5| Step: 8
Training loss: 2.385620355606079
Validation loss: 2.0099415522749706

Epoch: 5| Step: 9
Training loss: 2.949690341949463
Validation loss: 2.022750036690825

Epoch: 5| Step: 10
Training loss: 2.1643662452697754
Validation loss: 2.007541271948045

Epoch: 108| Step: 0
Training loss: 1.7416841983795166
Validation loss: 2.031764781603249

Epoch: 5| Step: 1
Training loss: 1.48672616481781
Validation loss: 2.0195211184922086

Epoch: 5| Step: 2
Training loss: 2.292036294937134
Validation loss: 1.9998635938090663

Epoch: 5| Step: 3
Training loss: 2.209139108657837
Validation loss: 1.9928902861892537

Epoch: 5| Step: 4
Training loss: 2.3669347763061523
Validation loss: 2.0120045267125612

Epoch: 5| Step: 5
Training loss: 2.1973588466644287
Validation loss: 2.004823823128977

Epoch: 5| Step: 6
Training loss: 1.792027235031128
Validation loss: 2.0138503966792936

Epoch: 5| Step: 7
Training loss: 2.5155789852142334
Validation loss: 1.9707156048026135

Epoch: 5| Step: 8
Training loss: 3.093980073928833
Validation loss: 2.0240978989549863

Epoch: 5| Step: 9
Training loss: 2.3584015369415283
Validation loss: 1.9898956001445811

Epoch: 5| Step: 10
Training loss: 2.6863038539886475
Validation loss: 1.9870846604788175

Epoch: 109| Step: 0
Training loss: 2.0482258796691895
Validation loss: 2.0104134416067474

Epoch: 5| Step: 1
Training loss: 1.9233827590942383
Validation loss: 2.01955908601002

Epoch: 5| Step: 2
Training loss: 2.3418567180633545
Validation loss: 2.0064294517681165

Epoch: 5| Step: 3
Training loss: 2.3163275718688965
Validation loss: 1.9834097188006166

Epoch: 5| Step: 4
Training loss: 2.259110689163208
Validation loss: 2.028371983958829

Epoch: 5| Step: 5
Training loss: 2.2562899589538574
Validation loss: 2.0414359415731123

Epoch: 5| Step: 6
Training loss: 1.861469030380249
Validation loss: 2.0469338906708585

Epoch: 5| Step: 7
Training loss: 3.0485434532165527
Validation loss: 2.038594412547286

Epoch: 5| Step: 8
Training loss: 2.278085947036743
Validation loss: 2.0330747160860287

Epoch: 5| Step: 9
Training loss: 2.2703213691711426
Validation loss: 2.015746808821155

Epoch: 5| Step: 10
Training loss: 2.1692612171173096
Validation loss: 2.0510366065527803

Epoch: 110| Step: 0
Training loss: 2.3910164833068848
Validation loss: 2.0400187533388854

Epoch: 5| Step: 1
Training loss: 2.63704252243042
Validation loss: 2.0395154440274803

Epoch: 5| Step: 2
Training loss: 2.367281436920166
Validation loss: 2.047260449778649

Epoch: 5| Step: 3
Training loss: 2.4277310371398926
Validation loss: 2.0275371536131828

Epoch: 5| Step: 4
Training loss: 1.7383426427841187
Validation loss: 2.0162603214222896

Epoch: 5| Step: 5
Training loss: 2.912749767303467
Validation loss: 2.0260921062961703

Epoch: 5| Step: 6
Training loss: 2.2433714866638184
Validation loss: 2.01670342106973

Epoch: 5| Step: 7
Training loss: 1.9839102029800415
Validation loss: 1.9993641914859894

Epoch: 5| Step: 8
Training loss: 2.1344761848449707
Validation loss: 2.0237154306903964

Epoch: 5| Step: 9
Training loss: 2.224090337753296
Validation loss: 2.0402131824083227

Epoch: 5| Step: 10
Training loss: 1.6549711227416992
Validation loss: 2.038232861667551

Epoch: 111| Step: 0
Training loss: 2.2316057682037354
Validation loss: 2.005834333358272

Epoch: 5| Step: 1
Training loss: 2.3622984886169434
Validation loss: 2.0209554203094973

Epoch: 5| Step: 2
Training loss: 2.6524975299835205
Validation loss: 2.038091169890537

Epoch: 5| Step: 3
Training loss: 1.9792940616607666
Validation loss: 1.99695586645475

Epoch: 5| Step: 4
Training loss: 1.9496209621429443
Validation loss: 2.030689216429187

Epoch: 5| Step: 5
Training loss: 2.424382448196411
Validation loss: 2.001378966915992

Epoch: 5| Step: 6
Training loss: 2.682762384414673
Validation loss: 2.03168140175522

Epoch: 5| Step: 7
Training loss: 2.021315574645996
Validation loss: 2.0004466284987745

Epoch: 5| Step: 8
Training loss: 1.9445550441741943
Validation loss: 2.029445066246935

Epoch: 5| Step: 9
Training loss: 2.429551124572754
Validation loss: 2.0267584272610244

Epoch: 5| Step: 10
Training loss: 1.9547278881072998
Validation loss: 2.001261092001392

Epoch: 112| Step: 0
Training loss: 2.5005335807800293
Validation loss: 2.016704326034874

Epoch: 5| Step: 1
Training loss: 2.5159811973571777
Validation loss: 2.012033966279799

Epoch: 5| Step: 2
Training loss: 2.098339080810547
Validation loss: 2.0111418436932307

Epoch: 5| Step: 3
Training loss: 2.163691997528076
Validation loss: 2.0235351631718297

Epoch: 5| Step: 4
Training loss: 2.0101847648620605
Validation loss: 2.013004938761393

Epoch: 5| Step: 5
Training loss: 2.8107430934906006
Validation loss: 2.001094046459403

Epoch: 5| Step: 6
Training loss: 2.25075364112854
Validation loss: 2.0273275785548712

Epoch: 5| Step: 7
Training loss: 2.206308126449585
Validation loss: 2.0139946706833376

Epoch: 5| Step: 8
Training loss: 2.1319687366485596
Validation loss: 2.0275798997571393

Epoch: 5| Step: 9
Training loss: 2.479060649871826
Validation loss: 2.0228563944498696

Epoch: 5| Step: 10
Training loss: 1.54108464717865
Validation loss: 2.0286930555938394

Epoch: 113| Step: 0
Training loss: 2.0348854064941406
Validation loss: 2.02698790642523

Epoch: 5| Step: 1
Training loss: 2.4002346992492676
Validation loss: 2.017780580828267

Epoch: 5| Step: 2
Training loss: 2.216797351837158
Validation loss: 2.0183161715025544

Epoch: 5| Step: 3
Training loss: 1.9413928985595703
Validation loss: 2.040731212144257

Epoch: 5| Step: 4
Training loss: 1.627051591873169
Validation loss: 2.0226917677028204

Epoch: 5| Step: 5
Training loss: 1.6182100772857666
Validation loss: 2.0435176075145765

Epoch: 5| Step: 6
Training loss: 2.325883150100708
Validation loss: 2.0477791165792816

Epoch: 5| Step: 7
Training loss: 2.529935121536255
Validation loss: 2.046069270821028

Epoch: 5| Step: 8
Training loss: 3.3758881092071533
Validation loss: 2.053644016224851

Epoch: 5| Step: 9
Training loss: 2.313352108001709
Validation loss: 2.0281395373805875

Epoch: 5| Step: 10
Training loss: 2.271915912628174
Validation loss: 2.0543143851782686

Epoch: 114| Step: 0
Training loss: 2.522334575653076
Validation loss: 2.0314531928749493

Epoch: 5| Step: 1
Training loss: 2.171187162399292
Validation loss: 2.0532052709210302

Epoch: 5| Step: 2
Training loss: 2.517702102661133
Validation loss: 2.0207463913066412

Epoch: 5| Step: 3
Training loss: 2.741366147994995
Validation loss: 2.0389326695472962

Epoch: 5| Step: 4
Training loss: 2.098437786102295
Validation loss: 2.0316638305623043

Epoch: 5| Step: 5
Training loss: 2.1994502544403076
Validation loss: 2.047303565086857

Epoch: 5| Step: 6
Training loss: 1.7908862829208374
Validation loss: 2.013602900248702

Epoch: 5| Step: 7
Training loss: 2.2125511169433594
Validation loss: 2.0286418199539185

Epoch: 5| Step: 8
Training loss: 2.130617618560791
Validation loss: 2.0360539779868176

Epoch: 5| Step: 9
Training loss: 2.269806385040283
Validation loss: 2.050758379761891

Epoch: 5| Step: 10
Training loss: 1.9208219051361084
Validation loss: 2.033184439905228

Epoch: 115| Step: 0
Training loss: 2.232318162918091
Validation loss: 2.0330312816045617

Epoch: 5| Step: 1
Training loss: 2.2559497356414795
Validation loss: 2.0093388903525566

Epoch: 5| Step: 2
Training loss: 2.527278423309326
Validation loss: 2.021958428044473

Epoch: 5| Step: 3
Training loss: 2.349940538406372
Validation loss: 2.029710118488599

Epoch: 5| Step: 4
Training loss: 2.388620615005493
Validation loss: 2.0166490898337415

Epoch: 5| Step: 5
Training loss: 1.9903056621551514
Validation loss: 2.002016303359821

Epoch: 5| Step: 6
Training loss: 1.7137916088104248
Validation loss: 2.01078099461012

Epoch: 5| Step: 7
Training loss: 1.875091791152954
Validation loss: 2.022094588125906

Epoch: 5| Step: 8
Training loss: 2.8232383728027344
Validation loss: 2.0076357536418463

Epoch: 5| Step: 9
Training loss: 2.4139747619628906
Validation loss: 2.021000308375205

Epoch: 5| Step: 10
Training loss: 2.132413625717163
Validation loss: 2.010978993549142

Epoch: 116| Step: 0
Training loss: 2.257189989089966
Validation loss: 2.020510350504229

Epoch: 5| Step: 1
Training loss: 2.189770460128784
Validation loss: 2.0469411560284194

Epoch: 5| Step: 2
Training loss: 2.402550220489502
Validation loss: 2.0132212972128265

Epoch: 5| Step: 3
Training loss: 2.3048102855682373
Validation loss: 2.008082866668701

Epoch: 5| Step: 4
Training loss: 1.9726320505142212
Validation loss: 2.0322401754317747

Epoch: 5| Step: 5
Training loss: 2.047136068344116
Validation loss: 2.0453913801459858

Epoch: 5| Step: 6
Training loss: 2.3842084407806396
Validation loss: 2.0378554969705562

Epoch: 5| Step: 7
Training loss: 2.7135186195373535
Validation loss: 2.036484049212548

Epoch: 5| Step: 8
Training loss: 1.7120939493179321
Validation loss: 2.0176115497466056

Epoch: 5| Step: 9
Training loss: 1.9979108572006226
Validation loss: 2.0279874211998394

Epoch: 5| Step: 10
Training loss: 2.864914894104004
Validation loss: 2.0450541998750422

Epoch: 117| Step: 0
Training loss: 2.4807581901550293
Validation loss: 2.0576859776691725

Epoch: 5| Step: 1
Training loss: 2.7219197750091553
Validation loss: 2.0457274644605574

Epoch: 5| Step: 2
Training loss: 2.701613664627075
Validation loss: 2.028268611559304

Epoch: 5| Step: 3
Training loss: 1.9557373523712158
Validation loss: 2.0602813279756935

Epoch: 5| Step: 4
Training loss: 2.451288938522339
Validation loss: 2.0569927948777393

Epoch: 5| Step: 5
Training loss: 2.336827516555786
Validation loss: 2.0343868014633015

Epoch: 5| Step: 6
Training loss: 1.6844037771224976
Validation loss: 2.055970281682989

Epoch: 5| Step: 7
Training loss: 1.6196378469467163
Validation loss: 2.0392893719416794

Epoch: 5| Step: 8
Training loss: 2.3340563774108887
Validation loss: 2.044063601442563

Epoch: 5| Step: 9
Training loss: 2.4113972187042236
Validation loss: 2.034502383201353

Epoch: 5| Step: 10
Training loss: 2.095348358154297
Validation loss: 2.0632773548044185

Epoch: 118| Step: 0
Training loss: 1.8832902908325195
Validation loss: 2.021380311699324

Epoch: 5| Step: 1
Training loss: 1.7313661575317383
Validation loss: 2.0538751386827037

Epoch: 5| Step: 2
Training loss: 2.4534714221954346
Validation loss: 2.0521108591428368

Epoch: 5| Step: 3
Training loss: 2.5035793781280518
Validation loss: 2.0516048016086703

Epoch: 5| Step: 4
Training loss: 2.4189867973327637
Validation loss: 2.053813107552067

Epoch: 5| Step: 5
Training loss: 2.4235663414001465
Validation loss: 2.0467121524195515

Epoch: 5| Step: 6
Training loss: 2.1091794967651367
Validation loss: 2.0601167191741285

Epoch: 5| Step: 7
Training loss: 1.838457703590393
Validation loss: 2.05490997786163

Epoch: 5| Step: 8
Training loss: 2.3069469928741455
Validation loss: 2.037167436333113

Epoch: 5| Step: 9
Training loss: 2.438401699066162
Validation loss: 2.042113338747332

Epoch: 5| Step: 10
Training loss: 2.6621737480163574
Validation loss: 2.0280803377910326

Epoch: 119| Step: 0
Training loss: 2.5998356342315674
Validation loss: 2.01292089621226

Epoch: 5| Step: 1
Training loss: 2.326993227005005
Validation loss: 2.0369042709309566

Epoch: 5| Step: 2
Training loss: 2.663569450378418
Validation loss: 2.03906669667972

Epoch: 5| Step: 3
Training loss: 1.9241247177124023
Validation loss: 2.0350227330320623

Epoch: 5| Step: 4
Training loss: 2.690192937850952
Validation loss: 2.025979911127398

Epoch: 5| Step: 5
Training loss: 2.1724414825439453
Validation loss: 2.0127435909804476

Epoch: 5| Step: 6
Training loss: 1.9814056158065796
Validation loss: 2.0353562639605616

Epoch: 5| Step: 7
Training loss: 2.532780170440674
Validation loss: 2.0364961367781445

Epoch: 5| Step: 8
Training loss: 1.4738061428070068
Validation loss: 2.037957255558301

Epoch: 5| Step: 9
Training loss: 1.9906457662582397
Validation loss: 2.0398408238605787

Epoch: 5| Step: 10
Training loss: 2.450387954711914
Validation loss: 2.0223920550397647

Epoch: 120| Step: 0
Training loss: 2.4966742992401123
Validation loss: 2.012344503915438

Epoch: 5| Step: 1
Training loss: 2.755303144454956
Validation loss: 2.0230300785392843

Epoch: 5| Step: 2
Training loss: 2.0765202045440674
Validation loss: 2.0477480580729823

Epoch: 5| Step: 3
Training loss: 1.9156758785247803
Validation loss: 2.0261274896642214

Epoch: 5| Step: 4
Training loss: 2.2889790534973145
Validation loss: 2.036050364535342

Epoch: 5| Step: 5
Training loss: 2.339346408843994
Validation loss: 2.0264935954924552

Epoch: 5| Step: 6
Training loss: 2.0038962364196777
Validation loss: 2.0181004885704286

Epoch: 5| Step: 7
Training loss: 2.304128646850586
Validation loss: 2.027371025854541

Epoch: 5| Step: 8
Training loss: 2.4349191188812256
Validation loss: 2.0253848657813123

Epoch: 5| Step: 9
Training loss: 1.7651532888412476
Validation loss: 2.0340306989608274

Epoch: 5| Step: 10
Training loss: 2.3193764686584473
Validation loss: 2.02304397859881

Epoch: 121| Step: 0
Training loss: 2.0267815589904785
Validation loss: 2.0191437326451784

Epoch: 5| Step: 1
Training loss: 2.4200186729431152
Validation loss: 2.025847778525404

Epoch: 5| Step: 2
Training loss: 1.9110634326934814
Validation loss: 2.048789515290209

Epoch: 5| Step: 3
Training loss: 1.5228523015975952
Validation loss: 2.0504374068270446

Epoch: 5| Step: 4
Training loss: 2.425379991531372
Validation loss: 2.0141339866063928

Epoch: 5| Step: 5
Training loss: 2.9069583415985107
Validation loss: 2.0370589686978247

Epoch: 5| Step: 6
Training loss: 2.375518560409546
Validation loss: 2.0196868770865986

Epoch: 5| Step: 7
Training loss: 1.9714267253875732
Validation loss: 2.022557153496691

Epoch: 5| Step: 8
Training loss: 2.305455207824707
Validation loss: 2.0219545005470194

Epoch: 5| Step: 9
Training loss: 2.5482258796691895
Validation loss: 2.018170336241363

Epoch: 5| Step: 10
Training loss: 2.1169545650482178
Validation loss: 2.003713030968943

Epoch: 122| Step: 0
Training loss: 2.0645761489868164
Validation loss: 2.015953263928813

Epoch: 5| Step: 1
Training loss: 2.089057445526123
Validation loss: 2.0263277945979947

Epoch: 5| Step: 2
Training loss: 3.029197931289673
Validation loss: 2.0335675131890083

Epoch: 5| Step: 3
Training loss: 2.0415799617767334
Validation loss: 2.02072916107793

Epoch: 5| Step: 4
Training loss: 2.202664852142334
Validation loss: 2.0176847545049523

Epoch: 5| Step: 5
Training loss: 2.115875720977783
Validation loss: 2.0424542452699397

Epoch: 5| Step: 6
Training loss: 2.7211809158325195
Validation loss: 2.026494926021945

Epoch: 5| Step: 7
Training loss: 2.4500315189361572
Validation loss: 2.036971353715466

Epoch: 5| Step: 8
Training loss: 2.148030996322632
Validation loss: 2.0260980654788274

Epoch: 5| Step: 9
Training loss: 1.6722532510757446
Validation loss: 2.0386035160351823

Epoch: 5| Step: 10
Training loss: 2.172579050064087
Validation loss: 2.0422418694342337

Epoch: 123| Step: 0
Training loss: 2.099548101425171
Validation loss: 2.066346522300474

Epoch: 5| Step: 1
Training loss: 2.1747052669525146
Validation loss: 2.040692114060925

Epoch: 5| Step: 2
Training loss: 2.09102201461792
Validation loss: 2.0407609772938553

Epoch: 5| Step: 3
Training loss: 2.1721179485321045
Validation loss: 2.031260598090387

Epoch: 5| Step: 4
Training loss: 2.019578695297241
Validation loss: 2.039458167168402

Epoch: 5| Step: 5
Training loss: 2.3814940452575684
Validation loss: 2.053689720810101

Epoch: 5| Step: 6
Training loss: 1.6614916324615479
Validation loss: 2.064193396158116

Epoch: 5| Step: 7
Training loss: 2.3530616760253906
Validation loss: 2.039136230304677

Epoch: 5| Step: 8
Training loss: 2.6670267581939697
Validation loss: 2.052063483063893

Epoch: 5| Step: 9
Training loss: 2.3322880268096924
Validation loss: 2.0444566357520317

Epoch: 5| Step: 10
Training loss: 2.7899351119995117
Validation loss: 2.058547999269219

Epoch: 124| Step: 0
Training loss: 2.234999179840088
Validation loss: 2.0545270442962646

Epoch: 5| Step: 1
Training loss: 2.5821239948272705
Validation loss: 2.0548139861834946

Epoch: 5| Step: 2
Training loss: 2.7438883781433105
Validation loss: 2.0077410333900043

Epoch: 5| Step: 3
Training loss: 1.9858999252319336
Validation loss: 2.0598656439012095

Epoch: 5| Step: 4
Training loss: 2.095127582550049
Validation loss: 2.0142855669862483

Epoch: 5| Step: 5
Training loss: 2.6639022827148438
Validation loss: 2.053685554894068

Epoch: 5| Step: 6
Training loss: 2.297621250152588
Validation loss: 2.0570845065578336

Epoch: 5| Step: 7
Training loss: 1.8209342956542969
Validation loss: 2.043264022437475

Epoch: 5| Step: 8
Training loss: 2.247122287750244
Validation loss: 2.0389523493346347

Epoch: 5| Step: 9
Training loss: 2.014277696609497
Validation loss: 2.055840261520878

Epoch: 5| Step: 10
Training loss: 1.9783432483673096
Validation loss: 2.0426431599483696

Epoch: 125| Step: 0
Training loss: 2.6584925651550293
Validation loss: 2.044680622316176

Epoch: 5| Step: 1
Training loss: 2.2574775218963623
Validation loss: 2.065822826918735

Epoch: 5| Step: 2
Training loss: 2.5359954833984375
Validation loss: 2.032560456183649

Epoch: 5| Step: 3
Training loss: 1.7999387979507446
Validation loss: 2.0358962833240466

Epoch: 5| Step: 4
Training loss: 2.1341159343719482
Validation loss: 2.0356194460263817

Epoch: 5| Step: 5
Training loss: 2.543707847595215
Validation loss: 2.0786562658125356

Epoch: 5| Step: 6
Training loss: 2.062762975692749
Validation loss: 2.0569253019107285

Epoch: 5| Step: 7
Training loss: 2.048175096511841
Validation loss: 2.050776313709956

Epoch: 5| Step: 8
Training loss: 2.2793025970458984
Validation loss: 2.050638783362604

Epoch: 5| Step: 9
Training loss: 1.8948978185653687
Validation loss: 2.040354436443698

Epoch: 5| Step: 10
Training loss: 2.31382155418396
Validation loss: 2.0489801488896853

Epoch: 126| Step: 0
Training loss: 2.0648369789123535
Validation loss: 2.0693358657180623

Epoch: 5| Step: 1
Training loss: 1.8328958749771118
Validation loss: 2.033553898975413

Epoch: 5| Step: 2
Training loss: 2.3156914710998535
Validation loss: 2.0552081036311325

Epoch: 5| Step: 3
Training loss: 2.73457670211792
Validation loss: 2.035956343015035

Epoch: 5| Step: 4
Training loss: 2.1507229804992676
Validation loss: 2.0328119800936792

Epoch: 5| Step: 5
Training loss: 1.8883466720581055
Validation loss: 2.01695075086368

Epoch: 5| Step: 6
Training loss: 2.4290900230407715
Validation loss: 2.04493990764823

Epoch: 5| Step: 7
Training loss: 2.5024077892303467
Validation loss: 2.0347481517381567

Epoch: 5| Step: 8
Training loss: 2.130403518676758
Validation loss: 2.0606993193267495

Epoch: 5| Step: 9
Training loss: 2.2989389896392822
Validation loss: 2.0286025206247964

Epoch: 5| Step: 10
Training loss: 2.2214608192443848
Validation loss: 2.04338901786394

Epoch: 127| Step: 0
Training loss: 2.2135372161865234
Validation loss: 2.0457225512432795

Epoch: 5| Step: 1
Training loss: 3.053051471710205
Validation loss: 2.0272687583841305

Epoch: 5| Step: 2
Training loss: 2.0058789253234863
Validation loss: 2.0258331221918904

Epoch: 5| Step: 3
Training loss: 2.6999499797821045
Validation loss: 2.0138784044532367

Epoch: 5| Step: 4
Training loss: 1.837773084640503
Validation loss: 2.0036457687295894

Epoch: 5| Step: 5
Training loss: 2.3683223724365234
Validation loss: 1.9892469439455258

Epoch: 5| Step: 6
Training loss: 2.073770523071289
Validation loss: 2.0234065683939124

Epoch: 5| Step: 7
Training loss: 1.3161230087280273
Validation loss: 2.044430999345677

Epoch: 5| Step: 8
Training loss: 2.3920445442199707
Validation loss: 2.0185838066121584

Epoch: 5| Step: 9
Training loss: 2.355358123779297
Validation loss: 2.0240566192134732

Epoch: 5| Step: 10
Training loss: 2.0455312728881836
Validation loss: 2.023918290292063

Epoch: 128| Step: 0
Training loss: 1.584652304649353
Validation loss: 2.014979818815826

Epoch: 5| Step: 1
Training loss: 2.180373430252075
Validation loss: 2.0377817794840825

Epoch: 5| Step: 2
Training loss: 1.934901237487793
Validation loss: 2.036816841812544

Epoch: 5| Step: 3
Training loss: 2.050558567047119
Validation loss: 2.003184433906309

Epoch: 5| Step: 4
Training loss: 2.5321993827819824
Validation loss: 2.0331349039590485

Epoch: 5| Step: 5
Training loss: 2.266038179397583
Validation loss: 2.0149150176714827

Epoch: 5| Step: 6
Training loss: 2.366412878036499
Validation loss: 2.029924464482133

Epoch: 5| Step: 7
Training loss: 2.91778564453125
Validation loss: 2.0274629105803785

Epoch: 5| Step: 8
Training loss: 2.4406561851501465
Validation loss: 2.038484122163506

Epoch: 5| Step: 9
Training loss: 2.3504929542541504
Validation loss: 2.007299403990469

Epoch: 5| Step: 10
Training loss: 1.982721209526062
Validation loss: 2.027215019349129

Epoch: 129| Step: 0
Training loss: 2.1755313873291016
Validation loss: 2.014518150719263

Epoch: 5| Step: 1
Training loss: 2.4335274696350098
Validation loss: 2.032043521122266

Epoch: 5| Step: 2
Training loss: 2.333263874053955
Validation loss: 1.983026610907688

Epoch: 5| Step: 3
Training loss: 2.4812655448913574
Validation loss: 1.987478171625445

Epoch: 5| Step: 4
Training loss: 2.746565580368042
Validation loss: 1.9966507483554143

Epoch: 5| Step: 5
Training loss: 2.138237476348877
Validation loss: 1.9989912099735712

Epoch: 5| Step: 6
Training loss: 1.8384462594985962
Validation loss: 2.0104223053942443

Epoch: 5| Step: 7
Training loss: 2.0416781902313232
Validation loss: 2.0281289982539352

Epoch: 5| Step: 8
Training loss: 2.5260589122772217
Validation loss: 1.9956419596108057

Epoch: 5| Step: 9
Training loss: 1.7836456298828125
Validation loss: 2.035310543993468

Epoch: 5| Step: 10
Training loss: 2.268444299697876
Validation loss: 2.020811098878102

Epoch: 130| Step: 0
Training loss: 1.5940349102020264
Validation loss: 2.0122786939785047

Epoch: 5| Step: 1
Training loss: 2.4803662300109863
Validation loss: 2.0259919448565413

Epoch: 5| Step: 2
Training loss: 2.1276021003723145
Validation loss: 2.0373121897379556

Epoch: 5| Step: 3
Training loss: 1.8481944799423218
Validation loss: 2.0204272706021547

Epoch: 5| Step: 4
Training loss: 2.0723185539245605
Validation loss: 2.022270361582438

Epoch: 5| Step: 5
Training loss: 1.9333750009536743
Validation loss: 2.035261272102274

Epoch: 5| Step: 6
Training loss: 2.8169679641723633
Validation loss: 2.0418127711101244

Epoch: 5| Step: 7
Training loss: 2.558875560760498
Validation loss: 2.026462367785874

Epoch: 5| Step: 8
Training loss: 2.0733540058135986
Validation loss: 2.0299914985574703

Epoch: 5| Step: 9
Training loss: 2.6178650856018066
Validation loss: 2.0324921364425332

Epoch: 5| Step: 10
Training loss: 2.354623794555664
Validation loss: 2.0349594751993814

Epoch: 131| Step: 0
Training loss: 2.473639726638794
Validation loss: 2.0422335414476294

Epoch: 5| Step: 1
Training loss: 2.409733533859253
Validation loss: 2.014784574508667

Epoch: 5| Step: 2
Training loss: 2.5480780601501465
Validation loss: 2.0448922059869252

Epoch: 5| Step: 3
Training loss: 2.693385601043701
Validation loss: 2.0318408768664122

Epoch: 5| Step: 4
Training loss: 2.125528335571289
Validation loss: 2.026344727444392

Epoch: 5| Step: 5
Training loss: 1.479034423828125
Validation loss: 2.0558046192251225

Epoch: 5| Step: 6
Training loss: 2.377553701400757
Validation loss: 2.042995814354189

Epoch: 5| Step: 7
Training loss: 1.9246238470077515
Validation loss: 2.0524989302440355

Epoch: 5| Step: 8
Training loss: 1.8607816696166992
Validation loss: 2.064860983561444

Epoch: 5| Step: 9
Training loss: 2.19356107711792
Validation loss: 2.071485383536226

Epoch: 5| Step: 10
Training loss: 2.360869884490967
Validation loss: 2.0282260423065512

Epoch: 132| Step: 0
Training loss: 1.9584583044052124
Validation loss: 2.065620829982142

Epoch: 5| Step: 1
Training loss: 2.4382851123809814
Validation loss: 2.0609740826391403

Epoch: 5| Step: 2
Training loss: 2.445288896560669
Validation loss: 2.064134129913904

Epoch: 5| Step: 3
Training loss: 2.1403253078460693
Validation loss: 2.035355608950379

Epoch: 5| Step: 4
Training loss: 2.4148383140563965
Validation loss: 2.0460982989239436

Epoch: 5| Step: 5
Training loss: 2.0403029918670654
Validation loss: 2.029298533675491

Epoch: 5| Step: 6
Training loss: 2.295865774154663
Validation loss: 2.0379355210129932

Epoch: 5| Step: 7
Training loss: 2.4765172004699707
Validation loss: 2.029374825057163

Epoch: 5| Step: 8
Training loss: 1.868800401687622
Validation loss: 2.05331059937836

Epoch: 5| Step: 9
Training loss: 2.1105287075042725
Validation loss: 2.062448065768006

Epoch: 5| Step: 10
Training loss: 2.4749093055725098
Validation loss: 2.054921002798183

Epoch: 133| Step: 0
Training loss: 2.358222007751465
Validation loss: 2.013291599929974

Epoch: 5| Step: 1
Training loss: 2.101984739303589
Validation loss: 2.074724098687531

Epoch: 5| Step: 2
Training loss: 2.5411651134490967
Validation loss: 2.0485695126236125

Epoch: 5| Step: 3
Training loss: 2.538013458251953
Validation loss: 2.043699533708634

Epoch: 5| Step: 4
Training loss: 2.1765806674957275
Validation loss: 2.024895732120801

Epoch: 5| Step: 5
Training loss: 2.6913552284240723
Validation loss: 2.039091379411759

Epoch: 5| Step: 6
Training loss: 2.2402453422546387
Validation loss: 2.04185918326019

Epoch: 5| Step: 7
Training loss: 2.160510540008545
Validation loss: 2.048003004443261

Epoch: 5| Step: 8
Training loss: 1.330814003944397
Validation loss: 2.0210477434178835

Epoch: 5| Step: 9
Training loss: 1.9416096210479736
Validation loss: 2.0357117729802288

Epoch: 5| Step: 10
Training loss: 2.5500309467315674
Validation loss: 2.035921763348323

Epoch: 134| Step: 0
Training loss: 2.4599318504333496
Validation loss: 2.0329048236211142

Epoch: 5| Step: 1
Training loss: 1.8836952447891235
Validation loss: 2.025903768436883

Epoch: 5| Step: 2
Training loss: 2.068544626235962
Validation loss: 2.025913658962455

Epoch: 5| Step: 3
Training loss: 1.885451078414917
Validation loss: 2.021233202308737

Epoch: 5| Step: 4
Training loss: 2.555361270904541
Validation loss: 2.035688925814885

Epoch: 5| Step: 5
Training loss: 2.7253334522247314
Validation loss: 2.0302946746990247

Epoch: 5| Step: 6
Training loss: 1.8361988067626953
Validation loss: 2.02152523943173

Epoch: 5| Step: 7
Training loss: 2.2198166847229004
Validation loss: 1.9957700314060334

Epoch: 5| Step: 8
Training loss: 2.052849292755127
Validation loss: 2.0184809110497914

Epoch: 5| Step: 9
Training loss: 2.298288345336914
Validation loss: 2.0173450593025453

Epoch: 5| Step: 10
Training loss: 2.4540021419525146
Validation loss: 2.0320881079601985

Epoch: 135| Step: 0
Training loss: 2.2579164505004883
Validation loss: 2.013926811115716

Epoch: 5| Step: 1
Training loss: 2.3597161769866943
Validation loss: 2.0294579203410814

Epoch: 5| Step: 2
Training loss: 2.1839537620544434
Validation loss: 2.0232095846565823

Epoch: 5| Step: 3
Training loss: 1.6752640008926392
Validation loss: 2.038012650705153

Epoch: 5| Step: 4
Training loss: 2.267092704772949
Validation loss: 2.02396382311339

Epoch: 5| Step: 5
Training loss: 2.743277072906494
Validation loss: 2.017982693128688

Epoch: 5| Step: 6
Training loss: 2.685274124145508
Validation loss: 2.0215932117995394

Epoch: 5| Step: 7
Training loss: 1.7165019512176514
Validation loss: 2.041653399826378

Epoch: 5| Step: 8
Training loss: 2.101642608642578
Validation loss: 2.0485858276326168

Epoch: 5| Step: 9
Training loss: 2.192221164703369
Validation loss: 2.0403517920483827

Epoch: 5| Step: 10
Training loss: 2.423414945602417
Validation loss: 2.033215125401815

Epoch: 136| Step: 0
Training loss: 2.298387050628662
Validation loss: 2.0465700818646337

Epoch: 5| Step: 1
Training loss: 1.7240161895751953
Validation loss: 2.030682158726518

Epoch: 5| Step: 2
Training loss: 2.2764739990234375
Validation loss: 2.017740800816526

Epoch: 5| Step: 3
Training loss: 2.378052234649658
Validation loss: 2.031475797776253

Epoch: 5| Step: 4
Training loss: 2.1113035678863525
Validation loss: 2.023847190282678

Epoch: 5| Step: 5
Training loss: 2.3641178607940674
Validation loss: 2.067542637548139

Epoch: 5| Step: 6
Training loss: 2.37678861618042
Validation loss: 2.042872198166386

Epoch: 5| Step: 7
Training loss: 2.4626803398132324
Validation loss: 2.0462163545752086

Epoch: 5| Step: 8
Training loss: 2.1541738510131836
Validation loss: 2.0545869488869943

Epoch: 5| Step: 9
Training loss: 2.7016005516052246
Validation loss: 2.060959239159861

Epoch: 5| Step: 10
Training loss: 1.5618311166763306
Validation loss: 2.0397826933091685

Epoch: 137| Step: 0
Training loss: 2.532647132873535
Validation loss: 2.03921704394843

Epoch: 5| Step: 1
Training loss: 2.5140461921691895
Validation loss: 2.050601754137265

Epoch: 5| Step: 2
Training loss: 1.658565878868103
Validation loss: 2.029587390602276

Epoch: 5| Step: 3
Training loss: 2.533982992172241
Validation loss: 2.0212646351065686

Epoch: 5| Step: 4
Training loss: 2.521467685699463
Validation loss: 2.0614998596970753

Epoch: 5| Step: 5
Training loss: 1.8919872045516968
Validation loss: 2.0220580588104906

Epoch: 5| Step: 6
Training loss: 2.420032501220703
Validation loss: 2.038159349913238

Epoch: 5| Step: 7
Training loss: 1.9258110523223877
Validation loss: 2.0287169333427184

Epoch: 5| Step: 8
Training loss: 2.050471782684326
Validation loss: 2.0229195548642065

Epoch: 5| Step: 9
Training loss: 2.6137337684631348
Validation loss: 2.0246073815130416

Epoch: 5| Step: 10
Training loss: 1.8242305517196655
Validation loss: 2.023105077846076

Epoch: 138| Step: 0
Training loss: 2.013533115386963
Validation loss: 2.0343526012154034

Epoch: 5| Step: 1
Training loss: 2.1647448539733887
Validation loss: 2.0333863971053914

Epoch: 5| Step: 2
Training loss: 1.8164615631103516
Validation loss: 2.001180289894022

Epoch: 5| Step: 3
Training loss: 2.1667046546936035
Validation loss: 2.037139159376903

Epoch: 5| Step: 4
Training loss: 1.7794692516326904
Validation loss: 2.017979903887677

Epoch: 5| Step: 5
Training loss: 2.4990234375
Validation loss: 2.0184056028243034

Epoch: 5| Step: 6
Training loss: 2.1371734142303467
Validation loss: 2.025487207597302

Epoch: 5| Step: 7
Training loss: 1.9237810373306274
Validation loss: 2.0293346476811234

Epoch: 5| Step: 8
Training loss: 2.8622655868530273
Validation loss: 2.0223004741053425

Epoch: 5| Step: 9
Training loss: 2.8697996139526367
Validation loss: 2.0361235718573294

Epoch: 5| Step: 10
Training loss: 2.167675733566284
Validation loss: 2.0143850477792884

Epoch: 139| Step: 0
Training loss: 1.8439044952392578
Validation loss: 2.003080088605163

Epoch: 5| Step: 1
Training loss: 1.899776816368103
Validation loss: 2.0166830580721617

Epoch: 5| Step: 2
Training loss: 2.3429934978485107
Validation loss: 2.034054660028027

Epoch: 5| Step: 3
Training loss: 2.5382444858551025
Validation loss: 2.040432414700908

Epoch: 5| Step: 4
Training loss: 2.129387378692627
Validation loss: 2.0517690848278742

Epoch: 5| Step: 5
Training loss: 2.042912721633911
Validation loss: 2.0390979525863484

Epoch: 5| Step: 6
Training loss: 2.306312084197998
Validation loss: 2.025350852679181

Epoch: 5| Step: 7
Training loss: 2.506963014602661
Validation loss: 2.060417393202423

Epoch: 5| Step: 8
Training loss: 2.5384521484375
Validation loss: 2.0500026813117405

Epoch: 5| Step: 9
Training loss: 2.153329372406006
Validation loss: 2.064399737183766

Epoch: 5| Step: 10
Training loss: 1.9387918710708618
Validation loss: 2.095686174208118

Epoch: 140| Step: 0
Training loss: 2.495422840118408
Validation loss: 2.068399526739633

Epoch: 5| Step: 1
Training loss: 2.0281333923339844
Validation loss: 2.0793556654325096

Epoch: 5| Step: 2
Training loss: 2.552269220352173
Validation loss: 2.09072958782155

Epoch: 5| Step: 3
Training loss: 1.8980516195297241
Validation loss: 2.0695527061339347

Epoch: 5| Step: 4
Training loss: 2.102696657180786
Validation loss: 2.069331528038107

Epoch: 5| Step: 5
Training loss: 2.19402813911438
Validation loss: 2.071077295528945

Epoch: 5| Step: 6
Training loss: 2.167835235595703
Validation loss: 2.0531356437231905

Epoch: 5| Step: 7
Training loss: 2.4243626594543457
Validation loss: 2.0753673789321736

Epoch: 5| Step: 8
Training loss: 2.086531639099121
Validation loss: 2.0626212781475437

Epoch: 5| Step: 9
Training loss: 2.2527976036071777
Validation loss: 2.055675173318514

Epoch: 5| Step: 10
Training loss: 2.239945888519287
Validation loss: 2.0283759076108216

Epoch: 141| Step: 0
Training loss: 2.337602376937866
Validation loss: 2.072998105838735

Epoch: 5| Step: 1
Training loss: 2.289553165435791
Validation loss: 2.0534123618115663

Epoch: 5| Step: 2
Training loss: 1.8079904317855835
Validation loss: 2.0473202402873705

Epoch: 5| Step: 3
Training loss: 2.4534597396850586
Validation loss: 2.043134161221084

Epoch: 5| Step: 4
Training loss: 1.8416827917099
Validation loss: 2.0617407855167182

Epoch: 5| Step: 5
Training loss: 2.819188356399536
Validation loss: 2.0263862738045315

Epoch: 5| Step: 6
Training loss: 1.8963028192520142
Validation loss: 2.0603859398954656

Epoch: 5| Step: 7
Training loss: 1.9746452569961548
Validation loss: 2.03421696283484

Epoch: 5| Step: 8
Training loss: 2.440927028656006
Validation loss: 2.0176245499682683

Epoch: 5| Step: 9
Training loss: 1.99588143825531
Validation loss: 2.0336293687102613

Epoch: 5| Step: 10
Training loss: 2.673886299133301
Validation loss: 2.0039722560554423

Epoch: 142| Step: 0
Training loss: 2.6011803150177
Validation loss: 2.009294622687883

Epoch: 5| Step: 1
Training loss: 2.020141839981079
Validation loss: 2.0272822892794045

Epoch: 5| Step: 2
Training loss: 1.8606250286102295
Validation loss: 2.0438998181332826

Epoch: 5| Step: 3
Training loss: 1.957275390625
Validation loss: 2.006458379889047

Epoch: 5| Step: 4
Training loss: 2.65474534034729
Validation loss: 2.03019993664116

Epoch: 5| Step: 5
Training loss: 2.4082045555114746
Validation loss: 2.0086594345749065

Epoch: 5| Step: 6
Training loss: 2.5022201538085938
Validation loss: 2.0064891230675483

Epoch: 5| Step: 7
Training loss: 1.5410948991775513
Validation loss: 2.0202358051012923

Epoch: 5| Step: 8
Training loss: 1.611074447631836
Validation loss: 2.00429662453231

Epoch: 5| Step: 9
Training loss: 2.862354040145874
Validation loss: 2.025108791166736

Epoch: 5| Step: 10
Training loss: 2.3906455039978027
Validation loss: 2.0542202418850315

Epoch: 143| Step: 0
Training loss: 1.8468496799468994
Validation loss: 2.0488902394489577

Epoch: 5| Step: 1
Training loss: 1.8167892694473267
Validation loss: 2.0225253028254353

Epoch: 5| Step: 2
Training loss: 2.160092353820801
Validation loss: 2.0143250009065032

Epoch: 5| Step: 3
Training loss: 2.753782272338867
Validation loss: 2.009192532108676

Epoch: 5| Step: 4
Training loss: 1.9296365976333618
Validation loss: 2.0142935245267806

Epoch: 5| Step: 5
Training loss: 2.0960135459899902
Validation loss: 2.0168677670981294

Epoch: 5| Step: 6
Training loss: 2.551520347595215
Validation loss: 2.0144999501525716

Epoch: 5| Step: 7
Training loss: 2.504697561264038
Validation loss: 2.010792624565863

Epoch: 5| Step: 8
Training loss: 2.0406346321105957
Validation loss: 2.020207525581442

Epoch: 5| Step: 9
Training loss: 2.0905678272247314
Validation loss: 2.0357305952297744

Epoch: 5| Step: 10
Training loss: 2.8809878826141357
Validation loss: 2.028164784113566

Epoch: 144| Step: 0
Training loss: 2.019646406173706
Validation loss: 2.032008488972982

Epoch: 5| Step: 1
Training loss: 2.7528743743896484
Validation loss: 2.0503240118744555

Epoch: 5| Step: 2
Training loss: 2.185222864151001
Validation loss: 2.0057285472910893

Epoch: 5| Step: 3
Training loss: 1.5181009769439697
Validation loss: 2.051665086900034

Epoch: 5| Step: 4
Training loss: 2.3997881412506104
Validation loss: 2.048488773325438

Epoch: 5| Step: 5
Training loss: 1.9710630178451538
Validation loss: 2.0544225951676727

Epoch: 5| Step: 6
Training loss: 2.1108739376068115
Validation loss: 2.0230390423087665

Epoch: 5| Step: 7
Training loss: 2.077399492263794
Validation loss: 2.0123548405144804

Epoch: 5| Step: 8
Training loss: 2.783214807510376
Validation loss: 2.030736368189576

Epoch: 5| Step: 9
Training loss: 2.142944574356079
Validation loss: 2.054011442328012

Epoch: 5| Step: 10
Training loss: 2.4644906520843506
Validation loss: 2.0403185582930043

Epoch: 145| Step: 0
Training loss: 2.3876283168792725
Validation loss: 2.0562572722793906

Epoch: 5| Step: 1
Training loss: 2.787759780883789
Validation loss: 2.0379222259726575

Epoch: 5| Step: 2
Training loss: 2.1228199005126953
Validation loss: 2.0557724173351

Epoch: 5| Step: 3
Training loss: 2.2876689434051514
Validation loss: 2.068833448553598

Epoch: 5| Step: 4
Training loss: 2.0879502296447754
Validation loss: 2.0720389145676807

Epoch: 5| Step: 5
Training loss: 1.6503950357437134
Validation loss: 2.03589637689693

Epoch: 5| Step: 6
Training loss: 2.7684788703918457
Validation loss: 2.0605268952667073

Epoch: 5| Step: 7
Training loss: 2.2929434776306152
Validation loss: 2.0737263515431392

Epoch: 5| Step: 8
Training loss: 1.8465389013290405
Validation loss: 2.0582639030230943

Epoch: 5| Step: 9
Training loss: 2.025110960006714
Validation loss: 2.052930606308804

Epoch: 5| Step: 10
Training loss: 2.2461280822753906
Validation loss: 2.077071597499232

Epoch: 146| Step: 0
Training loss: 2.257535934448242
Validation loss: 2.070920116157942

Epoch: 5| Step: 1
Training loss: 2.6617817878723145
Validation loss: 2.0692084784148843

Epoch: 5| Step: 2
Training loss: 2.3412444591522217
Validation loss: 2.0481897836090415

Epoch: 5| Step: 3
Training loss: 2.0359439849853516
Validation loss: 2.046214072935043

Epoch: 5| Step: 4
Training loss: 1.9691131114959717
Validation loss: 2.033416532701062

Epoch: 5| Step: 5
Training loss: 2.3347020149230957
Validation loss: 2.0632592862652195

Epoch: 5| Step: 6
Training loss: 2.5529987812042236
Validation loss: 2.054908731932281

Epoch: 5| Step: 7
Training loss: 1.888861060142517
Validation loss: 2.0573160186890633

Epoch: 5| Step: 8
Training loss: 1.8551260232925415
Validation loss: 2.0715110058425577

Epoch: 5| Step: 9
Training loss: 1.966500997543335
Validation loss: 2.0485981728440974

Epoch: 5| Step: 10
Training loss: 2.5231611728668213
Validation loss: 2.0613780329304356

Epoch: 147| Step: 0
Training loss: 2.092836856842041
Validation loss: 2.0660096842755555

Epoch: 5| Step: 1
Training loss: 2.2089486122131348
Validation loss: 2.067332001142604

Epoch: 5| Step: 2
Training loss: 2.24808406829834
Validation loss: 2.0411501622969106

Epoch: 5| Step: 3
Training loss: 2.8538460731506348
Validation loss: 2.0495534135449316

Epoch: 5| Step: 4
Training loss: 2.1627860069274902
Validation loss: 2.041064832800178

Epoch: 5| Step: 5
Training loss: 1.8276822566986084
Validation loss: 2.0211223171603296

Epoch: 5| Step: 6
Training loss: 2.0561349391937256
Validation loss: 2.037410707883937

Epoch: 5| Step: 7
Training loss: 2.2257707118988037
Validation loss: 2.05579149723053

Epoch: 5| Step: 8
Training loss: 2.1207704544067383
Validation loss: 2.0409678182294293

Epoch: 5| Step: 9
Training loss: 2.211150646209717
Validation loss: 2.047574025328441

Epoch: 5| Step: 10
Training loss: 2.5507619380950928
Validation loss: 2.056519217388604

Epoch: 148| Step: 0
Training loss: 2.2194435596466064
Validation loss: 2.047366570400935

Epoch: 5| Step: 1
Training loss: 1.5949561595916748
Validation loss: 2.038740365735946

Epoch: 5| Step: 2
Training loss: 2.081322193145752
Validation loss: 2.050609293804374

Epoch: 5| Step: 3
Training loss: 2.3811843395233154
Validation loss: 2.064368645350138

Epoch: 5| Step: 4
Training loss: 2.3155007362365723
Validation loss: 2.017462756044121

Epoch: 5| Step: 5
Training loss: 2.2627320289611816
Validation loss: 2.0629701588743474

Epoch: 5| Step: 6
Training loss: 2.5586013793945312
Validation loss: 2.052418114036642

Epoch: 5| Step: 7
Training loss: 2.5967228412628174
Validation loss: 2.023429883423672

Epoch: 5| Step: 8
Training loss: 1.8284631967544556
Validation loss: 2.0388668096193703

Epoch: 5| Step: 9
Training loss: 1.7291206121444702
Validation loss: 2.0338539423481112

Epoch: 5| Step: 10
Training loss: 2.603987693786621
Validation loss: 2.0266215608965967

Epoch: 149| Step: 0
Training loss: 2.301564931869507
Validation loss: 2.0493272196862007

Epoch: 5| Step: 1
Training loss: 1.8553940057754517
Validation loss: 2.0509641349956556

Epoch: 5| Step: 2
Training loss: 1.9537826776504517
Validation loss: 2.0516566922587733

Epoch: 5| Step: 3
Training loss: 1.4623363018035889
Validation loss: 2.047847825993774

Epoch: 5| Step: 4
Training loss: 2.3059849739074707
Validation loss: 2.05974478619073

Epoch: 5| Step: 5
Training loss: 2.311448812484741
Validation loss: 2.0386529571266583

Epoch: 5| Step: 6
Training loss: 2.6721854209899902
Validation loss: 2.0311046467032483

Epoch: 5| Step: 7
Training loss: 2.191688060760498
Validation loss: 2.051820666559281

Epoch: 5| Step: 8
Training loss: 2.2141168117523193
Validation loss: 2.0459776463047152

Epoch: 5| Step: 9
Training loss: 2.501539707183838
Validation loss: 2.054319290704625

Epoch: 5| Step: 10
Training loss: 2.489567756652832
Validation loss: 2.042035266917239

Epoch: 150| Step: 0
Training loss: 1.599143624305725
Validation loss: 2.038518049383676

Epoch: 5| Step: 1
Training loss: 2.6548056602478027
Validation loss: 2.0162664677507136

Epoch: 5| Step: 2
Training loss: 2.0238022804260254
Validation loss: 2.0521584121129846

Epoch: 5| Step: 3
Training loss: 2.600761890411377
Validation loss: 2.035811403746246

Epoch: 5| Step: 4
Training loss: 1.2577630281448364
Validation loss: 2.0476842875121744

Epoch: 5| Step: 5
Training loss: 2.008808135986328
Validation loss: 2.0166281705261557

Epoch: 5| Step: 6
Training loss: 2.6676535606384277
Validation loss: 2.023327806944488

Epoch: 5| Step: 7
Training loss: 2.437490940093994
Validation loss: 2.054597120131216

Epoch: 5| Step: 8
Training loss: 2.5860724449157715
Validation loss: 2.018990898645052

Epoch: 5| Step: 9
Training loss: 1.968042016029358
Validation loss: 2.0033861949879634

Epoch: 5| Step: 10
Training loss: 2.6850438117980957
Validation loss: 2.0541097182099537

Testing loss: 2.0492623382144504

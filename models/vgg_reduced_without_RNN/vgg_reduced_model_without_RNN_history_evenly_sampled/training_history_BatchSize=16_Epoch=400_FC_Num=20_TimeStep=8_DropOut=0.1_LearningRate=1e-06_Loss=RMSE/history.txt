Epoch: 1| Step: 0
Training loss: 6.370747962048669
Validation loss: 5.413023761911676

Epoch: 6| Step: 1
Training loss: 5.974636510542333
Validation loss: 5.409738246450935

Epoch: 6| Step: 2
Training loss: 5.513108325012335
Validation loss: 5.407510017270177

Epoch: 6| Step: 3
Training loss: 5.133945752227658
Validation loss: 5.403378589681979

Epoch: 6| Step: 4
Training loss: 6.077617241604576
Validation loss: 5.4028671637841725

Epoch: 6| Step: 5
Training loss: 6.333967812863793
Validation loss: 5.397668882708605

Epoch: 6| Step: 6
Training loss: 4.376777505819689
Validation loss: 5.396201391000804

Epoch: 6| Step: 7
Training loss: 6.129470128219435
Validation loss: 5.394054516672778

Epoch: 6| Step: 8
Training loss: 5.53684499065682
Validation loss: 5.390069128157074

Epoch: 6| Step: 9
Training loss: 4.400041250555703
Validation loss: 5.386930874247302

Epoch: 6| Step: 10
Training loss: 5.339400813743494
Validation loss: 5.38315826238523

Epoch: 6| Step: 11
Training loss: 4.4972021093947
Validation loss: 5.383142281868095

Epoch: 6| Step: 12
Training loss: 4.322377489161894
Validation loss: 5.376626057070396

Epoch: 6| Step: 13
Training loss: 5.757264565388576
Validation loss: 5.373088043507611

Epoch: 2| Step: 0
Training loss: 5.667525207705782
Validation loss: 5.371592111685254

Epoch: 6| Step: 1
Training loss: 4.826054693981592
Validation loss: 5.367108774951956

Epoch: 6| Step: 2
Training loss: 5.799936478365619
Validation loss: 5.367801009115182

Epoch: 6| Step: 3
Training loss: 5.764828676058521
Validation loss: 5.361885907367681

Epoch: 6| Step: 4
Training loss: 5.481233658978279
Validation loss: 5.3593576608520435

Epoch: 6| Step: 5
Training loss: 5.374770403549266
Validation loss: 5.355391939828769

Epoch: 6| Step: 6
Training loss: 6.232695771657016
Validation loss: 5.352124260302317

Epoch: 6| Step: 7
Training loss: 5.015996140863373
Validation loss: 5.349522662285448

Epoch: 6| Step: 8
Training loss: 5.554524050425726
Validation loss: 5.34843113425138

Epoch: 6| Step: 9
Training loss: 4.95902591431007
Validation loss: 5.3425956611012095

Epoch: 6| Step: 10
Training loss: 5.265319815374818
Validation loss: 5.340117015892315

Epoch: 6| Step: 11
Training loss: 5.609884849903321
Validation loss: 5.33493414178588

Epoch: 6| Step: 12
Training loss: 4.038275459390195
Validation loss: 5.332644090543893

Epoch: 6| Step: 13
Training loss: 5.998665979221833
Validation loss: 5.331518792578357

Epoch: 3| Step: 0
Training loss: 5.258038134785204
Validation loss: 5.328090089385229

Epoch: 6| Step: 1
Training loss: 5.049306372416839
Validation loss: 5.324710979953798

Epoch: 6| Step: 2
Training loss: 4.88561892006076
Validation loss: 5.3199512804242985

Epoch: 6| Step: 3
Training loss: 3.1963635047567753
Validation loss: 5.321414667285434

Epoch: 6| Step: 4
Training loss: 6.538148002529349
Validation loss: 5.313336378319431

Epoch: 6| Step: 5
Training loss: 5.744053958990032
Validation loss: 5.308496003605982

Epoch: 6| Step: 6
Training loss: 4.898383181330111
Validation loss: 5.307684335346979

Epoch: 6| Step: 7
Training loss: 5.805305869022765
Validation loss: 5.305261054671549

Epoch: 6| Step: 8
Training loss: 5.863368430814215
Validation loss: 5.300582255821258

Epoch: 6| Step: 9
Training loss: 4.798705307875858
Validation loss: 5.297981601807312

Epoch: 6| Step: 10
Training loss: 4.895912894826711
Validation loss: 5.295023838216412

Epoch: 6| Step: 11
Training loss: 5.474015670848711
Validation loss: 5.288198369123747

Epoch: 6| Step: 12
Training loss: 6.009474585189546
Validation loss: 5.2912826043436025

Epoch: 6| Step: 13
Training loss: 6.270592601460114
Validation loss: 5.2831088820835825

Epoch: 4| Step: 0
Training loss: 5.191235587570572
Validation loss: 5.279364618079641

Epoch: 6| Step: 1
Training loss: 5.407865955316739
Validation loss: 5.277069600334056

Epoch: 6| Step: 2
Training loss: 6.1649440043212085
Validation loss: 5.269226091167505

Epoch: 6| Step: 3
Training loss: 4.52594820488705
Validation loss: 5.268374056300256

Epoch: 6| Step: 4
Training loss: 6.215527797147973
Validation loss: 5.267158568092635

Epoch: 6| Step: 5
Training loss: 5.575399122790938
Validation loss: 5.263287010325432

Epoch: 6| Step: 6
Training loss: 3.543907234700025
Validation loss: 5.25890646494289

Epoch: 6| Step: 7
Training loss: 4.808799212456542
Validation loss: 5.2539968575269285

Epoch: 6| Step: 8
Training loss: 5.591702731263032
Validation loss: 5.249521467097613

Epoch: 6| Step: 9
Training loss: 5.352484096056256
Validation loss: 5.250538916988562

Epoch: 6| Step: 10
Training loss: 4.921026053707158
Validation loss: 5.243963213438193

Epoch: 6| Step: 11
Training loss: 4.853356857312661
Validation loss: 5.237804769144679

Epoch: 6| Step: 12
Training loss: 5.461935451499109
Validation loss: 5.237970944384082

Epoch: 6| Step: 13
Training loss: 6.732997248857673
Validation loss: 5.234611408672437

Epoch: 5| Step: 0
Training loss: 5.120183402591344
Validation loss: 5.227642646984203

Epoch: 6| Step: 1
Training loss: 5.02026608799158
Validation loss: 5.225228664008497

Epoch: 6| Step: 2
Training loss: 5.418975142539314
Validation loss: 5.218451299992746

Epoch: 6| Step: 3
Training loss: 6.021496095005623
Validation loss: 5.21643296743452

Epoch: 6| Step: 4
Training loss: 4.540467666872118
Validation loss: 5.212074534184137

Epoch: 6| Step: 5
Training loss: 5.446693628369714
Validation loss: 5.210053532528327

Epoch: 6| Step: 6
Training loss: 5.511134581020245
Validation loss: 5.202619679891116

Epoch: 6| Step: 7
Training loss: 5.574567071501424
Validation loss: 5.200777519002781

Epoch: 6| Step: 8
Training loss: 6.073471052755756
Validation loss: 5.1914180346008845

Epoch: 6| Step: 9
Training loss: 4.870094765871035
Validation loss: 5.193926192775763

Epoch: 6| Step: 10
Training loss: 4.6667084464973145
Validation loss: 5.186702114893767

Epoch: 6| Step: 11
Training loss: 5.062954352441106
Validation loss: 5.183121309011829

Epoch: 6| Step: 12
Training loss: 4.80805903048465
Validation loss: 5.1803739735605365

Epoch: 6| Step: 13
Training loss: 5.326288796216202
Validation loss: 5.174500859598624

Epoch: 6| Step: 0
Training loss: 6.483203019088539
Validation loss: 5.171811868686092

Epoch: 6| Step: 1
Training loss: 5.190536162037026
Validation loss: 5.164660342444014

Epoch: 6| Step: 2
Training loss: 4.180910077839452
Validation loss: 5.159905918955583

Epoch: 6| Step: 3
Training loss: 4.746132531027257
Validation loss: 5.155130564518163

Epoch: 6| Step: 4
Training loss: 5.270233220652881
Validation loss: 5.151836518291344

Epoch: 6| Step: 5
Training loss: 4.884269313925782
Validation loss: 5.1523356217250855

Epoch: 6| Step: 6
Training loss: 4.342030575975379
Validation loss: 5.1380379121983015

Epoch: 6| Step: 7
Training loss: 4.91215732922666
Validation loss: 5.1412502526588515

Epoch: 6| Step: 8
Training loss: 5.602124131483156
Validation loss: 5.132074321197384

Epoch: 6| Step: 9
Training loss: 4.359708164842318
Validation loss: 5.128943925921022

Epoch: 6| Step: 10
Training loss: 5.61940588936575
Validation loss: 5.12366755802641

Epoch: 6| Step: 11
Training loss: 6.036747931795812
Validation loss: 5.118972751717389

Epoch: 6| Step: 12
Training loss: 6.030728806819806
Validation loss: 5.118450597002386

Epoch: 6| Step: 13
Training loss: 3.962869808618011
Validation loss: 5.107214025728687

Epoch: 7| Step: 0
Training loss: 4.18545026574226
Validation loss: 5.105009714619747

Epoch: 6| Step: 1
Training loss: 5.380838815750197
Validation loss: 5.10152364093318

Epoch: 6| Step: 2
Training loss: 5.143480615062568
Validation loss: 5.096403697859907

Epoch: 6| Step: 3
Training loss: 4.544601238958439
Validation loss: 5.091453950251511

Epoch: 6| Step: 4
Training loss: 5.856309745101125
Validation loss: 5.08732059603157

Epoch: 6| Step: 5
Training loss: 5.8243648068121585
Validation loss: 5.0826300879807045

Epoch: 6| Step: 6
Training loss: 5.702915971661689
Validation loss: 5.078985441022183

Epoch: 6| Step: 7
Training loss: 5.379979687904104
Validation loss: 5.072529837185003

Epoch: 6| Step: 8
Training loss: 3.7055951985823863
Validation loss: 5.065388349665472

Epoch: 6| Step: 9
Training loss: 4.726441436982644
Validation loss: 5.056758732859855

Epoch: 6| Step: 10
Training loss: 5.144120068870767
Validation loss: 5.053073086427114

Epoch: 6| Step: 11
Training loss: 4.768766574655829
Validation loss: 5.050957910769166

Epoch: 6| Step: 12
Training loss: 5.724181217462595
Validation loss: 5.042477630557971

Epoch: 6| Step: 13
Training loss: 5.354344979766761
Validation loss: 5.036517123044141

Epoch: 8| Step: 0
Training loss: 4.534991804323708
Validation loss: 5.031515348980072

Epoch: 6| Step: 1
Training loss: 3.559175110948877
Validation loss: 5.021531738076263

Epoch: 6| Step: 2
Training loss: 5.726684402479804
Validation loss: 5.018793411198923

Epoch: 6| Step: 3
Training loss: 5.745966616221881
Validation loss: 5.010690435967056

Epoch: 6| Step: 4
Training loss: 4.888942246194067
Validation loss: 5.01034143811944

Epoch: 6| Step: 5
Training loss: 4.653343163801447
Validation loss: 5.002141887521635

Epoch: 6| Step: 6
Training loss: 4.504869581360637
Validation loss: 4.993878752235186

Epoch: 6| Step: 7
Training loss: 5.446635322309409
Validation loss: 4.986699636700329

Epoch: 6| Step: 8
Training loss: 5.490144743131743
Validation loss: 4.982608539369339

Epoch: 6| Step: 9
Training loss: 5.102609243686018
Validation loss: 4.971041247517721

Epoch: 6| Step: 10
Training loss: 5.3383994280466815
Validation loss: 4.9702175555718

Epoch: 6| Step: 11
Training loss: 4.938600827887088
Validation loss: 4.9676748464198885

Epoch: 6| Step: 12
Training loss: 5.722294277161355
Validation loss: 4.954945989436732

Epoch: 6| Step: 13
Training loss: 4.355855197612
Validation loss: 4.945384459354403

Epoch: 9| Step: 0
Training loss: 6.003706740748273
Validation loss: 4.940839870227595

Epoch: 6| Step: 1
Training loss: 3.666787636813227
Validation loss: 4.93920739942518

Epoch: 6| Step: 2
Training loss: 4.645791821942954
Validation loss: 4.930112941914002

Epoch: 6| Step: 3
Training loss: 4.6364604103473965
Validation loss: 4.919984059511819

Epoch: 6| Step: 4
Training loss: 5.500864307809689
Validation loss: 4.915712860901199

Epoch: 6| Step: 5
Training loss: 4.566252954682053
Validation loss: 4.901500891932283

Epoch: 6| Step: 6
Training loss: 5.110257973625875
Validation loss: 4.900391018408981

Epoch: 6| Step: 7
Training loss: 5.015677858828747
Validation loss: 4.886288218169975

Epoch: 6| Step: 8
Training loss: 4.834945059210698
Validation loss: 4.887612131993171

Epoch: 6| Step: 9
Training loss: 5.178107157223521
Validation loss: 4.878127208604976

Epoch: 6| Step: 10
Training loss: 4.937013626286905
Validation loss: 4.868764829227997

Epoch: 6| Step: 11
Training loss: 4.630931041199989
Validation loss: 4.860186358191171

Epoch: 6| Step: 12
Training loss: 5.242288466275722
Validation loss: 4.851190978000749

Epoch: 6| Step: 13
Training loss: 5.250533848822597
Validation loss: 4.843823603647412

Epoch: 10| Step: 0
Training loss: 4.566720134301442
Validation loss: 4.837033665435505

Epoch: 6| Step: 1
Training loss: 5.484926896977347
Validation loss: 4.821297939249935

Epoch: 6| Step: 2
Training loss: 5.1010943865439
Validation loss: 4.815706545060225

Epoch: 6| Step: 3
Training loss: 4.748602611386244
Validation loss: 4.806293490797805

Epoch: 6| Step: 4
Training loss: 4.808390063783633
Validation loss: 4.801161954755149

Epoch: 6| Step: 5
Training loss: 3.8123589161267457
Validation loss: 4.798012221374774

Epoch: 6| Step: 6
Training loss: 3.9355940065803723
Validation loss: 4.781209022324389

Epoch: 6| Step: 7
Training loss: 5.264676243463251
Validation loss: 4.782408461687061

Epoch: 6| Step: 8
Training loss: 5.5936690170137355
Validation loss: 4.77225284797625

Epoch: 6| Step: 9
Training loss: 5.721944614989029
Validation loss: 4.755214717260347

Epoch: 6| Step: 10
Training loss: 4.168003300217038
Validation loss: 4.749189144628383

Epoch: 6| Step: 11
Training loss: 5.284154330382932
Validation loss: 4.743256445678436

Epoch: 6| Step: 12
Training loss: 4.138779023580538
Validation loss: 4.722221025293445

Epoch: 6| Step: 13
Training loss: 4.823235365991582
Validation loss: 4.716482470487208

Epoch: 11| Step: 0
Training loss: 4.7352721116568945
Validation loss: 4.701554673694996

Epoch: 6| Step: 1
Training loss: 4.663619590700723
Validation loss: 4.694302066320937

Epoch: 6| Step: 2
Training loss: 4.1338219374357195
Validation loss: 4.686260441784943

Epoch: 6| Step: 3
Training loss: 5.048775235457985
Validation loss: 4.679661787307125

Epoch: 6| Step: 4
Training loss: 4.4810687380939
Validation loss: 4.671962086283646

Epoch: 6| Step: 5
Training loss: 3.9717880995853507
Validation loss: 4.659011989563843

Epoch: 6| Step: 6
Training loss: 3.323093533168753
Validation loss: 4.642627389986664

Epoch: 6| Step: 7
Training loss: 4.732249774393369
Validation loss: 4.6346161616971004

Epoch: 6| Step: 8
Training loss: 5.638076671532336
Validation loss: 4.628686082888867

Epoch: 6| Step: 9
Training loss: 4.915526349435245
Validation loss: 4.621317032564772

Epoch: 6| Step: 10
Training loss: 5.7238076779986935
Validation loss: 4.609659014406282

Epoch: 6| Step: 11
Training loss: 4.424149322638637
Validation loss: 4.597954414363976

Epoch: 6| Step: 12
Training loss: 4.278776470596139
Validation loss: 4.5875866899896796

Epoch: 6| Step: 13
Training loss: 5.860560752416417
Validation loss: 4.573974219488141

Epoch: 12| Step: 0
Training loss: 4.7685025892159665
Validation loss: 4.560900387671671

Epoch: 6| Step: 1
Training loss: 4.627127905362882
Validation loss: 4.54639535379933

Epoch: 6| Step: 2
Training loss: 5.351175425098282
Validation loss: 4.538561856013493

Epoch: 6| Step: 3
Training loss: 3.6126741466323935
Validation loss: 4.521769705317083

Epoch: 6| Step: 4
Training loss: 5.463600917188206
Validation loss: 4.505634390094928

Epoch: 6| Step: 5
Training loss: 5.092153287235962
Validation loss: 4.4940909073970605

Epoch: 6| Step: 6
Training loss: 3.5925934630615415
Validation loss: 4.481728410966639

Epoch: 6| Step: 7
Training loss: 5.112641110140017
Validation loss: 4.473368301668166

Epoch: 6| Step: 8
Training loss: 2.8981415219604156
Validation loss: 4.455635343923255

Epoch: 6| Step: 9
Training loss: 5.2952966729786155
Validation loss: 4.441211623121469

Epoch: 6| Step: 10
Training loss: 5.246803763964193
Validation loss: 4.428725199635679

Epoch: 6| Step: 11
Training loss: 4.523661395446646
Validation loss: 4.415999669615668

Epoch: 6| Step: 12
Training loss: 3.4691647118452877
Validation loss: 4.398280841591395

Epoch: 6| Step: 13
Training loss: 3.392668921046808
Validation loss: 4.383326427227038

Epoch: 13| Step: 0
Training loss: 4.051705912425191
Validation loss: 4.382676150583337

Epoch: 6| Step: 1
Training loss: 4.741671639402404
Validation loss: 4.362261877128998

Epoch: 6| Step: 2
Training loss: 3.125923020423609
Validation loss: 4.34235613321091

Epoch: 6| Step: 3
Training loss: 5.134163456631865
Validation loss: 4.34246206736185

Epoch: 6| Step: 4
Training loss: 3.9773346576575506
Validation loss: 4.3187915408045345

Epoch: 6| Step: 5
Training loss: 4.659027704795577
Validation loss: 4.311940051454185

Epoch: 6| Step: 6
Training loss: 5.237599578129308
Validation loss: 4.293052091706471

Epoch: 6| Step: 7
Training loss: 4.774836208869703
Validation loss: 4.283572863895977

Epoch: 6| Step: 8
Training loss: 3.7952626262354507
Validation loss: 4.26586873219373

Epoch: 6| Step: 9
Training loss: 4.357590395525465
Validation loss: 4.253502904789534

Epoch: 6| Step: 10
Training loss: 4.11759075238494
Validation loss: 4.239424577840534

Epoch: 6| Step: 11
Training loss: 3.6261640850042225
Validation loss: 4.217660239514362

Epoch: 6| Step: 12
Training loss: 4.530073973533818
Validation loss: 4.207719489844562

Epoch: 6| Step: 13
Training loss: 4.885348950576252
Validation loss: 4.184741789739628

Epoch: 14| Step: 0
Training loss: 3.583963885755465
Validation loss: 4.1689856620410195

Epoch: 6| Step: 1
Training loss: 4.3358618377407465
Validation loss: 4.156404203554664

Epoch: 6| Step: 2
Training loss: 4.7124834702907625
Validation loss: 4.14856439353934

Epoch: 6| Step: 3
Training loss: 3.5930344905923097
Validation loss: 4.1119436141069965

Epoch: 6| Step: 4
Training loss: 4.244311621109477
Validation loss: 4.111655002166451

Epoch: 6| Step: 5
Training loss: 4.594987559836134
Validation loss: 4.0805550885152355

Epoch: 6| Step: 6
Training loss: 3.795877710300048
Validation loss: 4.076208232219832

Epoch: 6| Step: 7
Training loss: 4.535578482174986
Validation loss: 4.056517778674617

Epoch: 6| Step: 8
Training loss: 3.49018478144851
Validation loss: 4.030704503037681

Epoch: 6| Step: 9
Training loss: 4.411459582380037
Validation loss: 4.020997109707425

Epoch: 6| Step: 10
Training loss: 4.4091040924790414
Validation loss: 4.004906739161008

Epoch: 6| Step: 11
Training loss: 4.123734482351236
Validation loss: 3.982443020485504

Epoch: 6| Step: 12
Training loss: 4.416570818358866
Validation loss: 3.964343958928429

Epoch: 6| Step: 13
Training loss: 3.8514204946961232
Validation loss: 3.9365843271205425

Epoch: 15| Step: 0
Training loss: 4.169497837778343
Validation loss: 3.9321276802151774

Epoch: 6| Step: 1
Training loss: 4.703390018942517
Validation loss: 3.903551386123713

Epoch: 6| Step: 2
Training loss: 4.13208162879633
Validation loss: 3.8787558722798163

Epoch: 6| Step: 3
Training loss: 3.9973177261259925
Validation loss: 3.8684438551280613

Epoch: 6| Step: 4
Training loss: 4.106310263645849
Validation loss: 3.85552035136145

Epoch: 6| Step: 5
Training loss: 4.303321247709301
Validation loss: 3.842642651872506

Epoch: 6| Step: 6
Training loss: 4.172690551136865
Validation loss: 3.831739010291543

Epoch: 6| Step: 7
Training loss: 4.021212597816228
Validation loss: 3.808273768965001

Epoch: 6| Step: 8
Training loss: 3.325470774887939
Validation loss: 3.7801641337164837

Epoch: 6| Step: 9
Training loss: 3.8942382457404174
Validation loss: 3.760084313708631

Epoch: 6| Step: 10
Training loss: 2.5839521733336954
Validation loss: 3.748681853321288

Epoch: 6| Step: 11
Training loss: 3.874092857252362
Validation loss: 3.7240961062108053

Epoch: 6| Step: 12
Training loss: 3.844177346392856
Validation loss: 3.7002091761879092

Epoch: 6| Step: 13
Training loss: 3.5758974562755363
Validation loss: 3.6762783279407296

Epoch: 16| Step: 0
Training loss: 4.586952837917594
Validation loss: 3.6638304339559244

Epoch: 6| Step: 1
Training loss: 4.645162605256404
Validation loss: 3.6460616815782996

Epoch: 6| Step: 2
Training loss: 3.941043530835263
Validation loss: 3.6191956539400034

Epoch: 6| Step: 3
Training loss: 3.732898116856403
Validation loss: 3.5924797848660868

Epoch: 6| Step: 4
Training loss: 3.5724562338089556
Validation loss: 3.5739281076974225

Epoch: 6| Step: 5
Training loss: 3.343814029258814
Validation loss: 3.5616314283789023

Epoch: 6| Step: 6
Training loss: 3.986047969470136
Validation loss: 3.5319655275438144

Epoch: 6| Step: 7
Training loss: 3.0402998967065837
Validation loss: 3.521683847175332

Epoch: 6| Step: 8
Training loss: 2.8425164534093432
Validation loss: 3.490026019758305

Epoch: 6| Step: 9
Training loss: 2.864748382437406
Validation loss: 3.472448732241315

Epoch: 6| Step: 10
Training loss: 2.875990821271954
Validation loss: 3.446418842590284

Epoch: 6| Step: 11
Training loss: 3.8083037750187767
Validation loss: 3.4310871032030095

Epoch: 6| Step: 12
Training loss: 3.7863462799716916
Validation loss: 3.4144585860418566

Epoch: 6| Step: 13
Training loss: 4.603337669250818
Validation loss: 3.401788596204911

Epoch: 17| Step: 0
Training loss: 3.7632459034209096
Validation loss: 3.3745639949327173

Epoch: 6| Step: 1
Training loss: 3.7727258615397123
Validation loss: 3.3495844273920876

Epoch: 6| Step: 2
Training loss: 3.977691190429556
Validation loss: 3.349774508539136

Epoch: 6| Step: 3
Training loss: 3.38620832528133
Validation loss: 3.3182884305826335

Epoch: 6| Step: 4
Training loss: 3.941826638220988
Validation loss: 3.2936461278238855

Epoch: 6| Step: 5
Training loss: 2.459723860586166
Validation loss: 3.2804056357649953

Epoch: 6| Step: 6
Training loss: 2.9465268007228325
Validation loss: 3.250219399214608

Epoch: 6| Step: 7
Training loss: 3.7060521142118867
Validation loss: 3.2266054725382514

Epoch: 6| Step: 8
Training loss: 3.5571853078168347
Validation loss: 3.223962975085061

Epoch: 6| Step: 9
Training loss: 3.158928820217184
Validation loss: 3.1929410708330606

Epoch: 6| Step: 10
Training loss: 3.6453358047335587
Validation loss: 3.173628870916594

Epoch: 6| Step: 11
Training loss: 3.067851159873266
Validation loss: 3.1423444820720894

Epoch: 6| Step: 12
Training loss: 2.331399240906994
Validation loss: 3.1430267323986647

Epoch: 6| Step: 13
Training loss: 4.628150691567048
Validation loss: 3.127562907157861

Epoch: 18| Step: 0
Training loss: 3.6243911758064513
Validation loss: 3.1137583204409776

Epoch: 6| Step: 1
Training loss: 2.9085854611418287
Validation loss: 3.0834171053196804

Epoch: 6| Step: 2
Training loss: 3.550046066200206
Validation loss: 3.0792120190629295

Epoch: 6| Step: 3
Training loss: 3.021753122895711
Validation loss: 3.0419090439639977

Epoch: 6| Step: 4
Training loss: 3.489619255829951
Validation loss: 3.0340149763382565

Epoch: 6| Step: 5
Training loss: 3.7265935932517698
Validation loss: 3.018761442762281

Epoch: 6| Step: 6
Training loss: 3.00287680974786
Validation loss: 3.0224703965720017

Epoch: 6| Step: 7
Training loss: 3.547731943001344
Validation loss: 2.988965483475423

Epoch: 6| Step: 8
Training loss: 3.605460814759686
Validation loss: 2.97801801855669

Epoch: 6| Step: 9
Training loss: 2.838958224374507
Validation loss: 2.9523920235899537

Epoch: 6| Step: 10
Training loss: 2.7544329200507986
Validation loss: 2.9251710608740797

Epoch: 6| Step: 11
Training loss: 2.863691396858759
Validation loss: 2.908701975209211

Epoch: 6| Step: 12
Training loss: 2.937625314190403
Validation loss: 2.9058893504598946

Epoch: 6| Step: 13
Training loss: 3.3321925277407294
Validation loss: 2.876291496780156

Epoch: 19| Step: 0
Training loss: 3.3452254988267214
Validation loss: 2.8652859929971117

Epoch: 6| Step: 1
Training loss: 2.9434658099638713
Validation loss: 2.8563475971390675

Epoch: 6| Step: 2
Training loss: 3.390832288180497
Validation loss: 2.8410818480526476

Epoch: 6| Step: 3
Training loss: 3.3400754613974803
Validation loss: 2.8372215402155154

Epoch: 6| Step: 4
Training loss: 2.931480896402308
Validation loss: 2.8260811256394223

Epoch: 6| Step: 5
Training loss: 3.6324143714552433
Validation loss: 2.8252853453447284

Epoch: 6| Step: 6
Training loss: 3.3711260001716346
Validation loss: 2.808511541605844

Epoch: 6| Step: 7
Training loss: 2.6852155335368986
Validation loss: 2.7884051295443872

Epoch: 6| Step: 8
Training loss: 2.321342573564176
Validation loss: 2.7668290809874767

Epoch: 6| Step: 9
Training loss: 3.0492297341190198
Validation loss: 2.765221395462483

Epoch: 6| Step: 10
Training loss: 3.0898837196668114
Validation loss: 2.7496238518366973

Epoch: 6| Step: 11
Training loss: 3.179884246062956
Validation loss: 2.739265259356851

Epoch: 6| Step: 12
Training loss: 2.7913951362155873
Validation loss: 2.726761001359912

Epoch: 6| Step: 13
Training loss: 2.856570843929686
Validation loss: 2.716417988333744

Epoch: 20| Step: 0
Training loss: 3.129270458574214
Validation loss: 2.705070219157564

Epoch: 6| Step: 1
Training loss: 2.662146631709212
Validation loss: 2.6963535623252364

Epoch: 6| Step: 2
Training loss: 2.6395752755884887
Validation loss: 2.7143858499766234

Epoch: 6| Step: 3
Training loss: 3.322001523526811
Validation loss: 2.6849152196580497

Epoch: 6| Step: 4
Training loss: 3.162045134751258
Validation loss: 2.6859938246874586

Epoch: 6| Step: 5
Training loss: 3.119880755664458
Validation loss: 2.6701981898810603

Epoch: 6| Step: 6
Training loss: 2.978796413506656
Validation loss: 2.6773193097044916

Epoch: 6| Step: 7
Training loss: 2.5656452879759857
Validation loss: 2.6382895657138006

Epoch: 6| Step: 8
Training loss: 2.6417887453898645
Validation loss: 2.6472057571283303

Epoch: 6| Step: 9
Training loss: 3.6055060453955403
Validation loss: 2.6493875910366573

Epoch: 6| Step: 10
Training loss: 2.8739077939803117
Validation loss: 2.6212128491323847

Epoch: 6| Step: 11
Training loss: 2.6861385179252557
Validation loss: 2.6509092983552

Epoch: 6| Step: 12
Training loss: 3.3247735426803766
Validation loss: 2.627454660851335

Epoch: 6| Step: 13
Training loss: 2.8391627947444356
Validation loss: 2.619689323562632

Epoch: 21| Step: 0
Training loss: 3.365144255721648
Validation loss: 2.6214094162476966

Epoch: 6| Step: 1
Training loss: 2.486866787007459
Validation loss: 2.586025570507725

Epoch: 6| Step: 2
Training loss: 2.5549371346580023
Validation loss: 2.6105193999169374

Epoch: 6| Step: 3
Training loss: 2.886100238203957
Validation loss: 2.615078994355303

Epoch: 6| Step: 4
Training loss: 3.3245047637864014
Validation loss: 2.6251960618670998

Epoch: 6| Step: 5
Training loss: 2.496336732598295
Validation loss: 2.594953555996478

Epoch: 6| Step: 6
Training loss: 2.7335228164248124
Validation loss: 2.5991260111885635

Epoch: 6| Step: 7
Training loss: 2.3871435608348737
Validation loss: 2.5898856078270884

Epoch: 6| Step: 8
Training loss: 3.2246025727706873
Validation loss: 2.5726396453800784

Epoch: 6| Step: 9
Training loss: 3.0682835366184764
Validation loss: 2.5763707685801234

Epoch: 6| Step: 10
Training loss: 3.1896253493764384
Validation loss: 2.5609732050392098

Epoch: 6| Step: 11
Training loss: 2.7465301644408258
Validation loss: 2.607585019663501

Epoch: 6| Step: 12
Training loss: 3.336580951464944
Validation loss: 2.556581190024071

Epoch: 6| Step: 13
Training loss: 3.2061895829544613
Validation loss: 2.54410194706941

Epoch: 22| Step: 0
Training loss: 3.1710966310866286
Validation loss: 2.555116424072084

Epoch: 6| Step: 1
Training loss: 3.2050245653660143
Validation loss: 2.538377194239535

Epoch: 6| Step: 2
Training loss: 3.307819442489467
Validation loss: 2.5507443225033555

Epoch: 6| Step: 3
Training loss: 3.143863052549916
Validation loss: 2.568240604861834

Epoch: 6| Step: 4
Training loss: 3.149222832130355
Validation loss: 2.5260162206767194

Epoch: 6| Step: 5
Training loss: 2.185932687859082
Validation loss: 2.5523199062597883

Epoch: 6| Step: 6
Training loss: 3.144051423989349
Validation loss: 2.5547340120143938

Epoch: 6| Step: 7
Training loss: 2.7493411922262347
Validation loss: 2.5450466913243934

Epoch: 6| Step: 8
Training loss: 2.819360461391002
Validation loss: 2.5565013950524333

Epoch: 6| Step: 9
Training loss: 2.8379302786716876
Validation loss: 2.5432048670535727

Epoch: 6| Step: 10
Training loss: 2.8891677069811124
Validation loss: 2.5275226334510186

Epoch: 6| Step: 11
Training loss: 3.4405163102475043
Validation loss: 2.538728813160978

Epoch: 6| Step: 12
Training loss: 1.7913584407078034
Validation loss: 2.5734292002637216

Epoch: 6| Step: 13
Training loss: 3.015968421790957
Validation loss: 2.543567147331645

Epoch: 23| Step: 0
Training loss: 3.1396938480872296
Validation loss: 2.5249172880242297

Epoch: 6| Step: 1
Training loss: 3.5189467271792543
Validation loss: 2.5393695957954705

Epoch: 6| Step: 2
Training loss: 2.335989756940567
Validation loss: 2.5223784471320516

Epoch: 6| Step: 3
Training loss: 3.0048161630364376
Validation loss: 2.5505272221946287

Epoch: 6| Step: 4
Training loss: 2.3811305379062704
Validation loss: 2.5448484901202835

Epoch: 6| Step: 5
Training loss: 2.4637122602586685
Validation loss: 2.5332083206818603

Epoch: 6| Step: 6
Training loss: 3.1241034175249567
Validation loss: 2.560858587218825

Epoch: 6| Step: 7
Training loss: 2.901294051217639
Validation loss: 2.545412082446785

Epoch: 6| Step: 8
Training loss: 2.7099241621941657
Validation loss: 2.523623907473922

Epoch: 6| Step: 9
Training loss: 3.2902128332801284
Validation loss: 2.536397819321911

Epoch: 6| Step: 10
Training loss: 2.7999870844951844
Validation loss: 2.531031487868934

Epoch: 6| Step: 11
Training loss: 3.151661474047787
Validation loss: 2.550794994908494

Epoch: 6| Step: 12
Training loss: 3.089937577589301
Validation loss: 2.517938385712973

Epoch: 6| Step: 13
Training loss: 2.66236245959711
Validation loss: 2.5312779054226464

Epoch: 24| Step: 0
Training loss: 3.178926493987366
Validation loss: 2.5391610155958415

Epoch: 6| Step: 1
Training loss: 3.1115218375823677
Validation loss: 2.5352286807484767

Epoch: 6| Step: 2
Training loss: 3.210955171930917
Validation loss: 2.5254572065448215

Epoch: 6| Step: 3
Training loss: 3.511880873013114
Validation loss: 2.5367964554808

Epoch: 6| Step: 4
Training loss: 3.4811064438866897
Validation loss: 2.5218588860687174

Epoch: 6| Step: 5
Training loss: 2.7816874127979063
Validation loss: 2.5541662586521445

Epoch: 6| Step: 6
Training loss: 2.931432423069887
Validation loss: 2.552821402757017

Epoch: 6| Step: 7
Training loss: 2.793263495838486
Validation loss: 2.516927952504175

Epoch: 6| Step: 8
Training loss: 2.9292286424515073
Validation loss: 2.513507659971589

Epoch: 6| Step: 9
Training loss: 2.4861140851998136
Validation loss: 2.55133134328498

Epoch: 6| Step: 10
Training loss: 2.680271807879068
Validation loss: 2.528633738538968

Epoch: 6| Step: 11
Training loss: 3.1144017365418204
Validation loss: 2.5318824768377226

Epoch: 6| Step: 12
Training loss: 2.320215499340289
Validation loss: 2.5119013803407237

Epoch: 6| Step: 13
Training loss: 1.2454053358368937
Validation loss: 2.5159718957034944

Epoch: 25| Step: 0
Training loss: 3.0866353851392065
Validation loss: 2.519166961252856

Epoch: 6| Step: 1
Training loss: 2.731308967425163
Validation loss: 2.519524423536417

Epoch: 6| Step: 2
Training loss: 3.3012388736027476
Validation loss: 2.538302989917837

Epoch: 6| Step: 3
Training loss: 1.3951508670839325
Validation loss: 2.526979012984658

Epoch: 6| Step: 4
Training loss: 2.8044770143205913
Validation loss: 2.539052149749182

Epoch: 6| Step: 5
Training loss: 2.9576936128626237
Validation loss: 2.5344295011469304

Epoch: 6| Step: 6
Training loss: 2.7967605141165617
Validation loss: 2.5565134616270986

Epoch: 6| Step: 7
Training loss: 2.6528540051633414
Validation loss: 2.534305484047409

Epoch: 6| Step: 8
Training loss: 2.2359413045641547
Validation loss: 2.5439433856964926

Epoch: 6| Step: 9
Training loss: 2.9432804778016486
Validation loss: 2.5234587349190747

Epoch: 6| Step: 10
Training loss: 3.365682668377806
Validation loss: 2.532436725154905

Epoch: 6| Step: 11
Training loss: 3.0773167202477403
Validation loss: 2.5146922616219243

Epoch: 6| Step: 12
Training loss: 3.4367239336138162
Validation loss: 2.5225381119903765

Epoch: 6| Step: 13
Training loss: 3.9835189797356385
Validation loss: 2.529877655629539

Epoch: 26| Step: 0
Training loss: 3.0278962449473092
Validation loss: 2.4995270445764275

Epoch: 6| Step: 1
Training loss: 3.4569573173591497
Validation loss: 2.53612850527998

Epoch: 6| Step: 2
Training loss: 3.0035054072100715
Validation loss: 2.5294718592004695

Epoch: 6| Step: 3
Training loss: 2.5530955169357474
Validation loss: 2.4982376593427533

Epoch: 6| Step: 4
Training loss: 2.531919131905487
Validation loss: 2.5436659440129774

Epoch: 6| Step: 5
Training loss: 2.2546019645639723
Validation loss: 2.5094327602237216

Epoch: 6| Step: 6
Training loss: 3.451338500062925
Validation loss: 2.525943006122325

Epoch: 6| Step: 7
Training loss: 2.814797543822205
Validation loss: 2.519385021863405

Epoch: 6| Step: 8
Training loss: 3.2313498387509063
Validation loss: 2.5167212599801174

Epoch: 6| Step: 9
Training loss: 2.1977062275258956
Validation loss: 2.524318827529376

Epoch: 6| Step: 10
Training loss: 3.103720579587864
Validation loss: 2.526606241016596

Epoch: 6| Step: 11
Training loss: 2.564304553781289
Validation loss: 2.537743939609443

Epoch: 6| Step: 12
Training loss: 3.0294105218245364
Validation loss: 2.5141954486814933

Epoch: 6| Step: 13
Training loss: 3.4944561239619136
Validation loss: 2.5249335343710033

Epoch: 27| Step: 0
Training loss: 2.475508312084256
Validation loss: 2.5309575077852573

Epoch: 6| Step: 1
Training loss: 3.096421908617983
Validation loss: 2.5343718152014625

Epoch: 6| Step: 2
Training loss: 2.5495178926284794
Validation loss: 2.54747305840906

Epoch: 6| Step: 3
Training loss: 3.226762003422843
Validation loss: 2.514506906886634

Epoch: 6| Step: 4
Training loss: 2.7137447190903248
Validation loss: 2.5307036447885616

Epoch: 6| Step: 5
Training loss: 3.1574502305845082
Validation loss: 2.530996362842925

Epoch: 6| Step: 6
Training loss: 2.6497827224906407
Validation loss: 2.5151660683750237

Epoch: 6| Step: 7
Training loss: 2.525502213493167
Validation loss: 2.534155291023513

Epoch: 6| Step: 8
Training loss: 2.8218120276436287
Validation loss: 2.535298471262984

Epoch: 6| Step: 9
Training loss: 2.684514538798588
Validation loss: 2.541939335652216

Epoch: 6| Step: 10
Training loss: 3.393874338587545
Validation loss: 2.5195553291876407

Epoch: 6| Step: 11
Training loss: 2.99639198778431
Validation loss: 2.4967372142634354

Epoch: 6| Step: 12
Training loss: 3.039594981167085
Validation loss: 2.517948323861977

Epoch: 6| Step: 13
Training loss: 3.409936432455695
Validation loss: 2.53808613345328

Epoch: 28| Step: 0
Training loss: 2.7181549297973415
Validation loss: 2.53320581847762

Epoch: 6| Step: 1
Training loss: 3.170446215009885
Validation loss: 2.5179468394037325

Epoch: 6| Step: 2
Training loss: 3.0701393292151185
Validation loss: 2.5356047550676957

Epoch: 6| Step: 3
Training loss: 2.964071341558209
Validation loss: 2.51640724774295

Epoch: 6| Step: 4
Training loss: 2.7744997381229797
Validation loss: 2.517148117731303

Epoch: 6| Step: 5
Training loss: 3.480541635214682
Validation loss: 2.5198252638724785

Epoch: 6| Step: 6
Training loss: 2.675958914323054
Validation loss: 2.504710522887856

Epoch: 6| Step: 7
Training loss: 1.6710251403962855
Validation loss: 2.5268055819488855

Epoch: 6| Step: 8
Training loss: 2.8606982895220385
Validation loss: 2.5130192555116504

Epoch: 6| Step: 9
Training loss: 3.0864161639251564
Validation loss: 2.5204207491961697

Epoch: 6| Step: 10
Training loss: 3.2386702613334237
Validation loss: 2.5334082056724414

Epoch: 6| Step: 11
Training loss: 2.289490331045787
Validation loss: 2.535768932246324

Epoch: 6| Step: 12
Training loss: 2.767511274113215
Validation loss: 2.5188526096531025

Epoch: 6| Step: 13
Training loss: 3.8294899511727163
Validation loss: 2.5041774702498563

Epoch: 29| Step: 0
Training loss: 2.3355136856857404
Validation loss: 2.5470744596622032

Epoch: 6| Step: 1
Training loss: 2.3101220147412005
Validation loss: 2.491730750918348

Epoch: 6| Step: 2
Training loss: 2.613535231492011
Validation loss: 2.533392241359227

Epoch: 6| Step: 3
Training loss: 2.9780093703983983
Validation loss: 2.5299386871846608

Epoch: 6| Step: 4
Training loss: 2.8907962232861433
Validation loss: 2.521982233393641

Epoch: 6| Step: 5
Training loss: 2.5829832598667677
Validation loss: 2.527753790702118

Epoch: 6| Step: 6
Training loss: 2.7565551139128015
Validation loss: 2.5018528368586006

Epoch: 6| Step: 7
Training loss: 2.559964390894472
Validation loss: 2.5078214958867058

Epoch: 6| Step: 8
Training loss: 3.214317751528011
Validation loss: 2.5103451839443243

Epoch: 6| Step: 9
Training loss: 3.0358885835249536
Validation loss: 2.5172628763503937

Epoch: 6| Step: 10
Training loss: 3.3348386862201225
Validation loss: 2.50971519777387

Epoch: 6| Step: 11
Training loss: 3.4730606101140644
Validation loss: 2.5087685464573153

Epoch: 6| Step: 12
Training loss: 3.2027738052994934
Validation loss: 2.5073825734853306

Epoch: 6| Step: 13
Training loss: 3.2266841246655447
Validation loss: 2.5183246068182368

Epoch: 30| Step: 0
Training loss: 3.051652341927437
Validation loss: 2.5070208214595295

Epoch: 6| Step: 1
Training loss: 3.38236281156929
Validation loss: 2.5155339714675513

Epoch: 6| Step: 2
Training loss: 3.150849509356998
Validation loss: 2.506030281318349

Epoch: 6| Step: 3
Training loss: 1.9472994174913894
Validation loss: 2.496955062192704

Epoch: 6| Step: 4
Training loss: 3.2360019001430596
Validation loss: 2.510589486094318

Epoch: 6| Step: 5
Training loss: 2.3427577652273834
Validation loss: 2.526477089031681

Epoch: 6| Step: 6
Training loss: 2.7082529104958923
Validation loss: 2.537650686918436

Epoch: 6| Step: 7
Training loss: 3.4633122341870544
Validation loss: 2.5198302165136512

Epoch: 6| Step: 8
Training loss: 2.634055367805328
Validation loss: 2.537258504006968

Epoch: 6| Step: 9
Training loss: 2.9473605940112537
Validation loss: 2.5101814559801006

Epoch: 6| Step: 10
Training loss: 2.61701586715675
Validation loss: 2.522943804303414

Epoch: 6| Step: 11
Training loss: 2.7129430031165747
Validation loss: 2.533973279179249

Epoch: 6| Step: 12
Training loss: 2.562995955691382
Validation loss: 2.5382431699919357

Epoch: 6| Step: 13
Training loss: 3.939413665286386
Validation loss: 2.515856648182389

Epoch: 31| Step: 0
Training loss: 2.939953043556211
Validation loss: 2.5345836752314503

Epoch: 6| Step: 1
Training loss: 2.878059417938343
Validation loss: 2.520876749251791

Epoch: 6| Step: 2
Training loss: 2.865950897222087
Validation loss: 2.508099692065382

Epoch: 6| Step: 3
Training loss: 2.363722276126624
Validation loss: 2.557159310798228

Epoch: 6| Step: 4
Training loss: 3.667322331385038
Validation loss: 2.526062126994431

Epoch: 6| Step: 5
Training loss: 3.095014833775286
Validation loss: 2.507465882310354

Epoch: 6| Step: 6
Training loss: 3.1138331978187095
Validation loss: 2.5081516747362116

Epoch: 6| Step: 7
Training loss: 2.1377255370964594
Validation loss: 2.5484062104931673

Epoch: 6| Step: 8
Training loss: 2.81909390062343
Validation loss: 2.5359280765108587

Epoch: 6| Step: 9
Training loss: 2.5731418423568204
Validation loss: 2.5026967311515524

Epoch: 6| Step: 10
Training loss: 3.27913808523387
Validation loss: 2.508037637919292

Epoch: 6| Step: 11
Training loss: 2.562427054506717
Validation loss: 2.4924087562497372

Epoch: 6| Step: 12
Training loss: 2.773230574504481
Validation loss: 2.495951582216709

Epoch: 6| Step: 13
Training loss: 3.6016893343820433
Validation loss: 2.495421664780031

Epoch: 32| Step: 0
Training loss: 2.1350832298904816
Validation loss: 2.525219437128529

Epoch: 6| Step: 1
Training loss: 3.6785553893481664
Validation loss: 2.522766182470333

Epoch: 6| Step: 2
Training loss: 2.547629496209619
Validation loss: 2.5368956461994694

Epoch: 6| Step: 3
Training loss: 2.6223847213151075
Validation loss: 2.5350191338476735

Epoch: 6| Step: 4
Training loss: 2.9861325186817176
Validation loss: 2.511308036851473

Epoch: 6| Step: 5
Training loss: 2.5935418838287205
Validation loss: 2.50592329078527

Epoch: 6| Step: 6
Training loss: 2.9305343828044106
Validation loss: 2.5032723882750703

Epoch: 6| Step: 7
Training loss: 3.131725398540363
Validation loss: 2.5511035685082173

Epoch: 6| Step: 8
Training loss: 2.7365501253850835
Validation loss: 2.4977571486388155

Epoch: 6| Step: 9
Training loss: 2.896231333508224
Validation loss: 2.5051968836723653

Epoch: 6| Step: 10
Training loss: 3.4542179363082566
Validation loss: 2.517339361877368

Epoch: 6| Step: 11
Training loss: 2.9660291551120346
Validation loss: 2.5223567721787323

Epoch: 6| Step: 12
Training loss: 2.858051431469405
Validation loss: 2.5422445057635663

Epoch: 6| Step: 13
Training loss: 2.5738863231096225
Validation loss: 2.5218916661021775

Epoch: 33| Step: 0
Training loss: 2.9492093723981707
Validation loss: 2.49716033453088

Epoch: 6| Step: 1
Training loss: 2.871362126062189
Validation loss: 2.4990659927627523

Epoch: 6| Step: 2
Training loss: 2.4997133090621495
Validation loss: 2.5166737051984023

Epoch: 6| Step: 3
Training loss: 2.9244190910783754
Validation loss: 2.5206709551568305

Epoch: 6| Step: 4
Training loss: 1.8552878201259804
Validation loss: 2.5206351151855633

Epoch: 6| Step: 5
Training loss: 3.5141256568426917
Validation loss: 2.521826884310131

Epoch: 6| Step: 6
Training loss: 2.674683817778365
Validation loss: 2.541954441479018

Epoch: 6| Step: 7
Training loss: 2.824462542425453
Validation loss: 2.5311806326308974

Epoch: 6| Step: 8
Training loss: 2.907921423247915
Validation loss: 2.521066560450571

Epoch: 6| Step: 9
Training loss: 3.3367220025391693
Validation loss: 2.5247007907285943

Epoch: 6| Step: 10
Training loss: 3.2195549115887667
Validation loss: 2.5272061450498664

Epoch: 6| Step: 11
Training loss: 2.479013187134858
Validation loss: 2.5179387502108836

Epoch: 6| Step: 12
Training loss: 2.888811026852526
Validation loss: 2.5271195732284095

Epoch: 6| Step: 13
Training loss: 2.981348230435043
Validation loss: 2.503583919233576

Epoch: 34| Step: 0
Training loss: 3.3442068768259188
Validation loss: 2.5303312249847134

Epoch: 6| Step: 1
Training loss: 3.2567751762978965
Validation loss: 2.5347748762224462

Epoch: 6| Step: 2
Training loss: 2.946828760490223
Validation loss: 2.5370724738422736

Epoch: 6| Step: 3
Training loss: 2.0101009641502787
Validation loss: 2.525183161713421

Epoch: 6| Step: 4
Training loss: 2.336687458814697
Validation loss: 2.506616442573646

Epoch: 6| Step: 5
Training loss: 2.5266646308521667
Validation loss: 2.527543645392891

Epoch: 6| Step: 6
Training loss: 3.428617062719101
Validation loss: 2.5174185946564154

Epoch: 6| Step: 7
Training loss: 2.386181360359599
Validation loss: 2.522258715232839

Epoch: 6| Step: 8
Training loss: 3.0356012948429907
Validation loss: 2.505892787866572

Epoch: 6| Step: 9
Training loss: 2.793187443634222
Validation loss: 2.5062831165866224

Epoch: 6| Step: 10
Training loss: 3.183498189229137
Validation loss: 2.52417382433061

Epoch: 6| Step: 11
Training loss: 3.0610061427506206
Validation loss: 2.4998643305072394

Epoch: 6| Step: 12
Training loss: 2.8412944546203662
Validation loss: 2.503584379004279

Epoch: 6| Step: 13
Training loss: 3.1102502252428255
Validation loss: 2.517500352941855

Epoch: 35| Step: 0
Training loss: 2.319192018561102
Validation loss: 2.5189590612023656

Epoch: 6| Step: 1
Training loss: 3.381206034094213
Validation loss: 2.507642752813408

Epoch: 6| Step: 2
Training loss: 3.2009511130215813
Validation loss: 2.5275055872377674

Epoch: 6| Step: 3
Training loss: 3.009500084611619
Validation loss: 2.5073516179926094

Epoch: 6| Step: 4
Training loss: 2.76056263855584
Validation loss: 2.507221945537492

Epoch: 6| Step: 5
Training loss: 2.684604593212828
Validation loss: 2.5279738091653776

Epoch: 6| Step: 6
Training loss: 2.0641569937503865
Validation loss: 2.5045422605386993

Epoch: 6| Step: 7
Training loss: 2.38607464729053
Validation loss: 2.5287233313770723

Epoch: 6| Step: 8
Training loss: 2.4645999842318003
Validation loss: 2.4985933484211666

Epoch: 6| Step: 9
Training loss: 3.476242359294265
Validation loss: 2.501906158438587

Epoch: 6| Step: 10
Training loss: 3.2685034810281546
Validation loss: 2.5103658933887223

Epoch: 6| Step: 11
Training loss: 2.510124400578221
Validation loss: 2.4871938342957463

Epoch: 6| Step: 12
Training loss: 2.9962119983082225
Validation loss: 2.548193557986732

Epoch: 6| Step: 13
Training loss: 3.7570525925716836
Validation loss: 2.514522847345647

Epoch: 36| Step: 0
Training loss: 2.82680501746238
Validation loss: 2.515379188291809

Epoch: 6| Step: 1
Training loss: 2.7001629073835365
Validation loss: 2.511631718865134

Epoch: 6| Step: 2
Training loss: 2.9560098433582325
Validation loss: 2.4984311626492244

Epoch: 6| Step: 3
Training loss: 2.899916443772023
Validation loss: 2.5463028131722485

Epoch: 6| Step: 4
Training loss: 2.711858763249433
Validation loss: 2.531052698578425

Epoch: 6| Step: 5
Training loss: 3.003821164675998
Validation loss: 2.5096893346194533

Epoch: 6| Step: 6
Training loss: 2.805096694721207
Validation loss: 2.529369225217317

Epoch: 6| Step: 7
Training loss: 3.402004711194956
Validation loss: 2.5242696492480072

Epoch: 6| Step: 8
Training loss: 2.7689470070355258
Validation loss: 2.4900319261674912

Epoch: 6| Step: 9
Training loss: 2.425206409597004
Validation loss: 2.5123019227759404

Epoch: 6| Step: 10
Training loss: 2.8658830133428324
Validation loss: 2.4920574540954172

Epoch: 6| Step: 11
Training loss: 3.2902139926873724
Validation loss: 2.5034489283369457

Epoch: 6| Step: 12
Training loss: 2.972008452843702
Validation loss: 2.4924043323252376

Epoch: 6| Step: 13
Training loss: 2.5133479457231447
Validation loss: 2.524089493687695

Epoch: 37| Step: 0
Training loss: 2.665339636425805
Validation loss: 2.5226845547051346

Epoch: 6| Step: 1
Training loss: 2.1384127803958597
Validation loss: 2.5196070734792912

Epoch: 6| Step: 2
Training loss: 2.480922579361879
Validation loss: 2.50475023800711

Epoch: 6| Step: 3
Training loss: 3.126726817817911
Validation loss: 2.5255603781291223

Epoch: 6| Step: 4
Training loss: 3.142014381196093
Validation loss: 2.467701200171404

Epoch: 6| Step: 5
Training loss: 2.8597061715878054
Validation loss: 2.5186433013933973

Epoch: 6| Step: 6
Training loss: 2.8777741026721926
Validation loss: 2.4992322460680105

Epoch: 6| Step: 7
Training loss: 3.5232741493540276
Validation loss: 2.5317346272767685

Epoch: 6| Step: 8
Training loss: 3.2743642583074877
Validation loss: 2.50768006556846

Epoch: 6| Step: 9
Training loss: 2.9393426513189636
Validation loss: 2.5270106248588116

Epoch: 6| Step: 10
Training loss: 2.2054970599258463
Validation loss: 2.499808860978323

Epoch: 6| Step: 11
Training loss: 2.0447544441067578
Validation loss: 2.5083234315899627

Epoch: 6| Step: 12
Training loss: 3.1743717983865865
Validation loss: 2.5566690153625706

Epoch: 6| Step: 13
Training loss: 3.6670323680804624
Validation loss: 2.497225524199824

Epoch: 38| Step: 0
Training loss: 3.4373487959465385
Validation loss: 2.503981238173304

Epoch: 6| Step: 1
Training loss: 3.4254537184415526
Validation loss: 2.515805860033261

Epoch: 6| Step: 2
Training loss: 2.557701362893891
Validation loss: 2.5053530296024458

Epoch: 6| Step: 3
Training loss: 2.546221502984519
Validation loss: 2.514733380385279

Epoch: 6| Step: 4
Training loss: 2.574689390837184
Validation loss: 2.4907784166440643

Epoch: 6| Step: 5
Training loss: 3.0644469008859674
Validation loss: 2.495668627613236

Epoch: 6| Step: 6
Training loss: 2.5446667605546565
Validation loss: 2.4939832281574725

Epoch: 6| Step: 7
Training loss: 3.031526218689129
Validation loss: 2.4969732009551056

Epoch: 6| Step: 8
Training loss: 2.6944031881558184
Validation loss: 2.5300657671444124

Epoch: 6| Step: 9
Training loss: 2.5030068435118173
Validation loss: 2.511316570538765

Epoch: 6| Step: 10
Training loss: 2.176446727294396
Validation loss: 2.5097245116721005

Epoch: 6| Step: 11
Training loss: 3.4350380143862047
Validation loss: 2.509710052540208

Epoch: 6| Step: 12
Training loss: 3.3611876809161925
Validation loss: 2.5214017093207106

Epoch: 6| Step: 13
Training loss: 2.262675292247965
Validation loss: 2.5245334362731584

Epoch: 39| Step: 0
Training loss: 2.362342433619995
Validation loss: 2.4962593375585773

Epoch: 6| Step: 1
Training loss: 3.179972118039526
Validation loss: 2.515613226460792

Epoch: 6| Step: 2
Training loss: 3.0297958189402423
Validation loss: 2.5222337440296343

Epoch: 6| Step: 3
Training loss: 3.3228286364782496
Validation loss: 2.5352526037675163

Epoch: 6| Step: 4
Training loss: 2.5304098736614082
Validation loss: 2.5237494754311762

Epoch: 6| Step: 5
Training loss: 2.769319727988962
Validation loss: 2.5205360687509564

Epoch: 6| Step: 6
Training loss: 3.5091971402433058
Validation loss: 2.4992475659149767

Epoch: 6| Step: 7
Training loss: 2.037859682503015
Validation loss: 2.529857362287226

Epoch: 6| Step: 8
Training loss: 3.3919790350607997
Validation loss: 2.5260544971427343

Epoch: 6| Step: 9
Training loss: 2.528616677511309
Validation loss: 2.509257169362566

Epoch: 6| Step: 10
Training loss: 2.8144269488015463
Validation loss: 2.51603668152267

Epoch: 6| Step: 11
Training loss: 2.9194116663360554
Validation loss: 2.5268931263142806

Epoch: 6| Step: 12
Training loss: 2.8949071560270463
Validation loss: 2.5184683969833648

Epoch: 6| Step: 13
Training loss: 2.5963478407149823
Validation loss: 2.528424343669097

Epoch: 40| Step: 0
Training loss: 2.763147224797655
Validation loss: 2.536041726025428

Epoch: 6| Step: 1
Training loss: 2.585132761779675
Validation loss: 2.532078273118224

Epoch: 6| Step: 2
Training loss: 1.8999630171539483
Validation loss: 2.5157902456378953

Epoch: 6| Step: 3
Training loss: 2.363565323835807
Validation loss: 2.533152438712402

Epoch: 6| Step: 4
Training loss: 2.529502830809928
Validation loss: 2.523616563834062

Epoch: 6| Step: 5
Training loss: 3.9192387812962983
Validation loss: 2.5210970882401793

Epoch: 6| Step: 6
Training loss: 3.1888555000704546
Validation loss: 2.5262960558107688

Epoch: 6| Step: 7
Training loss: 2.939039760179175
Validation loss: 2.51987084957581

Epoch: 6| Step: 8
Training loss: 3.0082306806498282
Validation loss: 2.5010904846116175

Epoch: 6| Step: 9
Training loss: 3.0001611666303587
Validation loss: 2.537719052086551

Epoch: 6| Step: 10
Training loss: 2.5647398301032727
Validation loss: 2.536075254254808

Epoch: 6| Step: 11
Training loss: 2.610118189068645
Validation loss: 2.5077772000332645

Epoch: 6| Step: 12
Training loss: 3.0559532099432256
Validation loss: 2.5317036181626547

Epoch: 6| Step: 13
Training loss: 3.8500719881139456
Validation loss: 2.5042187790673434

Epoch: 41| Step: 0
Training loss: 3.199112626063403
Validation loss: 2.5309097271988437

Epoch: 6| Step: 1
Training loss: 2.25627500990429
Validation loss: 2.521184322404609

Epoch: 6| Step: 2
Training loss: 3.1196907719179694
Validation loss: 2.5162560184830762

Epoch: 6| Step: 3
Training loss: 2.6245342931441935
Validation loss: 2.5243696383762493

Epoch: 6| Step: 4
Training loss: 2.6165630449645056
Validation loss: 2.494053383405719

Epoch: 6| Step: 5
Training loss: 3.1388526584331
Validation loss: 2.5070875033080307

Epoch: 6| Step: 6
Training loss: 3.500623647440625
Validation loss: 2.532788675840465

Epoch: 6| Step: 7
Training loss: 2.5726852827753697
Validation loss: 2.4784597500791015

Epoch: 6| Step: 8
Training loss: 3.1600562080985655
Validation loss: 2.496652252608227

Epoch: 6| Step: 9
Training loss: 2.806780191135622
Validation loss: 2.5291628601701865

Epoch: 6| Step: 10
Training loss: 2.3501315911967913
Validation loss: 2.4838006812100546

Epoch: 6| Step: 11
Training loss: 3.263146440910095
Validation loss: 2.4933260536509825

Epoch: 6| Step: 12
Training loss: 2.6356316480039395
Validation loss: 2.5323290553676396

Epoch: 6| Step: 13
Training loss: 2.6094676475295526
Validation loss: 2.50553177128922

Epoch: 42| Step: 0
Training loss: 2.9854363756046123
Validation loss: 2.5080879424853215

Epoch: 6| Step: 1
Training loss: 2.7025233133811994
Validation loss: 2.507094604958441

Epoch: 6| Step: 2
Training loss: 3.3782872333088116
Validation loss: 2.5080063818126694

Epoch: 6| Step: 3
Training loss: 1.9023335983350156
Validation loss: 2.4828068370898166

Epoch: 6| Step: 4
Training loss: 2.9219614923904618
Validation loss: 2.5185714690774406

Epoch: 6| Step: 5
Training loss: 2.277408448440427
Validation loss: 2.538062754302092

Epoch: 6| Step: 6
Training loss: 2.8522239440482147
Validation loss: 2.4843443914256214

Epoch: 6| Step: 7
Training loss: 2.8454516004469026
Validation loss: 2.5074751595779956

Epoch: 6| Step: 8
Training loss: 3.0143588081129047
Validation loss: 2.5214887590877697

Epoch: 6| Step: 9
Training loss: 2.732442990726577
Validation loss: 2.516155794551942

Epoch: 6| Step: 10
Training loss: 2.838862820344078
Validation loss: 2.4996537235360976

Epoch: 6| Step: 11
Training loss: 3.4656852035865704
Validation loss: 2.483252608903311

Epoch: 6| Step: 12
Training loss: 2.681242733392005
Validation loss: 2.489453764464977

Epoch: 6| Step: 13
Training loss: 3.0610640915869003
Validation loss: 2.494601226477025

Epoch: 43| Step: 0
Training loss: 2.283586220299819
Validation loss: 2.5074337336643926

Epoch: 6| Step: 1
Training loss: 2.9272252022424845
Validation loss: 2.5058331937741554

Epoch: 6| Step: 2
Training loss: 2.8570018154481667
Validation loss: 2.49505730623676

Epoch: 6| Step: 3
Training loss: 2.9759689099625963
Validation loss: 2.5088811491286727

Epoch: 6| Step: 4
Training loss: 2.3697075350293137
Validation loss: 2.493344973507968

Epoch: 6| Step: 5
Training loss: 2.492390591468985
Validation loss: 2.5040308164476452

Epoch: 6| Step: 6
Training loss: 3.170662634152649
Validation loss: 2.5151759603991546

Epoch: 6| Step: 7
Training loss: 3.043066373343783
Validation loss: 2.492141223269129

Epoch: 6| Step: 8
Training loss: 3.4418345086379554
Validation loss: 2.5249178180298175

Epoch: 6| Step: 9
Training loss: 2.245812439924542
Validation loss: 2.502428302913668

Epoch: 6| Step: 10
Training loss: 3.3121899153766017
Validation loss: 2.5144126608722086

Epoch: 6| Step: 11
Training loss: 3.625439321270844
Validation loss: 2.4853734137041936

Epoch: 6| Step: 12
Training loss: 2.6033712978094026
Validation loss: 2.4912413258726183

Epoch: 6| Step: 13
Training loss: 2.042524884001151
Validation loss: 2.518772809260135

Epoch: 44| Step: 0
Training loss: 2.54593851857594
Validation loss: 2.4836476270459187

Epoch: 6| Step: 1
Training loss: 2.551759120562715
Validation loss: 2.5188987798527798

Epoch: 6| Step: 2
Training loss: 3.6167177756312276
Validation loss: 2.4994446886297195

Epoch: 6| Step: 3
Training loss: 2.614316726607831
Validation loss: 2.5053135650216274

Epoch: 6| Step: 4
Training loss: 2.4219003983673266
Validation loss: 2.525524747626881

Epoch: 6| Step: 5
Training loss: 3.704870528868877
Validation loss: 2.505565519011199

Epoch: 6| Step: 6
Training loss: 3.5291439291361573
Validation loss: 2.5173523545009764

Epoch: 6| Step: 7
Training loss: 2.120861286611149
Validation loss: 2.521566318155789

Epoch: 6| Step: 8
Training loss: 2.3943231439314783
Validation loss: 2.490696400829044

Epoch: 6| Step: 9
Training loss: 3.116218741421206
Validation loss: 2.4860823296910675

Epoch: 6| Step: 10
Training loss: 2.8457347944038225
Validation loss: 2.4893552476842946

Epoch: 6| Step: 11
Training loss: 3.1842646915695565
Validation loss: 2.534001173834306

Epoch: 6| Step: 12
Training loss: 2.38849899402491
Validation loss: 2.522614897939772

Epoch: 6| Step: 13
Training loss: 2.2423406143278193
Validation loss: 2.5518611694902105

Epoch: 45| Step: 0
Training loss: 2.505601900926261
Validation loss: 2.500892797634072

Epoch: 6| Step: 1
Training loss: 3.0282614221539332
Validation loss: 2.523585451919834

Epoch: 6| Step: 2
Training loss: 2.6521855392799396
Validation loss: 2.491116604608191

Epoch: 6| Step: 3
Training loss: 2.563453310906455
Validation loss: 2.5231855180227987

Epoch: 6| Step: 4
Training loss: 3.0432999985182936
Validation loss: 2.4980728443241658

Epoch: 6| Step: 5
Training loss: 2.8333493026582555
Validation loss: 2.492611148827074

Epoch: 6| Step: 6
Training loss: 2.651958184736805
Validation loss: 2.5119300763208217

Epoch: 6| Step: 7
Training loss: 2.3216585820868114
Validation loss: 2.505767001348975

Epoch: 6| Step: 8
Training loss: 2.831840982293855
Validation loss: 2.5196764074344564

Epoch: 6| Step: 9
Training loss: 3.559516326536772
Validation loss: 2.488528072580494

Epoch: 6| Step: 10
Training loss: 3.209552846882056
Validation loss: 2.514669281757356

Epoch: 6| Step: 11
Training loss: 3.042882876476052
Validation loss: 2.5318410930954798

Epoch: 6| Step: 12
Training loss: 2.4036679967738057
Validation loss: 2.494800609142975

Epoch: 6| Step: 13
Training loss: 3.5527837664928437
Validation loss: 2.492133716392898

Epoch: 46| Step: 0
Training loss: 3.4648136107500456
Validation loss: 2.4690652783305977

Epoch: 6| Step: 1
Training loss: 3.0447101433761694
Validation loss: 2.510832646444045

Epoch: 6| Step: 2
Training loss: 3.1787086873551944
Validation loss: 2.5136468567387897

Epoch: 6| Step: 3
Training loss: 2.368965513545969
Validation loss: 2.4939827542816135

Epoch: 6| Step: 4
Training loss: 2.6906480316346024
Validation loss: 2.4946057348854977

Epoch: 6| Step: 5
Training loss: 2.4145720601470106
Validation loss: 2.5068855746200334

Epoch: 6| Step: 6
Training loss: 3.0508875321595714
Validation loss: 2.520149886468497

Epoch: 6| Step: 7
Training loss: 2.39763655320396
Validation loss: 2.521351878821675

Epoch: 6| Step: 8
Training loss: 2.5756823441199574
Validation loss: 2.4991487612530454

Epoch: 6| Step: 9
Training loss: 2.9789747340836232
Validation loss: 2.5285100982305453

Epoch: 6| Step: 10
Training loss: 3.401613211790393
Validation loss: 2.523667105400931

Epoch: 6| Step: 11
Training loss: 2.806478539487259
Validation loss: 2.492777630031505

Epoch: 6| Step: 12
Training loss: 2.941227597184955
Validation loss: 2.5180352802290384

Epoch: 6| Step: 13
Training loss: 2.02875588689433
Validation loss: 2.5120985602315526

Epoch: 47| Step: 0
Training loss: 2.7523985719486097
Validation loss: 2.5081209873362296

Epoch: 6| Step: 1
Training loss: 2.7847193820734883
Validation loss: 2.5205082259562994

Epoch: 6| Step: 2
Training loss: 2.59267660703721
Validation loss: 2.5302399028081424

Epoch: 6| Step: 3
Training loss: 2.7830144878507914
Validation loss: 2.505183574219965

Epoch: 6| Step: 4
Training loss: 3.0394428083859744
Validation loss: 2.4759496513686114

Epoch: 6| Step: 5
Training loss: 2.4019875363130576
Validation loss: 2.489003956416938

Epoch: 6| Step: 6
Training loss: 3.239484942905662
Validation loss: 2.5169790887595926

Epoch: 6| Step: 7
Training loss: 3.228700534443119
Validation loss: 2.5042022596146873

Epoch: 6| Step: 8
Training loss: 2.4560673070625096
Validation loss: 2.485595432253853

Epoch: 6| Step: 9
Training loss: 3.2826947619147204
Validation loss: 2.4949448215427243

Epoch: 6| Step: 10
Training loss: 2.2762703042379044
Validation loss: 2.5171363665933826

Epoch: 6| Step: 11
Training loss: 2.766477119509756
Validation loss: 2.5222971879655027

Epoch: 6| Step: 12
Training loss: 3.5648817165274966
Validation loss: 2.473336903232543

Epoch: 6| Step: 13
Training loss: 2.624534020617454
Validation loss: 2.499763662180544

Epoch: 48| Step: 0
Training loss: 2.2787679468767528
Validation loss: 2.4919471425512065

Epoch: 6| Step: 1
Training loss: 3.9028280910327444
Validation loss: 2.5212407980730163

Epoch: 6| Step: 2
Training loss: 2.561888086714105
Validation loss: 2.4785633275052863

Epoch: 6| Step: 3
Training loss: 2.57214605448885
Validation loss: 2.487058859855445

Epoch: 6| Step: 4
Training loss: 3.112886985406173
Validation loss: 2.5009439767875636

Epoch: 6| Step: 5
Training loss: 2.402255321829073
Validation loss: 2.531976890942952

Epoch: 6| Step: 6
Training loss: 3.2083914284999078
Validation loss: 2.484290793006874

Epoch: 6| Step: 7
Training loss: 2.8281789532146386
Validation loss: 2.4787763634934623

Epoch: 6| Step: 8
Training loss: 2.60821653679437
Validation loss: 2.4874758122653784

Epoch: 6| Step: 9
Training loss: 2.911425504459556
Validation loss: 2.4980713819212648

Epoch: 6| Step: 10
Training loss: 2.994126291856593
Validation loss: 2.472189201772624

Epoch: 6| Step: 11
Training loss: 2.3886842516799143
Validation loss: 2.498458239233735

Epoch: 6| Step: 12
Training loss: 2.6239254886701286
Validation loss: 2.4986425149527416

Epoch: 6| Step: 13
Training loss: 3.607274620962485
Validation loss: 2.517582789811294

Epoch: 49| Step: 0
Training loss: 2.7664162748468106
Validation loss: 2.503813103948199

Epoch: 6| Step: 1
Training loss: 2.699586084215973
Validation loss: 2.4824836534605237

Epoch: 6| Step: 2
Training loss: 2.7104694061269705
Validation loss: 2.4979401295703036

Epoch: 6| Step: 3
Training loss: 2.6325866421228556
Validation loss: 2.4999614569041158

Epoch: 6| Step: 4
Training loss: 2.8199106706850325
Validation loss: 2.5032789845958123

Epoch: 6| Step: 5
Training loss: 3.0870941698485947
Validation loss: 2.5133873820695714

Epoch: 6| Step: 6
Training loss: 3.7392444226794086
Validation loss: 2.5124736418198816

Epoch: 6| Step: 7
Training loss: 3.7922101033106497
Validation loss: 2.4987981706972877

Epoch: 6| Step: 8
Training loss: 3.29516412537512
Validation loss: 2.509160034974342

Epoch: 6| Step: 9
Training loss: 2.1160684140616772
Validation loss: 2.4992982894786264

Epoch: 6| Step: 10
Training loss: 2.68496105116116
Validation loss: 2.472693726788695

Epoch: 6| Step: 11
Training loss: 2.3533756198632005
Validation loss: 2.5035882558202607

Epoch: 6| Step: 12
Training loss: 1.961391381248628
Validation loss: 2.497223594199425

Epoch: 6| Step: 13
Training loss: 2.545940766094225
Validation loss: 2.4680772973129415

Epoch: 50| Step: 0
Training loss: 3.234101362216984
Validation loss: 2.4948584815043238

Epoch: 6| Step: 1
Training loss: 2.6811014343219184
Validation loss: 2.4880643715864155

Epoch: 6| Step: 2
Training loss: 3.312453503552284
Validation loss: 2.4864407242169473

Epoch: 6| Step: 3
Training loss: 2.954572696160061
Validation loss: 2.484502028467392

Epoch: 6| Step: 4
Training loss: 2.415638825647187
Validation loss: 2.4930271859992863

Epoch: 6| Step: 5
Training loss: 3.0456318201982495
Validation loss: 2.5214478826638005

Epoch: 6| Step: 6
Training loss: 2.805757961169106
Validation loss: 2.514991811519097

Epoch: 6| Step: 7
Training loss: 2.576758004955272
Validation loss: 2.473219732229587

Epoch: 6| Step: 8
Training loss: 2.6822396183828103
Validation loss: 2.510875663177378

Epoch: 6| Step: 9
Training loss: 2.4793689121703366
Validation loss: 2.5110270740518996

Epoch: 6| Step: 10
Training loss: 3.1222338831975285
Validation loss: 2.53466617295441

Epoch: 6| Step: 11
Training loss: 3.0359338184124294
Validation loss: 2.5116907058635545

Epoch: 6| Step: 12
Training loss: 2.797090490127364
Validation loss: 2.4908150366745208

Epoch: 6| Step: 13
Training loss: 2.6752348181790437
Validation loss: 2.4843989854994835

Epoch: 51| Step: 0
Training loss: 2.154578182775208
Validation loss: 2.53353669570616

Epoch: 6| Step: 1
Training loss: 2.7949065202015326
Validation loss: 2.5072550897360255

Epoch: 6| Step: 2
Training loss: 3.3836184063658035
Validation loss: 2.5142928004664387

Epoch: 6| Step: 3
Training loss: 2.886854527316825
Validation loss: 2.495474179673585

Epoch: 6| Step: 4
Training loss: 3.2220823944562684
Validation loss: 2.493380489108534

Epoch: 6| Step: 5
Training loss: 2.881357213340467
Validation loss: 2.513787214525939

Epoch: 6| Step: 6
Training loss: 2.9434259580537154
Validation loss: 2.502237565561166

Epoch: 6| Step: 7
Training loss: 3.1846938403192517
Validation loss: 2.4983487326107405

Epoch: 6| Step: 8
Training loss: 2.3241569959626918
Validation loss: 2.516665993912945

Epoch: 6| Step: 9
Training loss: 2.813848214725289
Validation loss: 2.4955745823863036

Epoch: 6| Step: 10
Training loss: 2.5800380198316564
Validation loss: 2.488585558217895

Epoch: 6| Step: 11
Training loss: 2.7599406473716352
Validation loss: 2.5028396220292275

Epoch: 6| Step: 12
Training loss: 2.687365683814259
Validation loss: 2.496403720369932

Epoch: 6| Step: 13
Training loss: 3.2877845728021415
Validation loss: 2.4709701513763114

Epoch: 52| Step: 0
Training loss: 3.310108401087965
Validation loss: 2.4949943697678005

Epoch: 6| Step: 1
Training loss: 2.6262691699367386
Validation loss: 2.5082465423519498

Epoch: 6| Step: 2
Training loss: 3.3348578463659813
Validation loss: 2.488834572452737

Epoch: 6| Step: 3
Training loss: 2.9081099926323364
Validation loss: 2.4975484261430942

Epoch: 6| Step: 4
Training loss: 2.8357273160532808
Validation loss: 2.5072683447365507

Epoch: 6| Step: 5
Training loss: 3.025610643024577
Validation loss: 2.506338060278506

Epoch: 6| Step: 6
Training loss: 2.1484519264430415
Validation loss: 2.493900118617425

Epoch: 6| Step: 7
Training loss: 2.8052836770073517
Validation loss: 2.505008015044958

Epoch: 6| Step: 8
Training loss: 2.0247841849530497
Validation loss: 2.507827273682376

Epoch: 6| Step: 9
Training loss: 2.71718104900803
Validation loss: 2.499105967357097

Epoch: 6| Step: 10
Training loss: 3.0216825848044575
Validation loss: 2.50384229194051

Epoch: 6| Step: 11
Training loss: 2.715651050438289
Validation loss: 2.518162855529189

Epoch: 6| Step: 12
Training loss: 2.575035880125857
Validation loss: 2.4851946993772684

Epoch: 6| Step: 13
Training loss: 3.796961653862641
Validation loss: 2.5016961640074005

Epoch: 53| Step: 0
Training loss: 3.097093721089485
Validation loss: 2.4999464470244526

Epoch: 6| Step: 1
Training loss: 2.999159377261376
Validation loss: 2.535565506546863

Epoch: 6| Step: 2
Training loss: 3.0334904836112733
Validation loss: 2.5030399093533338

Epoch: 6| Step: 3
Training loss: 2.4235569189717516
Validation loss: 2.503495379543796

Epoch: 6| Step: 4
Training loss: 2.457669270798841
Validation loss: 2.46440882716984

Epoch: 6| Step: 5
Training loss: 2.5808881344354084
Validation loss: 2.4738073370059643

Epoch: 6| Step: 6
Training loss: 2.9805936974649323
Validation loss: 2.498640961568012

Epoch: 6| Step: 7
Training loss: 2.4526393919984684
Validation loss: 2.507239660330226

Epoch: 6| Step: 8
Training loss: 2.7841749776628437
Validation loss: 2.48855221378425

Epoch: 6| Step: 9
Training loss: 2.3852618526767584
Validation loss: 2.497578732274395

Epoch: 6| Step: 10
Training loss: 3.439750628193165
Validation loss: 2.5190309287715658

Epoch: 6| Step: 11
Training loss: 2.7840265708948846
Validation loss: 2.4979491189322407

Epoch: 6| Step: 12
Training loss: 2.7731329952225323
Validation loss: 2.4926398868970825

Epoch: 6| Step: 13
Training loss: 3.918605946595966
Validation loss: 2.5007241102814577

Epoch: 54| Step: 0
Training loss: 2.684388422016521
Validation loss: 2.5036241821296836

Epoch: 6| Step: 1
Training loss: 2.6501522308385708
Validation loss: 2.4930452412533706

Epoch: 6| Step: 2
Training loss: 3.102848430190767
Validation loss: 2.4591172171110296

Epoch: 6| Step: 3
Training loss: 2.5523435706496724
Validation loss: 2.4867040627065573

Epoch: 6| Step: 4
Training loss: 3.4221380820671707
Validation loss: 2.4788595322833125

Epoch: 6| Step: 5
Training loss: 2.4989675297209466
Validation loss: 2.5175537621417554

Epoch: 6| Step: 6
Training loss: 3.1548109645404847
Validation loss: 2.491116354533704

Epoch: 6| Step: 7
Training loss: 3.109819696725612
Validation loss: 2.4809717576703383

Epoch: 6| Step: 8
Training loss: 2.464608497093466
Validation loss: 2.475808925900658

Epoch: 6| Step: 9
Training loss: 2.543744556318077
Validation loss: 2.518065713433604

Epoch: 6| Step: 10
Training loss: 2.486598717048343
Validation loss: 2.4962417142980717

Epoch: 6| Step: 11
Training loss: 2.636491599011868
Validation loss: 2.5051221314432413

Epoch: 6| Step: 12
Training loss: 3.0296341826934854
Validation loss: 2.4953181460363147

Epoch: 6| Step: 13
Training loss: 3.128345377320843
Validation loss: 2.471949966493003

Epoch: 55| Step: 0
Training loss: 2.933861447246305
Validation loss: 2.488836283376267

Epoch: 6| Step: 1
Training loss: 2.9466520546713397
Validation loss: 2.491801395519576

Epoch: 6| Step: 2
Training loss: 3.071398982035409
Validation loss: 2.4987527073852567

Epoch: 6| Step: 3
Training loss: 2.169091506146325
Validation loss: 2.5121322972942686

Epoch: 6| Step: 4
Training loss: 3.098391817172607
Validation loss: 2.499245725691921

Epoch: 6| Step: 5
Training loss: 3.2550382442542234
Validation loss: 2.5062569101233394

Epoch: 6| Step: 6
Training loss: 3.189306457847516
Validation loss: 2.487821599146758

Epoch: 6| Step: 7
Training loss: 2.2448077160703335
Validation loss: 2.458476231028146

Epoch: 6| Step: 8
Training loss: 3.3276596102133573
Validation loss: 2.4844070404642764

Epoch: 6| Step: 9
Training loss: 3.0614064658176634
Validation loss: 2.4982504567963333

Epoch: 6| Step: 10
Training loss: 2.756951908343824
Validation loss: 2.5179846836524025

Epoch: 6| Step: 11
Training loss: 2.363836453378345
Validation loss: 2.4776323799294366

Epoch: 6| Step: 12
Training loss: 2.129354279296668
Validation loss: 2.4735951571682544

Epoch: 6| Step: 13
Training loss: 2.8417748938690264
Validation loss: 2.497060624410055

Epoch: 56| Step: 0
Training loss: 3.232018243303838
Validation loss: 2.490675723403049

Epoch: 6| Step: 1
Training loss: 2.6716003276785343
Validation loss: 2.4889347125469494

Epoch: 6| Step: 2
Training loss: 2.753959060054373
Validation loss: 2.5114239361289012

Epoch: 6| Step: 3
Training loss: 2.694438140136405
Validation loss: 2.508530500271879

Epoch: 6| Step: 4
Training loss: 2.1619848751347175
Validation loss: 2.5004752702047686

Epoch: 6| Step: 5
Training loss: 2.794642148524882
Validation loss: 2.5007284128714344

Epoch: 6| Step: 6
Training loss: 2.8382790726921874
Validation loss: 2.4947434342187416

Epoch: 6| Step: 7
Training loss: 2.816227117547927
Validation loss: 2.5124005581208397

Epoch: 6| Step: 8
Training loss: 3.4637421896497815
Validation loss: 2.512295956285823

Epoch: 6| Step: 9
Training loss: 2.2932927229338875
Validation loss: 2.484144423394029

Epoch: 6| Step: 10
Training loss: 3.1323153512759325
Validation loss: 2.495904970069721

Epoch: 6| Step: 11
Training loss: 2.8342525636584495
Validation loss: 2.4821766750896592

Epoch: 6| Step: 12
Training loss: 2.8616564044831336
Validation loss: 2.4992787397441374

Epoch: 6| Step: 13
Training loss: 3.3003690744410474
Validation loss: 2.495301433585465

Epoch: 57| Step: 0
Training loss: 2.1126274747590816
Validation loss: 2.49468867807634

Epoch: 6| Step: 1
Training loss: 3.151328300281065
Validation loss: 2.488089394130237

Epoch: 6| Step: 2
Training loss: 2.804842038798239
Validation loss: 2.4861071195433073

Epoch: 6| Step: 3
Training loss: 3.2308302173168184
Validation loss: 2.475514227434143

Epoch: 6| Step: 4
Training loss: 2.332689968603634
Validation loss: 2.454068539272977

Epoch: 6| Step: 5
Training loss: 2.60475800984582
Validation loss: 2.4831837363427116

Epoch: 6| Step: 6
Training loss: 3.7580561251588236
Validation loss: 2.482465011753609

Epoch: 6| Step: 7
Training loss: 3.15845344603595
Validation loss: 2.5069218415065975

Epoch: 6| Step: 8
Training loss: 2.443538131700469
Validation loss: 2.498199737612321

Epoch: 6| Step: 9
Training loss: 3.0525533338131745
Validation loss: 2.4724367909378313

Epoch: 6| Step: 10
Training loss: 2.5539248200048044
Validation loss: 2.4574446270184316

Epoch: 6| Step: 11
Training loss: 2.3094769900543137
Validation loss: 2.4934955715194396

Epoch: 6| Step: 12
Training loss: 3.0335852682548663
Validation loss: 2.484896824263905

Epoch: 6| Step: 13
Training loss: 2.542498145088712
Validation loss: 2.488522856755813

Epoch: 58| Step: 0
Training loss: 2.5526796435769086
Validation loss: 2.5171067338868163

Epoch: 6| Step: 1
Training loss: 2.191192997366728
Validation loss: 2.509496049690724

Epoch: 6| Step: 2
Training loss: 2.2887854294506136
Validation loss: 2.4706103111161175

Epoch: 6| Step: 3
Training loss: 2.822663064711406
Validation loss: 2.4882124166893647

Epoch: 6| Step: 4
Training loss: 3.3304131750435872
Validation loss: 2.4954113656988097

Epoch: 6| Step: 5
Training loss: 3.0027542187047573
Validation loss: 2.4943051818360478

Epoch: 6| Step: 6
Training loss: 2.8792832022976143
Validation loss: 2.4881135952294993

Epoch: 6| Step: 7
Training loss: 2.752870621982979
Validation loss: 2.490986986504169

Epoch: 6| Step: 8
Training loss: 2.8242716804071373
Validation loss: 2.468849301628875

Epoch: 6| Step: 9
Training loss: 2.761422104935422
Validation loss: 2.47956967268795

Epoch: 6| Step: 10
Training loss: 3.133376225387596
Validation loss: 2.4779243589846485

Epoch: 6| Step: 11
Training loss: 3.1203688828938208
Validation loss: 2.4904565016629325

Epoch: 6| Step: 12
Training loss: 2.998146915319909
Validation loss: 2.4732103119544036

Epoch: 6| Step: 13
Training loss: 2.719691321865492
Validation loss: 2.485852991212535

Epoch: 59| Step: 0
Training loss: 3.1057002910930898
Validation loss: 2.4583297515336926

Epoch: 6| Step: 1
Training loss: 2.664752054328388
Validation loss: 2.4924740588802474

Epoch: 6| Step: 2
Training loss: 2.4858769129497684
Validation loss: 2.4945280651832715

Epoch: 6| Step: 3
Training loss: 2.978620483553819
Validation loss: 2.4936668214555784

Epoch: 6| Step: 4
Training loss: 2.574398607561151
Validation loss: 2.4814134817462934

Epoch: 6| Step: 5
Training loss: 2.489948666208113
Validation loss: 2.500027998900777

Epoch: 6| Step: 6
Training loss: 3.507308142071692
Validation loss: 2.4808273266832255

Epoch: 6| Step: 7
Training loss: 3.1395111386192216
Validation loss: 2.4946752344601273

Epoch: 6| Step: 8
Training loss: 2.685268806626773
Validation loss: 2.5071065851772425

Epoch: 6| Step: 9
Training loss: 2.9738504063963984
Validation loss: 2.4960016055182805

Epoch: 6| Step: 10
Training loss: 2.2985464229746015
Validation loss: 2.4966927366435714

Epoch: 6| Step: 11
Training loss: 2.956658726638173
Validation loss: 2.5270881189446186

Epoch: 6| Step: 12
Training loss: 2.2953770742038793
Validation loss: 2.4763902453081417

Epoch: 6| Step: 13
Training loss: 2.904577614940909
Validation loss: 2.488059430930863

Epoch: 60| Step: 0
Training loss: 2.6617860436785254
Validation loss: 2.487516064838558

Epoch: 6| Step: 1
Training loss: 2.9791466416617385
Validation loss: 2.511937622011981

Epoch: 6| Step: 2
Training loss: 2.5766057017205157
Validation loss: 2.480149647537375

Epoch: 6| Step: 3
Training loss: 2.9423738992874457
Validation loss: 2.4851718645427034

Epoch: 6| Step: 4
Training loss: 2.9385086924963257
Validation loss: 2.4987138454623343

Epoch: 6| Step: 5
Training loss: 3.040340988209211
Validation loss: 2.474354715419686

Epoch: 6| Step: 6
Training loss: 3.382847739514055
Validation loss: 2.496579614533433

Epoch: 6| Step: 7
Training loss: 2.5194803395413605
Validation loss: 2.517951742796086

Epoch: 6| Step: 8
Training loss: 2.490684319032662
Validation loss: 2.477392942808965

Epoch: 6| Step: 9
Training loss: 2.889153183152921
Validation loss: 2.4578939086498406

Epoch: 6| Step: 10
Training loss: 3.263004255240001
Validation loss: 2.497829880763759

Epoch: 6| Step: 11
Training loss: 1.9807607822936022
Validation loss: 2.493219563671717

Epoch: 6| Step: 12
Training loss: 3.0049816731482792
Validation loss: 2.5044553823044446

Epoch: 6| Step: 13
Training loss: 2.7638796023964156
Validation loss: 2.502765801181862

Epoch: 61| Step: 0
Training loss: 3.1285523632386862
Validation loss: 2.522448736486377

Epoch: 6| Step: 1
Training loss: 2.5713443250328063
Validation loss: 2.4897804683347755

Epoch: 6| Step: 2
Training loss: 2.902956665546107
Validation loss: 2.4996519646343764

Epoch: 6| Step: 3
Training loss: 2.524515305634202
Validation loss: 2.471418930566281

Epoch: 6| Step: 4
Training loss: 3.4265305712004226
Validation loss: 2.484893443416863

Epoch: 6| Step: 5
Training loss: 2.4422412634537185
Validation loss: 2.485725894614753

Epoch: 6| Step: 6
Training loss: 2.587684801661799
Validation loss: 2.492744551475479

Epoch: 6| Step: 7
Training loss: 2.251192094989066
Validation loss: 2.4884095358101455

Epoch: 6| Step: 8
Training loss: 2.958948922023584
Validation loss: 2.495742440618267

Epoch: 6| Step: 9
Training loss: 3.7300401371491505
Validation loss: 2.4783461748524735

Epoch: 6| Step: 10
Training loss: 2.790205179142105
Validation loss: 2.4946811023064144

Epoch: 6| Step: 11
Training loss: 2.9086280855564692
Validation loss: 2.4954183002469503

Epoch: 6| Step: 12
Training loss: 2.0447536279058727
Validation loss: 2.489967458319349

Epoch: 6| Step: 13
Training loss: 2.461129995416658
Validation loss: 2.490170703052221

Epoch: 62| Step: 0
Training loss: 3.1930793849444346
Validation loss: 2.4620135190579298

Epoch: 6| Step: 1
Training loss: 3.4586364123046427
Validation loss: 2.5119425743857233

Epoch: 6| Step: 2
Training loss: 2.4518843508232626
Validation loss: 2.494643336154096

Epoch: 6| Step: 3
Training loss: 2.807280890185185
Validation loss: 2.5041411137088887

Epoch: 6| Step: 4
Training loss: 2.704749088037917
Validation loss: 2.496892067804972

Epoch: 6| Step: 5
Training loss: 3.057054310065083
Validation loss: 2.4693796318856807

Epoch: 6| Step: 6
Training loss: 2.24270294325825
Validation loss: 2.4996092983249087

Epoch: 6| Step: 7
Training loss: 2.711914414160942
Validation loss: 2.5026834544992127

Epoch: 6| Step: 8
Training loss: 2.4071140471929966
Validation loss: 2.480631587754139

Epoch: 6| Step: 9
Training loss: 3.096354611554233
Validation loss: 2.4856136631956636

Epoch: 6| Step: 10
Training loss: 2.906781875998574
Validation loss: 2.4747702343488553

Epoch: 6| Step: 11
Training loss: 2.6479692523415816
Validation loss: 2.5092542075304034

Epoch: 6| Step: 12
Training loss: 2.871102718417115
Validation loss: 2.5095782463830405

Epoch: 6| Step: 13
Training loss: 2.6292123148764532
Validation loss: 2.486830023788541

Epoch: 63| Step: 0
Training loss: 3.3037044424725543
Validation loss: 2.480240884252115

Epoch: 6| Step: 1
Training loss: 2.3877248688099098
Validation loss: 2.4827644378547853

Epoch: 6| Step: 2
Training loss: 2.763103823025187
Validation loss: 2.487551493098086

Epoch: 6| Step: 3
Training loss: 2.5079235396951463
Validation loss: 2.4982311528422385

Epoch: 6| Step: 4
Training loss: 2.6066973075952014
Validation loss: 2.5066003863783086

Epoch: 6| Step: 5
Training loss: 3.5430909527919066
Validation loss: 2.4855224658922284

Epoch: 6| Step: 6
Training loss: 2.9218883820089614
Validation loss: 2.498097870266676

Epoch: 6| Step: 7
Training loss: 2.736569466794921
Validation loss: 2.4736953545494482

Epoch: 6| Step: 8
Training loss: 3.4094175956715547
Validation loss: 2.464997357065427

Epoch: 6| Step: 9
Training loss: 2.3012460650359103
Validation loss: 2.4887765888576605

Epoch: 6| Step: 10
Training loss: 2.860689288486471
Validation loss: 2.462919946140965

Epoch: 6| Step: 11
Training loss: 2.67801714293824
Validation loss: 2.4668295740256716

Epoch: 6| Step: 12
Training loss: 2.0196620522497946
Validation loss: 2.4763587916274292

Epoch: 6| Step: 13
Training loss: 3.4275892786687012
Validation loss: 2.4723862950017224

Epoch: 64| Step: 0
Training loss: 2.5424552903046775
Validation loss: 2.4943917106068896

Epoch: 6| Step: 1
Training loss: 2.5822973532624984
Validation loss: 2.495361884313308

Epoch: 6| Step: 2
Training loss: 2.8192881575824456
Validation loss: 2.4657271354693915

Epoch: 6| Step: 3
Training loss: 1.8689659300097559
Validation loss: 2.488617213795012

Epoch: 6| Step: 4
Training loss: 3.465198244971437
Validation loss: 2.4869098090355757

Epoch: 6| Step: 5
Training loss: 2.9822571914462066
Validation loss: 2.4867339381134546

Epoch: 6| Step: 6
Training loss: 2.88650235202521
Validation loss: 2.4736212930328145

Epoch: 6| Step: 7
Training loss: 2.9271916452009723
Validation loss: 2.502478535838664

Epoch: 6| Step: 8
Training loss: 2.9214173636852707
Validation loss: 2.4909905330097866

Epoch: 6| Step: 9
Training loss: 2.4679073392515845
Validation loss: 2.485972356801648

Epoch: 6| Step: 10
Training loss: 2.632087041592478
Validation loss: 2.492048303573915

Epoch: 6| Step: 11
Training loss: 3.536040710749956
Validation loss: 2.488272039228629

Epoch: 6| Step: 12
Training loss: 3.0151474332426025
Validation loss: 2.4883370218317706

Epoch: 6| Step: 13
Training loss: 2.0614310297636473
Validation loss: 2.485937798074527

Epoch: 65| Step: 0
Training loss: 2.7086757932461167
Validation loss: 2.5068625510604488

Epoch: 6| Step: 1
Training loss: 2.4148236405008294
Validation loss: 2.50307146509074

Epoch: 6| Step: 2
Training loss: 2.921783935433359
Validation loss: 2.5029161017321515

Epoch: 6| Step: 3
Training loss: 2.8078225977848215
Validation loss: 2.508309158572822

Epoch: 6| Step: 4
Training loss: 2.203092317812221
Validation loss: 2.4750024559561523

Epoch: 6| Step: 5
Training loss: 2.4956620728117707
Validation loss: 2.490841667027332

Epoch: 6| Step: 6
Training loss: 2.847457983495707
Validation loss: 2.484744997309036

Epoch: 6| Step: 7
Training loss: 3.5657862179656656
Validation loss: 2.488581961927667

Epoch: 6| Step: 8
Training loss: 2.930801383299805
Validation loss: 2.469911309703724

Epoch: 6| Step: 9
Training loss: 2.805983729994378
Validation loss: 2.5027488250188

Epoch: 6| Step: 10
Training loss: 3.0287833648619786
Validation loss: 2.4738132719678703

Epoch: 6| Step: 11
Training loss: 3.0753868045829185
Validation loss: 2.4839385408940577

Epoch: 6| Step: 12
Training loss: 2.769877467285666
Validation loss: 2.478645657393563

Epoch: 6| Step: 13
Training loss: 2.316961881183245
Validation loss: 2.4566895067783348

Epoch: 66| Step: 0
Training loss: 3.15222197986426
Validation loss: 2.4865390925332345

Epoch: 6| Step: 1
Training loss: 2.3990672524213705
Validation loss: 2.473272643905146

Epoch: 6| Step: 2
Training loss: 2.4067734978688677
Validation loss: 2.4952843902017805

Epoch: 6| Step: 3
Training loss: 2.735004897453205
Validation loss: 2.4980403685156904

Epoch: 6| Step: 4
Training loss: 2.7884413083503743
Validation loss: 2.48913830423867

Epoch: 6| Step: 5
Training loss: 2.1078754887879194
Validation loss: 2.490481054452743

Epoch: 6| Step: 6
Training loss: 2.93166307088759
Validation loss: 2.476630791727653

Epoch: 6| Step: 7
Training loss: 2.737041982020066
Validation loss: 2.4982751392469766

Epoch: 6| Step: 8
Training loss: 2.58645486410223
Validation loss: 2.476013285215712

Epoch: 6| Step: 9
Training loss: 2.682052947350245
Validation loss: 2.4711077226101996

Epoch: 6| Step: 10
Training loss: 3.6119098432067904
Validation loss: 2.492071317148637

Epoch: 6| Step: 11
Training loss: 2.5878933888546563
Validation loss: 2.481022732946538

Epoch: 6| Step: 12
Training loss: 3.1204471228626884
Validation loss: 2.48228721324085

Epoch: 6| Step: 13
Training loss: 3.509227577706905
Validation loss: 2.509710399846366

Epoch: 67| Step: 0
Training loss: 2.8991551911632345
Validation loss: 2.4997883999397756

Epoch: 6| Step: 1
Training loss: 3.0553254146796545
Validation loss: 2.4926464805002992

Epoch: 6| Step: 2
Training loss: 2.3994259863104213
Validation loss: 2.518326294652576

Epoch: 6| Step: 3
Training loss: 3.814571880751869
Validation loss: 2.4619603206289

Epoch: 6| Step: 4
Training loss: 2.0160469973702173
Validation loss: 2.4935042206582985

Epoch: 6| Step: 5
Training loss: 2.8034471409663464
Validation loss: 2.482610074443119

Epoch: 6| Step: 6
Training loss: 2.690629777893007
Validation loss: 2.4954854790970535

Epoch: 6| Step: 7
Training loss: 2.243860026347583
Validation loss: 2.4593938337571393

Epoch: 6| Step: 8
Training loss: 3.4105785060381777
Validation loss: 2.4814401551397762

Epoch: 6| Step: 9
Training loss: 2.394160828557273
Validation loss: 2.486801158786228

Epoch: 6| Step: 10
Training loss: 2.989197354981161
Validation loss: 2.480014104682291

Epoch: 6| Step: 11
Training loss: 2.9711671325143416
Validation loss: 2.4811286563286394

Epoch: 6| Step: 12
Training loss: 2.3117144513388346
Validation loss: 2.474345034226924

Epoch: 6| Step: 13
Training loss: 2.338232539644923
Validation loss: 2.484770673356797

Epoch: 68| Step: 0
Training loss: 2.294431255734737
Validation loss: 2.4848661075984673

Epoch: 6| Step: 1
Training loss: 3.068955138394742
Validation loss: 2.465374403250111

Epoch: 6| Step: 2
Training loss: 2.5587202899887087
Validation loss: 2.4964431029594047

Epoch: 6| Step: 3
Training loss: 3.6599238269988073
Validation loss: 2.458098950135576

Epoch: 6| Step: 4
Training loss: 3.248125122375662
Validation loss: 2.4902143423138074

Epoch: 6| Step: 5
Training loss: 2.2761688080520353
Validation loss: 2.4917794638487676

Epoch: 6| Step: 6
Training loss: 2.821952702159031
Validation loss: 2.4921866276869657

Epoch: 6| Step: 7
Training loss: 3.2108489903783504
Validation loss: 2.500294488564222

Epoch: 6| Step: 8
Training loss: 2.6317414502113423
Validation loss: 2.468809828806267

Epoch: 6| Step: 9
Training loss: 2.788704728973501
Validation loss: 2.4823342618278885

Epoch: 6| Step: 10
Training loss: 2.2030467567347767
Validation loss: 2.474214348923053

Epoch: 6| Step: 11
Training loss: 2.6399209251988003
Validation loss: 2.4908499692703034

Epoch: 6| Step: 12
Training loss: 3.3108271567377225
Validation loss: 2.503933343143554

Epoch: 6| Step: 13
Training loss: 2.029427047828045
Validation loss: 2.4551019518866335

Epoch: 69| Step: 0
Training loss: 2.5991803968172302
Validation loss: 2.5002410792942547

Epoch: 6| Step: 1
Training loss: 2.5059640316950706
Validation loss: 2.460728715281749

Epoch: 6| Step: 2
Training loss: 3.4250064265058606
Validation loss: 2.4659231639178167

Epoch: 6| Step: 3
Training loss: 3.086950053916645
Validation loss: 2.4859171419119686

Epoch: 6| Step: 4
Training loss: 2.2574592498785133
Validation loss: 2.4586092476184116

Epoch: 6| Step: 5
Training loss: 2.8898683949636808
Validation loss: 2.4962604230898444

Epoch: 6| Step: 6
Training loss: 2.145914045377616
Validation loss: 2.475758964716744

Epoch: 6| Step: 7
Training loss: 3.2336887976978312
Validation loss: 2.4619278766138937

Epoch: 6| Step: 8
Training loss: 2.9834230512487108
Validation loss: 2.4974843524846415

Epoch: 6| Step: 9
Training loss: 2.6928108436851192
Validation loss: 2.4741731701731093

Epoch: 6| Step: 10
Training loss: 2.800925108807611
Validation loss: 2.454764777521997

Epoch: 6| Step: 11
Training loss: 2.729590016694947
Validation loss: 2.4841806690843664

Epoch: 6| Step: 12
Training loss: 2.919010655529076
Validation loss: 2.4844721024892116

Epoch: 6| Step: 13
Training loss: 2.940359144527027
Validation loss: 2.501482663365931

Epoch: 70| Step: 0
Training loss: 3.443828790943738
Validation loss: 2.524239173537626

Epoch: 6| Step: 1
Training loss: 3.2244949181874967
Validation loss: 2.480383792320472

Epoch: 6| Step: 2
Training loss: 2.9376731983826256
Validation loss: 2.492305265628559

Epoch: 6| Step: 3
Training loss: 2.867715314049936
Validation loss: 2.468540951761908

Epoch: 6| Step: 4
Training loss: 2.45975807628591
Validation loss: 2.5081651912602547

Epoch: 6| Step: 5
Training loss: 2.9856567825756266
Validation loss: 2.4992466099018302

Epoch: 6| Step: 6
Training loss: 2.7821588638547556
Validation loss: 2.4943150959510505

Epoch: 6| Step: 7
Training loss: 2.678516276563788
Validation loss: 2.491073213736283

Epoch: 6| Step: 8
Training loss: 1.9802527676700128
Validation loss: 2.4805165275976613

Epoch: 6| Step: 9
Training loss: 3.1685834069635543
Validation loss: 2.493846434652742

Epoch: 6| Step: 10
Training loss: 2.596792986845182
Validation loss: 2.5032979265188215

Epoch: 6| Step: 11
Training loss: 2.6874441096682053
Validation loss: 2.493767314411505

Epoch: 6| Step: 12
Training loss: 2.348186312482441
Validation loss: 2.489107143482037

Epoch: 6| Step: 13
Training loss: 2.822855386854809
Validation loss: 2.50638540509412

Epoch: 71| Step: 0
Training loss: 3.200785457058042
Validation loss: 2.5016261913237314

Epoch: 6| Step: 1
Training loss: 2.8492509225587956
Validation loss: 2.4851990897153415

Epoch: 6| Step: 2
Training loss: 3.2059711000373823
Validation loss: 2.4787889046117995

Epoch: 6| Step: 3
Training loss: 2.8617343861545597
Validation loss: 2.4910592782346073

Epoch: 6| Step: 4
Training loss: 3.336569804317307
Validation loss: 2.5186008204100125

Epoch: 6| Step: 5
Training loss: 2.501860498504961
Validation loss: 2.502655549842009

Epoch: 6| Step: 6
Training loss: 2.5790628288044193
Validation loss: 2.501984341967085

Epoch: 6| Step: 7
Training loss: 2.702146319907916
Validation loss: 2.506439817745056

Epoch: 6| Step: 8
Training loss: 2.9370493441714007
Validation loss: 2.505858615890507

Epoch: 6| Step: 9
Training loss: 2.9363180176841475
Validation loss: 2.526277867293934

Epoch: 6| Step: 10
Training loss: 2.6384079333145025
Validation loss: 2.4972497229931183

Epoch: 6| Step: 11
Training loss: 2.2916687589693634
Validation loss: 2.4856408701213484

Epoch: 6| Step: 12
Training loss: 2.411348959719848
Validation loss: 2.4989876276538445

Epoch: 6| Step: 13
Training loss: 2.837896505876133
Validation loss: 2.4995441964673613

Epoch: 72| Step: 0
Training loss: 2.76037271332686
Validation loss: 2.5103530688513036

Epoch: 6| Step: 1
Training loss: 2.9202314844216413
Validation loss: 2.482455370967732

Epoch: 6| Step: 2
Training loss: 2.7158649088391646
Validation loss: 2.516826768816233

Epoch: 6| Step: 3
Training loss: 2.6515297630865264
Validation loss: 2.4777940042102578

Epoch: 6| Step: 4
Training loss: 3.2744188681276247
Validation loss: 2.477112616561664

Epoch: 6| Step: 5
Training loss: 2.5800805275939798
Validation loss: 2.501188308546959

Epoch: 6| Step: 6
Training loss: 2.963435505785176
Validation loss: 2.5087371022332254

Epoch: 6| Step: 7
Training loss: 2.866066529049524
Validation loss: 2.5198080353608217

Epoch: 6| Step: 8
Training loss: 2.9626775374512104
Validation loss: 2.499295418419061

Epoch: 6| Step: 9
Training loss: 3.3806049288638556
Validation loss: 2.500890910442348

Epoch: 6| Step: 10
Training loss: 2.7980637395005172
Validation loss: 2.4924028079671974

Epoch: 6| Step: 11
Training loss: 2.2941363349717254
Validation loss: 2.5048820039685915

Epoch: 6| Step: 12
Training loss: 2.654686321687128
Validation loss: 2.449318112896448

Epoch: 6| Step: 13
Training loss: 1.5949900421740568
Validation loss: 2.504572100245451

Epoch: 73| Step: 0
Training loss: 2.3637949992011342
Validation loss: 2.4968189027903684

Epoch: 6| Step: 1
Training loss: 2.73969131276268
Validation loss: 2.5063312193609772

Epoch: 6| Step: 2
Training loss: 2.8394870875152813
Validation loss: 2.486029761805923

Epoch: 6| Step: 3
Training loss: 3.224263330349663
Validation loss: 2.493323932471174

Epoch: 6| Step: 4
Training loss: 2.732293432024028
Validation loss: 2.5086965575991553

Epoch: 6| Step: 5
Training loss: 2.2464417826489096
Validation loss: 2.46988814777528

Epoch: 6| Step: 6
Training loss: 3.565491073192805
Validation loss: 2.486672478653407

Epoch: 6| Step: 7
Training loss: 2.357254995841027
Validation loss: 2.462585547425875

Epoch: 6| Step: 8
Training loss: 2.7575763028500297
Validation loss: 2.5174671180667216

Epoch: 6| Step: 9
Training loss: 2.771453319587943
Validation loss: 2.49309069244748

Epoch: 6| Step: 10
Training loss: 2.451539614627139
Validation loss: 2.4726940730733573

Epoch: 6| Step: 11
Training loss: 2.6761732719216176
Validation loss: 2.4974218077164676

Epoch: 6| Step: 12
Training loss: 3.58563603482436
Validation loss: 2.4881619070826924

Epoch: 6| Step: 13
Training loss: 2.523614835855939
Validation loss: 2.463150959832253

Epoch: 74| Step: 0
Training loss: 3.2241895322628933
Validation loss: 2.465970187884052

Epoch: 6| Step: 1
Training loss: 2.8528504689765715
Validation loss: 2.5018082785454685

Epoch: 6| Step: 2
Training loss: 2.847012335838656
Validation loss: 2.473160895347807

Epoch: 6| Step: 3
Training loss: 2.703660768543473
Validation loss: 2.497904348325507

Epoch: 6| Step: 4
Training loss: 2.6769635560864917
Validation loss: 2.4802315164850146

Epoch: 6| Step: 5
Training loss: 3.120640880590914
Validation loss: 2.4571288947105705

Epoch: 6| Step: 6
Training loss: 2.0044815397718514
Validation loss: 2.472147128053223

Epoch: 6| Step: 7
Training loss: 2.695535047990712
Validation loss: 2.4892446758115265

Epoch: 6| Step: 8
Training loss: 2.47319846814642
Validation loss: 2.5040063617089756

Epoch: 6| Step: 9
Training loss: 3.1980076548071423
Validation loss: 2.478339722157824

Epoch: 6| Step: 10
Training loss: 2.7907564377881764
Validation loss: 2.4837072084663343

Epoch: 6| Step: 11
Training loss: 2.639329490910632
Validation loss: 2.499147117912036

Epoch: 6| Step: 12
Training loss: 2.7273909832140646
Validation loss: 2.491444843292899

Epoch: 6| Step: 13
Training loss: 3.0528527723161623
Validation loss: 2.474166458952118

Epoch: 75| Step: 0
Training loss: 3.2685027515854386
Validation loss: 2.4894287999170155

Epoch: 6| Step: 1
Training loss: 2.981743896007327
Validation loss: 2.485234811470557

Epoch: 6| Step: 2
Training loss: 2.946958370384588
Validation loss: 2.496276504712992

Epoch: 6| Step: 3
Training loss: 2.569838738187536
Validation loss: 2.49828256661278

Epoch: 6| Step: 4
Training loss: 2.4760195741696274
Validation loss: 2.5027236704212497

Epoch: 6| Step: 5
Training loss: 3.0140482679221887
Validation loss: 2.4881429859363227

Epoch: 6| Step: 6
Training loss: 2.4305496893917926
Validation loss: 2.4632130977640307

Epoch: 6| Step: 7
Training loss: 2.5741372450109075
Validation loss: 2.4785995079384926

Epoch: 6| Step: 8
Training loss: 2.876951799049253
Validation loss: 2.5270971436027616

Epoch: 6| Step: 9
Training loss: 2.6456493003300556
Validation loss: 2.478180685543876

Epoch: 6| Step: 10
Training loss: 2.7560514279067077
Validation loss: 2.470241755856523

Epoch: 6| Step: 11
Training loss: 3.106472481755991
Validation loss: 2.4654932031864885

Epoch: 6| Step: 12
Training loss: 2.719743569011012
Validation loss: 2.5194285784011567

Epoch: 6| Step: 13
Training loss: 2.7386112204785755
Validation loss: 2.462727307487607

Epoch: 76| Step: 0
Training loss: 2.600711351064392
Validation loss: 2.4728406145236943

Epoch: 6| Step: 1
Training loss: 2.752910807457873
Validation loss: 2.51052795505244

Epoch: 6| Step: 2
Training loss: 2.8719250364375797
Validation loss: 2.497281673212736

Epoch: 6| Step: 3
Training loss: 2.486119743299031
Validation loss: 2.498714216355553

Epoch: 6| Step: 4
Training loss: 2.5557205024874525
Validation loss: 2.4716208803402426

Epoch: 6| Step: 5
Training loss: 3.2914876446550716
Validation loss: 2.4720714339984

Epoch: 6| Step: 6
Training loss: 2.093200269811122
Validation loss: 2.4671356116952112

Epoch: 6| Step: 7
Training loss: 2.924344411560779
Validation loss: 2.4917670899678015

Epoch: 6| Step: 8
Training loss: 3.1522964039098587
Validation loss: 2.4587998494166032

Epoch: 6| Step: 9
Training loss: 3.1764718556453526
Validation loss: 2.4987454291567626

Epoch: 6| Step: 10
Training loss: 2.242138586275049
Validation loss: 2.5237917529355776

Epoch: 6| Step: 11
Training loss: 2.735947423859206
Validation loss: 2.4722559084995157

Epoch: 6| Step: 12
Training loss: 3.0982433017771274
Validation loss: 2.487417304348997

Epoch: 6| Step: 13
Training loss: 2.877667889241899
Validation loss: 2.512812317993024

Epoch: 77| Step: 0
Training loss: 2.2282899814212893
Validation loss: 2.488443014023638

Epoch: 6| Step: 1
Training loss: 3.4202079718638134
Validation loss: 2.493224433435443

Epoch: 6| Step: 2
Training loss: 2.4998486473044585
Validation loss: 2.486882386109362

Epoch: 6| Step: 3
Training loss: 3.1899930890525647
Validation loss: 2.5062004169760557

Epoch: 6| Step: 4
Training loss: 2.9500577630029063
Validation loss: 2.5060593769937762

Epoch: 6| Step: 5
Training loss: 2.4847307250425463
Validation loss: 2.4567379722237632

Epoch: 6| Step: 6
Training loss: 3.2659037347868995
Validation loss: 2.4993508619042286

Epoch: 6| Step: 7
Training loss: 2.8644600766569814
Validation loss: 2.4754622957302836

Epoch: 6| Step: 8
Training loss: 2.377352402815803
Validation loss: 2.4876697899320748

Epoch: 6| Step: 9
Training loss: 2.79324830263435
Validation loss: 2.475495875512108

Epoch: 6| Step: 10
Training loss: 2.615321892206295
Validation loss: 2.484208451034564

Epoch: 6| Step: 11
Training loss: 2.876865113693246
Validation loss: 2.5015040262406947

Epoch: 6| Step: 12
Training loss: 2.1775473749589076
Validation loss: 2.4604023742413483

Epoch: 6| Step: 13
Training loss: 3.303442321375431
Validation loss: 2.4837332740496847

Epoch: 78| Step: 0
Training loss: 2.6318738039367755
Validation loss: 2.4840774048321346

Epoch: 6| Step: 1
Training loss: 2.7948942363020213
Validation loss: 2.4843292789350055

Epoch: 6| Step: 2
Training loss: 2.7292189459610374
Validation loss: 2.4935507270648016

Epoch: 6| Step: 3
Training loss: 3.5696967530950885
Validation loss: 2.511902879598218

Epoch: 6| Step: 4
Training loss: 2.5950511747984852
Validation loss: 2.4777931795969432

Epoch: 6| Step: 5
Training loss: 2.659622026708151
Validation loss: 2.5475638071087574

Epoch: 6| Step: 6
Training loss: 2.7454249732958615
Validation loss: 2.475406993989

Epoch: 6| Step: 7
Training loss: 2.220109227732443
Validation loss: 2.5069541869227354

Epoch: 6| Step: 8
Training loss: 2.883312647162819
Validation loss: 2.520895558437324

Epoch: 6| Step: 9
Training loss: 2.8435383812272863
Validation loss: 2.4912919984056283

Epoch: 6| Step: 10
Training loss: 2.1376995506610568
Validation loss: 2.4880695121246044

Epoch: 6| Step: 11
Training loss: 3.3649112947060336
Validation loss: 2.485259324556208

Epoch: 6| Step: 12
Training loss: 2.591144678061274
Validation loss: 2.5112765381629205

Epoch: 6| Step: 13
Training loss: 3.342394117163361
Validation loss: 2.5175088198142412

Epoch: 79| Step: 0
Training loss: 3.603425113192547
Validation loss: 2.499008304012015

Epoch: 6| Step: 1
Training loss: 1.7271777821642398
Validation loss: 2.484923257026318

Epoch: 6| Step: 2
Training loss: 2.870531507182603
Validation loss: 2.4778949215645683

Epoch: 6| Step: 3
Training loss: 2.4349536800482907
Validation loss: 2.5037496954025102

Epoch: 6| Step: 4
Training loss: 2.9632148942424763
Validation loss: 2.4786205084828667

Epoch: 6| Step: 5
Training loss: 2.797215957744023
Validation loss: 2.4865971303652286

Epoch: 6| Step: 6
Training loss: 2.6732156524378046
Validation loss: 2.4920152379692655

Epoch: 6| Step: 7
Training loss: 2.0293085063053935
Validation loss: 2.4485859153518605

Epoch: 6| Step: 8
Training loss: 2.564548882863264
Validation loss: 2.499641799901702

Epoch: 6| Step: 9
Training loss: 2.674644328920084
Validation loss: 2.4592198677752353

Epoch: 6| Step: 10
Training loss: 3.2657096888315893
Validation loss: 2.505063283484974

Epoch: 6| Step: 11
Training loss: 3.1093102836583353
Validation loss: 2.49471294669282

Epoch: 6| Step: 12
Training loss: 3.0189421598510906
Validation loss: 2.5000967458238077

Epoch: 6| Step: 13
Training loss: 2.978968331375132
Validation loss: 2.505788204927498

Epoch: 80| Step: 0
Training loss: 2.726236230954499
Validation loss: 2.4811464158661916

Epoch: 6| Step: 1
Training loss: 2.8100904527543475
Validation loss: 2.5003184177290154

Epoch: 6| Step: 2
Training loss: 2.5335187752093686
Validation loss: 2.4734378687340897

Epoch: 6| Step: 3
Training loss: 2.82201218044537
Validation loss: 2.4901892608217118

Epoch: 6| Step: 4
Training loss: 3.030101912644719
Validation loss: 2.5197416026472026

Epoch: 6| Step: 5
Training loss: 3.0723137387960113
Validation loss: 2.5045866207699863

Epoch: 6| Step: 6
Training loss: 2.7475941277608236
Validation loss: 2.489333548835301

Epoch: 6| Step: 7
Training loss: 2.7962292826212893
Validation loss: 2.4703802518178968

Epoch: 6| Step: 8
Training loss: 2.2368261590631358
Validation loss: 2.510002011779311

Epoch: 6| Step: 9
Training loss: 2.835927747809506
Validation loss: 2.467054278727897

Epoch: 6| Step: 10
Training loss: 2.7534838196899196
Validation loss: 2.492938531549839

Epoch: 6| Step: 11
Training loss: 2.6278099279428524
Validation loss: 2.493877925794414

Epoch: 6| Step: 12
Training loss: 3.2579224199737893
Validation loss: 2.484240064350868

Epoch: 6| Step: 13
Training loss: 2.758159669631835
Validation loss: 2.503725668992739

Epoch: 81| Step: 0
Training loss: 2.9432863101104676
Validation loss: 2.4844346888342304

Epoch: 6| Step: 1
Training loss: 2.8714752151453173
Validation loss: 2.48200792935874

Epoch: 6| Step: 2
Training loss: 3.7154414742000132
Validation loss: 2.478455291946959

Epoch: 6| Step: 3
Training loss: 2.1159520221888486
Validation loss: 2.495099755501544

Epoch: 6| Step: 4
Training loss: 1.9858550311957661
Validation loss: 2.4956871938074743

Epoch: 6| Step: 5
Training loss: 2.5832405894049093
Validation loss: 2.4763530087362913

Epoch: 6| Step: 6
Training loss: 2.9571592839892458
Validation loss: 2.500681946148209

Epoch: 6| Step: 7
Training loss: 2.9099025948598807
Validation loss: 2.5047114225685547

Epoch: 6| Step: 8
Training loss: 2.6149424474739544
Validation loss: 2.4533568701739696

Epoch: 6| Step: 9
Training loss: 2.40177878582928
Validation loss: 2.51947254933846

Epoch: 6| Step: 10
Training loss: 3.207730587574483
Validation loss: 2.4898439088115394

Epoch: 6| Step: 11
Training loss: 2.5472401550264454
Validation loss: 2.477558333662352

Epoch: 6| Step: 12
Training loss: 2.8351620961830912
Validation loss: 2.4911640606758865

Epoch: 6| Step: 13
Training loss: 2.883951267185183
Validation loss: 2.4986880777392053

Epoch: 82| Step: 0
Training loss: 2.8929414282736308
Validation loss: 2.483391624235101

Epoch: 6| Step: 1
Training loss: 2.3401956691606998
Validation loss: 2.466197481309865

Epoch: 6| Step: 2
Training loss: 3.141258972859499
Validation loss: 2.5108108759040273

Epoch: 6| Step: 3
Training loss: 3.0812919319812546
Validation loss: 2.4986038544697458

Epoch: 6| Step: 4
Training loss: 3.822482438680203
Validation loss: 2.4896742370405667

Epoch: 6| Step: 5
Training loss: 2.563232968336956
Validation loss: 2.5071371796722097

Epoch: 6| Step: 6
Training loss: 2.6053293811406304
Validation loss: 2.502277484366119

Epoch: 6| Step: 7
Training loss: 2.3217715418730083
Validation loss: 2.4984226121418938

Epoch: 6| Step: 8
Training loss: 2.3559901734229762
Validation loss: 2.49484110832526

Epoch: 6| Step: 9
Training loss: 2.1394725955385705
Validation loss: 2.4918825761051737

Epoch: 6| Step: 10
Training loss: 3.3093232090457274
Validation loss: 2.5113554125115995

Epoch: 6| Step: 11
Training loss: 2.985512721310436
Validation loss: 2.49012068306975

Epoch: 6| Step: 12
Training loss: 1.896965806474924
Validation loss: 2.4955811877538956

Epoch: 6| Step: 13
Training loss: 3.278018631859476
Validation loss: 2.506640381956149

Epoch: 83| Step: 0
Training loss: 3.222655806107924
Validation loss: 2.5046419650397826

Epoch: 6| Step: 1
Training loss: 2.518209989225522
Validation loss: 2.511186374421868

Epoch: 6| Step: 2
Training loss: 2.7065531430921204
Validation loss: 2.474059553596148

Epoch: 6| Step: 3
Training loss: 3.0707468594332616
Validation loss: 2.5135347230577696

Epoch: 6| Step: 4
Training loss: 2.477062379466613
Validation loss: 2.5066865674328307

Epoch: 6| Step: 5
Training loss: 3.104917158167353
Validation loss: 2.4840882653686056

Epoch: 6| Step: 6
Training loss: 3.1458199271280636
Validation loss: 2.4988769603218084

Epoch: 6| Step: 7
Training loss: 2.8168202597219145
Validation loss: 2.480645619039817

Epoch: 6| Step: 8
Training loss: 1.9725319398293277
Validation loss: 2.4923198648213116

Epoch: 6| Step: 9
Training loss: 2.617500491429149
Validation loss: 2.5137841958214144

Epoch: 6| Step: 10
Training loss: 2.539735957110904
Validation loss: 2.4837047270968937

Epoch: 6| Step: 11
Training loss: 2.862146087490164
Validation loss: 2.4819993589409557

Epoch: 6| Step: 12
Training loss: 2.7178164227534
Validation loss: 2.478560008353334

Epoch: 6| Step: 13
Training loss: 3.1970820295580693
Validation loss: 2.491616306200009

Epoch: 84| Step: 0
Training loss: 3.319358203854782
Validation loss: 2.5006570219303033

Epoch: 6| Step: 1
Training loss: 1.5912716613397933
Validation loss: 2.5068771217219346

Epoch: 6| Step: 2
Training loss: 2.990513106498067
Validation loss: 2.480189795743788

Epoch: 6| Step: 3
Training loss: 3.5166448173627622
Validation loss: 2.4879970376954246

Epoch: 6| Step: 4
Training loss: 2.1179505271789028
Validation loss: 2.472553537649883

Epoch: 6| Step: 5
Training loss: 2.3051624907773247
Validation loss: 2.4983515852616955

Epoch: 6| Step: 6
Training loss: 3.5053482065544266
Validation loss: 2.5033491568614656

Epoch: 6| Step: 7
Training loss: 2.97960693378479
Validation loss: 2.4942846833584356

Epoch: 6| Step: 8
Training loss: 2.6265760867319434
Validation loss: 2.4900877051176504

Epoch: 6| Step: 9
Training loss: 2.421151034528976
Validation loss: 2.503700276415869

Epoch: 6| Step: 10
Training loss: 2.634035454675821
Validation loss: 2.497116737605177

Epoch: 6| Step: 11
Training loss: 3.2211053619064436
Validation loss: 2.47599982096103

Epoch: 6| Step: 12
Training loss: 2.036740672904764
Validation loss: 2.467161750525303

Epoch: 6| Step: 13
Training loss: 2.909159526774302
Validation loss: 2.4927793989258804

Epoch: 85| Step: 0
Training loss: 2.525517884578755
Validation loss: 2.4888923631108897

Epoch: 6| Step: 1
Training loss: 2.841441632381692
Validation loss: 2.502422543386323

Epoch: 6| Step: 2
Training loss: 2.9833001403098636
Validation loss: 2.5200420548975866

Epoch: 6| Step: 3
Training loss: 2.609654942646199
Validation loss: 2.49233970417767

Epoch: 6| Step: 4
Training loss: 2.6704909102658902
Validation loss: 2.472109569867882

Epoch: 6| Step: 5
Training loss: 3.320313361672682
Validation loss: 2.47227703758983

Epoch: 6| Step: 6
Training loss: 3.188217979799805
Validation loss: 2.5122679083881723

Epoch: 6| Step: 7
Training loss: 2.71041768391908
Validation loss: 2.491539583564051

Epoch: 6| Step: 8
Training loss: 2.3474422617546375
Validation loss: 2.479662973663471

Epoch: 6| Step: 9
Training loss: 2.342878052322442
Validation loss: 2.482945731797059

Epoch: 6| Step: 10
Training loss: 2.607534341851149
Validation loss: 2.4944426716575765

Epoch: 6| Step: 11
Training loss: 3.2234186183951508
Validation loss: 2.4889105245820065

Epoch: 6| Step: 12
Training loss: 2.7817924056329417
Validation loss: 2.4905763810515817

Epoch: 6| Step: 13
Training loss: 2.3913375690097625
Validation loss: 2.4784158231637456

Epoch: 86| Step: 0
Training loss: 2.6103451689853427
Validation loss: 2.499771271773743

Epoch: 6| Step: 1
Training loss: 2.6991625723396035
Validation loss: 2.5035452915370247

Epoch: 6| Step: 2
Training loss: 2.897049975331857
Validation loss: 2.4980779868443412

Epoch: 6| Step: 3
Training loss: 2.551116407261903
Validation loss: 2.4877390164539674

Epoch: 6| Step: 4
Training loss: 2.299461724692898
Validation loss: 2.5013405394942403

Epoch: 6| Step: 5
Training loss: 2.999111679801191
Validation loss: 2.5290194859563075

Epoch: 6| Step: 6
Training loss: 3.0286882724997533
Validation loss: 2.4868863162800263

Epoch: 6| Step: 7
Training loss: 3.0004130714867143
Validation loss: 2.503509524354427

Epoch: 6| Step: 8
Training loss: 2.620468315173152
Validation loss: 2.4982566938764825

Epoch: 6| Step: 9
Training loss: 2.5054660170242538
Validation loss: 2.4655013635896017

Epoch: 6| Step: 10
Training loss: 2.89063786684214
Validation loss: 2.44802965799267

Epoch: 6| Step: 11
Training loss: 2.8075495915775877
Validation loss: 2.4999414960106985

Epoch: 6| Step: 12
Training loss: 3.108356865051485
Validation loss: 2.4918244720750726

Epoch: 6| Step: 13
Training loss: 2.6034234067091027
Validation loss: 2.4809559261062044

Epoch: 87| Step: 0
Training loss: 2.2175603551725502
Validation loss: 2.4867970949793543

Epoch: 6| Step: 1
Training loss: 2.3951751689523295
Validation loss: 2.4977613855197203

Epoch: 6| Step: 2
Training loss: 2.9677830777854286
Validation loss: 2.473687241904629

Epoch: 6| Step: 3
Training loss: 2.6973218421684977
Validation loss: 2.4915376182910216

Epoch: 6| Step: 4
Training loss: 3.0720085914412207
Validation loss: 2.445363649354055

Epoch: 6| Step: 5
Training loss: 3.0997665932678315
Validation loss: 2.515379862993214

Epoch: 6| Step: 6
Training loss: 2.842387784891806
Validation loss: 2.469515145941116

Epoch: 6| Step: 7
Training loss: 2.5709494871075917
Validation loss: 2.4731750032247346

Epoch: 6| Step: 8
Training loss: 3.3548464036964525
Validation loss: 2.4988945352460585

Epoch: 6| Step: 9
Training loss: 2.410384750045003
Validation loss: 2.487334800994102

Epoch: 6| Step: 10
Training loss: 1.740121137743113
Validation loss: 2.480161239014903

Epoch: 6| Step: 11
Training loss: 3.1434855173540948
Validation loss: 2.5072853634062233

Epoch: 6| Step: 12
Training loss: 3.3501818821055784
Validation loss: 2.5011185522797232

Epoch: 6| Step: 13
Training loss: 2.5173518728035553
Validation loss: 2.4811080531219547

Epoch: 88| Step: 0
Training loss: 2.811884833982373
Validation loss: 2.491434458316042

Epoch: 6| Step: 1
Training loss: 2.0467480336188326
Validation loss: 2.4841839487349424

Epoch: 6| Step: 2
Training loss: 2.2727091563109627
Validation loss: 2.480021939235187

Epoch: 6| Step: 3
Training loss: 2.9626978168354996
Validation loss: 2.4992894813979656

Epoch: 6| Step: 4
Training loss: 3.036985810639536
Validation loss: 2.4968840787853606

Epoch: 6| Step: 5
Training loss: 2.9622653208302685
Validation loss: 2.5085489845587294

Epoch: 6| Step: 6
Training loss: 2.4675567254036364
Validation loss: 2.5028112023516482

Epoch: 6| Step: 7
Training loss: 3.189651361624938
Validation loss: 2.5256115882921977

Epoch: 6| Step: 8
Training loss: 2.7666365503104435
Validation loss: 2.4889573902887348

Epoch: 6| Step: 9
Training loss: 2.4253499358652078
Validation loss: 2.4909278519471116

Epoch: 6| Step: 10
Training loss: 2.8871647099283857
Validation loss: 2.5298640260932044

Epoch: 6| Step: 11
Training loss: 2.779758953786462
Validation loss: 2.4922376780589466

Epoch: 6| Step: 12
Training loss: 2.744997329293708
Validation loss: 2.487808149336677

Epoch: 6| Step: 13
Training loss: 3.473514618148344
Validation loss: 2.4612179519695943

Epoch: 89| Step: 0
Training loss: 2.4955817280989003
Validation loss: 2.4816497262544432

Epoch: 6| Step: 1
Training loss: 2.8924684986716978
Validation loss: 2.4777648021783083

Epoch: 6| Step: 2
Training loss: 3.2927920835987425
Validation loss: 2.4877619075888004

Epoch: 6| Step: 3
Training loss: 2.6317219725441143
Validation loss: 2.4910229268012904

Epoch: 6| Step: 4
Training loss: 2.552752307224697
Validation loss: 2.5046857810302257

Epoch: 6| Step: 5
Training loss: 2.631416108965929
Validation loss: 2.4905958405779973

Epoch: 6| Step: 6
Training loss: 2.415383381868951
Validation loss: 2.4925892314806184

Epoch: 6| Step: 7
Training loss: 2.1066734816445
Validation loss: 2.481404886029704

Epoch: 6| Step: 8
Training loss: 3.2569110455638515
Validation loss: 2.493789536489149

Epoch: 6| Step: 9
Training loss: 2.6180850183890483
Validation loss: 2.5164191352345813

Epoch: 6| Step: 10
Training loss: 2.9599772081270888
Validation loss: 2.493176584715335

Epoch: 6| Step: 11
Training loss: 2.6219802926237312
Validation loss: 2.469733355333467

Epoch: 6| Step: 12
Training loss: 3.365983716856205
Validation loss: 2.479452670083331

Epoch: 6| Step: 13
Training loss: 2.8445949766275493
Validation loss: 2.5131273527086146

Epoch: 90| Step: 0
Training loss: 2.6478306798131865
Validation loss: 2.502446437859648

Epoch: 6| Step: 1
Training loss: 2.595948356255796
Validation loss: 2.498029015994127

Epoch: 6| Step: 2
Training loss: 3.120692068518012
Validation loss: 2.499664784589403

Epoch: 6| Step: 3
Training loss: 2.150280304312784
Validation loss: 2.4540258660284664

Epoch: 6| Step: 4
Training loss: 2.7504457632659256
Validation loss: 2.4824606336197754

Epoch: 6| Step: 5
Training loss: 2.9932964451250794
Validation loss: 2.467106045544192

Epoch: 6| Step: 6
Training loss: 2.7307986167988028
Validation loss: 2.4799971206015186

Epoch: 6| Step: 7
Training loss: 3.043354367465156
Validation loss: 2.4755437791174715

Epoch: 6| Step: 8
Training loss: 3.0572684621282322
Validation loss: 2.488030381326389

Epoch: 6| Step: 9
Training loss: 2.0477195336999374
Validation loss: 2.509060908495067

Epoch: 6| Step: 10
Training loss: 3.1728598282536993
Validation loss: 2.496420628772917

Epoch: 6| Step: 11
Training loss: 2.889789852434836
Validation loss: 2.500311055367264

Epoch: 6| Step: 12
Training loss: 2.658548146321103
Validation loss: 2.4783335280516763

Epoch: 6| Step: 13
Training loss: 2.5056634649462906
Validation loss: 2.4874070051065402

Epoch: 91| Step: 0
Training loss: 3.421645622752332
Validation loss: 2.516548655181805

Epoch: 6| Step: 1
Training loss: 2.377817690990656
Validation loss: 2.5011647669467454

Epoch: 6| Step: 2
Training loss: 2.704613424660809
Validation loss: 2.500576112877076

Epoch: 6| Step: 3
Training loss: 2.242918314112607
Validation loss: 2.492479476260824

Epoch: 6| Step: 4
Training loss: 2.670362613357296
Validation loss: 2.4475227658676286

Epoch: 6| Step: 5
Training loss: 2.8962866522091932
Validation loss: 2.470405752352137

Epoch: 6| Step: 6
Training loss: 3.0395038352925794
Validation loss: 2.5070183243069346

Epoch: 6| Step: 7
Training loss: 2.753748679556857
Validation loss: 2.4781242351258985

Epoch: 6| Step: 8
Training loss: 2.6164394846981276
Validation loss: 2.523603565874358

Epoch: 6| Step: 9
Training loss: 2.7359799279932293
Validation loss: 2.498731477962586

Epoch: 6| Step: 10
Training loss: 2.7373539855197784
Validation loss: 2.499491232714392

Epoch: 6| Step: 11
Training loss: 3.349785891850438
Validation loss: 2.474240469957916

Epoch: 6| Step: 12
Training loss: 2.5990357371533945
Validation loss: 2.5181605903448836

Epoch: 6| Step: 13
Training loss: 2.6286890356782724
Validation loss: 2.4823481006928985

Epoch: 92| Step: 0
Training loss: 2.4664251269396598
Validation loss: 2.5097042729500902

Epoch: 6| Step: 1
Training loss: 2.3946381835956916
Validation loss: 2.4969253532292295

Epoch: 6| Step: 2
Training loss: 2.7505799462274094
Validation loss: 2.4897874890979512

Epoch: 6| Step: 3
Training loss: 2.374540284487207
Validation loss: 2.4889085304523624

Epoch: 6| Step: 4
Training loss: 3.0791467819982747
Validation loss: 2.520246112076955

Epoch: 6| Step: 5
Training loss: 3.022355075640033
Validation loss: 2.4855446549311817

Epoch: 6| Step: 6
Training loss: 2.580768686268478
Validation loss: 2.4931308553945053

Epoch: 6| Step: 7
Training loss: 3.6116246730426824
Validation loss: 2.4834688112388785

Epoch: 6| Step: 8
Training loss: 2.4999151215449693
Validation loss: 2.4914008170734623

Epoch: 6| Step: 9
Training loss: 2.5181907695589882
Validation loss: 2.4625228106638044

Epoch: 6| Step: 10
Training loss: 2.4812010637737663
Validation loss: 2.4783363685705453

Epoch: 6| Step: 11
Training loss: 3.0807584544595703
Validation loss: 2.478579674961966

Epoch: 6| Step: 12
Training loss: 2.7055251210106923
Validation loss: 2.4837290276892965

Epoch: 6| Step: 13
Training loss: 3.2010279792437566
Validation loss: 2.4951363403646067

Epoch: 93| Step: 0
Training loss: 2.0686927072941295
Validation loss: 2.491430769928162

Epoch: 6| Step: 1
Training loss: 2.8724156249613273
Validation loss: 2.4945341296670556

Epoch: 6| Step: 2
Training loss: 3.092156201975733
Validation loss: 2.5038317111150366

Epoch: 6| Step: 3
Training loss: 2.983715044319454
Validation loss: 2.497379988176827

Epoch: 6| Step: 4
Training loss: 2.326661539627737
Validation loss: 2.4963442540498213

Epoch: 6| Step: 5
Training loss: 2.584885489843886
Validation loss: 2.494041730061818

Epoch: 6| Step: 6
Training loss: 3.1884306502724313
Validation loss: 2.4797018446738295

Epoch: 6| Step: 7
Training loss: 3.1469695879179183
Validation loss: 2.491602568200283

Epoch: 6| Step: 8
Training loss: 2.469815953614067
Validation loss: 2.4616364663117842

Epoch: 6| Step: 9
Training loss: 2.8007134890905268
Validation loss: 2.483690376599294

Epoch: 6| Step: 10
Training loss: 2.7946431722775746
Validation loss: 2.49896052705473

Epoch: 6| Step: 11
Training loss: 3.1094955344296116
Validation loss: 2.5091742244799846

Epoch: 6| Step: 12
Training loss: 2.913927008621229
Validation loss: 2.5115657129445683

Epoch: 6| Step: 13
Training loss: 1.9883929447959798
Validation loss: 2.4887831937260994

Epoch: 94| Step: 0
Training loss: 2.2707034315932293
Validation loss: 2.475389554716658

Epoch: 6| Step: 1
Training loss: 3.091255955179404
Validation loss: 2.508907219868355

Epoch: 6| Step: 2
Training loss: 3.2744472648741643
Validation loss: 2.4879994447166296

Epoch: 6| Step: 3
Training loss: 3.0068007315655723
Validation loss: 2.50416155095537

Epoch: 6| Step: 4
Training loss: 2.8805369258103655
Validation loss: 2.468216428863264

Epoch: 6| Step: 5
Training loss: 2.9773015584085782
Validation loss: 2.5135695647967644

Epoch: 6| Step: 6
Training loss: 2.687111227614658
Validation loss: 2.5227227729859427

Epoch: 6| Step: 7
Training loss: 2.6696239603816396
Validation loss: 2.50269918549605

Epoch: 6| Step: 8
Training loss: 2.357650025206757
Validation loss: 2.482152131044153

Epoch: 6| Step: 9
Training loss: 2.7052189649143608
Validation loss: 2.5044985699891718

Epoch: 6| Step: 10
Training loss: 2.781163246430511
Validation loss: 2.4815605403399017

Epoch: 6| Step: 11
Training loss: 2.8574387294978822
Validation loss: 2.4880575329746515

Epoch: 6| Step: 12
Training loss: 2.9783742602211283
Validation loss: 2.50304521373571

Epoch: 6| Step: 13
Training loss: 1.794473054413057
Validation loss: 2.4956363146687206

Epoch: 95| Step: 0
Training loss: 2.5848225843689105
Validation loss: 2.498232200061074

Epoch: 6| Step: 1
Training loss: 2.961692693621368
Validation loss: 2.506468218269867

Epoch: 6| Step: 2
Training loss: 2.4171184468284825
Validation loss: 2.5111956695920807

Epoch: 6| Step: 3
Training loss: 2.846754226886455
Validation loss: 2.495734363687488

Epoch: 6| Step: 4
Training loss: 2.9992318759480945
Validation loss: 2.4821165689108855

Epoch: 6| Step: 5
Training loss: 2.6027401386616824
Validation loss: 2.5010948603718575

Epoch: 6| Step: 6
Training loss: 2.720968108955188
Validation loss: 2.5008213129741645

Epoch: 6| Step: 7
Training loss: 2.8494865490364485
Validation loss: 2.465354070812172

Epoch: 6| Step: 8
Training loss: 3.0126459300045125
Validation loss: 2.487093706569198

Epoch: 6| Step: 9
Training loss: 2.592983546247847
Validation loss: 2.4515946223759912

Epoch: 6| Step: 10
Training loss: 3.4758991422957184
Validation loss: 2.5215318756561698

Epoch: 6| Step: 11
Training loss: 2.5002894234018482
Validation loss: 2.4743221272384517

Epoch: 6| Step: 12
Training loss: 2.6254385627237524
Validation loss: 2.525345590683807

Epoch: 6| Step: 13
Training loss: 2.4058759324262304
Validation loss: 2.4763413373176633

Epoch: 96| Step: 0
Training loss: 2.4240980220905106
Validation loss: 2.517254228898342

Epoch: 6| Step: 1
Training loss: 3.326989208427518
Validation loss: 2.492987492362218

Epoch: 6| Step: 2
Training loss: 2.99749826388273
Validation loss: 2.4825901960577386

Epoch: 6| Step: 3
Training loss: 2.224355467120781
Validation loss: 2.4786913260873145

Epoch: 6| Step: 4
Training loss: 3.120169912259919
Validation loss: 2.4675071124562504

Epoch: 6| Step: 5
Training loss: 2.13104085035834
Validation loss: 2.4981840172782466

Epoch: 6| Step: 6
Training loss: 2.7779632177973155
Validation loss: 2.496901523994868

Epoch: 6| Step: 7
Training loss: 3.107319214549873
Validation loss: 2.4844238231048776

Epoch: 6| Step: 8
Training loss: 2.7338128084172233
Validation loss: 2.4983893153686676

Epoch: 6| Step: 9
Training loss: 2.40077954983364
Validation loss: 2.4904054580140293

Epoch: 6| Step: 10
Training loss: 2.398702302411807
Validation loss: 2.475043778349791

Epoch: 6| Step: 11
Training loss: 3.4271931886699676
Validation loss: 2.491441012406797

Epoch: 6| Step: 12
Training loss: 2.7171708705891184
Validation loss: 2.4766565674240084

Epoch: 6| Step: 13
Training loss: 2.51022489034181
Validation loss: 2.4808992097136904

Epoch: 97| Step: 0
Training loss: 2.999391494072889
Validation loss: 2.471749591592878

Epoch: 6| Step: 1
Training loss: 2.642855723391708
Validation loss: 2.5153845971090116

Epoch: 6| Step: 2
Training loss: 3.5104744714945086
Validation loss: 2.4901874674385076

Epoch: 6| Step: 3
Training loss: 2.2203433266203736
Validation loss: 2.4998147577956753

Epoch: 6| Step: 4
Training loss: 2.580924254176852
Validation loss: 2.4744147267799335

Epoch: 6| Step: 5
Training loss: 2.7116605910139553
Validation loss: 2.500277017812036

Epoch: 6| Step: 6
Training loss: 2.056797699033562
Validation loss: 2.5071278612748293

Epoch: 6| Step: 7
Training loss: 2.931611998111398
Validation loss: 2.4545419385357

Epoch: 6| Step: 8
Training loss: 2.975030780280386
Validation loss: 2.5103554442244618

Epoch: 6| Step: 9
Training loss: 3.0416045160780785
Validation loss: 2.5122525893648993

Epoch: 6| Step: 10
Training loss: 2.527341108600822
Validation loss: 2.490647534964434

Epoch: 6| Step: 11
Training loss: 2.610867376201963
Validation loss: 2.511645870930902

Epoch: 6| Step: 12
Training loss: 2.703988438999133
Validation loss: 2.5190641384351795

Epoch: 6| Step: 13
Training loss: 3.303279206890746
Validation loss: 2.4961328468263075

Epoch: 98| Step: 0
Training loss: 2.8003319202963364
Validation loss: 2.4656502300562257

Epoch: 6| Step: 1
Training loss: 2.4893136510202
Validation loss: 2.5099314606796885

Epoch: 6| Step: 2
Training loss: 3.0240477899680656
Validation loss: 2.485561616576316

Epoch: 6| Step: 3
Training loss: 3.1731871347916476
Validation loss: 2.495887671966599

Epoch: 6| Step: 4
Training loss: 2.8885926396333805
Validation loss: 2.4951143583789763

Epoch: 6| Step: 5
Training loss: 2.883452388385566
Validation loss: 2.469253986006826

Epoch: 6| Step: 6
Training loss: 3.1506975642866077
Validation loss: 2.4912195981725107

Epoch: 6| Step: 7
Training loss: 2.4031376713094996
Validation loss: 2.5162757103792717

Epoch: 6| Step: 8
Training loss: 3.0200763323907553
Validation loss: 2.4962687109052477

Epoch: 6| Step: 9
Training loss: 2.685809291156376
Validation loss: 2.4961640667434666

Epoch: 6| Step: 10
Training loss: 2.8683354628837816
Validation loss: 2.489409599020815

Epoch: 6| Step: 11
Training loss: 2.2420785060004236
Validation loss: 2.494048128768901

Epoch: 6| Step: 12
Training loss: 2.1231371624373354
Validation loss: 2.4876643229168907

Epoch: 6| Step: 13
Training loss: 2.674689344389224
Validation loss: 2.4979819530741336

Epoch: 99| Step: 0
Training loss: 2.769875401472972
Validation loss: 2.4774965606963923

Epoch: 6| Step: 1
Training loss: 2.926152163780227
Validation loss: 2.478393765801271

Epoch: 6| Step: 2
Training loss: 2.4642316780721147
Validation loss: 2.5050454704070177

Epoch: 6| Step: 3
Training loss: 3.539104133007394
Validation loss: 2.520324572977172

Epoch: 6| Step: 4
Training loss: 3.1698598990783733
Validation loss: 2.494293439221945

Epoch: 6| Step: 5
Training loss: 2.1798710352696817
Validation loss: 2.464932292705669

Epoch: 6| Step: 6
Training loss: 2.6483373032242645
Validation loss: 2.4831309945602826

Epoch: 6| Step: 7
Training loss: 2.7084962257892746
Validation loss: 2.483508505268899

Epoch: 6| Step: 8
Training loss: 3.301873490237357
Validation loss: 2.4901831692846415

Epoch: 6| Step: 9
Training loss: 2.20019032782114
Validation loss: 2.504944354279212

Epoch: 6| Step: 10
Training loss: 1.8601774519697771
Validation loss: 2.4544233088680434

Epoch: 6| Step: 11
Training loss: 2.4516354065576333
Validation loss: 2.508543817515757

Epoch: 6| Step: 12
Training loss: 2.767212320622468
Validation loss: 2.4951190862509347

Epoch: 6| Step: 13
Training loss: 3.5990174542147177
Validation loss: 2.491861185271838

Epoch: 100| Step: 0
Training loss: 2.93215813792933
Validation loss: 2.50572414246056

Epoch: 6| Step: 1
Training loss: 2.5808077638874707
Validation loss: 2.491329360531902

Epoch: 6| Step: 2
Training loss: 2.5134567488801576
Validation loss: 2.506815814540018

Epoch: 6| Step: 3
Training loss: 2.445627990455661
Validation loss: 2.4990154994060845

Epoch: 6| Step: 4
Training loss: 3.1520179097978684
Validation loss: 2.49089596122929

Epoch: 6| Step: 5
Training loss: 2.9997680892316265
Validation loss: 2.4820433861460063

Epoch: 6| Step: 6
Training loss: 2.1438726584707695
Validation loss: 2.5038408216446193

Epoch: 6| Step: 7
Training loss: 2.5799554973283123
Validation loss: 2.4903033593419117

Epoch: 6| Step: 8
Training loss: 2.2358280605614653
Validation loss: 2.481025994038998

Epoch: 6| Step: 9
Training loss: 3.139889911009505
Validation loss: 2.471530684239302

Epoch: 6| Step: 10
Training loss: 2.5779030444229885
Validation loss: 2.467915716050288

Epoch: 6| Step: 11
Training loss: 2.8033158287321314
Validation loss: 2.5262277666320663

Epoch: 6| Step: 12
Training loss: 3.7716211721857076
Validation loss: 2.4917241703248583

Epoch: 6| Step: 13
Training loss: 2.1370921811120054
Validation loss: 2.480905704835321

Epoch: 101| Step: 0
Training loss: 2.683021984242027
Validation loss: 2.5110585743383167

Epoch: 6| Step: 1
Training loss: 2.0029286161854154
Validation loss: 2.4880323627648053

Epoch: 6| Step: 2
Training loss: 3.1835152645616867
Validation loss: 2.497873379030875

Epoch: 6| Step: 3
Training loss: 2.6599018798743463
Validation loss: 2.4985053260164287

Epoch: 6| Step: 4
Training loss: 2.9567606511135853
Validation loss: 2.5078057479524714

Epoch: 6| Step: 5
Training loss: 3.012850577188299
Validation loss: 2.4816320643259875

Epoch: 6| Step: 6
Training loss: 2.916915683107065
Validation loss: 2.5092168476867034

Epoch: 6| Step: 7
Training loss: 3.2461016823571853
Validation loss: 2.477393370187066

Epoch: 6| Step: 8
Training loss: 2.851990215675295
Validation loss: 2.4784055599565735

Epoch: 6| Step: 9
Training loss: 3.1082427297142172
Validation loss: 2.495828816843275

Epoch: 6| Step: 10
Training loss: 2.572573516655295
Validation loss: 2.49457954551217

Epoch: 6| Step: 11
Training loss: 2.179977999234955
Validation loss: 2.4954029887389564

Epoch: 6| Step: 12
Training loss: 2.5949856675768364
Validation loss: 2.473019696047947

Epoch: 6| Step: 13
Training loss: 2.6660785821321014
Validation loss: 2.5079158280825777

Epoch: 102| Step: 0
Training loss: 3.4769347259476957
Validation loss: 2.48306210004235

Epoch: 6| Step: 1
Training loss: 2.940328332122723
Validation loss: 2.526499955409744

Epoch: 6| Step: 2
Training loss: 2.597486814551716
Validation loss: 2.502235417613676

Epoch: 6| Step: 3
Training loss: 2.7212484871575806
Validation loss: 2.4866378074868756

Epoch: 6| Step: 4
Training loss: 2.9611399236648666
Validation loss: 2.468932525504229

Epoch: 6| Step: 5
Training loss: 2.605481469439296
Validation loss: 2.5078560663701293

Epoch: 6| Step: 6
Training loss: 2.4378086774952807
Validation loss: 2.5083073965420164

Epoch: 6| Step: 7
Training loss: 2.8223029639074126
Validation loss: 2.5001960164821484

Epoch: 6| Step: 8
Training loss: 2.638817162682251
Validation loss: 2.4961468341122957

Epoch: 6| Step: 9
Training loss: 2.7766462649374013
Validation loss: 2.505767537451313

Epoch: 6| Step: 10
Training loss: 2.6433255984500637
Validation loss: 2.503325979677694

Epoch: 6| Step: 11
Training loss: 2.5250815123440935
Validation loss: 2.4916944792446336

Epoch: 6| Step: 12
Training loss: 2.7810738325581887
Validation loss: 2.501510623125144

Epoch: 6| Step: 13
Training loss: 2.977713935398579
Validation loss: 2.5144478350586295

Epoch: 103| Step: 0
Training loss: 2.849003058622358
Validation loss: 2.480274969891481

Epoch: 6| Step: 1
Training loss: 2.8020695871538313
Validation loss: 2.4979944429275918

Epoch: 6| Step: 2
Training loss: 3.1419644512299896
Validation loss: 2.5214925829577295

Epoch: 6| Step: 3
Training loss: 2.912240693824027
Validation loss: 2.507412886513316

Epoch: 6| Step: 4
Training loss: 2.5664135890722215
Validation loss: 2.50246241723821

Epoch: 6| Step: 5
Training loss: 2.541495322616989
Validation loss: 2.506939637161515

Epoch: 6| Step: 6
Training loss: 2.3941122313844945
Validation loss: 2.5089815354191822

Epoch: 6| Step: 7
Training loss: 3.6339393098725403
Validation loss: 2.512497885579739

Epoch: 6| Step: 8
Training loss: 2.5350127333823482
Validation loss: 2.487566282506739

Epoch: 6| Step: 9
Training loss: 2.199726053001135
Validation loss: 2.5307171036694474

Epoch: 6| Step: 10
Training loss: 3.192927358726319
Validation loss: 2.5402563123560964

Epoch: 6| Step: 11
Training loss: 3.117172814815033
Validation loss: 2.4880950559803687

Epoch: 6| Step: 12
Training loss: 2.0965105902715706
Validation loss: 2.501179179130859

Epoch: 6| Step: 13
Training loss: 2.546316729248472
Validation loss: 2.5154336479355357

Epoch: 104| Step: 0
Training loss: 3.1505077739959297
Validation loss: 2.499033476516238

Epoch: 6| Step: 1
Training loss: 2.4629743593579567
Validation loss: 2.493102149719537

Epoch: 6| Step: 2
Training loss: 3.354673990934025
Validation loss: 2.4839960677242754

Epoch: 6| Step: 3
Training loss: 2.7965178021840176
Validation loss: 2.4968969858495718

Epoch: 6| Step: 4
Training loss: 2.714280236030729
Validation loss: 2.502546884558278

Epoch: 6| Step: 5
Training loss: 2.8686725813780822
Validation loss: 2.4970487443467952

Epoch: 6| Step: 6
Training loss: 3.126453214352981
Validation loss: 2.5015472238859013

Epoch: 6| Step: 7
Training loss: 2.7837669042477424
Validation loss: 2.489882930797915

Epoch: 6| Step: 8
Training loss: 3.0141286349320726
Validation loss: 2.507770606343906

Epoch: 6| Step: 9
Training loss: 2.6218633530986653
Validation loss: 2.504351364803795

Epoch: 6| Step: 10
Training loss: 2.230927268230691
Validation loss: 2.521172641938435

Epoch: 6| Step: 11
Training loss: 2.641041378622954
Validation loss: 2.474442598692919

Epoch: 6| Step: 12
Training loss: 2.559002419950974
Validation loss: 2.463976591002894

Epoch: 6| Step: 13
Training loss: 1.7559763494953975
Validation loss: 2.49352332576717

Epoch: 105| Step: 0
Training loss: 2.944798594195531
Validation loss: 2.4967246534322025

Epoch: 6| Step: 1
Training loss: 2.7012017966245407
Validation loss: 2.485103429960697

Epoch: 6| Step: 2
Training loss: 2.5901601376607597
Validation loss: 2.471144691719176

Epoch: 6| Step: 3
Training loss: 2.972536422981173
Validation loss: 2.502870185715842

Epoch: 6| Step: 4
Training loss: 3.173233417840024
Validation loss: 2.479365398672604

Epoch: 6| Step: 5
Training loss: 2.930972049117828
Validation loss: 2.494607970074733

Epoch: 6| Step: 6
Training loss: 2.4944576817675523
Validation loss: 2.5236827431851654

Epoch: 6| Step: 7
Training loss: 3.1479617533220474
Validation loss: 2.5115653179204607

Epoch: 6| Step: 8
Training loss: 2.2885199935736478
Validation loss: 2.489219103071972

Epoch: 6| Step: 9
Training loss: 2.5979293794379545
Validation loss: 2.4887356428145297

Epoch: 6| Step: 10
Training loss: 2.8237441047921013
Validation loss: 2.5006858807729353

Epoch: 6| Step: 11
Training loss: 2.7041230757089334
Validation loss: 2.4783833204482977

Epoch: 6| Step: 12
Training loss: 3.0214518648259086
Validation loss: 2.486805462785506

Epoch: 6| Step: 13
Training loss: 1.351979670888098
Validation loss: 2.4820966989673985

Epoch: 106| Step: 0
Training loss: 1.581710745288039
Validation loss: 2.5017979668165076

Epoch: 6| Step: 1
Training loss: 3.102124218557084
Validation loss: 2.4997945783162825

Epoch: 6| Step: 2
Training loss: 2.2651763471790844
Validation loss: 2.4700554853591448

Epoch: 6| Step: 3
Training loss: 2.817374942444597
Validation loss: 2.495897321983096

Epoch: 6| Step: 4
Training loss: 2.377244190208564
Validation loss: 2.4877744776058

Epoch: 6| Step: 5
Training loss: 2.4391237377076878
Validation loss: 2.4919056765997816

Epoch: 6| Step: 6
Training loss: 3.256750285839614
Validation loss: 2.482993138566595

Epoch: 6| Step: 7
Training loss: 2.9807563932913843
Validation loss: 2.501625724019606

Epoch: 6| Step: 8
Training loss: 3.009902980672959
Validation loss: 2.486872559882375

Epoch: 6| Step: 9
Training loss: 3.0454777570484337
Validation loss: 2.5080467771239086

Epoch: 6| Step: 10
Training loss: 2.7411350936916037
Validation loss: 2.5143716214903518

Epoch: 6| Step: 11
Training loss: 2.4974468545538264
Validation loss: 2.516427949067621

Epoch: 6| Step: 12
Training loss: 2.187003815098606
Validation loss: 2.527742802866549

Epoch: 6| Step: 13
Training loss: 4.101791520763771
Validation loss: 2.50105930773895

Epoch: 107| Step: 0
Training loss: 3.7281017523374076
Validation loss: 2.4848653492975883

Epoch: 6| Step: 1
Training loss: 2.7697038475672375
Validation loss: 2.524156071974647

Epoch: 6| Step: 2
Training loss: 2.3998504353650003
Validation loss: 2.5034645787450374

Epoch: 6| Step: 3
Training loss: 2.8731041337874315
Validation loss: 2.4698076486538607

Epoch: 6| Step: 4
Training loss: 2.546873876653318
Validation loss: 2.4925613989498165

Epoch: 6| Step: 5
Training loss: 2.5846736158852
Validation loss: 2.493566955704271

Epoch: 6| Step: 6
Training loss: 2.3319680670225296
Validation loss: 2.496010195132044

Epoch: 6| Step: 7
Training loss: 3.018689431793227
Validation loss: 2.4642974943508182

Epoch: 6| Step: 8
Training loss: 3.0537198380406747
Validation loss: 2.4750012088365403

Epoch: 6| Step: 9
Training loss: 2.4393405323295916
Validation loss: 2.4971501452831713

Epoch: 6| Step: 10
Training loss: 2.575220680039542
Validation loss: 2.5024909889077227

Epoch: 6| Step: 11
Training loss: 2.6509078216240485
Validation loss: 2.5121958514279843

Epoch: 6| Step: 12
Training loss: 2.5781984260969155
Validation loss: 2.490448765816398

Epoch: 6| Step: 13
Training loss: 3.011814378142961
Validation loss: 2.4969329437510632

Epoch: 108| Step: 0
Training loss: 2.4456496326230615
Validation loss: 2.525321948412247

Epoch: 6| Step: 1
Training loss: 3.1692105833212705
Validation loss: 2.4761787268074085

Epoch: 6| Step: 2
Training loss: 2.3968584162052595
Validation loss: 2.488934874259334

Epoch: 6| Step: 3
Training loss: 2.802577337989551
Validation loss: 2.483852293047936

Epoch: 6| Step: 4
Training loss: 2.3976351610575684
Validation loss: 2.4897362618354113

Epoch: 6| Step: 5
Training loss: 2.91432757360919
Validation loss: 2.494524643948751

Epoch: 6| Step: 6
Training loss: 3.4221012963842616
Validation loss: 2.4862227293850605

Epoch: 6| Step: 7
Training loss: 3.237574961615573
Validation loss: 2.497880290322817

Epoch: 6| Step: 8
Training loss: 1.752188608255269
Validation loss: 2.5063028571759642

Epoch: 6| Step: 9
Training loss: 2.913034211638488
Validation loss: 2.506484990251329

Epoch: 6| Step: 10
Training loss: 2.530601512586075
Validation loss: 2.474223714604671

Epoch: 6| Step: 11
Training loss: 2.5631122322945856
Validation loss: 2.488988514810455

Epoch: 6| Step: 12
Training loss: 3.072746108327246
Validation loss: 2.5094444963559743

Epoch: 6| Step: 13
Training loss: 2.761601684333756
Validation loss: 2.512258649839644

Epoch: 109| Step: 0
Training loss: 2.8443671594326023
Validation loss: 2.4837849502494533

Epoch: 6| Step: 1
Training loss: 2.8331443125186264
Validation loss: 2.4874250599297247

Epoch: 6| Step: 2
Training loss: 2.3834934949968165
Validation loss: 2.4987233645245324

Epoch: 6| Step: 3
Training loss: 2.6282873006049936
Validation loss: 2.4895408629494526

Epoch: 6| Step: 4
Training loss: 3.3826140242006257
Validation loss: 2.486652438922443

Epoch: 6| Step: 5
Training loss: 2.6783259633533967
Validation loss: 2.4936347097756455

Epoch: 6| Step: 6
Training loss: 2.5356611269285283
Validation loss: 2.5227983877460316

Epoch: 6| Step: 7
Training loss: 2.1600297611446844
Validation loss: 2.487746457757585

Epoch: 6| Step: 8
Training loss: 2.825188223720799
Validation loss: 2.50775257583739

Epoch: 6| Step: 9
Training loss: 2.5102548085396403
Validation loss: 2.5071810819157223

Epoch: 6| Step: 10
Training loss: 3.246543660297858
Validation loss: 2.5045396687910775

Epoch: 6| Step: 11
Training loss: 2.616605688303831
Validation loss: 2.4784364580603033

Epoch: 6| Step: 12
Training loss: 2.960210142150952
Validation loss: 2.5146499835450737

Epoch: 6| Step: 13
Training loss: 2.43986455228734
Validation loss: 2.4925675247868244

Epoch: 110| Step: 0
Training loss: 2.7617673532398834
Validation loss: 2.4797711158155455

Epoch: 6| Step: 1
Training loss: 2.636744068165483
Validation loss: 2.4934604575067576

Epoch: 6| Step: 2
Training loss: 2.823436074607502
Validation loss: 2.4912711530428

Epoch: 6| Step: 3
Training loss: 2.8282312194098163
Validation loss: 2.4837520006807576

Epoch: 6| Step: 4
Training loss: 2.640823266525879
Validation loss: 2.5142084590813294

Epoch: 6| Step: 5
Training loss: 3.0008087657490257
Validation loss: 2.490424265207615

Epoch: 6| Step: 6
Training loss: 2.813638075403507
Validation loss: 2.4952286801584003

Epoch: 6| Step: 7
Training loss: 2.5800320132435512
Validation loss: 2.495354680438713

Epoch: 6| Step: 8
Training loss: 2.7346839948179884
Validation loss: 2.5135216087065007

Epoch: 6| Step: 9
Training loss: 2.3224229045509794
Validation loss: 2.5055284121468837

Epoch: 6| Step: 10
Training loss: 2.8878295578260347
Validation loss: 2.484564522884452

Epoch: 6| Step: 11
Training loss: 2.8849950452527318
Validation loss: 2.507066428313438

Epoch: 6| Step: 12
Training loss: 3.1741880805495954
Validation loss: 2.5145739030330265

Epoch: 6| Step: 13
Training loss: 2.46281482601068
Validation loss: 2.505830831506375

Epoch: 111| Step: 0
Training loss: 2.916929905229381
Validation loss: 2.4935708254763824

Epoch: 6| Step: 1
Training loss: 3.1209021594238244
Validation loss: 2.4931351145333385

Epoch: 6| Step: 2
Training loss: 2.856740269226503
Validation loss: 2.5065153652739474

Epoch: 6| Step: 3
Training loss: 2.942222532609742
Validation loss: 2.5107793194151866

Epoch: 6| Step: 4
Training loss: 2.6610295098928822
Validation loss: 2.507194202846952

Epoch: 6| Step: 5
Training loss: 2.368203527377349
Validation loss: 2.5155861307039444

Epoch: 6| Step: 6
Training loss: 1.9713911475505375
Validation loss: 2.52028210812626

Epoch: 6| Step: 7
Training loss: 3.4445118914601176
Validation loss: 2.5059006632532195

Epoch: 6| Step: 8
Training loss: 2.852644874310622
Validation loss: 2.496835094786386

Epoch: 6| Step: 9
Training loss: 1.8471250516467201
Validation loss: 2.4935461889898636

Epoch: 6| Step: 10
Training loss: 2.8048296284061114
Validation loss: 2.4890167395473712

Epoch: 6| Step: 11
Training loss: 2.792466936180381
Validation loss: 2.467122503209249

Epoch: 6| Step: 12
Training loss: 3.1206075697873734
Validation loss: 2.4665191298908398

Epoch: 6| Step: 13
Training loss: 1.9818128730903262
Validation loss: 2.4774176157537524

Epoch: 112| Step: 0
Training loss: 2.2695790481130076
Validation loss: 2.4641158854259775

Epoch: 6| Step: 1
Training loss: 2.5804983601039204
Validation loss: 2.4970475801047414

Epoch: 6| Step: 2
Training loss: 2.638190959077841
Validation loss: 2.474681353180033

Epoch: 6| Step: 3
Training loss: 3.4376514748231117
Validation loss: 2.476087966806264

Epoch: 6| Step: 4
Training loss: 2.4433993817293262
Validation loss: 2.4805781256271353

Epoch: 6| Step: 5
Training loss: 1.4631094877063566
Validation loss: 2.503127975598314

Epoch: 6| Step: 6
Training loss: 2.3389079621967688
Validation loss: 2.492063620262627

Epoch: 6| Step: 7
Training loss: 3.1666955611935155
Validation loss: 2.5009768843675135

Epoch: 6| Step: 8
Training loss: 2.855627553883991
Validation loss: 2.478288857205847

Epoch: 6| Step: 9
Training loss: 2.8311348594848553
Validation loss: 2.5239243394132593

Epoch: 6| Step: 10
Training loss: 2.6211479624842533
Validation loss: 2.4756913013207513

Epoch: 6| Step: 11
Training loss: 3.1956225853322766
Validation loss: 2.480220303646095

Epoch: 6| Step: 12
Training loss: 2.733940133528355
Validation loss: 2.500026702738252

Epoch: 6| Step: 13
Training loss: 3.3873245419039875
Validation loss: 2.4723003710465896

Epoch: 113| Step: 0
Training loss: 2.5811472431262503
Validation loss: 2.537547977603559

Epoch: 6| Step: 1
Training loss: 3.148254389206456
Validation loss: 2.4982174178672456

Epoch: 6| Step: 2
Training loss: 2.4827295768434787
Validation loss: 2.51277711632133

Epoch: 6| Step: 3
Training loss: 3.0462940151210782
Validation loss: 2.482024342956035

Epoch: 6| Step: 4
Training loss: 1.9990014921548225
Validation loss: 2.4917312941560583

Epoch: 6| Step: 5
Training loss: 3.0631373773716657
Validation loss: 2.4843773775095137

Epoch: 6| Step: 6
Training loss: 3.1876685902010875
Validation loss: 2.5100492364175477

Epoch: 6| Step: 7
Training loss: 1.7729151288334788
Validation loss: 2.522508539690907

Epoch: 6| Step: 8
Training loss: 3.018751983957843
Validation loss: 2.514900028152703

Epoch: 6| Step: 9
Training loss: 2.659978516140562
Validation loss: 2.51318886703222

Epoch: 6| Step: 10
Training loss: 2.267526078920638
Validation loss: 2.5199032983783436

Epoch: 6| Step: 11
Training loss: 2.954196311755309
Validation loss: 2.5162600209590376

Epoch: 6| Step: 12
Training loss: 2.9300130027508517
Validation loss: 2.498934679803782

Epoch: 6| Step: 13
Training loss: 3.0616260760805405
Validation loss: 2.5283980544031643

Epoch: 114| Step: 0
Training loss: 3.1467491147452322
Validation loss: 2.5002199435133026

Epoch: 6| Step: 1
Training loss: 3.16872155291487
Validation loss: 2.495851245000762

Epoch: 6| Step: 2
Training loss: 2.8969554968125926
Validation loss: 2.48306972055353

Epoch: 6| Step: 3
Training loss: 2.2421021129552092
Validation loss: 2.4863296200873806

Epoch: 6| Step: 4
Training loss: 2.0326779554851604
Validation loss: 2.4941506764746473

Epoch: 6| Step: 5
Training loss: 3.046489045567166
Validation loss: 2.498549435337836

Epoch: 6| Step: 6
Training loss: 2.4136311661139067
Validation loss: 2.4899477055950667

Epoch: 6| Step: 7
Training loss: 2.4634588011232386
Validation loss: 2.4941008432115086

Epoch: 6| Step: 8
Training loss: 2.7298362337674575
Validation loss: 2.469003210845465

Epoch: 6| Step: 9
Training loss: 2.7778060678524956
Validation loss: 2.492516898241525

Epoch: 6| Step: 10
Training loss: 3.0449791903481573
Validation loss: 2.506180838225252

Epoch: 6| Step: 11
Training loss: 3.0128453543471867
Validation loss: 2.4914800381900255

Epoch: 6| Step: 12
Training loss: 2.605381633897797
Validation loss: 2.4809121973565826

Epoch: 6| Step: 13
Training loss: 2.752433220575574
Validation loss: 2.4870675226172017

Epoch: 115| Step: 0
Training loss: 3.1489949454651307
Validation loss: 2.479236687277798

Epoch: 6| Step: 1
Training loss: 1.9993442414026361
Validation loss: 2.5066260778778737

Epoch: 6| Step: 2
Training loss: 3.057641359743561
Validation loss: 2.5157877429234725

Epoch: 6| Step: 3
Training loss: 2.597249164090236
Validation loss: 2.501480322610971

Epoch: 6| Step: 4
Training loss: 2.054949029443514
Validation loss: 2.480409291298983

Epoch: 6| Step: 5
Training loss: 3.2817510313158995
Validation loss: 2.468467256952499

Epoch: 6| Step: 6
Training loss: 2.929341450916839
Validation loss: 2.4690910769381293

Epoch: 6| Step: 7
Training loss: 2.7043671152111446
Validation loss: 2.471714575747008

Epoch: 6| Step: 8
Training loss: 3.0275760684548887
Validation loss: 2.499363373124187

Epoch: 6| Step: 9
Training loss: 2.5406176690140136
Validation loss: 2.516297250761321

Epoch: 6| Step: 10
Training loss: 2.9347336100052313
Validation loss: 2.508441319619818

Epoch: 6| Step: 11
Training loss: 2.3028176173726473
Validation loss: 2.5063179568471603

Epoch: 6| Step: 12
Training loss: 2.8209452672149737
Validation loss: 2.4958558179165866

Epoch: 6| Step: 13
Training loss: 2.3514521880272463
Validation loss: 2.4772890432457597

Epoch: 116| Step: 0
Training loss: 2.2664830128748257
Validation loss: 2.4984575435452636

Epoch: 6| Step: 1
Training loss: 2.484104069745749
Validation loss: 2.5258429305858283

Epoch: 6| Step: 2
Training loss: 3.3408428638525622
Validation loss: 2.5030919766188213

Epoch: 6| Step: 3
Training loss: 2.315627792294938
Validation loss: 2.5249396019754857

Epoch: 6| Step: 4
Training loss: 2.416588902592516
Validation loss: 2.478686020267103

Epoch: 6| Step: 5
Training loss: 2.559123629381309
Validation loss: 2.5009425642418104

Epoch: 6| Step: 6
Training loss: 2.1647679249904326
Validation loss: 2.481296925632979

Epoch: 6| Step: 7
Training loss: 2.477267673350469
Validation loss: 2.503598600110755

Epoch: 6| Step: 8
Training loss: 3.4866623833109096
Validation loss: 2.515619743543965

Epoch: 6| Step: 9
Training loss: 3.029824462438931
Validation loss: 2.5179994037609905

Epoch: 6| Step: 10
Training loss: 3.418810024815478
Validation loss: 2.4678249440678046

Epoch: 6| Step: 11
Training loss: 2.79867376526103
Validation loss: 2.468789215212574

Epoch: 6| Step: 12
Training loss: 2.464249770571704
Validation loss: 2.5014394953518058

Epoch: 6| Step: 13
Training loss: 2.792162457371435
Validation loss: 2.4890847132634493

Epoch: 117| Step: 0
Training loss: 3.374111482434811
Validation loss: 2.452944589353993

Epoch: 6| Step: 1
Training loss: 2.4328984573065218
Validation loss: 2.499268586828359

Epoch: 6| Step: 2
Training loss: 2.6268787018965187
Validation loss: 2.487620756405168

Epoch: 6| Step: 3
Training loss: 1.784284382939292
Validation loss: 2.4791922963914086

Epoch: 6| Step: 4
Training loss: 2.3684485573314835
Validation loss: 2.5143562000692734

Epoch: 6| Step: 5
Training loss: 2.9378382711230704
Validation loss: 2.537289561427191

Epoch: 6| Step: 6
Training loss: 3.08236510737313
Validation loss: 2.5242450437522237

Epoch: 6| Step: 7
Training loss: 2.7410423735245377
Validation loss: 2.510527835577187

Epoch: 6| Step: 8
Training loss: 3.3794891736607378
Validation loss: 2.501428708612444

Epoch: 6| Step: 9
Training loss: 3.334587020354016
Validation loss: 2.490375512386414

Epoch: 6| Step: 10
Training loss: 2.508712749575286
Validation loss: 2.4976943777082927

Epoch: 6| Step: 11
Training loss: 2.0590945942933976
Validation loss: 2.5038572538927055

Epoch: 6| Step: 12
Training loss: 2.812649362624633
Validation loss: 2.50000893991164

Epoch: 6| Step: 13
Training loss: 1.9600584331871789
Validation loss: 2.514185062338035

Epoch: 118| Step: 0
Training loss: 3.503888422517705
Validation loss: 2.5004920434240945

Epoch: 6| Step: 1
Training loss: 2.6220908620586743
Validation loss: 2.488418961390387

Epoch: 6| Step: 2
Training loss: 2.4957568399449657
Validation loss: 2.506431395818342

Epoch: 6| Step: 3
Training loss: 2.567996480273868
Validation loss: 2.489624142210421

Epoch: 6| Step: 4
Training loss: 2.410696701712284
Validation loss: 2.4982365787753524

Epoch: 6| Step: 5
Training loss: 2.881075865806253
Validation loss: 2.511971623012169

Epoch: 6| Step: 6
Training loss: 3.2179417752818344
Validation loss: 2.5008742106299486

Epoch: 6| Step: 7
Training loss: 2.257272834036203
Validation loss: 2.4931339546333824

Epoch: 6| Step: 8
Training loss: 2.9274952735504383
Validation loss: 2.523130019130753

Epoch: 6| Step: 9
Training loss: 3.0035027082852377
Validation loss: 2.5066645788379547

Epoch: 6| Step: 10
Training loss: 2.54413171476997
Validation loss: 2.5047160622137183

Epoch: 6| Step: 11
Training loss: 2.924060351081622
Validation loss: 2.501241209715736

Epoch: 6| Step: 12
Training loss: 1.5612858441821769
Validation loss: 2.4899156705335765

Epoch: 6| Step: 13
Training loss: 3.1236311394042917
Validation loss: 2.5014921975129116

Epoch: 119| Step: 0
Training loss: 2.7208073165451956
Validation loss: 2.517915585143805

Epoch: 6| Step: 1
Training loss: 2.6525769138240727
Validation loss: 2.482785089270074

Epoch: 6| Step: 2
Training loss: 2.8354700296964244
Validation loss: 2.513677366021419

Epoch: 6| Step: 3
Training loss: 3.0854113166414114
Validation loss: 2.493895674723198

Epoch: 6| Step: 4
Training loss: 3.3746320735867013
Validation loss: 2.4864929194613765

Epoch: 6| Step: 5
Training loss: 2.769746543360823
Validation loss: 2.4992447235185944

Epoch: 6| Step: 6
Training loss: 1.916647275190805
Validation loss: 2.4750533698028776

Epoch: 6| Step: 7
Training loss: 2.235622031338123
Validation loss: 2.495892080472644

Epoch: 6| Step: 8
Training loss: 2.592151746911456
Validation loss: 2.5103847818379847

Epoch: 6| Step: 9
Training loss: 3.1111647472602164
Validation loss: 2.5029838864412577

Epoch: 6| Step: 10
Training loss: 2.762610133810836
Validation loss: 2.4889616040390394

Epoch: 6| Step: 11
Training loss: 3.0302875996688075
Validation loss: 2.503731621092638

Epoch: 6| Step: 12
Training loss: 2.332791765124696
Validation loss: 2.5210884295345704

Epoch: 6| Step: 13
Training loss: 2.4983181064269777
Validation loss: 2.4778892239923715

Epoch: 120| Step: 0
Training loss: 3.2182870180052663
Validation loss: 2.5289035638442683

Epoch: 6| Step: 1
Training loss: 2.593608438696244
Validation loss: 2.520459635477

Epoch: 6| Step: 2
Training loss: 3.1609824176940156
Validation loss: 2.4920266384615677

Epoch: 6| Step: 3
Training loss: 2.8268138733655777
Validation loss: 2.497536858906632

Epoch: 6| Step: 4
Training loss: 1.8652560407568597
Validation loss: 2.468139750586513

Epoch: 6| Step: 5
Training loss: 2.884036747532979
Validation loss: 2.4909607868719665

Epoch: 6| Step: 6
Training loss: 2.6036178213953933
Validation loss: 2.496708986445661

Epoch: 6| Step: 7
Training loss: 2.377784301574567
Validation loss: 2.485687883077799

Epoch: 6| Step: 8
Training loss: 3.0958870336736375
Validation loss: 2.470863130553687

Epoch: 6| Step: 9
Training loss: 2.876453198247624
Validation loss: 2.499806217148566

Epoch: 6| Step: 10
Training loss: 2.4145959554830005
Validation loss: 2.489728268388673

Epoch: 6| Step: 11
Training loss: 3.0835450761276753
Validation loss: 2.4685816615926255

Epoch: 6| Step: 12
Training loss: 2.642684223895576
Validation loss: 2.4973151608090123

Epoch: 6| Step: 13
Training loss: 2.382259016948877
Validation loss: 2.503229422154895

Epoch: 121| Step: 0
Training loss: 2.622683911125121
Validation loss: 2.5118914846064566

Epoch: 6| Step: 1
Training loss: 2.4915921926266162
Validation loss: 2.5279871568309438

Epoch: 6| Step: 2
Training loss: 2.34375
Validation loss: 2.4917112827644985

Epoch: 6| Step: 3
Training loss: 2.961473079344571
Validation loss: 2.490699214897244

Epoch: 6| Step: 4
Training loss: 2.9836861179834204
Validation loss: 2.481407728198424

Epoch: 6| Step: 5
Training loss: 2.4688570023537793
Validation loss: 2.5197971105473305

Epoch: 6| Step: 6
Training loss: 2.7926508369109473
Validation loss: 2.5337912360483346

Epoch: 6| Step: 7
Training loss: 2.1766774165699574
Validation loss: 2.4960125533420103

Epoch: 6| Step: 8
Training loss: 2.959762944188439
Validation loss: 2.4899704163239393

Epoch: 6| Step: 9
Training loss: 2.4414395505541417
Validation loss: 2.452231028013898

Epoch: 6| Step: 10
Training loss: 2.968715948612282
Validation loss: 2.465107998934756

Epoch: 6| Step: 11
Training loss: 3.3487548449895255
Validation loss: 2.4914890951053987

Epoch: 6| Step: 12
Training loss: 3.308085541049296
Validation loss: 2.457952895082205

Epoch: 6| Step: 13
Training loss: 1.970028840352516
Validation loss: 2.530676233468477

Epoch: 122| Step: 0
Training loss: 2.818716342496706
Validation loss: 2.5027008838682536

Epoch: 6| Step: 1
Training loss: 2.4884035089421146
Validation loss: 2.4861941996740105

Epoch: 6| Step: 2
Training loss: 2.861939161239155
Validation loss: 2.4797347198633832

Epoch: 6| Step: 3
Training loss: 2.4688118069794727
Validation loss: 2.496764456048536

Epoch: 6| Step: 4
Training loss: 2.9538965579693386
Validation loss: 2.4742729265914427

Epoch: 6| Step: 5
Training loss: 2.2250525993387704
Validation loss: 2.480932056120065

Epoch: 6| Step: 6
Training loss: 2.8246354973521117
Validation loss: 2.4901062151177524

Epoch: 6| Step: 7
Training loss: 2.221161297694514
Validation loss: 2.4690285968503907

Epoch: 6| Step: 8
Training loss: 2.353688036669771
Validation loss: 2.466934306927156

Epoch: 6| Step: 9
Training loss: 3.1629496560059605
Validation loss: 2.502172108940476

Epoch: 6| Step: 10
Training loss: 2.931030291231265
Validation loss: 2.486138876024379

Epoch: 6| Step: 11
Training loss: 2.940112960617569
Validation loss: 2.5116509009427155

Epoch: 6| Step: 12
Training loss: 2.8632399489688414
Validation loss: 2.489527946065776

Epoch: 6| Step: 13
Training loss: 2.9171481325489603
Validation loss: 2.4914577013623047

Epoch: 123| Step: 0
Training loss: 2.686873606938571
Validation loss: 2.4786458492545167

Epoch: 6| Step: 1
Training loss: 2.5657812370652002
Validation loss: 2.5114537859415433

Epoch: 6| Step: 2
Training loss: 2.8601896866003416
Validation loss: 2.5072750547758624

Epoch: 6| Step: 3
Training loss: 3.271446915597107
Validation loss: 2.4948952055425906

Epoch: 6| Step: 4
Training loss: 2.8181175080565906
Validation loss: 2.4993715701317782

Epoch: 6| Step: 5
Training loss: 3.5522111588166165
Validation loss: 2.48273123931211

Epoch: 6| Step: 6
Training loss: 2.5658829850879914
Validation loss: 2.5123065126880606

Epoch: 6| Step: 7
Training loss: 2.6180842898607977
Validation loss: 2.475255283014799

Epoch: 6| Step: 8
Training loss: 2.5991115996298277
Validation loss: 2.4749015128489145

Epoch: 6| Step: 9
Training loss: 2.776815415146005
Validation loss: 2.5058866797700317

Epoch: 6| Step: 10
Training loss: 2.178430776053517
Validation loss: 2.4898464334853267

Epoch: 6| Step: 11
Training loss: 2.789880883603464
Validation loss: 2.529017260906757

Epoch: 6| Step: 12
Training loss: 2.4825768831300783
Validation loss: 2.4930752617571543

Epoch: 6| Step: 13
Training loss: 2.179287343911202
Validation loss: 2.4804517690322623

Epoch: 124| Step: 0
Training loss: 2.273536076981177
Validation loss: 2.512115744667583

Epoch: 6| Step: 1
Training loss: 2.652014103715312
Validation loss: 2.511680723051178

Epoch: 6| Step: 2
Training loss: 2.32731153370462
Validation loss: 2.489857054179198

Epoch: 6| Step: 3
Training loss: 3.1959999296357835
Validation loss: 2.511850915362595

Epoch: 6| Step: 4
Training loss: 2.7013612847659827
Validation loss: 2.503343716919612

Epoch: 6| Step: 5
Training loss: 2.968202078600232
Validation loss: 2.5228007259964436

Epoch: 6| Step: 6
Training loss: 3.1050148302298437
Validation loss: 2.513943282730386

Epoch: 6| Step: 7
Training loss: 3.032236941407358
Validation loss: 2.4854914207085903

Epoch: 6| Step: 8
Training loss: 2.8493546809371275
Validation loss: 2.4938577846266026

Epoch: 6| Step: 9
Training loss: 2.5524274529170823
Validation loss: 2.512358084845845

Epoch: 6| Step: 10
Training loss: 3.151291077046122
Validation loss: 2.5190846550960915

Epoch: 6| Step: 11
Training loss: 1.9522263557642656
Validation loss: 2.512832397013412

Epoch: 6| Step: 12
Training loss: 2.5603854130657635
Validation loss: 2.482453325181028

Epoch: 6| Step: 13
Training loss: 2.194433939247364
Validation loss: 2.4982665143215637

Epoch: 125| Step: 0
Training loss: 2.6870185065872603
Validation loss: 2.525528059870769

Epoch: 6| Step: 1
Training loss: 2.808787459907581
Validation loss: 2.482738128735466

Epoch: 6| Step: 2
Training loss: 2.344443460236273
Validation loss: 2.5026431421520354

Epoch: 6| Step: 3
Training loss: 2.817908956233348
Validation loss: 2.50474486303078

Epoch: 6| Step: 4
Training loss: 2.581377786424358
Validation loss: 2.4788343752003534

Epoch: 6| Step: 5
Training loss: 2.5874611782534065
Validation loss: 2.5195074015585135

Epoch: 6| Step: 6
Training loss: 3.351897678643503
Validation loss: 2.4873147872130392

Epoch: 6| Step: 7
Training loss: 3.2941530390048315
Validation loss: 2.4973847461484144

Epoch: 6| Step: 8
Training loss: 2.5123216252563023
Validation loss: 2.533317617789692

Epoch: 6| Step: 9
Training loss: 2.662715448250077
Validation loss: 2.481352231450044

Epoch: 6| Step: 10
Training loss: 2.4226102635786835
Validation loss: 2.5437892860696034

Epoch: 6| Step: 11
Training loss: 2.6858575814581043
Validation loss: 2.510597654107527

Epoch: 6| Step: 12
Training loss: 3.0022269566471405
Validation loss: 2.5225464232299486

Epoch: 6| Step: 13
Training loss: 1.994777297589879
Validation loss: 2.505278681117701

Epoch: 126| Step: 0
Training loss: 2.9420672683338442
Validation loss: 2.4955134146190945

Epoch: 6| Step: 1
Training loss: 2.592172441674494
Validation loss: 2.4794540679890877

Epoch: 6| Step: 2
Training loss: 2.306276452551177
Validation loss: 2.5157088500967792

Epoch: 6| Step: 3
Training loss: 2.070485003500204
Validation loss: 2.4977182517245575

Epoch: 6| Step: 4
Training loss: 2.9099183260542967
Validation loss: 2.484200095636166

Epoch: 6| Step: 5
Training loss: 3.2430414385128694
Validation loss: 2.4770459940525362

Epoch: 6| Step: 6
Training loss: 2.1273086572975455
Validation loss: 2.495678522957918

Epoch: 6| Step: 7
Training loss: 3.0694159443617206
Validation loss: 2.5115275361803104

Epoch: 6| Step: 8
Training loss: 2.8282993326145953
Validation loss: 2.508911714831327

Epoch: 6| Step: 9
Training loss: 2.4461639189547366
Validation loss: 2.5122998574147943

Epoch: 6| Step: 10
Training loss: 2.689530780809703
Validation loss: 2.4908385165625506

Epoch: 6| Step: 11
Training loss: 2.9633618095182586
Validation loss: 2.50980258239474

Epoch: 6| Step: 12
Training loss: 3.103290989057996
Validation loss: 2.4888890355878788

Epoch: 6| Step: 13
Training loss: 2.815962122846671
Validation loss: 2.4931379793193926

Epoch: 127| Step: 0
Training loss: 1.869751705780455
Validation loss: 2.480962711960339

Epoch: 6| Step: 1
Training loss: 2.592321898946732
Validation loss: 2.511781945123569

Epoch: 6| Step: 2
Training loss: 2.6353737346383954
Validation loss: 2.499723285824782

Epoch: 6| Step: 3
Training loss: 2.055908536531598
Validation loss: 2.5157980645663858

Epoch: 6| Step: 4
Training loss: 2.1685708799241223
Validation loss: 2.494670550449629

Epoch: 6| Step: 5
Training loss: 3.246038663651499
Validation loss: 2.4741640944261967

Epoch: 6| Step: 6
Training loss: 3.1538773485040186
Validation loss: 2.510959202220799

Epoch: 6| Step: 7
Training loss: 2.842759850802726
Validation loss: 2.4793680084636263

Epoch: 6| Step: 8
Training loss: 2.7596649777902993
Validation loss: 2.49256613732103

Epoch: 6| Step: 9
Training loss: 3.351664650522995
Validation loss: 2.4964692317067

Epoch: 6| Step: 10
Training loss: 2.5164621038435224
Validation loss: 2.457126323899636

Epoch: 6| Step: 11
Training loss: 2.5682236552187385
Validation loss: 2.4650958510044982

Epoch: 6| Step: 12
Training loss: 3.1971500401469157
Validation loss: 2.516305003929264

Epoch: 6| Step: 13
Training loss: 3.0424299632328036
Validation loss: 2.492354982560228

Epoch: 128| Step: 0
Training loss: 2.8589603978149842
Validation loss: 2.518421699227374

Epoch: 6| Step: 1
Training loss: 1.966861363919158
Validation loss: 2.502548954895347

Epoch: 6| Step: 2
Training loss: 3.110739557089442
Validation loss: 2.4750281512450467

Epoch: 6| Step: 3
Training loss: 3.0728362681760424
Validation loss: 2.492810642330116

Epoch: 6| Step: 4
Training loss: 2.325550678626958
Validation loss: 2.557632185695408

Epoch: 6| Step: 5
Training loss: 2.374040409782246
Validation loss: 2.4718881560475765

Epoch: 6| Step: 6
Training loss: 3.381908128470746
Validation loss: 2.4703504422282574

Epoch: 6| Step: 7
Training loss: 2.6432639033598546
Validation loss: 2.4850986422856622

Epoch: 6| Step: 8
Training loss: 2.773856547760264
Validation loss: 2.4846381859821247

Epoch: 6| Step: 9
Training loss: 3.252056938095224
Validation loss: 2.4892088329124977

Epoch: 6| Step: 10
Training loss: 2.4381573108682475
Validation loss: 2.474617267067094

Epoch: 6| Step: 11
Training loss: 2.13735935573875
Validation loss: 2.4704571459265336

Epoch: 6| Step: 12
Training loss: 2.5857492245825386
Validation loss: 2.4945589865998112

Epoch: 6| Step: 13
Training loss: 2.777510883437018
Validation loss: 2.488167899504624

Epoch: 129| Step: 0
Training loss: 2.3176657226303083
Validation loss: 2.490604094747484

Epoch: 6| Step: 1
Training loss: 2.1037203123340458
Validation loss: 2.5076526826936365

Epoch: 6| Step: 2
Training loss: 2.7073993735313326
Validation loss: 2.503212767643594

Epoch: 6| Step: 3
Training loss: 2.9804785092095303
Validation loss: 2.506380591586831

Epoch: 6| Step: 4
Training loss: 3.4940748469737857
Validation loss: 2.4828498644174566

Epoch: 6| Step: 5
Training loss: 2.8634541083352447
Validation loss: 2.46062582073327

Epoch: 6| Step: 6
Training loss: 2.560030701125349
Validation loss: 2.472011737021453

Epoch: 6| Step: 7
Training loss: 2.8804280000126434
Validation loss: 2.5211097879602353

Epoch: 6| Step: 8
Training loss: 2.8754292043329563
Validation loss: 2.522445080744274

Epoch: 6| Step: 9
Training loss: 2.756428487452415
Validation loss: 2.5586787258785844

Epoch: 6| Step: 10
Training loss: 1.9761995125485727
Validation loss: 2.523935333715017

Epoch: 6| Step: 11
Training loss: 2.875046107710268
Validation loss: 2.5308205723773978

Epoch: 6| Step: 12
Training loss: 2.756165528678978
Validation loss: 2.5118715307440187

Epoch: 6| Step: 13
Training loss: 2.992198973949933
Validation loss: 2.4817516406778313

Epoch: 130| Step: 0
Training loss: 2.384404185505204
Validation loss: 2.5047736614696747

Epoch: 6| Step: 1
Training loss: 3.00244883091099
Validation loss: 2.523025353570326

Epoch: 6| Step: 2
Training loss: 2.717257736904
Validation loss: 2.5059117253776737

Epoch: 6| Step: 3
Training loss: 2.2483885610634555
Validation loss: 2.506800251528141

Epoch: 6| Step: 4
Training loss: 2.7288674828773245
Validation loss: 2.552548781729666

Epoch: 6| Step: 5
Training loss: 2.3965501348773643
Validation loss: 2.518540991597858

Epoch: 6| Step: 6
Training loss: 2.877752064950464
Validation loss: 2.4969858231643873

Epoch: 6| Step: 7
Training loss: 3.0049575533542794
Validation loss: 2.4971505343746028

Epoch: 6| Step: 8
Training loss: 2.8723035689770673
Validation loss: 2.497922683403828

Epoch: 6| Step: 9
Training loss: 2.8550211794413354
Validation loss: 2.5060254599680136

Epoch: 6| Step: 10
Training loss: 2.9759387867179794
Validation loss: 2.489273269943837

Epoch: 6| Step: 11
Training loss: 2.0396599022348476
Validation loss: 2.472254816576072

Epoch: 6| Step: 12
Training loss: 3.061829162615857
Validation loss: 2.495190280148496

Epoch: 6| Step: 13
Training loss: 2.2572167478438634
Validation loss: 2.510404224660217

Epoch: 131| Step: 0
Training loss: 2.074434603219554
Validation loss: 2.485889485279393

Epoch: 6| Step: 1
Training loss: 2.055846840894363
Validation loss: 2.4800572321470753

Epoch: 6| Step: 2
Training loss: 3.489915488457729
Validation loss: 2.492216738768741

Epoch: 6| Step: 3
Training loss: 2.810127189852914
Validation loss: 2.493867126908508

Epoch: 6| Step: 4
Training loss: 2.875308808037347
Validation loss: 2.494662812255934

Epoch: 6| Step: 5
Training loss: 2.2143562762712143
Validation loss: 2.486159784784613

Epoch: 6| Step: 6
Training loss: 3.21741376380388
Validation loss: 2.4699355685159547

Epoch: 6| Step: 7
Training loss: 2.764935941040342
Validation loss: 2.523689949495369

Epoch: 6| Step: 8
Training loss: 2.4002761999781694
Validation loss: 2.5002909870411254

Epoch: 6| Step: 9
Training loss: 2.545861165279045
Validation loss: 2.5061593709823553

Epoch: 6| Step: 10
Training loss: 3.2079815031740804
Validation loss: 2.498844270285114

Epoch: 6| Step: 11
Training loss: 2.4129534399012145
Validation loss: 2.4640893667917836

Epoch: 6| Step: 12
Training loss: 3.1223898095631446
Validation loss: 2.5037948427219923

Epoch: 6| Step: 13
Training loss: 2.256683595369403
Validation loss: 2.461556408664523

Epoch: 132| Step: 0
Training loss: 3.54552982045895
Validation loss: 2.497764429747057

Epoch: 6| Step: 1
Training loss: 2.4575204529013024
Validation loss: 2.4824064812644786

Epoch: 6| Step: 2
Training loss: 2.7583316822181514
Validation loss: 2.5116273818764014

Epoch: 6| Step: 3
Training loss: 2.9550189045722526
Validation loss: 2.5114867778299033

Epoch: 6| Step: 4
Training loss: 2.69992140196381
Validation loss: 2.5275968469379153

Epoch: 6| Step: 5
Training loss: 2.727204517754203
Validation loss: 2.4944349348124475

Epoch: 6| Step: 6
Training loss: 3.229725910285021
Validation loss: 2.500308548950048

Epoch: 6| Step: 7
Training loss: 2.231209707029885
Validation loss: 2.4926648131145077

Epoch: 6| Step: 8
Training loss: 1.8812019772911184
Validation loss: 2.494786007511854

Epoch: 6| Step: 9
Training loss: 1.7609873266202718
Validation loss: 2.5115902135044057

Epoch: 6| Step: 10
Training loss: 2.933725732508747
Validation loss: 2.5092389692113577

Epoch: 6| Step: 11
Training loss: 2.8165087634938484
Validation loss: 2.4828746034251097

Epoch: 6| Step: 12
Training loss: 3.053418766753282
Validation loss: 2.496794546832452

Epoch: 6| Step: 13
Training loss: 2.533820177865056
Validation loss: 2.504459607853716

Epoch: 133| Step: 0
Training loss: 2.4925796056831815
Validation loss: 2.5071941046856265

Epoch: 6| Step: 1
Training loss: 2.203541493107612
Validation loss: 2.4845112248393875

Epoch: 6| Step: 2
Training loss: 2.5365162450509695
Validation loss: 2.4841719642639335

Epoch: 6| Step: 3
Training loss: 2.872434715545048
Validation loss: 2.5155469571099776

Epoch: 6| Step: 4
Training loss: 2.788894861990859
Validation loss: 2.4956872492778044

Epoch: 6| Step: 5
Training loss: 2.980088115899903
Validation loss: 2.4863618622013175

Epoch: 6| Step: 6
Training loss: 2.9834936947763135
Validation loss: 2.4957780566642405

Epoch: 6| Step: 7
Training loss: 2.9480210859768725
Validation loss: 2.5025672897502638

Epoch: 6| Step: 8
Training loss: 3.020629683020711
Validation loss: 2.496191857050494

Epoch: 6| Step: 9
Training loss: 3.305170551464414
Validation loss: 2.4944094486855644

Epoch: 6| Step: 10
Training loss: 2.5230452761222
Validation loss: 2.509867181195995

Epoch: 6| Step: 11
Training loss: 1.9468559056014632
Validation loss: 2.5206658933092188

Epoch: 6| Step: 12
Training loss: 2.710397804032374
Validation loss: 2.525446707142914

Epoch: 6| Step: 13
Training loss: 2.3737749654653286
Validation loss: 2.5202109806699498

Epoch: 134| Step: 0
Training loss: 2.9434536600484367
Validation loss: 2.507803154977382

Epoch: 6| Step: 1
Training loss: 2.8086172641411795
Validation loss: 2.509260849426495

Epoch: 6| Step: 2
Training loss: 2.183459883460904
Validation loss: 2.4924263387262635

Epoch: 6| Step: 3
Training loss: 3.315968838479722
Validation loss: 2.490565440212195

Epoch: 6| Step: 4
Training loss: 1.92807303780598
Validation loss: 2.492566917963481

Epoch: 6| Step: 5
Training loss: 3.0862854580331347
Validation loss: 2.5010606054163755

Epoch: 6| Step: 6
Training loss: 1.7664788131285514
Validation loss: 2.5108232998991493

Epoch: 6| Step: 7
Training loss: 2.9329656139273896
Validation loss: 2.4870308675075323

Epoch: 6| Step: 8
Training loss: 2.733885279806841
Validation loss: 2.5221073148793915

Epoch: 6| Step: 9
Training loss: 2.1302738941304407
Validation loss: 2.508988713451164

Epoch: 6| Step: 10
Training loss: 2.9751069121824876
Validation loss: 2.4975552613497527

Epoch: 6| Step: 11
Training loss: 2.6785850815197993
Validation loss: 2.4803211884309464

Epoch: 6| Step: 12
Training loss: 3.1303133235510265
Validation loss: 2.493885690066826

Epoch: 6| Step: 13
Training loss: 3.0311985601406146
Validation loss: 2.500363483461681

Epoch: 135| Step: 0
Training loss: 2.694556088663514
Validation loss: 2.499283236637684

Epoch: 6| Step: 1
Training loss: 3.2641148337938968
Validation loss: 2.4883406122944334

Epoch: 6| Step: 2
Training loss: 2.576411562188922
Validation loss: 2.461729913770334

Epoch: 6| Step: 3
Training loss: 2.3471411014461094
Validation loss: 2.4967955263734756

Epoch: 6| Step: 4
Training loss: 2.67100171546781
Validation loss: 2.51113083034117

Epoch: 6| Step: 5
Training loss: 2.595749968715294
Validation loss: 2.4737832591743945

Epoch: 6| Step: 6
Training loss: 2.508010999007496
Validation loss: 2.495730001127332

Epoch: 6| Step: 7
Training loss: 2.512320391559845
Validation loss: 2.499243176662763

Epoch: 6| Step: 8
Training loss: 3.35638834350293
Validation loss: 2.5010528593176837

Epoch: 6| Step: 9
Training loss: 1.8905780880995495
Validation loss: 2.4628014155537192

Epoch: 6| Step: 10
Training loss: 1.8698107435976425
Validation loss: 2.496653194212336

Epoch: 6| Step: 11
Training loss: 3.3224368491389136
Validation loss: 2.513432806082282

Epoch: 6| Step: 12
Training loss: 2.684934411695187
Validation loss: 2.521295753385293

Epoch: 6| Step: 13
Training loss: 3.6602934299683185
Validation loss: 2.4894136740102004

Epoch: 136| Step: 0
Training loss: 1.9776317132038155
Validation loss: 2.492841412278349

Epoch: 6| Step: 1
Training loss: 2.013934113736454
Validation loss: 2.4886502029920297

Epoch: 6| Step: 2
Training loss: 2.3815633532586777
Validation loss: 2.481751687162671

Epoch: 6| Step: 3
Training loss: 2.6099878379567647
Validation loss: 2.482900293136045

Epoch: 6| Step: 4
Training loss: 2.322274248958966
Validation loss: 2.4918685834016188

Epoch: 6| Step: 5
Training loss: 3.0274675503920365
Validation loss: 2.498223764831715

Epoch: 6| Step: 6
Training loss: 3.6354893164311806
Validation loss: 2.530333536010859

Epoch: 6| Step: 7
Training loss: 2.5597621899655536
Validation loss: 2.5217694122797916

Epoch: 6| Step: 8
Training loss: 2.943214539506341
Validation loss: 2.4898104494346036

Epoch: 6| Step: 9
Training loss: 2.4180133180716914
Validation loss: 2.4973576835885227

Epoch: 6| Step: 10
Training loss: 2.413233444911091
Validation loss: 2.5283738373752866

Epoch: 6| Step: 11
Training loss: 2.8754457874497477
Validation loss: 2.5117717447649808

Epoch: 6| Step: 12
Training loss: 3.175125636220736
Validation loss: 2.525806590539303

Epoch: 6| Step: 13
Training loss: 3.5580699212685842
Validation loss: 2.5291677793219347

Epoch: 137| Step: 0
Training loss: 2.7060818238895603
Validation loss: 2.531777490374784

Epoch: 6| Step: 1
Training loss: 2.4364181465225934
Validation loss: 2.4955811877538956

Epoch: 6| Step: 2
Training loss: 3.240322081957937
Validation loss: 2.5194348078250437

Epoch: 6| Step: 3
Training loss: 2.419401911923622
Validation loss: 2.5466356606016816

Epoch: 6| Step: 4
Training loss: 2.828818894717604
Validation loss: 2.5200601302322276

Epoch: 6| Step: 5
Training loss: 2.3171158165367722
Validation loss: 2.5253235767504694

Epoch: 6| Step: 6
Training loss: 3.776794835536652
Validation loss: 2.5172842743636727

Epoch: 6| Step: 7
Training loss: 2.0467373168298866
Validation loss: 2.5799846565945845

Epoch: 6| Step: 8
Training loss: 2.1059191616588455
Validation loss: 2.524206047193593

Epoch: 6| Step: 9
Training loss: 2.822707746781922
Validation loss: 2.519221419406775

Epoch: 6| Step: 10
Training loss: 2.233476791708739
Validation loss: 2.5088768206689114

Epoch: 6| Step: 11
Training loss: 2.848978957264863
Validation loss: 2.5122261410738207

Epoch: 6| Step: 12
Training loss: 3.1403179872677267
Validation loss: 2.487469277614983

Epoch: 6| Step: 13
Training loss: 2.6315855342381007
Validation loss: 2.520778729315117

Epoch: 138| Step: 0
Training loss: 2.614796562669446
Validation loss: 2.533549855199739

Epoch: 6| Step: 1
Training loss: 2.773903047414433
Validation loss: 2.500270317188912

Epoch: 6| Step: 2
Training loss: 2.651781789493271
Validation loss: 2.497644375992807

Epoch: 6| Step: 3
Training loss: 2.5996345043042566
Validation loss: 2.494759304684416

Epoch: 6| Step: 4
Training loss: 3.434476736712172
Validation loss: 2.4994145169246034

Epoch: 6| Step: 5
Training loss: 2.372611802645398
Validation loss: 2.5302458330644155

Epoch: 6| Step: 6
Training loss: 2.689812397282469
Validation loss: 2.4923528019236163

Epoch: 6| Step: 7
Training loss: 2.6164647257282536
Validation loss: 2.5064547365756455

Epoch: 6| Step: 8
Training loss: 2.9818225750605327
Validation loss: 2.512954077688632

Epoch: 6| Step: 9
Training loss: 2.2585342468002843
Validation loss: 2.4946343184372504

Epoch: 6| Step: 10
Training loss: 3.2149941465997878
Validation loss: 2.519800164779154

Epoch: 6| Step: 11
Training loss: 2.7811201258135765
Validation loss: 2.486627584400618

Epoch: 6| Step: 12
Training loss: 2.535195748560751
Validation loss: 2.5098310081967266

Epoch: 6| Step: 13
Training loss: 2.1876025584566015
Validation loss: 2.476435182289803

Epoch: 139| Step: 0
Training loss: 2.5034926812377516
Validation loss: 2.490022669878955

Epoch: 6| Step: 1
Training loss: 3.2000537569775895
Validation loss: 2.5107863309814866

Epoch: 6| Step: 2
Training loss: 2.8096459212486735
Validation loss: 2.5229065160959614

Epoch: 6| Step: 3
Training loss: 3.5854925407318246
Validation loss: 2.4901424872769424

Epoch: 6| Step: 4
Training loss: 2.7096598579933437
Validation loss: 2.5150736084299763

Epoch: 6| Step: 5
Training loss: 2.21709584644182
Validation loss: 2.4982326546596094

Epoch: 6| Step: 6
Training loss: 2.2055055999650173
Validation loss: 2.5356669636234237

Epoch: 6| Step: 7
Training loss: 2.929448232417008
Validation loss: 2.4882310447007345

Epoch: 6| Step: 8
Training loss: 2.691812293617864
Validation loss: 2.4823019715313426

Epoch: 6| Step: 9
Training loss: 2.42268751727621
Validation loss: 2.472454500925562

Epoch: 6| Step: 10
Training loss: 2.936314120252288
Validation loss: 2.4986409738801796

Epoch: 6| Step: 11
Training loss: 2.4959820885076724
Validation loss: 2.5240554421233923

Epoch: 6| Step: 12
Training loss: 2.544417242694176
Validation loss: 2.509035469780427

Epoch: 6| Step: 13
Training loss: 2.23408154247779
Validation loss: 2.4857541037994157

Epoch: 140| Step: 0
Training loss: 2.7003401153533284
Validation loss: 2.4672503264200296

Epoch: 6| Step: 1
Training loss: 2.9021679502822555
Validation loss: 2.527549309134166

Epoch: 6| Step: 2
Training loss: 2.504835887566073
Validation loss: 2.519820795499288

Epoch: 6| Step: 3
Training loss: 2.8972236168397703
Validation loss: 2.5121864497464403

Epoch: 6| Step: 4
Training loss: 2.7361022725314843
Validation loss: 2.5388239175607303

Epoch: 6| Step: 5
Training loss: 2.172208142973446
Validation loss: 2.5159455649704574

Epoch: 6| Step: 6
Training loss: 1.8308026519251113
Validation loss: 2.5173012747116505

Epoch: 6| Step: 7
Training loss: 3.303332328244495
Validation loss: 2.4748332800987876

Epoch: 6| Step: 8
Training loss: 2.6822555292651895
Validation loss: 2.5217458106930564

Epoch: 6| Step: 9
Training loss: 2.2450107783205384
Validation loss: 2.525468189097864

Epoch: 6| Step: 10
Training loss: 3.361829278127591
Validation loss: 2.5340097469376737

Epoch: 6| Step: 11
Training loss: 2.845213125924593
Validation loss: 2.5079530008216944

Epoch: 6| Step: 12
Training loss: 2.9851576492877347
Validation loss: 2.4986925336300594

Epoch: 6| Step: 13
Training loss: 2.2926590822491324
Validation loss: 2.5468027947771827

Epoch: 141| Step: 0
Training loss: 2.2367622054152783
Validation loss: 2.5076030674034935

Epoch: 6| Step: 1
Training loss: 2.704845079650076
Validation loss: 2.510001421427409

Epoch: 6| Step: 2
Training loss: 2.768636669310878
Validation loss: 2.497080733070869

Epoch: 6| Step: 3
Training loss: 2.876970693789703
Validation loss: 2.4946769609032

Epoch: 6| Step: 4
Training loss: 2.4409339386886306
Validation loss: 2.506283820331741

Epoch: 6| Step: 5
Training loss: 3.175226855245998
Validation loss: 2.5281493980444956

Epoch: 6| Step: 6
Training loss: 3.0868544363111976
Validation loss: 2.4738292881070882

Epoch: 6| Step: 7
Training loss: 2.710042668390269
Validation loss: 2.485222908402083

Epoch: 6| Step: 8
Training loss: 2.4717566614965043
Validation loss: 2.4909326315057325

Epoch: 6| Step: 9
Training loss: 3.0619235372034788
Validation loss: 2.5030319860456505

Epoch: 6| Step: 10
Training loss: 3.139195966375406
Validation loss: 2.4677856262693725

Epoch: 6| Step: 11
Training loss: 2.8395520759666417
Validation loss: 2.521211316307042

Epoch: 6| Step: 12
Training loss: 2.276796147512258
Validation loss: 2.4753527867479113

Epoch: 6| Step: 13
Training loss: 1.739150886311733
Validation loss: 2.4978539800488733

Epoch: 142| Step: 0
Training loss: 2.270287812762316
Validation loss: 2.5010864542723863

Epoch: 6| Step: 1
Training loss: 2.5839413778428524
Validation loss: 2.508905166018079

Epoch: 6| Step: 2
Training loss: 2.265836508513575
Validation loss: 2.5105331854079234

Epoch: 6| Step: 3
Training loss: 2.9019550047737552
Validation loss: 2.49663230530404

Epoch: 6| Step: 4
Training loss: 3.2559381668818004
Validation loss: 2.501719150315545

Epoch: 6| Step: 5
Training loss: 2.929790200022845
Validation loss: 2.5004149441248558

Epoch: 6| Step: 6
Training loss: 2.49638353077548
Validation loss: 2.5060109979073446

Epoch: 6| Step: 7
Training loss: 2.9808051843107704
Validation loss: 2.4888139537230978

Epoch: 6| Step: 8
Training loss: 3.303916606765069
Validation loss: 2.475515187434446

Epoch: 6| Step: 9
Training loss: 3.042119780630702
Validation loss: 2.4970976913020007

Epoch: 6| Step: 10
Training loss: 3.0090282014569234
Validation loss: 2.5244400967205416

Epoch: 6| Step: 11
Training loss: 2.5853113067413935
Validation loss: 2.522195453287018

Epoch: 6| Step: 12
Training loss: 1.6404369428119707
Validation loss: 2.5163581985751664

Epoch: 6| Step: 13
Training loss: 1.4407163380032015
Validation loss: 2.5098065456219207

Epoch: 143| Step: 0
Training loss: 2.4043714448224787
Validation loss: 2.505787180816683

Epoch: 6| Step: 1
Training loss: 2.72203234142231
Validation loss: 2.503465895656416

Epoch: 6| Step: 2
Training loss: 2.3620033024596045
Validation loss: 2.514942526795779

Epoch: 6| Step: 3
Training loss: 2.4099882738136786
Validation loss: 2.5058756477150435

Epoch: 6| Step: 4
Training loss: 2.926746896615863
Validation loss: 2.504161248948484

Epoch: 6| Step: 5
Training loss: 2.9002114186008936
Validation loss: 2.5110557310182586

Epoch: 6| Step: 6
Training loss: 2.373175874194409
Validation loss: 2.486383113722316

Epoch: 6| Step: 7
Training loss: 3.234420112626964
Validation loss: 2.4921443880317575

Epoch: 6| Step: 8
Training loss: 3.4454154693371386
Validation loss: 2.491157793488849

Epoch: 6| Step: 9
Training loss: 3.0429689067012955
Validation loss: 2.508785859976918

Epoch: 6| Step: 10
Training loss: 2.2431980739966164
Validation loss: 2.509778997999234

Epoch: 6| Step: 11
Training loss: 2.066327329689769
Validation loss: 2.5125449051545705

Epoch: 6| Step: 12
Training loss: 2.340577687089612
Validation loss: 2.515994671091496

Epoch: 6| Step: 13
Training loss: 3.5903006544107834
Validation loss: 2.51541380982063

Epoch: 144| Step: 0
Training loss: 2.454481393768043
Validation loss: 2.5253936653609665

Epoch: 6| Step: 1
Training loss: 2.482272620107362
Validation loss: 2.4981815954411077

Epoch: 6| Step: 2
Training loss: 3.2692267163281756
Validation loss: 2.5218107826364156

Epoch: 6| Step: 3
Training loss: 2.5068753116429625
Validation loss: 2.485382380956171

Epoch: 6| Step: 4
Training loss: 2.8243344020774703
Validation loss: 2.538396120680614

Epoch: 6| Step: 5
Training loss: 2.353449270492253
Validation loss: 2.494584517444637

Epoch: 6| Step: 6
Training loss: 2.4579280799419236
Validation loss: 2.512355301160044

Epoch: 6| Step: 7
Training loss: 2.9397099075444735
Validation loss: 2.521474470105735

Epoch: 6| Step: 8
Training loss: 2.759988677236579
Validation loss: 2.5034999559051596

Epoch: 6| Step: 9
Training loss: 3.008272527219311
Validation loss: 2.553743600304111

Epoch: 6| Step: 10
Training loss: 2.7969104615419518
Validation loss: 2.533801947746146

Epoch: 6| Step: 11
Training loss: 2.9842254441420546
Validation loss: 2.5089826593828417

Epoch: 6| Step: 12
Training loss: 2.468300500823537
Validation loss: 2.5014832782749266

Epoch: 6| Step: 13
Training loss: 2.5162547016431502
Validation loss: 2.508491136779552

Epoch: 145| Step: 0
Training loss: 1.6886301141817182
Validation loss: 2.487724185305779

Epoch: 6| Step: 1
Training loss: 2.938367512620093
Validation loss: 2.511145227184286

Epoch: 6| Step: 2
Training loss: 2.938388122059333
Validation loss: 2.509783762087461

Epoch: 6| Step: 3
Training loss: 3.0121129590620295
Validation loss: 2.4992172189866118

Epoch: 6| Step: 4
Training loss: 2.370273102424724
Validation loss: 2.4869538539394394

Epoch: 6| Step: 5
Training loss: 3.0403525941055545
Validation loss: 2.4991354401068477

Epoch: 6| Step: 6
Training loss: 2.894544758424113
Validation loss: 2.5040668505602475

Epoch: 6| Step: 7
Training loss: 2.0974050474442065
Validation loss: 2.5201543542581906

Epoch: 6| Step: 8
Training loss: 2.629047361670616
Validation loss: 2.508731302518607

Epoch: 6| Step: 9
Training loss: 3.5782278787407136
Validation loss: 2.533088809206365

Epoch: 6| Step: 10
Training loss: 2.832340365673145
Validation loss: 2.507967045871059

Epoch: 6| Step: 11
Training loss: 2.2754048752236176
Validation loss: 2.4958370865757273

Epoch: 6| Step: 12
Training loss: 2.69998842519469
Validation loss: 2.5402397159174748

Epoch: 6| Step: 13
Training loss: 2.3786352344597272
Validation loss: 2.5157322831112388

Epoch: 146| Step: 0
Training loss: 2.512559053097119
Validation loss: 2.518283574179055

Epoch: 6| Step: 1
Training loss: 2.5872679454296486
Validation loss: 2.5216964526118106

Epoch: 6| Step: 2
Training loss: 2.6087590021582203
Validation loss: 2.5247431985215036

Epoch: 6| Step: 3
Training loss: 2.978117609164983
Validation loss: 2.515737437427991

Epoch: 6| Step: 4
Training loss: 2.478245497247782
Validation loss: 2.532140731313916

Epoch: 6| Step: 5
Training loss: 2.4524054960344506
Validation loss: 2.5142877522884675

Epoch: 6| Step: 6
Training loss: 2.700211205875468
Validation loss: 2.5137235088931726

Epoch: 6| Step: 7
Training loss: 2.9991651009056226
Validation loss: 2.482583143065086

Epoch: 6| Step: 8
Training loss: 2.707317386713934
Validation loss: 2.5159366470419777

Epoch: 6| Step: 9
Training loss: 2.458369874413248
Validation loss: 2.487157240831205

Epoch: 6| Step: 10
Training loss: 2.746108769990262
Validation loss: 2.4873867497022393

Epoch: 6| Step: 11
Training loss: 2.83741221691246
Validation loss: 2.4736056279223093

Epoch: 6| Step: 12
Training loss: 3.197843486598854
Validation loss: 2.496573992975691

Epoch: 6| Step: 13
Training loss: 2.1535707496799246
Validation loss: 2.50714652462082

Epoch: 147| Step: 0
Training loss: 2.9696560731830823
Validation loss: 2.486890760844561

Epoch: 6| Step: 1
Training loss: 2.759492443518808
Validation loss: 2.468993035188386

Epoch: 6| Step: 2
Training loss: 2.430332699201612
Validation loss: 2.49375453610033

Epoch: 6| Step: 3
Training loss: 3.1456152901441823
Validation loss: 2.521076046974595

Epoch: 6| Step: 4
Training loss: 2.285445757402279
Validation loss: 2.473861315929287

Epoch: 6| Step: 5
Training loss: 2.8785474701174767
Validation loss: 2.49470215861493

Epoch: 6| Step: 6
Training loss: 2.2518078640541717
Validation loss: 2.517756027307637

Epoch: 6| Step: 7
Training loss: 2.9476944982340747
Validation loss: 2.496647822854143

Epoch: 6| Step: 8
Training loss: 3.111916275133761
Validation loss: 2.487534110625412

Epoch: 6| Step: 9
Training loss: 2.2668346464332196
Validation loss: 2.474430487265324

Epoch: 6| Step: 10
Training loss: 2.8882685773765977
Validation loss: 2.4978418797597324

Epoch: 6| Step: 11
Training loss: 2.315717778168359
Validation loss: 2.50749606755478

Epoch: 6| Step: 12
Training loss: 2.8608379686293777
Validation loss: 2.5344315241975703

Epoch: 6| Step: 13
Training loss: 2.1497420355823067
Validation loss: 2.5212532606604117

Epoch: 148| Step: 0
Training loss: 2.1694249666614702
Validation loss: 2.503195827284473

Epoch: 6| Step: 1
Training loss: 2.717125505965309
Validation loss: 2.4840900910197634

Epoch: 6| Step: 2
Training loss: 3.087837503496265
Validation loss: 2.4682506545897978

Epoch: 6| Step: 3
Training loss: 2.718880222753465
Validation loss: 2.503920990418332

Epoch: 6| Step: 4
Training loss: 2.5998502027934864
Validation loss: 2.5138089776465518

Epoch: 6| Step: 5
Training loss: 1.8738215239796046
Validation loss: 2.5314910826530674

Epoch: 6| Step: 6
Training loss: 2.961100148587653
Validation loss: 2.505922626837176

Epoch: 6| Step: 7
Training loss: 2.442366900844061
Validation loss: 2.4838228639556017

Epoch: 6| Step: 8
Training loss: 2.3861291035260117
Validation loss: 2.5506287505170784

Epoch: 6| Step: 9
Training loss: 3.192541585844067
Validation loss: 2.5011058105014947

Epoch: 6| Step: 10
Training loss: 2.6031848328160856
Validation loss: 2.518186380747567

Epoch: 6| Step: 11
Training loss: 3.151756184592828
Validation loss: 2.5144424068937314

Epoch: 6| Step: 12
Training loss: 2.5582155836346305
Validation loss: 2.5074373131406267

Epoch: 6| Step: 13
Training loss: 3.259697750566581
Validation loss: 2.4642464191363005

Epoch: 149| Step: 0
Training loss: 2.5403212024315818
Validation loss: 2.500607714058589

Epoch: 6| Step: 1
Training loss: 3.1022524126412216
Validation loss: 2.4801946445769243

Epoch: 6| Step: 2
Training loss: 2.313520464112261
Validation loss: 2.5018141358099295

Epoch: 6| Step: 3
Training loss: 2.613062006901176
Validation loss: 2.5028774980422526

Epoch: 6| Step: 4
Training loss: 2.8640007582615534
Validation loss: 2.482772926639832

Epoch: 6| Step: 5
Training loss: 3.3838255600935723
Validation loss: 2.4925432815650517

Epoch: 6| Step: 6
Training loss: 3.3320489952267804
Validation loss: 2.5212934300079013

Epoch: 6| Step: 7
Training loss: 2.2066039625442238
Validation loss: 2.4982692470049024

Epoch: 6| Step: 8
Training loss: 3.2261061080065527
Validation loss: 2.4861892171493287

Epoch: 6| Step: 9
Training loss: 2.641552824180696
Validation loss: 2.5326728037672375

Epoch: 6| Step: 10
Training loss: 2.4490653298738754
Validation loss: 2.5033176507088966

Epoch: 6| Step: 11
Training loss: 2.517404246928978
Validation loss: 2.490374105169823

Epoch: 6| Step: 12
Training loss: 2.0357985992945493
Validation loss: 2.5154209888616275

Epoch: 6| Step: 13
Training loss: 2.0889382941015087
Validation loss: 2.4892284401055753

Epoch: 150| Step: 0
Training loss: 2.821354554638558
Validation loss: 2.5071868979978262

Epoch: 6| Step: 1
Training loss: 2.987354648517733
Validation loss: 2.4838765921966517

Epoch: 6| Step: 2
Training loss: 2.72020638580459
Validation loss: 2.5148614167722574

Epoch: 6| Step: 3
Training loss: 2.7370775219019428
Validation loss: 2.5202359246144153

Epoch: 6| Step: 4
Training loss: 2.4931353736598507
Validation loss: 2.509313780653975

Epoch: 6| Step: 5
Training loss: 2.2520463962213664
Validation loss: 2.5077072825309403

Epoch: 6| Step: 6
Training loss: 3.3287374443364612
Validation loss: 2.4857691106829316

Epoch: 6| Step: 7
Training loss: 2.6044598732235613
Validation loss: 2.519546166608184

Epoch: 6| Step: 8
Training loss: 2.1768607675293556
Validation loss: 2.5129482035422788

Epoch: 6| Step: 9
Training loss: 3.0626485360383504
Validation loss: 2.4817307534017075

Epoch: 6| Step: 10
Training loss: 2.386617255667282
Validation loss: 2.511790281747694

Epoch: 6| Step: 11
Training loss: 2.796513539404774
Validation loss: 2.482214294224447

Epoch: 6| Step: 12
Training loss: 1.9846918649074372
Validation loss: 2.5001647033581107

Epoch: 6| Step: 13
Training loss: 3.7787487490112777
Validation loss: 2.479926228640042

Epoch: 151| Step: 0
Training loss: 2.8319694564917963
Validation loss: 2.508210153825373

Epoch: 6| Step: 1
Training loss: 2.8231465901749946
Validation loss: 2.504130933414272

Epoch: 6| Step: 2
Training loss: 2.6614963556150415
Validation loss: 2.50879593043696

Epoch: 6| Step: 3
Training loss: 1.894672443594952
Validation loss: 2.497021208952823

Epoch: 6| Step: 4
Training loss: 2.2335495924746014
Validation loss: 2.5166020168363707

Epoch: 6| Step: 5
Training loss: 2.465599657515911
Validation loss: 2.506330426639811

Epoch: 6| Step: 6
Training loss: 3.208904281610406
Validation loss: 2.4933329641811186

Epoch: 6| Step: 7
Training loss: 3.037148625650709
Validation loss: 2.513468465203248

Epoch: 6| Step: 8
Training loss: 2.5568941717704208
Validation loss: 2.5037115213074808

Epoch: 6| Step: 9
Training loss: 2.7128231295599408
Validation loss: 2.501715309031323

Epoch: 6| Step: 10
Training loss: 2.63238168695094
Validation loss: 2.530225270882398

Epoch: 6| Step: 11
Training loss: 3.233859256038551
Validation loss: 2.53406781948997

Epoch: 6| Step: 12
Training loss: 2.4139699578679545
Validation loss: 2.5064483603360044

Epoch: 6| Step: 13
Training loss: 3.241374380341641
Validation loss: 2.512054800058418

Epoch: 152| Step: 0
Training loss: 2.9839959827631275
Validation loss: 2.487258571930318

Epoch: 6| Step: 1
Training loss: 2.5906968128664323
Validation loss: 2.51475443190428

Epoch: 6| Step: 2
Training loss: 2.860827134575051
Validation loss: 2.5034704567111556

Epoch: 6| Step: 3
Training loss: 1.9206395714179454
Validation loss: 2.5151719832087607

Epoch: 6| Step: 4
Training loss: 3.031205953690941
Validation loss: 2.48513457696074

Epoch: 6| Step: 5
Training loss: 1.9531986070114316
Validation loss: 2.51209982567298

Epoch: 6| Step: 6
Training loss: 2.437534625467709
Validation loss: 2.5054056177203154

Epoch: 6| Step: 7
Training loss: 2.4059660112322017
Validation loss: 2.4946054389154626

Epoch: 6| Step: 8
Training loss: 2.2531405888836913
Validation loss: 2.468303652012268

Epoch: 6| Step: 9
Training loss: 2.2031603871202234
Validation loss: 2.495018968322015

Epoch: 6| Step: 10
Training loss: 2.9444224178841876
Validation loss: 2.478217067078212

Epoch: 6| Step: 11
Training loss: 3.3821874309738225
Validation loss: 2.485508122795343

Epoch: 6| Step: 12
Training loss: 3.2622571306702492
Validation loss: 2.5081743402212386

Epoch: 6| Step: 13
Training loss: 3.044346939236392
Validation loss: 2.497348281479672

Epoch: 153| Step: 0
Training loss: 2.481619116090826
Validation loss: 2.4952117873091204

Epoch: 6| Step: 1
Training loss: 2.6808564330736173
Validation loss: 2.495204251142229

Epoch: 6| Step: 2
Training loss: 3.295039674001979
Validation loss: 2.506390286099721

Epoch: 6| Step: 3
Training loss: 2.5133964192022304
Validation loss: 2.5092886826724903

Epoch: 6| Step: 4
Training loss: 2.668919049164139
Validation loss: 2.4997772014972903

Epoch: 6| Step: 5
Training loss: 2.3438427716014316
Validation loss: 2.4827708047044346

Epoch: 6| Step: 6
Training loss: 3.4654763385062353
Validation loss: 2.4985749459284308

Epoch: 6| Step: 7
Training loss: 2.1222966894540516
Validation loss: 2.5336955420624827

Epoch: 6| Step: 8
Training loss: 2.4725418908067933
Validation loss: 2.5189664743958544

Epoch: 6| Step: 9
Training loss: 1.8575033817799564
Validation loss: 2.5329839089258623

Epoch: 6| Step: 10
Training loss: 3.0885048532248667
Validation loss: 2.5342168179280495

Epoch: 6| Step: 11
Training loss: 3.0822139632647567
Validation loss: 2.497119005452533

Epoch: 6| Step: 12
Training loss: 2.744184413574118
Validation loss: 2.485869059723338

Epoch: 6| Step: 13
Training loss: 2.3975217978527743
Validation loss: 2.5005539813550572

Epoch: 154| Step: 0
Training loss: 2.598265979310656
Validation loss: 2.4970888580060033

Epoch: 6| Step: 1
Training loss: 2.675425351274391
Validation loss: 2.5387058115427545

Epoch: 6| Step: 2
Training loss: 2.632534295390328
Validation loss: 2.5090217004589417

Epoch: 6| Step: 3
Training loss: 2.8408245087950275
Validation loss: 2.523293619623497

Epoch: 6| Step: 4
Training loss: 3.112449927482597
Validation loss: 2.4878962251048127

Epoch: 6| Step: 5
Training loss: 2.3127952593390306
Validation loss: 2.487062103756081

Epoch: 6| Step: 6
Training loss: 2.968917199496035
Validation loss: 2.5013461744248366

Epoch: 6| Step: 7
Training loss: 3.060144881265581
Validation loss: 2.5106754985656576

Epoch: 6| Step: 8
Training loss: 2.439100962402184
Validation loss: 2.514862996836157

Epoch: 6| Step: 9
Training loss: 2.651463762916546
Validation loss: 2.5074912827617846

Epoch: 6| Step: 10
Training loss: 2.3350454815308996
Validation loss: 2.477818291380718

Epoch: 6| Step: 11
Training loss: 2.7593764066422573
Validation loss: 2.491985166631652

Epoch: 6| Step: 12
Training loss: 2.691155896771865
Validation loss: 2.5234437896265094

Epoch: 6| Step: 13
Training loss: 2.583610571064505
Validation loss: 2.483713381945854

Epoch: 155| Step: 0
Training loss: 2.7601838979407987
Validation loss: 2.499929844476922

Epoch: 6| Step: 1
Training loss: 2.2306086668366216
Validation loss: 2.4877523388552216

Epoch: 6| Step: 2
Training loss: 2.5664513988965285
Validation loss: 2.486121911872737

Epoch: 6| Step: 3
Training loss: 3.4458857122189652
Validation loss: 2.504784477814543

Epoch: 6| Step: 4
Training loss: 2.48982035467613
Validation loss: 2.498394088862042

Epoch: 6| Step: 5
Training loss: 2.44587676634125
Validation loss: 2.513756713205521

Epoch: 6| Step: 6
Training loss: 2.906677870889237
Validation loss: 2.496798639545331

Epoch: 6| Step: 7
Training loss: 3.0824744772492547
Validation loss: 2.533481705384173

Epoch: 6| Step: 8
Training loss: 2.855196040916958
Validation loss: 2.502874503053601

Epoch: 6| Step: 9
Training loss: 2.1987056912917984
Validation loss: 2.53285116611381

Epoch: 6| Step: 10
Training loss: 2.1582639935915937
Validation loss: 2.5239000510188405

Epoch: 6| Step: 11
Training loss: 2.825532768446975
Validation loss: 2.522379724186114

Epoch: 6| Step: 12
Training loss: 2.8548180795087177
Validation loss: 2.52021416714889

Epoch: 6| Step: 13
Training loss: 2.5751689262904454
Validation loss: 2.517385325604731

Epoch: 156| Step: 0
Training loss: 2.2171874655897637
Validation loss: 2.4980621169375876

Epoch: 6| Step: 1
Training loss: 2.307846438933809
Validation loss: 2.518434966203435

Epoch: 6| Step: 2
Training loss: 2.5755313658362327
Validation loss: 2.5066287298556498

Epoch: 6| Step: 3
Training loss: 2.7248493047920435
Validation loss: 2.4945661804399526

Epoch: 6| Step: 4
Training loss: 2.890802161484044
Validation loss: 2.5091637969176266

Epoch: 6| Step: 5
Training loss: 2.4222356712286803
Validation loss: 2.509580976964444

Epoch: 6| Step: 6
Training loss: 2.9203650505028036
Validation loss: 2.5089154229908988

Epoch: 6| Step: 7
Training loss: 2.622207245900694
Validation loss: 2.5215042141564195

Epoch: 6| Step: 8
Training loss: 3.0852963324723737
Validation loss: 2.5266783588171475

Epoch: 6| Step: 9
Training loss: 2.5603671618260013
Validation loss: 2.517741204971328

Epoch: 6| Step: 10
Training loss: 2.594831493985252
Validation loss: 2.4978318749555193

Epoch: 6| Step: 11
Training loss: 2.405474451101051
Validation loss: 2.502416288004183

Epoch: 6| Step: 12
Training loss: 3.4632981905532554
Validation loss: 2.5020689227642845

Epoch: 6| Step: 13
Training loss: 2.8962194793634066
Validation loss: 2.4897633773760828

Epoch: 157| Step: 0
Training loss: 2.9524761476126575
Validation loss: 2.481834685488533

Epoch: 6| Step: 1
Training loss: 2.803389309911273
Validation loss: 2.524151863147876

Epoch: 6| Step: 2
Training loss: 2.2556186035606736
Validation loss: 2.513849146120567

Epoch: 6| Step: 3
Training loss: 2.5852663949908234
Validation loss: 2.4937559198208556

Epoch: 6| Step: 4
Training loss: 2.51474533338803
Validation loss: 2.506121387035057

Epoch: 6| Step: 5
Training loss: 2.239756685393667
Validation loss: 2.5240555010330157

Epoch: 6| Step: 6
Training loss: 2.811441349422512
Validation loss: 2.4829017944175775

Epoch: 6| Step: 7
Training loss: 2.7360069418590443
Validation loss: 2.4810482263916245

Epoch: 6| Step: 8
Training loss: 2.137839628308725
Validation loss: 2.512523415250831

Epoch: 6| Step: 9
Training loss: 2.2052167336999626
Validation loss: 2.511602711219071

Epoch: 6| Step: 10
Training loss: 2.801858394249392
Validation loss: 2.5028048296334404

Epoch: 6| Step: 11
Training loss: 3.4950341009350994
Validation loss: 2.5172951398122794

Epoch: 6| Step: 12
Training loss: 3.0440040560681543
Validation loss: 2.4945521565432442

Epoch: 6| Step: 13
Training loss: 2.76464309072028
Validation loss: 2.5363663335413666

Epoch: 158| Step: 0
Training loss: 2.4612808230258736
Validation loss: 2.5313916842115227

Epoch: 6| Step: 1
Training loss: 2.518606373972837
Validation loss: 2.4811063441020407

Epoch: 6| Step: 2
Training loss: 2.6477375737293865
Validation loss: 2.4836027555321105

Epoch: 6| Step: 3
Training loss: 2.1778463702634037
Validation loss: 2.499661661654416

Epoch: 6| Step: 4
Training loss: 2.3284252408382904
Validation loss: 2.482195338069049

Epoch: 6| Step: 5
Training loss: 2.674425391399771
Validation loss: 2.5269255610319417

Epoch: 6| Step: 6
Training loss: 2.3240503073810155
Validation loss: 2.504667696075113

Epoch: 6| Step: 7
Training loss: 3.267465757089871
Validation loss: 2.4976696761750836

Epoch: 6| Step: 8
Training loss: 2.950965535835511
Validation loss: 2.4962233555757334

Epoch: 6| Step: 9
Training loss: 2.779389777276138
Validation loss: 2.4906952459691185

Epoch: 6| Step: 10
Training loss: 2.996080699429693
Validation loss: 2.5128416891555236

Epoch: 6| Step: 11
Training loss: 2.964241539630912
Validation loss: 2.515650920343307

Epoch: 6| Step: 12
Training loss: 2.968205291568133
Validation loss: 2.5347641544766573

Epoch: 6| Step: 13
Training loss: 3.044383120634635
Validation loss: 2.51100060570576

Epoch: 159| Step: 0
Training loss: 2.6316800271247334
Validation loss: 2.492681060891807

Epoch: 6| Step: 1
Training loss: 2.501079421663378
Validation loss: 2.5290313166994234

Epoch: 6| Step: 2
Training loss: 1.9112227569295068
Validation loss: 2.5165849170257446

Epoch: 6| Step: 3
Training loss: 3.3544870701380183
Validation loss: 2.5028736150039514

Epoch: 6| Step: 4
Training loss: 1.7093640063837165
Validation loss: 2.488372308076534

Epoch: 6| Step: 5
Training loss: 2.642058034439939
Validation loss: 2.4898573558618424

Epoch: 6| Step: 6
Training loss: 3.3225060252069705
Validation loss: 2.5198941462203805

Epoch: 6| Step: 7
Training loss: 2.8450403325284124
Validation loss: 2.4834512954128254

Epoch: 6| Step: 8
Training loss: 2.3854064830135098
Validation loss: 2.534195800618745

Epoch: 6| Step: 9
Training loss: 3.3461839605404724
Validation loss: 2.522343171116481

Epoch: 6| Step: 10
Training loss: 3.1560623566398402
Validation loss: 2.5412686737534824

Epoch: 6| Step: 11
Training loss: 1.9152292860330362
Validation loss: 2.5318960228588745

Epoch: 6| Step: 12
Training loss: 2.6855074570970796
Validation loss: 2.5155481199240053

Epoch: 6| Step: 13
Training loss: 2.4634498003766647
Validation loss: 2.501618915306617

Epoch: 160| Step: 0
Training loss: 2.387246830484226
Validation loss: 2.536545589358073

Epoch: 6| Step: 1
Training loss: 2.581571737193709
Validation loss: 2.5167986614011393

Epoch: 6| Step: 2
Training loss: 2.7831260268926705
Validation loss: 2.5177769878788983

Epoch: 6| Step: 3
Training loss: 2.7941386726934674
Validation loss: 2.5099464333180155

Epoch: 6| Step: 4
Training loss: 2.932509870975984
Validation loss: 2.51207957853365

Epoch: 6| Step: 5
Training loss: 1.8133252336985013
Validation loss: 2.49474886003033

Epoch: 6| Step: 6
Training loss: 2.4286002029188904
Validation loss: 2.510424764098711

Epoch: 6| Step: 7
Training loss: 2.8081529719752933
Validation loss: 2.509717842401481

Epoch: 6| Step: 8
Training loss: 3.379518098505318
Validation loss: 2.521499202783772

Epoch: 6| Step: 9
Training loss: 3.328365979850352
Validation loss: 2.540606768094313

Epoch: 6| Step: 10
Training loss: 2.5721951810304766
Validation loss: 2.5252966958381364

Epoch: 6| Step: 11
Training loss: 2.6954939186849827
Validation loss: 2.465514472376338

Epoch: 6| Step: 12
Training loss: 2.213655021607364
Validation loss: 2.5156444074172177

Epoch: 6| Step: 13
Training loss: 2.8007809094970155
Validation loss: 2.540347396565302

Epoch: 161| Step: 0
Training loss: 3.0892180570065544
Validation loss: 2.5462758758544806

Epoch: 6| Step: 1
Training loss: 2.5020891043929714
Validation loss: 2.4909681526839846

Epoch: 6| Step: 2
Training loss: 3.722534305780787
Validation loss: 2.5295064125017457

Epoch: 6| Step: 3
Training loss: 2.563891614464987
Validation loss: 2.540225266986874

Epoch: 6| Step: 4
Training loss: 1.9132654515739957
Validation loss: 2.502435144256032

Epoch: 6| Step: 5
Training loss: 2.600679081485351
Validation loss: 2.521429127927144

Epoch: 6| Step: 6
Training loss: 2.5951451605326126
Validation loss: 2.519008437300607

Epoch: 6| Step: 7
Training loss: 2.356663238148801
Validation loss: 2.528969328099773

Epoch: 6| Step: 8
Training loss: 2.695030396666936
Validation loss: 2.5146077373813776

Epoch: 6| Step: 9
Training loss: 2.044573239517354
Validation loss: 2.548603814136512

Epoch: 6| Step: 10
Training loss: 3.3635026989559247
Validation loss: 2.5176974909425134

Epoch: 6| Step: 11
Training loss: 2.443079309551032
Validation loss: 2.5126557510188676

Epoch: 6| Step: 12
Training loss: 2.6880405680796726
Validation loss: 2.4963748664523457

Epoch: 6| Step: 13
Training loss: 2.706894908295419
Validation loss: 2.486840949104511

Epoch: 162| Step: 0
Training loss: 2.5562168408320756
Validation loss: 2.5132007610762646

Epoch: 6| Step: 1
Training loss: 2.5470431219673397
Validation loss: 2.5229545346207103

Epoch: 6| Step: 2
Training loss: 2.757474105865555
Validation loss: 2.507254451702837

Epoch: 6| Step: 3
Training loss: 2.767355770636149
Validation loss: 2.510480801711062

Epoch: 6| Step: 4
Training loss: 2.507663710508979
Validation loss: 2.5205655645174923

Epoch: 6| Step: 5
Training loss: 2.608744288074069
Validation loss: 2.504839623252466

Epoch: 6| Step: 6
Training loss: 2.2618583148594924
Validation loss: 2.4779780423456845

Epoch: 6| Step: 7
Training loss: 4.047314005130802
Validation loss: 2.479131212512671

Epoch: 6| Step: 8
Training loss: 2.390125895739668
Validation loss: 2.485842246156571

Epoch: 6| Step: 9
Training loss: 3.0263627757913953
Validation loss: 2.4973252061790876

Epoch: 6| Step: 10
Training loss: 2.3704453010548394
Validation loss: 2.5139506383111496

Epoch: 6| Step: 11
Training loss: 2.2258123907800935
Validation loss: 2.5051836550633846

Epoch: 6| Step: 12
Training loss: 2.545676575458312
Validation loss: 2.4825819637779754

Epoch: 6| Step: 13
Training loss: 2.5771996889442494
Validation loss: 2.50652916472825

Epoch: 163| Step: 0
Training loss: 2.3531608345778476
Validation loss: 2.5159202712708373

Epoch: 6| Step: 1
Training loss: 2.7533409458298395
Validation loss: 2.5235215872817593

Epoch: 6| Step: 2
Training loss: 2.706307010182292
Validation loss: 2.4868757576369633

Epoch: 6| Step: 3
Training loss: 2.6108209863641276
Validation loss: 2.5300795800081723

Epoch: 6| Step: 4
Training loss: 1.9257760502433605
Validation loss: 2.5220498390745467

Epoch: 6| Step: 5
Training loss: 2.852653566417611
Validation loss: 2.481373771766223

Epoch: 6| Step: 6
Training loss: 2.547077007122557
Validation loss: 2.4847055221637135

Epoch: 6| Step: 7
Training loss: 2.511658473500349
Validation loss: 2.521555455861449

Epoch: 6| Step: 8
Training loss: 2.2673596285908793
Validation loss: 2.4977822747360228

Epoch: 6| Step: 9
Training loss: 2.879990434365811
Validation loss: 2.5247086521254665

Epoch: 6| Step: 10
Training loss: 2.8989388991013
Validation loss: 2.5333014535463203

Epoch: 6| Step: 11
Training loss: 3.025738296578189
Validation loss: 2.503087805091324

Epoch: 6| Step: 12
Training loss: 2.14260060273362
Validation loss: 2.5115290652626907

Epoch: 6| Step: 13
Training loss: 4.001797748935929
Validation loss: 2.501119940126155

Epoch: 164| Step: 0
Training loss: 2.7564854873781695
Validation loss: 2.506042161257852

Epoch: 6| Step: 1
Training loss: 2.0182900958964862
Validation loss: 2.5224700285125983

Epoch: 6| Step: 2
Training loss: 2.2696590945664274
Validation loss: 2.5058499157836165

Epoch: 6| Step: 3
Training loss: 2.981189565625824
Validation loss: 2.467513237116526

Epoch: 6| Step: 4
Training loss: 2.5656970479794565
Validation loss: 2.508792749381753

Epoch: 6| Step: 5
Training loss: 2.5722442139449155
Validation loss: 2.481607591340249

Epoch: 6| Step: 6
Training loss: 2.9224268754176017
Validation loss: 2.526320484040662

Epoch: 6| Step: 7
Training loss: 2.944792441030769
Validation loss: 2.49224015710455

Epoch: 6| Step: 8
Training loss: 2.0711157050420614
Validation loss: 2.4983520500998337

Epoch: 6| Step: 9
Training loss: 2.978844756405354
Validation loss: 2.53109837377571

Epoch: 6| Step: 10
Training loss: 2.9929397474062194
Validation loss: 2.496263701247628

Epoch: 6| Step: 11
Training loss: 2.739435363563999
Validation loss: 2.5393021483555867

Epoch: 6| Step: 12
Training loss: 2.736273057752466
Validation loss: 2.5173542782345777

Epoch: 6| Step: 13
Training loss: 3.1019111648717246
Validation loss: 2.4889600374023235

Epoch: 165| Step: 0
Training loss: 3.0802996549118875
Validation loss: 2.5240284593400424

Epoch: 6| Step: 1
Training loss: 2.069702749609403
Validation loss: 2.530174876783253

Epoch: 6| Step: 2
Training loss: 2.3365699141638445
Validation loss: 2.5244968092830455

Epoch: 6| Step: 3
Training loss: 2.400706556742855
Validation loss: 2.494493781714253

Epoch: 6| Step: 4
Training loss: 2.9405339582662187
Validation loss: 2.4864033752405774

Epoch: 6| Step: 5
Training loss: 2.9889685943576993
Validation loss: 2.5262501999173823

Epoch: 6| Step: 6
Training loss: 2.682122728280592
Validation loss: 2.5068610661735713

Epoch: 6| Step: 7
Training loss: 3.0818191281268876
Validation loss: 2.5150137874502856

Epoch: 6| Step: 8
Training loss: 2.7979110418524957
Validation loss: 2.507994374204214

Epoch: 6| Step: 9
Training loss: 2.6049779518351244
Validation loss: 2.500645776630723

Epoch: 6| Step: 10
Training loss: 2.4605285304709383
Validation loss: 2.511128067754971

Epoch: 6| Step: 11
Training loss: 2.552886047760762
Validation loss: 2.5393251107513124

Epoch: 6| Step: 12
Training loss: 2.9353638044316885
Validation loss: 2.503851176154345

Epoch: 6| Step: 13
Training loss: 1.9135456866653182
Validation loss: 2.5113618375414615

Epoch: 166| Step: 0
Training loss: 2.032978906644946
Validation loss: 2.5461881858888287

Epoch: 6| Step: 1
Training loss: 2.64630914649941
Validation loss: 2.5396385570533853

Epoch: 6| Step: 2
Training loss: 3.2938187883349297
Validation loss: 2.524606520092633

Epoch: 6| Step: 3
Training loss: 2.701706177667508
Validation loss: 2.521663530760119

Epoch: 6| Step: 4
Training loss: 2.074244957020176
Validation loss: 2.5297670759170776

Epoch: 6| Step: 5
Training loss: 3.1329007790320724
Validation loss: 2.4892404671749007

Epoch: 6| Step: 6
Training loss: 2.230215829916525
Validation loss: 2.500776986415155

Epoch: 6| Step: 7
Training loss: 2.8825466788119467
Validation loss: 2.5037830524622957

Epoch: 6| Step: 8
Training loss: 2.6496748382881297
Validation loss: 2.492374659633561

Epoch: 6| Step: 9
Training loss: 2.2475222614276356
Validation loss: 2.48578893270757

Epoch: 6| Step: 10
Training loss: 2.9329052966826397
Validation loss: 2.5001006834203445

Epoch: 6| Step: 11
Training loss: 3.430443891045562
Validation loss: 2.535286239000813

Epoch: 6| Step: 12
Training loss: 2.227334942594554
Validation loss: 2.5146357612198766

Epoch: 6| Step: 13
Training loss: 2.9668872008345017
Validation loss: 2.5100199633134013

Epoch: 167| Step: 0
Training loss: 2.451541754183204
Validation loss: 2.5073646383431156

Epoch: 6| Step: 1
Training loss: 3.6960884553802438
Validation loss: 2.507878861283878

Epoch: 6| Step: 2
Training loss: 1.9911154461723113
Validation loss: 2.4942128335487412

Epoch: 6| Step: 3
Training loss: 3.4024166270180074
Validation loss: 2.5294586338965446

Epoch: 6| Step: 4
Training loss: 2.454513157044437
Validation loss: 2.511357789999144

Epoch: 6| Step: 5
Training loss: 2.826991407286219
Validation loss: 2.482003318534459

Epoch: 6| Step: 6
Training loss: 2.7481169756073145
Validation loss: 2.5104232220893254

Epoch: 6| Step: 7
Training loss: 2.6490536313472686
Validation loss: 2.511202312988918

Epoch: 6| Step: 8
Training loss: 2.8260627098064015
Validation loss: 2.5073975296486894

Epoch: 6| Step: 9
Training loss: 3.0149715684148015
Validation loss: 2.5139122876899025

Epoch: 6| Step: 10
Training loss: 2.033975035087544
Validation loss: 2.4915212282651598

Epoch: 6| Step: 11
Training loss: 2.2237968535094907
Validation loss: 2.5101690819158975

Epoch: 6| Step: 12
Training loss: 1.839052585001416
Validation loss: 2.524828928851534

Epoch: 6| Step: 13
Training loss: 2.471244613958036
Validation loss: 2.530027816845222

Epoch: 168| Step: 0
Training loss: 2.3021366809400114
Validation loss: 2.514796174557378

Epoch: 6| Step: 1
Training loss: 3.01611592936635
Validation loss: 2.4884639654940264

Epoch: 6| Step: 2
Training loss: 2.074306565202075
Validation loss: 2.510982713252009

Epoch: 6| Step: 3
Training loss: 2.886532417447041
Validation loss: 2.502497069949529

Epoch: 6| Step: 4
Training loss: 3.043876541922458
Validation loss: 2.5205428212777665

Epoch: 6| Step: 5
Training loss: 3.4112795872731376
Validation loss: 2.476113138332948

Epoch: 6| Step: 6
Training loss: 2.2386279411844017
Validation loss: 2.4964519683207995

Epoch: 6| Step: 7
Training loss: 2.690211237019572
Validation loss: 2.5244089543342536

Epoch: 6| Step: 8
Training loss: 2.548326324526436
Validation loss: 2.5069404102603134

Epoch: 6| Step: 9
Training loss: 2.374263448441942
Validation loss: 2.5102984867320193

Epoch: 6| Step: 10
Training loss: 3.2193469632823235
Validation loss: 2.5244186781200004

Epoch: 6| Step: 11
Training loss: 2.1697357400055703
Validation loss: 2.515793141691135

Epoch: 6| Step: 12
Training loss: 2.6993890424302642
Validation loss: 2.525181569325326

Epoch: 6| Step: 13
Training loss: 2.3726714916574982
Validation loss: 2.4980064293309443

Epoch: 169| Step: 0
Training loss: 2.1037042191454556
Validation loss: 2.496437888267859

Epoch: 6| Step: 1
Training loss: 2.7077625162185623
Validation loss: 2.478657127135906

Epoch: 6| Step: 2
Training loss: 2.796256567069516
Validation loss: 2.539221987073533

Epoch: 6| Step: 3
Training loss: 4.094534529430093
Validation loss: 2.506561711583799

Epoch: 6| Step: 4
Training loss: 2.319138971899676
Validation loss: 2.532336148997409

Epoch: 6| Step: 5
Training loss: 2.45480464119164
Validation loss: 2.5193447645941647

Epoch: 6| Step: 6
Training loss: 2.521262728687812
Validation loss: 2.531582435645376

Epoch: 6| Step: 7
Training loss: 2.4145521142722557
Validation loss: 2.5170567898413956

Epoch: 6| Step: 8
Training loss: 2.9521217862363818
Validation loss: 2.5201980989126658

Epoch: 6| Step: 9
Training loss: 2.864929640673275
Validation loss: 2.503997848668256

Epoch: 6| Step: 10
Training loss: 2.75076127352472
Validation loss: 2.4921341443291705

Epoch: 6| Step: 11
Training loss: 2.5381154333487137
Validation loss: 2.4852885597321936

Epoch: 6| Step: 12
Training loss: 2.0806838035810773
Validation loss: 2.5307845874409756

Epoch: 6| Step: 13
Training loss: 2.406536927393834
Validation loss: 2.495996869560516

Epoch: 170| Step: 0
Training loss: 2.2354108636738688
Validation loss: 2.4976463354343115

Epoch: 6| Step: 1
Training loss: 2.6266695798415602
Validation loss: 2.509175240567298

Epoch: 6| Step: 2
Training loss: 3.4499716881958653
Validation loss: 2.531117730411293

Epoch: 6| Step: 3
Training loss: 3.105521262822519
Validation loss: 2.514568225359048

Epoch: 6| Step: 4
Training loss: 3.0431545920665646
Validation loss: 2.5205925447048476

Epoch: 6| Step: 5
Training loss: 2.645774560310613
Validation loss: 2.50808753464767

Epoch: 6| Step: 6
Training loss: 1.9049743386289908
Validation loss: 2.523079277306942

Epoch: 6| Step: 7
Training loss: 3.2245774339726356
Validation loss: 2.519407552709877

Epoch: 6| Step: 8
Training loss: 2.832306862935674
Validation loss: 2.495344230057709

Epoch: 6| Step: 9
Training loss: 2.0728684105040593
Validation loss: 2.5689697418744206

Epoch: 6| Step: 10
Training loss: 2.8790062896034407
Validation loss: 2.498276774952256

Epoch: 6| Step: 11
Training loss: 2.622612548682205
Validation loss: 2.5119628869254242

Epoch: 6| Step: 12
Training loss: 2.3003315769519226
Validation loss: 2.521714538453709

Epoch: 6| Step: 13
Training loss: 1.8726613400755747
Validation loss: 2.537046282273465

Epoch: 171| Step: 0
Training loss: 3.1633061764847428
Validation loss: 2.531814820068754

Epoch: 6| Step: 1
Training loss: 1.8835700319585567
Validation loss: 2.5044546913527594

Epoch: 6| Step: 2
Training loss: 2.9504702306427664
Validation loss: 2.5189611679193735

Epoch: 6| Step: 3
Training loss: 2.9081322922032067
Validation loss: 2.5487475039101106

Epoch: 6| Step: 4
Training loss: 2.7079224103577224
Validation loss: 2.5173480548645

Epoch: 6| Step: 5
Training loss: 2.4618465621634478
Validation loss: 2.506888758350963

Epoch: 6| Step: 6
Training loss: 2.9035832465344718
Validation loss: 2.461125848076286

Epoch: 6| Step: 7
Training loss: 2.19711609734275
Validation loss: 2.476271656540882

Epoch: 6| Step: 8
Training loss: 2.525132876384131
Validation loss: 2.4875467158029787

Epoch: 6| Step: 9
Training loss: 2.6980246346765053
Validation loss: 2.5036176266534453

Epoch: 6| Step: 10
Training loss: 2.8898449644326996
Validation loss: 2.49420686181701

Epoch: 6| Step: 11
Training loss: 2.707096776127936
Validation loss: 2.508104544181254

Epoch: 6| Step: 12
Training loss: 2.7221213298274853
Validation loss: 2.4895082742960226

Epoch: 6| Step: 13
Training loss: 2.5764854997981583
Validation loss: 2.484393964289923

Epoch: 172| Step: 0
Training loss: 2.749933935585626
Validation loss: 2.5048331364597645

Epoch: 6| Step: 1
Training loss: 2.1152077740998174
Validation loss: 2.533329994148726

Epoch: 6| Step: 2
Training loss: 2.8654367366337703
Validation loss: 2.4942098446011522

Epoch: 6| Step: 3
Training loss: 1.6128463343615689
Validation loss: 2.4980617423556684

Epoch: 6| Step: 4
Training loss: 2.3790058685140556
Validation loss: 2.4946508503794056

Epoch: 6| Step: 5
Training loss: 2.780515595245391
Validation loss: 2.5370356267141005

Epoch: 6| Step: 6
Training loss: 2.4363416217860077
Validation loss: 2.5311551184515046

Epoch: 6| Step: 7
Training loss: 3.1035500411087327
Validation loss: 2.5150495525899657

Epoch: 6| Step: 8
Training loss: 3.2869548795299757
Validation loss: 2.4829650270758195

Epoch: 6| Step: 9
Training loss: 2.725113273156179
Validation loss: 2.4910237151314156

Epoch: 6| Step: 10
Training loss: 2.7884346391508634
Validation loss: 2.507041528710155

Epoch: 6| Step: 11
Training loss: 3.0430173270151863
Validation loss: 2.5018375964964403

Epoch: 6| Step: 12
Training loss: 2.419282078930711
Validation loss: 2.5211652281139925

Epoch: 6| Step: 13
Training loss: 2.6948585902042264
Validation loss: 2.531488117466175

Epoch: 173| Step: 0
Training loss: 2.132729106013286
Validation loss: 2.508290239162265

Epoch: 6| Step: 1
Training loss: 3.6000502053044885
Validation loss: 2.490264806173434

Epoch: 6| Step: 2
Training loss: 2.72186162619037
Validation loss: 2.5042480472998423

Epoch: 6| Step: 3
Training loss: 2.3149522717954505
Validation loss: 2.487410197020826

Epoch: 6| Step: 4
Training loss: 2.4135231971171236
Validation loss: 2.522203607092826

Epoch: 6| Step: 5
Training loss: 2.7182298359989168
Validation loss: 2.503436840498134

Epoch: 6| Step: 6
Training loss: 2.52800222129362
Validation loss: 2.52029545482249

Epoch: 6| Step: 7
Training loss: 2.441181630292018
Validation loss: 2.5296455472702144

Epoch: 6| Step: 8
Training loss: 2.437682707370162
Validation loss: 2.4924841551319705

Epoch: 6| Step: 9
Training loss: 2.1865325968432545
Validation loss: 2.5094648710011778

Epoch: 6| Step: 10
Training loss: 2.720164314804555
Validation loss: 2.5094726708178077

Epoch: 6| Step: 11
Training loss: 3.0636950905282743
Validation loss: 2.49030456894435

Epoch: 6| Step: 12
Training loss: 2.4640810310861916
Validation loss: 2.5093766246286684

Epoch: 6| Step: 13
Training loss: 3.696579181769914
Validation loss: 2.478905561069112

Epoch: 174| Step: 0
Training loss: 2.4861420878701
Validation loss: 2.4828434399481805

Epoch: 6| Step: 1
Training loss: 2.9232311671407323
Validation loss: 2.49626367557285

Epoch: 6| Step: 2
Training loss: 2.2961709117611577
Validation loss: 2.53357163373318

Epoch: 6| Step: 3
Training loss: 2.4191751505665424
Validation loss: 2.4867304649186748

Epoch: 6| Step: 4
Training loss: 2.6279116331888646
Validation loss: 2.50346279179964

Epoch: 6| Step: 5
Training loss: 2.7553694064405385
Validation loss: 2.5325527163429187

Epoch: 6| Step: 6
Training loss: 3.0155441984904345
Validation loss: 2.5049411171611675

Epoch: 6| Step: 7
Training loss: 2.417082542503038
Validation loss: 2.479272370856734

Epoch: 6| Step: 8
Training loss: 2.506716194458265
Validation loss: 2.4903619662209047

Epoch: 6| Step: 9
Training loss: 2.9199061977003886
Validation loss: 2.489545649804818

Epoch: 6| Step: 10
Training loss: 2.704591650892495
Validation loss: 2.4838697730185237

Epoch: 6| Step: 11
Training loss: 2.978916789070493
Validation loss: 2.498865835251877

Epoch: 6| Step: 12
Training loss: 2.707555854904563
Validation loss: 2.4922502789913015

Epoch: 6| Step: 13
Training loss: 2.197299587411138
Validation loss: 2.5254678236564385

Epoch: 175| Step: 0
Training loss: 2.2914696233019063
Validation loss: 2.4975505006236203

Epoch: 6| Step: 1
Training loss: 2.5326305439357744
Validation loss: 2.529310838017256

Epoch: 6| Step: 2
Training loss: 3.23167033606628
Validation loss: 2.501595615512802

Epoch: 6| Step: 3
Training loss: 3.320336483139486
Validation loss: 2.528483412405703

Epoch: 6| Step: 4
Training loss: 2.3476214160334283
Validation loss: 2.5232808993988636

Epoch: 6| Step: 5
Training loss: 2.4960706348572703
Validation loss: 2.489230241388968

Epoch: 6| Step: 6
Training loss: 2.947475782299772
Validation loss: 2.530038975121126

Epoch: 6| Step: 7
Training loss: 3.0399807128796352
Validation loss: 2.5173606105563966

Epoch: 6| Step: 8
Training loss: 2.8599035889707904
Validation loss: 2.549909440712081

Epoch: 6| Step: 9
Training loss: 2.5612134727751608
Validation loss: 2.487431500392937

Epoch: 6| Step: 10
Training loss: 1.9662791899535117
Validation loss: 2.5259232322771186

Epoch: 6| Step: 11
Training loss: 2.011513471777794
Validation loss: 2.520446160473692

Epoch: 6| Step: 12
Training loss: 2.4988164961393573
Validation loss: 2.514057685936514

Epoch: 6| Step: 13
Training loss: 3.2085372232492606
Validation loss: 2.495010464175093

Epoch: 176| Step: 0
Training loss: 2.6322175663998624
Validation loss: 2.510005214793438

Epoch: 6| Step: 1
Training loss: 3.0805036774526524
Validation loss: 2.5372943274112174

Epoch: 6| Step: 2
Training loss: 2.907152589491798
Validation loss: 2.4833386649155544

Epoch: 6| Step: 3
Training loss: 3.018479019544556
Validation loss: 2.5097648956730616

Epoch: 6| Step: 4
Training loss: 2.3686508842405574
Validation loss: 2.520859791321819

Epoch: 6| Step: 5
Training loss: 3.189688884667161
Validation loss: 2.494009909998499

Epoch: 6| Step: 6
Training loss: 2.5041124851079934
Validation loss: 2.4721822985066746

Epoch: 6| Step: 7
Training loss: 2.516759770504853
Validation loss: 2.4798734620296465

Epoch: 6| Step: 8
Training loss: 2.405067351383566
Validation loss: 2.530978104281411

Epoch: 6| Step: 9
Training loss: 2.2044581715073517
Validation loss: 2.517587291687126

Epoch: 6| Step: 10
Training loss: 2.6016228929446146
Validation loss: 2.509149806587809

Epoch: 6| Step: 11
Training loss: 2.4661641158836467
Validation loss: 2.506130765436874

Epoch: 6| Step: 12
Training loss: 3.0825284646259945
Validation loss: 2.5303137741869146

Epoch: 6| Step: 13
Training loss: 1.8045875554669186
Validation loss: 2.4994452999373586

Epoch: 177| Step: 0
Training loss: 2.882019099196736
Validation loss: 2.5328562430834762

Epoch: 6| Step: 1
Training loss: 1.737101979944415
Validation loss: 2.476980336428847

Epoch: 6| Step: 2
Training loss: 2.1930451002375726
Validation loss: 2.500316183543401

Epoch: 6| Step: 3
Training loss: 2.7272154455238242
Validation loss: 2.531069361334319

Epoch: 6| Step: 4
Training loss: 2.774461068410096
Validation loss: 2.5127056988797576

Epoch: 6| Step: 5
Training loss: 2.3509482154870733
Validation loss: 2.509044210951166

Epoch: 6| Step: 6
Training loss: 2.938487759356783
Validation loss: 2.5255774192001503

Epoch: 6| Step: 7
Training loss: 2.457373178339438
Validation loss: 2.5266815589543614

Epoch: 6| Step: 8
Training loss: 2.452716769127766
Validation loss: 2.499753632762856

Epoch: 6| Step: 9
Training loss: 3.155182761870257
Validation loss: 2.51843670893307

Epoch: 6| Step: 10
Training loss: 2.038934111663872
Validation loss: 2.5245085332679214

Epoch: 6| Step: 11
Training loss: 3.4004925707632294
Validation loss: 2.471411816647809

Epoch: 6| Step: 12
Training loss: 3.0888540657818826
Validation loss: 2.50045858350809

Epoch: 6| Step: 13
Training loss: 2.2749908489001327
Validation loss: 2.5116424434130233

Epoch: 178| Step: 0
Training loss: 2.723838166503253
Validation loss: 2.5133791384626596

Epoch: 6| Step: 1
Training loss: 2.5220088162188676
Validation loss: 2.521933560003387

Epoch: 6| Step: 2
Training loss: 2.3920683648600654
Validation loss: 2.51159289188299

Epoch: 6| Step: 3
Training loss: 3.365094944151546
Validation loss: 2.4894071006793186

Epoch: 6| Step: 4
Training loss: 2.4469986729651416
Validation loss: 2.4870846274629814

Epoch: 6| Step: 5
Training loss: 3.1252580154715295
Validation loss: 2.524550800091694

Epoch: 6| Step: 6
Training loss: 2.135390850624405
Validation loss: 2.4932799846742912

Epoch: 6| Step: 7
Training loss: 2.4498698997987507
Validation loss: 2.503973211366038

Epoch: 6| Step: 8
Training loss: 2.3777512125014773
Validation loss: 2.4956945744322474

Epoch: 6| Step: 9
Training loss: 2.0450398612392906
Validation loss: 2.507081476348043

Epoch: 6| Step: 10
Training loss: 3.2126786831690874
Validation loss: 2.5302886139854133

Epoch: 6| Step: 11
Training loss: 2.7508588663476496
Validation loss: 2.5496867593302475

Epoch: 6| Step: 12
Training loss: 2.039797946235976
Validation loss: 2.4915685526404983

Epoch: 6| Step: 13
Training loss: 3.8564213249443893
Validation loss: 2.511725729344447

Epoch: 179| Step: 0
Training loss: 2.7849328164124807
Validation loss: 2.523497312281327

Epoch: 6| Step: 1
Training loss: 2.9685551629629856
Validation loss: 2.51297596330268

Epoch: 6| Step: 2
Training loss: 2.991247442796748
Validation loss: 2.5177505512908263

Epoch: 6| Step: 3
Training loss: 2.3787927960682023
Validation loss: 2.517192223220341

Epoch: 6| Step: 4
Training loss: 1.937032704760119
Validation loss: 2.5296608491345105

Epoch: 6| Step: 5
Training loss: 2.6819494726668487
Validation loss: 2.506283720088989

Epoch: 6| Step: 6
Training loss: 2.852981172678024
Validation loss: 2.5048276593133387

Epoch: 6| Step: 7
Training loss: 2.4288216670325102
Validation loss: 2.547491286801451

Epoch: 6| Step: 8
Training loss: 2.4874095497747346
Validation loss: 2.5126684546329017

Epoch: 6| Step: 9
Training loss: 2.4970559905607077
Validation loss: 2.4986771919121047

Epoch: 6| Step: 10
Training loss: 2.3821053878442044
Validation loss: 2.4662644852317217

Epoch: 6| Step: 11
Training loss: 2.6773183598245307
Validation loss: 2.4934632437800346

Epoch: 6| Step: 12
Training loss: 3.5061836113993947
Validation loss: 2.4939774439876268

Epoch: 6| Step: 13
Training loss: 2.5168293978223715
Validation loss: 2.5229469827582185

Epoch: 180| Step: 0
Training loss: 3.5921533105923134
Validation loss: 2.5229590675563793

Epoch: 6| Step: 1
Training loss: 2.4185990357909586
Validation loss: 2.5176401710691145

Epoch: 6| Step: 2
Training loss: 2.4394924139275087
Validation loss: 2.4699274435218626

Epoch: 6| Step: 3
Training loss: 2.5124429041509884
Validation loss: 2.5216605926497557

Epoch: 6| Step: 4
Training loss: 2.4052429073377057
Validation loss: 2.479281396883336

Epoch: 6| Step: 5
Training loss: 2.5397876818970935
Validation loss: 2.497203573950388

Epoch: 6| Step: 6
Training loss: 2.3099858819439296
Validation loss: 2.5199545177008504

Epoch: 6| Step: 7
Training loss: 2.206662631689034
Validation loss: 2.5268873545750328

Epoch: 6| Step: 8
Training loss: 2.796034446869758
Validation loss: 2.4914685307563644

Epoch: 6| Step: 9
Training loss: 1.9688833282374
Validation loss: 2.490637903721366

Epoch: 6| Step: 10
Training loss: 2.3917563759419833
Validation loss: 2.514198769732782

Epoch: 6| Step: 11
Training loss: 3.700680726811876
Validation loss: 2.544230334353331

Epoch: 6| Step: 12
Training loss: 2.649609871703569
Validation loss: 2.505484750049157

Epoch: 6| Step: 13
Training loss: 2.62786309460172
Validation loss: 2.5137177140542297

Epoch: 181| Step: 0
Training loss: 2.5426851222817577
Validation loss: 2.5526152604949095

Epoch: 6| Step: 1
Training loss: 3.0546921830007263
Validation loss: 2.5274121363244406

Epoch: 6| Step: 2
Training loss: 1.9010571901567248
Validation loss: 2.525002795168835

Epoch: 6| Step: 3
Training loss: 2.929532385216575
Validation loss: 2.478409207216883

Epoch: 6| Step: 4
Training loss: 2.6007521458398486
Validation loss: 2.4934939809999404

Epoch: 6| Step: 5
Training loss: 3.1961083948232982
Validation loss: 2.532946948678532

Epoch: 6| Step: 6
Training loss: 2.5848401095084457
Validation loss: 2.5116938250696332

Epoch: 6| Step: 7
Training loss: 2.4064354763134945
Validation loss: 2.5169196654912205

Epoch: 6| Step: 8
Training loss: 2.448672292974821
Validation loss: 2.5349641230955897

Epoch: 6| Step: 9
Training loss: 2.7604292863281454
Validation loss: 2.5672418154377716

Epoch: 6| Step: 10
Training loss: 3.116240316866364
Validation loss: 2.5114463965075964

Epoch: 6| Step: 11
Training loss: 3.0716481494673142
Validation loss: 2.528006995659961

Epoch: 6| Step: 12
Training loss: 2.123437306879188
Validation loss: 2.5188250388293936

Epoch: 6| Step: 13
Training loss: 2.2630131892933747
Validation loss: 2.555610899536251

Epoch: 182| Step: 0
Training loss: 2.1600264498186164
Validation loss: 2.4969155470170583

Epoch: 6| Step: 1
Training loss: 2.6935453509769935
Validation loss: 2.480319970859035

Epoch: 6| Step: 2
Training loss: 2.3992236709668475
Validation loss: 2.510800534780339

Epoch: 6| Step: 3
Training loss: 2.0921583816063225
Validation loss: 2.4851309715501504

Epoch: 6| Step: 4
Training loss: 2.135162065416408
Validation loss: 2.530591039593152

Epoch: 6| Step: 5
Training loss: 3.0697528824148432
Validation loss: 2.4973816080439

Epoch: 6| Step: 6
Training loss: 2.3390376210084085
Validation loss: 2.5192988681617074

Epoch: 6| Step: 7
Training loss: 2.5441844747223925
Validation loss: 2.483290440921995

Epoch: 6| Step: 8
Training loss: 2.5417629494695975
Validation loss: 2.5321849057119405

Epoch: 6| Step: 9
Training loss: 3.509918600154698
Validation loss: 2.5287183535735487

Epoch: 6| Step: 10
Training loss: 3.2597938568373777
Validation loss: 2.468837873027655

Epoch: 6| Step: 11
Training loss: 2.9521797726173578
Validation loss: 2.538000203458529

Epoch: 6| Step: 12
Training loss: 2.7443147325772292
Validation loss: 2.51357931317613

Epoch: 6| Step: 13
Training loss: 2.2094132523954952
Validation loss: 2.517041651722343

Epoch: 183| Step: 0
Training loss: 3.5235256006429454
Validation loss: 2.54099545793943

Epoch: 6| Step: 1
Training loss: 2.7512700875921197
Validation loss: 2.5050639885961625

Epoch: 6| Step: 2
Training loss: 2.644469402722602
Validation loss: 2.5167787006065474

Epoch: 6| Step: 3
Training loss: 2.54926183525616
Validation loss: 2.4958970004876706

Epoch: 6| Step: 4
Training loss: 2.120569547225547
Validation loss: 2.5331885256234186

Epoch: 6| Step: 5
Training loss: 2.7262480371355338
Validation loss: 2.5236185467916266

Epoch: 6| Step: 6
Training loss: 2.1006358773410407
Validation loss: 2.4932352124837824

Epoch: 6| Step: 7
Training loss: 2.8184053942015606
Validation loss: 2.504343216341505

Epoch: 6| Step: 8
Training loss: 2.3625216689957806
Validation loss: 2.5227719738002286

Epoch: 6| Step: 9
Training loss: 2.551325087239011
Validation loss: 2.529124092500158

Epoch: 6| Step: 10
Training loss: 2.435593079202874
Validation loss: 2.5125565604284708

Epoch: 6| Step: 11
Training loss: 3.0367093034326955
Validation loss: 2.477497685491794

Epoch: 6| Step: 12
Training loss: 2.5554465003402194
Validation loss: 2.517784559830271

Epoch: 6| Step: 13
Training loss: 2.5400139067862364
Validation loss: 2.503635252264664

Epoch: 184| Step: 0
Training loss: 2.7117691743119536
Validation loss: 2.5346062297711707

Epoch: 6| Step: 1
Training loss: 2.274861207857448
Validation loss: 2.503495424600874

Epoch: 6| Step: 2
Training loss: 2.2743145800954228
Validation loss: 2.4824831949453

Epoch: 6| Step: 3
Training loss: 3.1045783535836295
Validation loss: 2.4633238593221027

Epoch: 6| Step: 4
Training loss: 2.3449658101401645
Validation loss: 2.535210220088815

Epoch: 6| Step: 5
Training loss: 2.6386535327649403
Validation loss: 2.5129371815744848

Epoch: 6| Step: 6
Training loss: 2.8725021125911847
Validation loss: 2.5289850963043627

Epoch: 6| Step: 7
Training loss: 3.0512549585710778
Validation loss: 2.4797430474036153

Epoch: 6| Step: 8
Training loss: 2.8624707749386595
Validation loss: 2.5122473738141444

Epoch: 6| Step: 9
Training loss: 2.919841854568236
Validation loss: 2.512108384734671

Epoch: 6| Step: 10
Training loss: 2.305853803971385
Validation loss: 2.559531643813633

Epoch: 6| Step: 11
Training loss: 2.6175761830957858
Validation loss: 2.512116145728199

Epoch: 6| Step: 12
Training loss: 2.4913682697280453
Validation loss: 2.5084201333579004

Epoch: 6| Step: 13
Training loss: 2.281383353738224
Validation loss: 2.5108457666717685

Epoch: 185| Step: 0
Training loss: 2.737217237944887
Validation loss: 2.529451944698176

Epoch: 6| Step: 1
Training loss: 2.6747176903751106
Validation loss: 2.5312655554905814

Epoch: 6| Step: 2
Training loss: 3.096983789713792
Validation loss: 2.4915999990048388

Epoch: 6| Step: 3
Training loss: 3.131824518366694
Validation loss: 2.530485537322351

Epoch: 6| Step: 4
Training loss: 1.978462783394034
Validation loss: 2.5008252412289136

Epoch: 6| Step: 5
Training loss: 2.369568385969249
Validation loss: 2.498506960544942

Epoch: 6| Step: 6
Training loss: 2.6844177313746376
Validation loss: 2.5234937698082964

Epoch: 6| Step: 7
Training loss: 2.1138196671801657
Validation loss: 2.5208223675379418

Epoch: 6| Step: 8
Training loss: 2.7663037173110063
Validation loss: 2.4842058143374315

Epoch: 6| Step: 9
Training loss: 2.6009307185841677
Validation loss: 2.5197054594875214

Epoch: 6| Step: 10
Training loss: 2.7788157579446615
Validation loss: 2.5193316957508123

Epoch: 6| Step: 11
Training loss: 2.420287270165112
Validation loss: 2.513175701912346

Epoch: 6| Step: 12
Training loss: 2.7567628586610096
Validation loss: 2.509469389483366

Epoch: 6| Step: 13
Training loss: 2.7838641964357342
Validation loss: 2.488451642090658

Epoch: 186| Step: 0
Training loss: 3.1732645232757033
Validation loss: 2.525052952624911

Epoch: 6| Step: 1
Training loss: 3.3549089419974187
Validation loss: 2.4968679256098474

Epoch: 6| Step: 2
Training loss: 1.986544286918351
Validation loss: 2.501657940130492

Epoch: 6| Step: 3
Training loss: 2.486469465554968
Validation loss: 2.526611069768274

Epoch: 6| Step: 4
Training loss: 3.0187159692334875
Validation loss: 2.483600452633671

Epoch: 6| Step: 5
Training loss: 2.2885720831045346
Validation loss: 2.5330124986503098

Epoch: 6| Step: 6
Training loss: 2.3723191640771186
Validation loss: 2.51242905554709

Epoch: 6| Step: 7
Training loss: 2.1698980321527057
Validation loss: 2.482949534490412

Epoch: 6| Step: 8
Training loss: 2.4194357124265444
Validation loss: 2.5335824576644765

Epoch: 6| Step: 9
Training loss: 3.3232126293498894
Validation loss: 2.5206326457593167

Epoch: 6| Step: 10
Training loss: 2.712927096446483
Validation loss: 2.5428970286726873

Epoch: 6| Step: 11
Training loss: 2.612589244139566
Validation loss: 2.5359589571396945

Epoch: 6| Step: 12
Training loss: 1.9045229107637172
Validation loss: 2.506239838950745

Epoch: 6| Step: 13
Training loss: 3.440240495835931
Validation loss: 2.536533101342166

Epoch: 187| Step: 0
Training loss: 3.023110699796264
Validation loss: 2.5485127071370823

Epoch: 6| Step: 1
Training loss: 3.3200729463215843
Validation loss: 2.51260189334861

Epoch: 6| Step: 2
Training loss: 2.1944643601856466
Validation loss: 2.5106743569801027

Epoch: 6| Step: 3
Training loss: 2.192573875478857
Validation loss: 2.513961828175286

Epoch: 6| Step: 4
Training loss: 2.76352141816598
Validation loss: 2.526832689290893

Epoch: 6| Step: 5
Training loss: 2.790772328006636
Validation loss: 2.5139557687471576

Epoch: 6| Step: 6
Training loss: 2.4361352278722594
Validation loss: 2.518985386896506

Epoch: 6| Step: 7
Training loss: 2.9421807190284643
Validation loss: 2.5083732316723637

Epoch: 6| Step: 8
Training loss: 2.1975927490867417
Validation loss: 2.5309383251596334

Epoch: 6| Step: 9
Training loss: 2.6887620136271027
Validation loss: 2.5192329277821415

Epoch: 6| Step: 10
Training loss: 3.1032101653091635
Validation loss: 2.4998338039079346

Epoch: 6| Step: 11
Training loss: 2.2054490621284617
Validation loss: 2.510423589720788

Epoch: 6| Step: 12
Training loss: 2.305124739211302
Validation loss: 2.5087632194322933

Epoch: 6| Step: 13
Training loss: 2.696116628801253
Validation loss: 2.530564476006676

Epoch: 188| Step: 0
Training loss: 2.982175485841994
Validation loss: 2.5527737662760512

Epoch: 6| Step: 1
Training loss: 2.09870960916955
Validation loss: 2.5092123869861567

Epoch: 6| Step: 2
Training loss: 2.5883095916753076
Validation loss: 2.5123270243293714

Epoch: 6| Step: 3
Training loss: 2.8776034301940987
Validation loss: 2.5145661200591896

Epoch: 6| Step: 4
Training loss: 2.4424696902656153
Validation loss: 2.5058949014762786

Epoch: 6| Step: 5
Training loss: 2.9458514056984555
Validation loss: 2.501785319699067

Epoch: 6| Step: 6
Training loss: 2.35265276908427
Validation loss: 2.475810027646839

Epoch: 6| Step: 7
Training loss: 3.1871882267485567
Validation loss: 2.510680744951274

Epoch: 6| Step: 8
Training loss: 2.4917295985955397
Validation loss: 2.52492340034245

Epoch: 6| Step: 9
Training loss: 2.618470381418644
Validation loss: 2.560894027933916

Epoch: 6| Step: 10
Training loss: 3.34238184809232
Validation loss: 2.5061305547096446

Epoch: 6| Step: 11
Training loss: 2.464853616202025
Validation loss: 2.5215493607965365

Epoch: 6| Step: 12
Training loss: 2.0246759697017196
Validation loss: 2.525988696580772

Epoch: 6| Step: 13
Training loss: 2.2870497463672272
Validation loss: 2.5172847133005973

Epoch: 189| Step: 0
Training loss: 2.736570338026381
Validation loss: 2.5494308747823338

Epoch: 6| Step: 1
Training loss: 2.922197619899882
Validation loss: 2.492316449813125

Epoch: 6| Step: 2
Training loss: 3.0043830641972984
Validation loss: 2.519351038998802

Epoch: 6| Step: 3
Training loss: 2.405965416663681
Validation loss: 2.5201495029625

Epoch: 6| Step: 4
Training loss: 3.31165432481493
Validation loss: 2.5058884562910615

Epoch: 6| Step: 5
Training loss: 2.5692912104739607
Validation loss: 2.526342234581492

Epoch: 6| Step: 6
Training loss: 3.2300210294245613
Validation loss: 2.494477608435424

Epoch: 6| Step: 7
Training loss: 2.5570902592941938
Validation loss: 2.5106820590986985

Epoch: 6| Step: 8
Training loss: 2.8092211790481185
Validation loss: 2.503160286056898

Epoch: 6| Step: 9
Training loss: 2.227542487530247
Validation loss: 2.5126693188150586

Epoch: 6| Step: 10
Training loss: 2.2421820770091756
Validation loss: 2.569029479999218

Epoch: 6| Step: 11
Training loss: 2.1731567991364114
Validation loss: 2.5244207762186233

Epoch: 6| Step: 12
Training loss: 2.050844725580155
Validation loss: 2.540619120044693

Epoch: 6| Step: 13
Training loss: 2.5160047351143677
Validation loss: 2.529476999709166

Epoch: 190| Step: 0
Training loss: 2.367938939041875
Validation loss: 2.505654057170575

Epoch: 6| Step: 1
Training loss: 2.622736181746179
Validation loss: 2.4913339293994974

Epoch: 6| Step: 2
Training loss: 2.262666546503325
Validation loss: 2.5037474919246288

Epoch: 6| Step: 3
Training loss: 2.3392495243014544
Validation loss: 2.5357754470666705

Epoch: 6| Step: 4
Training loss: 2.623596724585239
Validation loss: 2.529243117272042

Epoch: 6| Step: 5
Training loss: 2.6885765935576895
Validation loss: 2.5086115983401704

Epoch: 6| Step: 6
Training loss: 2.3565248364508213
Validation loss: 2.4979109639533887

Epoch: 6| Step: 7
Training loss: 2.638652358134706
Validation loss: 2.5017259874487

Epoch: 6| Step: 8
Training loss: 2.1755556164214287
Validation loss: 2.522107213232619

Epoch: 6| Step: 9
Training loss: 3.812454348431534
Validation loss: 2.5289223239734833

Epoch: 6| Step: 10
Training loss: 2.6109645364983516
Validation loss: 2.4571103209584226

Epoch: 6| Step: 11
Training loss: 2.504582972734098
Validation loss: 2.547573812332923

Epoch: 6| Step: 12
Training loss: 2.6269930584790044
Validation loss: 2.5331151752848093

Epoch: 6| Step: 13
Training loss: 2.8591838835336207
Validation loss: 2.500546410026905

Epoch: 191| Step: 0
Training loss: 2.550640388560126
Validation loss: 2.5123822756394723

Epoch: 6| Step: 1
Training loss: 2.1781834160785425
Validation loss: 2.4782181108579886

Epoch: 6| Step: 2
Training loss: 2.6169507275419
Validation loss: 2.5234987853472255

Epoch: 6| Step: 3
Training loss: 3.0754767319728256
Validation loss: 2.470995941508303

Epoch: 6| Step: 4
Training loss: 3.2712004307747304
Validation loss: 2.5039707224362626

Epoch: 6| Step: 5
Training loss: 2.1828119451348726
Validation loss: 2.50976247479641

Epoch: 6| Step: 6
Training loss: 2.9572282974926387
Validation loss: 2.49662421379577

Epoch: 6| Step: 7
Training loss: 2.375623119326406
Validation loss: 2.506343003770834

Epoch: 6| Step: 8
Training loss: 2.952724692042018
Validation loss: 2.512101868747229

Epoch: 6| Step: 9
Training loss: 2.418310483285158
Validation loss: 2.542803558659454

Epoch: 6| Step: 10
Training loss: 2.1887356129450684
Validation loss: 2.5633452667349426

Epoch: 6| Step: 11
Training loss: 3.08052333596704
Validation loss: 2.5218880979929215

Epoch: 6| Step: 12
Training loss: 2.7423368668699406
Validation loss: 2.5141909356339327

Epoch: 6| Step: 13
Training loss: 1.8252015825534358
Validation loss: 2.4781465928231827

Epoch: 192| Step: 0
Training loss: 2.732843635080732
Validation loss: 2.5288632624476386

Epoch: 6| Step: 1
Training loss: 2.6675446872847113
Validation loss: 2.5127311014575153

Epoch: 6| Step: 2
Training loss: 2.9611606966709507
Validation loss: 2.5169841539475035

Epoch: 6| Step: 3
Training loss: 2.749296271707366
Validation loss: 2.549954701233827

Epoch: 6| Step: 4
Training loss: 2.0483502611661497
Validation loss: 2.529839314391221

Epoch: 6| Step: 5
Training loss: 2.0687272822581946
Validation loss: 2.5603012037566533

Epoch: 6| Step: 6
Training loss: 2.1389726261850215
Validation loss: 2.53401987700225

Epoch: 6| Step: 7
Training loss: 3.468188953562759
Validation loss: 2.528909945304467

Epoch: 6| Step: 8
Training loss: 2.220810913583505
Validation loss: 2.514332844975383

Epoch: 6| Step: 9
Training loss: 3.418221809828397
Validation loss: 2.548096352437097

Epoch: 6| Step: 10
Training loss: 2.8123417279956637
Validation loss: 2.497660211602841

Epoch: 6| Step: 11
Training loss: 2.291100854093226
Validation loss: 2.535567324454623

Epoch: 6| Step: 12
Training loss: 2.2527561261440163
Validation loss: 2.5106875903550256

Epoch: 6| Step: 13
Training loss: 2.614591216015933
Validation loss: 2.4807679618325262

Epoch: 193| Step: 0
Training loss: 2.062233300738795
Validation loss: 2.5401901742220363

Epoch: 6| Step: 1
Training loss: 2.8731706853208885
Validation loss: 2.5260196936442134

Epoch: 6| Step: 2
Training loss: 2.4856405813348914
Validation loss: 2.5234409429908355

Epoch: 6| Step: 3
Training loss: 3.1719297207025523
Validation loss: 2.5192790543779053

Epoch: 6| Step: 4
Training loss: 2.238210307489937
Validation loss: 2.48044622306229

Epoch: 6| Step: 5
Training loss: 2.2771295410754258
Validation loss: 2.5094636307923444

Epoch: 6| Step: 6
Training loss: 2.628047264301173
Validation loss: 2.5243168359817103

Epoch: 6| Step: 7
Training loss: 2.808260625997928
Validation loss: 2.5062734339338637

Epoch: 6| Step: 8
Training loss: 3.3216908903589673
Validation loss: 2.528662612656769

Epoch: 6| Step: 9
Training loss: 2.371659288410533
Validation loss: 2.483667700311316

Epoch: 6| Step: 10
Training loss: 2.8283778967987305
Validation loss: 2.522931455236883

Epoch: 6| Step: 11
Training loss: 2.4941249479489684
Validation loss: 2.5224321255321476

Epoch: 6| Step: 12
Training loss: 2.5883490160065565
Validation loss: 2.510637404249275

Epoch: 6| Step: 13
Training loss: 2.4900693113186425
Validation loss: 2.5123545685032243

Epoch: 194| Step: 0
Training loss: 2.7685829336081147
Validation loss: 2.518510154932852

Epoch: 6| Step: 1
Training loss: 2.366787816918217
Validation loss: 2.5277471882701903

Epoch: 6| Step: 2
Training loss: 2.448816780554507
Validation loss: 2.538975102614044

Epoch: 6| Step: 3
Training loss: 2.8342115125853415
Validation loss: 2.5452548405692763

Epoch: 6| Step: 4
Training loss: 2.79990227392498
Validation loss: 2.558214024334441

Epoch: 6| Step: 5
Training loss: 2.8821635355775683
Validation loss: 2.5057371001617477

Epoch: 6| Step: 6
Training loss: 1.9489065772693743
Validation loss: 2.534638164244829

Epoch: 6| Step: 7
Training loss: 2.4881697170107615
Validation loss: 2.4882650981583687

Epoch: 6| Step: 8
Training loss: 2.9226546445840618
Validation loss: 2.5343268565186032

Epoch: 6| Step: 9
Training loss: 2.7092393288968504
Validation loss: 2.5216014139049046

Epoch: 6| Step: 10
Training loss: 1.81430516291558
Validation loss: 2.4986520845611495

Epoch: 6| Step: 11
Training loss: 2.6977799330616694
Validation loss: 2.510826062820675

Epoch: 6| Step: 12
Training loss: 3.2639386509197403
Validation loss: 2.4766300909429573

Epoch: 6| Step: 13
Training loss: 3.4235696297884415
Validation loss: 2.5150047296264986

Epoch: 195| Step: 0
Training loss: 3.462745487035309
Validation loss: 2.503979778197264

Epoch: 6| Step: 1
Training loss: 2.6445658694188943
Validation loss: 2.545814196609661

Epoch: 6| Step: 2
Training loss: 2.22836691036715
Validation loss: 2.497998319695618

Epoch: 6| Step: 3
Training loss: 1.7892533246348312
Validation loss: 2.5084238585963696

Epoch: 6| Step: 4
Training loss: 2.65003392539711
Validation loss: 2.5433095725633645

Epoch: 6| Step: 5
Training loss: 2.3579292162300423
Validation loss: 2.5555161429922895

Epoch: 6| Step: 6
Training loss: 2.7197453222535235
Validation loss: 2.5299672196168927

Epoch: 6| Step: 7
Training loss: 2.922282797359232
Validation loss: 2.5346466825988316

Epoch: 6| Step: 8
Training loss: 2.7581026178623556
Validation loss: 2.5250546258089774

Epoch: 6| Step: 9
Training loss: 2.83975358108871
Validation loss: 2.54397931399303

Epoch: 6| Step: 10
Training loss: 2.6542678618091027
Validation loss: 2.5268532321565926

Epoch: 6| Step: 11
Training loss: 2.214093762529044
Validation loss: 2.5167959478180313

Epoch: 6| Step: 12
Training loss: 2.90634548891768
Validation loss: 2.542451231762593

Epoch: 6| Step: 13
Training loss: 2.506924385795996
Validation loss: 2.5516948760082667

Epoch: 196| Step: 0
Training loss: 2.947057070725437
Validation loss: 2.507082943720871

Epoch: 6| Step: 1
Training loss: 1.9009126453947727
Validation loss: 2.5448150225761204

Epoch: 6| Step: 2
Training loss: 2.6044579508318746
Validation loss: 2.5263620853128774

Epoch: 6| Step: 3
Training loss: 2.430219487768137
Validation loss: 2.5141115430115804

Epoch: 6| Step: 4
Training loss: 2.899668799593993
Validation loss: 2.5194280156968034

Epoch: 6| Step: 5
Training loss: 2.692997742871154
Validation loss: 2.515893014765295

Epoch: 6| Step: 6
Training loss: 2.6121636753704864
Validation loss: 2.5266187608477098

Epoch: 6| Step: 7
Training loss: 2.999555395877364
Validation loss: 2.513339007357538

Epoch: 6| Step: 8
Training loss: 2.5507632104489883
Validation loss: 2.511712163595885

Epoch: 6| Step: 9
Training loss: 2.5188568395478943
Validation loss: 2.536765456701554

Epoch: 6| Step: 10
Training loss: 2.1533746760073535
Validation loss: 2.509485623465633

Epoch: 6| Step: 11
Training loss: 3.117933752847548
Validation loss: 2.4928055249382535

Epoch: 6| Step: 12
Training loss: 2.26757728386074
Validation loss: 2.521016212839391

Epoch: 6| Step: 13
Training loss: 3.128057128901639
Validation loss: 2.5052721130863227

Epoch: 197| Step: 0
Training loss: 2.2016410429178395
Validation loss: 2.5123515970572194

Epoch: 6| Step: 1
Training loss: 2.3255531391378916
Validation loss: 2.5032255703865505

Epoch: 6| Step: 2
Training loss: 2.8895817704163287
Validation loss: 2.5164207239986522

Epoch: 6| Step: 3
Training loss: 2.1790931465579284
Validation loss: 2.506085280678768

Epoch: 6| Step: 4
Training loss: 2.9117212785280477
Validation loss: 2.489377528199295

Epoch: 6| Step: 5
Training loss: 2.6075792358004737
Validation loss: 2.5304224699071867

Epoch: 6| Step: 6
Training loss: 3.120265884318638
Validation loss: 2.508773405454545

Epoch: 6| Step: 7
Training loss: 2.768164207196338
Validation loss: 2.4774688783582914

Epoch: 6| Step: 8
Training loss: 2.7016578176527246
Validation loss: 2.5088894555405705

Epoch: 6| Step: 9
Training loss: 2.671354633543723
Validation loss: 2.5051086977665227

Epoch: 6| Step: 10
Training loss: 2.307199228012284
Validation loss: 2.5171090886277963

Epoch: 6| Step: 11
Training loss: 2.92448822500174
Validation loss: 2.5038665209818802

Epoch: 6| Step: 12
Training loss: 2.5910130963599998
Validation loss: 2.5269708411088323

Epoch: 6| Step: 13
Training loss: 2.5018403908601186
Validation loss: 2.5320375878976935

Epoch: 198| Step: 0
Training loss: 2.6594287476896397
Validation loss: 2.5107032293368867

Epoch: 6| Step: 1
Training loss: 2.9253287888627524
Validation loss: 2.498501679362018

Epoch: 6| Step: 2
Training loss: 2.7915733853218345
Validation loss: 2.5258040165512594

Epoch: 6| Step: 3
Training loss: 2.9340114575495826
Validation loss: 2.5717511741757284

Epoch: 6| Step: 4
Training loss: 2.587298723583845
Validation loss: 2.5161433622453324

Epoch: 6| Step: 5
Training loss: 2.688420869837248
Validation loss: 2.5184518315409954

Epoch: 6| Step: 6
Training loss: 2.5439716476863845
Validation loss: 2.506018205930755

Epoch: 6| Step: 7
Training loss: 2.3554369669678885
Validation loss: 2.514575382345869

Epoch: 6| Step: 8
Training loss: 2.7353010081011986
Validation loss: 2.5292210886089284

Epoch: 6| Step: 9
Training loss: 2.6844690663883606
Validation loss: 2.5347309067635844

Epoch: 6| Step: 10
Training loss: 3.024424310555369
Validation loss: 2.546346703626322

Epoch: 6| Step: 11
Training loss: 2.1120644837129894
Validation loss: 2.5243568138929855

Epoch: 6| Step: 12
Training loss: 2.7149201526456905
Validation loss: 2.5024548711487014

Epoch: 6| Step: 13
Training loss: 1.436360197562479
Validation loss: 2.5059380326383156

Epoch: 199| Step: 0
Training loss: 2.5041343834986405
Validation loss: 2.482925647532352

Epoch: 6| Step: 1
Training loss: 2.8014705134373075
Validation loss: 2.5178976827749535

Epoch: 6| Step: 2
Training loss: 2.416624320963665
Validation loss: 2.5046393232410495

Epoch: 6| Step: 3
Training loss: 2.66286909383715
Validation loss: 2.546448630684592

Epoch: 6| Step: 4
Training loss: 2.6553087249689518
Validation loss: 2.500680234102687

Epoch: 6| Step: 5
Training loss: 2.712004613686747
Validation loss: 2.472909800029274

Epoch: 6| Step: 6
Training loss: 2.6050757893906344
Validation loss: 2.5125314285172955

Epoch: 6| Step: 7
Training loss: 2.3683080255855096
Validation loss: 2.5056414970443157

Epoch: 6| Step: 8
Training loss: 2.9572992442751596
Validation loss: 2.509064927046594

Epoch: 6| Step: 9
Training loss: 3.07426342089527
Validation loss: 2.524957102099778

Epoch: 6| Step: 10
Training loss: 2.5533174813411788
Validation loss: 2.514081299534455

Epoch: 6| Step: 11
Training loss: 2.4702263775458397
Validation loss: 2.5124992401010884

Epoch: 6| Step: 12
Training loss: 2.7112652583240306
Validation loss: 2.522137400111103

Epoch: 6| Step: 13
Training loss: 2.5815303622522956
Validation loss: 2.533746233168455

Epoch: 200| Step: 0
Training loss: 2.556972403450862
Validation loss: 2.518822743197899

Epoch: 6| Step: 1
Training loss: 2.986534096579877
Validation loss: 2.513861069674242

Epoch: 6| Step: 2
Training loss: 3.069462238641851
Validation loss: 2.517925413421966

Epoch: 6| Step: 3
Training loss: 1.9646082712359147
Validation loss: 2.5525979188555703

Epoch: 6| Step: 4
Training loss: 2.128454877966997
Validation loss: 2.511612689237943

Epoch: 6| Step: 5
Training loss: 2.2905953648668103
Validation loss: 2.516053679092275

Epoch: 6| Step: 6
Training loss: 3.0074356437513594
Validation loss: 2.5258512664746284

Epoch: 6| Step: 7
Training loss: 3.375416482729911
Validation loss: 2.5467711940305766

Epoch: 6| Step: 8
Training loss: 2.314103266674374
Validation loss: 2.533603525595217

Epoch: 6| Step: 9
Training loss: 3.0250096533755286
Validation loss: 2.507059640000529

Epoch: 6| Step: 10
Training loss: 2.8078645441308696
Validation loss: 2.5250659075891067

Epoch: 6| Step: 11
Training loss: 2.465946585566324
Validation loss: 2.515694864574616

Epoch: 6| Step: 12
Training loss: 2.242058407991681
Validation loss: 2.5038842277741606

Epoch: 6| Step: 13
Training loss: 1.971193099570427
Validation loss: 2.5142548231551918

Epoch: 201| Step: 0
Training loss: 2.021112940475608
Validation loss: 2.4940717344595456

Epoch: 6| Step: 1
Training loss: 2.6193606336159148
Validation loss: 2.509094348716917

Epoch: 6| Step: 2
Training loss: 2.4800031089763226
Validation loss: 2.5420127873711174

Epoch: 6| Step: 3
Training loss: 2.6646470130626367
Validation loss: 2.475482210641209

Epoch: 6| Step: 4
Training loss: 2.317333942706632
Validation loss: 2.5137185146443546

Epoch: 6| Step: 5
Training loss: 2.064752562432963
Validation loss: 2.517193686736114

Epoch: 6| Step: 6
Training loss: 2.478210286119603
Validation loss: 2.4901479550285015

Epoch: 6| Step: 7
Training loss: 2.8126539400145365
Validation loss: 2.525346696197465

Epoch: 6| Step: 8
Training loss: 3.0152081611601798
Validation loss: 2.543606387282939

Epoch: 6| Step: 9
Training loss: 2.464811636206211
Validation loss: 2.5264307721860066

Epoch: 6| Step: 10
Training loss: 2.199529649566309
Validation loss: 2.5032177603217223

Epoch: 6| Step: 11
Training loss: 2.703399644250854
Validation loss: 2.5295371323445828

Epoch: 6| Step: 12
Training loss: 2.985344055385965
Validation loss: 2.555157065885649

Epoch: 6| Step: 13
Training loss: 4.213863905886614
Validation loss: 2.5142440077857806

Epoch: 202| Step: 0
Training loss: 1.6271978333937962
Validation loss: 2.555768483270688

Epoch: 6| Step: 1
Training loss: 2.390715553712873
Validation loss: 2.5080874763851426

Epoch: 6| Step: 2
Training loss: 2.6071704071739266
Validation loss: 2.531414743174755

Epoch: 6| Step: 3
Training loss: 2.847684883633048
Validation loss: 2.54011898706138

Epoch: 6| Step: 4
Training loss: 2.4696152534908467
Validation loss: 2.5026664531928455

Epoch: 6| Step: 5
Training loss: 2.8154625442498307
Validation loss: 2.5267447643849623

Epoch: 6| Step: 6
Training loss: 2.1856172361118738
Validation loss: 2.5274441819825153

Epoch: 6| Step: 7
Training loss: 3.4268652354742595
Validation loss: 2.5001976406769795

Epoch: 6| Step: 8
Training loss: 2.7218563705490033
Validation loss: 2.4912039464753413

Epoch: 6| Step: 9
Training loss: 3.1531379389426513
Validation loss: 2.511624225842237

Epoch: 6| Step: 10
Training loss: 2.509032145981855
Validation loss: 2.518557393080777

Epoch: 6| Step: 11
Training loss: 2.173457385698254
Validation loss: 2.506266117195002

Epoch: 6| Step: 12
Training loss: 2.8360138349531896
Validation loss: 2.5372312423150762

Epoch: 6| Step: 13
Training loss: 2.2982406189576743
Validation loss: 2.5551981324537363

Epoch: 203| Step: 0
Training loss: 2.4171610578633107
Validation loss: 2.539205282912162

Epoch: 6| Step: 1
Training loss: 2.035058778855893
Validation loss: 2.5442300925223247

Epoch: 6| Step: 2
Training loss: 2.228715678671234
Validation loss: 2.533387258562368

Epoch: 6| Step: 3
Training loss: 2.3915723531737627
Validation loss: 2.499541302100431

Epoch: 6| Step: 4
Training loss: 2.8606487834759347
Validation loss: 2.535944677915726

Epoch: 6| Step: 5
Training loss: 2.2294651318920664
Validation loss: 2.5270968220187124

Epoch: 6| Step: 6
Training loss: 3.0087930245328507
Validation loss: 2.5047656597189336

Epoch: 6| Step: 7
Training loss: 2.8508920377790137
Validation loss: 2.5182280950169327

Epoch: 6| Step: 8
Training loss: 2.659546097340329
Validation loss: 2.5203196813215842

Epoch: 6| Step: 9
Training loss: 2.761123096594413
Validation loss: 2.5288430714649413

Epoch: 6| Step: 10
Training loss: 2.426792198259908
Validation loss: 2.51568225878742

Epoch: 6| Step: 11
Training loss: 2.7564426726730877
Validation loss: 2.5007754148786487

Epoch: 6| Step: 12
Training loss: 3.5015511481506643
Validation loss: 2.5280158152335432

Epoch: 6| Step: 13
Training loss: 1.5126816002142385
Validation loss: 2.555488912551054

Epoch: 204| Step: 0
Training loss: 2.9035287237571143
Validation loss: 2.4969868529383663

Epoch: 6| Step: 1
Training loss: 2.7052425844073062
Validation loss: 2.523000405196484

Epoch: 6| Step: 2
Training loss: 2.9173444414549152
Validation loss: 2.5177946075347952

Epoch: 6| Step: 3
Training loss: 3.064792320098057
Validation loss: 2.49078169532667

Epoch: 6| Step: 4
Training loss: 2.568363830794614
Validation loss: 2.524191334842153

Epoch: 6| Step: 5
Training loss: 2.641406964874848
Validation loss: 2.5288240613609014

Epoch: 6| Step: 6
Training loss: 2.213663637877299
Validation loss: 2.532753272537355

Epoch: 6| Step: 7
Training loss: 1.695864960971489
Validation loss: 2.506012518079775

Epoch: 6| Step: 8
Training loss: 2.325744538406673
Validation loss: 2.5219809637621906

Epoch: 6| Step: 9
Training loss: 1.8310266924943255
Validation loss: 2.513617742310472

Epoch: 6| Step: 10
Training loss: 2.9818809433419897
Validation loss: 2.5528486445798824

Epoch: 6| Step: 11
Training loss: 3.433489870620036
Validation loss: 2.5125659546031445

Epoch: 6| Step: 12
Training loss: 2.478544579254882
Validation loss: 2.5612938576203774

Epoch: 6| Step: 13
Training loss: 2.5770072594785747
Validation loss: 2.5367138865623504

Epoch: 205| Step: 0
Training loss: 2.4073816586065684
Validation loss: 2.5224480118428954

Epoch: 6| Step: 1
Training loss: 2.7937157545615046
Validation loss: 2.5299423478050316

Epoch: 6| Step: 2
Training loss: 2.7786859415884337
Validation loss: 2.5823337977809846

Epoch: 6| Step: 3
Training loss: 2.7966692118885095
Validation loss: 2.514962520512142

Epoch: 6| Step: 4
Training loss: 1.903093695422248
Validation loss: 2.5031403240436196

Epoch: 6| Step: 5
Training loss: 2.3306342817772823
Validation loss: 2.5532824470271738

Epoch: 6| Step: 6
Training loss: 2.812032194438932
Validation loss: 2.482265902383071

Epoch: 6| Step: 7
Training loss: 3.9922027886204314
Validation loss: 2.540583943921468

Epoch: 6| Step: 8
Training loss: 2.3175382630318007
Validation loss: 2.505908738107595

Epoch: 6| Step: 9
Training loss: 2.4332377018709357
Validation loss: 2.5026243699300306

Epoch: 6| Step: 10
Training loss: 2.7791927315453866
Validation loss: 2.5151623837007584

Epoch: 6| Step: 11
Training loss: 2.127015224142251
Validation loss: 2.5216555185555967

Epoch: 6| Step: 12
Training loss: 2.3277120544008714
Validation loss: 2.5053259487418633

Epoch: 6| Step: 13
Training loss: 2.3695730143412312
Validation loss: 2.5365458056439834

Epoch: 206| Step: 0
Training loss: 2.8894736806631616
Validation loss: 2.5237675638292703

Epoch: 6| Step: 1
Training loss: 2.4569428493201544
Validation loss: 2.5098551446970463

Epoch: 6| Step: 2
Training loss: 2.8677582133802977
Validation loss: 2.4991622854366438

Epoch: 6| Step: 3
Training loss: 2.490609844549345
Validation loss: 2.5324676119137557

Epoch: 6| Step: 4
Training loss: 2.2238015708497176
Validation loss: 2.5236777838827673

Epoch: 6| Step: 5
Training loss: 2.419568249201234
Validation loss: 2.503978987803148

Epoch: 6| Step: 6
Training loss: 2.164349588569399
Validation loss: 2.5087944272790272

Epoch: 6| Step: 7
Training loss: 2.413393983588161
Validation loss: 2.487719668548185

Epoch: 6| Step: 8
Training loss: 2.404195527730447
Validation loss: 2.5395287173261867

Epoch: 6| Step: 9
Training loss: 2.030178080673123
Validation loss: 2.5181597229569155

Epoch: 6| Step: 10
Training loss: 3.3965846129724273
Validation loss: 2.528671064965175

Epoch: 6| Step: 11
Training loss: 3.0043138641682554
Validation loss: 2.5412019425459422

Epoch: 6| Step: 12
Training loss: 2.4226941107897972
Validation loss: 2.5553079708170854

Epoch: 6| Step: 13
Training loss: 3.236433617962303
Validation loss: 2.507876062403882

Epoch: 207| Step: 0
Training loss: 2.930632171914959
Validation loss: 2.4998071473093155

Epoch: 6| Step: 1
Training loss: 2.777932749769809
Validation loss: 2.5467372288053047

Epoch: 6| Step: 2
Training loss: 2.1836246495116405
Validation loss: 2.508766945183191

Epoch: 6| Step: 3
Training loss: 3.5723059366745447
Validation loss: 2.5108930336381716

Epoch: 6| Step: 4
Training loss: 3.2535203794219525
Validation loss: 2.5592383168896014

Epoch: 6| Step: 5
Training loss: 2.740078628692698
Validation loss: 2.5530387257052265

Epoch: 6| Step: 6
Training loss: 2.100508074379203
Validation loss: 2.5194067244183933

Epoch: 6| Step: 7
Training loss: 2.192058826457703
Validation loss: 2.538465115150039

Epoch: 6| Step: 8
Training loss: 2.061421892853793
Validation loss: 2.5256240846324625

Epoch: 6| Step: 9
Training loss: 2.489815471046901
Validation loss: 2.5150743515071334

Epoch: 6| Step: 10
Training loss: 2.453989739077132
Validation loss: 2.495835455434544

Epoch: 6| Step: 11
Training loss: 2.259046910103267
Validation loss: 2.5365747261383125

Epoch: 6| Step: 12
Training loss: 2.406744571738711
Validation loss: 2.539822085771177

Epoch: 6| Step: 13
Training loss: 2.4714025419677346
Validation loss: 2.5364026718836126

Epoch: 208| Step: 0
Training loss: 3.548255147745982
Validation loss: 2.534967692014515

Epoch: 6| Step: 1
Training loss: 2.4838062331078663
Validation loss: 2.526656059206939

Epoch: 6| Step: 2
Training loss: 3.0491049406885056
Validation loss: 2.534949560155288

Epoch: 6| Step: 3
Training loss: 2.243599583251392
Validation loss: 2.496729803863247

Epoch: 6| Step: 4
Training loss: 2.68181470068425
Validation loss: 2.528710386026088

Epoch: 6| Step: 5
Training loss: 2.177502812275346
Validation loss: 2.5065043221783014

Epoch: 6| Step: 6
Training loss: 2.085959445109282
Validation loss: 2.52121893844527

Epoch: 6| Step: 7
Training loss: 2.488343913248936
Validation loss: 2.5168138814515704

Epoch: 6| Step: 8
Training loss: 2.543846529691989
Validation loss: 2.5578264626644525

Epoch: 6| Step: 9
Training loss: 2.2460729448764694
Validation loss: 2.5217402071136537

Epoch: 6| Step: 10
Training loss: 2.425788228244542
Validation loss: 2.5021309487093455

Epoch: 6| Step: 11
Training loss: 2.6855209515794267
Validation loss: 2.491381148772725

Epoch: 6| Step: 12
Training loss: 2.7871756702817456
Validation loss: 2.5361283081645962

Epoch: 6| Step: 13
Training loss: 2.904257470731345
Validation loss: 2.5358155036439567

Epoch: 209| Step: 0
Training loss: 2.110079160353413
Validation loss: 2.4760395715702144

Epoch: 6| Step: 1
Training loss: 2.5695814583020344
Validation loss: 2.5326674698296934

Epoch: 6| Step: 2
Training loss: 2.2933386743098914
Validation loss: 2.543621577949417

Epoch: 6| Step: 3
Training loss: 2.6909016217777153
Validation loss: 2.522598678327292

Epoch: 6| Step: 4
Training loss: 3.2500491505354185
Validation loss: 2.468092085521731

Epoch: 6| Step: 5
Training loss: 2.849658905498261
Validation loss: 2.5466696900980796

Epoch: 6| Step: 6
Training loss: 2.8733056092602722
Validation loss: 2.5422120950591416

Epoch: 6| Step: 7
Training loss: 2.426397027381311
Validation loss: 2.521086503565945

Epoch: 6| Step: 8
Training loss: 3.218671779237662
Validation loss: 2.509941429510832

Epoch: 6| Step: 9
Training loss: 2.3346480571165635
Validation loss: 2.509165754515432

Epoch: 6| Step: 10
Training loss: 2.298874173152863
Validation loss: 2.5374459056866265

Epoch: 6| Step: 11
Training loss: 2.3004388473464927
Validation loss: 2.503635974161508

Epoch: 6| Step: 12
Training loss: 2.4970768050854195
Validation loss: 2.4939701631306286

Epoch: 6| Step: 13
Training loss: 2.6015727641501725
Validation loss: 2.499633659675828

Epoch: 210| Step: 0
Training loss: 2.194681423069665
Validation loss: 2.493451799476827

Epoch: 6| Step: 1
Training loss: 2.2434438980123956
Validation loss: 2.541889378778941

Epoch: 6| Step: 2
Training loss: 3.114201465957019
Validation loss: 2.5427362577594437

Epoch: 6| Step: 3
Training loss: 2.569106076846212
Validation loss: 2.5398495265742977

Epoch: 6| Step: 4
Training loss: 2.458984568916295
Validation loss: 2.52758728548985

Epoch: 6| Step: 5
Training loss: 2.5032528695772287
Validation loss: 2.5178436378841353

Epoch: 6| Step: 6
Training loss: 2.2242721824538885
Validation loss: 2.4964174360562166

Epoch: 6| Step: 7
Training loss: 3.0259713686176624
Validation loss: 2.49440542913432

Epoch: 6| Step: 8
Training loss: 2.4126183594442856
Validation loss: 2.5323425167457545

Epoch: 6| Step: 9
Training loss: 3.2760215156230577
Validation loss: 2.515861106273374

Epoch: 6| Step: 10
Training loss: 2.6744839607252757
Validation loss: 2.5272337116789076

Epoch: 6| Step: 11
Training loss: 2.381134543035343
Validation loss: 2.5467770998858823

Epoch: 6| Step: 12
Training loss: 3.01383753164886
Validation loss: 2.558077563848545

Epoch: 6| Step: 13
Training loss: 2.362470402607881
Validation loss: 2.513998469966331

Epoch: 211| Step: 0
Training loss: 2.679570587543902
Validation loss: 2.539465104234034

Epoch: 6| Step: 1
Training loss: 2.5258763105142035
Validation loss: 2.5216772107195586

Epoch: 6| Step: 2
Training loss: 2.439664319977472
Validation loss: 2.5124687481436827

Epoch: 6| Step: 3
Training loss: 1.9514142898710387
Validation loss: 2.506507863092846

Epoch: 6| Step: 4
Training loss: 2.1682496035001626
Validation loss: 2.491903014097115

Epoch: 6| Step: 5
Training loss: 2.5445076641222952
Validation loss: 2.537615768682832

Epoch: 6| Step: 6
Training loss: 3.1268735228585918
Validation loss: 2.540718833533508

Epoch: 6| Step: 7
Training loss: 2.84120248562665
Validation loss: 2.5066541520856647

Epoch: 6| Step: 8
Training loss: 2.1736890503299717
Validation loss: 2.5332292363813975

Epoch: 6| Step: 9
Training loss: 3.242876315195246
Validation loss: 2.5482539642497244

Epoch: 6| Step: 10
Training loss: 2.2251732490896874
Validation loss: 2.537337643197789

Epoch: 6| Step: 11
Training loss: 2.8873282113104324
Validation loss: 2.530411604091231

Epoch: 6| Step: 12
Training loss: 2.906843227425264
Validation loss: 2.5308958246625406

Epoch: 6| Step: 13
Training loss: 2.7313593338441198
Validation loss: 2.535046277749265

Epoch: 212| Step: 0
Training loss: 2.127339813979672
Validation loss: 2.5445730351662315

Epoch: 6| Step: 1
Training loss: 2.1325134514834967
Validation loss: 2.4983258076453554

Epoch: 6| Step: 2
Training loss: 2.7230310168452307
Validation loss: 2.539737024059449

Epoch: 6| Step: 3
Training loss: 2.958603394409287
Validation loss: 2.511330703453478

Epoch: 6| Step: 4
Training loss: 2.921731710745392
Validation loss: 2.5154862607326263

Epoch: 6| Step: 5
Training loss: 2.2738828452268907
Validation loss: 2.5124175894738516

Epoch: 6| Step: 6
Training loss: 2.3428996260044386
Validation loss: 2.530860425269577

Epoch: 6| Step: 7
Training loss: 2.888265440580042
Validation loss: 2.530034177230643

Epoch: 6| Step: 8
Training loss: 2.944838589453101
Validation loss: 2.5253365577358986

Epoch: 6| Step: 9
Training loss: 1.788259067622804
Validation loss: 2.5372449767639744

Epoch: 6| Step: 10
Training loss: 2.1716495094978034
Validation loss: 2.5225669014057526

Epoch: 6| Step: 11
Training loss: 3.162080723407101
Validation loss: 2.533930732460512

Epoch: 6| Step: 12
Training loss: 3.2528046463977747
Validation loss: 2.493311217692914

Epoch: 6| Step: 13
Training loss: 2.0172954409297845
Validation loss: 2.5147368832071515

Epoch: 213| Step: 0
Training loss: 1.9643972811040624
Validation loss: 2.5376723150915965

Epoch: 6| Step: 1
Training loss: 2.511462636084545
Validation loss: 2.543799306643545

Epoch: 6| Step: 2
Training loss: 2.7563500349441683
Validation loss: 2.4906062789738144

Epoch: 6| Step: 3
Training loss: 2.3097993700829083
Validation loss: 2.507336675869769

Epoch: 6| Step: 4
Training loss: 2.9159379502904117
Validation loss: 2.5224830861780063

Epoch: 6| Step: 5
Training loss: 2.2587161252342693
Validation loss: 2.524079639643273

Epoch: 6| Step: 6
Training loss: 2.2058881381837865
Validation loss: 2.5187873985546956

Epoch: 6| Step: 7
Training loss: 2.9099692879855352
Validation loss: 2.5201272717220538

Epoch: 6| Step: 8
Training loss: 3.0828593293551823
Validation loss: 2.5538862871815695

Epoch: 6| Step: 9
Training loss: 2.2983545222238146
Validation loss: 2.49836874624856

Epoch: 6| Step: 10
Training loss: 1.791131301854953
Validation loss: 2.5173197210744926

Epoch: 6| Step: 11
Training loss: 2.4069860744477625
Validation loss: 2.5426039686374238

Epoch: 6| Step: 12
Training loss: 3.5963327166419212
Validation loss: 2.520228231349063

Epoch: 6| Step: 13
Training loss: 3.712181509208839
Validation loss: 2.5471487083801896

Epoch: 214| Step: 0
Training loss: 3.031179840219792
Validation loss: 2.5235900040396437

Epoch: 6| Step: 1
Training loss: 2.33645511875847
Validation loss: 2.515936720407149

Epoch: 6| Step: 2
Training loss: 2.5059191249940094
Validation loss: 2.5292076643389683

Epoch: 6| Step: 3
Training loss: 2.0914611831991663
Validation loss: 2.5173587316365795

Epoch: 6| Step: 4
Training loss: 2.3216902113809392
Validation loss: 2.5077194131677243

Epoch: 6| Step: 5
Training loss: 1.8990368309125427
Validation loss: 2.520239595767669

Epoch: 6| Step: 6
Training loss: 3.0704830209684704
Validation loss: 2.5185398708819444

Epoch: 6| Step: 7
Training loss: 2.6258282036117313
Validation loss: 2.5101795175577637

Epoch: 6| Step: 8
Training loss: 2.605533902155795
Validation loss: 2.503409505423416

Epoch: 6| Step: 9
Training loss: 3.346445156102262
Validation loss: 2.5530027085034477

Epoch: 6| Step: 10
Training loss: 2.5907643610854327
Validation loss: 2.5636935712038107

Epoch: 6| Step: 11
Training loss: 2.6793952982769658
Validation loss: 2.54499151554374

Epoch: 6| Step: 12
Training loss: 2.673037359813594
Validation loss: 2.5334424877236423

Epoch: 6| Step: 13
Training loss: 2.4258469036757537
Validation loss: 2.5499060555786284

Epoch: 215| Step: 0
Training loss: 2.405445013768537
Validation loss: 2.508430763814582

Epoch: 6| Step: 1
Training loss: 2.2408532615599244
Validation loss: 2.488530583136491

Epoch: 6| Step: 2
Training loss: 3.034119967930959
Validation loss: 2.528749571640477

Epoch: 6| Step: 3
Training loss: 2.2574193275544223
Validation loss: 2.502813930585046

Epoch: 6| Step: 4
Training loss: 2.138017165769033
Validation loss: 2.5233879433715956

Epoch: 6| Step: 5
Training loss: 2.0723328143560895
Validation loss: 2.5422665577141736

Epoch: 6| Step: 6
Training loss: 3.085183353261551
Validation loss: 2.5395973166081185

Epoch: 6| Step: 7
Training loss: 2.0460873318601016
Validation loss: 2.5639574166784986

Epoch: 6| Step: 8
Training loss: 2.6681592658231352
Validation loss: 2.4941874715774435

Epoch: 6| Step: 9
Training loss: 2.6568732035470872
Validation loss: 2.50445738657507

Epoch: 6| Step: 10
Training loss: 3.264159243233324
Validation loss: 2.489847283966214

Epoch: 6| Step: 11
Training loss: 2.7315199415249536
Validation loss: 2.5112965646390464

Epoch: 6| Step: 12
Training loss: 2.6576751476142793
Validation loss: 2.5223734974615692

Epoch: 6| Step: 13
Training loss: 3.1446155607874364
Validation loss: 2.5298170244032683

Epoch: 216| Step: 0
Training loss: 2.965919028238612
Validation loss: 2.517745791076875

Epoch: 6| Step: 1
Training loss: 2.070274842117712
Validation loss: 2.5009284766681943

Epoch: 6| Step: 2
Training loss: 3.0135000381381114
Validation loss: 2.51556234065786

Epoch: 6| Step: 3
Training loss: 2.6235027584572523
Validation loss: 2.5076238897152967

Epoch: 6| Step: 4
Training loss: 2.647280820251598
Validation loss: 2.534183622719897

Epoch: 6| Step: 5
Training loss: 3.1873447155994237
Validation loss: 2.5190430404981456

Epoch: 6| Step: 6
Training loss: 2.4501594063418803
Validation loss: 2.525122323855711

Epoch: 6| Step: 7
Training loss: 2.5334369960288927
Validation loss: 2.518356531941847

Epoch: 6| Step: 8
Training loss: 2.1392280865619377
Validation loss: 2.5721335260240634

Epoch: 6| Step: 9
Training loss: 2.433773615819993
Validation loss: 2.5150006522700092

Epoch: 6| Step: 10
Training loss: 3.4703445206375325
Validation loss: 2.5385206135559875

Epoch: 6| Step: 11
Training loss: 2.6591507275839965
Validation loss: 2.5526906707002888

Epoch: 6| Step: 12
Training loss: 2.062426825872583
Validation loss: 2.5466494057381825

Epoch: 6| Step: 13
Training loss: 1.6861078383099577
Validation loss: 2.550236558930078

Epoch: 217| Step: 0
Training loss: 2.3350288384431326
Validation loss: 2.5336493344087407

Epoch: 6| Step: 1
Training loss: 2.2435517629964834
Validation loss: 2.5425905575539667

Epoch: 6| Step: 2
Training loss: 2.4625310673182024
Validation loss: 2.5146271342987916

Epoch: 6| Step: 3
Training loss: 2.2676230203552503
Validation loss: 2.517585835528215

Epoch: 6| Step: 4
Training loss: 2.613193664451544
Validation loss: 2.542110228570343

Epoch: 6| Step: 5
Training loss: 3.1365591272235465
Validation loss: 2.532854392865247

Epoch: 6| Step: 6
Training loss: 2.69997915330539
Validation loss: 2.5070083510105254

Epoch: 6| Step: 7
Training loss: 2.9995549189694897
Validation loss: 2.516174955385435

Epoch: 6| Step: 8
Training loss: 2.1526685078552883
Validation loss: 2.505718888766332

Epoch: 6| Step: 9
Training loss: 3.081149711292198
Validation loss: 2.545055241322803

Epoch: 6| Step: 10
Training loss: 2.3615379271844783
Validation loss: 2.5324629623726547

Epoch: 6| Step: 11
Training loss: 2.3603368150268498
Validation loss: 2.5193949604112973

Epoch: 6| Step: 12
Training loss: 3.0563796251887694
Validation loss: 2.5503353530748067

Epoch: 6| Step: 13
Training loss: 1.7037412595909533
Validation loss: 2.5386883536823386

Epoch: 218| Step: 0
Training loss: 2.2798445042908337
Validation loss: 2.5473941193653395

Epoch: 6| Step: 1
Training loss: 3.3277622078892386
Validation loss: 2.523864864326614

Epoch: 6| Step: 2
Training loss: 2.1978871733749177
Validation loss: 2.5190797416966815

Epoch: 6| Step: 3
Training loss: 1.8987818276184452
Validation loss: 2.5278688304930026

Epoch: 6| Step: 4
Training loss: 2.5809628675613427
Validation loss: 2.4829586503992562

Epoch: 6| Step: 5
Training loss: 3.1181920471751585
Validation loss: 2.528219021978138

Epoch: 6| Step: 6
Training loss: 2.2308530994100577
Validation loss: 2.486777275836202

Epoch: 6| Step: 7
Training loss: 2.979264762348241
Validation loss: 2.5293275831978415

Epoch: 6| Step: 8
Training loss: 2.4470374510174806
Validation loss: 2.5004461069884103

Epoch: 6| Step: 9
Training loss: 1.83093808222467
Validation loss: 2.4769118820086318

Epoch: 6| Step: 10
Training loss: 2.2140845018369135
Validation loss: 2.491820883545834

Epoch: 6| Step: 11
Training loss: 2.5889937212688507
Validation loss: 2.531507328331907

Epoch: 6| Step: 12
Training loss: 2.948381438983296
Validation loss: 2.5056074740921366

Epoch: 6| Step: 13
Training loss: 3.339148519333556
Validation loss: 2.5200848208403226

Epoch: 219| Step: 0
Training loss: 2.911366051227623
Validation loss: 2.510412376936955

Epoch: 6| Step: 1
Training loss: 2.3400664820559895
Validation loss: 2.4986263233832355

Epoch: 6| Step: 2
Training loss: 2.4595350356424555
Validation loss: 2.508433939707792

Epoch: 6| Step: 3
Training loss: 2.070939800306648
Validation loss: 2.5274031787230085

Epoch: 6| Step: 4
Training loss: 2.1488597108218146
Validation loss: 2.50501914251543

Epoch: 6| Step: 5
Training loss: 3.019875806566383
Validation loss: 2.5048695597161803

Epoch: 6| Step: 6
Training loss: 3.1506954454788443
Validation loss: 2.5424189709121787

Epoch: 6| Step: 7
Training loss: 2.6167786236115265
Validation loss: 2.5256430223929582

Epoch: 6| Step: 8
Training loss: 2.6694544485563863
Validation loss: 2.493977041038028

Epoch: 6| Step: 9
Training loss: 2.4653637030824807
Validation loss: 2.532489754059313

Epoch: 6| Step: 10
Training loss: 2.5786309872788484
Validation loss: 2.517024668972719

Epoch: 6| Step: 11
Training loss: 2.9087979212519537
Validation loss: 2.4970723719778483

Epoch: 6| Step: 12
Training loss: 2.724760580481754
Validation loss: 2.5340687542729663

Epoch: 6| Step: 13
Training loss: 1.868614896145387
Validation loss: 2.518268568650711

Epoch: 220| Step: 0
Training loss: 2.0263685993970983
Validation loss: 2.5491351606412374

Epoch: 6| Step: 1
Training loss: 2.379437667546476
Validation loss: 2.532242894576275

Epoch: 6| Step: 2
Training loss: 3.1605165556727655
Validation loss: 2.508593874840593

Epoch: 6| Step: 3
Training loss: 2.823408715038572
Validation loss: 2.503042494460187

Epoch: 6| Step: 4
Training loss: 2.2504814480565876
Validation loss: 2.5271860442989595

Epoch: 6| Step: 5
Training loss: 2.8491647332910044
Validation loss: 2.517717465835855

Epoch: 6| Step: 6
Training loss: 2.532236166162371
Validation loss: 2.525319355657074

Epoch: 6| Step: 7
Training loss: 2.6284937542047833
Validation loss: 2.5187087177547625

Epoch: 6| Step: 8
Training loss: 2.9918765710779147
Validation loss: 2.511399988242129

Epoch: 6| Step: 9
Training loss: 2.2943613219990024
Validation loss: 2.5423836351614133

Epoch: 6| Step: 10
Training loss: 2.1852019091443307
Validation loss: 2.575159142268989

Epoch: 6| Step: 11
Training loss: 3.275060137713451
Validation loss: 2.540888672456763

Epoch: 6| Step: 12
Training loss: 2.572076163414543
Validation loss: 2.5379930115177394

Epoch: 6| Step: 13
Training loss: 1.639193027997398
Validation loss: 2.5468313521814765

Epoch: 221| Step: 0
Training loss: 2.8666986781558212
Validation loss: 2.5584764754462417

Epoch: 6| Step: 1
Training loss: 2.4475025448966115
Validation loss: 2.4994756456350276

Epoch: 6| Step: 2
Training loss: 2.4959019451443583
Validation loss: 2.5179854655776124

Epoch: 6| Step: 3
Training loss: 2.9472373116702477
Validation loss: 2.5096727434070423

Epoch: 6| Step: 4
Training loss: 2.7850816029179666
Validation loss: 2.5095491879298923

Epoch: 6| Step: 5
Training loss: 2.062138612470912
Validation loss: 2.5185695803730535

Epoch: 6| Step: 6
Training loss: 2.5621000768693722
Validation loss: 2.5180300430581193

Epoch: 6| Step: 7
Training loss: 2.8125790902779255
Validation loss: 2.5316730967550067

Epoch: 6| Step: 8
Training loss: 1.882357530509389
Validation loss: 2.560502978650777

Epoch: 6| Step: 9
Training loss: 2.494690979987767
Validation loss: 2.5152827428580364

Epoch: 6| Step: 10
Training loss: 2.1677245956267948
Validation loss: 2.521379306104077

Epoch: 6| Step: 11
Training loss: 3.5090266040349007
Validation loss: 2.5067648467990753

Epoch: 6| Step: 12
Training loss: 2.7829635142197375
Validation loss: 2.5441176245064567

Epoch: 6| Step: 13
Training loss: 2.3223657225523047
Validation loss: 2.545895648194525

Epoch: 222| Step: 0
Training loss: 2.12467202292179
Validation loss: 2.516557534267975

Epoch: 6| Step: 1
Training loss: 2.9859230061113924
Validation loss: 2.510268560884668

Epoch: 6| Step: 2
Training loss: 2.489903183399189
Validation loss: 2.5153828349386393

Epoch: 6| Step: 3
Training loss: 2.670315828538922
Validation loss: 2.4664302263070876

Epoch: 6| Step: 4
Training loss: 2.2573017743888784
Validation loss: 2.524159802427451

Epoch: 6| Step: 5
Training loss: 2.7426027162941096
Validation loss: 2.5220094454368724

Epoch: 6| Step: 6
Training loss: 2.5746979101067575
Validation loss: 2.5373435139248164

Epoch: 6| Step: 7
Training loss: 2.939300310085074
Validation loss: 2.4928406348070618

Epoch: 6| Step: 8
Training loss: 1.9672461927413214
Validation loss: 2.5159615054850732

Epoch: 6| Step: 9
Training loss: 2.8900383431290075
Validation loss: 2.5435977326342885

Epoch: 6| Step: 10
Training loss: 2.663940963881409
Validation loss: 2.5405648753529957

Epoch: 6| Step: 11
Training loss: 2.314748959204999
Validation loss: 2.51466140732575

Epoch: 6| Step: 12
Training loss: 2.8492284968643267
Validation loss: 2.5311837977040943

Epoch: 6| Step: 13
Training loss: 2.572378702075114
Validation loss: 2.5063868028097085

Epoch: 223| Step: 0
Training loss: 2.649420811679341
Validation loss: 2.5237390664424786

Epoch: 6| Step: 1
Training loss: 2.5769878307050753
Validation loss: 2.5347704989318416

Epoch: 6| Step: 2
Training loss: 2.559225828413802
Validation loss: 2.526136357002455

Epoch: 6| Step: 3
Training loss: 2.7385896299584425
Validation loss: 2.506900005302535

Epoch: 6| Step: 4
Training loss: 3.2596196347064588
Validation loss: 2.535414577235913

Epoch: 6| Step: 5
Training loss: 2.1264161271866504
Validation loss: 2.4782267838336978

Epoch: 6| Step: 6
Training loss: 2.418774299598092
Validation loss: 2.541868461722453

Epoch: 6| Step: 7
Training loss: 2.341806037891269
Validation loss: 2.518316449614981

Epoch: 6| Step: 8
Training loss: 3.142870958718983
Validation loss: 2.534800652323372

Epoch: 6| Step: 9
Training loss: 2.0694633613876756
Validation loss: 2.536394331253183

Epoch: 6| Step: 10
Training loss: 2.901859700024784
Validation loss: 2.5135504962944255

Epoch: 6| Step: 11
Training loss: 2.4912203164636266
Validation loss: 2.512801177082261

Epoch: 6| Step: 12
Training loss: 2.433841209015223
Validation loss: 2.4856828201232815

Epoch: 6| Step: 13
Training loss: 2.3874254944045914
Validation loss: 2.5044915592447867

Epoch: 224| Step: 0
Training loss: 2.2854712114342366
Validation loss: 2.5095266457018175

Epoch: 6| Step: 1
Training loss: 3.121653787070726
Validation loss: 2.4977068146007

Epoch: 6| Step: 2
Training loss: 2.6935058730949923
Validation loss: 2.5569777974754717

Epoch: 6| Step: 3
Training loss: 2.2055490563609434
Validation loss: 2.533370403602541

Epoch: 6| Step: 4
Training loss: 2.008581585663647
Validation loss: 2.5101206370106377

Epoch: 6| Step: 5
Training loss: 3.161417592690585
Validation loss: 2.5182476696895577

Epoch: 6| Step: 6
Training loss: 2.6625658233099614
Validation loss: 2.5415986470750545

Epoch: 6| Step: 7
Training loss: 2.7869302415388177
Validation loss: 2.545772333671245

Epoch: 6| Step: 8
Training loss: 1.979480624812061
Validation loss: 2.520557579335597

Epoch: 6| Step: 9
Training loss: 2.730668613175478
Validation loss: 2.522836150580717

Epoch: 6| Step: 10
Training loss: 2.9451395889645586
Validation loss: 2.539591221436797

Epoch: 6| Step: 11
Training loss: 2.4153744980984735
Validation loss: 2.524181240998217

Epoch: 6| Step: 12
Training loss: 2.765504392591101
Validation loss: 2.5376767570827634

Epoch: 6| Step: 13
Training loss: 1.8233101846921305
Validation loss: 2.5346851614940142

Epoch: 225| Step: 0
Training loss: 2.6617020248456407
Validation loss: 2.523047276802274

Epoch: 6| Step: 1
Training loss: 2.828267467977943
Validation loss: 2.5390182094954845

Epoch: 6| Step: 2
Training loss: 2.5635750539115008
Validation loss: 2.54705537124229

Epoch: 6| Step: 3
Training loss: 3.235502626972432
Validation loss: 2.515261650914438

Epoch: 6| Step: 4
Training loss: 3.380365416056278
Validation loss: 2.5248476705186165

Epoch: 6| Step: 5
Training loss: 2.5064259912053353
Validation loss: 2.532633227393786

Epoch: 6| Step: 6
Training loss: 1.6977757432491591
Validation loss: 2.520222428070422

Epoch: 6| Step: 7
Training loss: 2.201970873523135
Validation loss: 2.5298395535444187

Epoch: 6| Step: 8
Training loss: 3.1607941501185786
Validation loss: 2.5244230459409684

Epoch: 6| Step: 9
Training loss: 1.9946080481046808
Validation loss: 2.525536687113876

Epoch: 6| Step: 10
Training loss: 2.5452336364106176
Validation loss: 2.5496711835038277

Epoch: 6| Step: 11
Training loss: 2.113481156065958
Validation loss: 2.497753733880007

Epoch: 6| Step: 12
Training loss: 2.518315363411081
Validation loss: 2.5187779319096055

Epoch: 6| Step: 13
Training loss: 2.3129157646267693
Validation loss: 2.53579509656931

Epoch: 226| Step: 0
Training loss: 2.770173206904659
Validation loss: 2.507826899536775

Epoch: 6| Step: 1
Training loss: 2.3638200130008893
Validation loss: 2.5379656486230333

Epoch: 6| Step: 2
Training loss: 2.772804897318993
Validation loss: 2.542612185046287

Epoch: 6| Step: 3
Training loss: 2.676653598675908
Validation loss: 2.5498018954296984

Epoch: 6| Step: 4
Training loss: 2.0915980881268417
Validation loss: 2.519709317095557

Epoch: 6| Step: 5
Training loss: 2.6237403935605963
Validation loss: 2.508631666018179

Epoch: 6| Step: 6
Training loss: 2.7679738560693274
Validation loss: 2.492133932418429

Epoch: 6| Step: 7
Training loss: 2.1791056194713847
Validation loss: 2.4957723875909434

Epoch: 6| Step: 8
Training loss: 3.292200906270906
Validation loss: 2.5174133623127353

Epoch: 6| Step: 9
Training loss: 2.5155515482349777
Validation loss: 2.498980098770245

Epoch: 6| Step: 10
Training loss: 2.557553984220499
Validation loss: 2.495209916369198

Epoch: 6| Step: 11
Training loss: 2.2711436441699617
Validation loss: 2.5170189448823153

Epoch: 6| Step: 12
Training loss: 2.5258299643824342
Validation loss: 2.537672918200701

Epoch: 6| Step: 13
Training loss: 2.678558200621867
Validation loss: 2.4951370318418142

Epoch: 227| Step: 0
Training loss: 2.9080503076454636
Validation loss: 2.5067209807307385

Epoch: 6| Step: 1
Training loss: 1.9010723651515018
Validation loss: 2.514957819240626

Epoch: 6| Step: 2
Training loss: 2.944067896951487
Validation loss: 2.5209018630500606

Epoch: 6| Step: 3
Training loss: 1.7619267444515736
Validation loss: 2.5213614018926314

Epoch: 6| Step: 4
Training loss: 2.2949929344051863
Validation loss: 2.5362169873124896

Epoch: 6| Step: 5
Training loss: 2.682685265165035
Validation loss: 2.532686102375014

Epoch: 6| Step: 6
Training loss: 2.3549605749762317
Validation loss: 2.4733837976501927

Epoch: 6| Step: 7
Training loss: 2.992617424339756
Validation loss: 2.5265303586963213

Epoch: 6| Step: 8
Training loss: 2.6960886846609182
Validation loss: 2.5096663344830397

Epoch: 6| Step: 9
Training loss: 2.567192619072517
Validation loss: 2.541492731230693

Epoch: 6| Step: 10
Training loss: 3.017455499126538
Validation loss: 2.5138063281445646

Epoch: 6| Step: 11
Training loss: 2.4230499370874776
Validation loss: 2.498632167842951

Epoch: 6| Step: 12
Training loss: 2.3182867692795686
Validation loss: 2.5324039520403203

Epoch: 6| Step: 13
Training loss: 2.9419343633843895
Validation loss: 2.5106705329710413

Epoch: 228| Step: 0
Training loss: 2.298687589460919
Validation loss: 2.5073682905114283

Epoch: 6| Step: 1
Training loss: 1.5824990416850824
Validation loss: 2.5207876616404916

Epoch: 6| Step: 2
Training loss: 3.0999354940286534
Validation loss: 2.5066063173334534

Epoch: 6| Step: 3
Training loss: 3.7250679265939914
Validation loss: 2.5192260638754926

Epoch: 6| Step: 4
Training loss: 3.0081739332355197
Validation loss: 2.5414695690222033

Epoch: 6| Step: 5
Training loss: 2.155685129673603
Validation loss: 2.531667020985473

Epoch: 6| Step: 6
Training loss: 2.21848929579437
Validation loss: 2.533612324675274

Epoch: 6| Step: 7
Training loss: 2.6775548695303524
Validation loss: 2.541699380191725

Epoch: 6| Step: 8
Training loss: 2.7942685390627715
Validation loss: 2.5139291629735214

Epoch: 6| Step: 9
Training loss: 2.563046373874286
Validation loss: 2.559818667786834

Epoch: 6| Step: 10
Training loss: 1.9189285394066569
Validation loss: 2.5571985737610925

Epoch: 6| Step: 11
Training loss: 2.1585581491788055
Validation loss: 2.534126622132076

Epoch: 6| Step: 12
Training loss: 2.866995906405126
Validation loss: 2.5244405638635667

Epoch: 6| Step: 13
Training loss: 2.745121790645598
Validation loss: 2.524745300412068

Epoch: 229| Step: 0
Training loss: 2.0631167327798425
Validation loss: 2.531319624736919

Epoch: 6| Step: 1
Training loss: 2.5532462344573723
Validation loss: 2.514396302704438

Epoch: 6| Step: 2
Training loss: 2.9290577122559527
Validation loss: 2.584236402153368

Epoch: 6| Step: 3
Training loss: 2.5927037346445214
Validation loss: 2.532734261416276

Epoch: 6| Step: 4
Training loss: 2.314300351794609
Validation loss: 2.5293943134809473

Epoch: 6| Step: 5
Training loss: 2.257138267084735
Validation loss: 2.549232439212786

Epoch: 6| Step: 6
Training loss: 2.438660174367253
Validation loss: 2.559694480636242

Epoch: 6| Step: 7
Training loss: 1.9993945635893493
Validation loss: 2.5591098049861447

Epoch: 6| Step: 8
Training loss: 3.095849143835455
Validation loss: 2.551608645752165

Epoch: 6| Step: 9
Training loss: 2.5118345527591126
Validation loss: 2.5401781047905794

Epoch: 6| Step: 10
Training loss: 2.8132509712469096
Validation loss: 2.555811579242938

Epoch: 6| Step: 11
Training loss: 3.0659017548768
Validation loss: 2.5097035803790133

Epoch: 6| Step: 12
Training loss: 2.983346811899891
Validation loss: 2.5092196082903526

Epoch: 6| Step: 13
Training loss: 2.1583307150845745
Validation loss: 2.5584341668713537

Epoch: 230| Step: 0
Training loss: 1.8743548236917245
Validation loss: 2.5416653971178853

Epoch: 6| Step: 1
Training loss: 2.4083310330741834
Validation loss: 2.5464119348362995

Epoch: 6| Step: 2
Training loss: 2.9632499743459757
Validation loss: 2.5430221550749206

Epoch: 6| Step: 3
Training loss: 2.3148620501749844
Validation loss: 2.541776218672444

Epoch: 6| Step: 4
Training loss: 2.469831591899778
Validation loss: 2.5025908693768297

Epoch: 6| Step: 5
Training loss: 2.7344032504121
Validation loss: 2.5257054976232816

Epoch: 6| Step: 6
Training loss: 2.6119928990336194
Validation loss: 2.5321026592328475

Epoch: 6| Step: 7
Training loss: 3.338569406918846
Validation loss: 2.503145057751794

Epoch: 6| Step: 8
Training loss: 3.2101931115171904
Validation loss: 2.5400733398373374

Epoch: 6| Step: 9
Training loss: 2.193418724500356
Validation loss: 2.5292327227834455

Epoch: 6| Step: 10
Training loss: 2.867239886644631
Validation loss: 2.5310523425527496

Epoch: 6| Step: 11
Training loss: 2.228011666902578
Validation loss: 2.5213932519620554

Epoch: 6| Step: 12
Training loss: 2.261046736371271
Validation loss: 2.509556332159362

Epoch: 6| Step: 13
Training loss: 1.683584049488984
Validation loss: 2.4830296917015042

Epoch: 231| Step: 0
Training loss: 1.8795201177106426
Validation loss: 2.518483010944788

Epoch: 6| Step: 1
Training loss: 3.040893630641489
Validation loss: 2.5044793433673385

Epoch: 6| Step: 2
Training loss: 2.1810072318354665
Validation loss: 2.540517649833179

Epoch: 6| Step: 3
Training loss: 2.2519705937974344
Validation loss: 2.5223993616694194

Epoch: 6| Step: 4
Training loss: 1.7630781862141978
Validation loss: 2.543823579387846

Epoch: 6| Step: 5
Training loss: 2.932196028907501
Validation loss: 2.519076706945199

Epoch: 6| Step: 6
Training loss: 2.8919684510046864
Validation loss: 2.5210358909422412

Epoch: 6| Step: 7
Training loss: 2.0628401735850965
Validation loss: 2.5737722066405864

Epoch: 6| Step: 8
Training loss: 2.4700930841180386
Validation loss: 2.5689039448041426

Epoch: 6| Step: 9
Training loss: 2.582732735825484
Validation loss: 2.5228013042075323

Epoch: 6| Step: 10
Training loss: 3.3060134139622512
Validation loss: 2.517545843780197

Epoch: 6| Step: 11
Training loss: 2.6384260965290847
Validation loss: 2.5268557563770875

Epoch: 6| Step: 12
Training loss: 2.92388667249957
Validation loss: 2.5209049474689365

Epoch: 6| Step: 13
Training loss: 2.5031714350552114
Validation loss: 2.535890124979761

Epoch: 232| Step: 0
Training loss: 2.8615200980822264
Validation loss: 2.5184505835424598

Epoch: 6| Step: 1
Training loss: 2.363341779958304
Validation loss: 2.499061071805774

Epoch: 6| Step: 2
Training loss: 2.3109946635245326
Validation loss: 2.556478838136574

Epoch: 6| Step: 3
Training loss: 3.1144856381271206
Validation loss: 2.5298235261767257

Epoch: 6| Step: 4
Training loss: 2.6055963076088933
Validation loss: 2.513336536882742

Epoch: 6| Step: 5
Training loss: 2.4508700363522156
Validation loss: 2.5372655454600834

Epoch: 6| Step: 6
Training loss: 2.720132059930531
Validation loss: 2.519995698111307

Epoch: 6| Step: 7
Training loss: 2.4035695987954377
Validation loss: 2.5420491675298504

Epoch: 6| Step: 8
Training loss: 2.495743179178991
Validation loss: 2.5398174789656305

Epoch: 6| Step: 9
Training loss: 2.862267204047044
Validation loss: 2.5354828934976945

Epoch: 6| Step: 10
Training loss: 2.8190839210189464
Validation loss: 2.5581510422428337

Epoch: 6| Step: 11
Training loss: 3.0242969168768243
Validation loss: 2.545544362601064

Epoch: 6| Step: 12
Training loss: 2.2563391501442402
Validation loss: 2.4811021924393297

Epoch: 6| Step: 13
Training loss: 1.4669214594488447
Validation loss: 2.5310264761152346

Epoch: 233| Step: 0
Training loss: 3.002923177076276
Validation loss: 2.5108576575208765

Epoch: 6| Step: 1
Training loss: 1.8242599188592608
Validation loss: 2.5482615829650417

Epoch: 6| Step: 2
Training loss: 2.8466657844321723
Validation loss: 2.5464894032850927

Epoch: 6| Step: 3
Training loss: 2.9176098706265945
Validation loss: 2.5591401173793127

Epoch: 6| Step: 4
Training loss: 3.421867196409481
Validation loss: 2.5350008051533344

Epoch: 6| Step: 5
Training loss: 2.260739550990633
Validation loss: 2.523895241458347

Epoch: 6| Step: 6
Training loss: 2.5267036960005447
Validation loss: 2.5070780589509924

Epoch: 6| Step: 7
Training loss: 2.289674019745872
Validation loss: 2.4940771617324025

Epoch: 6| Step: 8
Training loss: 2.6529693090545083
Validation loss: 2.550706810777367

Epoch: 6| Step: 9
Training loss: 1.662419900900612
Validation loss: 2.5301149923526145

Epoch: 6| Step: 10
Training loss: 2.476552872985035
Validation loss: 2.5542927851302943

Epoch: 6| Step: 11
Training loss: 2.1888739494463683
Validation loss: 2.5265332505561418

Epoch: 6| Step: 12
Training loss: 2.605297168761163
Validation loss: 2.5210817588027465

Epoch: 6| Step: 13
Training loss: 2.858871665804201
Validation loss: 2.535612674663878

Epoch: 234| Step: 0
Training loss: 2.5232734744968903
Validation loss: 2.5424291985556007

Epoch: 6| Step: 1
Training loss: 1.8282704091726685
Validation loss: 2.5105349274957924

Epoch: 6| Step: 2
Training loss: 2.4415298796822755
Validation loss: 2.4982304514465254

Epoch: 6| Step: 3
Training loss: 2.974473916348995
Validation loss: 2.5310711054934165

Epoch: 6| Step: 4
Training loss: 2.2924065753699354
Validation loss: 2.518938022432285

Epoch: 6| Step: 5
Training loss: 1.7331523537910383
Validation loss: 2.4873459250741794

Epoch: 6| Step: 6
Training loss: 1.9880694743092293
Validation loss: 2.5077677766746915

Epoch: 6| Step: 7
Training loss: 2.772458529630867
Validation loss: 2.5142988794748136

Epoch: 6| Step: 8
Training loss: 2.9212056581578465
Validation loss: 2.483538169311043

Epoch: 6| Step: 9
Training loss: 2.2823588145650655
Validation loss: 2.484215367320359

Epoch: 6| Step: 10
Training loss: 2.9536190527949273
Validation loss: 2.535115235968628

Epoch: 6| Step: 11
Training loss: 3.1193395084233835
Validation loss: 2.5330697530363167

Epoch: 6| Step: 12
Training loss: 2.9763430216614437
Validation loss: 2.5193353315886005

Epoch: 6| Step: 13
Training loss: 2.9328823725527196
Validation loss: 2.498682164924959

Epoch: 235| Step: 0
Training loss: 2.8405374679256483
Validation loss: 2.50980429945347

Epoch: 6| Step: 1
Training loss: 2.720820197813175
Validation loss: 2.50419048611743

Epoch: 6| Step: 2
Training loss: 2.7362039607301893
Validation loss: 2.5739420009862504

Epoch: 6| Step: 3
Training loss: 2.708792505709741
Validation loss: 2.5123463133440698

Epoch: 6| Step: 4
Training loss: 2.611150445934924
Validation loss: 2.535482376822744

Epoch: 6| Step: 5
Training loss: 2.2312227434475664
Validation loss: 2.509464718784476

Epoch: 6| Step: 6
Training loss: 2.812068651818277
Validation loss: 2.5172575408220115

Epoch: 6| Step: 7
Training loss: 2.4272725655020224
Validation loss: 2.5090214715826713

Epoch: 6| Step: 8
Training loss: 2.648880732983885
Validation loss: 2.522462904596672

Epoch: 6| Step: 9
Training loss: 2.273440855882403
Validation loss: 2.5193248972390383

Epoch: 6| Step: 10
Training loss: 2.4124230802458815
Validation loss: 2.542648913047319

Epoch: 6| Step: 11
Training loss: 1.8249738874265622
Validation loss: 2.5273484201168297

Epoch: 6| Step: 12
Training loss: 2.920829055354912
Validation loss: 2.531914626149029

Epoch: 6| Step: 13
Training loss: 2.441985282897341
Validation loss: 2.552932984231699

Epoch: 236| Step: 0
Training loss: 2.724402416394968
Validation loss: 2.541879237212573

Epoch: 6| Step: 1
Training loss: 2.8857445008450964
Validation loss: 2.539770103279601

Epoch: 6| Step: 2
Training loss: 3.273958660873767
Validation loss: 2.5329563826107653

Epoch: 6| Step: 3
Training loss: 2.5370050633507386
Validation loss: 2.537739379541721

Epoch: 6| Step: 4
Training loss: 2.722278541406212
Validation loss: 2.548843842767822

Epoch: 6| Step: 5
Training loss: 2.3946498324978505
Validation loss: 2.563065487233585

Epoch: 6| Step: 6
Training loss: 1.808595595554539
Validation loss: 2.5651086764215045

Epoch: 6| Step: 7
Training loss: 2.124288439826342
Validation loss: 2.522306442190293

Epoch: 6| Step: 8
Training loss: 2.3994818287006567
Validation loss: 2.52134033843017

Epoch: 6| Step: 9
Training loss: 2.3753663834506544
Validation loss: 2.5200138337998683

Epoch: 6| Step: 10
Training loss: 2.6059587238114
Validation loss: 2.5407666284275527

Epoch: 6| Step: 11
Training loss: 2.2018747231501963
Validation loss: 2.5255408225768257

Epoch: 6| Step: 12
Training loss: 2.711399709561422
Validation loss: 2.5687086183800463

Epoch: 6| Step: 13
Training loss: 3.2957669244595147
Validation loss: 2.515106917234008

Epoch: 237| Step: 0
Training loss: 2.5327807848649098
Validation loss: 2.5386755318535776

Epoch: 6| Step: 1
Training loss: 2.5910424497801072
Validation loss: 2.5309855795100265

Epoch: 6| Step: 2
Training loss: 2.390488165635619
Validation loss: 2.5131081773100257

Epoch: 6| Step: 3
Training loss: 2.7686073903713835
Validation loss: 2.546015883619595

Epoch: 6| Step: 4
Training loss: 2.242971250080242
Validation loss: 2.5097384109512326

Epoch: 6| Step: 5
Training loss: 2.6214495671768994
Validation loss: 2.542052348821317

Epoch: 6| Step: 6
Training loss: 2.9481519373814895
Validation loss: 2.5362797933891708

Epoch: 6| Step: 7
Training loss: 3.0273535156092493
Validation loss: 2.546860072836235

Epoch: 6| Step: 8
Training loss: 2.551712590490985
Validation loss: 2.4941257445494665

Epoch: 6| Step: 9
Training loss: 2.1197038736994642
Validation loss: 2.553423039223796

Epoch: 6| Step: 10
Training loss: 2.330143405610575
Validation loss: 2.5055955869757933

Epoch: 6| Step: 11
Training loss: 2.2552984159478333
Validation loss: 2.5498885175595554

Epoch: 6| Step: 12
Training loss: 2.031135321094199
Validation loss: 2.5516769011882414

Epoch: 6| Step: 13
Training loss: 3.881231035809711
Validation loss: 2.551021456638495

Epoch: 238| Step: 0
Training loss: 2.55752200910508
Validation loss: 2.542446772904665

Epoch: 6| Step: 1
Training loss: 1.821613812577087
Validation loss: 2.526179628671672

Epoch: 6| Step: 2
Training loss: 1.788966680234571
Validation loss: 2.5197799316961023

Epoch: 6| Step: 3
Training loss: 2.4974092410897475
Validation loss: 2.5192767942676584

Epoch: 6| Step: 4
Training loss: 2.2610570700662733
Validation loss: 2.53357898696891

Epoch: 6| Step: 5
Training loss: 2.8479969883056317
Validation loss: 2.538792361899392

Epoch: 6| Step: 6
Training loss: 2.7392095932141887
Validation loss: 2.5768192767141103

Epoch: 6| Step: 7
Training loss: 3.2775896273418352
Validation loss: 2.5066691412315687

Epoch: 6| Step: 8
Training loss: 2.8252137938764244
Validation loss: 2.537748980020356

Epoch: 6| Step: 9
Training loss: 2.1578667149787165
Validation loss: 2.543660609442947

Epoch: 6| Step: 10
Training loss: 2.8398449253672773
Validation loss: 2.5457922886778492

Epoch: 6| Step: 11
Training loss: 2.141727581221647
Validation loss: 2.494378130752619

Epoch: 6| Step: 12
Training loss: 2.5005647974983645
Validation loss: 2.5136261391393706

Epoch: 6| Step: 13
Training loss: 3.163224474283643
Validation loss: 2.5171857081714637

Epoch: 239| Step: 0
Training loss: 3.441313968594603
Validation loss: 2.546948785478673

Epoch: 6| Step: 1
Training loss: 2.2217460069085004
Validation loss: 2.5338898650240993

Epoch: 6| Step: 2
Training loss: 2.831078099301696
Validation loss: 2.508411187652914

Epoch: 6| Step: 3
Training loss: 2.506389653983437
Validation loss: 2.524684664735722

Epoch: 6| Step: 4
Training loss: 2.964492636334479
Validation loss: 2.5429236519557925

Epoch: 6| Step: 5
Training loss: 2.539644802518932
Validation loss: 2.5317509848035784

Epoch: 6| Step: 6
Training loss: 1.8258342247175405
Validation loss: 2.5557278190809574

Epoch: 6| Step: 7
Training loss: 2.0410447114030923
Validation loss: 2.51006660749076

Epoch: 6| Step: 8
Training loss: 2.1631710268252915
Validation loss: 2.5290551898777602

Epoch: 6| Step: 9
Training loss: 2.5329986936834668
Validation loss: 2.5248422647123516

Epoch: 6| Step: 10
Training loss: 2.9609323466313993
Validation loss: 2.553912371797735

Epoch: 6| Step: 11
Training loss: 2.2825563426756883
Validation loss: 2.516779427900985

Epoch: 6| Step: 12
Training loss: 2.7648419494709278
Validation loss: 2.537195650417392

Epoch: 6| Step: 13
Training loss: 2.433326081913982
Validation loss: 2.541335774438672

Epoch: 240| Step: 0
Training loss: 2.5535841492784783
Validation loss: 2.5560005072452396

Epoch: 6| Step: 1
Training loss: 2.1962187176095673
Validation loss: 2.50960852982537

Epoch: 6| Step: 2
Training loss: 2.3482405305190444
Validation loss: 2.5166765880110717

Epoch: 6| Step: 3
Training loss: 2.6241510017374066
Validation loss: 2.5127606357888785

Epoch: 6| Step: 4
Training loss: 2.583014181378018
Validation loss: 2.562979556515023

Epoch: 6| Step: 5
Training loss: 2.43559327498153
Validation loss: 2.5390958697048553

Epoch: 6| Step: 6
Training loss: 2.1577306998491004
Validation loss: 2.550513136103423

Epoch: 6| Step: 7
Training loss: 2.1799331581297565
Validation loss: 2.5602920948751233

Epoch: 6| Step: 8
Training loss: 2.4508888111582388
Validation loss: 2.561163707175855

Epoch: 6| Step: 9
Training loss: 2.9921686953757995
Validation loss: 2.5336287115650213

Epoch: 6| Step: 10
Training loss: 2.3045207335056084
Validation loss: 2.535405908780294

Epoch: 6| Step: 11
Training loss: 2.745774143335485
Validation loss: 2.5342321984091774

Epoch: 6| Step: 12
Training loss: 3.4356468060366856
Validation loss: 2.5145416260656717

Epoch: 6| Step: 13
Training loss: 2.7262998088220245
Validation loss: 2.517813849571241

Epoch: 241| Step: 0
Training loss: 2.448440452432709
Validation loss: 2.541529579305202

Epoch: 6| Step: 1
Training loss: 2.537694568409323
Validation loss: 2.559302432187519

Epoch: 6| Step: 2
Training loss: 2.3794780476217405
Validation loss: 2.519396662791989

Epoch: 6| Step: 3
Training loss: 2.878793287196091
Validation loss: 2.5505523978483255

Epoch: 6| Step: 4
Training loss: 2.6550223711296166
Validation loss: 2.540096599556122

Epoch: 6| Step: 5
Training loss: 2.2106865514674645
Validation loss: 2.518120740074888

Epoch: 6| Step: 6
Training loss: 3.499994550428235
Validation loss: 2.5271005379858495

Epoch: 6| Step: 7
Training loss: 2.4819438242921894
Validation loss: 2.534438429868766

Epoch: 6| Step: 8
Training loss: 2.7020928501578223
Validation loss: 2.543211735797781

Epoch: 6| Step: 9
Training loss: 2.772132244021689
Validation loss: 2.562166581971399

Epoch: 6| Step: 10
Training loss: 2.194894771269411
Validation loss: 2.5275187659607368

Epoch: 6| Step: 11
Training loss: 2.2091499354264807
Validation loss: 2.5089314837852026

Epoch: 6| Step: 12
Training loss: 2.0156164834485772
Validation loss: 2.5709495603986405

Epoch: 6| Step: 13
Training loss: 2.6816517384431493
Validation loss: 2.5391636416702825

Epoch: 242| Step: 0
Training loss: 2.93230530829952
Validation loss: 2.5453699132594743

Epoch: 6| Step: 1
Training loss: 2.8393137777060646
Validation loss: 2.541072421620559

Epoch: 6| Step: 2
Training loss: 2.264726447455723
Validation loss: 2.515770472495652

Epoch: 6| Step: 3
Training loss: 1.996718575763941
Validation loss: 2.5222779902956844

Epoch: 6| Step: 4
Training loss: 2.400090247682675
Validation loss: 2.5275542466403818

Epoch: 6| Step: 5
Training loss: 2.703790307170008
Validation loss: 2.571435987948849

Epoch: 6| Step: 6
Training loss: 2.963578065850903
Validation loss: 2.530280205581381

Epoch: 6| Step: 7
Training loss: 2.1382685032702597
Validation loss: 2.5351654704308144

Epoch: 6| Step: 8
Training loss: 1.9545447720532505
Validation loss: 2.539792298850586

Epoch: 6| Step: 9
Training loss: 3.0048194955447265
Validation loss: 2.5329482462131905

Epoch: 6| Step: 10
Training loss: 2.121693675590223
Validation loss: 2.519080685094536

Epoch: 6| Step: 11
Training loss: 1.3892870692520736
Validation loss: 2.5012886520303312

Epoch: 6| Step: 12
Training loss: 3.200495687478558
Validation loss: 2.5136079634830026

Epoch: 6| Step: 13
Training loss: 3.7455476396612126
Validation loss: 2.5412551305309194

Epoch: 243| Step: 0
Training loss: 2.095689473103251
Validation loss: 2.5068974845089933

Epoch: 6| Step: 1
Training loss: 2.887210127907713
Validation loss: 2.5205620992913653

Epoch: 6| Step: 2
Training loss: 2.5944797454472703
Validation loss: 2.523260998011423

Epoch: 6| Step: 3
Training loss: 1.6354139129550695
Validation loss: 2.497729737036295

Epoch: 6| Step: 4
Training loss: 2.658733687752065
Validation loss: 2.5022170244891364

Epoch: 6| Step: 5
Training loss: 2.262304780240976
Validation loss: 2.5219930054131403

Epoch: 6| Step: 6
Training loss: 2.558054906582408
Validation loss: 2.5545634899137792

Epoch: 6| Step: 7
Training loss: 2.0489906850205606
Validation loss: 2.5130676720830127

Epoch: 6| Step: 8
Training loss: 2.921970957440037
Validation loss: 2.5316030088294803

Epoch: 6| Step: 9
Training loss: 3.3812068802486808
Validation loss: 2.534818860089161

Epoch: 6| Step: 10
Training loss: 2.6981503791667008
Validation loss: 2.515832579449575

Epoch: 6| Step: 11
Training loss: 2.5383928090685854
Validation loss: 2.546471765222501

Epoch: 6| Step: 12
Training loss: 2.3004566734394434
Validation loss: 2.545058693346302

Epoch: 6| Step: 13
Training loss: 3.0265291557779395
Validation loss: 2.5391008695696025

Epoch: 244| Step: 0
Training loss: 2.7333481086548708
Validation loss: 2.5360134115491726

Epoch: 6| Step: 1
Training loss: 2.5952956408665617
Validation loss: 2.5492061583742722

Epoch: 6| Step: 2
Training loss: 3.3309176910867855
Validation loss: 2.5262934686218554

Epoch: 6| Step: 3
Training loss: 1.764447384956487
Validation loss: 2.5564396142605235

Epoch: 6| Step: 4
Training loss: 2.5985529907766023
Validation loss: 2.551446129861686

Epoch: 6| Step: 5
Training loss: 3.057406023129901
Validation loss: 2.5317174625947922

Epoch: 6| Step: 6
Training loss: 2.1803214952530174
Validation loss: 2.520735750383412

Epoch: 6| Step: 7
Training loss: 2.2652371830996025
Validation loss: 2.51145270187344

Epoch: 6| Step: 8
Training loss: 2.3165679412040996
Validation loss: 2.564358694188605

Epoch: 6| Step: 9
Training loss: 2.5954801005815185
Validation loss: 2.493485373465048

Epoch: 6| Step: 10
Training loss: 2.2411158323366944
Validation loss: 2.512419612902343

Epoch: 6| Step: 11
Training loss: 3.053500596119629
Validation loss: 2.513921911347215

Epoch: 6| Step: 12
Training loss: 2.6826406504934974
Validation loss: 2.5227521944077744

Epoch: 6| Step: 13
Training loss: 2.369039484618481
Validation loss: 2.552537275922795

Epoch: 245| Step: 0
Training loss: 2.739754230308382
Validation loss: 2.5461768572419756

Epoch: 6| Step: 1
Training loss: 3.0849099295403204
Validation loss: 2.533792218487037

Epoch: 6| Step: 2
Training loss: 3.222100005261893
Validation loss: 2.5332366741011745

Epoch: 6| Step: 3
Training loss: 2.736511180780265
Validation loss: 2.528232048937475

Epoch: 6| Step: 4
Training loss: 2.3279868475355663
Validation loss: 2.5139151512340443

Epoch: 6| Step: 5
Training loss: 2.8609574738601635
Validation loss: 2.5290601639772885

Epoch: 6| Step: 6
Training loss: 2.3924932223456197
Validation loss: 2.5168182095029112

Epoch: 6| Step: 7
Training loss: 1.7108099902988918
Validation loss: 2.523829548140517

Epoch: 6| Step: 8
Training loss: 2.6694619508880004
Validation loss: 2.5016672358621848

Epoch: 6| Step: 9
Training loss: 2.694378588768523
Validation loss: 2.4959799716392803

Epoch: 6| Step: 10
Training loss: 2.12020355147104
Validation loss: 2.5126702482951817

Epoch: 6| Step: 11
Training loss: 2.4700565019701077
Validation loss: 2.5432681829874286

Epoch: 6| Step: 12
Training loss: 2.3229542089145596
Validation loss: 2.5451136964331065

Epoch: 6| Step: 13
Training loss: 2.3961098773962064
Validation loss: 2.5062275998846255

Epoch: 246| Step: 0
Training loss: 2.6548822808614196
Validation loss: 2.5400929772872676

Epoch: 6| Step: 1
Training loss: 2.422195708667329
Validation loss: 2.499380946679543

Epoch: 6| Step: 2
Training loss: 2.6555509657223815
Validation loss: 2.5349340576111414

Epoch: 6| Step: 3
Training loss: 2.579764844542465
Validation loss: 2.5121772715160593

Epoch: 6| Step: 4
Training loss: 1.846255071551693
Validation loss: 2.558557341065374

Epoch: 6| Step: 5
Training loss: 2.7110326568876713
Validation loss: 2.508633165185928

Epoch: 6| Step: 6
Training loss: 2.565600775396398
Validation loss: 2.50841292303837

Epoch: 6| Step: 7
Training loss: 2.7249594624155167
Validation loss: 2.539874401296121

Epoch: 6| Step: 8
Training loss: 1.7028823819717656
Validation loss: 2.531253953953669

Epoch: 6| Step: 9
Training loss: 2.050628249500959
Validation loss: 2.5595161299010623

Epoch: 6| Step: 10
Training loss: 3.33300309134774
Validation loss: 2.523225809519644

Epoch: 6| Step: 11
Training loss: 2.867296928791374
Validation loss: 2.532512532742644

Epoch: 6| Step: 12
Training loss: 2.639275742221992
Validation loss: 2.5420765212622456

Epoch: 6| Step: 13
Training loss: 2.624084676376497
Validation loss: 2.5357891621039585

Epoch: 247| Step: 0
Training loss: 3.466561529203599
Validation loss: 2.505904587637742

Epoch: 6| Step: 1
Training loss: 2.3151374416870736
Validation loss: 2.555165605129393

Epoch: 6| Step: 2
Training loss: 1.9569595536977662
Validation loss: 2.5746559268416034

Epoch: 6| Step: 3
Training loss: 2.0335868891022493
Validation loss: 2.5095147465028314

Epoch: 6| Step: 4
Training loss: 2.773008845340283
Validation loss: 2.5046388247688336

Epoch: 6| Step: 5
Training loss: 2.7835956069207777
Validation loss: 2.520035990760607

Epoch: 6| Step: 6
Training loss: 2.451188118565451
Validation loss: 2.556660967464309

Epoch: 6| Step: 7
Training loss: 2.466082713451631
Validation loss: 2.5316533079200414

Epoch: 6| Step: 8
Training loss: 2.1953432345699433
Validation loss: 2.517266584429024

Epoch: 6| Step: 9
Training loss: 2.9192888371062438
Validation loss: 2.5594505355305923

Epoch: 6| Step: 10
Training loss: 2.9884987346818175
Validation loss: 2.570248802979922

Epoch: 6| Step: 11
Training loss: 2.5387909259812487
Validation loss: 2.5485006650489783

Epoch: 6| Step: 12
Training loss: 2.2376823223143503
Validation loss: 2.4992857856259842

Epoch: 6| Step: 13
Training loss: 2.2556664850907526
Validation loss: 2.5218620831725413

Epoch: 248| Step: 0
Training loss: 1.7272376212061453
Validation loss: 2.5424909154553195

Epoch: 6| Step: 1
Training loss: 3.2127971229745516
Validation loss: 2.5116020916425548

Epoch: 6| Step: 2
Training loss: 2.525792773592449
Validation loss: 2.5368318256014595

Epoch: 6| Step: 3
Training loss: 1.8563921427650265
Validation loss: 2.5045125924426768

Epoch: 6| Step: 4
Training loss: 2.289376924031267
Validation loss: 2.5501605081682235

Epoch: 6| Step: 5
Training loss: 3.0948775577513796
Validation loss: 2.5306506189783016

Epoch: 6| Step: 6
Training loss: 2.702942152950382
Validation loss: 2.5407599599193142

Epoch: 6| Step: 7
Training loss: 2.356869206913293
Validation loss: 2.5467783581624714

Epoch: 6| Step: 8
Training loss: 3.081832279798118
Validation loss: 2.5481612128890547

Epoch: 6| Step: 9
Training loss: 2.2052865753444677
Validation loss: 2.5283542701231294

Epoch: 6| Step: 10
Training loss: 3.1547279842777503
Validation loss: 2.5404135013410247

Epoch: 6| Step: 11
Training loss: 2.2621205554409856
Validation loss: 2.5390905336105565

Epoch: 6| Step: 12
Training loss: 2.230862076747182
Validation loss: 2.509496735168046

Epoch: 6| Step: 13
Training loss: 2.7767870810579396
Validation loss: 2.4862775472109124

Epoch: 249| Step: 0
Training loss: 2.232506350757071
Validation loss: 2.550795783861582

Epoch: 6| Step: 1
Training loss: 2.168957953693182
Validation loss: 2.525914188200755

Epoch: 6| Step: 2
Training loss: 2.6608208317696436
Validation loss: 2.513222606799886

Epoch: 6| Step: 3
Training loss: 3.0230384583041188
Validation loss: 2.555443371337113

Epoch: 6| Step: 4
Training loss: 1.9436035002914598
Validation loss: 2.5226553372681852

Epoch: 6| Step: 5
Training loss: 2.972373598072516
Validation loss: 2.505711980678793

Epoch: 6| Step: 6
Training loss: 1.3914954215278377
Validation loss: 2.505143779497443

Epoch: 6| Step: 7
Training loss: 2.0356465808824233
Validation loss: 2.5778794963209775

Epoch: 6| Step: 8
Training loss: 3.1389020302496493
Validation loss: 2.5086459780772863

Epoch: 6| Step: 9
Training loss: 2.467069899188446
Validation loss: 2.541127582619839

Epoch: 6| Step: 10
Training loss: 2.7975360792354445
Validation loss: 2.5463589471903156

Epoch: 6| Step: 11
Training loss: 2.0949268734353246
Validation loss: 2.5564860683246624

Epoch: 6| Step: 12
Training loss: 2.721116713000654
Validation loss: 2.520721828347528

Epoch: 6| Step: 13
Training loss: 3.5516226163006346
Validation loss: 2.517144444106197

Epoch: 250| Step: 0
Training loss: 2.444245419443635
Validation loss: 2.5309980716009286

Epoch: 6| Step: 1
Training loss: 2.8356111196809244
Validation loss: 2.5151305566561732

Epoch: 6| Step: 2
Training loss: 1.9585575692327508
Validation loss: 2.539204959832708

Epoch: 6| Step: 3
Training loss: 2.7563965704389912
Validation loss: 2.5349784756004516

Epoch: 6| Step: 4
Training loss: 2.6035150755461722
Validation loss: 2.557188440277071

Epoch: 6| Step: 5
Training loss: 2.626152421390418
Validation loss: 2.4688367183268602

Epoch: 6| Step: 6
Training loss: 2.740134228530155
Validation loss: 2.5520754029027506

Epoch: 6| Step: 7
Training loss: 2.777723841673356
Validation loss: 2.5400548164870846

Epoch: 6| Step: 8
Training loss: 2.388931672011324
Validation loss: 2.524582348947671

Epoch: 6| Step: 9
Training loss: 2.7714700086892012
Validation loss: 2.549777594134156

Epoch: 6| Step: 10
Training loss: 2.2612276746534508
Validation loss: 2.507846988827622

Epoch: 6| Step: 11
Training loss: 1.9464604311394196
Validation loss: 2.517364363299909

Epoch: 6| Step: 12
Training loss: 2.494971176623124
Validation loss: 2.5023123701528935

Epoch: 6| Step: 13
Training loss: 2.6945926313081077
Validation loss: 2.4771007707131227

Epoch: 251| Step: 0
Training loss: 2.659090565264249
Validation loss: 2.5580805372991753

Epoch: 6| Step: 1
Training loss: 2.1302014811684633
Validation loss: 2.5188854959820373

Epoch: 6| Step: 2
Training loss: 2.776290586570783
Validation loss: 2.508633319496941

Epoch: 6| Step: 3
Training loss: 2.9416288213123503
Validation loss: 2.533498142701759

Epoch: 6| Step: 4
Training loss: 2.0070585623932296
Validation loss: 2.4952664395062714

Epoch: 6| Step: 5
Training loss: 1.8498425442472342
Validation loss: 2.5302713513467148

Epoch: 6| Step: 6
Training loss: 2.820550459441666
Validation loss: 2.504228510613458

Epoch: 6| Step: 7
Training loss: 2.6357143269183325
Validation loss: 2.5261024811992105

Epoch: 6| Step: 8
Training loss: 2.7677751361986265
Validation loss: 2.5499439352372057

Epoch: 6| Step: 9
Training loss: 2.0658542061515908
Validation loss: 2.569933667918753

Epoch: 6| Step: 10
Training loss: 2.646434555429371
Validation loss: 2.552904378593895

Epoch: 6| Step: 11
Training loss: 3.2135498112720096
Validation loss: 2.526453567952939

Epoch: 6| Step: 12
Training loss: 2.1311497057811137
Validation loss: 2.506492709334491

Epoch: 6| Step: 13
Training loss: 2.6415725001289476
Validation loss: 2.5158289375306584

Epoch: 252| Step: 0
Training loss: 2.3523994046471732
Validation loss: 2.5326376741740395

Epoch: 6| Step: 1
Training loss: 1.7459974838448449
Validation loss: 2.530778265408958

Epoch: 6| Step: 2
Training loss: 1.8392803520092726
Validation loss: 2.5130100874639454

Epoch: 6| Step: 3
Training loss: 2.516679815127486
Validation loss: 2.4956302251274924

Epoch: 6| Step: 4
Training loss: 2.3274967446801256
Validation loss: 2.554453467995596

Epoch: 6| Step: 5
Training loss: 2.68961765337341
Validation loss: 2.5268913569514266

Epoch: 6| Step: 6
Training loss: 3.2322895495479207
Validation loss: 2.559371617830185

Epoch: 6| Step: 7
Training loss: 2.223141689286153
Validation loss: 2.4851817119361055

Epoch: 6| Step: 8
Training loss: 2.9576952250547257
Validation loss: 2.5266895887037686

Epoch: 6| Step: 9
Training loss: 2.641218310634074
Validation loss: 2.5043762543019477

Epoch: 6| Step: 10
Training loss: 3.3131764998694395
Validation loss: 2.5147027478165964

Epoch: 6| Step: 11
Training loss: 2.526399556923236
Validation loss: 2.5468492524877417

Epoch: 6| Step: 12
Training loss: 2.442814828032943
Validation loss: 2.5589172885151363

Epoch: 6| Step: 13
Training loss: 1.9804097003146757
Validation loss: 2.535201666213263

Epoch: 253| Step: 0
Training loss: 2.327158067869813
Validation loss: 2.490381987425441

Epoch: 6| Step: 1
Training loss: 2.7128967768446266
Validation loss: 2.5079732270989363

Epoch: 6| Step: 2
Training loss: 2.9824540112156153
Validation loss: 2.5142324877908377

Epoch: 6| Step: 3
Training loss: 1.4432514238058125
Validation loss: 2.5101912624541467

Epoch: 6| Step: 4
Training loss: 3.125750489716414
Validation loss: 2.5442633481004333

Epoch: 6| Step: 5
Training loss: 2.785874711472147
Validation loss: 2.524415895547017

Epoch: 6| Step: 6
Training loss: 2.2431121939747687
Validation loss: 2.489624443406424

Epoch: 6| Step: 7
Training loss: 2.524548171002566
Validation loss: 2.5310691182458473

Epoch: 6| Step: 8
Training loss: 2.2464149311802983
Validation loss: 2.535571281786586

Epoch: 6| Step: 9
Training loss: 3.3961246732151205
Validation loss: 2.56203505697269

Epoch: 6| Step: 10
Training loss: 2.455983919675044
Validation loss: 2.5167417560135887

Epoch: 6| Step: 11
Training loss: 1.8756912228906841
Validation loss: 2.517496252641566

Epoch: 6| Step: 12
Training loss: 2.528087759981439
Validation loss: 2.5264952766216484

Epoch: 6| Step: 13
Training loss: 2.167186259054146
Validation loss: 2.5303927516632005

Epoch: 254| Step: 0
Training loss: 2.086502170213329
Validation loss: 2.533131218259508

Epoch: 6| Step: 1
Training loss: 2.6971411652132504
Validation loss: 2.5023290101668474

Epoch: 6| Step: 2
Training loss: 2.885286752872454
Validation loss: 2.552286170062817

Epoch: 6| Step: 3
Training loss: 2.456776713421591
Validation loss: 2.477974550670983

Epoch: 6| Step: 4
Training loss: 2.6557826809161917
Validation loss: 2.5360392427909293

Epoch: 6| Step: 5
Training loss: 2.889138989341192
Validation loss: 2.5348172176231816

Epoch: 6| Step: 6
Training loss: 2.508569335818698
Validation loss: 2.5360242948101734

Epoch: 6| Step: 7
Training loss: 2.3734236053200717
Validation loss: 2.5175642557654023

Epoch: 6| Step: 8
Training loss: 2.061418885760679
Validation loss: 2.519837924220156

Epoch: 6| Step: 9
Training loss: 3.0098895147076528
Validation loss: 2.546298431535631

Epoch: 6| Step: 10
Training loss: 2.522623881690277
Validation loss: 2.566378705566522

Epoch: 6| Step: 11
Training loss: 2.0781392261907325
Validation loss: 2.5098606195543693

Epoch: 6| Step: 12
Training loss: 2.4939055544874527
Validation loss: 2.5244984665876333

Epoch: 6| Step: 13
Training loss: 2.027806223781571
Validation loss: 2.516217991842079

Epoch: 255| Step: 0
Training loss: 2.8403091574015567
Validation loss: 2.5035714378091027

Epoch: 6| Step: 1
Training loss: 2.066141785858962
Validation loss: 2.519979456553962

Epoch: 6| Step: 2
Training loss: 2.794016309500778
Validation loss: 2.494606682400443

Epoch: 6| Step: 3
Training loss: 2.5563936748093923
Validation loss: 2.573536647753023

Epoch: 6| Step: 4
Training loss: 2.098453647213389
Validation loss: 2.5163647218792495

Epoch: 6| Step: 5
Training loss: 2.699077420323181
Validation loss: 2.5235655336324343

Epoch: 6| Step: 6
Training loss: 2.389877401902505
Validation loss: 2.530090353016825

Epoch: 6| Step: 7
Training loss: 2.3272298845837
Validation loss: 2.512423308741806

Epoch: 6| Step: 8
Training loss: 2.576263680604521
Validation loss: 2.5173248945456796

Epoch: 6| Step: 9
Training loss: 2.6564551779097747
Validation loss: 2.5209444598324384

Epoch: 6| Step: 10
Training loss: 2.278185105021473
Validation loss: 2.5032830626105915

Epoch: 6| Step: 11
Training loss: 3.384425247960605
Validation loss: 2.542329532990271

Epoch: 6| Step: 12
Training loss: 2.510776562568552
Validation loss: 2.5187310225857007

Epoch: 6| Step: 13
Training loss: 1.5812161076347468
Validation loss: 2.5136041000809404

Epoch: 256| Step: 0
Training loss: 3.3567471894269287
Validation loss: 2.539508593980247

Epoch: 6| Step: 1
Training loss: 2.59604965663846
Validation loss: 2.5222799763384

Epoch: 6| Step: 2
Training loss: 2.754463561460484
Validation loss: 2.5100538794737006

Epoch: 6| Step: 3
Training loss: 2.4030113719174224
Validation loss: 2.515433141410736

Epoch: 6| Step: 4
Training loss: 2.538975640791954
Validation loss: 2.5355473031316627

Epoch: 6| Step: 5
Training loss: 2.438383871386095
Validation loss: 2.505820721507109

Epoch: 6| Step: 6
Training loss: 1.9516699902609072
Validation loss: 2.539336482080459

Epoch: 6| Step: 7
Training loss: 2.9466209844209823
Validation loss: 2.5227662729123304

Epoch: 6| Step: 8
Training loss: 2.502391624885944
Validation loss: 2.529360437728841

Epoch: 6| Step: 9
Training loss: 2.4701271561854075
Validation loss: 2.549106609937965

Epoch: 6| Step: 10
Training loss: 2.5584167885189535
Validation loss: 2.506349050918064

Epoch: 6| Step: 11
Training loss: 2.4460975434947927
Validation loss: 2.5214990096083234

Epoch: 6| Step: 12
Training loss: 1.9585063195086447
Validation loss: 2.5666387666204145

Epoch: 6| Step: 13
Training loss: 2.05705849658091
Validation loss: 2.563669719585359

Epoch: 257| Step: 0
Training loss: 2.285234081540463
Validation loss: 2.5174514885165546

Epoch: 6| Step: 1
Training loss: 2.452497268348596
Validation loss: 2.5601722695630214

Epoch: 6| Step: 2
Training loss: 2.626697445590681
Validation loss: 2.5350577941754144

Epoch: 6| Step: 3
Training loss: 2.809922457130782
Validation loss: 2.539469278595516

Epoch: 6| Step: 4
Training loss: 2.561378233470219
Validation loss: 2.537329257143022

Epoch: 6| Step: 5
Training loss: 3.275749318835847
Validation loss: 2.5728521326912914

Epoch: 6| Step: 6
Training loss: 1.6993387114174225
Validation loss: 2.549742464930181

Epoch: 6| Step: 7
Training loss: 2.4320959193488
Validation loss: 2.5177414422189996

Epoch: 6| Step: 8
Training loss: 2.2287799701825795
Validation loss: 2.5414036864047262

Epoch: 6| Step: 9
Training loss: 2.5902215328252
Validation loss: 2.537101232722181

Epoch: 6| Step: 10
Training loss: 2.1739760740098846
Validation loss: 2.5245977871089296

Epoch: 6| Step: 11
Training loss: 2.2025023317703565
Validation loss: 2.5226472260806103

Epoch: 6| Step: 12
Training loss: 2.8270305391543835
Validation loss: 2.513861105367313

Epoch: 6| Step: 13
Training loss: 3.0361304566873297
Validation loss: 2.5377822775951393

Epoch: 258| Step: 0
Training loss: 2.760425745151394
Validation loss: 2.4877373583623714

Epoch: 6| Step: 1
Training loss: 2.71065324068036
Validation loss: 2.5562976696219932

Epoch: 6| Step: 2
Training loss: 2.5641754999546875
Validation loss: 2.546730682634663

Epoch: 6| Step: 3
Training loss: 2.50889977388674
Validation loss: 2.5384948449307037

Epoch: 6| Step: 4
Training loss: 1.977080627804008
Validation loss: 2.494814901898607

Epoch: 6| Step: 5
Training loss: 2.920581878271936
Validation loss: 2.5478788514230897

Epoch: 6| Step: 6
Training loss: 2.839935091378355
Validation loss: 2.5169005082899667

Epoch: 6| Step: 7
Training loss: 2.717927742627064
Validation loss: 2.5266558420744496

Epoch: 6| Step: 8
Training loss: 2.0887864905593654
Validation loss: 2.5308060129442853

Epoch: 6| Step: 9
Training loss: 2.7782223112658624
Validation loss: 2.511240641191634

Epoch: 6| Step: 10
Training loss: 1.7459195111960903
Validation loss: 2.536617590323169

Epoch: 6| Step: 11
Training loss: 1.9892548760985629
Validation loss: 2.5302319329595226

Epoch: 6| Step: 12
Training loss: 2.434953092557999
Validation loss: 2.509333317592482

Epoch: 6| Step: 13
Training loss: 3.668610635479183
Validation loss: 2.515737918415492

Epoch: 259| Step: 0
Training loss: 1.8366286255254123
Validation loss: 2.533717574319793

Epoch: 6| Step: 1
Training loss: 2.7707744008922215
Validation loss: 2.5290458458021035

Epoch: 6| Step: 2
Training loss: 2.352917860420491
Validation loss: 2.5656184468240553

Epoch: 6| Step: 3
Training loss: 2.443535985136445
Validation loss: 2.5699277025551392

Epoch: 6| Step: 4
Training loss: 2.8219955367898555
Validation loss: 2.551912190414446

Epoch: 6| Step: 5
Training loss: 2.938283288291137
Validation loss: 2.5553044242872587

Epoch: 6| Step: 6
Training loss: 2.6916974135808944
Validation loss: 2.5089729810174513

Epoch: 6| Step: 7
Training loss: 2.2731801310103035
Validation loss: 2.5473661831735757

Epoch: 6| Step: 8
Training loss: 2.7459649046275683
Validation loss: 2.573453867958159

Epoch: 6| Step: 9
Training loss: 2.8120655995906936
Validation loss: 2.523355505852616

Epoch: 6| Step: 10
Training loss: 1.814216655847939
Validation loss: 2.531769799777852

Epoch: 6| Step: 11
Training loss: 2.6294886948523284
Validation loss: 2.576863673105768

Epoch: 6| Step: 12
Training loss: 2.908490373658147
Validation loss: 2.554922438723522

Epoch: 6| Step: 13
Training loss: 1.8161028865035305
Validation loss: 2.534444998685162

Epoch: 260| Step: 0
Training loss: 1.9362140202488598
Validation loss: 2.55133177435472

Epoch: 6| Step: 1
Training loss: 2.4532501771904074
Validation loss: 2.5656472324547526

Epoch: 6| Step: 2
Training loss: 2.2107187978459435
Validation loss: 2.5299375152783625

Epoch: 6| Step: 3
Training loss: 2.6514361574535417
Validation loss: 2.5670260883958274

Epoch: 6| Step: 4
Training loss: 2.0596506492896167
Validation loss: 2.490880148485459

Epoch: 6| Step: 5
Training loss: 2.7161707427411748
Validation loss: 2.5492506434598265

Epoch: 6| Step: 6
Training loss: 2.47588312596091
Validation loss: 2.5347125335057177

Epoch: 6| Step: 7
Training loss: 2.6067486183116744
Validation loss: 2.5627057172755676

Epoch: 6| Step: 8
Training loss: 2.3370990225658126
Validation loss: 2.5214579101784143

Epoch: 6| Step: 9
Training loss: 2.9611648834607327
Validation loss: 2.5553995518494075

Epoch: 6| Step: 10
Training loss: 2.494388672541015
Validation loss: 2.542979734550256

Epoch: 6| Step: 11
Training loss: 2.2065296244444634
Validation loss: 2.578319233239731

Epoch: 6| Step: 12
Training loss: 2.7026446141624736
Validation loss: 2.545420767184176

Epoch: 6| Step: 13
Training loss: 3.486838799695718
Validation loss: 2.5242433944061102

Epoch: 261| Step: 0
Training loss: 2.4694751688482524
Validation loss: 2.515510511127404

Epoch: 6| Step: 1
Training loss: 2.3640303000042913
Validation loss: 2.533552387926415

Epoch: 6| Step: 2
Training loss: 2.617352563008452
Validation loss: 2.512871626187777

Epoch: 6| Step: 3
Training loss: 2.7362329764640396
Validation loss: 2.522724215001316

Epoch: 6| Step: 4
Training loss: 2.5145877571562187
Validation loss: 2.5540240652082975

Epoch: 6| Step: 5
Training loss: 2.7651451254866246
Validation loss: 2.527841892471311

Epoch: 6| Step: 6
Training loss: 2.319121700631795
Validation loss: 2.5243808186157946

Epoch: 6| Step: 7
Training loss: 3.1435116080265018
Validation loss: 2.5591701143640733

Epoch: 6| Step: 8
Training loss: 2.1523183831521844
Validation loss: 2.5274779951733937

Epoch: 6| Step: 9
Training loss: 2.734550949975197
Validation loss: 2.527289105745916

Epoch: 6| Step: 10
Training loss: 1.8246735163206895
Validation loss: 2.5514812858476086

Epoch: 6| Step: 11
Training loss: 2.6168724667555323
Validation loss: 2.5263379928652645

Epoch: 6| Step: 12
Training loss: 2.3441554418196326
Validation loss: 2.549860659971954

Epoch: 6| Step: 13
Training loss: 2.2220253208135747
Validation loss: 2.563110224880043

Epoch: 262| Step: 0
Training loss: 2.613684470597172
Validation loss: 2.533601960254291

Epoch: 6| Step: 1
Training loss: 2.4430345156179705
Validation loss: 2.5689648041338753

Epoch: 6| Step: 2
Training loss: 2.260232756790363
Validation loss: 2.532470047528874

Epoch: 6| Step: 3
Training loss: 2.4756417954743597
Validation loss: 2.5177663296810997

Epoch: 6| Step: 4
Training loss: 2.1345554258790993
Validation loss: 2.561593234191888

Epoch: 6| Step: 5
Training loss: 3.0020627241907496
Validation loss: 2.513790979742853

Epoch: 6| Step: 6
Training loss: 2.3288953901803575
Validation loss: 2.54083739688019

Epoch: 6| Step: 7
Training loss: 2.9702217068107664
Validation loss: 2.5325038381569582

Epoch: 6| Step: 8
Training loss: 2.589078073679209
Validation loss: 2.506700309695322

Epoch: 6| Step: 9
Training loss: 2.6321734549926545
Validation loss: 2.5441862653110277

Epoch: 6| Step: 10
Training loss: 2.0169777286115096
Validation loss: 2.5520189265013102

Epoch: 6| Step: 11
Training loss: 2.414407847371377
Validation loss: 2.497784115524082

Epoch: 6| Step: 12
Training loss: 3.2560185677844844
Validation loss: 2.562119450938549

Epoch: 6| Step: 13
Training loss: 1.358160682346406
Validation loss: 2.530646867702547

Epoch: 263| Step: 0
Training loss: 2.265246760924385
Validation loss: 2.5078115809865755

Epoch: 6| Step: 1
Training loss: 2.5464208467122154
Validation loss: 2.531828441128552

Epoch: 6| Step: 2
Training loss: 2.2595324449567022
Validation loss: 2.5372323507332273

Epoch: 6| Step: 3
Training loss: 1.7735480051440087
Validation loss: 2.5353084829267396

Epoch: 6| Step: 4
Training loss: 2.777021987668646
Validation loss: 2.5020368738353858

Epoch: 6| Step: 5
Training loss: 3.021437029970047
Validation loss: 2.5414236040928375

Epoch: 6| Step: 6
Training loss: 2.602906209445215
Validation loss: 2.5806037946303024

Epoch: 6| Step: 7
Training loss: 2.211574503116934
Validation loss: 2.499856296634055

Epoch: 6| Step: 8
Training loss: 2.827975443052211
Validation loss: 2.517688783379002

Epoch: 6| Step: 9
Training loss: 2.8217575302260127
Validation loss: 2.5995724192354293

Epoch: 6| Step: 10
Training loss: 2.8739144307500593
Validation loss: 2.543633998912632

Epoch: 6| Step: 11
Training loss: 1.5972420824815656
Validation loss: 2.5012933615698003

Epoch: 6| Step: 12
Training loss: 2.8939803161584394
Validation loss: 2.5560837188810095

Epoch: 6| Step: 13
Training loss: 2.230320272497611
Validation loss: 2.5827549383811195

Epoch: 264| Step: 0
Training loss: 2.5386470513480917
Validation loss: 2.5793408909750988

Epoch: 6| Step: 1
Training loss: 2.7614148524451534
Validation loss: 2.5153973399387612

Epoch: 6| Step: 2
Training loss: 3.004140698395199
Validation loss: 2.555877357277432

Epoch: 6| Step: 3
Training loss: 2.523380999331184
Validation loss: 2.5844424252275915

Epoch: 6| Step: 4
Training loss: 2.293864866285779
Validation loss: 2.548613006537295

Epoch: 6| Step: 5
Training loss: 1.5302221001130647
Validation loss: 2.5586619924365057

Epoch: 6| Step: 6
Training loss: 2.890664920015734
Validation loss: 2.5414207362378183

Epoch: 6| Step: 7
Training loss: 2.706573579750334
Validation loss: 2.5610149671519196

Epoch: 6| Step: 8
Training loss: 2.250229505913712
Validation loss: 2.5211017953502224

Epoch: 6| Step: 9
Training loss: 1.7969022002441957
Validation loss: 2.552201556911503

Epoch: 6| Step: 10
Training loss: 2.9430176878957233
Validation loss: 2.5161816691822616

Epoch: 6| Step: 11
Training loss: 2.145487365486319
Validation loss: 2.535269259143846

Epoch: 6| Step: 12
Training loss: 2.6196155729070045
Validation loss: 2.500371793562576

Epoch: 6| Step: 13
Training loss: 2.9595979664966348
Validation loss: 2.514992734024824

Epoch: 265| Step: 0
Training loss: 1.6086708676572776
Validation loss: 2.5300262249731174

Epoch: 6| Step: 1
Training loss: 2.820012380423999
Validation loss: 2.5197929544664346

Epoch: 6| Step: 2
Training loss: 1.8615490040811005
Validation loss: 2.5286932063916794

Epoch: 6| Step: 3
Training loss: 3.00763240126396
Validation loss: 2.5413871811857742

Epoch: 6| Step: 4
Training loss: 2.553958333798022
Validation loss: 2.5313170806613114

Epoch: 6| Step: 5
Training loss: 2.1898666658267394
Validation loss: 2.54691913022194

Epoch: 6| Step: 6
Training loss: 2.5517559438378155
Validation loss: 2.507616159801413

Epoch: 6| Step: 7
Training loss: 2.140009436541991
Validation loss: 2.5378777639472934

Epoch: 6| Step: 8
Training loss: 2.6889637354302462
Validation loss: 2.5580332513078536

Epoch: 6| Step: 9
Training loss: 2.3637959069646253
Validation loss: 2.5782916589729927

Epoch: 6| Step: 10
Training loss: 2.7572652906062527
Validation loss: 2.5516575970288966

Epoch: 6| Step: 11
Training loss: 1.817187332061471
Validation loss: 2.521588238822751

Epoch: 6| Step: 12
Training loss: 3.5580427159816392
Validation loss: 2.5664874031687903

Epoch: 6| Step: 13
Training loss: 2.9980630979813516
Validation loss: 2.5316485586576443

Epoch: 266| Step: 0
Training loss: 2.588871792185173
Validation loss: 2.556175087582938

Epoch: 6| Step: 1
Training loss: 2.4814041896927055
Validation loss: 2.5066620475803503

Epoch: 6| Step: 2
Training loss: 2.540958952216778
Validation loss: 2.549669083057492

Epoch: 6| Step: 3
Training loss: 1.8794436251382034
Validation loss: 2.521588960663281

Epoch: 6| Step: 4
Training loss: 2.2148590356477604
Validation loss: 2.481638283256469

Epoch: 6| Step: 5
Training loss: 3.033913454887203
Validation loss: 2.5607169195675925

Epoch: 6| Step: 6
Training loss: 2.1652349607964774
Validation loss: 2.537413361955202

Epoch: 6| Step: 7
Training loss: 2.7267018804475907
Validation loss: 2.5176490951826427

Epoch: 6| Step: 8
Training loss: 2.8053295707544446
Validation loss: 2.526570997140731

Epoch: 6| Step: 9
Training loss: 2.7675400477598493
Validation loss: 2.552622829033532

Epoch: 6| Step: 10
Training loss: 2.5953955887935662
Validation loss: 2.597940255941434

Epoch: 6| Step: 11
Training loss: 2.397358903442299
Validation loss: 2.5753161609711435

Epoch: 6| Step: 12
Training loss: 2.613225232081312
Validation loss: 2.556545310973584

Epoch: 6| Step: 13
Training loss: 2.4139917850939296
Validation loss: 2.527903088280758

Epoch: 267| Step: 0
Training loss: 2.6962135467399504
Validation loss: 2.527440342773229

Epoch: 6| Step: 1
Training loss: 2.5292244801415444
Validation loss: 2.5346939659220005

Epoch: 6| Step: 2
Training loss: 2.3630130836360297
Validation loss: 2.542868084306988

Epoch: 6| Step: 3
Training loss: 2.3500559089990056
Validation loss: 2.5226972144223487

Epoch: 6| Step: 4
Training loss: 2.1630397541980053
Validation loss: 2.5312500617805735

Epoch: 6| Step: 5
Training loss: 2.4868152078370613
Validation loss: 2.521900113656958

Epoch: 6| Step: 6
Training loss: 2.6357209302646414
Validation loss: 2.5410173547651893

Epoch: 6| Step: 7
Training loss: 3.248996432954351
Validation loss: 2.5399377235430687

Epoch: 6| Step: 8
Training loss: 2.478161894053419
Validation loss: 2.499409594602419

Epoch: 6| Step: 9
Training loss: 2.502770986299642
Validation loss: 2.5356292943105143

Epoch: 6| Step: 10
Training loss: 2.277923143860371
Validation loss: 2.517897850008898

Epoch: 6| Step: 11
Training loss: 3.0426844805642603
Validation loss: 2.57258676246669

Epoch: 6| Step: 12
Training loss: 1.871679799596287
Validation loss: 2.5317980528606956

Epoch: 6| Step: 13
Training loss: 1.9682314961979783
Validation loss: 2.5337799364581985

Epoch: 268| Step: 0
Training loss: 2.6298306839256673
Validation loss: 2.533448127146455

Epoch: 6| Step: 1
Training loss: 2.061586004091123
Validation loss: 2.513158729761577

Epoch: 6| Step: 2
Training loss: 1.8953817759005744
Validation loss: 2.5345279596311174

Epoch: 6| Step: 3
Training loss: 2.928857792666883
Validation loss: 2.543621130454966

Epoch: 6| Step: 4
Training loss: 2.64391938516873
Validation loss: 2.5316244193051998

Epoch: 6| Step: 5
Training loss: 2.3790152889624707
Validation loss: 2.5044388690188146

Epoch: 6| Step: 6
Training loss: 2.608717235876716
Validation loss: 2.514373908440809

Epoch: 6| Step: 7
Training loss: 2.9748176005891356
Validation loss: 2.525548369728499

Epoch: 6| Step: 8
Training loss: 2.287849599670059
Validation loss: 2.5129388322202773

Epoch: 6| Step: 9
Training loss: 2.7596036373472206
Validation loss: 2.5287791561134685

Epoch: 6| Step: 10
Training loss: 2.5554011570088537
Validation loss: 2.5366026427185875

Epoch: 6| Step: 11
Training loss: 2.624461345810194
Validation loss: 2.5618137418126614

Epoch: 6| Step: 12
Training loss: 2.091819898405626
Validation loss: 2.516378209569297

Epoch: 6| Step: 13
Training loss: 2.5443618638391308
Validation loss: 2.552058864239301

Epoch: 269| Step: 0
Training loss: 2.288665841272555
Validation loss: 2.5322719229316415

Epoch: 6| Step: 1
Training loss: 2.5915350591314636
Validation loss: 2.5435152292839316

Epoch: 6| Step: 2
Training loss: 2.52101817648727
Validation loss: 2.51983819891359

Epoch: 6| Step: 3
Training loss: 2.1770714815781496
Validation loss: 2.5123571419848707

Epoch: 6| Step: 4
Training loss: 3.129285086987066
Validation loss: 2.5644097343844994

Epoch: 6| Step: 5
Training loss: 2.596544162503228
Validation loss: 2.5182457252598804

Epoch: 6| Step: 6
Training loss: 2.136738165116889
Validation loss: 2.4899912447774843

Epoch: 6| Step: 7
Training loss: 2.356554986051188
Validation loss: 2.5329672051227283

Epoch: 6| Step: 8
Training loss: 2.153733595720852
Validation loss: 2.5149909175546608

Epoch: 6| Step: 9
Training loss: 3.0069287079205256
Validation loss: 2.522328137903684

Epoch: 6| Step: 10
Training loss: 2.48334870643039
Validation loss: 2.566340216385568

Epoch: 6| Step: 11
Training loss: 2.31789130585181
Validation loss: 2.5239464087191856

Epoch: 6| Step: 12
Training loss: 1.6062285395869627
Validation loss: 2.5542197829015088

Epoch: 6| Step: 13
Training loss: 3.638798857360573
Validation loss: 2.527516022300917

Epoch: 270| Step: 0
Training loss: 2.4875874412925008
Validation loss: 2.540218887712951

Epoch: 6| Step: 1
Training loss: 2.8767537282924707
Validation loss: 2.4811671716738113

Epoch: 6| Step: 2
Training loss: 3.4795986676364534
Validation loss: 2.544272516382534

Epoch: 6| Step: 3
Training loss: 2.2567776218936983
Validation loss: 2.5775264827087527

Epoch: 6| Step: 4
Training loss: 2.466951125299052
Validation loss: 2.521416607746103

Epoch: 6| Step: 5
Training loss: 2.1907057024882532
Validation loss: 2.5421218894764466

Epoch: 6| Step: 6
Training loss: 2.3988524832806513
Validation loss: 2.4953369624350517

Epoch: 6| Step: 7
Training loss: 2.3504937363447596
Validation loss: 2.5190651317059456

Epoch: 6| Step: 8
Training loss: 2.643526638686855
Validation loss: 2.5215071443153207

Epoch: 6| Step: 9
Training loss: 2.430361344586189
Validation loss: 2.518914211120032

Epoch: 6| Step: 10
Training loss: 2.0064293516807745
Validation loss: 2.5014302735892957

Epoch: 6| Step: 11
Training loss: 2.8836185806681183
Validation loss: 2.5153875211472347

Epoch: 6| Step: 12
Training loss: 2.555542222508834
Validation loss: 2.5258688952714627

Epoch: 6| Step: 13
Training loss: 1.4513581722402014
Validation loss: 2.533704221428784

Epoch: 271| Step: 0
Training loss: 2.1814305708943214
Validation loss: 2.534872729147034

Epoch: 6| Step: 1
Training loss: 2.1599788765404235
Validation loss: 2.5600940455398167

Epoch: 6| Step: 2
Training loss: 2.091203193938801
Validation loss: 2.4912547489151846

Epoch: 6| Step: 3
Training loss: 2.5931347956165536
Validation loss: 2.51968113042533

Epoch: 6| Step: 4
Training loss: 2.683486603176224
Validation loss: 2.5341911269346302

Epoch: 6| Step: 5
Training loss: 2.645717968766033
Validation loss: 2.53004220444588

Epoch: 6| Step: 6
Training loss: 2.49253742308535
Validation loss: 2.5318504450809756

Epoch: 6| Step: 7
Training loss: 2.5196298506021493
Validation loss: 2.5097405008927716

Epoch: 6| Step: 8
Training loss: 2.8101737680703107
Validation loss: 2.533966808287636

Epoch: 6| Step: 9
Training loss: 2.351872420561727
Validation loss: 2.5291002615321654

Epoch: 6| Step: 10
Training loss: 3.0234961688832933
Validation loss: 2.5340011839512733

Epoch: 6| Step: 11
Training loss: 3.169317708521635
Validation loss: 2.5480246426316073

Epoch: 6| Step: 12
Training loss: 2.103411573636484
Validation loss: 2.523571281476904

Epoch: 6| Step: 13
Training loss: 2.2544128800592085
Validation loss: 2.5608586663046475

Epoch: 272| Step: 0
Training loss: 2.614970255841226
Validation loss: 2.5511543081400028

Epoch: 6| Step: 1
Training loss: 2.2784421179398535
Validation loss: 2.5452663390074375

Epoch: 6| Step: 2
Training loss: 2.3332310949360995
Validation loss: 2.530080875973548

Epoch: 6| Step: 3
Training loss: 2.4078171816018528
Validation loss: 2.5278079918581424

Epoch: 6| Step: 4
Training loss: 2.1286871841131605
Validation loss: 2.5273288002934424

Epoch: 6| Step: 5
Training loss: 2.277817953215472
Validation loss: 2.5317587443139735

Epoch: 6| Step: 6
Training loss: 2.2749800544954346
Validation loss: 2.554192758449763

Epoch: 6| Step: 7
Training loss: 2.796670405400914
Validation loss: 2.5201365542602723

Epoch: 6| Step: 8
Training loss: 2.2654500959562243
Validation loss: 2.535380271292928

Epoch: 6| Step: 9
Training loss: 3.1111500336467786
Validation loss: 2.5424225333997637

Epoch: 6| Step: 10
Training loss: 2.6187262267715674
Validation loss: 2.5094285410053816

Epoch: 6| Step: 11
Training loss: 2.3902430011003237
Validation loss: 2.5188140410510895

Epoch: 6| Step: 12
Training loss: 2.1305208855164564
Validation loss: 2.4763660062465807

Epoch: 6| Step: 13
Training loss: 3.0643409334047895
Validation loss: 2.5312937362175902

Epoch: 273| Step: 0
Training loss: 2.0303781839261332
Validation loss: 2.5404273679378653

Epoch: 6| Step: 1
Training loss: 2.0725095211680045
Validation loss: 2.5464109371311237

Epoch: 6| Step: 2
Training loss: 2.503743420809835
Validation loss: 2.5457593974746966

Epoch: 6| Step: 3
Training loss: 2.3727015365685
Validation loss: 2.5446920566499127

Epoch: 6| Step: 4
Training loss: 3.0421553615445767
Validation loss: 2.5265755596088835

Epoch: 6| Step: 5
Training loss: 3.293106021938637
Validation loss: 2.5590351830175426

Epoch: 6| Step: 6
Training loss: 2.3152730656315077
Validation loss: 2.5355020043302554

Epoch: 6| Step: 7
Training loss: 2.483017652617075
Validation loss: 2.5289162223342774

Epoch: 6| Step: 8
Training loss: 2.316294263734843
Validation loss: 2.532592192748549

Epoch: 6| Step: 9
Training loss: 2.484569613913411
Validation loss: 2.536397972954501

Epoch: 6| Step: 10
Training loss: 2.240365594082293
Validation loss: 2.540980765112492

Epoch: 6| Step: 11
Training loss: 2.406262088101232
Validation loss: 2.539583382891724

Epoch: 6| Step: 12
Training loss: 2.4214313716044353
Validation loss: 2.4896950607640025

Epoch: 6| Step: 13
Training loss: 2.920657307050009
Validation loss: 2.528921539347502

Epoch: 274| Step: 0
Training loss: 2.289404313016701
Validation loss: 2.555175696470065

Epoch: 6| Step: 1
Training loss: 2.620400713787468
Validation loss: 2.509073501569804

Epoch: 6| Step: 2
Training loss: 1.9926295369733562
Validation loss: 2.5044424179717937

Epoch: 6| Step: 3
Training loss: 2.443972576961472
Validation loss: 2.5302176589039798

Epoch: 6| Step: 4
Training loss: 2.7224769040643966
Validation loss: 2.5250510174968106

Epoch: 6| Step: 5
Training loss: 2.981444192955489
Validation loss: 2.5350911988976064

Epoch: 6| Step: 6
Training loss: 2.412413592594736
Validation loss: 2.535125164422418

Epoch: 6| Step: 7
Training loss: 2.311340556869611
Validation loss: 2.561543252787995

Epoch: 6| Step: 8
Training loss: 2.5902534724627286
Validation loss: 2.539940953403683

Epoch: 6| Step: 9
Training loss: 2.309594674818654
Validation loss: 2.546349880052041

Epoch: 6| Step: 10
Training loss: 1.6135344595242282
Validation loss: 2.512174778468594

Epoch: 6| Step: 11
Training loss: 2.418527369147379
Validation loss: 2.52873557105474

Epoch: 6| Step: 12
Training loss: 3.072173896288367
Validation loss: 2.533605630253761

Epoch: 6| Step: 13
Training loss: 3.417591357513458
Validation loss: 2.5526241959076077

Epoch: 275| Step: 0
Training loss: 2.738008972649261
Validation loss: 2.577318155638645

Epoch: 6| Step: 1
Training loss: 2.4696725015089496
Validation loss: 2.5209592094374353

Epoch: 6| Step: 2
Training loss: 2.826821379776087
Validation loss: 2.5483832288216597

Epoch: 6| Step: 3
Training loss: 2.189218990115454
Validation loss: 2.534089693726387

Epoch: 6| Step: 4
Training loss: 2.610279314873037
Validation loss: 2.512668550539791

Epoch: 6| Step: 5
Training loss: 2.5632358517938583
Validation loss: 2.5225220798997823

Epoch: 6| Step: 6
Training loss: 3.0653629544842826
Validation loss: 2.522536017407338

Epoch: 6| Step: 7
Training loss: 2.5401600504934336
Validation loss: 2.542457155719363

Epoch: 6| Step: 8
Training loss: 1.67812484087668
Validation loss: 2.501576526449799

Epoch: 6| Step: 9
Training loss: 2.672929165444967
Validation loss: 2.5680670793479012

Epoch: 6| Step: 10
Training loss: 2.1452848850815083
Validation loss: 2.4960871275016476

Epoch: 6| Step: 11
Training loss: 2.3249274703987535
Validation loss: 2.5242684178363675

Epoch: 6| Step: 12
Training loss: 2.8800695752640797
Validation loss: 2.534593200171873

Epoch: 6| Step: 13
Training loss: 1.956606577102726
Validation loss: 2.5401803654794883

Epoch: 276| Step: 0
Training loss: 2.7979548410257506
Validation loss: 2.54394583299871

Epoch: 6| Step: 1
Training loss: 1.8283143556781232
Validation loss: 2.5097209487372676

Epoch: 6| Step: 2
Training loss: 2.3550871228343415
Validation loss: 2.5435198041943727

Epoch: 6| Step: 3
Training loss: 2.1453746811648093
Validation loss: 2.5281855284824473

Epoch: 6| Step: 4
Training loss: 2.6307092070113427
Validation loss: 2.5393831924848618

Epoch: 6| Step: 5
Training loss: 2.0502419770875377
Validation loss: 2.543389270371466

Epoch: 6| Step: 6
Training loss: 2.011082935779865
Validation loss: 2.5517877058856215

Epoch: 6| Step: 7
Training loss: 2.5604305750130525
Validation loss: 2.5471686415328763

Epoch: 6| Step: 8
Training loss: 3.2911839835460133
Validation loss: 2.560278683322326

Epoch: 6| Step: 9
Training loss: 2.1607795354185475
Validation loss: 2.5349803252796725

Epoch: 6| Step: 10
Training loss: 3.1791868823314062
Validation loss: 2.5383697003802146

Epoch: 6| Step: 11
Training loss: 1.8228055066321653
Validation loss: 2.5931875461893155

Epoch: 6| Step: 12
Training loss: 2.6482282797845405
Validation loss: 2.5339202529930698

Epoch: 6| Step: 13
Training loss: 3.04042881551306
Validation loss: 2.540673920568166

Epoch: 277| Step: 0
Training loss: 2.1454542498007925
Validation loss: 2.5539025043346184

Epoch: 6| Step: 1
Training loss: 2.894462883228122
Validation loss: 2.516696942815604

Epoch: 6| Step: 2
Training loss: 1.785248868780842
Validation loss: 2.5398055440300116

Epoch: 6| Step: 3
Training loss: 2.9406419548818796
Validation loss: 2.5449885227711673

Epoch: 6| Step: 4
Training loss: 3.2200753159333404
Validation loss: 2.533187251488336

Epoch: 6| Step: 5
Training loss: 2.212737735540613
Validation loss: 2.5271827453867934

Epoch: 6| Step: 6
Training loss: 2.8211966103643067
Validation loss: 2.5265821498824845

Epoch: 6| Step: 7
Training loss: 2.3014746043817773
Validation loss: 2.537558305701831

Epoch: 6| Step: 8
Training loss: 2.076143854908521
Validation loss: 2.571230029939491

Epoch: 6| Step: 9
Training loss: 2.1528005236446064
Validation loss: 2.5397007536140386

Epoch: 6| Step: 10
Training loss: 2.4880568374691627
Validation loss: 2.5328705306093644

Epoch: 6| Step: 11
Training loss: 2.1384066482586164
Validation loss: 2.517259029760445

Epoch: 6| Step: 12
Training loss: 2.926737609936113
Validation loss: 2.5368438594146485

Epoch: 6| Step: 13
Training loss: 2.1966894860980557
Validation loss: 2.5576588550621535

Epoch: 278| Step: 0
Training loss: 2.8751000096714536
Validation loss: 2.5636525117228275

Epoch: 6| Step: 1
Training loss: 2.2332380910274257
Validation loss: 2.5342226711003124

Epoch: 6| Step: 2
Training loss: 2.4095041638694767
Validation loss: 2.5226701556286577

Epoch: 6| Step: 3
Training loss: 2.652666524536993
Validation loss: 2.526058641906744

Epoch: 6| Step: 4
Training loss: 1.9139515747314169
Validation loss: 2.5389583937992213

Epoch: 6| Step: 5
Training loss: 2.3273081530575896
Validation loss: 2.5188162440710977

Epoch: 6| Step: 6
Training loss: 2.2994788325746205
Validation loss: 2.5672211483857486

Epoch: 6| Step: 7
Training loss: 2.736188886368631
Validation loss: 2.5537177709587917

Epoch: 6| Step: 8
Training loss: 2.229748290529097
Validation loss: 2.508471814032316

Epoch: 6| Step: 9
Training loss: 2.714619271857049
Validation loss: 2.5144675054310386

Epoch: 6| Step: 10
Training loss: 2.8054424321834532
Validation loss: 2.5106093225058537

Epoch: 6| Step: 11
Training loss: 2.0411297487905333
Validation loss: 2.527986857670392

Epoch: 6| Step: 12
Training loss: 2.4318510278575647
Validation loss: 2.530683809878038

Epoch: 6| Step: 13
Training loss: 3.5312031430536828
Validation loss: 2.513754604166476

Epoch: 279| Step: 0
Training loss: 2.269157024264527
Validation loss: 2.5645734600382464

Epoch: 6| Step: 1
Training loss: 2.9423936703769957
Validation loss: 2.5275035743651184

Epoch: 6| Step: 2
Training loss: 2.4682239986144747
Validation loss: 2.546004980639603

Epoch: 6| Step: 3
Training loss: 2.50692077184075
Validation loss: 2.5106814658434193

Epoch: 6| Step: 4
Training loss: 1.9875441348375753
Validation loss: 2.5000403636832327

Epoch: 6| Step: 5
Training loss: 2.8407441067039017
Validation loss: 2.5345115977738266

Epoch: 6| Step: 6
Training loss: 2.8278078354506055
Validation loss: 2.5350073027331144

Epoch: 6| Step: 7
Training loss: 2.4017001646813187
Validation loss: 2.5226398369149083

Epoch: 6| Step: 8
Training loss: 3.20972131868228
Validation loss: 2.589269021905518

Epoch: 6| Step: 9
Training loss: 2.0350525695984003
Validation loss: 2.532235401798842

Epoch: 6| Step: 10
Training loss: 2.2794765738262277
Validation loss: 2.528465872811433

Epoch: 6| Step: 11
Training loss: 2.0195305416615055
Validation loss: 2.5004961998022464

Epoch: 6| Step: 12
Training loss: 2.1906717466843797
Validation loss: 2.5517218435265865

Epoch: 6| Step: 13
Training loss: 2.766575364440909
Validation loss: 2.525313210796381

Epoch: 280| Step: 0
Training loss: 2.598735659634509
Validation loss: 2.5518712919812456

Epoch: 6| Step: 1
Training loss: 2.042139647316472
Validation loss: 2.518056849846552

Epoch: 6| Step: 2
Training loss: 2.542141593795289
Validation loss: 2.5136064693221383

Epoch: 6| Step: 3
Training loss: 2.440193644171939
Validation loss: 2.5331884031688436

Epoch: 6| Step: 4
Training loss: 2.472051802725106
Validation loss: 2.509440234250225

Epoch: 6| Step: 5
Training loss: 1.8365615108916375
Validation loss: 2.4848538984103263

Epoch: 6| Step: 6
Training loss: 2.148841847603202
Validation loss: 2.5279625058827637

Epoch: 6| Step: 7
Training loss: 2.591541131056955
Validation loss: 2.5191766788132997

Epoch: 6| Step: 8
Training loss: 2.928308269094573
Validation loss: 2.5154686111493336

Epoch: 6| Step: 9
Training loss: 2.755979106851564
Validation loss: 2.540919201213567

Epoch: 6| Step: 10
Training loss: 2.52159878377401
Validation loss: 2.55085784707152

Epoch: 6| Step: 11
Training loss: 2.9287140309774427
Validation loss: 2.5465009263506455

Epoch: 6| Step: 12
Training loss: 2.789780126405399
Validation loss: 2.492407974014879

Epoch: 6| Step: 13
Training loss: 1.4463643963014396
Validation loss: 2.534352743389616

Epoch: 281| Step: 0
Training loss: 2.173057947536897
Validation loss: 2.5240736573196565

Epoch: 6| Step: 1
Training loss: 2.7379108347114895
Validation loss: 2.570396744370177

Epoch: 6| Step: 2
Training loss: 2.7914969454363754
Validation loss: 2.5509036148697253

Epoch: 6| Step: 3
Training loss: 2.7696801752206435
Validation loss: 2.54258087704897

Epoch: 6| Step: 4
Training loss: 2.8450964789633013
Validation loss: 2.5856900174950472

Epoch: 6| Step: 5
Training loss: 2.288216183806287
Validation loss: 2.5589234127914158

Epoch: 6| Step: 6
Training loss: 1.7994104824032617
Validation loss: 2.5678604637693008

Epoch: 6| Step: 7
Training loss: 2.713114543157364
Validation loss: 2.498118386713903

Epoch: 6| Step: 8
Training loss: 2.2981765069861764
Validation loss: 2.5430612617985826

Epoch: 6| Step: 9
Training loss: 2.315112931728756
Validation loss: 2.523184225628046

Epoch: 6| Step: 10
Training loss: 2.9180550313548963
Validation loss: 2.4882875955216495

Epoch: 6| Step: 11
Training loss: 2.7474087731416743
Validation loss: 2.5389157983444637

Epoch: 6| Step: 12
Training loss: 2.288691572009188
Validation loss: 2.514705821488743

Epoch: 6| Step: 13
Training loss: 1.6232118304694332
Validation loss: 2.5126372653142997

Epoch: 282| Step: 0
Training loss: 2.1807510898442612
Validation loss: 2.519428962016892

Epoch: 6| Step: 1
Training loss: 2.8480128940118945
Validation loss: 2.564883388531227

Epoch: 6| Step: 2
Training loss: 2.532154439714802
Validation loss: 2.5447624429281652

Epoch: 6| Step: 3
Training loss: 2.1014898918683858
Validation loss: 2.5498975329046507

Epoch: 6| Step: 4
Training loss: 2.80522902847364
Validation loss: 2.5445975866553794

Epoch: 6| Step: 5
Training loss: 2.0198490328132066
Validation loss: 2.5498972101745467

Epoch: 6| Step: 6
Training loss: 2.270409524082265
Validation loss: 2.5248713994749488

Epoch: 6| Step: 7
Training loss: 2.561195041258607
Validation loss: 2.5820609458012598

Epoch: 6| Step: 8
Training loss: 2.0320082203030454
Validation loss: 2.5325757839680727

Epoch: 6| Step: 9
Training loss: 2.1962934047033555
Validation loss: 2.520478432009832

Epoch: 6| Step: 10
Training loss: 2.0325730929774455
Validation loss: 2.5378488966881445

Epoch: 6| Step: 11
Training loss: 3.2543824798750043
Validation loss: 2.5533617356605394

Epoch: 6| Step: 12
Training loss: 3.0526250330306137
Validation loss: 2.5463125097314405

Epoch: 6| Step: 13
Training loss: 2.611940413416065
Validation loss: 2.541684769099132

Epoch: 283| Step: 0
Training loss: 2.3618524945719455
Validation loss: 2.5755271802465036

Epoch: 6| Step: 1
Training loss: 2.4289475077661513
Validation loss: 2.523602102012412

Epoch: 6| Step: 2
Training loss: 1.8550376632239491
Validation loss: 2.5109183192722573

Epoch: 6| Step: 3
Training loss: 2.6914098819879793
Validation loss: 2.5475740543496768

Epoch: 6| Step: 4
Training loss: 2.1203472587193337
Validation loss: 2.5204143737051603

Epoch: 6| Step: 5
Training loss: 2.4631543059929957
Validation loss: 2.5040732395186622

Epoch: 6| Step: 6
Training loss: 2.453597491009514
Validation loss: 2.569878904762324

Epoch: 6| Step: 7
Training loss: 2.568763335627261
Validation loss: 2.5163474594865827

Epoch: 6| Step: 8
Training loss: 2.772656053944546
Validation loss: 2.5192827971404377

Epoch: 6| Step: 9
Training loss: 2.952999859131352
Validation loss: 2.4927437554620377

Epoch: 6| Step: 10
Training loss: 2.0649632858448617
Validation loss: 2.5567132743393493

Epoch: 6| Step: 11
Training loss: 2.4624387007311266
Validation loss: 2.5698657796668525

Epoch: 6| Step: 12
Training loss: 2.764843242954575
Validation loss: 2.512270995243731

Epoch: 6| Step: 13
Training loss: 2.8374512050651175
Validation loss: 2.530806646052943

Epoch: 284| Step: 0
Training loss: 2.8979689227031273
Validation loss: 2.543346399355322

Epoch: 6| Step: 1
Training loss: 2.5252656249274907
Validation loss: 2.5303029980767975

Epoch: 6| Step: 2
Training loss: 2.694939540545811
Validation loss: 2.5448825606395578

Epoch: 6| Step: 3
Training loss: 2.3789308042464836
Validation loss: 2.5707763319194163

Epoch: 6| Step: 4
Training loss: 2.2303629247142585
Validation loss: 2.5376017447804524

Epoch: 6| Step: 5
Training loss: 1.9788427656723164
Validation loss: 2.519992287036885

Epoch: 6| Step: 6
Training loss: 2.6213469745414284
Validation loss: 2.5289992637825405

Epoch: 6| Step: 7
Training loss: 2.469088277697901
Validation loss: 2.5385765550929116

Epoch: 6| Step: 8
Training loss: 3.1887357223831123
Validation loss: 2.4940514458110763

Epoch: 6| Step: 9
Training loss: 2.0328665536295594
Validation loss: 2.5389583574492507

Epoch: 6| Step: 10
Training loss: 2.2130565395932664
Validation loss: 2.5501823790764737

Epoch: 6| Step: 11
Training loss: 2.244107690209262
Validation loss: 2.5137030484218585

Epoch: 6| Step: 12
Training loss: 2.641420504134676
Validation loss: 2.558425985242321

Epoch: 6| Step: 13
Training loss: 2.8358788182095904
Validation loss: 2.525351752723462

Epoch: 285| Step: 0
Training loss: 2.8490614700943144
Validation loss: 2.563648071739551

Epoch: 6| Step: 1
Training loss: 2.6166170779670344
Validation loss: 2.4962312553065495

Epoch: 6| Step: 2
Training loss: 2.642027442997518
Validation loss: 2.522104324429629

Epoch: 6| Step: 3
Training loss: 2.3454839777158316
Validation loss: 2.521050394940908

Epoch: 6| Step: 4
Training loss: 2.191655381128919
Validation loss: 2.5440060987735444

Epoch: 6| Step: 5
Training loss: 2.999884126332772
Validation loss: 2.536444834502921

Epoch: 6| Step: 6
Training loss: 2.824377285001894
Validation loss: 2.516541680044757

Epoch: 6| Step: 7
Training loss: 2.7664991818331512
Validation loss: 2.525120701479237

Epoch: 6| Step: 8
Training loss: 2.5680815223695466
Validation loss: 2.5674448856396084

Epoch: 6| Step: 9
Training loss: 2.0717485161977733
Validation loss: 2.5224941660337956

Epoch: 6| Step: 10
Training loss: 2.1530483636380655
Validation loss: 2.527270212780644

Epoch: 6| Step: 11
Training loss: 1.776836347988114
Validation loss: 2.569509100957362

Epoch: 6| Step: 12
Training loss: 2.527928468965798
Validation loss: 2.517336331138152

Epoch: 6| Step: 13
Training loss: 2.5154320305209796
Validation loss: 2.507533200509706

Epoch: 286| Step: 0
Training loss: 1.8500658899893934
Validation loss: 2.4946846250628445

Epoch: 6| Step: 1
Training loss: 2.3400805421971542
Validation loss: 2.5113344049787614

Epoch: 6| Step: 2
Training loss: 3.0261401337160048
Validation loss: 2.5292922307795873

Epoch: 6| Step: 3
Training loss: 2.3786520736030017
Validation loss: 2.5358569698729667

Epoch: 6| Step: 4
Training loss: 2.678062724899492
Validation loss: 2.5338111295989876

Epoch: 6| Step: 5
Training loss: 2.440063204806574
Validation loss: 2.490881958351912

Epoch: 6| Step: 6
Training loss: 2.8788678232679694
Validation loss: 2.5567507974581254

Epoch: 6| Step: 7
Training loss: 2.4001031615020323
Validation loss: 2.5576201705070423

Epoch: 6| Step: 8
Training loss: 1.928106672054586
Validation loss: 2.542630806724336

Epoch: 6| Step: 9
Training loss: 2.7387586060549385
Validation loss: 2.5659803925377016

Epoch: 6| Step: 10
Training loss: 2.1566386218166187
Validation loss: 2.520416219832566

Epoch: 6| Step: 11
Training loss: 2.835977853535212
Validation loss: 2.5554322798479827

Epoch: 6| Step: 12
Training loss: 2.5094550627101246
Validation loss: 2.5082348118574074

Epoch: 6| Step: 13
Training loss: 2.182681744536044
Validation loss: 2.528188074188252

Epoch: 287| Step: 0
Training loss: 2.446287308195479
Validation loss: 2.549535644358855

Epoch: 6| Step: 1
Training loss: 2.556831137113791
Validation loss: 2.5269038845007454

Epoch: 6| Step: 2
Training loss: 2.2607750908782687
Validation loss: 2.537508608604776

Epoch: 6| Step: 3
Training loss: 3.0806384982871498
Validation loss: 2.4974735748383656

Epoch: 6| Step: 4
Training loss: 2.57571326071481
Validation loss: 2.5657824100854776

Epoch: 6| Step: 5
Training loss: 2.018242607482126
Validation loss: 2.531204875471413

Epoch: 6| Step: 6
Training loss: 2.3248680938711654
Validation loss: 2.5206287941411443

Epoch: 6| Step: 7
Training loss: 2.2102257759105215
Validation loss: 2.5170874355200366

Epoch: 6| Step: 8
Training loss: 2.2768381385217307
Validation loss: 2.5683657402759166

Epoch: 6| Step: 9
Training loss: 2.195186081908343
Validation loss: 2.5182739519197397

Epoch: 6| Step: 10
Training loss: 2.7348366483901163
Validation loss: 2.5169403432488267

Epoch: 6| Step: 11
Training loss: 2.4889647115594435
Validation loss: 2.5327070249176886

Epoch: 6| Step: 12
Training loss: 2.3776824258638323
Validation loss: 2.55670910908078

Epoch: 6| Step: 13
Training loss: 2.985794768223231
Validation loss: 2.5551231624686763

Epoch: 288| Step: 0
Training loss: 2.158316244207453
Validation loss: 2.5083053994359985

Epoch: 6| Step: 1
Training loss: 3.032496559788953
Validation loss: 2.548891381711804

Epoch: 6| Step: 2
Training loss: 2.4123971867765843
Validation loss: 2.50989830892438

Epoch: 6| Step: 3
Training loss: 1.821745476281685
Validation loss: 2.4753615091181156

Epoch: 6| Step: 4
Training loss: 2.5312238150290107
Validation loss: 2.5582184456895747

Epoch: 6| Step: 5
Training loss: 2.6437879050132476
Validation loss: 2.52570425117888

Epoch: 6| Step: 6
Training loss: 3.457384615149896
Validation loss: 2.5357225525907516

Epoch: 6| Step: 7
Training loss: 2.107088785655404
Validation loss: 2.4929352474769835

Epoch: 6| Step: 8
Training loss: 2.5280213663671027
Validation loss: 2.5392093133248914

Epoch: 6| Step: 9
Training loss: 2.2939386607236467
Validation loss: 2.5227282153447943

Epoch: 6| Step: 10
Training loss: 1.9641352868382187
Validation loss: 2.5566592076725643

Epoch: 6| Step: 11
Training loss: 2.578016059192153
Validation loss: 2.538029852827323

Epoch: 6| Step: 12
Training loss: 2.2156398627424294
Validation loss: 2.5023971386107196

Epoch: 6| Step: 13
Training loss: 2.2579269644157542
Validation loss: 2.523433125393478

Epoch: 289| Step: 0
Training loss: 1.9113682683225304
Validation loss: 2.5395894881765635

Epoch: 6| Step: 1
Training loss: 2.734165902998505
Validation loss: 2.56308654637167

Epoch: 6| Step: 2
Training loss: 2.4041198615724992
Validation loss: 2.538276262620276

Epoch: 6| Step: 3
Training loss: 2.25060539578176
Validation loss: 2.5466276454233907

Epoch: 6| Step: 4
Training loss: 2.4426363112209857
Validation loss: 2.5276662822307663

Epoch: 6| Step: 5
Training loss: 2.2509096214287085
Validation loss: 2.558174736871285

Epoch: 6| Step: 6
Training loss: 2.613663763724383
Validation loss: 2.516222391218692

Epoch: 6| Step: 7
Training loss: 2.31788822004614
Validation loss: 2.5305720912474037

Epoch: 6| Step: 8
Training loss: 2.9474122028038945
Validation loss: 2.5358619124336084

Epoch: 6| Step: 9
Training loss: 2.5242754132620933
Validation loss: 2.508191904170334

Epoch: 6| Step: 10
Training loss: 2.4245735147367746
Validation loss: 2.5323270498723036

Epoch: 6| Step: 11
Training loss: 3.0545042329335197
Validation loss: 2.517231045310647

Epoch: 6| Step: 12
Training loss: 2.436623635840549
Validation loss: 2.5386931746004935

Epoch: 6| Step: 13
Training loss: 2.3554628792811374
Validation loss: 2.5415537642833117

Epoch: 290| Step: 0
Training loss: 2.211784065923358
Validation loss: 2.50869661073803

Epoch: 6| Step: 1
Training loss: 2.6291742287439894
Validation loss: 2.5731118632881094

Epoch: 6| Step: 2
Training loss: 2.5096772768375044
Validation loss: 2.524868876316675

Epoch: 6| Step: 3
Training loss: 2.805245941586164
Validation loss: 2.5669199014234247

Epoch: 6| Step: 4
Training loss: 2.008724970112683
Validation loss: 2.4972044131994937

Epoch: 6| Step: 5
Training loss: 2.0779951241429595
Validation loss: 2.5215778631048087

Epoch: 6| Step: 6
Training loss: 1.9403219284293016
Validation loss: 2.522762881843311

Epoch: 6| Step: 7
Training loss: 2.897444315812199
Validation loss: 2.5617915448986706

Epoch: 6| Step: 8
Training loss: 2.4980216781860602
Validation loss: 2.5336044413242695

Epoch: 6| Step: 9
Training loss: 1.9059076470799938
Validation loss: 2.552510846317546

Epoch: 6| Step: 10
Training loss: 1.9243774249515946
Validation loss: 2.561201892285014

Epoch: 6| Step: 11
Training loss: 2.8554770995303715
Validation loss: 2.5541343776320935

Epoch: 6| Step: 12
Training loss: 2.5995516867366537
Validation loss: 2.530140916237431

Epoch: 6| Step: 13
Training loss: 3.1863670579636887
Validation loss: 2.538594271238067

Epoch: 291| Step: 0
Training loss: 2.335482651943554
Validation loss: 2.553772382758331

Epoch: 6| Step: 1
Training loss: 2.7658166334064314
Validation loss: 2.568001791244573

Epoch: 6| Step: 2
Training loss: 2.6051484558797897
Validation loss: 2.539783127517607

Epoch: 6| Step: 3
Training loss: 1.9346313158841553
Validation loss: 2.5524561392284815

Epoch: 6| Step: 4
Training loss: 2.102460024314163
Validation loss: 2.5253286810358

Epoch: 6| Step: 5
Training loss: 2.061798901317162
Validation loss: 2.5153434207047427

Epoch: 6| Step: 6
Training loss: 2.355019294028
Validation loss: 2.5283033852302137

Epoch: 6| Step: 7
Training loss: 2.3620800149593393
Validation loss: 2.554818369702725

Epoch: 6| Step: 8
Training loss: 2.361251388072812
Validation loss: 2.5227102195924807

Epoch: 6| Step: 9
Training loss: 2.839093766489052
Validation loss: 2.5258724679068725

Epoch: 6| Step: 10
Training loss: 2.30958703581419
Validation loss: 2.5577840821979643

Epoch: 6| Step: 11
Training loss: 2.777723412511547
Validation loss: 2.558896069332308

Epoch: 6| Step: 12
Training loss: 2.9545549432562135
Validation loss: 2.5115422114539054

Epoch: 6| Step: 13
Training loss: 2.706064731520064
Validation loss: 2.539719888226158

Epoch: 292| Step: 0
Training loss: 1.8392932497523593
Validation loss: 2.5204189061235276

Epoch: 6| Step: 1
Training loss: 2.06022842622193
Validation loss: 2.5308509916281574

Epoch: 6| Step: 2
Training loss: 2.3360638536452947
Validation loss: 2.5066479041953653

Epoch: 6| Step: 3
Training loss: 2.6112852128696438
Validation loss: 2.515076612845527

Epoch: 6| Step: 4
Training loss: 1.9798560765487592
Validation loss: 2.5392674991717743

Epoch: 6| Step: 5
Training loss: 2.7477239379701945
Validation loss: 2.5227582174646335

Epoch: 6| Step: 6
Training loss: 2.8705300121509234
Validation loss: 2.513907833281421

Epoch: 6| Step: 7
Training loss: 2.8867043790343403
Validation loss: 2.5335135154031314

Epoch: 6| Step: 8
Training loss: 2.9159229056864207
Validation loss: 2.512988586235886

Epoch: 6| Step: 9
Training loss: 2.069249063552215
Validation loss: 2.508161924058135

Epoch: 6| Step: 10
Training loss: 2.2595643107983867
Validation loss: 2.5364043689135185

Epoch: 6| Step: 11
Training loss: 2.7038537955851814
Validation loss: 2.5427095790779544

Epoch: 6| Step: 12
Training loss: 2.5463918215882475
Validation loss: 2.556816833079755

Epoch: 6| Step: 13
Training loss: 1.939084297444971
Validation loss: 2.5216167727165395

Epoch: 293| Step: 0
Training loss: 2.6862902912794473
Validation loss: 2.5218715809359256

Epoch: 6| Step: 1
Training loss: 2.285200278355231
Validation loss: 2.5371365763641314

Epoch: 6| Step: 2
Training loss: 3.248806660984766
Validation loss: 2.503200083636017

Epoch: 6| Step: 3
Training loss: 2.437144033409043
Validation loss: 2.5185385272436798

Epoch: 6| Step: 4
Training loss: 2.189428514789764
Validation loss: 2.5315166734488868

Epoch: 6| Step: 5
Training loss: 2.8016672688184188
Validation loss: 2.519402397761154

Epoch: 6| Step: 6
Training loss: 2.4096101361632716
Validation loss: 2.5360415142454995

Epoch: 6| Step: 7
Training loss: 2.0422539417809253
Validation loss: 2.5358251644821412

Epoch: 6| Step: 8
Training loss: 2.4276351847068898
Validation loss: 2.500843383699105

Epoch: 6| Step: 9
Training loss: 1.9492066407830906
Validation loss: 2.542959928855632

Epoch: 6| Step: 10
Training loss: 2.3546069135842567
Validation loss: 2.5335657001309517

Epoch: 6| Step: 11
Training loss: 2.476954576521663
Validation loss: 2.4951834888076525

Epoch: 6| Step: 12
Training loss: 2.431052165084907
Validation loss: 2.5423016419854383

Epoch: 6| Step: 13
Training loss: 2.6304678508778534
Validation loss: 2.5365824466366718

Epoch: 294| Step: 0
Training loss: 1.8413806497560852
Validation loss: 2.521389684164233

Epoch: 6| Step: 1
Training loss: 2.4185224401392333
Validation loss: 2.487274512829853

Epoch: 6| Step: 2
Training loss: 2.2822507988390033
Validation loss: 2.5200943151290796

Epoch: 6| Step: 3
Training loss: 2.391493197097051
Validation loss: 2.5455013788260676

Epoch: 6| Step: 4
Training loss: 2.673477138669102
Validation loss: 2.533594804397743

Epoch: 6| Step: 5
Training loss: 2.1442413968010055
Validation loss: 2.5242942605974723

Epoch: 6| Step: 6
Training loss: 2.5156973123523243
Validation loss: 2.4978824435548015

Epoch: 6| Step: 7
Training loss: 2.504513671325912
Validation loss: 2.520650856195433

Epoch: 6| Step: 8
Training loss: 2.3123590581059714
Validation loss: 2.4997837773001756

Epoch: 6| Step: 9
Training loss: 2.4330805303163303
Validation loss: 2.522855016289554

Epoch: 6| Step: 10
Training loss: 2.9674289374130245
Validation loss: 2.524697473337682

Epoch: 6| Step: 11
Training loss: 2.420626410757964
Validation loss: 2.5265410646492277

Epoch: 6| Step: 12
Training loss: 2.7051229865695645
Validation loss: 2.5447528140044042

Epoch: 6| Step: 13
Training loss: 2.4821764736898353
Validation loss: 2.54477144520351

Epoch: 295| Step: 0
Training loss: 2.185222639180751
Validation loss: 2.5560321202630267

Epoch: 6| Step: 1
Training loss: 3.0544286749962994
Validation loss: 2.4972746637660195

Epoch: 6| Step: 2
Training loss: 1.6283811459245614
Validation loss: 2.5522195461170227

Epoch: 6| Step: 3
Training loss: 2.2752649886519163
Validation loss: 2.5449584508021315

Epoch: 6| Step: 4
Training loss: 2.160184504788089
Validation loss: 2.522345759816315

Epoch: 6| Step: 5
Training loss: 2.024762518815201
Validation loss: 2.5485181613161463

Epoch: 6| Step: 6
Training loss: 3.1061283202871453
Validation loss: 2.546381654159085

Epoch: 6| Step: 7
Training loss: 2.427830124161468
Validation loss: 2.5501348159447117

Epoch: 6| Step: 8
Training loss: 2.6496341668731778
Validation loss: 2.5476018101476665

Epoch: 6| Step: 9
Training loss: 2.678139464651993
Validation loss: 2.5262888056879023

Epoch: 6| Step: 10
Training loss: 2.0767286384891848
Validation loss: 2.5295682161567177

Epoch: 6| Step: 11
Training loss: 1.9961801051143766
Validation loss: 2.5046002159007057

Epoch: 6| Step: 12
Training loss: 2.841918691010261
Validation loss: 2.5393052265775164

Epoch: 6| Step: 13
Training loss: 2.3618956988866873
Validation loss: 2.5456007661227873

Epoch: 296| Step: 0
Training loss: 2.759726835222575
Validation loss: 2.5190834287825132

Epoch: 6| Step: 1
Training loss: 2.6343157633754184
Validation loss: 2.565572196088617

Epoch: 6| Step: 2
Training loss: 2.1225739261976417
Validation loss: 2.5268994184994384

Epoch: 6| Step: 3
Training loss: 2.14275994307459
Validation loss: 2.580451579293542

Epoch: 6| Step: 4
Training loss: 1.6172836201431704
Validation loss: 2.5658031706571514

Epoch: 6| Step: 5
Training loss: 2.1343601745716123
Validation loss: 2.533675641533316

Epoch: 6| Step: 6
Training loss: 3.0413727988503667
Validation loss: 2.526366619237823

Epoch: 6| Step: 7
Training loss: 2.4910832652383554
Validation loss: 2.553809457201352

Epoch: 6| Step: 8
Training loss: 2.1189243751269946
Validation loss: 2.554547970918317

Epoch: 6| Step: 9
Training loss: 3.5326457008286307
Validation loss: 2.5258632460314088

Epoch: 6| Step: 10
Training loss: 2.092845892352847
Validation loss: 2.538479978569027

Epoch: 6| Step: 11
Training loss: 3.1067674908083096
Validation loss: 2.554987597481468

Epoch: 6| Step: 12
Training loss: 2.1497361575738023
Validation loss: 2.554170175123148

Epoch: 6| Step: 13
Training loss: 1.7733705482784654
Validation loss: 2.5189277696320604

Epoch: 297| Step: 0
Training loss: 1.5600365101381015
Validation loss: 2.5382646229320827

Epoch: 6| Step: 1
Training loss: 2.423241014711673
Validation loss: 2.5139090580405834

Epoch: 6| Step: 2
Training loss: 2.3776709946554178
Validation loss: 2.5166770199233572

Epoch: 6| Step: 3
Training loss: 2.955165259166461
Validation loss: 2.53801066708992

Epoch: 6| Step: 4
Training loss: 3.1962222270930067
Validation loss: 2.477773226354224

Epoch: 6| Step: 5
Training loss: 2.45087285744624
Validation loss: 2.5351131366083752

Epoch: 6| Step: 6
Training loss: 1.8892416188376078
Validation loss: 2.512219337619712

Epoch: 6| Step: 7
Training loss: 2.4059549125955937
Validation loss: 2.5146289102521124

Epoch: 6| Step: 8
Training loss: 2.331618996224445
Validation loss: 2.521660078225928

Epoch: 6| Step: 9
Training loss: 2.443788095943168
Validation loss: 2.505376717057197

Epoch: 6| Step: 10
Training loss: 2.4245547328253534
Validation loss: 2.534805557500053

Epoch: 6| Step: 11
Training loss: 2.541880572547179
Validation loss: 2.5025417829781813

Epoch: 6| Step: 12
Training loss: 2.575590147519925
Validation loss: 2.5322140774819837

Epoch: 6| Step: 13
Training loss: 2.6339680204600806
Validation loss: 2.548700073933174

Epoch: 298| Step: 0
Training loss: 1.7005515998833929
Validation loss: 2.5450591798727276

Epoch: 6| Step: 1
Training loss: 2.3023025845568026
Validation loss: 2.537712935230257

Epoch: 6| Step: 2
Training loss: 2.1387542569392886
Validation loss: 2.543313599495965

Epoch: 6| Step: 3
Training loss: 2.333149857346488
Validation loss: 2.5283379027882305

Epoch: 6| Step: 4
Training loss: 2.6117177713924655
Validation loss: 2.549939189879909

Epoch: 6| Step: 5
Training loss: 2.6506660560067368
Validation loss: 2.535607279675471

Epoch: 6| Step: 6
Training loss: 2.415255748562279
Validation loss: 2.51464152335867

Epoch: 6| Step: 7
Training loss: 2.578648092204315
Validation loss: 2.5383438888142993

Epoch: 6| Step: 8
Training loss: 2.2605285148079344
Validation loss: 2.5570889439329676

Epoch: 6| Step: 9
Training loss: 2.311363353270594
Validation loss: 2.5298442899891063

Epoch: 6| Step: 10
Training loss: 2.4434608542075975
Validation loss: 2.5199320009009276

Epoch: 6| Step: 11
Training loss: 3.2738176730496265
Validation loss: 2.546839263039196

Epoch: 6| Step: 12
Training loss: 2.359407740485789
Validation loss: 2.5191188695130937

Epoch: 6| Step: 13
Training loss: 2.3393039494659646
Validation loss: 2.506807536027855

Epoch: 299| Step: 0
Training loss: 2.8153124840112143
Validation loss: 2.513987783002402

Epoch: 6| Step: 1
Training loss: 2.764906967833002
Validation loss: 2.5157824205709645

Epoch: 6| Step: 2
Training loss: 1.7659561386839406
Validation loss: 2.525305410161995

Epoch: 6| Step: 3
Training loss: 2.680705775090993
Validation loss: 2.5328854233158644

Epoch: 6| Step: 4
Training loss: 3.117341996190957
Validation loss: 2.5062802504598385

Epoch: 6| Step: 5
Training loss: 2.0250501650024297
Validation loss: 2.5385137351481735

Epoch: 6| Step: 6
Training loss: 2.281809489998019
Validation loss: 2.484208126994441

Epoch: 6| Step: 7
Training loss: 2.2693850130381894
Validation loss: 2.5314218494939142

Epoch: 6| Step: 8
Training loss: 2.6744844064534288
Validation loss: 2.543162836668621

Epoch: 6| Step: 9
Training loss: 1.8142483270245024
Validation loss: 2.547102989864841

Epoch: 6| Step: 10
Training loss: 2.577062861968515
Validation loss: 2.5715037749303837

Epoch: 6| Step: 11
Training loss: 2.2481385054088534
Validation loss: 2.5436794864939

Epoch: 6| Step: 12
Training loss: 2.479177448238576
Validation loss: 2.499091686832893

Epoch: 6| Step: 13
Training loss: 2.2588680136439945
Validation loss: 2.5537341754015803

Epoch: 300| Step: 0
Training loss: 1.7809850930774003
Validation loss: 2.468267157074258

Epoch: 6| Step: 1
Training loss: 2.455563930374448
Validation loss: 2.5252157021498043

Epoch: 6| Step: 2
Training loss: 1.8530028973601531
Validation loss: 2.5179109224763074

Epoch: 6| Step: 3
Training loss: 2.3354098639501157
Validation loss: 2.5181595274891633

Epoch: 6| Step: 4
Training loss: 2.3433062324023974
Validation loss: 2.5234157828384447

Epoch: 6| Step: 5
Training loss: 2.5228486215459553
Validation loss: 2.5256535138734795

Epoch: 6| Step: 6
Training loss: 2.1396916714595995
Validation loss: 2.560969379055475

Epoch: 6| Step: 7
Training loss: 2.9490997492547937
Validation loss: 2.526035178839764

Epoch: 6| Step: 8
Training loss: 2.8402634930944486
Validation loss: 2.5222158712941045

Epoch: 6| Step: 9
Training loss: 2.8765801772652457
Validation loss: 2.5069822400720234

Epoch: 6| Step: 10
Training loss: 2.770307294966557
Validation loss: 2.5533342538250046

Epoch: 6| Step: 11
Training loss: 1.9921249080623709
Validation loss: 2.5667414499776693

Epoch: 6| Step: 12
Training loss: 2.623543017084701
Validation loss: 2.5501500551830585

Epoch: 6| Step: 13
Training loss: 2.107985992870084
Validation loss: 2.55816581333001

Epoch: 301| Step: 0
Training loss: 2.2969736091050965
Validation loss: 2.5482269922248997

Epoch: 6| Step: 1
Training loss: 2.570349695927908
Validation loss: 2.568315866713896

Epoch: 6| Step: 2
Training loss: 2.181592320903808
Validation loss: 2.5609599582218663

Epoch: 6| Step: 3
Training loss: 2.8995383914695725
Validation loss: 2.51217202315333

Epoch: 6| Step: 4
Training loss: 2.372737308608528
Validation loss: 2.5238971144963007

Epoch: 6| Step: 5
Training loss: 1.9928612859890906
Validation loss: 2.5573171158998287

Epoch: 6| Step: 6
Training loss: 2.971188316876475
Validation loss: 2.552986777346991

Epoch: 6| Step: 7
Training loss: 2.822536363122481
Validation loss: 2.5519126927123685

Epoch: 6| Step: 8
Training loss: 2.1938870509448445
Validation loss: 2.5199622077324992

Epoch: 6| Step: 9
Training loss: 2.2958762144686906
Validation loss: 2.572446799064831

Epoch: 6| Step: 10
Training loss: 2.4998596151990466
Validation loss: 2.468719497809994

Epoch: 6| Step: 11
Training loss: 2.1964525405338
Validation loss: 2.557448531683959

Epoch: 6| Step: 12
Training loss: 2.485703215237725
Validation loss: 2.5334698701256646

Epoch: 6| Step: 13
Training loss: 1.9629610115450764
Validation loss: 2.572588960794423

Epoch: 302| Step: 0
Training loss: 2.6196785530156883
Validation loss: 2.5249523666406586

Epoch: 6| Step: 1
Training loss: 2.2845998755113603
Validation loss: 2.513847947847206

Epoch: 6| Step: 2
Training loss: 1.7201198321006268
Validation loss: 2.5352360827718914

Epoch: 6| Step: 3
Training loss: 2.4763169994273415
Validation loss: 2.563287291399799

Epoch: 6| Step: 4
Training loss: 2.0626083114389986
Validation loss: 2.5724038332931825

Epoch: 6| Step: 5
Training loss: 2.768225013440313
Validation loss: 2.482341796789629

Epoch: 6| Step: 6
Training loss: 2.0929837319242615
Validation loss: 2.5287430747092263

Epoch: 6| Step: 7
Training loss: 2.822004407785976
Validation loss: 2.540317177819677

Epoch: 6| Step: 8
Training loss: 1.958476920308634
Validation loss: 2.52221481929509

Epoch: 6| Step: 9
Training loss: 2.853996513102593
Validation loss: 2.5324828218160254

Epoch: 6| Step: 10
Training loss: 2.3174256113041087
Validation loss: 2.534767322660671

Epoch: 6| Step: 11
Training loss: 3.13447659785178
Validation loss: 2.523590581054021

Epoch: 6| Step: 12
Training loss: 2.4149831849454637
Validation loss: 2.526337215554772

Epoch: 6| Step: 13
Training loss: 1.7297440985678059
Validation loss: 2.4884369264536357

Epoch: 303| Step: 0
Training loss: 2.155268570168949
Validation loss: 2.5333227757939336

Epoch: 6| Step: 1
Training loss: 2.427667201026621
Validation loss: 2.507327125604266

Epoch: 6| Step: 2
Training loss: 2.611093103964468
Validation loss: 2.573231247450207

Epoch: 6| Step: 3
Training loss: 1.9577076974448058
Validation loss: 2.4984076325224214

Epoch: 6| Step: 4
Training loss: 2.5690376807478983
Validation loss: 2.4910097659737853

Epoch: 6| Step: 5
Training loss: 2.701747388894761
Validation loss: 2.5367612829437842

Epoch: 6| Step: 6
Training loss: 2.440382988686351
Validation loss: 2.542629799469372

Epoch: 6| Step: 7
Training loss: 2.300789643613987
Validation loss: 2.489947976378846

Epoch: 6| Step: 8
Training loss: 2.1447898278051727
Validation loss: 2.539753992215882

Epoch: 6| Step: 9
Training loss: 2.7174619934175612
Validation loss: 2.518679758025445

Epoch: 6| Step: 10
Training loss: 2.1319887016526633
Validation loss: 2.4868447200695734

Epoch: 6| Step: 11
Training loss: 1.7189717929960244
Validation loss: 2.495712212855733

Epoch: 6| Step: 12
Training loss: 2.8179886561558662
Validation loss: 2.496258868222774

Epoch: 6| Step: 13
Training loss: 3.056153552946018
Validation loss: 2.4866915800636935

Epoch: 304| Step: 0
Training loss: 2.720714867560356
Validation loss: 2.4801823017920275

Epoch: 6| Step: 1
Training loss: 2.1732884480028547
Validation loss: 2.528359227350417

Epoch: 6| Step: 2
Training loss: 2.3728941566085067
Validation loss: 2.5642775966207845

Epoch: 6| Step: 3
Training loss: 2.6549071564039215
Validation loss: 2.529042013078943

Epoch: 6| Step: 4
Training loss: 1.8507027863423247
Validation loss: 2.5269488372760525

Epoch: 6| Step: 5
Training loss: 2.271766389247194
Validation loss: 2.5754035216182123

Epoch: 6| Step: 6
Training loss: 3.1909514048195597
Validation loss: 2.544205148538202

Epoch: 6| Step: 7
Training loss: 2.3882332599524005
Validation loss: 2.5024307078351167

Epoch: 6| Step: 8
Training loss: 1.8016983973519962
Validation loss: 2.5399486616513096

Epoch: 6| Step: 9
Training loss: 2.6646483551832887
Validation loss: 2.535888478152909

Epoch: 6| Step: 10
Training loss: 2.5910735511602083
Validation loss: 2.499449869354658

Epoch: 6| Step: 11
Training loss: 2.3468064787064433
Validation loss: 2.5837253335331103

Epoch: 6| Step: 12
Training loss: 1.9870121171118194
Validation loss: 2.5315890371888523

Epoch: 6| Step: 13
Training loss: 2.821220103972931
Validation loss: 2.490329902516914

Epoch: 305| Step: 0
Training loss: 2.30546055129483
Validation loss: 2.5260500844433125

Epoch: 6| Step: 1
Training loss: 3.0541673300318672
Validation loss: 2.5027505131140253

Epoch: 6| Step: 2
Training loss: 2.329298095088667
Validation loss: 2.5271393904638044

Epoch: 6| Step: 3
Training loss: 1.9107530281814522
Validation loss: 2.5553817044148888

Epoch: 6| Step: 4
Training loss: 2.9718797749863057
Validation loss: 2.526047570581433

Epoch: 6| Step: 5
Training loss: 1.8177322633875055
Validation loss: 2.518011539793544

Epoch: 6| Step: 6
Training loss: 3.016630173211279
Validation loss: 2.5191746536886

Epoch: 6| Step: 7
Training loss: 2.235882017458903
Validation loss: 2.5334831949066343

Epoch: 6| Step: 8
Training loss: 2.079811501764892
Validation loss: 2.50183546460082

Epoch: 6| Step: 9
Training loss: 2.4267009275588918
Validation loss: 2.522449223308459

Epoch: 6| Step: 10
Training loss: 1.844603454133131
Validation loss: 2.541350208985277

Epoch: 6| Step: 11
Training loss: 2.6267741429501106
Validation loss: 2.5592321292566464

Epoch: 6| Step: 12
Training loss: 2.24325993108427
Validation loss: 2.5267355040942965

Epoch: 6| Step: 13
Training loss: 2.558672768340887
Validation loss: 2.5542960751245505

Epoch: 306| Step: 0
Training loss: 2.7451424612797326
Validation loss: 2.5340991841038694

Epoch: 6| Step: 1
Training loss: 2.2190845935679526
Validation loss: 2.4746106989794083

Epoch: 6| Step: 2
Training loss: 2.316167449221241
Validation loss: 2.5190450138257807

Epoch: 6| Step: 3
Training loss: 2.158145900330146
Validation loss: 2.543972312789158

Epoch: 6| Step: 4
Training loss: 2.3522232496094877
Validation loss: 2.5016619316323467

Epoch: 6| Step: 5
Training loss: 2.693442937427436
Validation loss: 2.512629922213539

Epoch: 6| Step: 6
Training loss: 2.6685405246973306
Validation loss: 2.4936791363062114

Epoch: 6| Step: 7
Training loss: 2.2468148044380283
Validation loss: 2.542059346740889

Epoch: 6| Step: 8
Training loss: 2.942835568178326
Validation loss: 2.520818179586802

Epoch: 6| Step: 9
Training loss: 2.2759714588594844
Validation loss: 2.561958553918225

Epoch: 6| Step: 10
Training loss: 2.6669240668364527
Validation loss: 2.530236090131341

Epoch: 6| Step: 11
Training loss: 2.710921141825196
Validation loss: 2.5520896591767763

Epoch: 6| Step: 12
Training loss: 1.5956364854697347
Validation loss: 2.470056408560236

Epoch: 6| Step: 13
Training loss: 2.5171441650449085
Validation loss: 2.490501053083211

Epoch: 307| Step: 0
Training loss: 2.015622870865518
Validation loss: 2.5360904020861392

Epoch: 6| Step: 1
Training loss: 2.616068677747332
Validation loss: 2.5218978335315425

Epoch: 6| Step: 2
Training loss: 2.5592449262307317
Validation loss: 2.5429624451531905

Epoch: 6| Step: 3
Training loss: 2.529684265398546
Validation loss: 2.499448209800699

Epoch: 6| Step: 4
Training loss: 2.854638684820554
Validation loss: 2.5123954321619917

Epoch: 6| Step: 5
Training loss: 2.012806067087039
Validation loss: 2.5313559008298503

Epoch: 6| Step: 6
Training loss: 2.5776773699940216
Validation loss: 2.519182170047761

Epoch: 6| Step: 7
Training loss: 2.0132967491278646
Validation loss: 2.5361942146083147

Epoch: 6| Step: 8
Training loss: 2.5723345840699543
Validation loss: 2.4679567332828736

Epoch: 6| Step: 9
Training loss: 1.6804480671551976
Validation loss: 2.5013433333882977

Epoch: 6| Step: 10
Training loss: 2.4434032847897993
Validation loss: 2.521455195000828

Epoch: 6| Step: 11
Training loss: 2.548287965093339
Validation loss: 2.5306386316927947

Epoch: 6| Step: 12
Training loss: 2.6470845685592006
Validation loss: 2.5207377050965905

Epoch: 6| Step: 13
Training loss: 2.1136376156248557
Validation loss: 2.5003108959286493

Epoch: 308| Step: 0
Training loss: 2.102139645424915
Validation loss: 2.508259124184134

Epoch: 6| Step: 1
Training loss: 2.792770869574489
Validation loss: 2.5334187358305704

Epoch: 6| Step: 2
Training loss: 2.3103192330933986
Validation loss: 2.500227583504032

Epoch: 6| Step: 3
Training loss: 2.065252721252976
Validation loss: 2.545127291647061

Epoch: 6| Step: 4
Training loss: 3.204628493845946
Validation loss: 2.517864136999343

Epoch: 6| Step: 5
Training loss: 2.5794154781033627
Validation loss: 2.5168907646256033

Epoch: 6| Step: 6
Training loss: 1.9013402277637337
Validation loss: 2.512912916548141

Epoch: 6| Step: 7
Training loss: 2.584220569798395
Validation loss: 2.503224208286781

Epoch: 6| Step: 8
Training loss: 1.8880942564970733
Validation loss: 2.533365227480159

Epoch: 6| Step: 9
Training loss: 2.411822813981421
Validation loss: 2.509625677170934

Epoch: 6| Step: 10
Training loss: 1.786542425224681
Validation loss: 2.4813054318367134

Epoch: 6| Step: 11
Training loss: 2.8454561250689463
Validation loss: 2.505944248538701

Epoch: 6| Step: 12
Training loss: 2.6872922240380723
Validation loss: 2.512132800402607

Epoch: 6| Step: 13
Training loss: 2.31739690738576
Validation loss: 2.5210254819354465

Epoch: 309| Step: 0
Training loss: 2.215514389311617
Validation loss: 2.505961296149503

Epoch: 6| Step: 1
Training loss: 2.3578415491578433
Validation loss: 2.542788022334303

Epoch: 6| Step: 2
Training loss: 2.143733531121257
Validation loss: 2.545946518805599

Epoch: 6| Step: 3
Training loss: 3.16455752421024
Validation loss: 2.5340600802019133

Epoch: 6| Step: 4
Training loss: 2.584590872405567
Validation loss: 2.513801716495108

Epoch: 6| Step: 5
Training loss: 1.980717329266913
Validation loss: 2.50186313708979

Epoch: 6| Step: 6
Training loss: 2.8546049426605618
Validation loss: 2.5327266010541267

Epoch: 6| Step: 7
Training loss: 2.079116928739012
Validation loss: 2.5110586897045564

Epoch: 6| Step: 8
Training loss: 1.9757477660564124
Validation loss: 2.5024033612576444

Epoch: 6| Step: 9
Training loss: 2.2481202644677936
Validation loss: 2.5026358040436545

Epoch: 6| Step: 10
Training loss: 3.0572355526264516
Validation loss: 2.5650569984934455

Epoch: 6| Step: 11
Training loss: 2.5271494111188133
Validation loss: 2.5003236991759197

Epoch: 6| Step: 12
Training loss: 2.675485146281858
Validation loss: 2.563706087914802

Epoch: 6| Step: 13
Training loss: 1.5013696457371413
Validation loss: 2.553464964979446

Epoch: 310| Step: 0
Training loss: 1.8566039136535297
Validation loss: 2.552837224479996

Epoch: 6| Step: 1
Training loss: 2.365017541574342
Validation loss: 2.508344030992811

Epoch: 6| Step: 2
Training loss: 2.662040412888406
Validation loss: 2.559557936821115

Epoch: 6| Step: 3
Training loss: 2.2037744308909057
Validation loss: 2.5204921224698658

Epoch: 6| Step: 4
Training loss: 2.523862253822409
Validation loss: 2.5417279142453775

Epoch: 6| Step: 5
Training loss: 2.4370909127491314
Validation loss: 2.5514168906121926

Epoch: 6| Step: 6
Training loss: 2.5373802833316077
Validation loss: 2.5185258298266846

Epoch: 6| Step: 7
Training loss: 2.169945827458373
Validation loss: 2.518264888515961

Epoch: 6| Step: 8
Training loss: 2.645837188077106
Validation loss: 2.492387056212856

Epoch: 6| Step: 9
Training loss: 2.541245587181392
Validation loss: 2.522033770335682

Epoch: 6| Step: 10
Training loss: 2.727826019718589
Validation loss: 2.541820743018771

Epoch: 6| Step: 11
Training loss: 2.4634927713866084
Validation loss: 2.5286452466582308

Epoch: 6| Step: 12
Training loss: 2.5827080882638187
Validation loss: 2.508045894994326

Epoch: 6| Step: 13
Training loss: 1.6219006104551235
Validation loss: 2.5120109350265474

Epoch: 311| Step: 0
Training loss: 2.9433114213081044
Validation loss: 2.5167481866466153

Epoch: 6| Step: 1
Training loss: 2.0147639364917165
Validation loss: 2.551070239300503

Epoch: 6| Step: 2
Training loss: 2.7385016120040855
Validation loss: 2.526795742555041

Epoch: 6| Step: 3
Training loss: 2.0220815467577666
Validation loss: 2.5131088408899016

Epoch: 6| Step: 4
Training loss: 2.4827600184296004
Validation loss: 2.5307423720539197

Epoch: 6| Step: 5
Training loss: 1.664392572898218
Validation loss: 2.5430703089073976

Epoch: 6| Step: 6
Training loss: 2.199851776678481
Validation loss: 2.4949203322046825

Epoch: 6| Step: 7
Training loss: 2.1748738614041043
Validation loss: 2.5552580370297617

Epoch: 6| Step: 8
Training loss: 2.2699053090045243
Validation loss: 2.507522647527071

Epoch: 6| Step: 9
Training loss: 2.873982290841203
Validation loss: 2.535075432773046

Epoch: 6| Step: 10
Training loss: 1.8636595783775451
Validation loss: 2.549418381504084

Epoch: 6| Step: 11
Training loss: 2.7982656761595313
Validation loss: 2.562079402362156

Epoch: 6| Step: 12
Training loss: 2.827878319535708
Validation loss: 2.5215860671998547

Epoch: 6| Step: 13
Training loss: 1.9572768371355902
Validation loss: 2.4919296986648507

Epoch: 312| Step: 0
Training loss: 1.3900930265816618
Validation loss: 2.5627422783480927

Epoch: 6| Step: 1
Training loss: 2.1206913069239226
Validation loss: 2.5153455436996173

Epoch: 6| Step: 2
Training loss: 2.218467157092845
Validation loss: 2.5337733841298045

Epoch: 6| Step: 3
Training loss: 2.5616907842423653
Validation loss: 2.529523903327981

Epoch: 6| Step: 4
Training loss: 2.044150133072551
Validation loss: 2.563194643956198

Epoch: 6| Step: 5
Training loss: 2.1897411310142836
Validation loss: 2.508855948212026

Epoch: 6| Step: 6
Training loss: 2.0996949928037587
Validation loss: 2.5524973275917073

Epoch: 6| Step: 7
Training loss: 2.9101408477990063
Validation loss: 2.5532281801919345

Epoch: 6| Step: 8
Training loss: 3.04812877974707
Validation loss: 2.5491945992651064

Epoch: 6| Step: 9
Training loss: 2.9127366064011078
Validation loss: 2.5314170856190588

Epoch: 6| Step: 10
Training loss: 2.4215532642896775
Validation loss: 2.5204554723514976

Epoch: 6| Step: 11
Training loss: 2.360608921375011
Validation loss: 2.5395799244351687

Epoch: 6| Step: 12
Training loss: 2.4532311288995494
Validation loss: 2.5216014250882925

Epoch: 6| Step: 13
Training loss: 2.8995263863915532
Validation loss: 2.563778366050577

Epoch: 313| Step: 0
Training loss: 2.9545956133872977
Validation loss: 2.5005038809841422

Epoch: 6| Step: 1
Training loss: 2.255140578123592
Validation loss: 2.512447861651005

Epoch: 6| Step: 2
Training loss: 2.3868731809310995
Validation loss: 2.544774627625145

Epoch: 6| Step: 3
Training loss: 2.3795455800127887
Validation loss: 2.5490763461853034

Epoch: 6| Step: 4
Training loss: 2.8706256926087437
Validation loss: 2.514540541289744

Epoch: 6| Step: 5
Training loss: 2.7063866490493984
Validation loss: 2.5198511562706143

Epoch: 6| Step: 6
Training loss: 2.219349847219732
Validation loss: 2.533207992789514

Epoch: 6| Step: 7
Training loss: 2.6343534131596744
Validation loss: 2.53916892309051

Epoch: 6| Step: 8
Training loss: 2.619709951492713
Validation loss: 2.5186888708286967

Epoch: 6| Step: 9
Training loss: 1.6135058673838256
Validation loss: 2.542879435255813

Epoch: 6| Step: 10
Training loss: 2.4103345016767164
Validation loss: 2.52516561391681

Epoch: 6| Step: 11
Training loss: 2.0193369198239703
Validation loss: 2.553468958828828

Epoch: 6| Step: 12
Training loss: 2.0150495787169853
Validation loss: 2.5091686878441486

Epoch: 6| Step: 13
Training loss: 1.9056552521983936
Validation loss: 2.5277763189625384

Epoch: 314| Step: 0
Training loss: 2.679665078828394
Validation loss: 2.555622646302104

Epoch: 6| Step: 1
Training loss: 2.046647619765598
Validation loss: 2.5188209773260946

Epoch: 6| Step: 2
Training loss: 2.383031516093641
Validation loss: 2.50618895820849

Epoch: 6| Step: 3
Training loss: 2.762275520973962
Validation loss: 2.547401902175395

Epoch: 6| Step: 4
Training loss: 1.9560941786452872
Validation loss: 2.5121957641771124

Epoch: 6| Step: 5
Training loss: 2.5354007550139666
Validation loss: 2.520434665775118

Epoch: 6| Step: 6
Training loss: 2.0418879200947244
Validation loss: 2.541546541546137

Epoch: 6| Step: 7
Training loss: 3.2130244914171273
Validation loss: 2.5188708889357296

Epoch: 6| Step: 8
Training loss: 2.328998785730277
Validation loss: 2.5445772212980233

Epoch: 6| Step: 9
Training loss: 1.7670416760843308
Validation loss: 2.5044430495559187

Epoch: 6| Step: 10
Training loss: 2.0519771903482438
Validation loss: 2.492663573803182

Epoch: 6| Step: 11
Training loss: 2.8695127117369292
Validation loss: 2.513799050668233

Epoch: 6| Step: 12
Training loss: 2.200311582348674
Validation loss: 2.5484797011578095

Epoch: 6| Step: 13
Training loss: 2.529975193339325
Validation loss: 2.523316947657966

Epoch: 315| Step: 0
Training loss: 1.9071719332213946
Validation loss: 2.517733012276196

Epoch: 6| Step: 1
Training loss: 2.3546037746395623
Validation loss: 2.523621856458107

Epoch: 6| Step: 2
Training loss: 2.5965291037336065
Validation loss: 2.4899482399554245

Epoch: 6| Step: 3
Training loss: 2.602362707292901
Validation loss: 2.553115737028066

Epoch: 6| Step: 4
Training loss: 2.47338547883841
Validation loss: 2.534775600376078

Epoch: 6| Step: 5
Training loss: 2.643052469495516
Validation loss: 2.5195984340727007

Epoch: 6| Step: 6
Training loss: 2.937120778380909
Validation loss: 2.5899241925303205

Epoch: 6| Step: 7
Training loss: 2.4693418348707064
Validation loss: 2.5314882237997813

Epoch: 6| Step: 8
Training loss: 2.087141396792531
Validation loss: 2.5367628776636653

Epoch: 6| Step: 9
Training loss: 2.0968110212636764
Validation loss: 2.557797043775498

Epoch: 6| Step: 10
Training loss: 2.7985836329556015
Validation loss: 2.546854085136161

Epoch: 6| Step: 11
Training loss: 2.264979408625293
Validation loss: 2.5499616206637

Epoch: 6| Step: 12
Training loss: 2.0488076440825704
Validation loss: 2.4941461312684066

Epoch: 6| Step: 13
Training loss: 2.0058680518388887
Validation loss: 2.525078537346114

Epoch: 316| Step: 0
Training loss: 2.3237671485502056
Validation loss: 2.5388600389710416

Epoch: 6| Step: 1
Training loss: 2.5227268978184227
Validation loss: 2.5489040425212854

Epoch: 6| Step: 2
Training loss: 1.9325833850157832
Validation loss: 2.5168620549223095

Epoch: 6| Step: 3
Training loss: 2.7768572288629687
Validation loss: 2.5288663229636064

Epoch: 6| Step: 4
Training loss: 2.1123810999889616
Validation loss: 2.5231558242734105

Epoch: 6| Step: 5
Training loss: 1.895885243246622
Validation loss: 2.505539157703027

Epoch: 6| Step: 6
Training loss: 1.8446338280894818
Validation loss: 2.508998222140628

Epoch: 6| Step: 7
Training loss: 2.350664239711594
Validation loss: 2.530094617826707

Epoch: 6| Step: 8
Training loss: 3.416467862431798
Validation loss: 2.5134438880949115

Epoch: 6| Step: 9
Training loss: 2.426906355489018
Validation loss: 2.5175658198767388

Epoch: 6| Step: 10
Training loss: 1.98198165139829
Validation loss: 2.536092878194509

Epoch: 6| Step: 11
Training loss: 2.449429881655669
Validation loss: 2.5064696931613826

Epoch: 6| Step: 12
Training loss: 2.6859240679712952
Validation loss: 2.5249504375274077

Epoch: 6| Step: 13
Training loss: 2.082242285820536
Validation loss: 2.546096207579772

Epoch: 317| Step: 0
Training loss: 2.213183014181216
Validation loss: 2.529248305884739

Epoch: 6| Step: 1
Training loss: 1.5147630265729235
Validation loss: 2.5240922471661236

Epoch: 6| Step: 2
Training loss: 2.4088808020416503
Validation loss: 2.5175572009594855

Epoch: 6| Step: 3
Training loss: 2.666827067875762
Validation loss: 2.5432429039783955

Epoch: 6| Step: 4
Training loss: 2.6029292918042017
Validation loss: 2.515258770557008

Epoch: 6| Step: 5
Training loss: 2.40884507186135
Validation loss: 2.5231221045524714

Epoch: 6| Step: 6
Training loss: 2.809881559706323
Validation loss: 2.5334599736375787

Epoch: 6| Step: 7
Training loss: 3.005369784791043
Validation loss: 2.5233635700611594

Epoch: 6| Step: 8
Training loss: 2.324341432555203
Validation loss: 2.508212076394014

Epoch: 6| Step: 9
Training loss: 2.3372939646281985
Validation loss: 2.53550997884649

Epoch: 6| Step: 10
Training loss: 1.8898317548758303
Validation loss: 2.574926976881814

Epoch: 6| Step: 11
Training loss: 2.507103650458402
Validation loss: 2.50099244979003

Epoch: 6| Step: 12
Training loss: 1.8567776530507503
Validation loss: 2.5074428311064056

Epoch: 6| Step: 13
Training loss: 2.693676969241862
Validation loss: 2.540888774361149

Epoch: 318| Step: 0
Training loss: 2.4935828819781456
Validation loss: 2.543875642307881

Epoch: 6| Step: 1
Training loss: 2.538088473780716
Validation loss: 2.489044554036297

Epoch: 6| Step: 2
Training loss: 2.7893526156119437
Validation loss: 2.5542791011977974

Epoch: 6| Step: 3
Training loss: 2.9797263164422394
Validation loss: 2.5155614550490766

Epoch: 6| Step: 4
Training loss: 2.282959702358759
Validation loss: 2.535022314354137

Epoch: 6| Step: 5
Training loss: 2.206126231755439
Validation loss: 2.5628542311344007

Epoch: 6| Step: 6
Training loss: 2.170929847177745
Validation loss: 2.553787063225999

Epoch: 6| Step: 7
Training loss: 2.0720708325845916
Validation loss: 2.4968227608484095

Epoch: 6| Step: 8
Training loss: 1.8715383363833646
Validation loss: 2.5439951267167378

Epoch: 6| Step: 9
Training loss: 2.0159649229631142
Validation loss: 2.5701924037370305

Epoch: 6| Step: 10
Training loss: 3.036169562940976
Validation loss: 2.53618842562473

Epoch: 6| Step: 11
Training loss: 1.7786185251786062
Validation loss: 2.5268217431056375

Epoch: 6| Step: 12
Training loss: 2.585598557493544
Validation loss: 2.527692310338733

Epoch: 6| Step: 13
Training loss: 2.5054962775226164
Validation loss: 2.55017931398393

Epoch: 319| Step: 0
Training loss: 2.302310247730883
Validation loss: 2.537495534300783

Epoch: 6| Step: 1
Training loss: 2.5999235655480133
Validation loss: 2.515188486163482

Epoch: 6| Step: 2
Training loss: 2.0771560130768063
Validation loss: 2.4826789773743516

Epoch: 6| Step: 3
Training loss: 2.060106680616919
Validation loss: 2.5467801237751333

Epoch: 6| Step: 4
Training loss: 2.690830916656357
Validation loss: 2.518939441679534

Epoch: 6| Step: 5
Training loss: 1.8835606018719102
Validation loss: 2.538759841486658

Epoch: 6| Step: 6
Training loss: 1.8957020081808218
Validation loss: 2.5338436224672027

Epoch: 6| Step: 7
Training loss: 2.544596957985462
Validation loss: 2.5489258915229454

Epoch: 6| Step: 8
Training loss: 2.0891719130379296
Validation loss: 2.5202848169386267

Epoch: 6| Step: 9
Training loss: 2.6586560291637373
Validation loss: 2.53896744896463

Epoch: 6| Step: 10
Training loss: 2.2678807046050338
Validation loss: 2.5226335910154263

Epoch: 6| Step: 11
Training loss: 3.2382467932990413
Validation loss: 2.510947526259499

Epoch: 6| Step: 12
Training loss: 2.581399398802932
Validation loss: 2.5412346450716807

Epoch: 6| Step: 13
Training loss: 1.7814796032076532
Validation loss: 2.540215064783328

Epoch: 320| Step: 0
Training loss: 2.647714341720338
Validation loss: 2.5282121540756908

Epoch: 6| Step: 1
Training loss: 2.536030716983797
Validation loss: 2.5220574332664043

Epoch: 6| Step: 2
Training loss: 2.5455507919767957
Validation loss: 2.524510266727327

Epoch: 6| Step: 3
Training loss: 2.5049962663146803
Validation loss: 2.5230663355120435

Epoch: 6| Step: 4
Training loss: 2.4985309099075943
Validation loss: 2.5720201740333293

Epoch: 6| Step: 5
Training loss: 2.540134802117251
Validation loss: 2.5405286611268783

Epoch: 6| Step: 6
Training loss: 2.289471794746113
Validation loss: 2.527684623540051

Epoch: 6| Step: 7
Training loss: 2.3560745700116468
Validation loss: 2.5123624317977087

Epoch: 6| Step: 8
Training loss: 2.1325672274118705
Validation loss: 2.5579714433718923

Epoch: 6| Step: 9
Training loss: 2.5840917724988577
Validation loss: 2.5350499517254605

Epoch: 6| Step: 10
Training loss: 1.898636670123897
Validation loss: 2.533711136164943

Epoch: 6| Step: 11
Training loss: 1.9801394698603623
Validation loss: 2.542041342110742

Epoch: 6| Step: 12
Training loss: 2.487885591918435
Validation loss: 2.516166754533957

Epoch: 6| Step: 13
Training loss: 2.240114749285362
Validation loss: 2.5285948248716745

Epoch: 321| Step: 0
Training loss: 2.53704302952851
Validation loss: 2.53082603024391

Epoch: 6| Step: 1
Training loss: 2.464836205230628
Validation loss: 2.515840654015412

Epoch: 6| Step: 2
Training loss: 2.318306720653113
Validation loss: 2.5422678731814026

Epoch: 6| Step: 3
Training loss: 1.7728087533218275
Validation loss: 2.5180665920530423

Epoch: 6| Step: 4
Training loss: 2.304648913044293
Validation loss: 2.532770378578003

Epoch: 6| Step: 5
Training loss: 3.161157098252015
Validation loss: 2.528214779354645

Epoch: 6| Step: 6
Training loss: 2.737522776609103
Validation loss: 2.4858092496031308

Epoch: 6| Step: 7
Training loss: 1.8454588390339686
Validation loss: 2.5471795586547064

Epoch: 6| Step: 8
Training loss: 2.0099644391154756
Validation loss: 2.485236440285355

Epoch: 6| Step: 9
Training loss: 2.3008869451263636
Validation loss: 2.541054314153147

Epoch: 6| Step: 10
Training loss: 2.9311822358780013
Validation loss: 2.514249509793418

Epoch: 6| Step: 11
Training loss: 1.960978777325614
Validation loss: 2.526940909802788

Epoch: 6| Step: 12
Training loss: 2.3741620241840353
Validation loss: 2.4982122089317302

Epoch: 6| Step: 13
Training loss: 1.907843252273137
Validation loss: 2.5389738364314196

Epoch: 322| Step: 0
Training loss: 2.4151020466033755
Validation loss: 2.518869245230609

Epoch: 6| Step: 1
Training loss: 2.449324269432501
Validation loss: 2.5259512290246513

Epoch: 6| Step: 2
Training loss: 1.566563776709331
Validation loss: 2.525830764685839

Epoch: 6| Step: 3
Training loss: 2.76980146157918
Validation loss: 2.5573349568445267

Epoch: 6| Step: 4
Training loss: 2.404450276096078
Validation loss: 2.5171842752047224

Epoch: 6| Step: 5
Training loss: 2.0634250300445274
Validation loss: 2.4936678556843384

Epoch: 6| Step: 6
Training loss: 2.642226777329015
Validation loss: 2.510579276806288

Epoch: 6| Step: 7
Training loss: 2.61703126355608
Validation loss: 2.5411081882313065

Epoch: 6| Step: 8
Training loss: 2.3864074603924204
Validation loss: 2.5601506692488187

Epoch: 6| Step: 9
Training loss: 2.0012559523485
Validation loss: 2.5327258510094945

Epoch: 6| Step: 10
Training loss: 2.1954609559387555
Validation loss: 2.541971972718275

Epoch: 6| Step: 11
Training loss: 2.70692661628139
Validation loss: 2.5726211821241667

Epoch: 6| Step: 12
Training loss: 2.9849325727709926
Validation loss: 2.5631508349605157

Epoch: 6| Step: 13
Training loss: 1.3145112114098458
Validation loss: 2.542927902299635

Epoch: 323| Step: 0
Training loss: 2.3326791345757893
Validation loss: 2.536434119816203

Epoch: 6| Step: 1
Training loss: 2.2339965926701884
Validation loss: 2.537751336316843

Epoch: 6| Step: 2
Training loss: 2.321310836824674
Validation loss: 2.5485784411880275

Epoch: 6| Step: 3
Training loss: 3.4732374427967647
Validation loss: 2.4943116888135464

Epoch: 6| Step: 4
Training loss: 2.4814862424679323
Validation loss: 2.485905198797285

Epoch: 6| Step: 5
Training loss: 2.5812559594184856
Validation loss: 2.5128449574145115

Epoch: 6| Step: 6
Training loss: 2.0529444738535503
Validation loss: 2.5632926801368594

Epoch: 6| Step: 7
Training loss: 2.1332437267955573
Validation loss: 2.5123743134277614

Epoch: 6| Step: 8
Training loss: 2.0394754393591072
Validation loss: 2.542088664375526

Epoch: 6| Step: 9
Training loss: 2.136428618042324
Validation loss: 2.4712512459735385

Epoch: 6| Step: 10
Training loss: 2.2279027284930897
Validation loss: 2.542828464975116

Epoch: 6| Step: 11
Training loss: 1.9199059797154823
Validation loss: 2.5508492109958967

Epoch: 6| Step: 12
Training loss: 2.2124770772145204
Validation loss: 2.528150996168549

Epoch: 6| Step: 13
Training loss: 2.7800141063853725
Validation loss: 2.5481500293376778

Epoch: 324| Step: 0
Training loss: 2.423526127250402
Validation loss: 2.530632972839562

Epoch: 6| Step: 1
Training loss: 2.245890573351307
Validation loss: 2.535206274327842

Epoch: 6| Step: 2
Training loss: 3.014538504990654
Validation loss: 2.5059534516440065

Epoch: 6| Step: 3
Training loss: 2.329581809764626
Validation loss: 2.4766560322660984

Epoch: 6| Step: 4
Training loss: 2.5787719839530108
Validation loss: 2.48577802442564

Epoch: 6| Step: 5
Training loss: 2.7179972165495574
Validation loss: 2.5643304269502303

Epoch: 6| Step: 6
Training loss: 2.2413679479316055
Validation loss: 2.5064147226929316

Epoch: 6| Step: 7
Training loss: 1.9706547923063702
Validation loss: 2.5288941908236513

Epoch: 6| Step: 8
Training loss: 2.5643317026066756
Validation loss: 2.547629748787071

Epoch: 6| Step: 9
Training loss: 2.3796643081306383
Validation loss: 2.527903424974296

Epoch: 6| Step: 10
Training loss: 1.902562937553909
Validation loss: 2.5534424736074337

Epoch: 6| Step: 11
Training loss: 2.006713804069447
Validation loss: 2.4921036896905506

Epoch: 6| Step: 12
Training loss: 2.3544334553023107
Validation loss: 2.501216124002708

Epoch: 6| Step: 13
Training loss: 2.3781714095250317
Validation loss: 2.520575714032647

Epoch: 325| Step: 0
Training loss: 2.156358246228553
Validation loss: 2.521382730553455

Epoch: 6| Step: 1
Training loss: 2.2161355532009166
Validation loss: 2.5540883204332774

Epoch: 6| Step: 2
Training loss: 2.8620412933761514
Validation loss: 2.554164911673799

Epoch: 6| Step: 3
Training loss: 2.365995906642491
Validation loss: 2.513434599199098

Epoch: 6| Step: 4
Training loss: 3.090267803352251
Validation loss: 2.530475906753871

Epoch: 6| Step: 5
Training loss: 2.047741422661082
Validation loss: 2.499822524139287

Epoch: 6| Step: 6
Training loss: 2.45742265891986
Validation loss: 2.536522009029797

Epoch: 6| Step: 7
Training loss: 1.8192203503439237
Validation loss: 2.507122824233579

Epoch: 6| Step: 8
Training loss: 2.5185456943240463
Validation loss: 2.5438549093786276

Epoch: 6| Step: 9
Training loss: 2.2685397636589397
Validation loss: 2.5350394344108422

Epoch: 6| Step: 10
Training loss: 2.3791359475775367
Validation loss: 2.511444707111922

Epoch: 6| Step: 11
Training loss: 2.3728104587417036
Validation loss: 2.539414211805341

Epoch: 6| Step: 12
Training loss: 2.4285988285218005
Validation loss: 2.5036516223764096

Epoch: 6| Step: 13
Training loss: 2.006581207221456
Validation loss: 2.533920960191551

Epoch: 326| Step: 0
Training loss: 2.7714482440165997
Validation loss: 2.540702121050828

Epoch: 6| Step: 1
Training loss: 2.4516229586807925
Validation loss: 2.537778544942737

Epoch: 6| Step: 2
Training loss: 3.515495467448768
Validation loss: 2.516711272165134

Epoch: 6| Step: 3
Training loss: 2.068209288773844
Validation loss: 2.5344592094833183

Epoch: 6| Step: 4
Training loss: 2.378003881029406
Validation loss: 2.4921529713950363

Epoch: 6| Step: 5
Training loss: 2.4896546889630855
Validation loss: 2.53901494564584

Epoch: 6| Step: 6
Training loss: 1.90870714510491
Validation loss: 2.5042126878741695

Epoch: 6| Step: 7
Training loss: 1.6201664857911695
Validation loss: 2.5098852981426525

Epoch: 6| Step: 8
Training loss: 1.9247101131993058
Validation loss: 2.506897891517615

Epoch: 6| Step: 9
Training loss: 2.066096089671972
Validation loss: 2.595738083548229

Epoch: 6| Step: 10
Training loss: 1.5955900154507128
Validation loss: 2.5107391365444256

Epoch: 6| Step: 11
Training loss: 2.775682513185084
Validation loss: 2.5499011417483124

Epoch: 6| Step: 12
Training loss: 2.0048970827244212
Validation loss: 2.514015044868036

Epoch: 6| Step: 13
Training loss: 3.218566741170766
Validation loss: 2.503494366271336

Epoch: 327| Step: 0
Training loss: 2.5477288809877883
Validation loss: 2.5355109424195312

Epoch: 6| Step: 1
Training loss: 2.769343833877107
Validation loss: 2.5432772842965377

Epoch: 6| Step: 2
Training loss: 2.2307354288297567
Validation loss: 2.5179563651713304

Epoch: 6| Step: 3
Training loss: 2.804118234141514
Validation loss: 2.5262922803102335

Epoch: 6| Step: 4
Training loss: 2.037124355769461
Validation loss: 2.5615070459105045

Epoch: 6| Step: 5
Training loss: 1.908813004268913
Validation loss: 2.4932461416076994

Epoch: 6| Step: 6
Training loss: 2.0635027904080308
Validation loss: 2.525421667979262

Epoch: 6| Step: 7
Training loss: 2.07357438993151
Validation loss: 2.4949601214770913

Epoch: 6| Step: 8
Training loss: 2.816881369885508
Validation loss: 2.5051914180661847

Epoch: 6| Step: 9
Training loss: 2.2414206014062703
Validation loss: 2.5541923007629928

Epoch: 6| Step: 10
Training loss: 1.8578148715916054
Validation loss: 2.527939915391873

Epoch: 6| Step: 11
Training loss: 2.686492553666539
Validation loss: 2.5299914750978543

Epoch: 6| Step: 12
Training loss: 2.6385883851752694
Validation loss: 2.5172039649314746

Epoch: 6| Step: 13
Training loss: 1.5337033761670749
Validation loss: 2.5347888090638455

Epoch: 328| Step: 0
Training loss: 3.293199994892485
Validation loss: 2.501145029878838

Epoch: 6| Step: 1
Training loss: 2.5819392801084597
Validation loss: 2.5353783511251864

Epoch: 6| Step: 2
Training loss: 1.7337617907023486
Validation loss: 2.5040891829215512

Epoch: 6| Step: 3
Training loss: 1.91034608611416
Validation loss: 2.5050687565343903

Epoch: 6| Step: 4
Training loss: 2.25437459591914
Validation loss: 2.5480708476655467

Epoch: 6| Step: 5
Training loss: 2.7388347768263457
Validation loss: 2.464363024873881

Epoch: 6| Step: 6
Training loss: 1.6631363274763022
Validation loss: 2.4916794926152104

Epoch: 6| Step: 7
Training loss: 2.4054924899618157
Validation loss: 2.5644220371641135

Epoch: 6| Step: 8
Training loss: 1.9732988274087653
Validation loss: 2.5205212291985917

Epoch: 6| Step: 9
Training loss: 2.806191044041581
Validation loss: 2.5025854882260785

Epoch: 6| Step: 10
Training loss: 1.8420965895412718
Validation loss: 2.52393516053279

Epoch: 6| Step: 11
Training loss: 2.8641817308447255
Validation loss: 2.4981641468560922

Epoch: 6| Step: 12
Training loss: 2.328167908548326
Validation loss: 2.5390368757067145

Epoch: 6| Step: 13
Training loss: 2.333742264835429
Validation loss: 2.4836800474514154

Epoch: 329| Step: 0
Training loss: 2.7784004615526285
Validation loss: 2.487178600483936

Epoch: 6| Step: 1
Training loss: 2.1373698412434
Validation loss: 2.521778189622644

Epoch: 6| Step: 2
Training loss: 1.9654803297115833
Validation loss: 2.528291910030251

Epoch: 6| Step: 3
Training loss: 2.155280517234634
Validation loss: 2.530433346820839

Epoch: 6| Step: 4
Training loss: 2.4034802237534887
Validation loss: 2.5321931660525583

Epoch: 6| Step: 5
Training loss: 1.9458328225286932
Validation loss: 2.5429954259625154

Epoch: 6| Step: 6
Training loss: 2.922225849486188
Validation loss: 2.5334114640962198

Epoch: 6| Step: 7
Training loss: 2.596222124533217
Validation loss: 2.5244748559876293

Epoch: 6| Step: 8
Training loss: 2.2572067134440292
Validation loss: 2.544260074353716

Epoch: 6| Step: 9
Training loss: 2.511689134010856
Validation loss: 2.5311289456659307

Epoch: 6| Step: 10
Training loss: 2.5290815696247186
Validation loss: 2.5293548198522027

Epoch: 6| Step: 11
Training loss: 2.223941478322918
Validation loss: 2.538517133453585

Epoch: 6| Step: 12
Training loss: 2.105576776058163
Validation loss: 2.5003378650052066

Epoch: 6| Step: 13
Training loss: 2.4796911267240236
Validation loss: 2.552021778428317

Epoch: 330| Step: 0
Training loss: 2.7079160711222547
Validation loss: 2.525523816787103

Epoch: 6| Step: 1
Training loss: 2.958413208075707
Validation loss: 2.5199880203897282

Epoch: 6| Step: 2
Training loss: 1.777147755977518
Validation loss: 2.5183758348351475

Epoch: 6| Step: 3
Training loss: 2.2885480178886097
Validation loss: 2.499518442432652

Epoch: 6| Step: 4
Training loss: 2.0671821392332936
Validation loss: 2.546312368778711

Epoch: 6| Step: 5
Training loss: 2.5444284869845366
Validation loss: 2.539272992391222

Epoch: 6| Step: 6
Training loss: 2.420585239673839
Validation loss: 2.48048499498255

Epoch: 6| Step: 7
Training loss: 2.261074573972504
Validation loss: 2.488713137140205

Epoch: 6| Step: 8
Training loss: 2.8895621330365455
Validation loss: 2.5335303997674115

Epoch: 6| Step: 9
Training loss: 1.7677548146323239
Validation loss: 2.5557001981787435

Epoch: 6| Step: 10
Training loss: 2.7539299713628176
Validation loss: 2.5714146268134126

Epoch: 6| Step: 11
Training loss: 2.1587050462734925
Validation loss: 2.546823314467177

Epoch: 6| Step: 12
Training loss: 1.8936410601916485
Validation loss: 2.528507103186575

Epoch: 6| Step: 13
Training loss: 2.2294593571348114
Validation loss: 2.5379791669743437

Epoch: 331| Step: 0
Training loss: 2.7803837144994064
Validation loss: 2.53440150701792

Epoch: 6| Step: 1
Training loss: 1.9734627161053513
Validation loss: 2.511035873630427

Epoch: 6| Step: 2
Training loss: 2.4184201116610855
Validation loss: 2.5521220810547716

Epoch: 6| Step: 3
Training loss: 1.9479491967189613
Validation loss: 2.5670317758788306

Epoch: 6| Step: 4
Training loss: 2.7062508915642027
Validation loss: 2.5300095903056845

Epoch: 6| Step: 5
Training loss: 1.8589183984035496
Validation loss: 2.5177700645101684

Epoch: 6| Step: 6
Training loss: 2.476859763803914
Validation loss: 2.486049388915925

Epoch: 6| Step: 7
Training loss: 1.8965319579388902
Validation loss: 2.5155630570937366

Epoch: 6| Step: 8
Training loss: 2.517341549384454
Validation loss: 2.514495794894479

Epoch: 6| Step: 9
Training loss: 2.798234832768917
Validation loss: 2.5434318382813674

Epoch: 6| Step: 10
Training loss: 2.254070837930172
Validation loss: 2.5249128235857734

Epoch: 6| Step: 11
Training loss: 2.5597588368906066
Validation loss: 2.5181117748734723

Epoch: 6| Step: 12
Training loss: 2.0664673996298393
Validation loss: 2.531453954778961

Epoch: 6| Step: 13
Training loss: 2.962312806730071
Validation loss: 2.484179916766176

Epoch: 332| Step: 0
Training loss: 1.8017613852860919
Validation loss: 2.5364271164841043

Epoch: 6| Step: 1
Training loss: 1.9783514920659746
Validation loss: 2.5227745661248493

Epoch: 6| Step: 2
Training loss: 2.569869910632533
Validation loss: 2.529029045030739

Epoch: 6| Step: 3
Training loss: 2.299294890252246
Validation loss: 2.5031310767977377

Epoch: 6| Step: 4
Training loss: 3.202523821728006
Validation loss: 2.5738879545888342

Epoch: 6| Step: 5
Training loss: 2.9256163118979495
Validation loss: 2.5061864689218925

Epoch: 6| Step: 6
Training loss: 2.2033772425621474
Validation loss: 2.4969623969750065

Epoch: 6| Step: 7
Training loss: 2.395388774814503
Validation loss: 2.4886053068167073

Epoch: 6| Step: 8
Training loss: 2.135242014620014
Validation loss: 2.5095600700129337

Epoch: 6| Step: 9
Training loss: 2.0556626707239944
Validation loss: 2.5604394100606256

Epoch: 6| Step: 10
Training loss: 2.1861989102901878
Validation loss: 2.5044516214777066

Epoch: 6| Step: 11
Training loss: 2.3404342594020897
Validation loss: 2.5583603338615357

Epoch: 6| Step: 12
Training loss: 2.4390973457032024
Validation loss: 2.515839107173534

Epoch: 6| Step: 13
Training loss: 2.12780834200393
Validation loss: 2.5254750197974474

Epoch: 333| Step: 0
Training loss: 2.1858621187553147
Validation loss: 2.5219281926784207

Epoch: 6| Step: 1
Training loss: 2.5690423209749675
Validation loss: 2.5358881202787926

Epoch: 6| Step: 2
Training loss: 1.6052528848804832
Validation loss: 2.54944213416618

Epoch: 6| Step: 3
Training loss: 2.3573984114925954
Validation loss: 2.4976779095334254

Epoch: 6| Step: 4
Training loss: 2.7244776758884988
Validation loss: 2.574065949931213

Epoch: 6| Step: 5
Training loss: 2.1100927191516368
Validation loss: 2.525055622814975

Epoch: 6| Step: 6
Training loss: 2.0618487399147014
Validation loss: 2.4849062776119193

Epoch: 6| Step: 7
Training loss: 2.2934342127983176
Validation loss: 2.529807145040012

Epoch: 6| Step: 8
Training loss: 2.581332436785398
Validation loss: 2.5125557023295224

Epoch: 6| Step: 9
Training loss: 1.811344172818171
Validation loss: 2.5502103980134128

Epoch: 6| Step: 10
Training loss: 2.3719521089064597
Validation loss: 2.494694093730647

Epoch: 6| Step: 11
Training loss: 2.959412838976925
Validation loss: 2.5506525652794716

Epoch: 6| Step: 12
Training loss: 2.3700202133024946
Validation loss: 2.4867599678477337

Epoch: 6| Step: 13
Training loss: 2.8926991212666344
Validation loss: 2.5196188344610952

Epoch: 334| Step: 0
Training loss: 1.9435279351364523
Validation loss: 2.5149208101818514

Epoch: 6| Step: 1
Training loss: 2.732673071766358
Validation loss: 2.5765145630150843

Epoch: 6| Step: 2
Training loss: 2.213370558165235
Validation loss: 2.520121426502909

Epoch: 6| Step: 3
Training loss: 2.3693229686887465
Validation loss: 2.535544841151178

Epoch: 6| Step: 4
Training loss: 2.556881956595928
Validation loss: 2.530126843304346

Epoch: 6| Step: 5
Training loss: 2.417857720656124
Validation loss: 2.512989701776334

Epoch: 6| Step: 6
Training loss: 2.0541966073083295
Validation loss: 2.5367080886585303

Epoch: 6| Step: 7
Training loss: 2.35507699925593
Validation loss: 2.5547634601542546

Epoch: 6| Step: 8
Training loss: 2.230722068925554
Validation loss: 2.5150542750956917

Epoch: 6| Step: 9
Training loss: 2.0495743751627384
Validation loss: 2.4897879472976903

Epoch: 6| Step: 10
Training loss: 1.886318314353597
Validation loss: 2.544063376817573

Epoch: 6| Step: 11
Training loss: 2.805043742472883
Validation loss: 2.528359435211034

Epoch: 6| Step: 12
Training loss: 2.686994815629757
Validation loss: 2.477009584968109

Epoch: 6| Step: 13
Training loss: 2.620957031322773
Validation loss: 2.5646608847366044

Epoch: 335| Step: 0
Training loss: 2.4329916513835195
Validation loss: 2.477605803160656

Epoch: 6| Step: 1
Training loss: 2.3160838631318104
Validation loss: 2.4801318302123576

Epoch: 6| Step: 2
Training loss: 2.5076683692275608
Validation loss: 2.4930894759704296

Epoch: 6| Step: 3
Training loss: 1.992100971811135
Validation loss: 2.5514039981067684

Epoch: 6| Step: 4
Training loss: 2.548819799884632
Validation loss: 2.515829313543236

Epoch: 6| Step: 5
Training loss: 2.7588844701422865
Validation loss: 2.527937212754083

Epoch: 6| Step: 6
Training loss: 2.4606768757702997
Validation loss: 2.5083780495320815

Epoch: 6| Step: 7
Training loss: 1.968217505240201
Validation loss: 2.516056509631917

Epoch: 6| Step: 8
Training loss: 2.1175854298406334
Validation loss: 2.5192749534109433

Epoch: 6| Step: 9
Training loss: 2.393987148878235
Validation loss: 2.517422491920517

Epoch: 6| Step: 10
Training loss: 2.914989334270143
Validation loss: 2.550901150625354

Epoch: 6| Step: 11
Training loss: 2.183123324634118
Validation loss: 2.52409439225679

Epoch: 6| Step: 12
Training loss: 2.3851919831873865
Validation loss: 2.509052272623428

Epoch: 6| Step: 13
Training loss: 1.159326635621788
Validation loss: 2.520786003930216

Epoch: 336| Step: 0
Training loss: 1.9641570755092244
Validation loss: 2.529834444173305

Epoch: 6| Step: 1
Training loss: 2.533956893506078
Validation loss: 2.519075581379166

Epoch: 6| Step: 2
Training loss: 2.143723966479362
Validation loss: 2.551624639767329

Epoch: 6| Step: 3
Training loss: 2.2940382274844846
Validation loss: 2.5330188910126967

Epoch: 6| Step: 4
Training loss: 2.7870818299160054
Validation loss: 2.535971870543521

Epoch: 6| Step: 5
Training loss: 1.6984642019690859
Validation loss: 2.5257085436953255

Epoch: 6| Step: 6
Training loss: 2.6413640900942092
Validation loss: 2.5192753502790057

Epoch: 6| Step: 7
Training loss: 1.7444227445404172
Validation loss: 2.5578255325548547

Epoch: 6| Step: 8
Training loss: 2.260059861766026
Validation loss: 2.5520317917910305

Epoch: 6| Step: 9
Training loss: 2.3640850623264984
Validation loss: 2.5395812882345656

Epoch: 6| Step: 10
Training loss: 2.422415593018228
Validation loss: 2.5339437209505706

Epoch: 6| Step: 11
Training loss: 2.84222153050296
Validation loss: 2.5396128169323515

Epoch: 6| Step: 12
Training loss: 2.6233206327034777
Validation loss: 2.5475586869949356

Epoch: 6| Step: 13
Training loss: 2.2605626869208963
Validation loss: 2.55158276716502

Epoch: 337| Step: 0
Training loss: 1.993931803291825
Validation loss: 2.569344601238277

Epoch: 6| Step: 1
Training loss: 2.495811004623704
Validation loss: 2.5172467887661036

Epoch: 6| Step: 2
Training loss: 2.5805612785988554
Validation loss: 2.5243351066715407

Epoch: 6| Step: 3
Training loss: 2.2576304431628103
Validation loss: 2.487326239139299

Epoch: 6| Step: 4
Training loss: 1.747521825607721
Validation loss: 2.5118985879840126

Epoch: 6| Step: 5
Training loss: 3.0138212353622937
Validation loss: 2.519479970178836

Epoch: 6| Step: 6
Training loss: 2.0330733112152237
Validation loss: 2.521322003842287

Epoch: 6| Step: 7
Training loss: 3.1210940555577134
Validation loss: 2.519465536507201

Epoch: 6| Step: 8
Training loss: 2.5520591163783783
Validation loss: 2.5216271711492593

Epoch: 6| Step: 9
Training loss: 1.6145738498860434
Validation loss: 2.5198501856920026

Epoch: 6| Step: 10
Training loss: 1.4581737794332372
Validation loss: 2.511209865971256

Epoch: 6| Step: 11
Training loss: 1.9395938142301417
Validation loss: 2.557947522361768

Epoch: 6| Step: 12
Training loss: 2.08110830380251
Validation loss: 2.5143819596522063

Epoch: 6| Step: 13
Training loss: 3.376877580352594
Validation loss: 2.487219138739886

Epoch: 338| Step: 0
Training loss: 3.094264132232336
Validation loss: 2.57674071735239

Epoch: 6| Step: 1
Training loss: 1.9818717606685539
Validation loss: 2.507733407899499

Epoch: 6| Step: 2
Training loss: 2.336460731109418
Validation loss: 2.483053650446724

Epoch: 6| Step: 3
Training loss: 1.83057024841638
Validation loss: 2.5290907959837186

Epoch: 6| Step: 4
Training loss: 2.6472802798814152
Validation loss: 2.5631757856060857

Epoch: 6| Step: 5
Training loss: 2.675508760931606
Validation loss: 2.5534792938137922

Epoch: 6| Step: 6
Training loss: 1.6530966339850568
Validation loss: 2.486658666947495

Epoch: 6| Step: 7
Training loss: 2.270118939312808
Validation loss: 2.507488590800515

Epoch: 6| Step: 8
Training loss: 2.21041853242796
Validation loss: 2.5352679890880387

Epoch: 6| Step: 9
Training loss: 1.926581102514989
Validation loss: 2.479076665821786

Epoch: 6| Step: 10
Training loss: 2.561395081282982
Validation loss: 2.560309197156499

Epoch: 6| Step: 11
Training loss: 1.9217372937993877
Validation loss: 2.484340655362323

Epoch: 6| Step: 12
Training loss: 2.852498107428094
Validation loss: 2.5079351346692538

Epoch: 6| Step: 13
Training loss: 2.6314534378237764
Validation loss: 2.514469511916605

Epoch: 339| Step: 0
Training loss: 2.0935491209314696
Validation loss: 2.512922747049103

Epoch: 6| Step: 1
Training loss: 2.2525935589902573
Validation loss: 2.5152249459854867

Epoch: 6| Step: 2
Training loss: 1.8355057878395216
Validation loss: 2.5189043332675576

Epoch: 6| Step: 3
Training loss: 2.2301993666683457
Validation loss: 2.4982028736629522

Epoch: 6| Step: 4
Training loss: 3.2572849496044896
Validation loss: 2.5432999129390983

Epoch: 6| Step: 5
Training loss: 2.256462564784626
Validation loss: 2.5776246282291564

Epoch: 6| Step: 6
Training loss: 2.237911813016079
Validation loss: 2.5533696508843566

Epoch: 6| Step: 7
Training loss: 2.272650828376354
Validation loss: 2.537969310292215

Epoch: 6| Step: 8
Training loss: 2.4533125690258735
Validation loss: 2.5514074947945935

Epoch: 6| Step: 9
Training loss: 2.3985842184613317
Validation loss: 2.5474796167608593

Epoch: 6| Step: 10
Training loss: 2.529727902010259
Validation loss: 2.559151545415969

Epoch: 6| Step: 11
Training loss: 2.269579573361405
Validation loss: 2.5036204773968613

Epoch: 6| Step: 12
Training loss: 2.162973067738074
Validation loss: 2.553095286989892

Epoch: 6| Step: 13
Training loss: 2.600735369638869
Validation loss: 2.505170287210332

Epoch: 340| Step: 0
Training loss: 1.9097493680315487
Validation loss: 2.537264312777781

Epoch: 6| Step: 1
Training loss: 1.9031327822170652
Validation loss: 2.5334454910998523

Epoch: 6| Step: 2
Training loss: 1.9687107627228388
Validation loss: 2.5219102212054767

Epoch: 6| Step: 3
Training loss: 1.9191295583861288
Validation loss: 2.5595469703330953

Epoch: 6| Step: 4
Training loss: 1.5080734580536401
Validation loss: 2.5459319774003224

Epoch: 6| Step: 5
Training loss: 2.5435561572550083
Validation loss: 2.5312400300135836

Epoch: 6| Step: 6
Training loss: 2.162973067738074
Validation loss: 2.5407794084248425

Epoch: 6| Step: 7
Training loss: 2.7873972989309226
Validation loss: 2.534767547695139

Epoch: 6| Step: 8
Training loss: 3.3826633623122317
Validation loss: 2.543825245264606

Epoch: 6| Step: 9
Training loss: 2.174822227842378
Validation loss: 2.5354532153560974

Epoch: 6| Step: 10
Training loss: 2.5492082451227778
Validation loss: 2.5101830144787445

Epoch: 6| Step: 11
Training loss: 1.6888597449637965
Validation loss: 2.502927796722794

Epoch: 6| Step: 12
Training loss: 2.232780047341683
Validation loss: 2.499097991569752

Epoch: 6| Step: 13
Training loss: 3.3766993730121553
Validation loss: 2.5209309345396513

Epoch: 341| Step: 0
Training loss: 2.4719459031614623
Validation loss: 2.5435837845482467

Epoch: 6| Step: 1
Training loss: 2.5862238515100793
Validation loss: 2.541068705907743

Epoch: 6| Step: 2
Training loss: 2.2952644774189466
Validation loss: 2.5533955103574764

Epoch: 6| Step: 3
Training loss: 2.2954755399277813
Validation loss: 2.5425347416413553

Epoch: 6| Step: 4
Training loss: 1.4340557632404431
Validation loss: 2.494976868581105

Epoch: 6| Step: 5
Training loss: 2.160315288738899
Validation loss: 2.527454588898306

Epoch: 6| Step: 6
Training loss: 1.659907279699778
Validation loss: 2.496923766947912

Epoch: 6| Step: 7
Training loss: 3.0131300651161146
Validation loss: 2.5437450299941715

Epoch: 6| Step: 8
Training loss: 1.9492285351708298
Validation loss: 2.531333991829282

Epoch: 6| Step: 9
Training loss: 2.1141109847255666
Validation loss: 2.5461044066776775

Epoch: 6| Step: 10
Training loss: 1.9974201371671814
Validation loss: 2.531704617613297

Epoch: 6| Step: 11
Training loss: 2.6949749279094504
Validation loss: 2.5012617219036306

Epoch: 6| Step: 12
Training loss: 2.6274679480720127
Validation loss: 2.5510502180513743

Epoch: 6| Step: 13
Training loss: 3.3524377920365795
Validation loss: 2.557646512232735

Epoch: 342| Step: 0
Training loss: 2.015846417707747
Validation loss: 2.519693929869233

Epoch: 6| Step: 1
Training loss: 2.3342562848280557
Validation loss: 2.5292260269059557

Epoch: 6| Step: 2
Training loss: 2.3587849870845394
Validation loss: 2.537492774150103

Epoch: 6| Step: 3
Training loss: 2.3380368600489927
Validation loss: 2.568865184089216

Epoch: 6| Step: 4
Training loss: 2.5253329406900447
Validation loss: 2.529498388653693

Epoch: 6| Step: 5
Training loss: 2.309641953500399
Validation loss: 2.5244459603747664

Epoch: 6| Step: 6
Training loss: 2.4113462901331117
Validation loss: 2.5475078334818937

Epoch: 6| Step: 7
Training loss: 2.3564022106113014
Validation loss: 2.5263532822907813

Epoch: 6| Step: 8
Training loss: 2.096497284799897
Validation loss: 2.4909282831781216

Epoch: 6| Step: 9
Training loss: 2.978913107443179
Validation loss: 2.522164110389416

Epoch: 6| Step: 10
Training loss: 2.4100574244388904
Validation loss: 2.513736344281285

Epoch: 6| Step: 11
Training loss: 2.2025918518381227
Validation loss: 2.5008797256499977

Epoch: 6| Step: 12
Training loss: 1.8127496481660863
Validation loss: 2.5451049864731004

Epoch: 6| Step: 13
Training loss: 1.4367017602875365
Validation loss: 2.5339672251122964

Epoch: 343| Step: 0
Training loss: 2.7683469664138265
Validation loss: 2.5616999972330183

Epoch: 6| Step: 1
Training loss: 2.598703549012591
Validation loss: 2.5234168043665473

Epoch: 6| Step: 2
Training loss: 2.008040478302168
Validation loss: 2.4948917175066194

Epoch: 6| Step: 3
Training loss: 2.2208419394072876
Validation loss: 2.562335903737153

Epoch: 6| Step: 4
Training loss: 1.7776843069591237
Validation loss: 2.5646943202122894

Epoch: 6| Step: 5
Training loss: 1.9353628369398619
Validation loss: 2.545888767558963

Epoch: 6| Step: 6
Training loss: 2.8899367054274654
Validation loss: 2.4913027292179892

Epoch: 6| Step: 7
Training loss: 1.8842308432126664
Validation loss: 2.5351964918073393

Epoch: 6| Step: 8
Training loss: 2.0101689266749525
Validation loss: 2.553471902004685

Epoch: 6| Step: 9
Training loss: 2.5507122690591366
Validation loss: 2.5466067164478026

Epoch: 6| Step: 10
Training loss: 2.6983970793682523
Validation loss: 2.50434716261644

Epoch: 6| Step: 11
Training loss: 2.3668635684921657
Validation loss: 2.527639597639676

Epoch: 6| Step: 12
Training loss: 2.359522732789471
Validation loss: 2.547527091599478

Epoch: 6| Step: 13
Training loss: 1.7274059462165492
Validation loss: 2.537786889105343

Epoch: 344| Step: 0
Training loss: 2.488472588337371
Validation loss: 2.5006056144336877

Epoch: 6| Step: 1
Training loss: 2.487700533810863
Validation loss: 2.5322397895460793

Epoch: 6| Step: 2
Training loss: 2.9307446009530795
Validation loss: 2.49471455493248

Epoch: 6| Step: 3
Training loss: 2.6381555330181055
Validation loss: 2.4996082327090985

Epoch: 6| Step: 4
Training loss: 1.534861839736948
Validation loss: 2.5252829136634327

Epoch: 6| Step: 5
Training loss: 2.4551270691720317
Validation loss: 2.5222148487713967

Epoch: 6| Step: 6
Training loss: 2.2367735040260532
Validation loss: 2.548579239880472

Epoch: 6| Step: 7
Training loss: 2.130046126354246
Validation loss: 2.5278587680670155

Epoch: 6| Step: 8
Training loss: 2.0768495245079097
Validation loss: 2.561520627177538

Epoch: 6| Step: 9
Training loss: 2.3582486122828494
Validation loss: 2.54037829108677

Epoch: 6| Step: 10
Training loss: 2.2312239188585323
Validation loss: 2.4810931534045007

Epoch: 6| Step: 11
Training loss: 2.640399629805709
Validation loss: 2.517758115685028

Epoch: 6| Step: 12
Training loss: 1.7758239284476973
Validation loss: 2.4979966720057996

Epoch: 6| Step: 13
Training loss: 1.7709647260800814
Validation loss: 2.536781252240821

Epoch: 345| Step: 0
Training loss: 2.1025424643638293
Validation loss: 2.533493496063818

Epoch: 6| Step: 1
Training loss: 2.2148060736571704
Validation loss: 2.543844663280575

Epoch: 6| Step: 2
Training loss: 1.6992186096892903
Validation loss: 2.5583395389461447

Epoch: 6| Step: 3
Training loss: 2.1061831591043605
Validation loss: 2.523459772176595

Epoch: 6| Step: 4
Training loss: 2.470440056708825
Validation loss: 2.477525156417262

Epoch: 6| Step: 5
Training loss: 2.963072638556933
Validation loss: 2.4715747511564183

Epoch: 6| Step: 6
Training loss: 2.251769535624137
Validation loss: 2.4759321781254955

Epoch: 6| Step: 7
Training loss: 2.1379782470724145
Validation loss: 2.565332690355062

Epoch: 6| Step: 8
Training loss: 2.4595689631636892
Validation loss: 2.4764984734363065

Epoch: 6| Step: 9
Training loss: 2.2626074328127563
Validation loss: 2.490846362354904

Epoch: 6| Step: 10
Training loss: 2.213631972920441
Validation loss: 2.5287522470513335

Epoch: 6| Step: 11
Training loss: 2.347180005608698
Validation loss: 2.5206978528532575

Epoch: 6| Step: 12
Training loss: 2.7729553663236355
Validation loss: 2.4696212244788733

Epoch: 6| Step: 13
Training loss: 2.460548588130952
Validation loss: 2.5185036840101818

Epoch: 346| Step: 0
Training loss: 2.536557508301344
Validation loss: 2.528072145388845

Epoch: 6| Step: 1
Training loss: 2.2254472047383826
Validation loss: 2.5251325474431763

Epoch: 6| Step: 2
Training loss: 2.3055118444300233
Validation loss: 2.53283823882373

Epoch: 6| Step: 3
Training loss: 2.638882304623962
Validation loss: 2.5111225803472808

Epoch: 6| Step: 4
Training loss: 2.0377252512732213
Validation loss: 2.538145403575769

Epoch: 6| Step: 5
Training loss: 1.983467435176653
Validation loss: 2.5217788951432007

Epoch: 6| Step: 6
Training loss: 2.2074705513775204
Validation loss: 2.532948831216898

Epoch: 6| Step: 7
Training loss: 2.898158468711748
Validation loss: 2.5427283684162085

Epoch: 6| Step: 8
Training loss: 1.93628038967411
Validation loss: 2.5039666670418885

Epoch: 6| Step: 9
Training loss: 2.832634970672218
Validation loss: 2.5416511484402307

Epoch: 6| Step: 10
Training loss: 2.346338698956774
Validation loss: 2.5289516722570085

Epoch: 6| Step: 11
Training loss: 2.2426597815569327
Validation loss: 2.4884149579184327

Epoch: 6| Step: 12
Training loss: 2.234425257404354
Validation loss: 2.5227410760529616

Epoch: 6| Step: 13
Training loss: 1.792688883960941
Validation loss: 2.5250449136097908

Epoch: 347| Step: 0
Training loss: 1.9674616640032685
Validation loss: 2.5711296551238103

Epoch: 6| Step: 1
Training loss: 2.980372116102191
Validation loss: 2.488572909884544

Epoch: 6| Step: 2
Training loss: 2.932142851301664
Validation loss: 2.5338801659490415

Epoch: 6| Step: 3
Training loss: 3.06955187356865
Validation loss: 2.5336102544252865

Epoch: 6| Step: 4
Training loss: 1.822509356816138
Validation loss: 2.5473524771284954

Epoch: 6| Step: 5
Training loss: 2.527477042738193
Validation loss: 2.5236267498397904

Epoch: 6| Step: 6
Training loss: 2.0071963541167297
Validation loss: 2.477580038374484

Epoch: 6| Step: 7
Training loss: 1.7627784941851792
Validation loss: 2.5068897037799585

Epoch: 6| Step: 8
Training loss: 2.004870682729381
Validation loss: 2.5291012924213057

Epoch: 6| Step: 9
Training loss: 1.9111213352363594
Validation loss: 2.5611882888081894

Epoch: 6| Step: 10
Training loss: 2.1885554356045804
Validation loss: 2.493603805684227

Epoch: 6| Step: 11
Training loss: 2.399384252714703
Validation loss: 2.538041941565374

Epoch: 6| Step: 12
Training loss: 2.013026965791179
Validation loss: 2.542818037299861

Epoch: 6| Step: 13
Training loss: 1.852597361900595
Validation loss: 2.546623064015878

Epoch: 348| Step: 0
Training loss: 1.888835848580951
Validation loss: 2.5093521484295658

Epoch: 6| Step: 1
Training loss: 2.2349877217614775
Validation loss: 2.5119704983447617

Epoch: 6| Step: 2
Training loss: 2.8474561414279864
Validation loss: 2.5241524156583544

Epoch: 6| Step: 3
Training loss: 2.1811892350704034
Validation loss: 2.503696358815653

Epoch: 6| Step: 4
Training loss: 1.9622949408046797
Validation loss: 2.5393840485852324

Epoch: 6| Step: 5
Training loss: 1.7986321523737454
Validation loss: 2.5249781902179094

Epoch: 6| Step: 6
Training loss: 2.493988629924733
Validation loss: 2.5313627156418685

Epoch: 6| Step: 7
Training loss: 2.1704667829543496
Validation loss: 2.5788879170609125

Epoch: 6| Step: 8
Training loss: 2.013270696124598
Validation loss: 2.558546512571312

Epoch: 6| Step: 9
Training loss: 2.7525693854543833
Validation loss: 2.474785308873508

Epoch: 6| Step: 10
Training loss: 2.5616671441532137
Validation loss: 2.484618866551976

Epoch: 6| Step: 11
Training loss: 2.324956286439855
Validation loss: 2.497244413486688

Epoch: 6| Step: 12
Training loss: 2.6696587009388093
Validation loss: 2.5664269735599943

Epoch: 6| Step: 13
Training loss: 2.6131714026567576
Validation loss: 2.4537966009853545

Epoch: 349| Step: 0
Training loss: 2.457884429635739
Validation loss: 2.5171467270097043

Epoch: 6| Step: 1
Training loss: 2.4963310021489615
Validation loss: 2.5174423030046094

Epoch: 6| Step: 2
Training loss: 1.7257962144623389
Validation loss: 2.553828510195935

Epoch: 6| Step: 3
Training loss: 1.7994522850873687
Validation loss: 2.4922213996173244

Epoch: 6| Step: 4
Training loss: 2.3339382023414057
Validation loss: 2.571387971504475

Epoch: 6| Step: 5
Training loss: 1.8379633987402042
Validation loss: 2.5523225228128372

Epoch: 6| Step: 6
Training loss: 2.4370367147867817
Validation loss: 2.48692148239756

Epoch: 6| Step: 7
Training loss: 1.8922785944159723
Validation loss: 2.499039715745535

Epoch: 6| Step: 8
Training loss: 2.37349452439314
Validation loss: 2.4998069596363366

Epoch: 6| Step: 9
Training loss: 2.5504528728080422
Validation loss: 2.5285626599650426

Epoch: 6| Step: 10
Training loss: 2.1533127834335493
Validation loss: 2.5543149940147636

Epoch: 6| Step: 11
Training loss: 2.731250656442105
Validation loss: 2.5490029612279614

Epoch: 6| Step: 12
Training loss: 2.4497428953766733
Validation loss: 2.474881563225167

Epoch: 6| Step: 13
Training loss: 2.904558078946291
Validation loss: 2.495967652477263

Epoch: 350| Step: 0
Training loss: 1.9261476736844045
Validation loss: 2.5043894493003753

Epoch: 6| Step: 1
Training loss: 2.2316110228063035
Validation loss: 2.5454612415169313

Epoch: 6| Step: 2
Training loss: 2.196917832718056
Validation loss: 2.4861037104376877

Epoch: 6| Step: 3
Training loss: 3.2103765512807345
Validation loss: 2.5052386668705453

Epoch: 6| Step: 4
Training loss: 2.309549459802797
Validation loss: 2.5201255800075693

Epoch: 6| Step: 5
Training loss: 2.321714857285459
Validation loss: 2.531057258538694

Epoch: 6| Step: 6
Training loss: 2.5494009959959243
Validation loss: 2.518159066307375

Epoch: 6| Step: 7
Training loss: 2.320745459855716
Validation loss: 2.5126902917208755

Epoch: 6| Step: 8
Training loss: 1.9200238252194686
Validation loss: 2.558654653172999

Epoch: 6| Step: 9
Training loss: 2.2105768671207113
Validation loss: 2.5162234859674206

Epoch: 6| Step: 10
Training loss: 2.1870408802511325
Validation loss: 2.5113352257243897

Epoch: 6| Step: 11
Training loss: 2.14872656524821
Validation loss: 2.5019706567983193

Epoch: 6| Step: 12
Training loss: 1.8446014507269384
Validation loss: 2.48960136756813

Epoch: 6| Step: 13
Training loss: 2.1241983135625064
Validation loss: 2.4918923908066097

Epoch: 351| Step: 0
Training loss: 2.1703307884989207
Validation loss: 2.5259027122960513

Epoch: 6| Step: 1
Training loss: 1.3148694222313184
Validation loss: 2.51719564318141

Epoch: 6| Step: 2
Training loss: 2.058909440852687
Validation loss: 2.532286574162266

Epoch: 6| Step: 3
Training loss: 2.1434265515517374
Validation loss: 2.499527505093365

Epoch: 6| Step: 4
Training loss: 2.403839390084948
Validation loss: 2.5133480038637277

Epoch: 6| Step: 5
Training loss: 2.7102322499320564
Validation loss: 2.5115810147220596

Epoch: 6| Step: 6
Training loss: 2.2815844147009443
Validation loss: 2.5408240576952306

Epoch: 6| Step: 7
Training loss: 2.4934804785075015
Validation loss: 2.5368959953418475

Epoch: 6| Step: 8
Training loss: 2.466206749576956
Validation loss: 2.5173695336242097

Epoch: 6| Step: 9
Training loss: 2.138309758156234
Validation loss: 2.54686692719294

Epoch: 6| Step: 10
Training loss: 2.7169403542888877
Validation loss: 2.5664188269013897

Epoch: 6| Step: 11
Training loss: 1.9223457318183899
Validation loss: 2.572277300692274

Epoch: 6| Step: 12
Training loss: 2.622041306432906
Validation loss: 2.5022323301600466

Epoch: 6| Step: 13
Training loss: 2.043078680344834
Validation loss: 2.5275061562581653

Epoch: 352| Step: 0
Training loss: 1.728853388830175
Validation loss: 2.5277087315504967

Epoch: 6| Step: 1
Training loss: 1.994377338882648
Validation loss: 2.525398439583171

Epoch: 6| Step: 2
Training loss: 2.2132771651866205
Validation loss: 2.4903489182260263

Epoch: 6| Step: 3
Training loss: 2.1543869594577347
Validation loss: 2.4879907563482986

Epoch: 6| Step: 4
Training loss: 2.631084293756483
Validation loss: 2.539426583689892

Epoch: 6| Step: 5
Training loss: 2.977149244735179
Validation loss: 2.5293091453501355

Epoch: 6| Step: 6
Training loss: 1.8023695819460326
Validation loss: 2.513690034896174

Epoch: 6| Step: 7
Training loss: 1.8599537822149528
Validation loss: 2.48563530169372

Epoch: 6| Step: 8
Training loss: 1.957103491875526
Validation loss: 2.5398515776074846

Epoch: 6| Step: 9
Training loss: 2.6931114165626835
Validation loss: 2.5441653233055868

Epoch: 6| Step: 10
Training loss: 2.353798345149322
Validation loss: 2.5103494414499514

Epoch: 6| Step: 11
Training loss: 2.1442266084565516
Validation loss: 2.5093544746901606

Epoch: 6| Step: 12
Training loss: 2.923109477166897
Validation loss: 2.4791850269189615

Epoch: 6| Step: 13
Training loss: 2.3586765291206544
Validation loss: 2.516138115023239

Epoch: 353| Step: 0
Training loss: 1.7339901883860789
Validation loss: 2.5031309825737695

Epoch: 6| Step: 1
Training loss: 2.2407223902436377
Validation loss: 2.5064653697425348

Epoch: 6| Step: 2
Training loss: 1.9737810912531193
Validation loss: 2.49366760278152

Epoch: 6| Step: 3
Training loss: 2.5351684758107553
Validation loss: 2.5008604788947553

Epoch: 6| Step: 4
Training loss: 2.5919481016308663
Validation loss: 2.4794157473020073

Epoch: 6| Step: 5
Training loss: 2.7503478090377427
Validation loss: 2.49628390105409

Epoch: 6| Step: 6
Training loss: 2.643899185624815
Validation loss: 2.511206841105879

Epoch: 6| Step: 7
Training loss: 1.9756810933313598
Validation loss: 2.546194816519477

Epoch: 6| Step: 8
Training loss: 1.6594970830631395
Validation loss: 2.4898125900821344

Epoch: 6| Step: 9
Training loss: 3.088343664880453
Validation loss: 2.4776394646053057

Epoch: 6| Step: 10
Training loss: 2.309005571480064
Validation loss: 2.500746749797463

Epoch: 6| Step: 11
Training loss: 2.400649451757205
Validation loss: 2.512828218190826

Epoch: 6| Step: 12
Training loss: 1.953696327570523
Validation loss: 2.514114321693732

Epoch: 6| Step: 13
Training loss: 2.2978667109971345
Validation loss: 2.5080475744134154

Epoch: 354| Step: 0
Training loss: 1.930352162167816
Validation loss: 2.5474596548445594

Epoch: 6| Step: 1
Training loss: 2.1723173500601973
Validation loss: 2.512605461891155

Epoch: 6| Step: 2
Training loss: 2.207406827314719
Validation loss: 2.5548074982677655

Epoch: 6| Step: 3
Training loss: 2.3055161877479353
Validation loss: 2.4830511828785498

Epoch: 6| Step: 4
Training loss: 2.4125968162598466
Validation loss: 2.5558116735308802

Epoch: 6| Step: 5
Training loss: 2.4193715600492105
Validation loss: 2.5186129871345893

Epoch: 6| Step: 6
Training loss: 3.263899205689775
Validation loss: 2.5462308967953007

Epoch: 6| Step: 7
Training loss: 2.5799010661473822
Validation loss: 2.520552999363148

Epoch: 6| Step: 8
Training loss: 2.0011931674948147
Validation loss: 2.511898691064487

Epoch: 6| Step: 9
Training loss: 2.1443106671619874
Validation loss: 2.5356775551733364

Epoch: 6| Step: 10
Training loss: 1.8982790029162198
Validation loss: 2.518782581267322

Epoch: 6| Step: 11
Training loss: 2.167627366139646
Validation loss: 2.506549396869681

Epoch: 6| Step: 12
Training loss: 1.6203880888358697
Validation loss: 2.5408374079789113

Epoch: 6| Step: 13
Training loss: 3.0742524083390133
Validation loss: 2.4812587604908933

Epoch: 355| Step: 0
Training loss: 2.654098177694416
Validation loss: 2.5471264651930374

Epoch: 6| Step: 1
Training loss: 2.35515221640404
Validation loss: 2.5259652927440865

Epoch: 6| Step: 2
Training loss: 1.9496666158666467
Validation loss: 2.5432499485189464

Epoch: 6| Step: 3
Training loss: 2.882935229327746
Validation loss: 2.533696074279085

Epoch: 6| Step: 4
Training loss: 2.3619606048179884
Validation loss: 2.538388044137079

Epoch: 6| Step: 5
Training loss: 1.9822835037723547
Validation loss: 2.5487625824626257

Epoch: 6| Step: 6
Training loss: 2.1501018943594112
Validation loss: 2.5224240822297603

Epoch: 6| Step: 7
Training loss: 2.519947295621712
Validation loss: 2.5400187261978107

Epoch: 6| Step: 8
Training loss: 1.6800105294397176
Validation loss: 2.547721021192328

Epoch: 6| Step: 9
Training loss: 1.6955324373103806
Validation loss: 2.5474704519702516

Epoch: 6| Step: 10
Training loss: 2.2872849160057225
Validation loss: 2.5114823890420985

Epoch: 6| Step: 11
Training loss: 1.9042848557313
Validation loss: 2.536605110746997

Epoch: 6| Step: 12
Training loss: 2.6896358253405466
Validation loss: 2.536011444348767

Epoch: 6| Step: 13
Training loss: 2.465025784742586
Validation loss: 2.5369982707685437

Epoch: 356| Step: 0
Training loss: 2.3371739001948906
Validation loss: 2.529650367179968

Epoch: 6| Step: 1
Training loss: 2.055470480604614
Validation loss: 2.5022972627029287

Epoch: 6| Step: 2
Training loss: 2.0547299688903533
Validation loss: 2.4881743081654784

Epoch: 6| Step: 3
Training loss: 2.7291053784504204
Validation loss: 2.545675461653365

Epoch: 6| Step: 4
Training loss: 1.8100858593171694
Validation loss: 2.4890384195288147

Epoch: 6| Step: 5
Training loss: 2.6294771795999883
Validation loss: 2.5198638673568365

Epoch: 6| Step: 6
Training loss: 2.1265419246398545
Validation loss: 2.5208262880152796

Epoch: 6| Step: 7
Training loss: 3.2029112209530006
Validation loss: 2.5286395610394727

Epoch: 6| Step: 8
Training loss: 2.3420925128754386
Validation loss: 2.528243194858895

Epoch: 6| Step: 9
Training loss: 1.7442319361246286
Validation loss: 2.512701872860775

Epoch: 6| Step: 10
Training loss: 2.498097649625697
Validation loss: 2.5305953775124843

Epoch: 6| Step: 11
Training loss: 1.8535042411687759
Validation loss: 2.5374718324698304

Epoch: 6| Step: 12
Training loss: 1.8537919669567942
Validation loss: 2.5052060588234264

Epoch: 6| Step: 13
Training loss: 2.1418229809230516
Validation loss: 2.514206956610207

Epoch: 357| Step: 0
Training loss: 2.0433912843179143
Validation loss: 2.503471971770046

Epoch: 6| Step: 1
Training loss: 2.2409434838375346
Validation loss: 2.5106549407563126

Epoch: 6| Step: 2
Training loss: 2.2634988213416607
Validation loss: 2.519414371349693

Epoch: 6| Step: 3
Training loss: 2.476274924925481
Validation loss: 2.4875135795390384

Epoch: 6| Step: 4
Training loss: 2.4917934189859343
Validation loss: 2.5545662436625665

Epoch: 6| Step: 5
Training loss: 2.020782496281535
Validation loss: 2.562558186228364

Epoch: 6| Step: 6
Training loss: 2.0542246946401304
Validation loss: 2.5178494929730344

Epoch: 6| Step: 7
Training loss: 2.8894741757399505
Validation loss: 2.5222930883483143

Epoch: 6| Step: 8
Training loss: 2.4777958924392105
Validation loss: 2.5259036714145195

Epoch: 6| Step: 9
Training loss: 2.5440860760599953
Validation loss: 2.542811233024626

Epoch: 6| Step: 10
Training loss: 2.805160780053993
Validation loss: 2.5065718523456906

Epoch: 6| Step: 11
Training loss: 1.8280713978269498
Validation loss: 2.521477450118562

Epoch: 6| Step: 12
Training loss: 1.7783051625779027
Validation loss: 2.564079171242428

Epoch: 6| Step: 13
Training loss: 1.7888971110695326
Validation loss: 2.499094167286097

Epoch: 358| Step: 0
Training loss: 2.362799174424197
Validation loss: 2.537604341655591

Epoch: 6| Step: 1
Training loss: 1.9493405107221657
Validation loss: 2.539569157359591

Epoch: 6| Step: 2
Training loss: 2.679420658093611
Validation loss: 2.5353333910789395

Epoch: 6| Step: 3
Training loss: 1.8436167394034129
Validation loss: 2.5145785509882295

Epoch: 6| Step: 4
Training loss: 2.387628210330109
Validation loss: 2.5510299433993953

Epoch: 6| Step: 5
Training loss: 2.802651689220205
Validation loss: 2.4860474492079137

Epoch: 6| Step: 6
Training loss: 1.602927022090205
Validation loss: 2.490689505628014

Epoch: 6| Step: 7
Training loss: 1.940108850119312
Validation loss: 2.501329458178637

Epoch: 6| Step: 8
Training loss: 2.6902737165543043
Validation loss: 2.546214005027291

Epoch: 6| Step: 9
Training loss: 2.1747587530580685
Validation loss: 2.5589900114711503

Epoch: 6| Step: 10
Training loss: 2.189527498423191
Validation loss: 2.553252852281107

Epoch: 6| Step: 11
Training loss: 2.254529209092417
Validation loss: 2.535705384573313

Epoch: 6| Step: 12
Training loss: 2.5613235005844137
Validation loss: 2.52761866155005

Epoch: 6| Step: 13
Training loss: 1.8616348766366384
Validation loss: 2.512093016774175

Epoch: 359| Step: 0
Training loss: 2.3735688565578394
Validation loss: 2.5254106486789354

Epoch: 6| Step: 1
Training loss: 2.049602991133807
Validation loss: 2.5280277409274334

Epoch: 6| Step: 2
Training loss: 3.0821250060002927
Validation loss: 2.5323529237785336

Epoch: 6| Step: 3
Training loss: 2.403245412441404
Validation loss: 2.5365636875737247

Epoch: 6| Step: 4
Training loss: 1.8989239607096249
Validation loss: 2.5658045405012664

Epoch: 6| Step: 5
Training loss: 1.7917931386385995
Validation loss: 2.5385493065457956

Epoch: 6| Step: 6
Training loss: 1.9665808459592622
Validation loss: 2.5364080641694686

Epoch: 6| Step: 7
Training loss: 2.545243940372596
Validation loss: 2.4973442286763383

Epoch: 6| Step: 8
Training loss: 2.3505049954255592
Validation loss: 2.511803358189053

Epoch: 6| Step: 9
Training loss: 2.870735322546169
Validation loss: 2.53908100534768

Epoch: 6| Step: 10
Training loss: 1.8601156730984552
Validation loss: 2.5045169560823495

Epoch: 6| Step: 11
Training loss: 1.79334236856522
Validation loss: 2.525715729000332

Epoch: 6| Step: 12
Training loss: 2.64661238807668
Validation loss: 2.4858288248264975

Epoch: 6| Step: 13
Training loss: 1.7515235808078566
Validation loss: 2.5505676848479624

Epoch: 360| Step: 0
Training loss: 2.3166425561908115
Validation loss: 2.5178824682576924

Epoch: 6| Step: 1
Training loss: 2.118246679246925
Validation loss: 2.5500708501609632

Epoch: 6| Step: 2
Training loss: 1.8434543049226277
Validation loss: 2.5135787399830765

Epoch: 6| Step: 3
Training loss: 2.256447455306857
Validation loss: 2.4860267228022526

Epoch: 6| Step: 4
Training loss: 2.261534477461934
Validation loss: 2.522526347342279

Epoch: 6| Step: 5
Training loss: 2.3728660232011625
Validation loss: 2.4961867599417005

Epoch: 6| Step: 6
Training loss: 2.2608803362892806
Validation loss: 2.4815081958930993

Epoch: 6| Step: 7
Training loss: 2.7011147670266915
Validation loss: 2.538625647588045

Epoch: 6| Step: 8
Training loss: 2.0466801208443215
Validation loss: 2.4796573618294175

Epoch: 6| Step: 9
Training loss: 1.9038337653023047
Validation loss: 2.4654078375951407

Epoch: 6| Step: 10
Training loss: 2.588729779925692
Validation loss: 2.498895671953966

Epoch: 6| Step: 11
Training loss: 2.584450746692391
Validation loss: 2.465824542915633

Epoch: 6| Step: 12
Training loss: 2.31517781046454
Validation loss: 2.4711399994170042

Epoch: 6| Step: 13
Training loss: 1.9904214847797324
Validation loss: 2.5281296424706987

Epoch: 361| Step: 0
Training loss: 2.2583807523847033
Validation loss: 2.508165124822588

Epoch: 6| Step: 1
Training loss: 2.432966956770691
Validation loss: 2.504538128276311

Epoch: 6| Step: 2
Training loss: 2.1657291976280173
Validation loss: 2.520851939278543

Epoch: 6| Step: 3
Training loss: 2.252522326325962
Validation loss: 2.5391523397373668

Epoch: 6| Step: 4
Training loss: 2.2774858119618977
Validation loss: 2.5274958306987543

Epoch: 6| Step: 5
Training loss: 2.3973271785179002
Validation loss: 2.5468494115294313

Epoch: 6| Step: 6
Training loss: 2.3106144615471638
Validation loss: 2.558792864251823

Epoch: 6| Step: 7
Training loss: 2.422705132742589
Validation loss: 2.5079395056426703

Epoch: 6| Step: 8
Training loss: 3.089499125208733
Validation loss: 2.5408412400618934

Epoch: 6| Step: 9
Training loss: 2.0556699775375114
Validation loss: 2.5376118054468724

Epoch: 6| Step: 10
Training loss: 1.7384290310908659
Validation loss: 2.4988069158751873

Epoch: 6| Step: 11
Training loss: 2.433951313331165
Validation loss: 2.509593714530697

Epoch: 6| Step: 12
Training loss: 2.0838669919934487
Validation loss: 2.535445899911804

Epoch: 6| Step: 13
Training loss: 0.6486161916476176
Validation loss: 2.510933583644868

Epoch: 362| Step: 0
Training loss: 2.138330273803203
Validation loss: 2.479788481887894

Epoch: 6| Step: 1
Training loss: 2.4992037458765384
Validation loss: 2.5279040334564797

Epoch: 6| Step: 2
Training loss: 1.9100173244389649
Validation loss: 2.514087961308712

Epoch: 6| Step: 3
Training loss: 2.056618482972273
Validation loss: 2.5169439234649604

Epoch: 6| Step: 4
Training loss: 2.178205526387615
Validation loss: 2.5128029981960514

Epoch: 6| Step: 5
Training loss: 1.520051685634045
Validation loss: 2.5094929727022866

Epoch: 6| Step: 6
Training loss: 2.3263635308194752
Validation loss: 2.5334490995990393

Epoch: 6| Step: 7
Training loss: 2.5529866287293688
Validation loss: 2.5282955735419166

Epoch: 6| Step: 8
Training loss: 2.1059797300357697
Validation loss: 2.522142610965254

Epoch: 6| Step: 9
Training loss: 1.8334995396730267
Validation loss: 2.5303195978873445

Epoch: 6| Step: 10
Training loss: 1.9254085652155417
Validation loss: 2.4913640343229657

Epoch: 6| Step: 11
Training loss: 2.3726931963805162
Validation loss: 2.5465799907314666

Epoch: 6| Step: 12
Training loss: 3.0869486636980423
Validation loss: 2.539244213685411

Epoch: 6| Step: 13
Training loss: 1.9078909268142126
Validation loss: 2.532282537793135

Epoch: 363| Step: 0
Training loss: 2.2846050934558546
Validation loss: 2.5393644652170417

Epoch: 6| Step: 1
Training loss: 1.988149702295834
Validation loss: 2.4891987831105324

Epoch: 6| Step: 2
Training loss: 1.9124840280701145
Validation loss: 2.520099456455131

Epoch: 6| Step: 3
Training loss: 2.538191050121185
Validation loss: 2.5653494782197632

Epoch: 6| Step: 4
Training loss: 2.452578927266198
Validation loss: 2.5020038582653776

Epoch: 6| Step: 5
Training loss: 2.9791989802673267
Validation loss: 2.5449892409966126

Epoch: 6| Step: 6
Training loss: 1.8299955921432691
Validation loss: 2.5243395218610636

Epoch: 6| Step: 7
Training loss: 2.058694507659332
Validation loss: 2.5419032660682923

Epoch: 6| Step: 8
Training loss: 1.676434654061341
Validation loss: 2.5068278083631275

Epoch: 6| Step: 9
Training loss: 1.921232907101921
Validation loss: 2.542285165789465

Epoch: 6| Step: 10
Training loss: 2.231889743077903
Validation loss: 2.5308521534883948

Epoch: 6| Step: 11
Training loss: 2.143424326901654
Validation loss: 2.5290705338333748

Epoch: 6| Step: 12
Training loss: 3.1719345312686227
Validation loss: 2.545097934476012

Epoch: 6| Step: 13
Training loss: 1.9783707139174906
Validation loss: 2.5270958177023317

Epoch: 364| Step: 0
Training loss: 2.320976701710733
Validation loss: 2.5125620753142432

Epoch: 6| Step: 1
Training loss: 2.2863628625308268
Validation loss: 2.520460725842475

Epoch: 6| Step: 2
Training loss: 2.2153100224310363
Validation loss: 2.5152994499490595

Epoch: 6| Step: 3
Training loss: 1.9547463973066181
Validation loss: 2.549830363946996

Epoch: 6| Step: 4
Training loss: 1.7414081010427858
Validation loss: 2.541334458993351

Epoch: 6| Step: 5
Training loss: 2.6710874947371184
Validation loss: 2.502351450754181

Epoch: 6| Step: 6
Training loss: 2.1362858809777525
Validation loss: 2.5370599106130927

Epoch: 6| Step: 7
Training loss: 2.32169185444938
Validation loss: 2.498279873964341

Epoch: 6| Step: 8
Training loss: 2.3117161014976166
Validation loss: 2.540793054073393

Epoch: 6| Step: 9
Training loss: 2.4603947116562406
Validation loss: 2.4707385110775264

Epoch: 6| Step: 10
Training loss: 2.218718837465469
Validation loss: 2.52979148127052

Epoch: 6| Step: 11
Training loss: 2.65769739546507
Validation loss: 2.5446742197503904

Epoch: 6| Step: 12
Training loss: 1.9735190138363585
Validation loss: 2.4790993220577975

Epoch: 6| Step: 13
Training loss: 2.4444181616651375
Validation loss: 2.513116916046193

Epoch: 365| Step: 0
Training loss: 2.0830020895520303
Validation loss: 2.5107606789887607

Epoch: 6| Step: 1
Training loss: 1.5644973196199936
Validation loss: 2.4965290975876444

Epoch: 6| Step: 2
Training loss: 3.0689893205791874
Validation loss: 2.511104977683392

Epoch: 6| Step: 3
Training loss: 3.3204608480187052
Validation loss: 2.503669832425624

Epoch: 6| Step: 4
Training loss: 2.194750513657465
Validation loss: 2.4867964970569303

Epoch: 6| Step: 5
Training loss: 1.8119645478778312
Validation loss: 2.5525805108124437

Epoch: 6| Step: 6
Training loss: 2.5413965383931028
Validation loss: 2.5355661384671166

Epoch: 6| Step: 7
Training loss: 2.4276513893386666
Validation loss: 2.515875970255884

Epoch: 6| Step: 8
Training loss: 1.9159687125930576
Validation loss: 2.551550928215485

Epoch: 6| Step: 9
Training loss: 2.247975710381256
Validation loss: 2.496896201427241

Epoch: 6| Step: 10
Training loss: 2.1732809881135706
Validation loss: 2.557243894244384

Epoch: 6| Step: 11
Training loss: 1.948552386606742
Validation loss: 2.545160175910027

Epoch: 6| Step: 12
Training loss: 1.7520006868674618
Validation loss: 2.5011080163067

Epoch: 6| Step: 13
Training loss: 2.3324852378517087
Validation loss: 2.5574725956941613

Epoch: 366| Step: 0
Training loss: 2.071697189503118
Validation loss: 2.544606532087581

Epoch: 6| Step: 1
Training loss: 2.173706160942894
Validation loss: 2.4674367266029007

Epoch: 6| Step: 2
Training loss: 2.047828044765413
Validation loss: 2.5273534117764713

Epoch: 6| Step: 3
Training loss: 1.906296526231649
Validation loss: 2.516579947810805

Epoch: 6| Step: 4
Training loss: 2.646844885788789
Validation loss: 2.4955403369218705

Epoch: 6| Step: 5
Training loss: 2.519238169575796
Validation loss: 2.5312069162915174

Epoch: 6| Step: 6
Training loss: 3.1004373734092914
Validation loss: 2.4676120056836055

Epoch: 6| Step: 7
Training loss: 2.5048427407552567
Validation loss: 2.5146154570176478

Epoch: 6| Step: 8
Training loss: 2.2767747852052445
Validation loss: 2.457956121595258

Epoch: 6| Step: 9
Training loss: 1.6498437460693336
Validation loss: 2.469240311510603

Epoch: 6| Step: 10
Training loss: 2.2780066649127617
Validation loss: 2.582865313060167

Epoch: 6| Step: 11
Training loss: 1.5274713565369706
Validation loss: 2.475114168496529

Epoch: 6| Step: 12
Training loss: 1.5826998581114768
Validation loss: 2.489258084388708

Epoch: 6| Step: 13
Training loss: 2.907762195576342
Validation loss: 2.5368936271349347

Epoch: 367| Step: 0
Training loss: 2.337766409803212
Validation loss: 2.5228528792905967

Epoch: 6| Step: 1
Training loss: 2.0194412890232813
Validation loss: 2.490462194160072

Epoch: 6| Step: 2
Training loss: 2.1359676735678503
Validation loss: 2.525720582799831

Epoch: 6| Step: 3
Training loss: 2.17364649258039
Validation loss: 2.4904106410725255

Epoch: 6| Step: 4
Training loss: 2.4783649804617953
Validation loss: 2.5119701544127953

Epoch: 6| Step: 5
Training loss: 2.1829350387267095
Validation loss: 2.51311800092657

Epoch: 6| Step: 6
Training loss: 1.7198389245172905
Validation loss: 2.531330200042912

Epoch: 6| Step: 7
Training loss: 2.2312895271796447
Validation loss: 2.476200594767832

Epoch: 6| Step: 8
Training loss: 2.317185577988551
Validation loss: 2.4791008701074455

Epoch: 6| Step: 9
Training loss: 2.223493314512962
Validation loss: 2.4515585390716947

Epoch: 6| Step: 10
Training loss: 2.7996905768950575
Validation loss: 2.5130831595577945

Epoch: 6| Step: 11
Training loss: 2.42295488462431
Validation loss: 2.489747149686154

Epoch: 6| Step: 12
Training loss: 2.6400926041978594
Validation loss: 2.54535708278631

Epoch: 6| Step: 13
Training loss: 1.9724217039361966
Validation loss: 2.4952197981216897

Epoch: 368| Step: 0
Training loss: 2.0285619702876625
Validation loss: 2.4982911530018623

Epoch: 6| Step: 1
Training loss: 2.2235608665142284
Validation loss: 2.5243582295858955

Epoch: 6| Step: 2
Training loss: 2.2942724729453445
Validation loss: 2.5035641684693837

Epoch: 6| Step: 3
Training loss: 1.5687488723082117
Validation loss: 2.5194475790116466

Epoch: 6| Step: 4
Training loss: 2.168846049091974
Validation loss: 2.530371840379242

Epoch: 6| Step: 5
Training loss: 1.7830026269445147
Validation loss: 2.4730062579973318

Epoch: 6| Step: 6
Training loss: 1.5959841458577668
Validation loss: 2.5005512306639464

Epoch: 6| Step: 7
Training loss: 3.2973578944614856
Validation loss: 2.5214209634828593

Epoch: 6| Step: 8
Training loss: 1.964793088953774
Validation loss: 2.54571634381652

Epoch: 6| Step: 9
Training loss: 2.167673341712175
Validation loss: 2.498746385361659

Epoch: 6| Step: 10
Training loss: 2.2606324006482335
Validation loss: 2.51735800450951

Epoch: 6| Step: 11
Training loss: 3.094187387701327
Validation loss: 2.5438382467250564

Epoch: 6| Step: 12
Training loss: 2.0723813642131104
Validation loss: 2.527270776782082

Epoch: 6| Step: 13
Training loss: 2.2849546685928837
Validation loss: 2.5217808287158183

Epoch: 369| Step: 0
Training loss: 2.1455604848876955
Validation loss: 2.5326912110543818

Epoch: 6| Step: 1
Training loss: 2.5422491646423078
Validation loss: 2.527905439049764

Epoch: 6| Step: 2
Training loss: 1.8695473542338799
Validation loss: 2.5292004473960406

Epoch: 6| Step: 3
Training loss: 2.5513299465783277
Validation loss: 2.5103906190916767

Epoch: 6| Step: 4
Training loss: 2.504247299986514
Validation loss: 2.520469400944264

Epoch: 6| Step: 5
Training loss: 1.625344313310353
Validation loss: 2.459816825207057

Epoch: 6| Step: 6
Training loss: 2.1828356469250765
Validation loss: 2.5235095254954993

Epoch: 6| Step: 7
Training loss: 2.9456158792899654
Validation loss: 2.5685974577201707

Epoch: 6| Step: 8
Training loss: 1.6000155537564544
Validation loss: 2.511755834817402

Epoch: 6| Step: 9
Training loss: 2.024813269023606
Validation loss: 2.531254833059332

Epoch: 6| Step: 10
Training loss: 2.5931386571870676
Validation loss: 2.5694967691565993

Epoch: 6| Step: 11
Training loss: 1.7539689832313379
Validation loss: 2.4406222310932266

Epoch: 6| Step: 12
Training loss: 2.4135494736397147
Validation loss: 2.507358844655902

Epoch: 6| Step: 13
Training loss: 1.9337830113735368
Validation loss: 2.4941446110585006

Epoch: 370| Step: 0
Training loss: 2.496895388256188
Validation loss: 2.509466991818158

Epoch: 6| Step: 1
Training loss: 1.8850600887202342
Validation loss: 2.503860456576392

Epoch: 6| Step: 2
Training loss: 2.6379362785041343
Validation loss: 2.4951238546986154

Epoch: 6| Step: 3
Training loss: 2.084974888346183
Validation loss: 2.4918096117443516

Epoch: 6| Step: 4
Training loss: 2.3856802271426067
Validation loss: 2.478989852745252

Epoch: 6| Step: 5
Training loss: 1.9790652868172043
Validation loss: 2.533049690718844

Epoch: 6| Step: 6
Training loss: 2.060983504909082
Validation loss: 2.504648354066502

Epoch: 6| Step: 7
Training loss: 2.300066540626291
Validation loss: 2.5103515482436607

Epoch: 6| Step: 8
Training loss: 2.371522164016218
Validation loss: 2.513430222482255

Epoch: 6| Step: 9
Training loss: 2.835209524495593
Validation loss: 2.5132437148357414

Epoch: 6| Step: 10
Training loss: 2.409060731571215
Validation loss: 2.4873813057701057

Epoch: 6| Step: 11
Training loss: 2.205900135342598
Validation loss: 2.5043234756603794

Epoch: 6| Step: 12
Training loss: 1.435086837028555
Validation loss: 2.5204631567861915

Epoch: 6| Step: 13
Training loss: 2.291745745132573
Validation loss: 2.5555942392995674

Epoch: 371| Step: 0
Training loss: 2.587002169130033
Validation loss: 2.544822825866238

Epoch: 6| Step: 1
Training loss: 2.2072874747562494
Validation loss: 2.5149782562522818

Epoch: 6| Step: 2
Training loss: 2.335172598134843
Validation loss: 2.4687775319190544

Epoch: 6| Step: 3
Training loss: 2.2818839877475847
Validation loss: 2.5595352405759844

Epoch: 6| Step: 4
Training loss: 1.926068019600694
Validation loss: 2.5192634920378527

Epoch: 6| Step: 5
Training loss: 2.100814970007594
Validation loss: 2.5340604272057923

Epoch: 6| Step: 6
Training loss: 1.875561185143345
Validation loss: 2.507745145339763

Epoch: 6| Step: 7
Training loss: 2.271943850306991
Validation loss: 2.463119078966735

Epoch: 6| Step: 8
Training loss: 2.587769657402716
Validation loss: 2.4999765477054643

Epoch: 6| Step: 9
Training loss: 1.5953803605639147
Validation loss: 2.5538378976083025

Epoch: 6| Step: 10
Training loss: 1.867916587315062
Validation loss: 2.549963368991793

Epoch: 6| Step: 11
Training loss: 2.503420778246546
Validation loss: 2.495016395452012

Epoch: 6| Step: 12
Training loss: 2.2133186377569842
Validation loss: 2.5068243415323646

Epoch: 6| Step: 13
Training loss: 2.442514104109369
Validation loss: 2.5093978375500923

Epoch: 372| Step: 0
Training loss: 1.9549610510651205
Validation loss: 2.5194477509760316

Epoch: 6| Step: 1
Training loss: 1.9047156240313028
Validation loss: 2.525305210171442

Epoch: 6| Step: 2
Training loss: 2.258695541932879
Validation loss: 2.52181990649029

Epoch: 6| Step: 3
Training loss: 1.930475112861327
Validation loss: 2.529002456928153

Epoch: 6| Step: 4
Training loss: 1.8576290364068535
Validation loss: 2.5309896604893405

Epoch: 6| Step: 5
Training loss: 1.8083414185983413
Validation loss: 2.5282553243074317

Epoch: 6| Step: 6
Training loss: 2.783215289172979
Validation loss: 2.48649712192273

Epoch: 6| Step: 7
Training loss: 2.122389817180571
Validation loss: 2.5164627624639926

Epoch: 6| Step: 8
Training loss: 2.6460283062401877
Validation loss: 2.4743874275002296

Epoch: 6| Step: 9
Training loss: 2.607193085994724
Validation loss: 2.491196870017433

Epoch: 6| Step: 10
Training loss: 1.9486900942870284
Validation loss: 2.545730691088674

Epoch: 6| Step: 11
Training loss: 2.0909988776066677
Validation loss: 2.5477851304946455

Epoch: 6| Step: 12
Training loss: 2.987358479360002
Validation loss: 2.527112711468094

Epoch: 6| Step: 13
Training loss: 2.1844912955088214
Validation loss: 2.4887500744401914

Epoch: 373| Step: 0
Training loss: 2.2226632223707092
Validation loss: 2.529030522984091

Epoch: 6| Step: 1
Training loss: 2.346072660968111
Validation loss: 2.5513524816936677

Epoch: 6| Step: 2
Training loss: 1.9362498526329934
Validation loss: 2.5005424485266414

Epoch: 6| Step: 3
Training loss: 2.96715028728434
Validation loss: 2.530205534809741

Epoch: 6| Step: 4
Training loss: 2.232639198705303
Validation loss: 2.5191827755482334

Epoch: 6| Step: 5
Training loss: 1.870132231091582
Validation loss: 2.5080523591672974

Epoch: 6| Step: 6
Training loss: 1.8164449708154475
Validation loss: 2.4882072424459714

Epoch: 6| Step: 7
Training loss: 2.0673304548016955
Validation loss: 2.5195567231584968

Epoch: 6| Step: 8
Training loss: 2.248687149195836
Validation loss: 2.4813393489105726

Epoch: 6| Step: 9
Training loss: 2.1874346042122217
Validation loss: 2.5015724856207124

Epoch: 6| Step: 10
Training loss: 2.1722908994003154
Validation loss: 2.4950228928684135

Epoch: 6| Step: 11
Training loss: 2.6172796887914656
Validation loss: 2.5230575809399873

Epoch: 6| Step: 12
Training loss: 2.3697687057109382
Validation loss: 2.4968635059924664

Epoch: 6| Step: 13
Training loss: 1.4893196068122354
Validation loss: 2.5159149558219913

Epoch: 374| Step: 0
Training loss: 1.705653771828391
Validation loss: 2.4864874642930244

Epoch: 6| Step: 1
Training loss: 2.199893935854638
Validation loss: 2.535213250696033

Epoch: 6| Step: 2
Training loss: 2.201114467627063
Validation loss: 2.4977896199187546

Epoch: 6| Step: 3
Training loss: 3.0925602021829706
Validation loss: 2.4604582966361286

Epoch: 6| Step: 4
Training loss: 2.407233792617185
Validation loss: 2.473682445606956

Epoch: 6| Step: 5
Training loss: 1.898070248846355
Validation loss: 2.488603549892604

Epoch: 6| Step: 6
Training loss: 1.640708993850659
Validation loss: 2.4924151272632242

Epoch: 6| Step: 7
Training loss: 2.075511464887956
Validation loss: 2.552589588961556

Epoch: 6| Step: 8
Training loss: 2.186092032586681
Validation loss: 2.5255194102661136

Epoch: 6| Step: 9
Training loss: 2.2228308744836207
Validation loss: 2.5273197764625084

Epoch: 6| Step: 10
Training loss: 2.461046876444628
Validation loss: 2.4367125432667147

Epoch: 6| Step: 11
Training loss: 2.272982732113969
Validation loss: 2.5427188225302224

Epoch: 6| Step: 12
Training loss: 1.904865512556176
Validation loss: 2.518482849093836

Epoch: 6| Step: 13
Training loss: 2.7113308579948137
Validation loss: 2.5291521288244847

Epoch: 375| Step: 0
Training loss: 1.9300843112694321
Validation loss: 2.5158422813572563

Epoch: 6| Step: 1
Training loss: 2.1207023245387835
Validation loss: 2.5215831574662437

Epoch: 6| Step: 2
Training loss: 2.4753888836144657
Validation loss: 2.494029155655153

Epoch: 6| Step: 3
Training loss: 2.3700691032797523
Validation loss: 2.5337700199336117

Epoch: 6| Step: 4
Training loss: 2.265172242274395
Validation loss: 2.5359864518749027

Epoch: 6| Step: 5
Training loss: 1.6059893949593198
Validation loss: 2.44889572312735

Epoch: 6| Step: 6
Training loss: 2.7551307499039246
Validation loss: 2.552030947969846

Epoch: 6| Step: 7
Training loss: 2.3912420536114616
Validation loss: 2.533977171216325

Epoch: 6| Step: 8
Training loss: 2.198694413931707
Validation loss: 2.5225127644715286

Epoch: 6| Step: 9
Training loss: 2.516770664702057
Validation loss: 2.52000836370662

Epoch: 6| Step: 10
Training loss: 1.2775123788192813
Validation loss: 2.4902390910326546

Epoch: 6| Step: 11
Training loss: 2.350093344599852
Validation loss: 2.51538666401404

Epoch: 6| Step: 12
Training loss: 2.715346826280836
Validation loss: 2.5524991414754257

Epoch: 6| Step: 13
Training loss: 1.341942880563297
Validation loss: 2.5411561200675292

Epoch: 376| Step: 0
Training loss: 1.8198038462897443
Validation loss: 2.537698712345883

Epoch: 6| Step: 1
Training loss: 2.1899602950864603
Validation loss: 2.4964294993487366

Epoch: 6| Step: 2
Training loss: 2.6976524037041796
Validation loss: 2.519503887053684

Epoch: 6| Step: 3
Training loss: 1.4059313943004959
Validation loss: 2.4881027476269226

Epoch: 6| Step: 4
Training loss: 2.6293616616010738
Validation loss: 2.527154002468626

Epoch: 6| Step: 5
Training loss: 2.652534039824556
Validation loss: 2.5157318795699797

Epoch: 6| Step: 6
Training loss: 2.8909423653905253
Validation loss: 2.4997275368078644

Epoch: 6| Step: 7
Training loss: 2.160011990301973
Validation loss: 2.5289096280058736

Epoch: 6| Step: 8
Training loss: 2.109166792024659
Validation loss: 2.545820816130375

Epoch: 6| Step: 9
Training loss: 2.546012217412194
Validation loss: 2.4894158716388057

Epoch: 6| Step: 10
Training loss: 2.0636310366117616
Validation loss: 2.5228369721573047

Epoch: 6| Step: 11
Training loss: 2.4029475748071842
Validation loss: 2.4845153826717987

Epoch: 6| Step: 12
Training loss: 1.6992924115229902
Validation loss: 2.513686457184672

Epoch: 6| Step: 13
Training loss: 1.541013226912147
Validation loss: 2.4574692999488517

Epoch: 377| Step: 0
Training loss: 2.8036361041258386
Validation loss: 2.4971162201784205

Epoch: 6| Step: 1
Training loss: 2.42042813304992
Validation loss: 2.5110230678291146

Epoch: 6| Step: 2
Training loss: 2.501500632994147
Validation loss: 2.4867004528704153

Epoch: 6| Step: 3
Training loss: 1.9498999863432311
Validation loss: 2.5239033095289574

Epoch: 6| Step: 4
Training loss: 1.891029755953838
Validation loss: 2.532942615798622

Epoch: 6| Step: 5
Training loss: 2.3605623604864503
Validation loss: 2.5070182834035135

Epoch: 6| Step: 6
Training loss: 1.9801100306330697
Validation loss: 2.5603603921729965

Epoch: 6| Step: 7
Training loss: 2.2984454955500695
Validation loss: 2.515639556596031

Epoch: 6| Step: 8
Training loss: 1.6543980447038205
Validation loss: 2.5330237682650427

Epoch: 6| Step: 9
Training loss: 1.9348790068341861
Validation loss: 2.509674088727392

Epoch: 6| Step: 10
Training loss: 2.2557891966897845
Validation loss: 2.4730675750535296

Epoch: 6| Step: 11
Training loss: 2.411144678357801
Validation loss: 2.5022249216901558

Epoch: 6| Step: 12
Training loss: 2.2085141761675446
Validation loss: 2.568306797736665

Epoch: 6| Step: 13
Training loss: 1.7742397829503103
Validation loss: 2.521959557822025

Epoch: 378| Step: 0
Training loss: 2.2045190607580616
Validation loss: 2.53471583273204

Epoch: 6| Step: 1
Training loss: 2.560383643818624
Validation loss: 2.4887840559011756

Epoch: 6| Step: 2
Training loss: 2.0131148447552185
Validation loss: 2.505868610131709

Epoch: 6| Step: 3
Training loss: 2.2473198504696525
Validation loss: 2.5187714708352007

Epoch: 6| Step: 4
Training loss: 2.13865626777981
Validation loss: 2.5163884320063654

Epoch: 6| Step: 5
Training loss: 3.098881021171033
Validation loss: 2.5497904687219375

Epoch: 6| Step: 6
Training loss: 2.1656679884041634
Validation loss: 2.503473256420479

Epoch: 6| Step: 7
Training loss: 2.247060126688127
Validation loss: 2.4940198314547435

Epoch: 6| Step: 8
Training loss: 2.0968386514756605
Validation loss: 2.494176696130544

Epoch: 6| Step: 9
Training loss: 1.614655212114961
Validation loss: 2.481140272163428

Epoch: 6| Step: 10
Training loss: 2.43676864215815
Validation loss: 2.509670640129516

Epoch: 6| Step: 11
Training loss: 1.9446364610715186
Validation loss: 2.4877228446035478

Epoch: 6| Step: 12
Training loss: 1.6049571463547343
Validation loss: 2.505752700469826

Epoch: 6| Step: 13
Training loss: 1.9926378526468747
Validation loss: 2.534164963248979

Epoch: 379| Step: 0
Training loss: 2.78227455041539
Validation loss: 2.553355238100382

Epoch: 6| Step: 1
Training loss: 2.1029591510558454
Validation loss: 2.451762101727582

Epoch: 6| Step: 2
Training loss: 2.285275500193771
Validation loss: 2.496501765948398

Epoch: 6| Step: 3
Training loss: 1.969644857819876
Validation loss: 2.5497964269043227

Epoch: 6| Step: 4
Training loss: 1.798368148725964
Validation loss: 2.504771236792468

Epoch: 6| Step: 5
Training loss: 2.2462872496473967
Validation loss: 2.4920864094365722

Epoch: 6| Step: 6
Training loss: 2.4083676618441587
Validation loss: 2.535633922879897

Epoch: 6| Step: 7
Training loss: 2.529636952282197
Validation loss: 2.5627705080849985

Epoch: 6| Step: 8
Training loss: 1.738102387216376
Validation loss: 2.5306740281100746

Epoch: 6| Step: 9
Training loss: 2.0675434524047582
Validation loss: 2.4536825538847316

Epoch: 6| Step: 10
Training loss: 2.0509780644025426
Validation loss: 2.5233580670988403

Epoch: 6| Step: 11
Training loss: 2.516390192453764
Validation loss: 2.4924273243554533

Epoch: 6| Step: 12
Training loss: 2.1721658854917685
Validation loss: 2.517044662446135

Epoch: 6| Step: 13
Training loss: 2.5500832226206382
Validation loss: 2.5136574497929742

Epoch: 380| Step: 0
Training loss: 2.3994742771366866
Validation loss: 2.4731989967965062

Epoch: 6| Step: 1
Training loss: 2.9845152816956397
Validation loss: 2.527827947708361

Epoch: 6| Step: 2
Training loss: 2.74520221417035
Validation loss: 2.470660526353118

Epoch: 6| Step: 3
Training loss: 1.794414195260794
Validation loss: 2.472482383530311

Epoch: 6| Step: 4
Training loss: 2.342060751917812
Validation loss: 2.531021386359253

Epoch: 6| Step: 5
Training loss: 1.8824296617475518
Validation loss: 2.5242537870967188

Epoch: 6| Step: 6
Training loss: 1.579350892378484
Validation loss: 2.493425833431932

Epoch: 6| Step: 7
Training loss: 1.8351663471462194
Validation loss: 2.532585926854178

Epoch: 6| Step: 8
Training loss: 2.1090442539453975
Validation loss: 2.5392815184177207

Epoch: 6| Step: 9
Training loss: 2.2308832374703242
Validation loss: 2.5431880539749048

Epoch: 6| Step: 10
Training loss: 2.0364981124648147
Validation loss: 2.5062070214454515

Epoch: 6| Step: 11
Training loss: 2.024776884432499
Validation loss: 2.5211341408596377

Epoch: 6| Step: 12
Training loss: 2.639482149283821
Validation loss: 2.503346881345058

Epoch: 6| Step: 13
Training loss: 1.9549353792286432
Validation loss: 2.521540521655373

Epoch: 381| Step: 0
Training loss: 2.381102001166627
Validation loss: 2.4985717087666006

Epoch: 6| Step: 1
Training loss: 2.1210555324307925
Validation loss: 2.5658586992151786

Epoch: 6| Step: 2
Training loss: 1.707730566853227
Validation loss: 2.508844479603612

Epoch: 6| Step: 3
Training loss: 2.2201718354951665
Validation loss: 2.5311486089573094

Epoch: 6| Step: 4
Training loss: 3.1779166874465217
Validation loss: 2.482675805192553

Epoch: 6| Step: 5
Training loss: 1.6781036716576274
Validation loss: 2.5075900766612786

Epoch: 6| Step: 6
Training loss: 2.0320217133811203
Validation loss: 2.507124738433151

Epoch: 6| Step: 7
Training loss: 2.1963982663074293
Validation loss: 2.504426793123545

Epoch: 6| Step: 8
Training loss: 1.9542466261825924
Validation loss: 2.4687306310024457

Epoch: 6| Step: 9
Training loss: 2.1148682450904115
Validation loss: 2.50109846737015

Epoch: 6| Step: 10
Training loss: 2.2738049397009834
Validation loss: 2.5060835334518217

Epoch: 6| Step: 11
Training loss: 2.045375944804455
Validation loss: 2.5223268826756566

Epoch: 6| Step: 12
Training loss: 2.4720124525972555
Validation loss: 2.504978564312746

Epoch: 6| Step: 13
Training loss: 2.4128136228379193
Validation loss: 2.4965202376345705

Epoch: 382| Step: 0
Training loss: 1.9503187848212808
Validation loss: 2.485172254477942

Epoch: 6| Step: 1
Training loss: 2.346589466508493
Validation loss: 2.5248307874893223

Epoch: 6| Step: 2
Training loss: 2.145039927148793
Validation loss: 2.4667235006706996

Epoch: 6| Step: 3
Training loss: 3.3182254470104198
Validation loss: 2.5388040370349088

Epoch: 6| Step: 4
Training loss: 2.2679933994069335
Validation loss: 2.5314454601137943

Epoch: 6| Step: 5
Training loss: 2.259083004308435
Validation loss: 2.5021901361154777

Epoch: 6| Step: 6
Training loss: 1.790623213149639
Validation loss: 2.551465129165541

Epoch: 6| Step: 7
Training loss: 2.1411047627574273
Validation loss: 2.4826996997174473

Epoch: 6| Step: 8
Training loss: 1.769577059458093
Validation loss: 2.487191149226586

Epoch: 6| Step: 9
Training loss: 2.153073168215142
Validation loss: 2.505270369897102

Epoch: 6| Step: 10
Training loss: 1.9599155027315056
Validation loss: 2.5564256329502357

Epoch: 6| Step: 11
Training loss: 1.9304338008197883
Validation loss: 2.5166181773036156

Epoch: 6| Step: 12
Training loss: 2.39740644031747
Validation loss: 2.5345004596946357

Epoch: 6| Step: 13
Training loss: 2.0464902326889947
Validation loss: 2.548996507370106

Epoch: 383| Step: 0
Training loss: 2.584534786132696
Validation loss: 2.505896689756477

Epoch: 6| Step: 1
Training loss: 1.5899891329939775
Validation loss: 2.4921907516264374

Epoch: 6| Step: 2
Training loss: 1.6454504348305017
Validation loss: 2.4265080199258

Epoch: 6| Step: 3
Training loss: 2.1231642254610423
Validation loss: 2.513908642989044

Epoch: 6| Step: 4
Training loss: 2.086086082167252
Validation loss: 2.4843349514183615

Epoch: 6| Step: 5
Training loss: 1.729887233873626
Validation loss: 2.5057058204738856

Epoch: 6| Step: 6
Training loss: 1.9943975419480362
Validation loss: 2.539900971477334

Epoch: 6| Step: 7
Training loss: 2.211537849145505
Validation loss: 2.5049400005949787

Epoch: 6| Step: 8
Training loss: 2.251868849382516
Validation loss: 2.5451455665542597

Epoch: 6| Step: 9
Training loss: 2.5612993916803286
Validation loss: 2.499461453379846

Epoch: 6| Step: 10
Training loss: 2.3316481385802565
Validation loss: 2.46097233323282

Epoch: 6| Step: 11
Training loss: 2.5157203419311442
Validation loss: 2.4860695490026203

Epoch: 6| Step: 12
Training loss: 2.4952322319368165
Validation loss: 2.568852681039206

Epoch: 6| Step: 13
Training loss: 2.1464298369791783
Validation loss: 2.466200993816712

Epoch: 384| Step: 0
Training loss: 2.1145070129552455
Validation loss: 2.444171164109897

Epoch: 6| Step: 1
Training loss: 2.0768746651607017
Validation loss: 2.483006677439289

Epoch: 6| Step: 2
Training loss: 1.890650031814694
Validation loss: 2.485011879680519

Epoch: 6| Step: 3
Training loss: 2.01655237909151
Validation loss: 2.465609993783626

Epoch: 6| Step: 4
Training loss: 2.47053067657195
Validation loss: 2.5013599101555455

Epoch: 6| Step: 5
Training loss: 2.1425073519817612
Validation loss: 2.5470148588304395

Epoch: 6| Step: 6
Training loss: 2.1999891974444035
Validation loss: 2.4982268290192993

Epoch: 6| Step: 7
Training loss: 1.4795471382637602
Validation loss: 2.53261242571163

Epoch: 6| Step: 8
Training loss: 2.957125744135
Validation loss: 2.5312539782607417

Epoch: 6| Step: 9
Training loss: 2.344720868251103
Validation loss: 2.5109972865503623

Epoch: 6| Step: 10
Training loss: 2.502973600040316
Validation loss: 2.48117715998271

Epoch: 6| Step: 11
Training loss: 1.6672614784594921
Validation loss: 2.5342014626439933

Epoch: 6| Step: 12
Training loss: 2.1830165147842564
Validation loss: 2.5102023312027177

Epoch: 6| Step: 13
Training loss: 2.247592910017327
Validation loss: 2.495217340530008

Epoch: 385| Step: 0
Training loss: 1.8679581970912311
Validation loss: 2.496510980243891

Epoch: 6| Step: 1
Training loss: 2.010338526444181
Validation loss: 2.5130144057440593

Epoch: 6| Step: 2
Training loss: 2.463180343443694
Validation loss: 2.5242565271931054

Epoch: 6| Step: 3
Training loss: 1.8874180605452033
Validation loss: 2.5195150105362494

Epoch: 6| Step: 4
Training loss: 2.2372901946005923
Validation loss: 2.48684022130141

Epoch: 6| Step: 5
Training loss: 2.605634372371325
Validation loss: 2.5272308814850297

Epoch: 6| Step: 6
Training loss: 2.784335621427971
Validation loss: 2.5101861917295363

Epoch: 6| Step: 7
Training loss: 2.248997146909947
Validation loss: 2.5241527406644617

Epoch: 6| Step: 8
Training loss: 1.7039020410596801
Validation loss: 2.502357040892009

Epoch: 6| Step: 9
Training loss: 1.9077488369203945
Validation loss: 2.5178225511021415

Epoch: 6| Step: 10
Training loss: 1.802784895266508
Validation loss: 2.5041085312968088

Epoch: 6| Step: 11
Training loss: 1.971494306037066
Validation loss: 2.529268743482339

Epoch: 6| Step: 12
Training loss: 2.731742681764924
Validation loss: 2.5331486698887486

Epoch: 6| Step: 13
Training loss: 2.5273391275496273
Validation loss: 2.5103177014645337

Epoch: 386| Step: 0
Training loss: 2.7095113295384947
Validation loss: 2.5321370926018374

Epoch: 6| Step: 1
Training loss: 1.7696040056441984
Validation loss: 2.464593356162256

Epoch: 6| Step: 2
Training loss: 1.4925134113162883
Validation loss: 2.5005480862809497

Epoch: 6| Step: 3
Training loss: 2.032113698586862
Validation loss: 2.520276662011022

Epoch: 6| Step: 4
Training loss: 1.7669796093690915
Validation loss: 2.5148256295881564

Epoch: 6| Step: 5
Training loss: 1.7138748386136198
Validation loss: 2.5028730834031454

Epoch: 6| Step: 6
Training loss: 2.6168572516394093
Validation loss: 2.5520392244372463

Epoch: 6| Step: 7
Training loss: 2.382397424785383
Validation loss: 2.5430781598906678

Epoch: 6| Step: 8
Training loss: 2.2052329509972552
Validation loss: 2.519900972699735

Epoch: 6| Step: 9
Training loss: 2.133013369983775
Validation loss: 2.5026750629548253

Epoch: 6| Step: 10
Training loss: 2.4990268720668074
Validation loss: 2.474881405773839

Epoch: 6| Step: 11
Training loss: 2.541075392774468
Validation loss: 2.467318947588544

Epoch: 6| Step: 12
Training loss: 2.258146373521984
Validation loss: 2.490357411011525

Epoch: 6| Step: 13
Training loss: 2.393019530392378
Validation loss: 2.468454040257817

Epoch: 387| Step: 0
Training loss: 2.4106929434987094
Validation loss: 2.539194457708831

Epoch: 6| Step: 1
Training loss: 1.9933264375118704
Validation loss: 2.4715487284942177

Epoch: 6| Step: 2
Training loss: 2.8831649606161127
Validation loss: 2.4731592191898506

Epoch: 6| Step: 3
Training loss: 2.1293682751942673
Validation loss: 2.5244291655490785

Epoch: 6| Step: 4
Training loss: 1.9414752049932014
Validation loss: 2.541212174078725

Epoch: 6| Step: 5
Training loss: 2.4868558576814954
Validation loss: 2.514469330435761

Epoch: 6| Step: 6
Training loss: 1.6317211683767345
Validation loss: 2.508674301508877

Epoch: 6| Step: 7
Training loss: 2.323150235280972
Validation loss: 2.4559467035632587

Epoch: 6| Step: 8
Training loss: 1.5696475770564098
Validation loss: 2.4507950047287803

Epoch: 6| Step: 9
Training loss: 2.2234724051442574
Validation loss: 2.493841497228372

Epoch: 6| Step: 10
Training loss: 1.8519218514599354
Validation loss: 2.504441319608747

Epoch: 6| Step: 11
Training loss: 2.1200808641018414
Validation loss: 2.5198774716274817

Epoch: 6| Step: 12
Training loss: 2.2589988891409702
Validation loss: 2.504765177647946

Epoch: 6| Step: 13
Training loss: 2.4250769337513884
Validation loss: 2.4970971215122324

Epoch: 388| Step: 0
Training loss: 2.4633696632476494
Validation loss: 2.509705265839265

Epoch: 6| Step: 1
Training loss: 1.9629671451969233
Validation loss: 2.459547265238846

Epoch: 6| Step: 2
Training loss: 2.8933093005235535
Validation loss: 2.527123175543672

Epoch: 6| Step: 3
Training loss: 2.434250058847493
Validation loss: 2.51992942905104

Epoch: 6| Step: 4
Training loss: 1.8920362895646752
Validation loss: 2.5181125537049773

Epoch: 6| Step: 5
Training loss: 2.4836328705583224
Validation loss: 2.528072889715684

Epoch: 6| Step: 6
Training loss: 2.5890609455737073
Validation loss: 2.4676750275776445

Epoch: 6| Step: 7
Training loss: 2.0806012995599885
Validation loss: 2.545310474961673

Epoch: 6| Step: 8
Training loss: 2.229316587546454
Validation loss: 2.5054148084788057

Epoch: 6| Step: 9
Training loss: 1.6145447593100892
Validation loss: 2.485719797296437

Epoch: 6| Step: 10
Training loss: 1.900053239377645
Validation loss: 2.566962025194976

Epoch: 6| Step: 11
Training loss: 1.994981788227067
Validation loss: 2.4958201485400857

Epoch: 6| Step: 12
Training loss: 2.120032394449581
Validation loss: 2.5318761405869656

Epoch: 6| Step: 13
Training loss: 1.5025538320942329
Validation loss: 2.550620993132352

Epoch: 389| Step: 0
Training loss: 1.8058213144326942
Validation loss: 2.489291169624199

Epoch: 6| Step: 1
Training loss: 2.450288043314243
Validation loss: 2.472945781200869

Epoch: 6| Step: 2
Training loss: 2.2532758178179897
Validation loss: 2.512276183193422

Epoch: 6| Step: 3
Training loss: 2.742803694567585
Validation loss: 2.536736678855882

Epoch: 6| Step: 4
Training loss: 2.259712132881627
Validation loss: 2.4635448721584483

Epoch: 6| Step: 5
Training loss: 2.088323136183231
Validation loss: 2.464363014471028

Epoch: 6| Step: 6
Training loss: 2.286925167545791
Validation loss: 2.5146682571855044

Epoch: 6| Step: 7
Training loss: 2.689906617426269
Validation loss: 2.5494657578953097

Epoch: 6| Step: 8
Training loss: 2.1252634221779334
Validation loss: 2.5339293990068374

Epoch: 6| Step: 9
Training loss: 1.627339952512972
Validation loss: 2.495877687061931

Epoch: 6| Step: 10
Training loss: 1.7837763488577094
Validation loss: 2.490007095083002

Epoch: 6| Step: 11
Training loss: 2.221069735123152
Validation loss: 2.505905321157316

Epoch: 6| Step: 12
Training loss: 1.8068130328419658
Validation loss: 2.535865265766976

Epoch: 6| Step: 13
Training loss: 2.5152811324810274
Validation loss: 2.4191968132011077

Epoch: 390| Step: 0
Training loss: 2.104220046571777
Validation loss: 2.4771650792457907

Epoch: 6| Step: 1
Training loss: 2.5096209413853114
Validation loss: 2.4509007449941063

Epoch: 6| Step: 2
Training loss: 1.6776019256144157
Validation loss: 2.5304149737655806

Epoch: 6| Step: 3
Training loss: 2.4467011917373913
Validation loss: 2.5327698077037173

Epoch: 6| Step: 4
Training loss: 2.461583128510121
Validation loss: 2.484689954793361

Epoch: 6| Step: 5
Training loss: 1.5736710042723352
Validation loss: 2.4673661288598168

Epoch: 6| Step: 6
Training loss: 2.9615664657046166
Validation loss: 2.5110645100776066

Epoch: 6| Step: 7
Training loss: 1.6993034254001074
Validation loss: 2.5304588621990525

Epoch: 6| Step: 8
Training loss: 2.1430297486861125
Validation loss: 2.468169609727004

Epoch: 6| Step: 9
Training loss: 2.496350867650408
Validation loss: 2.490457247967548

Epoch: 6| Step: 10
Training loss: 2.1711235667539186
Validation loss: 2.489210722781567

Epoch: 6| Step: 11
Training loss: 1.871124458459188
Validation loss: 2.5359245458424478

Epoch: 6| Step: 12
Training loss: 2.0497373412665274
Validation loss: 2.478502040907816

Epoch: 6| Step: 13
Training loss: 1.938704885109134
Validation loss: 2.528505480954102

Epoch: 391| Step: 0
Training loss: 1.4436437922793892
Validation loss: 2.544359883949446

Epoch: 6| Step: 1
Training loss: 2.5083098586833263
Validation loss: 2.51200039218026

Epoch: 6| Step: 2
Training loss: 2.122105253645551
Validation loss: 2.5246238905166956

Epoch: 6| Step: 3
Training loss: 1.7290943620877202
Validation loss: 2.5425709353608554

Epoch: 6| Step: 4
Training loss: 2.5040747813432174
Validation loss: 2.546034114892799

Epoch: 6| Step: 5
Training loss: 2.2640343705793318
Validation loss: 2.5230397770380693

Epoch: 6| Step: 6
Training loss: 2.072080612912116
Validation loss: 2.5168880195659535

Epoch: 6| Step: 7
Training loss: 2.3123564804492798
Validation loss: 2.501416913349386

Epoch: 6| Step: 8
Training loss: 2.2758488926261933
Validation loss: 2.550058014185832

Epoch: 6| Step: 9
Training loss: 1.882139790129961
Validation loss: 2.476684227788152

Epoch: 6| Step: 10
Training loss: 1.851690746646378
Validation loss: 2.4941132661588967

Epoch: 6| Step: 11
Training loss: 2.37614614035978
Validation loss: 2.4955139108048883

Epoch: 6| Step: 12
Training loss: 2.999114541670194
Validation loss: 2.5244908177929224

Epoch: 6| Step: 13
Training loss: 2.2645872699196814
Validation loss: 2.520559841348678

Epoch: 392| Step: 0
Training loss: 1.6069946462925415
Validation loss: 2.4976344422473753

Epoch: 6| Step: 1
Training loss: 2.0411515916134912
Validation loss: 2.5026112946979078

Epoch: 6| Step: 2
Training loss: 2.9696221927565807
Validation loss: 2.481314696364316

Epoch: 6| Step: 3
Training loss: 1.9397335714083792
Validation loss: 2.4515066395027088

Epoch: 6| Step: 4
Training loss: 2.311223167317316
Validation loss: 2.4935260744438676

Epoch: 6| Step: 5
Training loss: 2.5372118969554927
Validation loss: 2.485507836056333

Epoch: 6| Step: 6
Training loss: 1.9350250187323463
Validation loss: 2.545590930895227

Epoch: 6| Step: 7
Training loss: 1.8472595434155283
Validation loss: 2.4685648979472123

Epoch: 6| Step: 8
Training loss: 1.8885480333218476
Validation loss: 2.4748581981672197

Epoch: 6| Step: 9
Training loss: 2.2854199901221657
Validation loss: 2.5182177970777095

Epoch: 6| Step: 10
Training loss: 2.683026782783312
Validation loss: 2.4974219719588424

Epoch: 6| Step: 11
Training loss: 1.9664900387905233
Validation loss: 2.4997286105773573

Epoch: 6| Step: 12
Training loss: 2.123422822768214
Validation loss: 2.5043574536123296

Epoch: 6| Step: 13
Training loss: 2.0742604741786868
Validation loss: 2.5303058329398493

Epoch: 393| Step: 0
Training loss: 1.8901694513480982
Validation loss: 2.5469736874946185

Epoch: 6| Step: 1
Training loss: 2.4822662808978064
Validation loss: 2.5468946494275517

Epoch: 6| Step: 2
Training loss: 2.349324932939981
Validation loss: 2.4763329631392526

Epoch: 6| Step: 3
Training loss: 2.156747290722211
Validation loss: 2.5219557061973013

Epoch: 6| Step: 4
Training loss: 1.5305558791778524
Validation loss: 2.451556565799693

Epoch: 6| Step: 5
Training loss: 1.7202259315655293
Validation loss: 2.514523610976229

Epoch: 6| Step: 6
Training loss: 2.0077792510014523
Validation loss: 2.5043684212102595

Epoch: 6| Step: 7
Training loss: 1.8338759515174954
Validation loss: 2.5249570493031133

Epoch: 6| Step: 8
Training loss: 2.048332452590211
Validation loss: 2.5019964439957865

Epoch: 6| Step: 9
Training loss: 2.1536238891024797
Validation loss: 2.487189119704057

Epoch: 6| Step: 10
Training loss: 2.9608046368504652
Validation loss: 2.5188567499833043

Epoch: 6| Step: 11
Training loss: 2.6690998500242387
Validation loss: 2.5009876730555898

Epoch: 6| Step: 12
Training loss: 2.2339767421239163
Validation loss: 2.4982500642845986

Epoch: 6| Step: 13
Training loss: 2.2557067555245176
Validation loss: 2.5788334066824783

Epoch: 394| Step: 0
Training loss: 1.5925281553309665
Validation loss: 2.4885353085790918

Epoch: 6| Step: 1
Training loss: 2.834932175351427
Validation loss: 2.5802006991517055

Epoch: 6| Step: 2
Training loss: 1.9501697613107707
Validation loss: 2.487606646978163

Epoch: 6| Step: 3
Training loss: 1.89374920936291
Validation loss: 2.5082394541839297

Epoch: 6| Step: 4
Training loss: 2.3241366844755493
Validation loss: 2.5240575750571503

Epoch: 6| Step: 5
Training loss: 2.283355890519964
Validation loss: 2.4287750849079317

Epoch: 6| Step: 6
Training loss: 1.5575963703546225
Validation loss: 2.4562595735783086

Epoch: 6| Step: 7
Training loss: 2.050356168742683
Validation loss: 2.502905688043676

Epoch: 6| Step: 8
Training loss: 1.8937312688625074
Validation loss: 2.4826008033905125

Epoch: 6| Step: 9
Training loss: 2.95642389991709
Validation loss: 2.538726316902719

Epoch: 6| Step: 10
Training loss: 2.184717534049371
Validation loss: 2.5553949329974777

Epoch: 6| Step: 11
Training loss: 1.9154007573518494
Validation loss: 2.4492797372946393

Epoch: 6| Step: 12
Training loss: 2.316238474438932
Validation loss: 2.5256430305133164

Epoch: 6| Step: 13
Training loss: 2.6626820497553965
Validation loss: 2.49535386162849

Epoch: 395| Step: 0
Training loss: 2.3509231661463925
Validation loss: 2.5071699180269325

Epoch: 6| Step: 1
Training loss: 1.9352933561738601
Validation loss: 2.4999890932234887

Epoch: 6| Step: 2
Training loss: 2.1268930417937026
Validation loss: 2.4383468418970824

Epoch: 6| Step: 3
Training loss: 2.505666034045623
Validation loss: 2.4904389958659654

Epoch: 6| Step: 4
Training loss: 1.2769273084923045
Validation loss: 2.4724450538864176

Epoch: 6| Step: 5
Training loss: 1.7857619578946327
Validation loss: 2.4527658586690904

Epoch: 6| Step: 6
Training loss: 2.1829207309549874
Validation loss: 2.496701221711871

Epoch: 6| Step: 7
Training loss: 3.1391628524666637
Validation loss: 2.496946843391584

Epoch: 6| Step: 8
Training loss: 1.7723577632247494
Validation loss: 2.4841015650362905

Epoch: 6| Step: 9
Training loss: 1.889776307345933
Validation loss: 2.4475711446876622

Epoch: 6| Step: 10
Training loss: 2.317606777295022
Validation loss: 2.493916579357658

Epoch: 6| Step: 11
Training loss: 2.7465780815943823
Validation loss: 2.484180759899167

Epoch: 6| Step: 12
Training loss: 1.8620002229651493
Validation loss: 2.503588979778628

Epoch: 6| Step: 13
Training loss: 1.832541974095685
Validation loss: 2.475884457542735

Epoch: 396| Step: 0
Training loss: 1.601180016970867
Validation loss: 2.491637488211578

Epoch: 6| Step: 1
Training loss: 1.7480680837356897
Validation loss: 2.5386618192818964

Epoch: 6| Step: 2
Training loss: 2.033345359495692
Validation loss: 2.5086139579894073

Epoch: 6| Step: 3
Training loss: 2.6039476429704846
Validation loss: 2.4655427901124134

Epoch: 6| Step: 4
Training loss: 2.0431314261385873
Validation loss: 2.5104913524706487

Epoch: 6| Step: 5
Training loss: 2.1308761582032685
Validation loss: 2.5346344330308836

Epoch: 6| Step: 6
Training loss: 1.8962032530120283
Validation loss: 2.4450028342772394

Epoch: 6| Step: 7
Training loss: 2.548995182803523
Validation loss: 2.4623115689413466

Epoch: 6| Step: 8
Training loss: 2.0447020899902024
Validation loss: 2.5333967829413284

Epoch: 6| Step: 9
Training loss: 2.7057755545542825
Validation loss: 2.522467900334386

Epoch: 6| Step: 10
Training loss: 2.264351954099382
Validation loss: 2.449756078023005

Epoch: 6| Step: 11
Training loss: 2.0212765499298673
Validation loss: 2.479595404393648

Epoch: 6| Step: 12
Training loss: 2.515343976169249
Validation loss: 2.4726773870740466

Epoch: 6| Step: 13
Training loss: 2.4867836654008806
Validation loss: 2.444367849391993

Epoch: 397| Step: 0
Training loss: 1.52705798195287
Validation loss: 2.478328424212432

Epoch: 6| Step: 1
Training loss: 2.033727924435784
Validation loss: 2.506885366257192

Epoch: 6| Step: 2
Training loss: 2.265692979516245
Validation loss: 2.4923136067123792

Epoch: 6| Step: 3
Training loss: 1.8438430374723678
Validation loss: 2.5664978695251706

Epoch: 6| Step: 4
Training loss: 2.417757730720348
Validation loss: 2.5066290305425016

Epoch: 6| Step: 5
Training loss: 2.8269266361072436
Validation loss: 2.5291471265239496

Epoch: 6| Step: 6
Training loss: 2.0295304282407973
Validation loss: 2.4761665451867576

Epoch: 6| Step: 7
Training loss: 2.233482022341419
Validation loss: 2.461731362354541

Epoch: 6| Step: 8
Training loss: 2.81913195811289
Validation loss: 2.5076559766285422

Epoch: 6| Step: 9
Training loss: 1.7132327217712584
Validation loss: 2.435694900925921

Epoch: 6| Step: 10
Training loss: 2.101030133401223
Validation loss: 2.465334742726262

Epoch: 6| Step: 11
Training loss: 1.8404628092835664
Validation loss: 2.495899317718946

Epoch: 6| Step: 12
Training loss: 2.0682888289137886
Validation loss: 2.445387628554355

Epoch: 6| Step: 13
Training loss: 2.569996730342225
Validation loss: 2.494906278444619

Epoch: 398| Step: 0
Training loss: 2.1466478696833384
Validation loss: 2.52059836950703

Epoch: 6| Step: 1
Training loss: 2.1592367743541674
Validation loss: 2.5014910640345867

Epoch: 6| Step: 2
Training loss: 2.233766913197874
Validation loss: 2.5242526039185234

Epoch: 6| Step: 3
Training loss: 2.007103820890502
Validation loss: 2.474613620433812

Epoch: 6| Step: 4
Training loss: 2.9696233167579718
Validation loss: 2.5046849028346183

Epoch: 6| Step: 5
Training loss: 2.197517346692324
Validation loss: 2.5123409408433495

Epoch: 6| Step: 6
Training loss: 2.201492245638167
Validation loss: 2.4336830240902234

Epoch: 6| Step: 7
Training loss: 2.229364499259069
Validation loss: 2.55826123082461

Epoch: 6| Step: 8
Training loss: 2.2740122275111805
Validation loss: 2.4973811553435117

Epoch: 6| Step: 9
Training loss: 1.8847477254857625
Validation loss: 2.486222278777379

Epoch: 6| Step: 10
Training loss: 1.969122533520251
Validation loss: 2.4884182340488175

Epoch: 6| Step: 11
Training loss: 2.2614712225571436
Validation loss: 2.48231594845005

Epoch: 6| Step: 12
Training loss: 1.6194625552854878
Validation loss: 2.512602822853675

Epoch: 6| Step: 13
Training loss: 1.7335686398613461
Validation loss: 2.4805472681525913

Epoch: 399| Step: 0
Training loss: 2.5117351715357916
Validation loss: 2.4722722961074357

Epoch: 6| Step: 1
Training loss: 2.22285994147798
Validation loss: 2.53024610460153

Epoch: 6| Step: 2
Training loss: 2.503982614194823
Validation loss: 2.5222418875475228

Epoch: 6| Step: 3
Training loss: 2.443602137470419
Validation loss: 2.5047287723205116

Epoch: 6| Step: 4
Training loss: 1.486980761133554
Validation loss: 2.5164487103284454

Epoch: 6| Step: 5
Training loss: 2.4649350592239556
Validation loss: 2.497508246985194

Epoch: 6| Step: 6
Training loss: 1.5829086319707018
Validation loss: 2.4934082466935865

Epoch: 6| Step: 7
Training loss: 1.5827709587949739
Validation loss: 2.5209077369681516

Epoch: 6| Step: 8
Training loss: 2.423710674972184
Validation loss: 2.4579144436069487

Epoch: 6| Step: 9
Training loss: 2.007868784549737
Validation loss: 2.4948147642018768

Epoch: 6| Step: 10
Training loss: 2.4182995398920983
Validation loss: 2.491916049839414

Epoch: 6| Step: 11
Training loss: 2.160299948246549
Validation loss: 2.5368176755912275

Epoch: 6| Step: 12
Training loss: 2.2095395039093413
Validation loss: 2.5345730639383732

Epoch: 6| Step: 13
Training loss: 1.372531235136221
Validation loss: 2.556808628227257

Epoch: 400| Step: 0
Training loss: 2.6327184099992307
Validation loss: 2.49859933685888

Epoch: 6| Step: 1
Training loss: 2.221709520747498
Validation loss: 2.4937185683495127

Epoch: 6| Step: 2
Training loss: 1.846793426932605
Validation loss: 2.4755386363877294

Epoch: 6| Step: 3
Training loss: 2.4329212906755355
Validation loss: 2.47170828932308

Epoch: 6| Step: 4
Training loss: 2.025776222184515
Validation loss: 2.508648516530369

Epoch: 6| Step: 5
Training loss: 2.0878773787874945
Validation loss: 2.5085370694805955

Epoch: 6| Step: 6
Training loss: 1.671349630075275
Validation loss: 2.5098879088842914

Epoch: 6| Step: 7
Training loss: 1.9908606205642085
Validation loss: 2.456170172345114

Epoch: 6| Step: 8
Training loss: 2.604945002907003
Validation loss: 2.4868838056104074

Epoch: 6| Step: 9
Training loss: 2.1678062034362617
Validation loss: 2.498550271569747

Epoch: 6| Step: 10
Training loss: 1.976899009073968
Validation loss: 2.4772306248627207

Epoch: 6| Step: 11
Training loss: 2.0361130235474216
Validation loss: 2.514626954868306

Epoch: 6| Step: 12
Training loss: 2.262365587994446
Validation loss: 2.488058315031702

Epoch: 6| Step: 13
Training loss: 2.2981244276604706
Validation loss: 2.503780651399565

Testing loss: 2.602724554414959

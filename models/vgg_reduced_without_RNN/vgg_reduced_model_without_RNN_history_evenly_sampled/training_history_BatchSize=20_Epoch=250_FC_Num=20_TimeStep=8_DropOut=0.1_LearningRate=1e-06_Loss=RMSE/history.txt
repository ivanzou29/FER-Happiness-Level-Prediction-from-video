Epoch: 1| Step: 0
Training loss: 3.4561950016991965
Validation loss: 3.5012896428069697

Epoch: 5| Step: 1
Training loss: 4.276234382750298
Validation loss: 3.498917091302244

Epoch: 5| Step: 2
Training loss: 3.1254958713023955
Validation loss: 3.4970828349908323

Epoch: 5| Step: 3
Training loss: 3.978980747426546
Validation loss: 3.496930434651684

Epoch: 5| Step: 4
Training loss: 3.378087080073076
Validation loss: 3.4934141497887006

Epoch: 5| Step: 5
Training loss: 3.69399749391447
Validation loss: 3.495670054473631

Epoch: 5| Step: 6
Training loss: 3.3631088443035577
Validation loss: 3.4915779247769554

Epoch: 5| Step: 7
Training loss: 3.65563933660729
Validation loss: 3.4915456623178653

Epoch: 5| Step: 8
Training loss: 4.302453099847993
Validation loss: 3.4896780095169397

Epoch: 5| Step: 9
Training loss: 3.5948470058435538
Validation loss: 3.4895192526876837

Epoch: 5| Step: 10
Training loss: 3.7949169745931433
Validation loss: 3.4847764745397174

Epoch: 2| Step: 0
Training loss: 3.720127780114336
Validation loss: 3.48667270208442

Epoch: 5| Step: 1
Training loss: 4.001462192313414
Validation loss: 3.483328186500807

Epoch: 5| Step: 2
Training loss: 3.8424802093889694
Validation loss: 3.4819496366518634

Epoch: 5| Step: 3
Training loss: 3.5335067694461375
Validation loss: 3.478488777726184

Epoch: 5| Step: 4
Training loss: 4.247249386350361
Validation loss: 3.480268670206949

Epoch: 5| Step: 5
Training loss: 3.6978304714132717
Validation loss: 3.477035969903306

Epoch: 5| Step: 6
Training loss: 3.610735748214291
Validation loss: 3.473309684086999

Epoch: 5| Step: 7
Training loss: 3.5963585715666286
Validation loss: 3.474839492352381

Epoch: 5| Step: 8
Training loss: 3.6182750278193225
Validation loss: 3.474973219878032

Epoch: 5| Step: 9
Training loss: 3.6890183248625745
Validation loss: 3.4707770794938964

Epoch: 5| Step: 10
Training loss: 2.779664005311051
Validation loss: 3.468173949509959

Epoch: 3| Step: 0
Training loss: 3.4942066747181015
Validation loss: 3.467453759952537

Epoch: 5| Step: 1
Training loss: 3.049080544383453
Validation loss: 3.4664018601033533

Epoch: 5| Step: 2
Training loss: 4.0751685172031245
Validation loss: 3.465123190381944

Epoch: 5| Step: 3
Training loss: 4.190358111753258
Validation loss: 3.4639562259560597

Epoch: 5| Step: 4
Training loss: 3.5257717613398523
Validation loss: 3.4626674341074284

Epoch: 5| Step: 5
Training loss: 3.7123804760326125
Validation loss: 3.46025559135659

Epoch: 5| Step: 6
Training loss: 3.4299035493062986
Validation loss: 3.4593933626695

Epoch: 5| Step: 7
Training loss: 4.073608703878473
Validation loss: 3.4572315415507386

Epoch: 5| Step: 8
Training loss: 3.6778064625502744
Validation loss: 3.455670924340812

Epoch: 5| Step: 9
Training loss: 3.8612272319820176
Validation loss: 3.4539878849911263

Epoch: 5| Step: 10
Training loss: 3.1031373299375824
Validation loss: 3.450951448930285

Epoch: 4| Step: 0
Training loss: 3.4843785581014624
Validation loss: 3.449605555588003

Epoch: 5| Step: 1
Training loss: 3.7098870326526043
Validation loss: 3.4476880250941906

Epoch: 5| Step: 2
Training loss: 3.840317596971046
Validation loss: 3.4471403211481384

Epoch: 5| Step: 3
Training loss: 4.072545469626196
Validation loss: 3.4447257301919967

Epoch: 5| Step: 4
Training loss: 3.758905390631397
Validation loss: 3.4445030376343344

Epoch: 5| Step: 5
Training loss: 4.371926672345859
Validation loss: 3.441673090515919

Epoch: 5| Step: 6
Training loss: 3.7750917840372136
Validation loss: 3.4374758707432056

Epoch: 5| Step: 7
Training loss: 2.9875565591350335
Validation loss: 3.4362678624074747

Epoch: 5| Step: 8
Training loss: 3.8289932200625096
Validation loss: 3.436335634756834

Epoch: 5| Step: 9
Training loss: 2.886139725155828
Validation loss: 3.4344399965981256

Epoch: 5| Step: 10
Training loss: 3.2267500335653163
Validation loss: 3.430677365417825

Epoch: 5| Step: 0
Training loss: 4.290213008474177
Validation loss: 3.4297568449956217

Epoch: 5| Step: 1
Training loss: 3.714988793911982
Validation loss: 3.429751364540591

Epoch: 5| Step: 2
Training loss: 2.85691018519561
Validation loss: 3.425936448692247

Epoch: 5| Step: 3
Training loss: 4.0036735355389315
Validation loss: 3.423945184826259

Epoch: 5| Step: 4
Training loss: 3.2098774517588766
Validation loss: 3.4229788699208434

Epoch: 5| Step: 5
Training loss: 3.596734706552098
Validation loss: 3.4204868241626487

Epoch: 5| Step: 6
Training loss: 3.233843478697501
Validation loss: 3.416309305486155

Epoch: 5| Step: 7
Training loss: 4.184363713722877
Validation loss: 3.416241138850594

Epoch: 5| Step: 8
Training loss: 3.086581933057987
Validation loss: 3.4152708050996097

Epoch: 5| Step: 9
Training loss: 4.041522282384299
Validation loss: 3.4122158519263306

Epoch: 5| Step: 10
Training loss: 3.524048069326222
Validation loss: 3.4101046844542946

Epoch: 6| Step: 0
Training loss: 4.256990405870541
Validation loss: 3.4067821465204395

Epoch: 5| Step: 1
Training loss: 3.6726727856732624
Validation loss: 3.4045888216790128

Epoch: 5| Step: 2
Training loss: 3.172179559827917
Validation loss: 3.4032110374681146

Epoch: 5| Step: 3
Training loss: 3.596256078901248
Validation loss: 3.397363005776755

Epoch: 5| Step: 4
Training loss: 3.8982460724152697
Validation loss: 3.4007548554705616

Epoch: 5| Step: 5
Training loss: 4.11852079225483
Validation loss: 3.3959824749323264

Epoch: 5| Step: 6
Training loss: 3.408234044675291
Validation loss: 3.3921421241329885

Epoch: 5| Step: 7
Training loss: 3.6076765812134868
Validation loss: 3.3879482620968298

Epoch: 5| Step: 8
Training loss: 2.5872347709612287
Validation loss: 3.388104874492334

Epoch: 5| Step: 9
Training loss: 3.5595352150190127
Validation loss: 3.3834285611347252

Epoch: 5| Step: 10
Training loss: 3.701856693735865
Validation loss: 3.3806701255725304

Epoch: 7| Step: 0
Training loss: 3.9438512532193113
Validation loss: 3.3811287480604166

Epoch: 5| Step: 1
Training loss: 3.3804013863770703
Validation loss: 3.3745243598732233

Epoch: 5| Step: 2
Training loss: 4.030612865636172
Validation loss: 3.376238435064212

Epoch: 5| Step: 3
Training loss: 3.4399878082502915
Validation loss: 3.3717778121546953

Epoch: 5| Step: 4
Training loss: 2.97690818561491
Validation loss: 3.3683663443069696

Epoch: 5| Step: 5
Training loss: 3.1529132087587395
Validation loss: 3.3672601230280454

Epoch: 5| Step: 6
Training loss: 3.9340906066157157
Validation loss: 3.3633277175919694

Epoch: 5| Step: 7
Training loss: 2.9942122896878525
Validation loss: 3.3611148160853404

Epoch: 5| Step: 8
Training loss: 4.318156195751632
Validation loss: 3.3561680154783256

Epoch: 5| Step: 9
Training loss: 3.16054703190659
Validation loss: 3.354633948162628

Epoch: 5| Step: 10
Training loss: 3.9638141366322444
Validation loss: 3.353293917110801

Epoch: 8| Step: 0
Training loss: 4.334970067326779
Validation loss: 3.3464587738910168

Epoch: 5| Step: 1
Training loss: 3.167502109012037
Validation loss: 3.3433998974232386

Epoch: 5| Step: 2
Training loss: 3.412771718592185
Validation loss: 3.340869357737257

Epoch: 5| Step: 3
Training loss: 3.64602875685324
Validation loss: 3.3410406643375263

Epoch: 5| Step: 4
Training loss: 3.5792178688043235
Validation loss: 3.3350654306179917

Epoch: 5| Step: 5
Training loss: 3.9782566141679934
Validation loss: 3.3324223401510595

Epoch: 5| Step: 6
Training loss: 2.7137868896113715
Validation loss: 3.328228310466256

Epoch: 5| Step: 7
Training loss: 3.9188881248942296
Validation loss: 3.3289672827176773

Epoch: 5| Step: 8
Training loss: 3.343698519016297
Validation loss: 3.32173347579539

Epoch: 5| Step: 9
Training loss: 3.8317542486370706
Validation loss: 3.321661769132558

Epoch: 5| Step: 10
Training loss: 2.9229740788827248
Validation loss: 3.314380105232524

Epoch: 9| Step: 0
Training loss: 3.643005116980847
Validation loss: 3.3136983566285063

Epoch: 5| Step: 1
Training loss: 3.332344925885993
Validation loss: 3.309808744149369

Epoch: 5| Step: 2
Training loss: 4.335189177401307
Validation loss: 3.31143072155409

Epoch: 5| Step: 3
Training loss: 3.2106675083226888
Validation loss: 3.3011771916831645

Epoch: 5| Step: 4
Training loss: 3.225700946897576
Validation loss: 3.302443903143488

Epoch: 5| Step: 5
Training loss: 2.6561058454105186
Validation loss: 3.2932919272012375

Epoch: 5| Step: 6
Training loss: 3.926775905944077
Validation loss: 3.2923105268791373

Epoch: 5| Step: 7
Training loss: 3.849231564016752
Validation loss: 3.2885222930235147

Epoch: 5| Step: 8
Training loss: 3.154311992030339
Validation loss: 3.2850310589396265

Epoch: 5| Step: 9
Training loss: 3.651198590108347
Validation loss: 3.27864473343154

Epoch: 5| Step: 10
Training loss: 3.6298352582975952
Validation loss: 3.277116863800286

Epoch: 10| Step: 0
Training loss: 4.302135451819338
Validation loss: 3.272648227028021

Epoch: 5| Step: 1
Training loss: 3.5272983281959225
Validation loss: 3.266370542685204

Epoch: 5| Step: 2
Training loss: 3.231622971751992
Validation loss: 3.261736256836931

Epoch: 5| Step: 3
Training loss: 3.661239610255468
Validation loss: 3.259408490572438

Epoch: 5| Step: 4
Training loss: 3.2230715582298535
Validation loss: 3.257134624629576

Epoch: 5| Step: 5
Training loss: 3.1010454260910425
Validation loss: 3.2538350592659087

Epoch: 5| Step: 6
Training loss: 3.574855326679592
Validation loss: 3.2482267216546576

Epoch: 5| Step: 7
Training loss: 3.9761461440922523
Validation loss: 3.2463804998210795

Epoch: 5| Step: 8
Training loss: 3.086756189612145
Validation loss: 3.239163107210076

Epoch: 5| Step: 9
Training loss: 2.861826361805005
Validation loss: 3.234993259919757

Epoch: 5| Step: 10
Training loss: 3.6856745711433283
Validation loss: 3.231591503046404

Epoch: 11| Step: 0
Training loss: 3.7270548433281445
Validation loss: 3.225979963630089

Epoch: 5| Step: 1
Training loss: 2.529525357682
Validation loss: 3.214884050701508

Epoch: 5| Step: 2
Training loss: 3.6880516027617873
Validation loss: 3.2158450605723177

Epoch: 5| Step: 3
Training loss: 4.0215716433586035
Validation loss: 3.2121942528423912

Epoch: 5| Step: 4
Training loss: 3.2471930780363776
Validation loss: 3.206440021095889

Epoch: 5| Step: 5
Training loss: 3.781842902492876
Validation loss: 3.197204483819445

Epoch: 5| Step: 6
Training loss: 4.057524230847073
Validation loss: 3.1924558634487137

Epoch: 5| Step: 7
Training loss: 3.532394898748316
Validation loss: 3.1904232141682125

Epoch: 5| Step: 8
Training loss: 2.843986585228638
Validation loss: 3.185991306992494

Epoch: 5| Step: 9
Training loss: 2.766841039179896
Validation loss: 3.1784667060355956

Epoch: 5| Step: 10
Training loss: 3.3987529038265683
Validation loss: 3.172383477241229

Epoch: 12| Step: 0
Training loss: 3.7733957987311424
Validation loss: 3.1665281196384663

Epoch: 5| Step: 1
Training loss: 3.927791191005031
Validation loss: 3.1610903050915238

Epoch: 5| Step: 2
Training loss: 3.088705246159991
Validation loss: 3.1537184039781825

Epoch: 5| Step: 3
Training loss: 2.9462304746412133
Validation loss: 3.1499251025844353

Epoch: 5| Step: 4
Training loss: 3.188863873872842
Validation loss: 3.1383706049508233

Epoch: 5| Step: 5
Training loss: 4.0699399459119885
Validation loss: 3.134745513367971

Epoch: 5| Step: 6
Training loss: 3.433504452808182
Validation loss: 3.136274978580796

Epoch: 5| Step: 7
Training loss: 3.482936134385892
Validation loss: 3.126945672531026

Epoch: 5| Step: 8
Training loss: 2.963526255875371
Validation loss: 3.1189194389445585

Epoch: 5| Step: 9
Training loss: 2.7933634445764652
Validation loss: 3.114744047196495

Epoch: 5| Step: 10
Training loss: 3.4951637096565027
Validation loss: 3.110591560359459

Epoch: 13| Step: 0
Training loss: 3.3062545632112736
Validation loss: 3.1028735472281084

Epoch: 5| Step: 1
Training loss: 2.962010171691203
Validation loss: 3.0929619257823577

Epoch: 5| Step: 2
Training loss: 3.3691397758151105
Validation loss: 3.0841045784190024

Epoch: 5| Step: 3
Training loss: 3.429233946821962
Validation loss: 3.0843053952253885

Epoch: 5| Step: 4
Training loss: 3.9179704538723863
Validation loss: 3.0705391035884757

Epoch: 5| Step: 5
Training loss: 3.395278760564216
Validation loss: 3.0678479869199444

Epoch: 5| Step: 6
Training loss: 3.096026421160663
Validation loss: 3.065087632189822

Epoch: 5| Step: 7
Training loss: 3.5811144738242855
Validation loss: 3.054615058603303

Epoch: 5| Step: 8
Training loss: 2.7271061658571263
Validation loss: 3.0513768982705396

Epoch: 5| Step: 9
Training loss: 3.120843488186934
Validation loss: 3.046311780410884

Epoch: 5| Step: 10
Training loss: 3.7210106910279284
Validation loss: 3.0387341456149572

Epoch: 14| Step: 0
Training loss: 3.3726170922631273
Validation loss: 3.03072038359698

Epoch: 5| Step: 1
Training loss: 3.195910111109738
Validation loss: 3.0240221801387017

Epoch: 5| Step: 2
Training loss: 3.080377364635237
Validation loss: 3.014864162976121

Epoch: 5| Step: 3
Training loss: 3.996331797917972
Validation loss: 3.010233646186885

Epoch: 5| Step: 4
Training loss: 3.2024321015997272
Validation loss: 2.999857694060441

Epoch: 5| Step: 5
Training loss: 3.6787992081508882
Validation loss: 2.9889965329716865

Epoch: 5| Step: 6
Training loss: 3.151529540238067
Validation loss: 2.9848880422404322

Epoch: 5| Step: 7
Training loss: 2.7978103181951277
Validation loss: 2.9833010941665044

Epoch: 5| Step: 8
Training loss: 3.1769365787485633
Validation loss: 2.9732627623447647

Epoch: 5| Step: 9
Training loss: 3.2079158032372326
Validation loss: 2.9652091083332555

Epoch: 5| Step: 10
Training loss: 3.0411032760228665
Validation loss: 2.9559683705074353

Epoch: 15| Step: 0
Training loss: 3.238492546964653
Validation loss: 2.9489869133438

Epoch: 5| Step: 1
Training loss: 2.9534101399006154
Validation loss: 2.946833707112373

Epoch: 5| Step: 2
Training loss: 3.872250227338119
Validation loss: 2.939103627204853

Epoch: 5| Step: 3
Training loss: 3.527958238713498
Validation loss: 2.9329977098949023

Epoch: 5| Step: 4
Training loss: 2.74070121285093
Validation loss: 2.915958779799831

Epoch: 5| Step: 5
Training loss: 2.943443292080941
Validation loss: 2.912957105169074

Epoch: 5| Step: 6
Training loss: 3.0899050160349066
Validation loss: 2.903637828325674

Epoch: 5| Step: 7
Training loss: 3.666297865150995
Validation loss: 2.894240727455225

Epoch: 5| Step: 8
Training loss: 3.8226068078357223
Validation loss: 2.8864979929926493

Epoch: 5| Step: 9
Training loss: 2.4556306325023094
Validation loss: 2.8776398639787613

Epoch: 5| Step: 10
Training loss: 2.5949427608285154
Validation loss: 2.869929735986417

Epoch: 16| Step: 0
Training loss: 3.4434585254088566
Validation loss: 2.869337610380724

Epoch: 5| Step: 1
Training loss: 3.7721673000446794
Validation loss: 2.8546026633530355

Epoch: 5| Step: 2
Training loss: 2.9222080632468295
Validation loss: 2.844974056503134

Epoch: 5| Step: 3
Training loss: 3.0607014259796954
Validation loss: 2.8420480482747474

Epoch: 5| Step: 4
Training loss: 3.1143747895677265
Validation loss: 2.831145752832945

Epoch: 5| Step: 5
Training loss: 3.0796480235332964
Validation loss: 2.825374565952716

Epoch: 5| Step: 6
Training loss: 3.062989020716175
Validation loss: 2.8178577195084737

Epoch: 5| Step: 7
Training loss: 3.3835521708655167
Validation loss: 2.8070226148425017

Epoch: 5| Step: 8
Training loss: 3.0781277884068907
Validation loss: 2.798855919034401

Epoch: 5| Step: 9
Training loss: 2.713658443190902
Validation loss: 2.800435759143668

Epoch: 5| Step: 10
Training loss: 2.810387305315352
Validation loss: 2.7843373441262362

Epoch: 17| Step: 0
Training loss: 3.132199348702085
Validation loss: 2.7760590395727913

Epoch: 5| Step: 1
Training loss: 2.9831898518020155
Validation loss: 2.7682829188059253

Epoch: 5| Step: 2
Training loss: 3.358325599175596
Validation loss: 2.7667632803545996

Epoch: 5| Step: 3
Training loss: 3.209513624664831
Validation loss: 2.7560783659635315

Epoch: 5| Step: 4
Training loss: 3.3320933578802805
Validation loss: 2.7487779532801553

Epoch: 5| Step: 5
Training loss: 2.654166807197635
Validation loss: 2.7443678869408217

Epoch: 5| Step: 6
Training loss: 3.0386962669953963
Validation loss: 2.736689084279113

Epoch: 5| Step: 7
Training loss: 3.2836463533735536
Validation loss: 2.7226497181447864

Epoch: 5| Step: 8
Training loss: 3.0321312635990587
Validation loss: 2.717723290008318

Epoch: 5| Step: 9
Training loss: 2.9305753862896213
Validation loss: 2.707146150748639

Epoch: 5| Step: 10
Training loss: 2.789677570812921
Validation loss: 2.7074834040575326

Epoch: 18| Step: 0
Training loss: 3.428171435365914
Validation loss: 2.682821818565791

Epoch: 5| Step: 1
Training loss: 2.888323718401854
Validation loss: 2.692561952118892

Epoch: 5| Step: 2
Training loss: 2.9901221096659065
Validation loss: 2.688962854494478

Epoch: 5| Step: 3
Training loss: 3.0767634909766364
Validation loss: 2.679638960739615

Epoch: 5| Step: 4
Training loss: 3.037063058579711
Validation loss: 2.673153549103607

Epoch: 5| Step: 5
Training loss: 3.1609913178867433
Validation loss: 2.66523840524353

Epoch: 5| Step: 6
Training loss: 2.538652780193662
Validation loss: 2.65980779973045

Epoch: 5| Step: 7
Training loss: 3.182840269826235
Validation loss: 2.6565756330969794

Epoch: 5| Step: 8
Training loss: 2.9169645611451234
Validation loss: 2.647520847978466

Epoch: 5| Step: 9
Training loss: 2.971607318940946
Validation loss: 2.636437515336911

Epoch: 5| Step: 10
Training loss: 2.9736565449586623
Validation loss: 2.639859316602189

Epoch: 19| Step: 0
Training loss: 2.659698760679223
Validation loss: 2.64212306057653

Epoch: 5| Step: 1
Training loss: 3.2180733571341986
Validation loss: 2.6292726967443425

Epoch: 5| Step: 2
Training loss: 2.5385834696858516
Validation loss: 2.607401238344785

Epoch: 5| Step: 3
Training loss: 2.778724895712245
Validation loss: 2.607407538790264

Epoch: 5| Step: 4
Training loss: 2.839576929038073
Validation loss: 2.6095227322566634

Epoch: 5| Step: 5
Training loss: 3.1194358117503187
Validation loss: 2.594839872038911

Epoch: 5| Step: 6
Training loss: 3.0409729745724605
Validation loss: 2.5888030785387977

Epoch: 5| Step: 7
Training loss: 3.0753361029814723
Validation loss: 2.589239756302948

Epoch: 5| Step: 8
Training loss: 3.149630261474604
Validation loss: 2.577856393533373

Epoch: 5| Step: 9
Training loss: 2.6191685708508645
Validation loss: 2.572233901532632

Epoch: 5| Step: 10
Training loss: 3.535273597413549
Validation loss: 2.559861477251293

Epoch: 20| Step: 0
Training loss: 2.5849590928402937
Validation loss: 2.5689574294390334

Epoch: 5| Step: 1
Training loss: 2.9807587928683876
Validation loss: 2.5526151460025126

Epoch: 5| Step: 2
Training loss: 3.0713738313023518
Validation loss: 2.5504813863274145

Epoch: 5| Step: 3
Training loss: 3.0247889610635927
Validation loss: 2.5561309416459745

Epoch: 5| Step: 4
Training loss: 3.3281002133301607
Validation loss: 2.538856779461988

Epoch: 5| Step: 5
Training loss: 2.9406461708855334
Validation loss: 2.534071599086632

Epoch: 5| Step: 6
Training loss: 3.03543179904689
Validation loss: 2.535190460885876

Epoch: 5| Step: 7
Training loss: 2.7214370252537305
Validation loss: 2.5310531270246197

Epoch: 5| Step: 8
Training loss: 3.1513830751361986
Validation loss: 2.5240445133488114

Epoch: 5| Step: 9
Training loss: 2.8965847952290553
Validation loss: 2.5233125189953887

Epoch: 5| Step: 10
Training loss: 2.3841215946435397
Validation loss: 2.5160407266326703

Epoch: 21| Step: 0
Training loss: 3.168959306353925
Validation loss: 2.517708670263361

Epoch: 5| Step: 1
Training loss: 2.86149310264576
Validation loss: 2.5082885425305004

Epoch: 5| Step: 2
Training loss: 2.6694799028102025
Validation loss: 2.504311248726121

Epoch: 5| Step: 3
Training loss: 3.1913414041568386
Validation loss: 2.5121827209030516

Epoch: 5| Step: 4
Training loss: 2.870867537325628
Validation loss: 2.5060522816165545

Epoch: 5| Step: 5
Training loss: 2.9932196607295514
Validation loss: 2.4935353331490875

Epoch: 5| Step: 6
Training loss: 2.8329531096601657
Validation loss: 2.5003833364058163

Epoch: 5| Step: 7
Training loss: 2.8428346607006634
Validation loss: 2.4942926591198464

Epoch: 5| Step: 8
Training loss: 3.196982546551373
Validation loss: 2.4971667550188643

Epoch: 5| Step: 9
Training loss: 2.382605371696868
Validation loss: 2.4974387626051096

Epoch: 5| Step: 10
Training loss: 2.8823372467227384
Validation loss: 2.4757162045401166

Epoch: 22| Step: 0
Training loss: 2.4563719042164056
Validation loss: 2.483230984743787

Epoch: 5| Step: 1
Training loss: 3.0423890566757197
Validation loss: 2.4895351873963096

Epoch: 5| Step: 2
Training loss: 2.856409588451823
Validation loss: 2.4919914132203704

Epoch: 5| Step: 3
Training loss: 2.760535260418028
Validation loss: 2.4804651295392706

Epoch: 5| Step: 4
Training loss: 2.7827773454470797
Validation loss: 2.463285827845817

Epoch: 5| Step: 5
Training loss: 3.1860086280624507
Validation loss: 2.4597861163583055

Epoch: 5| Step: 6
Training loss: 2.7986868844661266
Validation loss: 2.4648831365662174

Epoch: 5| Step: 7
Training loss: 3.0966256388672555
Validation loss: 2.460907932305054

Epoch: 5| Step: 8
Training loss: 3.077264345940622
Validation loss: 2.478315041842234

Epoch: 5| Step: 9
Training loss: 2.8236200692661795
Validation loss: 2.4661515646200867

Epoch: 5| Step: 10
Training loss: 3.0573625096047605
Validation loss: 2.4736129998123397

Epoch: 23| Step: 0
Training loss: 3.109362424892128
Validation loss: 2.468009078450499

Epoch: 5| Step: 1
Training loss: 2.7493937430986204
Validation loss: 2.4660757400741353

Epoch: 5| Step: 2
Training loss: 2.711480781578514
Validation loss: 2.472685437714713

Epoch: 5| Step: 3
Training loss: 2.7022731960379933
Validation loss: 2.479108788205993

Epoch: 5| Step: 4
Training loss: 2.9029555157319167
Validation loss: 2.4676914996284522

Epoch: 5| Step: 5
Training loss: 3.0962260192755573
Validation loss: 2.4541145242981126

Epoch: 5| Step: 6
Training loss: 2.9599003648742825
Validation loss: 2.465785879412319

Epoch: 5| Step: 7
Training loss: 3.0903174884735436
Validation loss: 2.455693143418297

Epoch: 5| Step: 8
Training loss: 2.6631583640052727
Validation loss: 2.4566369162429855

Epoch: 5| Step: 9
Training loss: 2.7608264738857984
Validation loss: 2.4452742921843367

Epoch: 5| Step: 10
Training loss: 3.043363298294384
Validation loss: 2.468291865673352

Epoch: 24| Step: 0
Training loss: 2.7209574189644847
Validation loss: 2.4585879093070564

Epoch: 5| Step: 1
Training loss: 3.2200301504204427
Validation loss: 2.474098306440677

Epoch: 5| Step: 2
Training loss: 2.5056953406219695
Validation loss: 2.462203645964779

Epoch: 5| Step: 3
Training loss: 2.8782686642919932
Validation loss: 2.4480294967198404

Epoch: 5| Step: 4
Training loss: 2.416298432631936
Validation loss: 2.4591324496175853

Epoch: 5| Step: 5
Training loss: 2.5104871135867937
Validation loss: 2.4459305944333094

Epoch: 5| Step: 6
Training loss: 3.2230887197982803
Validation loss: 2.449126633969316

Epoch: 5| Step: 7
Training loss: 3.00234956921925
Validation loss: 2.4575736170440385

Epoch: 5| Step: 8
Training loss: 3.242699860683164
Validation loss: 2.454471078004392

Epoch: 5| Step: 9
Training loss: 3.0218377033129316
Validation loss: 2.4539785609467435

Epoch: 5| Step: 10
Training loss: 2.945140236590372
Validation loss: 2.454845562108814

Epoch: 25| Step: 0
Training loss: 3.037367007008885
Validation loss: 2.4640700173410766

Epoch: 5| Step: 1
Training loss: 3.340238349313505
Validation loss: 2.46094605261584

Epoch: 5| Step: 2
Training loss: 2.8128530492658013
Validation loss: 2.450311750425864

Epoch: 5| Step: 3
Training loss: 2.367388121913133
Validation loss: 2.4628150633448196

Epoch: 5| Step: 4
Training loss: 2.845865322247883
Validation loss: 2.454068119323979

Epoch: 5| Step: 5
Training loss: 2.9875884805130113
Validation loss: 2.4491545937379904

Epoch: 5| Step: 6
Training loss: 2.8856020614244393
Validation loss: 2.463502883389054

Epoch: 5| Step: 7
Training loss: 2.8690405737958664
Validation loss: 2.4568224275831976

Epoch: 5| Step: 8
Training loss: 2.8023838794345868
Validation loss: 2.448461101750745

Epoch: 5| Step: 9
Training loss: 2.7840091863184675
Validation loss: 2.446938457542836

Epoch: 5| Step: 10
Training loss: 2.9797295169838427
Validation loss: 2.459350039554069

Epoch: 26| Step: 0
Training loss: 2.901995097560409
Validation loss: 2.45475094398493

Epoch: 5| Step: 1
Training loss: 2.4745260343540627
Validation loss: 2.4495691921852556

Epoch: 5| Step: 2
Training loss: 2.962207853825031
Validation loss: 2.4489787195875006

Epoch: 5| Step: 3
Training loss: 2.920718040494548
Validation loss: 2.45256733713378

Epoch: 5| Step: 4
Training loss: 2.9833327465660613
Validation loss: 2.4511538438992164

Epoch: 5| Step: 5
Training loss: 2.8763532356260058
Validation loss: 2.4452057569130914

Epoch: 5| Step: 6
Training loss: 3.049632541221091
Validation loss: 2.4499920413583016

Epoch: 5| Step: 7
Training loss: 2.529217693017324
Validation loss: 2.4457750152658253

Epoch: 5| Step: 8
Training loss: 2.8181169158420274
Validation loss: 2.447829673615949

Epoch: 5| Step: 9
Training loss: 2.8901942576555064
Validation loss: 2.4507206120529923

Epoch: 5| Step: 10
Training loss: 3.365477373291087
Validation loss: 2.4507359872335317

Epoch: 27| Step: 0
Training loss: 2.452552096778894
Validation loss: 2.4464419514895837

Epoch: 5| Step: 1
Training loss: 3.2138925584401954
Validation loss: 2.4463994302476504

Epoch: 5| Step: 2
Training loss: 2.8379097798088933
Validation loss: 2.4520334020513346

Epoch: 5| Step: 3
Training loss: 3.4411037628217995
Validation loss: 2.4554743482500134

Epoch: 5| Step: 4
Training loss: 3.3538304193944666
Validation loss: 2.4670343643677413

Epoch: 5| Step: 5
Training loss: 2.7599123992010015
Validation loss: 2.452785503721809

Epoch: 5| Step: 6
Training loss: 2.5230914844160743
Validation loss: 2.4502411787467064

Epoch: 5| Step: 7
Training loss: 2.788879388520303
Validation loss: 2.4526899992041953

Epoch: 5| Step: 8
Training loss: 2.131609790670567
Validation loss: 2.4516265652672034

Epoch: 5| Step: 9
Training loss: 2.9418202548325696
Validation loss: 2.4454738397679283

Epoch: 5| Step: 10
Training loss: 3.01109091699952
Validation loss: 2.453348026727643

Epoch: 28| Step: 0
Training loss: 2.89084488315938
Validation loss: 2.4430550012813272

Epoch: 5| Step: 1
Training loss: 2.9197147972139867
Validation loss: 2.4498384582757438

Epoch: 5| Step: 2
Training loss: 2.9221961513012245
Validation loss: 2.450158319219465

Epoch: 5| Step: 3
Training loss: 3.0591326515290045
Validation loss: 2.453876353909802

Epoch: 5| Step: 4
Training loss: 3.138006836581639
Validation loss: 2.455690080443456

Epoch: 5| Step: 5
Training loss: 2.993478839993816
Validation loss: 2.4592069724996968

Epoch: 5| Step: 6
Training loss: 2.513996047047957
Validation loss: 2.4407271064797436

Epoch: 5| Step: 7
Training loss: 2.736261033427586
Validation loss: 2.4634045214083313

Epoch: 5| Step: 8
Training loss: 2.9919079682378724
Validation loss: 2.4566066763840757

Epoch: 5| Step: 9
Training loss: 2.7410013181898254
Validation loss: 2.442148925177638

Epoch: 5| Step: 10
Training loss: 2.8034551351654007
Validation loss: 2.4479966832376814

Epoch: 29| Step: 0
Training loss: 2.6376389091757337
Validation loss: 2.453296228758993

Epoch: 5| Step: 1
Training loss: 2.858041587895398
Validation loss: 2.443828160693758

Epoch: 5| Step: 2
Training loss: 2.8431781571123986
Validation loss: 2.444597054877874

Epoch: 5| Step: 3
Training loss: 3.108394295620473
Validation loss: 2.446824269946297

Epoch: 5| Step: 4
Training loss: 2.2926084375059315
Validation loss: 2.4439431061242787

Epoch: 5| Step: 5
Training loss: 3.3855445876773453
Validation loss: 2.443763648905269

Epoch: 5| Step: 6
Training loss: 3.003605265724036
Validation loss: 2.445025194967144

Epoch: 5| Step: 7
Training loss: 2.8217028627667107
Validation loss: 2.4479490443392224

Epoch: 5| Step: 8
Training loss: 2.69214176043922
Validation loss: 2.4488132284557738

Epoch: 5| Step: 9
Training loss: 3.2277767676743814
Validation loss: 2.4508839618767495

Epoch: 5| Step: 10
Training loss: 2.6066074885442028
Validation loss: 2.4473950017180672

Epoch: 30| Step: 0
Training loss: 2.9119893489999176
Validation loss: 2.437129079501279

Epoch: 5| Step: 1
Training loss: 2.6518264738017385
Validation loss: 2.4439421935139305

Epoch: 5| Step: 2
Training loss: 3.045814055570103
Validation loss: 2.45121072613278

Epoch: 5| Step: 3
Training loss: 2.0536084260628744
Validation loss: 2.450446702600611

Epoch: 5| Step: 4
Training loss: 3.473951958341149
Validation loss: 2.452020751272726

Epoch: 5| Step: 5
Training loss: 3.0340115267610708
Validation loss: 2.4492597192351386

Epoch: 5| Step: 6
Training loss: 3.0425650606745456
Validation loss: 2.448338473879128

Epoch: 5| Step: 7
Training loss: 2.724008407764651
Validation loss: 2.456058608026115

Epoch: 5| Step: 8
Training loss: 3.3426764360195573
Validation loss: 2.4490413081733897

Epoch: 5| Step: 9
Training loss: 2.375086531819498
Validation loss: 2.4470746255745346

Epoch: 5| Step: 10
Training loss: 2.720133900572219
Validation loss: 2.4525351660760757

Epoch: 31| Step: 0
Training loss: 2.074172771971583
Validation loss: 2.44878086883982

Epoch: 5| Step: 1
Training loss: 3.183303763633389
Validation loss: 2.437440618523192

Epoch: 5| Step: 2
Training loss: 3.0326377602127113
Validation loss: 2.4495806949867105

Epoch: 5| Step: 3
Training loss: 2.826905129723542
Validation loss: 2.444495019812132

Epoch: 5| Step: 4
Training loss: 2.6527411229218125
Validation loss: 2.445626657075527

Epoch: 5| Step: 5
Training loss: 2.762736131751511
Validation loss: 2.4474224580500223

Epoch: 5| Step: 6
Training loss: 3.429541791110389
Validation loss: 2.4401976526910314

Epoch: 5| Step: 7
Training loss: 2.840015012674545
Validation loss: 2.4540483388090517

Epoch: 5| Step: 8
Training loss: 2.903973744213277
Validation loss: 2.4529501536082052

Epoch: 5| Step: 9
Training loss: 2.7063110626591125
Validation loss: 2.436191257977135

Epoch: 5| Step: 10
Training loss: 2.9634836165514478
Validation loss: 2.4573159745005158

Epoch: 32| Step: 0
Training loss: 2.7610376966998578
Validation loss: 2.446335836154165

Epoch: 5| Step: 1
Training loss: 2.7443854497659776
Validation loss: 2.454674077977229

Epoch: 5| Step: 2
Training loss: 3.5617950812827237
Validation loss: 2.454958590275287

Epoch: 5| Step: 3
Training loss: 2.919234934349874
Validation loss: 2.4516362661113122

Epoch: 5| Step: 4
Training loss: 3.055296385977071
Validation loss: 2.4441387650612536

Epoch: 5| Step: 5
Training loss: 3.0879951666858383
Validation loss: 2.452384667196534

Epoch: 5| Step: 6
Training loss: 1.9189697885287014
Validation loss: 2.4396603720549725

Epoch: 5| Step: 7
Training loss: 2.799980272496074
Validation loss: 2.4517072922282686

Epoch: 5| Step: 8
Training loss: 2.684912123138907
Validation loss: 2.4477731390779973

Epoch: 5| Step: 9
Training loss: 2.632425794868817
Validation loss: 2.4456178642958752

Epoch: 5| Step: 10
Training loss: 3.2514256504925885
Validation loss: 2.4526213413738267

Epoch: 33| Step: 0
Training loss: 2.879596187574086
Validation loss: 2.4322578596642335

Epoch: 5| Step: 1
Training loss: 1.913073350320702
Validation loss: 2.4406641156142643

Epoch: 5| Step: 2
Training loss: 2.8461987359042054
Validation loss: 2.454025998701213

Epoch: 5| Step: 3
Training loss: 3.2294980935272575
Validation loss: 2.45504820844902

Epoch: 5| Step: 4
Training loss: 2.7491134601712113
Validation loss: 2.434544381851571

Epoch: 5| Step: 5
Training loss: 2.7583263231939417
Validation loss: 2.44201655993118

Epoch: 5| Step: 6
Training loss: 3.296366575549939
Validation loss: 2.4445671332831815

Epoch: 5| Step: 7
Training loss: 3.2911196547912565
Validation loss: 2.4583470125340936

Epoch: 5| Step: 8
Training loss: 2.671043221909849
Validation loss: 2.446600659005802

Epoch: 5| Step: 9
Training loss: 2.403924288726955
Validation loss: 2.438017925541623

Epoch: 5| Step: 10
Training loss: 3.1677149660408372
Validation loss: 2.4445197227669597

Epoch: 34| Step: 0
Training loss: 3.034457053775198
Validation loss: 2.443796246999535

Epoch: 5| Step: 1
Training loss: 3.503151972970562
Validation loss: 2.4404091151933063

Epoch: 5| Step: 2
Training loss: 2.9743227404167416
Validation loss: 2.4465874310657405

Epoch: 5| Step: 3
Training loss: 2.6053415521917005
Validation loss: 2.440745149486659

Epoch: 5| Step: 4
Training loss: 1.7886931858596848
Validation loss: 2.4424809945392614

Epoch: 5| Step: 5
Training loss: 2.6400988353666137
Validation loss: 2.4534704757907484

Epoch: 5| Step: 6
Training loss: 2.88485372585722
Validation loss: 2.4472306668565893

Epoch: 5| Step: 7
Training loss: 3.1158514768880416
Validation loss: 2.4375210895998514

Epoch: 5| Step: 8
Training loss: 2.8195852255999263
Validation loss: 2.4409700031319344

Epoch: 5| Step: 9
Training loss: 2.976217895744809
Validation loss: 2.4472751765703156

Epoch: 5| Step: 10
Training loss: 2.9236850945622592
Validation loss: 2.4483329787234145

Epoch: 35| Step: 0
Training loss: 3.117645459486946
Validation loss: 2.448056638578114

Epoch: 5| Step: 1
Training loss: 1.9982097481040906
Validation loss: 2.43799731027599

Epoch: 5| Step: 2
Training loss: 2.684584877415928
Validation loss: 2.4382226539015073

Epoch: 5| Step: 3
Training loss: 2.6395641656393938
Validation loss: 2.453610197393979

Epoch: 5| Step: 4
Training loss: 3.5166499699441793
Validation loss: 2.4441132904583047

Epoch: 5| Step: 5
Training loss: 2.5821928357509276
Validation loss: 2.433246362384664

Epoch: 5| Step: 6
Training loss: 2.9953769985332874
Validation loss: 2.433153263934307

Epoch: 5| Step: 7
Training loss: 2.811056656827298
Validation loss: 2.4344411366964933

Epoch: 5| Step: 8
Training loss: 3.220255379353003
Validation loss: 2.435393930477809

Epoch: 5| Step: 9
Training loss: 3.0775203207725643
Validation loss: 2.4424007960417606

Epoch: 5| Step: 10
Training loss: 2.53540348205231
Validation loss: 2.435716737656713

Epoch: 36| Step: 0
Training loss: 3.0343600963222594
Validation loss: 2.430138308626153

Epoch: 5| Step: 1
Training loss: 3.3610730514963696
Validation loss: 2.4331700687306683

Epoch: 5| Step: 2
Training loss: 2.7912500174932826
Validation loss: 2.43315274607564

Epoch: 5| Step: 3
Training loss: 2.6581323573595697
Validation loss: 2.438289589120854

Epoch: 5| Step: 4
Training loss: 2.8712329233913105
Validation loss: 2.4432524112268426

Epoch: 5| Step: 5
Training loss: 2.784003277250531
Validation loss: 2.4428035987667975

Epoch: 5| Step: 6
Training loss: 2.774476622292739
Validation loss: 2.4449364409271777

Epoch: 5| Step: 7
Training loss: 2.6903166094657855
Validation loss: 2.428170437913653

Epoch: 5| Step: 8
Training loss: 2.603887944882497
Validation loss: 2.4357434620459957

Epoch: 5| Step: 9
Training loss: 3.3474913014533234
Validation loss: 2.4449075259128565

Epoch: 5| Step: 10
Training loss: 2.3404508640515815
Validation loss: 2.435879825436881

Epoch: 37| Step: 0
Training loss: 2.9300289514562943
Validation loss: 2.4356303452811425

Epoch: 5| Step: 1
Training loss: 3.464579644184974
Validation loss: 2.426743736910457

Epoch: 5| Step: 2
Training loss: 2.3895649273758552
Validation loss: 2.4259537440375363

Epoch: 5| Step: 3
Training loss: 3.464601114716417
Validation loss: 2.447918081883161

Epoch: 5| Step: 4
Training loss: 2.8123863197240255
Validation loss: 2.4456200551559464

Epoch: 5| Step: 5
Training loss: 2.736494539851703
Validation loss: 2.433509035238697

Epoch: 5| Step: 6
Training loss: 2.721169458475661
Validation loss: 2.4362299607635736

Epoch: 5| Step: 7
Training loss: 2.8500107815187206
Validation loss: 2.446298381566695

Epoch: 5| Step: 8
Training loss: 2.317278075554024
Validation loss: 2.4509064896159147

Epoch: 5| Step: 9
Training loss: 2.924382240725067
Validation loss: 2.43834843789858

Epoch: 5| Step: 10
Training loss: 2.611703256554824
Validation loss: 2.4399393295452647

Epoch: 38| Step: 0
Training loss: 3.4076287515207575
Validation loss: 2.4356186776549094

Epoch: 5| Step: 1
Training loss: 2.4454960860784762
Validation loss: 2.43348042058581

Epoch: 5| Step: 2
Training loss: 2.7690904532854397
Validation loss: 2.4304988368888187

Epoch: 5| Step: 3
Training loss: 2.93644403185998
Validation loss: 2.429923970970778

Epoch: 5| Step: 4
Training loss: 3.1029824338434806
Validation loss: 2.427234599701109

Epoch: 5| Step: 5
Training loss: 3.1898706633558027
Validation loss: 2.443472564124915

Epoch: 5| Step: 6
Training loss: 2.6556355382755554
Validation loss: 2.429980773076615

Epoch: 5| Step: 7
Training loss: 2.7626494009087197
Validation loss: 2.4379869957427944

Epoch: 5| Step: 8
Training loss: 2.622039306002658
Validation loss: 2.437009639548723

Epoch: 5| Step: 9
Training loss: 2.21066767795081
Validation loss: 2.443228291451948

Epoch: 5| Step: 10
Training loss: 3.0973994760773587
Validation loss: 2.435757931889523

Epoch: 39| Step: 0
Training loss: 2.453648408097436
Validation loss: 2.4312486020470185

Epoch: 5| Step: 1
Training loss: 2.501310005287971
Validation loss: 2.4459900202941114

Epoch: 5| Step: 2
Training loss: 2.982268064038867
Validation loss: 2.4334578948168293

Epoch: 5| Step: 3
Training loss: 2.9485953979584614
Validation loss: 2.438562387267477

Epoch: 5| Step: 4
Training loss: 2.936969790393258
Validation loss: 2.4290780743832494

Epoch: 5| Step: 5
Training loss: 3.2709214184475877
Validation loss: 2.4297244683729353

Epoch: 5| Step: 6
Training loss: 2.7324988331840236
Validation loss: 2.427106559762345

Epoch: 5| Step: 7
Training loss: 2.1963194577432095
Validation loss: 2.44100270627416

Epoch: 5| Step: 8
Training loss: 3.6834574377863913
Validation loss: 2.4380644667150655

Epoch: 5| Step: 9
Training loss: 2.8079421517100123
Validation loss: 2.427543695413375

Epoch: 5| Step: 10
Training loss: 2.448297890271786
Validation loss: 2.4322379787181814

Epoch: 40| Step: 0
Training loss: 2.957893195565396
Validation loss: 2.4330690485517223

Epoch: 5| Step: 1
Training loss: 3.228243263050306
Validation loss: 2.427888714344579

Epoch: 5| Step: 2
Training loss: 2.353562325295226
Validation loss: 2.4337877550349423

Epoch: 5| Step: 3
Training loss: 2.678994136335414
Validation loss: 2.4396716399574094

Epoch: 5| Step: 4
Training loss: 3.2664771907377315
Validation loss: 2.4421049634406025

Epoch: 5| Step: 5
Training loss: 2.8150035312013855
Validation loss: 2.440299124181005

Epoch: 5| Step: 6
Training loss: 2.9278451238226038
Validation loss: 2.4476342384998775

Epoch: 5| Step: 7
Training loss: 2.839289929985762
Validation loss: 2.440577081137903

Epoch: 5| Step: 8
Training loss: 2.1443257884841573
Validation loss: 2.4470766003654125

Epoch: 5| Step: 9
Training loss: 2.5091496880686512
Validation loss: 2.446360971135164

Epoch: 5| Step: 10
Training loss: 3.4403450029937437
Validation loss: 2.4302439550155923

Epoch: 41| Step: 0
Training loss: 2.8517163378879995
Validation loss: 2.436428601328448

Epoch: 5| Step: 1
Training loss: 2.937380524498507
Validation loss: 2.432598129401136

Epoch: 5| Step: 2
Training loss: 2.3851058179944564
Validation loss: 2.4390902300083535

Epoch: 5| Step: 3
Training loss: 3.325409833847449
Validation loss: 2.4368418721929217

Epoch: 5| Step: 4
Training loss: 2.7307919814380277
Validation loss: 2.4414960773965357

Epoch: 5| Step: 5
Training loss: 2.911865387896811
Validation loss: 2.4366141351036563

Epoch: 5| Step: 6
Training loss: 3.0049270859693396
Validation loss: 2.435437435455036

Epoch: 5| Step: 7
Training loss: 2.699154004260335
Validation loss: 2.4293624797178786

Epoch: 5| Step: 8
Training loss: 2.0278097510175903
Validation loss: 2.43438242093258

Epoch: 5| Step: 9
Training loss: 3.112192995181663
Validation loss: 2.4430643127466096

Epoch: 5| Step: 10
Training loss: 3.0245644689352558
Validation loss: 2.446530779801109

Epoch: 42| Step: 0
Training loss: 2.7024600583776377
Validation loss: 2.432493452628587

Epoch: 5| Step: 1
Training loss: 2.7104285914055892
Validation loss: 2.4461210868511256

Epoch: 5| Step: 2
Training loss: 3.245881038081788
Validation loss: 2.436032036460242

Epoch: 5| Step: 3
Training loss: 3.000007311494183
Validation loss: 2.434119470544761

Epoch: 5| Step: 4
Training loss: 2.630074274152925
Validation loss: 2.433066885375726

Epoch: 5| Step: 5
Training loss: 2.7847181834389523
Validation loss: 2.428557543466276

Epoch: 5| Step: 6
Training loss: 2.735723980164701
Validation loss: 2.4425918418736705

Epoch: 5| Step: 7
Training loss: 3.2839748149654175
Validation loss: 2.4316763122630887

Epoch: 5| Step: 8
Training loss: 2.6680471204917477
Validation loss: 2.4376356898535394

Epoch: 5| Step: 9
Training loss: 2.8504491017174702
Validation loss: 2.4287998981385126

Epoch: 5| Step: 10
Training loss: 2.4072808373698864
Validation loss: 2.424550619669896

Epoch: 43| Step: 0
Training loss: 3.0051451113886216
Validation loss: 2.427276644471956

Epoch: 5| Step: 1
Training loss: 3.3235335938299775
Validation loss: 2.4275193699215722

Epoch: 5| Step: 2
Training loss: 2.643417416839126
Validation loss: 2.4297784242847698

Epoch: 5| Step: 3
Training loss: 2.602410896967282
Validation loss: 2.449999488485625

Epoch: 5| Step: 4
Training loss: 3.090113496640397
Validation loss: 2.4245892597940055

Epoch: 5| Step: 5
Training loss: 2.994782997892798
Validation loss: 2.4392204523462815

Epoch: 5| Step: 6
Training loss: 2.545850301915968
Validation loss: 2.4294454544169826

Epoch: 5| Step: 7
Training loss: 2.471473640043562
Validation loss: 2.4251174503356143

Epoch: 5| Step: 8
Training loss: 3.07884618375816
Validation loss: 2.440083212160637

Epoch: 5| Step: 9
Training loss: 2.778774831697793
Validation loss: 2.4324801437572012

Epoch: 5| Step: 10
Training loss: 2.638106640659444
Validation loss: 2.4336791760185754

Epoch: 44| Step: 0
Training loss: 1.774179647830306
Validation loss: 2.4339830390751835

Epoch: 5| Step: 1
Training loss: 2.754399854644408
Validation loss: 2.44238892143991

Epoch: 5| Step: 2
Training loss: 3.152925458932539
Validation loss: 2.420268008522407

Epoch: 5| Step: 3
Training loss: 3.1687066551118144
Validation loss: 2.4390207757424074

Epoch: 5| Step: 4
Training loss: 3.0885777248874438
Validation loss: 2.4312282351761696

Epoch: 5| Step: 5
Training loss: 2.7792065432164588
Validation loss: 2.423342756663872

Epoch: 5| Step: 6
Training loss: 3.3585350228477333
Validation loss: 2.4237221185668525

Epoch: 5| Step: 7
Training loss: 2.8673446570625676
Validation loss: 2.4267451715172106

Epoch: 5| Step: 8
Training loss: 2.460020057782534
Validation loss: 2.435909509595678

Epoch: 5| Step: 9
Training loss: 2.9046311330430528
Validation loss: 2.4436825021629143

Epoch: 5| Step: 10
Training loss: 2.620604058583909
Validation loss: 2.4399281963183768

Epoch: 45| Step: 0
Training loss: 3.0415741022387044
Validation loss: 2.4314733053726836

Epoch: 5| Step: 1
Training loss: 2.3287876901325486
Validation loss: 2.425130109316927

Epoch: 5| Step: 2
Training loss: 2.477079415752713
Validation loss: 2.435119278224192

Epoch: 5| Step: 3
Training loss: 2.78002439776356
Validation loss: 2.4315440443105856

Epoch: 5| Step: 4
Training loss: 3.216162521174865
Validation loss: 2.4314322863261886

Epoch: 5| Step: 5
Training loss: 2.765158317544883
Validation loss: 2.4346588094856756

Epoch: 5| Step: 6
Training loss: 2.641699307196937
Validation loss: 2.440055652757361

Epoch: 5| Step: 7
Training loss: 2.6530419218717567
Validation loss: 2.4386350325512107

Epoch: 5| Step: 8
Training loss: 3.05886219936628
Validation loss: 2.4269290867840905

Epoch: 5| Step: 9
Training loss: 3.3712592410313476
Validation loss: 2.441249521389013

Epoch: 5| Step: 10
Training loss: 2.635832370545216
Validation loss: 2.4237063588856826

Epoch: 46| Step: 0
Training loss: 2.9749381371485426
Validation loss: 2.4230713894507274

Epoch: 5| Step: 1
Training loss: 3.242458837714061
Validation loss: 2.422653154737849

Epoch: 5| Step: 2
Training loss: 2.829934231698204
Validation loss: 2.435383843873284

Epoch: 5| Step: 3
Training loss: 2.4684330217663955
Validation loss: 2.4191536180843305

Epoch: 5| Step: 4
Training loss: 2.011098466075862
Validation loss: 2.436112712970693

Epoch: 5| Step: 5
Training loss: 3.0201022260934627
Validation loss: 2.4350571098216105

Epoch: 5| Step: 6
Training loss: 2.698611244921098
Validation loss: 2.4314131693837377

Epoch: 5| Step: 7
Training loss: 2.75106010811193
Validation loss: 2.4318023067800296

Epoch: 5| Step: 8
Training loss: 2.727595704634742
Validation loss: 2.4270514543737014

Epoch: 5| Step: 9
Training loss: 3.0564513906578967
Validation loss: 2.4242817183588676

Epoch: 5| Step: 10
Training loss: 3.111613325803649
Validation loss: 2.4408158045674964

Epoch: 47| Step: 0
Training loss: 2.8199044986543003
Validation loss: 2.4270530324522612

Epoch: 5| Step: 1
Training loss: 3.290837404762648
Validation loss: 2.4281135797321127

Epoch: 5| Step: 2
Training loss: 3.1256350062835043
Validation loss: 2.434763409484385

Epoch: 5| Step: 3
Training loss: 2.8364449039234243
Validation loss: 2.4392684745390842

Epoch: 5| Step: 4
Training loss: 2.8233517150850997
Validation loss: 2.418739370589968

Epoch: 5| Step: 5
Training loss: 2.3075668710460735
Validation loss: 2.413466979608628

Epoch: 5| Step: 6
Training loss: 3.1162132327729717
Validation loss: 2.42100726515841

Epoch: 5| Step: 7
Training loss: 2.6643142357522835
Validation loss: 2.428744353009457

Epoch: 5| Step: 8
Training loss: 2.5146192352673324
Validation loss: 2.425174905194813

Epoch: 5| Step: 9
Training loss: 2.9900980294784842
Validation loss: 2.4408016440957216

Epoch: 5| Step: 10
Training loss: 2.331328370768707
Validation loss: 2.4358305155712907

Epoch: 48| Step: 0
Training loss: 2.7142200659220066
Validation loss: 2.4239922633077904

Epoch: 5| Step: 1
Training loss: 2.4683368192240716
Validation loss: 2.4288590707805553

Epoch: 5| Step: 2
Training loss: 2.7524447411474164
Validation loss: 2.427206396934901

Epoch: 5| Step: 3
Training loss: 2.957981697639757
Validation loss: 2.429113514341089

Epoch: 5| Step: 4
Training loss: 2.5428304561390274
Validation loss: 2.4228234092347534

Epoch: 5| Step: 5
Training loss: 2.6324881968002676
Validation loss: 2.433372656987269

Epoch: 5| Step: 6
Training loss: 2.930876386375228
Validation loss: 2.4259642434118454

Epoch: 5| Step: 7
Training loss: 3.3227667859435353
Validation loss: 2.4257335547515932

Epoch: 5| Step: 8
Training loss: 3.160714645939915
Validation loss: 2.417903019182979

Epoch: 5| Step: 9
Training loss: 2.477664545871308
Validation loss: 2.4261870919662587

Epoch: 5| Step: 10
Training loss: 3.033611989758535
Validation loss: 2.4327518703823223

Epoch: 49| Step: 0
Training loss: 2.6278189100989593
Validation loss: 2.4239332320367004

Epoch: 5| Step: 1
Training loss: 2.8202069968147394
Validation loss: 2.4362120726791745

Epoch: 5| Step: 2
Training loss: 2.6667819196908362
Validation loss: 2.426467286199932

Epoch: 5| Step: 3
Training loss: 2.9137191774758304
Validation loss: 2.4294302009188424

Epoch: 5| Step: 4
Training loss: 3.0216656996032323
Validation loss: 2.409242159729639

Epoch: 5| Step: 5
Training loss: 3.2791552442055236
Validation loss: 2.4132260245525643

Epoch: 5| Step: 6
Training loss: 2.7856271139765525
Validation loss: 2.4252243534877276

Epoch: 5| Step: 7
Training loss: 2.5557591235126624
Validation loss: 2.42085990048933

Epoch: 5| Step: 8
Training loss: 2.839404968197614
Validation loss: 2.4197439047355083

Epoch: 5| Step: 9
Training loss: 2.901263645703769
Validation loss: 2.4269396595788906

Epoch: 5| Step: 10
Training loss: 2.58556646814226
Validation loss: 2.4185491043634824

Epoch: 50| Step: 0
Training loss: 2.5272328088575486
Validation loss: 2.4250985822523887

Epoch: 5| Step: 1
Training loss: 2.9826012578052605
Validation loss: 2.4410913649422037

Epoch: 5| Step: 2
Training loss: 2.4184642770844667
Validation loss: 2.4253638239963067

Epoch: 5| Step: 3
Training loss: 3.1161784974612012
Validation loss: 2.4275522885880183

Epoch: 5| Step: 4
Training loss: 2.825991927338535
Validation loss: 2.443710299718246

Epoch: 5| Step: 5
Training loss: 2.92990266136992
Validation loss: 2.424351510810524

Epoch: 5| Step: 6
Training loss: 3.3077127664168766
Validation loss: 2.4208468824212206

Epoch: 5| Step: 7
Training loss: 2.647347194883506
Validation loss: 2.4290547869465993

Epoch: 5| Step: 8
Training loss: 2.371372263103641
Validation loss: 2.4198604825803205

Epoch: 5| Step: 9
Training loss: 2.9686545908805315
Validation loss: 2.4170846043752254

Epoch: 5| Step: 10
Training loss: 2.9059362754897142
Validation loss: 2.4394866991755277

Epoch: 51| Step: 0
Training loss: 2.672213527359523
Validation loss: 2.4213049390710535

Epoch: 5| Step: 1
Training loss: 2.9835210247420956
Validation loss: 2.43162463988827

Epoch: 5| Step: 2
Training loss: 2.974867450742521
Validation loss: 2.4325287153053283

Epoch: 5| Step: 3
Training loss: 2.8927027477796123
Validation loss: 2.427363172574662

Epoch: 5| Step: 4
Training loss: 2.848022772247911
Validation loss: 2.431661156046595

Epoch: 5| Step: 5
Training loss: 3.1905117392169324
Validation loss: 2.423210434269397

Epoch: 5| Step: 6
Training loss: 2.8150418133789374
Validation loss: 2.4296551351980353

Epoch: 5| Step: 7
Training loss: 2.6139278324733652
Validation loss: 2.435077290959665

Epoch: 5| Step: 8
Training loss: 2.218803297665313
Validation loss: 2.430269468457078

Epoch: 5| Step: 9
Training loss: 3.0496014256432993
Validation loss: 2.4302037602002753

Epoch: 5| Step: 10
Training loss: 2.647532080437055
Validation loss: 2.431524120626423

Epoch: 52| Step: 0
Training loss: 2.5442913032470966
Validation loss: 2.4358024365393987

Epoch: 5| Step: 1
Training loss: 2.76989485448145
Validation loss: 2.43870921361874

Epoch: 5| Step: 2
Training loss: 2.355515816441301
Validation loss: 2.432086235432463

Epoch: 5| Step: 3
Training loss: 2.8373023077707376
Validation loss: 2.429649509153624

Epoch: 5| Step: 4
Training loss: 2.6462636482560735
Validation loss: 2.4204308667616354

Epoch: 5| Step: 5
Training loss: 3.084651167080453
Validation loss: 2.422805051821781

Epoch: 5| Step: 6
Training loss: 2.749250569962958
Validation loss: 2.434292510069358

Epoch: 5| Step: 7
Training loss: 3.046328138524062
Validation loss: 2.4389242823333275

Epoch: 5| Step: 8
Training loss: 3.212696345522358
Validation loss: 2.4226668636216

Epoch: 5| Step: 9
Training loss: 2.858052933028562
Validation loss: 2.4329100505183083

Epoch: 5| Step: 10
Training loss: 2.808998641114604
Validation loss: 2.431292481713871

Epoch: 53| Step: 0
Training loss: 2.8100142620444237
Validation loss: 2.4361257557485625

Epoch: 5| Step: 1
Training loss: 3.008506952349691
Validation loss: 2.4268444700200726

Epoch: 5| Step: 2
Training loss: 3.0486327749615603
Validation loss: 2.4341176832455718

Epoch: 5| Step: 3
Training loss: 2.5595718028658383
Validation loss: 2.4263361242288424

Epoch: 5| Step: 4
Training loss: 2.2707463752358987
Validation loss: 2.425507234197737

Epoch: 5| Step: 5
Training loss: 2.5931257852629903
Validation loss: 2.4336328554189115

Epoch: 5| Step: 6
Training loss: 2.945804463793177
Validation loss: 2.4215286753530694

Epoch: 5| Step: 7
Training loss: 3.109044503134869
Validation loss: 2.430880506350554

Epoch: 5| Step: 8
Training loss: 3.007170373320209
Validation loss: 2.426658987965966

Epoch: 5| Step: 9
Training loss: 2.0922995710377132
Validation loss: 2.4284946423134515

Epoch: 5| Step: 10
Training loss: 3.3781325319896087
Validation loss: 2.421090864515933

Epoch: 54| Step: 0
Training loss: 3.1306697528646272
Validation loss: 2.418089262459191

Epoch: 5| Step: 1
Training loss: 2.8978346534457304
Validation loss: 2.443996206872034

Epoch: 5| Step: 2
Training loss: 2.256638799332826
Validation loss: 2.425943001034513

Epoch: 5| Step: 3
Training loss: 2.5894221772562793
Validation loss: 2.425221672747599

Epoch: 5| Step: 4
Training loss: 3.3668010162388238
Validation loss: 2.4371830976407702

Epoch: 5| Step: 5
Training loss: 2.842338463211784
Validation loss: 2.414792192797523

Epoch: 5| Step: 6
Training loss: 2.694336202966415
Validation loss: 2.427501210657365

Epoch: 5| Step: 7
Training loss: 2.4572411282268893
Validation loss: 2.4351933253100433

Epoch: 5| Step: 8
Training loss: 2.8352451793003026
Validation loss: 2.42824642095146

Epoch: 5| Step: 9
Training loss: 2.796313266712011
Validation loss: 2.4341052900747893

Epoch: 5| Step: 10
Training loss: 2.9621935271431092
Validation loss: 2.4223128278634607

Epoch: 55| Step: 0
Training loss: 2.782140267862542
Validation loss: 2.434615915021577

Epoch: 5| Step: 1
Training loss: 2.5995051866799903
Validation loss: 2.4260231026831383

Epoch: 5| Step: 2
Training loss: 3.050192411013059
Validation loss: 2.4137309881824174

Epoch: 5| Step: 3
Training loss: 2.872810525203621
Validation loss: 2.4272505091613406

Epoch: 5| Step: 4
Training loss: 2.688649486186385
Validation loss: 2.418007962864136

Epoch: 5| Step: 5
Training loss: 2.5410949084780015
Validation loss: 2.4264907617018996

Epoch: 5| Step: 6
Training loss: 2.747599854811141
Validation loss: 2.4266323410151176

Epoch: 5| Step: 7
Training loss: 3.2952240340113708
Validation loss: 2.4266703067305095

Epoch: 5| Step: 8
Training loss: 2.8498759259437816
Validation loss: 2.420486400371645

Epoch: 5| Step: 9
Training loss: 2.7426708697464703
Validation loss: 2.4217885404536514

Epoch: 5| Step: 10
Training loss: 2.6874567959883477
Validation loss: 2.410082880330283

Epoch: 56| Step: 0
Training loss: 2.5549462796888602
Validation loss: 2.413264900178985

Epoch: 5| Step: 1
Training loss: 3.08168172848784
Validation loss: 2.417096756021418

Epoch: 5| Step: 2
Training loss: 2.6811837781717878
Validation loss: 2.418358741541915

Epoch: 5| Step: 3
Training loss: 2.589219146023143
Validation loss: 2.417947586345724

Epoch: 5| Step: 4
Training loss: 3.191765717114014
Validation loss: 2.4301967629874928

Epoch: 5| Step: 5
Training loss: 2.853354531087598
Validation loss: 2.42513835374029

Epoch: 5| Step: 6
Training loss: 2.7056708722754124
Validation loss: 2.419999716653708

Epoch: 5| Step: 7
Training loss: 3.3441830648753257
Validation loss: 2.426374394645797

Epoch: 5| Step: 8
Training loss: 2.41893979296403
Validation loss: 2.4354947405420924

Epoch: 5| Step: 9
Training loss: 2.295744325697478
Validation loss: 2.4287906011231626

Epoch: 5| Step: 10
Training loss: 3.0248206471640207
Validation loss: 2.4220199451611797

Epoch: 57| Step: 0
Training loss: 2.8381103934605414
Validation loss: 2.4265304707526827

Epoch: 5| Step: 1
Training loss: 3.0773298911610376
Validation loss: 2.4320089956751634

Epoch: 5| Step: 2
Training loss: 2.4804464504411015
Validation loss: 2.428726405569774

Epoch: 5| Step: 3
Training loss: 2.963199767831171
Validation loss: 2.4188721405315103

Epoch: 5| Step: 4
Training loss: 3.0143845927421777
Validation loss: 2.4204320816259437

Epoch: 5| Step: 5
Training loss: 2.4282584449497895
Validation loss: 2.4226969033188697

Epoch: 5| Step: 6
Training loss: 2.7050028546048543
Validation loss: 2.408804398643107

Epoch: 5| Step: 7
Training loss: 3.1577468193279183
Validation loss: 2.420889544316681

Epoch: 5| Step: 8
Training loss: 2.9815284774881414
Validation loss: 2.425987240850922

Epoch: 5| Step: 9
Training loss: 2.6512362565507654
Validation loss: 2.4251185973107563

Epoch: 5| Step: 10
Training loss: 2.4456731268312297
Validation loss: 2.4083165559873723

Epoch: 58| Step: 0
Training loss: 2.718043619228329
Validation loss: 2.409723251973155

Epoch: 5| Step: 1
Training loss: 2.9307371166648624
Validation loss: 2.42010231467309

Epoch: 5| Step: 2
Training loss: 2.988718756269507
Validation loss: 2.4292273983800095

Epoch: 5| Step: 3
Training loss: 3.0042795809946266
Validation loss: 2.4255874860505817

Epoch: 5| Step: 4
Training loss: 2.8994903708592594
Validation loss: 2.4101566467534794

Epoch: 5| Step: 5
Training loss: 2.595926773161175
Validation loss: 2.4275734772814492

Epoch: 5| Step: 6
Training loss: 2.9280315958309058
Validation loss: 2.425954252337012

Epoch: 5| Step: 7
Training loss: 2.700384437601403
Validation loss: 2.4318039218357184

Epoch: 5| Step: 8
Training loss: 2.629461674731742
Validation loss: 2.431249428739079

Epoch: 5| Step: 9
Training loss: 2.7561593004038936
Validation loss: 2.4170695953055374

Epoch: 5| Step: 10
Training loss: 2.7544866721287318
Validation loss: 2.4274808234343843

Epoch: 59| Step: 0
Training loss: 2.9671633043954913
Validation loss: 2.414973765737967

Epoch: 5| Step: 1
Training loss: 3.0853851983017315
Validation loss: 2.4179755737308066

Epoch: 5| Step: 2
Training loss: 2.9823590405033675
Validation loss: 2.4158840255847287

Epoch: 5| Step: 3
Training loss: 2.6904951748778996
Validation loss: 2.4304128381303034

Epoch: 5| Step: 4
Training loss: 2.273537335383094
Validation loss: 2.422356794255899

Epoch: 5| Step: 5
Training loss: 2.6968262775214815
Validation loss: 2.4271209865626076

Epoch: 5| Step: 6
Training loss: 2.972976887838612
Validation loss: 2.423498611757092

Epoch: 5| Step: 7
Training loss: 2.6200357453623258
Validation loss: 2.425715962334371

Epoch: 5| Step: 8
Training loss: 3.007079196070977
Validation loss: 2.43094816522532

Epoch: 5| Step: 9
Training loss: 2.942065971729192
Validation loss: 2.4359216262695997

Epoch: 5| Step: 10
Training loss: 2.5221545852670575
Validation loss: 2.4262705905095516

Epoch: 60| Step: 0
Training loss: 2.530835812437342
Validation loss: 2.426076354198493

Epoch: 5| Step: 1
Training loss: 2.70079689981364
Validation loss: 2.431815890292653

Epoch: 5| Step: 2
Training loss: 2.5584033691395995
Validation loss: 2.427322756845354

Epoch: 5| Step: 3
Training loss: 2.9602889102981287
Validation loss: 2.4234480487179253

Epoch: 5| Step: 4
Training loss: 3.5852079293405628
Validation loss: 2.419082543318897

Epoch: 5| Step: 5
Training loss: 2.3835119002701135
Validation loss: 2.423800049712632

Epoch: 5| Step: 6
Training loss: 2.8540755394266317
Validation loss: 2.4211424578125174

Epoch: 5| Step: 7
Training loss: 2.982018144282424
Validation loss: 2.436408835436414

Epoch: 5| Step: 8
Training loss: 2.9849773018898906
Validation loss: 2.4244608157987066

Epoch: 5| Step: 9
Training loss: 2.8694183235994197
Validation loss: 2.4136692587949744

Epoch: 5| Step: 10
Training loss: 2.099439718799646
Validation loss: 2.429598492723568

Epoch: 61| Step: 0
Training loss: 2.7058619055743725
Validation loss: 2.412790354800784

Epoch: 5| Step: 1
Training loss: 3.2109988315785674
Validation loss: 2.4125007228562256

Epoch: 5| Step: 2
Training loss: 2.799893758665577
Validation loss: 2.427236327640628

Epoch: 5| Step: 3
Training loss: 3.005669164557207
Validation loss: 2.4255759349952686

Epoch: 5| Step: 4
Training loss: 2.791696197201748
Validation loss: 2.413391398057289

Epoch: 5| Step: 5
Training loss: 3.0357498904151776
Validation loss: 2.417278194035212

Epoch: 5| Step: 6
Training loss: 2.9070733657570536
Validation loss: 2.4224854069562562

Epoch: 5| Step: 7
Training loss: 2.9632416066525815
Validation loss: 2.4248235552269644

Epoch: 5| Step: 8
Training loss: 2.278248314469969
Validation loss: 2.420014241436925

Epoch: 5| Step: 9
Training loss: 2.1954541143714543
Validation loss: 2.408330962817943

Epoch: 5| Step: 10
Training loss: 2.7190666452807424
Validation loss: 2.420369529620235

Epoch: 62| Step: 0
Training loss: 2.9187084317565763
Validation loss: 2.4172440080748254

Epoch: 5| Step: 1
Training loss: 3.0794663967771094
Validation loss: 2.4269859716590174

Epoch: 5| Step: 2
Training loss: 2.939096544505608
Validation loss: 2.4294218950823843

Epoch: 5| Step: 3
Training loss: 2.4150937541193973
Validation loss: 2.417945631235005

Epoch: 5| Step: 4
Training loss: 3.006338417196397
Validation loss: 2.4147944827544996

Epoch: 5| Step: 5
Training loss: 2.935708169065593
Validation loss: 2.409923498683782

Epoch: 5| Step: 6
Training loss: 2.2369586438861107
Validation loss: 2.430636361493365

Epoch: 5| Step: 7
Training loss: 2.698243689631549
Validation loss: 2.420181461377872

Epoch: 5| Step: 8
Training loss: 3.0508053201075103
Validation loss: 2.415735162468365

Epoch: 5| Step: 9
Training loss: 2.838256056296027
Validation loss: 2.424463145792819

Epoch: 5| Step: 10
Training loss: 2.485055410431288
Validation loss: 2.415214028439877

Epoch: 63| Step: 0
Training loss: 2.9158331270336837
Validation loss: 2.4183278216188397

Epoch: 5| Step: 1
Training loss: 3.1393360130810057
Validation loss: 2.4172583744245757

Epoch: 5| Step: 2
Training loss: 2.827870900241059
Validation loss: 2.4172529422447857

Epoch: 5| Step: 3
Training loss: 3.0739846825103254
Validation loss: 2.420920180020192

Epoch: 5| Step: 4
Training loss: 2.3399760060832695
Validation loss: 2.409866602531514

Epoch: 5| Step: 5
Training loss: 2.9893177902550363
Validation loss: 2.4244429244146426

Epoch: 5| Step: 6
Training loss: 2.6048909108463003
Validation loss: 2.424648153500882

Epoch: 5| Step: 7
Training loss: 2.91869127757694
Validation loss: 2.4303295873870367

Epoch: 5| Step: 8
Training loss: 2.3919755698522547
Validation loss: 2.414401230166934

Epoch: 5| Step: 9
Training loss: 2.5974513840738105
Validation loss: 2.418019042225196

Epoch: 5| Step: 10
Training loss: 2.786723804602973
Validation loss: 2.4280819008287673

Epoch: 64| Step: 0
Training loss: 2.5509732277847434
Validation loss: 2.421238575653869

Epoch: 5| Step: 1
Training loss: 2.832424373307321
Validation loss: 2.412426039815805

Epoch: 5| Step: 2
Training loss: 2.8540035303277125
Validation loss: 2.420408945081805

Epoch: 5| Step: 3
Training loss: 2.21767969239893
Validation loss: 2.4140620231793877

Epoch: 5| Step: 4
Training loss: 3.1659464937413127
Validation loss: 2.427151489216285

Epoch: 5| Step: 5
Training loss: 2.8455486268758508
Validation loss: 2.4240542697128924

Epoch: 5| Step: 6
Training loss: 2.9107948780186166
Validation loss: 2.412093494275489

Epoch: 5| Step: 7
Training loss: 2.9538807381307897
Validation loss: 2.4114233848222026

Epoch: 5| Step: 8
Training loss: 3.0649558360665776
Validation loss: 2.4189837782712504

Epoch: 5| Step: 9
Training loss: 2.1736034952449392
Validation loss: 2.4202722279946034

Epoch: 5| Step: 10
Training loss: 2.950595963619442
Validation loss: 2.427732515417282

Epoch: 65| Step: 0
Training loss: 2.325361416526098
Validation loss: 2.433603381566463

Epoch: 5| Step: 1
Training loss: 3.519777143316156
Validation loss: 2.4190061198260593

Epoch: 5| Step: 2
Training loss: 3.0108399055156534
Validation loss: 2.4077068372670642

Epoch: 5| Step: 3
Training loss: 2.71980966547184
Validation loss: 2.4196341000657515

Epoch: 5| Step: 4
Training loss: 2.652110026290903
Validation loss: 2.4144758839653977

Epoch: 5| Step: 5
Training loss: 2.7451375976151153
Validation loss: 2.428883471515545

Epoch: 5| Step: 6
Training loss: 3.2140716330065153
Validation loss: 2.425442178033384

Epoch: 5| Step: 7
Training loss: 2.091783197609346
Validation loss: 2.4341679388179442

Epoch: 5| Step: 8
Training loss: 2.624890007257717
Validation loss: 2.4274128910148023

Epoch: 5| Step: 9
Training loss: 2.893345392996036
Validation loss: 2.430145308117112

Epoch: 5| Step: 10
Training loss: 2.6628716008011946
Validation loss: 2.415100430993651

Epoch: 66| Step: 0
Training loss: 2.505226013586477
Validation loss: 2.4255301847082986

Epoch: 5| Step: 1
Training loss: 2.8787727480282355
Validation loss: 2.4303645544600294

Epoch: 5| Step: 2
Training loss: 2.733414748382694
Validation loss: 2.4150584003464166

Epoch: 5| Step: 3
Training loss: 2.796099336783239
Validation loss: 2.4295879473267914

Epoch: 5| Step: 4
Training loss: 3.186689236320259
Validation loss: 2.4279655297855767

Epoch: 5| Step: 5
Training loss: 2.5589953391303624
Validation loss: 2.4187956361990377

Epoch: 5| Step: 6
Training loss: 2.670672854533322
Validation loss: 2.4218504397350697

Epoch: 5| Step: 7
Training loss: 2.4672126329046624
Validation loss: 2.4176412465931634

Epoch: 5| Step: 8
Training loss: 2.843605960090499
Validation loss: 2.436370021849797

Epoch: 5| Step: 9
Training loss: 2.9960415473776862
Validation loss: 2.423113274758924

Epoch: 5| Step: 10
Training loss: 3.1116609844311847
Validation loss: 2.4336999784971427

Epoch: 67| Step: 0
Training loss: 2.8384247268713585
Validation loss: 2.4263282452301054

Epoch: 5| Step: 1
Training loss: 2.686727190905703
Validation loss: 2.4270972990390938

Epoch: 5| Step: 2
Training loss: 2.894228776579633
Validation loss: 2.429980802616741

Epoch: 5| Step: 3
Training loss: 2.9204168097989958
Validation loss: 2.426995474700276

Epoch: 5| Step: 4
Training loss: 2.444666280458099
Validation loss: 2.4228466190104965

Epoch: 5| Step: 5
Training loss: 2.626477779222538
Validation loss: 2.421596497590153

Epoch: 5| Step: 6
Training loss: 3.084208561852283
Validation loss: 2.4288262004350254

Epoch: 5| Step: 7
Training loss: 2.7683841713362294
Validation loss: 2.4339152802376303

Epoch: 5| Step: 8
Training loss: 2.76261557082691
Validation loss: 2.4230180025928463

Epoch: 5| Step: 9
Training loss: 2.9503305928827372
Validation loss: 2.4331140702034117

Epoch: 5| Step: 10
Training loss: 2.7051356781163163
Validation loss: 2.4178166757872246

Epoch: 68| Step: 0
Training loss: 2.903149006624201
Validation loss: 2.423603768519793

Epoch: 5| Step: 1
Training loss: 2.7848971167416905
Validation loss: 2.4221382731051864

Epoch: 5| Step: 2
Training loss: 2.8595572658460315
Validation loss: 2.4230775777599103

Epoch: 5| Step: 3
Training loss: 2.7639313592847943
Validation loss: 2.4252989161990444

Epoch: 5| Step: 4
Training loss: 2.723118133991543
Validation loss: 2.432749513024406

Epoch: 5| Step: 5
Training loss: 3.266175146168561
Validation loss: 2.412842708896923

Epoch: 5| Step: 6
Training loss: 2.2550934254986092
Validation loss: 2.4172461832894747

Epoch: 5| Step: 7
Training loss: 3.3023898747555074
Validation loss: 2.4176601787075778

Epoch: 5| Step: 8
Training loss: 2.47138035354702
Validation loss: 2.422032128144875

Epoch: 5| Step: 9
Training loss: 2.414824232888313
Validation loss: 2.4167746228754017

Epoch: 5| Step: 10
Training loss: 2.68846210734967
Validation loss: 2.4149688745932445

Epoch: 69| Step: 0
Training loss: 2.595786432747744
Validation loss: 2.424549869996337

Epoch: 5| Step: 1
Training loss: 3.2224146157044053
Validation loss: 2.4227370221568227

Epoch: 5| Step: 2
Training loss: 2.9237837651110508
Validation loss: 2.4077824929161022

Epoch: 5| Step: 3
Training loss: 2.4910732157945445
Validation loss: 2.412367186731178

Epoch: 5| Step: 4
Training loss: 2.6873830725494807
Validation loss: 2.4209036189979147

Epoch: 5| Step: 5
Training loss: 2.546203712038902
Validation loss: 2.4186372558606273

Epoch: 5| Step: 6
Training loss: 2.5829086775045154
Validation loss: 2.414447470582799

Epoch: 5| Step: 7
Training loss: 2.5497590568255206
Validation loss: 2.4253319284249804

Epoch: 5| Step: 8
Training loss: 2.817163035053271
Validation loss: 2.4193627492030174

Epoch: 5| Step: 9
Training loss: 2.865556882092351
Validation loss: 2.421349651136315

Epoch: 5| Step: 10
Training loss: 3.268230949897837
Validation loss: 2.4308092903872036

Epoch: 70| Step: 0
Training loss: 2.5496883650697724
Validation loss: 2.415921031150488

Epoch: 5| Step: 1
Training loss: 3.0037098834041327
Validation loss: 2.4135248828254987

Epoch: 5| Step: 2
Training loss: 3.3476827435685887
Validation loss: 2.4178252457457834

Epoch: 5| Step: 3
Training loss: 2.8908999750441815
Validation loss: 2.4190316670102203

Epoch: 5| Step: 4
Training loss: 2.399989418165403
Validation loss: 2.4018997220893836

Epoch: 5| Step: 5
Training loss: 2.2294867336291704
Validation loss: 2.4175780815653245

Epoch: 5| Step: 6
Training loss: 3.3387467930820134
Validation loss: 2.4162133926494156

Epoch: 5| Step: 7
Training loss: 2.552179908222684
Validation loss: 2.4181423292934627

Epoch: 5| Step: 8
Training loss: 2.9312622720750237
Validation loss: 2.40810438269947

Epoch: 5| Step: 9
Training loss: 2.385554002920158
Validation loss: 2.420590358296347

Epoch: 5| Step: 10
Training loss: 2.6247906374319347
Validation loss: 2.421154039549111

Epoch: 71| Step: 0
Training loss: 2.7043671152111446
Validation loss: 2.4154147199840237

Epoch: 5| Step: 1
Training loss: 3.0456681428950048
Validation loss: 2.4117843700587187

Epoch: 5| Step: 2
Training loss: 2.873151101737945
Validation loss: 2.4145701203526264

Epoch: 5| Step: 3
Training loss: 3.1020911700626033
Validation loss: 2.416630502443653

Epoch: 5| Step: 4
Training loss: 2.1496344543912818
Validation loss: 2.4163815493174092

Epoch: 5| Step: 5
Training loss: 2.420496197422305
Validation loss: 2.415534066654234

Epoch: 5| Step: 6
Training loss: 3.2597600663710846
Validation loss: 2.422453684084102

Epoch: 5| Step: 7
Training loss: 3.003328701894945
Validation loss: 2.4426668662416713

Epoch: 5| Step: 8
Training loss: 2.40054904300146
Validation loss: 2.417952922602454

Epoch: 5| Step: 9
Training loss: 2.5264022936776147
Validation loss: 2.4166776326588058

Epoch: 5| Step: 10
Training loss: 2.748298812411905
Validation loss: 2.421704839735242

Epoch: 72| Step: 0
Training loss: 2.60095629345091
Validation loss: 2.4291784130831133

Epoch: 5| Step: 1
Training loss: 2.9411800951093894
Validation loss: 2.4312288931610997

Epoch: 5| Step: 2
Training loss: 2.7616494263145577
Validation loss: 2.4268492955039163

Epoch: 5| Step: 3
Training loss: 2.4077588589663437
Validation loss: 2.4202045820891738

Epoch: 5| Step: 4
Training loss: 2.531522477166395
Validation loss: 2.4313622537514594

Epoch: 5| Step: 5
Training loss: 2.859777370137488
Validation loss: 2.425336975722282

Epoch: 5| Step: 6
Training loss: 2.6117895228010957
Validation loss: 2.426242029962966

Epoch: 5| Step: 7
Training loss: 3.352894338333787
Validation loss: 2.4286576408323746

Epoch: 5| Step: 8
Training loss: 2.628826259172623
Validation loss: 2.4293607986674055

Epoch: 5| Step: 9
Training loss: 2.788862547131574
Validation loss: 2.4282305887525664

Epoch: 5| Step: 10
Training loss: 2.894890684392929
Validation loss: 2.4289689587040733

Epoch: 73| Step: 0
Training loss: 3.0818691041789834
Validation loss: 2.4181723552263534

Epoch: 5| Step: 1
Training loss: 3.018397820505447
Validation loss: 2.412953782009857

Epoch: 5| Step: 2
Training loss: 2.3514030123644045
Validation loss: 2.417146945480619

Epoch: 5| Step: 3
Training loss: 2.614713131138518
Validation loss: 2.4168280299044804

Epoch: 5| Step: 4
Training loss: 2.5863805662138306
Validation loss: 2.422142591456696

Epoch: 5| Step: 5
Training loss: 2.488163392825514
Validation loss: 2.413680980479398

Epoch: 5| Step: 6
Training loss: 2.7848458351187806
Validation loss: 2.421130364590689

Epoch: 5| Step: 7
Training loss: 2.5968173170970736
Validation loss: 2.4205854059524556

Epoch: 5| Step: 8
Training loss: 2.589484050276611
Validation loss: 2.4220480813041902

Epoch: 5| Step: 9
Training loss: 2.7420910766690088
Validation loss: 2.4273737223559837

Epoch: 5| Step: 10
Training loss: 3.5904937587354966
Validation loss: 2.4192531200956897

Epoch: 74| Step: 0
Training loss: 2.2877967641972132
Validation loss: 2.427483391318278

Epoch: 5| Step: 1
Training loss: 2.8319724872675116
Validation loss: 2.433458189795976

Epoch: 5| Step: 2
Training loss: 1.807162944449431
Validation loss: 2.4126410319664457

Epoch: 5| Step: 3
Training loss: 2.7234871470773294
Validation loss: 2.4066248392560747

Epoch: 5| Step: 4
Training loss: 2.7567945119817554
Validation loss: 2.4107911209177733

Epoch: 5| Step: 5
Training loss: 3.0352586805029254
Validation loss: 2.425898605064701

Epoch: 5| Step: 6
Training loss: 2.228442338813318
Validation loss: 2.430432159135415

Epoch: 5| Step: 7
Training loss: 2.7456372072819875
Validation loss: 2.4136193220396054

Epoch: 5| Step: 8
Training loss: 3.07129294390653
Validation loss: 2.419188892935435

Epoch: 5| Step: 9
Training loss: 3.644626497616581
Validation loss: 2.4024726903104057

Epoch: 5| Step: 10
Training loss: 2.902290354015468
Validation loss: 2.401636293778011

Epoch: 75| Step: 0
Training loss: 3.1159780350132538
Validation loss: 2.4308506004771506

Epoch: 5| Step: 1
Training loss: 2.3503373208324794
Validation loss: 2.424143848053403

Epoch: 5| Step: 2
Training loss: 2.6135568516212717
Validation loss: 2.4179289766811274

Epoch: 5| Step: 3
Training loss: 3.225893851911254
Validation loss: 2.411255897734162

Epoch: 5| Step: 4
Training loss: 3.342445190254212
Validation loss: 2.42410377946621

Epoch: 5| Step: 5
Training loss: 1.8281546044194161
Validation loss: 2.4216191961533977

Epoch: 5| Step: 6
Training loss: 2.7668437966172124
Validation loss: 2.4250489331792897

Epoch: 5| Step: 7
Training loss: 2.507837788582095
Validation loss: 2.4382779552479

Epoch: 5| Step: 8
Training loss: 3.3166716596351753
Validation loss: 2.419815863774732

Epoch: 5| Step: 9
Training loss: 2.3473955412709966
Validation loss: 2.422400491040677

Epoch: 5| Step: 10
Training loss: 2.6139707012212785
Validation loss: 2.42523117794528

Epoch: 76| Step: 0
Training loss: 2.786086944833625
Validation loss: 2.4060679519883954

Epoch: 5| Step: 1
Training loss: 3.1036881626343304
Validation loss: 2.431931766114576

Epoch: 5| Step: 2
Training loss: 2.805969540317169
Validation loss: 2.4124644416332144

Epoch: 5| Step: 3
Training loss: 2.6119585781230463
Validation loss: 2.4181916054676664

Epoch: 5| Step: 4
Training loss: 2.8422099544063872
Validation loss: 2.414957692597718

Epoch: 5| Step: 5
Training loss: 2.529706130922263
Validation loss: 2.4212517303198537

Epoch: 5| Step: 6
Training loss: 2.3007485042502083
Validation loss: 2.434493862363607

Epoch: 5| Step: 7
Training loss: 3.1907230612455937
Validation loss: 2.4262623245746093

Epoch: 5| Step: 8
Training loss: 2.5632844050003443
Validation loss: 2.436073245356273

Epoch: 5| Step: 9
Training loss: 3.10044583221841
Validation loss: 2.4076284980780605

Epoch: 5| Step: 10
Training loss: 2.357890995000706
Validation loss: 2.4244847321010536

Epoch: 77| Step: 0
Training loss: 3.184013404568774
Validation loss: 2.4295454402823773

Epoch: 5| Step: 1
Training loss: 2.767953442071742
Validation loss: 2.4357879143357892

Epoch: 5| Step: 2
Training loss: 2.933519629140187
Validation loss: 2.419950290846704

Epoch: 5| Step: 3
Training loss: 2.9010830139464456
Validation loss: 2.4206836311956947

Epoch: 5| Step: 4
Training loss: 2.8880623670273886
Validation loss: 2.429423576090581

Epoch: 5| Step: 5
Training loss: 2.565955089006592
Validation loss: 2.4251788238433765

Epoch: 5| Step: 6
Training loss: 2.814637283346506
Validation loss: 2.439419137349849

Epoch: 5| Step: 7
Training loss: 2.6479803270260325
Validation loss: 2.415754724496129

Epoch: 5| Step: 8
Training loss: 2.161261222461623
Validation loss: 2.4319180504245086

Epoch: 5| Step: 9
Training loss: 2.636378468349194
Validation loss: 2.414495232692972

Epoch: 5| Step: 10
Training loss: 2.566913618452551
Validation loss: 2.4327678902391345

Epoch: 78| Step: 0
Training loss: 2.7366071037411603
Validation loss: 2.4182411605110894

Epoch: 5| Step: 1
Training loss: 2.6850530440846674
Validation loss: 2.425096986516512

Epoch: 5| Step: 2
Training loss: 2.8405252134885823
Validation loss: 2.430973272647161

Epoch: 5| Step: 3
Training loss: 2.8655908280959066
Validation loss: 2.4186345879586826

Epoch: 5| Step: 4
Training loss: 2.5418038461489045
Validation loss: 2.414970698887882

Epoch: 5| Step: 5
Training loss: 2.745803144768495
Validation loss: 2.413246198101037

Epoch: 5| Step: 6
Training loss: 2.5414692432044212
Validation loss: 2.412811761849461

Epoch: 5| Step: 7
Training loss: 2.805389656432804
Validation loss: 2.3992255275356764

Epoch: 5| Step: 8
Training loss: 2.8966501488194534
Validation loss: 2.4249646673407805

Epoch: 5| Step: 9
Training loss: 2.6472071487657636
Validation loss: 2.411003676081087

Epoch: 5| Step: 10
Training loss: 2.9029083729579805
Validation loss: 2.421079638261921

Epoch: 79| Step: 0
Training loss: 3.0164714672801134
Validation loss: 2.421021600176258

Epoch: 5| Step: 1
Training loss: 2.963056384927359
Validation loss: 2.418686506052672

Epoch: 5| Step: 2
Training loss: 2.3877331564932947
Validation loss: 2.4206226425430755

Epoch: 5| Step: 3
Training loss: 2.9770281570122985
Validation loss: 2.412481203515539

Epoch: 5| Step: 4
Training loss: 3.0500784441756577
Validation loss: 2.415992505584363

Epoch: 5| Step: 5
Training loss: 2.4882340119817044
Validation loss: 2.417314496335974

Epoch: 5| Step: 6
Training loss: 2.88104309530708
Validation loss: 2.418760930077689

Epoch: 5| Step: 7
Training loss: 2.4205407189575117
Validation loss: 2.431275216328843

Epoch: 5| Step: 8
Training loss: 2.5842838589558363
Validation loss: 2.4293651959890656

Epoch: 5| Step: 9
Training loss: 2.6300622175566484
Validation loss: 2.417654631848815

Epoch: 5| Step: 10
Training loss: 2.796973200091708
Validation loss: 2.4231837689682734

Epoch: 80| Step: 0
Training loss: 3.148391155346716
Validation loss: 2.424235277419402

Epoch: 5| Step: 1
Training loss: 2.453844390157448
Validation loss: 2.4177024016452657

Epoch: 5| Step: 2
Training loss: 2.924453006037451
Validation loss: 2.417687578794332

Epoch: 5| Step: 3
Training loss: 2.994328860582515
Validation loss: 2.4279879745264257

Epoch: 5| Step: 4
Training loss: 2.549710807109335
Validation loss: 2.4191654700338794

Epoch: 5| Step: 5
Training loss: 2.5043847732219966
Validation loss: 2.417771879830537

Epoch: 5| Step: 6
Training loss: 2.1244646407785215
Validation loss: 2.4326864863114253

Epoch: 5| Step: 7
Training loss: 2.8469006199107896
Validation loss: 2.4256939837874136

Epoch: 5| Step: 8
Training loss: 3.1477646783221855
Validation loss: 2.422200451332177

Epoch: 5| Step: 9
Training loss: 2.889441995572219
Validation loss: 2.4270325564335304

Epoch: 5| Step: 10
Training loss: 2.450372792122361
Validation loss: 2.407476860588326

Epoch: 81| Step: 0
Training loss: 2.8728820629032223
Validation loss: 2.427379883854869

Epoch: 5| Step: 1
Training loss: 2.825443577332842
Validation loss: 2.4325764132721575

Epoch: 5| Step: 2
Training loss: 2.779215036073287
Validation loss: 2.4189409842013534

Epoch: 5| Step: 3
Training loss: 2.7390030414331825
Validation loss: 2.416318075507652

Epoch: 5| Step: 4
Training loss: 3.0642520797260575
Validation loss: 2.4340978712041963

Epoch: 5| Step: 5
Training loss: 2.9626131575336334
Validation loss: 2.4218560574333594

Epoch: 5| Step: 6
Training loss: 2.3754948552227892
Validation loss: 2.4261870856263337

Epoch: 5| Step: 7
Training loss: 2.298576399536908
Validation loss: 2.409368747512448

Epoch: 5| Step: 8
Training loss: 2.6948883165138153
Validation loss: 2.4241742395843424

Epoch: 5| Step: 9
Training loss: 2.425692398483482
Validation loss: 2.4212561794268663

Epoch: 5| Step: 10
Training loss: 3.096878627617152
Validation loss: 2.422582789007042

Epoch: 82| Step: 0
Training loss: 2.4421325091652535
Validation loss: 2.424082513911308

Epoch: 5| Step: 1
Training loss: 2.7084403285875225
Validation loss: 2.4191443661264556

Epoch: 5| Step: 2
Training loss: 3.069162711614718
Validation loss: 2.4181236341930243

Epoch: 5| Step: 3
Training loss: 2.618844671973516
Validation loss: 2.410721506412993

Epoch: 5| Step: 4
Training loss: 2.4385914437483587
Validation loss: 2.426127912272728

Epoch: 5| Step: 5
Training loss: 2.9372389657123947
Validation loss: 2.4164473119723646

Epoch: 5| Step: 6
Training loss: 2.370332347407069
Validation loss: 2.4135509522034275

Epoch: 5| Step: 7
Training loss: 2.458278321334926
Validation loss: 2.4162811333159118

Epoch: 5| Step: 8
Training loss: 2.6435145532544024
Validation loss: 2.403058790575542

Epoch: 5| Step: 9
Training loss: 3.272728453982747
Validation loss: 2.4108331275980883

Epoch: 5| Step: 10
Training loss: 3.0813519753068683
Validation loss: 2.4149250109590743

Epoch: 83| Step: 0
Training loss: 2.597950487036514
Validation loss: 2.4123123376495714

Epoch: 5| Step: 1
Training loss: 3.056513014009522
Validation loss: 2.4115948615933944

Epoch: 5| Step: 2
Training loss: 3.1241942320555833
Validation loss: 2.415359830783929

Epoch: 5| Step: 3
Training loss: 2.6052369525064
Validation loss: 2.3885657604756774

Epoch: 5| Step: 4
Training loss: 2.725827355731539
Validation loss: 2.4010465625145976

Epoch: 5| Step: 5
Training loss: 2.237032823433509
Validation loss: 2.4074715160191453

Epoch: 5| Step: 6
Training loss: 2.9930608286642744
Validation loss: 2.403623212306303

Epoch: 5| Step: 7
Training loss: 3.0501494199079247
Validation loss: 2.427289182350345

Epoch: 5| Step: 8
Training loss: 2.194601792382875
Validation loss: 2.4308068320086846

Epoch: 5| Step: 9
Training loss: 2.78459163897612
Validation loss: 2.4227654171542437

Epoch: 5| Step: 10
Training loss: 2.5708382947019643
Validation loss: 2.4287793239055526

Epoch: 84| Step: 0
Training loss: 2.589626757707021
Validation loss: 2.4258060230471514

Epoch: 5| Step: 1
Training loss: 2.6207981085014738
Validation loss: 2.405515160374154

Epoch: 5| Step: 2
Training loss: 2.620721327223362
Validation loss: 2.415182063577109

Epoch: 5| Step: 3
Training loss: 2.8359270752434083
Validation loss: 2.41605817590355

Epoch: 5| Step: 4
Training loss: 2.6025291686022065
Validation loss: 2.41411194652957

Epoch: 5| Step: 5
Training loss: 3.00764857254186
Validation loss: 2.415228568204597

Epoch: 5| Step: 6
Training loss: 3.0047031729493243
Validation loss: 2.422477620202182

Epoch: 5| Step: 7
Training loss: 2.8450798865501215
Validation loss: 2.4189035595684194

Epoch: 5| Step: 8
Training loss: 2.013066168336701
Validation loss: 2.416391262221753

Epoch: 5| Step: 9
Training loss: 2.774388453826286
Validation loss: 2.415889842859438

Epoch: 5| Step: 10
Training loss: 3.0840856476306535
Validation loss: 2.4210136286990624

Epoch: 85| Step: 0
Training loss: 2.7084980743384457
Validation loss: 2.4209227649230325

Epoch: 5| Step: 1
Training loss: 2.6532183231974216
Validation loss: 2.4249287095081686

Epoch: 5| Step: 2
Training loss: 3.157096824729608
Validation loss: 2.418413745006322

Epoch: 5| Step: 3
Training loss: 2.1914978552182682
Validation loss: 2.429387241572661

Epoch: 5| Step: 4
Training loss: 3.011470799314194
Validation loss: 2.425912940240293

Epoch: 5| Step: 5
Training loss: 2.894703560049202
Validation loss: 2.43091784203628

Epoch: 5| Step: 6
Training loss: 2.8057096950964047
Validation loss: 2.4187575627783966

Epoch: 5| Step: 7
Training loss: 2.937212503852189
Validation loss: 2.4273820098535124

Epoch: 5| Step: 8
Training loss: 2.3662335075533156
Validation loss: 2.4322166303938144

Epoch: 5| Step: 9
Training loss: 3.065416154348512
Validation loss: 2.4195181205641987

Epoch: 5| Step: 10
Training loss: 2.1023549000569473
Validation loss: 2.4304245697760076

Epoch: 86| Step: 0
Training loss: 1.9541161425578948
Validation loss: 2.428311979472296

Epoch: 5| Step: 1
Training loss: 2.238446454247663
Validation loss: 2.420838443354718

Epoch: 5| Step: 2
Training loss: 2.525507122518389
Validation loss: 2.4164997077810457

Epoch: 5| Step: 3
Training loss: 2.42260160313801
Validation loss: 2.4075309498572146

Epoch: 5| Step: 4
Training loss: 2.5726326439472706
Validation loss: 2.435190487104506

Epoch: 5| Step: 5
Training loss: 2.622695092580344
Validation loss: 2.416854170787227

Epoch: 5| Step: 6
Training loss: 2.8766814580767672
Validation loss: 2.4267759065483148

Epoch: 5| Step: 7
Training loss: 3.2926949130376837
Validation loss: 2.414790277597717

Epoch: 5| Step: 8
Training loss: 3.235005340335925
Validation loss: 2.418370111345132

Epoch: 5| Step: 9
Training loss: 3.1818318552491305
Validation loss: 2.408512547067094

Epoch: 5| Step: 10
Training loss: 2.834645547120066
Validation loss: 2.4072228627674734

Epoch: 87| Step: 0
Training loss: 2.915492793414964
Validation loss: 2.4143050291286174

Epoch: 5| Step: 1
Training loss: 2.9542737875932468
Validation loss: 2.4250654489785033

Epoch: 5| Step: 2
Training loss: 2.6420614635471003
Validation loss: 2.393727265457271

Epoch: 5| Step: 3
Training loss: 2.6594599458102657
Validation loss: 2.4163679840751224

Epoch: 5| Step: 4
Training loss: 3.1915357883268967
Validation loss: 2.408488920281189

Epoch: 5| Step: 5
Training loss: 2.3865113613098794
Validation loss: 2.4078700633180476

Epoch: 5| Step: 6
Training loss: 2.4450447496558882
Validation loss: 2.4183358533632737

Epoch: 5| Step: 7
Training loss: 2.595861379881127
Validation loss: 2.406426751802387

Epoch: 5| Step: 8
Training loss: 2.409685926747294
Validation loss: 2.4095770755803945

Epoch: 5| Step: 9
Training loss: 3.1306462967982727
Validation loss: 2.407423166243254

Epoch: 5| Step: 10
Training loss: 2.525825244769372
Validation loss: 2.4233040934902585

Epoch: 88| Step: 0
Training loss: 2.701877195739913
Validation loss: 2.4062083442746323

Epoch: 5| Step: 1
Training loss: 2.3423284670173614
Validation loss: 2.411985884785932

Epoch: 5| Step: 2
Training loss: 3.119527373584963
Validation loss: 2.4188986429850576

Epoch: 5| Step: 3
Training loss: 2.7283239058337707
Validation loss: 2.4042155062100767

Epoch: 5| Step: 4
Training loss: 2.8964792715839227
Validation loss: 2.4305127662685435

Epoch: 5| Step: 5
Training loss: 2.584437277987949
Validation loss: 2.4081404331279592

Epoch: 5| Step: 6
Training loss: 2.398302503135376
Validation loss: 2.418893835555233

Epoch: 5| Step: 7
Training loss: 2.631506983877312
Validation loss: 2.420336257992842

Epoch: 5| Step: 8
Training loss: 3.007278355321349
Validation loss: 2.4257520802343344

Epoch: 5| Step: 9
Training loss: 2.993495406313434
Validation loss: 2.4288618625545473

Epoch: 5| Step: 10
Training loss: 2.6038266888265733
Validation loss: 2.4281498572913427

Epoch: 89| Step: 0
Training loss: 3.347895824001138
Validation loss: 2.4179813944555075

Epoch: 5| Step: 1
Training loss: 2.819191411340428
Validation loss: 2.4288064465598436

Epoch: 5| Step: 2
Training loss: 2.3685485150242713
Validation loss: 2.408975173230063

Epoch: 5| Step: 3
Training loss: 2.1496470982331353
Validation loss: 2.4218445425668818

Epoch: 5| Step: 4
Training loss: 2.1778046600905707
Validation loss: 2.4323014565702272

Epoch: 5| Step: 5
Training loss: 2.9900793711866736
Validation loss: 2.416515448156311

Epoch: 5| Step: 6
Training loss: 3.0649217644767512
Validation loss: 2.4116105117324675

Epoch: 5| Step: 7
Training loss: 2.588331146173808
Validation loss: 2.4350566476401116

Epoch: 5| Step: 8
Training loss: 2.7393825346216234
Validation loss: 2.4249583527558056

Epoch: 5| Step: 9
Training loss: 2.7445404304505048
Validation loss: 2.420077208346106

Epoch: 5| Step: 10
Training loss: 2.7993738530184964
Validation loss: 2.4145370958308017

Epoch: 90| Step: 0
Training loss: 2.64077144419876
Validation loss: 2.4117931161129533

Epoch: 5| Step: 1
Training loss: 3.198805711327401
Validation loss: 2.433947656328281

Epoch: 5| Step: 2
Training loss: 2.134959722491878
Validation loss: 2.4150708901966476

Epoch: 5| Step: 3
Training loss: 2.608076949107722
Validation loss: 2.424368016027886

Epoch: 5| Step: 4
Training loss: 2.8700302957138675
Validation loss: 2.4016840208546024

Epoch: 5| Step: 5
Training loss: 2.947345224446962
Validation loss: 2.411524626064882

Epoch: 5| Step: 6
Training loss: 2.8403714409898098
Validation loss: 2.4204484319243575

Epoch: 5| Step: 7
Training loss: 2.8083109706663696
Validation loss: 2.4251579398153744

Epoch: 5| Step: 8
Training loss: 2.982866635038643
Validation loss: 2.431416292464085

Epoch: 5| Step: 9
Training loss: 2.4325050625253435
Validation loss: 2.424052974701128

Epoch: 5| Step: 10
Training loss: 2.3782247434694725
Validation loss: 2.424080948704949

Epoch: 91| Step: 0
Training loss: 2.5470502360152136
Validation loss: 2.413492344303305

Epoch: 5| Step: 1
Training loss: 3.0013340526982715
Validation loss: 2.416545376546866

Epoch: 5| Step: 2
Training loss: 3.0097735464894133
Validation loss: 2.426833057522457

Epoch: 5| Step: 3
Training loss: 2.252565193206624
Validation loss: 2.4173895660253213

Epoch: 5| Step: 4
Training loss: 2.876070901201038
Validation loss: 2.4220802984906666

Epoch: 5| Step: 5
Training loss: 2.9937450050519394
Validation loss: 2.409163477804484

Epoch: 5| Step: 6
Training loss: 2.5328783049086283
Validation loss: 2.414236087241514

Epoch: 5| Step: 7
Training loss: 2.2585661267254284
Validation loss: 2.4207593514891914

Epoch: 5| Step: 8
Training loss: 2.962617342270789
Validation loss: 2.406148609272737

Epoch: 5| Step: 9
Training loss: 2.705009465086449
Validation loss: 2.4258583657266364

Epoch: 5| Step: 10
Training loss: 2.5992652441738824
Validation loss: 2.426349858793468

Epoch: 92| Step: 0
Training loss: 2.708344112276978
Validation loss: 2.416688644923099

Epoch: 5| Step: 1
Training loss: 2.594239705747891
Validation loss: 2.4242558130196836

Epoch: 5| Step: 2
Training loss: 2.850367967375173
Validation loss: 2.4319263403566436

Epoch: 5| Step: 3
Training loss: 2.5306277982093826
Validation loss: 2.409830892286651

Epoch: 5| Step: 4
Training loss: 2.9634701005622226
Validation loss: 2.419257796470847

Epoch: 5| Step: 5
Training loss: 3.2981298869727333
Validation loss: 2.4165762964677855

Epoch: 5| Step: 6
Training loss: 2.5049362087693297
Validation loss: 2.418836434995296

Epoch: 5| Step: 7
Training loss: 2.5151328796108587
Validation loss: 2.4275955518085697

Epoch: 5| Step: 8
Training loss: 2.6614761998919985
Validation loss: 2.4192197777450892

Epoch: 5| Step: 9
Training loss: 2.315438336919737
Validation loss: 2.400139612491887

Epoch: 5| Step: 10
Training loss: 2.90918739115504
Validation loss: 2.418936067694059

Epoch: 93| Step: 0
Training loss: 2.3805926880029467
Validation loss: 2.4322807865295166

Epoch: 5| Step: 1
Training loss: 2.7529890981639165
Validation loss: 2.4195094045534886

Epoch: 5| Step: 2
Training loss: 2.249046653531661
Validation loss: 2.423405131428411

Epoch: 5| Step: 3
Training loss: 2.761270230358611
Validation loss: 2.421903650152062

Epoch: 5| Step: 4
Training loss: 2.5869839213505315
Validation loss: 2.4099265857860748

Epoch: 5| Step: 5
Training loss: 3.3037383608084414
Validation loss: 2.408394128737699

Epoch: 5| Step: 6
Training loss: 2.4862516021339776
Validation loss: 2.422313382435737

Epoch: 5| Step: 7
Training loss: 3.0767081625782744
Validation loss: 2.4266711075147267

Epoch: 5| Step: 8
Training loss: 3.0130855956682896
Validation loss: 2.428562549224281

Epoch: 5| Step: 9
Training loss: 2.4686103491578493
Validation loss: 2.4326554982110506

Epoch: 5| Step: 10
Training loss: 2.6582181034605092
Validation loss: 2.4204737181630676

Epoch: 94| Step: 0
Training loss: 3.0263547401627693
Validation loss: 2.4316035244509875

Epoch: 5| Step: 1
Training loss: 2.3411289054646165
Validation loss: 2.4263974177812035

Epoch: 5| Step: 2
Training loss: 2.892871375612369
Validation loss: 2.419788711330838

Epoch: 5| Step: 3
Training loss: 2.169178228397555
Validation loss: 2.411224964343402

Epoch: 5| Step: 4
Training loss: 2.7544220137040796
Validation loss: 2.4170457360915374

Epoch: 5| Step: 5
Training loss: 2.477859879270583
Validation loss: 2.412016550674105

Epoch: 5| Step: 6
Training loss: 3.4136455639201584
Validation loss: 2.426881200710921

Epoch: 5| Step: 7
Training loss: 3.1970237123089076
Validation loss: 2.40993037072553

Epoch: 5| Step: 8
Training loss: 2.762973613617162
Validation loss: 2.418980173886477

Epoch: 5| Step: 9
Training loss: 1.7491049521240634
Validation loss: 2.411732753023911

Epoch: 5| Step: 10
Training loss: 2.6656693341949933
Validation loss: 2.417767736049946

Epoch: 95| Step: 0
Training loss: 3.0881363000684234
Validation loss: 2.4200202818396668

Epoch: 5| Step: 1
Training loss: 2.9196475101032235
Validation loss: 2.4209610520832867

Epoch: 5| Step: 2
Training loss: 3.0200179128557707
Validation loss: 2.418215947399974

Epoch: 5| Step: 3
Training loss: 2.5116149026573495
Validation loss: 2.4179622146375257

Epoch: 5| Step: 4
Training loss: 2.7103792434755576
Validation loss: 2.4213691640994726

Epoch: 5| Step: 5
Training loss: 2.3454591749708795
Validation loss: 2.40853023374191

Epoch: 5| Step: 6
Training loss: 3.0886530649681756
Validation loss: 2.4286906485715343

Epoch: 5| Step: 7
Training loss: 1.870800528914838
Validation loss: 2.426072121045102

Epoch: 5| Step: 8
Training loss: 2.548673965644546
Validation loss: 2.4067225626789908

Epoch: 5| Step: 9
Training loss: 2.508140471216086
Validation loss: 2.4286139595141916

Epoch: 5| Step: 10
Training loss: 3.064824526121443
Validation loss: 2.4149769875717815

Epoch: 96| Step: 0
Training loss: 2.347696262723182
Validation loss: 2.4378798351612354

Epoch: 5| Step: 1
Training loss: 2.90886004978679
Validation loss: 2.4219727495455543

Epoch: 5| Step: 2
Training loss: 2.356486895969346
Validation loss: 2.42018553535073

Epoch: 5| Step: 3
Training loss: 2.895345267096524
Validation loss: 2.4160233702412643

Epoch: 5| Step: 4
Training loss: 2.936449228204741
Validation loss: 2.422641657945699

Epoch: 5| Step: 5
Training loss: 2.923920104389904
Validation loss: 2.421759516317576

Epoch: 5| Step: 6
Training loss: 2.8957722792375584
Validation loss: 2.411283795868568

Epoch: 5| Step: 7
Training loss: 2.5240283191742625
Validation loss: 2.4138695976659936

Epoch: 5| Step: 8
Training loss: 2.6939341683504
Validation loss: 2.4122087987980043

Epoch: 5| Step: 9
Training loss: 1.9714729007925953
Validation loss: 2.4185581821106164

Epoch: 5| Step: 10
Training loss: 3.222950832890261
Validation loss: 2.4198536662835965

Epoch: 97| Step: 0
Training loss: 3.0828829943354474
Validation loss: 2.4022607740493362

Epoch: 5| Step: 1
Training loss: 3.0700276560578343
Validation loss: 2.409687447577495

Epoch: 5| Step: 2
Training loss: 2.3724424746325616
Validation loss: 2.4262254529632226

Epoch: 5| Step: 3
Training loss: 2.249358827575732
Validation loss: 2.423700971303131

Epoch: 5| Step: 4
Training loss: 2.618509442699714
Validation loss: 2.416173880097181

Epoch: 5| Step: 5
Training loss: 2.714213038669037
Validation loss: 2.4131170961626895

Epoch: 5| Step: 6
Training loss: 2.714731337600751
Validation loss: 2.4089580139461955

Epoch: 5| Step: 7
Training loss: 2.6280717588059868
Validation loss: 2.408003936131508

Epoch: 5| Step: 8
Training loss: 2.843254800794971
Validation loss: 2.44254697602632

Epoch: 5| Step: 9
Training loss: 3.026695684162246
Validation loss: 2.4171928279227854

Epoch: 5| Step: 10
Training loss: 2.1466830771669967
Validation loss: 2.416333130622977

Epoch: 98| Step: 0
Training loss: 2.2354099037753072
Validation loss: 2.411796202948052

Epoch: 5| Step: 1
Training loss: 2.5655228997567
Validation loss: 2.4006920726550116

Epoch: 5| Step: 2
Training loss: 2.708640584879101
Validation loss: 2.417279021262859

Epoch: 5| Step: 3
Training loss: 2.628172456038454
Validation loss: 2.414100175964366

Epoch: 5| Step: 4
Training loss: 2.8741304492373687
Validation loss: 2.411724409624283

Epoch: 5| Step: 5
Training loss: 3.1347662334500965
Validation loss: 2.4295076292383007

Epoch: 5| Step: 6
Training loss: 3.033137412259047
Validation loss: 2.4048809540517455

Epoch: 5| Step: 7
Training loss: 2.6449766699585924
Validation loss: 2.424387865283879

Epoch: 5| Step: 8
Training loss: 2.6097198075770622
Validation loss: 2.4087246168177345

Epoch: 5| Step: 9
Training loss: 2.4103963228475562
Validation loss: 2.4200941309532955

Epoch: 5| Step: 10
Training loss: 2.9416934983828167
Validation loss: 2.41916300989015

Epoch: 99| Step: 0
Training loss: 2.231568287605673
Validation loss: 2.4183391110011057

Epoch: 5| Step: 1
Training loss: 2.709949940152235
Validation loss: 2.425817378552865

Epoch: 5| Step: 2
Training loss: 2.9169851583339312
Validation loss: 2.409655690805274

Epoch: 5| Step: 3
Training loss: 2.579237449385477
Validation loss: 2.4230194409925634

Epoch: 5| Step: 4
Training loss: 3.4784276181139675
Validation loss: 2.428063624350367

Epoch: 5| Step: 5
Training loss: 2.61938994242967
Validation loss: 2.4228781330542253

Epoch: 5| Step: 6
Training loss: 2.263076717153479
Validation loss: 2.4202626630702886

Epoch: 5| Step: 7
Training loss: 2.635099600514669
Validation loss: 2.4376831480203442

Epoch: 5| Step: 8
Training loss: 2.797241186965586
Validation loss: 2.419381792882261

Epoch: 5| Step: 9
Training loss: 2.534256268411775
Validation loss: 2.4214670166442427

Epoch: 5| Step: 10
Training loss: 2.676975757698584
Validation loss: 2.4111381792560906

Epoch: 100| Step: 0
Training loss: 3.2356554528273898
Validation loss: 2.420029980139787

Epoch: 5| Step: 1
Training loss: 2.5367043210750904
Validation loss: 2.416791633314093

Epoch: 5| Step: 2
Training loss: 2.55821893873233
Validation loss: 2.4320454396536877

Epoch: 5| Step: 3
Training loss: 2.1883147765876565
Validation loss: 2.417781819361276

Epoch: 5| Step: 4
Training loss: 2.849505291215141
Validation loss: 2.423137857137569

Epoch: 5| Step: 5
Training loss: 2.9114189531987527
Validation loss: 2.417871393111804

Epoch: 5| Step: 6
Training loss: 2.5929258025205253
Validation loss: 2.417822827180908

Epoch: 5| Step: 7
Training loss: 2.841173618832301
Validation loss: 2.4357024759686556

Epoch: 5| Step: 8
Training loss: 2.7714584811763863
Validation loss: 2.423385535418434

Epoch: 5| Step: 9
Training loss: 2.4099139767248174
Validation loss: 2.407764035733661

Epoch: 5| Step: 10
Training loss: 2.748747019989494
Validation loss: 2.4298842677892303

Epoch: 101| Step: 0
Training loss: 2.8283769695517726
Validation loss: 2.435916858750268

Epoch: 5| Step: 1
Training loss: 3.0234633649168203
Validation loss: 2.42185360743257

Epoch: 5| Step: 2
Training loss: 2.691147746169174
Validation loss: 2.415023021640496

Epoch: 5| Step: 3
Training loss: 1.727496692748154
Validation loss: 2.4351359552274947

Epoch: 5| Step: 4
Training loss: 2.7282844068660683
Validation loss: 2.4391108896404985

Epoch: 5| Step: 5
Training loss: 2.68314363350075
Validation loss: 2.436354915877088

Epoch: 5| Step: 6
Training loss: 2.9040753832299075
Validation loss: 2.420581944813028

Epoch: 5| Step: 7
Training loss: 2.42690360477555
Validation loss: 2.4109221779574597

Epoch: 5| Step: 8
Training loss: 2.730471107580125
Validation loss: 2.434261523456948

Epoch: 5| Step: 9
Training loss: 2.634967861232889
Validation loss: 2.400593853315749

Epoch: 5| Step: 10
Training loss: 3.1474263955403687
Validation loss: 2.4267174542998178

Epoch: 102| Step: 0
Training loss: 2.708318407066491
Validation loss: 2.420417576826514

Epoch: 5| Step: 1
Training loss: 2.6139758089401135
Validation loss: 2.409287539374058

Epoch: 5| Step: 2
Training loss: 3.5375108159307653
Validation loss: 2.4210182211965017

Epoch: 5| Step: 3
Training loss: 2.1106435846443015
Validation loss: 2.4145604256042983

Epoch: 5| Step: 4
Training loss: 2.586572759224729
Validation loss: 2.4273596925836807

Epoch: 5| Step: 5
Training loss: 2.775807745968392
Validation loss: 2.430935165325501

Epoch: 5| Step: 6
Training loss: 2.61523419266943
Validation loss: 2.427579465081453

Epoch: 5| Step: 7
Training loss: 2.3285539475135018
Validation loss: 2.4068815864444773

Epoch: 5| Step: 8
Training loss: 2.993084885015166
Validation loss: 2.4147039540159603

Epoch: 5| Step: 9
Training loss: 2.3050943306510177
Validation loss: 2.4187255027170416

Epoch: 5| Step: 10
Training loss: 2.9556428378545565
Validation loss: 2.4215859475185657

Epoch: 103| Step: 0
Training loss: 2.7455983047351182
Validation loss: 2.4277901111333096

Epoch: 5| Step: 1
Training loss: 2.738386252834752
Validation loss: 2.4278270408174834

Epoch: 5| Step: 2
Training loss: 2.8366345266216566
Validation loss: 2.417458712790935

Epoch: 5| Step: 3
Training loss: 2.3675961786893134
Validation loss: 2.427244652574054

Epoch: 5| Step: 4
Training loss: 3.0293891149923575
Validation loss: 2.4188610036064278

Epoch: 5| Step: 5
Training loss: 2.5797155848914595
Validation loss: 2.4162257587571596

Epoch: 5| Step: 6
Training loss: 2.060703188494888
Validation loss: 2.412940374927096

Epoch: 5| Step: 7
Training loss: 3.476459218752033
Validation loss: 2.4138588837218142

Epoch: 5| Step: 8
Training loss: 2.7817078117160263
Validation loss: 2.4162542965851004

Epoch: 5| Step: 9
Training loss: 2.176532608779965
Validation loss: 2.4018033997739447

Epoch: 5| Step: 10
Training loss: 2.6250810156536346
Validation loss: 2.4069809450011954

Epoch: 104| Step: 0
Training loss: 2.8053160576735667
Validation loss: 2.4158780915680755

Epoch: 5| Step: 1
Training loss: 2.777637149641904
Validation loss: 2.4198678953076995

Epoch: 5| Step: 2
Training loss: 3.1920477643753564
Validation loss: 2.419942999141108

Epoch: 5| Step: 3
Training loss: 2.8527319613397806
Validation loss: 2.4258761209135438

Epoch: 5| Step: 4
Training loss: 3.13068087158793
Validation loss: 2.421450629816032

Epoch: 5| Step: 5
Training loss: 2.4557253910708536
Validation loss: 2.4088731490398585

Epoch: 5| Step: 6
Training loss: 2.65978714543036
Validation loss: 2.418442716012236

Epoch: 5| Step: 7
Training loss: 2.966986363219219
Validation loss: 2.4342588052772816

Epoch: 5| Step: 8
Training loss: 1.9784253656222786
Validation loss: 2.4299256463564998

Epoch: 5| Step: 9
Training loss: 1.9363468799525443
Validation loss: 2.4150195196214628

Epoch: 5| Step: 10
Training loss: 2.5985337230967005
Validation loss: 2.4019026284510288

Epoch: 105| Step: 0
Training loss: 2.4630805478541062
Validation loss: 2.4204715929810607

Epoch: 5| Step: 1
Training loss: 2.5009525391759135
Validation loss: 2.425482014175151

Epoch: 5| Step: 2
Training loss: 2.470031695297683
Validation loss: 2.424429325499805

Epoch: 5| Step: 3
Training loss: 2.906005028685533
Validation loss: 2.445723769693644

Epoch: 5| Step: 4
Training loss: 2.7861403429402265
Validation loss: 2.4365162753577505

Epoch: 5| Step: 5
Training loss: 2.8347158611881853
Validation loss: 2.448928091694063

Epoch: 5| Step: 6
Training loss: 2.349776391287114
Validation loss: 2.4450753993498724

Epoch: 5| Step: 7
Training loss: 2.774913813996727
Validation loss: 2.440708746128543

Epoch: 5| Step: 8
Training loss: 2.8926143914421303
Validation loss: 2.441033449254389

Epoch: 5| Step: 9
Training loss: 2.7875959029186332
Validation loss: 2.423000848592047

Epoch: 5| Step: 10
Training loss: 2.7503676602207094
Validation loss: 2.4189055690162977

Epoch: 106| Step: 0
Training loss: 2.22638634771259
Validation loss: 2.435922913392976

Epoch: 5| Step: 1
Training loss: 2.3468886658210955
Validation loss: 2.421333089875144

Epoch: 5| Step: 2
Training loss: 2.667795558239792
Validation loss: 2.4348587174564678

Epoch: 5| Step: 3
Training loss: 2.997336317720432
Validation loss: 2.4265941391113905

Epoch: 5| Step: 4
Training loss: 2.8409285749287467
Validation loss: 2.421206155619372

Epoch: 5| Step: 5
Training loss: 2.972748322633734
Validation loss: 2.4164400786559472

Epoch: 5| Step: 6
Training loss: 2.612871214853996
Validation loss: 2.427747464901421

Epoch: 5| Step: 7
Training loss: 2.986500726954884
Validation loss: 2.4284795580821874

Epoch: 5| Step: 8
Training loss: 2.137776839861242
Validation loss: 2.420629167545523

Epoch: 5| Step: 9
Training loss: 2.7616842178710947
Validation loss: 2.4116538323865826

Epoch: 5| Step: 10
Training loss: 2.9540390941225696
Validation loss: 2.4150272858380832

Epoch: 107| Step: 0
Training loss: 2.7748462806259226
Validation loss: 2.4177493847847678

Epoch: 5| Step: 1
Training loss: 3.1684591005565172
Validation loss: 2.4166610627185428

Epoch: 5| Step: 2
Training loss: 2.6034441034519222
Validation loss: 2.4298758991468192

Epoch: 5| Step: 3
Training loss: 2.9529160521524433
Validation loss: 2.4182298065336156

Epoch: 5| Step: 4
Training loss: 2.5460497683495107
Validation loss: 2.4301455074994527

Epoch: 5| Step: 5
Training loss: 2.3217614784059672
Validation loss: 2.408918230202064

Epoch: 5| Step: 6
Training loss: 2.867929471754153
Validation loss: 2.4197523264300145

Epoch: 5| Step: 7
Training loss: 2.383941983041853
Validation loss: 2.439442926968125

Epoch: 5| Step: 8
Training loss: 2.924968856254472
Validation loss: 2.430698319171815

Epoch: 5| Step: 9
Training loss: 2.4222328167818774
Validation loss: 2.428258859860602

Epoch: 5| Step: 10
Training loss: 2.28734328770211
Validation loss: 2.4273302872528064

Epoch: 108| Step: 0
Training loss: 2.392194943008455
Validation loss: 2.4261510904359413

Epoch: 5| Step: 1
Training loss: 2.317365322376581
Validation loss: 2.429685348585232

Epoch: 5| Step: 2
Training loss: 3.1251304599233176
Validation loss: 2.4260879409123217

Epoch: 5| Step: 3
Training loss: 2.34314038295789
Validation loss: 2.4337852722777473

Epoch: 5| Step: 4
Training loss: 2.457804014042139
Validation loss: 2.426289485931664

Epoch: 5| Step: 5
Training loss: 2.193411441772649
Validation loss: 2.4194321691124907

Epoch: 5| Step: 6
Training loss: 3.008167751189969
Validation loss: 2.4320910062444376

Epoch: 5| Step: 7
Training loss: 2.6768756493980232
Validation loss: 2.432774919047105

Epoch: 5| Step: 8
Training loss: 2.825955396479758
Validation loss: 2.4214393057097223

Epoch: 5| Step: 9
Training loss: 2.6786837018415817
Validation loss: 2.42469771932378

Epoch: 5| Step: 10
Training loss: 3.298467893112578
Validation loss: 2.4201326557938616

Epoch: 109| Step: 0
Training loss: 2.5740788932631173
Validation loss: 2.427339606759781

Epoch: 5| Step: 1
Training loss: 2.714060542798256
Validation loss: 2.4278505575520426

Epoch: 5| Step: 2
Training loss: 2.85725933246255
Validation loss: 2.4229712930207725

Epoch: 5| Step: 3
Training loss: 2.677239013974202
Validation loss: 2.4097687246877952

Epoch: 5| Step: 4
Training loss: 2.771047072424789
Validation loss: 2.4173182018337602

Epoch: 5| Step: 5
Training loss: 2.1713156219914125
Validation loss: 2.4253695160036197

Epoch: 5| Step: 6
Training loss: 3.0786353912890623
Validation loss: 2.4210227829768947

Epoch: 5| Step: 7
Training loss: 2.5192414819441473
Validation loss: 2.400431279515569

Epoch: 5| Step: 8
Training loss: 2.921388310172661
Validation loss: 2.429961597239274

Epoch: 5| Step: 9
Training loss: 2.815703707832252
Validation loss: 2.425347580826034

Epoch: 5| Step: 10
Training loss: 2.126632848468743
Validation loss: 2.4122969311798563

Epoch: 110| Step: 0
Training loss: 2.085751871714689
Validation loss: 2.420327343148438

Epoch: 5| Step: 1
Training loss: 2.752548164271965
Validation loss: 2.41944086103063

Epoch: 5| Step: 2
Training loss: 2.835092970519531
Validation loss: 2.4116536452947357

Epoch: 5| Step: 3
Training loss: 2.782174974567798
Validation loss: 2.422362143024296

Epoch: 5| Step: 4
Training loss: 3.17060247743
Validation loss: 2.4067970839755333

Epoch: 5| Step: 5
Training loss: 2.754428678698873
Validation loss: 2.415684254927264

Epoch: 5| Step: 6
Training loss: 2.211980566915511
Validation loss: 2.423983551751166

Epoch: 5| Step: 7
Training loss: 2.758916099121571
Validation loss: 2.427286819687118

Epoch: 5| Step: 8
Training loss: 2.6264208626668277
Validation loss: 2.4159382470929547

Epoch: 5| Step: 9
Training loss: 2.2141415729280416
Validation loss: 2.4150966541567143

Epoch: 5| Step: 10
Training loss: 3.0589395183907855
Validation loss: 2.411344350933247

Epoch: 111| Step: 0
Training loss: 3.1575767825494463
Validation loss: 2.430985759823035

Epoch: 5| Step: 1
Training loss: 2.5277736009369973
Validation loss: 2.4200873693398415

Epoch: 5| Step: 2
Training loss: 2.895109255348204
Validation loss: 2.4329843039299703

Epoch: 5| Step: 3
Training loss: 2.8135168144844918
Validation loss: 2.4240457545616065

Epoch: 5| Step: 4
Training loss: 2.179207697737958
Validation loss: 2.431216585966374

Epoch: 5| Step: 5
Training loss: 2.3859322558472167
Validation loss: 2.4339553537231247

Epoch: 5| Step: 6
Training loss: 2.136223493292183
Validation loss: 2.4301623177983043

Epoch: 5| Step: 7
Training loss: 2.91041676424136
Validation loss: 2.4333242213367554

Epoch: 5| Step: 8
Training loss: 2.882655855383818
Validation loss: 2.4151286880854097

Epoch: 5| Step: 9
Training loss: 2.77421150689991
Validation loss: 2.4270803201624935

Epoch: 5| Step: 10
Training loss: 2.612120776966619
Validation loss: 2.420694826455638

Epoch: 112| Step: 0
Training loss: 2.4208598316556715
Validation loss: 2.431112829878731

Epoch: 5| Step: 1
Training loss: 2.683995734568133
Validation loss: 2.414461375768766

Epoch: 5| Step: 2
Training loss: 3.4441869718105536
Validation loss: 2.424252758969538

Epoch: 5| Step: 3
Training loss: 2.1074554751796066
Validation loss: 2.4116492911531107

Epoch: 5| Step: 4
Training loss: 2.7246982792394876
Validation loss: 2.4275087331186813

Epoch: 5| Step: 5
Training loss: 2.7435643578640883
Validation loss: 2.442846120681889

Epoch: 5| Step: 6
Training loss: 2.5894635181733463
Validation loss: 2.4204291530287323

Epoch: 5| Step: 7
Training loss: 2.6378390270571885
Validation loss: 2.433943674907603

Epoch: 5| Step: 8
Training loss: 2.902110443390252
Validation loss: 2.41131599102021

Epoch: 5| Step: 9
Training loss: 2.5364873885749764
Validation loss: 2.430589462918818

Epoch: 5| Step: 10
Training loss: 2.366288722669058
Validation loss: 2.4267099388934863

Epoch: 113| Step: 0
Training loss: 2.7196258361902084
Validation loss: 2.4289644688412846

Epoch: 5| Step: 1
Training loss: 2.7885007319238233
Validation loss: 2.4178722031718936

Epoch: 5| Step: 2
Training loss: 2.342748097214716
Validation loss: 2.4369775208445463

Epoch: 5| Step: 3
Training loss: 2.535585058592093
Validation loss: 2.4218520974178874

Epoch: 5| Step: 4
Training loss: 2.641551560582681
Validation loss: 2.4328944541477555

Epoch: 5| Step: 5
Training loss: 2.9475908043150456
Validation loss: 2.412207613801079

Epoch: 5| Step: 6
Training loss: 3.0140424143357767
Validation loss: 2.429096504703399

Epoch: 5| Step: 7
Training loss: 2.573822593017228
Validation loss: 2.4079854178325935

Epoch: 5| Step: 8
Training loss: 2.7898220021883193
Validation loss: 2.4276127852818323

Epoch: 5| Step: 9
Training loss: 2.3074627217994004
Validation loss: 2.4363713687138104

Epoch: 5| Step: 10
Training loss: 2.6257874352275317
Validation loss: 2.418035555149975

Epoch: 114| Step: 0
Training loss: 2.675566237226957
Validation loss: 2.4318509234924433

Epoch: 5| Step: 1
Training loss: 2.3460446124245653
Validation loss: 2.4107583849215017

Epoch: 5| Step: 2
Training loss: 2.778337561045975
Validation loss: 2.420721807761898

Epoch: 5| Step: 3
Training loss: 2.770567706767991
Validation loss: 2.444136822510196

Epoch: 5| Step: 4
Training loss: 2.78714017042377
Validation loss: 2.4335728623376003

Epoch: 5| Step: 5
Training loss: 2.6017095850700103
Validation loss: 2.4328118963545964

Epoch: 5| Step: 6
Training loss: 2.774147394154797
Validation loss: 2.414116160832472

Epoch: 5| Step: 7
Training loss: 2.2700384889608207
Validation loss: 2.4133827172817766

Epoch: 5| Step: 8
Training loss: 2.802111364363865
Validation loss: 2.4203010242262417

Epoch: 5| Step: 9
Training loss: 2.7807309812968675
Validation loss: 2.415544081194383

Epoch: 5| Step: 10
Training loss: 2.5909915641726844
Validation loss: 2.410971736209634

Epoch: 115| Step: 0
Training loss: 2.2528175091274374
Validation loss: 2.427067561500833

Epoch: 5| Step: 1
Training loss: 2.8323603997840983
Validation loss: 2.414162302109653

Epoch: 5| Step: 2
Training loss: 2.565664988495364
Validation loss: 2.4359049178099035

Epoch: 5| Step: 3
Training loss: 2.583661971159033
Validation loss: 2.4185234301816756

Epoch: 5| Step: 4
Training loss: 2.397916557075593
Validation loss: 2.4267097835987887

Epoch: 5| Step: 5
Training loss: 2.389047438759292
Validation loss: 2.396313568250676

Epoch: 5| Step: 6
Training loss: 2.2308849474189847
Validation loss: 2.4400074191126064

Epoch: 5| Step: 7
Training loss: 2.9896001161819776
Validation loss: 2.4334965999894576

Epoch: 5| Step: 8
Training loss: 3.040214104641808
Validation loss: 2.425284089038204

Epoch: 5| Step: 9
Training loss: 3.017901416237761
Validation loss: 2.4146348525836174

Epoch: 5| Step: 10
Training loss: 2.8031194442302714
Validation loss: 2.42283535592188

Epoch: 116| Step: 0
Training loss: 2.2933173621346037
Validation loss: 2.418519618410049

Epoch: 5| Step: 1
Training loss: 2.7234297191445256
Validation loss: 2.4244065078498145

Epoch: 5| Step: 2
Training loss: 2.8085114065097914
Validation loss: 2.405564548138036

Epoch: 5| Step: 3
Training loss: 2.551124631426462
Validation loss: 2.4222900225561443

Epoch: 5| Step: 4
Training loss: 2.5242476447201057
Validation loss: 2.42475463645842

Epoch: 5| Step: 5
Training loss: 2.1783757244714588
Validation loss: 2.4260707029476314

Epoch: 5| Step: 6
Training loss: 3.3285927600516207
Validation loss: 2.4171630936838495

Epoch: 5| Step: 7
Training loss: 2.72121598229022
Validation loss: 2.4211321858310066

Epoch: 5| Step: 8
Training loss: 2.7137517475560067
Validation loss: 2.417382208267992

Epoch: 5| Step: 9
Training loss: 2.43646707412263
Validation loss: 2.4287814824560305

Epoch: 5| Step: 10
Training loss: 2.9150109268952074
Validation loss: 2.4247392297490307

Epoch: 117| Step: 0
Training loss: 2.2609651196103684
Validation loss: 2.4312357007639616

Epoch: 5| Step: 1
Training loss: 2.900729607189943
Validation loss: 2.4326358850318557

Epoch: 5| Step: 2
Training loss: 2.7279320367865947
Validation loss: 2.43813539084843

Epoch: 5| Step: 3
Training loss: 2.838433630517634
Validation loss: 2.4349364174608326

Epoch: 5| Step: 4
Training loss: 2.5624193318393145
Validation loss: 2.424840706905836

Epoch: 5| Step: 5
Training loss: 2.4319376936385
Validation loss: 2.431460550783917

Epoch: 5| Step: 6
Training loss: 2.9266227459345497
Validation loss: 2.431938987087025

Epoch: 5| Step: 7
Training loss: 2.501193429287153
Validation loss: 2.4002443854437865

Epoch: 5| Step: 8
Training loss: 2.930623873797668
Validation loss: 2.4279386268658962

Epoch: 5| Step: 9
Training loss: 2.4102802955694695
Validation loss: 2.4318416012443675

Epoch: 5| Step: 10
Training loss: 2.756843547965939
Validation loss: 2.4306189174176938

Epoch: 118| Step: 0
Training loss: 2.158584878567094
Validation loss: 2.4295763340422014

Epoch: 5| Step: 1
Training loss: 2.407462173909474
Validation loss: 2.4285047332518848

Epoch: 5| Step: 2
Training loss: 2.9687206066081897
Validation loss: 2.4230587831949797

Epoch: 5| Step: 3
Training loss: 2.8823524666290075
Validation loss: 2.4143466067450174

Epoch: 5| Step: 4
Training loss: 2.7758856485598637
Validation loss: 2.42824801831282

Epoch: 5| Step: 5
Training loss: 2.8042359054958044
Validation loss: 2.4192436157830426

Epoch: 5| Step: 6
Training loss: 2.9222416775164173
Validation loss: 2.428056939833931

Epoch: 5| Step: 7
Training loss: 2.3815689594184706
Validation loss: 2.4125852710145064

Epoch: 5| Step: 8
Training loss: 2.825365775405291
Validation loss: 2.411139317464044

Epoch: 5| Step: 9
Training loss: 2.618767560294551
Validation loss: 2.427622871437661

Epoch: 5| Step: 10
Training loss: 2.2537207039501532
Validation loss: 2.416692806466033

Epoch: 119| Step: 0
Training loss: 2.37446236799361
Validation loss: 2.429300424651743

Epoch: 5| Step: 1
Training loss: 2.941765144003781
Validation loss: 2.4228151437074734

Epoch: 5| Step: 2
Training loss: 2.7490445991636654
Validation loss: 2.4290101829401856

Epoch: 5| Step: 3
Training loss: 2.668993729264921
Validation loss: 2.4030102133234874

Epoch: 5| Step: 4
Training loss: 2.7347153806000772
Validation loss: 2.4271226485675843

Epoch: 5| Step: 5
Training loss: 2.628564231356251
Validation loss: 2.4189369749011576

Epoch: 5| Step: 6
Training loss: 2.1743738084193884
Validation loss: 2.4212354563866256

Epoch: 5| Step: 7
Training loss: 2.7219245179114124
Validation loss: 2.430511337049931

Epoch: 5| Step: 8
Training loss: 3.1526598765939453
Validation loss: 2.4407387454843965

Epoch: 5| Step: 9
Training loss: 2.2668805030859027
Validation loss: 2.427995105864639

Epoch: 5| Step: 10
Training loss: 2.756885405153068
Validation loss: 2.427524367259658

Epoch: 120| Step: 0
Training loss: 2.1084679808108864
Validation loss: 2.4209475887449803

Epoch: 5| Step: 1
Training loss: 2.038199640455524
Validation loss: 2.4204263356407285

Epoch: 5| Step: 2
Training loss: 2.827336744187533
Validation loss: 2.416541917047128

Epoch: 5| Step: 3
Training loss: 3.2906993136659306
Validation loss: 2.435508696084513

Epoch: 5| Step: 4
Training loss: 3.0040506990423768
Validation loss: 2.440403823312686

Epoch: 5| Step: 5
Training loss: 2.6618286790868195
Validation loss: 2.440560854134666

Epoch: 5| Step: 6
Training loss: 2.5623081414575775
Validation loss: 2.4299831257354976

Epoch: 5| Step: 7
Training loss: 2.741526466966823
Validation loss: 2.422765954692583

Epoch: 5| Step: 8
Training loss: 2.2130416724293043
Validation loss: 2.4166799059778548

Epoch: 5| Step: 9
Training loss: 2.565435263602619
Validation loss: 2.42680184203117

Epoch: 5| Step: 10
Training loss: 2.88054073317256
Validation loss: 2.426940363092226

Epoch: 121| Step: 0
Training loss: 2.921016628229805
Validation loss: 2.412255317172505

Epoch: 5| Step: 1
Training loss: 2.4474179889471417
Validation loss: 2.403959846767582

Epoch: 5| Step: 2
Training loss: 2.8108366392063213
Validation loss: 2.4180148511580297

Epoch: 5| Step: 3
Training loss: 2.8820768415278577
Validation loss: 2.43127829846318

Epoch: 5| Step: 4
Training loss: 2.881951759257602
Validation loss: 2.4327841881907046

Epoch: 5| Step: 5
Training loss: 2.642366185115889
Validation loss: 2.422017243938771

Epoch: 5| Step: 6
Training loss: 2.4237099863874008
Validation loss: 2.433650405354142

Epoch: 5| Step: 7
Training loss: 2.5280219322291377
Validation loss: 2.432627210752313

Epoch: 5| Step: 8
Training loss: 2.5769081712022333
Validation loss: 2.434719145394457

Epoch: 5| Step: 9
Training loss: 2.6067628863528234
Validation loss: 2.4432118302272983

Epoch: 5| Step: 10
Training loss: 2.2685185338003744
Validation loss: 2.430359519713532

Epoch: 122| Step: 0
Training loss: 2.8393826326758447
Validation loss: 2.418345866901829

Epoch: 5| Step: 1
Training loss: 2.59228271898136
Validation loss: 2.4258509385436557

Epoch: 5| Step: 2
Training loss: 2.6319424694347995
Validation loss: 2.4329649389159727

Epoch: 5| Step: 3
Training loss: 2.3602861072585406
Validation loss: 2.4332337919865727

Epoch: 5| Step: 4
Training loss: 3.016075614507494
Validation loss: 2.4229721489883667

Epoch: 5| Step: 5
Training loss: 2.488516660181175
Validation loss: 2.426786443036715

Epoch: 5| Step: 6
Training loss: 2.1926204153332343
Validation loss: 2.4454438293095895

Epoch: 5| Step: 7
Training loss: 2.7159083632290093
Validation loss: 2.434177877222392

Epoch: 5| Step: 8
Training loss: 3.2754768082728356
Validation loss: 2.4211214426110583

Epoch: 5| Step: 9
Training loss: 2.4068718639225564
Validation loss: 2.4328287388102354

Epoch: 5| Step: 10
Training loss: 2.420850475554799
Validation loss: 2.4183613042708623

Epoch: 123| Step: 0
Training loss: 3.085649153360089
Validation loss: 2.426704381027417

Epoch: 5| Step: 1
Training loss: 2.779997897387471
Validation loss: 2.4374070466657454

Epoch: 5| Step: 2
Training loss: 2.7325661043790004
Validation loss: 2.4305572098014894

Epoch: 5| Step: 3
Training loss: 2.1886599054099456
Validation loss: 2.4380902274367275

Epoch: 5| Step: 4
Training loss: 2.4974856130130223
Validation loss: 2.435329973202804

Epoch: 5| Step: 5
Training loss: 2.852313384388404
Validation loss: 2.422511694209708

Epoch: 5| Step: 6
Training loss: 2.4653268573394658
Validation loss: 2.4235002677836412

Epoch: 5| Step: 7
Training loss: 2.5775891296190654
Validation loss: 2.421330913038121

Epoch: 5| Step: 8
Training loss: 2.2696244291260808
Validation loss: 2.416007079394517

Epoch: 5| Step: 9
Training loss: 2.719217720577346
Validation loss: 2.42096612967177

Epoch: 5| Step: 10
Training loss: 2.858138854266201
Validation loss: 2.4345898669220554

Epoch: 124| Step: 0
Training loss: 2.9918622270880832
Validation loss: 2.4336628809004077

Epoch: 5| Step: 1
Training loss: 2.8231638182005967
Validation loss: 2.45076423304454

Epoch: 5| Step: 2
Training loss: 2.563006281267165
Validation loss: 2.4468127782846913

Epoch: 5| Step: 3
Training loss: 2.9656527776191033
Validation loss: 2.414600231052035

Epoch: 5| Step: 4
Training loss: 2.1812648739895133
Validation loss: 2.439594023513592

Epoch: 5| Step: 5
Training loss: 2.7122003873256713
Validation loss: 2.4018701582252056

Epoch: 5| Step: 6
Training loss: 2.6227565442800755
Validation loss: 2.4234978665183298

Epoch: 5| Step: 7
Training loss: 2.6508458532976897
Validation loss: 2.4307163775592184

Epoch: 5| Step: 8
Training loss: 2.1680988444567553
Validation loss: 2.422264359449395

Epoch: 5| Step: 9
Training loss: 2.6434012721864835
Validation loss: 2.4161681950747584

Epoch: 5| Step: 10
Training loss: 2.5881821953967235
Validation loss: 2.4173823344678236

Epoch: 125| Step: 0
Training loss: 2.365568003604059
Validation loss: 2.427011815111999

Epoch: 5| Step: 1
Training loss: 2.475764389678853
Validation loss: 2.4255721417050635

Epoch: 5| Step: 2
Training loss: 3.2403209046990527
Validation loss: 2.427318710693917

Epoch: 5| Step: 3
Training loss: 1.9910761107952384
Validation loss: 2.424288090752822

Epoch: 5| Step: 4
Training loss: 2.419068513149943
Validation loss: 2.4253274571978642

Epoch: 5| Step: 5
Training loss: 2.7492315779330054
Validation loss: 2.4240388834054896

Epoch: 5| Step: 6
Training loss: 2.999211684603963
Validation loss: 2.4295771043235637

Epoch: 5| Step: 7
Training loss: 2.8830761465865935
Validation loss: 2.415525156904989

Epoch: 5| Step: 8
Training loss: 2.4171029606875867
Validation loss: 2.4222144506593906

Epoch: 5| Step: 9
Training loss: 2.7332430867284163
Validation loss: 2.4412459026224345

Epoch: 5| Step: 10
Training loss: 2.4690840289933886
Validation loss: 2.4230106196053534

Epoch: 126| Step: 0
Training loss: 2.6044819348551482
Validation loss: 2.42420032663118

Epoch: 5| Step: 1
Training loss: 2.73567718016842
Validation loss: 2.435665599382072

Epoch: 5| Step: 2
Training loss: 2.544669009196191
Validation loss: 2.4320400046623547

Epoch: 5| Step: 3
Training loss: 2.1401808897556402
Validation loss: 2.4245840851151983

Epoch: 5| Step: 4
Training loss: 3.0822993598232626
Validation loss: 2.4302181522632775

Epoch: 5| Step: 5
Training loss: 2.443283165247089
Validation loss: 2.42367254019648

Epoch: 5| Step: 6
Training loss: 2.923148301060046
Validation loss: 2.428790943112096

Epoch: 5| Step: 7
Training loss: 2.4144386566326026
Validation loss: 2.435236394883656

Epoch: 5| Step: 8
Training loss: 2.269986289230818
Validation loss: 2.4353290931561644

Epoch: 5| Step: 9
Training loss: 2.672430594161265
Validation loss: 2.433499712494901

Epoch: 5| Step: 10
Training loss: 2.9310169509610975
Validation loss: 2.422084958822888

Epoch: 127| Step: 0
Training loss: 2.2252094641682225
Validation loss: 2.4461208290324783

Epoch: 5| Step: 1
Training loss: 2.7380434550646764
Validation loss: 2.4409521093489857

Epoch: 5| Step: 2
Training loss: 2.628591169999904
Validation loss: 2.421522137952768

Epoch: 5| Step: 3
Training loss: 2.783835591515048
Validation loss: 2.4409102297697434

Epoch: 5| Step: 4
Training loss: 2.6038579121980723
Validation loss: 2.4276368796245653

Epoch: 5| Step: 5
Training loss: 2.4009679312703294
Validation loss: 2.434034983676979

Epoch: 5| Step: 6
Training loss: 2.6112940692606808
Validation loss: 2.4253337517953746

Epoch: 5| Step: 7
Training loss: 2.6442854751583273
Validation loss: 2.425136321971934

Epoch: 5| Step: 8
Training loss: 3.0601769803795853
Validation loss: 2.424409542672562

Epoch: 5| Step: 9
Training loss: 2.730650626961842
Validation loss: 2.4214928004237475

Epoch: 5| Step: 10
Training loss: 2.3715789650388093
Validation loss: 2.43787888557877

Epoch: 128| Step: 0
Training loss: 2.918985661993199
Validation loss: 2.452159446244366

Epoch: 5| Step: 1
Training loss: 3.2595283508799904
Validation loss: 2.4280375397447296

Epoch: 5| Step: 2
Training loss: 2.7060573306667894
Validation loss: 2.4482618316780234

Epoch: 5| Step: 3
Training loss: 2.3615056200737987
Validation loss: 2.4519662413531957

Epoch: 5| Step: 4
Training loss: 3.0202768449015784
Validation loss: 2.43694939506539

Epoch: 5| Step: 5
Training loss: 2.646848849152749
Validation loss: 2.4485680992118146

Epoch: 5| Step: 6
Training loss: 2.4973197875048427
Validation loss: 2.4372560280662974

Epoch: 5| Step: 7
Training loss: 2.5864331096065003
Validation loss: 2.436030275032231

Epoch: 5| Step: 8
Training loss: 2.2229198976568916
Validation loss: 2.433121177034548

Epoch: 5| Step: 9
Training loss: 2.215571638790989
Validation loss: 2.4259580714485867

Epoch: 5| Step: 10
Training loss: 2.2905959893818606
Validation loss: 2.4151893601032692

Epoch: 129| Step: 0
Training loss: 2.827747551615548
Validation loss: 2.4315651043855824

Epoch: 5| Step: 1
Training loss: 2.403772936953068
Validation loss: 2.4238898738342423

Epoch: 5| Step: 2
Training loss: 2.284558027165448
Validation loss: 2.4271827377066675

Epoch: 5| Step: 3
Training loss: 2.2460438598399426
Validation loss: 2.41330235807716

Epoch: 5| Step: 4
Training loss: 2.418359580060765
Validation loss: 2.4219825564611788

Epoch: 5| Step: 5
Training loss: 2.915855367540383
Validation loss: 2.441628740702021

Epoch: 5| Step: 6
Training loss: 2.5649109640154015
Validation loss: 2.407558811825472

Epoch: 5| Step: 7
Training loss: 2.6764657363280473
Validation loss: 2.430698108233195

Epoch: 5| Step: 8
Training loss: 2.518758397999434
Validation loss: 2.4292270422055404

Epoch: 5| Step: 9
Training loss: 2.5587448890832527
Validation loss: 2.422470668402356

Epoch: 5| Step: 10
Training loss: 3.4233366052549403
Validation loss: 2.4028985968215095

Epoch: 130| Step: 0
Training loss: 2.586495975912324
Validation loss: 2.4267997535542167

Epoch: 5| Step: 1
Training loss: 2.561899068197716
Validation loss: 2.4268181050464097

Epoch: 5| Step: 2
Training loss: 2.4974359715285526
Validation loss: 2.4280012731777023

Epoch: 5| Step: 3
Training loss: 2.5660924153503615
Validation loss: 2.4328497224310732

Epoch: 5| Step: 4
Training loss: 2.9417061418544876
Validation loss: 2.428194918417509

Epoch: 5| Step: 5
Training loss: 2.764900500557772
Validation loss: 2.4356179092842374

Epoch: 5| Step: 6
Training loss: 2.666190631418306
Validation loss: 2.4275819309524698

Epoch: 5| Step: 7
Training loss: 2.3865962769696942
Validation loss: 2.4384547836010095

Epoch: 5| Step: 8
Training loss: 2.618202673041486
Validation loss: 2.4302215205612505

Epoch: 5| Step: 9
Training loss: 2.7208697068809653
Validation loss: 2.4112042619437224

Epoch: 5| Step: 10
Training loss: 2.4309860635387
Validation loss: 2.4296122763765986

Epoch: 131| Step: 0
Training loss: 2.454426899850098
Validation loss: 2.430504406118823

Epoch: 5| Step: 1
Training loss: 2.852694686410895
Validation loss: 2.426938915396769

Epoch: 5| Step: 2
Training loss: 2.4673893716085242
Validation loss: 2.4282633700315746

Epoch: 5| Step: 3
Training loss: 2.2510084435445674
Validation loss: 2.438936562722004

Epoch: 5| Step: 4
Training loss: 2.7236688774951774
Validation loss: 2.425031744915482

Epoch: 5| Step: 5
Training loss: 2.4337058248169696
Validation loss: 2.413656881713561

Epoch: 5| Step: 6
Training loss: 2.580556936255684
Validation loss: 2.417434408834423

Epoch: 5| Step: 7
Training loss: 2.092018094372952
Validation loss: 2.420602216979409

Epoch: 5| Step: 8
Training loss: 2.8941836334999453
Validation loss: 2.430969619600896

Epoch: 5| Step: 9
Training loss: 2.6146119155436027
Validation loss: 2.4263555210165335

Epoch: 5| Step: 10
Training loss: 3.4299538754400887
Validation loss: 2.4330345892349534

Epoch: 132| Step: 0
Training loss: 2.784753714172259
Validation loss: 2.4338091132783433

Epoch: 5| Step: 1
Training loss: 2.693497906636086
Validation loss: 2.4124111749805315

Epoch: 5| Step: 2
Training loss: 2.805338409463958
Validation loss: 2.4328288062514805

Epoch: 5| Step: 3
Training loss: 2.147694518191365
Validation loss: 2.4536053837915555

Epoch: 5| Step: 4
Training loss: 2.2622181500812713
Validation loss: 2.4264420185743623

Epoch: 5| Step: 5
Training loss: 2.259296180542692
Validation loss: 2.4050105871170877

Epoch: 5| Step: 6
Training loss: 2.6469208192068447
Validation loss: 2.4356032280950344

Epoch: 5| Step: 7
Training loss: 3.0608151530670904
Validation loss: 2.4220476335756347

Epoch: 5| Step: 8
Training loss: 2.596938964937348
Validation loss: 2.4432616794443542

Epoch: 5| Step: 9
Training loss: 2.9435263967935814
Validation loss: 2.4306270599004227

Epoch: 5| Step: 10
Training loss: 2.219683491078768
Validation loss: 2.4410349305999315

Epoch: 133| Step: 0
Training loss: 2.8528909175649204
Validation loss: 2.422562420183066

Epoch: 5| Step: 1
Training loss: 2.894752977864682
Validation loss: 2.420171127057145

Epoch: 5| Step: 2
Training loss: 2.191785918884687
Validation loss: 2.443266073782225

Epoch: 5| Step: 3
Training loss: 2.6422520427163585
Validation loss: 2.4165672865968624

Epoch: 5| Step: 4
Training loss: 2.8235352086026393
Validation loss: 2.4490491381765973

Epoch: 5| Step: 5
Training loss: 1.6702993222922067
Validation loss: 2.429938880597065

Epoch: 5| Step: 6
Training loss: 2.3858345254337032
Validation loss: 2.4310255979250717

Epoch: 5| Step: 7
Training loss: 2.3577681368527745
Validation loss: 2.439708774383802

Epoch: 5| Step: 8
Training loss: 2.4758810074396727
Validation loss: 2.434804485599887

Epoch: 5| Step: 9
Training loss: 3.1780439248335424
Validation loss: 2.4295155728458533

Epoch: 5| Step: 10
Training loss: 3.0357084226150954
Validation loss: 2.4233481847137557

Epoch: 134| Step: 0
Training loss: 2.4673854098626795
Validation loss: 2.428093931976752

Epoch: 5| Step: 1
Training loss: 2.8112415465231635
Validation loss: 2.416324446623225

Epoch: 5| Step: 2
Training loss: 2.8487545035821684
Validation loss: 2.4263172460754947

Epoch: 5| Step: 3
Training loss: 2.97943809357647
Validation loss: 2.4332573533961463

Epoch: 5| Step: 4
Training loss: 2.733127767307181
Validation loss: 2.4308810600228457

Epoch: 5| Step: 5
Training loss: 2.7707127901019217
Validation loss: 2.4274632304788106

Epoch: 5| Step: 6
Training loss: 2.0864763456859934
Validation loss: 2.435302983206702

Epoch: 5| Step: 7
Training loss: 2.7626102201127622
Validation loss: 2.445331856343911

Epoch: 5| Step: 8
Training loss: 2.536089286092844
Validation loss: 2.431090679727491

Epoch: 5| Step: 9
Training loss: 2.666647006995529
Validation loss: 2.4094391905890156

Epoch: 5| Step: 10
Training loss: 1.843736422213975
Validation loss: 2.4143141844206717

Epoch: 135| Step: 0
Training loss: 2.8214508886989487
Validation loss: 2.424604678029789

Epoch: 5| Step: 1
Training loss: 2.91355273822367
Validation loss: 2.4134778949896214

Epoch: 5| Step: 2
Training loss: 2.3692804029574326
Validation loss: 2.4252607577721736

Epoch: 5| Step: 3
Training loss: 2.4699519647722847
Validation loss: 2.4367571535739447

Epoch: 5| Step: 4
Training loss: 2.4784111560060675
Validation loss: 2.435288039048059

Epoch: 5| Step: 5
Training loss: 3.2142634557528544
Validation loss: 2.4197509247586506

Epoch: 5| Step: 6
Training loss: 2.3020755773445303
Validation loss: 2.4359683458866526

Epoch: 5| Step: 7
Training loss: 2.722385650279037
Validation loss: 2.4389916465943937

Epoch: 5| Step: 8
Training loss: 2.118811065784067
Validation loss: 2.435758519185692

Epoch: 5| Step: 9
Training loss: 2.7534419973819637
Validation loss: 2.425479111229189

Epoch: 5| Step: 10
Training loss: 2.3752786573265823
Validation loss: 2.425091238370127

Epoch: 136| Step: 0
Training loss: 2.1840916920581943
Validation loss: 2.4342502894882485

Epoch: 5| Step: 1
Training loss: 2.3624694943350737
Validation loss: 2.4288397773707984

Epoch: 5| Step: 2
Training loss: 2.626501017525456
Validation loss: 2.429325959610619

Epoch: 5| Step: 3
Training loss: 2.509023213698399
Validation loss: 2.433723538553609

Epoch: 5| Step: 4
Training loss: 2.918817072553082
Validation loss: 2.4389680648999725

Epoch: 5| Step: 5
Training loss: 3.5614843426528533
Validation loss: 2.4371168909887486

Epoch: 5| Step: 6
Training loss: 2.4984295680890547
Validation loss: 2.416175126810836

Epoch: 5| Step: 7
Training loss: 2.485409886962989
Validation loss: 2.430239709078656

Epoch: 5| Step: 8
Training loss: 2.0633050907813035
Validation loss: 2.419976424486553

Epoch: 5| Step: 9
Training loss: 2.473308266343884
Validation loss: 2.447243671331697

Epoch: 5| Step: 10
Training loss: 2.689780398909984
Validation loss: 2.4326390571273304

Epoch: 137| Step: 0
Training loss: 2.7258549950054913
Validation loss: 2.4393600138312785

Epoch: 5| Step: 1
Training loss: 2.8620744480497358
Validation loss: 2.43523971623706

Epoch: 5| Step: 2
Training loss: 2.358962976719009
Validation loss: 2.442338867568896

Epoch: 5| Step: 3
Training loss: 2.284005682402054
Validation loss: 2.435252256279184

Epoch: 5| Step: 4
Training loss: 2.7789074751642384
Validation loss: 2.4348657907673954

Epoch: 5| Step: 5
Training loss: 2.402090664069665
Validation loss: 2.428803833104318

Epoch: 5| Step: 6
Training loss: 2.7493544601155335
Validation loss: 2.4517596329894515

Epoch: 5| Step: 7
Training loss: 2.9652176572217606
Validation loss: 2.4252738441115866

Epoch: 5| Step: 8
Training loss: 2.646857136167305
Validation loss: 2.440008241786449

Epoch: 5| Step: 9
Training loss: 2.3828938454672857
Validation loss: 2.433014335307769

Epoch: 5| Step: 10
Training loss: 2.202276945910863
Validation loss: 2.443373950829428

Epoch: 138| Step: 0
Training loss: 2.835140063611999
Validation loss: 2.4299181239921626

Epoch: 5| Step: 1
Training loss: 2.5781766019338703
Validation loss: 2.4289203898534226

Epoch: 5| Step: 2
Training loss: 2.8792851896130403
Validation loss: 2.432110319188365

Epoch: 5| Step: 3
Training loss: 3.1909708312018545
Validation loss: 2.4267465591128476

Epoch: 5| Step: 4
Training loss: 2.3130556418475354
Validation loss: 2.4331511340225216

Epoch: 5| Step: 5
Training loss: 2.6778702426667462
Validation loss: 2.4333177377513167

Epoch: 5| Step: 6
Training loss: 2.4452191904808553
Validation loss: 2.433375542614966

Epoch: 5| Step: 7
Training loss: 2.2703341247410846
Validation loss: 2.4427274409413804

Epoch: 5| Step: 8
Training loss: 2.3442673175827498
Validation loss: 2.4464896883711695

Epoch: 5| Step: 9
Training loss: 2.3863804853761033
Validation loss: 2.4317487742456074

Epoch: 5| Step: 10
Training loss: 2.6145748021770574
Validation loss: 2.4405463549933137

Epoch: 139| Step: 0
Training loss: 2.3358103229515725
Validation loss: 2.443535521411223

Epoch: 5| Step: 1
Training loss: 2.357446147348118
Validation loss: 2.4517248006174897

Epoch: 5| Step: 2
Training loss: 2.523841660201575
Validation loss: 2.436285099517994

Epoch: 5| Step: 3
Training loss: 3.0323114797892097
Validation loss: 2.439483719890729

Epoch: 5| Step: 4
Training loss: 2.1974246904229884
Validation loss: 2.431120323238174

Epoch: 5| Step: 5
Training loss: 2.708273686461324
Validation loss: 2.4307645234626554

Epoch: 5| Step: 6
Training loss: 2.549706318717225
Validation loss: 2.4478253586879775

Epoch: 5| Step: 7
Training loss: 2.5016979173728386
Validation loss: 2.4441116578384103

Epoch: 5| Step: 8
Training loss: 2.343638608192775
Validation loss: 2.4399105696559547

Epoch: 5| Step: 9
Training loss: 3.3610611343521466
Validation loss: 2.439447339744982

Epoch: 5| Step: 10
Training loss: 2.547111734258648
Validation loss: 2.4402512115685253

Epoch: 140| Step: 0
Training loss: 2.3847767222753165
Validation loss: 2.4473479193860626

Epoch: 5| Step: 1
Training loss: 2.384244994772743
Validation loss: 2.4233311748087383

Epoch: 5| Step: 2
Training loss: 2.6256456943378375
Validation loss: 2.4188794386447463

Epoch: 5| Step: 3
Training loss: 2.4869346627985243
Validation loss: 2.440002257169382

Epoch: 5| Step: 4
Training loss: 2.5756033847848805
Validation loss: 2.455399115909119

Epoch: 5| Step: 5
Training loss: 2.937748026015075
Validation loss: 2.4399026577900345

Epoch: 5| Step: 6
Training loss: 2.5572948157970257
Validation loss: 2.441215489439213

Epoch: 5| Step: 7
Training loss: 2.912876736747349
Validation loss: 2.4316753750178837

Epoch: 5| Step: 8
Training loss: 3.1093496963774907
Validation loss: 2.438160737595031

Epoch: 5| Step: 9
Training loss: 2.264438818491065
Validation loss: 2.4337706880031305

Epoch: 5| Step: 10
Training loss: 2.078197047770098
Validation loss: 2.457337075039313

Epoch: 141| Step: 0
Training loss: 2.243878514426773
Validation loss: 2.434021456785052

Epoch: 5| Step: 1
Training loss: 3.3044172316252105
Validation loss: 2.4326053399646153

Epoch: 5| Step: 2
Training loss: 2.613576373440874
Validation loss: 2.422385109574471

Epoch: 5| Step: 3
Training loss: 2.313575494534097
Validation loss: 2.41061674961269

Epoch: 5| Step: 4
Training loss: 3.1523780868153946
Validation loss: 2.4350858764992713

Epoch: 5| Step: 5
Training loss: 2.1881093947489627
Validation loss: 2.427231408924473

Epoch: 5| Step: 6
Training loss: 1.977056388818764
Validation loss: 2.4298547220903743

Epoch: 5| Step: 7
Training loss: 2.4638442538506093
Validation loss: 2.4160948880693995

Epoch: 5| Step: 8
Training loss: 2.5727482996742146
Validation loss: 2.4519913609619213

Epoch: 5| Step: 9
Training loss: 2.9026981101493403
Validation loss: 2.4429692346528116

Epoch: 5| Step: 10
Training loss: 2.5136138269084207
Validation loss: 2.4395707271699956

Epoch: 142| Step: 0
Training loss: 2.5327109371204446
Validation loss: 2.4348663056295052

Epoch: 5| Step: 1
Training loss: 2.368178358526437
Validation loss: 2.4300688749047765

Epoch: 5| Step: 2
Training loss: 2.6356629468914834
Validation loss: 2.427998379051061

Epoch: 5| Step: 3
Training loss: 2.8807074244884587
Validation loss: 2.4263262429861725

Epoch: 5| Step: 4
Training loss: 2.84215005993628
Validation loss: 2.4315184978521307

Epoch: 5| Step: 5
Training loss: 2.7356465026414782
Validation loss: 2.4353180441224933

Epoch: 5| Step: 6
Training loss: 2.163711905053177
Validation loss: 2.4520804634202555

Epoch: 5| Step: 7
Training loss: 2.794846806294452
Validation loss: 2.4439570522365743

Epoch: 5| Step: 8
Training loss: 2.4157201513818776
Validation loss: 2.4356942546727476

Epoch: 5| Step: 9
Training loss: 2.35846293422419
Validation loss: 2.4726211821930253

Epoch: 5| Step: 10
Training loss: 2.6422334546334434
Validation loss: 2.441815748167425

Epoch: 143| Step: 0
Training loss: 2.3843491899085847
Validation loss: 2.4284666558121324

Epoch: 5| Step: 1
Training loss: 2.38873904766165
Validation loss: 2.4307792465321016

Epoch: 5| Step: 2
Training loss: 2.9010770967923625
Validation loss: 2.4457653268123987

Epoch: 5| Step: 3
Training loss: 2.1405943708247235
Validation loss: 2.417427184834451

Epoch: 5| Step: 4
Training loss: 3.0778298793228074
Validation loss: 2.424786073369755

Epoch: 5| Step: 5
Training loss: 2.188505432129137
Validation loss: 2.449676396075082

Epoch: 5| Step: 6
Training loss: 2.468589874096485
Validation loss: 2.455884437105179

Epoch: 5| Step: 7
Training loss: 2.8604839238323674
Validation loss: 2.438363185092244

Epoch: 5| Step: 8
Training loss: 2.7371367540129334
Validation loss: 2.4297707294840536

Epoch: 5| Step: 9
Training loss: 2.5603558944140543
Validation loss: 2.4353387947053733

Epoch: 5| Step: 10
Training loss: 2.454579013537252
Validation loss: 2.4253657276756337

Epoch: 144| Step: 0
Training loss: 3.0921125606267066
Validation loss: 2.454505698544758

Epoch: 5| Step: 1
Training loss: 1.9068741323843563
Validation loss: 2.4549234837413274

Epoch: 5| Step: 2
Training loss: 1.7925816868214437
Validation loss: 2.4345594327180136

Epoch: 5| Step: 3
Training loss: 2.60962406271828
Validation loss: 2.440865277346577

Epoch: 5| Step: 4
Training loss: 2.6289691889173703
Validation loss: 2.4291206856454766

Epoch: 5| Step: 5
Training loss: 2.470227342714598
Validation loss: 2.425919898027384

Epoch: 5| Step: 6
Training loss: 2.741419062308645
Validation loss: 2.4482158026915437

Epoch: 5| Step: 7
Training loss: 2.5080267793847097
Validation loss: 2.4621959608729744

Epoch: 5| Step: 8
Training loss: 2.8138912574563117
Validation loss: 2.4304050061018962

Epoch: 5| Step: 9
Training loss: 3.0122144318401753
Validation loss: 2.433399340748105

Epoch: 5| Step: 10
Training loss: 2.3941297583474226
Validation loss: 2.4402426310700895

Epoch: 145| Step: 0
Training loss: 2.674090888722042
Validation loss: 2.444383473240304

Epoch: 5| Step: 1
Training loss: 2.507664280964781
Validation loss: 2.425672994279404

Epoch: 5| Step: 2
Training loss: 2.4696987598524043
Validation loss: 2.437940444677923

Epoch: 5| Step: 3
Training loss: 2.6562083521551156
Validation loss: 2.4339259743593202

Epoch: 5| Step: 4
Training loss: 2.633868268964501
Validation loss: 2.4118974710579826

Epoch: 5| Step: 5
Training loss: 2.7303418742596492
Validation loss: 2.4228644068490586

Epoch: 5| Step: 6
Training loss: 2.771140939565073
Validation loss: 2.4378173364928872

Epoch: 5| Step: 7
Training loss: 1.9102950406505232
Validation loss: 2.4501456179527383

Epoch: 5| Step: 8
Training loss: 2.5183091149300645
Validation loss: 2.441074043888343

Epoch: 5| Step: 9
Training loss: 2.5578725948742806
Validation loss: 2.443507840423141

Epoch: 5| Step: 10
Training loss: 3.0080648737465703
Validation loss: 2.4427667013436

Epoch: 146| Step: 0
Training loss: 2.1973203118194613
Validation loss: 2.4267282361311167

Epoch: 5| Step: 1
Training loss: 2.7376055135380457
Validation loss: 2.4239963039068604

Epoch: 5| Step: 2
Training loss: 2.799934631674531
Validation loss: 2.4344811751996196

Epoch: 5| Step: 3
Training loss: 2.6712961741104064
Validation loss: 2.440795429301091

Epoch: 5| Step: 4
Training loss: 2.979267963385679
Validation loss: 2.4559570126069126

Epoch: 5| Step: 5
Training loss: 2.271748233076206
Validation loss: 2.431910289656225

Epoch: 5| Step: 6
Training loss: 2.4369378052900337
Validation loss: 2.4270258078125657

Epoch: 5| Step: 7
Training loss: 2.6108965977239214
Validation loss: 2.438406259129767

Epoch: 5| Step: 8
Training loss: 2.836379003705452
Validation loss: 2.444190375308374

Epoch: 5| Step: 9
Training loss: 2.425236000296674
Validation loss: 2.4574383447757024

Epoch: 5| Step: 10
Training loss: 2.294242959749864
Validation loss: 2.441143635069188

Epoch: 147| Step: 0
Training loss: 2.5393725396645994
Validation loss: 2.436151659041364

Epoch: 5| Step: 1
Training loss: 2.7844084903573894
Validation loss: 2.434433984224202

Epoch: 5| Step: 2
Training loss: 2.994056854768286
Validation loss: 2.4531475500059643

Epoch: 5| Step: 3
Training loss: 2.1619898376226003
Validation loss: 2.4434823739309173

Epoch: 5| Step: 4
Training loss: 2.421163343652284
Validation loss: 2.4370192923295333

Epoch: 5| Step: 5
Training loss: 2.764486304743391
Validation loss: 2.4550050139220914

Epoch: 5| Step: 6
Training loss: 2.716343746612708
Validation loss: 2.4374319213750137

Epoch: 5| Step: 7
Training loss: 2.9837353405314664
Validation loss: 2.434030355704194

Epoch: 5| Step: 8
Training loss: 2.0622269420779866
Validation loss: 2.4381393854021502

Epoch: 5| Step: 9
Training loss: 2.56633211490384
Validation loss: 2.458651800646737

Epoch: 5| Step: 10
Training loss: 2.04953563827274
Validation loss: 2.4547660234346522

Epoch: 148| Step: 0
Training loss: 2.3325190712822708
Validation loss: 2.445227297974959

Epoch: 5| Step: 1
Training loss: 2.8139049835430994
Validation loss: 2.4356827010264124

Epoch: 5| Step: 2
Training loss: 1.933896374380973
Validation loss: 2.447847024367246

Epoch: 5| Step: 3
Training loss: 2.6428608654982813
Validation loss: 2.4422662778715805

Epoch: 5| Step: 4
Training loss: 2.75855951755213
Validation loss: 2.4356401940404067

Epoch: 5| Step: 5
Training loss: 2.600518461033168
Validation loss: 2.417995566642286

Epoch: 5| Step: 6
Training loss: 2.8435383812272863
Validation loss: 2.433915695237418

Epoch: 5| Step: 7
Training loss: 1.9417303110369268
Validation loss: 2.433899482807316

Epoch: 5| Step: 8
Training loss: 2.561088917825253
Validation loss: 2.4419957390643963

Epoch: 5| Step: 9
Training loss: 2.90553307663355
Validation loss: 2.437883126623619

Epoch: 5| Step: 10
Training loss: 2.9000881050143463
Validation loss: 2.4380545325871625

Epoch: 149| Step: 0
Training loss: 2.6227999050420197
Validation loss: 2.4491683018738066

Epoch: 5| Step: 1
Training loss: 2.721304559331548
Validation loss: 2.4375374630472604

Epoch: 5| Step: 2
Training loss: 2.9194152596665695
Validation loss: 2.420862699370125

Epoch: 5| Step: 3
Training loss: 2.4006471675295247
Validation loss: 2.4356842577247813

Epoch: 5| Step: 4
Training loss: 2.5655450174221612
Validation loss: 2.4442215391676783

Epoch: 5| Step: 5
Training loss: 2.5654989232484993
Validation loss: 2.4440827440800255

Epoch: 5| Step: 6
Training loss: 2.6501658153839243
Validation loss: 2.4406942331605253

Epoch: 5| Step: 7
Training loss: 2.0053949310543024
Validation loss: 2.4387799129668943

Epoch: 5| Step: 8
Training loss: 2.663736541585161
Validation loss: 2.4381678517967185

Epoch: 5| Step: 9
Training loss: 2.730077712800888
Validation loss: 2.4341003673343904

Epoch: 5| Step: 10
Training loss: 2.1324254617487926
Validation loss: 2.440360249295037

Epoch: 150| Step: 0
Training loss: 2.4683688871547806
Validation loss: 2.4610301375186605

Epoch: 5| Step: 1
Training loss: 2.5736498283616362
Validation loss: 2.447811388271006

Epoch: 5| Step: 2
Training loss: 2.4080913483728468
Validation loss: 2.432139661456815

Epoch: 5| Step: 3
Training loss: 2.5392989297974755
Validation loss: 2.418159150943697

Epoch: 5| Step: 4
Training loss: 2.754035935831639
Validation loss: 2.4496858241750865

Epoch: 5| Step: 5
Training loss: 2.8494400277387286
Validation loss: 2.4457625669116316

Epoch: 5| Step: 6
Training loss: 2.1891095370808045
Validation loss: 2.4272719312648854

Epoch: 5| Step: 7
Training loss: 2.551462173124703
Validation loss: 2.456275442205035

Epoch: 5| Step: 8
Training loss: 2.6507772277567665
Validation loss: 2.457720162987405

Epoch: 5| Step: 9
Training loss: 2.5770755365778526
Validation loss: 2.4493826155024263

Epoch: 5| Step: 10
Training loss: 2.706546800649574
Validation loss: 2.429218256552115

Epoch: 151| Step: 0
Training loss: 2.4182150473433404
Validation loss: 2.448866100070255

Epoch: 5| Step: 1
Training loss: 2.338546469725255
Validation loss: 2.432918663726781

Epoch: 5| Step: 2
Training loss: 2.744385710391067
Validation loss: 2.4279764861013513

Epoch: 5| Step: 3
Training loss: 2.3475479888073467
Validation loss: 2.4360219282929343

Epoch: 5| Step: 4
Training loss: 2.7457670231763553
Validation loss: 2.4418006632854246

Epoch: 5| Step: 5
Training loss: 2.201862054375851
Validation loss: 2.4360329194100845

Epoch: 5| Step: 6
Training loss: 3.033603973332149
Validation loss: 2.432613103784793

Epoch: 5| Step: 7
Training loss: 2.9244808877524897
Validation loss: 2.437415272702346

Epoch: 5| Step: 8
Training loss: 2.4799265832186035
Validation loss: 2.4246711618367915

Epoch: 5| Step: 9
Training loss: 2.4181230585164912
Validation loss: 2.4455481624733637

Epoch: 5| Step: 10
Training loss: 2.362155917506988
Validation loss: 2.4378898241574753

Epoch: 152| Step: 0
Training loss: 2.954096558621795
Validation loss: 2.4362839367544167

Epoch: 5| Step: 1
Training loss: 2.72785023007983
Validation loss: 2.451817294262747

Epoch: 5| Step: 2
Training loss: 2.591447474623576
Validation loss: 2.45269270950044

Epoch: 5| Step: 3
Training loss: 2.739844992631642
Validation loss: 2.441666104702575

Epoch: 5| Step: 4
Training loss: 2.224459434593844
Validation loss: 2.4428379213287945

Epoch: 5| Step: 5
Training loss: 1.9278866167641677
Validation loss: 2.4488287380974643

Epoch: 5| Step: 6
Training loss: 2.8444174517734933
Validation loss: 2.453878095998084

Epoch: 5| Step: 7
Training loss: 2.8206260274407216
Validation loss: 2.458353449921835

Epoch: 5| Step: 8
Training loss: 2.2749565791084043
Validation loss: 2.449090912076809

Epoch: 5| Step: 9
Training loss: 2.319626523737225
Validation loss: 2.470725338179502

Epoch: 5| Step: 10
Training loss: 2.5698050603889935
Validation loss: 2.4438298957830638

Epoch: 153| Step: 0
Training loss: 2.815508526119329
Validation loss: 2.4560512178917966

Epoch: 5| Step: 1
Training loss: 2.8548955798719953
Validation loss: 2.45392963043225

Epoch: 5| Step: 2
Training loss: 1.9572577126565787
Validation loss: 2.4414939216869294

Epoch: 5| Step: 3
Training loss: 1.796381675337783
Validation loss: 2.439737124743976

Epoch: 5| Step: 4
Training loss: 2.922891042374529
Validation loss: 2.443325432244193

Epoch: 5| Step: 5
Training loss: 2.702674254834752
Validation loss: 2.435224433762963

Epoch: 5| Step: 6
Training loss: 2.3673414929348793
Validation loss: 2.451639587206393

Epoch: 5| Step: 7
Training loss: 1.8052342234884793
Validation loss: 2.439714251135837

Epoch: 5| Step: 8
Training loss: 2.9418419746991105
Validation loss: 2.4297408468800357

Epoch: 5| Step: 9
Training loss: 3.207550713011111
Validation loss: 2.4313836812764387

Epoch: 5| Step: 10
Training loss: 2.492745937813527
Validation loss: 2.4583103463203777

Epoch: 154| Step: 0
Training loss: 2.333504114258946
Validation loss: 2.436415466524376

Epoch: 5| Step: 1
Training loss: 2.3025916963722937
Validation loss: 2.4406332802734148

Epoch: 5| Step: 2
Training loss: 2.4680333123331484
Validation loss: 2.447043982063578

Epoch: 5| Step: 3
Training loss: 2.634898369727517
Validation loss: 2.445499146096595

Epoch: 5| Step: 4
Training loss: 3.1418448589056145
Validation loss: 2.4482632096965355

Epoch: 5| Step: 5
Training loss: 2.66695481968741
Validation loss: 2.455390678659372

Epoch: 5| Step: 6
Training loss: 2.5665115032937815
Validation loss: 2.4562018887326444

Epoch: 5| Step: 7
Training loss: 2.728295942021946
Validation loss: 2.4459376493514955

Epoch: 5| Step: 8
Training loss: 2.539453846042814
Validation loss: 2.454457991184481

Epoch: 5| Step: 9
Training loss: 2.3170638542085933
Validation loss: 2.4396669774921076

Epoch: 5| Step: 10
Training loss: 2.1571021123988148
Validation loss: 2.4524307245641364

Epoch: 155| Step: 0
Training loss: 2.3907439757018603
Validation loss: 2.456631634795362

Epoch: 5| Step: 1
Training loss: 3.0842790613839095
Validation loss: 2.4358403224750145

Epoch: 5| Step: 2
Training loss: 2.4847292857398138
Validation loss: 2.4529363944930545

Epoch: 5| Step: 3
Training loss: 2.5171598881652097
Validation loss: 2.4390865134422

Epoch: 5| Step: 4
Training loss: 2.1764933929097703
Validation loss: 2.437928575691397

Epoch: 5| Step: 5
Training loss: 2.372283486233548
Validation loss: 2.4596599804175807

Epoch: 5| Step: 6
Training loss: 2.5336433681036956
Validation loss: 2.430532895484063

Epoch: 5| Step: 7
Training loss: 2.6740247321172728
Validation loss: 2.4230928130579596

Epoch: 5| Step: 8
Training loss: 2.610717245476353
Validation loss: 2.4316648523334785

Epoch: 5| Step: 9
Training loss: 2.230518988529557
Validation loss: 2.4425191647067668

Epoch: 5| Step: 10
Training loss: 2.8989898895582136
Validation loss: 2.4200435386266017

Epoch: 156| Step: 0
Training loss: 2.7332123818812533
Validation loss: 2.441109789628072

Epoch: 5| Step: 1
Training loss: 2.7335612802902016
Validation loss: 2.43123958750017

Epoch: 5| Step: 2
Training loss: 2.555802408219309
Validation loss: 2.434488464429905

Epoch: 5| Step: 3
Training loss: 2.6351567819992328
Validation loss: 2.453223921484305

Epoch: 5| Step: 4
Training loss: 2.100794995916585
Validation loss: 2.4514665998500895

Epoch: 5| Step: 5
Training loss: 2.2070243362723567
Validation loss: 2.428024679471722

Epoch: 5| Step: 6
Training loss: 2.5073530780502002
Validation loss: 2.4543094204131335

Epoch: 5| Step: 7
Training loss: 2.75352711333542
Validation loss: 2.4564560329806406

Epoch: 5| Step: 8
Training loss: 2.4210035171262647
Validation loss: 2.4484534735172674

Epoch: 5| Step: 9
Training loss: 2.693853099292693
Validation loss: 2.4539217862274945

Epoch: 5| Step: 10
Training loss: 2.5657522451091164
Validation loss: 2.448287444266158

Epoch: 157| Step: 0
Training loss: 2.6072660592488326
Validation loss: 2.445330438931736

Epoch: 5| Step: 1
Training loss: 2.1162019244443506
Validation loss: 2.4558362366676683

Epoch: 5| Step: 2
Training loss: 2.4532288936312323
Validation loss: 2.440738780146091

Epoch: 5| Step: 3
Training loss: 2.0722666604689333
Validation loss: 2.4355416236057237

Epoch: 5| Step: 4
Training loss: 2.7385102310978566
Validation loss: 2.451388191849668

Epoch: 5| Step: 5
Training loss: 2.7665736408756922
Validation loss: 2.451454069546556

Epoch: 5| Step: 6
Training loss: 2.7037347535837934
Validation loss: 2.4549366333472684

Epoch: 5| Step: 7
Training loss: 2.335776639266889
Validation loss: 2.472287162400087

Epoch: 5| Step: 8
Training loss: 2.360537009160977
Validation loss: 2.4429368264849365

Epoch: 5| Step: 9
Training loss: 2.4693457934777205
Validation loss: 2.448618405798574

Epoch: 5| Step: 10
Training loss: 3.2824181793311933
Validation loss: 2.44839184263383

Epoch: 158| Step: 0
Training loss: 2.256998304718828
Validation loss: 2.4431190763723887

Epoch: 5| Step: 1
Training loss: 2.712631619683673
Validation loss: 2.4427216907281806

Epoch: 5| Step: 2
Training loss: 2.0081462656908133
Validation loss: 2.4494005591645513

Epoch: 5| Step: 3
Training loss: 2.767975234224077
Validation loss: 2.4511423254705687

Epoch: 5| Step: 4
Training loss: 2.8073305730064058
Validation loss: 2.4464779677457646

Epoch: 5| Step: 5
Training loss: 2.6087717055554216
Validation loss: 2.4605836185759093

Epoch: 5| Step: 6
Training loss: 2.7002777804427334
Validation loss: 2.4390525679820225

Epoch: 5| Step: 7
Training loss: 2.281895794320524
Validation loss: 2.4525062528441897

Epoch: 5| Step: 8
Training loss: 2.7276614360079336
Validation loss: 2.46212162529543

Epoch: 5| Step: 9
Training loss: 2.4134174954332255
Validation loss: 2.4597928553497694

Epoch: 5| Step: 10
Training loss: 2.7593212809361294
Validation loss: 2.4546400483345225

Epoch: 159| Step: 0
Training loss: 2.5050586066176397
Validation loss: 2.4797291257669603

Epoch: 5| Step: 1
Training loss: 2.1654552839088796
Validation loss: 2.4466047455697826

Epoch: 5| Step: 2
Training loss: 2.0099235153829134
Validation loss: 2.4506822803810735

Epoch: 5| Step: 3
Training loss: 2.684341082286494
Validation loss: 2.452358720018796

Epoch: 5| Step: 4
Training loss: 2.59560217851232
Validation loss: 2.4550509594774708

Epoch: 5| Step: 5
Training loss: 2.6598005911290254
Validation loss: 2.45279998172864

Epoch: 5| Step: 6
Training loss: 2.458894590112729
Validation loss: 2.4638473618351084

Epoch: 5| Step: 7
Training loss: 3.327167498615417
Validation loss: 2.4541773893013197

Epoch: 5| Step: 8
Training loss: 2.308979963860826
Validation loss: 2.4602461774089557

Epoch: 5| Step: 9
Training loss: 2.800446856127454
Validation loss: 2.4477912469247225

Epoch: 5| Step: 10
Training loss: 2.239747743707835
Validation loss: 2.454001306821446

Epoch: 160| Step: 0
Training loss: 2.7030685507111634
Validation loss: 2.454875402284274

Epoch: 5| Step: 1
Training loss: 2.304383442105131
Validation loss: 2.4439350961211312

Epoch: 5| Step: 2
Training loss: 2.334456060143502
Validation loss: 2.441888233148338

Epoch: 5| Step: 3
Training loss: 2.8592277447247207
Validation loss: 2.4667038637309378

Epoch: 5| Step: 4
Training loss: 2.4034824060915447
Validation loss: 2.4490010324605267

Epoch: 5| Step: 5
Training loss: 2.831969793244814
Validation loss: 2.4738542111068287

Epoch: 5| Step: 6
Training loss: 2.850002415973911
Validation loss: 2.4681152019830224

Epoch: 5| Step: 7
Training loss: 1.9129732107509136
Validation loss: 2.4468507839625353

Epoch: 5| Step: 8
Training loss: 2.4281639891333078
Validation loss: 2.460440483629903

Epoch: 5| Step: 9
Training loss: 2.6418668997978605
Validation loss: 2.459423134565406

Epoch: 5| Step: 10
Training loss: 2.4586516890776426
Validation loss: 2.4494153722158774

Epoch: 161| Step: 0
Training loss: 2.7321374948343893
Validation loss: 2.4459471788565743

Epoch: 5| Step: 1
Training loss: 2.733226600361884
Validation loss: 2.4557874298117643

Epoch: 5| Step: 2
Training loss: 2.162325275396524
Validation loss: 2.4489802594596766

Epoch: 5| Step: 3
Training loss: 2.508607257152061
Validation loss: 2.4372174246715983

Epoch: 5| Step: 4
Training loss: 2.161605155891239
Validation loss: 2.4591632469003546

Epoch: 5| Step: 5
Training loss: 2.050367215459514
Validation loss: 2.44751111827354

Epoch: 5| Step: 6
Training loss: 2.3120654960911753
Validation loss: 2.45173131970001

Epoch: 5| Step: 7
Training loss: 2.731100683255313
Validation loss: 2.446292675902242

Epoch: 5| Step: 8
Training loss: 3.129117465658904
Validation loss: 2.4542777276191887

Epoch: 5| Step: 9
Training loss: 2.5714642109747463
Validation loss: 2.4552175076491074

Epoch: 5| Step: 10
Training loss: 2.6062732466797573
Validation loss: 2.4400144932580545

Epoch: 162| Step: 0
Training loss: 2.888966677506876
Validation loss: 2.4501982217368754

Epoch: 5| Step: 1
Training loss: 2.4460012422684816
Validation loss: 2.441835917542155

Epoch: 5| Step: 2
Training loss: 2.9756729518886247
Validation loss: 2.452836508214568

Epoch: 5| Step: 3
Training loss: 2.3208079210553176
Validation loss: 2.447885107196095

Epoch: 5| Step: 4
Training loss: 2.5798612354724066
Validation loss: 2.4456624316641977

Epoch: 5| Step: 5
Training loss: 2.1335839074716905
Validation loss: 2.4598626464947357

Epoch: 5| Step: 6
Training loss: 2.374961250390611
Validation loss: 2.4491777863599493

Epoch: 5| Step: 7
Training loss: 1.8824291551286516
Validation loss: 2.442819984032901

Epoch: 5| Step: 8
Training loss: 2.6230492382365562
Validation loss: 2.460404043460557

Epoch: 5| Step: 9
Training loss: 2.5612963198731618
Validation loss: 2.424672343914418

Epoch: 5| Step: 10
Training loss: 2.7747539136216113
Validation loss: 2.442088427458483

Epoch: 163| Step: 0
Training loss: 2.247539552503933
Validation loss: 2.443969336707777

Epoch: 5| Step: 1
Training loss: 2.7259240045008974
Validation loss: 2.4480172807956992

Epoch: 5| Step: 2
Training loss: 2.3647429178528903
Validation loss: 2.4453729944872276

Epoch: 5| Step: 3
Training loss: 2.6286825053664766
Validation loss: 2.4568626251649075

Epoch: 5| Step: 4
Training loss: 2.9319798974028477
Validation loss: 2.4452872242401478

Epoch: 5| Step: 5
Training loss: 2.9659092211216733
Validation loss: 2.4531472260431233

Epoch: 5| Step: 6
Training loss: 2.944441527439168
Validation loss: 2.4792321447169012

Epoch: 5| Step: 7
Training loss: 2.3542535028787044
Validation loss: 2.4670262090354718

Epoch: 5| Step: 8
Training loss: 2.0896459182793956
Validation loss: 2.460012579504825

Epoch: 5| Step: 9
Training loss: 1.9084736096439152
Validation loss: 2.4658594287526077

Epoch: 5| Step: 10
Training loss: 2.4583235163950627
Validation loss: 2.4686015987796104

Epoch: 164| Step: 0
Training loss: 2.3615132930526053
Validation loss: 2.4750289353475035

Epoch: 5| Step: 1
Training loss: 2.641143567332476
Validation loss: 2.4559115662678845

Epoch: 5| Step: 2
Training loss: 2.367675428809393
Validation loss: 2.456273053150384

Epoch: 5| Step: 3
Training loss: 2.325657400896356
Validation loss: 2.4389526554770358

Epoch: 5| Step: 4
Training loss: 2.1675525712586357
Validation loss: 2.4539466758335595

Epoch: 5| Step: 5
Training loss: 2.6780207040569013
Validation loss: 2.464328115252572

Epoch: 5| Step: 6
Training loss: 2.595791851791971
Validation loss: 2.421662171030093

Epoch: 5| Step: 7
Training loss: 2.645547285662048
Validation loss: 2.4627728039753056

Epoch: 5| Step: 8
Training loss: 2.7984782477414893
Validation loss: 2.4620543899234186

Epoch: 5| Step: 9
Training loss: 2.062987645617506
Validation loss: 2.4544307864189965

Epoch: 5| Step: 10
Training loss: 3.159327149524045
Validation loss: 2.4622918067557693

Epoch: 165| Step: 0
Training loss: 2.0670950594277695
Validation loss: 2.4661241447623876

Epoch: 5| Step: 1
Training loss: 2.1249183190298933
Validation loss: 2.470125244450675

Epoch: 5| Step: 2
Training loss: 2.359727138488969
Validation loss: 2.453353940648755

Epoch: 5| Step: 3
Training loss: 3.246235354349724
Validation loss: 2.4704275920278493

Epoch: 5| Step: 4
Training loss: 2.297236926569755
Validation loss: 2.446529228956389

Epoch: 5| Step: 5
Training loss: 2.4577094326246955
Validation loss: 2.432627584871435

Epoch: 5| Step: 6
Training loss: 2.9831195207693764
Validation loss: 2.4566066424680555

Epoch: 5| Step: 7
Training loss: 2.0391225988658865
Validation loss: 2.479893506919753

Epoch: 5| Step: 8
Training loss: 2.869745677618737
Validation loss: 2.4595185824502064

Epoch: 5| Step: 9
Training loss: 2.05538209263446
Validation loss: 2.481125673317281

Epoch: 5| Step: 10
Training loss: 2.751914311738356
Validation loss: 2.4465040286397763

Epoch: 166| Step: 0
Training loss: 2.1797570658421703
Validation loss: 2.445981153357244

Epoch: 5| Step: 1
Training loss: 2.1241254689933275
Validation loss: 2.452908421393951

Epoch: 5| Step: 2
Training loss: 2.386597475757381
Validation loss: 2.4730669396030764

Epoch: 5| Step: 3
Training loss: 3.1092826791217494
Validation loss: 2.447365147851533

Epoch: 5| Step: 4
Training loss: 2.156121844477636
Validation loss: 2.4409516965957607

Epoch: 5| Step: 5
Training loss: 2.268329663391657
Validation loss: 2.4709244769294907

Epoch: 5| Step: 6
Training loss: 2.7696830159129173
Validation loss: 2.471496291292239

Epoch: 5| Step: 7
Training loss: 3.023402487382057
Validation loss: 2.458055161021855

Epoch: 5| Step: 8
Training loss: 2.48309571539593
Validation loss: 2.452180173495405

Epoch: 5| Step: 9
Training loss: 2.676930513485462
Validation loss: 2.4563398675260384

Epoch: 5| Step: 10
Training loss: 2.624482966775648
Validation loss: 2.443523470819492

Epoch: 167| Step: 0
Training loss: 2.9069616615436416
Validation loss: 2.457098117837108

Epoch: 5| Step: 1
Training loss: 2.259594487969254
Validation loss: 2.4637264159973786

Epoch: 5| Step: 2
Training loss: 2.495121582982919
Validation loss: 2.4745334014300884

Epoch: 5| Step: 3
Training loss: 2.6218843589497465
Validation loss: 2.4471919002466134

Epoch: 5| Step: 4
Training loss: 2.330019800650488
Validation loss: 2.456933979130769

Epoch: 5| Step: 5
Training loss: 2.2131610378551265
Validation loss: 2.4621168908074753

Epoch: 5| Step: 6
Training loss: 2.897068080619632
Validation loss: 2.4682267406701075

Epoch: 5| Step: 7
Training loss: 2.480710860323725
Validation loss: 2.4515042154742215

Epoch: 5| Step: 8
Training loss: 2.4401178240860344
Validation loss: 2.467220470701186

Epoch: 5| Step: 9
Training loss: 2.3535220070906724
Validation loss: 2.4536139640547985

Epoch: 5| Step: 10
Training loss: 2.6505806052950014
Validation loss: 2.4632336674751976

Epoch: 168| Step: 0
Training loss: 2.444663354675668
Validation loss: 2.4429971924846265

Epoch: 5| Step: 1
Training loss: 2.4697137231217763
Validation loss: 2.4636532769651156

Epoch: 5| Step: 2
Training loss: 2.760212316086701
Validation loss: 2.458037970979347

Epoch: 5| Step: 3
Training loss: 2.057238485558347
Validation loss: 2.447620336382374

Epoch: 5| Step: 4
Training loss: 2.5653268991708202
Validation loss: 2.4493533845515576

Epoch: 5| Step: 5
Training loss: 2.042883555749342
Validation loss: 2.4600061933544706

Epoch: 5| Step: 6
Training loss: 2.4905652353733663
Validation loss: 2.455950872697708

Epoch: 5| Step: 7
Training loss: 2.631336828688407
Validation loss: 2.4514788477363627

Epoch: 5| Step: 8
Training loss: 2.8986554745046003
Validation loss: 2.4426387828826956

Epoch: 5| Step: 9
Training loss: 2.4303011104066203
Validation loss: 2.4338035984844013

Epoch: 5| Step: 10
Training loss: 2.7413721855962825
Validation loss: 2.4623804818284527

Epoch: 169| Step: 0
Training loss: 2.8458758781533287
Validation loss: 2.4559996397716213

Epoch: 5| Step: 1
Training loss: 1.8659430790199512
Validation loss: 2.4747617875196806

Epoch: 5| Step: 2
Training loss: 1.8541511434983937
Validation loss: 2.439494515708614

Epoch: 5| Step: 3
Training loss: 2.921797317859401
Validation loss: 2.4735294981146194

Epoch: 5| Step: 4
Training loss: 1.9993152638342058
Validation loss: 2.462491150702644

Epoch: 5| Step: 5
Training loss: 2.8572032581484335
Validation loss: 2.462819060547938

Epoch: 5| Step: 6
Training loss: 3.177115184853572
Validation loss: 2.464604853334307

Epoch: 5| Step: 7
Training loss: 2.810098088680424
Validation loss: 2.4619107500584847

Epoch: 5| Step: 8
Training loss: 2.1159516841585972
Validation loss: 2.472564320754906

Epoch: 5| Step: 9
Training loss: 2.4277223938134567
Validation loss: 2.4706946518147292

Epoch: 5| Step: 10
Training loss: 2.374335547408372
Validation loss: 2.4749766143492824

Epoch: 170| Step: 0
Training loss: 2.3112382281732415
Validation loss: 2.4657095684702477

Epoch: 5| Step: 1
Training loss: 2.3476903725681586
Validation loss: 2.4550579589487516

Epoch: 5| Step: 2
Training loss: 2.8056146900839907
Validation loss: 2.4586742994416864

Epoch: 5| Step: 3
Training loss: 2.7193658942296124
Validation loss: 2.47677382114966

Epoch: 5| Step: 4
Training loss: 2.582060336181437
Validation loss: 2.449354815335595

Epoch: 5| Step: 5
Training loss: 2.4880070078588576
Validation loss: 2.45756761991423

Epoch: 5| Step: 6
Training loss: 2.627759663593567
Validation loss: 2.4485754773804453

Epoch: 5| Step: 7
Training loss: 2.3307519211892633
Validation loss: 2.4752807385045745

Epoch: 5| Step: 8
Training loss: 2.3603898448940908
Validation loss: 2.4528695652736845

Epoch: 5| Step: 9
Training loss: 2.275740567913287
Validation loss: 2.452619051195901

Epoch: 5| Step: 10
Training loss: 2.6846540596874924
Validation loss: 2.4689301061243776

Epoch: 171| Step: 0
Training loss: 2.655280979899181
Validation loss: 2.4470632126139655

Epoch: 5| Step: 1
Training loss: 2.4007554176696075
Validation loss: 2.4567651837894138

Epoch: 5| Step: 2
Training loss: 1.7584494559947779
Validation loss: 2.4710117362490562

Epoch: 5| Step: 3
Training loss: 2.2305520170739803
Validation loss: 2.4453728634417438

Epoch: 5| Step: 4
Training loss: 3.041836058894676
Validation loss: 2.464001509617371

Epoch: 5| Step: 5
Training loss: 2.2872460354599577
Validation loss: 2.44323132807829

Epoch: 5| Step: 6
Training loss: 2.325962777913327
Validation loss: 2.461316182602209

Epoch: 5| Step: 7
Training loss: 2.5808669796718027
Validation loss: 2.443194544143173

Epoch: 5| Step: 8
Training loss: 2.2647808738631228
Validation loss: 2.456189993178289

Epoch: 5| Step: 9
Training loss: 2.242264271129626
Validation loss: 2.4491738715671185

Epoch: 5| Step: 10
Training loss: 3.518862035129799
Validation loss: 2.465590489903709

Epoch: 172| Step: 0
Training loss: 2.0467447719933545
Validation loss: 2.464764463242436

Epoch: 5| Step: 1
Training loss: 2.7875998372216246
Validation loss: 2.4725291645487446

Epoch: 5| Step: 2
Training loss: 2.9555119954231843
Validation loss: 2.4571437289883353

Epoch: 5| Step: 3
Training loss: 1.8380914268294135
Validation loss: 2.452135121348769

Epoch: 5| Step: 4
Training loss: 2.3720258863300634
Validation loss: 2.453220979782713

Epoch: 5| Step: 5
Training loss: 2.7110179702292654
Validation loss: 2.4502343475689488

Epoch: 5| Step: 6
Training loss: 2.255692803644606
Validation loss: 2.43247979807126

Epoch: 5| Step: 7
Training loss: 2.3289156601647565
Validation loss: 2.458103207397885

Epoch: 5| Step: 8
Training loss: 2.8030445100079038
Validation loss: 2.44047606292658

Epoch: 5| Step: 9
Training loss: 2.625452456762866
Validation loss: 2.444184036972128

Epoch: 5| Step: 10
Training loss: 2.7160625987528784
Validation loss: 2.4527048263590023

Epoch: 173| Step: 0
Training loss: 2.2497001024230507
Validation loss: 2.4876630708089884

Epoch: 5| Step: 1
Training loss: 2.4932879944961304
Validation loss: 2.4581897610939256

Epoch: 5| Step: 2
Training loss: 2.485463413801872
Validation loss: 2.464782154509788

Epoch: 5| Step: 3
Training loss: 2.610330007159412
Validation loss: 2.467321911965058

Epoch: 5| Step: 4
Training loss: 3.044226801379507
Validation loss: 2.4560517940725597

Epoch: 5| Step: 5
Training loss: 2.4749536914539045
Validation loss: 2.452973497419201

Epoch: 5| Step: 6
Training loss: 2.32068957209422
Validation loss: 2.4576419743185958

Epoch: 5| Step: 7
Training loss: 2.2198832666791026
Validation loss: 2.4724028523158763

Epoch: 5| Step: 8
Training loss: 2.4326293405958967
Validation loss: 2.4645932968715343

Epoch: 5| Step: 9
Training loss: 2.3399270986954854
Validation loss: 2.4690931566386634

Epoch: 5| Step: 10
Training loss: 2.8227983757281403
Validation loss: 2.4579341272995503

Epoch: 174| Step: 0
Training loss: 3.1570333887218998
Validation loss: 2.466384527034029

Epoch: 5| Step: 1
Training loss: 2.090822593121129
Validation loss: 2.4667893575092084

Epoch: 5| Step: 2
Training loss: 2.4581690156838216
Validation loss: 2.4625338323698984

Epoch: 5| Step: 3
Training loss: 2.616947265530979
Validation loss: 2.4467880198989076

Epoch: 5| Step: 4
Training loss: 1.9694538977389682
Validation loss: 2.4644832924439006

Epoch: 5| Step: 5
Training loss: 2.519969813302301
Validation loss: 2.4602912269092387

Epoch: 5| Step: 6
Training loss: 2.269798171191782
Validation loss: 2.4610474597893015

Epoch: 5| Step: 7
Training loss: 2.6670890910626754
Validation loss: 2.4501998346066847

Epoch: 5| Step: 8
Training loss: 2.867978685861126
Validation loss: 2.4591846886370035

Epoch: 5| Step: 9
Training loss: 2.217020031960252
Validation loss: 2.4639416733053543

Epoch: 5| Step: 10
Training loss: 2.4096144897339373
Validation loss: 2.440368628206557

Epoch: 175| Step: 0
Training loss: 2.7777589468847528
Validation loss: 2.4775561306896647

Epoch: 5| Step: 1
Training loss: 2.5331496657331374
Validation loss: 2.4604971605504784

Epoch: 5| Step: 2
Training loss: 2.7354690542867504
Validation loss: 2.461071982015921

Epoch: 5| Step: 3
Training loss: 2.2416113136387192
Validation loss: 2.4304320431064714

Epoch: 5| Step: 4
Training loss: 2.569535714896879
Validation loss: 2.4522936632231

Epoch: 5| Step: 5
Training loss: 2.679585980419128
Validation loss: 2.457392325971276

Epoch: 5| Step: 6
Training loss: 2.426079625746828
Validation loss: 2.455378131838007

Epoch: 5| Step: 7
Training loss: 2.5033968736158942
Validation loss: 2.4440484820056887

Epoch: 5| Step: 8
Training loss: 1.7622666955298525
Validation loss: 2.4756188745959786

Epoch: 5| Step: 9
Training loss: 2.7246810411441666
Validation loss: 2.467138855813923

Epoch: 5| Step: 10
Training loss: 2.3011598647915874
Validation loss: 2.46118471069731

Epoch: 176| Step: 0
Training loss: 2.7203824634089413
Validation loss: 2.4820869648266797

Epoch: 5| Step: 1
Training loss: 1.9361666121236425
Validation loss: 2.476893959108749

Epoch: 5| Step: 2
Training loss: 2.060764160270498
Validation loss: 2.4706895342706816

Epoch: 5| Step: 3
Training loss: 2.2279030495374856
Validation loss: 2.4610830529279424

Epoch: 5| Step: 4
Training loss: 2.827203336983154
Validation loss: 2.4762798694230925

Epoch: 5| Step: 5
Training loss: 2.4459737547856766
Validation loss: 2.453895192948791

Epoch: 5| Step: 6
Training loss: 2.0722630938515407
Validation loss: 2.4573013259222183

Epoch: 5| Step: 7
Training loss: 2.538607230818364
Validation loss: 2.4419171655421112

Epoch: 5| Step: 8
Training loss: 3.026762954619914
Validation loss: 2.4531414887586274

Epoch: 5| Step: 9
Training loss: 2.9449276457675753
Validation loss: 2.461816131684632

Epoch: 5| Step: 10
Training loss: 2.367310372868175
Validation loss: 2.453259187143479

Epoch: 177| Step: 0
Training loss: 2.6474976799137946
Validation loss: 2.451598821916927

Epoch: 5| Step: 1
Training loss: 2.7274717547302965
Validation loss: 2.4631960842225564

Epoch: 5| Step: 2
Training loss: 2.05176280918448
Validation loss: 2.460099245262598

Epoch: 5| Step: 3
Training loss: 1.9531293945263062
Validation loss: 2.4676155037099896

Epoch: 5| Step: 4
Training loss: 2.3947357538462386
Validation loss: 2.4527440347129135

Epoch: 5| Step: 5
Training loss: 2.797559601095156
Validation loss: 2.4437447113046185

Epoch: 5| Step: 6
Training loss: 2.2205843797270557
Validation loss: 2.4841270330768017

Epoch: 5| Step: 7
Training loss: 2.8674747000977505
Validation loss: 2.472695969343897

Epoch: 5| Step: 8
Training loss: 2.6415903708173274
Validation loss: 2.474827408959279

Epoch: 5| Step: 9
Training loss: 2.3953036731773825
Validation loss: 2.452970154092422

Epoch: 5| Step: 10
Training loss: 2.404777076049242
Validation loss: 2.456219102062463

Epoch: 178| Step: 0
Training loss: 2.8153626178925957
Validation loss: 2.473599365995933

Epoch: 5| Step: 1
Training loss: 2.5229546524913666
Validation loss: 2.4750950217586256

Epoch: 5| Step: 2
Training loss: 2.301436481534965
Validation loss: 2.4708503811233307

Epoch: 5| Step: 3
Training loss: 2.611560933841949
Validation loss: 2.4609144994621546

Epoch: 5| Step: 4
Training loss: 2.2127926864566603
Validation loss: 2.4734753958672835

Epoch: 5| Step: 5
Training loss: 2.3576845087560736
Validation loss: 2.4630484277247295

Epoch: 5| Step: 6
Training loss: 2.5289538294453036
Validation loss: 2.4449504012958005

Epoch: 5| Step: 7
Training loss: 2.6145208181579935
Validation loss: 2.4717464764312425

Epoch: 5| Step: 8
Training loss: 3.0370544232367895
Validation loss: 2.4762042131812567

Epoch: 5| Step: 9
Training loss: 2.2240528619645037
Validation loss: 2.473646510387695

Epoch: 5| Step: 10
Training loss: 1.959252655736212
Validation loss: 2.4481854928481646

Epoch: 179| Step: 0
Training loss: 3.131500502139104
Validation loss: 2.4719615258983714

Epoch: 5| Step: 1
Training loss: 2.6752776848526505
Validation loss: 2.4726017460591843

Epoch: 5| Step: 2
Training loss: 2.3774918985479494
Validation loss: 2.478131335982208

Epoch: 5| Step: 3
Training loss: 2.1022151798554187
Validation loss: 2.4776442263404315

Epoch: 5| Step: 4
Training loss: 2.3092064988293792
Validation loss: 2.453624790690955

Epoch: 5| Step: 5
Training loss: 2.5109896395100004
Validation loss: 2.4591723331822544

Epoch: 5| Step: 6
Training loss: 1.9279088151072619
Validation loss: 2.454319253755415

Epoch: 5| Step: 7
Training loss: 2.410286527355496
Validation loss: 2.4633265766513657

Epoch: 5| Step: 8
Training loss: 2.7013385139307706
Validation loss: 2.442447011236843

Epoch: 5| Step: 9
Training loss: 2.461387956355498
Validation loss: 2.463665471030214

Epoch: 5| Step: 10
Training loss: 2.362568494015785
Validation loss: 2.4723157986729474

Epoch: 180| Step: 0
Training loss: 2.651993786023485
Validation loss: 2.4555327071405206

Epoch: 5| Step: 1
Training loss: 2.339568413278953
Validation loss: 2.449270743056826

Epoch: 5| Step: 2
Training loss: 2.4719605634808954
Validation loss: 2.4801540328517016

Epoch: 5| Step: 3
Training loss: 2.4364694226227197
Validation loss: 2.465348866271747

Epoch: 5| Step: 4
Training loss: 2.3444239346820903
Validation loss: 2.4539665940226834

Epoch: 5| Step: 5
Training loss: 1.9525834210069517
Validation loss: 2.4545940120558027

Epoch: 5| Step: 6
Training loss: 2.41291915335703
Validation loss: 2.4749925048770987

Epoch: 5| Step: 7
Training loss: 2.7448527107954592
Validation loss: 2.4605297302259643

Epoch: 5| Step: 8
Training loss: 3.086773027711849
Validation loss: 2.4673840238215874

Epoch: 5| Step: 9
Training loss: 1.8930388245185685
Validation loss: 2.4402274539933755

Epoch: 5| Step: 10
Training loss: 2.623436871108797
Validation loss: 2.472249415021763

Epoch: 181| Step: 0
Training loss: 2.5892328660955517
Validation loss: 2.467842553143958

Epoch: 5| Step: 1
Training loss: 2.436793885291099
Validation loss: 2.4805600059827113

Epoch: 5| Step: 2
Training loss: 2.1057189905935196
Validation loss: 2.4690967491277203

Epoch: 5| Step: 3
Training loss: 2.6835987252046314
Validation loss: 2.492280685550203

Epoch: 5| Step: 4
Training loss: 2.695626414718982
Validation loss: 2.4876124794546057

Epoch: 5| Step: 5
Training loss: 2.511604745511236
Validation loss: 2.4828429598160096

Epoch: 5| Step: 6
Training loss: 2.8432070035548724
Validation loss: 2.4641987500269944

Epoch: 5| Step: 7
Training loss: 1.9527387313351587
Validation loss: 2.4600557825020215

Epoch: 5| Step: 8
Training loss: 2.5098595746338286
Validation loss: 2.4726920897117455

Epoch: 5| Step: 9
Training loss: 2.696605339138826
Validation loss: 2.454579762394814

Epoch: 5| Step: 10
Training loss: 2.107668603236893
Validation loss: 2.443096804219943

Epoch: 182| Step: 0
Training loss: 2.4072301280399557
Validation loss: 2.481587354715674

Epoch: 5| Step: 1
Training loss: 2.6058645791316457
Validation loss: 2.4563682388461032

Epoch: 5| Step: 2
Training loss: 2.028086737461381
Validation loss: 2.4683598399193296

Epoch: 5| Step: 3
Training loss: 1.9727594027601514
Validation loss: 2.4636719912948983

Epoch: 5| Step: 4
Training loss: 2.532641087449648
Validation loss: 2.452712382314945

Epoch: 5| Step: 5
Training loss: 2.2327545265268904
Validation loss: 2.4786235710473883

Epoch: 5| Step: 6
Training loss: 2.2036865817252855
Validation loss: 2.451800047415159

Epoch: 5| Step: 7
Training loss: 2.8180089615083044
Validation loss: 2.467782247431767

Epoch: 5| Step: 8
Training loss: 3.238670408565809
Validation loss: 2.4595150051562094

Epoch: 5| Step: 9
Training loss: 2.4545270472777174
Validation loss: 2.474527804899298

Epoch: 5| Step: 10
Training loss: 2.4581130516369027
Validation loss: 2.4534210889602477

Epoch: 183| Step: 0
Training loss: 2.529903854533526
Validation loss: 2.4432153243644446

Epoch: 5| Step: 1
Training loss: 2.5153659663499526
Validation loss: 2.4659459659543583

Epoch: 5| Step: 2
Training loss: 1.854742225154547
Validation loss: 2.4730751683109946

Epoch: 5| Step: 3
Training loss: 2.91498115520373
Validation loss: 2.4824644019444175

Epoch: 5| Step: 4
Training loss: 1.7655084622163302
Validation loss: 2.4655967160230934

Epoch: 5| Step: 5
Training loss: 2.8315547147567415
Validation loss: 2.4789355040899412

Epoch: 5| Step: 6
Training loss: 2.406101569019274
Validation loss: 2.4576724439637054

Epoch: 5| Step: 7
Training loss: 1.850749742812196
Validation loss: 2.451105254442554

Epoch: 5| Step: 8
Training loss: 2.542026796519347
Validation loss: 2.4485906315058044

Epoch: 5| Step: 9
Training loss: 2.6210007857139948
Validation loss: 2.446612023828112

Epoch: 5| Step: 10
Training loss: 3.028280002654163
Validation loss: 2.4694087218110967

Epoch: 184| Step: 0
Training loss: 2.0510782662518685
Validation loss: 2.461782302822611

Epoch: 5| Step: 1
Training loss: 2.6254935027757496
Validation loss: 2.458270718872272

Epoch: 5| Step: 2
Training loss: 2.0998931267200813
Validation loss: 2.458097457693271

Epoch: 5| Step: 3
Training loss: 2.22755147820959
Validation loss: 2.485857626866354

Epoch: 5| Step: 4
Training loss: 3.0935493172955946
Validation loss: 2.472151622967828

Epoch: 5| Step: 5
Training loss: 2.8616500725447396
Validation loss: 2.49178796719106

Epoch: 5| Step: 6
Training loss: 2.2164860310009984
Validation loss: 2.4913896905185133

Epoch: 5| Step: 7
Training loss: 2.5321221438592016
Validation loss: 2.4938310250864912

Epoch: 5| Step: 8
Training loss: 2.322183798462463
Validation loss: 2.4602914686548467

Epoch: 5| Step: 9
Training loss: 2.261135309367413
Validation loss: 2.483439779164734

Epoch: 5| Step: 10
Training loss: 2.747859381791286
Validation loss: 2.487566019708356

Epoch: 185| Step: 0
Training loss: 2.9144921689140175
Validation loss: 2.496910613608142

Epoch: 5| Step: 1
Training loss: 1.9549380013081592
Validation loss: 2.4798402702712803

Epoch: 5| Step: 2
Training loss: 2.2156395399211073
Validation loss: 2.475736582350928

Epoch: 5| Step: 3
Training loss: 2.341875369273254
Validation loss: 2.478440854163743

Epoch: 5| Step: 4
Training loss: 1.8035047874926506
Validation loss: 2.468190146440351

Epoch: 5| Step: 5
Training loss: 2.684834422541704
Validation loss: 2.451986839545441

Epoch: 5| Step: 6
Training loss: 2.078433824122116
Validation loss: 2.465749418428866

Epoch: 5| Step: 7
Training loss: 2.4421479342154466
Validation loss: 2.4335266893066763

Epoch: 5| Step: 8
Training loss: 2.6917641101437924
Validation loss: 2.4595403952838484

Epoch: 5| Step: 9
Training loss: 3.0374274476731014
Validation loss: 2.4831289772056415

Epoch: 5| Step: 10
Training loss: 2.688043317655799
Validation loss: 2.440731041122848

Epoch: 186| Step: 0
Training loss: 2.851003764556465
Validation loss: 2.4480469790516546

Epoch: 5| Step: 1
Training loss: 2.1879671960285916
Validation loss: 2.4547718274159087

Epoch: 5| Step: 2
Training loss: 2.0659221810514548
Validation loss: 2.466537039312616

Epoch: 5| Step: 3
Training loss: 2.394826152122354
Validation loss: 2.4621581191768893

Epoch: 5| Step: 4
Training loss: 2.517557101165611
Validation loss: 2.4324851266881167

Epoch: 5| Step: 5
Training loss: 3.0458174997762684
Validation loss: 2.4630527898835357

Epoch: 5| Step: 6
Training loss: 2.627958356402521
Validation loss: 2.4670524072171744

Epoch: 5| Step: 7
Training loss: 2.0068793002082232
Validation loss: 2.4348671453072526

Epoch: 5| Step: 8
Training loss: 2.236163084111961
Validation loss: 2.459175014442136

Epoch: 5| Step: 9
Training loss: 2.052787685700169
Validation loss: 2.455608658615517

Epoch: 5| Step: 10
Training loss: 2.9557406028940836
Validation loss: 2.469509666764239

Epoch: 187| Step: 0
Training loss: 2.673557577106397
Validation loss: 2.4731977570600465

Epoch: 5| Step: 1
Training loss: 2.453507314642728
Validation loss: 2.4756562682211265

Epoch: 5| Step: 2
Training loss: 1.972832881460256
Validation loss: 2.4716855518549288

Epoch: 5| Step: 3
Training loss: 2.2684087028940527
Validation loss: 2.4561195695231386

Epoch: 5| Step: 4
Training loss: 2.817650889000906
Validation loss: 2.4854775116721672

Epoch: 5| Step: 5
Training loss: 3.0074603778769076
Validation loss: 2.4804652504625375

Epoch: 5| Step: 6
Training loss: 2.408786081262598
Validation loss: 2.4539366884713276

Epoch: 5| Step: 7
Training loss: 2.6541744425693152
Validation loss: 2.443708668402734

Epoch: 5| Step: 8
Training loss: 2.0999485327453726
Validation loss: 2.4759447279553526

Epoch: 5| Step: 9
Training loss: 2.2583146642309755
Validation loss: 2.4536608979399324

Epoch: 5| Step: 10
Training loss: 2.1814959280853423
Validation loss: 2.4479812280174236

Epoch: 188| Step: 0
Training loss: 2.630735670550054
Validation loss: 2.4710752285527606

Epoch: 5| Step: 1
Training loss: 2.375952178416029
Validation loss: 2.4571300392631126

Epoch: 5| Step: 2
Training loss: 1.9844922609300863
Validation loss: 2.463365446308186

Epoch: 5| Step: 3
Training loss: 2.4932852213938435
Validation loss: 2.4813842654867466

Epoch: 5| Step: 4
Training loss: 2.474730767913396
Validation loss: 2.4723736337868485

Epoch: 5| Step: 5
Training loss: 2.466009332947036
Validation loss: 2.4676160449841964

Epoch: 5| Step: 6
Training loss: 2.3482127109337894
Validation loss: 2.4563091131208674

Epoch: 5| Step: 7
Training loss: 2.5679403100964517
Validation loss: 2.4662701660011694

Epoch: 5| Step: 8
Training loss: 2.3057360316753766
Validation loss: 2.4660260931730904

Epoch: 5| Step: 9
Training loss: 2.8142618383013054
Validation loss: 2.4794444790842016

Epoch: 5| Step: 10
Training loss: 2.5043342212682367
Validation loss: 2.475183686622117

Epoch: 189| Step: 0
Training loss: 1.9511288627641508
Validation loss: 2.4721354746288715

Epoch: 5| Step: 1
Training loss: 2.3559699339834506
Validation loss: 2.4761712962951243

Epoch: 5| Step: 2
Training loss: 1.9862508243009322
Validation loss: 2.472166402359506

Epoch: 5| Step: 3
Training loss: 2.7053878221296555
Validation loss: 2.4548979121927195

Epoch: 5| Step: 4
Training loss: 2.8468247443745294
Validation loss: 2.460150532145673

Epoch: 5| Step: 5
Training loss: 2.5842859808686627
Validation loss: 2.4709282877480248

Epoch: 5| Step: 6
Training loss: 2.5522863086767402
Validation loss: 2.459394416451511

Epoch: 5| Step: 7
Training loss: 2.534510455292272
Validation loss: 2.4597564233040767

Epoch: 5| Step: 8
Training loss: 2.3771480082329024
Validation loss: 2.461983996565659

Epoch: 5| Step: 9
Training loss: 2.1859175271464015
Validation loss: 2.474779059255237

Epoch: 5| Step: 10
Training loss: 2.75486593636824
Validation loss: 2.4804305669474576

Epoch: 190| Step: 0
Training loss: 2.9985516548789914
Validation loss: 2.459402631489315

Epoch: 5| Step: 1
Training loss: 2.414002451720806
Validation loss: 2.469024770639923

Epoch: 5| Step: 2
Training loss: 2.3637153163808264
Validation loss: 2.4312620041247173

Epoch: 5| Step: 3
Training loss: 1.7871532664094425
Validation loss: 2.4631402708217607

Epoch: 5| Step: 4
Training loss: 2.3989124138551086
Validation loss: 2.468593540013136

Epoch: 5| Step: 5
Training loss: 2.411113827006055
Validation loss: 2.4866318577712545

Epoch: 5| Step: 6
Training loss: 2.0469981149237095
Validation loss: 2.456181799751636

Epoch: 5| Step: 7
Training loss: 3.183251934775038
Validation loss: 2.492196455592335

Epoch: 5| Step: 8
Training loss: 2.8245964167111315
Validation loss: 2.4650385538983453

Epoch: 5| Step: 9
Training loss: 2.338080198526131
Validation loss: 2.478124353059867

Epoch: 5| Step: 10
Training loss: 1.8536892036865662
Validation loss: 2.4524777323143323

Epoch: 191| Step: 0
Training loss: 2.2295222370196246
Validation loss: 2.4639260746316993

Epoch: 5| Step: 1
Training loss: 2.416248208571716
Validation loss: 2.470878223727389

Epoch: 5| Step: 2
Training loss: 2.5037318033517955
Validation loss: 2.4714268359334346

Epoch: 5| Step: 3
Training loss: 2.8535839701881125
Validation loss: 2.4653876416536815

Epoch: 5| Step: 4
Training loss: 2.1581771641891656
Validation loss: 2.4898829874271775

Epoch: 5| Step: 5
Training loss: 2.3380172809911777
Validation loss: 2.4684309799368096

Epoch: 5| Step: 6
Training loss: 2.4204316791426312
Validation loss: 2.4867414999499595

Epoch: 5| Step: 7
Training loss: 2.5635582204256098
Validation loss: 2.4689593290631136

Epoch: 5| Step: 8
Training loss: 2.4192335921844066
Validation loss: 2.491767770033862

Epoch: 5| Step: 9
Training loss: 2.5693904069138402
Validation loss: 2.480789137265646

Epoch: 5| Step: 10
Training loss: 2.2938406487204843
Validation loss: 2.4705440481864547

Epoch: 192| Step: 0
Training loss: 2.844633698707406
Validation loss: 2.4750027604852645

Epoch: 5| Step: 1
Training loss: 2.2141381271680407
Validation loss: 2.4643737730781905

Epoch: 5| Step: 2
Training loss: 2.638778582715847
Validation loss: 2.487415838258095

Epoch: 5| Step: 3
Training loss: 2.027171926989896
Validation loss: 2.475754518289232

Epoch: 5| Step: 4
Training loss: 2.498450275743944
Validation loss: 2.4834593895814137

Epoch: 5| Step: 5
Training loss: 2.3998448719116663
Validation loss: 2.496135906391339

Epoch: 5| Step: 6
Training loss: 2.0366683292330103
Validation loss: 2.463555295094347

Epoch: 5| Step: 7
Training loss: 2.5102105484760964
Validation loss: 2.4751967109872774

Epoch: 5| Step: 8
Training loss: 2.4089707684863324
Validation loss: 2.472273901835187

Epoch: 5| Step: 9
Training loss: 2.769433626463931
Validation loss: 2.4474240633217126

Epoch: 5| Step: 10
Training loss: 2.409269246948288
Validation loss: 2.478248609929652

Epoch: 193| Step: 0
Training loss: 2.557671720047934
Validation loss: 2.488003006815227

Epoch: 5| Step: 1
Training loss: 2.564555204615797
Validation loss: 2.4650748860382414

Epoch: 5| Step: 2
Training loss: 2.4763105486919916
Validation loss: 2.485350898213782

Epoch: 5| Step: 3
Training loss: 2.146473045423353
Validation loss: 2.462856907702508

Epoch: 5| Step: 4
Training loss: 2.3684436247681373
Validation loss: 2.4767393457477502

Epoch: 5| Step: 5
Training loss: 2.5923597907443265
Validation loss: 2.473118157629765

Epoch: 5| Step: 6
Training loss: 1.944386089297007
Validation loss: 2.4835506316746656

Epoch: 5| Step: 7
Training loss: 2.5234235166746855
Validation loss: 2.459928244092727

Epoch: 5| Step: 8
Training loss: 2.563129626820836
Validation loss: 2.481696554497759

Epoch: 5| Step: 9
Training loss: 2.610754961610525
Validation loss: 2.4668262900088567

Epoch: 5| Step: 10
Training loss: 2.229023747601314
Validation loss: 2.4632847616072304

Epoch: 194| Step: 0
Training loss: 2.5316664741655
Validation loss: 2.457267496459553

Epoch: 5| Step: 1
Training loss: 2.547818436266542
Validation loss: 2.460103902350794

Epoch: 5| Step: 2
Training loss: 2.6071955550479835
Validation loss: 2.4855607780373816

Epoch: 5| Step: 3
Training loss: 2.3830533265531524
Validation loss: 2.4603842232327704

Epoch: 5| Step: 4
Training loss: 2.232742032967268
Validation loss: 2.4656136490543283

Epoch: 5| Step: 5
Training loss: 2.893558807318616
Validation loss: 2.4473095275556602

Epoch: 5| Step: 6
Training loss: 2.4250736893931317
Validation loss: 2.4720267329579992

Epoch: 5| Step: 7
Training loss: 2.0585447591493224
Validation loss: 2.4528454705641636

Epoch: 5| Step: 8
Training loss: 2.2173997235768415
Validation loss: 2.47128703863219

Epoch: 5| Step: 9
Training loss: 1.9311590105368488
Validation loss: 2.4471290035043927

Epoch: 5| Step: 10
Training loss: 2.926575006803478
Validation loss: 2.462724434391483

Epoch: 195| Step: 0
Training loss: 2.981197882942483
Validation loss: 2.464292816058969

Epoch: 5| Step: 1
Training loss: 2.4380576645100245
Validation loss: 2.470240874756006

Epoch: 5| Step: 2
Training loss: 2.261639055024024
Validation loss: 2.4794919631354926

Epoch: 5| Step: 3
Training loss: 2.446887986500699
Validation loss: 2.4558036146626825

Epoch: 5| Step: 4
Training loss: 2.4628175366150145
Validation loss: 2.4754161169610196

Epoch: 5| Step: 5
Training loss: 2.5144531174144595
Validation loss: 2.4959918100563456

Epoch: 5| Step: 6
Training loss: 2.1482401809246015
Validation loss: 2.4900668290822545

Epoch: 5| Step: 7
Training loss: 1.9851409154882942
Validation loss: 2.4825037392357396

Epoch: 5| Step: 8
Training loss: 2.4265738895343016
Validation loss: 2.486004917558377

Epoch: 5| Step: 9
Training loss: 2.7676298127230523
Validation loss: 2.482162473780006

Epoch: 5| Step: 10
Training loss: 2.299394950894203
Validation loss: 2.479414747453153

Epoch: 196| Step: 0
Training loss: 2.5172464582854195
Validation loss: 2.483683186352601

Epoch: 5| Step: 1
Training loss: 2.5637762101654946
Validation loss: 2.458823791419576

Epoch: 5| Step: 2
Training loss: 2.1705092932048062
Validation loss: 2.474447656154741

Epoch: 5| Step: 3
Training loss: 2.410070878407217
Validation loss: 2.495343560212684

Epoch: 5| Step: 4
Training loss: 2.5828407392210013
Validation loss: 2.466228217394275

Epoch: 5| Step: 5
Training loss: 2.0554279110199665
Validation loss: 2.4714633614925514

Epoch: 5| Step: 6
Training loss: 2.0779181355935945
Validation loss: 2.489754520118332

Epoch: 5| Step: 7
Training loss: 2.9360265079829597
Validation loss: 2.4973737232437783

Epoch: 5| Step: 8
Training loss: 2.6511746556188913
Validation loss: 2.4862922529442506

Epoch: 5| Step: 9
Training loss: 2.0315977532473686
Validation loss: 2.467318741858812

Epoch: 5| Step: 10
Training loss: 2.6027800772254293
Validation loss: 2.4782243786997493

Epoch: 197| Step: 0
Training loss: 2.4875491036787376
Validation loss: 2.466052047284231

Epoch: 5| Step: 1
Training loss: 2.775324477736986
Validation loss: 2.4811067987386504

Epoch: 5| Step: 2
Training loss: 2.6884402027712935
Validation loss: 2.4750610967996045

Epoch: 5| Step: 3
Training loss: 2.357711710965513
Validation loss: 2.483174426135717

Epoch: 5| Step: 4
Training loss: 2.3767234170923173
Validation loss: 2.465211592421358

Epoch: 5| Step: 5
Training loss: 2.366784391922746
Validation loss: 2.4843872156692255

Epoch: 5| Step: 6
Training loss: 1.9599358785611556
Validation loss: 2.462927041910791

Epoch: 5| Step: 7
Training loss: 2.212748618085294
Validation loss: 2.4917252732648167

Epoch: 5| Step: 8
Training loss: 2.436407969458345
Validation loss: 2.4629295754402016

Epoch: 5| Step: 9
Training loss: 2.462709206926067
Validation loss: 2.4790625212101527

Epoch: 5| Step: 10
Training loss: 2.528877559354165
Validation loss: 2.4923713280166484

Epoch: 198| Step: 0
Training loss: 2.9309554547842733
Validation loss: 2.458394596455113

Epoch: 5| Step: 1
Training loss: 2.7080229556915216
Validation loss: 2.460477779724508

Epoch: 5| Step: 2
Training loss: 2.270159163419671
Validation loss: 2.46647812616924

Epoch: 5| Step: 3
Training loss: 2.180113829344451
Validation loss: 2.480217344349492

Epoch: 5| Step: 4
Training loss: 2.0040819711957214
Validation loss: 2.4762149327647

Epoch: 5| Step: 5
Training loss: 1.8230156208609947
Validation loss: 2.4781734937940323

Epoch: 5| Step: 6
Training loss: 2.5061615355139906
Validation loss: 2.4926321650260626

Epoch: 5| Step: 7
Training loss: 2.5412119995518396
Validation loss: 2.484606195445946

Epoch: 5| Step: 8
Training loss: 2.307625659567076
Validation loss: 2.5003946608080736

Epoch: 5| Step: 9
Training loss: 2.559816583678924
Validation loss: 2.4871800440383587

Epoch: 5| Step: 10
Training loss: 2.6133385439580756
Validation loss: 2.4755683141200295

Epoch: 199| Step: 0
Training loss: 2.592285202236179
Validation loss: 2.4631371302005625

Epoch: 5| Step: 1
Training loss: 2.3174977296161745
Validation loss: 2.4655306490034845

Epoch: 5| Step: 2
Training loss: 2.7238996996626037
Validation loss: 2.4791980871365693

Epoch: 5| Step: 3
Training loss: 2.4637325823132876
Validation loss: 2.462454388990763

Epoch: 5| Step: 4
Training loss: 3.0670331763583536
Validation loss: 2.4659537703507493

Epoch: 5| Step: 5
Training loss: 2.315533581369308
Validation loss: 2.472290752321829

Epoch: 5| Step: 6
Training loss: 1.4924021461651333
Validation loss: 2.457823383617068

Epoch: 5| Step: 7
Training loss: 3.0131878269421124
Validation loss: 2.480598504320192

Epoch: 5| Step: 8
Training loss: 2.505173194517215
Validation loss: 2.4745157591578195

Epoch: 5| Step: 9
Training loss: 1.9575258037479013
Validation loss: 2.4621746130385005

Epoch: 5| Step: 10
Training loss: 1.9405008887564381
Validation loss: 2.4595924755473244

Epoch: 200| Step: 0
Training loss: 1.701119458237383
Validation loss: 2.4828724851904633

Epoch: 5| Step: 1
Training loss: 2.850518691395929
Validation loss: 2.46612977127926

Epoch: 5| Step: 2
Training loss: 2.7298029577668097
Validation loss: 2.470013937839194

Epoch: 5| Step: 3
Training loss: 2.5045230004733243
Validation loss: 2.466580535456922

Epoch: 5| Step: 4
Training loss: 2.6309726523613173
Validation loss: 2.4969750387461747

Epoch: 5| Step: 5
Training loss: 1.9909276232207238
Validation loss: 2.486985985909195

Epoch: 5| Step: 6
Training loss: 2.222284215486267
Validation loss: 2.4971600080649483

Epoch: 5| Step: 7
Training loss: 2.8935271669611784
Validation loss: 2.4926612494499887

Epoch: 5| Step: 8
Training loss: 2.3822854381547134
Validation loss: 2.4916288984424653

Epoch: 5| Step: 9
Training loss: 2.3747762273474033
Validation loss: 2.4653376003018526

Epoch: 5| Step: 10
Training loss: 2.3115071407721888
Validation loss: 2.486295516405043

Epoch: 201| Step: 0
Training loss: 2.698712490540239
Validation loss: 2.469027334253508

Epoch: 5| Step: 1
Training loss: 2.4477034022723436
Validation loss: 2.4863098199021

Epoch: 5| Step: 2
Training loss: 2.8042608165003533
Validation loss: 2.466875258818806

Epoch: 5| Step: 3
Training loss: 2.5297999054862217
Validation loss: 2.472337136711304

Epoch: 5| Step: 4
Training loss: 2.5422550729353417
Validation loss: 2.4832814366936566

Epoch: 5| Step: 5
Training loss: 2.236890110882531
Validation loss: 2.484101374112722

Epoch: 5| Step: 6
Training loss: 2.2364394247685717
Validation loss: 2.4891532670308725

Epoch: 5| Step: 7
Training loss: 2.2014376624554344
Validation loss: 2.4805520904495624

Epoch: 5| Step: 8
Training loss: 2.3944462174568586
Validation loss: 2.4621894830330913

Epoch: 5| Step: 9
Training loss: 2.0932625729857426
Validation loss: 2.4767150821551356

Epoch: 5| Step: 10
Training loss: 2.0932216831771773
Validation loss: 2.464676621811648

Epoch: 202| Step: 0
Training loss: 2.72267560254174
Validation loss: 2.4892280013708294

Epoch: 5| Step: 1
Training loss: 2.586069063372246
Validation loss: 2.465705928417662

Epoch: 5| Step: 2
Training loss: 2.2252342143710115
Validation loss: 2.4674293004148984

Epoch: 5| Step: 3
Training loss: 2.200514416807552
Validation loss: 2.4642365099299974

Epoch: 5| Step: 4
Training loss: 1.9059982133518742
Validation loss: 2.4721546261407332

Epoch: 5| Step: 5
Training loss: 2.41826099114911
Validation loss: 2.4503400189132365

Epoch: 5| Step: 6
Training loss: 2.4558495619796092
Validation loss: 2.4969218644357207

Epoch: 5| Step: 7
Training loss: 2.5062201367356574
Validation loss: 2.494094463655527

Epoch: 5| Step: 8
Training loss: 3.00912835208654
Validation loss: 2.4937725686120107

Epoch: 5| Step: 9
Training loss: 2.3083040471979213
Validation loss: 2.4925939913974884

Epoch: 5| Step: 10
Training loss: 2.2025356721966127
Validation loss: 2.4877501480011297

Epoch: 203| Step: 0
Training loss: 2.4747281666991134
Validation loss: 2.4693080644661256

Epoch: 5| Step: 1
Training loss: 2.9650671350813544
Validation loss: 2.4727631330851025

Epoch: 5| Step: 2
Training loss: 2.42741724645204
Validation loss: 2.4679431648253796

Epoch: 5| Step: 3
Training loss: 2.597858897170779
Validation loss: 2.4635742333810944

Epoch: 5| Step: 4
Training loss: 2.304561495165091
Validation loss: 2.490952564759861

Epoch: 5| Step: 5
Training loss: 2.4807361368554868
Validation loss: 2.4874489572727367

Epoch: 5| Step: 6
Training loss: 2.349989862623061
Validation loss: 2.4954406879188364

Epoch: 5| Step: 7
Training loss: 2.3567995073179415
Validation loss: 2.460852001376307

Epoch: 5| Step: 8
Training loss: 2.1280161825582953
Validation loss: 2.479908612327616

Epoch: 5| Step: 9
Training loss: 2.422590580714194
Validation loss: 2.4656178522710164

Epoch: 5| Step: 10
Training loss: 1.8677283106379252
Validation loss: 2.4814696620794097

Epoch: 204| Step: 0
Training loss: 2.17783772176303
Validation loss: 2.481143969132706

Epoch: 5| Step: 1
Training loss: 2.094663961404324
Validation loss: 2.4679451821279454

Epoch: 5| Step: 2
Training loss: 2.603162302251397
Validation loss: 2.4636312242688874

Epoch: 5| Step: 3
Training loss: 2.3835220031042774
Validation loss: 2.440508891914123

Epoch: 5| Step: 4
Training loss: 2.334630083601747
Validation loss: 2.4660320520599037

Epoch: 5| Step: 5
Training loss: 2.5949039879532263
Validation loss: 2.490849615732101

Epoch: 5| Step: 6
Training loss: 2.407155745820667
Validation loss: 2.4471891974767606

Epoch: 5| Step: 7
Training loss: 2.313769224180777
Validation loss: 2.4865597806451354

Epoch: 5| Step: 8
Training loss: 2.4630623499683364
Validation loss: 2.4840942273685815

Epoch: 5| Step: 9
Training loss: 2.7549186414831577
Validation loss: 2.4692481625972613

Epoch: 5| Step: 10
Training loss: 2.2495820399016453
Validation loss: 2.4537355629694217

Epoch: 205| Step: 0
Training loss: 2.275920128505271
Validation loss: 2.464911067894882

Epoch: 5| Step: 1
Training loss: 1.992515387385936
Validation loss: 2.4744602331984806

Epoch: 5| Step: 2
Training loss: 2.4632716175378584
Validation loss: 2.479445993831725

Epoch: 5| Step: 3
Training loss: 2.2964204091823492
Validation loss: 2.48865772398151

Epoch: 5| Step: 4
Training loss: 3.1143608566658147
Validation loss: 2.490562023827719

Epoch: 5| Step: 5
Training loss: 2.0488440673906076
Validation loss: 2.4832950700035177

Epoch: 5| Step: 6
Training loss: 2.6331196501102783
Validation loss: 2.5095847679013774

Epoch: 5| Step: 7
Training loss: 2.487066574291588
Validation loss: 2.48841039090306

Epoch: 5| Step: 8
Training loss: 2.2663556203410713
Validation loss: 2.4833415998459056

Epoch: 5| Step: 9
Training loss: 2.756743918385452
Validation loss: 2.5005420220293444

Epoch: 5| Step: 10
Training loss: 1.8940914941458875
Validation loss: 2.509775057711455

Epoch: 206| Step: 0
Training loss: 2.6265188999857627
Validation loss: 2.4986296825705296

Epoch: 5| Step: 1
Training loss: 2.363488457793799
Validation loss: 2.4827352374929803

Epoch: 5| Step: 2
Training loss: 2.2736327621660504
Validation loss: 2.4869970259916157

Epoch: 5| Step: 3
Training loss: 2.2068560235334695
Validation loss: 2.4816176791156135

Epoch: 5| Step: 4
Training loss: 2.2723249616871146
Validation loss: 2.498608920997039

Epoch: 5| Step: 5
Training loss: 2.6442573439323076
Validation loss: 2.4750637038787144

Epoch: 5| Step: 6
Training loss: 2.0407327994458586
Validation loss: 2.490571181868588

Epoch: 5| Step: 7
Training loss: 2.023536591331202
Validation loss: 2.459193517349084

Epoch: 5| Step: 8
Training loss: 2.78540251981975
Validation loss: 2.4622547250295783

Epoch: 5| Step: 9
Training loss: 2.1595447081636485
Validation loss: 2.46905384761415

Epoch: 5| Step: 10
Training loss: 3.103764057719074
Validation loss: 2.4849305819292637

Epoch: 207| Step: 0
Training loss: 3.026943017832203
Validation loss: 2.476077311901068

Epoch: 5| Step: 1
Training loss: 2.122704781043134
Validation loss: 2.47710784449023

Epoch: 5| Step: 2
Training loss: 2.2240535051647137
Validation loss: 2.477282683004667

Epoch: 5| Step: 3
Training loss: 2.0373692991309307
Validation loss: 2.4742603045603224

Epoch: 5| Step: 4
Training loss: 2.7137376906064388
Validation loss: 2.46961800749668

Epoch: 5| Step: 5
Training loss: 2.7786492178174433
Validation loss: 2.470642196485726

Epoch: 5| Step: 6
Training loss: 2.1662938702032277
Validation loss: 2.4671194232504807

Epoch: 5| Step: 7
Training loss: 2.1900059106825402
Validation loss: 2.4787149963195967

Epoch: 5| Step: 8
Training loss: 2.381287333678897
Validation loss: 2.4676220727553444

Epoch: 5| Step: 9
Training loss: 2.3252885167756667
Validation loss: 2.4775642989590194

Epoch: 5| Step: 10
Training loss: 2.3050569917000647
Validation loss: 2.4965501352292483

Epoch: 208| Step: 0
Training loss: 2.710262423404243
Validation loss: 2.4741312115319727

Epoch: 5| Step: 1
Training loss: 2.6946898694908876
Validation loss: 2.4678500835281754

Epoch: 5| Step: 2
Training loss: 2.0700571730453516
Validation loss: 2.475206223132414

Epoch: 5| Step: 3
Training loss: 2.867019523668557
Validation loss: 2.481987728018123

Epoch: 5| Step: 4
Training loss: 2.1551879257433155
Validation loss: 2.4960829339967847

Epoch: 5| Step: 5
Training loss: 2.2923934708765383
Validation loss: 2.494398409549623

Epoch: 5| Step: 6
Training loss: 2.265165821767528
Validation loss: 2.487534180190585

Epoch: 5| Step: 7
Training loss: 2.4351135700537787
Validation loss: 2.474336867763252

Epoch: 5| Step: 8
Training loss: 2.0510542043054403
Validation loss: 2.464298080567105

Epoch: 5| Step: 9
Training loss: 2.452715602656502
Validation loss: 2.486702125571093

Epoch: 5| Step: 10
Training loss: 2.3445868460429646
Validation loss: 2.473449346560513

Epoch: 209| Step: 0
Training loss: 2.4583903376117853
Validation loss: 2.4603344927427044

Epoch: 5| Step: 1
Training loss: 2.3314339081986306
Validation loss: 2.4762556251399497

Epoch: 5| Step: 2
Training loss: 2.341474012909625
Validation loss: 2.496474813964529

Epoch: 5| Step: 3
Training loss: 2.5824118375547065
Validation loss: 2.4719764246538727

Epoch: 5| Step: 4
Training loss: 2.4784087510510404
Validation loss: 2.4975858743059183

Epoch: 5| Step: 5
Training loss: 2.349373238764065
Validation loss: 2.479653482750261

Epoch: 5| Step: 6
Training loss: 2.3262973242306444
Validation loss: 2.457686185952551

Epoch: 5| Step: 7
Training loss: 2.366499999271395
Validation loss: 2.4780436642235286

Epoch: 5| Step: 8
Training loss: 2.5855799309645677
Validation loss: 2.4875338055692238

Epoch: 5| Step: 9
Training loss: 2.1887791844398796
Validation loss: 2.4717527783156763

Epoch: 5| Step: 10
Training loss: 2.2581558758408877
Validation loss: 2.490285761876248

Epoch: 210| Step: 0
Training loss: 2.8723967002574526
Validation loss: 2.4668402148336668

Epoch: 5| Step: 1
Training loss: 2.233917082700061
Validation loss: 2.4849184494022256

Epoch: 5| Step: 2
Training loss: 2.7344647202077397
Validation loss: 2.4820693690142477

Epoch: 5| Step: 3
Training loss: 1.9724230335724915
Validation loss: 2.4836894868512625

Epoch: 5| Step: 4
Training loss: 2.409911899143473
Validation loss: 2.492111446117537

Epoch: 5| Step: 5
Training loss: 2.5770322391149842
Validation loss: 2.467292544427001

Epoch: 5| Step: 6
Training loss: 2.429845577874881
Validation loss: 2.503544577294065

Epoch: 5| Step: 7
Training loss: 1.958402720365162
Validation loss: 2.489039086950778

Epoch: 5| Step: 8
Training loss: 2.36273509879212
Validation loss: 2.4666785411027856

Epoch: 5| Step: 9
Training loss: 2.618778940555283
Validation loss: 2.485178764736871

Epoch: 5| Step: 10
Training loss: 1.9864378892246273
Validation loss: 2.4886831299445933

Epoch: 211| Step: 0
Training loss: 1.9619359973613588
Validation loss: 2.4530706222607264

Epoch: 5| Step: 1
Training loss: 2.4253435461777544
Validation loss: 2.494569317461604

Epoch: 5| Step: 2
Training loss: 2.1511782278781375
Validation loss: 2.494096420232596

Epoch: 5| Step: 3
Training loss: 2.205358360735864
Validation loss: 2.478475392784206

Epoch: 5| Step: 4
Training loss: 2.1759115347901132
Validation loss: 2.4539795628018153

Epoch: 5| Step: 5
Training loss: 2.510805144794827
Validation loss: 2.466376463102251

Epoch: 5| Step: 6
Training loss: 2.5214503350239097
Validation loss: 2.468636915805707

Epoch: 5| Step: 7
Training loss: 3.0826092118889568
Validation loss: 2.471507482511215

Epoch: 5| Step: 8
Training loss: 2.5115150380915616
Validation loss: 2.487787246959169

Epoch: 5| Step: 9
Training loss: 1.6913021487297086
Validation loss: 2.4719037832979014

Epoch: 5| Step: 10
Training loss: 2.818174952277716
Validation loss: 2.519644326065894

Epoch: 212| Step: 0
Training loss: 2.554512974555113
Validation loss: 2.487362504427059

Epoch: 5| Step: 1
Training loss: 2.639612669707725
Validation loss: 2.472739088629776

Epoch: 5| Step: 2
Training loss: 2.389524019320566
Validation loss: 2.494735793875029

Epoch: 5| Step: 3
Training loss: 2.0931586953632086
Validation loss: 2.465378262162145

Epoch: 5| Step: 4
Training loss: 2.4392644902422806
Validation loss: 2.4682617867810595

Epoch: 5| Step: 5
Training loss: 2.7361275424453138
Validation loss: 2.4578210179711886

Epoch: 5| Step: 6
Training loss: 2.627960896669939
Validation loss: 2.4896919685710146

Epoch: 5| Step: 7
Training loss: 2.3088342634932717
Validation loss: 2.4620422727420883

Epoch: 5| Step: 8
Training loss: 2.08458674554916
Validation loss: 2.4668949489531555

Epoch: 5| Step: 9
Training loss: 2.2509671357923544
Validation loss: 2.4809231311662594

Epoch: 5| Step: 10
Training loss: 1.9780243304099485
Validation loss: 2.4721923034140056

Epoch: 213| Step: 0
Training loss: 2.59987477954684
Validation loss: 2.475409070454539

Epoch: 5| Step: 1
Training loss: 2.2800368127026047
Validation loss: 2.5184540740686208

Epoch: 5| Step: 2
Training loss: 1.5278864137962398
Validation loss: 2.4534016616407306

Epoch: 5| Step: 3
Training loss: 2.3939953152922238
Validation loss: 2.484680461414719

Epoch: 5| Step: 4
Training loss: 2.6542376805680967
Validation loss: 2.4942627992181716

Epoch: 5| Step: 5
Training loss: 2.8785923614606275
Validation loss: 2.470116097786229

Epoch: 5| Step: 6
Training loss: 2.770211334016509
Validation loss: 2.462506774081823

Epoch: 5| Step: 7
Training loss: 2.1571395808134577
Validation loss: 2.496028717702091

Epoch: 5| Step: 8
Training loss: 2.5611295989944476
Validation loss: 2.48359495498403

Epoch: 5| Step: 9
Training loss: 1.9509618758793426
Validation loss: 2.4689689685541736

Epoch: 5| Step: 10
Training loss: 2.1605767225411867
Validation loss: 2.468610706400261

Epoch: 214| Step: 0
Training loss: 2.940215458662745
Validation loss: 2.48575537646421

Epoch: 5| Step: 1
Training loss: 2.1092011556690564
Validation loss: 2.4810556608971455

Epoch: 5| Step: 2
Training loss: 2.39333992177044
Validation loss: 2.471662469822892

Epoch: 5| Step: 3
Training loss: 2.7051340916762285
Validation loss: 2.4672133311688653

Epoch: 5| Step: 4
Training loss: 2.268673654327856
Validation loss: 2.464808214288113

Epoch: 5| Step: 5
Training loss: 2.4240349766628198
Validation loss: 2.461719602867035

Epoch: 5| Step: 6
Training loss: 2.4639360839307094
Validation loss: 2.458356274942172

Epoch: 5| Step: 7
Training loss: 2.289658400531723
Validation loss: 2.4651997315029286

Epoch: 5| Step: 8
Training loss: 2.13843931562343
Validation loss: 2.460966954833849

Epoch: 5| Step: 9
Training loss: 2.00108069786068
Validation loss: 2.484810442540409

Epoch: 5| Step: 10
Training loss: 2.430426874538806
Validation loss: 2.4918008265763865

Epoch: 215| Step: 0
Training loss: 2.5653692789447518
Validation loss: 2.4746415750557405

Epoch: 5| Step: 1
Training loss: 2.2600621825921845
Validation loss: 2.467441581276156

Epoch: 5| Step: 2
Training loss: 2.3720244791527145
Validation loss: 2.486042228178777

Epoch: 5| Step: 3
Training loss: 1.739067122872267
Validation loss: 2.492966359870173

Epoch: 5| Step: 4
Training loss: 2.393348688095836
Validation loss: 2.4564122897225116

Epoch: 5| Step: 5
Training loss: 2.356580177871126
Validation loss: 2.48759250552343

Epoch: 5| Step: 6
Training loss: 2.5384602463325465
Validation loss: 2.464003065071589

Epoch: 5| Step: 7
Training loss: 2.3719034587776187
Validation loss: 2.47656566759029

Epoch: 5| Step: 8
Training loss: 2.734292862884657
Validation loss: 2.4893623350414544

Epoch: 5| Step: 9
Training loss: 2.12703753007026
Validation loss: 2.495466964308994

Epoch: 5| Step: 10
Training loss: 2.657370925453034
Validation loss: 2.493976315317413

Epoch: 216| Step: 0
Training loss: 2.2950656537407528
Validation loss: 2.490993874706774

Epoch: 5| Step: 1
Training loss: 2.2059310466403383
Validation loss: 2.5033004386478237

Epoch: 5| Step: 2
Training loss: 2.216352752696092
Validation loss: 2.4869313218322313

Epoch: 5| Step: 3
Training loss: 2.6476359997220023
Validation loss: 2.504547657957109

Epoch: 5| Step: 4
Training loss: 1.9551162944653786
Validation loss: 2.4806521969850444

Epoch: 5| Step: 5
Training loss: 2.0646532551780776
Validation loss: 2.489754009398952

Epoch: 5| Step: 6
Training loss: 2.4484624592398063
Validation loss: 2.48744416792843

Epoch: 5| Step: 7
Training loss: 2.5379733052935305
Validation loss: 2.483569419552843

Epoch: 5| Step: 8
Training loss: 2.884629088149166
Validation loss: 2.4862366754977474

Epoch: 5| Step: 9
Training loss: 2.1238437199471702
Validation loss: 2.4784048358827024

Epoch: 5| Step: 10
Training loss: 2.647330083536526
Validation loss: 2.454520353362792

Epoch: 217| Step: 0
Training loss: 2.2683729672899227
Validation loss: 2.484090914574793

Epoch: 5| Step: 1
Training loss: 2.255901544350177
Validation loss: 2.4886754503930284

Epoch: 5| Step: 2
Training loss: 1.8609726397241853
Validation loss: 2.4925910406203613

Epoch: 5| Step: 3
Training loss: 2.468072822504198
Validation loss: 2.4892936510892474

Epoch: 5| Step: 4
Training loss: 2.946026379106542
Validation loss: 2.511701298491998

Epoch: 5| Step: 5
Training loss: 2.5801725637404953
Validation loss: 2.494445497943317

Epoch: 5| Step: 6
Training loss: 1.8958888901615227
Validation loss: 2.479799103789516

Epoch: 5| Step: 7
Training loss: 2.165597529601769
Validation loss: 2.4874524706894556

Epoch: 5| Step: 8
Training loss: 2.0741029983665835
Validation loss: 2.483905780172991

Epoch: 5| Step: 9
Training loss: 2.8930981752606977
Validation loss: 2.503812201896922

Epoch: 5| Step: 10
Training loss: 2.5672010703491472
Validation loss: 2.513859851010514

Epoch: 218| Step: 0
Training loss: 2.152462604654217
Validation loss: 2.5022913696867315

Epoch: 5| Step: 1
Training loss: 2.2813121839435153
Validation loss: 2.483709029236676

Epoch: 5| Step: 2
Training loss: 2.636654097152952
Validation loss: 2.4702883354261305

Epoch: 5| Step: 3
Training loss: 2.6117327425870305
Validation loss: 2.4615799718425038

Epoch: 5| Step: 4
Training loss: 2.0883189119823893
Validation loss: 2.4943250994443185

Epoch: 5| Step: 5
Training loss: 2.455628884871418
Validation loss: 2.47493015201933

Epoch: 5| Step: 6
Training loss: 2.058870763797836
Validation loss: 2.5010249067414145

Epoch: 5| Step: 7
Training loss: 2.050164527803672
Validation loss: 2.4800904065301417

Epoch: 5| Step: 8
Training loss: 2.6908007024978167
Validation loss: 2.466772796292759

Epoch: 5| Step: 9
Training loss: 3.0387019161686704
Validation loss: 2.4737704999220282

Epoch: 5| Step: 10
Training loss: 1.9959026327293468
Validation loss: 2.48753209374985

Epoch: 219| Step: 0
Training loss: 2.8435989172070166
Validation loss: 2.490932922766222

Epoch: 5| Step: 1
Training loss: 2.04787507996451
Validation loss: 2.469403124047611

Epoch: 5| Step: 2
Training loss: 1.9679701335276685
Validation loss: 2.478713253587578

Epoch: 5| Step: 3
Training loss: 2.9780855862710314
Validation loss: 2.4793469387688423

Epoch: 5| Step: 4
Training loss: 2.306635766989222
Validation loss: 2.4620439970777963

Epoch: 5| Step: 5
Training loss: 2.1768590151440597
Validation loss: 2.5068184479090636

Epoch: 5| Step: 6
Training loss: 2.677786105245443
Validation loss: 2.4798426505777065

Epoch: 5| Step: 7
Training loss: 2.4937100438332527
Validation loss: 2.4765938993361516

Epoch: 5| Step: 8
Training loss: 1.9936965911312592
Validation loss: 2.4595494447335318

Epoch: 5| Step: 9
Training loss: 2.161518571006596
Validation loss: 2.48494435062501

Epoch: 5| Step: 10
Training loss: 2.2292994759709726
Validation loss: 2.463906369095624

Epoch: 220| Step: 0
Training loss: 2.8518939100011007
Validation loss: 2.477764022045609

Epoch: 5| Step: 1
Training loss: 1.9288796034834568
Validation loss: 2.496159042497353

Epoch: 5| Step: 2
Training loss: 2.3787041940052416
Validation loss: 2.475978016457894

Epoch: 5| Step: 3
Training loss: 2.536750468533654
Validation loss: 2.5056024882220926

Epoch: 5| Step: 4
Training loss: 2.4539911964090706
Validation loss: 2.5067979167612235

Epoch: 5| Step: 5
Training loss: 2.445510807466619
Validation loss: 2.496649036570578

Epoch: 5| Step: 6
Training loss: 1.959140516544976
Validation loss: 2.490316850236264

Epoch: 5| Step: 7
Training loss: 2.093053673491138
Validation loss: 2.4955425199096135

Epoch: 5| Step: 8
Training loss: 2.0082047966098857
Validation loss: 2.4967538200720343

Epoch: 5| Step: 9
Training loss: 2.324806869853214
Validation loss: 2.4703064141814033

Epoch: 5| Step: 10
Training loss: 2.9263206565308026
Validation loss: 2.4784751052315674

Epoch: 221| Step: 0
Training loss: 2.2077732695531527
Validation loss: 2.5055192678489955

Epoch: 5| Step: 1
Training loss: 2.046768535145682
Validation loss: 2.4817684738144004

Epoch: 5| Step: 2
Training loss: 2.192357147678287
Validation loss: 2.4641285021988244

Epoch: 5| Step: 3
Training loss: 2.2731022014004285
Validation loss: 2.4847836371529737

Epoch: 5| Step: 4
Training loss: 2.1730878997172343
Validation loss: 2.469021029565991

Epoch: 5| Step: 5
Training loss: 2.3405907255344767
Validation loss: 2.4745767456247907

Epoch: 5| Step: 6
Training loss: 1.9807580138466496
Validation loss: 2.4811037960689553

Epoch: 5| Step: 7
Training loss: 2.801403790481954
Validation loss: 2.4656048511293385

Epoch: 5| Step: 8
Training loss: 2.6450915961459547
Validation loss: 2.497540929873768

Epoch: 5| Step: 9
Training loss: 3.0921973754046608
Validation loss: 2.4927554066463284

Epoch: 5| Step: 10
Training loss: 1.8757737152825296
Validation loss: 2.5039033770570733

Epoch: 222| Step: 0
Training loss: 2.348153110931898
Validation loss: 2.4779910447817133

Epoch: 5| Step: 1
Training loss: 2.1807632252909745
Validation loss: 2.4874339368192238

Epoch: 5| Step: 2
Training loss: 2.109104845619794
Validation loss: 2.4786278980381273

Epoch: 5| Step: 3
Training loss: 2.9403701720458986
Validation loss: 2.482724932251304

Epoch: 5| Step: 4
Training loss: 1.840427508550757
Validation loss: 2.492040631306381

Epoch: 5| Step: 5
Training loss: 2.2822084895452033
Validation loss: 2.4934519567836593

Epoch: 5| Step: 6
Training loss: 1.9205079217462848
Validation loss: 2.4958415609096725

Epoch: 5| Step: 7
Training loss: 2.523242671283528
Validation loss: 2.4653441307023405

Epoch: 5| Step: 8
Training loss: 2.45473082641983
Validation loss: 2.483938160053931

Epoch: 5| Step: 9
Training loss: 2.8382706725692324
Validation loss: 2.482847268611718

Epoch: 5| Step: 10
Training loss: 2.4397439653486344
Validation loss: 2.498467830069656

Epoch: 223| Step: 0
Training loss: 2.482904922944952
Validation loss: 2.4977961065290155

Epoch: 5| Step: 1
Training loss: 2.148412863850226
Validation loss: 2.506084611658758

Epoch: 5| Step: 2
Training loss: 2.4660371771900453
Validation loss: 2.470528907312863

Epoch: 5| Step: 3
Training loss: 2.2810834013840493
Validation loss: 2.4981800910275336

Epoch: 5| Step: 4
Training loss: 1.8555426331918459
Validation loss: 2.4985091440189025

Epoch: 5| Step: 5
Training loss: 2.565642407226984
Validation loss: 2.500568788694689

Epoch: 5| Step: 6
Training loss: 2.6427566682281967
Validation loss: 2.505710263885126

Epoch: 5| Step: 7
Training loss: 2.304153328754067
Validation loss: 2.48158010878833

Epoch: 5| Step: 8
Training loss: 2.363736599017262
Validation loss: 2.5140786809108717

Epoch: 5| Step: 9
Training loss: 2.624473882358079
Validation loss: 2.5018427958343534

Epoch: 5| Step: 10
Training loss: 2.065314943990648
Validation loss: 2.499533148213845

Epoch: 224| Step: 0
Training loss: 1.9963387116921159
Validation loss: 2.4963849859516083

Epoch: 5| Step: 1
Training loss: 2.536484286719254
Validation loss: 2.5023014611611485

Epoch: 5| Step: 2
Training loss: 2.86372120223766
Validation loss: 2.4832632856723036

Epoch: 5| Step: 3
Training loss: 2.1516600698206934
Validation loss: 2.504057083064951

Epoch: 5| Step: 4
Training loss: 1.8365681965018945
Validation loss: 2.4996416040114826

Epoch: 5| Step: 5
Training loss: 2.435741866442626
Validation loss: 2.495228107887005

Epoch: 5| Step: 6
Training loss: 2.361017627984709
Validation loss: 2.4679075137685142

Epoch: 5| Step: 7
Training loss: 2.3812483860120537
Validation loss: 2.508447631517378

Epoch: 5| Step: 8
Training loss: 2.245212123169511
Validation loss: 2.4917113259769312

Epoch: 5| Step: 9
Training loss: 2.4947923781062777
Validation loss: 2.4836624825372056

Epoch: 5| Step: 10
Training loss: 2.5890790866281366
Validation loss: 2.4849763466086534

Epoch: 225| Step: 0
Training loss: 2.065876710822809
Validation loss: 2.4564793184284506

Epoch: 5| Step: 1
Training loss: 2.149669169322717
Validation loss: 2.465653426221728

Epoch: 5| Step: 2
Training loss: 2.269489228849364
Validation loss: 2.4714245465848603

Epoch: 5| Step: 3
Training loss: 2.8634652654993373
Validation loss: 2.482835042268241

Epoch: 5| Step: 4
Training loss: 2.5663289562180913
Validation loss: 2.4966290964285807

Epoch: 5| Step: 5
Training loss: 2.256336191491242
Validation loss: 2.4632798186044544

Epoch: 5| Step: 6
Training loss: 2.3218084066501183
Validation loss: 2.493401354879159

Epoch: 5| Step: 7
Training loss: 2.4016359356465578
Validation loss: 2.4809616724352357

Epoch: 5| Step: 8
Training loss: 2.3477612565560504
Validation loss: 2.490380156864644

Epoch: 5| Step: 9
Training loss: 2.1535759529729592
Validation loss: 2.4650050261432797

Epoch: 5| Step: 10
Training loss: 2.4273225614557896
Validation loss: 2.4605901417977263

Epoch: 226| Step: 0
Training loss: 2.267508099088326
Validation loss: 2.5078867994343823

Epoch: 5| Step: 1
Training loss: 2.933797572708764
Validation loss: 2.4708036938660176

Epoch: 5| Step: 2
Training loss: 2.7504041548164415
Validation loss: 2.482460250487517

Epoch: 5| Step: 3
Training loss: 2.1078369184455337
Validation loss: 2.4787143312890656

Epoch: 5| Step: 4
Training loss: 2.2328120814243615
Validation loss: 2.5049728239535245

Epoch: 5| Step: 5
Training loss: 2.4059587773052225
Validation loss: 2.4777798667615767

Epoch: 5| Step: 6
Training loss: 2.319101036624431
Validation loss: 2.505368513604006

Epoch: 5| Step: 7
Training loss: 1.6505563144916775
Validation loss: 2.477819476556263

Epoch: 5| Step: 8
Training loss: 2.5654125873763927
Validation loss: 2.4834889871124233

Epoch: 5| Step: 9
Training loss: 2.132225207280931
Validation loss: 2.531351068976241

Epoch: 5| Step: 10
Training loss: 2.352851793049999
Validation loss: 2.4857854643803288

Epoch: 227| Step: 0
Training loss: 2.2552709299141145
Validation loss: 2.4734353594423846

Epoch: 5| Step: 1
Training loss: 2.357201794304481
Validation loss: 2.487534706309348

Epoch: 5| Step: 2
Training loss: 2.9184233325418716
Validation loss: 2.497186131364291

Epoch: 5| Step: 3
Training loss: 2.106177612328569
Validation loss: 2.4788470039327812

Epoch: 5| Step: 4
Training loss: 2.0289027810755624
Validation loss: 2.483265023146419

Epoch: 5| Step: 5
Training loss: 1.9847943921586153
Validation loss: 2.4880316651915027

Epoch: 5| Step: 6
Training loss: 2.429299571969228
Validation loss: 2.4894232543797425

Epoch: 5| Step: 7
Training loss: 2.6396181794244775
Validation loss: 2.4745017335185993

Epoch: 5| Step: 8
Training loss: 2.580410031405044
Validation loss: 2.4889729330282577

Epoch: 5| Step: 9
Training loss: 1.8079121502024735
Validation loss: 2.47469759109012

Epoch: 5| Step: 10
Training loss: 2.7114462251474682
Validation loss: 2.4753007097677044

Epoch: 228| Step: 0
Training loss: 2.488599532295446
Validation loss: 2.466273845760672

Epoch: 5| Step: 1
Training loss: 1.6447099239756682
Validation loss: 2.483199110806388

Epoch: 5| Step: 2
Training loss: 2.1056650951323874
Validation loss: 2.4758690128062604

Epoch: 5| Step: 3
Training loss: 2.674173537598466
Validation loss: 2.4899800892729953

Epoch: 5| Step: 4
Training loss: 1.957330798830025
Validation loss: 2.5194688333136486

Epoch: 5| Step: 5
Training loss: 3.0404971935859604
Validation loss: 2.5052957741889874

Epoch: 5| Step: 6
Training loss: 2.340084719469193
Validation loss: 2.507151309049241

Epoch: 5| Step: 7
Training loss: 2.1087403649617102
Validation loss: 2.476927828948943

Epoch: 5| Step: 8
Training loss: 2.2513922516456932
Validation loss: 2.4808194497215537

Epoch: 5| Step: 9
Training loss: 2.6222560710212983
Validation loss: 2.4984150025454173

Epoch: 5| Step: 10
Training loss: 2.542007100387249
Validation loss: 2.482058796593605

Epoch: 229| Step: 0
Training loss: 2.0778659286204624
Validation loss: 2.4694810799634936

Epoch: 5| Step: 1
Training loss: 1.7155989459062337
Validation loss: 2.4706110971394994

Epoch: 5| Step: 2
Training loss: 2.5344489334931124
Validation loss: 2.4732861406272786

Epoch: 5| Step: 3
Training loss: 1.9818576855536232
Validation loss: 2.4975024658468183

Epoch: 5| Step: 4
Training loss: 2.8008899534616827
Validation loss: 2.486109354123916

Epoch: 5| Step: 5
Training loss: 2.191502206914836
Validation loss: 2.486126509902692

Epoch: 5| Step: 6
Training loss: 2.912191409024381
Validation loss: 2.4743302512708896

Epoch: 5| Step: 7
Training loss: 2.027736383244948
Validation loss: 2.5002591496146915

Epoch: 5| Step: 8
Training loss: 2.4883336611128137
Validation loss: 2.4935210320309014

Epoch: 5| Step: 9
Training loss: 2.741557426556648
Validation loss: 2.490086699258539

Epoch: 5| Step: 10
Training loss: 1.9989357739464861
Validation loss: 2.4898531220051394

Epoch: 230| Step: 0
Training loss: 2.4814103389408686
Validation loss: 2.4851548187424735

Epoch: 5| Step: 1
Training loss: 2.218599985988269
Validation loss: 2.4891462501434436

Epoch: 5| Step: 2
Training loss: 1.6623734332787994
Validation loss: 2.479084050915027

Epoch: 5| Step: 3
Training loss: 1.97497754386014
Validation loss: 2.4949524992649685

Epoch: 5| Step: 4
Training loss: 2.911785146060129
Validation loss: 2.507564176252803

Epoch: 5| Step: 5
Training loss: 2.567056466100129
Validation loss: 2.5010099304076925

Epoch: 5| Step: 6
Training loss: 2.4240468777193356
Validation loss: 2.5102081239440936

Epoch: 5| Step: 7
Training loss: 1.750263058100836
Validation loss: 2.493153130445595

Epoch: 5| Step: 8
Training loss: 2.9561032410015655
Validation loss: 2.4999695037704526

Epoch: 5| Step: 9
Training loss: 2.261596254711201
Validation loss: 2.501209266001577

Epoch: 5| Step: 10
Training loss: 2.288338296009229
Validation loss: 2.5007857400502025

Epoch: 231| Step: 0
Training loss: 2.3870770424282575
Validation loss: 2.497122000157725

Epoch: 5| Step: 1
Training loss: 2.4151647329122854
Validation loss: 2.4843589280172873

Epoch: 5| Step: 2
Training loss: 1.9688896856253748
Validation loss: 2.492192561055446

Epoch: 5| Step: 3
Training loss: 1.9449220441675275
Validation loss: 2.4413259164370635

Epoch: 5| Step: 4
Training loss: 2.429878742504981
Validation loss: 2.460950975825559

Epoch: 5| Step: 5
Training loss: 2.4742520020842895
Validation loss: 2.475140311625825

Epoch: 5| Step: 6
Training loss: 2.340180998428907
Validation loss: 2.495399873825937

Epoch: 5| Step: 7
Training loss: 1.9583317438754944
Validation loss: 2.4858728826956265

Epoch: 5| Step: 8
Training loss: 2.895578460088808
Validation loss: 2.4607636921139497

Epoch: 5| Step: 9
Training loss: 2.1687897648312675
Validation loss: 2.4749889209458327

Epoch: 5| Step: 10
Training loss: 2.682908149662669
Validation loss: 2.49179989548463

Epoch: 232| Step: 0
Training loss: 2.415502618648094
Validation loss: 2.465598449310378

Epoch: 5| Step: 1
Training loss: 2.006562196202501
Validation loss: 2.4935534906188512

Epoch: 5| Step: 2
Training loss: 1.6771020907978065
Validation loss: 2.496688550830212

Epoch: 5| Step: 3
Training loss: 2.220692174074252
Validation loss: 2.5042031999155245

Epoch: 5| Step: 4
Training loss: 2.1745380568762696
Validation loss: 2.49665451985067

Epoch: 5| Step: 5
Training loss: 2.529327604482729
Validation loss: 2.4795344555499073

Epoch: 5| Step: 6
Training loss: 2.4724312871289214
Validation loss: 2.5112882580221996

Epoch: 5| Step: 7
Training loss: 2.52380840776184
Validation loss: 2.4741885799048378

Epoch: 5| Step: 8
Training loss: 2.415090200188976
Validation loss: 2.5052337314322064

Epoch: 5| Step: 9
Training loss: 2.895897752516443
Validation loss: 2.4851559720507277

Epoch: 5| Step: 10
Training loss: 2.1198313066554983
Validation loss: 2.486940704565418

Epoch: 233| Step: 0
Training loss: 2.3167484538874925
Validation loss: 2.5035846493371254

Epoch: 5| Step: 1
Training loss: 2.553452499470711
Validation loss: 2.484398242534467

Epoch: 5| Step: 2
Training loss: 2.0512876048991022
Validation loss: 2.479148152372643

Epoch: 5| Step: 3
Training loss: 2.0066697249067116
Validation loss: 2.4948810858771324

Epoch: 5| Step: 4
Training loss: 2.076978553672269
Validation loss: 2.489423734272593

Epoch: 5| Step: 5
Training loss: 2.4616368828867667
Validation loss: 2.4959974159781355

Epoch: 5| Step: 6
Training loss: 2.1136578067501004
Validation loss: 2.4825043609111255

Epoch: 5| Step: 7
Training loss: 2.5068740752662353
Validation loss: 2.510294193396185

Epoch: 5| Step: 8
Training loss: 2.6868557712304333
Validation loss: 2.4975163710427757

Epoch: 5| Step: 9
Training loss: 2.4420080313023274
Validation loss: 2.462732839229183

Epoch: 5| Step: 10
Training loss: 2.418957238597215
Validation loss: 2.5060632622567014

Epoch: 234| Step: 0
Training loss: 2.195433806736601
Validation loss: 2.4970735033552853

Epoch: 5| Step: 1
Training loss: 2.1353634959478587
Validation loss: 2.484600211458706

Epoch: 5| Step: 2
Training loss: 2.226179444674521
Validation loss: 2.507936920983562

Epoch: 5| Step: 3
Training loss: 2.4467248707361056
Validation loss: 2.5090652560505275

Epoch: 5| Step: 4
Training loss: 2.109756322231735
Validation loss: 2.493151146393146

Epoch: 5| Step: 5
Training loss: 1.8927593951422272
Validation loss: 2.497313799591789

Epoch: 5| Step: 6
Training loss: 3.0335099752226613
Validation loss: 2.489429368372506

Epoch: 5| Step: 7
Training loss: 2.2446469100220163
Validation loss: 2.4947515647107905

Epoch: 5| Step: 8
Training loss: 2.6933005964851646
Validation loss: 2.47310992491846

Epoch: 5| Step: 9
Training loss: 2.3812128419555125
Validation loss: 2.4691334129830484

Epoch: 5| Step: 10
Training loss: 2.432987143655434
Validation loss: 2.4773711598807533

Epoch: 235| Step: 0
Training loss: 2.0081474529475023
Validation loss: 2.4608275998875997

Epoch: 5| Step: 1
Training loss: 2.830452482896431
Validation loss: 2.495732876288508

Epoch: 5| Step: 2
Training loss: 2.6487469787724445
Validation loss: 2.4533966490914674

Epoch: 5| Step: 3
Training loss: 1.616801045263331
Validation loss: 2.508999242386331

Epoch: 5| Step: 4
Training loss: 2.1100894424500494
Validation loss: 2.480842371119661

Epoch: 5| Step: 5
Training loss: 2.4293557090768667
Validation loss: 2.4957243103734013

Epoch: 5| Step: 6
Training loss: 2.4391765208599705
Validation loss: 2.467023756611219

Epoch: 5| Step: 7
Training loss: 2.6499777846934673
Validation loss: 2.4792619054226255

Epoch: 5| Step: 8
Training loss: 2.6371103405685816
Validation loss: 2.4963322206407677

Epoch: 5| Step: 9
Training loss: 2.121462289396352
Validation loss: 2.477683297676713

Epoch: 5| Step: 10
Training loss: 1.7339539575282854
Validation loss: 2.487126378914139

Epoch: 236| Step: 0
Training loss: 2.2212000469543685
Validation loss: 2.4660913209923687

Epoch: 5| Step: 1
Training loss: 2.558041019286782
Validation loss: 2.472526248926223

Epoch: 5| Step: 2
Training loss: 2.622341717360856
Validation loss: 2.499620025225265

Epoch: 5| Step: 3
Training loss: 2.577879368033503
Validation loss: 2.507641221361274

Epoch: 5| Step: 4
Training loss: 2.0532371130399194
Validation loss: 2.4962286384978105

Epoch: 5| Step: 5
Training loss: 2.4272984967342883
Validation loss: 2.509018828264592

Epoch: 5| Step: 6
Training loss: 2.28429157819688
Validation loss: 2.4846546853829303

Epoch: 5| Step: 7
Training loss: 2.2681128161458672
Validation loss: 2.4802751352693093

Epoch: 5| Step: 8
Training loss: 1.8949999671875641
Validation loss: 2.499426087779646

Epoch: 5| Step: 9
Training loss: 2.325860579998628
Validation loss: 2.509744517334674

Epoch: 5| Step: 10
Training loss: 2.4476202169786268
Validation loss: 2.503681144010625

Epoch: 237| Step: 0
Training loss: 2.0862927080814777
Validation loss: 2.492201600980128

Epoch: 5| Step: 1
Training loss: 1.9793788461554462
Validation loss: 2.494368770856947

Epoch: 5| Step: 2
Training loss: 2.1642481314576254
Validation loss: 2.4899850106738453

Epoch: 5| Step: 3
Training loss: 2.01792256313137
Validation loss: 2.5039707132217917

Epoch: 5| Step: 4
Training loss: 2.3252046433493483
Validation loss: 2.480665035554369

Epoch: 5| Step: 5
Training loss: 2.0537982365242446
Validation loss: 2.498741181632734

Epoch: 5| Step: 6
Training loss: 2.8661952991481385
Validation loss: 2.517495431867876

Epoch: 5| Step: 7
Training loss: 2.695645342192212
Validation loss: 2.4930144532542973

Epoch: 5| Step: 8
Training loss: 2.375079705758022
Validation loss: 2.497090855869502

Epoch: 5| Step: 9
Training loss: 2.567737715877356
Validation loss: 2.489184429269933

Epoch: 5| Step: 10
Training loss: 2.463666486635934
Validation loss: 2.4843025983797675

Epoch: 238| Step: 0
Training loss: 2.3006721302315056
Validation loss: 2.486340739384907

Epoch: 5| Step: 1
Training loss: 2.866918067849807
Validation loss: 2.4901667229866753

Epoch: 5| Step: 2
Training loss: 2.699318206323904
Validation loss: 2.488345294826908

Epoch: 5| Step: 3
Training loss: 1.895793124879906
Validation loss: 2.4930668378720475

Epoch: 5| Step: 4
Training loss: 1.9635722784460685
Validation loss: 2.4912086920593173

Epoch: 5| Step: 5
Training loss: 2.575553860402786
Validation loss: 2.48096160216911

Epoch: 5| Step: 6
Training loss: 2.245801823760058
Validation loss: 2.495212091426625

Epoch: 5| Step: 7
Training loss: 2.6781457853456554
Validation loss: 2.4809346539296935

Epoch: 5| Step: 8
Training loss: 1.5782055975850868
Validation loss: 2.490805750875059

Epoch: 5| Step: 9
Training loss: 2.431006953390731
Validation loss: 2.506356795991278

Epoch: 5| Step: 10
Training loss: 1.99951434199309
Validation loss: 2.4555343668807113

Epoch: 239| Step: 0
Training loss: 2.3628537634737907
Validation loss: 2.4666895639869497

Epoch: 5| Step: 1
Training loss: 2.9384554668568117
Validation loss: 2.4841725421782686

Epoch: 5| Step: 2
Training loss: 2.1897286098013424
Validation loss: 2.4694925901951508

Epoch: 5| Step: 3
Training loss: 2.119351902990274
Validation loss: 2.487576989198429

Epoch: 5| Step: 4
Training loss: 2.471945710261943
Validation loss: 2.4791438480056773

Epoch: 5| Step: 5
Training loss: 2.581137544339052
Validation loss: 2.49060967265249

Epoch: 5| Step: 6
Training loss: 1.9172774737572424
Validation loss: 2.504058511258298

Epoch: 5| Step: 7
Training loss: 1.9304656031423537
Validation loss: 2.4847923336401765

Epoch: 5| Step: 8
Training loss: 2.0548551657841614
Validation loss: 2.5047368591410035

Epoch: 5| Step: 9
Training loss: 2.446216745126184
Validation loss: 2.49421606814514

Epoch: 5| Step: 10
Training loss: 2.5905456972581122
Validation loss: 2.5226498205659227

Epoch: 240| Step: 0
Training loss: 2.2312932670095385
Validation loss: 2.4845977412992397

Epoch: 5| Step: 1
Training loss: 2.16983023792498
Validation loss: 2.504869608842391

Epoch: 5| Step: 2
Training loss: 2.4804948940551346
Validation loss: 2.4772717755567726

Epoch: 5| Step: 3
Training loss: 2.258193462399324
Validation loss: 2.4930773055116098

Epoch: 5| Step: 4
Training loss: 2.2814423989459502
Validation loss: 2.483513392009896

Epoch: 5| Step: 5
Training loss: 2.2696290512153854
Validation loss: 2.4708788462526807

Epoch: 5| Step: 6
Training loss: 2.7545451836653108
Validation loss: 2.493910598692857

Epoch: 5| Step: 7
Training loss: 2.329766635586909
Validation loss: 2.5033177193234093

Epoch: 5| Step: 8
Training loss: 2.13153585715114
Validation loss: 2.4721647452303155

Epoch: 5| Step: 9
Training loss: 2.5093679387621504
Validation loss: 2.5071333339074453

Epoch: 5| Step: 10
Training loss: 1.8961936342817935
Validation loss: 2.515019457492745

Epoch: 241| Step: 0
Training loss: 2.443610235641955
Validation loss: 2.493596093994785

Epoch: 5| Step: 1
Training loss: 2.50815986295378
Validation loss: 2.480993582350743

Epoch: 5| Step: 2
Training loss: 2.478650581647623
Validation loss: 2.480840525511117

Epoch: 5| Step: 3
Training loss: 2.293837634499187
Validation loss: 2.5166948515203345

Epoch: 5| Step: 4
Training loss: 2.449109137362283
Validation loss: 2.508109965620032

Epoch: 5| Step: 5
Training loss: 2.570782001133327
Validation loss: 2.529629238962463

Epoch: 5| Step: 6
Training loss: 2.3638216267853327
Validation loss: 2.5027137148383827

Epoch: 5| Step: 7
Training loss: 1.7628288746793315
Validation loss: 2.4883920011894496

Epoch: 5| Step: 8
Training loss: 2.1146614799403833
Validation loss: 2.4851053977430104

Epoch: 5| Step: 9
Training loss: 2.6549933658667833
Validation loss: 2.505746306074077

Epoch: 5| Step: 10
Training loss: 1.9662179558938389
Validation loss: 2.508506506366807

Epoch: 242| Step: 0
Training loss: 2.477037835458606
Validation loss: 2.530696293328365

Epoch: 5| Step: 1
Training loss: 1.3235484539482227
Validation loss: 2.4966641402056884

Epoch: 5| Step: 2
Training loss: 2.691328293954464
Validation loss: 2.4885838028250897

Epoch: 5| Step: 3
Training loss: 2.549019127481798
Validation loss: 2.491399393972054

Epoch: 5| Step: 4
Training loss: 2.519122233939594
Validation loss: 2.4789809290722347

Epoch: 5| Step: 5
Training loss: 2.3072562692193093
Validation loss: 2.5237145700534565

Epoch: 5| Step: 6
Training loss: 2.781214595955095
Validation loss: 2.4899840819918353

Epoch: 5| Step: 7
Training loss: 2.532286443565021
Validation loss: 2.4604583497748753

Epoch: 5| Step: 8
Training loss: 2.163643586414075
Validation loss: 2.4732612005011356

Epoch: 5| Step: 9
Training loss: 1.8133914991240747
Validation loss: 2.5100653609405037

Epoch: 5| Step: 10
Training loss: 1.9567007672827588
Validation loss: 2.519421789308776

Epoch: 243| Step: 0
Training loss: 2.4760684895510283
Validation loss: 2.4934499632015625

Epoch: 5| Step: 1
Training loss: 2.2419428143588376
Validation loss: 2.5154271935585784

Epoch: 5| Step: 2
Training loss: 1.5145248667658124
Validation loss: 2.482141962814157

Epoch: 5| Step: 3
Training loss: 2.1743227112424024
Validation loss: 2.497724763153531

Epoch: 5| Step: 4
Training loss: 2.1908456557913887
Validation loss: 2.480965664167925

Epoch: 5| Step: 5
Training loss: 2.944850895576918
Validation loss: 2.481147630966046

Epoch: 5| Step: 6
Training loss: 2.1945907112124554
Validation loss: 2.4882672545594406

Epoch: 5| Step: 7
Training loss: 1.7650908615888041
Validation loss: 2.4589315353674985

Epoch: 5| Step: 8
Training loss: 2.5427507579555924
Validation loss: 2.479605576863088

Epoch: 5| Step: 9
Training loss: 2.881347614877245
Validation loss: 2.4762585984886663

Epoch: 5| Step: 10
Training loss: 2.0650034651831124
Validation loss: 2.4973016861630035

Epoch: 244| Step: 0
Training loss: 2.4073156003754037
Validation loss: 2.501693631821715

Epoch: 5| Step: 1
Training loss: 2.1715690342388583
Validation loss: 2.5085388972668627

Epoch: 5| Step: 2
Training loss: 2.2019425052686525
Validation loss: 2.4775563614379528

Epoch: 5| Step: 3
Training loss: 2.625716883543779
Validation loss: 2.4674605349304297

Epoch: 5| Step: 4
Training loss: 2.687268358050593
Validation loss: 2.48492062211933

Epoch: 5| Step: 5
Training loss: 2.2834686568689917
Validation loss: 2.486010360379655

Epoch: 5| Step: 6
Training loss: 1.7906347304474248
Validation loss: 2.515083519165274

Epoch: 5| Step: 7
Training loss: 1.795565849734741
Validation loss: 2.507735600207869

Epoch: 5| Step: 8
Training loss: 2.7720027167346846
Validation loss: 2.48590596399986

Epoch: 5| Step: 9
Training loss: 2.3217058204841794
Validation loss: 2.4692501269223652

Epoch: 5| Step: 10
Training loss: 2.2236072939219746
Validation loss: 2.484537988280896

Epoch: 245| Step: 0
Training loss: 2.2932334630127276
Validation loss: 2.500495347817056

Epoch: 5| Step: 1
Training loss: 2.0553137691448065
Validation loss: 2.4764002953286646

Epoch: 5| Step: 2
Training loss: 1.9255894068245512
Validation loss: 2.475476813022312

Epoch: 5| Step: 3
Training loss: 2.265404736565385
Validation loss: 2.4888166761839243

Epoch: 5| Step: 4
Training loss: 2.356430641757844
Validation loss: 2.4741747389196864

Epoch: 5| Step: 5
Training loss: 2.6068280976236817
Validation loss: 2.4923344124980376

Epoch: 5| Step: 6
Training loss: 2.683150475552347
Validation loss: 2.494403957387013

Epoch: 5| Step: 7
Training loss: 2.443535399709566
Validation loss: 2.488748797126235

Epoch: 5| Step: 8
Training loss: 2.28406194584848
Validation loss: 2.4866382466786274

Epoch: 5| Step: 9
Training loss: 2.0951913455809157
Validation loss: 2.4852524539846397

Epoch: 5| Step: 10
Training loss: 2.535149760891878
Validation loss: 2.489300068180378

Epoch: 246| Step: 0
Training loss: 2.601486434053605
Validation loss: 2.4882415450473414

Epoch: 5| Step: 1
Training loss: 1.9722077416916681
Validation loss: 2.502136864644136

Epoch: 5| Step: 2
Training loss: 2.243673118187963
Validation loss: 2.4881263458417044

Epoch: 5| Step: 3
Training loss: 2.252964398166722
Validation loss: 2.4766929433991

Epoch: 5| Step: 4
Training loss: 2.682634784764362
Validation loss: 2.502483725649408

Epoch: 5| Step: 5
Training loss: 2.4184979921103507
Validation loss: 2.4827580978358292

Epoch: 5| Step: 6
Training loss: 2.4144644294625954
Validation loss: 2.4928798628673134

Epoch: 5| Step: 7
Training loss: 2.2693003341675233
Validation loss: 2.4737791605101016

Epoch: 5| Step: 8
Training loss: 2.2293609700832255
Validation loss: 2.490269286401393

Epoch: 5| Step: 9
Training loss: 1.9581830866648473
Validation loss: 2.4935775656921084

Epoch: 5| Step: 10
Training loss: 2.201750630989907
Validation loss: 2.50138182444454

Epoch: 247| Step: 0
Training loss: 2.4712148023461493
Validation loss: 2.5166879195720533

Epoch: 5| Step: 1
Training loss: 1.3920931834458798
Validation loss: 2.506843532783708

Epoch: 5| Step: 2
Training loss: 1.8876356341405567
Validation loss: 2.5066855963591883

Epoch: 5| Step: 3
Training loss: 2.6289732699190713
Validation loss: 2.489778123270875

Epoch: 5| Step: 4
Training loss: 2.290697991221738
Validation loss: 2.519950645716778

Epoch: 5| Step: 5
Training loss: 2.747302553060765
Validation loss: 2.5137242411509972

Epoch: 5| Step: 6
Training loss: 2.0860093922316745
Validation loss: 2.5099368863435165

Epoch: 5| Step: 7
Training loss: 2.2973646822691
Validation loss: 2.513427881634299

Epoch: 5| Step: 8
Training loss: 2.167079323817043
Validation loss: 2.4865111623812126

Epoch: 5| Step: 9
Training loss: 2.7426779979434026
Validation loss: 2.4777560003220533

Epoch: 5| Step: 10
Training loss: 2.422016065857678
Validation loss: 2.490798065516957

Epoch: 248| Step: 0
Training loss: 2.5565120236289185
Validation loss: 2.5045390484908934

Epoch: 5| Step: 1
Training loss: 2.2071243671589813
Validation loss: 2.521111573583132

Epoch: 5| Step: 2
Training loss: 2.4868399429630577
Validation loss: 2.488992808847381

Epoch: 5| Step: 3
Training loss: 2.059753438767763
Validation loss: 2.486002269360615

Epoch: 5| Step: 4
Training loss: 2.0984625092675624
Validation loss: 2.497110783084433

Epoch: 5| Step: 5
Training loss: 2.2749278633502352
Validation loss: 2.4719995047914614

Epoch: 5| Step: 6
Training loss: 2.4180132194706694
Validation loss: 2.5002305908916442

Epoch: 5| Step: 7
Training loss: 1.913596084754026
Validation loss: 2.499817295986688

Epoch: 5| Step: 8
Training loss: 2.073256562192572
Validation loss: 2.493252489916785

Epoch: 5| Step: 9
Training loss: 2.9455284626352425
Validation loss: 2.470934139361014

Epoch: 5| Step: 10
Training loss: 2.2598805179913657
Validation loss: 2.4882020238881224

Epoch: 249| Step: 0
Training loss: 2.885601565683319
Validation loss: 2.4754626514660574

Epoch: 5| Step: 1
Training loss: 2.108710742487788
Validation loss: 2.5038298578789555

Epoch: 5| Step: 2
Training loss: 2.2447468575403455
Validation loss: 2.4834188369227284

Epoch: 5| Step: 3
Training loss: 2.306604654805269
Validation loss: 2.498506822025618

Epoch: 5| Step: 4
Training loss: 2.270204847898559
Validation loss: 2.509056476126422

Epoch: 5| Step: 5
Training loss: 1.729091328584485
Validation loss: 2.5129963531823507

Epoch: 5| Step: 6
Training loss: 2.2225026377607553
Validation loss: 2.5000573659283307

Epoch: 5| Step: 7
Training loss: 2.798048827987984
Validation loss: 2.501351512121122

Epoch: 5| Step: 8
Training loss: 2.241269339006243
Validation loss: 2.5098560466193263

Epoch: 5| Step: 9
Training loss: 2.0133226833735507
Validation loss: 2.5141034996016782

Epoch: 5| Step: 10
Training loss: 2.37212378361923
Validation loss: 2.5056191636828804

Epoch: 250| Step: 0
Training loss: 2.6325560311841834
Validation loss: 2.5022589906345445

Epoch: 5| Step: 1
Training loss: 1.747094194175441
Validation loss: 2.5005799974326277

Epoch: 5| Step: 2
Training loss: 2.6153206159347464
Validation loss: 2.507141781084676

Epoch: 5| Step: 3
Training loss: 1.6887532278350545
Validation loss: 2.4903066196051835

Epoch: 5| Step: 4
Training loss: 2.5190525753505764
Validation loss: 2.5144278178778237

Epoch: 5| Step: 5
Training loss: 2.110509270773062
Validation loss: 2.491640118073281

Epoch: 5| Step: 6
Training loss: 2.918752542041268
Validation loss: 2.506421610432683

Epoch: 5| Step: 7
Training loss: 1.890632251063542
Validation loss: 2.509408856674589

Epoch: 5| Step: 8
Training loss: 2.379422336966224
Validation loss: 2.492151424252161

Epoch: 5| Step: 9
Training loss: 2.3404499472335165
Validation loss: 2.5039370986056655

Epoch: 5| Step: 10
Training loss: 2.1629097963695676
Validation loss: 2.4701962723937814

Testing loss: 2.4562325634174336

Epoch: 1| Step: 0
Training loss: 3.985403369509816
Validation loss: 4.432300984306669

Epoch: 6| Step: 1
Training loss: 5.418027188536677
Validation loss: 4.432720396638203

Epoch: 6| Step: 2
Training loss: 5.164263422717585
Validation loss: 4.431322542489999

Epoch: 6| Step: 3
Training loss: 3.657473498005261
Validation loss: 4.429066500062989

Epoch: 6| Step: 4
Training loss: 4.728762481939191
Validation loss: 4.429620610954856

Epoch: 6| Step: 5
Training loss: 3.4303029405367362
Validation loss: 4.429995602754363

Epoch: 6| Step: 6
Training loss: 4.919928853210613
Validation loss: 4.426145530940114

Epoch: 6| Step: 7
Training loss: 4.4894852181295475
Validation loss: 4.427851439502263

Epoch: 6| Step: 8
Training loss: 5.067048466669154
Validation loss: 4.424620171016346

Epoch: 6| Step: 9
Training loss: 4.303884330911125
Validation loss: 4.427454157529116

Epoch: 6| Step: 10
Training loss: 4.64685749802311
Validation loss: 4.425191534953805

Epoch: 6| Step: 11
Training loss: 4.226505187486881
Validation loss: 4.42350394601774

Epoch: 6| Step: 12
Training loss: 4.15398901913789
Validation loss: 4.4224888246426906

Epoch: 6| Step: 13
Training loss: 5.168667539148656
Validation loss: 4.4199948060433885

Epoch: 2| Step: 0
Training loss: 4.431794703139066
Validation loss: 4.4180489573230135

Epoch: 6| Step: 1
Training loss: 3.7600431107017824
Validation loss: 4.417186292441122

Epoch: 6| Step: 2
Training loss: 4.939027960275464
Validation loss: 4.418056913914051

Epoch: 6| Step: 3
Training loss: 5.6986638427165675
Validation loss: 4.416916236319574

Epoch: 6| Step: 4
Training loss: 3.2692993519351323
Validation loss: 4.417339145283352

Epoch: 6| Step: 5
Training loss: 4.204645838933331
Validation loss: 4.415325777745022

Epoch: 6| Step: 6
Training loss: 4.515494902429543
Validation loss: 4.41552523821554

Epoch: 6| Step: 7
Training loss: 3.347938837256113
Validation loss: 4.411867508934102

Epoch: 6| Step: 8
Training loss: 5.412709517090886
Validation loss: 4.41214478150643

Epoch: 6| Step: 9
Training loss: 4.014207404206262
Validation loss: 4.413833925581774

Epoch: 6| Step: 10
Training loss: 4.90654855779498
Validation loss: 4.412875111547368

Epoch: 6| Step: 11
Training loss: 4.705136296794295
Validation loss: 4.409725913044981

Epoch: 6| Step: 12
Training loss: 5.6324545737312866
Validation loss: 4.410318779550684

Epoch: 6| Step: 13
Training loss: 2.806816801675254
Validation loss: 4.406664326519644

Epoch: 3| Step: 0
Training loss: 4.718173566561453
Validation loss: 4.407283568937298

Epoch: 6| Step: 1
Training loss: 4.771748595196704
Validation loss: 4.4053838421516955

Epoch: 6| Step: 2
Training loss: 4.61454126526865
Validation loss: 4.403933378422226

Epoch: 6| Step: 3
Training loss: 5.637316972759931
Validation loss: 4.4015989755193035

Epoch: 6| Step: 4
Training loss: 4.201259950843383
Validation loss: 4.402151153492232

Epoch: 6| Step: 5
Training loss: 4.536785454092989
Validation loss: 4.4006607188842395

Epoch: 6| Step: 6
Training loss: 4.829289268185892
Validation loss: 4.399999055554688

Epoch: 6| Step: 7
Training loss: 4.266117053594032
Validation loss: 4.394142273891779

Epoch: 6| Step: 8
Training loss: 4.489295519508
Validation loss: 4.393915977220182

Epoch: 6| Step: 9
Training loss: 5.4614308234009465
Validation loss: 4.395109891106431

Epoch: 6| Step: 10
Training loss: 2.900479895248664
Validation loss: 4.390701195774577

Epoch: 6| Step: 11
Training loss: 4.25691423652611
Validation loss: 4.390358761588473

Epoch: 6| Step: 12
Training loss: 3.750827825090632
Validation loss: 4.386903980562285

Epoch: 6| Step: 13
Training loss: 3.7397213257784894
Validation loss: 4.389766698703817

Epoch: 4| Step: 0
Training loss: 4.6505095664125795
Validation loss: 4.38654519327317

Epoch: 6| Step: 1
Training loss: 4.777797474685008
Validation loss: 4.3841589096207825

Epoch: 6| Step: 2
Training loss: 5.188741512415705
Validation loss: 4.379455683568909

Epoch: 6| Step: 3
Training loss: 5.368055248493527
Validation loss: 4.380349250459982

Epoch: 6| Step: 4
Training loss: 3.18895314322086
Validation loss: 4.379745087640778

Epoch: 6| Step: 5
Training loss: 4.599383785603827
Validation loss: 4.374768036817888

Epoch: 6| Step: 6
Training loss: 3.6635174376368322
Validation loss: 4.3747970815264505

Epoch: 6| Step: 7
Training loss: 4.463278241509576
Validation loss: 4.376702551925211

Epoch: 6| Step: 8
Training loss: 4.958448948300304
Validation loss: 4.370491787319793

Epoch: 6| Step: 9
Training loss: 3.9570404082754336
Validation loss: 4.373843287130129

Epoch: 6| Step: 10
Training loss: 4.628264022892448
Validation loss: 4.368146154040155

Epoch: 6| Step: 11
Training loss: 4.162948831640081
Validation loss: 4.369120801826717

Epoch: 6| Step: 12
Training loss: 4.226641472662158
Validation loss: 4.3667700572025705

Epoch: 6| Step: 13
Training loss: 4.582877858967533
Validation loss: 4.359293282646708

Epoch: 5| Step: 0
Training loss: 4.195366374261279
Validation loss: 4.357645499217295

Epoch: 6| Step: 1
Training loss: 4.7404647287071136
Validation loss: 4.361494933426408

Epoch: 6| Step: 2
Training loss: 4.478527975751613
Validation loss: 4.3582432692504485

Epoch: 6| Step: 3
Training loss: 4.929924670527946
Validation loss: 4.351143610010096

Epoch: 6| Step: 4
Training loss: 4.753152102581856
Validation loss: 4.350180765935667

Epoch: 6| Step: 5
Training loss: 4.372197806906697
Validation loss: 4.349547287624174

Epoch: 6| Step: 6
Training loss: 4.209536289220425
Validation loss: 4.348571304254713

Epoch: 6| Step: 7
Training loss: 5.105826262468178
Validation loss: 4.348528656989671

Epoch: 6| Step: 8
Training loss: 4.504387729712006
Validation loss: 4.3463627719872

Epoch: 6| Step: 9
Training loss: 4.169332694032151
Validation loss: 4.344326575637776

Epoch: 6| Step: 10
Training loss: 3.645619122706292
Validation loss: 4.342995014445548

Epoch: 6| Step: 11
Training loss: 5.224948331586545
Validation loss: 4.340564864619791

Epoch: 6| Step: 12
Training loss: 4.102525463594755
Validation loss: 4.335731686673333

Epoch: 6| Step: 13
Training loss: 3.2314327697418257
Validation loss: 4.334771352343031

Epoch: 6| Step: 0
Training loss: 4.806598562392145
Validation loss: 4.331054113336947

Epoch: 6| Step: 1
Training loss: 4.742264924524977
Validation loss: 4.330524416085983

Epoch: 6| Step: 2
Training loss: 4.295773118661147
Validation loss: 4.326141382647583

Epoch: 6| Step: 3
Training loss: 4.745346198843136
Validation loss: 4.323936921797212

Epoch: 6| Step: 4
Training loss: 3.7302139914238115
Validation loss: 4.322095475650916

Epoch: 6| Step: 5
Training loss: 4.738937345085344
Validation loss: 4.318043182062285

Epoch: 6| Step: 6
Training loss: 4.85059763578844
Validation loss: 4.3203190344322495

Epoch: 6| Step: 7
Training loss: 4.1367886026768765
Validation loss: 4.319252736144348

Epoch: 6| Step: 8
Training loss: 4.458951223937761
Validation loss: 4.313459382681654

Epoch: 6| Step: 9
Training loss: 4.214716134247377
Validation loss: 4.308706296360962

Epoch: 6| Step: 10
Training loss: 4.8628333440828735
Validation loss: 4.306307839654489

Epoch: 6| Step: 11
Training loss: 4.483179444764615
Validation loss: 4.304319553399573

Epoch: 6| Step: 12
Training loss: 3.2159331504933966
Validation loss: 4.304311453270153

Epoch: 6| Step: 13
Training loss: 4.660035303661558
Validation loss: 4.300605872802504

Epoch: 7| Step: 0
Training loss: 2.7966559127158312
Validation loss: 4.300196336621833

Epoch: 6| Step: 1
Training loss: 4.736206506170502
Validation loss: 4.296890166284788

Epoch: 6| Step: 2
Training loss: 3.871617471352564
Validation loss: 4.292279276306828

Epoch: 6| Step: 3
Training loss: 5.159625803664244
Validation loss: 4.292141185746715

Epoch: 6| Step: 4
Training loss: 3.835390009140765
Validation loss: 4.293599897663514

Epoch: 6| Step: 5
Training loss: 4.676018246004061
Validation loss: 4.287722090875976

Epoch: 6| Step: 6
Training loss: 4.174201687994444
Validation loss: 4.2854919161534974

Epoch: 6| Step: 7
Training loss: 4.941411846896534
Validation loss: 4.281726298983305

Epoch: 6| Step: 8
Training loss: 3.9081633497632384
Validation loss: 4.283735602384714

Epoch: 6| Step: 9
Training loss: 4.189490912562684
Validation loss: 4.276265148238131

Epoch: 6| Step: 10
Training loss: 4.457990805519312
Validation loss: 4.275250604952049

Epoch: 6| Step: 11
Training loss: 4.759124825427875
Validation loss: 4.270918136623123

Epoch: 6| Step: 12
Training loss: 4.782843430228108
Validation loss: 4.271576354519208

Epoch: 6| Step: 13
Training loss: 5.011044130537891
Validation loss: 4.262716359499519

Epoch: 8| Step: 0
Training loss: 4.624924736441559
Validation loss: 4.2608355866207726

Epoch: 6| Step: 1
Training loss: 4.910148092549907
Validation loss: 4.253490459957484

Epoch: 6| Step: 2
Training loss: 4.346808584817932
Validation loss: 4.252897767025022

Epoch: 6| Step: 3
Training loss: 4.058684919873504
Validation loss: 4.251434783706227

Epoch: 6| Step: 4
Training loss: 4.794551124923162
Validation loss: 4.247470980315997

Epoch: 6| Step: 5
Training loss: 4.96193587702472
Validation loss: 4.239357636649243

Epoch: 6| Step: 6
Training loss: 3.485318043452229
Validation loss: 4.238482850849958

Epoch: 6| Step: 7
Training loss: 3.263381114086488
Validation loss: 4.235472732517553

Epoch: 6| Step: 8
Training loss: 4.005253441419766
Validation loss: 4.228460463373162

Epoch: 6| Step: 9
Training loss: 4.380114154808671
Validation loss: 4.2271016853074155

Epoch: 6| Step: 10
Training loss: 5.034941841100225
Validation loss: 4.220281005787542

Epoch: 6| Step: 11
Training loss: 4.309317478739337
Validation loss: 4.2185641510262055

Epoch: 6| Step: 12
Training loss: 3.593383637703105
Validation loss: 4.207420919498205

Epoch: 6| Step: 13
Training loss: 5.094974862018899
Validation loss: 4.205369017207026

Epoch: 9| Step: 0
Training loss: 4.5392192347347935
Validation loss: 4.20244741172092

Epoch: 6| Step: 1
Training loss: 4.859514534961587
Validation loss: 4.196410185038693

Epoch: 6| Step: 2
Training loss: 3.2119382612059697
Validation loss: 4.190927943030802

Epoch: 6| Step: 3
Training loss: 4.738522767958485
Validation loss: 4.182796664725154

Epoch: 6| Step: 4
Training loss: 4.712241022496786
Validation loss: 4.179010186122225

Epoch: 6| Step: 5
Training loss: 3.911795745939041
Validation loss: 4.172055852458166

Epoch: 6| Step: 6
Training loss: 4.773129816955717
Validation loss: 4.168203062416758

Epoch: 6| Step: 7
Training loss: 4.262666339928745
Validation loss: 4.162672806818255

Epoch: 6| Step: 8
Training loss: 4.216154981685309
Validation loss: 4.157421628195228

Epoch: 6| Step: 9
Training loss: 3.1911136861960183
Validation loss: 4.151838242264834

Epoch: 6| Step: 10
Training loss: 3.729924570706708
Validation loss: 4.142772759122173

Epoch: 6| Step: 11
Training loss: 4.894334257869888
Validation loss: 4.14028565588686

Epoch: 6| Step: 12
Training loss: 4.087216354001102
Validation loss: 4.134931866591549

Epoch: 6| Step: 13
Training loss: 4.746293880811359
Validation loss: 4.127092685169047

Epoch: 10| Step: 0
Training loss: 4.819940735068761
Validation loss: 4.11968827905134

Epoch: 6| Step: 1
Training loss: 4.617913668308365
Validation loss: 4.112479585677393

Epoch: 6| Step: 2
Training loss: 4.541808566521891
Validation loss: 4.106416619719194

Epoch: 6| Step: 3
Training loss: 4.221505176714965
Validation loss: 4.098828254944939

Epoch: 6| Step: 4
Training loss: 4.154249584680295
Validation loss: 4.088596655206302

Epoch: 6| Step: 5
Training loss: 4.0482076564193425
Validation loss: 4.08917451559852

Epoch: 6| Step: 6
Training loss: 3.969430699883392
Validation loss: 4.0843947192423595

Epoch: 6| Step: 7
Training loss: 4.6411907223697595
Validation loss: 4.065914547234399

Epoch: 6| Step: 8
Training loss: 3.8780201863181065
Validation loss: 4.0651059207023135

Epoch: 6| Step: 9
Training loss: 3.7841573840068534
Validation loss: 4.054658660985277

Epoch: 6| Step: 10
Training loss: 3.87892518130136
Validation loss: 4.050232874268295

Epoch: 6| Step: 11
Training loss: 3.171168507035255
Validation loss: 4.040183154177832

Epoch: 6| Step: 12
Training loss: 4.404654355988394
Validation loss: 4.03980054653985

Epoch: 6| Step: 13
Training loss: 4.751930246168722
Validation loss: 4.026912170642889

Epoch: 11| Step: 0
Training loss: 3.887100400349842
Validation loss: 4.016026809359404

Epoch: 6| Step: 1
Training loss: 4.872674631949855
Validation loss: 4.011972263692325

Epoch: 6| Step: 2
Training loss: 4.608943181904971
Validation loss: 4.00713425117884

Epoch: 6| Step: 3
Training loss: 4.297141992770343
Validation loss: 3.9971868816005904

Epoch: 6| Step: 4
Training loss: 3.901209760982115
Validation loss: 3.9843080653745813

Epoch: 6| Step: 5
Training loss: 2.9595799215036545
Validation loss: 3.971856975010263

Epoch: 6| Step: 6
Training loss: 3.859188554098069
Validation loss: 3.9668630193760523

Epoch: 6| Step: 7
Training loss: 4.632962563771649
Validation loss: 3.9506174312400772

Epoch: 6| Step: 8
Training loss: 3.685981179075992
Validation loss: 3.9467296854418357

Epoch: 6| Step: 9
Training loss: 4.72022613274647
Validation loss: 3.9395588383295124

Epoch: 6| Step: 10
Training loss: 4.462643592725433
Validation loss: 3.9258359680460186

Epoch: 6| Step: 11
Training loss: 3.3547706455113104
Validation loss: 3.915025124005176

Epoch: 6| Step: 12
Training loss: 3.939097247032882
Validation loss: 3.9137185040364706

Epoch: 6| Step: 13
Training loss: 3.5184733741124687
Validation loss: 3.894375917001286

Epoch: 12| Step: 0
Training loss: 3.5353658540244117
Validation loss: 3.89207812549086

Epoch: 6| Step: 1
Training loss: 4.338941808278258
Validation loss: 3.886005235077675

Epoch: 6| Step: 2
Training loss: 4.654188480627746
Validation loss: 3.8685005863148767

Epoch: 6| Step: 3
Training loss: 3.924212606380817
Validation loss: 3.8623001513663158

Epoch: 6| Step: 4
Training loss: 3.9664999523444147
Validation loss: 3.857350113537271

Epoch: 6| Step: 5
Training loss: 2.7447353600958415
Validation loss: 3.837839614907834

Epoch: 6| Step: 6
Training loss: 4.0134259447921865
Validation loss: 3.825687130270172

Epoch: 6| Step: 7
Training loss: 3.953822379738683
Validation loss: 3.8249932248514447

Epoch: 6| Step: 8
Training loss: 3.667651665482812
Validation loss: 3.8046903569579533

Epoch: 6| Step: 9
Training loss: 4.902265157487547
Validation loss: 3.7974081255519314

Epoch: 6| Step: 10
Training loss: 3.68559215782661
Validation loss: 3.787827124585451

Epoch: 6| Step: 11
Training loss: 3.117888790014322
Validation loss: 3.778259279734488

Epoch: 6| Step: 12
Training loss: 4.262735247343005
Validation loss: 3.7583581460367834

Epoch: 6| Step: 13
Training loss: 4.502853760237388
Validation loss: 3.7522545617127103

Epoch: 13| Step: 0
Training loss: 4.184791258205821
Validation loss: 3.7345650672126554

Epoch: 6| Step: 1
Training loss: 4.029143026782921
Validation loss: 3.7232135510111672

Epoch: 6| Step: 2
Training loss: 4.074244030062989
Validation loss: 3.7063683731550903

Epoch: 6| Step: 3
Training loss: 3.929510457656095
Validation loss: 3.6992260655348788

Epoch: 6| Step: 4
Training loss: 3.6942343560495714
Validation loss: 3.687845721757513

Epoch: 6| Step: 5
Training loss: 3.493972765759088
Validation loss: 3.6741133319441133

Epoch: 6| Step: 6
Training loss: 4.177916306595549
Validation loss: 3.6783690950549017

Epoch: 6| Step: 7
Training loss: 3.905666216142765
Validation loss: 3.654599433081998

Epoch: 6| Step: 8
Training loss: 3.5627805365145897
Validation loss: 3.632693968907842

Epoch: 6| Step: 9
Training loss: 3.930085967776192
Validation loss: 3.6195167539212734

Epoch: 6| Step: 10
Training loss: 3.015089234507167
Validation loss: 3.616085629105159

Epoch: 6| Step: 11
Training loss: 4.505455101472738
Validation loss: 3.588295512527176

Epoch: 6| Step: 12
Training loss: 3.4669312528974228
Validation loss: 3.5817649344114293

Epoch: 6| Step: 13
Training loss: 2.9524977890897866
Validation loss: 3.5676329744070467

Epoch: 14| Step: 0
Training loss: 3.849166898879508
Validation loss: 3.543058878657209

Epoch: 6| Step: 1
Training loss: 3.5969794317043977
Validation loss: 3.5398608058452345

Epoch: 6| Step: 2
Training loss: 3.4443899290494686
Validation loss: 3.519209030111741

Epoch: 6| Step: 3
Training loss: 3.162540022404546
Validation loss: 3.507854654449064

Epoch: 6| Step: 4
Training loss: 3.4982465711511535
Validation loss: 3.504721078230917

Epoch: 6| Step: 5
Training loss: 3.38896848936
Validation loss: 3.4903209501162626

Epoch: 6| Step: 6
Training loss: 3.4213961610245938
Validation loss: 3.472474069191915

Epoch: 6| Step: 7
Training loss: 4.007369167502954
Validation loss: 3.4519890374799402

Epoch: 6| Step: 8
Training loss: 3.7179708225527426
Validation loss: 3.440776718252009

Epoch: 6| Step: 9
Training loss: 2.9363498465173694
Validation loss: 3.427019274944792

Epoch: 6| Step: 10
Training loss: 4.385659012550478
Validation loss: 3.4093203202113322

Epoch: 6| Step: 11
Training loss: 3.720518316850558
Validation loss: 3.403387659257231

Epoch: 6| Step: 12
Training loss: 4.145464907941993
Validation loss: 3.375078078095653

Epoch: 6| Step: 13
Training loss: 3.0645083634580916
Validation loss: 3.3733942381332227

Epoch: 15| Step: 0
Training loss: 3.954773329304935
Validation loss: 3.348046122947934

Epoch: 6| Step: 1
Training loss: 3.5030286491544533
Validation loss: 3.329387110624113

Epoch: 6| Step: 2
Training loss: 4.021878016188409
Validation loss: 3.3016184046722947

Epoch: 6| Step: 3
Training loss: 4.044955590078587
Validation loss: 3.298713071003292

Epoch: 6| Step: 4
Training loss: 3.4655218826239307
Validation loss: 3.2735681245635573

Epoch: 6| Step: 5
Training loss: 3.226793183954336
Validation loss: 3.258772402828119

Epoch: 6| Step: 6
Training loss: 3.21048631305804
Validation loss: 3.2387160399922763

Epoch: 6| Step: 7
Training loss: 3.4083032981338843
Validation loss: 3.223243705875313

Epoch: 6| Step: 8
Training loss: 3.4819264383099418
Validation loss: 3.2014971654805042

Epoch: 6| Step: 9
Training loss: 3.206914976291254
Validation loss: 3.200166293727595

Epoch: 6| Step: 10
Training loss: 3.3062438907098124
Validation loss: 3.163224334885949

Epoch: 6| Step: 11
Training loss: 2.7410581170385035
Validation loss: 3.1471079820751053

Epoch: 6| Step: 12
Training loss: 3.2203892357654507
Validation loss: 3.134591565803866

Epoch: 6| Step: 13
Training loss: 3.0150346721724204
Validation loss: 3.1119761501933847

Epoch: 16| Step: 0
Training loss: 3.1035873760093984
Validation loss: 3.094580961242875

Epoch: 6| Step: 1
Training loss: 2.3318656210811706
Validation loss: 3.089252831564573

Epoch: 6| Step: 2
Training loss: 3.3494761270872995
Validation loss: 3.065566750152894

Epoch: 6| Step: 3
Training loss: 3.728603100237616
Validation loss: 3.0561879403847194

Epoch: 6| Step: 4
Training loss: 2.7034769876445015
Validation loss: 3.0369886891549354

Epoch: 6| Step: 5
Training loss: 3.6200127330577168
Validation loss: 3.0228861678409626

Epoch: 6| Step: 6
Training loss: 3.3542088729066855
Validation loss: 2.9958854971171736

Epoch: 6| Step: 7
Training loss: 3.0856982947243
Validation loss: 2.9877529354204166

Epoch: 6| Step: 8
Training loss: 3.3171864592684868
Validation loss: 2.9656340761794744

Epoch: 6| Step: 9
Training loss: 3.550807973763229
Validation loss: 2.94840148280879

Epoch: 6| Step: 10
Training loss: 2.9308602795859424
Validation loss: 2.947050539519981

Epoch: 6| Step: 11
Training loss: 3.7829544387401093
Validation loss: 2.912031971130197

Epoch: 6| Step: 12
Training loss: 2.8663960957548746
Validation loss: 2.9029660539120616

Epoch: 6| Step: 13
Training loss: 3.2888734450564767
Validation loss: 2.8787840550969324

Epoch: 17| Step: 0
Training loss: 3.3346995573815175
Validation loss: 2.870863061683087

Epoch: 6| Step: 1
Training loss: 3.7114951266894387
Validation loss: 2.841518304859025

Epoch: 6| Step: 2
Training loss: 2.7022371101788396
Validation loss: 2.8397162621911667

Epoch: 6| Step: 3
Training loss: 2.2612544556632357
Validation loss: 2.821390107396357

Epoch: 6| Step: 4
Training loss: 2.296941431862434
Validation loss: 2.798832366392625

Epoch: 6| Step: 5
Training loss: 3.1502556106763238
Validation loss: 2.80049789692631

Epoch: 6| Step: 6
Training loss: 3.3591473169573085
Validation loss: 2.7897888526462844

Epoch: 6| Step: 7
Training loss: 2.941712463569945
Validation loss: 2.768930476789041

Epoch: 6| Step: 8
Training loss: 2.9655534098498992
Validation loss: 2.7685809575751765

Epoch: 6| Step: 9
Training loss: 3.4788603657340005
Validation loss: 2.729848950346042

Epoch: 6| Step: 10
Training loss: 3.247335074967061
Validation loss: 2.7282911319543213

Epoch: 6| Step: 11
Training loss: 3.3335667210572275
Validation loss: 2.7199351134981593

Epoch: 6| Step: 12
Training loss: 2.221906324291873
Validation loss: 2.7013326280647596

Epoch: 6| Step: 13
Training loss: 3.121065791342624
Validation loss: 2.695597606701292

Epoch: 18| Step: 0
Training loss: 3.0582095863467953
Validation loss: 2.6707017922191083

Epoch: 6| Step: 1
Training loss: 3.1371905778386635
Validation loss: 2.658285629413291

Epoch: 6| Step: 2
Training loss: 2.733668644814794
Validation loss: 2.642018388825192

Epoch: 6| Step: 3
Training loss: 2.6921640777011806
Validation loss: 2.631179000971852

Epoch: 6| Step: 4
Training loss: 3.269646755352198
Validation loss: 2.6121353150015336

Epoch: 6| Step: 5
Training loss: 2.675884428610616
Validation loss: 2.6167488383147117

Epoch: 6| Step: 6
Training loss: 3.5099267513968004
Validation loss: 2.575704213802316

Epoch: 6| Step: 7
Training loss: 2.354721836581412
Validation loss: 2.564617327700926

Epoch: 6| Step: 8
Training loss: 2.9187084317565763
Validation loss: 2.572861589693598

Epoch: 6| Step: 9
Training loss: 2.827953101566471
Validation loss: 2.547110528484214

Epoch: 6| Step: 10
Training loss: 2.795901848474168
Validation loss: 2.5393009570453438

Epoch: 6| Step: 11
Training loss: 2.949139847868384
Validation loss: 2.539651771741811

Epoch: 6| Step: 12
Training loss: 2.667862941715944
Validation loss: 2.52959852125164

Epoch: 6| Step: 13
Training loss: 3.599845692188536
Validation loss: 2.5471999520189104

Epoch: 19| Step: 0
Training loss: 3.078094017531258
Validation loss: 2.5296013589310955

Epoch: 6| Step: 1
Training loss: 2.288319958756331
Validation loss: 2.5173308124541127

Epoch: 6| Step: 2
Training loss: 2.971187514441268
Validation loss: 2.5241199960888814

Epoch: 6| Step: 3
Training loss: 3.1131476896539385
Validation loss: 2.5150825029166244

Epoch: 6| Step: 4
Training loss: 2.343052264305908
Validation loss: 2.48797755062979

Epoch: 6| Step: 5
Training loss: 2.3368307083637876
Validation loss: 2.5133170922640993

Epoch: 6| Step: 6
Training loss: 2.4401760572484625
Validation loss: 2.5101477621466763

Epoch: 6| Step: 7
Training loss: 2.806924422004616
Validation loss: 2.489125770977106

Epoch: 6| Step: 8
Training loss: 2.7374475273781824
Validation loss: 2.498980943065038

Epoch: 6| Step: 9
Training loss: 3.392280420605933
Validation loss: 2.4856421851306827

Epoch: 6| Step: 10
Training loss: 2.896448157013954
Validation loss: 2.5059828008944702

Epoch: 6| Step: 11
Training loss: 3.564330851926778
Validation loss: 2.496437629484143

Epoch: 6| Step: 12
Training loss: 2.6388823949722786
Validation loss: 2.4828710974695887

Epoch: 6| Step: 13
Training loss: 2.943354676868762
Validation loss: 2.4772046253933833

Epoch: 20| Step: 0
Training loss: 2.3856470477090017
Validation loss: 2.4834452399953793

Epoch: 6| Step: 1
Training loss: 3.0208260547067933
Validation loss: 2.457155543761895

Epoch: 6| Step: 2
Training loss: 2.7387317934393036
Validation loss: 2.4588686918212757

Epoch: 6| Step: 3
Training loss: 2.3375177556143827
Validation loss: 2.450046132707921

Epoch: 6| Step: 4
Training loss: 2.8162056141152267
Validation loss: 2.448287091126306

Epoch: 6| Step: 5
Training loss: 3.251284125536527
Validation loss: 2.473251411373295

Epoch: 6| Step: 6
Training loss: 2.298410849348646
Validation loss: 2.4487800355033955

Epoch: 6| Step: 7
Training loss: 3.3417867440761837
Validation loss: 2.449579745753843

Epoch: 6| Step: 8
Training loss: 2.362466365837176
Validation loss: 2.4882773905467053

Epoch: 6| Step: 9
Training loss: 3.056728451963067
Validation loss: 2.4269842614984727

Epoch: 6| Step: 10
Training loss: 2.599858272797818
Validation loss: 2.462724465620807

Epoch: 6| Step: 11
Training loss: 3.251080700119418
Validation loss: 2.426484270422649

Epoch: 6| Step: 12
Training loss: 2.9775256103131675
Validation loss: 2.4375216449189856

Epoch: 6| Step: 13
Training loss: 3.2812639508631998
Validation loss: 2.4369012135553128

Epoch: 21| Step: 0
Training loss: 3.3872959652647463
Validation loss: 2.4450786255559165

Epoch: 6| Step: 1
Training loss: 2.781881196480721
Validation loss: 2.4437234005585724

Epoch: 6| Step: 2
Training loss: 3.035633810563444
Validation loss: 2.4450201757303915

Epoch: 6| Step: 3
Training loss: 3.3192835032358285
Validation loss: 2.422825640811182

Epoch: 6| Step: 4
Training loss: 3.0777094991419185
Validation loss: 2.4333872357459896

Epoch: 6| Step: 5
Training loss: 3.2823693682207056
Validation loss: 2.4610163282617505

Epoch: 6| Step: 6
Training loss: 1.8865281160768352
Validation loss: 2.4536640888264225

Epoch: 6| Step: 7
Training loss: 2.7431458974734535
Validation loss: 2.4417623070754586

Epoch: 6| Step: 8
Training loss: 2.8191560609238193
Validation loss: 2.4568764530728044

Epoch: 6| Step: 9
Training loss: 2.768118472441511
Validation loss: 2.470474523509935

Epoch: 6| Step: 10
Training loss: 2.49110451250037
Validation loss: 2.451906943606489

Epoch: 6| Step: 11
Training loss: 2.5827018109446147
Validation loss: 2.4272721810518876

Epoch: 6| Step: 12
Training loss: 2.506715909122502
Validation loss: 2.452850976513834

Epoch: 6| Step: 13
Training loss: 2.011525917070214
Validation loss: 2.4466464239412313

Epoch: 22| Step: 0
Training loss: 1.5890759012713083
Validation loss: 2.4493418116026917

Epoch: 6| Step: 1
Training loss: 2.9861301234229014
Validation loss: 2.4563882678572577

Epoch: 6| Step: 2
Training loss: 3.149991468387736
Validation loss: 2.43655902957306

Epoch: 6| Step: 3
Training loss: 2.4413718747579964
Validation loss: 2.4427056899724295

Epoch: 6| Step: 4
Training loss: 3.152898236259459
Validation loss: 2.4494853355385837

Epoch: 6| Step: 5
Training loss: 3.365518319963764
Validation loss: 2.4496500884597396

Epoch: 6| Step: 6
Training loss: 2.560032656878554
Validation loss: 2.4521448714089624

Epoch: 6| Step: 7
Training loss: 2.818056509302825
Validation loss: 2.457769207877932

Epoch: 6| Step: 8
Training loss: 3.5111109257414994
Validation loss: 2.4507401202513934

Epoch: 6| Step: 9
Training loss: 2.662200187315242
Validation loss: 2.438583629577017

Epoch: 6| Step: 10
Training loss: 2.059462535890629
Validation loss: 2.4307804741536354

Epoch: 6| Step: 11
Training loss: 3.133236216670094
Validation loss: 2.444008457580058

Epoch: 6| Step: 12
Training loss: 2.4575856467269936
Validation loss: 2.438025494418151

Epoch: 6| Step: 13
Training loss: 3.0863086332404053
Validation loss: 2.44071929599482

Epoch: 23| Step: 0
Training loss: 2.723721398478699
Validation loss: 2.4231899580484235

Epoch: 6| Step: 1
Training loss: 2.5871199471558484
Validation loss: 2.4497239171389573

Epoch: 6| Step: 2
Training loss: 3.1895712312601243
Validation loss: 2.4341792858574762

Epoch: 6| Step: 3
Training loss: 3.0163216058078235
Validation loss: 2.4439938100099976

Epoch: 6| Step: 4
Training loss: 2.8283278250280035
Validation loss: 2.4489241273004256

Epoch: 6| Step: 5
Training loss: 3.397934168231236
Validation loss: 2.4528500599015817

Epoch: 6| Step: 6
Training loss: 2.677812014510838
Validation loss: 2.448652062651321

Epoch: 6| Step: 7
Training loss: 2.8497854051610183
Validation loss: 2.436923789551785

Epoch: 6| Step: 8
Training loss: 2.79220122348378
Validation loss: 2.4486042642753487

Epoch: 6| Step: 9
Training loss: 3.002256816128533
Validation loss: 2.4382335084004385

Epoch: 6| Step: 10
Training loss: 2.378846316530023
Validation loss: 2.4647328653598635

Epoch: 6| Step: 11
Training loss: 2.345893693308884
Validation loss: 2.433421165419608

Epoch: 6| Step: 12
Training loss: 2.893553698742659
Validation loss: 2.4335619264771435

Epoch: 6| Step: 13
Training loss: 2.6316830167771834
Validation loss: 2.4241207405901606

Epoch: 24| Step: 0
Training loss: 2.449465311833032
Validation loss: 2.4510548582189617

Epoch: 6| Step: 1
Training loss: 2.648063970910987
Validation loss: 2.451694213137342

Epoch: 6| Step: 2
Training loss: 2.300165842130987
Validation loss: 2.4344884433688843

Epoch: 6| Step: 3
Training loss: 2.4680898242413467
Validation loss: 2.43334113399465

Epoch: 6| Step: 4
Training loss: 2.888273365112141
Validation loss: 2.458994139604754

Epoch: 6| Step: 5
Training loss: 2.7781795740935546
Validation loss: 2.4373462377135566

Epoch: 6| Step: 6
Training loss: 2.833958968108459
Validation loss: 2.443918795431897

Epoch: 6| Step: 7
Training loss: 2.9499273194233737
Validation loss: 2.4312156617248

Epoch: 6| Step: 8
Training loss: 3.339902428897902
Validation loss: 2.442448293871848

Epoch: 6| Step: 9
Training loss: 2.9592641163146323
Validation loss: 2.4720531830379007

Epoch: 6| Step: 10
Training loss: 2.6887065042252263
Validation loss: 2.4224226116487686

Epoch: 6| Step: 11
Training loss: 2.9542008312349943
Validation loss: 2.464411293639217

Epoch: 6| Step: 12
Training loss: 3.426074373238549
Validation loss: 2.444250880786334

Epoch: 6| Step: 13
Training loss: 2.6462826585083317
Validation loss: 2.4273371770798886

Epoch: 25| Step: 0
Training loss: 2.967074915481095
Validation loss: 2.4427882177367954

Epoch: 6| Step: 1
Training loss: 3.1442343245876314
Validation loss: 2.443743089453645

Epoch: 6| Step: 2
Training loss: 3.244643565676534
Validation loss: 2.4649486411196326

Epoch: 6| Step: 3
Training loss: 2.3027844864405376
Validation loss: 2.4413751268597523

Epoch: 6| Step: 4
Training loss: 2.2607496751698357
Validation loss: 2.4524702205846327

Epoch: 6| Step: 5
Training loss: 2.830226054514607
Validation loss: 2.4182514585543595

Epoch: 6| Step: 6
Training loss: 2.6888502410689377
Validation loss: 2.45259362913657

Epoch: 6| Step: 7
Training loss: 2.5064172398806814
Validation loss: 2.4421379768076736

Epoch: 6| Step: 8
Training loss: 2.6257575622234683
Validation loss: 2.4409161318143813

Epoch: 6| Step: 9
Training loss: 3.1048071966341944
Validation loss: 2.447221771940188

Epoch: 6| Step: 10
Training loss: 3.360598603014209
Validation loss: 2.4464126581563286

Epoch: 6| Step: 11
Training loss: 2.830052683039971
Validation loss: 2.456546087537366

Epoch: 6| Step: 12
Training loss: 3.0347331376877245
Validation loss: 2.432429681521297

Epoch: 6| Step: 13
Training loss: 2.2839346987112603
Validation loss: 2.441569878589201

Epoch: 26| Step: 0
Training loss: 2.391439261795893
Validation loss: 2.447861757799345

Epoch: 6| Step: 1
Training loss: 2.5075516134257616
Validation loss: 2.4483895149933996

Epoch: 6| Step: 2
Training loss: 2.8827537799427003
Validation loss: 2.431273035740164

Epoch: 6| Step: 3
Training loss: 2.52273124519554
Validation loss: 2.4430820194910017

Epoch: 6| Step: 4
Training loss: 3.2837140232143973
Validation loss: 2.4622732094791044

Epoch: 6| Step: 5
Training loss: 2.978406280011348
Validation loss: 2.4462552033621807

Epoch: 6| Step: 6
Training loss: 2.9033395283483667
Validation loss: 2.447829710271858

Epoch: 6| Step: 7
Training loss: 2.876967710417882
Validation loss: 2.433723717628547

Epoch: 6| Step: 8
Training loss: 3.529342000870197
Validation loss: 2.4377791768356367

Epoch: 6| Step: 9
Training loss: 2.8944426199951407
Validation loss: 2.4324079765358655

Epoch: 6| Step: 10
Training loss: 2.8573861631611512
Validation loss: 2.457678442923956

Epoch: 6| Step: 11
Training loss: 1.7023046074998116
Validation loss: 2.443095196629634

Epoch: 6| Step: 12
Training loss: 2.5829958131219395
Validation loss: 2.4361056264426124

Epoch: 6| Step: 13
Training loss: 3.1605722273829455
Validation loss: 2.441237649588281

Epoch: 27| Step: 0
Training loss: 2.333549682487173
Validation loss: 2.4569323190317744

Epoch: 6| Step: 1
Training loss: 3.3917920613521066
Validation loss: 2.462590400731734

Epoch: 6| Step: 2
Training loss: 2.7881400669274843
Validation loss: 2.4554988238174063

Epoch: 6| Step: 3
Training loss: 2.257476464842734
Validation loss: 2.456180124009863

Epoch: 6| Step: 4
Training loss: 2.9593279245694823
Validation loss: 2.4332637191414044

Epoch: 6| Step: 5
Training loss: 3.061190422478376
Validation loss: 2.435338338893434

Epoch: 6| Step: 6
Training loss: 2.919794821127908
Validation loss: 2.4359809226955336

Epoch: 6| Step: 7
Training loss: 2.463801676045184
Validation loss: 2.4294435729287547

Epoch: 6| Step: 8
Training loss: 2.64341831877212
Validation loss: 2.435197719995289

Epoch: 6| Step: 9
Training loss: 2.8954704295120526
Validation loss: 2.4296040903965768

Epoch: 6| Step: 10
Training loss: 3.008796828079479
Validation loss: 2.428712249555873

Epoch: 6| Step: 11
Training loss: 2.8619851461434256
Validation loss: 2.4415386934841594

Epoch: 6| Step: 12
Training loss: 3.0391684074874536
Validation loss: 2.4623125445005853

Epoch: 6| Step: 13
Training loss: 2.128146366678714
Validation loss: 2.4472710209399504

Epoch: 28| Step: 0
Training loss: 2.2178093434042987
Validation loss: 2.442891601247644

Epoch: 6| Step: 1
Training loss: 2.526551772586128
Validation loss: 2.433792187545521

Epoch: 6| Step: 2
Training loss: 2.414676526443233
Validation loss: 2.4220584023241436

Epoch: 6| Step: 3
Training loss: 3.1680894633482684
Validation loss: 2.4457222770371865

Epoch: 6| Step: 4
Training loss: 2.9900541743343214
Validation loss: 2.4390321811391025

Epoch: 6| Step: 5
Training loss: 2.656874100912225
Validation loss: 2.4397921390545196

Epoch: 6| Step: 6
Training loss: 3.50561862639611
Validation loss: 2.4244225775631625

Epoch: 6| Step: 7
Training loss: 2.3810455274526965
Validation loss: 2.4391232289995077

Epoch: 6| Step: 8
Training loss: 2.861901672998757
Validation loss: 2.4483083739286933

Epoch: 6| Step: 9
Training loss: 2.3015427681154126
Validation loss: 2.429219602632368

Epoch: 6| Step: 10
Training loss: 2.788467129972402
Validation loss: 2.4453600157083812

Epoch: 6| Step: 11
Training loss: 3.5057005099799707
Validation loss: 2.4548576887052858

Epoch: 6| Step: 12
Training loss: 2.8058187174569804
Validation loss: 2.425635388151778

Epoch: 6| Step: 13
Training loss: 2.7963681747437126
Validation loss: 2.458067866265417

Epoch: 29| Step: 0
Training loss: 2.8201769006313224
Validation loss: 2.448961143411609

Epoch: 6| Step: 1
Training loss: 2.290719848167935
Validation loss: 2.4332864658507636

Epoch: 6| Step: 2
Training loss: 2.499928664143372
Validation loss: 2.4439415604558623

Epoch: 6| Step: 3
Training loss: 2.9373477328180284
Validation loss: 2.4712588136904317

Epoch: 6| Step: 4
Training loss: 3.123593128138069
Validation loss: 2.443401649072473

Epoch: 6| Step: 5
Training loss: 3.758424324462309
Validation loss: 2.4451339642350667

Epoch: 6| Step: 6
Training loss: 2.758247838227189
Validation loss: 2.4219017628048776

Epoch: 6| Step: 7
Training loss: 2.7008789962899296
Validation loss: 2.421118880156523

Epoch: 6| Step: 8
Training loss: 2.5635791460129167
Validation loss: 2.4317447238619376

Epoch: 6| Step: 9
Training loss: 2.9821206410986134
Validation loss: 2.440864848824167

Epoch: 6| Step: 10
Training loss: 3.163125886161362
Validation loss: 2.4336010503158216

Epoch: 6| Step: 11
Training loss: 2.601049515836211
Validation loss: 2.455527488830708

Epoch: 6| Step: 12
Training loss: 2.658636479646701
Validation loss: 2.4461479124184504

Epoch: 6| Step: 13
Training loss: 1.8506229125181495
Validation loss: 2.4377445059400404

Epoch: 30| Step: 0
Training loss: 2.517450464058679
Validation loss: 2.4557608212424014

Epoch: 6| Step: 1
Training loss: 2.9730838663568844
Validation loss: 2.458979713706394

Epoch: 6| Step: 2
Training loss: 2.728316215814091
Validation loss: 2.455572314828438

Epoch: 6| Step: 3
Training loss: 2.683742557936408
Validation loss: 2.4681373449712702

Epoch: 6| Step: 4
Training loss: 2.8408971876415223
Validation loss: 2.4511229658540064

Epoch: 6| Step: 5
Training loss: 2.830066667721805
Validation loss: 2.4238101189459247

Epoch: 6| Step: 6
Training loss: 2.3449745539604314
Validation loss: 2.440624779373274

Epoch: 6| Step: 7
Training loss: 3.3880723758249505
Validation loss: 2.4619328155625335

Epoch: 6| Step: 8
Training loss: 3.0741024164972544
Validation loss: 2.4576998120798317

Epoch: 6| Step: 9
Training loss: 3.0996446036612566
Validation loss: 2.447112516138103

Epoch: 6| Step: 10
Training loss: 3.0165426014491197
Validation loss: 2.420408791501186

Epoch: 6| Step: 11
Training loss: 1.9703872170374779
Validation loss: 2.422134626844056

Epoch: 6| Step: 12
Training loss: 3.005520826958978
Validation loss: 2.4545246768881803

Epoch: 6| Step: 13
Training loss: 2.3756583455793163
Validation loss: 2.4354872680099255

Epoch: 31| Step: 0
Training loss: 2.767288655909779
Validation loss: 2.4449378627619676

Epoch: 6| Step: 1
Training loss: 3.2092598572220288
Validation loss: 2.435494680542982

Epoch: 6| Step: 2
Training loss: 3.014385700052606
Validation loss: 2.4483082870188175

Epoch: 6| Step: 3
Training loss: 2.795777430498975
Validation loss: 2.4325152116544033

Epoch: 6| Step: 4
Training loss: 3.0168682164538145
Validation loss: 2.4457309939703116

Epoch: 6| Step: 5
Training loss: 1.8026324043902202
Validation loss: 2.4340951717997603

Epoch: 6| Step: 6
Training loss: 2.1209780835391183
Validation loss: 2.455918701076211

Epoch: 6| Step: 7
Training loss: 2.985636180004695
Validation loss: 2.4440165140208556

Epoch: 6| Step: 8
Training loss: 2.4759929013990165
Validation loss: 2.463265701929216

Epoch: 6| Step: 9
Training loss: 3.034720410410216
Validation loss: 2.443991992169521

Epoch: 6| Step: 10
Training loss: 2.6741377858089987
Validation loss: 2.4265441165278925

Epoch: 6| Step: 11
Training loss: 3.1624049236495706
Validation loss: 2.4345800728824774

Epoch: 6| Step: 12
Training loss: 2.8833635832094635
Validation loss: 2.422058357869019

Epoch: 6| Step: 13
Training loss: 3.3428046770834308
Validation loss: 2.445146414741045

Epoch: 32| Step: 0
Training loss: 2.43109060907446
Validation loss: 2.466602384577181

Epoch: 6| Step: 1
Training loss: 3.872721340485872
Validation loss: 2.427162627181342

Epoch: 6| Step: 2
Training loss: 3.2764313692949827
Validation loss: 2.445661836263933

Epoch: 6| Step: 3
Training loss: 2.6978393209753126
Validation loss: 2.435139971545737

Epoch: 6| Step: 4
Training loss: 3.24304746689878
Validation loss: 2.440990709344019

Epoch: 6| Step: 5
Training loss: 3.059790054549083
Validation loss: 2.462162058097258

Epoch: 6| Step: 6
Training loss: 2.9229887609275527
Validation loss: 2.444806939347589

Epoch: 6| Step: 7
Training loss: 2.6004051406361235
Validation loss: 2.4433839551365493

Epoch: 6| Step: 8
Training loss: 2.0243597690425044
Validation loss: 2.445603057594418

Epoch: 6| Step: 9
Training loss: 2.560570152985818
Validation loss: 2.415975780266105

Epoch: 6| Step: 10
Training loss: 2.134200430526883
Validation loss: 2.45528933647338

Epoch: 6| Step: 11
Training loss: 2.399074208990147
Validation loss: 2.450312055931079

Epoch: 6| Step: 12
Training loss: 3.1998174198039124
Validation loss: 2.463157753106245

Epoch: 6| Step: 13
Training loss: 2.0547524793670022
Validation loss: 2.455217100426559

Epoch: 33| Step: 0
Training loss: 2.1011954628733287
Validation loss: 2.4348294637594075

Epoch: 6| Step: 1
Training loss: 2.363581866856532
Validation loss: 2.42524754662866

Epoch: 6| Step: 2
Training loss: 2.654581421006324
Validation loss: 2.4455285782644496

Epoch: 6| Step: 3
Training loss: 3.106786829653304
Validation loss: 2.4106368822522795

Epoch: 6| Step: 4
Training loss: 2.7765331775531634
Validation loss: 2.447854309404892

Epoch: 6| Step: 5
Training loss: 2.8615527588871275
Validation loss: 2.4538637983008242

Epoch: 6| Step: 6
Training loss: 2.9094142300658006
Validation loss: 2.4396072704578784

Epoch: 6| Step: 7
Training loss: 3.2862779151656616
Validation loss: 2.4506504674520695

Epoch: 6| Step: 8
Training loss: 2.7723020996515113
Validation loss: 2.44223737218239

Epoch: 6| Step: 9
Training loss: 2.4717473051361862
Validation loss: 2.4374314428153485

Epoch: 6| Step: 10
Training loss: 3.14038631614004
Validation loss: 2.422764848929531

Epoch: 6| Step: 11
Training loss: 3.0656351662874566
Validation loss: 2.441062847047596

Epoch: 6| Step: 12
Training loss: 2.8236410096147098
Validation loss: 2.4480743935135396

Epoch: 6| Step: 13
Training loss: 2.5502196273468445
Validation loss: 2.456665521017211

Epoch: 34| Step: 0
Training loss: 2.807627038026198
Validation loss: 2.4510201544193477

Epoch: 6| Step: 1
Training loss: 2.3497067856989657
Validation loss: 2.424721244205655

Epoch: 6| Step: 2
Training loss: 3.5358725480719038
Validation loss: 2.4273935839982617

Epoch: 6| Step: 3
Training loss: 2.3518471783146295
Validation loss: 2.4223432614169496

Epoch: 6| Step: 4
Training loss: 2.067214202131728
Validation loss: 2.440644426039921

Epoch: 6| Step: 5
Training loss: 2.873121560148413
Validation loss: 2.4422796803715934

Epoch: 6| Step: 6
Training loss: 2.8319484093487457
Validation loss: 2.429842527685274

Epoch: 6| Step: 7
Training loss: 2.728808071271713
Validation loss: 2.442227044065297

Epoch: 6| Step: 8
Training loss: 2.9219406038966973
Validation loss: 2.4314105186635095

Epoch: 6| Step: 9
Training loss: 3.0366719314188986
Validation loss: 2.4324104975871372

Epoch: 6| Step: 10
Training loss: 2.727613361356014
Validation loss: 2.424319084290006

Epoch: 6| Step: 11
Training loss: 2.9693683181138315
Validation loss: 2.448649054733544

Epoch: 6| Step: 12
Training loss: 3.0358260703141773
Validation loss: 2.440869369311683

Epoch: 6| Step: 13
Training loss: 2.9167647935618524
Validation loss: 2.459668519756934

Epoch: 35| Step: 0
Training loss: 3.229858931304197
Validation loss: 2.4388817750821414

Epoch: 6| Step: 1
Training loss: 2.483740287650422
Validation loss: 2.4415934341044823

Epoch: 6| Step: 2
Training loss: 2.7432579277060993
Validation loss: 2.445640440018976

Epoch: 6| Step: 3
Training loss: 2.3977300240411714
Validation loss: 2.423311046072946

Epoch: 6| Step: 4
Training loss: 2.404362718686141
Validation loss: 2.4469483236492433

Epoch: 6| Step: 5
Training loss: 2.6092285297881443
Validation loss: 2.4136873181949916

Epoch: 6| Step: 6
Training loss: 2.7327970474879377
Validation loss: 2.4738777369446545

Epoch: 6| Step: 7
Training loss: 2.6712167386316845
Validation loss: 2.440859893503186

Epoch: 6| Step: 8
Training loss: 2.605096473005749
Validation loss: 2.4428290948985474

Epoch: 6| Step: 9
Training loss: 3.0742387589187636
Validation loss: 2.4310900248687224

Epoch: 6| Step: 10
Training loss: 2.579022800293001
Validation loss: 2.4294966560852003

Epoch: 6| Step: 11
Training loss: 3.7117723670255236
Validation loss: 2.437055931732591

Epoch: 6| Step: 12
Training loss: 3.158488773221524
Validation loss: 2.4578039003484373

Epoch: 6| Step: 13
Training loss: 2.463395891991117
Validation loss: 2.430269660445079

Epoch: 36| Step: 0
Training loss: 3.3011836963424304
Validation loss: 2.4547304473144664

Epoch: 6| Step: 1
Training loss: 2.3458246839599877
Validation loss: 2.4399565756497754

Epoch: 6| Step: 2
Training loss: 2.2239231461206286
Validation loss: 2.4427136473336946

Epoch: 6| Step: 3
Training loss: 1.7779746476415939
Validation loss: 2.4326648658168417

Epoch: 6| Step: 4
Training loss: 2.9783609719071147
Validation loss: 2.4411214845685665

Epoch: 6| Step: 5
Training loss: 3.505504503061419
Validation loss: 2.4478123080791976

Epoch: 6| Step: 6
Training loss: 2.9629694828208963
Validation loss: 2.428264025652641

Epoch: 6| Step: 7
Training loss: 2.5077404356492585
Validation loss: 2.44774975300341

Epoch: 6| Step: 8
Training loss: 2.9071427481515046
Validation loss: 2.4370264919157854

Epoch: 6| Step: 9
Training loss: 3.1470733792187007
Validation loss: 2.4212127986965197

Epoch: 6| Step: 10
Training loss: 3.090062882360707
Validation loss: 2.4627607382129466

Epoch: 6| Step: 11
Training loss: 2.2188129684754205
Validation loss: 2.429712296526772

Epoch: 6| Step: 12
Training loss: 2.734487040812827
Validation loss: 2.4275083212478945

Epoch: 6| Step: 13
Training loss: 3.147738925873457
Validation loss: 2.41629025883991

Epoch: 37| Step: 0
Training loss: 2.285144564606875
Validation loss: 2.4323214866639082

Epoch: 6| Step: 1
Training loss: 2.615275763710486
Validation loss: 2.406245262121089

Epoch: 6| Step: 2
Training loss: 3.0514762370099855
Validation loss: 2.434887168508689

Epoch: 6| Step: 3
Training loss: 3.060184615556163
Validation loss: 2.4224633102037023

Epoch: 6| Step: 4
Training loss: 3.4033608038749783
Validation loss: 2.442200548607955

Epoch: 6| Step: 5
Training loss: 2.77773766064815
Validation loss: 2.430501613067189

Epoch: 6| Step: 6
Training loss: 2.4827389878468704
Validation loss: 2.4384544929059833

Epoch: 6| Step: 7
Training loss: 2.6872168103431875
Validation loss: 2.4242873864698127

Epoch: 6| Step: 8
Training loss: 3.0777383164478507
Validation loss: 2.429158156553887

Epoch: 6| Step: 9
Training loss: 2.2590451159318246
Validation loss: 2.431361743946565

Epoch: 6| Step: 10
Training loss: 2.5471929808028713
Validation loss: 2.4251342563742644

Epoch: 6| Step: 11
Training loss: 2.6896525788990338
Validation loss: 2.441480041307547

Epoch: 6| Step: 12
Training loss: 3.31260623401658
Validation loss: 2.426256493074228

Epoch: 6| Step: 13
Training loss: 2.5337761412496693
Validation loss: 2.439173102917433

Epoch: 38| Step: 0
Training loss: 2.510044708765651
Validation loss: 2.443926399007054

Epoch: 6| Step: 1
Training loss: 2.953486990565962
Validation loss: 2.41789361347062

Epoch: 6| Step: 2
Training loss: 2.361323884340175
Validation loss: 2.440572648344778

Epoch: 6| Step: 3
Training loss: 2.6773294912091585
Validation loss: 2.4362106657456164

Epoch: 6| Step: 4
Training loss: 2.835387121341397
Validation loss: 2.4575844439699788

Epoch: 6| Step: 5
Training loss: 2.7723887005163053
Validation loss: 2.4399813466171874

Epoch: 6| Step: 6
Training loss: 3.215794511929601
Validation loss: 2.436820368521965

Epoch: 6| Step: 7
Training loss: 2.795270747410806
Validation loss: 2.4438666260967437

Epoch: 6| Step: 8
Training loss: 2.231705785194348
Validation loss: 2.441876292537109

Epoch: 6| Step: 9
Training loss: 2.81208052156072
Validation loss: 2.4478591667828105

Epoch: 6| Step: 10
Training loss: 3.1987167169435162
Validation loss: 2.430944542724326

Epoch: 6| Step: 11
Training loss: 2.9832670541856214
Validation loss: 2.456449641758707

Epoch: 6| Step: 12
Training loss: 2.7775407552261298
Validation loss: 2.4310445460247805

Epoch: 6| Step: 13
Training loss: 3.011589235521794
Validation loss: 2.4087253623699554

Epoch: 39| Step: 0
Training loss: 3.2381864195629313
Validation loss: 2.433215817605761

Epoch: 6| Step: 1
Training loss: 2.875654726527712
Validation loss: 2.446164767854738

Epoch: 6| Step: 2
Training loss: 2.4921405273602795
Validation loss: 2.4500395887372863

Epoch: 6| Step: 3
Training loss: 2.685764018209945
Validation loss: 2.455452987394111

Epoch: 6| Step: 4
Training loss: 2.4124519382887657
Validation loss: 2.441591303680536

Epoch: 6| Step: 5
Training loss: 2.6015981494190816
Validation loss: 2.4338107865162844

Epoch: 6| Step: 6
Training loss: 2.220542076531643
Validation loss: 2.462918242195541

Epoch: 6| Step: 7
Training loss: 2.419665700978585
Validation loss: 2.438046385993745

Epoch: 6| Step: 8
Training loss: 3.3536728837072176
Validation loss: 2.4256402213253883

Epoch: 6| Step: 9
Training loss: 3.1608976383307454
Validation loss: 2.4407534798111863

Epoch: 6| Step: 10
Training loss: 2.5031843885598763
Validation loss: 2.424813126507088

Epoch: 6| Step: 11
Training loss: 2.718459015105632
Validation loss: 2.427665815541217

Epoch: 6| Step: 12
Training loss: 3.242498102591966
Validation loss: 2.437348266666461

Epoch: 6| Step: 13
Training loss: 3.4803350318027935
Validation loss: 2.419169620957774

Epoch: 40| Step: 0
Training loss: 2.9286576966130173
Validation loss: 2.448907994340025

Epoch: 6| Step: 1
Training loss: 3.9059240586671926
Validation loss: 2.460960870134729

Epoch: 6| Step: 2
Training loss: 2.4833127995949935
Validation loss: 2.4490834402044173

Epoch: 6| Step: 3
Training loss: 3.124874570236686
Validation loss: 2.4287326575806656

Epoch: 6| Step: 4
Training loss: 2.3649246932897547
Validation loss: 2.4331680868675583

Epoch: 6| Step: 5
Training loss: 2.6762634288409948
Validation loss: 2.424745522701722

Epoch: 6| Step: 6
Training loss: 2.360226105025894
Validation loss: 2.448669821121559

Epoch: 6| Step: 7
Training loss: 2.8099725174832635
Validation loss: 2.4387803408044544

Epoch: 6| Step: 8
Training loss: 2.4030466927616643
Validation loss: 2.4439011466754437

Epoch: 6| Step: 9
Training loss: 2.7534876295580393
Validation loss: 2.4325633725413174

Epoch: 6| Step: 10
Training loss: 2.7743661964312922
Validation loss: 2.441113299376335

Epoch: 6| Step: 11
Training loss: 2.5826650698585807
Validation loss: 2.430781739742196

Epoch: 6| Step: 12
Training loss: 2.8182296035720205
Validation loss: 2.4390428969710074

Epoch: 6| Step: 13
Training loss: 2.84763983995186
Validation loss: 2.4155696766675203

Epoch: 41| Step: 0
Training loss: 3.26367041395464
Validation loss: 2.4436338732527934

Epoch: 6| Step: 1
Training loss: 3.4617924376995606
Validation loss: 2.4323330562761005

Epoch: 6| Step: 2
Training loss: 2.8270711041614462
Validation loss: 2.437060660205716

Epoch: 6| Step: 3
Training loss: 3.119861345144564
Validation loss: 2.4304662513636344

Epoch: 6| Step: 4
Training loss: 3.106485529054356
Validation loss: 2.4362626933344647

Epoch: 6| Step: 5
Training loss: 2.366910710535044
Validation loss: 2.412893909071472

Epoch: 6| Step: 6
Training loss: 2.6249026325878084
Validation loss: 2.450515001324559

Epoch: 6| Step: 7
Training loss: 2.058692423068063
Validation loss: 2.396425681343973

Epoch: 6| Step: 8
Training loss: 2.3871884047807024
Validation loss: 2.40614047664869

Epoch: 6| Step: 9
Training loss: 2.7584205367795263
Validation loss: 2.4305701273429094

Epoch: 6| Step: 10
Training loss: 2.8636271227512737
Validation loss: 2.4390307322120677

Epoch: 6| Step: 11
Training loss: 2.5458889790232337
Validation loss: 2.4311655317695533

Epoch: 6| Step: 12
Training loss: 2.2820213790413653
Validation loss: 2.4342746667237867

Epoch: 6| Step: 13
Training loss: 3.2543663624648316
Validation loss: 2.439106204031091

Epoch: 42| Step: 0
Training loss: 2.835340536881418
Validation loss: 2.45170455156429

Epoch: 6| Step: 1
Training loss: 2.959079773627175
Validation loss: 2.4413419787407067

Epoch: 6| Step: 2
Training loss: 2.6774062519775854
Validation loss: 2.431718959173405

Epoch: 6| Step: 3
Training loss: 3.2619525026176417
Validation loss: 2.450674321695535

Epoch: 6| Step: 4
Training loss: 2.7235766131818457
Validation loss: 2.464167249967477

Epoch: 6| Step: 5
Training loss: 3.207431038808688
Validation loss: 2.4360926824785873

Epoch: 6| Step: 6
Training loss: 2.5370508294402243
Validation loss: 2.4242987628360804

Epoch: 6| Step: 7
Training loss: 2.8608251344374582
Validation loss: 2.443742273282438

Epoch: 6| Step: 8
Training loss: 2.849845975762776
Validation loss: 2.450949571758341

Epoch: 6| Step: 9
Training loss: 2.2522160954709123
Validation loss: 2.443220434399861

Epoch: 6| Step: 10
Training loss: 2.7859677367139386
Validation loss: 2.4390729735738317

Epoch: 6| Step: 11
Training loss: 3.1005308804166876
Validation loss: 2.435903732763625

Epoch: 6| Step: 12
Training loss: 2.3423557457735367
Validation loss: 2.4448438972070425

Epoch: 6| Step: 13
Training loss: 2.2100025801837755
Validation loss: 2.4314517995600653

Epoch: 43| Step: 0
Training loss: 2.7107180635758734
Validation loss: 2.4302039711818044

Epoch: 6| Step: 1
Training loss: 3.400527767283975
Validation loss: 2.437807238883032

Epoch: 6| Step: 2
Training loss: 2.228932935873866
Validation loss: 2.453187563268013

Epoch: 6| Step: 3
Training loss: 2.955571851310964
Validation loss: 2.4213747151540757

Epoch: 6| Step: 4
Training loss: 3.047970305931448
Validation loss: 2.440097446143254

Epoch: 6| Step: 5
Training loss: 2.9450568535956387
Validation loss: 2.417576113430772

Epoch: 6| Step: 6
Training loss: 3.0167291230819484
Validation loss: 2.447470892797286

Epoch: 6| Step: 7
Training loss: 3.2165353804234185
Validation loss: 2.4451795962093765

Epoch: 6| Step: 8
Training loss: 2.7240414044542627
Validation loss: 2.430387181682425

Epoch: 6| Step: 9
Training loss: 2.6851732694670836
Validation loss: 2.4339169539292342

Epoch: 6| Step: 10
Training loss: 1.942446639065711
Validation loss: 2.4372212945054157

Epoch: 6| Step: 11
Training loss: 2.93478267872391
Validation loss: 2.4306459056353567

Epoch: 6| Step: 12
Training loss: 2.044126339481064
Validation loss: 2.4292842943383373

Epoch: 6| Step: 13
Training loss: 2.910654975092926
Validation loss: 2.4326531059828422

Epoch: 44| Step: 0
Training loss: 2.724380275706333
Validation loss: 2.4345501187351664

Epoch: 6| Step: 1
Training loss: 2.7510599347834113
Validation loss: 2.431160226622725

Epoch: 6| Step: 2
Training loss: 2.7237373296435092
Validation loss: 2.437886098400215

Epoch: 6| Step: 3
Training loss: 2.5304000746243056
Validation loss: 2.4526354001554007

Epoch: 6| Step: 4
Training loss: 2.959391892575286
Validation loss: 2.4449740290957402

Epoch: 6| Step: 5
Training loss: 1.5596297031242095
Validation loss: 2.4553041150413524

Epoch: 6| Step: 6
Training loss: 2.840475859467095
Validation loss: 2.4403184057821283

Epoch: 6| Step: 7
Training loss: 2.9029371186429307
Validation loss: 2.4376015170034258

Epoch: 6| Step: 8
Training loss: 2.9619547925715044
Validation loss: 2.446140455661306

Epoch: 6| Step: 9
Training loss: 2.4329232506113874
Validation loss: 2.4241656270324548

Epoch: 6| Step: 10
Training loss: 3.2592541914556903
Validation loss: 2.4400425595415243

Epoch: 6| Step: 11
Training loss: 3.1679591168164265
Validation loss: 2.433352317376393

Epoch: 6| Step: 12
Training loss: 3.4572224192376027
Validation loss: 2.420314164921993

Epoch: 6| Step: 13
Training loss: 1.8063672327982405
Validation loss: 2.4171922223281594

Epoch: 45| Step: 0
Training loss: 2.6313983503913625
Validation loss: 2.4569892459093987

Epoch: 6| Step: 1
Training loss: 2.3849992567337126
Validation loss: 2.4167818838312325

Epoch: 6| Step: 2
Training loss: 2.7835150935354998
Validation loss: 2.429817037192812

Epoch: 6| Step: 3
Training loss: 3.0945640032546198
Validation loss: 2.4497832823771533

Epoch: 6| Step: 4
Training loss: 2.8227996426545756
Validation loss: 2.423464902299996

Epoch: 6| Step: 5
Training loss: 3.1681695350019776
Validation loss: 2.4533553560377785

Epoch: 6| Step: 6
Training loss: 2.0342728643371246
Validation loss: 2.4338383471129448

Epoch: 6| Step: 7
Training loss: 2.8064826172250084
Validation loss: 2.4398141734321657

Epoch: 6| Step: 8
Training loss: 2.2265113155438105
Validation loss: 2.437133408107398

Epoch: 6| Step: 9
Training loss: 2.5086407110855222
Validation loss: 2.4290109227937378

Epoch: 6| Step: 10
Training loss: 3.020741603817866
Validation loss: 2.4288518997592314

Epoch: 6| Step: 11
Training loss: 2.570188664291018
Validation loss: 2.4351681171706714

Epoch: 6| Step: 12
Training loss: 3.5191448308610678
Validation loss: 2.437536285105075

Epoch: 6| Step: 13
Training loss: 3.4777064809373592
Validation loss: 2.4475212868776386

Epoch: 46| Step: 0
Training loss: 2.9231820676402847
Validation loss: 2.412633448272175

Epoch: 6| Step: 1
Training loss: 2.926583316401041
Validation loss: 2.4214655344444176

Epoch: 6| Step: 2
Training loss: 2.5439178523889816
Validation loss: 2.437299741728096

Epoch: 6| Step: 3
Training loss: 3.5076961370593955
Validation loss: 2.446457045477723

Epoch: 6| Step: 4
Training loss: 2.467712667280787
Validation loss: 2.4476179305004866

Epoch: 6| Step: 5
Training loss: 2.9860691235175394
Validation loss: 2.442518523932751

Epoch: 6| Step: 6
Training loss: 3.061883669745744
Validation loss: 2.4397200473166563

Epoch: 6| Step: 7
Training loss: 2.38513690579539
Validation loss: 2.450346887460889

Epoch: 6| Step: 8
Training loss: 2.3557926290598696
Validation loss: 2.4407998700922433

Epoch: 6| Step: 9
Training loss: 3.01059821790266
Validation loss: 2.4371106562785716

Epoch: 6| Step: 10
Training loss: 2.4058295539039634
Validation loss: 2.4215263038932995

Epoch: 6| Step: 11
Training loss: 2.4229817476549416
Validation loss: 2.423041576581596

Epoch: 6| Step: 12
Training loss: 2.8781863260932155
Validation loss: 2.4530380847071114

Epoch: 6| Step: 13
Training loss: 3.203969258021627
Validation loss: 2.426295649127326

Epoch: 47| Step: 0
Training loss: 3.2866341150411533
Validation loss: 2.4323799554863528

Epoch: 6| Step: 1
Training loss: 2.8767267100527736
Validation loss: 2.427023157582047

Epoch: 6| Step: 2
Training loss: 2.8376041275613866
Validation loss: 2.411319542007907

Epoch: 6| Step: 3
Training loss: 2.1052978672755156
Validation loss: 2.4578277008106233

Epoch: 6| Step: 4
Training loss: 2.8988790252908943
Validation loss: 2.4647036105122284

Epoch: 6| Step: 5
Training loss: 3.141904352102984
Validation loss: 2.439904043681815

Epoch: 6| Step: 6
Training loss: 3.574220084101527
Validation loss: 2.4536234741946106

Epoch: 6| Step: 7
Training loss: 2.5734782564865175
Validation loss: 2.4531638369213895

Epoch: 6| Step: 8
Training loss: 2.4569217918285697
Validation loss: 2.4531753280929047

Epoch: 6| Step: 9
Training loss: 3.302610642040245
Validation loss: 2.4422737325489274

Epoch: 6| Step: 10
Training loss: 2.564379305332706
Validation loss: 2.4524662755061324

Epoch: 6| Step: 11
Training loss: 1.8427307415065717
Validation loss: 2.4288366794760203

Epoch: 6| Step: 12
Training loss: 2.7519584964350523
Validation loss: 2.455894692128844

Epoch: 6| Step: 13
Training loss: 2.237597189555962
Validation loss: 2.45930801252214

Epoch: 48| Step: 0
Training loss: 2.8200338548291954
Validation loss: 2.435712374957313

Epoch: 6| Step: 1
Training loss: 2.95697174617104
Validation loss: 2.4218316499374497

Epoch: 6| Step: 2
Training loss: 2.740017023799387
Validation loss: 2.474756114847555

Epoch: 6| Step: 3
Training loss: 2.8015710203995416
Validation loss: 2.449046260548649

Epoch: 6| Step: 4
Training loss: 2.6334314722372647
Validation loss: 2.4537370152283704

Epoch: 6| Step: 5
Training loss: 3.3240575157358396
Validation loss: 2.4435989339694513

Epoch: 6| Step: 6
Training loss: 3.0274806231659057
Validation loss: 2.43581026805275

Epoch: 6| Step: 7
Training loss: 2.605771529006919
Validation loss: 2.4544267045292196

Epoch: 6| Step: 8
Training loss: 2.7134466075294057
Validation loss: 2.42710348183669

Epoch: 6| Step: 9
Training loss: 2.2884188323175705
Validation loss: 2.4402102833440193

Epoch: 6| Step: 10
Training loss: 2.686860119249798
Validation loss: 2.4413698796014014

Epoch: 6| Step: 11
Training loss: 3.4031329810799433
Validation loss: 2.4715830605513083

Epoch: 6| Step: 12
Training loss: 2.329917678361073
Validation loss: 2.4544593114111475

Epoch: 6| Step: 13
Training loss: 2.1910431640983568
Validation loss: 2.4104415414374896

Epoch: 49| Step: 0
Training loss: 3.345953099699059
Validation loss: 2.437361512142802

Epoch: 6| Step: 1
Training loss: 3.051826092986141
Validation loss: 2.4583628186737467

Epoch: 6| Step: 2
Training loss: 1.9458226526962445
Validation loss: 2.4556573030327993

Epoch: 6| Step: 3
Training loss: 2.9013376045067054
Validation loss: 2.429277992036968

Epoch: 6| Step: 4
Training loss: 2.6014919328712964
Validation loss: 2.443678264888434

Epoch: 6| Step: 5
Training loss: 2.4026974301388635
Validation loss: 2.426713738856555

Epoch: 6| Step: 6
Training loss: 2.645751851747051
Validation loss: 2.4522481615380713

Epoch: 6| Step: 7
Training loss: 3.1177139793690696
Validation loss: 2.4649074126223605

Epoch: 6| Step: 8
Training loss: 2.8335095145108022
Validation loss: 2.426883026086366

Epoch: 6| Step: 9
Training loss: 2.618761005241922
Validation loss: 2.4343598434829636

Epoch: 6| Step: 10
Training loss: 3.0136664953573176
Validation loss: 2.446073294565086

Epoch: 6| Step: 11
Training loss: 2.1267735429094996
Validation loss: 2.453601331351631

Epoch: 6| Step: 12
Training loss: 3.1348181033866704
Validation loss: 2.46069761045573

Epoch: 6| Step: 13
Training loss: 3.010081359422738
Validation loss: 2.4319865290126894

Epoch: 50| Step: 0
Training loss: 3.676677793161452
Validation loss: 2.4427477738553707

Epoch: 6| Step: 1
Training loss: 2.964708649342846
Validation loss: 2.4420094180988796

Epoch: 6| Step: 2
Training loss: 2.7976507038230856
Validation loss: 2.4191592314478596

Epoch: 6| Step: 3
Training loss: 2.103151762793477
Validation loss: 2.440920988820392

Epoch: 6| Step: 4
Training loss: 2.4453441873758113
Validation loss: 2.4239968512195262

Epoch: 6| Step: 5
Training loss: 2.848717511382229
Validation loss: 2.434398158888665

Epoch: 6| Step: 6
Training loss: 3.185468737066666
Validation loss: 2.415468686691319

Epoch: 6| Step: 7
Training loss: 2.0896887035432012
Validation loss: 2.4303038277384594

Epoch: 6| Step: 8
Training loss: 2.436316276106799
Validation loss: 2.440636570123276

Epoch: 6| Step: 9
Training loss: 2.750672258223772
Validation loss: 2.4275429889066955

Epoch: 6| Step: 10
Training loss: 2.9132826838102366
Validation loss: 2.4254736557107472

Epoch: 6| Step: 11
Training loss: 2.947012413275768
Validation loss: 2.461098985766278

Epoch: 6| Step: 12
Training loss: 2.7954309241177238
Validation loss: 2.436726001563442

Epoch: 6| Step: 13
Training loss: 2.4534580466285054
Validation loss: 2.4202096612647277

Epoch: 51| Step: 0
Training loss: 2.670056979396484
Validation loss: 2.4423581461876513

Epoch: 6| Step: 1
Training loss: 2.778676331676556
Validation loss: 2.444387966244985

Epoch: 6| Step: 2
Training loss: 2.3856234620478856
Validation loss: 2.456779367034587

Epoch: 6| Step: 3
Training loss: 2.963589328769208
Validation loss: 2.441579591040571

Epoch: 6| Step: 4
Training loss: 2.831475565095419
Validation loss: 2.447927838267063

Epoch: 6| Step: 5
Training loss: 2.7019657890813886
Validation loss: 2.4114889180476773

Epoch: 6| Step: 6
Training loss: 2.9491526211025687
Validation loss: 2.4254569862592765

Epoch: 6| Step: 7
Training loss: 2.3543829241873198
Validation loss: 2.4284786533842557

Epoch: 6| Step: 8
Training loss: 3.3480268560355593
Validation loss: 2.4225845424872596

Epoch: 6| Step: 9
Training loss: 3.1054618403519827
Validation loss: 2.4418776090675576

Epoch: 6| Step: 10
Training loss: 2.883571121759546
Validation loss: 2.426199615398806

Epoch: 6| Step: 11
Training loss: 2.843622561103975
Validation loss: 2.427217043512367

Epoch: 6| Step: 12
Training loss: 2.644922044466977
Validation loss: 2.434873745848638

Epoch: 6| Step: 13
Training loss: 2.358128907023251
Validation loss: 2.4336299858976553

Epoch: 52| Step: 0
Training loss: 2.3523604854519267
Validation loss: 2.438541689872378

Epoch: 6| Step: 1
Training loss: 2.594922823190121
Validation loss: 2.4280319025611945

Epoch: 6| Step: 2
Training loss: 2.446195497836437
Validation loss: 2.415747951267403

Epoch: 6| Step: 3
Training loss: 2.9210600506944706
Validation loss: 2.45310613989283

Epoch: 6| Step: 4
Training loss: 2.149123647534636
Validation loss: 2.425548391493818

Epoch: 6| Step: 5
Training loss: 3.1100220893492434
Validation loss: 2.4317376267132222

Epoch: 6| Step: 6
Training loss: 3.4266638842124872
Validation loss: 2.4332933361791107

Epoch: 6| Step: 7
Training loss: 3.450636713733217
Validation loss: 2.433118715723698

Epoch: 6| Step: 8
Training loss: 2.991476666749229
Validation loss: 2.4427293930092193

Epoch: 6| Step: 9
Training loss: 2.452253636420765
Validation loss: 2.4404091062640885

Epoch: 6| Step: 10
Training loss: 2.205559866276694
Validation loss: 2.427001258476027

Epoch: 6| Step: 11
Training loss: 3.0663544668696896
Validation loss: 2.421507761962155

Epoch: 6| Step: 12
Training loss: 2.621373168787508
Validation loss: 2.421873022654901

Epoch: 6| Step: 13
Training loss: 3.116038022075002
Validation loss: 2.4303948249546474

Epoch: 53| Step: 0
Training loss: 2.6721742696638655
Validation loss: 2.410658445078863

Epoch: 6| Step: 1
Training loss: 3.1617275335002777
Validation loss: 2.434479382900705

Epoch: 6| Step: 2
Training loss: 2.7093253935218224
Validation loss: 2.4298889395301737

Epoch: 6| Step: 3
Training loss: 2.5034331590412213
Validation loss: 2.4351493759158527

Epoch: 6| Step: 4
Training loss: 3.087277201285645
Validation loss: 2.4366710337585733

Epoch: 6| Step: 5
Training loss: 2.9387736704338354
Validation loss: 2.428759243488785

Epoch: 6| Step: 6
Training loss: 3.068304050463994
Validation loss: 2.446055767732984

Epoch: 6| Step: 7
Training loss: 2.687149557286512
Validation loss: 2.4298563342203328

Epoch: 6| Step: 8
Training loss: 3.140294603267741
Validation loss: 2.4382817760849584

Epoch: 6| Step: 9
Training loss: 2.578462058488868
Validation loss: 2.439634035770233

Epoch: 6| Step: 10
Training loss: 2.836438515704005
Validation loss: 2.4384901992477195

Epoch: 6| Step: 11
Training loss: 2.4633430470785695
Validation loss: 2.456101177063203

Epoch: 6| Step: 12
Training loss: 2.3832939286964843
Validation loss: 2.4484451809078975

Epoch: 6| Step: 13
Training loss: 2.3145095753928002
Validation loss: 2.4458592433416197

Epoch: 54| Step: 0
Training loss: 2.623491035174202
Validation loss: 2.4532138925141593

Epoch: 6| Step: 1
Training loss: 3.0802677654939536
Validation loss: 2.438017387159958

Epoch: 6| Step: 2
Training loss: 2.755639276292331
Validation loss: 2.4395239866925618

Epoch: 6| Step: 3
Training loss: 2.6292484961912828
Validation loss: 2.4248335166001955

Epoch: 6| Step: 4
Training loss: 2.1269038030897534
Validation loss: 2.4266036114647207

Epoch: 6| Step: 5
Training loss: 2.49086867687598
Validation loss: 2.431484072441418

Epoch: 6| Step: 6
Training loss: 2.848163072985784
Validation loss: 2.4318590766087578

Epoch: 6| Step: 7
Training loss: 3.066280756040732
Validation loss: 2.449115103912977

Epoch: 6| Step: 8
Training loss: 2.8001737268231457
Validation loss: 2.435674280734013

Epoch: 6| Step: 9
Training loss: 3.4360112521039508
Validation loss: 2.4370603088578617

Epoch: 6| Step: 10
Training loss: 3.208436311992999
Validation loss: 2.4302425857676715

Epoch: 6| Step: 11
Training loss: 2.4612494376953475
Validation loss: 2.4509978693411965

Epoch: 6| Step: 12
Training loss: 2.3805893830198595
Validation loss: 2.4251458105904167

Epoch: 6| Step: 13
Training loss: 3.124947814505674
Validation loss: 2.457596787587929

Epoch: 55| Step: 0
Training loss: 3.480791515792926
Validation loss: 2.4330775042017434

Epoch: 6| Step: 1
Training loss: 2.4570904407740195
Validation loss: 2.4597728603142586

Epoch: 6| Step: 2
Training loss: 2.755775888369322
Validation loss: 2.403481149594118

Epoch: 6| Step: 3
Training loss: 3.3011493184207477
Validation loss: 2.430141604244302

Epoch: 6| Step: 4
Training loss: 2.4621647756675102
Validation loss: 2.420774289496718

Epoch: 6| Step: 5
Training loss: 2.6214943137710196
Validation loss: 2.445103946984002

Epoch: 6| Step: 6
Training loss: 2.689832340673868
Validation loss: 2.4232818613835936

Epoch: 6| Step: 7
Training loss: 2.4011425954219803
Validation loss: 2.454568914904387

Epoch: 6| Step: 8
Training loss: 2.2913092825837786
Validation loss: 2.439552742732378

Epoch: 6| Step: 9
Training loss: 2.610909108088028
Validation loss: 2.4388172491326605

Epoch: 6| Step: 10
Training loss: 2.666454863084173
Validation loss: 2.446627436354073

Epoch: 6| Step: 11
Training loss: 2.637076346555453
Validation loss: 2.4338815997143946

Epoch: 6| Step: 12
Training loss: 2.8070471568072293
Validation loss: 2.4395148109392473

Epoch: 6| Step: 13
Training loss: 3.8722719002931347
Validation loss: 2.4328607447396253

Epoch: 56| Step: 0
Training loss: 3.052635030173343
Validation loss: 2.449715500110038

Epoch: 6| Step: 1
Training loss: 2.031752304179107
Validation loss: 2.4462779875146357

Epoch: 6| Step: 2
Training loss: 2.9724159494488798
Validation loss: 2.4563136991179544

Epoch: 6| Step: 3
Training loss: 2.509780444387182
Validation loss: 2.454637639933254

Epoch: 6| Step: 4
Training loss: 3.2028531587200875
Validation loss: 2.4179659520103436

Epoch: 6| Step: 5
Training loss: 2.846630607692257
Validation loss: 2.426492674005789

Epoch: 6| Step: 6
Training loss: 2.6732006688230308
Validation loss: 2.4421399162247903

Epoch: 6| Step: 7
Training loss: 2.108656923493268
Validation loss: 2.449277070322878

Epoch: 6| Step: 8
Training loss: 3.2384588287219507
Validation loss: 2.3995215712456863

Epoch: 6| Step: 9
Training loss: 2.7080320239575344
Validation loss: 2.439163750847226

Epoch: 6| Step: 10
Training loss: 2.2881636693875884
Validation loss: 2.422006576621152

Epoch: 6| Step: 11
Training loss: 2.851884045173769
Validation loss: 2.430712464673906

Epoch: 6| Step: 12
Training loss: 2.9157680398703016
Validation loss: 2.4313584715976084

Epoch: 6| Step: 13
Training loss: 3.116072299877518
Validation loss: 2.4421773931242345

Epoch: 57| Step: 0
Training loss: 2.732470737590269
Validation loss: 2.449355104213617

Epoch: 6| Step: 1
Training loss: 2.5586239412579412
Validation loss: 2.4534768131241407

Epoch: 6| Step: 2
Training loss: 3.211331753994954
Validation loss: 2.4660472236617146

Epoch: 6| Step: 3
Training loss: 2.9399232000583297
Validation loss: 2.4339367411110175

Epoch: 6| Step: 4
Training loss: 2.921437929480218
Validation loss: 2.4598226292562484

Epoch: 6| Step: 5
Training loss: 3.009754534879795
Validation loss: 2.465915021544648

Epoch: 6| Step: 6
Training loss: 3.303864938076727
Validation loss: 2.4488359375109883

Epoch: 6| Step: 7
Training loss: 2.373338620183535
Validation loss: 2.4328771716802264

Epoch: 6| Step: 8
Training loss: 2.4305245775943183
Validation loss: 2.445511420200678

Epoch: 6| Step: 9
Training loss: 2.377389007448469
Validation loss: 2.4306118053796975

Epoch: 6| Step: 10
Training loss: 2.7584884722857548
Validation loss: 2.4509776582129597

Epoch: 6| Step: 11
Training loss: 2.976707955558547
Validation loss: 2.467812728483018

Epoch: 6| Step: 12
Training loss: 2.3053325203047836
Validation loss: 2.4345398769984796

Epoch: 6| Step: 13
Training loss: 3.0461784936075222
Validation loss: 2.4311698920833136

Epoch: 58| Step: 0
Training loss: 2.820711989917779
Validation loss: 2.4515566672344873

Epoch: 6| Step: 1
Training loss: 2.728376511978008
Validation loss: 2.45690865702115

Epoch: 6| Step: 2
Training loss: 2.5897886060364654
Validation loss: 2.4794672508633466

Epoch: 6| Step: 3
Training loss: 3.128368850639166
Validation loss: 2.428236926501017

Epoch: 6| Step: 4
Training loss: 2.006111701167773
Validation loss: 2.433512151415393

Epoch: 6| Step: 5
Training loss: 2.6540859607381417
Validation loss: 2.4330969705242054

Epoch: 6| Step: 6
Training loss: 3.1321440861226213
Validation loss: 2.42571038950853

Epoch: 6| Step: 7
Training loss: 3.0215244598787394
Validation loss: 2.4287850311299573

Epoch: 6| Step: 8
Training loss: 2.666913070818212
Validation loss: 2.4611505825378988

Epoch: 6| Step: 9
Training loss: 2.58217095301549
Validation loss: 2.444707032363246

Epoch: 6| Step: 10
Training loss: 3.4697026499668544
Validation loss: 2.4188930767096175

Epoch: 6| Step: 11
Training loss: 2.070783568717518
Validation loss: 2.4441443997092596

Epoch: 6| Step: 12
Training loss: 2.585865123461984
Validation loss: 2.428499073928568

Epoch: 6| Step: 13
Training loss: 3.2270545861326676
Validation loss: 2.4139565712971294

Epoch: 59| Step: 0
Training loss: 2.3500544886663643
Validation loss: 2.409032161735944

Epoch: 6| Step: 1
Training loss: 2.364574437957926
Validation loss: 2.4394726486894087

Epoch: 6| Step: 2
Training loss: 3.343725756976222
Validation loss: 2.44546086362095

Epoch: 6| Step: 3
Training loss: 1.7069066611069996
Validation loss: 2.4584408978650294

Epoch: 6| Step: 4
Training loss: 2.8364398605934995
Validation loss: 2.4630611571715404

Epoch: 6| Step: 5
Training loss: 2.916095259869991
Validation loss: 2.458030555507077

Epoch: 6| Step: 6
Training loss: 2.476300824417392
Validation loss: 2.4029175933463574

Epoch: 6| Step: 7
Training loss: 2.5010982961941624
Validation loss: 2.439707703620599

Epoch: 6| Step: 8
Training loss: 2.8658801848129682
Validation loss: 2.432046515898591

Epoch: 6| Step: 9
Training loss: 2.7905554956065752
Validation loss: 2.4376926288158307

Epoch: 6| Step: 10
Training loss: 2.8762764792153925
Validation loss: 2.449581635846709

Epoch: 6| Step: 11
Training loss: 2.9607948127929964
Validation loss: 2.427679929080765

Epoch: 6| Step: 12
Training loss: 3.366710089098205
Validation loss: 2.461415670608935

Epoch: 6| Step: 13
Training loss: 3.434738575900331
Validation loss: 2.4330618720262023

Epoch: 60| Step: 0
Training loss: 2.428168604003893
Validation loss: 2.439337254394261

Epoch: 6| Step: 1
Training loss: 2.2369861417638695
Validation loss: 2.4308982638763736

Epoch: 6| Step: 2
Training loss: 3.6495027804392195
Validation loss: 2.448485447964563

Epoch: 6| Step: 3
Training loss: 2.522279928567688
Validation loss: 2.4305613992819044

Epoch: 6| Step: 4
Training loss: 2.7595826430514028
Validation loss: 2.4432204029212365

Epoch: 6| Step: 5
Training loss: 2.5010537787645553
Validation loss: 2.445743167855466

Epoch: 6| Step: 6
Training loss: 3.3516647927917473
Validation loss: 2.443428612562983

Epoch: 6| Step: 7
Training loss: 2.7554394072832507
Validation loss: 2.4314222128096414

Epoch: 6| Step: 8
Training loss: 2.9186800047754256
Validation loss: 2.4402360403233168

Epoch: 6| Step: 9
Training loss: 2.8793898529537416
Validation loss: 2.429370680230532

Epoch: 6| Step: 10
Training loss: 2.656215173829601
Validation loss: 2.4352330429965794

Epoch: 6| Step: 11
Training loss: 2.612823583102391
Validation loss: 2.4310585344953934

Epoch: 6| Step: 12
Training loss: 2.564541259552718
Validation loss: 2.437655110310663

Epoch: 6| Step: 13
Training loss: 2.6163394294104303
Validation loss: 2.446502079583725

Epoch: 61| Step: 0
Training loss: 2.822551989959255
Validation loss: 2.451528756821903

Epoch: 6| Step: 1
Training loss: 3.167396427860836
Validation loss: 2.4423336318281206

Epoch: 6| Step: 2
Training loss: 2.649488572487103
Validation loss: 2.457153064792569

Epoch: 6| Step: 3
Training loss: 3.4614023279661965
Validation loss: 2.4380303692753382

Epoch: 6| Step: 4
Training loss: 2.320711557513926
Validation loss: 2.4402083985994514

Epoch: 6| Step: 5
Training loss: 2.6643234527848847
Validation loss: 2.459915156082275

Epoch: 6| Step: 6
Training loss: 2.4919618127692185
Validation loss: 2.440689854149319

Epoch: 6| Step: 7
Training loss: 3.428801750212564
Validation loss: 2.428594211301917

Epoch: 6| Step: 8
Training loss: 2.2053871174811146
Validation loss: 2.4330313913131167

Epoch: 6| Step: 9
Training loss: 2.5161311428236335
Validation loss: 2.4341035043450145

Epoch: 6| Step: 10
Training loss: 3.2443418400215704
Validation loss: 2.4218370130975595

Epoch: 6| Step: 11
Training loss: 2.6274031583323474
Validation loss: 2.428585913162454

Epoch: 6| Step: 12
Training loss: 2.3201073963695036
Validation loss: 2.424409515179411

Epoch: 6| Step: 13
Training loss: 2.3568887305232775
Validation loss: 2.476879332129543

Epoch: 62| Step: 0
Training loss: 2.651286435522121
Validation loss: 2.4315176585994993

Epoch: 6| Step: 1
Training loss: 3.41606071185047
Validation loss: 2.4317221040002535

Epoch: 6| Step: 2
Training loss: 3.5322287224385156
Validation loss: 2.447403594326617

Epoch: 6| Step: 3
Training loss: 2.515345397953987
Validation loss: 2.42955307248521

Epoch: 6| Step: 4
Training loss: 2.6947518912851174
Validation loss: 2.4596229293680754

Epoch: 6| Step: 5
Training loss: 2.543007939706184
Validation loss: 2.436775997138153

Epoch: 6| Step: 6
Training loss: 2.9753207122013086
Validation loss: 2.4474164030500054

Epoch: 6| Step: 7
Training loss: 2.624500317925819
Validation loss: 2.446302096083862

Epoch: 6| Step: 8
Training loss: 2.7536264263864387
Validation loss: 2.447897474588543

Epoch: 6| Step: 9
Training loss: 2.5992246095151588
Validation loss: 2.4250383510840585

Epoch: 6| Step: 10
Training loss: 2.5773811885708193
Validation loss: 2.457239237765849

Epoch: 6| Step: 11
Training loss: 2.5022831981239215
Validation loss: 2.4052220900299073

Epoch: 6| Step: 12
Training loss: 3.075519213997686
Validation loss: 2.418951908795285

Epoch: 6| Step: 13
Training loss: 1.6809585375372689
Validation loss: 2.4431805632485113

Epoch: 63| Step: 0
Training loss: 2.810443210877057
Validation loss: 2.436174955458406

Epoch: 6| Step: 1
Training loss: 2.557681134951333
Validation loss: 2.4381006035711605

Epoch: 6| Step: 2
Training loss: 2.8107609776641573
Validation loss: 2.4459642735972174

Epoch: 6| Step: 3
Training loss: 3.056223607485656
Validation loss: 2.431170574337262

Epoch: 6| Step: 4
Training loss: 2.91863605677117
Validation loss: 2.4224515601099497

Epoch: 6| Step: 5
Training loss: 2.700398210900852
Validation loss: 2.426146138876113

Epoch: 6| Step: 6
Training loss: 3.281657892941638
Validation loss: 2.439633592845347

Epoch: 6| Step: 7
Training loss: 2.7627465737887444
Validation loss: 2.464527904578684

Epoch: 6| Step: 8
Training loss: 2.5761397607752414
Validation loss: 2.4542851826334364

Epoch: 6| Step: 9
Training loss: 2.180959897574713
Validation loss: 2.445613240945729

Epoch: 6| Step: 10
Training loss: 3.051591714229878
Validation loss: 2.4512330767335837

Epoch: 6| Step: 11
Training loss: 2.75360330849825
Validation loss: 2.432583147548169

Epoch: 6| Step: 12
Training loss: 2.933532795482336
Validation loss: 2.4724037616791787

Epoch: 6| Step: 13
Training loss: 2.0970065853031867
Validation loss: 2.437728155967956

Epoch: 64| Step: 0
Training loss: 2.5782581641745677
Validation loss: 2.435092324846023

Epoch: 6| Step: 1
Training loss: 2.873335480862528
Validation loss: 2.4365406194007875

Epoch: 6| Step: 2
Training loss: 3.266870872446784
Validation loss: 2.423358947682969

Epoch: 6| Step: 3
Training loss: 2.6522294077433854
Validation loss: 2.4460425295096697

Epoch: 6| Step: 4
Training loss: 3.1487852145925608
Validation loss: 2.4155207349189647

Epoch: 6| Step: 5
Training loss: 2.696776769093389
Validation loss: 2.452156676812758

Epoch: 6| Step: 6
Training loss: 2.9472973356064123
Validation loss: 2.4555699198696535

Epoch: 6| Step: 7
Training loss: 2.375330249512789
Validation loss: 2.4625303177554603

Epoch: 6| Step: 8
Training loss: 2.294675539472066
Validation loss: 2.4381339955394945

Epoch: 6| Step: 9
Training loss: 2.78898137506207
Validation loss: 2.4411059858197977

Epoch: 6| Step: 10
Training loss: 2.905813533043433
Validation loss: 2.424955261531584

Epoch: 6| Step: 11
Training loss: 2.7551460667709007
Validation loss: 2.4551560277653923

Epoch: 6| Step: 12
Training loss: 3.01477323372038
Validation loss: 2.447199678550173

Epoch: 6| Step: 13
Training loss: 2.0292378950313705
Validation loss: 2.4615810841230763

Epoch: 65| Step: 0
Training loss: 2.9034824114372366
Validation loss: 2.423888353982706

Epoch: 6| Step: 1
Training loss: 2.922515962033372
Validation loss: 2.4581519296859797

Epoch: 6| Step: 2
Training loss: 2.838647309650141
Validation loss: 2.4379173155061746

Epoch: 6| Step: 3
Training loss: 2.950461503476379
Validation loss: 2.4657931935855704

Epoch: 6| Step: 4
Training loss: 2.9024819178186383
Validation loss: 2.442952321488878

Epoch: 6| Step: 5
Training loss: 2.1581794841041164
Validation loss: 2.4546430207120147

Epoch: 6| Step: 6
Training loss: 3.4857801674595734
Validation loss: 2.423385587254342

Epoch: 6| Step: 7
Training loss: 2.4250782118307224
Validation loss: 2.430880822207429

Epoch: 6| Step: 8
Training loss: 2.788497568398589
Validation loss: 2.4109021694542285

Epoch: 6| Step: 9
Training loss: 3.235753452108493
Validation loss: 2.459842823989027

Epoch: 6| Step: 10
Training loss: 1.7972463472854272
Validation loss: 2.431606008908676

Epoch: 6| Step: 11
Training loss: 3.186913529840383
Validation loss: 2.4461642113536595

Epoch: 6| Step: 12
Training loss: 2.3401391252093737
Validation loss: 2.44922683483423

Epoch: 6| Step: 13
Training loss: 2.4590296540397842
Validation loss: 2.4553843107666182

Epoch: 66| Step: 0
Training loss: 2.231688478271619
Validation loss: 2.439360046410668

Epoch: 6| Step: 1
Training loss: 3.4413472234403018
Validation loss: 2.4708372083029455

Epoch: 6| Step: 2
Training loss: 3.0893653084465753
Validation loss: 2.435068150564059

Epoch: 6| Step: 3
Training loss: 2.701112472090628
Validation loss: 2.442971492950632

Epoch: 6| Step: 4
Training loss: 2.773701400021627
Validation loss: 2.461920653006189

Epoch: 6| Step: 5
Training loss: 2.488326666640566
Validation loss: 2.444910533190941

Epoch: 6| Step: 6
Training loss: 2.014693643836607
Validation loss: 2.4281739726995424

Epoch: 6| Step: 7
Training loss: 2.9874371701580755
Validation loss: 2.46077941397191

Epoch: 6| Step: 8
Training loss: 2.721986882639943
Validation loss: 2.4429835042895536

Epoch: 6| Step: 9
Training loss: 2.618516180484145
Validation loss: 2.4408207767830907

Epoch: 6| Step: 10
Training loss: 2.9211517907717504
Validation loss: 2.428265312611997

Epoch: 6| Step: 11
Training loss: 2.6283415461355264
Validation loss: 2.4256026685920014

Epoch: 6| Step: 12
Training loss: 3.0655598826982944
Validation loss: 2.4404120970251038

Epoch: 6| Step: 13
Training loss: 3.145069829653777
Validation loss: 2.428386011376237

Epoch: 67| Step: 0
Training loss: 2.051835085445463
Validation loss: 2.4385657724247154

Epoch: 6| Step: 1
Training loss: 3.122991603149891
Validation loss: 2.4469511953628755

Epoch: 6| Step: 2
Training loss: 2.878551942718121
Validation loss: 2.4602015261870145

Epoch: 6| Step: 3
Training loss: 3.0588547167763176
Validation loss: 2.4415472384525594

Epoch: 6| Step: 4
Training loss: 2.8403994765527907
Validation loss: 2.443229360147585

Epoch: 6| Step: 5
Training loss: 2.8151022846075655
Validation loss: 2.423407336022511

Epoch: 6| Step: 6
Training loss: 2.9154283528795433
Validation loss: 2.4350076622206593

Epoch: 6| Step: 7
Training loss: 2.78212998431117
Validation loss: 2.428909631474112

Epoch: 6| Step: 8
Training loss: 2.9159251950876874
Validation loss: 2.435229632151979

Epoch: 6| Step: 9
Training loss: 3.141214951100697
Validation loss: 2.444870259684336

Epoch: 6| Step: 10
Training loss: 2.4450414342821363
Validation loss: 2.451719606879383

Epoch: 6| Step: 11
Training loss: 2.757538260421862
Validation loss: 2.432527525452577

Epoch: 6| Step: 12
Training loss: 2.109490737565586
Validation loss: 2.448990654356648

Epoch: 6| Step: 13
Training loss: 2.8001107193990658
Validation loss: 2.436010428820365

Epoch: 68| Step: 0
Training loss: 2.905707195823648
Validation loss: 2.4081426011293487

Epoch: 6| Step: 1
Training loss: 2.5263973863917935
Validation loss: 2.4411029990595052

Epoch: 6| Step: 2
Training loss: 3.034857422246432
Validation loss: 2.4486489416617276

Epoch: 6| Step: 3
Training loss: 2.8985727283775953
Validation loss: 2.4311627700577114

Epoch: 6| Step: 4
Training loss: 2.434286493496991
Validation loss: 2.431572156702344

Epoch: 6| Step: 5
Training loss: 3.3376067583818485
Validation loss: 2.4346506815397717

Epoch: 6| Step: 6
Training loss: 2.665043714497589
Validation loss: 2.417969073374875

Epoch: 6| Step: 7
Training loss: 1.9995827239567483
Validation loss: 2.450974724275042

Epoch: 6| Step: 8
Training loss: 2.5697247143267847
Validation loss: 2.415605918680394

Epoch: 6| Step: 9
Training loss: 3.1737662253579195
Validation loss: 2.429454396478524

Epoch: 6| Step: 10
Training loss: 2.7428406375008065
Validation loss: 2.4147745673814374

Epoch: 6| Step: 11
Training loss: 2.3517792560634785
Validation loss: 2.4494144093126984

Epoch: 6| Step: 12
Training loss: 2.5641064144374597
Validation loss: 2.428325029309802

Epoch: 6| Step: 13
Training loss: 3.290333119604133
Validation loss: 2.425246378573222

Epoch: 69| Step: 0
Training loss: 2.623176213624123
Validation loss: 2.413858095149004

Epoch: 6| Step: 1
Training loss: 2.9152590670224225
Validation loss: 2.4291938792315486

Epoch: 6| Step: 2
Training loss: 2.617098223174134
Validation loss: 2.444154375689536

Epoch: 6| Step: 3
Training loss: 2.4395421594478313
Validation loss: 2.4147449429734027

Epoch: 6| Step: 4
Training loss: 2.6136742540108364
Validation loss: 2.419814042603139

Epoch: 6| Step: 5
Training loss: 2.8305528873043775
Validation loss: 2.430947223481039

Epoch: 6| Step: 6
Training loss: 2.9032094493528233
Validation loss: 2.4236027794948893

Epoch: 6| Step: 7
Training loss: 2.0272172068828334
Validation loss: 2.43316210123079

Epoch: 6| Step: 8
Training loss: 2.6464765372635326
Validation loss: 2.4160491259012984

Epoch: 6| Step: 9
Training loss: 3.1351242858489243
Validation loss: 2.4518283876194404

Epoch: 6| Step: 10
Training loss: 2.876016022649495
Validation loss: 2.438994041014444

Epoch: 6| Step: 11
Training loss: 2.8615234308344992
Validation loss: 2.4161010380108965

Epoch: 6| Step: 12
Training loss: 3.310341761713691
Validation loss: 2.443256304029378

Epoch: 6| Step: 13
Training loss: 2.827697721629246
Validation loss: 2.450772407971317

Epoch: 70| Step: 0
Training loss: 3.0941753673011623
Validation loss: 2.43527989530621

Epoch: 6| Step: 1
Training loss: 2.477808208829907
Validation loss: 2.4324064008774937

Epoch: 6| Step: 2
Training loss: 2.9666803473967955
Validation loss: 2.4345948866021567

Epoch: 6| Step: 3
Training loss: 2.6111833165603495
Validation loss: 2.4383052035521104

Epoch: 6| Step: 4
Training loss: 2.98376842146302
Validation loss: 2.4265780605362948

Epoch: 6| Step: 5
Training loss: 2.5990625231899833
Validation loss: 2.4248104431978206

Epoch: 6| Step: 6
Training loss: 2.5418392081140784
Validation loss: 2.4281920509162123

Epoch: 6| Step: 7
Training loss: 2.477929829900928
Validation loss: 2.436112134178707

Epoch: 6| Step: 8
Training loss: 2.5845929018202556
Validation loss: 2.4481393943597447

Epoch: 6| Step: 9
Training loss: 2.8660841645728983
Validation loss: 2.458869436244572

Epoch: 6| Step: 10
Training loss: 2.7951300095021576
Validation loss: 2.4733671567273796

Epoch: 6| Step: 11
Training loss: 3.3423531725297124
Validation loss: 2.4581447278339996

Epoch: 6| Step: 12
Training loss: 2.3961864929857777
Validation loss: 2.4415748293192405

Epoch: 6| Step: 13
Training loss: 2.8587614141118776
Validation loss: 2.4236438675863754

Epoch: 71| Step: 0
Training loss: 2.653755093397588
Validation loss: 2.459548488924766

Epoch: 6| Step: 1
Training loss: 2.8786604425676043
Validation loss: 2.4464508083731142

Epoch: 6| Step: 2
Training loss: 3.1660728567081926
Validation loss: 2.433999734396266

Epoch: 6| Step: 3
Training loss: 2.7775683090915915
Validation loss: 2.440553441232255

Epoch: 6| Step: 4
Training loss: 2.887278005810475
Validation loss: 2.4491121771646633

Epoch: 6| Step: 5
Training loss: 3.1731174084052443
Validation loss: 2.43975646437627

Epoch: 6| Step: 6
Training loss: 2.674847115399173
Validation loss: 2.4337589129717725

Epoch: 6| Step: 7
Training loss: 2.8049295900045
Validation loss: 2.422048422128236

Epoch: 6| Step: 8
Training loss: 3.2856118411329818
Validation loss: 2.4586342957067577

Epoch: 6| Step: 9
Training loss: 2.857993370577022
Validation loss: 2.4255466623518274

Epoch: 6| Step: 10
Training loss: 2.2564457647296243
Validation loss: 2.4632245774408577

Epoch: 6| Step: 11
Training loss: 2.48810187491216
Validation loss: 2.439087524567376

Epoch: 6| Step: 12
Training loss: 2.1620311912453354
Validation loss: 2.442727283516487

Epoch: 6| Step: 13
Training loss: 2.2344987741613367
Validation loss: 2.4465992486157253

Epoch: 72| Step: 0
Training loss: 2.559137697129377
Validation loss: 2.433906337712327

Epoch: 6| Step: 1
Training loss: 2.325231302712593
Validation loss: 2.4496446485730874

Epoch: 6| Step: 2
Training loss: 3.13553856563073
Validation loss: 2.4417763664344787

Epoch: 6| Step: 3
Training loss: 2.982528994372143
Validation loss: 2.443705114124394

Epoch: 6| Step: 4
Training loss: 2.672531047387339
Validation loss: 2.4482687929672506

Epoch: 6| Step: 5
Training loss: 2.0548223298947486
Validation loss: 2.458095104826413

Epoch: 6| Step: 6
Training loss: 2.965399367147239
Validation loss: 2.444697951566644

Epoch: 6| Step: 7
Training loss: 3.026522381013945
Validation loss: 2.439440169372628

Epoch: 6| Step: 8
Training loss: 3.145589520098749
Validation loss: 2.445777806598255

Epoch: 6| Step: 9
Training loss: 2.8973093639259457
Validation loss: 2.4565854714751993

Epoch: 6| Step: 10
Training loss: 3.099491071456502
Validation loss: 2.4375444686362995

Epoch: 6| Step: 11
Training loss: 2.532134949262309
Validation loss: 2.4373893849421617

Epoch: 6| Step: 12
Training loss: 2.436133368388155
Validation loss: 2.450085535328892

Epoch: 6| Step: 13
Training loss: 2.618741066855988
Validation loss: 2.447935008912148

Epoch: 73| Step: 0
Training loss: 2.0771814943596656
Validation loss: 2.440069555948398

Epoch: 6| Step: 1
Training loss: 3.241543699022634
Validation loss: 2.451786758649149

Epoch: 6| Step: 2
Training loss: 2.6811160180822076
Validation loss: 2.451830217425091

Epoch: 6| Step: 3
Training loss: 2.6954584496829637
Validation loss: 2.4619067690831087

Epoch: 6| Step: 4
Training loss: 2.5219000038694244
Validation loss: 2.4454621824149236

Epoch: 6| Step: 5
Training loss: 3.0185561092179602
Validation loss: 2.447034923034901

Epoch: 6| Step: 6
Training loss: 3.055646896923661
Validation loss: 2.4461188199325767

Epoch: 6| Step: 7
Training loss: 2.0157131205377072
Validation loss: 2.465040464378007

Epoch: 6| Step: 8
Training loss: 3.2451533345112904
Validation loss: 2.4713737529980095

Epoch: 6| Step: 9
Training loss: 2.1679831564484737
Validation loss: 2.4366692430704155

Epoch: 6| Step: 10
Training loss: 3.227237510733227
Validation loss: 2.4419157125527367

Epoch: 6| Step: 11
Training loss: 2.9878481482509844
Validation loss: 2.433003154587256

Epoch: 6| Step: 12
Training loss: 2.895201653027702
Validation loss: 2.4552802650573735

Epoch: 6| Step: 13
Training loss: 2.2501157095084596
Validation loss: 2.461951979797367

Epoch: 74| Step: 0
Training loss: 3.1309315654691923
Validation loss: 2.4431179483412806

Epoch: 6| Step: 1
Training loss: 2.9664212378946324
Validation loss: 2.444833203655439

Epoch: 6| Step: 2
Training loss: 2.8116448691864275
Validation loss: 2.4492495285408014

Epoch: 6| Step: 3
Training loss: 1.6583318761438968
Validation loss: 2.433181474158178

Epoch: 6| Step: 4
Training loss: 2.5383800352345958
Validation loss: 2.427715138123443

Epoch: 6| Step: 5
Training loss: 2.5043891524392667
Validation loss: 2.454095136941077

Epoch: 6| Step: 6
Training loss: 3.010788592074673
Validation loss: 2.4741786245162385

Epoch: 6| Step: 7
Training loss: 2.727432593081759
Validation loss: 2.4357990201772117

Epoch: 6| Step: 8
Training loss: 2.6616063583825507
Validation loss: 2.435974615618957

Epoch: 6| Step: 9
Training loss: 2.9994240843788096
Validation loss: 2.4330370959391145

Epoch: 6| Step: 10
Training loss: 3.090311779354265
Validation loss: 2.465907395842638

Epoch: 6| Step: 11
Training loss: 2.9291337367269246
Validation loss: 2.442911604851519

Epoch: 6| Step: 12
Training loss: 2.6914179432120804
Validation loss: 2.4165154205733477

Epoch: 6| Step: 13
Training loss: 2.3998659334884236
Validation loss: 2.4554784503232647

Epoch: 75| Step: 0
Training loss: 2.715659127498357
Validation loss: 2.45161258331042

Epoch: 6| Step: 1
Training loss: 2.4501832465326663
Validation loss: 2.4479798964426887

Epoch: 6| Step: 2
Training loss: 2.9740979662359064
Validation loss: 2.447291464855331

Epoch: 6| Step: 3
Training loss: 1.6149529392956088
Validation loss: 2.436545274170442

Epoch: 6| Step: 4
Training loss: 2.861635742316639
Validation loss: 2.432118672763852

Epoch: 6| Step: 5
Training loss: 3.039637650929827
Validation loss: 2.4166381404269472

Epoch: 6| Step: 6
Training loss: 2.7035535351416122
Validation loss: 2.4249016909625736

Epoch: 6| Step: 7
Training loss: 2.5084852225446443
Validation loss: 2.453140647498115

Epoch: 6| Step: 8
Training loss: 2.7457922909746433
Validation loss: 2.442274126446654

Epoch: 6| Step: 9
Training loss: 2.8597988794209472
Validation loss: 2.45651527518467

Epoch: 6| Step: 10
Training loss: 2.566682704858915
Validation loss: 2.42977736444427

Epoch: 6| Step: 11
Training loss: 2.8633621849705526
Validation loss: 2.4274341390111354

Epoch: 6| Step: 12
Training loss: 3.0486892385085835
Validation loss: 2.4411270862739904

Epoch: 6| Step: 13
Training loss: 3.7593009047210266
Validation loss: 2.432697178458101

Epoch: 76| Step: 0
Training loss: 2.6367793323479987
Validation loss: 2.445985420181586

Epoch: 6| Step: 1
Training loss: 2.5704542312272327
Validation loss: 2.4253708636914175

Epoch: 6| Step: 2
Training loss: 2.8703263483351966
Validation loss: 2.412779095238531

Epoch: 6| Step: 3
Training loss: 3.0328317821426256
Validation loss: 2.4418682400587115

Epoch: 6| Step: 4
Training loss: 2.441503025425699
Validation loss: 2.4409501275027115

Epoch: 6| Step: 5
Training loss: 3.0117136835236242
Validation loss: 2.4378474124418914

Epoch: 6| Step: 6
Training loss: 3.117618540549803
Validation loss: 2.412071632866171

Epoch: 6| Step: 7
Training loss: 2.7082772958288994
Validation loss: 2.436339886625029

Epoch: 6| Step: 8
Training loss: 2.7530837541969717
Validation loss: 2.433003748870555

Epoch: 6| Step: 9
Training loss: 2.785275064852599
Validation loss: 2.4441969758452626

Epoch: 6| Step: 10
Training loss: 2.675052114674244
Validation loss: 2.423450152777475

Epoch: 6| Step: 11
Training loss: 2.9648080454248333
Validation loss: 2.46651032636093

Epoch: 6| Step: 12
Training loss: 2.4766978518686296
Validation loss: 2.4168019873894275

Epoch: 6| Step: 13
Training loss: 2.4297247880729964
Validation loss: 2.447822461817019

Epoch: 77| Step: 0
Training loss: 2.7379299923492146
Validation loss: 2.4545678062351084

Epoch: 6| Step: 1
Training loss: 2.638377932109117
Validation loss: 2.4552360643957316

Epoch: 6| Step: 2
Training loss: 3.4332373806282552
Validation loss: 2.433635593259276

Epoch: 6| Step: 3
Training loss: 3.27041085464856
Validation loss: 2.423066447467725

Epoch: 6| Step: 4
Training loss: 2.2527904161547414
Validation loss: 2.443089155564429

Epoch: 6| Step: 5
Training loss: 2.24356451516412
Validation loss: 2.432253461248168

Epoch: 6| Step: 6
Training loss: 2.5801519575391136
Validation loss: 2.450556872635747

Epoch: 6| Step: 7
Training loss: 3.095436022092868
Validation loss: 2.4456258960422006

Epoch: 6| Step: 8
Training loss: 2.416762536438311
Validation loss: 2.4263933685018637

Epoch: 6| Step: 9
Training loss: 2.7564372234810985
Validation loss: 2.4213698724075488

Epoch: 6| Step: 10
Training loss: 2.555939813648524
Validation loss: 2.4226827914375852

Epoch: 6| Step: 11
Training loss: 2.942539194473158
Validation loss: 2.4267400341843945

Epoch: 6| Step: 12
Training loss: 2.567827594662111
Validation loss: 2.45595713317111

Epoch: 6| Step: 13
Training loss: 3.2010704336333404
Validation loss: 2.4537093500700236

Epoch: 78| Step: 0
Training loss: 2.176421970012418
Validation loss: 2.4216205358711917

Epoch: 6| Step: 1
Training loss: 3.1240693804745368
Validation loss: 2.4401287999058168

Epoch: 6| Step: 2
Training loss: 2.290106109093149
Validation loss: 2.449096142792628

Epoch: 6| Step: 3
Training loss: 2.4058908962458525
Validation loss: 2.44711296504264

Epoch: 6| Step: 4
Training loss: 2.518042025208585
Validation loss: 2.43120121546172

Epoch: 6| Step: 5
Training loss: 2.5793683377386247
Validation loss: 2.427068377998037

Epoch: 6| Step: 6
Training loss: 3.4748213962240904
Validation loss: 2.45283867067836

Epoch: 6| Step: 7
Training loss: 2.567333593872554
Validation loss: 2.440217767677646

Epoch: 6| Step: 8
Training loss: 3.0596532241199617
Validation loss: 2.4307780864080897

Epoch: 6| Step: 9
Training loss: 2.816627864081309
Validation loss: 2.4168663469280474

Epoch: 6| Step: 10
Training loss: 2.8341519538238034
Validation loss: 2.408528790946684

Epoch: 6| Step: 11
Training loss: 2.8789787956217476
Validation loss: 2.435036557925791

Epoch: 6| Step: 12
Training loss: 2.9547431186099167
Validation loss: 2.4505282488637907

Epoch: 6| Step: 13
Training loss: 2.9922273399150927
Validation loss: 2.445146696252805

Epoch: 79| Step: 0
Training loss: 2.3781492535310544
Validation loss: 2.4425336159267124

Epoch: 6| Step: 1
Training loss: 3.1496363172546187
Validation loss: 2.425616672592246

Epoch: 6| Step: 2
Training loss: 3.2808882740999374
Validation loss: 2.4287578544041937

Epoch: 6| Step: 3
Training loss: 2.014558495174801
Validation loss: 2.4458734688656834

Epoch: 6| Step: 4
Training loss: 3.026325748697824
Validation loss: 2.4142311388532147

Epoch: 6| Step: 5
Training loss: 2.3388577072811523
Validation loss: 2.4439321212101857

Epoch: 6| Step: 6
Training loss: 2.4729535977166983
Validation loss: 2.435364396356564

Epoch: 6| Step: 7
Training loss: 3.270716444384978
Validation loss: 2.4407766115164753

Epoch: 6| Step: 8
Training loss: 2.4138819556020246
Validation loss: 2.413939349717698

Epoch: 6| Step: 9
Training loss: 2.552966923763519
Validation loss: 2.457294676079422

Epoch: 6| Step: 10
Training loss: 3.054049139810371
Validation loss: 2.418820260337935

Epoch: 6| Step: 11
Training loss: 2.8611686375666876
Validation loss: 2.446098625085573

Epoch: 6| Step: 12
Training loss: 2.8979694163286256
Validation loss: 2.4417769732812493

Epoch: 6| Step: 13
Training loss: 2.246067106669423
Validation loss: 2.436508814375953

Epoch: 80| Step: 0
Training loss: 2.5560227384018503
Validation loss: 2.4293121880518913

Epoch: 6| Step: 1
Training loss: 3.5160274021005264
Validation loss: 2.4208404628453897

Epoch: 6| Step: 2
Training loss: 2.0028295766287254
Validation loss: 2.428222169012856

Epoch: 6| Step: 3
Training loss: 2.1628266809927603
Validation loss: 2.431155846256233

Epoch: 6| Step: 4
Training loss: 2.081688944033352
Validation loss: 2.434934878182941

Epoch: 6| Step: 5
Training loss: 3.0038414837783916
Validation loss: 2.4245054622624456

Epoch: 6| Step: 6
Training loss: 2.6358924305606743
Validation loss: 2.445511360971473

Epoch: 6| Step: 7
Training loss: 2.9148407988480938
Validation loss: 2.429458359403108

Epoch: 6| Step: 8
Training loss: 2.8708421246560576
Validation loss: 2.4487654080997823

Epoch: 6| Step: 9
Training loss: 3.4709278864687474
Validation loss: 2.4154926973332955

Epoch: 6| Step: 10
Training loss: 2.6187947817958888
Validation loss: 2.4363724535701428

Epoch: 6| Step: 11
Training loss: 2.7225334763723774
Validation loss: 2.4472420717032763

Epoch: 6| Step: 12
Training loss: 2.8320029632208024
Validation loss: 2.4737334570932923

Epoch: 6| Step: 13
Training loss: 2.449076038443404
Validation loss: 2.4290589030291434

Epoch: 81| Step: 0
Training loss: 2.6482323311074323
Validation loss: 2.445988552954532

Epoch: 6| Step: 1
Training loss: 2.4417132619463024
Validation loss: 2.42096763229706

Epoch: 6| Step: 2
Training loss: 2.8254310042814414
Validation loss: 2.426374630261469

Epoch: 6| Step: 3
Training loss: 2.702744473701595
Validation loss: 2.444154606444565

Epoch: 6| Step: 4
Training loss: 3.0209019795653087
Validation loss: 2.428828777451565

Epoch: 6| Step: 5
Training loss: 2.519797898014517
Validation loss: 2.421277507921533

Epoch: 6| Step: 6
Training loss: 2.3458287493681853
Validation loss: 2.451080921176844

Epoch: 6| Step: 7
Training loss: 2.92638127243949
Validation loss: 2.4492551608515303

Epoch: 6| Step: 8
Training loss: 2.9281255601945504
Validation loss: 2.414661356952804

Epoch: 6| Step: 9
Training loss: 2.756801517175754
Validation loss: 2.428287373459755

Epoch: 6| Step: 10
Training loss: 3.966697101850247
Validation loss: 2.399821119196525

Epoch: 6| Step: 11
Training loss: 2.184700837070732
Validation loss: 2.432809012165897

Epoch: 6| Step: 12
Training loss: 2.475366249343699
Validation loss: 2.435257255644999

Epoch: 6| Step: 13
Training loss: 2.0999389957468177
Validation loss: 2.404034308674808

Epoch: 82| Step: 0
Training loss: 2.9077603917113715
Validation loss: 2.4403285203150635

Epoch: 6| Step: 1
Training loss: 2.3830312159483102
Validation loss: 2.4293817436501417

Epoch: 6| Step: 2
Training loss: 2.4962234582765026
Validation loss: 2.423399992304436

Epoch: 6| Step: 3
Training loss: 2.212662633748338
Validation loss: 2.452297378590494

Epoch: 6| Step: 4
Training loss: 2.225971451328639
Validation loss: 2.4327514119780784

Epoch: 6| Step: 5
Training loss: 3.177030085549985
Validation loss: 2.4500509271441326

Epoch: 6| Step: 6
Training loss: 2.978671550734206
Validation loss: 2.4373002655425773

Epoch: 6| Step: 7
Training loss: 2.7754273918878876
Validation loss: 2.4458304723541313

Epoch: 6| Step: 8
Training loss: 2.9853569931664703
Validation loss: 2.452311989108411

Epoch: 6| Step: 9
Training loss: 2.7987500943196237
Validation loss: 2.4566294725396705

Epoch: 6| Step: 10
Training loss: 2.826564463651111
Validation loss: 2.4483767354436496

Epoch: 6| Step: 11
Training loss: 2.78120388035629
Validation loss: 2.4438629167944153

Epoch: 6| Step: 12
Training loss: 2.5676171916599473
Validation loss: 2.4363540688202585

Epoch: 6| Step: 13
Training loss: 3.8844652553639083
Validation loss: 2.4698310376173493

Epoch: 83| Step: 0
Training loss: 2.5970608824892722
Validation loss: 2.435076263956548

Epoch: 6| Step: 1
Training loss: 2.513302601779008
Validation loss: 2.4316951224554963

Epoch: 6| Step: 2
Training loss: 2.2055829993183944
Validation loss: 2.4345112523479906

Epoch: 6| Step: 3
Training loss: 2.1149707182532347
Validation loss: 2.4315887125391678

Epoch: 6| Step: 4
Training loss: 2.698885553444388
Validation loss: 2.4457954161323316

Epoch: 6| Step: 5
Training loss: 2.5179076648656245
Validation loss: 2.4053202743899282

Epoch: 6| Step: 6
Training loss: 3.040829652209226
Validation loss: 2.456536976931071

Epoch: 6| Step: 7
Training loss: 3.2703411598608305
Validation loss: 2.4267859560394127

Epoch: 6| Step: 8
Training loss: 2.640123308365354
Validation loss: 2.456643073221534

Epoch: 6| Step: 9
Training loss: 3.1440307977153186
Validation loss: 2.459030019971848

Epoch: 6| Step: 10
Training loss: 2.51980896831206
Validation loss: 2.4428911195598086

Epoch: 6| Step: 11
Training loss: 2.695137969185087
Validation loss: 2.4335033106494732

Epoch: 6| Step: 12
Training loss: 3.160015918836455
Validation loss: 2.417606810209069

Epoch: 6| Step: 13
Training loss: 3.469261750283478
Validation loss: 2.43033369920084

Epoch: 84| Step: 0
Training loss: 2.7800746534464413
Validation loss: 2.4375994314691334

Epoch: 6| Step: 1
Training loss: 2.4941495149912534
Validation loss: 2.453396918684746

Epoch: 6| Step: 2
Training loss: 3.074744832686578
Validation loss: 2.4284836244682633

Epoch: 6| Step: 3
Training loss: 2.2722729194082034
Validation loss: 2.452789455080622

Epoch: 6| Step: 4
Training loss: 2.944008293077979
Validation loss: 2.4435776151326913

Epoch: 6| Step: 5
Training loss: 2.5404302581061478
Validation loss: 2.4558121580162067

Epoch: 6| Step: 6
Training loss: 3.6070183000007816
Validation loss: 2.459560071175653

Epoch: 6| Step: 7
Training loss: 2.679477872517615
Validation loss: 2.4404836084225168

Epoch: 6| Step: 8
Training loss: 2.3650138115832413
Validation loss: 2.4496797093620026

Epoch: 6| Step: 9
Training loss: 2.7095862107191
Validation loss: 2.4580835709487174

Epoch: 6| Step: 10
Training loss: 2.878468287370296
Validation loss: 2.4551065745974743

Epoch: 6| Step: 11
Training loss: 2.505263700025416
Validation loss: 2.457018919191812

Epoch: 6| Step: 12
Training loss: 2.35616635040017
Validation loss: 2.4388297749844337

Epoch: 6| Step: 13
Training loss: 3.2059096722743496
Validation loss: 2.4405293389020835

Epoch: 85| Step: 0
Training loss: 2.8208466338422116
Validation loss: 2.4344043326216322

Epoch: 6| Step: 1
Training loss: 2.886590895447546
Validation loss: 2.448961278452393

Epoch: 6| Step: 2
Training loss: 2.376216777507064
Validation loss: 2.4487994387681256

Epoch: 6| Step: 3
Training loss: 2.8534026596821644
Validation loss: 2.431496233836714

Epoch: 6| Step: 4
Training loss: 1.9929414407077013
Validation loss: 2.4356053279694327

Epoch: 6| Step: 5
Training loss: 2.870398446313784
Validation loss: 2.4479142886542764

Epoch: 6| Step: 6
Training loss: 2.848517644762267
Validation loss: 2.4361608764009497

Epoch: 6| Step: 7
Training loss: 2.887227469129527
Validation loss: 2.4335912027850837

Epoch: 6| Step: 8
Training loss: 2.5917747973814773
Validation loss: 2.4316790449264616

Epoch: 6| Step: 9
Training loss: 2.873731582137893
Validation loss: 2.4399039585740026

Epoch: 6| Step: 10
Training loss: 2.7396938364541543
Validation loss: 2.430389025521203

Epoch: 6| Step: 11
Training loss: 2.796258016548381
Validation loss: 2.4468760571914743

Epoch: 6| Step: 12
Training loss: 2.6469041554760473
Validation loss: 2.417732016854545

Epoch: 6| Step: 13
Training loss: 3.3154098397785883
Validation loss: 2.4385133030107493

Epoch: 86| Step: 0
Training loss: 3.125531876123965
Validation loss: 2.4283115592919113

Epoch: 6| Step: 1
Training loss: 2.864545139867306
Validation loss: 2.4269507509816193

Epoch: 6| Step: 2
Training loss: 2.324396411933552
Validation loss: 2.4412568439667277

Epoch: 6| Step: 3
Training loss: 2.572049096334492
Validation loss: 2.4407722063945654

Epoch: 6| Step: 4
Training loss: 3.868881810027406
Validation loss: 2.4292312498099284

Epoch: 6| Step: 5
Training loss: 2.8187675999939903
Validation loss: 2.4291503373659977

Epoch: 6| Step: 6
Training loss: 2.652365143679828
Validation loss: 2.45753201863082

Epoch: 6| Step: 7
Training loss: 2.937947016041539
Validation loss: 2.440924631174705

Epoch: 6| Step: 8
Training loss: 3.048607279997909
Validation loss: 2.4595789641363215

Epoch: 6| Step: 9
Training loss: 2.7040292627568245
Validation loss: 2.447165079837797

Epoch: 6| Step: 10
Training loss: 1.971427852273103
Validation loss: 2.4431319558273956

Epoch: 6| Step: 11
Training loss: 2.5686931668790822
Validation loss: 2.423091458813123

Epoch: 6| Step: 12
Training loss: 2.3148124061854514
Validation loss: 2.4528454120346397

Epoch: 6| Step: 13
Training loss: 2.110045601953137
Validation loss: 2.429778124638523

Epoch: 87| Step: 0
Training loss: 2.4295954548858245
Validation loss: 2.4395668106218045

Epoch: 6| Step: 1
Training loss: 2.5450223023162506
Validation loss: 2.426880780282817

Epoch: 6| Step: 2
Training loss: 2.9309931986224558
Validation loss: 2.449585288347616

Epoch: 6| Step: 3
Training loss: 2.5257482194377894
Validation loss: 2.439747277413307

Epoch: 6| Step: 4
Training loss: 3.084433659978379
Validation loss: 2.454317685896703

Epoch: 6| Step: 5
Training loss: 2.2989220871435263
Validation loss: 2.451874656188621

Epoch: 6| Step: 6
Training loss: 2.323248242261709
Validation loss: 2.4269117164135654

Epoch: 6| Step: 7
Training loss: 2.8129665411658826
Validation loss: 2.4444263724979405

Epoch: 6| Step: 8
Training loss: 2.9424785873165513
Validation loss: 2.453317535768294

Epoch: 6| Step: 9
Training loss: 3.085776795657325
Validation loss: 2.447879527239407

Epoch: 6| Step: 10
Training loss: 2.462124977020017
Validation loss: 2.445187646170381

Epoch: 6| Step: 11
Training loss: 3.506968237331409
Validation loss: 2.4420375758824595

Epoch: 6| Step: 12
Training loss: 2.3289480098754107
Validation loss: 2.427419382979137

Epoch: 6| Step: 13
Training loss: 2.9883426834473616
Validation loss: 2.471248224067227

Epoch: 88| Step: 0
Training loss: 3.286871463464001
Validation loss: 2.4460877357922235

Epoch: 6| Step: 1
Training loss: 2.9383861747169275
Validation loss: 2.434185865094059

Epoch: 6| Step: 2
Training loss: 2.4563272556259594
Validation loss: 2.4351648409882314

Epoch: 6| Step: 3
Training loss: 2.084964482395991
Validation loss: 2.450433099959091

Epoch: 6| Step: 4
Training loss: 2.468176473320858
Validation loss: 2.450067943026857

Epoch: 6| Step: 5
Training loss: 2.9636942327522626
Validation loss: 2.4307420632273353

Epoch: 6| Step: 6
Training loss: 3.168112190658673
Validation loss: 2.4556612596883833

Epoch: 6| Step: 7
Training loss: 2.435243785020397
Validation loss: 2.450249977953584

Epoch: 6| Step: 8
Training loss: 3.3474199351848797
Validation loss: 2.4287970324069894

Epoch: 6| Step: 9
Training loss: 2.7172150937873436
Validation loss: 2.449515265607609

Epoch: 6| Step: 10
Training loss: 2.5448392443295864
Validation loss: 2.4101452578816507

Epoch: 6| Step: 11
Training loss: 2.5044375137207626
Validation loss: 2.440157565627884

Epoch: 6| Step: 12
Training loss: 2.5436618874025525
Validation loss: 2.453567890238576

Epoch: 6| Step: 13
Training loss: 2.7468908246553383
Validation loss: 2.4097426537555036

Epoch: 89| Step: 0
Training loss: 2.8971385255718336
Validation loss: 2.455689254671466

Epoch: 6| Step: 1
Training loss: 2.7185826743364174
Validation loss: 2.4062249787373062

Epoch: 6| Step: 2
Training loss: 2.6816749431639653
Validation loss: 2.456754195683412

Epoch: 6| Step: 3
Training loss: 2.092066415326941
Validation loss: 2.459874257489745

Epoch: 6| Step: 4
Training loss: 3.400481212445716
Validation loss: 2.443695668195439

Epoch: 6| Step: 5
Training loss: 2.7355284955383565
Validation loss: 2.450329028150703

Epoch: 6| Step: 6
Training loss: 2.9687059901137576
Validation loss: 2.4506311944388024

Epoch: 6| Step: 7
Training loss: 2.279615051498831
Validation loss: 2.433983714220981

Epoch: 6| Step: 8
Training loss: 2.6833577725578963
Validation loss: 2.468875913471108

Epoch: 6| Step: 9
Training loss: 3.1138371793287645
Validation loss: 2.469712551183937

Epoch: 6| Step: 10
Training loss: 2.7198477096559475
Validation loss: 2.415075078942086

Epoch: 6| Step: 11
Training loss: 2.675032595854768
Validation loss: 2.42356735203935

Epoch: 6| Step: 12
Training loss: 3.196960471941321
Validation loss: 2.442263771192831

Epoch: 6| Step: 13
Training loss: 1.3587876728243218
Validation loss: 2.4584220941814556

Epoch: 90| Step: 0
Training loss: 2.842384429702608
Validation loss: 2.4184683136689573

Epoch: 6| Step: 1
Training loss: 2.156840146893248
Validation loss: 2.4394568861965844

Epoch: 6| Step: 2
Training loss: 3.1834140094966243
Validation loss: 2.442946815266978

Epoch: 6| Step: 3
Training loss: 2.68008722376867
Validation loss: 2.441671304601408

Epoch: 6| Step: 4
Training loss: 2.7533994817138505
Validation loss: 2.430818234814195

Epoch: 6| Step: 5
Training loss: 2.5541692085523127
Validation loss: 2.4430977801064944

Epoch: 6| Step: 6
Training loss: 2.94216353963292
Validation loss: 2.4444617021519512

Epoch: 6| Step: 7
Training loss: 2.975041198445206
Validation loss: 2.430001636755918

Epoch: 6| Step: 8
Training loss: 2.3374581889120734
Validation loss: 2.43789062861744

Epoch: 6| Step: 9
Training loss: 2.1898054780313734
Validation loss: 2.4341844864784856

Epoch: 6| Step: 10
Training loss: 2.9800705149967555
Validation loss: 2.4437931712115746

Epoch: 6| Step: 11
Training loss: 2.4829261441686583
Validation loss: 2.4531062412635505

Epoch: 6| Step: 12
Training loss: 3.140281544594575
Validation loss: 2.4552336471817298

Epoch: 6| Step: 13
Training loss: 2.9936755591198394
Validation loss: 2.441655358384889

Epoch: 91| Step: 0
Training loss: 2.0467657394950223
Validation loss: 2.4416335537667164

Epoch: 6| Step: 1
Training loss: 2.3737117887598345
Validation loss: 2.4401258077469574

Epoch: 6| Step: 2
Training loss: 2.3923252018739
Validation loss: 2.433288625670844

Epoch: 6| Step: 3
Training loss: 3.2072507014213247
Validation loss: 2.4311681579763955

Epoch: 6| Step: 4
Training loss: 2.6990622269423925
Validation loss: 2.4374113789853746

Epoch: 6| Step: 5
Training loss: 2.4068778073619637
Validation loss: 2.4515213959074225

Epoch: 6| Step: 6
Training loss: 3.3024552834751644
Validation loss: 2.416888790792356

Epoch: 6| Step: 7
Training loss: 2.7947973280906324
Validation loss: 2.466365927355835

Epoch: 6| Step: 8
Training loss: 2.7610163679014255
Validation loss: 2.419219946501876

Epoch: 6| Step: 9
Training loss: 2.5882689692479084
Validation loss: 2.4449778520510512

Epoch: 6| Step: 10
Training loss: 2.418813628707752
Validation loss: 2.4088605131793486

Epoch: 6| Step: 11
Training loss: 2.7829163092346314
Validation loss: 2.4445471427020515

Epoch: 6| Step: 12
Training loss: 3.2693938633209227
Validation loss: 2.406703720284183

Epoch: 6| Step: 13
Training loss: 3.0933509338086465
Validation loss: 2.4649844296127594

Epoch: 92| Step: 0
Training loss: 2.664723870749562
Validation loss: 2.458656865044119

Epoch: 6| Step: 1
Training loss: 2.5208328078928837
Validation loss: 2.454651726853909

Epoch: 6| Step: 2
Training loss: 2.9413731789724533
Validation loss: 2.4497710835702184

Epoch: 6| Step: 3
Training loss: 3.1969634550056405
Validation loss: 2.4280122910999866

Epoch: 6| Step: 4
Training loss: 2.600573744191984
Validation loss: 2.4266037002084317

Epoch: 6| Step: 5
Training loss: 2.5980048153710027
Validation loss: 2.4459349168963684

Epoch: 6| Step: 6
Training loss: 2.3735968811886377
Validation loss: 2.4373782274462465

Epoch: 6| Step: 7
Training loss: 2.164856362280283
Validation loss: 2.4447466605040167

Epoch: 6| Step: 8
Training loss: 3.2703523869458238
Validation loss: 2.42402657936571

Epoch: 6| Step: 9
Training loss: 2.5355218703030658
Validation loss: 2.4523327757026867

Epoch: 6| Step: 10
Training loss: 3.0595664162219847
Validation loss: 2.4481057555914183

Epoch: 6| Step: 11
Training loss: 2.4469520995171816
Validation loss: 2.4392320438878987

Epoch: 6| Step: 12
Training loss: 2.9297954081689777
Validation loss: 2.4393085641272023

Epoch: 6| Step: 13
Training loss: 2.961761923485115
Validation loss: 2.434668184125172

Epoch: 93| Step: 0
Training loss: 2.6655329340680294
Validation loss: 2.447927144974428

Epoch: 6| Step: 1
Training loss: 2.54318044223759
Validation loss: 2.444175336548347

Epoch: 6| Step: 2
Training loss: 3.0955823616035003
Validation loss: 2.420442763296773

Epoch: 6| Step: 3
Training loss: 2.275445634561711
Validation loss: 2.4450240866886808

Epoch: 6| Step: 4
Training loss: 3.0432099037446485
Validation loss: 2.4389566707657884

Epoch: 6| Step: 5
Training loss: 2.9678759442645903
Validation loss: 2.436691671818059

Epoch: 6| Step: 6
Training loss: 3.166610181873562
Validation loss: 2.429242916455157

Epoch: 6| Step: 7
Training loss: 3.207774291110576
Validation loss: 2.4573304654588632

Epoch: 6| Step: 8
Training loss: 2.4707955686139482
Validation loss: 2.4347060134771805

Epoch: 6| Step: 9
Training loss: 2.536453173956562
Validation loss: 2.430135879107095

Epoch: 6| Step: 10
Training loss: 2.720808368079562
Validation loss: 2.439372094974604

Epoch: 6| Step: 11
Training loss: 2.432403910386628
Validation loss: 2.4120210237245105

Epoch: 6| Step: 12
Training loss: 2.20455431735287
Validation loss: 2.426449395875525

Epoch: 6| Step: 13
Training loss: 3.0671144870301617
Validation loss: 2.4395340320089898

Epoch: 94| Step: 0
Training loss: 2.9056515641535663
Validation loss: 2.412281256799938

Epoch: 6| Step: 1
Training loss: 3.0949125321312345
Validation loss: 2.4333996647062115

Epoch: 6| Step: 2
Training loss: 1.993702988982152
Validation loss: 2.448498472997939

Epoch: 6| Step: 3
Training loss: 2.731008495416771
Validation loss: 2.414301221315413

Epoch: 6| Step: 4
Training loss: 2.114172784447856
Validation loss: 2.4617254763694105

Epoch: 6| Step: 5
Training loss: 2.6018538211690867
Validation loss: 2.4271315146873924

Epoch: 6| Step: 6
Training loss: 2.406647983682803
Validation loss: 2.455586303470852

Epoch: 6| Step: 7
Training loss: 2.826460712175335
Validation loss: 2.4258289633332324

Epoch: 6| Step: 8
Training loss: 3.219262091630909
Validation loss: 2.435158254921763

Epoch: 6| Step: 9
Training loss: 2.504654842361321
Validation loss: 2.4435230055181583

Epoch: 6| Step: 10
Training loss: 3.3621137943196673
Validation loss: 2.4335222942373984

Epoch: 6| Step: 11
Training loss: 2.946611436726161
Validation loss: 2.420626623633335

Epoch: 6| Step: 12
Training loss: 2.5417053553237143
Validation loss: 2.4268235802546245

Epoch: 6| Step: 13
Training loss: 2.589404038605502
Validation loss: 2.4038302908816527

Epoch: 95| Step: 0
Training loss: 2.777490024497233
Validation loss: 2.4261094012833637

Epoch: 6| Step: 1
Training loss: 3.136996778277955
Validation loss: 2.423656782837323

Epoch: 6| Step: 2
Training loss: 2.6614217338857373
Validation loss: 2.440254548158239

Epoch: 6| Step: 3
Training loss: 2.179708720004795
Validation loss: 2.4264722989114165

Epoch: 6| Step: 4
Training loss: 2.464025201315625
Validation loss: 2.4287951604438907

Epoch: 6| Step: 5
Training loss: 2.8156066690709824
Validation loss: 2.423612911956579

Epoch: 6| Step: 6
Training loss: 2.840802855846159
Validation loss: 2.435114451231088

Epoch: 6| Step: 7
Training loss: 3.2361816667788457
Validation loss: 2.4432761362405597

Epoch: 6| Step: 8
Training loss: 2.7635773228083607
Validation loss: 2.4500275204334816

Epoch: 6| Step: 9
Training loss: 2.0699181523013324
Validation loss: 2.432568964445615

Epoch: 6| Step: 10
Training loss: 2.5912016333828967
Validation loss: 2.4630240292609793

Epoch: 6| Step: 11
Training loss: 2.673137522666556
Validation loss: 2.4212811851196805

Epoch: 6| Step: 12
Training loss: 2.923160046001761
Validation loss: 2.4591476710836266

Epoch: 6| Step: 13
Training loss: 2.98104256921962
Validation loss: 2.4541783419789116

Epoch: 96| Step: 0
Training loss: 2.612913462213559
Validation loss: 2.43848023636572

Epoch: 6| Step: 1
Training loss: 3.0605097940678054
Validation loss: 2.417752894513443

Epoch: 6| Step: 2
Training loss: 2.507797478841793
Validation loss: 2.4253678586108536

Epoch: 6| Step: 3
Training loss: 2.6724995557964872
Validation loss: 2.4265979815148064

Epoch: 6| Step: 4
Training loss: 3.055753790079616
Validation loss: 2.4377624790043457

Epoch: 6| Step: 5
Training loss: 1.8969590195174264
Validation loss: 2.441607156342119

Epoch: 6| Step: 6
Training loss: 2.97680631024523
Validation loss: 2.4351413601480654

Epoch: 6| Step: 7
Training loss: 2.961754517576988
Validation loss: 2.410976240439785

Epoch: 6| Step: 8
Training loss: 2.6543788827068764
Validation loss: 2.4253592143590534

Epoch: 6| Step: 9
Training loss: 2.5833705366183484
Validation loss: 2.4347076587218703

Epoch: 6| Step: 10
Training loss: 2.103642338555205
Validation loss: 2.4128342747727847

Epoch: 6| Step: 11
Training loss: 3.129624264135867
Validation loss: 2.4288815537056014

Epoch: 6| Step: 12
Training loss: 3.036919708707932
Validation loss: 2.4604874816177382

Epoch: 6| Step: 13
Training loss: 2.4596800482870544
Validation loss: 2.4209764023578417

Epoch: 97| Step: 0
Training loss: 2.898516137220818
Validation loss: 2.444489848467849

Epoch: 6| Step: 1
Training loss: 2.7435524523966213
Validation loss: 2.4446808631766626

Epoch: 6| Step: 2
Training loss: 2.762713090091706
Validation loss: 2.4403748157243057

Epoch: 6| Step: 3
Training loss: 2.787161042684194
Validation loss: 2.4609735739193512

Epoch: 6| Step: 4
Training loss: 2.9400883086559344
Validation loss: 2.4318620336088896

Epoch: 6| Step: 5
Training loss: 2.8684258969798826
Validation loss: 2.4522489518789667

Epoch: 6| Step: 6
Training loss: 2.2526132454808616
Validation loss: 2.444616328796823

Epoch: 6| Step: 7
Training loss: 2.6512373356784216
Validation loss: 2.435593313400458

Epoch: 6| Step: 8
Training loss: 2.9033010965244506
Validation loss: 2.4180683590790513

Epoch: 6| Step: 9
Training loss: 2.6021520731275736
Validation loss: 2.4407353297295575

Epoch: 6| Step: 10
Training loss: 2.857445070768377
Validation loss: 2.451796319792336

Epoch: 6| Step: 11
Training loss: 2.4145031375639485
Validation loss: 2.4486867534234515

Epoch: 6| Step: 12
Training loss: 2.975530971298606
Validation loss: 2.4339111997537173

Epoch: 6| Step: 13
Training loss: 2.8556242142507364
Validation loss: 2.4409235336377173

Epoch: 98| Step: 0
Training loss: 2.6275693036057994
Validation loss: 2.452431269189764

Epoch: 6| Step: 1
Training loss: 3.06519525998125
Validation loss: 2.431315323838635

Epoch: 6| Step: 2
Training loss: 3.5556656717759334
Validation loss: 2.442437926760962

Epoch: 6| Step: 3
Training loss: 3.0833540907152504
Validation loss: 2.4504229382072262

Epoch: 6| Step: 4
Training loss: 2.6051652951799356
Validation loss: 2.4508318283566757

Epoch: 6| Step: 5
Training loss: 2.8902541463964235
Validation loss: 2.4432223755809455

Epoch: 6| Step: 6
Training loss: 1.9296392519706815
Validation loss: 2.4452429488328415

Epoch: 6| Step: 7
Training loss: 3.092931774098568
Validation loss: 2.472329052791529

Epoch: 6| Step: 8
Training loss: 2.683350842183805
Validation loss: 2.4641178600847202

Epoch: 6| Step: 9
Training loss: 2.534741666071851
Validation loss: 2.4565761689957384

Epoch: 6| Step: 10
Training loss: 2.47980852126208
Validation loss: 2.428763892060925

Epoch: 6| Step: 11
Training loss: 2.4464781301687624
Validation loss: 2.454569376545145

Epoch: 6| Step: 12
Training loss: 2.47947968716079
Validation loss: 2.429385729381393

Epoch: 6| Step: 13
Training loss: 2.21142615859251
Validation loss: 2.4536342213814155

Epoch: 99| Step: 0
Training loss: 2.451143472818517
Validation loss: 2.4325356531154068

Epoch: 6| Step: 1
Training loss: 2.9321127656846553
Validation loss: 2.4354859701301184

Epoch: 6| Step: 2
Training loss: 2.09246775521395
Validation loss: 2.4707521881766423

Epoch: 6| Step: 3
Training loss: 2.719483725974444
Validation loss: 2.4197973468771714

Epoch: 6| Step: 4
Training loss: 3.15503117662312
Validation loss: 2.417796950187848

Epoch: 6| Step: 5
Training loss: 2.6000292042779334
Validation loss: 2.4469092571131053

Epoch: 6| Step: 6
Training loss: 2.3596623732014987
Validation loss: 2.445687142743797

Epoch: 6| Step: 7
Training loss: 2.77444207708709
Validation loss: 2.431026766367372

Epoch: 6| Step: 8
Training loss: 2.649832749037168
Validation loss: 2.4584799511296187

Epoch: 6| Step: 9
Training loss: 2.8758120841124932
Validation loss: 2.4223057401222503

Epoch: 6| Step: 10
Training loss: 2.6277717307542936
Validation loss: 2.4531039348173613

Epoch: 6| Step: 11
Training loss: 3.1372735659894957
Validation loss: 2.424974422512351

Epoch: 6| Step: 12
Training loss: 3.0541896560726594
Validation loss: 2.430764843025926

Epoch: 6| Step: 13
Training loss: 2.848213633132771
Validation loss: 2.425255133161461

Epoch: 100| Step: 0
Training loss: 2.1935730691621416
Validation loss: 2.455425608329899

Epoch: 6| Step: 1
Training loss: 2.694027270852356
Validation loss: 2.4357311908187804

Epoch: 6| Step: 2
Training loss: 2.817810385801502
Validation loss: 2.4442929307236247

Epoch: 6| Step: 3
Training loss: 2.0994903672668026
Validation loss: 2.424802897564002

Epoch: 6| Step: 4
Training loss: 2.942848206743278
Validation loss: 2.4527548568273314

Epoch: 6| Step: 5
Training loss: 3.410054872804053
Validation loss: 2.439156362076809

Epoch: 6| Step: 6
Training loss: 2.7680305321172667
Validation loss: 2.4350650637514346

Epoch: 6| Step: 7
Training loss: 2.8368587626387316
Validation loss: 2.4611189663820605

Epoch: 6| Step: 8
Training loss: 2.8417243869620923
Validation loss: 2.417302650158694

Epoch: 6| Step: 9
Training loss: 2.8283303539265674
Validation loss: 2.460589367680469

Epoch: 6| Step: 10
Training loss: 2.660775133645948
Validation loss: 2.4303658487457844

Epoch: 6| Step: 11
Training loss: 2.872684707479898
Validation loss: 2.4369370147174365

Epoch: 6| Step: 12
Training loss: 2.641352987669151
Validation loss: 2.4354052633839642

Epoch: 6| Step: 13
Training loss: 2.4501698182088365
Validation loss: 2.4505512540398757

Epoch: 101| Step: 0
Training loss: 2.69243169331308
Validation loss: 2.4355079392578567

Epoch: 6| Step: 1
Training loss: 2.4040641269160155
Validation loss: 2.4516720450272675

Epoch: 6| Step: 2
Training loss: 3.375090986367592
Validation loss: 2.435467370268094

Epoch: 6| Step: 3
Training loss: 2.3493821691437673
Validation loss: 2.467813947031609

Epoch: 6| Step: 4
Training loss: 2.0886208637344303
Validation loss: 2.4462948053757736

Epoch: 6| Step: 5
Training loss: 2.7680277758621457
Validation loss: 2.4364741648603987

Epoch: 6| Step: 6
Training loss: 2.780565241837967
Validation loss: 2.4497674533224725

Epoch: 6| Step: 7
Training loss: 2.9743313975704258
Validation loss: 2.442682542950527

Epoch: 6| Step: 8
Training loss: 2.746512021677183
Validation loss: 2.4635592161714372

Epoch: 6| Step: 9
Training loss: 2.3920565040446853
Validation loss: 2.4416451727009862

Epoch: 6| Step: 10
Training loss: 3.3441913349133117
Validation loss: 2.4586960858783633

Epoch: 6| Step: 11
Training loss: 2.8884940876133443
Validation loss: 2.442883157529474

Epoch: 6| Step: 12
Training loss: 2.5729356731582795
Validation loss: 2.4323847199254436

Epoch: 6| Step: 13
Training loss: 2.8891690273255097
Validation loss: 2.4445561710988004

Epoch: 102| Step: 0
Training loss: 3.077193530741082
Validation loss: 2.4464515219942555

Epoch: 6| Step: 1
Training loss: 2.972495517069626
Validation loss: 2.424885089470293

Epoch: 6| Step: 2
Training loss: 2.7291576200922543
Validation loss: 2.4554653495573002

Epoch: 6| Step: 3
Training loss: 2.9455326716447336
Validation loss: 2.457035661446894

Epoch: 6| Step: 4
Training loss: 2.892667471505843
Validation loss: 2.4557188470970064

Epoch: 6| Step: 5
Training loss: 2.402373225333659
Validation loss: 2.448837037783024

Epoch: 6| Step: 6
Training loss: 2.643784478145358
Validation loss: 2.4495580346988097

Epoch: 6| Step: 7
Training loss: 2.147578619160676
Validation loss: 2.437495497454073

Epoch: 6| Step: 8
Training loss: 2.825117334950496
Validation loss: 2.4525672299916983

Epoch: 6| Step: 9
Training loss: 3.0567471714158003
Validation loss: 2.4508979950602194

Epoch: 6| Step: 10
Training loss: 2.5742978444198465
Validation loss: 2.454884332126979

Epoch: 6| Step: 11
Training loss: 2.838763381745055
Validation loss: 2.4479901793435275

Epoch: 6| Step: 12
Training loss: 2.361010862229342
Validation loss: 2.4356336571506105

Epoch: 6| Step: 13
Training loss: 2.7536504099080137
Validation loss: 2.4844416911679663

Epoch: 103| Step: 0
Training loss: 2.585626773663033
Validation loss: 2.458123807353778

Epoch: 6| Step: 1
Training loss: 2.7068480502583627
Validation loss: 2.432510456429113

Epoch: 6| Step: 2
Training loss: 2.8034323431334767
Validation loss: 2.444237885560614

Epoch: 6| Step: 3
Training loss: 2.300891815274974
Validation loss: 2.4438521853818216

Epoch: 6| Step: 4
Training loss: 3.029601917366366
Validation loss: 2.457325042574593

Epoch: 6| Step: 5
Training loss: 3.0023111342108506
Validation loss: 2.4672838610697965

Epoch: 6| Step: 6
Training loss: 2.85327130681058
Validation loss: 2.433299865659665

Epoch: 6| Step: 7
Training loss: 2.5529439500094098
Validation loss: 2.4183569531954316

Epoch: 6| Step: 8
Training loss: 2.8847183502600644
Validation loss: 2.451241023669343

Epoch: 6| Step: 9
Training loss: 2.613158173221657
Validation loss: 2.4566377490013256

Epoch: 6| Step: 10
Training loss: 2.7037806074197444
Validation loss: 2.4316189087541598

Epoch: 6| Step: 11
Training loss: 3.4304390259864324
Validation loss: 2.4533944160638264

Epoch: 6| Step: 12
Training loss: 2.477960907745508
Validation loss: 2.4369391791920343

Epoch: 6| Step: 13
Training loss: 1.976259049956455
Validation loss: 2.4180203569019905

Epoch: 104| Step: 0
Training loss: 3.3507924295613805
Validation loss: 2.4678057423339164

Epoch: 6| Step: 1
Training loss: 2.8905835947731355
Validation loss: 2.4395469629679356

Epoch: 6| Step: 2
Training loss: 2.7902645651508347
Validation loss: 2.447013530938509

Epoch: 6| Step: 3
Training loss: 1.9394301367988696
Validation loss: 2.4250683318053814

Epoch: 6| Step: 4
Training loss: 2.5350652128781466
Validation loss: 2.445193026775541

Epoch: 6| Step: 5
Training loss: 3.5705805371128765
Validation loss: 2.464581063188848

Epoch: 6| Step: 6
Training loss: 2.5173434435929654
Validation loss: 2.4360813601369156

Epoch: 6| Step: 7
Training loss: 3.1465386280044956
Validation loss: 2.4712935460496945

Epoch: 6| Step: 8
Training loss: 2.3929460016926
Validation loss: 2.4312513995127683

Epoch: 6| Step: 9
Training loss: 2.5785351975993636
Validation loss: 2.4334274780608314

Epoch: 6| Step: 10
Training loss: 2.1261873854758755
Validation loss: 2.4339004455286997

Epoch: 6| Step: 11
Training loss: 2.61442762015051
Validation loss: 2.4623619122909153

Epoch: 6| Step: 12
Training loss: 2.635191072199417
Validation loss: 2.43852859430979

Epoch: 6| Step: 13
Training loss: 3.0491279293357905
Validation loss: 2.4290249108384234

Epoch: 105| Step: 0
Training loss: 2.4259249388175337
Validation loss: 2.4484022253700397

Epoch: 6| Step: 1
Training loss: 1.9672620085042993
Validation loss: 2.42809161972038

Epoch: 6| Step: 2
Training loss: 3.032757099319713
Validation loss: 2.4399426907275057

Epoch: 6| Step: 3
Training loss: 3.3604107346849927
Validation loss: 2.42648343788102

Epoch: 6| Step: 4
Training loss: 2.607456072750926
Validation loss: 2.433050675719037

Epoch: 6| Step: 5
Training loss: 2.4904932463484863
Validation loss: 2.443677564620157

Epoch: 6| Step: 6
Training loss: 2.344510273011737
Validation loss: 2.456892322932459

Epoch: 6| Step: 7
Training loss: 2.753570406300226
Validation loss: 2.4148889529997097

Epoch: 6| Step: 8
Training loss: 3.1491180517383244
Validation loss: 2.463133787659302

Epoch: 6| Step: 9
Training loss: 3.1442886164155337
Validation loss: 2.4265727411340645

Epoch: 6| Step: 10
Training loss: 2.769519714417531
Validation loss: 2.422160063767005

Epoch: 6| Step: 11
Training loss: 2.6641322151973785
Validation loss: 2.430121150029872

Epoch: 6| Step: 12
Training loss: 2.646194002895149
Validation loss: 2.4371635976859776

Epoch: 6| Step: 13
Training loss: 2.5499840231002247
Validation loss: 2.4244365862649997

Epoch: 106| Step: 0
Training loss: 2.722047494181061
Validation loss: 2.443940134370019

Epoch: 6| Step: 1
Training loss: 2.756118383746796
Validation loss: 2.451105628879056

Epoch: 6| Step: 2
Training loss: 3.1041906641772874
Validation loss: 2.460204183400263

Epoch: 6| Step: 3
Training loss: 2.7755668954623527
Validation loss: 2.433956508119595

Epoch: 6| Step: 4
Training loss: 2.5387901746988586
Validation loss: 2.4402555861140587

Epoch: 6| Step: 5
Training loss: 2.401445025299078
Validation loss: 2.4619712573842825

Epoch: 6| Step: 6
Training loss: 2.3104630855739297
Validation loss: 2.428378080964923

Epoch: 6| Step: 7
Training loss: 2.2143659665028177
Validation loss: 2.412750733064047

Epoch: 6| Step: 8
Training loss: 2.5788053597421925
Validation loss: 2.426863421186493

Epoch: 6| Step: 9
Training loss: 3.7472032926543353
Validation loss: 2.4364302490926697

Epoch: 6| Step: 10
Training loss: 2.555588216388623
Validation loss: 2.435973273797622

Epoch: 6| Step: 11
Training loss: 2.1140644081936024
Validation loss: 2.468799266061407

Epoch: 6| Step: 12
Training loss: 3.317129822282139
Validation loss: 2.4416871425693083

Epoch: 6| Step: 13
Training loss: 2.4446747652073477
Validation loss: 2.4360151519561453

Epoch: 107| Step: 0
Training loss: 2.3063088096832476
Validation loss: 2.4565769000253805

Epoch: 6| Step: 1
Training loss: 2.468020947161892
Validation loss: 2.447957379472312

Epoch: 6| Step: 2
Training loss: 2.740122134138834
Validation loss: 2.445441969565292

Epoch: 6| Step: 3
Training loss: 2.750033465095119
Validation loss: 2.446474644883232

Epoch: 6| Step: 4
Training loss: 2.616381347391941
Validation loss: 2.41887898503045

Epoch: 6| Step: 5
Training loss: 2.5137113317610336
Validation loss: 2.4387912595720143

Epoch: 6| Step: 6
Training loss: 2.3018115704570374
Validation loss: 2.4470256554891896

Epoch: 6| Step: 7
Training loss: 2.9151628886493457
Validation loss: 2.461931151545102

Epoch: 6| Step: 8
Training loss: 3.0038037028375646
Validation loss: 2.44712188916394

Epoch: 6| Step: 9
Training loss: 2.9143328093882745
Validation loss: 2.417441835894096

Epoch: 6| Step: 10
Training loss: 3.429779816169304
Validation loss: 2.437640339377046

Epoch: 6| Step: 11
Training loss: 2.6528997498632902
Validation loss: 2.4213100974646413

Epoch: 6| Step: 12
Training loss: 2.6973865434970685
Validation loss: 2.465056760074943

Epoch: 6| Step: 13
Training loss: 3.0749127817577984
Validation loss: 2.4342324247699563

Epoch: 108| Step: 0
Training loss: 3.0883090793119057
Validation loss: 2.4149975297132604

Epoch: 6| Step: 1
Training loss: 2.288798346266671
Validation loss: 2.447201421201055

Epoch: 6| Step: 2
Training loss: 2.820310640202382
Validation loss: 2.4376736398444327

Epoch: 6| Step: 3
Training loss: 2.833440180241129
Validation loss: 2.459695579042176

Epoch: 6| Step: 4
Training loss: 2.7249786236125795
Validation loss: 2.431344072577228

Epoch: 6| Step: 5
Training loss: 2.6152608127981267
Validation loss: 2.4244754354569533

Epoch: 6| Step: 6
Training loss: 3.3260279393034784
Validation loss: 2.454546564912343

Epoch: 6| Step: 7
Training loss: 2.769270912922778
Validation loss: 2.431340486516197

Epoch: 6| Step: 8
Training loss: 2.8704413055441873
Validation loss: 2.4449991308927466

Epoch: 6| Step: 9
Training loss: 2.4839462774060364
Validation loss: 2.4400607851695337

Epoch: 6| Step: 10
Training loss: 2.4058611667116643
Validation loss: 2.4243507108463054

Epoch: 6| Step: 11
Training loss: 2.703394617294639
Validation loss: 2.432580431708584

Epoch: 6| Step: 12
Training loss: 2.622162238699351
Validation loss: 2.4446329525478387

Epoch: 6| Step: 13
Training loss: 1.8840903856419178
Validation loss: 2.442265361485552

Epoch: 109| Step: 0
Training loss: 2.6907807662836727
Validation loss: 2.4463945867382053

Epoch: 6| Step: 1
Training loss: 2.418953591780341
Validation loss: 2.425332755019729

Epoch: 6| Step: 2
Training loss: 2.642911564329241
Validation loss: 2.4252339728384005

Epoch: 6| Step: 3
Training loss: 2.401932049961413
Validation loss: 2.4551380979745385

Epoch: 6| Step: 4
Training loss: 2.786142396693119
Validation loss: 2.4362552968178943

Epoch: 6| Step: 5
Training loss: 3.00376560392863
Validation loss: 2.4113484972468493

Epoch: 6| Step: 6
Training loss: 2.7864531805163124
Validation loss: 2.442088424309161

Epoch: 6| Step: 7
Training loss: 2.764014944613033
Validation loss: 2.4267996928119415

Epoch: 6| Step: 8
Training loss: 3.2828445193341946
Validation loss: 2.441327128254191

Epoch: 6| Step: 9
Training loss: 2.551502353685816
Validation loss: 2.432264608524413

Epoch: 6| Step: 10
Training loss: 2.0784259090694013
Validation loss: 2.428196575992193

Epoch: 6| Step: 11
Training loss: 3.295794703228658
Validation loss: 2.4379559877366237

Epoch: 6| Step: 12
Training loss: 2.641423753546708
Validation loss: 2.4405274417985683

Epoch: 6| Step: 13
Training loss: 2.695328864448737
Validation loss: 2.433603418436654

Epoch: 110| Step: 0
Training loss: 2.8956319798418972
Validation loss: 2.4462684928335934

Epoch: 6| Step: 1
Training loss: 2.647365566943455
Validation loss: 2.449105602435831

Epoch: 6| Step: 2
Training loss: 2.561021890233158
Validation loss: 2.436345306762957

Epoch: 6| Step: 3
Training loss: 2.4335212513027273
Validation loss: 2.4263808693138813

Epoch: 6| Step: 4
Training loss: 2.7727094527466427
Validation loss: 2.4458205713414767

Epoch: 6| Step: 5
Training loss: 3.1140546233005315
Validation loss: 2.4318622286337273

Epoch: 6| Step: 6
Training loss: 2.5981585253657844
Validation loss: 2.449091254371342

Epoch: 6| Step: 7
Training loss: 2.929173131929283
Validation loss: 2.4615398742088104

Epoch: 6| Step: 8
Training loss: 2.9984255473906014
Validation loss: 2.4479271407853482

Epoch: 6| Step: 9
Training loss: 2.817825361960498
Validation loss: 2.455236461173613

Epoch: 6| Step: 10
Training loss: 2.212521150994361
Validation loss: 2.445377645024823

Epoch: 6| Step: 11
Training loss: 2.8545464775777196
Validation loss: 2.4399511330638557

Epoch: 6| Step: 12
Training loss: 2.100607048577668
Validation loss: 2.4272804530534375

Epoch: 6| Step: 13
Training loss: 3.1710062575505593
Validation loss: 2.4349550877081434

Epoch: 111| Step: 0
Training loss: 2.0940649450481685
Validation loss: 2.437805750843884

Epoch: 6| Step: 1
Training loss: 3.458157699598584
Validation loss: 2.4533650333269006

Epoch: 6| Step: 2
Training loss: 2.5401533864482615
Validation loss: 2.4151714637254664

Epoch: 6| Step: 3
Training loss: 2.508025448513357
Validation loss: 2.423626500114623

Epoch: 6| Step: 4
Training loss: 2.78543769944514
Validation loss: 2.4173474792026095

Epoch: 6| Step: 5
Training loss: 1.90121539293202
Validation loss: 2.466578527951458

Epoch: 6| Step: 6
Training loss: 2.9180532338526906
Validation loss: 2.4157685282403403

Epoch: 6| Step: 7
Training loss: 2.5548229123312645
Validation loss: 2.4302912495045383

Epoch: 6| Step: 8
Training loss: 2.8634424515502044
Validation loss: 2.4308093953243692

Epoch: 6| Step: 9
Training loss: 2.930593609876675
Validation loss: 2.429829142044863

Epoch: 6| Step: 10
Training loss: 2.500975513867654
Validation loss: 2.4470703491253785

Epoch: 6| Step: 11
Training loss: 3.4933536639119422
Validation loss: 2.4283456886818326

Epoch: 6| Step: 12
Training loss: 2.3060723750658276
Validation loss: 2.4373107007871213

Epoch: 6| Step: 13
Training loss: 2.819087388512645
Validation loss: 2.445305903522603

Epoch: 112| Step: 0
Training loss: 2.226253126635918
Validation loss: 2.439678881104584

Epoch: 6| Step: 1
Training loss: 2.6365861873505363
Validation loss: 2.442148377209188

Epoch: 6| Step: 2
Training loss: 2.201301631809358
Validation loss: 2.4571400136455583

Epoch: 6| Step: 3
Training loss: 2.574564006152759
Validation loss: 2.408078021742901

Epoch: 6| Step: 4
Training loss: 2.611714028580415
Validation loss: 2.4310058482124863

Epoch: 6| Step: 5
Training loss: 3.632058473494447
Validation loss: 2.4583726639588246

Epoch: 6| Step: 6
Training loss: 3.144732318396928
Validation loss: 2.449539185851454

Epoch: 6| Step: 7
Training loss: 2.4743893110762816
Validation loss: 2.440647118200751

Epoch: 6| Step: 8
Training loss: 2.66470731835004
Validation loss: 2.449330388272959

Epoch: 6| Step: 9
Training loss: 2.909539278925925
Validation loss: 2.470636505016074

Epoch: 6| Step: 10
Training loss: 2.6970760160036824
Validation loss: 2.4339481397860627

Epoch: 6| Step: 11
Training loss: 2.7999381228830535
Validation loss: 2.4285020529659143

Epoch: 6| Step: 12
Training loss: 3.008516303615637
Validation loss: 2.453266497909197

Epoch: 6| Step: 13
Training loss: 1.5997204119066406
Validation loss: 2.4458565605863374

Epoch: 113| Step: 0
Training loss: 3.7539095526292705
Validation loss: 2.4593259192090158

Epoch: 6| Step: 1
Training loss: 2.5310552074691763
Validation loss: 2.423669454736555

Epoch: 6| Step: 2
Training loss: 3.132165095152006
Validation loss: 2.4252579893363944

Epoch: 6| Step: 3
Training loss: 2.0829520703175075
Validation loss: 2.449331194208726

Epoch: 6| Step: 4
Training loss: 2.6705555474311486
Validation loss: 2.4531460482810803

Epoch: 6| Step: 5
Training loss: 2.0348955745288406
Validation loss: 2.438356826877417

Epoch: 6| Step: 6
Training loss: 1.7438749343798776
Validation loss: 2.44379434089265

Epoch: 6| Step: 7
Training loss: 2.8468875553930397
Validation loss: 2.452986565508666

Epoch: 6| Step: 8
Training loss: 2.111044893843625
Validation loss: 2.448463313625131

Epoch: 6| Step: 9
Training loss: 2.700676928539393
Validation loss: 2.4507745858550787

Epoch: 6| Step: 10
Training loss: 3.259955782912242
Validation loss: 2.437933699962802

Epoch: 6| Step: 11
Training loss: 3.0953212562679813
Validation loss: 2.4555382819734293

Epoch: 6| Step: 12
Training loss: 2.8657523989047995
Validation loss: 2.453306474747083

Epoch: 6| Step: 13
Training loss: 2.405741452069216
Validation loss: 2.4413214766027957

Epoch: 114| Step: 0
Training loss: 2.4679116865892823
Validation loss: 2.428476681416067

Epoch: 6| Step: 1
Training loss: 2.8543142758553346
Validation loss: 2.445175837000614

Epoch: 6| Step: 2
Training loss: 2.954556879941817
Validation loss: 2.4685012995336617

Epoch: 6| Step: 3
Training loss: 2.7053795381488657
Validation loss: 2.4522297484048727

Epoch: 6| Step: 4
Training loss: 2.3013125779213106
Validation loss: 2.443522029276853

Epoch: 6| Step: 5
Training loss: 2.724432082885417
Validation loss: 2.4439729818617613

Epoch: 6| Step: 6
Training loss: 2.967944869236316
Validation loss: 2.454227366770033

Epoch: 6| Step: 7
Training loss: 3.1917876782563166
Validation loss: 2.472502565068515

Epoch: 6| Step: 8
Training loss: 2.4794372816944175
Validation loss: 2.431071362901458

Epoch: 6| Step: 9
Training loss: 2.983062615356686
Validation loss: 2.4311528356723753

Epoch: 6| Step: 10
Training loss: 2.2645227315936345
Validation loss: 2.431120554175848

Epoch: 6| Step: 11
Training loss: 2.7107730342875276
Validation loss: 2.444691249089729

Epoch: 6| Step: 12
Training loss: 2.6132047040417308
Validation loss: 2.4270846223271016

Epoch: 6| Step: 13
Training loss: 2.863834426547852
Validation loss: 2.4513561987910575

Epoch: 115| Step: 0
Training loss: 2.4308886732677184
Validation loss: 2.46017084610759

Epoch: 6| Step: 1
Training loss: 2.7388784761297527
Validation loss: 2.461459873827655

Epoch: 6| Step: 2
Training loss: 2.7362521458491473
Validation loss: 2.4255131932762932

Epoch: 6| Step: 3
Training loss: 2.3437451171824137
Validation loss: 2.3988861292903514

Epoch: 6| Step: 4
Training loss: 3.1385989510041163
Validation loss: 2.4914450727548663

Epoch: 6| Step: 5
Training loss: 3.409391861549591
Validation loss: 2.4532604777104803

Epoch: 6| Step: 6
Training loss: 2.090192933189499
Validation loss: 2.4524492297462297

Epoch: 6| Step: 7
Training loss: 2.986752985688969
Validation loss: 2.4378895759841517

Epoch: 6| Step: 8
Training loss: 1.6856470177739684
Validation loss: 2.426569073011175

Epoch: 6| Step: 9
Training loss: 2.230240203853012
Validation loss: 2.433711184457834

Epoch: 6| Step: 10
Training loss: 3.151133403020699
Validation loss: 2.4454118879524045

Epoch: 6| Step: 11
Training loss: 2.6231370400549423
Validation loss: 2.4566753365803953

Epoch: 6| Step: 12
Training loss: 3.3451761787216845
Validation loss: 2.4572176298597745

Epoch: 6| Step: 13
Training loss: 2.3982821237143845
Validation loss: 2.462792513376629

Epoch: 116| Step: 0
Training loss: 2.926191273213636
Validation loss: 2.449255658035

Epoch: 6| Step: 1
Training loss: 3.0570931486613695
Validation loss: 2.4633168708495066

Epoch: 6| Step: 2
Training loss: 2.555688784300805
Validation loss: 2.447816930417775

Epoch: 6| Step: 3
Training loss: 2.575774537419929
Validation loss: 2.438844637524193

Epoch: 6| Step: 4
Training loss: 2.0751051221209935
Validation loss: 2.428139774380344

Epoch: 6| Step: 5
Training loss: 2.6412579381690766
Validation loss: 2.4507415449954952

Epoch: 6| Step: 6
Training loss: 2.579613550652351
Validation loss: 2.458259842825494

Epoch: 6| Step: 7
Training loss: 2.4317589665919486
Validation loss: 2.4547932829680206

Epoch: 6| Step: 8
Training loss: 2.310894073429074
Validation loss: 2.4636336582177774

Epoch: 6| Step: 9
Training loss: 2.622784087991785
Validation loss: 2.4428059380265905

Epoch: 6| Step: 10
Training loss: 2.831923320949871
Validation loss: 2.429086015173962

Epoch: 6| Step: 11
Training loss: 3.268412299426957
Validation loss: 2.4596727409499857

Epoch: 6| Step: 12
Training loss: 3.2235655085654296
Validation loss: 2.436784646128576

Epoch: 6| Step: 13
Training loss: 2.948305425632239
Validation loss: 2.455924043557008

Epoch: 117| Step: 0
Training loss: 2.5339628211337155
Validation loss: 2.460293637070207

Epoch: 6| Step: 1
Training loss: 2.8118237848105228
Validation loss: 2.455095324280369

Epoch: 6| Step: 2
Training loss: 2.5267950343769385
Validation loss: 2.4235150064229702

Epoch: 6| Step: 3
Training loss: 2.44598847329893
Validation loss: 2.456437140004632

Epoch: 6| Step: 4
Training loss: 2.382141419012175
Validation loss: 2.4278838771997044

Epoch: 6| Step: 5
Training loss: 2.8309568274415438
Validation loss: 2.40361189113605

Epoch: 6| Step: 6
Training loss: 2.99880130979935
Validation loss: 2.418935679799331

Epoch: 6| Step: 7
Training loss: 2.795991214514643
Validation loss: 2.413092678361094

Epoch: 6| Step: 8
Training loss: 2.3470783252234866
Validation loss: 2.4414939185368407

Epoch: 6| Step: 9
Training loss: 3.2289687229318087
Validation loss: 2.4605368146554647

Epoch: 6| Step: 10
Training loss: 2.5772066272383363
Validation loss: 2.4539889769809813

Epoch: 6| Step: 11
Training loss: 3.1378711374542703
Validation loss: 2.4502212292268744

Epoch: 6| Step: 12
Training loss: 2.557336769301911
Validation loss: 2.4511619092771952

Epoch: 6| Step: 13
Training loss: 2.826788570711407
Validation loss: 2.4114924198784586

Epoch: 118| Step: 0
Training loss: 2.6660215272942906
Validation loss: 2.42508309685727

Epoch: 6| Step: 1
Training loss: 2.7308963991933934
Validation loss: 2.432306507322936

Epoch: 6| Step: 2
Training loss: 2.8870225057434924
Validation loss: 2.431942804179273

Epoch: 6| Step: 3
Training loss: 3.2617987160388418
Validation loss: 2.4422043386395877

Epoch: 6| Step: 4
Training loss: 2.7283904061225246
Validation loss: 2.4420523611608536

Epoch: 6| Step: 5
Training loss: 2.7470336868225855
Validation loss: 2.444373861077205

Epoch: 6| Step: 6
Training loss: 2.327108378831943
Validation loss: 2.423300104103693

Epoch: 6| Step: 7
Training loss: 2.2630682890159353
Validation loss: 2.444665728859242

Epoch: 6| Step: 8
Training loss: 2.3115019835559107
Validation loss: 2.4578103887039653

Epoch: 6| Step: 9
Training loss: 3.399458247387725
Validation loss: 2.4329621118091325

Epoch: 6| Step: 10
Training loss: 3.0017518650512036
Validation loss: 2.401316851857958

Epoch: 6| Step: 11
Training loss: 2.462242337606427
Validation loss: 2.465612380029799

Epoch: 6| Step: 12
Training loss: 2.192496995564994
Validation loss: 2.4507008107956003

Epoch: 6| Step: 13
Training loss: 2.539590727806018
Validation loss: 2.4501122358764174

Epoch: 119| Step: 0
Training loss: 2.7898242241510767
Validation loss: 2.447494796884322

Epoch: 6| Step: 1
Training loss: 3.1556692061158333
Validation loss: 2.4410940629113362

Epoch: 6| Step: 2
Training loss: 2.7803931470101344
Validation loss: 2.444985079580236

Epoch: 6| Step: 3
Training loss: 2.4718406742379533
Validation loss: 2.396766861302749

Epoch: 6| Step: 4
Training loss: 2.8519203275062504
Validation loss: 2.475383156454096

Epoch: 6| Step: 5
Training loss: 2.6368160427651737
Validation loss: 2.4349822531729317

Epoch: 6| Step: 6
Training loss: 2.958229779504075
Validation loss: 2.4406807347728687

Epoch: 6| Step: 7
Training loss: 2.4283659551554835
Validation loss: 2.4722006107332204

Epoch: 6| Step: 8
Training loss: 2.698568218800213
Validation loss: 2.458607892082249

Epoch: 6| Step: 9
Training loss: 2.691959761575324
Validation loss: 2.4437619525839627

Epoch: 6| Step: 10
Training loss: 2.819693796023341
Validation loss: 2.451624960132996

Epoch: 6| Step: 11
Training loss: 2.3694303355878303
Validation loss: 2.46704327409978

Epoch: 6| Step: 12
Training loss: 2.494126668605725
Validation loss: 2.473261001484945

Epoch: 6| Step: 13
Training loss: 2.9621727613800175
Validation loss: 2.448782943280956

Epoch: 120| Step: 0
Training loss: 2.884475848417695
Validation loss: 2.46023265604162

Epoch: 6| Step: 1
Training loss: 2.583217977183742
Validation loss: 2.432189326526658

Epoch: 6| Step: 2
Training loss: 2.9959600585393793
Validation loss: 2.427596386080882

Epoch: 6| Step: 3
Training loss: 3.1483884291693958
Validation loss: 2.434503265017433

Epoch: 6| Step: 4
Training loss: 3.1664080932521466
Validation loss: 2.4556726702715554

Epoch: 6| Step: 5
Training loss: 2.997838513219612
Validation loss: 2.4667172862293625

Epoch: 6| Step: 6
Training loss: 2.037838506370329
Validation loss: 2.4536912665689665

Epoch: 6| Step: 7
Training loss: 2.418810080242901
Validation loss: 2.461803044859388

Epoch: 6| Step: 8
Training loss: 2.4061809381259613
Validation loss: 2.4574519201166733

Epoch: 6| Step: 9
Training loss: 2.136546014641544
Validation loss: 2.4616189451041897

Epoch: 6| Step: 10
Training loss: 2.9157376217530704
Validation loss: 2.447867041411872

Epoch: 6| Step: 11
Training loss: 2.4781398623623323
Validation loss: 2.4361823038122337

Epoch: 6| Step: 12
Training loss: 2.9920211227813756
Validation loss: 2.446716995574392

Epoch: 6| Step: 13
Training loss: 2.572288982311379
Validation loss: 2.4543675905256395

Epoch: 121| Step: 0
Training loss: 2.83388240954144
Validation loss: 2.4231899379471633

Epoch: 6| Step: 1
Training loss: 2.4725345623813655
Validation loss: 2.4476279174609865

Epoch: 6| Step: 2
Training loss: 2.8592339152565533
Validation loss: 2.459081748768937

Epoch: 6| Step: 3
Training loss: 2.879618707953927
Validation loss: 2.458203239467835

Epoch: 6| Step: 4
Training loss: 2.659057210902859
Validation loss: 2.4066048339064845

Epoch: 6| Step: 5
Training loss: 1.82578799859017
Validation loss: 2.449296937518035

Epoch: 6| Step: 6
Training loss: 2.6245926132155173
Validation loss: 2.4406541904920553

Epoch: 6| Step: 7
Training loss: 2.914433269077751
Validation loss: 2.43204284970685

Epoch: 6| Step: 8
Training loss: 2.3103125252627588
Validation loss: 2.412968744434501

Epoch: 6| Step: 9
Training loss: 2.856608068279882
Validation loss: 2.4448802594415118

Epoch: 6| Step: 10
Training loss: 2.454700037234006
Validation loss: 2.4402327909057875

Epoch: 6| Step: 11
Training loss: 2.372681138225574
Validation loss: 2.4547113269602945

Epoch: 6| Step: 12
Training loss: 3.5939235479494913
Validation loss: 2.4253195527246536

Epoch: 6| Step: 13
Training loss: 2.910394809852909
Validation loss: 2.4456460025470315

Epoch: 122| Step: 0
Training loss: 3.167395223496912
Validation loss: 2.465875777777495

Epoch: 6| Step: 1
Training loss: 2.6056687765820112
Validation loss: 2.4163550574508426

Epoch: 6| Step: 2
Training loss: 2.5731087637373538
Validation loss: 2.4074756253432654

Epoch: 6| Step: 3
Training loss: 2.7625810499082215
Validation loss: 2.450979425895785

Epoch: 6| Step: 4
Training loss: 2.776585986586002
Validation loss: 2.423469442020572

Epoch: 6| Step: 5
Training loss: 2.8962384130437933
Validation loss: 2.420565286156874

Epoch: 6| Step: 6
Training loss: 2.771858044973511
Validation loss: 2.4339006551364775

Epoch: 6| Step: 7
Training loss: 2.700806698556476
Validation loss: 2.473832222919934

Epoch: 6| Step: 8
Training loss: 2.4225502303420403
Validation loss: 2.419371822837636

Epoch: 6| Step: 9
Training loss: 2.5082552034834897
Validation loss: 2.4214365699603153

Epoch: 6| Step: 10
Training loss: 2.72488290377668
Validation loss: 2.434428754650037

Epoch: 6| Step: 11
Training loss: 2.44275275368282
Validation loss: 2.453563360756242

Epoch: 6| Step: 12
Training loss: 2.529880388621407
Validation loss: 2.4498852164858467

Epoch: 6| Step: 13
Training loss: 3.2079013847452496
Validation loss: 2.4433999724348467

Epoch: 123| Step: 0
Training loss: 2.676439903110503
Validation loss: 2.4422134806692504

Epoch: 6| Step: 1
Training loss: 3.132292364230563
Validation loss: 2.444619139279954

Epoch: 6| Step: 2
Training loss: 2.151162600562221
Validation loss: 2.441061849868321

Epoch: 6| Step: 3
Training loss: 2.934715412126174
Validation loss: 2.458608730429318

Epoch: 6| Step: 4
Training loss: 3.0221360828552872
Validation loss: 2.4429310101269435

Epoch: 6| Step: 5
Training loss: 2.680953546954694
Validation loss: 2.455907141319286

Epoch: 6| Step: 6
Training loss: 2.2815356141102354
Validation loss: 2.4245496680391043

Epoch: 6| Step: 7
Training loss: 2.898133953476354
Validation loss: 2.4212496455260584

Epoch: 6| Step: 8
Training loss: 2.378405288039102
Validation loss: 2.404066704349797

Epoch: 6| Step: 9
Training loss: 2.961213192145398
Validation loss: 2.417168571140913

Epoch: 6| Step: 10
Training loss: 2.323110005040661
Validation loss: 2.4342364373131598

Epoch: 6| Step: 11
Training loss: 2.7793186639935605
Validation loss: 2.4516917213262888

Epoch: 6| Step: 12
Training loss: 2.8533069030333817
Validation loss: 2.4686597280845315

Epoch: 6| Step: 13
Training loss: 3.1375747975708186
Validation loss: 2.4335759931808085

Epoch: 124| Step: 0
Training loss: 2.6142955687528486
Validation loss: 2.4508418817186923

Epoch: 6| Step: 1
Training loss: 2.800734345323989
Validation loss: 2.453634947540988

Epoch: 6| Step: 2
Training loss: 2.4798642841243916
Validation loss: 2.454207728558256

Epoch: 6| Step: 3
Training loss: 2.5697999576537427
Validation loss: 2.426191462283846

Epoch: 6| Step: 4
Training loss: 1.7701237173001043
Validation loss: 2.4176846988307776

Epoch: 6| Step: 5
Training loss: 3.5242219377358257
Validation loss: 2.4380174281695033

Epoch: 6| Step: 6
Training loss: 3.2644150239994874
Validation loss: 2.4266571349543096

Epoch: 6| Step: 7
Training loss: 2.4948432189669436
Validation loss: 2.4764270475639902

Epoch: 6| Step: 8
Training loss: 2.204964377186871
Validation loss: 2.460947703756888

Epoch: 6| Step: 9
Training loss: 2.6702184170599055
Validation loss: 2.4484659040023455

Epoch: 6| Step: 10
Training loss: 3.4493851929334474
Validation loss: 2.4488439429994715

Epoch: 6| Step: 11
Training loss: 2.650132708555427
Validation loss: 2.436569951455785

Epoch: 6| Step: 12
Training loss: 2.3217659967026876
Validation loss: 2.4443373827578703

Epoch: 6| Step: 13
Training loss: 2.451259024845144
Validation loss: 2.4488761981424134

Epoch: 125| Step: 0
Training loss: 3.1369723054820438
Validation loss: 2.4341577702500152

Epoch: 6| Step: 1
Training loss: 2.7630653389747857
Validation loss: 2.46706743537703

Epoch: 6| Step: 2
Training loss: 2.326324176046431
Validation loss: 2.4566587536195827

Epoch: 6| Step: 3
Training loss: 2.6208131187965322
Validation loss: 2.436468389367135

Epoch: 6| Step: 4
Training loss: 2.935469229782357
Validation loss: 2.47489403706763

Epoch: 6| Step: 5
Training loss: 2.8179257086334695
Validation loss: 2.4559446158618266

Epoch: 6| Step: 6
Training loss: 2.269852370952611
Validation loss: 2.4685306358153425

Epoch: 6| Step: 7
Training loss: 2.2532589940029477
Validation loss: 2.4766821285841623

Epoch: 6| Step: 8
Training loss: 3.2260987177086458
Validation loss: 2.4425625254570966

Epoch: 6| Step: 9
Training loss: 2.312512990554201
Validation loss: 2.4563230224215467

Epoch: 6| Step: 10
Training loss: 2.8392518068055765
Validation loss: 2.4456884027149504

Epoch: 6| Step: 11
Training loss: 2.917867431515762
Validation loss: 2.4334258735642886

Epoch: 6| Step: 12
Training loss: 2.2027628715986776
Validation loss: 2.447961170016287

Epoch: 6| Step: 13
Training loss: 3.2558665512912066
Validation loss: 2.431475691381158

Epoch: 126| Step: 0
Training loss: 3.0876840019224017
Validation loss: 2.4663140608083447

Epoch: 6| Step: 1
Training loss: 2.047093503515115
Validation loss: 2.42576623661346

Epoch: 6| Step: 2
Training loss: 2.9266921536574824
Validation loss: 2.448626964798832

Epoch: 6| Step: 3
Training loss: 2.0675637477286233
Validation loss: 2.4680586854622684

Epoch: 6| Step: 4
Training loss: 3.2106623102403575
Validation loss: 2.459798661539951

Epoch: 6| Step: 5
Training loss: 2.857459588887308
Validation loss: 2.4588361767765567

Epoch: 6| Step: 6
Training loss: 2.790923195143314
Validation loss: 2.4493546677565927

Epoch: 6| Step: 7
Training loss: 2.516042544382754
Validation loss: 2.4325188297085534

Epoch: 6| Step: 8
Training loss: 2.5993654063534564
Validation loss: 2.4534242185081436

Epoch: 6| Step: 9
Training loss: 2.8667118187239065
Validation loss: 2.4689740543608645

Epoch: 6| Step: 10
Training loss: 2.5132959613837875
Validation loss: 2.4544172606939827

Epoch: 6| Step: 11
Training loss: 2.6868232939652703
Validation loss: 2.4348430156435468

Epoch: 6| Step: 12
Training loss: 2.537299004472698
Validation loss: 2.4406058688870926

Epoch: 6| Step: 13
Training loss: 3.1025608867661743
Validation loss: 2.458671380423002

Epoch: 127| Step: 0
Training loss: 2.8499431872142855
Validation loss: 2.4286414218028867

Epoch: 6| Step: 1
Training loss: 3.154578342221336
Validation loss: 2.4370287672919964

Epoch: 6| Step: 2
Training loss: 2.7236786814889817
Validation loss: 2.413837446663281

Epoch: 6| Step: 3
Training loss: 2.519044625063058
Validation loss: 2.4338394699649575

Epoch: 6| Step: 4
Training loss: 2.076342054481745
Validation loss: 2.4328671257482974

Epoch: 6| Step: 5
Training loss: 2.882448581789475
Validation loss: 2.448122775569544

Epoch: 6| Step: 6
Training loss: 2.7063995108590175
Validation loss: 2.4569673895584474

Epoch: 6| Step: 7
Training loss: 2.5831050977071723
Validation loss: 2.451743132846121

Epoch: 6| Step: 8
Training loss: 2.741484201344356
Validation loss: 2.465787776836746

Epoch: 6| Step: 9
Training loss: 2.914254271714306
Validation loss: 2.423680186662169

Epoch: 6| Step: 10
Training loss: 2.5966768411805106
Validation loss: 2.439845781961572

Epoch: 6| Step: 11
Training loss: 2.7865092240199196
Validation loss: 2.4356278770328927

Epoch: 6| Step: 12
Training loss: 2.4464241401703832
Validation loss: 2.4506539933504627

Epoch: 6| Step: 13
Training loss: 3.3136079482774807
Validation loss: 2.429984768375349

Epoch: 128| Step: 0
Training loss: 2.835844348396957
Validation loss: 2.416943843619513

Epoch: 6| Step: 1
Training loss: 2.674862268057212
Validation loss: 2.4350001829295973

Epoch: 6| Step: 2
Training loss: 3.0620163516175483
Validation loss: 2.4320940314782917

Epoch: 6| Step: 3
Training loss: 2.288473841258677
Validation loss: 2.4396174436335145

Epoch: 6| Step: 4
Training loss: 3.0526240957968045
Validation loss: 2.4438461315015907

Epoch: 6| Step: 5
Training loss: 2.0034448044030437
Validation loss: 2.424747470215022

Epoch: 6| Step: 6
Training loss: 3.031710560211584
Validation loss: 2.44422127380666

Epoch: 6| Step: 7
Training loss: 2.6945664409884427
Validation loss: 2.4515730975243626

Epoch: 6| Step: 8
Training loss: 3.344331209526603
Validation loss: 2.4556605779748284

Epoch: 6| Step: 9
Training loss: 2.470319225053603
Validation loss: 2.43915564106615

Epoch: 6| Step: 10
Training loss: 2.2909925220622216
Validation loss: 2.4573218971293107

Epoch: 6| Step: 11
Training loss: 2.1123185706244527
Validation loss: 2.4439562481999944

Epoch: 6| Step: 12
Training loss: 2.3159215204451735
Validation loss: 2.432081284888225

Epoch: 6| Step: 13
Training loss: 3.5586281867931047
Validation loss: 2.431818204278897

Epoch: 129| Step: 0
Training loss: 2.6885501672747574
Validation loss: 2.4658454973495614

Epoch: 6| Step: 1
Training loss: 3.0191023153055285
Validation loss: 2.4787952599539107

Epoch: 6| Step: 2
Training loss: 2.2622335371889055
Validation loss: 2.4548570788262185

Epoch: 6| Step: 3
Training loss: 2.515110034241938
Validation loss: 2.468443771981694

Epoch: 6| Step: 4
Training loss: 2.4236103362695816
Validation loss: 2.4298719353206306

Epoch: 6| Step: 5
Training loss: 2.7354029004074287
Validation loss: 2.441121552830896

Epoch: 6| Step: 6
Training loss: 2.9315022048954917
Validation loss: 2.4483143319598066

Epoch: 6| Step: 7
Training loss: 2.245923375841248
Validation loss: 2.4483904919115824

Epoch: 6| Step: 8
Training loss: 2.709599497292908
Validation loss: 2.4633223461087206

Epoch: 6| Step: 9
Training loss: 2.3968816923721965
Validation loss: 2.453117582217073

Epoch: 6| Step: 10
Training loss: 2.8133435150010455
Validation loss: 2.4210340190246717

Epoch: 6| Step: 11
Training loss: 3.1993040043204
Validation loss: 2.4582917105806428

Epoch: 6| Step: 12
Training loss: 2.9019211555144406
Validation loss: 2.4381057222235634

Epoch: 6| Step: 13
Training loss: 3.223555893602921
Validation loss: 2.4355744327926465

Epoch: 130| Step: 0
Training loss: 2.634134023195202
Validation loss: 2.4541139581088127

Epoch: 6| Step: 1
Training loss: 2.755597486698213
Validation loss: 2.438476038938202

Epoch: 6| Step: 2
Training loss: 2.8196212470916695
Validation loss: 2.465347871116612

Epoch: 6| Step: 3
Training loss: 2.6232859146828313
Validation loss: 2.448781453536124

Epoch: 6| Step: 4
Training loss: 2.395519457173338
Validation loss: 2.446324454329436

Epoch: 6| Step: 5
Training loss: 2.5721199150786003
Validation loss: 2.4504074648111356

Epoch: 6| Step: 6
Training loss: 3.0757150268401925
Validation loss: 2.4390482869273176

Epoch: 6| Step: 7
Training loss: 2.612470606588347
Validation loss: 2.4538542932713723

Epoch: 6| Step: 8
Training loss: 3.0358345520815697
Validation loss: 2.4398349225518805

Epoch: 6| Step: 9
Training loss: 2.623647341307805
Validation loss: 2.4192962307821415

Epoch: 6| Step: 10
Training loss: 2.529402635638753
Validation loss: 2.457156611616808

Epoch: 6| Step: 11
Training loss: 2.3664711853568527
Validation loss: 2.4422495309413628

Epoch: 6| Step: 12
Training loss: 3.0402471983941948
Validation loss: 2.454431218317633

Epoch: 6| Step: 13
Training loss: 2.572174881686461
Validation loss: 2.44061213248323

Epoch: 131| Step: 0
Training loss: 3.082574407293596
Validation loss: 2.437988305961175

Epoch: 6| Step: 1
Training loss: 2.3819693652728593
Validation loss: 2.4439691583835574

Epoch: 6| Step: 2
Training loss: 3.1133105038521287
Validation loss: 2.459974895975815

Epoch: 6| Step: 3
Training loss: 1.9513619361847254
Validation loss: 2.43723281035399

Epoch: 6| Step: 4
Training loss: 2.7477024190547272
Validation loss: 2.4659154904179896

Epoch: 6| Step: 5
Training loss: 2.3832617164291006
Validation loss: 2.446649306487383

Epoch: 6| Step: 6
Training loss: 2.263163103753659
Validation loss: 2.435545873457147

Epoch: 6| Step: 7
Training loss: 2.707774667095863
Validation loss: 2.4442669626758238

Epoch: 6| Step: 8
Training loss: 3.1819706248636423
Validation loss: 2.440329941683023

Epoch: 6| Step: 9
Training loss: 2.1428493999159657
Validation loss: 2.451293897302293

Epoch: 6| Step: 10
Training loss: 3.342473579707472
Validation loss: 2.449313287718817

Epoch: 6| Step: 11
Training loss: 2.8662433784897487
Validation loss: 2.4648649946318164

Epoch: 6| Step: 12
Training loss: 2.456303669195141
Validation loss: 2.4530616377420156

Epoch: 6| Step: 13
Training loss: 2.9391609528279354
Validation loss: 2.470144102268773

Epoch: 132| Step: 0
Training loss: 2.9460553514987184
Validation loss: 2.428629672043253

Epoch: 6| Step: 1
Training loss: 2.658323847196708
Validation loss: 2.4432905226509813

Epoch: 6| Step: 2
Training loss: 2.437572575980331
Validation loss: 2.466062147752886

Epoch: 6| Step: 3
Training loss: 2.3828467194648413
Validation loss: 2.437561588650418

Epoch: 6| Step: 4
Training loss: 2.439547241447809
Validation loss: 2.4197442511814384

Epoch: 6| Step: 5
Training loss: 3.2033462075865575
Validation loss: 2.443774606252318

Epoch: 6| Step: 6
Training loss: 3.2326723491513603
Validation loss: 2.437423536575373

Epoch: 6| Step: 7
Training loss: 2.378909857986796
Validation loss: 2.4371597435389996

Epoch: 6| Step: 8
Training loss: 2.6685475828786345
Validation loss: 2.467831650719139

Epoch: 6| Step: 9
Training loss: 2.672636760070799
Validation loss: 2.449004658086915

Epoch: 6| Step: 10
Training loss: 2.387011021576244
Validation loss: 2.4681180906165956

Epoch: 6| Step: 11
Training loss: 2.348435765999911
Validation loss: 2.4503979181268782

Epoch: 6| Step: 12
Training loss: 3.055719459838604
Validation loss: 2.4429039341029712

Epoch: 6| Step: 13
Training loss: 2.9440168774058826
Validation loss: 2.4338017045650813

Epoch: 133| Step: 0
Training loss: 2.196880934206679
Validation loss: 2.453925273994801

Epoch: 6| Step: 1
Training loss: 2.700547770283377
Validation loss: 2.4298347518473338

Epoch: 6| Step: 2
Training loss: 2.7182317656396
Validation loss: 2.4349943049740044

Epoch: 6| Step: 3
Training loss: 2.2454674950648523
Validation loss: 2.455748856739733

Epoch: 6| Step: 4
Training loss: 3.367958635506588
Validation loss: 2.452592765737305

Epoch: 6| Step: 5
Training loss: 3.026612814985745
Validation loss: 2.4724852452874284

Epoch: 6| Step: 6
Training loss: 2.4071816956708734
Validation loss: 2.4407183853301446

Epoch: 6| Step: 7
Training loss: 2.8942630453086764
Validation loss: 2.456160831761172

Epoch: 6| Step: 8
Training loss: 2.0374000758844266
Validation loss: 2.451457811284697

Epoch: 6| Step: 9
Training loss: 2.738502830867478
Validation loss: 2.461169866401913

Epoch: 6| Step: 10
Training loss: 3.1221642406988197
Validation loss: 2.4456356028682573

Epoch: 6| Step: 11
Training loss: 2.8428338220355887
Validation loss: 2.4291111080708547

Epoch: 6| Step: 12
Training loss: 2.5075240400080436
Validation loss: 2.4501605415943204

Epoch: 6| Step: 13
Training loss: 2.874648528798527
Validation loss: 2.4413937636189433

Epoch: 134| Step: 0
Training loss: 2.6783233818370986
Validation loss: 2.4530593814176767

Epoch: 6| Step: 1
Training loss: 1.9833020891672406
Validation loss: 2.4717691199938097

Epoch: 6| Step: 2
Training loss: 2.738680517933606
Validation loss: 2.4503855099972873

Epoch: 6| Step: 3
Training loss: 3.0181979730994746
Validation loss: 2.4333649524815106

Epoch: 6| Step: 4
Training loss: 2.2307170455808754
Validation loss: 2.444617977334335

Epoch: 6| Step: 5
Training loss: 2.595259629273141
Validation loss: 2.461105236269077

Epoch: 6| Step: 6
Training loss: 2.7259300394687545
Validation loss: 2.4213420174275235

Epoch: 6| Step: 7
Training loss: 2.6924956266603224
Validation loss: 2.441983309238296

Epoch: 6| Step: 8
Training loss: 2.2689735659744517
Validation loss: 2.4542115120600014

Epoch: 6| Step: 9
Training loss: 2.799327606120212
Validation loss: 2.4708437978345867

Epoch: 6| Step: 10
Training loss: 2.981555185751824
Validation loss: 2.4292815331260944

Epoch: 6| Step: 11
Training loss: 2.7581349473432852
Validation loss: 2.439465696979179

Epoch: 6| Step: 12
Training loss: 3.0943821733176233
Validation loss: 2.44696204624018

Epoch: 6| Step: 13
Training loss: 3.022255363313812
Validation loss: 2.4728957818685924

Epoch: 135| Step: 0
Training loss: 3.3570099778241556
Validation loss: 2.4285676151422178

Epoch: 6| Step: 1
Training loss: 2.1507379596093386
Validation loss: 2.472994272227073

Epoch: 6| Step: 2
Training loss: 3.0779674509073702
Validation loss: 2.442284403979539

Epoch: 6| Step: 3
Training loss: 2.2441202267349705
Validation loss: 2.4449157759960496

Epoch: 6| Step: 4
Training loss: 2.3758359742816157
Validation loss: 2.4615838002518

Epoch: 6| Step: 5
Training loss: 2.7783544662333965
Validation loss: 2.461009085832052

Epoch: 6| Step: 6
Training loss: 3.4196538814721267
Validation loss: 2.423970664681796

Epoch: 6| Step: 7
Training loss: 2.283426578965989
Validation loss: 2.4411695733313823

Epoch: 6| Step: 8
Training loss: 1.7647704360412004
Validation loss: 2.4635396929335847

Epoch: 6| Step: 9
Training loss: 2.591290698323006
Validation loss: 2.450594340073651

Epoch: 6| Step: 10
Training loss: 2.9257980366570338
Validation loss: 2.4375870339142804

Epoch: 6| Step: 11
Training loss: 2.1935874161424023
Validation loss: 2.4593985432579695

Epoch: 6| Step: 12
Training loss: 3.2733353846971514
Validation loss: 2.4375559135441818

Epoch: 6| Step: 13
Training loss: 3.1112472035145253
Validation loss: 2.425412623672868

Epoch: 136| Step: 0
Training loss: 2.651716065329297
Validation loss: 2.447476059414027

Epoch: 6| Step: 1
Training loss: 2.730563575506048
Validation loss: 2.4389821372128933

Epoch: 6| Step: 2
Training loss: 2.4976696597524755
Validation loss: 2.464273007342564

Epoch: 6| Step: 3
Training loss: 3.0278855361972488
Validation loss: 2.4482567384478546

Epoch: 6| Step: 4
Training loss: 2.5626741675757683
Validation loss: 2.470422419449561

Epoch: 6| Step: 5
Training loss: 2.8670238479351555
Validation loss: 2.4383635714735115

Epoch: 6| Step: 6
Training loss: 2.702421945861549
Validation loss: 2.450067281730461

Epoch: 6| Step: 7
Training loss: 2.843282975633767
Validation loss: 2.4332264536929773

Epoch: 6| Step: 8
Training loss: 3.0994839946384656
Validation loss: 2.4443957356416233

Epoch: 6| Step: 9
Training loss: 2.6907300833319674
Validation loss: 2.4349379662134605

Epoch: 6| Step: 10
Training loss: 2.788535615964093
Validation loss: 2.4478682143827792

Epoch: 6| Step: 11
Training loss: 2.329807876557357
Validation loss: 2.439646218537427

Epoch: 6| Step: 12
Training loss: 2.852174458094087
Validation loss: 2.4679827938194383

Epoch: 6| Step: 13
Training loss: 1.9231152758075085
Validation loss: 2.445489958692556

Epoch: 137| Step: 0
Training loss: 2.1809720318595702
Validation loss: 2.46646596212042

Epoch: 6| Step: 1
Training loss: 2.3438641329632364
Validation loss: 2.4363377316144033

Epoch: 6| Step: 2
Training loss: 2.4407300821460822
Validation loss: 2.448338629896136

Epoch: 6| Step: 3
Training loss: 2.3648712610283447
Validation loss: 2.432912638488238

Epoch: 6| Step: 4
Training loss: 2.8019982836743256
Validation loss: 2.4500839469730202

Epoch: 6| Step: 5
Training loss: 2.6973061969421
Validation loss: 2.4203899719761295

Epoch: 6| Step: 6
Training loss: 2.5224932625324263
Validation loss: 2.442361284138066

Epoch: 6| Step: 7
Training loss: 2.82550838250905
Validation loss: 2.4424620973750026

Epoch: 6| Step: 8
Training loss: 2.8370727284588497
Validation loss: 2.4484680923140245

Epoch: 6| Step: 9
Training loss: 2.7816055584917545
Validation loss: 2.468016611442852

Epoch: 6| Step: 10
Training loss: 3.1162101724086337
Validation loss: 2.438015452349868

Epoch: 6| Step: 11
Training loss: 2.8777356608224696
Validation loss: 2.432834427051151

Epoch: 6| Step: 12
Training loss: 2.699261765751445
Validation loss: 2.477804429285409

Epoch: 6| Step: 13
Training loss: 3.459349257072733
Validation loss: 2.4451537434723956

Epoch: 138| Step: 0
Training loss: 2.4188855825664537
Validation loss: 2.444946444083588

Epoch: 6| Step: 1
Training loss: 2.699988778408891
Validation loss: 2.4380267141848844

Epoch: 6| Step: 2
Training loss: 2.408907129203803
Validation loss: 2.444416669784585

Epoch: 6| Step: 3
Training loss: 3.1003187938557475
Validation loss: 2.44354992150713

Epoch: 6| Step: 4
Training loss: 2.8768636219529777
Validation loss: 2.474628320891397

Epoch: 6| Step: 5
Training loss: 2.429936141756638
Validation loss: 2.4317218225157498

Epoch: 6| Step: 6
Training loss: 3.026504262384564
Validation loss: 2.4403714435816033

Epoch: 6| Step: 7
Training loss: 2.8216387306568453
Validation loss: 2.427765960761123

Epoch: 6| Step: 8
Training loss: 2.5878624335138993
Validation loss: 2.448114141503261

Epoch: 6| Step: 9
Training loss: 2.706792559428563
Validation loss: 2.432318566058016

Epoch: 6| Step: 10
Training loss: 2.9155230505353793
Validation loss: 2.4360231858964614

Epoch: 6| Step: 11
Training loss: 2.9915281520502655
Validation loss: 2.4362715188257074

Epoch: 6| Step: 12
Training loss: 2.467936417964704
Validation loss: 2.4195630998086717

Epoch: 6| Step: 13
Training loss: 2.185357925115302
Validation loss: 2.4437353148348424

Epoch: 139| Step: 0
Training loss: 2.506526439004526
Validation loss: 2.4470071810569674

Epoch: 6| Step: 1
Training loss: 2.3993776945122356
Validation loss: 2.433826835216602

Epoch: 6| Step: 2
Training loss: 2.691758175722441
Validation loss: 2.4334479097516075

Epoch: 6| Step: 3
Training loss: 3.2001748990899896
Validation loss: 2.4401189821334826

Epoch: 6| Step: 4
Training loss: 2.8147062337393183
Validation loss: 2.4293674447722764

Epoch: 6| Step: 5
Training loss: 2.389185453335954
Validation loss: 2.4443541143348506

Epoch: 6| Step: 6
Training loss: 2.4336527270767907
Validation loss: 2.4306370787028175

Epoch: 6| Step: 7
Training loss: 2.8095324649323397
Validation loss: 2.4747292564953147

Epoch: 6| Step: 8
Training loss: 2.8937137082421533
Validation loss: 2.4483806148633858

Epoch: 6| Step: 9
Training loss: 2.602063746463876
Validation loss: 2.4376274351132996

Epoch: 6| Step: 10
Training loss: 3.4959077071658347
Validation loss: 2.426279367817541

Epoch: 6| Step: 11
Training loss: 2.4739729281881724
Validation loss: 2.452709103436383

Epoch: 6| Step: 12
Training loss: 2.5737747944123424
Validation loss: 2.4458185074899412

Epoch: 6| Step: 13
Training loss: 1.8762509305724213
Validation loss: 2.4463421563398366

Epoch: 140| Step: 0
Training loss: 2.625717065146428
Validation loss: 2.4253966133690987

Epoch: 6| Step: 1
Training loss: 3.104990873215879
Validation loss: 2.4724917111873284

Epoch: 6| Step: 2
Training loss: 2.630768115210286
Validation loss: 2.4695830802882957

Epoch: 6| Step: 3
Training loss: 2.9115938668047168
Validation loss: 2.431877428926887

Epoch: 6| Step: 4
Training loss: 2.038763148524431
Validation loss: 2.4725354815462723

Epoch: 6| Step: 5
Training loss: 2.953534779046971
Validation loss: 2.4391863773722395

Epoch: 6| Step: 6
Training loss: 2.8204718462873304
Validation loss: 2.4399583576212143

Epoch: 6| Step: 7
Training loss: 3.252390056044963
Validation loss: 2.473783709975236

Epoch: 6| Step: 8
Training loss: 2.3718007273347284
Validation loss: 2.4284405005827487

Epoch: 6| Step: 9
Training loss: 2.635008849503715
Validation loss: 2.429739428813568

Epoch: 6| Step: 10
Training loss: 2.5069453085923987
Validation loss: 2.4555106021339683

Epoch: 6| Step: 11
Training loss: 2.738508838114843
Validation loss: 2.420333286911294

Epoch: 6| Step: 12
Training loss: 2.7469084441456726
Validation loss: 2.4431863197268364

Epoch: 6| Step: 13
Training loss: 2.380450269115936
Validation loss: 2.4407388767786906

Epoch: 141| Step: 0
Training loss: 2.3084778732419244
Validation loss: 2.4706348468607358

Epoch: 6| Step: 1
Training loss: 3.186228629482432
Validation loss: 2.484616200368021

Epoch: 6| Step: 2
Training loss: 3.219517884739305
Validation loss: 2.4341554479558867

Epoch: 6| Step: 3
Training loss: 2.537173463453925
Validation loss: 2.4450108669953905

Epoch: 6| Step: 4
Training loss: 2.2019373079915776
Validation loss: 2.438544648756461

Epoch: 6| Step: 5
Training loss: 2.971038418218744
Validation loss: 2.455804679451766

Epoch: 6| Step: 6
Training loss: 2.1178964926883124
Validation loss: 2.4332742939118015

Epoch: 6| Step: 7
Training loss: 2.641568167822649
Validation loss: 2.447903733141857

Epoch: 6| Step: 8
Training loss: 2.35507699925593
Validation loss: 2.4194231582210755

Epoch: 6| Step: 9
Training loss: 2.857265841017016
Validation loss: 2.42369333653817

Epoch: 6| Step: 10
Training loss: 2.7184510340684245
Validation loss: 2.4564347552818155

Epoch: 6| Step: 11
Training loss: 3.113477444659087
Validation loss: 2.450107810917479

Epoch: 6| Step: 12
Training loss: 2.5613302026332696
Validation loss: 2.484861156456984

Epoch: 6| Step: 13
Training loss: 2.482385666625932
Validation loss: 2.4520235919532416

Epoch: 142| Step: 0
Training loss: 2.3205054617128904
Validation loss: 2.4332604340733557

Epoch: 6| Step: 1
Training loss: 2.661593996734774
Validation loss: 2.425979401932135

Epoch: 6| Step: 2
Training loss: 2.8179515139034406
Validation loss: 2.425623077942165

Epoch: 6| Step: 3
Training loss: 2.585247212732264
Validation loss: 2.4513834659138745

Epoch: 6| Step: 4
Training loss: 2.9026243502900066
Validation loss: 2.464670462018237

Epoch: 6| Step: 5
Training loss: 2.4689179375422983
Validation loss: 2.4520019903615364

Epoch: 6| Step: 6
Training loss: 2.8977036691068117
Validation loss: 2.4370251475177467

Epoch: 6| Step: 7
Training loss: 2.576244061133264
Validation loss: 2.4330425471424655

Epoch: 6| Step: 8
Training loss: 2.8301851135207454
Validation loss: 2.4536547804953233

Epoch: 6| Step: 9
Training loss: 2.9189810879896116
Validation loss: 2.4403475133154386

Epoch: 6| Step: 10
Training loss: 2.7379016041654594
Validation loss: 2.4175537915339715

Epoch: 6| Step: 11
Training loss: 2.909724629799983
Validation loss: 2.4674233895792597

Epoch: 6| Step: 12
Training loss: 2.503421349669187
Validation loss: 2.4668624430518444

Epoch: 6| Step: 13
Training loss: 2.8533640566030405
Validation loss: 2.451357998620848

Epoch: 143| Step: 0
Training loss: 2.1842083825129293
Validation loss: 2.4329575977040347

Epoch: 6| Step: 1
Training loss: 2.8898505745771335
Validation loss: 2.4473000326864844

Epoch: 6| Step: 2
Training loss: 2.5499570955145194
Validation loss: 2.421345943337908

Epoch: 6| Step: 3
Training loss: 2.718008883082096
Validation loss: 2.4547107577755223

Epoch: 6| Step: 4
Training loss: 2.652741931808464
Validation loss: 2.4221414039107985

Epoch: 6| Step: 5
Training loss: 2.879316324042304
Validation loss: 2.4528589720337757

Epoch: 6| Step: 6
Training loss: 2.8063680985032726
Validation loss: 2.44736528088536

Epoch: 6| Step: 7
Training loss: 3.078484838530476
Validation loss: 2.455068896170053

Epoch: 6| Step: 8
Training loss: 2.9328813970538548
Validation loss: 2.450234971154046

Epoch: 6| Step: 9
Training loss: 3.150809556383201
Validation loss: 2.4518547670112376

Epoch: 6| Step: 10
Training loss: 2.594349619484644
Validation loss: 2.4617675089928497

Epoch: 6| Step: 11
Training loss: 2.364815710359609
Validation loss: 2.453283228153

Epoch: 6| Step: 12
Training loss: 2.2068867053745387
Validation loss: 2.4225743496382113

Epoch: 6| Step: 13
Training loss: 2.680973734105552
Validation loss: 2.4584837165860915

Epoch: 144| Step: 0
Training loss: 2.6481335670921426
Validation loss: 2.4867652492247863

Epoch: 6| Step: 1
Training loss: 2.5173000658606495
Validation loss: 2.4447252504819086

Epoch: 6| Step: 2
Training loss: 2.826646787315617
Validation loss: 2.449443710182273

Epoch: 6| Step: 3
Training loss: 2.58465276883672
Validation loss: 2.416612745630443

Epoch: 6| Step: 4
Training loss: 2.5082796321286946
Validation loss: 2.4337605404228055

Epoch: 6| Step: 5
Training loss: 2.4139129691123355
Validation loss: 2.4616186253805674

Epoch: 6| Step: 6
Training loss: 2.892816650941493
Validation loss: 2.443262276478784

Epoch: 6| Step: 7
Training loss: 2.139654231792702
Validation loss: 2.4305120332007992

Epoch: 6| Step: 8
Training loss: 2.68491949347879
Validation loss: 2.434999648092025

Epoch: 6| Step: 9
Training loss: 2.811939946679678
Validation loss: 2.4607917530232646

Epoch: 6| Step: 10
Training loss: 3.6722380762981137
Validation loss: 2.450608198120624

Epoch: 6| Step: 11
Training loss: 2.595757316672363
Validation loss: 2.429411117818897

Epoch: 6| Step: 12
Training loss: 2.605561170411539
Validation loss: 2.455311913587113

Epoch: 6| Step: 13
Training loss: 2.6403162849138178
Validation loss: 2.457599734483242

Epoch: 145| Step: 0
Training loss: 2.584737170672306
Validation loss: 2.472426232282326

Epoch: 6| Step: 1
Training loss: 2.2438116803630366
Validation loss: 2.4583583512101246

Epoch: 6| Step: 2
Training loss: 2.5376303991627953
Validation loss: 2.466374980865983

Epoch: 6| Step: 3
Training loss: 2.466340253041825
Validation loss: 2.455624251659965

Epoch: 6| Step: 4
Training loss: 2.487316381683837
Validation loss: 2.462993049181674

Epoch: 6| Step: 5
Training loss: 2.988061834520247
Validation loss: 2.4776314983530465

Epoch: 6| Step: 6
Training loss: 3.1836081288018847
Validation loss: 2.4553564167168127

Epoch: 6| Step: 7
Training loss: 2.576970437202629
Validation loss: 2.4484791416875766

Epoch: 6| Step: 8
Training loss: 2.454557838633472
Validation loss: 2.4500173052037866

Epoch: 6| Step: 9
Training loss: 2.5403765755192236
Validation loss: 2.481003232433477

Epoch: 6| Step: 10
Training loss: 3.1770081724854835
Validation loss: 2.4640560633359883

Epoch: 6| Step: 11
Training loss: 2.9265621350273396
Validation loss: 2.4387910798179813

Epoch: 6| Step: 12
Training loss: 3.0528819804466147
Validation loss: 2.4649853989140555

Epoch: 6| Step: 13
Training loss: 1.6638621894733596
Validation loss: 2.441749608331594

Epoch: 146| Step: 0
Training loss: 2.564766974321129
Validation loss: 2.4159132073214935

Epoch: 6| Step: 1
Training loss: 2.010333308203095
Validation loss: 2.441929575757893

Epoch: 6| Step: 2
Training loss: 3.088391528198022
Validation loss: 2.447675020591525

Epoch: 6| Step: 3
Training loss: 3.2108081503846755
Validation loss: 2.4483016671996984

Epoch: 6| Step: 4
Training loss: 2.7130107592134896
Validation loss: 2.457228463570778

Epoch: 6| Step: 5
Training loss: 2.275544124550139
Validation loss: 2.4377201223883973

Epoch: 6| Step: 6
Training loss: 3.1232539067165694
Validation loss: 2.425625483445001

Epoch: 6| Step: 7
Training loss: 3.0337660267787325
Validation loss: 2.42104109672322

Epoch: 6| Step: 8
Training loss: 1.9564855123013232
Validation loss: 2.439960510484737

Epoch: 6| Step: 9
Training loss: 2.3959003162695414
Validation loss: 2.4110064151621557

Epoch: 6| Step: 10
Training loss: 3.2443552147194583
Validation loss: 2.4274546232595706

Epoch: 6| Step: 11
Training loss: 2.371197417002467
Validation loss: 2.440834653559344

Epoch: 6| Step: 12
Training loss: 2.586231042157287
Validation loss: 2.4220213635134322

Epoch: 6| Step: 13
Training loss: 2.632483487272142
Validation loss: 2.4284668553322946

Epoch: 147| Step: 0
Training loss: 3.107971181217942
Validation loss: 2.4347925076165304

Epoch: 6| Step: 1
Training loss: 1.7441765759085284
Validation loss: 2.4310702556434314

Epoch: 6| Step: 2
Training loss: 2.70361588271107
Validation loss: 2.456082510991901

Epoch: 6| Step: 3
Training loss: 2.1593124093508926
Validation loss: 2.443265744312086

Epoch: 6| Step: 4
Training loss: 2.6747838298406386
Validation loss: 2.4301163832627615

Epoch: 6| Step: 5
Training loss: 2.5835362836631317
Validation loss: 2.430932340603752

Epoch: 6| Step: 6
Training loss: 2.979562764189919
Validation loss: 2.4379536469778804

Epoch: 6| Step: 7
Training loss: 2.6626518743476932
Validation loss: 2.460531530117884

Epoch: 6| Step: 8
Training loss: 2.5185530781971885
Validation loss: 2.461860363118646

Epoch: 6| Step: 9
Training loss: 3.0487441369657615
Validation loss: 2.450124709216118

Epoch: 6| Step: 10
Training loss: 2.646988914227303
Validation loss: 2.4531986436357265

Epoch: 6| Step: 11
Training loss: 3.0889204456659805
Validation loss: 2.42132392088884

Epoch: 6| Step: 12
Training loss: 2.6659784223594776
Validation loss: 2.4162079273507424

Epoch: 6| Step: 13
Training loss: 2.9050661362491295
Validation loss: 2.4563525701613327

Epoch: 148| Step: 0
Training loss: 2.6434930879471144
Validation loss: 2.4315325774316032

Epoch: 6| Step: 1
Training loss: 2.506432649801451
Validation loss: 2.4518120008647615

Epoch: 6| Step: 2
Training loss: 2.711837487243743
Validation loss: 2.4718059599381417

Epoch: 6| Step: 3
Training loss: 2.879634935765513
Validation loss: 2.4472426667190783

Epoch: 6| Step: 4
Training loss: 2.4814380103690277
Validation loss: 2.4314655047007103

Epoch: 6| Step: 5
Training loss: 2.4077134079194917
Validation loss: 2.443542112176371

Epoch: 6| Step: 6
Training loss: 2.7163873688834568
Validation loss: 2.454877831338377

Epoch: 6| Step: 7
Training loss: 2.656582800268536
Validation loss: 2.438683407924918

Epoch: 6| Step: 8
Training loss: 2.8125907035612694
Validation loss: 2.4183156597271047

Epoch: 6| Step: 9
Training loss: 2.6097573554129663
Validation loss: 2.426516014028748

Epoch: 6| Step: 10
Training loss: 2.61604798974632
Validation loss: 2.44436990922439

Epoch: 6| Step: 11
Training loss: 2.8542411033409523
Validation loss: 2.4441567189009517

Epoch: 6| Step: 12
Training loss: 2.2325592132268217
Validation loss: 2.442894772619214

Epoch: 6| Step: 13
Training loss: 3.278901194999603
Validation loss: 2.4560477033949764

Epoch: 149| Step: 0
Training loss: 2.6964043658567656
Validation loss: 2.4581800600396684

Epoch: 6| Step: 1
Training loss: 2.5857167682771274
Validation loss: 2.4550641804521978

Epoch: 6| Step: 2
Training loss: 2.790460145530035
Validation loss: 2.4168416615127244

Epoch: 6| Step: 3
Training loss: 2.4102727778379
Validation loss: 2.470413729971014

Epoch: 6| Step: 4
Training loss: 2.4659318894929285
Validation loss: 2.426645855217141

Epoch: 6| Step: 5
Training loss: 2.8600634803929514
Validation loss: 2.471531407215259

Epoch: 6| Step: 6
Training loss: 2.488787589064737
Validation loss: 2.466242802555186

Epoch: 6| Step: 7
Training loss: 2.743670722667687
Validation loss: 2.4147867497617246

Epoch: 6| Step: 8
Training loss: 2.669137098482276
Validation loss: 2.4735667834095127

Epoch: 6| Step: 9
Training loss: 2.3431729432840998
Validation loss: 2.440044841561088

Epoch: 6| Step: 10
Training loss: 2.5650804134501946
Validation loss: 2.465416479727146

Epoch: 6| Step: 11
Training loss: 2.4963313841793267
Validation loss: 2.448559009176982

Epoch: 6| Step: 12
Training loss: 3.2701720198902153
Validation loss: 2.453474992905739

Epoch: 6| Step: 13
Training loss: 3.282394500222411
Validation loss: 2.450738146320422

Epoch: 150| Step: 0
Training loss: 2.2397999030386297
Validation loss: 2.4273885283037164

Epoch: 6| Step: 1
Training loss: 2.8027294411433052
Validation loss: 2.4952238214981644

Epoch: 6| Step: 2
Training loss: 2.8034009612639212
Validation loss: 2.452618225435194

Epoch: 6| Step: 3
Training loss: 3.0797576447292982
Validation loss: 2.4761720406932777

Epoch: 6| Step: 4
Training loss: 2.6548415096126
Validation loss: 2.445353057670712

Epoch: 6| Step: 5
Training loss: 2.555337525709724
Validation loss: 2.452569452796959

Epoch: 6| Step: 6
Training loss: 2.787382073751864
Validation loss: 2.464391312149486

Epoch: 6| Step: 7
Training loss: 3.150686970233542
Validation loss: 2.4498986882267166

Epoch: 6| Step: 8
Training loss: 2.3537850759914254
Validation loss: 2.459365344107748

Epoch: 6| Step: 9
Training loss: 2.6940967416307418
Validation loss: 2.461860999379032

Epoch: 6| Step: 10
Training loss: 2.477978034069128
Validation loss: 2.4325169980232038

Epoch: 6| Step: 11
Training loss: 2.617691765786045
Validation loss: 2.4599036208377223

Epoch: 6| Step: 12
Training loss: 2.8351467911327815
Validation loss: 2.450248669060483

Epoch: 6| Step: 13
Training loss: 2.2919091761037977
Validation loss: 2.465201984515059

Epoch: 151| Step: 0
Training loss: 3.341095200251857
Validation loss: 2.4456576191906585

Epoch: 6| Step: 1
Training loss: 1.9761510126656867
Validation loss: 2.450187613808661

Epoch: 6| Step: 2
Training loss: 2.4553065227888777
Validation loss: 2.4512803045912124

Epoch: 6| Step: 3
Training loss: 3.348372357224769
Validation loss: 2.4499183065653956

Epoch: 6| Step: 4
Training loss: 1.7056439172150686
Validation loss: 2.454364660640034

Epoch: 6| Step: 5
Training loss: 2.6747348939244615
Validation loss: 2.5022363361137465

Epoch: 6| Step: 6
Training loss: 2.7240457806430243
Validation loss: 2.419620257981577

Epoch: 6| Step: 7
Training loss: 2.0946318633897083
Validation loss: 2.4354536155419217

Epoch: 6| Step: 8
Training loss: 2.6729495915926953
Validation loss: 2.4704085366089106

Epoch: 6| Step: 9
Training loss: 2.821156383443454
Validation loss: 2.4552098012119603

Epoch: 6| Step: 10
Training loss: 2.8098608562188687
Validation loss: 2.4450362756271313

Epoch: 6| Step: 11
Training loss: 3.145547226448969
Validation loss: 2.4401277545417446

Epoch: 6| Step: 12
Training loss: 2.6216263663397004
Validation loss: 2.4577593758217375

Epoch: 6| Step: 13
Training loss: 2.4205626839265344
Validation loss: 2.4607930771444004

Epoch: 152| Step: 0
Training loss: 2.348399319214642
Validation loss: 2.4489105151548074

Epoch: 6| Step: 1
Training loss: 2.2127563759062516
Validation loss: 2.4872994670485165

Epoch: 6| Step: 2
Training loss: 2.3499679482080134
Validation loss: 2.4420659443478723

Epoch: 6| Step: 3
Training loss: 3.1384058461562914
Validation loss: 2.444371884102803

Epoch: 6| Step: 4
Training loss: 2.8516833972124274
Validation loss: 2.4525441723841053

Epoch: 6| Step: 5
Training loss: 2.4085448580273283
Validation loss: 2.4599354485393623

Epoch: 6| Step: 6
Training loss: 2.937289940648449
Validation loss: 2.4273036233796184

Epoch: 6| Step: 7
Training loss: 2.372428103805688
Validation loss: 2.45635880299722

Epoch: 6| Step: 8
Training loss: 2.9466993067157734
Validation loss: 2.4624528742019858

Epoch: 6| Step: 9
Training loss: 2.777824177884289
Validation loss: 2.4476213010387484

Epoch: 6| Step: 10
Training loss: 2.771455040118492
Validation loss: 2.4490323004983603

Epoch: 6| Step: 11
Training loss: 2.884588423382867
Validation loss: 2.4401884847238353

Epoch: 6| Step: 12
Training loss: 2.650076659911136
Validation loss: 2.4711166736774235

Epoch: 6| Step: 13
Training loss: 2.985409861785594
Validation loss: 2.44450597283212

Epoch: 153| Step: 0
Training loss: 3.0109316023574344
Validation loss: 2.4160977274853277

Epoch: 6| Step: 1
Training loss: 3.372921338862544
Validation loss: 2.4434558831675806

Epoch: 6| Step: 2
Training loss: 2.243533803570807
Validation loss: 2.44391930786231

Epoch: 6| Step: 3
Training loss: 2.089233993989334
Validation loss: 2.4424673491093665

Epoch: 6| Step: 4
Training loss: 2.390023548536925
Validation loss: 2.4527077472524637

Epoch: 6| Step: 5
Training loss: 2.2131599605786287
Validation loss: 2.4568870034261923

Epoch: 6| Step: 6
Training loss: 2.896964055969136
Validation loss: 2.441257424689361

Epoch: 6| Step: 7
Training loss: 2.6465448238887426
Validation loss: 2.43362152373983

Epoch: 6| Step: 8
Training loss: 2.6636955478353035
Validation loss: 2.456150421284156

Epoch: 6| Step: 9
Training loss: 2.9620808429593968
Validation loss: 2.452528435365368

Epoch: 6| Step: 10
Training loss: 3.0651260327661443
Validation loss: 2.4477809857058364

Epoch: 6| Step: 11
Training loss: 2.2619606639770815
Validation loss: 2.4253534028846055

Epoch: 6| Step: 12
Training loss: 2.7144439622452925
Validation loss: 2.4410305206930443

Epoch: 6| Step: 13
Training loss: 2.325000381469695
Validation loss: 2.4593462983569676

Epoch: 154| Step: 0
Training loss: 2.886545467724985
Validation loss: 2.450879267394934

Epoch: 6| Step: 1
Training loss: 3.414190713321198
Validation loss: 2.4685738530421837

Epoch: 6| Step: 2
Training loss: 2.342346890391245
Validation loss: 2.448344430789902

Epoch: 6| Step: 3
Training loss: 2.1134865708656614
Validation loss: 2.444761013093979

Epoch: 6| Step: 4
Training loss: 3.0085579877187523
Validation loss: 2.461383057975348

Epoch: 6| Step: 5
Training loss: 2.6694067546694713
Validation loss: 2.4573587439684923

Epoch: 6| Step: 6
Training loss: 2.141467742983368
Validation loss: 2.472740186559939

Epoch: 6| Step: 7
Training loss: 2.7788351483669045
Validation loss: 2.443086701147878

Epoch: 6| Step: 8
Training loss: 2.851215999924007
Validation loss: 2.4562718080021506

Epoch: 6| Step: 9
Training loss: 2.720979850371989
Validation loss: 2.4499750511082805

Epoch: 6| Step: 10
Training loss: 2.289173527130893
Validation loss: 2.4375471368759216

Epoch: 6| Step: 11
Training loss: 3.0186369880704556
Validation loss: 2.4629680162768395

Epoch: 6| Step: 12
Training loss: 2.4223652005163987
Validation loss: 2.4310062183628895

Epoch: 6| Step: 13
Training loss: 2.710040820894683
Validation loss: 2.4546068589783325

Epoch: 155| Step: 0
Training loss: 2.093340477819865
Validation loss: 2.466924752494523

Epoch: 6| Step: 1
Training loss: 2.4619471825048076
Validation loss: 2.4260602013922785

Epoch: 6| Step: 2
Training loss: 1.911161006355015
Validation loss: 2.4347259463972626

Epoch: 6| Step: 3
Training loss: 2.8035477471501244
Validation loss: 2.44963588382072

Epoch: 6| Step: 4
Training loss: 2.9213292229242045
Validation loss: 2.4603507784505294

Epoch: 6| Step: 5
Training loss: 2.373633945514726
Validation loss: 2.4701801610881913

Epoch: 6| Step: 6
Training loss: 2.7441195124021016
Validation loss: 2.462831199930765

Epoch: 6| Step: 7
Training loss: 2.625604196678076
Validation loss: 2.437523137338532

Epoch: 6| Step: 8
Training loss: 3.2662383603203584
Validation loss: 2.443608197200734

Epoch: 6| Step: 9
Training loss: 2.895013560525235
Validation loss: 2.4393570827354227

Epoch: 6| Step: 10
Training loss: 2.781772692994653
Validation loss: 2.4450369529633478

Epoch: 6| Step: 11
Training loss: 2.532944100577506
Validation loss: 2.4743710595991226

Epoch: 6| Step: 12
Training loss: 2.8274381024696402
Validation loss: 2.44150077521984

Epoch: 6| Step: 13
Training loss: 2.9208982742498284
Validation loss: 2.445113371221148

Epoch: 156| Step: 0
Training loss: 3.195961585490761
Validation loss: 2.458132085041583

Epoch: 6| Step: 1
Training loss: 2.5622346321960454
Validation loss: 2.4684865231233193

Epoch: 6| Step: 2
Training loss: 1.9583383790079683
Validation loss: 2.4580709575657713

Epoch: 6| Step: 3
Training loss: 3.3568944955109474
Validation loss: 2.434757603083202

Epoch: 6| Step: 4
Training loss: 2.783384468461675
Validation loss: 2.470642625550409

Epoch: 6| Step: 5
Training loss: 2.4350417040752994
Validation loss: 2.477855600083163

Epoch: 6| Step: 6
Training loss: 2.2876017730252447
Validation loss: 2.4508148575269377

Epoch: 6| Step: 7
Training loss: 2.5527255956357253
Validation loss: 2.472991334346481

Epoch: 6| Step: 8
Training loss: 2.5528512123881315
Validation loss: 2.456366578364761

Epoch: 6| Step: 9
Training loss: 2.9100325385303125
Validation loss: 2.436498077929392

Epoch: 6| Step: 10
Training loss: 2.607038171088543
Validation loss: 2.437573786508242

Epoch: 6| Step: 11
Training loss: 3.3679552375736534
Validation loss: 2.4382131189148137

Epoch: 6| Step: 12
Training loss: 1.857060827550102
Validation loss: 2.477626594837329

Epoch: 6| Step: 13
Training loss: 2.1496626256564744
Validation loss: 2.4239950532837353

Epoch: 157| Step: 0
Training loss: 2.44481217503462
Validation loss: 2.449843204981641

Epoch: 6| Step: 1
Training loss: 2.526905239923479
Validation loss: 2.4298559755004083

Epoch: 6| Step: 2
Training loss: 3.384585297057328
Validation loss: 2.438345431989752

Epoch: 6| Step: 3
Training loss: 2.228977219086545
Validation loss: 2.445349070183066

Epoch: 6| Step: 4
Training loss: 2.888821095706352
Validation loss: 2.432042093909878

Epoch: 6| Step: 5
Training loss: 3.024818755465849
Validation loss: 2.4708847986240943

Epoch: 6| Step: 6
Training loss: 2.2630166659885633
Validation loss: 2.463864824535509

Epoch: 6| Step: 7
Training loss: 3.0438472473776756
Validation loss: 2.4556038687665054

Epoch: 6| Step: 8
Training loss: 1.893382559906465
Validation loss: 2.458917744038647

Epoch: 6| Step: 9
Training loss: 2.291139044856023
Validation loss: 2.4716183671293908

Epoch: 6| Step: 10
Training loss: 2.839755596067723
Validation loss: 2.4471498267907963

Epoch: 6| Step: 11
Training loss: 2.5792291299859853
Validation loss: 2.4199096909268367

Epoch: 6| Step: 12
Training loss: 3.028363298580353
Validation loss: 2.4888062817856857

Epoch: 6| Step: 13
Training loss: 2.7242105827942082
Validation loss: 2.4671301230772227

Epoch: 158| Step: 0
Training loss: 2.6910885649658107
Validation loss: 2.4765139214934297

Epoch: 6| Step: 1
Training loss: 2.904820244327602
Validation loss: 2.4334066795738534

Epoch: 6| Step: 2
Training loss: 2.8768157201042555
Validation loss: 2.425642494699805

Epoch: 6| Step: 3
Training loss: 2.9083011731079327
Validation loss: 2.4615186847186084

Epoch: 6| Step: 4
Training loss: 3.2247587246796403
Validation loss: 2.462258536772127

Epoch: 6| Step: 5
Training loss: 2.08751101919224
Validation loss: 2.4331178464673444

Epoch: 6| Step: 6
Training loss: 2.7568883455100925
Validation loss: 2.438103799049958

Epoch: 6| Step: 7
Training loss: 3.006332548598247
Validation loss: 2.451925555729541

Epoch: 6| Step: 8
Training loss: 2.98038827529995
Validation loss: 2.4575248321746415

Epoch: 6| Step: 9
Training loss: 2.2066222225373346
Validation loss: 2.435137986022003

Epoch: 6| Step: 10
Training loss: 2.662158990790846
Validation loss: 2.440007485304765

Epoch: 6| Step: 11
Training loss: 2.313595692637696
Validation loss: 2.4187074406578537

Epoch: 6| Step: 12
Training loss: 2.0610401882823255
Validation loss: 2.462561285958337

Epoch: 6| Step: 13
Training loss: 2.257521666700579
Validation loss: 2.4536757416913124

Epoch: 159| Step: 0
Training loss: 3.2024187007116933
Validation loss: 2.4250458119417164

Epoch: 6| Step: 1
Training loss: 2.1108363987834773
Validation loss: 2.4457589453888096

Epoch: 6| Step: 2
Training loss: 2.4415841732042147
Validation loss: 2.445221579851173

Epoch: 6| Step: 3
Training loss: 2.6150321625870583
Validation loss: 2.4668308335904547

Epoch: 6| Step: 4
Training loss: 2.9681190221729916
Validation loss: 2.4639294769655504

Epoch: 6| Step: 5
Training loss: 2.7784157359491792
Validation loss: 2.4448806206756175

Epoch: 6| Step: 6
Training loss: 2.3203329477147743
Validation loss: 2.4510238455685105

Epoch: 6| Step: 7
Training loss: 2.28337113518528
Validation loss: 2.434252519014425

Epoch: 6| Step: 8
Training loss: 2.2625080635390775
Validation loss: 2.434613055082967

Epoch: 6| Step: 9
Training loss: 2.7620207150709346
Validation loss: 2.4536354939889855

Epoch: 6| Step: 10
Training loss: 2.7754045414961617
Validation loss: 2.4404363328742398

Epoch: 6| Step: 11
Training loss: 2.9913183797631584
Validation loss: 2.4647962562592034

Epoch: 6| Step: 12
Training loss: 2.9509511545728446
Validation loss: 2.4415202048338074

Epoch: 6| Step: 13
Training loss: 2.8218442186470045
Validation loss: 2.45145493752992

Epoch: 160| Step: 0
Training loss: 2.8218998127434682
Validation loss: 2.4375809897153276

Epoch: 6| Step: 1
Training loss: 2.1503452777845204
Validation loss: 2.44431859224127

Epoch: 6| Step: 2
Training loss: 2.7240660860668866
Validation loss: 2.4496392505358764

Epoch: 6| Step: 3
Training loss: 3.054319237562414
Validation loss: 2.4376079839362887

Epoch: 6| Step: 4
Training loss: 2.6082168110260873
Validation loss: 2.4580556032346856

Epoch: 6| Step: 5
Training loss: 3.073882766841024
Validation loss: 2.4398657482812873

Epoch: 6| Step: 6
Training loss: 2.527147335574379
Validation loss: 2.4481640689187247

Epoch: 6| Step: 7
Training loss: 2.5358056410791057
Validation loss: 2.437562291201568

Epoch: 6| Step: 8
Training loss: 3.0185750653934447
Validation loss: 2.44553060986348

Epoch: 6| Step: 9
Training loss: 2.6308500407279194
Validation loss: 2.4307955367240752

Epoch: 6| Step: 10
Training loss: 2.9246794761387056
Validation loss: 2.4245046216396093

Epoch: 6| Step: 11
Training loss: 2.8442177859374222
Validation loss: 2.4482964656763095

Epoch: 6| Step: 12
Training loss: 1.8217306874930514
Validation loss: 2.447777448861351

Epoch: 6| Step: 13
Training loss: 2.555297498758104
Validation loss: 2.4598634041656804

Epoch: 161| Step: 0
Training loss: 2.450159503649253
Validation loss: 2.4436633845377727

Epoch: 6| Step: 1
Training loss: 3.0316199637309733
Validation loss: 2.4508825278013733

Epoch: 6| Step: 2
Training loss: 3.017475094317138
Validation loss: 2.4850063923749226

Epoch: 6| Step: 3
Training loss: 2.717355655602189
Validation loss: 2.4801018153225547

Epoch: 6| Step: 4
Training loss: 2.7395458049645347
Validation loss: 2.4300019954543397

Epoch: 6| Step: 5
Training loss: 2.5784636304005413
Validation loss: 2.486567351259233

Epoch: 6| Step: 6
Training loss: 2.329869173822098
Validation loss: 2.461921869264527

Epoch: 6| Step: 7
Training loss: 2.223233488243984
Validation loss: 2.470992822802811

Epoch: 6| Step: 8
Training loss: 2.0907329628384907
Validation loss: 2.4251774253083465

Epoch: 6| Step: 9
Training loss: 3.027186865894625
Validation loss: 2.452347852250193

Epoch: 6| Step: 10
Training loss: 1.7765933286597224
Validation loss: 2.4509093786577005

Epoch: 6| Step: 11
Training loss: 3.30415762041801
Validation loss: 2.4768043017790395

Epoch: 6| Step: 12
Training loss: 2.8423535617761564
Validation loss: 2.449558055630264

Epoch: 6| Step: 13
Training loss: 2.9357648554090434
Validation loss: 2.426214952606006

Epoch: 162| Step: 0
Training loss: 2.9168664772983215
Validation loss: 2.447078704013571

Epoch: 6| Step: 1
Training loss: 2.771498311110575
Validation loss: 2.435636138549938

Epoch: 6| Step: 2
Training loss: 1.7736414317373295
Validation loss: 2.4648746828969506

Epoch: 6| Step: 3
Training loss: 2.391373859872836
Validation loss: 2.453649296201102

Epoch: 6| Step: 4
Training loss: 2.484249663640188
Validation loss: 2.478859239604226

Epoch: 6| Step: 5
Training loss: 2.156024644657751
Validation loss: 2.4366741038072415

Epoch: 6| Step: 6
Training loss: 3.3487725015975713
Validation loss: 2.4279987264309093

Epoch: 6| Step: 7
Training loss: 2.902029603203781
Validation loss: 2.453711250564273

Epoch: 6| Step: 8
Training loss: 2.464121185165097
Validation loss: 2.451439601368956

Epoch: 6| Step: 9
Training loss: 2.2755957777016667
Validation loss: 2.4444711409257946

Epoch: 6| Step: 10
Training loss: 2.5021049221186313
Validation loss: 2.447949709349626

Epoch: 6| Step: 11
Training loss: 3.493366494737238
Validation loss: 2.4360515528395137

Epoch: 6| Step: 12
Training loss: 2.7782136437398854
Validation loss: 2.45678326449087

Epoch: 6| Step: 13
Training loss: 2.6447601444689157
Validation loss: 2.4541512155933494

Epoch: 163| Step: 0
Training loss: 3.0676414771747544
Validation loss: 2.4550969083498555

Epoch: 6| Step: 1
Training loss: 3.3123140102929858
Validation loss: 2.4844963109638667

Epoch: 6| Step: 2
Training loss: 2.52420419672859
Validation loss: 2.450252987568042

Epoch: 6| Step: 3
Training loss: 2.8924731146008886
Validation loss: 2.459109901325188

Epoch: 6| Step: 4
Training loss: 2.751423034079525
Validation loss: 2.458079229173882

Epoch: 6| Step: 5
Training loss: 2.5917063555688737
Validation loss: 2.4653384711962296

Epoch: 6| Step: 6
Training loss: 2.2336934790907894
Validation loss: 2.4525374584229356

Epoch: 6| Step: 7
Training loss: 2.578823295583424
Validation loss: 2.4737984744346315

Epoch: 6| Step: 8
Training loss: 2.463675389817336
Validation loss: 2.4592308886475145

Epoch: 6| Step: 9
Training loss: 2.767786765192666
Validation loss: 2.45700509573761

Epoch: 6| Step: 10
Training loss: 2.4453413599035776
Validation loss: 2.4757869654290077

Epoch: 6| Step: 11
Training loss: 2.636145861519113
Validation loss: 2.4464283543504868

Epoch: 6| Step: 12
Training loss: 2.011653802804764
Validation loss: 2.4425927486911996

Epoch: 6| Step: 13
Training loss: 2.802635270868711
Validation loss: 2.4594232403664433

Epoch: 164| Step: 0
Training loss: 2.2887770959822133
Validation loss: 2.464081809308526

Epoch: 6| Step: 1
Training loss: 2.918537212187527
Validation loss: 2.4793546089567133

Epoch: 6| Step: 2
Training loss: 2.8859089088366146
Validation loss: 2.4435628632133897

Epoch: 6| Step: 3
Training loss: 3.4317531751783488
Validation loss: 2.405254704194017

Epoch: 6| Step: 4
Training loss: 2.449418882622739
Validation loss: 2.4741542031815213

Epoch: 6| Step: 5
Training loss: 2.8703888112048923
Validation loss: 2.436696296840448

Epoch: 6| Step: 6
Training loss: 2.568980883689429
Validation loss: 2.464047927278061

Epoch: 6| Step: 7
Training loss: 2.8428542853927765
Validation loss: 2.4495240825976343

Epoch: 6| Step: 8
Training loss: 2.9180854253154545
Validation loss: 2.4635823142898956

Epoch: 6| Step: 9
Training loss: 1.456548861505428
Validation loss: 2.472546493353764

Epoch: 6| Step: 10
Training loss: 2.7864704642819076
Validation loss: 2.44535422660565

Epoch: 6| Step: 11
Training loss: 2.641897673589669
Validation loss: 2.433570230824951

Epoch: 6| Step: 12
Training loss: 2.6746403176038918
Validation loss: 2.449178750402892

Epoch: 6| Step: 13
Training loss: 2.451649993832876
Validation loss: 2.4419620797048744

Epoch: 165| Step: 0
Training loss: 2.7968392822712773
Validation loss: 2.4368351475819656

Epoch: 6| Step: 1
Training loss: 2.2091888953661103
Validation loss: 2.448300876631682

Epoch: 6| Step: 2
Training loss: 2.5805861314436527
Validation loss: 2.46702345109718

Epoch: 6| Step: 3
Training loss: 2.3601037725166067
Validation loss: 2.4660039265491114

Epoch: 6| Step: 4
Training loss: 2.736260510629653
Validation loss: 2.490015783622734

Epoch: 6| Step: 5
Training loss: 2.489686673838039
Validation loss: 2.464489344521003

Epoch: 6| Step: 6
Training loss: 2.9743346039172476
Validation loss: 2.45849622878843

Epoch: 6| Step: 7
Training loss: 2.788597944449607
Validation loss: 2.461602825566373

Epoch: 6| Step: 8
Training loss: 2.752163816012051
Validation loss: 2.461141491059729

Epoch: 6| Step: 9
Training loss: 2.736224263062777
Validation loss: 2.480111701453043

Epoch: 6| Step: 10
Training loss: 2.642550156044247
Validation loss: 2.4844472483405298

Epoch: 6| Step: 11
Training loss: 2.647585571462296
Validation loss: 2.4286772798051044

Epoch: 6| Step: 12
Training loss: 2.7159535726138158
Validation loss: 2.473733499583426

Epoch: 6| Step: 13
Training loss: 3.236470451287276
Validation loss: 2.472988224377337

Epoch: 166| Step: 0
Training loss: 2.4290300565315053
Validation loss: 2.4699821299875935

Epoch: 6| Step: 1
Training loss: 2.9023315922314334
Validation loss: 2.4648547519672124

Epoch: 6| Step: 2
Training loss: 2.427772675117738
Validation loss: 2.4486266768818785

Epoch: 6| Step: 3
Training loss: 2.4004170452802516
Validation loss: 2.475684828244502

Epoch: 6| Step: 4
Training loss: 2.6535246387442593
Validation loss: 2.4748812545376855

Epoch: 6| Step: 5
Training loss: 2.5523119973226716
Validation loss: 2.4727761816037717

Epoch: 6| Step: 6
Training loss: 3.0459515076833057
Validation loss: 2.4348863419988342

Epoch: 6| Step: 7
Training loss: 2.9668698430584066
Validation loss: 2.4556869704865645

Epoch: 6| Step: 8
Training loss: 2.707847571214655
Validation loss: 2.4381017949100765

Epoch: 6| Step: 9
Training loss: 1.9241119020559332
Validation loss: 2.4595478312197177

Epoch: 6| Step: 10
Training loss: 2.882295060692873
Validation loss: 2.4539346209986914

Epoch: 6| Step: 11
Training loss: 3.1012251740168844
Validation loss: 2.4887642752429686

Epoch: 6| Step: 12
Training loss: 2.710112872289288
Validation loss: 2.4449101777283473

Epoch: 6| Step: 13
Training loss: 1.6621634522728301
Validation loss: 2.4488383767432835

Epoch: 167| Step: 0
Training loss: 2.3432942265177137
Validation loss: 2.441020099250087

Epoch: 6| Step: 1
Training loss: 2.7800070739216993
Validation loss: 2.434768923157268

Epoch: 6| Step: 2
Training loss: 3.0518974968032024
Validation loss: 2.4395421394813095

Epoch: 6| Step: 3
Training loss: 1.902231452024257
Validation loss: 2.4651089297085544

Epoch: 6| Step: 4
Training loss: 2.80660613588978
Validation loss: 2.45518461115865

Epoch: 6| Step: 5
Training loss: 2.2776766447033583
Validation loss: 2.4317848763392167

Epoch: 6| Step: 6
Training loss: 2.8997664785087114
Validation loss: 2.4558733740745904

Epoch: 6| Step: 7
Training loss: 3.0053688328205346
Validation loss: 2.4739552176714428

Epoch: 6| Step: 8
Training loss: 2.348083152611327
Validation loss: 2.4076725686442795

Epoch: 6| Step: 9
Training loss: 2.8863145186464547
Validation loss: 2.4506567200208202

Epoch: 6| Step: 10
Training loss: 2.596935843479096
Validation loss: 2.4480801976514432

Epoch: 6| Step: 11
Training loss: 2.8597738686109158
Validation loss: 2.4489607958647506

Epoch: 6| Step: 12
Training loss: 2.625074294719182
Validation loss: 2.436799697886782

Epoch: 6| Step: 13
Training loss: 2.7066098720561333
Validation loss: 2.442702105897922

Epoch: 168| Step: 0
Training loss: 2.135073626497214
Validation loss: 2.482674303774364

Epoch: 6| Step: 1
Training loss: 2.8057966244136088
Validation loss: 2.442432060407283

Epoch: 6| Step: 2
Training loss: 2.4127366458869908
Validation loss: 2.4400220843091858

Epoch: 6| Step: 3
Training loss: 2.6500461610336394
Validation loss: 2.4464175529778904

Epoch: 6| Step: 4
Training loss: 2.142640438929759
Validation loss: 2.43542441109648

Epoch: 6| Step: 5
Training loss: 2.8343018578089385
Validation loss: 2.450636495617902

Epoch: 6| Step: 6
Training loss: 2.8616875640811146
Validation loss: 2.425033084333737

Epoch: 6| Step: 7
Training loss: 2.1880191731842022
Validation loss: 2.4496400636957905

Epoch: 6| Step: 8
Training loss: 2.6954507543515835
Validation loss: 2.440309451002777

Epoch: 6| Step: 9
Training loss: 2.9648772025451366
Validation loss: 2.430225984893869

Epoch: 6| Step: 10
Training loss: 2.913764999789623
Validation loss: 2.4462575319854465

Epoch: 6| Step: 11
Training loss: 2.8598037148189874
Validation loss: 2.4446951616228807

Epoch: 6| Step: 12
Training loss: 2.4003387768378257
Validation loss: 2.447720702598162

Epoch: 6| Step: 13
Training loss: 3.445782480168295
Validation loss: 2.4582351976443237

Epoch: 169| Step: 0
Training loss: 2.263067867608234
Validation loss: 2.468922007933968

Epoch: 6| Step: 1
Training loss: 2.9755360993836644
Validation loss: 2.460079385063649

Epoch: 6| Step: 2
Training loss: 2.8421331147731
Validation loss: 2.472322747191323

Epoch: 6| Step: 3
Training loss: 2.073256102203953
Validation loss: 2.475512654358408

Epoch: 6| Step: 4
Training loss: 3.3530219041772718
Validation loss: 2.4706085704495857

Epoch: 6| Step: 5
Training loss: 2.3853222246645243
Validation loss: 2.449091563169121

Epoch: 6| Step: 6
Training loss: 2.6697931042351177
Validation loss: 2.4359569245661468

Epoch: 6| Step: 7
Training loss: 2.8270310451663727
Validation loss: 2.466588391891538

Epoch: 6| Step: 8
Training loss: 2.6134208334008537
Validation loss: 2.433193729796768

Epoch: 6| Step: 9
Training loss: 3.0705390827155883
Validation loss: 2.488459943550664

Epoch: 6| Step: 10
Training loss: 2.9555434562161373
Validation loss: 2.456129212958103

Epoch: 6| Step: 11
Training loss: 2.597295979921833
Validation loss: 2.429104577356996

Epoch: 6| Step: 12
Training loss: 2.3194420206002713
Validation loss: 2.468281026512559

Epoch: 6| Step: 13
Training loss: 1.982678626961838
Validation loss: 2.454306298261269

Epoch: 170| Step: 0
Training loss: 2.3226234923874634
Validation loss: 2.4529583735061045

Epoch: 6| Step: 1
Training loss: 2.2456549970585886
Validation loss: 2.444987325530841

Epoch: 6| Step: 2
Training loss: 3.205075298263912
Validation loss: 2.459305256352679

Epoch: 6| Step: 3
Training loss: 2.679101373836975
Validation loss: 2.4811891553027112

Epoch: 6| Step: 4
Training loss: 2.6222318858796556
Validation loss: 2.447169328883959

Epoch: 6| Step: 5
Training loss: 1.8651028411982675
Validation loss: 2.4322724461589305

Epoch: 6| Step: 6
Training loss: 2.985695431976345
Validation loss: 2.4360878784614313

Epoch: 6| Step: 7
Training loss: 2.92393135698896
Validation loss: 2.445236368418365

Epoch: 6| Step: 8
Training loss: 2.1248483042707096
Validation loss: 2.460426076631203

Epoch: 6| Step: 9
Training loss: 2.9693604494194807
Validation loss: 2.46423374938968

Epoch: 6| Step: 10
Training loss: 2.9904882322027886
Validation loss: 2.440244413357922

Epoch: 6| Step: 11
Training loss: 2.659187039502782
Validation loss: 2.485021396552358

Epoch: 6| Step: 12
Training loss: 2.9698957942573605
Validation loss: 2.4338471013422582

Epoch: 6| Step: 13
Training loss: 1.952441164466142
Validation loss: 2.4838006724368284

Epoch: 171| Step: 0
Training loss: 2.9047867567680283
Validation loss: 2.474791495285996

Epoch: 6| Step: 1
Training loss: 2.4075560555116695
Validation loss: 2.474989617016126

Epoch: 6| Step: 2
Training loss: 3.0244894243113163
Validation loss: 2.4804708981926566

Epoch: 6| Step: 3
Training loss: 2.3325781508393133
Validation loss: 2.470102761218313

Epoch: 6| Step: 4
Training loss: 2.393337232095984
Validation loss: 2.4573815148907396

Epoch: 6| Step: 5
Training loss: 2.7173458287953496
Validation loss: 2.4226963287290277

Epoch: 6| Step: 6
Training loss: 2.468810744685082
Validation loss: 2.457142048164203

Epoch: 6| Step: 7
Training loss: 2.5740346191496832
Validation loss: 2.44541890295744

Epoch: 6| Step: 8
Training loss: 2.7955771050720157
Validation loss: 2.459843655662035

Epoch: 6| Step: 9
Training loss: 2.054271815499489
Validation loss: 2.468320121425949

Epoch: 6| Step: 10
Training loss: 2.939474700138844
Validation loss: 2.4417905716489203

Epoch: 6| Step: 11
Training loss: 3.193904650502877
Validation loss: 2.4500131709665816

Epoch: 6| Step: 12
Training loss: 2.6836648234765876
Validation loss: 2.462851159727437

Epoch: 6| Step: 13
Training loss: 2.3542307167004797
Validation loss: 2.4414762995032926

Epoch: 172| Step: 0
Training loss: 2.47180460023036
Validation loss: 2.4615569741834364

Epoch: 6| Step: 1
Training loss: 2.6744593564160017
Validation loss: 2.467313443792772

Epoch: 6| Step: 2
Training loss: 2.987291438911043
Validation loss: 2.474458270937288

Epoch: 6| Step: 3
Training loss: 2.8994224499394514
Validation loss: 2.4658477929175096

Epoch: 6| Step: 4
Training loss: 3.125658042289121
Validation loss: 2.4495729927938585

Epoch: 6| Step: 5
Training loss: 2.2993580378378295
Validation loss: 2.4612835832325963

Epoch: 6| Step: 6
Training loss: 2.81309443655745
Validation loss: 2.460145938711928

Epoch: 6| Step: 7
Training loss: 2.5633526755778435
Validation loss: 2.439857990465175

Epoch: 6| Step: 8
Training loss: 2.3112793871350794
Validation loss: 2.4811581080778606

Epoch: 6| Step: 9
Training loss: 3.3307162660652647
Validation loss: 2.444671934339209

Epoch: 6| Step: 10
Training loss: 1.9891031960806365
Validation loss: 2.476218934219872

Epoch: 6| Step: 11
Training loss: 2.440069262819139
Validation loss: 2.468854934915026

Epoch: 6| Step: 12
Training loss: 2.6432259294962988
Validation loss: 2.4614787188783445

Epoch: 6| Step: 13
Training loss: 2.4936868109859747
Validation loss: 2.448469572826176

Epoch: 173| Step: 0
Training loss: 3.1592062523033038
Validation loss: 2.4581628103818525

Epoch: 6| Step: 1
Training loss: 2.5788132182446843
Validation loss: 2.445743983358953

Epoch: 6| Step: 2
Training loss: 2.724955000199598
Validation loss: 2.4377058776793574

Epoch: 6| Step: 3
Training loss: 3.1194001951520502
Validation loss: 2.448971910011988

Epoch: 6| Step: 4
Training loss: 2.46491319948465
Validation loss: 2.46045123855

Epoch: 6| Step: 5
Training loss: 2.6780342362646277
Validation loss: 2.436721227201206

Epoch: 6| Step: 6
Training loss: 2.744293360684953
Validation loss: 2.4654502306486914

Epoch: 6| Step: 7
Training loss: 1.86060619463914
Validation loss: 2.463799319781264

Epoch: 6| Step: 8
Training loss: 2.931115537377712
Validation loss: 2.4386544072048975

Epoch: 6| Step: 9
Training loss: 2.4342177373309926
Validation loss: 2.4365494601938313

Epoch: 6| Step: 10
Training loss: 2.5693168218953484
Validation loss: 2.4559157323303924

Epoch: 6| Step: 11
Training loss: 2.6113432810307184
Validation loss: 2.43500188430188

Epoch: 6| Step: 12
Training loss: 2.33444697053363
Validation loss: 2.460723818714768

Epoch: 6| Step: 13
Training loss: 3.0818203659336314
Validation loss: 2.454798846164843

Epoch: 174| Step: 0
Training loss: 2.144186245764266
Validation loss: 2.440409484442712

Epoch: 6| Step: 1
Training loss: 2.5327298583303985
Validation loss: 2.4575630185232695

Epoch: 6| Step: 2
Training loss: 2.9347895027853665
Validation loss: 2.409080225950153

Epoch: 6| Step: 3
Training loss: 2.906114800702698
Validation loss: 2.4619361092307663

Epoch: 6| Step: 4
Training loss: 2.1403785062446072
Validation loss: 2.4515736088777222

Epoch: 6| Step: 5
Training loss: 2.5696163452198646
Validation loss: 2.4555971663029887

Epoch: 6| Step: 6
Training loss: 2.639947206084911
Validation loss: 2.4525101278224155

Epoch: 6| Step: 7
Training loss: 2.9315906904162548
Validation loss: 2.4421699462882893

Epoch: 6| Step: 8
Training loss: 2.7404417909740024
Validation loss: 2.4512651796439826

Epoch: 6| Step: 9
Training loss: 2.6710555398266016
Validation loss: 2.4089777975553086

Epoch: 6| Step: 10
Training loss: 2.713628131760446
Validation loss: 2.4466742836966446

Epoch: 6| Step: 11
Training loss: 2.715831022711262
Validation loss: 2.452259700908026

Epoch: 6| Step: 12
Training loss: 2.682386012717395
Validation loss: 2.4444616434216893

Epoch: 6| Step: 13
Training loss: 2.5664183269431406
Validation loss: 2.4407842001973523

Epoch: 175| Step: 0
Training loss: 2.8943829826651823
Validation loss: 2.482916513433383

Epoch: 6| Step: 1
Training loss: 1.7550883568151978
Validation loss: 2.4467593928995

Epoch: 6| Step: 2
Training loss: 2.2783540084708167
Validation loss: 2.4542838805928575

Epoch: 6| Step: 3
Training loss: 2.88273806592168
Validation loss: 2.4213785266661754

Epoch: 6| Step: 4
Training loss: 2.646020377048218
Validation loss: 2.4581779815353193

Epoch: 6| Step: 5
Training loss: 2.8746617989024115
Validation loss: 2.463006415920048

Epoch: 6| Step: 6
Training loss: 2.841380714803118
Validation loss: 2.4484462687909545

Epoch: 6| Step: 7
Training loss: 2.468238777635104
Validation loss: 2.458638563511869

Epoch: 6| Step: 8
Training loss: 3.1152548858063027
Validation loss: 2.4644894693488375

Epoch: 6| Step: 9
Training loss: 2.6814977464725116
Validation loss: 2.444711746037889

Epoch: 6| Step: 10
Training loss: 2.525786826790984
Validation loss: 2.4681445856998674

Epoch: 6| Step: 11
Training loss: 2.309393574765138
Validation loss: 2.4357203583228992

Epoch: 6| Step: 12
Training loss: 3.0696769231229393
Validation loss: 2.4495415176323254

Epoch: 6| Step: 13
Training loss: 2.1174855603082667
Validation loss: 2.452469122987913

Epoch: 176| Step: 0
Training loss: 2.615905356564563
Validation loss: 2.4530643925659534

Epoch: 6| Step: 1
Training loss: 2.4315005097370057
Validation loss: 2.4390227097550023

Epoch: 6| Step: 2
Training loss: 2.6777201289737746
Validation loss: 2.475849221119903

Epoch: 6| Step: 3
Training loss: 2.6241917046904764
Validation loss: 2.4208102762085364

Epoch: 6| Step: 4
Training loss: 2.524802202087912
Validation loss: 2.423426561606013

Epoch: 6| Step: 5
Training loss: 2.635670545409268
Validation loss: 2.4365539260841746

Epoch: 6| Step: 6
Training loss: 2.014201171457395
Validation loss: 2.486760080217575

Epoch: 6| Step: 7
Training loss: 2.3431416039782884
Validation loss: 2.443782696509007

Epoch: 6| Step: 8
Training loss: 2.237156237402318
Validation loss: 2.4169442233483505

Epoch: 6| Step: 9
Training loss: 2.686803505712337
Validation loss: 2.4413539004384144

Epoch: 6| Step: 10
Training loss: 2.8245195199619775
Validation loss: 2.4931910862628617

Epoch: 6| Step: 11
Training loss: 2.5571531010915
Validation loss: 2.434038841718572

Epoch: 6| Step: 12
Training loss: 3.218594741780395
Validation loss: 2.4678155852669166

Epoch: 6| Step: 13
Training loss: 3.5285570797427157
Validation loss: 2.4726804678857377

Epoch: 177| Step: 0
Training loss: 2.8351535186376
Validation loss: 2.4429703952838295

Epoch: 6| Step: 1
Training loss: 2.7071165041138703
Validation loss: 2.4623708129013715

Epoch: 6| Step: 2
Training loss: 3.4641060199679403
Validation loss: 2.4764714476448573

Epoch: 6| Step: 3
Training loss: 2.70456635078768
Validation loss: 2.4728476061461637

Epoch: 6| Step: 4
Training loss: 1.8185628578413016
Validation loss: 2.448037688109498

Epoch: 6| Step: 5
Training loss: 2.596042952383515
Validation loss: 2.478550492530016

Epoch: 6| Step: 6
Training loss: 2.772806445041745
Validation loss: 2.4892323269212846

Epoch: 6| Step: 7
Training loss: 2.473589921256403
Validation loss: 2.4138739860362217

Epoch: 6| Step: 8
Training loss: 2.14748713888063
Validation loss: 2.4509870007683427

Epoch: 6| Step: 9
Training loss: 2.8127575226611023
Validation loss: 2.4581472709855015

Epoch: 6| Step: 10
Training loss: 2.4740832702142628
Validation loss: 2.450864811514328

Epoch: 6| Step: 11
Training loss: 3.300202484854534
Validation loss: 2.474084540592932

Epoch: 6| Step: 12
Training loss: 2.315003869566351
Validation loss: 2.4515371812197064

Epoch: 6| Step: 13
Training loss: 2.3893727532091087
Validation loss: 2.4724654876902563

Epoch: 178| Step: 0
Training loss: 2.61047559358578
Validation loss: 2.4679077547680635

Epoch: 6| Step: 1
Training loss: 3.1358533451666037
Validation loss: 2.4542173178749156

Epoch: 6| Step: 2
Training loss: 2.8624442882197063
Validation loss: 2.425948686399112

Epoch: 6| Step: 3
Training loss: 2.675845759372932
Validation loss: 2.445246440067619

Epoch: 6| Step: 4
Training loss: 2.2836612865002466
Validation loss: 2.4816725903984844

Epoch: 6| Step: 5
Training loss: 2.4270444765923473
Validation loss: 2.446964953558889

Epoch: 6| Step: 6
Training loss: 2.867659776754356
Validation loss: 2.47843403761414

Epoch: 6| Step: 7
Training loss: 3.2364599906656037
Validation loss: 2.4600744017521303

Epoch: 6| Step: 8
Training loss: 2.3214458947006937
Validation loss: 2.446863486616326

Epoch: 6| Step: 9
Training loss: 2.866327224270076
Validation loss: 2.4324135535151394

Epoch: 6| Step: 10
Training loss: 1.803464532946029
Validation loss: 2.4522050959375674

Epoch: 6| Step: 11
Training loss: 2.8535390196251806
Validation loss: 2.4156364430989337

Epoch: 6| Step: 12
Training loss: 2.34540479098501
Validation loss: 2.4549414328523484

Epoch: 6| Step: 13
Training loss: 2.333806625684499
Validation loss: 2.454433674445917

Epoch: 179| Step: 0
Training loss: 2.4445101363092268
Validation loss: 2.459575844504303

Epoch: 6| Step: 1
Training loss: 2.846390556714763
Validation loss: 2.472504090290072

Epoch: 6| Step: 2
Training loss: 2.487829242176062
Validation loss: 2.4644244508748963

Epoch: 6| Step: 3
Training loss: 1.7717485493745135
Validation loss: 2.421859873480635

Epoch: 6| Step: 4
Training loss: 2.5760573910486384
Validation loss: 2.4575818167835934

Epoch: 6| Step: 5
Training loss: 2.634842539937845
Validation loss: 2.439900032053563

Epoch: 6| Step: 6
Training loss: 2.6813667752105688
Validation loss: 2.4833359230288425

Epoch: 6| Step: 7
Training loss: 2.607566526593865
Validation loss: 2.4087743687514687

Epoch: 6| Step: 8
Training loss: 2.8249934441144373
Validation loss: 2.445914102133205

Epoch: 6| Step: 9
Training loss: 2.748073423212647
Validation loss: 2.481509302340587

Epoch: 6| Step: 10
Training loss: 3.0284297447274553
Validation loss: 2.435176192330966

Epoch: 6| Step: 11
Training loss: 2.366628146417757
Validation loss: 2.4659215784859256

Epoch: 6| Step: 12
Training loss: 3.0075355303180524
Validation loss: 2.437961667169318

Epoch: 6| Step: 13
Training loss: 2.6978453303937378
Validation loss: 2.4662516496590845

Epoch: 180| Step: 0
Training loss: 3.173243636084408
Validation loss: 2.438613606717239

Epoch: 6| Step: 1
Training loss: 2.334934287116575
Validation loss: 2.4333880048210226

Epoch: 6| Step: 2
Training loss: 3.7130687496314616
Validation loss: 2.435710085721088

Epoch: 6| Step: 3
Training loss: 2.4054629537345926
Validation loss: 2.4429270926695423

Epoch: 6| Step: 4
Training loss: 2.8911308876896973
Validation loss: 2.4320233106458042

Epoch: 6| Step: 5
Training loss: 2.3974400536895923
Validation loss: 2.4636346925672834

Epoch: 6| Step: 6
Training loss: 2.5903136688645816
Validation loss: 2.453120548078872

Epoch: 6| Step: 7
Training loss: 2.15558691471706
Validation loss: 2.435010113199557

Epoch: 6| Step: 8
Training loss: 2.47959257255435
Validation loss: 2.447120515740805

Epoch: 6| Step: 9
Training loss: 2.481762742265853
Validation loss: 2.451010398839105

Epoch: 6| Step: 10
Training loss: 2.3289298899979007
Validation loss: 2.46086183566103

Epoch: 6| Step: 11
Training loss: 2.8832643563136338
Validation loss: 2.481595977692885

Epoch: 6| Step: 12
Training loss: 2.6442122612152072
Validation loss: 2.466314532724211

Epoch: 6| Step: 13
Training loss: 2.2530520190563132
Validation loss: 2.4171897792666877

Epoch: 181| Step: 0
Training loss: 2.7047361302315034
Validation loss: 2.449241061733029

Epoch: 6| Step: 1
Training loss: 1.823750928077174
Validation loss: 2.4549775082053054

Epoch: 6| Step: 2
Training loss: 2.4730494278512136
Validation loss: 2.459169419445903

Epoch: 6| Step: 3
Training loss: 3.0479430845258233
Validation loss: 2.4383690607093453

Epoch: 6| Step: 4
Training loss: 1.6652226232212002
Validation loss: 2.4424400910839896

Epoch: 6| Step: 5
Training loss: 2.4169359605202327
Validation loss: 2.4453790487809393

Epoch: 6| Step: 6
Training loss: 2.8610601410343044
Validation loss: 2.4372905213125957

Epoch: 6| Step: 7
Training loss: 2.850853735226225
Validation loss: 2.457859826583929

Epoch: 6| Step: 8
Training loss: 3.0347216674276543
Validation loss: 2.4549222112792957

Epoch: 6| Step: 9
Training loss: 2.6962459108960974
Validation loss: 2.4154897500114525

Epoch: 6| Step: 10
Training loss: 2.0290361517801285
Validation loss: 2.4399562184147783

Epoch: 6| Step: 11
Training loss: 3.37821214194742
Validation loss: 2.4884045371165313

Epoch: 6| Step: 12
Training loss: 2.944636988392322
Validation loss: 2.479515514060416

Epoch: 6| Step: 13
Training loss: 2.0610850711535966
Validation loss: 2.463628582198516

Epoch: 182| Step: 0
Training loss: 2.0842582429729957
Validation loss: 2.4512848707130748

Epoch: 6| Step: 1
Training loss: 3.3611815806866443
Validation loss: 2.4390411400823235

Epoch: 6| Step: 2
Training loss: 2.724415980759594
Validation loss: 2.482530723108788

Epoch: 6| Step: 3
Training loss: 2.6963652835228946
Validation loss: 2.424055774125445

Epoch: 6| Step: 4
Training loss: 2.3640684219760177
Validation loss: 2.4453318794083034

Epoch: 6| Step: 5
Training loss: 2.282335519536199
Validation loss: 2.431097121271694

Epoch: 6| Step: 6
Training loss: 3.135395915996707
Validation loss: 2.4738722042166406

Epoch: 6| Step: 7
Training loss: 2.3830037024657718
Validation loss: 2.453367829608065

Epoch: 6| Step: 8
Training loss: 2.69934011095433
Validation loss: 2.446344503743389

Epoch: 6| Step: 9
Training loss: 2.693671747119847
Validation loss: 2.4332075383885043

Epoch: 6| Step: 10
Training loss: 2.7996238966797145
Validation loss: 2.4702328763415324

Epoch: 6| Step: 11
Training loss: 2.6226612753503202
Validation loss: 2.417396215878833

Epoch: 6| Step: 12
Training loss: 2.489238177731262
Validation loss: 2.4659207987649494

Epoch: 6| Step: 13
Training loss: 2.547841736967208
Validation loss: 2.421752780525711

Epoch: 183| Step: 0
Training loss: 2.3660980840040087
Validation loss: 2.471050448706768

Epoch: 6| Step: 1
Training loss: 2.607612425738574
Validation loss: 2.445420911584033

Epoch: 6| Step: 2
Training loss: 2.4629019510854264
Validation loss: 2.4384975811004255

Epoch: 6| Step: 3
Training loss: 1.8004894889094638
Validation loss: 2.446322299730745

Epoch: 6| Step: 4
Training loss: 2.771788286747217
Validation loss: 2.453974624561228

Epoch: 6| Step: 5
Training loss: 2.7556610793072482
Validation loss: 2.449583280519353

Epoch: 6| Step: 6
Training loss: 2.72584336205409
Validation loss: 2.4632411214032164

Epoch: 6| Step: 7
Training loss: 2.316445155725657
Validation loss: 2.4675708237640444

Epoch: 6| Step: 8
Training loss: 3.195469187879689
Validation loss: 2.4621170969718644

Epoch: 6| Step: 9
Training loss: 2.8368833031357408
Validation loss: 2.4791150372503705

Epoch: 6| Step: 10
Training loss: 2.758634018043679
Validation loss: 2.464104450476927

Epoch: 6| Step: 11
Training loss: 3.0353713186450886
Validation loss: 2.471934189155282

Epoch: 6| Step: 12
Training loss: 2.549988230509805
Validation loss: 2.4560108379304455

Epoch: 6| Step: 13
Training loss: 2.783529568986052
Validation loss: 2.4235770498966436

Epoch: 184| Step: 0
Training loss: 2.0314030516257224
Validation loss: 2.457892700828553

Epoch: 6| Step: 1
Training loss: 2.6826465162098074
Validation loss: 2.461949894064416

Epoch: 6| Step: 2
Training loss: 2.6089058642762075
Validation loss: 2.449350342955208

Epoch: 6| Step: 3
Training loss: 2.4591438659865723
Validation loss: 2.45803731495605

Epoch: 6| Step: 4
Training loss: 2.512337188759585
Validation loss: 2.4669590002980857

Epoch: 6| Step: 5
Training loss: 2.788569815579937
Validation loss: 2.441883433708903

Epoch: 6| Step: 6
Training loss: 3.300185435315825
Validation loss: 2.4505366359010807

Epoch: 6| Step: 7
Training loss: 2.6636762143194965
Validation loss: 2.455978853941211

Epoch: 6| Step: 8
Training loss: 3.052271675487201
Validation loss: 2.4618145946345207

Epoch: 6| Step: 9
Training loss: 2.6747969327482055
Validation loss: 2.4336542018540928

Epoch: 6| Step: 10
Training loss: 2.6660083812573294
Validation loss: 2.481527640783537

Epoch: 6| Step: 11
Training loss: 2.966932202003305
Validation loss: 2.4629181719350903

Epoch: 6| Step: 12
Training loss: 2.039321473728865
Validation loss: 2.4671052048889512

Epoch: 6| Step: 13
Training loss: 1.9299519469648418
Validation loss: 2.4525019607786147

Epoch: 185| Step: 0
Training loss: 2.5142365409018215
Validation loss: 2.45094211392006

Epoch: 6| Step: 1
Training loss: 2.9374592555040797
Validation loss: 2.471282575859635

Epoch: 6| Step: 2
Training loss: 2.4410968553955157
Validation loss: 2.472386157092766

Epoch: 6| Step: 3
Training loss: 2.978528112166447
Validation loss: 2.4309960133694535

Epoch: 6| Step: 4
Training loss: 2.36512167651873
Validation loss: 2.442146507607094

Epoch: 6| Step: 5
Training loss: 2.4810761436454154
Validation loss: 2.414121845908528

Epoch: 6| Step: 6
Training loss: 2.310450909025149
Validation loss: 2.4440159617508774

Epoch: 6| Step: 7
Training loss: 2.848250631876925
Validation loss: 2.4598152181218826

Epoch: 6| Step: 8
Training loss: 2.866697513798782
Validation loss: 2.451152739437688

Epoch: 6| Step: 9
Training loss: 1.8719330181625904
Validation loss: 2.4654552842006003

Epoch: 6| Step: 10
Training loss: 3.3622665380950036
Validation loss: 2.4494710702873883

Epoch: 6| Step: 11
Training loss: 3.1533156247545677
Validation loss: 2.4515496567208723

Epoch: 6| Step: 12
Training loss: 2.3246021627687017
Validation loss: 2.4563510098625327

Epoch: 6| Step: 13
Training loss: 2.027628207968551
Validation loss: 2.4900960649545243

Epoch: 186| Step: 0
Training loss: 2.4759955975766093
Validation loss: 2.4589495323723223

Epoch: 6| Step: 1
Training loss: 2.949917135852876
Validation loss: 2.4421455649323907

Epoch: 6| Step: 2
Training loss: 1.5715249917558214
Validation loss: 2.431396802141827

Epoch: 6| Step: 3
Training loss: 2.881349435277899
Validation loss: 2.429370042847707

Epoch: 6| Step: 4
Training loss: 2.4276413719426975
Validation loss: 2.4539337329982875

Epoch: 6| Step: 5
Training loss: 2.4659270552391153
Validation loss: 2.462424375177461

Epoch: 6| Step: 6
Training loss: 2.5176925218869806
Validation loss: 2.445973996898233

Epoch: 6| Step: 7
Training loss: 3.0825269177229138
Validation loss: 2.465001805216689

Epoch: 6| Step: 8
Training loss: 2.404008093441051
Validation loss: 2.4235424006112223

Epoch: 6| Step: 9
Training loss: 2.9470132222935557
Validation loss: 2.4461573331575566

Epoch: 6| Step: 10
Training loss: 2.7213369755300807
Validation loss: 2.448213292680071

Epoch: 6| Step: 11
Training loss: 3.138975858655677
Validation loss: 2.471091607910113

Epoch: 6| Step: 12
Training loss: 2.334759571513714
Validation loss: 2.434967567098313

Epoch: 6| Step: 13
Training loss: 2.8900568223270797
Validation loss: 2.461411640922338

Epoch: 187| Step: 0
Training loss: 3.1617459329238256
Validation loss: 2.4764738927835634

Epoch: 6| Step: 1
Training loss: 2.777106209212625
Validation loss: 2.4383563453451353

Epoch: 6| Step: 2
Training loss: 2.1834733141610787
Validation loss: 2.4523787650504967

Epoch: 6| Step: 3
Training loss: 2.2669577000343692
Validation loss: 2.463727880056569

Epoch: 6| Step: 4
Training loss: 2.9189716132450934
Validation loss: 2.446693504055317

Epoch: 6| Step: 5
Training loss: 2.451265541505931
Validation loss: 2.437501207408706

Epoch: 6| Step: 6
Training loss: 2.7708264592212126
Validation loss: 2.471030332082161

Epoch: 6| Step: 7
Training loss: 2.6562871593793274
Validation loss: 2.4713941697263593

Epoch: 6| Step: 8
Training loss: 2.5441807262716476
Validation loss: 2.4414851770256147

Epoch: 6| Step: 9
Training loss: 2.913151575641256
Validation loss: 2.459696222116091

Epoch: 6| Step: 10
Training loss: 2.8253737075870893
Validation loss: 2.462291976464903

Epoch: 6| Step: 11
Training loss: 2.472200517404362
Validation loss: 2.4270362819515743

Epoch: 6| Step: 12
Training loss: 2.2323075980292764
Validation loss: 2.4492582193132946

Epoch: 6| Step: 13
Training loss: 2.7695493280549788
Validation loss: 2.454744379659209

Epoch: 188| Step: 0
Training loss: 2.365273082362878
Validation loss: 2.4822821252766265

Epoch: 6| Step: 1
Training loss: 1.9677411476685407
Validation loss: 2.453144836032069

Epoch: 6| Step: 2
Training loss: 2.3121838740188787
Validation loss: 2.470939287523961

Epoch: 6| Step: 3
Training loss: 2.3731406110923703
Validation loss: 2.4578376181233694

Epoch: 6| Step: 4
Training loss: 2.9489171966590493
Validation loss: 2.4498124720955095

Epoch: 6| Step: 5
Training loss: 2.211562213323622
Validation loss: 2.457225808356158

Epoch: 6| Step: 6
Training loss: 2.3896913388185133
Validation loss: 2.4275049106382682

Epoch: 6| Step: 7
Training loss: 2.772759325095192
Validation loss: 2.446006365349405

Epoch: 6| Step: 8
Training loss: 3.02230411547926
Validation loss: 2.43083109718211

Epoch: 6| Step: 9
Training loss: 2.622947980730678
Validation loss: 2.474083263997074

Epoch: 6| Step: 10
Training loss: 3.2632449297154325
Validation loss: 2.463905797873077

Epoch: 6| Step: 11
Training loss: 2.6335763247764152
Validation loss: 2.4596023086142456

Epoch: 6| Step: 12
Training loss: 2.6778618735601705
Validation loss: 2.4606599551290222

Epoch: 6| Step: 13
Training loss: 3.3747130377949737
Validation loss: 2.425373552194211

Epoch: 189| Step: 0
Training loss: 2.8951871594690544
Validation loss: 2.4613566359259673

Epoch: 6| Step: 1
Training loss: 3.08530359638405
Validation loss: 2.4612298482215755

Epoch: 6| Step: 2
Training loss: 2.859016604449691
Validation loss: 2.443379323887144

Epoch: 6| Step: 3
Training loss: 2.1382127521597414
Validation loss: 2.41007432273017

Epoch: 6| Step: 4
Training loss: 2.483018996892336
Validation loss: 2.462757798534169

Epoch: 6| Step: 5
Training loss: 2.2449638377380303
Validation loss: 2.4621972852801552

Epoch: 6| Step: 6
Training loss: 2.7003967982579877
Validation loss: 2.4723956085045775

Epoch: 6| Step: 7
Training loss: 2.6660306489961285
Validation loss: 2.439877799864281

Epoch: 6| Step: 8
Training loss: 3.1743079565170764
Validation loss: 2.437911755323624

Epoch: 6| Step: 9
Training loss: 2.477861996099926
Validation loss: 2.4643077039611176

Epoch: 6| Step: 10
Training loss: 1.9299159358795315
Validation loss: 2.4568985789200406

Epoch: 6| Step: 11
Training loss: 2.5059408173144897
Validation loss: 2.4815327607707878

Epoch: 6| Step: 12
Training loss: 2.893895294298886
Validation loss: 2.457709571357204

Epoch: 6| Step: 13
Training loss: 2.1859961653847666
Validation loss: 2.44157020618819

Epoch: 190| Step: 0
Training loss: 2.164818697026916
Validation loss: 2.4577232338631823

Epoch: 6| Step: 1
Training loss: 2.6109991444148366
Validation loss: 2.475183766373905

Epoch: 6| Step: 2
Training loss: 2.324416105722886
Validation loss: 2.4289655411741014

Epoch: 6| Step: 3
Training loss: 2.8667602220690522
Validation loss: 2.452109000071079

Epoch: 6| Step: 4
Training loss: 2.534831398016371
Validation loss: 2.439969433973461

Epoch: 6| Step: 5
Training loss: 3.1834089167011856
Validation loss: 2.457446354580206

Epoch: 6| Step: 6
Training loss: 2.7145293348733026
Validation loss: 2.432877855562898

Epoch: 6| Step: 7
Training loss: 2.5014140898160924
Validation loss: 2.452325658687025

Epoch: 6| Step: 8
Training loss: 2.7295355122903247
Validation loss: 2.4602187615505122

Epoch: 6| Step: 9
Training loss: 1.9233572601513744
Validation loss: 2.4494640600843467

Epoch: 6| Step: 10
Training loss: 2.8385379522923424
Validation loss: 2.471967018308176

Epoch: 6| Step: 11
Training loss: 2.8499517202470743
Validation loss: 2.444339956531381

Epoch: 6| Step: 12
Training loss: 3.038833099572213
Validation loss: 2.4271529056260577

Epoch: 6| Step: 13
Training loss: 1.8876997961947204
Validation loss: 2.4532893830984333

Epoch: 191| Step: 0
Training loss: 2.3970887806237764
Validation loss: 2.460431993857411

Epoch: 6| Step: 1
Training loss: 2.978259786655879
Validation loss: 2.4347912441127404

Epoch: 6| Step: 2
Training loss: 2.6562164304519373
Validation loss: 2.463732052672517

Epoch: 6| Step: 3
Training loss: 2.749671916464464
Validation loss: 2.4539836496140963

Epoch: 6| Step: 4
Training loss: 2.441906101174225
Validation loss: 2.4613226226400324

Epoch: 6| Step: 5
Training loss: 3.0048079110952446
Validation loss: 2.4439008671182

Epoch: 6| Step: 6
Training loss: 2.992860882636776
Validation loss: 2.4477338348651916

Epoch: 6| Step: 7
Training loss: 2.948266609548153
Validation loss: 2.442465278225954

Epoch: 6| Step: 8
Training loss: 2.2009745173419817
Validation loss: 2.4763571145282746

Epoch: 6| Step: 9
Training loss: 2.8815604284709506
Validation loss: 2.489546345924139

Epoch: 6| Step: 10
Training loss: 1.912634367367711
Validation loss: 2.4543252128628326

Epoch: 6| Step: 11
Training loss: 2.3956582931486157
Validation loss: 2.464243795931392

Epoch: 6| Step: 12
Training loss: 2.245669754478427
Validation loss: 2.432516822021144

Epoch: 6| Step: 13
Training loss: 2.8037193561296023
Validation loss: 2.45938329103047

Epoch: 192| Step: 0
Training loss: 2.8440300101455325
Validation loss: 2.4642296639715973

Epoch: 6| Step: 1
Training loss: 2.6377072438178875
Validation loss: 2.444840989463232

Epoch: 6| Step: 2
Training loss: 2.9881950814782328
Validation loss: 2.452535788030862

Epoch: 6| Step: 3
Training loss: 2.1997992120495318
Validation loss: 2.449212888373823

Epoch: 6| Step: 4
Training loss: 2.5085574555838037
Validation loss: 2.4966085318030493

Epoch: 6| Step: 5
Training loss: 2.2927998833155154
Validation loss: 2.444031381207645

Epoch: 6| Step: 6
Training loss: 3.126222142610587
Validation loss: 2.4591081686770315

Epoch: 6| Step: 7
Training loss: 2.1423212834603924
Validation loss: 2.455718311030098

Epoch: 6| Step: 8
Training loss: 3.3706617376899093
Validation loss: 2.4567547623077437

Epoch: 6| Step: 9
Training loss: 2.4040755317996263
Validation loss: 2.435870183927437

Epoch: 6| Step: 10
Training loss: 2.6385480850288245
Validation loss: 2.4647212273267134

Epoch: 6| Step: 11
Training loss: 2.922949934915322
Validation loss: 2.428176575219317

Epoch: 6| Step: 12
Training loss: 2.217779242685599
Validation loss: 2.4407146218761

Epoch: 6| Step: 13
Training loss: 1.614829145198455
Validation loss: 2.4422458191829772

Epoch: 193| Step: 0
Training loss: 2.6371294168042114
Validation loss: 2.4279642236614656

Epoch: 6| Step: 1
Training loss: 2.831801580126861
Validation loss: 2.4641333389407425

Epoch: 6| Step: 2
Training loss: 1.7307280955357718
Validation loss: 2.451299674469282

Epoch: 6| Step: 3
Training loss: 2.3729179692676463
Validation loss: 2.4647873893698318

Epoch: 6| Step: 4
Training loss: 2.7920627218158462
Validation loss: 2.4501283211469387

Epoch: 6| Step: 5
Training loss: 2.015434670148713
Validation loss: 2.461515397261511

Epoch: 6| Step: 6
Training loss: 3.0427186444473597
Validation loss: 2.4741494803210164

Epoch: 6| Step: 7
Training loss: 3.128267334892382
Validation loss: 2.4408018321043037

Epoch: 6| Step: 8
Training loss: 2.5783465665918643
Validation loss: 2.47824279679396

Epoch: 6| Step: 9
Training loss: 2.7276545307918605
Validation loss: 2.4839876770372533

Epoch: 6| Step: 10
Training loss: 2.4254224822129693
Validation loss: 2.4576548465119123

Epoch: 6| Step: 11
Training loss: 2.9142310372467293
Validation loss: 2.438388070023637

Epoch: 6| Step: 12
Training loss: 2.536366883391101
Validation loss: 2.4479244262600255

Epoch: 6| Step: 13
Training loss: 2.676252025762056
Validation loss: 2.4590894597522093

Epoch: 194| Step: 0
Training loss: 3.14429377257391
Validation loss: 2.4602604166538846

Epoch: 6| Step: 1
Training loss: 2.530454439957174
Validation loss: 2.450753959952498

Epoch: 6| Step: 2
Training loss: 3.378398455792128
Validation loss: 2.4229932486526677

Epoch: 6| Step: 3
Training loss: 2.540357335854416
Validation loss: 2.4643366170899785

Epoch: 6| Step: 4
Training loss: 2.2363189563813144
Validation loss: 2.470148927231682

Epoch: 6| Step: 5
Training loss: 2.476981816458769
Validation loss: 2.4672586264805187

Epoch: 6| Step: 6
Training loss: 1.839535567922473
Validation loss: 2.460386194634673

Epoch: 6| Step: 7
Training loss: 3.437330900714725
Validation loss: 2.4819706066386473

Epoch: 6| Step: 8
Training loss: 2.54297981318398
Validation loss: 2.4423640384354246

Epoch: 6| Step: 9
Training loss: 2.695679747398995
Validation loss: 2.456522056544438

Epoch: 6| Step: 10
Training loss: 2.1750261239423176
Validation loss: 2.4649639504337246

Epoch: 6| Step: 11
Training loss: 2.3906656897578857
Validation loss: 2.463435252794549

Epoch: 6| Step: 12
Training loss: 2.5999950922406168
Validation loss: 2.4375515536106516

Epoch: 6| Step: 13
Training loss: 2.1483180897213576
Validation loss: 2.473458700628353

Epoch: 195| Step: 0
Training loss: 3.018455639500411
Validation loss: 2.4542987900390525

Epoch: 6| Step: 1
Training loss: 2.6127162716066143
Validation loss: 2.437428127603837

Epoch: 6| Step: 2
Training loss: 2.09095418074771
Validation loss: 2.4653674569852564

Epoch: 6| Step: 3
Training loss: 2.7893646674933876
Validation loss: 2.4667413218285996

Epoch: 6| Step: 4
Training loss: 2.941561062856121
Validation loss: 2.466948695142849

Epoch: 6| Step: 5
Training loss: 2.1861452539737996
Validation loss: 2.465243348474308

Epoch: 6| Step: 6
Training loss: 2.0146015733158116
Validation loss: 2.4427364844597417

Epoch: 6| Step: 7
Training loss: 2.8145464763209374
Validation loss: 2.4527581795434346

Epoch: 6| Step: 8
Training loss: 2.4129193509753954
Validation loss: 2.4597733032605116

Epoch: 6| Step: 9
Training loss: 2.5916967882897954
Validation loss: 2.4479235025666948

Epoch: 6| Step: 10
Training loss: 2.764022794085748
Validation loss: 2.453358890065722

Epoch: 6| Step: 11
Training loss: 2.770694719638227
Validation loss: 2.4510376279712145

Epoch: 6| Step: 12
Training loss: 2.9477202189379748
Validation loss: 2.45362743204022

Epoch: 6| Step: 13
Training loss: 2.89859197569947
Validation loss: 2.4835500494866203

Epoch: 196| Step: 0
Training loss: 3.3243102656138204
Validation loss: 2.455380615209545

Epoch: 6| Step: 1
Training loss: 2.9909882295989747
Validation loss: 2.43354529768079

Epoch: 6| Step: 2
Training loss: 2.7116525020436524
Validation loss: 2.467426857742228

Epoch: 6| Step: 3
Training loss: 2.539580400906988
Validation loss: 2.4447573460451433

Epoch: 6| Step: 4
Training loss: 3.083678526929532
Validation loss: 2.4504430278443

Epoch: 6| Step: 5
Training loss: 2.2995367910581854
Validation loss: 2.451407088181249

Epoch: 6| Step: 6
Training loss: 2.8182949130167056
Validation loss: 2.477246141830303

Epoch: 6| Step: 7
Training loss: 2.505878308671407
Validation loss: 2.459551241694699

Epoch: 6| Step: 8
Training loss: 2.4984605817939327
Validation loss: 2.4514969757924723

Epoch: 6| Step: 9
Training loss: 2.55599466181811
Validation loss: 2.4630640454873953

Epoch: 6| Step: 10
Training loss: 2.521323626629512
Validation loss: 2.4980235500988655

Epoch: 6| Step: 11
Training loss: 2.1295411449361747
Validation loss: 2.4591765187402377

Epoch: 6| Step: 12
Training loss: 2.451287231162567
Validation loss: 2.4561328463282064

Epoch: 6| Step: 13
Training loss: 2.1639728183273172
Validation loss: 2.468689068973941

Epoch: 197| Step: 0
Training loss: 2.577549818206844
Validation loss: 2.4250019476895113

Epoch: 6| Step: 1
Training loss: 2.8355235068218456
Validation loss: 2.430240203822994

Epoch: 6| Step: 2
Training loss: 1.7785626937030161
Validation loss: 2.482794610030956

Epoch: 6| Step: 3
Training loss: 1.8114744771011722
Validation loss: 2.4448075748030917

Epoch: 6| Step: 4
Training loss: 3.100737416329286
Validation loss: 2.4769796771425066

Epoch: 6| Step: 5
Training loss: 2.6532364748843054
Validation loss: 2.454676112450943

Epoch: 6| Step: 6
Training loss: 2.4525573462451584
Validation loss: 2.4621838933250415

Epoch: 6| Step: 7
Training loss: 2.5707700374319105
Validation loss: 2.444740130136826

Epoch: 6| Step: 8
Training loss: 2.8346608548767187
Validation loss: 2.463885974599837

Epoch: 6| Step: 9
Training loss: 2.6337036073687723
Validation loss: 2.471033952879181

Epoch: 6| Step: 10
Training loss: 2.3586357929213877
Validation loss: 2.429051693547666

Epoch: 6| Step: 11
Training loss: 2.9365338298993025
Validation loss: 2.476754919617401

Epoch: 6| Step: 12
Training loss: 3.192868218870571
Validation loss: 2.4360522063645256

Epoch: 6| Step: 13
Training loss: 2.3740324760086065
Validation loss: 2.4772301953867784

Epoch: 198| Step: 0
Training loss: 2.2510731574990177
Validation loss: 2.4804634459147104

Epoch: 6| Step: 1
Training loss: 3.023895150178402
Validation loss: 2.462907023918214

Epoch: 6| Step: 2
Training loss: 2.107029493852392
Validation loss: 2.449760967210629

Epoch: 6| Step: 3
Training loss: 2.662845456631556
Validation loss: 2.459171562788614

Epoch: 6| Step: 4
Training loss: 2.2517600063551853
Validation loss: 2.458973985860012

Epoch: 6| Step: 5
Training loss: 2.6879495643924893
Validation loss: 2.440902860990003

Epoch: 6| Step: 6
Training loss: 2.644615724192042
Validation loss: 2.433883802192175

Epoch: 6| Step: 7
Training loss: 2.9698058810518866
Validation loss: 2.448564657734041

Epoch: 6| Step: 8
Training loss: 2.6127321496057525
Validation loss: 2.444032441685076

Epoch: 6| Step: 9
Training loss: 2.597649733456347
Validation loss: 2.4365010755954866

Epoch: 6| Step: 10
Training loss: 2.8096981927742273
Validation loss: 2.4548455088485697

Epoch: 6| Step: 11
Training loss: 2.5086926018155458
Validation loss: 2.458936173815648

Epoch: 6| Step: 12
Training loss: 2.4855216397172417
Validation loss: 2.461803200023077

Epoch: 6| Step: 13
Training loss: 3.0925304436630707
Validation loss: 2.4576593731476635

Epoch: 199| Step: 0
Training loss: 3.0593126799146124
Validation loss: 2.4582475421661325

Epoch: 6| Step: 1
Training loss: 2.608466806947479
Validation loss: 2.449024089405819

Epoch: 6| Step: 2
Training loss: 2.425905577687782
Validation loss: 2.4563828105393126

Epoch: 6| Step: 3
Training loss: 2.6874746277077213
Validation loss: 2.4545472646908704

Epoch: 6| Step: 4
Training loss: 2.498724802948324
Validation loss: 2.443063936552977

Epoch: 6| Step: 5
Training loss: 2.6591547622661262
Validation loss: 2.441571533383646

Epoch: 6| Step: 6
Training loss: 3.0053702607761843
Validation loss: 2.4327433751365395

Epoch: 6| Step: 7
Training loss: 2.8442969163081617
Validation loss: 2.4867764902835545

Epoch: 6| Step: 8
Training loss: 2.446629178888631
Validation loss: 2.4575747937273835

Epoch: 6| Step: 9
Training loss: 2.6598576001363172
Validation loss: 2.4443809708302875

Epoch: 6| Step: 10
Training loss: 1.8678231533972758
Validation loss: 2.4692348607943932

Epoch: 6| Step: 11
Training loss: 2.572645803755268
Validation loss: 2.4605647416644105

Epoch: 6| Step: 12
Training loss: 2.9811028722130333
Validation loss: 2.4391287207299306

Epoch: 6| Step: 13
Training loss: 2.007791838187097
Validation loss: 2.4432198132215888

Epoch: 200| Step: 0
Training loss: 2.8766527815476683
Validation loss: 2.4467301484214348

Epoch: 6| Step: 1
Training loss: 2.9782706738416236
Validation loss: 2.498243827187898

Epoch: 6| Step: 2
Training loss: 3.396675161538874
Validation loss: 2.4564623313027734

Epoch: 6| Step: 3
Training loss: 2.5247769889926372
Validation loss: 2.4539623671929585

Epoch: 6| Step: 4
Training loss: 2.1178971681279535
Validation loss: 2.461821467615375

Epoch: 6| Step: 5
Training loss: 2.8371433185120964
Validation loss: 2.4392614739014116

Epoch: 6| Step: 6
Training loss: 2.699058251915862
Validation loss: 2.463710896084021

Epoch: 6| Step: 7
Training loss: 2.477589391333261
Validation loss: 2.4660419135077403

Epoch: 6| Step: 8
Training loss: 2.106474740464351
Validation loss: 2.4415745468706493

Epoch: 6| Step: 9
Training loss: 3.0954614394516895
Validation loss: 2.4505930313644786

Epoch: 6| Step: 10
Training loss: 2.2973261798777527
Validation loss: 2.451413256203492

Epoch: 6| Step: 11
Training loss: 2.462232751431696
Validation loss: 2.446876467897624

Epoch: 6| Step: 12
Training loss: 2.534805626273597
Validation loss: 2.4677602015065996

Epoch: 6| Step: 13
Training loss: 2.0556075788335573
Validation loss: 2.4809965097221345

Epoch: 201| Step: 0
Training loss: 2.409222636866352
Validation loss: 2.4629791936824397

Epoch: 6| Step: 1
Training loss: 2.824060712642596
Validation loss: 2.4671999633235684

Epoch: 6| Step: 2
Training loss: 2.2184435135754086
Validation loss: 2.462792992212979

Epoch: 6| Step: 3
Training loss: 2.2859922265615653
Validation loss: 2.472465420293302

Epoch: 6| Step: 4
Training loss: 2.5317497231087884
Validation loss: 2.478871312846393

Epoch: 6| Step: 5
Training loss: 2.3528757072204067
Validation loss: 2.42515978551572

Epoch: 6| Step: 6
Training loss: 3.2339241340151177
Validation loss: 2.4518621969793575

Epoch: 6| Step: 7
Training loss: 2.861384785061875
Validation loss: 2.4542214398071414

Epoch: 6| Step: 8
Training loss: 3.151109947957032
Validation loss: 2.4976685194073602

Epoch: 6| Step: 9
Training loss: 2.372618434832008
Validation loss: 2.4374801376202426

Epoch: 6| Step: 10
Training loss: 2.6073650911127255
Validation loss: 2.455218597230133

Epoch: 6| Step: 11
Training loss: 2.0208174190353416
Validation loss: 2.4500730869022216

Epoch: 6| Step: 12
Training loss: 2.950250750730775
Validation loss: 2.490533146493748

Epoch: 6| Step: 13
Training loss: 2.846079113587811
Validation loss: 2.4685133206203465

Epoch: 202| Step: 0
Training loss: 2.41815707408529
Validation loss: 2.4603755582149796

Epoch: 6| Step: 1
Training loss: 2.812376061993614
Validation loss: 2.433977041750006

Epoch: 6| Step: 2
Training loss: 2.109729200224239
Validation loss: 2.4460689534813747

Epoch: 6| Step: 3
Training loss: 3.2413161244324593
Validation loss: 2.427462131079869

Epoch: 6| Step: 4
Training loss: 2.9988283412506433
Validation loss: 2.463151673298634

Epoch: 6| Step: 5
Training loss: 2.968119664784832
Validation loss: 2.4558618381136967

Epoch: 6| Step: 6
Training loss: 2.5360458529505774
Validation loss: 2.45675894572349

Epoch: 6| Step: 7
Training loss: 2.0586770201893767
Validation loss: 2.480041900228543

Epoch: 6| Step: 8
Training loss: 2.5361900631594048
Validation loss: 2.454451689808021

Epoch: 6| Step: 9
Training loss: 2.75159416808708
Validation loss: 2.4623716364326578

Epoch: 6| Step: 10
Training loss: 2.2240945623932094
Validation loss: 2.4692857305631404

Epoch: 6| Step: 11
Training loss: 3.038451459398054
Validation loss: 2.4207657257422373

Epoch: 6| Step: 12
Training loss: 1.9932323632880433
Validation loss: 2.481032980160859

Epoch: 6| Step: 13
Training loss: 2.6483716928413963
Validation loss: 2.446851152763707

Epoch: 203| Step: 0
Training loss: 2.5070237675237457
Validation loss: 2.479322700695122

Epoch: 6| Step: 1
Training loss: 2.730524632833004
Validation loss: 2.4745892614072367

Epoch: 6| Step: 2
Training loss: 2.7693498603163573
Validation loss: 2.435522473656903

Epoch: 6| Step: 3
Training loss: 2.5533003000989427
Validation loss: 2.443001156527104

Epoch: 6| Step: 4
Training loss: 2.5210164741846395
Validation loss: 2.450828000934369

Epoch: 6| Step: 5
Training loss: 2.4767506043631804
Validation loss: 2.473177260892206

Epoch: 6| Step: 6
Training loss: 2.6687462764240526
Validation loss: 2.4579270713522

Epoch: 6| Step: 7
Training loss: 2.4033464029656035
Validation loss: 2.443634365285315

Epoch: 6| Step: 8
Training loss: 3.186838717212009
Validation loss: 2.486679198391221

Epoch: 6| Step: 9
Training loss: 2.3185104410883706
Validation loss: 2.45443098382854

Epoch: 6| Step: 10
Training loss: 2.6765690667049027
Validation loss: 2.4614052542357268

Epoch: 6| Step: 11
Training loss: 2.5778179563847727
Validation loss: 2.434900547400524

Epoch: 6| Step: 12
Training loss: 2.267478868413369
Validation loss: 2.456733365626591

Epoch: 6| Step: 13
Training loss: 2.8352394611070943
Validation loss: 2.4659926672104744

Epoch: 204| Step: 0
Training loss: 2.845805672268647
Validation loss: 2.444522659208585

Epoch: 6| Step: 1
Training loss: 2.8527871206548037
Validation loss: 2.45818106748269

Epoch: 6| Step: 2
Training loss: 2.9542873456559917
Validation loss: 2.4415212895023606

Epoch: 6| Step: 3
Training loss: 3.1053225692796143
Validation loss: 2.4495963933821097

Epoch: 6| Step: 4
Training loss: 2.3315648006994873
Validation loss: 2.4344625518647933

Epoch: 6| Step: 5
Training loss: 2.6290530749013405
Validation loss: 2.4705525997141753

Epoch: 6| Step: 6
Training loss: 2.7643120014240505
Validation loss: 2.454083080746187

Epoch: 6| Step: 7
Training loss: 3.1241307384765045
Validation loss: 2.4655094303835643

Epoch: 6| Step: 8
Training loss: 2.5718831652005294
Validation loss: 2.478812696984628

Epoch: 6| Step: 9
Training loss: 1.7990388529358
Validation loss: 2.4816951392608053

Epoch: 6| Step: 10
Training loss: 1.953816894049914
Validation loss: 2.468429719630676

Epoch: 6| Step: 11
Training loss: 2.6022513912306056
Validation loss: 2.479900635790554

Epoch: 6| Step: 12
Training loss: 2.4888864496858694
Validation loss: 2.4457311889374362

Epoch: 6| Step: 13
Training loss: 1.8530826685993282
Validation loss: 2.486125469443193

Epoch: 205| Step: 0
Training loss: 2.630548879401671
Validation loss: 2.451842925649059

Epoch: 6| Step: 1
Training loss: 2.3865864868477114
Validation loss: 2.467890384041992

Epoch: 6| Step: 2
Training loss: 2.3817738755239586
Validation loss: 2.4582786571358897

Epoch: 6| Step: 3
Training loss: 2.4065240480991643
Validation loss: 2.445454972020744

Epoch: 6| Step: 4
Training loss: 2.0714581468660014
Validation loss: 2.4793322962750173

Epoch: 6| Step: 5
Training loss: 2.4149619590337905
Validation loss: 2.4460853042975783

Epoch: 6| Step: 6
Training loss: 2.6094583281043753
Validation loss: 2.4673936824545657

Epoch: 6| Step: 7
Training loss: 3.4048047280579916
Validation loss: 2.417340216764092

Epoch: 6| Step: 8
Training loss: 2.5907249735096736
Validation loss: 2.4521946132582686

Epoch: 6| Step: 9
Training loss: 2.8134569871162545
Validation loss: 2.4544506380103086

Epoch: 6| Step: 10
Training loss: 2.3329794024928843
Validation loss: 2.441655702771768

Epoch: 6| Step: 11
Training loss: 2.4374098149757533
Validation loss: 2.442042637992254

Epoch: 6| Step: 12
Training loss: 2.6332201543058416
Validation loss: 2.4159653298014887

Epoch: 6| Step: 13
Training loss: 3.6263606707658766
Validation loss: 2.43150955494675

Epoch: 206| Step: 0
Training loss: 2.8122819604067266
Validation loss: 2.447246662120734

Epoch: 6| Step: 1
Training loss: 2.3236415627199287
Validation loss: 2.4640884210672427

Epoch: 6| Step: 2
Training loss: 1.8542775395994298
Validation loss: 2.4638503855350464

Epoch: 6| Step: 3
Training loss: 2.5190139594332988
Validation loss: 2.4526937657103334

Epoch: 6| Step: 4
Training loss: 2.7259732459498345
Validation loss: 2.420640820577881

Epoch: 6| Step: 5
Training loss: 2.5081573914661357
Validation loss: 2.467540320504094

Epoch: 6| Step: 6
Training loss: 2.6725651256879974
Validation loss: 2.453988175186626

Epoch: 6| Step: 7
Training loss: 2.8202485053497486
Validation loss: 2.420997924981629

Epoch: 6| Step: 8
Training loss: 2.5550303564489187
Validation loss: 2.490223997843654

Epoch: 6| Step: 9
Training loss: 2.557120841251961
Validation loss: 2.4552797983296077

Epoch: 6| Step: 10
Training loss: 2.582010566293857
Validation loss: 2.435342304349181

Epoch: 6| Step: 11
Training loss: 2.770795912716994
Validation loss: 2.4561516727565054

Epoch: 6| Step: 12
Training loss: 3.2461347336368127
Validation loss: 2.4448714298986873

Epoch: 6| Step: 13
Training loss: 2.3352011856549173
Validation loss: 2.4480845927648467

Epoch: 207| Step: 0
Training loss: 3.2666793654318274
Validation loss: 2.463603794050154

Epoch: 6| Step: 1
Training loss: 3.189856163297853
Validation loss: 2.4642239535299293

Epoch: 6| Step: 2
Training loss: 3.0551370353369585
Validation loss: 2.4662066830484695

Epoch: 6| Step: 3
Training loss: 2.8815180655870765
Validation loss: 2.472951020549886

Epoch: 6| Step: 4
Training loss: 2.3333008854744306
Validation loss: 2.463546585036126

Epoch: 6| Step: 5
Training loss: 2.306158908606075
Validation loss: 2.4482551593768105

Epoch: 6| Step: 6
Training loss: 2.1057871505248174
Validation loss: 2.4853803061183792

Epoch: 6| Step: 7
Training loss: 2.4879573689086465
Validation loss: 2.478418286037515

Epoch: 6| Step: 8
Training loss: 2.4188391577875406
Validation loss: 2.4323301261982313

Epoch: 6| Step: 9
Training loss: 2.7868826759918783
Validation loss: 2.4475599665711867

Epoch: 6| Step: 10
Training loss: 2.055364112988959
Validation loss: 2.4696153188894376

Epoch: 6| Step: 11
Training loss: 2.3567399221398846
Validation loss: 2.49966961410581

Epoch: 6| Step: 12
Training loss: 2.6343397470742334
Validation loss: 2.47420473090412

Epoch: 6| Step: 13
Training loss: 2.3355170544553108
Validation loss: 2.4327515500262624

Epoch: 208| Step: 0
Training loss: 2.3816269223327518
Validation loss: 2.452579207402198

Epoch: 6| Step: 1
Training loss: 2.3232805682648303
Validation loss: 2.445809987401825

Epoch: 6| Step: 2
Training loss: 2.7872482083446846
Validation loss: 2.44591239577385

Epoch: 6| Step: 3
Training loss: 2.5524238099805476
Validation loss: 2.4442491617285595

Epoch: 6| Step: 4
Training loss: 2.7748650973409794
Validation loss: 2.465270677778053

Epoch: 6| Step: 5
Training loss: 1.9827916594387798
Validation loss: 2.4479120914693127

Epoch: 6| Step: 6
Training loss: 3.300095707054602
Validation loss: 2.447327394758966

Epoch: 6| Step: 7
Training loss: 2.6446137408374626
Validation loss: 2.4527087250637583

Epoch: 6| Step: 8
Training loss: 3.495307501498158
Validation loss: 2.4496627389228123

Epoch: 6| Step: 9
Training loss: 2.0890297133184683
Validation loss: 2.43909768204277

Epoch: 6| Step: 10
Training loss: 2.141355292497837
Validation loss: 2.45454571368894

Epoch: 6| Step: 11
Training loss: 2.26759326545333
Validation loss: 2.4748784152325674

Epoch: 6| Step: 12
Training loss: 3.106649459851247
Validation loss: 2.4529519313659534

Epoch: 6| Step: 13
Training loss: 2.1154726410270785
Validation loss: 2.491759239872606

Epoch: 209| Step: 0
Training loss: 2.3169736119110715
Validation loss: 2.4373640785572737

Epoch: 6| Step: 1
Training loss: 2.669083950044519
Validation loss: 2.429382103495355

Epoch: 6| Step: 2
Training loss: 2.3177002867369683
Validation loss: 2.427996108938366

Epoch: 6| Step: 3
Training loss: 2.34818519561834
Validation loss: 2.4620805702036184

Epoch: 6| Step: 4
Training loss: 2.173391347980628
Validation loss: 2.474515711501032

Epoch: 6| Step: 5
Training loss: 2.183883838799218
Validation loss: 2.4649664121875174

Epoch: 6| Step: 6
Training loss: 2.65070743115099
Validation loss: 2.439208209121327

Epoch: 6| Step: 7
Training loss: 2.6236994109926473
Validation loss: 2.4632869289487243

Epoch: 6| Step: 8
Training loss: 3.797874036679775
Validation loss: 2.465593725657763

Epoch: 6| Step: 9
Training loss: 1.9193399969826435
Validation loss: 2.45779858490126

Epoch: 6| Step: 10
Training loss: 3.001854164611956
Validation loss: 2.4552549457310264

Epoch: 6| Step: 11
Training loss: 2.6500247486326383
Validation loss: 2.475379496449914

Epoch: 6| Step: 12
Training loss: 2.54007735877993
Validation loss: 2.4486599263548614

Epoch: 6| Step: 13
Training loss: 3.242262800330388
Validation loss: 2.4603976239415344

Epoch: 210| Step: 0
Training loss: 2.642714717517961
Validation loss: 2.4876716129548253

Epoch: 6| Step: 1
Training loss: 2.382973987547827
Validation loss: 2.45421544075346

Epoch: 6| Step: 2
Training loss: 3.230216333567383
Validation loss: 2.463358709805265

Epoch: 6| Step: 3
Training loss: 2.2956932297435415
Validation loss: 2.4396521210129256

Epoch: 6| Step: 4
Training loss: 1.938206205564332
Validation loss: 2.4586555251747124

Epoch: 6| Step: 5
Training loss: 2.4764050936089976
Validation loss: 2.4534179259707223

Epoch: 6| Step: 6
Training loss: 2.733320021790756
Validation loss: 2.43553213443702

Epoch: 6| Step: 7
Training loss: 2.4878900960094232
Validation loss: 2.4811031595769086

Epoch: 6| Step: 8
Training loss: 2.1008549176198947
Validation loss: 2.478449836671798

Epoch: 6| Step: 9
Training loss: 2.9286663259356525
Validation loss: 2.4618236950818706

Epoch: 6| Step: 10
Training loss: 2.421533080545166
Validation loss: 2.448806832990039

Epoch: 6| Step: 11
Training loss: 2.0683349377392557
Validation loss: 2.4600179245581972

Epoch: 6| Step: 12
Training loss: 3.1848288919188077
Validation loss: 2.4762191288571125

Epoch: 6| Step: 13
Training loss: 3.8119142426314196
Validation loss: 2.436733845889722

Epoch: 211| Step: 0
Training loss: 2.87014626160206
Validation loss: 2.4279661295254344

Epoch: 6| Step: 1
Training loss: 2.522948037500328
Validation loss: 2.479794945286076

Epoch: 6| Step: 2
Training loss: 2.5988357504843598
Validation loss: 2.446609138094532

Epoch: 6| Step: 3
Training loss: 2.1104827232855676
Validation loss: 2.473898233526728

Epoch: 6| Step: 4
Training loss: 2.0363167585900523
Validation loss: 2.4485393862293052

Epoch: 6| Step: 5
Training loss: 2.5674720536803206
Validation loss: 2.477376567859806

Epoch: 6| Step: 6
Training loss: 2.682307972287489
Validation loss: 2.4487410023775693

Epoch: 6| Step: 7
Training loss: 2.5871110080101576
Validation loss: 2.4824907346184366

Epoch: 6| Step: 8
Training loss: 2.224146660030747
Validation loss: 2.4252935918771765

Epoch: 6| Step: 9
Training loss: 2.8344835489804696
Validation loss: 2.4728030736101525

Epoch: 6| Step: 10
Training loss: 2.8889151800214554
Validation loss: 2.448068792520506

Epoch: 6| Step: 11
Training loss: 2.7123036748620923
Validation loss: 2.471527090101743

Epoch: 6| Step: 12
Training loss: 2.9826041355163477
Validation loss: 2.4492893165978353

Epoch: 6| Step: 13
Training loss: 2.5494001543207347
Validation loss: 2.4740150983134526

Epoch: 212| Step: 0
Training loss: 2.3837147489760775
Validation loss: 2.452800688799193

Epoch: 6| Step: 1
Training loss: 2.9471093320263826
Validation loss: 2.471048678782365

Epoch: 6| Step: 2
Training loss: 2.196690679987167
Validation loss: 2.441195100706221

Epoch: 6| Step: 3
Training loss: 2.7588706431521888
Validation loss: 2.4491718293825993

Epoch: 6| Step: 4
Training loss: 2.290358349110481
Validation loss: 2.4447848126000835

Epoch: 6| Step: 5
Training loss: 2.7442577404946222
Validation loss: 2.474246430820245

Epoch: 6| Step: 6
Training loss: 2.594890205985954
Validation loss: 2.484917095321873

Epoch: 6| Step: 7
Training loss: 2.6580396121240106
Validation loss: 2.448672522257145

Epoch: 6| Step: 8
Training loss: 2.613330424349327
Validation loss: 2.4618590926800663

Epoch: 6| Step: 9
Training loss: 2.629437011828963
Validation loss: 2.4620323380041986

Epoch: 6| Step: 10
Training loss: 2.547330944228215
Validation loss: 2.4362582158612605

Epoch: 6| Step: 11
Training loss: 2.7720611165728095
Validation loss: 2.47103859558393

Epoch: 6| Step: 12
Training loss: 2.586108521859575
Validation loss: 2.4505066675677702

Epoch: 6| Step: 13
Training loss: 2.5141742385111296
Validation loss: 2.4519536749176405

Epoch: 213| Step: 0
Training loss: 1.891827413756324
Validation loss: 2.477578300535185

Epoch: 6| Step: 1
Training loss: 2.1990075560505433
Validation loss: 2.468790486238594

Epoch: 6| Step: 2
Training loss: 2.5098768632631954
Validation loss: 2.438554884186375

Epoch: 6| Step: 3
Training loss: 2.9481676262205516
Validation loss: 2.4638692757718847

Epoch: 6| Step: 4
Training loss: 2.394387668704555
Validation loss: 2.4574033320885538

Epoch: 6| Step: 5
Training loss: 2.822593970816663
Validation loss: 2.437191739927079

Epoch: 6| Step: 6
Training loss: 2.3377482562906637
Validation loss: 2.4574878918517404

Epoch: 6| Step: 7
Training loss: 2.098201744714611
Validation loss: 2.4599262509595072

Epoch: 6| Step: 8
Training loss: 2.472351826855477
Validation loss: 2.451037586133576

Epoch: 6| Step: 9
Training loss: 3.592936813567567
Validation loss: 2.480123701926221

Epoch: 6| Step: 10
Training loss: 3.388236756106292
Validation loss: 2.465501840860049

Epoch: 6| Step: 11
Training loss: 2.468376807483149
Validation loss: 2.459322016400494

Epoch: 6| Step: 12
Training loss: 2.4257820362820786
Validation loss: 2.471237953915171

Epoch: 6| Step: 13
Training loss: 2.438340066764471
Validation loss: 2.477843735024845

Epoch: 214| Step: 0
Training loss: 2.9218462774323553
Validation loss: 2.4731963587277925

Epoch: 6| Step: 1
Training loss: 2.6210909301924565
Validation loss: 2.42475660933906

Epoch: 6| Step: 2
Training loss: 2.693803890246817
Validation loss: 2.4574331109521186

Epoch: 6| Step: 3
Training loss: 3.1425335147742643
Validation loss: 2.444698967187178

Epoch: 6| Step: 4
Training loss: 2.5238675438997173
Validation loss: 2.463897016220166

Epoch: 6| Step: 5
Training loss: 2.6757391042244256
Validation loss: 2.4485558859747787

Epoch: 6| Step: 6
Training loss: 2.5918310949370555
Validation loss: 2.439861138460845

Epoch: 6| Step: 7
Training loss: 2.3750572197696997
Validation loss: 2.47791881770388

Epoch: 6| Step: 8
Training loss: 2.7490656739373036
Validation loss: 2.4386184541170848

Epoch: 6| Step: 9
Training loss: 2.156259287938556
Validation loss: 2.4405233849753616

Epoch: 6| Step: 10
Training loss: 2.2524407182316586
Validation loss: 2.4385156390270244

Epoch: 6| Step: 11
Training loss: 2.719409117330126
Validation loss: 2.462306903534777

Epoch: 6| Step: 12
Training loss: 2.176327429658783
Validation loss: 2.4407955164784787

Epoch: 6| Step: 13
Training loss: 2.7280131418689084
Validation loss: 2.4562330541438695

Epoch: 215| Step: 0
Training loss: 2.8198704254195803
Validation loss: 2.487474510592834

Epoch: 6| Step: 1
Training loss: 2.3732648835687447
Validation loss: 2.475990290121299

Epoch: 6| Step: 2
Training loss: 2.343488958444663
Validation loss: 2.4506006105697744

Epoch: 6| Step: 3
Training loss: 3.158718843174335
Validation loss: 2.4673127954308987

Epoch: 6| Step: 4
Training loss: 2.6828127061758256
Validation loss: 2.4546288825652725

Epoch: 6| Step: 5
Training loss: 2.655941395503692
Validation loss: 2.461058688366048

Epoch: 6| Step: 6
Training loss: 2.6482833772448533
Validation loss: 2.44946527101515

Epoch: 6| Step: 7
Training loss: 2.564841526482531
Validation loss: 2.4709445093397093

Epoch: 6| Step: 8
Training loss: 2.476248640034055
Validation loss: 2.475390613152153

Epoch: 6| Step: 9
Training loss: 2.5118026600729233
Validation loss: 2.4619280796701686

Epoch: 6| Step: 10
Training loss: 1.9727443562084754
Validation loss: 2.4732805931009154

Epoch: 6| Step: 11
Training loss: 2.8435184258712027
Validation loss: 2.48160879175151

Epoch: 6| Step: 12
Training loss: 2.388246237885331
Validation loss: 2.4577170968017334

Epoch: 6| Step: 13
Training loss: 2.4524173566326923
Validation loss: 2.435500498870897

Epoch: 216| Step: 0
Training loss: 2.702034967519525
Validation loss: 2.4590958972814096

Epoch: 6| Step: 1
Training loss: 2.792797760964564
Validation loss: 2.420195278537257

Epoch: 6| Step: 2
Training loss: 2.424874005534825
Validation loss: 2.453775575001043

Epoch: 6| Step: 3
Training loss: 2.424045894163161
Validation loss: 2.4497616746350848

Epoch: 6| Step: 4
Training loss: 2.6884807305350455
Validation loss: 2.458252703340493

Epoch: 6| Step: 5
Training loss: 2.417089249945005
Validation loss: 2.472162032430695

Epoch: 6| Step: 6
Training loss: 2.124990575432915
Validation loss: 2.437856929404416

Epoch: 6| Step: 7
Training loss: 2.8082198741719524
Validation loss: 2.4182023320198396

Epoch: 6| Step: 8
Training loss: 2.332533994633434
Validation loss: 2.4612828124582005

Epoch: 6| Step: 9
Training loss: 2.4348991407620137
Validation loss: 2.459051348191613

Epoch: 6| Step: 10
Training loss: 3.186426617773286
Validation loss: 2.4702450602401136

Epoch: 6| Step: 11
Training loss: 2.124748551694639
Validation loss: 2.4502236618486504

Epoch: 6| Step: 12
Training loss: 2.973518316761265
Validation loss: 2.4622835961514444

Epoch: 6| Step: 13
Training loss: 2.763859503210159
Validation loss: 2.4465635411660904

Epoch: 217| Step: 0
Training loss: 2.33249862818049
Validation loss: 2.450005494984887

Epoch: 6| Step: 1
Training loss: 2.207297628075718
Validation loss: 2.449683779277801

Epoch: 6| Step: 2
Training loss: 2.413393983588161
Validation loss: 2.4638840299293907

Epoch: 6| Step: 3
Training loss: 2.1810386052481987
Validation loss: 2.481674907482246

Epoch: 6| Step: 4
Training loss: 2.710532826058625
Validation loss: 2.4768263691562744

Epoch: 6| Step: 5
Training loss: 3.04530834121381
Validation loss: 2.4425436509647587

Epoch: 6| Step: 6
Training loss: 3.320807494123214
Validation loss: 2.4978025910697146

Epoch: 6| Step: 7
Training loss: 2.138951559373411
Validation loss: 2.4605608012289153

Epoch: 6| Step: 8
Training loss: 2.354161445364324
Validation loss: 2.4368512962957607

Epoch: 6| Step: 9
Training loss: 2.688160305079603
Validation loss: 2.4527982555976746

Epoch: 6| Step: 10
Training loss: 2.7838015050281464
Validation loss: 2.4504348188634295

Epoch: 6| Step: 11
Training loss: 2.7694994839889726
Validation loss: 2.43123062353408

Epoch: 6| Step: 12
Training loss: 2.6157907886899734
Validation loss: 2.457198494429764

Epoch: 6| Step: 13
Training loss: 2.153744444305817
Validation loss: 2.4494281264607487

Epoch: 218| Step: 0
Training loss: 2.7952370562535616
Validation loss: 2.4736895871954605

Epoch: 6| Step: 1
Training loss: 1.7518841273777375
Validation loss: 2.450479829037646

Epoch: 6| Step: 2
Training loss: 2.850598650515502
Validation loss: 2.464468074805883

Epoch: 6| Step: 3
Training loss: 2.6406129012874646
Validation loss: 2.453010993786703

Epoch: 6| Step: 4
Training loss: 2.955923701983155
Validation loss: 2.4898828551206265

Epoch: 6| Step: 5
Training loss: 2.9531094989672537
Validation loss: 2.459432950067884

Epoch: 6| Step: 6
Training loss: 2.6113522285256314
Validation loss: 2.4683951240539264

Epoch: 6| Step: 7
Training loss: 2.535940181333543
Validation loss: 2.4969355249133818

Epoch: 6| Step: 8
Training loss: 2.5098577697700515
Validation loss: 2.458945301597143

Epoch: 6| Step: 9
Training loss: 3.235481257304642
Validation loss: 2.4678182124663532

Epoch: 6| Step: 10
Training loss: 2.3008252902260526
Validation loss: 2.476597899143879

Epoch: 6| Step: 11
Training loss: 2.2659145827864684
Validation loss: 2.4679437361522263

Epoch: 6| Step: 12
Training loss: 2.39642398574712
Validation loss: 2.4793263476402614

Epoch: 6| Step: 13
Training loss: 2.4270498794629507
Validation loss: 2.470774720491479

Epoch: 219| Step: 0
Training loss: 2.839100988501923
Validation loss: 2.4329665605758497

Epoch: 6| Step: 1
Training loss: 1.9463210957403712
Validation loss: 2.4496402368977708

Epoch: 6| Step: 2
Training loss: 1.9787270375580377
Validation loss: 2.4736218931035494

Epoch: 6| Step: 3
Training loss: 3.150793211838811
Validation loss: 2.4547697977211

Epoch: 6| Step: 4
Training loss: 2.9538831595401405
Validation loss: 2.4713259708459265

Epoch: 6| Step: 5
Training loss: 2.233034812372302
Validation loss: 2.4405502699695503

Epoch: 6| Step: 6
Training loss: 2.4111014665780885
Validation loss: 2.4470373525383073

Epoch: 6| Step: 7
Training loss: 2.6453938056541615
Validation loss: 2.485322803076509

Epoch: 6| Step: 8
Training loss: 2.558831729450202
Validation loss: 2.468022976865792

Epoch: 6| Step: 9
Training loss: 3.4004770056518243
Validation loss: 2.47133313895378

Epoch: 6| Step: 10
Training loss: 2.937966005412997
Validation loss: 2.443611687099194

Epoch: 6| Step: 11
Training loss: 2.3815906831630214
Validation loss: 2.4478815966842875

Epoch: 6| Step: 12
Training loss: 2.131355542564877
Validation loss: 2.4606925794298813

Epoch: 6| Step: 13
Training loss: 2.3773141929952795
Validation loss: 2.4625785058471297

Epoch: 220| Step: 0
Training loss: 2.707955250881666
Validation loss: 2.4436501805636017

Epoch: 6| Step: 1
Training loss: 2.743740587500761
Validation loss: 2.489129751677177

Epoch: 6| Step: 2
Training loss: 2.763095712081566
Validation loss: 2.473726740015795

Epoch: 6| Step: 3
Training loss: 2.5473315993962613
Validation loss: 2.471289788700247

Epoch: 6| Step: 4
Training loss: 2.6815776775887703
Validation loss: 2.4493493894464216

Epoch: 6| Step: 5
Training loss: 2.3280313972243594
Validation loss: 2.460064282430709

Epoch: 6| Step: 6
Training loss: 2.1753484819480806
Validation loss: 2.447204486943644

Epoch: 6| Step: 7
Training loss: 2.582675132167782
Validation loss: 2.4575580238501957

Epoch: 6| Step: 8
Training loss: 1.9272097846806147
Validation loss: 2.4924027585952615

Epoch: 6| Step: 9
Training loss: 3.313210321237012
Validation loss: 2.4481375837862505

Epoch: 6| Step: 10
Training loss: 2.5216752740175634
Validation loss: 2.441055032910938

Epoch: 6| Step: 11
Training loss: 2.3379592566745813
Validation loss: 2.4398537055700165

Epoch: 6| Step: 12
Training loss: 2.587272921563223
Validation loss: 2.456221462988785

Epoch: 6| Step: 13
Training loss: 2.8884260732029974
Validation loss: 2.4665721541237726

Epoch: 221| Step: 0
Training loss: 2.5432105352305316
Validation loss: 2.4487387054297964

Epoch: 6| Step: 1
Training loss: 3.1687116210539488
Validation loss: 2.4550275487481676

Epoch: 6| Step: 2
Training loss: 2.040159902102642
Validation loss: 2.4196024817877233

Epoch: 6| Step: 3
Training loss: 2.5728445828078956
Validation loss: 2.4595199234129463

Epoch: 6| Step: 4
Training loss: 1.5882134854205232
Validation loss: 2.484224326368893

Epoch: 6| Step: 5
Training loss: 2.835498281888352
Validation loss: 2.478195259325072

Epoch: 6| Step: 6
Training loss: 2.297512874131354
Validation loss: 2.4324128932157945

Epoch: 6| Step: 7
Training loss: 3.0324490722002704
Validation loss: 2.457805954135683

Epoch: 6| Step: 8
Training loss: 2.8727660414666394
Validation loss: 2.4491309418942757

Epoch: 6| Step: 9
Training loss: 2.6089380320993585
Validation loss: 2.4682613360103414

Epoch: 6| Step: 10
Training loss: 2.4470197184165343
Validation loss: 2.4781775303627542

Epoch: 6| Step: 11
Training loss: 2.5209807255638728
Validation loss: 2.464945438840075

Epoch: 6| Step: 12
Training loss: 2.93586523140665
Validation loss: 2.4845622226831576

Epoch: 6| Step: 13
Training loss: 2.816415900479739
Validation loss: 2.444669578513308

Epoch: 222| Step: 0
Training loss: 2.6843747442065506
Validation loss: 2.4640047162449914

Epoch: 6| Step: 1
Training loss: 2.6442751964758253
Validation loss: 2.4746381983273946

Epoch: 6| Step: 2
Training loss: 2.349148851832077
Validation loss: 2.44879064167735

Epoch: 6| Step: 3
Training loss: 3.493565912463797
Validation loss: 2.4768859785559716

Epoch: 6| Step: 4
Training loss: 3.015398876824182
Validation loss: 2.4667227144478217

Epoch: 6| Step: 5
Training loss: 3.0159725324963524
Validation loss: 2.4438209297430196

Epoch: 6| Step: 6
Training loss: 2.830937625587293
Validation loss: 2.449705737241189

Epoch: 6| Step: 7
Training loss: 1.7153265410307454
Validation loss: 2.4693664086543152

Epoch: 6| Step: 8
Training loss: 3.0757615363720068
Validation loss: 2.4754327099339415

Epoch: 6| Step: 9
Training loss: 2.5252240827480676
Validation loss: 2.482909804678802

Epoch: 6| Step: 10
Training loss: 1.6321549985433428
Validation loss: 2.450566415562338

Epoch: 6| Step: 11
Training loss: 2.1733303545519926
Validation loss: 2.4523546325893526

Epoch: 6| Step: 12
Training loss: 2.466190508256886
Validation loss: 2.470627782566949

Epoch: 6| Step: 13
Training loss: 1.9095348135188652
Validation loss: 2.4559019313731247

Epoch: 223| Step: 0
Training loss: 2.6490407611134503
Validation loss: 2.46332056959086

Epoch: 6| Step: 1
Training loss: 2.8090631466778966
Validation loss: 2.4505080181697463

Epoch: 6| Step: 2
Training loss: 2.6647728115672575
Validation loss: 2.448542185927435

Epoch: 6| Step: 3
Training loss: 2.8183559912055225
Validation loss: 2.4445380723230823

Epoch: 6| Step: 4
Training loss: 2.1694609034792154
Validation loss: 2.446143058978681

Epoch: 6| Step: 5
Training loss: 2.998252359611189
Validation loss: 2.4387116976704273

Epoch: 6| Step: 6
Training loss: 2.982873029379463
Validation loss: 2.4669166986423177

Epoch: 6| Step: 7
Training loss: 1.7779830956443925
Validation loss: 2.4641491911943922

Epoch: 6| Step: 8
Training loss: 2.7091267719536702
Validation loss: 2.4708170141997394

Epoch: 6| Step: 9
Training loss: 2.5455111731076276
Validation loss: 2.44845550687755

Epoch: 6| Step: 10
Training loss: 3.147908888141776
Validation loss: 2.4505386591636062

Epoch: 6| Step: 11
Training loss: 2.511648031754987
Validation loss: 2.4603382949563786

Epoch: 6| Step: 12
Training loss: 1.7983088204543054
Validation loss: 2.4560629648547696

Epoch: 6| Step: 13
Training loss: 1.709694537157188
Validation loss: 2.4478014882014425

Epoch: 224| Step: 0
Training loss: 1.7520636924462265
Validation loss: 2.442987978881232

Epoch: 6| Step: 1
Training loss: 2.6713857816353914
Validation loss: 2.465460533228735

Epoch: 6| Step: 2
Training loss: 2.47462883680439
Validation loss: 2.4717346453154723

Epoch: 6| Step: 3
Training loss: 3.016837237121553
Validation loss: 2.476468310990061

Epoch: 6| Step: 4
Training loss: 2.1071544120416337
Validation loss: 2.481719374823956

Epoch: 6| Step: 5
Training loss: 2.377650037297251
Validation loss: 2.452138154263562

Epoch: 6| Step: 6
Training loss: 2.15086045013719
Validation loss: 2.438714788802732

Epoch: 6| Step: 7
Training loss: 2.748238866588632
Validation loss: 2.4533102862855314

Epoch: 6| Step: 8
Training loss: 2.86500070941377
Validation loss: 2.4402119674261447

Epoch: 6| Step: 9
Training loss: 2.750923175178628
Validation loss: 2.419416798444256

Epoch: 6| Step: 10
Training loss: 2.802868181284395
Validation loss: 2.4876501075906354

Epoch: 6| Step: 11
Training loss: 2.565567785408025
Validation loss: 2.4461706986182583

Epoch: 6| Step: 12
Training loss: 2.738760782392092
Validation loss: 2.4375598112371724

Epoch: 6| Step: 13
Training loss: 3.1825162035221677
Validation loss: 2.4535487054465333

Epoch: 225| Step: 0
Training loss: 1.9431075503258122
Validation loss: 2.4652588566603737

Epoch: 6| Step: 1
Training loss: 2.9753939520031123
Validation loss: 2.467429291063965

Epoch: 6| Step: 2
Training loss: 2.5614268218888476
Validation loss: 2.433979389492866

Epoch: 6| Step: 3
Training loss: 2.3554431414121497
Validation loss: 2.4526150018299733

Epoch: 6| Step: 4
Training loss: 2.407597152318378
Validation loss: 2.450324574288394

Epoch: 6| Step: 5
Training loss: 2.3863521113261883
Validation loss: 2.4436728389842646

Epoch: 6| Step: 6
Training loss: 2.3806951402029486
Validation loss: 2.4500767376241375

Epoch: 6| Step: 7
Training loss: 2.9835314132599127
Validation loss: 2.458249554910706

Epoch: 6| Step: 8
Training loss: 2.319063820385554
Validation loss: 2.456619162143693

Epoch: 6| Step: 9
Training loss: 2.9419768287892434
Validation loss: 2.465206414618297

Epoch: 6| Step: 10
Training loss: 2.565510168072883
Validation loss: 2.465548743928593

Epoch: 6| Step: 11
Training loss: 1.9925784456708597
Validation loss: 2.447800845144575

Epoch: 6| Step: 12
Training loss: 2.648958498141851
Validation loss: 2.459181054563524

Epoch: 6| Step: 13
Training loss: 3.5684654750407234
Validation loss: 2.4536924994441986

Epoch: 226| Step: 0
Training loss: 2.0569811880450417
Validation loss: 2.4767072785456943

Epoch: 6| Step: 1
Training loss: 2.2917556283040477
Validation loss: 2.4809736135144624

Epoch: 6| Step: 2
Training loss: 2.516906506644555
Validation loss: 2.4523707136055615

Epoch: 6| Step: 3
Training loss: 2.66603431555376
Validation loss: 2.462906299971956

Epoch: 6| Step: 4
Training loss: 2.763933947103772
Validation loss: 2.4496289776970332

Epoch: 6| Step: 5
Training loss: 1.833191721677723
Validation loss: 2.4756965534988007

Epoch: 6| Step: 6
Training loss: 2.400631177874906
Validation loss: 2.4684354364406076

Epoch: 6| Step: 7
Training loss: 3.080470397004537
Validation loss: 2.4705026494912343

Epoch: 6| Step: 8
Training loss: 2.8379801810219774
Validation loss: 2.470568320501525

Epoch: 6| Step: 9
Training loss: 2.2931943714194536
Validation loss: 2.4810350725837083

Epoch: 6| Step: 10
Training loss: 2.3835906212210087
Validation loss: 2.5032102113885384

Epoch: 6| Step: 11
Training loss: 2.7401470189434733
Validation loss: 2.488377571608085

Epoch: 6| Step: 12
Training loss: 2.9991978526598433
Validation loss: 2.4811229357011726

Epoch: 6| Step: 13
Training loss: 3.0827954484687448
Validation loss: 2.444154434427182

Epoch: 227| Step: 0
Training loss: 1.824559116671779
Validation loss: 2.4803565328980115

Epoch: 6| Step: 1
Training loss: 2.748224725776912
Validation loss: 2.474517222013529

Epoch: 6| Step: 2
Training loss: 2.3451110956680474
Validation loss: 2.4369603956752806

Epoch: 6| Step: 3
Training loss: 2.675690898622848
Validation loss: 2.490068479445421

Epoch: 6| Step: 4
Training loss: 3.3885108931502708
Validation loss: 2.4530709368280177

Epoch: 6| Step: 5
Training loss: 2.380525485778285
Validation loss: 2.4864248748910147

Epoch: 6| Step: 6
Training loss: 2.6494040736830224
Validation loss: 2.4681224656279968

Epoch: 6| Step: 7
Training loss: 2.1642462586975015
Validation loss: 2.452974051330356

Epoch: 6| Step: 8
Training loss: 2.254680639350377
Validation loss: 2.491721639322735

Epoch: 6| Step: 9
Training loss: 2.890888099038022
Validation loss: 2.492913619381361

Epoch: 6| Step: 10
Training loss: 1.8446399674454281
Validation loss: 2.441607051344037

Epoch: 6| Step: 11
Training loss: 2.690233570296498
Validation loss: 2.470693713806638

Epoch: 6| Step: 12
Training loss: 2.903083306578804
Validation loss: 2.461135467207792

Epoch: 6| Step: 13
Training loss: 3.0022874694361468
Validation loss: 2.4713246731153498

Epoch: 228| Step: 0
Training loss: 3.139596951123333
Validation loss: 2.4779408761903445

Epoch: 6| Step: 1
Training loss: 2.3709356767479726
Validation loss: 2.4682169793536377

Epoch: 6| Step: 2
Training loss: 1.614727784944963
Validation loss: 2.476577345197795

Epoch: 6| Step: 3
Training loss: 2.8380828393267277
Validation loss: 2.473172642933811

Epoch: 6| Step: 4
Training loss: 2.7048882703569745
Validation loss: 2.493445178186767

Epoch: 6| Step: 5
Training loss: 2.343622839974949
Validation loss: 2.4800388425166253

Epoch: 6| Step: 6
Training loss: 1.6838298021420655
Validation loss: 2.4600496580332694

Epoch: 6| Step: 7
Training loss: 2.5991660871522417
Validation loss: 2.4216570758456752

Epoch: 6| Step: 8
Training loss: 2.716289415279792
Validation loss: 2.4922013541005086

Epoch: 6| Step: 9
Training loss: 2.3381263913596837
Validation loss: 2.461667077095188

Epoch: 6| Step: 10
Training loss: 2.7061251710642074
Validation loss: 2.459929266452199

Epoch: 6| Step: 11
Training loss: 3.4216138487497467
Validation loss: 2.4856396365903914

Epoch: 6| Step: 12
Training loss: 2.4003948959672643
Validation loss: 2.450044107992299

Epoch: 6| Step: 13
Training loss: 2.8045206259385393
Validation loss: 2.4488365771554004

Epoch: 229| Step: 0
Training loss: 1.9352353304312035
Validation loss: 2.4482465728983973

Epoch: 6| Step: 1
Training loss: 2.611060049581306
Validation loss: 2.476254869378739

Epoch: 6| Step: 2
Training loss: 2.6394041952532414
Validation loss: 2.4819758083443713

Epoch: 6| Step: 3
Training loss: 2.3793692302709055
Validation loss: 2.458583501688098

Epoch: 6| Step: 4
Training loss: 1.9647420625556649
Validation loss: 2.4431615938460753

Epoch: 6| Step: 5
Training loss: 2.550405664526705
Validation loss: 2.4566919663882842

Epoch: 6| Step: 6
Training loss: 2.52704563180884
Validation loss: 2.4742020861366365

Epoch: 6| Step: 7
Training loss: 3.4720352698965016
Validation loss: 2.4221818171887395

Epoch: 6| Step: 8
Training loss: 2.802586695804431
Validation loss: 2.439889488101341

Epoch: 6| Step: 9
Training loss: 2.291574083249812
Validation loss: 2.436006814899682

Epoch: 6| Step: 10
Training loss: 2.307707485735937
Validation loss: 2.4415944126918916

Epoch: 6| Step: 11
Training loss: 2.4780763638162537
Validation loss: 2.430260508296905

Epoch: 6| Step: 12
Training loss: 2.107819838684292
Validation loss: 2.4403861601664363

Epoch: 6| Step: 13
Training loss: 3.9039756562615264
Validation loss: 2.451082628121737

Epoch: 230| Step: 0
Training loss: 1.8926164211811671
Validation loss: 2.4433012177081443

Epoch: 6| Step: 1
Training loss: 2.88597599121597
Validation loss: 2.4552151019026103

Epoch: 6| Step: 2
Training loss: 2.7201221555038733
Validation loss: 2.453537918179892

Epoch: 6| Step: 3
Training loss: 2.420590262977591
Validation loss: 2.465913104465135

Epoch: 6| Step: 4
Training loss: 2.3841583953504166
Validation loss: 2.453662471442433

Epoch: 6| Step: 5
Training loss: 2.8430282179820496
Validation loss: 2.4676559701227987

Epoch: 6| Step: 6
Training loss: 2.425219091369643
Validation loss: 2.4603564140024266

Epoch: 6| Step: 7
Training loss: 2.468298762163299
Validation loss: 2.4639006672689985

Epoch: 6| Step: 8
Training loss: 2.7803288338022654
Validation loss: 2.4404684963978993

Epoch: 6| Step: 9
Training loss: 3.145353335377461
Validation loss: 2.4649127377010265

Epoch: 6| Step: 10
Training loss: 2.697036236200972
Validation loss: 2.435503923546419

Epoch: 6| Step: 11
Training loss: 2.276085848055764
Validation loss: 2.4672979724189017

Epoch: 6| Step: 12
Training loss: 2.2576237899956983
Validation loss: 2.447028305009289

Epoch: 6| Step: 13
Training loss: 2.692428594012514
Validation loss: 2.45802973469323

Epoch: 231| Step: 0
Training loss: 2.9622696670290156
Validation loss: 2.452016339167645

Epoch: 6| Step: 1
Training loss: 2.7422289722925606
Validation loss: 2.46849383760289

Epoch: 6| Step: 2
Training loss: 2.774855903802076
Validation loss: 2.4678333689320215

Epoch: 6| Step: 3
Training loss: 2.324367896672037
Validation loss: 2.4669588475370605

Epoch: 6| Step: 4
Training loss: 2.341739046147949
Validation loss: 2.446185221015594

Epoch: 6| Step: 5
Training loss: 2.7217106976503036
Validation loss: 2.4584603302030112

Epoch: 6| Step: 6
Training loss: 2.978393632235343
Validation loss: 2.468852392928692

Epoch: 6| Step: 7
Training loss: 3.039142205534151
Validation loss: 2.4631042988903755

Epoch: 6| Step: 8
Training loss: 2.2113084246053214
Validation loss: 2.4699341122881417

Epoch: 6| Step: 9
Training loss: 2.7511354183200982
Validation loss: 2.4629001076465262

Epoch: 6| Step: 10
Training loss: 1.8652137954934738
Validation loss: 2.4910157844778196

Epoch: 6| Step: 11
Training loss: 1.9500331973649052
Validation loss: 2.515855643454964

Epoch: 6| Step: 12
Training loss: 2.244947376441582
Validation loss: 2.459965628719743

Epoch: 6| Step: 13
Training loss: 2.9063346604352476
Validation loss: 2.454816284465073

Epoch: 232| Step: 0
Training loss: 1.990154470649733
Validation loss: 2.45492157635339

Epoch: 6| Step: 1
Training loss: 1.8596939326287083
Validation loss: 2.4803714442753813

Epoch: 6| Step: 2
Training loss: 3.1336720493209422
Validation loss: 2.4702421585266188

Epoch: 6| Step: 3
Training loss: 2.0714145246508653
Validation loss: 2.4607341879651856

Epoch: 6| Step: 4
Training loss: 2.387552318698781
Validation loss: 2.476948544551615

Epoch: 6| Step: 5
Training loss: 2.48745967889819
Validation loss: 2.461453594542232

Epoch: 6| Step: 6
Training loss: 3.0426728835626067
Validation loss: 2.435633255600492

Epoch: 6| Step: 7
Training loss: 2.25635880395496
Validation loss: 2.453249501076844

Epoch: 6| Step: 8
Training loss: 3.0132326113766377
Validation loss: 2.467821241689279

Epoch: 6| Step: 9
Training loss: 2.4013589468710275
Validation loss: 2.476600764426603

Epoch: 6| Step: 10
Training loss: 2.668321175707399
Validation loss: 2.460640441185456

Epoch: 6| Step: 11
Training loss: 2.3972677055848375
Validation loss: 2.437763173610327

Epoch: 6| Step: 12
Training loss: 2.977148123575149
Validation loss: 2.450824644220462

Epoch: 6| Step: 13
Training loss: 3.109848676477423
Validation loss: 2.4403505677084882

Epoch: 233| Step: 0
Training loss: 2.652012845101757
Validation loss: 2.4637858236366434

Epoch: 6| Step: 1
Training loss: 2.3562669303144137
Validation loss: 2.4617896715870895

Epoch: 6| Step: 2
Training loss: 2.734103205660474
Validation loss: 2.4550025108519544

Epoch: 6| Step: 3
Training loss: 2.8750437857569286
Validation loss: 2.4989157160920286

Epoch: 6| Step: 4
Training loss: 2.451238210320023
Validation loss: 2.4738645584822305

Epoch: 6| Step: 5
Training loss: 2.1538937728459566
Validation loss: 2.4804329968143777

Epoch: 6| Step: 6
Training loss: 2.366682848628783
Validation loss: 2.469537541093763

Epoch: 6| Step: 7
Training loss: 2.112185943553276
Validation loss: 2.4427760375210985

Epoch: 6| Step: 8
Training loss: 2.144880311556643
Validation loss: 2.4636470027498922

Epoch: 6| Step: 9
Training loss: 2.823818743013793
Validation loss: 2.423441181156396

Epoch: 6| Step: 10
Training loss: 2.927348838701959
Validation loss: 2.459484335275269

Epoch: 6| Step: 11
Training loss: 2.963049625966042
Validation loss: 2.4620550813204694

Epoch: 6| Step: 12
Training loss: 2.5239036294884207
Validation loss: 2.468680071719803

Epoch: 6| Step: 13
Training loss: 2.523436366219753
Validation loss: 2.462733546050367

Epoch: 234| Step: 0
Training loss: 1.9195247018133375
Validation loss: 2.481603359937576

Epoch: 6| Step: 1
Training loss: 2.1460175913492345
Validation loss: 2.486374918746555

Epoch: 6| Step: 2
Training loss: 2.8192052807607997
Validation loss: 2.464122568879952

Epoch: 6| Step: 3
Training loss: 2.0981765187119783
Validation loss: 2.434873009355874

Epoch: 6| Step: 4
Training loss: 2.671955754359187
Validation loss: 2.497680705471667

Epoch: 6| Step: 5
Training loss: 2.929346659860812
Validation loss: 2.4591758833490176

Epoch: 6| Step: 6
Training loss: 2.729837194485861
Validation loss: 2.4564049366583673

Epoch: 6| Step: 7
Training loss: 2.7452239263811142
Validation loss: 2.474571360534776

Epoch: 6| Step: 8
Training loss: 1.7768433254127851
Validation loss: 2.4687355636106045

Epoch: 6| Step: 9
Training loss: 2.775255064368363
Validation loss: 2.4535162567789293

Epoch: 6| Step: 10
Training loss: 2.934937840961893
Validation loss: 2.4531262749618397

Epoch: 6| Step: 11
Training loss: 2.764085416553266
Validation loss: 2.4649728806496976

Epoch: 6| Step: 12
Training loss: 2.3344755669395942
Validation loss: 2.4788573966559713

Epoch: 6| Step: 13
Training loss: 2.5468596475524627
Validation loss: 2.4544273813629993

Epoch: 235| Step: 0
Training loss: 2.8830033732754083
Validation loss: 2.477617641420964

Epoch: 6| Step: 1
Training loss: 2.471485216198786
Validation loss: 2.4976017010489033

Epoch: 6| Step: 2
Training loss: 2.174397382892384
Validation loss: 2.4501492622886616

Epoch: 6| Step: 3
Training loss: 2.4956797463908327
Validation loss: 2.4691311183932108

Epoch: 6| Step: 4
Training loss: 2.787479068694623
Validation loss: 2.471757239202105

Epoch: 6| Step: 5
Training loss: 2.523920821917284
Validation loss: 2.4437232804398796

Epoch: 6| Step: 6
Training loss: 2.60639609972508
Validation loss: 2.470506294928696

Epoch: 6| Step: 7
Training loss: 2.4396397906339438
Validation loss: 2.4696885227025307

Epoch: 6| Step: 8
Training loss: 3.2082448839818483
Validation loss: 2.4797666931233264

Epoch: 6| Step: 9
Training loss: 1.9584985893269753
Validation loss: 2.4866445994858513

Epoch: 6| Step: 10
Training loss: 3.0024813880311787
Validation loss: 2.4903105942999124

Epoch: 6| Step: 11
Training loss: 2.6224340886214508
Validation loss: 2.4935334301078256

Epoch: 6| Step: 12
Training loss: 1.906937584614237
Validation loss: 2.453127179975333

Epoch: 6| Step: 13
Training loss: 2.078543025830712
Validation loss: 2.4631818838047006

Epoch: 236| Step: 0
Training loss: 2.6131181195580684
Validation loss: 2.4607835895038135

Epoch: 6| Step: 1
Training loss: 2.9111034925456574
Validation loss: 2.4684696310895906

Epoch: 6| Step: 2
Training loss: 2.3845582664123617
Validation loss: 2.471092693085388

Epoch: 6| Step: 3
Training loss: 3.07185010773827
Validation loss: 2.4649442219935533

Epoch: 6| Step: 4
Training loss: 3.035904290159489
Validation loss: 2.4999083912380105

Epoch: 6| Step: 5
Training loss: 2.738552803799481
Validation loss: 2.4790960356806573

Epoch: 6| Step: 6
Training loss: 1.951714822949826
Validation loss: 2.482162731986361

Epoch: 6| Step: 7
Training loss: 2.629467749749492
Validation loss: 2.486463093741581

Epoch: 6| Step: 8
Training loss: 2.6384888083585833
Validation loss: 2.483788671150318

Epoch: 6| Step: 9
Training loss: 2.8311586074730863
Validation loss: 2.471828335372798

Epoch: 6| Step: 10
Training loss: 1.9394335788998063
Validation loss: 2.486321883773797

Epoch: 6| Step: 11
Training loss: 2.132560854867484
Validation loss: 2.46553612610516

Epoch: 6| Step: 12
Training loss: 2.0457918052835153
Validation loss: 2.4335511933579532

Epoch: 6| Step: 13
Training loss: 2.490315849616926
Validation loss: 2.4636523347152215

Epoch: 237| Step: 0
Training loss: 2.199620482915147
Validation loss: 2.43688720286668

Epoch: 6| Step: 1
Training loss: 2.3432994155093194
Validation loss: 2.450291789971752

Epoch: 6| Step: 2
Training loss: 2.918506986315915
Validation loss: 2.4614833077419505

Epoch: 6| Step: 3
Training loss: 2.050552675133837
Validation loss: 2.4485169257079598

Epoch: 6| Step: 4
Training loss: 2.959959326541957
Validation loss: 2.462411247858197

Epoch: 6| Step: 5
Training loss: 2.3280544398323246
Validation loss: 2.4701355607432443

Epoch: 6| Step: 6
Training loss: 1.840338703162654
Validation loss: 2.473334930751532

Epoch: 6| Step: 7
Training loss: 2.3294996261317995
Validation loss: 2.431736434364904

Epoch: 6| Step: 8
Training loss: 2.363783198244031
Validation loss: 2.4739663324836636

Epoch: 6| Step: 9
Training loss: 2.824707157857063
Validation loss: 2.476494676882689

Epoch: 6| Step: 10
Training loss: 2.3362219391714825
Validation loss: 2.4381719966560595

Epoch: 6| Step: 11
Training loss: 3.191936322831906
Validation loss: 2.4678845636475724

Epoch: 6| Step: 12
Training loss: 2.8719960981039674
Validation loss: 2.490465350247816

Epoch: 6| Step: 13
Training loss: 3.476438095781553
Validation loss: 2.4688125629423583

Epoch: 238| Step: 0
Training loss: 2.2923290913646004
Validation loss: 2.4432958119517565

Epoch: 6| Step: 1
Training loss: 2.471548691152828
Validation loss: 2.4642264888469834

Epoch: 6| Step: 2
Training loss: 2.117058218637039
Validation loss: 2.467874276381101

Epoch: 6| Step: 3
Training loss: 2.4553020560290197
Validation loss: 2.491049024924122

Epoch: 6| Step: 4
Training loss: 1.945040948137661
Validation loss: 2.4664876989621964

Epoch: 6| Step: 5
Training loss: 2.58668945074698
Validation loss: 2.460001626746581

Epoch: 6| Step: 6
Training loss: 3.4493202203187523
Validation loss: 2.4328143105559534

Epoch: 6| Step: 7
Training loss: 3.1528053749284517
Validation loss: 2.449385528321192

Epoch: 6| Step: 8
Training loss: 2.7787608462802824
Validation loss: 2.43090524008707

Epoch: 6| Step: 9
Training loss: 2.614405095280336
Validation loss: 2.4602864534706246

Epoch: 6| Step: 10
Training loss: 2.9595058069856917
Validation loss: 2.452742124062295

Epoch: 6| Step: 11
Training loss: 1.269134976088849
Validation loss: 2.4396473397671814

Epoch: 6| Step: 12
Training loss: 2.3523961614054962
Validation loss: 2.458053633092475

Epoch: 6| Step: 13
Training loss: 2.7068484025774575
Validation loss: 2.4560694197108073

Epoch: 239| Step: 0
Training loss: 3.343650174656212
Validation loss: 2.467926462286256

Epoch: 6| Step: 1
Training loss: 1.8648443483336519
Validation loss: 2.461430158262004

Epoch: 6| Step: 2
Training loss: 2.0476764536781085
Validation loss: 2.4379311288901517

Epoch: 6| Step: 3
Training loss: 2.9286225277898685
Validation loss: 2.45957148399632

Epoch: 6| Step: 4
Training loss: 2.6735310023693164
Validation loss: 2.467664541009004

Epoch: 6| Step: 5
Training loss: 2.820336085508851
Validation loss: 2.4452546166667384

Epoch: 6| Step: 6
Training loss: 2.4782468441116032
Validation loss: 2.458919601931575

Epoch: 6| Step: 7
Training loss: 2.2295213815228876
Validation loss: 2.4741833887673623

Epoch: 6| Step: 8
Training loss: 2.3401436080229465
Validation loss: 2.4704857027541713

Epoch: 6| Step: 9
Training loss: 2.406943283281329
Validation loss: 2.4631887800307504

Epoch: 6| Step: 10
Training loss: 3.2304966474479375
Validation loss: 2.455698037488778

Epoch: 6| Step: 11
Training loss: 2.2715881796264488
Validation loss: 2.4616080317782982

Epoch: 6| Step: 12
Training loss: 2.396134255349525
Validation loss: 2.4690290734390135

Epoch: 6| Step: 13
Training loss: 2.351107733973862
Validation loss: 2.4396465211749376

Epoch: 240| Step: 0
Training loss: 2.5254318812187404
Validation loss: 2.4624356648867423

Epoch: 6| Step: 1
Training loss: 2.684916651904364
Validation loss: 2.474311963094498

Epoch: 6| Step: 2
Training loss: 1.694691346459769
Validation loss: 2.461396958392514

Epoch: 6| Step: 3
Training loss: 2.7996238115187815
Validation loss: 2.4755525878037217

Epoch: 6| Step: 4
Training loss: 3.1492022397274044
Validation loss: 2.4420235316747125

Epoch: 6| Step: 5
Training loss: 2.369560034753452
Validation loss: 2.48535658074341

Epoch: 6| Step: 6
Training loss: 2.7864044089164977
Validation loss: 2.461062398574258

Epoch: 6| Step: 7
Training loss: 3.3285621033660098
Validation loss: 2.4585758104940054

Epoch: 6| Step: 8
Training loss: 1.7275278146053064
Validation loss: 2.4563885287729006

Epoch: 6| Step: 9
Training loss: 2.705380066914355
Validation loss: 2.443758105696339

Epoch: 6| Step: 10
Training loss: 2.430135311550838
Validation loss: 2.473672437399256

Epoch: 6| Step: 11
Training loss: 2.4813901616632994
Validation loss: 2.4884657755724606

Epoch: 6| Step: 12
Training loss: 2.6189368934675255
Validation loss: 2.4401335886130977

Epoch: 6| Step: 13
Training loss: 1.5645539326282163
Validation loss: 2.4494606010263342

Epoch: 241| Step: 0
Training loss: 2.1777962303785507
Validation loss: 2.5097658844523894

Epoch: 6| Step: 1
Training loss: 3.086019703216844
Validation loss: 2.470187496498536

Epoch: 6| Step: 2
Training loss: 2.242617469526184
Validation loss: 2.448551050919058

Epoch: 6| Step: 3
Training loss: 2.898205853276946
Validation loss: 2.4475970892009076

Epoch: 6| Step: 4
Training loss: 2.1671393930256797
Validation loss: 2.4544596325896

Epoch: 6| Step: 5
Training loss: 2.033121508628458
Validation loss: 2.4781390678646718

Epoch: 6| Step: 6
Training loss: 1.5822768785975587
Validation loss: 2.5033172733290443

Epoch: 6| Step: 7
Training loss: 2.7513523244656044
Validation loss: 2.4560266580610954

Epoch: 6| Step: 8
Training loss: 2.8930564757872816
Validation loss: 2.429924628253853

Epoch: 6| Step: 9
Training loss: 2.165833777730597
Validation loss: 2.461396913606342

Epoch: 6| Step: 10
Training loss: 3.2499741773313082
Validation loss: 2.4605468127361316

Epoch: 6| Step: 11
Training loss: 2.4757787384858276
Validation loss: 2.4704761973385474

Epoch: 6| Step: 12
Training loss: 2.626052191437196
Validation loss: 2.4826457064377925

Epoch: 6| Step: 13
Training loss: 2.355307299899363
Validation loss: 2.44689569766911

Epoch: 242| Step: 0
Training loss: 3.17853407126038
Validation loss: 2.491302583094848

Epoch: 6| Step: 1
Training loss: 2.118464573902265
Validation loss: 2.4705533177879624

Epoch: 6| Step: 2
Training loss: 2.3567959666448757
Validation loss: 2.4975692211670766

Epoch: 6| Step: 3
Training loss: 2.3725788424126932
Validation loss: 2.430529369402652

Epoch: 6| Step: 4
Training loss: 2.7510415618985053
Validation loss: 2.438320373142848

Epoch: 6| Step: 5
Training loss: 2.8081061904314626
Validation loss: 2.4218685429249143

Epoch: 6| Step: 6
Training loss: 2.223088281138944
Validation loss: 2.4990971534694624

Epoch: 6| Step: 7
Training loss: 2.8813198122516064
Validation loss: 2.477923069882723

Epoch: 6| Step: 8
Training loss: 2.559842662446646
Validation loss: 2.4470437033892045

Epoch: 6| Step: 9
Training loss: 2.2915963480015566
Validation loss: 2.462620877858095

Epoch: 6| Step: 10
Training loss: 2.728761502153005
Validation loss: 2.45902768884814

Epoch: 6| Step: 11
Training loss: 2.0958149534396586
Validation loss: 2.4934588464019325

Epoch: 6| Step: 12
Training loss: 2.9086169376929605
Validation loss: 2.4758198418642117

Epoch: 6| Step: 13
Training loss: 2.1623450118956433
Validation loss: 2.480403640844694

Epoch: 243| Step: 0
Training loss: 2.8558192422870006
Validation loss: 2.4975257376187887

Epoch: 6| Step: 1
Training loss: 2.7489783383289965
Validation loss: 2.4576833007045478

Epoch: 6| Step: 2
Training loss: 2.725526804267169
Validation loss: 2.4671120070181964

Epoch: 6| Step: 3
Training loss: 2.3444126463968953
Validation loss: 2.462066176967054

Epoch: 6| Step: 4
Training loss: 2.876758866701976
Validation loss: 2.4571527361411447

Epoch: 6| Step: 5
Training loss: 2.346557766417706
Validation loss: 2.480352572225976

Epoch: 6| Step: 6
Training loss: 2.8332646492505553
Validation loss: 2.479414927363938

Epoch: 6| Step: 7
Training loss: 2.863543530950752
Validation loss: 2.4630342251545145

Epoch: 6| Step: 8
Training loss: 2.7914500555769868
Validation loss: 2.4676290282571833

Epoch: 6| Step: 9
Training loss: 1.240743937904726
Validation loss: 2.445633334451449

Epoch: 6| Step: 10
Training loss: 2.3122865733041222
Validation loss: 2.4892991536609386

Epoch: 6| Step: 11
Training loss: 2.4215401694892345
Validation loss: 2.4474054169655375

Epoch: 6| Step: 12
Training loss: 2.2858988802314313
Validation loss: 2.502360006277564

Epoch: 6| Step: 13
Training loss: 2.472863645127114
Validation loss: 2.4296256389431465

Epoch: 244| Step: 0
Training loss: 2.582325790157243
Validation loss: 2.4537814905036384

Epoch: 6| Step: 1
Training loss: 2.2337493020405077
Validation loss: 2.4671040015772423

Epoch: 6| Step: 2
Training loss: 1.7982307958257617
Validation loss: 2.414148819970377

Epoch: 6| Step: 3
Training loss: 2.803702773956245
Validation loss: 2.4562801096742577

Epoch: 6| Step: 4
Training loss: 3.6899178546973186
Validation loss: 2.453233504196469

Epoch: 6| Step: 5
Training loss: 2.5028554821288838
Validation loss: 2.486187085755858

Epoch: 6| Step: 6
Training loss: 2.9119859102558037
Validation loss: 2.444544372511836

Epoch: 6| Step: 7
Training loss: 2.351257912816671
Validation loss: 2.469685606840579

Epoch: 6| Step: 8
Training loss: 2.368816960578552
Validation loss: 2.4919923596718405

Epoch: 6| Step: 9
Training loss: 2.8516027996403186
Validation loss: 2.452863083706694

Epoch: 6| Step: 10
Training loss: 1.990067614456208
Validation loss: 2.463135892165024

Epoch: 6| Step: 11
Training loss: 2.121079811907474
Validation loss: 2.490158124540894

Epoch: 6| Step: 12
Training loss: 2.272175022133479
Validation loss: 2.4744315476651377

Epoch: 6| Step: 13
Training loss: 2.574760877746841
Validation loss: 2.46341667976047

Epoch: 245| Step: 0
Training loss: 2.485760476398845
Validation loss: 2.456075049951397

Epoch: 6| Step: 1
Training loss: 2.616518031796795
Validation loss: 2.4753673046821776

Epoch: 6| Step: 2
Training loss: 1.9427330963220146
Validation loss: 2.464995308229903

Epoch: 6| Step: 3
Training loss: 2.7644789740483326
Validation loss: 2.475382065910122

Epoch: 6| Step: 4
Training loss: 2.095922908297062
Validation loss: 2.4565872904324575

Epoch: 6| Step: 5
Training loss: 2.521460357953493
Validation loss: 2.461470507644858

Epoch: 6| Step: 6
Training loss: 2.7792783456427306
Validation loss: 2.4548573315500968

Epoch: 6| Step: 7
Training loss: 2.8298361643892314
Validation loss: 2.47743905782609

Epoch: 6| Step: 8
Training loss: 2.8582613084206203
Validation loss: 2.476872865263195

Epoch: 6| Step: 9
Training loss: 2.6985563798704217
Validation loss: 2.4617063832698203

Epoch: 6| Step: 10
Training loss: 2.6805740535890634
Validation loss: 2.414728368529728

Epoch: 6| Step: 11
Training loss: 2.1578557766297486
Validation loss: 2.4862980431353305

Epoch: 6| Step: 12
Training loss: 2.459872351335183
Validation loss: 2.4842804034067667

Epoch: 6| Step: 13
Training loss: 2.2189789774768327
Validation loss: 2.477792827817027

Epoch: 246| Step: 0
Training loss: 2.1207155905705193
Validation loss: 2.4424875944475404

Epoch: 6| Step: 1
Training loss: 3.043285113478753
Validation loss: 2.450736335574714

Epoch: 6| Step: 2
Training loss: 2.3480709680753185
Validation loss: 2.4631073011249582

Epoch: 6| Step: 3
Training loss: 2.378878989478454
Validation loss: 2.4726707609572145

Epoch: 6| Step: 4
Training loss: 2.7603426557491804
Validation loss: 2.4657819265267134

Epoch: 6| Step: 5
Training loss: 2.6636878502486008
Validation loss: 2.465881133513786

Epoch: 6| Step: 6
Training loss: 2.684638340637251
Validation loss: 2.4562333437785333

Epoch: 6| Step: 7
Training loss: 2.1919986787638894
Validation loss: 2.434732391485501

Epoch: 6| Step: 8
Training loss: 2.1541748012288298
Validation loss: 2.4828127887436833

Epoch: 6| Step: 9
Training loss: 2.953590961812781
Validation loss: 2.4706750158276853

Epoch: 6| Step: 10
Training loss: 3.2627571326173377
Validation loss: 2.4330427879076164

Epoch: 6| Step: 11
Training loss: 2.1646064597966665
Validation loss: 2.469548338406019

Epoch: 6| Step: 12
Training loss: 1.70328304012475
Validation loss: 2.4712730309766378

Epoch: 6| Step: 13
Training loss: 3.0263856220699434
Validation loss: 2.485643880718162

Epoch: 247| Step: 0
Training loss: 2.47356428253991
Validation loss: 2.4475661599997873

Epoch: 6| Step: 1
Training loss: 2.026305298377183
Validation loss: 2.4753890513900307

Epoch: 6| Step: 2
Training loss: 2.0615679629029273
Validation loss: 2.4756958006734906

Epoch: 6| Step: 3
Training loss: 2.741952130685179
Validation loss: 2.4838862764914613

Epoch: 6| Step: 4
Training loss: 2.8076730633026896
Validation loss: 2.499390162669076

Epoch: 6| Step: 5
Training loss: 2.389219880892445
Validation loss: 2.470761638575741

Epoch: 6| Step: 6
Training loss: 2.7575080854862812
Validation loss: 2.5009606720338593

Epoch: 6| Step: 7
Training loss: 2.5966361660159687
Validation loss: 2.4774219267417457

Epoch: 6| Step: 8
Training loss: 2.425943415284577
Validation loss: 2.463944024751062

Epoch: 6| Step: 9
Training loss: 2.4386065978919658
Validation loss: 2.476378993336275

Epoch: 6| Step: 10
Training loss: 2.9072126209461655
Validation loss: 2.458656302507597

Epoch: 6| Step: 11
Training loss: 2.296320841884799
Validation loss: 2.4597690884934034

Epoch: 6| Step: 12
Training loss: 3.0898530094282877
Validation loss: 2.5018175614220306

Epoch: 6| Step: 13
Training loss: 2.3461759092392724
Validation loss: 2.4523327924289036

Epoch: 248| Step: 0
Training loss: 2.191432143549673
Validation loss: 2.4471660698159274

Epoch: 6| Step: 1
Training loss: 2.582246756974207
Validation loss: 2.454987305455328

Epoch: 6| Step: 2
Training loss: 2.359660049296994
Validation loss: 2.475215681383341

Epoch: 6| Step: 3
Training loss: 1.617283472724012
Validation loss: 2.4833649810929384

Epoch: 6| Step: 4
Training loss: 2.3922961010440997
Validation loss: 2.4722194174633527

Epoch: 6| Step: 5
Training loss: 2.837550185511765
Validation loss: 2.4488634305542765

Epoch: 6| Step: 6
Training loss: 2.560756834778346
Validation loss: 2.4656844092652124

Epoch: 6| Step: 7
Training loss: 2.609672849182583
Validation loss: 2.4823928224198926

Epoch: 6| Step: 8
Training loss: 2.4231811938830887
Validation loss: 2.4802146910058056

Epoch: 6| Step: 9
Training loss: 3.187191218959248
Validation loss: 2.4690017509527507

Epoch: 6| Step: 10
Training loss: 2.687279448219836
Validation loss: 2.4746878884479266

Epoch: 6| Step: 11
Training loss: 2.4629787154027247
Validation loss: 2.466733243473924

Epoch: 6| Step: 12
Training loss: 2.866739097685595
Validation loss: 2.485664716547242

Epoch: 6| Step: 13
Training loss: 2.1766455421571576
Validation loss: 2.4264712518900184

Epoch: 249| Step: 0
Training loss: 2.921201087630319
Validation loss: 2.4770791963443903

Epoch: 6| Step: 1
Training loss: 3.071724215248012
Validation loss: 2.464774826415127

Epoch: 6| Step: 2
Training loss: 2.8255020539440494
Validation loss: 2.458733803604262

Epoch: 6| Step: 3
Training loss: 2.2426346921074
Validation loss: 2.496488843491185

Epoch: 6| Step: 4
Training loss: 2.649619769762523
Validation loss: 2.4790583878411576

Epoch: 6| Step: 5
Training loss: 2.40512088194265
Validation loss: 2.451868599113677

Epoch: 6| Step: 6
Training loss: 2.685002963385909
Validation loss: 2.4485867215326653

Epoch: 6| Step: 7
Training loss: 2.1756821888916926
Validation loss: 2.488870969238805

Epoch: 6| Step: 8
Training loss: 2.2534953623796525
Validation loss: 2.4596118101921274

Epoch: 6| Step: 9
Training loss: 2.639905120416225
Validation loss: 2.438339385465178

Epoch: 6| Step: 10
Training loss: 2.0354913885899104
Validation loss: 2.4447456270765366

Epoch: 6| Step: 11
Training loss: 2.6347893331378676
Validation loss: 2.468335115902199

Epoch: 6| Step: 12
Training loss: 2.0935462738673762
Validation loss: 2.423374108218333

Epoch: 6| Step: 13
Training loss: 2.3776740028734866
Validation loss: 2.4649667647578846

Epoch: 250| Step: 0
Training loss: 3.173936746939324
Validation loss: 2.4542283768801876

Epoch: 6| Step: 1
Training loss: 2.1456480470925765
Validation loss: 2.475748833394731

Epoch: 6| Step: 2
Training loss: 2.300251146785358
Validation loss: 2.4695006423943147

Epoch: 6| Step: 3
Training loss: 2.268028089776585
Validation loss: 2.4728316510357216

Epoch: 6| Step: 4
Training loss: 2.053315955910079
Validation loss: 2.472838511538484

Epoch: 6| Step: 5
Training loss: 2.652890223527151
Validation loss: 2.4650402875781006

Epoch: 6| Step: 6
Training loss: 2.5245992625791107
Validation loss: 2.448943842427247

Epoch: 6| Step: 7
Training loss: 2.9662003661454803
Validation loss: 2.4769990344202006

Epoch: 6| Step: 8
Training loss: 2.432640415541702
Validation loss: 2.4312936964230416

Epoch: 6| Step: 9
Training loss: 3.431252090092146
Validation loss: 2.4471062807041366

Epoch: 6| Step: 10
Training loss: 1.9213684158713698
Validation loss: 2.479284176342809

Epoch: 6| Step: 11
Training loss: 2.728424048883777
Validation loss: 2.4749173292778024

Epoch: 6| Step: 12
Training loss: 2.2841430504556994
Validation loss: 2.4793600508672045

Epoch: 6| Step: 13
Training loss: 1.5762980244011464
Validation loss: 2.4616053969094662

Epoch: 251| Step: 0
Training loss: 1.9171991299724933
Validation loss: 2.4524837983380983

Epoch: 6| Step: 1
Training loss: 2.404677037940712
Validation loss: 2.479680276393898

Epoch: 6| Step: 2
Training loss: 2.6107600756206675
Validation loss: 2.4495139134116615

Epoch: 6| Step: 3
Training loss: 2.608115983221144
Validation loss: 2.4892344392290253

Epoch: 6| Step: 4
Training loss: 2.91665852863448
Validation loss: 2.4689192469215753

Epoch: 6| Step: 5
Training loss: 2.7786901459144295
Validation loss: 2.4495234284805254

Epoch: 6| Step: 6
Training loss: 2.249741963319299
Validation loss: 2.4514308566292455

Epoch: 6| Step: 7
Training loss: 2.4734168065663997
Validation loss: 2.4572449915652084

Epoch: 6| Step: 8
Training loss: 3.310222202460597
Validation loss: 2.4686365398743684

Epoch: 6| Step: 9
Training loss: 2.663437577054825
Validation loss: 2.4644509388569773

Epoch: 6| Step: 10
Training loss: 2.3263742917743193
Validation loss: 2.462249346828846

Epoch: 6| Step: 11
Training loss: 2.660689469950481
Validation loss: 2.496356536436965

Epoch: 6| Step: 12
Training loss: 1.8232587947405812
Validation loss: 2.462997106465948

Epoch: 6| Step: 13
Training loss: 2.247043680748583
Validation loss: 2.480187326876802

Epoch: 252| Step: 0
Training loss: 2.677340978749569
Validation loss: 2.4684440451244156

Epoch: 6| Step: 1
Training loss: 2.8757412825555666
Validation loss: 2.4348411530667855

Epoch: 6| Step: 2
Training loss: 2.0195190901547
Validation loss: 2.4820968399518075

Epoch: 6| Step: 3
Training loss: 2.521745616519885
Validation loss: 2.450307538216512

Epoch: 6| Step: 4
Training loss: 3.414056214909475
Validation loss: 2.458961613707996

Epoch: 6| Step: 5
Training loss: 1.9037681432449312
Validation loss: 2.4786521837605187

Epoch: 6| Step: 6
Training loss: 2.4832119888596322
Validation loss: 2.4944100026452

Epoch: 6| Step: 7
Training loss: 2.700076639006076
Validation loss: 2.4630908728358953

Epoch: 6| Step: 8
Training loss: 1.9015701883077551
Validation loss: 2.4486871826709113

Epoch: 6| Step: 9
Training loss: 2.5815798643364234
Validation loss: 2.4822966362490493

Epoch: 6| Step: 10
Training loss: 3.2526625950464574
Validation loss: 2.5034683052133118

Epoch: 6| Step: 11
Training loss: 2.2544819379274617
Validation loss: 2.5007881983204667

Epoch: 6| Step: 12
Training loss: 2.265824513036469
Validation loss: 2.4717601360254604

Epoch: 6| Step: 13
Training loss: 1.7632408587046933
Validation loss: 2.4985017640128913

Epoch: 253| Step: 0
Training loss: 2.7998392603921394
Validation loss: 2.4824750748745075

Epoch: 6| Step: 1
Training loss: 2.064817109587414
Validation loss: 2.4970095786996853

Epoch: 6| Step: 2
Training loss: 2.998046238802864
Validation loss: 2.461377809625689

Epoch: 6| Step: 3
Training loss: 2.8596371389829147
Validation loss: 2.5001012576526547

Epoch: 6| Step: 4
Training loss: 2.2551358206164633
Validation loss: 2.492008503815309

Epoch: 6| Step: 5
Training loss: 2.9644011115561457
Validation loss: 2.4508339120451264

Epoch: 6| Step: 6
Training loss: 2.6790842873192307
Validation loss: 2.460311494903556

Epoch: 6| Step: 7
Training loss: 1.8689423299457273
Validation loss: 2.501854868831561

Epoch: 6| Step: 8
Training loss: 2.219584133615189
Validation loss: 2.507672319470637

Epoch: 6| Step: 9
Training loss: 2.727203468686016
Validation loss: 2.490653994886016

Epoch: 6| Step: 10
Training loss: 2.0359743783908755
Validation loss: 2.4328146835925275

Epoch: 6| Step: 11
Training loss: 1.710303689971264
Validation loss: 2.460045866836702

Epoch: 6| Step: 12
Training loss: 2.956551315081634
Validation loss: 2.5069938373544987

Epoch: 6| Step: 13
Training loss: 2.5862931758914334
Validation loss: 2.4348347345889754

Epoch: 254| Step: 0
Training loss: 2.93804504532427
Validation loss: 2.449107098264082

Epoch: 6| Step: 1
Training loss: 2.1459927052198786
Validation loss: 2.4845109085776227

Epoch: 6| Step: 2
Training loss: 2.3560551408490404
Validation loss: 2.4573941620663047

Epoch: 6| Step: 3
Training loss: 2.3746225157454304
Validation loss: 2.418966688925037

Epoch: 6| Step: 4
Training loss: 2.4025497720348046
Validation loss: 2.443667961744964

Epoch: 6| Step: 5
Training loss: 2.8996666618031726
Validation loss: 2.483963890885187

Epoch: 6| Step: 6
Training loss: 2.334129322610817
Validation loss: 2.4485604969631547

Epoch: 6| Step: 7
Training loss: 2.954807992754834
Validation loss: 2.4601676365669514

Epoch: 6| Step: 8
Training loss: 2.6456886812652485
Validation loss: 2.4325051415685683

Epoch: 6| Step: 9
Training loss: 2.4143435613968647
Validation loss: 2.458490275098626

Epoch: 6| Step: 10
Training loss: 2.5009738932531635
Validation loss: 2.444239277385107

Epoch: 6| Step: 11
Training loss: 2.644589129086431
Validation loss: 2.470219683104198

Epoch: 6| Step: 12
Training loss: 2.0894245618578395
Validation loss: 2.468275955896084

Epoch: 6| Step: 13
Training loss: 2.127807109465699
Validation loss: 2.479743698718432

Epoch: 255| Step: 0
Training loss: 2.6128456653633987
Validation loss: 2.4652881725122366

Epoch: 6| Step: 1
Training loss: 2.7756306318308837
Validation loss: 2.4411356998949336

Epoch: 6| Step: 2
Training loss: 3.012778089675201
Validation loss: 2.495382295349615

Epoch: 6| Step: 3
Training loss: 2.430977727162661
Validation loss: 2.450901598529444

Epoch: 6| Step: 4
Training loss: 2.6194076913965794
Validation loss: 2.4792628887885844

Epoch: 6| Step: 5
Training loss: 2.738495778864626
Validation loss: 2.460047459181656

Epoch: 6| Step: 6
Training loss: 2.854592915655633
Validation loss: 2.4780754658453485

Epoch: 6| Step: 7
Training loss: 2.3659607380683245
Validation loss: 2.4586418396963516

Epoch: 6| Step: 8
Training loss: 2.2607449294664796
Validation loss: 2.4409153215247312

Epoch: 6| Step: 9
Training loss: 2.5635369226524687
Validation loss: 2.450198350954833

Epoch: 6| Step: 10
Training loss: 2.272035461384101
Validation loss: 2.447486229734973

Epoch: 6| Step: 11
Training loss: 2.8169705781872842
Validation loss: 2.4631307984670747

Epoch: 6| Step: 12
Training loss: 1.609666223935269
Validation loss: 2.464838324924163

Epoch: 6| Step: 13
Training loss: 1.5590363832428358
Validation loss: 2.453060395144014

Epoch: 256| Step: 0
Training loss: 2.7472049206877927
Validation loss: 2.483819586924657

Epoch: 6| Step: 1
Training loss: 2.2146243570379256
Validation loss: 2.459239137590068

Epoch: 6| Step: 2
Training loss: 1.9725997463481035
Validation loss: 2.4619263630631334

Epoch: 6| Step: 3
Training loss: 2.6474122169933776
Validation loss: 2.452996491942765

Epoch: 6| Step: 4
Training loss: 2.3317160337437794
Validation loss: 2.466846795296704

Epoch: 6| Step: 5
Training loss: 2.3258174238636937
Validation loss: 2.4623698508998735

Epoch: 6| Step: 6
Training loss: 2.908879229040048
Validation loss: 2.4474623653680796

Epoch: 6| Step: 7
Training loss: 2.816134837877949
Validation loss: 2.4682321416798683

Epoch: 6| Step: 8
Training loss: 2.6305994529870307
Validation loss: 2.46785441226809

Epoch: 6| Step: 9
Training loss: 2.457279453567959
Validation loss: 2.5104449658659744

Epoch: 6| Step: 10
Training loss: 2.3057659147677665
Validation loss: 2.462871091257632

Epoch: 6| Step: 11
Training loss: 2.626332081270539
Validation loss: 2.4134857267132004

Epoch: 6| Step: 12
Training loss: 2.253645169427214
Validation loss: 2.452352262713458

Epoch: 6| Step: 13
Training loss: 3.2252129417912023
Validation loss: 2.457758583080505

Epoch: 257| Step: 0
Training loss: 2.807420934037645
Validation loss: 2.4906022563726595

Epoch: 6| Step: 1
Training loss: 2.4341501545914657
Validation loss: 2.469997727002098

Epoch: 6| Step: 2
Training loss: 2.4183624390792353
Validation loss: 2.456861809178173

Epoch: 6| Step: 3
Training loss: 2.470114511931964
Validation loss: 2.456246013565584

Epoch: 6| Step: 4
Training loss: 2.9402235675327812
Validation loss: 2.478719188179784

Epoch: 6| Step: 5
Training loss: 1.7191700508773464
Validation loss: 2.4375132414965877

Epoch: 6| Step: 6
Training loss: 2.5709988219808566
Validation loss: 2.4298369558831654

Epoch: 6| Step: 7
Training loss: 2.121990653525289
Validation loss: 2.4463641422009332

Epoch: 6| Step: 8
Training loss: 3.0036982946751776
Validation loss: 2.4475298051983163

Epoch: 6| Step: 9
Training loss: 2.7427668380630745
Validation loss: 2.4734937337596676

Epoch: 6| Step: 10
Training loss: 1.740357057504813
Validation loss: 2.4891575432748936

Epoch: 6| Step: 11
Training loss: 2.504469880985919
Validation loss: 2.4179053835937006

Epoch: 6| Step: 12
Training loss: 2.4982952027298357
Validation loss: 2.484577048200235

Epoch: 6| Step: 13
Training loss: 2.8126930170584257
Validation loss: 2.4463006462229164

Epoch: 258| Step: 0
Training loss: 2.423614074452607
Validation loss: 2.459404240407666

Epoch: 6| Step: 1
Training loss: 2.7226288409611663
Validation loss: 2.439598468593349

Epoch: 6| Step: 2
Training loss: 2.4179701304393273
Validation loss: 2.4752198667410563

Epoch: 6| Step: 3
Training loss: 2.0183371106886017
Validation loss: 2.4806157653871677

Epoch: 6| Step: 4
Training loss: 2.498050501793204
Validation loss: 2.476989634728031

Epoch: 6| Step: 5
Training loss: 2.7609508261496627
Validation loss: 2.478051273269405

Epoch: 6| Step: 6
Training loss: 2.5812362855265207
Validation loss: 2.4727624540128437

Epoch: 6| Step: 7
Training loss: 2.5126565039936852
Validation loss: 2.4737326103995017

Epoch: 6| Step: 8
Training loss: 2.3886887432011212
Validation loss: 2.4985990926634654

Epoch: 6| Step: 9
Training loss: 2.3211097246799657
Validation loss: 2.458058841606885

Epoch: 6| Step: 10
Training loss: 3.085676351192964
Validation loss: 2.4485534579780905

Epoch: 6| Step: 11
Training loss: 2.7937859893460626
Validation loss: 2.453115773227725

Epoch: 6| Step: 12
Training loss: 2.52815471768448
Validation loss: 2.501981362303591

Epoch: 6| Step: 13
Training loss: 2.0725188392730827
Validation loss: 2.486474688780106

Epoch: 259| Step: 0
Training loss: 2.3925751355210916
Validation loss: 2.458500196516602

Epoch: 6| Step: 1
Training loss: 2.0434063357100136
Validation loss: 2.4449532082492826

Epoch: 6| Step: 2
Training loss: 2.6531840862618754
Validation loss: 2.4805917960141777

Epoch: 6| Step: 3
Training loss: 2.6574605539432112
Validation loss: 2.4794361133185836

Epoch: 6| Step: 4
Training loss: 1.847760510073719
Validation loss: 2.446966087150329

Epoch: 6| Step: 5
Training loss: 2.548665078746964
Validation loss: 2.4496407936557154

Epoch: 6| Step: 6
Training loss: 2.549825912965802
Validation loss: 2.4676269816066987

Epoch: 6| Step: 7
Training loss: 3.0439648938153465
Validation loss: 2.4900926082889243

Epoch: 6| Step: 8
Training loss: 2.706676289239074
Validation loss: 2.475340826839111

Epoch: 6| Step: 9
Training loss: 2.8055892812214664
Validation loss: 2.4579442402672615

Epoch: 6| Step: 10
Training loss: 2.374291464874218
Validation loss: 2.4748048904942004

Epoch: 6| Step: 11
Training loss: 1.6578640540589336
Validation loss: 2.5019622971890607

Epoch: 6| Step: 12
Training loss: 2.8371276880088434
Validation loss: 2.447608770950286

Epoch: 6| Step: 13
Training loss: 2.4478135648618204
Validation loss: 2.484399371428446

Epoch: 260| Step: 0
Training loss: 1.939654721080976
Validation loss: 2.493441640313241

Epoch: 6| Step: 1
Training loss: 2.2317505475386823
Validation loss: 2.4463066536823512

Epoch: 6| Step: 2
Training loss: 2.617337259591193
Validation loss: 2.471879207774013

Epoch: 6| Step: 3
Training loss: 2.8652877958657497
Validation loss: 2.4708213169904343

Epoch: 6| Step: 4
Training loss: 3.22084347689184
Validation loss: 2.471136628796923

Epoch: 6| Step: 5
Training loss: 2.9241366685481127
Validation loss: 2.4848479970404886

Epoch: 6| Step: 6
Training loss: 3.0867223586570907
Validation loss: 2.4576822471605064

Epoch: 6| Step: 7
Training loss: 2.3127783530479413
Validation loss: 2.4530589027714873

Epoch: 6| Step: 8
Training loss: 2.3086447669780674
Validation loss: 2.469273588650775

Epoch: 6| Step: 9
Training loss: 2.011279248865991
Validation loss: 2.4776546054840867

Epoch: 6| Step: 10
Training loss: 2.363672044458038
Validation loss: 2.4637403510585423

Epoch: 6| Step: 11
Training loss: 2.277570500544106
Validation loss: 2.4518519292719043

Epoch: 6| Step: 12
Training loss: 2.1156978081891022
Validation loss: 2.46351115340898

Epoch: 6| Step: 13
Training loss: 2.3266002603288567
Validation loss: 2.4686169269793177

Epoch: 261| Step: 0
Training loss: 1.9876075307679222
Validation loss: 2.4399943865770832

Epoch: 6| Step: 1
Training loss: 2.9312710564025832
Validation loss: 2.4694619471665966

Epoch: 6| Step: 2
Training loss: 2.5762299942505846
Validation loss: 2.4706068209617498

Epoch: 6| Step: 3
Training loss: 1.8830428239811812
Validation loss: 2.4533951924512998

Epoch: 6| Step: 4
Training loss: 2.6590760400292646
Validation loss: 2.4778617664147182

Epoch: 6| Step: 5
Training loss: 1.8849114079994502
Validation loss: 2.430857974417833

Epoch: 6| Step: 6
Training loss: 2.5687660272474173
Validation loss: 2.459765969102513

Epoch: 6| Step: 7
Training loss: 2.5261357765101913
Validation loss: 2.4630745519029063

Epoch: 6| Step: 8
Training loss: 2.047962394920449
Validation loss: 2.4415325923920226

Epoch: 6| Step: 9
Training loss: 3.119736014471063
Validation loss: 2.449193783561988

Epoch: 6| Step: 10
Training loss: 2.2361752387013754
Validation loss: 2.4786115886516353

Epoch: 6| Step: 11
Training loss: 2.825957168195121
Validation loss: 2.460469643295483

Epoch: 6| Step: 12
Training loss: 2.8841414037739015
Validation loss: 2.454584086337774

Epoch: 6| Step: 13
Training loss: 2.621941010254594
Validation loss: 2.4870533234648904

Epoch: 262| Step: 0
Training loss: 3.0377147206209067
Validation loss: 2.4472920211000786

Epoch: 6| Step: 1
Training loss: 2.483817751785439
Validation loss: 2.4719549974233357

Epoch: 6| Step: 2
Training loss: 3.004040698947845
Validation loss: 2.4805723877109926

Epoch: 6| Step: 3
Training loss: 1.9328120337249022
Validation loss: 2.4763378381589085

Epoch: 6| Step: 4
Training loss: 2.253409239978354
Validation loss: 2.434657661740069

Epoch: 6| Step: 5
Training loss: 2.7558163775381996
Validation loss: 2.4806492702407907

Epoch: 6| Step: 6
Training loss: 1.9894560395374292
Validation loss: 2.455644458995814

Epoch: 6| Step: 7
Training loss: 2.463895249481
Validation loss: 2.453290754113038

Epoch: 6| Step: 8
Training loss: 3.2839392404731895
Validation loss: 2.503633333349204

Epoch: 6| Step: 9
Training loss: 1.752384264575847
Validation loss: 2.4431148758966996

Epoch: 6| Step: 10
Training loss: 2.7051783353787218
Validation loss: 2.4321488655613432

Epoch: 6| Step: 11
Training loss: 2.4541985173471255
Validation loss: 2.475230364800936

Epoch: 6| Step: 12
Training loss: 2.4900577258117442
Validation loss: 2.45810890181365

Epoch: 6| Step: 13
Training loss: 1.3572408212353833
Validation loss: 2.4341660567671974

Epoch: 263| Step: 0
Training loss: 2.21529560088585
Validation loss: 2.4473651415664697

Epoch: 6| Step: 1
Training loss: 2.3934492993042165
Validation loss: 2.484844505733972

Epoch: 6| Step: 2
Training loss: 1.950079779337879
Validation loss: 2.473709670794655

Epoch: 6| Step: 3
Training loss: 2.929332009682279
Validation loss: 2.4618117454659223

Epoch: 6| Step: 4
Training loss: 1.8108549874361726
Validation loss: 2.5034200834251132

Epoch: 6| Step: 5
Training loss: 2.7882622604599927
Validation loss: 2.484381485518637

Epoch: 6| Step: 6
Training loss: 2.9503486944466824
Validation loss: 2.4253242892808973

Epoch: 6| Step: 7
Training loss: 2.4058860404470366
Validation loss: 2.4907853944535825

Epoch: 6| Step: 8
Training loss: 2.7535336073235346
Validation loss: 2.4629102252049755

Epoch: 6| Step: 9
Training loss: 2.9784378191681906
Validation loss: 2.4672813455192997

Epoch: 6| Step: 10
Training loss: 2.1150808516395228
Validation loss: 2.4736169080555452

Epoch: 6| Step: 11
Training loss: 2.5412231642042156
Validation loss: 2.4620190956162094

Epoch: 6| Step: 12
Training loss: 2.9481854175865068
Validation loss: 2.442848358104577

Epoch: 6| Step: 13
Training loss: 1.5566843522079412
Validation loss: 2.446814649560036

Epoch: 264| Step: 0
Training loss: 2.643280950831386
Validation loss: 2.4747624515405313

Epoch: 6| Step: 1
Training loss: 2.6281983101108044
Validation loss: 2.5052161078573016

Epoch: 6| Step: 2
Training loss: 2.6727327453495917
Validation loss: 2.472718562740817

Epoch: 6| Step: 3
Training loss: 2.347556621449601
Validation loss: 2.4559837390916766

Epoch: 6| Step: 4
Training loss: 3.7865456313503394
Validation loss: 2.496365533555288

Epoch: 6| Step: 5
Training loss: 2.1080588296502443
Validation loss: 2.4549841643246397

Epoch: 6| Step: 6
Training loss: 2.167002749664435
Validation loss: 2.4935923342676602

Epoch: 6| Step: 7
Training loss: 2.871132280778869
Validation loss: 2.45140941295646

Epoch: 6| Step: 8
Training loss: 1.897241034848109
Validation loss: 2.500675052851195

Epoch: 6| Step: 9
Training loss: 3.0045076519746003
Validation loss: 2.4688291494159245

Epoch: 6| Step: 10
Training loss: 2.0720311354871632
Validation loss: 2.464227309677428

Epoch: 6| Step: 11
Training loss: 2.119082795509905
Validation loss: 2.4974314446195236

Epoch: 6| Step: 12
Training loss: 2.069751706735124
Validation loss: 2.5124371298271204

Epoch: 6| Step: 13
Training loss: 2.1676424347959635
Validation loss: 2.45685174497168

Epoch: 265| Step: 0
Training loss: 2.826292171172084
Validation loss: 2.484490651231212

Epoch: 6| Step: 1
Training loss: 2.6393727600680728
Validation loss: 2.483392656549166

Epoch: 6| Step: 2
Training loss: 2.224986271333666
Validation loss: 2.4858491450055467

Epoch: 6| Step: 3
Training loss: 2.725384476862377
Validation loss: 2.4825892160750387

Epoch: 6| Step: 4
Training loss: 1.4875940052749705
Validation loss: 2.490968133129671

Epoch: 6| Step: 5
Training loss: 2.081857183741501
Validation loss: 2.494815144409248

Epoch: 6| Step: 6
Training loss: 2.516845880742094
Validation loss: 2.493408352594808

Epoch: 6| Step: 7
Training loss: 3.0300063895482343
Validation loss: 2.443359620519298

Epoch: 6| Step: 8
Training loss: 2.7916938059232557
Validation loss: 2.481450160429499

Epoch: 6| Step: 9
Training loss: 2.5416503864037914
Validation loss: 2.4857439637111023

Epoch: 6| Step: 10
Training loss: 2.7407329646803067
Validation loss: 2.482687898081323

Epoch: 6| Step: 11
Training loss: 1.8172697250380965
Validation loss: 2.453581392957169

Epoch: 6| Step: 12
Training loss: 2.515196485333256
Validation loss: 2.453303561358801

Epoch: 6| Step: 13
Training loss: 2.5632569660407243
Validation loss: 2.4866369703418196

Epoch: 266| Step: 0
Training loss: 2.9544134003798854
Validation loss: 2.473928606581184

Epoch: 6| Step: 1
Training loss: 3.1156855815480493
Validation loss: 2.4861252498020345

Epoch: 6| Step: 2
Training loss: 2.22584966659504
Validation loss: 2.4367248889842728

Epoch: 6| Step: 3
Training loss: 2.6267351138044472
Validation loss: 2.4804244297400193

Epoch: 6| Step: 4
Training loss: 2.196523529197638
Validation loss: 2.450924423690465

Epoch: 6| Step: 5
Training loss: 1.911443670426205
Validation loss: 2.4548726891762906

Epoch: 6| Step: 6
Training loss: 2.95502794100853
Validation loss: 2.4536516836311275

Epoch: 6| Step: 7
Training loss: 2.207170384165284
Validation loss: 2.4620597149252084

Epoch: 6| Step: 8
Training loss: 3.1927569553143265
Validation loss: 2.470552727348693

Epoch: 6| Step: 9
Training loss: 2.7623573438999434
Validation loss: 2.4720530741477384

Epoch: 6| Step: 10
Training loss: 2.2781396852289455
Validation loss: 2.455492557477313

Epoch: 6| Step: 11
Training loss: 1.9487308969595183
Validation loss: 2.4860233898967725

Epoch: 6| Step: 12
Training loss: 1.9090809047734092
Validation loss: 2.458705411617519

Epoch: 6| Step: 13
Training loss: 1.9273039270561594
Validation loss: 2.503180078925664

Epoch: 267| Step: 0
Training loss: 2.173597791452869
Validation loss: 2.4772982793298906

Epoch: 6| Step: 1
Training loss: 2.709776528072432
Validation loss: 2.4581299387066635

Epoch: 6| Step: 2
Training loss: 2.5412069332228175
Validation loss: 2.4969598148137617

Epoch: 6| Step: 3
Training loss: 3.304981408001983
Validation loss: 2.4857417762435836

Epoch: 6| Step: 4
Training loss: 1.5950666486633023
Validation loss: 2.4791806662593294

Epoch: 6| Step: 5
Training loss: 2.4090227277735856
Validation loss: 2.4716248612278107

Epoch: 6| Step: 6
Training loss: 2.120468019251058
Validation loss: 2.4636526770677922

Epoch: 6| Step: 7
Training loss: 2.2147831445976816
Validation loss: 2.5148276393559246

Epoch: 6| Step: 8
Training loss: 2.198279063162494
Validation loss: 2.425617882217002

Epoch: 6| Step: 9
Training loss: 2.1133895536018525
Validation loss: 2.452593930176436

Epoch: 6| Step: 10
Training loss: 2.8670366543787456
Validation loss: 2.4814343345003986

Epoch: 6| Step: 11
Training loss: 3.1147114568956136
Validation loss: 2.4556093403434858

Epoch: 6| Step: 12
Training loss: 2.1365029402127202
Validation loss: 2.471937208148816

Epoch: 6| Step: 13
Training loss: 3.0108155159027565
Validation loss: 2.445640665916668

Epoch: 268| Step: 0
Training loss: 2.5697077355648683
Validation loss: 2.4767672505002927

Epoch: 6| Step: 1
Training loss: 2.249851221887768
Validation loss: 2.5036568763127725

Epoch: 6| Step: 2
Training loss: 2.311933293366942
Validation loss: 2.4796215194032585

Epoch: 6| Step: 3
Training loss: 2.6201888908843145
Validation loss: 2.4888288206096822

Epoch: 6| Step: 4
Training loss: 2.822922531816271
Validation loss: 2.4571652019110437

Epoch: 6| Step: 5
Training loss: 2.896989568689572
Validation loss: 2.4633301598580637

Epoch: 6| Step: 6
Training loss: 2.338465314835036
Validation loss: 2.448569154583403

Epoch: 6| Step: 7
Training loss: 2.389327251766475
Validation loss: 2.478368757082633

Epoch: 6| Step: 8
Training loss: 2.154276511406476
Validation loss: 2.480300550609616

Epoch: 6| Step: 9
Training loss: 2.4742352354116592
Validation loss: 2.500043729174056

Epoch: 6| Step: 10
Training loss: 2.7334888875319208
Validation loss: 2.462729753780578

Epoch: 6| Step: 11
Training loss: 2.4882064161321114
Validation loss: 2.434230611224638

Epoch: 6| Step: 12
Training loss: 2.1354898827879625
Validation loss: 2.4847366700246414

Epoch: 6| Step: 13
Training loss: 2.4123803855218777
Validation loss: 2.4591851848554556

Epoch: 269| Step: 0
Training loss: 1.5369958314559886
Validation loss: 2.475740746117946

Epoch: 6| Step: 1
Training loss: 2.7968286265359446
Validation loss: 2.4371344999878732

Epoch: 6| Step: 2
Training loss: 2.767294256045467
Validation loss: 2.4790268697710847

Epoch: 6| Step: 3
Training loss: 2.6223154418704784
Validation loss: 2.4784795581562706

Epoch: 6| Step: 4
Training loss: 1.7390203726191011
Validation loss: 2.4429628144434226

Epoch: 6| Step: 5
Training loss: 2.2444684591781647
Validation loss: 2.4510834387109606

Epoch: 6| Step: 6
Training loss: 2.3530392493116787
Validation loss: 2.4731587910800608

Epoch: 6| Step: 7
Training loss: 3.0755620055170847
Validation loss: 2.4453777252245033

Epoch: 6| Step: 8
Training loss: 2.3103444130838744
Validation loss: 2.4267066840413225

Epoch: 6| Step: 9
Training loss: 2.9368308096429003
Validation loss: 2.4415097487302444

Epoch: 6| Step: 10
Training loss: 3.1335544230871
Validation loss: 2.451898436310455

Epoch: 6| Step: 11
Training loss: 2.438841817233103
Validation loss: 2.463016210377041

Epoch: 6| Step: 12
Training loss: 2.1444843335743444
Validation loss: 2.451756446420993

Epoch: 6| Step: 13
Training loss: 1.9344682045515187
Validation loss: 2.4952252506377355

Epoch: 270| Step: 0
Training loss: 1.8596136116345288
Validation loss: 2.4839334248336122

Epoch: 6| Step: 1
Training loss: 1.9930274539019366
Validation loss: 2.458404529208691

Epoch: 6| Step: 2
Training loss: 2.7223622670989207
Validation loss: 2.490257238564199

Epoch: 6| Step: 3
Training loss: 2.8965452860461567
Validation loss: 2.471323174137137

Epoch: 6| Step: 4
Training loss: 3.0727504534408734
Validation loss: 2.45046075764262

Epoch: 6| Step: 5
Training loss: 2.67040743315881
Validation loss: 2.458420889748065

Epoch: 6| Step: 6
Training loss: 2.6850630778712103
Validation loss: 2.450724367463864

Epoch: 6| Step: 7
Training loss: 1.8519945887904694
Validation loss: 2.4737668686218006

Epoch: 6| Step: 8
Training loss: 2.425684535360666
Validation loss: 2.450396685686463

Epoch: 6| Step: 9
Training loss: 2.459692843109946
Validation loss: 2.467689162661963

Epoch: 6| Step: 10
Training loss: 1.5292943805854498
Validation loss: 2.4626806848430176

Epoch: 6| Step: 11
Training loss: 2.619073626566644
Validation loss: 2.4934227427811404

Epoch: 6| Step: 12
Training loss: 3.027767107494657
Validation loss: 2.50204634541809

Epoch: 6| Step: 13
Training loss: 2.5194821375118845
Validation loss: 2.473491786278425

Epoch: 271| Step: 0
Training loss: 2.557495999897115
Validation loss: 2.496832773289791

Epoch: 6| Step: 1
Training loss: 2.5718536858121586
Validation loss: 2.4618463236945916

Epoch: 6| Step: 2
Training loss: 2.6690605465312705
Validation loss: 2.469258077647229

Epoch: 6| Step: 3
Training loss: 2.550967620069886
Validation loss: 2.4766727401106206

Epoch: 6| Step: 4
Training loss: 2.472170813760584
Validation loss: 2.494087542386737

Epoch: 6| Step: 5
Training loss: 2.4753466970197886
Validation loss: 2.4304856172999116

Epoch: 6| Step: 6
Training loss: 1.630398366493777
Validation loss: 2.5048879287578885

Epoch: 6| Step: 7
Training loss: 1.8351893422410124
Validation loss: 2.4652637639779504

Epoch: 6| Step: 8
Training loss: 2.4186768119049673
Validation loss: 2.480743765545129

Epoch: 6| Step: 9
Training loss: 2.476736068660222
Validation loss: 2.4685946366715634

Epoch: 6| Step: 10
Training loss: 2.1703290308411343
Validation loss: 2.4412737909295505

Epoch: 6| Step: 11
Training loss: 3.090480733962305
Validation loss: 2.4642016338884054

Epoch: 6| Step: 12
Training loss: 2.7436997463342574
Validation loss: 2.4587837468470384

Epoch: 6| Step: 13
Training loss: 2.7286026544146567
Validation loss: 2.4609215843466026

Epoch: 272| Step: 0
Training loss: 1.9624563467431102
Validation loss: 2.4753083811010046

Epoch: 6| Step: 1
Training loss: 3.136416676038894
Validation loss: 2.475394021477077

Epoch: 6| Step: 2
Training loss: 2.481626321615379
Validation loss: 2.431379948712596

Epoch: 6| Step: 3
Training loss: 2.2206107919733955
Validation loss: 2.4601404397102065

Epoch: 6| Step: 4
Training loss: 2.14205149991682
Validation loss: 2.5123047003982832

Epoch: 6| Step: 5
Training loss: 2.389738429611678
Validation loss: 2.4757437925702535

Epoch: 6| Step: 6
Training loss: 2.07342859077983
Validation loss: 2.46416727389594

Epoch: 6| Step: 7
Training loss: 3.0350783250936253
Validation loss: 2.4673626382725433

Epoch: 6| Step: 8
Training loss: 2.939349627019283
Validation loss: 2.463310727429745

Epoch: 6| Step: 9
Training loss: 2.131105627276742
Validation loss: 2.5149169783584195

Epoch: 6| Step: 10
Training loss: 2.5579413827200264
Validation loss: 2.471611083678472

Epoch: 6| Step: 11
Training loss: 2.4150473551717715
Validation loss: 2.454652407803245

Epoch: 6| Step: 12
Training loss: 2.503114572186603
Validation loss: 2.4943779981706156

Epoch: 6| Step: 13
Training loss: 2.6259025656934383
Validation loss: 2.501472058211668

Epoch: 273| Step: 0
Training loss: 2.279301791279777
Validation loss: 2.4737145064209427

Epoch: 6| Step: 1
Training loss: 2.604493469077769
Validation loss: 2.461464582500078

Epoch: 6| Step: 2
Training loss: 2.203289269349613
Validation loss: 2.480018683024758

Epoch: 6| Step: 3
Training loss: 2.456557284248812
Validation loss: 2.50765612588806

Epoch: 6| Step: 4
Training loss: 2.3217977272205927
Validation loss: 2.490060148343326

Epoch: 6| Step: 5
Training loss: 2.817719088737752
Validation loss: 2.485408560484506

Epoch: 6| Step: 6
Training loss: 2.885979626177212
Validation loss: 2.480575382756053

Epoch: 6| Step: 7
Training loss: 2.404957808285074
Validation loss: 2.4440330616078714

Epoch: 6| Step: 8
Training loss: 2.2715628849018192
Validation loss: 2.477695259729276

Epoch: 6| Step: 9
Training loss: 2.882288774094384
Validation loss: 2.4532351302233026

Epoch: 6| Step: 10
Training loss: 2.4983444454191144
Validation loss: 2.4566557127196513

Epoch: 6| Step: 11
Training loss: 2.256090609766713
Validation loss: 2.467920943224754

Epoch: 6| Step: 12
Training loss: 2.0959586265935903
Validation loss: 2.457771359743249

Epoch: 6| Step: 13
Training loss: 2.431137192175747
Validation loss: 2.436724135165568

Epoch: 274| Step: 0
Training loss: 2.7736436364305384
Validation loss: 2.469740989982644

Epoch: 6| Step: 1
Training loss: 1.4618809555473862
Validation loss: 2.462309490801061

Epoch: 6| Step: 2
Training loss: 2.6392013052556424
Validation loss: 2.4685927029799553

Epoch: 6| Step: 3
Training loss: 2.5618906924941363
Validation loss: 2.4848669494669275

Epoch: 6| Step: 4
Training loss: 2.3224136651851603
Validation loss: 2.470254999309467

Epoch: 6| Step: 5
Training loss: 2.569939305074888
Validation loss: 2.4466813589981986

Epoch: 6| Step: 6
Training loss: 2.6429990671410937
Validation loss: 2.4713417064232863

Epoch: 6| Step: 7
Training loss: 2.115471739408687
Validation loss: 2.4727633601336483

Epoch: 6| Step: 8
Training loss: 2.8609988078408555
Validation loss: 2.4669068074366276

Epoch: 6| Step: 9
Training loss: 2.7391161115310045
Validation loss: 2.4603151137613293

Epoch: 6| Step: 10
Training loss: 2.0670782197308344
Validation loss: 2.43860750514017

Epoch: 6| Step: 11
Training loss: 2.4801686503378457
Validation loss: 2.4458433501092682

Epoch: 6| Step: 12
Training loss: 2.3164601826299456
Validation loss: 2.44527953211272

Epoch: 6| Step: 13
Training loss: 2.5516978277608064
Validation loss: 2.458982054258638

Epoch: 275| Step: 0
Training loss: 2.375186310537686
Validation loss: 2.5011906403504893

Epoch: 6| Step: 1
Training loss: 1.968538545424442
Validation loss: 2.4681153821981248

Epoch: 6| Step: 2
Training loss: 1.6772500028384214
Validation loss: 2.5107944748702336

Epoch: 6| Step: 3
Training loss: 2.2613718033560772
Validation loss: 2.464999797985526

Epoch: 6| Step: 4
Training loss: 2.1429052529156842
Validation loss: 2.4812375900988526

Epoch: 6| Step: 5
Training loss: 1.8160694753532125
Validation loss: 2.466151146728649

Epoch: 6| Step: 6
Training loss: 3.2525014787299673
Validation loss: 2.463613068953053

Epoch: 6| Step: 7
Training loss: 2.4313131211429746
Validation loss: 2.4700282027688814

Epoch: 6| Step: 8
Training loss: 2.6620829546618605
Validation loss: 2.4628624568486193

Epoch: 6| Step: 9
Training loss: 3.1484835410715486
Validation loss: 2.470875700423266

Epoch: 6| Step: 10
Training loss: 2.298223294377971
Validation loss: 2.468975720380807

Epoch: 6| Step: 11
Training loss: 2.4895213823045705
Validation loss: 2.4803081485974654

Epoch: 6| Step: 12
Training loss: 2.4967059368241875
Validation loss: 2.4885671430143637

Epoch: 6| Step: 13
Training loss: 3.2725065450697595
Validation loss: 2.4666036905086215

Epoch: 276| Step: 0
Training loss: 2.5001600214289215
Validation loss: 2.479527413509768

Epoch: 6| Step: 1
Training loss: 2.812743621447336
Validation loss: 2.4540436890329658

Epoch: 6| Step: 2
Training loss: 2.8933758817201616
Validation loss: 2.463642500643328

Epoch: 6| Step: 3
Training loss: 2.537912243208921
Validation loss: 2.467630099371557

Epoch: 6| Step: 4
Training loss: 1.968854447273579
Validation loss: 2.4589775879200415

Epoch: 6| Step: 5
Training loss: 2.525958145700955
Validation loss: 2.4525292486126498

Epoch: 6| Step: 6
Training loss: 2.4583222555999305
Validation loss: 2.467536968862689

Epoch: 6| Step: 7
Training loss: 2.2414377268152954
Validation loss: 2.5042810066669583

Epoch: 6| Step: 8
Training loss: 2.2186002009153123
Validation loss: 2.43615862020838

Epoch: 6| Step: 9
Training loss: 2.506955769925118
Validation loss: 2.4765856957765404

Epoch: 6| Step: 10
Training loss: 2.4052011755474294
Validation loss: 2.4580852876320156

Epoch: 6| Step: 11
Training loss: 2.883679432759968
Validation loss: 2.4583120117458654

Epoch: 6| Step: 12
Training loss: 2.169746069048263
Validation loss: 2.469028583352237

Epoch: 6| Step: 13
Training loss: 2.113431294133304
Validation loss: 2.4627071353672343

Epoch: 277| Step: 0
Training loss: 2.2763426790427754
Validation loss: 2.4869408478523236

Epoch: 6| Step: 1
Training loss: 3.0429380363902494
Validation loss: 2.4601809821845744

Epoch: 6| Step: 2
Training loss: 2.8917019873962
Validation loss: 2.4518804853118663

Epoch: 6| Step: 3
Training loss: 2.4806174406405965
Validation loss: 2.445650226978052

Epoch: 6| Step: 4
Training loss: 2.0714194739193634
Validation loss: 2.455042444279961

Epoch: 6| Step: 5
Training loss: 3.087097259076803
Validation loss: 2.4574247098960598

Epoch: 6| Step: 6
Training loss: 1.7195767581642374
Validation loss: 2.5000210033067707

Epoch: 6| Step: 7
Training loss: 2.3702076194055524
Validation loss: 2.4739861402692145

Epoch: 6| Step: 8
Training loss: 2.5336698103693966
Validation loss: 2.451764059149695

Epoch: 6| Step: 9
Training loss: 2.57942869573726
Validation loss: 2.4644010438875723

Epoch: 6| Step: 10
Training loss: 2.767584844458586
Validation loss: 2.4701360682539097

Epoch: 6| Step: 11
Training loss: 2.547849784549255
Validation loss: 2.45358044109265

Epoch: 6| Step: 12
Training loss: 1.8494740228161566
Validation loss: 2.440303564824777

Epoch: 6| Step: 13
Training loss: 1.968049848277987
Validation loss: 2.4782941442220365

Epoch: 278| Step: 0
Training loss: 2.5433339971472333
Validation loss: 2.4501328036168326

Epoch: 6| Step: 1
Training loss: 2.502114831968097
Validation loss: 2.4913495910847363

Epoch: 6| Step: 2
Training loss: 2.405308130395862
Validation loss: 2.466005401211465

Epoch: 6| Step: 3
Training loss: 2.546549583193519
Validation loss: 2.4525717811741625

Epoch: 6| Step: 4
Training loss: 2.0868347752691063
Validation loss: 2.4664343475777883

Epoch: 6| Step: 5
Training loss: 2.088696544609256
Validation loss: 2.4702254103010715

Epoch: 6| Step: 6
Training loss: 1.8598232209890482
Validation loss: 2.445245921100603

Epoch: 6| Step: 7
Training loss: 3.250525505422372
Validation loss: 2.502408889299128

Epoch: 6| Step: 8
Training loss: 2.409099625462096
Validation loss: 2.4679451738177427

Epoch: 6| Step: 9
Training loss: 2.44616128736286
Validation loss: 2.4649349292184666

Epoch: 6| Step: 10
Training loss: 2.410661888562309
Validation loss: 2.4795089558461343

Epoch: 6| Step: 11
Training loss: 2.5903722991833558
Validation loss: 2.4679792308716944

Epoch: 6| Step: 12
Training loss: 2.068353150442121
Validation loss: 2.4874421520105736

Epoch: 6| Step: 13
Training loss: 3.4208319480852922
Validation loss: 2.4476329334449662

Epoch: 279| Step: 0
Training loss: 2.132796514542029
Validation loss: 2.4772143233525767

Epoch: 6| Step: 1
Training loss: 2.634891492864308
Validation loss: 2.464810621072341

Epoch: 6| Step: 2
Training loss: 2.794944565825519
Validation loss: 2.4823678952995185

Epoch: 6| Step: 3
Training loss: 2.1819844056156996
Validation loss: 2.486506194925966

Epoch: 6| Step: 4
Training loss: 2.1974114534941482
Validation loss: 2.4717689307105832

Epoch: 6| Step: 5
Training loss: 2.239083617297814
Validation loss: 2.4655185858196

Epoch: 6| Step: 6
Training loss: 2.959556720636727
Validation loss: 2.510145571434382

Epoch: 6| Step: 7
Training loss: 2.5227452323580466
Validation loss: 2.4841807299715626

Epoch: 6| Step: 8
Training loss: 2.7318487213832756
Validation loss: 2.4585222290225777

Epoch: 6| Step: 9
Training loss: 2.63427784153436
Validation loss: 2.4970892276006045

Epoch: 6| Step: 10
Training loss: 2.2921517523413932
Validation loss: 2.4682156644085027

Epoch: 6| Step: 11
Training loss: 2.5495716632796834
Validation loss: 2.487011267728633

Epoch: 6| Step: 12
Training loss: 2.423505468048733
Validation loss: 2.4599008382391236

Epoch: 6| Step: 13
Training loss: 1.9530291724538604
Validation loss: 2.43989214116785

Epoch: 280| Step: 0
Training loss: 2.031652322686714
Validation loss: 2.4563813546262803

Epoch: 6| Step: 1
Training loss: 2.967221639636653
Validation loss: 2.50914502188324

Epoch: 6| Step: 2
Training loss: 1.685649917299561
Validation loss: 2.4664986000518634

Epoch: 6| Step: 3
Training loss: 2.807599354562931
Validation loss: 2.4938932548894233

Epoch: 6| Step: 4
Training loss: 2.6663534656020067
Validation loss: 2.442621223010173

Epoch: 6| Step: 5
Training loss: 2.6373870680798825
Validation loss: 2.452515179814776

Epoch: 6| Step: 6
Training loss: 2.483817943762946
Validation loss: 2.461895247856263

Epoch: 6| Step: 7
Training loss: 2.194571590629968
Validation loss: 2.5043401145957516

Epoch: 6| Step: 8
Training loss: 2.8267878116282827
Validation loss: 2.4559977159962814

Epoch: 6| Step: 9
Training loss: 2.1106213313808757
Validation loss: 2.5018255254652977

Epoch: 6| Step: 10
Training loss: 1.733111703287231
Validation loss: 2.4672422133644636

Epoch: 6| Step: 11
Training loss: 3.0754226208067443
Validation loss: 2.453673502648763

Epoch: 6| Step: 12
Training loss: 1.975093370958732
Validation loss: 2.467486721747446

Epoch: 6| Step: 13
Training loss: 2.4472745878477133
Validation loss: 2.4353219696090425

Epoch: 281| Step: 0
Training loss: 2.6300299454931446
Validation loss: 2.442640638464026

Epoch: 6| Step: 1
Training loss: 2.5530060533809658
Validation loss: 2.4600160133043385

Epoch: 6| Step: 2
Training loss: 2.2177609670499243
Validation loss: 2.4722047928994626

Epoch: 6| Step: 3
Training loss: 2.5002748338312615
Validation loss: 2.462862167473085

Epoch: 6| Step: 4
Training loss: 2.5475247730252293
Validation loss: 2.4543191263212334

Epoch: 6| Step: 5
Training loss: 2.7337081532007437
Validation loss: 2.478782956741468

Epoch: 6| Step: 6
Training loss: 2.798092539796904
Validation loss: 2.5135198727684234

Epoch: 6| Step: 7
Training loss: 1.7910421820848337
Validation loss: 2.470505418073322

Epoch: 6| Step: 8
Training loss: 2.1508937042801106
Validation loss: 2.50380848054801

Epoch: 6| Step: 9
Training loss: 2.599419888555722
Validation loss: 2.466645810973919

Epoch: 6| Step: 10
Training loss: 2.238504182322133
Validation loss: 2.4594628044234574

Epoch: 6| Step: 11
Training loss: 2.776669190975414
Validation loss: 2.488198562015475

Epoch: 6| Step: 12
Training loss: 2.2616230313564447
Validation loss: 2.4573890335156188

Epoch: 6| Step: 13
Training loss: 2.078261750963429
Validation loss: 2.4499663628786066

Epoch: 282| Step: 0
Training loss: 2.1476996247097766
Validation loss: 2.4834654315446727

Epoch: 6| Step: 1
Training loss: 3.125967562613958
Validation loss: 2.4163223650049637

Epoch: 6| Step: 2
Training loss: 2.7300852231986705
Validation loss: 2.448735252152164

Epoch: 6| Step: 3
Training loss: 2.3909228768859427
Validation loss: 2.443849761628593

Epoch: 6| Step: 4
Training loss: 2.5220738555899858
Validation loss: 2.4641472092781216

Epoch: 6| Step: 5
Training loss: 1.9052417152223657
Validation loss: 2.462702599792389

Epoch: 6| Step: 6
Training loss: 1.9874767182144137
Validation loss: 2.4409872582333483

Epoch: 6| Step: 7
Training loss: 2.61895646621977
Validation loss: 2.4627493979455175

Epoch: 6| Step: 8
Training loss: 2.435593960206701
Validation loss: 2.50037912344851

Epoch: 6| Step: 9
Training loss: 1.3562377595129382
Validation loss: 2.4734685563054377

Epoch: 6| Step: 10
Training loss: 2.345620896345737
Validation loss: 2.484614907515406

Epoch: 6| Step: 11
Training loss: 2.450743472896804
Validation loss: 2.4724439247186387

Epoch: 6| Step: 12
Training loss: 3.082396511002048
Validation loss: 2.487016417659202

Epoch: 6| Step: 13
Training loss: 3.044589393379637
Validation loss: 2.4845389633675863

Epoch: 283| Step: 0
Training loss: 1.8833831302282262
Validation loss: 2.4598367500578293

Epoch: 6| Step: 1
Training loss: 2.369004059304608
Validation loss: 2.483417914043704

Epoch: 6| Step: 2
Training loss: 2.171178582597599
Validation loss: 2.4502935999997164

Epoch: 6| Step: 3
Training loss: 2.7447386609242894
Validation loss: 2.4329509614064033

Epoch: 6| Step: 4
Training loss: 2.3197923070743696
Validation loss: 2.461110904988994

Epoch: 6| Step: 5
Training loss: 2.762976806366556
Validation loss: 2.4612242537300606

Epoch: 6| Step: 6
Training loss: 2.889563618221194
Validation loss: 2.486219176078336

Epoch: 6| Step: 7
Training loss: 1.9628145876908127
Validation loss: 2.487219448988296

Epoch: 6| Step: 8
Training loss: 2.1509430303118715
Validation loss: 2.4863859780393813

Epoch: 6| Step: 9
Training loss: 2.7221621444024016
Validation loss: 2.4436779868793295

Epoch: 6| Step: 10
Training loss: 2.565221690214374
Validation loss: 2.4613349089730723

Epoch: 6| Step: 11
Training loss: 2.919464259186768
Validation loss: 2.458021525496713

Epoch: 6| Step: 12
Training loss: 1.8467723191790884
Validation loss: 2.4837952985718657

Epoch: 6| Step: 13
Training loss: 2.7754468918854336
Validation loss: 2.4265080991643777

Epoch: 284| Step: 0
Training loss: 3.1462599273512897
Validation loss: 2.444908247324394

Epoch: 6| Step: 1
Training loss: 2.93669263417616
Validation loss: 2.4710158747781374

Epoch: 6| Step: 2
Training loss: 1.5113071403712852
Validation loss: 2.4838893913935647

Epoch: 6| Step: 3
Training loss: 2.241467828925209
Validation loss: 2.471259003531421

Epoch: 6| Step: 4
Training loss: 2.43272705320074
Validation loss: 2.485311952576655

Epoch: 6| Step: 5
Training loss: 2.225405101116358
Validation loss: 2.492879852583462

Epoch: 6| Step: 6
Training loss: 2.1403612405933665
Validation loss: 2.4889993019032985

Epoch: 6| Step: 7
Training loss: 2.257898137669535
Validation loss: 2.480254419026467

Epoch: 6| Step: 8
Training loss: 2.342932800241038
Validation loss: 2.478839036390143

Epoch: 6| Step: 9
Training loss: 2.394736152084013
Validation loss: 2.5003776480402697

Epoch: 6| Step: 10
Training loss: 2.434087761324123
Validation loss: 2.4402072261489187

Epoch: 6| Step: 11
Training loss: 2.6671892985163
Validation loss: 2.4593125543574357

Epoch: 6| Step: 12
Training loss: 2.7027330059208485
Validation loss: 2.501154930206725

Epoch: 6| Step: 13
Training loss: 2.4029685099709712
Validation loss: 2.503033107559831

Epoch: 285| Step: 0
Training loss: 2.243270559278132
Validation loss: 2.439879391712158

Epoch: 6| Step: 1
Training loss: 2.494146360481855
Validation loss: 2.4747098773195852

Epoch: 6| Step: 2
Training loss: 2.4072454796100007
Validation loss: 2.4511064718839317

Epoch: 6| Step: 3
Training loss: 2.6646079123175888
Validation loss: 2.498148559706168

Epoch: 6| Step: 4
Training loss: 1.8689467310613541
Validation loss: 2.5082854875683633

Epoch: 6| Step: 5
Training loss: 2.6026861839865467
Validation loss: 2.4312817865530234

Epoch: 6| Step: 6
Training loss: 2.2706157568426604
Validation loss: 2.4671264705728735

Epoch: 6| Step: 7
Training loss: 2.375011444064225
Validation loss: 2.4851933511210453

Epoch: 6| Step: 8
Training loss: 2.8165752132699806
Validation loss: 2.4801144572410716

Epoch: 6| Step: 9
Training loss: 2.64316522430099
Validation loss: 2.4588065077002925

Epoch: 6| Step: 10
Training loss: 2.7545131582741123
Validation loss: 2.477188387875906

Epoch: 6| Step: 11
Training loss: 2.3919610173547814
Validation loss: 2.516528323627164

Epoch: 6| Step: 12
Training loss: 2.425791766501711
Validation loss: 2.492758415849699

Epoch: 6| Step: 13
Training loss: 2.3362152036603394
Validation loss: 2.440068197467343

Epoch: 286| Step: 0
Training loss: 1.7732726033048933
Validation loss: 2.4203364793674

Epoch: 6| Step: 1
Training loss: 1.784853853285442
Validation loss: 2.4542184146897923

Epoch: 6| Step: 2
Training loss: 2.4361924797145593
Validation loss: 2.4683895136059406

Epoch: 6| Step: 3
Training loss: 2.338483054963634
Validation loss: 2.464981189938816

Epoch: 6| Step: 4
Training loss: 2.7054429892696827
Validation loss: 2.45483778818959

Epoch: 6| Step: 5
Training loss: 2.2594924537446572
Validation loss: 2.482534738135324

Epoch: 6| Step: 6
Training loss: 2.3819150141265504
Validation loss: 2.4565763359689807

Epoch: 6| Step: 7
Training loss: 3.054929133465791
Validation loss: 2.4722101022597536

Epoch: 6| Step: 8
Training loss: 2.892756155898771
Validation loss: 2.4917634828366975

Epoch: 6| Step: 9
Training loss: 2.2398508902716348
Validation loss: 2.4815905355202674

Epoch: 6| Step: 10
Training loss: 3.0110875914321005
Validation loss: 2.4796904226687877

Epoch: 6| Step: 11
Training loss: 2.6355089817288495
Validation loss: 2.4831460534311223

Epoch: 6| Step: 12
Training loss: 2.207321931044334
Validation loss: 2.501098198818595

Epoch: 6| Step: 13
Training loss: 2.489067396600939
Validation loss: 2.463093790259903

Epoch: 287| Step: 0
Training loss: 2.5129789095265687
Validation loss: 2.457771690398102

Epoch: 6| Step: 1
Training loss: 2.0054713749617608
Validation loss: 2.4832675638000814

Epoch: 6| Step: 2
Training loss: 2.3854997334461148
Validation loss: 2.4637180364009215

Epoch: 6| Step: 3
Training loss: 2.0431356270724645
Validation loss: 2.4688255191548945

Epoch: 6| Step: 4
Training loss: 2.130841696549604
Validation loss: 2.509162746087486

Epoch: 6| Step: 5
Training loss: 2.6339096364225094
Validation loss: 2.4805939456504946

Epoch: 6| Step: 6
Training loss: 2.976421683277107
Validation loss: 2.4385604718130693

Epoch: 6| Step: 7
Training loss: 3.1598636600199304
Validation loss: 2.4744347485468223

Epoch: 6| Step: 8
Training loss: 2.3223823537276367
Validation loss: 2.4757336223546225

Epoch: 6| Step: 9
Training loss: 2.770498260277197
Validation loss: 2.456200515168022

Epoch: 6| Step: 10
Training loss: 1.8311380189371234
Validation loss: 2.461681608049656

Epoch: 6| Step: 11
Training loss: 1.7428794820881512
Validation loss: 2.4955405505979313

Epoch: 6| Step: 12
Training loss: 2.815753834747979
Validation loss: 2.4899809273532414

Epoch: 6| Step: 13
Training loss: 2.4519098272871824
Validation loss: 2.4664666595566103

Epoch: 288| Step: 0
Training loss: 2.050613134850355
Validation loss: 2.46610490275732

Epoch: 6| Step: 1
Training loss: 2.4525264325598584
Validation loss: 2.473052509754379

Epoch: 6| Step: 2
Training loss: 2.913781037429208
Validation loss: 2.4726156736104867

Epoch: 6| Step: 3
Training loss: 2.0962627762170287
Validation loss: 2.462886230285793

Epoch: 6| Step: 4
Training loss: 2.546905142950939
Validation loss: 2.4512324424208938

Epoch: 6| Step: 5
Training loss: 2.423502418342141
Validation loss: 2.4530384996070076

Epoch: 6| Step: 6
Training loss: 2.5020173516468454
Validation loss: 2.500996469517748

Epoch: 6| Step: 7
Training loss: 3.234532154337979
Validation loss: 2.473014907785412

Epoch: 6| Step: 8
Training loss: 1.7537285049872207
Validation loss: 2.455336825645373

Epoch: 6| Step: 9
Training loss: 2.4061620126465955
Validation loss: 2.46677283890279

Epoch: 6| Step: 10
Training loss: 2.5745621540437083
Validation loss: 2.459356711984257

Epoch: 6| Step: 11
Training loss: 1.9420545635317188
Validation loss: 2.4951962556912033

Epoch: 6| Step: 12
Training loss: 2.5314300849809155
Validation loss: 2.4763986865827565

Epoch: 6| Step: 13
Training loss: 2.285880210507977
Validation loss: 2.4867488009405645

Epoch: 289| Step: 0
Training loss: 2.261494205377186
Validation loss: 2.47680043323991

Epoch: 6| Step: 1
Training loss: 1.818654299705628
Validation loss: 2.446189471763777

Epoch: 6| Step: 2
Training loss: 2.107106210791741
Validation loss: 2.475172035583231

Epoch: 6| Step: 3
Training loss: 2.7096058325899306
Validation loss: 2.4657181315661942

Epoch: 6| Step: 4
Training loss: 2.5382790633286403
Validation loss: 2.5000337957333394

Epoch: 6| Step: 5
Training loss: 2.617154978080353
Validation loss: 2.4570142771241943

Epoch: 6| Step: 6
Training loss: 2.516114370951758
Validation loss: 2.434506410459767

Epoch: 6| Step: 7
Training loss: 2.631743533860231
Validation loss: 2.4945438650801983

Epoch: 6| Step: 8
Training loss: 2.656080981094346
Validation loss: 2.504655093131173

Epoch: 6| Step: 9
Training loss: 2.8446221324270726
Validation loss: 2.4496227214584647

Epoch: 6| Step: 10
Training loss: 1.9934914304665334
Validation loss: 2.515676992260367

Epoch: 6| Step: 11
Training loss: 2.3618082799923514
Validation loss: 2.4562440127476393

Epoch: 6| Step: 12
Training loss: 2.3149576273061987
Validation loss: 2.4727217684318434

Epoch: 6| Step: 13
Training loss: 2.6761757664226926
Validation loss: 2.41580275524293

Epoch: 290| Step: 0
Training loss: 2.0415263873844403
Validation loss: 2.4449450274965248

Epoch: 6| Step: 1
Training loss: 2.808761655327484
Validation loss: 2.4600900373531642

Epoch: 6| Step: 2
Training loss: 2.366917157237159
Validation loss: 2.47625466956786

Epoch: 6| Step: 3
Training loss: 2.765614568830743
Validation loss: 2.5179253869499125

Epoch: 6| Step: 4
Training loss: 2.0932625729857426
Validation loss: 2.4650747102807884

Epoch: 6| Step: 5
Training loss: 2.518043634838015
Validation loss: 2.4635801831095154

Epoch: 6| Step: 6
Training loss: 2.957820006057235
Validation loss: 2.4782086578591973

Epoch: 6| Step: 7
Training loss: 2.362731365197778
Validation loss: 2.4542196692364935

Epoch: 6| Step: 8
Training loss: 1.967011122791859
Validation loss: 2.47964810506692

Epoch: 6| Step: 9
Training loss: 2.0016708547192077
Validation loss: 2.45195863186609

Epoch: 6| Step: 10
Training loss: 1.4813272608956742
Validation loss: 2.471142588846852

Epoch: 6| Step: 11
Training loss: 2.5049646672785557
Validation loss: 2.4504669358932367

Epoch: 6| Step: 12
Training loss: 2.4535675621517856
Validation loss: 2.5074129294551497

Epoch: 6| Step: 13
Training loss: 3.5478483368585665
Validation loss: 2.4349690316036723

Epoch: 291| Step: 0
Training loss: 2.172240631283508
Validation loss: 2.4627979866787486

Epoch: 6| Step: 1
Training loss: 2.717959760501356
Validation loss: 2.472856997738415

Epoch: 6| Step: 2
Training loss: 1.611089293379692
Validation loss: 2.4661758385855337

Epoch: 6| Step: 3
Training loss: 2.6220108451706263
Validation loss: 2.4728929382104172

Epoch: 6| Step: 4
Training loss: 1.9491353905843092
Validation loss: 2.491102066285548

Epoch: 6| Step: 5
Training loss: 1.627874472996983
Validation loss: 2.4524416222804213

Epoch: 6| Step: 6
Training loss: 2.637219823627208
Validation loss: 2.479797686952769

Epoch: 6| Step: 7
Training loss: 2.8953836398332373
Validation loss: 2.500121067561181

Epoch: 6| Step: 8
Training loss: 2.4704796248960177
Validation loss: 2.4715617035556288

Epoch: 6| Step: 9
Training loss: 2.4531753437683474
Validation loss: 2.4550769805350794

Epoch: 6| Step: 10
Training loss: 2.491684818095389
Validation loss: 2.4570834533528556

Epoch: 6| Step: 11
Training loss: 2.5932552371323867
Validation loss: 2.4910968712870782

Epoch: 6| Step: 12
Training loss: 3.0917655668523016
Validation loss: 2.4636134279607096

Epoch: 6| Step: 13
Training loss: 2.198398579440706
Validation loss: 2.5171040797081403

Epoch: 292| Step: 0
Training loss: 2.226200542810895
Validation loss: 2.474769505067709

Epoch: 6| Step: 1
Training loss: 2.518130742660871
Validation loss: 2.4881122135228706

Epoch: 6| Step: 2
Training loss: 2.368476038567791
Validation loss: 2.4557395135093487

Epoch: 6| Step: 3
Training loss: 2.925435879826303
Validation loss: 2.5145177812778785

Epoch: 6| Step: 4
Training loss: 1.9563507297895226
Validation loss: 2.4725692415677827

Epoch: 6| Step: 5
Training loss: 2.5126815540516736
Validation loss: 2.4408005759126095

Epoch: 6| Step: 6
Training loss: 3.222855403410762
Validation loss: 2.5092356037933885

Epoch: 6| Step: 7
Training loss: 2.4284643301870266
Validation loss: 2.41510753669615

Epoch: 6| Step: 8
Training loss: 1.6849219090735046
Validation loss: 2.457903083065736

Epoch: 6| Step: 9
Training loss: 2.555159592244602
Validation loss: 2.481721416050641

Epoch: 6| Step: 10
Training loss: 2.297639680586719
Validation loss: 2.483546436614493

Epoch: 6| Step: 11
Training loss: 2.7349054312526984
Validation loss: 2.4765160260159003

Epoch: 6| Step: 12
Training loss: 1.9051613122119913
Validation loss: 2.479316654840314

Epoch: 6| Step: 13
Training loss: 2.4430254396229727
Validation loss: 2.504039834110651

Epoch: 293| Step: 0
Training loss: 2.0872571107188347
Validation loss: 2.4664254086212583

Epoch: 6| Step: 1
Training loss: 2.113296029256026
Validation loss: 2.468220652056086

Epoch: 6| Step: 2
Training loss: 2.3069864477347366
Validation loss: 2.4904053571321483

Epoch: 6| Step: 3
Training loss: 2.324740208703731
Validation loss: 2.477857678117919

Epoch: 6| Step: 4
Training loss: 2.2586978641608844
Validation loss: 2.4489256766285155

Epoch: 6| Step: 5
Training loss: 2.2486751682623303
Validation loss: 2.426638838765251

Epoch: 6| Step: 6
Training loss: 2.255248200825501
Validation loss: 2.472582947421219

Epoch: 6| Step: 7
Training loss: 3.2787436949668933
Validation loss: 2.4818656942941297

Epoch: 6| Step: 8
Training loss: 2.3377614125020214
Validation loss: 2.4328836142899375

Epoch: 6| Step: 9
Training loss: 2.6262071649398018
Validation loss: 2.4885312579059313

Epoch: 6| Step: 10
Training loss: 2.2184409342673534
Validation loss: 2.462417359165506

Epoch: 6| Step: 11
Training loss: 2.5811332953350847
Validation loss: 2.4832307276808896

Epoch: 6| Step: 12
Training loss: 2.7244895771994266
Validation loss: 2.473176281325994

Epoch: 6| Step: 13
Training loss: 2.3473095122088092
Validation loss: 2.4451832794000854

Epoch: 294| Step: 0
Training loss: 2.363095816911533
Validation loss: 2.4965896982896463

Epoch: 6| Step: 1
Training loss: 2.763836470915721
Validation loss: 2.4443936307332828

Epoch: 6| Step: 2
Training loss: 2.201173932972363
Validation loss: 2.4702025502117313

Epoch: 6| Step: 3
Training loss: 2.304431758851013
Validation loss: 2.4705247036312064

Epoch: 6| Step: 4
Training loss: 2.152710705057507
Validation loss: 2.441399099031866

Epoch: 6| Step: 5
Training loss: 2.2092958428043796
Validation loss: 2.4589228464640063

Epoch: 6| Step: 6
Training loss: 2.5111871278372755
Validation loss: 2.462303240752347

Epoch: 6| Step: 7
Training loss: 1.877779743875732
Validation loss: 2.505730251464865

Epoch: 6| Step: 8
Training loss: 3.361433383284397
Validation loss: 2.488228450387598

Epoch: 6| Step: 9
Training loss: 2.410870562135306
Validation loss: 2.4208177516854383

Epoch: 6| Step: 10
Training loss: 2.5920950884037146
Validation loss: 2.474266952323369

Epoch: 6| Step: 11
Training loss: 2.1666044324104314
Validation loss: 2.4945275040559127

Epoch: 6| Step: 12
Training loss: 2.6372752414775182
Validation loss: 2.415309872726418

Epoch: 6| Step: 13
Training loss: 1.100035183517278
Validation loss: 2.470185218455352

Epoch: 295| Step: 0
Training loss: 1.8290498138618683
Validation loss: 2.477685482946573

Epoch: 6| Step: 1
Training loss: 3.2072037198599643
Validation loss: 2.465153776469684

Epoch: 6| Step: 2
Training loss: 2.775082210663306
Validation loss: 2.4470059772910617

Epoch: 6| Step: 3
Training loss: 2.212421902932329
Validation loss: 2.466715632714531

Epoch: 6| Step: 4
Training loss: 2.2113184516451225
Validation loss: 2.457809278367674

Epoch: 6| Step: 5
Training loss: 2.225844310912904
Validation loss: 2.4886522292627653

Epoch: 6| Step: 6
Training loss: 2.0315596124463933
Validation loss: 2.4572539701778733

Epoch: 6| Step: 7
Training loss: 2.1520453105963786
Validation loss: 2.448011944098519

Epoch: 6| Step: 8
Training loss: 3.1030778618158616
Validation loss: 2.518690569615247

Epoch: 6| Step: 9
Training loss: 2.661293178940897
Validation loss: 2.450055544212913

Epoch: 6| Step: 10
Training loss: 2.333574509645113
Validation loss: 2.451630602671792

Epoch: 6| Step: 11
Training loss: 2.5158587045096805
Validation loss: 2.468092220554486

Epoch: 6| Step: 12
Training loss: 2.110197909867943
Validation loss: 2.493340499834838

Epoch: 6| Step: 13
Training loss: 2.5152007509351777
Validation loss: 2.442947770225354

Epoch: 296| Step: 0
Training loss: 3.4815350236074702
Validation loss: 2.437221966650429

Epoch: 6| Step: 1
Training loss: 1.858637880023494
Validation loss: 2.473937640705367

Epoch: 6| Step: 2
Training loss: 2.7949155624819526
Validation loss: 2.4685877628168043

Epoch: 6| Step: 3
Training loss: 2.6063452394497064
Validation loss: 2.472740902961699

Epoch: 6| Step: 4
Training loss: 2.749100277844823
Validation loss: 2.4835467834504454

Epoch: 6| Step: 5
Training loss: 2.5276950314943476
Validation loss: 2.4732991936423754

Epoch: 6| Step: 6
Training loss: 1.8708553918955477
Validation loss: 2.4484579705708307

Epoch: 6| Step: 7
Training loss: 2.8695283320095597
Validation loss: 2.4965209728845386

Epoch: 6| Step: 8
Training loss: 2.351990214132854
Validation loss: 2.4774355074431673

Epoch: 6| Step: 9
Training loss: 1.7167715997321753
Validation loss: 2.5165817478535124

Epoch: 6| Step: 10
Training loss: 2.0574783691159144
Validation loss: 2.4659987561274708

Epoch: 6| Step: 11
Training loss: 1.9645665847317892
Validation loss: 2.4695330813882794

Epoch: 6| Step: 12
Training loss: 2.166399413523461
Validation loss: 2.475418886779394

Epoch: 6| Step: 13
Training loss: 2.175312203986925
Validation loss: 2.490204992505012

Epoch: 297| Step: 0
Training loss: 2.7234195640791885
Validation loss: 2.4920885666459154

Epoch: 6| Step: 1
Training loss: 2.487133869465726
Validation loss: 2.461823125458793

Epoch: 6| Step: 2
Training loss: 2.069424420742759
Validation loss: 2.462629089419931

Epoch: 6| Step: 3
Training loss: 2.5471529194666944
Validation loss: 2.471224790422513

Epoch: 6| Step: 4
Training loss: 2.673903916303929
Validation loss: 2.4726605350750344

Epoch: 6| Step: 5
Training loss: 2.080393306153568
Validation loss: 2.4809655990685306

Epoch: 6| Step: 6
Training loss: 1.9706705807133797
Validation loss: 2.4771201135904777

Epoch: 6| Step: 7
Training loss: 2.4879556439852357
Validation loss: 2.478742714385202

Epoch: 6| Step: 8
Training loss: 2.887835006766514
Validation loss: 2.4910328601542964

Epoch: 6| Step: 9
Training loss: 2.4205567740889977
Validation loss: 2.4774416448113166

Epoch: 6| Step: 10
Training loss: 2.0819552123193033
Validation loss: 2.480710846889137

Epoch: 6| Step: 11
Training loss: 2.8705730355291657
Validation loss: 2.4691881974991743

Epoch: 6| Step: 12
Training loss: 1.9432065050583969
Validation loss: 2.4858746302055494

Epoch: 6| Step: 13
Training loss: 1.9436057083199931
Validation loss: 2.4921051535379695

Epoch: 298| Step: 0
Training loss: 1.7636023219388548
Validation loss: 2.4803360256026092

Epoch: 6| Step: 1
Training loss: 2.700640291657576
Validation loss: 2.4868969836339083

Epoch: 6| Step: 2
Training loss: 2.9842417422596843
Validation loss: 2.472025021808012

Epoch: 6| Step: 3
Training loss: 3.0164319475724852
Validation loss: 2.4723256443890076

Epoch: 6| Step: 4
Training loss: 2.04464681937916
Validation loss: 2.4731255724368255

Epoch: 6| Step: 5
Training loss: 2.483757662096574
Validation loss: 2.4958357368783757

Epoch: 6| Step: 6
Training loss: 2.5768310999085786
Validation loss: 2.508668281426838

Epoch: 6| Step: 7
Training loss: 2.765359294649326
Validation loss: 2.44040707040149

Epoch: 6| Step: 8
Training loss: 1.8926831227122722
Validation loss: 2.4841184364297058

Epoch: 6| Step: 9
Training loss: 2.060335005584188
Validation loss: 2.4597737378688755

Epoch: 6| Step: 10
Training loss: 1.9087120790842933
Validation loss: 2.467307863095492

Epoch: 6| Step: 11
Training loss: 1.8237949834848948
Validation loss: 2.48646702405947

Epoch: 6| Step: 12
Training loss: 2.9403811995234124
Validation loss: 2.4843211669620446

Epoch: 6| Step: 13
Training loss: 2.3641595894895073
Validation loss: 2.4924720192596377

Epoch: 299| Step: 0
Training loss: 2.2347673191756927
Validation loss: 2.4614656167195563

Epoch: 6| Step: 1
Training loss: 2.1788608528515234
Validation loss: 2.4719062215465915

Epoch: 6| Step: 2
Training loss: 1.897102168901746
Validation loss: 2.513388666241983

Epoch: 6| Step: 3
Training loss: 2.4976879873159543
Validation loss: 2.4773005497972878

Epoch: 6| Step: 4
Training loss: 2.5839852976779345
Validation loss: 2.48077619393151

Epoch: 6| Step: 5
Training loss: 2.474027184314681
Validation loss: 2.504004060170575

Epoch: 6| Step: 6
Training loss: 2.179413918447706
Validation loss: 2.4616539978089698

Epoch: 6| Step: 7
Training loss: 2.876447893519168
Validation loss: 2.42448730791652

Epoch: 6| Step: 8
Training loss: 2.725594422488808
Validation loss: 2.4746988093564064

Epoch: 6| Step: 9
Training loss: 2.1336822413193084
Validation loss: 2.4520135037063504

Epoch: 6| Step: 10
Training loss: 2.2788436949065702
Validation loss: 2.467373084044143

Epoch: 6| Step: 11
Training loss: 2.613458510518189
Validation loss: 2.4530797593267892

Epoch: 6| Step: 12
Training loss: 2.5613293648781217
Validation loss: 2.4410335747565806

Epoch: 6| Step: 13
Training loss: 2.1301177611057045
Validation loss: 2.486390661159743

Epoch: 300| Step: 0
Training loss: 1.9106671794793948
Validation loss: 2.467789948889307

Epoch: 6| Step: 1
Training loss: 1.9445004894111784
Validation loss: 2.496441729970584

Epoch: 6| Step: 2
Training loss: 3.0415427474467456
Validation loss: 2.471626014107419

Epoch: 6| Step: 3
Training loss: 2.1205979921761187
Validation loss: 2.43362773578611

Epoch: 6| Step: 4
Training loss: 3.164033413977037
Validation loss: 2.4455854796958483

Epoch: 6| Step: 5
Training loss: 2.4146835367795485
Validation loss: 2.4780135971424353

Epoch: 6| Step: 6
Training loss: 2.6328076099384297
Validation loss: 2.4873788043607172

Epoch: 6| Step: 7
Training loss: 2.742605237306568
Validation loss: 2.4851656008127065

Epoch: 6| Step: 8
Training loss: 2.098348663108444
Validation loss: 2.4543182885980053

Epoch: 6| Step: 9
Training loss: 2.0657935000209977
Validation loss: 2.4689691118457264

Epoch: 6| Step: 10
Training loss: 2.3891142016867195
Validation loss: 2.4777196813410436

Epoch: 6| Step: 11
Training loss: 2.2105405201730424
Validation loss: 2.504859093764449

Epoch: 6| Step: 12
Training loss: 2.317586819895886
Validation loss: 2.5015000652321184

Epoch: 6| Step: 13
Training loss: 2.4057062698951714
Validation loss: 2.493226983476843

Epoch: 301| Step: 0
Training loss: 2.562489486300602
Validation loss: 2.47389335888101

Epoch: 6| Step: 1
Training loss: 2.551216030016181
Validation loss: 2.484608672303763

Epoch: 6| Step: 2
Training loss: 2.44917328958134
Validation loss: 2.477449157401102

Epoch: 6| Step: 3
Training loss: 2.37789459515944
Validation loss: 2.4712904723266482

Epoch: 6| Step: 4
Training loss: 2.170931823995409
Validation loss: 2.4571565401482722

Epoch: 6| Step: 5
Training loss: 2.156342988151479
Validation loss: 2.4956675397700847

Epoch: 6| Step: 6
Training loss: 2.1196877893838257
Validation loss: 2.475988837454509

Epoch: 6| Step: 7
Training loss: 2.51162876187457
Validation loss: 2.49931930996539

Epoch: 6| Step: 8
Training loss: 2.5047312789203655
Validation loss: 2.443431105456375

Epoch: 6| Step: 9
Training loss: 2.269148093362484
Validation loss: 2.481731332917526

Epoch: 6| Step: 10
Training loss: 2.500193969831098
Validation loss: 2.485536330325877

Epoch: 6| Step: 11
Training loss: 2.528782336357816
Validation loss: 2.484610408837057

Epoch: 6| Step: 12
Training loss: 2.6890447745661623
Validation loss: 2.4622619065308733

Epoch: 6| Step: 13
Training loss: 2.007150147429173
Validation loss: 2.475866899450205

Epoch: 302| Step: 0
Training loss: 1.6817789231341371
Validation loss: 2.4749938814782197

Epoch: 6| Step: 1
Training loss: 2.043642826361667
Validation loss: 2.463457888457984

Epoch: 6| Step: 2
Training loss: 2.203679441121969
Validation loss: 2.4699396320409144

Epoch: 6| Step: 3
Training loss: 2.6331517032162957
Validation loss: 2.483544331855994

Epoch: 6| Step: 4
Training loss: 2.1730567406638355
Validation loss: 2.4510383350271976

Epoch: 6| Step: 5
Training loss: 3.0108532088575966
Validation loss: 2.4629395919116086

Epoch: 6| Step: 6
Training loss: 2.6974042211857503
Validation loss: 2.4564331939915753

Epoch: 6| Step: 7
Training loss: 2.944915501883275
Validation loss: 2.4728802417499365

Epoch: 6| Step: 8
Training loss: 2.7396110755666454
Validation loss: 2.518645226176707

Epoch: 6| Step: 9
Training loss: 2.380959712880063
Validation loss: 2.50033073903713

Epoch: 6| Step: 10
Training loss: 2.4042175428625843
Validation loss: 2.4596899352014967

Epoch: 6| Step: 11
Training loss: 2.166355942184318
Validation loss: 2.4513611030888254

Epoch: 6| Step: 12
Training loss: 2.2323891945196532
Validation loss: 2.462431905473437

Epoch: 6| Step: 13
Training loss: 1.795352124606066
Validation loss: 2.4980632288811186

Epoch: 303| Step: 0
Training loss: 2.3080983929478225
Validation loss: 2.439507506777439

Epoch: 6| Step: 1
Training loss: 2.5769478624818367
Validation loss: 2.4491306660742556

Epoch: 6| Step: 2
Training loss: 2.230776363078081
Validation loss: 2.4483854345495732

Epoch: 6| Step: 3
Training loss: 2.1622094990084886
Validation loss: 2.4786882077557593

Epoch: 6| Step: 4
Training loss: 2.14674249542762
Validation loss: 2.5174533765322633

Epoch: 6| Step: 5
Training loss: 2.27365646094404
Validation loss: 2.449180898301765

Epoch: 6| Step: 6
Training loss: 2.3592428208266463
Validation loss: 2.495252623999232

Epoch: 6| Step: 7
Training loss: 2.725406084513001
Validation loss: 2.4777953073470504

Epoch: 6| Step: 8
Training loss: 2.6725047300776845
Validation loss: 2.4764970919780023

Epoch: 6| Step: 9
Training loss: 2.3574799259500225
Validation loss: 2.4494666734753148

Epoch: 6| Step: 10
Training loss: 2.3209503016511093
Validation loss: 2.484717336915547

Epoch: 6| Step: 11
Training loss: 2.918498653723292
Validation loss: 2.4910328107552084

Epoch: 6| Step: 12
Training loss: 2.0626977767907944
Validation loss: 2.4815193243975506

Epoch: 6| Step: 13
Training loss: 2.1487365514524126
Validation loss: 2.4405269076470635

Epoch: 304| Step: 0
Training loss: 2.295659372854945
Validation loss: 2.474400034377972

Epoch: 6| Step: 1
Training loss: 2.3882376524914415
Validation loss: 2.4599015385788428

Epoch: 6| Step: 2
Training loss: 2.4648087343367626
Validation loss: 2.486157565201236

Epoch: 6| Step: 3
Training loss: 2.1299385577728986
Validation loss: 2.4542449094099466

Epoch: 6| Step: 4
Training loss: 2.6265154505866533
Validation loss: 2.449746185550822

Epoch: 6| Step: 5
Training loss: 2.06543915297772
Validation loss: 2.4654247828419225

Epoch: 6| Step: 6
Training loss: 2.1682772030359314
Validation loss: 2.479388002639124

Epoch: 6| Step: 7
Training loss: 3.1022905316828
Validation loss: 2.4710466619384754

Epoch: 6| Step: 8
Training loss: 1.9421693464982128
Validation loss: 2.4556505265928195

Epoch: 6| Step: 9
Training loss: 2.6510600830717506
Validation loss: 2.4400446072654387

Epoch: 6| Step: 10
Training loss: 2.6241574524707287
Validation loss: 2.476444482635875

Epoch: 6| Step: 11
Training loss: 2.109777002528165
Validation loss: 2.5005562460850497

Epoch: 6| Step: 12
Training loss: 2.200404225406775
Validation loss: 2.4554971606600566

Epoch: 6| Step: 13
Training loss: 1.7620690921482478
Validation loss: 2.479378164288282

Epoch: 305| Step: 0
Training loss: 2.7458086150643353
Validation loss: 2.5113301848720044

Epoch: 6| Step: 1
Training loss: 2.4359848008080975
Validation loss: 2.4402412259350865

Epoch: 6| Step: 2
Training loss: 2.4520426506701645
Validation loss: 2.477143465592799

Epoch: 6| Step: 3
Training loss: 2.6642957121048947
Validation loss: 2.469414896258236

Epoch: 6| Step: 4
Training loss: 2.129594324190929
Validation loss: 2.488390056095827

Epoch: 6| Step: 5
Training loss: 2.4055963594511223
Validation loss: 2.4004299840413115

Epoch: 6| Step: 6
Training loss: 2.32228061424147
Validation loss: 2.5017631855526035

Epoch: 6| Step: 7
Training loss: 2.329088664469004
Validation loss: 2.4925344115581223

Epoch: 6| Step: 8
Training loss: 2.668912617287389
Validation loss: 2.4474623878886606

Epoch: 6| Step: 9
Training loss: 1.9403728597735226
Validation loss: 2.473152071923477

Epoch: 6| Step: 10
Training loss: 2.4879733723077213
Validation loss: 2.4661284526223697

Epoch: 6| Step: 11
Training loss: 2.3662050934332495
Validation loss: 2.443223813103241

Epoch: 6| Step: 12
Training loss: 2.692175236262785
Validation loss: 2.457868014416795

Epoch: 6| Step: 13
Training loss: 1.7989486670515091
Validation loss: 2.461058902692119

Epoch: 306| Step: 0
Training loss: 2.5325822503157243
Validation loss: 2.4774550598385834

Epoch: 6| Step: 1
Training loss: 2.277820883965891
Validation loss: 2.4679918268557395

Epoch: 6| Step: 2
Training loss: 2.2452223173651493
Validation loss: 2.4565241437551264

Epoch: 6| Step: 3
Training loss: 2.5861469656478704
Validation loss: 2.4864600418640133

Epoch: 6| Step: 4
Training loss: 2.9033470832624664
Validation loss: 2.4621625422622926

Epoch: 6| Step: 5
Training loss: 2.411155357579895
Validation loss: 2.471790651514435

Epoch: 6| Step: 6
Training loss: 2.6971154416292333
Validation loss: 2.4732248341726244

Epoch: 6| Step: 7
Training loss: 2.5065398507640007
Validation loss: 2.4682669945271156

Epoch: 6| Step: 8
Training loss: 2.187311327835878
Validation loss: 2.4774727122232854

Epoch: 6| Step: 9
Training loss: 1.955089100321966
Validation loss: 2.474261208060339

Epoch: 6| Step: 10
Training loss: 1.828073745400285
Validation loss: 2.474792997342957

Epoch: 6| Step: 11
Training loss: 2.7536049535978315
Validation loss: 2.4756314752027135

Epoch: 6| Step: 12
Training loss: 2.168603532665865
Validation loss: 2.484689234614403

Epoch: 6| Step: 13
Training loss: 2.4581908384070927
Validation loss: 2.477695946761767

Epoch: 307| Step: 0
Training loss: 2.908432991776047
Validation loss: 2.4638906531439306

Epoch: 6| Step: 1
Training loss: 2.1396401917487164
Validation loss: 2.478925264752003

Epoch: 6| Step: 2
Training loss: 2.345194460486191
Validation loss: 2.4547515695557434

Epoch: 6| Step: 3
Training loss: 1.7665100580053825
Validation loss: 2.439414571086997

Epoch: 6| Step: 4
Training loss: 2.2541603196407896
Validation loss: 2.4686774101296822

Epoch: 6| Step: 5
Training loss: 2.5819413116073093
Validation loss: 2.4835617510272385

Epoch: 6| Step: 6
Training loss: 2.275085271557725
Validation loss: 2.4523705223024064

Epoch: 6| Step: 7
Training loss: 1.9999017691330843
Validation loss: 2.478652057060214

Epoch: 6| Step: 8
Training loss: 2.954485060334664
Validation loss: 2.504968538888066

Epoch: 6| Step: 9
Training loss: 2.525426877640024
Validation loss: 2.507962702545053

Epoch: 6| Step: 10
Training loss: 2.739195840987943
Validation loss: 2.462302071534409

Epoch: 6| Step: 11
Training loss: 1.7125554904161548
Validation loss: 2.458900387998016

Epoch: 6| Step: 12
Training loss: 2.5103174931312573
Validation loss: 2.4803451020216176

Epoch: 6| Step: 13
Training loss: 2.820259579825094
Validation loss: 2.483782999483666

Epoch: 308| Step: 0
Training loss: 2.3585634162016142
Validation loss: 2.487422726043556

Epoch: 6| Step: 1
Training loss: 1.5524937473238778
Validation loss: 2.491116970972414

Epoch: 6| Step: 2
Training loss: 1.907808385996089
Validation loss: 2.488200455742835

Epoch: 6| Step: 3
Training loss: 2.0651193805020323
Validation loss: 2.450270517334804

Epoch: 6| Step: 4
Training loss: 2.6153129582923746
Validation loss: 2.4673121346003524

Epoch: 6| Step: 5
Training loss: 2.3877690028889367
Validation loss: 2.455341947519443

Epoch: 6| Step: 6
Training loss: 2.8425020267207586
Validation loss: 2.488201518001676

Epoch: 6| Step: 7
Training loss: 2.3109875449848167
Validation loss: 2.476279265855341

Epoch: 6| Step: 8
Training loss: 2.446887109563296
Validation loss: 2.46327439424722

Epoch: 6| Step: 9
Training loss: 2.2632187265510653
Validation loss: 2.4578604941280116

Epoch: 6| Step: 10
Training loss: 2.005495270515444
Validation loss: 2.447486600535309

Epoch: 6| Step: 11
Training loss: 2.7578242574554612
Validation loss: 2.4914497093226484

Epoch: 6| Step: 12
Training loss: 2.8983878159698877
Validation loss: 2.4332039087151465

Epoch: 6| Step: 13
Training loss: 2.4655789640550396
Validation loss: 2.4503352815396764

Epoch: 309| Step: 0
Training loss: 2.1937615288325536
Validation loss: 2.49778409807586

Epoch: 6| Step: 1
Training loss: 2.0973724230045625
Validation loss: 2.466840579607109

Epoch: 6| Step: 2
Training loss: 2.062923734659615
Validation loss: 2.439472917719671

Epoch: 6| Step: 3
Training loss: 1.9106256887168254
Validation loss: 2.4712612836969194

Epoch: 6| Step: 4
Training loss: 2.6157253451224047
Validation loss: 2.4497042386389456

Epoch: 6| Step: 5
Training loss: 2.4867889384803727
Validation loss: 2.5002568225818593

Epoch: 6| Step: 6
Training loss: 3.0325240770910966
Validation loss: 2.4984111043626247

Epoch: 6| Step: 7
Training loss: 2.2112644344740686
Validation loss: 2.4793536700889414

Epoch: 6| Step: 8
Training loss: 2.6434883078339473
Validation loss: 2.4680871059240586

Epoch: 6| Step: 9
Training loss: 2.284794496773522
Validation loss: 2.491364892517871

Epoch: 6| Step: 10
Training loss: 2.460911051290164
Validation loss: 2.4951184985424666

Epoch: 6| Step: 11
Training loss: 2.678629763740007
Validation loss: 2.479962514272487

Epoch: 6| Step: 12
Training loss: 1.740344385508132
Validation loss: 2.481012021736262

Epoch: 6| Step: 13
Training loss: 2.703318506271523
Validation loss: 2.4753863576587496

Epoch: 310| Step: 0
Training loss: 2.4406660010573016
Validation loss: 2.5084743158699334

Epoch: 6| Step: 1
Training loss: 2.5127497762388282
Validation loss: 2.445332236906359

Epoch: 6| Step: 2
Training loss: 2.265083090329671
Validation loss: 2.4755224469716106

Epoch: 6| Step: 3
Training loss: 1.996149767782032
Validation loss: 2.465816220353519

Epoch: 6| Step: 4
Training loss: 2.8256016217238975
Validation loss: 2.4599263588232287

Epoch: 6| Step: 5
Training loss: 2.1934617681882256
Validation loss: 2.498523407351434

Epoch: 6| Step: 6
Training loss: 2.317176832198977
Validation loss: 2.457653194201728

Epoch: 6| Step: 7
Training loss: 2.7057893004112894
Validation loss: 2.4687620022223546

Epoch: 6| Step: 8
Training loss: 3.017828892037892
Validation loss: 2.446543685305937

Epoch: 6| Step: 9
Training loss: 1.9301540413097096
Validation loss: 2.483845453160584

Epoch: 6| Step: 10
Training loss: 2.6194956152745097
Validation loss: 2.4725211828683875

Epoch: 6| Step: 11
Training loss: 2.132913663931202
Validation loss: 2.4787141585672656

Epoch: 6| Step: 12
Training loss: 1.8693058654387844
Validation loss: 2.4887961428020455

Epoch: 6| Step: 13
Training loss: 2.5251702657267776
Validation loss: 2.5007298685947443

Epoch: 311| Step: 0
Training loss: 1.9245196498661548
Validation loss: 2.4593962677300314

Epoch: 6| Step: 1
Training loss: 2.3030787138512707
Validation loss: 2.4827650481065704

Epoch: 6| Step: 2
Training loss: 3.0547205930512416
Validation loss: 2.496299892152082

Epoch: 6| Step: 3
Training loss: 2.5104259527728496
Validation loss: 2.4766525780622626

Epoch: 6| Step: 4
Training loss: 2.6086874415779824
Validation loss: 2.467995497815736

Epoch: 6| Step: 5
Training loss: 2.628143970906374
Validation loss: 2.460346495380775

Epoch: 6| Step: 6
Training loss: 1.653204078441493
Validation loss: 2.4428166362548063

Epoch: 6| Step: 7
Training loss: 2.019204679607041
Validation loss: 2.472071359331412

Epoch: 6| Step: 8
Training loss: 1.9722753780287794
Validation loss: 2.4924497633212335

Epoch: 6| Step: 9
Training loss: 2.5848716544601857
Validation loss: 2.435775971710804

Epoch: 6| Step: 10
Training loss: 2.429771004864014
Validation loss: 2.5029938107347487

Epoch: 6| Step: 11
Training loss: 2.5698978356267554
Validation loss: 2.484203536765483

Epoch: 6| Step: 12
Training loss: 1.693386337259217
Validation loss: 2.5024312364565184

Epoch: 6| Step: 13
Training loss: 3.034287651211231
Validation loss: 2.4893935265879876

Epoch: 312| Step: 0
Training loss: 2.7048974372754317
Validation loss: 2.4872651602156797

Epoch: 6| Step: 1
Training loss: 2.600286864081095
Validation loss: 2.5071837660270195

Epoch: 6| Step: 2
Training loss: 2.15938174830485
Validation loss: 2.468755177640022

Epoch: 6| Step: 3
Training loss: 2.789680134748682
Validation loss: 2.453742988287253

Epoch: 6| Step: 4
Training loss: 1.7936816159282136
Validation loss: 2.4712490643505065

Epoch: 6| Step: 5
Training loss: 2.0559289467107904
Validation loss: 2.487872689625928

Epoch: 6| Step: 6
Training loss: 1.7434795703177821
Validation loss: 2.494814013035252

Epoch: 6| Step: 7
Training loss: 3.105400113480891
Validation loss: 2.4210740049965658

Epoch: 6| Step: 8
Training loss: 2.353510357229582
Validation loss: 2.4836079486583156

Epoch: 6| Step: 9
Training loss: 2.5659346473797853
Validation loss: 2.477005539251254

Epoch: 6| Step: 10
Training loss: 1.8790250808032658
Validation loss: 2.4948200007826262

Epoch: 6| Step: 11
Training loss: 2.0581123614393775
Validation loss: 2.4343994842022134

Epoch: 6| Step: 12
Training loss: 2.3787733520067196
Validation loss: 2.4593328179082885

Epoch: 6| Step: 13
Training loss: 2.6127942918814395
Validation loss: 2.444645799907548

Epoch: 313| Step: 0
Training loss: 1.8554205637246335
Validation loss: 2.4839012131236564

Epoch: 6| Step: 1
Training loss: 2.524773306159893
Validation loss: 2.4479302516988803

Epoch: 6| Step: 2
Training loss: 2.3188194326200113
Validation loss: 2.4877598223695583

Epoch: 6| Step: 3
Training loss: 2.6400443798727613
Validation loss: 2.476889720688551

Epoch: 6| Step: 4
Training loss: 2.7344817222476805
Validation loss: 2.4757432779251065

Epoch: 6| Step: 5
Training loss: 2.516502085308361
Validation loss: 2.4880964727288415

Epoch: 6| Step: 6
Training loss: 2.696621076845277
Validation loss: 2.4694487461770076

Epoch: 6| Step: 7
Training loss: 2.1791392084534977
Validation loss: 2.4966084291181208

Epoch: 6| Step: 8
Training loss: 2.7083575810056133
Validation loss: 2.4855103104684972

Epoch: 6| Step: 9
Training loss: 1.560649386250935
Validation loss: 2.466646412741607

Epoch: 6| Step: 10
Training loss: 2.5054209110747023
Validation loss: 2.5015869436217404

Epoch: 6| Step: 11
Training loss: 2.174635306289896
Validation loss: 2.460930237024453

Epoch: 6| Step: 12
Training loss: 2.3105596701621627
Validation loss: 2.480175398033532

Epoch: 6| Step: 13
Training loss: 2.230214440166505
Validation loss: 2.4468528322751575

Epoch: 314| Step: 0
Training loss: 2.3974698876487723
Validation loss: 2.477702020369025

Epoch: 6| Step: 1
Training loss: 2.1041194293570995
Validation loss: 2.4908507417022134

Epoch: 6| Step: 2
Training loss: 2.6781779227263467
Validation loss: 2.466901867014984

Epoch: 6| Step: 3
Training loss: 2.516090397404063
Validation loss: 2.475780464126962

Epoch: 6| Step: 4
Training loss: 1.5600714311384964
Validation loss: 2.489974999010502

Epoch: 6| Step: 5
Training loss: 1.4909355943612062
Validation loss: 2.4554425211907454

Epoch: 6| Step: 6
Training loss: 1.9904236408718625
Validation loss: 2.4541869724689316

Epoch: 6| Step: 7
Training loss: 2.9961440419100427
Validation loss: 2.465312109737082

Epoch: 6| Step: 8
Training loss: 2.9368434740490432
Validation loss: 2.4771294186094535

Epoch: 6| Step: 9
Training loss: 2.1174094446268694
Validation loss: 2.468311794814882

Epoch: 6| Step: 10
Training loss: 2.1877989428475715
Validation loss: 2.4706789598471195

Epoch: 6| Step: 11
Training loss: 2.327008280373379
Validation loss: 2.479754119732233

Epoch: 6| Step: 12
Training loss: 3.025594252548528
Validation loss: 2.4497570585821786

Epoch: 6| Step: 13
Training loss: 2.3298223056078777
Validation loss: 2.44987370569626

Epoch: 315| Step: 0
Training loss: 2.8155059010236028
Validation loss: 2.4783550097827436

Epoch: 6| Step: 1
Training loss: 2.518101202084704
Validation loss: 2.4673418125750413

Epoch: 6| Step: 2
Training loss: 2.075302386943507
Validation loss: 2.4523503925274674

Epoch: 6| Step: 3
Training loss: 2.329239341771232
Validation loss: 2.492093503428415

Epoch: 6| Step: 4
Training loss: 2.8233436927770494
Validation loss: 2.475348569509215

Epoch: 6| Step: 5
Training loss: 2.198069839805005
Validation loss: 2.505301866841975

Epoch: 6| Step: 6
Training loss: 1.9734802942174083
Validation loss: 2.477224407285676

Epoch: 6| Step: 7
Training loss: 2.163489751387307
Validation loss: 2.477739181829855

Epoch: 6| Step: 8
Training loss: 1.9271955577718385
Validation loss: 2.4984140359526203

Epoch: 6| Step: 9
Training loss: 2.6024647657371034
Validation loss: 2.450339395877992

Epoch: 6| Step: 10
Training loss: 2.2673281877693547
Validation loss: 2.4598161623627153

Epoch: 6| Step: 11
Training loss: 2.3841362949940406
Validation loss: 2.470403601114695

Epoch: 6| Step: 12
Training loss: 2.1627916260698465
Validation loss: 2.50094591826787

Epoch: 6| Step: 13
Training loss: 2.6919043181137843
Validation loss: 2.4884461500075177

Epoch: 316| Step: 0
Training loss: 2.3444660364555827
Validation loss: 2.5056356650972296

Epoch: 6| Step: 1
Training loss: 2.304084310962584
Validation loss: 2.4786732433296623

Epoch: 6| Step: 2
Training loss: 2.282234293084361
Validation loss: 2.4480779838600983

Epoch: 6| Step: 3
Training loss: 2.389182459611945
Validation loss: 2.5075377623452613

Epoch: 6| Step: 4
Training loss: 2.493875248949758
Validation loss: 2.4491391641428475

Epoch: 6| Step: 5
Training loss: 2.227494536627546
Validation loss: 2.4601635433545037

Epoch: 6| Step: 6
Training loss: 2.8909750236770315
Validation loss: 2.4615252788870956

Epoch: 6| Step: 7
Training loss: 2.3077014935139477
Validation loss: 2.469335052380313

Epoch: 6| Step: 8
Training loss: 2.3870241060354185
Validation loss: 2.485365896185456

Epoch: 6| Step: 9
Training loss: 2.11868492184059
Validation loss: 2.5183685110119454

Epoch: 6| Step: 10
Training loss: 1.665899704257731
Validation loss: 2.5048768938393597

Epoch: 6| Step: 11
Training loss: 2.3644553554593086
Validation loss: 2.443985730941741

Epoch: 6| Step: 12
Training loss: 2.5945788977847317
Validation loss: 2.4516603512684485

Epoch: 6| Step: 13
Training loss: 2.4292864207809406
Validation loss: 2.472254220321376

Epoch: 317| Step: 0
Training loss: 2.587945348772818
Validation loss: 2.4483414046936054

Epoch: 6| Step: 1
Training loss: 2.3566436115020237
Validation loss: 2.4929741352083923

Epoch: 6| Step: 2
Training loss: 1.9730204935222764
Validation loss: 2.4865182062940967

Epoch: 6| Step: 3
Training loss: 2.2261438879719417
Validation loss: 2.454366226379693

Epoch: 6| Step: 4
Training loss: 2.321656630914946
Validation loss: 2.4479081107683665

Epoch: 6| Step: 5
Training loss: 2.1238752642282193
Validation loss: 2.461458067323978

Epoch: 6| Step: 6
Training loss: 2.349917016694752
Validation loss: 2.469873085918036

Epoch: 6| Step: 7
Training loss: 2.4315918944351584
Validation loss: 2.4585723819821608

Epoch: 6| Step: 8
Training loss: 2.2537038834561693
Validation loss: 2.4832205969038372

Epoch: 6| Step: 9
Training loss: 2.1147316064824997
Validation loss: 2.4762892065871123

Epoch: 6| Step: 10
Training loss: 2.6587715297987633
Validation loss: 2.48662244603643

Epoch: 6| Step: 11
Training loss: 2.6786193498230273
Validation loss: 2.4703014291883894

Epoch: 6| Step: 12
Training loss: 2.6853485367100562
Validation loss: 2.4523711620702757

Epoch: 6| Step: 13
Training loss: 2.407302824286422
Validation loss: 2.468043623346976

Epoch: 318| Step: 0
Training loss: 2.810248087998829
Validation loss: 2.4710360164198657

Epoch: 6| Step: 1
Training loss: 2.666873824497247
Validation loss: 2.475874161062457

Epoch: 6| Step: 2
Training loss: 2.7148172281191023
Validation loss: 2.4586593602230984

Epoch: 6| Step: 3
Training loss: 2.566366395684411
Validation loss: 2.469326553736787

Epoch: 6| Step: 4
Training loss: 1.9820039656298216
Validation loss: 2.48050417297872

Epoch: 6| Step: 5
Training loss: 2.2713936863882607
Validation loss: 2.471426596314358

Epoch: 6| Step: 6
Training loss: 1.7147044340298252
Validation loss: 2.4455430531086404

Epoch: 6| Step: 7
Training loss: 1.9111584489649651
Validation loss: 2.48070405001171

Epoch: 6| Step: 8
Training loss: 1.5553164336170306
Validation loss: 2.442372049392405

Epoch: 6| Step: 9
Training loss: 2.4621831738515922
Validation loss: 2.458029360268525

Epoch: 6| Step: 10
Training loss: 1.8726445980290398
Validation loss: 2.479575557670873

Epoch: 6| Step: 11
Training loss: 2.292457120571332
Validation loss: 2.4463814058887396

Epoch: 6| Step: 12
Training loss: 2.7585889895648443
Validation loss: 2.4956701910659245

Epoch: 6| Step: 13
Training loss: 3.0602332309275613
Validation loss: 2.452700569133568

Epoch: 319| Step: 0
Training loss: 1.9117959440708485
Validation loss: 2.4746989264176764

Epoch: 6| Step: 1
Training loss: 1.9132745483330913
Validation loss: 2.4415065314586

Epoch: 6| Step: 2
Training loss: 2.5608635305785654
Validation loss: 2.4615058374175574

Epoch: 6| Step: 3
Training loss: 3.0286569417463864
Validation loss: 2.474413445174125

Epoch: 6| Step: 4
Training loss: 2.122144912833922
Validation loss: 2.451079153567216

Epoch: 6| Step: 5
Training loss: 1.8903676007552361
Validation loss: 2.505875724443928

Epoch: 6| Step: 6
Training loss: 2.1335554121575626
Validation loss: 2.492314636359587

Epoch: 6| Step: 7
Training loss: 2.0617398103749207
Validation loss: 2.458465317829035

Epoch: 6| Step: 8
Training loss: 2.1258636290413846
Validation loss: 2.4856022415407035

Epoch: 6| Step: 9
Training loss: 2.875578283118251
Validation loss: 2.436242097941882

Epoch: 6| Step: 10
Training loss: 2.067993246927063
Validation loss: 2.4615619878205695

Epoch: 6| Step: 11
Training loss: 2.215299260092771
Validation loss: 2.457599541500825

Epoch: 6| Step: 12
Training loss: 3.194411619450822
Validation loss: 2.4609045174597752

Epoch: 6| Step: 13
Training loss: 2.3073303586817033
Validation loss: 2.4664188156003513

Epoch: 320| Step: 0
Training loss: 1.5001182509541384
Validation loss: 2.4948294278333747

Epoch: 6| Step: 1
Training loss: 2.7097254085429707
Validation loss: 2.431983728176003

Epoch: 6| Step: 2
Training loss: 2.434744034929545
Validation loss: 2.489276114457324

Epoch: 6| Step: 3
Training loss: 1.8102209956312771
Validation loss: 2.5068157889733147

Epoch: 6| Step: 4
Training loss: 1.7666555452296644
Validation loss: 2.4847298222541916

Epoch: 6| Step: 5
Training loss: 3.1126415787328052
Validation loss: 2.454495699356885

Epoch: 6| Step: 6
Training loss: 1.603955300348026
Validation loss: 2.5106785822735955

Epoch: 6| Step: 7
Training loss: 2.2360690437398354
Validation loss: 2.4428110321269756

Epoch: 6| Step: 8
Training loss: 2.2220930247367
Validation loss: 2.4945286571416667

Epoch: 6| Step: 9
Training loss: 3.374647616604567
Validation loss: 2.453995731369804

Epoch: 6| Step: 10
Training loss: 2.3800166502939155
Validation loss: 2.4985538442776534

Epoch: 6| Step: 11
Training loss: 2.2450495407313698
Validation loss: 2.4828342131339087

Epoch: 6| Step: 12
Training loss: 2.258505533369953
Validation loss: 2.4927991811472023

Epoch: 6| Step: 13
Training loss: 2.51798502472658
Validation loss: 2.4779637104122214

Epoch: 321| Step: 0
Training loss: 2.2229471401806005
Validation loss: 2.4946194409316074

Epoch: 6| Step: 1
Training loss: 1.9161579728595253
Validation loss: 2.5066440239295797

Epoch: 6| Step: 2
Training loss: 2.0442499698295094
Validation loss: 2.459353303324323

Epoch: 6| Step: 3
Training loss: 2.504588684303086
Validation loss: 2.5026666903330277

Epoch: 6| Step: 4
Training loss: 2.658366000015623
Validation loss: 2.468223609117689

Epoch: 6| Step: 5
Training loss: 2.200334878923374
Validation loss: 2.467153509368097

Epoch: 6| Step: 6
Training loss: 2.144754811569777
Validation loss: 2.496341791403387

Epoch: 6| Step: 7
Training loss: 2.4310144070034028
Validation loss: 2.449458388482581

Epoch: 6| Step: 8
Training loss: 2.907088292133013
Validation loss: 2.454148071303296

Epoch: 6| Step: 9
Training loss: 2.3046368092280245
Validation loss: 2.447411078659963

Epoch: 6| Step: 10
Training loss: 2.7148304890919386
Validation loss: 2.441945790516316

Epoch: 6| Step: 11
Training loss: 1.8559267783766569
Validation loss: 2.464631276967236

Epoch: 6| Step: 12
Training loss: 2.41273496600317
Validation loss: 2.451286587974295

Epoch: 6| Step: 13
Training loss: 3.0005756461675857
Validation loss: 2.487579283262672

Epoch: 322| Step: 0
Training loss: 1.6792806443119743
Validation loss: 2.4564902138382667

Epoch: 6| Step: 1
Training loss: 2.100037883235193
Validation loss: 2.4462046878306647

Epoch: 6| Step: 2
Training loss: 2.0746174517132956
Validation loss: 2.48405249564835

Epoch: 6| Step: 3
Training loss: 1.813151406927888
Validation loss: 2.4743294679831185

Epoch: 6| Step: 4
Training loss: 2.2782311518130265
Validation loss: 2.452258243592511

Epoch: 6| Step: 5
Training loss: 2.7260049069445103
Validation loss: 2.490495934033786

Epoch: 6| Step: 6
Training loss: 2.579047298185529
Validation loss: 2.5016850852918164

Epoch: 6| Step: 7
Training loss: 2.4075704147166386
Validation loss: 2.4946281103240073

Epoch: 6| Step: 8
Training loss: 2.330256363349987
Validation loss: 2.4835090172726315

Epoch: 6| Step: 9
Training loss: 2.3088880632113677
Validation loss: 2.4708405118902066

Epoch: 6| Step: 10
Training loss: 2.827806317831367
Validation loss: 2.4617552758468437

Epoch: 6| Step: 11
Training loss: 2.917078170811757
Validation loss: 2.4934315983182316

Epoch: 6| Step: 12
Training loss: 2.39248305774139
Validation loss: 2.4876464929440116

Epoch: 6| Step: 13
Training loss: 2.064010529365592
Validation loss: 2.447931284829196

Epoch: 323| Step: 0
Training loss: 2.589698568794364
Validation loss: 2.4390145259281786

Epoch: 6| Step: 1
Training loss: 2.1406548491018813
Validation loss: 2.4724884263961635

Epoch: 6| Step: 2
Training loss: 2.0791986889816325
Validation loss: 2.467608287401831

Epoch: 6| Step: 3
Training loss: 2.416297643263819
Validation loss: 2.4687806586151484

Epoch: 6| Step: 4
Training loss: 2.7275838168768303
Validation loss: 2.4749601239733137

Epoch: 6| Step: 5
Training loss: 2.196514411516018
Validation loss: 2.462821876282178

Epoch: 6| Step: 6
Training loss: 2.3465841831897674
Validation loss: 2.418610158022853

Epoch: 6| Step: 7
Training loss: 2.4573655136113906
Validation loss: 2.48297911125435

Epoch: 6| Step: 8
Training loss: 2.2578581175106183
Validation loss: 2.4687523614039564

Epoch: 6| Step: 9
Training loss: 1.948472669539468
Validation loss: 2.438141030958702

Epoch: 6| Step: 10
Training loss: 2.310982489776632
Validation loss: 2.474142721371794

Epoch: 6| Step: 11
Training loss: 2.6628542310732715
Validation loss: 2.467373079888078

Epoch: 6| Step: 12
Training loss: 2.3785174573570806
Validation loss: 2.460083462784699

Epoch: 6| Step: 13
Training loss: 1.9764505588360761
Validation loss: 2.483372546993939

Epoch: 324| Step: 0
Training loss: 2.3253253257960647
Validation loss: 2.4976301820564712

Epoch: 6| Step: 1
Training loss: 2.3979217272941775
Validation loss: 2.4452381512600434

Epoch: 6| Step: 2
Training loss: 2.587592848434154
Validation loss: 2.4663996029192887

Epoch: 6| Step: 3
Training loss: 2.1524508634800577
Validation loss: 2.448811434607314

Epoch: 6| Step: 4
Training loss: 1.7402432802188674
Validation loss: 2.468814905595104

Epoch: 6| Step: 5
Training loss: 2.173699909003788
Validation loss: 2.502146345069526

Epoch: 6| Step: 6
Training loss: 2.1328246846392447
Validation loss: 2.4837243880626336

Epoch: 6| Step: 7
Training loss: 2.0994703805736936
Validation loss: 2.457606534756674

Epoch: 6| Step: 8
Training loss: 2.3175624387162976
Validation loss: 2.4949815818145766

Epoch: 6| Step: 9
Training loss: 2.54641532260121
Validation loss: 2.4660808515991937

Epoch: 6| Step: 10
Training loss: 2.688059194185521
Validation loss: 2.4442550142815005

Epoch: 6| Step: 11
Training loss: 3.12531690897982
Validation loss: 2.5047460758941984

Epoch: 6| Step: 12
Training loss: 2.2986097985680383
Validation loss: 2.450039316681845

Epoch: 6| Step: 13
Training loss: 1.3154731409135751
Validation loss: 2.454112819462039

Epoch: 325| Step: 0
Training loss: 1.681250692034157
Validation loss: 2.4480736421424036

Epoch: 6| Step: 1
Training loss: 1.9963575812142305
Validation loss: 2.4527634212522

Epoch: 6| Step: 2
Training loss: 3.020347731572757
Validation loss: 2.463243049929091

Epoch: 6| Step: 3
Training loss: 1.794371810160864
Validation loss: 2.483674503013009

Epoch: 6| Step: 4
Training loss: 2.354159217305058
Validation loss: 2.4847479171654974

Epoch: 6| Step: 5
Training loss: 2.6633440318586996
Validation loss: 2.479020612231381

Epoch: 6| Step: 6
Training loss: 2.6242496689669683
Validation loss: 2.453896918308143

Epoch: 6| Step: 7
Training loss: 2.899810876927708
Validation loss: 2.445775804553914

Epoch: 6| Step: 8
Training loss: 1.8795149168217693
Validation loss: 2.4627364930449

Epoch: 6| Step: 9
Training loss: 2.067500670131958
Validation loss: 2.498627428406555

Epoch: 6| Step: 10
Training loss: 2.4020961230671736
Validation loss: 2.4896064596242895

Epoch: 6| Step: 11
Training loss: 1.7108317303297755
Validation loss: 2.484334914269158

Epoch: 6| Step: 12
Training loss: 1.646902011020078
Validation loss: 2.44897741420224

Epoch: 6| Step: 13
Training loss: 3.8892123557094096
Validation loss: 2.433733049521351

Epoch: 326| Step: 0
Training loss: 2.3053235227060953
Validation loss: 2.4719094002891455

Epoch: 6| Step: 1
Training loss: 2.1566768721270746
Validation loss: 2.4537606931967257

Epoch: 6| Step: 2
Training loss: 2.501230890524184
Validation loss: 2.4468904088160026

Epoch: 6| Step: 3
Training loss: 2.5688846413626014
Validation loss: 2.4766477233295197

Epoch: 6| Step: 4
Training loss: 2.5383442493720807
Validation loss: 2.466058387630597

Epoch: 6| Step: 5
Training loss: 2.560158753241071
Validation loss: 2.4281566951704576

Epoch: 6| Step: 6
Training loss: 2.0542552188669116
Validation loss: 2.4514916477160518

Epoch: 6| Step: 7
Training loss: 2.0217078633630368
Validation loss: 2.498559051474888

Epoch: 6| Step: 8
Training loss: 2.290134530426095
Validation loss: 2.470098473781846

Epoch: 6| Step: 9
Training loss: 2.2727835327467623
Validation loss: 2.4662121300623747

Epoch: 6| Step: 10
Training loss: 2.064498280127916
Validation loss: 2.4680484124250825

Epoch: 6| Step: 11
Training loss: 2.223858606989625
Validation loss: 2.4870142416189838

Epoch: 6| Step: 12
Training loss: 2.4384441014559455
Validation loss: 2.483086832289514

Epoch: 6| Step: 13
Training loss: 2.495286790193408
Validation loss: 2.4680814407625498

Epoch: 327| Step: 0
Training loss: 2.2447469637521316
Validation loss: 2.450920374139682

Epoch: 6| Step: 1
Training loss: 2.023960119676284
Validation loss: 2.47564817857087

Epoch: 6| Step: 2
Training loss: 2.0864172680210413
Validation loss: 2.4848599111902154

Epoch: 6| Step: 3
Training loss: 2.9505246940423677
Validation loss: 2.4464697313138077

Epoch: 6| Step: 4
Training loss: 2.2560217068216324
Validation loss: 2.5011155623597947

Epoch: 6| Step: 5
Training loss: 2.900526091112543
Validation loss: 2.4572452252641184

Epoch: 6| Step: 6
Training loss: 2.4569774918974976
Validation loss: 2.501237517852533

Epoch: 6| Step: 7
Training loss: 1.8386663407225639
Validation loss: 2.516810450785676

Epoch: 6| Step: 8
Training loss: 2.487097442104639
Validation loss: 2.503063446632233

Epoch: 6| Step: 9
Training loss: 2.587988647907718
Validation loss: 2.491933525713361

Epoch: 6| Step: 10
Training loss: 2.2656711968283156
Validation loss: 2.474580513523244

Epoch: 6| Step: 11
Training loss: 2.1040054360236082
Validation loss: 2.4580420082826224

Epoch: 6| Step: 12
Training loss: 1.7746950639797632
Validation loss: 2.4916079833600584

Epoch: 6| Step: 13
Training loss: 2.240855708674342
Validation loss: 2.451160784424477

Epoch: 328| Step: 0
Training loss: 2.124078214092532
Validation loss: 2.490338759787101

Epoch: 6| Step: 1
Training loss: 2.506100744811699
Validation loss: 2.482937482111033

Epoch: 6| Step: 2
Training loss: 1.857977462130899
Validation loss: 2.4929394570745216

Epoch: 6| Step: 3
Training loss: 3.04863011598758
Validation loss: 2.4766269969295744

Epoch: 6| Step: 4
Training loss: 2.829359259438385
Validation loss: 2.4624499377995073

Epoch: 6| Step: 5
Training loss: 1.746053605610249
Validation loss: 2.4435313368641434

Epoch: 6| Step: 6
Training loss: 2.3698576418457598
Validation loss: 2.5098872235138034

Epoch: 6| Step: 7
Training loss: 2.261552610208357
Validation loss: 2.461177297414674

Epoch: 6| Step: 8
Training loss: 2.5502067257728824
Validation loss: 2.456327258757025

Epoch: 6| Step: 9
Training loss: 1.9649825606409455
Validation loss: 2.4629884038015537

Epoch: 6| Step: 10
Training loss: 2.386061257860625
Validation loss: 2.495921549060733

Epoch: 6| Step: 11
Training loss: 2.257181680062618
Validation loss: 2.4107094470534016

Epoch: 6| Step: 12
Training loss: 2.0679194601228277
Validation loss: 2.5213229423340238

Epoch: 6| Step: 13
Training loss: 2.3916967644830156
Validation loss: 2.441951199261251

Epoch: 329| Step: 0
Training loss: 1.7975137653274726
Validation loss: 2.4794386785748905

Epoch: 6| Step: 1
Training loss: 2.265916476738104
Validation loss: 2.4854386856059163

Epoch: 6| Step: 2
Training loss: 2.168115889221385
Validation loss: 2.46137950109741

Epoch: 6| Step: 3
Training loss: 1.6852251189992193
Validation loss: 2.496601551271988

Epoch: 6| Step: 4
Training loss: 2.401497941647882
Validation loss: 2.4794906521019286

Epoch: 6| Step: 5
Training loss: 1.7508955435191402
Validation loss: 2.4873323356117027

Epoch: 6| Step: 6
Training loss: 1.68343853511605
Validation loss: 2.482593476776776

Epoch: 6| Step: 7
Training loss: 2.718452174218031
Validation loss: 2.476341859602917

Epoch: 6| Step: 8
Training loss: 2.4949012259767662
Validation loss: 2.4909620805476673

Epoch: 6| Step: 9
Training loss: 2.8431793311012354
Validation loss: 2.454972622096805

Epoch: 6| Step: 10
Training loss: 2.6145787232701565
Validation loss: 2.469283893966462

Epoch: 6| Step: 11
Training loss: 2.3137748915592056
Validation loss: 2.463481974590211

Epoch: 6| Step: 12
Training loss: 2.366169021115528
Validation loss: 2.476372878180246

Epoch: 6| Step: 13
Training loss: 3.3123220719828743
Validation loss: 2.493230168968444

Epoch: 330| Step: 0
Training loss: 2.87141443654413
Validation loss: 2.4579960664877722

Epoch: 6| Step: 1
Training loss: 2.482840778052954
Validation loss: 2.4287448090033736

Epoch: 6| Step: 2
Training loss: 2.4039621747701916
Validation loss: 2.4518426182433983

Epoch: 6| Step: 3
Training loss: 2.227508665125783
Validation loss: 2.460523851264459

Epoch: 6| Step: 4
Training loss: 2.439358418491626
Validation loss: 2.4461192800239364

Epoch: 6| Step: 5
Training loss: 1.9495938538361706
Validation loss: 2.4926478339815032

Epoch: 6| Step: 6
Training loss: 2.72403930388116
Validation loss: 2.4681025796221863

Epoch: 6| Step: 7
Training loss: 1.981346161731933
Validation loss: 2.4538185158766774

Epoch: 6| Step: 8
Training loss: 2.605725414439388
Validation loss: 2.4907117206849114

Epoch: 6| Step: 9
Training loss: 1.7033523267867754
Validation loss: 2.479944467144433

Epoch: 6| Step: 10
Training loss: 1.5136896735344707
Validation loss: 2.4688419643192883

Epoch: 6| Step: 11
Training loss: 2.602335313905288
Validation loss: 2.4724321622643903

Epoch: 6| Step: 12
Training loss: 2.365809275455659
Validation loss: 2.4985188013535926

Epoch: 6| Step: 13
Training loss: 1.6580750828205537
Validation loss: 2.503565614351849

Epoch: 331| Step: 0
Training loss: 2.1678688919851483
Validation loss: 2.4990741892070023

Epoch: 6| Step: 1
Training loss: 2.229958390673086
Validation loss: 2.4930752041720985

Epoch: 6| Step: 2
Training loss: 2.3752659096682374
Validation loss: 2.4917265181871313

Epoch: 6| Step: 3
Training loss: 2.757349423873752
Validation loss: 2.425785556582185

Epoch: 6| Step: 4
Training loss: 2.1026852242871024
Validation loss: 2.4968088333078033

Epoch: 6| Step: 5
Training loss: 2.666613389516135
Validation loss: 2.468837663270896

Epoch: 6| Step: 6
Training loss: 1.8083076003509975
Validation loss: 2.490589219929952

Epoch: 6| Step: 7
Training loss: 2.5732325515711425
Validation loss: 2.500681037841569

Epoch: 6| Step: 8
Training loss: 2.0611405950027195
Validation loss: 2.478555465586586

Epoch: 6| Step: 9
Training loss: 2.499791136599923
Validation loss: 2.4700038421134

Epoch: 6| Step: 10
Training loss: 2.229468233144419
Validation loss: 2.4438474364806804

Epoch: 6| Step: 11
Training loss: 2.3121505679498577
Validation loss: 2.450936226081915

Epoch: 6| Step: 12
Training loss: 2.0542959558166083
Validation loss: 2.475424474046136

Epoch: 6| Step: 13
Training loss: 2.5621435336153446
Validation loss: 2.520505125286679

Epoch: 332| Step: 0
Training loss: 2.3067131838582466
Validation loss: 2.4571238010801215

Epoch: 6| Step: 1
Training loss: 2.7314563105896745
Validation loss: 2.4703826282675254

Epoch: 6| Step: 2
Training loss: 2.6016640399900868
Validation loss: 2.4550893273620393

Epoch: 6| Step: 3
Training loss: 2.1228218134789896
Validation loss: 2.4639991603074005

Epoch: 6| Step: 4
Training loss: 2.031821537305983
Validation loss: 2.4689157351668647

Epoch: 6| Step: 5
Training loss: 2.909185915988633
Validation loss: 2.448414123160959

Epoch: 6| Step: 6
Training loss: 1.6830840788639374
Validation loss: 2.4685796853120117

Epoch: 6| Step: 7
Training loss: 2.340920431441255
Validation loss: 2.487961491615013

Epoch: 6| Step: 8
Training loss: 2.89509113781035
Validation loss: 2.5063476465328383

Epoch: 6| Step: 9
Training loss: 2.0755837182474375
Validation loss: 2.497864459187006

Epoch: 6| Step: 10
Training loss: 2.5404118635013226
Validation loss: 2.5144139465601407

Epoch: 6| Step: 11
Training loss: 1.8278079939949894
Validation loss: 2.5026694648202734

Epoch: 6| Step: 12
Training loss: 2.1839708470086077
Validation loss: 2.476007423863149

Epoch: 6| Step: 13
Training loss: 1.359188154687284
Validation loss: 2.4849214392090317

Epoch: 333| Step: 0
Training loss: 1.4550813201254769
Validation loss: 2.4774089451193233

Epoch: 6| Step: 1
Training loss: 2.329040756871371
Validation loss: 2.453866427900734

Epoch: 6| Step: 2
Training loss: 2.528604231445373
Validation loss: 2.478578915772123

Epoch: 6| Step: 3
Training loss: 2.5043954356506704
Validation loss: 2.5034020625468925

Epoch: 6| Step: 4
Training loss: 1.9105042060699926
Validation loss: 2.490147036702398

Epoch: 6| Step: 5
Training loss: 2.3251352250347215
Validation loss: 2.5069721914706453

Epoch: 6| Step: 6
Training loss: 2.058303957380746
Validation loss: 2.4930895428098436

Epoch: 6| Step: 7
Training loss: 2.103111178620539
Validation loss: 2.5184186300903706

Epoch: 6| Step: 8
Training loss: 2.920254018003072
Validation loss: 2.4545940533106103

Epoch: 6| Step: 9
Training loss: 1.8243753174078765
Validation loss: 2.485484256301033

Epoch: 6| Step: 10
Training loss: 2.248144868493008
Validation loss: 2.502953048620784

Epoch: 6| Step: 11
Training loss: 2.4502540845880714
Validation loss: 2.488062965123425

Epoch: 6| Step: 12
Training loss: 3.013217894303737
Validation loss: 2.482507840018508

Epoch: 6| Step: 13
Training loss: 2.1231441247419522
Validation loss: 2.4750088645251775

Epoch: 334| Step: 0
Training loss: 2.0052966077786243
Validation loss: 2.497088991470727

Epoch: 6| Step: 1
Training loss: 2.330925607483446
Validation loss: 2.474566960665113

Epoch: 6| Step: 2
Training loss: 2.268632458040591
Validation loss: 2.46400019034172

Epoch: 6| Step: 3
Training loss: 1.9864784566429774
Validation loss: 2.477412411721047

Epoch: 6| Step: 4
Training loss: 2.373497337003909
Validation loss: 2.4648864065309035

Epoch: 6| Step: 5
Training loss: 1.876644240099091
Validation loss: 2.4835985492054693

Epoch: 6| Step: 6
Training loss: 1.566093889327977
Validation loss: 2.4490723914636923

Epoch: 6| Step: 7
Training loss: 2.772194855307956
Validation loss: 2.4804082443070525

Epoch: 6| Step: 8
Training loss: 2.5253556935785015
Validation loss: 2.4799533242918534

Epoch: 6| Step: 9
Training loss: 2.064139206043592
Validation loss: 2.495056255116645

Epoch: 6| Step: 10
Training loss: 3.0147374878024333
Validation loss: 2.4405123552562173

Epoch: 6| Step: 11
Training loss: 2.6912595490225537
Validation loss: 2.47773290344866

Epoch: 6| Step: 12
Training loss: 2.2848027404181024
Validation loss: 2.4917374143174205

Epoch: 6| Step: 13
Training loss: 2.024294638527044
Validation loss: 2.4420613987809072

Epoch: 335| Step: 0
Training loss: 2.6218853592241715
Validation loss: 2.4941486957853836

Epoch: 6| Step: 1
Training loss: 2.509879428049716
Validation loss: 2.440907614569674

Epoch: 6| Step: 2
Training loss: 2.2739656759001705
Validation loss: 2.4681925883576246

Epoch: 6| Step: 3
Training loss: 1.9582602778125715
Validation loss: 2.517061174511367

Epoch: 6| Step: 4
Training loss: 2.3654539099319196
Validation loss: 2.489239948625885

Epoch: 6| Step: 5
Training loss: 1.6578821741072725
Validation loss: 2.5036635064350117

Epoch: 6| Step: 6
Training loss: 2.1797349712712575
Validation loss: 2.472957048793875

Epoch: 6| Step: 7
Training loss: 2.5919194023646823
Validation loss: 2.47519119052914

Epoch: 6| Step: 8
Training loss: 2.767598972497786
Validation loss: 2.473038592956319

Epoch: 6| Step: 9
Training loss: 2.0800459165273395
Validation loss: 2.4798720591914054

Epoch: 6| Step: 10
Training loss: 2.829921088854003
Validation loss: 2.4641308389008802

Epoch: 6| Step: 11
Training loss: 1.4816366536061822
Validation loss: 2.495693428048537

Epoch: 6| Step: 12
Training loss: 2.1987737879678066
Validation loss: 2.474129632396009

Epoch: 6| Step: 13
Training loss: 2.178752739751935
Validation loss: 2.4355805451258563

Epoch: 336| Step: 0
Training loss: 2.797026646088564
Validation loss: 2.4929808739523454

Epoch: 6| Step: 1
Training loss: 2.17072216147093
Validation loss: 2.4993522461164783

Epoch: 6| Step: 2
Training loss: 2.571148119119484
Validation loss: 2.472244024862311

Epoch: 6| Step: 3
Training loss: 2.2560354453068676
Validation loss: 2.5108171869499945

Epoch: 6| Step: 4
Training loss: 2.5843458549950276
Validation loss: 2.452083268484768

Epoch: 6| Step: 5
Training loss: 2.6955105473830865
Validation loss: 2.4788659970694398

Epoch: 6| Step: 6
Training loss: 2.138132914038148
Validation loss: 2.470083165154699

Epoch: 6| Step: 7
Training loss: 1.7416115394641054
Validation loss: 2.5037576532999757

Epoch: 6| Step: 8
Training loss: 2.1644875009254725
Validation loss: 2.5059511723542895

Epoch: 6| Step: 9
Training loss: 2.4266133869875577
Validation loss: 2.4847711727195576

Epoch: 6| Step: 10
Training loss: 2.2746987531142304
Validation loss: 2.4766680365444524

Epoch: 6| Step: 11
Training loss: 2.175889401170636
Validation loss: 2.495415349727489

Epoch: 6| Step: 12
Training loss: 2.0338156122972944
Validation loss: 2.456128755785615

Epoch: 6| Step: 13
Training loss: 2.1005610806365778
Validation loss: 2.453179323237506

Epoch: 337| Step: 0
Training loss: 2.357554257396551
Validation loss: 2.4779208661997076

Epoch: 6| Step: 1
Training loss: 2.067236807329563
Validation loss: 2.47228443832832

Epoch: 6| Step: 2
Training loss: 1.8506906766616547
Validation loss: 2.501232235772814

Epoch: 6| Step: 3
Training loss: 2.3008211452995364
Validation loss: 2.4750173327809724

Epoch: 6| Step: 4
Training loss: 3.160310154433706
Validation loss: 2.4813422681252932

Epoch: 6| Step: 5
Training loss: 2.493564237401672
Validation loss: 2.490769905223572

Epoch: 6| Step: 6
Training loss: 1.7116502644546336
Validation loss: 2.44299259303297

Epoch: 6| Step: 7
Training loss: 2.4236087622960563
Validation loss: 2.475832133914676

Epoch: 6| Step: 8
Training loss: 2.3992873683546216
Validation loss: 2.4643154043137585

Epoch: 6| Step: 9
Training loss: 1.8950346916868126
Validation loss: 2.4700827188678742

Epoch: 6| Step: 10
Training loss: 1.4842890664373167
Validation loss: 2.489785009669586

Epoch: 6| Step: 11
Training loss: 2.4383396756482334
Validation loss: 2.4969900870213824

Epoch: 6| Step: 12
Training loss: 2.370588723309163
Validation loss: 2.4667979267297944

Epoch: 6| Step: 13
Training loss: 2.6528514887325962
Validation loss: 2.5111297879910066

Epoch: 338| Step: 0
Training loss: 2.8694916076165202
Validation loss: 2.438949291879275

Epoch: 6| Step: 1
Training loss: 1.9561698677786714
Validation loss: 2.51667055651125

Epoch: 6| Step: 2
Training loss: 2.315089142403413
Validation loss: 2.4714114525490687

Epoch: 6| Step: 3
Training loss: 2.0564785537547485
Validation loss: 2.4843124987515113

Epoch: 6| Step: 4
Training loss: 2.7579872140843906
Validation loss: 2.4759413297073043

Epoch: 6| Step: 5
Training loss: 2.00263874501973
Validation loss: 2.532917850125295

Epoch: 6| Step: 6
Training loss: 2.2445321932510853
Validation loss: 2.4815732585059407

Epoch: 6| Step: 7
Training loss: 1.7561272574869184
Validation loss: 2.46655609683798

Epoch: 6| Step: 8
Training loss: 2.1970826746751952
Validation loss: 2.435046794415024

Epoch: 6| Step: 9
Training loss: 2.466835341829432
Validation loss: 2.466353434808534

Epoch: 6| Step: 10
Training loss: 2.4334834335575257
Validation loss: 2.5127112675124432

Epoch: 6| Step: 11
Training loss: 2.5127436088015425
Validation loss: 2.480821768635531

Epoch: 6| Step: 12
Training loss: 2.411468395983887
Validation loss: 2.4755278351706655

Epoch: 6| Step: 13
Training loss: 2.025758332852198
Validation loss: 2.422403431012573

Epoch: 339| Step: 0
Training loss: 2.691182474652628
Validation loss: 2.467781068863035

Epoch: 6| Step: 1
Training loss: 2.1111249658341897
Validation loss: 2.4752701298382958

Epoch: 6| Step: 2
Training loss: 2.2939415708781747
Validation loss: 2.4774873615685618

Epoch: 6| Step: 3
Training loss: 2.6595886790119105
Validation loss: 2.479898659226341

Epoch: 6| Step: 4
Training loss: 2.2455517985778988
Validation loss: 2.473495313301604

Epoch: 6| Step: 5
Training loss: 2.0299640986833634
Validation loss: 2.471048617571592

Epoch: 6| Step: 6
Training loss: 3.1203147861432226
Validation loss: 2.487666895661725

Epoch: 6| Step: 7
Training loss: 1.9075047475173286
Validation loss: 2.4543356216809578

Epoch: 6| Step: 8
Training loss: 2.323604008775938
Validation loss: 2.5095373210169036

Epoch: 6| Step: 9
Training loss: 1.5515406980644253
Validation loss: 2.4759230171910063

Epoch: 6| Step: 10
Training loss: 1.857712715876436
Validation loss: 2.454899346011025

Epoch: 6| Step: 11
Training loss: 2.2076408692080016
Validation loss: 2.479469022015935

Epoch: 6| Step: 12
Training loss: 2.596912524231092
Validation loss: 2.4727925813148413

Epoch: 6| Step: 13
Training loss: 1.870484573492348
Validation loss: 2.453385898774085

Epoch: 340| Step: 0
Training loss: 2.3241344276326874
Validation loss: 2.4395034503598825

Epoch: 6| Step: 1
Training loss: 2.2835554205189643
Validation loss: 2.514796136838733

Epoch: 6| Step: 2
Training loss: 2.6458169130631792
Validation loss: 2.445762832105422

Epoch: 6| Step: 3
Training loss: 1.8379601557649958
Validation loss: 2.4729238035969145

Epoch: 6| Step: 4
Training loss: 1.9888790408600427
Validation loss: 2.4775141496648407

Epoch: 6| Step: 5
Training loss: 2.1598113131231345
Validation loss: 2.4608378374370012

Epoch: 6| Step: 6
Training loss: 3.187080954139657
Validation loss: 2.4683480798138553

Epoch: 6| Step: 7
Training loss: 1.902952124349727
Validation loss: 2.453646878991136

Epoch: 6| Step: 8
Training loss: 2.0899487050163716
Validation loss: 2.463492652752187

Epoch: 6| Step: 9
Training loss: 1.6886171951988962
Validation loss: 2.4527461042343335

Epoch: 6| Step: 10
Training loss: 2.682703661835452
Validation loss: 2.512997208070375

Epoch: 6| Step: 11
Training loss: 2.2081486126878627
Validation loss: 2.494275271721606

Epoch: 6| Step: 12
Training loss: 2.398156165108631
Validation loss: 2.4700538117643815

Epoch: 6| Step: 13
Training loss: 2.687892086792038
Validation loss: 2.470909888277459

Epoch: 341| Step: 0
Training loss: 2.714240181333054
Validation loss: 2.470386182041318

Epoch: 6| Step: 1
Training loss: 2.0600208062908187
Validation loss: 2.4882951320310474

Epoch: 6| Step: 2
Training loss: 2.776909516588157
Validation loss: 2.4501016940185423

Epoch: 6| Step: 3
Training loss: 2.01045237560546
Validation loss: 2.462768136319611

Epoch: 6| Step: 4
Training loss: 2.379592070040536
Validation loss: 2.5053243386161075

Epoch: 6| Step: 5
Training loss: 2.797811937301206
Validation loss: 2.508728910790262

Epoch: 6| Step: 6
Training loss: 2.435597777886269
Validation loss: 2.4628454752354565

Epoch: 6| Step: 7
Training loss: 2.225823423629442
Validation loss: 2.440202462800191

Epoch: 6| Step: 8
Training loss: 1.9753894566778976
Validation loss: 2.4606806284883502

Epoch: 6| Step: 9
Training loss: 2.4223646099730534
Validation loss: 2.461937341101123

Epoch: 6| Step: 10
Training loss: 2.512929384349819
Validation loss: 2.479700719844387

Epoch: 6| Step: 11
Training loss: 1.791167773790598
Validation loss: 2.4720715169617176

Epoch: 6| Step: 12
Training loss: 2.0987018841879013
Validation loss: 2.5071570168123456

Epoch: 6| Step: 13
Training loss: 1.418405849038945
Validation loss: 2.461053515107179

Epoch: 342| Step: 0
Training loss: 2.5575392552082463
Validation loss: 2.4440286623553935

Epoch: 6| Step: 1
Training loss: 2.470860412174803
Validation loss: 2.474274553297386

Epoch: 6| Step: 2
Training loss: 2.8964017314786124
Validation loss: 2.4589089153789954

Epoch: 6| Step: 3
Training loss: 2.0968270536532403
Validation loss: 2.497969915751996

Epoch: 6| Step: 4
Training loss: 2.09356517830046
Validation loss: 2.4943083854788046

Epoch: 6| Step: 5
Training loss: 1.975304184195925
Validation loss: 2.4446590047962475

Epoch: 6| Step: 6
Training loss: 2.1948869503166697
Validation loss: 2.441932854412777

Epoch: 6| Step: 7
Training loss: 1.7552136007016739
Validation loss: 2.477660361539131

Epoch: 6| Step: 8
Training loss: 2.051377331526023
Validation loss: 2.4891407256052096

Epoch: 6| Step: 9
Training loss: 2.636522797261525
Validation loss: 2.4729088597513478

Epoch: 6| Step: 10
Training loss: 1.6773468031764394
Validation loss: 2.486263074424183

Epoch: 6| Step: 11
Training loss: 2.590447771109503
Validation loss: 2.477406668542418

Epoch: 6| Step: 12
Training loss: 2.232512544808165
Validation loss: 2.451026110042364

Epoch: 6| Step: 13
Training loss: 2.7154202295147
Validation loss: 2.446121886507976

Epoch: 343| Step: 0
Training loss: 1.9631313499000644
Validation loss: 2.4821345687444705

Epoch: 6| Step: 1
Training loss: 1.9443812458424814
Validation loss: 2.4910831787917

Epoch: 6| Step: 2
Training loss: 2.5807777397647844
Validation loss: 2.4644005549609744

Epoch: 6| Step: 3
Training loss: 2.6871877089632434
Validation loss: 2.4908795062578655

Epoch: 6| Step: 4
Training loss: 3.168314471589168
Validation loss: 2.4506572953776917

Epoch: 6| Step: 5
Training loss: 2.2465807046867687
Validation loss: 2.4921623066948593

Epoch: 6| Step: 6
Training loss: 2.7466576812498404
Validation loss: 2.5049675625430123

Epoch: 6| Step: 7
Training loss: 1.7353713464333715
Validation loss: 2.477807640812227

Epoch: 6| Step: 8
Training loss: 2.639876039369058
Validation loss: 2.4655181927760252

Epoch: 6| Step: 9
Training loss: 2.0931146141562302
Validation loss: 2.462574150135472

Epoch: 6| Step: 10
Training loss: 1.7926464580679753
Validation loss: 2.4657933547362645

Epoch: 6| Step: 11
Training loss: 1.6428135398148875
Validation loss: 2.497069764263836

Epoch: 6| Step: 12
Training loss: 1.8717902366539256
Validation loss: 2.479587915882769

Epoch: 6| Step: 13
Training loss: 2.0229228775552413
Validation loss: 2.51450774902662

Epoch: 344| Step: 0
Training loss: 2.348966261574729
Validation loss: 2.485141977578278

Epoch: 6| Step: 1
Training loss: 2.6591303747608137
Validation loss: 2.473814452326154

Epoch: 6| Step: 2
Training loss: 2.6598651295364992
Validation loss: 2.456433719987863

Epoch: 6| Step: 3
Training loss: 2.365388696677747
Validation loss: 2.4179970499089216

Epoch: 6| Step: 4
Training loss: 2.803411251834383
Validation loss: 2.4649231808541234

Epoch: 6| Step: 5
Training loss: 2.083483677843396
Validation loss: 2.46857800085088

Epoch: 6| Step: 6
Training loss: 1.8463701929886338
Validation loss: 2.4340113960906855

Epoch: 6| Step: 7
Training loss: 2.033943854848332
Validation loss: 2.4798096305355433

Epoch: 6| Step: 8
Training loss: 2.0028037445929314
Validation loss: 2.504948184982894

Epoch: 6| Step: 9
Training loss: 2.54709909775449
Validation loss: 2.509403964172939

Epoch: 6| Step: 10
Training loss: 1.6485878392224669
Validation loss: 2.5296170988731603

Epoch: 6| Step: 11
Training loss: 1.8193268297083152
Validation loss: 2.496724580529295

Epoch: 6| Step: 12
Training loss: 2.183357458005608
Validation loss: 2.479885393875962

Epoch: 6| Step: 13
Training loss: 2.70355785630666
Validation loss: 2.4775356995509163

Epoch: 345| Step: 0
Training loss: 1.8206541207443612
Validation loss: 2.4687482159650824

Epoch: 6| Step: 1
Training loss: 2.232119081643049
Validation loss: 2.452285157791351

Epoch: 6| Step: 2
Training loss: 2.7567680477548913
Validation loss: 2.4741402853271595

Epoch: 6| Step: 3
Training loss: 1.6447215932841897
Validation loss: 2.448887341985656

Epoch: 6| Step: 4
Training loss: 2.2773428077761237
Validation loss: 2.510982055746572

Epoch: 6| Step: 5
Training loss: 3.2805980216290753
Validation loss: 2.495596199234235

Epoch: 6| Step: 6
Training loss: 2.489167874411192
Validation loss: 2.461291805505524

Epoch: 6| Step: 7
Training loss: 2.4769982757808764
Validation loss: 2.482522029025729

Epoch: 6| Step: 8
Training loss: 2.4020235670142163
Validation loss: 2.4760375722515757

Epoch: 6| Step: 9
Training loss: 1.9166405579267582
Validation loss: 2.468577712145343

Epoch: 6| Step: 10
Training loss: 1.9340168807862086
Validation loss: 2.4787223799047466

Epoch: 6| Step: 11
Training loss: 2.2101423902943393
Validation loss: 2.4876351491543227

Epoch: 6| Step: 12
Training loss: 1.9282633356211631
Validation loss: 2.478856887828265

Epoch: 6| Step: 13
Training loss: 1.878672690358645
Validation loss: 2.4925317188712017

Epoch: 346| Step: 0
Training loss: 2.849177620007493
Validation loss: 2.470379512938769

Epoch: 6| Step: 1
Training loss: 2.223284748162307
Validation loss: 2.449872181035801

Epoch: 6| Step: 2
Training loss: 2.6781884273821133
Validation loss: 2.462965482786971

Epoch: 6| Step: 3
Training loss: 2.35913650620977
Validation loss: 2.5139667301624127

Epoch: 6| Step: 4
Training loss: 1.7843288115020035
Validation loss: 2.507588250739297

Epoch: 6| Step: 5
Training loss: 2.31845583630674
Validation loss: 2.477469195001617

Epoch: 6| Step: 6
Training loss: 2.432127288774934
Validation loss: 2.4995037745143525

Epoch: 6| Step: 7
Training loss: 2.1052245952186377
Validation loss: 2.473676501003848

Epoch: 6| Step: 8
Training loss: 2.2261793375768715
Validation loss: 2.476693351230975

Epoch: 6| Step: 9
Training loss: 1.7677750451427614
Validation loss: 2.461153773088787

Epoch: 6| Step: 10
Training loss: 2.2188471383335573
Validation loss: 2.4558688937313606

Epoch: 6| Step: 11
Training loss: 2.327083277780594
Validation loss: 2.4827482325689254

Epoch: 6| Step: 12
Training loss: 2.3151289971088858
Validation loss: 2.49043222862675

Epoch: 6| Step: 13
Training loss: 1.5070971437467782
Validation loss: 2.486510529335371

Epoch: 347| Step: 0
Training loss: 2.765003544010147
Validation loss: 2.4726908486843553

Epoch: 6| Step: 1
Training loss: 2.356585135265828
Validation loss: 2.49881974837318

Epoch: 6| Step: 2
Training loss: 2.4634106031936205
Validation loss: 2.457556440323906

Epoch: 6| Step: 3
Training loss: 2.096942346801778
Validation loss: 2.4290729493721637

Epoch: 6| Step: 4
Training loss: 2.425001722512174
Validation loss: 2.431970508225466

Epoch: 6| Step: 5
Training loss: 1.4960525546863876
Validation loss: 2.481964932366804

Epoch: 6| Step: 6
Training loss: 2.5205474460209674
Validation loss: 2.5037983229625915

Epoch: 6| Step: 7
Training loss: 2.3191634393089977
Validation loss: 2.4682567441684538

Epoch: 6| Step: 8
Training loss: 1.9656892629528604
Validation loss: 2.459607428380824

Epoch: 6| Step: 9
Training loss: 1.9022107714286578
Validation loss: 2.472558900176836

Epoch: 6| Step: 10
Training loss: 2.370329531040753
Validation loss: 2.4688630437093355

Epoch: 6| Step: 11
Training loss: 2.2964100269754546
Validation loss: 2.4466663303102862

Epoch: 6| Step: 12
Training loss: 2.1110428609467866
Validation loss: 2.4981261070200347

Epoch: 6| Step: 13
Training loss: 2.7492151007231036
Validation loss: 2.4828577488767993

Epoch: 348| Step: 0
Training loss: 1.9947337913251468
Validation loss: 2.4845485006239

Epoch: 6| Step: 1
Training loss: 2.9752575674676365
Validation loss: 2.456935675748696

Epoch: 6| Step: 2
Training loss: 2.484142748455118
Validation loss: 2.488072928840443

Epoch: 6| Step: 3
Training loss: 2.713143014942495
Validation loss: 2.464211712811876

Epoch: 6| Step: 4
Training loss: 2.3978551101627255
Validation loss: 2.4638021010985147

Epoch: 6| Step: 5
Training loss: 2.077602234676365
Validation loss: 2.489934730590254

Epoch: 6| Step: 6
Training loss: 1.667752254113441
Validation loss: 2.467989765963524

Epoch: 6| Step: 7
Training loss: 2.34681419974453
Validation loss: 2.46654440946287

Epoch: 6| Step: 8
Training loss: 2.2574209117870847
Validation loss: 2.4962591629697766

Epoch: 6| Step: 9
Training loss: 2.568849280450565
Validation loss: 2.4570420845327985

Epoch: 6| Step: 10
Training loss: 2.1960987570049757
Validation loss: 2.445958724894375

Epoch: 6| Step: 11
Training loss: 1.844062649355638
Validation loss: 2.5274023479801957

Epoch: 6| Step: 12
Training loss: 2.0256146239968627
Validation loss: 2.4624158027096326

Epoch: 6| Step: 13
Training loss: 1.5067159984468161
Validation loss: 2.4629889866863874

Epoch: 349| Step: 0
Training loss: 2.2814784523479137
Validation loss: 2.4512539514385936

Epoch: 6| Step: 1
Training loss: 2.421599144761105
Validation loss: 2.465835628878227

Epoch: 6| Step: 2
Training loss: 2.6078055224545285
Validation loss: 2.4619623553293333

Epoch: 6| Step: 3
Training loss: 2.3870301987743767
Validation loss: 2.4652392798081175

Epoch: 6| Step: 4
Training loss: 1.9118655929009767
Validation loss: 2.450174273398767

Epoch: 6| Step: 5
Training loss: 2.519187915681679
Validation loss: 2.4532631643866507

Epoch: 6| Step: 6
Training loss: 1.5961096256342282
Validation loss: 2.48642471198433

Epoch: 6| Step: 7
Training loss: 2.5756051435772016
Validation loss: 2.450594060756884

Epoch: 6| Step: 8
Training loss: 1.9273383169480325
Validation loss: 2.5047205810745052

Epoch: 6| Step: 9
Training loss: 2.818827991294757
Validation loss: 2.47620340046225

Epoch: 6| Step: 10
Training loss: 2.1165042915039387
Validation loss: 2.504571373500505

Epoch: 6| Step: 11
Training loss: 2.5570020544038647
Validation loss: 2.4757406829521718

Epoch: 6| Step: 12
Training loss: 1.6336791006638736
Validation loss: 2.5109448492344875

Epoch: 6| Step: 13
Training loss: 2.1073441513195776
Validation loss: 2.4830830380659643

Epoch: 350| Step: 0
Training loss: 2.3575421218102592
Validation loss: 2.432800935997836

Epoch: 6| Step: 1
Training loss: 2.175283378468618
Validation loss: 2.4612021256259657

Epoch: 6| Step: 2
Training loss: 2.64067231085114
Validation loss: 2.468290123891141

Epoch: 6| Step: 3
Training loss: 1.6118787529832532
Validation loss: 2.5109690995872396

Epoch: 6| Step: 4
Training loss: 1.7228201098371747
Validation loss: 2.474498135408208

Epoch: 6| Step: 5
Training loss: 1.786081020298849
Validation loss: 2.476351170134346

Epoch: 6| Step: 6
Training loss: 2.4228364082179232
Validation loss: 2.4897732869235525

Epoch: 6| Step: 7
Training loss: 2.4831084855800536
Validation loss: 2.473545127173326

Epoch: 6| Step: 8
Training loss: 2.280422530926158
Validation loss: 2.435062996048021

Epoch: 6| Step: 9
Training loss: 2.6888980334190355
Validation loss: 2.4720623474241927

Epoch: 6| Step: 10
Training loss: 1.9489008887052137
Validation loss: 2.4794078343196393

Epoch: 6| Step: 11
Training loss: 2.7633278137200987
Validation loss: 2.4400193599388116

Epoch: 6| Step: 12
Training loss: 1.9347987880132798
Validation loss: 2.489421215349004

Epoch: 6| Step: 13
Training loss: 2.151090336603851
Validation loss: 2.4603332892455105

Testing loss: 2.400615567720115

Epoch: 1| Step: 0
Training loss: 5.20347580285603
Validation loss: 5.411672861398685

Epoch: 5| Step: 1
Training loss: 6.034742223864752
Validation loss: 5.409679362000342

Epoch: 5| Step: 2
Training loss: 6.315191318395623
Validation loss: 5.404009493528004

Epoch: 5| Step: 3
Training loss: 6.10677662885075
Validation loss: 5.401995859430178

Epoch: 5| Step: 4
Training loss: 5.487720132292299
Validation loss: 5.399844326964178

Epoch: 5| Step: 5
Training loss: 4.597423926798062
Validation loss: 5.39915160104387

Epoch: 5| Step: 6
Training loss: 5.3531261971868185
Validation loss: 5.396943120475856

Epoch: 5| Step: 7
Training loss: 4.757401371874182
Validation loss: 5.39336578141765

Epoch: 5| Step: 8
Training loss: 5.559910375023675
Validation loss: 5.391741341731519

Epoch: 5| Step: 9
Training loss: 5.081945115106429
Validation loss: 5.389152355425722

Epoch: 5| Step: 10
Training loss: 5.186632014815056
Validation loss: 5.3838542022716345

Epoch: 2| Step: 0
Training loss: 5.557293802229362
Validation loss: 5.38256045129912

Epoch: 5| Step: 1
Training loss: 5.677219027609484
Validation loss: 5.380340357588102

Epoch: 5| Step: 2
Training loss: 5.640251337497868
Validation loss: 5.375164803079182

Epoch: 5| Step: 3
Training loss: 4.807359798963911
Validation loss: 5.3750922931322425

Epoch: 5| Step: 4
Training loss: 5.090243952512861
Validation loss: 5.373971257321659

Epoch: 5| Step: 5
Training loss: 4.870327397489848
Validation loss: 5.366940367413473

Epoch: 5| Step: 6
Training loss: 5.360111997772258
Validation loss: 5.367061673878428

Epoch: 5| Step: 7
Training loss: 5.447255470701364
Validation loss: 5.361842562479548

Epoch: 5| Step: 8
Training loss: 4.546316210831269
Validation loss: 5.359963365055243

Epoch: 5| Step: 9
Training loss: 5.9517750248178
Validation loss: 5.355143910551791

Epoch: 5| Step: 10
Training loss: 6.58962985213394
Validation loss: 5.35403196806134

Epoch: 3| Step: 0
Training loss: 5.345980173276134
Validation loss: 5.351447018188066

Epoch: 5| Step: 1
Training loss: 4.513382193011182
Validation loss: 5.350240194720961

Epoch: 5| Step: 2
Training loss: 5.625654224921225
Validation loss: 5.344496123133456

Epoch: 5| Step: 3
Training loss: 5.07998576184778
Validation loss: 5.341544843326986

Epoch: 5| Step: 4
Training loss: 5.574137483257027
Validation loss: 5.34055452982606

Epoch: 5| Step: 5
Training loss: 5.352017615798219
Validation loss: 5.334626840171165

Epoch: 5| Step: 6
Training loss: 4.759314188878679
Validation loss: 5.332019745405142

Epoch: 5| Step: 7
Training loss: 5.860913209552104
Validation loss: 5.33017191068291

Epoch: 5| Step: 8
Training loss: 5.094841775730289
Validation loss: 5.3276869580839685

Epoch: 5| Step: 9
Training loss: 6.219252982956685
Validation loss: 5.325948990973203

Epoch: 5| Step: 10
Training loss: 5.69887804734714
Validation loss: 5.32360011045522

Epoch: 4| Step: 0
Training loss: 5.946033970371061
Validation loss: 5.319458677508318

Epoch: 5| Step: 1
Training loss: 5.50402649964008
Validation loss: 5.314766289344318

Epoch: 5| Step: 2
Training loss: 4.632215902116217
Validation loss: 5.313121632369424

Epoch: 5| Step: 3
Training loss: 5.113403225603025
Validation loss: 5.308010393639287

Epoch: 5| Step: 4
Training loss: 5.505295891427894
Validation loss: 5.306122839697728

Epoch: 5| Step: 5
Training loss: 4.89310087963264
Validation loss: 5.302191487605037

Epoch: 5| Step: 6
Training loss: 5.662765207037906
Validation loss: 5.297220080258946

Epoch: 5| Step: 7
Training loss: 5.356033929764365
Validation loss: 5.294075159145946

Epoch: 5| Step: 8
Training loss: 4.625496296416113
Validation loss: 5.294655853874591

Epoch: 5| Step: 9
Training loss: 5.728785646657724
Validation loss: 5.290389127272762

Epoch: 5| Step: 10
Training loss: 5.8632594545939005
Validation loss: 5.285961531212905

Epoch: 5| Step: 0
Training loss: 5.312534466799956
Validation loss: 5.285230877586349

Epoch: 5| Step: 1
Training loss: 5.537491719603537
Validation loss: 5.279672248054822

Epoch: 5| Step: 2
Training loss: 5.621489552658566
Validation loss: 5.277277893042654

Epoch: 5| Step: 3
Training loss: 5.6608561102217365
Validation loss: 5.272359904274101

Epoch: 5| Step: 4
Training loss: 4.522802436800022
Validation loss: 5.267985128095259

Epoch: 5| Step: 5
Training loss: 5.52512050169293
Validation loss: 5.263303754153031

Epoch: 5| Step: 6
Training loss: 5.235217763894869
Validation loss: 5.259437742701021

Epoch: 5| Step: 7
Training loss: 4.538135847618
Validation loss: 5.258252840817313

Epoch: 5| Step: 8
Training loss: 5.708086588554996
Validation loss: 5.2532223092044665

Epoch: 5| Step: 9
Training loss: 5.511911498335085
Validation loss: 5.248609963358332

Epoch: 5| Step: 10
Training loss: 5.223306277330181
Validation loss: 5.2472912724359775

Epoch: 6| Step: 0
Training loss: 5.921936840046507
Validation loss: 5.239920081204164

Epoch: 5| Step: 1
Training loss: 5.557538166198231
Validation loss: 5.239075149489496

Epoch: 5| Step: 2
Training loss: 4.497491349216727
Validation loss: 5.232757662312355

Epoch: 5| Step: 3
Training loss: 5.4596375322023185
Validation loss: 5.228342902370561

Epoch: 5| Step: 4
Training loss: 3.8085965044060552
Validation loss: 5.226977030551969

Epoch: 5| Step: 5
Training loss: 5.620814143497558
Validation loss: 5.221799948955011

Epoch: 5| Step: 6
Training loss: 5.985345746779888
Validation loss: 5.219169021691867

Epoch: 5| Step: 7
Training loss: 5.387575210081722
Validation loss: 5.21426319569937

Epoch: 5| Step: 8
Training loss: 5.11390079623762
Validation loss: 5.208555125118417

Epoch: 5| Step: 9
Training loss: 5.045064784561237
Validation loss: 5.200771288311903

Epoch: 5| Step: 10
Training loss: 5.309569145007458
Validation loss: 5.1988484165288025

Epoch: 7| Step: 0
Training loss: 5.494126738562259
Validation loss: 5.1958750884416425

Epoch: 5| Step: 1
Training loss: 5.308777705605977
Validation loss: 5.189716243750731

Epoch: 5| Step: 2
Training loss: 4.9349689032010655
Validation loss: 5.188324199721299

Epoch: 5| Step: 3
Training loss: 5.8158477658545795
Validation loss: 5.180703039541602

Epoch: 5| Step: 4
Training loss: 4.703955896936351
Validation loss: 5.175084295265466

Epoch: 5| Step: 5
Training loss: 4.717035045099752
Validation loss: 5.171829614532156

Epoch: 5| Step: 6
Training loss: 5.636356503378642
Validation loss: 5.166542489699464

Epoch: 5| Step: 7
Training loss: 4.934077175011526
Validation loss: 5.163757657763831

Epoch: 5| Step: 8
Training loss: 5.361648299865917
Validation loss: 5.156244958481697

Epoch: 5| Step: 9
Training loss: 6.142251767463
Validation loss: 5.147030970710997

Epoch: 5| Step: 10
Training loss: 4.03881457396125
Validation loss: 5.144724990528306

Epoch: 8| Step: 0
Training loss: 4.960647790814885
Validation loss: 5.144874465813904

Epoch: 5| Step: 1
Training loss: 6.175245383634216
Validation loss: 5.1337133610251

Epoch: 5| Step: 2
Training loss: 4.742606582851568
Validation loss: 5.131367389311973

Epoch: 5| Step: 3
Training loss: 5.294357196524395
Validation loss: 5.121643558021049

Epoch: 5| Step: 4
Training loss: 4.313858481684349
Validation loss: 5.122782295669126

Epoch: 5| Step: 5
Training loss: 5.136168243730464
Validation loss: 5.114667505765821

Epoch: 5| Step: 6
Training loss: 4.328344897251205
Validation loss: 5.110689050741287

Epoch: 5| Step: 7
Training loss: 5.748601328699714
Validation loss: 5.105478674210049

Epoch: 5| Step: 8
Training loss: 5.263895809243965
Validation loss: 5.0975514051428465

Epoch: 5| Step: 9
Training loss: 5.038838226295459
Validation loss: 5.0933349287261125

Epoch: 5| Step: 10
Training loss: 5.733152503481294
Validation loss: 5.0854505806108605

Epoch: 9| Step: 0
Training loss: 5.3109031521258006
Validation loss: 5.077148201189429

Epoch: 5| Step: 1
Training loss: 5.713111307169279
Validation loss: 5.075191846286985

Epoch: 5| Step: 2
Training loss: 4.783675033823225
Validation loss: 5.068969672115156

Epoch: 5| Step: 3
Training loss: 5.214019641694363
Validation loss: 5.062483224014283

Epoch: 5| Step: 4
Training loss: 5.355889524350357
Validation loss: 5.053658146744235

Epoch: 5| Step: 5
Training loss: 5.308038071945017
Validation loss: 5.049906906070734

Epoch: 5| Step: 6
Training loss: 4.368343411846001
Validation loss: 5.043277125625701

Epoch: 5| Step: 7
Training loss: 5.356002413748438
Validation loss: 5.038788121772615

Epoch: 5| Step: 8
Training loss: 4.8663511583312
Validation loss: 5.030431586224736

Epoch: 5| Step: 9
Training loss: 5.568976072095942
Validation loss: 5.0245886777841955

Epoch: 5| Step: 10
Training loss: 4.081475646249495
Validation loss: 5.019972669445904

Epoch: 10| Step: 0
Training loss: 5.202906148620925
Validation loss: 5.009033128081814

Epoch: 5| Step: 1
Training loss: 4.514997709322293
Validation loss: 5.006899181786022

Epoch: 5| Step: 2
Training loss: 4.794306859743216
Validation loss: 5.000837547130861

Epoch: 5| Step: 3
Training loss: 6.079437820142942
Validation loss: 4.991595002626392

Epoch: 5| Step: 4
Training loss: 5.119425835900177
Validation loss: 4.984574161099121

Epoch: 5| Step: 5
Training loss: 5.428684674422666
Validation loss: 4.977693530407192

Epoch: 5| Step: 6
Training loss: 4.884415557165771
Validation loss: 4.9703972530865865

Epoch: 5| Step: 7
Training loss: 4.088652020886638
Validation loss: 4.962597705047326

Epoch: 5| Step: 8
Training loss: 5.24000287762046
Validation loss: 4.9509816828435955

Epoch: 5| Step: 9
Training loss: 5.606660573973428
Validation loss: 4.94970332566224

Epoch: 5| Step: 10
Training loss: 4.014518615063749
Validation loss: 4.94102763231171

Epoch: 11| Step: 0
Training loss: 5.626065640703622
Validation loss: 4.935400418163072

Epoch: 5| Step: 1
Training loss: 4.745682611436033
Validation loss: 4.9240575849539745

Epoch: 5| Step: 2
Training loss: 4.644548703249902
Validation loss: 4.915992162178983

Epoch: 5| Step: 3
Training loss: 5.005320388645782
Validation loss: 4.910095375798327

Epoch: 5| Step: 4
Training loss: 5.282526888907547
Validation loss: 4.899294513077446

Epoch: 5| Step: 5
Training loss: 4.191985503572049
Validation loss: 4.890691870036608

Epoch: 5| Step: 6
Training loss: 4.068640191490579
Validation loss: 4.885926229142545

Epoch: 5| Step: 7
Training loss: 5.442611571794881
Validation loss: 4.880103352099642

Epoch: 5| Step: 8
Training loss: 5.463747188556046
Validation loss: 4.87012038907742

Epoch: 5| Step: 9
Training loss: 5.413163896329971
Validation loss: 4.861934693979913

Epoch: 5| Step: 10
Training loss: 4.331584553093774
Validation loss: 4.853924597324974

Epoch: 12| Step: 0
Training loss: 4.707301924271483
Validation loss: 4.841815038290546

Epoch: 5| Step: 1
Training loss: 4.116753397863219
Validation loss: 4.835431344087678

Epoch: 5| Step: 2
Training loss: 4.8837776389900895
Validation loss: 4.826305113982826

Epoch: 5| Step: 3
Training loss: 5.017212423418903
Validation loss: 4.817646625880784

Epoch: 5| Step: 4
Training loss: 4.867800752021076
Validation loss: 4.808448020045184

Epoch: 5| Step: 5
Training loss: 4.727300312055121
Validation loss: 4.7995465573758125

Epoch: 5| Step: 6
Training loss: 5.673525567449393
Validation loss: 4.790929658270331

Epoch: 5| Step: 7
Training loss: 5.029566702409787
Validation loss: 4.779711217334356

Epoch: 5| Step: 8
Training loss: 5.032834958461901
Validation loss: 4.7704722293837385

Epoch: 5| Step: 9
Training loss: 4.629679606415182
Validation loss: 4.759120399639095

Epoch: 5| Step: 10
Training loss: 4.748350760598748
Validation loss: 4.751994210374182

Epoch: 13| Step: 0
Training loss: 5.1718124892960375
Validation loss: 4.7422563290703375

Epoch: 5| Step: 1
Training loss: 5.079523920894502
Validation loss: 4.7243850887349375

Epoch: 5| Step: 2
Training loss: 5.278248311980019
Validation loss: 4.7207937592231

Epoch: 5| Step: 3
Training loss: 3.4613648574855183
Validation loss: 4.714067066937403

Epoch: 5| Step: 4
Training loss: 5.247042958500494
Validation loss: 4.703029891207246

Epoch: 5| Step: 5
Training loss: 3.5220386917267827
Validation loss: 4.688873540430714

Epoch: 5| Step: 6
Training loss: 4.8347637865972715
Validation loss: 4.681134105546603

Epoch: 5| Step: 7
Training loss: 5.3826953891809595
Validation loss: 4.664637886385714

Epoch: 5| Step: 8
Training loss: 4.481719716246235
Validation loss: 4.651106745684399

Epoch: 5| Step: 9
Training loss: 5.067116033971731
Validation loss: 4.643379345049814

Epoch: 5| Step: 10
Training loss: 4.410610989827698
Validation loss: 4.632870346300574

Epoch: 14| Step: 0
Training loss: 3.5977856077449744
Validation loss: 4.615507439942394

Epoch: 5| Step: 1
Training loss: 4.428074672631039
Validation loss: 4.613857052782338

Epoch: 5| Step: 2
Training loss: 5.132980831643303
Validation loss: 4.595486526242024

Epoch: 5| Step: 3
Training loss: 4.585569818629244
Validation loss: 4.579723360944368

Epoch: 5| Step: 4
Training loss: 5.254735718096678
Validation loss: 4.578428176707481

Epoch: 5| Step: 5
Training loss: 4.746115049463523
Validation loss: 4.557967754039711

Epoch: 5| Step: 6
Training loss: 4.2776772843704265
Validation loss: 4.552995055392177

Epoch: 5| Step: 7
Training loss: 4.704055643362397
Validation loss: 4.534660841753869

Epoch: 5| Step: 8
Training loss: 4.94787242133452
Validation loss: 4.522496956180881

Epoch: 5| Step: 9
Training loss: 5.1814690887551
Validation loss: 4.506755645893185

Epoch: 5| Step: 10
Training loss: 3.9086151287158306
Validation loss: 4.495970951584823

Epoch: 15| Step: 0
Training loss: 4.708243085058167
Validation loss: 4.484444509245284

Epoch: 5| Step: 1
Training loss: 4.809559902431627
Validation loss: 4.468698882362698

Epoch: 5| Step: 2
Training loss: 4.6987052757446435
Validation loss: 4.458852606407093

Epoch: 5| Step: 3
Training loss: 4.448489269173761
Validation loss: 4.4397669883705815

Epoch: 5| Step: 4
Training loss: 4.962914068593783
Validation loss: 4.421343093199959

Epoch: 5| Step: 5
Training loss: 3.9916699217674876
Validation loss: 4.413988767010439

Epoch: 5| Step: 6
Training loss: 3.3960256853603803
Validation loss: 4.395368024892377

Epoch: 5| Step: 7
Training loss: 4.56777648990924
Validation loss: 4.3886415084655095

Epoch: 5| Step: 8
Training loss: 3.9840414468608603
Validation loss: 4.370500029926345

Epoch: 5| Step: 9
Training loss: 4.253280271320743
Validation loss: 4.355610690403773

Epoch: 5| Step: 10
Training loss: 5.615819263017022
Validation loss: 4.350236500935336

Epoch: 16| Step: 0
Training loss: 4.508517575450872
Validation loss: 4.3192774521830755

Epoch: 5| Step: 1
Training loss: 3.780051301476856
Validation loss: 4.310779724800809

Epoch: 5| Step: 2
Training loss: 4.395776733226894
Validation loss: 4.294525944645356

Epoch: 5| Step: 3
Training loss: 4.564170009204829
Validation loss: 4.2795592615446205

Epoch: 5| Step: 4
Training loss: 4.441025958391862
Validation loss: 4.266612477753166

Epoch: 5| Step: 5
Training loss: 3.3605496504205816
Validation loss: 4.253512808554696

Epoch: 5| Step: 6
Training loss: 4.374967956425621
Validation loss: 4.224952120897082

Epoch: 5| Step: 7
Training loss: 4.151007766617117
Validation loss: 4.219607044884852

Epoch: 5| Step: 8
Training loss: 4.9903564915953655
Validation loss: 4.194657128413288

Epoch: 5| Step: 9
Training loss: 5.085496827707116
Validation loss: 4.189085203480277

Epoch: 5| Step: 10
Training loss: 3.8945799798700365
Validation loss: 4.175183346668157

Epoch: 17| Step: 0
Training loss: 4.758018150958812
Validation loss: 4.152262791301336

Epoch: 5| Step: 1
Training loss: 4.271189877893801
Validation loss: 4.137780834967874

Epoch: 5| Step: 2
Training loss: 4.6450835622245625
Validation loss: 4.115509118877999

Epoch: 5| Step: 3
Training loss: 4.015455902932871
Validation loss: 4.103200175385934

Epoch: 5| Step: 4
Training loss: 3.961841969049211
Validation loss: 4.0812311233701815

Epoch: 5| Step: 5
Training loss: 4.3531232736642975
Validation loss: 4.054095387804699

Epoch: 5| Step: 6
Training loss: 3.9446066045486003
Validation loss: 4.046275151531242

Epoch: 5| Step: 7
Training loss: 3.6974758404812307
Validation loss: 4.0264703808126265

Epoch: 5| Step: 8
Training loss: 3.9022635908934316
Validation loss: 3.9976207030047175

Epoch: 5| Step: 9
Training loss: 3.862308885111499
Validation loss: 3.9883896080585086

Epoch: 5| Step: 10
Training loss: 4.562626249055934
Validation loss: 3.9638541269604226

Epoch: 18| Step: 0
Training loss: 4.6366224913660385
Validation loss: 3.9503089296951304

Epoch: 5| Step: 1
Training loss: 3.528381803237011
Validation loss: 3.9349542029291107

Epoch: 5| Step: 2
Training loss: 3.463778257774415
Validation loss: 3.9213052893180635

Epoch: 5| Step: 3
Training loss: 4.728237693840394
Validation loss: 3.9032293769869586

Epoch: 5| Step: 4
Training loss: 3.570444183926172
Validation loss: 3.87298310791291

Epoch: 5| Step: 5
Training loss: 4.214276463865606
Validation loss: 3.860872495363335

Epoch: 5| Step: 6
Training loss: 3.672440765297867
Validation loss: 3.838012815607998

Epoch: 5| Step: 7
Training loss: 3.4149945206828582
Validation loss: 3.8137875270222965

Epoch: 5| Step: 8
Training loss: 2.9801428380438484
Validation loss: 3.8030073880660415

Epoch: 5| Step: 9
Training loss: 5.228665189773395
Validation loss: 3.771118353623028

Epoch: 5| Step: 10
Training loss: 3.8512727887681315
Validation loss: 3.76007215848675

Epoch: 19| Step: 0
Training loss: 3.798098520070808
Validation loss: 3.733876659883583

Epoch: 5| Step: 1
Training loss: 3.5469161379945002
Validation loss: 3.7213497310669887

Epoch: 5| Step: 2
Training loss: 3.9161423812480973
Validation loss: 3.696861677914065

Epoch: 5| Step: 3
Training loss: 3.119292425714778
Validation loss: 3.6726512387792454

Epoch: 5| Step: 4
Training loss: 3.520302335586469
Validation loss: 3.669572361733952

Epoch: 5| Step: 5
Training loss: 3.7106157344055406
Validation loss: 3.6438495457541653

Epoch: 5| Step: 6
Training loss: 4.442428216305949
Validation loss: 3.6219203541933505

Epoch: 5| Step: 7
Training loss: 3.5981411850409364
Validation loss: 3.5976185582616336

Epoch: 5| Step: 8
Training loss: 4.3855215801577465
Validation loss: 3.5793930151942033

Epoch: 5| Step: 9
Training loss: 3.6097932383324594
Validation loss: 3.5469669230565213

Epoch: 5| Step: 10
Training loss: 3.9209628925275024
Validation loss: 3.5253379976241015

Epoch: 20| Step: 0
Training loss: 3.4580535928733207
Validation loss: 3.5025228821291425

Epoch: 5| Step: 1
Training loss: 3.9866698593798797
Validation loss: 3.4885918291200517

Epoch: 5| Step: 2
Training loss: 3.3753687339260905
Validation loss: 3.471448394739915

Epoch: 5| Step: 3
Training loss: 3.762253387581459
Validation loss: 3.4410862253517025

Epoch: 5| Step: 4
Training loss: 3.1484670330016082
Validation loss: 3.425186457123383

Epoch: 5| Step: 5
Training loss: 3.697943688235707
Validation loss: 3.402174651522892

Epoch: 5| Step: 6
Training loss: 3.8097937312321952
Validation loss: 3.3935576866114814

Epoch: 5| Step: 7
Training loss: 3.4325027207977112
Validation loss: 3.370480142504059

Epoch: 5| Step: 8
Training loss: 3.86951353460701
Validation loss: 3.3596988828054184

Epoch: 5| Step: 9
Training loss: 3.002175019501242
Validation loss: 3.336941420372302

Epoch: 5| Step: 10
Training loss: 3.997447152900758
Validation loss: 3.314519715731981

Epoch: 21| Step: 0
Training loss: 3.0523280717199563
Validation loss: 3.2871527703161854

Epoch: 5| Step: 1
Training loss: 3.136037940301854
Validation loss: 3.2702581180573933

Epoch: 5| Step: 2
Training loss: 4.097869663844905
Validation loss: 3.2443554138460575

Epoch: 5| Step: 3
Training loss: 2.783638260940785
Validation loss: 3.236709338096844

Epoch: 5| Step: 4
Training loss: 3.2418380365126924
Validation loss: 3.213788796841525

Epoch: 5| Step: 5
Training loss: 4.338343926816286
Validation loss: 3.185204855674067

Epoch: 5| Step: 6
Training loss: 3.553361247050259
Validation loss: 3.16633675858539

Epoch: 5| Step: 7
Training loss: 3.731358542852373
Validation loss: 3.1546304601354476

Epoch: 5| Step: 8
Training loss: 3.4996126505541234
Validation loss: 3.1204965188842064

Epoch: 5| Step: 9
Training loss: 2.5024900433535677
Validation loss: 3.099157909165456

Epoch: 5| Step: 10
Training loss: 3.3051365035545666
Validation loss: 3.0952499033556076

Epoch: 22| Step: 0
Training loss: 2.9656957073244015
Validation loss: 3.0658077268531176

Epoch: 5| Step: 1
Training loss: 3.547070334009003
Validation loss: 3.0669125211839727

Epoch: 5| Step: 2
Training loss: 3.681922703467785
Validation loss: 3.051305956131436

Epoch: 5| Step: 3
Training loss: 3.4363992489193733
Validation loss: 3.0424720269967653

Epoch: 5| Step: 4
Training loss: 3.031498377740883
Validation loss: 3.0099521986469306

Epoch: 5| Step: 5
Training loss: 2.8319566598474624
Validation loss: 2.99513439526853

Epoch: 5| Step: 6
Training loss: 3.002924129822093
Validation loss: 2.9830918545410827

Epoch: 5| Step: 7
Training loss: 3.6621283853673243
Validation loss: 2.961519071715822

Epoch: 5| Step: 8
Training loss: 3.1000176336955945
Validation loss: 2.937433431862083

Epoch: 5| Step: 9
Training loss: 3.451898417395998
Validation loss: 2.9508295460932765

Epoch: 5| Step: 10
Training loss: 2.9446857301548226
Validation loss: 2.926045727228215

Epoch: 23| Step: 0
Training loss: 3.2314274575001583
Validation loss: 2.9076730995318902

Epoch: 5| Step: 1
Training loss: 3.266911887352354
Validation loss: 2.889051249876667

Epoch: 5| Step: 2
Training loss: 3.3349803829045097
Validation loss: 2.8754002451672527

Epoch: 5| Step: 3
Training loss: 3.5354439465547967
Validation loss: 2.8802411367689045

Epoch: 5| Step: 4
Training loss: 2.6793358574854014
Validation loss: 2.8703560973216966

Epoch: 5| Step: 5
Training loss: 2.725628012288333
Validation loss: 2.8537017201646604

Epoch: 5| Step: 6
Training loss: 2.657044056404745
Validation loss: 2.858170305152686

Epoch: 5| Step: 7
Training loss: 2.5508012522545576
Validation loss: 2.8196597938878276

Epoch: 5| Step: 8
Training loss: 3.318510253062753
Validation loss: 2.8279943005973944

Epoch: 5| Step: 9
Training loss: 3.4514327239423905
Validation loss: 2.8212255098196803

Epoch: 5| Step: 10
Training loss: 3.7639759299210973
Validation loss: 2.8067452826696657

Epoch: 24| Step: 0
Training loss: 2.5474638462964014
Validation loss: 2.782075763750154

Epoch: 5| Step: 1
Training loss: 3.2929487668988866
Validation loss: 2.7809567086558546

Epoch: 5| Step: 2
Training loss: 3.1766832110149936
Validation loss: 2.789980595159867

Epoch: 5| Step: 3
Training loss: 3.362800023190746
Validation loss: 2.7663627610552144

Epoch: 5| Step: 4
Training loss: 2.4735638969932725
Validation loss: 2.755560359397672

Epoch: 5| Step: 5
Training loss: 3.7091245057289495
Validation loss: 2.75098440724008

Epoch: 5| Step: 6
Training loss: 2.7978841143939692
Validation loss: 2.743065279078943

Epoch: 5| Step: 7
Training loss: 2.8627869308124327
Validation loss: 2.72843221591744

Epoch: 5| Step: 8
Training loss: 2.8432881745318426
Validation loss: 2.7183739299879717

Epoch: 5| Step: 9
Training loss: 3.2594411607356637
Validation loss: 2.71126039629442

Epoch: 5| Step: 10
Training loss: 3.4756677054690273
Validation loss: 2.7230487201110067

Epoch: 25| Step: 0
Training loss: 3.6184889098014996
Validation loss: 2.705626706743742

Epoch: 5| Step: 1
Training loss: 3.036470774150159
Validation loss: 2.70234181637599

Epoch: 5| Step: 2
Training loss: 3.0452190887859047
Validation loss: 2.7064079480137893

Epoch: 5| Step: 3
Training loss: 3.2866718365961956
Validation loss: 2.693509378519803

Epoch: 5| Step: 4
Training loss: 2.83130429112673
Validation loss: 2.6913966618329948

Epoch: 5| Step: 5
Training loss: 3.278144747635497
Validation loss: 2.6911539010386822

Epoch: 5| Step: 6
Training loss: 2.848891086005549
Validation loss: 2.679570826727797

Epoch: 5| Step: 7
Training loss: 3.0072919915391467
Validation loss: 2.6811841491618353

Epoch: 5| Step: 8
Training loss: 2.675270377069126
Validation loss: 2.667339610807608

Epoch: 5| Step: 9
Training loss: 3.2067531971614036
Validation loss: 2.688194082251823

Epoch: 5| Step: 10
Training loss: 2.6295331233060595
Validation loss: 2.6727284069599695

Epoch: 26| Step: 0
Training loss: 3.1495827231970317
Validation loss: 2.6635041130886385

Epoch: 5| Step: 1
Training loss: 2.7962731933996086
Validation loss: 2.6650660162690327

Epoch: 5| Step: 2
Training loss: 3.351715866883608
Validation loss: 2.672049312761977

Epoch: 5| Step: 3
Training loss: 2.745424278558656
Validation loss: 2.684658502467799

Epoch: 5| Step: 4
Training loss: 2.934240440070316
Validation loss: 2.6555678049555205

Epoch: 5| Step: 5
Training loss: 3.390264676544645
Validation loss: 2.6428788690976197

Epoch: 5| Step: 6
Training loss: 3.0573729591457957
Validation loss: 2.661652930283443

Epoch: 5| Step: 7
Training loss: 2.733162660232431
Validation loss: 2.6655590951579318

Epoch: 5| Step: 8
Training loss: 3.2203109067480744
Validation loss: 2.6632590466383887

Epoch: 5| Step: 9
Training loss: 2.629047905788363
Validation loss: 2.6524098481232237

Epoch: 5| Step: 10
Training loss: 3.283579843887845
Validation loss: 2.645133301752924

Epoch: 27| Step: 0
Training loss: 2.7578408561307777
Validation loss: 2.648071592899692

Epoch: 5| Step: 1
Training loss: 3.073275080754221
Validation loss: 2.640952224781353

Epoch: 5| Step: 2
Training loss: 3.234654658661914
Validation loss: 2.6590720032994657

Epoch: 5| Step: 3
Training loss: 2.533313616458392
Validation loss: 2.63012494263981

Epoch: 5| Step: 4
Training loss: 3.197188966609263
Validation loss: 2.6372006411137936

Epoch: 5| Step: 5
Training loss: 3.4123505723815963
Validation loss: 2.6399770476784425

Epoch: 5| Step: 6
Training loss: 2.8195978247186173
Validation loss: 2.653867166470526

Epoch: 5| Step: 7
Training loss: 2.6514859729664133
Validation loss: 2.6491071100362227

Epoch: 5| Step: 8
Training loss: 3.3634089889813645
Validation loss: 2.6524754091535394

Epoch: 5| Step: 9
Training loss: 3.0481301876709943
Validation loss: 2.646094966126047

Epoch: 5| Step: 10
Training loss: 3.192794441858327
Validation loss: 2.6465281519784254

Epoch: 28| Step: 0
Training loss: 2.770115197606015
Validation loss: 2.6473912694079313

Epoch: 5| Step: 1
Training loss: 3.442436419130477
Validation loss: 2.6248276727680344

Epoch: 5| Step: 2
Training loss: 2.572961248302452
Validation loss: 2.6284833952374846

Epoch: 5| Step: 3
Training loss: 2.9829401691305013
Validation loss: 2.645193801869418

Epoch: 5| Step: 4
Training loss: 3.1739080518782115
Validation loss: 2.6265561442278673

Epoch: 5| Step: 5
Training loss: 3.5326828201835525
Validation loss: 2.629449085916532

Epoch: 5| Step: 6
Training loss: 2.759118654260997
Validation loss: 2.6262003512218715

Epoch: 5| Step: 7
Training loss: 3.292907786681758
Validation loss: 2.6316272580796976

Epoch: 5| Step: 8
Training loss: 3.509623376577943
Validation loss: 2.6245687240816182

Epoch: 5| Step: 9
Training loss: 2.331368459192388
Validation loss: 2.6221331749875803

Epoch: 5| Step: 10
Training loss: 2.6392661667104886
Validation loss: 2.6294578304306393

Epoch: 29| Step: 0
Training loss: 2.498996151607095
Validation loss: 2.6249139442538088

Epoch: 5| Step: 1
Training loss: 3.3527546787458298
Validation loss: 2.626754148271187

Epoch: 5| Step: 2
Training loss: 3.5363050080191347
Validation loss: 2.6319983539223943

Epoch: 5| Step: 3
Training loss: 3.1998169727435375
Validation loss: 2.6188988184412576

Epoch: 5| Step: 4
Training loss: 2.974444258898121
Validation loss: 2.631975597603036

Epoch: 5| Step: 5
Training loss: 3.1748245130686734
Validation loss: 2.6290089289255927

Epoch: 5| Step: 6
Training loss: 2.6579063188162353
Validation loss: 2.629342801037676

Epoch: 5| Step: 7
Training loss: 3.0671158862389176
Validation loss: 2.6323286524607417

Epoch: 5| Step: 8
Training loss: 2.38970969635672
Validation loss: 2.637927633039628

Epoch: 5| Step: 9
Training loss: 2.867754389047444
Validation loss: 2.6217202173517915

Epoch: 5| Step: 10
Training loss: 3.3688676010299132
Validation loss: 2.6272832215775304

Epoch: 30| Step: 0
Training loss: 2.9842923936845427
Validation loss: 2.622288187504396

Epoch: 5| Step: 1
Training loss: 2.8326381690726414
Validation loss: 2.6291081207898666

Epoch: 5| Step: 2
Training loss: 3.5021701624218116
Validation loss: 2.6269619998095033

Epoch: 5| Step: 3
Training loss: 2.9069836418602306
Validation loss: 2.6454674181335203

Epoch: 5| Step: 4
Training loss: 2.58437685245692
Validation loss: 2.6285961069170094

Epoch: 5| Step: 5
Training loss: 3.148147304592975
Validation loss: 2.623849727108711

Epoch: 5| Step: 6
Training loss: 3.1450575488818657
Validation loss: 2.6121284704659105

Epoch: 5| Step: 7
Training loss: 2.2813232619135544
Validation loss: 2.6401092371227985

Epoch: 5| Step: 8
Training loss: 3.0596652242998807
Validation loss: 2.6174990124997404

Epoch: 5| Step: 9
Training loss: 2.7457058932455545
Validation loss: 2.622220420883132

Epoch: 5| Step: 10
Training loss: 3.7654064320807232
Validation loss: 2.6245112747913164

Epoch: 31| Step: 0
Training loss: 3.2824972053045274
Validation loss: 2.6306465192480593

Epoch: 5| Step: 1
Training loss: 2.237701074526709
Validation loss: 2.601553523762514

Epoch: 5| Step: 2
Training loss: 2.7971891940643085
Validation loss: 2.632103726080569

Epoch: 5| Step: 3
Training loss: 3.159551423502062
Validation loss: 2.6196163039452944

Epoch: 5| Step: 4
Training loss: 2.695499137276816
Validation loss: 2.624985463873193

Epoch: 5| Step: 5
Training loss: 3.3892220262168076
Validation loss: 2.6199382492337024

Epoch: 5| Step: 6
Training loss: 2.8781911306001096
Validation loss: 2.6350120932015946

Epoch: 5| Step: 7
Training loss: 2.872614658813003
Validation loss: 2.6313359060516386

Epoch: 5| Step: 8
Training loss: 3.3779631428579613
Validation loss: 2.625280387736678

Epoch: 5| Step: 9
Training loss: 3.088354472791194
Validation loss: 2.627083595537849

Epoch: 5| Step: 10
Training loss: 3.2531986634986576
Validation loss: 2.6227083394004764

Epoch: 32| Step: 0
Training loss: 2.8030143146074527
Validation loss: 2.645478041047275

Epoch: 5| Step: 1
Training loss: 3.437493619045924
Validation loss: 2.6306342635460873

Epoch: 5| Step: 2
Training loss: 2.740028857615183
Validation loss: 2.6200881695951104

Epoch: 5| Step: 3
Training loss: 2.5797769513671005
Validation loss: 2.6306082843096465

Epoch: 5| Step: 4
Training loss: 3.1407558736529877
Validation loss: 2.6105943983967776

Epoch: 5| Step: 5
Training loss: 2.982351046197672
Validation loss: 2.633915349812977

Epoch: 5| Step: 6
Training loss: 3.188727348244191
Validation loss: 2.6287544325013235

Epoch: 5| Step: 7
Training loss: 3.327645567268591
Validation loss: 2.6136828973082524

Epoch: 5| Step: 8
Training loss: 2.944215119826329
Validation loss: 2.6199961471404043

Epoch: 5| Step: 9
Training loss: 3.34991015982794
Validation loss: 2.6155603578105877

Epoch: 5| Step: 10
Training loss: 2.405651563070099
Validation loss: 2.6040776378221

Epoch: 33| Step: 0
Training loss: 2.80321436370649
Validation loss: 2.6122424697865343

Epoch: 5| Step: 1
Training loss: 2.8719137461044117
Validation loss: 2.612364277643493

Epoch: 5| Step: 2
Training loss: 3.7640481393555185
Validation loss: 2.61040178591019

Epoch: 5| Step: 3
Training loss: 3.0101230218530026
Validation loss: 2.619171987837354

Epoch: 5| Step: 4
Training loss: 2.935555321636218
Validation loss: 2.626321443387985

Epoch: 5| Step: 5
Training loss: 2.930051084209813
Validation loss: 2.609479099786238

Epoch: 5| Step: 6
Training loss: 2.9268876595216224
Validation loss: 2.617819331059319

Epoch: 5| Step: 7
Training loss: 2.7930559051360015
Validation loss: 2.624820190342005

Epoch: 5| Step: 8
Training loss: 2.7578443141755606
Validation loss: 2.6117289521894387

Epoch: 5| Step: 9
Training loss: 2.9872593547353086
Validation loss: 2.599663179539618

Epoch: 5| Step: 10
Training loss: 3.228276940284682
Validation loss: 2.6201293201902596

Epoch: 34| Step: 0
Training loss: 2.8230418685418974
Validation loss: 2.618189799007595

Epoch: 5| Step: 1
Training loss: 3.3970145918897976
Validation loss: 2.6212751580898956

Epoch: 5| Step: 2
Training loss: 3.215292842455278
Validation loss: 2.6371321300261292

Epoch: 5| Step: 3
Training loss: 3.0769507012594395
Validation loss: 2.619703296045189

Epoch: 5| Step: 4
Training loss: 3.179146385544814
Validation loss: 2.6137488206373556

Epoch: 5| Step: 5
Training loss: 2.88246826763172
Validation loss: 2.620865950047322

Epoch: 5| Step: 6
Training loss: 3.084126310361915
Validation loss: 2.606289667552742

Epoch: 5| Step: 7
Training loss: 3.0876877082877296
Validation loss: 2.618893001809677

Epoch: 5| Step: 8
Training loss: 2.31323642861213
Validation loss: 2.6181970741957667

Epoch: 5| Step: 9
Training loss: 3.3530676958603833
Validation loss: 2.6054594664526585

Epoch: 5| Step: 10
Training loss: 2.3131844822085568
Validation loss: 2.5987735634687916

Epoch: 35| Step: 0
Training loss: 3.3297325872324985
Validation loss: 2.627669051289901

Epoch: 5| Step: 1
Training loss: 2.890415823301519
Validation loss: 2.6312798983736894

Epoch: 5| Step: 2
Training loss: 2.3939164385190677
Validation loss: 2.612765459532884

Epoch: 5| Step: 3
Training loss: 3.5369569017816764
Validation loss: 2.6017944021698587

Epoch: 5| Step: 4
Training loss: 2.654796247404756
Validation loss: 2.5998009716746773

Epoch: 5| Step: 5
Training loss: 2.8634334591407495
Validation loss: 2.617591060061294

Epoch: 5| Step: 6
Training loss: 3.1867215571072003
Validation loss: 2.6116540977037945

Epoch: 5| Step: 7
Training loss: 2.2561711347750397
Validation loss: 2.6058597329452344

Epoch: 5| Step: 8
Training loss: 3.1356594629581767
Validation loss: 2.6313339828352027

Epoch: 5| Step: 9
Training loss: 3.5368826175762034
Validation loss: 2.6036348931235294

Epoch: 5| Step: 10
Training loss: 3.0034893723857556
Validation loss: 2.6187550992194315

Epoch: 36| Step: 0
Training loss: 2.6838945556144225
Validation loss: 2.6114687345984406

Epoch: 5| Step: 1
Training loss: 3.4213568586916803
Validation loss: 2.5944833876313216

Epoch: 5| Step: 2
Training loss: 2.825525933655151
Validation loss: 2.6277601572467946

Epoch: 5| Step: 3
Training loss: 3.0455233192183626
Validation loss: 2.610444057521796

Epoch: 5| Step: 4
Training loss: 2.8476619432599124
Validation loss: 2.625987797296892

Epoch: 5| Step: 5
Training loss: 2.7298157092480655
Validation loss: 2.633105203594679

Epoch: 5| Step: 6
Training loss: 2.3221779462658616
Validation loss: 2.616933420380401

Epoch: 5| Step: 7
Training loss: 2.7935664037674672
Validation loss: 2.6112047470427684

Epoch: 5| Step: 8
Training loss: 3.888420246284955
Validation loss: 2.609923388801394

Epoch: 5| Step: 9
Training loss: 2.8350014731320923
Validation loss: 2.60927223718947

Epoch: 5| Step: 10
Training loss: 3.1483484449641534
Validation loss: 2.6249313638506506

Epoch: 37| Step: 0
Training loss: 2.5070789250601413
Validation loss: 2.631994829885985

Epoch: 5| Step: 1
Training loss: 3.0157828017933874
Validation loss: 2.620581637672638

Epoch: 5| Step: 2
Training loss: 2.57168319932523
Validation loss: 2.6051282588103115

Epoch: 5| Step: 3
Training loss: 2.3917335483180193
Validation loss: 2.6063364134561184

Epoch: 5| Step: 4
Training loss: 3.044256875446955
Validation loss: 2.6119505906535796

Epoch: 5| Step: 5
Training loss: 2.8331157002358465
Validation loss: 2.6080873399787357

Epoch: 5| Step: 6
Training loss: 2.8887862672266507
Validation loss: 2.6065425156721833

Epoch: 5| Step: 7
Training loss: 3.3125443365620715
Validation loss: 2.5995544786268012

Epoch: 5| Step: 8
Training loss: 2.9683826420019335
Validation loss: 2.6095153994826084

Epoch: 5| Step: 9
Training loss: 3.7393885201348898
Validation loss: 2.5990533104658726

Epoch: 5| Step: 10
Training loss: 3.2504547974677114
Validation loss: 2.601363842877307

Epoch: 38| Step: 0
Training loss: 2.3405654634815862
Validation loss: 2.608992507977142

Epoch: 5| Step: 1
Training loss: 3.217817893755874
Validation loss: 2.604871169419834

Epoch: 5| Step: 2
Training loss: 3.435199175616993
Validation loss: 2.618090196417332

Epoch: 5| Step: 3
Training loss: 2.333671670180465
Validation loss: 2.5969971959317704

Epoch: 5| Step: 4
Training loss: 3.3012777282405446
Validation loss: 2.613753662985631

Epoch: 5| Step: 5
Training loss: 2.617539658268784
Validation loss: 2.6300201345435896

Epoch: 5| Step: 6
Training loss: 3.5333190527813305
Validation loss: 2.6076693762852066

Epoch: 5| Step: 7
Training loss: 3.133049934503094
Validation loss: 2.614357308102625

Epoch: 5| Step: 8
Training loss: 2.9036816148591758
Validation loss: 2.6106737547360117

Epoch: 5| Step: 9
Training loss: 2.858285665160946
Validation loss: 2.61937993209631

Epoch: 5| Step: 10
Training loss: 2.9277161336156214
Validation loss: 2.6071982315738063

Epoch: 39| Step: 0
Training loss: 2.6946797830727087
Validation loss: 2.605163457938067

Epoch: 5| Step: 1
Training loss: 2.9792732450899315
Validation loss: 2.6182240069035183

Epoch: 5| Step: 2
Training loss: 2.6064749494849324
Validation loss: 2.6065992073248268

Epoch: 5| Step: 3
Training loss: 3.3140986201841733
Validation loss: 2.5929544422016533

Epoch: 5| Step: 4
Training loss: 3.2176769653121777
Validation loss: 2.6048635947308934

Epoch: 5| Step: 5
Training loss: 3.923258503211332
Validation loss: 2.593979701913682

Epoch: 5| Step: 6
Training loss: 3.0113259144640185
Validation loss: 2.6146626924744556

Epoch: 5| Step: 7
Training loss: 2.538557829953469
Validation loss: 2.6139892308752835

Epoch: 5| Step: 8
Training loss: 2.8365536695567526
Validation loss: 2.6150494519410605

Epoch: 5| Step: 9
Training loss: 2.546678968503043
Validation loss: 2.6098092130340977

Epoch: 5| Step: 10
Training loss: 2.8019134488893727
Validation loss: 2.607464361080856

Epoch: 40| Step: 0
Training loss: 3.2312013840107836
Validation loss: 2.5895763986724165

Epoch: 5| Step: 1
Training loss: 2.9084933246957614
Validation loss: 2.6002196647876783

Epoch: 5| Step: 2
Training loss: 2.4976930464634375
Validation loss: 2.6108672269514144

Epoch: 5| Step: 3
Training loss: 2.896874348168161
Validation loss: 2.608023218866252

Epoch: 5| Step: 4
Training loss: 2.8461384227596502
Validation loss: 2.5962510704935586

Epoch: 5| Step: 5
Training loss: 2.8328403343184045
Validation loss: 2.605492179617545

Epoch: 5| Step: 6
Training loss: 3.145044813215899
Validation loss: 2.6038156971025135

Epoch: 5| Step: 7
Training loss: 3.1010294343252
Validation loss: 2.6024631472472284

Epoch: 5| Step: 8
Training loss: 2.9920687739071985
Validation loss: 2.5900311368599787

Epoch: 5| Step: 9
Training loss: 2.876232712224128
Validation loss: 2.608137324886201

Epoch: 5| Step: 10
Training loss: 3.380203897544799
Validation loss: 2.6102120742568786

Epoch: 41| Step: 0
Training loss: 3.6215561918170316
Validation loss: 2.6028975382415935

Epoch: 5| Step: 1
Training loss: 3.20145607245933
Validation loss: 2.593782334040291

Epoch: 5| Step: 2
Training loss: 3.0792606021109825
Validation loss: 2.5917867620496966

Epoch: 5| Step: 3
Training loss: 3.1455775445351972
Validation loss: 2.5998764736027273

Epoch: 5| Step: 4
Training loss: 2.4606404443110335
Validation loss: 2.5934120192267116

Epoch: 5| Step: 5
Training loss: 2.89271675926431
Validation loss: 2.592009898530803

Epoch: 5| Step: 6
Training loss: 3.5045909744147345
Validation loss: 2.5811395446858567

Epoch: 5| Step: 7
Training loss: 2.903786384134525
Validation loss: 2.5847837400240268

Epoch: 5| Step: 8
Training loss: 2.3955723053640128
Validation loss: 2.587678082669845

Epoch: 5| Step: 9
Training loss: 2.4075952707931534
Validation loss: 2.5764513447013235

Epoch: 5| Step: 10
Training loss: 2.744506898376168
Validation loss: 2.592272119382672

Epoch: 42| Step: 0
Training loss: 3.5003839009776674
Validation loss: 2.5895347338473047

Epoch: 5| Step: 1
Training loss: 2.8410413648849437
Validation loss: 2.591956370315534

Epoch: 5| Step: 2
Training loss: 2.772174300385004
Validation loss: 2.597680863300756

Epoch: 5| Step: 3
Training loss: 2.8084440867906113
Validation loss: 2.591158239531049

Epoch: 5| Step: 4
Training loss: 3.18555989780255
Validation loss: 2.5804641249950415

Epoch: 5| Step: 5
Training loss: 2.9398036608368967
Validation loss: 2.5913013019361517

Epoch: 5| Step: 6
Training loss: 2.7832104920406886
Validation loss: 2.5826630369437638

Epoch: 5| Step: 7
Training loss: 2.9153542608589773
Validation loss: 2.585642465043553

Epoch: 5| Step: 8
Training loss: 2.986721534338553
Validation loss: 2.5918607200009736

Epoch: 5| Step: 9
Training loss: 3.197800691139757
Validation loss: 2.592520030650703

Epoch: 5| Step: 10
Training loss: 2.621263205768317
Validation loss: 2.5846993714484165

Epoch: 43| Step: 0
Training loss: 3.5092675264772413
Validation loss: 2.566975868222915

Epoch: 5| Step: 1
Training loss: 3.0091248658864416
Validation loss: 2.6092280807727137

Epoch: 5| Step: 2
Training loss: 2.7728792730189387
Validation loss: 2.5943918354463302

Epoch: 5| Step: 3
Training loss: 2.5453790120894664
Validation loss: 2.572403324033914

Epoch: 5| Step: 4
Training loss: 3.2775775521397703
Validation loss: 2.5920658675661596

Epoch: 5| Step: 5
Training loss: 2.4976393998872006
Validation loss: 2.5897162876313393

Epoch: 5| Step: 6
Training loss: 2.736793277040424
Validation loss: 2.5863693992424546

Epoch: 5| Step: 7
Training loss: 2.801459279581591
Validation loss: 2.5878728213464988

Epoch: 5| Step: 8
Training loss: 3.083331511900768
Validation loss: 2.5977222198318093

Epoch: 5| Step: 9
Training loss: 3.0579022518625085
Validation loss: 2.576048781732606

Epoch: 5| Step: 10
Training loss: 3.1328350266100955
Validation loss: 2.597684077618766

Epoch: 44| Step: 0
Training loss: 2.697008920396671
Validation loss: 2.581296074465551

Epoch: 5| Step: 1
Training loss: 3.424933890936816
Validation loss: 2.584082167106732

Epoch: 5| Step: 2
Training loss: 2.8654846622959815
Validation loss: 2.5884026416019523

Epoch: 5| Step: 3
Training loss: 3.041595736856699
Validation loss: 2.5949575583575575

Epoch: 5| Step: 4
Training loss: 2.241256361025258
Validation loss: 2.592211085302486

Epoch: 5| Step: 5
Training loss: 2.553588257390226
Validation loss: 2.5862197317974873

Epoch: 5| Step: 6
Training loss: 2.9240058839793566
Validation loss: 2.5783767114930334

Epoch: 5| Step: 7
Training loss: 2.8711804435253154
Validation loss: 2.588022278284785

Epoch: 5| Step: 8
Training loss: 2.993980089874422
Validation loss: 2.5815329154342805

Epoch: 5| Step: 9
Training loss: 2.9311083793996957
Validation loss: 2.601511038708795

Epoch: 5| Step: 10
Training loss: 3.919402053343024
Validation loss: 2.5820966867457327

Epoch: 45| Step: 0
Training loss: 2.4910805853906366
Validation loss: 2.5913086654759008

Epoch: 5| Step: 1
Training loss: 2.1794801018601424
Validation loss: 2.6098855933637104

Epoch: 5| Step: 2
Training loss: 3.257475105945851
Validation loss: 2.5844319660735033

Epoch: 5| Step: 3
Training loss: 2.906408490197076
Validation loss: 2.5832753652948655

Epoch: 5| Step: 4
Training loss: 3.1244478882393354
Validation loss: 2.5847679462314668

Epoch: 5| Step: 5
Training loss: 3.6795251247596874
Validation loss: 2.5958853238332322

Epoch: 5| Step: 6
Training loss: 3.255821589369073
Validation loss: 2.577338193653637

Epoch: 5| Step: 7
Training loss: 2.6578897239594053
Validation loss: 2.6036091397613292

Epoch: 5| Step: 8
Training loss: 2.6924619777194336
Validation loss: 2.57994988503846

Epoch: 5| Step: 9
Training loss: 2.9060711549263756
Validation loss: 2.586924260895903

Epoch: 5| Step: 10
Training loss: 3.1036887771768824
Validation loss: 2.5917743483096864

Epoch: 46| Step: 0
Training loss: 3.1136436107933387
Validation loss: 2.5916372847977907

Epoch: 5| Step: 1
Training loss: 2.432479970914237
Validation loss: 2.5956495168959037

Epoch: 5| Step: 2
Training loss: 2.7883947091227106
Validation loss: 2.596959028262559

Epoch: 5| Step: 3
Training loss: 2.687761338413079
Validation loss: 2.5667676047091743

Epoch: 5| Step: 4
Training loss: 2.7394364079479776
Validation loss: 2.572573813620517

Epoch: 5| Step: 5
Training loss: 2.9435262347983735
Validation loss: 2.5819262957547435

Epoch: 5| Step: 6
Training loss: 3.1158384688087564
Validation loss: 2.6016287807062524

Epoch: 5| Step: 7
Training loss: 2.8022945470986325
Validation loss: 2.580901282942001

Epoch: 5| Step: 8
Training loss: 3.204185496367118
Validation loss: 2.5901452961454012

Epoch: 5| Step: 9
Training loss: 3.591924186968483
Validation loss: 2.574289915369127

Epoch: 5| Step: 10
Training loss: 2.9578388678437912
Validation loss: 2.588272875716346

Epoch: 47| Step: 0
Training loss: 3.323987511236093
Validation loss: 2.5732219482218714

Epoch: 5| Step: 1
Training loss: 2.977048018299214
Validation loss: 2.5811954405423236

Epoch: 5| Step: 2
Training loss: 3.072139283896986
Validation loss: 2.5728481131309477

Epoch: 5| Step: 3
Training loss: 3.175159876845617
Validation loss: 2.585814347144722

Epoch: 5| Step: 4
Training loss: 2.8269682989890015
Validation loss: 2.5751985100860506

Epoch: 5| Step: 5
Training loss: 2.3784954048258298
Validation loss: 2.569420096154455

Epoch: 5| Step: 6
Training loss: 3.25365330696418
Validation loss: 2.5970750566719722

Epoch: 5| Step: 7
Training loss: 3.182839221121187
Validation loss: 2.602178353209102

Epoch: 5| Step: 8
Training loss: 2.1110909723135545
Validation loss: 2.5737508340166344

Epoch: 5| Step: 9
Training loss: 3.1946135784837137
Validation loss: 2.5747261441745266

Epoch: 5| Step: 10
Training loss: 2.6862728067160364
Validation loss: 2.598235038520077

Epoch: 48| Step: 0
Training loss: 2.920045657388828
Validation loss: 2.584272441864348

Epoch: 5| Step: 1
Training loss: 3.403777318670423
Validation loss: 2.576961108689813

Epoch: 5| Step: 2
Training loss: 3.4337481744995375
Validation loss: 2.5682388579956608

Epoch: 5| Step: 3
Training loss: 3.171380967626136
Validation loss: 2.559813671333072

Epoch: 5| Step: 4
Training loss: 3.2552575354354496
Validation loss: 2.5823056111529703

Epoch: 5| Step: 5
Training loss: 2.163959266596535
Validation loss: 2.5985460611369535

Epoch: 5| Step: 6
Training loss: 2.455758303283678
Validation loss: 2.564906449748325

Epoch: 5| Step: 7
Training loss: 2.7796971132778014
Validation loss: 2.561915932598801

Epoch: 5| Step: 8
Training loss: 2.5077729504280164
Validation loss: 2.5804810985525433

Epoch: 5| Step: 9
Training loss: 3.178347593481209
Validation loss: 2.5884737953460215

Epoch: 5| Step: 10
Training loss: 2.7661996880121906
Validation loss: 2.572286614298688

Epoch: 49| Step: 0
Training loss: 3.0974145629036007
Validation loss: 2.584974432236639

Epoch: 5| Step: 1
Training loss: 3.3465007269958407
Validation loss: 2.579042618305842

Epoch: 5| Step: 2
Training loss: 2.5483089225137103
Validation loss: 2.5801012108745978

Epoch: 5| Step: 3
Training loss: 2.7663488787713653
Validation loss: 2.581415801128832

Epoch: 5| Step: 4
Training loss: 3.002581757028708
Validation loss: 2.58559571880571

Epoch: 5| Step: 5
Training loss: 3.334252675752352
Validation loss: 2.5793051585586464

Epoch: 5| Step: 6
Training loss: 2.603540716601238
Validation loss: 2.5880833640970975

Epoch: 5| Step: 7
Training loss: 2.920759508303736
Validation loss: 2.573593285497748

Epoch: 5| Step: 8
Training loss: 2.863929997702158
Validation loss: 2.5828252035267956

Epoch: 5| Step: 9
Training loss: 2.542765947684405
Validation loss: 2.570310772495242

Epoch: 5| Step: 10
Training loss: 3.167802874529898
Validation loss: 2.5736213105448074

Epoch: 50| Step: 0
Training loss: 3.4516259991877805
Validation loss: 2.5555095189914585

Epoch: 5| Step: 1
Training loss: 2.5839730260344593
Validation loss: 2.5683725043000654

Epoch: 5| Step: 2
Training loss: 2.64767634163823
Validation loss: 2.5795305772801367

Epoch: 5| Step: 3
Training loss: 3.289531900794211
Validation loss: 2.56018979621503

Epoch: 5| Step: 4
Training loss: 2.576541761288249
Validation loss: 2.5612701618195346

Epoch: 5| Step: 5
Training loss: 3.1630906107656895
Validation loss: 2.5637007190462526

Epoch: 5| Step: 6
Training loss: 3.0493621847169194
Validation loss: 2.5731649854807395

Epoch: 5| Step: 7
Training loss: 3.206649107009278
Validation loss: 2.568544167203987

Epoch: 5| Step: 8
Training loss: 2.2065250862816814
Validation loss: 2.5756365219423736

Epoch: 5| Step: 9
Training loss: 2.9778293907222126
Validation loss: 2.571969806171381

Epoch: 5| Step: 10
Training loss: 3.004439407080441
Validation loss: 2.5671812138493486

Epoch: 51| Step: 0
Training loss: 3.2935752807834815
Validation loss: 2.5672919405363515

Epoch: 5| Step: 1
Training loss: 2.7440271537371452
Validation loss: 2.580530808527559

Epoch: 5| Step: 2
Training loss: 2.950440170294312
Validation loss: 2.5687833126311164

Epoch: 5| Step: 3
Training loss: 2.7696850818691052
Validation loss: 2.563100202786908

Epoch: 5| Step: 4
Training loss: 3.135589510355846
Validation loss: 2.5634865571201733

Epoch: 5| Step: 5
Training loss: 2.87015290706975
Validation loss: 2.5724605527836024

Epoch: 5| Step: 6
Training loss: 2.971584372444463
Validation loss: 2.576322764999506

Epoch: 5| Step: 7
Training loss: 2.630551960974116
Validation loss: 2.584370184383505

Epoch: 5| Step: 8
Training loss: 3.263742296610828
Validation loss: 2.5836688474478664

Epoch: 5| Step: 9
Training loss: 2.7637489982061414
Validation loss: 2.5763635927029496

Epoch: 5| Step: 10
Training loss: 2.7629942369873826
Validation loss: 2.562969768972452

Epoch: 52| Step: 0
Training loss: 2.871031800724096
Validation loss: 2.567673901068698

Epoch: 5| Step: 1
Training loss: 2.483668292793776
Validation loss: 2.549317081522969

Epoch: 5| Step: 2
Training loss: 2.756925532107714
Validation loss: 2.572496005567148

Epoch: 5| Step: 3
Training loss: 3.2665965993983224
Validation loss: 2.580551042141493

Epoch: 5| Step: 4
Training loss: 3.079454318914751
Validation loss: 2.5700098697317544

Epoch: 5| Step: 5
Training loss: 2.771213897346026
Validation loss: 2.5874520916670067

Epoch: 5| Step: 6
Training loss: 2.4783926858915954
Validation loss: 2.582980427237343

Epoch: 5| Step: 7
Training loss: 2.774905222056069
Validation loss: 2.5540740853527906

Epoch: 5| Step: 8
Training loss: 3.7926070475951317
Validation loss: 2.5564913821488906

Epoch: 5| Step: 9
Training loss: 3.2974474080065836
Validation loss: 2.5585097983548493

Epoch: 5| Step: 10
Training loss: 2.2590259076541455
Validation loss: 2.568747880463384

Epoch: 53| Step: 0
Training loss: 2.7086172590839177
Validation loss: 2.5806439506107486

Epoch: 5| Step: 1
Training loss: 2.714676095739426
Validation loss: 2.5680512536241307

Epoch: 5| Step: 2
Training loss: 3.422198415304609
Validation loss: 2.5538768607979687

Epoch: 5| Step: 3
Training loss: 2.943186835260959
Validation loss: 2.5642094366496924

Epoch: 5| Step: 4
Training loss: 2.8245201108341127
Validation loss: 2.5755072714779517

Epoch: 5| Step: 5
Training loss: 2.9491403329289207
Validation loss: 2.57161372633214

Epoch: 5| Step: 6
Training loss: 2.8483256323814268
Validation loss: 2.576335099949798

Epoch: 5| Step: 7
Training loss: 2.5565676988381663
Validation loss: 2.5564913350174545

Epoch: 5| Step: 8
Training loss: 3.3116602283012675
Validation loss: 2.5519147631833614

Epoch: 5| Step: 9
Training loss: 3.210705231296606
Validation loss: 2.575489824179021

Epoch: 5| Step: 10
Training loss: 2.280673019392898
Validation loss: 2.568482182015061

Epoch: 54| Step: 0
Training loss: 2.906839290474197
Validation loss: 2.569194612567562

Epoch: 5| Step: 1
Training loss: 2.4737827814290454
Validation loss: 2.5607374879423803

Epoch: 5| Step: 2
Training loss: 2.85077428505889
Validation loss: 2.564058983616233

Epoch: 5| Step: 3
Training loss: 2.5664982191354984
Validation loss: 2.5829920376237157

Epoch: 5| Step: 4
Training loss: 2.766997088575193
Validation loss: 2.5692757016104664

Epoch: 5| Step: 5
Training loss: 3.0541821620352056
Validation loss: 2.572760111683316

Epoch: 5| Step: 6
Training loss: 2.9675883379468364
Validation loss: 2.560159616413268

Epoch: 5| Step: 7
Training loss: 3.5151212204152884
Validation loss: 2.575728067942264

Epoch: 5| Step: 8
Training loss: 2.818305656782978
Validation loss: 2.563545363980068

Epoch: 5| Step: 9
Training loss: 2.705440345502693
Validation loss: 2.576770612415731

Epoch: 5| Step: 10
Training loss: 3.4332946022095925
Validation loss: 2.563786215616845

Epoch: 55| Step: 0
Training loss: 2.282439875233873
Validation loss: 2.564098896792292

Epoch: 5| Step: 1
Training loss: 2.7718587330853715
Validation loss: 2.5597830113810174

Epoch: 5| Step: 2
Training loss: 3.3106955165745457
Validation loss: 2.573789500237535

Epoch: 5| Step: 3
Training loss: 2.7048639426150594
Validation loss: 2.540396936203809

Epoch: 5| Step: 4
Training loss: 3.0417721002847666
Validation loss: 2.564838246017106

Epoch: 5| Step: 5
Training loss: 3.3153081542986294
Validation loss: 2.569655885748072

Epoch: 5| Step: 6
Training loss: 3.0540144781382788
Validation loss: 2.5789151121734313

Epoch: 5| Step: 7
Training loss: 2.93885706944618
Validation loss: 2.5641847840009024

Epoch: 5| Step: 8
Training loss: 2.1451140621064937
Validation loss: 2.5725861396400416

Epoch: 5| Step: 9
Training loss: 3.3750856176748676
Validation loss: 2.554839137085479

Epoch: 5| Step: 10
Training loss: 2.8528090169356544
Validation loss: 2.558242382204396

Epoch: 56| Step: 0
Training loss: 2.8988415212852434
Validation loss: 2.569835361349478

Epoch: 5| Step: 1
Training loss: 3.221132600278458
Validation loss: 2.5658427659511367

Epoch: 5| Step: 2
Training loss: 2.6487738021638045
Validation loss: 2.569222429153649

Epoch: 5| Step: 3
Training loss: 2.72819911510911
Validation loss: 2.554058360625752

Epoch: 5| Step: 4
Training loss: 2.993360165119627
Validation loss: 2.5553435732894694

Epoch: 5| Step: 5
Training loss: 2.74031816111104
Validation loss: 2.5406587940102625

Epoch: 5| Step: 6
Training loss: 3.0760599558013446
Validation loss: 2.5483695161747058

Epoch: 5| Step: 7
Training loss: 2.7292524911523715
Validation loss: 2.562532320176419

Epoch: 5| Step: 8
Training loss: 3.6213684307058687
Validation loss: 2.5353340655259697

Epoch: 5| Step: 9
Training loss: 2.336034868437365
Validation loss: 2.571165168135667

Epoch: 5| Step: 10
Training loss: 2.946770021544859
Validation loss: 2.556505633853164

Epoch: 57| Step: 0
Training loss: 2.866719470166183
Validation loss: 2.5510387858808077

Epoch: 5| Step: 1
Training loss: 3.3984321681890646
Validation loss: 2.5552544212041766

Epoch: 5| Step: 2
Training loss: 3.2326875421887533
Validation loss: 2.5766885235290338

Epoch: 5| Step: 3
Training loss: 3.2066401848390185
Validation loss: 2.5477886643482504

Epoch: 5| Step: 4
Training loss: 2.8855353008537503
Validation loss: 2.5472118266436374

Epoch: 5| Step: 5
Training loss: 1.9029060175145067
Validation loss: 2.573064858015546

Epoch: 5| Step: 6
Training loss: 2.435586226940008
Validation loss: 2.5588694759776787

Epoch: 5| Step: 7
Training loss: 2.767902449487545
Validation loss: 2.5650297653945717

Epoch: 5| Step: 8
Training loss: 3.148004923369497
Validation loss: 2.5514439705897374

Epoch: 5| Step: 9
Training loss: 2.7568825512741335
Validation loss: 2.5538474656764114

Epoch: 5| Step: 10
Training loss: 3.0161769539706342
Validation loss: 2.547909665690282

Epoch: 58| Step: 0
Training loss: 2.901213516998675
Validation loss: 2.5715709500124273

Epoch: 5| Step: 1
Training loss: 3.042381376839313
Validation loss: 2.5568781906648126

Epoch: 5| Step: 2
Training loss: 2.6242555743464706
Validation loss: 2.5722446494832116

Epoch: 5| Step: 3
Training loss: 2.9667226193184266
Validation loss: 2.558573678407687

Epoch: 5| Step: 4
Training loss: 3.27439848056798
Validation loss: 2.5667251101891524

Epoch: 5| Step: 5
Training loss: 3.1969506278093176
Validation loss: 2.5546966348859623

Epoch: 5| Step: 6
Training loss: 2.7968286265359446
Validation loss: 2.561589938557776

Epoch: 5| Step: 7
Training loss: 3.072454971716047
Validation loss: 2.5696882525991325

Epoch: 5| Step: 8
Training loss: 3.077678357556358
Validation loss: 2.5451626134805285

Epoch: 5| Step: 9
Training loss: 2.6136575607525847
Validation loss: 2.5773997997514067

Epoch: 5| Step: 10
Training loss: 1.9372840884098368
Validation loss: 2.556986387787085

Epoch: 59| Step: 0
Training loss: 2.82040396671495
Validation loss: 2.562747174058985

Epoch: 5| Step: 1
Training loss: 3.345794052963905
Validation loss: 2.562101541749238

Epoch: 5| Step: 2
Training loss: 3.0057178684609034
Validation loss: 2.564808595696857

Epoch: 5| Step: 3
Training loss: 2.7796034493049873
Validation loss: 2.5491328958255597

Epoch: 5| Step: 4
Training loss: 2.7952640092118433
Validation loss: 2.5572759099293245

Epoch: 5| Step: 5
Training loss: 2.8297151763253447
Validation loss: 2.5431122329578506

Epoch: 5| Step: 6
Training loss: 2.72553695148115
Validation loss: 2.5690167597060736

Epoch: 5| Step: 7
Training loss: 2.8505466271591486
Validation loss: 2.542150808050739

Epoch: 5| Step: 8
Training loss: 2.6944031881558184
Validation loss: 2.549396931416156

Epoch: 5| Step: 9
Training loss: 2.5603489104561366
Validation loss: 2.5440586658378166

Epoch: 5| Step: 10
Training loss: 3.4135014053281485
Validation loss: 2.54847222392784

Epoch: 60| Step: 0
Training loss: 2.9795355579541725
Validation loss: 2.5440954062101584

Epoch: 5| Step: 1
Training loss: 2.571258927229172
Validation loss: 2.551030579528471

Epoch: 5| Step: 2
Training loss: 2.6750845565979273
Validation loss: 2.5560025633720573

Epoch: 5| Step: 3
Training loss: 3.3214253075462046
Validation loss: 2.5370877693104656

Epoch: 5| Step: 4
Training loss: 2.9529759606591717
Validation loss: 2.5282735741405027

Epoch: 5| Step: 5
Training loss: 2.6443781618240485
Validation loss: 2.547889344917753

Epoch: 5| Step: 6
Training loss: 2.805242202012276
Validation loss: 2.559369547882437

Epoch: 5| Step: 7
Training loss: 3.484213239396092
Validation loss: 2.563940455718206

Epoch: 5| Step: 8
Training loss: 2.8473874816902303
Validation loss: 2.572398750661914

Epoch: 5| Step: 9
Training loss: 3.1131297688504147
Validation loss: 2.5512052979812405

Epoch: 5| Step: 10
Training loss: 2.092543980581757
Validation loss: 2.5443808908531085

Epoch: 61| Step: 0
Training loss: 2.468470497244024
Validation loss: 2.573812537933075

Epoch: 5| Step: 1
Training loss: 3.262078070122064
Validation loss: 2.5347337680279676

Epoch: 5| Step: 2
Training loss: 3.0139808718878687
Validation loss: 2.5460613215981076

Epoch: 5| Step: 3
Training loss: 2.9302478305303183
Validation loss: 2.5609295912887546

Epoch: 5| Step: 4
Training loss: 3.2476654471055424
Validation loss: 2.5552556251422316

Epoch: 5| Step: 5
Training loss: 2.95161148927259
Validation loss: 2.564788889598952

Epoch: 5| Step: 6
Training loss: 2.5841925227956697
Validation loss: 2.55571561839384

Epoch: 5| Step: 7
Training loss: 3.0653340208371676
Validation loss: 2.5722275956620133

Epoch: 5| Step: 8
Training loss: 2.5828793239545313
Validation loss: 2.550738813786009

Epoch: 5| Step: 9
Training loss: 3.116050264190615
Validation loss: 2.5620513916508703

Epoch: 5| Step: 10
Training loss: 2.268752892022089
Validation loss: 2.5570506247172267

Epoch: 62| Step: 0
Training loss: 2.8326282371858365
Validation loss: 2.5574701618372218

Epoch: 5| Step: 1
Training loss: 3.0229400304359837
Validation loss: 2.5572870793852918

Epoch: 5| Step: 2
Training loss: 3.421175671886095
Validation loss: 2.542585367936717

Epoch: 5| Step: 3
Training loss: 2.8860082100310627
Validation loss: 2.5542571489417174

Epoch: 5| Step: 4
Training loss: 3.2696312965390484
Validation loss: 2.5414405408139626

Epoch: 5| Step: 5
Training loss: 3.1064600483971456
Validation loss: 2.5588637433022585

Epoch: 5| Step: 6
Training loss: 2.9929889770875824
Validation loss: 2.551715953134406

Epoch: 5| Step: 7
Training loss: 2.0651514754262164
Validation loss: 2.5529517294570327

Epoch: 5| Step: 8
Training loss: 2.2601666173027977
Validation loss: 2.5355624642277905

Epoch: 5| Step: 9
Training loss: 3.1045238280866383
Validation loss: 2.555841060104504

Epoch: 5| Step: 10
Training loss: 2.396198333358344
Validation loss: 2.5432506747955026

Epoch: 63| Step: 0
Training loss: 3.001594437643237
Validation loss: 2.566402264296615

Epoch: 5| Step: 1
Training loss: 3.452426062917158
Validation loss: 2.5461333001566984

Epoch: 5| Step: 2
Training loss: 2.7254052097126595
Validation loss: 2.552186389187574

Epoch: 5| Step: 3
Training loss: 3.3130507101362894
Validation loss: 2.5626105317958925

Epoch: 5| Step: 4
Training loss: 2.7851485456919214
Validation loss: 2.534135931287188

Epoch: 5| Step: 5
Training loss: 2.4632342565465657
Validation loss: 2.5316141409222475

Epoch: 5| Step: 6
Training loss: 2.6425372541358523
Validation loss: 2.5539043513499435

Epoch: 5| Step: 7
Training loss: 2.923592618479552
Validation loss: 2.5629538947692634

Epoch: 5| Step: 8
Training loss: 2.7517632120417783
Validation loss: 2.537020388529631

Epoch: 5| Step: 9
Training loss: 2.879913940468429
Validation loss: 2.5491656389108233

Epoch: 5| Step: 10
Training loss: 2.6544078946842937
Validation loss: 2.550530350192909

Epoch: 64| Step: 0
Training loss: 3.842025369928766
Validation loss: 2.5442485612916386

Epoch: 5| Step: 1
Training loss: 1.9137592308972946
Validation loss: 2.5401713368411305

Epoch: 5| Step: 2
Training loss: 2.702364687853667
Validation loss: 2.5394940449864913

Epoch: 5| Step: 3
Training loss: 2.9577352071377168
Validation loss: 2.5629106608233228

Epoch: 5| Step: 4
Training loss: 2.948731398578364
Validation loss: 2.545464005112766

Epoch: 5| Step: 5
Training loss: 2.967999975929363
Validation loss: 2.541492642463813

Epoch: 5| Step: 6
Training loss: 2.7421944555645403
Validation loss: 2.5434650599475566

Epoch: 5| Step: 7
Training loss: 3.0315469812637534
Validation loss: 2.556394503150834

Epoch: 5| Step: 8
Training loss: 2.5712811810017357
Validation loss: 2.5424079325132047

Epoch: 5| Step: 9
Training loss: 2.8894287933484457
Validation loss: 2.560536967024067

Epoch: 5| Step: 10
Training loss: 2.829150609311214
Validation loss: 2.552776974874788

Epoch: 65| Step: 0
Training loss: 2.720448807265386
Validation loss: 2.55286649767272

Epoch: 5| Step: 1
Training loss: 3.2252513817096067
Validation loss: 2.5590027415327694

Epoch: 5| Step: 2
Training loss: 2.288568749410069
Validation loss: 2.562046215937134

Epoch: 5| Step: 3
Training loss: 2.527682014952312
Validation loss: 2.5509705063361334

Epoch: 5| Step: 4
Training loss: 2.6522303066797215
Validation loss: 2.5455811258425007

Epoch: 5| Step: 5
Training loss: 2.6517681233169856
Validation loss: 2.5431481931576467

Epoch: 5| Step: 6
Training loss: 3.3914774161611416
Validation loss: 2.560235755667695

Epoch: 5| Step: 7
Training loss: 2.5590395007705347
Validation loss: 2.5521219384138756

Epoch: 5| Step: 8
Training loss: 3.0713654476788985
Validation loss: 2.5329173187573018

Epoch: 5| Step: 9
Training loss: 3.406834263520414
Validation loss: 2.5457458590089708

Epoch: 5| Step: 10
Training loss: 2.798109751666837
Validation loss: 2.5552651703443674

Epoch: 66| Step: 0
Training loss: 2.1428714297590368
Validation loss: 2.5449479371638546

Epoch: 5| Step: 1
Training loss: 2.5084046232246298
Validation loss: 2.5545763534276675

Epoch: 5| Step: 2
Training loss: 3.127199238343145
Validation loss: 2.551291836288159

Epoch: 5| Step: 3
Training loss: 2.7409552171869778
Validation loss: 2.549081344576774

Epoch: 5| Step: 4
Training loss: 2.80372956049523
Validation loss: 2.5424968201616203

Epoch: 5| Step: 5
Training loss: 3.2994564128874977
Validation loss: 2.5517704058409167

Epoch: 5| Step: 6
Training loss: 3.1127434508929035
Validation loss: 2.5419693999713524

Epoch: 5| Step: 7
Training loss: 3.020811848180375
Validation loss: 2.5503414361386616

Epoch: 5| Step: 8
Training loss: 2.4262345976146564
Validation loss: 2.5554138537845397

Epoch: 5| Step: 9
Training loss: 3.5245917009385916
Validation loss: 2.5592989768380336

Epoch: 5| Step: 10
Training loss: 2.6397017269986267
Validation loss: 2.5610844849091206

Epoch: 67| Step: 0
Training loss: 3.216370082352594
Validation loss: 2.5534539924033237

Epoch: 5| Step: 1
Training loss: 3.019043876922979
Validation loss: 2.5519730682024995

Epoch: 5| Step: 2
Training loss: 2.736970029767892
Validation loss: 2.5621992025201705

Epoch: 5| Step: 3
Training loss: 2.897132600361459
Validation loss: 2.530679869215915

Epoch: 5| Step: 4
Training loss: 2.7282550444310334
Validation loss: 2.5441369536315683

Epoch: 5| Step: 5
Training loss: 2.4360648600896235
Validation loss: 2.5529504430933136

Epoch: 5| Step: 6
Training loss: 2.69498439395045
Validation loss: 2.543782613391562

Epoch: 5| Step: 7
Training loss: 3.0387784928147616
Validation loss: 2.547394604439345

Epoch: 5| Step: 8
Training loss: 3.386487836346479
Validation loss: 2.531188362492292

Epoch: 5| Step: 9
Training loss: 2.4893366373541137
Validation loss: 2.5523119340430407

Epoch: 5| Step: 10
Training loss: 2.778177771909192
Validation loss: 2.531533815187897

Epoch: 68| Step: 0
Training loss: 2.197583310367518
Validation loss: 2.553740060134704

Epoch: 5| Step: 1
Training loss: 3.5737126893707383
Validation loss: 2.54653379992242

Epoch: 5| Step: 2
Training loss: 3.441200484014147
Validation loss: 2.532868072099918

Epoch: 5| Step: 3
Training loss: 2.6782783384281146
Validation loss: 2.541162001645403

Epoch: 5| Step: 4
Training loss: 2.736561625699301
Validation loss: 2.5381643913374514

Epoch: 5| Step: 5
Training loss: 2.7410393291881996
Validation loss: 2.5241069448601823

Epoch: 5| Step: 6
Training loss: 3.0977390658262824
Validation loss: 2.527899803488136

Epoch: 5| Step: 7
Training loss: 3.169236763128719
Validation loss: 2.544960718325398

Epoch: 5| Step: 8
Training loss: 2.821656643856639
Validation loss: 2.5370826704944913

Epoch: 5| Step: 9
Training loss: 2.2796639977878446
Validation loss: 2.5270398350558225

Epoch: 5| Step: 10
Training loss: 2.2460786769195327
Validation loss: 2.5298200503242283

Epoch: 69| Step: 0
Training loss: 3.060962836183587
Validation loss: 2.5385072818900105

Epoch: 5| Step: 1
Training loss: 2.807265772844596
Validation loss: 2.538680424494151

Epoch: 5| Step: 2
Training loss: 2.830149394968286
Validation loss: 2.539852009616232

Epoch: 5| Step: 3
Training loss: 2.5416953184253464
Validation loss: 2.5329479162629425

Epoch: 5| Step: 4
Training loss: 2.7060441148070256
Validation loss: 2.541461775610801

Epoch: 5| Step: 5
Training loss: 2.76197522387486
Validation loss: 2.5322756394037085

Epoch: 5| Step: 6
Training loss: 2.895435351643914
Validation loss: 2.539241291878904

Epoch: 5| Step: 7
Training loss: 3.212828884285197
Validation loss: 2.5350751759109635

Epoch: 5| Step: 8
Training loss: 2.52489578107528
Validation loss: 2.532105256179237

Epoch: 5| Step: 9
Training loss: 3.435988492704402
Validation loss: 2.5384835299210424

Epoch: 5| Step: 10
Training loss: 2.6447422050507603
Validation loss: 2.553745880606293

Epoch: 70| Step: 0
Training loss: 2.7256832071276507
Validation loss: 2.536037651659547

Epoch: 5| Step: 1
Training loss: 2.7260834456145346
Validation loss: 2.53961035535999

Epoch: 5| Step: 2
Training loss: 2.4887899839905376
Validation loss: 2.5495155798868616

Epoch: 5| Step: 3
Training loss: 3.2105142356177914
Validation loss: 2.554387398374431

Epoch: 5| Step: 4
Training loss: 2.0966725236035626
Validation loss: 2.5346106700555824

Epoch: 5| Step: 5
Training loss: 3.1310937595029538
Validation loss: 2.5256813938373393

Epoch: 5| Step: 6
Training loss: 2.5303362857340894
Validation loss: 2.5502632473323197

Epoch: 5| Step: 7
Training loss: 3.6503740354174266
Validation loss: 2.519742042173484

Epoch: 5| Step: 8
Training loss: 2.7268245538298546
Validation loss: 2.547994698054284

Epoch: 5| Step: 9
Training loss: 3.0767292401806845
Validation loss: 2.5431716399723645

Epoch: 5| Step: 10
Training loss: 2.830625324514568
Validation loss: 2.529881124309481

Epoch: 71| Step: 0
Training loss: 2.241697782941029
Validation loss: 2.5356030403146455

Epoch: 5| Step: 1
Training loss: 2.917457872885824
Validation loss: 2.5351370374591577

Epoch: 5| Step: 2
Training loss: 2.657216603051209
Validation loss: 2.543659264964246

Epoch: 5| Step: 3
Training loss: 2.6273303813974205
Validation loss: 2.5388803103374515

Epoch: 5| Step: 4
Training loss: 3.0894940319432704
Validation loss: 2.524334611580802

Epoch: 5| Step: 5
Training loss: 2.633038519601095
Validation loss: 2.5215134062222537

Epoch: 5| Step: 6
Training loss: 2.8989546897902754
Validation loss: 2.523030403571583

Epoch: 5| Step: 7
Training loss: 3.697556441406965
Validation loss: 2.5338407197272184

Epoch: 5| Step: 8
Training loss: 2.4331135528632255
Validation loss: 2.546488005936188

Epoch: 5| Step: 9
Training loss: 2.8562321301324882
Validation loss: 2.5244652583425404

Epoch: 5| Step: 10
Training loss: 3.1726966132043697
Validation loss: 2.5353683296258143

Epoch: 72| Step: 0
Training loss: 2.582579861760446
Validation loss: 2.544741309216169

Epoch: 5| Step: 1
Training loss: 2.8487861390644067
Validation loss: 2.5320974714068227

Epoch: 5| Step: 2
Training loss: 2.4358487527180674
Validation loss: 2.5451034977085945

Epoch: 5| Step: 3
Training loss: 2.9988518743157213
Validation loss: 2.5247282445858037

Epoch: 5| Step: 4
Training loss: 3.2178250066998166
Validation loss: 2.531260599903886

Epoch: 5| Step: 5
Training loss: 3.0301964885593384
Validation loss: 2.540195711366129

Epoch: 5| Step: 6
Training loss: 2.5237963158569485
Validation loss: 2.5276536266091925

Epoch: 5| Step: 7
Training loss: 3.2013460010700023
Validation loss: 2.5423806821743593

Epoch: 5| Step: 8
Training loss: 3.0298690009676825
Validation loss: 2.5422730397492486

Epoch: 5| Step: 9
Training loss: 2.0511910165399585
Validation loss: 2.5304091219169367

Epoch: 5| Step: 10
Training loss: 3.4218479660568817
Validation loss: 2.524835401839828

Epoch: 73| Step: 0
Training loss: 3.0207161891629593
Validation loss: 2.5555738512272708

Epoch: 5| Step: 1
Training loss: 2.6428245096879905
Validation loss: 2.53967566319096

Epoch: 5| Step: 2
Training loss: 3.0608125046761256
Validation loss: 2.54899563287463

Epoch: 5| Step: 3
Training loss: 2.6659224385614246
Validation loss: 2.5615754649392413

Epoch: 5| Step: 4
Training loss: 3.049683982861947
Validation loss: 2.5382087618869438

Epoch: 5| Step: 5
Training loss: 2.653145894933225
Validation loss: 2.5410149217938676

Epoch: 5| Step: 6
Training loss: 2.5142904023027683
Validation loss: 2.5299038839202472

Epoch: 5| Step: 7
Training loss: 2.5656312559100325
Validation loss: 2.5349471921526083

Epoch: 5| Step: 8
Training loss: 2.8793530887327323
Validation loss: 2.544969790412934

Epoch: 5| Step: 9
Training loss: 3.241722275780801
Validation loss: 2.539604585772279

Epoch: 5| Step: 10
Training loss: 2.909036592496625
Validation loss: 2.5476992247488064

Epoch: 74| Step: 0
Training loss: 2.9696957336652043
Validation loss: 2.5269964147818964

Epoch: 5| Step: 1
Training loss: 3.4720635610994295
Validation loss: 2.5698570328912336

Epoch: 5| Step: 2
Training loss: 2.503497823889114
Validation loss: 2.531156136349737

Epoch: 5| Step: 3
Training loss: 2.737692863603144
Validation loss: 2.548893361601833

Epoch: 5| Step: 4
Training loss: 3.0372101696497764
Validation loss: 2.555212000084688

Epoch: 5| Step: 5
Training loss: 2.045144434260946
Validation loss: 2.5348286501352346

Epoch: 5| Step: 6
Training loss: 3.2144577888568975
Validation loss: 2.5360619289405917

Epoch: 5| Step: 7
Training loss: 2.7521162560048515
Validation loss: 2.5416018435561645

Epoch: 5| Step: 8
Training loss: 2.6372718965505717
Validation loss: 2.540865477499734

Epoch: 5| Step: 9
Training loss: 2.348924544966129
Validation loss: 2.518230061347927

Epoch: 5| Step: 10
Training loss: 3.371673781619545
Validation loss: 2.522895891254035

Epoch: 75| Step: 0
Training loss: 2.5110966459305897
Validation loss: 2.5367640519751293

Epoch: 5| Step: 1
Training loss: 3.5757955772974657
Validation loss: 2.551464513240251

Epoch: 5| Step: 2
Training loss: 2.4753050876193896
Validation loss: 2.536874104375495

Epoch: 5| Step: 3
Training loss: 2.397176603715937
Validation loss: 2.534872900064985

Epoch: 5| Step: 4
Training loss: 3.0079364701522007
Validation loss: 2.5301044180468373

Epoch: 5| Step: 5
Training loss: 2.75629285921782
Validation loss: 2.5552705047664808

Epoch: 5| Step: 6
Training loss: 3.0269168675843416
Validation loss: 2.5594850447012707

Epoch: 5| Step: 7
Training loss: 3.3114377603834795
Validation loss: 2.5459863443702715

Epoch: 5| Step: 8
Training loss: 2.6360319020044143
Validation loss: 2.5439554977283954

Epoch: 5| Step: 9
Training loss: 2.640025956864341
Validation loss: 2.5447415852511015

Epoch: 5| Step: 10
Training loss: 2.7651011515045445
Validation loss: 2.5347825081440396

Epoch: 76| Step: 0
Training loss: 2.5431992855552203
Validation loss: 2.5301795193835424

Epoch: 5| Step: 1
Training loss: 2.5392685087700646
Validation loss: 2.5508867329390856

Epoch: 5| Step: 2
Training loss: 3.0235926861820515
Validation loss: 2.5413652194548555

Epoch: 5| Step: 3
Training loss: 3.3507403452380573
Validation loss: 2.553180436833453

Epoch: 5| Step: 4
Training loss: 2.490970709151864
Validation loss: 2.529053669364438

Epoch: 5| Step: 5
Training loss: 3.025864999224234
Validation loss: 2.516715596318353

Epoch: 5| Step: 6
Training loss: 3.055713686078364
Validation loss: 2.5246531750385497

Epoch: 5| Step: 7
Training loss: 3.0261023160248204
Validation loss: 2.537194383346735

Epoch: 5| Step: 8
Training loss: 3.1052516261557876
Validation loss: 2.5459267371976324

Epoch: 5| Step: 9
Training loss: 2.5820933001243147
Validation loss: 2.543269566985254

Epoch: 5| Step: 10
Training loss: 2.3854803440529104
Validation loss: 2.547825216101154

Epoch: 77| Step: 0
Training loss: 2.853687069734131
Validation loss: 2.5513543687380364

Epoch: 5| Step: 1
Training loss: 2.0454695440956225
Validation loss: 2.5483474808195887

Epoch: 5| Step: 2
Training loss: 3.296110525583569
Validation loss: 2.539564338104336

Epoch: 5| Step: 3
Training loss: 2.834247011703823
Validation loss: 2.5278079163020752

Epoch: 5| Step: 4
Training loss: 2.611252617439795
Validation loss: 2.5326594413189745

Epoch: 5| Step: 5
Training loss: 2.7366285356597997
Validation loss: 2.533187263632597

Epoch: 5| Step: 6
Training loss: 3.091042615178985
Validation loss: 2.5313699679428443

Epoch: 5| Step: 7
Training loss: 3.2093800575342786
Validation loss: 2.5263662853837348

Epoch: 5| Step: 8
Training loss: 3.0390013073249658
Validation loss: 2.538619770232404

Epoch: 5| Step: 9
Training loss: 2.688768220677481
Validation loss: 2.5337792019021528

Epoch: 5| Step: 10
Training loss: 2.6088418929946755
Validation loss: 2.5281472776892584

Epoch: 78| Step: 0
Training loss: 2.7782922512038515
Validation loss: 2.52308645587461

Epoch: 5| Step: 1
Training loss: 2.7994575724397284
Validation loss: 2.538344675577572

Epoch: 5| Step: 2
Training loss: 2.830991188329352
Validation loss: 2.511130472001447

Epoch: 5| Step: 3
Training loss: 2.968598532577221
Validation loss: 2.5245926315879736

Epoch: 5| Step: 4
Training loss: 2.9341780363854104
Validation loss: 2.5083019438460368

Epoch: 5| Step: 5
Training loss: 2.842037649540169
Validation loss: 2.5107205570843285

Epoch: 5| Step: 6
Training loss: 2.622755544337906
Validation loss: 2.52232686539721

Epoch: 5| Step: 7
Training loss: 3.0618761945396282
Validation loss: 2.5160959361300046

Epoch: 5| Step: 8
Training loss: 3.0234375
Validation loss: 2.5471561321264784

Epoch: 5| Step: 9
Training loss: 2.8117216092726065
Validation loss: 2.524670953346737

Epoch: 5| Step: 10
Training loss: 2.35042100746941
Validation loss: 2.538698032873017

Epoch: 79| Step: 0
Training loss: 3.3391855048522583
Validation loss: 2.516932413790411

Epoch: 5| Step: 1
Training loss: 2.6896308613031756
Validation loss: 2.5409345945982436

Epoch: 5| Step: 2
Training loss: 3.150810464410959
Validation loss: 2.525148094932701

Epoch: 5| Step: 3
Training loss: 3.1510435162163133
Validation loss: 2.5304575502174456

Epoch: 5| Step: 4
Training loss: 2.9943250386579257
Validation loss: 2.5268740711039652

Epoch: 5| Step: 5
Training loss: 2.241510056292074
Validation loss: 2.5361726728914813

Epoch: 5| Step: 6
Training loss: 2.4862342451161226
Validation loss: 2.5344045719728636

Epoch: 5| Step: 7
Training loss: 2.6967170041372217
Validation loss: 2.5168444496190587

Epoch: 5| Step: 8
Training loss: 2.439412173926944
Validation loss: 2.522745564658915

Epoch: 5| Step: 9
Training loss: 3.1980219688018803
Validation loss: 2.5341976498576124

Epoch: 5| Step: 10
Training loss: 2.324956594082495
Validation loss: 2.5120586097138586

Epoch: 80| Step: 0
Training loss: 2.8423055816162885
Validation loss: 2.5290992914620594

Epoch: 5| Step: 1
Training loss: 3.3357305172174945
Validation loss: 2.52124763209116

Epoch: 5| Step: 2
Training loss: 2.224888972394722
Validation loss: 2.5088364397442247

Epoch: 5| Step: 3
Training loss: 2.951640245293162
Validation loss: 2.530076862937093

Epoch: 5| Step: 4
Training loss: 2.6658829987901806
Validation loss: 2.54575399075747

Epoch: 5| Step: 5
Training loss: 2.976264197753469
Validation loss: 2.542121258177356

Epoch: 5| Step: 6
Training loss: 3.3354940722742645
Validation loss: 2.5167082875273654

Epoch: 5| Step: 7
Training loss: 3.049949933250858
Validation loss: 2.513829000774889

Epoch: 5| Step: 8
Training loss: 2.494069790711792
Validation loss: 2.5072817980277535

Epoch: 5| Step: 9
Training loss: 2.3935460215380147
Validation loss: 2.49831197211506

Epoch: 5| Step: 10
Training loss: 2.5975138919227536
Validation loss: 2.5167134795724286

Epoch: 81| Step: 0
Training loss: 3.2941369714290336
Validation loss: 2.515946023501117

Epoch: 5| Step: 1
Training loss: 3.2029537992548587
Validation loss: 2.512309117852326

Epoch: 5| Step: 2
Training loss: 2.604095600430061
Validation loss: 2.5192696374081023

Epoch: 5| Step: 3
Training loss: 2.490261756896027
Validation loss: 2.5199235904944195

Epoch: 5| Step: 4
Training loss: 2.3606222531739753
Validation loss: 2.52164578513141

Epoch: 5| Step: 5
Training loss: 2.8461263599773656
Validation loss: 2.538282619506716

Epoch: 5| Step: 6
Training loss: 2.871851316402106
Validation loss: 2.5324041727292963

Epoch: 5| Step: 7
Training loss: 2.7485172435822633
Validation loss: 2.5228141284686165

Epoch: 5| Step: 8
Training loss: 3.2607708495857697
Validation loss: 2.525755505114881

Epoch: 5| Step: 9
Training loss: 3.0166783840169664
Validation loss: 2.540088291240514

Epoch: 5| Step: 10
Training loss: 2.0306176008123145
Validation loss: 2.522605361308074

Epoch: 82| Step: 0
Training loss: 3.092621876960153
Validation loss: 2.514204022022012

Epoch: 5| Step: 1
Training loss: 2.438232018518572
Validation loss: 2.523016859994107

Epoch: 5| Step: 2
Training loss: 3.1975606084333243
Validation loss: 2.5356425440480947

Epoch: 5| Step: 3
Training loss: 2.5916189608420064
Validation loss: 2.516421940403425

Epoch: 5| Step: 4
Training loss: 2.756279711227999
Validation loss: 2.5185267825932405

Epoch: 5| Step: 5
Training loss: 2.7271066904099857
Validation loss: 2.5325995204848364

Epoch: 5| Step: 6
Training loss: 2.4235831850830065
Validation loss: 2.524421340856677

Epoch: 5| Step: 7
Training loss: 2.7410664671528586
Validation loss: 2.51358502368128

Epoch: 5| Step: 8
Training loss: 2.9440666012279375
Validation loss: 2.5092078588459295

Epoch: 5| Step: 9
Training loss: 3.10133541630713
Validation loss: 2.5379218001029584

Epoch: 5| Step: 10
Training loss: 3.0043779853502826
Validation loss: 2.535488477827083

Epoch: 83| Step: 0
Training loss: 2.8341963706459743
Validation loss: 2.510951620407409

Epoch: 5| Step: 1
Training loss: 3.1117267245555142
Validation loss: 2.518185829982244

Epoch: 5| Step: 2
Training loss: 2.6516436859452166
Validation loss: 2.5411115336326517

Epoch: 5| Step: 3
Training loss: 2.687899094157881
Validation loss: 2.527671474080271

Epoch: 5| Step: 4
Training loss: 2.481286486302159
Validation loss: 2.5088677386450264

Epoch: 5| Step: 5
Training loss: 2.8959298609464774
Validation loss: 2.547552408602326

Epoch: 5| Step: 6
Training loss: 2.752646992956865
Validation loss: 2.5294314796953805

Epoch: 5| Step: 7
Training loss: 2.588496667687001
Validation loss: 2.535234611469778

Epoch: 5| Step: 8
Training loss: 2.7002776038545404
Validation loss: 2.5078042217111998

Epoch: 5| Step: 9
Training loss: 3.5137564207362377
Validation loss: 2.5347589518699616

Epoch: 5| Step: 10
Training loss: 2.827452521697079
Validation loss: 2.5388212790175784

Epoch: 84| Step: 0
Training loss: 2.5210794586164305
Validation loss: 2.5086753653174925

Epoch: 5| Step: 1
Training loss: 3.085652553102309
Validation loss: 2.535204974915209

Epoch: 5| Step: 2
Training loss: 3.1500265877222264
Validation loss: 2.5270746417506875

Epoch: 5| Step: 3
Training loss: 2.9739137413480914
Validation loss: 2.5285322385318634

Epoch: 5| Step: 4
Training loss: 2.0382625720804817
Validation loss: 2.5146104186629055

Epoch: 5| Step: 5
Training loss: 2.968612828348249
Validation loss: 2.5352021000255918

Epoch: 5| Step: 6
Training loss: 2.5529141585325936
Validation loss: 2.5362487074311155

Epoch: 5| Step: 7
Training loss: 2.1397634289900966
Validation loss: 2.536151097633986

Epoch: 5| Step: 8
Training loss: 2.5683784049021714
Validation loss: 2.5301855703548077

Epoch: 5| Step: 9
Training loss: 3.9517653467885845
Validation loss: 2.5076933525700693

Epoch: 5| Step: 10
Training loss: 2.529632899523913
Validation loss: 2.534728163830539

Epoch: 85| Step: 0
Training loss: 2.485851209139089
Validation loss: 2.534457296709874

Epoch: 5| Step: 1
Training loss: 3.0676773838280322
Validation loss: 2.522027405529515

Epoch: 5| Step: 2
Training loss: 2.8374678420878734
Validation loss: 2.5520599330677296

Epoch: 5| Step: 3
Training loss: 2.4545731856003026
Validation loss: 2.5228556676521667

Epoch: 5| Step: 4
Training loss: 2.7783899067239286
Validation loss: 2.522422474379278

Epoch: 5| Step: 5
Training loss: 2.6860968009394846
Validation loss: 2.532532182254613

Epoch: 5| Step: 6
Training loss: 3.02396658277465
Validation loss: 2.53126652270492

Epoch: 5| Step: 7
Training loss: 2.8237653819522635
Validation loss: 2.53028930649461

Epoch: 5| Step: 8
Training loss: 2.641202603883917
Validation loss: 2.536155966841147

Epoch: 5| Step: 9
Training loss: 2.754676051124154
Validation loss: 2.525549616249833

Epoch: 5| Step: 10
Training loss: 3.531933473593415
Validation loss: 2.5292354488701796

Epoch: 86| Step: 0
Training loss: 2.7677499829480863
Validation loss: 2.5369423237344226

Epoch: 5| Step: 1
Training loss: 2.1860412093149666
Validation loss: 2.5373625223374225

Epoch: 5| Step: 2
Training loss: 2.5718431176472545
Validation loss: 2.5220519930177807

Epoch: 5| Step: 3
Training loss: 2.5427101578029374
Validation loss: 2.5046437828740995

Epoch: 5| Step: 4
Training loss: 3.0084451381657313
Validation loss: 2.538485283124699

Epoch: 5| Step: 5
Training loss: 2.4590667880475743
Validation loss: 2.518149692998287

Epoch: 5| Step: 6
Training loss: 2.698160717701027
Validation loss: 2.4997160165756824

Epoch: 5| Step: 7
Training loss: 2.815620048086742
Validation loss: 2.518368129779617

Epoch: 5| Step: 8
Training loss: 3.3879312107079107
Validation loss: 2.523427702333806

Epoch: 5| Step: 9
Training loss: 3.475908607967123
Validation loss: 2.5219635598866663

Epoch: 5| Step: 10
Training loss: 2.6011308191613725
Validation loss: 2.5297911468550267

Epoch: 87| Step: 0
Training loss: 2.9958336827928305
Validation loss: 2.5327616059265003

Epoch: 5| Step: 1
Training loss: 3.2243740982554256
Validation loss: 2.517883133124752

Epoch: 5| Step: 2
Training loss: 2.810047351804728
Validation loss: 2.518625740095194

Epoch: 5| Step: 3
Training loss: 2.629815906420097
Validation loss: 2.5381603138137128

Epoch: 5| Step: 4
Training loss: 2.6928209371037277
Validation loss: 2.5154246843679293

Epoch: 5| Step: 5
Training loss: 2.7261826215041087
Validation loss: 2.5184507168930668

Epoch: 5| Step: 6
Training loss: 3.581681661904318
Validation loss: 2.5120289303755245

Epoch: 5| Step: 7
Training loss: 2.398491973755373
Validation loss: 2.526715583244419

Epoch: 5| Step: 8
Training loss: 2.483833493891734
Validation loss: 2.5424728866565522

Epoch: 5| Step: 9
Training loss: 2.774355970000751
Validation loss: 2.53369279193765

Epoch: 5| Step: 10
Training loss: 2.4022670330420746
Validation loss: 2.524026955096737

Epoch: 88| Step: 0
Training loss: 2.4808094661727638
Validation loss: 2.55131359801987

Epoch: 5| Step: 1
Training loss: 2.688478956903901
Validation loss: 2.537748307730432

Epoch: 5| Step: 2
Training loss: 2.796616014818334
Validation loss: 2.5310110194315842

Epoch: 5| Step: 3
Training loss: 2.9354765395661504
Validation loss: 2.5081917140584156

Epoch: 5| Step: 4
Training loss: 3.155617981130554
Validation loss: 2.5293909769072496

Epoch: 5| Step: 5
Training loss: 2.586073212073302
Validation loss: 2.5338920569603434

Epoch: 5| Step: 6
Training loss: 2.3886407333926667
Validation loss: 2.529647739337122

Epoch: 5| Step: 7
Training loss: 3.3870641054901376
Validation loss: 2.525299171870539

Epoch: 5| Step: 8
Training loss: 2.159419287573837
Validation loss: 2.5130642332426696

Epoch: 5| Step: 9
Training loss: 3.1094458490475727
Validation loss: 2.524721099117883

Epoch: 5| Step: 10
Training loss: 2.955863046624933
Validation loss: 2.5197119792130223

Epoch: 89| Step: 0
Training loss: 3.146246438765149
Validation loss: 2.517501803040247

Epoch: 5| Step: 1
Training loss: 2.3841381950327283
Validation loss: 2.510894887787119

Epoch: 5| Step: 2
Training loss: 3.047966394824906
Validation loss: 2.5309881553187834

Epoch: 5| Step: 3
Training loss: 3.3496580604151376
Validation loss: 2.542522319341172

Epoch: 5| Step: 4
Training loss: 3.011924410080468
Validation loss: 2.5309964180458544

Epoch: 5| Step: 5
Training loss: 2.952649597977637
Validation loss: 2.5463336052330807

Epoch: 5| Step: 6
Training loss: 3.095738552143501
Validation loss: 2.5139699454610316

Epoch: 5| Step: 7
Training loss: 2.4440070131766283
Validation loss: 2.519706087245845

Epoch: 5| Step: 8
Training loss: 2.3043018082990927
Validation loss: 2.5388237176252946

Epoch: 5| Step: 9
Training loss: 2.3021033330770693
Validation loss: 2.5452120241429363

Epoch: 5| Step: 10
Training loss: 2.5675533988176427
Validation loss: 2.509064186787586

Epoch: 90| Step: 0
Training loss: 3.1617892163914343
Validation loss: 2.526177625398206

Epoch: 5| Step: 1
Training loss: 2.575674753761489
Validation loss: 2.522324519592858

Epoch: 5| Step: 2
Training loss: 2.5294967042200622
Validation loss: 2.533174425092003

Epoch: 5| Step: 3
Training loss: 2.6986934962327394
Validation loss: 2.5216586345853775

Epoch: 5| Step: 4
Training loss: 3.1490265931708277
Validation loss: 2.50403107649424

Epoch: 5| Step: 5
Training loss: 2.7056599456088293
Validation loss: 2.5205352235395937

Epoch: 5| Step: 6
Training loss: 2.9699632273751218
Validation loss: 2.5097221591959133

Epoch: 5| Step: 7
Training loss: 2.7660975510502643
Validation loss: 2.5140001137968278

Epoch: 5| Step: 8
Training loss: 2.5877187074382104
Validation loss: 2.5057176978592346

Epoch: 5| Step: 9
Training loss: 2.6962116013411377
Validation loss: 2.5127939354637765

Epoch: 5| Step: 10
Training loss: 2.9136694267192085
Validation loss: 2.524235661558422

Epoch: 91| Step: 0
Training loss: 2.7422687051166252
Validation loss: 2.519439469198639

Epoch: 5| Step: 1
Training loss: 2.7414705474957897
Validation loss: 2.5310739334189694

Epoch: 5| Step: 2
Training loss: 3.0813411428346247
Validation loss: 2.497092790079094

Epoch: 5| Step: 3
Training loss: 3.095625183857158
Validation loss: 2.5082400275750505

Epoch: 5| Step: 4
Training loss: 2.55838305355636
Validation loss: 2.521742862512412

Epoch: 5| Step: 5
Training loss: 3.2003732165609624
Validation loss: 2.5125869671438847

Epoch: 5| Step: 6
Training loss: 2.566379216021285
Validation loss: 2.5322067911516677

Epoch: 5| Step: 7
Training loss: 3.3464930326194624
Validation loss: 2.5397262061696657

Epoch: 5| Step: 8
Training loss: 2.898030460700415
Validation loss: 2.525688533543395

Epoch: 5| Step: 9
Training loss: 2.406210762793931
Validation loss: 2.5352141830342614

Epoch: 5| Step: 10
Training loss: 2.0078565540750026
Validation loss: 2.5122015701802707

Epoch: 92| Step: 0
Training loss: 3.2408737278450737
Validation loss: 2.5188677674217463

Epoch: 5| Step: 1
Training loss: 2.225695632032321
Validation loss: 2.5307511263800406

Epoch: 5| Step: 2
Training loss: 2.679812236585252
Validation loss: 2.5139003715176598

Epoch: 5| Step: 3
Training loss: 3.1091724118892197
Validation loss: 2.5320672777244213

Epoch: 5| Step: 4
Training loss: 2.7884517396304433
Validation loss: 2.5396227792855255

Epoch: 5| Step: 5
Training loss: 2.98182337463375
Validation loss: 2.513868677385914

Epoch: 5| Step: 6
Training loss: 3.119661272197203
Validation loss: 2.5077396658639697

Epoch: 5| Step: 7
Training loss: 2.7819105928261374
Validation loss: 2.5310532855395644

Epoch: 5| Step: 8
Training loss: 2.9956864499165095
Validation loss: 2.529310410289504

Epoch: 5| Step: 9
Training loss: 2.56772081682886
Validation loss: 2.539583425289586

Epoch: 5| Step: 10
Training loss: 2.109016105355221
Validation loss: 2.530367420011577

Epoch: 93| Step: 0
Training loss: 2.4839045241062485
Validation loss: 2.5241839613685415

Epoch: 5| Step: 1
Training loss: 2.7716832590285105
Validation loss: 2.5321231016343186

Epoch: 5| Step: 2
Training loss: 2.201969249396291
Validation loss: 2.523852662989615

Epoch: 5| Step: 3
Training loss: 2.5933334751505024
Validation loss: 2.528246746394221

Epoch: 5| Step: 4
Training loss: 3.4175881484522685
Validation loss: 2.529049385059816

Epoch: 5| Step: 5
Training loss: 2.50574814868483
Validation loss: 2.5225713994681547

Epoch: 5| Step: 6
Training loss: 2.8502987086043254
Validation loss: 2.52969551437067

Epoch: 5| Step: 7
Training loss: 3.1059998256096235
Validation loss: 2.5442254151030914

Epoch: 5| Step: 8
Training loss: 2.5948482164744293
Validation loss: 2.531919210882719

Epoch: 5| Step: 9
Training loss: 2.8286137448347843
Validation loss: 2.5309025860148715

Epoch: 5| Step: 10
Training loss: 3.1764482874492628
Validation loss: 2.529251598048401

Epoch: 94| Step: 0
Training loss: 2.478202589631515
Validation loss: 2.5280637985697307

Epoch: 5| Step: 1
Training loss: 2.953598065304768
Validation loss: 2.5207611523834084

Epoch: 5| Step: 2
Training loss: 2.1798221450838593
Validation loss: 2.5219322212228312

Epoch: 5| Step: 3
Training loss: 2.805565061857361
Validation loss: 2.5273780767802734

Epoch: 5| Step: 4
Training loss: 2.3488672976111507
Validation loss: 2.53551845079461

Epoch: 5| Step: 5
Training loss: 2.1929377951508062
Validation loss: 2.534545495204935

Epoch: 5| Step: 6
Training loss: 3.412368458878013
Validation loss: 2.530166392008989

Epoch: 5| Step: 7
Training loss: 3.0335374833320854
Validation loss: 2.534869110538243

Epoch: 5| Step: 8
Training loss: 3.367487989147944
Validation loss: 2.5318878648280188

Epoch: 5| Step: 9
Training loss: 2.7234027556119758
Validation loss: 2.509659197206524

Epoch: 5| Step: 10
Training loss: 2.8640685202644036
Validation loss: 2.5286953957357805

Epoch: 95| Step: 0
Training loss: 2.854923639829586
Validation loss: 2.5245637495088893

Epoch: 5| Step: 1
Training loss: 3.1026569424509476
Validation loss: 2.534882641357794

Epoch: 5| Step: 2
Training loss: 2.7414299334168626
Validation loss: 2.5145719914455356

Epoch: 5| Step: 3
Training loss: 2.9766230539485266
Validation loss: 2.515788508208155

Epoch: 5| Step: 4
Training loss: 2.3791294337720217
Validation loss: 2.542790682974529

Epoch: 5| Step: 5
Training loss: 2.414454653593949
Validation loss: 2.5333447161690246

Epoch: 5| Step: 6
Training loss: 3.028369754312615
Validation loss: 2.5043635946132055

Epoch: 5| Step: 7
Training loss: 3.031978087142228
Validation loss: 2.5296939800543554

Epoch: 5| Step: 8
Training loss: 2.6973943216943614
Validation loss: 2.5514253840960692

Epoch: 5| Step: 9
Training loss: 2.749271382995796
Validation loss: 2.5397301903320635

Epoch: 5| Step: 10
Training loss: 2.678042070669427
Validation loss: 2.523096412356665

Epoch: 96| Step: 0
Training loss: 2.6679835842719917
Validation loss: 2.5320001673258785

Epoch: 5| Step: 1
Training loss: 2.7434213150334354
Validation loss: 2.5158623728792686

Epoch: 5| Step: 2
Training loss: 2.8940393027251328
Validation loss: 2.530246220106115

Epoch: 5| Step: 3
Training loss: 2.7889063174330926
Validation loss: 2.519932850892669

Epoch: 5| Step: 4
Training loss: 2.8614387777239108
Validation loss: 2.5082420911687486

Epoch: 5| Step: 5
Training loss: 2.830760423381845
Validation loss: 2.5164172785340355

Epoch: 5| Step: 6
Training loss: 2.1088092186464364
Validation loss: 2.5218536304075188

Epoch: 5| Step: 7
Training loss: 3.047573537713805
Validation loss: 2.5285382949660122

Epoch: 5| Step: 8
Training loss: 2.5031466708156027
Validation loss: 2.5124764049672867

Epoch: 5| Step: 9
Training loss: 3.3900961749182446
Validation loss: 2.5145642920661637

Epoch: 5| Step: 10
Training loss: 2.5283832092830822
Validation loss: 2.518898937605957

Epoch: 97| Step: 0
Training loss: 3.2668835710668875
Validation loss: 2.5178969843122117

Epoch: 5| Step: 1
Training loss: 2.754053336469875
Validation loss: 2.5207480308793997

Epoch: 5| Step: 2
Training loss: 3.195807756688816
Validation loss: 2.511531589573555

Epoch: 5| Step: 3
Training loss: 2.718845409604622
Validation loss: 2.5142427556593065

Epoch: 5| Step: 4
Training loss: 2.285746493282922
Validation loss: 2.51153508665348

Epoch: 5| Step: 5
Training loss: 2.835906393758037
Validation loss: 2.51471541254504

Epoch: 5| Step: 6
Training loss: 2.619953026678057
Validation loss: 2.516803530360493

Epoch: 5| Step: 7
Training loss: 2.6342895168132783
Validation loss: 2.518173081920162

Epoch: 5| Step: 8
Training loss: 2.2651664532936007
Validation loss: 2.5362461713322104

Epoch: 5| Step: 9
Training loss: 2.965627534019103
Validation loss: 2.528199263906334

Epoch: 5| Step: 10
Training loss: 3.067017007248885
Validation loss: 2.5310486450484277

Epoch: 98| Step: 0
Training loss: 2.6559478587926924
Validation loss: 2.5165715445383685

Epoch: 5| Step: 1
Training loss: 2.8931220738974455
Validation loss: 2.499452564588969

Epoch: 5| Step: 2
Training loss: 3.0303785506866014
Validation loss: 2.5489015688003853

Epoch: 5| Step: 3
Training loss: 2.7645280462130133
Validation loss: 2.5090524401914993

Epoch: 5| Step: 4
Training loss: 2.561657836971648
Validation loss: 2.5231801574251476

Epoch: 5| Step: 5
Training loss: 2.4550107280551186
Validation loss: 2.5206042654909213

Epoch: 5| Step: 6
Training loss: 2.521956915866296
Validation loss: 2.5364446283156212

Epoch: 5| Step: 7
Training loss: 3.16026156972598
Validation loss: 2.5246497225310107

Epoch: 5| Step: 8
Training loss: 3.225102794103118
Validation loss: 2.520277759576097

Epoch: 5| Step: 9
Training loss: 2.84082048035192
Validation loss: 2.548142338353949

Epoch: 5| Step: 10
Training loss: 2.4384255241580037
Validation loss: 2.532261050878117

Epoch: 99| Step: 0
Training loss: 2.723357582342295
Validation loss: 2.5441332091414774

Epoch: 5| Step: 1
Training loss: 3.1261172014208047
Validation loss: 2.5356384442702278

Epoch: 5| Step: 2
Training loss: 3.376410013168775
Validation loss: 2.5204061296872124

Epoch: 5| Step: 3
Training loss: 2.975151308202939
Validation loss: 2.5425057255943666

Epoch: 5| Step: 4
Training loss: 2.966855699610207
Validation loss: 2.5288276987574787

Epoch: 5| Step: 5
Training loss: 2.6536191590191085
Validation loss: 2.5199855279482515

Epoch: 5| Step: 6
Training loss: 2.4109679699181896
Validation loss: 2.529119812876255

Epoch: 5| Step: 7
Training loss: 2.9185499559747163
Validation loss: 2.5309379503788274

Epoch: 5| Step: 8
Training loss: 2.4178545652183803
Validation loss: 2.535916801606087

Epoch: 5| Step: 9
Training loss: 2.8625312436993218
Validation loss: 2.537508474235098

Epoch: 5| Step: 10
Training loss: 1.6756554758671935
Validation loss: 2.539018507861113

Epoch: 100| Step: 0
Training loss: 2.1468242344969095
Validation loss: 2.516514258104896

Epoch: 5| Step: 1
Training loss: 2.498720032138701
Validation loss: 2.5212820022137157

Epoch: 5| Step: 2
Training loss: 3.1734194448418553
Validation loss: 2.541564256153838

Epoch: 5| Step: 3
Training loss: 2.5918788365137218
Validation loss: 2.5515438196829607

Epoch: 5| Step: 4
Training loss: 3.566043898025037
Validation loss: 2.553764874847044

Epoch: 5| Step: 5
Training loss: 2.8157291947708525
Validation loss: 2.5136059818073435

Epoch: 5| Step: 6
Training loss: 2.576340953955639
Validation loss: 2.520870107451849

Epoch: 5| Step: 7
Training loss: 2.867766527129776
Validation loss: 2.5356079894365955

Epoch: 5| Step: 8
Training loss: 2.503176387873734
Validation loss: 2.519260939857824

Epoch: 5| Step: 9
Training loss: 2.639011589900821
Validation loss: 2.5201091215907305

Epoch: 5| Step: 10
Training loss: 2.957323914007534
Validation loss: 2.5230866982080666

Epoch: 101| Step: 0
Training loss: 2.820543358988708
Validation loss: 2.517271984098712

Epoch: 5| Step: 1
Training loss: 2.8423809067496877
Validation loss: 2.50819071545955

Epoch: 5| Step: 2
Training loss: 3.364995468612726
Validation loss: 2.51327894414341

Epoch: 5| Step: 3
Training loss: 3.1892424663954997
Validation loss: 2.5290793491912833

Epoch: 5| Step: 4
Training loss: 2.732147355703716
Validation loss: 2.536837203837755

Epoch: 5| Step: 5
Training loss: 2.523763535167114
Validation loss: 2.537681418285967

Epoch: 5| Step: 6
Training loss: 2.2756518299980097
Validation loss: 2.5147751641650644

Epoch: 5| Step: 7
Training loss: 2.474030749950954
Validation loss: 2.5550174330115083

Epoch: 5| Step: 8
Training loss: 3.0792325733308323
Validation loss: 2.5403388165933185

Epoch: 5| Step: 9
Training loss: 2.585991527534643
Validation loss: 2.543699907414981

Epoch: 5| Step: 10
Training loss: 2.3905410253199575
Validation loss: 2.5305060809267386

Epoch: 102| Step: 0
Training loss: 2.9175938540336155
Validation loss: 2.5217895328240383

Epoch: 5| Step: 1
Training loss: 2.8817818305980483
Validation loss: 2.535579350114173

Epoch: 5| Step: 2
Training loss: 2.903565346096963
Validation loss: 2.5371689973416367

Epoch: 5| Step: 3
Training loss: 2.6740014610086815
Validation loss: 2.5142146927719127

Epoch: 5| Step: 4
Training loss: 2.015987984553362
Validation loss: 2.5257475104584772

Epoch: 5| Step: 5
Training loss: 3.1825326848056026
Validation loss: 2.5055916104373255

Epoch: 5| Step: 6
Training loss: 2.8023517201216985
Validation loss: 2.526243605733405

Epoch: 5| Step: 7
Training loss: 2.702078556084999
Validation loss: 2.509765046850091

Epoch: 5| Step: 8
Training loss: 2.4084078538534395
Validation loss: 2.541807492202971

Epoch: 5| Step: 9
Training loss: 3.109694881374863
Validation loss: 2.5258984190946343

Epoch: 5| Step: 10
Training loss: 2.5926948147507263
Validation loss: 2.523336317250506

Epoch: 103| Step: 0
Training loss: 3.037778921625529
Validation loss: 2.542857359383127

Epoch: 5| Step: 1
Training loss: 2.6883884114793686
Validation loss: 2.5281635113919108

Epoch: 5| Step: 2
Training loss: 3.0217572257317773
Validation loss: 2.525984431942906

Epoch: 5| Step: 3
Training loss: 2.2580969608190986
Validation loss: 2.513322729923567

Epoch: 5| Step: 4
Training loss: 2.8712993522258827
Validation loss: 2.5152634371216966

Epoch: 5| Step: 5
Training loss: 2.453728862716106
Validation loss: 2.521552964460966

Epoch: 5| Step: 6
Training loss: 3.1192497755048896
Validation loss: 2.5091733049436002

Epoch: 5| Step: 7
Training loss: 2.476126358665807
Validation loss: 2.5179199678092035

Epoch: 5| Step: 8
Training loss: 2.2792790926229003
Validation loss: 2.5211692883829113

Epoch: 5| Step: 9
Training loss: 3.335400433156342
Validation loss: 2.521327194520809

Epoch: 5| Step: 10
Training loss: 2.902659341281005
Validation loss: 2.535954251322908

Epoch: 104| Step: 0
Training loss: 3.460348040324942
Validation loss: 2.5410986544186316

Epoch: 5| Step: 1
Training loss: 2.310978259900061
Validation loss: 2.5250818494138594

Epoch: 5| Step: 2
Training loss: 2.588190762371336
Validation loss: 2.5348430266684527

Epoch: 5| Step: 3
Training loss: 2.403292039242216
Validation loss: 2.5475303606446977

Epoch: 5| Step: 4
Training loss: 2.570029292403543
Validation loss: 2.526957268939354

Epoch: 5| Step: 5
Training loss: 3.521044271496855
Validation loss: 2.5218352741370063

Epoch: 5| Step: 6
Training loss: 2.3980154852856304
Validation loss: 2.525009302231597

Epoch: 5| Step: 7
Training loss: 2.0450544341660915
Validation loss: 2.520260864751758

Epoch: 5| Step: 8
Training loss: 2.4076334951750074
Validation loss: 2.515739063817562

Epoch: 5| Step: 9
Training loss: 3.164755361542672
Validation loss: 2.5115535437237733

Epoch: 5| Step: 10
Training loss: 3.1982933261655835
Validation loss: 2.5199858565437383

Epoch: 105| Step: 0
Training loss: 2.678427085620331
Validation loss: 2.513163548648108

Epoch: 5| Step: 1
Training loss: 2.701176641298669
Validation loss: 2.5420099464020467

Epoch: 5| Step: 2
Training loss: 2.4939136805153894
Validation loss: 2.5301351255627287

Epoch: 5| Step: 3
Training loss: 2.929192992119674
Validation loss: 2.522472863031793

Epoch: 5| Step: 4
Training loss: 2.2351965827596287
Validation loss: 2.5333740547104617

Epoch: 5| Step: 5
Training loss: 3.0013122867347346
Validation loss: 2.521113751715955

Epoch: 5| Step: 6
Training loss: 2.555311587570156
Validation loss: 2.5157868573940347

Epoch: 5| Step: 7
Training loss: 3.4348221492118496
Validation loss: 2.517122661955542

Epoch: 5| Step: 8
Training loss: 3.169866818780201
Validation loss: 2.5342911540322843

Epoch: 5| Step: 9
Training loss: 2.525139768905584
Validation loss: 2.522891221024357

Epoch: 5| Step: 10
Training loss: 2.616375059737528
Validation loss: 2.5062973919358753

Epoch: 106| Step: 0
Training loss: 2.1036049373158936
Validation loss: 2.520941956133337

Epoch: 5| Step: 1
Training loss: 2.9241206877050487
Validation loss: 2.5272500151569757

Epoch: 5| Step: 2
Training loss: 2.463065447490303
Validation loss: 2.5185242439174513

Epoch: 5| Step: 3
Training loss: 2.8022520919711678
Validation loss: 2.5063252739461204

Epoch: 5| Step: 4
Training loss: 3.4872096552701803
Validation loss: 2.5012551884215757

Epoch: 5| Step: 5
Training loss: 2.7391234230625714
Validation loss: 2.4961131285124245

Epoch: 5| Step: 6
Training loss: 2.5599774295586775
Validation loss: 2.5189081574986916

Epoch: 5| Step: 7
Training loss: 2.3163714607289543
Validation loss: 2.5266343478949427

Epoch: 5| Step: 8
Training loss: 3.2609765950204985
Validation loss: 2.507725713592447

Epoch: 5| Step: 9
Training loss: 2.3930254086055354
Validation loss: 2.5278947855256217

Epoch: 5| Step: 10
Training loss: 3.08300239917762
Validation loss: 2.512740102180739

Epoch: 107| Step: 0
Training loss: 2.57658913843231
Validation loss: 2.4993989847547673

Epoch: 5| Step: 1
Training loss: 1.9970464952163198
Validation loss: 2.516689388474371

Epoch: 5| Step: 2
Training loss: 3.484958069602446
Validation loss: 2.5104713976913957

Epoch: 5| Step: 3
Training loss: 2.78293789854678
Validation loss: 2.520815240491148

Epoch: 5| Step: 4
Training loss: 3.0478607930511457
Validation loss: 2.5302012549885746

Epoch: 5| Step: 5
Training loss: 2.923445172608184
Validation loss: 2.5473432514966

Epoch: 5| Step: 6
Training loss: 2.746804374680897
Validation loss: 2.501738135815905

Epoch: 5| Step: 7
Training loss: 2.497904662378359
Validation loss: 2.527977169919903

Epoch: 5| Step: 8
Training loss: 2.5816202226093212
Validation loss: 2.5245859680843887

Epoch: 5| Step: 9
Training loss: 2.535316027292426
Validation loss: 2.5193038767880456

Epoch: 5| Step: 10
Training loss: 3.2707878806161292
Validation loss: 2.518082603642337

Epoch: 108| Step: 0
Training loss: 3.165180526815806
Validation loss: 2.519445961121299

Epoch: 5| Step: 1
Training loss: 3.186871934235784
Validation loss: 2.542305795555171

Epoch: 5| Step: 2
Training loss: 1.9434396698432486
Validation loss: 2.536679081759964

Epoch: 5| Step: 3
Training loss: 2.7107244842124922
Validation loss: 2.5230059470428667

Epoch: 5| Step: 4
Training loss: 2.5262569584814365
Validation loss: 2.543511752991558

Epoch: 5| Step: 5
Training loss: 2.6581502064097933
Validation loss: 2.522361762536642

Epoch: 5| Step: 6
Training loss: 2.5744619529581327
Validation loss: 2.534562413153436

Epoch: 5| Step: 7
Training loss: 2.8838805001059944
Validation loss: 2.5233000630227216

Epoch: 5| Step: 8
Training loss: 3.0185313080419416
Validation loss: 2.531379363191402

Epoch: 5| Step: 9
Training loss: 3.086127398478733
Validation loss: 2.517810120916337

Epoch: 5| Step: 10
Training loss: 2.457875990487119
Validation loss: 2.509284099500186

Epoch: 109| Step: 0
Training loss: 1.9669804567879836
Validation loss: 2.5103287471883853

Epoch: 5| Step: 1
Training loss: 2.835279320037417
Validation loss: 2.532344670033186

Epoch: 5| Step: 2
Training loss: 2.838869539042714
Validation loss: 2.5333181480617526

Epoch: 5| Step: 3
Training loss: 2.588355648078042
Validation loss: 2.520089147343214

Epoch: 5| Step: 4
Training loss: 3.2441250446457595
Validation loss: 2.5188577321402814

Epoch: 5| Step: 5
Training loss: 2.4661208047103638
Validation loss: 2.536378796098908

Epoch: 5| Step: 6
Training loss: 2.713522610197801
Validation loss: 2.523876569916161

Epoch: 5| Step: 7
Training loss: 2.7445091570261684
Validation loss: 2.5233301563849544

Epoch: 5| Step: 8
Training loss: 2.5115738941146337
Validation loss: 2.5203497156726127

Epoch: 5| Step: 9
Training loss: 3.3494627450595704
Validation loss: 2.518497253787875

Epoch: 5| Step: 10
Training loss: 3.072294648573572
Validation loss: 2.523620737998557

Epoch: 110| Step: 0
Training loss: 2.255430027120941
Validation loss: 2.523836340612645

Epoch: 5| Step: 1
Training loss: 2.2792736532798434
Validation loss: 2.5179504110617317

Epoch: 5| Step: 2
Training loss: 2.155072761599498
Validation loss: 2.521142929565892

Epoch: 5| Step: 3
Training loss: 2.699419425427438
Validation loss: 2.51490424736602

Epoch: 5| Step: 4
Training loss: 2.882679840591822
Validation loss: 2.5275049167875445

Epoch: 5| Step: 5
Training loss: 3.01880063474534
Validation loss: 2.511308651396344

Epoch: 5| Step: 6
Training loss: 2.504747555893828
Validation loss: 2.5300429436333793

Epoch: 5| Step: 7
Training loss: 2.7474425301342302
Validation loss: 2.528900879469861

Epoch: 5| Step: 8
Training loss: 2.989057293023431
Validation loss: 2.530237176790409

Epoch: 5| Step: 9
Training loss: 3.2431184833819158
Validation loss: 2.525099003362773

Epoch: 5| Step: 10
Training loss: 3.487175060160806
Validation loss: 2.5268239355931073

Epoch: 111| Step: 0
Training loss: 2.8236471734780406
Validation loss: 2.5355675964329243

Epoch: 5| Step: 1
Training loss: 3.0386637840452604
Validation loss: 2.5267988979020535

Epoch: 5| Step: 2
Training loss: 2.451484277278393
Validation loss: 2.5262397230925595

Epoch: 5| Step: 3
Training loss: 3.8115269404128154
Validation loss: 2.526810578739195

Epoch: 5| Step: 4
Training loss: 2.386011696331969
Validation loss: 2.5374973366815983

Epoch: 5| Step: 5
Training loss: 2.733234451025016
Validation loss: 2.5241788700029106

Epoch: 5| Step: 6
Training loss: 2.0395929223972655
Validation loss: 2.5142569419710523

Epoch: 5| Step: 7
Training loss: 2.1902105703977153
Validation loss: 2.5365667802362903

Epoch: 5| Step: 8
Training loss: 2.4033203125
Validation loss: 2.5252166270118623

Epoch: 5| Step: 9
Training loss: 3.16203035629342
Validation loss: 2.537701092429254

Epoch: 5| Step: 10
Training loss: 3.1122573451549123
Validation loss: 2.537381967583491

Epoch: 112| Step: 0
Training loss: 2.6128358104931153
Validation loss: 2.524438815123364

Epoch: 5| Step: 1
Training loss: 2.4574812581617604
Validation loss: 2.5117085167231368

Epoch: 5| Step: 2
Training loss: 2.6337524004157182
Validation loss: 2.5333740516746204

Epoch: 5| Step: 3
Training loss: 3.2179376262170067
Validation loss: 2.52737584217087

Epoch: 5| Step: 4
Training loss: 2.4072445882311917
Validation loss: 2.5243111243645946

Epoch: 5| Step: 5
Training loss: 2.9730541950396847
Validation loss: 2.50721472256963

Epoch: 5| Step: 6
Training loss: 2.8306425070226506
Validation loss: 2.538136470723598

Epoch: 5| Step: 7
Training loss: 3.1360615080912777
Validation loss: 2.5292032470047165

Epoch: 5| Step: 8
Training loss: 2.99399028283672
Validation loss: 2.517641302368233

Epoch: 5| Step: 9
Training loss: 2.298696509315063
Validation loss: 2.5102398499959344

Epoch: 5| Step: 10
Training loss: 2.5998488272220674
Validation loss: 2.52573426717555

Epoch: 113| Step: 0
Training loss: 2.6933871704172176
Validation loss: 2.524548322309817

Epoch: 5| Step: 1
Training loss: 3.1513174057212265
Validation loss: 2.524802017288242

Epoch: 5| Step: 2
Training loss: 2.67100912419185
Validation loss: 2.5207122011572825

Epoch: 5| Step: 3
Training loss: 3.0199553869122684
Validation loss: 2.542127710329045

Epoch: 5| Step: 4
Training loss: 2.4193132203121808
Validation loss: 2.5213828047769207

Epoch: 5| Step: 5
Training loss: 1.8486915575014728
Validation loss: 2.523237018195683

Epoch: 5| Step: 6
Training loss: 2.678351689362624
Validation loss: 2.5426860599461705

Epoch: 5| Step: 7
Training loss: 2.899087920304129
Validation loss: 2.5333876228614143

Epoch: 5| Step: 8
Training loss: 3.031666992450324
Validation loss: 2.5275516003897907

Epoch: 5| Step: 9
Training loss: 2.814866659688983
Validation loss: 2.52547951268028

Epoch: 5| Step: 10
Training loss: 2.992406772318166
Validation loss: 2.5299181972393305

Epoch: 114| Step: 0
Training loss: 2.691556751932021
Validation loss: 2.511570150071553

Epoch: 5| Step: 1
Training loss: 2.9353659162247197
Validation loss: 2.527496457536471

Epoch: 5| Step: 2
Training loss: 2.120084800103842
Validation loss: 2.5261694864491426

Epoch: 5| Step: 3
Training loss: 2.7608679252627475
Validation loss: 2.5236566523912156

Epoch: 5| Step: 4
Training loss: 2.9823478484693933
Validation loss: 2.5175934340208994

Epoch: 5| Step: 5
Training loss: 3.260937552641725
Validation loss: 2.511671639428056

Epoch: 5| Step: 6
Training loss: 2.55678516559284
Validation loss: 2.524621299075537

Epoch: 5| Step: 7
Training loss: 2.56655990163852
Validation loss: 2.5127380983952254

Epoch: 5| Step: 8
Training loss: 3.276987413351352
Validation loss: 2.5102714622869122

Epoch: 5| Step: 9
Training loss: 2.4719738734324475
Validation loss: 2.5363936783136762

Epoch: 5| Step: 10
Training loss: 2.538757118053591
Validation loss: 2.509154922318859

Epoch: 115| Step: 0
Training loss: 2.9628587594075833
Validation loss: 2.5309559661306764

Epoch: 5| Step: 1
Training loss: 2.5717641331449435
Validation loss: 2.524760797485493

Epoch: 5| Step: 2
Training loss: 2.8844898998647706
Validation loss: 2.524895360722361

Epoch: 5| Step: 3
Training loss: 2.5989546434966186
Validation loss: 2.5270471859937733

Epoch: 5| Step: 4
Training loss: 2.859248591062548
Validation loss: 2.52515782399411

Epoch: 5| Step: 5
Training loss: 3.1296682965684095
Validation loss: 2.526529647399767

Epoch: 5| Step: 6
Training loss: 2.5671079190271935
Validation loss: 2.501510020520917

Epoch: 5| Step: 7
Training loss: 2.348957634113036
Validation loss: 2.5169531668159264

Epoch: 5| Step: 8
Training loss: 2.4564913837352407
Validation loss: 2.519867107686713

Epoch: 5| Step: 9
Training loss: 3.273450755818551
Validation loss: 2.5415231352177137

Epoch: 5| Step: 10
Training loss: 2.4059354898583343
Validation loss: 2.54495998901116

Epoch: 116| Step: 0
Training loss: 2.590217390768429
Validation loss: 2.53907173855523

Epoch: 5| Step: 1
Training loss: 2.6434060524570384
Validation loss: 2.518570786578929

Epoch: 5| Step: 2
Training loss: 2.9914847960661968
Validation loss: 2.508459658418118

Epoch: 5| Step: 3
Training loss: 2.4389446574646363
Validation loss: 2.5292975662603623

Epoch: 5| Step: 4
Training loss: 3.380697739041275
Validation loss: 2.5120138885055847

Epoch: 5| Step: 5
Training loss: 3.2398514386249797
Validation loss: 2.5142547905266373

Epoch: 5| Step: 6
Training loss: 2.775011760239984
Validation loss: 2.5206106771161942

Epoch: 5| Step: 7
Training loss: 2.328413055828159
Validation loss: 2.526546658595582

Epoch: 5| Step: 8
Training loss: 2.594445376653547
Validation loss: 2.512457939370962

Epoch: 5| Step: 9
Training loss: 2.9143265918995644
Validation loss: 2.510997439695125

Epoch: 5| Step: 10
Training loss: 2.1406764560278813
Validation loss: 2.532043480531835

Epoch: 117| Step: 0
Training loss: 2.6565013093433567
Validation loss: 2.523061573134285

Epoch: 5| Step: 1
Training loss: 2.7443799766333816
Validation loss: 2.5323478037869642

Epoch: 5| Step: 2
Training loss: 3.0978982260864307
Validation loss: 2.534027895638454

Epoch: 5| Step: 3
Training loss: 2.2625165991411333
Validation loss: 2.525359883141245

Epoch: 5| Step: 4
Training loss: 3.050390162292672
Validation loss: 2.528268447408836

Epoch: 5| Step: 5
Training loss: 2.2602998436713615
Validation loss: 2.49736734434938

Epoch: 5| Step: 6
Training loss: 3.026882682883488
Validation loss: 2.520208341458724

Epoch: 5| Step: 7
Training loss: 2.5886848353828085
Validation loss: 2.5140150958550143

Epoch: 5| Step: 8
Training loss: 3.295744788085224
Validation loss: 2.5451149716485264

Epoch: 5| Step: 9
Training loss: 2.7585662589787074
Validation loss: 2.5274322231053166

Epoch: 5| Step: 10
Training loss: 2.201430081350796
Validation loss: 2.531514630853117

Epoch: 118| Step: 0
Training loss: 2.3901642997482955
Validation loss: 2.515866680149733

Epoch: 5| Step: 1
Training loss: 3.2045157041957992
Validation loss: 2.5387995626834026

Epoch: 5| Step: 2
Training loss: 2.2569597475234064
Validation loss: 2.5302748275924434

Epoch: 5| Step: 3
Training loss: 2.807403014928997
Validation loss: 2.522635523932929

Epoch: 5| Step: 4
Training loss: 2.8975259421950583
Validation loss: 2.520052630733839

Epoch: 5| Step: 5
Training loss: 3.1701515664530406
Validation loss: 2.534419888599748

Epoch: 5| Step: 6
Training loss: 2.229654514253203
Validation loss: 2.526460865802009

Epoch: 5| Step: 7
Training loss: 2.9156529072594273
Validation loss: 2.526651038774332

Epoch: 5| Step: 8
Training loss: 3.1657466221122372
Validation loss: 2.532860662144085

Epoch: 5| Step: 9
Training loss: 2.1116819976193915
Validation loss: 2.53252601440351

Epoch: 5| Step: 10
Training loss: 2.840989334360342
Validation loss: 2.5197594812325295

Epoch: 119| Step: 0
Training loss: 2.433561615708789
Validation loss: 2.5570867102234813

Epoch: 5| Step: 1
Training loss: 2.6723058698925097
Validation loss: 2.521414696262611

Epoch: 5| Step: 2
Training loss: 2.6370925299301833
Validation loss: 2.519113864589617

Epoch: 5| Step: 3
Training loss: 2.3188585034918168
Validation loss: 2.5230928296940074

Epoch: 5| Step: 4
Training loss: 3.037091162525779
Validation loss: 2.514459830201147

Epoch: 5| Step: 5
Training loss: 3.1915942058521636
Validation loss: 2.531458234507671

Epoch: 5| Step: 6
Training loss: 2.9418016145207306
Validation loss: 2.5583399157252207

Epoch: 5| Step: 7
Training loss: 2.7828859811084783
Validation loss: 2.529181474441385

Epoch: 5| Step: 8
Training loss: 2.7969333919631283
Validation loss: 2.5225802238180655

Epoch: 5| Step: 9
Training loss: 2.4667007053019936
Validation loss: 2.5334765406201596

Epoch: 5| Step: 10
Training loss: 2.961811832383066
Validation loss: 2.5450954223043527

Epoch: 120| Step: 0
Training loss: 2.4180266291727284
Validation loss: 2.5288651936447963

Epoch: 5| Step: 1
Training loss: 2.6020536675046153
Validation loss: 2.5356180635744967

Epoch: 5| Step: 2
Training loss: 2.537051487263007
Validation loss: 2.5199368159909903

Epoch: 5| Step: 3
Training loss: 2.605653404544015
Validation loss: 2.5292919560993266

Epoch: 5| Step: 4
Training loss: 2.9762472150934576
Validation loss: 2.515351208407849

Epoch: 5| Step: 5
Training loss: 3.311112311133296
Validation loss: 2.5269067718749447

Epoch: 5| Step: 6
Training loss: 2.795555783938729
Validation loss: 2.5319080993693874

Epoch: 5| Step: 7
Training loss: 2.3669453613524603
Validation loss: 2.539920998855375

Epoch: 5| Step: 8
Training loss: 2.9874216875482107
Validation loss: 2.5297373540274184

Epoch: 5| Step: 9
Training loss: 2.9841676010844806
Validation loss: 2.5252792275077582

Epoch: 5| Step: 10
Training loss: 2.6528707213926968
Validation loss: 2.5488703566633246

Epoch: 121| Step: 0
Training loss: 2.8163728116784488
Validation loss: 2.53151842337593

Epoch: 5| Step: 1
Training loss: 2.3828415165369536
Validation loss: 2.5457874469647406

Epoch: 5| Step: 2
Training loss: 2.8581715537091417
Validation loss: 2.5417484214487

Epoch: 5| Step: 3
Training loss: 2.9907427056088163
Validation loss: 2.5266062876908526

Epoch: 5| Step: 4
Training loss: 2.5016797144899225
Validation loss: 2.5450980523284303

Epoch: 5| Step: 5
Training loss: 2.8035178122977658
Validation loss: 2.5263447430763497

Epoch: 5| Step: 6
Training loss: 3.0543698196882385
Validation loss: 2.5164848697012125

Epoch: 5| Step: 7
Training loss: 2.956550669955391
Validation loss: 2.5330744662359113

Epoch: 5| Step: 8
Training loss: 2.42136914292434
Validation loss: 2.5179487148302577

Epoch: 5| Step: 9
Training loss: 2.9195199543094135
Validation loss: 2.508207182580029

Epoch: 5| Step: 10
Training loss: 2.370825159989556
Validation loss: 2.534023157908321

Epoch: 122| Step: 0
Training loss: 2.9307310966800353
Validation loss: 2.525289037278921

Epoch: 5| Step: 1
Training loss: 3.168810938262359
Validation loss: 2.5413884713849084

Epoch: 5| Step: 2
Training loss: 2.1758566386176286
Validation loss: 2.527291150742438

Epoch: 5| Step: 3
Training loss: 2.9235079684957093
Validation loss: 2.549449062528954

Epoch: 5| Step: 4
Training loss: 2.140363579818788
Validation loss: 2.504811169463467

Epoch: 5| Step: 5
Training loss: 2.6257081666018536
Validation loss: 2.525504963401151

Epoch: 5| Step: 6
Training loss: 3.0994701486434004
Validation loss: 2.540354884590564

Epoch: 5| Step: 7
Training loss: 2.681179865562283
Validation loss: 2.5468669352456277

Epoch: 5| Step: 8
Training loss: 2.7826207244265793
Validation loss: 2.5072088758768385

Epoch: 5| Step: 9
Training loss: 2.6285671338496455
Validation loss: 2.5312356091364587

Epoch: 5| Step: 10
Training loss: 2.8810569979886167
Validation loss: 2.5409668864018085

Epoch: 123| Step: 0
Training loss: 2.86987810414338
Validation loss: 2.5267577603903026

Epoch: 5| Step: 1
Training loss: 2.3248638892581774
Validation loss: 2.537707947784247

Epoch: 5| Step: 2
Training loss: 3.162380044343317
Validation loss: 2.522379206351695

Epoch: 5| Step: 3
Training loss: 2.803526231507298
Validation loss: 2.537609469731116

Epoch: 5| Step: 4
Training loss: 2.7980870013013903
Validation loss: 2.5377500407328717

Epoch: 5| Step: 5
Training loss: 2.747825196088364
Validation loss: 2.530732379807601

Epoch: 5| Step: 6
Training loss: 2.791158193333223
Validation loss: 2.5252947619120008

Epoch: 5| Step: 7
Training loss: 2.4519599042977713
Validation loss: 2.522626620508304

Epoch: 5| Step: 8
Training loss: 2.1831436375724227
Validation loss: 2.4983949328418116

Epoch: 5| Step: 9
Training loss: 2.6742309534372266
Validation loss: 2.502652756388505

Epoch: 5| Step: 10
Training loss: 3.189483025732469
Validation loss: 2.5259329086214706

Epoch: 124| Step: 0
Training loss: 2.2395299387636354
Validation loss: 2.5303504648969035

Epoch: 5| Step: 1
Training loss: 2.453661623046788
Validation loss: 2.5259325087397135

Epoch: 5| Step: 2
Training loss: 2.9243052774275844
Validation loss: 2.51737599473249

Epoch: 5| Step: 3
Training loss: 3.086214849828864
Validation loss: 2.5235628425639542

Epoch: 5| Step: 4
Training loss: 2.6799292274638686
Validation loss: 2.519606721432446

Epoch: 5| Step: 5
Training loss: 2.782701006652626
Validation loss: 2.5278127574637694

Epoch: 5| Step: 6
Training loss: 2.586305713081134
Validation loss: 2.530356301683891

Epoch: 5| Step: 7
Training loss: 3.4868305944803932
Validation loss: 2.5166546398425003

Epoch: 5| Step: 8
Training loss: 2.4628104696760365
Validation loss: 2.512476511085147

Epoch: 5| Step: 9
Training loss: 2.6953190457914173
Validation loss: 2.5232448943131653

Epoch: 5| Step: 10
Training loss: 2.408647902996943
Validation loss: 2.5316715687003417

Epoch: 125| Step: 0
Training loss: 2.8738862243727827
Validation loss: 2.5049479475469827

Epoch: 5| Step: 1
Training loss: 3.090327055082204
Validation loss: 2.5229526303991725

Epoch: 5| Step: 2
Training loss: 2.76537162352857
Validation loss: 2.5179136475848036

Epoch: 5| Step: 3
Training loss: 2.6327344390435354
Validation loss: 2.5326514618762452

Epoch: 5| Step: 4
Training loss: 2.34699970020736
Validation loss: 2.4987122531345287

Epoch: 5| Step: 5
Training loss: 2.7925314821058413
Validation loss: 2.515814306623633

Epoch: 5| Step: 6
Training loss: 2.8659409143995105
Validation loss: 2.5051594684378675

Epoch: 5| Step: 7
Training loss: 2.8939727367917523
Validation loss: 2.532885087284578

Epoch: 5| Step: 8
Training loss: 2.5292359805048967
Validation loss: 2.52570966732044

Epoch: 5| Step: 9
Training loss: 2.8514344069046866
Validation loss: 2.501069557937233

Epoch: 5| Step: 10
Training loss: 2.594354765833111
Validation loss: 2.543612406313178

Epoch: 126| Step: 0
Training loss: 2.1234094613689334
Validation loss: 2.5290599794887503

Epoch: 5| Step: 1
Training loss: 2.8616380751493593
Validation loss: 2.55657972699494

Epoch: 5| Step: 2
Training loss: 2.859199393461363
Validation loss: 2.513529782490476

Epoch: 5| Step: 3
Training loss: 2.6522416332514545
Validation loss: 2.5372675470519566

Epoch: 5| Step: 4
Training loss: 3.566345013764393
Validation loss: 2.529287791294224

Epoch: 5| Step: 5
Training loss: 2.3522521366965545
Validation loss: 2.5357991273473113

Epoch: 5| Step: 6
Training loss: 2.1005054637580898
Validation loss: 2.5101686509264276

Epoch: 5| Step: 7
Training loss: 2.773828957023974
Validation loss: 2.5127531614361844

Epoch: 5| Step: 8
Training loss: 2.929667968684895
Validation loss: 2.5314357344568426

Epoch: 5| Step: 9
Training loss: 3.1025064795070616
Validation loss: 2.531766007631595

Epoch: 5| Step: 10
Training loss: 2.556174309316439
Validation loss: 2.5430242791573052

Epoch: 127| Step: 0
Training loss: 2.435635562802366
Validation loss: 2.5230838018899058

Epoch: 5| Step: 1
Training loss: 2.911392584224941
Validation loss: 2.535281468230668

Epoch: 5| Step: 2
Training loss: 2.4594832710368766
Validation loss: 2.502923795460688

Epoch: 5| Step: 3
Training loss: 2.722180099118152
Validation loss: 2.51743376004108

Epoch: 5| Step: 4
Training loss: 2.6438885447315834
Validation loss: 2.5189222493459247

Epoch: 5| Step: 5
Training loss: 2.9300925826720534
Validation loss: 2.5213115461940467

Epoch: 5| Step: 6
Training loss: 2.9774706799194974
Validation loss: 2.518504144110943

Epoch: 5| Step: 7
Training loss: 3.056847942500067
Validation loss: 2.519247521535446

Epoch: 5| Step: 8
Training loss: 2.8897862222663444
Validation loss: 2.5243547198034517

Epoch: 5| Step: 9
Training loss: 2.3882854707460353
Validation loss: 2.5147736034151866

Epoch: 5| Step: 10
Training loss: 2.606469827067181
Validation loss: 2.526363832721799

Epoch: 128| Step: 0
Training loss: 2.1945269390755096
Validation loss: 2.5379218218208357

Epoch: 5| Step: 1
Training loss: 2.843352908402112
Validation loss: 2.5421429481527587

Epoch: 5| Step: 2
Training loss: 3.0747919772116292
Validation loss: 2.5405839802481807

Epoch: 5| Step: 3
Training loss: 3.1762763994569285
Validation loss: 2.533922414045319

Epoch: 5| Step: 4
Training loss: 2.860803466190568
Validation loss: 2.5257837858882897

Epoch: 5| Step: 5
Training loss: 2.9166765122020397
Validation loss: 2.5169501768629727

Epoch: 5| Step: 6
Training loss: 2.1829668213039377
Validation loss: 2.5398607174030503

Epoch: 5| Step: 7
Training loss: 2.8281589737848463
Validation loss: 2.5258319131233193

Epoch: 5| Step: 8
Training loss: 2.870642469582617
Validation loss: 2.528473451788565

Epoch: 5| Step: 9
Training loss: 2.5539479716415148
Validation loss: 2.5401356236496904

Epoch: 5| Step: 10
Training loss: 2.4614618620737563
Validation loss: 2.5680153780952004

Epoch: 129| Step: 0
Training loss: 3.2548179127849446
Validation loss: 2.5165041910260584

Epoch: 5| Step: 1
Training loss: 2.7843069357211117
Validation loss: 2.5427851993708077

Epoch: 5| Step: 2
Training loss: 2.597193626568246
Validation loss: 2.5115641512209903

Epoch: 5| Step: 3
Training loss: 2.7883544365022916
Validation loss: 2.5105035135650704

Epoch: 5| Step: 4
Training loss: 2.8832436836138426
Validation loss: 2.5316105480352897

Epoch: 5| Step: 5
Training loss: 2.3729374613086587
Validation loss: 2.544653983961405

Epoch: 5| Step: 6
Training loss: 2.0845345912157702
Validation loss: 2.5291411815277725

Epoch: 5| Step: 7
Training loss: 2.914392038484097
Validation loss: 2.5480154365376793

Epoch: 5| Step: 8
Training loss: 3.001410311763139
Validation loss: 2.5234918720891244

Epoch: 5| Step: 9
Training loss: 2.461466027074395
Validation loss: 2.5492922657523724

Epoch: 5| Step: 10
Training loss: 2.7199861193751533
Validation loss: 2.5144934356671436

Epoch: 130| Step: 0
Training loss: 2.060130058182837
Validation loss: 2.5269967820312673

Epoch: 5| Step: 1
Training loss: 3.3755537920940193
Validation loss: 2.5460531696861186

Epoch: 5| Step: 2
Training loss: 2.789851998536226
Validation loss: 2.5341849823420093

Epoch: 5| Step: 3
Training loss: 2.64018290955351
Validation loss: 2.527917690800556

Epoch: 5| Step: 4
Training loss: 2.595061097206785
Validation loss: 2.53856277432471

Epoch: 5| Step: 5
Training loss: 2.6366834850072283
Validation loss: 2.530885451142897

Epoch: 5| Step: 6
Training loss: 2.818071737964188
Validation loss: 2.5217518930838185

Epoch: 5| Step: 7
Training loss: 3.5437677023040135
Validation loss: 2.542744296294473

Epoch: 5| Step: 8
Training loss: 2.456299010120291
Validation loss: 2.524317482904811

Epoch: 5| Step: 9
Training loss: 1.7776997974631468
Validation loss: 2.5326870437413147

Epoch: 5| Step: 10
Training loss: 2.859556598837583
Validation loss: 2.5201416110675283

Epoch: 131| Step: 0
Training loss: 2.808261390089017
Validation loss: 2.5466647614664844

Epoch: 5| Step: 1
Training loss: 2.634938816178175
Validation loss: 2.5322330833967936

Epoch: 5| Step: 2
Training loss: 3.0831174688983825
Validation loss: 2.531013187525831

Epoch: 5| Step: 3
Training loss: 2.8915783109184208
Validation loss: 2.5287083685364906

Epoch: 5| Step: 4
Training loss: 2.6387039510159487
Validation loss: 2.5170906335918906

Epoch: 5| Step: 5
Training loss: 2.427812447677417
Validation loss: 2.522632432483721

Epoch: 5| Step: 6
Training loss: 2.8892670612113682
Validation loss: 2.5573686234317496

Epoch: 5| Step: 7
Training loss: 2.006756575377249
Validation loss: 2.5548528732075733

Epoch: 5| Step: 8
Training loss: 3.3889160068309425
Validation loss: 2.5293331456423545

Epoch: 5| Step: 9
Training loss: 2.7400264212455205
Validation loss: 2.5496264714750376

Epoch: 5| Step: 10
Training loss: 2.293993745165066
Validation loss: 2.526559998580554

Epoch: 132| Step: 0
Training loss: 2.9245143128499906
Validation loss: 2.516170036304169

Epoch: 5| Step: 1
Training loss: 2.9612668138624896
Validation loss: 2.5433819898489642

Epoch: 5| Step: 2
Training loss: 2.883536064503187
Validation loss: 2.5377984537056846

Epoch: 5| Step: 3
Training loss: 2.6377509915217408
Validation loss: 2.536633488353206

Epoch: 5| Step: 4
Training loss: 2.4617012897381927
Validation loss: 2.548466423602222

Epoch: 5| Step: 5
Training loss: 2.753947632390762
Validation loss: 2.527213985465962

Epoch: 5| Step: 6
Training loss: 2.7402928428366837
Validation loss: 2.5393947437582267

Epoch: 5| Step: 7
Training loss: 3.215885109571526
Validation loss: 2.5439442009602886

Epoch: 5| Step: 8
Training loss: 2.509116430963226
Validation loss: 2.5081392927022104

Epoch: 5| Step: 9
Training loss: 2.390460438767805
Validation loss: 2.511655976875837

Epoch: 5| Step: 10
Training loss: 2.3326325272406523
Validation loss: 2.522713435941856

Epoch: 133| Step: 0
Training loss: 2.5837680902018496
Validation loss: 2.517445425267751

Epoch: 5| Step: 1
Training loss: 3.0867982074510394
Validation loss: 2.5245909905926855

Epoch: 5| Step: 2
Training loss: 2.6776989378695424
Validation loss: 2.5209276996449614

Epoch: 5| Step: 3
Training loss: 2.45571917750625
Validation loss: 2.5043758944839105

Epoch: 5| Step: 4
Training loss: 2.6970002570740914
Validation loss: 2.5296155102896707

Epoch: 5| Step: 5
Training loss: 2.947469796502408
Validation loss: 2.54817742472588

Epoch: 5| Step: 6
Training loss: 2.874198636033606
Validation loss: 2.5189209181246697

Epoch: 5| Step: 7
Training loss: 2.7716488510479866
Validation loss: 2.524469469693365

Epoch: 5| Step: 8
Training loss: 2.684264875113722
Validation loss: 2.5218427845993294

Epoch: 5| Step: 9
Training loss: 2.50600646398248
Validation loss: 2.5067579717633786

Epoch: 5| Step: 10
Training loss: 2.8329330797407746
Validation loss: 2.509606737039416

Epoch: 134| Step: 0
Training loss: 2.2343411743164583
Validation loss: 2.5194707991850693

Epoch: 5| Step: 1
Training loss: 2.671134444554641
Validation loss: 2.5228535418315987

Epoch: 5| Step: 2
Training loss: 2.887557593097424
Validation loss: 2.521969798296837

Epoch: 5| Step: 3
Training loss: 2.8465848772804954
Validation loss: 2.533957703889169

Epoch: 5| Step: 4
Training loss: 2.988252208294336
Validation loss: 2.545011891688362

Epoch: 5| Step: 5
Training loss: 3.0969828659041854
Validation loss: 2.5204124126383136

Epoch: 5| Step: 6
Training loss: 2.635085757342133
Validation loss: 2.536537214834287

Epoch: 5| Step: 7
Training loss: 2.36145927879024
Validation loss: 2.5183549520338935

Epoch: 5| Step: 8
Training loss: 2.978774642938449
Validation loss: 2.5338739882264276

Epoch: 5| Step: 9
Training loss: 2.4569774918974976
Validation loss: 2.550428420907083

Epoch: 5| Step: 10
Training loss: 2.769879274870509
Validation loss: 2.5229649773298135

Epoch: 135| Step: 0
Training loss: 2.6622746976705716
Validation loss: 2.5410951939888875

Epoch: 5| Step: 1
Training loss: 3.124685347928583
Validation loss: 2.5107329662102913

Epoch: 5| Step: 2
Training loss: 2.597772260097492
Validation loss: 2.5275540336421614

Epoch: 5| Step: 3
Training loss: 2.801747005278259
Validation loss: 2.5320856499274873

Epoch: 5| Step: 4
Training loss: 2.891695061653259
Validation loss: 2.5088927233279956

Epoch: 5| Step: 5
Training loss: 2.1292395094662027
Validation loss: 2.546108346625316

Epoch: 5| Step: 6
Training loss: 2.2562818783861838
Validation loss: 2.538338763232451

Epoch: 5| Step: 7
Training loss: 2.836087141476413
Validation loss: 2.5325684511017608

Epoch: 5| Step: 8
Training loss: 2.6289827015432086
Validation loss: 2.5216866695304

Epoch: 5| Step: 9
Training loss: 3.0469665513589246
Validation loss: 2.55219574698075

Epoch: 5| Step: 10
Training loss: 2.8388673554674013
Validation loss: 2.522210251479787

Epoch: 136| Step: 0
Training loss: 2.7319413172226614
Validation loss: 2.528129898010366

Epoch: 5| Step: 1
Training loss: 2.6742313992075495
Validation loss: 2.5254655944625966

Epoch: 5| Step: 2
Training loss: 3.025170906220146
Validation loss: 2.544935600179469

Epoch: 5| Step: 3
Training loss: 2.9137537079292053
Validation loss: 2.53522484018191

Epoch: 5| Step: 4
Training loss: 2.278536502213415
Validation loss: 2.5221481043821075

Epoch: 5| Step: 5
Training loss: 3.1798623526670426
Validation loss: 2.536822299971201

Epoch: 5| Step: 6
Training loss: 2.257726436677463
Validation loss: 2.5359363226369447

Epoch: 5| Step: 7
Training loss: 2.459335920127583
Validation loss: 2.5158273478833673

Epoch: 5| Step: 8
Training loss: 2.728551013801687
Validation loss: 2.523008166216663

Epoch: 5| Step: 9
Training loss: 2.617455676558822
Validation loss: 2.5306672089109217

Epoch: 5| Step: 10
Training loss: 2.8050255531874257
Validation loss: 2.5258554618126148

Epoch: 137| Step: 0
Training loss: 2.797914279947141
Validation loss: 2.5411486702174892

Epoch: 5| Step: 1
Training loss: 1.8277561435108354
Validation loss: 2.525045908589054

Epoch: 5| Step: 2
Training loss: 2.54623283294299
Validation loss: 2.529380662082778

Epoch: 5| Step: 3
Training loss: 2.6557095146063054
Validation loss: 2.533343519527388

Epoch: 5| Step: 4
Training loss: 3.8211564144096206
Validation loss: 2.5326440259806873

Epoch: 5| Step: 5
Training loss: 2.830823422394851
Validation loss: 2.5269528552801033

Epoch: 5| Step: 6
Training loss: 2.8598248904313466
Validation loss: 2.5563083201057597

Epoch: 5| Step: 7
Training loss: 2.417488703735848
Validation loss: 2.533649161890493

Epoch: 5| Step: 8
Training loss: 2.441792059359503
Validation loss: 2.5398094281390313

Epoch: 5| Step: 9
Training loss: 2.6982085218617757
Validation loss: 2.5100696081966625

Epoch: 5| Step: 10
Training loss: 2.6734456582226396
Validation loss: 2.5388836309062404

Epoch: 138| Step: 0
Training loss: 2.159047178345752
Validation loss: 2.5188712466831724

Epoch: 5| Step: 1
Training loss: 2.509941742057562
Validation loss: 2.544147409657669

Epoch: 5| Step: 2
Training loss: 2.5457010196280807
Validation loss: 2.547080871084623

Epoch: 5| Step: 3
Training loss: 2.4777846344347836
Validation loss: 2.5316932935207688

Epoch: 5| Step: 4
Training loss: 3.2750224280135396
Validation loss: 2.517134277700149

Epoch: 5| Step: 5
Training loss: 2.4897196159144386
Validation loss: 2.527108579591903

Epoch: 5| Step: 6
Training loss: 2.178358212742165
Validation loss: 2.5134152053182626

Epoch: 5| Step: 7
Training loss: 2.5932944943413396
Validation loss: 2.5445724709698436

Epoch: 5| Step: 8
Training loss: 2.8424308987205764
Validation loss: 2.5428849317624285

Epoch: 5| Step: 9
Training loss: 3.661941923254927
Validation loss: 2.51302304432231

Epoch: 5| Step: 10
Training loss: 2.756689777884726
Validation loss: 2.534364583629149

Epoch: 139| Step: 0
Training loss: 3.689676709162965
Validation loss: 2.535816607626072

Epoch: 5| Step: 1
Training loss: 2.3999430252306047
Validation loss: 2.5212088179570937

Epoch: 5| Step: 2
Training loss: 2.410204424755317
Validation loss: 2.5425019746686153

Epoch: 5| Step: 3
Training loss: 2.8170588526917233
Validation loss: 2.5089305222658007

Epoch: 5| Step: 4
Training loss: 2.6016170278339166
Validation loss: 2.5023240751366944

Epoch: 5| Step: 5
Training loss: 3.3604833859187777
Validation loss: 2.5142114849239086

Epoch: 5| Step: 6
Training loss: 2.4542558335414357
Validation loss: 2.543302629491632

Epoch: 5| Step: 7
Training loss: 2.798303505710344
Validation loss: 2.521468459233255

Epoch: 5| Step: 8
Training loss: 2.1387336338828313
Validation loss: 2.517899810743338

Epoch: 5| Step: 9
Training loss: 2.30217396369311
Validation loss: 2.5215354147852507

Epoch: 5| Step: 10
Training loss: 2.651103160771979
Validation loss: 2.5418398929377886

Epoch: 140| Step: 0
Training loss: 2.5166977200476457
Validation loss: 2.5097364476712323

Epoch: 5| Step: 1
Training loss: 3.440009986729874
Validation loss: 2.521854882821394

Epoch: 5| Step: 2
Training loss: 3.1410081923802124
Validation loss: 2.533212208834162

Epoch: 5| Step: 3
Training loss: 2.5235265001599005
Validation loss: 2.528883990553434

Epoch: 5| Step: 4
Training loss: 2.8609668073918733
Validation loss: 2.512091590087105

Epoch: 5| Step: 5
Training loss: 2.88590213442098
Validation loss: 2.5206603142809523

Epoch: 5| Step: 6
Training loss: 2.3763918562701565
Validation loss: 2.529002411818664

Epoch: 5| Step: 7
Training loss: 2.5496022419094047
Validation loss: 2.534465571897555

Epoch: 5| Step: 8
Training loss: 2.5119566144780485
Validation loss: 2.5328144336921885

Epoch: 5| Step: 9
Training loss: 2.1302387512074783
Validation loss: 2.551005320158039

Epoch: 5| Step: 10
Training loss: 2.6915035147178927
Validation loss: 2.527134591126099

Epoch: 141| Step: 0
Training loss: 2.859524915757065
Validation loss: 2.514934759233856

Epoch: 5| Step: 1
Training loss: 2.4445938175493067
Validation loss: 2.535371089055837

Epoch: 5| Step: 2
Training loss: 2.8206203641373695
Validation loss: 2.5178875896640505

Epoch: 5| Step: 3
Training loss: 2.665992731128808
Validation loss: 2.492690608134259

Epoch: 5| Step: 4
Training loss: 2.074990798745954
Validation loss: 2.532894973873188

Epoch: 5| Step: 5
Training loss: 2.8332769350908795
Validation loss: 2.532753322134918

Epoch: 5| Step: 6
Training loss: 3.2118310729150097
Validation loss: 2.52925212511854

Epoch: 5| Step: 7
Training loss: 2.464311496923558
Validation loss: 2.5261858718926695

Epoch: 5| Step: 8
Training loss: 3.0185595845257107
Validation loss: 2.539659291091919

Epoch: 5| Step: 9
Training loss: 2.5221229176104094
Validation loss: 2.5335402408328984

Epoch: 5| Step: 10
Training loss: 2.8902978660434924
Validation loss: 2.521745297303226

Epoch: 142| Step: 0
Training loss: 2.6914404436439106
Validation loss: 2.5186787727455133

Epoch: 5| Step: 1
Training loss: 2.392439409244108
Validation loss: 2.5125588867832422

Epoch: 5| Step: 2
Training loss: 3.029257837433832
Validation loss: 2.521319710992642

Epoch: 5| Step: 3
Training loss: 2.186645667956096
Validation loss: 2.49523584484132

Epoch: 5| Step: 4
Training loss: 3.037071065875754
Validation loss: 2.5270380465204867

Epoch: 5| Step: 5
Training loss: 2.327000493618823
Validation loss: 2.505184591925861

Epoch: 5| Step: 6
Training loss: 3.160012146407335
Validation loss: 2.5178460947729357

Epoch: 5| Step: 7
Training loss: 3.2031293171178987
Validation loss: 2.5287456776403068

Epoch: 5| Step: 8
Training loss: 2.5874365757342277
Validation loss: 2.5377805703745593

Epoch: 5| Step: 9
Training loss: 2.388998038979736
Validation loss: 2.538856946072682

Epoch: 5| Step: 10
Training loss: 2.6626829451628637
Validation loss: 2.5313612593052195

Epoch: 143| Step: 0
Training loss: 2.339792190518639
Validation loss: 2.528792910109278

Epoch: 5| Step: 1
Training loss: 2.7376552415837976
Validation loss: 2.551486828128662

Epoch: 5| Step: 2
Training loss: 2.8118088614780214
Validation loss: 2.543199891385777

Epoch: 5| Step: 3
Training loss: 2.982144465906967
Validation loss: 2.5479433320522054

Epoch: 5| Step: 4
Training loss: 2.691292592875232
Validation loss: 2.528248076254059

Epoch: 5| Step: 5
Training loss: 1.7795693349083006
Validation loss: 2.534528250433449

Epoch: 5| Step: 6
Training loss: 3.0925915022861017
Validation loss: 2.5194217170626407

Epoch: 5| Step: 7
Training loss: 2.83202586206384
Validation loss: 2.547636239314607

Epoch: 5| Step: 8
Training loss: 3.173190140204894
Validation loss: 2.5454075935247493

Epoch: 5| Step: 9
Training loss: 2.7318220154968054
Validation loss: 2.523188846545859

Epoch: 5| Step: 10
Training loss: 2.3963119581607013
Validation loss: 2.516490658168099

Epoch: 144| Step: 0
Training loss: 2.2982893760558762
Validation loss: 2.5277423566178334

Epoch: 5| Step: 1
Training loss: 2.9955842580876015
Validation loss: 2.536565474445889

Epoch: 5| Step: 2
Training loss: 2.30380076790734
Validation loss: 2.523786398704545

Epoch: 5| Step: 3
Training loss: 2.5079030053275306
Validation loss: 2.5407782147802567

Epoch: 5| Step: 4
Training loss: 2.6500099037993
Validation loss: 2.499052325465577

Epoch: 5| Step: 5
Training loss: 2.616494066980693
Validation loss: 2.529773244414435

Epoch: 5| Step: 6
Training loss: 3.208502001271479
Validation loss: 2.5363819465994997

Epoch: 5| Step: 7
Training loss: 3.400787873703132
Validation loss: 2.51172108172377

Epoch: 5| Step: 8
Training loss: 2.651765066399493
Validation loss: 2.521778213004452

Epoch: 5| Step: 9
Training loss: 2.4586021362588344
Validation loss: 2.536330372705617

Epoch: 5| Step: 10
Training loss: 2.6386109746742483
Validation loss: 2.537068004520788

Epoch: 145| Step: 0
Training loss: 3.251850188404618
Validation loss: 2.5424512549542806

Epoch: 5| Step: 1
Training loss: 2.2989775707118123
Validation loss: 2.5701527253340832

Epoch: 5| Step: 2
Training loss: 2.440624874959984
Validation loss: 2.520732088088928

Epoch: 5| Step: 3
Training loss: 3.058577848078873
Validation loss: 2.5702402081200524

Epoch: 5| Step: 4
Training loss: 3.330197544138095
Validation loss: 2.540535501788327

Epoch: 5| Step: 5
Training loss: 2.7653047192998956
Validation loss: 2.5351770732810746

Epoch: 5| Step: 6
Training loss: 2.1562553073983595
Validation loss: 2.5355340407633684

Epoch: 5| Step: 7
Training loss: 2.2847105977369164
Validation loss: 2.5284783023220077

Epoch: 5| Step: 8
Training loss: 3.076281620066185
Validation loss: 2.5236054960171947

Epoch: 5| Step: 9
Training loss: 2.857218278161908
Validation loss: 2.527826868634043

Epoch: 5| Step: 10
Training loss: 1.8760734028720378
Validation loss: 2.5248128473383553

Epoch: 146| Step: 0
Training loss: 2.7016073387577237
Validation loss: 2.521378834835219

Epoch: 5| Step: 1
Training loss: 2.650083047537189
Validation loss: 2.5438463886026215

Epoch: 5| Step: 2
Training loss: 3.122907319329549
Validation loss: 2.538530825599106

Epoch: 5| Step: 3
Training loss: 2.74651723013834
Validation loss: 2.540472709092246

Epoch: 5| Step: 4
Training loss: 1.8777636823118868
Validation loss: 2.5132383279325032

Epoch: 5| Step: 5
Training loss: 2.283839806772902
Validation loss: 2.5100873447669305

Epoch: 5| Step: 6
Training loss: 2.5315506368052336
Validation loss: 2.5473462425090956

Epoch: 5| Step: 7
Training loss: 2.5227064839474274
Validation loss: 2.5432138143603935

Epoch: 5| Step: 8
Training loss: 3.5044056548194122
Validation loss: 2.5370998382862773

Epoch: 5| Step: 9
Training loss: 2.743657079700375
Validation loss: 2.5319586262895943

Epoch: 5| Step: 10
Training loss: 2.9030215471803777
Validation loss: 2.5115733847699513

Epoch: 147| Step: 0
Training loss: 2.3448189904275925
Validation loss: 2.50627707234897

Epoch: 5| Step: 1
Training loss: 2.841714151233684
Validation loss: 2.5217862878481725

Epoch: 5| Step: 2
Training loss: 2.9215014213781836
Validation loss: 2.554211782489154

Epoch: 5| Step: 3
Training loss: 2.8655855032591955
Validation loss: 2.5388508359969744

Epoch: 5| Step: 4
Training loss: 2.0896485424674593
Validation loss: 2.5309954603508285

Epoch: 5| Step: 5
Training loss: 2.5411284038313178
Validation loss: 2.527079333668661

Epoch: 5| Step: 6
Training loss: 2.9559812911581083
Validation loss: 2.5316854214087177

Epoch: 5| Step: 7
Training loss: 3.046596416551066
Validation loss: 2.5287405508519742

Epoch: 5| Step: 8
Training loss: 2.774314720150342
Validation loss: 2.556617009408953

Epoch: 5| Step: 9
Training loss: 2.7852909863425532
Validation loss: 2.534333787764101

Epoch: 5| Step: 10
Training loss: 2.4600033879450596
Validation loss: 2.551577876155904

Epoch: 148| Step: 0
Training loss: 3.1135624431905145
Validation loss: 2.537974338639244

Epoch: 5| Step: 1
Training loss: 1.740940622858613
Validation loss: 2.511833937322283

Epoch: 5| Step: 2
Training loss: 3.175800168589867
Validation loss: 2.524008951845135

Epoch: 5| Step: 3
Training loss: 2.65950324604362
Validation loss: 2.5383159893644187

Epoch: 5| Step: 4
Training loss: 2.3270501849603447
Validation loss: 2.527308876005061

Epoch: 5| Step: 5
Training loss: 2.713664154002214
Validation loss: 2.5082829875865427

Epoch: 5| Step: 6
Training loss: 2.6714199637936606
Validation loss: 2.5306776851342128

Epoch: 5| Step: 7
Training loss: 2.4909444836188523
Validation loss: 2.5566840451909556

Epoch: 5| Step: 8
Training loss: 3.0707940653374046
Validation loss: 2.5660674590962147

Epoch: 5| Step: 9
Training loss: 3.2744210525014865
Validation loss: 2.5320832625408407

Epoch: 5| Step: 10
Training loss: 2.259842009971976
Validation loss: 2.549528769524728

Epoch: 149| Step: 0
Training loss: 2.7682276833722557
Validation loss: 2.5462803129048974

Epoch: 5| Step: 1
Training loss: 2.1477695605879563
Validation loss: 2.5385907902267726

Epoch: 5| Step: 2
Training loss: 3.066775393286279
Validation loss: 2.5200294484753427

Epoch: 5| Step: 3
Training loss: 2.6036084810311935
Validation loss: 2.5400065126304083

Epoch: 5| Step: 4
Training loss: 2.7690630733625867
Validation loss: 2.5275176329969504

Epoch: 5| Step: 5
Training loss: 2.746259920348011
Validation loss: 2.5451526798624755

Epoch: 5| Step: 6
Training loss: 3.120761438279267
Validation loss: 2.54380813293958

Epoch: 5| Step: 7
Training loss: 2.6969054891160202
Validation loss: 2.528228425387026

Epoch: 5| Step: 8
Training loss: 2.8595580996063736
Validation loss: 2.5276119755963196

Epoch: 5| Step: 9
Training loss: 2.2840119455629746
Validation loss: 2.5150158760660646

Epoch: 5| Step: 10
Training loss: 2.6056293398024226
Validation loss: 2.5444443336921814

Epoch: 150| Step: 0
Training loss: 2.3644696739092828
Validation loss: 2.514597328272762

Epoch: 5| Step: 1
Training loss: 2.54074463509839
Validation loss: 2.5425937749701832

Epoch: 5| Step: 2
Training loss: 2.4042744639364724
Validation loss: 2.522293389708773

Epoch: 5| Step: 3
Training loss: 2.890988218839663
Validation loss: 2.5209394829399066

Epoch: 5| Step: 4
Training loss: 2.584090480802883
Validation loss: 2.5466987976205977

Epoch: 5| Step: 5
Training loss: 2.850288336372983
Validation loss: 2.519612765235624

Epoch: 5| Step: 6
Training loss: 3.65660432582166
Validation loss: 2.552224660898412

Epoch: 5| Step: 7
Training loss: 2.589221540134974
Validation loss: 2.5380124640544226

Epoch: 5| Step: 8
Training loss: 2.688332894363257
Validation loss: 2.5386514098363966

Epoch: 5| Step: 9
Training loss: 2.3753941861342223
Validation loss: 2.5291251740619454

Epoch: 5| Step: 10
Training loss: 2.463579678931075
Validation loss: 2.525886624425

Epoch: 151| Step: 0
Training loss: 2.969089528795803
Validation loss: 2.5450934097448354

Epoch: 5| Step: 1
Training loss: 2.1931397896260103
Validation loss: 2.5104158357491126

Epoch: 5| Step: 2
Training loss: 2.8444960736847587
Validation loss: 2.5281394645045623

Epoch: 5| Step: 3
Training loss: 2.76883584399649
Validation loss: 2.5279494775344253

Epoch: 5| Step: 4
Training loss: 2.789081648218583
Validation loss: 2.5347180092885626

Epoch: 5| Step: 5
Training loss: 2.6564036605370887
Validation loss: 2.5143643956104795

Epoch: 5| Step: 6
Training loss: 2.9590976605273736
Validation loss: 2.5232228366517813

Epoch: 5| Step: 7
Training loss: 2.2818067733440768
Validation loss: 2.5102909213056184

Epoch: 5| Step: 8
Training loss: 2.8128263496097805
Validation loss: 2.535634146320888

Epoch: 5| Step: 9
Training loss: 2.7720261972298643
Validation loss: 2.5235427402221022

Epoch: 5| Step: 10
Training loss: 2.5940048827902573
Validation loss: 2.534605677516575

Epoch: 152| Step: 0
Training loss: 1.7622930094059208
Validation loss: 2.532266739501944

Epoch: 5| Step: 1
Training loss: 3.204679233014005
Validation loss: 2.541035004961576

Epoch: 5| Step: 2
Training loss: 2.8372456709697387
Validation loss: 2.5472535612555967

Epoch: 5| Step: 3
Training loss: 2.7553231131881515
Validation loss: 2.525885714017525

Epoch: 5| Step: 4
Training loss: 1.777769201310981
Validation loss: 2.53195706094425

Epoch: 5| Step: 5
Training loss: 2.879063429870091
Validation loss: 2.532856576082128

Epoch: 5| Step: 6
Training loss: 3.1238804146789683
Validation loss: 2.540358234011431

Epoch: 5| Step: 7
Training loss: 2.734806222972566
Validation loss: 2.5249721348899294

Epoch: 5| Step: 8
Training loss: 2.6730713424666583
Validation loss: 2.5171917058464333

Epoch: 5| Step: 9
Training loss: 2.7747750509004088
Validation loss: 2.5321134702069474

Epoch: 5| Step: 10
Training loss: 2.8901180338267647
Validation loss: 2.542751579651063

Epoch: 153| Step: 0
Training loss: 2.033521936872804
Validation loss: 2.53528408012226

Epoch: 5| Step: 1
Training loss: 2.800802105728879
Validation loss: 2.5268468394148726

Epoch: 5| Step: 2
Training loss: 2.626373204314936
Validation loss: 2.5334745775213583

Epoch: 5| Step: 3
Training loss: 2.815188500443582
Validation loss: 2.522600793183679

Epoch: 5| Step: 4
Training loss: 2.575539326898709
Validation loss: 2.5207250502850904

Epoch: 5| Step: 5
Training loss: 2.9507026223555926
Validation loss: 2.5421211563223762

Epoch: 5| Step: 6
Training loss: 2.7186901919045696
Validation loss: 2.545406886496146

Epoch: 5| Step: 7
Training loss: 2.4853256613949966
Validation loss: 2.5420124424613033

Epoch: 5| Step: 8
Training loss: 3.1340401163461036
Validation loss: 2.555130335289716

Epoch: 5| Step: 9
Training loss: 2.816612119729625
Validation loss: 2.5341619718534134

Epoch: 5| Step: 10
Training loss: 2.7771356561019087
Validation loss: 2.5557580492096608

Epoch: 154| Step: 0
Training loss: 2.6355276172387243
Validation loss: 2.5413223970139778

Epoch: 5| Step: 1
Training loss: 2.4683808642268614
Validation loss: 2.5414922404910287

Epoch: 5| Step: 2
Training loss: 2.856063843384849
Validation loss: 2.5418745756386834

Epoch: 5| Step: 3
Training loss: 2.7796176020307697
Validation loss: 2.5551893113893005

Epoch: 5| Step: 4
Training loss: 2.5636398757502232
Validation loss: 2.517753062236421

Epoch: 5| Step: 5
Training loss: 2.5412412714773107
Validation loss: 2.5390790122530955

Epoch: 5| Step: 6
Training loss: 2.886691164290133
Validation loss: 2.5683780595403984

Epoch: 5| Step: 7
Training loss: 2.9834936947763135
Validation loss: 2.5256229538639032

Epoch: 5| Step: 8
Training loss: 2.77072225553587
Validation loss: 2.527287977751095

Epoch: 5| Step: 9
Training loss: 2.470766041071456
Validation loss: 2.5337452087222676

Epoch: 5| Step: 10
Training loss: 2.8888428839256406
Validation loss: 2.525970334837896

Epoch: 155| Step: 0
Training loss: 2.8723626686780066
Validation loss: 2.511801521040927

Epoch: 5| Step: 1
Training loss: 2.820173349936817
Validation loss: 2.515560264725157

Epoch: 5| Step: 2
Training loss: 1.8780824754823056
Validation loss: 2.526840593768714

Epoch: 5| Step: 3
Training loss: 2.485555692132817
Validation loss: 2.5182236747114684

Epoch: 5| Step: 4
Training loss: 3.0657417111826395
Validation loss: 2.524658304034415

Epoch: 5| Step: 5
Training loss: 2.796486257463747
Validation loss: 2.5415799341141345

Epoch: 5| Step: 6
Training loss: 2.739401943066455
Validation loss: 2.499660645289511

Epoch: 5| Step: 7
Training loss: 2.6485282409786985
Validation loss: 2.530927198197745

Epoch: 5| Step: 8
Training loss: 2.878484852978755
Validation loss: 2.5143056008302653

Epoch: 5| Step: 9
Training loss: 2.867545206899246
Validation loss: 2.5223994084215198

Epoch: 5| Step: 10
Training loss: 2.4913672170515078
Validation loss: 2.528751435000062

Epoch: 156| Step: 0
Training loss: 2.6721069949080123
Validation loss: 2.52808361753008

Epoch: 5| Step: 1
Training loss: 2.252649442772649
Validation loss: 2.5485801854354357

Epoch: 5| Step: 2
Training loss: 2.255965906575465
Validation loss: 2.507967953583334

Epoch: 5| Step: 3
Training loss: 3.034591720494738
Validation loss: 2.521702847227283

Epoch: 5| Step: 4
Training loss: 2.8011150932594746
Validation loss: 2.5085568270794876

Epoch: 5| Step: 5
Training loss: 2.7674849123659806
Validation loss: 2.526011394847508

Epoch: 5| Step: 6
Training loss: 1.645522156604765
Validation loss: 2.5444810977141383

Epoch: 5| Step: 7
Training loss: 2.443595112528794
Validation loss: 2.53897373142103

Epoch: 5| Step: 8
Training loss: 3.0028906247459317
Validation loss: 2.5519078897355865

Epoch: 5| Step: 9
Training loss: 3.334369657274451
Validation loss: 2.517515913451954

Epoch: 5| Step: 10
Training loss: 3.21167993431604
Validation loss: 2.5481737908044395

Epoch: 157| Step: 0
Training loss: 3.1022757759803326
Validation loss: 2.540410543541016

Epoch: 5| Step: 1
Training loss: 2.9103921884223247
Validation loss: 2.5359769352001553

Epoch: 5| Step: 2
Training loss: 2.46908895362749
Validation loss: 2.5322392418373907

Epoch: 5| Step: 3
Training loss: 2.907018908438849
Validation loss: 2.5420356864542275

Epoch: 5| Step: 4
Training loss: 2.331791799958146
Validation loss: 2.5338712747182193

Epoch: 5| Step: 5
Training loss: 2.913803784542125
Validation loss: 2.534058707361028

Epoch: 5| Step: 6
Training loss: 2.4616719436675036
Validation loss: 2.5331747540007212

Epoch: 5| Step: 7
Training loss: 2.940526012412625
Validation loss: 2.5246966041338332

Epoch: 5| Step: 8
Training loss: 2.3016853045961874
Validation loss: 2.5231874352770025

Epoch: 5| Step: 9
Training loss: 2.456571939351011
Validation loss: 2.5221245754584363

Epoch: 5| Step: 10
Training loss: 2.767949393708341
Validation loss: 2.526302246990269

Epoch: 158| Step: 0
Training loss: 3.458803780664588
Validation loss: 2.5186831479758203

Epoch: 5| Step: 1
Training loss: 3.0163582814553864
Validation loss: 2.526329690040303

Epoch: 5| Step: 2
Training loss: 2.293732757919326
Validation loss: 2.539887176698677

Epoch: 5| Step: 3
Training loss: 2.5046934892817214
Validation loss: 2.5278051754915376

Epoch: 5| Step: 4
Training loss: 2.5419306591910122
Validation loss: 2.5437216282740014

Epoch: 5| Step: 5
Training loss: 2.1958856343574102
Validation loss: 2.5431781055879275

Epoch: 5| Step: 6
Training loss: 2.5898634505637395
Validation loss: 2.557009891686632

Epoch: 5| Step: 7
Training loss: 2.751097720142177
Validation loss: 2.512413902809481

Epoch: 5| Step: 8
Training loss: 3.073968084602742
Validation loss: 2.5215925363125495

Epoch: 5| Step: 9
Training loss: 2.5344856209842805
Validation loss: 2.5236014091919583

Epoch: 5| Step: 10
Training loss: 2.5831050977071723
Validation loss: 2.542770473532559

Epoch: 159| Step: 0
Training loss: 2.5190925157010673
Validation loss: 2.5249795852562005

Epoch: 5| Step: 1
Training loss: 2.9442920485431228
Validation loss: 2.54300802237149

Epoch: 5| Step: 2
Training loss: 2.6500240288850616
Validation loss: 2.517146990284412

Epoch: 5| Step: 3
Training loss: 2.641635317771855
Validation loss: 2.5293073249715365

Epoch: 5| Step: 4
Training loss: 2.738451028694882
Validation loss: 2.5261455332278238

Epoch: 5| Step: 5
Training loss: 2.913454866849568
Validation loss: 2.5159679080533963

Epoch: 5| Step: 6
Training loss: 3.2904778921314692
Validation loss: 2.530705122776947

Epoch: 5| Step: 7
Training loss: 2.562909628529178
Validation loss: 2.5585144606817303

Epoch: 5| Step: 8
Training loss: 2.644385554978492
Validation loss: 2.5234242928508333

Epoch: 5| Step: 9
Training loss: 2.2255196253901754
Validation loss: 2.523445349077596

Epoch: 5| Step: 10
Training loss: 2.4239657329932167
Validation loss: 2.5279455985277277

Epoch: 160| Step: 0
Training loss: 2.791394623743674
Validation loss: 2.538075141864998

Epoch: 5| Step: 1
Training loss: 2.7171873666281776
Validation loss: 2.538425196832513

Epoch: 5| Step: 2
Training loss: 3.292374128457062
Validation loss: 2.523935663827735

Epoch: 5| Step: 3
Training loss: 2.7696811221183917
Validation loss: 2.535446801830887

Epoch: 5| Step: 4
Training loss: 2.135666387579499
Validation loss: 2.538034882064494

Epoch: 5| Step: 5
Training loss: 2.3845385694082117
Validation loss: 2.5635358316080983

Epoch: 5| Step: 6
Training loss: 2.457956403725906
Validation loss: 2.5354846922536964

Epoch: 5| Step: 7
Training loss: 2.7412178955424693
Validation loss: 2.5391449047060046

Epoch: 5| Step: 8
Training loss: 2.705866487388245
Validation loss: 2.5320733039351144

Epoch: 5| Step: 9
Training loss: 2.9924389606519193
Validation loss: 2.5310460936103

Epoch: 5| Step: 10
Training loss: 2.6362889370455247
Validation loss: 2.5652307406168124

Epoch: 161| Step: 0
Training loss: 2.592052961632575
Validation loss: 2.52738992940765

Epoch: 5| Step: 1
Training loss: 2.9063855211388168
Validation loss: 2.5343289747383717

Epoch: 5| Step: 2
Training loss: 3.166356439619862
Validation loss: 2.522081660628731

Epoch: 5| Step: 3
Training loss: 2.9199413082024726
Validation loss: 2.5448395949006026

Epoch: 5| Step: 4
Training loss: 2.8355233386563654
Validation loss: 2.524963268133473

Epoch: 5| Step: 5
Training loss: 2.714331708962952
Validation loss: 2.5161479354706535

Epoch: 5| Step: 6
Training loss: 2.5638867789338224
Validation loss: 2.531487858214697

Epoch: 5| Step: 7
Training loss: 2.1485303824027406
Validation loss: 2.5584577546670255

Epoch: 5| Step: 8
Training loss: 2.0068228691028267
Validation loss: 2.5275802596737824

Epoch: 5| Step: 9
Training loss: 2.722702310574786
Validation loss: 2.5444768373706395

Epoch: 5| Step: 10
Training loss: 3.0988851757657248
Validation loss: 2.5238729548409764

Epoch: 162| Step: 0
Training loss: 2.7215612500019555
Validation loss: 2.519917167964777

Epoch: 5| Step: 1
Training loss: 2.8763275398634884
Validation loss: 2.5250455075515412

Epoch: 5| Step: 2
Training loss: 2.836130855473724
Validation loss: 2.548900700307034

Epoch: 5| Step: 3
Training loss: 2.507404900341992
Validation loss: 2.531318995807638

Epoch: 5| Step: 4
Training loss: 2.6181318259042077
Validation loss: 2.5214051012036367

Epoch: 5| Step: 5
Training loss: 2.4260352058592143
Validation loss: 2.5529028220748855

Epoch: 5| Step: 6
Training loss: 2.7936572100017263
Validation loss: 2.5413209070423264

Epoch: 5| Step: 7
Training loss: 2.5040050850106668
Validation loss: 2.513044777335885

Epoch: 5| Step: 8
Training loss: 2.601890382925134
Validation loss: 2.5497106924865167

Epoch: 5| Step: 9
Training loss: 2.7523453854575752
Validation loss: 2.5349959327160447

Epoch: 5| Step: 10
Training loss: 3.0654973523612745
Validation loss: 2.5452822902848737

Epoch: 163| Step: 0
Training loss: 2.11315477595637
Validation loss: 2.5161636459671675

Epoch: 5| Step: 1
Training loss: 2.852997217732546
Validation loss: 2.526408584041012

Epoch: 5| Step: 2
Training loss: 2.1278055407796455
Validation loss: 2.5612821238523362

Epoch: 5| Step: 3
Training loss: 2.9842188929128293
Validation loss: 2.541098360837166

Epoch: 5| Step: 4
Training loss: 3.0161465999176538
Validation loss: 2.5320946183004702

Epoch: 5| Step: 5
Training loss: 2.4283171588757875
Validation loss: 2.534751437199526

Epoch: 5| Step: 6
Training loss: 2.6949280395525776
Validation loss: 2.540859980660404

Epoch: 5| Step: 7
Training loss: 2.6639970788166423
Validation loss: 2.527699565056831

Epoch: 5| Step: 8
Training loss: 2.664400409288293
Validation loss: 2.545062996533323

Epoch: 5| Step: 9
Training loss: 3.1090857596509394
Validation loss: 2.541644445436823

Epoch: 5| Step: 10
Training loss: 2.7547758420387622
Validation loss: 2.518807088981091

Epoch: 164| Step: 0
Training loss: 2.637745025968485
Validation loss: 2.5179097968982216

Epoch: 5| Step: 1
Training loss: 2.7929165921976438
Validation loss: 2.5379974686042326

Epoch: 5| Step: 2
Training loss: 2.219733758890391
Validation loss: 2.542549111908351

Epoch: 5| Step: 3
Training loss: 2.400345233087647
Validation loss: 2.5118058179241616

Epoch: 5| Step: 4
Training loss: 2.97252840225857
Validation loss: 2.534176651610243

Epoch: 5| Step: 5
Training loss: 2.896145554504465
Validation loss: 2.528403373519988

Epoch: 5| Step: 6
Training loss: 2.8412714626510254
Validation loss: 2.5422412989979133

Epoch: 5| Step: 7
Training loss: 2.937180197331563
Validation loss: 2.531123557326186

Epoch: 5| Step: 8
Training loss: 2.8881592827722598
Validation loss: 2.5166365746835053

Epoch: 5| Step: 9
Training loss: 2.584982612152944
Validation loss: 2.5317547840604977

Epoch: 5| Step: 10
Training loss: 2.1419183763754135
Validation loss: 2.532805038706044

Epoch: 165| Step: 0
Training loss: 2.496718541902156
Validation loss: 2.523241106627125

Epoch: 5| Step: 1
Training loss: 3.1948405992267865
Validation loss: 2.5232509537861976

Epoch: 5| Step: 2
Training loss: 2.0539514652004134
Validation loss: 2.5356446601577267

Epoch: 5| Step: 3
Training loss: 3.2172357645439424
Validation loss: 2.511579185577125

Epoch: 5| Step: 4
Training loss: 2.818219028713867
Validation loss: 2.5308664026934498

Epoch: 5| Step: 5
Training loss: 2.5497571866984914
Validation loss: 2.530775899074694

Epoch: 5| Step: 6
Training loss: 2.633075282172797
Validation loss: 2.5273833077669616

Epoch: 5| Step: 7
Training loss: 2.6592904136823754
Validation loss: 2.5196389243579693

Epoch: 5| Step: 8
Training loss: 2.1979946707191456
Validation loss: 2.5235973386139845

Epoch: 5| Step: 9
Training loss: 2.1924380560968597
Validation loss: 2.5153211153910386

Epoch: 5| Step: 10
Training loss: 3.531035020073899
Validation loss: 2.531860234454513

Epoch: 166| Step: 0
Training loss: 2.265542285330082
Validation loss: 2.53164612123956

Epoch: 5| Step: 1
Training loss: 3.0934204542563437
Validation loss: 2.5374516594711043

Epoch: 5| Step: 2
Training loss: 2.8754917221839023
Validation loss: 2.5376439223307554

Epoch: 5| Step: 3
Training loss: 3.0439178984468933
Validation loss: 2.5531932149389944

Epoch: 5| Step: 4
Training loss: 2.4259839057734447
Validation loss: 2.541510123427039

Epoch: 5| Step: 5
Training loss: 2.6915035147178927
Validation loss: 2.5382785391429246

Epoch: 5| Step: 6
Training loss: 2.7735287933367956
Validation loss: 2.540963329944771

Epoch: 5| Step: 7
Training loss: 2.3287347596714616
Validation loss: 2.5443721753618247

Epoch: 5| Step: 8
Training loss: 2.668190361861182
Validation loss: 2.541648896624415

Epoch: 5| Step: 9
Training loss: 2.3610888187128993
Validation loss: 2.514684629895684

Epoch: 5| Step: 10
Training loss: 2.929168411044546
Validation loss: 2.542383426430667

Epoch: 167| Step: 0
Training loss: 2.8145080179848097
Validation loss: 2.5282439249393693

Epoch: 5| Step: 1
Training loss: 2.605171243819731
Validation loss: 2.528980977624344

Epoch: 5| Step: 2
Training loss: 3.4593820628685945
Validation loss: 2.5288209612567907

Epoch: 5| Step: 3
Training loss: 2.84560962279899
Validation loss: 2.553382970212389

Epoch: 5| Step: 4
Training loss: 3.1167253424141155
Validation loss: 2.5287287299003673

Epoch: 5| Step: 5
Training loss: 1.8556961080893615
Validation loss: 2.5265057645631277

Epoch: 5| Step: 6
Training loss: 1.9777494342192197
Validation loss: 2.5006129385082128

Epoch: 5| Step: 7
Training loss: 3.0608575270108833
Validation loss: 2.532127235452364

Epoch: 5| Step: 8
Training loss: 2.4534174264959545
Validation loss: 2.5311285638234358

Epoch: 5| Step: 9
Training loss: 2.4660242219216437
Validation loss: 2.533075825947327

Epoch: 5| Step: 10
Training loss: 2.3745275328649056
Validation loss: 2.5126631389368277

Epoch: 168| Step: 0
Training loss: 3.1734775948257203
Validation loss: 2.538307617651693

Epoch: 5| Step: 1
Training loss: 2.6590310292293124
Validation loss: 2.5562421600264096

Epoch: 5| Step: 2
Training loss: 2.309358989572324
Validation loss: 2.5365886551575785

Epoch: 5| Step: 3
Training loss: 2.2424343916907796
Validation loss: 2.535483501172012

Epoch: 5| Step: 4
Training loss: 2.6758881707652518
Validation loss: 2.5354318574351957

Epoch: 5| Step: 5
Training loss: 3.1287596402722726
Validation loss: 2.5278634189776152

Epoch: 5| Step: 6
Training loss: 3.0538085297305204
Validation loss: 2.5490822497170385

Epoch: 5| Step: 7
Training loss: 2.8233512928589284
Validation loss: 2.5146672387300852

Epoch: 5| Step: 8
Training loss: 2.050848096938808
Validation loss: 2.5272429468127986

Epoch: 5| Step: 9
Training loss: 2.731943673532008
Validation loss: 2.5353933716718497

Epoch: 5| Step: 10
Training loss: 2.6631098411502845
Validation loss: 2.5250969048124237

Epoch: 169| Step: 0
Training loss: 2.2899600407808
Validation loss: 2.5342624492488817

Epoch: 5| Step: 1
Training loss: 2.5359443180276973
Validation loss: 2.5358083444295887

Epoch: 5| Step: 2
Training loss: 2.4580081034900005
Validation loss: 2.533173971703916

Epoch: 5| Step: 3
Training loss: 2.537920604104237
Validation loss: 2.5162638583784656

Epoch: 5| Step: 4
Training loss: 2.9622889833907435
Validation loss: 2.5229356183412732

Epoch: 5| Step: 5
Training loss: 2.797442075046153
Validation loss: 2.5396688202135147

Epoch: 5| Step: 6
Training loss: 3.0880192555533807
Validation loss: 2.5517748996442933

Epoch: 5| Step: 7
Training loss: 2.4273971114663895
Validation loss: 2.5396940934108363

Epoch: 5| Step: 8
Training loss: 2.369191344520979
Validation loss: 2.5546484312605147

Epoch: 5| Step: 9
Training loss: 2.8846687605394536
Validation loss: 2.544423389780574

Epoch: 5| Step: 10
Training loss: 3.175784853549616
Validation loss: 2.535441106186273

Epoch: 170| Step: 0
Training loss: 2.6945948433166915
Validation loss: 2.5363580089635467

Epoch: 5| Step: 1
Training loss: 2.747396016535218
Validation loss: 2.5316469455266195

Epoch: 5| Step: 2
Training loss: 2.797816794613819
Validation loss: 2.5390250436246076

Epoch: 5| Step: 3
Training loss: 2.766843365767813
Validation loss: 2.5322544257767046

Epoch: 5| Step: 4
Training loss: 2.743926798136031
Validation loss: 2.5284306847678404

Epoch: 5| Step: 5
Training loss: 2.8129358589679003
Validation loss: 2.530518308942289

Epoch: 5| Step: 6
Training loss: 2.828256677755064
Validation loss: 2.5166033839197537

Epoch: 5| Step: 7
Training loss: 2.54528478112065
Validation loss: 2.5289224202776888

Epoch: 5| Step: 8
Training loss: 2.6272246606875114
Validation loss: 2.5419605541799206

Epoch: 5| Step: 9
Training loss: 2.598506105839696
Validation loss: 2.516998983797597

Epoch: 5| Step: 10
Training loss: 2.1951494801224327
Validation loss: 2.5577964734754866

Epoch: 171| Step: 0
Training loss: 2.450657277593936
Validation loss: 2.526307705992617

Epoch: 5| Step: 1
Training loss: 2.6898103586163513
Validation loss: 2.5508981426762047

Epoch: 5| Step: 2
Training loss: 2.018013891408572
Validation loss: 2.5597063709111025

Epoch: 5| Step: 3
Training loss: 2.5965705151399208
Validation loss: 2.5291494599184596

Epoch: 5| Step: 4
Training loss: 3.1288163342456494
Validation loss: 2.517242898868213

Epoch: 5| Step: 5
Training loss: 2.699426314547841
Validation loss: 2.532716080164861

Epoch: 5| Step: 6
Training loss: 3.1535097823129634
Validation loss: 2.5341606951728646

Epoch: 5| Step: 7
Training loss: 2.3649270120209374
Validation loss: 2.54403396554952

Epoch: 5| Step: 8
Training loss: 3.026657715866813
Validation loss: 2.5591729347808134

Epoch: 5| Step: 9
Training loss: 2.836305032249161
Validation loss: 2.5371404019114903

Epoch: 5| Step: 10
Training loss: 2.166977004298431
Validation loss: 2.5336078047284682

Epoch: 172| Step: 0
Training loss: 2.497019898909361
Validation loss: 2.530150850025423

Epoch: 5| Step: 1
Training loss: 2.127015336232926
Validation loss: 2.532685183277255

Epoch: 5| Step: 2
Training loss: 2.664037441460227
Validation loss: 2.5407203904536346

Epoch: 5| Step: 3
Training loss: 2.7929247872687313
Validation loss: 2.5527670919760554

Epoch: 5| Step: 4
Training loss: 2.8323373352910823
Validation loss: 2.5400177512139246

Epoch: 5| Step: 5
Training loss: 2.7665790701024884
Validation loss: 2.540744399998692

Epoch: 5| Step: 6
Training loss: 2.915745798697491
Validation loss: 2.5396051985171475

Epoch: 5| Step: 7
Training loss: 2.473551848630582
Validation loss: 2.5417739533512917

Epoch: 5| Step: 8
Training loss: 2.763235493098593
Validation loss: 2.5459732461369513

Epoch: 5| Step: 9
Training loss: 2.824944831512011
Validation loss: 2.550914086374262

Epoch: 5| Step: 10
Training loss: 2.754639353469322
Validation loss: 2.5353378563800777

Epoch: 173| Step: 0
Training loss: 2.615311864341634
Validation loss: 2.533989786150618

Epoch: 5| Step: 1
Training loss: 3.135352420254274
Validation loss: 2.5436940810895394

Epoch: 5| Step: 2
Training loss: 2.695592008831487
Validation loss: 2.515085410997173

Epoch: 5| Step: 3
Training loss: 2.490081471255507
Validation loss: 2.5503971867455593

Epoch: 5| Step: 4
Training loss: 2.6563005330383964
Validation loss: 2.5354609241027526

Epoch: 5| Step: 5
Training loss: 2.450509785145182
Validation loss: 2.5394951625120727

Epoch: 5| Step: 6
Training loss: 2.2918832734361487
Validation loss: 2.5405482698301145

Epoch: 5| Step: 7
Training loss: 3.1008003124709314
Validation loss: 2.5516671948734353

Epoch: 5| Step: 8
Training loss: 2.5854568265203226
Validation loss: 2.546931712258177

Epoch: 5| Step: 9
Training loss: 2.8908310275500697
Validation loss: 2.5251996687847904

Epoch: 5| Step: 10
Training loss: 2.631989845772261
Validation loss: 2.491141721014953

Epoch: 174| Step: 0
Training loss: 2.4553781840426057
Validation loss: 2.5272495738932435

Epoch: 5| Step: 1
Training loss: 2.781612929763563
Validation loss: 2.538448999843499

Epoch: 5| Step: 2
Training loss: 2.4813321270551145
Validation loss: 2.543635165012604

Epoch: 5| Step: 3
Training loss: 2.5915245712266524
Validation loss: 2.5231459188282637

Epoch: 5| Step: 4
Training loss: 2.2304832872227665
Validation loss: 2.519625769538054

Epoch: 5| Step: 5
Training loss: 2.8832539372914656
Validation loss: 2.5342233129656844

Epoch: 5| Step: 6
Training loss: 2.45524088002189
Validation loss: 2.5501806298932497

Epoch: 5| Step: 7
Training loss: 3.070330204803438
Validation loss: 2.531375012437108

Epoch: 5| Step: 8
Training loss: 2.9315581592852586
Validation loss: 2.544435859222134

Epoch: 5| Step: 9
Training loss: 2.0933921849832453
Validation loss: 2.5310437437324964

Epoch: 5| Step: 10
Training loss: 3.4078287088392605
Validation loss: 2.533714585431457

Epoch: 175| Step: 0
Training loss: 2.325649507101793
Validation loss: 2.5419597201263984

Epoch: 5| Step: 1
Training loss: 2.690555964170191
Validation loss: 2.5539760376002247

Epoch: 5| Step: 2
Training loss: 3.024560212252198
Validation loss: 2.5203626124211285

Epoch: 5| Step: 3
Training loss: 3.071685095938918
Validation loss: 2.546137989174137

Epoch: 5| Step: 4
Training loss: 2.190217428344239
Validation loss: 2.5514182822472975

Epoch: 5| Step: 5
Training loss: 2.7210337375678875
Validation loss: 2.521474760888445

Epoch: 5| Step: 6
Training loss: 2.9929747977478214
Validation loss: 2.5252702400062232

Epoch: 5| Step: 7
Training loss: 2.6692018577533774
Validation loss: 2.542483120133017

Epoch: 5| Step: 8
Training loss: 2.666474712933606
Validation loss: 2.548962839832008

Epoch: 5| Step: 9
Training loss: 2.6558471374184895
Validation loss: 2.5465146349655567

Epoch: 5| Step: 10
Training loss: 2.341405076951658
Validation loss: 2.5402695137389064

Epoch: 176| Step: 0
Training loss: 2.9199202419518846
Validation loss: 2.574598393402116

Epoch: 5| Step: 1
Training loss: 2.5482606453401417
Validation loss: 2.529340866959962

Epoch: 5| Step: 2
Training loss: 3.097653479171995
Validation loss: 2.5425045478849415

Epoch: 5| Step: 3
Training loss: 2.7851033466552533
Validation loss: 2.514900565366293

Epoch: 5| Step: 4
Training loss: 2.6897848308402406
Validation loss: 2.5453260341635398

Epoch: 5| Step: 5
Training loss: 3.18192102216513
Validation loss: 2.556778493742864

Epoch: 5| Step: 6
Training loss: 2.5023389365453332
Validation loss: 2.535787882198708

Epoch: 5| Step: 7
Training loss: 2.110077239516619
Validation loss: 2.5423581497164633

Epoch: 5| Step: 8
Training loss: 2.745255017967908
Validation loss: 2.5404298514241477

Epoch: 5| Step: 9
Training loss: 2.445470250315514
Validation loss: 2.540456478361819

Epoch: 5| Step: 10
Training loss: 2.2321890428397038
Validation loss: 2.5676187172930285

Epoch: 177| Step: 0
Training loss: 2.4233893797356276
Validation loss: 2.533205499187073

Epoch: 5| Step: 1
Training loss: 2.0677356734177876
Validation loss: 2.524755443278703

Epoch: 5| Step: 2
Training loss: 3.036472972662279
Validation loss: 2.5724079860944524

Epoch: 5| Step: 3
Training loss: 2.7915750934517902
Validation loss: 2.53880618787135

Epoch: 5| Step: 4
Training loss: 2.7019263459903
Validation loss: 2.5474048825521054

Epoch: 5| Step: 5
Training loss: 2.4541342050405675
Validation loss: 2.545947320337607

Epoch: 5| Step: 6
Training loss: 2.618238186951921
Validation loss: 2.5397464307510127

Epoch: 5| Step: 7
Training loss: 2.6224972737809984
Validation loss: 2.518147491940307

Epoch: 5| Step: 8
Training loss: 2.76542059363904
Validation loss: 2.5245980226967935

Epoch: 5| Step: 9
Training loss: 3.3282930573490885
Validation loss: 2.5264323977801144

Epoch: 5| Step: 10
Training loss: 2.4211483757501235
Validation loss: 2.513683435299525

Epoch: 178| Step: 0
Training loss: 2.8956018442256877
Validation loss: 2.5291307764837803

Epoch: 5| Step: 1
Training loss: 2.7223823223480577
Validation loss: 2.525277200170603

Epoch: 5| Step: 2
Training loss: 2.6804181317914884
Validation loss: 2.5575211490522527

Epoch: 5| Step: 3
Training loss: 2.603629451017711
Validation loss: 2.5330352534032174

Epoch: 5| Step: 4
Training loss: 2.3243044028006876
Validation loss: 2.538028358397526

Epoch: 5| Step: 5
Training loss: 2.35380715744949
Validation loss: 2.5216166110667286

Epoch: 5| Step: 6
Training loss: 2.680512059528111
Validation loss: 2.5273656905477595

Epoch: 5| Step: 7
Training loss: 2.9673921390975164
Validation loss: 2.5447294930796067

Epoch: 5| Step: 8
Training loss: 3.068832390139535
Validation loss: 2.530356959220758

Epoch: 5| Step: 9
Training loss: 2.2701715560818236
Validation loss: 2.5294174626134733

Epoch: 5| Step: 10
Training loss: 2.860664785523938
Validation loss: 2.53565484789159

Epoch: 179| Step: 0
Training loss: 3.1551265416711134
Validation loss: 2.5390105009473265

Epoch: 5| Step: 1
Training loss: 2.450897079809573
Validation loss: 2.5300974412707773

Epoch: 5| Step: 2
Training loss: 2.7934481122721846
Validation loss: 2.523069574777292

Epoch: 5| Step: 3
Training loss: 2.759883977966896
Validation loss: 2.552339376664822

Epoch: 5| Step: 4
Training loss: 2.7688599540973633
Validation loss: 2.5329241496108756

Epoch: 5| Step: 5
Training loss: 2.528120673214825
Validation loss: 2.518451101675695

Epoch: 5| Step: 6
Training loss: 2.574033692904868
Validation loss: 2.536864420246882

Epoch: 5| Step: 7
Training loss: 2.3772068812910496
Validation loss: 2.5365626263651126

Epoch: 5| Step: 8
Training loss: 3.192996503178718
Validation loss: 2.5199698651861255

Epoch: 5| Step: 9
Training loss: 2.5971497464961657
Validation loss: 2.5326720688900877

Epoch: 5| Step: 10
Training loss: 2.0339266234584366
Validation loss: 2.545872915744446

Epoch: 180| Step: 0
Training loss: 2.438322662031177
Validation loss: 2.521195484230705

Epoch: 5| Step: 1
Training loss: 2.224363184461004
Validation loss: 2.560903218263686

Epoch: 5| Step: 2
Training loss: 2.8433319455225763
Validation loss: 2.5340430678485073

Epoch: 5| Step: 3
Training loss: 2.359860805997176
Validation loss: 2.5298071501068815

Epoch: 5| Step: 4
Training loss: 3.1492293429356235
Validation loss: 2.520484478808369

Epoch: 5| Step: 5
Training loss: 2.8228127341944456
Validation loss: 2.566546088330781

Epoch: 5| Step: 6
Training loss: 3.2852018264711553
Validation loss: 2.535291969373518

Epoch: 5| Step: 7
Training loss: 2.3976203445923163
Validation loss: 2.527434137138066

Epoch: 5| Step: 8
Training loss: 2.457756384265327
Validation loss: 2.524557011798931

Epoch: 5| Step: 9
Training loss: 2.5981242052361044
Validation loss: 2.52800463383033

Epoch: 5| Step: 10
Training loss: 2.5869368267389774
Validation loss: 2.5306268206204647

Epoch: 181| Step: 0
Training loss: 2.506809212185891
Validation loss: 2.542377936906726

Epoch: 5| Step: 1
Training loss: 2.6368747241193433
Validation loss: 2.5256492506997317

Epoch: 5| Step: 2
Training loss: 2.4966895119301347
Validation loss: 2.533609742427717

Epoch: 5| Step: 3
Training loss: 2.136732697659288
Validation loss: 2.5206624170334626

Epoch: 5| Step: 4
Training loss: 2.670617594067004
Validation loss: 2.5409222431725103

Epoch: 5| Step: 5
Training loss: 3.484911137546649
Validation loss: 2.5275073693564822

Epoch: 5| Step: 6
Training loss: 2.0468228166058062
Validation loss: 2.5235175033683324

Epoch: 5| Step: 7
Training loss: 2.745391973084438
Validation loss: 2.5313610922011693

Epoch: 5| Step: 8
Training loss: 2.511150194902942
Validation loss: 2.518174644635284

Epoch: 5| Step: 9
Training loss: 2.858810952841568
Validation loss: 2.5130951448921786

Epoch: 5| Step: 10
Training loss: 3.1405727704767736
Validation loss: 2.5377435395683845

Epoch: 182| Step: 0
Training loss: 2.7635724915876128
Validation loss: 2.5348227063276805

Epoch: 5| Step: 1
Training loss: 2.336401851774663
Validation loss: 2.5480140873139057

Epoch: 5| Step: 2
Training loss: 3.189343386909604
Validation loss: 2.543009578394151

Epoch: 5| Step: 3
Training loss: 2.038318717615103
Validation loss: 2.551334553697731

Epoch: 5| Step: 4
Training loss: 3.65706240765607
Validation loss: 2.5628919084151507

Epoch: 5| Step: 5
Training loss: 2.084256755900727
Validation loss: 2.5601469211436627

Epoch: 5| Step: 6
Training loss: 2.780224385982825
Validation loss: 2.5475746198939344

Epoch: 5| Step: 7
Training loss: 2.8589618988967413
Validation loss: 2.521298765638352

Epoch: 5| Step: 8
Training loss: 2.3412198460597904
Validation loss: 2.5400047927745337

Epoch: 5| Step: 9
Training loss: 2.431884753370175
Validation loss: 2.5208739587013214

Epoch: 5| Step: 10
Training loss: 2.5773984867977466
Validation loss: 2.5480524699863074

Epoch: 183| Step: 0
Training loss: 2.788048396884961
Validation loss: 2.5397457504098577

Epoch: 5| Step: 1
Training loss: 3.1200413845447583
Validation loss: 2.552827748027864

Epoch: 5| Step: 2
Training loss: 3.030745946131123
Validation loss: 2.5232190489326296

Epoch: 5| Step: 3
Training loss: 2.752155932716135
Validation loss: 2.5258104007675772

Epoch: 5| Step: 4
Training loss: 1.9578534681413247
Validation loss: 2.526655265760089

Epoch: 5| Step: 5
Training loss: 2.7197827537114936
Validation loss: 2.5589130166409166

Epoch: 5| Step: 6
Training loss: 2.4477339872550044
Validation loss: 2.5371807062184075

Epoch: 5| Step: 7
Training loss: 2.326524120088541
Validation loss: 2.5413898382487963

Epoch: 5| Step: 8
Training loss: 2.57432247989034
Validation loss: 2.5329139635453997

Epoch: 5| Step: 9
Training loss: 2.7939142507448462
Validation loss: 2.530533223596761

Epoch: 5| Step: 10
Training loss: 2.634692960943192
Validation loss: 2.5369636499920087

Epoch: 184| Step: 0
Training loss: 3.084385426048583
Validation loss: 2.5379863468330672

Epoch: 5| Step: 1
Training loss: 2.894476391971301
Validation loss: 2.548283044621781

Epoch: 5| Step: 2
Training loss: 2.186336207754009
Validation loss: 2.536490327703585

Epoch: 5| Step: 3
Training loss: 1.9867295841307153
Validation loss: 2.530907872518816

Epoch: 5| Step: 4
Training loss: 2.4561152608087107
Validation loss: 2.5386422343817365

Epoch: 5| Step: 5
Training loss: 2.7287499689651744
Validation loss: 2.5341302630456513

Epoch: 5| Step: 6
Training loss: 3.0811093188005216
Validation loss: 2.514748728132397

Epoch: 5| Step: 7
Training loss: 2.2141570787816844
Validation loss: 2.524446959652202

Epoch: 5| Step: 8
Training loss: 2.8964417365055337
Validation loss: 2.5477452550645188

Epoch: 5| Step: 9
Training loss: 2.7159350500248287
Validation loss: 2.5254287962380255

Epoch: 5| Step: 10
Training loss: 2.8498898133510684
Validation loss: 2.506387095854261

Epoch: 185| Step: 0
Training loss: 2.260640732402231
Validation loss: 2.5161401120255587

Epoch: 5| Step: 1
Training loss: 3.092006924422827
Validation loss: 2.5354881027077507

Epoch: 5| Step: 2
Training loss: 1.958227587611203
Validation loss: 2.543472436993499

Epoch: 5| Step: 3
Training loss: 2.6615076427532727
Validation loss: 2.5390046446766146

Epoch: 5| Step: 4
Training loss: 2.641690011227184
Validation loss: 2.5302205080475635

Epoch: 5| Step: 5
Training loss: 3.151878426584185
Validation loss: 2.543802916580153

Epoch: 5| Step: 6
Training loss: 2.522343259540901
Validation loss: 2.543328569146204

Epoch: 5| Step: 7
Training loss: 2.4576633531796532
Validation loss: 2.5270577872950057

Epoch: 5| Step: 8
Training loss: 2.518779979745104
Validation loss: 2.5193002327640595

Epoch: 5| Step: 9
Training loss: 3.260804922043639
Validation loss: 2.52674936357226

Epoch: 5| Step: 10
Training loss: 2.60564892101585
Validation loss: 2.542037029774639

Epoch: 186| Step: 0
Training loss: 2.64012439203533
Validation loss: 2.5275692102275693

Epoch: 5| Step: 1
Training loss: 2.1838963935227333
Validation loss: 2.5477063389771306

Epoch: 5| Step: 2
Training loss: 2.194085915433345
Validation loss: 2.504827888572814

Epoch: 5| Step: 3
Training loss: 3.071504706012611
Validation loss: 2.5523082829061043

Epoch: 5| Step: 4
Training loss: 3.0280489979940763
Validation loss: 2.51385403099074

Epoch: 5| Step: 5
Training loss: 2.5712404749966953
Validation loss: 2.533971255762759

Epoch: 5| Step: 6
Training loss: 2.9805570616831374
Validation loss: 2.5093700228824756

Epoch: 5| Step: 7
Training loss: 2.1556296078042907
Validation loss: 2.5480315527139177

Epoch: 5| Step: 8
Training loss: 2.5975033363710036
Validation loss: 2.5510960165655483

Epoch: 5| Step: 9
Training loss: 2.6287083544223506
Validation loss: 2.5112080069504756

Epoch: 5| Step: 10
Training loss: 3.2485966220065254
Validation loss: 2.52852506427219

Epoch: 187| Step: 0
Training loss: 2.493062980685663
Validation loss: 2.533287452817685

Epoch: 5| Step: 1
Training loss: 2.8612924619151228
Validation loss: 2.531218029870086

Epoch: 5| Step: 2
Training loss: 2.077936379039771
Validation loss: 2.5283383327084303

Epoch: 5| Step: 3
Training loss: 2.861217967962747
Validation loss: 2.5402094353374167

Epoch: 5| Step: 4
Training loss: 2.597515268730689
Validation loss: 2.5539185773384294

Epoch: 5| Step: 5
Training loss: 2.3365483840775165
Validation loss: 2.5512616888939315

Epoch: 5| Step: 6
Training loss: 2.462403747691326
Validation loss: 2.5190602447285833

Epoch: 5| Step: 7
Training loss: 2.2138279868055637
Validation loss: 2.5559571220822357

Epoch: 5| Step: 8
Training loss: 3.190131503656255
Validation loss: 2.5199110384166272

Epoch: 5| Step: 9
Training loss: 2.7489485898169845
Validation loss: 2.5390574919822404

Epoch: 5| Step: 10
Training loss: 3.5142708438022576
Validation loss: 2.5307490264385697

Epoch: 188| Step: 0
Training loss: 2.513160019151258
Validation loss: 2.5367844426650605

Epoch: 5| Step: 1
Training loss: 2.3314565081318377
Validation loss: 2.5294049961630005

Epoch: 5| Step: 2
Training loss: 2.505584582799491
Validation loss: 2.529543070329308

Epoch: 5| Step: 3
Training loss: 2.678464204367792
Validation loss: 2.5342768107855043

Epoch: 5| Step: 4
Training loss: 2.1906234240560263
Validation loss: 2.5287064960208356

Epoch: 5| Step: 5
Training loss: 2.7258810596336613
Validation loss: 2.533090537806126

Epoch: 5| Step: 6
Training loss: 2.6482010907460953
Validation loss: 2.5441928391889417

Epoch: 5| Step: 7
Training loss: 2.8507764595139937
Validation loss: 2.5429314096344724

Epoch: 5| Step: 8
Training loss: 2.9800721150831553
Validation loss: 2.5480108868033495

Epoch: 5| Step: 9
Training loss: 3.6073430935157718
Validation loss: 2.5320530950133566

Epoch: 5| Step: 10
Training loss: 1.9188268416236458
Validation loss: 2.5245756082409305

Epoch: 189| Step: 0
Training loss: 2.535616182006818
Validation loss: 2.5204032658891027

Epoch: 5| Step: 1
Training loss: 2.9916929628142728
Validation loss: 2.523642163371358

Epoch: 5| Step: 2
Training loss: 3.0072953213038676
Validation loss: 2.5239468734142676

Epoch: 5| Step: 3
Training loss: 2.800184880692407
Validation loss: 2.5093129234894076

Epoch: 5| Step: 4
Training loss: 2.7049950982858526
Validation loss: 2.5394414662148477

Epoch: 5| Step: 5
Training loss: 2.20451678960839
Validation loss: 2.526133472807061

Epoch: 5| Step: 6
Training loss: 2.193935845048232
Validation loss: 2.5406073250988417

Epoch: 5| Step: 7
Training loss: 2.421217896933515
Validation loss: 2.52906550298866

Epoch: 5| Step: 8
Training loss: 2.9389652899282144
Validation loss: 2.5232648578037047

Epoch: 5| Step: 9
Training loss: 2.420350905742283
Validation loss: 2.530087498660365

Epoch: 5| Step: 10
Training loss: 2.912069748954911
Validation loss: 2.5402077559879235

Epoch: 190| Step: 0
Training loss: 2.6412715684594135
Validation loss: 2.5453208410588357

Epoch: 5| Step: 1
Training loss: 3.02081342668661
Validation loss: 2.5537559785609734

Epoch: 5| Step: 2
Training loss: 2.2726041439688363
Validation loss: 2.5176591515739

Epoch: 5| Step: 3
Training loss: 3.272348008673308
Validation loss: 2.54433462989314

Epoch: 5| Step: 4
Training loss: 2.5574350310395535
Validation loss: 2.5211191604326

Epoch: 5| Step: 5
Training loss: 2.6502577566352343
Validation loss: 2.547325438193317

Epoch: 5| Step: 6
Training loss: 2.5096896267676363
Validation loss: 2.527485081117726

Epoch: 5| Step: 7
Training loss: 2.1383879172577815
Validation loss: 2.527787703705299

Epoch: 5| Step: 8
Training loss: 2.499739442598726
Validation loss: 2.53996204732049

Epoch: 5| Step: 9
Training loss: 2.688669792904949
Validation loss: 2.533289260216422

Epoch: 5| Step: 10
Training loss: 2.9817650052253812
Validation loss: 2.533331460485214

Epoch: 191| Step: 0
Training loss: 2.5432198161752027
Validation loss: 2.530058050065627

Epoch: 5| Step: 1
Training loss: 3.720310941437095
Validation loss: 2.5358615889281952

Epoch: 5| Step: 2
Training loss: 2.3611368838630216
Validation loss: 2.546369669447409

Epoch: 5| Step: 3
Training loss: 2.6613134256368784
Validation loss: 2.5289617090464978

Epoch: 5| Step: 4
Training loss: 2.3761139315316715
Validation loss: 2.5202079681339606

Epoch: 5| Step: 5
Training loss: 2.694807630053467
Validation loss: 2.518092085118542

Epoch: 5| Step: 6
Training loss: 2.4983855756811693
Validation loss: 2.5257055179236736

Epoch: 5| Step: 7
Training loss: 2.393974799613923
Validation loss: 2.532634124241058

Epoch: 5| Step: 8
Training loss: 2.637416718962495
Validation loss: 2.5242877161458495

Epoch: 5| Step: 9
Training loss: 2.6439126219457654
Validation loss: 2.5326283493893493

Epoch: 5| Step: 10
Training loss: 2.469251364487907
Validation loss: 2.534074226388592

Epoch: 192| Step: 0
Training loss: 2.772183674824836
Validation loss: 2.5588620581617425

Epoch: 5| Step: 1
Training loss: 3.2098385306178816
Validation loss: 2.525172802797175

Epoch: 5| Step: 2
Training loss: 2.9846978936600026
Validation loss: 2.539659311280774

Epoch: 5| Step: 3
Training loss: 2.9771622181276713
Validation loss: 2.540082948147581

Epoch: 5| Step: 4
Training loss: 2.826672766007135
Validation loss: 2.533147600165224

Epoch: 5| Step: 5
Training loss: 2.6032413415452518
Validation loss: 2.5357790057448315

Epoch: 5| Step: 6
Training loss: 2.542766510265211
Validation loss: 2.537662025846349

Epoch: 5| Step: 7
Training loss: 2.1744469432765556
Validation loss: 2.531322702031963

Epoch: 5| Step: 8
Training loss: 2.613187277890472
Validation loss: 2.5292471483557337

Epoch: 5| Step: 9
Training loss: 2.0811445054819604
Validation loss: 2.536292790065216

Epoch: 5| Step: 10
Training loss: 2.3390896048341885
Validation loss: 2.5348394383581248

Epoch: 193| Step: 0
Training loss: 2.5844604330460292
Validation loss: 2.555802668013606

Epoch: 5| Step: 1
Training loss: 2.0402780468638104
Validation loss: 2.522323297903592

Epoch: 5| Step: 2
Training loss: 2.297991422911093
Validation loss: 2.545247011408522

Epoch: 5| Step: 3
Training loss: 3.2442050034062238
Validation loss: 2.520322957684175

Epoch: 5| Step: 4
Training loss: 2.8235681399092263
Validation loss: 2.544203516162305

Epoch: 5| Step: 5
Training loss: 2.469745869710021
Validation loss: 2.5284499563785494

Epoch: 5| Step: 6
Training loss: 2.681438885773537
Validation loss: 2.524518939083986

Epoch: 5| Step: 7
Training loss: 2.466631982748515
Validation loss: 2.550557918024718

Epoch: 5| Step: 8
Training loss: 2.9542910579718447
Validation loss: 2.5323150350895993

Epoch: 5| Step: 9
Training loss: 2.7561547156924076
Validation loss: 2.5236047767851946

Epoch: 5| Step: 10
Training loss: 2.743946261327377
Validation loss: 2.5322880826105787

Epoch: 194| Step: 0
Training loss: 3.3493534090428545
Validation loss: 2.5086955929240067

Epoch: 5| Step: 1
Training loss: 2.506887960692907
Validation loss: 2.56534679600624

Epoch: 5| Step: 2
Training loss: 2.3094968110661482
Validation loss: 2.543255881199441

Epoch: 5| Step: 3
Training loss: 2.8448690371547882
Validation loss: 2.513772708393756

Epoch: 5| Step: 4
Training loss: 2.7693818002254442
Validation loss: 2.527995238208855

Epoch: 5| Step: 5
Training loss: 2.863283581492059
Validation loss: 2.5402409289901184

Epoch: 5| Step: 6
Training loss: 2.4736023549743535
Validation loss: 2.55158645350074

Epoch: 5| Step: 7
Training loss: 2.5428707730921443
Validation loss: 2.5464175415092756

Epoch: 5| Step: 8
Training loss: 2.455872473204742
Validation loss: 2.540788938390455

Epoch: 5| Step: 9
Training loss: 2.712449589274062
Validation loss: 2.534284138202943

Epoch: 5| Step: 10
Training loss: 2.31013584431114
Validation loss: 2.512530310222523

Epoch: 195| Step: 0
Training loss: 2.6276807266640554
Validation loss: 2.53562503476552

Epoch: 5| Step: 1
Training loss: 2.6959761977234526
Validation loss: 2.5368328482958984

Epoch: 5| Step: 2
Training loss: 2.503148480518333
Validation loss: 2.531222611814273

Epoch: 5| Step: 3
Training loss: 2.887152488241043
Validation loss: 2.552890050549516

Epoch: 5| Step: 4
Training loss: 2.782562117795772
Validation loss: 2.5219438636088474

Epoch: 5| Step: 5
Training loss: 2.391610933334541
Validation loss: 2.5578348216083744

Epoch: 5| Step: 6
Training loss: 3.1790860893734005
Validation loss: 2.5296348666174544

Epoch: 5| Step: 7
Training loss: 2.4319836723068153
Validation loss: 2.539241702789697

Epoch: 5| Step: 8
Training loss: 2.755973137684478
Validation loss: 2.554856263827851

Epoch: 5| Step: 9
Training loss: 2.6312187646022185
Validation loss: 2.5667467480639714

Epoch: 5| Step: 10
Training loss: 2.32943739799034
Validation loss: 2.5343236255725285

Epoch: 196| Step: 0
Training loss: 2.7809720704158654
Validation loss: 2.5397784287864167

Epoch: 5| Step: 1
Training loss: 2.8966651289057483
Validation loss: 2.548193578107971

Epoch: 5| Step: 2
Training loss: 2.48676669559644
Validation loss: 2.546294644917582

Epoch: 5| Step: 3
Training loss: 2.4035144465539835
Validation loss: 2.528727713052635

Epoch: 5| Step: 4
Training loss: 2.586156092506473
Validation loss: 2.5477210855922356

Epoch: 5| Step: 5
Training loss: 2.962727752815754
Validation loss: 2.53358713348326

Epoch: 5| Step: 6
Training loss: 2.518617544170069
Validation loss: 2.520884767999343

Epoch: 5| Step: 7
Training loss: 2.647096727760549
Validation loss: 2.538085727405745

Epoch: 5| Step: 8
Training loss: 2.5403229856515184
Validation loss: 2.546877447001675

Epoch: 5| Step: 9
Training loss: 2.441495017917479
Validation loss: 2.534537316369059

Epoch: 5| Step: 10
Training loss: 2.8461814797739278
Validation loss: 2.55242567313181

Epoch: 197| Step: 0
Training loss: 1.8529866210322754
Validation loss: 2.516564516492025

Epoch: 5| Step: 1
Training loss: 3.4909978628160223
Validation loss: 2.536289367553003

Epoch: 5| Step: 2
Training loss: 2.239494168184748
Validation loss: 2.5579607121185175

Epoch: 5| Step: 3
Training loss: 2.8184035331454886
Validation loss: 2.5191343462448557

Epoch: 5| Step: 4
Training loss: 2.9219525168840943
Validation loss: 2.5145315189740836

Epoch: 5| Step: 5
Training loss: 2.5872520954589144
Validation loss: 2.5338225545092556

Epoch: 5| Step: 6
Training loss: 2.639545377976799
Validation loss: 2.532848434809232

Epoch: 5| Step: 7
Training loss: 2.8213875959018098
Validation loss: 2.5162390431664887

Epoch: 5| Step: 8
Training loss: 2.2829713989351057
Validation loss: 2.5418317718647696

Epoch: 5| Step: 9
Training loss: 2.6961474023928456
Validation loss: 2.5340963888924475

Epoch: 5| Step: 10
Training loss: 2.4793096762904114
Validation loss: 2.5446944442965593

Epoch: 198| Step: 0
Training loss: 2.430050053179113
Validation loss: 2.5364294350955445

Epoch: 5| Step: 1
Training loss: 2.315095218488109
Validation loss: 2.554331752421299

Epoch: 5| Step: 2
Training loss: 2.505710184081816
Validation loss: 2.531872667552377

Epoch: 5| Step: 3
Training loss: 2.830034654492747
Validation loss: 2.5322337809424367

Epoch: 5| Step: 4
Training loss: 2.6116621763884167
Validation loss: 2.5311668971801904

Epoch: 5| Step: 5
Training loss: 2.7621457041014392
Validation loss: 2.531302342797211

Epoch: 5| Step: 6
Training loss: 3.1063898987611003
Validation loss: 2.5425813610246806

Epoch: 5| Step: 7
Training loss: 2.654523400527438
Validation loss: 2.505213817663515

Epoch: 5| Step: 8
Training loss: 2.5434502352358095
Validation loss: 2.5478438459634107

Epoch: 5| Step: 9
Training loss: 2.966811179452431
Validation loss: 2.5479226634137517

Epoch: 5| Step: 10
Training loss: 2.269727688559225
Validation loss: 2.544426869865192

Epoch: 199| Step: 0
Training loss: 2.9862179483232123
Validation loss: 2.539514031155657

Epoch: 5| Step: 1
Training loss: 2.674592983618487
Validation loss: 2.5380478141985905

Epoch: 5| Step: 2
Training loss: 2.8163821236559814
Validation loss: 2.5391161558478483

Epoch: 5| Step: 3
Training loss: 2.648595394502856
Validation loss: 2.534347995139615

Epoch: 5| Step: 4
Training loss: 2.4668495492755502
Validation loss: 2.5237155513357306

Epoch: 5| Step: 5
Training loss: 2.9745558335012325
Validation loss: 2.5333296025183407

Epoch: 5| Step: 6
Training loss: 3.1232052035870743
Validation loss: 2.5150252834799107

Epoch: 5| Step: 7
Training loss: 2.529727996256986
Validation loss: 2.5180859022593904

Epoch: 5| Step: 8
Training loss: 2.1977285753615465
Validation loss: 2.5344380272829143

Epoch: 5| Step: 9
Training loss: 1.8857812546526094
Validation loss: 2.541118070066057

Epoch: 5| Step: 10
Training loss: 2.718543033833866
Validation loss: 2.527252825041463

Epoch: 200| Step: 0
Training loss: 2.779205170631101
Validation loss: 2.5195900098342037

Epoch: 5| Step: 1
Training loss: 2.788449687578431
Validation loss: 2.5442032672752704

Epoch: 5| Step: 2
Training loss: 2.953290016367571
Validation loss: 2.529708788600146

Epoch: 5| Step: 3
Training loss: 2.8564903842597915
Validation loss: 2.5395438001408555

Epoch: 5| Step: 4
Training loss: 2.5599876741768224
Validation loss: 2.546631174843114

Epoch: 5| Step: 5
Training loss: 2.56345386894717
Validation loss: 2.5562844727886973

Epoch: 5| Step: 6
Training loss: 2.376877594674816
Validation loss: 2.54158744624671

Epoch: 5| Step: 7
Training loss: 2.6120504038038237
Validation loss: 2.550078426251302

Epoch: 5| Step: 8
Training loss: 2.5589140014568303
Validation loss: 2.558511958677581

Epoch: 5| Step: 9
Training loss: 2.5352525501740577
Validation loss: 2.540466159898031

Epoch: 5| Step: 10
Training loss: 2.4503826192916307
Validation loss: 2.5384220922950456

Testing loss: 2.4352003372917883

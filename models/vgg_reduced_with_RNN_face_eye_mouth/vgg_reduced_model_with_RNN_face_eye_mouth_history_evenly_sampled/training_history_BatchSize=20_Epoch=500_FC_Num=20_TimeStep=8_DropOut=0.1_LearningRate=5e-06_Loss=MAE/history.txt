Epoch: 1| Step: 0
Training loss: 4.25045108795166
Validation loss: 5.207751602254888

Epoch: 5| Step: 1
Training loss: 5.1352033615112305
Validation loss: 5.203455904478668

Epoch: 5| Step: 2
Training loss: 6.001287937164307
Validation loss: 5.1990183091932725

Epoch: 5| Step: 3
Training loss: 5.150527000427246
Validation loss: 5.194404781505626

Epoch: 5| Step: 4
Training loss: 5.146758556365967
Validation loss: 5.190061558959305

Epoch: 5| Step: 5
Training loss: 4.912253379821777
Validation loss: 5.1856909413491525

Epoch: 5| Step: 6
Training loss: 4.788514614105225
Validation loss: 5.18062561814503

Epoch: 5| Step: 7
Training loss: 4.553790092468262
Validation loss: 5.1760334302020325

Epoch: 5| Step: 8
Training loss: 5.680075168609619
Validation loss: 5.1714183848391295

Epoch: 5| Step: 9
Training loss: 4.215296745300293
Validation loss: 5.166477244387391

Epoch: 5| Step: 10
Training loss: 4.946545600891113
Validation loss: 5.161766011227844

Epoch: 2| Step: 0
Training loss: 3.8683688640594482
Validation loss: 5.156252409822198

Epoch: 5| Step: 1
Training loss: 5.278469085693359
Validation loss: 5.151113940823462

Epoch: 5| Step: 2
Training loss: 4.614590644836426
Validation loss: 5.145308340749433

Epoch: 5| Step: 3
Training loss: 5.464329242706299
Validation loss: 5.139637557409143

Epoch: 5| Step: 4
Training loss: 6.521491050720215
Validation loss: 5.133333857341479

Epoch: 5| Step: 5
Training loss: 5.848343372344971
Validation loss: 5.126907461433

Epoch: 5| Step: 6
Training loss: 5.299046516418457
Validation loss: 5.12069841610488

Epoch: 5| Step: 7
Training loss: 4.1412763595581055
Validation loss: 5.113741392730384

Epoch: 5| Step: 8
Training loss: 4.045956611633301
Validation loss: 5.106456454082202

Epoch: 5| Step: 9
Training loss: 4.532284736633301
Validation loss: 5.098898641524777

Epoch: 5| Step: 10
Training loss: 4.431412696838379
Validation loss: 5.090955329197709

Epoch: 3| Step: 0
Training loss: 4.8654704093933105
Validation loss: 5.083220051180932

Epoch: 5| Step: 1
Training loss: 4.808379173278809
Validation loss: 5.074579490128384

Epoch: 5| Step: 2
Training loss: 5.08925724029541
Validation loss: 5.065589535620905

Epoch: 5| Step: 3
Training loss: 4.840758323669434
Validation loss: 5.056290595762191

Epoch: 5| Step: 4
Training loss: 5.0254645347595215
Validation loss: 5.046968942047448

Epoch: 5| Step: 5
Training loss: 5.551111698150635
Validation loss: 5.037125605408863

Epoch: 5| Step: 6
Training loss: 4.142271995544434
Validation loss: 5.027004790562455

Epoch: 5| Step: 7
Training loss: 3.9082674980163574
Validation loss: 5.016094879437518

Epoch: 5| Step: 8
Training loss: 5.11697244644165
Validation loss: 5.004366874694824

Epoch: 5| Step: 9
Training loss: 4.999482154846191
Validation loss: 4.993027328163065

Epoch: 5| Step: 10
Training loss: 4.7559494972229
Validation loss: 4.9805582364400225

Epoch: 4| Step: 0
Training loss: 5.189241886138916
Validation loss: 4.967312838441583

Epoch: 5| Step: 1
Training loss: 4.565683841705322
Validation loss: 4.954613906081005

Epoch: 5| Step: 2
Training loss: 5.0074639320373535
Validation loss: 4.940926305709347

Epoch: 5| Step: 3
Training loss: 4.28752326965332
Validation loss: 4.926817888854652

Epoch: 5| Step: 4
Training loss: 4.598563194274902
Validation loss: 4.911942635813067

Epoch: 5| Step: 5
Training loss: 3.509946346282959
Validation loss: 4.896988720022222

Epoch: 5| Step: 6
Training loss: 5.0119829177856445
Validation loss: 4.881001205854519

Epoch: 5| Step: 7
Training loss: 5.170706748962402
Validation loss: 4.865567914901241

Epoch: 5| Step: 8
Training loss: 4.988460540771484
Validation loss: 4.848057259795486

Epoch: 5| Step: 9
Training loss: 4.929163932800293
Validation loss: 4.830493665510608

Epoch: 5| Step: 10
Training loss: 4.195573806762695
Validation loss: 4.812841897369713

Epoch: 5| Step: 0
Training loss: 3.1059837341308594
Validation loss: 4.794197138919626

Epoch: 5| Step: 1
Training loss: 5.508695602416992
Validation loss: 4.774655270320113

Epoch: 5| Step: 2
Training loss: 4.474193096160889
Validation loss: 4.754245199182982

Epoch: 5| Step: 3
Training loss: 4.342954158782959
Validation loss: 4.734367714133314

Epoch: 5| Step: 4
Training loss: 4.215222358703613
Validation loss: 4.71374346107565

Epoch: 5| Step: 5
Training loss: 5.211369514465332
Validation loss: 4.693738311849614

Epoch: 5| Step: 6
Training loss: 5.025395393371582
Validation loss: 4.670713327264273

Epoch: 5| Step: 7
Training loss: 4.36919641494751
Validation loss: 4.649042319226009

Epoch: 5| Step: 8
Training loss: 4.09950065612793
Validation loss: 4.625387960864652

Epoch: 5| Step: 9
Training loss: 4.277827262878418
Validation loss: 4.601554014349497

Epoch: 5| Step: 10
Training loss: 4.647539138793945
Validation loss: 4.577450880440333

Epoch: 6| Step: 0
Training loss: 5.537252902984619
Validation loss: 4.552985888655468

Epoch: 5| Step: 1
Training loss: 4.054882526397705
Validation loss: 4.5291509679568716

Epoch: 5| Step: 2
Training loss: 4.546926975250244
Validation loss: 4.502456675293625

Epoch: 5| Step: 3
Training loss: 4.157884120941162
Validation loss: 4.476633384663572

Epoch: 5| Step: 4
Training loss: 3.5354411602020264
Validation loss: 4.450684614078973

Epoch: 5| Step: 5
Training loss: 4.750328063964844
Validation loss: 4.423300686702933

Epoch: 5| Step: 6
Training loss: 4.15123987197876
Validation loss: 4.3983970611326155

Epoch: 5| Step: 7
Training loss: 5.491024017333984
Validation loss: 4.370953862385083

Epoch: 5| Step: 8
Training loss: 3.1603474617004395
Validation loss: 4.343834743704847

Epoch: 5| Step: 9
Training loss: 3.3745522499084473
Validation loss: 4.31861360098726

Epoch: 5| Step: 10
Training loss: 3.540529251098633
Validation loss: 4.291925594370852

Epoch: 7| Step: 0
Training loss: 4.25478458404541
Validation loss: 4.2656489444035355

Epoch: 5| Step: 1
Training loss: 5.010388374328613
Validation loss: 4.24323239890478

Epoch: 5| Step: 2
Training loss: 3.894106388092041
Validation loss: 4.221805357163952

Epoch: 5| Step: 3
Training loss: 2.754668712615967
Validation loss: 4.197758900221958

Epoch: 5| Step: 4
Training loss: 5.23020601272583
Validation loss: 4.179431548682592

Epoch: 5| Step: 5
Training loss: 4.161165237426758
Validation loss: 4.157140834357149

Epoch: 5| Step: 6
Training loss: 3.17750883102417
Validation loss: 4.138680622141848

Epoch: 5| Step: 7
Training loss: 3.992684841156006
Validation loss: 4.11840433202764

Epoch: 5| Step: 8
Training loss: 3.3409476280212402
Validation loss: 4.099723487771968

Epoch: 5| Step: 9
Training loss: 3.163341999053955
Validation loss: 4.082345152413973

Epoch: 5| Step: 10
Training loss: 5.008223056793213
Validation loss: 4.065175810167866

Epoch: 8| Step: 0
Training loss: 4.439706325531006
Validation loss: 4.04649753468011

Epoch: 5| Step: 1
Training loss: 4.607373237609863
Validation loss: 4.030689859902987

Epoch: 5| Step: 2
Training loss: 3.965109348297119
Validation loss: 4.0134852368344545

Epoch: 5| Step: 3
Training loss: 4.734025001525879
Validation loss: 3.9945231970920356

Epoch: 5| Step: 4
Training loss: 4.126921653747559
Validation loss: 3.9785625447509108

Epoch: 5| Step: 5
Training loss: 2.7844011783599854
Validation loss: 3.961199237454322

Epoch: 5| Step: 6
Training loss: 3.7811570167541504
Validation loss: 3.9446749687194824

Epoch: 5| Step: 7
Training loss: 3.016369342803955
Validation loss: 3.9286416781845914

Epoch: 5| Step: 8
Training loss: 2.928636074066162
Validation loss: 3.9127198342354066

Epoch: 5| Step: 9
Training loss: 3.4990932941436768
Validation loss: 3.897574158125026

Epoch: 5| Step: 10
Training loss: 4.187237739562988
Validation loss: 3.8828156122597317

Epoch: 9| Step: 0
Training loss: 4.282328128814697
Validation loss: 3.8691286733073573

Epoch: 5| Step: 1
Training loss: 2.935290813446045
Validation loss: 3.854617585418045

Epoch: 5| Step: 2
Training loss: 3.612884521484375
Validation loss: 3.8396483775108092

Epoch: 5| Step: 3
Training loss: 4.987279415130615
Validation loss: 3.825823158346197

Epoch: 5| Step: 4
Training loss: 2.958834171295166
Validation loss: 3.814467640333278

Epoch: 5| Step: 5
Training loss: 3.199476957321167
Validation loss: 3.8016916654443227

Epoch: 5| Step: 6
Training loss: 3.7642035484313965
Validation loss: 3.7903391058726976

Epoch: 5| Step: 7
Training loss: 4.369980812072754
Validation loss: 3.7793828595069145

Epoch: 5| Step: 8
Training loss: 3.849896192550659
Validation loss: 3.7684650728779454

Epoch: 5| Step: 9
Training loss: 3.117828845977783
Validation loss: 3.7583673077244915

Epoch: 5| Step: 10
Training loss: 3.492898941040039
Validation loss: 3.748300019130912

Epoch: 10| Step: 0
Training loss: 3.83636212348938
Validation loss: 3.738105958507907

Epoch: 5| Step: 1
Training loss: 2.9390451908111572
Validation loss: 3.7289525796008367

Epoch: 5| Step: 2
Training loss: 2.7462570667266846
Validation loss: 3.718908945719401

Epoch: 5| Step: 3
Training loss: 3.459944486618042
Validation loss: 3.7085742386438514

Epoch: 5| Step: 4
Training loss: 3.9394690990448
Validation loss: 3.7002614031555834

Epoch: 5| Step: 5
Training loss: 2.9045324325561523
Validation loss: 3.6892865601406304

Epoch: 5| Step: 6
Training loss: 4.38489294052124
Validation loss: 3.679716561430244

Epoch: 5| Step: 7
Training loss: 4.243094444274902
Validation loss: 3.671260056957122

Epoch: 5| Step: 8
Training loss: 3.6055240631103516
Validation loss: 3.658620657459382

Epoch: 5| Step: 9
Training loss: 3.569567918777466
Validation loss: 3.648728944922006

Epoch: 5| Step: 10
Training loss: 3.9695115089416504
Validation loss: 3.6379231714433238

Epoch: 11| Step: 0
Training loss: 3.47236704826355
Validation loss: 3.62825499298752

Epoch: 5| Step: 1
Training loss: 4.366384983062744
Validation loss: 3.619790297682567

Epoch: 5| Step: 2
Training loss: 2.45538592338562
Validation loss: 3.60926668618315

Epoch: 5| Step: 3
Training loss: 3.0031027793884277
Validation loss: 3.5984621714520197

Epoch: 5| Step: 4
Training loss: 4.534606456756592
Validation loss: 3.5882972517321186

Epoch: 5| Step: 5
Training loss: 4.43346643447876
Validation loss: 3.5783257330617597

Epoch: 5| Step: 6
Training loss: 3.566720485687256
Validation loss: 3.5699639115282285

Epoch: 5| Step: 7
Training loss: 2.3059515953063965
Validation loss: 3.5597004121349705

Epoch: 5| Step: 8
Training loss: 3.9859790802001953
Validation loss: 3.5512585870681272

Epoch: 5| Step: 9
Training loss: 3.3132617473602295
Validation loss: 3.542822581465526

Epoch: 5| Step: 10
Training loss: 3.0290493965148926
Validation loss: 3.5344244792897213

Epoch: 12| Step: 0
Training loss: 3.067650318145752
Validation loss: 3.5252944833488873

Epoch: 5| Step: 1
Training loss: 2.8817696571350098
Validation loss: 3.515877567311769

Epoch: 5| Step: 2
Training loss: 3.252791166305542
Validation loss: 3.5091560912388626

Epoch: 5| Step: 3
Training loss: 3.7540760040283203
Validation loss: 3.5014964611299577

Epoch: 5| Step: 4
Training loss: 2.8519129753112793
Validation loss: 3.4932282304251068

Epoch: 5| Step: 5
Training loss: 3.9064993858337402
Validation loss: 3.4831936385041926

Epoch: 5| Step: 6
Training loss: 3.6651177406311035
Validation loss: 3.477087402856478

Epoch: 5| Step: 7
Training loss: 3.19284987449646
Validation loss: 3.4684935128816994

Epoch: 5| Step: 8
Training loss: 3.8207974433898926
Validation loss: 3.460313750851539

Epoch: 5| Step: 9
Training loss: 3.478025436401367
Validation loss: 3.453454876458773

Epoch: 5| Step: 10
Training loss: 3.801727294921875
Validation loss: 3.4413558642069497

Epoch: 13| Step: 0
Training loss: 3.455655574798584
Validation loss: 3.4356721062814035

Epoch: 5| Step: 1
Training loss: 3.966557025909424
Validation loss: 3.426813528101931

Epoch: 5| Step: 2
Training loss: 2.7802987098693848
Validation loss: 3.418633778889974

Epoch: 5| Step: 3
Training loss: 2.5989387035369873
Validation loss: 3.409936951052758

Epoch: 5| Step: 4
Training loss: 4.082825660705566
Validation loss: 3.403544948947045

Epoch: 5| Step: 5
Training loss: 3.9610862731933594
Validation loss: 3.395014847478559

Epoch: 5| Step: 6
Training loss: 3.4144248962402344
Validation loss: 3.3884956170153875

Epoch: 5| Step: 7
Training loss: 3.1929712295532227
Validation loss: 3.382346712132936

Epoch: 5| Step: 8
Training loss: 2.204606533050537
Validation loss: 3.3738735593775266

Epoch: 5| Step: 9
Training loss: 3.4609782695770264
Validation loss: 3.365782958205028

Epoch: 5| Step: 10
Training loss: 3.813100814819336
Validation loss: 3.3590830090225383

Epoch: 14| Step: 0
Training loss: 2.858839750289917
Validation loss: 3.352104686921643

Epoch: 5| Step: 1
Training loss: 4.1466474533081055
Validation loss: 3.3445955963544947

Epoch: 5| Step: 2
Training loss: 2.8173375129699707
Validation loss: 3.3358963920224096

Epoch: 5| Step: 3
Training loss: 3.2028040885925293
Validation loss: 3.3315308965662473

Epoch: 5| Step: 4
Training loss: 3.694445848464966
Validation loss: 3.3249625288030153

Epoch: 5| Step: 5
Training loss: 2.662597179412842
Validation loss: 3.31800344938873

Epoch: 5| Step: 6
Training loss: 3.5628647804260254
Validation loss: 3.311606914766373

Epoch: 5| Step: 7
Training loss: 3.0927491188049316
Validation loss: 3.3016607274291334

Epoch: 5| Step: 8
Training loss: 3.576630115509033
Validation loss: 3.2959527405359412

Epoch: 5| Step: 9
Training loss: 2.8463573455810547
Validation loss: 3.289837932073942

Epoch: 5| Step: 10
Training loss: 3.754889965057373
Validation loss: 3.284309084697436

Epoch: 15| Step: 0
Training loss: 3.0884993076324463
Validation loss: 3.279526420818862

Epoch: 5| Step: 1
Training loss: 2.8755311965942383
Validation loss: 3.2726540616763535

Epoch: 5| Step: 2
Training loss: 3.386359453201294
Validation loss: 3.271998546456778

Epoch: 5| Step: 3
Training loss: 3.2372536659240723
Validation loss: 3.2694925902992167

Epoch: 5| Step: 4
Training loss: 3.504535675048828
Validation loss: 3.2631486718372633

Epoch: 5| Step: 5
Training loss: 3.5000953674316406
Validation loss: 3.2531776146222184

Epoch: 5| Step: 6
Training loss: 3.6580421924591064
Validation loss: 3.246748762746011

Epoch: 5| Step: 7
Training loss: 3.8361644744873047
Validation loss: 3.2418937862560315

Epoch: 5| Step: 8
Training loss: 3.2182459831237793
Validation loss: 3.2379072507222495

Epoch: 5| Step: 9
Training loss: 3.4981040954589844
Validation loss: 3.234173966992286

Epoch: 5| Step: 10
Training loss: 1.5097545385360718
Validation loss: 3.2274722283886326

Epoch: 16| Step: 0
Training loss: 2.697643756866455
Validation loss: 3.2218344570488058

Epoch: 5| Step: 1
Training loss: 2.9768993854522705
Validation loss: 3.215555057730726

Epoch: 5| Step: 2
Training loss: 3.882725954055786
Validation loss: 3.212552511563865

Epoch: 5| Step: 3
Training loss: 3.4161934852600098
Validation loss: 3.209488658494847

Epoch: 5| Step: 4
Training loss: 2.4859633445739746
Validation loss: 3.2036149296709286

Epoch: 5| Step: 5
Training loss: 3.2863762378692627
Validation loss: 3.20371816747932

Epoch: 5| Step: 6
Training loss: 3.8481147289276123
Validation loss: 3.1979359324260423

Epoch: 5| Step: 7
Training loss: 3.413306474685669
Validation loss: 3.1926388484175487

Epoch: 5| Step: 8
Training loss: 2.9953389167785645
Validation loss: 3.1882887758234495

Epoch: 5| Step: 9
Training loss: 3.312431812286377
Validation loss: 3.1891365333270003

Epoch: 5| Step: 10
Training loss: 2.8153092861175537
Validation loss: 3.1839789677691717

Epoch: 17| Step: 0
Training loss: 3.517226457595825
Validation loss: 3.175198242228518

Epoch: 5| Step: 1
Training loss: 3.092390537261963
Validation loss: 3.1720647965708086

Epoch: 5| Step: 2
Training loss: 2.7977263927459717
Validation loss: 3.1675768385651293

Epoch: 5| Step: 3
Training loss: 3.170825719833374
Validation loss: 3.1639628359066543

Epoch: 5| Step: 4
Training loss: 2.726968765258789
Validation loss: 3.163371116884293

Epoch: 5| Step: 5
Training loss: 3.434356689453125
Validation loss: 3.1601932433343705

Epoch: 5| Step: 6
Training loss: 2.3791236877441406
Validation loss: 3.1593771801199964

Epoch: 5| Step: 7
Training loss: 2.7838873863220215
Validation loss: 3.1559530124869397

Epoch: 5| Step: 8
Training loss: 4.125399589538574
Validation loss: 3.1503742843545894

Epoch: 5| Step: 9
Training loss: 3.321218490600586
Validation loss: 3.1442626932615876

Epoch: 5| Step: 10
Training loss: 3.516913890838623
Validation loss: 3.141016662761729

Epoch: 18| Step: 0
Training loss: 2.636256694793701
Validation loss: 3.138995014211183

Epoch: 5| Step: 1
Training loss: 3.4051513671875
Validation loss: 3.134865355748002

Epoch: 5| Step: 2
Training loss: 4.20314359664917
Validation loss: 3.1333382642397316

Epoch: 5| Step: 3
Training loss: 3.0244851112365723
Validation loss: 3.1273824886609147

Epoch: 5| Step: 4
Training loss: 2.826376438140869
Validation loss: 3.1208314331628944

Epoch: 5| Step: 5
Training loss: 3.2136523723602295
Validation loss: 3.120571005728937

Epoch: 5| Step: 6
Training loss: 2.6988134384155273
Validation loss: 3.118830362955729

Epoch: 5| Step: 7
Training loss: 2.956425189971924
Validation loss: 3.115260147279309

Epoch: 5| Step: 8
Training loss: 3.425100803375244
Validation loss: 3.108416203529604

Epoch: 5| Step: 9
Training loss: 2.866598606109619
Validation loss: 3.112058357525897

Epoch: 5| Step: 10
Training loss: 3.3249804973602295
Validation loss: 3.1110063393910727

Epoch: 19| Step: 0
Training loss: 3.4451446533203125
Validation loss: 3.1120754826453423

Epoch: 5| Step: 1
Training loss: 2.69211745262146
Validation loss: 3.105657882587884

Epoch: 5| Step: 2
Training loss: 2.520007848739624
Validation loss: 3.103636154564478

Epoch: 5| Step: 3
Training loss: 3.4585025310516357
Validation loss: 3.0945141930733957

Epoch: 5| Step: 4
Training loss: 4.009832382202148
Validation loss: 3.0899319161650953

Epoch: 5| Step: 5
Training loss: 3.4096851348876953
Validation loss: 3.085602670587519

Epoch: 5| Step: 6
Training loss: 3.159890651702881
Validation loss: 3.082316057656401

Epoch: 5| Step: 7
Training loss: 2.636474132537842
Validation loss: 3.081392570208478

Epoch: 5| Step: 8
Training loss: 3.269254684448242
Validation loss: 3.0850390875211327

Epoch: 5| Step: 9
Training loss: 2.7431726455688477
Validation loss: 3.0758036208409134

Epoch: 5| Step: 10
Training loss: 2.948944568634033
Validation loss: 3.068801651718796

Epoch: 20| Step: 0
Training loss: 2.4661929607391357
Validation loss: 3.064619343767884

Epoch: 5| Step: 1
Training loss: 3.209972858428955
Validation loss: 3.068070803919146

Epoch: 5| Step: 2
Training loss: 3.062953472137451
Validation loss: 3.066215310045468

Epoch: 5| Step: 3
Training loss: 2.853538990020752
Validation loss: 3.0663198501833024

Epoch: 5| Step: 4
Training loss: 2.6036994457244873
Validation loss: 3.065035209860853

Epoch: 5| Step: 5
Training loss: 2.9327988624572754
Validation loss: 3.0622097599890923

Epoch: 5| Step: 6
Training loss: 3.325575351715088
Validation loss: 3.052744973090387

Epoch: 5| Step: 7
Training loss: 4.332279682159424
Validation loss: 3.049114078603765

Epoch: 5| Step: 8
Training loss: 3.1872432231903076
Validation loss: 3.0513973646266486

Epoch: 5| Step: 9
Training loss: 3.0845704078674316
Validation loss: 3.0555889350111767

Epoch: 5| Step: 10
Training loss: 3.0214273929595947
Validation loss: 3.041814140094224

Epoch: 21| Step: 0
Training loss: 3.035247325897217
Validation loss: 3.038148000676145

Epoch: 5| Step: 1
Training loss: 3.0606322288513184
Validation loss: 3.037715260700513

Epoch: 5| Step: 2
Training loss: 2.749776601791382
Validation loss: 3.036277171104185

Epoch: 5| Step: 3
Training loss: 3.4095993041992188
Validation loss: 3.03247312320176

Epoch: 5| Step: 4
Training loss: 3.105473279953003
Validation loss: 3.02733192905303

Epoch: 5| Step: 5
Training loss: 3.3527839183807373
Validation loss: 3.0261629550687728

Epoch: 5| Step: 6
Training loss: 2.892420530319214
Validation loss: 3.027787341866442

Epoch: 5| Step: 7
Training loss: 3.191326141357422
Validation loss: 3.0201399095596804

Epoch: 5| Step: 8
Training loss: 2.8177456855773926
Validation loss: 3.0211689062015985

Epoch: 5| Step: 9
Training loss: 3.269960403442383
Validation loss: 3.021426313666887

Epoch: 5| Step: 10
Training loss: 2.9977574348449707
Validation loss: 3.0178830931263585

Epoch: 22| Step: 0
Training loss: 3.1495649814605713
Validation loss: 3.0122208313275407

Epoch: 5| Step: 1
Training loss: 3.7095634937286377
Validation loss: 3.0107019537238666

Epoch: 5| Step: 2
Training loss: 3.6688225269317627
Validation loss: 3.0088107867907454

Epoch: 5| Step: 3
Training loss: 3.174379348754883
Validation loss: 3.0089914004007974

Epoch: 5| Step: 4
Training loss: 2.8533596992492676
Validation loss: 3.0023300622099187

Epoch: 5| Step: 5
Training loss: 3.022914409637451
Validation loss: 2.9979667176482496

Epoch: 5| Step: 6
Training loss: 2.774925708770752
Validation loss: 2.994796073564919

Epoch: 5| Step: 7
Training loss: 1.9576858282089233
Validation loss: 2.992728110282652

Epoch: 5| Step: 8
Training loss: 3.011577606201172
Validation loss: 2.999259061710809

Epoch: 5| Step: 9
Training loss: 3.99824857711792
Validation loss: 3.010320504506429

Epoch: 5| Step: 10
Training loss: 2.2463512420654297
Validation loss: 2.98508333903487

Epoch: 23| Step: 0
Training loss: 3.303849458694458
Validation loss: 2.9812307921789025

Epoch: 5| Step: 1
Training loss: 3.1270954608917236
Validation loss: 2.9817065064625075

Epoch: 5| Step: 2
Training loss: 2.9707303047180176
Validation loss: 2.9846025795064945

Epoch: 5| Step: 3
Training loss: 2.923212766647339
Validation loss: 2.9844750794031287

Epoch: 5| Step: 4
Training loss: 3.0636022090911865
Validation loss: 2.9835427807223414

Epoch: 5| Step: 5
Training loss: 1.9772390127182007
Validation loss: 2.975936266683763

Epoch: 5| Step: 6
Training loss: 3.384347915649414
Validation loss: 2.9764661917122464

Epoch: 5| Step: 7
Training loss: 3.8144001960754395
Validation loss: 2.9706224421019196

Epoch: 5| Step: 8
Training loss: 2.510779857635498
Validation loss: 2.9688324184827906

Epoch: 5| Step: 9
Training loss: 2.9883861541748047
Validation loss: 2.971511520365233

Epoch: 5| Step: 10
Training loss: 3.5174598693847656
Validation loss: 2.9699673627012517

Epoch: 24| Step: 0
Training loss: 3.316145420074463
Validation loss: 2.965178220502792

Epoch: 5| Step: 1
Training loss: 3.2220306396484375
Validation loss: 2.9548649121356267

Epoch: 5| Step: 2
Training loss: 3.704474925994873
Validation loss: 2.9532738142116095

Epoch: 5| Step: 3
Training loss: 2.8453636169433594
Validation loss: 2.953167082161032

Epoch: 5| Step: 4
Training loss: 2.8563857078552246
Validation loss: 2.958979796337825

Epoch: 5| Step: 5
Training loss: 2.4235405921936035
Validation loss: 2.9567928109117734

Epoch: 5| Step: 6
Training loss: 2.022664785385132
Validation loss: 2.958809286035517

Epoch: 5| Step: 7
Training loss: 3.2102572917938232
Validation loss: 2.9410744995199223

Epoch: 5| Step: 8
Training loss: 2.4253482818603516
Validation loss: 2.937172491063354

Epoch: 5| Step: 9
Training loss: 3.834348201751709
Validation loss: 2.9370930553764425

Epoch: 5| Step: 10
Training loss: 3.5276637077331543
Validation loss: 2.9360048899086575

Epoch: 25| Step: 0
Training loss: 2.499640703201294
Validation loss: 2.9327204406902356

Epoch: 5| Step: 1
Training loss: 2.767110586166382
Validation loss: 2.9376119746956775

Epoch: 5| Step: 2
Training loss: 2.9077157974243164
Validation loss: 2.9285046592835458

Epoch: 5| Step: 3
Training loss: 3.3612403869628906
Validation loss: 2.925316761898738

Epoch: 5| Step: 4
Training loss: 2.653489351272583
Validation loss: 2.9225395110345658

Epoch: 5| Step: 5
Training loss: 2.804135799407959
Validation loss: 2.9266533928532756

Epoch: 5| Step: 6
Training loss: 3.518136501312256
Validation loss: 2.9348901779420915

Epoch: 5| Step: 7
Training loss: 4.237175941467285
Validation loss: 2.9305669415381645

Epoch: 5| Step: 8
Training loss: 3.45182728767395
Validation loss: 2.9255256063194683

Epoch: 5| Step: 9
Training loss: 1.9209562540054321
Validation loss: 2.9174909796766055

Epoch: 5| Step: 10
Training loss: 2.9849870204925537
Validation loss: 2.915371125744235

Epoch: 26| Step: 0
Training loss: 2.4253346920013428
Validation loss: 2.9222643401033137

Epoch: 5| Step: 1
Training loss: 3.055105686187744
Validation loss: 2.924238743320588

Epoch: 5| Step: 2
Training loss: 3.7118492126464844
Validation loss: 2.9181143545335337

Epoch: 5| Step: 3
Training loss: 3.0979762077331543
Validation loss: 2.912809633439587

Epoch: 5| Step: 4
Training loss: 3.48559832572937
Validation loss: 2.9136199746080624

Epoch: 5| Step: 5
Training loss: 2.799708843231201
Validation loss: 2.9121433124747327

Epoch: 5| Step: 6
Training loss: 3.6276397705078125
Validation loss: 2.9068704420520413

Epoch: 5| Step: 7
Training loss: 2.7798585891723633
Validation loss: 2.9101115503618793

Epoch: 5| Step: 8
Training loss: 2.1939892768859863
Validation loss: 2.902261790408883

Epoch: 5| Step: 9
Training loss: 3.2970175743103027
Validation loss: 2.9039067760590584

Epoch: 5| Step: 10
Training loss: 2.468076705932617
Validation loss: 2.900205568600726

Epoch: 27| Step: 0
Training loss: 2.8946683406829834
Validation loss: 2.899589897483908

Epoch: 5| Step: 1
Training loss: 2.7336554527282715
Validation loss: 2.8928263623227357

Epoch: 5| Step: 2
Training loss: 3.417067289352417
Validation loss: 2.8903073777434645

Epoch: 5| Step: 3
Training loss: 3.523613452911377
Validation loss: 2.890205406373547

Epoch: 5| Step: 4
Training loss: 2.7832393646240234
Validation loss: 2.8915012344237296

Epoch: 5| Step: 5
Training loss: 2.714853286743164
Validation loss: 2.885345451293453

Epoch: 5| Step: 6
Training loss: 2.909374475479126
Validation loss: 2.886998132992816

Epoch: 5| Step: 7
Training loss: 3.4292731285095215
Validation loss: 2.883743732206283

Epoch: 5| Step: 8
Training loss: 2.8772261142730713
Validation loss: 2.8843324594600226

Epoch: 5| Step: 9
Training loss: 2.260901689529419
Validation loss: 2.8863475989269953

Epoch: 5| Step: 10
Training loss: 3.3429059982299805
Validation loss: 2.881380699014151

Epoch: 28| Step: 0
Training loss: 2.916252613067627
Validation loss: 2.8835246409139326

Epoch: 5| Step: 1
Training loss: 2.759181499481201
Validation loss: 2.8786767349448255

Epoch: 5| Step: 2
Training loss: 3.813223361968994
Validation loss: 2.8735638562069146

Epoch: 5| Step: 3
Training loss: 2.9454185962677
Validation loss: 2.8797849685915056

Epoch: 5| Step: 4
Training loss: 2.596837282180786
Validation loss: 2.877264727828323

Epoch: 5| Step: 5
Training loss: 3.2198760509490967
Validation loss: 2.880260375238234

Epoch: 5| Step: 6
Training loss: 3.276210069656372
Validation loss: 2.8752574869381484

Epoch: 5| Step: 7
Training loss: 2.1320698261260986
Validation loss: 2.8756866506350938

Epoch: 5| Step: 8
Training loss: 2.301220655441284
Validation loss: 2.874766622820208

Epoch: 5| Step: 9
Training loss: 2.970036268234253
Validation loss: 2.871341379739905

Epoch: 5| Step: 10
Training loss: 3.9544053077697754
Validation loss: 2.8701177130463305

Epoch: 29| Step: 0
Training loss: 3.861841917037964
Validation loss: 2.8703379682315293

Epoch: 5| Step: 1
Training loss: 2.201355218887329
Validation loss: 2.8682943210806897

Epoch: 5| Step: 2
Training loss: 2.5853047370910645
Validation loss: 2.8632404240228797

Epoch: 5| Step: 3
Training loss: 3.349203109741211
Validation loss: 2.8630372196115474

Epoch: 5| Step: 4
Training loss: 3.1937663555145264
Validation loss: 2.859760335696641

Epoch: 5| Step: 5
Training loss: 2.219696044921875
Validation loss: 2.859664104318106

Epoch: 5| Step: 6
Training loss: 2.9186453819274902
Validation loss: 2.860445522492932

Epoch: 5| Step: 7
Training loss: 2.8640546798706055
Validation loss: 2.8605521609706264

Epoch: 5| Step: 8
Training loss: 2.4221043586730957
Validation loss: 2.8669765610848703

Epoch: 5| Step: 9
Training loss: 3.159899950027466
Validation loss: 2.859589135775002

Epoch: 5| Step: 10
Training loss: 4.047114372253418
Validation loss: 2.855279532811975

Epoch: 30| Step: 0
Training loss: 3.2982394695281982
Validation loss: 2.8491120671713226

Epoch: 5| Step: 1
Training loss: 3.0085337162017822
Validation loss: 2.846379364690473

Epoch: 5| Step: 2
Training loss: 3.412062406539917
Validation loss: 2.8461606861442648

Epoch: 5| Step: 3
Training loss: 2.2514665126800537
Validation loss: 2.8497880786977787

Epoch: 5| Step: 4
Training loss: 2.8917312622070312
Validation loss: 2.847738365973196

Epoch: 5| Step: 5
Training loss: 2.9565937519073486
Validation loss: 2.848364758235152

Epoch: 5| Step: 6
Training loss: 2.4412944316864014
Validation loss: 2.847297304420061

Epoch: 5| Step: 7
Training loss: 3.2635440826416016
Validation loss: 2.8488573002558883

Epoch: 5| Step: 8
Training loss: 2.790030002593994
Validation loss: 2.8449466792486047

Epoch: 5| Step: 9
Training loss: 2.7477972507476807
Validation loss: 2.8397838505365516

Epoch: 5| Step: 10
Training loss: 3.5210230350494385
Validation loss: 2.8463231389240553

Epoch: 31| Step: 0
Training loss: 3.293921947479248
Validation loss: 2.8530263131664646

Epoch: 5| Step: 1
Training loss: 2.5332751274108887
Validation loss: 2.852781521376743

Epoch: 5| Step: 2
Training loss: 2.6200950145721436
Validation loss: 2.8383801214156614

Epoch: 5| Step: 3
Training loss: 2.458075761795044
Validation loss: 2.837142990481469

Epoch: 5| Step: 4
Training loss: 3.2102973461151123
Validation loss: 2.828340766250446

Epoch: 5| Step: 5
Training loss: 2.5032436847686768
Validation loss: 2.828194195224393

Epoch: 5| Step: 6
Training loss: 2.9003703594207764
Validation loss: 2.825600634339035

Epoch: 5| Step: 7
Training loss: 2.8883109092712402
Validation loss: 2.8240897194031747

Epoch: 5| Step: 8
Training loss: 3.9110260009765625
Validation loss: 2.82798050039558

Epoch: 5| Step: 9
Training loss: 2.793043613433838
Validation loss: 2.825703546565066

Epoch: 5| Step: 10
Training loss: 3.3281211853027344
Validation loss: 2.8239529235388643

Epoch: 32| Step: 0
Training loss: 3.8849289417266846
Validation loss: 2.8217599981574604

Epoch: 5| Step: 1
Training loss: 3.253884792327881
Validation loss: 2.821292277305357

Epoch: 5| Step: 2
Training loss: 2.90997314453125
Validation loss: 2.822120958758939

Epoch: 5| Step: 3
Training loss: 2.4523568153381348
Validation loss: 2.8235664777858283

Epoch: 5| Step: 4
Training loss: 3.2235965728759766
Validation loss: 2.8174392715577157

Epoch: 5| Step: 5
Training loss: 2.4205026626586914
Validation loss: 2.8152372785793838

Epoch: 5| Step: 6
Training loss: 3.1607778072357178
Validation loss: 2.818211106843846

Epoch: 5| Step: 7
Training loss: 3.036698818206787
Validation loss: 2.8144262606097805

Epoch: 5| Step: 8
Training loss: 2.387200117111206
Validation loss: 2.814058872961229

Epoch: 5| Step: 9
Training loss: 2.807257652282715
Validation loss: 2.810284968345396

Epoch: 5| Step: 10
Training loss: 2.789267063140869
Validation loss: 2.814486652292231

Epoch: 33| Step: 0
Training loss: 2.592362642288208
Validation loss: 2.81010147576691

Epoch: 5| Step: 1
Training loss: 1.791375756263733
Validation loss: 2.8120793834809334

Epoch: 5| Step: 2
Training loss: 3.3420207500457764
Validation loss: 2.811061495093889

Epoch: 5| Step: 3
Training loss: 2.9120171070098877
Validation loss: 2.815135112372778

Epoch: 5| Step: 4
Training loss: 2.532412052154541
Validation loss: 2.8073021750296316

Epoch: 5| Step: 5
Training loss: 3.081183910369873
Validation loss: 2.8142941536441928

Epoch: 5| Step: 6
Training loss: 2.8840792179107666
Validation loss: 2.817609892096571

Epoch: 5| Step: 7
Training loss: 3.4956791400909424
Validation loss: 2.8409362736568657

Epoch: 5| Step: 8
Training loss: 3.6738314628601074
Validation loss: 2.8182747492226223

Epoch: 5| Step: 9
Training loss: 2.9818294048309326
Validation loss: 2.8067027881581295

Epoch: 5| Step: 10
Training loss: 2.977299928665161
Validation loss: 2.8103300627841743

Epoch: 34| Step: 0
Training loss: 3.1510415077209473
Validation loss: 2.805246571058868

Epoch: 5| Step: 1
Training loss: 3.0106027126312256
Validation loss: 2.80346857091432

Epoch: 5| Step: 2
Training loss: 3.92952299118042
Validation loss: 2.8035349333158104

Epoch: 5| Step: 3
Training loss: 3.4712424278259277
Validation loss: 2.8027530870129986

Epoch: 5| Step: 4
Training loss: 2.248267650604248
Validation loss: 2.803368570984051

Epoch: 5| Step: 5
Training loss: 3.0863149166107178
Validation loss: 2.8021304786846204

Epoch: 5| Step: 6
Training loss: 2.6436851024627686
Validation loss: 2.807249128177602

Epoch: 5| Step: 7
Training loss: 2.695908784866333
Validation loss: 2.813085614994008

Epoch: 5| Step: 8
Training loss: 2.422463893890381
Validation loss: 2.804876578751431

Epoch: 5| Step: 9
Training loss: 2.478027820587158
Validation loss: 2.8096381669403403

Epoch: 5| Step: 10
Training loss: 3.1248748302459717
Validation loss: 2.8002921124940277

Epoch: 35| Step: 0
Training loss: 2.074265718460083
Validation loss: 2.795770249059123

Epoch: 5| Step: 1
Training loss: 3.2038509845733643
Validation loss: 2.790757299751364

Epoch: 5| Step: 2
Training loss: 3.6684346199035645
Validation loss: 2.7919130581681446

Epoch: 5| Step: 3
Training loss: 2.7582688331604004
Validation loss: 2.8081603537323656

Epoch: 5| Step: 4
Training loss: 3.190850257873535
Validation loss: 2.8061120381919284

Epoch: 5| Step: 5
Training loss: 3.27679181098938
Validation loss: 2.8086041045445267

Epoch: 5| Step: 6
Training loss: 2.2378382682800293
Validation loss: 2.8010624377958235

Epoch: 5| Step: 7
Training loss: 2.492786169052124
Validation loss: 2.793282521668301

Epoch: 5| Step: 8
Training loss: 3.0144002437591553
Validation loss: 2.789616105377033

Epoch: 5| Step: 9
Training loss: 3.1066782474517822
Validation loss: 2.794696782224922

Epoch: 5| Step: 10
Training loss: 3.2105422019958496
Validation loss: 2.7887199463382846

Epoch: 36| Step: 0
Training loss: 2.245323657989502
Validation loss: 2.789551288850846

Epoch: 5| Step: 1
Training loss: 2.4422593116760254
Validation loss: 2.7911685794912358

Epoch: 5| Step: 2
Training loss: 2.9794259071350098
Validation loss: 2.792091069682952

Epoch: 5| Step: 3
Training loss: 3.016517400741577
Validation loss: 2.7891969244967223

Epoch: 5| Step: 4
Training loss: 3.4541656970977783
Validation loss: 2.791437784830729

Epoch: 5| Step: 5
Training loss: 2.98124361038208
Validation loss: 2.789180924815516

Epoch: 5| Step: 6
Training loss: 2.944446325302124
Validation loss: 2.7843934566743913

Epoch: 5| Step: 7
Training loss: 2.512784242630005
Validation loss: 2.787687491345149

Epoch: 5| Step: 8
Training loss: 3.8272476196289062
Validation loss: 2.7866098086039224

Epoch: 5| Step: 9
Training loss: 2.798793315887451
Validation loss: 2.789454321707449

Epoch: 5| Step: 10
Training loss: 2.908212184906006
Validation loss: 2.780481187246179

Epoch: 37| Step: 0
Training loss: 2.9474709033966064
Validation loss: 2.781164120602351

Epoch: 5| Step: 1
Training loss: 3.1278164386749268
Validation loss: 2.7809816355346353

Epoch: 5| Step: 2
Training loss: 2.6629257202148438
Validation loss: 2.777687385518064

Epoch: 5| Step: 3
Training loss: 2.667254686355591
Validation loss: 2.780561298452398

Epoch: 5| Step: 4
Training loss: 2.8906426429748535
Validation loss: 2.777205882533904

Epoch: 5| Step: 5
Training loss: 2.8042221069335938
Validation loss: 2.7771467495990056

Epoch: 5| Step: 6
Training loss: 2.844663619995117
Validation loss: 2.7791235575111966

Epoch: 5| Step: 7
Training loss: 2.7332375049591064
Validation loss: 2.7765710302578506

Epoch: 5| Step: 8
Training loss: 3.313131332397461
Validation loss: 2.775538818810576

Epoch: 5| Step: 9
Training loss: 3.2209999561309814
Validation loss: 2.7766460346919235

Epoch: 5| Step: 10
Training loss: 2.7890655994415283
Validation loss: 2.7732344237706994

Epoch: 38| Step: 0
Training loss: 2.809847593307495
Validation loss: 2.775105655834239

Epoch: 5| Step: 1
Training loss: 3.3375186920166016
Validation loss: 2.776937815450853

Epoch: 5| Step: 2
Training loss: 3.6376609802246094
Validation loss: 2.7737401044496925

Epoch: 5| Step: 3
Training loss: 2.6702847480773926
Validation loss: 2.768508111276934

Epoch: 5| Step: 4
Training loss: 2.7230019569396973
Validation loss: 2.770187529184485

Epoch: 5| Step: 5
Training loss: 2.689375162124634
Validation loss: 2.7673453823212655

Epoch: 5| Step: 6
Training loss: 2.4644882678985596
Validation loss: 2.7676050022084224

Epoch: 5| Step: 7
Training loss: 2.5141944885253906
Validation loss: 2.768817563210764

Epoch: 5| Step: 8
Training loss: 2.4889414310455322
Validation loss: 2.7750270469214326

Epoch: 5| Step: 9
Training loss: 3.778632402420044
Validation loss: 2.773269179046795

Epoch: 5| Step: 10
Training loss: 2.877481698989868
Validation loss: 2.7787630558013916

Epoch: 39| Step: 0
Training loss: 3.13523530960083
Validation loss: 2.77419029256349

Epoch: 5| Step: 1
Training loss: 2.3598856925964355
Validation loss: 2.769403780660322

Epoch: 5| Step: 2
Training loss: 2.4463143348693848
Validation loss: 2.772886122426679

Epoch: 5| Step: 3
Training loss: 3.064507246017456
Validation loss: 2.762840937542659

Epoch: 5| Step: 4
Training loss: 2.7333872318267822
Validation loss: 2.7636008775362404

Epoch: 5| Step: 5
Training loss: 3.5534636974334717
Validation loss: 2.7661846914599018

Epoch: 5| Step: 6
Training loss: 2.731978416442871
Validation loss: 2.762917785234349

Epoch: 5| Step: 7
Training loss: 2.986743211746216
Validation loss: 2.763219348845943

Epoch: 5| Step: 8
Training loss: 3.068135976791382
Validation loss: 2.760018684530771

Epoch: 5| Step: 9
Training loss: 2.2358105182647705
Validation loss: 2.7600325333174838

Epoch: 5| Step: 10
Training loss: 3.725233793258667
Validation loss: 2.7667268527451383

Epoch: 40| Step: 0
Training loss: 2.1441314220428467
Validation loss: 2.758149039360785

Epoch: 5| Step: 1
Training loss: 3.417950391769409
Validation loss: 2.7629967992023756

Epoch: 5| Step: 2
Training loss: 2.972449779510498
Validation loss: 2.760497329055622

Epoch: 5| Step: 3
Training loss: 3.3679118156433105
Validation loss: 2.769200360903176

Epoch: 5| Step: 4
Training loss: 2.78656268119812
Validation loss: 2.780267920545352

Epoch: 5| Step: 5
Training loss: 2.6900534629821777
Validation loss: 2.7966853828840357

Epoch: 5| Step: 6
Training loss: 2.612632989883423
Validation loss: 2.788822038199312

Epoch: 5| Step: 7
Training loss: 3.777726650238037
Validation loss: 2.7658357979148946

Epoch: 5| Step: 8
Training loss: 2.0327963829040527
Validation loss: 2.7592983963668987

Epoch: 5| Step: 9
Training loss: 3.171168804168701
Validation loss: 2.763531200347408

Epoch: 5| Step: 10
Training loss: 2.97245717048645
Validation loss: 2.772147863141952

Epoch: 41| Step: 0
Training loss: 3.2515785694122314
Validation loss: 2.778186518658874

Epoch: 5| Step: 1
Training loss: 3.9469432830810547
Validation loss: 2.766804720765801

Epoch: 5| Step: 2
Training loss: 3.025545835494995
Validation loss: 2.778148748541391

Epoch: 5| Step: 3
Training loss: 2.472968578338623
Validation loss: 2.766017072944231

Epoch: 5| Step: 4
Training loss: 3.0990970134735107
Validation loss: 2.7689686436806955

Epoch: 5| Step: 5
Training loss: 3.3788700103759766
Validation loss: 2.7607259212001676

Epoch: 5| Step: 6
Training loss: 2.6405513286590576
Validation loss: 2.7641173947241997

Epoch: 5| Step: 7
Training loss: 2.6333820819854736
Validation loss: 2.7556748595289005

Epoch: 5| Step: 8
Training loss: 2.3029181957244873
Validation loss: 2.7577416486637567

Epoch: 5| Step: 9
Training loss: 2.313659191131592
Validation loss: 2.7552140271791847

Epoch: 5| Step: 10
Training loss: 2.872030735015869
Validation loss: 2.756201446697276

Epoch: 42| Step: 0
Training loss: 3.2179133892059326
Validation loss: 2.758077121550037

Epoch: 5| Step: 1
Training loss: 1.9265716075897217
Validation loss: 2.7553459059807563

Epoch: 5| Step: 2
Training loss: 3.2922699451446533
Validation loss: 2.756131556726271

Epoch: 5| Step: 3
Training loss: 2.8065249919891357
Validation loss: 2.756713669787171

Epoch: 5| Step: 4
Training loss: 3.0095906257629395
Validation loss: 2.7538329055232387

Epoch: 5| Step: 5
Training loss: 2.4846293926239014
Validation loss: 2.7519925179020053

Epoch: 5| Step: 6
Training loss: 3.328843593597412
Validation loss: 2.7508459347550587

Epoch: 5| Step: 7
Training loss: 3.1962084770202637
Validation loss: 2.7458462920240176

Epoch: 5| Step: 8
Training loss: 3.1025660037994385
Validation loss: 2.743590134446339

Epoch: 5| Step: 9
Training loss: 2.550271987915039
Validation loss: 2.744124848355529

Epoch: 5| Step: 10
Training loss: 2.924872636795044
Validation loss: 2.746391955242362

Epoch: 43| Step: 0
Training loss: 3.341961622238159
Validation loss: 2.7349328764023317

Epoch: 5| Step: 1
Training loss: 2.1854758262634277
Validation loss: 2.7389128028705554

Epoch: 5| Step: 2
Training loss: 2.234215021133423
Validation loss: 2.7427844052673667

Epoch: 5| Step: 3
Training loss: 2.8297157287597656
Validation loss: 2.7388064656206357

Epoch: 5| Step: 4
Training loss: 3.4110023975372314
Validation loss: 2.737343047254829

Epoch: 5| Step: 5
Training loss: 2.659514904022217
Validation loss: 2.7450025901999524

Epoch: 5| Step: 6
Training loss: 2.539315700531006
Validation loss: 2.7424019357209564

Epoch: 5| Step: 7
Training loss: 3.351990222930908
Validation loss: 2.742762334885136

Epoch: 5| Step: 8
Training loss: 3.2548317909240723
Validation loss: 2.746980236422631

Epoch: 5| Step: 9
Training loss: 3.0288493633270264
Validation loss: 2.740952373832785

Epoch: 5| Step: 10
Training loss: 2.942904233932495
Validation loss: 2.7499105648327897

Epoch: 44| Step: 0
Training loss: 4.049536228179932
Validation loss: 2.736610879180252

Epoch: 5| Step: 1
Training loss: 2.7475342750549316
Validation loss: 2.7342914355698453

Epoch: 5| Step: 2
Training loss: 2.213665008544922
Validation loss: 2.7383130955439743

Epoch: 5| Step: 3
Training loss: 2.972158908843994
Validation loss: 2.7344944066898798

Epoch: 5| Step: 4
Training loss: 3.033576488494873
Validation loss: 2.7346148029450448

Epoch: 5| Step: 5
Training loss: 2.7441508769989014
Validation loss: 2.729957519039031

Epoch: 5| Step: 6
Training loss: 2.787876605987549
Validation loss: 2.7326627341649865

Epoch: 5| Step: 7
Training loss: 2.5250840187072754
Validation loss: 2.7343373401190645

Epoch: 5| Step: 8
Training loss: 2.5873448848724365
Validation loss: 2.7336437727815364

Epoch: 5| Step: 9
Training loss: 2.8425798416137695
Validation loss: 2.7341807785854546

Epoch: 5| Step: 10
Training loss: 3.1798014640808105
Validation loss: 2.7375443725175757

Epoch: 45| Step: 0
Training loss: 2.235661029815674
Validation loss: 2.7358420997537594

Epoch: 5| Step: 1
Training loss: 1.8578754663467407
Validation loss: 2.7275267929159184

Epoch: 5| Step: 2
Training loss: 3.5253281593322754
Validation loss: 2.7315208040257937

Epoch: 5| Step: 3
Training loss: 3.5257515907287598
Validation loss: 2.7385289592127644

Epoch: 5| Step: 4
Training loss: 3.0095059871673584
Validation loss: 2.7433532976335093

Epoch: 5| Step: 5
Training loss: 3.0302181243896484
Validation loss: 2.73584327390117

Epoch: 5| Step: 6
Training loss: 2.866821765899658
Validation loss: 2.7453968217295985

Epoch: 5| Step: 7
Training loss: 2.1439170837402344
Validation loss: 2.7361243309513217

Epoch: 5| Step: 8
Training loss: 2.9120426177978516
Validation loss: 2.7201299359721522

Epoch: 5| Step: 9
Training loss: 3.1710989475250244
Validation loss: 2.7200416441886657

Epoch: 5| Step: 10
Training loss: 3.3986754417419434
Validation loss: 2.7334021111970306

Epoch: 46| Step: 0
Training loss: 2.434177875518799
Validation loss: 2.7321223930646013

Epoch: 5| Step: 1
Training loss: 2.766165256500244
Validation loss: 2.7464927986104

Epoch: 5| Step: 2
Training loss: 2.6708035469055176
Validation loss: 2.7598451747689197

Epoch: 5| Step: 3
Training loss: 3.4454751014709473
Validation loss: 2.7472121356635966

Epoch: 5| Step: 4
Training loss: 3.0996899604797363
Validation loss: 2.734066060794297

Epoch: 5| Step: 5
Training loss: 2.081066608428955
Validation loss: 2.731215497498871

Epoch: 5| Step: 6
Training loss: 4.244454383850098
Validation loss: 2.7282911551895963

Epoch: 5| Step: 7
Training loss: 3.312316417694092
Validation loss: 2.7309010695385676

Epoch: 5| Step: 8
Training loss: 2.861062526702881
Validation loss: 2.762348764686174

Epoch: 5| Step: 9
Training loss: 2.1741509437561035
Validation loss: 2.7879004529727403

Epoch: 5| Step: 10
Training loss: 2.596498727798462
Validation loss: 2.7852307442695863

Epoch: 47| Step: 0
Training loss: 1.9066669940948486
Validation loss: 2.805172217789517

Epoch: 5| Step: 1
Training loss: 2.8602702617645264
Validation loss: 2.794323234147923

Epoch: 5| Step: 2
Training loss: 2.858861207962036
Validation loss: 2.804687400018015

Epoch: 5| Step: 3
Training loss: 3.66796875
Validation loss: 2.7948875683610157

Epoch: 5| Step: 4
Training loss: 3.204680919647217
Validation loss: 2.7655769009743967

Epoch: 5| Step: 5
Training loss: 2.679546594619751
Validation loss: 2.7184146194047827

Epoch: 5| Step: 6
Training loss: 3.1461784839630127
Validation loss: 2.717557309776224

Epoch: 5| Step: 7
Training loss: 2.265235662460327
Validation loss: 2.728699358560706

Epoch: 5| Step: 8
Training loss: 2.8184680938720703
Validation loss: 2.7375428881696475

Epoch: 5| Step: 9
Training loss: 2.9874227046966553
Validation loss: 2.7409054181909047

Epoch: 5| Step: 10
Training loss: 3.4839107990264893
Validation loss: 2.7332812816865983

Epoch: 48| Step: 0
Training loss: 3.3391571044921875
Validation loss: 2.7257673355840866

Epoch: 5| Step: 1
Training loss: 2.8342738151550293
Validation loss: 2.724487691797236

Epoch: 5| Step: 2
Training loss: 2.5084216594696045
Validation loss: 2.7236809448529313

Epoch: 5| Step: 3
Training loss: 3.132153034210205
Validation loss: 2.7262166520600677

Epoch: 5| Step: 4
Training loss: 2.8585643768310547
Validation loss: 2.7203121185302734

Epoch: 5| Step: 5
Training loss: 2.5777039527893066
Validation loss: 2.7179374592278593

Epoch: 5| Step: 6
Training loss: 3.1213393211364746
Validation loss: 2.7120467052664807

Epoch: 5| Step: 7
Training loss: 3.451258897781372
Validation loss: 2.7204904120455504

Epoch: 5| Step: 8
Training loss: 2.778135299682617
Validation loss: 2.719980337286508

Epoch: 5| Step: 9
Training loss: 1.8484344482421875
Validation loss: 2.7211997073183776

Epoch: 5| Step: 10
Training loss: 3.1564362049102783
Validation loss: 2.7210350113530315

Epoch: 49| Step: 0
Training loss: 2.6685078144073486
Validation loss: 2.7208173223721084

Epoch: 5| Step: 1
Training loss: 2.7255771160125732
Validation loss: 2.7121570969140656

Epoch: 5| Step: 2
Training loss: 2.1669135093688965
Validation loss: 2.713507398482292

Epoch: 5| Step: 3
Training loss: 3.7515251636505127
Validation loss: 2.706394895430534

Epoch: 5| Step: 4
Training loss: 2.6455490589141846
Validation loss: 2.7122440927772113

Epoch: 5| Step: 5
Training loss: 2.5700695514678955
Validation loss: 2.705275435601511

Epoch: 5| Step: 6
Training loss: 3.226426362991333
Validation loss: 2.706604631998206

Epoch: 5| Step: 7
Training loss: 2.433964967727661
Validation loss: 2.7048302055687032

Epoch: 5| Step: 8
Training loss: 3.283766508102417
Validation loss: 2.7029484061784643

Epoch: 5| Step: 9
Training loss: 2.978717803955078
Validation loss: 2.701965957559565

Epoch: 5| Step: 10
Training loss: 3.034675359725952
Validation loss: 2.7023174301270516

Epoch: 50| Step: 0
Training loss: 3.3537464141845703
Validation loss: 2.702729368722567

Epoch: 5| Step: 1
Training loss: 2.428955078125
Validation loss: 2.700230477958597

Epoch: 5| Step: 2
Training loss: 3.008805274963379
Validation loss: 2.6973619794332855

Epoch: 5| Step: 3
Training loss: 2.4070141315460205
Validation loss: 2.6947134822927494

Epoch: 5| Step: 4
Training loss: 2.3931987285614014
Validation loss: 2.7046995726964806

Epoch: 5| Step: 5
Training loss: 3.322390079498291
Validation loss: 2.6995799080018075

Epoch: 5| Step: 6
Training loss: 3.364899158477783
Validation loss: 2.7018801678893385

Epoch: 5| Step: 7
Training loss: 2.7504799365997314
Validation loss: 2.7016583668288363

Epoch: 5| Step: 8
Training loss: 3.1748056411743164
Validation loss: 2.7020902402939333

Epoch: 5| Step: 9
Training loss: 2.8692212104797363
Validation loss: 2.70052783976319

Epoch: 5| Step: 10
Training loss: 2.219677209854126
Validation loss: 2.6919824307964695

Epoch: 51| Step: 0
Training loss: 2.721900463104248
Validation loss: 2.69382211982563

Epoch: 5| Step: 1
Training loss: 3.195303440093994
Validation loss: 2.6896600415629726

Epoch: 5| Step: 2
Training loss: 2.7477495670318604
Validation loss: 2.6924866630185034

Epoch: 5| Step: 3
Training loss: 2.73960542678833
Validation loss: 2.6928326109404206

Epoch: 5| Step: 4
Training loss: 2.3104541301727295
Validation loss: 2.702864431565808

Epoch: 5| Step: 5
Training loss: 3.028214693069458
Validation loss: 2.7053234782270206

Epoch: 5| Step: 6
Training loss: 3.048842191696167
Validation loss: 2.712273472098894

Epoch: 5| Step: 7
Training loss: 2.7731730937957764
Validation loss: 2.7076561784231536

Epoch: 5| Step: 8
Training loss: 2.339812755584717
Validation loss: 2.7164961471352527

Epoch: 5| Step: 9
Training loss: 3.1686055660247803
Validation loss: 2.712300487743911

Epoch: 5| Step: 10
Training loss: 3.3626933097839355
Validation loss: 2.716084095739549

Epoch: 52| Step: 0
Training loss: 2.9694156646728516
Validation loss: 2.7033594654452417

Epoch: 5| Step: 1
Training loss: 3.1408004760742188
Validation loss: 2.6948244315321728

Epoch: 5| Step: 2
Training loss: 2.8316292762756348
Validation loss: 2.6901390962703253

Epoch: 5| Step: 3
Training loss: 3.3550636768341064
Validation loss: 2.686944341146818

Epoch: 5| Step: 4
Training loss: 2.394794225692749
Validation loss: 2.689351458703318

Epoch: 5| Step: 5
Training loss: 2.8257064819335938
Validation loss: 2.6941452898005003

Epoch: 5| Step: 6
Training loss: 2.5113511085510254
Validation loss: 2.7098283203699256

Epoch: 5| Step: 7
Training loss: 2.6635818481445312
Validation loss: 2.7179952257422992

Epoch: 5| Step: 8
Training loss: 2.897303581237793
Validation loss: 2.708584531661003

Epoch: 5| Step: 9
Training loss: 2.570516586303711
Validation loss: 2.6919654518045406

Epoch: 5| Step: 10
Training loss: 3.217923641204834
Validation loss: 2.6920602757443666

Epoch: 53| Step: 0
Training loss: 2.6942238807678223
Validation loss: 2.692001373537125

Epoch: 5| Step: 1
Training loss: 2.8690614700317383
Validation loss: 2.6968653048238447

Epoch: 5| Step: 2
Training loss: 2.8252789974212646
Validation loss: 2.700263520722748

Epoch: 5| Step: 3
Training loss: 2.7139394283294678
Validation loss: 2.6993607116001908

Epoch: 5| Step: 4
Training loss: 2.7263035774230957
Validation loss: 2.702827174176452

Epoch: 5| Step: 5
Training loss: 2.5649914741516113
Validation loss: 2.704397101556101

Epoch: 5| Step: 6
Training loss: 3.2631659507751465
Validation loss: 2.710001773731683

Epoch: 5| Step: 7
Training loss: 2.887507915496826
Validation loss: 2.7196336433451664

Epoch: 5| Step: 8
Training loss: 2.851302146911621
Validation loss: 2.711844903166576

Epoch: 5| Step: 9
Training loss: 2.8182525634765625
Validation loss: 2.70219168355388

Epoch: 5| Step: 10
Training loss: 3.192192554473877
Validation loss: 2.6958119228322017

Epoch: 54| Step: 0
Training loss: 2.6565780639648438
Validation loss: 2.693453096574353

Epoch: 5| Step: 1
Training loss: 2.9906208515167236
Validation loss: 2.695202096816032

Epoch: 5| Step: 2
Training loss: 2.991781234741211
Validation loss: 2.690418143426218

Epoch: 5| Step: 3
Training loss: 3.003922462463379
Validation loss: 2.687373145934074

Epoch: 5| Step: 4
Training loss: 2.5908610820770264
Validation loss: 2.6813284197161273

Epoch: 5| Step: 5
Training loss: 2.4144978523254395
Validation loss: 2.6788391836227907

Epoch: 5| Step: 6
Training loss: 2.9597675800323486
Validation loss: 2.6816841530543503

Epoch: 5| Step: 7
Training loss: 2.759125232696533
Validation loss: 2.6781257326884935

Epoch: 5| Step: 8
Training loss: 2.5338387489318848
Validation loss: 2.6793656041545253

Epoch: 5| Step: 9
Training loss: 3.3084774017333984
Validation loss: 2.6803329119118313

Epoch: 5| Step: 10
Training loss: 3.0456132888793945
Validation loss: 2.683585423295216

Epoch: 55| Step: 0
Training loss: 2.8207106590270996
Validation loss: 2.689657034412507

Epoch: 5| Step: 1
Training loss: 2.4400992393493652
Validation loss: 2.690760374069214

Epoch: 5| Step: 2
Training loss: 2.81239914894104
Validation loss: 2.686151232770694

Epoch: 5| Step: 3
Training loss: 3.0918283462524414
Validation loss: 2.685811827259679

Epoch: 5| Step: 4
Training loss: 3.7713215351104736
Validation loss: 2.68095144917888

Epoch: 5| Step: 5
Training loss: 2.5784718990325928
Validation loss: 2.6827681910607124

Epoch: 5| Step: 6
Training loss: 3.6748268604278564
Validation loss: 2.681876459429341

Epoch: 5| Step: 7
Training loss: 2.43715763092041
Validation loss: 2.6810424174031904

Epoch: 5| Step: 8
Training loss: 2.98936128616333
Validation loss: 2.6816184084902526

Epoch: 5| Step: 9
Training loss: 2.1477413177490234
Validation loss: 2.680706044679047

Epoch: 5| Step: 10
Training loss: 2.3326213359832764
Validation loss: 2.681316375732422

Epoch: 56| Step: 0
Training loss: 2.545297622680664
Validation loss: 2.6777912801311863

Epoch: 5| Step: 1
Training loss: 3.481649398803711
Validation loss: 2.6766007561837473

Epoch: 5| Step: 2
Training loss: 2.7444326877593994
Validation loss: 2.676027121082429

Epoch: 5| Step: 3
Training loss: 2.543976306915283
Validation loss: 2.676574114830263

Epoch: 5| Step: 4
Training loss: 3.1716699600219727
Validation loss: 2.676368021195935

Epoch: 5| Step: 5
Training loss: 2.857377529144287
Validation loss: 2.6785461441163094

Epoch: 5| Step: 6
Training loss: 2.5278677940368652
Validation loss: 2.678577599986907

Epoch: 5| Step: 7
Training loss: 3.051931381225586
Validation loss: 2.677827368500412

Epoch: 5| Step: 8
Training loss: 3.013341188430786
Validation loss: 2.679244210643153

Epoch: 5| Step: 9
Training loss: 2.252671718597412
Validation loss: 2.680273114994008

Epoch: 5| Step: 10
Training loss: 2.951939582824707
Validation loss: 2.679528364571192

Epoch: 57| Step: 0
Training loss: 3.280815839767456
Validation loss: 2.6745195619521605

Epoch: 5| Step: 1
Training loss: 2.2157583236694336
Validation loss: 2.6777469317118325

Epoch: 5| Step: 2
Training loss: 2.530543327331543
Validation loss: 2.67967475357876

Epoch: 5| Step: 3
Training loss: 2.768533945083618
Validation loss: 2.677253528307843

Epoch: 5| Step: 4
Training loss: 3.5441455841064453
Validation loss: 2.675417761648855

Epoch: 5| Step: 5
Training loss: 2.4327075481414795
Validation loss: 2.674056189034575

Epoch: 5| Step: 6
Training loss: 3.0338919162750244
Validation loss: 2.6708648076621433

Epoch: 5| Step: 7
Training loss: 2.943777561187744
Validation loss: 2.6731349370812856

Epoch: 5| Step: 8
Training loss: 2.2297706604003906
Validation loss: 2.671577956086846

Epoch: 5| Step: 9
Training loss: 2.3452277183532715
Validation loss: 2.674660728823754

Epoch: 5| Step: 10
Training loss: 3.9751057624816895
Validation loss: 2.674823781495453

Epoch: 58| Step: 0
Training loss: 2.791386365890503
Validation loss: 2.66999545661352

Epoch: 5| Step: 1
Training loss: 2.51863431930542
Validation loss: 2.67365603293142

Epoch: 5| Step: 2
Training loss: 3.527566909790039
Validation loss: 2.6719340893530075

Epoch: 5| Step: 3
Training loss: 2.4205164909362793
Validation loss: 2.6740066466792936

Epoch: 5| Step: 4
Training loss: 2.6264312267303467
Validation loss: 2.671638481078609

Epoch: 5| Step: 5
Training loss: 2.7521462440490723
Validation loss: 2.6697665260684107

Epoch: 5| Step: 6
Training loss: 2.305098295211792
Validation loss: 2.675878583744008

Epoch: 5| Step: 7
Training loss: 3.28334379196167
Validation loss: 2.6719272546870734

Epoch: 5| Step: 8
Training loss: 2.9543824195861816
Validation loss: 2.6743833993070867

Epoch: 5| Step: 9
Training loss: 2.9394211769104004
Validation loss: 2.6724160691743255

Epoch: 5| Step: 10
Training loss: 3.029993772506714
Validation loss: 2.675540008852559

Epoch: 59| Step: 0
Training loss: 3.143911361694336
Validation loss: 2.675823470597626

Epoch: 5| Step: 1
Training loss: 2.636948347091675
Validation loss: 2.6713038490664576

Epoch: 5| Step: 2
Training loss: 2.806820869445801
Validation loss: 2.6676765667494906

Epoch: 5| Step: 3
Training loss: 3.3142666816711426
Validation loss: 2.6676573958448184

Epoch: 5| Step: 4
Training loss: 2.9081034660339355
Validation loss: 2.6674636333219466

Epoch: 5| Step: 5
Training loss: 2.8489036560058594
Validation loss: 2.668009202967408

Epoch: 5| Step: 6
Training loss: 2.3908932209014893
Validation loss: 2.6680605590984388

Epoch: 5| Step: 7
Training loss: 3.103008270263672
Validation loss: 2.6686129364916074

Epoch: 5| Step: 8
Training loss: 2.7600491046905518
Validation loss: 2.666340499795893

Epoch: 5| Step: 9
Training loss: 2.744777202606201
Validation loss: 2.6662432045064945

Epoch: 5| Step: 10
Training loss: 2.352924108505249
Validation loss: 2.667851710832247

Epoch: 60| Step: 0
Training loss: 3.6163673400878906
Validation loss: 2.66859148651041

Epoch: 5| Step: 1
Training loss: 3.087642192840576
Validation loss: 2.6626429327072634

Epoch: 5| Step: 2
Training loss: 2.5321755409240723
Validation loss: 2.6614061940100884

Epoch: 5| Step: 3
Training loss: 3.3170666694641113
Validation loss: 2.6665140544214556

Epoch: 5| Step: 4
Training loss: 1.956117868423462
Validation loss: 2.663406008033342

Epoch: 5| Step: 5
Training loss: 2.421280860900879
Validation loss: 2.6671526867856263

Epoch: 5| Step: 6
Training loss: 3.1516146659851074
Validation loss: 2.66187366618905

Epoch: 5| Step: 7
Training loss: 2.8573176860809326
Validation loss: 2.6608425237799205

Epoch: 5| Step: 8
Training loss: 2.4572646617889404
Validation loss: 2.664233351266512

Epoch: 5| Step: 9
Training loss: 2.6040279865264893
Validation loss: 2.661560889213316

Epoch: 5| Step: 10
Training loss: 3.0590577125549316
Validation loss: 2.6646535986213276

Epoch: 61| Step: 0
Training loss: 3.3688957691192627
Validation loss: 2.667053955857472

Epoch: 5| Step: 1
Training loss: 2.140552043914795
Validation loss: 2.6653855769864974

Epoch: 5| Step: 2
Training loss: 2.7031497955322266
Validation loss: 2.667988689996863

Epoch: 5| Step: 3
Training loss: 2.983809232711792
Validation loss: 2.666530445057859

Epoch: 5| Step: 4
Training loss: 2.6079208850860596
Validation loss: 2.6621537388011975

Epoch: 5| Step: 5
Training loss: 2.7464680671691895
Validation loss: 2.659766815041983

Epoch: 5| Step: 6
Training loss: 2.327326536178589
Validation loss: 2.6610674063364663

Epoch: 5| Step: 7
Training loss: 3.2854220867156982
Validation loss: 2.660671014939585

Epoch: 5| Step: 8
Training loss: 3.0982165336608887
Validation loss: 2.668330269475137

Epoch: 5| Step: 9
Training loss: 2.3830039501190186
Validation loss: 2.6746999063799457

Epoch: 5| Step: 10
Training loss: 3.379744052886963
Validation loss: 2.7067155863649104

Epoch: 62| Step: 0
Training loss: 2.577786445617676
Validation loss: 2.711914284254915

Epoch: 5| Step: 1
Training loss: 2.0199027061462402
Validation loss: 2.7120175848725023

Epoch: 5| Step: 2
Training loss: 3.2330398559570312
Validation loss: 2.7189144396012828

Epoch: 5| Step: 3
Training loss: 2.647566795349121
Validation loss: 2.7147360514569026

Epoch: 5| Step: 4
Training loss: 2.339785575866699
Validation loss: 2.686621589045371

Epoch: 5| Step: 5
Training loss: 2.4531915187835693
Validation loss: 2.658424603041782

Epoch: 5| Step: 6
Training loss: 2.648629665374756
Validation loss: 2.6584086187424196

Epoch: 5| Step: 7
Training loss: 3.8347480297088623
Validation loss: 2.6671222281712357

Epoch: 5| Step: 8
Training loss: 2.4488120079040527
Validation loss: 2.669895971975019

Epoch: 5| Step: 9
Training loss: 3.91491436958313
Validation loss: 2.6735467167310816

Epoch: 5| Step: 10
Training loss: 2.868194818496704
Validation loss: 2.672105280301904

Epoch: 63| Step: 0
Training loss: 1.8301042318344116
Validation loss: 2.6740399637529926

Epoch: 5| Step: 1
Training loss: 3.1627745628356934
Validation loss: 2.6703794669079524

Epoch: 5| Step: 2
Training loss: 2.724257707595825
Validation loss: 2.6625673411994852

Epoch: 5| Step: 3
Training loss: 2.6726841926574707
Validation loss: 2.6658016532979985

Epoch: 5| Step: 4
Training loss: 2.2586846351623535
Validation loss: 2.6672893570315455

Epoch: 5| Step: 5
Training loss: 3.6116232872009277
Validation loss: 2.6788452902147846

Epoch: 5| Step: 6
Training loss: 2.444129705429077
Validation loss: 2.679385867170108

Epoch: 5| Step: 7
Training loss: 3.146301746368408
Validation loss: 2.681747257068593

Epoch: 5| Step: 8
Training loss: 2.498032331466675
Validation loss: 2.678434520639399

Epoch: 5| Step: 9
Training loss: 3.6112887859344482
Validation loss: 2.6774340752632386

Epoch: 5| Step: 10
Training loss: 3.210348606109619
Validation loss: 2.670369912219304

Epoch: 64| Step: 0
Training loss: 2.695174217224121
Validation loss: 2.664011157968993

Epoch: 5| Step: 1
Training loss: 3.068800210952759
Validation loss: 2.663111545706308

Epoch: 5| Step: 2
Training loss: 3.273099899291992
Validation loss: 2.656928559785248

Epoch: 5| Step: 3
Training loss: 2.8479790687561035
Validation loss: 2.658747416670604

Epoch: 5| Step: 4
Training loss: 2.564263105392456
Validation loss: 2.6554279686302267

Epoch: 5| Step: 5
Training loss: 3.159919261932373
Validation loss: 2.658020570713987

Epoch: 5| Step: 6
Training loss: 3.4602222442626953
Validation loss: 2.662275986004901

Epoch: 5| Step: 7
Training loss: 2.30946683883667
Validation loss: 2.6646724336890766

Epoch: 5| Step: 8
Training loss: 2.238525390625
Validation loss: 2.6613780631813952

Epoch: 5| Step: 9
Training loss: 2.8499999046325684
Validation loss: 2.662341461386732

Epoch: 5| Step: 10
Training loss: 2.555903911590576
Validation loss: 2.6587889399579776

Epoch: 65| Step: 0
Training loss: 3.278806686401367
Validation loss: 2.6564273039499917

Epoch: 5| Step: 1
Training loss: 2.633676528930664
Validation loss: 2.654500881830851

Epoch: 5| Step: 2
Training loss: 2.7749340534210205
Validation loss: 2.6478869069007134

Epoch: 5| Step: 3
Training loss: 3.03645658493042
Validation loss: 2.646964124453965

Epoch: 5| Step: 4
Training loss: 2.518805742263794
Validation loss: 2.6469877176387335

Epoch: 5| Step: 5
Training loss: 3.129215717315674
Validation loss: 2.64517391368907

Epoch: 5| Step: 6
Training loss: 2.6356425285339355
Validation loss: 2.6435263900346655

Epoch: 5| Step: 7
Training loss: 2.129946708679199
Validation loss: 2.646398175147272

Epoch: 5| Step: 8
Training loss: 2.6961426734924316
Validation loss: 2.6449940332802395

Epoch: 5| Step: 9
Training loss: 3.4217567443847656
Validation loss: 2.648004344714585

Epoch: 5| Step: 10
Training loss: 2.6661593914031982
Validation loss: 2.6427446847320883

Epoch: 66| Step: 0
Training loss: 2.7315492630004883
Validation loss: 2.644144353046212

Epoch: 5| Step: 1
Training loss: 2.511535406112671
Validation loss: 2.6443351776369157

Epoch: 5| Step: 2
Training loss: 2.854553699493408
Validation loss: 2.645246426264445

Epoch: 5| Step: 3
Training loss: 2.428985118865967
Validation loss: 2.646923736859393

Epoch: 5| Step: 4
Training loss: 3.0848922729492188
Validation loss: 2.6447769134275374

Epoch: 5| Step: 5
Training loss: 2.98534893989563
Validation loss: 2.648628419445407

Epoch: 5| Step: 6
Training loss: 2.28267240524292
Validation loss: 2.6486068951186312

Epoch: 5| Step: 7
Training loss: 3.5785140991210938
Validation loss: 2.6590028783326507

Epoch: 5| Step: 8
Training loss: 2.4468085765838623
Validation loss: 2.6567645354937484

Epoch: 5| Step: 9
Training loss: 2.6985983848571777
Validation loss: 2.6680423828863327

Epoch: 5| Step: 10
Training loss: 3.288424491882324
Validation loss: 2.6776128507429555

Epoch: 67| Step: 0
Training loss: 3.286545991897583
Validation loss: 2.676090519915345

Epoch: 5| Step: 1
Training loss: 2.66813325881958
Validation loss: 2.6732016019923712

Epoch: 5| Step: 2
Training loss: 3.0374526977539062
Validation loss: 2.6516195369023148

Epoch: 5| Step: 3
Training loss: 2.3487377166748047
Validation loss: 2.64727283036837

Epoch: 5| Step: 4
Training loss: 3.3642826080322266
Validation loss: 2.64185086629724

Epoch: 5| Step: 5
Training loss: 3.305772066116333
Validation loss: 2.642672231120448

Epoch: 5| Step: 6
Training loss: 2.3551688194274902
Validation loss: 2.6419360714574016

Epoch: 5| Step: 7
Training loss: 2.812671422958374
Validation loss: 2.6369583299083095

Epoch: 5| Step: 8
Training loss: 2.7383618354797363
Validation loss: 2.6386268651613625

Epoch: 5| Step: 9
Training loss: 2.8342018127441406
Validation loss: 2.639516791989726

Epoch: 5| Step: 10
Training loss: 1.9709104299545288
Validation loss: 2.6381346794866745

Epoch: 68| Step: 0
Training loss: 2.8188724517822266
Validation loss: 2.6425301977383193

Epoch: 5| Step: 1
Training loss: 2.708922863006592
Validation loss: 2.6442314783732095

Epoch: 5| Step: 2
Training loss: 2.3030483722686768
Validation loss: 2.646950278230893

Epoch: 5| Step: 3
Training loss: 3.1791152954101562
Validation loss: 2.642943272026636

Epoch: 5| Step: 4
Training loss: 3.0934548377990723
Validation loss: 2.6399078215322187

Epoch: 5| Step: 5
Training loss: 3.6105427742004395
Validation loss: 2.637948200266848

Epoch: 5| Step: 6
Training loss: 3.253265857696533
Validation loss: 2.637401339828327

Epoch: 5| Step: 7
Training loss: 2.3185646533966064
Validation loss: 2.632439474905691

Epoch: 5| Step: 8
Training loss: 1.9933960437774658
Validation loss: 2.6375955689337944

Epoch: 5| Step: 9
Training loss: 2.7587380409240723
Validation loss: 2.635675940462338

Epoch: 5| Step: 10
Training loss: 2.8487699031829834
Validation loss: 2.6304942766825357

Epoch: 69| Step: 0
Training loss: 2.076871395111084
Validation loss: 2.6382245222727456

Epoch: 5| Step: 1
Training loss: 2.7802505493164062
Validation loss: 2.6331938082172024

Epoch: 5| Step: 2
Training loss: 2.776087522506714
Validation loss: 2.634138213690891

Epoch: 5| Step: 3
Training loss: 2.901737928390503
Validation loss: 2.635414787518081

Epoch: 5| Step: 4
Training loss: 2.849487781524658
Validation loss: 2.6313618331827144

Epoch: 5| Step: 5
Training loss: 2.8437092304229736
Validation loss: 2.6332004403555267

Epoch: 5| Step: 6
Training loss: 3.0259275436401367
Validation loss: 2.6308503279121975

Epoch: 5| Step: 7
Training loss: 2.4950788021087646
Validation loss: 2.6314365735618015

Epoch: 5| Step: 8
Training loss: 2.7949483394622803
Validation loss: 2.629568648594682

Epoch: 5| Step: 9
Training loss: 3.4099509716033936
Validation loss: 2.6287361934620845

Epoch: 5| Step: 10
Training loss: 2.792848825454712
Validation loss: 2.628635711567376

Epoch: 70| Step: 0
Training loss: 3.1765646934509277
Validation loss: 2.6278939759859474

Epoch: 5| Step: 1
Training loss: 3.0340240001678467
Validation loss: 2.632504406795707

Epoch: 5| Step: 2
Training loss: 2.393902540206909
Validation loss: 2.6261968971580587

Epoch: 5| Step: 3
Training loss: 2.846278667449951
Validation loss: 2.626701536998954

Epoch: 5| Step: 4
Training loss: 3.208402156829834
Validation loss: 2.6286578588588263

Epoch: 5| Step: 5
Training loss: 3.255924940109253
Validation loss: 2.626904523500832

Epoch: 5| Step: 6
Training loss: 2.541611433029175
Validation loss: 2.624745527903239

Epoch: 5| Step: 7
Training loss: 1.7404648065567017
Validation loss: 2.623352753218784

Epoch: 5| Step: 8
Training loss: 2.611226797103882
Validation loss: 2.6214376623912523

Epoch: 5| Step: 9
Training loss: 3.0426230430603027
Validation loss: 2.6209718437605005

Epoch: 5| Step: 10
Training loss: 2.858781337738037
Validation loss: 2.622833281434992

Epoch: 71| Step: 0
Training loss: 1.7456371784210205
Validation loss: 2.6196857113992014

Epoch: 5| Step: 1
Training loss: 2.869469404220581
Validation loss: 2.62101988638601

Epoch: 5| Step: 2
Training loss: 1.9896507263183594
Validation loss: 2.6238593670629684

Epoch: 5| Step: 3
Training loss: 3.6590964794158936
Validation loss: 2.625723123550415

Epoch: 5| Step: 4
Training loss: 2.587583303451538
Validation loss: 2.623645095415013

Epoch: 5| Step: 5
Training loss: 2.412466287612915
Validation loss: 2.6286657497447026

Epoch: 5| Step: 6
Training loss: 3.1132683753967285
Validation loss: 2.626772613935573

Epoch: 5| Step: 7
Training loss: 2.951956272125244
Validation loss: 2.620676919978152

Epoch: 5| Step: 8
Training loss: 3.247645616531372
Validation loss: 2.621953018250004

Epoch: 5| Step: 9
Training loss: 2.912501096725464
Validation loss: 2.6214211397273566

Epoch: 5| Step: 10
Training loss: 3.2647645473480225
Validation loss: 2.6201809478062454

Epoch: 72| Step: 0
Training loss: 2.6001999378204346
Validation loss: 2.621793782839211

Epoch: 5| Step: 1
Training loss: 3.4013404846191406
Validation loss: 2.6205311667534614

Epoch: 5| Step: 2
Training loss: 2.4615302085876465
Validation loss: 2.6175472018539265

Epoch: 5| Step: 3
Training loss: 3.0421228408813477
Validation loss: 2.618632880590295

Epoch: 5| Step: 4
Training loss: 2.388434886932373
Validation loss: 2.6185120254434566

Epoch: 5| Step: 5
Training loss: 2.317166566848755
Validation loss: 2.6187959409529165

Epoch: 5| Step: 6
Training loss: 2.383603572845459
Validation loss: 2.6187286825590235

Epoch: 5| Step: 7
Training loss: 3.3852856159210205
Validation loss: 2.6151621956979074

Epoch: 5| Step: 8
Training loss: 2.7559731006622314
Validation loss: 2.618334709957082

Epoch: 5| Step: 9
Training loss: 3.242234468460083
Validation loss: 2.6152871859970914

Epoch: 5| Step: 10
Training loss: 2.6422126293182373
Validation loss: 2.6178569768064763

Epoch: 73| Step: 0
Training loss: 2.880147933959961
Validation loss: 2.621489465877574

Epoch: 5| Step: 1
Training loss: 2.6944165229797363
Validation loss: 2.619608025397024

Epoch: 5| Step: 2
Training loss: 3.3491463661193848
Validation loss: 2.6179975540407243

Epoch: 5| Step: 3
Training loss: 3.246112823486328
Validation loss: 2.6211058785838466

Epoch: 5| Step: 4
Training loss: 2.5554776191711426
Validation loss: 2.620256244495351

Epoch: 5| Step: 5
Training loss: 2.526615858078003
Validation loss: 2.6262856503968597

Epoch: 5| Step: 6
Training loss: 3.0139379501342773
Validation loss: 2.6346071330449914

Epoch: 5| Step: 7
Training loss: 2.331195831298828
Validation loss: 2.6313068661638486

Epoch: 5| Step: 8
Training loss: 2.678027868270874
Validation loss: 2.6299752804540817

Epoch: 5| Step: 9
Training loss: 2.9037468433380127
Validation loss: 2.628834088643392

Epoch: 5| Step: 10
Training loss: 2.4302713871002197
Validation loss: 2.6255523799568095

Epoch: 74| Step: 0
Training loss: 2.439728260040283
Validation loss: 2.6177339092377694

Epoch: 5| Step: 1
Training loss: 3.179666757583618
Validation loss: 2.619732936223348

Epoch: 5| Step: 2
Training loss: 2.279674768447876
Validation loss: 2.6205493891110985

Epoch: 5| Step: 3
Training loss: 2.935438394546509
Validation loss: 2.6159865497260966

Epoch: 5| Step: 4
Training loss: 2.4566166400909424
Validation loss: 2.614899742987848

Epoch: 5| Step: 5
Training loss: 2.907442569732666
Validation loss: 2.6185351853729575

Epoch: 5| Step: 6
Training loss: 2.5781455039978027
Validation loss: 2.614950882491245

Epoch: 5| Step: 7
Training loss: 3.3098533153533936
Validation loss: 2.6128267857336227

Epoch: 5| Step: 8
Training loss: 2.8319551944732666
Validation loss: 2.6113978996071765

Epoch: 5| Step: 9
Training loss: 2.832561731338501
Validation loss: 2.6126125474129953

Epoch: 5| Step: 10
Training loss: 2.8931884765625
Validation loss: 2.610687942915065

Epoch: 75| Step: 0
Training loss: 2.976271152496338
Validation loss: 2.6094434107503583

Epoch: 5| Step: 1
Training loss: 2.6851303577423096
Validation loss: 2.613263632661553

Epoch: 5| Step: 2
Training loss: 2.593756914138794
Validation loss: 2.6087098044733845

Epoch: 5| Step: 3
Training loss: 3.7487595081329346
Validation loss: 2.6077114587189048

Epoch: 5| Step: 4
Training loss: 2.6176695823669434
Validation loss: 2.607238036330028

Epoch: 5| Step: 5
Training loss: 2.9350733757019043
Validation loss: 2.609464813304204

Epoch: 5| Step: 6
Training loss: 2.688721179962158
Validation loss: 2.609341864944786

Epoch: 5| Step: 7
Training loss: 2.5438175201416016
Validation loss: 2.6112053958318566

Epoch: 5| Step: 8
Training loss: 3.3971107006073
Validation loss: 2.610330084318756

Epoch: 5| Step: 9
Training loss: 2.107912540435791
Validation loss: 2.6090770639399046

Epoch: 5| Step: 10
Training loss: 2.169759511947632
Validation loss: 2.608011781528432

Epoch: 76| Step: 0
Training loss: 2.3814549446105957
Validation loss: 2.6113182114016626

Epoch: 5| Step: 1
Training loss: 2.449357509613037
Validation loss: 2.614438303055302

Epoch: 5| Step: 2
Training loss: 2.142320156097412
Validation loss: 2.6107833052194245

Epoch: 5| Step: 3
Training loss: 3.3837921619415283
Validation loss: 2.6176078832277687

Epoch: 5| Step: 4
Training loss: 3.6363816261291504
Validation loss: 2.607276185866325

Epoch: 5| Step: 5
Training loss: 3.082580089569092
Validation loss: 2.605465894104332

Epoch: 5| Step: 6
Training loss: 3.001343250274658
Validation loss: 2.603532632191976

Epoch: 5| Step: 7
Training loss: 2.523711919784546
Validation loss: 2.6031498037358767

Epoch: 5| Step: 8
Training loss: 2.520420551300049
Validation loss: 2.6035195345519693

Epoch: 5| Step: 9
Training loss: 2.9649338722229004
Validation loss: 2.6068909142607

Epoch: 5| Step: 10
Training loss: 2.4192605018615723
Validation loss: 2.604961866973549

Epoch: 77| Step: 0
Training loss: 2.4119157791137695
Validation loss: 2.606058415546212

Epoch: 5| Step: 1
Training loss: 3.3429653644561768
Validation loss: 2.6067406926103818

Epoch: 5| Step: 2
Training loss: 2.122732639312744
Validation loss: 2.6081543686569377

Epoch: 5| Step: 3
Training loss: 2.316117763519287
Validation loss: 2.6099935552125335

Epoch: 5| Step: 4
Training loss: 3.282651424407959
Validation loss: 2.615692033562609

Epoch: 5| Step: 5
Training loss: 2.3801276683807373
Validation loss: 2.6144300609506588

Epoch: 5| Step: 6
Training loss: 2.4517037868499756
Validation loss: 2.6139552695776826

Epoch: 5| Step: 7
Training loss: 3.1741580963134766
Validation loss: 2.607632995933615

Epoch: 5| Step: 8
Training loss: 2.924854278564453
Validation loss: 2.6048098700020903

Epoch: 5| Step: 9
Training loss: 2.9037373065948486
Validation loss: 2.602538529262748

Epoch: 5| Step: 10
Training loss: 3.3018112182617188
Validation loss: 2.6031336630544355

Epoch: 78| Step: 0
Training loss: 2.2498538494110107
Validation loss: 2.6023623404964322

Epoch: 5| Step: 1
Training loss: 3.0575740337371826
Validation loss: 2.6030084138275473

Epoch: 5| Step: 2
Training loss: 2.6775128841400146
Validation loss: 2.6042572144539125

Epoch: 5| Step: 3
Training loss: 2.354316473007202
Validation loss: 2.6029488104645924

Epoch: 5| Step: 4
Training loss: 2.766167163848877
Validation loss: 2.604765686937558

Epoch: 5| Step: 5
Training loss: 2.9512269496917725
Validation loss: 2.6025643912694787

Epoch: 5| Step: 6
Training loss: 3.2761950492858887
Validation loss: 2.603833783057428

Epoch: 5| Step: 7
Training loss: 3.253683567047119
Validation loss: 2.6037094541775283

Epoch: 5| Step: 8
Training loss: 3.0583393573760986
Validation loss: 2.6039606960870887

Epoch: 5| Step: 9
Training loss: 2.3121590614318848
Validation loss: 2.6047591060720463

Epoch: 5| Step: 10
Training loss: 2.5002527236938477
Validation loss: 2.6002851865624868

Epoch: 79| Step: 0
Training loss: 3.2972521781921387
Validation loss: 2.599404586258755

Epoch: 5| Step: 1
Training loss: 2.5658936500549316
Validation loss: 2.605655449692921

Epoch: 5| Step: 2
Training loss: 2.959819793701172
Validation loss: 2.606215161661948

Epoch: 5| Step: 3
Training loss: 3.42958402633667
Validation loss: 2.6083263709980953

Epoch: 5| Step: 4
Training loss: 2.5926473140716553
Validation loss: 2.6031859510688373

Epoch: 5| Step: 5
Training loss: 2.0967941284179688
Validation loss: 2.6081361411720194

Epoch: 5| Step: 6
Training loss: 2.8330769538879395
Validation loss: 2.6017466001613165

Epoch: 5| Step: 7
Training loss: 2.239048480987549
Validation loss: 2.602272997620285

Epoch: 5| Step: 8
Training loss: 2.469494104385376
Validation loss: 2.596858086124543

Epoch: 5| Step: 9
Training loss: 3.013030767440796
Validation loss: 2.5970671279456026

Epoch: 5| Step: 10
Training loss: 3.0281784534454346
Validation loss: 2.597268050716769

Epoch: 80| Step: 0
Training loss: 2.182133436203003
Validation loss: 2.6029499679483394

Epoch: 5| Step: 1
Training loss: 3.1378726959228516
Validation loss: 2.6019897794210785

Epoch: 5| Step: 2
Training loss: 2.5327823162078857
Validation loss: 2.5991403672002975

Epoch: 5| Step: 3
Training loss: 2.822659492492676
Validation loss: 2.5977058897736254

Epoch: 5| Step: 4
Training loss: 2.7787628173828125
Validation loss: 2.5999233825232393

Epoch: 5| Step: 5
Training loss: 2.3653087615966797
Validation loss: 2.5991747738212667

Epoch: 5| Step: 6
Training loss: 3.7110695838928223
Validation loss: 2.6001456014571653

Epoch: 5| Step: 7
Training loss: 2.725465774536133
Validation loss: 2.600037395313222

Epoch: 5| Step: 8
Training loss: 2.6802306175231934
Validation loss: 2.597619505338771

Epoch: 5| Step: 9
Training loss: 2.7848167419433594
Validation loss: 2.5989615532659713

Epoch: 5| Step: 10
Training loss: 2.7193496227264404
Validation loss: 2.5975732521344255

Epoch: 81| Step: 0
Training loss: 3.3453757762908936
Validation loss: 2.5969381204215427

Epoch: 5| Step: 1
Training loss: 2.742363452911377
Validation loss: 2.601102139360161

Epoch: 5| Step: 2
Training loss: 2.57244873046875
Validation loss: 2.600315393940095

Epoch: 5| Step: 3
Training loss: 2.7046573162078857
Validation loss: 2.5958284267815213

Epoch: 5| Step: 4
Training loss: 1.9778200387954712
Validation loss: 2.600757537349578

Epoch: 5| Step: 5
Training loss: 3.3674685955047607
Validation loss: 2.5992939215834423

Epoch: 5| Step: 6
Training loss: 1.856713056564331
Validation loss: 2.602527733772032

Epoch: 5| Step: 7
Training loss: 3.1003823280334473
Validation loss: 2.6020545985109065

Epoch: 5| Step: 8
Training loss: 2.689711332321167
Validation loss: 2.6059564082853255

Epoch: 5| Step: 9
Training loss: 3.129362106323242
Validation loss: 2.602718796781314

Epoch: 5| Step: 10
Training loss: 2.9188473224639893
Validation loss: 2.6014349332419773

Epoch: 82| Step: 0
Training loss: 2.117946147918701
Validation loss: 2.600233680458479

Epoch: 5| Step: 1
Training loss: 3.1533291339874268
Validation loss: 2.604959967315838

Epoch: 5| Step: 2
Training loss: 2.6929984092712402
Validation loss: 2.608064543816351

Epoch: 5| Step: 3
Training loss: 2.4949135780334473
Validation loss: 2.606331771419894

Epoch: 5| Step: 4
Training loss: 2.8302884101867676
Validation loss: 2.6078455768605715

Epoch: 5| Step: 5
Training loss: 2.82544207572937
Validation loss: 2.610174984060308

Epoch: 5| Step: 6
Training loss: 2.6738147735595703
Validation loss: 2.615012184266121

Epoch: 5| Step: 7
Training loss: 2.7258810997009277
Validation loss: 2.6140338246540358

Epoch: 5| Step: 8
Training loss: 3.009394884109497
Validation loss: 2.6039581350100938

Epoch: 5| Step: 9
Training loss: 3.1038565635681152
Validation loss: 2.6018044025667253

Epoch: 5| Step: 10
Training loss: 2.830472230911255
Validation loss: 2.5929076440872683

Epoch: 83| Step: 0
Training loss: 2.5958473682403564
Validation loss: 2.5924179759076846

Epoch: 5| Step: 1
Training loss: 3.1636691093444824
Validation loss: 2.5977134755862656

Epoch: 5| Step: 2
Training loss: 2.2755744457244873
Validation loss: 2.6018444133061234

Epoch: 5| Step: 3
Training loss: 3.023848056793213
Validation loss: 2.6066088214997323

Epoch: 5| Step: 4
Training loss: 2.722282886505127
Validation loss: 2.6123045285542807

Epoch: 5| Step: 5
Training loss: 3.400780200958252
Validation loss: 2.609588248755342

Epoch: 5| Step: 6
Training loss: 2.7595393657684326
Validation loss: 2.6182958567014305

Epoch: 5| Step: 7
Training loss: 3.145503282546997
Validation loss: 2.612699439448695

Epoch: 5| Step: 8
Training loss: 1.9743961095809937
Validation loss: 2.6097706722956833

Epoch: 5| Step: 9
Training loss: 3.2649543285369873
Validation loss: 2.6041565684862036

Epoch: 5| Step: 10
Training loss: 2.157332181930542
Validation loss: 2.5983117729104976

Epoch: 84| Step: 0
Training loss: 2.0721945762634277
Validation loss: 2.595910187690489

Epoch: 5| Step: 1
Training loss: 3.7922987937927246
Validation loss: 2.592205680826659

Epoch: 5| Step: 2
Training loss: 2.4374589920043945
Validation loss: 2.588902117103659

Epoch: 5| Step: 3
Training loss: 2.9028913974761963
Validation loss: 2.5949804039411646

Epoch: 5| Step: 4
Training loss: 2.681455612182617
Validation loss: 2.602512210927984

Epoch: 5| Step: 5
Training loss: 2.9056522846221924
Validation loss: 2.5969630056811916

Epoch: 5| Step: 6
Training loss: 3.6058897972106934
Validation loss: 2.6021880539514686

Epoch: 5| Step: 7
Training loss: 2.533170700073242
Validation loss: 2.603131389105192

Epoch: 5| Step: 8
Training loss: 2.373342752456665
Validation loss: 2.6093978561380857

Epoch: 5| Step: 9
Training loss: 2.5817062854766846
Validation loss: 2.6208594870823685

Epoch: 5| Step: 10
Training loss: 2.495081901550293
Validation loss: 2.6284683391612065

Epoch: 85| Step: 0
Training loss: 2.854419231414795
Validation loss: 2.6410024114834365

Epoch: 5| Step: 1
Training loss: 1.981652021408081
Validation loss: 2.62012045614181

Epoch: 5| Step: 2
Training loss: 3.8092243671417236
Validation loss: 2.6123081381602953

Epoch: 5| Step: 3
Training loss: 2.145646572113037
Validation loss: 2.6040746165860083

Epoch: 5| Step: 4
Training loss: 2.3801865577697754
Validation loss: 2.5915121698892243

Epoch: 5| Step: 5
Training loss: 2.5054450035095215
Validation loss: 2.586501416339669

Epoch: 5| Step: 6
Training loss: 3.2055678367614746
Validation loss: 2.592477616443429

Epoch: 5| Step: 7
Training loss: 3.2202370166778564
Validation loss: 2.6002745628356934

Epoch: 5| Step: 8
Training loss: 2.80794095993042
Validation loss: 2.601565489204981

Epoch: 5| Step: 9
Training loss: 2.4565224647521973
Validation loss: 2.609544987319618

Epoch: 5| Step: 10
Training loss: 3.248741388320923
Validation loss: 2.6099712130843953

Epoch: 86| Step: 0
Training loss: 2.7646827697753906
Validation loss: 2.607935477328557

Epoch: 5| Step: 1
Training loss: 2.41369366645813
Validation loss: 2.6200980832499843

Epoch: 5| Step: 2
Training loss: 2.777261257171631
Validation loss: 2.609399067458286

Epoch: 5| Step: 3
Training loss: 2.999293804168701
Validation loss: 2.621585064036872

Epoch: 5| Step: 4
Training loss: 3.295513868331909
Validation loss: 2.6069132563888386

Epoch: 5| Step: 5
Training loss: 3.432593584060669
Validation loss: 2.5960950313075895

Epoch: 5| Step: 6
Training loss: 2.859285831451416
Validation loss: 2.598787958903979

Epoch: 5| Step: 7
Training loss: 2.1370577812194824
Validation loss: 2.592865877254035

Epoch: 5| Step: 8
Training loss: 2.431755542755127
Validation loss: 2.5875520654903945

Epoch: 5| Step: 9
Training loss: 1.9564107656478882
Validation loss: 2.591277519861857

Epoch: 5| Step: 10
Training loss: 3.4940483570098877
Validation loss: 2.585333190938478

Epoch: 87| Step: 0
Training loss: 1.9573218822479248
Validation loss: 2.589426863578058

Epoch: 5| Step: 1
Training loss: 3.3798935413360596
Validation loss: 2.591926800307407

Epoch: 5| Step: 2
Training loss: 2.7930710315704346
Validation loss: 2.5897146937667683

Epoch: 5| Step: 3
Training loss: 2.741490602493286
Validation loss: 2.5900577704111734

Epoch: 5| Step: 4
Training loss: 2.7941527366638184
Validation loss: 2.590542624073644

Epoch: 5| Step: 5
Training loss: 2.6397528648376465
Validation loss: 2.596410564197007

Epoch: 5| Step: 6
Training loss: 3.143230676651001
Validation loss: 2.601785385480491

Epoch: 5| Step: 7
Training loss: 2.758343458175659
Validation loss: 2.6099079962699645

Epoch: 5| Step: 8
Training loss: 2.841912031173706
Validation loss: 2.606112631418372

Epoch: 5| Step: 9
Training loss: 3.2265563011169434
Validation loss: 2.598667647248955

Epoch: 5| Step: 10
Training loss: 2.0245096683502197
Validation loss: 2.605113706281108

Epoch: 88| Step: 0
Training loss: 2.9194271564483643
Validation loss: 2.5989392034469114

Epoch: 5| Step: 1
Training loss: 3.3105061054229736
Validation loss: 2.6057712134494575

Epoch: 5| Step: 2
Training loss: 2.9484286308288574
Validation loss: 2.6082417734207644

Epoch: 5| Step: 3
Training loss: 3.243238925933838
Validation loss: 2.5946447900546494

Epoch: 5| Step: 4
Training loss: 2.1057279109954834
Validation loss: 2.597294142169337

Epoch: 5| Step: 5
Training loss: 2.7336783409118652
Validation loss: 2.5874667142027166

Epoch: 5| Step: 6
Training loss: 2.545034885406494
Validation loss: 2.582453143212103

Epoch: 5| Step: 7
Training loss: 2.384089708328247
Validation loss: 2.5822676843212498

Epoch: 5| Step: 8
Training loss: 2.9040846824645996
Validation loss: 2.5915841287182224

Epoch: 5| Step: 9
Training loss: 2.7171878814697266
Validation loss: 2.594521117466752

Epoch: 5| Step: 10
Training loss: 2.4459264278411865
Validation loss: 2.597243078293339

Epoch: 89| Step: 0
Training loss: 2.41731858253479
Validation loss: 2.5910663143281014

Epoch: 5| Step: 1
Training loss: 2.4551913738250732
Validation loss: 2.587157298159856

Epoch: 5| Step: 2
Training loss: 2.6995646953582764
Validation loss: 2.5899137707166773

Epoch: 5| Step: 3
Training loss: 3.133699655532837
Validation loss: 2.588677026892221

Epoch: 5| Step: 4
Training loss: 2.7242259979248047
Validation loss: 2.5858216875342914

Epoch: 5| Step: 5
Training loss: 2.374659538269043
Validation loss: 2.5827137449736237

Epoch: 5| Step: 6
Training loss: 3.8385729789733887
Validation loss: 2.5803206428404777

Epoch: 5| Step: 7
Training loss: 3.181999683380127
Validation loss: 2.5802295566886984

Epoch: 5| Step: 8
Training loss: 2.4282331466674805
Validation loss: 2.5786025229320733

Epoch: 5| Step: 9
Training loss: 2.194223165512085
Validation loss: 2.5778223955503075

Epoch: 5| Step: 10
Training loss: 2.8735365867614746
Validation loss: 2.579565904473746

Epoch: 90| Step: 0
Training loss: 3.047093629837036
Validation loss: 2.5810836950937905

Epoch: 5| Step: 1
Training loss: 2.8668503761291504
Validation loss: 2.5786282375294673

Epoch: 5| Step: 2
Training loss: 2.4472649097442627
Validation loss: 2.58396238921791

Epoch: 5| Step: 3
Training loss: 3.5701630115509033
Validation loss: 2.5800857915673205

Epoch: 5| Step: 4
Training loss: 2.31638765335083
Validation loss: 2.5835327615020094

Epoch: 5| Step: 5
Training loss: 2.884077310562134
Validation loss: 2.579496850249588

Epoch: 5| Step: 6
Training loss: 2.8555397987365723
Validation loss: 2.5819250511866745

Epoch: 5| Step: 7
Training loss: 2.387265682220459
Validation loss: 2.5792551925105434

Epoch: 5| Step: 8
Training loss: 2.521453380584717
Validation loss: 2.5748916441394436

Epoch: 5| Step: 9
Training loss: 2.442561626434326
Validation loss: 2.5757194231915217

Epoch: 5| Step: 10
Training loss: 3.0197017192840576
Validation loss: 2.5765417878345778

Epoch: 91| Step: 0
Training loss: 2.5727219581604004
Validation loss: 2.575086403918523

Epoch: 5| Step: 1
Training loss: 2.538743495941162
Validation loss: 2.5765624533417406

Epoch: 5| Step: 2
Training loss: 2.517055034637451
Validation loss: 2.577162547778058

Epoch: 5| Step: 3
Training loss: 2.916546583175659
Validation loss: 2.5754601288867254

Epoch: 5| Step: 4
Training loss: 2.7429697513580322
Validation loss: 2.574919316076463

Epoch: 5| Step: 5
Training loss: 2.4132590293884277
Validation loss: 2.5744805976908696

Epoch: 5| Step: 6
Training loss: 2.4877545833587646
Validation loss: 2.577866103059502

Epoch: 5| Step: 7
Training loss: 2.9973042011260986
Validation loss: 2.571478630906792

Epoch: 5| Step: 8
Training loss: 2.5014243125915527
Validation loss: 2.577133652984455

Epoch: 5| Step: 9
Training loss: 3.2121925354003906
Validation loss: 2.579380250746204

Epoch: 5| Step: 10
Training loss: 3.4906041622161865
Validation loss: 2.578302534677649

Epoch: 92| Step: 0
Training loss: 2.400475025177002
Validation loss: 2.5890604860039166

Epoch: 5| Step: 1
Training loss: 2.381387710571289
Validation loss: 2.596835292795653

Epoch: 5| Step: 2
Training loss: 3.465519666671753
Validation loss: 2.5899490694845877

Epoch: 5| Step: 3
Training loss: 2.6757421493530273
Validation loss: 2.5845211910945114

Epoch: 5| Step: 4
Training loss: 2.3878519535064697
Validation loss: 2.5823528023176294

Epoch: 5| Step: 5
Training loss: 2.5479960441589355
Validation loss: 2.57485697602713

Epoch: 5| Step: 6
Training loss: 3.051673412322998
Validation loss: 2.573311044323829

Epoch: 5| Step: 7
Training loss: 2.340871810913086
Validation loss: 2.569035353199128

Epoch: 5| Step: 8
Training loss: 2.9492335319519043
Validation loss: 2.5705719609414377

Epoch: 5| Step: 9
Training loss: 3.121457099914551
Validation loss: 2.5732355579253166

Epoch: 5| Step: 10
Training loss: 3.027158498764038
Validation loss: 2.5723173028679303

Epoch: 93| Step: 0
Training loss: 2.355926990509033
Validation loss: 2.573585092380483

Epoch: 5| Step: 1
Training loss: 2.4614901542663574
Validation loss: 2.57245970285067

Epoch: 5| Step: 2
Training loss: 1.9597972631454468
Validation loss: 2.5708972869380826

Epoch: 5| Step: 3
Training loss: 3.0718395709991455
Validation loss: 2.570539015595631

Epoch: 5| Step: 4
Training loss: 2.8849289417266846
Validation loss: 2.5758079764663533

Epoch: 5| Step: 5
Training loss: 3.217806339263916
Validation loss: 2.5724824243976223

Epoch: 5| Step: 6
Training loss: 2.7923436164855957
Validation loss: 2.5724777226806967

Epoch: 5| Step: 7
Training loss: 2.899846315383911
Validation loss: 2.572840867503997

Epoch: 5| Step: 8
Training loss: 2.5185492038726807
Validation loss: 2.573907490699522

Epoch: 5| Step: 9
Training loss: 3.336102247238159
Validation loss: 2.5758280677180134

Epoch: 5| Step: 10
Training loss: 2.666626214981079
Validation loss: 2.5736601403964463

Epoch: 94| Step: 0
Training loss: 2.5481362342834473
Validation loss: 2.5698780731488298

Epoch: 5| Step: 1
Training loss: 2.620035409927368
Validation loss: 2.573592811502436

Epoch: 5| Step: 2
Training loss: 2.9479775428771973
Validation loss: 2.571209094857657

Epoch: 5| Step: 3
Training loss: 2.9825968742370605
Validation loss: 2.5657526831473074

Epoch: 5| Step: 4
Training loss: 2.807372570037842
Validation loss: 2.571740406815724

Epoch: 5| Step: 5
Training loss: 2.8416974544525146
Validation loss: 2.574665738690284

Epoch: 5| Step: 6
Training loss: 2.9890129566192627
Validation loss: 2.5711911186095207

Epoch: 5| Step: 7
Training loss: 3.2473435401916504
Validation loss: 2.5809719101075204

Epoch: 5| Step: 8
Training loss: 2.6703341007232666
Validation loss: 2.5762916303450063

Epoch: 5| Step: 9
Training loss: 2.3031985759735107
Validation loss: 2.574804946940432

Epoch: 5| Step: 10
Training loss: 2.1865007877349854
Validation loss: 2.5769534008477324

Epoch: 95| Step: 0
Training loss: 2.59958553314209
Validation loss: 2.5776296738655335

Epoch: 5| Step: 1
Training loss: 2.9185800552368164
Validation loss: 2.587514723500898

Epoch: 5| Step: 2
Training loss: 3.528933048248291
Validation loss: 2.592966082275555

Epoch: 5| Step: 3
Training loss: 2.6214585304260254
Validation loss: 2.603771312262422

Epoch: 5| Step: 4
Training loss: 2.4573099613189697
Validation loss: 2.6167917815587853

Epoch: 5| Step: 5
Training loss: 3.1737868785858154
Validation loss: 2.6116013321825253

Epoch: 5| Step: 6
Training loss: 2.68890118598938
Validation loss: 2.606450080871582

Epoch: 5| Step: 7
Training loss: 2.967467784881592
Validation loss: 2.603855240729547

Epoch: 5| Step: 8
Training loss: 2.769745349884033
Validation loss: 2.5884331144312376

Epoch: 5| Step: 9
Training loss: 2.502514362335205
Validation loss: 2.5727372477131505

Epoch: 5| Step: 10
Training loss: 2.0549087524414062
Validation loss: 2.5622125312846196

Epoch: 96| Step: 0
Training loss: 2.1842408180236816
Validation loss: 2.569574561170352

Epoch: 5| Step: 1
Training loss: 3.203049421310425
Validation loss: 2.572335688016748

Epoch: 5| Step: 2
Training loss: 3.0637881755828857
Validation loss: 2.575684637151739

Epoch: 5| Step: 3
Training loss: 2.270712375640869
Validation loss: 2.5847618784955753

Epoch: 5| Step: 4
Training loss: 2.639413833618164
Validation loss: 2.5882095995769707

Epoch: 5| Step: 5
Training loss: 2.2361831665039062
Validation loss: 2.5945432660400227

Epoch: 5| Step: 6
Training loss: 2.6031882762908936
Validation loss: 2.5878462432533182

Epoch: 5| Step: 7
Training loss: 2.8519387245178223
Validation loss: 2.5979366584490706

Epoch: 5| Step: 8
Training loss: 3.0197017192840576
Validation loss: 2.594129882833009

Epoch: 5| Step: 9
Training loss: 2.8455169200897217
Validation loss: 2.5881828774688063

Epoch: 5| Step: 10
Training loss: 3.499441385269165
Validation loss: 2.582554276271533

Epoch: 97| Step: 0
Training loss: 2.541980266571045
Validation loss: 2.5764986186899166

Epoch: 5| Step: 1
Training loss: 2.2721595764160156
Validation loss: 2.565850119436941

Epoch: 5| Step: 2
Training loss: 2.5794217586517334
Validation loss: 2.5587781603618334

Epoch: 5| Step: 3
Training loss: 2.9669647216796875
Validation loss: 2.5601493902103876

Epoch: 5| Step: 4
Training loss: 2.8327949047088623
Validation loss: 2.5628187374402116

Epoch: 5| Step: 5
Training loss: 2.310805559158325
Validation loss: 2.563077172925395

Epoch: 5| Step: 6
Training loss: 2.610283374786377
Validation loss: 2.5679412836669595

Epoch: 5| Step: 7
Training loss: 2.4180006980895996
Validation loss: 2.5634464474134546

Epoch: 5| Step: 8
Training loss: 2.767482280731201
Validation loss: 2.5616449617570445

Epoch: 5| Step: 9
Training loss: 3.5126953125
Validation loss: 2.563202181170064

Epoch: 5| Step: 10
Training loss: 3.4567782878875732
Validation loss: 2.561834607073056

Epoch: 98| Step: 0
Training loss: 3.3405661582946777
Validation loss: 2.5608312929830244

Epoch: 5| Step: 1
Training loss: 1.787196397781372
Validation loss: 2.5620728615791566

Epoch: 5| Step: 2
Training loss: 2.0958633422851562
Validation loss: 2.5618544778516217

Epoch: 5| Step: 3
Training loss: 3.1673736572265625
Validation loss: 2.565262161275392

Epoch: 5| Step: 4
Training loss: 3.5933144092559814
Validation loss: 2.5662970568544123

Epoch: 5| Step: 5
Training loss: 2.2955687046051025
Validation loss: 2.56862905204937

Epoch: 5| Step: 6
Training loss: 3.0226099491119385
Validation loss: 2.5609205102407806

Epoch: 5| Step: 7
Training loss: 3.383143663406372
Validation loss: 2.568003757025606

Epoch: 5| Step: 8
Training loss: 2.1284255981445312
Validation loss: 2.5565585013358825

Epoch: 5| Step: 9
Training loss: 2.747204303741455
Validation loss: 2.5572063666518017

Epoch: 5| Step: 10
Training loss: 2.583583354949951
Validation loss: 2.5601573887691704

Epoch: 99| Step: 0
Training loss: 3.817509889602661
Validation loss: 2.559040772017612

Epoch: 5| Step: 1
Training loss: 2.8188233375549316
Validation loss: 2.561167114524431

Epoch: 5| Step: 2
Training loss: 2.483015537261963
Validation loss: 2.556547180298836

Epoch: 5| Step: 3
Training loss: 2.579911470413208
Validation loss: 2.5518293432010117

Epoch: 5| Step: 4
Training loss: 2.320619821548462
Validation loss: 2.5553662238582486

Epoch: 5| Step: 5
Training loss: 2.9134926795959473
Validation loss: 2.5544818857664704

Epoch: 5| Step: 6
Training loss: 2.614825487136841
Validation loss: 2.5570691631686304

Epoch: 5| Step: 7
Training loss: 2.7215919494628906
Validation loss: 2.5526757547932286

Epoch: 5| Step: 8
Training loss: 1.6396067142486572
Validation loss: 2.555019272271023

Epoch: 5| Step: 9
Training loss: 2.523069143295288
Validation loss: 2.5574117347758305

Epoch: 5| Step: 10
Training loss: 3.893468141555786
Validation loss: 2.5579223684085313

Epoch: 100| Step: 0
Training loss: 3.8514628410339355
Validation loss: 2.55818437504512

Epoch: 5| Step: 1
Training loss: 2.401986598968506
Validation loss: 2.5583075810504217

Epoch: 5| Step: 2
Training loss: 2.797837972640991
Validation loss: 2.563754532926826

Epoch: 5| Step: 3
Training loss: 2.3879246711730957
Validation loss: 2.565029436542142

Epoch: 5| Step: 4
Training loss: 2.0059714317321777
Validation loss: 2.560356973319925

Epoch: 5| Step: 5
Training loss: 3.563007354736328
Validation loss: 2.553475720908052

Epoch: 5| Step: 6
Training loss: 2.368600368499756
Validation loss: 2.5545922171685005

Epoch: 5| Step: 7
Training loss: 2.2687935829162598
Validation loss: 2.551086074562483

Epoch: 5| Step: 8
Training loss: 3.140610456466675
Validation loss: 2.550317008008239

Epoch: 5| Step: 9
Training loss: 2.825498580932617
Validation loss: 2.556577818368071

Epoch: 5| Step: 10
Training loss: 2.4269754886627197
Validation loss: 2.5576814682252946

Epoch: 101| Step: 0
Training loss: 2.466020107269287
Validation loss: 2.5514001077221287

Epoch: 5| Step: 1
Training loss: 2.604417562484741
Validation loss: 2.5550918399646716

Epoch: 5| Step: 2
Training loss: 2.9821393489837646
Validation loss: 2.553192728309221

Epoch: 5| Step: 3
Training loss: 3.018216371536255
Validation loss: 2.5514646807024555

Epoch: 5| Step: 4
Training loss: 2.531045436859131
Validation loss: 2.547733158193609

Epoch: 5| Step: 5
Training loss: 2.3322932720184326
Validation loss: 2.550525608883109

Epoch: 5| Step: 6
Training loss: 2.9817662239074707
Validation loss: 2.550966001326038

Epoch: 5| Step: 7
Training loss: 3.070413589477539
Validation loss: 2.5465790148704284

Epoch: 5| Step: 8
Training loss: 2.6810028553009033
Validation loss: 2.5470951885305424

Epoch: 5| Step: 9
Training loss: 2.4863321781158447
Validation loss: 2.548445204252838

Epoch: 5| Step: 10
Training loss: 2.9845829010009766
Validation loss: 2.5462723188502814

Epoch: 102| Step: 0
Training loss: 1.693058729171753
Validation loss: 2.551152995837632

Epoch: 5| Step: 1
Training loss: 2.849046230316162
Validation loss: 2.547478501514722

Epoch: 5| Step: 2
Training loss: 3.286649703979492
Validation loss: 2.5491560095099994

Epoch: 5| Step: 3
Training loss: 2.775031566619873
Validation loss: 2.545495997193039

Epoch: 5| Step: 4
Training loss: 2.5016541481018066
Validation loss: 2.547141913444765

Epoch: 5| Step: 5
Training loss: 2.613051652908325
Validation loss: 2.549283855704851

Epoch: 5| Step: 6
Training loss: 2.809077739715576
Validation loss: 2.547167231959681

Epoch: 5| Step: 7
Training loss: 2.8981521129608154
Validation loss: 2.545326332892141

Epoch: 5| Step: 8
Training loss: 2.690368413925171
Validation loss: 2.545375152300763

Epoch: 5| Step: 9
Training loss: 3.0754787921905518
Validation loss: 2.5477841541331303

Epoch: 5| Step: 10
Training loss: 2.8950960636138916
Validation loss: 2.547121501738025

Epoch: 103| Step: 0
Training loss: 2.9058127403259277
Validation loss: 2.5506975471332507

Epoch: 5| Step: 1
Training loss: 2.906364917755127
Validation loss: 2.5492983838563323

Epoch: 5| Step: 2
Training loss: 2.6039161682128906
Validation loss: 2.547211482960691

Epoch: 5| Step: 3
Training loss: 2.954990863800049
Validation loss: 2.54804479434926

Epoch: 5| Step: 4
Training loss: 3.3292007446289062
Validation loss: 2.5491565196744856

Epoch: 5| Step: 5
Training loss: 2.9412927627563477
Validation loss: 2.5445080880195863

Epoch: 5| Step: 6
Training loss: 2.5656745433807373
Validation loss: 2.5422182852222073

Epoch: 5| Step: 7
Training loss: 2.493340253829956
Validation loss: 2.5429558420694

Epoch: 5| Step: 8
Training loss: 2.7536416053771973
Validation loss: 2.5424062282808366

Epoch: 5| Step: 9
Training loss: 2.3583521842956543
Validation loss: 2.541233326799126

Epoch: 5| Step: 10
Training loss: 2.1076323986053467
Validation loss: 2.541955130074614

Epoch: 104| Step: 0
Training loss: 3.3590927124023438
Validation loss: 2.5410510980954735

Epoch: 5| Step: 1
Training loss: 2.4648966789245605
Validation loss: 2.5436149540767876

Epoch: 5| Step: 2
Training loss: 2.668789863586426
Validation loss: 2.5465708906932543

Epoch: 5| Step: 3
Training loss: 2.502493381500244
Validation loss: 2.5482197320589455

Epoch: 5| Step: 4
Training loss: 2.6830854415893555
Validation loss: 2.5485452810923257

Epoch: 5| Step: 5
Training loss: 3.2217984199523926
Validation loss: 2.546730802905175

Epoch: 5| Step: 6
Training loss: 2.7861275672912598
Validation loss: 2.5459696041640414

Epoch: 5| Step: 7
Training loss: 2.7982990741729736
Validation loss: 2.5442580253847185

Epoch: 5| Step: 8
Training loss: 2.414719343185425
Validation loss: 2.544560491397817

Epoch: 5| Step: 9
Training loss: 2.6912474632263184
Validation loss: 2.5415629007483043

Epoch: 5| Step: 10
Training loss: 2.35884952545166
Validation loss: 2.5413007531114804

Epoch: 105| Step: 0
Training loss: 2.9336020946502686
Validation loss: 2.5410321707366617

Epoch: 5| Step: 1
Training loss: 3.1776068210601807
Validation loss: 2.5479764579444804

Epoch: 5| Step: 2
Training loss: 2.56543231010437
Validation loss: 2.5535810378289994

Epoch: 5| Step: 3
Training loss: 2.5473275184631348
Validation loss: 2.5528283580656974

Epoch: 5| Step: 4
Training loss: 2.819326877593994
Validation loss: 2.567774654716574

Epoch: 5| Step: 5
Training loss: 3.0049262046813965
Validation loss: 2.570369505113171

Epoch: 5| Step: 6
Training loss: 2.965458631515503
Validation loss: 2.5781337958510204

Epoch: 5| Step: 7
Training loss: 2.3816893100738525
Validation loss: 2.568677497166459

Epoch: 5| Step: 8
Training loss: 3.018826723098755
Validation loss: 2.5573586597237536

Epoch: 5| Step: 9
Training loss: 2.655968427658081
Validation loss: 2.5431190459958968

Epoch: 5| Step: 10
Training loss: 1.9028452634811401
Validation loss: 2.537282564306772

Epoch: 106| Step: 0
Training loss: 2.6722559928894043
Validation loss: 2.540669628368911

Epoch: 5| Step: 1
Training loss: 2.0225417613983154
Validation loss: 2.550451858069307

Epoch: 5| Step: 2
Training loss: 3.327218532562256
Validation loss: 2.5535979732390373

Epoch: 5| Step: 3
Training loss: 3.509183406829834
Validation loss: 2.558955256656934

Epoch: 5| Step: 4
Training loss: 2.8680758476257324
Validation loss: 2.561469393391763

Epoch: 5| Step: 5
Training loss: 2.1150364875793457
Validation loss: 2.5626973182924333

Epoch: 5| Step: 6
Training loss: 2.874568462371826
Validation loss: 2.5647365380358953

Epoch: 5| Step: 7
Training loss: 2.611109495162964
Validation loss: 2.5555558230287287

Epoch: 5| Step: 8
Training loss: 2.7563984394073486
Validation loss: 2.5577020414413942

Epoch: 5| Step: 9
Training loss: 2.9142086505889893
Validation loss: 2.5536406501646964

Epoch: 5| Step: 10
Training loss: 2.3479721546173096
Validation loss: 2.5434255241065897

Epoch: 107| Step: 0
Training loss: 2.9439926147460938
Validation loss: 2.5536096121675227

Epoch: 5| Step: 1
Training loss: 2.4079411029815674
Validation loss: 2.548241315349456

Epoch: 5| Step: 2
Training loss: 2.6872506141662598
Validation loss: 2.5533921821143037

Epoch: 5| Step: 3
Training loss: 3.116917371749878
Validation loss: 2.5425874135827504

Epoch: 5| Step: 4
Training loss: 2.929083824157715
Validation loss: 2.5495895416505876

Epoch: 5| Step: 5
Training loss: 2.3633909225463867
Validation loss: 2.5541018952605543

Epoch: 5| Step: 6
Training loss: 2.9637856483459473
Validation loss: 2.552846429168537

Epoch: 5| Step: 7
Training loss: 2.4452977180480957
Validation loss: 2.557078233329199

Epoch: 5| Step: 8
Training loss: 1.9056050777435303
Validation loss: 2.557429982769874

Epoch: 5| Step: 9
Training loss: 3.4561569690704346
Validation loss: 2.5643478362791

Epoch: 5| Step: 10
Training loss: 2.8465702533721924
Validation loss: 2.5577329845838648

Epoch: 108| Step: 0
Training loss: 3.05468487739563
Validation loss: 2.5592457761046705

Epoch: 5| Step: 1
Training loss: 3.3470160961151123
Validation loss: 2.552387947677284

Epoch: 5| Step: 2
Training loss: 2.683112382888794
Validation loss: 2.5478585868753414

Epoch: 5| Step: 3
Training loss: 2.2423079013824463
Validation loss: 2.5498290420860372

Epoch: 5| Step: 4
Training loss: 2.4834985733032227
Validation loss: 2.551477732196931

Epoch: 5| Step: 5
Training loss: 2.350900173187256
Validation loss: 2.5438914042647167

Epoch: 5| Step: 6
Training loss: 2.886007785797119
Validation loss: 2.5464862033885014

Epoch: 5| Step: 7
Training loss: 2.753589153289795
Validation loss: 2.540804027229227

Epoch: 5| Step: 8
Training loss: 2.8706576824188232
Validation loss: 2.5477593868009505

Epoch: 5| Step: 9
Training loss: 3.0365755558013916
Validation loss: 2.544680590270668

Epoch: 5| Step: 10
Training loss: 2.157572031021118
Validation loss: 2.5453353056343655

Epoch: 109| Step: 0
Training loss: 2.681313991546631
Validation loss: 2.53738477922255

Epoch: 5| Step: 1
Training loss: 3.005486011505127
Validation loss: 2.5415550406261156

Epoch: 5| Step: 2
Training loss: 2.582427501678467
Validation loss: 2.541112574197913

Epoch: 5| Step: 3
Training loss: 2.8387553691864014
Validation loss: 2.5395222376751643

Epoch: 5| Step: 4
Training loss: 2.560051441192627
Validation loss: 2.5374552972855104

Epoch: 5| Step: 5
Training loss: 2.938143253326416
Validation loss: 2.5413852353249826

Epoch: 5| Step: 6
Training loss: 2.9943127632141113
Validation loss: 2.540242974476148

Epoch: 5| Step: 7
Training loss: 3.047532796859741
Validation loss: 2.5419326674553657

Epoch: 5| Step: 8
Training loss: 2.2868103981018066
Validation loss: 2.544701160923127

Epoch: 5| Step: 9
Training loss: 2.330066680908203
Validation loss: 2.540863134527719

Epoch: 5| Step: 10
Training loss: 2.693918228149414
Validation loss: 2.5473160205348844

Epoch: 110| Step: 0
Training loss: 2.5024783611297607
Validation loss: 2.547176632829892

Epoch: 5| Step: 1
Training loss: 2.6655919551849365
Validation loss: 2.5431239271676667

Epoch: 5| Step: 2
Training loss: 3.029924154281616
Validation loss: 2.544378642112978

Epoch: 5| Step: 3
Training loss: 2.382669687271118
Validation loss: 2.5465039719817457

Epoch: 5| Step: 4
Training loss: 2.8296828269958496
Validation loss: 2.5424143370761665

Epoch: 5| Step: 5
Training loss: 2.684255838394165
Validation loss: 2.5345849913935505

Epoch: 5| Step: 6
Training loss: 1.8865200281143188
Validation loss: 2.5346564169852965

Epoch: 5| Step: 7
Training loss: 3.1276206970214844
Validation loss: 2.533054058269788

Epoch: 5| Step: 8
Training loss: 2.759597063064575
Validation loss: 2.5354669094085693

Epoch: 5| Step: 9
Training loss: 3.0738987922668457
Validation loss: 2.536796582642422

Epoch: 5| Step: 10
Training loss: 3.03311824798584
Validation loss: 2.5314342744888796

Epoch: 111| Step: 0
Training loss: 2.412444829940796
Validation loss: 2.533830047935568

Epoch: 5| Step: 1
Training loss: 2.7876977920532227
Validation loss: 2.532655226287021

Epoch: 5| Step: 2
Training loss: 2.862069606781006
Validation loss: 2.530442025071831

Epoch: 5| Step: 3
Training loss: 2.5192248821258545
Validation loss: 2.5286139160074215

Epoch: 5| Step: 4
Training loss: 2.928184986114502
Validation loss: 2.5276117171010664

Epoch: 5| Step: 5
Training loss: 2.9351859092712402
Validation loss: 2.528236678851548

Epoch: 5| Step: 6
Training loss: 2.5684285163879395
Validation loss: 2.5279741876868793

Epoch: 5| Step: 7
Training loss: 3.004305601119995
Validation loss: 2.5297568100754932

Epoch: 5| Step: 8
Training loss: 2.395470142364502
Validation loss: 2.530151185169015

Epoch: 5| Step: 9
Training loss: 2.739781618118286
Validation loss: 2.5287503427074802

Epoch: 5| Step: 10
Training loss: 2.7495014667510986
Validation loss: 2.536401323092881

Epoch: 112| Step: 0
Training loss: 2.6464667320251465
Validation loss: 2.5400188251208236

Epoch: 5| Step: 1
Training loss: 3.0943055152893066
Validation loss: 2.5377827075219925

Epoch: 5| Step: 2
Training loss: 2.894219160079956
Validation loss: 2.539555726512786

Epoch: 5| Step: 3
Training loss: 2.6244778633117676
Validation loss: 2.5462253990993706

Epoch: 5| Step: 4
Training loss: 2.700955390930176
Validation loss: 2.5421394686545096

Epoch: 5| Step: 5
Training loss: 2.1042375564575195
Validation loss: 2.5388133910394486

Epoch: 5| Step: 6
Training loss: 2.220499038696289
Validation loss: 2.5428120141388266

Epoch: 5| Step: 7
Training loss: 3.084561824798584
Validation loss: 2.5355747976610736

Epoch: 5| Step: 8
Training loss: 2.6636321544647217
Validation loss: 2.5309127633289625

Epoch: 5| Step: 9
Training loss: 2.6512179374694824
Validation loss: 2.5262581045909593

Epoch: 5| Step: 10
Training loss: 3.345384120941162
Validation loss: 2.5342932977984027

Epoch: 113| Step: 0
Training loss: 2.2793526649475098
Validation loss: 2.5243764115918066

Epoch: 5| Step: 1
Training loss: 2.6804165840148926
Validation loss: 2.525682618541102

Epoch: 5| Step: 2
Training loss: 3.3927769660949707
Validation loss: 2.5257975414235103

Epoch: 5| Step: 3
Training loss: 2.3366525173187256
Validation loss: 2.5255319841446413

Epoch: 5| Step: 4
Training loss: 2.304311752319336
Validation loss: 2.524908891288183

Epoch: 5| Step: 5
Training loss: 2.658140182495117
Validation loss: 2.525447778804328

Epoch: 5| Step: 6
Training loss: 2.8851981163024902
Validation loss: 2.5250777352240776

Epoch: 5| Step: 7
Training loss: 3.026801586151123
Validation loss: 2.526479369850569

Epoch: 5| Step: 8
Training loss: 3.2477524280548096
Validation loss: 2.5262041322646605

Epoch: 5| Step: 9
Training loss: 2.811567544937134
Validation loss: 2.527365643491027

Epoch: 5| Step: 10
Training loss: 2.1859493255615234
Validation loss: 2.523873362489926

Epoch: 114| Step: 0
Training loss: 2.9327189922332764
Validation loss: 2.535816641264064

Epoch: 5| Step: 1
Training loss: 2.691287040710449
Validation loss: 2.535805117699408

Epoch: 5| Step: 2
Training loss: 2.6058623790740967
Validation loss: 2.539488411718799

Epoch: 5| Step: 3
Training loss: 2.1598525047302246
Validation loss: 2.53269858770473

Epoch: 5| Step: 4
Training loss: 3.2101492881774902
Validation loss: 2.5427937789629866

Epoch: 5| Step: 5
Training loss: 2.492570161819458
Validation loss: 2.5470154875068256

Epoch: 5| Step: 6
Training loss: 2.485048294067383
Validation loss: 2.5380048828740276

Epoch: 5| Step: 7
Training loss: 2.9907257556915283
Validation loss: 2.5385694452511367

Epoch: 5| Step: 8
Training loss: 2.5096652507781982
Validation loss: 2.536775932517103

Epoch: 5| Step: 9
Training loss: 2.7830841541290283
Validation loss: 2.5305846173276185

Epoch: 5| Step: 10
Training loss: 3.067833185195923
Validation loss: 2.5293691055749052

Epoch: 115| Step: 0
Training loss: 2.654689311981201
Validation loss: 2.5338429763752925

Epoch: 5| Step: 1
Training loss: 2.6384129524230957
Validation loss: 2.537413607361496

Epoch: 5| Step: 2
Training loss: 3.0181808471679688
Validation loss: 2.5367233266112623

Epoch: 5| Step: 3
Training loss: 2.6501100063323975
Validation loss: 2.5416941488942792

Epoch: 5| Step: 4
Training loss: 2.272831678390503
Validation loss: 2.5448328371970885

Epoch: 5| Step: 5
Training loss: 2.4925599098205566
Validation loss: 2.5457263223586546

Epoch: 5| Step: 6
Training loss: 2.3966293334960938
Validation loss: 2.540742410126553

Epoch: 5| Step: 7
Training loss: 2.97723650932312
Validation loss: 2.538361095613049

Epoch: 5| Step: 8
Training loss: 3.104038715362549
Validation loss: 2.5352929176822787

Epoch: 5| Step: 9
Training loss: 2.7422983646392822
Validation loss: 2.5427906923396613

Epoch: 5| Step: 10
Training loss: 3.056562662124634
Validation loss: 2.542490261857228

Epoch: 116| Step: 0
Training loss: 2.3187777996063232
Validation loss: 2.541129719826483

Epoch: 5| Step: 1
Training loss: 2.6549248695373535
Validation loss: 2.5331024790322907

Epoch: 5| Step: 2
Training loss: 2.012561321258545
Validation loss: 2.5281307158931607

Epoch: 5| Step: 3
Training loss: 2.6191344261169434
Validation loss: 2.5227184500745548

Epoch: 5| Step: 4
Training loss: 2.707685947418213
Validation loss: 2.5261012200386292

Epoch: 5| Step: 5
Training loss: 3.0329840183258057
Validation loss: 2.5237494668652936

Epoch: 5| Step: 6
Training loss: 2.6062488555908203
Validation loss: 2.5207274549750873

Epoch: 5| Step: 7
Training loss: 2.5553650856018066
Validation loss: 2.5226967334747314

Epoch: 5| Step: 8
Training loss: 3.9991183280944824
Validation loss: 2.5213933196119083

Epoch: 5| Step: 9
Training loss: 2.448164463043213
Validation loss: 2.5215164512716313

Epoch: 5| Step: 10
Training loss: 2.9335782527923584
Validation loss: 2.524119523263747

Epoch: 117| Step: 0
Training loss: 3.1346468925476074
Validation loss: 2.5241328336859263

Epoch: 5| Step: 1
Training loss: 2.5274856090545654
Validation loss: 2.5240273475646973

Epoch: 5| Step: 2
Training loss: 2.4390082359313965
Validation loss: 2.520206666761829

Epoch: 5| Step: 3
Training loss: 2.737882614135742
Validation loss: 2.5241074997891664

Epoch: 5| Step: 4
Training loss: 2.556445360183716
Validation loss: 2.522153272423693

Epoch: 5| Step: 5
Training loss: 2.545163631439209
Validation loss: 2.5237155601542485

Epoch: 5| Step: 6
Training loss: 2.4949443340301514
Validation loss: 2.5212633199589227

Epoch: 5| Step: 7
Training loss: 3.396782636642456
Validation loss: 2.524313929260418

Epoch: 5| Step: 8
Training loss: 2.6561920642852783
Validation loss: 2.524397373199463

Epoch: 5| Step: 9
Training loss: 2.894698143005371
Validation loss: 2.529780795497279

Epoch: 5| Step: 10
Training loss: 2.4537556171417236
Validation loss: 2.5346402352856052

Epoch: 118| Step: 0
Training loss: 3.431896209716797
Validation loss: 2.5328591023722002

Epoch: 5| Step: 1
Training loss: 2.101576089859009
Validation loss: 2.525317499714513

Epoch: 5| Step: 2
Training loss: 2.7524654865264893
Validation loss: 2.518615686765281

Epoch: 5| Step: 3
Training loss: 3.3454620838165283
Validation loss: 2.5226095158566713

Epoch: 5| Step: 4
Training loss: 2.521183729171753
Validation loss: 2.5197570529035342

Epoch: 5| Step: 5
Training loss: 3.4245765209198
Validation loss: 2.5202577883197415

Epoch: 5| Step: 6
Training loss: 2.6248984336853027
Validation loss: 2.517333394737654

Epoch: 5| Step: 7
Training loss: 2.178083896636963
Validation loss: 2.523902911011891

Epoch: 5| Step: 8
Training loss: 2.748619318008423
Validation loss: 2.522375491357619

Epoch: 5| Step: 9
Training loss: 1.9359699487686157
Validation loss: 2.5192041115094255

Epoch: 5| Step: 10
Training loss: 2.813056707382202
Validation loss: 2.522236221580095

Epoch: 119| Step: 0
Training loss: 2.5514919757843018
Validation loss: 2.529089702072964

Epoch: 5| Step: 1
Training loss: 2.3954949378967285
Validation loss: 2.542305302876298

Epoch: 5| Step: 2
Training loss: 2.8223624229431152
Validation loss: 2.5567379228530394

Epoch: 5| Step: 3
Training loss: 2.887840747833252
Validation loss: 2.5746109024170907

Epoch: 5| Step: 4
Training loss: 2.481346845626831
Validation loss: 2.581017430110644

Epoch: 5| Step: 5
Training loss: 3.186426877975464
Validation loss: 2.5652822679088962

Epoch: 5| Step: 6
Training loss: 2.8584790229797363
Validation loss: 2.547984948722265

Epoch: 5| Step: 7
Training loss: 2.323019027709961
Validation loss: 2.53281061110958

Epoch: 5| Step: 8
Training loss: 2.5075297355651855
Validation loss: 2.529062645409697

Epoch: 5| Step: 9
Training loss: 2.6442668437957764
Validation loss: 2.5187030735836236

Epoch: 5| Step: 10
Training loss: 3.4689581394195557
Validation loss: 2.5182840926672823

Epoch: 120| Step: 0
Training loss: 2.9491333961486816
Validation loss: 2.518620773028302

Epoch: 5| Step: 1
Training loss: 2.44500994682312
Validation loss: 2.5140516937419934

Epoch: 5| Step: 2
Training loss: 3.4066779613494873
Validation loss: 2.5227565867926485

Epoch: 5| Step: 3
Training loss: 3.1803853511810303
Validation loss: 2.515338723377515

Epoch: 5| Step: 4
Training loss: 3.04766583442688
Validation loss: 2.514429033443492

Epoch: 5| Step: 5
Training loss: 2.7975947856903076
Validation loss: 2.5121504593920965

Epoch: 5| Step: 6
Training loss: 2.0965497493743896
Validation loss: 2.5150573279268

Epoch: 5| Step: 7
Training loss: 2.354891538619995
Validation loss: 2.514247789177843

Epoch: 5| Step: 8
Training loss: 2.930086851119995
Validation loss: 2.516234172287808

Epoch: 5| Step: 9
Training loss: 2.3472371101379395
Validation loss: 2.5301940518040813

Epoch: 5| Step: 10
Training loss: 2.2162373065948486
Validation loss: 2.543494698821857

Epoch: 121| Step: 0
Training loss: 2.24031925201416
Validation loss: 2.5459083639165407

Epoch: 5| Step: 1
Training loss: 2.9581356048583984
Validation loss: 2.562222352591894

Epoch: 5| Step: 2
Training loss: 2.6813504695892334
Validation loss: 2.576290438252111

Epoch: 5| Step: 3
Training loss: 2.7951672077178955
Validation loss: 2.545466951144639

Epoch: 5| Step: 4
Training loss: 2.816514492034912
Validation loss: 2.5248802836223314

Epoch: 5| Step: 5
Training loss: 2.606174945831299
Validation loss: 2.5174889256877284

Epoch: 5| Step: 6
Training loss: 2.8860409259796143
Validation loss: 2.5133464156940417

Epoch: 5| Step: 7
Training loss: 3.0144615173339844
Validation loss: 2.512387511550739

Epoch: 5| Step: 8
Training loss: 2.365041732788086
Validation loss: 2.5153995149879047

Epoch: 5| Step: 9
Training loss: 3.056558609008789
Validation loss: 2.5153525772915093

Epoch: 5| Step: 10
Training loss: 2.5221643447875977
Validation loss: 2.5161064132567375

Epoch: 122| Step: 0
Training loss: 2.945291042327881
Validation loss: 2.5186461171796246

Epoch: 5| Step: 1
Training loss: 3.379464626312256
Validation loss: 2.5235924592582126

Epoch: 5| Step: 2
Training loss: 3.2363829612731934
Validation loss: 2.5205458799997964

Epoch: 5| Step: 3
Training loss: 2.4822936058044434
Validation loss: 2.523293343923425

Epoch: 5| Step: 4
Training loss: 2.686945676803589
Validation loss: 2.5244383247949744

Epoch: 5| Step: 5
Training loss: 2.2626094818115234
Validation loss: 2.517286551895962

Epoch: 5| Step: 6
Training loss: 2.9706382751464844
Validation loss: 2.518831904216479

Epoch: 5| Step: 7
Training loss: 2.720207929611206
Validation loss: 2.5174809014925392

Epoch: 5| Step: 8
Training loss: 2.549808979034424
Validation loss: 2.5186374623288392

Epoch: 5| Step: 9
Training loss: 2.2712562084198
Validation loss: 2.5155579479791785

Epoch: 5| Step: 10
Training loss: 2.3260059356689453
Validation loss: 2.520244316388202

Epoch: 123| Step: 0
Training loss: 2.48083233833313
Validation loss: 2.5322259677353727

Epoch: 5| Step: 1
Training loss: 2.377002000808716
Validation loss: 2.534720043982229

Epoch: 5| Step: 2
Training loss: 2.692234516143799
Validation loss: 2.5434269879453923

Epoch: 5| Step: 3
Training loss: 3.0558784008026123
Validation loss: 2.5441349706342145

Epoch: 5| Step: 4
Training loss: 2.435729503631592
Validation loss: 2.538318875015423

Epoch: 5| Step: 5
Training loss: 3.1124210357666016
Validation loss: 2.543806609287057

Epoch: 5| Step: 6
Training loss: 2.5062201023101807
Validation loss: 2.5311351309540453

Epoch: 5| Step: 7
Training loss: 2.833193778991699
Validation loss: 2.5407578560613815

Epoch: 5| Step: 8
Training loss: 3.048025131225586
Validation loss: 2.5312645691697315

Epoch: 5| Step: 9
Training loss: 3.0085175037384033
Validation loss: 2.5276842988947386

Epoch: 5| Step: 10
Training loss: 2.233060359954834
Validation loss: 2.5226575020820863

Epoch: 124| Step: 0
Training loss: 2.5322372913360596
Validation loss: 2.5184202732578402

Epoch: 5| Step: 1
Training loss: 2.66929292678833
Validation loss: 2.5203163034172467

Epoch: 5| Step: 2
Training loss: 2.835908889770508
Validation loss: 2.513169542435677

Epoch: 5| Step: 3
Training loss: 2.504788875579834
Validation loss: 2.5096384248425885

Epoch: 5| Step: 4
Training loss: 2.6716835498809814
Validation loss: 2.5109585228786675

Epoch: 5| Step: 5
Training loss: 2.7239890098571777
Validation loss: 2.5128938895399853

Epoch: 5| Step: 6
Training loss: 2.7150676250457764
Validation loss: 2.514754354312856

Epoch: 5| Step: 7
Training loss: 3.0664374828338623
Validation loss: 2.516833525831981

Epoch: 5| Step: 8
Training loss: 2.8422298431396484
Validation loss: 2.520117175194525

Epoch: 5| Step: 9
Training loss: 2.5364301204681396
Validation loss: 2.5196430721590595

Epoch: 5| Step: 10
Training loss: 2.709606409072876
Validation loss: 2.5162036367641982

Epoch: 125| Step: 0
Training loss: 2.590115547180176
Validation loss: 2.512792315534366

Epoch: 5| Step: 1
Training loss: 2.7650139331817627
Validation loss: 2.5125376870555263

Epoch: 5| Step: 2
Training loss: 2.5132250785827637
Validation loss: 2.5108766965968634

Epoch: 5| Step: 3
Training loss: 2.834470510482788
Validation loss: 2.51112021938447

Epoch: 5| Step: 4
Training loss: 3.6769371032714844
Validation loss: 2.5117073392355316

Epoch: 5| Step: 5
Training loss: 2.8622965812683105
Validation loss: 2.5124232820285264

Epoch: 5| Step: 6
Training loss: 2.1730542182922363
Validation loss: 2.5126275836780505

Epoch: 5| Step: 7
Training loss: 2.415642499923706
Validation loss: 2.5135444979513846

Epoch: 5| Step: 8
Training loss: 2.0178866386413574
Validation loss: 2.511806406000609

Epoch: 5| Step: 9
Training loss: 2.6031148433685303
Validation loss: 2.517817774126607

Epoch: 5| Step: 10
Training loss: 3.3980612754821777
Validation loss: 2.5275900876650246

Epoch: 126| Step: 0
Training loss: 2.429004669189453
Validation loss: 2.533282992660358

Epoch: 5| Step: 1
Training loss: 2.808687210083008
Validation loss: 2.535320702419486

Epoch: 5| Step: 2
Training loss: 2.373220920562744
Validation loss: 2.541316511810467

Epoch: 5| Step: 3
Training loss: 2.8961844444274902
Validation loss: 2.543057544257051

Epoch: 5| Step: 4
Training loss: 3.1238696575164795
Validation loss: 2.5375933288246073

Epoch: 5| Step: 5
Training loss: 2.4928529262542725
Validation loss: 2.5330209296236754

Epoch: 5| Step: 6
Training loss: 2.879415512084961
Validation loss: 2.5256993283507643

Epoch: 5| Step: 7
Training loss: 2.4572439193725586
Validation loss: 2.510168819017308

Epoch: 5| Step: 8
Training loss: 2.585066318511963
Validation loss: 2.506815092537993

Epoch: 5| Step: 9
Training loss: 3.0590152740478516
Validation loss: 2.507506993509108

Epoch: 5| Step: 10
Training loss: 2.710827589035034
Validation loss: 2.5082370363255984

Epoch: 127| Step: 0
Training loss: 2.6454005241394043
Validation loss: 2.510563865784676

Epoch: 5| Step: 1
Training loss: 2.8806281089782715
Validation loss: 2.5135203253838325

Epoch: 5| Step: 2
Training loss: 2.8945670127868652
Validation loss: 2.5148173480905514

Epoch: 5| Step: 3
Training loss: 2.843029022216797
Validation loss: 2.52081585186784

Epoch: 5| Step: 4
Training loss: 2.5980122089385986
Validation loss: 2.5114364495841404

Epoch: 5| Step: 5
Training loss: 2.4035158157348633
Validation loss: 2.5152801159889466

Epoch: 5| Step: 6
Training loss: 2.073368787765503
Validation loss: 2.509922804371003

Epoch: 5| Step: 7
Training loss: 2.2269339561462402
Validation loss: 2.5120118023246847

Epoch: 5| Step: 8
Training loss: 2.8442635536193848
Validation loss: 2.504761900953067

Epoch: 5| Step: 9
Training loss: 2.711580753326416
Validation loss: 2.5043258000445623

Epoch: 5| Step: 10
Training loss: 3.8697590827941895
Validation loss: 2.5055975478182555

Epoch: 128| Step: 0
Training loss: 2.6415743827819824
Validation loss: 2.5057068896550003

Epoch: 5| Step: 1
Training loss: 2.849611759185791
Validation loss: 2.510512626299294

Epoch: 5| Step: 2
Training loss: 2.6587162017822266
Validation loss: 2.5075138051022767

Epoch: 5| Step: 3
Training loss: 2.8211333751678467
Validation loss: 2.5084691047668457

Epoch: 5| Step: 4
Training loss: 3.060319185256958
Validation loss: 2.512796219959054

Epoch: 5| Step: 5
Training loss: 3.0216708183288574
Validation loss: 2.514087066855482

Epoch: 5| Step: 6
Training loss: 2.374049663543701
Validation loss: 2.510161346004855

Epoch: 5| Step: 7
Training loss: 3.0059990882873535
Validation loss: 2.516299163141558

Epoch: 5| Step: 8
Training loss: 2.7039096355438232
Validation loss: 2.5153063215235227

Epoch: 5| Step: 9
Training loss: 2.043111801147461
Validation loss: 2.511857032775879

Epoch: 5| Step: 10
Training loss: 2.571463108062744
Validation loss: 2.516266269068564

Epoch: 129| Step: 0
Training loss: 2.6491031646728516
Validation loss: 2.523559598512547

Epoch: 5| Step: 1
Training loss: 2.3747916221618652
Validation loss: 2.517410034774452

Epoch: 5| Step: 2
Training loss: 2.726029872894287
Validation loss: 2.5148074857650267

Epoch: 5| Step: 3
Training loss: 3.045764923095703
Validation loss: 2.51474940905007

Epoch: 5| Step: 4
Training loss: 2.5613439083099365
Validation loss: 2.5114229955980854

Epoch: 5| Step: 5
Training loss: 2.337036609649658
Validation loss: 2.5165329030765

Epoch: 5| Step: 6
Training loss: 3.355210542678833
Validation loss: 2.519995794501356

Epoch: 5| Step: 7
Training loss: 2.9460647106170654
Validation loss: 2.5177290208878054

Epoch: 5| Step: 8
Training loss: 2.2275938987731934
Validation loss: 2.519322088969651

Epoch: 5| Step: 9
Training loss: 3.0512733459472656
Validation loss: 2.5164914028618925

Epoch: 5| Step: 10
Training loss: 2.4013655185699463
Validation loss: 2.5201057772482596

Epoch: 130| Step: 0
Training loss: 2.2645397186279297
Validation loss: 2.5163952637744207

Epoch: 5| Step: 1
Training loss: 3.000706434249878
Validation loss: 2.5157729477010746

Epoch: 5| Step: 2
Training loss: 2.4301724433898926
Validation loss: 2.501608448643838

Epoch: 5| Step: 3
Training loss: 2.6986136436462402
Validation loss: 2.4981727471915622

Epoch: 5| Step: 4
Training loss: 2.770355463027954
Validation loss: 2.4977771774415047

Epoch: 5| Step: 5
Training loss: 2.6109116077423096
Validation loss: 2.4968448223606234

Epoch: 5| Step: 6
Training loss: 2.8506834506988525
Validation loss: 2.496058863978232

Epoch: 5| Step: 7
Training loss: 2.682701587677002
Validation loss: 2.496928976428124

Epoch: 5| Step: 8
Training loss: 2.414947032928467
Validation loss: 2.497366900085121

Epoch: 5| Step: 9
Training loss: 3.2091586589813232
Validation loss: 2.4968224776688444

Epoch: 5| Step: 10
Training loss: 2.8747823238372803
Validation loss: 2.4959018896984797

Epoch: 131| Step: 0
Training loss: 2.3714733123779297
Validation loss: 2.496790196305962

Epoch: 5| Step: 1
Training loss: 2.324111223220825
Validation loss: 2.4974779877611386

Epoch: 5| Step: 2
Training loss: 3.093614101409912
Validation loss: 2.500393183000626

Epoch: 5| Step: 3
Training loss: 3.056614398956299
Validation loss: 2.503426791519247

Epoch: 5| Step: 4
Training loss: 1.9170154333114624
Validation loss: 2.503554738977904

Epoch: 5| Step: 5
Training loss: 2.575803756713867
Validation loss: 2.5054143167311147

Epoch: 5| Step: 6
Training loss: 3.295928955078125
Validation loss: 2.5117998840988323

Epoch: 5| Step: 7
Training loss: 3.1935667991638184
Validation loss: 2.5101901023618636

Epoch: 5| Step: 8
Training loss: 2.775437355041504
Validation loss: 2.5114401617357807

Epoch: 5| Step: 9
Training loss: 2.5206751823425293
Validation loss: 2.5124885164281374

Epoch: 5| Step: 10
Training loss: 2.4838669300079346
Validation loss: 2.5046721530216995

Epoch: 132| Step: 0
Training loss: 2.940786838531494
Validation loss: 2.5099923610687256

Epoch: 5| Step: 1
Training loss: 2.339745283126831
Validation loss: 2.5045877707901822

Epoch: 5| Step: 2
Training loss: 2.540459156036377
Validation loss: 2.5074804623921714

Epoch: 5| Step: 3
Training loss: 2.847139835357666
Validation loss: 2.502408399376818

Epoch: 5| Step: 4
Training loss: 1.9987322092056274
Validation loss: 2.5007648647472425

Epoch: 5| Step: 5
Training loss: 2.704787492752075
Validation loss: 2.4999517035740677

Epoch: 5| Step: 6
Training loss: 2.8903396129608154
Validation loss: 2.498739580954275

Epoch: 5| Step: 7
Training loss: 2.7153561115264893
Validation loss: 2.4977657051496607

Epoch: 5| Step: 8
Training loss: 2.6704933643341064
Validation loss: 2.4976720861209336

Epoch: 5| Step: 9
Training loss: 3.1427016258239746
Validation loss: 2.499872697296963

Epoch: 5| Step: 10
Training loss: 2.8687820434570312
Validation loss: 2.4957793220396964

Epoch: 133| Step: 0
Training loss: 2.7146403789520264
Validation loss: 2.505040171325848

Epoch: 5| Step: 1
Training loss: 2.858243942260742
Validation loss: 2.5120408381185224

Epoch: 5| Step: 2
Training loss: 3.1468002796173096
Validation loss: 2.5185707922904723

Epoch: 5| Step: 3
Training loss: 3.09879469871521
Validation loss: 2.5214685599009194

Epoch: 5| Step: 4
Training loss: 2.1663763523101807
Validation loss: 2.514722378023209

Epoch: 5| Step: 5
Training loss: 2.1179862022399902
Validation loss: 2.520201806099184

Epoch: 5| Step: 6
Training loss: 2.6051735877990723
Validation loss: 2.514646899315619

Epoch: 5| Step: 7
Training loss: 2.7419028282165527
Validation loss: 2.5112362600141958

Epoch: 5| Step: 8
Training loss: 2.589371681213379
Validation loss: 2.5050531228383384

Epoch: 5| Step: 9
Training loss: 3.121748447418213
Validation loss: 2.50575912896023

Epoch: 5| Step: 10
Training loss: 2.4904422760009766
Validation loss: 2.5133737851214666

Epoch: 134| Step: 0
Training loss: 3.149207592010498
Validation loss: 2.517285023966143

Epoch: 5| Step: 1
Training loss: 2.9767158031463623
Validation loss: 2.5019687426987516

Epoch: 5| Step: 2
Training loss: 3.384838819503784
Validation loss: 2.5024938429555585

Epoch: 5| Step: 3
Training loss: 2.6807868480682373
Validation loss: 2.4980165830222507

Epoch: 5| Step: 4
Training loss: 2.502012252807617
Validation loss: 2.4945108813624226

Epoch: 5| Step: 5
Training loss: 2.4396920204162598
Validation loss: 2.496168872361542

Epoch: 5| Step: 6
Training loss: 2.755797863006592
Validation loss: 2.496239241733346

Epoch: 5| Step: 7
Training loss: 2.3543434143066406
Validation loss: 2.4973147607618764

Epoch: 5| Step: 8
Training loss: 1.7943916320800781
Validation loss: 2.4899525796213458

Epoch: 5| Step: 9
Training loss: 2.8458824157714844
Validation loss: 2.490483581378896

Epoch: 5| Step: 10
Training loss: 2.7901859283447266
Validation loss: 2.4905889777727026

Epoch: 135| Step: 0
Training loss: 2.5941872596740723
Validation loss: 2.4886408749447075

Epoch: 5| Step: 1
Training loss: 2.5025219917297363
Validation loss: 2.4920958190835933

Epoch: 5| Step: 2
Training loss: 2.666534900665283
Validation loss: 2.4895451479060675

Epoch: 5| Step: 3
Training loss: 3.5058116912841797
Validation loss: 2.491755203534198

Epoch: 5| Step: 4
Training loss: 2.438594341278076
Validation loss: 2.4972327498979467

Epoch: 5| Step: 5
Training loss: 2.7336018085479736
Validation loss: 2.490385724652198

Epoch: 5| Step: 6
Training loss: 2.764416217803955
Validation loss: 2.4956031794189126

Epoch: 5| Step: 7
Training loss: 2.6960859298706055
Validation loss: 2.5000261798981698

Epoch: 5| Step: 8
Training loss: 2.3179752826690674
Validation loss: 2.498278123076244

Epoch: 5| Step: 9
Training loss: 2.6098670959472656
Validation loss: 2.511338549275552

Epoch: 5| Step: 10
Training loss: 2.802412271499634
Validation loss: 2.5084609472623436

Epoch: 136| Step: 0
Training loss: 3.1692168712615967
Validation loss: 2.5204276730937343

Epoch: 5| Step: 1
Training loss: 2.230128526687622
Validation loss: 2.52953928004029

Epoch: 5| Step: 2
Training loss: 2.87620210647583
Validation loss: 2.5482713355812976

Epoch: 5| Step: 3
Training loss: 3.303555727005005
Validation loss: 2.5470676601573987

Epoch: 5| Step: 4
Training loss: 2.9938251972198486
Validation loss: 2.5521803927677933

Epoch: 5| Step: 5
Training loss: 2.5270767211914062
Validation loss: 2.5343361311061408

Epoch: 5| Step: 6
Training loss: 2.726804494857788
Validation loss: 2.509120146433512

Epoch: 5| Step: 7
Training loss: 2.091449022293091
Validation loss: 2.493485866054412

Epoch: 5| Step: 8
Training loss: 3.078936815261841
Validation loss: 2.4945591547155894

Epoch: 5| Step: 9
Training loss: 2.489170789718628
Validation loss: 2.4933569508214153

Epoch: 5| Step: 10
Training loss: 2.3947949409484863
Validation loss: 2.500721218765423

Epoch: 137| Step: 0
Training loss: 2.516342878341675
Validation loss: 2.501230437268493

Epoch: 5| Step: 1
Training loss: 2.947481632232666
Validation loss: 2.504309636290355

Epoch: 5| Step: 2
Training loss: 3.065166473388672
Validation loss: 2.5048263329331593

Epoch: 5| Step: 3
Training loss: 2.6823737621307373
Validation loss: 2.503408060278944

Epoch: 5| Step: 4
Training loss: 2.9305508136749268
Validation loss: 2.5021224867913032

Epoch: 5| Step: 5
Training loss: 2.4435782432556152
Validation loss: 2.5034938909674205

Epoch: 5| Step: 6
Training loss: 3.4442965984344482
Validation loss: 2.5037089509348713

Epoch: 5| Step: 7
Training loss: 2.510878801345825
Validation loss: 2.499869995219733

Epoch: 5| Step: 8
Training loss: 1.871957778930664
Validation loss: 2.493512112607238

Epoch: 5| Step: 9
Training loss: 2.7046661376953125
Validation loss: 2.495130923486525

Epoch: 5| Step: 10
Training loss: 2.6274795532226562
Validation loss: 2.4908482541320143

Epoch: 138| Step: 0
Training loss: 2.3893237113952637
Validation loss: 2.4917930864518687

Epoch: 5| Step: 1
Training loss: 2.7974579334259033
Validation loss: 2.496920072904197

Epoch: 5| Step: 2
Training loss: 2.515033006668091
Validation loss: 2.4950596645314205

Epoch: 5| Step: 3
Training loss: 2.6527457237243652
Validation loss: 2.5049417762346167

Epoch: 5| Step: 4
Training loss: 2.3116583824157715
Validation loss: 2.5070544750459733

Epoch: 5| Step: 5
Training loss: 3.5823981761932373
Validation loss: 2.508682289431172

Epoch: 5| Step: 6
Training loss: 2.7421624660491943
Validation loss: 2.506699703072989

Epoch: 5| Step: 7
Training loss: 2.9845519065856934
Validation loss: 2.5088890778121127

Epoch: 5| Step: 8
Training loss: 1.966339111328125
Validation loss: 2.510189498624494

Epoch: 5| Step: 9
Training loss: 2.7829151153564453
Validation loss: 2.504912184130761

Epoch: 5| Step: 10
Training loss: 2.9213764667510986
Validation loss: 2.5119287813863447

Epoch: 139| Step: 0
Training loss: 2.0266995429992676
Validation loss: 2.5087385023793867

Epoch: 5| Step: 1
Training loss: 2.5322265625
Validation loss: 2.5074327197126163

Epoch: 5| Step: 2
Training loss: 2.7853453159332275
Validation loss: 2.5025180334685952

Epoch: 5| Step: 3
Training loss: 2.671861171722412
Validation loss: 2.4969429636514313

Epoch: 5| Step: 4
Training loss: 2.9971625804901123
Validation loss: 2.502176643699728

Epoch: 5| Step: 5
Training loss: 2.869708299636841
Validation loss: 2.5014674714816514

Epoch: 5| Step: 6
Training loss: 2.9133870601654053
Validation loss: 2.4994597896452873

Epoch: 5| Step: 7
Training loss: 2.6957478523254395
Validation loss: 2.4911158110505793

Epoch: 5| Step: 8
Training loss: 3.4733688831329346
Validation loss: 2.4891826388656453

Epoch: 5| Step: 9
Training loss: 2.074469804763794
Validation loss: 2.491935342870733

Epoch: 5| Step: 10
Training loss: 2.5091562271118164
Validation loss: 2.491480119766728

Epoch: 140| Step: 0
Training loss: 2.5567002296447754
Validation loss: 2.4914082455378708

Epoch: 5| Step: 1
Training loss: 3.388307571411133
Validation loss: 2.4931703818741666

Epoch: 5| Step: 2
Training loss: 3.066688060760498
Validation loss: 2.4884870436883744

Epoch: 5| Step: 3
Training loss: 2.158341884613037
Validation loss: 2.486228371179232

Epoch: 5| Step: 4
Training loss: 2.3757972717285156
Validation loss: 2.488812559394426

Epoch: 5| Step: 5
Training loss: 2.3225486278533936
Validation loss: 2.487379127933133

Epoch: 5| Step: 6
Training loss: 2.511549472808838
Validation loss: 2.4897785981496177

Epoch: 5| Step: 7
Training loss: 2.337096691131592
Validation loss: 2.486772443658562

Epoch: 5| Step: 8
Training loss: 3.3726723194122314
Validation loss: 2.492667002062644

Epoch: 5| Step: 9
Training loss: 2.524097204208374
Validation loss: 2.4913864674106723

Epoch: 5| Step: 10
Training loss: 3.0342624187469482
Validation loss: 2.4886602432497087

Epoch: 141| Step: 0
Training loss: 2.1346535682678223
Validation loss: 2.4898641545285463

Epoch: 5| Step: 1
Training loss: 1.9679107666015625
Validation loss: 2.4874578547734085

Epoch: 5| Step: 2
Training loss: 2.849025249481201
Validation loss: 2.4883795528001684

Epoch: 5| Step: 3
Training loss: 2.2941956520080566
Validation loss: 2.4852179327318744

Epoch: 5| Step: 4
Training loss: 3.1413893699645996
Validation loss: 2.488152680858489

Epoch: 5| Step: 5
Training loss: 3.164928913116455
Validation loss: 2.4890561001275175

Epoch: 5| Step: 6
Training loss: 2.721421718597412
Validation loss: 2.4897387976287515

Epoch: 5| Step: 7
Training loss: 3.232786178588867
Validation loss: 2.48547278681109

Epoch: 5| Step: 8
Training loss: 2.5918827056884766
Validation loss: 2.4893227597718597

Epoch: 5| Step: 9
Training loss: 3.1898531913757324
Validation loss: 2.492528300131521

Epoch: 5| Step: 10
Training loss: 2.223720073699951
Validation loss: 2.4972458629197973

Epoch: 142| Step: 0
Training loss: 2.866727590560913
Validation loss: 2.5026752923124578

Epoch: 5| Step: 1
Training loss: 2.924811840057373
Validation loss: 2.501529576957867

Epoch: 5| Step: 2
Training loss: 2.3011958599090576
Validation loss: 2.5002107158783944

Epoch: 5| Step: 3
Training loss: 2.8983218669891357
Validation loss: 2.5003323119173766

Epoch: 5| Step: 4
Training loss: 2.4783215522766113
Validation loss: 2.4977327213492444

Epoch: 5| Step: 5
Training loss: 3.1965627670288086
Validation loss: 2.4899303297842703

Epoch: 5| Step: 6
Training loss: 2.558488130569458
Validation loss: 2.492410167571037

Epoch: 5| Step: 7
Training loss: 3.108515977859497
Validation loss: 2.4950691320562877

Epoch: 5| Step: 8
Training loss: 2.5225539207458496
Validation loss: 2.4956275980959655

Epoch: 5| Step: 9
Training loss: 2.5126938819885254
Validation loss: 2.495099380452146

Epoch: 5| Step: 10
Training loss: 2.1568446159362793
Validation loss: 2.499153349989204

Epoch: 143| Step: 0
Training loss: 2.432630777359009
Validation loss: 2.4952957501975437

Epoch: 5| Step: 1
Training loss: 2.8420209884643555
Validation loss: 2.493484022796795

Epoch: 5| Step: 2
Training loss: 2.7782702445983887
Validation loss: 2.496785961171632

Epoch: 5| Step: 3
Training loss: 3.02997088432312
Validation loss: 2.497992874473654

Epoch: 5| Step: 4
Training loss: 2.43070125579834
Validation loss: 2.487780640202184

Epoch: 5| Step: 5
Training loss: 2.1017937660217285
Validation loss: 2.4799565038373395

Epoch: 5| Step: 6
Training loss: 2.870793581008911
Validation loss: 2.480568931948754

Epoch: 5| Step: 7
Training loss: 2.7808756828308105
Validation loss: 2.4766805377057803

Epoch: 5| Step: 8
Training loss: 2.9668657779693604
Validation loss: 2.481540620967906

Epoch: 5| Step: 9
Training loss: 2.22102427482605
Validation loss: 2.482046324719665

Epoch: 5| Step: 10
Training loss: 3.168771266937256
Validation loss: 2.4795581576644734

Epoch: 144| Step: 0
Training loss: 2.1977667808532715
Validation loss: 2.485213536088185

Epoch: 5| Step: 1
Training loss: 3.758662462234497
Validation loss: 2.4868002066048245

Epoch: 5| Step: 2
Training loss: 2.665863037109375
Validation loss: 2.483669191278437

Epoch: 5| Step: 3
Training loss: 2.4648804664611816
Validation loss: 2.4863524667678343

Epoch: 5| Step: 4
Training loss: 2.8106367588043213
Validation loss: 2.4854557001462547

Epoch: 5| Step: 5
Training loss: 3.01725435256958
Validation loss: 2.4846439515390704

Epoch: 5| Step: 6
Training loss: 2.3624420166015625
Validation loss: 2.481396316200174

Epoch: 5| Step: 7
Training loss: 3.0629215240478516
Validation loss: 2.482606398162021

Epoch: 5| Step: 8
Training loss: 2.227245569229126
Validation loss: 2.484000828958327

Epoch: 5| Step: 9
Training loss: 2.255781412124634
Validation loss: 2.4762150318391862

Epoch: 5| Step: 10
Training loss: 2.7099592685699463
Validation loss: 2.4776522164703696

Epoch: 145| Step: 0
Training loss: 2.7437195777893066
Validation loss: 2.470688086684032

Epoch: 5| Step: 1
Training loss: 1.9578489065170288
Validation loss: 2.469738721847534

Epoch: 5| Step: 2
Training loss: 3.007258892059326
Validation loss: 2.471701475881761

Epoch: 5| Step: 3
Training loss: 2.565467357635498
Validation loss: 2.468019470091789

Epoch: 5| Step: 4
Training loss: 1.6632449626922607
Validation loss: 2.4680917545031478

Epoch: 5| Step: 5
Training loss: 2.418773651123047
Validation loss: 2.470539354508923

Epoch: 5| Step: 6
Training loss: 3.1078009605407715
Validation loss: 2.470311798075194

Epoch: 5| Step: 7
Training loss: 2.1813788414001465
Validation loss: 2.4738655218514065

Epoch: 5| Step: 8
Training loss: 4.31985330581665
Validation loss: 2.4844725131988525

Epoch: 5| Step: 9
Training loss: 3.2156779766082764
Validation loss: 2.490153561356247

Epoch: 5| Step: 10
Training loss: 2.2882888317108154
Validation loss: 2.493443772356997

Epoch: 146| Step: 0
Training loss: 2.3491978645324707
Validation loss: 2.493057735504643

Epoch: 5| Step: 1
Training loss: 2.7610068321228027
Validation loss: 2.4926567616001254

Epoch: 5| Step: 2
Training loss: 2.418635845184326
Validation loss: 2.4986492818401707

Epoch: 5| Step: 3
Training loss: 2.9411938190460205
Validation loss: 2.517669798225485

Epoch: 5| Step: 4
Training loss: 2.71610689163208
Validation loss: 2.5050271198313725

Epoch: 5| Step: 5
Training loss: 2.964296817779541
Validation loss: 2.4964847282696794

Epoch: 5| Step: 6
Training loss: 2.4457967281341553
Validation loss: 2.4831970404553156

Epoch: 5| Step: 7
Training loss: 2.247124433517456
Validation loss: 2.472467630140243

Epoch: 5| Step: 8
Training loss: 2.398256540298462
Validation loss: 2.471308377481276

Epoch: 5| Step: 9
Training loss: 3.3349056243896484
Validation loss: 2.469843190203431

Epoch: 5| Step: 10
Training loss: 3.058319330215454
Validation loss: 2.4633434895546205

Epoch: 147| Step: 0
Training loss: 2.0380444526672363
Validation loss: 2.4702365859862296

Epoch: 5| Step: 1
Training loss: 3.2338454723358154
Validation loss: 2.475350864471928

Epoch: 5| Step: 2
Training loss: 2.664130926132202
Validation loss: 2.469741008614981

Epoch: 5| Step: 3
Training loss: 3.376861572265625
Validation loss: 2.4678776956373647

Epoch: 5| Step: 4
Training loss: 2.5896878242492676
Validation loss: 2.4683719719609907

Epoch: 5| Step: 5
Training loss: 2.758286952972412
Validation loss: 2.4724812533265803

Epoch: 5| Step: 6
Training loss: 2.529829263687134
Validation loss: 2.475407441457113

Epoch: 5| Step: 7
Training loss: 2.4029293060302734
Validation loss: 2.471116587679873

Epoch: 5| Step: 8
Training loss: 2.985414981842041
Validation loss: 2.4877894565623295

Epoch: 5| Step: 9
Training loss: 2.425858974456787
Validation loss: 2.4780930396049254

Epoch: 5| Step: 10
Training loss: 2.4051401615142822
Validation loss: 2.4874069254885436

Epoch: 148| Step: 0
Training loss: 2.282468795776367
Validation loss: 2.4751141891684583

Epoch: 5| Step: 1
Training loss: 3.1243743896484375
Validation loss: 2.47682697285888

Epoch: 5| Step: 2
Training loss: 2.5771992206573486
Validation loss: 2.477911073674438

Epoch: 5| Step: 3
Training loss: 2.769479751586914
Validation loss: 2.465771626400691

Epoch: 5| Step: 4
Training loss: 2.858513355255127
Validation loss: 2.4682127314229168

Epoch: 5| Step: 5
Training loss: 2.1008505821228027
Validation loss: 2.4688323979736655

Epoch: 5| Step: 6
Training loss: 2.5026564598083496
Validation loss: 2.469389625774917

Epoch: 5| Step: 7
Training loss: 2.5662930011749268
Validation loss: 2.463382867074782

Epoch: 5| Step: 8
Training loss: 2.9285147190093994
Validation loss: 2.468638453432309

Epoch: 5| Step: 9
Training loss: 2.7522895336151123
Validation loss: 2.475808548670943

Epoch: 5| Step: 10
Training loss: 2.983006238937378
Validation loss: 2.4760616466563237

Epoch: 149| Step: 0
Training loss: 2.6615471839904785
Validation loss: 2.478979810591667

Epoch: 5| Step: 1
Training loss: 3.177924156188965
Validation loss: 2.4812090627608763

Epoch: 5| Step: 2
Training loss: 2.641573190689087
Validation loss: 2.48867711200509

Epoch: 5| Step: 3
Training loss: 2.2332749366760254
Validation loss: 2.485454677253641

Epoch: 5| Step: 4
Training loss: 2.5899815559387207
Validation loss: 2.483247500593944

Epoch: 5| Step: 5
Training loss: 2.5067343711853027
Validation loss: 2.4858449171948176

Epoch: 5| Step: 6
Training loss: 2.6874217987060547
Validation loss: 2.479252597337128

Epoch: 5| Step: 7
Training loss: 2.9433653354644775
Validation loss: 2.4687479926693823

Epoch: 5| Step: 8
Training loss: 2.4876604080200195
Validation loss: 2.4658301004799466

Epoch: 5| Step: 9
Training loss: 2.440035104751587
Validation loss: 2.4699377526519117

Epoch: 5| Step: 10
Training loss: 3.1609339714050293
Validation loss: 2.465872280059322

Epoch: 150| Step: 0
Training loss: 1.9747596979141235
Validation loss: 2.4646814612932104

Epoch: 5| Step: 1
Training loss: 2.7430033683776855
Validation loss: 2.4627515936410553

Epoch: 5| Step: 2
Training loss: 2.9711430072784424
Validation loss: 2.4711018198279926

Epoch: 5| Step: 3
Training loss: 2.8390371799468994
Validation loss: 2.4652917615828978

Epoch: 5| Step: 4
Training loss: 2.4806487560272217
Validation loss: 2.462504222828855

Epoch: 5| Step: 5
Training loss: 2.7858684062957764
Validation loss: 2.4790046420148624

Epoch: 5| Step: 6
Training loss: 2.6360666751861572
Validation loss: 2.4884036894767516

Epoch: 5| Step: 7
Training loss: 2.843966007232666
Validation loss: 2.4877707842857606

Epoch: 5| Step: 8
Training loss: 3.1383328437805176
Validation loss: 2.490563943821897

Epoch: 5| Step: 9
Training loss: 2.7382917404174805
Validation loss: 2.5089962302997546

Epoch: 5| Step: 10
Training loss: 2.339656114578247
Validation loss: 2.509737609535135

Epoch: 151| Step: 0
Training loss: 2.9618563652038574
Validation loss: 2.5119168835301555

Epoch: 5| Step: 1
Training loss: 2.4406704902648926
Validation loss: 2.4989113935860257

Epoch: 5| Step: 2
Training loss: 2.5099027156829834
Validation loss: 2.4845108652627594

Epoch: 5| Step: 3
Training loss: 2.463132381439209
Validation loss: 2.468187109116585

Epoch: 5| Step: 4
Training loss: 3.064868211746216
Validation loss: 2.465637835123206

Epoch: 5| Step: 5
Training loss: 2.571798801422119
Validation loss: 2.467955381639542

Epoch: 5| Step: 6
Training loss: 2.502089738845825
Validation loss: 2.464402824319819

Epoch: 5| Step: 7
Training loss: 2.5993235111236572
Validation loss: 2.468666268933204

Epoch: 5| Step: 8
Training loss: 2.672192096710205
Validation loss: 2.473769546836935

Epoch: 5| Step: 9
Training loss: 2.8812289237976074
Validation loss: 2.4766428470611572

Epoch: 5| Step: 10
Training loss: 2.8339755535125732
Validation loss: 2.498032631412629

Epoch: 152| Step: 0
Training loss: 2.658402919769287
Validation loss: 2.5086828918867212

Epoch: 5| Step: 1
Training loss: 3.021937847137451
Validation loss: 2.5151584635498705

Epoch: 5| Step: 2
Training loss: 2.437330484390259
Validation loss: 2.5128011575309177

Epoch: 5| Step: 3
Training loss: 2.8005006313323975
Validation loss: 2.4978062465626705

Epoch: 5| Step: 4
Training loss: 2.8247170448303223
Validation loss: 2.4755347646692747

Epoch: 5| Step: 5
Training loss: 3.0672669410705566
Validation loss: 2.472327357979231

Epoch: 5| Step: 6
Training loss: 3.0230813026428223
Validation loss: 2.466992949926725

Epoch: 5| Step: 7
Training loss: 2.0814106464385986
Validation loss: 2.4607093334198

Epoch: 5| Step: 8
Training loss: 2.798222064971924
Validation loss: 2.4561827298133605

Epoch: 5| Step: 9
Training loss: 1.9885234832763672
Validation loss: 2.457308843571653

Epoch: 5| Step: 10
Training loss: 2.9249520301818848
Validation loss: 2.4597952083874772

Epoch: 153| Step: 0
Training loss: 2.5932457447052
Validation loss: 2.4641934312799925

Epoch: 5| Step: 1
Training loss: 3.009411096572876
Validation loss: 2.4809067864571848

Epoch: 5| Step: 2
Training loss: 2.2349343299865723
Validation loss: 2.4926020278725574

Epoch: 5| Step: 3
Training loss: 2.407172918319702
Validation loss: 2.510497731547202

Epoch: 5| Step: 4
Training loss: 2.9695682525634766
Validation loss: 2.504918541959537

Epoch: 5| Step: 5
Training loss: 3.0403552055358887
Validation loss: 2.4956631045187674

Epoch: 5| Step: 6
Training loss: 2.117478847503662
Validation loss: 2.486268171700098

Epoch: 5| Step: 7
Training loss: 2.461686849594116
Validation loss: 2.478702809221001

Epoch: 5| Step: 8
Training loss: 3.049117088317871
Validation loss: 2.4770072506320093

Epoch: 5| Step: 9
Training loss: 3.003891944885254
Validation loss: 2.4673377621558403

Epoch: 5| Step: 10
Training loss: 2.5005011558532715
Validation loss: 2.4728446673321467

Epoch: 154| Step: 0
Training loss: 2.447765827178955
Validation loss: 2.4672205422514226

Epoch: 5| Step: 1
Training loss: 2.3043904304504395
Validation loss: 2.4674566253539054

Epoch: 5| Step: 2
Training loss: 2.6926369667053223
Validation loss: 2.4652850807354016

Epoch: 5| Step: 3
Training loss: 3.0909810066223145
Validation loss: 2.47586650489479

Epoch: 5| Step: 4
Training loss: 2.089231491088867
Validation loss: 2.4696295645929154

Epoch: 5| Step: 5
Training loss: 2.9995405673980713
Validation loss: 2.479161157402941

Epoch: 5| Step: 6
Training loss: 2.760789632797241
Validation loss: 2.477388387085289

Epoch: 5| Step: 7
Training loss: 3.0925986766815186
Validation loss: 2.4853152049485074

Epoch: 5| Step: 8
Training loss: 2.959106922149658
Validation loss: 2.4868556812245357

Epoch: 5| Step: 9
Training loss: 2.6613364219665527
Validation loss: 2.480489456525413

Epoch: 5| Step: 10
Training loss: 2.3258275985717773
Validation loss: 2.4664201531358945

Epoch: 155| Step: 0
Training loss: 2.540729522705078
Validation loss: 2.4778301382577546

Epoch: 5| Step: 1
Training loss: 2.3782458305358887
Validation loss: 2.4793869577428347

Epoch: 5| Step: 2
Training loss: 2.4803004264831543
Validation loss: 2.483540368336503

Epoch: 5| Step: 3
Training loss: 2.6441314220428467
Validation loss: 2.4882112421015257

Epoch: 5| Step: 4
Training loss: 2.299936532974243
Validation loss: 2.494959424900752

Epoch: 5| Step: 5
Training loss: 3.276402235031128
Validation loss: 2.491712418935632

Epoch: 5| Step: 6
Training loss: 2.384552001953125
Validation loss: 2.4950826014241865

Epoch: 5| Step: 7
Training loss: 3.449498414993286
Validation loss: 2.495505904638639

Epoch: 5| Step: 8
Training loss: 1.920019507408142
Validation loss: 2.4857138818310154

Epoch: 5| Step: 9
Training loss: 2.944392442703247
Validation loss: 2.487637112217565

Epoch: 5| Step: 10
Training loss: 3.1737542152404785
Validation loss: 2.475640444345372

Epoch: 156| Step: 0
Training loss: 2.824584484100342
Validation loss: 2.4742251211597073

Epoch: 5| Step: 1
Training loss: 2.742154598236084
Validation loss: 2.4717786773558585

Epoch: 5| Step: 2
Training loss: 2.3162567615509033
Validation loss: 2.473158008308821

Epoch: 5| Step: 3
Training loss: 2.676687717437744
Validation loss: 2.4851220628266693

Epoch: 5| Step: 4
Training loss: 2.4835846424102783
Validation loss: 2.4856990255335325

Epoch: 5| Step: 5
Training loss: 3.464259624481201
Validation loss: 2.5011221208880023

Epoch: 5| Step: 6
Training loss: 2.8352041244506836
Validation loss: 2.496570981958861

Epoch: 5| Step: 7
Training loss: 3.1712851524353027
Validation loss: 2.497625102279007

Epoch: 5| Step: 8
Training loss: 2.4588804244995117
Validation loss: 2.4814306997483775

Epoch: 5| Step: 9
Training loss: 2.2153573036193848
Validation loss: 2.4651542261082637

Epoch: 5| Step: 10
Training loss: 2.292292594909668
Validation loss: 2.462162571568643

Epoch: 157| Step: 0
Training loss: 3.0294530391693115
Validation loss: 2.4640835638969176

Epoch: 5| Step: 1
Training loss: 2.3000361919403076
Validation loss: 2.463410915866975

Epoch: 5| Step: 2
Training loss: 2.6001548767089844
Validation loss: 2.466097982980872

Epoch: 5| Step: 3
Training loss: 3.1282169818878174
Validation loss: 2.4710306711094354

Epoch: 5| Step: 4
Training loss: 2.1750094890594482
Validation loss: 2.4765154982125885

Epoch: 5| Step: 5
Training loss: 2.4343864917755127
Validation loss: 2.483522025487756

Epoch: 5| Step: 6
Training loss: 2.5333406925201416
Validation loss: 2.486867791862898

Epoch: 5| Step: 7
Training loss: 2.8927483558654785
Validation loss: 2.4770097168543006

Epoch: 5| Step: 8
Training loss: 2.7924180030822754
Validation loss: 2.4865070004617014

Epoch: 5| Step: 9
Training loss: 2.7390031814575195
Validation loss: 2.480854376669853

Epoch: 5| Step: 10
Training loss: 2.8151631355285645
Validation loss: 2.4742075550940728

Epoch: 158| Step: 0
Training loss: 2.93991756439209
Validation loss: 2.465420481979206

Epoch: 5| Step: 1
Training loss: 2.0752856731414795
Validation loss: 2.4673098338547574

Epoch: 5| Step: 2
Training loss: 2.561619997024536
Validation loss: 2.460345927105155

Epoch: 5| Step: 3
Training loss: 3.4232094287872314
Validation loss: 2.4603935108389905

Epoch: 5| Step: 4
Training loss: 2.3586678504943848
Validation loss: 2.4690940662096907

Epoch: 5| Step: 5
Training loss: 1.8465417623519897
Validation loss: 2.4622692062008764

Epoch: 5| Step: 6
Training loss: 3.2377753257751465
Validation loss: 2.4668071269989014

Epoch: 5| Step: 7
Training loss: 2.785961151123047
Validation loss: 2.4511951451660483

Epoch: 5| Step: 8
Training loss: 3.0766289234161377
Validation loss: 2.4610123993248068

Epoch: 5| Step: 9
Training loss: 2.128009796142578
Validation loss: 2.4695312874291533

Epoch: 5| Step: 10
Training loss: 2.9716851711273193
Validation loss: 2.46743243996815

Epoch: 159| Step: 0
Training loss: 2.4191203117370605
Validation loss: 2.471488880854781

Epoch: 5| Step: 1
Training loss: 2.548793315887451
Validation loss: 2.461975082274406

Epoch: 5| Step: 2
Training loss: 3.271338939666748
Validation loss: 2.4553056122154318

Epoch: 5| Step: 3
Training loss: 1.9873508214950562
Validation loss: 2.46728418719384

Epoch: 5| Step: 4
Training loss: 3.2772018909454346
Validation loss: 2.4691321516549714

Epoch: 5| Step: 5
Training loss: 3.2025229930877686
Validation loss: 2.460362131877612

Epoch: 5| Step: 6
Training loss: 2.7920467853546143
Validation loss: 2.4576349386604885

Epoch: 5| Step: 7
Training loss: 2.7678236961364746
Validation loss: 2.4574138015829106

Epoch: 5| Step: 8
Training loss: 2.2361457347869873
Validation loss: 2.455217457586719

Epoch: 5| Step: 9
Training loss: 2.4787378311157227
Validation loss: 2.456162334770285

Epoch: 5| Step: 10
Training loss: 2.2419636249542236
Validation loss: 2.4564462707888697

Epoch: 160| Step: 0
Training loss: 2.2657692432403564
Validation loss: 2.4555469969267487

Epoch: 5| Step: 1
Training loss: 2.427537441253662
Validation loss: 2.451464009541337

Epoch: 5| Step: 2
Training loss: 2.832576036453247
Validation loss: 2.4564779625144055

Epoch: 5| Step: 3
Training loss: 3.171268939971924
Validation loss: 2.4606137583332677

Epoch: 5| Step: 4
Training loss: 3.0460453033447266
Validation loss: 2.4552945859970583

Epoch: 5| Step: 5
Training loss: 3.179457664489746
Validation loss: 2.4559799676300376

Epoch: 5| Step: 6
Training loss: 2.630704402923584
Validation loss: 2.45733537597041

Epoch: 5| Step: 7
Training loss: 2.4901461601257324
Validation loss: 2.455440869895361

Epoch: 5| Step: 8
Training loss: 2.352177143096924
Validation loss: 2.449962980003767

Epoch: 5| Step: 9
Training loss: 2.485086441040039
Validation loss: 2.4515515540235784

Epoch: 5| Step: 10
Training loss: 2.381988525390625
Validation loss: 2.466696411050776

Epoch: 161| Step: 0
Training loss: 2.6026108264923096
Validation loss: 2.4785206651174896

Epoch: 5| Step: 1
Training loss: 2.8819050788879395
Validation loss: 2.4798084100087485

Epoch: 5| Step: 2
Training loss: 2.3783724308013916
Validation loss: 2.487726173093242

Epoch: 5| Step: 3
Training loss: 2.7484817504882812
Validation loss: 2.489072363863709

Epoch: 5| Step: 4
Training loss: 2.683236837387085
Validation loss: 2.4815100085350776

Epoch: 5| Step: 5
Training loss: 2.1392903327941895
Validation loss: 2.472480712398406

Epoch: 5| Step: 6
Training loss: 3.0451231002807617
Validation loss: 2.4786754321026545

Epoch: 5| Step: 7
Training loss: 2.9709994792938232
Validation loss: 2.4677649005766837

Epoch: 5| Step: 8
Training loss: 3.2086291313171387
Validation loss: 2.463971371291786

Epoch: 5| Step: 9
Training loss: 2.2699403762817383
Validation loss: 2.4652599980754237

Epoch: 5| Step: 10
Training loss: 2.492154836654663
Validation loss: 2.451899100375432

Epoch: 162| Step: 0
Training loss: 2.5480613708496094
Validation loss: 2.450199216924688

Epoch: 5| Step: 1
Training loss: 2.7065885066986084
Validation loss: 2.4517183662742696

Epoch: 5| Step: 2
Training loss: 2.233642578125
Validation loss: 2.4503473389533257

Epoch: 5| Step: 3
Training loss: 2.56493878364563
Validation loss: 2.4539011473296792

Epoch: 5| Step: 4
Training loss: 2.8742775917053223
Validation loss: 2.454456977946784

Epoch: 5| Step: 5
Training loss: 2.6824822425842285
Validation loss: 2.464922256367181

Epoch: 5| Step: 6
Training loss: 2.973680019378662
Validation loss: 2.463439631205733

Epoch: 5| Step: 7
Training loss: 2.7046375274658203
Validation loss: 2.464147734385665

Epoch: 5| Step: 8
Training loss: 2.414398670196533
Validation loss: 2.4671686131467103

Epoch: 5| Step: 9
Training loss: 2.839756488800049
Validation loss: 2.472379699830086

Epoch: 5| Step: 10
Training loss: 3.0288243293762207
Validation loss: 2.4757826841005715

Epoch: 163| Step: 0
Training loss: 2.962864637374878
Validation loss: 2.4754410405312814

Epoch: 5| Step: 1
Training loss: 2.5150482654571533
Validation loss: 2.4810247087991364

Epoch: 5| Step: 2
Training loss: 2.798395872116089
Validation loss: 2.476843628832089

Epoch: 5| Step: 3
Training loss: 2.1932120323181152
Validation loss: 2.4667225089124454

Epoch: 5| Step: 4
Training loss: 2.942134380340576
Validation loss: 2.459372792192685

Epoch: 5| Step: 5
Training loss: 2.7769150733947754
Validation loss: 2.447938708848851

Epoch: 5| Step: 6
Training loss: 2.697591781616211
Validation loss: 2.449195049142325

Epoch: 5| Step: 7
Training loss: 2.508638858795166
Validation loss: 2.4560777295020317

Epoch: 5| Step: 8
Training loss: 2.3682022094726562
Validation loss: 2.448566311149187

Epoch: 5| Step: 9
Training loss: 2.8051440715789795
Validation loss: 2.453051356859105

Epoch: 5| Step: 10
Training loss: 2.9475109577178955
Validation loss: 2.46430475993823

Epoch: 164| Step: 0
Training loss: 2.4950568675994873
Validation loss: 2.4751393718104207

Epoch: 5| Step: 1
Training loss: 2.7476279735565186
Validation loss: 2.480336489215974

Epoch: 5| Step: 2
Training loss: 2.6298742294311523
Validation loss: 2.495423783538162

Epoch: 5| Step: 3
Training loss: 2.7739930152893066
Validation loss: 2.5078017711639404

Epoch: 5| Step: 4
Training loss: 3.120034694671631
Validation loss: 2.5042459823751964

Epoch: 5| Step: 5
Training loss: 1.7911609411239624
Validation loss: 2.498350792033698

Epoch: 5| Step: 6
Training loss: 2.8089945316314697
Validation loss: 2.4875566318470943

Epoch: 5| Step: 7
Training loss: 2.9008405208587646
Validation loss: 2.4734461076797976

Epoch: 5| Step: 8
Training loss: 2.279597282409668
Validation loss: 2.4578499819642756

Epoch: 5| Step: 9
Training loss: 2.509127378463745
Validation loss: 2.4520463328207693

Epoch: 5| Step: 10
Training loss: 3.455117702484131
Validation loss: 2.4464740906992266

Epoch: 165| Step: 0
Training loss: 2.8670454025268555
Validation loss: 2.445906126370994

Epoch: 5| Step: 1
Training loss: 2.679227113723755
Validation loss: 2.4454269562998125

Epoch: 5| Step: 2
Training loss: 2.676105260848999
Validation loss: 2.4481507539749146

Epoch: 5| Step: 3
Training loss: 3.0155210494995117
Validation loss: 2.456323931294103

Epoch: 5| Step: 4
Training loss: 2.2327044010162354
Validation loss: 2.454339245314239

Epoch: 5| Step: 5
Training loss: 2.441797971725464
Validation loss: 2.4583298852366786

Epoch: 5| Step: 6
Training loss: 2.7139110565185547
Validation loss: 2.4498241460451515

Epoch: 5| Step: 7
Training loss: 2.733288049697876
Validation loss: 2.4525442892505276

Epoch: 5| Step: 8
Training loss: 2.712538242340088
Validation loss: 2.4545747580066806

Epoch: 5| Step: 9
Training loss: 2.515352725982666
Validation loss: 2.4520048351698023

Epoch: 5| Step: 10
Training loss: 2.75339412689209
Validation loss: 2.446274442057456

Epoch: 166| Step: 0
Training loss: 2.6931910514831543
Validation loss: 2.45128648255461

Epoch: 5| Step: 1
Training loss: 2.413083791732788
Validation loss: 2.4457996096662296

Epoch: 5| Step: 2
Training loss: 3.2760684490203857
Validation loss: 2.448243687229772

Epoch: 5| Step: 3
Training loss: 2.141425371170044
Validation loss: 2.45420471314461

Epoch: 5| Step: 4
Training loss: 2.8687801361083984
Validation loss: 2.455363501784622

Epoch: 5| Step: 5
Training loss: 2.709088087081909
Validation loss: 2.4471147214212725

Epoch: 5| Step: 6
Training loss: 2.727606773376465
Validation loss: 2.465228772932483

Epoch: 5| Step: 7
Training loss: 2.4371776580810547
Validation loss: 2.4576050876289286

Epoch: 5| Step: 8
Training loss: 2.819262981414795
Validation loss: 2.458469372923656

Epoch: 5| Step: 9
Training loss: 2.706688642501831
Validation loss: 2.451420489177909

Epoch: 5| Step: 10
Training loss: 2.514500617980957
Validation loss: 2.4573304012257564

Epoch: 167| Step: 0
Training loss: 2.494020462036133
Validation loss: 2.4480461023187123

Epoch: 5| Step: 1
Training loss: 2.4433019161224365
Validation loss: 2.450196414865473

Epoch: 5| Step: 2
Training loss: 2.595655918121338
Validation loss: 2.453137415711598

Epoch: 5| Step: 3
Training loss: 2.956838369369507
Validation loss: 2.450463359073926

Epoch: 5| Step: 4
Training loss: 3.0447657108306885
Validation loss: 2.4529811079784105

Epoch: 5| Step: 5
Training loss: 2.6342480182647705
Validation loss: 2.451077333060644

Epoch: 5| Step: 6
Training loss: 3.1121630668640137
Validation loss: 2.4569688612414944

Epoch: 5| Step: 7
Training loss: 2.4422550201416016
Validation loss: 2.4561337655590427

Epoch: 5| Step: 8
Training loss: 2.6526265144348145
Validation loss: 2.453354935492239

Epoch: 5| Step: 9
Training loss: 2.1578147411346436
Validation loss: 2.451368062726913

Epoch: 5| Step: 10
Training loss: 2.716426372528076
Validation loss: 2.4528103336211173

Epoch: 168| Step: 0
Training loss: 3.0980796813964844
Validation loss: 2.445577638123625

Epoch: 5| Step: 1
Training loss: 2.838387966156006
Validation loss: 2.4488522493711082

Epoch: 5| Step: 2
Training loss: 2.5752129554748535
Validation loss: 2.4513240860354517

Epoch: 5| Step: 3
Training loss: 2.6192049980163574
Validation loss: 2.443954062718217

Epoch: 5| Step: 4
Training loss: 2.0318562984466553
Validation loss: 2.4468233764812513

Epoch: 5| Step: 5
Training loss: 2.7505605220794678
Validation loss: 2.4465313291036956

Epoch: 5| Step: 6
Training loss: 2.3110833168029785
Validation loss: 2.4461843326527584

Epoch: 5| Step: 7
Training loss: 3.277683734893799
Validation loss: 2.4466456136395855

Epoch: 5| Step: 8
Training loss: 2.8536109924316406
Validation loss: 2.4441038895678777

Epoch: 5| Step: 9
Training loss: 2.177781343460083
Validation loss: 2.4428878932870846

Epoch: 5| Step: 10
Training loss: 2.7131459712982178
Validation loss: 2.442557798918857

Epoch: 169| Step: 0
Training loss: 2.5197107791900635
Validation loss: 2.448012177662183

Epoch: 5| Step: 1
Training loss: 2.4374659061431885
Validation loss: 2.441677990780082

Epoch: 5| Step: 2
Training loss: 3.3143208026885986
Validation loss: 2.4505006908088602

Epoch: 5| Step: 3
Training loss: 2.679226875305176
Validation loss: 2.4543329003036662

Epoch: 5| Step: 4
Training loss: 2.7884650230407715
Validation loss: 2.4518825636115125

Epoch: 5| Step: 5
Training loss: 2.102642297744751
Validation loss: 2.4524359113426617

Epoch: 5| Step: 6
Training loss: 3.1904232501983643
Validation loss: 2.4463843440496795

Epoch: 5| Step: 7
Training loss: 3.1495866775512695
Validation loss: 2.448690096537272

Epoch: 5| Step: 8
Training loss: 2.486757278442383
Validation loss: 2.4517534702054915

Epoch: 5| Step: 9
Training loss: 2.2099156379699707
Validation loss: 2.4513806835297616

Epoch: 5| Step: 10
Training loss: 2.2865095138549805
Validation loss: 2.4465231613446305

Epoch: 170| Step: 0
Training loss: 2.520580291748047
Validation loss: 2.439598819260956

Epoch: 5| Step: 1
Training loss: 2.905690908432007
Validation loss: 2.4420712532535678

Epoch: 5| Step: 2
Training loss: 2.544633388519287
Validation loss: 2.4403541600832375

Epoch: 5| Step: 3
Training loss: 2.594791889190674
Validation loss: 2.4416431278310795

Epoch: 5| Step: 4
Training loss: 2.946599245071411
Validation loss: 2.4451739839328233

Epoch: 5| Step: 5
Training loss: 2.0316364765167236
Validation loss: 2.4482945472963396

Epoch: 5| Step: 6
Training loss: 3.038626194000244
Validation loss: 2.44177254041036

Epoch: 5| Step: 7
Training loss: 3.2215378284454346
Validation loss: 2.4572557300649662

Epoch: 5| Step: 8
Training loss: 2.4818615913391113
Validation loss: 2.4614107378067507

Epoch: 5| Step: 9
Training loss: 2.383131504058838
Validation loss: 2.4666377293166293

Epoch: 5| Step: 10
Training loss: 2.5061581134796143
Validation loss: 2.453775203356179

Epoch: 171| Step: 0
Training loss: 2.2056403160095215
Validation loss: 2.443476464158745

Epoch: 5| Step: 1
Training loss: 3.052993059158325
Validation loss: 2.4413084266006306

Epoch: 5| Step: 2
Training loss: 3.024221658706665
Validation loss: 2.438101171165384

Epoch: 5| Step: 3
Training loss: 2.503483295440674
Validation loss: 2.4383591144315657

Epoch: 5| Step: 4
Training loss: 2.0761396884918213
Validation loss: 2.4355104789938977

Epoch: 5| Step: 5
Training loss: 2.705749034881592
Validation loss: 2.4344236363646803

Epoch: 5| Step: 6
Training loss: 2.3972792625427246
Validation loss: 2.4337286513338805

Epoch: 5| Step: 7
Training loss: 3.150397777557373
Validation loss: 2.4385566173061246

Epoch: 5| Step: 8
Training loss: 3.0308737754821777
Validation loss: 2.434437346714799

Epoch: 5| Step: 9
Training loss: 2.5508322715759277
Validation loss: 2.435711442783315

Epoch: 5| Step: 10
Training loss: 2.4445295333862305
Validation loss: 2.440526852043726

Epoch: 172| Step: 0
Training loss: 2.800191879272461
Validation loss: 2.430448175758444

Epoch: 5| Step: 1
Training loss: 2.2907040119171143
Validation loss: 2.4326772869274182

Epoch: 5| Step: 2
Training loss: 2.5535569190979004
Validation loss: 2.437766634007936

Epoch: 5| Step: 3
Training loss: 2.296116590499878
Validation loss: 2.4370360502632717

Epoch: 5| Step: 4
Training loss: 2.7570865154266357
Validation loss: 2.4380648546321417

Epoch: 5| Step: 5
Training loss: 3.041372776031494
Validation loss: 2.4458124304330475

Epoch: 5| Step: 6
Training loss: 3.0233330726623535
Validation loss: 2.4359839270191808

Epoch: 5| Step: 7
Training loss: 2.283400297164917
Validation loss: 2.441672432807184

Epoch: 5| Step: 8
Training loss: 3.2670695781707764
Validation loss: 2.4537008629050305

Epoch: 5| Step: 9
Training loss: 2.6145904064178467
Validation loss: 2.4391263608009583

Epoch: 5| Step: 10
Training loss: 2.154938220977783
Validation loss: 2.453237659187727

Epoch: 173| Step: 0
Training loss: 3.1295077800750732
Validation loss: 2.4470981936300955

Epoch: 5| Step: 1
Training loss: 2.4724836349487305
Validation loss: 2.452640261701358

Epoch: 5| Step: 2
Training loss: 2.7013633251190186
Validation loss: 2.4531971998112176

Epoch: 5| Step: 3
Training loss: 2.8593926429748535
Validation loss: 2.448715135615359

Epoch: 5| Step: 4
Training loss: 2.6050333976745605
Validation loss: 2.4371632196569957

Epoch: 5| Step: 5
Training loss: 2.2680795192718506
Validation loss: 2.4395050284683064

Epoch: 5| Step: 6
Training loss: 2.4446659088134766
Validation loss: 2.437435996147894

Epoch: 5| Step: 7
Training loss: 2.733264684677124
Validation loss: 2.4361834628607637

Epoch: 5| Step: 8
Training loss: 3.470510482788086
Validation loss: 2.435532451957785

Epoch: 5| Step: 9
Training loss: 2.125047445297241
Validation loss: 2.438712412311185

Epoch: 5| Step: 10
Training loss: 2.3268959522247314
Validation loss: 2.4329656375351774

Epoch: 174| Step: 0
Training loss: 2.6605381965637207
Validation loss: 2.432059852025842

Epoch: 5| Step: 1
Training loss: 2.708892345428467
Validation loss: 2.437670394938479

Epoch: 5| Step: 2
Training loss: 3.0195212364196777
Validation loss: 2.439172693478164

Epoch: 5| Step: 3
Training loss: 1.8138372898101807
Validation loss: 2.4358628411446848

Epoch: 5| Step: 4
Training loss: 2.7407004833221436
Validation loss: 2.436011083664433

Epoch: 5| Step: 5
Training loss: 3.1110177040100098
Validation loss: 2.4375954110135316

Epoch: 5| Step: 6
Training loss: 1.775231957435608
Validation loss: 2.437560930046984

Epoch: 5| Step: 7
Training loss: 3.2558517456054688
Validation loss: 2.4293194637503674

Epoch: 5| Step: 8
Training loss: 3.2267327308654785
Validation loss: 2.4341768154533963

Epoch: 5| Step: 9
Training loss: 2.3539581298828125
Validation loss: 2.4338405209202922

Epoch: 5| Step: 10
Training loss: 2.4094038009643555
Validation loss: 2.433494439689062

Epoch: 175| Step: 0
Training loss: 2.7537875175476074
Validation loss: 2.4317309138595418

Epoch: 5| Step: 1
Training loss: 2.8089137077331543
Validation loss: 2.4358825581048125

Epoch: 5| Step: 2
Training loss: 2.464664936065674
Validation loss: 2.43877734163756

Epoch: 5| Step: 3
Training loss: 2.4198219776153564
Validation loss: 2.435279487281717

Epoch: 5| Step: 4
Training loss: 2.874300241470337
Validation loss: 2.4357366895162933

Epoch: 5| Step: 5
Training loss: 2.128159999847412
Validation loss: 2.444375909784789

Epoch: 5| Step: 6
Training loss: 2.5794711112976074
Validation loss: 2.435899173059771

Epoch: 5| Step: 7
Training loss: 2.574768543243408
Validation loss: 2.4322896593360492

Epoch: 5| Step: 8
Training loss: 2.5557913780212402
Validation loss: 2.4327077942509807

Epoch: 5| Step: 9
Training loss: 3.6561622619628906
Validation loss: 2.4294338559591644

Epoch: 5| Step: 10
Training loss: 2.228557825088501
Validation loss: 2.428535484498547

Epoch: 176| Step: 0
Training loss: 2.744158983230591
Validation loss: 2.429410514011178

Epoch: 5| Step: 1
Training loss: 2.44205904006958
Validation loss: 2.4337890814709406

Epoch: 5| Step: 2
Training loss: 2.4622044563293457
Validation loss: 2.4321715447210495

Epoch: 5| Step: 3
Training loss: 2.4534196853637695
Validation loss: 2.429954723645282

Epoch: 5| Step: 4
Training loss: 3.201228618621826
Validation loss: 2.434283051439511

Epoch: 5| Step: 5
Training loss: 1.9815925359725952
Validation loss: 2.4400852905806674

Epoch: 5| Step: 6
Training loss: 2.3788955211639404
Validation loss: 2.4397604926939933

Epoch: 5| Step: 7
Training loss: 2.8471779823303223
Validation loss: 2.4384647902622016

Epoch: 5| Step: 8
Training loss: 2.8655524253845215
Validation loss: 2.4392796844564457

Epoch: 5| Step: 9
Training loss: 3.3658485412597656
Validation loss: 2.4359325798608924

Epoch: 5| Step: 10
Training loss: 2.3218729496002197
Validation loss: 2.4373926142210602

Epoch: 177| Step: 0
Training loss: 2.785212755203247
Validation loss: 2.435616713698192

Epoch: 5| Step: 1
Training loss: 2.755134105682373
Validation loss: 2.4387097333067205

Epoch: 5| Step: 2
Training loss: 2.6372783184051514
Validation loss: 2.4334988388963925

Epoch: 5| Step: 3
Training loss: 2.101047992706299
Validation loss: 2.4339561962312266

Epoch: 5| Step: 4
Training loss: 2.5415635108947754
Validation loss: 2.438318944746448

Epoch: 5| Step: 5
Training loss: 2.148453712463379
Validation loss: 2.4277860400497273

Epoch: 5| Step: 6
Training loss: 3.1944234371185303
Validation loss: 2.4266621758860927

Epoch: 5| Step: 7
Training loss: 2.9226317405700684
Validation loss: 2.4265699540415118

Epoch: 5| Step: 8
Training loss: 1.9457361698150635
Validation loss: 2.4246853423374954

Epoch: 5| Step: 9
Training loss: 2.538154125213623
Validation loss: 2.4262151436139177

Epoch: 5| Step: 10
Training loss: 3.700975179672241
Validation loss: 2.4251202126984954

Epoch: 178| Step: 0
Training loss: 2.055598497390747
Validation loss: 2.421880499009163

Epoch: 5| Step: 1
Training loss: 2.8470823764801025
Validation loss: 2.4220351737032653

Epoch: 5| Step: 2
Training loss: 2.8267226219177246
Validation loss: 2.4214460260124615

Epoch: 5| Step: 3
Training loss: 2.6264710426330566
Validation loss: 2.423576042216311

Epoch: 5| Step: 4
Training loss: 2.5780246257781982
Validation loss: 2.4141957862402803

Epoch: 5| Step: 5
Training loss: 2.3204779624938965
Validation loss: 2.417815746799592

Epoch: 5| Step: 6
Training loss: 3.46941876411438
Validation loss: 2.4227261722728772

Epoch: 5| Step: 7
Training loss: 2.7717537879943848
Validation loss: 2.4173457853255735

Epoch: 5| Step: 8
Training loss: 2.6708824634552
Validation loss: 2.4239200417713453

Epoch: 5| Step: 9
Training loss: 2.0165557861328125
Validation loss: 2.42707751130545

Epoch: 5| Step: 10
Training loss: 2.9063467979431152
Validation loss: 2.440234852093522

Epoch: 179| Step: 0
Training loss: 2.9134202003479004
Validation loss: 2.453520033949165

Epoch: 5| Step: 1
Training loss: 2.2249197959899902
Validation loss: 2.45283688781082

Epoch: 5| Step: 2
Training loss: 2.8528449535369873
Validation loss: 2.4639895628857356

Epoch: 5| Step: 3
Training loss: 2.423915147781372
Validation loss: 2.4405334662365656

Epoch: 5| Step: 4
Training loss: 3.627135753631592
Validation loss: 2.438483084401777

Epoch: 5| Step: 5
Training loss: 2.391268014907837
Validation loss: 2.426337339544809

Epoch: 5| Step: 6
Training loss: 2.1981539726257324
Validation loss: 2.424485550131849

Epoch: 5| Step: 7
Training loss: 1.9061038494110107
Validation loss: 2.429631233215332

Epoch: 5| Step: 8
Training loss: 2.73789644241333
Validation loss: 2.4257254574888494

Epoch: 5| Step: 9
Training loss: 3.031285047531128
Validation loss: 2.418316456579393

Epoch: 5| Step: 10
Training loss: 2.9586052894592285
Validation loss: 2.4202426966800483

Epoch: 180| Step: 0
Training loss: 3.0482287406921387
Validation loss: 2.4185948961524555

Epoch: 5| Step: 1
Training loss: 2.0528857707977295
Validation loss: 2.418822334658715

Epoch: 5| Step: 2
Training loss: 2.6264095306396484
Validation loss: 2.414445005437379

Epoch: 5| Step: 3
Training loss: 2.002896785736084
Validation loss: 2.415036830850827

Epoch: 5| Step: 4
Training loss: 3.1660218238830566
Validation loss: 2.4172205002077165

Epoch: 5| Step: 5
Training loss: 2.1572792530059814
Validation loss: 2.4177550808075936

Epoch: 5| Step: 6
Training loss: 2.9899864196777344
Validation loss: 2.4245517715331046

Epoch: 5| Step: 7
Training loss: 2.2761430740356445
Validation loss: 2.427168112929149

Epoch: 5| Step: 8
Training loss: 2.6462137699127197
Validation loss: 2.4280299230288436

Epoch: 5| Step: 9
Training loss: 3.2768280506134033
Validation loss: 2.425800620868642

Epoch: 5| Step: 10
Training loss: 2.9895763397216797
Validation loss: 2.423316973511891

Epoch: 181| Step: 0
Training loss: 3.0054688453674316
Validation loss: 2.4317134657213764

Epoch: 5| Step: 1
Training loss: 1.8476409912109375
Validation loss: 2.425452042651433

Epoch: 5| Step: 2
Training loss: 2.6772544384002686
Validation loss: 2.422665580626457

Epoch: 5| Step: 3
Training loss: 2.84883189201355
Validation loss: 2.4239626904969573

Epoch: 5| Step: 4
Training loss: 3.0300650596618652
Validation loss: 2.4214152751430387

Epoch: 5| Step: 5
Training loss: 2.4844000339508057
Validation loss: 2.4283696912950083

Epoch: 5| Step: 6
Training loss: 2.3212311267852783
Validation loss: 2.446240104654784

Epoch: 5| Step: 7
Training loss: 3.1127724647521973
Validation loss: 2.4375571025315153

Epoch: 5| Step: 8
Training loss: 2.932758331298828
Validation loss: 2.438033598725514

Epoch: 5| Step: 9
Training loss: 2.117584228515625
Validation loss: 2.4447780322003108

Epoch: 5| Step: 10
Training loss: 2.7256815433502197
Validation loss: 2.431777318318685

Epoch: 182| Step: 0
Training loss: 3.1030163764953613
Validation loss: 2.424854076036843

Epoch: 5| Step: 1
Training loss: 2.535705089569092
Validation loss: 2.4229443355273177

Epoch: 5| Step: 2
Training loss: 2.471097230911255
Validation loss: 2.415679395839732

Epoch: 5| Step: 3
Training loss: 2.874570369720459
Validation loss: 2.415029095065209

Epoch: 5| Step: 4
Training loss: 2.7201733589172363
Validation loss: 2.416930029469152

Epoch: 5| Step: 5
Training loss: 1.6273791790008545
Validation loss: 2.416302337441393

Epoch: 5| Step: 6
Training loss: 2.046062707901001
Validation loss: 2.414783082982545

Epoch: 5| Step: 7
Training loss: 3.213359832763672
Validation loss: 2.4150677701478362

Epoch: 5| Step: 8
Training loss: 3.0017120838165283
Validation loss: 2.4191191811715402

Epoch: 5| Step: 9
Training loss: 2.2549829483032227
Validation loss: 2.420867935303719

Epoch: 5| Step: 10
Training loss: 3.3473730087280273
Validation loss: 2.4183667270086144

Epoch: 183| Step: 0
Training loss: 2.175189256668091
Validation loss: 2.424802767333164

Epoch: 5| Step: 1
Training loss: 2.6360080242156982
Validation loss: 2.4233054807109218

Epoch: 5| Step: 2
Training loss: 2.853212594985962
Validation loss: 2.4248144165162118

Epoch: 5| Step: 3
Training loss: 2.8944997787475586
Validation loss: 2.4277831328812467

Epoch: 5| Step: 4
Training loss: 2.65934157371521
Validation loss: 2.432242835721662

Epoch: 5| Step: 5
Training loss: 2.423182964324951
Validation loss: 2.437757376701601

Epoch: 5| Step: 6
Training loss: 3.2003650665283203
Validation loss: 2.4243733703449206

Epoch: 5| Step: 7
Training loss: 2.470932722091675
Validation loss: 2.4403734053334882

Epoch: 5| Step: 8
Training loss: 2.512671947479248
Validation loss: 2.4349234334884153

Epoch: 5| Step: 9
Training loss: 2.3881921768188477
Validation loss: 2.4444186251650573

Epoch: 5| Step: 10
Training loss: 2.8110551834106445
Validation loss: 2.4460613419932704

Epoch: 184| Step: 0
Training loss: 2.6069817543029785
Validation loss: 2.440441672519971

Epoch: 5| Step: 1
Training loss: 2.8657987117767334
Validation loss: 2.4447771990171043

Epoch: 5| Step: 2
Training loss: 1.842911958694458
Validation loss: 2.4417997124374553

Epoch: 5| Step: 3
Training loss: 2.3547306060791016
Validation loss: 2.43663542245024

Epoch: 5| Step: 4
Training loss: 2.5670762062072754
Validation loss: 2.4472223661279164

Epoch: 5| Step: 5
Training loss: 2.3929615020751953
Validation loss: 2.470284451720535

Epoch: 5| Step: 6
Training loss: 3.0454676151275635
Validation loss: 2.5007308913815405

Epoch: 5| Step: 7
Training loss: 2.5657777786254883
Validation loss: 2.494149825906241

Epoch: 5| Step: 8
Training loss: 2.73907470703125
Validation loss: 2.47266508430563

Epoch: 5| Step: 9
Training loss: 2.7114946842193604
Validation loss: 2.4569941054108324

Epoch: 5| Step: 10
Training loss: 3.5349059104919434
Validation loss: 2.4478962985418176

Epoch: 185| Step: 0
Training loss: 2.97241473197937
Validation loss: 2.4427639271623347

Epoch: 5| Step: 1
Training loss: 2.657756805419922
Validation loss: 2.443881311724263

Epoch: 5| Step: 2
Training loss: 2.8322360515594482
Validation loss: 2.4314922209708922

Epoch: 5| Step: 3
Training loss: 2.5092053413391113
Validation loss: 2.424352571528445

Epoch: 5| Step: 4
Training loss: 2.7105181217193604
Validation loss: 2.4132487850804485

Epoch: 5| Step: 5
Training loss: 2.8919031620025635
Validation loss: 2.4212725803416264

Epoch: 5| Step: 6
Training loss: 3.033212184906006
Validation loss: 2.4161084954456618

Epoch: 5| Step: 7
Training loss: 2.6932406425476074
Validation loss: 2.415296044400943

Epoch: 5| Step: 8
Training loss: 2.060136318206787
Validation loss: 2.4172323237183275

Epoch: 5| Step: 9
Training loss: 2.032867908477783
Validation loss: 2.415887035349364

Epoch: 5| Step: 10
Training loss: 2.624218463897705
Validation loss: 2.4168505540458103

Epoch: 186| Step: 0
Training loss: 2.428650140762329
Validation loss: 2.41207278672085

Epoch: 5| Step: 1
Training loss: 2.2786221504211426
Validation loss: 2.4135874240629134

Epoch: 5| Step: 2
Training loss: 2.66886305809021
Validation loss: 2.414581314209969

Epoch: 5| Step: 3
Training loss: 2.7262425422668457
Validation loss: 2.4190891045396046

Epoch: 5| Step: 4
Training loss: 2.075291872024536
Validation loss: 2.417335207744311

Epoch: 5| Step: 5
Training loss: 2.4721744060516357
Validation loss: 2.4181555240384993

Epoch: 5| Step: 6
Training loss: 3.0742812156677246
Validation loss: 2.4082476016013854

Epoch: 5| Step: 7
Training loss: 3.520268201828003
Validation loss: 2.4046833668985674

Epoch: 5| Step: 8
Training loss: 2.8623528480529785
Validation loss: 2.4043366780845066

Epoch: 5| Step: 9
Training loss: 2.668591022491455
Validation loss: 2.4074118278359853

Epoch: 5| Step: 10
Training loss: 2.2793571949005127
Validation loss: 2.4051644238092567

Epoch: 187| Step: 0
Training loss: 2.9575934410095215
Validation loss: 2.4055759163313013

Epoch: 5| Step: 1
Training loss: 2.093808650970459
Validation loss: 2.401017622281146

Epoch: 5| Step: 2
Training loss: 2.4117863178253174
Validation loss: 2.410973483516324

Epoch: 5| Step: 3
Training loss: 2.3741555213928223
Validation loss: 2.4080712051801783

Epoch: 5| Step: 4
Training loss: 1.9838817119598389
Validation loss: 2.4153581178316506

Epoch: 5| Step: 5
Training loss: 2.724174976348877
Validation loss: 2.4175403528316046

Epoch: 5| Step: 6
Training loss: 2.913302421569824
Validation loss: 2.410940111324351

Epoch: 5| Step: 7
Training loss: 3.129153251647949
Validation loss: 2.415676224616266

Epoch: 5| Step: 8
Training loss: 2.4221463203430176
Validation loss: 2.412154620693576

Epoch: 5| Step: 9
Training loss: 3.272902011871338
Validation loss: 2.41062419901612

Epoch: 5| Step: 10
Training loss: 2.7770018577575684
Validation loss: 2.40641015319414

Epoch: 188| Step: 0
Training loss: 2.4718470573425293
Validation loss: 2.411833276030838

Epoch: 5| Step: 1
Training loss: 2.9404044151306152
Validation loss: 2.40837844469214

Epoch: 5| Step: 2
Training loss: 2.4273886680603027
Validation loss: 2.408665085351595

Epoch: 5| Step: 3
Training loss: 2.5250136852264404
Validation loss: 2.410227398718557

Epoch: 5| Step: 4
Training loss: 2.574108123779297
Validation loss: 2.4108172257741294

Epoch: 5| Step: 5
Training loss: 3.417755603790283
Validation loss: 2.4184242320317093

Epoch: 5| Step: 6
Training loss: 2.404360294342041
Validation loss: 2.4125019760542017

Epoch: 5| Step: 7
Training loss: 2.603480577468872
Validation loss: 2.412153743928479

Epoch: 5| Step: 8
Training loss: 2.3044352531433105
Validation loss: 2.4093621418040287

Epoch: 5| Step: 9
Training loss: 2.72413969039917
Validation loss: 2.410089895289431

Epoch: 5| Step: 10
Training loss: 2.6263387203216553
Validation loss: 2.4062810841427056

Epoch: 189| Step: 0
Training loss: 2.5481553077697754
Validation loss: 2.4012248759628623

Epoch: 5| Step: 1
Training loss: 2.147573471069336
Validation loss: 2.4030046745013167

Epoch: 5| Step: 2
Training loss: 2.4600963592529297
Validation loss: 2.4060204669993412

Epoch: 5| Step: 3
Training loss: 2.6143603324890137
Validation loss: 2.40488189266574

Epoch: 5| Step: 4
Training loss: 3.0777735710144043
Validation loss: 2.4120048656258533

Epoch: 5| Step: 5
Training loss: 2.5894203186035156
Validation loss: 2.4080112031711045

Epoch: 5| Step: 6
Training loss: 1.9932861328125
Validation loss: 2.4077508449554443

Epoch: 5| Step: 7
Training loss: 2.726252317428589
Validation loss: 2.4058259558934036

Epoch: 5| Step: 8
Training loss: 3.2520172595977783
Validation loss: 2.4098652511514644

Epoch: 5| Step: 9
Training loss: 2.661355972290039
Validation loss: 2.411206653041224

Epoch: 5| Step: 10
Training loss: 3.0450520515441895
Validation loss: 2.4138189259395806

Epoch: 190| Step: 0
Training loss: 3.193424701690674
Validation loss: 2.4254809528268795

Epoch: 5| Step: 1
Training loss: 2.897261381149292
Validation loss: 2.4244106277342765

Epoch: 5| Step: 2
Training loss: 2.557744264602661
Validation loss: 2.4229919038793093

Epoch: 5| Step: 3
Training loss: 2.952375888824463
Validation loss: 2.421303149192564

Epoch: 5| Step: 4
Training loss: 2.2703120708465576
Validation loss: 2.412563080428749

Epoch: 5| Step: 5
Training loss: 2.2557787895202637
Validation loss: 2.4190546261366976

Epoch: 5| Step: 6
Training loss: 2.4990687370300293
Validation loss: 2.413600993412797

Epoch: 5| Step: 7
Training loss: 2.87593936920166
Validation loss: 2.41302401532409

Epoch: 5| Step: 8
Training loss: 2.987586259841919
Validation loss: 2.412755309894521

Epoch: 5| Step: 9
Training loss: 1.927210807800293
Validation loss: 2.4095408057653778

Epoch: 5| Step: 10
Training loss: 2.6093389987945557
Validation loss: 2.408649880398986

Epoch: 191| Step: 0
Training loss: 2.2908082008361816
Validation loss: 2.4075307282068397

Epoch: 5| Step: 1
Training loss: 3.330535888671875
Validation loss: 2.409069848316972

Epoch: 5| Step: 2
Training loss: 3.2306694984436035
Validation loss: 2.4094823688589115

Epoch: 5| Step: 3
Training loss: 2.4058737754821777
Validation loss: 2.4080711821074128

Epoch: 5| Step: 4
Training loss: 2.6067404747009277
Validation loss: 2.403185631639214

Epoch: 5| Step: 5
Training loss: 2.552175998687744
Validation loss: 2.4063106172828266

Epoch: 5| Step: 6
Training loss: 2.850512742996216
Validation loss: 2.4046366445479856

Epoch: 5| Step: 7
Training loss: 2.2663121223449707
Validation loss: 2.406058943399819

Epoch: 5| Step: 8
Training loss: 2.4547009468078613
Validation loss: 2.408881564294138

Epoch: 5| Step: 9
Training loss: 2.240525007247925
Validation loss: 2.4013271600969377

Epoch: 5| Step: 10
Training loss: 2.7574071884155273
Validation loss: 2.4065551245084373

Epoch: 192| Step: 0
Training loss: 2.9951255321502686
Validation loss: 2.4056418531684467

Epoch: 5| Step: 1
Training loss: 2.697479486465454
Validation loss: 2.406259940516564

Epoch: 5| Step: 2
Training loss: 2.6607210636138916
Validation loss: 2.4057227001395276

Epoch: 5| Step: 3
Training loss: 2.465860605239868
Validation loss: 2.4076610124239357

Epoch: 5| Step: 4
Training loss: 2.5047621726989746
Validation loss: 2.4034908176750265

Epoch: 5| Step: 5
Training loss: 2.663538694381714
Validation loss: 2.4107380272239767

Epoch: 5| Step: 6
Training loss: 2.512815237045288
Validation loss: 2.412567689854612

Epoch: 5| Step: 7
Training loss: 2.616283416748047
Validation loss: 2.4140979936045985

Epoch: 5| Step: 8
Training loss: 1.9626432657241821
Validation loss: 2.4200005941493536

Epoch: 5| Step: 9
Training loss: 3.1716151237487793
Validation loss: 2.4220008645006406

Epoch: 5| Step: 10
Training loss: 2.7301218509674072
Validation loss: 2.4201540459868727

Epoch: 193| Step: 0
Training loss: 3.201716661453247
Validation loss: 2.4213780920992614

Epoch: 5| Step: 1
Training loss: 2.690039873123169
Validation loss: 2.4165679203566683

Epoch: 5| Step: 2
Training loss: 2.60481333732605
Validation loss: 2.4063148703626407

Epoch: 5| Step: 3
Training loss: 2.6336028575897217
Validation loss: 2.410989717770648

Epoch: 5| Step: 4
Training loss: 2.4466679096221924
Validation loss: 2.4158897040992655

Epoch: 5| Step: 5
Training loss: 2.655937433242798
Validation loss: 2.4219688471927436

Epoch: 5| Step: 6
Training loss: 2.9520044326782227
Validation loss: 2.4169706990641933

Epoch: 5| Step: 7
Training loss: 2.114896297454834
Validation loss: 2.416869383986278

Epoch: 5| Step: 8
Training loss: 2.7596213817596436
Validation loss: 2.4228630937555784

Epoch: 5| Step: 9
Training loss: 2.2873287200927734
Validation loss: 2.4167308730463826

Epoch: 5| Step: 10
Training loss: 2.591475486755371
Validation loss: 2.4232414819860972

Epoch: 194| Step: 0
Training loss: 2.169736623764038
Validation loss: 2.4233996919406358

Epoch: 5| Step: 1
Training loss: 2.5724499225616455
Validation loss: 2.4258161949855026

Epoch: 5| Step: 2
Training loss: 2.9700400829315186
Validation loss: 2.429758366718087

Epoch: 5| Step: 3
Training loss: 2.7496132850646973
Validation loss: 2.4285439957854567

Epoch: 5| Step: 4
Training loss: 2.654477596282959
Validation loss: 2.4309126818051903

Epoch: 5| Step: 5
Training loss: 2.8533267974853516
Validation loss: 2.4401090555293585

Epoch: 5| Step: 6
Training loss: 2.242831230163574
Validation loss: 2.4401238772176925

Epoch: 5| Step: 7
Training loss: 3.043980836868286
Validation loss: 2.4361614924605175

Epoch: 5| Step: 8
Training loss: 2.2040324211120605
Validation loss: 2.4350494543711343

Epoch: 5| Step: 9
Training loss: 2.7817516326904297
Validation loss: 2.437093762941258

Epoch: 5| Step: 10
Training loss: 2.741882085800171
Validation loss: 2.428011102061118

Epoch: 195| Step: 0
Training loss: 1.9828872680664062
Validation loss: 2.4393596546624297

Epoch: 5| Step: 1
Training loss: 2.9879536628723145
Validation loss: 2.427711368888937

Epoch: 5| Step: 2
Training loss: 2.8535869121551514
Validation loss: 2.4303981924569733

Epoch: 5| Step: 3
Training loss: 2.9060473442077637
Validation loss: 2.4237491520502235

Epoch: 5| Step: 4
Training loss: 3.0639021396636963
Validation loss: 2.41269160855201

Epoch: 5| Step: 5
Training loss: 2.928689956665039
Validation loss: 2.414725552323044

Epoch: 5| Step: 6
Training loss: 2.3685717582702637
Validation loss: 2.4143039231659262

Epoch: 5| Step: 7
Training loss: 2.41078519821167
Validation loss: 2.411028418489682

Epoch: 5| Step: 8
Training loss: 2.461759567260742
Validation loss: 2.4136105199013986

Epoch: 5| Step: 9
Training loss: 2.175265073776245
Validation loss: 2.412530283774099

Epoch: 5| Step: 10
Training loss: 2.778090715408325
Validation loss: 2.4096850195238666

Epoch: 196| Step: 0
Training loss: 2.3200478553771973
Validation loss: 2.4115673342058734

Epoch: 5| Step: 1
Training loss: 2.8369877338409424
Validation loss: 2.4081557950665875

Epoch: 5| Step: 2
Training loss: 2.7290749549865723
Validation loss: 2.4067722623066237

Epoch: 5| Step: 3
Training loss: 2.9742188453674316
Validation loss: 2.4072412880518104

Epoch: 5| Step: 4
Training loss: 2.616952419281006
Validation loss: 2.404854897529848

Epoch: 5| Step: 5
Training loss: 3.204663038253784
Validation loss: 2.4051300543610767

Epoch: 5| Step: 6
Training loss: 2.035350799560547
Validation loss: 2.4091156733933317

Epoch: 5| Step: 7
Training loss: 2.0941131114959717
Validation loss: 2.4103239582430933

Epoch: 5| Step: 8
Training loss: 3.087456703186035
Validation loss: 2.4117994693017777

Epoch: 5| Step: 9
Training loss: 2.6460366249084473
Validation loss: 2.411776783645794

Epoch: 5| Step: 10
Training loss: 2.2938663959503174
Validation loss: 2.4104226558439192

Epoch: 197| Step: 0
Training loss: 3.3590087890625
Validation loss: 2.4163632046791816

Epoch: 5| Step: 1
Training loss: 2.785993814468384
Validation loss: 2.419966927138708

Epoch: 5| Step: 2
Training loss: 1.9965298175811768
Validation loss: 2.417077928461054

Epoch: 5| Step: 3
Training loss: 2.553293466567993
Validation loss: 2.416857938612661

Epoch: 5| Step: 4
Training loss: 3.2339234352111816
Validation loss: 2.4013993406808503

Epoch: 5| Step: 5
Training loss: 2.130486011505127
Validation loss: 2.40169600261155

Epoch: 5| Step: 6
Training loss: 2.1907660961151123
Validation loss: 2.400761401781472

Epoch: 5| Step: 7
Training loss: 2.667842149734497
Validation loss: 2.400615353738108

Epoch: 5| Step: 8
Training loss: 2.5510449409484863
Validation loss: 2.396968021187731

Epoch: 5| Step: 9
Training loss: 2.411591053009033
Validation loss: 2.405937735752393

Epoch: 5| Step: 10
Training loss: 3.10588002204895
Validation loss: 2.399837334950765

Epoch: 198| Step: 0
Training loss: 2.4852232933044434
Validation loss: 2.3979778264158513

Epoch: 5| Step: 1
Training loss: 2.5867013931274414
Validation loss: 2.3987115557475756

Epoch: 5| Step: 2
Training loss: 1.7979955673217773
Validation loss: 2.398589967399515

Epoch: 5| Step: 3
Training loss: 2.4967894554138184
Validation loss: 2.400906676887184

Epoch: 5| Step: 4
Training loss: 2.7001640796661377
Validation loss: 2.395437238036945

Epoch: 5| Step: 5
Training loss: 2.7798168659210205
Validation loss: 2.4023255430242068

Epoch: 5| Step: 6
Training loss: 2.68502140045166
Validation loss: 2.4033364057540894

Epoch: 5| Step: 7
Training loss: 2.668576717376709
Validation loss: 2.4060486798645346

Epoch: 5| Step: 8
Training loss: 3.1200504302978516
Validation loss: 2.406915477527085

Epoch: 5| Step: 9
Training loss: 3.0816454887390137
Validation loss: 2.3990696707079486

Epoch: 5| Step: 10
Training loss: 2.484313726425171
Validation loss: 2.394210923102594

Epoch: 199| Step: 0
Training loss: 2.7250890731811523
Validation loss: 2.4005956624143865

Epoch: 5| Step: 1
Training loss: 2.131178855895996
Validation loss: 2.4022267274959113

Epoch: 5| Step: 2
Training loss: 1.7122539281845093
Validation loss: 2.4086663338445846

Epoch: 5| Step: 3
Training loss: 3.0512871742248535
Validation loss: 2.403782234396986

Epoch: 5| Step: 4
Training loss: 3.461817502975464
Validation loss: 2.4118741635353333

Epoch: 5| Step: 5
Training loss: 2.796853542327881
Validation loss: 2.413616503438642

Epoch: 5| Step: 6
Training loss: 3.203946590423584
Validation loss: 2.4169603932288384

Epoch: 5| Step: 7
Training loss: 2.3842790126800537
Validation loss: 2.412744470821914

Epoch: 5| Step: 8
Training loss: 2.8837695121765137
Validation loss: 2.4089570788926977

Epoch: 5| Step: 9
Training loss: 1.8661830425262451
Validation loss: 2.413791823130782

Epoch: 5| Step: 10
Training loss: 2.6917953491210938
Validation loss: 2.400376573685677

Epoch: 200| Step: 0
Training loss: 2.6313483715057373
Validation loss: 2.396710749595396

Epoch: 5| Step: 1
Training loss: 3.1046581268310547
Validation loss: 2.395837829959008

Epoch: 5| Step: 2
Training loss: 2.785301685333252
Validation loss: 2.3919231789086455

Epoch: 5| Step: 3
Training loss: 2.5480189323425293
Validation loss: 2.389142015928863

Epoch: 5| Step: 4
Training loss: 2.659900188446045
Validation loss: 2.397506918958438

Epoch: 5| Step: 5
Training loss: 2.33644437789917
Validation loss: 2.4021961765904583

Epoch: 5| Step: 6
Training loss: 2.5533382892608643
Validation loss: 2.4003606714228147

Epoch: 5| Step: 7
Training loss: 2.7169389724731445
Validation loss: 2.4030331744942615

Epoch: 5| Step: 8
Training loss: 2.510798215866089
Validation loss: 2.417048991367381

Epoch: 5| Step: 9
Training loss: 2.4796206951141357
Validation loss: 2.431572102731274

Epoch: 5| Step: 10
Training loss: 2.580490827560425
Validation loss: 2.4280131427190637

Epoch: 201| Step: 0
Training loss: 2.936293601989746
Validation loss: 2.4231544463865218

Epoch: 5| Step: 1
Training loss: 3.125840663909912
Validation loss: 2.4326832191918486

Epoch: 5| Step: 2
Training loss: 2.1892309188842773
Validation loss: 2.43868193575131

Epoch: 5| Step: 3
Training loss: 2.1106438636779785
Validation loss: 2.4427999347768803

Epoch: 5| Step: 4
Training loss: 2.507598400115967
Validation loss: 2.431899719340827

Epoch: 5| Step: 5
Training loss: 2.3017492294311523
Validation loss: 2.42871061448128

Epoch: 5| Step: 6
Training loss: 2.9716217517852783
Validation loss: 2.4337524906281502

Epoch: 5| Step: 7
Training loss: 2.524109363555908
Validation loss: 2.42599828268892

Epoch: 5| Step: 8
Training loss: 3.088824510574341
Validation loss: 2.417216247127902

Epoch: 5| Step: 9
Training loss: 2.7397589683532715
Validation loss: 2.421663563738587

Epoch: 5| Step: 10
Training loss: 2.287323474884033
Validation loss: 2.4312562788686445

Epoch: 202| Step: 0
Training loss: 2.406153440475464
Validation loss: 2.42373848730518

Epoch: 5| Step: 1
Training loss: 2.5895984172821045
Validation loss: 2.4235699484425206

Epoch: 5| Step: 2
Training loss: 2.2616846561431885
Validation loss: 2.4198326936332126

Epoch: 5| Step: 3
Training loss: 3.0044312477111816
Validation loss: 2.426482633877826

Epoch: 5| Step: 4
Training loss: 2.940105438232422
Validation loss: 2.436525844758557

Epoch: 5| Step: 5
Training loss: 3.2869715690612793
Validation loss: 2.416255335653982

Epoch: 5| Step: 6
Training loss: 2.3268134593963623
Validation loss: 2.401170639581578

Epoch: 5| Step: 7
Training loss: 2.3157331943511963
Validation loss: 2.393908700635356

Epoch: 5| Step: 8
Training loss: 2.7927322387695312
Validation loss: 2.380315514021022

Epoch: 5| Step: 9
Training loss: 2.6798768043518066
Validation loss: 2.3864307659928516

Epoch: 5| Step: 10
Training loss: 2.258643865585327
Validation loss: 2.387867953187676

Epoch: 203| Step: 0
Training loss: 2.808292865753174
Validation loss: 2.395491243690573

Epoch: 5| Step: 1
Training loss: 2.6742050647735596
Validation loss: 2.400373148661788

Epoch: 5| Step: 2
Training loss: 2.1154401302337646
Validation loss: 2.405154856302405

Epoch: 5| Step: 3
Training loss: 2.818560838699341
Validation loss: 2.4113962111934537

Epoch: 5| Step: 4
Training loss: 2.1017847061157227
Validation loss: 2.4127862068914596

Epoch: 5| Step: 5
Training loss: 2.225104808807373
Validation loss: 2.4090757293085896

Epoch: 5| Step: 6
Training loss: 3.048701047897339
Validation loss: 2.4143225941606747

Epoch: 5| Step: 7
Training loss: 2.8829445838928223
Validation loss: 2.410447571867256

Epoch: 5| Step: 8
Training loss: 3.020799160003662
Validation loss: 2.4070463718906527

Epoch: 5| Step: 9
Training loss: 2.810455560684204
Validation loss: 2.4051754602821926

Epoch: 5| Step: 10
Training loss: 2.406741142272949
Validation loss: 2.390839276775237

Epoch: 204| Step: 0
Training loss: 2.5279903411865234
Validation loss: 2.3846140600019887

Epoch: 5| Step: 1
Training loss: 2.833686113357544
Validation loss: 2.3949846657373572

Epoch: 5| Step: 2
Training loss: 2.851985454559326
Validation loss: 2.402958280296736

Epoch: 5| Step: 3
Training loss: 2.561176300048828
Validation loss: 2.419734283160138

Epoch: 5| Step: 4
Training loss: 2.538011074066162
Validation loss: 2.4136652279925603

Epoch: 5| Step: 5
Training loss: 2.425403118133545
Validation loss: 2.435015675842121

Epoch: 5| Step: 6
Training loss: 2.6591849327087402
Validation loss: 2.442209651393275

Epoch: 5| Step: 7
Training loss: 2.627509832382202
Validation loss: 2.4404040613482074

Epoch: 5| Step: 8
Training loss: 2.5982863903045654
Validation loss: 2.435245229351905

Epoch: 5| Step: 9
Training loss: 1.9649055004119873
Validation loss: 2.4487476912877892

Epoch: 5| Step: 10
Training loss: 3.4582338333129883
Validation loss: 2.4379731403884066

Epoch: 205| Step: 0
Training loss: 2.2575812339782715
Validation loss: 2.423694374740765

Epoch: 5| Step: 1
Training loss: 2.6095094680786133
Validation loss: 2.425657441539149

Epoch: 5| Step: 2
Training loss: 3.0016863346099854
Validation loss: 2.4180913612406743

Epoch: 5| Step: 3
Training loss: 2.997284173965454
Validation loss: 2.4094844018259356

Epoch: 5| Step: 4
Training loss: 2.5170445442199707
Validation loss: 2.395333279845535

Epoch: 5| Step: 5
Training loss: 2.5421946048736572
Validation loss: 2.39343088160279

Epoch: 5| Step: 6
Training loss: 2.8834712505340576
Validation loss: 2.3907761573791504

Epoch: 5| Step: 7
Training loss: 2.365445613861084
Validation loss: 2.397129399802095

Epoch: 5| Step: 8
Training loss: 2.1474671363830566
Validation loss: 2.392117269577519

Epoch: 5| Step: 9
Training loss: 2.7264962196350098
Validation loss: 2.393734980654973

Epoch: 5| Step: 10
Training loss: 2.7470462322235107
Validation loss: 2.397256980660141

Epoch: 206| Step: 0
Training loss: 2.8618972301483154
Validation loss: 2.3949956996466524

Epoch: 5| Step: 1
Training loss: 2.02172589302063
Validation loss: 2.4009259259828957

Epoch: 5| Step: 2
Training loss: 2.5261802673339844
Validation loss: 2.4021704530203216

Epoch: 5| Step: 3
Training loss: 2.849203586578369
Validation loss: 2.407351686108497

Epoch: 5| Step: 4
Training loss: 2.4385974407196045
Validation loss: 2.402227060769194

Epoch: 5| Step: 5
Training loss: 3.173886299133301
Validation loss: 2.4035219210450367

Epoch: 5| Step: 6
Training loss: 2.4989845752716064
Validation loss: 2.4023349490216983

Epoch: 5| Step: 7
Training loss: 2.4502906799316406
Validation loss: 2.3985463649995866

Epoch: 5| Step: 8
Training loss: 2.401859760284424
Validation loss: 2.3938464682589293

Epoch: 5| Step: 9
Training loss: 2.560849189758301
Validation loss: 2.403234415156867

Epoch: 5| Step: 10
Training loss: 3.2049367427825928
Validation loss: 2.396574392113634

Epoch: 207| Step: 0
Training loss: 2.7787561416625977
Validation loss: 2.4059073232835337

Epoch: 5| Step: 1
Training loss: 2.4482314586639404
Validation loss: 2.4073260291930167

Epoch: 5| Step: 2
Training loss: 2.2399439811706543
Validation loss: 2.40332724971156

Epoch: 5| Step: 3
Training loss: 2.9113454818725586
Validation loss: 2.4037608433795232

Epoch: 5| Step: 4
Training loss: 2.83215594291687
Validation loss: 2.399014722916388

Epoch: 5| Step: 5
Training loss: 2.4820117950439453
Validation loss: 2.4018890678241687

Epoch: 5| Step: 6
Training loss: 2.3377254009246826
Validation loss: 2.406394589331842

Epoch: 5| Step: 7
Training loss: 2.7425851821899414
Validation loss: 2.3977770318267164

Epoch: 5| Step: 8
Training loss: 2.5967016220092773
Validation loss: 2.401215286665065

Epoch: 5| Step: 9
Training loss: 2.613081455230713
Validation loss: 2.3877324006890737

Epoch: 5| Step: 10
Training loss: 2.7672274112701416
Validation loss: 2.3823192914326987

Epoch: 208| Step: 0
Training loss: 2.724534511566162
Validation loss: 2.3810640483774166

Epoch: 5| Step: 1
Training loss: 2.4287757873535156
Validation loss: 2.380850371494088

Epoch: 5| Step: 2
Training loss: 2.393655776977539
Validation loss: 2.3796861838268977

Epoch: 5| Step: 3
Training loss: 2.9891746044158936
Validation loss: 2.374783477475566

Epoch: 5| Step: 4
Training loss: 2.6410953998565674
Validation loss: 2.377785005877095

Epoch: 5| Step: 5
Training loss: 2.261009931564331
Validation loss: 2.3756400154482935

Epoch: 5| Step: 6
Training loss: 3.367684841156006
Validation loss: 2.3755052064054754

Epoch: 5| Step: 7
Training loss: 2.048920154571533
Validation loss: 2.3760605819763674

Epoch: 5| Step: 8
Training loss: 2.9556407928466797
Validation loss: 2.3789957851491947

Epoch: 5| Step: 9
Training loss: 2.295313596725464
Validation loss: 2.3797611626245643

Epoch: 5| Step: 10
Training loss: 2.6494803428649902
Validation loss: 2.3902732531229653

Epoch: 209| Step: 0
Training loss: 2.920349597930908
Validation loss: 2.384751812104256

Epoch: 5| Step: 1
Training loss: 2.9737801551818848
Validation loss: 2.386363849844984

Epoch: 5| Step: 2
Training loss: 2.267050266265869
Validation loss: 2.3855992312072427

Epoch: 5| Step: 3
Training loss: 2.831819772720337
Validation loss: 2.3941518850223993

Epoch: 5| Step: 4
Training loss: 3.0430452823638916
Validation loss: 2.3887136290150304

Epoch: 5| Step: 5
Training loss: 2.7935850620269775
Validation loss: 2.3860162868294665

Epoch: 5| Step: 6
Training loss: 2.0998034477233887
Validation loss: 2.3818787169712845

Epoch: 5| Step: 7
Training loss: 2.6661980152130127
Validation loss: 2.3804838119014615

Epoch: 5| Step: 8
Training loss: 2.189984083175659
Validation loss: 2.379397156418011

Epoch: 5| Step: 9
Training loss: 2.351531505584717
Validation loss: 2.3782238960266113

Epoch: 5| Step: 10
Training loss: 2.608147621154785
Validation loss: 2.379412461352605

Epoch: 210| Step: 0
Training loss: 2.924290418624878
Validation loss: 2.3818632223272838

Epoch: 5| Step: 1
Training loss: 2.905651092529297
Validation loss: 2.381829797580678

Epoch: 5| Step: 2
Training loss: 2.649441957473755
Validation loss: 2.3987060054655998

Epoch: 5| Step: 3
Training loss: 2.955799102783203
Validation loss: 2.3952423859668035

Epoch: 5| Step: 4
Training loss: 2.650534152984619
Validation loss: 2.4014238798490135

Epoch: 5| Step: 5
Training loss: 2.2049553394317627
Validation loss: 2.3918472361821

Epoch: 5| Step: 6
Training loss: 2.9350290298461914
Validation loss: 2.3925182050274265

Epoch: 5| Step: 7
Training loss: 2.032271385192871
Validation loss: 2.383595594795801

Epoch: 5| Step: 8
Training loss: 2.199514865875244
Validation loss: 2.3916694810313563

Epoch: 5| Step: 9
Training loss: 2.6096043586730957
Validation loss: 2.3898202693590553

Epoch: 5| Step: 10
Training loss: 2.7275757789611816
Validation loss: 2.383874129223567

Epoch: 211| Step: 0
Training loss: 2.6015632152557373
Validation loss: 2.4004843927198842

Epoch: 5| Step: 1
Training loss: 2.2779579162597656
Validation loss: 2.400361693033608

Epoch: 5| Step: 2
Training loss: 1.9593524932861328
Validation loss: 2.400169949377737

Epoch: 5| Step: 3
Training loss: 2.5552568435668945
Validation loss: 2.3997980868944557

Epoch: 5| Step: 4
Training loss: 2.5010507106781006
Validation loss: 2.3965615559649724

Epoch: 5| Step: 5
Training loss: 3.2994415760040283
Validation loss: 2.3909183343251548

Epoch: 5| Step: 6
Training loss: 3.102062225341797
Validation loss: 2.380111235444264

Epoch: 5| Step: 7
Training loss: 2.4755706787109375
Validation loss: 2.3864632755197506

Epoch: 5| Step: 8
Training loss: 2.936814785003662
Validation loss: 2.3884822014839417

Epoch: 5| Step: 9
Training loss: 2.7970528602600098
Validation loss: 2.380583865668184

Epoch: 5| Step: 10
Training loss: 2.1109888553619385
Validation loss: 2.384703008077478

Epoch: 212| Step: 0
Training loss: 2.9011268615722656
Validation loss: 2.3837936796167845

Epoch: 5| Step: 1
Training loss: 2.3267903327941895
Validation loss: 2.3772715906943045

Epoch: 5| Step: 2
Training loss: 3.220843553543091
Validation loss: 2.3871243948577554

Epoch: 5| Step: 3
Training loss: 2.362400531768799
Validation loss: 2.386538246626495

Epoch: 5| Step: 4
Training loss: 2.88862681388855
Validation loss: 2.382767577325144

Epoch: 5| Step: 5
Training loss: 2.0472145080566406
Validation loss: 2.3839478287645566

Epoch: 5| Step: 6
Training loss: 2.864283561706543
Validation loss: 2.389235063265729

Epoch: 5| Step: 7
Training loss: 2.295092821121216
Validation loss: 2.3897214243488927

Epoch: 5| Step: 8
Training loss: 2.7316346168518066
Validation loss: 2.3858938127435665

Epoch: 5| Step: 9
Training loss: 2.9976906776428223
Validation loss: 2.377100747118714

Epoch: 5| Step: 10
Training loss: 1.9322000741958618
Validation loss: 2.3742302335718626

Epoch: 213| Step: 0
Training loss: 2.548032283782959
Validation loss: 2.3731385994982976

Epoch: 5| Step: 1
Training loss: 1.8621553182601929
Validation loss: 2.373011125031338

Epoch: 5| Step: 2
Training loss: 2.85007381439209
Validation loss: 2.377575277000345

Epoch: 5| Step: 3
Training loss: 2.555666923522949
Validation loss: 2.3805161804281254

Epoch: 5| Step: 4
Training loss: 2.8356199264526367
Validation loss: 2.3814960782245924

Epoch: 5| Step: 5
Training loss: 2.6226601600646973
Validation loss: 2.3755378774417344

Epoch: 5| Step: 6
Training loss: 2.1720504760742188
Validation loss: 2.384186678035285

Epoch: 5| Step: 7
Training loss: 2.8835277557373047
Validation loss: 2.382798589685912

Epoch: 5| Step: 8
Training loss: 2.639672040939331
Validation loss: 2.379102037798974

Epoch: 5| Step: 9
Training loss: 3.004826068878174
Validation loss: 2.3870295965543358

Epoch: 5| Step: 10
Training loss: 2.761686325073242
Validation loss: 2.3786380598621983

Epoch: 214| Step: 0
Training loss: 2.368926525115967
Validation loss: 2.382404283810687

Epoch: 5| Step: 1
Training loss: 1.9507381916046143
Validation loss: 2.3807674172104045

Epoch: 5| Step: 2
Training loss: 2.895411252975464
Validation loss: 2.3790395285493586

Epoch: 5| Step: 3
Training loss: 3.1287028789520264
Validation loss: 2.379075816882554

Epoch: 5| Step: 4
Training loss: 2.8215882778167725
Validation loss: 2.375776652366884

Epoch: 5| Step: 5
Training loss: 2.4720818996429443
Validation loss: 2.3777302952222925

Epoch: 5| Step: 6
Training loss: 2.501986265182495
Validation loss: 2.3754289791148198

Epoch: 5| Step: 7
Training loss: 3.0178067684173584
Validation loss: 2.383237915654336

Epoch: 5| Step: 8
Training loss: 2.7851037979125977
Validation loss: 2.383835433631815

Epoch: 5| Step: 9
Training loss: 2.3961124420166016
Validation loss: 2.389350729603921

Epoch: 5| Step: 10
Training loss: 2.263106346130371
Validation loss: 2.380462246556436

Epoch: 215| Step: 0
Training loss: 2.6440601348876953
Validation loss: 2.3750530853066394

Epoch: 5| Step: 1
Training loss: 2.914498805999756
Validation loss: 2.3858972287947133

Epoch: 5| Step: 2
Training loss: 2.8579235076904297
Validation loss: 2.3788019431534635

Epoch: 5| Step: 3
Training loss: 2.3894102573394775
Validation loss: 2.3783410492763726

Epoch: 5| Step: 4
Training loss: 2.9101667404174805
Validation loss: 2.3801972584057878

Epoch: 5| Step: 5
Training loss: 2.3736400604248047
Validation loss: 2.3876613750252673

Epoch: 5| Step: 6
Training loss: 2.5068840980529785
Validation loss: 2.383699119731944

Epoch: 5| Step: 7
Training loss: 2.4237418174743652
Validation loss: 2.3890249011337117

Epoch: 5| Step: 8
Training loss: 2.472050905227661
Validation loss: 2.39588350378057

Epoch: 5| Step: 9
Training loss: 2.549851179122925
Validation loss: 2.392640695776991

Epoch: 5| Step: 10
Training loss: 2.613919973373413
Validation loss: 2.3959513377117854

Epoch: 216| Step: 0
Training loss: 2.5726726055145264
Validation loss: 2.3954533146273707

Epoch: 5| Step: 1
Training loss: 2.683104991912842
Validation loss: 2.391797868154382

Epoch: 5| Step: 2
Training loss: 2.356947660446167
Validation loss: 2.3946476751758206

Epoch: 5| Step: 3
Training loss: 2.6137237548828125
Validation loss: 2.394426167652171

Epoch: 5| Step: 4
Training loss: 2.2793943881988525
Validation loss: 2.401066859563192

Epoch: 5| Step: 5
Training loss: 2.6128547191619873
Validation loss: 2.3944984174543813

Epoch: 5| Step: 6
Training loss: 2.730741262435913
Validation loss: 2.3981126405859507

Epoch: 5| Step: 7
Training loss: 2.6371986865997314
Validation loss: 2.398120726308515

Epoch: 5| Step: 8
Training loss: 2.3432106971740723
Validation loss: 2.3907935414262997

Epoch: 5| Step: 9
Training loss: 3.5646605491638184
Validation loss: 2.3919406052558654

Epoch: 5| Step: 10
Training loss: 2.219338893890381
Validation loss: 2.3919556166536067

Epoch: 217| Step: 0
Training loss: 3.373887538909912
Validation loss: 2.3854564082237983

Epoch: 5| Step: 1
Training loss: 2.5076990127563477
Validation loss: 2.389663278415639

Epoch: 5| Step: 2
Training loss: 2.876845598220825
Validation loss: 2.388047541341474

Epoch: 5| Step: 3
Training loss: 2.6464178562164307
Validation loss: 2.384112517038981

Epoch: 5| Step: 4
Training loss: 2.6882619857788086
Validation loss: 2.379327017773864

Epoch: 5| Step: 5
Training loss: 1.9909569025039673
Validation loss: 2.3785262953850532

Epoch: 5| Step: 6
Training loss: 2.385542392730713
Validation loss: 2.365675334007509

Epoch: 5| Step: 7
Training loss: 1.8934577703475952
Validation loss: 2.37422780324054

Epoch: 5| Step: 8
Training loss: 2.597825288772583
Validation loss: 2.372285218649013

Epoch: 5| Step: 9
Training loss: 3.053220748901367
Validation loss: 2.372274339839976

Epoch: 5| Step: 10
Training loss: 2.517775535583496
Validation loss: 2.370905041694641

Epoch: 218| Step: 0
Training loss: 2.403995990753174
Validation loss: 2.374124810259829

Epoch: 5| Step: 1
Training loss: 2.0220112800598145
Validation loss: 2.369950516249544

Epoch: 5| Step: 2
Training loss: 2.457237720489502
Validation loss: 2.3758961744205926

Epoch: 5| Step: 3
Training loss: 2.317164897918701
Validation loss: 2.371186387154364

Epoch: 5| Step: 4
Training loss: 3.2430286407470703
Validation loss: 2.3742208327016523

Epoch: 5| Step: 5
Training loss: 2.291714906692505
Validation loss: 2.3707859234143327

Epoch: 5| Step: 6
Training loss: 3.1890015602111816
Validation loss: 2.3683710790449575

Epoch: 5| Step: 7
Training loss: 2.9558606147766113
Validation loss: 2.369250802583592

Epoch: 5| Step: 8
Training loss: 2.394589900970459
Validation loss: 2.3736102004205026

Epoch: 5| Step: 9
Training loss: 2.7380547523498535
Validation loss: 2.37414950196461

Epoch: 5| Step: 10
Training loss: 2.5609304904937744
Validation loss: 2.372349746765629

Epoch: 219| Step: 0
Training loss: 3.092986583709717
Validation loss: 2.3714269643188803

Epoch: 5| Step: 1
Training loss: 2.666754722595215
Validation loss: 2.370461480591887

Epoch: 5| Step: 2
Training loss: 2.561676502227783
Validation loss: 2.373984238152863

Epoch: 5| Step: 3
Training loss: 2.4897685050964355
Validation loss: 2.3771856215692337

Epoch: 5| Step: 4
Training loss: 2.4269020557403564
Validation loss: 2.3818299616536787

Epoch: 5| Step: 5
Training loss: 2.38084077835083
Validation loss: 2.3867039859935804

Epoch: 5| Step: 6
Training loss: 2.137136459350586
Validation loss: 2.3967012154158724

Epoch: 5| Step: 7
Training loss: 2.8895018100738525
Validation loss: 2.392953739371351

Epoch: 5| Step: 8
Training loss: 2.5799248218536377
Validation loss: 2.4031187744550806

Epoch: 5| Step: 9
Training loss: 3.088021755218506
Validation loss: 2.4177720367267566

Epoch: 5| Step: 10
Training loss: 2.3212392330169678
Validation loss: 2.4211036005327777

Epoch: 220| Step: 0
Training loss: 2.789273738861084
Validation loss: 2.419067831449611

Epoch: 5| Step: 1
Training loss: 2.3261446952819824
Validation loss: 2.4319260197301067

Epoch: 5| Step: 2
Training loss: 2.859029769897461
Validation loss: 2.4311677409756567

Epoch: 5| Step: 3
Training loss: 2.0245888233184814
Validation loss: 2.425080640341646

Epoch: 5| Step: 4
Training loss: 2.561183452606201
Validation loss: 2.4373668291235484

Epoch: 5| Step: 5
Training loss: 2.409895658493042
Validation loss: 2.4379754553559008

Epoch: 5| Step: 6
Training loss: 3.05410099029541
Validation loss: 2.4417422868872203

Epoch: 5| Step: 7
Training loss: 2.553152322769165
Validation loss: 2.44907578345268

Epoch: 5| Step: 8
Training loss: 2.478241205215454
Validation loss: 2.4373984362489436

Epoch: 5| Step: 9
Training loss: 3.001974582672119
Validation loss: 2.447457987775085

Epoch: 5| Step: 10
Training loss: 2.9257545471191406
Validation loss: 2.452559978731217

Epoch: 221| Step: 0
Training loss: 2.491542100906372
Validation loss: 2.4473826757041355

Epoch: 5| Step: 1
Training loss: 2.7784924507141113
Validation loss: 2.452920300986177

Epoch: 5| Step: 2
Training loss: 2.961002826690674
Validation loss: 2.451915958876251

Epoch: 5| Step: 3
Training loss: 2.845444440841675
Validation loss: 2.4425488800130863

Epoch: 5| Step: 4
Training loss: 2.6461453437805176
Validation loss: 2.449462708606515

Epoch: 5| Step: 5
Training loss: 2.7929577827453613
Validation loss: 2.4487746095144622

Epoch: 5| Step: 6
Training loss: 2.2290070056915283
Validation loss: 2.4452746452823764

Epoch: 5| Step: 7
Training loss: 2.9967925548553467
Validation loss: 2.4393907208596506

Epoch: 5| Step: 8
Training loss: 2.411494255065918
Validation loss: 2.437953784901609

Epoch: 5| Step: 9
Training loss: 2.4774978160858154
Validation loss: 2.437204281489054

Epoch: 5| Step: 10
Training loss: 2.4125254154205322
Validation loss: 2.4442772967841035

Epoch: 222| Step: 0
Training loss: 2.2625491619110107
Validation loss: 2.442751743460214

Epoch: 5| Step: 1
Training loss: 2.8130431175231934
Validation loss: 2.4548595054175264

Epoch: 5| Step: 2
Training loss: 2.635054588317871
Validation loss: 2.4505134885029127

Epoch: 5| Step: 3
Training loss: 2.560814380645752
Validation loss: 2.44464014422509

Epoch: 5| Step: 4
Training loss: 2.709282398223877
Validation loss: 2.4350029653118503

Epoch: 5| Step: 5
Training loss: 3.2443413734436035
Validation loss: 2.443740975472235

Epoch: 5| Step: 6
Training loss: 2.6975088119506836
Validation loss: 2.4424355645333566

Epoch: 5| Step: 7
Training loss: 2.4492387771606445
Validation loss: 2.436058326434064

Epoch: 5| Step: 8
Training loss: 2.2092056274414062
Validation loss: 2.4338293152470745

Epoch: 5| Step: 9
Training loss: 2.2793283462524414
Validation loss: 2.4440188305352324

Epoch: 5| Step: 10
Training loss: 3.24924635887146
Validation loss: 2.4379340294868714

Epoch: 223| Step: 0
Training loss: 3.2046775817871094
Validation loss: 2.4404748024479037

Epoch: 5| Step: 1
Training loss: 2.7933413982391357
Validation loss: 2.443937232417445

Epoch: 5| Step: 2
Training loss: 2.4710748195648193
Validation loss: 2.440444051578481

Epoch: 5| Step: 3
Training loss: 2.83124041557312
Validation loss: 2.4444745458582395

Epoch: 5| Step: 4
Training loss: 2.339756488800049
Validation loss: 2.456546711665328

Epoch: 5| Step: 5
Training loss: 1.597172737121582
Validation loss: 2.4528086518728607

Epoch: 5| Step: 6
Training loss: 2.458789587020874
Validation loss: 2.4410284232067805

Epoch: 5| Step: 7
Training loss: 2.5892035961151123
Validation loss: 2.451854549428468

Epoch: 5| Step: 8
Training loss: 2.8041775226593018
Validation loss: 2.445620431694933

Epoch: 5| Step: 9
Training loss: 2.2992589473724365
Validation loss: 2.447413565010153

Epoch: 5| Step: 10
Training loss: 3.6578369140625
Validation loss: 2.4392942408079743

Epoch: 224| Step: 0
Training loss: 2.9888980388641357
Validation loss: 2.4508554627818446

Epoch: 5| Step: 1
Training loss: 2.257965326309204
Validation loss: 2.436540793347102

Epoch: 5| Step: 2
Training loss: 3.0697109699249268
Validation loss: 2.4356654049247823

Epoch: 5| Step: 3
Training loss: 2.533568859100342
Validation loss: 2.435412196702855

Epoch: 5| Step: 4
Training loss: 3.3024520874023438
Validation loss: 2.4343032067821873

Epoch: 5| Step: 5
Training loss: 2.1902706623077393
Validation loss: 2.431803341834776

Epoch: 5| Step: 6
Training loss: 2.6414153575897217
Validation loss: 2.432332479825584

Epoch: 5| Step: 7
Training loss: 2.465791940689087
Validation loss: 2.4375857101973666

Epoch: 5| Step: 8
Training loss: 2.563384771347046
Validation loss: 2.425558792647495

Epoch: 5| Step: 9
Training loss: 2.43784499168396
Validation loss: 2.4225285694163334

Epoch: 5| Step: 10
Training loss: 2.4497389793395996
Validation loss: 2.425620132876981

Epoch: 225| Step: 0
Training loss: 1.987820029258728
Validation loss: 2.4215151725276822

Epoch: 5| Step: 1
Training loss: 2.46502423286438
Validation loss: 2.425572179978894

Epoch: 5| Step: 2
Training loss: 2.049621105194092
Validation loss: 2.4246560976069462

Epoch: 5| Step: 3
Training loss: 3.1417288780212402
Validation loss: 2.422873863609888

Epoch: 5| Step: 4
Training loss: 2.848693370819092
Validation loss: 2.4264347040525047

Epoch: 5| Step: 5
Training loss: 2.4932968616485596
Validation loss: 2.4265826773899857

Epoch: 5| Step: 6
Training loss: 2.924583911895752
Validation loss: 2.4259454255462973

Epoch: 5| Step: 7
Training loss: 2.104085922241211
Validation loss: 2.4210743186294392

Epoch: 5| Step: 8
Training loss: 3.0400500297546387
Validation loss: 2.4223421696693666

Epoch: 5| Step: 9
Training loss: 2.8191890716552734
Validation loss: 2.417680989029587

Epoch: 5| Step: 10
Training loss: 3.057577133178711
Validation loss: 2.4179919073658604

Epoch: 226| Step: 0
Training loss: 2.000062942504883
Validation loss: 2.410461594981532

Epoch: 5| Step: 1
Training loss: 3.089240312576294
Validation loss: 2.4169332199199225

Epoch: 5| Step: 2
Training loss: 2.1228766441345215
Validation loss: 2.4058163217318955

Epoch: 5| Step: 3
Training loss: 2.3108971118927
Validation loss: 2.4046865227401897

Epoch: 5| Step: 4
Training loss: 1.8098329305648804
Validation loss: 2.4067958760005173

Epoch: 5| Step: 5
Training loss: 3.5669167041778564
Validation loss: 2.408756284303563

Epoch: 5| Step: 6
Training loss: 3.1900124549865723
Validation loss: 2.4057467188886417

Epoch: 5| Step: 7
Training loss: 2.4169445037841797
Validation loss: 2.402227073587397

Epoch: 5| Step: 8
Training loss: 3.052974224090576
Validation loss: 2.3927649836386404

Epoch: 5| Step: 9
Training loss: 2.6085867881774902
Validation loss: 2.3970770887149278

Epoch: 5| Step: 10
Training loss: 2.686326265335083
Validation loss: 2.3996340459392917

Epoch: 227| Step: 0
Training loss: 2.2355709075927734
Validation loss: 2.3972316249724357

Epoch: 5| Step: 1
Training loss: 3.5595035552978516
Validation loss: 2.393012132695926

Epoch: 5| Step: 2
Training loss: 2.6159138679504395
Validation loss: 2.3998086785757415

Epoch: 5| Step: 3
Training loss: 2.4821860790252686
Validation loss: 2.385306445501184

Epoch: 5| Step: 4
Training loss: 2.679863929748535
Validation loss: 2.3842096021098476

Epoch: 5| Step: 5
Training loss: 2.587996006011963
Validation loss: 2.3711757172820387

Epoch: 5| Step: 6
Training loss: 2.8481805324554443
Validation loss: 2.364929432510048

Epoch: 5| Step: 7
Training loss: 2.60206937789917
Validation loss: 2.3588865546769995

Epoch: 5| Step: 8
Training loss: 2.9121599197387695
Validation loss: 2.36254644650285

Epoch: 5| Step: 9
Training loss: 1.8907407522201538
Validation loss: 2.3667466845563663

Epoch: 5| Step: 10
Training loss: 2.0763192176818848
Validation loss: 2.36523675662215

Epoch: 228| Step: 0
Training loss: 3.467693328857422
Validation loss: 2.3741960769058554

Epoch: 5| Step: 1
Training loss: 2.0675365924835205
Validation loss: 2.3685403767452446

Epoch: 5| Step: 2
Training loss: 2.1524786949157715
Validation loss: 2.3706671627618934

Epoch: 5| Step: 3
Training loss: 2.599410057067871
Validation loss: 2.3615501516608783

Epoch: 5| Step: 4
Training loss: 2.9297547340393066
Validation loss: 2.3515677272632556

Epoch: 5| Step: 5
Training loss: 2.752669334411621
Validation loss: 2.343808344615403

Epoch: 5| Step: 6
Training loss: 2.180450439453125
Validation loss: 2.3422961388864825

Epoch: 5| Step: 7
Training loss: 2.676544666290283
Validation loss: 2.3494650471595024

Epoch: 5| Step: 8
Training loss: 3.1360182762145996
Validation loss: 2.345140290516679

Epoch: 5| Step: 9
Training loss: 2.7011146545410156
Validation loss: 2.3485301181834233

Epoch: 5| Step: 10
Training loss: 1.8410181999206543
Validation loss: 2.3451141003639466

Epoch: 229| Step: 0
Training loss: 2.503302812576294
Validation loss: 2.34868630286186

Epoch: 5| Step: 1
Training loss: 2.2229857444763184
Validation loss: 2.355370649727442

Epoch: 5| Step: 2
Training loss: 2.9908440113067627
Validation loss: 2.3588232301896617

Epoch: 5| Step: 3
Training loss: 2.3790647983551025
Validation loss: 2.369136292447326

Epoch: 5| Step: 4
Training loss: 2.9655115604400635
Validation loss: 2.371154546737671

Epoch: 5| Step: 5
Training loss: 1.9710792303085327
Validation loss: 2.381013775384554

Epoch: 5| Step: 6
Training loss: 2.727820873260498
Validation loss: 2.3837947614731325

Epoch: 5| Step: 7
Training loss: 2.5662035942077637
Validation loss: 2.3854459690791305

Epoch: 5| Step: 8
Training loss: 2.937845468521118
Validation loss: 2.3759121074471423

Epoch: 5| Step: 9
Training loss: 2.3454480171203613
Validation loss: 2.371867791298897

Epoch: 5| Step: 10
Training loss: 3.0283596515655518
Validation loss: 2.362148830967565

Epoch: 230| Step: 0
Training loss: 2.2361464500427246
Validation loss: 2.3611737669155164

Epoch: 5| Step: 1
Training loss: 2.4866509437561035
Validation loss: 2.363903240490985

Epoch: 5| Step: 2
Training loss: 2.7009472846984863
Validation loss: 2.358656674303034

Epoch: 5| Step: 3
Training loss: 2.7410576343536377
Validation loss: 2.3614596859101327

Epoch: 5| Step: 4
Training loss: 2.7563724517822266
Validation loss: 2.3542223489412697

Epoch: 5| Step: 5
Training loss: 3.533269166946411
Validation loss: 2.355285882949829

Epoch: 5| Step: 6
Training loss: 2.700127363204956
Validation loss: 2.3475029942809895

Epoch: 5| Step: 7
Training loss: 2.03336763381958
Validation loss: 2.3439683811638945

Epoch: 5| Step: 8
Training loss: 2.4600565433502197
Validation loss: 2.3514390478851976

Epoch: 5| Step: 9
Training loss: 2.597395420074463
Validation loss: 2.349920739409744

Epoch: 5| Step: 10
Training loss: 2.1904749870300293
Validation loss: 2.349279656205126

Epoch: 231| Step: 0
Training loss: 2.59051775932312
Validation loss: 2.342402537663778

Epoch: 5| Step: 1
Training loss: 2.6432504653930664
Validation loss: 2.344962904530187

Epoch: 5| Step: 2
Training loss: 2.6280391216278076
Validation loss: 2.341571133623841

Epoch: 5| Step: 3
Training loss: 2.4388670921325684
Validation loss: 2.349679298298333

Epoch: 5| Step: 4
Training loss: 3.2377305030822754
Validation loss: 2.351028307791679

Epoch: 5| Step: 5
Training loss: 2.5242676734924316
Validation loss: 2.344193240647675

Epoch: 5| Step: 6
Training loss: 2.791919231414795
Validation loss: 2.347286501238423

Epoch: 5| Step: 7
Training loss: 2.2593612670898438
Validation loss: 2.3372900306537585

Epoch: 5| Step: 8
Training loss: 2.348155975341797
Validation loss: 2.3398211617623605

Epoch: 5| Step: 9
Training loss: 2.1722915172576904
Validation loss: 2.3474909695245887

Epoch: 5| Step: 10
Training loss: 2.8015670776367188
Validation loss: 2.341017425701182

Epoch: 232| Step: 0
Training loss: 2.5326426029205322
Validation loss: 2.3469044290563112

Epoch: 5| Step: 1
Training loss: 1.9530712366104126
Validation loss: 2.342377462694722

Epoch: 5| Step: 2
Training loss: 2.956284284591675
Validation loss: 2.3444936172936552

Epoch: 5| Step: 3
Training loss: 2.807145595550537
Validation loss: 2.3505124763775895

Epoch: 5| Step: 4
Training loss: 2.664961099624634
Validation loss: 2.3465016580397084

Epoch: 5| Step: 5
Training loss: 3.1916089057922363
Validation loss: 2.3419086497317076

Epoch: 5| Step: 6
Training loss: 1.7574430704116821
Validation loss: 2.3456073704586236

Epoch: 5| Step: 7
Training loss: 2.188931703567505
Validation loss: 2.3466663924596642

Epoch: 5| Step: 8
Training loss: 2.968467950820923
Validation loss: 2.355877855772613

Epoch: 5| Step: 9
Training loss: 2.6375527381896973
Validation loss: 2.34881325690977

Epoch: 5| Step: 10
Training loss: 2.8216357231140137
Validation loss: 2.3561404699920327

Epoch: 233| Step: 0
Training loss: 3.0682733058929443
Validation loss: 2.3502874066752772

Epoch: 5| Step: 1
Training loss: 2.2735347747802734
Validation loss: 2.3505990684673352

Epoch: 5| Step: 2
Training loss: 2.7112183570861816
Validation loss: 2.3513738545038367

Epoch: 5| Step: 3
Training loss: 2.6465907096862793
Validation loss: 2.3567800598759807

Epoch: 5| Step: 4
Training loss: 2.2021496295928955
Validation loss: 2.356344794714323

Epoch: 5| Step: 5
Training loss: 2.5238072872161865
Validation loss: 2.361189026986399

Epoch: 5| Step: 6
Training loss: 3.1304121017456055
Validation loss: 2.3710553799906084

Epoch: 5| Step: 7
Training loss: 2.231600522994995
Validation loss: 2.3648216493668093

Epoch: 5| Step: 8
Training loss: 1.9535245895385742
Validation loss: 2.364048957824707

Epoch: 5| Step: 9
Training loss: 2.550753593444824
Validation loss: 2.3734061564168623

Epoch: 5| Step: 10
Training loss: 3.248135805130005
Validation loss: 2.377921918387054

Epoch: 234| Step: 0
Training loss: 2.4020802974700928
Validation loss: 2.383670876103063

Epoch: 5| Step: 1
Training loss: 2.2553250789642334
Validation loss: 2.3836337212593324

Epoch: 5| Step: 2
Training loss: 2.700904369354248
Validation loss: 2.3823264516809934

Epoch: 5| Step: 3
Training loss: 2.677027940750122
Validation loss: 2.37845120891448

Epoch: 5| Step: 4
Training loss: 2.823786735534668
Validation loss: 2.377109173805483

Epoch: 5| Step: 5
Training loss: 3.128237247467041
Validation loss: 2.3761037447119273

Epoch: 5| Step: 6
Training loss: 2.7986621856689453
Validation loss: 2.3716466811395462

Epoch: 5| Step: 7
Training loss: 2.2481510639190674
Validation loss: 2.38082790887484

Epoch: 5| Step: 8
Training loss: 2.530778408050537
Validation loss: 2.366784159855176

Epoch: 5| Step: 9
Training loss: 2.3136909008026123
Validation loss: 2.364824010479835

Epoch: 5| Step: 10
Training loss: 2.497077703475952
Validation loss: 2.3737130600919008

Epoch: 235| Step: 0
Training loss: 2.4996230602264404
Validation loss: 2.3614238077594387

Epoch: 5| Step: 1
Training loss: 2.7938857078552246
Validation loss: 2.359649519766531

Epoch: 5| Step: 2
Training loss: 2.828467845916748
Validation loss: 2.3536866429031535

Epoch: 5| Step: 3
Training loss: 1.6018365621566772
Validation loss: 2.348206079134377

Epoch: 5| Step: 4
Training loss: 2.4778778553009033
Validation loss: 2.35164132425862

Epoch: 5| Step: 5
Training loss: 2.6549484729766846
Validation loss: 2.3568167019915838

Epoch: 5| Step: 6
Training loss: 3.016998767852783
Validation loss: 2.369410066194432

Epoch: 5| Step: 7
Training loss: 2.437268018722534
Validation loss: 2.367178970767606

Epoch: 5| Step: 8
Training loss: 2.34798526763916
Validation loss: 2.3720370108081448

Epoch: 5| Step: 9
Training loss: 3.016000747680664
Validation loss: 2.381031228650001

Epoch: 5| Step: 10
Training loss: 2.897827386856079
Validation loss: 2.3811192217693535

Epoch: 236| Step: 0
Training loss: 2.2449023723602295
Validation loss: 2.377040901491719

Epoch: 5| Step: 1
Training loss: 1.944885492324829
Validation loss: 2.3674302921500257

Epoch: 5| Step: 2
Training loss: 2.7170653343200684
Validation loss: 2.372627701810611

Epoch: 5| Step: 3
Training loss: 3.217595338821411
Validation loss: 2.3759438478818504

Epoch: 5| Step: 4
Training loss: 3.075913190841675
Validation loss: 2.386685545726489

Epoch: 5| Step: 5
Training loss: 2.459876775741577
Validation loss: 2.3931321508140972

Epoch: 5| Step: 6
Training loss: 3.0974011421203613
Validation loss: 2.3926913584432294

Epoch: 5| Step: 7
Training loss: 2.5389299392700195
Validation loss: 2.39355626157535

Epoch: 5| Step: 8
Training loss: 1.4421741962432861
Validation loss: 2.382654215699883

Epoch: 5| Step: 9
Training loss: 2.679424285888672
Validation loss: 2.40584639323655

Epoch: 5| Step: 10
Training loss: 3.1317503452301025
Validation loss: 2.3929283875291065

Epoch: 237| Step: 0
Training loss: 3.2591888904571533
Validation loss: 2.417081066357192

Epoch: 5| Step: 1
Training loss: 2.460415840148926
Validation loss: 2.4086544077883483

Epoch: 5| Step: 2
Training loss: 2.9172611236572266
Validation loss: 2.4112988723221647

Epoch: 5| Step: 3
Training loss: 1.8377907276153564
Validation loss: 2.4106424188101165

Epoch: 5| Step: 4
Training loss: 2.7608225345611572
Validation loss: 2.4280429527323735

Epoch: 5| Step: 5
Training loss: 3.0724594593048096
Validation loss: 2.4223270211168515

Epoch: 5| Step: 6
Training loss: 2.659893751144409
Validation loss: 2.413362249251335

Epoch: 5| Step: 7
Training loss: 2.6729354858398438
Validation loss: 2.43024427916414

Epoch: 5| Step: 8
Training loss: 2.0552687644958496
Validation loss: 2.419410180020076

Epoch: 5| Step: 9
Training loss: 2.5172677040100098
Validation loss: 2.4229891941111577

Epoch: 5| Step: 10
Training loss: 2.292107105255127
Validation loss: 2.425799536448653

Epoch: 238| Step: 0
Training loss: 2.3571019172668457
Validation loss: 2.4229194336040045

Epoch: 5| Step: 1
Training loss: 2.5245914459228516
Validation loss: 2.4198472461392804

Epoch: 5| Step: 2
Training loss: 2.786604404449463
Validation loss: 2.4119575254378782

Epoch: 5| Step: 3
Training loss: 3.1323201656341553
Validation loss: 2.3921212765478317

Epoch: 5| Step: 4
Training loss: 1.9993159770965576
Validation loss: 2.372155515096521

Epoch: 5| Step: 5
Training loss: 2.2655224800109863
Validation loss: 2.365244598798854

Epoch: 5| Step: 6
Training loss: 2.780325412750244
Validation loss: 2.344460605293192

Epoch: 5| Step: 7
Training loss: 2.690969705581665
Validation loss: 2.3464772419262956

Epoch: 5| Step: 8
Training loss: 2.685183048248291
Validation loss: 2.3485610126167216

Epoch: 5| Step: 9
Training loss: 2.683223247528076
Validation loss: 2.350692954114688

Epoch: 5| Step: 10
Training loss: 2.463329792022705
Validation loss: 2.344811342095816

Epoch: 239| Step: 0
Training loss: 3.0141124725341797
Validation loss: 2.3454596278487996

Epoch: 5| Step: 1
Training loss: 3.4088542461395264
Validation loss: 2.350715655152516

Epoch: 5| Step: 2
Training loss: 2.5284876823425293
Validation loss: 2.342235001184607

Epoch: 5| Step: 3
Training loss: 2.582489490509033
Validation loss: 2.3397470264024633

Epoch: 5| Step: 4
Training loss: 2.8891665935516357
Validation loss: 2.3457811519663823

Epoch: 5| Step: 5
Training loss: 2.5211000442504883
Validation loss: 2.3442287598886797

Epoch: 5| Step: 6
Training loss: 2.289801836013794
Validation loss: 2.3593529680723786

Epoch: 5| Step: 7
Training loss: 1.9318408966064453
Validation loss: 2.356406734835717

Epoch: 5| Step: 8
Training loss: 2.8373916149139404
Validation loss: 2.340622332788283

Epoch: 5| Step: 9
Training loss: 2.4488635063171387
Validation loss: 2.3465939773026334

Epoch: 5| Step: 10
Training loss: 1.877289891242981
Validation loss: 2.346814578579318

Epoch: 240| Step: 0
Training loss: 1.7397350072860718
Validation loss: 2.358755429585775

Epoch: 5| Step: 1
Training loss: 2.6873257160186768
Validation loss: 2.354887382958525

Epoch: 5| Step: 2
Training loss: 1.8483703136444092
Validation loss: 2.3586239302030174

Epoch: 5| Step: 3
Training loss: 2.795766592025757
Validation loss: 2.3591484049315095

Epoch: 5| Step: 4
Training loss: 3.293870449066162
Validation loss: 2.371735857379052

Epoch: 5| Step: 5
Training loss: 2.2243220806121826
Validation loss: 2.372286529951198

Epoch: 5| Step: 6
Training loss: 3.071122169494629
Validation loss: 2.3665737464863765

Epoch: 5| Step: 7
Training loss: 2.49676775932312
Validation loss: 2.3696084509613695

Epoch: 5| Step: 8
Training loss: 3.2210144996643066
Validation loss: 2.3592927404629287

Epoch: 5| Step: 9
Training loss: 2.252927303314209
Validation loss: 2.365246030592149

Epoch: 5| Step: 10
Training loss: 2.877037286758423
Validation loss: 2.372107313525292

Epoch: 241| Step: 0
Training loss: 2.580592632293701
Validation loss: 2.372559775588333

Epoch: 5| Step: 1
Training loss: 2.1249117851257324
Validation loss: 2.375181987721433

Epoch: 5| Step: 2
Training loss: 2.334317684173584
Validation loss: 2.3685505031257548

Epoch: 5| Step: 3
Training loss: 2.5812618732452393
Validation loss: 2.364793928720618

Epoch: 5| Step: 4
Training loss: 2.53498911857605
Validation loss: 2.3636077603986188

Epoch: 5| Step: 5
Training loss: 2.5479331016540527
Validation loss: 2.3538037910256335

Epoch: 5| Step: 6
Training loss: 3.199936628341675
Validation loss: 2.345863870395127

Epoch: 5| Step: 7
Training loss: 2.7693421840667725
Validation loss: 2.3492277360731557

Epoch: 5| Step: 8
Training loss: 2.7051291465759277
Validation loss: 2.350000309687789

Epoch: 5| Step: 9
Training loss: 2.5804286003112793
Validation loss: 2.3351369160477833

Epoch: 5| Step: 10
Training loss: 2.367780923843384
Validation loss: 2.349155367061656

Epoch: 242| Step: 0
Training loss: 2.8535537719726562
Validation loss: 2.3411308949993503

Epoch: 5| Step: 1
Training loss: 1.9939868450164795
Validation loss: 2.344781234700193

Epoch: 5| Step: 2
Training loss: 2.04516863822937
Validation loss: 2.343663889874694

Epoch: 5| Step: 3
Training loss: 2.3636860847473145
Validation loss: 2.3521310975474696

Epoch: 5| Step: 4
Training loss: 3.542524814605713
Validation loss: 2.349536654769733

Epoch: 5| Step: 5
Training loss: 2.3446316719055176
Validation loss: 2.350143286489671

Epoch: 5| Step: 6
Training loss: 2.47761869430542
Validation loss: 2.3506772005429832

Epoch: 5| Step: 7
Training loss: 2.2923166751861572
Validation loss: 2.3462188551502843

Epoch: 5| Step: 8
Training loss: 2.6141648292541504
Validation loss: 2.3420216934655302

Epoch: 5| Step: 9
Training loss: 2.6232008934020996
Validation loss: 2.346979042535187

Epoch: 5| Step: 10
Training loss: 3.322606086730957
Validation loss: 2.340637542868173

Epoch: 243| Step: 0
Training loss: 3.2762293815612793
Validation loss: 2.3310296420128114

Epoch: 5| Step: 1
Training loss: 3.1524062156677246
Validation loss: 2.3504774288464616

Epoch: 5| Step: 2
Training loss: 2.4596025943756104
Validation loss: 2.344503502691946

Epoch: 5| Step: 3
Training loss: 3.023616313934326
Validation loss: 2.3458210229873657

Epoch: 5| Step: 4
Training loss: 1.5154534578323364
Validation loss: 2.3450821497107066

Epoch: 5| Step: 5
Training loss: 2.3592982292175293
Validation loss: 2.34732969345585

Epoch: 5| Step: 6
Training loss: 2.661527156829834
Validation loss: 2.350684896592171

Epoch: 5| Step: 7
Training loss: 2.126943588256836
Validation loss: 2.3604373111519763

Epoch: 5| Step: 8
Training loss: 2.464770793914795
Validation loss: 2.357639928017893

Epoch: 5| Step: 9
Training loss: 2.6502044200897217
Validation loss: 2.35208773356612

Epoch: 5| Step: 10
Training loss: 2.5997023582458496
Validation loss: 2.3538206931083434

Epoch: 244| Step: 0
Training loss: 2.5556745529174805
Validation loss: 2.35428197409517

Epoch: 5| Step: 1
Training loss: 2.331216335296631
Validation loss: 2.3552092095857025

Epoch: 5| Step: 2
Training loss: 3.464264392852783
Validation loss: 2.356685705082391

Epoch: 5| Step: 3
Training loss: 2.4069793224334717
Validation loss: 2.3637470276125017

Epoch: 5| Step: 4
Training loss: 3.414167881011963
Validation loss: 2.3495919909528507

Epoch: 5| Step: 5
Training loss: 2.495704174041748
Validation loss: 2.352198446950605

Epoch: 5| Step: 6
Training loss: 2.4327988624572754
Validation loss: 2.353896297434325

Epoch: 5| Step: 7
Training loss: 2.3763108253479004
Validation loss: 2.3464009607991865

Epoch: 5| Step: 8
Training loss: 2.80802321434021
Validation loss: 2.3580354285496536

Epoch: 5| Step: 9
Training loss: 1.8230148553848267
Validation loss: 2.3599001335841354

Epoch: 5| Step: 10
Training loss: 2.1011672019958496
Validation loss: 2.3619602393078547

Epoch: 245| Step: 0
Training loss: 2.1733717918395996
Validation loss: 2.3643975052782285

Epoch: 5| Step: 1
Training loss: 2.2189414501190186
Validation loss: 2.3771556269737983

Epoch: 5| Step: 2
Training loss: 3.0118820667266846
Validation loss: 2.3879366869567544

Epoch: 5| Step: 3
Training loss: 3.0129802227020264
Validation loss: 2.3806115452961256

Epoch: 5| Step: 4
Training loss: 2.271026849746704
Validation loss: 2.3720923290457776

Epoch: 5| Step: 5
Training loss: 2.2231268882751465
Validation loss: 2.373093415332097

Epoch: 5| Step: 6
Training loss: 2.7523245811462402
Validation loss: 2.3751799419362056

Epoch: 5| Step: 7
Training loss: 2.597952365875244
Validation loss: 2.3652222823071223

Epoch: 5| Step: 8
Training loss: 3.671938419342041
Validation loss: 2.3748016793240785

Epoch: 5| Step: 9
Training loss: 2.144510507583618
Validation loss: 2.3641035890066497

Epoch: 5| Step: 10
Training loss: 2.260282516479492
Validation loss: 2.3578198725177395

Epoch: 246| Step: 0
Training loss: 2.6995596885681152
Validation loss: 2.3429924313740065

Epoch: 5| Step: 1
Training loss: 2.9968957901000977
Validation loss: 2.343971385750719

Epoch: 5| Step: 2
Training loss: 3.0892493724823
Validation loss: 2.3430328317867812

Epoch: 5| Step: 3
Training loss: 2.2178590297698975
Validation loss: 2.3425183296203613

Epoch: 5| Step: 4
Training loss: 1.7912296056747437
Validation loss: 2.3367189694476385

Epoch: 5| Step: 5
Training loss: 2.961474895477295
Validation loss: 2.3358746446588987

Epoch: 5| Step: 6
Training loss: 2.7345402240753174
Validation loss: 2.342633257630051

Epoch: 5| Step: 7
Training loss: 2.170567750930786
Validation loss: 2.33140302729863

Epoch: 5| Step: 8
Training loss: 2.404087543487549
Validation loss: 2.3321002465422436

Epoch: 5| Step: 9
Training loss: 2.257103204727173
Validation loss: 2.3253329005292667

Epoch: 5| Step: 10
Training loss: 2.9567296504974365
Validation loss: 2.337124570723503

Epoch: 247| Step: 0
Training loss: 2.3717386722564697
Validation loss: 2.3373449694725776

Epoch: 5| Step: 1
Training loss: 2.184998035430908
Validation loss: 2.343842146217182

Epoch: 5| Step: 2
Training loss: 3.17160964012146
Validation loss: 2.3366133987262683

Epoch: 5| Step: 3
Training loss: 3.31703519821167
Validation loss: 2.34665423311213

Epoch: 5| Step: 4
Training loss: 2.511014699935913
Validation loss: 2.352236117086103

Epoch: 5| Step: 5
Training loss: 2.4583473205566406
Validation loss: 2.346575783145043

Epoch: 5| Step: 6
Training loss: 1.9115447998046875
Validation loss: 2.350448792980563

Epoch: 5| Step: 7
Training loss: 2.5983033180236816
Validation loss: 2.34832739573653

Epoch: 5| Step: 8
Training loss: 2.223099708557129
Validation loss: 2.3474002422824984

Epoch: 5| Step: 9
Training loss: 2.826439619064331
Validation loss: 2.352534529983356

Epoch: 5| Step: 10
Training loss: 2.574988603591919
Validation loss: 2.3481826064407185

Epoch: 248| Step: 0
Training loss: 2.3331170082092285
Validation loss: 2.352126888049546

Epoch: 5| Step: 1
Training loss: 2.6247334480285645
Validation loss: 2.366235779177758

Epoch: 5| Step: 2
Training loss: 2.2307231426239014
Validation loss: 2.366212784603078

Epoch: 5| Step: 3
Training loss: 2.006730318069458
Validation loss: 2.3600528265840266

Epoch: 5| Step: 4
Training loss: 2.5415611267089844
Validation loss: 2.3801074028015137

Epoch: 5| Step: 5
Training loss: 2.5051865577697754
Validation loss: 2.367415492252637

Epoch: 5| Step: 6
Training loss: 3.2851219177246094
Validation loss: 2.3775607283397386

Epoch: 5| Step: 7
Training loss: 2.638029098510742
Validation loss: 2.3869936209852978

Epoch: 5| Step: 8
Training loss: 3.175629138946533
Validation loss: 2.3919935123894804

Epoch: 5| Step: 9
Training loss: 2.791186809539795
Validation loss: 2.3768286833199124

Epoch: 5| Step: 10
Training loss: 2.0128512382507324
Validation loss: 2.3833420712460756

Epoch: 249| Step: 0
Training loss: 2.8022055625915527
Validation loss: 2.3859911298239105

Epoch: 5| Step: 1
Training loss: 2.4512763023376465
Validation loss: 2.378944607191188

Epoch: 5| Step: 2
Training loss: 2.6100399494171143
Validation loss: 2.380710924825361

Epoch: 5| Step: 3
Training loss: 2.1016173362731934
Validation loss: 2.3658637897942656

Epoch: 5| Step: 4
Training loss: 2.9737281799316406
Validation loss: 2.3734135320109706

Epoch: 5| Step: 5
Training loss: 2.2616703510284424
Validation loss: 2.370457755622043

Epoch: 5| Step: 6
Training loss: 2.5157063007354736
Validation loss: 2.3509278502515567

Epoch: 5| Step: 7
Training loss: 2.5904712677001953
Validation loss: 2.351238953169956

Epoch: 5| Step: 8
Training loss: 2.0626959800720215
Validation loss: 2.356636147345266

Epoch: 5| Step: 9
Training loss: 2.5807080268859863
Validation loss: 2.352008194051763

Epoch: 5| Step: 10
Training loss: 3.448230743408203
Validation loss: 2.347879999427385

Epoch: 250| Step: 0
Training loss: 2.0164082050323486
Validation loss: 2.3521265881035918

Epoch: 5| Step: 1
Training loss: 2.610111713409424
Validation loss: 2.360405755299394

Epoch: 5| Step: 2
Training loss: 2.530968189239502
Validation loss: 2.369214501432193

Epoch: 5| Step: 3
Training loss: 1.927189588546753
Validation loss: 2.375219461738422

Epoch: 5| Step: 4
Training loss: 3.1068053245544434
Validation loss: 2.3760770290128645

Epoch: 5| Step: 5
Training loss: 2.7226483821868896
Validation loss: 2.367200736076601

Epoch: 5| Step: 6
Training loss: 2.836510419845581
Validation loss: 2.3757777649869203

Epoch: 5| Step: 7
Training loss: 2.8217506408691406
Validation loss: 2.371057682139899

Epoch: 5| Step: 8
Training loss: 2.7593600749969482
Validation loss: 2.3783967751328663

Epoch: 5| Step: 9
Training loss: 1.9682838916778564
Validation loss: 2.3579305346294115

Epoch: 5| Step: 10
Training loss: 3.1192471981048584
Validation loss: 2.3570092775488414

Epoch: 251| Step: 0
Training loss: 1.4097732305526733
Validation loss: 2.3381437037580755

Epoch: 5| Step: 1
Training loss: 2.6438589096069336
Validation loss: 2.3376454486641833

Epoch: 5| Step: 2
Training loss: 2.6360573768615723
Validation loss: 2.3372511248434744

Epoch: 5| Step: 3
Training loss: 2.699824094772339
Validation loss: 2.3318213621775308

Epoch: 5| Step: 4
Training loss: 2.9883837699890137
Validation loss: 2.336346948018638

Epoch: 5| Step: 5
Training loss: 2.4915719032287598
Validation loss: 2.3402354845436673

Epoch: 5| Step: 6
Training loss: 2.662309169769287
Validation loss: 2.3496850395715363

Epoch: 5| Step: 7
Training loss: 2.8094897270202637
Validation loss: 2.347771034445814

Epoch: 5| Step: 8
Training loss: 2.912938356399536
Validation loss: 2.342974429489464

Epoch: 5| Step: 9
Training loss: 2.9310383796691895
Validation loss: 2.331709982246481

Epoch: 5| Step: 10
Training loss: 1.9631742238998413
Validation loss: 2.3393200520546205

Epoch: 252| Step: 0
Training loss: 2.141124725341797
Validation loss: 2.3414816446201776

Epoch: 5| Step: 1
Training loss: 2.8005852699279785
Validation loss: 2.335632185782156

Epoch: 5| Step: 2
Training loss: 3.431952714920044
Validation loss: 2.3297459028100453

Epoch: 5| Step: 3
Training loss: 2.358161449432373
Validation loss: 2.3289648179085023

Epoch: 5| Step: 4
Training loss: 2.7591826915740967
Validation loss: 2.330355028952322

Epoch: 5| Step: 5
Training loss: 2.090157985687256
Validation loss: 2.3180484694819294

Epoch: 5| Step: 6
Training loss: 2.8137216567993164
Validation loss: 2.3105646435932448

Epoch: 5| Step: 7
Training loss: 2.1352219581604004
Validation loss: 2.316852952844353

Epoch: 5| Step: 8
Training loss: 2.639876127243042
Validation loss: 2.3264904470853907

Epoch: 5| Step: 9
Training loss: 2.5168840885162354
Validation loss: 2.3230442513701735

Epoch: 5| Step: 10
Training loss: 2.486715078353882
Validation loss: 2.3358973303148822

Epoch: 253| Step: 0
Training loss: 2.091299533843994
Validation loss: 2.3414019282146166

Epoch: 5| Step: 1
Training loss: 3.079204559326172
Validation loss: 2.3477539554719002

Epoch: 5| Step: 2
Training loss: 2.1658787727355957
Validation loss: 2.349223659884545

Epoch: 5| Step: 3
Training loss: 2.4076550006866455
Validation loss: 2.3632190509509017

Epoch: 5| Step: 4
Training loss: 2.7680835723876953
Validation loss: 2.3518151121754802

Epoch: 5| Step: 5
Training loss: 3.094360589981079
Validation loss: 2.3673656614877845

Epoch: 5| Step: 6
Training loss: 2.3992981910705566
Validation loss: 2.3566669892239314

Epoch: 5| Step: 7
Training loss: 2.144728899002075
Validation loss: 2.3504926645627586

Epoch: 5| Step: 8
Training loss: 3.312873363494873
Validation loss: 2.3373329203615905

Epoch: 5| Step: 9
Training loss: 2.320211887359619
Validation loss: 2.3375445745324575

Epoch: 5| Step: 10
Training loss: 2.4903345108032227
Validation loss: 2.3272309790375414

Epoch: 254| Step: 0
Training loss: 2.277087450027466
Validation loss: 2.3367014290184103

Epoch: 5| Step: 1
Training loss: 1.8898437023162842
Validation loss: 2.3402403631517963

Epoch: 5| Step: 2
Training loss: 2.8533120155334473
Validation loss: 2.3330202743571293

Epoch: 5| Step: 3
Training loss: 2.643671751022339
Validation loss: 2.363373210353236

Epoch: 5| Step: 4
Training loss: 2.5287938117980957
Validation loss: 2.388587381250115

Epoch: 5| Step: 5
Training loss: 2.6355643272399902
Validation loss: 2.399214290803479

Epoch: 5| Step: 6
Training loss: 2.049870014190674
Validation loss: 2.4052504006252495

Epoch: 5| Step: 7
Training loss: 3.1406967639923096
Validation loss: 2.3974586430416314

Epoch: 5| Step: 8
Training loss: 2.433328628540039
Validation loss: 2.3719806209687264

Epoch: 5| Step: 9
Training loss: 3.068957567214966
Validation loss: 2.3528730561656337

Epoch: 5| Step: 10
Training loss: 3.106534242630005
Validation loss: 2.338498996150109

Epoch: 255| Step: 0
Training loss: 2.6588540077209473
Validation loss: 2.3431725450741347

Epoch: 5| Step: 1
Training loss: 2.544848918914795
Validation loss: 2.3375222580407256

Epoch: 5| Step: 2
Training loss: 1.9494645595550537
Validation loss: 2.354896463373656

Epoch: 5| Step: 3
Training loss: 2.2344727516174316
Validation loss: 2.3608922650737147

Epoch: 5| Step: 4
Training loss: 2.7979559898376465
Validation loss: 2.377916689841978

Epoch: 5| Step: 5
Training loss: 3.0468990802764893
Validation loss: 2.379605398383192

Epoch: 5| Step: 6
Training loss: 3.3742003440856934
Validation loss: 2.3839907389815136

Epoch: 5| Step: 7
Training loss: 2.4426584243774414
Validation loss: 2.3769810661192863

Epoch: 5| Step: 8
Training loss: 2.693448305130005
Validation loss: 2.3539362005008164

Epoch: 5| Step: 9
Training loss: 2.3531670570373535
Validation loss: 2.341919663131878

Epoch: 5| Step: 10
Training loss: 2.1718780994415283
Validation loss: 2.3357794515548216

Epoch: 256| Step: 0
Training loss: 2.8913464546203613
Validation loss: 2.330826336337674

Epoch: 5| Step: 1
Training loss: 2.7452430725097656
Validation loss: 2.316960947487944

Epoch: 5| Step: 2
Training loss: 2.492358684539795
Validation loss: 2.3111537348839546

Epoch: 5| Step: 3
Training loss: 3.1128973960876465
Validation loss: 2.314951301902853

Epoch: 5| Step: 4
Training loss: 2.9622340202331543
Validation loss: 2.3099430966120895

Epoch: 5| Step: 5
Training loss: 2.58566951751709
Validation loss: 2.308889114728538

Epoch: 5| Step: 6
Training loss: 2.7116878032684326
Validation loss: 2.3163001050231276

Epoch: 5| Step: 7
Training loss: 1.6287599802017212
Validation loss: 2.3152890179746892

Epoch: 5| Step: 8
Training loss: 1.9486643075942993
Validation loss: 2.316262609215193

Epoch: 5| Step: 9
Training loss: 2.587465763092041
Validation loss: 2.3191749972681843

Epoch: 5| Step: 10
Training loss: 2.4939186573028564
Validation loss: 2.3195888919215046

Epoch: 257| Step: 0
Training loss: 3.414842128753662
Validation loss: 2.3314596991385184

Epoch: 5| Step: 1
Training loss: 2.328944206237793
Validation loss: 2.3320030909712597

Epoch: 5| Step: 2
Training loss: 1.7344510555267334
Validation loss: 2.3373942657183577

Epoch: 5| Step: 3
Training loss: 2.1329798698425293
Validation loss: 2.3459608939386185

Epoch: 5| Step: 4
Training loss: 2.893211603164673
Validation loss: 2.3469417684821674

Epoch: 5| Step: 5
Training loss: 2.7517199516296387
Validation loss: 2.345751190698275

Epoch: 5| Step: 6
Training loss: 2.152768850326538
Validation loss: 2.3443531861869236

Epoch: 5| Step: 7
Training loss: 2.562598705291748
Validation loss: 2.356479911394017

Epoch: 5| Step: 8
Training loss: 2.997483491897583
Validation loss: 2.354886713848319

Epoch: 5| Step: 9
Training loss: 2.598651885986328
Validation loss: 2.3460795853727605

Epoch: 5| Step: 10
Training loss: 2.7121191024780273
Validation loss: 2.3373584978042112

Epoch: 258| Step: 0
Training loss: 2.909170627593994
Validation loss: 2.338811533425444

Epoch: 5| Step: 1
Training loss: 2.4075045585632324
Validation loss: 2.3384767373402915

Epoch: 5| Step: 2
Training loss: 2.7557947635650635
Validation loss: 2.346083323160807

Epoch: 5| Step: 3
Training loss: 2.93324875831604
Validation loss: 2.3378140336723736

Epoch: 5| Step: 4
Training loss: 2.2115378379821777
Validation loss: 2.3417034046624297

Epoch: 5| Step: 5
Training loss: 2.747826337814331
Validation loss: 2.3494899554919173

Epoch: 5| Step: 6
Training loss: 3.110954761505127
Validation loss: 2.3418662830065657

Epoch: 5| Step: 7
Training loss: 1.7479331493377686
Validation loss: 2.3368024415867303

Epoch: 5| Step: 8
Training loss: 2.0342981815338135
Validation loss: 2.337622527153261

Epoch: 5| Step: 9
Training loss: 2.62431001663208
Validation loss: 2.344571944205992

Epoch: 5| Step: 10
Training loss: 2.4685487747192383
Validation loss: 2.35967061596532

Epoch: 259| Step: 0
Training loss: 2.4702465534210205
Validation loss: 2.372462921245124

Epoch: 5| Step: 1
Training loss: 2.105858564376831
Validation loss: 2.3969247776974916

Epoch: 5| Step: 2
Training loss: 2.445908784866333
Validation loss: 2.396408660437471

Epoch: 5| Step: 3
Training loss: 2.5664830207824707
Validation loss: 2.391994708327837

Epoch: 5| Step: 4
Training loss: 2.9260106086730957
Validation loss: 2.3668761535357405

Epoch: 5| Step: 5
Training loss: 3.4665703773498535
Validation loss: 2.3570950415826615

Epoch: 5| Step: 6
Training loss: 2.427325963973999
Validation loss: 2.352277312227475

Epoch: 5| Step: 7
Training loss: 2.1474192142486572
Validation loss: 2.346044144322795

Epoch: 5| Step: 8
Training loss: 2.3183205127716064
Validation loss: 2.35248920225328

Epoch: 5| Step: 9
Training loss: 2.629819393157959
Validation loss: 2.353905377849456

Epoch: 5| Step: 10
Training loss: 2.7614598274230957
Validation loss: 2.3499320373740247

Epoch: 260| Step: 0
Training loss: 1.7219032049179077
Validation loss: 2.354598097903754

Epoch: 5| Step: 1
Training loss: 2.582399606704712
Validation loss: 2.3483773508379535

Epoch: 5| Step: 2
Training loss: 2.076902389526367
Validation loss: 2.3434055620624172

Epoch: 5| Step: 3
Training loss: 2.7868309020996094
Validation loss: 2.3352333063720376

Epoch: 5| Step: 4
Training loss: 2.9224398136138916
Validation loss: 2.335257366139402

Epoch: 5| Step: 5
Training loss: 2.946918487548828
Validation loss: 2.3308589971193703

Epoch: 5| Step: 6
Training loss: 2.3957865238189697
Validation loss: 2.326284800806353

Epoch: 5| Step: 7
Training loss: 2.683964252471924
Validation loss: 2.331741843172299

Epoch: 5| Step: 8
Training loss: 2.076477527618408
Validation loss: 2.337121002135738

Epoch: 5| Step: 9
Training loss: 3.076270580291748
Validation loss: 2.3362348515500306

Epoch: 5| Step: 10
Training loss: 2.9213664531707764
Validation loss: 2.3375581515732633

Epoch: 261| Step: 0
Training loss: 2.1769049167633057
Validation loss: 2.3419231061012513

Epoch: 5| Step: 1
Training loss: 2.671704053878784
Validation loss: 2.3471187109588296

Epoch: 5| Step: 2
Training loss: 3.3683559894561768
Validation loss: 2.342021173046481

Epoch: 5| Step: 3
Training loss: 2.3403704166412354
Validation loss: 2.344754820228905

Epoch: 5| Step: 4
Training loss: 1.880626916885376
Validation loss: 2.3299273149941557

Epoch: 5| Step: 5
Training loss: 2.6706454753875732
Validation loss: 2.33655107918606

Epoch: 5| Step: 6
Training loss: 2.077681064605713
Validation loss: 2.319587920301704

Epoch: 5| Step: 7
Training loss: 2.3295655250549316
Validation loss: 2.3185623717564408

Epoch: 5| Step: 8
Training loss: 2.4509437084198
Validation loss: 2.3110588289076284

Epoch: 5| Step: 9
Training loss: 2.9106194972991943
Validation loss: 2.315429005571591

Epoch: 5| Step: 10
Training loss: 3.2405543327331543
Validation loss: 2.3095878990747596

Epoch: 262| Step: 0
Training loss: 2.1444239616394043
Validation loss: 2.3089463172420377

Epoch: 5| Step: 1
Training loss: 3.07747220993042
Validation loss: 2.312507411485077

Epoch: 5| Step: 2
Training loss: 2.7474381923675537
Validation loss: 2.312967602924634

Epoch: 5| Step: 3
Training loss: 2.011192560195923
Validation loss: 2.3041510223060526

Epoch: 5| Step: 4
Training loss: 2.651003122329712
Validation loss: 2.3106602340616207

Epoch: 5| Step: 5
Training loss: 2.820114850997925
Validation loss: 2.3177858244988228

Epoch: 5| Step: 6
Training loss: 2.334751844406128
Validation loss: 2.312440187700333

Epoch: 5| Step: 7
Training loss: 3.0012919902801514
Validation loss: 2.3160552209423435

Epoch: 5| Step: 8
Training loss: 2.522700786590576
Validation loss: 2.312217670102273

Epoch: 5| Step: 9
Training loss: 2.1441879272460938
Validation loss: 2.3249093332598285

Epoch: 5| Step: 10
Training loss: 2.578453779220581
Validation loss: 2.3300202328671693

Epoch: 263| Step: 0
Training loss: 2.0524253845214844
Validation loss: 2.334388961074173

Epoch: 5| Step: 1
Training loss: 1.8909399509429932
Validation loss: 2.32981288561257

Epoch: 5| Step: 2
Training loss: 2.365602493286133
Validation loss: 2.325596563277706

Epoch: 5| Step: 3
Training loss: 3.2942252159118652
Validation loss: 2.343793430636006

Epoch: 5| Step: 4
Training loss: 3.011923313140869
Validation loss: 2.331719547189692

Epoch: 5| Step: 5
Training loss: 2.9819741249084473
Validation loss: 2.3270930859350387

Epoch: 5| Step: 6
Training loss: 2.2707645893096924
Validation loss: 2.334942438269174

Epoch: 5| Step: 7
Training loss: 2.6781811714172363
Validation loss: 2.3340534240968767

Epoch: 5| Step: 8
Training loss: 2.838491439819336
Validation loss: 2.336782265734929

Epoch: 5| Step: 9
Training loss: 2.0396924018859863
Validation loss: 2.3363471838735763

Epoch: 5| Step: 10
Training loss: 2.5303030014038086
Validation loss: 2.3414491556024037

Epoch: 264| Step: 0
Training loss: 3.3950226306915283
Validation loss: 2.3506967662483134

Epoch: 5| Step: 1
Training loss: 2.6342673301696777
Validation loss: 2.349940597370107

Epoch: 5| Step: 2
Training loss: 2.009168863296509
Validation loss: 2.3548892082706576

Epoch: 5| Step: 3
Training loss: 2.18330979347229
Validation loss: 2.3602097419000443

Epoch: 5| Step: 4
Training loss: 2.952563762664795
Validation loss: 2.3508757186192337

Epoch: 5| Step: 5
Training loss: 2.5074563026428223
Validation loss: 2.352684526033299

Epoch: 5| Step: 6
Training loss: 2.5037894248962402
Validation loss: 2.3468160731818086

Epoch: 5| Step: 7
Training loss: 2.302107572555542
Validation loss: 2.350218326814713

Epoch: 5| Step: 8
Training loss: 2.289746046066284
Validation loss: 2.352578665620537

Epoch: 5| Step: 9
Training loss: 2.539416790008545
Validation loss: 2.3601460815757833

Epoch: 5| Step: 10
Training loss: 2.761592388153076
Validation loss: 2.354271881041988

Epoch: 265| Step: 0
Training loss: 2.014373302459717
Validation loss: 2.3533295559626755

Epoch: 5| Step: 1
Training loss: 2.4422926902770996
Validation loss: 2.354654965862151

Epoch: 5| Step: 2
Training loss: 2.6956207752227783
Validation loss: 2.359586887462165

Epoch: 5| Step: 3
Training loss: 2.1348586082458496
Validation loss: 2.358410140519501

Epoch: 5| Step: 4
Training loss: 2.696728229522705
Validation loss: 2.357876980176536

Epoch: 5| Step: 5
Training loss: 3.000951051712036
Validation loss: 2.347596853010116

Epoch: 5| Step: 6
Training loss: 2.45045804977417
Validation loss: 2.3577815358356764

Epoch: 5| Step: 7
Training loss: 2.6399333477020264
Validation loss: 2.346962446807533

Epoch: 5| Step: 8
Training loss: 2.7417054176330566
Validation loss: 2.334542659021193

Epoch: 5| Step: 9
Training loss: 2.462977647781372
Validation loss: 2.33002927354587

Epoch: 5| Step: 10
Training loss: 2.6003308296203613
Validation loss: 2.323285061825988

Epoch: 266| Step: 0
Training loss: 2.4188072681427
Validation loss: 2.3106461058380785

Epoch: 5| Step: 1
Training loss: 2.444225788116455
Validation loss: 2.309794520819059

Epoch: 5| Step: 2
Training loss: 2.963019847869873
Validation loss: 2.3094456324013333

Epoch: 5| Step: 3
Training loss: 2.744008779525757
Validation loss: 2.310230105153976

Epoch: 5| Step: 4
Training loss: 2.535719394683838
Validation loss: 2.31319183944374

Epoch: 5| Step: 5
Training loss: 3.022181987762451
Validation loss: 2.310348026214107

Epoch: 5| Step: 6
Training loss: 2.9978554248809814
Validation loss: 2.3130371955133255

Epoch: 5| Step: 7
Training loss: 2.2162110805511475
Validation loss: 2.3148206613397084

Epoch: 5| Step: 8
Training loss: 2.1642627716064453
Validation loss: 2.3174749881990495

Epoch: 5| Step: 9
Training loss: 1.752808928489685
Validation loss: 2.320011972099222

Epoch: 5| Step: 10
Training loss: 2.6609883308410645
Validation loss: 2.329241229641822

Epoch: 267| Step: 0
Training loss: 2.6112046241760254
Validation loss: 2.311812021399057

Epoch: 5| Step: 1
Training loss: 2.22890043258667
Validation loss: 2.3282703635513142

Epoch: 5| Step: 2
Training loss: 2.780823230743408
Validation loss: 2.334692226943149

Epoch: 5| Step: 3
Training loss: 3.0233466625213623
Validation loss: 2.3359971533539476

Epoch: 5| Step: 4
Training loss: 2.738110303878784
Validation loss: 2.3410215057352537

Epoch: 5| Step: 5
Training loss: 2.8134310245513916
Validation loss: 2.359681060237269

Epoch: 5| Step: 6
Training loss: 2.749380588531494
Validation loss: 2.3533302942911782

Epoch: 5| Step: 7
Training loss: 2.423227310180664
Validation loss: 2.340600223951442

Epoch: 5| Step: 8
Training loss: 2.182033061981201
Validation loss: 2.3398013384111467

Epoch: 5| Step: 9
Training loss: 2.4903404712677
Validation loss: 2.3426874940113356

Epoch: 5| Step: 10
Training loss: 1.721763014793396
Validation loss: 2.3335308567170174

Epoch: 268| Step: 0
Training loss: 2.430056571960449
Validation loss: 2.3412683676647883

Epoch: 5| Step: 1
Training loss: 2.9837417602539062
Validation loss: 2.3433733909360823

Epoch: 5| Step: 2
Training loss: 1.6394182443618774
Validation loss: 2.3409076993183424

Epoch: 5| Step: 3
Training loss: 2.653658628463745
Validation loss: 2.349997215373542

Epoch: 5| Step: 4
Training loss: 2.7757036685943604
Validation loss: 2.353039828679895

Epoch: 5| Step: 5
Training loss: 3.0262844562530518
Validation loss: 2.3540042549051265

Epoch: 5| Step: 6
Training loss: 2.4547154903411865
Validation loss: 2.34718559890665

Epoch: 5| Step: 7
Training loss: 2.609837055206299
Validation loss: 2.3437018484197636

Epoch: 5| Step: 8
Training loss: 1.9344520568847656
Validation loss: 2.350834726005472

Epoch: 5| Step: 9
Training loss: 2.93635630607605
Validation loss: 2.36412158320027

Epoch: 5| Step: 10
Training loss: 2.2943851947784424
Validation loss: 2.351986860716215

Epoch: 269| Step: 0
Training loss: 2.5408427715301514
Validation loss: 2.356036545127951

Epoch: 5| Step: 1
Training loss: 2.490239381790161
Validation loss: 2.356256136330225

Epoch: 5| Step: 2
Training loss: 2.6213550567626953
Validation loss: 2.352905155510031

Epoch: 5| Step: 3
Training loss: 2.5700106620788574
Validation loss: 2.3368079252140497

Epoch: 5| Step: 4
Training loss: 2.4205098152160645
Validation loss: 2.3409339971439813

Epoch: 5| Step: 5
Training loss: 2.470590591430664
Validation loss: 2.3551858727649977

Epoch: 5| Step: 6
Training loss: 2.0129027366638184
Validation loss: 2.379955155875093

Epoch: 5| Step: 7
Training loss: 2.985687255859375
Validation loss: 2.3825538107143935

Epoch: 5| Step: 8
Training loss: 2.698613166809082
Validation loss: 2.395334668056939

Epoch: 5| Step: 9
Training loss: 2.8367860317230225
Validation loss: 2.395866499152235

Epoch: 5| Step: 10
Training loss: 2.2579684257507324
Validation loss: 2.389856940956526

Epoch: 270| Step: 0
Training loss: 2.7870826721191406
Validation loss: 2.353463613858787

Epoch: 5| Step: 1
Training loss: 2.3503031730651855
Validation loss: 2.3538029040059736

Epoch: 5| Step: 2
Training loss: 2.6612539291381836
Validation loss: 2.3582889033902075

Epoch: 5| Step: 3
Training loss: 2.5412752628326416
Validation loss: 2.3484043254647204

Epoch: 5| Step: 4
Training loss: 2.274935483932495
Validation loss: 2.335435196917544

Epoch: 5| Step: 5
Training loss: 2.296846628189087
Validation loss: 2.328076080609393

Epoch: 5| Step: 6
Training loss: 3.0869548320770264
Validation loss: 2.330462227585495

Epoch: 5| Step: 7
Training loss: 1.8465607166290283
Validation loss: 2.329163751294536

Epoch: 5| Step: 8
Training loss: 3.138887882232666
Validation loss: 2.3253293345051427

Epoch: 5| Step: 9
Training loss: 2.607468843460083
Validation loss: 2.3169527489651918

Epoch: 5| Step: 10
Training loss: 2.1249277591705322
Validation loss: 2.319419409639092

Epoch: 271| Step: 0
Training loss: 2.5089893341064453
Validation loss: 2.3164559256645942

Epoch: 5| Step: 1
Training loss: 2.2838168144226074
Validation loss: 2.32036498285109

Epoch: 5| Step: 2
Training loss: 3.082223653793335
Validation loss: 2.323994703190301

Epoch: 5| Step: 3
Training loss: 2.190840244293213
Validation loss: 2.329899731502738

Epoch: 5| Step: 4
Training loss: 2.7288169860839844
Validation loss: 2.325531005859375

Epoch: 5| Step: 5
Training loss: 2.8520350456237793
Validation loss: 2.334868118327151

Epoch: 5| Step: 6
Training loss: 2.4471962451934814
Validation loss: 2.354895771190684

Epoch: 5| Step: 7
Training loss: 2.089146614074707
Validation loss: 2.351631659333424

Epoch: 5| Step: 8
Training loss: 2.7274551391601562
Validation loss: 2.3565408004227506

Epoch: 5| Step: 9
Training loss: 2.5406017303466797
Validation loss: 2.3211570657709593

Epoch: 5| Step: 10
Training loss: 2.4045534133911133
Validation loss: 2.320392067714404

Epoch: 272| Step: 0
Training loss: 2.393624782562256
Validation loss: 2.3169648108943814

Epoch: 5| Step: 1
Training loss: 2.572380542755127
Validation loss: 2.330036240239297

Epoch: 5| Step: 2
Training loss: 2.6945483684539795
Validation loss: 2.34336821750928

Epoch: 5| Step: 3
Training loss: 2.309619188308716
Validation loss: 2.34552082707805

Epoch: 5| Step: 4
Training loss: 2.7025694847106934
Validation loss: 2.348395775723201

Epoch: 5| Step: 5
Training loss: 2.373657703399658
Validation loss: 2.353147984832846

Epoch: 5| Step: 6
Training loss: 3.390599012374878
Validation loss: 2.359708147664224

Epoch: 5| Step: 7
Training loss: 2.2630114555358887
Validation loss: 2.3579037394574893

Epoch: 5| Step: 8
Training loss: 2.367133378982544
Validation loss: 2.348754362393451

Epoch: 5| Step: 9
Training loss: 2.703582763671875
Validation loss: 2.3449104114245345

Epoch: 5| Step: 10
Training loss: 2.2084076404571533
Validation loss: 2.3355292376651557

Epoch: 273| Step: 0
Training loss: 3.075960159301758
Validation loss: 2.315759394758491

Epoch: 5| Step: 1
Training loss: 1.989611029624939
Validation loss: 2.314454370929349

Epoch: 5| Step: 2
Training loss: 2.524740219116211
Validation loss: 2.309377078087099

Epoch: 5| Step: 3
Training loss: 2.1171936988830566
Validation loss: 2.314836145729147

Epoch: 5| Step: 4
Training loss: 3.1875905990600586
Validation loss: 2.322863740305747

Epoch: 5| Step: 5
Training loss: 2.397893190383911
Validation loss: 2.335206723982288

Epoch: 5| Step: 6
Training loss: 1.964694619178772
Validation loss: 2.362449002522294

Epoch: 5| Step: 7
Training loss: 2.3934879302978516
Validation loss: 2.3575915700645855

Epoch: 5| Step: 8
Training loss: 2.8395442962646484
Validation loss: 2.3576925698147027

Epoch: 5| Step: 9
Training loss: 3.031262159347534
Validation loss: 2.334417453376196

Epoch: 5| Step: 10
Training loss: 2.406510591506958
Validation loss: 2.320441512651341

Epoch: 274| Step: 0
Training loss: 2.229976177215576
Validation loss: 2.316459719852735

Epoch: 5| Step: 1
Training loss: 2.696528196334839
Validation loss: 2.295929854915988

Epoch: 5| Step: 2
Training loss: 2.5928001403808594
Validation loss: 2.3056959054803334

Epoch: 5| Step: 3
Training loss: 2.294044256210327
Validation loss: 2.3263205353931715

Epoch: 5| Step: 4
Training loss: 2.801025390625
Validation loss: 2.326398176531638

Epoch: 5| Step: 5
Training loss: 2.5420336723327637
Validation loss: 2.344451371059623

Epoch: 5| Step: 6
Training loss: 2.2459545135498047
Validation loss: 2.3476840962645826

Epoch: 5| Step: 7
Training loss: 2.726041316986084
Validation loss: 2.35354176644356

Epoch: 5| Step: 8
Training loss: 2.6249756813049316
Validation loss: 2.3556837625400995

Epoch: 5| Step: 9
Training loss: 2.706089735031128
Validation loss: 2.335402937345607

Epoch: 5| Step: 10
Training loss: 2.4528567790985107
Validation loss: 2.3277620859043573

Epoch: 275| Step: 0
Training loss: 2.2639732360839844
Validation loss: 2.333455998410461

Epoch: 5| Step: 1
Training loss: 2.929306983947754
Validation loss: 2.3255255529957433

Epoch: 5| Step: 2
Training loss: 1.8270533084869385
Validation loss: 2.3068401685325046

Epoch: 5| Step: 3
Training loss: 2.859931230545044
Validation loss: 2.3337399062289985

Epoch: 5| Step: 4
Training loss: 2.8577640056610107
Validation loss: 2.323909354466264

Epoch: 5| Step: 5
Training loss: 2.6611335277557373
Validation loss: 2.322639221786171

Epoch: 5| Step: 6
Training loss: 1.9552602767944336
Validation loss: 2.3253904696433776

Epoch: 5| Step: 7
Training loss: 2.883795976638794
Validation loss: 2.331526821659457

Epoch: 5| Step: 8
Training loss: 2.4524412155151367
Validation loss: 2.3185648431060133

Epoch: 5| Step: 9
Training loss: 2.73809552192688
Validation loss: 2.3179088151583107

Epoch: 5| Step: 10
Training loss: 2.261910915374756
Validation loss: 2.3283726194853425

Epoch: 276| Step: 0
Training loss: 2.7573349475860596
Validation loss: 2.319660045767343

Epoch: 5| Step: 1
Training loss: 2.420065402984619
Validation loss: 2.3155989262365524

Epoch: 5| Step: 2
Training loss: 2.737128257751465
Validation loss: 2.3213655512820006

Epoch: 5| Step: 3
Training loss: 2.404606342315674
Validation loss: 2.3182956428938013

Epoch: 5| Step: 4
Training loss: 2.12567138671875
Validation loss: 2.318470275530251

Epoch: 5| Step: 5
Training loss: 2.5551645755767822
Validation loss: 2.327916617034584

Epoch: 5| Step: 6
Training loss: 3.0709125995635986
Validation loss: 2.3360438167407946

Epoch: 5| Step: 7
Training loss: 2.530749797821045
Validation loss: 2.3196873459764706

Epoch: 5| Step: 8
Training loss: 2.9557254314422607
Validation loss: 2.3285378922698317

Epoch: 5| Step: 9
Training loss: 1.8542444705963135
Validation loss: 2.3514999740867206

Epoch: 5| Step: 10
Training loss: 2.294299840927124
Validation loss: 2.34395033057018

Epoch: 277| Step: 0
Training loss: 2.313974618911743
Validation loss: 2.354731993008685

Epoch: 5| Step: 1
Training loss: 2.4773287773132324
Validation loss: 2.3313400899210284

Epoch: 5| Step: 2
Training loss: 2.5847408771514893
Validation loss: 2.3201225342289096

Epoch: 5| Step: 3
Training loss: 2.206920862197876
Validation loss: 2.313666843598889

Epoch: 5| Step: 4
Training loss: 2.492645502090454
Validation loss: 2.3133462039373254

Epoch: 5| Step: 5
Training loss: 2.419562816619873
Validation loss: 2.299915526502876

Epoch: 5| Step: 6
Training loss: 2.4295761585235596
Validation loss: 2.3024335061350176

Epoch: 5| Step: 7
Training loss: 2.6442103385925293
Validation loss: 2.2959254069994857

Epoch: 5| Step: 8
Training loss: 2.25373911857605
Validation loss: 2.288614329471383

Epoch: 5| Step: 9
Training loss: 2.842888355255127
Validation loss: 2.293767657331241

Epoch: 5| Step: 10
Training loss: 3.1689014434814453
Validation loss: 2.2993275016866703

Epoch: 278| Step: 0
Training loss: 2.3143997192382812
Validation loss: 2.2997007318722305

Epoch: 5| Step: 1
Training loss: 2.8025991916656494
Validation loss: 2.3058480447338474

Epoch: 5| Step: 2
Training loss: 2.1871018409729004
Validation loss: 2.3118269212784304

Epoch: 5| Step: 3
Training loss: 2.611574649810791
Validation loss: 2.3144037390267975

Epoch: 5| Step: 4
Training loss: 2.724505662918091
Validation loss: 2.312177852917743

Epoch: 5| Step: 5
Training loss: 2.856797695159912
Validation loss: 2.313990577574699

Epoch: 5| Step: 6
Training loss: 2.5179598331451416
Validation loss: 2.318142701220769

Epoch: 5| Step: 7
Training loss: 3.004171133041382
Validation loss: 2.339908371689499

Epoch: 5| Step: 8
Training loss: 1.868371605873108
Validation loss: 2.3553983626827115

Epoch: 5| Step: 9
Training loss: 2.2886264324188232
Validation loss: 2.3496659135305755

Epoch: 5| Step: 10
Training loss: 2.5191171169281006
Validation loss: 2.3348224829601985

Epoch: 279| Step: 0
Training loss: 2.626413345336914
Validation loss: 2.335178698262861

Epoch: 5| Step: 1
Training loss: 2.1638472080230713
Validation loss: 2.319227680083244

Epoch: 5| Step: 2
Training loss: 2.232074499130249
Validation loss: 2.3008302565543883

Epoch: 5| Step: 3
Training loss: 2.3218204975128174
Validation loss: 2.311790499635922

Epoch: 5| Step: 4
Training loss: 2.248643398284912
Validation loss: 2.3211195366356963

Epoch: 5| Step: 5
Training loss: 3.2474093437194824
Validation loss: 2.3208724426966842

Epoch: 5| Step: 6
Training loss: 3.1785473823547363
Validation loss: 2.3374964037249164

Epoch: 5| Step: 7
Training loss: 2.1122565269470215
Validation loss: 2.321132734257688

Epoch: 5| Step: 8
Training loss: 2.230046272277832
Validation loss: 2.3338086451253583

Epoch: 5| Step: 9
Training loss: 2.634603977203369
Validation loss: 2.327602694111486

Epoch: 5| Step: 10
Training loss: 2.722719430923462
Validation loss: 2.3262322064368957

Epoch: 280| Step: 0
Training loss: 2.77402925491333
Validation loss: 2.3364467133757887

Epoch: 5| Step: 1
Training loss: 2.6177666187286377
Validation loss: 2.3256943866770756

Epoch: 5| Step: 2
Training loss: 2.0204195976257324
Validation loss: 2.336103077857725

Epoch: 5| Step: 3
Training loss: 2.7871029376983643
Validation loss: 2.339568361159294

Epoch: 5| Step: 4
Training loss: 2.3258161544799805
Validation loss: 2.349342971719721

Epoch: 5| Step: 5
Training loss: 1.9675254821777344
Validation loss: 2.349434626999722

Epoch: 5| Step: 6
Training loss: 2.113662004470825
Validation loss: 2.343208255306367

Epoch: 5| Step: 7
Training loss: 2.478691577911377
Validation loss: 2.349391307882083

Epoch: 5| Step: 8
Training loss: 3.482325792312622
Validation loss: 2.347270078556512

Epoch: 5| Step: 9
Training loss: 2.7129454612731934
Validation loss: 2.335833236735354

Epoch: 5| Step: 10
Training loss: 2.464759111404419
Validation loss: 2.3295964733246834

Epoch: 281| Step: 0
Training loss: 2.758939504623413
Validation loss: 2.3399684070259013

Epoch: 5| Step: 1
Training loss: 2.4609501361846924
Validation loss: 2.327871109849663

Epoch: 5| Step: 2
Training loss: 2.6894898414611816
Validation loss: 2.324931170350762

Epoch: 5| Step: 3
Training loss: 2.117481231689453
Validation loss: 2.331826932968632

Epoch: 5| Step: 4
Training loss: 2.6337826251983643
Validation loss: 2.32606344069204

Epoch: 5| Step: 5
Training loss: 2.872011661529541
Validation loss: 2.33507941871561

Epoch: 5| Step: 6
Training loss: 2.6958024501800537
Validation loss: 2.3305182046787714

Epoch: 5| Step: 7
Training loss: 2.678962230682373
Validation loss: 2.3176643745873564

Epoch: 5| Step: 8
Training loss: 2.8984389305114746
Validation loss: 2.319739298153949

Epoch: 5| Step: 9
Training loss: 2.3646292686462402
Validation loss: 2.3204618884671118

Epoch: 5| Step: 10
Training loss: 1.2710978984832764
Validation loss: 2.316439646546559

Epoch: 282| Step: 0
Training loss: 2.523375988006592
Validation loss: 2.311806058370939

Epoch: 5| Step: 1
Training loss: 2.8097691535949707
Validation loss: 2.3067996501922607

Epoch: 5| Step: 2
Training loss: 2.6708669662475586
Validation loss: 2.3098698815991803

Epoch: 5| Step: 3
Training loss: 2.743908405303955
Validation loss: 2.327339956837316

Epoch: 5| Step: 4
Training loss: 2.589141368865967
Validation loss: 2.3367963939584713

Epoch: 5| Step: 5
Training loss: 2.501082181930542
Validation loss: 2.343875897827969

Epoch: 5| Step: 6
Training loss: 2.64433217048645
Validation loss: 2.3277167607379217

Epoch: 5| Step: 7
Training loss: 2.6518192291259766
Validation loss: 2.3245417507745887

Epoch: 5| Step: 8
Training loss: 2.3127808570861816
Validation loss: 2.3442213484036025

Epoch: 5| Step: 9
Training loss: 2.609732151031494
Validation loss: 2.339826747935305

Epoch: 5| Step: 10
Training loss: 1.5193378925323486
Validation loss: 2.314528908780826

Epoch: 283| Step: 0
Training loss: 2.558891534805298
Validation loss: 2.308956846114128

Epoch: 5| Step: 1
Training loss: 2.3957979679107666
Validation loss: 2.3038472949817614

Epoch: 5| Step: 2
Training loss: 2.2991089820861816
Validation loss: 2.2955249996595484

Epoch: 5| Step: 3
Training loss: 2.216132640838623
Validation loss: 2.2956260276097122

Epoch: 5| Step: 4
Training loss: 1.9247798919677734
Validation loss: 2.295219272695562

Epoch: 5| Step: 5
Training loss: 2.6356301307678223
Validation loss: 2.3086922835278254

Epoch: 5| Step: 6
Training loss: 2.451411008834839
Validation loss: 2.299555565721245

Epoch: 5| Step: 7
Training loss: 2.9671812057495117
Validation loss: 2.3053517444159395

Epoch: 5| Step: 8
Training loss: 3.4328536987304688
Validation loss: 2.3147732621879986

Epoch: 5| Step: 9
Training loss: 2.7299153804779053
Validation loss: 2.3119070247937272

Epoch: 5| Step: 10
Training loss: 2.1838083267211914
Validation loss: 2.3044466587804977

Epoch: 284| Step: 0
Training loss: 2.801717519760132
Validation loss: 2.2983594991827525

Epoch: 5| Step: 1
Training loss: 2.4513416290283203
Validation loss: 2.302037295474801

Epoch: 5| Step: 2
Training loss: 2.6850249767303467
Validation loss: 2.3112605156437045

Epoch: 5| Step: 3
Training loss: 2.7643446922302246
Validation loss: 2.342788298924764

Epoch: 5| Step: 4
Training loss: 1.9317047595977783
Validation loss: 2.354520187583021

Epoch: 5| Step: 5
Training loss: 2.3653645515441895
Validation loss: 2.363749950162826

Epoch: 5| Step: 6
Training loss: 2.181577205657959
Validation loss: 2.3571487190902873

Epoch: 5| Step: 7
Training loss: 2.5197930335998535
Validation loss: 2.36088498561613

Epoch: 5| Step: 8
Training loss: 2.8587613105773926
Validation loss: 2.3319363363327517

Epoch: 5| Step: 9
Training loss: 2.44508695602417
Validation loss: 2.3268145412527104

Epoch: 5| Step: 10
Training loss: 2.842641592025757
Validation loss: 2.330118174194008

Epoch: 285| Step: 0
Training loss: 2.1627020835876465
Validation loss: 2.335357548088156

Epoch: 5| Step: 1
Training loss: 2.862766742706299
Validation loss: 2.340784667640604

Epoch: 5| Step: 2
Training loss: 1.9555981159210205
Validation loss: 2.3324519459919264

Epoch: 5| Step: 3
Training loss: 2.7796738147735596
Validation loss: 2.334842489611718

Epoch: 5| Step: 4
Training loss: 2.5388705730438232
Validation loss: 2.3409257473484164

Epoch: 5| Step: 5
Training loss: 2.5734143257141113
Validation loss: 2.335069711490344

Epoch: 5| Step: 6
Training loss: 1.7786400318145752
Validation loss: 2.336936225173294

Epoch: 5| Step: 7
Training loss: 2.1782851219177246
Validation loss: 2.3409452925446215

Epoch: 5| Step: 8
Training loss: 3.248662233352661
Validation loss: 2.3375590488474858

Epoch: 5| Step: 9
Training loss: 2.5774099826812744
Validation loss: 2.3474496449193647

Epoch: 5| Step: 10
Training loss: 2.9794764518737793
Validation loss: 2.3453767350924912

Epoch: 286| Step: 0
Training loss: 2.214017868041992
Validation loss: 2.33691422400936

Epoch: 5| Step: 1
Training loss: 2.336761951446533
Validation loss: 2.3421617528443694

Epoch: 5| Step: 2
Training loss: 2.864722967147827
Validation loss: 2.329660851468322

Epoch: 5| Step: 3
Training loss: 2.798429250717163
Validation loss: 2.3174044342451197

Epoch: 5| Step: 4
Training loss: 2.1251790523529053
Validation loss: 2.3225961654416976

Epoch: 5| Step: 5
Training loss: 2.5861618518829346
Validation loss: 2.312607939525317

Epoch: 5| Step: 6
Training loss: 2.317457675933838
Validation loss: 2.306877605376705

Epoch: 5| Step: 7
Training loss: 2.418078899383545
Validation loss: 2.2962579188808316

Epoch: 5| Step: 8
Training loss: 2.4946582317352295
Validation loss: 2.3016507676852647

Epoch: 5| Step: 9
Training loss: 2.2847678661346436
Validation loss: 2.301375114789573

Epoch: 5| Step: 10
Training loss: 3.0778470039367676
Validation loss: 2.313587716830674

Epoch: 287| Step: 0
Training loss: 2.70631742477417
Validation loss: 2.3100706428609867

Epoch: 5| Step: 1
Training loss: 2.5008604526519775
Validation loss: 2.3087466493729623

Epoch: 5| Step: 2
Training loss: 2.591693639755249
Validation loss: 2.317161703622469

Epoch: 5| Step: 3
Training loss: 1.8712761402130127
Validation loss: 2.3096795248728927

Epoch: 5| Step: 4
Training loss: 2.934110164642334
Validation loss: 2.343474436831731

Epoch: 5| Step: 5
Training loss: 2.5630242824554443
Validation loss: 2.3434629004488707

Epoch: 5| Step: 6
Training loss: 3.0751636028289795
Validation loss: 2.36073701612411

Epoch: 5| Step: 7
Training loss: 2.4782662391662598
Validation loss: 2.385075174352174

Epoch: 5| Step: 8
Training loss: 2.2002460956573486
Validation loss: 2.393843532890402

Epoch: 5| Step: 9
Training loss: 2.0792222023010254
Validation loss: 2.414248933074295

Epoch: 5| Step: 10
Training loss: 2.5967092514038086
Validation loss: 2.4182257421555056

Epoch: 288| Step: 0
Training loss: 2.5610973834991455
Validation loss: 2.392083353893731

Epoch: 5| Step: 1
Training loss: 2.5968258380889893
Validation loss: 2.3439783691078104

Epoch: 5| Step: 2
Training loss: 2.951946258544922
Validation loss: 2.3240280946095786

Epoch: 5| Step: 3
Training loss: 2.2395269870758057
Validation loss: 2.305465712342211

Epoch: 5| Step: 4
Training loss: 2.7533528804779053
Validation loss: 2.3006069378186296

Epoch: 5| Step: 5
Training loss: 3.187507152557373
Validation loss: 2.3111815811485372

Epoch: 5| Step: 6
Training loss: 2.534026622772217
Validation loss: 2.2993151052023775

Epoch: 5| Step: 7
Training loss: 2.46733021736145
Validation loss: 2.3041745116633754

Epoch: 5| Step: 8
Training loss: 2.328305721282959
Validation loss: 2.3005549420592604

Epoch: 5| Step: 9
Training loss: 1.7975019216537476
Validation loss: 2.287842389075987

Epoch: 5| Step: 10
Training loss: 2.2057998180389404
Validation loss: 2.305180239421065

Epoch: 289| Step: 0
Training loss: 2.1341214179992676
Validation loss: 2.3000204306776806

Epoch: 5| Step: 1
Training loss: 2.6048195362091064
Validation loss: 2.2991741677766204

Epoch: 5| Step: 2
Training loss: 2.4631025791168213
Validation loss: 2.3000675683380454

Epoch: 5| Step: 3
Training loss: 2.3443427085876465
Validation loss: 2.3021674976553967

Epoch: 5| Step: 4
Training loss: 2.862487316131592
Validation loss: 2.2940385008371003

Epoch: 5| Step: 5
Training loss: 2.6548683643341064
Validation loss: 2.29622305849547

Epoch: 5| Step: 6
Training loss: 1.8939098119735718
Validation loss: 2.2935041663467244

Epoch: 5| Step: 7
Training loss: 2.7355191707611084
Validation loss: 2.3100324599973616

Epoch: 5| Step: 8
Training loss: 2.8036396503448486
Validation loss: 2.3020290508065173

Epoch: 5| Step: 9
Training loss: 2.3236136436462402
Validation loss: 2.3085893251562632

Epoch: 5| Step: 10
Training loss: 2.4897518157958984
Validation loss: 2.3212213003507225

Epoch: 290| Step: 0
Training loss: 2.221266031265259
Validation loss: 2.353724894985076

Epoch: 5| Step: 1
Training loss: 2.5436666011810303
Validation loss: 2.3587249350804154

Epoch: 5| Step: 2
Training loss: 2.908329486846924
Validation loss: 2.354120695462791

Epoch: 5| Step: 3
Training loss: 2.930286407470703
Validation loss: 2.3415198377383653

Epoch: 5| Step: 4
Training loss: 2.294023275375366
Validation loss: 2.3228381987540954

Epoch: 5| Step: 5
Training loss: 3.060394763946533
Validation loss: 2.2986834100497666

Epoch: 5| Step: 6
Training loss: 2.767221450805664
Validation loss: 2.2996621465170257

Epoch: 5| Step: 7
Training loss: 2.2909865379333496
Validation loss: 2.3110418473520586

Epoch: 5| Step: 8
Training loss: 1.7985146045684814
Validation loss: 2.3046261802796395

Epoch: 5| Step: 9
Training loss: 2.7643489837646484
Validation loss: 2.309411182198473

Epoch: 5| Step: 10
Training loss: 1.849426031112671
Validation loss: 2.3032209334834928

Epoch: 291| Step: 0
Training loss: 2.6278133392333984
Validation loss: 2.316296708199286

Epoch: 5| Step: 1
Training loss: 2.4150333404541016
Validation loss: 2.314589833700529

Epoch: 5| Step: 2
Training loss: 2.2866504192352295
Validation loss: 2.3175535227662776

Epoch: 5| Step: 3
Training loss: 1.945265531539917
Validation loss: 2.322522618437326

Epoch: 5| Step: 4
Training loss: 2.378697633743286
Validation loss: 2.325474525010714

Epoch: 5| Step: 5
Training loss: 2.1752676963806152
Validation loss: 2.3053045452281995

Epoch: 5| Step: 6
Training loss: 2.5616579055786133
Validation loss: 2.3199652189849527

Epoch: 5| Step: 7
Training loss: 3.1480324268341064
Validation loss: 2.3397724897630754

Epoch: 5| Step: 8
Training loss: 2.3285374641418457
Validation loss: 2.3371596515819593

Epoch: 5| Step: 9
Training loss: 2.9752273559570312
Validation loss: 2.332677815550117

Epoch: 5| Step: 10
Training loss: 2.712233304977417
Validation loss: 2.342205430871697

Epoch: 292| Step: 0
Training loss: 1.8802273273468018
Validation loss: 2.328935830823837

Epoch: 5| Step: 1
Training loss: 2.2726857662200928
Validation loss: 2.3179800228406022

Epoch: 5| Step: 2
Training loss: 2.2153868675231934
Validation loss: 2.3160245187820925

Epoch: 5| Step: 3
Training loss: 2.7210240364074707
Validation loss: 2.315203474413964

Epoch: 5| Step: 4
Training loss: 3.516705274581909
Validation loss: 2.306381602441111

Epoch: 5| Step: 5
Training loss: 2.5466103553771973
Validation loss: 2.300700823465983

Epoch: 5| Step: 6
Training loss: 2.36564564704895
Validation loss: 2.296443849481562

Epoch: 5| Step: 7
Training loss: 2.6038460731506348
Validation loss: 2.2829743559642504

Epoch: 5| Step: 8
Training loss: 2.2779765129089355
Validation loss: 2.2842314063861804

Epoch: 5| Step: 9
Training loss: 1.8626296520233154
Validation loss: 2.2828681110053934

Epoch: 5| Step: 10
Training loss: 3.3313310146331787
Validation loss: 2.2894168848632486

Epoch: 293| Step: 0
Training loss: 2.689493417739868
Validation loss: 2.2974380703382593

Epoch: 5| Step: 1
Training loss: 2.109421968460083
Validation loss: 2.304628095319194

Epoch: 5| Step: 2
Training loss: 2.4410908222198486
Validation loss: 2.30601679894232

Epoch: 5| Step: 3
Training loss: 2.1094508171081543
Validation loss: 2.311798986568246

Epoch: 5| Step: 4
Training loss: 2.8517279624938965
Validation loss: 2.315669044371574

Epoch: 5| Step: 5
Training loss: 3.051438331604004
Validation loss: 2.3294320016778927

Epoch: 5| Step: 6
Training loss: 2.2079243659973145
Validation loss: 2.3211413711629887

Epoch: 5| Step: 7
Training loss: 2.25235652923584
Validation loss: 2.310768924733644

Epoch: 5| Step: 8
Training loss: 2.5546915531158447
Validation loss: 2.3068630618433796

Epoch: 5| Step: 9
Training loss: 2.645547389984131
Validation loss: 2.293562860899074

Epoch: 5| Step: 10
Training loss: 2.422107696533203
Validation loss: 2.2828109648919876

Epoch: 294| Step: 0
Training loss: 2.283179759979248
Validation loss: 2.2805501953248055

Epoch: 5| Step: 1
Training loss: 2.7526581287384033
Validation loss: 2.281604923227782

Epoch: 5| Step: 2
Training loss: 2.4961020946502686
Validation loss: 2.2921891520100255

Epoch: 5| Step: 3
Training loss: 2.173827648162842
Validation loss: 2.3163552258604314

Epoch: 5| Step: 4
Training loss: 2.569277286529541
Validation loss: 2.3181445239692606

Epoch: 5| Step: 5
Training loss: 2.613870620727539
Validation loss: 2.3216290012482674

Epoch: 5| Step: 6
Training loss: 2.248826503753662
Validation loss: 2.315845807393392

Epoch: 5| Step: 7
Training loss: 2.3713886737823486
Validation loss: 2.309980120710147

Epoch: 5| Step: 8
Training loss: 2.1403753757476807
Validation loss: 2.318586762233447

Epoch: 5| Step: 9
Training loss: 3.2561230659484863
Validation loss: 2.314506948635142

Epoch: 5| Step: 10
Training loss: 2.6940255165100098
Validation loss: 2.3137848505409817

Epoch: 295| Step: 0
Training loss: 2.037247896194458
Validation loss: 2.3057177259076025

Epoch: 5| Step: 1
Training loss: 2.359687328338623
Validation loss: 2.291784446726563

Epoch: 5| Step: 2
Training loss: 2.298471689224243
Validation loss: 2.28503494621605

Epoch: 5| Step: 3
Training loss: 2.19963002204895
Validation loss: 2.299334861898935

Epoch: 5| Step: 4
Training loss: 2.692919969558716
Validation loss: 2.306151529794098

Epoch: 5| Step: 5
Training loss: 2.7571144104003906
Validation loss: 2.293482841983918

Epoch: 5| Step: 6
Training loss: 2.764460325241089
Validation loss: 2.305688414522397

Epoch: 5| Step: 7
Training loss: 2.7816128730773926
Validation loss: 2.2868621221152683

Epoch: 5| Step: 8
Training loss: 2.065423011779785
Validation loss: 2.2972941167892946

Epoch: 5| Step: 9
Training loss: 2.88897967338562
Validation loss: 2.311717535859795

Epoch: 5| Step: 10
Training loss: 2.389488697052002
Validation loss: 2.3360398994979037

Epoch: 296| Step: 0
Training loss: 2.475172758102417
Validation loss: 2.3428008223092682

Epoch: 5| Step: 1
Training loss: 2.5588722229003906
Validation loss: 2.360873668424545

Epoch: 5| Step: 2
Training loss: 2.5095229148864746
Validation loss: 2.350552135898221

Epoch: 5| Step: 3
Training loss: 3.3877952098846436
Validation loss: 2.3467814896696355

Epoch: 5| Step: 4
Training loss: 2.290098190307617
Validation loss: 2.3356761650372575

Epoch: 5| Step: 5
Training loss: 2.184037208557129
Validation loss: 2.3205681385532504

Epoch: 5| Step: 6
Training loss: 1.9886080026626587
Validation loss: 2.3070841912300355

Epoch: 5| Step: 7
Training loss: 2.6617987155914307
Validation loss: 2.308006384039438

Epoch: 5| Step: 8
Training loss: 2.414008617401123
Validation loss: 2.305738397823867

Epoch: 5| Step: 9
Training loss: 2.7964282035827637
Validation loss: 2.308811861981628

Epoch: 5| Step: 10
Training loss: 2.1953067779541016
Validation loss: 2.3108304905635055

Epoch: 297| Step: 0
Training loss: 3.101430654525757
Validation loss: 2.3183977655185166

Epoch: 5| Step: 1
Training loss: 2.49394154548645
Validation loss: 2.309436659659109

Epoch: 5| Step: 2
Training loss: 1.9755964279174805
Validation loss: 2.300542813475414

Epoch: 5| Step: 3
Training loss: 1.8538997173309326
Validation loss: 2.304336696542719

Epoch: 5| Step: 4
Training loss: 3.0160069465637207
Validation loss: 2.322096813109613

Epoch: 5| Step: 5
Training loss: 2.334827423095703
Validation loss: 2.3168329526019353

Epoch: 5| Step: 6
Training loss: 2.293480157852173
Validation loss: 2.3087282949878323

Epoch: 5| Step: 7
Training loss: 2.19504976272583
Validation loss: 2.299132885471467

Epoch: 5| Step: 8
Training loss: 3.052746534347534
Validation loss: 2.2988089053861556

Epoch: 5| Step: 9
Training loss: 2.022380828857422
Validation loss: 2.294197492702033

Epoch: 5| Step: 10
Training loss: 3.0011515617370605
Validation loss: 2.300888938288535

Epoch: 298| Step: 0
Training loss: 2.4583988189697266
Validation loss: 2.293674402339484

Epoch: 5| Step: 1
Training loss: 2.3869690895080566
Validation loss: 2.3067121659555743

Epoch: 5| Step: 2
Training loss: 2.462825298309326
Validation loss: 2.3042075864730345

Epoch: 5| Step: 3
Training loss: 2.70835018157959
Validation loss: 2.2950303323807253

Epoch: 5| Step: 4
Training loss: 2.416985034942627
Validation loss: 2.3120543931120183

Epoch: 5| Step: 5
Training loss: 1.8783289194107056
Validation loss: 2.28692687455044

Epoch: 5| Step: 6
Training loss: 2.6876494884490967
Validation loss: 2.2929167004041773

Epoch: 5| Step: 7
Training loss: 2.5180697441101074
Validation loss: 2.2905258414565877

Epoch: 5| Step: 8
Training loss: 2.6491169929504395
Validation loss: 2.298541158758184

Epoch: 5| Step: 9
Training loss: 2.1556992530822754
Validation loss: 2.2750656553494033

Epoch: 5| Step: 10
Training loss: 2.931171178817749
Validation loss: 2.2762014430056334

Epoch: 299| Step: 0
Training loss: 2.637101650238037
Validation loss: 2.279519340043427

Epoch: 5| Step: 1
Training loss: 2.568450450897217
Validation loss: 2.276722358119103

Epoch: 5| Step: 2
Training loss: 2.4106717109680176
Validation loss: 2.2768476727188274

Epoch: 5| Step: 3
Training loss: 2.8058249950408936
Validation loss: 2.2809083743761946

Epoch: 5| Step: 4
Training loss: 2.3750858306884766
Validation loss: 2.269962386418414

Epoch: 5| Step: 5
Training loss: 2.5848965644836426
Validation loss: 2.271058342790091

Epoch: 5| Step: 6
Training loss: 2.237859010696411
Validation loss: 2.276511997304937

Epoch: 5| Step: 7
Training loss: 2.7091734409332275
Validation loss: 2.270843739150673

Epoch: 5| Step: 8
Training loss: 2.590956211090088
Validation loss: 2.279842927891721

Epoch: 5| Step: 9
Training loss: 2.3494670391082764
Validation loss: 2.276490678069412

Epoch: 5| Step: 10
Training loss: 2.096956968307495
Validation loss: 2.2919900930056007

Epoch: 300| Step: 0
Training loss: 2.738046646118164
Validation loss: 2.2884754852582048

Epoch: 5| Step: 1
Training loss: 2.1528897285461426
Validation loss: 2.307140786160705

Epoch: 5| Step: 2
Training loss: 1.8377097845077515
Validation loss: 2.324480269544868

Epoch: 5| Step: 3
Training loss: 2.5847740173339844
Validation loss: 2.340785131659559

Epoch: 5| Step: 4
Training loss: 2.5180892944335938
Validation loss: 2.346056845880324

Epoch: 5| Step: 5
Training loss: 2.3351898193359375
Validation loss: 2.3496005945308234

Epoch: 5| Step: 6
Training loss: 2.694417715072632
Validation loss: 2.330976947661369

Epoch: 5| Step: 7
Training loss: 2.7073185443878174
Validation loss: 2.3184584084377495

Epoch: 5| Step: 8
Training loss: 2.250234603881836
Validation loss: 2.328236526058566

Epoch: 5| Step: 9
Training loss: 2.663477897644043
Validation loss: 2.312626423374299

Epoch: 5| Step: 10
Training loss: 2.9477956295013428
Validation loss: 2.312236962779876

Epoch: 301| Step: 0
Training loss: 2.9432950019836426
Validation loss: 2.32036986402286

Epoch: 5| Step: 1
Training loss: 2.5957093238830566
Validation loss: 2.317673260165799

Epoch: 5| Step: 2
Training loss: 2.48275089263916
Validation loss: 2.309593205810875

Epoch: 5| Step: 3
Training loss: 2.409651517868042
Validation loss: 2.335388001575265

Epoch: 5| Step: 4
Training loss: 2.3467793464660645
Validation loss: 2.316019804246964

Epoch: 5| Step: 5
Training loss: 2.0991342067718506
Validation loss: 2.3139495682972733

Epoch: 5| Step: 6
Training loss: 1.9923818111419678
Validation loss: 2.319669441510272

Epoch: 5| Step: 7
Training loss: 2.6474032402038574
Validation loss: 2.3162164765019573

Epoch: 5| Step: 8
Training loss: 2.5985589027404785
Validation loss: 2.315618391959898

Epoch: 5| Step: 9
Training loss: 2.280813694000244
Validation loss: 2.30476475274691

Epoch: 5| Step: 10
Training loss: 2.7524335384368896
Validation loss: 2.3123237035607778

Epoch: 302| Step: 0
Training loss: 2.1690919399261475
Validation loss: 2.322365571093816

Epoch: 5| Step: 1
Training loss: 2.479292631149292
Validation loss: 2.2960249121471117

Epoch: 5| Step: 2
Training loss: 2.038278341293335
Validation loss: 2.3007776019393757

Epoch: 5| Step: 3
Training loss: 3.4051578044891357
Validation loss: 2.293064614777924

Epoch: 5| Step: 4
Training loss: 2.708024024963379
Validation loss: 2.2989288940224597

Epoch: 5| Step: 5
Training loss: 2.314840316772461
Validation loss: 2.3001553486752253

Epoch: 5| Step: 6
Training loss: 2.3931736946105957
Validation loss: 2.303739301619991

Epoch: 5| Step: 7
Training loss: 2.380627393722534
Validation loss: 2.2912227543451453

Epoch: 5| Step: 8
Training loss: 2.416607141494751
Validation loss: 2.3055216189353698

Epoch: 5| Step: 9
Training loss: 2.4384565353393555
Validation loss: 2.302078068897288

Epoch: 5| Step: 10
Training loss: 2.384251832962036
Validation loss: 2.3055201320238012

Epoch: 303| Step: 0
Training loss: 1.99728262424469
Validation loss: 2.295136652966981

Epoch: 5| Step: 1
Training loss: 2.2438504695892334
Validation loss: 2.296608906920238

Epoch: 5| Step: 2
Training loss: 2.6155269145965576
Validation loss: 2.2827941192093717

Epoch: 5| Step: 3
Training loss: 2.225590229034424
Validation loss: 2.2948193524473455

Epoch: 5| Step: 4
Training loss: 2.3303563594818115
Validation loss: 2.2890668299890335

Epoch: 5| Step: 5
Training loss: 2.421436309814453
Validation loss: 2.2844912877646824

Epoch: 5| Step: 6
Training loss: 2.6508352756500244
Validation loss: 2.285315321337792

Epoch: 5| Step: 7
Training loss: 2.8326008319854736
Validation loss: 2.278628464668028

Epoch: 5| Step: 8
Training loss: 3.0440685749053955
Validation loss: 2.300245249143211

Epoch: 5| Step: 9
Training loss: 2.3883743286132812
Validation loss: 2.2903689312678512

Epoch: 5| Step: 10
Training loss: 2.4696176052093506
Validation loss: 2.2961875469453874

Epoch: 304| Step: 0
Training loss: 2.363466501235962
Validation loss: 2.3203423817952475

Epoch: 5| Step: 1
Training loss: 2.762655258178711
Validation loss: 2.306435751658614

Epoch: 5| Step: 2
Training loss: 2.4051239490509033
Validation loss: 2.303843511048184

Epoch: 5| Step: 3
Training loss: 1.628760576248169
Validation loss: 2.30620329354399

Epoch: 5| Step: 4
Training loss: 2.195474147796631
Validation loss: 2.3037400476394163

Epoch: 5| Step: 5
Training loss: 2.2986960411071777
Validation loss: 2.3056819438934326

Epoch: 5| Step: 6
Training loss: 2.946737289428711
Validation loss: 2.302614212036133

Epoch: 5| Step: 7
Training loss: 2.5632119178771973
Validation loss: 2.2898590539091375

Epoch: 5| Step: 8
Training loss: 2.4925999641418457
Validation loss: 2.2828926758099626

Epoch: 5| Step: 9
Training loss: 2.6644492149353027
Validation loss: 2.3001609412572717

Epoch: 5| Step: 10
Training loss: 2.896893262863159
Validation loss: 2.2870645946071995

Epoch: 305| Step: 0
Training loss: 1.948469877243042
Validation loss: 2.2942203462764783

Epoch: 5| Step: 1
Training loss: 2.7089343070983887
Validation loss: 2.29703777323487

Epoch: 5| Step: 2
Training loss: 2.3239874839782715
Validation loss: 2.3005011876424155

Epoch: 5| Step: 3
Training loss: 2.4038500785827637
Validation loss: 2.301959458217826

Epoch: 5| Step: 4
Training loss: 2.7143170833587646
Validation loss: 2.2908134793722503

Epoch: 5| Step: 5
Training loss: 2.4185123443603516
Validation loss: 2.29763549886724

Epoch: 5| Step: 6
Training loss: 3.189136505126953
Validation loss: 2.2917061723688597

Epoch: 5| Step: 7
Training loss: 1.962355375289917
Validation loss: 2.299602654672438

Epoch: 5| Step: 8
Training loss: 2.537360429763794
Validation loss: 2.3102959843092066

Epoch: 5| Step: 9
Training loss: 2.294950008392334
Validation loss: 2.2986601783383276

Epoch: 5| Step: 10
Training loss: 2.5119152069091797
Validation loss: 2.2874494752576275

Epoch: 306| Step: 0
Training loss: 2.155506134033203
Validation loss: 2.287873242491035

Epoch: 5| Step: 1
Training loss: 2.5554733276367188
Validation loss: 2.2984283149883313

Epoch: 5| Step: 2
Training loss: 2.6200056076049805
Validation loss: 2.2892345382321264

Epoch: 5| Step: 3
Training loss: 3.0861034393310547
Validation loss: 2.28212284785445

Epoch: 5| Step: 4
Training loss: 2.6841843128204346
Validation loss: 2.293113713623375

Epoch: 5| Step: 5
Training loss: 2.5236434936523438
Validation loss: 2.2881553455065657

Epoch: 5| Step: 6
Training loss: 2.0816144943237305
Validation loss: 2.2845418914671867

Epoch: 5| Step: 7
Training loss: 1.776391625404358
Validation loss: 2.288772436880296

Epoch: 5| Step: 8
Training loss: 2.568645715713501
Validation loss: 2.2854502201080322

Epoch: 5| Step: 9
Training loss: 2.633863687515259
Validation loss: 2.2925501946480042

Epoch: 5| Step: 10
Training loss: 2.3787460327148438
Validation loss: 2.295089796025266

Epoch: 307| Step: 0
Training loss: 2.350567579269409
Validation loss: 2.3040660401826263

Epoch: 5| Step: 1
Training loss: 2.3012993335723877
Validation loss: 2.32109998631221

Epoch: 5| Step: 2
Training loss: 2.462155818939209
Validation loss: 2.3336190818458475

Epoch: 5| Step: 3
Training loss: 2.4021928310394287
Validation loss: 2.3077520349974274

Epoch: 5| Step: 4
Training loss: 2.9524669647216797
Validation loss: 2.2955499336283696

Epoch: 5| Step: 5
Training loss: 1.92507004737854
Validation loss: 2.295321156901698

Epoch: 5| Step: 6
Training loss: 2.01176381111145
Validation loss: 2.2748810706600064

Epoch: 5| Step: 7
Training loss: 2.719825029373169
Validation loss: 2.289556336659257

Epoch: 5| Step: 8
Training loss: 2.77703857421875
Validation loss: 2.280228994225943

Epoch: 5| Step: 9
Training loss: 2.2948858737945557
Validation loss: 2.27792352502064

Epoch: 5| Step: 10
Training loss: 2.9300968647003174
Validation loss: 2.2750097449107836

Epoch: 308| Step: 0
Training loss: 2.5803680419921875
Validation loss: 2.27817270576313

Epoch: 5| Step: 1
Training loss: 2.2082314491271973
Validation loss: 2.2838828409871748

Epoch: 5| Step: 2
Training loss: 2.65438175201416
Validation loss: 2.2884028675735637

Epoch: 5| Step: 3
Training loss: 2.3802490234375
Validation loss: 2.291064175226355

Epoch: 5| Step: 4
Training loss: 3.156705856323242
Validation loss: 2.2937556415475826

Epoch: 5| Step: 5
Training loss: 2.1216983795166016
Validation loss: 2.298973306532829

Epoch: 5| Step: 6
Training loss: 2.0850419998168945
Validation loss: 2.3103203619680097

Epoch: 5| Step: 7
Training loss: 2.1882991790771484
Validation loss: 2.3062343494866484

Epoch: 5| Step: 8
Training loss: 2.695517063140869
Validation loss: 2.3127278256159958

Epoch: 5| Step: 9
Training loss: 2.443981647491455
Validation loss: 2.3026390126956406

Epoch: 5| Step: 10
Training loss: 2.574033737182617
Validation loss: 2.3090100544755177

Epoch: 309| Step: 0
Training loss: 2.25215744972229
Validation loss: 2.315052009397937

Epoch: 5| Step: 1
Training loss: 2.9733269214630127
Validation loss: 2.3195496272015315

Epoch: 5| Step: 2
Training loss: 2.066209554672241
Validation loss: 2.2915349698835805

Epoch: 5| Step: 3
Training loss: 2.1580023765563965
Validation loss: 2.292265692064839

Epoch: 5| Step: 4
Training loss: 3.329692840576172
Validation loss: 2.2873495291638117

Epoch: 5| Step: 5
Training loss: 2.55147123336792
Validation loss: 2.290364942243022

Epoch: 5| Step: 6
Training loss: 1.8126208782196045
Validation loss: 2.279405980981806

Epoch: 5| Step: 7
Training loss: 2.38981556892395
Validation loss: 2.283930173484228

Epoch: 5| Step: 8
Training loss: 2.4083638191223145
Validation loss: 2.2877909098902056

Epoch: 5| Step: 9
Training loss: 2.8160319328308105
Validation loss: 2.297622408918155

Epoch: 5| Step: 10
Training loss: 2.0949370861053467
Validation loss: 2.294544594262236

Epoch: 310| Step: 0
Training loss: 2.9052741527557373
Validation loss: 2.288313647752167

Epoch: 5| Step: 1
Training loss: 2.639237880706787
Validation loss: 2.3114405267982074

Epoch: 5| Step: 2
Training loss: 2.810694694519043
Validation loss: 2.310833623332362

Epoch: 5| Step: 3
Training loss: 1.9195928573608398
Validation loss: 2.3381756556931363

Epoch: 5| Step: 4
Training loss: 2.4436986446380615
Validation loss: 2.3361642322232647

Epoch: 5| Step: 5
Training loss: 3.0493431091308594
Validation loss: 2.310403577743038

Epoch: 5| Step: 6
Training loss: 2.142146348953247
Validation loss: 2.298365826247841

Epoch: 5| Step: 7
Training loss: 2.328939199447632
Validation loss: 2.289058405865905

Epoch: 5| Step: 8
Training loss: 2.9686031341552734
Validation loss: 2.2950252204812984

Epoch: 5| Step: 9
Training loss: 2.4426703453063965
Validation loss: 2.2776302983683925

Epoch: 5| Step: 10
Training loss: 1.293628215789795
Validation loss: 2.278548025315808

Epoch: 311| Step: 0
Training loss: 2.3041434288024902
Validation loss: 2.273197825236987

Epoch: 5| Step: 1
Training loss: 2.8942532539367676
Validation loss: 2.2666518970202376

Epoch: 5| Step: 2
Training loss: 2.14617919921875
Validation loss: 2.2732363926467074

Epoch: 5| Step: 3
Training loss: 2.448258638381958
Validation loss: 2.2728583863986436

Epoch: 5| Step: 4
Training loss: 1.8412030935287476
Validation loss: 2.268557648504934

Epoch: 5| Step: 5
Training loss: 3.1288185119628906
Validation loss: 2.276927141733067

Epoch: 5| Step: 6
Training loss: 2.5918688774108887
Validation loss: 2.297942325633059

Epoch: 5| Step: 7
Training loss: 1.7087905406951904
Validation loss: 2.272304742567001

Epoch: 5| Step: 8
Training loss: 2.663482666015625
Validation loss: 2.278316455502664

Epoch: 5| Step: 9
Training loss: 2.9197940826416016
Validation loss: 2.2737762774190595

Epoch: 5| Step: 10
Training loss: 2.499922275543213
Validation loss: 2.2757600327973724

Epoch: 312| Step: 0
Training loss: 2.738375186920166
Validation loss: 2.2867295588216474

Epoch: 5| Step: 1
Training loss: 2.7222187519073486
Validation loss: 2.2739177750002955

Epoch: 5| Step: 2
Training loss: 1.7594444751739502
Validation loss: 2.271516038525489

Epoch: 5| Step: 3
Training loss: 2.4689745903015137
Validation loss: 2.2778189541191183

Epoch: 5| Step: 4
Training loss: 2.507633686065674
Validation loss: 2.2951638954941944

Epoch: 5| Step: 5
Training loss: 3.1271157264709473
Validation loss: 2.2922661945384037

Epoch: 5| Step: 6
Training loss: 2.90527081489563
Validation loss: 2.2822704827913673

Epoch: 5| Step: 7
Training loss: 1.9755504131317139
Validation loss: 2.278125539902718

Epoch: 5| Step: 8
Training loss: 2.155991315841675
Validation loss: 2.272970320076071

Epoch: 5| Step: 9
Training loss: 2.386070728302002
Validation loss: 2.276396628349058

Epoch: 5| Step: 10
Training loss: 2.1881046295166016
Validation loss: 2.277930759614514

Epoch: 313| Step: 0
Training loss: 2.5708677768707275
Validation loss: 2.2615328373447543

Epoch: 5| Step: 1
Training loss: 2.421454668045044
Validation loss: 2.270875859004195

Epoch: 5| Step: 2
Training loss: 2.680229663848877
Validation loss: 2.2757184864372335

Epoch: 5| Step: 3
Training loss: 2.528197765350342
Validation loss: 2.276678721110026

Epoch: 5| Step: 4
Training loss: 2.6717677116394043
Validation loss: 2.275000633731965

Epoch: 5| Step: 5
Training loss: 2.290558338165283
Validation loss: 2.2547660168781074

Epoch: 5| Step: 6
Training loss: 1.9768222570419312
Validation loss: 2.267469358700578

Epoch: 5| Step: 7
Training loss: 2.4246909618377686
Validation loss: 2.2844986402860252

Epoch: 5| Step: 8
Training loss: 2.4717533588409424
Validation loss: 2.3103635208581084

Epoch: 5| Step: 9
Training loss: 2.5224196910858154
Validation loss: 2.3207436287274925

Epoch: 5| Step: 10
Training loss: 2.466407537460327
Validation loss: 2.3282527692856325

Epoch: 314| Step: 0
Training loss: 2.4137930870056152
Validation loss: 2.3037796674236173

Epoch: 5| Step: 1
Training loss: 2.171583652496338
Validation loss: 2.272263075715752

Epoch: 5| Step: 2
Training loss: 2.3451828956604004
Validation loss: 2.2924389569990096

Epoch: 5| Step: 3
Training loss: 2.4304919242858887
Validation loss: 2.2827177945003716

Epoch: 5| Step: 4
Training loss: 2.022040843963623
Validation loss: 2.268485838367093

Epoch: 5| Step: 5
Training loss: 2.6882405281066895
Validation loss: 2.281116044649514

Epoch: 5| Step: 6
Training loss: 2.9986538887023926
Validation loss: 2.273330537221765

Epoch: 5| Step: 7
Training loss: 2.4156997203826904
Validation loss: 2.2646336427298923

Epoch: 5| Step: 8
Training loss: 2.7226779460906982
Validation loss: 2.2723134615088023

Epoch: 5| Step: 9
Training loss: 2.800210475921631
Validation loss: 2.2704693066176547

Epoch: 5| Step: 10
Training loss: 1.7904999256134033
Validation loss: 2.2620094335207375

Epoch: 315| Step: 0
Training loss: 2.5636870861053467
Validation loss: 2.273531298483572

Epoch: 5| Step: 1
Training loss: 2.798220157623291
Validation loss: 2.256939093271891

Epoch: 5| Step: 2
Training loss: 2.9075372219085693
Validation loss: 2.261621036837178

Epoch: 5| Step: 3
Training loss: 1.8509738445281982
Validation loss: 2.26394139054001

Epoch: 5| Step: 4
Training loss: 3.2633724212646484
Validation loss: 2.2704315826457035

Epoch: 5| Step: 5
Training loss: 2.2269608974456787
Validation loss: 2.3000233519461846

Epoch: 5| Step: 6
Training loss: 2.1281611919403076
Validation loss: 2.291887434579993

Epoch: 5| Step: 7
Training loss: 1.9307804107666016
Validation loss: 2.306255841767916

Epoch: 5| Step: 8
Training loss: 2.4897475242614746
Validation loss: 2.279457535794986

Epoch: 5| Step: 9
Training loss: 2.336747884750366
Validation loss: 2.276051732801622

Epoch: 5| Step: 10
Training loss: 2.3890321254730225
Validation loss: 2.2664224255469536

Epoch: 316| Step: 0
Training loss: 2.6287288665771484
Validation loss: 2.2645334812902633

Epoch: 5| Step: 1
Training loss: 3.0853724479675293
Validation loss: 2.2484108171155377

Epoch: 5| Step: 2
Training loss: 2.468055486679077
Validation loss: 2.239537207029199

Epoch: 5| Step: 3
Training loss: 1.7831077575683594
Validation loss: 2.2381181281100035

Epoch: 5| Step: 4
Training loss: 2.4233593940734863
Validation loss: 2.242302317773142

Epoch: 5| Step: 5
Training loss: 2.3501009941101074
Validation loss: 2.241211350246142

Epoch: 5| Step: 6
Training loss: 2.7364354133605957
Validation loss: 2.2652687513700096

Epoch: 5| Step: 7
Training loss: 2.5674331188201904
Validation loss: 2.2531061069939726

Epoch: 5| Step: 8
Training loss: 2.491086006164551
Validation loss: 2.246672066309119

Epoch: 5| Step: 9
Training loss: 2.2361834049224854
Validation loss: 2.2499928423153457

Epoch: 5| Step: 10
Training loss: 2.6759440898895264
Validation loss: 2.2371545171224945

Epoch: 317| Step: 0
Training loss: 2.5841829776763916
Validation loss: 2.252447728187807

Epoch: 5| Step: 1
Training loss: 1.9928123950958252
Validation loss: 2.2459151001386743

Epoch: 5| Step: 2
Training loss: 2.767423391342163
Validation loss: 2.2467207600993495

Epoch: 5| Step: 3
Training loss: 2.700665235519409
Validation loss: 2.2585853940697125

Epoch: 5| Step: 4
Training loss: 2.167499303817749
Validation loss: 2.249764191207065

Epoch: 5| Step: 5
Training loss: 1.647251844406128
Validation loss: 2.27566094808681

Epoch: 5| Step: 6
Training loss: 2.476146697998047
Validation loss: 2.2707821656298894

Epoch: 5| Step: 7
Training loss: 2.55485463142395
Validation loss: 2.2813101994094027

Epoch: 5| Step: 8
Training loss: 2.3312933444976807
Validation loss: 2.2641504477429133

Epoch: 5| Step: 9
Training loss: 3.103170394897461
Validation loss: 2.2762628396352134

Epoch: 5| Step: 10
Training loss: 2.7995734214782715
Validation loss: 2.256322996590727

Epoch: 318| Step: 0
Training loss: 2.7145674228668213
Validation loss: 2.2795271976019746

Epoch: 5| Step: 1
Training loss: 2.4999682903289795
Validation loss: 2.2832718151871876

Epoch: 5| Step: 2
Training loss: 2.042450428009033
Validation loss: 2.2736872626889135

Epoch: 5| Step: 3
Training loss: 2.349249839782715
Validation loss: 2.2853955722624257

Epoch: 5| Step: 4
Training loss: 2.89131760597229
Validation loss: 2.2888183465567966

Epoch: 5| Step: 5
Training loss: 2.3533029556274414
Validation loss: 2.284366589720531

Epoch: 5| Step: 6
Training loss: 2.6840317249298096
Validation loss: 2.2784456924725602

Epoch: 5| Step: 7
Training loss: 2.395530939102173
Validation loss: 2.288550220510011

Epoch: 5| Step: 8
Training loss: 1.7277692556381226
Validation loss: 2.2733581553223314

Epoch: 5| Step: 9
Training loss: 2.712634801864624
Validation loss: 2.2694806975703083

Epoch: 5| Step: 10
Training loss: 2.5348904132843018
Validation loss: 2.277003190850699

Epoch: 319| Step: 0
Training loss: 2.0421271324157715
Validation loss: 2.281143972950597

Epoch: 5| Step: 1
Training loss: 2.30841326713562
Validation loss: 2.2721324248980452

Epoch: 5| Step: 2
Training loss: 2.753328800201416
Validation loss: 2.2685589944162676

Epoch: 5| Step: 3
Training loss: 2.356204032897949
Validation loss: 2.2762533823649087

Epoch: 5| Step: 4
Training loss: 2.3985812664031982
Validation loss: 2.2631298162603892

Epoch: 5| Step: 5
Training loss: 2.4606564044952393
Validation loss: 2.261248750071372

Epoch: 5| Step: 6
Training loss: 2.332209348678589
Validation loss: 2.274571088052565

Epoch: 5| Step: 7
Training loss: 2.240478515625
Validation loss: 2.2663823558438208

Epoch: 5| Step: 8
Training loss: 3.4046318531036377
Validation loss: 2.262709638123871

Epoch: 5| Step: 9
Training loss: 2.12752103805542
Validation loss: 2.255428309081703

Epoch: 5| Step: 10
Training loss: 2.318220853805542
Validation loss: 2.2586638312185965

Epoch: 320| Step: 0
Training loss: 2.6867268085479736
Validation loss: 2.257431496856033

Epoch: 5| Step: 1
Training loss: 2.0533652305603027
Validation loss: 2.2530223118361605

Epoch: 5| Step: 2
Training loss: 2.0513699054718018
Validation loss: 2.253963901150611

Epoch: 5| Step: 3
Training loss: 2.851022243499756
Validation loss: 2.2643812164183585

Epoch: 5| Step: 4
Training loss: 2.682668685913086
Validation loss: 2.2654997046275804

Epoch: 5| Step: 5
Training loss: 2.785175323486328
Validation loss: 2.2594519661318873

Epoch: 5| Step: 6
Training loss: 1.771501898765564
Validation loss: 2.2772035329572615

Epoch: 5| Step: 7
Training loss: 2.3417484760284424
Validation loss: 2.2606471917962514

Epoch: 5| Step: 8
Training loss: 2.0374197959899902
Validation loss: 2.2620940592981156

Epoch: 5| Step: 9
Training loss: 3.0796730518341064
Validation loss: 2.2624024550120034

Epoch: 5| Step: 10
Training loss: 2.410268545150757
Validation loss: 2.2696184727453415

Epoch: 321| Step: 0
Training loss: 2.9800963401794434
Validation loss: 2.2764589889075166

Epoch: 5| Step: 1
Training loss: 1.9240443706512451
Validation loss: 2.2764004379190426

Epoch: 5| Step: 2
Training loss: 2.584197521209717
Validation loss: 2.2723006650965702

Epoch: 5| Step: 3
Training loss: 1.5417687892913818
Validation loss: 2.2748806412502

Epoch: 5| Step: 4
Training loss: 2.555905818939209
Validation loss: 2.2947804517643426

Epoch: 5| Step: 5
Training loss: 2.845726251602173
Validation loss: 2.2961194822865147

Epoch: 5| Step: 6
Training loss: 2.745840549468994
Validation loss: 2.328540578965218

Epoch: 5| Step: 7
Training loss: 2.4452595710754395
Validation loss: 2.322285020223228

Epoch: 5| Step: 8
Training loss: 2.3775722980499268
Validation loss: 2.319711272434522

Epoch: 5| Step: 9
Training loss: 2.068899631500244
Validation loss: 2.284803239248132

Epoch: 5| Step: 10
Training loss: 2.8092639446258545
Validation loss: 2.292554583600772

Epoch: 322| Step: 0
Training loss: 2.6981723308563232
Validation loss: 2.277516159960019

Epoch: 5| Step: 1
Training loss: 2.665247678756714
Validation loss: 2.2547284146790862

Epoch: 5| Step: 2
Training loss: 2.320260524749756
Validation loss: 2.2622016040227746

Epoch: 5| Step: 3
Training loss: 2.502645969390869
Validation loss: 2.2538067884342645

Epoch: 5| Step: 4
Training loss: 2.1598355770111084
Validation loss: 2.254886652833672

Epoch: 5| Step: 5
Training loss: 2.63590145111084
Validation loss: 2.2522496869487147

Epoch: 5| Step: 6
Training loss: 2.5908865928649902
Validation loss: 2.2657214287788636

Epoch: 5| Step: 7
Training loss: 2.152601957321167
Validation loss: 2.2626348849265807

Epoch: 5| Step: 8
Training loss: 2.335803508758545
Validation loss: 2.2515595164350284

Epoch: 5| Step: 9
Training loss: 2.599506378173828
Validation loss: 2.258964373219398

Epoch: 5| Step: 10
Training loss: 2.194133758544922
Validation loss: 2.2520318415857132

Epoch: 323| Step: 0
Training loss: 2.30428409576416
Validation loss: 2.262805818229593

Epoch: 5| Step: 1
Training loss: 2.724137783050537
Validation loss: 2.253492439946821

Epoch: 5| Step: 2
Training loss: 2.3349313735961914
Validation loss: 2.281973572187526

Epoch: 5| Step: 3
Training loss: 2.932499408721924
Validation loss: 2.275351321825417

Epoch: 5| Step: 4
Training loss: 2.30021071434021
Validation loss: 2.290744848148797

Epoch: 5| Step: 5
Training loss: 2.121553897857666
Validation loss: 2.278173338982367

Epoch: 5| Step: 6
Training loss: 2.0473034381866455
Validation loss: 2.3088968825596634

Epoch: 5| Step: 7
Training loss: 2.2712106704711914
Validation loss: 2.281562464211577

Epoch: 5| Step: 8
Training loss: 2.7731447219848633
Validation loss: 2.2828449587668143

Epoch: 5| Step: 9
Training loss: 2.540113687515259
Validation loss: 2.2919829263482043

Epoch: 5| Step: 10
Training loss: 2.4512665271759033
Validation loss: 2.2763986561888006

Epoch: 324| Step: 0
Training loss: 2.4165632724761963
Validation loss: 2.2577099236108924

Epoch: 5| Step: 1
Training loss: 2.5766987800598145
Validation loss: 2.2655988893201275

Epoch: 5| Step: 2
Training loss: 1.9762065410614014
Validation loss: 2.260007599348663

Epoch: 5| Step: 3
Training loss: 2.4872207641601562
Validation loss: 2.2597957605956704

Epoch: 5| Step: 4
Training loss: 2.504289150238037
Validation loss: 2.262249626139159

Epoch: 5| Step: 5
Training loss: 2.506626844406128
Validation loss: 2.255534984732187

Epoch: 5| Step: 6
Training loss: 2.633749008178711
Validation loss: 2.263469132043982

Epoch: 5| Step: 7
Training loss: 2.0080058574676514
Validation loss: 2.2517232023259646

Epoch: 5| Step: 8
Training loss: 2.6988635063171387
Validation loss: 2.251905354120398

Epoch: 5| Step: 9
Training loss: 2.136545419692993
Validation loss: 2.252876466320407

Epoch: 5| Step: 10
Training loss: 2.853489875793457
Validation loss: 2.2743651328548307

Epoch: 325| Step: 0
Training loss: 3.128410577774048
Validation loss: 2.2611431588408766

Epoch: 5| Step: 1
Training loss: 2.63862681388855
Validation loss: 2.266195063949913

Epoch: 5| Step: 2
Training loss: 2.2628796100616455
Validation loss: 2.275817753166281

Epoch: 5| Step: 3
Training loss: 2.6059188842773438
Validation loss: 2.269109219633123

Epoch: 5| Step: 4
Training loss: 2.013240098953247
Validation loss: 2.268309252236479

Epoch: 5| Step: 5
Training loss: 2.624098777770996
Validation loss: 2.2550078130537465

Epoch: 5| Step: 6
Training loss: 1.7807925939559937
Validation loss: 2.2603979546536683

Epoch: 5| Step: 7
Training loss: 2.4761996269226074
Validation loss: 2.243611789518787

Epoch: 5| Step: 8
Training loss: 2.3308959007263184
Validation loss: 2.2336589700432232

Epoch: 5| Step: 9
Training loss: 1.9726015329360962
Validation loss: 2.2480852296275478

Epoch: 5| Step: 10
Training loss: 3.134526252746582
Validation loss: 2.246167839214366

Epoch: 326| Step: 0
Training loss: 2.1230897903442383
Validation loss: 2.2456239782353884

Epoch: 5| Step: 1
Training loss: 2.2668840885162354
Validation loss: 2.253455397903278

Epoch: 5| Step: 2
Training loss: 2.9702563285827637
Validation loss: 2.261232422244164

Epoch: 5| Step: 3
Training loss: 2.7023935317993164
Validation loss: 2.269084169018653

Epoch: 5| Step: 4
Training loss: 1.617186188697815
Validation loss: 2.2640247575698362

Epoch: 5| Step: 5
Training loss: 2.585655927658081
Validation loss: 2.2648815365247827

Epoch: 5| Step: 6
Training loss: 2.7335011959075928
Validation loss: 2.255045835689832

Epoch: 5| Step: 7
Training loss: 2.2056403160095215
Validation loss: 2.2602095860306934

Epoch: 5| Step: 8
Training loss: 2.0195624828338623
Validation loss: 2.249383536718225

Epoch: 5| Step: 9
Training loss: 2.820024013519287
Validation loss: 2.2565026026900097

Epoch: 5| Step: 10
Training loss: 2.701876401901245
Validation loss: 2.247287396461733

Epoch: 327| Step: 0
Training loss: 2.28572416305542
Validation loss: 2.2439253099503054

Epoch: 5| Step: 1
Training loss: 2.5582165718078613
Validation loss: 2.2506165363455333

Epoch: 5| Step: 2
Training loss: 2.611934185028076
Validation loss: 2.247893817963139

Epoch: 5| Step: 3
Training loss: 2.3654403686523438
Validation loss: 2.2497392572382444

Epoch: 5| Step: 4
Training loss: 2.041940450668335
Validation loss: 2.2352037891264884

Epoch: 5| Step: 5
Training loss: 2.658217668533325
Validation loss: 2.263837009347895

Epoch: 5| Step: 6
Training loss: 2.028090715408325
Validation loss: 2.2637421213170534

Epoch: 5| Step: 7
Training loss: 2.9779350757598877
Validation loss: 2.2523986216514342

Epoch: 5| Step: 8
Training loss: 2.4315104484558105
Validation loss: 2.2569314972046883

Epoch: 5| Step: 9
Training loss: 2.5213623046875
Validation loss: 2.24911480821589

Epoch: 5| Step: 10
Training loss: 2.219583511352539
Validation loss: 2.24718802718706

Epoch: 328| Step: 0
Training loss: 2.735731601715088
Validation loss: 2.2349525318350842

Epoch: 5| Step: 1
Training loss: 1.8698952198028564
Validation loss: 2.2502364650849374

Epoch: 5| Step: 2
Training loss: 3.3167572021484375
Validation loss: 2.238212926413423

Epoch: 5| Step: 3
Training loss: 2.376157760620117
Validation loss: 2.2427657176089544

Epoch: 5| Step: 4
Training loss: 2.5781636238098145
Validation loss: 2.252509775982108

Epoch: 5| Step: 5
Training loss: 1.9022254943847656
Validation loss: 2.2620066827343357

Epoch: 5| Step: 6
Training loss: 2.4504315853118896
Validation loss: 2.2591344079663678

Epoch: 5| Step: 7
Training loss: 1.9153445959091187
Validation loss: 2.275758902231852

Epoch: 5| Step: 8
Training loss: 2.452059507369995
Validation loss: 2.2627523919587493

Epoch: 5| Step: 9
Training loss: 2.505544662475586
Validation loss: 2.267146025934527

Epoch: 5| Step: 10
Training loss: 2.7881782054901123
Validation loss: 2.253576858069307

Epoch: 329| Step: 0
Training loss: 2.310210704803467
Validation loss: 2.2707726852868193

Epoch: 5| Step: 1
Training loss: 2.2287561893463135
Validation loss: 2.261986470991565

Epoch: 5| Step: 2
Training loss: 2.744830369949341
Validation loss: 2.276908923220891

Epoch: 5| Step: 3
Training loss: 2.3979458808898926
Validation loss: 2.2743795533334055

Epoch: 5| Step: 4
Training loss: 2.535585403442383
Validation loss: 2.2783076327334166

Epoch: 5| Step: 5
Training loss: 2.0871331691741943
Validation loss: 2.2816358638066117

Epoch: 5| Step: 6
Training loss: 2.4814915657043457
Validation loss: 2.2700930333906606

Epoch: 5| Step: 7
Training loss: 2.185698986053467
Validation loss: 2.2541389593514065

Epoch: 5| Step: 8
Training loss: 2.699138641357422
Validation loss: 2.243779702853131

Epoch: 5| Step: 9
Training loss: 2.2965400218963623
Validation loss: 2.2449932995662896

Epoch: 5| Step: 10
Training loss: 2.829007148742676
Validation loss: 2.2412876647005797

Epoch: 330| Step: 0
Training loss: 2.644922971725464
Validation loss: 2.2229678284737373

Epoch: 5| Step: 1
Training loss: 2.4178872108459473
Validation loss: 2.2380593258847474

Epoch: 5| Step: 2
Training loss: 1.8149540424346924
Validation loss: 2.232563336690267

Epoch: 5| Step: 3
Training loss: 2.207981824874878
Validation loss: 2.227312416158697

Epoch: 5| Step: 4
Training loss: 3.0034067630767822
Validation loss: 2.2346416160624516

Epoch: 5| Step: 5
Training loss: 1.9921131134033203
Validation loss: 2.234452183528613

Epoch: 5| Step: 6
Training loss: 2.280799150466919
Validation loss: 2.2471056087042696

Epoch: 5| Step: 7
Training loss: 2.789339542388916
Validation loss: 2.2490439953342563

Epoch: 5| Step: 8
Training loss: 2.132211208343506
Validation loss: 2.2571668368513866

Epoch: 5| Step: 9
Training loss: 3.1147990226745605
Validation loss: 2.2399173590444748

Epoch: 5| Step: 10
Training loss: 2.1366677284240723
Validation loss: 2.25283121806319

Epoch: 331| Step: 0
Training loss: 1.8707002401351929
Validation loss: 2.2356684515553136

Epoch: 5| Step: 1
Training loss: 2.616091728210449
Validation loss: 2.235988396470265

Epoch: 5| Step: 2
Training loss: 2.8012969493865967
Validation loss: 2.2371512638625277

Epoch: 5| Step: 3
Training loss: 2.285435676574707
Validation loss: 2.2372262247147097

Epoch: 5| Step: 4
Training loss: 1.9192593097686768
Validation loss: 2.250788609186808

Epoch: 5| Step: 5
Training loss: 2.7395377159118652
Validation loss: 2.250769228063604

Epoch: 5| Step: 6
Training loss: 2.303675651550293
Validation loss: 2.244770775559128

Epoch: 5| Step: 7
Training loss: 2.3820722103118896
Validation loss: 2.2570704183270855

Epoch: 5| Step: 8
Training loss: 3.163637638092041
Validation loss: 2.2548544509436494

Epoch: 5| Step: 9
Training loss: 1.5772594213485718
Validation loss: 2.240587306279008

Epoch: 5| Step: 10
Training loss: 3.019500255584717
Validation loss: 2.263090507958525

Epoch: 332| Step: 0
Training loss: 1.510411262512207
Validation loss: 2.271407396562638

Epoch: 5| Step: 1
Training loss: 1.9359710216522217
Validation loss: 2.272807576323068

Epoch: 5| Step: 2
Training loss: 2.5826780796051025
Validation loss: 2.280849487550797

Epoch: 5| Step: 3
Training loss: 2.11053204536438
Validation loss: 2.2929532733014835

Epoch: 5| Step: 4
Training loss: 2.4577114582061768
Validation loss: 2.26299229873124

Epoch: 5| Step: 5
Training loss: 2.528759479522705
Validation loss: 2.2843252945971746

Epoch: 5| Step: 6
Training loss: 2.6617543697357178
Validation loss: 2.265629299225346

Epoch: 5| Step: 7
Training loss: 2.2459893226623535
Validation loss: 2.2696688893020793

Epoch: 5| Step: 8
Training loss: 3.2655587196350098
Validation loss: 2.247394013148482

Epoch: 5| Step: 9
Training loss: 2.892698287963867
Validation loss: 2.2311225527076313

Epoch: 5| Step: 10
Training loss: 2.4007701873779297
Validation loss: 2.2494541342540453

Epoch: 333| Step: 0
Training loss: 3.019275188446045
Validation loss: 2.24107144084028

Epoch: 5| Step: 1
Training loss: 2.4694714546203613
Validation loss: 2.2306168630558956

Epoch: 5| Step: 2
Training loss: 2.567986249923706
Validation loss: 2.2249413228804067

Epoch: 5| Step: 3
Training loss: 2.443739175796509
Validation loss: 2.2130817521002983

Epoch: 5| Step: 4
Training loss: 2.7526891231536865
Validation loss: 2.226402541642548

Epoch: 5| Step: 5
Training loss: 1.7416675090789795
Validation loss: 2.213213300192228

Epoch: 5| Step: 6
Training loss: 2.685281753540039
Validation loss: 2.216553854685958

Epoch: 5| Step: 7
Training loss: 2.4740681648254395
Validation loss: 2.2111722077092817

Epoch: 5| Step: 8
Training loss: 2.0354080200195312
Validation loss: 2.2327493800911853

Epoch: 5| Step: 9
Training loss: 2.2402138710021973
Validation loss: 2.2290707429250083

Epoch: 5| Step: 10
Training loss: 2.1635217666625977
Validation loss: 2.2388437178827103

Epoch: 334| Step: 0
Training loss: 2.525385856628418
Validation loss: 2.252484260066863

Epoch: 5| Step: 1
Training loss: 2.2599613666534424
Validation loss: 2.2577904116722847

Epoch: 5| Step: 2
Training loss: 2.196814775466919
Validation loss: 2.2512710825089486

Epoch: 5| Step: 3
Training loss: 2.0834250450134277
Validation loss: 2.2603475919333835

Epoch: 5| Step: 4
Training loss: 2.600172519683838
Validation loss: 2.2698956022980394

Epoch: 5| Step: 5
Training loss: 2.313123941421509
Validation loss: 2.2704305700076524

Epoch: 5| Step: 6
Training loss: 2.5024008750915527
Validation loss: 2.262249405666064

Epoch: 5| Step: 7
Training loss: 2.4100863933563232
Validation loss: 2.258227163745511

Epoch: 5| Step: 8
Training loss: 2.6185085773468018
Validation loss: 2.243992833681004

Epoch: 5| Step: 9
Training loss: 2.589590311050415
Validation loss: 2.252074254456387

Epoch: 5| Step: 10
Training loss: 2.5170512199401855
Validation loss: 2.2490882412079842

Epoch: 335| Step: 0
Training loss: 3.4117908477783203
Validation loss: 2.2437964177900747

Epoch: 5| Step: 1
Training loss: 2.1741461753845215
Validation loss: 2.2494300283411497

Epoch: 5| Step: 2
Training loss: 2.5867512226104736
Validation loss: 2.236952761168121

Epoch: 5| Step: 3
Training loss: 2.662362575531006
Validation loss: 2.257294293372862

Epoch: 5| Step: 4
Training loss: 1.8000977039337158
Validation loss: 2.241928526150283

Epoch: 5| Step: 5
Training loss: 2.5484981536865234
Validation loss: 2.24959663421877

Epoch: 5| Step: 6
Training loss: 1.8658790588378906
Validation loss: 2.2318303982416787

Epoch: 5| Step: 7
Training loss: 1.7049732208251953
Validation loss: 2.227251982176176

Epoch: 5| Step: 8
Training loss: 2.5909523963928223
Validation loss: 2.2303361251790035

Epoch: 5| Step: 9
Training loss: 2.8469812870025635
Validation loss: 2.226692868817237

Epoch: 5| Step: 10
Training loss: 2.733614444732666
Validation loss: 2.2249526951902654

Epoch: 336| Step: 0
Training loss: 2.5911431312561035
Validation loss: 2.2412841089310183

Epoch: 5| Step: 1
Training loss: 3.0676023960113525
Validation loss: 2.2363947078745854

Epoch: 5| Step: 2
Training loss: 2.2255380153656006
Validation loss: 2.256543315866942

Epoch: 5| Step: 3
Training loss: 2.345153331756592
Validation loss: 2.3007570928142917

Epoch: 5| Step: 4
Training loss: 2.39174222946167
Validation loss: 2.3470216361425256

Epoch: 5| Step: 5
Training loss: 2.61077880859375
Validation loss: 2.3365282371479976

Epoch: 5| Step: 6
Training loss: 2.196657657623291
Validation loss: 2.3054937829253492

Epoch: 5| Step: 7
Training loss: 2.540621519088745
Validation loss: 2.311580550286078

Epoch: 5| Step: 8
Training loss: 1.3330676555633545
Validation loss: 2.2828013589305263

Epoch: 5| Step: 9
Training loss: 3.288844347000122
Validation loss: 2.2785465063587313

Epoch: 5| Step: 10
Training loss: 2.2203445434570312
Validation loss: 2.264756800026022

Epoch: 337| Step: 0
Training loss: 2.315533399581909
Validation loss: 2.2292886754517913

Epoch: 5| Step: 1
Training loss: 2.4110212326049805
Validation loss: 2.2512314396519817

Epoch: 5| Step: 2
Training loss: 2.6096668243408203
Validation loss: 2.2382928017647035

Epoch: 5| Step: 3
Training loss: 2.2364184856414795
Validation loss: 2.2291739422787904

Epoch: 5| Step: 4
Training loss: 2.2858529090881348
Validation loss: 2.235447197832087

Epoch: 5| Step: 5
Training loss: 2.1733431816101074
Validation loss: 2.232107966176925

Epoch: 5| Step: 6
Training loss: 2.4753739833831787
Validation loss: 2.2280918346938265

Epoch: 5| Step: 7
Training loss: 2.793851852416992
Validation loss: 2.2275401623018327

Epoch: 5| Step: 8
Training loss: 2.0233676433563232
Validation loss: 2.2162571940370785

Epoch: 5| Step: 9
Training loss: 2.9519705772399902
Validation loss: 2.2330779055113434

Epoch: 5| Step: 10
Training loss: 2.616316795349121
Validation loss: 2.229048009841673

Epoch: 338| Step: 0
Training loss: 2.210210084915161
Validation loss: 2.2223414349299606

Epoch: 5| Step: 1
Training loss: 2.5436959266662598
Validation loss: 2.2236924209902362

Epoch: 5| Step: 2
Training loss: 2.482461452484131
Validation loss: 2.2304237401613625

Epoch: 5| Step: 3
Training loss: 2.1412112712860107
Validation loss: 2.2535608558244604

Epoch: 5| Step: 4
Training loss: 2.396395683288574
Validation loss: 2.2662701555477676

Epoch: 5| Step: 5
Training loss: 2.7530784606933594
Validation loss: 2.299118649575018

Epoch: 5| Step: 6
Training loss: 2.448375940322876
Validation loss: 2.3146399759477183

Epoch: 5| Step: 7
Training loss: 2.8575940132141113
Validation loss: 2.3412083938557613

Epoch: 5| Step: 8
Training loss: 2.126809597015381
Validation loss: 2.353437823633994

Epoch: 5| Step: 9
Training loss: 2.2518012523651123
Validation loss: 2.3427475575477845

Epoch: 5| Step: 10
Training loss: 2.7836642265319824
Validation loss: 2.3038847228532195

Epoch: 339| Step: 0
Training loss: 2.3580429553985596
Validation loss: 2.266511450531662

Epoch: 5| Step: 1
Training loss: 2.8050484657287598
Validation loss: 2.2247853099658923

Epoch: 5| Step: 2
Training loss: 1.8162529468536377
Validation loss: 2.219782611375214

Epoch: 5| Step: 3
Training loss: 2.048992156982422
Validation loss: 2.2098684028912614

Epoch: 5| Step: 4
Training loss: 2.506901264190674
Validation loss: 2.228931397520086

Epoch: 5| Step: 5
Training loss: 2.6460297107696533
Validation loss: 2.2343405023697884

Epoch: 5| Step: 6
Training loss: 1.9698489904403687
Validation loss: 2.2292356644907305

Epoch: 5| Step: 7
Training loss: 2.7422454357147217
Validation loss: 2.2249874530300016

Epoch: 5| Step: 8
Training loss: 2.8191871643066406
Validation loss: 2.2252579837717037

Epoch: 5| Step: 9
Training loss: 2.228313684463501
Validation loss: 2.2268755230852353

Epoch: 5| Step: 10
Training loss: 2.7963454723358154
Validation loss: 2.2317634449210217

Epoch: 340| Step: 0
Training loss: 1.853287935256958
Validation loss: 2.235259853383546

Epoch: 5| Step: 1
Training loss: 2.2825539112091064
Validation loss: 2.206841230392456

Epoch: 5| Step: 2
Training loss: 2.058468818664551
Validation loss: 2.233243916624336

Epoch: 5| Step: 3
Training loss: 2.373080253601074
Validation loss: 2.236431301281016

Epoch: 5| Step: 4
Training loss: 3.18784761428833
Validation loss: 2.2592388378676547

Epoch: 5| Step: 5
Training loss: 2.1036946773529053
Validation loss: 2.2826930169136292

Epoch: 5| Step: 6
Training loss: 3.0366923809051514
Validation loss: 2.294727220330187

Epoch: 5| Step: 7
Training loss: 2.6627845764160156
Validation loss: 2.2980486475011355

Epoch: 5| Step: 8
Training loss: 2.194614887237549
Validation loss: 2.2850426448288785

Epoch: 5| Step: 9
Training loss: 2.289332866668701
Validation loss: 2.267147292373001

Epoch: 5| Step: 10
Training loss: 2.620838165283203
Validation loss: 2.2608358885652278

Epoch: 341| Step: 0
Training loss: 2.359637975692749
Validation loss: 2.2579927713640275

Epoch: 5| Step: 1
Training loss: 2.668025493621826
Validation loss: 2.2467498343477965

Epoch: 5| Step: 2
Training loss: 2.6478095054626465
Validation loss: 2.2599545294238674

Epoch: 5| Step: 3
Training loss: 2.3108913898468018
Validation loss: 2.2530815396257626

Epoch: 5| Step: 4
Training loss: 2.697753667831421
Validation loss: 2.2638434453677108

Epoch: 5| Step: 5
Training loss: 2.670131206512451
Validation loss: 2.247927083764025

Epoch: 5| Step: 6
Training loss: 2.436843156814575
Validation loss: 2.244956736923546

Epoch: 5| Step: 7
Training loss: 2.3841044902801514
Validation loss: 2.250043248617521

Epoch: 5| Step: 8
Training loss: 2.103029251098633
Validation loss: 2.2421863566162767

Epoch: 5| Step: 9
Training loss: 2.4569852352142334
Validation loss: 2.230377533102548

Epoch: 5| Step: 10
Training loss: 1.9511123895645142
Validation loss: 2.237763113872979

Epoch: 342| Step: 0
Training loss: 2.4019789695739746
Validation loss: 2.223731258864044

Epoch: 5| Step: 1
Training loss: 2.7411766052246094
Validation loss: 2.230434312615343

Epoch: 5| Step: 2
Training loss: 2.2662291526794434
Validation loss: 2.227303320361722

Epoch: 5| Step: 3
Training loss: 2.799212694168091
Validation loss: 2.238761414763748

Epoch: 5| Step: 4
Training loss: 2.8517463207244873
Validation loss: 2.236992810362129

Epoch: 5| Step: 5
Training loss: 2.1271579265594482
Validation loss: 2.2497166510551208

Epoch: 5| Step: 6
Training loss: 2.3893165588378906
Validation loss: 2.257054518627864

Epoch: 5| Step: 7
Training loss: 2.599226474761963
Validation loss: 2.2658977585454143

Epoch: 5| Step: 8
Training loss: 1.770638108253479
Validation loss: 2.258414796603623

Epoch: 5| Step: 9
Training loss: 2.026916027069092
Validation loss: 2.235674497901752

Epoch: 5| Step: 10
Training loss: 2.509420156478882
Validation loss: 2.2479839145496325

Epoch: 343| Step: 0
Training loss: 3.1418004035949707
Validation loss: 2.213789442534088

Epoch: 5| Step: 1
Training loss: 2.7866299152374268
Validation loss: 2.2102987279174147

Epoch: 5| Step: 2
Training loss: 1.878726601600647
Validation loss: 2.2202375704242336

Epoch: 5| Step: 3
Training loss: 2.6935248374938965
Validation loss: 2.2048376080810383

Epoch: 5| Step: 4
Training loss: 1.9476585388183594
Validation loss: 2.1974112654245026

Epoch: 5| Step: 5
Training loss: 2.1251139640808105
Validation loss: 2.1986527109658844

Epoch: 5| Step: 6
Training loss: 3.0254275798797607
Validation loss: 2.210799886334327

Epoch: 5| Step: 7
Training loss: 2.2213516235351562
Validation loss: 2.2119265423026135

Epoch: 5| Step: 8
Training loss: 2.173219680786133
Validation loss: 2.20206924920441

Epoch: 5| Step: 9
Training loss: 2.275090456008911
Validation loss: 2.2189911898746284

Epoch: 5| Step: 10
Training loss: 2.1552515029907227
Validation loss: 2.2241811611319102

Epoch: 344| Step: 0
Training loss: 2.8355231285095215
Validation loss: 2.220405697822571

Epoch: 5| Step: 1
Training loss: 2.340614080429077
Validation loss: 2.2356274691961144

Epoch: 5| Step: 2
Training loss: 2.525386333465576
Validation loss: 2.2318844077407674

Epoch: 5| Step: 3
Training loss: 2.409472942352295
Validation loss: 2.234722788615893

Epoch: 5| Step: 4
Training loss: 2.136207342147827
Validation loss: 2.2256063389521774

Epoch: 5| Step: 5
Training loss: 2.269106864929199
Validation loss: 2.2531752022363807

Epoch: 5| Step: 6
Training loss: 1.6871906518936157
Validation loss: 2.2358476961812666

Epoch: 5| Step: 7
Training loss: 2.7681095600128174
Validation loss: 2.250122081848883

Epoch: 5| Step: 8
Training loss: 2.268603563308716
Validation loss: 2.269960741842947

Epoch: 5| Step: 9
Training loss: 2.5295937061309814
Validation loss: 2.273792189936484

Epoch: 5| Step: 10
Training loss: 2.7286009788513184
Validation loss: 2.245761744437679

Epoch: 345| Step: 0
Training loss: 1.9159629344940186
Validation loss: 2.243274329811014

Epoch: 5| Step: 1
Training loss: 3.035073757171631
Validation loss: 2.21706397046325

Epoch: 5| Step: 2
Training loss: 2.231583833694458
Validation loss: 2.235037762631652

Epoch: 5| Step: 3
Training loss: 2.772395610809326
Validation loss: 2.2370298652238745

Epoch: 5| Step: 4
Training loss: 1.7253673076629639
Validation loss: 2.2255356286161687

Epoch: 5| Step: 5
Training loss: 2.801335096359253
Validation loss: 2.2169527828052478

Epoch: 5| Step: 6
Training loss: 2.1844913959503174
Validation loss: 2.2265738325734294

Epoch: 5| Step: 7
Training loss: 2.3184409141540527
Validation loss: 2.2278994257732103

Epoch: 5| Step: 8
Training loss: 2.7945353984832764
Validation loss: 2.2295588113928355

Epoch: 5| Step: 9
Training loss: 3.069438934326172
Validation loss: 2.2248984049725276

Epoch: 5| Step: 10
Training loss: 1.6322331428527832
Validation loss: 2.2271025026998212

Epoch: 346| Step: 0
Training loss: 2.3190484046936035
Validation loss: 2.2271023181176957

Epoch: 5| Step: 1
Training loss: 2.8611292839050293
Validation loss: 2.23261078967843

Epoch: 5| Step: 2
Training loss: 2.6936020851135254
Validation loss: 2.245384306036016

Epoch: 5| Step: 3
Training loss: 1.6891635656356812
Validation loss: 2.256976341688505

Epoch: 5| Step: 4
Training loss: 2.439300775527954
Validation loss: 2.2363644287150395

Epoch: 5| Step: 5
Training loss: 2.0811469554901123
Validation loss: 2.232946995765932

Epoch: 5| Step: 6
Training loss: 1.857802391052246
Validation loss: 2.2179129174960557

Epoch: 5| Step: 7
Training loss: 2.7027618885040283
Validation loss: 2.217940908606334

Epoch: 5| Step: 8
Training loss: 2.490208148956299
Validation loss: 2.2175356136855258

Epoch: 5| Step: 9
Training loss: 2.7218427658081055
Validation loss: 2.2084263422155894

Epoch: 5| Step: 10
Training loss: 2.4744224548339844
Validation loss: 2.207589312266278

Epoch: 347| Step: 0
Training loss: 2.523439884185791
Validation loss: 2.2128709644399662

Epoch: 5| Step: 1
Training loss: 2.608687162399292
Validation loss: 2.206283587281422

Epoch: 5| Step: 2
Training loss: 2.108921766281128
Validation loss: 2.222199075965471

Epoch: 5| Step: 3
Training loss: 2.6084940433502197
Validation loss: 2.2216948616889214

Epoch: 5| Step: 4
Training loss: 2.0615298748016357
Validation loss: 2.213352946824925

Epoch: 5| Step: 5
Training loss: 3.0968430042266846
Validation loss: 2.209078332429291

Epoch: 5| Step: 6
Training loss: 2.2754011154174805
Validation loss: 2.2190362099678285

Epoch: 5| Step: 7
Training loss: 1.6007468700408936
Validation loss: 2.2209758091998357

Epoch: 5| Step: 8
Training loss: 1.9089202880859375
Validation loss: 2.223662627640591

Epoch: 5| Step: 9
Training loss: 2.5039424896240234
Validation loss: 2.2445321595796974

Epoch: 5| Step: 10
Training loss: 3.2385220527648926
Validation loss: 2.2638399549709853

Epoch: 348| Step: 0
Training loss: 2.6363892555236816
Validation loss: 2.2534237138686644

Epoch: 5| Step: 1
Training loss: 2.4159624576568604
Validation loss: 2.231963247381231

Epoch: 5| Step: 2
Training loss: 2.2909350395202637
Validation loss: 2.23244563482141

Epoch: 5| Step: 3
Training loss: 2.885784149169922
Validation loss: 2.2257440731089604

Epoch: 5| Step: 4
Training loss: 2.0783658027648926
Validation loss: 2.199899629880023

Epoch: 5| Step: 5
Training loss: 2.435964584350586
Validation loss: 2.217028722968153

Epoch: 5| Step: 6
Training loss: 3.1238627433776855
Validation loss: 2.2121078916775283

Epoch: 5| Step: 7
Training loss: 1.6202936172485352
Validation loss: 2.2057742752054685

Epoch: 5| Step: 8
Training loss: 2.6526520252227783
Validation loss: 2.2182978173737884

Epoch: 5| Step: 9
Training loss: 2.248628616333008
Validation loss: 2.2150051337416454

Epoch: 5| Step: 10
Training loss: 1.8676155805587769
Validation loss: 2.228348301303002

Epoch: 349| Step: 0
Training loss: 2.2209081649780273
Validation loss: 2.231608954809045

Epoch: 5| Step: 1
Training loss: 2.047426700592041
Validation loss: 2.2336780127658638

Epoch: 5| Step: 2
Training loss: 2.3373732566833496
Validation loss: 2.22470332730201

Epoch: 5| Step: 3
Training loss: 2.0301687717437744
Validation loss: 2.224511008108816

Epoch: 5| Step: 4
Training loss: 2.2064244747161865
Validation loss: 2.236541963392688

Epoch: 5| Step: 5
Training loss: 2.2217202186584473
Validation loss: 2.2246759988928355

Epoch: 5| Step: 6
Training loss: 2.475710391998291
Validation loss: 2.2601311078635593

Epoch: 5| Step: 7
Training loss: 3.1567423343658447
Validation loss: 2.2678744869847454

Epoch: 5| Step: 8
Training loss: 2.897618532180786
Validation loss: 2.2746835421490412

Epoch: 5| Step: 9
Training loss: 2.3201544284820557
Validation loss: 2.2792284001586256

Epoch: 5| Step: 10
Training loss: 2.7273941040039062
Validation loss: 2.2537347232141802

Epoch: 350| Step: 0
Training loss: 2.233499765396118
Validation loss: 2.233152425417336

Epoch: 5| Step: 1
Training loss: 2.21004056930542
Validation loss: 2.2314615275270198

Epoch: 5| Step: 2
Training loss: 2.5172176361083984
Validation loss: 2.228272989232053

Epoch: 5| Step: 3
Training loss: 1.8658336400985718
Validation loss: 2.2137116411680817

Epoch: 5| Step: 4
Training loss: 2.131199836730957
Validation loss: 2.2242398672206427

Epoch: 5| Step: 5
Training loss: 2.0956664085388184
Validation loss: 2.199621841471682

Epoch: 5| Step: 6
Training loss: 2.46840238571167
Validation loss: 2.2088942117588495

Epoch: 5| Step: 7
Training loss: 3.1982390880584717
Validation loss: 2.212796426588489

Epoch: 5| Step: 8
Training loss: 2.5723111629486084
Validation loss: 2.1993055394900742

Epoch: 5| Step: 9
Training loss: 2.4381566047668457
Validation loss: 2.22520685708651

Epoch: 5| Step: 10
Training loss: 2.488537073135376
Validation loss: 2.203103401327646

Epoch: 351| Step: 0
Training loss: 2.6337788105010986
Validation loss: 2.216142908219368

Epoch: 5| Step: 1
Training loss: 2.641528606414795
Validation loss: 2.234261599920129

Epoch: 5| Step: 2
Training loss: 1.8284868001937866
Validation loss: 2.2606239831575783

Epoch: 5| Step: 3
Training loss: 2.4460253715515137
Validation loss: 2.281790102681806

Epoch: 5| Step: 4
Training loss: 2.2778522968292236
Validation loss: 2.297875499212614

Epoch: 5| Step: 5
Training loss: 2.392300605773926
Validation loss: 2.2947822975856003

Epoch: 5| Step: 6
Training loss: 2.4065041542053223
Validation loss: 2.271561961020193

Epoch: 5| Step: 7
Training loss: 2.420346736907959
Validation loss: 2.238198567462224

Epoch: 5| Step: 8
Training loss: 1.892141342163086
Validation loss: 2.2053560031357633

Epoch: 5| Step: 9
Training loss: 3.050706148147583
Validation loss: 2.203565664188836

Epoch: 5| Step: 10
Training loss: 2.448822021484375
Validation loss: 2.172048371325257

Epoch: 352| Step: 0
Training loss: 2.474968433380127
Validation loss: 2.1920594989612536

Epoch: 5| Step: 1
Training loss: 2.4814019203186035
Validation loss: 2.1936931828016877

Epoch: 5| Step: 2
Training loss: 2.7512855529785156
Validation loss: 2.19878710726256

Epoch: 5| Step: 3
Training loss: 2.319460391998291
Validation loss: 2.2065345600087154

Epoch: 5| Step: 4
Training loss: 1.927666425704956
Validation loss: 2.1924386011656893

Epoch: 5| Step: 5
Training loss: 2.470208168029785
Validation loss: 2.2013583593471076

Epoch: 5| Step: 6
Training loss: 1.8010613918304443
Validation loss: 2.1894111607664373

Epoch: 5| Step: 7
Training loss: 2.6477372646331787
Validation loss: 2.181674172801356

Epoch: 5| Step: 8
Training loss: 2.2302186489105225
Validation loss: 2.183090868816581

Epoch: 5| Step: 9
Training loss: 2.9419331550598145
Validation loss: 2.18443541372976

Epoch: 5| Step: 10
Training loss: 2.6312828063964844
Validation loss: 2.177736266966789

Epoch: 353| Step: 0
Training loss: 3.212048292160034
Validation loss: 2.1886788311825005

Epoch: 5| Step: 1
Training loss: 2.4344894886016846
Validation loss: 2.1966503820111676

Epoch: 5| Step: 2
Training loss: 2.4157872200012207
Validation loss: 2.2067186012062976

Epoch: 5| Step: 3
Training loss: 1.7134323120117188
Validation loss: 2.2106585605170137

Epoch: 5| Step: 4
Training loss: 1.9835760593414307
Validation loss: 2.2140441556130686

Epoch: 5| Step: 5
Training loss: 2.254031181335449
Validation loss: 2.222635804965932

Epoch: 5| Step: 6
Training loss: 1.8514600992202759
Validation loss: 2.2599941966354207

Epoch: 5| Step: 7
Training loss: 2.5413269996643066
Validation loss: 2.2857839984278523

Epoch: 5| Step: 8
Training loss: 2.6874847412109375
Validation loss: 2.2887263964581233

Epoch: 5| Step: 9
Training loss: 2.5606846809387207
Validation loss: 2.271689071450182

Epoch: 5| Step: 10
Training loss: 2.7594385147094727
Validation loss: 2.253632717235114

Epoch: 354| Step: 0
Training loss: 2.505901575088501
Validation loss: 2.250721122628899

Epoch: 5| Step: 1
Training loss: 1.9016097784042358
Validation loss: 2.2307884590600127

Epoch: 5| Step: 2
Training loss: 1.9598388671875
Validation loss: 2.2091877524570753

Epoch: 5| Step: 3
Training loss: 2.263547420501709
Validation loss: 2.2038937614810084

Epoch: 5| Step: 4
Training loss: 2.534046173095703
Validation loss: 2.20807836773575

Epoch: 5| Step: 5
Training loss: 1.928436279296875
Validation loss: 2.2121090171157674

Epoch: 5| Step: 6
Training loss: 2.5733249187469482
Validation loss: 2.2061107556025186

Epoch: 5| Step: 7
Training loss: 3.425046443939209
Validation loss: 2.2083441980423464

Epoch: 5| Step: 8
Training loss: 2.1963260173797607
Validation loss: 2.211401862482871

Epoch: 5| Step: 9
Training loss: 2.744856357574463
Validation loss: 2.1995062879336778

Epoch: 5| Step: 10
Training loss: 2.2308225631713867
Validation loss: 2.2258715783396075

Epoch: 355| Step: 0
Training loss: 2.936103343963623
Validation loss: 2.213240482473886

Epoch: 5| Step: 1
Training loss: 2.1769988536834717
Validation loss: 2.21288901247004

Epoch: 5| Step: 2
Training loss: 2.1802000999450684
Validation loss: 2.229168486851518

Epoch: 5| Step: 3
Training loss: 2.0945918560028076
Validation loss: 2.217770920004896

Epoch: 5| Step: 4
Training loss: 2.067671298980713
Validation loss: 2.212660233179728

Epoch: 5| Step: 5
Training loss: 3.00810170173645
Validation loss: 2.210959570382231

Epoch: 5| Step: 6
Training loss: 2.1546311378479004
Validation loss: 2.2111369281686764

Epoch: 5| Step: 7
Training loss: 2.633979320526123
Validation loss: 2.1947800779855378

Epoch: 5| Step: 8
Training loss: 2.667832136154175
Validation loss: 2.202455113011022

Epoch: 5| Step: 9
Training loss: 2.133309841156006
Validation loss: 2.2080444674338064

Epoch: 5| Step: 10
Training loss: 2.103569746017456
Validation loss: 2.205812285023351

Epoch: 356| Step: 0
Training loss: 2.7044265270233154
Validation loss: 2.2150214077323995

Epoch: 5| Step: 1
Training loss: 2.717193126678467
Validation loss: 2.2065052857962986

Epoch: 5| Step: 2
Training loss: 2.5379137992858887
Validation loss: 2.2054555723744054

Epoch: 5| Step: 3
Training loss: 2.2427146434783936
Validation loss: 2.2034065774692

Epoch: 5| Step: 4
Training loss: 2.3570196628570557
Validation loss: 2.206235521583147

Epoch: 5| Step: 5
Training loss: 2.1327598094940186
Validation loss: 2.194047551001272

Epoch: 5| Step: 6
Training loss: 1.5897423028945923
Validation loss: 2.191056643762896

Epoch: 5| Step: 7
Training loss: 2.284595489501953
Validation loss: 2.199759021882088

Epoch: 5| Step: 8
Training loss: 2.7996222972869873
Validation loss: 2.2063337449104554

Epoch: 5| Step: 9
Training loss: 2.6960623264312744
Validation loss: 2.194965477912657

Epoch: 5| Step: 10
Training loss: 2.119778871536255
Validation loss: 2.192854248067384

Epoch: 357| Step: 0
Training loss: 2.004631280899048
Validation loss: 2.190626359754993

Epoch: 5| Step: 1
Training loss: 2.5924899578094482
Validation loss: 2.2021085267425864

Epoch: 5| Step: 2
Training loss: 3.181365489959717
Validation loss: 2.193987005500383

Epoch: 5| Step: 3
Training loss: 2.5976719856262207
Validation loss: 2.1886428709953063

Epoch: 5| Step: 4
Training loss: 1.5043745040893555
Validation loss: 2.1908764813535955

Epoch: 5| Step: 5
Training loss: 2.455641984939575
Validation loss: 2.1884299965314966

Epoch: 5| Step: 6
Training loss: 2.277287244796753
Validation loss: 2.1902670014289116

Epoch: 5| Step: 7
Training loss: 2.452585220336914
Validation loss: 2.195004035067815

Epoch: 5| Step: 8
Training loss: 1.5943410396575928
Validation loss: 2.179020825252738

Epoch: 5| Step: 9
Training loss: 2.8677799701690674
Validation loss: 2.188800370821389

Epoch: 5| Step: 10
Training loss: 2.5706658363342285
Validation loss: 2.1851553968203965

Epoch: 358| Step: 0
Training loss: 1.9876234531402588
Validation loss: 2.2083774638432327

Epoch: 5| Step: 1
Training loss: 2.325753688812256
Validation loss: 2.2055505065507788

Epoch: 5| Step: 2
Training loss: 2.6297104358673096
Validation loss: 2.233642170506139

Epoch: 5| Step: 3
Training loss: 2.02506685256958
Validation loss: 2.217785170001368

Epoch: 5| Step: 4
Training loss: 2.067418336868286
Validation loss: 2.213360813356215

Epoch: 5| Step: 5
Training loss: 2.8340413570404053
Validation loss: 2.216016661736273

Epoch: 5| Step: 6
Training loss: 1.8184707164764404
Validation loss: 2.1972135625859743

Epoch: 5| Step: 7
Training loss: 2.431760311126709
Validation loss: 2.207699716732066

Epoch: 5| Step: 8
Training loss: 2.600438356399536
Validation loss: 2.2065864993679907

Epoch: 5| Step: 9
Training loss: 2.696260929107666
Validation loss: 2.2111748418500348

Epoch: 5| Step: 10
Training loss: 2.8214924335479736
Validation loss: 2.204188623735982

Epoch: 359| Step: 0
Training loss: 2.4533910751342773
Validation loss: 2.207451046154063

Epoch: 5| Step: 1
Training loss: 2.313749074935913
Validation loss: 2.2229102119322746

Epoch: 5| Step: 2
Training loss: 2.4061787128448486
Validation loss: 2.220299090108564

Epoch: 5| Step: 3
Training loss: 2.231828212738037
Validation loss: 2.224708390492265

Epoch: 5| Step: 4
Training loss: 2.4354052543640137
Validation loss: 2.217289052983766

Epoch: 5| Step: 5
Training loss: 1.4245251417160034
Validation loss: 2.2423434283143733

Epoch: 5| Step: 6
Training loss: 2.7952890396118164
Validation loss: 2.1929906363128335

Epoch: 5| Step: 7
Training loss: 2.3026556968688965
Validation loss: 2.20324448872638

Epoch: 5| Step: 8
Training loss: 2.776205062866211
Validation loss: 2.207024601198012

Epoch: 5| Step: 9
Training loss: 2.2497997283935547
Validation loss: 2.19157483244455

Epoch: 5| Step: 10
Training loss: 2.7950358390808105
Validation loss: 2.2019976313396166

Epoch: 360| Step: 0
Training loss: 1.8042205572128296
Validation loss: 2.207156647918045

Epoch: 5| Step: 1
Training loss: 1.8417373895645142
Validation loss: 2.1953292021187405

Epoch: 5| Step: 2
Training loss: 3.3666610717773438
Validation loss: 2.2024133359232256

Epoch: 5| Step: 3
Training loss: 2.737546920776367
Validation loss: 2.207024899862146

Epoch: 5| Step: 4
Training loss: 2.9953701496124268
Validation loss: 2.1955131100070093

Epoch: 5| Step: 5
Training loss: 2.42521595954895
Validation loss: 2.200194856171967

Epoch: 5| Step: 6
Training loss: 2.756256341934204
Validation loss: 2.177346788426881

Epoch: 5| Step: 7
Training loss: 2.0754222869873047
Validation loss: 2.192962528556906

Epoch: 5| Step: 8
Training loss: 1.8915191888809204
Validation loss: 2.1797278773400093

Epoch: 5| Step: 9
Training loss: 2.00761079788208
Validation loss: 2.201091904794016

Epoch: 5| Step: 10
Training loss: 2.169945240020752
Validation loss: 2.2146626877528366

Epoch: 361| Step: 0
Training loss: 2.3873696327209473
Validation loss: 2.224637295610161

Epoch: 5| Step: 1
Training loss: 1.8711988925933838
Validation loss: 2.2294018627494894

Epoch: 5| Step: 2
Training loss: 1.8630635738372803
Validation loss: 2.2407719012229674

Epoch: 5| Step: 3
Training loss: 1.837876319885254
Validation loss: 2.2286391258239746

Epoch: 5| Step: 4
Training loss: 2.7661001682281494
Validation loss: 2.2350985901330107

Epoch: 5| Step: 5
Training loss: 3.0149874687194824
Validation loss: 2.2144948436367895

Epoch: 5| Step: 6
Training loss: 2.504460573196411
Validation loss: 2.21550666645009

Epoch: 5| Step: 7
Training loss: 1.8307682275772095
Validation loss: 2.2066142251414638

Epoch: 5| Step: 8
Training loss: 2.6982340812683105
Validation loss: 2.2160212583439325

Epoch: 5| Step: 9
Training loss: 2.8421883583068848
Validation loss: 2.2070647593467467

Epoch: 5| Step: 10
Training loss: 2.7751405239105225
Validation loss: 2.2099423511053926

Epoch: 362| Step: 0
Training loss: 2.4780964851379395
Validation loss: 2.217403143964788

Epoch: 5| Step: 1
Training loss: 2.745903968811035
Validation loss: 2.2294603483651274

Epoch: 5| Step: 2
Training loss: 2.4821712970733643
Validation loss: 2.2258021998149093

Epoch: 5| Step: 3
Training loss: 2.671541690826416
Validation loss: 2.2072178317654516

Epoch: 5| Step: 4
Training loss: 1.52191162109375
Validation loss: 2.222535248725645

Epoch: 5| Step: 5
Training loss: 2.309790849685669
Validation loss: 2.212329795283656

Epoch: 5| Step: 6
Training loss: 2.4371204376220703
Validation loss: 2.2250383464238976

Epoch: 5| Step: 7
Training loss: 2.726339817047119
Validation loss: 2.195331455558859

Epoch: 5| Step: 8
Training loss: 2.4171435832977295
Validation loss: 2.198639392852783

Epoch: 5| Step: 9
Training loss: 2.3073322772979736
Validation loss: 2.197103443966117

Epoch: 5| Step: 10
Training loss: 2.102275848388672
Validation loss: 2.169091647671115

Epoch: 363| Step: 0
Training loss: 2.2557778358459473
Validation loss: 2.1825841601176927

Epoch: 5| Step: 1
Training loss: 2.3376338481903076
Validation loss: 2.1868210402868127

Epoch: 5| Step: 2
Training loss: 2.156273365020752
Validation loss: 2.1804297547186575

Epoch: 5| Step: 3
Training loss: 2.6501052379608154
Validation loss: 2.1620030838956117

Epoch: 5| Step: 4
Training loss: 2.267512083053589
Validation loss: 2.174186480942593

Epoch: 5| Step: 5
Training loss: 2.796621561050415
Validation loss: 2.1702673486483994

Epoch: 5| Step: 6
Training loss: 2.515455961227417
Validation loss: 2.1651167920840684

Epoch: 5| Step: 7
Training loss: 2.1806275844573975
Validation loss: 2.1808892244933755

Epoch: 5| Step: 8
Training loss: 2.5068626403808594
Validation loss: 2.1666198981705533

Epoch: 5| Step: 9
Training loss: 2.0196170806884766
Validation loss: 2.181629870527534

Epoch: 5| Step: 10
Training loss: 2.2376744747161865
Validation loss: 2.1824871596469673

Epoch: 364| Step: 0
Training loss: 1.514953851699829
Validation loss: 2.2046998649515133

Epoch: 5| Step: 1
Training loss: 2.0724244117736816
Validation loss: 2.2007282318607455

Epoch: 5| Step: 2
Training loss: 2.2459189891815186
Validation loss: 2.2167435717839066

Epoch: 5| Step: 3
Training loss: 2.5489633083343506
Validation loss: 2.2018077963141987

Epoch: 5| Step: 4
Training loss: 2.124722957611084
Validation loss: 2.2033416481428247

Epoch: 5| Step: 5
Training loss: 2.7880749702453613
Validation loss: 2.1866104782268567

Epoch: 5| Step: 6
Training loss: 2.318300247192383
Validation loss: 2.198665003622732

Epoch: 5| Step: 7
Training loss: 1.9905903339385986
Validation loss: 2.1871257507672874

Epoch: 5| Step: 8
Training loss: 2.374800443649292
Validation loss: 2.2013142134553645

Epoch: 5| Step: 9
Training loss: 3.131847858428955
Validation loss: 2.188272786396806

Epoch: 5| Step: 10
Training loss: 2.994227170944214
Validation loss: 2.213977201010591

Epoch: 365| Step: 0
Training loss: 2.0378684997558594
Validation loss: 2.1780290578001287

Epoch: 5| Step: 1
Training loss: 2.403643846511841
Validation loss: 2.1907680496092765

Epoch: 5| Step: 2
Training loss: 2.4584317207336426
Validation loss: 2.189414647317702

Epoch: 5| Step: 3
Training loss: 2.4941465854644775
Validation loss: 2.176477047704881

Epoch: 5| Step: 4
Training loss: 2.709251880645752
Validation loss: 2.177045709343367

Epoch: 5| Step: 5
Training loss: 3.1164729595184326
Validation loss: 2.1742499874484156

Epoch: 5| Step: 6
Training loss: 2.143796443939209
Validation loss: 2.1914966337142454

Epoch: 5| Step: 7
Training loss: 2.103584051132202
Validation loss: 2.1675738032146166

Epoch: 5| Step: 8
Training loss: 2.09787654876709
Validation loss: 2.1753076020107476

Epoch: 5| Step: 9
Training loss: 2.1307082176208496
Validation loss: 2.1926592344878824

Epoch: 5| Step: 10
Training loss: 2.2626559734344482
Validation loss: 2.1875167713370374

Epoch: 366| Step: 0
Training loss: 2.519590139389038
Validation loss: 2.187218512258222

Epoch: 5| Step: 1
Training loss: 2.122812032699585
Validation loss: 2.18301377501539

Epoch: 5| Step: 2
Training loss: 2.738424062728882
Validation loss: 2.188920917049531

Epoch: 5| Step: 3
Training loss: 1.9770902395248413
Validation loss: 2.204931543719384

Epoch: 5| Step: 4
Training loss: 2.5198071002960205
Validation loss: 2.2079177543681157

Epoch: 5| Step: 5
Training loss: 2.4782655239105225
Validation loss: 2.20458649563533

Epoch: 5| Step: 6
Training loss: 2.9494969844818115
Validation loss: 2.1917844562120337

Epoch: 5| Step: 7
Training loss: 1.8668702840805054
Validation loss: 2.1968252710116807

Epoch: 5| Step: 8
Training loss: 2.1875412464141846
Validation loss: 2.1832363528590046

Epoch: 5| Step: 9
Training loss: 2.374588966369629
Validation loss: 2.176522026779831

Epoch: 5| Step: 10
Training loss: 2.2201595306396484
Validation loss: 2.1738112690628215

Epoch: 367| Step: 0
Training loss: 2.038382053375244
Validation loss: 2.1654393583215694

Epoch: 5| Step: 1
Training loss: 1.8649346828460693
Validation loss: 2.1771452888365714

Epoch: 5| Step: 2
Training loss: 3.10542368888855
Validation loss: 2.1808355674948743

Epoch: 5| Step: 3
Training loss: 2.457848072052002
Validation loss: 2.1839078062324115

Epoch: 5| Step: 4
Training loss: 2.1346325874328613
Validation loss: 2.166599481336532

Epoch: 5| Step: 5
Training loss: 2.0655601024627686
Validation loss: 2.1787157802171606

Epoch: 5| Step: 6
Training loss: 1.9531883001327515
Validation loss: 2.187376058229836

Epoch: 5| Step: 7
Training loss: 2.4399027824401855
Validation loss: 2.18404229482015

Epoch: 5| Step: 8
Training loss: 3.0342977046966553
Validation loss: 2.187819852623888

Epoch: 5| Step: 9
Training loss: 2.6805553436279297
Validation loss: 2.212662184110252

Epoch: 5| Step: 10
Training loss: 2.0227012634277344
Validation loss: 2.221221859737109

Epoch: 368| Step: 0
Training loss: 2.968820333480835
Validation loss: 2.2188996243220505

Epoch: 5| Step: 1
Training loss: 1.8328933715820312
Validation loss: 2.2288079620689474

Epoch: 5| Step: 2
Training loss: 2.6492810249328613
Validation loss: 2.219217808015885

Epoch: 5| Step: 3
Training loss: 1.6355159282684326
Validation loss: 2.2079440970574655

Epoch: 5| Step: 4
Training loss: 2.3761203289031982
Validation loss: 2.206270279422883

Epoch: 5| Step: 5
Training loss: 2.34306001663208
Validation loss: 2.194187705234815

Epoch: 5| Step: 6
Training loss: 2.9139163494110107
Validation loss: 2.2060597840175835

Epoch: 5| Step: 7
Training loss: 1.6843417882919312
Validation loss: 2.1726930756722727

Epoch: 5| Step: 8
Training loss: 2.626002073287964
Validation loss: 2.1786534350405455

Epoch: 5| Step: 9
Training loss: 2.3082799911499023
Validation loss: 2.1791362249723045

Epoch: 5| Step: 10
Training loss: 2.6071360111236572
Validation loss: 2.1837505537976503

Epoch: 369| Step: 0
Training loss: 2.5849385261535645
Validation loss: 2.2102595631794264

Epoch: 5| Step: 1
Training loss: 2.5062789916992188
Validation loss: 2.239816645140289

Epoch: 5| Step: 2
Training loss: 2.0869052410125732
Validation loss: 2.270940637075773

Epoch: 5| Step: 3
Training loss: 2.143035888671875
Validation loss: 2.3019221700647825

Epoch: 5| Step: 4
Training loss: 2.019505500793457
Validation loss: 2.306359028303495

Epoch: 5| Step: 5
Training loss: 2.6888279914855957
Validation loss: 2.282695718990859

Epoch: 5| Step: 6
Training loss: 2.4253928661346436
Validation loss: 2.2764044961621686

Epoch: 5| Step: 7
Training loss: 3.1464972496032715
Validation loss: 2.213834031935661

Epoch: 5| Step: 8
Training loss: 2.2931969165802
Validation loss: 2.1836657703563733

Epoch: 5| Step: 9
Training loss: 2.126340866088867
Validation loss: 2.162544555561517

Epoch: 5| Step: 10
Training loss: 2.1604905128479004
Validation loss: 2.161876888685329

Epoch: 370| Step: 0
Training loss: 1.79292893409729
Validation loss: 2.1593097794440483

Epoch: 5| Step: 1
Training loss: 2.2887909412384033
Validation loss: 2.1549644521487656

Epoch: 5| Step: 2
Training loss: 2.5931003093719482
Validation loss: 2.143037260219615

Epoch: 5| Step: 3
Training loss: 1.9889447689056396
Validation loss: 2.151519544662968

Epoch: 5| Step: 4
Training loss: 2.6259734630584717
Validation loss: 2.1448965636632775

Epoch: 5| Step: 5
Training loss: 2.5923447608947754
Validation loss: 2.1506659189860025

Epoch: 5| Step: 6
Training loss: 2.7462570667266846
Validation loss: 2.1508464467140938

Epoch: 5| Step: 7
Training loss: 2.3145294189453125
Validation loss: 2.153042180563814

Epoch: 5| Step: 8
Training loss: 2.560082197189331
Validation loss: 2.1574547777893724

Epoch: 5| Step: 9
Training loss: 2.6446101665496826
Validation loss: 2.1621795213350685

Epoch: 5| Step: 10
Training loss: 1.8600993156433105
Validation loss: 2.1881033746145104

Epoch: 371| Step: 0
Training loss: 2.6215782165527344
Validation loss: 2.181420344178395

Epoch: 5| Step: 1
Training loss: 2.9521937370300293
Validation loss: 2.1737250333191245

Epoch: 5| Step: 2
Training loss: 2.1056199073791504
Validation loss: 2.1762818777433006

Epoch: 5| Step: 3
Training loss: 1.8284565210342407
Validation loss: 2.1724157538465274

Epoch: 5| Step: 4
Training loss: 1.9868214130401611
Validation loss: 2.173148614104076

Epoch: 5| Step: 5
Training loss: 2.208303213119507
Validation loss: 2.1800911529089815

Epoch: 5| Step: 6
Training loss: 2.3026797771453857
Validation loss: 2.180136255038682

Epoch: 5| Step: 7
Training loss: 3.0815210342407227
Validation loss: 2.1955145097547963

Epoch: 5| Step: 8
Training loss: 2.8069097995758057
Validation loss: 2.1751559498489543

Epoch: 5| Step: 9
Training loss: 1.6255781650543213
Validation loss: 2.174848234781655

Epoch: 5| Step: 10
Training loss: 2.2135531902313232
Validation loss: 2.175756464722336

Epoch: 372| Step: 0
Training loss: 2.2848610877990723
Validation loss: 2.1745801535985803

Epoch: 5| Step: 1
Training loss: 2.76745343208313
Validation loss: 2.176409113791681

Epoch: 5| Step: 2
Training loss: 2.2259387969970703
Validation loss: 2.1786190284195768

Epoch: 5| Step: 3
Training loss: 2.176208019256592
Validation loss: 2.1694835821787515

Epoch: 5| Step: 4
Training loss: 2.350790023803711
Validation loss: 2.187975953984004

Epoch: 5| Step: 5
Training loss: 1.977958083152771
Validation loss: 2.19441000235978

Epoch: 5| Step: 6
Training loss: 2.9191532135009766
Validation loss: 2.187884061567245

Epoch: 5| Step: 7
Training loss: 2.6780669689178467
Validation loss: 2.1966701528077484

Epoch: 5| Step: 8
Training loss: 2.177983283996582
Validation loss: 2.1829912303596415

Epoch: 5| Step: 9
Training loss: 2.243252992630005
Validation loss: 2.1718320564557145

Epoch: 5| Step: 10
Training loss: 1.8867892026901245
Validation loss: 2.180728009952012

Epoch: 373| Step: 0
Training loss: 1.9837563037872314
Validation loss: 2.1833942487675655

Epoch: 5| Step: 1
Training loss: 2.458040714263916
Validation loss: 2.176019578851679

Epoch: 5| Step: 2
Training loss: 2.4614696502685547
Validation loss: 2.180889650057721

Epoch: 5| Step: 3
Training loss: 2.568856954574585
Validation loss: 2.173847175413562

Epoch: 5| Step: 4
Training loss: 2.745394706726074
Validation loss: 2.169954179435648

Epoch: 5| Step: 5
Training loss: 2.037349224090576
Validation loss: 2.1764671930702786

Epoch: 5| Step: 6
Training loss: 2.500617265701294
Validation loss: 2.1600392710778022

Epoch: 5| Step: 7
Training loss: 2.5658304691314697
Validation loss: 2.1661876068320325

Epoch: 5| Step: 8
Training loss: 2.4520459175109863
Validation loss: 2.1690889917394167

Epoch: 5| Step: 9
Training loss: 1.8804092407226562
Validation loss: 2.178651007272864

Epoch: 5| Step: 10
Training loss: 1.9173797369003296
Validation loss: 2.183261166336716

Epoch: 374| Step: 0
Training loss: 2.5839619636535645
Validation loss: 2.2072061518187165

Epoch: 5| Step: 1
Training loss: 2.1927876472473145
Validation loss: 2.1771069534363283

Epoch: 5| Step: 2
Training loss: 2.2990682125091553
Validation loss: 2.161520616982573

Epoch: 5| Step: 3
Training loss: 2.361855983734131
Validation loss: 2.1728129771447953

Epoch: 5| Step: 4
Training loss: 1.2635419368743896
Validation loss: 2.1530849318350516

Epoch: 5| Step: 5
Training loss: 2.7032599449157715
Validation loss: 2.141887628903953

Epoch: 5| Step: 6
Training loss: 2.2371954917907715
Validation loss: 2.15112119079918

Epoch: 5| Step: 7
Training loss: 2.990490436553955
Validation loss: 2.160799467435447

Epoch: 5| Step: 8
Training loss: 2.385240077972412
Validation loss: 2.162135442097982

Epoch: 5| Step: 9
Training loss: 2.6664059162139893
Validation loss: 2.1584006406927623

Epoch: 5| Step: 10
Training loss: 2.21622371673584
Validation loss: 2.1616502372167443

Epoch: 375| Step: 0
Training loss: 2.8661532402038574
Validation loss: 2.1586106541336223

Epoch: 5| Step: 1
Training loss: 2.528975486755371
Validation loss: 2.210490316473028

Epoch: 5| Step: 2
Training loss: 2.096031665802002
Validation loss: 2.236876501831957

Epoch: 5| Step: 3
Training loss: 1.5996155738830566
Validation loss: 2.2394953876413326

Epoch: 5| Step: 4
Training loss: 2.4830410480499268
Validation loss: 2.2611826260884604

Epoch: 5| Step: 5
Training loss: 2.9464075565338135
Validation loss: 2.2447994934615267

Epoch: 5| Step: 6
Training loss: 2.9118895530700684
Validation loss: 2.2100627114695888

Epoch: 5| Step: 7
Training loss: 2.4901890754699707
Validation loss: 2.1863746514884372

Epoch: 5| Step: 8
Training loss: 1.7498124837875366
Validation loss: 2.1774146454308623

Epoch: 5| Step: 9
Training loss: 1.891509771347046
Validation loss: 2.143127379878875

Epoch: 5| Step: 10
Training loss: 2.398817300796509
Validation loss: 2.1505954316867295

Epoch: 376| Step: 0
Training loss: 2.493443727493286
Validation loss: 2.167045698370985

Epoch: 5| Step: 1
Training loss: 2.820396661758423
Validation loss: 2.1692330811613347

Epoch: 5| Step: 2
Training loss: 2.9639196395874023
Validation loss: 2.1643687473830355

Epoch: 5| Step: 3
Training loss: 2.3759517669677734
Validation loss: 2.157646986746019

Epoch: 5| Step: 4
Training loss: 2.526944637298584
Validation loss: 2.1600425474105345

Epoch: 5| Step: 5
Training loss: 2.660788059234619
Validation loss: 2.1637961633743776

Epoch: 5| Step: 6
Training loss: 1.9595730304718018
Validation loss: 2.1647701494155394

Epoch: 5| Step: 7
Training loss: 2.2934629917144775
Validation loss: 2.162159586465487

Epoch: 5| Step: 8
Training loss: 1.9300868511199951
Validation loss: 2.160881801318097

Epoch: 5| Step: 9
Training loss: 2.120987892150879
Validation loss: 2.1664572428631526

Epoch: 5| Step: 10
Training loss: 1.642004370689392
Validation loss: 2.163377405494772

Epoch: 377| Step: 0
Training loss: 2.2335994243621826
Validation loss: 2.1645891025502193

Epoch: 5| Step: 1
Training loss: 2.7163400650024414
Validation loss: 2.196803780012233

Epoch: 5| Step: 2
Training loss: 2.312260627746582
Validation loss: 2.180233552891721

Epoch: 5| Step: 3
Training loss: 2.8563573360443115
Validation loss: 2.1889897251641877

Epoch: 5| Step: 4
Training loss: 2.755687713623047
Validation loss: 2.1832633633767404

Epoch: 5| Step: 5
Training loss: 2.4483673572540283
Validation loss: 2.170121172423004

Epoch: 5| Step: 6
Training loss: 2.35683012008667
Validation loss: 2.1517783800760903

Epoch: 5| Step: 7
Training loss: 1.4245342016220093
Validation loss: 2.1751191872422413

Epoch: 5| Step: 8
Training loss: 1.757742166519165
Validation loss: 2.1629839738210044

Epoch: 5| Step: 9
Training loss: 2.64845609664917
Validation loss: 2.162657094258134

Epoch: 5| Step: 10
Training loss: 2.31292462348938
Validation loss: 2.1484676484138734

Epoch: 378| Step: 0
Training loss: 2.6329550743103027
Validation loss: 2.1508627066048245

Epoch: 5| Step: 1
Training loss: 2.789766788482666
Validation loss: 2.1630244921612483

Epoch: 5| Step: 2
Training loss: 1.998460054397583
Validation loss: 2.175718345949727

Epoch: 5| Step: 3
Training loss: 1.763494849205017
Validation loss: 2.1754994212940173

Epoch: 5| Step: 4
Training loss: 2.962087392807007
Validation loss: 2.226508989129015

Epoch: 5| Step: 5
Training loss: 2.799426555633545
Validation loss: 2.182990922722765

Epoch: 5| Step: 6
Training loss: 1.9467029571533203
Validation loss: 2.1652341504250803

Epoch: 5| Step: 7
Training loss: 1.914178490638733
Validation loss: 2.1744890853922856

Epoch: 5| Step: 8
Training loss: 2.777357578277588
Validation loss: 2.178399621799428

Epoch: 5| Step: 9
Training loss: 1.6241304874420166
Validation loss: 2.170230004095262

Epoch: 5| Step: 10
Training loss: 2.434584379196167
Validation loss: 2.1586162941430205

Epoch: 379| Step: 0
Training loss: 2.109959602355957
Validation loss: 2.1627319961465816

Epoch: 5| Step: 1
Training loss: 2.1175851821899414
Validation loss: 2.1552638174385153

Epoch: 5| Step: 2
Training loss: 2.4704654216766357
Validation loss: 2.163735689655427

Epoch: 5| Step: 3
Training loss: 2.592607021331787
Validation loss: 2.1456400668749245

Epoch: 5| Step: 4
Training loss: 2.787489175796509
Validation loss: 2.141046856039314

Epoch: 5| Step: 5
Training loss: 2.2096660137176514
Validation loss: 2.1405257640346402

Epoch: 5| Step: 6
Training loss: 1.8113524913787842
Validation loss: 2.1330615807605047

Epoch: 5| Step: 7
Training loss: 2.553570032119751
Validation loss: 2.121676650098575

Epoch: 5| Step: 8
Training loss: 2.727653980255127
Validation loss: 2.137173682130793

Epoch: 5| Step: 9
Training loss: 2.3242480754852295
Validation loss: 2.1371340136374197

Epoch: 5| Step: 10
Training loss: 2.1664607524871826
Validation loss: 2.1714883183920257

Epoch: 380| Step: 0
Training loss: 1.9489978551864624
Validation loss: 2.2040129771796604

Epoch: 5| Step: 1
Training loss: 2.521498441696167
Validation loss: 2.1937860750382945

Epoch: 5| Step: 2
Training loss: 2.6861326694488525
Validation loss: 2.19281522176599

Epoch: 5| Step: 3
Training loss: 2.146096706390381
Validation loss: 2.194935014170985

Epoch: 5| Step: 4
Training loss: 2.4451472759246826
Validation loss: 2.1776740320267214

Epoch: 5| Step: 5
Training loss: 2.655089855194092
Validation loss: 2.158716381237071

Epoch: 5| Step: 6
Training loss: 3.062713146209717
Validation loss: 2.1490078690231487

Epoch: 5| Step: 7
Training loss: 2.1316494941711426
Validation loss: 2.1560305485161404

Epoch: 5| Step: 8
Training loss: 2.2391035556793213
Validation loss: 2.1468814496071107

Epoch: 5| Step: 9
Training loss: 2.1870527267456055
Validation loss: 2.1429698890255344

Epoch: 5| Step: 10
Training loss: 1.547400712966919
Validation loss: 2.1500234347517773

Epoch: 381| Step: 0
Training loss: 2.1718106269836426
Validation loss: 2.159883378654398

Epoch: 5| Step: 1
Training loss: 2.702819347381592
Validation loss: 2.1486317137236237

Epoch: 5| Step: 2
Training loss: 1.3436026573181152
Validation loss: 2.160325857900804

Epoch: 5| Step: 3
Training loss: 2.3632051944732666
Validation loss: 2.168727136427356

Epoch: 5| Step: 4
Training loss: 2.387036085128784
Validation loss: 2.1830236322136334

Epoch: 5| Step: 5
Training loss: 2.189659833908081
Validation loss: 2.1835926553254486

Epoch: 5| Step: 6
Training loss: 3.13336181640625
Validation loss: 2.1728254082382366

Epoch: 5| Step: 7
Training loss: 2.637303590774536
Validation loss: 2.194554517346044

Epoch: 5| Step: 8
Training loss: 1.9977598190307617
Validation loss: 2.206922524718828

Epoch: 5| Step: 9
Training loss: 2.9209256172180176
Validation loss: 2.191518327241303

Epoch: 5| Step: 10
Training loss: 1.6582883596420288
Validation loss: 2.196302097330811

Epoch: 382| Step: 0
Training loss: 2.387157440185547
Validation loss: 2.1871496759435183

Epoch: 5| Step: 1
Training loss: 2.865333080291748
Validation loss: 2.186326201244067

Epoch: 5| Step: 2
Training loss: 2.165222644805908
Validation loss: 2.179697270034462

Epoch: 5| Step: 3
Training loss: 1.7833175659179688
Validation loss: 2.1601966606673373

Epoch: 5| Step: 4
Training loss: 2.114685297012329
Validation loss: 2.1842752233628304

Epoch: 5| Step: 5
Training loss: 2.1629536151885986
Validation loss: 2.1717712340816373

Epoch: 5| Step: 6
Training loss: 2.6953418254852295
Validation loss: 2.161618491654755

Epoch: 5| Step: 7
Training loss: 2.0680699348449707
Validation loss: 2.1668659333259828

Epoch: 5| Step: 8
Training loss: 2.08498477935791
Validation loss: 2.1744878163901706

Epoch: 5| Step: 9
Training loss: 2.803497314453125
Validation loss: 2.1669570156323013

Epoch: 5| Step: 10
Training loss: 2.41015625
Validation loss: 2.154784212830246

Epoch: 383| Step: 0
Training loss: 1.9558799266815186
Validation loss: 2.157812688940315

Epoch: 5| Step: 1
Training loss: 2.861030101776123
Validation loss: 2.1399509573495514

Epoch: 5| Step: 2
Training loss: 2.4678611755371094
Validation loss: 2.1597621107614167

Epoch: 5| Step: 3
Training loss: 1.885786771774292
Validation loss: 2.1405665746299167

Epoch: 5| Step: 4
Training loss: 2.6274819374084473
Validation loss: 2.1431172278619584

Epoch: 5| Step: 5
Training loss: 2.2812652587890625
Validation loss: 2.16243548803432

Epoch: 5| Step: 6
Training loss: 2.396815061569214
Validation loss: 2.148377413390785

Epoch: 5| Step: 7
Training loss: 1.871270775794983
Validation loss: 2.1573185407987205

Epoch: 5| Step: 8
Training loss: 1.3181064128875732
Validation loss: 2.151853956202025

Epoch: 5| Step: 9
Training loss: 3.3563785552978516
Validation loss: 2.1682524988728185

Epoch: 5| Step: 10
Training loss: 2.555050849914551
Validation loss: 2.1531209586769022

Epoch: 384| Step: 0
Training loss: 2.734553575515747
Validation loss: 2.1866712724008868

Epoch: 5| Step: 1
Training loss: 2.4077656269073486
Validation loss: 2.1669468392607985

Epoch: 5| Step: 2
Training loss: 2.1492841243743896
Validation loss: 2.1690673494851715

Epoch: 5| Step: 3
Training loss: 2.0760738849639893
Validation loss: 2.1677131511831798

Epoch: 5| Step: 4
Training loss: 1.3476152420043945
Validation loss: 2.1650290322560135

Epoch: 5| Step: 5
Training loss: 2.7968242168426514
Validation loss: 2.1639926292563

Epoch: 5| Step: 6
Training loss: 2.591407060623169
Validation loss: 2.1694879506223943

Epoch: 5| Step: 7
Training loss: 2.0400969982147217
Validation loss: 2.1812383667115243

Epoch: 5| Step: 8
Training loss: 2.3214805126190186
Validation loss: 2.1582011227966635

Epoch: 5| Step: 9
Training loss: 2.2758936882019043
Validation loss: 2.1683999261548443

Epoch: 5| Step: 10
Training loss: 2.800534725189209
Validation loss: 2.154503599289925

Epoch: 385| Step: 0
Training loss: 3.132176399230957
Validation loss: 2.1686748612311577

Epoch: 5| Step: 1
Training loss: 1.693255066871643
Validation loss: 2.1516759395599365

Epoch: 5| Step: 2
Training loss: 2.526951789855957
Validation loss: 2.1759738024844917

Epoch: 5| Step: 3
Training loss: 2.653679370880127
Validation loss: 2.169608546841529

Epoch: 5| Step: 4
Training loss: 1.8412494659423828
Validation loss: 2.175080289122879

Epoch: 5| Step: 5
Training loss: 2.2416436672210693
Validation loss: 2.1757103332909207

Epoch: 5| Step: 6
Training loss: 2.0596258640289307
Validation loss: 2.183953054489628

Epoch: 5| Step: 7
Training loss: 2.4961049556732178
Validation loss: 2.171925034574283

Epoch: 5| Step: 8
Training loss: 2.0877952575683594
Validation loss: 2.174006103187479

Epoch: 5| Step: 9
Training loss: 2.251354932785034
Validation loss: 2.189730609616926

Epoch: 5| Step: 10
Training loss: 2.5458405017852783
Validation loss: 2.1954646648899203

Epoch: 386| Step: 0
Training loss: 2.223025321960449
Validation loss: 2.1735198164498932

Epoch: 5| Step: 1
Training loss: 2.6551589965820312
Validation loss: 2.17068055880967

Epoch: 5| Step: 2
Training loss: 2.783538341522217
Validation loss: 2.17220498413168

Epoch: 5| Step: 3
Training loss: 2.6556637287139893
Validation loss: 2.171348792250438

Epoch: 5| Step: 4
Training loss: 2.017169237136841
Validation loss: 2.1587478781259186

Epoch: 5| Step: 5
Training loss: 2.263277530670166
Validation loss: 2.1667345544343353

Epoch: 5| Step: 6
Training loss: 1.9698318243026733
Validation loss: 2.172105235438193

Epoch: 5| Step: 7
Training loss: 2.237208604812622
Validation loss: 2.152341406832459

Epoch: 5| Step: 8
Training loss: 2.214578866958618
Validation loss: 2.15273884547654

Epoch: 5| Step: 9
Training loss: 2.100571870803833
Validation loss: 2.143300765304155

Epoch: 5| Step: 10
Training loss: 2.331989049911499
Validation loss: 2.1473186298083236

Epoch: 387| Step: 0
Training loss: 1.7361218929290771
Validation loss: 2.144507390196605

Epoch: 5| Step: 1
Training loss: 1.707463026046753
Validation loss: 2.1384811119366716

Epoch: 5| Step: 2
Training loss: 2.1969878673553467
Validation loss: 2.1477510903471257

Epoch: 5| Step: 3
Training loss: 2.445948600769043
Validation loss: 2.1469265491731706

Epoch: 5| Step: 4
Training loss: 1.9210994243621826
Validation loss: 2.1609358633718183

Epoch: 5| Step: 5
Training loss: 2.743577480316162
Validation loss: 2.1622054294873307

Epoch: 5| Step: 6
Training loss: 2.667095899581909
Validation loss: 2.1661598925949423

Epoch: 5| Step: 7
Training loss: 2.9232001304626465
Validation loss: 2.1590481881172425

Epoch: 5| Step: 8
Training loss: 2.319611072540283
Validation loss: 2.168937046040771

Epoch: 5| Step: 9
Training loss: 2.8261427879333496
Validation loss: 2.150379107844445

Epoch: 5| Step: 10
Training loss: 1.8448492288589478
Validation loss: 2.1423021260128228

Epoch: 388| Step: 0
Training loss: 2.0282790660858154
Validation loss: 2.1409656642585673

Epoch: 5| Step: 1
Training loss: 2.07222580909729
Validation loss: 2.146138043813808

Epoch: 5| Step: 2
Training loss: 2.561511754989624
Validation loss: 2.1440400987543087

Epoch: 5| Step: 3
Training loss: 2.478877544403076
Validation loss: 2.1390259778627785

Epoch: 5| Step: 4
Training loss: 2.167126178741455
Validation loss: 2.1360372087006927

Epoch: 5| Step: 5
Training loss: 2.0105271339416504
Validation loss: 2.145066774019631

Epoch: 5| Step: 6
Training loss: 2.529034376144409
Validation loss: 2.1348917920102357

Epoch: 5| Step: 7
Training loss: 2.889601707458496
Validation loss: 2.1510364137670046

Epoch: 5| Step: 8
Training loss: 1.9665790796279907
Validation loss: 2.146143615886729

Epoch: 5| Step: 9
Training loss: 2.389360189437866
Validation loss: 2.1506751762923373

Epoch: 5| Step: 10
Training loss: 2.270944595336914
Validation loss: 2.1533239336423975

Epoch: 389| Step: 0
Training loss: 1.5985819101333618
Validation loss: 2.1493304467970327

Epoch: 5| Step: 1
Training loss: 2.039670944213867
Validation loss: 2.167536445843276

Epoch: 5| Step: 2
Training loss: 2.072648048400879
Validation loss: 2.1692071883909163

Epoch: 5| Step: 3
Training loss: 2.429518461227417
Validation loss: 2.162167364551175

Epoch: 5| Step: 4
Training loss: 2.1899960041046143
Validation loss: 2.17456938118063

Epoch: 5| Step: 5
Training loss: 2.557473659515381
Validation loss: 2.153895314021777

Epoch: 5| Step: 6
Training loss: 2.5044498443603516
Validation loss: 2.1532685295228036

Epoch: 5| Step: 7
Training loss: 2.640913486480713
Validation loss: 2.1438560844749532

Epoch: 5| Step: 8
Training loss: 2.3263840675354004
Validation loss: 2.1414298139592653

Epoch: 5| Step: 9
Training loss: 2.4386515617370605
Validation loss: 2.1341926154269966

Epoch: 5| Step: 10
Training loss: 2.60131573677063
Validation loss: 2.1579495629956646

Epoch: 390| Step: 0
Training loss: 2.699333906173706
Validation loss: 2.1297168424052577

Epoch: 5| Step: 1
Training loss: 1.7160289287567139
Validation loss: 2.145426845037809

Epoch: 5| Step: 2
Training loss: 2.9952666759490967
Validation loss: 2.120110434870566

Epoch: 5| Step: 3
Training loss: 1.7501055002212524
Validation loss: 2.1322281181171374

Epoch: 5| Step: 4
Training loss: 2.732353925704956
Validation loss: 2.1452408888006724

Epoch: 5| Step: 5
Training loss: 2.131500244140625
Validation loss: 2.1539967803544897

Epoch: 5| Step: 6
Training loss: 1.5932095050811768
Validation loss: 2.150142072349466

Epoch: 5| Step: 7
Training loss: 2.0790305137634277
Validation loss: 2.144247704936612

Epoch: 5| Step: 8
Training loss: 2.503488063812256
Validation loss: 2.1505599393639514

Epoch: 5| Step: 9
Training loss: 2.499098539352417
Validation loss: 2.163870975535403

Epoch: 5| Step: 10
Training loss: 2.8875651359558105
Validation loss: 2.1726844669670187

Epoch: 391| Step: 0
Training loss: 1.7370468378067017
Validation loss: 2.176452736700735

Epoch: 5| Step: 1
Training loss: 2.817476511001587
Validation loss: 2.1653870587707846

Epoch: 5| Step: 2
Training loss: 2.028789758682251
Validation loss: 2.1476613385702974

Epoch: 5| Step: 3
Training loss: 2.2875819206237793
Validation loss: 2.1576883023785007

Epoch: 5| Step: 4
Training loss: 2.485490322113037
Validation loss: 2.1419760334876274

Epoch: 5| Step: 5
Training loss: 1.8998641967773438
Validation loss: 2.1418046925657537

Epoch: 5| Step: 6
Training loss: 2.6223087310791016
Validation loss: 2.1183667913559945

Epoch: 5| Step: 7
Training loss: 2.610543727874756
Validation loss: 2.1271570395397883

Epoch: 5| Step: 8
Training loss: 2.639137029647827
Validation loss: 2.12965008904857

Epoch: 5| Step: 9
Training loss: 2.2957096099853516
Validation loss: 2.130530006142073

Epoch: 5| Step: 10
Training loss: 1.8810838460922241
Validation loss: 2.1401662018991288

Epoch: 392| Step: 0
Training loss: 2.0827949047088623
Validation loss: 2.128279427046417

Epoch: 5| Step: 1
Training loss: 1.7352206707000732
Validation loss: 2.13295562805668

Epoch: 5| Step: 2
Training loss: 2.208549976348877
Validation loss: 2.1332439068825013

Epoch: 5| Step: 3
Training loss: 2.582294225692749
Validation loss: 2.123160103315948

Epoch: 5| Step: 4
Training loss: 1.8903964757919312
Validation loss: 2.1377800651775893

Epoch: 5| Step: 5
Training loss: 2.1021292209625244
Validation loss: 2.1401618091008996

Epoch: 5| Step: 6
Training loss: 2.127751588821411
Validation loss: 2.1449302601557907

Epoch: 5| Step: 7
Training loss: 3.017345428466797
Validation loss: 2.1404853687491467

Epoch: 5| Step: 8
Training loss: 2.1009738445281982
Validation loss: 2.1335442463556924

Epoch: 5| Step: 9
Training loss: 2.554680347442627
Validation loss: 2.141504177483179

Epoch: 5| Step: 10
Training loss: 3.0282514095306396
Validation loss: 2.166683525167486

Epoch: 393| Step: 0
Training loss: 1.8658519983291626
Validation loss: 2.1489984860984226

Epoch: 5| Step: 1
Training loss: 2.8338654041290283
Validation loss: 2.135814200165451

Epoch: 5| Step: 2
Training loss: 1.8913898468017578
Validation loss: 2.156627124355685

Epoch: 5| Step: 3
Training loss: 1.6740314960479736
Validation loss: 2.1710120836893716

Epoch: 5| Step: 4
Training loss: 2.0983734130859375
Validation loss: 2.1225945795736005

Epoch: 5| Step: 5
Training loss: 3.2041239738464355
Validation loss: 2.132889775819676

Epoch: 5| Step: 6
Training loss: 2.854281187057495
Validation loss: 2.1672359384516233

Epoch: 5| Step: 7
Training loss: 2.3814241886138916
Validation loss: 2.1696216162814888

Epoch: 5| Step: 8
Training loss: 2.3646836280822754
Validation loss: 2.1649682303910613

Epoch: 5| Step: 9
Training loss: 2.5805301666259766
Validation loss: 2.1639640702996203

Epoch: 5| Step: 10
Training loss: 1.527831792831421
Validation loss: 2.133218373021772

Epoch: 394| Step: 0
Training loss: 2.1425671577453613
Validation loss: 2.145973406812196

Epoch: 5| Step: 1
Training loss: 2.454533576965332
Validation loss: 2.1432683519137803

Epoch: 5| Step: 2
Training loss: 2.4484355449676514
Validation loss: 2.126090988036125

Epoch: 5| Step: 3
Training loss: 1.9167503118515015
Validation loss: 2.1247353015407437

Epoch: 5| Step: 4
Training loss: 2.006390333175659
Validation loss: 2.140651064534341

Epoch: 5| Step: 5
Training loss: 2.2559239864349365
Validation loss: 2.1305771643115627

Epoch: 5| Step: 6
Training loss: 2.745579719543457
Validation loss: 2.1292919317881265

Epoch: 5| Step: 7
Training loss: 1.9784218072891235
Validation loss: 2.1520655129545476

Epoch: 5| Step: 8
Training loss: 2.491398572921753
Validation loss: 2.1457762000381306

Epoch: 5| Step: 9
Training loss: 2.1873390674591064
Validation loss: 2.164069608975482

Epoch: 5| Step: 10
Training loss: 2.739966630935669
Validation loss: 2.1457281215216524

Epoch: 395| Step: 0
Training loss: 2.478559970855713
Validation loss: 2.1366788264243834

Epoch: 5| Step: 1
Training loss: 2.673874616622925
Validation loss: 2.1613765890880297

Epoch: 5| Step: 2
Training loss: 2.7695040702819824
Validation loss: 2.1620357228863623

Epoch: 5| Step: 3
Training loss: 2.3899483680725098
Validation loss: 2.1536411341800483

Epoch: 5| Step: 4
Training loss: 2.5771422386169434
Validation loss: 2.1341467877869964

Epoch: 5| Step: 5
Training loss: 2.3733325004577637
Validation loss: 2.1351031436715076

Epoch: 5| Step: 6
Training loss: 2.1638922691345215
Validation loss: 2.1327323247027654

Epoch: 5| Step: 7
Training loss: 2.022507905960083
Validation loss: 2.1342549606036116

Epoch: 5| Step: 8
Training loss: 1.9421577453613281
Validation loss: 2.1553026501850416

Epoch: 5| Step: 9
Training loss: 2.042637825012207
Validation loss: 2.1539336122492307

Epoch: 5| Step: 10
Training loss: 1.8251433372497559
Validation loss: 2.1414928820825394

Epoch: 396| Step: 0
Training loss: 2.847806215286255
Validation loss: 2.146804025096278

Epoch: 5| Step: 1
Training loss: 1.9704824686050415
Validation loss: 2.1709985656123005

Epoch: 5| Step: 2
Training loss: 1.9460361003875732
Validation loss: 2.1703559967779342

Epoch: 5| Step: 3
Training loss: 2.4475302696228027
Validation loss: 2.170761913381597

Epoch: 5| Step: 4
Training loss: 2.307138204574585
Validation loss: 2.1599683069413707

Epoch: 5| Step: 5
Training loss: 1.6647460460662842
Validation loss: 2.1169402189152215

Epoch: 5| Step: 6
Training loss: 2.309929609298706
Validation loss: 2.124642284967566

Epoch: 5| Step: 7
Training loss: 2.9320998191833496
Validation loss: 2.1410122391998128

Epoch: 5| Step: 8
Training loss: 2.8006157875061035
Validation loss: 2.150639513487457

Epoch: 5| Step: 9
Training loss: 1.8608076572418213
Validation loss: 2.1445061006853656

Epoch: 5| Step: 10
Training loss: 2.366635799407959
Validation loss: 2.143010570156959

Epoch: 397| Step: 0
Training loss: 2.4737496376037598
Validation loss: 2.146877001690608

Epoch: 5| Step: 1
Training loss: 2.1778130531311035
Validation loss: 2.126970301392258

Epoch: 5| Step: 2
Training loss: 2.614473581314087
Validation loss: 2.130758088122132

Epoch: 5| Step: 3
Training loss: 2.521632194519043
Validation loss: 2.141121018317438

Epoch: 5| Step: 4
Training loss: 1.9340779781341553
Validation loss: 2.1277746692780526

Epoch: 5| Step: 5
Training loss: 2.5466878414154053
Validation loss: 2.1336251945905786

Epoch: 5| Step: 6
Training loss: 1.9895832538604736
Validation loss: 2.13579366284032

Epoch: 5| Step: 7
Training loss: 2.5182652473449707
Validation loss: 2.1274582006598033

Epoch: 5| Step: 8
Training loss: 1.701195478439331
Validation loss: 2.156626160426806

Epoch: 5| Step: 9
Training loss: 2.3422863483428955
Validation loss: 2.1643270266953336

Epoch: 5| Step: 10
Training loss: 2.464843273162842
Validation loss: 2.1582016150156655

Epoch: 398| Step: 0
Training loss: 2.4280083179473877
Validation loss: 2.1537198661476054

Epoch: 5| Step: 1
Training loss: 2.511507511138916
Validation loss: 2.138931500014438

Epoch: 5| Step: 2
Training loss: 1.753519058227539
Validation loss: 2.136179695847214

Epoch: 5| Step: 3
Training loss: 2.2550153732299805
Validation loss: 2.140579982470441

Epoch: 5| Step: 4
Training loss: 1.9105021953582764
Validation loss: 2.1474112233807965

Epoch: 5| Step: 5
Training loss: 2.5657596588134766
Validation loss: 2.154897730837586

Epoch: 5| Step: 6
Training loss: 2.7852559089660645
Validation loss: 2.1394661293234876

Epoch: 5| Step: 7
Training loss: 2.8454501628875732
Validation loss: 2.1537710338510494

Epoch: 5| Step: 8
Training loss: 2.175671339035034
Validation loss: 2.1637986501057944

Epoch: 5| Step: 9
Training loss: 2.1658623218536377
Validation loss: 2.156169536293194

Epoch: 5| Step: 10
Training loss: 1.9949289560317993
Validation loss: 2.1496266229178316

Epoch: 399| Step: 0
Training loss: 2.9385464191436768
Validation loss: 2.1642282124488585

Epoch: 5| Step: 1
Training loss: 2.160317897796631
Validation loss: 2.1729115529726912

Epoch: 5| Step: 2
Training loss: 1.6747949123382568
Validation loss: 2.178179507614464

Epoch: 5| Step: 3
Training loss: 2.3239834308624268
Validation loss: 2.1704408173920005

Epoch: 5| Step: 4
Training loss: 2.5817711353302
Validation loss: 2.160151163736979

Epoch: 5| Step: 5
Training loss: 1.6290489435195923
Validation loss: 2.146386638764412

Epoch: 5| Step: 6
Training loss: 1.5681387186050415
Validation loss: 2.136983638168663

Epoch: 5| Step: 7
Training loss: 2.828948497772217
Validation loss: 2.1374213951890186

Epoch: 5| Step: 8
Training loss: 2.5235230922698975
Validation loss: 2.1328076085736676

Epoch: 5| Step: 9
Training loss: 2.55739164352417
Validation loss: 2.1341983502910984

Epoch: 5| Step: 10
Training loss: 2.4131217002868652
Validation loss: 2.1504643732501614

Epoch: 400| Step: 0
Training loss: 2.5928142070770264
Validation loss: 2.140430583748766

Epoch: 5| Step: 1
Training loss: 2.304633140563965
Validation loss: 2.133502596168108

Epoch: 5| Step: 2
Training loss: 2.890989065170288
Validation loss: 2.1195110018535326

Epoch: 5| Step: 3
Training loss: 2.1178901195526123
Validation loss: 2.1205582439258532

Epoch: 5| Step: 4
Training loss: 2.13578724861145
Validation loss: 2.149565530079667

Epoch: 5| Step: 5
Training loss: 2.494194507598877
Validation loss: 2.150701302354054

Epoch: 5| Step: 6
Training loss: 2.309232234954834
Validation loss: 2.1527279628220426

Epoch: 5| Step: 7
Training loss: 1.796219825744629
Validation loss: 2.1524258531549925

Epoch: 5| Step: 8
Training loss: 2.415980100631714
Validation loss: 2.1903765637387513

Epoch: 5| Step: 9
Training loss: 2.4363138675689697
Validation loss: 2.163040377760446

Epoch: 5| Step: 10
Training loss: 1.751665472984314
Validation loss: 2.174229819287536

Epoch: 401| Step: 0
Training loss: 2.132279872894287
Validation loss: 2.1496550472833778

Epoch: 5| Step: 1
Training loss: 2.2397713661193848
Validation loss: 2.121275630048526

Epoch: 5| Step: 2
Training loss: 2.3979856967926025
Validation loss: 2.127559326028311

Epoch: 5| Step: 3
Training loss: 2.792693853378296
Validation loss: 2.1233353973716818

Epoch: 5| Step: 4
Training loss: 2.584010362625122
Validation loss: 2.1115651310131116

Epoch: 5| Step: 5
Training loss: 2.096346616744995
Validation loss: 2.1214016663130892

Epoch: 5| Step: 6
Training loss: 2.263796329498291
Validation loss: 2.111520962048602

Epoch: 5| Step: 7
Training loss: 2.2802088260650635
Validation loss: 2.12832223984503

Epoch: 5| Step: 8
Training loss: 2.126310348510742
Validation loss: 2.1309412833183043

Epoch: 5| Step: 9
Training loss: 2.010385274887085
Validation loss: 2.113565129618491

Epoch: 5| Step: 10
Training loss: 2.4767067432403564
Validation loss: 2.125125806818726

Epoch: 402| Step: 0
Training loss: 1.8184579610824585
Validation loss: 2.1241901946324173

Epoch: 5| Step: 1
Training loss: 2.2590091228485107
Validation loss: 2.111711255965694

Epoch: 5| Step: 2
Training loss: 2.4774184226989746
Validation loss: 2.111245926990304

Epoch: 5| Step: 3
Training loss: 3.0916740894317627
Validation loss: 2.1323113595285723

Epoch: 5| Step: 4
Training loss: 1.9256985187530518
Validation loss: 2.123695960608862

Epoch: 5| Step: 5
Training loss: 2.281093120574951
Validation loss: 2.1234061333440963

Epoch: 5| Step: 6
Training loss: 2.5931105613708496
Validation loss: 2.1209510808349936

Epoch: 5| Step: 7
Training loss: 2.1018595695495605
Validation loss: 2.1318291694887224

Epoch: 5| Step: 8
Training loss: 2.3524951934814453
Validation loss: 2.152599747462939

Epoch: 5| Step: 9
Training loss: 2.3242897987365723
Validation loss: 2.1329882862747356

Epoch: 5| Step: 10
Training loss: 1.9307405948638916
Validation loss: 2.136040227387541

Epoch: 403| Step: 0
Training loss: 2.2033801078796387
Validation loss: 2.1382251221646547

Epoch: 5| Step: 1
Training loss: 2.3357512950897217
Validation loss: 2.136386855956047

Epoch: 5| Step: 2
Training loss: 1.8921512365341187
Validation loss: 2.114537969712288

Epoch: 5| Step: 3
Training loss: 2.711050510406494
Validation loss: 2.1141385288648706

Epoch: 5| Step: 4
Training loss: 2.2704086303710938
Validation loss: 2.1082651461324384

Epoch: 5| Step: 5
Training loss: 2.476355791091919
Validation loss: 2.1142526826550885

Epoch: 5| Step: 6
Training loss: 2.605149745941162
Validation loss: 2.1163388067676174

Epoch: 5| Step: 7
Training loss: 2.76395320892334
Validation loss: 2.1209173997243247

Epoch: 5| Step: 8
Training loss: 1.7580063343048096
Validation loss: 2.13534309787135

Epoch: 5| Step: 9
Training loss: 2.186371326446533
Validation loss: 2.1220917637630174

Epoch: 5| Step: 10
Training loss: 1.9201316833496094
Validation loss: 2.114944951508635

Epoch: 404| Step: 0
Training loss: 2.397726058959961
Validation loss: 2.1238368608618297

Epoch: 5| Step: 1
Training loss: 2.145735502243042
Validation loss: 2.12606995080107

Epoch: 5| Step: 2
Training loss: 1.7651798725128174
Validation loss: 2.1071109771728516

Epoch: 5| Step: 3
Training loss: 2.5866286754608154
Validation loss: 2.127220871627972

Epoch: 5| Step: 4
Training loss: 1.8983142375946045
Validation loss: 2.1304857859047512

Epoch: 5| Step: 5
Training loss: 2.482440233230591
Validation loss: 2.1493204921804447

Epoch: 5| Step: 6
Training loss: 2.5535378456115723
Validation loss: 2.156006929694965

Epoch: 5| Step: 7
Training loss: 2.964073657989502
Validation loss: 2.1390327035739856

Epoch: 5| Step: 8
Training loss: 1.9252370595932007
Validation loss: 2.14720122532178

Epoch: 5| Step: 9
Training loss: 2.359358549118042
Validation loss: 2.1460026630791287

Epoch: 5| Step: 10
Training loss: 1.908373236656189
Validation loss: 2.152326504389445

Epoch: 405| Step: 0
Training loss: 2.180659055709839
Validation loss: 2.1288262195484613

Epoch: 5| Step: 1
Training loss: 2.296635627746582
Validation loss: 2.1380520046398206

Epoch: 5| Step: 2
Training loss: 2.6210105419158936
Validation loss: 2.1363451429592666

Epoch: 5| Step: 3
Training loss: 2.4004452228546143
Validation loss: 2.1389001402803647

Epoch: 5| Step: 4
Training loss: 2.5586423873901367
Validation loss: 2.121400310147193

Epoch: 5| Step: 5
Training loss: 1.6337741613388062
Validation loss: 2.131092402242845

Epoch: 5| Step: 6
Training loss: 2.0817339420318604
Validation loss: 2.1389154041967084

Epoch: 5| Step: 7
Training loss: 2.892972707748413
Validation loss: 2.134543408629715

Epoch: 5| Step: 8
Training loss: 2.0804600715637207
Validation loss: 2.1353151541884228

Epoch: 5| Step: 9
Training loss: 2.2751481533050537
Validation loss: 2.14996265339595

Epoch: 5| Step: 10
Training loss: 2.00054669380188
Validation loss: 2.1477152096327914

Epoch: 406| Step: 0
Training loss: 2.288700819015503
Validation loss: 2.134702400494647

Epoch: 5| Step: 1
Training loss: 3.0283045768737793
Validation loss: 2.142147433373236

Epoch: 5| Step: 2
Training loss: 1.9675136804580688
Validation loss: 2.1490986526653333

Epoch: 5| Step: 3
Training loss: 2.253213405609131
Validation loss: 2.1361881738067954

Epoch: 5| Step: 4
Training loss: 2.3259501457214355
Validation loss: 2.1296787082508044

Epoch: 5| Step: 5
Training loss: 2.6031136512756348
Validation loss: 2.147347504092801

Epoch: 5| Step: 6
Training loss: 1.4301332235336304
Validation loss: 2.126338871576453

Epoch: 5| Step: 7
Training loss: 2.257202625274658
Validation loss: 2.1414586254345473

Epoch: 5| Step: 8
Training loss: 2.427225112915039
Validation loss: 2.1273381017869517

Epoch: 5| Step: 9
Training loss: 2.5608139038085938
Validation loss: 2.1252413565112698

Epoch: 5| Step: 10
Training loss: 1.8393833637237549
Validation loss: 2.1329297916863554

Epoch: 407| Step: 0
Training loss: 2.0788733959198
Validation loss: 2.1305190401692546

Epoch: 5| Step: 1
Training loss: 1.9902255535125732
Validation loss: 2.122047383298156

Epoch: 5| Step: 2
Training loss: 2.166175127029419
Validation loss: 2.1170681958557456

Epoch: 5| Step: 3
Training loss: 2.6599862575531006
Validation loss: 2.1115612676066737

Epoch: 5| Step: 4
Training loss: 3.165769100189209
Validation loss: 2.1128443069355463

Epoch: 5| Step: 5
Training loss: 2.2090861797332764
Validation loss: 2.1160791150985228

Epoch: 5| Step: 6
Training loss: 2.171945095062256
Validation loss: 2.127799290482716

Epoch: 5| Step: 7
Training loss: 2.008435010910034
Validation loss: 2.123881052899104

Epoch: 5| Step: 8
Training loss: 2.015937328338623
Validation loss: 2.1430534983194

Epoch: 5| Step: 9
Training loss: 2.1831133365631104
Validation loss: 2.128778399959687

Epoch: 5| Step: 10
Training loss: 2.3556859493255615
Validation loss: 2.128938308326147

Epoch: 408| Step: 0
Training loss: 2.3829345703125
Validation loss: 2.141247428873534

Epoch: 5| Step: 1
Training loss: 1.5159615278244019
Validation loss: 2.121785238224973

Epoch: 5| Step: 2
Training loss: 1.5305441617965698
Validation loss: 2.100585313253505

Epoch: 5| Step: 3
Training loss: 2.288447618484497
Validation loss: 2.1041184804772817

Epoch: 5| Step: 4
Training loss: 3.342042922973633
Validation loss: 2.1099149027178363

Epoch: 5| Step: 5
Training loss: 1.9482402801513672
Validation loss: 2.1103916668122813

Epoch: 5| Step: 6
Training loss: 2.4502224922180176
Validation loss: 2.1210796320310203

Epoch: 5| Step: 7
Training loss: 2.1247875690460205
Validation loss: 2.1164966219214985

Epoch: 5| Step: 8
Training loss: 2.889193534851074
Validation loss: 2.1034230327093475

Epoch: 5| Step: 9
Training loss: 2.6320464611053467
Validation loss: 2.1376848374643633

Epoch: 5| Step: 10
Training loss: 1.9380322694778442
Validation loss: 2.1032367278170843

Epoch: 409| Step: 0
Training loss: 1.9976832866668701
Validation loss: 2.139611287783551

Epoch: 5| Step: 1
Training loss: 2.8120651245117188
Validation loss: 2.107068738629741

Epoch: 5| Step: 2
Training loss: 2.0323073863983154
Validation loss: 2.1255250720567602

Epoch: 5| Step: 3
Training loss: 2.2545008659362793
Validation loss: 2.142359910472747

Epoch: 5| Step: 4
Training loss: 2.232445240020752
Validation loss: 2.136327948621524

Epoch: 5| Step: 5
Training loss: 1.3797388076782227
Validation loss: 2.1570943555524273

Epoch: 5| Step: 6
Training loss: 2.6709089279174805
Validation loss: 2.1544538620979554

Epoch: 5| Step: 7
Training loss: 2.3679611682891846
Validation loss: 2.1577424721051286

Epoch: 5| Step: 8
Training loss: 2.632871150970459
Validation loss: 2.1514970820437194

Epoch: 5| Step: 9
Training loss: 2.2160751819610596
Validation loss: 2.145650476537725

Epoch: 5| Step: 10
Training loss: 2.5302183628082275
Validation loss: 2.110422932973472

Epoch: 410| Step: 0
Training loss: 2.293504476547241
Validation loss: 2.109352737344721

Epoch: 5| Step: 1
Training loss: 1.8388961553573608
Validation loss: 2.1120376433095625

Epoch: 5| Step: 2
Training loss: 2.598038673400879
Validation loss: 2.110658279029272

Epoch: 5| Step: 3
Training loss: 2.4437978267669678
Validation loss: 2.1172943615144297

Epoch: 5| Step: 4
Training loss: 2.4358928203582764
Validation loss: 2.138497212881683

Epoch: 5| Step: 5
Training loss: 2.737154245376587
Validation loss: 2.1245640913645425

Epoch: 5| Step: 6
Training loss: 2.319305896759033
Validation loss: 2.107732933054688

Epoch: 5| Step: 7
Training loss: 1.4207792282104492
Validation loss: 2.1149296311921972

Epoch: 5| Step: 8
Training loss: 2.2166085243225098
Validation loss: 2.1176600917693107

Epoch: 5| Step: 9
Training loss: 2.212446689605713
Validation loss: 2.1301928027983634

Epoch: 5| Step: 10
Training loss: 2.629240036010742
Validation loss: 2.119061493104504

Epoch: 411| Step: 0
Training loss: 2.2781596183776855
Validation loss: 2.1221555099692395

Epoch: 5| Step: 1
Training loss: 2.1272695064544678
Validation loss: 2.123060364877024

Epoch: 5| Step: 2
Training loss: 2.4327187538146973
Validation loss: 2.1199838666505713

Epoch: 5| Step: 3
Training loss: 2.294435977935791
Validation loss: 2.136443204777215

Epoch: 5| Step: 4
Training loss: 2.78259015083313
Validation loss: 2.1125991370088313

Epoch: 5| Step: 5
Training loss: 1.812238097190857
Validation loss: 2.102418427826256

Epoch: 5| Step: 6
Training loss: 2.0030930042266846
Validation loss: 2.121092488688807

Epoch: 5| Step: 7
Training loss: 2.2256431579589844
Validation loss: 2.1203021105899604

Epoch: 5| Step: 8
Training loss: 1.6887279748916626
Validation loss: 2.1161858445854596

Epoch: 5| Step: 9
Training loss: 2.547330379486084
Validation loss: 2.117176007199031

Epoch: 5| Step: 10
Training loss: 2.7701516151428223
Validation loss: 2.117469351778748

Epoch: 412| Step: 0
Training loss: 1.7670440673828125
Validation loss: 2.1202376427189

Epoch: 5| Step: 1
Training loss: 2.5415778160095215
Validation loss: 2.1165506173205633

Epoch: 5| Step: 2
Training loss: 3.328972578048706
Validation loss: 2.112659505618516

Epoch: 5| Step: 3
Training loss: 2.5459280014038086
Validation loss: 2.112007955069183

Epoch: 5| Step: 4
Training loss: 2.1893117427825928
Validation loss: 2.1274560113107004

Epoch: 5| Step: 5
Training loss: 1.8087574243545532
Validation loss: 2.1397377752488658

Epoch: 5| Step: 6
Training loss: 1.9075477123260498
Validation loss: 2.14331970419935

Epoch: 5| Step: 7
Training loss: 2.2581419944763184
Validation loss: 2.158504383538359

Epoch: 5| Step: 8
Training loss: 2.1856188774108887
Validation loss: 2.1393087166611866

Epoch: 5| Step: 9
Training loss: 2.4296553134918213
Validation loss: 2.143508388150123

Epoch: 5| Step: 10
Training loss: 1.9020917415618896
Validation loss: 2.146726001975357

Epoch: 413| Step: 0
Training loss: 2.285006046295166
Validation loss: 2.120717763900757

Epoch: 5| Step: 1
Training loss: 2.947993040084839
Validation loss: 2.1158596905328895

Epoch: 5| Step: 2
Training loss: 1.7495920658111572
Validation loss: 2.1213219627257316

Epoch: 5| Step: 3
Training loss: 2.187819719314575
Validation loss: 2.1138812085633636

Epoch: 5| Step: 4
Training loss: 1.9614320993423462
Validation loss: 2.1326675030492965

Epoch: 5| Step: 5
Training loss: 2.3089559078216553
Validation loss: 2.1487010371300483

Epoch: 5| Step: 6
Training loss: 2.5750460624694824
Validation loss: 2.155584184072351

Epoch: 5| Step: 7
Training loss: 2.7777957916259766
Validation loss: 2.184786541487581

Epoch: 5| Step: 8
Training loss: 1.9841585159301758
Validation loss: 2.169275091540429

Epoch: 5| Step: 9
Training loss: 2.601510524749756
Validation loss: 2.1699074622123473

Epoch: 5| Step: 10
Training loss: 1.4775105714797974
Validation loss: 2.1378903850432365

Epoch: 414| Step: 0
Training loss: 2.5253207683563232
Validation loss: 2.1241832522935766

Epoch: 5| Step: 1
Training loss: 2.3039886951446533
Validation loss: 2.123111836371883

Epoch: 5| Step: 2
Training loss: 2.115440607070923
Validation loss: 2.1186253665595927

Epoch: 5| Step: 3
Training loss: 1.9346692562103271
Validation loss: 2.12758586611799

Epoch: 5| Step: 4
Training loss: 1.6678760051727295
Validation loss: 2.1271325772808445

Epoch: 5| Step: 5
Training loss: 2.3660330772399902
Validation loss: 2.1232159624817553

Epoch: 5| Step: 6
Training loss: 2.7280189990997314
Validation loss: 2.1110715173905894

Epoch: 5| Step: 7
Training loss: 2.2901949882507324
Validation loss: 2.1186773930826495

Epoch: 5| Step: 8
Training loss: 2.2305054664611816
Validation loss: 2.127250794441469

Epoch: 5| Step: 9
Training loss: 2.8384757041931152
Validation loss: 2.1215676197441677

Epoch: 5| Step: 10
Training loss: 2.146789073944092
Validation loss: 2.1262921030803392

Epoch: 415| Step: 0
Training loss: 2.4487130641937256
Validation loss: 2.1345144869178854

Epoch: 5| Step: 1
Training loss: 3.253328800201416
Validation loss: 2.146092850674865

Epoch: 5| Step: 2
Training loss: 2.3481268882751465
Validation loss: 2.118152549189906

Epoch: 5| Step: 3
Training loss: 1.811985969543457
Validation loss: 2.1115260252388577

Epoch: 5| Step: 4
Training loss: 2.097402572631836
Validation loss: 2.113178846656635

Epoch: 5| Step: 5
Training loss: 1.9206393957138062
Validation loss: 2.1114152503269974

Epoch: 5| Step: 6
Training loss: 2.130666732788086
Validation loss: 2.1054555857053368

Epoch: 5| Step: 7
Training loss: 1.6567068099975586
Validation loss: 2.114693372480331

Epoch: 5| Step: 8
Training loss: 2.1841976642608643
Validation loss: 2.103880238789384

Epoch: 5| Step: 9
Training loss: 2.5764541625976562
Validation loss: 2.1016984537083614

Epoch: 5| Step: 10
Training loss: 2.5662097930908203
Validation loss: 2.1073672130543697

Epoch: 416| Step: 0
Training loss: 1.532531976699829
Validation loss: 2.1280791785127375

Epoch: 5| Step: 1
Training loss: 1.5331666469573975
Validation loss: 2.1183041308515813

Epoch: 5| Step: 2
Training loss: 2.3123764991760254
Validation loss: 2.1124836155163345

Epoch: 5| Step: 3
Training loss: 1.9742320775985718
Validation loss: 2.117307975728025

Epoch: 5| Step: 4
Training loss: 2.7632031440734863
Validation loss: 2.111656565820017

Epoch: 5| Step: 5
Training loss: 2.514801025390625
Validation loss: 2.128259652404375

Epoch: 5| Step: 6
Training loss: 2.1045987606048584
Validation loss: 2.140035185762631

Epoch: 5| Step: 7
Training loss: 2.5378952026367188
Validation loss: 2.1427930452490367

Epoch: 5| Step: 8
Training loss: 2.1752076148986816
Validation loss: 2.1308398131401307

Epoch: 5| Step: 9
Training loss: 3.0734262466430664
Validation loss: 2.1231762760428974

Epoch: 5| Step: 10
Training loss: 2.493232488632202
Validation loss: 2.1227107150580293

Epoch: 417| Step: 0
Training loss: 2.452529191970825
Validation loss: 2.1156747546247257

Epoch: 5| Step: 1
Training loss: 2.489287853240967
Validation loss: 2.102647855717649

Epoch: 5| Step: 2
Training loss: 2.3273751735687256
Validation loss: 2.1288947700172343

Epoch: 5| Step: 3
Training loss: 2.5383846759796143
Validation loss: 2.138899423742807

Epoch: 5| Step: 4
Training loss: 2.550877809524536
Validation loss: 2.119091349263345

Epoch: 5| Step: 5
Training loss: 2.246983051300049
Validation loss: 2.108865507187382

Epoch: 5| Step: 6
Training loss: 2.435467481613159
Validation loss: 2.1111681243424774

Epoch: 5| Step: 7
Training loss: 1.4012752771377563
Validation loss: 2.103743600588973

Epoch: 5| Step: 8
Training loss: 2.2503414154052734
Validation loss: 2.1145998290790025

Epoch: 5| Step: 9
Training loss: 2.484731674194336
Validation loss: 2.10357569366373

Epoch: 5| Step: 10
Training loss: 1.5591660737991333
Validation loss: 2.1129515145414617

Epoch: 418| Step: 0
Training loss: 2.2416739463806152
Validation loss: 2.1084803842729136

Epoch: 5| Step: 1
Training loss: 2.4633705615997314
Validation loss: 2.1168718056012223

Epoch: 5| Step: 2
Training loss: 2.304961919784546
Validation loss: 2.119476290159328

Epoch: 5| Step: 3
Training loss: 2.1112618446350098
Validation loss: 2.1229653050822597

Epoch: 5| Step: 4
Training loss: 2.046947479248047
Validation loss: 2.139640949105704

Epoch: 5| Step: 5
Training loss: 2.721686840057373
Validation loss: 2.1442126138235933

Epoch: 5| Step: 6
Training loss: 2.071511745452881
Validation loss: 2.1249389699710313

Epoch: 5| Step: 7
Training loss: 2.3930556774139404
Validation loss: 2.135124296270391

Epoch: 5| Step: 8
Training loss: 2.105201005935669
Validation loss: 2.1387890051769953

Epoch: 5| Step: 9
Training loss: 1.7732603549957275
Validation loss: 2.1251486014294367

Epoch: 5| Step: 10
Training loss: 2.697815179824829
Validation loss: 2.1248743739179385

Epoch: 419| Step: 0
Training loss: 2.151808500289917
Validation loss: 2.1092276983363654

Epoch: 5| Step: 1
Training loss: 2.530184268951416
Validation loss: 2.112184057953537

Epoch: 5| Step: 2
Training loss: 1.7819344997406006
Validation loss: 2.1217546027193785

Epoch: 5| Step: 3
Training loss: 2.354785442352295
Validation loss: 2.1118552941148

Epoch: 5| Step: 4
Training loss: 2.347252130508423
Validation loss: 2.1171110804362963

Epoch: 5| Step: 5
Training loss: 2.480342388153076
Validation loss: 2.116286281616457

Epoch: 5| Step: 6
Training loss: 2.366753101348877
Validation loss: 2.0879393059720277

Epoch: 5| Step: 7
Training loss: 2.170759677886963
Validation loss: 2.106350924379082

Epoch: 5| Step: 8
Training loss: 1.8945255279541016
Validation loss: 2.10126579705105

Epoch: 5| Step: 9
Training loss: 2.519773006439209
Validation loss: 2.1073620345002864

Epoch: 5| Step: 10
Training loss: 2.0729284286499023
Validation loss: 2.0995221599455802

Epoch: 420| Step: 0
Training loss: 1.8288428783416748
Validation loss: 2.1171442667643228

Epoch: 5| Step: 1
Training loss: 2.053379774093628
Validation loss: 2.1508503460115

Epoch: 5| Step: 2
Training loss: 2.430739641189575
Validation loss: 2.132694549458001

Epoch: 5| Step: 3
Training loss: 2.3620481491088867
Validation loss: 2.1184232927137807

Epoch: 5| Step: 4
Training loss: 2.084473133087158
Validation loss: 2.125538154314923

Epoch: 5| Step: 5
Training loss: 1.9960289001464844
Validation loss: 2.1181240786788282

Epoch: 5| Step: 6
Training loss: 2.3283252716064453
Validation loss: 2.1198724508285522

Epoch: 5| Step: 7
Training loss: 2.9604759216308594
Validation loss: 2.110027681114853

Epoch: 5| Step: 8
Training loss: 2.133376121520996
Validation loss: 2.103885578852828

Epoch: 5| Step: 9
Training loss: 2.3315463066101074
Validation loss: 2.0999993290952457

Epoch: 5| Step: 10
Training loss: 2.3319013118743896
Validation loss: 2.1117120506942912

Epoch: 421| Step: 0
Training loss: 2.1194710731506348
Validation loss: 2.1206430722308416

Epoch: 5| Step: 1
Training loss: 2.1854753494262695
Validation loss: 2.091225770211989

Epoch: 5| Step: 2
Training loss: 2.6626579761505127
Validation loss: 2.1279570197546356

Epoch: 5| Step: 3
Training loss: 2.8150172233581543
Validation loss: 2.1045735830901773

Epoch: 5| Step: 4
Training loss: 2.551201581954956
Validation loss: 2.127216946694159

Epoch: 5| Step: 5
Training loss: 1.8575413227081299
Validation loss: 2.149292788197917

Epoch: 5| Step: 6
Training loss: 1.5661059617996216
Validation loss: 2.1528881313980266

Epoch: 5| Step: 7
Training loss: 2.240482807159424
Validation loss: 2.1528371636585524

Epoch: 5| Step: 8
Training loss: 2.04215931892395
Validation loss: 2.128411623739427

Epoch: 5| Step: 9
Training loss: 2.187072277069092
Validation loss: 2.1197532658935874

Epoch: 5| Step: 10
Training loss: 2.6233174800872803
Validation loss: 2.087351570847214

Epoch: 422| Step: 0
Training loss: 2.358414888381958
Validation loss: 2.1059779505575857

Epoch: 5| Step: 1
Training loss: 2.9345896244049072
Validation loss: 2.106152347339097

Epoch: 5| Step: 2
Training loss: 2.429612636566162
Validation loss: 2.07504339115594

Epoch: 5| Step: 3
Training loss: 0.826999306678772
Validation loss: 2.0885401977005826

Epoch: 5| Step: 4
Training loss: 2.237368583679199
Validation loss: 2.08636506911247

Epoch: 5| Step: 5
Training loss: 1.8620433807373047
Validation loss: 2.0864005345170216

Epoch: 5| Step: 6
Training loss: 2.2068586349487305
Validation loss: 2.114188689057545

Epoch: 5| Step: 7
Training loss: 2.273089647293091
Validation loss: 2.101532134958493

Epoch: 5| Step: 8
Training loss: 3.016458034515381
Validation loss: 2.121916114643056

Epoch: 5| Step: 9
Training loss: 2.4373602867126465
Validation loss: 2.1531112655516593

Epoch: 5| Step: 10
Training loss: 2.191307544708252
Validation loss: 2.139598795162734

Epoch: 423| Step: 0
Training loss: 2.632669687271118
Validation loss: 2.1308129474680912

Epoch: 5| Step: 1
Training loss: 1.6907352209091187
Validation loss: 2.1000821064877253

Epoch: 5| Step: 2
Training loss: 3.205397129058838
Validation loss: 2.094722697811742

Epoch: 5| Step: 3
Training loss: 2.4873414039611816
Validation loss: 2.090735812341013

Epoch: 5| Step: 4
Training loss: 1.5733931064605713
Validation loss: 2.0903497562613538

Epoch: 5| Step: 5
Training loss: 2.1245226860046387
Validation loss: 2.0854717095692954

Epoch: 5| Step: 6
Training loss: 2.6527628898620605
Validation loss: 2.0811886531050487

Epoch: 5| Step: 7
Training loss: 2.1326053142547607
Validation loss: 2.093823232958394

Epoch: 5| Step: 8
Training loss: 2.0103652477264404
Validation loss: 2.0861675277833016

Epoch: 5| Step: 9
Training loss: 2.4733848571777344
Validation loss: 2.1031735481754428

Epoch: 5| Step: 10
Training loss: 1.605027675628662
Validation loss: 2.1130770919143513

Epoch: 424| Step: 0
Training loss: 2.2295093536376953
Validation loss: 2.1074793774594545

Epoch: 5| Step: 1
Training loss: 2.5814201831817627
Validation loss: 2.102188485924916

Epoch: 5| Step: 2
Training loss: 2.3084964752197266
Validation loss: 2.1058125419001423

Epoch: 5| Step: 3
Training loss: 2.2286956310272217
Validation loss: 2.091232640768892

Epoch: 5| Step: 4
Training loss: 1.7164322137832642
Validation loss: 2.0823205824821227

Epoch: 5| Step: 5
Training loss: 2.3962507247924805
Validation loss: 2.0966900804991364

Epoch: 5| Step: 6
Training loss: 2.1273834705352783
Validation loss: 2.0806097548495055

Epoch: 5| Step: 7
Training loss: 2.2026219367980957
Validation loss: 2.1019400371018278

Epoch: 5| Step: 8
Training loss: 2.292604923248291
Validation loss: 2.0986389678011657

Epoch: 5| Step: 9
Training loss: 2.9872090816497803
Validation loss: 2.1034084391850296

Epoch: 5| Step: 10
Training loss: 1.5633903741836548
Validation loss: 2.1099522831619426

Epoch: 425| Step: 0
Training loss: 2.026087999343872
Validation loss: 2.1001947156844603

Epoch: 5| Step: 1
Training loss: 2.615661144256592
Validation loss: 2.1065823916466004

Epoch: 5| Step: 2
Training loss: 1.936484932899475
Validation loss: 2.108300575646021

Epoch: 5| Step: 3
Training loss: 1.5765318870544434
Validation loss: 2.1200023748541392

Epoch: 5| Step: 4
Training loss: 2.300586223602295
Validation loss: 2.1174344298660115

Epoch: 5| Step: 5
Training loss: 2.556018114089966
Validation loss: 2.1028110929714736

Epoch: 5| Step: 6
Training loss: 2.5372660160064697
Validation loss: 2.1263020192423174

Epoch: 5| Step: 7
Training loss: 2.160860776901245
Validation loss: 2.143879939151067

Epoch: 5| Step: 8
Training loss: 2.4781365394592285
Validation loss: 2.1200034362013622

Epoch: 5| Step: 9
Training loss: 2.536076068878174
Validation loss: 2.111969164622727

Epoch: 5| Step: 10
Training loss: 1.813961386680603
Validation loss: 2.0983383886275755

Epoch: 426| Step: 0
Training loss: 2.440206289291382
Validation loss: 2.113344092522898

Epoch: 5| Step: 1
Training loss: 2.384168863296509
Validation loss: 2.0861369166322934

Epoch: 5| Step: 2
Training loss: 2.376915693283081
Validation loss: 2.0807108725270917

Epoch: 5| Step: 3
Training loss: 2.3580095767974854
Validation loss: 2.0765926684102705

Epoch: 5| Step: 4
Training loss: 2.0885961055755615
Validation loss: 2.1039692522377096

Epoch: 5| Step: 5
Training loss: 2.278769016265869
Validation loss: 2.0916433898351525

Epoch: 5| Step: 6
Training loss: 2.0458083152770996
Validation loss: 2.0849504829734884

Epoch: 5| Step: 7
Training loss: 2.2884175777435303
Validation loss: 2.0920458173239105

Epoch: 5| Step: 8
Training loss: 1.7423330545425415
Validation loss: 2.126295789595573

Epoch: 5| Step: 9
Training loss: 2.2884180545806885
Validation loss: 2.1300495157959642

Epoch: 5| Step: 10
Training loss: 2.674558162689209
Validation loss: 2.1335007811105378

Epoch: 427| Step: 0
Training loss: 2.6826229095458984
Validation loss: 2.1033936649240474

Epoch: 5| Step: 1
Training loss: 2.1004490852355957
Validation loss: 2.101297566967626

Epoch: 5| Step: 2
Training loss: 1.8480743169784546
Validation loss: 2.111325881814444

Epoch: 5| Step: 3
Training loss: 2.0925800800323486
Validation loss: 2.0935956893428678

Epoch: 5| Step: 4
Training loss: 2.7721595764160156
Validation loss: 2.1195321313796507

Epoch: 5| Step: 5
Training loss: 1.7725414037704468
Validation loss: 2.099236526796895

Epoch: 5| Step: 6
Training loss: 2.3120765686035156
Validation loss: 2.094853611402614

Epoch: 5| Step: 7
Training loss: 2.193748712539673
Validation loss: 2.0982293621186288

Epoch: 5| Step: 8
Training loss: 2.4905247688293457
Validation loss: 2.10032505501983

Epoch: 5| Step: 9
Training loss: 2.386776924133301
Validation loss: 2.073999527961977

Epoch: 5| Step: 10
Training loss: 1.9189480543136597
Validation loss: 2.1036140380367154

Epoch: 428| Step: 0
Training loss: 2.8067264556884766
Validation loss: 2.0951118802511566

Epoch: 5| Step: 1
Training loss: 2.121978521347046
Validation loss: 2.1099192224523073

Epoch: 5| Step: 2
Training loss: 2.145240068435669
Validation loss: 2.0936464391728884

Epoch: 5| Step: 3
Training loss: 2.019101619720459
Validation loss: 2.0993684927622476

Epoch: 5| Step: 4
Training loss: 2.1740918159484863
Validation loss: 2.1010364999053297

Epoch: 5| Step: 5
Training loss: 2.337799310684204
Validation loss: 2.1095795451953845

Epoch: 5| Step: 6
Training loss: 3.109218120574951
Validation loss: 2.105617541138844

Epoch: 5| Step: 7
Training loss: 2.1409542560577393
Validation loss: 2.1124786792262906

Epoch: 5| Step: 8
Training loss: 1.782027006149292
Validation loss: 2.1090329539391304

Epoch: 5| Step: 9
Training loss: 1.8127968311309814
Validation loss: 2.084605614344279

Epoch: 5| Step: 10
Training loss: 2.1787526607513428
Validation loss: 2.084906621645856

Epoch: 429| Step: 0
Training loss: 2.0513269901275635
Validation loss: 2.0954731061894405

Epoch: 5| Step: 1
Training loss: 2.1199333667755127
Validation loss: 2.0882119876082226

Epoch: 5| Step: 2
Training loss: 1.884442687034607
Validation loss: 2.1013057026811826

Epoch: 5| Step: 3
Training loss: 1.7335008382797241
Validation loss: 2.0929757625825944

Epoch: 5| Step: 4
Training loss: 2.6527903079986572
Validation loss: 2.1181295200060775

Epoch: 5| Step: 5
Training loss: 2.2042343616485596
Validation loss: 2.1229178431213542

Epoch: 5| Step: 6
Training loss: 2.331914186477661
Validation loss: 2.1232745185975106

Epoch: 5| Step: 7
Training loss: 1.7518901824951172
Validation loss: 2.112419997492144

Epoch: 5| Step: 8
Training loss: 2.9105148315429688
Validation loss: 2.1298858260595672

Epoch: 5| Step: 9
Training loss: 2.6032638549804688
Validation loss: 2.1153374307899067

Epoch: 5| Step: 10
Training loss: 2.302748203277588
Validation loss: 2.1050656482737553

Epoch: 430| Step: 0
Training loss: 2.278846502304077
Validation loss: 2.0979484870869625

Epoch: 5| Step: 1
Training loss: 1.8295751810073853
Validation loss: 2.089389172933435

Epoch: 5| Step: 2
Training loss: 1.8267844915390015
Validation loss: 2.0775261758476176

Epoch: 5| Step: 3
Training loss: 2.0321545600891113
Validation loss: 2.0774330862106813

Epoch: 5| Step: 4
Training loss: 2.055288791656494
Validation loss: 2.089422320806852

Epoch: 5| Step: 5
Training loss: 1.9644591808319092
Validation loss: 2.102313736433624

Epoch: 5| Step: 6
Training loss: 3.048274278640747
Validation loss: 2.114953689677741

Epoch: 5| Step: 7
Training loss: 2.2240066528320312
Validation loss: 2.1193906055983676

Epoch: 5| Step: 8
Training loss: 2.437387466430664
Validation loss: 2.1164135368921424

Epoch: 5| Step: 9
Training loss: 2.4945685863494873
Validation loss: 2.0989762608722975

Epoch: 5| Step: 10
Training loss: 2.28894305229187
Validation loss: 2.1049975400329917

Epoch: 431| Step: 0
Training loss: 2.153597593307495
Validation loss: 2.112029908805765

Epoch: 5| Step: 1
Training loss: 1.4480386972427368
Validation loss: 2.1077750498248684

Epoch: 5| Step: 2
Training loss: 2.4874846935272217
Validation loss: 2.078942584735091

Epoch: 5| Step: 3
Training loss: 2.376227617263794
Validation loss: 2.1040689945220947

Epoch: 5| Step: 4
Training loss: 1.8692764043807983
Validation loss: 2.098851268009473

Epoch: 5| Step: 5
Training loss: 2.3182260990142822
Validation loss: 2.069455349317161

Epoch: 5| Step: 6
Training loss: 2.3659656047821045
Validation loss: 2.08715509727437

Epoch: 5| Step: 7
Training loss: 3.244504928588867
Validation loss: 2.0942990702967488

Epoch: 5| Step: 8
Training loss: 2.0642330646514893
Validation loss: 2.074987070534819

Epoch: 5| Step: 9
Training loss: 2.0007057189941406
Validation loss: 2.0922816773896575

Epoch: 5| Step: 10
Training loss: 2.1387391090393066
Validation loss: 2.0857439682047856

Epoch: 432| Step: 0
Training loss: 3.7891883850097656
Validation loss: 2.1075497327312345

Epoch: 5| Step: 1
Training loss: 2.1549670696258545
Validation loss: 2.087973507501746

Epoch: 5| Step: 2
Training loss: 2.1788032054901123
Validation loss: 2.0781496186410227

Epoch: 5| Step: 3
Training loss: 2.082993268966675
Validation loss: 2.074139653995473

Epoch: 5| Step: 4
Training loss: 1.711198091506958
Validation loss: 2.07863091140665

Epoch: 5| Step: 5
Training loss: 2.0000381469726562
Validation loss: 2.0810896094127367

Epoch: 5| Step: 6
Training loss: 1.3466575145721436
Validation loss: 2.0682006779537407

Epoch: 5| Step: 7
Training loss: 2.4410433769226074
Validation loss: 2.085081201727672

Epoch: 5| Step: 8
Training loss: 2.8479678630828857
Validation loss: 2.1117540021096506

Epoch: 5| Step: 9
Training loss: 2.1421310901641846
Validation loss: 2.1166509928241855

Epoch: 5| Step: 10
Training loss: 2.0344505310058594
Validation loss: 2.118492700720346

Epoch: 433| Step: 0
Training loss: 2.119706392288208
Validation loss: 2.12608524035382

Epoch: 5| Step: 1
Training loss: 2.425422191619873
Validation loss: 2.0955035301946823

Epoch: 5| Step: 2
Training loss: 1.8614848852157593
Validation loss: 2.1004421582785984

Epoch: 5| Step: 3
Training loss: 2.3335041999816895
Validation loss: 2.0955953649295274

Epoch: 5| Step: 4
Training loss: 2.2212612628936768
Validation loss: 2.0786917235261653

Epoch: 5| Step: 5
Training loss: 2.4470062255859375
Validation loss: 2.075168319927749

Epoch: 5| Step: 6
Training loss: 2.1983578205108643
Validation loss: 2.0734627913403254

Epoch: 5| Step: 7
Training loss: 1.6759519577026367
Validation loss: 2.080074151357015

Epoch: 5| Step: 8
Training loss: 2.2017359733581543
Validation loss: 2.074402350251393

Epoch: 5| Step: 9
Training loss: 2.711202621459961
Validation loss: 2.0927251910650604

Epoch: 5| Step: 10
Training loss: 2.4263157844543457
Validation loss: 2.106788645508469

Epoch: 434| Step: 0
Training loss: 2.420896053314209
Validation loss: 2.0796861751105196

Epoch: 5| Step: 1
Training loss: 2.165337324142456
Validation loss: 2.093654465931718

Epoch: 5| Step: 2
Training loss: 2.0136289596557617
Validation loss: 2.1177340630562074

Epoch: 5| Step: 3
Training loss: 2.458456516265869
Validation loss: 2.100046637237713

Epoch: 5| Step: 4
Training loss: 1.488172173500061
Validation loss: 2.087445174494097

Epoch: 5| Step: 5
Training loss: 2.621420383453369
Validation loss: 2.0885555641625517

Epoch: 5| Step: 6
Training loss: 2.0650737285614014
Validation loss: 2.099049622012723

Epoch: 5| Step: 7
Training loss: 2.468960762023926
Validation loss: 2.0770567488926712

Epoch: 5| Step: 8
Training loss: 2.2099547386169434
Validation loss: 2.1051947839798464

Epoch: 5| Step: 9
Training loss: 2.8736867904663086
Validation loss: 2.0756951621783677

Epoch: 5| Step: 10
Training loss: 1.6447157859802246
Validation loss: 2.089110828215076

Epoch: 435| Step: 0
Training loss: 2.355595111846924
Validation loss: 2.085556391746767

Epoch: 5| Step: 1
Training loss: 2.6657803058624268
Validation loss: 2.094970821052469

Epoch: 5| Step: 2
Training loss: 2.1771254539489746
Validation loss: 2.096702032191779

Epoch: 5| Step: 3
Training loss: 1.9958341121673584
Validation loss: 2.0702396233876548

Epoch: 5| Step: 4
Training loss: 2.021883010864258
Validation loss: 2.084155632603553

Epoch: 5| Step: 5
Training loss: 2.487828254699707
Validation loss: 2.0612931174616658

Epoch: 5| Step: 6
Training loss: 2.2289512157440186
Validation loss: 2.0902773308497604

Epoch: 5| Step: 7
Training loss: 2.147775650024414
Validation loss: 2.0724447260620775

Epoch: 5| Step: 8
Training loss: 1.4791830778121948
Validation loss: 2.0910032256957023

Epoch: 5| Step: 9
Training loss: 2.9761574268341064
Validation loss: 2.075568174803129

Epoch: 5| Step: 10
Training loss: 1.7587400674819946
Validation loss: 2.095464528247874

Epoch: 436| Step: 0
Training loss: 2.519050121307373
Validation loss: 2.1432918579347673

Epoch: 5| Step: 1
Training loss: 2.4907021522521973
Validation loss: 2.139281617697849

Epoch: 5| Step: 2
Training loss: 2.3592214584350586
Validation loss: 2.1483519666938373

Epoch: 5| Step: 3
Training loss: 1.5586353540420532
Validation loss: 2.119982275911557

Epoch: 5| Step: 4
Training loss: 2.5197081565856934
Validation loss: 2.100257488989061

Epoch: 5| Step: 5
Training loss: 2.023186445236206
Validation loss: 2.069910672403151

Epoch: 5| Step: 6
Training loss: 2.277268648147583
Validation loss: 2.0654397382531116

Epoch: 5| Step: 7
Training loss: 1.910112977027893
Validation loss: 2.0936990925060806

Epoch: 5| Step: 8
Training loss: 1.7301051616668701
Validation loss: 2.095385302779495

Epoch: 5| Step: 9
Training loss: 2.5998072624206543
Validation loss: 2.1085235687994186

Epoch: 5| Step: 10
Training loss: 2.7258591651916504
Validation loss: 2.1331375619416595

Epoch: 437| Step: 0
Training loss: 1.9375858306884766
Validation loss: 2.0957148998014388

Epoch: 5| Step: 1
Training loss: 2.2592689990997314
Validation loss: 2.069928494832849

Epoch: 5| Step: 2
Training loss: 2.472168207168579
Validation loss: 2.104778289794922

Epoch: 5| Step: 3
Training loss: 1.9839231967926025
Validation loss: 2.085723523170717

Epoch: 5| Step: 4
Training loss: 1.686401128768921
Validation loss: 2.105077617911882

Epoch: 5| Step: 5
Training loss: 2.319845676422119
Validation loss: 2.1426479406254266

Epoch: 5| Step: 6
Training loss: 2.4361798763275146
Validation loss: 2.151870773684594

Epoch: 5| Step: 7
Training loss: 2.475857734680176
Validation loss: 2.136282395291072

Epoch: 5| Step: 8
Training loss: 2.4022679328918457
Validation loss: 2.102413410781532

Epoch: 5| Step: 9
Training loss: 2.4388370513916016
Validation loss: 2.0814741452534995

Epoch: 5| Step: 10
Training loss: 2.2187724113464355
Validation loss: 2.078521232451162

Epoch: 438| Step: 0
Training loss: 1.8511559963226318
Validation loss: 2.077143899856075

Epoch: 5| Step: 1
Training loss: 2.791522741317749
Validation loss: 2.0780116076110513

Epoch: 5| Step: 2
Training loss: 2.704559087753296
Validation loss: 2.070610006650289

Epoch: 5| Step: 3
Training loss: 2.1294100284576416
Validation loss: 2.0719276551277406

Epoch: 5| Step: 4
Training loss: 2.967494010925293
Validation loss: 2.0836023925453104

Epoch: 5| Step: 5
Training loss: 2.4731180667877197
Validation loss: 2.072590563886909

Epoch: 5| Step: 6
Training loss: 1.9956486225128174
Validation loss: 2.0766630300911526

Epoch: 5| Step: 7
Training loss: 1.6453605890274048
Validation loss: 2.0810056642819474

Epoch: 5| Step: 8
Training loss: 1.7117732763290405
Validation loss: 2.073214569399434

Epoch: 5| Step: 9
Training loss: 1.9669348001480103
Validation loss: 2.0896893957609772

Epoch: 5| Step: 10
Training loss: 2.003735065460205
Validation loss: 2.0796843562074887

Epoch: 439| Step: 0
Training loss: 2.4200901985168457
Validation loss: 2.0865885237211823

Epoch: 5| Step: 1
Training loss: 2.1022114753723145
Validation loss: 2.1154512077249508

Epoch: 5| Step: 2
Training loss: 2.062396764755249
Validation loss: 2.1224628520268265

Epoch: 5| Step: 3
Training loss: 2.45428729057312
Validation loss: 2.0755926473166353

Epoch: 5| Step: 4
Training loss: 1.776105284690857
Validation loss: 2.098310219344272

Epoch: 5| Step: 5
Training loss: 2.2336230278015137
Validation loss: 2.0964006659805134

Epoch: 5| Step: 6
Training loss: 2.489535093307495
Validation loss: 2.0870229274995866

Epoch: 5| Step: 7
Training loss: 2.285578966140747
Validation loss: 2.081850136480024

Epoch: 5| Step: 8
Training loss: 2.0274271965026855
Validation loss: 2.0813806723522883

Epoch: 5| Step: 9
Training loss: 2.375537633895874
Validation loss: 2.093906448733422

Epoch: 5| Step: 10
Training loss: 2.149763822555542
Validation loss: 2.0841497195664274

Epoch: 440| Step: 0
Training loss: 1.7195926904678345
Validation loss: 2.089704223858413

Epoch: 5| Step: 1
Training loss: 2.274704933166504
Validation loss: 2.087411161391966

Epoch: 5| Step: 2
Training loss: 1.5992271900177002
Validation loss: 2.0675852055190713

Epoch: 5| Step: 3
Training loss: 1.9560229778289795
Validation loss: 2.0845847514367875

Epoch: 5| Step: 4
Training loss: 2.094558000564575
Validation loss: 2.083114816296485

Epoch: 5| Step: 5
Training loss: 2.6047096252441406
Validation loss: 2.086415926615397

Epoch: 5| Step: 6
Training loss: 2.6371726989746094
Validation loss: 2.084645240537582

Epoch: 5| Step: 7
Training loss: 2.0299599170684814
Validation loss: 2.0912043638126825

Epoch: 5| Step: 8
Training loss: 2.1632742881774902
Validation loss: 2.0822645054068616

Epoch: 5| Step: 9
Training loss: 2.808525800704956
Validation loss: 2.0947886359307075

Epoch: 5| Step: 10
Training loss: 2.3911244869232178
Validation loss: 2.084730006033374

Epoch: 441| Step: 0
Training loss: 2.411585569381714
Validation loss: 2.0878933975773473

Epoch: 5| Step: 1
Training loss: 2.544015645980835
Validation loss: 2.094156453686376

Epoch: 5| Step: 2
Training loss: 2.7055821418762207
Validation loss: 2.071322594919512

Epoch: 5| Step: 3
Training loss: 2.361790657043457
Validation loss: 2.091668890368554

Epoch: 5| Step: 4
Training loss: 1.867128610610962
Validation loss: 2.0929782082957606

Epoch: 5| Step: 5
Training loss: 1.8146682977676392
Validation loss: 2.0873803797588555

Epoch: 5| Step: 6
Training loss: 2.5448639392852783
Validation loss: 2.0671120894852506

Epoch: 5| Step: 7
Training loss: 2.225839138031006
Validation loss: 2.061265294269849

Epoch: 5| Step: 8
Training loss: 1.9847408533096313
Validation loss: 2.075834943402198

Epoch: 5| Step: 9
Training loss: 2.0344719886779785
Validation loss: 2.0593610322603615

Epoch: 5| Step: 10
Training loss: 1.794962763786316
Validation loss: 2.083982995761338

Epoch: 442| Step: 0
Training loss: 1.8517160415649414
Validation loss: 2.0916371486520253

Epoch: 5| Step: 1
Training loss: 2.226839542388916
Validation loss: 2.075950466176515

Epoch: 5| Step: 2
Training loss: 1.818982720375061
Validation loss: 2.0842126056712162

Epoch: 5| Step: 3
Training loss: 2.519702434539795
Validation loss: 2.1032245761604718

Epoch: 5| Step: 4
Training loss: 1.7020705938339233
Validation loss: 2.095544086989536

Epoch: 5| Step: 5
Training loss: 2.0840229988098145
Validation loss: 2.101049666763634

Epoch: 5| Step: 6
Training loss: 2.2151284217834473
Validation loss: 2.1105116157121557

Epoch: 5| Step: 7
Training loss: 2.0265872478485107
Validation loss: 2.0734136822403118

Epoch: 5| Step: 8
Training loss: 2.4747519493103027
Validation loss: 2.0986262059980825

Epoch: 5| Step: 9
Training loss: 2.633486747741699
Validation loss: 2.094400235401687

Epoch: 5| Step: 10
Training loss: 2.779402256011963
Validation loss: 2.0721815324598745

Epoch: 443| Step: 0
Training loss: 1.8753337860107422
Validation loss: 2.0905696602277857

Epoch: 5| Step: 1
Training loss: 1.858253836631775
Validation loss: 2.085524853839669

Epoch: 5| Step: 2
Training loss: 1.7845470905303955
Validation loss: 2.1000807592945714

Epoch: 5| Step: 3
Training loss: 2.349377393722534
Validation loss: 2.077933960063483

Epoch: 5| Step: 4
Training loss: 1.5388543605804443
Validation loss: 2.0855759830885034

Epoch: 5| Step: 5
Training loss: 2.727332592010498
Validation loss: 2.08136652105598

Epoch: 5| Step: 6
Training loss: 2.896824598312378
Validation loss: 2.1198009470457673

Epoch: 5| Step: 7
Training loss: 2.529411792755127
Validation loss: 2.0907964373147614

Epoch: 5| Step: 8
Training loss: 2.5324625968933105
Validation loss: 2.1031687746765795

Epoch: 5| Step: 9
Training loss: 2.250537872314453
Validation loss: 2.0807335428012315

Epoch: 5| Step: 10
Training loss: 1.944450855255127
Validation loss: 2.082015900201695

Epoch: 444| Step: 0
Training loss: 2.318413496017456
Validation loss: 2.0856076055957424

Epoch: 5| Step: 1
Training loss: 2.1180808544158936
Validation loss: 2.0845379419224237

Epoch: 5| Step: 2
Training loss: 2.1938066482543945
Validation loss: 2.0678314265384468

Epoch: 5| Step: 3
Training loss: 2.2072672843933105
Validation loss: 2.077136067933934

Epoch: 5| Step: 4
Training loss: 2.504880905151367
Validation loss: 2.0851002021502425

Epoch: 5| Step: 5
Training loss: 2.6148927211761475
Validation loss: 2.068029042213194

Epoch: 5| Step: 6
Training loss: 2.523449420928955
Validation loss: 2.088504554123007

Epoch: 5| Step: 7
Training loss: 2.0343313217163086
Validation loss: 2.073634042534777

Epoch: 5| Step: 8
Training loss: 1.5511893033981323
Validation loss: 2.078335810733098

Epoch: 5| Step: 9
Training loss: 2.272848606109619
Validation loss: 2.0914261405185988

Epoch: 5| Step: 10
Training loss: 1.8869471549987793
Validation loss: 2.0848593045306463

Epoch: 445| Step: 0
Training loss: 2.5497381687164307
Validation loss: 2.1100631862558346

Epoch: 5| Step: 1
Training loss: 2.038377523422241
Validation loss: 2.0995038991333335

Epoch: 5| Step: 2
Training loss: 1.8500770330429077
Validation loss: 2.0974195746965307

Epoch: 5| Step: 3
Training loss: 2.072514533996582
Validation loss: 2.1066769938315115

Epoch: 5| Step: 4
Training loss: 2.2530815601348877
Validation loss: 2.108284996401879

Epoch: 5| Step: 5
Training loss: 1.7880750894546509
Validation loss: 2.1048800176189792

Epoch: 5| Step: 6
Training loss: 2.098057508468628
Validation loss: 2.1073355726016465

Epoch: 5| Step: 7
Training loss: 2.397993564605713
Validation loss: 2.085967690713944

Epoch: 5| Step: 8
Training loss: 2.808687686920166
Validation loss: 2.087553649820307

Epoch: 5| Step: 9
Training loss: 1.8643827438354492
Validation loss: 2.086691188555892

Epoch: 5| Step: 10
Training loss: 2.561979055404663
Validation loss: 2.088099892421435

Epoch: 446| Step: 0
Training loss: 2.2830135822296143
Validation loss: 2.0865954878509685

Epoch: 5| Step: 1
Training loss: 2.2443392276763916
Validation loss: 2.0629632037173034

Epoch: 5| Step: 2
Training loss: 1.7378215789794922
Validation loss: 2.069108259293341

Epoch: 5| Step: 3
Training loss: 1.5746148824691772
Validation loss: 2.0875021975527526

Epoch: 5| Step: 4
Training loss: 2.0274198055267334
Validation loss: 2.076366687333712

Epoch: 5| Step: 5
Training loss: 1.6643489599227905
Validation loss: 2.116977704468594

Epoch: 5| Step: 6
Training loss: 2.7186763286590576
Validation loss: 2.14069470795252

Epoch: 5| Step: 7
Training loss: 2.686445713043213
Validation loss: 2.1637610953341246

Epoch: 5| Step: 8
Training loss: 2.0681400299072266
Validation loss: 2.184166972355176

Epoch: 5| Step: 9
Training loss: 3.0210909843444824
Validation loss: 2.132282490371376

Epoch: 5| Step: 10
Training loss: 2.5169713497161865
Validation loss: 2.12924878058895

Epoch: 447| Step: 0
Training loss: 1.7013006210327148
Validation loss: 2.0879139272115563

Epoch: 5| Step: 1
Training loss: 1.8900893926620483
Validation loss: 2.091058749024586

Epoch: 5| Step: 2
Training loss: 2.2606635093688965
Validation loss: 2.0652860095424037

Epoch: 5| Step: 3
Training loss: 3.167343854904175
Validation loss: 2.071530390811223

Epoch: 5| Step: 4
Training loss: 2.356809377670288
Validation loss: 2.0717963275089057

Epoch: 5| Step: 5
Training loss: 2.460132598876953
Validation loss: 2.065171810888475

Epoch: 5| Step: 6
Training loss: 1.980229139328003
Validation loss: 2.0660187044451312

Epoch: 5| Step: 7
Training loss: 2.570124626159668
Validation loss: 2.064099857884069

Epoch: 5| Step: 8
Training loss: 1.9869292974472046
Validation loss: 2.069805573391658

Epoch: 5| Step: 9
Training loss: 1.608420968055725
Validation loss: 2.068527401134532

Epoch: 5| Step: 10
Training loss: 2.180276870727539
Validation loss: 2.077202385471713

Epoch: 448| Step: 0
Training loss: 1.7163059711456299
Validation loss: 2.0938338938579766

Epoch: 5| Step: 1
Training loss: 1.8006728887557983
Validation loss: 2.122175303838586

Epoch: 5| Step: 2
Training loss: 2.504390239715576
Validation loss: 2.140561875476632

Epoch: 5| Step: 3
Training loss: 1.9320411682128906
Validation loss: 2.136925715272145

Epoch: 5| Step: 4
Training loss: 2.1400187015533447
Validation loss: 2.131209497810692

Epoch: 5| Step: 5
Training loss: 2.4089417457580566
Validation loss: 2.0969405994620374

Epoch: 5| Step: 6
Training loss: 2.2561256885528564
Validation loss: 2.0718929703517626

Epoch: 5| Step: 7
Training loss: 2.5500667095184326
Validation loss: 2.054909899670591

Epoch: 5| Step: 8
Training loss: 2.166487216949463
Validation loss: 2.0851993842791487

Epoch: 5| Step: 9
Training loss: 2.3212063312530518
Validation loss: 2.081415045645929

Epoch: 5| Step: 10
Training loss: 2.475057601928711
Validation loss: 2.0941744953073482

Epoch: 449| Step: 0
Training loss: 1.9366629123687744
Validation loss: 2.073615725322436

Epoch: 5| Step: 1
Training loss: 2.3995702266693115
Validation loss: 2.0895568657946844

Epoch: 5| Step: 2
Training loss: 2.4690704345703125
Validation loss: 2.07516493592211

Epoch: 5| Step: 3
Training loss: 1.9338855743408203
Validation loss: 2.0788375510964343

Epoch: 5| Step: 4
Training loss: 2.184314250946045
Validation loss: 2.077127759174634

Epoch: 5| Step: 5
Training loss: 1.6299011707305908
Validation loss: 2.077016056224864

Epoch: 5| Step: 6
Training loss: 2.1819634437561035
Validation loss: 2.0910820191906345

Epoch: 5| Step: 7
Training loss: 2.630431652069092
Validation loss: 2.094360379762547

Epoch: 5| Step: 8
Training loss: 2.1426384449005127
Validation loss: 2.109224662985853

Epoch: 5| Step: 9
Training loss: 1.7803428173065186
Validation loss: 2.0714064592956216

Epoch: 5| Step: 10
Training loss: 2.718888759613037
Validation loss: 2.07080925151866

Epoch: 450| Step: 0
Training loss: 2.586632013320923
Validation loss: 2.0813905398050943

Epoch: 5| Step: 1
Training loss: 2.298536777496338
Validation loss: 2.0653366798995645

Epoch: 5| Step: 2
Training loss: 1.98894464969635
Validation loss: 2.0807946382030362

Epoch: 5| Step: 3
Training loss: 1.959511399269104
Validation loss: 2.067574716383411

Epoch: 5| Step: 4
Training loss: 2.2915377616882324
Validation loss: 2.0797768587707193

Epoch: 5| Step: 5
Training loss: 2.273163318634033
Validation loss: 2.051980436489146

Epoch: 5| Step: 6
Training loss: 1.9450056552886963
Validation loss: 2.060417749548471

Epoch: 5| Step: 7
Training loss: 2.4597976207733154
Validation loss: 2.08552183130736

Epoch: 5| Step: 8
Training loss: 1.911603569984436
Validation loss: 2.0651987932061635

Epoch: 5| Step: 9
Training loss: 2.185549259185791
Validation loss: 2.0792939278387252

Epoch: 5| Step: 10
Training loss: 2.2541487216949463
Validation loss: 2.054672110465265

Epoch: 451| Step: 0
Training loss: 2.3751790523529053
Validation loss: 2.060020947969088

Epoch: 5| Step: 1
Training loss: 1.8799378871917725
Validation loss: 2.0615319795505975

Epoch: 5| Step: 2
Training loss: 1.997581124305725
Validation loss: 2.0674571990966797

Epoch: 5| Step: 3
Training loss: 2.0700230598449707
Validation loss: 2.076866485739267

Epoch: 5| Step: 4
Training loss: 2.202758312225342
Validation loss: 2.0556282228039158

Epoch: 5| Step: 5
Training loss: 1.7228243350982666
Validation loss: 2.067750184766708

Epoch: 5| Step: 6
Training loss: 2.131108045578003
Validation loss: 2.072295988759687

Epoch: 5| Step: 7
Training loss: 2.192195415496826
Validation loss: 2.0774494653107016

Epoch: 5| Step: 8
Training loss: 2.4764785766601562
Validation loss: 2.086251304995629

Epoch: 5| Step: 9
Training loss: 3.1533362865448
Validation loss: 2.1065096675708728

Epoch: 5| Step: 10
Training loss: 1.9090512990951538
Validation loss: 2.09033791993254

Epoch: 452| Step: 0
Training loss: 2.4945271015167236
Validation loss: 2.0900720396349506

Epoch: 5| Step: 1
Training loss: 1.2239866256713867
Validation loss: 2.0640095472335815

Epoch: 5| Step: 2
Training loss: 2.153759002685547
Validation loss: 2.0853931442383797

Epoch: 5| Step: 3
Training loss: 2.048074722290039
Validation loss: 2.0747547380385862

Epoch: 5| Step: 4
Training loss: 2.5953288078308105
Validation loss: 2.0805815765934605

Epoch: 5| Step: 5
Training loss: 1.8645206689834595
Validation loss: 2.0681546362497474

Epoch: 5| Step: 6
Training loss: 2.0402779579162598
Validation loss: 2.068355160374795

Epoch: 5| Step: 7
Training loss: 1.933465600013733
Validation loss: 2.0689113114469793

Epoch: 5| Step: 8
Training loss: 2.61069655418396
Validation loss: 2.078885628331092

Epoch: 5| Step: 9
Training loss: 2.805284023284912
Validation loss: 2.0966708352488856

Epoch: 5| Step: 10
Training loss: 2.235689163208008
Validation loss: 2.099784933110719

Epoch: 453| Step: 0
Training loss: 2.1475613117218018
Validation loss: 2.093686890858476

Epoch: 5| Step: 1
Training loss: 1.7362178564071655
Validation loss: 2.083440201256865

Epoch: 5| Step: 2
Training loss: 2.1753487586975098
Validation loss: 2.0641693235725485

Epoch: 5| Step: 3
Training loss: 2.0934906005859375
Validation loss: 2.0590667673336562

Epoch: 5| Step: 4
Training loss: 3.126251697540283
Validation loss: 2.059219778224986

Epoch: 5| Step: 5
Training loss: 2.20165753364563
Validation loss: 2.0668841203053794

Epoch: 5| Step: 6
Training loss: 2.116056203842163
Validation loss: 2.062422393470682

Epoch: 5| Step: 7
Training loss: 2.8483662605285645
Validation loss: 2.071567384145593

Epoch: 5| Step: 8
Training loss: 1.747277855873108
Validation loss: 2.04136130245783

Epoch: 5| Step: 9
Training loss: 2.0032989978790283
Validation loss: 2.0710242896951656

Epoch: 5| Step: 10
Training loss: 1.802842378616333
Validation loss: 2.05582288260101

Epoch: 454| Step: 0
Training loss: 3.091665506362915
Validation loss: 2.084788759549459

Epoch: 5| Step: 1
Training loss: 1.5856906175613403
Validation loss: 2.0678553837601856

Epoch: 5| Step: 2
Training loss: 2.719674825668335
Validation loss: 2.078802175419305

Epoch: 5| Step: 3
Training loss: 1.936872124671936
Validation loss: 2.085108495527698

Epoch: 5| Step: 4
Training loss: 2.239521026611328
Validation loss: 2.085187787650734

Epoch: 5| Step: 5
Training loss: 2.4971179962158203
Validation loss: 2.0805489529845533

Epoch: 5| Step: 6
Training loss: 2.17537260055542
Validation loss: 2.0811534876464517

Epoch: 5| Step: 7
Training loss: 2.108220338821411
Validation loss: 2.0882854666761173

Epoch: 5| Step: 8
Training loss: 1.8667398691177368
Validation loss: 2.0734537750162105

Epoch: 5| Step: 9
Training loss: 1.5705775022506714
Validation loss: 2.0670750602599113

Epoch: 5| Step: 10
Training loss: 2.085538625717163
Validation loss: 2.0525289274031118

Epoch: 455| Step: 0
Training loss: 1.8654584884643555
Validation loss: 2.0365055056028467

Epoch: 5| Step: 1
Training loss: 1.973717451095581
Validation loss: 2.0502129844439927

Epoch: 5| Step: 2
Training loss: 1.8191686868667603
Validation loss: 2.070653712877663

Epoch: 5| Step: 3
Training loss: 2.7111287117004395
Validation loss: 2.0608981450398765

Epoch: 5| Step: 4
Training loss: 2.1386218070983887
Validation loss: 2.0599967920652

Epoch: 5| Step: 5
Training loss: 3.2027714252471924
Validation loss: 2.0723953144524687

Epoch: 5| Step: 6
Training loss: 2.0491902828216553
Validation loss: 2.0862888443854546

Epoch: 5| Step: 7
Training loss: 1.535746693611145
Validation loss: 2.0998570713945615

Epoch: 5| Step: 8
Training loss: 1.8414567708969116
Validation loss: 2.105661348630023

Epoch: 5| Step: 9
Training loss: 2.6414968967437744
Validation loss: 2.0904261117340415

Epoch: 5| Step: 10
Training loss: 2.2657413482666016
Validation loss: 2.0982687832206808

Epoch: 456| Step: 0
Training loss: 1.7880172729492188
Validation loss: 2.0773163046888126

Epoch: 5| Step: 1
Training loss: 2.1315996646881104
Validation loss: 2.0704783816491403

Epoch: 5| Step: 2
Training loss: 1.6433547735214233
Validation loss: 2.073770296189093

Epoch: 5| Step: 3
Training loss: 2.549790620803833
Validation loss: 2.0622845183136644

Epoch: 5| Step: 4
Training loss: 2.3332648277282715
Validation loss: 2.0502976679032847

Epoch: 5| Step: 5
Training loss: 2.3237109184265137
Validation loss: 2.0486253846076226

Epoch: 5| Step: 6
Training loss: 2.2743606567382812
Validation loss: 2.0394259037510043

Epoch: 5| Step: 7
Training loss: 2.0591304302215576
Validation loss: 2.0665659366115445

Epoch: 5| Step: 8
Training loss: 2.047532558441162
Validation loss: 2.0750198364257812

Epoch: 5| Step: 9
Training loss: 2.7424604892730713
Validation loss: 2.046491740852274

Epoch: 5| Step: 10
Training loss: 1.995134711265564
Validation loss: 2.0546232064565024

Epoch: 457| Step: 0
Training loss: 2.3299694061279297
Validation loss: 2.0768409441876154

Epoch: 5| Step: 1
Training loss: 2.1906065940856934
Validation loss: 2.136160208332923

Epoch: 5| Step: 2
Training loss: 2.4343416690826416
Validation loss: 2.1398824055989585

Epoch: 5| Step: 3
Training loss: 1.8875854015350342
Validation loss: 2.149113029562017

Epoch: 5| Step: 4
Training loss: 1.799673080444336
Validation loss: 2.1224499517871487

Epoch: 5| Step: 5
Training loss: 2.317532539367676
Validation loss: 2.1238913920617875

Epoch: 5| Step: 6
Training loss: 1.669450044631958
Validation loss: 2.1168224811553955

Epoch: 5| Step: 7
Training loss: 2.838710308074951
Validation loss: 2.0668241311145086

Epoch: 5| Step: 8
Training loss: 2.5977070331573486
Validation loss: 2.068150968961818

Epoch: 5| Step: 9
Training loss: 1.4401744604110718
Validation loss: 2.0674416275434595

Epoch: 5| Step: 10
Training loss: 2.720219373703003
Validation loss: 2.073565206220073

Epoch: 458| Step: 0
Training loss: 2.6453230381011963
Validation loss: 2.084678979330165

Epoch: 5| Step: 1
Training loss: 2.25054669380188
Validation loss: 2.079897975408903

Epoch: 5| Step: 2
Training loss: 2.1668858528137207
Validation loss: 2.0884560590149253

Epoch: 5| Step: 3
Training loss: 1.9073301553726196
Validation loss: 2.0682504446275773

Epoch: 5| Step: 4
Training loss: 1.6476049423217773
Validation loss: 2.080124565350112

Epoch: 5| Step: 5
Training loss: 2.1429805755615234
Validation loss: 2.1031780499283985

Epoch: 5| Step: 6
Training loss: 2.575117349624634
Validation loss: 2.161245620378884

Epoch: 5| Step: 7
Training loss: 1.6907596588134766
Validation loss: 2.2142062238467637

Epoch: 5| Step: 8
Training loss: 2.3403220176696777
Validation loss: 2.2432681719462075

Epoch: 5| Step: 9
Training loss: 2.509122371673584
Validation loss: 2.2694355877496863

Epoch: 5| Step: 10
Training loss: 2.7515711784362793
Validation loss: 2.2259763594596618

Epoch: 459| Step: 0
Training loss: 1.954690933227539
Validation loss: 2.119251999803769

Epoch: 5| Step: 1
Training loss: 1.9577049016952515
Validation loss: 2.0753947816869265

Epoch: 5| Step: 2
Training loss: 2.3250904083251953
Validation loss: 2.083448262624843

Epoch: 5| Step: 3
Training loss: 2.367981195449829
Validation loss: 2.077166768812364

Epoch: 5| Step: 4
Training loss: 2.282392978668213
Validation loss: 2.0783857760890836

Epoch: 5| Step: 5
Training loss: 1.3615411520004272
Validation loss: 2.056788500919137

Epoch: 5| Step: 6
Training loss: 2.196350574493408
Validation loss: 2.0965902292600243

Epoch: 5| Step: 7
Training loss: 2.282968759536743
Validation loss: 2.073985069028793

Epoch: 5| Step: 8
Training loss: 2.3360111713409424
Validation loss: 2.0734676930212204

Epoch: 5| Step: 9
Training loss: 2.6455204486846924
Validation loss: 2.0710122636569444

Epoch: 5| Step: 10
Training loss: 2.3333959579467773
Validation loss: 2.080315480950058

Epoch: 460| Step: 0
Training loss: 2.2872462272644043
Validation loss: 2.076619440509427

Epoch: 5| Step: 1
Training loss: 1.853590726852417
Validation loss: 2.0622097317890455

Epoch: 5| Step: 2
Training loss: 1.8842027187347412
Validation loss: 2.0745567224359

Epoch: 5| Step: 3
Training loss: 1.8475685119628906
Validation loss: 2.075428226942657

Epoch: 5| Step: 4
Training loss: 3.091602325439453
Validation loss: 2.054986167979497

Epoch: 5| Step: 5
Training loss: 2.338578224182129
Validation loss: 2.076635335081367

Epoch: 5| Step: 6
Training loss: 2.0506675243377686
Validation loss: 2.0802380884847333

Epoch: 5| Step: 7
Training loss: 1.8476536273956299
Validation loss: 2.0738305673804334

Epoch: 5| Step: 8
Training loss: 1.7009748220443726
Validation loss: 2.060142150489233

Epoch: 5| Step: 9
Training loss: 2.7613141536712646
Validation loss: 2.050468837061236

Epoch: 5| Step: 10
Training loss: 2.3782103061676025
Validation loss: 2.0464638702331053

Epoch: 461| Step: 0
Training loss: 2.249577045440674
Validation loss: 2.0384452906988

Epoch: 5| Step: 1
Training loss: 2.441563129425049
Validation loss: 2.0561482675613894

Epoch: 5| Step: 2
Training loss: 2.3639705181121826
Validation loss: 2.082604249318441

Epoch: 5| Step: 3
Training loss: 2.1173887252807617
Validation loss: 2.0748761648772867

Epoch: 5| Step: 4
Training loss: 2.6811554431915283
Validation loss: 2.0784785311709166

Epoch: 5| Step: 5
Training loss: 1.9448436498641968
Validation loss: 2.078264028795304

Epoch: 5| Step: 6
Training loss: 2.389334201812744
Validation loss: 2.0785184701283774

Epoch: 5| Step: 7
Training loss: 1.7997512817382812
Validation loss: 2.105207927765385

Epoch: 5| Step: 8
Training loss: 2.3919014930725098
Validation loss: 2.136217281382571

Epoch: 5| Step: 9
Training loss: 1.9245655536651611
Validation loss: 2.1622190065281366

Epoch: 5| Step: 10
Training loss: 1.9464699029922485
Validation loss: 2.187416086914719

Epoch: 462| Step: 0
Training loss: 2.59443736076355
Validation loss: 2.134560597840176

Epoch: 5| Step: 1
Training loss: 2.4230895042419434
Validation loss: 2.100486409279608

Epoch: 5| Step: 2
Training loss: 2.2517731189727783
Validation loss: 2.0807146564606698

Epoch: 5| Step: 3
Training loss: 2.1117348670959473
Validation loss: 2.0622320098261677

Epoch: 5| Step: 4
Training loss: 1.6283581256866455
Validation loss: 2.0713908595423542

Epoch: 5| Step: 5
Training loss: 2.1855380535125732
Validation loss: 2.065700187478014

Epoch: 5| Step: 6
Training loss: 2.1734282970428467
Validation loss: 2.0530787962739185

Epoch: 5| Step: 7
Training loss: 1.9015624523162842
Validation loss: 2.0431425109986336

Epoch: 5| Step: 8
Training loss: 2.5093464851379395
Validation loss: 2.0422384867104153

Epoch: 5| Step: 9
Training loss: 1.8623632192611694
Validation loss: 2.0547703286652923

Epoch: 5| Step: 10
Training loss: 2.6686437129974365
Validation loss: 2.0903875571425243

Epoch: 463| Step: 0
Training loss: 1.961449384689331
Validation loss: 2.0965479778987106

Epoch: 5| Step: 1
Training loss: 2.6395092010498047
Validation loss: 2.0839359991012083

Epoch: 5| Step: 2
Training loss: 2.222945213317871
Validation loss: 2.10171010417323

Epoch: 5| Step: 3
Training loss: 2.165813684463501
Validation loss: 2.1076214851871615

Epoch: 5| Step: 4
Training loss: 1.8220173120498657
Validation loss: 2.134530826281476

Epoch: 5| Step: 5
Training loss: 2.232827663421631
Validation loss: 2.1109950209176667

Epoch: 5| Step: 6
Training loss: 1.6709563732147217
Validation loss: 2.123583016857024

Epoch: 5| Step: 7
Training loss: 2.2606589794158936
Validation loss: 2.1359632861229683

Epoch: 5| Step: 8
Training loss: 2.3313939571380615
Validation loss: 2.1211131516323296

Epoch: 5| Step: 9
Training loss: 2.3536322116851807
Validation loss: 2.0846605108630274

Epoch: 5| Step: 10
Training loss: 2.4550893306732178
Validation loss: 2.071472688387799

Epoch: 464| Step: 0
Training loss: 1.7343518733978271
Validation loss: 2.0649142970320997

Epoch: 5| Step: 1
Training loss: 1.8951330184936523
Validation loss: 2.071991180860868

Epoch: 5| Step: 2
Training loss: 2.791290760040283
Validation loss: 2.065126854886291

Epoch: 5| Step: 3
Training loss: 2.211975574493408
Validation loss: 2.066848390845842

Epoch: 5| Step: 4
Training loss: 1.970564842224121
Validation loss: 2.0416043753265054

Epoch: 5| Step: 5
Training loss: 2.5482850074768066
Validation loss: 2.0538892874153714

Epoch: 5| Step: 6
Training loss: 2.534759998321533
Validation loss: 2.068424682463369

Epoch: 5| Step: 7
Training loss: 1.6827484369277954
Validation loss: 2.0653332920484644

Epoch: 5| Step: 8
Training loss: 1.6026782989501953
Validation loss: 2.0925207984062935

Epoch: 5| Step: 9
Training loss: 2.6208693981170654
Validation loss: 2.111626881425099

Epoch: 5| Step: 10
Training loss: 2.754518508911133
Validation loss: 2.1752224506870395

Epoch: 465| Step: 0
Training loss: 2.4310405254364014
Validation loss: 2.142953945744422

Epoch: 5| Step: 1
Training loss: 2.1315224170684814
Validation loss: 2.0873074813555648

Epoch: 5| Step: 2
Training loss: 1.712588906288147
Validation loss: 2.051842858714442

Epoch: 5| Step: 3
Training loss: 2.438464403152466
Validation loss: 2.0661348937660136

Epoch: 5| Step: 4
Training loss: 2.259333848953247
Validation loss: 2.059949308313349

Epoch: 5| Step: 5
Training loss: 2.2680277824401855
Validation loss: 2.0726009491951234

Epoch: 5| Step: 6
Training loss: 2.766787052154541
Validation loss: 2.072511403791366

Epoch: 5| Step: 7
Training loss: 2.4977550506591797
Validation loss: 2.068044606075492

Epoch: 5| Step: 8
Training loss: 2.021883249282837
Validation loss: 2.0581909815470376

Epoch: 5| Step: 9
Training loss: 2.2033917903900146
Validation loss: 2.052615752784155

Epoch: 5| Step: 10
Training loss: 1.4938842058181763
Validation loss: 2.062174512493995

Epoch: 466| Step: 0
Training loss: 2.4740490913391113
Validation loss: 2.0827298728368615

Epoch: 5| Step: 1
Training loss: 2.2077219486236572
Validation loss: 2.1010490553353423

Epoch: 5| Step: 2
Training loss: 1.9913966655731201
Validation loss: 2.146364915755487

Epoch: 5| Step: 3
Training loss: 2.477896213531494
Validation loss: 2.1472258926719747

Epoch: 5| Step: 4
Training loss: 2.9857611656188965
Validation loss: 2.17037211438661

Epoch: 5| Step: 5
Training loss: 1.836655616760254
Validation loss: 2.1478791698332755

Epoch: 5| Step: 6
Training loss: 1.7545063495635986
Validation loss: 2.1645380450833227

Epoch: 5| Step: 7
Training loss: 1.7372848987579346
Validation loss: 2.1045671996249946

Epoch: 5| Step: 8
Training loss: 2.4215598106384277
Validation loss: 2.0787241305074384

Epoch: 5| Step: 9
Training loss: 2.5327491760253906
Validation loss: 2.0506195560578377

Epoch: 5| Step: 10
Training loss: 1.5395095348358154
Validation loss: 2.0524057944615683

Epoch: 467| Step: 0
Training loss: 1.7811405658721924
Validation loss: 2.057476753829628

Epoch: 5| Step: 1
Training loss: 2.561760902404785
Validation loss: 2.080373264128162

Epoch: 5| Step: 2
Training loss: 2.3330273628234863
Validation loss: 2.084347806951051

Epoch: 5| Step: 3
Training loss: 1.4807883501052856
Validation loss: 2.1019708212985786

Epoch: 5| Step: 4
Training loss: 2.5935170650482178
Validation loss: 2.0963852328638874

Epoch: 5| Step: 5
Training loss: 1.6735528707504272
Validation loss: 2.086424767330129

Epoch: 5| Step: 6
Training loss: 2.6888978481292725
Validation loss: 2.08203637215399

Epoch: 5| Step: 7
Training loss: 1.9181188344955444
Validation loss: 2.103913394353723

Epoch: 5| Step: 8
Training loss: 2.647754192352295
Validation loss: 2.062670498765925

Epoch: 5| Step: 9
Training loss: 2.5245513916015625
Validation loss: 2.0742723044528755

Epoch: 5| Step: 10
Training loss: 1.8142216205596924
Validation loss: 2.0603659986167826

Epoch: 468| Step: 0
Training loss: 2.7712912559509277
Validation loss: 2.0613020466219996

Epoch: 5| Step: 1
Training loss: 2.3314037322998047
Validation loss: 2.0516252440790974

Epoch: 5| Step: 2
Training loss: 2.022702217102051
Validation loss: 2.066774627213837

Epoch: 5| Step: 3
Training loss: 2.069045305252075
Validation loss: 2.0670781456014162

Epoch: 5| Step: 4
Training loss: 2.713665008544922
Validation loss: 2.0377976561105378

Epoch: 5| Step: 5
Training loss: 1.9611785411834717
Validation loss: 2.0551345143266904

Epoch: 5| Step: 6
Training loss: 2.458024024963379
Validation loss: 2.058197408594111

Epoch: 5| Step: 7
Training loss: 1.6681751012802124
Validation loss: 2.0556820938664098

Epoch: 5| Step: 8
Training loss: 2.275546073913574
Validation loss: 2.0538786367703508

Epoch: 5| Step: 9
Training loss: 1.6026411056518555
Validation loss: 2.0779449298817623

Epoch: 5| Step: 10
Training loss: 1.9281036853790283
Validation loss: 2.075599217927584

Epoch: 469| Step: 0
Training loss: 1.9010670185089111
Validation loss: 2.0682193310030046

Epoch: 5| Step: 1
Training loss: 1.6778627634048462
Validation loss: 2.084942074232204

Epoch: 5| Step: 2
Training loss: 2.3903629779815674
Validation loss: 2.0776600465979627

Epoch: 5| Step: 3
Training loss: 2.1310641765594482
Validation loss: 2.0692127391856205

Epoch: 5| Step: 4
Training loss: 2.9170894622802734
Validation loss: 2.0823271146384617

Epoch: 5| Step: 5
Training loss: 2.280564785003662
Validation loss: 2.0703480371864895

Epoch: 5| Step: 6
Training loss: 2.3539628982543945
Validation loss: 2.0598872976918376

Epoch: 5| Step: 7
Training loss: 2.298893690109253
Validation loss: 2.046217523595338

Epoch: 5| Step: 8
Training loss: 1.5703777074813843
Validation loss: 2.071293365570807

Epoch: 5| Step: 9
Training loss: 2.272599697113037
Validation loss: 2.0483271178378852

Epoch: 5| Step: 10
Training loss: 2.0114712715148926
Validation loss: 2.05510244190052

Epoch: 470| Step: 0
Training loss: 2.0347890853881836
Validation loss: 2.0435162295577345

Epoch: 5| Step: 1
Training loss: 2.2562530040740967
Validation loss: 2.049350360388397

Epoch: 5| Step: 2
Training loss: 1.6288944482803345
Validation loss: 2.0579478048509166

Epoch: 5| Step: 3
Training loss: 1.6534830331802368
Validation loss: 2.0591945955830235

Epoch: 5| Step: 4
Training loss: 2.6902830600738525
Validation loss: 2.0811457736517793

Epoch: 5| Step: 5
Training loss: 2.459052562713623
Validation loss: 2.0901129438031103

Epoch: 5| Step: 6
Training loss: 2.190959930419922
Validation loss: 2.0756325619195097

Epoch: 5| Step: 7
Training loss: 1.6866216659545898
Validation loss: 2.063912140425815

Epoch: 5| Step: 8
Training loss: 2.3000309467315674
Validation loss: 2.053135848814441

Epoch: 5| Step: 9
Training loss: 2.4173152446746826
Validation loss: 2.040506056559983

Epoch: 5| Step: 10
Training loss: 2.581604480743408
Validation loss: 2.0543917853345155

Epoch: 471| Step: 0
Training loss: 1.963280439376831
Validation loss: 2.029952290237591

Epoch: 5| Step: 1
Training loss: 1.7793489694595337
Validation loss: 2.0496353974906345

Epoch: 5| Step: 2
Training loss: 1.7992843389511108
Validation loss: 2.042359398257348

Epoch: 5| Step: 3
Training loss: 2.7896437644958496
Validation loss: 2.063565975876265

Epoch: 5| Step: 4
Training loss: 2.513258934020996
Validation loss: 2.0614041692467144

Epoch: 5| Step: 5
Training loss: 2.4240710735321045
Validation loss: 2.0581372707120833

Epoch: 5| Step: 6
Training loss: 1.7054901123046875
Validation loss: 2.0521133228014876

Epoch: 5| Step: 7
Training loss: 2.3206982612609863
Validation loss: 2.0540268805719193

Epoch: 5| Step: 8
Training loss: 2.460977554321289
Validation loss: 2.059358660892774

Epoch: 5| Step: 9
Training loss: 1.9527511596679688
Validation loss: 2.0634709570997503

Epoch: 5| Step: 10
Training loss: 2.1767754554748535
Validation loss: 2.070021762642809

Epoch: 472| Step: 0
Training loss: 1.947940468788147
Validation loss: 2.0528195340146302

Epoch: 5| Step: 1
Training loss: 2.103055953979492
Validation loss: 2.083088678698386

Epoch: 5| Step: 2
Training loss: 2.029064178466797
Validation loss: 2.043868928827265

Epoch: 5| Step: 3
Training loss: 2.533010482788086
Validation loss: 2.0704323655815533

Epoch: 5| Step: 4
Training loss: 2.5780954360961914
Validation loss: 2.055110708359749

Epoch: 5| Step: 5
Training loss: 1.5153990983963013
Validation loss: 2.036967444163497

Epoch: 5| Step: 6
Training loss: 2.153916835784912
Validation loss: 2.0516171763020177

Epoch: 5| Step: 7
Training loss: 2.261706590652466
Validation loss: 2.0513263030718734

Epoch: 5| Step: 8
Training loss: 2.2235848903656006
Validation loss: 2.049649046313378

Epoch: 5| Step: 9
Training loss: 2.459482431411743
Validation loss: 2.0552965030875257

Epoch: 5| Step: 10
Training loss: 1.8709673881530762
Validation loss: 2.0451606037796184

Epoch: 473| Step: 0
Training loss: 2.557206630706787
Validation loss: 2.048177141015248

Epoch: 5| Step: 1
Training loss: 2.2116518020629883
Validation loss: 2.0459950303518646

Epoch: 5| Step: 2
Training loss: 2.082261323928833
Validation loss: 2.0590595993944394

Epoch: 5| Step: 3
Training loss: 2.402639389038086
Validation loss: 2.0501587608809113

Epoch: 5| Step: 4
Training loss: 1.8210618495941162
Validation loss: 2.0620603548583163

Epoch: 5| Step: 5
Training loss: 1.8123743534088135
Validation loss: 2.07257874806722

Epoch: 5| Step: 6
Training loss: 2.43748140335083
Validation loss: 2.0604836581855692

Epoch: 5| Step: 7
Training loss: 1.359402060508728
Validation loss: 2.0540477370703094

Epoch: 5| Step: 8
Training loss: 2.2582528591156006
Validation loss: 2.0646287779654227

Epoch: 5| Step: 9
Training loss: 2.756258010864258
Validation loss: 2.0271696506008023

Epoch: 5| Step: 10
Training loss: 2.0223312377929688
Validation loss: 2.0414649799305904

Epoch: 474| Step: 0
Training loss: 2.640860080718994
Validation loss: 2.0367113287730882

Epoch: 5| Step: 1
Training loss: 1.8313041925430298
Validation loss: 2.0272321906141055

Epoch: 5| Step: 2
Training loss: 2.368100643157959
Validation loss: 2.040924128665719

Epoch: 5| Step: 3
Training loss: 2.212118625640869
Validation loss: 2.0538927688393542

Epoch: 5| Step: 4
Training loss: 1.6952769756317139
Validation loss: 2.0442363908213954

Epoch: 5| Step: 5
Training loss: 2.036102294921875
Validation loss: 2.032485031312512

Epoch: 5| Step: 6
Training loss: 2.482682943344116
Validation loss: 2.0437259943254533

Epoch: 5| Step: 7
Training loss: 2.36014986038208
Validation loss: 2.061382012982522

Epoch: 5| Step: 8
Training loss: 1.8601181507110596
Validation loss: 2.070735869869109

Epoch: 5| Step: 9
Training loss: 2.1293411254882812
Validation loss: 2.049616802123285

Epoch: 5| Step: 10
Training loss: 1.89730703830719
Validation loss: 2.057602515784643

Epoch: 475| Step: 0
Training loss: 2.048964738845825
Validation loss: 2.051259708660905

Epoch: 5| Step: 1
Training loss: 2.013049602508545
Validation loss: 2.053511061976033

Epoch: 5| Step: 2
Training loss: 2.2757925987243652
Validation loss: 2.0329970928930465

Epoch: 5| Step: 3
Training loss: 1.8823089599609375
Validation loss: 2.036735714122813

Epoch: 5| Step: 4
Training loss: 2.470527172088623
Validation loss: 2.0475447511160247

Epoch: 5| Step: 5
Training loss: 3.103912830352783
Validation loss: 2.032132920398507

Epoch: 5| Step: 6
Training loss: 2.268247604370117
Validation loss: 2.0451741192930486

Epoch: 5| Step: 7
Training loss: 2.2812821865081787
Validation loss: 2.0293193504374516

Epoch: 5| Step: 8
Training loss: 2.102128505706787
Validation loss: 2.040105083937286

Epoch: 5| Step: 9
Training loss: 1.5207322835922241
Validation loss: 2.02588995810478

Epoch: 5| Step: 10
Training loss: 1.5184208154678345
Validation loss: 2.0308044572030344

Epoch: 476| Step: 0
Training loss: 2.2157108783721924
Validation loss: 2.0250930632314375

Epoch: 5| Step: 1
Training loss: 2.442439556121826
Validation loss: 2.0460491821330082

Epoch: 5| Step: 2
Training loss: 2.319977283477783
Validation loss: 2.0665461247967136

Epoch: 5| Step: 3
Training loss: 2.408308506011963
Validation loss: 2.0634582017057683

Epoch: 5| Step: 4
Training loss: 2.0129451751708984
Validation loss: 2.05717332132401

Epoch: 5| Step: 5
Training loss: 2.0501999855041504
Validation loss: 2.0375414522745277

Epoch: 5| Step: 6
Training loss: 2.011204481124878
Validation loss: 2.028951084741982

Epoch: 5| Step: 7
Training loss: 1.9528133869171143
Validation loss: 2.0456988439765027

Epoch: 5| Step: 8
Training loss: 2.233004093170166
Validation loss: 2.0310784668050785

Epoch: 5| Step: 9
Training loss: 1.5980263948440552
Validation loss: 2.0414661694598455

Epoch: 5| Step: 10
Training loss: 2.339073419570923
Validation loss: 2.04176950711076

Epoch: 477| Step: 0
Training loss: 2.259937286376953
Validation loss: 2.0248623791561333

Epoch: 5| Step: 1
Training loss: 1.4752233028411865
Validation loss: 2.0345635696124007

Epoch: 5| Step: 2
Training loss: 1.557189702987671
Validation loss: 2.0341456500432824

Epoch: 5| Step: 3
Training loss: 2.4399800300598145
Validation loss: 2.0409755168422574

Epoch: 5| Step: 4
Training loss: 2.1494715213775635
Validation loss: 2.0523167323040705

Epoch: 5| Step: 5
Training loss: 2.3711495399475098
Validation loss: 2.078282545971614

Epoch: 5| Step: 6
Training loss: 1.8130874633789062
Validation loss: 2.1028070142192226

Epoch: 5| Step: 7
Training loss: 2.810917377471924
Validation loss: 2.086889465649923

Epoch: 5| Step: 8
Training loss: 2.0275168418884277
Validation loss: 2.0805392290956233

Epoch: 5| Step: 9
Training loss: 2.083369493484497
Validation loss: 2.067308315666773

Epoch: 5| Step: 10
Training loss: 2.6248817443847656
Validation loss: 2.044236485676099

Epoch: 478| Step: 0
Training loss: 2.3697874546051025
Validation loss: 2.0642880496158393

Epoch: 5| Step: 1
Training loss: 1.923295021057129
Validation loss: 2.047268457310174

Epoch: 5| Step: 2
Training loss: 2.4697093963623047
Validation loss: 2.0689706161458004

Epoch: 5| Step: 3
Training loss: 2.1449055671691895
Validation loss: 2.0715168599159486

Epoch: 5| Step: 4
Training loss: 1.7090171575546265
Validation loss: 2.060929752165271

Epoch: 5| Step: 5
Training loss: 1.941640853881836
Validation loss: 2.0482218573170323

Epoch: 5| Step: 6
Training loss: 1.94970703125
Validation loss: 2.093473962558213

Epoch: 5| Step: 7
Training loss: 2.102625846862793
Validation loss: 2.092436857120965

Epoch: 5| Step: 8
Training loss: 2.223146915435791
Validation loss: 2.139586094887026

Epoch: 5| Step: 9
Training loss: 2.2493021488189697
Validation loss: 2.137295403788167

Epoch: 5| Step: 10
Training loss: 3.046311378479004
Validation loss: 2.100127248353856

Epoch: 479| Step: 0
Training loss: 1.8702551126480103
Validation loss: 2.0433512195464103

Epoch: 5| Step: 1
Training loss: 1.671735167503357
Validation loss: 2.0317269807220786

Epoch: 5| Step: 2
Training loss: 1.9900785684585571
Validation loss: 2.0320290775709253

Epoch: 5| Step: 3
Training loss: 2.2574548721313477
Validation loss: 2.0280398399599138

Epoch: 5| Step: 4
Training loss: 2.517603635787964
Validation loss: 2.0342677613740325

Epoch: 5| Step: 5
Training loss: 2.579552412033081
Validation loss: 2.0045894627930014

Epoch: 5| Step: 6
Training loss: 2.098505735397339
Validation loss: 2.012244091239027

Epoch: 5| Step: 7
Training loss: 1.7787243127822876
Validation loss: 2.031052256143221

Epoch: 5| Step: 8
Training loss: 2.284088611602783
Validation loss: 2.0388531018328924

Epoch: 5| Step: 9
Training loss: 2.324612855911255
Validation loss: 2.0448838075002036

Epoch: 5| Step: 10
Training loss: 2.2988650798797607
Validation loss: 2.035108515011367

Epoch: 480| Step: 0
Training loss: 2.3459906578063965
Validation loss: 2.0329822007045952

Epoch: 5| Step: 1
Training loss: 2.365190029144287
Validation loss: 2.048817306436518

Epoch: 5| Step: 2
Training loss: 2.6688194274902344
Validation loss: 2.047191826246118

Epoch: 5| Step: 3
Training loss: 2.0686705112457275
Validation loss: 2.0553862971644246

Epoch: 5| Step: 4
Training loss: 1.432639479637146
Validation loss: 2.064342870507189

Epoch: 5| Step: 5
Training loss: 1.8860695362091064
Validation loss: 2.0500504355276785

Epoch: 5| Step: 6
Training loss: 2.495450973510742
Validation loss: 2.052456448155065

Epoch: 5| Step: 7
Training loss: 2.4383175373077393
Validation loss: 2.0272589857860277

Epoch: 5| Step: 8
Training loss: 2.6225924491882324
Validation loss: 2.0488001146624164

Epoch: 5| Step: 9
Training loss: 1.8428897857666016
Validation loss: 2.038338541984558

Epoch: 5| Step: 10
Training loss: 1.1302998065948486
Validation loss: 2.047255362233808

Epoch: 481| Step: 0
Training loss: 2.548377513885498
Validation loss: 2.0392472923442884

Epoch: 5| Step: 1
Training loss: 2.3500783443450928
Validation loss: 2.047011965064592

Epoch: 5| Step: 2
Training loss: 2.3561408519744873
Validation loss: 2.0223021353444746

Epoch: 5| Step: 3
Training loss: 2.594994306564331
Validation loss: 2.042193388426176

Epoch: 5| Step: 4
Training loss: 2.1925413608551025
Validation loss: 2.0320515363447127

Epoch: 5| Step: 5
Training loss: 1.6265439987182617
Validation loss: 2.055648075636997

Epoch: 5| Step: 6
Training loss: 2.0838191509246826
Validation loss: 2.0348252378484255

Epoch: 5| Step: 7
Training loss: 1.953454613685608
Validation loss: 2.04397306647352

Epoch: 5| Step: 8
Training loss: 2.126666307449341
Validation loss: 2.0632124306053243

Epoch: 5| Step: 9
Training loss: 1.8448762893676758
Validation loss: 2.083619244637028

Epoch: 5| Step: 10
Training loss: 1.9265203475952148
Validation loss: 2.112265991908248

Epoch: 482| Step: 0
Training loss: 2.212289333343506
Validation loss: 2.129531575787452

Epoch: 5| Step: 1
Training loss: 2.5860164165496826
Validation loss: 2.1577979749248875

Epoch: 5| Step: 2
Training loss: 2.8369946479797363
Validation loss: 2.1623663581827635

Epoch: 5| Step: 3
Training loss: 1.7229573726654053
Validation loss: 2.175039783600838

Epoch: 5| Step: 4
Training loss: 1.9376205205917358
Validation loss: 2.1551877606299614

Epoch: 5| Step: 5
Training loss: 2.432664155960083
Validation loss: 2.087599515914917

Epoch: 5| Step: 6
Training loss: 2.1019229888916016
Validation loss: 2.0641996322139615

Epoch: 5| Step: 7
Training loss: 2.3498988151550293
Validation loss: 2.0495253275799494

Epoch: 5| Step: 8
Training loss: 2.0668742656707764
Validation loss: 2.0251085988936888

Epoch: 5| Step: 9
Training loss: 1.670067548751831
Validation loss: 2.0214043022483907

Epoch: 5| Step: 10
Training loss: 1.8509485721588135
Validation loss: 2.047335268348776

Epoch: 483| Step: 0
Training loss: 2.8073678016662598
Validation loss: 2.0342213979331394

Epoch: 5| Step: 1
Training loss: 2.1968483924865723
Validation loss: 2.0380747164449384

Epoch: 5| Step: 2
Training loss: 2.688028335571289
Validation loss: 2.030787529483918

Epoch: 5| Step: 3
Training loss: 2.083085536956787
Validation loss: 2.0315147189683813

Epoch: 5| Step: 4
Training loss: 2.147735118865967
Validation loss: 2.0319484485092985

Epoch: 5| Step: 5
Training loss: 1.7592846155166626
Validation loss: 2.009521440793109

Epoch: 5| Step: 6
Training loss: 2.5156426429748535
Validation loss: 2.009440751485927

Epoch: 5| Step: 7
Training loss: 1.9265615940093994
Validation loss: 2.0177610023047334

Epoch: 5| Step: 8
Training loss: 1.4297740459442139
Validation loss: 2.0182097573434152

Epoch: 5| Step: 9
Training loss: 2.2952957153320312
Validation loss: 2.0737520481950495

Epoch: 5| Step: 10
Training loss: 2.1430397033691406
Validation loss: 2.051666941694034

Epoch: 484| Step: 0
Training loss: 2.3161797523498535
Validation loss: 2.069118957365713

Epoch: 5| Step: 1
Training loss: 2.2858786582946777
Validation loss: 2.08410378425352

Epoch: 5| Step: 2
Training loss: 2.161578416824341
Validation loss: 2.073143396326291

Epoch: 5| Step: 3
Training loss: 1.965383529663086
Validation loss: 2.077095182993079

Epoch: 5| Step: 4
Training loss: 1.9812164306640625
Validation loss: 2.058796441683205

Epoch: 5| Step: 5
Training loss: 1.8701165914535522
Validation loss: 2.057319038657732

Epoch: 5| Step: 6
Training loss: 2.482569456100464
Validation loss: 2.0386975093554427

Epoch: 5| Step: 7
Training loss: 1.9695348739624023
Validation loss: 2.0474521626708326

Epoch: 5| Step: 8
Training loss: 2.311781167984009
Validation loss: 2.068502443452035

Epoch: 5| Step: 9
Training loss: 1.9447476863861084
Validation loss: 2.061025386215538

Epoch: 5| Step: 10
Training loss: 2.2687795162200928
Validation loss: 2.057390655240705

Epoch: 485| Step: 0
Training loss: 2.568981170654297
Validation loss: 2.0451276533065306

Epoch: 5| Step: 1
Training loss: 2.5169005393981934
Validation loss: 2.045056930152319

Epoch: 5| Step: 2
Training loss: 2.441831588745117
Validation loss: 2.0351759413237214

Epoch: 5| Step: 3
Training loss: 1.5281236171722412
Validation loss: 2.05144945011344

Epoch: 5| Step: 4
Training loss: 2.063547134399414
Validation loss: 2.0755692143594064

Epoch: 5| Step: 5
Training loss: 2.292414903640747
Validation loss: 2.0543995877747894

Epoch: 5| Step: 6
Training loss: 2.2003138065338135
Validation loss: 2.0724022170548797

Epoch: 5| Step: 7
Training loss: 1.8637737035751343
Validation loss: 2.049267902169176

Epoch: 5| Step: 8
Training loss: 2.4906229972839355
Validation loss: 2.0232458524806525

Epoch: 5| Step: 9
Training loss: 1.8162803649902344
Validation loss: 2.0252520217690417

Epoch: 5| Step: 10
Training loss: 1.6824597120285034
Validation loss: 2.027282614861765

Epoch: 486| Step: 0
Training loss: 1.9903628826141357
Validation loss: 2.0349310341701714

Epoch: 5| Step: 1
Training loss: 1.6334857940673828
Validation loss: 2.022702391429614

Epoch: 5| Step: 2
Training loss: 3.19289493560791
Validation loss: 2.016313942529822

Epoch: 5| Step: 3
Training loss: 1.902024507522583
Validation loss: 2.0126916593120945

Epoch: 5| Step: 4
Training loss: 2.3971779346466064
Validation loss: 2.0074661624047065

Epoch: 5| Step: 5
Training loss: 1.9733966588974
Validation loss: 2.0225879812753327

Epoch: 5| Step: 6
Training loss: 2.0694451332092285
Validation loss: 2.0462768385487218

Epoch: 5| Step: 7
Training loss: 2.2452778816223145
Validation loss: 2.059574878343972

Epoch: 5| Step: 8
Training loss: 2.1633713245391846
Validation loss: 2.0536077894190305

Epoch: 5| Step: 9
Training loss: 1.9885857105255127
Validation loss: 2.0742340498073126

Epoch: 5| Step: 10
Training loss: 1.9426584243774414
Validation loss: 2.0810206859342513

Epoch: 487| Step: 0
Training loss: 1.463951826095581
Validation loss: 2.0727125880538777

Epoch: 5| Step: 1
Training loss: 2.4256815910339355
Validation loss: 2.0696577666908182

Epoch: 5| Step: 2
Training loss: 2.5258638858795166
Validation loss: 2.05380424504639

Epoch: 5| Step: 3
Training loss: 1.9638526439666748
Validation loss: 2.060765735564693

Epoch: 5| Step: 4
Training loss: 2.6148314476013184
Validation loss: 2.041611045919439

Epoch: 5| Step: 5
Training loss: 1.3135472536087036
Validation loss: 2.0566766031326784

Epoch: 5| Step: 6
Training loss: 2.6922316551208496
Validation loss: 2.032939244342107

Epoch: 5| Step: 7
Training loss: 2.04093599319458
Validation loss: 2.0454404097731396

Epoch: 5| Step: 8
Training loss: 2.428748369216919
Validation loss: 2.046011911925449

Epoch: 5| Step: 9
Training loss: 2.018437147140503
Validation loss: 2.0566116981608893

Epoch: 5| Step: 10
Training loss: 1.9254974126815796
Validation loss: 2.0432095091830016

Epoch: 488| Step: 0
Training loss: 2.4178919792175293
Validation loss: 2.0262610091957995

Epoch: 5| Step: 1
Training loss: 2.0773496627807617
Validation loss: 2.0324092270225607

Epoch: 5| Step: 2
Training loss: 1.7734897136688232
Validation loss: 2.024696671834556

Epoch: 5| Step: 3
Training loss: 1.8538278341293335
Validation loss: 2.0288981058264293

Epoch: 5| Step: 4
Training loss: 2.2462544441223145
Validation loss: 2.024475127138117

Epoch: 5| Step: 5
Training loss: 1.99639093875885
Validation loss: 2.0256787435982817

Epoch: 5| Step: 6
Training loss: 2.1233716011047363
Validation loss: 2.0526541843209216

Epoch: 5| Step: 7
Training loss: 1.9432117938995361
Validation loss: 2.061795296207551

Epoch: 5| Step: 8
Training loss: 2.1307168006896973
Validation loss: 2.0429318976658646

Epoch: 5| Step: 9
Training loss: 2.4781253337860107
Validation loss: 2.0612031772572506

Epoch: 5| Step: 10
Training loss: 2.388105869293213
Validation loss: 2.0467294044392084

Epoch: 489| Step: 0
Training loss: 2.2691617012023926
Validation loss: 2.039308650519258

Epoch: 5| Step: 1
Training loss: 2.344522714614868
Validation loss: 2.0245896988017584

Epoch: 5| Step: 2
Training loss: 1.6507641077041626
Validation loss: 2.0426000843765917

Epoch: 5| Step: 3
Training loss: 2.4556267261505127
Validation loss: 2.02634794481339

Epoch: 5| Step: 4
Training loss: 2.759814500808716
Validation loss: 2.036348082685983

Epoch: 5| Step: 5
Training loss: 1.4778684377670288
Validation loss: 2.043468826560564

Epoch: 5| Step: 6
Training loss: 2.148735523223877
Validation loss: 2.0324685112122567

Epoch: 5| Step: 7
Training loss: 1.7366504669189453
Validation loss: 2.0308591268395864

Epoch: 5| Step: 8
Training loss: 2.3916172981262207
Validation loss: 2.032996808328936

Epoch: 5| Step: 9
Training loss: 2.5183262825012207
Validation loss: 2.0482613835283505

Epoch: 5| Step: 10
Training loss: 1.5282135009765625
Validation loss: 2.0398018488319973

Epoch: 490| Step: 0
Training loss: 1.9674594402313232
Validation loss: 2.041027884329519

Epoch: 5| Step: 1
Training loss: 1.7155606746673584
Validation loss: 2.037754565156916

Epoch: 5| Step: 2
Training loss: 2.189267635345459
Validation loss: 2.048949603111513

Epoch: 5| Step: 3
Training loss: 1.8727343082427979
Validation loss: 2.030095882313226

Epoch: 5| Step: 4
Training loss: 1.8998454809188843
Validation loss: 2.0407378442825808

Epoch: 5| Step: 5
Training loss: 2.7493326663970947
Validation loss: 2.040138442029235

Epoch: 5| Step: 6
Training loss: 2.5946125984191895
Validation loss: 2.0480841026511243

Epoch: 5| Step: 7
Training loss: 2.1893279552459717
Validation loss: 2.0494955021847963

Epoch: 5| Step: 8
Training loss: 1.5672338008880615
Validation loss: 2.0529975352748746

Epoch: 5| Step: 9
Training loss: 2.7332053184509277
Validation loss: 2.0455553275282665

Epoch: 5| Step: 10
Training loss: 1.7715853452682495
Validation loss: 2.0450828511227845

Epoch: 491| Step: 0
Training loss: 3.1443419456481934
Validation loss: 2.0491960766494914

Epoch: 5| Step: 1
Training loss: 1.6957916021347046
Validation loss: 2.034558790986256

Epoch: 5| Step: 2
Training loss: 2.349499464035034
Validation loss: 2.014832468443019

Epoch: 5| Step: 3
Training loss: 1.9674001932144165
Validation loss: 2.035601794078786

Epoch: 5| Step: 4
Training loss: 2.6867775917053223
Validation loss: 2.0107677290516515

Epoch: 5| Step: 5
Training loss: 1.9272863864898682
Validation loss: 2.0321618254466722

Epoch: 5| Step: 6
Training loss: 1.7001653909683228
Validation loss: 2.019041743329776

Epoch: 5| Step: 7
Training loss: 1.9494218826293945
Validation loss: 2.042714954704367

Epoch: 5| Step: 8
Training loss: 1.8935706615447998
Validation loss: 2.0553578561352146

Epoch: 5| Step: 9
Training loss: 1.691474199295044
Validation loss: 2.044151521498157

Epoch: 5| Step: 10
Training loss: 2.3935089111328125
Validation loss: 2.050748017526442

Epoch: 492| Step: 0
Training loss: 1.8350034952163696
Validation loss: 2.041481482085361

Epoch: 5| Step: 1
Training loss: 2.0364723205566406
Validation loss: 2.0411593683304323

Epoch: 5| Step: 2
Training loss: 2.4478049278259277
Validation loss: 2.0521973435596754

Epoch: 5| Step: 3
Training loss: 2.757999897003174
Validation loss: 2.034967883940666

Epoch: 5| Step: 4
Training loss: 2.3445231914520264
Validation loss: 2.026404074443284

Epoch: 5| Step: 5
Training loss: 2.1778347492218018
Validation loss: 2.0492005655842442

Epoch: 5| Step: 6
Training loss: 1.9904630184173584
Validation loss: 2.058925977317236

Epoch: 5| Step: 7
Training loss: 1.7413980960845947
Validation loss: 2.0606685569209438

Epoch: 5| Step: 8
Training loss: 2.1665897369384766
Validation loss: 2.074039328482843

Epoch: 5| Step: 9
Training loss: 1.893707275390625
Validation loss: 2.047251614191199

Epoch: 5| Step: 10
Training loss: 1.8917332887649536
Validation loss: 2.0493320649670017

Epoch: 493| Step: 0
Training loss: 2.2838797569274902
Validation loss: 2.02626544942138

Epoch: 5| Step: 1
Training loss: 1.6575250625610352
Validation loss: 2.041290360112344

Epoch: 5| Step: 2
Training loss: 1.6334145069122314
Validation loss: 2.0293548581420735

Epoch: 5| Step: 3
Training loss: 2.612964153289795
Validation loss: 2.0199266736225416

Epoch: 5| Step: 4
Training loss: 1.7399479150772095
Validation loss: 2.023699282318033

Epoch: 5| Step: 5
Training loss: 1.8881912231445312
Validation loss: 2.015327158794608

Epoch: 5| Step: 6
Training loss: 2.131868362426758
Validation loss: 2.0273821584640013

Epoch: 5| Step: 7
Training loss: 2.4519245624542236
Validation loss: 2.0102401805180374

Epoch: 5| Step: 8
Training loss: 2.9279580116271973
Validation loss: 2.0138501813334804

Epoch: 5| Step: 9
Training loss: 1.729418158531189
Validation loss: 2.0233948025652158

Epoch: 5| Step: 10
Training loss: 2.098209857940674
Validation loss: 2.0255186198860087

Epoch: 494| Step: 0
Training loss: 1.8810482025146484
Validation loss: 2.0317281625604116

Epoch: 5| Step: 1
Training loss: 2.706712007522583
Validation loss: 2.0086701531564035

Epoch: 5| Step: 2
Training loss: 1.924555778503418
Validation loss: 2.0231089015160837

Epoch: 5| Step: 3
Training loss: 2.5718185901641846
Validation loss: 2.0401536880000943

Epoch: 5| Step: 4
Training loss: 2.057586431503296
Validation loss: 2.017722209294637

Epoch: 5| Step: 5
Training loss: 1.5071957111358643
Validation loss: 2.0208891809627576

Epoch: 5| Step: 6
Training loss: 1.4693701267242432
Validation loss: 2.0533741520297144

Epoch: 5| Step: 7
Training loss: 2.060756206512451
Validation loss: 2.0352030261870353

Epoch: 5| Step: 8
Training loss: 2.2385966777801514
Validation loss: 2.0287374963042555

Epoch: 5| Step: 9
Training loss: 2.1393065452575684
Validation loss: 2.0784010400054274

Epoch: 5| Step: 10
Training loss: 2.644218683242798
Validation loss: 2.0713558966113674

Epoch: 495| Step: 0
Training loss: 1.178694248199463
Validation loss: 2.044148405392965

Epoch: 5| Step: 1
Training loss: 2.2635817527770996
Validation loss: 2.046053650558636

Epoch: 5| Step: 2
Training loss: 2.0980541706085205
Validation loss: 2.050616164361277

Epoch: 5| Step: 3
Training loss: 2.3213887214660645
Validation loss: 2.0277958890443206

Epoch: 5| Step: 4
Training loss: 2.9347400665283203
Validation loss: 2.019296928118634

Epoch: 5| Step: 5
Training loss: 1.867376685142517
Validation loss: 2.0232387691415767

Epoch: 5| Step: 6
Training loss: 1.8535726070404053
Validation loss: 2.030275176930171

Epoch: 5| Step: 7
Training loss: 2.1875646114349365
Validation loss: 2.00641728472966

Epoch: 5| Step: 8
Training loss: 2.3259329795837402
Validation loss: 2.0356160363843365

Epoch: 5| Step: 9
Training loss: 1.949995756149292
Validation loss: 2.028630905253913

Epoch: 5| Step: 10
Training loss: 2.250659704208374
Validation loss: 2.0185954647679485

Epoch: 496| Step: 0
Training loss: 2.4945054054260254
Validation loss: 2.022418195201505

Epoch: 5| Step: 1
Training loss: 2.1657981872558594
Validation loss: 2.0342424838773665

Epoch: 5| Step: 2
Training loss: 2.0818989276885986
Validation loss: 2.0671296940054944

Epoch: 5| Step: 3
Training loss: 2.568049907684326
Validation loss: 2.042958482619255

Epoch: 5| Step: 4
Training loss: 1.8482249975204468
Validation loss: 2.0300818104897775

Epoch: 5| Step: 5
Training loss: 1.564793586730957
Validation loss: 2.011612912660004

Epoch: 5| Step: 6
Training loss: 2.30328631401062
Validation loss: 1.9978768940894835

Epoch: 5| Step: 7
Training loss: 1.957489252090454
Validation loss: 1.9947628616004862

Epoch: 5| Step: 8
Training loss: 2.421794891357422
Validation loss: 2.004845206455518

Epoch: 5| Step: 9
Training loss: 1.8773720264434814
Validation loss: 2.019876100683725

Epoch: 5| Step: 10
Training loss: 1.831858515739441
Validation loss: 2.0079788200316893

Epoch: 497| Step: 0
Training loss: 1.8593740463256836
Validation loss: 1.9952999673863894

Epoch: 5| Step: 1
Training loss: 2.4160122871398926
Validation loss: 2.024799532787774

Epoch: 5| Step: 2
Training loss: 2.0373730659484863
Validation loss: 1.9928945610600133

Epoch: 5| Step: 3
Training loss: 2.062303066253662
Validation loss: 2.0294350321574877

Epoch: 5| Step: 4
Training loss: 3.403642177581787
Validation loss: 2.011298830791186

Epoch: 5| Step: 5
Training loss: 1.987065076828003
Validation loss: 2.0317072586346696

Epoch: 5| Step: 6
Training loss: 2.3346104621887207
Validation loss: 2.0419797564065583

Epoch: 5| Step: 7
Training loss: 1.8858997821807861
Validation loss: 2.0409455401923067

Epoch: 5| Step: 8
Training loss: 1.7027218341827393
Validation loss: 2.0511355464176466

Epoch: 5| Step: 9
Training loss: 1.743613839149475
Validation loss: 2.0258225112833004

Epoch: 5| Step: 10
Training loss: 1.806223750114441
Validation loss: 2.046373914646846

Epoch: 498| Step: 0
Training loss: 2.4183876514434814
Validation loss: 2.0508348249620005

Epoch: 5| Step: 1
Training loss: 2.244819402694702
Validation loss: 2.031273473975479

Epoch: 5| Step: 2
Training loss: 1.4732753038406372
Validation loss: 2.0170791174775813

Epoch: 5| Step: 3
Training loss: 2.5107381343841553
Validation loss: 2.019000553315686

Epoch: 5| Step: 4
Training loss: 1.7587238550186157
Validation loss: 2.0125430360917123

Epoch: 5| Step: 5
Training loss: 2.2898898124694824
Validation loss: 1.997281115542176

Epoch: 5| Step: 6
Training loss: 2.5849738121032715
Validation loss: 2.0267832253568914

Epoch: 5| Step: 7
Training loss: 2.0470516681671143
Validation loss: 2.0169379326605026

Epoch: 5| Step: 8
Training loss: 1.8042396306991577
Validation loss: 2.0249644505080355

Epoch: 5| Step: 9
Training loss: 2.2254364490509033
Validation loss: 2.0439242368103354

Epoch: 5| Step: 10
Training loss: 1.716382384300232
Validation loss: 2.0436820022521482

Epoch: 499| Step: 0
Training loss: 1.8961963653564453
Validation loss: 2.0498547195106425

Epoch: 5| Step: 1
Training loss: 2.3658459186553955
Validation loss: 2.03143596905534

Epoch: 5| Step: 2
Training loss: 2.088554859161377
Validation loss: 2.0190960745657645

Epoch: 5| Step: 3
Training loss: 2.0090105533599854
Validation loss: 2.0198629030617337

Epoch: 5| Step: 4
Training loss: 2.2094995975494385
Validation loss: 2.00861116122174

Epoch: 5| Step: 5
Training loss: 2.207190990447998
Validation loss: 2.020231568685142

Epoch: 5| Step: 6
Training loss: 2.3264622688293457
Validation loss: 2.027827766633803

Epoch: 5| Step: 7
Training loss: 2.241990566253662
Validation loss: 2.0315773974182787

Epoch: 5| Step: 8
Training loss: 2.020068883895874
Validation loss: 2.0255904684784594

Epoch: 5| Step: 9
Training loss: 1.7555338144302368
Validation loss: 2.035051925207979

Epoch: 5| Step: 10
Training loss: 1.959344506263733
Validation loss: 2.020728744486327

Epoch: 500| Step: 0
Training loss: 2.0869452953338623
Validation loss: 2.0093224151160127

Epoch: 5| Step: 1
Training loss: 2.0229365825653076
Validation loss: 2.044194020250792

Epoch: 5| Step: 2
Training loss: 1.6587793827056885
Validation loss: 2.0230759625793784

Epoch: 5| Step: 3
Training loss: 2.019622325897217
Validation loss: 2.0384126734989945

Epoch: 5| Step: 4
Training loss: 2.933910846710205
Validation loss: 2.032654014966821

Epoch: 5| Step: 5
Training loss: 1.9489043951034546
Validation loss: 2.0437081565139112

Epoch: 5| Step: 6
Training loss: 1.4187524318695068
Validation loss: 2.0624044377316713

Epoch: 5| Step: 7
Training loss: 2.4736151695251465
Validation loss: 2.0542213762960126

Epoch: 5| Step: 8
Training loss: 2.2773077487945557
Validation loss: 2.069165183651832

Epoch: 5| Step: 9
Training loss: 2.2484934329986572
Validation loss: 2.06775725272394

Epoch: 5| Step: 10
Training loss: 2.017606258392334
Validation loss: 2.0721481038678076

Testing loss: 2.352598693635729

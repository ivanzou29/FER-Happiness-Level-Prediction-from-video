Epoch: 1| Step: 0
Training loss: 4.1451735496521
Validation loss: 5.137577559358331

Epoch: 5| Step: 1
Training loss: 5.081243991851807
Validation loss: 5.132772722551899

Epoch: 5| Step: 2
Training loss: 5.255999565124512
Validation loss: 5.128574340574203

Epoch: 5| Step: 3
Training loss: 4.55259895324707
Validation loss: 5.124224847362887

Epoch: 5| Step: 4
Training loss: 4.236483097076416
Validation loss: 5.119807284365418

Epoch: 5| Step: 5
Training loss: 6.187518119812012
Validation loss: 5.115649033618229

Epoch: 5| Step: 6
Training loss: 5.409358024597168
Validation loss: 5.111305359871157

Epoch: 5| Step: 7
Training loss: 4.582339286804199
Validation loss: 5.107538397594165

Epoch: 5| Step: 8
Training loss: 4.977121829986572
Validation loss: 5.103390139918173

Epoch: 5| Step: 9
Training loss: 4.061434745788574
Validation loss: 5.099509859597811

Epoch: 5| Step: 10
Training loss: 5.662781238555908
Validation loss: 5.095594129254741

Epoch: 2| Step: 0
Training loss: 5.346471786499023
Validation loss: 5.091354239371515

Epoch: 5| Step: 1
Training loss: 3.5829575061798096
Validation loss: 5.087011762844619

Epoch: 5| Step: 2
Training loss: 4.8688740730285645
Validation loss: 5.0826290781779955

Epoch: 5| Step: 3
Training loss: 5.284750938415527
Validation loss: 5.077970684215587

Epoch: 5| Step: 4
Training loss: 5.602203369140625
Validation loss: 5.072597903590048

Epoch: 5| Step: 5
Training loss: 5.480265140533447
Validation loss: 5.067393938700358

Epoch: 5| Step: 6
Training loss: 3.016023635864258
Validation loss: 5.061768742017849

Epoch: 5| Step: 7
Training loss: 6.2645263671875
Validation loss: 5.056071199396605

Epoch: 5| Step: 8
Training loss: 5.124128818511963
Validation loss: 5.049333223732569

Epoch: 5| Step: 9
Training loss: 5.170902252197266
Validation loss: 5.043048453587358

Epoch: 5| Step: 10
Training loss: 3.5386834144592285
Validation loss: 5.035936017190257

Epoch: 3| Step: 0
Training loss: 4.108990669250488
Validation loss: 5.029048119821856

Epoch: 5| Step: 1
Training loss: 5.18770694732666
Validation loss: 5.021057805707378

Epoch: 5| Step: 2
Training loss: 4.597216606140137
Validation loss: 5.012805026064637

Epoch: 5| Step: 3
Training loss: 6.106969356536865
Validation loss: 5.004414889120286

Epoch: 5| Step: 4
Training loss: 4.976784706115723
Validation loss: 4.9961819443651425

Epoch: 5| Step: 5
Training loss: 5.2350592613220215
Validation loss: 4.986777536330685

Epoch: 5| Step: 6
Training loss: 4.5092267990112305
Validation loss: 4.977059933447069

Epoch: 5| Step: 7
Training loss: 4.067373275756836
Validation loss: 4.967019491298224

Epoch: 5| Step: 8
Training loss: 4.759821891784668
Validation loss: 4.956585232929517

Epoch: 5| Step: 9
Training loss: 4.228444576263428
Validation loss: 4.945284515298823

Epoch: 5| Step: 10
Training loss: 4.789798736572266
Validation loss: 4.9340693976289485

Epoch: 4| Step: 0
Training loss: 3.5502567291259766
Validation loss: 4.9220904278498825

Epoch: 5| Step: 1
Training loss: 4.346741199493408
Validation loss: 4.909580461440548

Epoch: 5| Step: 2
Training loss: 6.045605182647705
Validation loss: 4.896787053795271

Epoch: 5| Step: 3
Training loss: 3.4264957904815674
Validation loss: 4.883490393238683

Epoch: 5| Step: 4
Training loss: 5.209826469421387
Validation loss: 4.869743013894686

Epoch: 5| Step: 5
Training loss: 4.6031999588012695
Validation loss: 4.8552897258471415

Epoch: 5| Step: 6
Training loss: 5.0218586921691895
Validation loss: 4.840629798109814

Epoch: 5| Step: 7
Training loss: 4.124878883361816
Validation loss: 4.824609587269444

Epoch: 5| Step: 8
Training loss: 4.73006534576416
Validation loss: 4.808082447257093

Epoch: 5| Step: 9
Training loss: 4.455541610717773
Validation loss: 4.791038810565907

Epoch: 5| Step: 10
Training loss: 5.71990966796875
Validation loss: 4.774172003551196

Epoch: 5| Step: 0
Training loss: 3.9929022789001465
Validation loss: 4.756383865110336

Epoch: 5| Step: 1
Training loss: 3.9569308757781982
Validation loss: 4.738178812047487

Epoch: 5| Step: 2
Training loss: 4.867110729217529
Validation loss: 4.7198017848435265

Epoch: 5| Step: 3
Training loss: 5.162543296813965
Validation loss: 4.7002830043915775

Epoch: 5| Step: 4
Training loss: 2.9361729621887207
Validation loss: 4.680620803627916

Epoch: 5| Step: 5
Training loss: 6.091454982757568
Validation loss: 4.660635804617277

Epoch: 5| Step: 6
Training loss: 5.200252532958984
Validation loss: 4.639502494565902

Epoch: 5| Step: 7
Training loss: 5.291676044464111
Validation loss: 4.6171608637737975

Epoch: 5| Step: 8
Training loss: 3.810551404953003
Validation loss: 4.59590987236269

Epoch: 5| Step: 9
Training loss: 3.935452938079834
Validation loss: 4.573272479477749

Epoch: 5| Step: 10
Training loss: 3.4571022987365723
Validation loss: 4.548899558282668

Epoch: 6| Step: 0
Training loss: 4.733424186706543
Validation loss: 4.52479923412364

Epoch: 5| Step: 1
Training loss: 4.679036617279053
Validation loss: 4.5008965922940165

Epoch: 5| Step: 2
Training loss: 3.6184134483337402
Validation loss: 4.474402960910592

Epoch: 5| Step: 3
Training loss: 4.654638767242432
Validation loss: 4.450284034975113

Epoch: 5| Step: 4
Training loss: 5.3395490646362305
Validation loss: 4.422326252024661

Epoch: 5| Step: 5
Training loss: 3.3001255989074707
Validation loss: 4.397148527124877

Epoch: 5| Step: 6
Training loss: 4.852627754211426
Validation loss: 4.369889228574691

Epoch: 5| Step: 7
Training loss: 4.543885231018066
Validation loss: 4.342633957503944

Epoch: 5| Step: 8
Training loss: 3.529759168624878
Validation loss: 4.316183018428023

Epoch: 5| Step: 9
Training loss: 3.0419082641601562
Validation loss: 4.289190497449649

Epoch: 5| Step: 10
Training loss: 3.7643849849700928
Validation loss: 4.2629267964311826

Epoch: 7| Step: 0
Training loss: 3.4460506439208984
Validation loss: 4.237595365893457

Epoch: 5| Step: 1
Training loss: 4.959758758544922
Validation loss: 4.211921522694249

Epoch: 5| Step: 2
Training loss: 3.9588608741760254
Validation loss: 4.18590138548164

Epoch: 5| Step: 3
Training loss: 4.529601573944092
Validation loss: 4.15873913098407

Epoch: 5| Step: 4
Training loss: 3.9786007404327393
Validation loss: 4.13383351602862

Epoch: 5| Step: 5
Training loss: 4.152064800262451
Validation loss: 4.107258955637614

Epoch: 5| Step: 6
Training loss: 3.2836990356445312
Validation loss: 4.083253306727255

Epoch: 5| Step: 7
Training loss: 3.739729642868042
Validation loss: 4.060522438377462

Epoch: 5| Step: 8
Training loss: 3.8703181743621826
Validation loss: 4.037200594461092

Epoch: 5| Step: 9
Training loss: 3.8405113220214844
Validation loss: 4.014290735285769

Epoch: 5| Step: 10
Training loss: 3.5020949840545654
Validation loss: 3.992967659427274

Epoch: 8| Step: 0
Training loss: 4.053800106048584
Validation loss: 3.970926089953351

Epoch: 5| Step: 1
Training loss: 3.91483736038208
Validation loss: 3.9492743451108216

Epoch: 5| Step: 2
Training loss: 4.16513729095459
Validation loss: 3.9290564034574773

Epoch: 5| Step: 3
Training loss: 4.834986209869385
Validation loss: 3.908702440159295

Epoch: 5| Step: 4
Training loss: 2.3559582233428955
Validation loss: 3.8906313142468854

Epoch: 5| Step: 5
Training loss: 3.6589560508728027
Validation loss: 3.8717817465464273

Epoch: 5| Step: 6
Training loss: 4.665266990661621
Validation loss: 3.8543955433753228

Epoch: 5| Step: 7
Training loss: 3.6245129108428955
Validation loss: 3.8387558434599187

Epoch: 5| Step: 8
Training loss: 3.4839210510253906
Validation loss: 3.820424623386834

Epoch: 5| Step: 9
Training loss: 3.2268059253692627
Validation loss: 3.802721292741837

Epoch: 5| Step: 10
Training loss: 3.1493828296661377
Validation loss: 3.787302383812525

Epoch: 9| Step: 0
Training loss: 3.5180022716522217
Validation loss: 3.770496681172361

Epoch: 5| Step: 1
Training loss: 4.389309406280518
Validation loss: 3.7585708505363873

Epoch: 5| Step: 2
Training loss: 3.6506152153015137
Validation loss: 3.743254156522853

Epoch: 5| Step: 3
Training loss: 3.776082992553711
Validation loss: 3.7289330626046784

Epoch: 5| Step: 4
Training loss: 3.9144980907440186
Validation loss: 3.7154174081740843

Epoch: 5| Step: 5
Training loss: 3.3969085216522217
Validation loss: 3.702061417282269

Epoch: 5| Step: 6
Training loss: 4.726487636566162
Validation loss: 3.690005058883339

Epoch: 5| Step: 7
Training loss: 3.2071807384490967
Validation loss: 3.6791223120945755

Epoch: 5| Step: 8
Training loss: 2.5891103744506836
Validation loss: 3.668705763355378

Epoch: 5| Step: 9
Training loss: 3.637176513671875
Validation loss: 3.658105404146256

Epoch: 5| Step: 10
Training loss: 2.699256420135498
Validation loss: 3.647899850722282

Epoch: 10| Step: 0
Training loss: 4.0617828369140625
Validation loss: 3.6375599958563365

Epoch: 5| Step: 1
Training loss: 3.4269967079162598
Validation loss: 3.627747735669536

Epoch: 5| Step: 2
Training loss: 3.193798542022705
Validation loss: 3.618394726066179

Epoch: 5| Step: 3
Training loss: 4.273595809936523
Validation loss: 3.610618147798764

Epoch: 5| Step: 4
Training loss: 4.062358379364014
Validation loss: 3.6032298149601107

Epoch: 5| Step: 5
Training loss: 3.9096322059631348
Validation loss: 3.593948479621641

Epoch: 5| Step: 6
Training loss: 3.650691509246826
Validation loss: 3.587737760236186

Epoch: 5| Step: 7
Training loss: 3.382178544998169
Validation loss: 3.5811986359216834

Epoch: 5| Step: 8
Training loss: 2.6339352130889893
Validation loss: 3.5715850066113215

Epoch: 5| Step: 9
Training loss: 3.7421252727508545
Validation loss: 3.5647085251346713

Epoch: 5| Step: 10
Training loss: 2.0960216522216797
Validation loss: 3.5603086461303053

Epoch: 11| Step: 0
Training loss: 2.5188679695129395
Validation loss: 3.552033911469162

Epoch: 5| Step: 1
Training loss: 3.412480115890503
Validation loss: 3.5477139565252487

Epoch: 5| Step: 2
Training loss: 2.9026248455047607
Validation loss: 3.540288758534257

Epoch: 5| Step: 3
Training loss: 4.458698272705078
Validation loss: 3.535849114899994

Epoch: 5| Step: 4
Training loss: 4.500410079956055
Validation loss: 3.5273897365857194

Epoch: 5| Step: 5
Training loss: 3.4789981842041016
Validation loss: 3.522416958244898

Epoch: 5| Step: 6
Training loss: 3.642589569091797
Validation loss: 3.5140074017227336

Epoch: 5| Step: 7
Training loss: 3.082268476486206
Validation loss: 3.510203271783808

Epoch: 5| Step: 8
Training loss: 2.626718282699585
Validation loss: 3.5053503000608055

Epoch: 5| Step: 9
Training loss: 4.207526683807373
Validation loss: 3.5009203213517384

Epoch: 5| Step: 10
Training loss: 3.102006435394287
Validation loss: 3.4951542192889797

Epoch: 12| Step: 0
Training loss: 3.734051465988159
Validation loss: 3.49003864360112

Epoch: 5| Step: 1
Training loss: 3.8449950218200684
Validation loss: 3.4868138964458177

Epoch: 5| Step: 2
Training loss: 3.1522974967956543
Validation loss: 3.4790868733518865

Epoch: 5| Step: 3
Training loss: 3.2968239784240723
Validation loss: 3.4753892831904913

Epoch: 5| Step: 4
Training loss: 3.363377332687378
Validation loss: 3.471662808490056

Epoch: 5| Step: 5
Training loss: 4.395345211029053
Validation loss: 3.4649369306461786

Epoch: 5| Step: 6
Training loss: 3.490248203277588
Validation loss: 3.458735014802666

Epoch: 5| Step: 7
Training loss: 3.4103329181671143
Validation loss: 3.4551905534600698

Epoch: 5| Step: 8
Training loss: 2.861384391784668
Validation loss: 3.450119346700689

Epoch: 5| Step: 9
Training loss: 2.713651180267334
Validation loss: 3.4446089037003054

Epoch: 5| Step: 10
Training loss: 3.159816026687622
Validation loss: 3.441392908814133

Epoch: 13| Step: 0
Training loss: 2.737990140914917
Validation loss: 3.4367891383427445

Epoch: 5| Step: 1
Training loss: 3.734635829925537
Validation loss: 3.4344052781340895

Epoch: 5| Step: 2
Training loss: 3.3698532581329346
Validation loss: 3.431835033560312

Epoch: 5| Step: 3
Training loss: 2.780837297439575
Validation loss: 3.4249351742447063

Epoch: 5| Step: 4
Training loss: 2.6603052616119385
Validation loss: 3.422205512241651

Epoch: 5| Step: 5
Training loss: 3.8237228393554688
Validation loss: 3.417243739610077

Epoch: 5| Step: 6
Training loss: 3.731433391571045
Validation loss: 3.4121365265179704

Epoch: 5| Step: 7
Training loss: 3.521158218383789
Validation loss: 3.4085499214869674

Epoch: 5| Step: 8
Training loss: 4.037729263305664
Validation loss: 3.4056638902233494

Epoch: 5| Step: 9
Training loss: 2.8433868885040283
Validation loss: 3.3990326722462973

Epoch: 5| Step: 10
Training loss: 3.818152904510498
Validation loss: 3.3961854903928694

Epoch: 14| Step: 0
Training loss: 2.4040446281433105
Validation loss: 3.395406589713148

Epoch: 5| Step: 1
Training loss: 2.820535659790039
Validation loss: 3.3909497825048303

Epoch: 5| Step: 2
Training loss: 3.2313010692596436
Validation loss: 3.3879812122673116

Epoch: 5| Step: 3
Training loss: 2.7395224571228027
Validation loss: 3.386772007070562

Epoch: 5| Step: 4
Training loss: 3.6897616386413574
Validation loss: 3.3790836693138204

Epoch: 5| Step: 5
Training loss: 3.3923745155334473
Validation loss: 3.372240556183682

Epoch: 5| Step: 6
Training loss: 3.6590113639831543
Validation loss: 3.371148027399535

Epoch: 5| Step: 7
Training loss: 3.845470428466797
Validation loss: 3.3660765899124967

Epoch: 5| Step: 8
Training loss: 3.9651846885681152
Validation loss: 3.3645837845340854

Epoch: 5| Step: 9
Training loss: 2.6792004108428955
Validation loss: 3.361677405654743

Epoch: 5| Step: 10
Training loss: 4.372383117675781
Validation loss: 3.358066092255295

Epoch: 15| Step: 0
Training loss: 2.896838665008545
Validation loss: 3.353616117149271

Epoch: 5| Step: 1
Training loss: 3.802100419998169
Validation loss: 3.3492003897184968

Epoch: 5| Step: 2
Training loss: 3.224802017211914
Validation loss: 3.34689482309485

Epoch: 5| Step: 3
Training loss: 2.6900832653045654
Validation loss: 3.3405012007682555

Epoch: 5| Step: 4
Training loss: 3.644171953201294
Validation loss: 3.3403026570555983

Epoch: 5| Step: 5
Training loss: 3.7223830223083496
Validation loss: 3.337132820519068

Epoch: 5| Step: 6
Training loss: 3.3358638286590576
Validation loss: 3.3346643319693943

Epoch: 5| Step: 7
Training loss: 2.184396982192993
Validation loss: 3.3280788442139984

Epoch: 5| Step: 8
Training loss: 4.292848587036133
Validation loss: 3.324585594156737

Epoch: 5| Step: 9
Training loss: 3.0154671669006348
Validation loss: 3.3220978783022974

Epoch: 5| Step: 10
Training loss: 3.4974722862243652
Validation loss: 3.32488670656758

Epoch: 16| Step: 0
Training loss: 3.0755834579467773
Validation loss: 3.3162771424939557

Epoch: 5| Step: 1
Training loss: 3.0963504314422607
Validation loss: 3.3130086057929584

Epoch: 5| Step: 2
Training loss: 4.035618305206299
Validation loss: 3.310247236682523

Epoch: 5| Step: 3
Training loss: 3.89848256111145
Validation loss: 3.3055366444331344

Epoch: 5| Step: 4
Training loss: 3.224231719970703
Validation loss: 3.3018590942505868

Epoch: 5| Step: 5
Training loss: 3.500380277633667
Validation loss: 3.2991205517963698

Epoch: 5| Step: 6
Training loss: 2.649115800857544
Validation loss: 3.2962696372821765

Epoch: 5| Step: 7
Training loss: 2.8770253658294678
Validation loss: 3.294258389421689

Epoch: 5| Step: 8
Training loss: 3.46227765083313
Validation loss: 3.292834310121434

Epoch: 5| Step: 9
Training loss: 3.2369015216827393
Validation loss: 3.289015103411931

Epoch: 5| Step: 10
Training loss: 2.8400630950927734
Validation loss: 3.283189724850398

Epoch: 17| Step: 0
Training loss: 3.0096170902252197
Validation loss: 3.2824569620111936

Epoch: 5| Step: 1
Training loss: 2.603666067123413
Validation loss: 3.280315101787608

Epoch: 5| Step: 2
Training loss: 3.9246582984924316
Validation loss: 3.2788965753329697

Epoch: 5| Step: 3
Training loss: 2.3552327156066895
Validation loss: 3.275606827069354

Epoch: 5| Step: 4
Training loss: 2.826018810272217
Validation loss: 3.2783321821561424

Epoch: 5| Step: 5
Training loss: 3.580960512161255
Validation loss: 3.2772082487742105

Epoch: 5| Step: 6
Training loss: 2.7927582263946533
Validation loss: 3.2752529805706394

Epoch: 5| Step: 7
Training loss: 3.5536110401153564
Validation loss: 3.270501305980067

Epoch: 5| Step: 8
Training loss: 3.5430264472961426
Validation loss: 3.2631897490511657

Epoch: 5| Step: 9
Training loss: 3.8306357860565186
Validation loss: 3.256962535201862

Epoch: 5| Step: 10
Training loss: 3.7693660259246826
Validation loss: 3.2585848454506166

Epoch: 18| Step: 0
Training loss: 3.365338087081909
Validation loss: 3.2559259399291007

Epoch: 5| Step: 1
Training loss: 3.8093643188476562
Validation loss: 3.2529002363963793

Epoch: 5| Step: 2
Training loss: 3.762883424758911
Validation loss: 3.2513472341722056

Epoch: 5| Step: 3
Training loss: 3.480311155319214
Validation loss: 3.2468628806452595

Epoch: 5| Step: 4
Training loss: 2.4586338996887207
Validation loss: 3.241746643538116

Epoch: 5| Step: 5
Training loss: 2.7726688385009766
Validation loss: 3.238758707559237

Epoch: 5| Step: 6
Training loss: 2.848388195037842
Validation loss: 3.2355279127756753

Epoch: 5| Step: 7
Training loss: 3.47857403755188
Validation loss: 3.232831060245473

Epoch: 5| Step: 8
Training loss: 3.5302066802978516
Validation loss: 3.2293229615816506

Epoch: 5| Step: 9
Training loss: 2.946463108062744
Validation loss: 3.2283477860112346

Epoch: 5| Step: 10
Training loss: 3.0058443546295166
Validation loss: 3.226523353207496

Epoch: 19| Step: 0
Training loss: 3.3388073444366455
Validation loss: 3.223322909365418

Epoch: 5| Step: 1
Training loss: 2.965363025665283
Validation loss: 3.22085060868212

Epoch: 5| Step: 2
Training loss: 2.7776618003845215
Validation loss: 3.2208770193079466

Epoch: 5| Step: 3
Training loss: 3.4020657539367676
Validation loss: 3.2202723821004233

Epoch: 5| Step: 4
Training loss: 3.2993216514587402
Validation loss: 3.214861598066104

Epoch: 5| Step: 5
Training loss: 2.6276257038116455
Validation loss: 3.2139745476425334

Epoch: 5| Step: 6
Training loss: 2.894892454147339
Validation loss: 3.208607465990128

Epoch: 5| Step: 7
Training loss: 3.856919050216675
Validation loss: 3.206560106687648

Epoch: 5| Step: 8
Training loss: 3.2102363109588623
Validation loss: 3.2033980533640873

Epoch: 5| Step: 9
Training loss: 3.3852524757385254
Validation loss: 3.2005538735338437

Epoch: 5| Step: 10
Training loss: 3.551187753677368
Validation loss: 3.2001175982977754

Epoch: 20| Step: 0
Training loss: 2.8841030597686768
Validation loss: 3.2015021411321496

Epoch: 5| Step: 1
Training loss: 3.330287218093872
Validation loss: 3.1933231123032106

Epoch: 5| Step: 2
Training loss: 3.4800026416778564
Validation loss: 3.1911600943534606

Epoch: 5| Step: 3
Training loss: 3.0014872550964355
Validation loss: 3.1892753288310063

Epoch: 5| Step: 4
Training loss: 4.418095588684082
Validation loss: 3.1861775280326925

Epoch: 5| Step: 5
Training loss: 3.009281635284424
Validation loss: 3.1862135805109495

Epoch: 5| Step: 6
Training loss: 2.8093931674957275
Validation loss: 3.181229060696017

Epoch: 5| Step: 7
Training loss: 2.373603343963623
Validation loss: 3.1805659571001605

Epoch: 5| Step: 8
Training loss: 3.17814564704895
Validation loss: 3.1764840105528473

Epoch: 5| Step: 9
Training loss: 3.022228717803955
Validation loss: 3.170547157205561

Epoch: 5| Step: 10
Training loss: 3.6171281337738037
Validation loss: 3.1728090624655447

Epoch: 21| Step: 0
Training loss: 2.72459077835083
Validation loss: 3.171220653800554

Epoch: 5| Step: 1
Training loss: 2.6597113609313965
Validation loss: 3.167917533587384

Epoch: 5| Step: 2
Training loss: 3.178473711013794
Validation loss: 3.1662063034631873

Epoch: 5| Step: 3
Training loss: 3.3437581062316895
Validation loss: 3.161974573648104

Epoch: 5| Step: 4
Training loss: 2.9540822505950928
Validation loss: 3.1629111125905025

Epoch: 5| Step: 5
Training loss: 3.162464141845703
Validation loss: 3.159038641119516

Epoch: 5| Step: 6
Training loss: 3.354616641998291
Validation loss: 3.1597739906721216

Epoch: 5| Step: 7
Training loss: 3.601378917694092
Validation loss: 3.155898086486324

Epoch: 5| Step: 8
Training loss: 3.47216796875
Validation loss: 3.1501427183869066

Epoch: 5| Step: 9
Training loss: 2.380586862564087
Validation loss: 3.148356424864902

Epoch: 5| Step: 10
Training loss: 4.149478912353516
Validation loss: 3.147228830604143

Epoch: 22| Step: 0
Training loss: 3.063034772872925
Validation loss: 3.144680210339126

Epoch: 5| Step: 1
Training loss: 3.038632392883301
Validation loss: 3.1409917005928616

Epoch: 5| Step: 2
Training loss: 2.7628889083862305
Validation loss: 3.142197339765487

Epoch: 5| Step: 3
Training loss: 3.4511680603027344
Validation loss: 3.1356778144836426

Epoch: 5| Step: 4
Training loss: 3.6105988025665283
Validation loss: 3.1340217718514065

Epoch: 5| Step: 5
Training loss: 2.7333905696868896
Validation loss: 3.1318167819771716

Epoch: 5| Step: 6
Training loss: 2.802959680557251
Validation loss: 3.1315227323962795

Epoch: 5| Step: 7
Training loss: 3.431119203567505
Validation loss: 3.128392865580897

Epoch: 5| Step: 8
Training loss: 2.789517879486084
Validation loss: 3.1277101116795696

Epoch: 5| Step: 9
Training loss: 3.515227794647217
Validation loss: 3.126511848101052

Epoch: 5| Step: 10
Training loss: 3.4752750396728516
Validation loss: 3.126559116507089

Epoch: 23| Step: 0
Training loss: 2.9227073192596436
Validation loss: 3.1251393261776177

Epoch: 5| Step: 1
Training loss: 2.900799512863159
Validation loss: 3.119976858938894

Epoch: 5| Step: 2
Training loss: 2.910597562789917
Validation loss: 3.1159266553899294

Epoch: 5| Step: 3
Training loss: 2.9066555500030518
Validation loss: 3.110840787169754

Epoch: 5| Step: 4
Training loss: 3.6430556774139404
Validation loss: 3.1119399070739746

Epoch: 5| Step: 5
Training loss: 2.5850937366485596
Validation loss: 3.108218377636325

Epoch: 5| Step: 6
Training loss: 3.2032151222229004
Validation loss: 3.109377771295527

Epoch: 5| Step: 7
Training loss: 3.668431520462036
Validation loss: 3.1062056274824243

Epoch: 5| Step: 8
Training loss: 3.3163139820098877
Validation loss: 3.1045656178587224

Epoch: 5| Step: 9
Training loss: 3.4609665870666504
Validation loss: 3.1017036079078593

Epoch: 5| Step: 10
Training loss: 2.9024498462677
Validation loss: 3.098878388763756

Epoch: 24| Step: 0
Training loss: 3.571214199066162
Validation loss: 3.097814239481444

Epoch: 5| Step: 1
Training loss: 2.748584508895874
Validation loss: 3.0960754143294467

Epoch: 5| Step: 2
Training loss: 3.091984987258911
Validation loss: 3.0947821447926183

Epoch: 5| Step: 3
Training loss: 4.43782901763916
Validation loss: 3.0930510182534494

Epoch: 5| Step: 4
Training loss: 2.961911678314209
Validation loss: 3.091740903034005

Epoch: 5| Step: 5
Training loss: 2.171933650970459
Validation loss: 3.0899862909829743

Epoch: 5| Step: 6
Training loss: 2.7215142250061035
Validation loss: 3.087144228719896

Epoch: 5| Step: 7
Training loss: 3.007080078125
Validation loss: 3.08508554838037

Epoch: 5| Step: 8
Training loss: 3.7176883220672607
Validation loss: 3.0828359152681086

Epoch: 5| Step: 9
Training loss: 3.586718797683716
Validation loss: 3.081695969386767

Epoch: 5| Step: 10
Training loss: 2.128685712814331
Validation loss: 3.079149333379602

Epoch: 25| Step: 0
Training loss: 2.1406238079071045
Validation loss: 3.0733346426358787

Epoch: 5| Step: 1
Training loss: 3.26054048538208
Validation loss: 3.0718898645011325

Epoch: 5| Step: 2
Training loss: 2.8857452869415283
Validation loss: 3.071408946027038

Epoch: 5| Step: 3
Training loss: 3.071122884750366
Validation loss: 3.0733206964308217

Epoch: 5| Step: 4
Training loss: 3.475598096847534
Validation loss: 3.0676115969175934

Epoch: 5| Step: 5
Training loss: 3.886500120162964
Validation loss: 3.066556917723789

Epoch: 5| Step: 6
Training loss: 2.4987921714782715
Validation loss: 3.0632950234156784

Epoch: 5| Step: 7
Training loss: 2.0263330936431885
Validation loss: 3.063264246909849

Epoch: 5| Step: 8
Training loss: 3.4608497619628906
Validation loss: 3.061630766878846

Epoch: 5| Step: 9
Training loss: 4.169412136077881
Validation loss: 3.058428531051964

Epoch: 5| Step: 10
Training loss: 3.284935474395752
Validation loss: 3.0561938926737797

Epoch: 26| Step: 0
Training loss: 3.3012948036193848
Validation loss: 3.056317696007349

Epoch: 5| Step: 1
Training loss: 2.4231958389282227
Validation loss: 3.052233526783605

Epoch: 5| Step: 2
Training loss: 2.863225221633911
Validation loss: 3.0520367007101736

Epoch: 5| Step: 3
Training loss: 2.363532066345215
Validation loss: 3.0524232028633036

Epoch: 5| Step: 4
Training loss: 3.643765926361084
Validation loss: 3.054371731255644

Epoch: 5| Step: 5
Training loss: 3.4863007068634033
Validation loss: 3.0474995695134646

Epoch: 5| Step: 6
Training loss: 3.0775904655456543
Validation loss: 3.0527709530245875

Epoch: 5| Step: 7
Training loss: 3.366342544555664
Validation loss: 3.052398871350032

Epoch: 5| Step: 8
Training loss: 2.99463152885437
Validation loss: 3.0454145605846117

Epoch: 5| Step: 9
Training loss: 2.9845099449157715
Validation loss: 3.0438005847315632

Epoch: 5| Step: 10
Training loss: 3.5458157062530518
Validation loss: 3.042780758232199

Epoch: 27| Step: 0
Training loss: 2.194065809249878
Validation loss: 3.041126112784109

Epoch: 5| Step: 1
Training loss: 2.6926960945129395
Validation loss: 3.0400301974306823

Epoch: 5| Step: 2
Training loss: 3.3442368507385254
Validation loss: 3.0318366430139028

Epoch: 5| Step: 3
Training loss: 3.4806721210479736
Validation loss: 3.031376128555626

Epoch: 5| Step: 4
Training loss: 3.001478672027588
Validation loss: 3.0316190924695743

Epoch: 5| Step: 5
Training loss: 3.7823081016540527
Validation loss: 3.028330672171808

Epoch: 5| Step: 6
Training loss: 3.0594024658203125
Validation loss: 3.025324721490183

Epoch: 5| Step: 7
Training loss: 2.9238345623016357
Validation loss: 3.0220284974703224

Epoch: 5| Step: 8
Training loss: 2.7753853797912598
Validation loss: 3.0157263330233994

Epoch: 5| Step: 9
Training loss: 2.9862277507781982
Validation loss: 3.0172758999691216

Epoch: 5| Step: 10
Training loss: 3.7239201068878174
Validation loss: 3.0195232155502483

Epoch: 28| Step: 0
Training loss: 3.4123034477233887
Validation loss: 3.014361704549482

Epoch: 5| Step: 1
Training loss: 2.348501682281494
Validation loss: 3.0107463482887513

Epoch: 5| Step: 2
Training loss: 1.9983224868774414
Validation loss: 3.0081637059488604

Epoch: 5| Step: 3
Training loss: 3.608912229537964
Validation loss: 3.0038574818641908

Epoch: 5| Step: 4
Training loss: 3.2983779907226562
Validation loss: 3.00337202061889

Epoch: 5| Step: 5
Training loss: 2.6044180393218994
Validation loss: 3.003846735082647

Epoch: 5| Step: 6
Training loss: 3.1808884143829346
Validation loss: 3.005145572846936

Epoch: 5| Step: 7
Training loss: 3.425936222076416
Validation loss: 3.0061794224605767

Epoch: 5| Step: 8
Training loss: 3.195411205291748
Validation loss: 3.00038476913206

Epoch: 5| Step: 9
Training loss: 3.463819980621338
Validation loss: 2.9994735076863277

Epoch: 5| Step: 10
Training loss: 3.126826047897339
Validation loss: 2.997689939314319

Epoch: 29| Step: 0
Training loss: 3.188751697540283
Validation loss: 2.9934829153040403

Epoch: 5| Step: 1
Training loss: 2.6816108226776123
Validation loss: 2.9906276964372203

Epoch: 5| Step: 2
Training loss: 3.1235404014587402
Validation loss: 2.9913949556248163

Epoch: 5| Step: 3
Training loss: 3.1781349182128906
Validation loss: 2.991581368189986

Epoch: 5| Step: 4
Training loss: 2.612799882888794
Validation loss: 2.993908525795065

Epoch: 5| Step: 5
Training loss: 2.724142551422119
Validation loss: 2.993044868592293

Epoch: 5| Step: 6
Training loss: 4.126430511474609
Validation loss: 2.9975745857402845

Epoch: 5| Step: 7
Training loss: 2.7324061393737793
Validation loss: 2.9848920555524927

Epoch: 5| Step: 8
Training loss: 2.4265260696411133
Validation loss: 2.984530823205107

Epoch: 5| Step: 9
Training loss: 3.8915183544158936
Validation loss: 2.985759614616312

Epoch: 5| Step: 10
Training loss: 2.8312151432037354
Validation loss: 2.979048195705619

Epoch: 30| Step: 0
Training loss: 2.648855686187744
Validation loss: 2.98370720237814

Epoch: 5| Step: 1
Training loss: 3.7199599742889404
Validation loss: 2.990757111580141

Epoch: 5| Step: 2
Training loss: 3.2752742767333984
Validation loss: 2.9796234843551472

Epoch: 5| Step: 3
Training loss: 3.2812793254852295
Validation loss: 2.9728097069648003

Epoch: 5| Step: 4
Training loss: 3.1526598930358887
Validation loss: 2.9726823965708413

Epoch: 5| Step: 5
Training loss: 2.252761125564575
Validation loss: 2.9716219671310915

Epoch: 5| Step: 6
Training loss: 2.7097411155700684
Validation loss: 2.970031187098513

Epoch: 5| Step: 7
Training loss: 2.278855323791504
Validation loss: 2.9745345833481

Epoch: 5| Step: 8
Training loss: 3.3984501361846924
Validation loss: 2.968924142981088

Epoch: 5| Step: 9
Training loss: 2.8958332538604736
Validation loss: 2.967089852979106

Epoch: 5| Step: 10
Training loss: 3.9461171627044678
Validation loss: 2.963237888069563

Epoch: 31| Step: 0
Training loss: 2.8795719146728516
Validation loss: 2.9593619761928434

Epoch: 5| Step: 1
Training loss: 3.383021593093872
Validation loss: 2.9574022626364105

Epoch: 5| Step: 2
Training loss: 2.9300687313079834
Validation loss: 2.9599653777255805

Epoch: 5| Step: 3
Training loss: 2.865307092666626
Validation loss: 2.9527348805499334

Epoch: 5| Step: 4
Training loss: 3.149573564529419
Validation loss: 2.951532220327726

Epoch: 5| Step: 5
Training loss: 2.988727331161499
Validation loss: 2.952747952553534

Epoch: 5| Step: 6
Training loss: 3.2640914916992188
Validation loss: 2.9537826430413032

Epoch: 5| Step: 7
Training loss: 3.475691318511963
Validation loss: 2.9552678036433395

Epoch: 5| Step: 8
Training loss: 2.349836826324463
Validation loss: 2.9528781137158795

Epoch: 5| Step: 9
Training loss: 3.1496100425720215
Validation loss: 2.9479757919106433

Epoch: 5| Step: 10
Training loss: 2.8062915802001953
Validation loss: 2.950087631902387

Epoch: 32| Step: 0
Training loss: 2.57786226272583
Validation loss: 2.947256318984493

Epoch: 5| Step: 1
Training loss: 3.465359926223755
Validation loss: 2.944030859137094

Epoch: 5| Step: 2
Training loss: 3.136112689971924
Validation loss: 2.9386946103906118

Epoch: 5| Step: 3
Training loss: 2.815237283706665
Validation loss: 2.939390782387026

Epoch: 5| Step: 4
Training loss: 3.3292670249938965
Validation loss: 2.9346809284661406

Epoch: 5| Step: 5
Training loss: 2.1337015628814697
Validation loss: 2.9329811783247095

Epoch: 5| Step: 6
Training loss: 3.1382336616516113
Validation loss: 2.932244054732784

Epoch: 5| Step: 7
Training loss: 3.1798157691955566
Validation loss: 2.9269423279710995

Epoch: 5| Step: 8
Training loss: 3.0484752655029297
Validation loss: 2.9310481984128236

Epoch: 5| Step: 9
Training loss: 2.861659288406372
Validation loss: 2.930779254564675

Epoch: 5| Step: 10
Training loss: 3.5389654636383057
Validation loss: 2.9392850168289675

Epoch: 33| Step: 0
Training loss: 2.90956449508667
Validation loss: 2.930286289543234

Epoch: 5| Step: 1
Training loss: 2.9967293739318848
Validation loss: 2.929014031605054

Epoch: 5| Step: 2
Training loss: 3.5051143169403076
Validation loss: 2.926885502312773

Epoch: 5| Step: 3
Training loss: 2.9004766941070557
Validation loss: 2.9237758574947232

Epoch: 5| Step: 4
Training loss: 2.12864351272583
Validation loss: 2.924317805997787

Epoch: 5| Step: 5
Training loss: 2.6384475231170654
Validation loss: 2.922547981303225

Epoch: 5| Step: 6
Training loss: 2.7953879833221436
Validation loss: 2.923902194987061

Epoch: 5| Step: 7
Training loss: 3.3343842029571533
Validation loss: 2.918834432478874

Epoch: 5| Step: 8
Training loss: 3.3999252319335938
Validation loss: 2.9187779093301423

Epoch: 5| Step: 9
Training loss: 3.391037702560425
Validation loss: 2.927697963612054

Epoch: 5| Step: 10
Training loss: 3.0457959175109863
Validation loss: 2.923971309456774

Epoch: 34| Step: 0
Training loss: 3.3839073181152344
Validation loss: 2.9186449435449417

Epoch: 5| Step: 1
Training loss: 2.653775691986084
Validation loss: 2.9124637931905766

Epoch: 5| Step: 2
Training loss: 2.2562224864959717
Validation loss: 2.9107150570038827

Epoch: 5| Step: 3
Training loss: 3.362342357635498
Validation loss: 2.9088084877178235

Epoch: 5| Step: 4
Training loss: 2.5968172550201416
Validation loss: 2.9040142669472644

Epoch: 5| Step: 5
Training loss: 3.4123053550720215
Validation loss: 2.903689876679451

Epoch: 5| Step: 6
Training loss: 3.6386096477508545
Validation loss: 2.9015539256475305

Epoch: 5| Step: 7
Training loss: 2.8314690589904785
Validation loss: 2.904381819950637

Epoch: 5| Step: 8
Training loss: 2.6732163429260254
Validation loss: 2.9006000231671076

Epoch: 5| Step: 9
Training loss: 3.3587117195129395
Validation loss: 2.8999736155233076

Epoch: 5| Step: 10
Training loss: 2.679227113723755
Validation loss: 2.900030897509667

Epoch: 35| Step: 0
Training loss: 2.580371379852295
Validation loss: 2.9111857055335917

Epoch: 5| Step: 1
Training loss: 2.996428966522217
Validation loss: 2.9149872308136313

Epoch: 5| Step: 2
Training loss: 2.007503032684326
Validation loss: 2.9179277214952695

Epoch: 5| Step: 3
Training loss: 2.9592292308807373
Validation loss: 2.9019461242101525

Epoch: 5| Step: 4
Training loss: 3.0841128826141357
Validation loss: 2.897436482931978

Epoch: 5| Step: 5
Training loss: 3.3524036407470703
Validation loss: 2.8992764719070925

Epoch: 5| Step: 6
Training loss: 3.0246541500091553
Validation loss: 2.89636081264865

Epoch: 5| Step: 7
Training loss: 3.869729995727539
Validation loss: 2.8946036831025155

Epoch: 5| Step: 8
Training loss: 3.4822585582733154
Validation loss: 2.8937848280834895

Epoch: 5| Step: 9
Training loss: 2.719231128692627
Validation loss: 2.8878932819571546

Epoch: 5| Step: 10
Training loss: 2.7062206268310547
Validation loss: 2.8837108201878046

Epoch: 36| Step: 0
Training loss: 3.1556949615478516
Validation loss: 2.881193801920901

Epoch: 5| Step: 1
Training loss: 1.8189102411270142
Validation loss: 2.88040909459514

Epoch: 5| Step: 2
Training loss: 2.452669858932495
Validation loss: 2.87967711622997

Epoch: 5| Step: 3
Training loss: 2.8475208282470703
Validation loss: 2.879848575079313

Epoch: 5| Step: 4
Training loss: 3.1350629329681396
Validation loss: 2.878023344983337

Epoch: 5| Step: 5
Training loss: 3.3039984703063965
Validation loss: 2.872893479562575

Epoch: 5| Step: 6
Training loss: 3.090333938598633
Validation loss: 2.8713190863209386

Epoch: 5| Step: 7
Training loss: 2.817373752593994
Validation loss: 2.8759872887724187

Epoch: 5| Step: 8
Training loss: 2.565680980682373
Validation loss: 2.871384556575488

Epoch: 5| Step: 9
Training loss: 3.707460880279541
Validation loss: 2.8677001358360372

Epoch: 5| Step: 10
Training loss: 3.9182286262512207
Validation loss: 2.8673312484577136

Epoch: 37| Step: 0
Training loss: 2.342316150665283
Validation loss: 2.8676861152854016

Epoch: 5| Step: 1
Training loss: 2.7453958988189697
Validation loss: 2.867824008387904

Epoch: 5| Step: 2
Training loss: 2.4150795936584473
Validation loss: 2.871003899523007

Epoch: 5| Step: 3
Training loss: 3.609755277633667
Validation loss: 2.8768389045551257

Epoch: 5| Step: 4
Training loss: 2.3505985736846924
Validation loss: 2.878308665367865

Epoch: 5| Step: 5
Training loss: 2.817955255508423
Validation loss: 2.8779064122066704

Epoch: 5| Step: 6
Training loss: 3.0853240489959717
Validation loss: 2.8724510490253405

Epoch: 5| Step: 7
Training loss: 2.921212911605835
Validation loss: 2.8609596196041314

Epoch: 5| Step: 8
Training loss: 2.625062942504883
Validation loss: 2.8628734029749388

Epoch: 5| Step: 9
Training loss: 3.3904285430908203
Validation loss: 2.8586750620154926

Epoch: 5| Step: 10
Training loss: 4.518436431884766
Validation loss: 2.8592695728425057

Epoch: 38| Step: 0
Training loss: 2.4469313621520996
Validation loss: 2.8561630120841404

Epoch: 5| Step: 1
Training loss: 2.791163682937622
Validation loss: 2.8587871213113107

Epoch: 5| Step: 2
Training loss: 3.145545482635498
Validation loss: 2.860989985927459

Epoch: 5| Step: 3
Training loss: 2.6371865272521973
Validation loss: 2.8527668419704644

Epoch: 5| Step: 4
Training loss: 2.6811091899871826
Validation loss: 2.84570175345226

Epoch: 5| Step: 5
Training loss: 3.2431671619415283
Validation loss: 2.8430118304426952

Epoch: 5| Step: 6
Training loss: 2.8836803436279297
Validation loss: 2.83988937511239

Epoch: 5| Step: 7
Training loss: 2.7261221408843994
Validation loss: 2.8390667976871615

Epoch: 5| Step: 8
Training loss: 3.930799961090088
Validation loss: 2.8369113578591296

Epoch: 5| Step: 9
Training loss: 3.2103095054626465
Validation loss: 2.838728532996229

Epoch: 5| Step: 10
Training loss: 2.632535934448242
Validation loss: 2.839009646446474

Epoch: 39| Step: 0
Training loss: 2.568021535873413
Validation loss: 2.840213467997889

Epoch: 5| Step: 1
Training loss: 4.370318412780762
Validation loss: 2.842561921765727

Epoch: 5| Step: 2
Training loss: 2.37031888961792
Validation loss: 2.8325664330554265

Epoch: 5| Step: 3
Training loss: 2.6173837184906006
Validation loss: 2.833888441003779

Epoch: 5| Step: 4
Training loss: 2.2994110584259033
Validation loss: 2.844352719604328

Epoch: 5| Step: 5
Training loss: 3.304534435272217
Validation loss: 2.837809290937198

Epoch: 5| Step: 6
Training loss: 2.8198249340057373
Validation loss: 2.82678807679043

Epoch: 5| Step: 7
Training loss: 2.7962093353271484
Validation loss: 2.8276590813872633

Epoch: 5| Step: 8
Training loss: 3.013150691986084
Validation loss: 2.82805626110364

Epoch: 5| Step: 9
Training loss: 3.713334321975708
Validation loss: 2.8295765974188365

Epoch: 5| Step: 10
Training loss: 2.320342540740967
Validation loss: 2.8330472054020053

Epoch: 40| Step: 0
Training loss: 3.3797218799591064
Validation loss: 2.83399688043902

Epoch: 5| Step: 1
Training loss: 3.1886465549468994
Validation loss: 2.836260898138887

Epoch: 5| Step: 2
Training loss: 3.2132620811462402
Validation loss: 2.838059563790598

Epoch: 5| Step: 3
Training loss: 2.2770285606384277
Validation loss: 2.834072987238566

Epoch: 5| Step: 4
Training loss: 3.307727336883545
Validation loss: 2.83069170418606

Epoch: 5| Step: 5
Training loss: 3.3012607097625732
Validation loss: 2.8340104574798257

Epoch: 5| Step: 6
Training loss: 2.658547878265381
Validation loss: 2.8322822996365127

Epoch: 5| Step: 7
Training loss: 2.7824273109436035
Validation loss: 2.82887831810982

Epoch: 5| Step: 8
Training loss: 2.4082043170928955
Validation loss: 2.816731658033145

Epoch: 5| Step: 9
Training loss: 2.907860040664673
Validation loss: 2.8172620265714583

Epoch: 5| Step: 10
Training loss: 2.8409574031829834
Validation loss: 2.8120420004731868

Epoch: 41| Step: 0
Training loss: 2.867133617401123
Validation loss: 2.8152543344805316

Epoch: 5| Step: 1
Training loss: 3.010523796081543
Validation loss: 2.810235264480755

Epoch: 5| Step: 2
Training loss: 2.982492208480835
Validation loss: 2.811731784574447

Epoch: 5| Step: 3
Training loss: 2.877426862716675
Validation loss: 2.811382560319798

Epoch: 5| Step: 4
Training loss: 2.967897415161133
Validation loss: 2.8105407325170373

Epoch: 5| Step: 5
Training loss: 3.4367473125457764
Validation loss: 2.8146698167247157

Epoch: 5| Step: 6
Training loss: 2.872162342071533
Validation loss: 2.813897640474381

Epoch: 5| Step: 7
Training loss: 2.17396879196167
Validation loss: 2.8090169865597963

Epoch: 5| Step: 8
Training loss: 2.682622194290161
Validation loss: 2.81087298803432

Epoch: 5| Step: 9
Training loss: 3.1065995693206787
Validation loss: 2.812032538075601

Epoch: 5| Step: 10
Training loss: 3.2275145053863525
Validation loss: 2.8044235552510908

Epoch: 42| Step: 0
Training loss: 2.819545030593872
Validation loss: 2.8066064952522196

Epoch: 5| Step: 1
Training loss: 4.14315128326416
Validation loss: 2.8034024751314552

Epoch: 5| Step: 2
Training loss: 2.052508592605591
Validation loss: 2.8055319016979587

Epoch: 5| Step: 3
Training loss: 2.7244906425476074
Validation loss: 2.818172175397155

Epoch: 5| Step: 4
Training loss: 2.537909984588623
Validation loss: 2.807789735896613

Epoch: 5| Step: 5
Training loss: 3.1968131065368652
Validation loss: 2.801899602336268

Epoch: 5| Step: 6
Training loss: 3.314704418182373
Validation loss: 2.7937144822971796

Epoch: 5| Step: 7
Training loss: 3.302520751953125
Validation loss: 2.7965711419300368

Epoch: 5| Step: 8
Training loss: 2.1299073696136475
Validation loss: 2.7975661677698933

Epoch: 5| Step: 9
Training loss: 3.0508713722229004
Validation loss: 2.7982100825155936

Epoch: 5| Step: 10
Training loss: 2.781618118286133
Validation loss: 2.7986019836959017

Epoch: 43| Step: 0
Training loss: 2.5099124908447266
Validation loss: 2.799632067321449

Epoch: 5| Step: 1
Training loss: 2.9871039390563965
Validation loss: 2.7994636822772283

Epoch: 5| Step: 2
Training loss: 2.1910202503204346
Validation loss: 2.8056803339271137

Epoch: 5| Step: 3
Training loss: 2.7652931213378906
Validation loss: 2.8153358454345376

Epoch: 5| Step: 4
Training loss: 2.6604437828063965
Validation loss: 2.8174764853651806

Epoch: 5| Step: 5
Training loss: 2.316375732421875
Validation loss: 2.8165154841638382

Epoch: 5| Step: 6
Training loss: 3.8380331993103027
Validation loss: 2.815176979187996

Epoch: 5| Step: 7
Training loss: 4.234866619110107
Validation loss: 2.8112735697018203

Epoch: 5| Step: 8
Training loss: 3.0243494510650635
Validation loss: 2.804096355233141

Epoch: 5| Step: 9
Training loss: 2.998253107070923
Validation loss: 2.7985046114972842

Epoch: 5| Step: 10
Training loss: 2.474498987197876
Validation loss: 2.7950061290494856

Epoch: 44| Step: 0
Training loss: 2.8650054931640625
Validation loss: 2.7953751702462473

Epoch: 5| Step: 1
Training loss: 2.665597677230835
Validation loss: 2.7941083190261677

Epoch: 5| Step: 2
Training loss: 3.9009385108947754
Validation loss: 2.7891648584796536

Epoch: 5| Step: 3
Training loss: 3.0919177532196045
Validation loss: 2.786688063734321

Epoch: 5| Step: 4
Training loss: 2.6431665420532227
Validation loss: 2.79003619891341

Epoch: 5| Step: 5
Training loss: 3.1443357467651367
Validation loss: 2.7917398278431227

Epoch: 5| Step: 6
Training loss: 2.2947041988372803
Validation loss: 2.7861555699379212

Epoch: 5| Step: 7
Training loss: 3.231886625289917
Validation loss: 2.788909040471559

Epoch: 5| Step: 8
Training loss: 2.718837261199951
Validation loss: 2.7888700859521025

Epoch: 5| Step: 9
Training loss: 2.516759157180786
Validation loss: 2.7905733995540167

Epoch: 5| Step: 10
Training loss: 2.8590402603149414
Validation loss: 2.7951472907937984

Epoch: 45| Step: 0
Training loss: 3.346930742263794
Validation loss: 2.7911461809630036

Epoch: 5| Step: 1
Training loss: 2.765230655670166
Validation loss: 2.7853707498119724

Epoch: 5| Step: 2
Training loss: 3.361539125442505
Validation loss: 2.779086305249122

Epoch: 5| Step: 3
Training loss: 2.5718560218811035
Validation loss: 2.7766423840676584

Epoch: 5| Step: 4
Training loss: 3.62147855758667
Validation loss: 2.7761782625670075

Epoch: 5| Step: 5
Training loss: 2.8451080322265625
Validation loss: 2.7734756367180937

Epoch: 5| Step: 6
Training loss: 2.6470234394073486
Validation loss: 2.7784474742028022

Epoch: 5| Step: 7
Training loss: 3.155026912689209
Validation loss: 2.780978479693013

Epoch: 5| Step: 8
Training loss: 2.5806565284729004
Validation loss: 2.776572101859636

Epoch: 5| Step: 9
Training loss: 2.863740921020508
Validation loss: 2.777294353772235

Epoch: 5| Step: 10
Training loss: 1.9605822563171387
Validation loss: 2.7792969416546565

Epoch: 46| Step: 0
Training loss: 2.59018611907959
Validation loss: 2.7731056572288595

Epoch: 5| Step: 1
Training loss: 2.7858266830444336
Validation loss: 2.769139999984413

Epoch: 5| Step: 2
Training loss: 2.5741870403289795
Validation loss: 2.765549869947536

Epoch: 5| Step: 3
Training loss: 2.9163622856140137
Validation loss: 2.7631150573812504

Epoch: 5| Step: 4
Training loss: 3.253976345062256
Validation loss: 2.7657279686261247

Epoch: 5| Step: 5
Training loss: 3.2794463634490967
Validation loss: 2.7599080557464273

Epoch: 5| Step: 6
Training loss: 2.78558349609375
Validation loss: 2.7578207856865338

Epoch: 5| Step: 7
Training loss: 3.0465948581695557
Validation loss: 2.7526446542432232

Epoch: 5| Step: 8
Training loss: 2.5281271934509277
Validation loss: 2.758495610247376

Epoch: 5| Step: 9
Training loss: 2.8201327323913574
Validation loss: 2.7579223468739498

Epoch: 5| Step: 10
Training loss: 3.1997687816619873
Validation loss: 2.756776161091302

Epoch: 47| Step: 0
Training loss: 3.7002015113830566
Validation loss: 2.7506903807322183

Epoch: 5| Step: 1
Training loss: 3.1290247440338135
Validation loss: 2.7508238592455463

Epoch: 5| Step: 2
Training loss: 2.379981517791748
Validation loss: 2.7513385101031234

Epoch: 5| Step: 3
Training loss: 2.866615056991577
Validation loss: 2.7514247073922107

Epoch: 5| Step: 4
Training loss: 2.699434995651245
Validation loss: 2.7496795705569688

Epoch: 5| Step: 5
Training loss: 2.894989490509033
Validation loss: 2.7472404023652435

Epoch: 5| Step: 6
Training loss: 2.5251471996307373
Validation loss: 2.7455348917233047

Epoch: 5| Step: 7
Training loss: 3.2999987602233887
Validation loss: 2.7416857186184136

Epoch: 5| Step: 8
Training loss: 2.417747974395752
Validation loss: 2.7455950629326606

Epoch: 5| Step: 9
Training loss: 2.719735622406006
Validation loss: 2.749953328922231

Epoch: 5| Step: 10
Training loss: 3.0320701599121094
Validation loss: 2.7516911260543333

Epoch: 48| Step: 0
Training loss: 3.2810020446777344
Validation loss: 2.7532105753498692

Epoch: 5| Step: 1
Training loss: 2.8358492851257324
Validation loss: 2.7458060428660405

Epoch: 5| Step: 2
Training loss: 3.122781276702881
Validation loss: 2.746118253277194

Epoch: 5| Step: 3
Training loss: 2.944551467895508
Validation loss: 2.7454231041733936

Epoch: 5| Step: 4
Training loss: 2.9636757373809814
Validation loss: 2.7370645564089537

Epoch: 5| Step: 5
Training loss: 2.9996256828308105
Validation loss: 2.7391892479312037

Epoch: 5| Step: 6
Training loss: 2.1941208839416504
Validation loss: 2.736062924067179

Epoch: 5| Step: 7
Training loss: 2.4454991817474365
Validation loss: 2.7365855145198044

Epoch: 5| Step: 8
Training loss: 2.6443984508514404
Validation loss: 2.7369362615769908

Epoch: 5| Step: 9
Training loss: 2.887402296066284
Validation loss: 2.7347310307205364

Epoch: 5| Step: 10
Training loss: 3.3318822383880615
Validation loss: 2.7351290615656043

Epoch: 49| Step: 0
Training loss: 3.3466193675994873
Validation loss: 2.7322808952741724

Epoch: 5| Step: 1
Training loss: 2.209874391555786
Validation loss: 2.7345137903767247

Epoch: 5| Step: 2
Training loss: 2.523674964904785
Validation loss: 2.733747741227509

Epoch: 5| Step: 3
Training loss: 2.751016139984131
Validation loss: 2.7349317355822493

Epoch: 5| Step: 4
Training loss: 3.0114569664001465
Validation loss: 2.7350569899364183

Epoch: 5| Step: 5
Training loss: 3.1288795471191406
Validation loss: 2.7333116377553632

Epoch: 5| Step: 6
Training loss: 3.170300006866455
Validation loss: 2.733675838798605

Epoch: 5| Step: 7
Training loss: 2.1266181468963623
Validation loss: 2.7338383428512083

Epoch: 5| Step: 8
Training loss: 2.981812000274658
Validation loss: 2.7343952014882076

Epoch: 5| Step: 9
Training loss: 3.0793824195861816
Validation loss: 2.7308519168566634

Epoch: 5| Step: 10
Training loss: 3.302205801010132
Validation loss: 2.7293946768647883

Epoch: 50| Step: 0
Training loss: 2.1893811225891113
Validation loss: 2.729115829672865

Epoch: 5| Step: 1
Training loss: 2.0740275382995605
Validation loss: 2.7277811035033195

Epoch: 5| Step: 2
Training loss: 3.637622833251953
Validation loss: 2.72561123037851

Epoch: 5| Step: 3
Training loss: 3.4830918312072754
Validation loss: 2.7254665205555577

Epoch: 5| Step: 4
Training loss: 2.757791757583618
Validation loss: 2.726347674605667

Epoch: 5| Step: 5
Training loss: 2.8021416664123535
Validation loss: 2.723937062806981

Epoch: 5| Step: 6
Training loss: 2.085094928741455
Validation loss: 2.7263815556803057

Epoch: 5| Step: 7
Training loss: 2.8747119903564453
Validation loss: 2.725033531906784

Epoch: 5| Step: 8
Training loss: 3.0039703845977783
Validation loss: 2.720312938895277

Epoch: 5| Step: 9
Training loss: 3.5124480724334717
Validation loss: 2.7181090642047185

Epoch: 5| Step: 10
Training loss: 3.067117929458618
Validation loss: 2.720076319991901

Epoch: 51| Step: 0
Training loss: 3.6289620399475098
Validation loss: 2.719403302797707

Epoch: 5| Step: 1
Training loss: 2.688551664352417
Validation loss: 2.720358935735559

Epoch: 5| Step: 2
Training loss: 2.690467357635498
Validation loss: 2.7223375330689135

Epoch: 5| Step: 3
Training loss: 3.0167202949523926
Validation loss: 2.718874457061932

Epoch: 5| Step: 4
Training loss: 2.4566478729248047
Validation loss: 2.71788695550734

Epoch: 5| Step: 5
Training loss: 2.1987922191619873
Validation loss: 2.7205626298022527

Epoch: 5| Step: 6
Training loss: 2.837970018386841
Validation loss: 2.718220292880971

Epoch: 5| Step: 7
Training loss: 3.4180984497070312
Validation loss: 2.7172513777209866

Epoch: 5| Step: 8
Training loss: 2.704657793045044
Validation loss: 2.7211024889381985

Epoch: 5| Step: 9
Training loss: 2.974607467651367
Validation loss: 2.717129691954582

Epoch: 5| Step: 10
Training loss: 2.805208444595337
Validation loss: 2.7208630141391548

Epoch: 52| Step: 0
Training loss: 2.5183897018432617
Validation loss: 2.7134726226970716

Epoch: 5| Step: 1
Training loss: 3.4848599433898926
Validation loss: 2.7147372422679776

Epoch: 5| Step: 2
Training loss: 3.082240343093872
Validation loss: 2.709802371199413

Epoch: 5| Step: 3
Training loss: 2.6843700408935547
Validation loss: 2.7126402342191307

Epoch: 5| Step: 4
Training loss: 2.311812162399292
Validation loss: 2.7109751983355452

Epoch: 5| Step: 5
Training loss: 2.7166900634765625
Validation loss: 2.709338221498715

Epoch: 5| Step: 6
Training loss: 3.057844638824463
Validation loss: 2.7094099726728214

Epoch: 5| Step: 7
Training loss: 2.4470956325531006
Validation loss: 2.7113819199223674

Epoch: 5| Step: 8
Training loss: 2.494901180267334
Validation loss: 2.710809599968695

Epoch: 5| Step: 9
Training loss: 3.255190372467041
Validation loss: 2.709502645718154

Epoch: 5| Step: 10
Training loss: 3.4083194732666016
Validation loss: 2.7096271668711016

Epoch: 53| Step: 0
Training loss: 2.353384494781494
Validation loss: 2.7135107055787118

Epoch: 5| Step: 1
Training loss: 3.234050750732422
Validation loss: 2.7115315596262612

Epoch: 5| Step: 2
Training loss: 2.8662772178649902
Validation loss: 2.705834304132769

Epoch: 5| Step: 3
Training loss: 2.874302387237549
Validation loss: 2.7051523731600855

Epoch: 5| Step: 4
Training loss: 2.420164108276367
Validation loss: 2.703749354167651

Epoch: 5| Step: 5
Training loss: 2.8907554149627686
Validation loss: 2.703597555878342

Epoch: 5| Step: 6
Training loss: 3.5525920391082764
Validation loss: 2.7055452767238823

Epoch: 5| Step: 7
Training loss: 2.923491954803467
Validation loss: 2.705712472238848

Epoch: 5| Step: 8
Training loss: 2.0325608253479004
Validation loss: 2.7058827261770926

Epoch: 5| Step: 9
Training loss: 3.300220489501953
Validation loss: 2.7041501563082457

Epoch: 5| Step: 10
Training loss: 2.883056402206421
Validation loss: 2.705334953082505

Epoch: 54| Step: 0
Training loss: 2.5562901496887207
Validation loss: 2.7035322804604807

Epoch: 5| Step: 1
Training loss: 3.4809322357177734
Validation loss: 2.704810619354248

Epoch: 5| Step: 2
Training loss: 2.6653714179992676
Validation loss: 2.7121233017213884

Epoch: 5| Step: 3
Training loss: 2.8281493186950684
Validation loss: 2.7101515031629995

Epoch: 5| Step: 4
Training loss: 2.633822202682495
Validation loss: 2.7086758254676737

Epoch: 5| Step: 5
Training loss: 3.4096882343292236
Validation loss: 2.7045395220479658

Epoch: 5| Step: 6
Training loss: 2.940248966217041
Validation loss: 2.7027857072891726

Epoch: 5| Step: 7
Training loss: 2.558483600616455
Validation loss: 2.701506527521277

Epoch: 5| Step: 8
Training loss: 2.0348920822143555
Validation loss: 2.6947630528480775

Epoch: 5| Step: 9
Training loss: 3.596942186355591
Validation loss: 2.6991865404190554

Epoch: 5| Step: 10
Training loss: 2.5408196449279785
Validation loss: 2.696021318435669

Epoch: 55| Step: 0
Training loss: 2.6332948207855225
Validation loss: 2.700107092498451

Epoch: 5| Step: 1
Training loss: 2.46574330329895
Validation loss: 2.6990253233140513

Epoch: 5| Step: 2
Training loss: 2.4721686840057373
Validation loss: 2.6963449857568227

Epoch: 5| Step: 3
Training loss: 2.8900489807128906
Validation loss: 2.702978790447276

Epoch: 5| Step: 4
Training loss: 3.0462183952331543
Validation loss: 2.700404046684183

Epoch: 5| Step: 5
Training loss: 3.056093215942383
Validation loss: 2.69400091068719

Epoch: 5| Step: 6
Training loss: 3.0764412879943848
Validation loss: 2.6943701877388904

Epoch: 5| Step: 7
Training loss: 2.4377198219299316
Validation loss: 2.6967551272402526

Epoch: 5| Step: 8
Training loss: 2.671809673309326
Validation loss: 2.6944339582996983

Epoch: 5| Step: 9
Training loss: 3.0698916912078857
Validation loss: 2.693519292339202

Epoch: 5| Step: 10
Training loss: 3.5321972370147705
Validation loss: 2.6921367081262733

Epoch: 56| Step: 0
Training loss: 2.6798510551452637
Validation loss: 2.6927295372050297

Epoch: 5| Step: 1
Training loss: 2.948488473892212
Validation loss: 2.6965934332980903

Epoch: 5| Step: 2
Training loss: 2.713228940963745
Validation loss: 2.697604671601326

Epoch: 5| Step: 3
Training loss: 3.4087576866149902
Validation loss: 2.699985040131436

Epoch: 5| Step: 4
Training loss: 2.8226895332336426
Validation loss: 2.6955170913409163

Epoch: 5| Step: 5
Training loss: 3.9143950939178467
Validation loss: 2.6928408325359388

Epoch: 5| Step: 6
Training loss: 2.582310914993286
Validation loss: 2.689093223182104

Epoch: 5| Step: 7
Training loss: 2.531498670578003
Validation loss: 2.690065073710616

Epoch: 5| Step: 8
Training loss: 2.5636203289031982
Validation loss: 2.6888772672222507

Epoch: 5| Step: 9
Training loss: 2.7241148948669434
Validation loss: 2.693476994832357

Epoch: 5| Step: 10
Training loss: 2.262601613998413
Validation loss: 2.6872180251665014

Epoch: 57| Step: 0
Training loss: 2.5941736698150635
Validation loss: 2.6882340472231627

Epoch: 5| Step: 1
Training loss: 3.1087405681610107
Validation loss: 2.685871513940955

Epoch: 5| Step: 2
Training loss: 2.1335620880126953
Validation loss: 2.688580861655615

Epoch: 5| Step: 3
Training loss: 2.4283251762390137
Validation loss: 2.6888459984974196

Epoch: 5| Step: 4
Training loss: 2.492446184158325
Validation loss: 2.6856144833308395

Epoch: 5| Step: 5
Training loss: 2.401381015777588
Validation loss: 2.6864043128105903

Epoch: 5| Step: 6
Training loss: 3.6664798259735107
Validation loss: 2.6893017522750364

Epoch: 5| Step: 7
Training loss: 3.3340022563934326
Validation loss: 2.688193844210717

Epoch: 5| Step: 8
Training loss: 2.7911181449890137
Validation loss: 2.6943753893657396

Epoch: 5| Step: 9
Training loss: 3.011779546737671
Validation loss: 2.687972561005623

Epoch: 5| Step: 10
Training loss: 3.280425548553467
Validation loss: 2.685063951758928

Epoch: 58| Step: 0
Training loss: 2.3252077102661133
Validation loss: 2.682187062437816

Epoch: 5| Step: 1
Training loss: 3.8298308849334717
Validation loss: 2.6843959182821293

Epoch: 5| Step: 2
Training loss: 2.302046775817871
Validation loss: 2.6833611611397035

Epoch: 5| Step: 3
Training loss: 3.1114354133605957
Validation loss: 2.6801043325854885

Epoch: 5| Step: 4
Training loss: 2.6897189617156982
Validation loss: 2.6818573910702943

Epoch: 5| Step: 5
Training loss: 3.125756025314331
Validation loss: 2.685005070060812

Epoch: 5| Step: 6
Training loss: 2.5317447185516357
Validation loss: 2.685955857717863

Epoch: 5| Step: 7
Training loss: 2.983255386352539
Validation loss: 2.684768566521265

Epoch: 5| Step: 8
Training loss: 2.543280601501465
Validation loss: 2.691916101722307

Epoch: 5| Step: 9
Training loss: 2.178661823272705
Validation loss: 2.682709860545333

Epoch: 5| Step: 10
Training loss: 3.5863585472106934
Validation loss: 2.6837329710683515

Epoch: 59| Step: 0
Training loss: 3.331408977508545
Validation loss: 2.6850879038533857

Epoch: 5| Step: 1
Training loss: 3.6582984924316406
Validation loss: 2.6813446193613033

Epoch: 5| Step: 2
Training loss: 2.688267230987549
Validation loss: 2.6826635381226898

Epoch: 5| Step: 3
Training loss: 2.776419162750244
Validation loss: 2.6814819382083033

Epoch: 5| Step: 4
Training loss: 2.4294323921203613
Validation loss: 2.6746043056570072

Epoch: 5| Step: 5
Training loss: 2.117586612701416
Validation loss: 2.674601152379026

Epoch: 5| Step: 6
Training loss: 2.7712039947509766
Validation loss: 2.6751208254086074

Epoch: 5| Step: 7
Training loss: 2.084566354751587
Validation loss: 2.675239045132873

Epoch: 5| Step: 8
Training loss: 3.2082149982452393
Validation loss: 2.6753498302992953

Epoch: 5| Step: 9
Training loss: 3.062221050262451
Validation loss: 2.676041982507193

Epoch: 5| Step: 10
Training loss: 3.0061659812927246
Validation loss: 2.6752088608280307

Epoch: 60| Step: 0
Training loss: 2.911386489868164
Validation loss: 2.676286451278194

Epoch: 5| Step: 1
Training loss: 2.5395398139953613
Validation loss: 2.677137205677648

Epoch: 5| Step: 2
Training loss: 2.729840040206909
Validation loss: 2.677342983984178

Epoch: 5| Step: 3
Training loss: 2.905210018157959
Validation loss: 2.6797098293099353

Epoch: 5| Step: 4
Training loss: 2.847231149673462
Validation loss: 2.6766180992126465

Epoch: 5| Step: 5
Training loss: 3.152052879333496
Validation loss: 2.677327481649255

Epoch: 5| Step: 6
Training loss: 2.6067137718200684
Validation loss: 2.6817021780116583

Epoch: 5| Step: 7
Training loss: 2.7537829875946045
Validation loss: 2.675294937626008

Epoch: 5| Step: 8
Training loss: 2.9957711696624756
Validation loss: 2.6746273322771956

Epoch: 5| Step: 9
Training loss: 3.0230987071990967
Validation loss: 2.674014191473684

Epoch: 5| Step: 10
Training loss: 2.545079231262207
Validation loss: 2.670898788718767

Epoch: 61| Step: 0
Training loss: 2.939823627471924
Validation loss: 2.6732226110273793

Epoch: 5| Step: 1
Training loss: 2.5760321617126465
Validation loss: 2.669354792564146

Epoch: 5| Step: 2
Training loss: 2.3394582271575928
Validation loss: 2.6714060793640795

Epoch: 5| Step: 3
Training loss: 3.289649486541748
Validation loss: 2.677297128144131

Epoch: 5| Step: 4
Training loss: 3.3224990367889404
Validation loss: 2.6850356901845625

Epoch: 5| Step: 5
Training loss: 2.7919623851776123
Validation loss: 2.673330481334399

Epoch: 5| Step: 6
Training loss: 2.5981786251068115
Validation loss: 2.670733795371107

Epoch: 5| Step: 7
Training loss: 2.997187614440918
Validation loss: 2.669937846481159

Epoch: 5| Step: 8
Training loss: 2.9595823287963867
Validation loss: 2.6684623815680064

Epoch: 5| Step: 9
Training loss: 2.6855955123901367
Validation loss: 2.670218106239073

Epoch: 5| Step: 10
Training loss: 2.4784982204437256
Validation loss: 2.6693094853431947

Epoch: 62| Step: 0
Training loss: 2.4515578746795654
Validation loss: 2.66955029836265

Epoch: 5| Step: 1
Training loss: 1.958706259727478
Validation loss: 2.6679694550011748

Epoch: 5| Step: 2
Training loss: 3.510791063308716
Validation loss: 2.6687687750785583

Epoch: 5| Step: 3
Training loss: 2.5122697353363037
Validation loss: 2.6725052172137844

Epoch: 5| Step: 4
Training loss: 3.409879207611084
Validation loss: 2.6727616094773814

Epoch: 5| Step: 5
Training loss: 2.998131275177002
Validation loss: 2.672493452666908

Epoch: 5| Step: 6
Training loss: 2.268007755279541
Validation loss: 2.671105920627553

Epoch: 5| Step: 7
Training loss: 3.110611915588379
Validation loss: 2.671001454835297

Epoch: 5| Step: 8
Training loss: 2.8433029651641846
Validation loss: 2.6693615323753765

Epoch: 5| Step: 9
Training loss: 2.2387213706970215
Validation loss: 2.6658709818317043

Epoch: 5| Step: 10
Training loss: 3.7951831817626953
Validation loss: 2.6663336523117556

Epoch: 63| Step: 0
Training loss: 3.132359504699707
Validation loss: 2.670661562232561

Epoch: 5| Step: 1
Training loss: 2.9099364280700684
Validation loss: 2.668107719831569

Epoch: 5| Step: 2
Training loss: 2.817061185836792
Validation loss: 2.668452832006639

Epoch: 5| Step: 3
Training loss: 3.4290497303009033
Validation loss: 2.6715885695590766

Epoch: 5| Step: 4
Training loss: 2.8727710247039795
Validation loss: 2.6753949529381207

Epoch: 5| Step: 5
Training loss: 2.742750644683838
Validation loss: 2.684112625737344

Epoch: 5| Step: 6
Training loss: 3.3167433738708496
Validation loss: 2.67918719783906

Epoch: 5| Step: 7
Training loss: 2.412226438522339
Validation loss: 2.672436732117848

Epoch: 5| Step: 8
Training loss: 2.681384563446045
Validation loss: 2.6698077545371106

Epoch: 5| Step: 9
Training loss: 2.1492419242858887
Validation loss: 2.6671125119732273

Epoch: 5| Step: 10
Training loss: 2.388014078140259
Validation loss: 2.6670451138609197

Epoch: 64| Step: 0
Training loss: 3.1037392616271973
Validation loss: 2.664726418833579

Epoch: 5| Step: 1
Training loss: 2.6037089824676514
Validation loss: 2.6655107826314945

Epoch: 5| Step: 2
Training loss: 2.6435675621032715
Validation loss: 2.6656616067373626

Epoch: 5| Step: 3
Training loss: 3.1709442138671875
Validation loss: 2.668953290549658

Epoch: 5| Step: 4
Training loss: 2.943894147872925
Validation loss: 2.677575329298614

Epoch: 5| Step: 5
Training loss: 2.3765616416931152
Validation loss: 2.6763419643525155

Epoch: 5| Step: 6
Training loss: 3.4201552867889404
Validation loss: 2.67256248125466

Epoch: 5| Step: 7
Training loss: 3.329329013824463
Validation loss: 2.667500085728143

Epoch: 5| Step: 8
Training loss: 2.42710280418396
Validation loss: 2.6694559076780915

Epoch: 5| Step: 9
Training loss: 2.84997296333313
Validation loss: 2.665017625337006

Epoch: 5| Step: 10
Training loss: 1.8727693557739258
Validation loss: 2.6658405078354703

Epoch: 65| Step: 0
Training loss: 3.7558484077453613
Validation loss: 2.6679025209078224

Epoch: 5| Step: 1
Training loss: 3.1889491081237793
Validation loss: 2.6664217108039447

Epoch: 5| Step: 2
Training loss: 2.2858986854553223
Validation loss: 2.67003882315851

Epoch: 5| Step: 3
Training loss: 2.9110522270202637
Validation loss: 2.6705864501255814

Epoch: 5| Step: 4
Training loss: 2.7796459197998047
Validation loss: 2.674080733330019

Epoch: 5| Step: 5
Training loss: 1.8463809490203857
Validation loss: 2.6660135279419603

Epoch: 5| Step: 6
Training loss: 2.8653855323791504
Validation loss: 2.667040317289291

Epoch: 5| Step: 7
Training loss: 3.0719361305236816
Validation loss: 2.664887974339147

Epoch: 5| Step: 8
Training loss: 2.71067214012146
Validation loss: 2.6653993821913198

Epoch: 5| Step: 9
Training loss: 2.5932631492614746
Validation loss: 2.665940579547677

Epoch: 5| Step: 10
Training loss: 2.8775808811187744
Validation loss: 2.660606863678143

Epoch: 66| Step: 0
Training loss: 3.1031177043914795
Validation loss: 2.6619443842159805

Epoch: 5| Step: 1
Training loss: 3.1168389320373535
Validation loss: 2.657536522034676

Epoch: 5| Step: 2
Training loss: 2.680720090866089
Validation loss: 2.6592367925951557

Epoch: 5| Step: 3
Training loss: 2.459433078765869
Validation loss: 2.6577093396135556

Epoch: 5| Step: 4
Training loss: 3.3635735511779785
Validation loss: 2.6562284244004117

Epoch: 5| Step: 5
Training loss: 3.014266014099121
Validation loss: 2.6567178387795725

Epoch: 5| Step: 6
Training loss: 2.53773832321167
Validation loss: 2.657364747857535

Epoch: 5| Step: 7
Training loss: 2.295215606689453
Validation loss: 2.6553405638664

Epoch: 5| Step: 8
Training loss: 2.5931925773620605
Validation loss: 2.6578083217784925

Epoch: 5| Step: 9
Training loss: 2.970655918121338
Validation loss: 2.655967204801498

Epoch: 5| Step: 10
Training loss: 2.687451124191284
Validation loss: 2.6593583527431695

Epoch: 67| Step: 0
Training loss: 2.74981427192688
Validation loss: 2.655795666479295

Epoch: 5| Step: 1
Training loss: 3.0372822284698486
Validation loss: 2.656704669357628

Epoch: 5| Step: 2
Training loss: 2.5400214195251465
Validation loss: 2.656685672780519

Epoch: 5| Step: 3
Training loss: 2.54307222366333
Validation loss: 2.6585433662578626

Epoch: 5| Step: 4
Training loss: 2.453315019607544
Validation loss: 2.6570236067618094

Epoch: 5| Step: 5
Training loss: 3.0530779361724854
Validation loss: 2.6605283855110087

Epoch: 5| Step: 6
Training loss: 3.40655779838562
Validation loss: 2.6585715765594156

Epoch: 5| Step: 7
Training loss: 3.3839943408966064
Validation loss: 2.6565809942060903

Epoch: 5| Step: 8
Training loss: 2.2355716228485107
Validation loss: 2.652461923578734

Epoch: 5| Step: 9
Training loss: 2.367830276489258
Validation loss: 2.6558528010563185

Epoch: 5| Step: 10
Training loss: 3.039422035217285
Validation loss: 2.653094514723747

Epoch: 68| Step: 0
Training loss: 2.7858681678771973
Validation loss: 2.657005322876797

Epoch: 5| Step: 1
Training loss: 2.890565872192383
Validation loss: 2.6592885089176956

Epoch: 5| Step: 2
Training loss: 2.877945899963379
Validation loss: 2.669247801585864

Epoch: 5| Step: 3
Training loss: 2.950705051422119
Validation loss: 2.6575589692720802

Epoch: 5| Step: 4
Training loss: 3.7790603637695312
Validation loss: 2.6486523100124892

Epoch: 5| Step: 5
Training loss: 2.155033588409424
Validation loss: 2.6528616182265745

Epoch: 5| Step: 6
Training loss: 2.036473035812378
Validation loss: 2.6514615422935894

Epoch: 5| Step: 7
Training loss: 2.338658571243286
Validation loss: 2.65080528361823

Epoch: 5| Step: 8
Training loss: 2.6910817623138428
Validation loss: 2.6487797024429485

Epoch: 5| Step: 9
Training loss: 3.6419320106506348
Validation loss: 2.650955548850439

Epoch: 5| Step: 10
Training loss: 2.5918657779693604
Validation loss: 2.6552093054658625

Epoch: 69| Step: 0
Training loss: 3.3569424152374268
Validation loss: 2.6695095851857173

Epoch: 5| Step: 1
Training loss: 2.5916762351989746
Validation loss: 2.6603030466264292

Epoch: 5| Step: 2
Training loss: 3.0490565299987793
Validation loss: 2.659325735543364

Epoch: 5| Step: 3
Training loss: 2.3733153343200684
Validation loss: 2.6521004989583004

Epoch: 5| Step: 4
Training loss: 2.738051652908325
Validation loss: 2.6516090926303657

Epoch: 5| Step: 5
Training loss: 2.7479827404022217
Validation loss: 2.6489782256464802

Epoch: 5| Step: 6
Training loss: 2.0692660808563232
Validation loss: 2.649822142816359

Epoch: 5| Step: 7
Training loss: 2.0996363162994385
Validation loss: 2.6505058324465187

Epoch: 5| Step: 8
Training loss: 3.2370002269744873
Validation loss: 2.650715588241495

Epoch: 5| Step: 9
Training loss: 3.1013474464416504
Validation loss: 2.6498324999245266

Epoch: 5| Step: 10
Training loss: 3.539168357849121
Validation loss: 2.651671260915777

Epoch: 70| Step: 0
Training loss: 2.7296009063720703
Validation loss: 2.6505603405737106

Epoch: 5| Step: 1
Training loss: 2.3679497241973877
Validation loss: 2.6475289252496537

Epoch: 5| Step: 2
Training loss: 2.385499954223633
Validation loss: 2.6483156783606416

Epoch: 5| Step: 3
Training loss: 1.7622438669204712
Validation loss: 2.648571862969347

Epoch: 5| Step: 4
Training loss: 3.5193817615509033
Validation loss: 2.653383711332916

Epoch: 5| Step: 5
Training loss: 2.866938591003418
Validation loss: 2.654512110576835

Epoch: 5| Step: 6
Training loss: 2.4286036491394043
Validation loss: 2.6489534942052697

Epoch: 5| Step: 7
Training loss: 3.4484946727752686
Validation loss: 2.6471180608195644

Epoch: 5| Step: 8
Training loss: 2.7013802528381348
Validation loss: 2.645829539145193

Epoch: 5| Step: 9
Training loss: 3.4380557537078857
Validation loss: 2.645415525282583

Epoch: 5| Step: 10
Training loss: 3.1482620239257812
Validation loss: 2.642807391382033

Epoch: 71| Step: 0
Training loss: 2.394559383392334
Validation loss: 2.6416715960348807

Epoch: 5| Step: 1
Training loss: 3.1487228870391846
Validation loss: 2.641125766179895

Epoch: 5| Step: 2
Training loss: 3.0260822772979736
Validation loss: 2.6440130869547525

Epoch: 5| Step: 3
Training loss: 2.8382205963134766
Validation loss: 2.6451844374338784

Epoch: 5| Step: 4
Training loss: 2.5186526775360107
Validation loss: 2.642378983959075

Epoch: 5| Step: 5
Training loss: 2.9724578857421875
Validation loss: 2.6396336991299867

Epoch: 5| Step: 6
Training loss: 3.173180103302002
Validation loss: 2.648948851452079

Epoch: 5| Step: 7
Training loss: 2.6288509368896484
Validation loss: 2.648018665211175

Epoch: 5| Step: 8
Training loss: 3.2656216621398926
Validation loss: 2.6466945627684235

Epoch: 5| Step: 9
Training loss: 2.3261849880218506
Validation loss: 2.649579491666568

Epoch: 5| Step: 10
Training loss: 2.323988676071167
Validation loss: 2.649632151408862

Epoch: 72| Step: 0
Training loss: 3.0754897594451904
Validation loss: 2.6626860967246433

Epoch: 5| Step: 1
Training loss: 1.2905175685882568
Validation loss: 2.6446608753614527

Epoch: 5| Step: 2
Training loss: 2.887371301651001
Validation loss: 2.6501156745418424

Epoch: 5| Step: 3
Training loss: 3.1505179405212402
Validation loss: 2.643982369412658

Epoch: 5| Step: 4
Training loss: 3.196326732635498
Validation loss: 2.647519257760817

Epoch: 5| Step: 5
Training loss: 1.975134253501892
Validation loss: 2.6473868380310717

Epoch: 5| Step: 6
Training loss: 2.759719133377075
Validation loss: 2.6452409221280004

Epoch: 5| Step: 7
Training loss: 2.727769136428833
Validation loss: 2.6434966312941683

Epoch: 5| Step: 8
Training loss: 3.460249423980713
Validation loss: 2.646397726510161

Epoch: 5| Step: 9
Training loss: 3.3083252906799316
Validation loss: 2.6481331035655034

Epoch: 5| Step: 10
Training loss: 2.959784746170044
Validation loss: 2.649160082622241

Epoch: 73| Step: 0
Training loss: 3.8715667724609375
Validation loss: 2.6370290633170836

Epoch: 5| Step: 1
Training loss: 3.38165020942688
Validation loss: 2.6323874381280716

Epoch: 5| Step: 2
Training loss: 3.0673069953918457
Validation loss: 2.6324722100329656

Epoch: 5| Step: 3
Training loss: 2.8029260635375977
Validation loss: 2.6320767992286274

Epoch: 5| Step: 4
Training loss: 2.3197152614593506
Validation loss: 2.6334751626496673

Epoch: 5| Step: 5
Training loss: 2.0401108264923096
Validation loss: 2.6355192994558685

Epoch: 5| Step: 6
Training loss: 2.916940212249756
Validation loss: 2.6333604986949632

Epoch: 5| Step: 7
Training loss: 2.6490211486816406
Validation loss: 2.6308014879944506

Epoch: 5| Step: 8
Training loss: 2.518862009048462
Validation loss: 2.6318779350608907

Epoch: 5| Step: 9
Training loss: 2.3105578422546387
Validation loss: 2.6330613397782847

Epoch: 5| Step: 10
Training loss: 2.8019421100616455
Validation loss: 2.633453487068094

Epoch: 74| Step: 0
Training loss: 2.4566187858581543
Validation loss: 2.632927699755597

Epoch: 5| Step: 1
Training loss: 2.590256929397583
Validation loss: 2.632120722083635

Epoch: 5| Step: 2
Training loss: 3.194535493850708
Validation loss: 2.633185996804186

Epoch: 5| Step: 3
Training loss: 2.3510100841522217
Validation loss: 2.635301374620007

Epoch: 5| Step: 4
Training loss: 2.4004158973693848
Validation loss: 2.633092777703398

Epoch: 5| Step: 5
Training loss: 3.0248782634735107
Validation loss: 2.635313405785509

Epoch: 5| Step: 6
Training loss: 3.03096866607666
Validation loss: 2.6312526887462986

Epoch: 5| Step: 7
Training loss: 3.399801254272461
Validation loss: 2.6325238212462394

Epoch: 5| Step: 8
Training loss: 2.5331714153289795
Validation loss: 2.637140622702978

Epoch: 5| Step: 9
Training loss: 3.1644718647003174
Validation loss: 2.633003363045313

Epoch: 5| Step: 10
Training loss: 2.418638229370117
Validation loss: 2.634837258246637

Epoch: 75| Step: 0
Training loss: 2.572575092315674
Validation loss: 2.634719951178438

Epoch: 5| Step: 1
Training loss: 2.103579044342041
Validation loss: 2.6385068252522457

Epoch: 5| Step: 2
Training loss: 2.222992181777954
Validation loss: 2.6385683705729823

Epoch: 5| Step: 3
Training loss: 2.213914394378662
Validation loss: 2.6395566207106396

Epoch: 5| Step: 4
Training loss: 2.8039379119873047
Validation loss: 2.636754548677834

Epoch: 5| Step: 5
Training loss: 2.8987324237823486
Validation loss: 2.6366179732866186

Epoch: 5| Step: 6
Training loss: 2.9450104236602783
Validation loss: 2.6349316412402737

Epoch: 5| Step: 7
Training loss: 3.558126926422119
Validation loss: 2.633625190745118

Epoch: 5| Step: 8
Training loss: 2.942858934402466
Validation loss: 2.629409308074623

Epoch: 5| Step: 9
Training loss: 2.9611270427703857
Validation loss: 2.6316764457251436

Epoch: 5| Step: 10
Training loss: 3.497710943222046
Validation loss: 2.627735617340252

Epoch: 76| Step: 0
Training loss: 3.4997706413269043
Validation loss: 2.627008153546241

Epoch: 5| Step: 1
Training loss: 2.976174831390381
Validation loss: 2.6266880086673203

Epoch: 5| Step: 2
Training loss: 3.1040594577789307
Validation loss: 2.628665583108061

Epoch: 5| Step: 3
Training loss: 2.631859064102173
Validation loss: 2.6280210684704524

Epoch: 5| Step: 4
Training loss: 2.342771530151367
Validation loss: 2.628335455412506

Epoch: 5| Step: 5
Training loss: 2.8673832416534424
Validation loss: 2.625888386080342

Epoch: 5| Step: 6
Training loss: 3.095073699951172
Validation loss: 2.6274494406997517

Epoch: 5| Step: 7
Training loss: 3.3129172325134277
Validation loss: 2.6260512234062277

Epoch: 5| Step: 8
Training loss: 2.0543572902679443
Validation loss: 2.6240963012941423

Epoch: 5| Step: 9
Training loss: 2.207855224609375
Validation loss: 2.6248467737628567

Epoch: 5| Step: 10
Training loss: 2.494065284729004
Validation loss: 2.6299525230161604

Epoch: 77| Step: 0
Training loss: 2.09277606010437
Validation loss: 2.631169411443895

Epoch: 5| Step: 1
Training loss: 2.4134626388549805
Validation loss: 2.6328931316252677

Epoch: 5| Step: 2
Training loss: 3.2133069038391113
Validation loss: 2.6404687307214223

Epoch: 5| Step: 3
Training loss: 3.099152088165283
Validation loss: 2.647127430926087

Epoch: 5| Step: 4
Training loss: 2.622361421585083
Validation loss: 2.6430014410326557

Epoch: 5| Step: 5
Training loss: 2.628765821456909
Validation loss: 2.634901269789665

Epoch: 5| Step: 6
Training loss: 2.3070757389068604
Validation loss: 2.6324246673173803

Epoch: 5| Step: 7
Training loss: 3.028691053390503
Validation loss: 2.622216927107944

Epoch: 5| Step: 8
Training loss: 3.677781581878662
Validation loss: 2.6200260603299705

Epoch: 5| Step: 9
Training loss: 3.0392401218414307
Validation loss: 2.619372534495528

Epoch: 5| Step: 10
Training loss: 2.415189027786255
Validation loss: 2.6219627934117473

Epoch: 78| Step: 0
Training loss: 2.5763003826141357
Validation loss: 2.6209405878538727

Epoch: 5| Step: 1
Training loss: 2.591458797454834
Validation loss: 2.619527188680505

Epoch: 5| Step: 2
Training loss: 2.6809804439544678
Validation loss: 2.6198300494942615

Epoch: 5| Step: 3
Training loss: 2.8210043907165527
Validation loss: 2.622240474147181

Epoch: 5| Step: 4
Training loss: 2.4296681880950928
Validation loss: 2.6240205303315194

Epoch: 5| Step: 5
Training loss: 3.072275161743164
Validation loss: 2.6227629646178214

Epoch: 5| Step: 6
Training loss: 2.6930878162384033
Validation loss: 2.6209444384421072

Epoch: 5| Step: 7
Training loss: 3.11846923828125
Validation loss: 2.6213140205670427

Epoch: 5| Step: 8
Training loss: 3.329530715942383
Validation loss: 2.619736756047895

Epoch: 5| Step: 9
Training loss: 3.222876787185669
Validation loss: 2.620096024646554

Epoch: 5| Step: 10
Training loss: 1.8894076347351074
Validation loss: 2.6180946737207393

Epoch: 79| Step: 0
Training loss: 2.5441646575927734
Validation loss: 2.6201474538413425

Epoch: 5| Step: 1
Training loss: 3.1253180503845215
Validation loss: 2.6218002175772064

Epoch: 5| Step: 2
Training loss: 2.57026743888855
Validation loss: 2.630402636784379

Epoch: 5| Step: 3
Training loss: 1.7218462228775024
Validation loss: 2.6296428377910326

Epoch: 5| Step: 4
Training loss: 3.0784199237823486
Validation loss: 2.63482641917403

Epoch: 5| Step: 5
Training loss: 3.478914737701416
Validation loss: 2.62866041480854

Epoch: 5| Step: 6
Training loss: 3.750458240509033
Validation loss: 2.6241987751376246

Epoch: 5| Step: 7
Training loss: 2.135599374771118
Validation loss: 2.620499554500785

Epoch: 5| Step: 8
Training loss: 2.8845996856689453
Validation loss: 2.6182433251411683

Epoch: 5| Step: 9
Training loss: 2.2128653526306152
Validation loss: 2.6188690893111692

Epoch: 5| Step: 10
Training loss: 3.041153907775879
Validation loss: 2.6143321016783356

Epoch: 80| Step: 0
Training loss: 3.097958564758301
Validation loss: 2.6164315977404193

Epoch: 5| Step: 1
Training loss: 2.381199836730957
Validation loss: 2.6154893623885287

Epoch: 5| Step: 2
Training loss: 2.8959851264953613
Validation loss: 2.6129908561706543

Epoch: 5| Step: 3
Training loss: 3.2745862007141113
Validation loss: 2.6166662272586616

Epoch: 5| Step: 4
Training loss: 2.6564688682556152
Validation loss: 2.615913526986235

Epoch: 5| Step: 5
Training loss: 2.243898868560791
Validation loss: 2.6132758227727746

Epoch: 5| Step: 6
Training loss: 2.5526046752929688
Validation loss: 2.6168269752174296

Epoch: 5| Step: 7
Training loss: 2.7014544010162354
Validation loss: 2.6158659970888527

Epoch: 5| Step: 8
Training loss: 2.7716500759124756
Validation loss: 2.6132202199710313

Epoch: 5| Step: 9
Training loss: 2.4371767044067383
Validation loss: 2.621797607791039

Epoch: 5| Step: 10
Training loss: 3.6041195392608643
Validation loss: 2.618187194229454

Epoch: 81| Step: 0
Training loss: 2.509249448776245
Validation loss: 2.6123484155183196

Epoch: 5| Step: 1
Training loss: 3.099583148956299
Validation loss: 2.624156436612529

Epoch: 5| Step: 2
Training loss: 2.288001775741577
Validation loss: 2.6280874898356776

Epoch: 5| Step: 3
Training loss: 3.3642070293426514
Validation loss: 2.645788913132042

Epoch: 5| Step: 4
Training loss: 3.2483296394348145
Validation loss: 2.6433998461692565

Epoch: 5| Step: 5
Training loss: 2.8035049438476562
Validation loss: 2.6458327565141904

Epoch: 5| Step: 6
Training loss: 2.1197874546051025
Validation loss: 2.6360787832608787

Epoch: 5| Step: 7
Training loss: 1.9825255870819092
Validation loss: 2.629602280996179

Epoch: 5| Step: 8
Training loss: 3.1449995040893555
Validation loss: 2.619293351327219

Epoch: 5| Step: 9
Training loss: 2.7751693725585938
Validation loss: 2.6136669522972515

Epoch: 5| Step: 10
Training loss: 3.2399306297302246
Validation loss: 2.6122727919650335

Epoch: 82| Step: 0
Training loss: 2.3544421195983887
Validation loss: 2.6118372486483667

Epoch: 5| Step: 1
Training loss: 2.8853769302368164
Validation loss: 2.6104341860740417

Epoch: 5| Step: 2
Training loss: 2.575650930404663
Validation loss: 2.613954887595228

Epoch: 5| Step: 3
Training loss: 2.7796719074249268
Validation loss: 2.6171866924532

Epoch: 5| Step: 4
Training loss: 3.157623767852783
Validation loss: 2.6198285125917002

Epoch: 5| Step: 5
Training loss: 2.973963499069214
Validation loss: 2.622075952509398

Epoch: 5| Step: 6
Training loss: 2.1583549976348877
Validation loss: 2.6182039681301323

Epoch: 5| Step: 7
Training loss: 3.2201480865478516
Validation loss: 2.614616991371237

Epoch: 5| Step: 8
Training loss: 2.5483131408691406
Validation loss: 2.6120604558657576

Epoch: 5| Step: 9
Training loss: 2.7263195514678955
Validation loss: 2.6102462071244434

Epoch: 5| Step: 10
Training loss: 3.1531119346618652
Validation loss: 2.6049836502280286

Epoch: 83| Step: 0
Training loss: 3.6521034240722656
Validation loss: 2.6089506200564805

Epoch: 5| Step: 1
Training loss: 3.2063140869140625
Validation loss: 2.610264637136972

Epoch: 5| Step: 2
Training loss: 2.6994800567626953
Validation loss: 2.613063722528437

Epoch: 5| Step: 3
Training loss: 2.8238890171051025
Validation loss: 2.610163947587372

Epoch: 5| Step: 4
Training loss: 2.433476448059082
Validation loss: 2.614823402897004

Epoch: 5| Step: 5
Training loss: 2.7352123260498047
Validation loss: 2.6219608181266376

Epoch: 5| Step: 6
Training loss: 2.271117687225342
Validation loss: 2.628403740544473

Epoch: 5| Step: 7
Training loss: 2.798788070678711
Validation loss: 2.632472963743312

Epoch: 5| Step: 8
Training loss: 2.7849371433258057
Validation loss: 2.6413935820261636

Epoch: 5| Step: 9
Training loss: 3.1094117164611816
Validation loss: 2.615095579495994

Epoch: 5| Step: 10
Training loss: 1.725036859512329
Validation loss: 2.6116184726838143

Epoch: 84| Step: 0
Training loss: 2.171750545501709
Validation loss: 2.6099376678466797

Epoch: 5| Step: 1
Training loss: 2.5150554180145264
Validation loss: 2.6118788667904433

Epoch: 5| Step: 2
Training loss: 2.9487338066101074
Validation loss: 2.6137873152250886

Epoch: 5| Step: 3
Training loss: 3.237143039703369
Validation loss: 2.6111334959665933

Epoch: 5| Step: 4
Training loss: 2.1680798530578613
Validation loss: 2.617008275883172

Epoch: 5| Step: 5
Training loss: 2.59791898727417
Validation loss: 2.614378882992652

Epoch: 5| Step: 6
Training loss: 2.941805839538574
Validation loss: 2.618134467832504

Epoch: 5| Step: 7
Training loss: 3.7905516624450684
Validation loss: 2.6145353958170903

Epoch: 5| Step: 8
Training loss: 2.4400830268859863
Validation loss: 2.6090078584609495

Epoch: 5| Step: 9
Training loss: 2.932638645172119
Validation loss: 2.6088586879032913

Epoch: 5| Step: 10
Training loss: 2.5687272548675537
Validation loss: 2.6060650656300206

Epoch: 85| Step: 0
Training loss: 2.9500324726104736
Validation loss: 2.6036840484988306

Epoch: 5| Step: 1
Training loss: 2.683797836303711
Validation loss: 2.6030562359799623

Epoch: 5| Step: 2
Training loss: 2.4813365936279297
Validation loss: 2.600809686927385

Epoch: 5| Step: 3
Training loss: 2.7218449115753174
Validation loss: 2.6042032139275664

Epoch: 5| Step: 4
Training loss: 2.347038745880127
Validation loss: 2.603609856738839

Epoch: 5| Step: 5
Training loss: 3.2701995372772217
Validation loss: 2.6011554220671296

Epoch: 5| Step: 6
Training loss: 3.237835645675659
Validation loss: 2.599451998228668

Epoch: 5| Step: 7
Training loss: 2.8765358924865723
Validation loss: 2.600493956637639

Epoch: 5| Step: 8
Training loss: 2.3372771739959717
Validation loss: 2.602199521116031

Epoch: 5| Step: 9
Training loss: 3.297293186187744
Validation loss: 2.601449961303383

Epoch: 5| Step: 10
Training loss: 1.9882593154907227
Validation loss: 2.599085933418684

Epoch: 86| Step: 0
Training loss: 2.787062883377075
Validation loss: 2.6026116494209535

Epoch: 5| Step: 1
Training loss: 2.4960293769836426
Validation loss: 2.602055767531036

Epoch: 5| Step: 2
Training loss: 2.3131980895996094
Validation loss: 2.607559806557112

Epoch: 5| Step: 3
Training loss: 2.5837645530700684
Validation loss: 2.60686400628859

Epoch: 5| Step: 4
Training loss: 2.368023633956909
Validation loss: 2.610525797772151

Epoch: 5| Step: 5
Training loss: 3.121490478515625
Validation loss: 2.610663208910214

Epoch: 5| Step: 6
Training loss: 2.8165555000305176
Validation loss: 2.6053380222730738

Epoch: 5| Step: 7
Training loss: 2.665278434753418
Validation loss: 2.6082214540050876

Epoch: 5| Step: 8
Training loss: 2.8192851543426514
Validation loss: 2.600419067567395

Epoch: 5| Step: 9
Training loss: 2.9688591957092285
Validation loss: 2.6001464090039654

Epoch: 5| Step: 10
Training loss: 3.5105161666870117
Validation loss: 2.5975534531377975

Epoch: 87| Step: 0
Training loss: 2.974316120147705
Validation loss: 2.5973382842156196

Epoch: 5| Step: 1
Training loss: 3.1124210357666016
Validation loss: 2.598227362478933

Epoch: 5| Step: 2
Training loss: 2.661142349243164
Validation loss: 2.5979425009860786

Epoch: 5| Step: 3
Training loss: 2.7745580673217773
Validation loss: 2.596353792375134

Epoch: 5| Step: 4
Training loss: 3.1724982261657715
Validation loss: 2.59599405975752

Epoch: 5| Step: 5
Training loss: 2.1636829376220703
Validation loss: 2.59839722418016

Epoch: 5| Step: 6
Training loss: 2.8106226921081543
Validation loss: 2.5987814088021555

Epoch: 5| Step: 7
Training loss: 2.899473190307617
Validation loss: 2.6004802693602858

Epoch: 5| Step: 8
Training loss: 2.4452812671661377
Validation loss: 2.598264801886774

Epoch: 5| Step: 9
Training loss: 2.410148859024048
Validation loss: 2.6007821303541943

Epoch: 5| Step: 10
Training loss: 2.8738934993743896
Validation loss: 2.599710979769307

Epoch: 88| Step: 0
Training loss: 2.138681173324585
Validation loss: 2.596327184348978

Epoch: 5| Step: 1
Training loss: 2.8450591564178467
Validation loss: 2.5987700595650622

Epoch: 5| Step: 2
Training loss: 2.650372266769409
Validation loss: 2.6023618687865553

Epoch: 5| Step: 3
Training loss: 3.5393662452697754
Validation loss: 2.6052833064909904

Epoch: 5| Step: 4
Training loss: 2.6637091636657715
Validation loss: 2.5975195874450026

Epoch: 5| Step: 5
Training loss: 3.166992425918579
Validation loss: 2.5980957861869567

Epoch: 5| Step: 6
Training loss: 3.014672040939331
Validation loss: 2.598134545869725

Epoch: 5| Step: 7
Training loss: 2.8363258838653564
Validation loss: 2.593284014732607

Epoch: 5| Step: 8
Training loss: 2.5164332389831543
Validation loss: 2.5916504449741815

Epoch: 5| Step: 9
Training loss: 3.0740504264831543
Validation loss: 2.5934882599820375

Epoch: 5| Step: 10
Training loss: 1.6681157350540161
Validation loss: 2.5912125572081535

Epoch: 89| Step: 0
Training loss: 2.341660976409912
Validation loss: 2.5926652903197915

Epoch: 5| Step: 1
Training loss: 2.244328737258911
Validation loss: 2.5916108392900035

Epoch: 5| Step: 2
Training loss: 2.863187074661255
Validation loss: 2.5915630632831204

Epoch: 5| Step: 3
Training loss: 3.171691656112671
Validation loss: 2.5929954769790813

Epoch: 5| Step: 4
Training loss: 2.763248920440674
Validation loss: 2.591221709405222

Epoch: 5| Step: 5
Training loss: 2.1205315589904785
Validation loss: 2.5898643949980378

Epoch: 5| Step: 6
Training loss: 2.9491212368011475
Validation loss: 2.594279960919452

Epoch: 5| Step: 7
Training loss: 3.6386122703552246
Validation loss: 2.595095460132886

Epoch: 5| Step: 8
Training loss: 2.2380714416503906
Validation loss: 2.598449017411919

Epoch: 5| Step: 9
Training loss: 2.7768256664276123
Validation loss: 2.6006070542079147

Epoch: 5| Step: 10
Training loss: 3.220015287399292
Validation loss: 2.6098099600884224

Epoch: 90| Step: 0
Training loss: 2.7790300846099854
Validation loss: 2.604730567624492

Epoch: 5| Step: 1
Training loss: 2.664912462234497
Validation loss: 2.601736041807359

Epoch: 5| Step: 2
Training loss: 2.7828497886657715
Validation loss: 2.6025452844558226

Epoch: 5| Step: 3
Training loss: 2.817469358444214
Validation loss: 2.598707317024149

Epoch: 5| Step: 4
Training loss: 2.1807312965393066
Validation loss: 2.6026013102582706

Epoch: 5| Step: 5
Training loss: 3.3409264087677
Validation loss: 2.6002496211759505

Epoch: 5| Step: 6
Training loss: 2.4478015899658203
Validation loss: 2.5971948485220633

Epoch: 5| Step: 7
Training loss: 3.0877625942230225
Validation loss: 2.603202412205358

Epoch: 5| Step: 8
Training loss: 2.6375956535339355
Validation loss: 2.623737983806159

Epoch: 5| Step: 9
Training loss: 2.783379316329956
Validation loss: 2.621903286185316

Epoch: 5| Step: 10
Training loss: 2.758795976638794
Validation loss: 2.5939852883738856

Epoch: 91| Step: 0
Training loss: 2.5151360034942627
Validation loss: 2.5891644980317805

Epoch: 5| Step: 1
Training loss: 3.241020679473877
Validation loss: 2.5926198138985583

Epoch: 5| Step: 2
Training loss: 2.6988651752471924
Validation loss: 2.5920289306230444

Epoch: 5| Step: 3
Training loss: 2.4630627632141113
Validation loss: 2.597670083404869

Epoch: 5| Step: 4
Training loss: 2.089616298675537
Validation loss: 2.600772819211406

Epoch: 5| Step: 5
Training loss: 3.418550968170166
Validation loss: 2.5993296254065728

Epoch: 5| Step: 6
Training loss: 2.7144718170166016
Validation loss: 2.5956392390753633

Epoch: 5| Step: 7
Training loss: 2.7467305660247803
Validation loss: 2.5919683441039054

Epoch: 5| Step: 8
Training loss: 2.2430498600006104
Validation loss: 2.5892840803310437

Epoch: 5| Step: 9
Training loss: 3.3610706329345703
Validation loss: 2.5923886760588615

Epoch: 5| Step: 10
Training loss: 2.790363311767578
Validation loss: 2.5892994301293486

Epoch: 92| Step: 0
Training loss: 2.998384952545166
Validation loss: 2.5885909090759935

Epoch: 5| Step: 1
Training loss: 2.461459159851074
Validation loss: 2.5908101245921147

Epoch: 5| Step: 2
Training loss: 2.4270718097686768
Validation loss: 2.588594892973541

Epoch: 5| Step: 3
Training loss: 2.3979744911193848
Validation loss: 2.5915928143326954

Epoch: 5| Step: 4
Training loss: 3.132150888442993
Validation loss: 2.592361054112834

Epoch: 5| Step: 5
Training loss: 2.9582810401916504
Validation loss: 2.5962006302290064

Epoch: 5| Step: 6
Training loss: 2.970181465148926
Validation loss: 2.5928817308077248

Epoch: 5| Step: 7
Training loss: 3.191918134689331
Validation loss: 2.591451588497367

Epoch: 5| Step: 8
Training loss: 2.3929717540740967
Validation loss: 2.590639970635855

Epoch: 5| Step: 9
Training loss: 2.814039945602417
Validation loss: 2.5868076842318297

Epoch: 5| Step: 10
Training loss: 2.4527175426483154
Validation loss: 2.584555814343114

Epoch: 93| Step: 0
Training loss: 1.7011104822158813
Validation loss: 2.5869988523503786

Epoch: 5| Step: 1
Training loss: 2.3873417377471924
Validation loss: 2.5824881881795902

Epoch: 5| Step: 2
Training loss: 2.856358051300049
Validation loss: 2.5835220685569187

Epoch: 5| Step: 3
Training loss: 2.9272472858428955
Validation loss: 2.5845830568703274

Epoch: 5| Step: 4
Training loss: 3.2662456035614014
Validation loss: 2.587562040616107

Epoch: 5| Step: 5
Training loss: 2.102750062942505
Validation loss: 2.584930558358469

Epoch: 5| Step: 6
Training loss: 2.9507715702056885
Validation loss: 2.583279094388408

Epoch: 5| Step: 7
Training loss: 2.2849318981170654
Validation loss: 2.584257100218086

Epoch: 5| Step: 8
Training loss: 3.0149500370025635
Validation loss: 2.5866780973249868

Epoch: 5| Step: 9
Training loss: 3.3469531536102295
Validation loss: 2.582884639822027

Epoch: 5| Step: 10
Training loss: 3.448437213897705
Validation loss: 2.584402107423352

Epoch: 94| Step: 0
Training loss: 3.049381732940674
Validation loss: 2.586581542927732

Epoch: 5| Step: 1
Training loss: 2.265357732772827
Validation loss: 2.5841311562445854

Epoch: 5| Step: 2
Training loss: 2.55788516998291
Validation loss: 2.5839380448864353

Epoch: 5| Step: 3
Training loss: 2.715268611907959
Validation loss: 2.5851443300965014

Epoch: 5| Step: 4
Training loss: 2.700451374053955
Validation loss: 2.582748346431281

Epoch: 5| Step: 5
Training loss: 2.695606231689453
Validation loss: 2.580500074612197

Epoch: 5| Step: 6
Training loss: 3.161742687225342
Validation loss: 2.583807140268305

Epoch: 5| Step: 7
Training loss: 2.566896915435791
Validation loss: 2.5822056390905894

Epoch: 5| Step: 8
Training loss: 2.948019504547119
Validation loss: 2.5774341219214985

Epoch: 5| Step: 9
Training loss: 2.466813802719116
Validation loss: 2.579480658295334

Epoch: 5| Step: 10
Training loss: 3.033033847808838
Validation loss: 2.5767991106997252

Epoch: 95| Step: 0
Training loss: 2.683725118637085
Validation loss: 2.5765976828913533

Epoch: 5| Step: 1
Training loss: 2.291008710861206
Validation loss: 2.575144424233385

Epoch: 5| Step: 2
Training loss: 2.9033596515655518
Validation loss: 2.578613914469237

Epoch: 5| Step: 3
Training loss: 3.3337063789367676
Validation loss: 2.5769895097260833

Epoch: 5| Step: 4
Training loss: 2.9772067070007324
Validation loss: 2.5791587316861717

Epoch: 5| Step: 5
Training loss: 2.806204080581665
Validation loss: 2.5815216110598658

Epoch: 5| Step: 6
Training loss: 2.1966135501861572
Validation loss: 2.5801662398922827

Epoch: 5| Step: 7
Training loss: 2.6715540885925293
Validation loss: 2.5846378803253174

Epoch: 5| Step: 8
Training loss: 2.6516451835632324
Validation loss: 2.574523695053593

Epoch: 5| Step: 9
Training loss: 2.492255926132202
Validation loss: 2.5766861284932783

Epoch: 5| Step: 10
Training loss: 3.1437621116638184
Validation loss: 2.5758109989986626

Epoch: 96| Step: 0
Training loss: 2.397122621536255
Validation loss: 2.5765735231420046

Epoch: 5| Step: 1
Training loss: 2.936650037765503
Validation loss: 2.5735106596382717

Epoch: 5| Step: 2
Training loss: 3.0417351722717285
Validation loss: 2.57556402555076

Epoch: 5| Step: 3
Training loss: 2.424426555633545
Validation loss: 2.5739621782815583

Epoch: 5| Step: 4
Training loss: 2.420330762863159
Validation loss: 2.575320928327499

Epoch: 5| Step: 5
Training loss: 4.003750801086426
Validation loss: 2.57991107561255

Epoch: 5| Step: 6
Training loss: 3.0567944049835205
Validation loss: 2.586515090798819

Epoch: 5| Step: 7
Training loss: 2.7836806774139404
Validation loss: 2.58435889726044

Epoch: 5| Step: 8
Training loss: 2.218305826187134
Validation loss: 2.5786272402732604

Epoch: 5| Step: 9
Training loss: 2.8315093517303467
Validation loss: 2.57225549861949

Epoch: 5| Step: 10
Training loss: 1.8961899280548096
Validation loss: 2.5716890288937475

Epoch: 97| Step: 0
Training loss: 2.820584774017334
Validation loss: 2.573764447242983

Epoch: 5| Step: 1
Training loss: 3.140280246734619
Validation loss: 2.5750658204478603

Epoch: 5| Step: 2
Training loss: 2.8225624561309814
Validation loss: 2.5784573465265255

Epoch: 5| Step: 3
Training loss: 2.877073287963867
Validation loss: 2.583870280173517

Epoch: 5| Step: 4
Training loss: 2.3156237602233887
Validation loss: 2.586213852769585

Epoch: 5| Step: 5
Training loss: 2.279932737350464
Validation loss: 2.594792858246834

Epoch: 5| Step: 6
Training loss: 3.1440348625183105
Validation loss: 2.5792593955993652

Epoch: 5| Step: 7
Training loss: 2.7660186290740967
Validation loss: 2.5684483589664584

Epoch: 5| Step: 8
Training loss: 2.5508837699890137
Validation loss: 2.569193160662087

Epoch: 5| Step: 9
Training loss: 2.592350721359253
Validation loss: 2.567970206660609

Epoch: 5| Step: 10
Training loss: 2.8448972702026367
Validation loss: 2.5719596878174813

Epoch: 98| Step: 0
Training loss: 3.0271377563476562
Validation loss: 2.568220123167961

Epoch: 5| Step: 1
Training loss: 3.523472547531128
Validation loss: 2.568526683315154

Epoch: 5| Step: 2
Training loss: 2.8595316410064697
Validation loss: 2.569829176830989

Epoch: 5| Step: 3
Training loss: 2.3959946632385254
Validation loss: 2.568844464517409

Epoch: 5| Step: 4
Training loss: 2.4972240924835205
Validation loss: 2.5700818107974146

Epoch: 5| Step: 5
Training loss: 2.4491569995880127
Validation loss: 2.572151509664392

Epoch: 5| Step: 6
Training loss: 2.196542501449585
Validation loss: 2.575589700411725

Epoch: 5| Step: 7
Training loss: 3.1667749881744385
Validation loss: 2.5796405243617233

Epoch: 5| Step: 8
Training loss: 2.523458957672119
Validation loss: 2.5819304271410872

Epoch: 5| Step: 9
Training loss: 2.7568225860595703
Validation loss: 2.5775368649472474

Epoch: 5| Step: 10
Training loss: 2.723825454711914
Validation loss: 2.581203455566078

Epoch: 99| Step: 0
Training loss: 2.4052109718322754
Validation loss: 2.582895376349008

Epoch: 5| Step: 1
Training loss: 2.925170421600342
Validation loss: 2.58387690718456

Epoch: 5| Step: 2
Training loss: 2.6398892402648926
Validation loss: 2.584827382077453

Epoch: 5| Step: 3
Training loss: 3.332376003265381
Validation loss: 2.5756589930544616

Epoch: 5| Step: 4
Training loss: 2.6059699058532715
Validation loss: 2.5681769822233464

Epoch: 5| Step: 5
Training loss: 3.195223569869995
Validation loss: 2.5666878531056065

Epoch: 5| Step: 6
Training loss: 2.142683982849121
Validation loss: 2.5641247892892487

Epoch: 5| Step: 7
Training loss: 2.616917371749878
Validation loss: 2.5676116353722027

Epoch: 5| Step: 8
Training loss: 2.354649066925049
Validation loss: 2.571069504625054

Epoch: 5| Step: 9
Training loss: 2.9726791381835938
Validation loss: 2.57179396383224

Epoch: 5| Step: 10
Training loss: 2.971510171890259
Validation loss: 2.5700898247380413

Epoch: 100| Step: 0
Training loss: 2.639709949493408
Validation loss: 2.5701156649538266

Epoch: 5| Step: 1
Training loss: 2.791658878326416
Validation loss: 2.5706989995894896

Epoch: 5| Step: 2
Training loss: 2.606208562850952
Validation loss: 2.568710075911655

Epoch: 5| Step: 3
Training loss: 2.8052985668182373
Validation loss: 2.5727337611618863

Epoch: 5| Step: 4
Training loss: 2.737011194229126
Validation loss: 2.5701485218540316

Epoch: 5| Step: 5
Training loss: 3.0886142253875732
Validation loss: 2.567916111279559

Epoch: 5| Step: 6
Training loss: 2.944612979888916
Validation loss: 2.570268620726883

Epoch: 5| Step: 7
Training loss: 3.1272690296173096
Validation loss: 2.567189356332184

Epoch: 5| Step: 8
Training loss: 2.0452322959899902
Validation loss: 2.566941127982191

Epoch: 5| Step: 9
Training loss: 2.605196714401245
Validation loss: 2.5660176918070805

Epoch: 5| Step: 10
Training loss: 2.7019073963165283
Validation loss: 2.566728427845945

Epoch: 101| Step: 0
Training loss: 2.365838050842285
Validation loss: 2.565291832852107

Epoch: 5| Step: 1
Training loss: 2.703725814819336
Validation loss: 2.5616842726225495

Epoch: 5| Step: 2
Training loss: 3.3124289512634277
Validation loss: 2.563547744545885

Epoch: 5| Step: 3
Training loss: 2.8089044094085693
Validation loss: 2.562253452116443

Epoch: 5| Step: 4
Training loss: 2.087615728378296
Validation loss: 2.562005553194272

Epoch: 5| Step: 5
Training loss: 3.375910520553589
Validation loss: 2.5627480911952194

Epoch: 5| Step: 6
Training loss: 2.9282045364379883
Validation loss: 2.5636169628430436

Epoch: 5| Step: 7
Training loss: 2.587068557739258
Validation loss: 2.566688617070516

Epoch: 5| Step: 8
Training loss: 2.3573386669158936
Validation loss: 2.5613293775948147

Epoch: 5| Step: 9
Training loss: 2.5031871795654297
Validation loss: 2.564902167166433

Epoch: 5| Step: 10
Training loss: 3.078411340713501
Validation loss: 2.5650580313897904

Epoch: 102| Step: 0
Training loss: 3.2630882263183594
Validation loss: 2.5607691170066915

Epoch: 5| Step: 1
Training loss: 2.6174306869506836
Validation loss: 2.561644115755635

Epoch: 5| Step: 2
Training loss: 2.649723768234253
Validation loss: 2.5628411128956783

Epoch: 5| Step: 3
Training loss: 2.286928176879883
Validation loss: 2.5613427879989787

Epoch: 5| Step: 4
Training loss: 2.460706949234009
Validation loss: 2.5598592501814648

Epoch: 5| Step: 5
Training loss: 2.882474899291992
Validation loss: 2.563586065846105

Epoch: 5| Step: 6
Training loss: 2.9264378547668457
Validation loss: 2.562471378234125

Epoch: 5| Step: 7
Training loss: 2.686771869659424
Validation loss: 2.5671423481356714

Epoch: 5| Step: 8
Training loss: 2.8227133750915527
Validation loss: 2.5682382224708475

Epoch: 5| Step: 9
Training loss: 2.3896467685699463
Validation loss: 2.5710722554114556

Epoch: 5| Step: 10
Training loss: 3.160543918609619
Validation loss: 2.5668253872984197

Epoch: 103| Step: 0
Training loss: 2.7317166328430176
Validation loss: 2.5637659642004196

Epoch: 5| Step: 1
Training loss: 1.8536638021469116
Validation loss: 2.560207954017065

Epoch: 5| Step: 2
Training loss: 3.2095274925231934
Validation loss: 2.561090689833446

Epoch: 5| Step: 3
Training loss: 2.799199342727661
Validation loss: 2.55829272988022

Epoch: 5| Step: 4
Training loss: 3.696068286895752
Validation loss: 2.5616918404897056

Epoch: 5| Step: 5
Training loss: 2.4165120124816895
Validation loss: 2.5587276694595174

Epoch: 5| Step: 6
Training loss: 2.7983336448669434
Validation loss: 2.55745784185266

Epoch: 5| Step: 7
Training loss: 2.1954689025878906
Validation loss: 2.5590513880534838

Epoch: 5| Step: 8
Training loss: 3.2583422660827637
Validation loss: 2.5622322008173954

Epoch: 5| Step: 9
Training loss: 2.5490987300872803
Validation loss: 2.562682887559296

Epoch: 5| Step: 10
Training loss: 2.413182497024536
Validation loss: 2.5621154321137296

Epoch: 104| Step: 0
Training loss: 2.997142791748047
Validation loss: 2.5623556952322684

Epoch: 5| Step: 1
Training loss: 2.316225290298462
Validation loss: 2.566614168946461

Epoch: 5| Step: 2
Training loss: 2.399815797805786
Validation loss: 2.563839581704909

Epoch: 5| Step: 3
Training loss: 2.8937325477600098
Validation loss: 2.5610730955677647

Epoch: 5| Step: 4
Training loss: 3.1712965965270996
Validation loss: 2.558748891276698

Epoch: 5| Step: 5
Training loss: 2.4972281455993652
Validation loss: 2.55906157852501

Epoch: 5| Step: 6
Training loss: 2.8705894947052
Validation loss: 2.55763663271422

Epoch: 5| Step: 7
Training loss: 2.649869203567505
Validation loss: 2.5540490868271037

Epoch: 5| Step: 8
Training loss: 3.077968120574951
Validation loss: 2.557689218110936

Epoch: 5| Step: 9
Training loss: 2.250892162322998
Validation loss: 2.560985570312828

Epoch: 5| Step: 10
Training loss: 2.805936098098755
Validation loss: 2.5621372089591077

Epoch: 105| Step: 0
Training loss: 1.9605159759521484
Validation loss: 2.5616873438640306

Epoch: 5| Step: 1
Training loss: 1.9229882955551147
Validation loss: 2.5660171842062347

Epoch: 5| Step: 2
Training loss: 2.466121196746826
Validation loss: 2.562182032933799

Epoch: 5| Step: 3
Training loss: 2.6541266441345215
Validation loss: 2.560980555831745

Epoch: 5| Step: 4
Training loss: 3.3952953815460205
Validation loss: 2.5592202063529723

Epoch: 5| Step: 5
Training loss: 3.363633394241333
Validation loss: 2.554381080853042

Epoch: 5| Step: 6
Training loss: 2.765934467315674
Validation loss: 2.5538565369062525

Epoch: 5| Step: 7
Training loss: 3.1963584423065186
Validation loss: 2.5542643839313137

Epoch: 5| Step: 8
Training loss: 2.8258748054504395
Validation loss: 2.554468498435072

Epoch: 5| Step: 9
Training loss: 2.478128433227539
Validation loss: 2.5607821377374793

Epoch: 5| Step: 10
Training loss: 3.03896164894104
Validation loss: 2.556478402947867

Epoch: 106| Step: 0
Training loss: 2.682506799697876
Validation loss: 2.5551130130726802

Epoch: 5| Step: 1
Training loss: 3.125598430633545
Validation loss: 2.5549818059449554

Epoch: 5| Step: 2
Training loss: 2.5663845539093018
Validation loss: 2.5546977494352605

Epoch: 5| Step: 3
Training loss: 2.4875431060791016
Validation loss: 2.5531000988457793

Epoch: 5| Step: 4
Training loss: 2.248528003692627
Validation loss: 2.5521293378645376

Epoch: 5| Step: 5
Training loss: 3.523723602294922
Validation loss: 2.552112094817623

Epoch: 5| Step: 6
Training loss: 2.6066665649414062
Validation loss: 2.5508236423615487

Epoch: 5| Step: 7
Training loss: 3.2720446586608887
Validation loss: 2.5515341104999667

Epoch: 5| Step: 8
Training loss: 2.4660048484802246
Validation loss: 2.553053558513682

Epoch: 5| Step: 9
Training loss: 2.4256577491760254
Validation loss: 2.552348429156888

Epoch: 5| Step: 10
Training loss: 2.5506019592285156
Validation loss: 2.5521357162024385

Epoch: 107| Step: 0
Training loss: 3.762054920196533
Validation loss: 2.5520469475817937

Epoch: 5| Step: 1
Training loss: 2.8475518226623535
Validation loss: 2.5482159404344458

Epoch: 5| Step: 2
Training loss: 2.6594326496124268
Validation loss: 2.5514065655328895

Epoch: 5| Step: 3
Training loss: 2.646982192993164
Validation loss: 2.5506038281225387

Epoch: 5| Step: 4
Training loss: 2.2362163066864014
Validation loss: 2.552402832174814

Epoch: 5| Step: 5
Training loss: 3.0335853099823
Validation loss: 2.5617429389748523

Epoch: 5| Step: 6
Training loss: 2.2512459754943848
Validation loss: 2.563700552909605

Epoch: 5| Step: 7
Training loss: 2.6559882164001465
Validation loss: 2.56715972961918

Epoch: 5| Step: 8
Training loss: 2.8645179271698
Validation loss: 2.5674272865377445

Epoch: 5| Step: 9
Training loss: 2.7872071266174316
Validation loss: 2.567222974633658

Epoch: 5| Step: 10
Training loss: 2.1649811267852783
Validation loss: 2.56245300333987

Epoch: 108| Step: 0
Training loss: 3.1558873653411865
Validation loss: 2.5542480073949343

Epoch: 5| Step: 1
Training loss: 3.1799418926239014
Validation loss: 2.5500997856099117

Epoch: 5| Step: 2
Training loss: 2.713216781616211
Validation loss: 2.548748262466923

Epoch: 5| Step: 3
Training loss: 2.9807541370391846
Validation loss: 2.548344648012551

Epoch: 5| Step: 4
Training loss: 2.1785151958465576
Validation loss: 2.5493433911313295

Epoch: 5| Step: 5
Training loss: 2.7522010803222656
Validation loss: 2.558009011771089

Epoch: 5| Step: 6
Training loss: 2.5661990642547607
Validation loss: 2.569455805645194

Epoch: 5| Step: 7
Training loss: 2.5512657165527344
Validation loss: 2.565430994956724

Epoch: 5| Step: 8
Training loss: 2.670165538787842
Validation loss: 2.571247652012815

Epoch: 5| Step: 9
Training loss: 2.777355432510376
Validation loss: 2.5813509828300885

Epoch: 5| Step: 10
Training loss: 2.466096878051758
Validation loss: 2.5876212222601778

Epoch: 109| Step: 0
Training loss: 3.079000473022461
Validation loss: 2.5629400284059587

Epoch: 5| Step: 1
Training loss: 2.452439785003662
Validation loss: 2.5505834958886586

Epoch: 5| Step: 2
Training loss: 2.3786346912384033
Validation loss: 2.5562627418066866

Epoch: 5| Step: 3
Training loss: 2.31904935836792
Validation loss: 2.558323903750348

Epoch: 5| Step: 4
Training loss: 2.6245875358581543
Validation loss: 2.5686892668406167

Epoch: 5| Step: 5
Training loss: 3.0700316429138184
Validation loss: 2.5674502439396356

Epoch: 5| Step: 6
Training loss: 2.385638475418091
Validation loss: 2.5631066829927507

Epoch: 5| Step: 7
Training loss: 3.023188829421997
Validation loss: 2.55940237609289

Epoch: 5| Step: 8
Training loss: 2.9641494750976562
Validation loss: 2.5513535878991567

Epoch: 5| Step: 9
Training loss: 2.6019113063812256
Validation loss: 2.544337513626263

Epoch: 5| Step: 10
Training loss: 3.125627279281616
Validation loss: 2.5446255207061768

Epoch: 110| Step: 0
Training loss: 2.7824301719665527
Validation loss: 2.5457156960682203

Epoch: 5| Step: 1
Training loss: 3.530315399169922
Validation loss: 2.5475005744605936

Epoch: 5| Step: 2
Training loss: 2.041891574859619
Validation loss: 2.543993542271276

Epoch: 5| Step: 3
Training loss: 2.1740562915802
Validation loss: 2.545870029798118

Epoch: 5| Step: 4
Training loss: 2.1100001335144043
Validation loss: 2.5424029442571823

Epoch: 5| Step: 5
Training loss: 3.5454354286193848
Validation loss: 2.5421287782730593

Epoch: 5| Step: 6
Training loss: 2.9856696128845215
Validation loss: 2.545953522446335

Epoch: 5| Step: 7
Training loss: 2.8443076610565186
Validation loss: 2.546364914986395

Epoch: 5| Step: 8
Training loss: 2.4585163593292236
Validation loss: 2.545871680782687

Epoch: 5| Step: 9
Training loss: 2.8202526569366455
Validation loss: 2.5646169724002963

Epoch: 5| Step: 10
Training loss: 2.6575236320495605
Validation loss: 2.5771094752896215

Epoch: 111| Step: 0
Training loss: 3.0075600147247314
Validation loss: 2.5807740329414286

Epoch: 5| Step: 1
Training loss: 3.155513286590576
Validation loss: 2.5884050605117634

Epoch: 5| Step: 2
Training loss: 2.1634509563446045
Validation loss: 2.5585321534064507

Epoch: 5| Step: 3
Training loss: 2.535068988800049
Validation loss: 2.5493299115088677

Epoch: 5| Step: 4
Training loss: 2.4878957271575928
Validation loss: 2.550597144711402

Epoch: 5| Step: 5
Training loss: 2.6957268714904785
Validation loss: 2.5503734952660015

Epoch: 5| Step: 6
Training loss: 2.8138318061828613
Validation loss: 2.552074678482548

Epoch: 5| Step: 7
Training loss: 2.7005224227905273
Validation loss: 2.5498582214437504

Epoch: 5| Step: 8
Training loss: 2.5725104808807373
Validation loss: 2.5486489495923443

Epoch: 5| Step: 9
Training loss: 2.866670846939087
Validation loss: 2.542561049102455

Epoch: 5| Step: 10
Training loss: 2.9263205528259277
Validation loss: 2.544624792632236

Epoch: 112| Step: 0
Training loss: 2.3376526832580566
Validation loss: 2.5415065416725735

Epoch: 5| Step: 1
Training loss: 2.4665026664733887
Validation loss: 2.5384834069077686

Epoch: 5| Step: 2
Training loss: 2.114208221435547
Validation loss: 2.5362144875270065

Epoch: 5| Step: 3
Training loss: 2.2792186737060547
Validation loss: 2.537137387901224

Epoch: 5| Step: 4
Training loss: 3.1570217609405518
Validation loss: 2.537507003353488

Epoch: 5| Step: 5
Training loss: 3.2916176319122314
Validation loss: 2.540104854491449

Epoch: 5| Step: 6
Training loss: 3.090174674987793
Validation loss: 2.5405958775551087

Epoch: 5| Step: 7
Training loss: 2.158836603164673
Validation loss: 2.5403913708143335

Epoch: 5| Step: 8
Training loss: 3.0294933319091797
Validation loss: 2.5447389130951255

Epoch: 5| Step: 9
Training loss: 2.944052219390869
Validation loss: 2.5450568224794123

Epoch: 5| Step: 10
Training loss: 3.105520248413086
Validation loss: 2.551415276783769

Epoch: 113| Step: 0
Training loss: 2.3398454189300537
Validation loss: 2.5463658968607583

Epoch: 5| Step: 1
Training loss: 2.7149922847747803
Validation loss: 2.5482281920730427

Epoch: 5| Step: 2
Training loss: 2.396027088165283
Validation loss: 2.545182066579019

Epoch: 5| Step: 3
Training loss: 2.2771239280700684
Validation loss: 2.5438104086024786

Epoch: 5| Step: 4
Training loss: 3.2129924297332764
Validation loss: 2.5380077874788673

Epoch: 5| Step: 5
Training loss: 2.756096601486206
Validation loss: 2.539947348256265

Epoch: 5| Step: 6
Training loss: 3.146238327026367
Validation loss: 2.5381449166164605

Epoch: 5| Step: 7
Training loss: 2.5513453483581543
Validation loss: 2.536525644281859

Epoch: 5| Step: 8
Training loss: 2.799647092819214
Validation loss: 2.5364551672371487

Epoch: 5| Step: 9
Training loss: 3.2261478900909424
Validation loss: 2.5362849799535607

Epoch: 5| Step: 10
Training loss: 2.3487493991851807
Validation loss: 2.5365803754457863

Epoch: 114| Step: 0
Training loss: 3.010486602783203
Validation loss: 2.5407253414072017

Epoch: 5| Step: 1
Training loss: 2.553290843963623
Validation loss: 2.5409873198437434

Epoch: 5| Step: 2
Training loss: 2.34232759475708
Validation loss: 2.5408048091396207

Epoch: 5| Step: 3
Training loss: 2.720085620880127
Validation loss: 2.5409433918614543

Epoch: 5| Step: 4
Training loss: 3.035538911819458
Validation loss: 2.5365712668306086

Epoch: 5| Step: 5
Training loss: 2.6484992504119873
Validation loss: 2.5357654453605734

Epoch: 5| Step: 6
Training loss: 2.439260482788086
Validation loss: 2.536751539476456

Epoch: 5| Step: 7
Training loss: 2.9384448528289795
Validation loss: 2.5350243224892566

Epoch: 5| Step: 8
Training loss: 3.2012734413146973
Validation loss: 2.533229761226203

Epoch: 5| Step: 9
Training loss: 2.777365207672119
Validation loss: 2.5329796652640066

Epoch: 5| Step: 10
Training loss: 2.0425305366516113
Validation loss: 2.532571003001223

Epoch: 115| Step: 0
Training loss: 2.0863335132598877
Validation loss: 2.532680655038485

Epoch: 5| Step: 1
Training loss: 3.100468635559082
Validation loss: 2.5348237970823884

Epoch: 5| Step: 2
Training loss: 3.26845121383667
Validation loss: 2.537245427408526

Epoch: 5| Step: 3
Training loss: 2.598018169403076
Validation loss: 2.534805426033594

Epoch: 5| Step: 4
Training loss: 2.9161181449890137
Validation loss: 2.53217718678136

Epoch: 5| Step: 5
Training loss: 2.8429722785949707
Validation loss: 2.5357622869553103

Epoch: 5| Step: 6
Training loss: 2.9163060188293457
Validation loss: 2.5342906367394233

Epoch: 5| Step: 7
Training loss: 2.288930892944336
Validation loss: 2.536029549055202

Epoch: 5| Step: 8
Training loss: 2.6257753372192383
Validation loss: 2.543593286186136

Epoch: 5| Step: 9
Training loss: 3.1217551231384277
Validation loss: 2.539535996734455

Epoch: 5| Step: 10
Training loss: 1.84282648563385
Validation loss: 2.5401912094444357

Epoch: 116| Step: 0
Training loss: 2.265989065170288
Validation loss: 2.5370365086422173

Epoch: 5| Step: 1
Training loss: 3.036691188812256
Validation loss: 2.5323084503091793

Epoch: 5| Step: 2
Training loss: 3.4899678230285645
Validation loss: 2.5342178677999847

Epoch: 5| Step: 3
Training loss: 3.108245849609375
Validation loss: 2.52803175680099

Epoch: 5| Step: 4
Training loss: 2.5960865020751953
Validation loss: 2.529953225966423

Epoch: 5| Step: 5
Training loss: 2.4631056785583496
Validation loss: 2.5331716691294024

Epoch: 5| Step: 6
Training loss: 2.316715717315674
Validation loss: 2.534045501421857

Epoch: 5| Step: 7
Training loss: 2.282498836517334
Validation loss: 2.540718586214127

Epoch: 5| Step: 8
Training loss: 3.045771598815918
Validation loss: 2.538429367926813

Epoch: 5| Step: 9
Training loss: 2.344888925552368
Validation loss: 2.5375547921785744

Epoch: 5| Step: 10
Training loss: 2.7761270999908447
Validation loss: 2.5358741309053157

Epoch: 117| Step: 0
Training loss: 3.0712649822235107
Validation loss: 2.5340621932860343

Epoch: 5| Step: 1
Training loss: 2.9229328632354736
Validation loss: 2.5373231646835164

Epoch: 5| Step: 2
Training loss: 2.312861680984497
Validation loss: 2.5503244835843324

Epoch: 5| Step: 3
Training loss: 2.91721773147583
Validation loss: 2.5643914181699037

Epoch: 5| Step: 4
Training loss: 2.885380506515503
Validation loss: 2.5509534907597367

Epoch: 5| Step: 5
Training loss: 2.6027846336364746
Validation loss: 2.540792513919133

Epoch: 5| Step: 6
Training loss: 2.785428285598755
Validation loss: 2.5397150619055635

Epoch: 5| Step: 7
Training loss: 2.372450351715088
Validation loss: 2.5296336963612545

Epoch: 5| Step: 8
Training loss: 2.312469244003296
Validation loss: 2.527452996982041

Epoch: 5| Step: 9
Training loss: 2.9352645874023438
Validation loss: 2.5282850650049027

Epoch: 5| Step: 10
Training loss: 2.6274912357330322
Validation loss: 2.529401925302321

Epoch: 118| Step: 0
Training loss: 3.492933988571167
Validation loss: 2.5306456524838685

Epoch: 5| Step: 1
Training loss: 2.633776903152466
Validation loss: 2.531426073402487

Epoch: 5| Step: 2
Training loss: 2.3254294395446777
Validation loss: 2.5307824355299755

Epoch: 5| Step: 3
Training loss: 2.597106456756592
Validation loss: 2.5325228629573697

Epoch: 5| Step: 4
Training loss: 1.899976372718811
Validation loss: 2.532669036619125

Epoch: 5| Step: 5
Training loss: 2.2127556800842285
Validation loss: 2.5350451828331075

Epoch: 5| Step: 6
Training loss: 3.0809073448181152
Validation loss: 2.539762373893492

Epoch: 5| Step: 7
Training loss: 3.2183632850646973
Validation loss: 2.5450617010875414

Epoch: 5| Step: 8
Training loss: 2.7040021419525146
Validation loss: 2.5417649643395537

Epoch: 5| Step: 9
Training loss: 2.7773969173431396
Validation loss: 2.5419474365890666

Epoch: 5| Step: 10
Training loss: 2.8462717533111572
Validation loss: 2.536757110267557

Epoch: 119| Step: 0
Training loss: 2.5038702487945557
Validation loss: 2.537848147012854

Epoch: 5| Step: 1
Training loss: 2.46854829788208
Validation loss: 2.535607848116147

Epoch: 5| Step: 2
Training loss: 2.7919065952301025
Validation loss: 2.537320577970115

Epoch: 5| Step: 3
Training loss: 2.1245341300964355
Validation loss: 2.5310882522213842

Epoch: 5| Step: 4
Training loss: 2.5042271614074707
Validation loss: 2.540235932155322

Epoch: 5| Step: 5
Training loss: 3.066350221633911
Validation loss: 2.5364596894992295

Epoch: 5| Step: 6
Training loss: 2.7631900310516357
Validation loss: 2.5353472476364463

Epoch: 5| Step: 7
Training loss: 2.9628663063049316
Validation loss: 2.5365577333716938

Epoch: 5| Step: 8
Training loss: 3.066265344619751
Validation loss: 2.5291197505048526

Epoch: 5| Step: 9
Training loss: 2.585160732269287
Validation loss: 2.526065065014747

Epoch: 5| Step: 10
Training loss: 2.948852062225342
Validation loss: 2.529905529432399

Epoch: 120| Step: 0
Training loss: 2.8694913387298584
Validation loss: 2.5343949743496474

Epoch: 5| Step: 1
Training loss: 2.438901901245117
Validation loss: 2.538593310181813

Epoch: 5| Step: 2
Training loss: 2.490267276763916
Validation loss: 2.540956307482976

Epoch: 5| Step: 3
Training loss: 2.347504138946533
Validation loss: 2.5458213206260436

Epoch: 5| Step: 4
Training loss: 3.0933334827423096
Validation loss: 2.546768175658359

Epoch: 5| Step: 5
Training loss: 1.7247717380523682
Validation loss: 2.5376092105783443

Epoch: 5| Step: 6
Training loss: 3.471343994140625
Validation loss: 2.5330293768195697

Epoch: 5| Step: 7
Training loss: 1.82200026512146
Validation loss: 2.5347782642610612

Epoch: 5| Step: 8
Training loss: 2.6244900226593018
Validation loss: 2.53014019227797

Epoch: 5| Step: 9
Training loss: 3.809828519821167
Validation loss: 2.525680031827701

Epoch: 5| Step: 10
Training loss: 3.113725185394287
Validation loss: 2.533856443179551

Epoch: 121| Step: 0
Training loss: 2.747570514678955
Validation loss: 2.5353639151460383

Epoch: 5| Step: 1
Training loss: 3.283859968185425
Validation loss: 2.5379371643066406

Epoch: 5| Step: 2
Training loss: 2.3662476539611816
Validation loss: 2.5403119543547272

Epoch: 5| Step: 3
Training loss: 2.454514503479004
Validation loss: 2.5421526893492667

Epoch: 5| Step: 4
Training loss: 2.5516107082366943
Validation loss: 2.545038354012274

Epoch: 5| Step: 5
Training loss: 2.812276601791382
Validation loss: 2.53623568114414

Epoch: 5| Step: 6
Training loss: 2.5375568866729736
Validation loss: 2.5285071878023047

Epoch: 5| Step: 7
Training loss: 2.9607062339782715
Validation loss: 2.5276957788775043

Epoch: 5| Step: 8
Training loss: 3.1192708015441895
Validation loss: 2.523260821578323

Epoch: 5| Step: 9
Training loss: 2.3868637084960938
Validation loss: 2.520033677419027

Epoch: 5| Step: 10
Training loss: 2.648953437805176
Validation loss: 2.528601469532136

Epoch: 122| Step: 0
Training loss: 2.341055154800415
Validation loss: 2.526302586319626

Epoch: 5| Step: 1
Training loss: 2.6373775005340576
Validation loss: 2.5246730696770454

Epoch: 5| Step: 2
Training loss: 2.664402723312378
Validation loss: 2.5287739794741393

Epoch: 5| Step: 3
Training loss: 2.774035930633545
Validation loss: 2.5370917615070137

Epoch: 5| Step: 4
Training loss: 3.263969898223877
Validation loss: 2.534175044746809

Epoch: 5| Step: 5
Training loss: 2.6161723136901855
Validation loss: 2.537065100926225

Epoch: 5| Step: 6
Training loss: 2.9055464267730713
Validation loss: 2.538981709429013

Epoch: 5| Step: 7
Training loss: 3.1335532665252686
Validation loss: 2.536763280950567

Epoch: 5| Step: 8
Training loss: 2.325573444366455
Validation loss: 2.534012871403848

Epoch: 5| Step: 9
Training loss: 2.782975673675537
Validation loss: 2.526172355938983

Epoch: 5| Step: 10
Training loss: 2.2043001651763916
Validation loss: 2.5236658127077165

Epoch: 123| Step: 0
Training loss: 3.0062191486358643
Validation loss: 2.521620153098978

Epoch: 5| Step: 1
Training loss: 2.149867534637451
Validation loss: 2.5187148458214215

Epoch: 5| Step: 2
Training loss: 2.983381986618042
Validation loss: 2.5212972830700617

Epoch: 5| Step: 3
Training loss: 2.7879559993743896
Validation loss: 2.528090899990451

Epoch: 5| Step: 4
Training loss: 2.6921894550323486
Validation loss: 2.5416233667763333

Epoch: 5| Step: 5
Training loss: 3.1677374839782715
Validation loss: 2.555830855523386

Epoch: 5| Step: 6
Training loss: 3.178504467010498
Validation loss: 2.5576188513027724

Epoch: 5| Step: 7
Training loss: 2.422396421432495
Validation loss: 2.5461817736266763

Epoch: 5| Step: 8
Training loss: 2.4570603370666504
Validation loss: 2.5248376912968133

Epoch: 5| Step: 9
Training loss: 2.2751941680908203
Validation loss: 2.517328211056289

Epoch: 5| Step: 10
Training loss: 2.6900181770324707
Validation loss: 2.5199080487733245

Epoch: 124| Step: 0
Training loss: 2.8304898738861084
Validation loss: 2.5202458186816146

Epoch: 5| Step: 1
Training loss: 2.9350838661193848
Validation loss: 2.524940483031734

Epoch: 5| Step: 2
Training loss: 2.4942727088928223
Validation loss: 2.523213222462644

Epoch: 5| Step: 3
Training loss: 2.9091084003448486
Validation loss: 2.5262509392153834

Epoch: 5| Step: 4
Training loss: 2.7013936042785645
Validation loss: 2.53204615910848

Epoch: 5| Step: 5
Training loss: 2.8160746097564697
Validation loss: 2.5277541119565248

Epoch: 5| Step: 6
Training loss: 2.117702007293701
Validation loss: 2.523515575675554

Epoch: 5| Step: 7
Training loss: 2.770731210708618
Validation loss: 2.5227591837606123

Epoch: 5| Step: 8
Training loss: 2.8797085285186768
Validation loss: 2.51925483698486

Epoch: 5| Step: 9
Training loss: 2.272097587585449
Validation loss: 2.5192045832193024

Epoch: 5| Step: 10
Training loss: 2.99214243888855
Validation loss: 2.517064207343645

Epoch: 125| Step: 0
Training loss: 2.277916669845581
Validation loss: 2.5158877885469826

Epoch: 5| Step: 1
Training loss: 2.7526583671569824
Validation loss: 2.512762705485026

Epoch: 5| Step: 2
Training loss: 2.232689619064331
Validation loss: 2.5136594721066055

Epoch: 5| Step: 3
Training loss: 2.688242197036743
Validation loss: 2.5148649292607463

Epoch: 5| Step: 4
Training loss: 3.051345109939575
Validation loss: 2.517805735270182

Epoch: 5| Step: 5
Training loss: 2.364279270172119
Validation loss: 2.5205957658829226

Epoch: 5| Step: 6
Training loss: 2.4011127948760986
Validation loss: 2.517988194701492

Epoch: 5| Step: 7
Training loss: 2.4835782051086426
Validation loss: 2.531641642252604

Epoch: 5| Step: 8
Training loss: 2.7681612968444824
Validation loss: 2.53689581091686

Epoch: 5| Step: 9
Training loss: 3.382871150970459
Validation loss: 2.5337073136401433

Epoch: 5| Step: 10
Training loss: 3.3853392601013184
Validation loss: 2.5277943354780956

Epoch: 126| Step: 0
Training loss: 1.8766438961029053
Validation loss: 2.5163285360541394

Epoch: 5| Step: 1
Training loss: 3.0594236850738525
Validation loss: 2.512529880769791

Epoch: 5| Step: 2
Training loss: 3.3609886169433594
Validation loss: 2.513153314590454

Epoch: 5| Step: 3
Training loss: 2.552483081817627
Validation loss: 2.5126636669199955

Epoch: 5| Step: 4
Training loss: 3.2098605632781982
Validation loss: 2.5151641804684877

Epoch: 5| Step: 5
Training loss: 2.0687289237976074
Validation loss: 2.5118165657084477

Epoch: 5| Step: 6
Training loss: 3.2122578620910645
Validation loss: 2.512484876058435

Epoch: 5| Step: 7
Training loss: 2.479449987411499
Validation loss: 2.5148283614907214

Epoch: 5| Step: 8
Training loss: 2.655611991882324
Validation loss: 2.5129686735009633

Epoch: 5| Step: 9
Training loss: 2.828735828399658
Validation loss: 2.516821868958012

Epoch: 5| Step: 10
Training loss: 2.3321008682250977
Validation loss: 2.5181584563306583

Epoch: 127| Step: 0
Training loss: 2.317078113555908
Validation loss: 2.5168539631751274

Epoch: 5| Step: 1
Training loss: 2.448760509490967
Validation loss: 2.515129704629221

Epoch: 5| Step: 2
Training loss: 3.181800127029419
Validation loss: 2.5136952143843456

Epoch: 5| Step: 3
Training loss: 1.884385108947754
Validation loss: 2.513926654733637

Epoch: 5| Step: 4
Training loss: 2.59706449508667
Validation loss: 2.5105156616498063

Epoch: 5| Step: 5
Training loss: 2.679802656173706
Validation loss: 2.5119314116816365

Epoch: 5| Step: 6
Training loss: 2.343036413192749
Validation loss: 2.512400622008949

Epoch: 5| Step: 7
Training loss: 3.140387773513794
Validation loss: 2.514307301531556

Epoch: 5| Step: 8
Training loss: 2.6309385299682617
Validation loss: 2.5181622889734085

Epoch: 5| Step: 9
Training loss: 3.086582660675049
Validation loss: 2.5166584112310924

Epoch: 5| Step: 10
Training loss: 3.437145948410034
Validation loss: 2.5173896769041657

Epoch: 128| Step: 0
Training loss: 2.6916940212249756
Validation loss: 2.5218677136205856

Epoch: 5| Step: 1
Training loss: 2.416262149810791
Validation loss: 2.5211849571556173

Epoch: 5| Step: 2
Training loss: 2.673949956893921
Validation loss: 2.5122367489722466

Epoch: 5| Step: 3
Training loss: 2.1488077640533447
Validation loss: 2.5145805548596125

Epoch: 5| Step: 4
Training loss: 2.7119462490081787
Validation loss: 2.5157099180324103

Epoch: 5| Step: 5
Training loss: 3.3656883239746094
Validation loss: 2.514498608086699

Epoch: 5| Step: 6
Training loss: 2.79352068901062
Validation loss: 2.5145027150389967

Epoch: 5| Step: 7
Training loss: 2.6676344871520996
Validation loss: 2.5144462303448747

Epoch: 5| Step: 8
Training loss: 2.290261745452881
Validation loss: 2.5117193319464244

Epoch: 5| Step: 9
Training loss: 3.145125389099121
Validation loss: 2.510874245756416

Epoch: 5| Step: 10
Training loss: 2.7078216075897217
Validation loss: 2.511956553305349

Epoch: 129| Step: 0
Training loss: 2.8710923194885254
Validation loss: 2.510824582910025

Epoch: 5| Step: 1
Training loss: 2.744370937347412
Validation loss: 2.5117722839437504

Epoch: 5| Step: 2
Training loss: 3.1488471031188965
Validation loss: 2.5098342152052027

Epoch: 5| Step: 3
Training loss: 3.1268630027770996
Validation loss: 2.508180956686697

Epoch: 5| Step: 4
Training loss: 2.7253847122192383
Validation loss: 2.5087641105856946

Epoch: 5| Step: 5
Training loss: 2.7431998252868652
Validation loss: 2.5083196496450775

Epoch: 5| Step: 6
Training loss: 2.3015429973602295
Validation loss: 2.51003098487854

Epoch: 5| Step: 7
Training loss: 2.283154010772705
Validation loss: 2.508985132299444

Epoch: 5| Step: 8
Training loss: 2.2326931953430176
Validation loss: 2.5091626400588662

Epoch: 5| Step: 9
Training loss: 2.645376682281494
Validation loss: 2.512530593461888

Epoch: 5| Step: 10
Training loss: 2.7330925464630127
Validation loss: 2.513240632190499

Epoch: 130| Step: 0
Training loss: 2.5197064876556396
Validation loss: 2.507166249777681

Epoch: 5| Step: 1
Training loss: 2.70703387260437
Validation loss: 2.514978608777446

Epoch: 5| Step: 2
Training loss: 2.5921967029571533
Validation loss: 2.524147205455329

Epoch: 5| Step: 3
Training loss: 2.7246029376983643
Validation loss: 2.5201511126692577

Epoch: 5| Step: 4
Training loss: 3.057809591293335
Validation loss: 2.5196254868661203

Epoch: 5| Step: 5
Training loss: 1.9877516031265259
Validation loss: 2.5213006491302163

Epoch: 5| Step: 6
Training loss: 3.037731647491455
Validation loss: 2.5214011464067685

Epoch: 5| Step: 7
Training loss: 2.7732269763946533
Validation loss: 2.52804398023954

Epoch: 5| Step: 8
Training loss: 2.3683571815490723
Validation loss: 2.5246876824286675

Epoch: 5| Step: 9
Training loss: 3.0205025672912598
Validation loss: 2.521290768859207

Epoch: 5| Step: 10
Training loss: 2.7478585243225098
Validation loss: 2.5198653474930794

Epoch: 131| Step: 0
Training loss: 2.9183969497680664
Validation loss: 2.5155305606062695

Epoch: 5| Step: 1
Training loss: 2.7500643730163574
Validation loss: 2.5133419600866174

Epoch: 5| Step: 2
Training loss: 2.973445177078247
Validation loss: 2.520114439789967

Epoch: 5| Step: 3
Training loss: 2.4927027225494385
Validation loss: 2.523364185005106

Epoch: 5| Step: 4
Training loss: 2.435481548309326
Validation loss: 2.5273768978734172

Epoch: 5| Step: 5
Training loss: 2.5557315349578857
Validation loss: 2.52901775862581

Epoch: 5| Step: 6
Training loss: 2.6494994163513184
Validation loss: 2.524650002038607

Epoch: 5| Step: 7
Training loss: 2.282230854034424
Validation loss: 2.5231540100548857

Epoch: 5| Step: 8
Training loss: 2.3649680614471436
Validation loss: 2.521027365038472

Epoch: 5| Step: 9
Training loss: 3.0738842487335205
Validation loss: 2.515456220155121

Epoch: 5| Step: 10
Training loss: 3.1198461055755615
Validation loss: 2.509180312515587

Epoch: 132| Step: 0
Training loss: 2.84405517578125
Validation loss: 2.5114101184311735

Epoch: 5| Step: 1
Training loss: 2.4563984870910645
Validation loss: 2.5112233033744236

Epoch: 5| Step: 2
Training loss: 2.5346930027008057
Validation loss: 2.5127640616509224

Epoch: 5| Step: 3
Training loss: 2.6355998516082764
Validation loss: 2.5152102542179886

Epoch: 5| Step: 4
Training loss: 2.745023250579834
Validation loss: 2.5275109955059585

Epoch: 5| Step: 5
Training loss: 2.7703583240509033
Validation loss: 2.5402278618146013

Epoch: 5| Step: 6
Training loss: 2.2294368743896484
Validation loss: 2.5304683613520798

Epoch: 5| Step: 7
Training loss: 2.361362934112549
Validation loss: 2.5278403733366277

Epoch: 5| Step: 8
Training loss: 3.2046303749084473
Validation loss: 2.5295228086492068

Epoch: 5| Step: 9
Training loss: 3.0399584770202637
Validation loss: 2.5203078152031027

Epoch: 5| Step: 10
Training loss: 2.8590662479400635
Validation loss: 2.5187427254133326

Epoch: 133| Step: 0
Training loss: 3.0693297386169434
Validation loss: 2.5088046686623686

Epoch: 5| Step: 1
Training loss: 2.9875693321228027
Validation loss: 2.49895619064249

Epoch: 5| Step: 2
Training loss: 3.0076701641082764
Validation loss: 2.498872185266146

Epoch: 5| Step: 3
Training loss: 2.4361367225646973
Validation loss: 2.49769845316487

Epoch: 5| Step: 4
Training loss: 2.5854580402374268
Validation loss: 2.501159632077781

Epoch: 5| Step: 5
Training loss: 2.6908531188964844
Validation loss: 2.5053389200600247

Epoch: 5| Step: 6
Training loss: 2.1624746322631836
Validation loss: 2.5096344204359156

Epoch: 5| Step: 7
Training loss: 2.325289249420166
Validation loss: 2.514075707363826

Epoch: 5| Step: 8
Training loss: 3.0578529834747314
Validation loss: 2.5156160836578696

Epoch: 5| Step: 9
Training loss: 2.3103480339050293
Validation loss: 2.5174035974728164

Epoch: 5| Step: 10
Training loss: 3.0890238285064697
Validation loss: 2.5291257930058304

Epoch: 134| Step: 0
Training loss: 2.7805392742156982
Validation loss: 2.5264422021886355

Epoch: 5| Step: 1
Training loss: 2.4853789806365967
Validation loss: 2.516081561324417

Epoch: 5| Step: 2
Training loss: 2.5493922233581543
Validation loss: 2.5098808503920034

Epoch: 5| Step: 3
Training loss: 2.064396858215332
Validation loss: 2.510884404182434

Epoch: 5| Step: 4
Training loss: 2.48811936378479
Validation loss: 2.502623442680605

Epoch: 5| Step: 5
Training loss: 2.9701552391052246
Validation loss: 2.4989478921377533

Epoch: 5| Step: 6
Training loss: 2.6115074157714844
Validation loss: 2.498227383500786

Epoch: 5| Step: 7
Training loss: 2.5185422897338867
Validation loss: 2.4987739824479624

Epoch: 5| Step: 8
Training loss: 3.136796712875366
Validation loss: 2.4999629246291293

Epoch: 5| Step: 9
Training loss: 3.1510729789733887
Validation loss: 2.4976859887441

Epoch: 5| Step: 10
Training loss: 2.817969560623169
Validation loss: 2.4981972222687094

Epoch: 135| Step: 0
Training loss: 2.742889881134033
Validation loss: 2.500198787258517

Epoch: 5| Step: 1
Training loss: 2.930856466293335
Validation loss: 2.497688106311265

Epoch: 5| Step: 2
Training loss: 3.0784518718719482
Validation loss: 2.4979695555984334

Epoch: 5| Step: 3
Training loss: 2.318655490875244
Validation loss: 2.4979833326032086

Epoch: 5| Step: 4
Training loss: 2.333345413208008
Validation loss: 2.499851231933922

Epoch: 5| Step: 5
Training loss: 2.860617160797119
Validation loss: 2.5013090359267367

Epoch: 5| Step: 6
Training loss: 2.7495827674865723
Validation loss: 2.5030788349848923

Epoch: 5| Step: 7
Training loss: 3.2475745677948
Validation loss: 2.5039973489699827

Epoch: 5| Step: 8
Training loss: 2.1785285472869873
Validation loss: 2.500387191772461

Epoch: 5| Step: 9
Training loss: 2.3474504947662354
Validation loss: 2.5087112508794314

Epoch: 5| Step: 10
Training loss: 2.762199878692627
Validation loss: 2.5055406067961004

Epoch: 136| Step: 0
Training loss: 2.4977054595947266
Validation loss: 2.5090932846069336

Epoch: 5| Step: 1
Training loss: 2.73767352104187
Validation loss: 2.505609199564944

Epoch: 5| Step: 2
Training loss: 2.0892980098724365
Validation loss: 2.5063765689890873

Epoch: 5| Step: 3
Training loss: 2.006995677947998
Validation loss: 2.505013870936568

Epoch: 5| Step: 4
Training loss: 2.365273952484131
Validation loss: 2.5028215557016353

Epoch: 5| Step: 5
Training loss: 3.208637237548828
Validation loss: 2.4968258437289985

Epoch: 5| Step: 6
Training loss: 2.393165111541748
Validation loss: 2.499734422212006

Epoch: 5| Step: 7
Training loss: 2.309692144393921
Validation loss: 2.4984907591214744

Epoch: 5| Step: 8
Training loss: 3.135226011276245
Validation loss: 2.499089397409911

Epoch: 5| Step: 9
Training loss: 3.4238505363464355
Validation loss: 2.4967246978513655

Epoch: 5| Step: 10
Training loss: 3.398026943206787
Validation loss: 2.496146371287684

Epoch: 137| Step: 0
Training loss: 2.9812684059143066
Validation loss: 2.4976653565642652

Epoch: 5| Step: 1
Training loss: 2.796868324279785
Validation loss: 2.49481027613404

Epoch: 5| Step: 2
Training loss: 2.6068577766418457
Validation loss: 2.4999864947411323

Epoch: 5| Step: 3
Training loss: 3.0060317516326904
Validation loss: 2.499196478115615

Epoch: 5| Step: 4
Training loss: 2.898284435272217
Validation loss: 2.5011960767930552

Epoch: 5| Step: 5
Training loss: 2.1845266819000244
Validation loss: 2.498849332973521

Epoch: 5| Step: 6
Training loss: 2.326943874359131
Validation loss: 2.4989029310082875

Epoch: 5| Step: 7
Training loss: 2.6928060054779053
Validation loss: 2.49659643378309

Epoch: 5| Step: 8
Training loss: 2.2687785625457764
Validation loss: 2.49380462913103

Epoch: 5| Step: 9
Training loss: 2.9832446575164795
Validation loss: 2.4964133206234185

Epoch: 5| Step: 10
Training loss: 2.819197177886963
Validation loss: 2.498061695406514

Epoch: 138| Step: 0
Training loss: 3.1512134075164795
Validation loss: 2.50098878722037

Epoch: 5| Step: 1
Training loss: 2.48458194732666
Validation loss: 2.5038386211600354

Epoch: 5| Step: 2
Training loss: 3.0158703327178955
Validation loss: 2.5028591027823825

Epoch: 5| Step: 3
Training loss: 2.842444658279419
Validation loss: 2.499588615150862

Epoch: 5| Step: 4
Training loss: 2.3617329597473145
Validation loss: 2.4999883021077802

Epoch: 5| Step: 5
Training loss: 2.6898980140686035
Validation loss: 2.4957641375962125

Epoch: 5| Step: 6
Training loss: 2.747227668762207
Validation loss: 2.48976275485049

Epoch: 5| Step: 7
Training loss: 2.509733200073242
Validation loss: 2.4891727739764797

Epoch: 5| Step: 8
Training loss: 3.2198925018310547
Validation loss: 2.4902875628522647

Epoch: 5| Step: 9
Training loss: 2.714587688446045
Validation loss: 2.4886436154765468

Epoch: 5| Step: 10
Training loss: 1.6039918661117554
Validation loss: 2.4877318823209373

Epoch: 139| Step: 0
Training loss: 2.0084314346313477
Validation loss: 2.4915324539266606

Epoch: 5| Step: 1
Training loss: 3.1273269653320312
Validation loss: 2.4898370581288494

Epoch: 5| Step: 2
Training loss: 2.004120349884033
Validation loss: 2.4916663503134124

Epoch: 5| Step: 3
Training loss: 2.4254703521728516
Validation loss: 2.4958801654077347

Epoch: 5| Step: 4
Training loss: 2.725379467010498
Validation loss: 2.4884909019675305

Epoch: 5| Step: 5
Training loss: 3.108309268951416
Validation loss: 2.495041311428111

Epoch: 5| Step: 6
Training loss: 2.6701369285583496
Validation loss: 2.495867706114246

Epoch: 5| Step: 7
Training loss: 2.8161327838897705
Validation loss: 2.493380346605855

Epoch: 5| Step: 8
Training loss: 2.5657241344451904
Validation loss: 2.49938843839912

Epoch: 5| Step: 9
Training loss: 2.5865182876586914
Validation loss: 2.4957207300329722

Epoch: 5| Step: 10
Training loss: 3.5504825115203857
Validation loss: 2.497933708211427

Epoch: 140| Step: 0
Training loss: 1.9730980396270752
Validation loss: 2.4939233615834224

Epoch: 5| Step: 1
Training loss: 2.701728343963623
Validation loss: 2.4916090273088023

Epoch: 5| Step: 2
Training loss: 2.605225086212158
Validation loss: 2.490588059989355

Epoch: 5| Step: 3
Training loss: 2.9132823944091797
Validation loss: 2.4874700243755052

Epoch: 5| Step: 4
Training loss: 2.223384141921997
Validation loss: 2.486184961052351

Epoch: 5| Step: 5
Training loss: 3.2841877937316895
Validation loss: 2.492454744154407

Epoch: 5| Step: 6
Training loss: 2.253190040588379
Validation loss: 2.48851913790549

Epoch: 5| Step: 7
Training loss: 3.0261967182159424
Validation loss: 2.493327607390701

Epoch: 5| Step: 8
Training loss: 3.072533369064331
Validation loss: 2.4926733868096465

Epoch: 5| Step: 9
Training loss: 3.152761936187744
Validation loss: 2.4965827926512687

Epoch: 5| Step: 10
Training loss: 2.1478638648986816
Validation loss: 2.4972952847839682

Epoch: 141| Step: 0
Training loss: 2.760409116744995
Validation loss: 2.490233746908044

Epoch: 5| Step: 1
Training loss: 1.8140357732772827
Validation loss: 2.485090688992572

Epoch: 5| Step: 2
Training loss: 2.9572482109069824
Validation loss: 2.487941654779578

Epoch: 5| Step: 3
Training loss: 2.2659802436828613
Validation loss: 2.4866852401405253

Epoch: 5| Step: 4
Training loss: 2.6978859901428223
Validation loss: 2.4918055662544827

Epoch: 5| Step: 5
Training loss: 3.131969451904297
Validation loss: 2.491327449839602

Epoch: 5| Step: 6
Training loss: 2.500100612640381
Validation loss: 2.495341177909605

Epoch: 5| Step: 7
Training loss: 3.284870147705078
Validation loss: 2.4962705489127868

Epoch: 5| Step: 8
Training loss: 2.4711437225341797
Validation loss: 2.4981327851613364

Epoch: 5| Step: 9
Training loss: 2.8489105701446533
Validation loss: 2.4915152006251837

Epoch: 5| Step: 10
Training loss: 2.6775569915771484
Validation loss: 2.498511322083012

Epoch: 142| Step: 0
Training loss: 2.9263370037078857
Validation loss: 2.4995608714319046

Epoch: 5| Step: 1
Training loss: 3.1107988357543945
Validation loss: 2.498927313794372

Epoch: 5| Step: 2
Training loss: 2.557532787322998
Validation loss: 2.4996351349738335

Epoch: 5| Step: 3
Training loss: 3.1797261238098145
Validation loss: 2.4977251522002684

Epoch: 5| Step: 4
Training loss: 2.3246805667877197
Validation loss: 2.4900532896800707

Epoch: 5| Step: 5
Training loss: 2.379653215408325
Validation loss: 2.48466323268029

Epoch: 5| Step: 6
Training loss: 2.4372034072875977
Validation loss: 2.4876177439125637

Epoch: 5| Step: 7
Training loss: 2.547991991043091
Validation loss: 2.4828949205337034

Epoch: 5| Step: 8
Training loss: 2.6175193786621094
Validation loss: 2.4859994919069353

Epoch: 5| Step: 9
Training loss: 3.0718867778778076
Validation loss: 2.4859524106466644

Epoch: 5| Step: 10
Training loss: 2.134840250015259
Validation loss: 2.4849555569310344

Epoch: 143| Step: 0
Training loss: 2.7613892555236816
Validation loss: 2.491750547962804

Epoch: 5| Step: 1
Training loss: 2.4617676734924316
Validation loss: 2.49571793822832

Epoch: 5| Step: 2
Training loss: 2.3889870643615723
Validation loss: 2.4916746539454304

Epoch: 5| Step: 3
Training loss: 2.818946361541748
Validation loss: 2.4927982617450017

Epoch: 5| Step: 4
Training loss: 2.67368745803833
Validation loss: 2.493835331291281

Epoch: 5| Step: 5
Training loss: 2.619368314743042
Validation loss: 2.489578100942796

Epoch: 5| Step: 6
Training loss: 2.7539150714874268
Validation loss: 2.4889652498306765

Epoch: 5| Step: 7
Training loss: 3.1681923866271973
Validation loss: 2.490670533590419

Epoch: 5| Step: 8
Training loss: 2.240939140319824
Validation loss: 2.4845827112915697

Epoch: 5| Step: 9
Training loss: 2.5634171962738037
Validation loss: 2.4869215821707122

Epoch: 5| Step: 10
Training loss: 3.0158326625823975
Validation loss: 2.4860558612372285

Epoch: 144| Step: 0
Training loss: 2.5665628910064697
Validation loss: 2.4846469048530824

Epoch: 5| Step: 1
Training loss: 3.0280086994171143
Validation loss: 2.4833200900785384

Epoch: 5| Step: 2
Training loss: 3.0672218799591064
Validation loss: 2.4863440246992212

Epoch: 5| Step: 3
Training loss: 2.018887996673584
Validation loss: 2.484238871964075

Epoch: 5| Step: 4
Training loss: 2.099461317062378
Validation loss: 2.484787587196596

Epoch: 5| Step: 5
Training loss: 3.2633490562438965
Validation loss: 2.4846873155204197

Epoch: 5| Step: 6
Training loss: 2.5503861904144287
Validation loss: 2.4847017180535103

Epoch: 5| Step: 7
Training loss: 2.9112374782562256
Validation loss: 2.4809776224115843

Epoch: 5| Step: 8
Training loss: 2.652242660522461
Validation loss: 2.482250436659782

Epoch: 5| Step: 9
Training loss: 2.6802055835723877
Validation loss: 2.4829491235876597

Epoch: 5| Step: 10
Training loss: 2.6017708778381348
Validation loss: 2.486135813497728

Epoch: 145| Step: 0
Training loss: 2.715320110321045
Validation loss: 2.4938416891200568

Epoch: 5| Step: 1
Training loss: 2.6423933506011963
Validation loss: 2.4916095964370237

Epoch: 5| Step: 2
Training loss: 2.3535938262939453
Validation loss: 2.4927830798651582

Epoch: 5| Step: 3
Training loss: 2.5025622844696045
Validation loss: 2.488499585018363

Epoch: 5| Step: 4
Training loss: 3.2769923210144043
Validation loss: 2.4919831906595538

Epoch: 5| Step: 5
Training loss: 2.5707809925079346
Validation loss: 2.497275585769325

Epoch: 5| Step: 6
Training loss: 2.4198341369628906
Validation loss: 2.491309727391889

Epoch: 5| Step: 7
Training loss: 3.2283225059509277
Validation loss: 2.490295112773936

Epoch: 5| Step: 8
Training loss: 1.869018793106079
Validation loss: 2.4957729206290296

Epoch: 5| Step: 9
Training loss: 2.9758810997009277
Validation loss: 2.49099027469594

Epoch: 5| Step: 10
Training loss: 2.88584303855896
Validation loss: 2.4949489896015455

Epoch: 146| Step: 0
Training loss: 2.5075502395629883
Validation loss: 2.4889992078145347

Epoch: 5| Step: 1
Training loss: 2.7997636795043945
Validation loss: 2.4910339052959154

Epoch: 5| Step: 2
Training loss: 3.2564189434051514
Validation loss: 2.4859418484472458

Epoch: 5| Step: 3
Training loss: 2.0760562419891357
Validation loss: 2.483974410641578

Epoch: 5| Step: 4
Training loss: 2.4822819232940674
Validation loss: 2.4869988913177163

Epoch: 5| Step: 5
Training loss: 2.8008713722229004
Validation loss: 2.484886177124516

Epoch: 5| Step: 6
Training loss: 2.362685441970825
Validation loss: 2.483892281850179

Epoch: 5| Step: 7
Training loss: 3.319366455078125
Validation loss: 2.4855299752245665

Epoch: 5| Step: 8
Training loss: 2.179142713546753
Validation loss: 2.4861690434076453

Epoch: 5| Step: 9
Training loss: 2.6180336475372314
Validation loss: 2.485020047874861

Epoch: 5| Step: 10
Training loss: 2.9348886013031006
Validation loss: 2.4918367939610637

Epoch: 147| Step: 0
Training loss: 2.6108174324035645
Validation loss: 2.4901904290722263

Epoch: 5| Step: 1
Training loss: 2.491760492324829
Validation loss: 2.482342577749683

Epoch: 5| Step: 2
Training loss: 2.744270086288452
Validation loss: 2.4900333291740826

Epoch: 5| Step: 3
Training loss: 3.0818305015563965
Validation loss: 2.4927852743415424

Epoch: 5| Step: 4
Training loss: 2.4081578254699707
Validation loss: 2.5080740246721493

Epoch: 5| Step: 5
Training loss: 3.2691619396209717
Validation loss: 2.514253065150271

Epoch: 5| Step: 6
Training loss: 2.8951165676116943
Validation loss: 2.5170250990057506

Epoch: 5| Step: 7
Training loss: 2.482609272003174
Validation loss: 2.5231037524438675

Epoch: 5| Step: 8
Training loss: 2.523247241973877
Validation loss: 2.5132666121246996

Epoch: 5| Step: 9
Training loss: 2.3995048999786377
Validation loss: 2.51279955269188

Epoch: 5| Step: 10
Training loss: 2.573809862136841
Validation loss: 2.5121617676109396

Epoch: 148| Step: 0
Training loss: 3.180845260620117
Validation loss: 2.4907440370129

Epoch: 5| Step: 1
Training loss: 2.5717077255249023
Validation loss: 2.4873733289780153

Epoch: 5| Step: 2
Training loss: 2.7453644275665283
Validation loss: 2.489785567406685

Epoch: 5| Step: 3
Training loss: 2.307219982147217
Validation loss: 2.494144367915328

Epoch: 5| Step: 4
Training loss: 2.698029041290283
Validation loss: 2.5049307782162904

Epoch: 5| Step: 5
Training loss: 2.241825819015503
Validation loss: 2.51543854641658

Epoch: 5| Step: 6
Training loss: 2.5101513862609863
Validation loss: 2.5033400776565715

Epoch: 5| Step: 7
Training loss: 3.0484321117401123
Validation loss: 2.4981378611697944

Epoch: 5| Step: 8
Training loss: 2.613974094390869
Validation loss: 2.4882776083484774

Epoch: 5| Step: 9
Training loss: 2.787166118621826
Validation loss: 2.4839084430407454

Epoch: 5| Step: 10
Training loss: 2.7100539207458496
Validation loss: 2.479785942262219

Epoch: 149| Step: 0
Training loss: 3.305652618408203
Validation loss: 2.478540207750054

Epoch: 5| Step: 1
Training loss: 1.6087570190429688
Validation loss: 2.4764399272139355

Epoch: 5| Step: 2
Training loss: 2.195870876312256
Validation loss: 2.479457255332701

Epoch: 5| Step: 3
Training loss: 2.5382797718048096
Validation loss: 2.4754104075893277

Epoch: 5| Step: 4
Training loss: 3.053562879562378
Validation loss: 2.4739112392548592

Epoch: 5| Step: 5
Training loss: 2.0975394248962402
Validation loss: 2.478188619818739

Epoch: 5| Step: 6
Training loss: 2.929889678955078
Validation loss: 2.475124346312656

Epoch: 5| Step: 7
Training loss: 3.0316333770751953
Validation loss: 2.473697239352811

Epoch: 5| Step: 8
Training loss: 2.6629958152770996
Validation loss: 2.4701076169167795

Epoch: 5| Step: 9
Training loss: 2.947582960128784
Validation loss: 2.4744246339285247

Epoch: 5| Step: 10
Training loss: 3.0661606788635254
Validation loss: 2.4747053705235964

Epoch: 150| Step: 0
Training loss: 3.101078748703003
Validation loss: 2.4718359695967806

Epoch: 5| Step: 1
Training loss: 2.894685983657837
Validation loss: 2.474771597052133

Epoch: 5| Step: 2
Training loss: 2.773160457611084
Validation loss: 2.4754814178712907

Epoch: 5| Step: 3
Training loss: 2.506589889526367
Validation loss: 2.4722608109956146

Epoch: 5| Step: 4
Training loss: 2.8313469886779785
Validation loss: 2.473516453978836

Epoch: 5| Step: 5
Training loss: 2.381234645843506
Validation loss: 2.472306589926443

Epoch: 5| Step: 6
Training loss: 2.606527328491211
Validation loss: 2.474805055126067

Epoch: 5| Step: 7
Training loss: 3.3403594493865967
Validation loss: 2.478494531364851

Epoch: 5| Step: 8
Training loss: 1.9806137084960938
Validation loss: 2.476384439776021

Epoch: 5| Step: 9
Training loss: 2.849053144454956
Validation loss: 2.4767817528017106

Epoch: 5| Step: 10
Training loss: 2.0047199726104736
Validation loss: 2.4772990544637046

Epoch: 151| Step: 0
Training loss: 2.6269874572753906
Validation loss: 2.479620200331493

Epoch: 5| Step: 1
Training loss: 2.897184133529663
Validation loss: 2.4772298233483427

Epoch: 5| Step: 2
Training loss: 2.321077823638916
Validation loss: 2.483563889739334

Epoch: 5| Step: 3
Training loss: 3.108525514602661
Validation loss: 2.4772735385484594

Epoch: 5| Step: 4
Training loss: 2.4532039165496826
Validation loss: 2.478523931195659

Epoch: 5| Step: 5
Training loss: 2.5528571605682373
Validation loss: 2.4734360953812957

Epoch: 5| Step: 6
Training loss: 2.2036900520324707
Validation loss: 2.4782934957934963

Epoch: 5| Step: 7
Training loss: 2.1142404079437256
Validation loss: 2.4783633088552826

Epoch: 5| Step: 8
Training loss: 3.6371092796325684
Validation loss: 2.4841467924015497

Epoch: 5| Step: 9
Training loss: 3.0148539543151855
Validation loss: 2.4826311731851227

Epoch: 5| Step: 10
Training loss: 2.297898292541504
Validation loss: 2.481448358105075

Epoch: 152| Step: 0
Training loss: 2.6437289714813232
Validation loss: 2.4789007402235463

Epoch: 5| Step: 1
Training loss: 1.924277901649475
Validation loss: 2.474947690963745

Epoch: 5| Step: 2
Training loss: 2.194211006164551
Validation loss: 2.4744619118270053

Epoch: 5| Step: 3
Training loss: 3.156370162963867
Validation loss: 2.4764013572405745

Epoch: 5| Step: 4
Training loss: 2.473936080932617
Validation loss: 2.4827563993392454

Epoch: 5| Step: 5
Training loss: 2.7556569576263428
Validation loss: 2.4963482067149174

Epoch: 5| Step: 6
Training loss: 2.9132771492004395
Validation loss: 2.4980312573012484

Epoch: 5| Step: 7
Training loss: 3.2705211639404297
Validation loss: 2.506154865346929

Epoch: 5| Step: 8
Training loss: 2.689069986343384
Validation loss: 2.5047983866865917

Epoch: 5| Step: 9
Training loss: 3.0313122272491455
Validation loss: 2.49979055312372

Epoch: 5| Step: 10
Training loss: 2.149893283843994
Validation loss: 2.490927270663682

Epoch: 153| Step: 0
Training loss: 3.411637783050537
Validation loss: 2.488777365735782

Epoch: 5| Step: 1
Training loss: 2.839873790740967
Validation loss: 2.485354769614435

Epoch: 5| Step: 2
Training loss: 2.4612038135528564
Validation loss: 2.4822226724317

Epoch: 5| Step: 3
Training loss: 2.213632822036743
Validation loss: 2.492752205940985

Epoch: 5| Step: 4
Training loss: 2.457571506500244
Validation loss: 2.485041895220357

Epoch: 5| Step: 5
Training loss: 3.059025764465332
Validation loss: 2.4829565363545574

Epoch: 5| Step: 6
Training loss: 2.3413138389587402
Validation loss: 2.482682292179395

Epoch: 5| Step: 7
Training loss: 2.458935499191284
Validation loss: 2.4801302186904417

Epoch: 5| Step: 8
Training loss: 2.2009623050689697
Validation loss: 2.4784739991670013

Epoch: 5| Step: 9
Training loss: 2.2915377616882324
Validation loss: 2.4829226924527075

Epoch: 5| Step: 10
Training loss: 3.6235945224761963
Validation loss: 2.480864942714732

Epoch: 154| Step: 0
Training loss: 2.6295204162597656
Validation loss: 2.480222855844805

Epoch: 5| Step: 1
Training loss: 2.505718231201172
Validation loss: 2.4813416029817317

Epoch: 5| Step: 2
Training loss: 2.5058939456939697
Validation loss: 2.479133126556232

Epoch: 5| Step: 3
Training loss: 2.6732211112976074
Validation loss: 2.4803435520459245

Epoch: 5| Step: 4
Training loss: 2.2860748767852783
Validation loss: 2.482656084081178

Epoch: 5| Step: 5
Training loss: 2.9211630821228027
Validation loss: 2.490434838879493

Epoch: 5| Step: 6
Training loss: 2.6200594902038574
Validation loss: 2.490484655544322

Epoch: 5| Step: 7
Training loss: 3.022688388824463
Validation loss: 2.4930103799348236

Epoch: 5| Step: 8
Training loss: 2.8452255725860596
Validation loss: 2.4913182822606896

Epoch: 5| Step: 9
Training loss: 2.8864104747772217
Validation loss: 2.4796089844037126

Epoch: 5| Step: 10
Training loss: 2.2975406646728516
Validation loss: 2.478400376535231

Epoch: 155| Step: 0
Training loss: 2.8214218616485596
Validation loss: 2.4768758948131273

Epoch: 5| Step: 1
Training loss: 2.939711332321167
Validation loss: 2.4686262812665714

Epoch: 5| Step: 2
Training loss: 2.4278297424316406
Validation loss: 2.473906809283841

Epoch: 5| Step: 3
Training loss: 2.56462025642395
Validation loss: 2.4695971114661104

Epoch: 5| Step: 4
Training loss: 2.658945322036743
Validation loss: 2.4694645840634584

Epoch: 5| Step: 5
Training loss: 2.5280203819274902
Validation loss: 2.4683279247694117

Epoch: 5| Step: 6
Training loss: 2.921713352203369
Validation loss: 2.46695625910195

Epoch: 5| Step: 7
Training loss: 2.8560166358947754
Validation loss: 2.469236740501978

Epoch: 5| Step: 8
Training loss: 2.0924487113952637
Validation loss: 2.4733101526896157

Epoch: 5| Step: 9
Training loss: 2.237544536590576
Validation loss: 2.46947201349402

Epoch: 5| Step: 10
Training loss: 3.284719944000244
Validation loss: 2.469976097024897

Epoch: 156| Step: 0
Training loss: 2.2233054637908936
Validation loss: 2.4749877657941592

Epoch: 5| Step: 1
Training loss: 3.232252597808838
Validation loss: 2.4729869263146513

Epoch: 5| Step: 2
Training loss: 3.0350749492645264
Validation loss: 2.4789977124942246

Epoch: 5| Step: 3
Training loss: 2.510868549346924
Validation loss: 2.477720137565367

Epoch: 5| Step: 4
Training loss: 3.0584917068481445
Validation loss: 2.4753513797636955

Epoch: 5| Step: 5
Training loss: 3.0759167671203613
Validation loss: 2.477044692603491

Epoch: 5| Step: 6
Training loss: 2.991541624069214
Validation loss: 2.4737254573452856

Epoch: 5| Step: 7
Training loss: 2.579897880554199
Validation loss: 2.471523795076596

Epoch: 5| Step: 8
Training loss: 2.133909225463867
Validation loss: 2.479767435340471

Epoch: 5| Step: 9
Training loss: 2.071509838104248
Validation loss: 2.480626421590005

Epoch: 5| Step: 10
Training loss: 2.1832666397094727
Validation loss: 2.4888866075905423

Epoch: 157| Step: 0
Training loss: 3.01533842086792
Validation loss: 2.4864536869910454

Epoch: 5| Step: 1
Training loss: 2.414954423904419
Validation loss: 2.4780666494882233

Epoch: 5| Step: 2
Training loss: 2.5652854442596436
Validation loss: 2.480086567581341

Epoch: 5| Step: 3
Training loss: 2.8126533031463623
Validation loss: 2.473503899830644

Epoch: 5| Step: 4
Training loss: 2.318265199661255
Validation loss: 2.4727650303994455

Epoch: 5| Step: 5
Training loss: 2.7423388957977295
Validation loss: 2.4682592807277555

Epoch: 5| Step: 6
Training loss: 2.5632596015930176
Validation loss: 2.476499967677619

Epoch: 5| Step: 7
Training loss: 2.98801851272583
Validation loss: 2.4746847485983245

Epoch: 5| Step: 8
Training loss: 2.81931734085083
Validation loss: 2.4666401570843113

Epoch: 5| Step: 9
Training loss: 2.188189744949341
Validation loss: 2.4689288421343734

Epoch: 5| Step: 10
Training loss: 2.8159875869750977
Validation loss: 2.4766917485062794

Epoch: 158| Step: 0
Training loss: 2.880523204803467
Validation loss: 2.4798317314476095

Epoch: 5| Step: 1
Training loss: 3.288835048675537
Validation loss: 2.473936652624479

Epoch: 5| Step: 2
Training loss: 3.2314934730529785
Validation loss: 2.4679615574498333

Epoch: 5| Step: 3
Training loss: 2.4242987632751465
Validation loss: 2.4720102074325725

Epoch: 5| Step: 4
Training loss: 2.743816614151001
Validation loss: 2.47749489097185

Epoch: 5| Step: 5
Training loss: 2.101978063583374
Validation loss: 2.4852337939764864

Epoch: 5| Step: 6
Training loss: 2.7469420433044434
Validation loss: 2.478675752557734

Epoch: 5| Step: 7
Training loss: 2.7340469360351562
Validation loss: 2.4818353755499727

Epoch: 5| Step: 8
Training loss: 2.5798726081848145
Validation loss: 2.475380830867316

Epoch: 5| Step: 9
Training loss: 2.2805280685424805
Validation loss: 2.472105961973949

Epoch: 5| Step: 10
Training loss: 2.1153321266174316
Validation loss: 2.468954710550206

Epoch: 159| Step: 0
Training loss: 3.0103919506073
Validation loss: 2.464393749031969

Epoch: 5| Step: 1
Training loss: 2.2126708030700684
Validation loss: 2.4709737198327177

Epoch: 5| Step: 2
Training loss: 2.8475239276885986
Validation loss: 2.4744281076615855

Epoch: 5| Step: 3
Training loss: 2.5472774505615234
Validation loss: 2.479474934198523

Epoch: 5| Step: 4
Training loss: 2.8199126720428467
Validation loss: 2.479469612080564

Epoch: 5| Step: 5
Training loss: 2.431776523590088
Validation loss: 2.4860577852495256

Epoch: 5| Step: 6
Training loss: 2.679288387298584
Validation loss: 2.471471958262946

Epoch: 5| Step: 7
Training loss: 2.9223437309265137
Validation loss: 2.46970046207469

Epoch: 5| Step: 8
Training loss: 2.425865888595581
Validation loss: 2.464791544022099

Epoch: 5| Step: 9
Training loss: 2.906985282897949
Validation loss: 2.459049019762265

Epoch: 5| Step: 10
Training loss: 2.392425775527954
Validation loss: 2.4551658553461873

Epoch: 160| Step: 0
Training loss: 2.9747228622436523
Validation loss: 2.456680461924563

Epoch: 5| Step: 1
Training loss: 2.538133144378662
Validation loss: 2.4595508626712266

Epoch: 5| Step: 2
Training loss: 2.6236860752105713
Validation loss: 2.462614628576463

Epoch: 5| Step: 3
Training loss: 2.757026195526123
Validation loss: 2.4641184653005292

Epoch: 5| Step: 4
Training loss: 2.6423497200012207
Validation loss: 2.463041892615698

Epoch: 5| Step: 5
Training loss: 3.5467212200164795
Validation loss: 2.468498019761937

Epoch: 5| Step: 6
Training loss: 3.133183002471924
Validation loss: 2.4634281076410764

Epoch: 5| Step: 7
Training loss: 1.7151588201522827
Validation loss: 2.4684892008381505

Epoch: 5| Step: 8
Training loss: 2.67287015914917
Validation loss: 2.4797535404082267

Epoch: 5| Step: 9
Training loss: 2.408862829208374
Validation loss: 2.489667812983195

Epoch: 5| Step: 10
Training loss: 2.1053807735443115
Validation loss: 2.487104321038851

Epoch: 161| Step: 0
Training loss: 2.758967638015747
Validation loss: 2.500997571535008

Epoch: 5| Step: 1
Training loss: 2.324425458908081
Validation loss: 2.495674151246266

Epoch: 5| Step: 2
Training loss: 2.4493701457977295
Validation loss: 2.499646350901614

Epoch: 5| Step: 3
Training loss: 2.4770312309265137
Validation loss: 2.500470351147395

Epoch: 5| Step: 4
Training loss: 2.892826557159424
Validation loss: 2.4966124667916247

Epoch: 5| Step: 5
Training loss: 2.8371691703796387
Validation loss: 2.48808987422656

Epoch: 5| Step: 6
Training loss: 2.907078981399536
Validation loss: 2.4753994377710486

Epoch: 5| Step: 7
Training loss: 2.5226173400878906
Validation loss: 2.4682846171881563

Epoch: 5| Step: 8
Training loss: 2.5197970867156982
Validation loss: 2.4623438158342914

Epoch: 5| Step: 9
Training loss: 2.609205484390259
Validation loss: 2.4587322486344205

Epoch: 5| Step: 10
Training loss: 2.9131877422332764
Validation loss: 2.455547232781687

Epoch: 162| Step: 0
Training loss: 3.627220869064331
Validation loss: 2.452425459379791

Epoch: 5| Step: 1
Training loss: 2.179422378540039
Validation loss: 2.449258614611882

Epoch: 5| Step: 2
Training loss: 2.5885534286499023
Validation loss: 2.4528631394909275

Epoch: 5| Step: 3
Training loss: 2.655386209487915
Validation loss: 2.4505453519923712

Epoch: 5| Step: 4
Training loss: 2.310269832611084
Validation loss: 2.4512766971383044

Epoch: 5| Step: 5
Training loss: 1.98809814453125
Validation loss: 2.4503078409420547

Epoch: 5| Step: 6
Training loss: 2.3100152015686035
Validation loss: 2.4509232941494195

Epoch: 5| Step: 7
Training loss: 3.4620418548583984
Validation loss: 2.447370562502133

Epoch: 5| Step: 8
Training loss: 2.797131299972534
Validation loss: 2.4510028029000885

Epoch: 5| Step: 9
Training loss: 2.9136414527893066
Validation loss: 2.4560640012064288

Epoch: 5| Step: 10
Training loss: 2.2911434173583984
Validation loss: 2.4611799204221336

Epoch: 163| Step: 0
Training loss: 2.297550678253174
Validation loss: 2.4671770039425103

Epoch: 5| Step: 1
Training loss: 3.0058436393737793
Validation loss: 2.4810948576978458

Epoch: 5| Step: 2
Training loss: 2.4357190132141113
Validation loss: 2.484079360961914

Epoch: 5| Step: 3
Training loss: 2.6316933631896973
Validation loss: 2.482931003775648

Epoch: 5| Step: 4
Training loss: 2.8182239532470703
Validation loss: 2.471577964803224

Epoch: 5| Step: 5
Training loss: 2.4896347522735596
Validation loss: 2.465572613541798

Epoch: 5| Step: 6
Training loss: 2.7812914848327637
Validation loss: 2.4559636526210333

Epoch: 5| Step: 7
Training loss: 3.138807773590088
Validation loss: 2.4557655447272846

Epoch: 5| Step: 8
Training loss: 2.2419304847717285
Validation loss: 2.457825101831908

Epoch: 5| Step: 9
Training loss: 3.1639559268951416
Validation loss: 2.4591773248487905

Epoch: 5| Step: 10
Training loss: 2.1500120162963867
Validation loss: 2.4591813882191977

Epoch: 164| Step: 0
Training loss: 2.1448240280151367
Validation loss: 2.4666515268305296

Epoch: 5| Step: 1
Training loss: 2.0918922424316406
Validation loss: 2.4613366870469946

Epoch: 5| Step: 2
Training loss: 2.7870442867279053
Validation loss: 2.464598435227589

Epoch: 5| Step: 3
Training loss: 2.7361183166503906
Validation loss: 2.460802972957652

Epoch: 5| Step: 4
Training loss: 2.289309501647949
Validation loss: 2.464084807262626

Epoch: 5| Step: 5
Training loss: 3.1451756954193115
Validation loss: 2.4628664396142446

Epoch: 5| Step: 6
Training loss: 3.326662540435791
Validation loss: 2.455450009274226

Epoch: 5| Step: 7
Training loss: 2.5360209941864014
Validation loss: 2.455835647480462

Epoch: 5| Step: 8
Training loss: 2.9071459770202637
Validation loss: 2.4622752820291827

Epoch: 5| Step: 9
Training loss: 2.8620073795318604
Validation loss: 2.458802866679366

Epoch: 5| Step: 10
Training loss: 2.3320887088775635
Validation loss: 2.4648005475280104

Epoch: 165| Step: 0
Training loss: 2.3867971897125244
Validation loss: 2.472107859068019

Epoch: 5| Step: 1
Training loss: 2.564124584197998
Validation loss: 2.466184252051897

Epoch: 5| Step: 2
Training loss: 2.8553173542022705
Validation loss: 2.4729142778663227

Epoch: 5| Step: 3
Training loss: 1.8021522760391235
Validation loss: 2.4793435271068285

Epoch: 5| Step: 4
Training loss: 3.1883835792541504
Validation loss: 2.4825005249310563

Epoch: 5| Step: 5
Training loss: 2.930603504180908
Validation loss: 2.4932687667108353

Epoch: 5| Step: 6
Training loss: 1.923204779624939
Validation loss: 2.4794238152042514

Epoch: 5| Step: 7
Training loss: 2.4255242347717285
Validation loss: 2.4800480334989485

Epoch: 5| Step: 8
Training loss: 3.352954864501953
Validation loss: 2.469762150959302

Epoch: 5| Step: 9
Training loss: 2.75169038772583
Validation loss: 2.4685934769209994

Epoch: 5| Step: 10
Training loss: 3.0883164405822754
Validation loss: 2.4661250652805453

Epoch: 166| Step: 0
Training loss: 2.6347460746765137
Validation loss: 2.466600238635976

Epoch: 5| Step: 1
Training loss: 2.616682767868042
Validation loss: 2.461548500163581

Epoch: 5| Step: 2
Training loss: 3.1031877994537354
Validation loss: 2.459188792013353

Epoch: 5| Step: 3
Training loss: 2.3035712242126465
Validation loss: 2.449917877874067

Epoch: 5| Step: 4
Training loss: 2.9859042167663574
Validation loss: 2.4551517040498796

Epoch: 5| Step: 5
Training loss: 2.0652847290039062
Validation loss: 2.451354601049936

Epoch: 5| Step: 6
Training loss: 2.717930555343628
Validation loss: 2.4466670841299076

Epoch: 5| Step: 7
Training loss: 2.7569022178649902
Validation loss: 2.4506233840860348

Epoch: 5| Step: 8
Training loss: 2.6987929344177246
Validation loss: 2.4547107988788235

Epoch: 5| Step: 9
Training loss: 2.7514777183532715
Validation loss: 2.453848954169981

Epoch: 5| Step: 10
Training loss: 2.4187538623809814
Validation loss: 2.4594767119294856

Epoch: 167| Step: 0
Training loss: 3.505934476852417
Validation loss: 2.472671006315498

Epoch: 5| Step: 1
Training loss: 2.162731885910034
Validation loss: 2.4684718834456576

Epoch: 5| Step: 2
Training loss: 2.8075873851776123
Validation loss: 2.4596440330628426

Epoch: 5| Step: 3
Training loss: 2.186917781829834
Validation loss: 2.456908948959843

Epoch: 5| Step: 4
Training loss: 3.4681403636932373
Validation loss: 2.4543996882695023

Epoch: 5| Step: 5
Training loss: 2.460883855819702
Validation loss: 2.4576505948138494

Epoch: 5| Step: 6
Training loss: 3.343489170074463
Validation loss: 2.4660135571674635

Epoch: 5| Step: 7
Training loss: 1.8692805767059326
Validation loss: 2.4722511973432315

Epoch: 5| Step: 8
Training loss: 2.8157103061676025
Validation loss: 2.480072429103236

Epoch: 5| Step: 9
Training loss: 2.542210817337036
Validation loss: 2.4781227880908596

Epoch: 5| Step: 10
Training loss: 1.9257563352584839
Validation loss: 2.495075794958299

Epoch: 168| Step: 0
Training loss: 3.108396053314209
Validation loss: 2.4972705200154293

Epoch: 5| Step: 1
Training loss: 2.2277493476867676
Validation loss: 2.4972389731355893

Epoch: 5| Step: 2
Training loss: 2.5347535610198975
Validation loss: 2.494128409252372

Epoch: 5| Step: 3
Training loss: 2.3957226276397705
Validation loss: 2.49703388573021

Epoch: 5| Step: 4
Training loss: 2.515331268310547
Validation loss: 2.507805896061723

Epoch: 5| Step: 5
Training loss: 2.8187808990478516
Validation loss: 2.5095022083610616

Epoch: 5| Step: 6
Training loss: 2.3582069873809814
Validation loss: 2.510382524100683

Epoch: 5| Step: 7
Training loss: 3.018958330154419
Validation loss: 2.5074628860719743

Epoch: 5| Step: 8
Training loss: 2.8061349391937256
Validation loss: 2.5041630755188646

Epoch: 5| Step: 9
Training loss: 2.7753045558929443
Validation loss: 2.492644561234341

Epoch: 5| Step: 10
Training loss: 2.8918893337249756
Validation loss: 2.4954973061879477

Epoch: 169| Step: 0
Training loss: 2.6667065620422363
Validation loss: 2.4959381139406593

Epoch: 5| Step: 1
Training loss: 3.13795804977417
Validation loss: 2.4979785450043215

Epoch: 5| Step: 2
Training loss: 2.578615665435791
Validation loss: 2.492080380839686

Epoch: 5| Step: 3
Training loss: 3.7620368003845215
Validation loss: 2.486096146286175

Epoch: 5| Step: 4
Training loss: 2.885840892791748
Validation loss: 2.4713624446622786

Epoch: 5| Step: 5
Training loss: 2.7350900173187256
Validation loss: 2.4655084661258164

Epoch: 5| Step: 6
Training loss: 2.640993595123291
Validation loss: 2.4664478866002892

Epoch: 5| Step: 7
Training loss: 2.2082653045654297
Validation loss: 2.4597916705634004

Epoch: 5| Step: 8
Training loss: 2.3270678520202637
Validation loss: 2.4629488555333947

Epoch: 5| Step: 9
Training loss: 2.0868473052978516
Validation loss: 2.469109847981443

Epoch: 5| Step: 10
Training loss: 2.2359085083007812
Validation loss: 2.471522900366014

Epoch: 170| Step: 0
Training loss: 2.880114793777466
Validation loss: 2.482198102499849

Epoch: 5| Step: 1
Training loss: 2.0884227752685547
Validation loss: 2.4811278273982387

Epoch: 5| Step: 2
Training loss: 2.4628307819366455
Validation loss: 2.47884694222481

Epoch: 5| Step: 3
Training loss: 1.9597762823104858
Validation loss: 2.4609953716237056

Epoch: 5| Step: 4
Training loss: 3.158872127532959
Validation loss: 2.4570714504488054

Epoch: 5| Step: 5
Training loss: 3.5000903606414795
Validation loss: 2.4537642002105713

Epoch: 5| Step: 6
Training loss: 3.0971124172210693
Validation loss: 2.4450184760555143

Epoch: 5| Step: 7
Training loss: 2.1862099170684814
Validation loss: 2.439470750029369

Epoch: 5| Step: 8
Training loss: 2.6654739379882812
Validation loss: 2.447866960238385

Epoch: 5| Step: 9
Training loss: 2.287153720855713
Validation loss: 2.4505315467875493

Epoch: 5| Step: 10
Training loss: 2.9722588062286377
Validation loss: 2.4532187882290093

Epoch: 171| Step: 0
Training loss: 2.4332752227783203
Validation loss: 2.4604431198489283

Epoch: 5| Step: 1
Training loss: 2.593356132507324
Validation loss: 2.4663430029346096

Epoch: 5| Step: 2
Training loss: 3.262816905975342
Validation loss: 2.4692740722369124

Epoch: 5| Step: 3
Training loss: 2.664828062057495
Validation loss: 2.4749798518355175

Epoch: 5| Step: 4
Training loss: 2.5659308433532715
Validation loss: 2.4643705916661087

Epoch: 5| Step: 5
Training loss: 2.6210837364196777
Validation loss: 2.455026106167865

Epoch: 5| Step: 6
Training loss: 3.2541236877441406
Validation loss: 2.4522625143809984

Epoch: 5| Step: 7
Training loss: 2.429205894470215
Validation loss: 2.457580868915845

Epoch: 5| Step: 8
Training loss: 1.8507152795791626
Validation loss: 2.450473472636233

Epoch: 5| Step: 9
Training loss: 2.6899311542510986
Validation loss: 2.457807916466908

Epoch: 5| Step: 10
Training loss: 2.759228229522705
Validation loss: 2.4571560544352375

Epoch: 172| Step: 0
Training loss: 2.7080962657928467
Validation loss: 2.4663552186822377

Epoch: 5| Step: 1
Training loss: 2.275784730911255
Validation loss: 2.4623362966763076

Epoch: 5| Step: 2
Training loss: 2.6878585815429688
Validation loss: 2.4615746800617506

Epoch: 5| Step: 3
Training loss: 2.806236505508423
Validation loss: 2.4583418048838133

Epoch: 5| Step: 4
Training loss: 3.0320217609405518
Validation loss: 2.457127740306239

Epoch: 5| Step: 5
Training loss: 3.0509331226348877
Validation loss: 2.4574143963475383

Epoch: 5| Step: 6
Training loss: 1.7830009460449219
Validation loss: 2.4612663689480034

Epoch: 5| Step: 7
Training loss: 3.0401692390441895
Validation loss: 2.459756294886271

Epoch: 5| Step: 8
Training loss: 2.7001800537109375
Validation loss: 2.4580071946626068

Epoch: 5| Step: 9
Training loss: 2.149341106414795
Validation loss: 2.4540288576515774

Epoch: 5| Step: 10
Training loss: 2.905073404312134
Validation loss: 2.4490156583888556

Epoch: 173| Step: 0
Training loss: 2.6185200214385986
Validation loss: 2.450405974541941

Epoch: 5| Step: 1
Training loss: 2.8339600563049316
Validation loss: 2.4458116151953257

Epoch: 5| Step: 2
Training loss: 2.8122589588165283
Validation loss: 2.448506934668428

Epoch: 5| Step: 3
Training loss: 2.5375475883483887
Validation loss: 2.449174340053271

Epoch: 5| Step: 4
Training loss: 2.1597938537597656
Validation loss: 2.4482124825959564

Epoch: 5| Step: 5
Training loss: 2.876572847366333
Validation loss: 2.447468747374832

Epoch: 5| Step: 6
Training loss: 2.5651538372039795
Validation loss: 2.4501503231704875

Epoch: 5| Step: 7
Training loss: 2.2807419300079346
Validation loss: 2.4499243485030306

Epoch: 5| Step: 8
Training loss: 2.9525198936462402
Validation loss: 2.4490068497196322

Epoch: 5| Step: 9
Training loss: 2.73650860786438
Validation loss: 2.457444542197771

Epoch: 5| Step: 10
Training loss: 2.6844241619110107
Validation loss: 2.4518429720273582

Epoch: 174| Step: 0
Training loss: 3.4694793224334717
Validation loss: 2.45259343424151

Epoch: 5| Step: 1
Training loss: 2.463088274002075
Validation loss: 2.4533699738082064

Epoch: 5| Step: 2
Training loss: 3.193828582763672
Validation loss: 2.452528656169932

Epoch: 5| Step: 3
Training loss: 2.548711061477661
Validation loss: 2.4459858684129614

Epoch: 5| Step: 4
Training loss: 2.51011323928833
Validation loss: 2.444281147372338

Epoch: 5| Step: 5
Training loss: 2.6425833702087402
Validation loss: 2.4447258595497376

Epoch: 5| Step: 6
Training loss: 1.7556337118148804
Validation loss: 2.438893101548636

Epoch: 5| Step: 7
Training loss: 2.365846872329712
Validation loss: 2.4350801257676977

Epoch: 5| Step: 8
Training loss: 2.606224775314331
Validation loss: 2.4409503962403987

Epoch: 5| Step: 9
Training loss: 2.670786142349243
Validation loss: 2.432232372222408

Epoch: 5| Step: 10
Training loss: 2.873525857925415
Validation loss: 2.4349717504234722

Epoch: 175| Step: 0
Training loss: 2.831747531890869
Validation loss: 2.443203792777113

Epoch: 5| Step: 1
Training loss: 2.194136142730713
Validation loss: 2.444651496025824

Epoch: 5| Step: 2
Training loss: 3.2107932567596436
Validation loss: 2.445885201936127

Epoch: 5| Step: 3
Training loss: 2.187992811203003
Validation loss: 2.4449474452644266

Epoch: 5| Step: 4
Training loss: 2.3470616340637207
Validation loss: 2.442246596018473

Epoch: 5| Step: 5
Training loss: 2.8915772438049316
Validation loss: 2.4555818060392975

Epoch: 5| Step: 6
Training loss: 2.6387500762939453
Validation loss: 2.4564340037684285

Epoch: 5| Step: 7
Training loss: 3.026923418045044
Validation loss: 2.471181914370547

Epoch: 5| Step: 8
Training loss: 2.85548734664917
Validation loss: 2.4654777511473625

Epoch: 5| Step: 9
Training loss: 2.7744336128234863
Validation loss: 2.4783279716327624

Epoch: 5| Step: 10
Training loss: 1.9571279287338257
Validation loss: 2.4797928025645595

Epoch: 176| Step: 0
Training loss: 2.3180994987487793
Validation loss: 2.475770604225897

Epoch: 5| Step: 1
Training loss: 1.7638556957244873
Validation loss: 2.479738494401337

Epoch: 5| Step: 2
Training loss: 3.553691864013672
Validation loss: 2.4876057742744364

Epoch: 5| Step: 3
Training loss: 2.9935848712921143
Validation loss: 2.4849105137650684

Epoch: 5| Step: 4
Training loss: 2.9436497688293457
Validation loss: 2.482747652197397

Epoch: 5| Step: 5
Training loss: 2.40647554397583
Validation loss: 2.4722748623099378

Epoch: 5| Step: 6
Training loss: 2.977517604827881
Validation loss: 2.46135691929889

Epoch: 5| Step: 7
Training loss: 2.4964075088500977
Validation loss: 2.4440127393250823

Epoch: 5| Step: 8
Training loss: 2.372570514678955
Validation loss: 2.440509791015297

Epoch: 5| Step: 9
Training loss: 2.2959647178649902
Validation loss: 2.439116924039779

Epoch: 5| Step: 10
Training loss: 3.0308845043182373
Validation loss: 2.4358011138054634

Epoch: 177| Step: 0
Training loss: 2.8345885276794434
Validation loss: 2.438613817255984

Epoch: 5| Step: 1
Training loss: 2.5685794353485107
Validation loss: 2.4353523741486254

Epoch: 5| Step: 2
Training loss: 2.7225186824798584
Validation loss: 2.4351975994725383

Epoch: 5| Step: 3
Training loss: 2.4538440704345703
Validation loss: 2.433771638460057

Epoch: 5| Step: 4
Training loss: 2.787146806716919
Validation loss: 2.4367261958378617

Epoch: 5| Step: 5
Training loss: 3.3196442127227783
Validation loss: 2.4303760964383363

Epoch: 5| Step: 6
Training loss: 3.0830554962158203
Validation loss: 2.432319543694937

Epoch: 5| Step: 7
Training loss: 2.2110891342163086
Validation loss: 2.4329904087128176

Epoch: 5| Step: 8
Training loss: 2.395226240158081
Validation loss: 2.430409180220737

Epoch: 5| Step: 9
Training loss: 1.9967050552368164
Validation loss: 2.43095729684317

Epoch: 5| Step: 10
Training loss: 2.783266067504883
Validation loss: 2.4378203576610935

Epoch: 178| Step: 0
Training loss: 1.9392898082733154
Validation loss: 2.443288428809053

Epoch: 5| Step: 1
Training loss: 3.0291261672973633
Validation loss: 2.4604665233242895

Epoch: 5| Step: 2
Training loss: 3.317673921585083
Validation loss: 2.4717898394471858

Epoch: 5| Step: 3
Training loss: 2.4397926330566406
Validation loss: 2.454484401210662

Epoch: 5| Step: 4
Training loss: 2.5855655670166016
Validation loss: 2.4450338860993743

Epoch: 5| Step: 5
Training loss: 2.695472240447998
Validation loss: 2.442988100872245

Epoch: 5| Step: 6
Training loss: 1.9619483947753906
Validation loss: 2.436389656477077

Epoch: 5| Step: 7
Training loss: 2.6943423748016357
Validation loss: 2.442562592926846

Epoch: 5| Step: 8
Training loss: 3.2057042121887207
Validation loss: 2.444147512476931

Epoch: 5| Step: 9
Training loss: 2.123791456222534
Validation loss: 2.4520925757705525

Epoch: 5| Step: 10
Training loss: 3.2416934967041016
Validation loss: 2.44355349386892

Epoch: 179| Step: 0
Training loss: 1.8869578838348389
Validation loss: 2.4418661338026806

Epoch: 5| Step: 1
Training loss: 2.8105835914611816
Validation loss: 2.433409244783463

Epoch: 5| Step: 2
Training loss: 2.729555130004883
Validation loss: 2.4261772786417315

Epoch: 5| Step: 3
Training loss: 2.465754747390747
Validation loss: 2.428480153442711

Epoch: 5| Step: 4
Training loss: 3.0137436389923096
Validation loss: 2.4322256683021464

Epoch: 5| Step: 5
Training loss: 2.9974205493927
Validation loss: 2.4247264169877574

Epoch: 5| Step: 6
Training loss: 2.8106772899627686
Validation loss: 2.4285304648901826

Epoch: 5| Step: 7
Training loss: 2.624605178833008
Validation loss: 2.4345015633490776

Epoch: 5| Step: 8
Training loss: 2.4772751331329346
Validation loss: 2.438787216781288

Epoch: 5| Step: 9
Training loss: 2.605931043624878
Validation loss: 2.44636946083397

Epoch: 5| Step: 10
Training loss: 2.607172966003418
Validation loss: 2.4571561146807928

Epoch: 180| Step: 0
Training loss: 2.7909114360809326
Validation loss: 2.457999083303636

Epoch: 5| Step: 1
Training loss: 3.0365355014801025
Validation loss: 2.4658608846766974

Epoch: 5| Step: 2
Training loss: 2.3489723205566406
Validation loss: 2.4609592063452608

Epoch: 5| Step: 3
Training loss: 2.9829580783843994
Validation loss: 2.465138858364474

Epoch: 5| Step: 4
Training loss: 2.3856277465820312
Validation loss: 2.450982662939256

Epoch: 5| Step: 5
Training loss: 2.554537296295166
Validation loss: 2.4516383012135825

Epoch: 5| Step: 6
Training loss: 2.136577844619751
Validation loss: 2.445346483620264

Epoch: 5| Step: 7
Training loss: 2.7202563285827637
Validation loss: 2.438198640782346

Epoch: 5| Step: 8
Training loss: 2.1647942066192627
Validation loss: 2.4385214108292774

Epoch: 5| Step: 9
Training loss: 2.8597233295440674
Validation loss: 2.4374761530148086

Epoch: 5| Step: 10
Training loss: 3.0187439918518066
Validation loss: 2.437155000625118

Epoch: 181| Step: 0
Training loss: 2.660037040710449
Validation loss: 2.43834186625737

Epoch: 5| Step: 1
Training loss: 2.8595266342163086
Validation loss: 2.4479534497825046

Epoch: 5| Step: 2
Training loss: 2.6238038539886475
Validation loss: 2.4458655183033278

Epoch: 5| Step: 3
Training loss: 2.2894492149353027
Validation loss: 2.4493844227124284

Epoch: 5| Step: 4
Training loss: 3.099238157272339
Validation loss: 2.4526323759427635

Epoch: 5| Step: 5
Training loss: 2.5006251335144043
Validation loss: 2.4479166897394324

Epoch: 5| Step: 6
Training loss: 2.3431613445281982
Validation loss: 2.4457342778482745

Epoch: 5| Step: 7
Training loss: 2.421497344970703
Validation loss: 2.4462456472458376

Epoch: 5| Step: 8
Training loss: 3.1819846630096436
Validation loss: 2.4449077498528267

Epoch: 5| Step: 9
Training loss: 2.6456820964813232
Validation loss: 2.446495045897781

Epoch: 5| Step: 10
Training loss: 2.2167141437530518
Validation loss: 2.438845967733732

Epoch: 182| Step: 0
Training loss: 3.0822927951812744
Validation loss: 2.4466058823370163

Epoch: 5| Step: 1
Training loss: 2.439528226852417
Validation loss: 2.441362509163477

Epoch: 5| Step: 2
Training loss: 2.3048954010009766
Validation loss: 2.444164304323094

Epoch: 5| Step: 3
Training loss: 2.625971794128418
Validation loss: 2.444196331885553

Epoch: 5| Step: 4
Training loss: 2.9755656719207764
Validation loss: 2.4494697509273404

Epoch: 5| Step: 5
Training loss: 2.876890182495117
Validation loss: 2.445562375489102

Epoch: 5| Step: 6
Training loss: 2.6809208393096924
Validation loss: 2.4434964092828895

Epoch: 5| Step: 7
Training loss: 2.5745973587036133
Validation loss: 2.4477899125827256

Epoch: 5| Step: 8
Training loss: 2.371098518371582
Validation loss: 2.4497292836507163

Epoch: 5| Step: 9
Training loss: 2.5632855892181396
Validation loss: 2.44194822413947

Epoch: 5| Step: 10
Training loss: 2.3862428665161133
Validation loss: 2.4406398957775486

Epoch: 183| Step: 0
Training loss: 2.3004696369171143
Validation loss: 2.4424401457591722

Epoch: 5| Step: 1
Training loss: 3.064852476119995
Validation loss: 2.4308825641550045

Epoch: 5| Step: 2
Training loss: 2.3354814052581787
Validation loss: 2.4297644246009087

Epoch: 5| Step: 3
Training loss: 2.3139660358428955
Validation loss: 2.431848059418381

Epoch: 5| Step: 4
Training loss: 2.754056930541992
Validation loss: 2.4333397111585064

Epoch: 5| Step: 5
Training loss: 2.799173355102539
Validation loss: 2.435057565730105

Epoch: 5| Step: 6
Training loss: 2.289867877960205
Validation loss: 2.434919347045242

Epoch: 5| Step: 7
Training loss: 3.1205599308013916
Validation loss: 2.434259186508835

Epoch: 5| Step: 8
Training loss: 2.599315643310547
Validation loss: 2.4367990314319568

Epoch: 5| Step: 9
Training loss: 2.4796559810638428
Validation loss: 2.438166300455729

Epoch: 5| Step: 10
Training loss: 2.8721323013305664
Validation loss: 2.436158836528819

Epoch: 184| Step: 0
Training loss: 3.1741819381713867
Validation loss: 2.4338025508388395

Epoch: 5| Step: 1
Training loss: 2.9090940952301025
Validation loss: 2.434182108089488

Epoch: 5| Step: 2
Training loss: 2.1556832790374756
Validation loss: 2.4392265658224783

Epoch: 5| Step: 3
Training loss: 1.7682908773422241
Validation loss: 2.440607742596698

Epoch: 5| Step: 4
Training loss: 2.6385912895202637
Validation loss: 2.4412873880837553

Epoch: 5| Step: 5
Training loss: 2.7440719604492188
Validation loss: 2.439337322788854

Epoch: 5| Step: 6
Training loss: 2.574002265930176
Validation loss: 2.440795716419015

Epoch: 5| Step: 7
Training loss: 2.983721971511841
Validation loss: 2.4386938054074525

Epoch: 5| Step: 8
Training loss: 2.7197604179382324
Validation loss: 2.444705904171031

Epoch: 5| Step: 9
Training loss: 2.8780782222747803
Validation loss: 2.44797860166078

Epoch: 5| Step: 10
Training loss: 2.294478416442871
Validation loss: 2.457259434525685

Epoch: 185| Step: 0
Training loss: 3.157081127166748
Validation loss: 2.4561354703800653

Epoch: 5| Step: 1
Training loss: 2.3451075553894043
Validation loss: 2.457097484219459

Epoch: 5| Step: 2
Training loss: 1.9068409204483032
Validation loss: 2.4508204357598418

Epoch: 5| Step: 3
Training loss: 3.3932945728302
Validation loss: 2.4705996333911853

Epoch: 5| Step: 4
Training loss: 2.8469817638397217
Validation loss: 2.4432543913523355

Epoch: 5| Step: 5
Training loss: 2.090503692626953
Validation loss: 2.4369861131073325

Epoch: 5| Step: 6
Training loss: 2.9801063537597656
Validation loss: 2.432076720781224

Epoch: 5| Step: 7
Training loss: 2.100961685180664
Validation loss: 2.4397885696862334

Epoch: 5| Step: 8
Training loss: 2.547496795654297
Validation loss: 2.44286080842377

Epoch: 5| Step: 9
Training loss: 2.7602005004882812
Validation loss: 2.445854966358472

Epoch: 5| Step: 10
Training loss: 2.8649585247039795
Validation loss: 2.4560432562264065

Epoch: 186| Step: 0
Training loss: 2.859513998031616
Validation loss: 2.449860585633145

Epoch: 5| Step: 1
Training loss: 2.3532538414001465
Validation loss: 2.4500179931681645

Epoch: 5| Step: 2
Training loss: 2.5032076835632324
Validation loss: 2.4439024617595058

Epoch: 5| Step: 3
Training loss: 2.888044834136963
Validation loss: 2.4370856695277716

Epoch: 5| Step: 4
Training loss: 2.7261359691619873
Validation loss: 2.441195595648981

Epoch: 5| Step: 5
Training loss: 2.8560080528259277
Validation loss: 2.4369634992332867

Epoch: 5| Step: 6
Training loss: 2.5041656494140625
Validation loss: 2.446280466612949

Epoch: 5| Step: 7
Training loss: 2.794191360473633
Validation loss: 2.4445233293758926

Epoch: 5| Step: 8
Training loss: 2.3378024101257324
Validation loss: 2.4454870787999963

Epoch: 5| Step: 9
Training loss: 3.11399245262146
Validation loss: 2.4508728096562047

Epoch: 5| Step: 10
Training loss: 1.9344888925552368
Validation loss: 2.465890294762068

Epoch: 187| Step: 0
Training loss: 1.9315617084503174
Validation loss: 2.472901536572364

Epoch: 5| Step: 1
Training loss: 2.9892220497131348
Validation loss: 2.4852413618436424

Epoch: 5| Step: 2
Training loss: 2.410557270050049
Validation loss: 2.4930634639596425

Epoch: 5| Step: 3
Training loss: 2.526524066925049
Validation loss: 2.4983847038720244

Epoch: 5| Step: 4
Training loss: 2.829777240753174
Validation loss: 2.4892624655077533

Epoch: 5| Step: 5
Training loss: 3.1390435695648193
Validation loss: 2.4635922652418896

Epoch: 5| Step: 6
Training loss: 2.678342342376709
Validation loss: 2.4488237057962725

Epoch: 5| Step: 7
Training loss: 2.912485122680664
Validation loss: 2.4408038303416264

Epoch: 5| Step: 8
Training loss: 2.99371337890625
Validation loss: 2.431164597952238

Epoch: 5| Step: 9
Training loss: 2.3876328468322754
Validation loss: 2.435524614908362

Epoch: 5| Step: 10
Training loss: 2.2695178985595703
Validation loss: 2.432212875735375

Epoch: 188| Step: 0
Training loss: 3.205317258834839
Validation loss: 2.4312433017197477

Epoch: 5| Step: 1
Training loss: 2.788939952850342
Validation loss: 2.436181140202348

Epoch: 5| Step: 2
Training loss: 2.634983539581299
Validation loss: 2.4327542487011162

Epoch: 5| Step: 3
Training loss: 2.2833330631256104
Validation loss: 2.439830569810765

Epoch: 5| Step: 4
Training loss: 1.9233043193817139
Validation loss: 2.436258674949728

Epoch: 5| Step: 5
Training loss: 3.0292370319366455
Validation loss: 2.430237370152627

Epoch: 5| Step: 6
Training loss: 2.1094863414764404
Validation loss: 2.430639402840727

Epoch: 5| Step: 7
Training loss: 2.511606454849243
Validation loss: 2.435368614812051

Epoch: 5| Step: 8
Training loss: 3.3545615673065186
Validation loss: 2.428775951426516

Epoch: 5| Step: 9
Training loss: 2.3656866550445557
Validation loss: 2.4379704883021693

Epoch: 5| Step: 10
Training loss: 2.9071671962738037
Validation loss: 2.442090957395492

Epoch: 189| Step: 0
Training loss: 2.7200467586517334
Validation loss: 2.4487650702076573

Epoch: 5| Step: 1
Training loss: 2.3366284370422363
Validation loss: 2.454138501997917

Epoch: 5| Step: 2
Training loss: 2.675504684448242
Validation loss: 2.4618730339952695

Epoch: 5| Step: 3
Training loss: 2.4829061031341553
Validation loss: 2.4691236096043743

Epoch: 5| Step: 4
Training loss: 2.08465576171875
Validation loss: 2.4698003466411302

Epoch: 5| Step: 5
Training loss: 2.6779448986053467
Validation loss: 2.4742142872143815

Epoch: 5| Step: 6
Training loss: 2.9493210315704346
Validation loss: 2.4749165260663597

Epoch: 5| Step: 7
Training loss: 2.63073468208313
Validation loss: 2.477980931599935

Epoch: 5| Step: 8
Training loss: 2.058277130126953
Validation loss: 2.4591002695022093

Epoch: 5| Step: 9
Training loss: 3.4378979206085205
Validation loss: 2.4532551175804547

Epoch: 5| Step: 10
Training loss: 3.02162766456604
Validation loss: 2.4438559932093464

Epoch: 190| Step: 0
Training loss: 2.362534284591675
Validation loss: 2.4318997013953423

Epoch: 5| Step: 1
Training loss: 1.984627366065979
Validation loss: 2.428776225736064

Epoch: 5| Step: 2
Training loss: 2.8712096214294434
Validation loss: 2.4236032526980162

Epoch: 5| Step: 3
Training loss: 2.746598243713379
Validation loss: 2.4221800681083434

Epoch: 5| Step: 4
Training loss: 3.059645414352417
Validation loss: 2.4245957610427693

Epoch: 5| Step: 5
Training loss: 2.2665657997131348
Validation loss: 2.428780583925145

Epoch: 5| Step: 6
Training loss: 2.5804238319396973
Validation loss: 2.4216502046072357

Epoch: 5| Step: 7
Training loss: 2.3793275356292725
Validation loss: 2.431779529458733

Epoch: 5| Step: 8
Training loss: 2.252361297607422
Validation loss: 2.4290029361683834

Epoch: 5| Step: 9
Training loss: 3.3567206859588623
Validation loss: 2.4373119979776363

Epoch: 5| Step: 10
Training loss: 3.0152697563171387
Validation loss: 2.445332950161349

Epoch: 191| Step: 0
Training loss: 2.8632397651672363
Validation loss: 2.4393147909513084

Epoch: 5| Step: 1
Training loss: 2.638707160949707
Validation loss: 2.4408917965427523

Epoch: 5| Step: 2
Training loss: 3.1407244205474854
Validation loss: 2.4382669797507663

Epoch: 5| Step: 3
Training loss: 2.3149421215057373
Validation loss: 2.4371785194643083

Epoch: 5| Step: 4
Training loss: 2.568253993988037
Validation loss: 2.4325620358990085

Epoch: 5| Step: 5
Training loss: 2.24916410446167
Validation loss: 2.4266937983933317

Epoch: 5| Step: 6
Training loss: 2.5752151012420654
Validation loss: 2.4227286948952624

Epoch: 5| Step: 7
Training loss: 2.138669013977051
Validation loss: 2.4244079320661482

Epoch: 5| Step: 8
Training loss: 2.736694812774658
Validation loss: 2.425065202097739

Epoch: 5| Step: 9
Training loss: 3.0215423107147217
Validation loss: 2.4285853139815794

Epoch: 5| Step: 10
Training loss: 2.5068888664245605
Validation loss: 2.4268019635190248

Epoch: 192| Step: 0
Training loss: 2.7695934772491455
Validation loss: 2.4318357295887445

Epoch: 5| Step: 1
Training loss: 2.6260385513305664
Validation loss: 2.4262823853441464

Epoch: 5| Step: 2
Training loss: 2.113939046859741
Validation loss: 2.4220167411270963

Epoch: 5| Step: 3
Training loss: 2.891749620437622
Validation loss: 2.424025886802263

Epoch: 5| Step: 4
Training loss: 2.172328233718872
Validation loss: 2.423436162292316

Epoch: 5| Step: 5
Training loss: 2.811673164367676
Validation loss: 2.4209843809886644

Epoch: 5| Step: 6
Training loss: 2.913508176803589
Validation loss: 2.4210864061950357

Epoch: 5| Step: 7
Training loss: 2.9371981620788574
Validation loss: 2.426685215324484

Epoch: 5| Step: 8
Training loss: 2.4480321407318115
Validation loss: 2.4262016152822845

Epoch: 5| Step: 9
Training loss: 2.5284481048583984
Validation loss: 2.425580040101082

Epoch: 5| Step: 10
Training loss: 2.643247365951538
Validation loss: 2.427977961878623

Epoch: 193| Step: 0
Training loss: 1.6161730289459229
Validation loss: 2.432586098230013

Epoch: 5| Step: 1
Training loss: 2.5400097370147705
Validation loss: 2.4284204308704664

Epoch: 5| Step: 2
Training loss: 2.873892307281494
Validation loss: 2.434263188351867

Epoch: 5| Step: 3
Training loss: 2.794501543045044
Validation loss: 2.432853347511702

Epoch: 5| Step: 4
Training loss: 3.153381586074829
Validation loss: 2.445828935151459

Epoch: 5| Step: 5
Training loss: 2.7751612663269043
Validation loss: 2.4362430982692267

Epoch: 5| Step: 6
Training loss: 2.9915785789489746
Validation loss: 2.4378403796944568

Epoch: 5| Step: 7
Training loss: 2.446560859680176
Validation loss: 2.439834387071671

Epoch: 5| Step: 8
Training loss: 2.503775119781494
Validation loss: 2.428855008976434

Epoch: 5| Step: 9
Training loss: 2.8325295448303223
Validation loss: 2.4312077414604927

Epoch: 5| Step: 10
Training loss: 2.1127541065216064
Validation loss: 2.4307219853965183

Epoch: 194| Step: 0
Training loss: 3.1936211585998535
Validation loss: 2.4359714677256923

Epoch: 5| Step: 1
Training loss: 3.1034626960754395
Validation loss: 2.4399389477186304

Epoch: 5| Step: 2
Training loss: 2.6678953170776367
Validation loss: 2.4531749884287515

Epoch: 5| Step: 3
Training loss: 1.8718204498291016
Validation loss: 2.45120168501331

Epoch: 5| Step: 4
Training loss: 2.848827838897705
Validation loss: 2.448647419611613

Epoch: 5| Step: 5
Training loss: 2.749206781387329
Validation loss: 2.446646915969028

Epoch: 5| Step: 6
Training loss: 2.861985445022583
Validation loss: 2.442889698090092

Epoch: 5| Step: 7
Training loss: 2.1628575325012207
Validation loss: 2.4371041328676286

Epoch: 5| Step: 8
Training loss: 2.3401827812194824
Validation loss: 2.435225184245776

Epoch: 5| Step: 9
Training loss: 2.5197510719299316
Validation loss: 2.4312226080125376

Epoch: 5| Step: 10
Training loss: 2.3942830562591553
Validation loss: 2.423941558407199

Epoch: 195| Step: 0
Training loss: 2.829258680343628
Validation loss: 2.4225607097789807

Epoch: 5| Step: 1
Training loss: 2.831465482711792
Validation loss: 2.4197148379459175

Epoch: 5| Step: 2
Training loss: 2.676555633544922
Validation loss: 2.425090310394123

Epoch: 5| Step: 3
Training loss: 2.3092551231384277
Validation loss: 2.420827857909664

Epoch: 5| Step: 4
Training loss: 1.8835920095443726
Validation loss: 2.425391758641889

Epoch: 5| Step: 5
Training loss: 2.476135015487671
Validation loss: 2.423767835863175

Epoch: 5| Step: 6
Training loss: 2.4477291107177734
Validation loss: 2.423829165838098

Epoch: 5| Step: 7
Training loss: 2.336764097213745
Validation loss: 2.42731991634574

Epoch: 5| Step: 8
Training loss: 3.0950236320495605
Validation loss: 2.425822804051061

Epoch: 5| Step: 9
Training loss: 2.6524498462677
Validation loss: 2.4316087384377756

Epoch: 5| Step: 10
Training loss: 3.2583141326904297
Validation loss: 2.4245953252238612

Epoch: 196| Step: 0
Training loss: 1.8821617364883423
Validation loss: 2.427962254452449

Epoch: 5| Step: 1
Training loss: 1.9443050622940063
Validation loss: 2.41942947910678

Epoch: 5| Step: 2
Training loss: 2.912546157836914
Validation loss: 2.4267361061547392

Epoch: 5| Step: 3
Training loss: 2.5012612342834473
Validation loss: 2.417316852077361

Epoch: 5| Step: 4
Training loss: 2.7547080516815186
Validation loss: 2.4199313784158356

Epoch: 5| Step: 5
Training loss: 2.4068846702575684
Validation loss: 2.4262876818256993

Epoch: 5| Step: 6
Training loss: 3.380242109298706
Validation loss: 2.419232878633725

Epoch: 5| Step: 7
Training loss: 2.598512887954712
Validation loss: 2.420438507551788

Epoch: 5| Step: 8
Training loss: 3.122957229614258
Validation loss: 2.4185347864704747

Epoch: 5| Step: 9
Training loss: 2.6478705406188965
Validation loss: 2.419794485133181

Epoch: 5| Step: 10
Training loss: 2.576298952102661
Validation loss: 2.418454465045724

Epoch: 197| Step: 0
Training loss: 1.876974105834961
Validation loss: 2.415728576721684

Epoch: 5| Step: 1
Training loss: 2.390901565551758
Validation loss: 2.4172261120170675

Epoch: 5| Step: 2
Training loss: 3.001957416534424
Validation loss: 2.4261744842734387

Epoch: 5| Step: 3
Training loss: 2.6297144889831543
Validation loss: 2.4315464445339736

Epoch: 5| Step: 4
Training loss: 2.481718063354492
Validation loss: 2.4391170265854045

Epoch: 5| Step: 5
Training loss: 3.240933895111084
Validation loss: 2.4434245401813137

Epoch: 5| Step: 6
Training loss: 2.7811522483825684
Validation loss: 2.4440336483781055

Epoch: 5| Step: 7
Training loss: 2.4596498012542725
Validation loss: 2.4501269043132825

Epoch: 5| Step: 8
Training loss: 2.62855863571167
Validation loss: 2.4448706898637997

Epoch: 5| Step: 9
Training loss: 2.7210800647735596
Validation loss: 2.4293853595692623

Epoch: 5| Step: 10
Training loss: 2.6140074729919434
Validation loss: 2.422130415516515

Epoch: 198| Step: 0
Training loss: 3.119387149810791
Validation loss: 2.4262413799121814

Epoch: 5| Step: 1
Training loss: 2.1857147216796875
Validation loss: 2.4193740070507093

Epoch: 5| Step: 2
Training loss: 2.6961445808410645
Validation loss: 2.4220370989973827

Epoch: 5| Step: 3
Training loss: 2.6267776489257812
Validation loss: 2.4305183195298716

Epoch: 5| Step: 4
Training loss: 2.602604627609253
Validation loss: 2.426403243054626

Epoch: 5| Step: 5
Training loss: 3.159100294113159
Validation loss: 2.4329591592152915

Epoch: 5| Step: 6
Training loss: 2.9275214672088623
Validation loss: 2.4391974685012654

Epoch: 5| Step: 7
Training loss: 2.579296112060547
Validation loss: 2.4344438506710913

Epoch: 5| Step: 8
Training loss: 1.9970659017562866
Validation loss: 2.4188528291640745

Epoch: 5| Step: 9
Training loss: 2.6570658683776855
Validation loss: 2.4196377185083207

Epoch: 5| Step: 10
Training loss: 2.1668362617492676
Validation loss: 2.419446596535303

Epoch: 199| Step: 0
Training loss: 2.7373108863830566
Validation loss: 2.4293856466970136

Epoch: 5| Step: 1
Training loss: 2.2491135597229004
Validation loss: 2.4438762921158985

Epoch: 5| Step: 2
Training loss: 2.49001407623291
Validation loss: 2.4557202913427867

Epoch: 5| Step: 3
Training loss: 2.443031072616577
Validation loss: 2.4431424499839864

Epoch: 5| Step: 4
Training loss: 3.0939087867736816
Validation loss: 2.4434085481910297

Epoch: 5| Step: 5
Training loss: 2.3592941761016846
Validation loss: 2.4247846244483866

Epoch: 5| Step: 6
Training loss: 3.008330821990967
Validation loss: 2.415575045411305

Epoch: 5| Step: 7
Training loss: 2.984865188598633
Validation loss: 2.4052056881689254

Epoch: 5| Step: 8
Training loss: 2.6219122409820557
Validation loss: 2.4007623170011785

Epoch: 5| Step: 9
Training loss: 2.2913875579833984
Validation loss: 2.4029711728454917

Epoch: 5| Step: 10
Training loss: 2.462146043777466
Validation loss: 2.400861058183896

Epoch: 200| Step: 0
Training loss: 2.5676238536834717
Validation loss: 2.4036398395415275

Epoch: 5| Step: 1
Training loss: 2.3702890872955322
Validation loss: 2.4042255032447075

Epoch: 5| Step: 2
Training loss: 2.5809695720672607
Validation loss: 2.398115078608195

Epoch: 5| Step: 3
Training loss: 2.832573652267456
Validation loss: 2.405920925960746

Epoch: 5| Step: 4
Training loss: 3.150839328765869
Validation loss: 2.4095586192223335

Epoch: 5| Step: 5
Training loss: 2.487266778945923
Validation loss: 2.4053044729335333

Epoch: 5| Step: 6
Training loss: 2.8427257537841797
Validation loss: 2.409367435721941

Epoch: 5| Step: 7
Training loss: 2.0743346214294434
Validation loss: 2.4068711598714194

Epoch: 5| Step: 8
Training loss: 3.3874645233154297
Validation loss: 2.4006436588943645

Epoch: 5| Step: 9
Training loss: 2.236981153488159
Validation loss: 2.4012575969901135

Epoch: 5| Step: 10
Training loss: 2.2039496898651123
Validation loss: 2.4001602459979314

Epoch: 201| Step: 0
Training loss: 1.8840776681900024
Validation loss: 2.4008517239683416

Epoch: 5| Step: 1
Training loss: 2.6284565925598145
Validation loss: 2.4055305527102564

Epoch: 5| Step: 2
Training loss: 2.638582706451416
Validation loss: 2.403559987263013

Epoch: 5| Step: 3
Training loss: 2.547578811645508
Validation loss: 2.4032255347057054

Epoch: 5| Step: 4
Training loss: 1.9937496185302734
Validation loss: 2.4075845697874665

Epoch: 5| Step: 5
Training loss: 2.75515079498291
Validation loss: 2.407834017148582

Epoch: 5| Step: 6
Training loss: 2.682830333709717
Validation loss: 2.4125741963745444

Epoch: 5| Step: 7
Training loss: 2.744154453277588
Validation loss: 2.4081638730982298

Epoch: 5| Step: 8
Training loss: 3.1962010860443115
Validation loss: 2.4074663692905056

Epoch: 5| Step: 9
Training loss: 2.289672374725342
Validation loss: 2.4076440129228818

Epoch: 5| Step: 10
Training loss: 3.4460721015930176
Validation loss: 2.4134083281281176

Epoch: 202| Step: 0
Training loss: 2.638824701309204
Validation loss: 2.4079017639160156

Epoch: 5| Step: 1
Training loss: 2.9289016723632812
Validation loss: 2.409595117774061

Epoch: 5| Step: 2
Training loss: 2.774962902069092
Validation loss: 2.4144118011638684

Epoch: 5| Step: 3
Training loss: 2.176673412322998
Validation loss: 2.4185644272835023

Epoch: 5| Step: 4
Training loss: 2.1414341926574707
Validation loss: 2.4134701990312144

Epoch: 5| Step: 5
Training loss: 2.6325478553771973
Validation loss: 2.4152979466222946

Epoch: 5| Step: 6
Training loss: 2.8805105686187744
Validation loss: 2.424336101419182

Epoch: 5| Step: 7
Training loss: 2.611642360687256
Validation loss: 2.4201551098977365

Epoch: 5| Step: 8
Training loss: 2.6097187995910645
Validation loss: 2.428402261067462

Epoch: 5| Step: 9
Training loss: 2.260596752166748
Validation loss: 2.4173961711186234

Epoch: 5| Step: 10
Training loss: 3.0468833446502686
Validation loss: 2.4282797241723664

Epoch: 203| Step: 0
Training loss: 3.0796399116516113
Validation loss: 2.4266165841010308

Epoch: 5| Step: 1
Training loss: 3.168426036834717
Validation loss: 2.417639050432431

Epoch: 5| Step: 2
Training loss: 2.738269805908203
Validation loss: 2.4194323785843386

Epoch: 5| Step: 3
Training loss: 2.765796661376953
Validation loss: 2.400118704765074

Epoch: 5| Step: 4
Training loss: 1.8067731857299805
Validation loss: 2.4003825264592327

Epoch: 5| Step: 5
Training loss: 2.5697684288024902
Validation loss: 2.388664745515393

Epoch: 5| Step: 6
Training loss: 2.0230369567871094
Validation loss: 2.395299765371507

Epoch: 5| Step: 7
Training loss: 3.091581106185913
Validation loss: 2.391622010097709

Epoch: 5| Step: 8
Training loss: 2.0998313426971436
Validation loss: 2.404583295186361

Epoch: 5| Step: 9
Training loss: 2.4174134731292725
Validation loss: 2.4020555506470385

Epoch: 5| Step: 10
Training loss: 2.9710052013397217
Validation loss: 2.407043226303593

Epoch: 204| Step: 0
Training loss: 2.5873608589172363
Validation loss: 2.408342692159837

Epoch: 5| Step: 1
Training loss: 2.9281952381134033
Validation loss: 2.4121285997411257

Epoch: 5| Step: 2
Training loss: 2.2865099906921387
Validation loss: 2.4118047427105647

Epoch: 5| Step: 3
Training loss: 2.8387038707733154
Validation loss: 2.405784394151421

Epoch: 5| Step: 4
Training loss: 3.2377731800079346
Validation loss: 2.399133953996884

Epoch: 5| Step: 5
Training loss: 2.3987770080566406
Validation loss: 2.401258594246321

Epoch: 5| Step: 6
Training loss: 2.195896625518799
Validation loss: 2.3992871686976445

Epoch: 5| Step: 7
Training loss: 2.914830446243286
Validation loss: 2.3988568334169287

Epoch: 5| Step: 8
Training loss: 3.1263561248779297
Validation loss: 2.402192146547379

Epoch: 5| Step: 9
Training loss: 2.203721523284912
Validation loss: 2.4051268895467124

Epoch: 5| Step: 10
Training loss: 1.8746304512023926
Validation loss: 2.4017185139399704

Epoch: 205| Step: 0
Training loss: 2.233757972717285
Validation loss: 2.395605407735353

Epoch: 5| Step: 1
Training loss: 2.962277889251709
Validation loss: 2.3924191638987553

Epoch: 5| Step: 2
Training loss: 2.316277265548706
Validation loss: 2.3967033611830844

Epoch: 5| Step: 3
Training loss: 2.8741297721862793
Validation loss: 2.3999748537617345

Epoch: 5| Step: 4
Training loss: 3.050433397293091
Validation loss: 2.4084971797081733

Epoch: 5| Step: 5
Training loss: 2.3256402015686035
Validation loss: 2.4104122859175487

Epoch: 5| Step: 6
Training loss: 2.5095362663269043
Validation loss: 2.4129284915103706

Epoch: 5| Step: 7
Training loss: 2.2222378253936768
Validation loss: 2.4081894761772564

Epoch: 5| Step: 8
Training loss: 2.86045503616333
Validation loss: 2.3952755210220174

Epoch: 5| Step: 9
Training loss: 3.2302982807159424
Validation loss: 2.391881835076117

Epoch: 5| Step: 10
Training loss: 1.9850354194641113
Validation loss: 2.392060943829116

Epoch: 206| Step: 0
Training loss: 2.5425853729248047
Validation loss: 2.388234712744272

Epoch: 5| Step: 1
Training loss: 2.718900203704834
Validation loss: 2.3906778045879897

Epoch: 5| Step: 2
Training loss: 3.1297335624694824
Validation loss: 2.4016831510810444

Epoch: 5| Step: 3
Training loss: 2.589496612548828
Validation loss: 2.3996358033149474

Epoch: 5| Step: 4
Training loss: 2.7697572708129883
Validation loss: 2.4044838310569845

Epoch: 5| Step: 5
Training loss: 2.4539175033569336
Validation loss: 2.405162395969514

Epoch: 5| Step: 6
Training loss: 1.922600507736206
Validation loss: 2.4011046142988306

Epoch: 5| Step: 7
Training loss: 2.190460681915283
Validation loss: 2.41050281575931

Epoch: 5| Step: 8
Training loss: 3.0122148990631104
Validation loss: 2.412195054433679

Epoch: 5| Step: 9
Training loss: 2.226322889328003
Validation loss: 2.3966943781862975

Epoch: 5| Step: 10
Training loss: 3.1783201694488525
Validation loss: 2.3954773487583285

Epoch: 207| Step: 0
Training loss: 3.0210986137390137
Validation loss: 2.3941479805977113

Epoch: 5| Step: 1
Training loss: 2.24005389213562
Validation loss: 2.3855078117821806

Epoch: 5| Step: 2
Training loss: 3.4364089965820312
Validation loss: 2.3850031680958246

Epoch: 5| Step: 3
Training loss: 2.401787042617798
Validation loss: 2.385374426841736

Epoch: 5| Step: 4
Training loss: 2.6325254440307617
Validation loss: 2.3815990929962485

Epoch: 5| Step: 5
Training loss: 3.0029115676879883
Validation loss: 2.38799572760059

Epoch: 5| Step: 6
Training loss: 2.5854926109313965
Validation loss: 2.3950572757310766

Epoch: 5| Step: 7
Training loss: 2.341027021408081
Validation loss: 2.3913724499364055

Epoch: 5| Step: 8
Training loss: 2.185115337371826
Validation loss: 2.3891997029704433

Epoch: 5| Step: 9
Training loss: 2.6149332523345947
Validation loss: 2.393511700373824

Epoch: 5| Step: 10
Training loss: 2.0705342292785645
Validation loss: 2.396253765270274

Epoch: 208| Step: 0
Training loss: 1.9972352981567383
Validation loss: 2.407868357114894

Epoch: 5| Step: 1
Training loss: 3.0915610790252686
Validation loss: 2.4223758046345045

Epoch: 5| Step: 2
Training loss: 2.4193873405456543
Validation loss: 2.4217196100501606

Epoch: 5| Step: 3
Training loss: 2.7432074546813965
Validation loss: 2.4312394613860757

Epoch: 5| Step: 4
Training loss: 2.1753368377685547
Validation loss: 2.438630452720068

Epoch: 5| Step: 5
Training loss: 2.6465446949005127
Validation loss: 2.4226437384082424

Epoch: 5| Step: 6
Training loss: 2.380807399749756
Validation loss: 2.4258488532035583

Epoch: 5| Step: 7
Training loss: 2.727544069290161
Validation loss: 2.4218630277982323

Epoch: 5| Step: 8
Training loss: 3.28014874458313
Validation loss: 2.4107210610502507

Epoch: 5| Step: 9
Training loss: 2.334376096725464
Validation loss: 2.411577770786901

Epoch: 5| Step: 10
Training loss: 2.9231462478637695
Validation loss: 2.399682173164942

Epoch: 209| Step: 0
Training loss: 3.1491858959198
Validation loss: 2.3999551803834978

Epoch: 5| Step: 1
Training loss: 3.005556344985962
Validation loss: 2.3996804555257163

Epoch: 5| Step: 2
Training loss: 2.21557354927063
Validation loss: 2.4039113444666707

Epoch: 5| Step: 3
Training loss: 2.1942667961120605
Validation loss: 2.40056860831476

Epoch: 5| Step: 4
Training loss: 2.695683002471924
Validation loss: 2.3983505195186985

Epoch: 5| Step: 5
Training loss: 2.1480705738067627
Validation loss: 2.395700547002977

Epoch: 5| Step: 6
Training loss: 3.1443493366241455
Validation loss: 2.396103543619956

Epoch: 5| Step: 7
Training loss: 2.754492998123169
Validation loss: 2.396080773363831

Epoch: 5| Step: 8
Training loss: 3.1239571571350098
Validation loss: 2.393805783282044

Epoch: 5| Step: 9
Training loss: 2.0915255546569824
Validation loss: 2.3919950864648305

Epoch: 5| Step: 10
Training loss: 2.0886857509613037
Validation loss: 2.390031071119411

Epoch: 210| Step: 0
Training loss: 2.532963275909424
Validation loss: 2.3963636659806773

Epoch: 5| Step: 1
Training loss: 2.765897274017334
Validation loss: 2.391806464041433

Epoch: 5| Step: 2
Training loss: 2.536036491394043
Validation loss: 2.39071237656378

Epoch: 5| Step: 3
Training loss: 2.80165696144104
Validation loss: 2.3959610205824657

Epoch: 5| Step: 4
Training loss: 2.118490695953369
Validation loss: 2.396497905895274

Epoch: 5| Step: 5
Training loss: 3.06284761428833
Validation loss: 2.4025953123646397

Epoch: 5| Step: 6
Training loss: 2.399449110031128
Validation loss: 2.398563128645702

Epoch: 5| Step: 7
Training loss: 2.784687042236328
Validation loss: 2.4041051300623084

Epoch: 5| Step: 8
Training loss: 2.4316821098327637
Validation loss: 2.398763761725477

Epoch: 5| Step: 9
Training loss: 2.8208727836608887
Validation loss: 2.3916889429092407

Epoch: 5| Step: 10
Training loss: 2.185952663421631
Validation loss: 2.392610829363587

Epoch: 211| Step: 0
Training loss: 2.719151735305786
Validation loss: 2.4000395344149683

Epoch: 5| Step: 1
Training loss: 3.346698045730591
Validation loss: 2.3943387436610397

Epoch: 5| Step: 2
Training loss: 2.387373924255371
Validation loss: 2.3911235947762766

Epoch: 5| Step: 3
Training loss: 2.635742664337158
Validation loss: 2.387518172623009

Epoch: 5| Step: 4
Training loss: 2.616581678390503
Validation loss: 2.389180234683457

Epoch: 5| Step: 5
Training loss: 2.8128130435943604
Validation loss: 2.3847071547662058

Epoch: 5| Step: 6
Training loss: 2.70214581489563
Validation loss: 2.3876381894593597

Epoch: 5| Step: 7
Training loss: 2.044156074523926
Validation loss: 2.384302048272984

Epoch: 5| Step: 8
Training loss: 2.527479648590088
Validation loss: 2.387820061816964

Epoch: 5| Step: 9
Training loss: 2.197733163833618
Validation loss: 2.395885708511517

Epoch: 5| Step: 10
Training loss: 2.541144847869873
Validation loss: 2.4038331098453973

Epoch: 212| Step: 0
Training loss: 2.2601101398468018
Validation loss: 2.4067918946666103

Epoch: 5| Step: 1
Training loss: 2.831979513168335
Validation loss: 2.411521855221

Epoch: 5| Step: 2
Training loss: 2.528179407119751
Validation loss: 2.4123284073286158

Epoch: 5| Step: 3
Training loss: 2.7479891777038574
Validation loss: 2.4071889846555647

Epoch: 5| Step: 4
Training loss: 1.864884614944458
Validation loss: 2.3922329551430157

Epoch: 5| Step: 5
Training loss: 2.8129444122314453
Validation loss: 2.385017628310829

Epoch: 5| Step: 6
Training loss: 2.755894184112549
Validation loss: 2.3925707878605014

Epoch: 5| Step: 7
Training loss: 3.021360397338867
Validation loss: 2.3837598575058805

Epoch: 5| Step: 8
Training loss: 2.531491756439209
Validation loss: 2.383183379327097

Epoch: 5| Step: 9
Training loss: 2.3045363426208496
Validation loss: 2.3879606851967434

Epoch: 5| Step: 10
Training loss: 2.8394923210144043
Validation loss: 2.3842959993629047

Epoch: 213| Step: 0
Training loss: 2.7387170791625977
Validation loss: 2.3873138222643124

Epoch: 5| Step: 1
Training loss: 2.7807793617248535
Validation loss: 2.381946402211343

Epoch: 5| Step: 2
Training loss: 2.6491451263427734
Validation loss: 2.385895980301724

Epoch: 5| Step: 3
Training loss: 2.0906291007995605
Validation loss: 2.3893995028670116

Epoch: 5| Step: 4
Training loss: 2.199012279510498
Validation loss: 2.3953575780314784

Epoch: 5| Step: 5
Training loss: 3.1541404724121094
Validation loss: 2.3896759838186283

Epoch: 5| Step: 6
Training loss: 2.3811302185058594
Validation loss: 2.3852843571734685

Epoch: 5| Step: 7
Training loss: 2.5629019737243652
Validation loss: 2.395362328457576

Epoch: 5| Step: 8
Training loss: 2.592808485031128
Validation loss: 2.4011920023989934

Epoch: 5| Step: 9
Training loss: 2.501370668411255
Validation loss: 2.410396740000735

Epoch: 5| Step: 10
Training loss: 2.8616909980773926
Validation loss: 2.4197535848104827

Epoch: 214| Step: 0
Training loss: 1.9265658855438232
Validation loss: 2.415890150172736

Epoch: 5| Step: 1
Training loss: 2.98392915725708
Validation loss: 2.434297303999624

Epoch: 5| Step: 2
Training loss: 2.509272813796997
Validation loss: 2.4293201200423704

Epoch: 5| Step: 3
Training loss: 3.3301939964294434
Validation loss: 2.4274032423573155

Epoch: 5| Step: 4
Training loss: 2.352135419845581
Validation loss: 2.4049948235993743

Epoch: 5| Step: 5
Training loss: 2.8056910037994385
Validation loss: 2.395114019352903

Epoch: 5| Step: 6
Training loss: 2.7406206130981445
Validation loss: 2.3886251347039336

Epoch: 5| Step: 7
Training loss: 2.684055805206299
Validation loss: 2.387147908569664

Epoch: 5| Step: 8
Training loss: 2.9483935832977295
Validation loss: 2.38099746037555

Epoch: 5| Step: 9
Training loss: 2.2234063148498535
Validation loss: 2.381034363982498

Epoch: 5| Step: 10
Training loss: 1.9961940050125122
Validation loss: 2.3791622243901736

Epoch: 215| Step: 0
Training loss: 2.719081401824951
Validation loss: 2.3749923270235778

Epoch: 5| Step: 1
Training loss: 2.3482441902160645
Validation loss: 2.3802339351305397

Epoch: 5| Step: 2
Training loss: 2.4357447624206543
Validation loss: 2.3799492595016316

Epoch: 5| Step: 3
Training loss: 2.664543628692627
Validation loss: 2.3871701584067395

Epoch: 5| Step: 4
Training loss: 3.276456356048584
Validation loss: 2.383213104740266

Epoch: 5| Step: 5
Training loss: 3.10878324508667
Validation loss: 2.380462231174592

Epoch: 5| Step: 6
Training loss: 1.8340976238250732
Validation loss: 2.378419432588803

Epoch: 5| Step: 7
Training loss: 3.1270012855529785
Validation loss: 2.375623272311303

Epoch: 5| Step: 8
Training loss: 2.252532958984375
Validation loss: 2.3718305223731586

Epoch: 5| Step: 9
Training loss: 2.498518228530884
Validation loss: 2.373992153393325

Epoch: 5| Step: 10
Training loss: 2.3366496562957764
Validation loss: 2.3864511610359274

Epoch: 216| Step: 0
Training loss: 1.8807350397109985
Validation loss: 2.3889970010326755

Epoch: 5| Step: 1
Training loss: 3.306938648223877
Validation loss: 2.3893179380765526

Epoch: 5| Step: 2
Training loss: 2.2064406871795654
Validation loss: 2.396524288321054

Epoch: 5| Step: 3
Training loss: 2.7079224586486816
Validation loss: 2.3925730118187527

Epoch: 5| Step: 4
Training loss: 2.126722574234009
Validation loss: 2.3903238440072663

Epoch: 5| Step: 5
Training loss: 1.8399324417114258
Validation loss: 2.3920927457911993

Epoch: 5| Step: 6
Training loss: 3.257418394088745
Validation loss: 2.3870922365496234

Epoch: 5| Step: 7
Training loss: 2.5607941150665283
Validation loss: 2.390294139103223

Epoch: 5| Step: 8
Training loss: 3.049312114715576
Validation loss: 2.39216620691361

Epoch: 5| Step: 9
Training loss: 2.917914628982544
Validation loss: 2.401559822020992

Epoch: 5| Step: 10
Training loss: 2.5768909454345703
Validation loss: 2.4180719480719617

Epoch: 217| Step: 0
Training loss: 2.209651470184326
Validation loss: 2.421574951500021

Epoch: 5| Step: 1
Training loss: 3.000361204147339
Validation loss: 2.430319281034572

Epoch: 5| Step: 2
Training loss: 2.442736864089966
Validation loss: 2.4289661786889516

Epoch: 5| Step: 3
Training loss: 2.587750196456909
Validation loss: 2.4295597794235393

Epoch: 5| Step: 4
Training loss: 2.2542672157287598
Validation loss: 2.4087639803527505

Epoch: 5| Step: 5
Training loss: 3.203862428665161
Validation loss: 2.3982239923169537

Epoch: 5| Step: 6
Training loss: 2.6071038246154785
Validation loss: 2.405933633927376

Epoch: 5| Step: 7
Training loss: 2.010998249053955
Validation loss: 2.4053693920053463

Epoch: 5| Step: 8
Training loss: 2.3460724353790283
Validation loss: 2.4047344935837613

Epoch: 5| Step: 9
Training loss: 2.685469388961792
Validation loss: 2.397688276024275

Epoch: 5| Step: 10
Training loss: 3.1824727058410645
Validation loss: 2.3952493654784335

Epoch: 218| Step: 0
Training loss: 2.3659307956695557
Validation loss: 2.386246512013097

Epoch: 5| Step: 1
Training loss: 3.0662403106689453
Validation loss: 2.3769909976631083

Epoch: 5| Step: 2
Training loss: 2.2861037254333496
Validation loss: 2.369813556312233

Epoch: 5| Step: 3
Training loss: 2.020601749420166
Validation loss: 2.3695292395930134

Epoch: 5| Step: 4
Training loss: 2.58828067779541
Validation loss: 2.3634620943377094

Epoch: 5| Step: 5
Training loss: 2.8434433937072754
Validation loss: 2.3627636407011297

Epoch: 5| Step: 6
Training loss: 1.8662564754486084
Validation loss: 2.3630050664306967

Epoch: 5| Step: 7
Training loss: 2.727104663848877
Validation loss: 2.3618789667724283

Epoch: 5| Step: 8
Training loss: 2.2394723892211914
Validation loss: 2.365938671173588

Epoch: 5| Step: 9
Training loss: 2.8859660625457764
Validation loss: 2.374385233848326

Epoch: 5| Step: 10
Training loss: 3.7494029998779297
Validation loss: 2.3780639812510502

Epoch: 219| Step: 0
Training loss: 2.4346835613250732
Validation loss: 2.391078761828843

Epoch: 5| Step: 1
Training loss: 2.854745626449585
Validation loss: 2.3838091050424883

Epoch: 5| Step: 2
Training loss: 1.9320493936538696
Validation loss: 2.391620825695735

Epoch: 5| Step: 3
Training loss: 2.9762535095214844
Validation loss: 2.399942495489633

Epoch: 5| Step: 4
Training loss: 2.6755595207214355
Validation loss: 2.4066060717387865

Epoch: 5| Step: 5
Training loss: 2.323118209838867
Validation loss: 2.408435167804841

Epoch: 5| Step: 6
Training loss: 2.608952760696411
Validation loss: 2.4074573209208827

Epoch: 5| Step: 7
Training loss: 2.5113778114318848
Validation loss: 2.409136249173072

Epoch: 5| Step: 8
Training loss: 2.7553369998931885
Validation loss: 2.4081405952412593

Epoch: 5| Step: 9
Training loss: 2.3076789379119873
Validation loss: 2.4097595842935706

Epoch: 5| Step: 10
Training loss: 3.261838912963867
Validation loss: 2.407906578433129

Epoch: 220| Step: 0
Training loss: 2.35107421875
Validation loss: 2.40530300653109

Epoch: 5| Step: 1
Training loss: 3.3800883293151855
Validation loss: 2.4118137321164532

Epoch: 5| Step: 2
Training loss: 1.9702599048614502
Validation loss: 2.405587934678601

Epoch: 5| Step: 3
Training loss: 3.1562647819519043
Validation loss: 2.402050446438533

Epoch: 5| Step: 4
Training loss: 1.399871587753296
Validation loss: 2.3920045334805726

Epoch: 5| Step: 5
Training loss: 2.895440101623535
Validation loss: 2.3904969256411315

Epoch: 5| Step: 6
Training loss: 2.010026216506958
Validation loss: 2.3837439321702525

Epoch: 5| Step: 7
Training loss: 3.124129056930542
Validation loss: 2.3716387235990135

Epoch: 5| Step: 8
Training loss: 2.743439197540283
Validation loss: 2.367696331393334

Epoch: 5| Step: 9
Training loss: 2.510855197906494
Validation loss: 2.3661759386780443

Epoch: 5| Step: 10
Training loss: 3.099388360977173
Validation loss: 2.360580757100095

Epoch: 221| Step: 0
Training loss: 2.162996292114258
Validation loss: 2.3584260274005193

Epoch: 5| Step: 1
Training loss: 2.735224485397339
Validation loss: 2.3609490317683064

Epoch: 5| Step: 2
Training loss: 2.5336673259735107
Validation loss: 2.3611922161553496

Epoch: 5| Step: 3
Training loss: 3.0579752922058105
Validation loss: 2.3706964344106694

Epoch: 5| Step: 4
Training loss: 2.4637742042541504
Validation loss: 2.3734673735915974

Epoch: 5| Step: 5
Training loss: 2.0352962017059326
Validation loss: 2.3646757423236804

Epoch: 5| Step: 6
Training loss: 2.545506238937378
Validation loss: 2.3687125867412937

Epoch: 5| Step: 7
Training loss: 3.177700996398926
Validation loss: 2.364104793917748

Epoch: 5| Step: 8
Training loss: 2.774880886077881
Validation loss: 2.36164241708735

Epoch: 5| Step: 9
Training loss: 2.4404754638671875
Validation loss: 2.3668310488423994

Epoch: 5| Step: 10
Training loss: 2.5724713802337646
Validation loss: 2.3623196232703423

Epoch: 222| Step: 0
Training loss: 2.5030429363250732
Validation loss: 2.3704242578116794

Epoch: 5| Step: 1
Training loss: 2.2683379650115967
Validation loss: 2.3768810149162047

Epoch: 5| Step: 2
Training loss: 2.8086864948272705
Validation loss: 2.380928885552191

Epoch: 5| Step: 3
Training loss: 2.163801908493042
Validation loss: 2.383659629411595

Epoch: 5| Step: 4
Training loss: 2.372959613800049
Validation loss: 2.390806826212073

Epoch: 5| Step: 5
Training loss: 2.4234025478363037
Validation loss: 2.389376160919025

Epoch: 5| Step: 6
Training loss: 2.3567841053009033
Validation loss: 2.383444211816275

Epoch: 5| Step: 7
Training loss: 3.053683280944824
Validation loss: 2.3902304967244468

Epoch: 5| Step: 8
Training loss: 2.6657676696777344
Validation loss: 2.3901756040511595

Epoch: 5| Step: 9
Training loss: 3.2647833824157715
Validation loss: 2.378007194047333

Epoch: 5| Step: 10
Training loss: 2.4858667850494385
Validation loss: 2.3775468744257444

Epoch: 223| Step: 0
Training loss: 2.5030300617218018
Validation loss: 2.3718036631102204

Epoch: 5| Step: 1
Training loss: 2.52878999710083
Validation loss: 2.364801514533258

Epoch: 5| Step: 2
Training loss: 3.690261125564575
Validation loss: 2.3772737415887977

Epoch: 5| Step: 3
Training loss: 2.3173470497131348
Validation loss: 2.368826017584852

Epoch: 5| Step: 4
Training loss: 2.9232096672058105
Validation loss: 2.3722659362259733

Epoch: 5| Step: 5
Training loss: 2.5803163051605225
Validation loss: 2.369569288787021

Epoch: 5| Step: 6
Training loss: 2.5973167419433594
Validation loss: 2.3748142309086298

Epoch: 5| Step: 7
Training loss: 2.214986801147461
Validation loss: 2.3840881419438187

Epoch: 5| Step: 8
Training loss: 2.049034833908081
Validation loss: 2.391915072676956

Epoch: 5| Step: 9
Training loss: 2.7471683025360107
Validation loss: 2.3836632185084845

Epoch: 5| Step: 10
Training loss: 2.234591007232666
Validation loss: 2.3821331813771236

Epoch: 224| Step: 0
Training loss: 3.039574146270752
Validation loss: 2.382919708887736

Epoch: 5| Step: 1
Training loss: 2.03812837600708
Validation loss: 2.3860491475751324

Epoch: 5| Step: 2
Training loss: 2.145698070526123
Validation loss: 2.380041014763617

Epoch: 5| Step: 3
Training loss: 2.31564998626709
Validation loss: 2.3801360463583343

Epoch: 5| Step: 4
Training loss: 2.4884841442108154
Validation loss: 2.380483491446382

Epoch: 5| Step: 5
Training loss: 2.8664767742156982
Validation loss: 2.3803362948920137

Epoch: 5| Step: 6
Training loss: 2.791877031326294
Validation loss: 2.3780554186913276

Epoch: 5| Step: 7
Training loss: 1.9967477321624756
Validation loss: 2.3783461201575493

Epoch: 5| Step: 8
Training loss: 2.5876262187957764
Validation loss: 2.378753856946063

Epoch: 5| Step: 9
Training loss: 3.224592685699463
Validation loss: 2.3817009566932597

Epoch: 5| Step: 10
Training loss: 2.8519845008850098
Validation loss: 2.3806260631930445

Epoch: 225| Step: 0
Training loss: 2.9828507900238037
Validation loss: 2.3823657753647014

Epoch: 5| Step: 1
Training loss: 2.4754810333251953
Validation loss: 2.395714775208504

Epoch: 5| Step: 2
Training loss: 2.339479446411133
Validation loss: 2.3911659127922467

Epoch: 5| Step: 3
Training loss: 2.1566998958587646
Validation loss: 2.3791480654029438

Epoch: 5| Step: 4
Training loss: 2.9838337898254395
Validation loss: 2.376901521477648

Epoch: 5| Step: 5
Training loss: 2.4628477096557617
Validation loss: 2.3885161902314875

Epoch: 5| Step: 6
Training loss: 1.7164335250854492
Validation loss: 2.3827235673063543

Epoch: 5| Step: 7
Training loss: 2.6391141414642334
Validation loss: 2.3732682043506252

Epoch: 5| Step: 8
Training loss: 2.9973225593566895
Validation loss: 2.379586822243147

Epoch: 5| Step: 9
Training loss: 2.527984619140625
Validation loss: 2.373413421774423

Epoch: 5| Step: 10
Training loss: 3.107804298400879
Validation loss: 2.3839910748184368

Epoch: 226| Step: 0
Training loss: 2.3990728855133057
Validation loss: 2.393528443510814

Epoch: 5| Step: 1
Training loss: 2.1385509967803955
Validation loss: 2.4000974444932837

Epoch: 5| Step: 2
Training loss: 2.7767271995544434
Validation loss: 2.4037643658217562

Epoch: 5| Step: 3
Training loss: 2.934811592102051
Validation loss: 2.409786262819844

Epoch: 5| Step: 4
Training loss: 2.5730319023132324
Validation loss: 2.4046922550406507

Epoch: 5| Step: 5
Training loss: 2.7679221630096436
Validation loss: 2.3926837341759795

Epoch: 5| Step: 6
Training loss: 2.728302001953125
Validation loss: 2.4079093830559843

Epoch: 5| Step: 7
Training loss: 2.185420513153076
Validation loss: 2.39551483174806

Epoch: 5| Step: 8
Training loss: 2.6770071983337402
Validation loss: 2.3957847061977593

Epoch: 5| Step: 9
Training loss: 2.5328218936920166
Validation loss: 2.3873015911348405

Epoch: 5| Step: 10
Training loss: 2.5293338298797607
Validation loss: 2.3822662112533406

Epoch: 227| Step: 0
Training loss: 3.0988731384277344
Validation loss: 2.3852187715550905

Epoch: 5| Step: 1
Training loss: 2.065582036972046
Validation loss: 2.382062658186882

Epoch: 5| Step: 2
Training loss: 3.1436715126037598
Validation loss: 2.389862255383563

Epoch: 5| Step: 3
Training loss: 1.8539882898330688
Validation loss: 2.3980694919504146

Epoch: 5| Step: 4
Training loss: 2.361825942993164
Validation loss: 2.399160195422429

Epoch: 5| Step: 5
Training loss: 1.894411325454712
Validation loss: 2.3956570856032835

Epoch: 5| Step: 6
Training loss: 2.6640095710754395
Validation loss: 2.3962774738188712

Epoch: 5| Step: 7
Training loss: 2.798961877822876
Validation loss: 2.410175061994983

Epoch: 5| Step: 8
Training loss: 2.565837860107422
Validation loss: 2.4195891682819655

Epoch: 5| Step: 9
Training loss: 2.4349169731140137
Validation loss: 2.4393196926322034

Epoch: 5| Step: 10
Training loss: 3.6123616695404053
Validation loss: 2.456092655017812

Epoch: 228| Step: 0
Training loss: 2.5410778522491455
Validation loss: 2.4684503078460693

Epoch: 5| Step: 1
Training loss: 3.022141218185425
Validation loss: 2.4578495666544926

Epoch: 5| Step: 2
Training loss: 2.440756320953369
Validation loss: 2.4667828570130053

Epoch: 5| Step: 3
Training loss: 2.6657235622406006
Validation loss: 2.4616236173978416

Epoch: 5| Step: 4
Training loss: 2.1195290088653564
Validation loss: 2.438434052210982

Epoch: 5| Step: 5
Training loss: 2.505056381225586
Validation loss: 2.4206316112190165

Epoch: 5| Step: 6
Training loss: 2.483154535293579
Validation loss: 2.4084600581917712

Epoch: 5| Step: 7
Training loss: 2.616414785385132
Validation loss: 2.3949824686973327

Epoch: 5| Step: 8
Training loss: 2.452800750732422
Validation loss: 2.3891161795585387

Epoch: 5| Step: 9
Training loss: 2.5113329887390137
Validation loss: 2.387809899545485

Epoch: 5| Step: 10
Training loss: 3.281637191772461
Validation loss: 2.3811347740952686

Epoch: 229| Step: 0
Training loss: 3.139371633529663
Validation loss: 2.3972247621064544

Epoch: 5| Step: 1
Training loss: 2.2013347148895264
Validation loss: 2.392648263644147

Epoch: 5| Step: 2
Training loss: 2.1287193298339844
Validation loss: 2.3868366543964674

Epoch: 5| Step: 3
Training loss: 3.127556324005127
Validation loss: 2.3818117469869633

Epoch: 5| Step: 4
Training loss: 2.6397595405578613
Validation loss: 2.3846106144689743

Epoch: 5| Step: 5
Training loss: 2.5544769763946533
Validation loss: 2.38231316433158

Epoch: 5| Step: 6
Training loss: 2.4038925170898438
Validation loss: 2.3766394033226916

Epoch: 5| Step: 7
Training loss: 2.5785646438598633
Validation loss: 2.3626712470926265

Epoch: 5| Step: 8
Training loss: 2.109236240386963
Validation loss: 2.368146482334342

Epoch: 5| Step: 9
Training loss: 2.5562000274658203
Validation loss: 2.3595293926936325

Epoch: 5| Step: 10
Training loss: 3.253213882446289
Validation loss: 2.359163443247477

Epoch: 230| Step: 0
Training loss: 2.441895008087158
Validation loss: 2.3575093361639206

Epoch: 5| Step: 1
Training loss: 3.1528310775756836
Validation loss: 2.360899099739649

Epoch: 5| Step: 2
Training loss: 2.7896130084991455
Validation loss: 2.367066696125974

Epoch: 5| Step: 3
Training loss: 2.7152695655822754
Validation loss: 2.3768957507225776

Epoch: 5| Step: 4
Training loss: 2.5091676712036133
Validation loss: 2.3840801459486767

Epoch: 5| Step: 5
Training loss: 2.7039809226989746
Validation loss: 2.383776585261027

Epoch: 5| Step: 6
Training loss: 2.526259183883667
Validation loss: 2.387077493052329

Epoch: 5| Step: 7
Training loss: 2.2745823860168457
Validation loss: 2.3873766340235227

Epoch: 5| Step: 8
Training loss: 2.27443265914917
Validation loss: 2.3932766299093924

Epoch: 5| Step: 9
Training loss: 2.077470302581787
Validation loss: 2.3873394535433863

Epoch: 5| Step: 10
Training loss: 2.927248239517212
Validation loss: 2.3797013631431003

Epoch: 231| Step: 0
Training loss: 2.816554546356201
Validation loss: 2.375716141475144

Epoch: 5| Step: 1
Training loss: 2.4788260459899902
Validation loss: 2.3800172805786133

Epoch: 5| Step: 2
Training loss: 1.7677547931671143
Validation loss: 2.375058845807147

Epoch: 5| Step: 3
Training loss: 2.5433712005615234
Validation loss: 2.3789460710299912

Epoch: 5| Step: 4
Training loss: 3.0435333251953125
Validation loss: 2.3745827572320097

Epoch: 5| Step: 5
Training loss: 2.261345624923706
Validation loss: 2.3814632328607703

Epoch: 5| Step: 6
Training loss: 2.093555212020874
Validation loss: 2.3812686422819733

Epoch: 5| Step: 7
Training loss: 2.5303969383239746
Validation loss: 2.3802882189391763

Epoch: 5| Step: 8
Training loss: 3.104616403579712
Validation loss: 2.3728805101045998

Epoch: 5| Step: 9
Training loss: 3.127143383026123
Validation loss: 2.3789572715759277

Epoch: 5| Step: 10
Training loss: 2.439018726348877
Validation loss: 2.379298048634683

Epoch: 232| Step: 0
Training loss: 2.1584715843200684
Validation loss: 2.377158034232355

Epoch: 5| Step: 1
Training loss: 1.7931878566741943
Validation loss: 2.3786991975640737

Epoch: 5| Step: 2
Training loss: 3.644737958908081
Validation loss: 2.3769259863002326

Epoch: 5| Step: 3
Training loss: 2.755748987197876
Validation loss: 2.3857306075352493

Epoch: 5| Step: 4
Training loss: 2.82236909866333
Validation loss: 2.388141482107101

Epoch: 5| Step: 5
Training loss: 2.6245055198669434
Validation loss: 2.3907679819291636

Epoch: 5| Step: 6
Training loss: 2.6035799980163574
Validation loss: 2.3872880935668945

Epoch: 5| Step: 7
Training loss: 2.1514554023742676
Validation loss: 2.3911556505387828

Epoch: 5| Step: 8
Training loss: 2.4982354640960693
Validation loss: 2.387795222702847

Epoch: 5| Step: 9
Training loss: 2.3656129837036133
Validation loss: 2.378060387026879

Epoch: 5| Step: 10
Training loss: 2.796621799468994
Validation loss: 2.375598430633545

Epoch: 233| Step: 0
Training loss: 2.83150053024292
Validation loss: 2.3726202467436432

Epoch: 5| Step: 1
Training loss: 2.560016632080078
Validation loss: 2.3754335398315103

Epoch: 5| Step: 2
Training loss: 2.600262403488159
Validation loss: 2.3724938823330786

Epoch: 5| Step: 3
Training loss: 2.488520383834839
Validation loss: 2.3695569653664865

Epoch: 5| Step: 4
Training loss: 2.216046094894409
Validation loss: 2.3679189605097615

Epoch: 5| Step: 5
Training loss: 2.6359734535217285
Validation loss: 2.374472607848465

Epoch: 5| Step: 6
Training loss: 2.5536837577819824
Validation loss: 2.3820119852660806

Epoch: 5| Step: 7
Training loss: 1.9575248956680298
Validation loss: 2.3868222723725023

Epoch: 5| Step: 8
Training loss: 2.8026156425476074
Validation loss: 2.3815683369995444

Epoch: 5| Step: 9
Training loss: 2.821962833404541
Validation loss: 2.387417152363767

Epoch: 5| Step: 10
Training loss: 3.060154676437378
Validation loss: 2.374752570224065

Epoch: 234| Step: 0
Training loss: 1.6968908309936523
Validation loss: 2.3658789255285777

Epoch: 5| Step: 1
Training loss: 3.015058994293213
Validation loss: 2.3643920549782376

Epoch: 5| Step: 2
Training loss: 2.994394540786743
Validation loss: 2.3691716835062993

Epoch: 5| Step: 3
Training loss: 2.616633653640747
Validation loss: 2.3663914152370986

Epoch: 5| Step: 4
Training loss: 2.484484910964966
Validation loss: 2.369331882845971

Epoch: 5| Step: 5
Training loss: 2.5809426307678223
Validation loss: 2.362465748222925

Epoch: 5| Step: 6
Training loss: 2.2689261436462402
Validation loss: 2.366550418638414

Epoch: 5| Step: 7
Training loss: 2.963106870651245
Validation loss: 2.372935382268762

Epoch: 5| Step: 8
Training loss: 2.214853525161743
Validation loss: 2.3669346019785893

Epoch: 5| Step: 9
Training loss: 2.5834286212921143
Validation loss: 2.3714998550312494

Epoch: 5| Step: 10
Training loss: 3.0391387939453125
Validation loss: 2.3694762440137964

Epoch: 235| Step: 0
Training loss: 2.330626964569092
Validation loss: 2.3720017274220786

Epoch: 5| Step: 1
Training loss: 3.436756134033203
Validation loss: 2.3811559369487147

Epoch: 5| Step: 2
Training loss: 2.6033644676208496
Validation loss: 2.3802705221278693

Epoch: 5| Step: 3
Training loss: 1.7585331201553345
Validation loss: 2.395919644704429

Epoch: 5| Step: 4
Training loss: 2.237046480178833
Validation loss: 2.4101131936555267

Epoch: 5| Step: 5
Training loss: 2.804959774017334
Validation loss: 2.4055781672077794

Epoch: 5| Step: 6
Training loss: 2.787508010864258
Validation loss: 2.399399959912864

Epoch: 5| Step: 7
Training loss: 3.2926299571990967
Validation loss: 2.4071566622744323

Epoch: 5| Step: 8
Training loss: 2.474668025970459
Validation loss: 2.3969458892781246

Epoch: 5| Step: 9
Training loss: 2.423060655593872
Validation loss: 2.3853153464614705

Epoch: 5| Step: 10
Training loss: 2.080521821975708
Validation loss: 2.383493461916524

Epoch: 236| Step: 0
Training loss: 2.842510938644409
Validation loss: 2.375756066332581

Epoch: 5| Step: 1
Training loss: 2.861184597015381
Validation loss: 2.373650445733019

Epoch: 5| Step: 2
Training loss: 2.735858201980591
Validation loss: 2.371751682732695

Epoch: 5| Step: 3
Training loss: 2.2293953895568848
Validation loss: 2.366434456199728

Epoch: 5| Step: 4
Training loss: 2.578842878341675
Validation loss: 2.3608180938228482

Epoch: 5| Step: 5
Training loss: 2.31143856048584
Validation loss: 2.358371270600186

Epoch: 5| Step: 6
Training loss: 2.6715328693389893
Validation loss: 2.362212945056218

Epoch: 5| Step: 7
Training loss: 2.7266783714294434
Validation loss: 2.3592377119166876

Epoch: 5| Step: 8
Training loss: 2.935671091079712
Validation loss: 2.354376037915548

Epoch: 5| Step: 9
Training loss: 2.1650896072387695
Validation loss: 2.359430774565666

Epoch: 5| Step: 10
Training loss: 2.3897242546081543
Validation loss: 2.3674319892801265

Epoch: 237| Step: 0
Training loss: 2.5427136421203613
Validation loss: 2.370014944384175

Epoch: 5| Step: 1
Training loss: 2.902553081512451
Validation loss: 2.3832059739738383

Epoch: 5| Step: 2
Training loss: 2.290714740753174
Validation loss: 2.3832901216322377

Epoch: 5| Step: 3
Training loss: 2.1666312217712402
Validation loss: 2.381301044135965

Epoch: 5| Step: 4
Training loss: 2.4958138465881348
Validation loss: 2.3893780336585095

Epoch: 5| Step: 5
Training loss: 2.3424201011657715
Validation loss: 2.387243555438134

Epoch: 5| Step: 6
Training loss: 3.203233242034912
Validation loss: 2.3886381759438464

Epoch: 5| Step: 7
Training loss: 2.6117770671844482
Validation loss: 2.3841723203659058

Epoch: 5| Step: 8
Training loss: 2.4688727855682373
Validation loss: 2.3880983526988695

Epoch: 5| Step: 9
Training loss: 2.4133620262145996
Validation loss: 2.386546988641062

Epoch: 5| Step: 10
Training loss: 2.8072140216827393
Validation loss: 2.371402648187453

Epoch: 238| Step: 0
Training loss: 2.9209885597229004
Validation loss: 2.3603605801059353

Epoch: 5| Step: 1
Training loss: 2.5224499702453613
Validation loss: 2.3558781352094424

Epoch: 5| Step: 2
Training loss: 2.697274684906006
Validation loss: 2.3565223140101277

Epoch: 5| Step: 3
Training loss: 2.9341788291931152
Validation loss: 2.349894159583635

Epoch: 5| Step: 4
Training loss: 2.4499409198760986
Validation loss: 2.353534467758671

Epoch: 5| Step: 5
Training loss: 1.5507186651229858
Validation loss: 2.352347173998433

Epoch: 5| Step: 6
Training loss: 2.2405550479888916
Validation loss: 2.358587885415682

Epoch: 5| Step: 7
Training loss: 2.5512709617614746
Validation loss: 2.379104547603156

Epoch: 5| Step: 8
Training loss: 2.656046152114868
Validation loss: 2.3764240716093328

Epoch: 5| Step: 9
Training loss: 2.7796082496643066
Validation loss: 2.375827968761485

Epoch: 5| Step: 10
Training loss: 3.0117180347442627
Validation loss: 2.3691899622640302

Epoch: 239| Step: 0
Training loss: 2.4150338172912598
Validation loss: 2.3717343525219987

Epoch: 5| Step: 1
Training loss: 2.7305729389190674
Validation loss: 2.3747553492105133

Epoch: 5| Step: 2
Training loss: 2.0239193439483643
Validation loss: 2.383339351223361

Epoch: 5| Step: 3
Training loss: 3.138272762298584
Validation loss: 2.3906984406132854

Epoch: 5| Step: 4
Training loss: 2.511638641357422
Validation loss: 2.3921384042309177

Epoch: 5| Step: 5
Training loss: 2.5939018726348877
Validation loss: 2.3971640012597524

Epoch: 5| Step: 6
Training loss: 2.0920729637145996
Validation loss: 2.3977521978398806

Epoch: 5| Step: 7
Training loss: 2.314742088317871
Validation loss: 2.3799767596747285

Epoch: 5| Step: 8
Training loss: 3.2700912952423096
Validation loss: 2.3792768678357525

Epoch: 5| Step: 9
Training loss: 2.31986403465271
Validation loss: 2.3762582425148255

Epoch: 5| Step: 10
Training loss: 2.784191131591797
Validation loss: 2.375131648073914

Epoch: 240| Step: 0
Training loss: 2.1864874362945557
Validation loss: 2.369636002407279

Epoch: 5| Step: 1
Training loss: 2.4103598594665527
Validation loss: 2.3669908892723823

Epoch: 5| Step: 2
Training loss: 2.7853970527648926
Validation loss: 2.378532104594733

Epoch: 5| Step: 3
Training loss: 2.3848490715026855
Validation loss: 2.367265696166664

Epoch: 5| Step: 4
Training loss: 2.334672689437866
Validation loss: 2.365291513422484

Epoch: 5| Step: 5
Training loss: 2.2413010597229004
Validation loss: 2.369651284269107

Epoch: 5| Step: 6
Training loss: 3.150648593902588
Validation loss: 2.36572423032535

Epoch: 5| Step: 7
Training loss: 2.7841601371765137
Validation loss: 2.364254031130063

Epoch: 5| Step: 8
Training loss: 2.5287561416625977
Validation loss: 2.3681431355014926

Epoch: 5| Step: 9
Training loss: 2.1404266357421875
Validation loss: 2.373417613326862

Epoch: 5| Step: 10
Training loss: 3.2282426357269287
Validation loss: 2.3911742420606714

Epoch: 241| Step: 0
Training loss: 2.5960288047790527
Validation loss: 2.393080626764605

Epoch: 5| Step: 1
Training loss: 2.192488431930542
Validation loss: 2.38556920841176

Epoch: 5| Step: 2
Training loss: 2.9838695526123047
Validation loss: 2.3894322226124425

Epoch: 5| Step: 3
Training loss: 2.9244742393493652
Validation loss: 2.3665419983607467

Epoch: 5| Step: 4
Training loss: 2.460205554962158
Validation loss: 2.361378377483737

Epoch: 5| Step: 5
Training loss: 2.5852370262145996
Validation loss: 2.3520455514231036

Epoch: 5| Step: 6
Training loss: 2.2819015979766846
Validation loss: 2.348545656409315

Epoch: 5| Step: 7
Training loss: 2.249373435974121
Validation loss: 2.3492891711573445

Epoch: 5| Step: 8
Training loss: 2.583303213119507
Validation loss: 2.3502339855317147

Epoch: 5| Step: 9
Training loss: 3.036763906478882
Validation loss: 2.349010129128733

Epoch: 5| Step: 10
Training loss: 2.1245782375335693
Validation loss: 2.3448212582577943

Epoch: 242| Step: 0
Training loss: 3.0189719200134277
Validation loss: 2.338125128899851

Epoch: 5| Step: 1
Training loss: 2.6528658866882324
Validation loss: 2.3487526614178895

Epoch: 5| Step: 2
Training loss: 2.8026719093322754
Validation loss: 2.3532956748880367

Epoch: 5| Step: 3
Training loss: 1.3210817575454712
Validation loss: 2.361691341605238

Epoch: 5| Step: 4
Training loss: 2.58687162399292
Validation loss: 2.377440906340076

Epoch: 5| Step: 5
Training loss: 2.481887102127075
Validation loss: 2.36384093376898

Epoch: 5| Step: 6
Training loss: 2.2432408332824707
Validation loss: 2.3701615051556657

Epoch: 5| Step: 7
Training loss: 2.796926498413086
Validation loss: 2.3548774052691717

Epoch: 5| Step: 8
Training loss: 3.431790828704834
Validation loss: 2.3699827296759493

Epoch: 5| Step: 9
Training loss: 2.268500328063965
Validation loss: 2.3643103209874963

Epoch: 5| Step: 10
Training loss: 2.4561305046081543
Validation loss: 2.3718387080777075

Epoch: 243| Step: 0
Training loss: 2.4509799480438232
Validation loss: 2.3768706129443262

Epoch: 5| Step: 1
Training loss: 3.2188239097595215
Validation loss: 2.3678014727048975

Epoch: 5| Step: 2
Training loss: 2.47590970993042
Validation loss: 2.3714220472561416

Epoch: 5| Step: 3
Training loss: 2.278257369995117
Validation loss: 2.3693480491638184

Epoch: 5| Step: 4
Training loss: 1.986964464187622
Validation loss: 2.3681738427890244

Epoch: 5| Step: 5
Training loss: 1.8744007349014282
Validation loss: 2.3565107135362524

Epoch: 5| Step: 6
Training loss: 3.311925172805786
Validation loss: 2.356815743189986

Epoch: 5| Step: 7
Training loss: 2.685393810272217
Validation loss: 2.3478393811051563

Epoch: 5| Step: 8
Training loss: 3.290123462677002
Validation loss: 2.351362928267448

Epoch: 5| Step: 9
Training loss: 2.0818114280700684
Validation loss: 2.3506725988080426

Epoch: 5| Step: 10
Training loss: 2.3201000690460205
Validation loss: 2.3553999470126246

Epoch: 244| Step: 0
Training loss: 2.2756874561309814
Validation loss: 2.3430845891275713

Epoch: 5| Step: 1
Training loss: 2.9527924060821533
Validation loss: 2.3453752225445164

Epoch: 5| Step: 2
Training loss: 2.3439955711364746
Validation loss: 2.349278842249224

Epoch: 5| Step: 3
Training loss: 2.298614025115967
Validation loss: 2.3442986524233254

Epoch: 5| Step: 4
Training loss: 2.800114393234253
Validation loss: 2.343803669816704

Epoch: 5| Step: 5
Training loss: 2.2424817085266113
Validation loss: 2.341563259401629

Epoch: 5| Step: 6
Training loss: 2.4120593070983887
Validation loss: 2.3368854394523044

Epoch: 5| Step: 7
Training loss: 2.6360414028167725
Validation loss: 2.3413329060359667

Epoch: 5| Step: 8
Training loss: 3.107182025909424
Validation loss: 2.344341911295409

Epoch: 5| Step: 9
Training loss: 2.7944438457489014
Validation loss: 2.3453139387151247

Epoch: 5| Step: 10
Training loss: 2.1330835819244385
Validation loss: 2.3517735337698333

Epoch: 245| Step: 0
Training loss: 2.9680960178375244
Validation loss: 2.3521248653370845

Epoch: 5| Step: 1
Training loss: 2.287076711654663
Validation loss: 2.350409723097278

Epoch: 5| Step: 2
Training loss: 2.568864345550537
Validation loss: 2.3373168130074777

Epoch: 5| Step: 3
Training loss: 2.9435524940490723
Validation loss: 2.3399120812774985

Epoch: 5| Step: 4
Training loss: 2.1767537593841553
Validation loss: 2.3341552365210747

Epoch: 5| Step: 5
Training loss: 2.606999158859253
Validation loss: 2.337279604327294

Epoch: 5| Step: 6
Training loss: 2.9806816577911377
Validation loss: 2.330444966593096

Epoch: 5| Step: 7
Training loss: 2.1778721809387207
Validation loss: 2.3312284690077587

Epoch: 5| Step: 8
Training loss: 2.5636239051818848
Validation loss: 2.3419726638383764

Epoch: 5| Step: 9
Training loss: 2.6361758708953857
Validation loss: 2.3409135777463197

Epoch: 5| Step: 10
Training loss: 2.018587827682495
Validation loss: 2.3461797621942337

Epoch: 246| Step: 0
Training loss: 2.330503225326538
Validation loss: 2.3534691154315905

Epoch: 5| Step: 1
Training loss: 3.1200711727142334
Validation loss: 2.3515478308482836

Epoch: 5| Step: 2
Training loss: 2.4022490978240967
Validation loss: 2.3643501086901595

Epoch: 5| Step: 3
Training loss: 3.272414445877075
Validation loss: 2.3652928593338176

Epoch: 5| Step: 4
Training loss: 2.152352809906006
Validation loss: 2.3738145366791756

Epoch: 5| Step: 5
Training loss: 2.387915849685669
Validation loss: 2.373395932618008

Epoch: 5| Step: 6
Training loss: 2.305388927459717
Validation loss: 2.3773081917916574

Epoch: 5| Step: 7
Training loss: 2.4321084022521973
Validation loss: 2.3628400474466305

Epoch: 5| Step: 8
Training loss: 2.1520066261291504
Validation loss: 2.362350476685391

Epoch: 5| Step: 9
Training loss: 3.0333666801452637
Validation loss: 2.3529088420252644

Epoch: 5| Step: 10
Training loss: 2.3647055625915527
Validation loss: 2.3471682353686263

Epoch: 247| Step: 0
Training loss: 2.3115203380584717
Validation loss: 2.3330908308746996

Epoch: 5| Step: 1
Training loss: 2.1994881629943848
Validation loss: 2.334859986459055

Epoch: 5| Step: 2
Training loss: 2.6495306491851807
Validation loss: 2.322509850225141

Epoch: 5| Step: 3
Training loss: 2.6920218467712402
Validation loss: 2.3258732877751833

Epoch: 5| Step: 4
Training loss: 2.8043532371520996
Validation loss: 2.3163418000744236

Epoch: 5| Step: 5
Training loss: 2.0052621364593506
Validation loss: 2.319218611204496

Epoch: 5| Step: 6
Training loss: 3.2615489959716797
Validation loss: 2.318378525395547

Epoch: 5| Step: 7
Training loss: 1.9037811756134033
Validation loss: 2.3168832384130007

Epoch: 5| Step: 8
Training loss: 3.065216541290283
Validation loss: 2.3255575036489837

Epoch: 5| Step: 9
Training loss: 2.1580190658569336
Validation loss: 2.323867772215156

Epoch: 5| Step: 10
Training loss: 3.293609857559204
Validation loss: 2.318572521209717

Epoch: 248| Step: 0
Training loss: 3.5936665534973145
Validation loss: 2.3241872864384807

Epoch: 5| Step: 1
Training loss: 2.4124057292938232
Validation loss: 2.326529754105435

Epoch: 5| Step: 2
Training loss: 2.657625913619995
Validation loss: 2.323899351140504

Epoch: 5| Step: 3
Training loss: 1.8382213115692139
Validation loss: 2.3333401103173532

Epoch: 5| Step: 4
Training loss: 2.21234393119812
Validation loss: 2.334136732162968

Epoch: 5| Step: 5
Training loss: 2.3811347484588623
Validation loss: 2.347907684182608

Epoch: 5| Step: 6
Training loss: 2.170889377593994
Validation loss: 2.3514441649119058

Epoch: 5| Step: 7
Training loss: 2.4428157806396484
Validation loss: 2.371143694846861

Epoch: 5| Step: 8
Training loss: 2.94810152053833
Validation loss: 2.3721303939819336

Epoch: 5| Step: 9
Training loss: 2.3745484352111816
Validation loss: 2.3739370351196616

Epoch: 5| Step: 10
Training loss: 3.164000988006592
Validation loss: 2.3683225108731176

Epoch: 249| Step: 0
Training loss: 1.8509804010391235
Validation loss: 2.36926019319924

Epoch: 5| Step: 1
Training loss: 3.1961536407470703
Validation loss: 2.346396884610576

Epoch: 5| Step: 2
Training loss: 2.634326696395874
Validation loss: 2.3481863673015306

Epoch: 5| Step: 3
Training loss: 2.976752758026123
Validation loss: 2.349165093514227

Epoch: 5| Step: 4
Training loss: 2.2737441062927246
Validation loss: 2.3379896430559057

Epoch: 5| Step: 5
Training loss: 2.67694091796875
Validation loss: 2.3387336628411406

Epoch: 5| Step: 6
Training loss: 2.1400339603424072
Validation loss: 2.3343579640952488

Epoch: 5| Step: 7
Training loss: 2.788559675216675
Validation loss: 2.336958485264932

Epoch: 5| Step: 8
Training loss: 2.2602360248565674
Validation loss: 2.330685584775863

Epoch: 5| Step: 9
Training loss: 2.6448709964752197
Validation loss: 2.329761548708844

Epoch: 5| Step: 10
Training loss: 2.529775619506836
Validation loss: 2.335417442424323

Epoch: 250| Step: 0
Training loss: 2.711958885192871
Validation loss: 2.3354232823976906

Epoch: 5| Step: 1
Training loss: 3.036349058151245
Validation loss: 2.340461036210419

Epoch: 5| Step: 2
Training loss: 2.5027339458465576
Validation loss: 2.343283199494885

Epoch: 5| Step: 3
Training loss: 2.0649445056915283
Validation loss: 2.3568029544686757

Epoch: 5| Step: 4
Training loss: 2.0203089714050293
Validation loss: 2.357291765110467

Epoch: 5| Step: 5
Training loss: 2.5152640342712402
Validation loss: 2.3591443595065864

Epoch: 5| Step: 6
Training loss: 2.4972527027130127
Validation loss: 2.356404872350795

Epoch: 5| Step: 7
Training loss: 2.8477420806884766
Validation loss: 2.3598846389401342

Epoch: 5| Step: 8
Training loss: 2.18314790725708
Validation loss: 2.3510792563038487

Epoch: 5| Step: 9
Training loss: 2.5728812217712402
Validation loss: 2.3451862053204606

Epoch: 5| Step: 10
Training loss: 3.1407217979431152
Validation loss: 2.351612356401259

Epoch: 251| Step: 0
Training loss: 2.734431505203247
Validation loss: 2.350281471847206

Epoch: 5| Step: 1
Training loss: 2.4167561531066895
Validation loss: 2.366263525460356

Epoch: 5| Step: 2
Training loss: 2.798706531524658
Validation loss: 2.373035127116788

Epoch: 5| Step: 3
Training loss: 2.3655519485473633
Validation loss: 2.381103382315687

Epoch: 5| Step: 4
Training loss: 2.4239916801452637
Validation loss: 2.3972738224972963

Epoch: 5| Step: 5
Training loss: 2.7745285034179688
Validation loss: 2.4032864955163773

Epoch: 5| Step: 6
Training loss: 2.7579946517944336
Validation loss: 2.400273320495441

Epoch: 5| Step: 7
Training loss: 2.000295877456665
Validation loss: 2.391094346200266

Epoch: 5| Step: 8
Training loss: 3.1719489097595215
Validation loss: 2.371064539878599

Epoch: 5| Step: 9
Training loss: 2.3935070037841797
Validation loss: 2.357096379803073

Epoch: 5| Step: 10
Training loss: 2.1567978858947754
Validation loss: 2.34639169580193

Epoch: 252| Step: 0
Training loss: 2.4362740516662598
Validation loss: 2.3364500255994898

Epoch: 5| Step: 1
Training loss: 2.1550517082214355
Validation loss: 2.3323301910072245

Epoch: 5| Step: 2
Training loss: 2.065528154373169
Validation loss: 2.3220132320157942

Epoch: 5| Step: 3
Training loss: 2.5773959159851074
Validation loss: 2.3163449225887174

Epoch: 5| Step: 4
Training loss: 3.137545585632324
Validation loss: 2.3213251816329135

Epoch: 5| Step: 5
Training loss: 1.8937246799468994
Validation loss: 2.3240643726882113

Epoch: 5| Step: 6
Training loss: 2.9949605464935303
Validation loss: 2.3249531715146956

Epoch: 5| Step: 7
Training loss: 2.7838549613952637
Validation loss: 2.329671741813742

Epoch: 5| Step: 8
Training loss: 2.397455930709839
Validation loss: 2.320640781874298

Epoch: 5| Step: 9
Training loss: 3.250683307647705
Validation loss: 2.32127974366629

Epoch: 5| Step: 10
Training loss: 2.449732780456543
Validation loss: 2.3234578306956957

Epoch: 253| Step: 0
Training loss: 2.887582778930664
Validation loss: 2.3203167376979703

Epoch: 5| Step: 1
Training loss: 2.260819911956787
Validation loss: 2.325465702241467

Epoch: 5| Step: 2
Training loss: 2.128815174102783
Validation loss: 2.334272276970648

Epoch: 5| Step: 3
Training loss: 2.7285261154174805
Validation loss: 2.3290320211841213

Epoch: 5| Step: 4
Training loss: 2.8200325965881348
Validation loss: 2.3372692972101192

Epoch: 5| Step: 5
Training loss: 2.452388286590576
Validation loss: 2.333253070872317

Epoch: 5| Step: 6
Training loss: 2.525059461593628
Validation loss: 2.3472884470416653

Epoch: 5| Step: 7
Training loss: 2.714110851287842
Validation loss: 2.3464311835586384

Epoch: 5| Step: 8
Training loss: 2.633481502532959
Validation loss: 2.3643336552445606

Epoch: 5| Step: 9
Training loss: 2.181351661682129
Validation loss: 2.36850901316571

Epoch: 5| Step: 10
Training loss: 2.7408573627471924
Validation loss: 2.360796138804446

Epoch: 254| Step: 0
Training loss: 2.7080061435699463
Validation loss: 2.3604546336717505

Epoch: 5| Step: 1
Training loss: 2.870365619659424
Validation loss: 2.3593665015312935

Epoch: 5| Step: 2
Training loss: 2.008267641067505
Validation loss: 2.3520031693161174

Epoch: 5| Step: 3
Training loss: 2.306323766708374
Validation loss: 2.3557879617137294

Epoch: 5| Step: 4
Training loss: 2.202415943145752
Validation loss: 2.349336821545837

Epoch: 5| Step: 5
Training loss: 2.7523646354675293
Validation loss: 2.3459961363064346

Epoch: 5| Step: 6
Training loss: 2.6485660076141357
Validation loss: 2.347571643449927

Epoch: 5| Step: 7
Training loss: 2.559354782104492
Validation loss: 2.340839914096299

Epoch: 5| Step: 8
Training loss: 2.9226913452148438
Validation loss: 2.3396491209665933

Epoch: 5| Step: 9
Training loss: 2.4583561420440674
Validation loss: 2.336677159032514

Epoch: 5| Step: 10
Training loss: 2.4014596939086914
Validation loss: 2.332315621837493

Epoch: 255| Step: 0
Training loss: 2.5445477962493896
Validation loss: 2.3245295183632964

Epoch: 5| Step: 1
Training loss: 2.3388113975524902
Validation loss: 2.3258726007194928

Epoch: 5| Step: 2
Training loss: 1.9834108352661133
Validation loss: 2.330307896419238

Epoch: 5| Step: 3
Training loss: 2.761730670928955
Validation loss: 2.3264953039025746

Epoch: 5| Step: 4
Training loss: 2.392014265060425
Validation loss: 2.3402676300335954

Epoch: 5| Step: 5
Training loss: 2.6007227897644043
Validation loss: 2.3505979661018617

Epoch: 5| Step: 6
Training loss: 2.858677387237549
Validation loss: 2.3557474561916885

Epoch: 5| Step: 7
Training loss: 2.99631929397583
Validation loss: 2.348440276679172

Epoch: 5| Step: 8
Training loss: 2.639146089553833
Validation loss: 2.3571490215998825

Epoch: 5| Step: 9
Training loss: 1.8476839065551758
Validation loss: 2.3536575404546594

Epoch: 5| Step: 10
Training loss: 3.0785045623779297
Validation loss: 2.350130095276781

Epoch: 256| Step: 0
Training loss: 2.239945411682129
Validation loss: 2.337629146473382

Epoch: 5| Step: 1
Training loss: 2.742915391921997
Validation loss: 2.3329857780087377

Epoch: 5| Step: 2
Training loss: 2.9583740234375
Validation loss: 2.339570117253129

Epoch: 5| Step: 3
Training loss: 2.2885982990264893
Validation loss: 2.3365885288484636

Epoch: 5| Step: 4
Training loss: 2.521934986114502
Validation loss: 2.3405562446963404

Epoch: 5| Step: 5
Training loss: 2.5434882640838623
Validation loss: 2.339973085670061

Epoch: 5| Step: 6
Training loss: 2.516237497329712
Validation loss: 2.342505283253167

Epoch: 5| Step: 7
Training loss: 2.863478660583496
Validation loss: 2.3479913485947477

Epoch: 5| Step: 8
Training loss: 2.544626235961914
Validation loss: 2.3591538065223285

Epoch: 5| Step: 9
Training loss: 2.187340259552002
Validation loss: 2.3593648428558023

Epoch: 5| Step: 10
Training loss: 2.4324653148651123
Validation loss: 2.3543254585676294

Epoch: 257| Step: 0
Training loss: 2.4135327339172363
Validation loss: 2.3581596395020843

Epoch: 5| Step: 1
Training loss: 2.4965274333953857
Validation loss: 2.3645596683666272

Epoch: 5| Step: 2
Training loss: 2.6038095951080322
Validation loss: 2.3527443434602473

Epoch: 5| Step: 3
Training loss: 2.959137201309204
Validation loss: 2.3565627426229496

Epoch: 5| Step: 4
Training loss: 2.637667655944824
Validation loss: 2.3587469977717244

Epoch: 5| Step: 5
Training loss: 2.889772653579712
Validation loss: 2.355495665663032

Epoch: 5| Step: 6
Training loss: 2.5044851303100586
Validation loss: 2.359730310337518

Epoch: 5| Step: 7
Training loss: 1.9194761514663696
Validation loss: 2.3476070639907674

Epoch: 5| Step: 8
Training loss: 2.6608333587646484
Validation loss: 2.3509019190265286

Epoch: 5| Step: 9
Training loss: 2.343076229095459
Validation loss: 2.348024252922304

Epoch: 5| Step: 10
Training loss: 2.2781200408935547
Validation loss: 2.33076814938617

Epoch: 258| Step: 0
Training loss: 1.8693195581436157
Validation loss: 2.3303924824601863

Epoch: 5| Step: 1
Training loss: 2.0268752574920654
Validation loss: 2.3253756287277385

Epoch: 5| Step: 2
Training loss: 2.6287899017333984
Validation loss: 2.3259234966770297

Epoch: 5| Step: 3
Training loss: 2.621497631072998
Validation loss: 2.323906483188752

Epoch: 5| Step: 4
Training loss: 2.4964663982391357
Validation loss: 2.3261156184698946

Epoch: 5| Step: 5
Training loss: 2.3610310554504395
Validation loss: 2.3286084590419645

Epoch: 5| Step: 6
Training loss: 2.2278244495391846
Validation loss: 2.341391953088904

Epoch: 5| Step: 7
Training loss: 3.1459667682647705
Validation loss: 2.3472183724885345

Epoch: 5| Step: 8
Training loss: 2.7364096641540527
Validation loss: 2.343650766598281

Epoch: 5| Step: 9
Training loss: 2.7472329139709473
Validation loss: 2.348913977223058

Epoch: 5| Step: 10
Training loss: 2.9861607551574707
Validation loss: 2.346940594334756

Epoch: 259| Step: 0
Training loss: 2.4877257347106934
Validation loss: 2.3453221962016118

Epoch: 5| Step: 1
Training loss: 2.7131128311157227
Validation loss: 2.3460204575651433

Epoch: 5| Step: 2
Training loss: 2.7241649627685547
Validation loss: 2.3555041436226136

Epoch: 5| Step: 3
Training loss: 2.6591594219207764
Validation loss: 2.348576643133676

Epoch: 5| Step: 4
Training loss: 2.5355446338653564
Validation loss: 2.361734059549147

Epoch: 5| Step: 5
Training loss: 2.6686389446258545
Validation loss: 2.3718526337736394

Epoch: 5| Step: 6
Training loss: 2.743504047393799
Validation loss: 2.3549377841334187

Epoch: 5| Step: 7
Training loss: 2.2714855670928955
Validation loss: 2.3532364855530443

Epoch: 5| Step: 8
Training loss: 2.1578209400177
Validation loss: 2.3516393835826586

Epoch: 5| Step: 9
Training loss: 2.1707844734191895
Validation loss: 2.3476439393976682

Epoch: 5| Step: 10
Training loss: 2.63684344291687
Validation loss: 2.336850884140179

Epoch: 260| Step: 0
Training loss: 3.390521287918091
Validation loss: 2.337965924252746

Epoch: 5| Step: 1
Training loss: 2.3507626056671143
Validation loss: 2.331946631913544

Epoch: 5| Step: 2
Training loss: 2.2287888526916504
Validation loss: 2.3232290796054307

Epoch: 5| Step: 3
Training loss: 2.6045215129852295
Validation loss: 2.32926953992536

Epoch: 5| Step: 4
Training loss: 2.4510555267333984
Validation loss: 2.3224397731083695

Epoch: 5| Step: 5
Training loss: 2.019926071166992
Validation loss: 2.3269338787242932

Epoch: 5| Step: 6
Training loss: 2.4797239303588867
Validation loss: 2.3446565084559943

Epoch: 5| Step: 7
Training loss: 2.928055763244629
Validation loss: 2.3569251901359967

Epoch: 5| Step: 8
Training loss: 2.3963675498962402
Validation loss: 2.380409025376843

Epoch: 5| Step: 9
Training loss: 2.5767414569854736
Validation loss: 2.396769687693606

Epoch: 5| Step: 10
Training loss: 2.396522045135498
Validation loss: 2.426967692631547

Epoch: 261| Step: 0
Training loss: 3.249764919281006
Validation loss: 2.409308341241652

Epoch: 5| Step: 1
Training loss: 2.209045171737671
Validation loss: 2.4075495145654164

Epoch: 5| Step: 2
Training loss: 2.6524250507354736
Validation loss: 2.380360608459801

Epoch: 5| Step: 3
Training loss: 2.4938666820526123
Validation loss: 2.3688362670201126

Epoch: 5| Step: 4
Training loss: 3.031521797180176
Validation loss: 2.3361820251710954

Epoch: 5| Step: 5
Training loss: 2.75948166847229
Validation loss: 2.323295098479076

Epoch: 5| Step: 6
Training loss: 2.776623249053955
Validation loss: 2.322217197828395

Epoch: 5| Step: 7
Training loss: 2.1952598094940186
Validation loss: 2.3221583930394982

Epoch: 5| Step: 8
Training loss: 2.4890055656433105
Validation loss: 2.3227085426289547

Epoch: 5| Step: 9
Training loss: 2.3288702964782715
Validation loss: 2.321320346606675

Epoch: 5| Step: 10
Training loss: 1.6694960594177246
Validation loss: 2.3223211560198056

Epoch: 262| Step: 0
Training loss: 2.6631884574890137
Validation loss: 2.325609735263291

Epoch: 5| Step: 1
Training loss: 2.48681902885437
Validation loss: 2.328951512613604

Epoch: 5| Step: 2
Training loss: 2.557425022125244
Validation loss: 2.332993604803598

Epoch: 5| Step: 3
Training loss: 1.798977255821228
Validation loss: 2.335689144749795

Epoch: 5| Step: 4
Training loss: 3.2012760639190674
Validation loss: 2.328819433848063

Epoch: 5| Step: 5
Training loss: 2.299821138381958
Validation loss: 2.3208361389816448

Epoch: 5| Step: 6
Training loss: 2.382917881011963
Validation loss: 2.3365898927052817

Epoch: 5| Step: 7
Training loss: 2.112166404724121
Validation loss: 2.338627694755472

Epoch: 5| Step: 8
Training loss: 2.6271402835845947
Validation loss: 2.3495580611690396

Epoch: 5| Step: 9
Training loss: 3.2821452617645264
Validation loss: 2.347916031396517

Epoch: 5| Step: 10
Training loss: 2.3654704093933105
Validation loss: 2.3464504493180143

Epoch: 263| Step: 0
Training loss: 2.617147922515869
Validation loss: 2.3449275903804327

Epoch: 5| Step: 1
Training loss: 2.3129591941833496
Validation loss: 2.3393508606059576

Epoch: 5| Step: 2
Training loss: 2.978834390640259
Validation loss: 2.343418680211549

Epoch: 5| Step: 3
Training loss: 2.3380284309387207
Validation loss: 2.3337693496416976

Epoch: 5| Step: 4
Training loss: 2.7594189643859863
Validation loss: 2.3261230299549718

Epoch: 5| Step: 5
Training loss: 2.3730533123016357
Validation loss: 2.3294042054043023

Epoch: 5| Step: 6
Training loss: 1.9872957468032837
Validation loss: 2.3475516098801807

Epoch: 5| Step: 7
Training loss: 2.949063539505005
Validation loss: 2.347616805825182

Epoch: 5| Step: 8
Training loss: 2.572366237640381
Validation loss: 2.3508306933987524

Epoch: 5| Step: 9
Training loss: 2.276581048965454
Validation loss: 2.358225631457503

Epoch: 5| Step: 10
Training loss: 2.5896811485290527
Validation loss: 2.3481194665355067

Epoch: 264| Step: 0
Training loss: 3.000769853591919
Validation loss: 2.3388756680232223

Epoch: 5| Step: 1
Training loss: 2.5688724517822266
Validation loss: 2.3292868009177585

Epoch: 5| Step: 2
Training loss: 2.895477533340454
Validation loss: 2.3256644510453746

Epoch: 5| Step: 3
Training loss: 2.777841567993164
Validation loss: 2.328686744936051

Epoch: 5| Step: 4
Training loss: 2.1271440982818604
Validation loss: 2.3222242811674714

Epoch: 5| Step: 5
Training loss: 2.5188326835632324
Validation loss: 2.3340139619765745

Epoch: 5| Step: 6
Training loss: 2.213500499725342
Validation loss: 2.3240157942618094

Epoch: 5| Step: 7
Training loss: 2.487865447998047
Validation loss: 2.321061490684427

Epoch: 5| Step: 8
Training loss: 2.1914725303649902
Validation loss: 2.32314545108426

Epoch: 5| Step: 9
Training loss: 2.4542396068573
Validation loss: 2.329218417085627

Epoch: 5| Step: 10
Training loss: 2.3835198879241943
Validation loss: 2.3312467105927004

Epoch: 265| Step: 0
Training loss: 1.997616171836853
Validation loss: 2.3275928048677343

Epoch: 5| Step: 1
Training loss: 2.1888020038604736
Validation loss: 2.3455503127908193

Epoch: 5| Step: 2
Training loss: 2.777749538421631
Validation loss: 2.363542955408814

Epoch: 5| Step: 3
Training loss: 2.884477138519287
Validation loss: 2.358944600628268

Epoch: 5| Step: 4
Training loss: 2.417837381362915
Validation loss: 2.375218455509473

Epoch: 5| Step: 5
Training loss: 2.7047317028045654
Validation loss: 2.3685395973984913

Epoch: 5| Step: 6
Training loss: 2.3702473640441895
Validation loss: 2.3774234402564263

Epoch: 5| Step: 7
Training loss: 3.000267505645752
Validation loss: 2.3661293675822597

Epoch: 5| Step: 8
Training loss: 1.723663568496704
Validation loss: 2.3561174997719387

Epoch: 5| Step: 9
Training loss: 2.673083782196045
Validation loss: 2.339593425873787

Epoch: 5| Step: 10
Training loss: 3.2039995193481445
Validation loss: 2.3151253936111287

Epoch: 266| Step: 0
Training loss: 2.655587673187256
Validation loss: 2.30731548032453

Epoch: 5| Step: 1
Training loss: 2.244518756866455
Validation loss: 2.3081406470268004

Epoch: 5| Step: 2
Training loss: 2.651397466659546
Validation loss: 2.314233156942552

Epoch: 5| Step: 3
Training loss: 2.8853275775909424
Validation loss: 2.307351850694226

Epoch: 5| Step: 4
Training loss: 2.366720199584961
Validation loss: 2.3116681883412022

Epoch: 5| Step: 5
Training loss: 2.4707794189453125
Validation loss: 2.3114131842890093

Epoch: 5| Step: 6
Training loss: 2.244798183441162
Validation loss: 2.3100392126267955

Epoch: 5| Step: 7
Training loss: 2.76566481590271
Validation loss: 2.3078323205312095

Epoch: 5| Step: 8
Training loss: 2.866623878479004
Validation loss: 2.3088381905709543

Epoch: 5| Step: 9
Training loss: 2.34643816947937
Validation loss: 2.313890066198123

Epoch: 5| Step: 10
Training loss: 2.416180372238159
Validation loss: 2.308171820896928

Epoch: 267| Step: 0
Training loss: 3.2848010063171387
Validation loss: 2.313580410454863

Epoch: 5| Step: 1
Training loss: 1.8520336151123047
Validation loss: 2.3229598127385622

Epoch: 5| Step: 2
Training loss: 2.614645004272461
Validation loss: 2.3248721989252235

Epoch: 5| Step: 3
Training loss: 2.2566158771514893
Validation loss: 2.333743344071091

Epoch: 5| Step: 4
Training loss: 2.3046035766601562
Validation loss: 2.339944495949694

Epoch: 5| Step: 5
Training loss: 2.1037118434906006
Validation loss: 2.3580787566400345

Epoch: 5| Step: 6
Training loss: 2.6614341735839844
Validation loss: 2.360036496193178

Epoch: 5| Step: 7
Training loss: 3.1077771186828613
Validation loss: 2.3623537107180526

Epoch: 5| Step: 8
Training loss: 2.299129009246826
Validation loss: 2.360672443143783

Epoch: 5| Step: 9
Training loss: 2.6289260387420654
Validation loss: 2.3443816733616654

Epoch: 5| Step: 10
Training loss: 3.018887519836426
Validation loss: 2.3410725388475644

Epoch: 268| Step: 0
Training loss: 2.0945611000061035
Validation loss: 2.333202344115062

Epoch: 5| Step: 1
Training loss: 3.0354456901550293
Validation loss: 2.3271661163658224

Epoch: 5| Step: 2
Training loss: 2.430537700653076
Validation loss: 2.325598991045388

Epoch: 5| Step: 3
Training loss: 2.3156070709228516
Validation loss: 2.337660163961431

Epoch: 5| Step: 4
Training loss: 3.467726469039917
Validation loss: 2.3350025992239676

Epoch: 5| Step: 5
Training loss: 2.286830425262451
Validation loss: 2.337896913610479

Epoch: 5| Step: 6
Training loss: 2.190641403198242
Validation loss: 2.3321166910151

Epoch: 5| Step: 7
Training loss: 2.646322250366211
Validation loss: 2.3254836195258686

Epoch: 5| Step: 8
Training loss: 2.722226619720459
Validation loss: 2.303781268417194

Epoch: 5| Step: 9
Training loss: 2.0658633708953857
Validation loss: 2.3128717971104447

Epoch: 5| Step: 10
Training loss: 2.6053054332733154
Validation loss: 2.306321459431802

Epoch: 269| Step: 0
Training loss: 1.802891492843628
Validation loss: 2.312809964661957

Epoch: 5| Step: 1
Training loss: 2.501809597015381
Validation loss: 2.3138518230889433

Epoch: 5| Step: 2
Training loss: 2.511643171310425
Validation loss: 2.3259934558663318

Epoch: 5| Step: 3
Training loss: 2.519890308380127
Validation loss: 2.3275614566700433

Epoch: 5| Step: 4
Training loss: 3.0982353687286377
Validation loss: 2.3312995485080186

Epoch: 5| Step: 5
Training loss: 2.53397798538208
Validation loss: 2.3239384492238364

Epoch: 5| Step: 6
Training loss: 2.8272452354431152
Validation loss: 2.3359169088384157

Epoch: 5| Step: 7
Training loss: 2.762507200241089
Validation loss: 2.3242313502937235

Epoch: 5| Step: 8
Training loss: 2.1165268421173096
Validation loss: 2.3299465897262737

Epoch: 5| Step: 9
Training loss: 2.214226007461548
Validation loss: 2.327764036834881

Epoch: 5| Step: 10
Training loss: 3.0717244148254395
Validation loss: 2.3232293564786195

Epoch: 270| Step: 0
Training loss: 2.2804811000823975
Validation loss: 2.3136239474819553

Epoch: 5| Step: 1
Training loss: 2.352933406829834
Validation loss: 2.3266431170125164

Epoch: 5| Step: 2
Training loss: 2.378673791885376
Validation loss: 2.3365108172098794

Epoch: 5| Step: 3
Training loss: 3.2103934288024902
Validation loss: 2.3428030578039025

Epoch: 5| Step: 4
Training loss: 1.877509355545044
Validation loss: 2.3350406436509985

Epoch: 5| Step: 5
Training loss: 2.516422986984253
Validation loss: 2.325436148592221

Epoch: 5| Step: 6
Training loss: 2.2129573822021484
Validation loss: 2.320657830084524

Epoch: 5| Step: 7
Training loss: 2.724461793899536
Validation loss: 2.315526472624912

Epoch: 5| Step: 8
Training loss: 2.775020122528076
Validation loss: 2.313021829051356

Epoch: 5| Step: 9
Training loss: 2.761594295501709
Validation loss: 2.3181707948766728

Epoch: 5| Step: 10
Training loss: 2.6261675357818604
Validation loss: 2.3402726445146786

Epoch: 271| Step: 0
Training loss: 2.7633917331695557
Validation loss: 2.342182292733141

Epoch: 5| Step: 1
Training loss: 2.4109504222869873
Validation loss: 2.3277225238020702

Epoch: 5| Step: 2
Training loss: 2.719010591506958
Validation loss: 2.3368211920543382

Epoch: 5| Step: 3
Training loss: 2.6218466758728027
Validation loss: 2.3186310106708157

Epoch: 5| Step: 4
Training loss: 2.997400999069214
Validation loss: 2.320908256756362

Epoch: 5| Step: 5
Training loss: 2.5458855628967285
Validation loss: 2.314984908667944

Epoch: 5| Step: 6
Training loss: 2.164954423904419
Validation loss: 2.315278066101895

Epoch: 5| Step: 7
Training loss: 1.81845223903656
Validation loss: 2.313502368106637

Epoch: 5| Step: 8
Training loss: 2.875617265701294
Validation loss: 2.3070928076262116

Epoch: 5| Step: 9
Training loss: 2.6686718463897705
Validation loss: 2.314955549855386

Epoch: 5| Step: 10
Training loss: 2.0994174480438232
Validation loss: 2.3058253257505354

Epoch: 272| Step: 0
Training loss: 2.462592124938965
Validation loss: 2.3088240726019746

Epoch: 5| Step: 1
Training loss: 2.5255284309387207
Validation loss: 2.3178313034836964

Epoch: 5| Step: 2
Training loss: 2.5944366455078125
Validation loss: 2.3268291206770044

Epoch: 5| Step: 3
Training loss: 2.311636209487915
Validation loss: 2.3312273948423323

Epoch: 5| Step: 4
Training loss: 2.4460341930389404
Validation loss: 2.3308678903887348

Epoch: 5| Step: 5
Training loss: 2.6444180011749268
Validation loss: 2.3262001109379593

Epoch: 5| Step: 6
Training loss: 2.806863307952881
Validation loss: 2.32654062650537

Epoch: 5| Step: 7
Training loss: 2.059343099594116
Validation loss: 2.3266788400629514

Epoch: 5| Step: 8
Training loss: 2.5733020305633545
Validation loss: 2.3131399795573246

Epoch: 5| Step: 9
Training loss: 2.4053471088409424
Validation loss: 2.3195519396053847

Epoch: 5| Step: 10
Training loss: 2.9804978370666504
Validation loss: 2.317832600685858

Epoch: 273| Step: 0
Training loss: 2.671694278717041
Validation loss: 2.3195377754908737

Epoch: 5| Step: 1
Training loss: 2.3572816848754883
Validation loss: 2.324915893616215

Epoch: 5| Step: 2
Training loss: 2.3232696056365967
Validation loss: 2.318426875657933

Epoch: 5| Step: 3
Training loss: 1.9771978855133057
Validation loss: 2.3191079926747147

Epoch: 5| Step: 4
Training loss: 3.0067527294158936
Validation loss: 2.3163226304515714

Epoch: 5| Step: 5
Training loss: 2.170088052749634
Validation loss: 2.3179039775684314

Epoch: 5| Step: 6
Training loss: 3.0866405963897705
Validation loss: 2.3187011006057903

Epoch: 5| Step: 7
Training loss: 2.9254069328308105
Validation loss: 2.3170792005395375

Epoch: 5| Step: 8
Training loss: 2.1440701484680176
Validation loss: 2.3229232500958186

Epoch: 5| Step: 9
Training loss: 2.1631453037261963
Validation loss: 2.323253559809859

Epoch: 5| Step: 10
Training loss: 2.8059558868408203
Validation loss: 2.3256202974627094

Epoch: 274| Step: 0
Training loss: 3.126312494277954
Validation loss: 2.3243535949337866

Epoch: 5| Step: 1
Training loss: 2.575326919555664
Validation loss: 2.321616539391138

Epoch: 5| Step: 2
Training loss: 2.224125623703003
Validation loss: 2.2989566479959795

Epoch: 5| Step: 3
Training loss: 2.3002407550811768
Validation loss: 2.3109006676622617

Epoch: 5| Step: 4
Training loss: 2.1710867881774902
Validation loss: 2.304720168472618

Epoch: 5| Step: 5
Training loss: 2.122408151626587
Validation loss: 2.3112488177514847

Epoch: 5| Step: 6
Training loss: 1.9878580570220947
Validation loss: 2.322609060554094

Epoch: 5| Step: 7
Training loss: 2.6112587451934814
Validation loss: 2.3312280921525854

Epoch: 5| Step: 8
Training loss: 2.7928857803344727
Validation loss: 2.341090497150216

Epoch: 5| Step: 9
Training loss: 2.5020134449005127
Validation loss: 2.3459795803152104

Epoch: 5| Step: 10
Training loss: 3.310572385787964
Validation loss: 2.3529663034664687

Epoch: 275| Step: 0
Training loss: 2.0106148719787598
Validation loss: 2.358639288974065

Epoch: 5| Step: 1
Training loss: 2.071817398071289
Validation loss: 2.3571789213406142

Epoch: 5| Step: 2
Training loss: 2.7955756187438965
Validation loss: 2.3506252945110364

Epoch: 5| Step: 3
Training loss: 2.9082131385803223
Validation loss: 2.329111514552947

Epoch: 5| Step: 4
Training loss: 2.22430419921875
Validation loss: 2.319310580530474

Epoch: 5| Step: 5
Training loss: 2.7419605255126953
Validation loss: 2.3284161513851536

Epoch: 5| Step: 6
Training loss: 2.438845157623291
Validation loss: 2.3175147771835327

Epoch: 5| Step: 7
Training loss: 2.6721889972686768
Validation loss: 2.3161086292677027

Epoch: 5| Step: 8
Training loss: 3.1719467639923096
Validation loss: 2.314048381261928

Epoch: 5| Step: 9
Training loss: 2.3603599071502686
Validation loss: 2.3258353689665436

Epoch: 5| Step: 10
Training loss: 2.164344310760498
Validation loss: 2.3278588966656755

Epoch: 276| Step: 0
Training loss: 3.1518523693084717
Validation loss: 2.329079476735925

Epoch: 5| Step: 1
Training loss: 2.6914916038513184
Validation loss: 2.31798215066233

Epoch: 5| Step: 2
Training loss: 2.5931713581085205
Validation loss: 2.3183700423086844

Epoch: 5| Step: 3
Training loss: 2.6300270557403564
Validation loss: 2.3230663243160454

Epoch: 5| Step: 4
Training loss: 1.7871589660644531
Validation loss: 2.3209667436538206

Epoch: 5| Step: 5
Training loss: 2.272132158279419
Validation loss: 2.3189088195882817

Epoch: 5| Step: 6
Training loss: 2.8696165084838867
Validation loss: 2.3185014801640667

Epoch: 5| Step: 7
Training loss: 2.6153838634490967
Validation loss: 2.3281765740404845

Epoch: 5| Step: 8
Training loss: 1.5254743099212646
Validation loss: 2.33310446687924

Epoch: 5| Step: 9
Training loss: 2.79491925239563
Validation loss: 2.339241376487158

Epoch: 5| Step: 10
Training loss: 2.7117011547088623
Validation loss: 2.342645522086851

Epoch: 277| Step: 0
Training loss: 1.9627532958984375
Validation loss: 2.34124852764991

Epoch: 5| Step: 1
Training loss: 2.1535613536834717
Validation loss: 2.3564731664555048

Epoch: 5| Step: 2
Training loss: 2.4838478565216064
Validation loss: 2.365208092556205

Epoch: 5| Step: 3
Training loss: 3.1121206283569336
Validation loss: 2.3805093303803475

Epoch: 5| Step: 4
Training loss: 2.7547035217285156
Validation loss: 2.3692873524081324

Epoch: 5| Step: 5
Training loss: 2.9454731941223145
Validation loss: 2.3582276708336285

Epoch: 5| Step: 6
Training loss: 2.3184571266174316
Validation loss: 2.3275568626260243

Epoch: 5| Step: 7
Training loss: 2.268317699432373
Validation loss: 2.309882738256967

Epoch: 5| Step: 8
Training loss: 2.926785945892334
Validation loss: 2.2913893615045855

Epoch: 5| Step: 9
Training loss: 2.79044771194458
Validation loss: 2.2940895672767394

Epoch: 5| Step: 10
Training loss: 1.8474552631378174
Validation loss: 2.2922814764002317

Epoch: 278| Step: 0
Training loss: 2.1359641551971436
Validation loss: 2.289845807577974

Epoch: 5| Step: 1
Training loss: 1.9557520151138306
Validation loss: 2.2900769479813112

Epoch: 5| Step: 2
Training loss: 2.652496337890625
Validation loss: 2.3037258091793267

Epoch: 5| Step: 3
Training loss: 2.965355396270752
Validation loss: 2.292565186818441

Epoch: 5| Step: 4
Training loss: 2.298006534576416
Validation loss: 2.3015947111191286

Epoch: 5| Step: 5
Training loss: 3.17441987991333
Validation loss: 2.2930490970611572

Epoch: 5| Step: 6
Training loss: 2.0843160152435303
Validation loss: 2.3069100815762758

Epoch: 5| Step: 7
Training loss: 2.172300100326538
Validation loss: 2.3103318419507755

Epoch: 5| Step: 8
Training loss: 2.6013565063476562
Validation loss: 2.3166382979321223

Epoch: 5| Step: 9
Training loss: 2.2645044326782227
Validation loss: 2.3180861216719433

Epoch: 5| Step: 10
Training loss: 3.398115396499634
Validation loss: 2.3263740821551253

Epoch: 279| Step: 0
Training loss: 2.4403843879699707
Validation loss: 2.341529534709069

Epoch: 5| Step: 1
Training loss: 2.444410562515259
Validation loss: 2.336387393295124

Epoch: 5| Step: 2
Training loss: 2.357797861099243
Validation loss: 2.345028487584924

Epoch: 5| Step: 3
Training loss: 2.716207981109619
Validation loss: 2.341374356259582

Epoch: 5| Step: 4
Training loss: 2.6634674072265625
Validation loss: 2.324087150635258

Epoch: 5| Step: 5
Training loss: 2.4346728324890137
Validation loss: 2.3216295370491604

Epoch: 5| Step: 6
Training loss: 2.647080659866333
Validation loss: 2.308648665746053

Epoch: 5| Step: 7
Training loss: 2.9287543296813965
Validation loss: 2.2990425094481437

Epoch: 5| Step: 8
Training loss: 2.1072473526000977
Validation loss: 2.3040890988483222

Epoch: 5| Step: 9
Training loss: 2.6620564460754395
Validation loss: 2.2951997146811536

Epoch: 5| Step: 10
Training loss: 2.0253944396972656
Validation loss: 2.30320131650535

Epoch: 280| Step: 0
Training loss: 2.703735828399658
Validation loss: 2.300954541852397

Epoch: 5| Step: 1
Training loss: 2.779085874557495
Validation loss: 2.294980715679866

Epoch: 5| Step: 2
Training loss: 2.4684081077575684
Validation loss: 2.3056507443868988

Epoch: 5| Step: 3
Training loss: 2.102722644805908
Validation loss: 2.3115050228693153

Epoch: 5| Step: 4
Training loss: 2.420302629470825
Validation loss: 2.327539356805945

Epoch: 5| Step: 5
Training loss: 2.3757758140563965
Validation loss: 2.314476546420846

Epoch: 5| Step: 6
Training loss: 3.037059783935547
Validation loss: 2.3210320395808064

Epoch: 5| Step: 7
Training loss: 2.1287550926208496
Validation loss: 2.3182717548903597

Epoch: 5| Step: 8
Training loss: 2.2776923179626465
Validation loss: 2.3176267249609834

Epoch: 5| Step: 9
Training loss: 2.922724485397339
Validation loss: 2.3174829559941448

Epoch: 5| Step: 10
Training loss: 2.303892135620117
Validation loss: 2.3082707928073023

Epoch: 281| Step: 0
Training loss: 2.252462148666382
Validation loss: 2.3223341382959837

Epoch: 5| Step: 1
Training loss: 2.5281100273132324
Validation loss: 2.3140623979671027

Epoch: 5| Step: 2
Training loss: 1.9002841711044312
Validation loss: 2.3319169962277977

Epoch: 5| Step: 3
Training loss: 2.423543930053711
Validation loss: 2.3465207161441928

Epoch: 5| Step: 4
Training loss: 3.1695966720581055
Validation loss: 2.346700373516288

Epoch: 5| Step: 5
Training loss: 2.1373677253723145
Validation loss: 2.3463716430048787

Epoch: 5| Step: 6
Training loss: 2.4039907455444336
Validation loss: 2.3239824771881104

Epoch: 5| Step: 7
Training loss: 2.4684534072875977
Validation loss: 2.3199539876753286

Epoch: 5| Step: 8
Training loss: 2.6107707023620605
Validation loss: 2.316954897296044

Epoch: 5| Step: 9
Training loss: 3.169398069381714
Validation loss: 2.3090476784654843

Epoch: 5| Step: 10
Training loss: 2.296543598175049
Validation loss: 2.3003156979878745

Epoch: 282| Step: 0
Training loss: 3.2305703163146973
Validation loss: 2.300351112119613

Epoch: 5| Step: 1
Training loss: 2.3355321884155273
Validation loss: 2.3033926692060245

Epoch: 5| Step: 2
Training loss: 2.532257318496704
Validation loss: 2.2982136664852018

Epoch: 5| Step: 3
Training loss: 2.483473539352417
Validation loss: 2.299926519393921

Epoch: 5| Step: 4
Training loss: 2.1238291263580322
Validation loss: 2.3014851975184616

Epoch: 5| Step: 5
Training loss: 2.753502368927002
Validation loss: 2.2909423997325282

Epoch: 5| Step: 6
Training loss: 2.712282180786133
Validation loss: 2.290544520142258

Epoch: 5| Step: 7
Training loss: 1.9008586406707764
Validation loss: 2.3017657085131575

Epoch: 5| Step: 8
Training loss: 2.4923267364501953
Validation loss: 2.308286623288226

Epoch: 5| Step: 9
Training loss: 2.7251973152160645
Validation loss: 2.316189658257269

Epoch: 5| Step: 10
Training loss: 2.33353328704834
Validation loss: 2.3204162531001593

Epoch: 283| Step: 0
Training loss: 3.3122849464416504
Validation loss: 2.3175868372763357

Epoch: 5| Step: 1
Training loss: 2.1810784339904785
Validation loss: 2.3092398976766937

Epoch: 5| Step: 2
Training loss: 2.2444522380828857
Validation loss: 2.3143316340702835

Epoch: 5| Step: 3
Training loss: 2.9868454933166504
Validation loss: 2.319237273226502

Epoch: 5| Step: 4
Training loss: 2.343989372253418
Validation loss: 2.328869735040972

Epoch: 5| Step: 5
Training loss: 2.5087826251983643
Validation loss: 2.3286732294226207

Epoch: 5| Step: 6
Training loss: 2.098008871078491
Validation loss: 2.3047046507558515

Epoch: 5| Step: 7
Training loss: 1.9944698810577393
Validation loss: 2.307083737465643

Epoch: 5| Step: 8
Training loss: 2.2640581130981445
Validation loss: 2.3012959649485927

Epoch: 5| Step: 9
Training loss: 2.655230760574341
Validation loss: 2.2965412934621177

Epoch: 5| Step: 10
Training loss: 2.804415225982666
Validation loss: 2.30291618967569

Epoch: 284| Step: 0
Training loss: 2.6349844932556152
Validation loss: 2.2914123996611564

Epoch: 5| Step: 1
Training loss: 2.3396708965301514
Validation loss: 2.292806820202899

Epoch: 5| Step: 2
Training loss: 2.588737964630127
Validation loss: 2.2953780992056734

Epoch: 5| Step: 3
Training loss: 2.658250570297241
Validation loss: 2.2934495351647817

Epoch: 5| Step: 4
Training loss: 2.2135839462280273
Validation loss: 2.3004997545673

Epoch: 5| Step: 5
Training loss: 2.6627516746520996
Validation loss: 2.2941124105966217

Epoch: 5| Step: 6
Training loss: 2.5510313510894775
Validation loss: 2.284671729610812

Epoch: 5| Step: 7
Training loss: 3.0225892066955566
Validation loss: 2.2898536061727874

Epoch: 5| Step: 8
Training loss: 2.200411319732666
Validation loss: 2.2870370059885006

Epoch: 5| Step: 9
Training loss: 2.2067065238952637
Validation loss: 2.3015945765279953

Epoch: 5| Step: 10
Training loss: 2.3507182598114014
Validation loss: 2.3079220018079205

Epoch: 285| Step: 0
Training loss: 2.050445556640625
Validation loss: 2.321934997394521

Epoch: 5| Step: 1
Training loss: 2.287497043609619
Validation loss: 2.3308814417931343

Epoch: 5| Step: 2
Training loss: 3.124933958053589
Validation loss: 2.341485825918054

Epoch: 5| Step: 3
Training loss: 3.031270980834961
Validation loss: 2.36020653734925

Epoch: 5| Step: 4
Training loss: 2.4981226921081543
Validation loss: 2.3545447985331216

Epoch: 5| Step: 5
Training loss: 2.1650760173797607
Validation loss: 2.358751275206125

Epoch: 5| Step: 6
Training loss: 2.3550379276275635
Validation loss: 2.340008133201189

Epoch: 5| Step: 7
Training loss: 2.6493897438049316
Validation loss: 2.322863171177526

Epoch: 5| Step: 8
Training loss: 2.3910021781921387
Validation loss: 2.3230003080060406

Epoch: 5| Step: 9
Training loss: 2.282583236694336
Validation loss: 2.2946310504790275

Epoch: 5| Step: 10
Training loss: 2.5260517597198486
Validation loss: 2.292914903292092

Epoch: 286| Step: 0
Training loss: 2.1940197944641113
Validation loss: 2.2963041079941617

Epoch: 5| Step: 1
Training loss: 3.3291831016540527
Validation loss: 2.277116039747833

Epoch: 5| Step: 2
Training loss: 2.0995824337005615
Validation loss: 2.2770736473862843

Epoch: 5| Step: 3
Training loss: 2.3732361793518066
Validation loss: 2.2893009211427424

Epoch: 5| Step: 4
Training loss: 2.5779576301574707
Validation loss: 2.274742490501814

Epoch: 5| Step: 5
Training loss: 2.2307779788970947
Validation loss: 2.2872275383241716

Epoch: 5| Step: 6
Training loss: 2.6504766941070557
Validation loss: 2.2979833528559697

Epoch: 5| Step: 7
Training loss: 2.553945302963257
Validation loss: 2.309899407048379

Epoch: 5| Step: 8
Training loss: 2.4763247966766357
Validation loss: 2.2873970206065843

Epoch: 5| Step: 9
Training loss: 2.8175413608551025
Validation loss: 2.311231243994928

Epoch: 5| Step: 10
Training loss: 2.0510294437408447
Validation loss: 2.2949814155537593

Epoch: 287| Step: 0
Training loss: 3.2184531688690186
Validation loss: 2.3010015872216996

Epoch: 5| Step: 1
Training loss: 2.7292559146881104
Validation loss: 2.2940769016101794

Epoch: 5| Step: 2
Training loss: 2.317798614501953
Validation loss: 2.2874200959359445

Epoch: 5| Step: 3
Training loss: 3.0824921131134033
Validation loss: 2.2935238038339922

Epoch: 5| Step: 4
Training loss: 2.2212302684783936
Validation loss: 2.2877662809946204

Epoch: 5| Step: 5
Training loss: 2.30133056640625
Validation loss: 2.29241301936488

Epoch: 5| Step: 6
Training loss: 1.80498468875885
Validation loss: 2.2920782437888523

Epoch: 5| Step: 7
Training loss: 2.080416202545166
Validation loss: 2.3062531883998583

Epoch: 5| Step: 8
Training loss: 2.2368807792663574
Validation loss: 2.3129028748440486

Epoch: 5| Step: 9
Training loss: 3.188804864883423
Validation loss: 2.3182541119155062

Epoch: 5| Step: 10
Training loss: 2.010221004486084
Validation loss: 2.3349390773363012

Epoch: 288| Step: 0
Training loss: 2.3425610065460205
Validation loss: 2.323024295991467

Epoch: 5| Step: 1
Training loss: 2.1669764518737793
Validation loss: 2.3221706023780246

Epoch: 5| Step: 2
Training loss: 2.7600433826446533
Validation loss: 2.317864966648881

Epoch: 5| Step: 3
Training loss: 2.3540616035461426
Validation loss: 2.31685334379955

Epoch: 5| Step: 4
Training loss: 2.5083420276641846
Validation loss: 2.304254367787351

Epoch: 5| Step: 5
Training loss: 2.5097782611846924
Validation loss: 2.3153304387164373

Epoch: 5| Step: 6
Training loss: 2.300419569015503
Validation loss: 2.3126964133272887

Epoch: 5| Step: 7
Training loss: 2.832165479660034
Validation loss: 2.3253553810939995

Epoch: 5| Step: 8
Training loss: 2.3003735542297363
Validation loss: 2.3098231310485513

Epoch: 5| Step: 9
Training loss: 2.7186312675476074
Validation loss: 2.3055125859475907

Epoch: 5| Step: 10
Training loss: 2.443159580230713
Validation loss: 2.3092380416008735

Epoch: 289| Step: 0
Training loss: 3.0180439949035645
Validation loss: 2.3057293763724704

Epoch: 5| Step: 1
Training loss: 2.445072650909424
Validation loss: 2.298400519996561

Epoch: 5| Step: 2
Training loss: 2.511526107788086
Validation loss: 2.298246596449165

Epoch: 5| Step: 3
Training loss: 1.7286580801010132
Validation loss: 2.2978432460497786

Epoch: 5| Step: 4
Training loss: 2.5569887161254883
Validation loss: 2.294749697049459

Epoch: 5| Step: 5
Training loss: 2.446012020111084
Validation loss: 2.3062280634398102

Epoch: 5| Step: 6
Training loss: 2.5800881385803223
Validation loss: 2.31154195467631

Epoch: 5| Step: 7
Training loss: 2.663806200027466
Validation loss: 2.319153093522595

Epoch: 5| Step: 8
Training loss: 2.8859667778015137
Validation loss: 2.330789742931243

Epoch: 5| Step: 9
Training loss: 1.8064327239990234
Validation loss: 2.3271797036611908

Epoch: 5| Step: 10
Training loss: 2.575066089630127
Validation loss: 2.3124740713386127

Epoch: 290| Step: 0
Training loss: 2.434319019317627
Validation loss: 2.322509588733796

Epoch: 5| Step: 1
Training loss: 2.2148051261901855
Validation loss: 2.3260775125154884

Epoch: 5| Step: 2
Training loss: 2.3896610736846924
Validation loss: 2.327916927235101

Epoch: 5| Step: 3
Training loss: 2.5864813327789307
Validation loss: 2.3141505205503075

Epoch: 5| Step: 4
Training loss: 2.36645245552063
Validation loss: 2.3167671362559

Epoch: 5| Step: 5
Training loss: 2.310720682144165
Validation loss: 2.3160479145665325

Epoch: 5| Step: 6
Training loss: 2.9322633743286133
Validation loss: 2.3114733055073726

Epoch: 5| Step: 7
Training loss: 2.307530641555786
Validation loss: 2.3091086931126092

Epoch: 5| Step: 8
Training loss: 2.5819525718688965
Validation loss: 2.3113390989201044

Epoch: 5| Step: 9
Training loss: 2.265423536300659
Validation loss: 2.3132785520245953

Epoch: 5| Step: 10
Training loss: 2.8461716175079346
Validation loss: 2.3011375037572717

Epoch: 291| Step: 0
Training loss: 2.302584648132324
Validation loss: 2.303826649983724

Epoch: 5| Step: 1
Training loss: 2.218116283416748
Validation loss: 2.302728227389756

Epoch: 5| Step: 2
Training loss: 3.159519672393799
Validation loss: 2.2996855397378244

Epoch: 5| Step: 3
Training loss: 2.3491859436035156
Validation loss: 2.3094277792079474

Epoch: 5| Step: 4
Training loss: 2.3447444438934326
Validation loss: 2.308725926183885

Epoch: 5| Step: 5
Training loss: 2.5511443614959717
Validation loss: 2.3037236172665834

Epoch: 5| Step: 6
Training loss: 2.6967098712921143
Validation loss: 2.304585564521051

Epoch: 5| Step: 7
Training loss: 2.6636881828308105
Validation loss: 2.2973527933961604

Epoch: 5| Step: 8
Training loss: 2.127560615539551
Validation loss: 2.3015687055485223

Epoch: 5| Step: 9
Training loss: 2.3874878883361816
Validation loss: 2.315305463729366

Epoch: 5| Step: 10
Training loss: 2.428727149963379
Validation loss: 2.316690978183541

Epoch: 292| Step: 0
Training loss: 2.00006365776062
Validation loss: 2.3079565058472338

Epoch: 5| Step: 1
Training loss: 3.2958552837371826
Validation loss: 2.3113812784994803

Epoch: 5| Step: 2
Training loss: 2.422518014907837
Validation loss: 2.30952254931132

Epoch: 5| Step: 3
Training loss: 2.3879430294036865
Validation loss: 2.305159717477778

Epoch: 5| Step: 4
Training loss: 3.0751900672912598
Validation loss: 2.3299210404837005

Epoch: 5| Step: 5
Training loss: 2.4070749282836914
Validation loss: 2.3263050458764516

Epoch: 5| Step: 6
Training loss: 2.411647319793701
Validation loss: 2.322130733920682

Epoch: 5| Step: 7
Training loss: 1.6356077194213867
Validation loss: 2.3216041980251187

Epoch: 5| Step: 8
Training loss: 2.4715607166290283
Validation loss: 2.306522636003392

Epoch: 5| Step: 9
Training loss: 2.5971972942352295
Validation loss: 2.2996159779128207

Epoch: 5| Step: 10
Training loss: 2.4288532733917236
Validation loss: 2.304467062796316

Epoch: 293| Step: 0
Training loss: 2.6281702518463135
Validation loss: 2.3104389944384174

Epoch: 5| Step: 1
Training loss: 2.761565923690796
Validation loss: 2.303491917989587

Epoch: 5| Step: 2
Training loss: 2.6164114475250244
Validation loss: 2.3008384730226252

Epoch: 5| Step: 3
Training loss: 2.756836414337158
Validation loss: 2.3087775937972532

Epoch: 5| Step: 4
Training loss: 2.3466477394104004
Validation loss: 2.304164194291638

Epoch: 5| Step: 5
Training loss: 2.335442066192627
Validation loss: 2.295183704745385

Epoch: 5| Step: 6
Training loss: 2.6638712882995605
Validation loss: 2.3046632992324008

Epoch: 5| Step: 7
Training loss: 2.2474260330200195
Validation loss: 2.2919602970923147

Epoch: 5| Step: 8
Training loss: 2.4619879722595215
Validation loss: 2.2870566588576122

Epoch: 5| Step: 9
Training loss: 2.295724391937256
Validation loss: 2.2844255508915072

Epoch: 5| Step: 10
Training loss: 2.0366547107696533
Validation loss: 2.2910107310100267

Epoch: 294| Step: 0
Training loss: 1.549839735031128
Validation loss: 2.285815054370511

Epoch: 5| Step: 1
Training loss: 2.6663119792938232
Validation loss: 2.287054137517047

Epoch: 5| Step: 2
Training loss: 2.5984630584716797
Validation loss: 2.2898497825027793

Epoch: 5| Step: 3
Training loss: 2.5208146572113037
Validation loss: 2.2755611019749797

Epoch: 5| Step: 4
Training loss: 2.1965980529785156
Validation loss: 2.2806260278148036

Epoch: 5| Step: 5
Training loss: 2.9746813774108887
Validation loss: 2.293024993711902

Epoch: 5| Step: 6
Training loss: 2.3741455078125
Validation loss: 2.2819657069380566

Epoch: 5| Step: 7
Training loss: 2.895918607711792
Validation loss: 2.286227690276279

Epoch: 5| Step: 8
Training loss: 2.009108781814575
Validation loss: 2.297064878607309

Epoch: 5| Step: 9
Training loss: 2.800116539001465
Validation loss: 2.3007559391760055

Epoch: 5| Step: 10
Training loss: 2.5466740131378174
Validation loss: 2.308603557207251

Epoch: 295| Step: 0
Training loss: 2.3730478286743164
Validation loss: 2.314161846714635

Epoch: 5| Step: 1
Training loss: 3.0054233074188232
Validation loss: 2.3112503584995063

Epoch: 5| Step: 2
Training loss: 2.347639322280884
Validation loss: 2.3066043917850783

Epoch: 5| Step: 3
Training loss: 2.9140865802764893
Validation loss: 2.3193746202735492

Epoch: 5| Step: 4
Training loss: 2.6074931621551514
Validation loss: 2.3079030462490615

Epoch: 5| Step: 5
Training loss: 2.767869472503662
Validation loss: 2.3162143230438232

Epoch: 5| Step: 6
Training loss: 2.4454989433288574
Validation loss: 2.310039456172656

Epoch: 5| Step: 7
Training loss: 2.0855441093444824
Validation loss: 2.304374797369844

Epoch: 5| Step: 8
Training loss: 2.362654209136963
Validation loss: 2.2943716126103557

Epoch: 5| Step: 9
Training loss: 2.0157248973846436
Validation loss: 2.2905812981308147

Epoch: 5| Step: 10
Training loss: 2.1056902408599854
Validation loss: 2.281325481271231

Epoch: 296| Step: 0
Training loss: 2.0821518898010254
Validation loss: 2.2808430835764897

Epoch: 5| Step: 1
Training loss: 2.1975440979003906
Validation loss: 2.297042990243563

Epoch: 5| Step: 2
Training loss: 2.099483013153076
Validation loss: 2.301243823061707

Epoch: 5| Step: 3
Training loss: 2.5322213172912598
Validation loss: 2.3059641622727916

Epoch: 5| Step: 4
Training loss: 2.2117905616760254
Validation loss: 2.2954733397371028

Epoch: 5| Step: 5
Training loss: 2.1008830070495605
Validation loss: 2.2904482374909105

Epoch: 5| Step: 6
Training loss: 2.432340145111084
Validation loss: 2.311227265224662

Epoch: 5| Step: 7
Training loss: 2.624262571334839
Validation loss: 2.327652500521752

Epoch: 5| Step: 8
Training loss: 2.5497055053710938
Validation loss: 2.3241715328667754

Epoch: 5| Step: 9
Training loss: 3.4179635047912598
Validation loss: 2.313693356770341

Epoch: 5| Step: 10
Training loss: 2.9165070056915283
Validation loss: 2.28446190972482

Epoch: 297| Step: 0
Training loss: 2.38553524017334
Validation loss: 2.2811991988971667

Epoch: 5| Step: 1
Training loss: 2.3881428241729736
Validation loss: 2.276650426208332

Epoch: 5| Step: 2
Training loss: 2.5141496658325195
Validation loss: 2.278650754241533

Epoch: 5| Step: 3
Training loss: 2.373853921890259
Validation loss: 2.269807272059943

Epoch: 5| Step: 4
Training loss: 1.908943772315979
Validation loss: 2.275493862808392

Epoch: 5| Step: 5
Training loss: 2.5082688331604004
Validation loss: 2.2848618312548568

Epoch: 5| Step: 6
Training loss: 2.429563045501709
Validation loss: 2.289758381023202

Epoch: 5| Step: 7
Training loss: 2.654050350189209
Validation loss: 2.299297798064447

Epoch: 5| Step: 8
Training loss: 2.2313618659973145
Validation loss: 2.3088189248115785

Epoch: 5| Step: 9
Training loss: 2.862866163253784
Validation loss: 2.299960519677849

Epoch: 5| Step: 10
Training loss: 3.016671657562256
Validation loss: 2.296108873941565

Epoch: 298| Step: 0
Training loss: 3.1663637161254883
Validation loss: 2.295131791022516

Epoch: 5| Step: 1
Training loss: 2.4472925662994385
Validation loss: 2.2982802621779905

Epoch: 5| Step: 2
Training loss: 2.6283204555511475
Validation loss: 2.2828304613790205

Epoch: 5| Step: 3
Training loss: 2.388751268386841
Validation loss: 2.274715805566439

Epoch: 5| Step: 4
Training loss: 2.5022835731506348
Validation loss: 2.2708675630630983

Epoch: 5| Step: 5
Training loss: 2.0942156314849854
Validation loss: 2.276208157180458

Epoch: 5| Step: 6
Training loss: 2.387876510620117
Validation loss: 2.2658829201934156

Epoch: 5| Step: 7
Training loss: 2.499350070953369
Validation loss: 2.2720541287493963

Epoch: 5| Step: 8
Training loss: 2.0661513805389404
Validation loss: 2.2803632136314147

Epoch: 5| Step: 9
Training loss: 2.2573742866516113
Validation loss: 2.285451586528491

Epoch: 5| Step: 10
Training loss: 2.775848627090454
Validation loss: 2.2918400546555877

Epoch: 299| Step: 0
Training loss: 2.47041916847229
Validation loss: 2.2929819348037883

Epoch: 5| Step: 1
Training loss: 2.6655051708221436
Validation loss: 2.3040134650404736

Epoch: 5| Step: 2
Training loss: 2.487013339996338
Validation loss: 2.310119234105592

Epoch: 5| Step: 3
Training loss: 2.0140395164489746
Validation loss: 2.31769383850918

Epoch: 5| Step: 4
Training loss: 2.2666423320770264
Validation loss: 2.315704159839179

Epoch: 5| Step: 5
Training loss: 2.2349255084991455
Validation loss: 2.312917387613686

Epoch: 5| Step: 6
Training loss: 2.1841495037078857
Validation loss: 2.3204469937150196

Epoch: 5| Step: 7
Training loss: 2.9901514053344727
Validation loss: 2.3209471523120837

Epoch: 5| Step: 8
Training loss: 2.6891894340515137
Validation loss: 2.3123524932451147

Epoch: 5| Step: 9
Training loss: 2.8055315017700195
Validation loss: 2.2983702741643435

Epoch: 5| Step: 10
Training loss: 2.168733596801758
Validation loss: 2.297184590370424

Epoch: 300| Step: 0
Training loss: 1.6950209140777588
Validation loss: 2.2773666561290784

Epoch: 5| Step: 1
Training loss: 2.6381614208221436
Validation loss: 2.27698786540698

Epoch: 5| Step: 2
Training loss: 2.1100852489471436
Validation loss: 2.2705318684219034

Epoch: 5| Step: 3
Training loss: 2.288648843765259
Validation loss: 2.2716274979293987

Epoch: 5| Step: 4
Training loss: 2.8966586589813232
Validation loss: 2.267968790505522

Epoch: 5| Step: 5
Training loss: 2.6973557472229004
Validation loss: 2.266931890159525

Epoch: 5| Step: 6
Training loss: 2.201859712600708
Validation loss: 2.2789540983015493

Epoch: 5| Step: 7
Training loss: 2.815641403198242
Validation loss: 2.2626678353996685

Epoch: 5| Step: 8
Training loss: 2.5879390239715576
Validation loss: 2.268751472555181

Epoch: 5| Step: 9
Training loss: 2.6690139770507812
Validation loss: 2.288573803440217

Epoch: 5| Step: 10
Training loss: 2.5286736488342285
Validation loss: 2.2886616645320768

Epoch: 301| Step: 0
Training loss: 2.7045395374298096
Validation loss: 2.298283241128409

Epoch: 5| Step: 1
Training loss: 2.564554452896118
Validation loss: 2.3002720776424614

Epoch: 5| Step: 2
Training loss: 2.3605518341064453
Validation loss: 2.3013067809484338

Epoch: 5| Step: 3
Training loss: 2.5291614532470703
Validation loss: 2.316182155762949

Epoch: 5| Step: 4
Training loss: 2.1860508918762207
Validation loss: 2.319214144060689

Epoch: 5| Step: 5
Training loss: 2.1396541595458984
Validation loss: 2.3172595270218386

Epoch: 5| Step: 6
Training loss: 2.367645263671875
Validation loss: 2.2937344299849642

Epoch: 5| Step: 7
Training loss: 2.8087143898010254
Validation loss: 2.284301973158313

Epoch: 5| Step: 8
Training loss: 2.551755428314209
Validation loss: 2.2794529750782955

Epoch: 5| Step: 9
Training loss: 2.4193856716156006
Validation loss: 2.273203919010778

Epoch: 5| Step: 10
Training loss: 2.466287851333618
Validation loss: 2.268278332166774

Epoch: 302| Step: 0
Training loss: 2.4603893756866455
Validation loss: 2.265773686029578

Epoch: 5| Step: 1
Training loss: 2.884829044342041
Validation loss: 2.2661580116518083

Epoch: 5| Step: 2
Training loss: 2.4384148120880127
Validation loss: 2.268846208049405

Epoch: 5| Step: 3
Training loss: 2.0820472240448
Validation loss: 2.262229246477927

Epoch: 5| Step: 4
Training loss: 2.647761583328247
Validation loss: 2.280831775357646

Epoch: 5| Step: 5
Training loss: 1.5316358804702759
Validation loss: 2.283636890431886

Epoch: 5| Step: 6
Training loss: 3.1576955318450928
Validation loss: 2.2838697971836215

Epoch: 5| Step: 7
Training loss: 2.9181766510009766
Validation loss: 2.300178204813311

Epoch: 5| Step: 8
Training loss: 2.088862180709839
Validation loss: 2.306798570899553

Epoch: 5| Step: 9
Training loss: 2.5253074169158936
Validation loss: 2.3102351260441605

Epoch: 5| Step: 10
Training loss: 2.2136967182159424
Validation loss: 2.2900129928383777

Epoch: 303| Step: 0
Training loss: 2.2620983123779297
Validation loss: 2.2992091819804203

Epoch: 5| Step: 1
Training loss: 2.9193315505981445
Validation loss: 2.2960872137418358

Epoch: 5| Step: 2
Training loss: 2.2399611473083496
Validation loss: 2.2795769091575377

Epoch: 5| Step: 3
Training loss: 2.1430790424346924
Validation loss: 2.285937834811467

Epoch: 5| Step: 4
Training loss: 2.653987169265747
Validation loss: 2.2762028683898268

Epoch: 5| Step: 5
Training loss: 2.832594871520996
Validation loss: 2.277668970887379

Epoch: 5| Step: 6
Training loss: 2.352349281311035
Validation loss: 2.2777064256770636

Epoch: 5| Step: 7
Training loss: 2.1085128784179688
Validation loss: 2.277444234458349

Epoch: 5| Step: 8
Training loss: 2.274266481399536
Validation loss: 2.2982868584253455

Epoch: 5| Step: 9
Training loss: 2.7465274333953857
Validation loss: 2.2912432660338697

Epoch: 5| Step: 10
Training loss: 2.327862024307251
Validation loss: 2.298766989861765

Epoch: 304| Step: 0
Training loss: 2.1675167083740234
Validation loss: 2.3051942830444663

Epoch: 5| Step: 1
Training loss: 2.2856178283691406
Validation loss: 2.296379468774283

Epoch: 5| Step: 2
Training loss: 2.772343158721924
Validation loss: 2.2903065348184235

Epoch: 5| Step: 3
Training loss: 2.1403422355651855
Validation loss: 2.292506371774981

Epoch: 5| Step: 4
Training loss: 2.287884473800659
Validation loss: 2.295518477757772

Epoch: 5| Step: 5
Training loss: 2.581204891204834
Validation loss: 2.2890677772542483

Epoch: 5| Step: 6
Training loss: 2.9235756397247314
Validation loss: 2.285771066142667

Epoch: 5| Step: 7
Training loss: 2.778412103652954
Validation loss: 2.283856343197566

Epoch: 5| Step: 8
Training loss: 3.0393154621124268
Validation loss: 2.2866880047705864

Epoch: 5| Step: 9
Training loss: 1.8784328699111938
Validation loss: 2.2867695631519442

Epoch: 5| Step: 10
Training loss: 1.978652000427246
Validation loss: 2.2949788621676865

Epoch: 305| Step: 0
Training loss: 3.0814502239227295
Validation loss: 2.2993929898867043

Epoch: 5| Step: 1
Training loss: 2.91388201713562
Validation loss: 2.2964006829005417

Epoch: 5| Step: 2
Training loss: 2.2141387462615967
Validation loss: 2.316078103998656

Epoch: 5| Step: 3
Training loss: 2.1355605125427246
Validation loss: 2.295772810136118

Epoch: 5| Step: 4
Training loss: 2.925327777862549
Validation loss: 2.3061588489881126

Epoch: 5| Step: 5
Training loss: 2.241178035736084
Validation loss: 2.300312434473345

Epoch: 5| Step: 6
Training loss: 2.1252095699310303
Validation loss: 2.2976012024828183

Epoch: 5| Step: 7
Training loss: 2.181194305419922
Validation loss: 2.288521243679908

Epoch: 5| Step: 8
Training loss: 2.3370044231414795
Validation loss: 2.282690037963211

Epoch: 5| Step: 9
Training loss: 2.2633862495422363
Validation loss: 2.2759337476504746

Epoch: 5| Step: 10
Training loss: 2.327974557876587
Validation loss: 2.2683787358704435

Epoch: 306| Step: 0
Training loss: 2.321967363357544
Validation loss: 2.274932889528172

Epoch: 5| Step: 1
Training loss: 2.920921564102173
Validation loss: 2.2663283322447088

Epoch: 5| Step: 2
Training loss: 2.036179780960083
Validation loss: 2.271925792899183

Epoch: 5| Step: 3
Training loss: 2.6049036979675293
Validation loss: 2.2745615769458074

Epoch: 5| Step: 4
Training loss: 2.111908435821533
Validation loss: 2.2853646022017284

Epoch: 5| Step: 5
Training loss: 1.6961133480072021
Validation loss: 2.2847942254876576

Epoch: 5| Step: 6
Training loss: 2.8535783290863037
Validation loss: 2.3094066060999388

Epoch: 5| Step: 7
Training loss: 2.2117533683776855
Validation loss: 2.3052916449885212

Epoch: 5| Step: 8
Training loss: 2.7942166328430176
Validation loss: 2.301994140430163

Epoch: 5| Step: 9
Training loss: 2.6707847118377686
Validation loss: 2.301230976658483

Epoch: 5| Step: 10
Training loss: 2.6511647701263428
Validation loss: 2.312105142942039

Epoch: 307| Step: 0
Training loss: 2.0634467601776123
Validation loss: 2.319997790039227

Epoch: 5| Step: 1
Training loss: 2.728463649749756
Validation loss: 2.3239493011146464

Epoch: 5| Step: 2
Training loss: 2.937293529510498
Validation loss: 2.3000483666696856

Epoch: 5| Step: 3
Training loss: 3.0178489685058594
Validation loss: 2.308348177581705

Epoch: 5| Step: 4
Training loss: 2.1178555488586426
Validation loss: 2.290381262379308

Epoch: 5| Step: 5
Training loss: 2.1355464458465576
Validation loss: 2.287075158088438

Epoch: 5| Step: 6
Training loss: 2.489400863647461
Validation loss: 2.2745130344103743

Epoch: 5| Step: 7
Training loss: 2.5648193359375
Validation loss: 2.2839004455074186

Epoch: 5| Step: 8
Training loss: 1.7019952535629272
Validation loss: 2.2973172151914207

Epoch: 5| Step: 9
Training loss: 2.5808212757110596
Validation loss: 2.2948687768751577

Epoch: 5| Step: 10
Training loss: 2.480462074279785
Validation loss: 2.2911470205553117

Epoch: 308| Step: 0
Training loss: 2.7179219722747803
Validation loss: 2.299363072200488

Epoch: 5| Step: 1
Training loss: 2.7033467292785645
Validation loss: 2.3015844668111494

Epoch: 5| Step: 2
Training loss: 2.0348966121673584
Validation loss: 2.313437892544654

Epoch: 5| Step: 3
Training loss: 3.106849431991577
Validation loss: 2.3312293919183875

Epoch: 5| Step: 4
Training loss: 2.2261176109313965
Validation loss: 2.3164265027610202

Epoch: 5| Step: 5
Training loss: 2.3796005249023438
Validation loss: 2.313849295339277

Epoch: 5| Step: 6
Training loss: 2.1060073375701904
Validation loss: 2.311601400375366

Epoch: 5| Step: 7
Training loss: 2.318559169769287
Validation loss: 2.281337430400233

Epoch: 5| Step: 8
Training loss: 2.4137215614318848
Validation loss: 2.2703272193990727

Epoch: 5| Step: 9
Training loss: 2.7790768146514893
Validation loss: 2.274781265566426

Epoch: 5| Step: 10
Training loss: 1.9889569282531738
Validation loss: 2.2655457809407222

Epoch: 309| Step: 0
Training loss: 1.799736738204956
Validation loss: 2.2670389657379477

Epoch: 5| Step: 1
Training loss: 2.7134385108947754
Validation loss: 2.257164224501579

Epoch: 5| Step: 2
Training loss: 2.740772008895874
Validation loss: 2.2436277584363054

Epoch: 5| Step: 3
Training loss: 2.6931633949279785
Validation loss: 2.248185488485521

Epoch: 5| Step: 4
Training loss: 2.8552613258361816
Validation loss: 2.2499772143620316

Epoch: 5| Step: 5
Training loss: 2.2496776580810547
Validation loss: 2.249558130900065

Epoch: 5| Step: 6
Training loss: 2.617293357849121
Validation loss: 2.2506797954600346

Epoch: 5| Step: 7
Training loss: 2.063120126724243
Validation loss: 2.2480667611604095

Epoch: 5| Step: 8
Training loss: 2.4929566383361816
Validation loss: 2.2546472703256915

Epoch: 5| Step: 9
Training loss: 2.576435089111328
Validation loss: 2.265930391127063

Epoch: 5| Step: 10
Training loss: 2.1405842304229736
Validation loss: 2.2787216107050576

Epoch: 310| Step: 0
Training loss: 3.0555484294891357
Validation loss: 2.300632615243235

Epoch: 5| Step: 1
Training loss: 2.790435791015625
Validation loss: 2.3356118484209945

Epoch: 5| Step: 2
Training loss: 2.621462345123291
Validation loss: 2.3714282153755106

Epoch: 5| Step: 3
Training loss: 2.503211259841919
Validation loss: 2.3692170753273913

Epoch: 5| Step: 4
Training loss: 2.236732244491577
Validation loss: 2.3649033141392533

Epoch: 5| Step: 5
Training loss: 1.7650600671768188
Validation loss: 2.347445910976779

Epoch: 5| Step: 6
Training loss: 2.594132900238037
Validation loss: 2.3478463926622943

Epoch: 5| Step: 7
Training loss: 2.2316126823425293
Validation loss: 2.309173325056671

Epoch: 5| Step: 8
Training loss: 2.950021505355835
Validation loss: 2.302197640941989

Epoch: 5| Step: 9
Training loss: 2.2049307823181152
Validation loss: 2.2926745658279746

Epoch: 5| Step: 10
Training loss: 1.8359708786010742
Validation loss: 2.2885100380066903

Epoch: 311| Step: 0
Training loss: 2.4719936847686768
Validation loss: 2.286756980803705

Epoch: 5| Step: 1
Training loss: 1.7939023971557617
Validation loss: 2.2737928539194088

Epoch: 5| Step: 2
Training loss: 2.43890118598938
Validation loss: 2.2860796067022506

Epoch: 5| Step: 3
Training loss: 3.0963330268859863
Validation loss: 2.2806319549519527

Epoch: 5| Step: 4
Training loss: 2.4640541076660156
Validation loss: 2.289990996801725

Epoch: 5| Step: 5
Training loss: 2.175318479537964
Validation loss: 2.2928097747987315

Epoch: 5| Step: 6
Training loss: 2.56154203414917
Validation loss: 2.2957554440344534

Epoch: 5| Step: 7
Training loss: 2.8702175617218018
Validation loss: 2.3080961550435712

Epoch: 5| Step: 8
Training loss: 2.081047296524048
Validation loss: 2.30539688756389

Epoch: 5| Step: 9
Training loss: 2.179521083831787
Validation loss: 2.293487782119423

Epoch: 5| Step: 10
Training loss: 2.7223622798919678
Validation loss: 2.305942097017842

Epoch: 312| Step: 0
Training loss: 2.707926034927368
Validation loss: 2.295761154543969

Epoch: 5| Step: 1
Training loss: 2.2104547023773193
Validation loss: 2.2793546453599007

Epoch: 5| Step: 2
Training loss: 2.4630253314971924
Validation loss: 2.283747701234715

Epoch: 5| Step: 3
Training loss: 2.238447904586792
Validation loss: 2.2775616389448925

Epoch: 5| Step: 4
Training loss: 2.7746899127960205
Validation loss: 2.2785282417010237

Epoch: 5| Step: 5
Training loss: 2.409278631210327
Validation loss: 2.2799028273551696

Epoch: 5| Step: 6
Training loss: 2.567115306854248
Validation loss: 2.2888783959932226

Epoch: 5| Step: 7
Training loss: 2.52500057220459
Validation loss: 2.2767710826730214

Epoch: 5| Step: 8
Training loss: 2.4135658740997314
Validation loss: 2.2860656784426783

Epoch: 5| Step: 9
Training loss: 2.377110719680786
Validation loss: 2.273814801246889

Epoch: 5| Step: 10
Training loss: 1.9509429931640625
Validation loss: 2.278094196832308

Epoch: 313| Step: 0
Training loss: 1.2422847747802734
Validation loss: 2.2735349337259927

Epoch: 5| Step: 1
Training loss: 2.6056151390075684
Validation loss: 2.276744786129203

Epoch: 5| Step: 2
Training loss: 2.2889559268951416
Validation loss: 2.27230901615594

Epoch: 5| Step: 3
Training loss: 2.463779926300049
Validation loss: 2.2883390303580993

Epoch: 5| Step: 4
Training loss: 2.2816243171691895
Validation loss: 2.307196860672325

Epoch: 5| Step: 5
Training loss: 2.405869722366333
Validation loss: 2.3156145977717575

Epoch: 5| Step: 6
Training loss: 2.1208832263946533
Validation loss: 2.3042776148806334

Epoch: 5| Step: 7
Training loss: 2.850170612335205
Validation loss: 2.304145984752204

Epoch: 5| Step: 8
Training loss: 2.370779037475586
Validation loss: 2.2832372624387025

Epoch: 5| Step: 9
Training loss: 3.2362008094787598
Validation loss: 2.2868486245473227

Epoch: 5| Step: 10
Training loss: 2.9271674156188965
Validation loss: 2.2715102934068248

Epoch: 314| Step: 0
Training loss: 2.0343704223632812
Validation loss: 2.269423912930232

Epoch: 5| Step: 1
Training loss: 2.515746593475342
Validation loss: 2.269303749966365

Epoch: 5| Step: 2
Training loss: 2.337174654006958
Validation loss: 2.2674718210774083

Epoch: 5| Step: 3
Training loss: 2.5416042804718018
Validation loss: 2.2560716687992053

Epoch: 5| Step: 4
Training loss: 2.6599793434143066
Validation loss: 2.272858045434439

Epoch: 5| Step: 5
Training loss: 1.8946300745010376
Validation loss: 2.2681565207819783

Epoch: 5| Step: 6
Training loss: 2.4793460369110107
Validation loss: 2.2598764973302043

Epoch: 5| Step: 7
Training loss: 3.1542563438415527
Validation loss: 2.2685367420155513

Epoch: 5| Step: 8
Training loss: 1.7719717025756836
Validation loss: 2.274473759435838

Epoch: 5| Step: 9
Training loss: 2.432291030883789
Validation loss: 2.2733413634761686

Epoch: 5| Step: 10
Training loss: 2.8845081329345703
Validation loss: 2.2775948227092786

Epoch: 315| Step: 0
Training loss: 2.159522533416748
Validation loss: 2.2759644703198503

Epoch: 5| Step: 1
Training loss: 2.441544771194458
Validation loss: 2.2808543635952856

Epoch: 5| Step: 2
Training loss: 1.9746782779693604
Validation loss: 2.282578840050646

Epoch: 5| Step: 3
Training loss: 3.0389838218688965
Validation loss: 2.280371562127144

Epoch: 5| Step: 4
Training loss: 2.2205429077148438
Validation loss: 2.279134609365976

Epoch: 5| Step: 5
Training loss: 2.82979154586792
Validation loss: 2.275037883430399

Epoch: 5| Step: 6
Training loss: 2.038443088531494
Validation loss: 2.278355970177599

Epoch: 5| Step: 7
Training loss: 2.175013303756714
Validation loss: 2.283494241776005

Epoch: 5| Step: 8
Training loss: 2.9056293964385986
Validation loss: 2.2834535644900416

Epoch: 5| Step: 9
Training loss: 2.484644889831543
Validation loss: 2.274052830152614

Epoch: 5| Step: 10
Training loss: 2.44588303565979
Validation loss: 2.2887213409587903

Epoch: 316| Step: 0
Training loss: 2.3946757316589355
Validation loss: 2.282552082051513

Epoch: 5| Step: 1
Training loss: 2.752554416656494
Validation loss: 2.27840946566674

Epoch: 5| Step: 2
Training loss: 2.6881814002990723
Validation loss: 2.2881744292474564

Epoch: 5| Step: 3
Training loss: 2.3954319953918457
Validation loss: 2.271283224064817

Epoch: 5| Step: 4
Training loss: 2.330719470977783
Validation loss: 2.29431700962846

Epoch: 5| Step: 5
Training loss: 1.7964417934417725
Validation loss: 2.2975008077518915

Epoch: 5| Step: 6
Training loss: 2.319817066192627
Validation loss: 2.2955531522791874

Epoch: 5| Step: 7
Training loss: 2.1491692066192627
Validation loss: 2.298491548466426

Epoch: 5| Step: 8
Training loss: 2.7772879600524902
Validation loss: 2.314539788871683

Epoch: 5| Step: 9
Training loss: 2.069610118865967
Validation loss: 2.313146757823165

Epoch: 5| Step: 10
Training loss: 2.9759368896484375
Validation loss: 2.3193187713623047

Epoch: 317| Step: 0
Training loss: 2.190199851989746
Validation loss: 2.297310854799004

Epoch: 5| Step: 1
Training loss: 2.257513999938965
Validation loss: 2.3110858419890046

Epoch: 5| Step: 2
Training loss: 3.153773069381714
Validation loss: 2.29734306950723

Epoch: 5| Step: 3
Training loss: 2.231307029724121
Validation loss: 2.277868534929009

Epoch: 5| Step: 4
Training loss: 2.0829365253448486
Validation loss: 2.2747626266171856

Epoch: 5| Step: 5
Training loss: 2.1529598236083984
Validation loss: 2.256877194168747

Epoch: 5| Step: 6
Training loss: 2.1117501258850098
Validation loss: 2.261825417959562

Epoch: 5| Step: 7
Training loss: 2.562866449356079
Validation loss: 2.2680221270489436

Epoch: 5| Step: 8
Training loss: 2.5529422760009766
Validation loss: 2.2621082387944704

Epoch: 5| Step: 9
Training loss: 2.3952267169952393
Validation loss: 2.2733550815172094

Epoch: 5| Step: 10
Training loss: 3.043225049972534
Validation loss: 2.265956037787981

Epoch: 318| Step: 0
Training loss: 2.6074328422546387
Validation loss: 2.2745175489815335

Epoch: 5| Step: 1
Training loss: 2.441227436065674
Validation loss: 2.282302510353827

Epoch: 5| Step: 2
Training loss: 3.385406970977783
Validation loss: 2.291467951190087

Epoch: 5| Step: 3
Training loss: 2.6689138412475586
Validation loss: 2.2818628934121903

Epoch: 5| Step: 4
Training loss: 2.281958818435669
Validation loss: 2.2876359083319224

Epoch: 5| Step: 5
Training loss: 2.0164737701416016
Validation loss: 2.3036574753381873

Epoch: 5| Step: 6
Training loss: 2.30794358253479
Validation loss: 2.2858638122517574

Epoch: 5| Step: 7
Training loss: 2.315040111541748
Validation loss: 2.3131207035433863

Epoch: 5| Step: 8
Training loss: 2.338043689727783
Validation loss: 2.291915370571998

Epoch: 5| Step: 9
Training loss: 2.209545373916626
Validation loss: 2.282543871992378

Epoch: 5| Step: 10
Training loss: 1.8542671203613281
Validation loss: 2.261340674533639

Epoch: 319| Step: 0
Training loss: 2.6421117782592773
Validation loss: 2.273669478713825

Epoch: 5| Step: 1
Training loss: 1.9047943353652954
Validation loss: 2.2689834358871623

Epoch: 5| Step: 2
Training loss: 2.756821393966675
Validation loss: 2.264711779932822

Epoch: 5| Step: 3
Training loss: 2.9253990650177
Validation loss: 2.266519682381743

Epoch: 5| Step: 4
Training loss: 3.0686376094818115
Validation loss: 2.2528930197479906

Epoch: 5| Step: 5
Training loss: 1.884223222732544
Validation loss: 2.25127500103366

Epoch: 5| Step: 6
Training loss: 1.9954338073730469
Validation loss: 2.2550300295634935

Epoch: 5| Step: 7
Training loss: 1.8338077068328857
Validation loss: 2.2761945955214964

Epoch: 5| Step: 8
Training loss: 2.071558713912964
Validation loss: 2.2820976447033625

Epoch: 5| Step: 9
Training loss: 3.26411771774292
Validation loss: 2.287298784461073

Epoch: 5| Step: 10
Training loss: 2.29349684715271
Validation loss: 2.285300898295577

Epoch: 320| Step: 0
Training loss: 2.0435822010040283
Validation loss: 2.2791634657049693

Epoch: 5| Step: 1
Training loss: 2.7681822776794434
Validation loss: 2.271581026815599

Epoch: 5| Step: 2
Training loss: 2.6470913887023926
Validation loss: 2.275008652799873

Epoch: 5| Step: 3
Training loss: 2.3262908458709717
Validation loss: 2.287261173289309

Epoch: 5| Step: 4
Training loss: 2.527460813522339
Validation loss: 2.3001778792309504

Epoch: 5| Step: 5
Training loss: 2.395883083343506
Validation loss: 2.272775178314537

Epoch: 5| Step: 6
Training loss: 2.8443262577056885
Validation loss: 2.2749390550839004

Epoch: 5| Step: 7
Training loss: 2.1947286128997803
Validation loss: 2.283169372107393

Epoch: 5| Step: 8
Training loss: 2.736097574234009
Validation loss: 2.283224654454057

Epoch: 5| Step: 9
Training loss: 2.0707287788391113
Validation loss: 2.2825662602660475

Epoch: 5| Step: 10
Training loss: 1.810732364654541
Validation loss: 2.2785310104329097

Epoch: 321| Step: 0
Training loss: 2.582643985748291
Validation loss: 2.2806576298129175

Epoch: 5| Step: 1
Training loss: 1.6298913955688477
Validation loss: 2.279162073648104

Epoch: 5| Step: 2
Training loss: 2.3842613697052
Validation loss: 2.2760272077334824

Epoch: 5| Step: 3
Training loss: 2.831757068634033
Validation loss: 2.2604867848016883

Epoch: 5| Step: 4
Training loss: 2.3832709789276123
Validation loss: 2.25578660093328

Epoch: 5| Step: 5
Training loss: 2.788771152496338
Validation loss: 2.2635501379607827

Epoch: 5| Step: 6
Training loss: 2.4385790824890137
Validation loss: 2.272244640575942

Epoch: 5| Step: 7
Training loss: 2.5585379600524902
Validation loss: 2.278708960420342

Epoch: 5| Step: 8
Training loss: 2.0973336696624756
Validation loss: 2.264691088789253

Epoch: 5| Step: 9
Training loss: 2.3673899173736572
Validation loss: 2.2698354233977613

Epoch: 5| Step: 10
Training loss: 2.2929537296295166
Validation loss: 2.2671695473373576

Epoch: 322| Step: 0
Training loss: 2.3341753482818604
Validation loss: 2.2783994995137697

Epoch: 5| Step: 1
Training loss: 1.856574296951294
Validation loss: 2.2738875701863277

Epoch: 5| Step: 2
Training loss: 2.7379863262176514
Validation loss: 2.264248578779159

Epoch: 5| Step: 3
Training loss: 2.95747709274292
Validation loss: 2.2602268085684827

Epoch: 5| Step: 4
Training loss: 2.4387993812561035
Validation loss: 2.2383214145578365

Epoch: 5| Step: 5
Training loss: 2.6069726943969727
Validation loss: 2.252847912491009

Epoch: 5| Step: 6
Training loss: 2.1780693531036377
Validation loss: 2.2478670484276226

Epoch: 5| Step: 7
Training loss: 2.59773325920105
Validation loss: 2.2556035775010304

Epoch: 5| Step: 8
Training loss: 2.2072665691375732
Validation loss: 2.279568461961644

Epoch: 5| Step: 9
Training loss: 1.994363784790039
Validation loss: 2.274239211954096

Epoch: 5| Step: 10
Training loss: 2.610031843185425
Validation loss: 2.278047171972131

Epoch: 323| Step: 0
Training loss: 1.9249929189682007
Validation loss: 2.285982667758901

Epoch: 5| Step: 1
Training loss: 1.8695110082626343
Validation loss: 2.288285414377848

Epoch: 5| Step: 2
Training loss: 2.931326389312744
Validation loss: 2.3047059684671383

Epoch: 5| Step: 3
Training loss: 2.582775592803955
Validation loss: 2.3030596138328634

Epoch: 5| Step: 4
Training loss: 2.5886669158935547
Validation loss: 2.296865378656695

Epoch: 5| Step: 5
Training loss: 2.36303448677063
Validation loss: 2.3080259202629008

Epoch: 5| Step: 6
Training loss: 2.5962178707122803
Validation loss: 2.289240975533762

Epoch: 5| Step: 7
Training loss: 2.4174208641052246
Validation loss: 2.2778992370892595

Epoch: 5| Step: 8
Training loss: 2.336231231689453
Validation loss: 2.254504507587802

Epoch: 5| Step: 9
Training loss: 2.1274571418762207
Validation loss: 2.253799961459252

Epoch: 5| Step: 10
Training loss: 2.7477595806121826
Validation loss: 2.25715232792721

Epoch: 324| Step: 0
Training loss: 1.863215684890747
Validation loss: 2.2597927201178765

Epoch: 5| Step: 1
Training loss: 2.3500847816467285
Validation loss: 2.251806725737869

Epoch: 5| Step: 2
Training loss: 2.377643585205078
Validation loss: 2.248246567223662

Epoch: 5| Step: 3
Training loss: 3.324193239212036
Validation loss: 2.2545622779477026

Epoch: 5| Step: 4
Training loss: 1.9378172159194946
Validation loss: 2.2441869653681272

Epoch: 5| Step: 5
Training loss: 2.744719982147217
Validation loss: 2.2417081402194117

Epoch: 5| Step: 6
Training loss: 2.0209670066833496
Validation loss: 2.2530691867233603

Epoch: 5| Step: 7
Training loss: 2.685546875
Validation loss: 2.25156137763813

Epoch: 5| Step: 8
Training loss: 1.719286561012268
Validation loss: 2.2606888714657036

Epoch: 5| Step: 9
Training loss: 2.7213428020477295
Validation loss: 2.2689450453686457

Epoch: 5| Step: 10
Training loss: 2.613140106201172
Validation loss: 2.275882413310389

Epoch: 325| Step: 0
Training loss: 2.167208433151245
Validation loss: 2.2885449586376065

Epoch: 5| Step: 1
Training loss: 1.5309574604034424
Validation loss: 2.3037270551086753

Epoch: 5| Step: 2
Training loss: 2.2674832344055176
Validation loss: 2.3176320265698176

Epoch: 5| Step: 3
Training loss: 2.9363021850585938
Validation loss: 2.2934168051647883

Epoch: 5| Step: 4
Training loss: 2.3523077964782715
Validation loss: 2.2891330206266014

Epoch: 5| Step: 5
Training loss: 2.200374126434326
Validation loss: 2.2854493766702633

Epoch: 5| Step: 6
Training loss: 2.42063570022583
Validation loss: 2.271441000764088

Epoch: 5| Step: 7
Training loss: 2.823441743850708
Validation loss: 2.254055666667159

Epoch: 5| Step: 8
Training loss: 2.571718215942383
Validation loss: 2.260650528374539

Epoch: 5| Step: 9
Training loss: 2.6170246601104736
Validation loss: 2.2491166617280696

Epoch: 5| Step: 10
Training loss: 2.538146734237671
Validation loss: 2.2412035119148994

Epoch: 326| Step: 0
Training loss: 2.9188754558563232
Validation loss: 2.2595671966511715

Epoch: 5| Step: 1
Training loss: 2.5319759845733643
Validation loss: 2.2421918376799552

Epoch: 5| Step: 2
Training loss: 2.5283145904541016
Validation loss: 2.244172648716998

Epoch: 5| Step: 3
Training loss: 2.4822208881378174
Validation loss: 2.263621478952387

Epoch: 5| Step: 4
Training loss: 2.0693259239196777
Validation loss: 2.2608450587077806

Epoch: 5| Step: 5
Training loss: 1.8044906854629517
Validation loss: 2.2789594204195085

Epoch: 5| Step: 6
Training loss: 1.7032501697540283
Validation loss: 2.295316514148507

Epoch: 5| Step: 7
Training loss: 2.534975051879883
Validation loss: 2.30138389782239

Epoch: 5| Step: 8
Training loss: 2.824497699737549
Validation loss: 2.304876232659945

Epoch: 5| Step: 9
Training loss: 2.9021973609924316
Validation loss: 2.2924126860915974

Epoch: 5| Step: 10
Training loss: 1.9641530513763428
Validation loss: 2.297310600998581

Epoch: 327| Step: 0
Training loss: 2.9556775093078613
Validation loss: 2.2874135163522538

Epoch: 5| Step: 1
Training loss: 2.387277126312256
Validation loss: 2.2797870815441175

Epoch: 5| Step: 2
Training loss: 2.324169635772705
Validation loss: 2.277773357206775

Epoch: 5| Step: 3
Training loss: 2.791781187057495
Validation loss: 2.2731904560519802

Epoch: 5| Step: 4
Training loss: 1.9189687967300415
Validation loss: 2.26894732188153

Epoch: 5| Step: 5
Training loss: 2.3408210277557373
Validation loss: 2.273302719157229

Epoch: 5| Step: 6
Training loss: 1.9217727184295654
Validation loss: 2.2668895311253046

Epoch: 5| Step: 7
Training loss: 3.133812427520752
Validation loss: 2.2546610601486696

Epoch: 5| Step: 8
Training loss: 2.2262039184570312
Validation loss: 2.252152014804143

Epoch: 5| Step: 9
Training loss: 2.1698989868164062
Validation loss: 2.256700074800881

Epoch: 5| Step: 10
Training loss: 2.1027605533599854
Validation loss: 2.2651561537096576

Epoch: 328| Step: 0
Training loss: 2.355278730392456
Validation loss: 2.2555690426980295

Epoch: 5| Step: 1
Training loss: 2.2108712196350098
Validation loss: 2.264307442531791

Epoch: 5| Step: 2
Training loss: 2.2405807971954346
Validation loss: 2.270075136615384

Epoch: 5| Step: 3
Training loss: 2.5482583045959473
Validation loss: 2.2740921871636504

Epoch: 5| Step: 4
Training loss: 2.7134854793548584
Validation loss: 2.2868068679686515

Epoch: 5| Step: 5
Training loss: 2.283473253250122
Validation loss: 2.2680711848761446

Epoch: 5| Step: 6
Training loss: 1.7525489330291748
Validation loss: 2.2759340809237574

Epoch: 5| Step: 7
Training loss: 2.559156894683838
Validation loss: 2.2758557283750145

Epoch: 5| Step: 8
Training loss: 2.4033102989196777
Validation loss: 2.2796933804788897

Epoch: 5| Step: 9
Training loss: 2.7891793251037598
Validation loss: 2.2711935620154104

Epoch: 5| Step: 10
Training loss: 2.3043859004974365
Validation loss: 2.27951745576756

Epoch: 329| Step: 0
Training loss: 1.734331727027893
Validation loss: 2.271385431289673

Epoch: 5| Step: 1
Training loss: 2.4844791889190674
Validation loss: 2.2802725427894184

Epoch: 5| Step: 2
Training loss: 2.5324227809906006
Validation loss: 2.2605423260760564

Epoch: 5| Step: 3
Training loss: 2.3936827182769775
Validation loss: 2.25719892081394

Epoch: 5| Step: 4
Training loss: 2.1608238220214844
Validation loss: 2.2559689219279955

Epoch: 5| Step: 5
Training loss: 2.8161349296569824
Validation loss: 2.2541444891242572

Epoch: 5| Step: 6
Training loss: 2.7870118618011475
Validation loss: 2.2514190699464534

Epoch: 5| Step: 7
Training loss: 2.1924917697906494
Validation loss: 2.257332073744907

Epoch: 5| Step: 8
Training loss: 2.8065013885498047
Validation loss: 2.2710553471760084

Epoch: 5| Step: 9
Training loss: 2.1929988861083984
Validation loss: 2.266964461213799

Epoch: 5| Step: 10
Training loss: 2.009066343307495
Validation loss: 2.2636216340526456

Epoch: 330| Step: 0
Training loss: 2.051847219467163
Validation loss: 2.254500135298698

Epoch: 5| Step: 1
Training loss: 2.133021116256714
Validation loss: 2.2572748609768447

Epoch: 5| Step: 2
Training loss: 2.5945816040039062
Validation loss: 2.249946362228804

Epoch: 5| Step: 3
Training loss: 2.321500539779663
Validation loss: 2.275361684060866

Epoch: 5| Step: 4
Training loss: 2.569380044937134
Validation loss: 2.2598117936042046

Epoch: 5| Step: 5
Training loss: 2.3019309043884277
Validation loss: 2.26195611235916

Epoch: 5| Step: 6
Training loss: 3.0064167976379395
Validation loss: 2.267264099531276

Epoch: 5| Step: 7
Training loss: 2.4725430011749268
Validation loss: 2.2641408597269366

Epoch: 5| Step: 8
Training loss: 2.5255165100097656
Validation loss: 2.2681452228176977

Epoch: 5| Step: 9
Training loss: 2.6292693614959717
Validation loss: 2.268505201544813

Epoch: 5| Step: 10
Training loss: 1.437698483467102
Validation loss: 2.259504190055273

Epoch: 331| Step: 0
Training loss: 2.380521297454834
Validation loss: 2.2547836329347346

Epoch: 5| Step: 1
Training loss: 2.4278433322906494
Validation loss: 2.249954831215643

Epoch: 5| Step: 2
Training loss: 2.649343729019165
Validation loss: 2.2402801103489374

Epoch: 5| Step: 3
Training loss: 2.580357074737549
Validation loss: 2.2414887233447005

Epoch: 5| Step: 4
Training loss: 2.7257704734802246
Validation loss: 2.243954122707408

Epoch: 5| Step: 5
Training loss: 2.2332260608673096
Validation loss: 2.2445446842460224

Epoch: 5| Step: 6
Training loss: 2.024644374847412
Validation loss: 2.2537402863143594

Epoch: 5| Step: 7
Training loss: 2.010045051574707
Validation loss: 2.2608294563908733

Epoch: 5| Step: 8
Training loss: 2.3608999252319336
Validation loss: 2.2522658019937496

Epoch: 5| Step: 9
Training loss: 2.3892662525177
Validation loss: 2.260802807346467

Epoch: 5| Step: 10
Training loss: 2.40973162651062
Validation loss: 2.2663682840203725

Epoch: 332| Step: 0
Training loss: 2.7248666286468506
Validation loss: 2.2922481695810952

Epoch: 5| Step: 1
Training loss: 3.147517681121826
Validation loss: 2.3105935306959253

Epoch: 5| Step: 2
Training loss: 3.1811881065368652
Validation loss: 2.317381574261573

Epoch: 5| Step: 3
Training loss: 1.8620250225067139
Validation loss: 2.3134077236216557

Epoch: 5| Step: 4
Training loss: 2.6596953868865967
Validation loss: 2.2787722003075386

Epoch: 5| Step: 5
Training loss: 1.8536205291748047
Validation loss: 2.281928426475935

Epoch: 5| Step: 6
Training loss: 2.307661294937134
Validation loss: 2.268522711210353

Epoch: 5| Step: 7
Training loss: 2.184173583984375
Validation loss: 2.2553629054818103

Epoch: 5| Step: 8
Training loss: 2.179856777191162
Validation loss: 2.244128555379888

Epoch: 5| Step: 9
Training loss: 1.884847640991211
Validation loss: 2.2560398296643327

Epoch: 5| Step: 10
Training loss: 2.3130154609680176
Validation loss: 2.257549388434297

Epoch: 333| Step: 0
Training loss: 2.527015447616577
Validation loss: 2.264905309164396

Epoch: 5| Step: 1
Training loss: 2.5094010829925537
Validation loss: 2.284114776119109

Epoch: 5| Step: 2
Training loss: 1.9419008493423462
Validation loss: 2.292344436850599

Epoch: 5| Step: 3
Training loss: 2.305375814437866
Validation loss: 2.288471039905343

Epoch: 5| Step: 4
Training loss: 1.8082672357559204
Validation loss: 2.3051586830487816

Epoch: 5| Step: 5
Training loss: 2.255314350128174
Validation loss: 2.2980563896958546

Epoch: 5| Step: 6
Training loss: 2.1210389137268066
Validation loss: 2.3083335635482625

Epoch: 5| Step: 7
Training loss: 2.814852714538574
Validation loss: 2.3037438738730645

Epoch: 5| Step: 8
Training loss: 2.067099094390869
Validation loss: 2.2872604195789625

Epoch: 5| Step: 9
Training loss: 2.899768352508545
Validation loss: 2.2739213461517007

Epoch: 5| Step: 10
Training loss: 2.888895273208618
Validation loss: 2.253270337658544

Epoch: 334| Step: 0
Training loss: 2.727116107940674
Validation loss: 2.256226985685287

Epoch: 5| Step: 1
Training loss: 2.794992446899414
Validation loss: 2.248873079976728

Epoch: 5| Step: 2
Training loss: 2.3808910846710205
Validation loss: 2.2397487381453156

Epoch: 5| Step: 3
Training loss: 1.8626149892807007
Validation loss: 2.240466785687272

Epoch: 5| Step: 4
Training loss: 2.8947994709014893
Validation loss: 2.2392891119885188

Epoch: 5| Step: 5
Training loss: 1.9423186779022217
Validation loss: 2.2338885145802654

Epoch: 5| Step: 6
Training loss: 2.665597438812256
Validation loss: 2.235761063073271

Epoch: 5| Step: 7
Training loss: 2.131762981414795
Validation loss: 2.2344346738630727

Epoch: 5| Step: 8
Training loss: 2.4368655681610107
Validation loss: 2.2409005908555883

Epoch: 5| Step: 9
Training loss: 2.213928699493408
Validation loss: 2.2627223717269076

Epoch: 5| Step: 10
Training loss: 1.970774531364441
Validation loss: 2.269296843518493

Epoch: 335| Step: 0
Training loss: 2.305032730102539
Validation loss: 2.292858798016784

Epoch: 5| Step: 1
Training loss: 2.2068583965301514
Validation loss: 2.2847244149895123

Epoch: 5| Step: 2
Training loss: 2.658154010772705
Validation loss: 2.2841269944303777

Epoch: 5| Step: 3
Training loss: 2.1525158882141113
Validation loss: 2.2692344957782375

Epoch: 5| Step: 4
Training loss: 2.291978120803833
Validation loss: 2.2687258540943103

Epoch: 5| Step: 5
Training loss: 1.5946643352508545
Validation loss: 2.2595651713750695

Epoch: 5| Step: 6
Training loss: 3.2559738159179688
Validation loss: 2.247256839147178

Epoch: 5| Step: 7
Training loss: 2.194434881210327
Validation loss: 2.2492670166877007

Epoch: 5| Step: 8
Training loss: 2.2373580932617188
Validation loss: 2.2471138251725065

Epoch: 5| Step: 9
Training loss: 2.731341600418091
Validation loss: 2.240411789186539

Epoch: 5| Step: 10
Training loss: 2.441317558288574
Validation loss: 2.234262468994305

Epoch: 336| Step: 0
Training loss: 2.8795320987701416
Validation loss: 2.2329791848377516

Epoch: 5| Step: 1
Training loss: 2.7425014972686768
Validation loss: 2.2470548588742494

Epoch: 5| Step: 2
Training loss: 2.5414721965789795
Validation loss: 2.27188733572601

Epoch: 5| Step: 3
Training loss: 2.0087223052978516
Validation loss: 2.26439046090649

Epoch: 5| Step: 4
Training loss: 1.692087173461914
Validation loss: 2.267394076111496

Epoch: 5| Step: 5
Training loss: 2.33198618888855
Validation loss: 2.2591071487754903

Epoch: 5| Step: 6
Training loss: 2.3484928607940674
Validation loss: 2.275245854931493

Epoch: 5| Step: 7
Training loss: 3.15120267868042
Validation loss: 2.269116317072222

Epoch: 5| Step: 8
Training loss: 2.3907039165496826
Validation loss: 2.2728768625567035

Epoch: 5| Step: 9
Training loss: 1.810960054397583
Validation loss: 2.277383276211318

Epoch: 5| Step: 10
Training loss: 2.161619186401367
Validation loss: 2.2885148191964753

Epoch: 337| Step: 0
Training loss: 1.7826257944107056
Validation loss: 2.285011988814159

Epoch: 5| Step: 1
Training loss: 2.5036654472351074
Validation loss: 2.2758246698687152

Epoch: 5| Step: 2
Training loss: 2.117511749267578
Validation loss: 2.2824265597968973

Epoch: 5| Step: 3
Training loss: 2.059020757675171
Validation loss: 2.283907640364862

Epoch: 5| Step: 4
Training loss: 2.6800503730773926
Validation loss: 2.2907091007437757

Epoch: 5| Step: 5
Training loss: 2.512808322906494
Validation loss: 2.2790597997685915

Epoch: 5| Step: 6
Training loss: 2.813124656677246
Validation loss: 2.2801219096747776

Epoch: 5| Step: 7
Training loss: 2.6323788166046143
Validation loss: 2.2757766272432063

Epoch: 5| Step: 8
Training loss: 2.5734026432037354
Validation loss: 2.2538958685372465

Epoch: 5| Step: 9
Training loss: 1.7417198419570923
Validation loss: 2.2613606504214707

Epoch: 5| Step: 10
Training loss: 2.7570159435272217
Validation loss: 2.264649739829443

Epoch: 338| Step: 0
Training loss: 2.5476174354553223
Validation loss: 2.2696476700485393

Epoch: 5| Step: 1
Training loss: 2.1333439350128174
Validation loss: 2.265117000508052

Epoch: 5| Step: 2
Training loss: 2.1815083026885986
Validation loss: 2.256886571966192

Epoch: 5| Step: 3
Training loss: 1.8940407037734985
Validation loss: 2.253758861172584

Epoch: 5| Step: 4
Training loss: 2.019378185272217
Validation loss: 2.238428618318291

Epoch: 5| Step: 5
Training loss: 2.843909740447998
Validation loss: 2.2344432005318264

Epoch: 5| Step: 6
Training loss: 2.4492921829223633
Validation loss: 2.23636059863593

Epoch: 5| Step: 7
Training loss: 2.3851158618927
Validation loss: 2.2630845423667663

Epoch: 5| Step: 8
Training loss: 2.4235658645629883
Validation loss: 2.271039185985442

Epoch: 5| Step: 9
Training loss: 2.7819783687591553
Validation loss: 2.2708546653870614

Epoch: 5| Step: 10
Training loss: 2.674107074737549
Validation loss: 2.274509988805299

Epoch: 339| Step: 0
Training loss: 2.331535816192627
Validation loss: 2.2831757965908257

Epoch: 5| Step: 1
Training loss: 3.04807186126709
Validation loss: 2.2946335384922643

Epoch: 5| Step: 2
Training loss: 2.3346107006073
Validation loss: 2.293342767223235

Epoch: 5| Step: 3
Training loss: 2.3264107704162598
Validation loss: 2.3048867358956286

Epoch: 5| Step: 4
Training loss: 2.628169298171997
Validation loss: 2.3046036202420472

Epoch: 5| Step: 5
Training loss: 1.8338191509246826
Validation loss: 2.2943906809694026

Epoch: 5| Step: 6
Training loss: 2.137263536453247
Validation loss: 2.28099916314566

Epoch: 5| Step: 7
Training loss: 2.0038604736328125
Validation loss: 2.277150243841192

Epoch: 5| Step: 8
Training loss: 1.5282009840011597
Validation loss: 2.2734287887491207

Epoch: 5| Step: 9
Training loss: 3.2418525218963623
Validation loss: 2.283494717331343

Epoch: 5| Step: 10
Training loss: 2.7582812309265137
Validation loss: 2.2613424280638337

Epoch: 340| Step: 0
Training loss: 1.7930185794830322
Validation loss: 2.2556112017682803

Epoch: 5| Step: 1
Training loss: 2.3705904483795166
Validation loss: 2.2431174606405277

Epoch: 5| Step: 2
Training loss: 1.993755578994751
Validation loss: 2.2523027338007444

Epoch: 5| Step: 3
Training loss: 2.3094325065612793
Validation loss: 2.2487256808947493

Epoch: 5| Step: 4
Training loss: 2.7695090770721436
Validation loss: 2.2287116499357325

Epoch: 5| Step: 5
Training loss: 2.7995638847351074
Validation loss: 2.2320483474321264

Epoch: 5| Step: 6
Training loss: 2.536031723022461
Validation loss: 2.2350226371519026

Epoch: 5| Step: 7
Training loss: 2.244917154312134
Validation loss: 2.234914405371553

Epoch: 5| Step: 8
Training loss: 2.55775785446167
Validation loss: 2.2365231975432365

Epoch: 5| Step: 9
Training loss: 2.360434055328369
Validation loss: 2.2404491824488484

Epoch: 5| Step: 10
Training loss: 2.3243911266326904
Validation loss: 2.246793185510943

Epoch: 341| Step: 0
Training loss: 2.380164623260498
Validation loss: 2.2701655510933167

Epoch: 5| Step: 1
Training loss: 2.7532880306243896
Validation loss: 2.2697318036069154

Epoch: 5| Step: 2
Training loss: 2.798905611038208
Validation loss: 2.283019047911449

Epoch: 5| Step: 3
Training loss: 2.405529022216797
Validation loss: 2.283491355116649

Epoch: 5| Step: 4
Training loss: 2.6529541015625
Validation loss: 2.275440190428047

Epoch: 5| Step: 5
Training loss: 1.4236136674880981
Validation loss: 2.2623204544026363

Epoch: 5| Step: 6
Training loss: 2.2394015789031982
Validation loss: 2.239645731064581

Epoch: 5| Step: 7
Training loss: 2.577496290206909
Validation loss: 2.2371363537285918

Epoch: 5| Step: 8
Training loss: 2.1970744132995605
Validation loss: 2.2499153793499036

Epoch: 5| Step: 9
Training loss: 1.9952828884124756
Validation loss: 2.2760282485715804

Epoch: 5| Step: 10
Training loss: 2.5296149253845215
Validation loss: 2.270995542567263

Epoch: 342| Step: 0
Training loss: 2.725599527359009
Validation loss: 2.2957801882938673

Epoch: 5| Step: 1
Training loss: 1.6360595226287842
Validation loss: 2.2952906213780886

Epoch: 5| Step: 2
Training loss: 2.968602418899536
Validation loss: 2.2908967720564974

Epoch: 5| Step: 3
Training loss: 2.831847667694092
Validation loss: 2.278688646131946

Epoch: 5| Step: 4
Training loss: 1.9643951654434204
Validation loss: 2.258981502184304

Epoch: 5| Step: 5
Training loss: 2.09157133102417
Validation loss: 2.24141945633837

Epoch: 5| Step: 6
Training loss: 2.0874385833740234
Validation loss: 2.2371368459475938

Epoch: 5| Step: 7
Training loss: 2.2803101539611816
Validation loss: 2.226694986384402

Epoch: 5| Step: 8
Training loss: 1.9725914001464844
Validation loss: 2.221745624337145

Epoch: 5| Step: 9
Training loss: 2.893373489379883
Validation loss: 2.2311273774793072

Epoch: 5| Step: 10
Training loss: 2.634773015975952
Validation loss: 2.212625736831337

Epoch: 343| Step: 0
Training loss: 2.5770909786224365
Validation loss: 2.236211516523874

Epoch: 5| Step: 1
Training loss: 1.9345481395721436
Validation loss: 2.2363778378373835

Epoch: 5| Step: 2
Training loss: 2.6306042671203613
Validation loss: 2.2531720720311648

Epoch: 5| Step: 3
Training loss: 2.6011135578155518
Validation loss: 2.2662351721076557

Epoch: 5| Step: 4
Training loss: 2.358452081680298
Validation loss: 2.282698513359152

Epoch: 5| Step: 5
Training loss: 2.4989593029022217
Validation loss: 2.2768386871584

Epoch: 5| Step: 6
Training loss: 1.9425220489501953
Validation loss: 2.2606176625015917

Epoch: 5| Step: 7
Training loss: 2.1863350868225098
Validation loss: 2.263041921841201

Epoch: 5| Step: 8
Training loss: 2.0040626525878906
Validation loss: 2.2527932633635817

Epoch: 5| Step: 9
Training loss: 2.8328278064727783
Validation loss: 2.2393774652993805

Epoch: 5| Step: 10
Training loss: 2.2638492584228516
Validation loss: 2.235504141417883

Epoch: 344| Step: 0
Training loss: 2.38525390625
Validation loss: 2.243549262323687

Epoch: 5| Step: 1
Training loss: 1.8975820541381836
Validation loss: 2.2316215140845186

Epoch: 5| Step: 2
Training loss: 2.0403189659118652
Validation loss: 2.2377197973189817

Epoch: 5| Step: 3
Training loss: 2.579702138900757
Validation loss: 2.26213357140941

Epoch: 5| Step: 4
Training loss: 2.7053470611572266
Validation loss: 2.2577085289903867

Epoch: 5| Step: 5
Training loss: 2.9489970207214355
Validation loss: 2.266823337924096

Epoch: 5| Step: 6
Training loss: 2.306381940841675
Validation loss: 2.2806072696562736

Epoch: 5| Step: 7
Training loss: 2.261770486831665
Validation loss: 2.2928517172413487

Epoch: 5| Step: 8
Training loss: 2.7933497428894043
Validation loss: 2.2802286609526603

Epoch: 5| Step: 9
Training loss: 1.9853582382202148
Validation loss: 2.2930060868622153

Epoch: 5| Step: 10
Training loss: 1.8012986183166504
Validation loss: 2.2599252911024195

Epoch: 345| Step: 0
Training loss: 2.2841646671295166
Validation loss: 2.2788267289438555

Epoch: 5| Step: 1
Training loss: 2.251861333847046
Validation loss: 2.2792975184737996

Epoch: 5| Step: 2
Training loss: 2.254197120666504
Validation loss: 2.2810270453012116

Epoch: 5| Step: 3
Training loss: 3.0988430976867676
Validation loss: 2.2649314352261123

Epoch: 5| Step: 4
Training loss: 2.184959888458252
Validation loss: 2.263043377989082

Epoch: 5| Step: 5
Training loss: 2.931598663330078
Validation loss: 2.256421107117848

Epoch: 5| Step: 6
Training loss: 2.394339084625244
Validation loss: 2.2453685114460606

Epoch: 5| Step: 7
Training loss: 1.5118695497512817
Validation loss: 2.256640916229576

Epoch: 5| Step: 8
Training loss: 2.411191940307617
Validation loss: 2.2510437478301344

Epoch: 5| Step: 9
Training loss: 2.0535550117492676
Validation loss: 2.247891220995175

Epoch: 5| Step: 10
Training loss: 2.4218404293060303
Validation loss: 2.255596000661132

Epoch: 346| Step: 0
Training loss: 2.749178409576416
Validation loss: 2.2379421623804236

Epoch: 5| Step: 1
Training loss: 2.1985840797424316
Validation loss: 2.24253967244138

Epoch: 5| Step: 2
Training loss: 2.0466017723083496
Validation loss: 2.2417802400486444

Epoch: 5| Step: 3
Training loss: 1.8890819549560547
Validation loss: 2.2329189495373796

Epoch: 5| Step: 4
Training loss: 2.3297996520996094
Validation loss: 2.257155318414011

Epoch: 5| Step: 5
Training loss: 2.287120819091797
Validation loss: 2.2740045388539634

Epoch: 5| Step: 6
Training loss: 2.166149854660034
Validation loss: 2.2793534699306695

Epoch: 5| Step: 7
Training loss: 2.325997829437256
Validation loss: 2.2807763263743412

Epoch: 5| Step: 8
Training loss: 2.170210361480713
Validation loss: 2.278632422929169

Epoch: 5| Step: 9
Training loss: 2.8537371158599854
Validation loss: 2.2826687289822485

Epoch: 5| Step: 10
Training loss: 2.744694471359253
Validation loss: 2.2755522753602717

Epoch: 347| Step: 0
Training loss: 2.8206539154052734
Validation loss: 2.2583328011215373

Epoch: 5| Step: 1
Training loss: 2.488755941390991
Validation loss: 2.242202094806138

Epoch: 5| Step: 2
Training loss: 2.356510639190674
Validation loss: 2.231371869323074

Epoch: 5| Step: 3
Training loss: 2.323704242706299
Validation loss: 2.24222324227774

Epoch: 5| Step: 4
Training loss: 2.2659976482391357
Validation loss: 2.2252926159930486

Epoch: 5| Step: 5
Training loss: 1.950392484664917
Validation loss: 2.244728752361831

Epoch: 5| Step: 6
Training loss: 2.4484524726867676
Validation loss: 2.2583978740117883

Epoch: 5| Step: 7
Training loss: 2.1232995986938477
Validation loss: 2.256482570402084

Epoch: 5| Step: 8
Training loss: 2.5753166675567627
Validation loss: 2.2614264411310994

Epoch: 5| Step: 9
Training loss: 2.5619921684265137
Validation loss: 2.288772270243655

Epoch: 5| Step: 10
Training loss: 1.682917833328247
Validation loss: 2.2773174137197514

Epoch: 348| Step: 0
Training loss: 2.188389301300049
Validation loss: 2.2606013372380245

Epoch: 5| Step: 1
Training loss: 2.789560079574585
Validation loss: 2.268142738649922

Epoch: 5| Step: 2
Training loss: 2.294384479522705
Validation loss: 2.2507738451803885

Epoch: 5| Step: 3
Training loss: 2.0667195320129395
Validation loss: 2.2574899965716946

Epoch: 5| Step: 4
Training loss: 2.0695369243621826
Validation loss: 2.249823498469527

Epoch: 5| Step: 5
Training loss: 2.3326144218444824
Validation loss: 2.2441887445347284

Epoch: 5| Step: 6
Training loss: 2.740123987197876
Validation loss: 2.2360357853674118

Epoch: 5| Step: 7
Training loss: 2.1602418422698975
Validation loss: 2.229336953932239

Epoch: 5| Step: 8
Training loss: 2.080742835998535
Validation loss: 2.244163090182889

Epoch: 5| Step: 9
Training loss: 2.505065441131592
Validation loss: 2.2367181419044413

Epoch: 5| Step: 10
Training loss: 2.436983346939087
Validation loss: 2.235920548439026

Epoch: 349| Step: 0
Training loss: 1.4452930688858032
Validation loss: 2.221999204286965

Epoch: 5| Step: 1
Training loss: 2.2602550983428955
Validation loss: 2.223456562206309

Epoch: 5| Step: 2
Training loss: 2.680293560028076
Validation loss: 2.2332980068781043

Epoch: 5| Step: 3
Training loss: 2.237169027328491
Validation loss: 2.219289945017907

Epoch: 5| Step: 4
Training loss: 2.7602224349975586
Validation loss: 2.2278173239000383

Epoch: 5| Step: 5
Training loss: 2.523559808731079
Validation loss: 2.2405703401052826

Epoch: 5| Step: 6
Training loss: 2.5166068077087402
Validation loss: 2.2502738737290904

Epoch: 5| Step: 7
Training loss: 2.3338751792907715
Validation loss: 2.2451210021972656

Epoch: 5| Step: 8
Training loss: 2.1638362407684326
Validation loss: 2.25981843086981

Epoch: 5| Step: 9
Training loss: 2.077528238296509
Validation loss: 2.247082805120817

Epoch: 5| Step: 10
Training loss: 2.741225481033325
Validation loss: 2.271820798996956

Epoch: 350| Step: 0
Training loss: 2.0023021697998047
Validation loss: 2.265484761166316

Epoch: 5| Step: 1
Training loss: 2.3079745769500732
Validation loss: 2.2524063125733407

Epoch: 5| Step: 2
Training loss: 2.1169705390930176
Validation loss: 2.2465322017669678

Epoch: 5| Step: 3
Training loss: 2.8194172382354736
Validation loss: 2.2442701414067256

Epoch: 5| Step: 4
Training loss: 2.1922245025634766
Validation loss: 2.2401829675961564

Epoch: 5| Step: 5
Training loss: 2.1133549213409424
Validation loss: 2.2193495432535806

Epoch: 5| Step: 6
Training loss: 2.351499080657959
Validation loss: 2.23241425073275

Epoch: 5| Step: 7
Training loss: 2.069692850112915
Validation loss: 2.210862136656238

Epoch: 5| Step: 8
Training loss: 2.585418701171875
Validation loss: 2.23022275329918

Epoch: 5| Step: 9
Training loss: 2.653723955154419
Validation loss: 2.218098020040861

Epoch: 5| Step: 10
Training loss: 2.5181055068969727
Validation loss: 2.2510803643093316

Testing loss: 2.397326694594489
